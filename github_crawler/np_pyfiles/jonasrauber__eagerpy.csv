file_path,api_count,code
setup.py,0,"b'from setuptools import setup\nfrom setuptools import find_packages\nfrom os.path import join, dirname\n\nwith open(join(dirname(__file__), ""eagerpy/VERSION"")) as f:\n    version = f.read().strip()\n\ntry:\n    # obtain long description from README\n    readme_path = join(dirname(__file__), ""README.rst"")\n    with open(readme_path, encoding=""utf-8"") as f:\n        README = f.read()\n        # remove raw html not supported by PyPI\n        README = ""\\n"".join(README.split(""\\n"")[3:])\nexcept IOError:\n    README = """"\n\n\ninstall_requires = [""numpy"", ""typing-extensions>=3.7.4.1""]\ntests_require = [""pytest>=5.3.5"", ""pytest-cov>=2.8.1""]\n\nsetup(\n    name=""eagerpy"",\n    version=version,\n    description=""EagerPy is a thin wrapper around PyTorch, TensorFlow Eager, JAX and NumPy that unifies their interface and thus allows writing code that works natively across all of them."",\n    long_description=README,\n    long_description_content_type=""text/x-rst"",\n    classifiers=[\n        ""Development Status :: 3 - Alpha"",\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Science/Research"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n    ],\n    keywords="""",\n    author=""Jonas Rauber"",\n    author_email=""jonas.rauber@bethgelab.org"",\n    url=""https://github.com/jonasrauber/eagerpy"",\n    license="""",\n    packages=find_packages(),\n    include_package_data=True,\n    zip_safe=False,\n    install_requires=install_requires,\n    extras_require={""testing"": tests_require},\n)\n'"
eagerpy/__init__.py,0,"b'from typing import TypeVar\nfrom os.path import join as _join\nfrom os.path import dirname as _dirname\n\nwith open(_join(_dirname(__file__), ""VERSION"")) as _f:\n    __version__ = _f.read().strip()\n\n_T = TypeVar(""_T"")\n\n\nclass _Indexable:\n    __slots__ = ()\n\n    def __getitem__(self, index: _T) -> _T:\n        return index\n\n\nindex = _Indexable()\n\n\nfrom .tensor import Tensor  # noqa: F401\nfrom .tensor import TensorType  # noqa: F401\nfrom .tensor import istensor  # noqa: F401\n\nfrom .tensor import PyTorchTensor  # noqa: F401\nfrom .tensor import TensorFlowTensor  # noqa: F401\nfrom .tensor import NumPyTensor  # noqa: F401\nfrom .tensor import JAXTensor  # noqa: F401\n\nfrom . import types  # noqa: F401\n\nfrom .astensor import astensor  # noqa: F401\nfrom .astensor import astensors  # noqa: F401\nfrom .astensor import astensor_  # noqa: F401\nfrom .astensor import astensors_  # noqa: F401\n\nfrom .modules import torch  # noqa: F401\nfrom .modules import tensorflow  # noqa: F401\nfrom .modules import jax  # noqa: F401\nfrom .modules import numpy  # noqa: F401\n\nfrom . import utils  # noqa: F401\n\nfrom .framework import *  # noqa: F401,F403\n\nfrom . import norms  # noqa: F401\nfrom .lib import *  # noqa: F401,F403\n'"
eagerpy/astensor.py,0,"b'from typing import TYPE_CHECKING, Union, overload, Tuple, TypeVar, Generic, Any\nimport sys\n\nfrom .tensor import Tensor\nfrom .tensor import TensorType\n\nfrom .tensor import PyTorchTensor\nfrom .tensor import TensorFlowTensor\nfrom .tensor import JAXTensor\nfrom .tensor import NumPyTensor\n\nfrom .types import NativeTensor\n\nif TYPE_CHECKING:\n    # for static analyzers\n    import torch\n\n\ndef _get_module_name(x: Any) -> str:\n    # splitting is necessary for TensorFlow tensors\n    return type(x).__module__.split(""."")[0]\n\n\n@overload\ndef astensor(x: TensorType) -> TensorType:\n    ...\n\n\n@overload\ndef astensor(x: ""torch.Tensor"") -> PyTorchTensor:\n    ...\n\n\n@overload\ndef astensor(x: NativeTensor) -> Tensor:  # type: ignore\n    ...\n\n\ndef astensor(x: Union[NativeTensor, Tensor]) -> Tensor:  # type: ignore\n    if isinstance(x, Tensor):\n        return x\n    # we use the module name instead of isinstance\n    # to avoid importing all the frameworks\n    name = _get_module_name(x)\n    m = sys.modules\n    if name == ""torch"" and isinstance(x, m[name].Tensor):  # type: ignore\n        return PyTorchTensor(x)\n    if name == ""tensorflow"" and isinstance(x, m[name].Tensor):  # type: ignore\n        return TensorFlowTensor(x)\n    if name == ""jax"" and isinstance(x, m[name].numpy.ndarray):  # type: ignore\n        return JAXTensor(x)\n    if name == ""numpy"" and isinstance(x, m[name].ndarray):  # type: ignore\n        return NumPyTensor(x)\n    raise ValueError(f""Unknown type: {type(x)}"")\n\n\ndef astensors(*xs: Union[NativeTensor, Tensor]) -> Tuple[Tensor, ...]:  # type: ignore\n    return tuple(astensor(x) for x in xs)\n\n\nT = TypeVar(""T"")\n\n\nclass RestoreTypeFunc(Generic[T]):\n    def __init__(self, x: T):\n        self.unwrap = not isinstance(x, Tensor)\n\n    @overload\n    def __call__(self, x: Tensor) -> T:\n        ...\n\n    @overload  # noqa: F811\n    def __call__(self, x: Tensor, y: Tensor) -> Tuple[T, T]:\n        ...\n\n    @overload  # noqa: F811\n    def __call__(self, x: Tensor, y: Tensor, z: Tensor, *args: Tensor) -> Tuple[T, ...]:\n        ...\n\n    @overload  # noqa: F811\n    def __call__(self, *args: Any) -> Any:\n        # catch other types, otherwise we would return type T for input type Any\n        ...\n\n    def __call__(self, *args):  # type: ignore  # noqa: F811\n        result = tuple(x.raw for x in args) if self.unwrap else args\n        if len(result) == 1:\n            (result,) = result\n        return result\n\n\ndef astensor_(x: T) -> Tuple[Tensor, RestoreTypeFunc[T]]:\n    return astensor(x), RestoreTypeFunc[T](x)\n\n\ndef astensors_(x: T, *xs: T) -> Tuple[Tuple[Tensor, ...], RestoreTypeFunc[T]]:\n    return astensors(x, *xs), RestoreTypeFunc[T](x)\n'"
eagerpy/framework.py,0,"b'from typing import overload, Sequence, Callable, Tuple, Any, Optional, cast, Union\nfrom typing_extensions import Literal\n\nfrom .types import Axes, AxisAxes, Shape, ShapeOrScalar\n\nfrom .tensor import Tensor\nfrom .tensor import TensorType\nfrom .tensor import TensorOrScalar\n\nnewaxis = None\ninf = float(""inf"")\nnan = float(""nan"")\n\n\ndef clip(t: TensorType, min_: float, max_: float) -> TensorType:\n    return t.clip(min_, max_)\n\n\ndef abs(t: TensorType) -> TensorType:\n    return t.abs()\n\n\ndef sign(t: TensorType) -> TensorType:\n    return t.sign()\n\n\ndef sqrt(t: TensorType) -> TensorType:\n    return t.sqrt()\n\n\ndef square(t: TensorType) -> TensorType:\n    return t.square()\n\n\ndef pow(t: TensorType, exponent: TensorOrScalar) -> TensorType:\n    return t.pow(exponent)\n\n\ndef tanh(t: TensorType) -> TensorType:\n    return t.tanh()\n\n\ndef arctanh(t: TensorType) -> TensorType:\n    return t.arctanh()\n\n\ndef sum(\n    t: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n) -> TensorType:\n    return t.sum(axis=axis, keepdims=keepdims)\n\n\ndef mean(\n    t: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n) -> TensorType:\n    return t.mean(axis=axis, keepdims=keepdims)\n\n\ndef min(\n    t: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n) -> TensorType:\n    return t.min(axis=axis, keepdims=keepdims)\n\n\ndef max(\n    t: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n) -> TensorType:\n    return t.max(axis=axis, keepdims=keepdims)\n\n\n@overload\ndef minimum(x: TensorType, y: TensorOrScalar) -> TensorType:\n    ...\n\n\n@overload\ndef minimum(x: TensorOrScalar, y: TensorType) -> TensorType:\n    ...\n\n\ndef minimum(x: TensorOrScalar, y: TensorOrScalar) -> Tensor:\n    if not isinstance(x, Tensor):\n        return cast(Tensor, y).minimum(x)\n    return x.minimum(y)\n\n\n@overload\ndef maximum(x: TensorType, y: TensorOrScalar) -> TensorType:\n    ...\n\n\n@overload\ndef maximum(x: TensorOrScalar, y: TensorType) -> TensorType:\n    ...\n\n\ndef maximum(x: TensorOrScalar, y: TensorOrScalar) -> Tensor:\n    if not isinstance(x, Tensor):\n        return cast(Tensor, y).maximum(x)\n    return x.maximum(y)\n\n\ndef argmin(t: TensorType, axis: Optional[int] = None) -> TensorType:\n    return t.argmin(axis=axis)\n\n\ndef argmax(t: TensorType, axis: Optional[int] = None) -> TensorType:\n    return t.argmax(axis=axis)\n\n\ndef argsort(t: TensorType, axis: int = -1) -> TensorType:\n    return t.argsort(axis=axis)\n\n\ndef sort(t: TensorType, axis: int = -1) -> TensorType:\n    return t.sort(axis=axis)\n\n\ndef uniform(\n    t: TensorType, shape: ShapeOrScalar, low: float = 0.0, high: float = 1.0\n) -> TensorType:\n    return t.uniform(shape, low=low, high=high)\n\n\ndef normal(\n    t: TensorType, shape: ShapeOrScalar, mean: float = 0.0, stddev: float = 1.0\n) -> TensorType:\n    return t.normal(shape, mean=mean, stddev=stddev)\n\n\ndef ones(t: TensorType, shape: ShapeOrScalar) -> TensorType:\n    return t.ones(shape)\n\n\ndef zeros(t: TensorType, shape: ShapeOrScalar) -> TensorType:\n    return t.zeros(shape)\n\n\ndef ones_like(t: TensorType) -> TensorType:\n    return t.ones_like()\n\n\ndef zeros_like(t: TensorType) -> TensorType:\n    return t.zeros_like()\n\n\ndef full_like(t: TensorType, fill_value: float) -> TensorType:\n    return t.full_like(fill_value)\n\n\ndef onehot_like(t: TensorType, indices: TensorType, *, value: float = 1) -> TensorType:\n    return t.onehot_like(indices, value=value)\n\n\ndef from_numpy(t: TensorType, a: Any) -> TensorType:\n    return t.from_numpy(a)\n\n\ndef concatenate(tensors: Sequence[TensorType], axis: int = 0) -> TensorType:\n    t = tensors[0]\n    return t._concatenate(tensors, axis=axis)\n\n\ndef transpose(t: TensorType, axes: Optional[Axes] = None) -> TensorType:\n    return t.transpose(axes=axes)\n\n\n@overload\ndef logical_and(x: TensorType, y: TensorOrScalar) -> TensorType:\n    ...\n\n\n@overload\ndef logical_and(x: TensorOrScalar, y: TensorType) -> TensorType:\n    ...\n\n\ndef logical_and(x: TensorOrScalar, y: TensorOrScalar) -> Tensor:\n    if not isinstance(x, Tensor):\n        return cast(Tensor, y).logical_and(x)\n    return x.logical_and(y)\n\n\n@overload\ndef logical_or(x: TensorType, y: TensorOrScalar) -> TensorType:\n    ...\n\n\n@overload\ndef logical_or(x: TensorOrScalar, y: TensorType) -> TensorType:\n    ...\n\n\ndef logical_or(x: TensorOrScalar, y: TensorOrScalar) -> Tensor:\n    if not isinstance(x, Tensor):\n        return cast(Tensor, y).logical_or(x)\n    return x.logical_or(y)\n\n\ndef logical_not(t: TensorType) -> TensorType:\n    return t.logical_not()\n\n\ndef exp(t: TensorType) -> TensorType:\n    return t.exp()\n\n\ndef log(t: TensorType) -> TensorType:\n    return t.log()\n\n\ndef log2(t: TensorType) -> TensorType:\n    return t.log2()\n\n\ndef log10(t: TensorType) -> TensorType:\n    return t.log10()\n\n\ndef log1p(t: TensorType) -> TensorType:\n    return t.log1p()\n\n\ndef where(condition: TensorType, x: TensorOrScalar, y: TensorOrScalar) -> TensorType:\n    return condition.where(x, y)\n\n\ndef tile(t: TensorType, multiples: Axes) -> TensorType:\n    return t.tile(multiples)\n\n\ndef matmul(x: TensorType, y: TensorType) -> TensorType:\n    return x.matmul(y)\n\n\ndef softmax(t: TensorType, axis: int = -1) -> TensorType:\n    return t.softmax(axis=axis)\n\n\ndef log_softmax(t: TensorType, axis: int = -1) -> TensorType:\n    return t.log_softmax(axis=axis)\n\n\ndef stack(tensors: Sequence[TensorType], axis: int = 0) -> TensorType:\n    t = tensors[0]\n    return t._stack(tensors, axis=axis)\n\n\ndef squeeze(t: TensorType, axis: Optional[AxisAxes] = None) -> TensorType:\n    return t.squeeze(axis=axis)\n\n\ndef expand_dims(t: TensorType, axis: int) -> TensorType:\n    return t.expand_dims(axis=axis)\n\n\ndef full(t: TensorType, shape: ShapeOrScalar, value: float) -> TensorType:\n    return t.full(shape, value)\n\n\ndef index_update(t: TensorType, indices: Any, values: TensorOrScalar) -> TensorType:\n    return t.index_update(indices, values)\n\n\ndef arange(\n    t: TensorType, start: int, stop: Optional[int] = None, step: Optional[int] = None\n) -> TensorType:\n    return t.arange(start, stop, step)\n\n\ndef cumsum(t: TensorType, axis: Optional[int] = None) -> TensorType:\n    return t.cumsum(axis=axis)\n\n\ndef flip(t: TensorType, axis: Optional[AxisAxes] = None) -> TensorType:\n    return t.flip(axis=axis)\n\n\ndef meshgrid(\n    t: TensorType, *tensors: TensorType, indexing: str = ""xy""\n) -> Tuple[TensorType, ...]:\n    return t.meshgrid(*tensors, indexing=indexing)\n\n\ndef pad(\n    t: TensorType,\n    paddings: Tuple[Tuple[int, int], ...],\n    mode: str = ""constant"",\n    value: float = 0,\n) -> TensorType:\n    return t.pad(paddings, mode=mode, value=value)\n\n\ndef isnan(t: TensorType) -> TensorType:\n    return t.isnan()\n\n\ndef isinf(t: TensorType) -> TensorType:\n    return t.isinf()\n\n\ndef all(\n    t: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n) -> TensorType:\n    return t.all(axis=axis, keepdims=keepdims)\n\n\ndef any(\n    t: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n) -> TensorType:\n    return t.any(axis=axis, keepdims=keepdims)\n\n\ndef crossentropy(logits: TensorType, labels: TensorType) -> TensorType:\n    return logits.crossentropy(labels)\n\n\n@overload\ndef value_and_grad_fn(\n    t: TensorType, f: Callable[..., TensorType]\n) -> Callable[..., Tuple[TensorType, TensorType]]:\n    ...\n\n\n@overload\ndef value_and_grad_fn(\n    t: TensorType, f: Callable[..., TensorType], has_aux: Literal[False]\n) -> Callable[..., Tuple[TensorType, TensorType]]:\n    ...\n\n\n@overload\ndef value_and_grad_fn(\n    t: TensorType, f: Callable[..., Tuple[TensorType, Any]], has_aux: Literal[True]\n) -> Callable[..., Tuple[TensorType, Any, TensorType]]:\n    ...\n\n\ndef value_and_grad_fn(t: Any, f: Any, has_aux: bool = False) -> Any:\n    return t._value_and_grad_fn(f, has_aux=has_aux)\n\n\ndef value_and_grad(\n    f: Callable[..., TensorType], t: TensorType, *args: Any, **kwargs: Any\n) -> Tuple[TensorType, TensorType]:\n    return t.value_and_grad(f, *args, **kwargs)\n\n\ndef value_aux_and_grad(\n    f: Callable[..., Tuple[TensorType, Any]], t: TensorType, *args: Any, **kwargs: Any\n) -> Tuple[TensorType, Any, TensorType]:\n    return t.value_aux_and_grad(f, *args, **kwargs)\n\n\ndef reshape(t: TensorType, shape: Union[Shape, int]) -> TensorType:\n    return t.reshape(shape)\n\n\ndef take_along_axis(t: TensorType, indices: TensorType, axis: int) -> TensorType:\n    return t.take_along_axis(indices, axis)\n\n\ndef flatten(t: TensorType, start: int = 0, end: int = -1) -> TensorType:\n    return t.flatten(start=start, end=end)\n'"
eagerpy/lib.py,0,"b'from .tensor import TensorType\n\n\ndef kl_div_with_logits(\n    logits_p: TensorType, logits_q: TensorType, axis: int = -1, keepdims: bool = False\n) -> TensorType:\n    log_p = logits_p.log_softmax(axis=axis)\n    log_q = logits_q.log_softmax(axis=axis)\n    p = logits_p.softmax(axis=-1)\n    return (p * (log_p - log_q)).sum(axis=axis, keepdims=keepdims)\n'"
eagerpy/modules.py,0,"b'from importlib import import_module\nimport inspect\nfrom types import ModuleType\nfrom typing import Any, Callable, Iterable\nimport functools\n\nfrom .astensor import astensor\n\n\ndef wrap(f: Callable) -> Callable:\n    @functools.wraps(f)\n    def wrapper(*args: Any, **kwargs: Any) -> Any:\n        result = f(*args, **kwargs)\n        try:\n            result = astensor(result)\n        except ValueError:\n            pass\n        return result\n\n    return wrapper\n\n\nclass ModuleWrapper(ModuleType):\n    """"""A wrapper for modules that delays the import until it is needed\n    and wraps the output of functions as EagerPy tensors""""""\n\n    def __init__(self, *args: Any, **kwargs: Any):\n        super().__init__(*args, **kwargs)\n        if self.__doc__ is None:\n            self.__doc__ = f""EagerPy wrapper of the \'{self.__name__}\' module""\n\n    def __dir__(self) -> Iterable[str]:\n        # makes sure tab completion works\n        return import_module(self.__name__).__dir__()\n\n    def __getattr__(self, name: str) -> Any:\n        attr = getattr(import_module(self.__name__), name)\n        if callable(attr):\n            attr = wrap(attr)\n        elif inspect.ismodule(attr):\n            attr = ModuleWrapper(attr.__name__)\n        return attr\n\n\ntorch = ModuleWrapper(""torch"")\ntensorflow = ModuleWrapper(""tensorflow"")\njax = ModuleWrapper(""jax"")\nnumpy = ModuleWrapper(""numpy"")\n'"
eagerpy/norms.py,0,"b'from typing import Union, Optional\n\nfrom .tensor import TensorType\nfrom .types import AxisAxes\nfrom .framework import inf\n\n\ndef l0(\n    x: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n) -> TensorType:\n    return (x != 0).sum(axis=axis, keepdims=keepdims).astype(x.dtype)\n\n\ndef l1(\n    x: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n) -> TensorType:\n    return x.abs().sum(axis=axis, keepdims=keepdims)\n\n\ndef l2(\n    x: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n) -> TensorType:\n    return x.square().sum(axis=axis, keepdims=keepdims).sqrt()\n\n\ndef linf(\n    x: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n) -> TensorType:\n    return x.abs().max(axis=axis, keepdims=keepdims)\n\n\ndef lp(\n    x: TensorType,\n    p: Union[int, float],\n    axis: Optional[AxisAxes] = None,\n    keepdims: bool = False,\n) -> TensorType:\n    if p == 0:\n        return l0(x, axis=axis, keepdims=keepdims)\n    if p == 1:\n        return l1(x, axis=axis, keepdims=keepdims)\n    if p == 2:\n        return l2(x, axis=axis, keepdims=keepdims)\n    if p == inf:\n        return linf(x, axis=axis, keepdims=keepdims)\n    return x.abs().pow(p).sum(axis=axis, keepdims=keepdims).pow(1.0 / p)\n'"
eagerpy/types.py,0,"b'from typing import Union, Tuple, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    # for static analyzers\n    import torch  # noqa: F401\n    import tensorflow  # noqa: F401\n    import jax  # noqa: F401\n    import numpy  # noqa: F401\n\nAxes = Tuple[int, ...]\nAxisAxes = Union[int, Axes]\nShape = Tuple[int, ...]\nShapeOrScalar = Union[Shape, int]\n\n# tensorflow.Tensor, jax.numpy.ndarray and numpy.ndarray currently evaluate to Any\n# we can therefore only provide additional type information for torch.Tensor\nNativeTensor = Union[\n    ""torch.Tensor"", ""tensorflow.Tensor"", ""jax.numpy.ndarray"", ""numpy.ndarray""\n]\n'"
eagerpy/utils.py,0,"b'from typing import overload\nfrom typing_extensions import Literal\n\nfrom .tensor import Tensor\nfrom .tensor import PyTorchTensor\nfrom .tensor import TensorFlowTensor\nfrom .tensor import JAXTensor\nfrom .tensor import NumPyTensor\n\nfrom . import modules\n\n\n@overload\ndef get_dummy(framework: Literal[""pytorch""]) -> PyTorchTensor:\n    ...\n\n\n@overload\ndef get_dummy(framework: Literal[""tensorflow""]) -> TensorFlowTensor:\n    ...\n\n\n@overload\ndef get_dummy(framework: Literal[""jax""]) -> JAXTensor:\n    ...\n\n\n@overload\ndef get_dummy(framework: Literal[""numpy""]) -> NumPyTensor:\n    ...\n\n\n@overload\ndef get_dummy(framework: str) -> Tensor:\n    ...\n\n\ndef get_dummy(framework: str) -> Tensor:\n    x: Tensor\n    if framework == ""pytorch"":\n        x = modules.torch.zeros(0)\n        assert isinstance(x, PyTorchTensor)\n    elif framework == ""pytorch-gpu"":\n        x = modules.torch.zeros(0, device=""cuda:0"")  # pragma: no cover\n        assert isinstance(x, PyTorchTensor)  # pragma: no cover\n    elif framework == ""tensorflow"":\n        x = modules.tensorflow.zeros(0)\n        assert isinstance(x, TensorFlowTensor)\n    elif framework == ""jax"":\n        x = modules.jax.numpy.zeros(0)\n        assert isinstance(x, JAXTensor)\n    elif framework == ""numpy"":\n        x = modules.numpy.zeros(0)\n        assert isinstance(x, NumPyTensor)\n    else:\n        raise ValueError(f""unknown framework: {framework}"")  # pragma: no cover\n    return x.float32()\n'"
tests/conftest.py,0,"b'from typing import Any, Optional\nimport pytest\nimport eagerpy as ep\n\n\ndef pytest_addoption(parser: Any) -> None:\n    parser.addoption(""--backend"")\n\n\n@pytest.fixture(scope=""session"")\ndef dummy(request: Any) -> ep.Tensor:\n    backend: Optional[str] = request.config.option.backend\n    if backend is None:\n        pytest.skip()\n        assert False\n    return ep.utils.get_dummy(backend)\n\n\n@pytest.fixture(scope=""session"")\ndef t1(dummy: ep.Tensor) -> ep.Tensor:\n    return ep.arange(dummy, 5).float32()\n\n\n@pytest.fixture(scope=""session"")\ndef t1int(dummy: ep.Tensor) -> ep.Tensor:\n    return ep.arange(dummy, 5)\n\n\n@pytest.fixture(scope=""session"")\ndef t2(dummy: ep.Tensor) -> ep.Tensor:\n    return ep.arange(dummy, 7, 17, 2).float32()\n\n\n@pytest.fixture(scope=""session"")\ndef t2int(dummy: ep.Tensor) -> ep.Tensor:\n    return ep.arange(dummy, 7, 17, 2)\n\n\n@pytest.fixture(scope=""session"", params=[""t1"", ""t2""])\ndef t(request: Any, t1: ep.Tensor, t2: ep.Tensor) -> ep.Tensor:\n    return {""t1"": t1, ""t2"": t2}[request.param]\n'"
tests/test_lib.py,0,"b'import pytest\nimport eagerpy as ep\n\n\n@pytest.mark.parametrize(""axis"", [0, 1, -1])\ndef test_kl_div_with_logits(dummy: ep.Tensor, axis: int) -> None:\n    logits_p = logits_q = ep.arange(dummy, 12).float32().reshape((3, 4))\n    assert (ep.kl_div_with_logits(logits_p, logits_q, axis=axis) == 0).all()\n'"
tests/test_main.py,6,"b'from typing import Callable, Dict, Any, Tuple, Union, Optional, cast\nimport pytest\nimport functools\nimport numpy as np\nimport eagerpy as ep\nfrom eagerpy import Tensor\nfrom eagerpy.types import Shape, AxisAxes\n\n# make sure there are no undecorated tests in the ""special tests"" section below\n# -> /\\n\\ndef test_\n# make sure the undecorated tests in the ""normal tests"" section all contain\n# assertions and do not return something\n# -> /\\n    return\n\n\n###############################################################################\n# normal tests\n# - no decorator\n# - assertions\n###############################################################################\n\n\ndef test_astensor_raw(t: Tensor) -> None:\n    assert (ep.astensor(t.raw) == t).all()\n\n\ndef test_astensor_tensor(t: Tensor) -> None:\n    assert (ep.astensor(t) == t).all()\n\n\ndef test_astensor_restore_raw(t: Tensor) -> None:\n    r = t.raw\n    y, restore_type = ep.astensor_(r)\n    assert (y == t).all()\n    assert type(restore_type(y)) == type(r)\n    y = y + 1\n    assert type(restore_type(y)) == type(r)\n\n\ndef test_astensor_restore_tensor(t: Tensor) -> None:\n    r = t\n    y, restore_type = ep.astensor_(r)\n    assert (y == t).all()\n    assert type(restore_type(y)) == type(r)\n    y = y + 1\n    assert type(restore_type(y)) == type(r)\n\n\ndef test_astensors_raw(t: Tensor) -> None:\n    ts = (t, t + 1, t + 2)\n    rs = tuple(t.raw for t in ts)\n    ys = ep.astensors(*rs)\n    assert isinstance(ys, tuple)\n    assert len(ts) == len(ys)\n    for ti, yi in zip(ts, ys):\n        assert (ti == yi).all()\n\n\ndef test_astensors_tensor(t: Tensor) -> None:\n    ts = (t, t + 1, t + 2)\n    ys = ep.astensors(*ts)\n    assert isinstance(ys, tuple)\n    assert len(ts) == len(ys)\n    for ti, yi in zip(ts, ys):\n        assert (ti == yi).all()\n\n\ndef test_astensors_raw_restore(t: Tensor) -> None:\n    ts = (t, t + 1, t + 2)\n    rs = tuple(t.raw for t in ts)\n    ys, restore_type = ep.astensors_(*rs)\n    assert isinstance(ys, tuple)\n    assert len(ts) == len(ys)\n    for ti, yi in zip(ts, ys):\n        assert (ti == yi).all()\n\n    ys = tuple(y + 1 for y in ys)\n    xs = restore_type(*ys)\n    assert isinstance(xs, tuple)\n    assert len(xs) == len(ys)\n    for xi, ri in zip(xs, rs):\n        assert type(xi) == type(ri)\n\n    x0 = restore_type(ys[0])\n    assert not isinstance(x0, tuple)\n\n\ndef test_astensors_tensors_restore(t: Tensor) -> None:\n    ts = (t, t + 1, t + 2)\n    rs = ts\n    ys, restore_type = ep.astensors_(*rs)\n    assert isinstance(ys, tuple)\n    assert len(ts) == len(ys)\n    for ti, yi in zip(ts, ys):\n        assert (ti == yi).all()\n\n    ys = tuple(y + 1 for y in ys)\n    xs = restore_type(*ys)\n    assert isinstance(xs, tuple)\n    assert len(xs) == len(ys)\n    for xi, ri in zip(xs, rs):\n        assert type(xi) == type(ri)\n\n    x0 = restore_type(ys[0])\n    assert not isinstance(x0, tuple)  # type: ignore\n\n\ndef test_module() -> None:\n    assert ep.istensor(ep.numpy.tanh([3, 5]))\n    assert not ep.istensor(ep.numpy.tanh(3))\n\n\ndef test_module_dir() -> None:\n    assert ""zeros"" in dir(ep.numpy)\n\n\ndef test_repr(t: Tensor) -> None:\n    assert not repr(t).startswith(""<"")\n    t = ep.zeros(t, (10, 10))\n    assert not repr(t).startswith(""<"")\n    assert len(repr(t).split(""\\n"")) > 1\n\n\ndef test_logical_or_manual(t: Tensor) -> None:\n    assert (ep.logical_or(t < 3, ep.zeros_like(t).bool()) == (t < 3)).all()\n\n\ndef test_logical_not_manual(t: Tensor) -> None:\n    assert (ep.logical_not(t > 3) == (t <= 3)).all()\n\n\ndef test_softmax_manual(t: Tensor) -> None:\n    s = ep.softmax(t)\n    assert (s >= 0).all()\n    assert (s <= 1).all()\n    np.testing.assert_allclose(s.sum().numpy(), 1.0, rtol=1e-6)\n\n\ndef test_log_softmax_manual(t: Tensor) -> None:\n    np.testing.assert_allclose(\n        ep.log_softmax(t).exp().numpy(), ep.softmax(t).numpy(), rtol=1e-6\n    )\n\n\ndef test_value_and_grad_fn(dummy: Tensor) -> None:\n    if isinstance(dummy, ep.NumPyTensor):\n        pytest.skip()\n\n    def f(x: ep.Tensor) -> ep.Tensor:\n        return x.square().sum()\n\n    vgf = ep.value_and_grad_fn(dummy, f)\n    t = ep.arange(dummy, 8).float32().reshape((2, 4))\n    v, g = vgf(t)\n    assert v.item() == 140\n    assert (g == 2 * t).all()\n\n\ndef test_value_and_grad_fn_with_aux(dummy: Tensor) -> None:\n    if isinstance(dummy, ep.NumPyTensor):\n        pytest.skip()\n\n    def f(x: Tensor) -> Tuple[Tensor, Tensor]:\n        x = x.square()\n        return x.sum(), x\n\n    vgf = ep.value_and_grad_fn(dummy, f, has_aux=True)\n    t = ep.arange(dummy, 8).float32().reshape((2, 4))\n    v, aux, g = vgf(t)\n    assert v.item() == 140\n    assert (aux == t.square()).all()\n    assert (g == 2 * t).all()\n\n\ndef test_value_and_grad(dummy: Tensor) -> None:\n    if isinstance(dummy, ep.NumPyTensor):\n        pytest.skip()\n\n    def f(x: Tensor) -> Tensor:\n        return x.square().sum()\n\n    t = ep.arange(dummy, 8).float32().reshape((2, 4))\n    v, g = ep.value_and_grad(f, t)\n    assert v.item() == 140\n    assert (g == 2 * t).all()\n\n\ndef test_value_aux_and_grad(dummy: Tensor) -> None:\n    if isinstance(dummy, ep.NumPyTensor):\n        pytest.skip()\n\n    def f(x: Tensor) -> Tuple[Tensor, Tensor]:\n        x = x.square()\n        return x.sum(), x\n\n    t = ep.arange(dummy, 8).float32().reshape((2, 4))\n    v, aux, g = ep.value_aux_and_grad(f, t)\n    assert v.item() == 140\n    assert (aux == t.square()).all()\n    assert (g == 2 * t).all()\n\n\ndef test_value_aux_and_grad_multiple_aux(dummy: Tensor) -> None:\n    if isinstance(dummy, ep.NumPyTensor):\n        pytest.skip()\n\n    def f(x: Tensor) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:\n        x = x.square()\n        return x.sum(), (x, x + 1)\n\n    t = ep.arange(dummy, 8).float32().reshape((2, 4))\n    v, (aux0, aux1), g = ep.value_aux_and_grad(f, t)\n    assert v.item() == 140\n    assert (aux0 == t.square()).all()\n    assert (aux1 == t.square() + 1).all()\n    assert (g == 2 * t).all()\n\n\ndef test_value_and_grad_multiple_args(dummy: Tensor) -> None:\n    if isinstance(dummy, ep.NumPyTensor):\n        pytest.skip()\n\n    def f(x: Tensor, y: Tensor) -> Tensor:\n        return (x * y).sum()\n\n    t = ep.arange(dummy, 8).float32().reshape((2, 4))\n    v, g = ep.value_and_grad(f, t, t)\n    assert v.item() == 140\n    assert (g == t).all()\n\n\ndef test_logical_and_manual(t: Tensor) -> None:\n    assert (ep.logical_and(t < 3, ep.ones_like(t).bool()) == (t < 3)).all()\n\n\ndef test_transpose_1d(dummy: Tensor) -> None:\n    t = ep.arange(dummy, 8).float32()\n    assert (ep.transpose(t) == t).all()\n\n\ndef test_onehot_like_raises(dummy: Tensor) -> None:\n    t = ep.arange(dummy, 18).float32().reshape((6, 3))\n    indices = ep.arange(t, 6) // 2\n    ep.onehot_like(t, indices)\n\n    t = ep.arange(dummy, 90).float32().reshape((6, 3, 5))\n    indices = ep.arange(t, 6) // 2\n    with pytest.raises(ValueError):\n        ep.onehot_like(t, indices)\n\n    t = ep.arange(dummy, 18).float32().reshape((6, 3))\n    indices = ep.arange(t, 6).reshape((6, 1)) // 2\n    with pytest.raises(ValueError):\n        ep.onehot_like(t, indices)\n\n    t = ep.arange(dummy, 18).float32().reshape((6, 3))\n    indices = ep.arange(t, 5) // 2\n    with pytest.raises(ValueError):\n        ep.onehot_like(t, indices)\n\n\ndef test_tile_raises(t: Tensor) -> None:\n    ep.tile(t, (3,) * t.ndim)\n    with pytest.raises(ValueError):\n        ep.tile(t, (3,) * (t.ndim - 1))\n\n\ndef test_pad_raises(dummy: Tensor) -> None:\n    t = ep.arange(dummy, 120).reshape((2, 3, 4, 5)).float32()\n    ep.pad(t, ((0, 0), (0, 0), (2, 3), (1, 2)), mode=""constant"")\n    with pytest.raises(ValueError):\n        ep.pad(t, ((0, 0), (2, 3), (1, 2)), mode=""constant"")\n    with pytest.raises(ValueError):\n        ep.pad(\n            t, ((0, 0), (0, 0, 1, 2), (2, 3), (1, 2)), mode=""constant""  # type: ignore\n        )\n    with pytest.raises(ValueError):\n        ep.pad(t, ((0, 0), (0, 0), (2, 3), (1, 2)), mode=""foo"")\n\n\ndef test_mean_bool(t: Tensor) -> None:\n    with pytest.raises(ValueError):\n        ep.mean(t != 0)\n\n\ndef test_mean_int(t: Tensor) -> None:\n    with pytest.raises(ValueError):\n        ep.mean(ep.arange(t, 5))\n\n\n@pytest.mark.parametrize(""f"", [ep.logical_and, ep.logical_or])\ndef test_logical_and_nonboolean(\n    t: Tensor, f: Callable[[Tensor, Tensor], Tensor]\n) -> None:\n    t = t.float32()\n    f(t > 1, t > 1)\n    with pytest.raises(ValueError):\n        f(t, t > 1)\n    with pytest.raises(ValueError):\n        f(t > 1, t)\n    with pytest.raises(ValueError):\n        f(t, t)\n\n\ndef test_crossentropy_raises(dummy: Tensor) -> None:\n    t = ep.arange(dummy, 50).reshape((10, 5)).float32()\n    t = t / t.max()\n    ep.crossentropy(t, t.argmax(axis=-1))\n\n    t = ep.arange(dummy, 150).reshape((10, 5, 3)).float32()\n    t = t / t.max()\n    with pytest.raises(ValueError):\n        ep.crossentropy(t, t.argmax(axis=-1))\n\n    t = ep.arange(dummy, 50).reshape((10, 5)).float32()\n    t = t / t.max()\n    with pytest.raises(ValueError):\n        ep.crossentropy(t, t.argmax(axis=-1)[:8])\n\n\ndef test_matmul_raise(dummy: Tensor) -> None:\n    t = ep.arange(dummy, 8).float32().reshape((2, 4))\n    ep.matmul(t, t.T)\n    with pytest.raises(ValueError):\n        ep.matmul(t, t[0])\n    with pytest.raises(ValueError):\n        ep.matmul(t[0], t)\n    with pytest.raises(ValueError):\n        ep.matmul(t[0], t[0])\n\n\ndef test_take_along_axis_2d_first_raises(dummy: Tensor) -> None:\n    t = ep.arange(dummy, 32).float32().reshape((8, 4))\n    indices = ep.arange(t, t.shape[-1]) % t.shape[0]\n    with pytest.raises(NotImplementedError):\n        ep.take_along_axis(t, indices[ep.newaxis], axis=0)\n\n\ndef test_norms_class() -> None:\n    assert ep.Tensor.norms is not None\n\n\ndef test_numpy_readonly(t: Tensor) -> None:\n    a = t.numpy()\n    assert a.flags.writeable is False\n    with pytest.raises(ValueError, match=""read-only""):\n        a[:] += 1\n\n\ndef test_numpy_inplace(t: Tensor) -> None:\n    copy = t + 0\n    a = t.numpy().copy()\n    a[:] += 1\n    assert (t == copy).all()\n\n\ndef test_iter_list_stack(t: Tensor) -> None:\n    t2 = ep.stack(list(iter(t)))\n    assert t.shape == t2.shape\n    assert (t == t2).all()\n\n\ndef test_list_stack(t: Tensor) -> None:\n    t2 = ep.stack(list(t))\n    assert t.shape == t2.shape\n    assert (t == t2).all()\n\n\ndef test_iter_next(t: Tensor) -> None:\n    assert isinstance(next(iter(t)), Tensor)\n\n\ndef test_flatten(dummy: Tensor) -> None:\n    t = ep.ones(dummy, (16, 3, 32, 32))\n    assert ep.flatten(t).shape == (16 * 3 * 32 * 32,)\n    assert ep.flatten(t, start=1).shape == (16, 3 * 32 * 32)\n    assert ep.flatten(t, start=2).shape == (16, 3, 32 * 32)\n    assert ep.flatten(t, start=3).shape == (16, 3, 32, 32)\n    assert ep.flatten(t, end=-2).shape == (16 * 3 * 32, 32)\n    assert ep.flatten(t, end=-3).shape == (16 * 3, 32, 32)\n    assert ep.flatten(t, end=-4).shape == (16, 3, 32, 32)\n    assert ep.flatten(t, start=1, end=-2).shape == (16, 3 * 32, 32)\n\n\n@pytest.mark.parametrize(""axis"", [None, 0, 1, (0, 1)])\ndef test_squeeze_not_one(dummy: Tensor, axis: Optional[AxisAxes]) -> None:\n    t = ep.zeros(dummy, (3, 4, 5))\n    if axis is None:\n        t.squeeze(axis=axis)\n    else:\n        with pytest.raises(Exception):\n            # squeezing specifc axis should fail if they are not 1\n            t.squeeze(axis=axis)\n\n\n###############################################################################\n# special tests\n# - decorated with compare_*\n# - return values\n###############################################################################\n\n\ndef get_numpy_kwargs(kwargs: Any) -> Dict:\n    return {\n        k: ep.astensor(t.numpy()) if ep.istensor(t) else t for k, t in kwargs.items()\n    }\n\n\ndef compare_all(f: Callable[..., Tensor]) -> Callable[..., None]:\n    """"""A decorator to simplify writing test functions""""""\n\n    @functools.wraps(f)\n    def test_fn(*args: Any, **kwargs: Any) -> None:\n        assert len(args) == 0\n        nkwargs = get_numpy_kwargs(kwargs)\n        t = f(*args, **kwargs)\n        n = f(*args, **nkwargs)\n        t = t.numpy()\n        n = n.numpy()\n        assert t.shape == n.shape\n        assert (t == n).all()\n\n    return test_fn\n\n\ndef compare_allclose(*args: Any, rtol: float = 1e-07, atol: float = 0) -> Callable:\n    """"""A decorator to simplify writing test functions""""""\n\n    def compare_allclose_inner(f: Callable[..., Tensor]) -> Callable[..., None]:\n        @functools.wraps(f)\n        def test_fn(*args: Any, **kwargs: Any) -> None:\n            assert len(args) == 0\n            nkwargs = get_numpy_kwargs(kwargs)\n            t = f(*args, **kwargs)\n            n = f(*args, **nkwargs)\n            t = t.numpy()\n            n = n.numpy()\n            assert t.shape == n.shape\n            np.testing.assert_allclose(t, n, rtol=rtol, atol=atol)\n\n        return test_fn\n\n    if len(args) == 1 and callable(args[0]):\n        # decorator applied without parenthesis\n        return compare_allclose_inner(args[0])\n    return compare_allclose_inner\n\n\ndef compare_equal(\n    f: Callable[..., Union[Tensor, int, float, bool, Shape]]\n) -> Callable[..., None]:\n    """"""A decorator to simplify writing test functions""""""\n\n    @functools.wraps(f)\n    def test_fn(*args: Any, **kwargs: Any) -> None:\n        assert len(args) == 0\n        nkwargs = get_numpy_kwargs(kwargs)\n        t = f(*args, **kwargs)\n        n = f(*args, **nkwargs)\n        assert isinstance(t, type(n))\n        assert t == n\n\n    return test_fn\n\n\n@compare_equal\ndef test_format(dummy: Tensor) -> bool:\n    t = ep.arange(dummy, 5).sum()\n    return f""{t:.1f}"" == ""10.0""\n\n\n@compare_equal\ndef test_item(t: Tensor) -> float:\n    t = t.sum()\n    return t.item()\n\n\n@compare_equal\ndef test_len(t: Tensor) -> int:\n    return len(t)\n\n\n@compare_equal\ndef test_scalar_bool(t: Tensor) -> bool:\n    return bool(ep.sum(t) == 0)\n\n\n@compare_all\ndef test_neg(t: Tensor) -> Tensor:\n    return -t\n\n\n@compare_all\ndef test_square(t: Tensor) -> Tensor:\n    return ep.square(t)\n\n\n@compare_allclose\ndef test_pow(t: Tensor) -> Tensor:\n    return ep.pow(t, 3)\n\n\n@compare_allclose\ndef test_pow_float(t: Tensor) -> Tensor:\n    return ep.pow(t, 2.5)\n\n\n@compare_allclose\ndef test_pow_op(t: Tensor) -> Tensor:\n    return t ** 3\n\n\n@compare_allclose\ndef test_pow_tensor(t: Tensor) -> Tensor:\n    return ep.pow(t, (t + 0.5))\n\n\n@compare_allclose\ndef test_pow_op_tensor(t: Tensor) -> Tensor:\n    return t ** (t + 0.5)\n\n\n@compare_all\ndef test_add(t1: Tensor, t2: Tensor) -> Tensor:\n    return t1 + t2\n\n\n@compare_all\ndef test_add_scalar(t: Tensor) -> Tensor:\n    return t + 3\n\n\n@compare_all\ndef test_radd_scalar(t: Tensor) -> Tensor:\n    return 3 + t\n\n\n@compare_all\ndef test_sub(t1: Tensor, t2: Tensor) -> Tensor:\n    return t1 - t2\n\n\n@compare_all\ndef test_sub_scalar(t: Tensor) -> Tensor:\n    return t - 3\n\n\n@compare_all\ndef test_rsub_scalar(t: Tensor) -> Tensor:\n    return 3 - t\n\n\n@compare_all\ndef test_mul(t1: Tensor, t2: Tensor) -> Tensor:\n    return t1 * t2\n\n\n@compare_all\ndef test_mul_scalar(t: Tensor) -> Tensor:\n    return t * 3\n\n\n@compare_all\ndef test_rmul_scalar(t: Tensor) -> Tensor:\n    return 3 * t\n\n\n@compare_allclose\ndef test_truediv(t1: Tensor, t2: Tensor) -> Tensor:\n    return t1 / t2\n\n\n@compare_allclose(rtol=1e-6)\ndef test_truediv_scalar(t: Tensor) -> Tensor:\n    return t / 3\n\n\n@compare_allclose\ndef test_rtruediv_scalar(t: Tensor) -> Tensor:\n    return 3 / (abs(t) + 3e-8)\n\n\n@compare_allclose\ndef test_floordiv(t1: Tensor, t2: Tensor) -> Tensor:\n    return t1 // t2\n\n\n@compare_allclose(rtol=1e-6)\ndef test_floordiv_scalar(t: Tensor) -> Tensor:\n    return t // 3\n\n\n@compare_allclose\ndef test_rfloordiv_scalar(t: Tensor) -> Tensor:\n    return 3 // (abs(t) + 1e-8)\n\n\n@compare_all\ndef test_mod(t1: Tensor, t2: Tensor) -> Tensor:\n    return t1 % (abs(t2) + 1)\n\n\n@compare_all\ndef test_mod_scalar(t: Tensor) -> Tensor:\n    return t % 3\n\n\n@compare_all\ndef test_getitem(t: Tensor) -> Tensor:\n    return t[2]\n\n\n@compare_all\ndef test_getitem_tuple(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 8).float32().reshape((2, 4))\n    return t[1, 3]\n\n\n@compare_all\ndef test_getitem_newaxis(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 8).float32()\n    return t[ep.newaxis]\n\n\n@compare_all\ndef test_getitem_ellipsis_newaxis(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 8).float32()\n    return t[..., ep.newaxis]\n\n\n@compare_all\ndef test_getitem_tensor(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 32).float32()\n    indices = ep.arange(t, 3, 10, 2)\n    return t[indices]\n\n\n@compare_all\ndef test_getitem_range(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 32).float32()\n    indices = range(3, 10, 2)\n    return t[indices]\n\n\n@compare_all\ndef test_getitem_list(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 32).float32()\n    indices = list(range(3, 10, 2))\n    return t[indices]\n\n\n@compare_all\ndef test_getitem_ndarray(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 32).float32()\n    indices = np.arange(3, 10, 2)\n    return t[indices]\n\n\n@compare_all\ndef test_getitem_tuple_tensors(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 32).float32().reshape((8, 4))\n    rows = ep.arange(t, len(t))\n    indices = ep.arange(t, len(t)) % t.shape[1]\n    return t[rows, indices]\n\n\n@compare_all\ndef test_getitem_tuple_range_tensor(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 32).float32().reshape((8, 4))\n    rows = range(len(t))\n    indices = ep.arange(t, len(t)) % t.shape[1]\n    return t[rows, indices]\n\n\n@compare_all\ndef test_getitem_tuple_range_range(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 36).float32().reshape((6, 6))\n    rows = cols = range(len(t))\n    return t[rows, cols]\n\n\n@compare_all\ndef test_getitem_tuple_list_tensor(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 32).float32().reshape((8, 4))\n    rows = list(range(len(t)))\n    indices = ep.arange(t, len(t)) % t.shape[1]\n    return t[rows, indices]\n\n\n@compare_all\ndef test_getitem_slice(t: Tensor) -> Tensor:\n    return t[1:3]\n\n\n@compare_all\ndef test_getitem_slice_slice(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 32).float32().reshape((4, 8))\n    return t[:, :3]\n\n\n@compare_all\ndef test_getitem_boolean_tensor(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 32).float32().reshape((4, 8))\n    indices = ep.arange(t, 4) <= 2\n    return t[indices]\n\n\n@compare_all\ndef test_take_along_axis_2d(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 32).float32().reshape((8, 4))\n    indices = ep.arange(t, len(t)) % t.shape[-1]\n    return ep.take_along_axis(t, indices[..., ep.newaxis], axis=-1)\n\n\n@compare_all\ndef test_take_along_axis_3d(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 64).float32().reshape((2, 8, 4))\n    indices = ep.arange(t, 2 * 8).reshape((2, 8, 1)) % t.shape[-1]\n    return ep.take_along_axis(t, indices, axis=-1)\n\n\n@compare_all\ndef test_sqrt(t: Tensor) -> Tensor:\n    return ep.sqrt(t)\n\n\n@compare_equal\ndef test_shape(t: Tensor) -> Shape:\n    return t.shape\n\n\n@compare_all\ndef test_reshape(t: Tensor) -> Tensor:\n    shape = (1,) + t.shape + (1,)\n    return ep.reshape(t, shape)\n\n\n@compare_all\ndef test_reshape_minus_1(t: Tensor) -> Tensor:\n    return ep.reshape(t, -1)\n\n\n@compare_all\ndef test_reshape_int(t: Tensor) -> Tensor:\n    n = 1\n    for k in t.shape:\n        n *= k\n    return ep.reshape(t, n)\n\n\n@compare_all\ndef test_clip(t: Tensor) -> Tensor:\n    return ep.clip(t, 2, 3.5)\n\n\n@compare_all\ndef test_sign(t: Tensor) -> Tensor:\n    return ep.sign(t)\n\n\n@compare_all\ndef test_sum(t: Tensor) -> Tensor:\n    return ep.sum(t)\n\n\n@compare_all\ndef test_sum_axis(t: Tensor) -> Tensor:\n    return ep.sum(t, axis=0)\n\n\n@compare_all\ndef test_sum_axes(dummy: Tensor) -> Tensor:\n    t = ep.ones(dummy, 30).float32().reshape((3, 5, 2))\n    return ep.sum(t, axis=(0, 1))\n\n\n@compare_all\ndef test_sum_keepdims(t: Tensor) -> Tensor:\n    return ep.sum(t, axis=0, keepdims=True)\n\n\n@compare_all\ndef test_sum_none_keepdims(t: Tensor) -> Tensor:\n    return ep.sum(t, axis=None, keepdims=True)\n\n\n@compare_all\ndef test_sum_bool(t: Tensor) -> Tensor:\n    return ep.sum(t != 0)\n\n\n@compare_all\ndef test_sum_int(t: Tensor) -> Tensor:\n    return ep.sum(ep.arange(t, 5))\n\n\n@compare_all\ndef test_mean(t: Tensor) -> Tensor:\n    return ep.mean(t)\n\n\n@compare_all\ndef test_mean_axis(t: Tensor) -> Tensor:\n    return ep.mean(t, axis=0)\n\n\n@compare_all\ndef test_mean_axes(dummy: Tensor) -> Tensor:\n    t = ep.ones(dummy, 30).float32().reshape((3, 5, 2))\n    return ep.mean(t, axis=(0, 1))\n\n\n@compare_all\ndef test_mean_keepdims(t: Tensor) -> Tensor:\n    return ep.mean(t, axis=0, keepdims=True)\n\n\n@compare_all\ndef test_mean_none_keepdims(t: Tensor) -> Tensor:\n    return ep.mean(t, axis=None, keepdims=True)\n\n\n@compare_all\ndef test_all(t: Tensor) -> Tensor:\n    return ep.all(t > 3)\n\n\n@compare_all\ndef test_all_axis(t: Tensor) -> Tensor:\n    return ep.all(t > 3, axis=0)\n\n\n@compare_all\ndef test_all_axes(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 30).float32().reshape((3, 5, 2))\n    return ep.all(t > 3, axis=(0, 1))\n\n\n@compare_all\ndef test_all_keepdims(t: Tensor) -> Tensor:\n    return ep.all(t > 3, axis=0, keepdims=True)\n\n\n@compare_all\ndef test_all_none_keepdims(t: Tensor) -> Tensor:\n    return ep.all(t > 3, axis=None, keepdims=True)\n\n\n@compare_all\ndef test_any(t: Tensor) -> Tensor:\n    return ep.any(t > 3)\n\n\n@compare_all\ndef test_any_axis(t: Tensor) -> Tensor:\n    return ep.any(t > 3, axis=0)\n\n\n@compare_all\ndef test_any_axes(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 30).float32().reshape((3, 5, 2))\n    return ep.any(t > 3, axis=(0, 1))\n\n\n@compare_all\ndef test_any_keepdims(t: Tensor) -> Tensor:\n    return ep.any(t > 3, axis=0, keepdims=True)\n\n\n@compare_all\ndef test_any_none_keepdims(t: Tensor) -> Tensor:\n    return ep.any(t > 3, axis=None, keepdims=True)\n\n\n@compare_all\ndef test_min(t: Tensor) -> Tensor:\n    return ep.min(t)\n\n\n@compare_all\ndef test_min_axis(t: Tensor) -> Tensor:\n    return ep.min(t, axis=0)\n\n\n@compare_all\ndef test_min_axes(dummy: Tensor) -> Tensor:\n    t = ep.ones(dummy, 30).float32().reshape((3, 5, 2))\n    return ep.min(t, axis=(0, 1))\n\n\n@compare_all\ndef test_min_keepdims(t: Tensor) -> Tensor:\n    return ep.min(t, axis=0, keepdims=True)\n\n\n@compare_all\ndef test_min_none_keepdims(t: Tensor) -> Tensor:\n    return ep.min(t, axis=None, keepdims=True)\n\n\n@compare_all\ndef test_max(t: Tensor) -> Tensor:\n    return ep.max(t)\n\n\n@compare_all\ndef test_max_axis(t: Tensor) -> Tensor:\n    return ep.max(t, axis=0)\n\n\n@compare_all\ndef test_max_axes(dummy: Tensor) -> Tensor:\n    t = ep.ones(dummy, 30).float32().reshape((3, 5, 2))\n    return ep.max(t, axis=(0, 1))\n\n\n@compare_all\ndef test_max_keepdims(t: Tensor) -> Tensor:\n    return ep.max(t, axis=0, keepdims=True)\n\n\n@compare_all\ndef test_max_none_keepdims(t: Tensor) -> Tensor:\n    return ep.max(t, axis=None, keepdims=True)\n\n\n@compare_allclose\ndef test_exp(t: Tensor) -> Tensor:\n    return ep.exp(t)\n\n\n@compare_allclose\ndef test_log(t: Tensor) -> Tensor:\n    return ep.log(t.maximum(1e-8))\n\n\n@compare_allclose\ndef test_log2(t: Tensor) -> Tensor:\n    return ep.log2(t.maximum(1e-8))\n\n\n@compare_allclose\ndef test_log10(t: Tensor) -> Tensor:\n    return ep.log10(t.maximum(1e-8))\n\n\n@compare_allclose\ndef test_log1p(t: Tensor) -> Tensor:\n    return ep.log1p(t)\n\n\n@compare_allclose(rtol=1e-6)\ndef test_tanh(t: Tensor) -> Tensor:\n    return ep.tanh(t)\n\n\n@compare_allclose(rtol=1e-6)\ndef test_arctanh(t: Tensor) -> Tensor:\n    return ep.arctanh((t - t.mean()) / t.max())\n\n\n@compare_all\ndef test_abs_op(t: Tensor) -> Tensor:\n    return abs(t)\n\n\n@compare_all\ndef test_abs(t: Tensor) -> Tensor:\n    return ep.abs(t)\n\n\n@compare_all\ndef test_minimum(t1: Tensor, t2: Tensor) -> Tensor:\n    return ep.minimum(t1, t2)\n\n\n@compare_all\ndef test_minimum_scalar(t: Tensor) -> Tensor:\n    return ep.minimum(t, 3)\n\n\n@compare_all\ndef test_rminimum_scalar(t: Tensor) -> Tensor:\n    return ep.minimum(3, t)\n\n\n@compare_all\ndef test_maximum(t1: Tensor, t2: Tensor) -> Tensor:\n    return ep.maximum(t1, t2)\n\n\n@compare_all\ndef test_maximum_scalar(t: Tensor) -> Tensor:\n    return ep.maximum(t, 3)\n\n\n@compare_all\ndef test_rmaximum_scalar(t: Tensor) -> Tensor:\n    return ep.maximum(3, t)\n\n\n@compare_all\ndef test_argmin(t: Tensor) -> Tensor:\n    return ep.argmin(t)\n\n\n@compare_all\ndef test_argmin_axis(t: Tensor) -> Tensor:\n    return ep.argmin(t, axis=0)\n\n\n@compare_all\ndef test_argmax(t: Tensor) -> Tensor:\n    return ep.argmax(t)\n\n\n@compare_all\ndef test_argmax_axis(t: Tensor) -> Tensor:\n    return ep.argmax(t, axis=0)\n\n\n@compare_all\ndef test_logical_and(t: Tensor) -> Tensor:\n    return ep.logical_and(t < 3, t > 1)\n\n\n@compare_all\ndef test_logical_and_scalar(t: Tensor) -> Tensor:\n    return ep.logical_and(True, t < 3)\n\n\n@compare_all\ndef test_logical_or(t: Tensor) -> Tensor:\n    return ep.logical_or(t > 3, t < 1)\n\n\n@compare_all\ndef test_logical_or_scalar(t: Tensor) -> Tensor:\n    return ep.logical_or(True, t < 1)\n\n\n@compare_all\ndef test_logical_not(t: Tensor) -> Tensor:\n    return ep.logical_not(t > 3)\n\n\n@compare_all\ndef test_isnan_false(t: Tensor) -> Tensor:\n    return ep.isnan(t)\n\n\n@compare_all\ndef test_isnan_true(t: Tensor) -> Tensor:\n    return ep.isnan(t + ep.nan)\n\n\n@compare_all\ndef test_isinf(t: Tensor) -> Tensor:\n    return ep.isinf(t)\n\n\n@compare_all\ndef test_isinf_posinf(t: Tensor) -> Tensor:\n    return ep.isinf(t + ep.inf)\n\n\n@compare_all\ndef test_isinf_neginf(t: Tensor) -> Tensor:\n    return ep.isinf(t - ep.inf)\n\n\n@compare_all\ndef test_zeros_like(t: Tensor) -> Tensor:\n    return ep.zeros_like(t)\n\n\n@compare_all\ndef test_ones_like(t: Tensor) -> Tensor:\n    return ep.ones_like(t)\n\n\n@compare_all\ndef test_full_like(t: Tensor) -> Tensor:\n    return ep.full_like(t, 5)\n\n\n@pytest.mark.parametrize(""value"", [1, -1, 2])\n@compare_all\ndef test_onehot_like(dummy: Tensor, value: float) -> Tensor:\n    t = ep.arange(dummy, 18).float32().reshape((6, 3))\n    indices = ep.arange(t, 6) // 2\n    return ep.onehot_like(t, indices, value=value)\n\n\n@compare_all\ndef test_zeros_scalar(t: Tensor) -> Tensor:\n    return ep.zeros(t, 5)\n\n\n@compare_all\ndef test_zeros_tuple(t: Tensor) -> Tensor:\n    return ep.zeros(t, (2, 3))\n\n\n@compare_all\ndef test_ones_scalar(t: Tensor) -> Tensor:\n    return ep.ones(t, 5)\n\n\n@compare_all\ndef test_ones_tuple(t: Tensor) -> Tensor:\n    return ep.ones(t, (2, 3))\n\n\n@compare_all\ndef test_full_scalar(t: Tensor) -> Tensor:\n    return ep.full(t, 5, 4.0)\n\n\n@compare_all\ndef test_full_tuple(t: Tensor) -> Tensor:\n    return ep.full(t, (2, 3), 4.0)\n\n\n@compare_equal\ndef test_uniform_scalar(t: Tensor) -> Shape:\n    return ep.uniform(t, 5).shape\n\n\n@compare_equal\ndef test_uniform_tuple(t: Tensor) -> Shape:\n    return ep.uniform(t, (2, 3)).shape\n\n\n@compare_equal\ndef test_normal_scalar(t: Tensor) -> Shape:\n    return ep.normal(t, 5).shape\n\n\n@compare_equal\ndef test_normal_tuple(t: Tensor) -> Shape:\n    return ep.normal(t, (2, 3)).shape\n\n\n@compare_all\ndef test_argsort(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 6).float32().reshape((2, 3))\n    return ep.argsort(t)\n\n\n@compare_all\ndef test_sort(dummy: Tensor) -> Tensor:\n    t = -ep.arange(dummy, 6).float32().reshape((2, 3))\n    return ep.sort(t)\n\n\n@compare_all\ndef test_transpose(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 8).float32().reshape((2, 4))\n    return ep.transpose(t)\n\n\n@compare_all\ndef test_transpose_axes(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 60).float32().reshape((3, 4, 5))\n    return ep.transpose(t, axes=(1, 2, 0))\n\n\n@compare_all\ndef test_where(t: Tensor) -> Tensor:\n    return ep.where(t >= 3, t, -t)\n\n\n@compare_all\ndef test_where_first_scalar(t: Tensor) -> Tensor:\n    return ep.where(t >= 3, 2, -t)\n\n\n@compare_all\ndef test_where_second_scalar(t: Tensor) -> Tensor:\n    return ep.where(t >= 3, t, 2)\n\n\n@compare_all\ndef test_where_both_scalars(t: Tensor) -> Tensor:\n    return ep.where(t >= 3, 2, 5)\n\n\n@compare_all\ndef test_tile(t: Tensor) -> Tensor:\n    return ep.tile(t, (3,) * t.ndim)\n\n\n@compare_all\ndef test_matmul(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 8).float32().reshape((2, 4))\n    return ep.matmul(t, t.T)\n\n\n@compare_allclose(rtol=1e-6)\ndef test_softmax(t: Tensor) -> Tensor:\n    return ep.softmax(t)\n\n\n@compare_allclose(rtol=1e-5)\ndef test_log_softmax(t: Tensor) -> Tensor:\n    return ep.log_softmax(t)\n\n\n@compare_allclose\ndef test_crossentropy(dummy: Tensor) -> Tensor:\n    t = ep.arange(dummy, 50).reshape((10, 5)).float32()\n    t = t / t.max()\n    return ep.crossentropy(t, t.argmax(axis=-1))\n\n\n@pytest.mark.parametrize(""axis"", [0, 1, -1])\n@compare_all\ndef test_stack(t1: Tensor, t2: Tensor, axis: int) -> Tensor:\n    return ep.stack([t1, t2], axis=axis)\n\n\n@compare_all\ndef test_concatenate_axis0(dummy: Tensor) -> Tensor:\n    t1 = ep.arange(dummy, 12).float32().reshape((4, 3))\n    t2 = ep.arange(dummy, 20, 32, 2).float32().reshape((2, 3))\n    return ep.concatenate([t1, t2], axis=0)\n\n\n@compare_all\ndef test_concatenate_axis1(dummy: Tensor) -> Tensor:\n    t1 = ep.arange(dummy, 12).float32().reshape((3, 4))\n    t2 = ep.arange(dummy, 20, 32, 2).float32().reshape((3, 2))\n    return ep.concatenate([t1, t2], axis=1)\n\n\n@pytest.mark.parametrize(""axis"", [0, 1, -1])\n@compare_all\ndef test_expand_dims(t: Tensor, axis: int) -> Tensor:\n    return ep.expand_dims(t, axis)\n\n\n@pytest.mark.parametrize(""axis"", [None, 0, 1, (0, 1)])\n@compare_all\ndef test_squeeze(t: Tensor, axis: Optional[AxisAxes]) -> Tensor:\n    t = t.expand_dims(axis=0).expand_dims(axis=1)\n    return ep.squeeze(t, axis=axis)\n\n\n@compare_all\ndef test_arange(dummy: Tensor) -> Tensor:\n    return ep.arange(dummy, 6)\n\n\n@compare_all\ndef test_arange_start(dummy: Tensor) -> Tensor:\n    return ep.arange(dummy, 5, 10)\n\n\n@compare_all\ndef test_arange_step(dummy: Tensor) -> Tensor:\n    return ep.arange(dummy, 4, 8, 2)\n\n\n@compare_all\ndef test_cumsum(t: Tensor) -> Tensor:\n    return ep.cumsum(t)\n\n\n@compare_all\ndef test_cumsum_axis(t: Tensor) -> Tensor:\n    return ep.cumsum(t, axis=0)\n\n\n@compare_all\ndef test_flip(t: Tensor) -> Tensor:\n    return ep.flip(t)\n\n\n@compare_all\ndef test_flip_axis(t: Tensor) -> Tensor:\n    return ep.flip(t, axis=0)\n\n\n@pytest.mark.parametrize(""indexing"", [""ij"", ""xy""])\n@pytest.mark.parametrize(""i"", [0, 1])\n@compare_all\ndef test_meshgrid_a(dummy: Tensor, indexing: str, i: int) -> Tensor:\n    t1 = ep.arange(dummy, 5)\n    t2 = ep.arange(dummy, 3)\n    results = ep.meshgrid(t1, t2, indexing=indexing)\n    assert len(results) == 2\n    return results[i]\n\n\n@pytest.mark.parametrize(\n    ""mode,value"", [(""constant"", 0), (""constant"", -2), (""reflect"", 0)]\n)\n@compare_all\ndef test_pad(dummy: Tensor, mode: str, value: float) -> Tensor:\n    t = ep.arange(dummy, 120).reshape((2, 3, 4, 5)).float32()\n    return ep.pad(t, ((0, 0), (0, 0), (2, 3), (1, 2)), mode=mode, value=value)\n\n\n@compare_all\ndef test_index_update_row(dummy: Tensor) -> Tensor:\n    x = ep.ones(dummy, (3, 4))\n    return ep.index_update(x, ep.index[1], ep.ones(x, 4) * 66.0)\n\n\n@compare_all\ndef test_index_update_row_scalar(dummy: Tensor) -> Tensor:\n    x = ep.ones(dummy, (3, 4))\n    return ep.index_update(x, ep.index[1], 66.0)\n\n\n@compare_all\ndef test_index_update_column(dummy: Tensor) -> Tensor:\n    x = ep.ones(dummy, (3, 4))\n    return ep.index_update(x, ep.index[:, 1], ep.ones(x, 3) * 66.0)\n\n\n@compare_all\ndef test_index_update_column_scalar(dummy: Tensor) -> Tensor:\n    x = ep.ones(dummy, (3, 4))\n    return ep.index_update(x, ep.index[:, 1], 66.0)\n\n\n@compare_all\ndef test_index_update_indices(dummy: Tensor) -> Tensor:\n    x = ep.ones(dummy, (3, 4))\n    ind = ep.from_numpy(dummy, np.array([0, 1, 2, 1]))\n    return ep.index_update(x, ep.index[ind, ep.arange(x, 4)], ep.ones(x, 4) * 33.0)\n\n\n@compare_all\ndef test_index_update_indices_scalar(dummy: Tensor) -> Tensor:\n    x = ep.ones(dummy, (3, 4))\n    ind = ep.from_numpy(dummy, np.array([0, 1, 2, 1]))\n    return ep.index_update(x, ep.index[ind, ep.arange(x, 4)], 33.0)\n\n\n@compare_all\ndef test_lt(t1: Tensor, t2: Tensor) -> Tensor:\n    return t1 < t2\n\n\n@compare_all\ndef test_lt_scalar(t1: Tensor, t2: Tensor) -> Tensor:\n    return 3 < t2\n\n\n@compare_all\ndef test_le(t1: Tensor, t2: Tensor) -> Tensor:\n    return t1 <= t2\n\n\n@compare_all\ndef test_le_scalar(t1: Tensor, t2: Tensor) -> Tensor:\n    return 3 <= t2\n\n\n@compare_all\ndef test_gt(t1: Tensor, t2: Tensor) -> Tensor:\n    return t1 > t2\n\n\n@compare_all\ndef test_gt_scalar(t1: Tensor, t2: Tensor) -> Tensor:\n    return 3 > t2\n\n\n@compare_all\ndef test_ge(t1: Tensor, t2: Tensor) -> Tensor:\n    return t1 >= t2\n\n\n@compare_all\ndef test_ge_scalar(t1: Tensor, t2: Tensor) -> Tensor:\n    return 3 >= t2\n\n\n@compare_all\ndef test_eq(t1: Tensor, t2: Tensor) -> Tensor:\n    return t1 == t2\n\n\n@compare_all\ndef test_eq_scalar(t1: Tensor, t2: Tensor) -> Tensor:\n    return cast(Tensor, 3 == t2)\n\n\n@compare_all\ndef test_ne(t1: Tensor, t2: Tensor) -> Tensor:\n    return t1 != t2\n\n\n@compare_all\ndef test_ne_scalar(t1: Tensor, t2: Tensor) -> Tensor:\n    return cast(Tensor, 3 != t2)\n\n\n@compare_all\ndef test_float_int_lt(t1: Tensor, t2int: Tensor) -> Tensor:\n    return t1 < t2int\n\n\n@compare_all\ndef test_float_int_le(t1: Tensor, t2int: Tensor) -> Tensor:\n    return t1 <= t2int\n\n\n@compare_all\ndef test_float_int_gt(t1: Tensor, t2int: Tensor) -> Tensor:\n    return t1 > t2int\n\n\n@compare_all\ndef test_float_int_ge(t1: Tensor, t2int: Tensor) -> Tensor:\n    return t1 >= t2int\n\n\n@compare_all\ndef test_float_int_eq(t1: Tensor, t2int: Tensor) -> Tensor:\n    return t1 == t2int\n\n\n@compare_all\ndef test_float_int_ne(t1: Tensor, t2int: Tensor) -> Tensor:\n    return t1 != t2int\n\n\n@compare_all\ndef test_int_float_lt(t1int: Tensor, t2: Tensor) -> Tensor:\n    return t1int < t2\n\n\n@compare_all\ndef test_int_float_le(t1int: Tensor, t2: Tensor) -> Tensor:\n    return t1int <= t2\n\n\n@compare_all\ndef test_int_float_gt(t1int: Tensor, t2: Tensor) -> Tensor:\n    return t1int > t2\n\n\n@compare_all\ndef test_int_float_ge(t1int: Tensor, t2: Tensor) -> Tensor:\n    return t1int >= t2\n\n\n@compare_all\ndef test_int_float_eq(t1int: Tensor, t2: Tensor) -> Tensor:\n    return t1int == t2\n\n\n@compare_all\ndef test_int_float_ne(t1int: Tensor, t2: Tensor) -> Tensor:\n    return t1int != t2\n\n\n@compare_all\ndef test_norms_l0(t: Tensor) -> Tensor:\n    return t.norms.l0()\n\n\n@compare_all\ndef test_norms_l1(t: Tensor) -> Tensor:\n    return t.norms.l1()\n\n\n@compare_all\ndef test_norms_l2(t: Tensor) -> Tensor:\n    return t.norms.l2()\n\n\n@compare_all\ndef test_norms_linf(t: Tensor) -> Tensor:\n    return t.norms.linf()\n\n\n@compare_all\ndef test_norms_lp(t: Tensor) -> Tensor:\n    return t.norms.lp(2)\n\n\n@compare_all\ndef test_norms_cache(t: Tensor) -> Tensor:\n    return t.norms.l1() + t.norms.l2()\n'"
tests/test_norms.py,1,"b'from typing import Optional\nimport pytest\nfrom numpy.testing import assert_allclose\nfrom numpy.linalg import norm\nimport numpy as np\nimport eagerpy as ep\nfrom eagerpy import Tensor\nfrom eagerpy.norms import l0, l1, l2, linf, lp\n\nnorms = {0: l0, 1: l1, 2: l2, ep.inf: linf}\n\n\n@pytest.fixture\ndef x1d(dummy: Tensor) -> Tensor:\n    return ep.arange(dummy, 10).float32() / 7.0\n\n\n@pytest.fixture\ndef x2d(dummy: Tensor) -> Tensor:\n    return ep.arange(dummy, 12).float32().reshape((3, 4)) / 7.0\n\n\n@pytest.fixture\ndef x4d(dummy: Tensor) -> Tensor:\n    return ep.arange(dummy, 2 * 3 * 4 * 5).float32().reshape((2, 3, 4, 5)) / 7.0\n\n\n@pytest.mark.parametrize(""p"", [0, 1, 2, ep.inf])\ndef test_1d(x1d: Tensor, p: float) -> None:\n    assert_allclose(lp(x1d, p).numpy(), norm(x1d.numpy(), ord=p))\n    assert_allclose(norms[p](x1d).numpy(), norm(x1d.numpy(), ord=p))\n\n\n@pytest.mark.parametrize(""p"", [0, 1, 2, 3, 4, ep.inf])\n@pytest.mark.parametrize(""axis"", [0, 1, -1])\n@pytest.mark.parametrize(""keepdims"", [False, True])\ndef test_2d(x2d: Tensor, p: float, axis: int, keepdims: bool) -> None:\n    assert isinstance(axis, int)  # see test4d for the more general test\n    assert_allclose(\n        lp(x2d, p, axis=axis, keepdims=keepdims).numpy(),\n        norm(x2d.numpy(), ord=p, axis=axis, keepdims=keepdims),\n        rtol=1e-6,\n    )\n    if p not in norms:\n        return\n    assert_allclose(\n        norms[p](x2d, axis=axis, keepdims=keepdims).numpy(),\n        norm(x2d.numpy(), ord=p, axis=axis, keepdims=keepdims),\n        rtol=1e-6,\n    )\n\n\n@pytest.mark.parametrize(""p"", [0, 1, 2, 3, 4, ep.inf])\n@pytest.mark.parametrize(\n    ""axis"",\n    [\n        None,\n        0,\n        1,\n        2,\n        3,\n        -1,\n        -2,\n        -3,\n        -4,\n        (0, 1),\n        (1, 2),\n        (1, 3),\n        (1, 2, 3),\n        (0, 1, 3),\n        (2, 1, 0),\n    ],\n)\n@pytest.mark.parametrize(""keepdims"", [False, True])\ndef test_4d(\n    x4d: Tensor, p: float, axis: Optional[ep.types.AxisAxes], keepdims: bool\n) -> None:\n    actual = lp(x4d, p, axis=axis, keepdims=keepdims).numpy()\n\n    # numpy does not support arbitrary axes (limited to vector and matrix norms)\n    if axis is None:\n        axes = tuple(range(x4d.ndim))\n    elif not isinstance(axis, tuple):\n        axes = (axis,)\n    else:\n        axes = axis\n    del axis\n    axes = tuple(i % x4d.ndim for i in axes)\n    x = x4d.numpy()\n    other = tuple(i for i in range(x.ndim) if i not in axes)\n    x = np.transpose(x, other + axes)\n    x = x.reshape(x.shape[: len(other)] + (-1,))\n    desired = norm(x, ord=p, axis=-1)\n    if keepdims:\n        shape = tuple(1 if i in axes else x4d.shape[i] for i in range(x4d.ndim))\n        desired = desired.reshape(shape)\n\n    assert_allclose(actual, desired, rtol=1e-6)\n'"
eagerpy/tensor/__init__.py,0,b'from .tensor import Tensor  # noqa: F401\nfrom .tensor import TensorType  # noqa: F401\nfrom .tensor import TensorOrScalar  # noqa: F401\nfrom .tensor import istensor  # noqa: F401\n\nfrom .pytorch import PyTorchTensor  # noqa: F401\nfrom .tensorflow import TensorFlowTensor  # noqa: F401\nfrom .numpy import NumPyTensor  # noqa: F401\nfrom .jax import JAXTensor  # noqa: F401\n'
eagerpy/tensor/base.py,0,"b'from typing_extensions import final\nfrom typing import Any, cast\n\nfrom .tensor import Tensor\nfrom .tensor import TensorType\nfrom .tensor import TensorOrScalar\n\n\ndef unwrap_(*args: Any) -> Any:\n    return tuple(t.raw if isinstance(t, Tensor) else t for t in args)\n\n\ndef unwrap1(t: Any) -> Any:\n    return t.raw if isinstance(t, Tensor) else t\n\n\nclass BaseTensor(Tensor):\n    __slots__ = ""_raw""\n\n    def __init__(self: TensorType, raw: Any):\n        assert not isinstance(raw, Tensor)\n        self._raw = raw\n\n    @property\n    def raw(self) -> Any:\n        return self._raw\n\n    @final\n    def __repr__(self: TensorType) -> str:\n        lines = repr(self.raw).split(""\\n"")\n        prefix = self.__class__.__name__ + ""(""\n        lines[0] = prefix + lines[0]\n        prefix = "" "" * len(prefix)\n        for i in range(1, len(lines)):\n            lines[i] = prefix + lines[i]\n        lines[-1] = lines[-1] + "")""\n        return ""\\n"".join(lines)\n\n    @final\n    def __format__(self: TensorType, format_spec: str) -> str:\n        return format(self.raw, format_spec)\n\n    @final\n    @property\n    def dtype(self: TensorType) -> Any:\n        return self.raw.dtype\n\n    @final\n    def __bool__(self: TensorType) -> bool:\n        return bool(self.raw)\n\n    @final\n    def __len__(self: TensorType) -> int:\n        return len(self.raw)\n\n    @final\n    def __abs__(self: TensorType) -> TensorType:\n        return type(self)(abs(self.raw))\n\n    @final\n    def __neg__(self: TensorType) -> TensorType:\n        return type(self)(-self.raw)\n\n    @final\n    def __add__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__add__(unwrap1(other)))\n\n    @final\n    def __radd__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__radd__(unwrap1(other)))\n\n    @final\n    def __sub__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__sub__(unwrap1(other)))\n\n    @final\n    def __rsub__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__rsub__(unwrap1(other)))\n\n    @final\n    def __mul__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__mul__(unwrap1(other)))\n\n    @final\n    def __rmul__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__rmul__(unwrap1(other)))\n\n    @final\n    def __truediv__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__truediv__(unwrap1(other)))\n\n    @final\n    def __rtruediv__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__rtruediv__(unwrap1(other)))\n\n    @final\n    def __floordiv__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__floordiv__(unwrap1(other)))\n\n    @final\n    def __rfloordiv__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__rfloordiv__(unwrap1(other)))\n\n    @final\n    def __mod__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__mod__(unwrap1(other)))\n\n    @final\n    def __pow__(self: TensorType, exponent: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__pow__(unwrap1(exponent)))\n\n    @final\n    @property\n    def ndim(self: TensorType) -> int:\n        return cast(int, self.raw.ndim)\n'"
eagerpy/tensor/extensions.py,0,"b'from typing import TypeVar, Callable, Any, Generic\nimport typing\nimport functools\n\nfrom .. import norms\n\nfrom .tensor import Tensor\n\n\nT = TypeVar(""T"")\n\n\ndef extensionmethod(f: Callable[..., T]) -> Callable[..., T]:\n    @functools.wraps(f)\n    def wrapper(self: Any, *args: Any, **kwargs: Any) -> Any:\n        return f(self._instance, *args, **kwargs)\n\n    return wrapper\n\n\nclass ExtensionMeta(type):\n    def __new__(cls, name, bases, attrs):  # type: ignore\n        if bases != ():\n            # creating a subclass of ExtensionMethods\n            # wrap the attributes with extensionmethod\n            attrs = {\n                k: extensionmethod(v) if not k.startswith(""__"") else v\n                for k, v in attrs.items()\n            }\n        return super().__new__(cls, name, bases, attrs)\n\n\nif hasattr(typing, ""GenericMeta""):  # Python 3.6\n    # workaround for https://github.com/python/typing/issues/449\n    class GenericExtensionMeta(typing.GenericMeta, ExtensionMeta):\n        pass\n\n\nelse:  # pragma: no cover  # Python 3.7 and newer\n\n    class GenericExtensionMeta(ExtensionMeta):  # type: ignore\n        pass\n\n\nclass ExtensionMethods(metaclass=GenericExtensionMeta):\n    def __init__(self, instance: Tensor):\n        self._instance = instance\n\n\nT_co = TypeVar(""T_co"", bound=Tensor, covariant=True)\n\n\nclass NormsMethods(Generic[T_co], ExtensionMethods):\n    l0: Callable[..., T_co] = norms.l0\n    l1: Callable[..., T_co] = norms.l1\n    l2: Callable[..., T_co] = norms.l2\n    linf: Callable[..., T_co] = norms.linf\n    lp: Callable[..., T_co] = norms.lp\n'"
eagerpy/tensor/jax.py,49,"b'from typing import (\n    Tuple,\n    cast,\n    Union,\n    Any,\n    TypeVar,\n    TYPE_CHECKING,\n    Iterable,\n    Optional,\n    overload,\n    Callable,\n    Type,\n)\nfrom typing_extensions import Literal\nfrom importlib import import_module\nimport numpy as onp\n\nfrom ..types import Axes, AxisAxes, Shape, ShapeOrScalar\n\nfrom .tensor import Tensor\nfrom .tensor import TensorOrScalar\n\nfrom .base import BaseTensor\nfrom .base import unwrap_\nfrom .base import unwrap1\n\n\nif TYPE_CHECKING:\n    # for static analyzers\n    import jax\n    import jax.numpy as np\n    from .extensions import NormsMethods  # noqa: F401\nelse:\n    # lazy import in JAXTensor\n    jax = None\n    np = None\n\n\n# stricter TensorType to support additional internal methods\nTensorType = TypeVar(""TensorType"", bound=""JAXTensor"")\n\n\ndef assert_bool(x: Any) -> None:\n    if not isinstance(x, Tensor):\n        return\n    if x.dtype != jax.numpy.bool_:\n        raise ValueError(f""requires dtype bool, consider t.bool().all()"")\n\n\ndef getitem_preprocess(x: Any) -> Any:\n    if isinstance(x, range):\n        return list(x)\n    elif isinstance(x, Tensor):\n        return x.raw\n    else:\n        return x\n\n\nclass JAXTensor(BaseTensor):\n    __slots__ = ()\n\n    # more specific types for the extensions\n    norms: ""NormsMethods[JAXTensor]""\n\n    _registered = False\n    key = None\n\n    def __new__(cls: Type[""JAXTensor""], *args: Any, **kwargs: Any) -> ""JAXTensor"":\n        if not cls._registered:\n            import jax\n\n            def flatten(t: JAXTensor) -> Tuple[Any, None]:\n                return ((t.raw,), None)\n\n            def unflatten(aux_data: None, children: Tuple) -> JAXTensor:\n                return cls(*children)\n\n            jax.tree_util.register_pytree_node(cls, flatten, unflatten)\n            cls._registered = True\n        return cast(JAXTensor, super().__new__(cls))\n\n    def __init__(self, raw: ""np.ndarray""):  # type: ignore\n        global jax\n        global np\n        if jax is None:\n            jax = import_module(""jax"")\n            np = import_module(""jax.numpy"")\n        super().__init__(raw)\n\n    @property\n    def raw(self) -> ""np.ndarray"":  # type: ignore\n        return super().raw\n\n    @classmethod\n    def _get_subkey(cls) -> Any:\n        if cls.key is None:\n            cls.key = jax.random.PRNGKey(0)\n        cls.key, subkey = jax.random.split(cls.key)\n        return subkey\n\n    def numpy(self) -> Any:\n        a = onp.asarray(self.raw)\n        assert a.flags.writeable is False\n        return a\n\n    def item(self) -> Union[int, float, bool]:\n        return self.raw.item()  # type: ignore\n\n    @property\n    def shape(self) -> Shape:\n        return cast(Tuple, self.raw.shape)\n\n    def reshape(self: TensorType, shape: Union[Shape, int]) -> TensorType:\n        if isinstance(shape, int):\n            shape = (shape,)\n        return type(self)(self.raw.reshape(shape))\n\n    def astype(self: TensorType, dtype: Any) -> TensorType:\n        return type(self)(self.raw.astype(dtype))\n\n    def clip(self: TensorType, min_: float, max_: float) -> TensorType:\n        return type(self)(np.clip(self.raw, min_, max_))\n\n    def square(self: TensorType) -> TensorType:\n        return type(self)(np.square(self.raw))\n\n    def arctanh(self: TensorType) -> TensorType:\n        return type(self)(np.arctanh(self.raw))\n\n    def sum(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        return type(self)(self.raw.sum(axis=axis, keepdims=keepdims))\n\n    def mean(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        if self.raw.dtype not in [np.float16, np.float32, np.float64]:\n            raise ValueError(\n                f""Can only calculate the mean of floating types. Got {self.raw.dtype} instead.""\n            )\n        return type(self)(self.raw.mean(axis=axis, keepdims=keepdims))\n\n    def min(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        return type(self)(self.raw.min(axis=axis, keepdims=keepdims))\n\n    def max(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        return type(self)(self.raw.max(axis=axis, keepdims=keepdims))\n\n    def minimum(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(np.minimum(self.raw, unwrap1(other)))\n\n    def maximum(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(np.maximum(self.raw, unwrap1(other)))\n\n    def argmin(self: TensorType, axis: Optional[int] = None) -> TensorType:\n        return type(self)(self.raw.argmin(axis=axis))\n\n    def argmax(self: TensorType, axis: Optional[int] = None) -> TensorType:\n        return type(self)(self.raw.argmax(axis=axis))\n\n    def argsort(self: TensorType, axis: int = -1) -> TensorType:\n        return type(self)(self.raw.argsort(axis=axis))\n\n    def sort(self: TensorType, axis: int = -1) -> TensorType:\n        return type(self)(self.raw.sort(axis=axis))\n\n    def uniform(\n        self: TensorType, shape: ShapeOrScalar, low: float = 0.0, high: float = 1.0\n    ) -> TensorType:\n        if not isinstance(shape, Iterable):\n            shape = (shape,)\n\n        subkey = self._get_subkey()\n        return type(self)(jax.random.uniform(subkey, shape, minval=low, maxval=high))\n\n    def normal(\n        self: TensorType, shape: ShapeOrScalar, mean: float = 0.0, stddev: float = 1.0\n    ) -> TensorType:\n        if not isinstance(shape, Iterable):\n            shape = (shape,)\n\n        subkey = self._get_subkey()\n        return type(self)(jax.random.normal(subkey, shape) * stddev + mean)\n\n    def ones(self: TensorType, shape: ShapeOrScalar) -> TensorType:\n        return type(self)(np.ones(shape, dtype=self.raw.dtype))\n\n    def zeros(self: TensorType, shape: ShapeOrScalar) -> TensorType:\n        return type(self)(np.zeros(shape, dtype=self.raw.dtype))\n\n    def ones_like(self: TensorType) -> TensorType:\n        return type(self)(np.ones_like(self.raw))\n\n    def zeros_like(self: TensorType) -> TensorType:\n        return type(self)(np.zeros_like(self.raw))\n\n    def full_like(self: TensorType, fill_value: float) -> TensorType:\n        return type(self)(np.full_like(self.raw, fill_value))\n\n    def onehot_like(\n        self: TensorType, indices: TensorType, *, value: float = 1\n    ) -> TensorType:\n        if self.ndim != 2:\n            raise ValueError(""onehot_like only supported for 2D tensors"")\n        if indices.ndim != 1:\n            raise ValueError(""onehot_like requires 1D indices"")\n        if len(indices) != len(self):\n            raise ValueError(""length of indices must match length of tensor"")\n        x = np.arange(self.raw.shape[1]).reshape(1, -1)\n        indices = indices.raw.reshape(-1, 1)\n        return type(self)((x == indices) * value)\n\n    def from_numpy(self: TensorType, a: Any) -> TensorType:\n        return type(self)(np.asarray(a))\n\n    def _concatenate(\n        self: TensorType, tensors: Iterable[TensorType], axis: int = 0\n    ) -> TensorType:\n        # concatenates only ""tensors"", but not ""self""\n        tensors_ = unwrap_(*tensors)\n        return type(self)(np.concatenate(tensors_, axis=axis))\n\n    def _stack(\n        self: TensorType, tensors: Iterable[TensorType], axis: int = 0\n    ) -> TensorType:\n        # stacks only ""tensors"", but not ""self""\n        tensors_ = unwrap_(*tensors)\n        return type(self)(np.stack(tensors_, axis=axis))\n\n    def transpose(self: TensorType, axes: Optional[Axes] = None) -> TensorType:\n        if axes is None:\n            axes = tuple(range(self.ndim - 1, -1, -1))\n        return type(self)(np.transpose(self.raw, axes=axes))\n\n    def all(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        assert_bool(self)\n        return type(self)(self.raw.all(axis=axis, keepdims=keepdims))\n\n    def any(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        assert_bool(self)\n        return type(self)(self.raw.any(axis=axis, keepdims=keepdims))\n\n    def logical_and(self: TensorType, other: TensorOrScalar) -> TensorType:\n        assert_bool(self)\n        assert_bool(other)\n        return type(self)(np.logical_and(self.raw, unwrap1(other)))\n\n    def logical_or(self: TensorType, other: TensorOrScalar) -> TensorType:\n        assert_bool(self)\n        assert_bool(other)\n        return type(self)(np.logical_or(self.raw, unwrap1(other)))\n\n    def logical_not(self: TensorType) -> TensorType:\n        assert_bool(self)\n        return type(self)(np.logical_not(self.raw))\n\n    def exp(self: TensorType) -> TensorType:\n        return type(self)(np.exp(self.raw))\n\n    def log(self: TensorType) -> TensorType:\n        return type(self)(np.log(self.raw))\n\n    def log2(self: TensorType) -> TensorType:\n        return type(self)(np.log2(self.raw))\n\n    def log10(self: TensorType) -> TensorType:\n        return type(self)(np.log10(self.raw))\n\n    def log1p(self: TensorType) -> TensorType:\n        return type(self)(np.log1p(self.raw))\n\n    def tile(self: TensorType, multiples: Axes) -> TensorType:\n        multiples = unwrap1(multiples)\n        if len(multiples) != self.ndim:\n            raise ValueError(""multiples requires one entry for each dimension"")\n        return type(self)(np.tile(self.raw, multiples))\n\n    def softmax(self: TensorType, axis: int = -1) -> TensorType:\n        return type(self)(jax.nn.softmax(self.raw, axis=axis))\n\n    def log_softmax(self: TensorType, axis: int = -1) -> TensorType:\n        return type(self)(jax.nn.log_softmax(self.raw, axis=axis))\n\n    def squeeze(self: TensorType, axis: Optional[AxisAxes] = None) -> TensorType:\n        if axis is not None:\n            # workaround for https://github.com/google/jax/issues/2284\n            axis = (axis,) if isinstance(axis, int) else axis\n            shape = self.shape\n            if any(shape[i] != 1 for i in axis):\n                raise ValueError(\n                    ""cannot select an axis to squeeze out which has size not equal to one""\n                )\n        return type(self)(self.raw.squeeze(axis=axis))\n\n    def expand_dims(self: TensorType, axis: int) -> TensorType:\n        return type(self)(np.expand_dims(self.raw, axis=axis))\n\n    def full(self: TensorType, shape: ShapeOrScalar, value: float) -> TensorType:\n        if not isinstance(shape, Iterable):\n            shape = (shape,)\n        return type(self)(np.full(shape, value, dtype=self.raw.dtype))\n\n    def index_update(\n        self: TensorType, indices: Any, values: TensorOrScalar\n    ) -> TensorType:\n        indices, values = unwrap_(indices, values)\n        if isinstance(indices, tuple):\n            indices = unwrap_(*indices)\n        return type(self)(jax.ops.index_update(self.raw, indices, values))\n\n    def arange(\n        self: TensorType,\n        start: int,\n        stop: Optional[int] = None,\n        step: Optional[int] = None,\n    ) -> TensorType:\n        return type(self)(np.arange(start, stop, step))\n\n    def cumsum(self: TensorType, axis: Optional[int] = None) -> TensorType:\n        return type(self)(self.raw.cumsum(axis=axis))\n\n    def flip(self: TensorType, axis: Optional[AxisAxes] = None) -> TensorType:\n        return type(self)(np.flip(self.raw, axis=axis))\n\n    def meshgrid(\n        self: TensorType, *tensors: TensorType, indexing: str = ""xy""\n    ) -> Tuple[TensorType, ...]:\n        tensors = unwrap_(*tensors)\n        outputs = np.meshgrid(self.raw, *tensors, indexing=indexing)\n        return tuple(type(self)(out) for out in outputs)\n\n    def pad(\n        self: TensorType,\n        paddings: Tuple[Tuple[int, int], ...],\n        mode: str = ""constant"",\n        value: float = 0,\n    ) -> TensorType:\n        if len(paddings) != self.ndim:\n            raise ValueError(""pad requires a tuple for each dimension"")\n        for p in paddings:\n            if len(p) != 2:\n                raise ValueError(""pad requires a tuple for each dimension"")\n        if not (mode == ""constant"" or mode == ""reflect""):\n            raise ValueError(""pad requires mode \'constant\' or \'reflect\'"")\n        if mode == ""reflect"":\n            # PyTorch\'s pad has limited support for \'reflect\' padding\n            if self.ndim != 3 and self.ndim != 4:\n                raise NotImplementedError  # pragma: no cover\n            k = self.ndim - 2\n            if paddings[:k] != ((0, 0),) * k:\n                raise NotImplementedError  # pragma: no cover\n        if mode == ""constant"":\n            return type(self)(\n                np.pad(self.raw, paddings, mode=mode, constant_values=value)\n            )\n        else:\n            return type(self)(np.pad(self.raw, paddings, mode=mode))\n\n    def isnan(self: TensorType) -> TensorType:\n        return type(self)(np.isnan(self.raw))\n\n    def isinf(self: TensorType) -> TensorType:\n        return type(self)(np.isinf(self.raw))\n\n    def crossentropy(self: TensorType, labels: TensorType) -> TensorType:\n        if self.ndim != 2:\n            raise ValueError(""crossentropy only supported for 2D logits tensors"")\n        if self.shape[:1] != labels.shape:\n            raise ValueError(""labels must be 1D and must match the length of logits"")\n        # for numerical reasons we subtract the max logit\n        # (mathematically it doesn\'t matter!)\n        # otherwise exp(logits) might become too large or too small\n        logits = self.raw\n        logits = logits - logits.max(axis=1, keepdims=True)\n        e = np.exp(logits)\n        s = np.sum(e, axis=1)\n        ces = np.log(s) - np.take_along_axis(\n            logits, labels.raw[:, np.newaxis], axis=1\n        ).squeeze(axis=1)\n        return type(self)(ces)\n\n    @overload\n    def _value_and_grad_fn(\n        self: TensorType, f: Callable[..., TensorType]\n    ) -> Callable[..., Tuple[TensorType, TensorType]]:\n        ...\n\n    @overload  # noqa: F811 (waiting for pyflakes > 2.1.1)\n    def _value_and_grad_fn(\n        self: TensorType, f: Callable[..., TensorType], has_aux: Literal[False]\n    ) -> Callable[..., Tuple[TensorType, TensorType]]:\n        ...\n\n    @overload  # noqa: F811 (waiting for pyflakes > 2.1.1)\n    def _value_and_grad_fn(\n        self: TensorType,\n        f: Callable[..., Tuple[TensorType, Any]],\n        has_aux: Literal[True],\n    ) -> Callable[..., Tuple[TensorType, Any, TensorType]]:\n        ...\n\n    def _value_and_grad_fn(  # noqa: F811 (waiting for pyflakes > 2.1.1)\n        self: TensorType, f: Callable, has_aux: bool = False\n    ) -> Callable[..., Tuple]:\n        # f takes and returns JAXTensor instances\n        # jax.value_and_grad accepts functions that take JAXTensor instances\n        # because we registered JAXTensor as JAX type, but it still requires\n        # the output to be a scalar (that is not not wrapped as a JAXTensor)\n\n        # f_jax is like f but unwraps loss\n        if has_aux:\n\n            def f_jax(*args: Any, **kwargs: Any) -> Tuple[Any, Any]:\n                loss, aux = f(*args, **kwargs)\n                return loss.raw, aux\n\n        else:\n\n            def f_jax(*args: Any, **kwargs: Any) -> Any:  # type: ignore\n                loss = f(*args, **kwargs)\n                return loss.raw\n\n        value_and_grad_jax = jax.value_and_grad(f_jax, has_aux=has_aux)\n\n        # value_and_grad is like value_and_grad_jax but wraps loss\n        if has_aux:\n\n            def value_and_grad(\n                x: JAXTensor, *args: Any, **kwargs: Any\n            ) -> Tuple[JAXTensor, Any, JAXTensor]:\n                assert isinstance(x, JAXTensor)\n                (loss, aux), grad = value_and_grad_jax(x, *args, **kwargs)\n                assert grad.shape == x.shape\n                return JAXTensor(loss), aux, grad\n\n        else:\n\n            def value_and_grad(  # type: ignore\n                x: JAXTensor, *args: Any, **kwargs: Any\n            ) -> Tuple[JAXTensor, JAXTensor]:\n                assert isinstance(x, JAXTensor)\n                loss, grad = value_and_grad_jax(x, *args, **kwargs)\n                assert grad.shape == x.shape\n                return JAXTensor(loss), grad\n\n        return value_and_grad\n\n    def sign(self: TensorType) -> TensorType:\n        return type(self)(np.sign(self.raw))\n\n    def sqrt(self: TensorType) -> TensorType:\n        return type(self)(np.sqrt(self.raw))\n\n    def tanh(self: TensorType) -> TensorType:\n        return type(self)(np.tanh(self.raw))\n\n    def float32(self: TensorType) -> TensorType:\n        return self.astype(np.float32)\n\n    def where(self: TensorType, x: TensorOrScalar, y: TensorOrScalar) -> TensorType:\n        x, y = unwrap_(x, y)\n        return type(self)(np.where(self.raw, x, y))\n\n    def matmul(self: TensorType, other: TensorType) -> TensorType:\n        if self.ndim != 2 or other.ndim != 2:\n            raise ValueError(\n                f""matmul requires both tensors to be 2D, got {self.ndim}D and {other.ndim}D""\n            )\n        return type(self)(np.matmul(self.raw, other.raw))\n\n    def __lt__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__lt__(unwrap1(other)))\n\n    def __le__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__le__(unwrap1(other)))\n\n    def __eq__(self: TensorType, other: TensorOrScalar) -> TensorType:  # type: ignore\n        return type(self)(self.raw.__eq__(unwrap1(other)))\n\n    def __ne__(self: TensorType, other: TensorOrScalar) -> TensorType:  # type: ignore\n        return type(self)(self.raw.__ne__(unwrap1(other)))\n\n    def __gt__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__gt__(unwrap1(other)))\n\n    def __ge__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__ge__(unwrap1(other)))\n\n    def __getitem__(self: TensorType, index: Any) -> TensorType:\n        if isinstance(index, tuple):\n            index = tuple(getitem_preprocess(x) for x in index)\n        else:\n            index = getitem_preprocess(index)\n        return type(self)(self.raw[index])\n\n    def take_along_axis(self: TensorType, index: TensorType, axis: int) -> TensorType:\n        if axis % self.ndim != self.ndim - 1:\n            raise NotImplementedError(\n                f""take_along_axis is currently only supported for the last axis""\n            )\n        return type(self)(np.take_along_axis(self.raw, index.raw, axis=axis))\n\n    def bool(self: TensorType) -> TensorType:\n        return self.astype(np.bool_)\n'"
eagerpy/tensor/numpy.py,55,"b'from typing import (\n    Tuple,\n    cast,\n    Union,\n    Any,\n    Iterable,\n    Optional,\n    overload,\n    Callable,\n    TYPE_CHECKING,\n)\nfrom typing_extensions import Literal\nimport numpy as np\n\nfrom ..types import Axes, AxisAxes, Shape, ShapeOrScalar\n\nfrom .tensor import TensorType\nfrom .tensor import Tensor\nfrom .tensor import TensorOrScalar\n\nfrom .base import BaseTensor\nfrom .base import unwrap_\nfrom .base import unwrap1\n\nif TYPE_CHECKING:\n    from .extensions import NormsMethods  # noqa: F401\n\n\ndef assert_bool(x: Any) -> None:\n    if not isinstance(x, Tensor):\n        return\n    if x.dtype != np.dtype(""bool""):\n        raise ValueError(f""requires dtype bool, consider t.bool().all()"")\n\n\nclass NumPyTensor(BaseTensor):\n    __slots__ = ()\n\n    # more specific types for the extensions\n    norms: ""NormsMethods[NumPyTensor]""\n\n    def __init__(self, raw: ""np.ndarray""):  # type: ignore\n        super().__init__(raw)\n\n    @property\n    def raw(self) -> ""np.ndarray"":  # type: ignore\n        return super().raw\n\n    def numpy(self: TensorType) -> Any:\n        a = self.raw.view()\n        if a.flags.writeable:\n            # without the check, we would attempt to set it on array\n            # scalars, and that would fail\n            a.flags.writeable = False\n        return a\n\n    def item(self) -> Union[int, float, bool]:\n        return self.raw.item()  # type: ignore\n\n    @property\n    def shape(self: TensorType) -> Shape:\n        return cast(Tuple, self.raw.shape)\n\n    def reshape(self: TensorType, shape: Union[Shape, int]) -> TensorType:\n        if isinstance(shape, int):\n            shape = (shape,)\n        return type(self)(self.raw.reshape(shape))\n\n    def astype(self: TensorType, dtype: Any) -> TensorType:\n        return type(self)(self.raw.astype(dtype))\n\n    def clip(self: TensorType, min_: float, max_: float) -> TensorType:\n        return type(self)(np.clip(self.raw, min_, max_))\n\n    def square(self: TensorType) -> TensorType:\n        return type(self)(np.square(self.raw))\n\n    def arctanh(self: TensorType) -> TensorType:\n        return type(self)(np.arctanh(self.raw))\n\n    def sum(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        return type(self)(self.raw.sum(axis=axis, keepdims=keepdims))\n\n    def mean(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        if self.raw.dtype not in [np.float16, np.float32, np.float64]:\n            raise ValueError(\n                f""Can only calculate the mean of floating types. Got {self.raw.dtype} instead.""\n            )\n        return type(self)(self.raw.mean(axis=axis, keepdims=keepdims))\n\n    def min(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        return type(self)(self.raw.min(axis=axis, keepdims=keepdims))\n\n    def max(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        return type(self)(self.raw.max(axis=axis, keepdims=keepdims))\n\n    def minimum(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(np.minimum(self.raw, unwrap1(other)))\n\n    def maximum(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(np.maximum(self.raw, unwrap1(other)))\n\n    def argmin(self: TensorType, axis: Optional[int] = None) -> TensorType:\n        return type(self)(self.raw.argmin(axis=axis))\n\n    def argmax(self: TensorType, axis: Optional[int] = None) -> TensorType:\n        return type(self)(self.raw.argmax(axis=axis))\n\n    def argsort(self: TensorType, axis: int = -1) -> TensorType:\n        return type(self)(self.raw.argsort(axis=axis))\n\n    def sort(self: TensorType, axis: int = -1) -> TensorType:\n        return type(self)(np.sort(self.raw, axis=axis))\n\n    def uniform(\n        self: TensorType, shape: ShapeOrScalar, low: float = 0.0, high: float = 1.0\n    ) -> TensorType:\n        return type(self)(np.random.uniform(low, high, size=shape))\n\n    def normal(\n        self: TensorType, shape: ShapeOrScalar, mean: float = 0.0, stddev: float = 1.0\n    ) -> TensorType:\n        return type(self)(np.random.normal(mean, stddev, size=shape))\n\n    def ones(self: TensorType, shape: ShapeOrScalar) -> TensorType:\n        return type(self)(np.ones(shape, dtype=self.raw.dtype))\n\n    def zeros(self: TensorType, shape: ShapeOrScalar) -> TensorType:\n        return type(self)(np.zeros(shape, dtype=self.raw.dtype))\n\n    def ones_like(self: TensorType) -> TensorType:\n        return type(self)(np.ones_like(self.raw))\n\n    def zeros_like(self: TensorType) -> TensorType:\n        return type(self)(np.zeros_like(self.raw))\n\n    def full_like(self: TensorType, fill_value: float) -> TensorType:\n        return type(self)(np.full_like(self.raw, fill_value))\n\n    def onehot_like(\n        self: TensorType, indices: TensorType, *, value: float = 1\n    ) -> TensorType:\n        if self.ndim != 2:\n            raise ValueError(""onehot_like only supported for 2D tensors"")\n        if indices.ndim != 1:\n            raise ValueError(""onehot_like requires 1D indices"")\n        if len(indices) != len(self):\n            raise ValueError(""length of indices must match length of tensor"")\n        x = np.zeros_like(self.raw)\n        rows = np.arange(len(x))\n        x[rows, indices.raw] = value\n        return type(self)(x)\n\n    def from_numpy(self: TensorType, a: Any) -> TensorType:\n        return type(self)(np.asarray(a))\n\n    def _concatenate(\n        self: TensorType, tensors: Iterable[TensorType], axis: int = 0\n    ) -> TensorType:\n        # concatenates only ""tensors"", but not ""self""\n        tensors_ = unwrap_(*tensors)\n        return type(self)(np.concatenate(tensors_, axis=axis))\n\n    def _stack(\n        self: TensorType, tensors: Iterable[TensorType], axis: int = 0\n    ) -> TensorType:\n        # stacks only ""tensors"", but not ""self""\n        tensors_ = unwrap_(*tensors)\n        return type(self)(np.stack(tensors_, axis=axis))\n\n    def transpose(self: TensorType, axes: Optional[Axes] = None) -> TensorType:\n        if axes is None:\n            axes = tuple(range(self.ndim - 1, -1, -1))\n        return type(self)(np.transpose(self.raw, axes=axes))\n\n    def all(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        assert_bool(self)\n        return type(self)(self.raw.all(axis=axis, keepdims=keepdims))\n\n    def any(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        assert_bool(self)\n        return type(self)(self.raw.any(axis=axis, keepdims=keepdims))\n\n    def logical_and(self: TensorType, other: TensorOrScalar) -> TensorType:\n        assert_bool(self)\n        assert_bool(other)\n        return type(self)(np.logical_and(self.raw, unwrap1(other)))\n\n    def logical_or(self: TensorType, other: TensorOrScalar) -> TensorType:\n        assert_bool(self)\n        assert_bool(other)\n        return type(self)(np.logical_or(self.raw, unwrap1(other)))\n\n    def logical_not(self: TensorType) -> TensorType:\n        assert_bool(self)\n        return type(self)(np.logical_not(self.raw))\n\n    def exp(self: TensorType) -> TensorType:\n        return type(self)(np.exp(self.raw))\n\n    def log(self: TensorType) -> TensorType:\n        return type(self)(np.log(self.raw))\n\n    def log2(self: TensorType) -> TensorType:\n        return type(self)(np.log2(self.raw))\n\n    def log10(self: TensorType) -> TensorType:\n        return type(self)(np.log10(self.raw))\n\n    def log1p(self: TensorType) -> TensorType:\n        return type(self)(np.log1p(self.raw))\n\n    def tile(self: TensorType, multiples: Axes) -> TensorType:\n        multiples = unwrap1(multiples)\n        if len(multiples) != self.ndim:\n            raise ValueError(""multiples requires one entry for each dimension"")\n        return type(self)(np.tile(self.raw, multiples))\n\n    def softmax(self: TensorType, axis: int = -1) -> TensorType:\n        # for numerical reasons we subtract the max logit\n        # (mathematically it doesn\'t matter!)\n        # otherwise exp(logits) might become too large or too small\n        logits = self.raw\n        logits = logits - logits.max(axis=axis, keepdims=True)\n        e = np.exp(logits)\n        return type(self)(e / e.sum(axis=axis, keepdims=True))\n\n    def log_softmax(self: TensorType, axis: int = -1) -> TensorType:\n        # for numerical reasons we subtract the max logit\n        # (mathematically it doesn\'t matter!)\n        # otherwise exp(logits) might become too large or too small\n        logits = self.raw\n        logits = logits - logits.max(axis=axis, keepdims=True)\n        log_sum_exp = np.log(np.exp(logits).sum(axis=axis, keepdims=True))\n        return type(self)(logits - log_sum_exp)\n\n    def squeeze(self: TensorType, axis: Optional[AxisAxes] = None) -> TensorType:\n        return type(self)(self.raw.squeeze(axis=axis))\n\n    def expand_dims(self: TensorType, axis: int) -> TensorType:\n        return type(self)(np.expand_dims(self.raw, axis=axis))\n\n    def full(self: TensorType, shape: ShapeOrScalar, value: float) -> TensorType:\n        return type(self)(np.full(shape, value, dtype=self.raw.dtype))\n\n    def index_update(\n        self: TensorType, indices: Any, values: TensorOrScalar\n    ) -> TensorType:\n        indices, values = unwrap_(indices, values)\n        if isinstance(indices, tuple):\n            indices = unwrap_(*indices)\n        x = self.raw.copy()\n        x[indices] = values\n        return type(self)(x)\n\n    def arange(\n        self: TensorType,\n        start: int,\n        stop: Optional[int] = None,\n        step: Optional[int] = None,\n    ) -> TensorType:\n        return type(self)(np.arange(start, stop, step))\n\n    def cumsum(self: TensorType, axis: Optional[int] = None) -> TensorType:\n        return type(self)(self.raw.cumsum(axis=axis))\n\n    def flip(self: TensorType, axis: Optional[AxisAxes] = None) -> TensorType:\n        return type(self)(np.flip(self.raw, axis=axis))\n\n    def meshgrid(\n        self: TensorType, *tensors: TensorType, indexing: str = ""xy""\n    ) -> Tuple[TensorType, ...]:\n        tensors = unwrap_(*tensors)\n        outputs = np.meshgrid(self.raw, *tensors, indexing=indexing)\n        return tuple(type(self)(out) for out in outputs)\n\n    def pad(\n        self: TensorType,\n        paddings: Tuple[Tuple[int, int], ...],\n        mode: str = ""constant"",\n        value: float = 0,\n    ) -> TensorType:\n        if len(paddings) != self.ndim:\n            raise ValueError(""pad requires a tuple for each dimension"")\n        for p in paddings:\n            if len(p) != 2:\n                raise ValueError(""pad requires a tuple for each dimension"")\n        if not (mode == ""constant"" or mode == ""reflect""):\n            raise ValueError(""pad requires mode \'constant\' or \'reflect\'"")\n        if mode == ""reflect"":\n            # PyTorch\'s pad has limited support for \'reflect\' padding\n            if self.ndim != 3 and self.ndim != 4:\n                raise NotImplementedError  # pragma: no cover\n            k = self.ndim - 2\n            if paddings[:k] != ((0, 0),) * k:\n                raise NotImplementedError  # pragma: no cover\n        if mode == ""constant"":\n            return type(self)(\n                np.pad(self.raw, paddings, mode=mode, constant_values=value)\n            )\n        else:\n            return type(self)(np.pad(self.raw, paddings, mode=mode))\n\n    def isnan(self: TensorType) -> TensorType:\n        return type(self)(np.isnan(self.raw))\n\n    def isinf(self: TensorType) -> TensorType:\n        return type(self)(np.isinf(self.raw))\n\n    def crossentropy(self: TensorType, labels: TensorType) -> TensorType:\n        if self.ndim != 2:\n            raise ValueError(""crossentropy only supported for 2D logits tensors"")\n        if self.shape[:1] != labels.shape:\n            raise ValueError(""labels must be 1D and must match the length of logits"")\n        # for numerical reasons we subtract the max logit\n        # (mathematically it doesn\'t matter!)\n        # otherwise exp(logits) might become too large or too small\n        logits = self.raw\n        logits = logits - logits.max(axis=1, keepdims=True)\n        e = np.exp(logits)\n        s = np.sum(e, axis=1)\n        ces = np.log(s) - np.take_along_axis(\n            logits, labels.raw[:, np.newaxis], axis=1\n        ).squeeze(axis=1)\n        return type(self)(ces)\n\n    @overload\n    def _value_and_grad_fn(\n        self: TensorType, f: Callable[..., TensorType]\n    ) -> Callable[..., Tuple[TensorType, TensorType]]:\n        ...\n\n    @overload  # noqa: F811 (waiting for pyflakes > 2.1.1)\n    def _value_and_grad_fn(\n        self: TensorType, f: Callable[..., TensorType], has_aux: Literal[False]\n    ) -> Callable[..., Tuple[TensorType, TensorType]]:\n        ...\n\n    @overload  # noqa: F811 (waiting for pyflakes > 2.1.1)\n    def _value_and_grad_fn(\n        self: TensorType,\n        f: Callable[..., Tuple[TensorType, Any]],\n        has_aux: Literal[True],\n    ) -> Callable[..., Tuple[TensorType, Any, TensorType]]:\n        ...\n\n    def _value_and_grad_fn(  # noqa: F811 (waiting for pyflakes > 2.1.1)\n        self: TensorType, f: Callable, has_aux: bool = False\n    ) -> Callable[..., Tuple]:\n        # TODO: maybe implement this using https://github.com/HIPS/autograd\n        raise NotImplementedError  # pragma: no cover\n\n    def sign(self: TensorType) -> TensorType:\n        return type(self)(np.sign(self.raw))\n\n    def sqrt(self: TensorType) -> TensorType:\n        return type(self)(np.sqrt(self.raw))\n\n    def tanh(self: TensorType) -> TensorType:\n        return type(self)(np.tanh(self.raw))\n\n    def float32(self: TensorType) -> TensorType:\n        return self.astype(np.float32)\n\n    def where(self: TensorType, x: TensorOrScalar, y: TensorOrScalar) -> TensorType:\n        x, y = unwrap_(x, y)\n        return type(self)(np.where(self.raw, x, y))\n\n    def matmul(self: TensorType, other: TensorType) -> TensorType:\n        if self.ndim != 2 or other.ndim != 2:\n            raise ValueError(\n                f""matmul requires both tensors to be 2D, got {self.ndim}D and {other.ndim}D""\n            )\n        return type(self)(np.matmul(self.raw, other.raw))\n\n    def __lt__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__lt__(unwrap1(other)))\n\n    def __le__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__le__(unwrap1(other)))\n\n    def __eq__(self: TensorType, other: TensorOrScalar) -> TensorType:  # type: ignore\n        return type(self)(self.raw.__eq__(unwrap1(other)))\n\n    def __ne__(self: TensorType, other: TensorOrScalar) -> TensorType:  # type: ignore\n        return type(self)(self.raw.__ne__(unwrap1(other)))\n\n    def __gt__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__gt__(unwrap1(other)))\n\n    def __ge__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__ge__(unwrap1(other)))\n\n    def __getitem__(self: TensorType, index: Any) -> TensorType:\n        if isinstance(index, tuple):\n            index = tuple(x.raw if isinstance(x, Tensor) else x for x in index)\n        elif isinstance(index, Tensor):\n            index = index.raw\n        return type(self)(self.raw[index])\n\n    def take_along_axis(self: TensorType, index: TensorType, axis: int) -> TensorType:\n        if axis % self.ndim != self.ndim - 1:\n            raise NotImplementedError(\n                f""take_along_axis is currently only supported for the last axis""\n            )\n        return type(self)(np.take_along_axis(self.raw, index.raw, axis=axis))\n\n    def bool(self: TensorType) -> TensorType:\n        return self.astype(np.dtype(""bool""))\n'"
eagerpy/tensor/pytorch.py,1,"b'from typing import (\n    Tuple,\n    cast,\n    Union,\n    Any,\n    TypeVar,\n    TYPE_CHECKING,\n    Iterable,\n    Optional,\n    overload,\n    Callable,\n)\nfrom typing_extensions import Literal\nimport numpy as np\nfrom importlib import import_module\n\nfrom ..types import Axes, AxisAxes, Shape, ShapeOrScalar\n\nfrom .tensor import Tensor\nfrom .tensor import TensorOrScalar\n\nfrom .base import BaseTensor\nfrom .base import unwrap_\nfrom .base import unwrap1\n\nif TYPE_CHECKING:\n    import torch  # for static analyzers\n    from .extensions import NormsMethods  # noqa: F401\nelse:\n    # lazy import in PyTorchTensor\n    torch = None\n\n\n# stricter TensorType to get additional type information from the raw method\nTensorType = TypeVar(""TensorType"", bound=""PyTorchTensor"")\n\n\ndef assert_bool(x: Any) -> None:\n    if not isinstance(x, Tensor):\n        return\n    if x.dtype != torch.bool:\n        raise ValueError(f""requires dtype bool, consider t.bool().all()"")\n\n\nclass PyTorchTensor(BaseTensor):\n    __slots__ = ()\n\n    # more specific types for the extensions\n    norms: ""NormsMethods[PyTorchTensor]""\n\n    def __init__(self, raw: ""torch.Tensor""):\n        global torch\n        if torch is None:\n            torch = import_module(""torch"")  # type: ignore\n        super().__init__(raw)\n\n    @property\n    def raw(self) -> ""torch.Tensor"":\n        return cast(torch.Tensor, super().raw)\n\n    def tanh(self: TensorType) -> TensorType:\n        return type(self)(torch.tanh(self.raw))\n\n    def numpy(self: TensorType) -> Any:\n        a = self.raw.detach().cpu().numpy()\n        if a.flags.writeable:\n            # without the check, we would attempt to set it on array\n            # scalars, and that would fail\n            a.flags.writeable = False\n        return a\n\n    def item(self) -> Union[int, float, bool]:\n        return self.raw.item()\n\n    @property\n    def shape(self) -> Shape:\n        return self.raw.shape\n\n    def reshape(self: TensorType, shape: Union[Shape, int]) -> TensorType:\n        if isinstance(shape, int):\n            shape = (shape,)\n        return type(self)(self.raw.reshape(shape))\n\n    def astype(self: TensorType, dtype: Any) -> TensorType:\n        return type(self)(self.raw.to(dtype))\n\n    def clip(self: TensorType, min_: float, max_: float) -> TensorType:\n        return type(self)(self.raw.clamp(min_, max_))\n\n    def square(self: TensorType) -> TensorType:\n        return type(self)(self.raw ** 2)\n\n    def arctanh(self: TensorType) -> TensorType:\n        """"""\n        improve once this issue has been fixed:\n        https://github.com/pytorch/pytorch/issues/10324\n        """"""\n        return type(self)(0.5 * (torch.log1p(self.raw) - torch.log1p(-self.raw)))\n\n    def sum(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        if axis is None and not keepdims:\n            return type(self)(self.raw.sum())\n        if axis is None:\n            axis = tuple(range(self.ndim))\n        return type(self)(self.raw.sum(dim=axis, keepdim=keepdims))\n\n    def mean(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        if self.raw.dtype not in [torch.float16, torch.float32, torch.float64]:\n            raise ValueError(\n                f""Can only calculate the mean of floating types. Got {self.raw.dtype} instead.""\n            )\n        if axis is None and not keepdims:\n            return type(self)(self.raw.mean())\n        if axis is None:\n            axis = tuple(range(self.ndim))\n        return type(self)(self.raw.mean(dim=axis, keepdim=keepdims))\n\n    def min(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        """"""\n        simplify once this issue has been fixed:\n        https://github.com/pytorch/pytorch/issues/28213\n        """"""\n        if axis is None and not keepdims:\n            return type(self)(self.raw.min())\n        if axis is None:\n            axis = tuple(range(self.ndim))\n        elif not isinstance(axis, Iterable):\n            axis = (axis,)\n        x = self.raw\n        for i in sorted(axis, reverse=True):\n            x, _ = x.min(i, keepdim=keepdims)\n        return type(self)(x)\n\n    def max(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        """"""\n        simplify once this issue has been fixed:\n        https://github.com/pytorch/pytorch/issues/28213\n        """"""\n        if axis is None and not keepdims:\n            return type(self)(self.raw.max())\n        if axis is None:\n            axis = tuple(range(self.ndim))\n        elif not isinstance(axis, Iterable):\n            axis = (axis,)\n        x = self.raw\n        for i in sorted(axis, reverse=True):\n            x, _ = x.max(i, keepdim=keepdims)\n        return type(self)(x)\n\n    def minimum(self: TensorType, other: TensorOrScalar) -> TensorType:\n        if isinstance(other, Tensor):\n            other_ = other.raw\n        elif isinstance(other, int) or isinstance(other, float):\n            other_ = torch.full_like(self.raw, other)\n        else:\n            raise TypeError(\n                ""expected x to be a Tensor, int or float""\n            )  # pragma: no cover\n        return type(self)(torch.min(self.raw, other_))\n\n    def maximum(self: TensorType, other: TensorOrScalar) -> TensorType:\n        if isinstance(other, Tensor):\n            other_ = other.raw\n        elif isinstance(other, int) or isinstance(other, float):\n            other_ = torch.full_like(self.raw, other)\n        else:\n            raise TypeError(\n                ""expected x to be a Tensor, int or float""\n            )  # pragma: no cover\n        return type(self)(torch.max(self.raw, other_))\n\n    def argmin(self: TensorType, axis: Optional[int] = None) -> TensorType:\n        return type(self)(self.raw.argmin(dim=axis))\n\n    def argmax(self: TensorType, axis: Optional[int] = None) -> TensorType:\n        return type(self)(self.raw.argmax(dim=axis))\n\n    def argsort(self: TensorType, axis: int = -1) -> TensorType:\n        return type(self)(self.raw.argsort(dim=axis))\n\n    def sort(self: TensorType, axis: int = -1) -> TensorType:\n        return type(self)(self.raw.sort(dim=axis).values)  # type: ignore\n\n    def uniform(\n        self: TensorType, shape: ShapeOrScalar, low: float = 0.0, high: float = 1.0\n    ) -> TensorType:\n        return type(self)(\n            torch.rand(shape, dtype=self.raw.dtype, device=self.raw.device)\n            * (high - low)\n            + low\n        )\n\n    def normal(\n        self: TensorType, shape: ShapeOrScalar, mean: float = 0.0, stddev: float = 1.0\n    ) -> TensorType:\n        return type(self)(\n            torch.randn(shape, dtype=self.raw.dtype, device=self.raw.device) * stddev\n            + mean\n        )\n\n    def ones(self: TensorType, shape: ShapeOrScalar) -> TensorType:\n        return type(self)(\n            torch.ones(shape, dtype=self.raw.dtype, device=self.raw.device)\n        )\n\n    def zeros(self: TensorType, shape: ShapeOrScalar) -> TensorType:\n        return type(self)(\n            torch.zeros(shape, dtype=self.raw.dtype, device=self.raw.device)\n        )\n\n    def ones_like(self: TensorType) -> TensorType:\n        return type(self)(torch.ones_like(self.raw))\n\n    def zeros_like(self: TensorType) -> TensorType:\n        return type(self)(torch.zeros_like(self.raw))\n\n    def full_like(self: TensorType, fill_value: float) -> TensorType:\n        return type(self)(torch.full_like(self.raw, fill_value))\n\n    def onehot_like(\n        self: TensorType, indices: TensorType, *, value: float = 1\n    ) -> TensorType:\n        if self.ndim != 2:\n            raise ValueError(""onehot_like only supported for 2D tensors"")\n        if indices.ndim != 1:\n            raise ValueError(""onehot_like requires 1D indices"")\n        if len(indices) != len(self):\n            raise ValueError(""length of indices must match length of tensor"")\n        x = torch.zeros_like(self.raw)\n        rows = np.arange(x.shape[0])\n        x[rows, indices.raw] = value\n        return type(self)(x)\n\n    def from_numpy(self: TensorType, a: Any) -> TensorType:\n        return type(self)(torch.as_tensor(a, device=self.raw.device))\n\n    def _concatenate(\n        self: TensorType, tensors: Iterable[TensorType], axis: int = 0\n    ) -> TensorType:\n        # concatenates only ""tensors"", but not ""self""\n        tensors_ = unwrap_(*tensors)\n        return type(self)(torch.cat(tensors_, dim=axis))\n\n    def _stack(\n        self: TensorType, tensors: Iterable[TensorType], axis: int = 0\n    ) -> TensorType:\n        # stacks only ""tensors"", but not ""self""\n        tensors_ = unwrap_(*tensors)\n        return type(self)(torch.stack(tensors_, dim=axis))\n\n    def transpose(self: TensorType, axes: Optional[Axes] = None) -> TensorType:\n        if axes is None:\n            axes = tuple(range(self.ndim - 1, -1, -1))\n        return type(self)(self.raw.permute(*axes))\n\n    def all(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        assert_bool(self)\n        if axis is None and not keepdims:\n            return type(self)(self.raw.all())\n        if axis is None:\n            axis = tuple(range(self.ndim))\n        elif not isinstance(axis, Iterable):\n            axis = (axis,)\n        x = self.raw\n        for i in sorted(axis, reverse=True):\n            x = x.all(i, keepdim=keepdims)\n        return type(self)(x)\n\n    def any(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        assert_bool(self)\n        if axis is None and not keepdims:\n            return type(self)(self.raw.any())\n        if axis is None:\n            axis = tuple(range(self.ndim))\n        elif not isinstance(axis, Iterable):\n            axis = (axis,)\n        x = self.raw\n        for i in sorted(axis, reverse=True):\n            x = x.any(i, keepdim=keepdims)\n        return type(self)(x)\n\n    def logical_and(self: TensorType, other: TensorOrScalar) -> TensorType:\n        assert_bool(self)\n        assert_bool(other)\n        return type(self)(self.raw & unwrap1(other))\n\n    def logical_or(self: TensorType, other: TensorOrScalar) -> TensorType:\n        assert_bool(self)\n        assert_bool(other)\n        return type(self)(self.raw | unwrap1(other))\n\n    def logical_not(self: TensorType) -> TensorType:\n        assert_bool(self)\n        return type(self)(~self.raw)\n\n    def exp(self: TensorType) -> TensorType:\n        return type(self)(torch.exp(self.raw))\n\n    def log(self: TensorType) -> TensorType:\n        return type(self)(torch.log(self.raw))\n\n    def log2(self: TensorType) -> TensorType:\n        return type(self)(torch.log2(self.raw))\n\n    def log10(self: TensorType) -> TensorType:\n        return type(self)(torch.log10(self.raw))\n\n    def log1p(self: TensorType) -> TensorType:\n        return type(self)(torch.log1p(self.raw))\n\n    def tile(self: TensorType, multiples: Axes) -> TensorType:\n        if len(multiples) != self.ndim:\n            raise ValueError(""multiples requires one entry for each dimension"")\n        return type(self)(self.raw.repeat(multiples))\n\n    def softmax(self: TensorType, axis: int = -1) -> TensorType:\n        return type(self)(torch.nn.functional.softmax(self.raw, dim=axis))\n\n    def log_softmax(self: TensorType, axis: int = -1) -> TensorType:\n        return type(self)(torch.nn.functional.log_softmax(self.raw, dim=axis))\n\n    def squeeze(self: TensorType, axis: Optional[AxisAxes] = None) -> TensorType:\n        if axis is None:\n            return type(self)(self.raw.squeeze())\n        if not isinstance(axis, Iterable):\n            axis = (axis,)\n        x = self.raw\n        for i in sorted(axis, reverse=True):\n            if x.shape[i] != 1:\n                raise ValueError(\n                    ""cannot select an axis to squeeze out which has size not equal to one""\n                )\n            x = x.squeeze(dim=i)\n        return type(self)(x)\n\n    def expand_dims(self: TensorType, axis: int) -> TensorType:\n        return type(self)(self.raw.unsqueeze(dim=axis))\n\n    def full(self: TensorType, shape: ShapeOrScalar, value: float) -> TensorType:\n        if not isinstance(shape, Iterable):\n            shape = (shape,)\n        return type(self)(\n            torch.full(shape, value, dtype=self.raw.dtype, device=self.raw.device)\n        )\n\n    def index_update(\n        self: TensorType, indices: Any, values: TensorOrScalar\n    ) -> TensorType:\n        indices, values_ = unwrap_(indices, values)\n        if isinstance(indices, tuple):\n            indices = unwrap_(*indices)\n        x = self.raw.clone()\n        x[indices] = values_\n        return type(self)(x)\n\n    def arange(\n        self: TensorType,\n        start: int,\n        stop: Optional[int] = None,\n        step: Optional[int] = None,\n    ) -> TensorType:\n        if step is None:\n            step = 1\n        if stop is None:\n            stop = start\n            start = 0\n        return type(self)(\n            torch.arange(start=start, end=stop, step=step, device=self.raw.device)\n        )\n\n    def cumsum(self: TensorType, axis: Optional[int] = None) -> TensorType:\n        if axis is None:\n            return type(self)(self.raw.reshape(-1).cumsum(dim=0))\n        return type(self)(self.raw.cumsum(dim=axis))\n\n    def flip(self: TensorType, axis: Optional[AxisAxes] = None) -> TensorType:\n        if axis is None:\n            axis = tuple(range(self.ndim))\n        if not isinstance(axis, Iterable):\n            axis = (axis,)\n        return type(self)(self.raw.flip(dims=axis))\n\n    def meshgrid(\n        self: TensorType, *tensors: TensorType, indexing: str = ""xy""\n    ) -> Tuple[TensorType, ...]:\n        tensors = unwrap_(*tensors)\n        if indexing == ""ij"" or len(tensors) == 0:\n            outputs = torch.meshgrid(self.raw, *tensors)  # type: ignore\n        elif indexing == ""xy"":\n            outputs = torch.meshgrid(tensors[0], self.raw, *tensors[1:])  # type: ignore\n        else:\n            raise ValueError(  # pragma: no cover\n                f""Valid values for indexing are \'xy\' and \'ij\', got {indexing}""\n            )\n        results = [type(self)(out) for out in outputs]\n        if indexing == ""xy"" and len(results) >= 2:\n            results[0], results[1] = results[1], results[0]\n        return tuple(results)\n\n    def pad(\n        self: TensorType,\n        paddings: Tuple[Tuple[int, int], ...],\n        mode: str = ""constant"",\n        value: float = 0,\n    ) -> TensorType:\n        if len(paddings) != self.ndim:\n            raise ValueError(""pad requires a tuple for each dimension"")\n        for p in paddings:\n            if len(p) != 2:\n                raise ValueError(""pad requires a tuple for each dimension"")\n        if not (mode == ""constant"" or mode == ""reflect""):\n            raise ValueError(""pad requires mode \'constant\' or \'reflect\'"")\n        if mode == ""reflect"":\n            # PyTorch\'s pad has limited support for \'reflect\' padding\n            if self.ndim != 3 and self.ndim != 4:\n                raise NotImplementedError  # pragma: no cover\n            k = self.ndim - 2\n            if paddings[:k] != ((0, 0),) * k:\n                raise NotImplementedError  # pragma: no cover\n            paddings = paddings[k:]\n        paddings_ = list(x for p in reversed(paddings) for x in p)\n        return type(self)(\n            torch.nn.functional.pad(self.raw, paddings_, mode=mode, value=value)\n        )\n\n    def isnan(self: TensorType) -> TensorType:\n        return type(self)(torch.isnan(self.raw))\n\n    def isinf(self: TensorType) -> TensorType:\n        return type(self)(torch.isinf(self.raw))  # type: ignore\n\n    def crossentropy(self: TensorType, labels: TensorType) -> TensorType:\n        if self.ndim != 2:\n            raise ValueError(""crossentropy only supported for 2D logits tensors"")\n        if self.shape[:1] != labels.shape:\n            raise ValueError(""labels must be 1D and must match the length of logits"")\n        return type(self)(\n            torch.nn.functional.cross_entropy(self.raw, labels.raw, reduction=""none"")\n        )\n\n    @overload\n    def _value_and_grad_fn(\n        self: TensorType, f: Callable[..., TensorType]\n    ) -> Callable[..., Tuple[TensorType, TensorType]]:\n        ...\n\n    @overload  # noqa: F811 (waiting for pyflakes > 2.1.1)\n    def _value_and_grad_fn(\n        self: TensorType, f: Callable[..., TensorType], has_aux: Literal[False]\n    ) -> Callable[..., Tuple[TensorType, TensorType]]:\n        ...\n\n    @overload  # noqa: F811 (waiting for pyflakes > 2.1.1)\n    def _value_and_grad_fn(\n        self: TensorType,\n        f: Callable[..., Tuple[TensorType, Any]],\n        has_aux: Literal[True],\n    ) -> Callable[..., Tuple[TensorType, Any, TensorType]]:\n        ...\n\n    def _value_and_grad_fn(  # noqa: F811 (waiting for pyflakes > 2.1.1)\n        self: TensorType, f: Callable, has_aux: bool = False\n    ) -> Callable[..., Tuple]:\n        def value_and_grad(x: TensorType, *args: Any, **kwargs: Any) -> Tuple:\n            x = type(self)(x.raw.clone().requires_grad_())\n            if has_aux:\n                loss, aux = f(x, *args, **kwargs)\n            else:\n                loss = f(x, *args, **kwargs)\n            loss = loss.raw\n            loss.backward()\n            assert x.raw.grad is not None\n            grad = type(self)(x.raw.grad)\n            assert grad.shape == x.shape\n            loss = loss.detach()\n            loss = type(self)(loss)\n            if has_aux:\n                if isinstance(aux, PyTorchTensor):\n                    aux = PyTorchTensor(aux.raw.detach())\n                elif isinstance(aux, tuple):\n                    aux = tuple(\n                        PyTorchTensor(t.raw.detach())\n                        if isinstance(t, PyTorchTensor)\n                        else t\n                        for t in aux\n                    )\n                return loss, aux, grad\n            else:\n                return loss, grad\n\n        return value_and_grad\n\n    def sign(self: TensorType) -> TensorType:\n        return type(self)(torch.sign(self.raw))\n\n    def sqrt(self: TensorType) -> TensorType:\n        return type(self)(torch.sqrt(self.raw))\n\n    def float32(self: TensorType) -> TensorType:\n        return self.astype(torch.float32)\n\n    def where(self: TensorType, x: TensorOrScalar, y: TensorOrScalar) -> TensorType:\n        if isinstance(x, Tensor):\n            x_ = x.raw\n        elif isinstance(x, int) or isinstance(x, float):\n            x_ = torch.full_like(self.raw, x, dtype=torch.float32)\n        else:\n            raise TypeError(\n                ""expected x to be a Tensor, int or float""\n            )  # pragma: no cover\n        if isinstance(y, Tensor):\n            y_ = y.raw\n        elif isinstance(y, int) or isinstance(y, float):\n            y_ = torch.full_like(self.raw, y, dtype=torch.float32)\n        return type(self)(torch.where(self.raw, x_, y_))\n\n    def matmul(self: TensorType, other: TensorType) -> TensorType:\n        if self.ndim != 2 or other.ndim != 2:\n            raise ValueError(\n                f""matmul requires both tensors to be 2D, got {self.ndim}D and {other.ndim}D""\n            )\n        return type(self)(torch.matmul(self.raw, other.raw))\n\n    def __lt__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__lt__(unwrap1(other)))\n\n    def __le__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__le__(unwrap1(other)))\n\n    def __eq__(self: TensorType, other: TensorOrScalar) -> TensorType:  # type: ignore\n        return type(self)(self.raw.__eq__(unwrap1(other)))\n\n    def __ne__(self: TensorType, other: TensorOrScalar) -> TensorType:  # type: ignore\n        return type(self)(self.raw.__ne__(unwrap1(other)))\n\n    def __gt__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__gt__(unwrap1(other)))\n\n    def __ge__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__ge__(unwrap1(other)))\n\n    def __getitem__(self: TensorType, index: Any) -> TensorType:\n        if isinstance(index, tuple):\n            index = tuple(x.raw if isinstance(x, Tensor) else x for x in index)\n        elif isinstance(index, Tensor):\n            index = index.raw\n        return type(self)(self.raw[index])\n\n    def take_along_axis(self: TensorType, index: TensorType, axis: int) -> TensorType:\n        if axis % self.ndim != self.ndim - 1:\n            raise NotImplementedError(\n                f""take_along_axis is currently only supported for the last axis""\n            )\n        return type(self)(torch.gather(self.raw, axis, index.raw))\n\n    def bool(self: TensorType) -> TensorType:\n        return self.astype(torch.bool)\n'"
eagerpy/tensor/tensor.py,0,"b'from abc import ABCMeta, abstractmethod\nfrom typing import (\n    TypeVar,\n    Callable,\n    Tuple,\n    Any,\n    overload,\n    Iterable,\n    Iterator,\n    Union,\n    Optional,\n    Type,\n    TYPE_CHECKING,\n    cast,\n)\nfrom typing_extensions import Literal, final\n\nfrom ..types import Axes, AxisAxes, Shape, ShapeOrScalar\n\nif TYPE_CHECKING:\n    from .extensions import NormsMethods  # noqa: F401\n\n\nTensorType = TypeVar(""TensorType"", bound=""Tensor"")\n\n# using Tensor instead of TensorType because of a MyPy bug\n# https://github.com/python/mypy/issues/3644\nTensorOrScalar = Union[""Tensor"", int, float]\n\n\nclass LazyCachedAccessor:\n    # supports caching under a different name (because Tensor uses __slots__\n    # and thus we cannot override the LazyCachedAccessor class var intself)\n\n    # supports lazy extension loading to break cyclic dependencies\n\n    def __init__(self, cache_name: str, extension_name: str):\n        self._cache_name = cache_name\n        self._extension_name = extension_name\n\n    @property\n    def _extension(self) -> Any:  # Type[object]:\n        # only imported once needed to break cyclic dependencies\n        from . import extensions\n\n        return getattr(extensions, self._extension_name)\n\n    def __get__(\n        self, instance: Optional[""Tensor""], owner: Optional[Type[""Tensor""]] = None\n    ) -> Any:\n        if instance is None:\n            # accessed as a class attribute\n            return self._extension\n\n        methods = getattr(instance, self._cache_name, None)\n        if methods is not None:\n            return methods\n\n        # create the extension for this instance\n        methods = self._extension(instance)\n\n        # add it to the instance to avoid recreation\n        instance.__setattr__(self._cache_name, methods)\n        return methods\n\n\nclass Tensor(metaclass=ABCMeta):\n    """"""Base class defining the common interface of all EagerPy Tensors""""""\n\n    # each extension neeeds a slot to cache the instantiated extension\n    __slots__ = (""_norms"",)\n\n    __array_ufunc__ = None\n\n    # shorten the class name to eagerpy.Tensor (does not help with MyPy)\n    __module__ = ""eagerpy""\n\n    @abstractmethod\n    def __init__(self, raw: Any):\n        ...\n\n    @property\n    @abstractmethod\n    def raw(self) -> Any:\n        ...\n\n    @property\n    @abstractmethod\n    def dtype(self: TensorType) -> Any:\n        ...\n\n    @abstractmethod\n    def __repr__(self: TensorType) -> str:\n        ...\n\n    @abstractmethod\n    def __format__(self: TensorType, format_spec: str) -> str:\n        ...\n\n    @abstractmethod\n    def __getitem__(self: TensorType, index: Any) -> TensorType:\n        ...\n\n    @abstractmethod\n    def __bool__(self: TensorType) -> bool:\n        ...\n\n    @abstractmethod\n    def __len__(self: TensorType) -> int:\n        ...\n\n    @abstractmethod\n    def __abs__(self: TensorType) -> TensorType:\n        ...\n\n    @abstractmethod\n    def __neg__(self: TensorType) -> TensorType:\n        ...\n\n    @abstractmethod\n    def __add__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def __radd__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def __sub__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def __rsub__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def __mul__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def __rmul__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def __truediv__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def __rtruediv__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def __floordiv__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def __rfloordiv__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def __mod__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def __lt__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def __le__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def __eq__(self: TensorType, other: TensorOrScalar) -> TensorType:  # type: ignore\n        # we ignore the type errors caused by wrong type annotations for object\n        # https://github.com/python/typeshed/issues/3685\n        ...\n\n    @abstractmethod\n    def __ne__(self: TensorType, other: TensorOrScalar) -> TensorType:  # type: ignore\n        # we ignore the type errors caused by wrong type annotations for object\n        # https://github.com/python/typeshed/issues/3685\n        ...\n\n    @abstractmethod\n    def __gt__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def __ge__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def __pow__(self: TensorType, exponent: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def sign(self: TensorType) -> TensorType:\n        ...\n\n    @abstractmethod\n    def sqrt(self: TensorType) -> TensorType:\n        ...\n\n    @abstractmethod\n    def tanh(self: TensorType) -> TensorType:\n        ...\n\n    @abstractmethod\n    def float32(self: TensorType) -> TensorType:\n        ...\n\n    @abstractmethod\n    def where(self: TensorType, x: TensorOrScalar, y: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def matmul(self: TensorType, other: TensorType) -> TensorType:\n        ...\n\n    @property\n    @abstractmethod\n    def ndim(self: TensorType) -> int:\n        ...\n\n    @abstractmethod\n    def numpy(self: TensorType) -> Any:\n        ...\n\n    @abstractmethod\n    def item(self: TensorType) -> Union[int, float, bool]:\n        ...\n\n    @property\n    @abstractmethod\n    def shape(self: TensorType) -> Shape:\n        ...\n\n    @abstractmethod\n    def reshape(self: TensorType, shape: Union[Shape, int]) -> TensorType:\n        ...\n\n    @abstractmethod\n    def astype(self: TensorType, dtype: Any) -> TensorType:\n        ...\n\n    @abstractmethod\n    def clip(self: TensorType, min_: float, max_: float) -> TensorType:\n        ...\n\n    @abstractmethod\n    def square(self: TensorType) -> TensorType:\n        ...\n\n    @abstractmethod\n    def arctanh(self: TensorType) -> TensorType:\n        ...\n\n    @abstractmethod\n    def sum(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        ...\n\n    @abstractmethod\n    def mean(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        ...\n\n    @abstractmethod\n    def min(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        ...\n\n    @abstractmethod\n    def max(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        ...\n\n    @abstractmethod\n    def minimum(self: TensorType, other: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def maximum(self: TensorType, other: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def argmin(self: TensorType, axis: Optional[int] = None) -> TensorType:\n        ...\n\n    @abstractmethod\n    def argmax(self: TensorType, axis: Optional[int] = None) -> TensorType:\n        ...\n\n    @abstractmethod\n    def argsort(self: TensorType, axis: int = -1) -> TensorType:\n        ...\n\n    @abstractmethod\n    def sort(self: TensorType, axis: int = -1) -> TensorType:\n        ...\n\n    @abstractmethod\n    def uniform(\n        self: TensorType, shape: ShapeOrScalar, low: float = 0.0, high: float = 1.0\n    ) -> TensorType:\n        ...\n\n    @abstractmethod\n    def normal(\n        self: TensorType, shape: ShapeOrScalar, mean: float = 0.0, stddev: float = 1.0\n    ) -> TensorType:\n        ...\n\n    @abstractmethod\n    def ones(self: TensorType, shape: ShapeOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def zeros(self: TensorType, shape: ShapeOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def ones_like(self: TensorType) -> TensorType:\n        ...\n\n    @abstractmethod\n    def zeros_like(self: TensorType) -> TensorType:\n        ...\n\n    @abstractmethod\n    def full_like(self: TensorType, fill_value: float) -> TensorType:\n        ...\n\n    @abstractmethod\n    def onehot_like(\n        self: TensorType, indices: TensorType, *, value: float = 1\n    ) -> TensorType:\n        ...\n\n    @abstractmethod\n    def from_numpy(self: TensorType, a: Any) -> TensorType:\n        ...\n\n    @abstractmethod\n    def _concatenate(\n        self: TensorType, tensors: Iterable[TensorType], axis: int = 0\n    ) -> TensorType:\n        ...\n\n    @abstractmethod\n    def _stack(\n        self: TensorType, tensors: Iterable[TensorType], axis: int = 0\n    ) -> TensorType:\n        ...\n\n    @abstractmethod\n    def transpose(self: TensorType, axes: Optional[Axes] = None) -> TensorType:\n        ...\n\n    @abstractmethod\n    def take_along_axis(self: TensorType, index: TensorType, axis: int) -> TensorType:\n        ...\n\n    @abstractmethod\n    def all(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        ...\n\n    @abstractmethod\n    def any(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        ...\n\n    @abstractmethod\n    def logical_and(self: TensorType, other: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def logical_or(self: TensorType, other: TensorOrScalar) -> TensorType:\n        ...\n\n    @abstractmethod\n    def logical_not(self: TensorType) -> TensorType:\n        ...\n\n    @abstractmethod\n    def exp(self: TensorType) -> TensorType:\n        ...\n\n    @abstractmethod\n    def log(self: TensorType) -> TensorType:\n        ...\n\n    @abstractmethod\n    def log2(self: TensorType) -> TensorType:\n        ...\n\n    @abstractmethod\n    def log10(self: TensorType) -> TensorType:\n        ...\n\n    @abstractmethod\n    def log1p(self: TensorType) -> TensorType:\n        ...\n\n    @abstractmethod\n    def tile(self: TensorType, multiples: Axes) -> TensorType:\n        ...\n\n    @abstractmethod\n    def softmax(self: TensorType, axis: int = -1) -> TensorType:\n        ...\n\n    @abstractmethod\n    def log_softmax(self: TensorType, axis: int = -1) -> TensorType:\n        ...\n\n    @abstractmethod\n    def squeeze(self: TensorType, axis: Optional[AxisAxes] = None) -> TensorType:\n        ...\n\n    @abstractmethod\n    def expand_dims(self: TensorType, axis: int) -> TensorType:\n        ...\n\n    @abstractmethod\n    def full(self: TensorType, shape: ShapeOrScalar, value: float) -> TensorType:\n        ...\n\n    @abstractmethod\n    def index_update(\n        self: TensorType, indices: Any, values: TensorOrScalar\n    ) -> TensorType:\n        ...\n\n    @abstractmethod\n    def arange(\n        self: TensorType,\n        start: int,\n        stop: Optional[int] = None,\n        step: Optional[int] = None,\n    ) -> TensorType:\n        ...\n\n    @abstractmethod\n    def cumsum(self: TensorType, axis: Optional[int] = None) -> TensorType:\n        ...\n\n    @abstractmethod\n    def flip(self: TensorType, axis: Optional[AxisAxes] = None) -> TensorType:\n        ...\n\n    @abstractmethod\n    def meshgrid(\n        self: TensorType, *tensors: TensorType, indexing: str = ""xy""\n    ) -> Tuple[TensorType, ...]:\n        ...\n\n    @abstractmethod\n    def pad(\n        self: TensorType,\n        paddings: Tuple[Tuple[int, int], ...],\n        mode: str = ""constant"",\n        value: float = 0,\n    ) -> TensorType:\n        ...\n\n    @abstractmethod\n    def isnan(self: TensorType) -> TensorType:\n        ...\n\n    @abstractmethod\n    def isinf(self: TensorType) -> TensorType:\n        ...\n\n    @abstractmethod\n    def crossentropy(self: TensorType, labels: TensorType) -> TensorType:\n        ...\n\n    @overload\n    def _value_and_grad_fn(\n        self: TensorType, f: Callable[..., TensorType]\n    ) -> Callable[..., Tuple[TensorType, TensorType]]:\n        ...\n\n    @overload  # noqa: F811 (waiting for pyflakes > 2.1.1)\n    def _value_and_grad_fn(\n        self: TensorType, f: Callable[..., TensorType], has_aux: Literal[False]\n    ) -> Callable[..., Tuple[TensorType, TensorType]]:\n        ...\n\n    @overload  # noqa: F811 (waiting for pyflakes > 2.1.1)\n    def _value_and_grad_fn(\n        self: TensorType,\n        f: Callable[..., Tuple[TensorType, Any]],\n        has_aux: Literal[True],\n    ) -> Callable[..., Tuple[TensorType, Any, TensorType]]:\n        ...\n\n    @abstractmethod  # noqa: F811 (waiting for pyflakes > 2.1.1)\n    def _value_and_grad_fn(\n        self: TensorType, f: Callable, has_aux: bool = False\n    ) -> Callable[..., Tuple]:\n        ...\n\n    @abstractmethod\n    def bool(self: TensorType) -> TensorType:\n        ...\n\n    # #########################################################################\n    # aliases and shared implementations\n    # #########################################################################\n\n    @final\n    @property\n    def T(self: TensorType) -> TensorType:\n        return self.transpose()\n\n    @final\n    def abs(self: TensorType) -> TensorType:\n        return self.__abs__()\n\n    @final\n    def pow(self: TensorType, exponent: TensorOrScalar) -> TensorType:\n        return self.__pow__(exponent)\n\n    @final\n    def value_and_grad(\n        self: TensorType, f: Callable[..., TensorType], *args: Any, **kwargs: Any\n    ) -> Tuple[TensorType, TensorType]:\n        return self._value_and_grad_fn(f, has_aux=False)(self, *args, **kwargs)\n\n    @final\n    def value_aux_and_grad(\n        self: TensorType,\n        f: Callable[..., Tuple[TensorType, Any]],\n        *args: Any,\n        **kwargs: Any,\n    ) -> Tuple[TensorType, Any, TensorType]:\n        return self._value_and_grad_fn(f, has_aux=True)(self, *args, **kwargs)\n\n    def __iter__(self: TensorType) -> Iterator[TensorType]:\n        for i in range(len(self)):\n            yield self[i]\n\n    @final\n    def flatten(self: TensorType, start: int = 0, end: int = -1) -> TensorType:\n        start = start % self.ndim\n        end = end % self.ndim\n        shape = self.shape[:start] + (-1,) + self.shape[end + 1 :]\n        return self.reshape(shape)\n\n    # #########################################################################\n    # extensions\n    # #########################################################################\n\n    norms = cast(""NormsMethods[Tensor]"", LazyCachedAccessor(""_norms"", ""NormsMethods""))\n\n\ndef istensor(x: Any) -> bool:\n    return isinstance(x, Tensor)\n'"
eagerpy/tensor/tensorflow.py,3,"b'from typing import (\n    Tuple,\n    cast,\n    Union,\n    Any,\n    TypeVar,\n    TYPE_CHECKING,\n    Iterable,\n    Optional,\n    overload,\n    Callable,\n)\nfrom typing_extensions import Literal\nimport numpy as np\nfrom importlib import import_module\nimport functools\n\nfrom ..types import Axes, AxisAxes, Shape, ShapeOrScalar\n\nfrom .. import index\n\nfrom .tensor import Tensor\nfrom .tensor import TensorOrScalar\nfrom .tensor import TensorType\n\nfrom .base import BaseTensor\nfrom .base import unwrap_\nfrom .base import unwrap1\n\nif TYPE_CHECKING:\n    import tensorflow as tf  # for static analyzers\n    from .extensions import NormsMethods  # noqa: F401\nelse:\n    # lazy import in TensorFlowTensor\n    tf = None\n\nFuncType = Callable[..., Any]\nF = TypeVar(""F"", bound=FuncType)\n\n\ndef samedevice(f: F) -> F:\n    @functools.wraps(f)\n    def wrapper(self: ""TensorFlowTensor"", *args: Any, **kwargs: Any) -> Any:\n        with tf.device(self.raw.device):\n            return f(self, *args, **kwargs)\n\n    return cast(F, wrapper)\n\n\ndef common_dtype(f: F) -> F:\n    @functools.wraps(f)\n    def wrapper(self: ""TensorFlowTensor"", *args: Any, **kwargs: Any) -> Any:\n        dtypes = {self.dtype} | {arg.dtype for arg in args if isinstance(arg, Tensor)}\n        if len(dtypes) == 1:\n            # all dtypes are the same, nothing more to do\n            return f(self, *args, **kwargs)\n        numpy_dtypes = [np.dtype(dtype.name) for dtype in dtypes]\n        common = np.find_common_type(numpy_dtypes, [])\n        common = getattr(tf, common.name)\n        if self.dtype != common:\n            self = self.astype(common)\n        args = tuple(\n            arg.astype(common)\n            if isinstance(arg, Tensor) and arg.dtype != common\n            else arg\n            for arg in args\n        )\n        return f(self, *args, **kwargs)\n\n    return cast(F, wrapper)\n\n\ndef assert_bool(x: Any) -> None:\n    if not isinstance(x, Tensor):\n        return\n    if x.dtype != tf.bool:\n        raise ValueError(f""requires dtype bool, consider t.bool().all()"")\n\n\nclass TensorFlowTensor(BaseTensor):\n    __slots__ = ()\n\n    # more specific types for the extensions\n    norms: ""NormsMethods[TensorFlowTensor]""\n\n    def __init__(self, raw: ""tf.Tensor""):  # type: ignore\n        global tf\n        if tf is None:\n            tf = import_module(""tensorflow"")\n        super().__init__(raw)\n\n    @property\n    def raw(self) -> ""tf.Tensor"":  # type: ignore\n        return super().raw\n\n    def numpy(self: TensorType) -> Any:\n        a = self.raw.numpy()\n        if a.flags.writeable:\n            # without the check, we would attempt to set it on array\n            # scalars, and that would fail\n            a.flags.writeable = False\n        return a\n\n    def item(self: TensorType) -> Union[int, float, bool]:\n        return self.numpy().item()  # type: ignore\n\n    @property\n    def shape(self: TensorType) -> Shape:\n        return tuple(self.raw.shape.as_list())\n\n    def reshape(self: TensorType, shape: Union[Shape, int]) -> TensorType:\n        if isinstance(shape, int):\n            shape = (shape,)\n        return type(self)(tf.reshape(self.raw, shape))\n\n    def astype(self: TensorType, dtype: Any) -> TensorType:\n        return type(self)(tf.cast(self.raw, dtype))\n\n    def clip(self: TensorType, min_: float, max_: float) -> TensorType:\n        return type(self)(tf.clip_by_value(self.raw, min_, max_))\n\n    def square(self: TensorType) -> TensorType:\n        return type(self)(tf.square(self.raw))\n\n    def arctanh(self: TensorType) -> TensorType:\n        return type(self)(tf.atanh(self.raw))\n\n    def sum(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        if self.raw.dtype == tf.bool:\n            return self.astype(tf.int64).sum(axis=axis, keepdims=keepdims)\n        return type(self)(tf.reduce_sum(self.raw, axis=axis, keepdims=keepdims))\n\n    def mean(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        if self.raw.dtype not in [tf.float16, tf.float32, tf.float64]:\n            raise ValueError(\n                f""Can only calculate the mean of floating types. Got {self.raw.dtype} instead.""\n            )\n        return type(self)(tf.reduce_mean(self.raw, axis=axis, keepdims=keepdims))\n\n    def min(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        return type(self)(tf.reduce_min(self.raw, axis=axis, keepdims=keepdims))\n\n    def max(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        return type(self)(tf.reduce_max(self.raw, axis=axis, keepdims=keepdims))\n\n    def minimum(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(tf.minimum(self.raw, unwrap1(other)))\n\n    def maximum(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(tf.maximum(self.raw, unwrap1(other)))\n\n    def argmin(self: TensorType, axis: Optional[int] = None) -> TensorType:\n        return type(self)(tf.argmin(self.raw, axis=axis))\n\n    def argmax(self: TensorType, axis: Optional[int] = None) -> TensorType:\n        return type(self)(tf.argmax(self.raw, axis=axis))\n\n    def argsort(self: TensorType, axis: Optional[int] = -1) -> TensorType:\n        return type(self)(tf.argsort(self.raw, axis=axis))\n\n    def sort(self: TensorType, axis: Optional[int] = -1) -> TensorType:\n        return type(self)(tf.sort(self.raw, axis=axis))\n\n    @samedevice\n    def uniform(\n        self: TensorType, shape: ShapeOrScalar, low: float = 0.0, high: float = 1.0\n    ) -> TensorType:\n        if not isinstance(shape, Iterable):\n            shape = (shape,)\n        return type(self)(\n            tf.random.uniform(shape, minval=low, maxval=high, dtype=self.raw.dtype)\n        )\n\n    @samedevice\n    def normal(\n        self: TensorType, shape: ShapeOrScalar, mean: float = 0.0, stddev: float = 1.0\n    ) -> TensorType:\n        if not isinstance(shape, Iterable):\n            shape = (shape,)\n        return type(self)(\n            tf.random.normal(shape, mean=mean, stddev=stddev, dtype=self.raw.dtype)\n        )\n\n    @samedevice\n    def ones(self: TensorType, shape: ShapeOrScalar) -> TensorType:\n        return type(self)(tf.ones(shape, dtype=self.raw.dtype))\n\n    @samedevice\n    def zeros(self: TensorType, shape: ShapeOrScalar) -> TensorType:\n        return type(self)(tf.zeros(shape, dtype=self.raw.dtype))\n\n    def ones_like(self: TensorType) -> TensorType:\n        return type(self)(tf.ones_like(self.raw))\n\n    def zeros_like(self: TensorType) -> TensorType:\n        return type(self)(tf.zeros_like(self.raw))\n\n    def full_like(self: TensorType, fill_value: float) -> TensorType:\n        fill_value = tf.cast(fill_value, self.raw.dtype)\n        return type(self)(tf.fill(self.raw.shape, fill_value))\n\n    @samedevice\n    def onehot_like(\n        self: TensorType, indices: TensorType, *, value: float = 1\n    ) -> TensorType:\n        if self.ndim != 2:\n            raise ValueError(""onehot_like only supported for 2D tensors"")\n        if indices.ndim != 1:\n            raise ValueError(""onehot_like requires 1D indices"")\n        if len(indices) != len(self):\n            raise ValueError(""length of indices must match length of tensor"")\n        value = tf.cast(value, self.raw.dtype)\n        return type(self)(\n            tf.one_hot(\n                indices.raw,\n                depth=self.raw.shape[-1],\n                on_value=value,\n                dtype=self.raw.dtype,\n            )\n        )\n\n    @samedevice\n    def from_numpy(self: TensorType, a: Any) -> TensorType:\n        return type(self)(tf.convert_to_tensor(a))\n\n    def _concatenate(\n        self: TensorType, tensors: Iterable[TensorType], axis: int = 0\n    ) -> TensorType:\n        # concatenates only ""tensors"", but not ""self""\n        tensors_ = unwrap_(*tensors)\n        return type(self)(tf.concat(tensors_, axis=axis))\n\n    def _stack(\n        self: TensorType, tensors: Iterable[TensorType], axis: int = 0\n    ) -> TensorType:\n        # stacks only ""tensors"", but not ""self""\n        tensors_ = unwrap_(*tensors)\n        return type(self)(tf.stack(tensors_, axis=axis))\n\n    def transpose(self: TensorType, axes: Optional[Axes] = None) -> TensorType:\n        if axes is None:\n            axes = tuple(range(self.ndim - 1, -1, -1))\n        return type(self)(tf.transpose(self.raw, perm=axes))\n\n    def all(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        assert_bool(self)\n        return type(self)(tf.reduce_all(self.raw, axis=axis, keepdims=keepdims))\n\n    def any(\n        self: TensorType, axis: Optional[AxisAxes] = None, keepdims: bool = False\n    ) -> TensorType:\n        assert_bool(self)\n        return type(self)(tf.reduce_any(self.raw, axis=axis, keepdims=keepdims))\n\n    def logical_and(self: TensorType, other: TensorOrScalar) -> TensorType:\n        assert_bool(self)\n        assert_bool(other)\n        return type(self)(tf.logical_and(self.raw, unwrap1(other)))\n\n    def logical_or(self: TensorType, other: TensorOrScalar) -> TensorType:\n        assert_bool(self)\n        assert_bool(other)\n        return type(self)(tf.logical_or(self.raw, unwrap1(other)))\n\n    def logical_not(self: TensorType) -> TensorType:\n        assert_bool(self)\n        return type(self)(tf.logical_not(self.raw))\n\n    def exp(self: TensorType) -> TensorType:\n        return type(self)(tf.exp(self.raw))\n\n    def log(self: TensorType) -> TensorType:\n        return type(self)(tf.math.log(self.raw))\n\n    def log2(self: TensorType) -> TensorType:\n        return type(self)(tf.math.log(self.raw) / tf.math.log(2.0))\n\n    def log10(self: TensorType) -> TensorType:\n        return type(self)(tf.math.log(self.raw) / tf.math.log(10.0))\n\n    def log1p(self: TensorType) -> TensorType:\n        return type(self)(tf.math.log1p(self.raw))\n\n    def tile(self: TensorType, multiples: Axes) -> TensorType:\n        multiples = unwrap1(multiples)\n        if len(multiples) != self.ndim:\n            raise ValueError(""multiples requires one entry for each dimension"")\n        return type(self)(tf.tile(self.raw, multiples))\n\n    def softmax(self: TensorType, axis: int = -1) -> TensorType:\n        return type(self)(tf.nn.softmax(self.raw, axis=axis))\n\n    def log_softmax(self: TensorType, axis: int = -1) -> TensorType:\n        return type(self)(tf.nn.log_softmax(self.raw, axis=axis))\n\n    def squeeze(self: TensorType, axis: Optional[AxisAxes] = None) -> TensorType:\n        return type(self)(tf.squeeze(self.raw, axis=axis))\n\n    def expand_dims(self: TensorType, axis: int) -> TensorType:\n        return type(self)(tf.expand_dims(self.raw, axis=axis))\n\n    @samedevice\n    def full(self: TensorType, shape: ShapeOrScalar, value: float) -> TensorType:\n        if not isinstance(shape, Iterable):\n            shape = (shape,)\n        return type(self)(tf.fill(shape, value))\n\n    def index_update(\n        self: TensorType, indices: Any, values: TensorOrScalar\n    ) -> TensorType:\n        indices, values_ = unwrap_(indices, values)\n        del values\n        if isinstance(indices, tuple):\n            indices = unwrap_(*indices)\n\n        x = self.raw\n        if isinstance(indices, int):\n            if isinstance(values_, int) or isinstance(values_, float):\n                values_ = tf.fill(x.shape[-1:], values_)\n            return type(self)(\n                tf.tensor_scatter_nd_update(x, [[indices]], values_[None])\n            )\n        elif isinstance(indices, tuple) and any(\n            isinstance(idx, slice) for idx in indices\n        ):\n            if (\n                len(indices) == x.ndim == 2\n                and indices[0] == index[:]\n                and not isinstance(indices[1], slice)\n            ):\n                x = tf.transpose(x)\n                if isinstance(values_, int) or isinstance(values_, float):\n                    values_ = tf.fill(x.shape[-1:], values_)\n                result = tf.tensor_scatter_nd_update(x, [[indices[-1]]], values_[None])\n                return type(self)(tf.transpose(result))\n            else:\n                raise NotImplementedError  # pragma: no cover\n        elif isinstance(indices, tuple):\n            if all(idx.dtype in [tf.int32, tf.int64] for idx in indices):\n                indices = [\n                    tf.cast(idx, tf.int64) if idx.dtype == tf.int32 else idx\n                    for idx in indices\n                ]\n            indices = tf.stack(indices, axis=-1)\n            if isinstance(values_, int) or isinstance(values_, float):\n                values_ = tf.fill((indices.shape[0],), values_)\n            return type(self)(tf.tensor_scatter_nd_update(x, indices, values_))\n        else:\n            raise ValueError  # pragma: no cover\n\n    @samedevice\n    def arange(\n        self: TensorType,\n        start: int,\n        stop: Optional[int] = None,\n        step: Optional[int] = None,\n    ) -> TensorType:\n        if step is None:\n            step = 1\n        if stop is None:\n            stop = start\n            start = 0\n        return type(self)(tf.range(start, stop, step))\n\n    def cumsum(self: TensorType, axis: Optional[int] = None) -> TensorType:\n        if axis is None:\n            x = tf.reshape(self.raw, (-1,))\n            return type(self)(tf.cumsum(x, axis=0))\n        return type(self)(tf.cumsum(self.raw, axis=axis))\n\n    def flip(self: TensorType, axis: Optional[AxisAxes] = None) -> TensorType:\n        if axis is None:\n            axis = tuple(range(self.ndim))\n        if not isinstance(axis, Iterable):\n            axis = (axis,)\n        return type(self)(tf.reverse(self.raw, axis=axis))\n\n    def meshgrid(\n        self: TensorType, *tensors: TensorType, indexing: str = ""xy""\n    ) -> Tuple[TensorType, ...]:\n        tensors = unwrap_(*tensors)\n        outputs = tf.meshgrid(self.raw, *tensors, indexing=indexing)\n        return tuple(type(self)(out) for out in outputs)\n\n    def pad(\n        self: TensorType,\n        paddings: Tuple[Tuple[int, int], ...],\n        mode: str = ""constant"",\n        value: float = 0,\n    ) -> TensorType:\n        if len(paddings) != self.ndim:\n            raise ValueError(""pad requires a tuple for each dimension"")\n        for p in paddings:\n            if len(p) != 2:\n                raise ValueError(""pad requires a tuple for each dimension"")\n        if not (mode == ""constant"" or mode == ""reflect""):\n            raise ValueError(""pad requires mode \'constant\' or \'reflect\'"")\n        if mode == ""reflect"":\n            # PyTorch\'s pad has limited support for \'reflect\' padding\n            if self.ndim != 3 and self.ndim != 4:\n                raise NotImplementedError  # pragma: no cover\n            k = self.ndim - 2\n            if paddings[:k] != ((0, 0),) * k:\n                raise NotImplementedError  # pragma: no cover\n        return type(self)(tf.pad(self.raw, paddings, mode=mode, constant_values=value))\n\n    def isnan(self: TensorType) -> TensorType:\n        return type(self)(tf.math.is_nan(self.raw))\n\n    def isinf(self: TensorType) -> TensorType:\n        return type(self)(tf.math.is_inf(self.raw))\n\n    def crossentropy(self: TensorType, labels: TensorType) -> TensorType:\n        if self.ndim != 2:\n            raise ValueError(""crossentropy only supported for 2D logits tensors"")\n        if self.shape[:1] != labels.shape:\n            raise ValueError(""labels must be 1D and must match the length of logits"")\n        return type(self)(\n            tf.nn.sparse_softmax_cross_entropy_with_logits(labels.raw, self.raw)\n        )\n\n    @overload\n    def _value_and_grad_fn(\n        self: TensorType, f: Callable[..., TensorType]\n    ) -> Callable[..., Tuple[TensorType, TensorType]]:\n        ...\n\n    @overload  # noqa: F811 (waiting for pyflakes > 2.1.1)\n    def _value_and_grad_fn(\n        self: TensorType, f: Callable[..., TensorType], has_aux: Literal[False]\n    ) -> Callable[..., Tuple[TensorType, TensorType]]:\n        ...\n\n    @overload  # noqa: F811 (waiting for pyflakes > 2.1.1)\n    def _value_and_grad_fn(\n        self: TensorType,\n        f: Callable[..., Tuple[TensorType, Any]],\n        has_aux: Literal[True],\n    ) -> Callable[..., Tuple[TensorType, Any, TensorType]]:\n        ...\n\n    def _value_and_grad_fn(  # noqa: F811 (waiting for pyflakes > 2.1.1)\n        self: TensorType, f: Callable, has_aux: bool = False\n    ) -> Callable[..., Tuple]:\n        def value_and_grad(x: TensorType, *args: Any, **kwargs: Any) -> Tuple:\n            # using tf.identity to make x independent from possible other instances of x in args\n            x_ = TensorFlowTensor(tf.identity(x.raw))\n            del x\n            with tf.GradientTape() as tape:\n                tape.watch(x_.raw)\n                if has_aux:\n                    loss, aux = f(x_, *args, **kwargs)\n                else:\n                    loss = f(x_, *args, **kwargs)\n            grad = tape.gradient(loss.raw, x_.raw)\n            grad = TensorFlowTensor(grad)\n            assert grad.shape == x_.shape\n            if has_aux:\n                return loss, aux, grad\n            else:\n                return loss, grad\n\n        return value_and_grad\n\n    def sign(self: TensorType) -> TensorType:\n        return type(self)(tf.sign(self.raw))\n\n    def sqrt(self: TensorType) -> TensorType:\n        return type(self)(tf.sqrt(self.raw))\n\n    def tanh(self: TensorType) -> TensorType:\n        return type(self)(tf.tanh(self.raw))\n\n    def float32(self: TensorType) -> TensorType:\n        return self.astype(tf.float32)\n\n    def where(self: TensorType, x: TensorOrScalar, y: TensorOrScalar) -> TensorType:\n        x, y = unwrap_(x, y)\n        return type(self)(tf.where(self.raw, x, y))\n\n    def matmul(self: TensorType, other: TensorType) -> TensorType:\n        if self.ndim != 2 or other.ndim != 2:\n            raise ValueError(\n                f""matmul requires both tensors to be 2D, got {self.ndim}D and {other.ndim}D""\n            )\n        return type(self)(tf.matmul(self.raw, other.raw))\n\n    @common_dtype\n    def __lt__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__lt__(unwrap1(other)))\n\n    @common_dtype\n    def __le__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__le__(unwrap1(other)))\n\n    @common_dtype\n    def __eq__(self: TensorType, other: TensorOrScalar) -> TensorType:  # type: ignore\n        return type(self)(self.raw.__eq__(unwrap1(other)))\n\n    @common_dtype\n    def __ne__(self: TensorType, other: TensorOrScalar) -> TensorType:  # type: ignore\n        return type(self)(self.raw.__ne__(unwrap1(other)))\n\n    @common_dtype\n    def __gt__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__gt__(unwrap1(other)))\n\n    @common_dtype\n    def __ge__(self: TensorType, other: TensorOrScalar) -> TensorType:\n        return type(self)(self.raw.__ge__(unwrap1(other)))\n\n    def __getitem__(self: TensorType, index: Any) -> TensorType:\n        if isinstance(index, tuple):\n            index = tuple(x.raw if isinstance(x, Tensor) else x for x in index)\n            basic = all(\n                x is None or x is Ellipsis or isinstance(x, int) or isinstance(x, slice)\n                for x in index\n            )\n            if not basic:\n                # workaround for missing support for this in TensorFlow\n                # TODO: maybe convert each index individually and then stack them instead\n                index = tf.convert_to_tensor(index)\n                index = tf.transpose(index)\n                return type(self)(tf.gather_nd(self.raw, index))\n        elif (\n            isinstance(index, range)\n            or isinstance(index, list)\n            or isinstance(index, np.ndarray)\n        ):\n            return type(self)(tf.gather(self.raw, index))\n        elif isinstance(index, Tensor):\n            if index.raw.dtype == tf.bool:\n                return type(self)(self.raw.__getitem__(index.raw))\n            else:\n                return type(self)(tf.gather(self.raw, index.raw))\n        return type(self)(self.raw.__getitem__(index))\n\n    def take_along_axis(self: TensorType, index: TensorType, axis: int) -> TensorType:\n        axis = batch_dims = axis % self.ndim\n        if axis != self.ndim - 1:\n            raise NotImplementedError(\n                f""take_along_axis is currently only supported for the last axis""\n            )\n        return type(self)(\n            tf.gather(self.raw, index.raw, axis=axis, batch_dims=batch_dims)\n        )\n\n    def bool(self: TensorType) -> TensorType:\n        return self.astype(tf.bool)\n'"
