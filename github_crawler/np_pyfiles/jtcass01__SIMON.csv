file_path,api_count,code
1.1/src/classifier/SIMON.py,17,"b'from prediction_model import PredictionModel\nfrom deep_neural_network import DeepNeuralNetwork\nfrom data_utilities import load_practice_dataset\n\nimport numpy as np\nimport scipy\nfrom scipy import ndimage\n\nclass SIMON(object):\n    def __init__(self):\n        self.prediction_model = self.load_model()\n        print(""Hello, my name is SIMON :-).  I am a Sign Integrated Machine Operating Network.  "")\n\n    def predict_image(self, image_location):\n        image = np.array(ndimage.imread(image_location, flatten = False))\n        X = scipy.misc.imresize(image, size=(64,64)).reshape((1, 64*64*3)).T\n\n        return self.prediction_model.predict(X)\n\n\n    def load_model(self, model=""dnn_best""):\n        parameters = {\n            \'W1\' : np.load(\'../../models/\' + model + \'/paramW1.npy\'),\n            \'b1\' : np.load(\'../../models/\' + model + \'/paramb1.npy\'),\n            \'W2\' : np.load(\'../../models/\' + model + \'/paramW2.npy\'),\n            \'b2\' : np.load(\'../../models/\' + model + \'/paramb2.npy\'),\n            \'W3\' : np.load(\'../../models/\' + model + \'/paramW3.npy\'),\n            \'b3\' : np.load(\'../../models/\' + model + \'/paramb3.npy\')\n        }\n\n        accuracies = {\n            \'train_accuracy\' : np.load(\'../../models/\' + model + \'/trainaccuracy.npy\'),\n            \'test_accuracy\' : np.load(\'../../models/\' + model + \'/testaccuracy.npy\')\n        }\n\n        return PredictionModel(parameters, accuracies)\n\n\n    def improve_prediction_model(self, epochs = 5):\n        # Load Data Set\n        print(""Loading data set."")\n\n        X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_practice_dataset()\n\n        # Show some images (optional of course : ) )\n    #    show_some_images(X_train_orig, X_test_orig)\n\n        test_model = DeepNeuralNetwork(X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes)\n\n        for i in range(epochs):\n            parameters, accuracies = test_model.train(num_epochs = epochs, print_cost = True)\n            new_model = PredictionModel(parameters, accuracies)\n\n            if  new_model > self:\n                print(""\\n\\tNew model is better... Displaying accuracies and updating files.. "")\n                self = new_model\n                print(self)\n                self.save_model()\n            else:\n                print(""Previous model is superior or equivalent."")\n\n        print(self)\n\n\n    def save_model(self):\n        parameters = self.parameters\n        accuracies = self.accuracies\n\n        W1 = parameters[\'W1\']\n        np.save(\'../../prior_best/paramW1.npy\',W1)\n\n        b1 = parameters[\'b1\']\n        np.save(\'../../prior_best/paramb1.npy\',b1)\n\n        W2 = parameters[\'W2\']\n        np.save(\'../../prior_best/paramW2.npy\',W2)\n\n        b2 = parameters[\'b2\']\n        np.save(\'../../prior_best/paramb2.npy\',b2)\n\n        W3 = parameters[\'W3\']\n        np.save(\'../../prior_best/paramW3.npy\',W3)\n\n        b3 = parameters[\'b3\']\n        np.save(\'../../prior_best/paramb3.npy\',b3)\n\n        train_accuracy = accuracies[\'train_accuracy\']\n        np.save(\'../../prior_best/trainaccuracy.npy\',train_accuracy)\n\n        test_accuracy = accuracies[\'test_accuracy\']\n        np.save(\'../../prior_best/testaccuracy.npy\',test_accuracy)\n'"
1.1/src/classifier/data_utilities.py,10,"b'import h5py\nimport math\n\nimport numpy as np\n\ndef load_practice_dataset():\n    """"""\n    Created By: Jacob Taylor Cassady\n    Last Updated: 2/7/2018\n    Objective: Load in practice data from example tensorflow model.\n    \n    Arguments: \n    None\n    \n    Returns: \n    train_set_x_orig -- A NumPy array of (currently) 1080 training images of shape (64,64,3).  Total nparray shape of (1080,64,64,3)\n    train_set_y_orig -- A NumPy array of (currently) 1080 training targets.  Total nparray shape of (1, 1080) [After reshape]\n    test_set_x_orig -- A NumPy array of (currently) 120 test images of shape (64,64,3).  Total nparray shape of (120,64,64,3)\n    test_set_y_orig -- A NumPy array of (currently) 120 test targets.  Total nparray shape of (1,120) [After reshape]\n    classes -- A NumPy array of (currently) 6 classes. (0-5)\n    """"""\n\n    train_dataset = h5py.File(\'../../Practice_Data/train_signs.h5\', \'r\')\n    train_set_x_orig = np.array(train_dataset[""train_set_x""][:])\n    train_set_y_orig = np.array(train_dataset[""train_set_y""][:])\n\n    test_dataset = h5py.File(\'../../Practice_Data/test_signs.h5\', \'r\')\n    test_set_x_orig = np.array(test_dataset[""test_set_x""][:])\n    test_set_y_orig = np.array(test_dataset[""test_set_y""][:])\n\n    classes = np.array(test_dataset[\'list_classes\'][:])\n\n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n\n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n\n\ndef convert_to_one_hot(Y, C):\n    Y = np.eye(C)[Y.reshape(-1)].T\n    return Y\n\ndef random_mini_batches(X, Y, mini_batch_size = 64):\n    """"""\n    Created By: Jacob Taylor Cassady\n    Last Updated: 2/8/2018\n    Objective: Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true ""label"" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n    mini_batch_size - size of the mini-batches, integer\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    """"""\n    \n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n    \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches\n\n\ndef predict(X, parameters):\n    \n    W1 = tf.convert_to_tensor(parameters[""W1""])\n    b1 = tf.convert_to_tensor(parameters[""b1""])\n    W2 = tf.convert_to_tensor(parameters[""W2""])\n    b2 = tf.convert_to_tensor(parameters[""b2""])\n    W3 = tf.convert_to_tensor(parameters[""W3""])\n    b3 = tf.convert_to_tensor(parameters[""b3""])\n    \n    params = {""W1"": W1,\n              ""b1"": b1,\n              ""W2"": W2,\n              ""b2"": b2,\n              ""W3"": W3,\n              ""b3"": b3}\n    \n    x = tf.placeholder(""float"", [12288, 1])\n    \n    z3 = forward_propagation_for_predict(x, params)\n    p = tf.argmax(z3)\n    \n    sess = tf.Session()\n    prediction = sess.run(p, feed_dict = {x: X})\n        \n    return prediction\n\ndef forward_propagation_for_predict(X, parameters):\n    """"""\n    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n    \n    Arguments:\n    X -- input dataset placeholder, of shape (input size, number of examples)\n    parameters -- python dictionary containing your parameters ""W1"", ""b1"", ""W2"", ""b2"", ""W3"", ""b3""\n                  the shapes are given in initialize_parameters\n    Returns:\n    Z3 -- the output of the last LINEAR unit\n    """"""\n    \n    # Retrieve the parameters from the dictionary ""parameters"" \n    W1 = parameters[\'W1\']\n    b1 = parameters[\'b1\']\n    W2 = parameters[\'W2\']\n    b2 = parameters[\'b2\']\n    W3 = parameters[\'W3\']\n    b3 = parameters[\'b3\'] \n                                                           # Numpy Equivalents:\n    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n    \n    return Z3'"
1.1/src/classifier/deep_neural_network.py,4,"b'import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.python.framework import ops\n\nfrom data_utilities import convert_to_one_hot, random_mini_batches\n\nclass DeepNeuralNetwork(object):\n\tdef __init__(self, X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes):\n\t\ttf.reset_default_graph()\n\t\tself.train_matrix, self.train_targets, self.test_matrix, self.test_targets = self.flatten_data(X_train_orig, Y_train_orig, X_test_orig, Y_test_orig)\n\t\tself.classes = classes.reshape((classes.shape[0],1))\n\t\tself.parameters = self.initialize_parameters()\n\n\tdef print_dataset_shapes(self):\n\t\tprint(""===== Printing dataset shapes ====="")\n\t\tprint(self.train_matrix.shape, ""train_parameter_matrix"")\n\t\tprint(self.train_targets.shape, ""train_targets"")\n\t\tprint(self.test_matrix.shape, ""test_parameter_matrix"")\n\t\tprint(self.test_targets.shape, ""test_targets"")\n\t\tprint(self.classes.shape, ""classes"")\n\n\tdef train(self, learning_rate = 0.0001, num_epochs = 1500, minibatch_size = 32, print_cost = True):\n\t\t""""""\n\t\tCreated By: Jacob Taylor Cassady\n\t\tLast Updated: 2/7/2018\n\t    Objective: Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n\n\t\tArguments:\n\t\tlearning_rate -- learning rate of the optimization\n\t\tnum_epochs -- number of epochs of the optimization loop\n\t\tminibatch_size -- size of a minibatch\n\t\tprint_cost -- True to print the cost every 100 epochs\t\t\n\n\t\tReturns:\n\t\tparameters -- parameters learnt by the model. They can then be used to predict.\n\t\t""""""\n\t\tprint(""Entering train...."")\n\t\tops.reset_default_graph()\n\t\t(n_x, m) = self.train_matrix.shape    # (n_x: input size, m : number of examples in the train set)\n\t\tn_y = self.train_targets.shape[0]\t\t  # n_y : output size.\n\t\tcosts = []\t\t\t\t\t\t\t  # Used to keep track of varying costs.\n\n\t\t# Create placeholders for TensorFlow graph of shape (n_x, n_y)\n\t\tprint(""Creating placeholders for TensorFlow graph..."")\n\t\tX, Y = self.create_placeholders(n_x, n_y)\n\t\tprint(""Complete.\\n"")\n\n\t\t# Initialize Parameters\n\t\tprint(""Initailizing parameters for TensorFlow graph..."")\n\t\tparameters = self.initialize_parameters()\n\t\tprint(""Complete.\\n"")\n\n\t\t# Build the forward propagation in the TensorFlow Graph\n\t\tprint(""Building the forward propagation in the TensorFlow Graph..."")\n\t\tZ3 = self.forward_propagation(X, parameters)\n\t\tprint(""Complete.\\n"")\n\n\t\t# Add the cost function to the Tensorflow Graph\n\t\tprint(""Adding cost function to the TensorFlow Graph"")\n\t\tcost = self.compute_cost(Z3, Y)\n\t\tprint(""Complete.\\n"")\n\n\t\t# Define the TensorFlow Optimizer.. We are using an AdamOptimizer.\n\t\toptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n\t\t# Initialize all the variables with our newly made TensorFlow Graph\n\t\tinit = tf.global_variables_initializer()\n\n\t\t# Use the TensorFlow Graph to train the parameters.\n\t\twith tf.Session() as session:\n\t\t\t# Run the initialization\n\t\t\tsession.run(init)\n\n\t\t\t# Perform Training\n\t\t\tfor epoch in range(num_epochs):\n\t\t\t\tepoch_cost = 0.\t\t\t\t\t\t\t\t# Defines a cost related to the current epoch\n\t\t\t\tnum_minibatches = int(m / minibatch_size)\t# Calculates the number of minibatches in the trainset given a minibatch size\n\t\t\t\tminibatches = random_mini_batches(self.train_matrix, self.train_targets, minibatch_size)\n\n\t\t\t\tfor minibatch in minibatches:\n\t\t\t\t\t# Retrieve train_matrix and train_targets from minibatch\n\t\t\t\t\tmini_matrix, mini_targets = minibatch\n\n\t\t\t\t\t# Run the session to execute the ""optimizer"" and the ""cost"",\n\t\t\t\t\t_, minibatch_cost = session.run([optimizer, cost], feed_dict={X:mini_matrix, Y:mini_targets})\n\n\t\t\t\t\t# Sum epoch cost\n\t\t\t\t\tepoch_cost += minibatch_cost / num_minibatches\n\n\t\t\t\t# Done training.  Print the cost of every 100 epochs\n\t\t\t\tif print_cost == True and epoch % 100 == 0:\n\t\t\t\t\tprint(""Cost after epoch %i: %f"" % (epoch, epoch_cost))\n\t\t\t\t# Keep track of the cost of every 5 epochs for plotting later\n\t\t\t\tif print_cost == True and epoch % 5 == 0:\n\t\t\t\t\tcosts.append(epoch_cost)\n\n\t\t\t# Plot the costs for analysis\n\t\t\tplt.plot(np.squeeze(costs))\n\t\t\tplt.ylabel(\'cost\')\n\t\t\tplt.xlabel(\'iteration ( per 5 )\')\n\t\t\tplt.title(\'Learning rate = \' + str(learning_rate))\n\t\t\tif print_cost == True:\n\t\t\t\t#plt.show()\n\t\t\t\tpass\n\n\t\t\t# Save the parameters as a varaible for prediction and evaluation of fit to test set.\n\t\t\tparameters = session.run(parameters)\n\n\t\t\t# Develop TensorFlow prediction standards for testing accuracy  of test and train sets\n\t\t\tcorrect_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n\n\t\t\t# Develop accuracy identifier using TensorFlow\n\t\t\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, \'float\'))\n\n\t\t\t# Display accuracy of train and test predictions.\n\t\t\tprint(""Train Accuracy: "", accuracy.eval({X: self.train_matrix, Y: self.train_targets}))\n\t\t\tprint(""Test Accuracy: "", accuracy.eval({X: self.test_matrix, Y: self.test_targets}))\n\n\t\t\t# Return parameters for prediction against the model.\n\t\t\tself.parameters = parameters\n\n\t\t\ttrain_accuracy = accuracy.eval({X: self.train_matrix, Y: self.train_targets})\n\t\t\ttest_accuracy = accuracy.eval({X: self.test_matrix, Y: self.test_targets})\n\n\t\t\taccuracies = {""train_accuracy"": train_accuracy,\n                          ""test_accuracy"" : test_accuracy}\n\n\t\t\t# Return parameters for prediction against the model.\n\t\t\treturn parameters, accuracies\n\n\tdef flatten_data(self, X_train_orig, Y_train_orig, X_test_orig, Y_test_orig):\n\t\t""""""\n\t\tCreated By: Jacob Taylor Cassady\n\t\tLast Updated: 2/7/2018\n\t\tObjective: Load in practice data from example tensorflow model.\n    \n\t\tArguments: \n\t\ttrain_set_x_orig -- A NumPy array of (currently) 1080 training images of shape (64,64,3).  Total nparray shape of (1080,64,64,3)\n\t\ttrain_set_y_orig -- A NumPy array of (currently) 1080 training targets.  Total nparray shape of (1, 1080) [After reshape]\n\t\ttest_set_x_orig -- A NumPy array of (currently) 120 test images of shape (64,64,3).  Total nparray shape of (120,64,64,3)\n\t\ttest_set_y_orig -- A NumPy array of (currently) 120 test targets.  Total nparray shape of (1,120) [After reshape]\n    \n\t\tReturns: \n\t\tX_train -- A NumPy array of training data.  [Practice shape = (12288, 1080)]\n\t\tY_train -- A NumPy array of training targets.  [Practice shape = (6, 1080)]\n\t\tX_train -- A NumPy array of test data.  [Practice shape = (12288, 120)]\n\t\tY_train -- A NumPy array of test targets.  [Practice shape = (6, 120)]\n\t\t""""""\n\t\t# Flatten the training and test images\n\t\tX_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n\t\tX_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n\t\t# Normalize image vectors\n\t\tX_train = X_train_flatten/255.\n\t\tX_test = X_test_flatten/255.\n\t\t# Convert training and test labels to one hot matrices\n\t\tY_train = convert_to_one_hot(Y_train_orig, 6)\n\t\tY_test = convert_to_one_hot(Y_test_orig, 6)\n\n\n\t\treturn X_train, Y_train, X_test, Y_test\n\n\tdef create_placeholders(self, n_x, n_y):\n\t\t""""""\n\t\tCreated By: Jacob Taylor Cassady\n\t\tLast Updated: 2/8/2018\n\t\tObjective: Creates the placeholders for the tensorflow session.  These are used in the Tensorflow Graph.\n    \n\t\tArguments:\n\t\tn_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n\t\tn_y -- scalar, number of classes (from 0 to 5, so -> 6)\n    \n\t\tReturns:\n\t\tX -- placeholder for the data input, of shape [n_x, None] and dtype ""float""\n\t\tY -- placeholder for the input labels, of shape [n_y, None] and dtype ""float""\n    \n\t\tNote:\n\t\t- We will use None because it let\'s us be flexible on the number of examples you will for the placeholders.\n\t\t\tIn fact, the number of examples during test/train is different.\n\t\t""""""\n\n\t\tX = tf.placeholder(shape=[n_x, None], dtype=tf.float32, name=\'X\')\n\t\tY = tf.placeholder(shape=[n_y, None], dtype=tf.float32, name=\'Y\')\n\n\t\treturn X, Y\n\n\tdef initialize_parameters(self, N1 = 25, N2 = 12):\n\t\t""""""\n\t\tInitializes parameters to build a neural network with tensorflow. The shapes are:\n\t\t\t\t\t\t\tW1 : [N1, X_train.shape[0]]\n\t\t\t\t\t\t\tb1 : [N1, 1]\n\t\t\t\t\t\t\tW2 : [N2, N1]\n\t\t\t\t\t\t\tb2 : [N2, 1]\n\t\t\t\t\t\t\tW3 : [classes.shape[0], N2]\n\t\t\t\t\t\t\tb3 : [classes.shape[0], 1]\n    \n\t\tReturns:\n\t\tparameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n\t\t""""""\n\t\tW1 = tf.get_variable(\'W1\', [N1, self.train_matrix.shape[0]], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n\t\tb1 = tf.get_variable(\'b1\', [N1, 1], initializer = tf.zeros_initializer())\n\t\tW2 = tf.get_variable(\'W2\', [N2,N1], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n\t\tb2 = tf.get_variable(\'b2\', [N2, 1], initializer = tf.zeros_initializer())\n\t\tW3 = tf.get_variable(\'W3\', [self.classes.shape[0],N2], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n\t\tb3 = tf.get_variable(\'b3\', [self.classes.shape[0], 1], initializer = tf.zeros_initializer())\n\n\t\tparameters = {\n\t\t\t""W1"" : W1,\n\t\t\t""b1"" : b1,\n\t\t\t""W2"" : W2,\n\t\t\t""b2"" : b2,\n\t\t\t""W3"" : W3,\n\t\t\t""b3"" : b3,\n\t\t\t}\n\n\t\treturn parameters\n\n\tdef forward_propagation(self, input_matrix, parameters):\n\t\t""""""\n\t\tImplements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n    \n\t\tArguments:\n\t\tinput_matrix -- input dataset placeholder, of shape (input size, number of examples)\n\t\tparameters -- python dictionary containing your parameters ""W1"", ""b1"", ""W2"", ""b2"", ""W3"", ""b3""\n\t\t\t\t\t  the shapes are given in initialize_parameters\n\n\t\tReturns:\n\t\tZ3 -- the output of the last LINEAR unit\n\t\t""""""\n\t\t# Retrieve the parameters from the dictionary ""parameters"" \n\t\tW1 = parameters[\'W1\']\n\t\tb1 = parameters[\'b1\']\n\t\tW2 = parameters[\'W2\']\n\t\tb2 = parameters[\'b2\']\n\t\tW3 = parameters[\'W3\']\n\t\tb3 = parameters[\'b3\']\n\n\t\tZ1 = tf.matmul(W1, input_matrix) + b1                            # Z1 = np.dot(W1, X) + b1\n\t\tA1 = tf.nn.relu(Z1)                                              # A1 = relu(Z1)\n\t\tZ2 = tf.matmul(W2, A1) + b2                                      # Z2 = np.dot(W2, a1) + b2\n\t\tA2 = tf.nn.relu(Z2)                                              # A2 = relu(Z2)\n\t\tZ3 = tf.matmul(W3, A2) + b3                                      # Z3 = np.dot(W3,A2) + b3\n\n\t\treturn Z3\n\n\tdef compute_cost(self, Z3, targets):\n\t\t""""""\n\t\tComputes the cost\n\t\n\t\tArguments:\n\t\tZ3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n\t\tY -- ""true"" labels vector placeholder, same shape as Z3\n\t\t\tReturns:\n\t\tcost - Tensor of the cost function\n\t\t""""""\n\t\n\t\t# to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n\t\tlogits = tf.transpose(Z3)\n\t\tlabels = tf.transpose(targets)\n\t\n\t\tcost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n\n\t\treturn cost'"
1.1/src/classifier/main.py,0,"b'from SIMON import SIMON\nfrom tkinter.filedialog import askopenfilename\nimport serial\n\n"""""" Initialize objects. """"""\nsimon = SIMON()\n#com_port = input(\'Where is the machine I am controlling?\\n\')\n#ser = serial.Serial(com_port, 9600)\n\n\ndef menu_prompt():\n    user_response = input(""\\n\\nWhat would you like to do?\\n\\t1) Improve prediction model.\\n\\t2) Apply the prediction model to an image.\\n\\t0) exit.\\n\\t"")\n    menu(user_response)\n\ndef menu(user_response):\n    if user_response == \'1\':\n        epochs = int(input(\'How many improvement attempts would you like to make?\\n\'))\n        simon.improve_prediction_model(epochs = epochs)\n    elif user_response == \'2\':\n        prediction = simon.predict_image(image_location = askopenfilename())\n        print(""SIMON guessed "", prediction)\n#        write_response(prediction)\n    elif user_response == \'0\':\n#        ser.close()\n        exit()\n    else:\n        print(\'Invalid response please try again.\')\n\ndef write_response(integer_value):\n    if integer_value == 0:\n        ser.write(b\'0\')\n    if integer_value == 1:\n        ser.write(b\'1\')\n    if integer_value == 2:\n        ser.write(b\'2\')\n    if integer_value == 3:\n        ser.write(b\'3\')\n    if integer_value == 4:\n        ser.write(b\'4\')\n    if integer_value == 5:\n        ser.write(b\'5\')\n\n\nif __name__ == \'__main__\':\n    while(1):\n        menu_prompt()\n'"
1.1/src/classifier/prediction_model.py,3,"b'import tensorflow as tf\n\nclass PredictionModel(object):\n    def __init__(self, parameters, accuracies):\n        self.parameters = parameters\n        self.accuracies = accuracies\n\n    def predict(self, X):\n        W1 = tf.convert_to_tensor(self.parameters[""W1""])\n        b1 = tf.convert_to_tensor(self.parameters[""b1""])\n        W2 = tf.convert_to_tensor(self.parameters[""W2""])\n        b2 = tf.convert_to_tensor(self.parameters[""b2""])\n        W3 = tf.convert_to_tensor(self.parameters[""W3""])\n        b3 = tf.convert_to_tensor(self.parameters[""b3""])\n    \n        params = {""W1"": W1,\n                  ""b1"": b1,\n                  ""W2"": W2,\n                  ""b2"": b2,\n                  ""W3"": W3,\n                  ""b3"": b3}\n    \n        x = tf.placeholder(""float"", [12288, 1])\n    \n        z3 = self.forward_propagation_for_predict(x)\n        p = tf.argmax(z3)\n    \n        sess = tf.Session()\n        prediction = sess.run(p, feed_dict = {x: X})\n        \n        return prediction\n\n    def forward_propagation_for_predict(self, X):\n        """"""\n        Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n    \n        Arguments:\n        X -- input dataset placeholder, of shape (input size, number of examples)\n        parameters -- python dictionary containing your parameters ""W1"", ""b1"", ""W2"", ""b2"", ""W3"", ""b3""\n                      the shapes are given in initialize_parameters\n        Returns:\n        Z3 -- the output of the last LINEAR unit\n        """"""\n    \n        # Retrieve the parameters from the dictionary ""parameters"" \n        W1 = self.parameters[\'W1\']\n        b1 = self.parameters[\'b1\']\n        W2 = self.parameters[\'W2\']\n        b2 = self.parameters[\'b2\']\n        W3 = self.parameters[\'W3\']\n        b3 = self.parameters[\'b3\'] \n                                                               # Numpy Equivalents:\n        Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n        A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n        Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n        A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n        Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n    \n        return Z3\n\n    """""" Overloading of string operator """"""\n    def __str__(self):\n        return ""\\t === CURRENT PREDICTION MODEL ===\\n"" + \'\\tTrain Accuracy: \' + str(self.accuracies[\'train_accuracy\']) + \'\\n\\tTest Accuracy: \' + str(self.accuracies[\'test_accuracy\']) + \'\\n\\tParameters: \' + str(self.parameters)\n\n    """""" Overloading of comparison operators """"""\n    def __eq__(self, other):\n        return self.accuracies[\'train_accuracy\'] == other.accuracies[\'train_accuracy\'] and self.accuracies[\'test_accuracy\'] == other.accuracies[\'test_accuracy\']\n\n    def __le__(self, other):\n        return self.accuracies[\'train_accuracy\'] <= other.accuracies[\'train_accuracy\'] and self.accuracies[\'test_accuracy\'] <= other.accuracies[\'test_accuracy\']\n\n    def __lt__(self,other):\n        return self.accuracies[\'train_accuracy\'] > other.accuracies[\'train_accuracy\'] and self.accuracies[\'test_accuracy\'] > other.accuracies[\'test_accuracy\']\n\n    def __ge__(self, other):\n        return self.accuracies[\'train_accuracy\'] >= other.accuracies[\'train_accuracy\'] and self.accuracies[\'test_accuracy\'] >= other.accuracies[\'test_accuracy\']\n\n    def __gt__(self,other):\n        return self.accuracies[\'train_accuracy\'] > other.accuracies[\'train_accuracy\'] and self.accuracies[\'test_accuracy\'] > other.accuracies[\'test_accuracy\']\n\n\n'"
1.1/src/classifier/test_suite.py,16,"b'from data_utilities import load_practice_dataset\nfrom prediction_model import PredictionModel\nfrom deep_neural_network import DeepNeuralNetwork\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef display_image(data_set, index):\n    plt.imshow(data_set[index])\n    plt.show()\n\ndef show_some_images(train_images, test_images):\n    display_image(test_images, 0)\n    display_image(train_images, 1)\n\n    display_image(test_images, 90)\n    display_image(train_images, 723)\n\n\n    display_image(test_images, 7)\n    display_image(train_images, 1000)\n\n\ndef save_model(parameters, accuracies):\n\tW1 = parameters[\'W1\']\n\tnp.save(\'../../prior_best/paramW1.npy\',W1)\n\n\tb1 = parameters[\'b1\']\n\tnp.save(\'../../prior_best/paramb1.npy\',b1)\n\n\tW2 = parameters[\'W2\']\n\tnp.save(\'../../prior_best/paramW2.npy\',W2)\n\n\tb2 = parameters[\'b2\']\n\tnp.save(\'../../prior_best/paramb2.npy\',b2)\n\n\tW3 = parameters[\'W3\']\n\tnp.save(\'../../prior_best/paramW3.npy\',W3)\n\n\tb3 = parameters[\'b3\']\n\tnp.save(\'../../prior_best/paramb3.npy\',b3)\n\n\ttrain_accuracy = accuracies[\'train_accuracy\']\n\tnp.save(\'../../prior_best/trainaccuracy.npy\',train_accuracy)\n\n\ttest_accuracy = accuracies[\'test_accuracy\']\n\tnp.save(\'../../prior_best/testaccuracy.npy\',test_accuracy)\n\n\ndef load_model():\n    parameters = {\n        \'W1\' : np.load(\'../../prior_best/paramW1.npy\'),\n        \'b1\' : np.load(\'../../prior_best/paramb1.npy\'),\n        \'W2\' : np.load(\'../../prior_best/paramW2.npy\'),\n        \'b2\' : np.load(\'../../prior_best/paramb2.npy\'),\n        \'W3\' : np.load(\'../../prior_best/paramW3.npy\'),\n        \'b3\' : np.load(\'../../prior_best/paramb3.npy\')\n        }\n\n    accuracies = {\n        \'train_accuracy\' : np.load(\'../../prior_best/trainaccuracy.npy\'),\n        \'test_accuracy\' : np.load(\'../../prior_best/testaccuracy.npy\')\n        }\n\n    return parameters, accuracies\n\ndef main():\n    # Load Data Set\n    print(""Loading practice data set."")\n\n    X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_practice_dataset()\n\n    # Show some images (optional of course : ) )\n#    show_some_images(X_train_orig, X_test_orig)\n\n    test_model = DeepNeuralNetwork(X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes)\n\n    for i in range(5):\n        parameters, accuracies = test_model.train(num_epochs = 1500, print_cost = True)\n        new_model = PredictionModel(parameters, accuracies)\n\n        parameters, accuracies = load_model()\n        previous_model = PredictionModel(parameters, accuracies)\n\n        if  new_model > previous_model :\n            print(""\\n\\tNew model is better... Displaying accuracies and updating files.. "")\n            print(new_model)\n            save_model(new_model.parameters, new_model.accuracies)\n        else:\n            print(""Previous model is superior."")\n\n    parameters, accuracies = load_model()\n    best_model = PredictionModel(parameters, accuracies)\n    print(best_model)\n\n\nif __name__ == ""__main__"":\n    main()'"
1.2/src/classifier/FileSystem.py,0,"b'import os, shutil, datetime\n\nclass FileSystem():\n    """"""\n    Class Description:\n    Class Object to handle the querying of File Directories information regarding the files within.\n    Author:\n    Jacob Taylor Cassady\n    """"""\n    @staticmethod\n    def log(data, file_name):\n        """"""\n        Function Description:\n        Logs the data to the location [file_name]. Note all previous information contained within this file will be deleted.\n        Author:\n        Jacob Taylor Cassady\n        """"""\n        try:\n            with open(file_name, ""a+"") as file:\n                file.write(data + ""\\n"")\n\n        except PermissionError:\n            print(""Permission error when accessing file: "" + file_name)\n\n    @staticmethod\n    def start_log(data, file_name):\n        """"""\n        Function Description:\n        Logs the data to the location [file_name]. Note all previous information contained within this file will be deleted.\n        Author:\n        Jacob Taylor Cassady\n        """"""\n        with open(file_name, ""w+"") as file:\n            file.write(data + ""\\n"")\n\n    @staticmethod\n    def load_evaluation(file_name):\n        print(""Loading evaluation for file:"", file_name)\n\n        with open(file_name) as evaluation_file:\n            data = evaluation_file.readlines()\n\n        loss = float(data[0])\n        accuracy = float(data[1])\n\n        print(""Evaluation loaded: Loss = {} | Accuracy = {}"".format(loss, accuracy))\n\n        return loss, accuracy\n'"
1.2/src/classifier/ResNet50.py,1,"b'\xef\xbb\xbfimport matplotlib.pyplot as plt\nimport numpy as np\nimport pydot\nfrom IPython.display import SVG\nimport scipy.misc\nimport os\n\nfrom keras import layers\nfrom keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\nfrom keras.models import Model, load_model, model_from_json\nfrom keras.preprocessing import image\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.applications.imagenet_utils import preprocess_input\nfrom keras.utils.vis_utils import model_to_dot, plot_model\nfrom keras.utils import plot_model\nfrom keras.initializers import glorot_uniform\nimport keras.backend as K\nK.set_image_data_format(\'channels_last\')\nK.set_learning_phase(1)\n\n# User create modules\nfrom resnet_utils import *\nfrom FileSystem import FileSystem\n\nclass ResNet50(object):\n    """"""\n\tAuthor: Jacob Taylor Cassady\n\n    Description -- Notes Taken from Professor Ng\'s Convolutoinal Neural Network Course on Coursera:\n\tImplementation of the popular ResNet50 the following architecture:\n\tCONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n\t-> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n\n\tArguments:\n\tinput_shape -- shape of the images of the dataset\n\tclasses -- integer, number of classes\n    """"""\n    def __init__(self, input_shape = (64, 64, 3), classes = 6):\n        tf.reset_default_graph()\n\n        print(""\\nBuilding ResNet50 model with input_shape:"", str(input_shape), ""and classes"", str(classes))\n        self.input_shape = input_shape\n        self.model = self.build_model(input_shape, classes)\n\n        print(""\\tCompiling model with the following parameters:"")\n        print(""\\t\\tOptimizer [Flavor of Gradient Descent] : Adam"")\n        print(""\\t\\tLoss Function : Categorical Cross Entropy"")\n        print(""\\t\\tMetrics : Accuracy"")\n        self.model.compile(optimizer=\'adam\', loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])\n\n        print(""\\tResNet50 model now ready to load data."")\n        self.X_train_orig = None\n        self.Y_train_orig = None\n        self.X_test_orig = None\n        self.Y_test_orig = None\n        self.classes = classes\n\n    def __del__(self):\n        del self.input_shape\n        del self.model\n        del self.X_train_orig\n        del self.Y_train_orig\n        del self.X_test_orig\n        del self.Y_test_orig\n        del self.classes\n\n    def identity_block(self, X, f, filters, stage, block):\n        """"""\n        Implementation of the identity block as defined in 1.2/reference_images/identity_block.png\n\n        Arguments:\n        X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n        f -- integer, specifying the shape of the middle CONV\'s window for the main path\n        filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n        stage -- integer, used to name the layers, depending on their position in the network\n        block -- string/character, used to name the layers, depending on their position in the network\n\n        Returns:\n        X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n        """"""\n\n        # defining name basis\n        conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n        bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n        # Retrieve Filters\n        F1, F2, F3 = filters\n\n        # Save the input value. You\'ll need this later to add back to the main path.\n        X_shortcut = X\n\n        # First component of main path\n        X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = \'valid\', name = conv_name_base + \'2a\', kernel_initializer = glorot_uniform(seed=0))(X)\n        X = BatchNormalization(axis = 3, name = bn_name_base + \'2a\')(X)\n        X = Activation(\'relu\')(X)\n\n        # Second component of main path\n        X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = \'same\', name = conv_name_base + \'2b\', kernel_initializer = glorot_uniform(seed=0))(X)\n        X = BatchNormalization(axis = 3, name = bn_name_base + \'2b\')(X)\n        X = Activation(\'relu\')(X)\n\n        # Third component of main path\n        X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = \'valid\', name = conv_name_base + \'2c\', kernel_initializer = glorot_uniform(seed=0))(X)\n        X = BatchNormalization(axis = 3, name = bn_name_base + \'2c\')(X)\n\n        # Final step: Add shortcut value to main path, and pass it through a RELU activation\n        X = Add()([X_shortcut, X])\n        X = Activation(\'relu\')(X)\n\n        return X\n\n    def convolutional_block(self, X, f, filters, stage, block, s = 2):\n        """"""\n        Implementation of the convolutional block as defined in 1.2/reference_images/convolutional_block.png\n\n        Arguments:\n        X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n        f -- integer, specifying the shape of the middle CONV\'s window for the main path\n        filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n        stage -- integer, used to name the layers, depending on their position in the network\n        block -- string/character, used to name the layers, depending on their position in the network\n        s -- Integer, specifying the stride to be used\n\n        Returns:\n        X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n        """"""\n\n        # defining name basis\n        conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n        bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n        # Retrieve Filters\n        F1, F2, F3 = filters\n\n        # Save the input value\n        X_shortcut = X\n\n\n        ##### MAIN PATH #####\n        # First component of main path\n        X = Conv2D(F1, (1, 1), strides = (s,s), padding = \'valid\', name = conv_name_base + \'2a\', kernel_initializer = glorot_uniform(seed=0))(X)\n        X = BatchNormalization(axis = 3, name = bn_name_base + \'2a\')(X)\n        X = Activation(\'relu\')(X)\n\n        # Second component of main path\n        X = Conv2D(F2, (f, f), strides = (1,1), padding = \'same\', name = conv_name_base + \'2b\', kernel_initializer = glorot_uniform(seed=0))(X)\n        X = BatchNormalization(axis = 3, name = bn_name_base + \'2b\')(X)\n        X = Activation(\'relu\')(X)\n\n        # Third component of main path\n        X = Conv2D(F3, (1, 1), strides = (1,1), padding = \'valid\', name = conv_name_base + \'2c\', kernel_initializer = glorot_uniform(seed=0))(X)\n        X = BatchNormalization(axis = 3, name = bn_name_base + \'2c\')(X)\n\n        ##### SHORTCUT PATH ####\n        X_shortcut = Conv2D(F3, (1, 1), strides = (s,s), padding = \'valid\', name = conv_name_base + \'1\', kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n        X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + \'1\')(X_shortcut)\n\n        # Final step: Add shortcut value to main path, and pass it through a RELU activation\n        X = Add()([X, X_shortcut])\n        X = Activation(\'relu\')(X)\n\n        return X\n\n\n    def build_model(self, input_shape, classes):\n        """"""\n        Implementation of the popular ResNet50 the following architecture:\n        CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n        -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n\n        Arguments:\n        input_shape -- shape of the images of the dataset\n        classes -- integer, number of classes\n\n        Returns:\n        model -- a Model() instance in Keras\n        """"""\n        # Define the input as a tensor with shape input_shape\n        X_input = Input(input_shape)\n\n\n        # Zero-Padding\n        X = ZeroPadding2D((3, 3))(X_input)\n\n        # Stage 1\n        X = Conv2D(64, (7, 7), strides = (2, 2), name = \'conv1\', kernel_initializer = glorot_uniform(seed=0))(X)\n        X = BatchNormalization(axis = 3, name = \'bn_conv1\')(X)\n        X = Activation(\'relu\')(X)\n        X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n\n        # Stage 2\n        X = self.convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block=\'a\', s = 1)\n        X = self.identity_block(X, 3, [64, 64, 256], stage=2, block=\'b\')\n        X = self.identity_block(X, 3, [64, 64, 256], stage=2, block=\'c\')\n\n        # Stage 3\n        X = self.convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block=\'a\', s = 2)\n        X = self.identity_block(X, 3, [128, 128, 512], stage=3, block=\'b\')\n        X = self.identity_block(X, 3, [128, 128, 512], stage=3, block=\'c\')\n        X = self.identity_block(X, 3, [128, 128, 512], stage=3, block=\'d\')\n\n        # Stage 4\n        X = self.convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block=\'a\', s = 2)\n        X = self.identity_block(X, 3, [256, 256, 1024], stage=4, block=\'b\')\n        X = self.identity_block(X, 3, [256, 256, 1024], stage=4, block=\'c\')\n        X = self.identity_block(X, 3, [256, 256, 1024], stage=4, block=\'d\')\n        X = self.identity_block(X, 3, [256, 256, 1024], stage=4, block=\'e\')\n        X = self.identity_block(X, 3, [256, 256, 1024], stage=4, block=\'f\')\n\n        # Stage 5\n        X = self.convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block=\'a\', s = 2)\n        X = self.identity_block(X, 3, [512, 512, 2048], stage=5, block=\'b\')\n        X = self.identity_block(X, 3, [512, 512, 2048], stage=5, block=\'c\')\n\n        # AVGPOOL\n        X = AveragePooling2D(pool_size=(2, 2), name=\'avg_pool\')(X)\n\n        # output layer\n        X = Flatten()(X)\n        X = Dense(classes, activation=\'softmax\', name=\'fc\' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n\n        # Create model\n        model = Model(inputs = X_input, outputs = X, name=\'ResNet50\')\n\n        return model\n\n\n    def save_model(self, model = \'best_model\'):\n        # Save the model to JSON\n        json_model = self.model.to_json()\n        with open("".."" + os.path.sep + "".."" + os.path.sep + ""models"" + os.path.sep + model + "".json"", ""w"") as json_file:\n            json_file.write(json_model)\n\n        # Save weights\n        self.model.save_weights("".."" + os.path.sep + "".."" + os.path.sep + ""models"" + os.path.sep + model + "".h5"")\n        print(""Saved model "" + model + "" to disk"")\n\n        # Save loss and accuracy\n        loss, accuracy = self.evaluate_model()\n        FileSystem.start_log(str(loss), os.getcwd() + os.path.sep + "".."" + os.path.sep + "".."" + os.path.sep + ""models"" + os.path.sep + model + ""_evaluation.txt"")\n        FileSystem.log(str(accuracy), os.getcwd() + os.path.sep + "".."" + os.path.sep + "".."" + os.path.sep + ""models"" + os.path.sep + model + ""_evaluation.txt"")\n\n        # Save graphical model summary and print summary to console.\n        print(self.model.summary())\n        plot_model(self.model, to_file= os.getcwd() + os.path.sep + "".."" + os.path.sep + "".."" + os.path.sep + ""models"" + os.path.sep + model + "".png"", show_shapes=True, show_layer_names=True)\n\n\n\n    def load_model(self, model = ""best_model""):\n        print(""Attemping to load the model: "" + model + "" from disk."")\n\n        # read in the model from json\n        print(""Building model graph using initial specifications"")\n        self.model = self.build_model(self.input_shape, self.classes)\n        print(""Successfully built model."")\n\n        #load weights into new model\n        print(""Attempting to load model from disk.."")\n        self.model.load_weights("".."" + os.path.sep + "".."" + os.path.sep + ""models"" + os.path.sep + model + "".h5"")\n        self.model.compile(optimizer=\'adam\', loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])\n        print(""Successfully loaded model weights for: "" + model + "" from disk."")\n\n\n\n    def load_data_h5(self, relative_directory_path):\n        print(""\\nLoading data from relative directory path:"", relative_directory_path)\n        X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, self.classes = load_dataset(relative_directory_path)\n\n        # Normalize image vectors\n        self.X_train = X_train_orig/255.\n        self.X_test = X_test_orig/255.\n\n        # Convert training and test labels to one hot matrices\n        self.Y_train = convert_to_one_hot(Y_train_orig, 6).T\n        self.Y_test = convert_to_one_hot(Y_test_orig, 6).T\n\n        print (""\\tnumber of training examples = "" + str(self.X_train.shape[0]))\n        print (""\\tnumber of test examples = "" + str(self.X_test.shape[0]))\n        print (""\\tX_train shape: "" + str(self.X_train.shape))\n        print (""\\tY_train shape: "" + str(self.Y_train.shape))\n        print (""\\tX_test shape: "" + str(self.X_test.shape))\n        print (""\\tY_test shape: "" + str(self.Y_test.shape))\n\n    def train_model(self, epochs = 2, batch_size = 32):\n        print(""\\nTraining model... for "", epochs, ""epochs with a batch size of"", batch_size)\n        self.model.fit(self.X_train, self.Y_train, epochs = epochs, batch_size = batch_size)\n\n    def evaluate_model(self):\n        print(""\\nEvaluating Model..."")\n        preds = self.model.evaluate(self.X_test, self.Y_test, verbose=1)\n        print (""\\tLoss = "" + str(preds[0]))\n        print (""\\tTest Accuracy = "" + str(preds[1]))\n        return preds[0], preds[1]\n\n    def predict_image(self, image_path):\n        print(""preparing to predict image:"", image_path)\n        img = image.load_img(image_path, target_size=(64, 64))\n\n        if img is None:\n            print(""Unable to open image"")\n            return None\n\n        pixels = image.img_to_array(img)\n        pixels = np.expand_dims(pixels, axis=0)\n        pixels = preprocess_input(pixels)\n\n        print(\'Input image shape:\', pixels.shape)\n\n        for index, response in enumerate(self.model.predict(pixels)[0]):\n            if response == 1:\n                return index\n\ndef test_ResNet50(epochs = 2, batch_size = 32):\n    test_model = ResNet50(input_shape = (64, 64, 3), classes = 6)\n    test_model.load_data_h5(""../../../Practice_Data/"")\n    test_model.train_model(epochs, batch_size)\n    test_model.evaluate_model()\n\n\nif __name__ == ""__main__"":\n    test_ResNet50(2, 32)\n'"
1.2/src/classifier/SIMON.py,0,"b'import serial\nimport os\nfrom tkinter.filedialog import askopenfilename\n\nfrom ResNet50 import ResNet50\nfrom deep_neural_network import DNNPredictionModel\nfrom FileSystem import FileSystem\n\nclass SIMON(object):\n    def __init__(self):\n        self.test_loss = 100.0\n        self.test_accuracy = 0.0\n        self.dnn_model = None\n\n    def update_model(self, source_model_name, destination_model_name):\n        # Create a new process to update the model.  I\'m doing this because I\'ve been having trouble with pythons garbage collection\n        print(""source_model_name:"", source_model_name)\n        print(""destination_model_name:"", destination_model_name)\n        print(""self.test_loss:"", self.test_loss)\n        print(""self.test_accuracy:"", self.test_accuracy)\n        os_call = ""python model_processes.py 2 {0} {1} {2:.3f} {3:.3f}"".format(source_model_name, destination_model_name, self.test_loss, self.test_accuracy)\n        print(""\\nMaking command line call: "", os_call)\n        os.system(os_call)\n\n        self.test_loss, self.test_accuracy = FileSystem.load_evaluation(os.getcwd() + os.path.sep + "".."" + os.path.sep + "".."" + os.path.sep + ""models"" + os.path.sep + destination_model_name + ""_evaluation.txt"")\n\n    def display_menu(self):\n        print(""\\nHello.  My name is SIMON.  I am a neural network designed to classify representations of american sign language."")\n        print(""1 ) Load a previously trained model."")\n        print(""2 ) Train a new model or attempt to improve a previous one."")\n        print(""0 ) quit."")\n\n        return input(""What would you like me to do : "")\n\n    def loop_menu(self):\n        response = 1\n        while(response != 0):\n            response = int(self.display_menu())\n            self.perform_action(menu_response = response)\n            if(response == 1):\n                response = 0\n            print(""Action Complete.\\n"")\n\n    def loop_model_menu(self):\n        response = 1\n        print(""\\nA model has been loaded."")\n        while(response != 0):\n            print("" ==== MODEL MENU ===="")\n            print(""1 ) Predict an image."")\n            print(""2 ) Evaluate the model."")\n            print(""3 ) Control a device."")\n            print(""0 ) Exit."")\n            response = int(input(\'What would you like to do : \'))\n\n            if response == 0:\n                print(""Exiting..."")\n            elif response == 1:\n                self.prompt_predict_image()\n            elif response == 2:\n                self.dnn_model.evaluate_model()\n            elif response == 3:\n                serial_port = input(""What serial port is your device connected to? "")\n\n                serial_connection = serial.Serial(\n                    port = serial_port,\n                    baudrate = 115200,\n                    parity = serial.PARITY_NONE,\n                    stopbits = serial.STOPBITS_ONE,\n                    bytesize = serial.EIGHTBITS,\n                    timeout = 1\n                )\n\n                device_control_response = 1\n                while device_control_response != 0:\n                    print(""\\nDevice control menu:"")\n                    print(""1 ) Predict an image."")\n                    print(""2 ) Evaluate the model."")\n                    print(""0 ) Exit."")\n                    device_control_response = int(input(\'What would you like to do : \'))\n\n                    if device_control_response == 0:\n                        print(""Exiting..."")\n                    elif device_control_response == 1:\n                        prediction = str(self.dnn_model.predict_image(image_path = askopenfilename()))\n                        serial_connection.write(prediction.encode())\n                        print(""The model predicted:"", prediction)\n                    elif device_control_response == 2:\n                        self.dnn_model.evaluate_model()\n\n            else:\n                print(""Invalid menu response.  Please try again."")\n\n\n    def perform_action(self, menu_response):\n        if menu_response == 0:\n            print(""Exiting..."")\n        elif menu_response == 1:\n            self.prompt_load_previous_model()\n            self.loop_model_menu()\n        elif menu_response == 2:\n            self.prompt_train_model()\n        else:\n            print(""Invalid menu response.  Please try again."")\n\n\n    def prompt_load_previous_model(self):\n        model_type = 0\n\n        while model_type != 1 and model_type != 2:\n            print(""== DNN model types =="")\n            print(""1 ) Classic DNN"")\n            print(""2 ) ResNet50"")\n            model_type = int(input(""What type of model would you like to load :""))\n\n        model_name = input(""\\nWhat is the alias of the model you would like to load : "")\n\n        if model_type == 1:\n            # Classic DNN\n            if os.path.isfile("".."" + os.path.sep + "".."" + os.path.sep + ""models"" + os.path.sep + model_name + os.path.sep + ""paramb1.npy""): # previous model exists\n                self.dnn_model = DNNPredictionModel(parameters=None, accuracies = None)\n                self.dnn_model.load_model(model = ""dnn_best"")\n            else:\n                print(""Unable to find a previous model matching the given alias."")\n\n        else: #ResNet50\n            if os.path.isfile("".."" + os.path.sep + "".."" + os.path.sep + ""models"" + os.path.sep + model_name + "".h5""): # previous model exists\n                # Initialize a ResNet50 model to use in evaluation\n                self.dnn_model = ResNet50(input_shape = (64, 64, 3), classes = 6)\n\n                # Load a model given a source model alias\n                self.dnn_model.load_model(model_name)\n\n                # Load the test and train data into the model\n                self.dnn_model.load_data_h5("".."" + os.path.sep + "".."" + os.path.sep + "".."" + os.path.sep + ""Practice_Data"" + os.path.sep)\n            else:\n                print(""Unable to find a previous model matching the given alias."")\n\n\n\n    def prompt_train_model(self):\n        model_name = input(""\\nWhat is the alias of the model you would like to train : "")\n\n        if os.path.isfile(""../../models/"" + model_name + "".h5""): # previous model exists\n            print(""Previous model found matching given alias.  Evaluating previous model..."")\n            self.update_model(source_model_name=model_name, destination_model_name=model_name)\n            print(""Previous model has a loss of {} and an accuracy of {}"".format(self.test_loss, self.test_accuracy))\n            print(""Maybe we can do better."")\n        else:\n            print(""Unable to find a previous model matching the given alias."")\n\n        attempts = int(input(""Let\'s start training new models.  How many training attempts would you like to perform : ""))\n        epochs = int(input(""How many epochs should each training attempt complete : ""))\n        batch_size = int(input(""How large should each training batch be : ""))\n\n        for attempt in range(attempts):\n            print(""\\nBeginning training attempt: "", attempt)\n            # Train a new model and log it under the alias ""recent_model""\n            self.train_new_model(model_name, epochs, batch_size)\n\n            # Update the evaluation of the model and overwrite the previous save if the recent model is better.\n            self.update_model(source_model_name=""recent_model"", destination_model_name=model_name)\n\n        print(""Done with all training attempts.  You will need to reload this model using the alias {} before performing predictions."".format(model_name))\n\n    def train_new_model(self, model_name, epochs, batch_size):\n        print(""\\nCreating a new process to train "" + str(model_name))\n\n        # Create a new process to train a model.  I\'m doing this because I\'ve been having trouble with pythons garbage collection\n        print(""\\nMaking command line call: "", ""python model_processes.py 1 "" + str(epochs) + "" "" + str(batch_size))\n        os.system(""python model_processes.py 1 "" + str(epochs) + "" "" + str(batch_size))\n\n    def prompt_predict_image(self):\n        if self.dnn_model is None:\n            print(""You need to load or train a model for me to perform predictions with."")\n        else:\n            prediction = self.dnn_model.predict_image(image_path = askopenfilename())\n            print(""The model predicted:"", prediction)\n\n\nif __name__ == ""__main__"":\n    simon = SIMON()\n    simon.loop_menu()\n'"
1.2/src/classifier/data_utilities.py,10,"b'import h5py\nimport math\n\nimport numpy as np\n\ndef load_practice_dataset():\n    """"""\n    Created By: Jacob Taylor Cassady\n    Last Updated: 2/7/2018\n    Objective: Load in practice data from example tensorflow model.\n    \n    Arguments: \n    None\n    \n    Returns: \n    train_set_x_orig -- A NumPy array of (currently) 1080 training images of shape (64,64,3).  Total nparray shape of (1080,64,64,3)\n    train_set_y_orig -- A NumPy array of (currently) 1080 training targets.  Total nparray shape of (1, 1080) [After reshape]\n    test_set_x_orig -- A NumPy array of (currently) 120 test images of shape (64,64,3).  Total nparray shape of (120,64,64,3)\n    test_set_y_orig -- A NumPy array of (currently) 120 test targets.  Total nparray shape of (1,120) [After reshape]\n    classes -- A NumPy array of (currently) 6 classes. (0-5)\n    """"""\n\n    train_dataset = h5py.File(\'../../Practice_Data/train_signs.h5\', \'r\')\n    train_set_x_orig = np.array(train_dataset[""train_set_x""][:])\n    train_set_y_orig = np.array(train_dataset[""train_set_y""][:])\n\n    test_dataset = h5py.File(\'../../Practice_Data/test_signs.h5\', \'r\')\n    test_set_x_orig = np.array(test_dataset[""test_set_x""][:])\n    test_set_y_orig = np.array(test_dataset[""test_set_y""][:])\n\n    classes = np.array(test_dataset[\'list_classes\'][:])\n\n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n\n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n\n\ndef convert_to_one_hot(Y, C):\n    Y = np.eye(C)[Y.reshape(-1)].T\n    return Y\n\ndef random_mini_batches(X, Y, mini_batch_size = 64):\n    """"""\n    Created By: Jacob Taylor Cassady\n    Last Updated: 2/8/2018\n    Objective: Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true ""label"" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n    mini_batch_size - size of the mini-batches, integer\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    """"""\n    \n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n    \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches\n\n\ndef predict(X, parameters):\n    \n    W1 = tf.convert_to_tensor(parameters[""W1""])\n    b1 = tf.convert_to_tensor(parameters[""b1""])\n    W2 = tf.convert_to_tensor(parameters[""W2""])\n    b2 = tf.convert_to_tensor(parameters[""b2""])\n    W3 = tf.convert_to_tensor(parameters[""W3""])\n    b3 = tf.convert_to_tensor(parameters[""b3""])\n    \n    params = {""W1"": W1,\n              ""b1"": b1,\n              ""W2"": W2,\n              ""b2"": b2,\n              ""W3"": W3,\n              ""b3"": b3}\n    \n    x = tf.placeholder(""float"", [12288, 1])\n    \n    z3 = forward_propagation_for_predict(x, params)\n    p = tf.argmax(z3)\n    \n    sess = tf.Session()\n    prediction = sess.run(p, feed_dict = {x: X})\n        \n    return prediction\n\ndef forward_propagation_for_predict(X, parameters):\n    """"""\n    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n    \n    Arguments:\n    X -- input dataset placeholder, of shape (input size, number of examples)\n    parameters -- python dictionary containing your parameters ""W1"", ""b1"", ""W2"", ""b2"", ""W3"", ""b3""\n                  the shapes are given in initialize_parameters\n    Returns:\n    Z3 -- the output of the last LINEAR unit\n    """"""\n    \n    # Retrieve the parameters from the dictionary ""parameters"" \n    W1 = parameters[\'W1\']\n    b1 = parameters[\'b1\']\n    W2 = parameters[\'W2\']\n    b2 = parameters[\'b2\']\n    W3 = parameters[\'W3\']\n    b3 = parameters[\'b3\'] \n                                                           # Numpy Equivalents:\n    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n    \n    return Z3\n'"
1.2/src/classifier/deep_neural_network.py,25,"b'import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy\n\nfrom scipy import ndimage\nfrom tensorflow.python.framework import ops\n\nfrom data_utilities import convert_to_one_hot, random_mini_batches, load_practice_dataset\n\n\nclass DeepNeuralNetwork(object):\n\tdef __init__(self, X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes):\n\t\ttf.reset_default_graph()\n\t\tself.train_matrix, self.train_targets, self.test_matrix, self.test_targets = self.flatten_data(X_train_orig, Y_train_orig, X_test_orig, Y_test_orig)\n\t\tself.classes = classes.reshape((classes.shape[0],1))\n\t\tself.parameters = self.initialize_parameters()\n\n\tdef print_dataset_shapes(self):\n\t\tprint(""===== Printing dataset shapes ====="")\n\t\tprint(self.train_matrix.shape, ""train_parameter_matrix"")\n\t\tprint(self.train_targets.shape, ""train_targets"")\n\t\tprint(self.test_matrix.shape, ""test_parameter_matrix"")\n\t\tprint(self.test_targets.shape, ""test_targets"")\n\t\tprint(self.classes.shape, ""classes"")\n\n\tdef train_model(self, learning_rate = 0.0001, epochs = 1500, batch_size = 32, print_cost = True):\n\t\t""""""\n\t\tCreated By: Jacob Taylor Cassady\n\t\tLast Updated: 2/7/2018\n\t    Objective: Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n\n\t\tArguments:\n\t\tlearning_rate -- learning rate of the optimization\n\t\tepochs -- number of epochs of the optimization loop\n\t\tbatch_size -- size of a minibatch\n\t\tprint_cost -- True to print the cost every 100 epochs\n\n\t\tReturns:\n\t\tparameters -- parameters learnt by the model. They can then be used to predict.\n\t\t""""""\n\t\tprint(""Entering train...."")\n\t\tops.reset_default_graph()\n\t\t(n_x, m) = self.train_matrix.shape    # (n_x: input size, m : number of examples in the train set)\n\t\tn_y = self.train_targets.shape[0]\t\t  # n_y : output size.\n\t\tcosts = []\t\t\t\t\t\t\t  # Used to keep track of varying costs.\n\n\t\t# Create placeholders for TensorFlow graph of shape (n_x, n_y)\n\t\tprint(""Creating placeholders for TensorFlow graph..."")\n\t\tX, Y = self.create_placeholders(n_x, n_y)\n\t\tprint(""Complete.\\n"")\n\n\t\t# Initialize Parameters\n\t\tprint(""Initailizing parameters for TensorFlow graph..."")\n\t\tparameters = self.initialize_parameters()\n\t\tprint(""Complete.\\n"")\n\n\t\t# Build the forward propagation in the TensorFlow Graph\n\t\tprint(""Building the forward propagation in the TensorFlow Graph..."")\n\t\tZ3 = self.forward_propagation(X, parameters)\n\t\tprint(""Complete.\\n"")\n\n\t\t# Add the cost function to the Tensorflow Graph\n\t\tprint(""Adding cost function to the TensorFlow Graph"")\n\t\tcost = self.compute_cost(Z3, Y)\n\t\tprint(""Complete.\\n"")\n\n\t\t# Define the TensorFlow Optimizer.. We are using an AdamOptimizer.\n\t\toptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n\t\t# Initialize all the variables with our newly made TensorFlow Graph\n\t\tinit = tf.global_variables_initializer()\n\n\t\t# Use the TensorFlow Graph to train the parameters.\n\t\twith tf.Session() as session:\n\t\t\t# Run the initialization\n\t\t\tsession.run(init)\n\n\t\t\t# Perform Training\n\t\t\tfor epoch in range(epochs):\n\t\t\t\tepoch_cost = 0.\t\t\t\t\t\t\t\t# Defines a cost related to the current epoch\n\t\t\t\tnum_minibatches = int(m / batch_size)\t# Calculates the number of minibatches in the trainset given a minibatch size\n\t\t\t\tminibatches = random_mini_batches(self.train_matrix, self.train_targets, batch_size)\n\n\t\t\t\tfor minibatch in minibatches:\n\t\t\t\t\t# Retrieve train_matrix and train_targets from minibatch\n\t\t\t\t\tmini_matrix, mini_targets = minibatch\n\n\t\t\t\t\t# Run the session to execute the ""optimizer"" and the ""cost"",\n\t\t\t\t\t_, minibatch_cost = session.run([optimizer, cost], feed_dict={X:mini_matrix, Y:mini_targets})\n\n\t\t\t\t\t# Sum epoch cost\n\t\t\t\t\tepoch_cost += minibatch_cost / num_minibatches\n\n\t\t\t\t# Done training.  Print the cost of every 100 epochs\n\t\t\t\tif print_cost == True and epoch % 100 == 0:\n\t\t\t\t\tprint(""Cost after epoch %i: %f"" % (epoch, epoch_cost))\n\t\t\t\t# Keep track of the cost of every 5 epochs for plotting later\n\t\t\t\tif print_cost == True and epoch % 5 == 0:\n\t\t\t\t\tcosts.append(epoch_cost)\n\n\t\t\t# Plot the costs for analysis\n\t\t\tplt.plot(np.squeeze(costs))\n\t\t\tplt.ylabel(\'cost\')\n\t\t\tplt.xlabel(\'iteration ( per 5 )\')\n\t\t\tplt.title(\'Learning rate = \' + str(learning_rate))\n\t\t\tif print_cost == True:\n\t\t\t\t#plt.show()\n\t\t\t\tpass\n\n\t\t\t# Save the parameters as a varaible for prediction and evaluation of fit to test set.\n\t\t\tparameters = session.run(parameters)\n\n\t\t\t# Develop TensorFlow prediction standards for testing accuracy  of test and train sets\n\t\t\tcorrect_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n\n\t\t\t# Develop accuracy identifier using TensorFlow\n\t\t\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, \'float\'))\n\n\t\t\t# Display accuracy of train and test predictions.\n\t\t\tprint(""Train Accuracy: "", accuracy.eval({X: self.train_matrix, Y: self.train_targets}))\n\t\t\tprint(""Test Accuracy: "", accuracy.eval({X: self.test_matrix, Y: self.test_targets}))\n\n\t\t\t# Return parameters for prediction against the model.\n\t\t\tself.parameters = parameters\n\n\t\t\ttrain_accuracy = accuracy.eval({X: self.train_matrix, Y: self.train_targets})\n\t\t\ttest_accuracy = accuracy.eval({X: self.test_matrix, Y: self.test_targets})\n\n\t\t\taccuracies = {""train_accuracy"": train_accuracy,\n                          ""test_accuracy"" : test_accuracy}\n\n\t\t\t# Return parameters for prediction against the model.\n\t\t\treturn parameters, accuracies\n\n\tdef flatten_data(self, X_train_orig, Y_train_orig, X_test_orig, Y_test_orig):\n\t\t""""""\n\t\tCreated By: Jacob Taylor Cassady\n\t\tLast Updated: 2/7/2018\n\t\tObjective: Load in practice data from example tensorflow model.\n\n\t\tArguments:\n\t\ttrain_set_x_orig -- A NumPy array of (currently) 1080 training images of shape (64,64,3).  Total nparray shape of (1080,64,64,3)\n\t\ttrain_set_y_orig -- A NumPy array of (currently) 1080 training targets.  Total nparray shape of (1, 1080) [After reshape]\n\t\ttest_set_x_orig -- A NumPy array of (currently) 120 test images of shape (64,64,3).  Total nparray shape of (120,64,64,3)\n\t\ttest_set_y_orig -- A NumPy array of (currently) 120 test targets.  Total nparray shape of (1,120) [After reshape]\n\n\t\tReturns:\n\t\tX_train -- A NumPy array of training data.  [Practice shape = (12288, 1080)]\n\t\tY_train -- A NumPy array of training targets.  [Practice shape = (6, 1080)]\n\t\tX_train -- A NumPy array of test data.  [Practice shape = (12288, 120)]\n\t\tY_train -- A NumPy array of test targets.  [Practice shape = (6, 120)]\n\t\t""""""\n\t\t# Flatten the training and test images\n\t\tX_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n\t\tX_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n\t\t# Normalize image vectors\n\t\tX_train = X_train_flatten/255.\n\t\tX_test = X_test_flatten/255.\n\t\t# Convert training and test labels to one hot matrices\n\t\tY_train = convert_to_one_hot(Y_train_orig, 6)\n\t\tY_test = convert_to_one_hot(Y_test_orig, 6)\n\n\n\t\treturn X_train, Y_train, X_test, Y_test\n\n\tdef create_placeholders(self, n_x, n_y):\n\t\t""""""\n\t\tCreated By: Jacob Taylor Cassady\n\t\tLast Updated: 2/8/2018\n\t\tObjective: Creates the placeholders for the tensorflow session.  These are used in the Tensorflow Graph.\n\n\t\tArguments:\n\t\tn_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n\t\tn_y -- scalar, number of classes (from 0 to 5, so -> 6)\n\n\t\tReturns:\n\t\tX -- placeholder for the data input, of shape [n_x, None] and dtype ""float""\n\t\tY -- placeholder for the input labels, of shape [n_y, None] and dtype ""float""\n\n\t\tNote:\n\t\t- We will use None because it let\'s us be flexible on the number of examples you will for the placeholders.\n\t\t\tIn fact, the number of examples during test/train is different.\n\t\t""""""\n\n\t\tX = tf.placeholder(shape=[n_x, None], dtype=tf.float32, name=\'X\')\n\t\tY = tf.placeholder(shape=[n_y, None], dtype=tf.float32, name=\'Y\')\n\n\t\treturn X, Y\n\n\tdef initialize_parameters(self, N1 = 25, N2 = 12):\n\t\t""""""\n\t\tInitializes parameters to build a neural network with tensorflow. The shapes are:\n\t\t\t\t\t\t\tW1 : [N1, X_train.shape[0]]\n\t\t\t\t\t\t\tb1 : [N1, 1]\n\t\t\t\t\t\t\tW2 : [N2, N1]\n\t\t\t\t\t\t\tb2 : [N2, 1]\n\t\t\t\t\t\t\tW3 : [classes.shape[0], N2]\n\t\t\t\t\t\t\tb3 : [classes.shape[0], 1]\n\n\t\tReturns:\n\t\tparameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n\t\t""""""\n\t\tW1 = tf.get_variable(\'W1\', [N1, self.train_matrix.shape[0]], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n\t\tb1 = tf.get_variable(\'b1\', [N1, 1], initializer = tf.zeros_initializer())\n\t\tW2 = tf.get_variable(\'W2\', [N2,N1], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n\t\tb2 = tf.get_variable(\'b2\', [N2, 1], initializer = tf.zeros_initializer())\n\t\tW3 = tf.get_variable(\'W3\', [self.classes.shape[0],N2], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n\t\tb3 = tf.get_variable(\'b3\', [self.classes.shape[0], 1], initializer = tf.zeros_initializer())\n\n\t\tparameters = {\n\t\t\t""W1"" : W1,\n\t\t\t""b1"" : b1,\n\t\t\t""W2"" : W2,\n\t\t\t""b2"" : b2,\n\t\t\t""W3"" : W3,\n\t\t\t""b3"" : b3,\n\t\t\t}\n\n\t\treturn parameters\n\n\tdef forward_propagation(self, input_matrix, parameters):\n\t\t""""""\n\t\tImplements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n\n\t\tArguments:\n\t\tinput_matrix -- input dataset placeholder, of shape (input size, number of examples)\n\t\tparameters -- python dictionary containing your parameters ""W1"", ""b1"", ""W2"", ""b2"", ""W3"", ""b3""\n\t\t\t\t\t  the shapes are given in initialize_parameters\n\n\t\tReturns:\n\t\tZ3 -- the output of the last LINEAR unit\n\t\t""""""\n\t\t# Retrieve the parameters from the dictionary ""parameters""\n\t\tW1 = parameters[\'W1\']\n\t\tb1 = parameters[\'b1\']\n\t\tW2 = parameters[\'W2\']\n\t\tb2 = parameters[\'b2\']\n\t\tW3 = parameters[\'W3\']\n\t\tb3 = parameters[\'b3\']\n\n\t\tZ1 = tf.matmul(W1, input_matrix) + b1                            # Z1 = np.dot(W1, X) + b1\n\t\tA1 = tf.nn.relu(Z1)                                              # A1 = relu(Z1)\n\t\tZ2 = tf.matmul(W2, A1) + b2                                      # Z2 = np.dot(W2, a1) + b2\n\t\tA2 = tf.nn.relu(Z2)                                              # A2 = relu(Z2)\n\t\tZ3 = tf.matmul(W3, A2) + b3                                      # Z3 = np.dot(W3,A2) + b3\n\n\t\treturn Z3\n\n\tdef compute_cost(self, Z3, targets):\n\t\t""""""\n\t\tComputes the cost\n\n\t\tArguments:\n\t\tZ3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n\t\tY -- ""true"" labels vector placeholder, same shape as Z3\n\t\t\tReturns:\n\t\tcost - Tensor of the cost function\n\t\t""""""\n\n\t\t# to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n\t\tlogits = tf.transpose(Z3)\n\t\tlabels = tf.transpose(targets)\n\n\t\tcost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n\n\t\treturn cost\n\n\nclass DNNPredictionModel(object):\n\tdef __init__(self, parameters, accuracies):\n\t\tself.parameters = parameters\n\t\tself.accuracies = accuracies\n\tdef predict_image(self, image_path):\n\t\timage = np.array(ndimage.imread(ismage_path, flatten = False))\n\t\tpixels = scipy.misc.imresize(image, size=(64,64)).reshape((1, 64*64*3)).T\n\t\treturn self.predict(pixels)\n\n\tdef evaluate_model(self):\n\t\tprint(self)\n\n\tdef predict(self, X):\n\t\tW1 = tf.convert_to_tensor(self.parameters[""W1""])\n\t\tb1 = tf.convert_to_tensor(self.parameters[""b1""])\n\t\tW2 = tf.convert_to_tensor(self.parameters[""W2""])\n\t\tb2 = tf.convert_to_tensor(self.parameters[""b2""])\n\t\tW3 = tf.convert_to_tensor(self.parameters[""W3""])\n\t\tb3 = tf.convert_to_tensor(self.parameters[""b3""])\n\n\t\tparams = {""W1"": W1,\n                  ""b1"": b1,\n                  ""W2"": W2,\n                  ""b2"": b2,\n                  ""W3"": W3,\n                  ""b3"": b3}\n\n\t\tx = tf.placeholder(""float"", [12288, 1])\n\n\t\tz3 = self.forward_propagation_for_predict(x)\n\t\tp = tf.argmax(z3)\n\n\t\tsess = tf.Session()\n\t\tprediction = sess.run(p, feed_dict = {x: X})\n\n\t\treturn prediction[0]\n\n\tdef forward_propagation_for_predict(self, X):\n\t\t""""""\n\t\tImplements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n\n\t\tArguments:\n\t\tX -- input dataset placeholder, of shape (input size, number of examples)\n\t\tparameters -- python dictionary containing your parameters ""W1"", ""b1"", ""W2"", ""b2"", ""W3"", ""b3""\n\t\t              the shapes are given in initialize_parameters\n\t\tReturns:\n\t\tZ3 -- the output of the last LINEAR unit\n\t\t""""""\n\n\t\t# Retrieve the parameters from the dictionary ""parameters""\n\t\tW1 = self.parameters[\'W1\']\n\t\tb1 = self.parameters[\'b1\']\n\t\tW2 = self.parameters[\'W2\']\n\t\tb2 = self.parameters[\'b2\']\n\t\tW3 = self.parameters[\'W3\']\n\t\tb3 = self.parameters[\'b3\']\n\t\t                                                       # Numpy Equivalents:\n\t\tZ1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n\t\tA1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n\t\tZ2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n\t\tA2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n\t\tZ3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n\n\t\treturn Z3\n\n\t"""""" Overloading of string operator """"""\n\tdef __str__(self):\n\t\treturn ""\\t === CURRENT PREDICTION MODEL ===\\n"" + \'\\tTrain Accuracy: \' + str(self.accuracies[\'train_accuracy\']) + \'\\n\\tTest Accuracy: \' + str(self.accuracies[\'test_accuracy\']) + \'\\n\\tParameters: \' + str(self.parameters)\n\n\t"""""" Overloading of comparison operators """"""\n\tdef __eq__(self, other):\n\t\treturn self.accuracies[\'train_accuracy\'] == other.accuracies[\'train_accuracy\'] and self.accuracies[\'test_accuracy\'] == other.accuracies[\'test_accuracy\']\n\n\tdef __le__(self, other):\n\t\treturn self.accuracies[\'train_accuracy\'] <= other.accuracies[\'train_accuracy\'] and self.accuracies[\'test_accuracy\'] <= other.accuracies[\'test_accuracy\']\n\n\tdef __lt__(self,other):\n\t\treturn self.accuracies[\'train_accuracy\'] > other.accuracies[\'train_accuracy\'] and self.accuracies[\'test_accuracy\'] > other.accuracies[\'test_accuracy\']\n\n\tdef __ge__(self, other):\n\t\treturn self.accuracies[\'train_accuracy\'] >= other.accuracies[\'train_accuracy\'] and self.accuracies[\'test_accuracy\'] >= other.accuracies[\'test_accuracy\']\n\n\tdef __gt__(self,other):\n\t\treturn self.accuracies[\'train_accuracy\'] > other.accuracies[\'train_accuracy\'] and self.accuracies[\'test_accuracy\'] > other.accuracies[\'test_accuracy\']\n\n\tdef load_model(self, model=""dnn_best""):\n\t\tself.parameters = {\n\t\t    \'W1\' : np.load(\'../../models/\' + model + \'/paramW1.npy\'),\n\t\t    \'b1\' : np.load(\'../../models/\' + model + \'/paramb1.npy\'),\n\t\t    \'W2\' : np.load(\'../../models/\' + model + \'/paramW2.npy\'),\n\t\t    \'b2\' : np.load(\'../../models/\' + model + \'/paramb2.npy\'),\n\t\t    \'W3\' : np.load(\'../../models/\' + model + \'/paramW3.npy\'),\n\t\t    \'b3\' : np.load(\'../../models/\' + model + \'/paramb3.npy\')\n\t\t}\n\n\t\tself.accuracies = {\n\t\t    \'train_accuracy\' : np.load(\'../../models/\' + model + \'/trainaccuracy.npy\'),\n\t\t    \'test_accuracy\' : np.load(\'../../models/\' + model + \'/testaccuracy.npy\')\n\t\t}\n\n\n\tdef improve_prediction_model(self, epochs = 5):\n\t\t# Load Data Set\n\t\tprint(""Loading data set."")\n\n\t\tX_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_practice_dataset()\n\n\t\ttest_model = DeepNeuralNetwork(X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes)\n\n\t\tfor i in range(epochs):\n\t\t\tparameters, accuracies = test_model.train(num_epochs = 1500, print_cost = True)\n\t\t\tnew_model = PredictionModel(parameters, accuracies)\n\n\t\t\tif  new_model > self.prediction_model:\n\t\t\t\tprint(""\\n\\tNew model is better... Displaying accuracies and updating files.. "")\n\t\t\t\tself.prediction_model = new_model\n\t\t\t\tprint(self.prediction_model)\n\t\t\t\tself.save_model()\n\t\t\telse:\n\t\t\t\tprint(""Previous model is superior or equivalent."")\n\n\t\tprint(self)\n\n\tdef save_model(self):\n\t\tparameters = self.parameters\n\t\taccuracies = self.accuracies\n\n\t\tW1 = parameters[\'W1\']\n\t\tnp.save(\'../../prior_best/paramW1.npy\',W1)\n\n\t\tb1 = parameters[\'b1\']\n\t\tnp.save(\'../../prior_best/paramb1.npy\',b1)\n\n\t\tW2 = parameters[\'W2\']\n\t\tnp.save(\'../../prior_best/paramW2.npy\',W2)\n\n\t\tb2 = parameters[\'b2\']\n\t\tnp.save(\'../../prior_best/paramb2.npy\',b2)\n\n\t\tW3 = parameters[\'W3\']\n\t\tnp.save(\'../../prior_best/paramW3.npy\',W3)\n\n\t\tb3 = parameters[\'b3\']\n\t\tnp.save(\'../../prior_best/paramb3.npy\',b3)\n\n\t\ttrain_accuracy = accuracies[\'train_accuracy\']\n\t\tnp.save(\'../../prior_best/trainaccuracy.npy\',train_accuracy)\n\n\t\ttest_accuracy = accuracies[\'test_accuracy\']\n\t\tnp.save(\'../../prior_best/testaccuracy.npy\',test_accuracy)\n\n\n\tdef predict_image(self, image_path):\n\t\timage = np.array(ndimage.imread(image_path, flatten = False))\n\t\tX = scipy.misc.imresize(image, size=(64,64)).reshape((1, 64*64*3)).T\n\n\t\treturn self.predict(X)\n'"
1.2/src/classifier/model_processes.py,0,"b'from ResNet50 import ResNet50\nimport sys, os\n\ndef train_new_model():\n    print(""Training new model under alias recent_model"")\n    epochs = int(sys.argv[2])\n    batch_size = int(sys.argv[3])\n\n    model = ResNet50(input_shape = (64, 64, 3), classes = 6)\n\n    model.load_data_h5("".."" + os.path.sep + "".."" + os.path.sep + "".."" + os.path.sep + ""Practice_Data"" + os.path.sep)\n\n    model.train_model(epochs, batch_size)\n\n    model.evaluate_model()\n\n    model.save_model(model = ""recent_model"")\n\n    del model\n\ndef update_model():\n    source_model_alias = sys.argv[2]\n    destination_model_alias = sys.argv[3]\n    previous_loss = float(sys.argv[4])\n    previous_accuracy = float(sys.argv[5])\n\n    # Initialize a ResNet50 model to use in evaluation\n    model = ResNet50(input_shape = (64, 64, 3), classes = 6)\n\n    # Load a model given a source model alias\n    model.load_model(source_model_alias)\n\n    # Load the test and train data into the model\n    model.load_data_h5("".."" + os.path.sep + "".."" + os.path.sep + "".."" + os.path.sep + ""Practice_Data"" + os.path.sep)\n\n    new_loss, new_accuracy = model.evaluate_model()\n\n    if new_loss < previous_loss and new_accuracy > previous_accuracy:\n        print(""New model is better than previous best for given alias.  Saving model under alias:"", destination_model_alias)\n        model.save_model(destination_model_alias)\n    else:\n        print(""Previous model is superior.  Can\'t say we didn\'t try :-)"")\n\n    del model\n\n\nif __name__ == ""__main__"":\n    if(sys.argv[1] == \'1\'):\n        train_new_model()\n    elif(sys.argv[1] == \'2\'):\n        update_model()\n'"
1.2/src/classifier/resnet_utils.py,11,"b'import os\nimport numpy as np\nimport tensorflow as tf\nimport h5py\nimport math\n\ndef load_dataset(relative_directory_path):\n    train_dataset = h5py.File(relative_directory_path + \'train_signs.h5\', ""r"")\n    train_set_x_orig = np.array(train_dataset[""train_set_x""][:]) # your train set features\n    train_set_y_orig = np.array(train_dataset[""train_set_y""][:]) # your train set labels\n\n    test_dataset = h5py.File(relative_directory_path + \'test_signs.h5\', ""r"")\n    test_set_x_orig = np.array(test_dataset[""test_set_x""][:]) # your test set features\n    test_set_y_orig = np.array(test_dataset[""test_set_y""][:]) # your test set labels\n\n    classes = np.array(test_dataset[""list_classes""][:]) # the list of classes\n    \n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n    \n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n\n\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    """"""\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n    Y -- true ""label"" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)\n    mini_batch_size - size of the mini-batches, integer\n    seed -- this is only for the purpose of grading, so that you\'re ""random minibatches are the same as ours.\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    """"""\n    \n    m = X.shape[0]                  # number of training examples\n    mini_batches = []\n    np.random.seed(seed)\n    \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[permutation,:,:,:]\n    shuffled_Y = Y[permutation,:]\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches\n\n\ndef convert_to_one_hot(Y, C):\n    Y = np.eye(C)[Y.reshape(-1)].T\n    return Y\n\n\ndef forward_propagation_for_predict(X, parameters):\n    """"""\n    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n    \n    Arguments:\n    X -- input dataset placeholder, of shape (input size, number of examples)\n    parameters -- python dictionary containing your parameters ""W1"", ""b1"", ""W2"", ""b2"", ""W3"", ""b3""\n                  the shapes are given in initialize_parameters\n    Returns:\n    Z3 -- the output of the last LINEAR unit\n    """"""\n    \n    # Retrieve the parameters from the dictionary ""parameters"" \n    W1 = parameters[\'W1\']\n    b1 = parameters[\'b1\']\n    W2 = parameters[\'W2\']\n    b2 = parameters[\'b2\']\n    W3 = parameters[\'W3\']\n    b3 = parameters[\'b3\'] \n                                                           # Numpy Equivalents:\n    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n    \n    return Z3\n\ndef predict(X, parameters):\n    W1 = tf.convert_to_tensor(parameters[""W1""])\n    b1 = tf.convert_to_tensor(parameters[""b1""])\n    W2 = tf.convert_to_tensor(parameters[""W2""])\n    b2 = tf.convert_to_tensor(parameters[""b2""])\n    W3 = tf.convert_to_tensor(parameters[""W3""])\n    b3 = tf.convert_to_tensor(parameters[""b3""])\n    \n    params = {""W1"": W1,\n              ""b1"": b1,\n              ""W2"": W2,\n              ""b2"": b2,\n              ""W3"": W3,\n              ""b3"": b3}\n    \n    x = tf.placeholder(""float"", [12288, 1])\n    \n    z3 = forward_propagation_for_predict(x, params)\n    p = tf.argmax(z3)\n    \n    sess = tf.Session()\n    prediction = sess.run(p, feed_dict = {x: X})\n        \n    return prediction'"
1.2/src/classifier/test_suite.py,2,"b'\xef\xbb\xbfimport matplotlib.pyplot as plt\nimport numpy as np\nfrom keras import layers\nfrom keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\nfrom keras.models import Model, load_model\nfrom keras.preprocessing import image\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.applications.imagenet_utils import preprocess_input\nimport pydot\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils import plot_model\nfrom resnet_utils import *\nfrom keras.initializers import glorot_uniform\nimport scipy.misc\n\nimport keras.backend as K\nK.set_image_data_format(\'channels_last\')\nK.set_learning_phase(1)\n\ndef identity_block(X, f, filters, stage, block):\n    """"""\n    Implementation of the identity block as defined in 1.2/reference_images/identity_block.png\n    \n    Arguments:\n    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n    f -- integer, specifying the shape of the middle CONV\'s window for the main path\n    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n    stage -- integer, used to name the layers, depending on their position in the network\n    block -- string/character, used to name the layers, depending on their position in the network\n    \n    Returns:\n    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n    """"""\n    \n    # defining name basis\n    conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n    bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n    \n    # Retrieve Filters\n    F1, F2, F3 = filters\n    \n    # Save the input value. You\'ll need this later to add back to the main path. \n    X_shortcut = X\n    \n    # First component of main path\n    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = \'valid\', name = conv_name_base + \'2a\', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + \'2a\')(X)\n    X = Activation(\'relu\')(X)\n    \n    # Second component of main path (\xe2\x89\x883 lines)\n    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = \'same\', name = conv_name_base + \'2b\', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + \'2b\')(X)\n    X = Activation(\'relu\')(X)\n\n    # Third component of main path (\xe2\x89\x882 lines)\n    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = \'valid\', name = conv_name_base + \'2c\', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + \'2c\')(X)\n\n    # Final step: Add shortcut value to main path, and pass it through a RELU activation (\xe2\x89\x882 lines)\n    X = Add()([X_shortcut, X])\n    X = Activation(\'relu\')(X)\n    \n    return X\n\ndef test_identity_block():\n    tf.reset_default_graph()\n\n    with tf.Session() as test:\n        A_prev = tf.placeholder(""float"", [3, 4, 4, 6])\n        X = np.random.randn(3, 4, 4, 6)\n        A = identity_block(A_prev, f = 2, filters = [2, 4, 6], stage = 1, block = \'a\')\n        test.run(tf.global_variables_initializer())\n        out = test.run([A], feed_dict={A_prev: X, K.learning_phase(): 0})\n        print(""out = "" + str(out[0][1][1][0]))\n\n\ndef convolutional_block(X, f, filters, stage, block, s = 2):\n    """"""\n    Implementation of the convolutional block as defined in 1.2/reference_images/convolutional_block.png\n    \n    Arguments:\n    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n    f -- integer, specifying the shape of the middle CONV\'s window for the main path\n    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n    stage -- integer, used to name the layers, depending on their position in the network\n    block -- string/character, used to name the layers, depending on their position in the network\n    s -- Integer, specifying the stride to be used\n    \n    Returns:\n    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n    """"""\n    \n    # defining name basis\n    conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n    bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n    \n    # Retrieve Filters\n    F1, F2, F3 = filters\n    \n    # Save the input value\n    X_shortcut = X\n\n\n    ##### MAIN PATH #####\n    # First component of main path \n    X = Conv2D(F1, (1, 1), strides = (s,s), padding = \'valid\', name = conv_name_base + \'2a\', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + \'2a\')(X)\n    X = Activation(\'relu\')(X)\n\n    # Second component of main path (\xe2\x89\x883 lines)\n    X = Conv2D(F2, (f, f), strides = (1,1), padding = \'same\', name = conv_name_base + \'2b\', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + \'2b\')(X)\n    X = Activation(\'relu\')(X)\n\n    # Third component of main path (\xe2\x89\x882 lines)\n    X = Conv2D(F3, (1, 1), strides = (1,1), padding = \'valid\', name = conv_name_base + \'2c\', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + \'2c\')(X)\n\n    ##### SHORTCUT PATH #### (\xe2\x89\x882 lines)\n    X_shortcut = Conv2D(F3, (1, 1), strides = (s,s), padding = \'valid\', name = conv_name_base + \'1\', kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + \'1\')(X_shortcut)\n\n    # Final step: Add shortcut value to main path, and pass it through a RELU activation (\xe2\x89\x882 lines)\n    X = Add()([X, X_shortcut])\n    X = Activation(\'relu\')(X)\n    \n    return X\n\ndef test_convolutional_block():\n    tf.reset_default_graph()\n\n    with tf.Session() as test:\n        A_prev = tf.placeholder(""float"", [3, 4, 4, 6])\n        X = np.random.randn(3, 4, 4, 6)\n        A = convolutional_block(A_prev, f = 2, filters = [2, 4, 6], stage = 1, block = \'a\')\n        test.run(tf.global_variables_initializer())\n        out = test.run([A], feed_dict={A_prev: X, K.learning_phase(): 0})\n        print(""out = "" + str(out[0][1][1][0]))\n\ndef ResNet50(input_shape = (64, 64, 3), classes = 6):\n    """"""\n    Implementation of the popular ResNet50 the following architecture:\n    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n\n    Arguments:\n    input_shape -- shape of the images of the dataset\n    classes -- integer, number of classes\n\n    Returns:\n    model -- a Model() instance in Keras\n    """"""\n    \n    # Define the input as a tensor with shape input_shape\n    X_input = Input(input_shape)\n\n    \n    # Zero-Padding\n    X = ZeroPadding2D((3, 3))(X_input)\n    \n    # Stage 1\n    X = Conv2D(64, (7, 7), strides = (2, 2), name = \'conv1\', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = \'bn_conv1\')(X)\n    X = Activation(\'relu\')(X)\n    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n\n    # Stage 2\n    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block=\'a\', s = 1)\n    X = identity_block(X, 3, [64, 64, 256], stage=2, block=\'b\')\n    X = identity_block(X, 3, [64, 64, 256], stage=2, block=\'c\')\n\n    # Stage 3 (\xe2\x89\x884 lines)\n    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block=\'a\', s = 2)\n    X = identity_block(X, 3, [128, 128, 512], stage=3, block=\'b\')\n    X = identity_block(X, 3, [128, 128, 512], stage=3, block=\'c\')\n    X = identity_block(X, 3, [128, 128, 512], stage=3, block=\'d\')\n\n    # Stage 4 (\xe2\x89\x886 lines)\n    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block=\'a\', s = 2)\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block=\'b\')\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block=\'c\')\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block=\'d\')\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block=\'e\')\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block=\'f\')\n\n    # Stage 5 (\xe2\x89\x883 lines)\n    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block=\'a\', s = 2)\n    X = identity_block(X, 3, [512, 512, 2048], stage=5, block=\'b\')\n    X = identity_block(X, 3, [512, 512, 2048], stage=5, block=\'c\')\n\n    # AVGPOOL (\xe2\x89\x881 line). Use ""X = AveragePooling2D(...)(X)""\n    X = AveragePooling2D(pool_size=(2, 2), name=\'avg_pool\')(X)\n\n    # output layer\n    X = Flatten()(X)\n    X = Dense(classes, activation=\'softmax\', name=\'fc\' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n    \n    \n    # Create model\n    model = Model(inputs = X_input, outputs = X, name=\'ResNet50\')\n\n    return model\n\ndef test_ResNet50(epochs = 2, batch_size = 32):\n    model = ResNet50(input_shape = (64, 64, 3), classes = 6)\n    model.compile(optimizer=\'adam\', loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])\n\n    X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset(""../../../Practice_Data/"")\n\n    # Normalize image vectors\n    X_train = X_train_orig/255.\n    X_test = X_test_orig/255.\n\n    # Convert training and test labels to one hot matrices\n    Y_train = convert_to_one_hot(Y_train_orig, 6).T\n    Y_test = convert_to_one_hot(Y_test_orig, 6).T\n\n    print (""number of training examples = "" + str(X_train.shape[0]))\n    print (""number of test examples = "" + str(X_test.shape[0]))\n    print (""X_train shape: "" + str(X_train.shape))\n    print (""Y_train shape: "" + str(Y_train.shape))\n    print (""X_test shape: "" + str(X_test.shape))\n    print (""Y_test shape: "" + str(Y_test.shape))\n\n    model.fit(X_train, Y_train, epochs = epochs, batch_size = batch_size)\n\n    preds = model.evaluate(X_test, Y_test)\n    print (""Loss = "" + str(preds[0]))\n    print (""Test Accuracy = "" + str(preds[1]))\n\n\nif __name__ == ""__main__"":\n    test_ResNet50()'"
1.2/src/response_system/main.py,0,"b'import pigpio\nimport time\nimport random\nfrom gpiozero import Button\nimport serial\n\nser = serial.Serial(\n  port=\'/dev/ttyS0\',\n  baudrate = 115200,\n  parity = serial.PARITY_NONE,\n  stopbits=serial.STOPBITS_ONE,\n  bytesize=serial.EIGHTBITS,\n  timeout=1\n)\n\npi = pigpio.pi()\n\n# Set pins\nRED_PIN = 27\nGREEN_PIN = 17\nBLUE_PIN = 22\n\ndef toMaxColor(color, value):\n  for i in range(1, value, 1):\n    pi.set_PWM_dutycycle(color, i)\n    time.sleep(0.05)\n\ndef toMinColor(color, value):\n  for i in range(value, 1, -1):\n    pi.set_PWM_dutycycle(color, i)\n    time.sleep(0.05)\n\ndef toMaxColors(colorOne, colorTwo, value):\n  for i in range(1, value, 1):\n    pi.set_PWM_dutycycle(colorOne, i)\n    pi.set_PWM_dutycycle(colorTwo, i)\n    time.sleep(0.05)\n\ndef turnOffLEDs():\n  pi.set_PWM_dutycycle(RED_PIN, 0)\n  pi.set_PWM_dutycycle(GREEN_PIN, 0)\n  pi.set_PWM_dutycycle(BLUE_PIN, 0)\n\ndef getValue():\n  turnOffLEDs()\n  net_num = random.randint(0,6)\n  setValue(net_num)\n\ndef setValue(net_num):\n  if net_num == 0:\n    intensity = 0\n  elif net_num == 1:\n    intensity = 43\n  elif net_num == 2:\n    intensity = 86\n  elif net_num == 3:\n    intensity = 129\n  elif net_num == 4:\n    intensity = 172\n  elif net_num == 5:\n    intensity = 215\n  else:\n    intensity = 255\n  return intensity\n\ndef promptForColorAndValue():\n  color = raw_input(""[R], [G], or [B]: "")\n  value = raw_input(""Enter value: "")\n  value = int(value) # cast value to int\n  if color is ""R"" and value >= 1 and value <= 255:\n    pi.set_PWM_dutycycle(RED_PIN, value)\n  elif color is ""G"" and value >= 1 and value <= 255:\n    pi.set_PWM_dutycycle(GREEN_PIN, value)\n  elif color is ""B"" and value >= 1 and value <= 255:\n    pi.set_PWM_dutycycle(BLUE_PIN, value)\n  else:\n    print(""Invalid selection."")\n  menu()\n\ndef promptRandomColorSingle():\n  color = raw_input(""[R], [G], or [B]: "")\n  if color is ""R"":\n    net_num = random.randint(0,6)\n    intensity = setValue (net_num)\n    pi.set_PWM_dutycycle(RED_PIN, intensity)\n  if color is ""G"":\n    net_num = random.randint(0,6)\n    intensity = setValue (net_num)\n    pi.set_PWM_dutycycle(GREEN_PIN, intensity)\n  if color is ""B"":\n    net_num = random.randint(0,6)\n    intensity = setValue (net_num)\n    pi.set_PWM_dutycycle(BLUE_PIN, intensity)\n    menu()\n\ndef promptRandomColorAll():\n  net_num1 = random.randint(0,6)\n  net_num2 = random.randint(0,6)\n  net_num3 = random.randint(0,6)\n  intensity1 = setValue(net_num1)\n  intensity2 = setValue (net_num2)\n  intensity3 = setValue (net_num3)\n  pi.set_PWM_dutycycle(RED_PIN, intensity1)\n  pi.set_PWM_dutycycle(GREEN_PIN, intensity2)\n  pi.set_PWM_dutycycle(BLUE_PIN, intensity3)\n  menu()\n\ndef promptScanSign():\n  raw_input(""0 is RED\\n1 is GREEN\\n2 is BLUE\\n3 is MAGENTA\\n4 is YELLOW\\n5 is CYAN"")\n  temp()\n\ndef set_color_strip(color_enum):\n  if color_enum == 0:\n    pi.set_PWM_dutycycle(RED_PIN, 255)\n  elif color_enum == 1:\n    pi.set_PWM_dutycycle(GREEN_PIN, 255)\n  elif color_enum == 2:\n    pi.set_PWM_dutycycle(BLUE_PIN, 255)\n  elif color_enum == 3:\n    pi.set_PWM_dutycycle(BLUE_PIN, 255)\n    pi.set_PWM_dutycycle(RED_PIN, 255)\n  elif color_enum == 4:\n    pi.set_PWM_dutycycle(RED_PIN, 255)\n    pi.set_PWM_dutycycle(GREEN_PIN, 255)\n  lif color_enum == 5:\n    pi.set_PWM_dutycycle(BLUE_PIN, 255)\n    pi.set_PWM_dutycycle(GREEN_PIN, 255)\n\ndef setZero():\n  pi.set_PWM_dutycycle(RED_PIN, 0)\n  pi.set_PWM_dutycycle(GREEN_PIN, 0)\n  pi.set_PWM_dutycycle(BLUE_PIN, 0)\n\ndef temp():\n  color_enum = int(ser.readline())\n  setZero()\n  set_color_strip(color_enum)\n\nbutton = Button(23)\nbutton.when_pressed = temp\n\ntry:\n  while 1:\n    pass\nexcept KeyboardInterrupt:\n  turnOffLEDs()\n  pi.stop()\n'"
