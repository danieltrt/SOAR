file_path,api_count,code
__init__.py,0,b''
breast_cancer.py,21,"b'""""""\nTrain a neural network on breast cancer data\n""""""\n\nfrom decimal import Decimal\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy import genfromtxt\n\n\nclass NeuralNetwork:\n    """"""\n    Neural network implementation using Numpy\n    """"""\n\n    def __init__(self):\n        self.dataset = genfromtxt(\'res/breast-cancer-wisconsin.data\', delimiter=\',\')\n\n        # Remove rows containing nan values\n        self.dataset = self.dataset[~np.isnan(self.dataset).any(axis=1)]\n\n        # Update output values to 0 and 1 (because the sigmoid function outputs between 0 and 1)\n        self.dataset[:, -1] = (self.dataset[:, -1] / 2) - 1\n\n        self.input_data = np.delete(self.dataset, [0, 10], axis=1)\n        self.output_data = np.asarray([[output] for output in self.dataset[:, -1]])\n\n        # The error values\n        self.errors = []\n\n        # Global network properties\n        self.layers = [9, 9, 1]\n\n        self.learning_rate = 0.005\n\n        self.neuron_input = []\n        self.neuron_output = []\n\n        self.neuron_weights = np.array(\n            [np.random.rand(weights_count, self.layers[index]) - 0.5\n             for index, weights_count in enumerate(self.layers[1:])])\n        self.bias_weights = np.array(\n            [np.random.rand(bias_count) - 0.5\n             for bias_count in self.layers[1:]]\n        )\n\n    @staticmethod\n    def sigmoid_activation(val: Decimal) -> float:\n        """"""\n        Sigmoid activation function. This function is used\n        to squash values between 0 and 1.\n        :param val: the variable used in the sigmoid\n        :return: a value between 0 and 1\n        """"""\n        result = 1 / (1 + np.exp(-val))\n        return result\n\n    def sigmoid_derivative(self, val: Decimal) -> float:\n        """"""\n        Return the slope of the sigmoid function at position val\n        :param val: position\n        :return: slope\n        """"""\n        sigmoid = self.sigmoid_activation(val)\n        return sigmoid * (1 - sigmoid)\n\n    @staticmethod\n    def cost_function(val: float, target: float) -> float:\n        """"""\n        Calculate the cost using the ""sum of squares"" function\n        :param val: calculated output\n        :param target: expected output\n        :return: cost\n        """"""\n        return (1 / 2) * (abs(target - val)) ** 2\n\n    def forward_pass(self, input_data: list) -> list:\n        """"""\n        The forward pass calculates the output of the neural network based on the given input values\n        :type input_data: list\n        :return: Output of the neural network\n        """"""\n\n        # Setup\n        self.neuron_input = []\n        self.neuron_output = []\n\n        current_layer = 0\n        current_input = input_data\n\n        # Start with the input layer\n        self.neuron_input.append(current_input)\n        self.neuron_output.append(current_input)\n\n        current_layer += 1\n\n        # Continue with hidden layers\n        hidden_layers = self.layers[1:]\n        for _ in hidden_layers:\n            layer_weights = self.neuron_weights[current_layer - 1]\n\n            # multiply weights by input\n            layer_weights = layer_weights.dot(current_input)\n\n            new_input = []\n            neuron_inputs = [weighted_input + bias for weighted_input, bias in\n                             zip(layer_weights, self.bias_weights[current_layer - 1])]\n\n            for activation_input in neuron_inputs:\n                new_input.append(self.sigmoid_activation(activation_input))\n\n            self.neuron_input.append(neuron_inputs)\n\n            current_input = new_input\n            self.neuron_output.append(new_input)\n            current_layer += 1\n\n        # End with output layer\n        output = current_input\n\n        return output\n\n    def backward_pass(self, input_data: list, output_data: list) -> None:\n        """"""\n        Do the backward pass of the backpropagation algorithm.\n        Calculate the total error and calculate the error gradients (= current derivative)\n        for every output. After that, propagate the error gradient back over the entire network.\n        Use the error gradients to update the weights by the learning rate.\n        :param input_data: training input data for the forward pass\n        :param output_data: expected output for the provided input data\n        :return: None\n        """"""\n        input_data = input_data\n        expected_output = np.asarray(output_data)\n        actual_output = np.asarray(self.forward_pass(input_data))\n        errors = []\n        gradients = []\n\n        # Start by calculating errors using backpropagation\n        error_signal = actual_output - expected_output\n        output_layer_input_derivative = np.asarray(\n            [self.sigmoid_derivative(neuron_input) for neuron_input in self.neuron_input[-1]])\n\n        errors.append(np.multiply(error_signal, output_layer_input_derivative))\n\n        for index, weights in enumerate(self.neuron_weights[::-1]):\n            # Calculate the hadamar product of the derivative of neuron inputs (layer k) and\n            # the weights matrix (layer k) transposed multiplied by the error matrix (layer k + 1)\n            errors.append(np.multiply(\n                [self.sigmoid_derivative(neuron_input) for neuron_input in\n                 self.neuron_input[- index - 2]],\n                np.dot(weights.T, errors[-1])\n            ))\n\n        errors.reverse()  # backpropagation works from the end to the beginning of the network \xce\xb4\n\n        for index, (layer_error, neuron_output) in enumerate(\n                zip(errors[1:], np.array(self.neuron_output))):\n            neuron_output = np.array(neuron_output).reshape(-1, 1)\n            gradients.append(np.array(np.outer(layer_error, neuron_output)))\n\n        # Update weights according to gradients\n        self.neuron_weights -= np.multiply(gradients, self.learning_rate)\n\n        # Update biases according to the gradients\n        self.bias_weights -= np.multiply(errors[1:], self.learning_rate)\n\n    def train(self, epochs: int, plot: bool = False) -> None:\n        """"""\n        Train the neural network\n        :param epochs: The number of times to loop through the training dataset\n        :param plot: Show a graph of the error progression during training after training is done\n        :return: None\n        """"""\n        data_size = self.input_data.shape[0]\n        training_rows = int(data_size * .8)\n        test_rows = data_size - training_rows\n\n        # One epoch goes once through the entire training set\n        for current_epoch in range(epochs):\n            # Shuffle the training data for better training accuracy\n            combined_training_data = np.column_stack((\n                # Training input data\n                self.input_data[:training_rows, ],\n                # Training output data\n                self.output_data[:training_rows, ]))\n            np.random.shuffle(combined_training_data)\n\n            output_columns_count = self.output_data.shape[1]\n            for input_data, output_data in zip(combined_training_data[:, :-output_columns_count],\n                                               combined_training_data[:, -output_columns_count]):\n                self.backward_pass(input_data, output_data)\n\n            epoch_errors = []\n            # Run the training dataset through the updated network\n            for test_input, expected_output in zip(self.input_data[-test_rows:, ],\n                                                   self.output_data[-test_rows:, ]):\n                calculated_output = self.forward_pass(test_input)\n\n                epoch_errors.append(sum(self.cost_function(calculated_output, expected_output)))\n\n            # Add the average error to self.errors\n            self.errors.append(sum(epoch_errors) / float(len(epoch_errors)))\n\n            # Lekker data\'s naar de console sturen voor de gebuiker\n            print(\n                ""Epoch {:d}/{:d} - error: {:f}"".format(current_epoch + 1, epochs, self.errors[-1]))\n\n        if plot:\n            plt.plot(self.errors)\n            plt.show()\n\n\nif __name__ == \'__main__\':\n    NeuralNetwork().train(100, True)\n'"
