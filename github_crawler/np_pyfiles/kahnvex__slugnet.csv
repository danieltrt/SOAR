file_path,api_count,code
setup.py,0,b''
docs-src/conf.py,0,"b'import os\nimport sys\n\n# Source file types:\nsource_suffix = [\'.rst\', \'.md\']\n\n# -*- coding: utf-8 -*-\n#\n# Slugnet documentation build configuration file, created by\n# sphinx-quickstart on Sun, Dec. 3rd 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nsys.path.insert(0, os.path.abspath(\'../\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.githubpages\',\n    # \'sphinx.ext.mathjax\',\n    \'sphinxcontrib.tikz\',\n    \'sphinx.ext.imgmath\',\n    \'sphinx.ext.ifconfig\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\',\n    \'matplotlib.sphinxext.only_directives\',\n    \'matplotlib.sphinxext.plot_directive\',\n    \'IPython.sphinxext.ipython_directive\',\n    \'IPython.sphinxext.ipython_console_highlighting\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'Slugnet\'\ncopyright = u\'2017, <a href=""https://jarrodkahn.com"">Jarrod Kahn</a>\'\nauthor = u\'Jarrod Kahn\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = u\'0.0.1\'\n# The full version, including alpha/beta/rc tags.\nrelease = u\'0.0.1\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'alabaster\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'slugnetdoc\'\n\n\ntikz_tikzlibraries = \'chains\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nimgmath_font_size = 16\nimgmath_image_format = \'svg\'\nimgmath_latex_preamble = """"""\n\\usepackage{graphicx}\n\\usepackage{amsmath, bm}\n\\usepackage{graphicx}\n\\usepackage{xfrac}\n\\usepackage{amssymb}\n\\usepackage{dsfont}\n\\usepackage{algorithm}\n\\usepackage{algpseudocode}\n\\usepackage{tikz}\n\\usepackage{isomath}\n\\usetikzlibrary{decorations.pathreplacing}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{shadows.blur}\n\\usetikzlibrary{arrows.meta}\n\\usetikzlibrary{arrows}\n\\\\newcommand{\\\\thickhat}[1]{\\mathbf{\\hat{\\\\text{$#1$}}}}\n\\\\newcommand{\\R}{\\mathbb{R}}\n\\\\renewcommand{\\\\thesubsection}{\\\\thesection.\\\\alph{subsection}}\n\\let\\oldReturn\\Return\n\\\\renewcommand{\\Return}{\\State\\oldReturn}\n\\DeclareMathOperator*{\\\\argmin}{arg\\,min}\n\n\\makeatletter\n\\def\\BState{\\State\\hskip-\\ALG@thistlm}\n\\makeatother\n""""""\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': (\'\\usepackage{tkiz}\\n\'\n    #              \'\\usetikzlibrary{positioning}\\n\'\n    #              \'\\usepackage{bm}\\n\'\n    #              \'\\usepackage{amsmath}\'),\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'slugnet\', u\'Slugnet\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'Slugnet\', u\'Slugnet Documentation\',\n     author, \'Slugnet\', \'An experimental neural networks library\',\n     \'Miscellaneous\'),\n]\n\nhtml_sidebars = {\'**\': [\n    \'navigation.html\',\n    \'relations.html\',\n    \'sourcelink.html\',\n    \'searchbox.html\'\n], }\n\nsidebar_includehidden = False\n\n# Order autodoc docs by source order\nautodoc_member_order = \'bysource\'\n\ndef setup(app):\n    app.add_stylesheet(\'caption.css\')\n\nhtml_theme_options = {\n    \'font_size\': \'19px\',\n    \'github_user\': \'kahnvex\',\n    \'github_repo\': \'slugnet\',\n    \'github_button\': True,\n    \'github_banner\': True\n}\n'"
docs/activation-1.py,3,"b""import numpy as np\nimport matplotlib.pyplot as plt\n\nz = np.arange(-2, 2, .1)\nzero = np.zeros(len(z))\ny = np.max([zero, z], axis=0)\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(z, y)\nax.set_ylim([-1.0, 2.0])\nax.set_xlim([-2.0, 2.0])\nax.grid(True)\nax.set_xlabel('z')\nax.set_ylabel('phi(z)')\nax.set_title('Rectified linear unit')\n\nplt.show()"""
docs/activation-2.py,2,"b""import numpy as np\nimport matplotlib.pyplot as plt\n\nz = np.arange(-2, 2, .01)\nphi_z = np.tanh(z)\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(z, phi_z)\nax.set_ylim([-1.0, 1.0])\nax.set_xlim([-2.0, 2.0])\nax.grid(True)\nax.set_xlabel('z')\nax.set_ylabel('phi(z)')\nax.set_title('Hyperbolic Tangent')\n\nplt.show()"""
docs/activation-3.py,2,"b""import numpy as np\nimport matplotlib.pyplot as plt\n\nz = np.arange(-4, 4, .01)\nphi_z = 1 / (1 + np.exp(-z))\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(z, phi_z)\nax.set_ylim([0.0, 1.0])\nax.set_xlim([-4.0, 4.0])\nax.grid(True)\nax.set_xlabel('z')\nax.set_ylabel('phi(z)')\nax.set_title('Sigmoid')\n\nplt.show()"""
slugnet/__init__.py,0,b''
slugnet/activation.py,16,"b'import numpy as np\n\n\nclass Activation(object):\n    pass\n\n\nclass Noop(Activation):\n    def call(self, x):\n        return x\n\n    def derivative(self, x=1.):\n        return x\n\n\nclass ReLU(Activation):\n    """"""\n    The common rectified linean unit, or ReLU activation funtion.\n\n    A rectified linear unit implements the nonlinear function\n    :math:`\\phi(z) = \\\\text{max}\\{0, z\\}`.\n\n    .. plot::\n\n        import numpy as np\n        import matplotlib.pyplot as plt\n\n        z = np.arange(-2, 2, .1)\n        zero = np.zeros(len(z))\n        y = np.max([zero, z], axis=0)\n\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        ax.plot(z, y)\n        ax.set_ylim([-1.0, 2.0])\n        ax.set_xlim([-2.0, 2.0])\n        ax.grid(True)\n        ax.set_xlabel(\'z\')\n        ax.set_ylabel(\'phi(z)\')\n        ax.set_title(\'Rectified linear unit\')\n\n        plt.show()\n    """"""\n    def __init__(self):\n        super(ReLU, self).__init__()\n\n    def call(self, x):\n        self.last_forward = x\n\n        return np.maximum(0.0, x)\n\n    def derivative(self, x=None):\n        last_forward = x if x else self.last_forward\n        res = np.zeros(last_forward.shape, dtype=\'float32\')\n        res[last_forward > 0] = 1.\n\n        return res\n\n\nclass Tanh(Activation):\n    """"""\n    The hyperbolic tangent activation function.\n\n    A hyperbolic tangent activation function implements the\n    nonlinearity given by :math:`\\phi(z) = \\\\text{tanh}(z)`, which is\n    equivalent to :math:`\\\\sfrac{\\\\text{sinh}(z)}{\\\\text{cosh}(z)}`.\n\n    .. plot::\n\n        import numpy as np\n        import matplotlib.pyplot as plt\n\n        z = np.arange(-2, 2, .01)\n        phi_z = np.tanh(z)\n\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        ax.plot(z, phi_z)\n        ax.set_ylim([-1.0, 1.0])\n        ax.set_xlim([-2.0, 2.0])\n        ax.grid(True)\n        ax.set_xlabel(\'z\')\n        ax.set_ylabel(\'phi(z)\')\n        ax.set_title(\'Hyperbolic Tangent\')\n\n        plt.show()\n    """"""\n    def call(self, x):\n        self.last_forward = np.tanh(x)\n\n        return self.last_forward\n\n    def derivative(self, x=None):\n        h = self.call(x) if x else self.last_forward\n\n        return 1 - h**2\n\n\nclass Sigmoid(Activation):\n    """"""\n    Represent a probability distribution over two classes.\n\n    The sigmoid function is given by :math:`\\phi(z) = \\\\frac{1}{1 + e^{-z}}`.\n\n    .. plot::\n\n        import numpy as np\n        import matplotlib.pyplot as plt\n\n        z = np.arange(-4, 4, .01)\n        phi_z = 1 / (1 + np.exp(-z))\n\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        ax.plot(z, phi_z)\n        ax.set_ylim([0.0, 1.0])\n        ax.set_xlim([-4.0, 4.0])\n        ax.grid(True)\n        ax.set_xlabel(\'z\')\n        ax.set_ylabel(\'phi(z)\')\n        ax.set_title(\'Sigmoid\')\n\n        plt.show()\n    """"""\n    def call(self, x):\n        self.last_out = 1. / (1. + np.exp(-x))\n\n        return self.last_out\n\n    def derivative(self, x=None):\n        z = self.call(x) if x else self.last_out\n\n        return z * (1 - z)\n\n\nclass Softmax(Activation):\n    r""""""\n    Represent a probability distribution over :math:`n` classes.\n\n    The softmax activation function is given by\n\n    .. math::\n\n        \\phi(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}, \\,\n        \\forall \\, i \\in \\{1, \\dots, K\\}\n\n    where :math:`K` is the number of classes. We can see that softmax is\n    a generalization of the sigmoid function to :math:`n` classes. Below,\n    we derive the sigmoid function using softmax with two classes.\n\n    .. math::\n\n        \\phi(z_1) &= \\frac{e^{z_1}}{\\sum_{i=1}^2 e^{z_i}} \\\\\n                  &= \\frac{1}{e^{z_1 - z_1} + e^{z_2 - z_1}} \\\\\n                  &= \\frac{1}{1 + e^{-z_1}}, \\, \\text{substituting} \\, z_2 = 0\n\n    We substitute :math:`z_2 = 0` because we only need one variable to\n    represent the probability distribution over two classes. This leaves\n    us with the definition of the sigmoid function.\n    """"""\n    def __init__(self):\n        super(Softmax, self).__init__()\n\n    def call(self, x):\n        assert np.ndim(x) == 2\n        self.last_forward = x\n        x = x - np.max(x, axis=1, keepdims=True)\n        exp_x = np.exp(x)\n        s = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n        return s\n\n    def derivative(self, x=None):\n        last_forward = x if x else self.last_forward\n        return np.ones(last_forward.shape, dtype=\'float32\')\n'"
slugnet/initializations.py,12,"b'# -*- coding: utf-8 -*-\n""""""\nFunctions to create initializers for parameter variables.\nExamples\n--------\n>>> from npdl.layers import Dense\n>>> from npdl.initializations import GlorotUniform\n>>> l1 = Dense(n_out=300, n_in=100, init=GlorotUniform())\n""""""\n\nimport copy\nimport numpy as np\n\n\nclass Initializer(object):\n    """"""Base class for parameter weight initializers.\n    The :class:`Initializer` class represents a weight initializer used\n    to initialize weight parameters in a neural network layer. It should be\n    subclassed when implementing new types of weight initializers.\n    """"""\n    def __call__(self, size):\n        """"""Makes :class:`Initializer` instances callable like a function, invoking\n        their :meth:`call()` method.\n        """"""\n        return self.call(size)\n\n    def call(self, size):\n        """"""Sample should return a numpy.array of size shape and data type\n        ``numpy.float32``.\n\n        Parameters\n        ----------\n        size : tuple or int.\n            Integer or tuple specifying the size of the returned\n            matrix.\n\n        Returns\n        -------\n        numpy.array.\n            Matrix of size shape and dtype ``numpy.float32``.\n        """"""\n        raise NotImplementedError()\n\n    def __str__(self):\n        return self.__class__.__name__\n\n\nclass Zero(Initializer):\n    """"""Initialize weights with zero value.\n    """"""\n    def call(self, size):\n        return _cast_dtype(np.zeros(size))\n\n\nclass One(Initializer):\n    """"""Initialize weights with one value.\n    """"""\n    def call(self, size):\n        return _cast_dtype(np.ones(size))\n\n\nclass Uniform(Initializer):\n    """"""Sample initial weights from the uniform distribution.\n    Parameters are sampled from U(a, b).\n\n    Parameters\n    ----------\n    scale : float or tuple.\n        When std is None then range determines a, b. If range is a float the\n        weights are sampled from U(-range, range). If range is a tuple the\n        weights are sampled from U(range[0], range[1]).\n    """"""\n    def __init__(self, scale=0.05):\n        self.scale = scale\n\n    def call(self, size):\n        return _cast_dtype(np.random.uniform(-self.scale,\n                                             self.scale, size=size))\n\n\nclass Normal(Initializer):\n\n    """"""Sample initial weights from the Gaussian distribution.\n    Initial weight parameters are sampled from N(mean, std).\n\n    Parameters\n    ----------\n    std : float.\n        Std of initial parameters.\n    mean : float.\n        Mean of initial parameters.\n    """"""\n    def __init__(self, std=0.01, mean=0.0):\n        self.std = std\n        self.mean = mean\n\n    def call(self, size):\n        return _cast_dtype(np.random.normal(loc=self.mean,\n                                            scale=self.std, size=size))\n\n\nclass LecunUniform(Initializer):\n    """"""LeCun uniform initializer.\n    It draws samples from a uniform distribution within [-limit, limit]\n    where `limit` is `sqrt(3 / fan_in)` [1]_\n    where `fan_in` is the number of input units in the weight matrix.\n\n    References\n    ----------\n    .. [1] LeCun 98, Efficient Backprop, http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n\n    """"""\n    def call(self, size):\n        fan_in, fan_out = decompose_size(size)\n        return Uniform(np.sqrt(3. / fan_in))(size)\n\n\nclass GlorotUniform(Initializer):\n    """"""Glorot uniform initializer, also called Xavier uniform initializer.\n    It draws samples from a uniform distribution within [-limit, limit]\n    where `limit` is `sqrt(6 / (fan_in + fan_out))` [1]_\n    where `fan_in` is the number of input units in the weight matrix\n    and `fan_out` is the number of output units in the weight matrix.\n\n    References\n    ----------\n    .. [1] Glorot & Bengio, AISTATS 2010. http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n    """"""\n    def call(self, size):\n        fan_in, fan_out = decompose_size(size)\n        return Uniform(np.sqrt(6 / (fan_in + fan_out)))(size)\n\n\nclass GlorotNormal(Initializer):\n    """"""Glorot normal initializer, also called Xavier normal initializer.\n    It draws samples from a truncated normal distribution centered on 0\n    with `stddev = sqrt(2 / (fan_in + fan_out))` [1]_\n    where `fan_in` is the number of input units in the weight matrix\n    and `fan_out` is the number of output units in the weight matrix.\n\n    References\n    ----------\n    .. [1] Glorot & Bengio, AISTATS 2010. http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n    """"""\n    def call(self, size):\n        fan_in, fan_out = decompose_size(size)\n        return Normal(np.sqrt(2 / (fan_out + fan_in)))(size)\n\n\nclass HeNormal(Initializer):\n    """"""He normal initializer.\n    It draws samples from a truncated normal distribution centered on 0\n    with `stddev = sqrt(2 / fan_in)` [1]_\n    where `fan_in` is the number of input units in the weight matrix.\n\n    References\n    ----------\n    .. [1] He et al., http://arxiv.org/abs/1502.01852\n    """"""\n    def call(self, size):\n        fan_in, fan_out = decompose_size(size)\n        return Normal(np.sqrt(2. / fan_in))(size)\n\n\nclass HeUniform(Initializer):\n    """"""He uniform variance scaling initializer.\n    It draws samples from a uniform distribution within [-limit, limit]\n    where `limit` is `sqrt(6 / fan_in)` [1]_\n    where `fan_in` is the number of input units in the weight matrix.\n\n    References\n    ----------\n    .. [1] He et al., http://arxiv.org/abs/1502.01852\n    """"""\n    def call(self, size):\n        fan_in, fan_out = decompose_size(size)\n        return Uniform(np.sqrt(6. / fan_in))(size)\n\n\ndef decompose_size(size):\n    """"""Computes the number of input and output units for a weight shape.\n\n    Parameters\n    ----------\n    size\n        Integer shape tuple.\n\n    Returns\n    -------\n    A tuple of scalars, `(fan_in, fan_out)`.\n    """"""\n    if len(size) == 2:\n        fan_in = size[0]\n        fan_out = size[1]\n\n    elif len(size) == 4 or len(size) == 5:\n        respective_field_size = np.prod(size[2:])\n        fan_in = size[1] * respective_field_size\n        fan_out = size[0] * respective_field_size\n\n    else:\n        fan_in = fan_out = int(np.sqrt(np.prod(size)))\n\n    return fan_in, fan_out\n\n\ndef _cast_dtype(res):\n    return np.array(res, dtype=\'float32\')\n\n\n_zero = Zero()\n_one = One()\n'"
slugnet/layers.py,47,"b'import numpy as np\n\nfrom slugnet.activation import Noop, ReLU\nfrom slugnet.initializations import _zero, GlorotUniform\n\n\nclass Layer(object):\n    first_layer = False\n\n    def set_first_layer(self):\n        self.first_layer = True\n\n    def get_params(self):\n        return []\n\n    def get_grads(self):\n        return []\n\n\nclass Dense(Layer):\n    r""""""\n    A common densely connected neural network layer.\n\n    :param outshape: The output shape at this layer.\n    :type outshape: int\n\n    :param inshape: The input shape at this layer.\n    :type inshape: int\n\n    :param activation: The activation function to be used at the layer.\n    :type activation: slugnet.activation.Activation\n\n    :param init: The initialization function to be used\n    :type init: slugnet.initializations.Initializer\n    """"""\n\n    def __init__(self, outshape, inshape=None, activation=Noop(),\n                 init=GlorotUniform()):\n        self.outshape = None, outshape\n        self.activation = activation\n        self.inshape = inshape\n        self.init = init\n\n    def connect(self, prev_layer=None):\n        if prev_layer:\n            if len(prev_layer.outshape) != 2:\n                raise ValueError(\'Previous layers outshape is incompatible\')\n\n            self.inshape = prev_layer.outshape[-1]\n        elif not self.inshape:\n            raise ValueError(\'inshape must be given to first layer of network\')\n\n        self.shape = self.inshape, self.outshape[-1]\n        self.w = self.init(self.shape)\n        self.b = _zero((self.outshape[-1], ))\n        self.dw = None\n        self.db = None\n\n    def call(self, X, *args, **kwargs):\n        self.last_X = X\n\n        return self.activation.call(np.dot(X, self.w) + self.b)\n\n    def backprop(self, nxt_grad):\n        """"""\n        Computes the derivative for the next layer\n        and computes update for this layers weights\n        """"""\n        act_grad = nxt_grad * self.activation.derivative()\n        self.dw = np.dot(self.last_X.T, act_grad)\n        self.db = np.mean(act_grad, axis=0)\n\n        return np.dot(act_grad, self.w.T)\n\n    def get_params(self):\n        return self.w, self.b\n\n    def get_grads(self):\n        return self.dw, self.db\n\n\nclass Dropout(Layer):\n    r""""""\n    A layer that removes units from a network with probability :code:`p`.\n\n    :param p: The probability of a non-ouput node being removed from the network.\n    :type p: float\n    """"""\n    def __init__(self, p=0.0, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.p = p\n\n    def connect(self, prev_layer=None):\n        self.outshape = prev_layer.outshape\n\n    def call(self, X, train=True, *args, **kwargs):\n        if 0. > self.p or self.p > 1.:\n            return X\n\n        if not train:\n            return X * (1 - self.p)\n\n        binomial = np.random.binomial(1, 1 - self.p, X.shape)\n        self.last_mask = binomial / (1 - self.p)\n\n        return X * self.last_mask\n\n    def backprop(self, pre_grad, *args, **kwargs):\n        if 0. < self.p < 1.:\n            return pre_grad * self.last_mask\n\n        return pre_grad\n\n\nclass Convolution(Layer):\n    r""""""\n    A layer that implements the convolution operation.\n\n    :param nb_kernel: The number of kernels to use.\n    :type nb_kernel: int\n\n    :param kernel_size: The size of the kernel as a tuple, heigh by width\n    :type kernel_size: (int, int)\n\n    :param stride: The stide width to use\n    :type stride: int\n\n    :param init: The initializer to use\n    :type init: slugnet.initializations.Initializer\n\n    :param activation: The activation function to be used at the layer.\n    :type activation: slugnet.activation.Activation\n    """"""\n    def __init__(self, nb_kernel, kernel_size, stride=1,\n            inshape=None, init=GlorotUniform(), activation=None):\n\n        if activation is None:\n            activation = ReLU()\n        self.inshape = inshape\n        self.nb_kernel = nb_kernel\n        self.kernel_size = kernel_size\n        self.stride = stride\n\n        self.w, self.b = None, None\n        self.dw, self.db = None, None\n\n        self.outshape = None\n        self.last_output = None\n        self.last_input = None\n\n        self.init = init\n        self.activation = activation\n\n    def connect(self, prev_layer=None):\n        if prev_layer:\n            self.inshape = prev_layer.outshape\n        elif not self.inshape:\n            raise ValueError(\'inshape must be given to first layer of network\')\n\n        prev_nb_kernel = self.inshape[1]\n\n        kernel_h, kernel_w = self.kernel_size\n        self.w = self.init((self.nb_kernel, prev_nb_kernel, kernel_h, kernel_w))\n        self.b = _zero((self.nb_kernel,))\n\n        prev_kh, prev_kw = self.inshape[2], self.inshape[3]\n        height = (prev_kh - kernel_h) // self.stride + 1\n        width = (prev_kw - kernel_w) // self.stride + 1\n        self.outshape = (self.inshape[0], self.nb_kernel, height, width)\n\n    def call(self, X, *args, **kwargs):\n        self.last_input = X\n        batch_size, depth, height, width = X.shape\n        kernel_h, kernel_w = self.kernel_size\n        out_h, out_w = self.outshape[2:]\n\n        outputs = _zero((batch_size, self.nb_kernel, out_h, out_w))\n\n        for x in np.arange(batch_size):\n            for y in np.arange(self.nb_kernel):\n                for h in np.arange(out_h):\n                    for w in np.arange(out_w):\n                        h1, w1 = h * self.stride, w * self.stride\n                        h2, w2 = h1 + kernel_h, w1 + kernel_w\n                        patch = X[x, :, h1: h2, w1: w2]\n                        conv_product = patch * self.w[y]\n                        outputs[x, y, h, w] = np.sum(conv_product) + self.b[y]\n\n        self.last_output = self.activation.call(outputs)\n\n        return self.last_output\n\n    def backprop(self, grad, *args, **kwargs):\n        batch_size, depth, input_h, input_w = self.last_input.shape\n        out_h, out_w = self.outshape[2:]\n        kernel_h, kernel_w = self.kernel_size\n\n        # gradients\n        self.dw = _zero(self.w.shape)\n        self.db = _zero(self.b.shape)\n        delta = grad * self.activation.derivative()\n\n        # dw\n        for r in np.arange(self.nb_kernel):\n            for t in np.arange(depth):\n                for h in np.arange(kernel_h):\n                    for w in np.arange(kernel_w):\n                        input_window = self.last_input[:, t,\n                                       h:input_h - kernel_h + h + 1:self.stride,\n                                       w:input_w - kernel_w + w + 1:self.stride]\n                        delta_window = delta[:, r]\n                        self.dw[r, t, h, w] = np.sum(input_window * delta_window) / batch_size\n\n        # db\n        for r in np.arange(self.nb_kernel):\n            self.db[r] = np.sum(delta[:, r]) / batch_size\n\n        # dX\n        if not self.first_layer:\n            layer_grads = _zero(self.last_input.shape)\n            for b in np.arange(batch_size):\n                for r in np.arange(self.nb_kernel):\n                    for t in np.arange(depth):\n                        for h in np.arange(out_h):\n                            for w in np.arange(out_w):\n                                h1, w1 = h * self.stride, w * self.stride\n                                h2, w2 = h1 + kernel_h, w1 + kernel_w\n                                layer_grads[b, t, h1:h2, w1:w2] += self.w[r, t] * delta[b, r, h, w]\n            return layer_grads\n\n\nclass MeanPooling(Layer):\n    r""""""\n    Pool outputs using the arithmetic mean.\n    """"""\n\n    def __init__(self, pool_size, inshape=None):\n        self.pool_size = pool_size\n\n        self.outshape = None\n        self.inshape = inshape\n\n    def connect(self, prev_layer=None):\n        if prev_layer:\n            self.inshape = prev_layer.outshape\n        elif not self.inshape:\n            raise ValueError(\'inshape must be given to first layer of network\')\n\n        old_h, old_w = self.inshape[-2:]\n        pool_h, pool_w = self.pool_size\n        new_h, new_w = old_h // pool_h, old_w // pool_w\n\n        self.outshape = self.inshape[:-2] + (new_h, new_w)\n\n    def call(self, X, *args, **kwargs):\n        self.inshape = X.shape\n        pool_h, pool_w = self.pool_size\n        new_h, new_w = self.outshape[-2:]\n\n        # forward\n        outputs = _zero(self.inshape[:-2] + self.outshape[-2:])\n\n        if np.ndim(X) == 4:\n            nb_batch, nb_axis, _, _ = X.shape\n\n            for a in np.arange(nb_batch):\n                for b in np.arange(nb_axis):\n                    for h in np.arange(new_h):\n                        for w in np.arange(new_w):\n                            outputs[a, b, h, w] = np.mean(X[a, b, h:h + pool_h, w:w + pool_w])\n\n        elif np.ndim(X) == 3:\n            nb_batch, _, _ = X.shape\n\n            for a in np.arange(nb_batch):\n                for h in np.arange(new_h):\n                    for w in np.arange(new_w):\n                        outputs[a, h, w] = np.mean(X[a, h:h + pool_h, w:w + pool_w])\n\n        else:\n            raise ValueError()\n\n        return outputs\n\n    def backprop(self, pre_grad, *args, **kwargs):\n        new_h, new_w = self.outshape[-2:]\n        pool_h, pool_w = self.pool_size\n        length = np.prod(self.pool_size)\n\n        layer_grads = _zero(self.inshape)\n\n        if np.ndim(pre_grad) == 4:\n            nb_batch, nb_axis, _, _ = pre_grad.shape\n\n            for a in np.arange(nb_batch):\n                for b in np.arange(nb_axis):\n                    for h in np.arange(new_h):\n                        for w in np.arange(new_w):\n                            h1, w1 = h * pool_h, w * pool_w\n                            h2, w2 = h1 + pool_h, w1 + pool_w\n                            layer_grads[a, b, h1:h2, w1:w2] = pre_grad[a, b, h, w] / length\n\n        elif np.ndim(pre_grad) == 3:\n            nb_batch, _, _ = pre_grad.shape\n\n            for a in np.arange(nb_batch):\n                for h in np.arange(new_h):\n                    for w in np.arange(new_w):\n                        h_shift, w_shift = h * pool_h, w * pool_w\n                        layer_grads[a, h_shift: h_shift + pool_h, w_shift: w_shift + pool_w] = \\\n                            pre_grad[a, h, w] / length\n\n        else:\n            raise ValueError()\n\n        return layer_grads\n\n\nclass Flatten(Layer):\n    def __init__(self, outdim=2, inshape=None):\n        self.outdim = outdim\n        if outdim < 1:\n            raise ValueError(\'Dim must be >0, was %i\', outdim)\n\n        self.last_input_shape = None\n        self.outshape = None\n        self.inshape = inshape\n\n    def connect(self, prev_layer=None):\n        if prev_layer:\n            self.inshape = prev_layer.outshape\n        elif not self.inshape:\n            raise ValueError(\'inshape must be given to first layer of network\')\n\n        to_flatten = np.prod(self.inshape[self.outdim - 1:])\n        flattened_shape = self.inshape[:self.outdim - 1] + (to_flatten,)\n        self.outshape = flattened_shape\n\n    def call(self, X, *args, **kwargs):\n        self.last_input_shape = X.shape\n\n        # to_flatten = np.prod(self.last_input_shape[self.outdim-1:])\n        # flattened_shape = input.shape[:self.outdim-1] + (to_flatten, )\n        flattened_shape = X.shape[:self.outdim - 1] + (-1,)\n        return np.reshape(X, flattened_shape)\n\n    def backprop(self, pre_grad, *args, **kwargs):\n        return np.reshape(pre_grad, self.last_input_shape)\n'"
slugnet/loss.py,8,"b'import numpy as np\n\n\nclass Objective(object):\n    """"""An objective function (or loss function, or optimization score\n    function) is one of the two parameters required to compile a model.\n\n    """"""\n    def forward(self, outputs, targets):\n        raise NotImplementedError()\n\n    def backward(self, outputs, targets):\n        """"""Backward function.\n\n        Parameters\n        ----------\n        outputs, targets : numpy.array\n            The arrays to compute the derivatives of them.\n\n        Returns\n        -------\n        numpy.array\n            An array of derivative.\n        """"""\n        raise NotImplementedError()\n\n    def __str__(self):\n        return self.__class__.__name__\n\n\nclass BinaryCrossEntropy(Objective):\n    r""""""\n    Standard binary cross-entropy loss function.\n\n    Binary cross-entropy is given by\n\n    .. math::\n\n        \\bm{\\ell}(\\bm{\\hat{y}}, \\bm{y}) = - \\frac{1}{N} \\sum_{i=1}^N\n            [\\bm{y}_i \\, \\text{log}(\\bm{\\hat{y}}_i) + (1 - \\bm{y}_i) \\text{log}(1 - \\bm{\\hat{y}}_i)]\n\n    """"""\n    def __init__(self, epsilon=1e-11):\n        self.epsilon = epsilon\n\n    def forward(self, yh, y):\n        yh = np.clip(yh, self.epsilon, 1 - self.epsilon)\n        loss = -np.sum(y * np.log(yh) + (1 - y) * np.log(1 - yh), axis=1)\n        return np.mean(loss)\n\n    def backward(self, outputs, targets):\n        outputs = np.clip(outputs, self.epsilon, 1 - self.epsilon)\n        divisor = np.maximum(outputs * (1 - outputs), self.epsilon)\n\n        return (outputs - targets) / divisor\n\n\nclass SoftmaxCategoricalCrossEntropy(Objective):\n    def __init__(self, epsilon=1e-11):\n        self.epsilon = epsilon\n\n    def forward(self, outputs, targets):\n        outputs = np.clip(outputs, self.epsilon, 1 - self.epsilon)\n        return np.mean(-np.sum(targets * np.log(outputs), axis=1))\n\n    def backward(self, outputs, targets):\n        outputs = np.clip(outputs, self.epsilon, 1 - self.epsilon)\n        return outputs - targets\n'"
slugnet/model.py,8,"b'import numpy as np\n\nfrom tabulate import tabulate\nfrom tqdm import tqdm, trange\n\nfrom slugnet.optimizers import SGD\nfrom slugnet.loss import BinaryCrossEntropy\nfrom sklearn.model_selection import train_test_split\n\n\nclass Model(object):\n    """"""\n    A model implement functionality for fitting a neural network and\n    making predictions.\n\n    Parameters\n    ----------\n\n    :param lr: The learning rate to be used during training.\n    :type lr: float\n\n    :param n_epoch: The number of training epochs to use.\n    :type n_epoch: int\n\n    :param batch_size: The size of each batch for training.\n    :type batch_size: int\n\n    :param layers: Initial layers to add the the network, more can\n        be added layer using the :code:`model.add_layer` method.\n    :type layers: list[slugnet.layers.Layer]\n\n    :param optimizer: The opimization method to use during training.\n    :type optimizer: slugnet.optimizers.Optimizer\n\n    :param loss: The loss function to use during training and validation.\n    :type loss: slugnet.loss.Objective\n\n    :param validation_split: The percent of data to use for validation,\n        default is zero.\n    :type validation_split: float\n\n    :param metrics: The metrics to print during training, options are\n        :code:`loss` and :code:`accuracy`.\n    :type metrics: list[str]\n\n    :param progress: Display progress-bar while training.\n    :type progress: bool\n\n    :param log_interval: The epoch interval on which to print progress.\n    :type log_interval: int\n    """"""\n    def __init__(self, lr=0.1, n_epoch=400000, batch_size=32, layers=None,\n                 optimizer=SGD(), loss=BinaryCrossEntropy(),\n                 validation_split=0.2, metrics=[\'loss\'], progress=True,\n                 log_interval=1):\n        self.layers = layers if layers else []\n        self.lr = lr\n        self.n_epoch = n_epoch\n        self.optimizer = optimizer\n        self.loss = loss\n        self.batch_size = batch_size\n        self.validation_split = validation_split\n        self.metrics = metrics\n        self.progress = progress\n        self.log_interval = log_interval\n        self.compiled = False\n\n    def add_layer(self, layer):\n        self.layers.append(layer)\n\n    def compile(self):\n        for i, layer in enumerate(self.layers):\n            if i == 0:\n                layer.connect()\n            else:\n                layer.connect(self.layers[i - 1])\n\n        self.layers[0].set_first_layer()\n        self.compiled = True\n\n    def get_n_output(self):\n        out_layer = self.layers[-1]\n        ind, outd = out_layer.shape\n\n        return outd\n\n    def feedforward(self, X, train=True):\n        for layer in self.layers:\n            X = layer.call(X, train=train)\n\n        return X\n\n    def backpropogation(self, grad):\n        for layer in self.layers[::-1]:\n            grad = layer.backprop(grad)\n\n    def get_metrics(self, yh, y):\n        metrics = {}\n\n        if \'loss\' in self.metrics:\n            metrics[\'loss\'] = self.loss.forward(yh, y)\n\n        if \'accuracy\' in self.metrics:\n            metrics[\'accuracy\'] = self.accuracy(yh, y)\n\n        return metrics\n\n    def init_predictions(self):\n        outd = self.get_n_output()\n        self.metrics_dict = {\n            \'yh\': np.empty(dtype=np.float64, shape=(0, outd)),\n            \'y\': np.empty(dtype=np.int, shape=(0, outd))\n        }\n\n    def log_metrics(self, metrics, epoch):\n        validation = \'val\' in metrics\n        header = [\'run\', \'epoch\'] + self.metrics\n        train, val = [\'train\', epoch], [\'validation\', epoch]\n\n        for metric_name in self.metrics:\n            train.append(metrics[\'train\'][metric_name])\n            if validation:\n                val.append(metrics[\'val\'][metric_name])\n\n        if validation:\n            tqdm.write(tabulate([train, val], header, tablefmt=\'grid\'))\n        else:\n            tqdm.write(tabulate([train], header, tablefmt=\'grid\'))\n\n    def stash_predictions(self, yh, y):\n        yh_concat = [self.metrics_dict[\'yh\'], yh]\n        y_concat = [self.metrics_dict[\'y\'], y]\n\n        self.metrics_dict[\'yh\'] = np.concatenate(yh_concat)\n        self.metrics_dict[\'y\'] = np.concatenate(y_concat)\n\n    def get_predictions(self):\n        return self.metrics_dict[\'yh\'], self.metrics_dict[\'y\']\n\n    def accuracy(self, yh, y):\n        if len(y.shape) == 1 or (len(y.shape) == 2 and y.shape[1] == 1):\n            yh = np.rint(yh)\n            acc = yh == y\n        else:\n            y_predicts = np.argmax(yh, axis=1)\n            y_targets = np.argmax(y, axis=1)\n            acc = y_predicts == y_targets\n\n        return np.mean(acc)\n\n    def fit(self, X, y):\n        """"""\n        Train the model given samples :code:`X` and labels or values :code`y`.\n        """"""\n\n        if not self.compiled:\n            self.compile()\n\n        X_train, X_test, Y_train, Y_test = train_test_split(\n            X, y, test_size=self.validation_split)\n        n_samples = X_train.shape[0]\n\n        epoch_iter = trange(self.n_epoch, total=self.n_epoch,\n                            disable=not self.progress)\n\n        for epoch in epoch_iter:\n            epoch_iter.set_description(\'Epoch %s\' % epoch)\n            self.init_predictions()\n\n            for batch in range(n_samples // self.batch_size):\n                batch_start = self.batch_size * batch\n                batch_end = batch_start + self.batch_size\n                X_mb = X_train[batch_start:batch_end]\n                y_mb = Y_train[batch_start:batch_end]\n                yhi = self.feedforward(X_mb)\n                grad = self.loss.backward(yhi, y_mb)\n                self.backpropogation(grad)\n                params = []\n                grads = []\n\n                for layer in self.layers:\n                    params += layer.get_params()\n                    grads += layer.get_grads()\n                self.optimizer.update(params, grads)\n                self.stash_predictions(yhi, y_mb)\n\n            metrics = {}\n            train_yh, train_y = self.get_predictions()\n            metrics[\'train\'] = self.get_metrics(train_yh, train_y)\n\n            if self.validation_split > 0:\n                val_yh = self.feedforward(X_test)\n                metrics[\'val\'] = self.get_metrics(val_yh, Y_test)\n\n            if epoch % self.log_interval == 0:\n                self.log_metrics(metrics, epoch)\n\n        return metrics\n\n\n    def transform(self, X):\n        """"""\n        Predict the labels or values of some input matrix :code:`X`.\n        """"""\n        return self.feedforward(X, train=False)[0]\n'"
slugnet/optimizers.py,5,"b'import numpy as np\n\nfrom slugnet.initializations import _zero\n\n\nclass Optimizer(object):\n    def __init__(self, lr=0.001, clip=-1, decay=0., lr_min=0., lr_max=np.inf):\n        self.lr = lr\n        self.clip = clip\n        self.decay = decay\n        self.lr_min = lr_min\n        self.lr_max = lr_max\n\n        self.iterations = 0\n\n    def update(self, params, grads):\n        """"""Update parameters.\n        Parameters\n        ----------\n        params : list\n            A list of parameters in model.\n        grads : list\n            A list of gradients in model.\n        """"""\n        self.iterations += 1\n        self.lr *= (1. / 1 + self.decay * self.iterations)\n        self.lr = np.clip(self.lr, self.lr_min, self.lr_max)\n\n    def __str__(self):\n        return self.__class__.__name__\n\n\nclass SGD(Optimizer):\n    """"""\n    Optimize model parameters using common stochastic gradient descent.\n    """"""\n\n    def update(self, params, grads):\n        for p, g in zip(params, grads):\n            p -= self.lr * npdl_clip(g, self.clip)\n\n        super(SGD, self).update(params, grads)\n\n\ndef npdl_clip(grad, boundary):\n    if boundary > 0:\n        return np.clip(grad, -boundary, boundary)\n    else:\n        return grad\n\n\nclass RMSProp(Optimizer):\n    """"""RMSProp updates\n    Scale learning rates by dividing with the moving average of the root mean\n    squared (RMS) gradients. See [1]_ for further description.\n\n    :param rho: Gradient moving average decay factor.\n    :type rho: float\n    :param epsilon: Small value added for numerical stability.\n    :type epsilon: float\n\n    `rho` should be between 0 and 1. A value of `rho` close to 1 will decay the\n    moving average slowly and a value close to 0 will decay the moving average\n    fast.\n    Using the step size :math:`\\\\eta` and a decay factor :math:`\\\\rho` the\n    learning rate :math:`\\\\eta_t` is calculated as:\n\n    .. math::\n\n       r_t &= \\\\rho r_{t-1} + (1-\\\\rho)*g^2\\\\\\\\\n       \\\\eta_t &= \\\\frac{\\\\eta}{\\\\sqrt{r_t + \\\\epsilon}}\n\n    References\n    ----------\n    .. [1] Tieleman, T. and Hinton, G. (2012):\n           Neural Networks for Machine Learning, Lecture 6.5 - rmsprop.\n           Coursera. http://www.youtube.com/watch?v=O3sxAc4hxZU (formula @5:20)\n    """"""\n\n    def __init__(self, rho=0.9, epsilon=1e-6, *args, **kwargs):\n        super(RMSProp, self).__init__(*args, **kwargs)\n\n        self.rho = rho\n        self.epsilon = epsilon\n\n        self.cache = None\n        self.iterations = 0\n\n    def update(self, params, grads):\n        # init cache\n        if self.cache is None:\n            self.cache = [_zero(p.shape) for p in params]\n\n        # update parameters\n        for i, (c, p, g) in enumerate(zip(self.cache, params, grads)):\n            c = self.rho * c + (1 - self.rho) * np.power(g, 2)\n            p -= (self.lr * g / np.sqrt(c + self.epsilon))\n            self.cache[i] = c\n'"
slugnet/util.py,1,b'def softmax(z):\n    return np.exp(z) / np.sum(np.exp(z))\n'
tests/__init__.py,0,b''
tests/test_conv.py,7,"b""import numpy as np\nimport unittest\n\nfrom sklearn.datasets import fetch_mldata\n\nfrom slugnet.activation import ReLU, Softmax\nfrom slugnet.layers import Convolution, Dense, MeanPooling, Flatten\nfrom slugnet.loss import SoftmaxCategoricalCrossEntropy as SCCE\nfrom slugnet.model import Model\nfrom slugnet.optimizers import SGD\n\n\ndef get_mnist():\n    ndigits = 10\n    mnist_bunch = fetch_mldata('MNIST original')\n    mnist_target = mnist_bunch['target']\n    y = mnist_target.astype(np.int8)\n    y_ohe = np.zeros(shape=(len(y), ndigits))\n    y_ohe[np.arange(len(y)), y] = 1\n    X = mnist_bunch['data'].reshape((-1, 1, 28, 28)) / 255.0\n\n    return X, y_ohe\n\n\nclass TestMNISTWithConvnet(unittest.TestCase):\n    @classmethod\n    def setUpClass(self):\n        self.X, self.y = get_mnist()\n        np.random.seed(100)\n        self.X = np.random.permutation(self.X)[:1000]\n        np.random.seed(100)\n        self.y = np.random.permutation(self.y)[:1000]\n\n        self.model = Model(lr=0.001, n_epoch=100, batch_size=3, loss=SCCE(),\n                           metrics=['loss', 'accuracy'], optimizer=SGD())\n\n        self.model.add_layer(Convolution(1, (3, 3), inshape=(None, 1, 28, 28)))\n        self.model.add_layer(MeanPooling((2, 2)))\n        self.model.add_layer(Convolution(2, (4, 4)))\n        self.model.add_layer(MeanPooling((2, 2)))\n        self.model.add_layer(Flatten())\n        self.model.add_layer(Dense(10, activation=Softmax()))\n\n        self.fit_metrics = self.model.fit(self.X, self.y)\n\n    def test_training_accuracy_above_ninety(self):\n        self.assertGreater(self.fit_metrics['train']['accuracy'], 0.8)\n\n    def test_validation_accuracy_above_ninety(self):\n        self.assertGreater(self.fit_metrics['val']['accuracy'], 0.8)\n"""
tests/test_mnist.py,3,"b""import numpy as np\nimport unittest\n\nfrom sklearn.datasets import fetch_mldata\n\nfrom slugnet.activation import ReLU, Softmax\nfrom slugnet.layers import Dense, Dropout\nfrom slugnet.loss import SoftmaxCategoricalCrossEntropy as SCCE\nfrom slugnet.model import Model\nfrom slugnet.optimizers import RMSProp\n\n\ndef get_mnist():\n    ndigits = 10\n    mnist_bunch = fetch_mldata('MNIST original')\n    mnist_target = mnist_bunch['target']\n    y = mnist_target.astype(np.int8)\n    y_ohe = np.zeros(shape=(len(y), ndigits))\n    y_ohe[np.arange(len(y)), y] = 1\n\n    return mnist_bunch['data'], y_ohe\n\n\nclass TestMNIST(unittest.TestCase):\n    @classmethod\n    def setUpClass(self):\n        self.X, self.y = get_mnist()\n        self.model = Model(lr=0.01, n_epoch=3, loss=SCCE(),\n                           metrics=['loss', 'accuracy'], optimizer=RMSProp())\n\n        self.model.add_layer(Dense(200, inshape=784, activation=ReLU()))\n        self.model.add_layer(Dense(10, activation=Softmax()))\n\n        self.fit_metrics = self.model.fit(self.X, self.y)\n\n    def test_training_accuracy_above_ninety(self):\n        self.assertGreater(self.fit_metrics['train']['accuracy'], 0.9)\n\n    def test_validation_accuracy_above_ninety(self):\n        self.assertGreater(self.fit_metrics['val']['accuracy'], 0.9)\n\n\nclass TestMNISTWithDropout(unittest.TestCase):\n    @classmethod\n    def setUpClass(self):\n        self.X, self.y = get_mnist()\n        self.model = Model(lr=0.01, n_epoch=3, loss=SCCE(),\n                           metrics=['loss', 'accuracy'], optimizer=RMSProp())\n\n        self.model.add_layer(Dense(200, inshape=784, activation=ReLU()))\n        self.model.add_layer(Dropout(0.5))\n        self.model.add_layer(Dense(10, activation=Softmax()))\n\n        self.fit_metrics = self.model.fit(self.X, self.y)\n\n    def test_training_accuracy_above_ninety(self):\n        self.assertGreater(self.fit_metrics['train']['accuracy'], 0.8)\n\n    def test_validation_accuracy_above_ninety(self):\n        self.assertGreater(self.fit_metrics['val']['accuracy'], 0.8)\n"""
tests/test_xor.py,2,"b""import numpy as np\nimport unittest\n\nfrom slugnet.model import Model\nfrom slugnet.layers import Dense\nfrom slugnet.activation import Sigmoid\n\n\nclass TestSlugnetOnXOR(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.model = Model(progress=False, validation_split=0, batch_size=4,\n                          metrics=['loss', 'accuracy'], log_interval=5000)\n\n        cls.model.add_layer(Dense(3, inshape=2, activation=Sigmoid()))\n        cls.model.add_layer(Dense(1, activation=Sigmoid()))\n        cls.model.compile()\n\n        cls.X_train = np.array([\n            [0, 0],\n            [0, 1],\n            [1, 0],\n            [1, 1]\n        ])\n\n        cls.Y_train = np.array([\n            [0],\n            [1],\n            [1],\n            [0]\n        ])\n        cls.model.fit(cls.X_train, cls.Y_train)\n\n    def test_xor_01(self):\n        output = self.model.transform([0, 1])\n        self.assertGreater(output, 0.9)\n\n    def test_xor_10(self):\n        output = self.model.transform([1, 0])\n        self.assertGreater(output, 0.9)\n\n    def test_xor_00(self):\n        output = self.model.transform([0, 0])\n        self.assertLess(output, 0.1)\n\n    def test_xor_11(self):\n        output = self.model.transform([1, 1])\n        self.assertLess(output, 0.1)\n"""
slugnet/examples/mnist.py,0,"b""from slugnet.activation import ReLU, Softmax\nfrom slugnet.layers import Dense\nfrom slugnet.loss import SoftmaxCategoricalCrossEntropy as SCCE\nfrom slugnet.model import Model\nfrom slugnet.optimizers import RMSProp\nfrom slugnet.data.mnist import get_mnist\n\n\nX, y = get_mnist()\nmodel = Model(lr=0.01, n_epoch=3, loss=SCCE(),\n              metrics=['loss', 'accuracy'],\n              optimizer=RMSProp())\n\nmodel.add_layer(Dense(200, inshape=784, activation=ReLU()))\nmodel.add_layer(Dense(10, activation=Softmax()))\n\nmodel.fit(X, y)\n"""
slugnet/examples/mnist_conv.py,4,"b""import numpy as np\n\nfrom slugnet.activation import ReLU, Softmax\nfrom slugnet.layers import Convolution, Dense, MeanPooling, Flatten\nfrom slugnet.loss import SoftmaxCategoricalCrossEntropy as SCCE\nfrom slugnet.model import Model\nfrom slugnet.optimizers import SGD\nfrom slugnet.data.mnist import get_mnist\n\n\nX, y = get_mnist()\nX = X.reshape((-1, 1, 28, 28)) / 255.0\nnp.random.seed(100)\nX = np.random.permutation(X)[:1000]\nnp.random.seed(100)\ny = np.random.permutation(y)[:1000]\n\nmodel = Model(lr=0.001, n_epoch=100, batch_size=3, loss=SCCE(),\n              metrics=['loss', 'accuracy'], optimizer=SGD())\n\nmodel.add_layer(Convolution(1, (3, 3), inshape=(None, 1, 28, 28)))\nmodel.add_layer(MeanPooling((2, 2)))\nmodel.add_layer(Convolution(2, (4, 4)))\nmodel.add_layer(MeanPooling((2, 2)))\nmodel.add_layer(Flatten())\nmodel.add_layer(Dense(10, activation=Softmax()))\n\nmodel.fit(X, y)\n"""
slugnet/examples/mnist_dropout.py,0,"b""from slugnet.activation import ReLU, Softmax\nfrom slugnet.layers import Dense, Dropout\nfrom slugnet.loss import SoftmaxCategoricalCrossEntropy as SCCE\nfrom slugnet.model import Model\nfrom slugnet.optimizers import RMSProp\nfrom slugnet.data.mnist import get_mnist\n\n\nX, y = get_mnist()\nmodel = Model(lr=0.01, n_epoch=3, loss=SCCE(),\n              metrics=['loss', 'accuracy'],\n              optimizer=RMSProp())\n\nmodel.add_layer(Dense(200, inshape=784, activation=ReLU()))\nmodel.add_layer(Dropout(0.5))\nmodel.add_layer(Dense(10, activation=Softmax()))\n\nmodel.fit(X, y)\n"""
