file_path,api_count,code
4-Data-Wrangling/Audit_Zipcode.py,0,"b'\'\'\'\r\nThis code checks for zipcode whether they begin with \'94\' or \'95\' or something else\r\n\'\'\'\r\nOSMFILE = ""san-jose_california.osm""\r\nzip_type_re = re.compile(r\'\\d{5}$\')\r\n\r\ndef audit_ziptype(zip_types, zipcode):\r\n    if zipcode[0:2] != 95:\r\n        zip_types[zipcode[0:2]].add(zipcode)\r\n    elif zipcode[0:2] != 94:\r\n        zip_types[zipcode[0:2]].add(zipcode)\r\n        \r\ndef is_zipcode(elem):\r\n    return (elem.attrib[\'k\'] == ""addr:postcode"")\r\n\r\ndef audit_zip(osmfile):\r\n    osm_file = open(osmfile, ""r"")\r\n    zip_types = defaultdict(set)\r\n    for event, elem in ET.iterparse(osm_file, events=(""start"",)):\r\n        if elem.tag == ""node"" or elem.tag == ""way"":\r\n            for tag in elem.iter(""tag""):\r\n                if is_zipcode(tag):\r\n                    audit_ziptype(zip_types,tag.attrib[\'v\'])\r\n    osm_file.close()\r\n    return zip_types\r\n\r\nzip_print = audit_zip(OSMFILE)\r\n\r\ndef test():    \r\n    pprint.pprint(dict(zip_print))\r\n\r\nif __name__ == \'__main__\':\r\n    test()\r\n'"
4-Data-Wrangling/Auditing_Street_Names.py,0,"b'import xml.etree.cElementTree as ET\r\nimport pprint\r\nfrom collections import defaultdict\r\nimport re\r\n\r\n\'\'\'\r\nThe code below lists out all the street types not in the expected list.\r\n\'\'\'\r\nOSMFILE = ""san-jose_california.osm""\r\nstreet_type_re = re.compile(r\'\\b\\S+\\.?$\', re.IGNORECASE)\r\n\r\nexpected = [""Street"", ""Avenue"", ""Boulevard"", ""Drive"", ""Court"", ""Place"", ""Square"", ""Lane"", ""Road"", \r\n            ""Trail"", ""Parkway"", ""Commons"", ""Circle"", ""Terrace"", ""Way""]\r\n\r\n\r\ndef audit_street_type(street_types, street_name):\r\n    m = street_type_re.search(street_name)\r\n    if m:\r\n        street_type = m.group()\r\n        if street_type not in expected:\r\n            street_types[street_type].add(street_name)\r\n\r\n\r\ndef is_street_name(elem):\r\n    return (elem.attrib[\'k\'] == ""addr:street"")\r\n\r\n\r\ndef audit(osmfile):\r\n    osm_file = open(osmfile, ""r"")\r\n    street_types = defaultdict(set)\r\n    for event, elem in ET.iterparse(osm_file, events=(""start"",)):\r\n\r\n        if elem.tag == ""node"" or elem.tag == ""way"":\r\n            for tag in elem.iter(""tag""):\r\n                if is_street_name(tag):\r\n                    audit_street_type(street_types, tag.attrib[\'v\'])\r\n    osm_file.close()\r\n    return street_types\r\n\r\nst_types = audit(OSMFILE)\r\n\r\ndef test():    \r\n    pprint.pprint(dict(st_types))\r\n\r\nif __name__ == \'__main__\':\r\n    test()'"
4-Data-Wrangling/Convert_to_CSV_files.py,0,"b'\'\'\'\r\nThe code below is mostly derived from Udacity Lession 13: Case study: OpenStreetMap Data [SQL]\r\nhttps://classroom.udacity.com/nanodegrees/nd002/parts/860b269a-d0b0-4f0c-8f3d-ab08865d43bf/modules/316820862075461/lessons/5436095827/concepts/54908788190923\r\n\'\'\'\r\nOSM_PATH = ""san-jose_california.osm""\r\nOSMFILE = ""san-jose_california.osm""\r\nstreet_type_re = re.compile(r\'\\b\\S+\\.?$\', re.IGNORECASE)\r\n\r\nexpected = [""Street"", ""Avenue"", ""Boulevard"", ""Drive"", ""Court"", ""Place"", ""Square"", ""Lane"", ""Road"", \r\n            ""Trail"", ""Parkway"", ""Commons"", ""Circle"", ""Terrace"", ""Way""]\r\n\r\nmapping = { ""St"": ""Street"",\r\n            ""St."": ""Street"",\r\n            ""Rd."": ""Road"",\r\n            ""Ave"": ""Avenue"",\r\n            ""Blvd"": ""Boulevard"",\r\n            ""Dr"": ""Drive"",\r\n            ""Rd"": ""Road""\r\n            }\r\n\r\nNODES_PATH = ""nodes.csv""\r\nNODE_TAGS_PATH = ""nodes_tags.csv""\r\nWAYS_PATH = ""ways.csv""\r\nWAY_NODES_PATH = ""ways_nodes.csv""\r\nWAY_TAGS_PATH = ""ways_tags.csv""\r\n\r\nLOWER_COLON = re.compile(r\'^([a-z]|_)+:([a-z]|_)+\')\r\nPROBLEMCHARS = re.compile(r\'[=\\+/&<>;\\\'""\\?%#$@\\,\\. \\t\\r\\n]\')\r\n\r\nSCHEMA = schema\r\n\r\n# Make sure the fields order in the csvs matches the column order in the sql table schema\r\nNODE_FIELDS = [\'id\', \'lat\', \'lon\', \'user\', \'uid\', \'version\', \'changeset\', \'timestamp\']\r\nNODE_TAGS_FIELDS = [\'id\', \'key\', \'value\', \'type\']\r\nWAY_FIELDS = [\'id\', \'user\', \'uid\', \'version\', \'changeset\', \'timestamp\']\r\nWAY_TAGS_FIELDS = [\'id\', \'key\', \'value\', \'type\']\r\nWAY_NODES_FIELDS = [\'id\', \'node_id\', \'position\']\r\n    \r\ndef shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\r\n                  problem_chars=PROBLEMCHARS, default_tag_type=\'regular\'):\r\n    """"""Clean and shape node or way XML element to Python dict""""""\r\n    node_attribs = {}\r\n    way_attribs = {}\r\n    way_nodes = []\r\n    tags = []  # Handle secondary tags the same way for both node and way elements\r\n    p=0\r\n    \r\n    if element.tag == \'node\':\r\n        for i in NODE_FIELDS:\r\n            node_attribs[i] = element.attrib[i]\r\n        for tag in element.iter(""tag""):\r\n            node_tags_attribs = {}\r\n            temp = LOWER_COLON.search(tag.attrib[\'k\'])\r\n            is_p = PROBLEMCHARS.search(tag.attrib[\'k\'])\r\n            if is_p:\r\n                continue\r\n            elif temp:\r\n                split_char = temp.group(1)\r\n                split_index = tag.attrib[\'k\'].index(split_char)\r\n                type1 = temp.group(1)\r\n                node_tags_attribs[\'id\'] = element.attrib[\'id\']\r\n                node_tags_attribs[\'key\'] = tag.attrib[\'k\'][split_index+2:]\r\n                node_tags_attribs[\'value\'] = tag.attrib[\'v\']\r\n                node_tags_attribs[\'type\'] = tag.attrib[\'k\'][:split_index+1]\r\n                if node_tags_attribs[\'type\'] == ""addr"" and node_tags_attribs[\'key\'] == ""street"":\r\n                    # update street name\r\n                    node_tags_attribs[\'value\'] = update_name(tag.attrib[\'v\'], mapping) \r\n                #elif node_tags_attribs[\'type\'] == ""addr"" and node_tags_attribs[\'key\'] == ""postcode"":\r\n                #    # update post code\r\n                #    node_tags_attribs[\'value\'] = update_zipcode(tag.attrib[\'v\']) \r\n            else:\r\n                node_tags_attribs[\'id\'] = element.attrib[\'id\']\r\n                node_tags_attribs[\'key\'] = tag.attrib[\'k\']\r\n                node_tags_attribs[\'value\'] = tag.attrib[\'v\']\r\n                node_tags_attribs[\'type\'] = \'regular\'\r\n                if node_tags_attribs[\'type\'] == ""addr"" and node_tags_attribs[\'key\'] == ""street"":\r\n                    # update street name\r\n                    node_tags_attribs[\'value\'] = update_name(tag.attrib[\'v\'], mapping) \r\n                #elif node_tags_attribs[\'type\'] == ""addr"" and node_tags_attribs[\'key\'] == ""postcode"":\r\n                #    # update post code\r\n                #    node_tags_attribs[\'value\'] = update_zipcode(tag.attrib[\'v\']) \r\n            tags.append(node_tags_attribs)\r\n        return {\'node\': node_attribs, \'node_tags\': tags}\r\n    elif element.tag == \'way\':\r\n        id = element.attrib[\'id\']\r\n        for i in WAY_FIELDS:\r\n            way_attribs[i] = element.attrib[i]\r\n        for i in element.iter(\'nd\'):\r\n            d = {}\r\n            d[\'id\'] = id\r\n            d[\'node_id\'] = i.attrib[\'ref\']\r\n            d[\'position\'] = p\r\n            p+=1\r\n            way_nodes.append(d)\r\n        for c in element.iter(\'tag\'):\r\n            temp = LOWER_COLON.search(c.attrib[\'k\'])\r\n            is_p = PROBLEMCHARS.search(c.attrib[\'k\'])\r\n            e = {}\r\n            if is_p:\r\n                continue\r\n            elif temp:\r\n                split_char = temp.group(1)\r\n                split_index = c.attrib[\'k\'].index(split_char)\r\n                e[\'id\'] = id\r\n                e[\'key\'] = c.attrib[\'k\'][split_index+2:]\r\n                e[\'type\'] = c.attrib[\'k\'][:split_index+1]\r\n                e[\'value\'] = c.attrib[\'v\']\r\n                if e[\'type\'] == ""addr"" and e[\'key\'] == ""street"":\r\n                    e[\'value\'] = update_name(c.attrib[\'v\'], mapping) \r\n                #elif e[\'type\'] == ""addr"" and e[\'key\'] == ""postcode"":\r\n                #    e[\'value\'] = update_zipcode(c.attrib[\'v\'])\r\n            else:\r\n                e[\'id\'] = id\r\n                e[\'key\'] = c.attrib[\'k\']\r\n                e[\'type\'] = \'regular\'\r\n                e[\'value\'] =  c.attrib[\'v\']\r\n                if e[\'type\'] == ""addr"" and e[\'key\'] == ""street"":\r\n                    e[\'value\'] = update_name(c.attrib[\'v\'], mapping) \r\n                #elif e[\'type\'] == ""addr"" and e[\'key\'] == ""postcode"":\r\n                #    e[\'value\'] = update_zipcode(c.attrib[\'v\'])\r\n            tags.append(e)\r\n        \r\n    return {\'way\': way_attribs, \'way_nodes\': way_nodes, \'way_tags\': tags}\r\n        \r\n    if element.tag == \'node\':\r\n        return {\'node\': node_attribs, \'node_tags\': tags}\r\n    elif element.tag == \'way\':\r\n        return {\'way\': way_attribs, \'way_nodes\': way_nodes, \'way_tags\': tags}\r\n    \r\n\r\n# ================================================== #\r\n#               Helper Functions                     #\r\n# ================================================== #\r\ndef get_element(osm_file, tags=(\'node\', \'way\', \'relation\')):\r\n    """"""Yield element if it is the right type of tag""""""\r\n    context = ET.iterparse(osm_file, events=(\'start\', \'end\'))\r\n    _, root = next(context)\r\n    for event, elem in context:\r\n        if event == \'end\' and elem.tag in tags:\r\n            yield elem\r\n            root.clear()\r\n\r\n\r\ndef validate_element(element, validator, schema=SCHEMA):\r\n    """"""Raise ValidationError if element does not match schema""""""\r\n    if validator.validate(element, schema) is not True:\r\n        field, errors = next(validator.errors.iteritems())\r\n        message_string = ""\\nElement of type \'{0}\' has the following errors:\\n{1}""\r\n        error_string = pprint.pformat(errors)\r\n        \r\n        raise Exception(message_string.format(field, error_string))\r\n\r\n\r\nclass UnicodeDictWriter(csv.DictWriter, object):\r\n    """"""Extend csv.DictWriter to handle Unicode input""""""\r\n    def writerow(self, row):\r\n        super(UnicodeDictWriter, self).writerow({\r\n            k: (v.encode(\'utf-8\') if isinstance(v, unicode) else v) for k, v in row.iteritems()\r\n        })\r\n\r\n    def writerows(self, rows):\r\n        for row in rows:\r\n            self.writerow(row)\r\n\r\n\r\n# ================================================== #\r\n#               Main Function                        #\r\n# ================================================== #\r\ndef process_map(file_in, validate):\r\n    """"""Iteratively process each XML element and write to csv(s)""""""\r\n    with codecs.open(NODES_PATH, \'wb\') as nodes_file, \\\r\n        codecs.open(NODE_TAGS_PATH, \'wb\') as nodes_tags_file, \\\r\n        codecs.open(WAYS_PATH, \'wb\') as ways_file, \\\r\n        codecs.open(WAY_NODES_PATH, \'wb\') as way_nodes_file, \\\r\n        codecs.open(WAY_TAGS_PATH, \'wb\') as way_tags_file:\r\n\r\n        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\r\n        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\r\n        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\r\n        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\r\n        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\r\n\r\n        nodes_writer.writeheader()\r\n        node_tags_writer.writeheader()\r\n        ways_writer.writeheader()\r\n        way_nodes_writer.writeheader()\r\n        way_tags_writer.writeheader()\r\n\r\n        validator = cerberus.Validator()\r\n\r\n        for element in get_element(file_in, tags=(\'node\', \'way\')):\r\n            el = shape_element(element)\r\n            if el:\r\n                if validate is True:\r\n                    validate_element(el, validator)\r\n\r\n                if element.tag == \'node\':\r\n                    nodes_writer.writerow(el[\'node\'])\r\n                    node_tags_writer.writerows(el[\'node_tags\'])\r\n                elif element.tag == \'way\':\r\n                    ways_writer.writerow(el[\'way\'])\r\n                    way_nodes_writer.writerows(el[\'way_nodes\'])\r\n                    way_tags_writer.writerows(el[\'way_tags\'])\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    # Note: Validation is ~ 10X slower. For the project consider using a small\r\n    # sample of the map when validating.\r\n    process_map(OSM_PATH, validate=True)\r\n'"
4-Data-Wrangling/Convert_to_SQL_Database.py,0,"b'# Creating the database and tables\r\nimport sqlite3\r\nconn = sqlite3.connect(\'data_wrangling.sqlite\')\r\n\r\nconn.text_factory = str\r\ncur = conn.cursor()\r\n\r\n#Make some fresh tables using executescript()\r\ncur.execute(\'\'\'DROP TABLE IF EXISTS nodes\'\'\')\r\ncur.execute(\'\'\'DROP TABLE IF EXISTS nodes_tags\'\'\')\r\ncur.execute(\'\'\'DROP TABLE IF EXISTS ways\'\'\')\r\ncur.execute(\'\'\'DROP TABLE IF EXISTS ways_tags\'\'\')\r\ncur.execute(\'\'\'DROP TABLE IF EXISTS ways_nodes\'\'\')\r\n\r\n\r\ncur.execute(\'\'\'CREATE TABLE nodes (\r\n    id INTEGER PRIMARY KEY NOT NULL,\r\n    lat REAL,\r\n    lon REAL,\r\n    user TEXT,\r\n    uid INTEGER,\r\n    version INTEGER,\r\n    changeset INTEGER,\r\n    timestamp TEXT)\r\n\'\'\')\r\n\r\nwith open(\'nodes.csv\',\'r\') as nodes_table: # `with` statement available in 2.5+\r\n    # csv.DictReader uses first line in file for column headings by default\r\n    dr = csv.DictReader(nodes_table) # comma is default delimiter\r\n    to_db = [(i[\'id\'], i[\'lat\'], i[\'lon\'], i[\'user\'], i[\'uid\'], i[\'version\'], i[\'changeset\'], i[\'timestamp\']) for i in dr]\r\n\r\ncur.executemany(""INSERT INTO nodes VALUES (?, ?, ?, ?, ?, ?, ?, ?);"", to_db)\r\n\r\n\r\ncur.execute(\'\'\'CREATE TABLE nodes_tags (\r\n    id INTEGER,\r\n    key TEXT,\r\n    value TEXT,\r\n    type TEXT,\r\n    FOREIGN KEY (id) REFERENCES nodes(id))\r\n\'\'\')\r\n\r\nwith open(\'nodes_tags.csv\',\'r\') as nodes_tags_table: # `with` statement available in 2.5+\r\n    # csv.DictReader uses first line in file for column headings by default\r\n    dr = csv.DictReader(nodes_tags_table) # comma is default delimiter\r\n    to_db = [(i[\'id\'], i[\'key\'], i[\'value\'], i[\'type\']) for i in dr]\r\n\r\ncur.executemany(""INSERT INTO nodes_tags VALUES (?, ?, ?, ?);"", to_db)\r\n\r\ncur.execute(\'\'\'CREATE TABLE ways (\r\n    id INTEGER PRIMARY KEY NOT NULL,\r\n    user TEXT,\r\n    uid INTEGER,\r\n    version TEXT,\r\n    changeset INTEGER,\r\n    timestamp TEXT)\r\n\'\'\')\r\n\r\nwith open(\'ways.csv\',\'r\') as ways_table: # `with` statement available in 2.5+\r\n    # csv.DictReader uses first line in file for column headings by default\r\n    dr = csv.DictReader(ways_table) # comma is default delimiter\r\n    to_db = [(i[\'id\'], i[\'user\'], i[\'uid\'], i[\'version\'], i[\'changeset\'], i[\'timestamp\']) for i in dr]\r\n\r\ncur.executemany(""INSERT INTO ways VALUES (?, ?, ?, ?, ?, ?);"", to_db)\r\n\r\ncur.execute(\'\'\'CREATE TABLE ways_tags (\r\n    id INTEGER NOT NULL,\r\n    key TEXT NOT NULL,\r\n    value TEXT NOT NULL,\r\n    type TEXT,\r\n    FOREIGN KEY (id) REFERENCES ways(id))\r\n\'\'\')\r\n\r\nwith open(\'ways_tags.csv\',\'r\') as ways_tags_table: # `with` statement available in 2.5+\r\n    # csv.DictReader uses first line in file for column headings by default\r\n    dr = csv.DictReader(ways_tags_table) # comma is default delimiter\r\n    to_db = [(i[\'id\'], i[\'key\'], i[\'value\'], i[\'type\']) for i in dr]\r\n\r\ncur.executemany(""INSERT INTO ways_tags VALUES (?, ?, ?, ?);"", to_db)\r\n\r\ncur.execute(\'\'\'CREATE TABLE ways_nodes (\r\n    id INTEGER NOT NULL,\r\n    node_id INTEGER NOT NULL,\r\n    position INTEGER NOT NULL,\r\n    FOREIGN KEY (id) REFERENCES ways(id),\r\n    FOREIGN KEY (node_id) REFERENCES nodes(id))\r\n\'\'\')\r\n\r\nwith open(\'ways_nodes.csv\',\'r\') as ways_nodes_table: # `with` statement available in 2.5+\r\n    # csv.DictReader uses first line in file for column headings by default\r\n    dr = csv.DictReader(ways_nodes_table) # comma is default delimiter\r\n    to_db = [(i[\'id\'], i[\'node_id\'], i[\'position\']) for i in dr]\r\n\r\ncur.executemany(""INSERT INTO ways_nodes VALUES (?, ?, ?);"", to_db)\r\n\r\n#Save changes\r\nconn.commit()\r\n'"
4-Data-Wrangling/Improving_Street_Names.py,0,"b'import xml.etree.cElementTree as ET\r\nimport pprint\r\nfrom collections import defaultdict\r\nimport re\r\n\r\n\'\'\'\r\nThe code below updates the unexpected street types listed in the mapping list\r\nwhile keeping others unchanged.\r\n\'\'\'\r\nmapping = { ""St"": ""Street"",\r\n            ""St."": ""Street"",\r\n            ""Rd."": ""Road"",\r\n            ""Ave"": ""Avenue"",\r\n            ""Blvd"": ""Boulevard"",\r\n            ""Dr"": ""Drive"",\r\n            ""Rd"": ""Road""\r\n            }\r\n\r\ndef update_name(name, mapping):\r\n    m = street_type_re.search(name)\r\n    if m.group() not in expected:\r\n        if m.group() in mapping.keys():\r\n            name = re.sub(m.group(), mapping[m.group()], name)\r\n    return name\r\n\r\ndef test():\r\n    for st_type, ways in st_types.iteritems():\r\n        for name in ways:\r\n            better_name = update_name(name, mapping)\r\n            print name, ""=>"", better_name\r\n\r\nif __name__ == \'__main__\':\r\n    test()'"
4-Data-Wrangling/Number_of_Tags.py,0,"b'import xml.etree.cElementTree as ET\r\nimport pprint\r\nfrom collections import defaultdict\r\nimport re\r\n\r\n\'\'\'\r\nThe code below is to find out how many types of tags are there and the number of each tag.\r\n\'\'\'\r\n\r\ndef count_tags(filename):\r\n    tags = {}\r\n    for event, element in ET.iterparse(filename):\r\n        if element.tag not in tags.keys():\r\n            tags[element.tag] = 1\r\n        else:\r\n            tags[element.tag] += 1\r\n    return tags\r\n\r\ndef test():\r\n\r\n    tags = count_tags(\'san-jose_california.osm\')\r\n    pprint.pprint(tags)\r\n\r\nif __name__ == ""__main__"":\r\n    test()'"
4-Data-Wrangling/Sample.py,0,"b'#!/usr/bin/env python\r\n# -*- coding: utf-8 -*-\r\n\r\nimport xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\r\n\r\nOSM_FILE = ""san-jose_california.osm""  # Replace this with your osm file\r\nSAMPLE_FILE = ""sample.osm""\r\n\r\nk = 50 # Parameter: take every k-th top level element\r\n\r\ndef get_element(osm_file, tags=(\'node\', \'way\', \'relation\')):\r\n    """"""Yield element if it is the right type of tag\r\n\r\n    Reference:\r\n    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\r\n    """"""\r\n    context = iter(ET.iterparse(osm_file, events=(\'start\', \'end\')))\r\n    _, root = next(context)\r\n    for event, elem in context:\r\n        if event == \'end\' and elem.tag in tags:\r\n            yield elem\r\n            root.clear()\r\n\r\n\r\nwith open(SAMPLE_FILE, \'wb\') as output:\r\n    output.write(\'<?xml version=""1.0"" encoding=""UTF-8""?>\\n\')\r\n    output.write(\'<osm>\\n  \')\r\n\r\n    # Write every kth top level element\r\n    for i, element in enumerate(get_element(OSM_FILE)):\r\n        if i % k == 0:\r\n            output.write(ET.tostring(element, encoding=\'utf-8\'))\r\n\r\n    output.write(\'</osm>\')\r\n'"
4-Data-Wrangling/Schema.py,0,"b""import xml.etree.cElementTree as ET\r\nimport pprint\r\nfrom collections import defaultdict\r\nimport re\r\n\r\nimport csv\r\nimport codecs\r\nimport cerberus\r\nimport schema\r\n\r\n'''\r\nThe schema below for the 5 csv files which will be used to construct a SQL databases\r\n'''\r\n\r\nschema = {\r\n    'node': {\r\n        'type': 'dict',\r\n        'schema': {\r\n            'id': {'required': True, 'type': 'integer', 'coerce': int},\r\n            'lat': {'required': True, 'type': 'float', 'coerce': float},\r\n            'lon': {'required': True, 'type': 'float', 'coerce': float},\r\n            'user': {'required': True, 'type': 'string'},\r\n            'uid': {'required': True, 'type': 'integer', 'coerce': int},\r\n            'version': {'required': True, 'type': 'string'},\r\n            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\r\n            'timestamp': {'required': True, 'type': 'string'}\r\n        }\r\n    },\r\n    'node_tags': {\r\n        'type': 'list',\r\n        'schema': {\r\n            'type': 'dict',\r\n            'schema': {\r\n                'id': {'required': True, 'type': 'integer', 'coerce': int},\r\n                'key': {'required': True, 'type': 'string'},\r\n                'value': {'required': True, 'type': 'string'},\r\n                'type': {'required': True, 'type': 'string'}\r\n            }\r\n        }\r\n    },\r\n    'way': {\r\n        'type': 'dict',\r\n        'schema': {\r\n            'id': {'required': True, 'type': 'integer', 'coerce': int},\r\n            'user': {'required': True, 'type': 'string'},\r\n            'uid': {'required': True, 'type': 'integer', 'coerce': int},\r\n            'version': {'required': True, 'type': 'string'},\r\n            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\r\n            'timestamp': {'required': True, 'type': 'string'}\r\n        }\r\n    },\r\n    'way_nodes': {\r\n        'type': 'list',\r\n        'schema': {\r\n            'type': 'dict',\r\n            'schema': {\r\n                'id': {'required': True, 'type': 'integer', 'coerce': int},\r\n                'node_id': {'required': True, 'type': 'integer', 'coerce': int},\r\n                'position': {'required': True, 'type': 'integer', 'coerce': int}\r\n            }\r\n        }\r\n    },\r\n    'way_tags': {\r\n        'type': 'list',\r\n        'schema': {\r\n            'type': 'dict',\r\n            'schema': {\r\n                'id': {'required': True, 'type': 'integer', 'coerce': int},\r\n                'key': {'required': True, 'type': 'string'},\r\n                'value': {'required': True, 'type': 'string'},\r\n                'type': {'required': True, 'type': 'string'}\r\n            }\r\n        }\r\n    }\r\n}\r\n"""
4-Data-Wrangling/Tags_Types.py,0,"b'import xml.etree.cElementTree as ET\r\nimport pprint\r\nfrom collections import defaultdict\r\nimport re\r\n\r\n\'\'\'\r\nThe code below allows you to check the k value for each tag.\r\nBy classifying the tagss into few categories:\r\n1. ""lower"": valid tags containing only lowercase letters\r\n2. ""lower_colon"": valid tags with a colon in the names\r\n3. ""problemchars"": tags with problematic characters\r\n4. ""other"": other tags that don\'t fall into the 3 categories above\r\n\'\'\'\r\nlower = re.compile(r\'^([a-z]|_)*$\')\r\nlower_colon = re.compile(r\'^([a-z]|_)*:([a-z]|_)*$\')\r\nproblemchars = re.compile(r\'[=\\+/&<>;\\\'""\\?%#$@\\,\\. \\t\\r\\n]\')\r\n\r\ndef key_type(element, keys):\r\n    if element.tag == ""tag"":\r\n        k = element.attrib[\'k\']\r\n        if re.search(lower,k):\r\n            keys[""lower""] += 1\r\n        elif re.search(lower_colon,k):\r\n            keys[""lower_colon""] += 1\r\n        elif re.search(problemchars,k):\r\n            keys[""problemchars""] += 1\r\n        else:\r\n            keys[""other""] += 1\r\n    return keys\r\n\r\ndef process_map(filename):\r\n    keys = {""lower"": 0, ""lower_colon"": 0, ""problemchars"": 0, ""other"": 0}\r\n    for _, element in ET.iterparse(filename):\r\n        keys = key_type(element, keys)\r\n    return keys\r\n\r\ndef test():\r\n    keys = process_map(\'san-jose_california.osm\')\r\n    pprint.pprint(keys)\r\n\r\nif __name__ == ""__main__"":\r\n    test()'"
4-Data-Wrangling/Update_Zipcode.py,0,"b'\'\'\'\r\nThis code will update non 5-digit zipcode.\r\nIf it is 8/9-digit, only the first 5 digits are kept.\r\nIf it has the state name in front, only the 5 digits are kept.\r\nIf it is something else, will not change anything as it might result in error when validating the csv file.\r\n\'\'\'\r\ndef update_zipcode(zipcode):\r\n    """"""Clean postcode to a uniform format of 5 digit; Return updated postcode""""""\r\n    if re.findall(r\'^\\d{5}$\', zipcode): # 5 digits 02118\r\n        valid_zipcode = zipcode\r\n        return valid_zipcode\r\n    elif re.findall(r\'(^\\d{5})-\\d{3}$\', zipcode): # 8 digits 02118-029\r\n        valid_zipcode = re.findall(r\'(^\\d{5})-\\d{3}$\', zipcode)[0]\r\n        return valid_zipcode\r\n    elif re.findall(r\'(^\\d{5})-\\d{4}$\', zipcode): # 9 digits 02118-0239\r\n        valid_zipcode = re.findall(r\'(^\\d{5})-\\d{4}$\', zipcode)[0]\r\n        return valid_zipcode\r\n    elif re.findall(r\'CA\\s*\\d{5}\', zipcode): # with state code CA 02118\r\n        valid_zipcode =re.findall(r\'\\d{5}\', zipcode)[0]  \r\n        return valid_zipcode  \r\n    else: #return default zipcode to avoid overwriting\r\n        return zipcode\r\n    \r\ndef test_zip():\r\n    for zips, ways in zip_print.iteritems():\r\n        for name in ways:\r\n            better_name = update_zipcode(name)\r\n            print name, ""=>"", better_name\r\n\r\nif __name__ == \'__main__\':\r\n    test_zip()\r\n'"
7-Intro-to-Machine-Learning/poi_id.py,0,"b'#!/usr/bin/python\r\n\r\nimport sys\r\nimport pickle\r\nsys.path.append(""../tools/"")\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport random\r\n\r\nfrom feature_format import featureFormat, targetFeatureSplit\r\nfrom tester import dump_classifier_and_data\r\n\r\nfrom sklearn.metrics import accuracy_score\r\nfrom sklearn.naive_bayes import GaussianNB\r\nfrom sklearn.svm import SVC\r\nfrom sklearn import tree\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.metrics import accuracy_score\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nfrom sklearn.feature_selection import SelectKBest\r\nfrom sklearn.model_selection import cross_val_score\r\nfrom sklearn.ensemble import AdaBoostClassifier\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.grid_search import GridSearchCV\r\nfrom sklearn.feature_selection import SelectKBest, f_classif\r\n\r\n\r\n### Task 1: Select what features you\'ll use.\r\n### features_list is a list of strings, each of which is a feature name.\r\n### The first feature must be ""poi"".\r\nfeatures_list = [\'poi\',\r\n                 \'salary\',\r\n                 \'bonus\',\r\n                 \'deferral_payments\',\r\n                 \'expenses\',\r\n                 \'deferred_income\',\r\n                 \'long_term_incentive\',\r\n                 \'restricted_stock_deferred\',\r\n                 \'shared_receipt_with_poi\',\r\n                 \'loan_advances\',\r\n                 \'from_messages\',\r\n                 \'director_fees\',\r\n                 \'total_stock_value\',\r\n                 \'from_poi_to_this_person\',\r\n                 \'from_this_person_to_poi\',\r\n                 \'total_payments\',\r\n                 \'exercised_stock_options\',\r\n                 \'to_messages\',\r\n                 \'restricted_stock\',\r\n                 \'other\'] \r\n# You will need to use more features\r\n\r\n### Load the dictionary containing the dataset\r\nwith open(""final_project_dataset.pkl"", ""r"") as data_file:\r\n    data_dict = pickle.load(data_file)\r\n\r\n### Summarize the dataset\r\n\'\'\'\r\nprint \'Total number of ppl:\', len(data_dict) #How many people in the dataset\r\nprint \'Number of features:\', len(features_list) #How mant features in the list\r\n\r\npoi_num = 0\r\nfor i in data_dict:\r\n    if data_dict[i][\'poi\'] == True:\r\n        poi_num += 1\r\nprint \'Number of poi:\', poi_num #Number of poi\r\n\'\'\'\r\n\r\n### Checking the incompleteness of the dataset (% of NaN in every feature)\r\n### Need to remove the new features created when running the code below\r\n\'\'\'\r\nnan = [0 for i in range(len(features_list))]\r\nfor k, v in data_dict.iteritems():\r\n    for j, feature in enumerate(features_list):\r\n        if v[feature] == \'NaN\':\r\n            nan[j] += 1\r\n\r\nfor i, feature in enumerate(features_list):\r\n    print \'NaN count for\', feature, \':\', nan[i] #Number of NaN in each feature\r\n\'\'\'\r\n\r\n### Task 2: Remove outliers\r\n### After plotting and checking the pdf, ""TOTAL"" and ""THE AGENCY IN THE PARK"" \r\n### are removed as both don\'t seem to help with predicting\r\n\r\ndata_dict.pop(\'TOTAL\', 0)\r\ndata_dict.pop(\'THE TRAVEL AGENCY IN THE PARK\', 0)\r\n\r\n### Scatterplot of Bonus vs Salary\r\n\'\'\'\r\ndata = featureFormat(data_dict, [\'salary\', \'bonus\'])\r\nfor point in data:\r\n    x = point[0]\r\n    y = point[1]\r\n    plt.scatter(x, y)\r\nplt.xlabel(\'salary\')\r\nplt.ylabel(\'bonus\')\r\n\'\'\'\r\n\r\n### Task 3: Create new feature(s)\r\n\r\n### Create new salary-bonus-ratio, from_this_person_to_poi % & from_poi_to_this_person % features\r\n\r\ndef ratio_calc(numerator, denominator):\r\n    fraction = 0\r\n    if numerator == \'NaN\' or denominator == \'NaN\':\r\n        fraction == \'NaN\'\r\n    else:\r\n        fraction = float(numerator) / float(denominator)\r\n    return fraction\r\n\r\nfor name in data_dict:\r\n    salary_bonus_ratio_temp = ratio_calc(data_dict[name][\'salary\'], data_dict[name][\'bonus\'])\r\n    data_dict[name][\'salary_bonus_ratio\'] = salary_bonus_ratio_temp\r\n\r\n    from_this_person_to_poi_ratio_temp = ratio_calc(data_dict[name][\'from_this_person_to_poi\'], data_dict[name][\'from_messages\'])\r\n    data_dict[name][\'from_this_person_to_poi_ratio\'] = from_this_person_to_poi_ratio_temp\r\n\r\n    from_poi_to_this_person_ratio_temp = ratio_calc(data_dict[name][\'from_poi_to_this_person\'], data_dict[name][\'to_messages\'])\r\n    data_dict[name][\'from_poi_to_this_person_ratio\'] = from_poi_to_this_person_ratio_temp\r\n\r\n\r\n### Store to my_dataset for easy export below.\r\nmy_dataset = data_dict\r\n\r\n### Extract features and labels from dataset for local testing\r\nfeatures_list = [\'poi\',\r\n                 \'salary\',\r\n                 \'bonus\',\r\n                 #\'deferral_payments\',\r\n                 #\'expenses\',\r\n                 \'deferred_income\',\r\n                 \'long_term_incentive\',\r\n                 #\'restricted_stock_deferred\',\r\n                 \'shared_receipt_with_poi\',\r\n                 #\'loan_advances\',\r\n                 #\'from_messages\',\r\n                 #\'director_fees\',\r\n                 \'total_stock_value\',\r\n                 #\'from_poi_to_this_person\',\r\n                 #\'from_this_person_to_poi\',\r\n                 \'total_payments\',\r\n                 \'exercised_stock_options\',\r\n                 #\'from_poi_to_this_person_ratio\',\r\n                 \'from_this_person_to_poi_ratio\',\r\n                 #\'salary_bonus_ratio\',\r\n                 #\'to_messages\',\r\n                 \'restricted_stock\'\r\n                 #\'other\'\r\n                 ] \r\n### Other is removed from the dataset as it doesn\'t tell much and including it\r\n### might skew the predictions\r\n\r\ndata = featureFormat(my_dataset, features_list, sort_keys = True)\r\nlabels, features = targetFeatureSplit(data)\r\n\r\n\r\n### Applying feature scaling using MinMaxScaler\r\n\r\nscaler = MinMaxScaler()\r\nfeatures = scaler.fit_transform(features)\r\n\r\n\r\n### Using SelectKBest to determine by features to use\r\n\'\'\'\r\nselector = SelectKBest(f_classif, k=20)\r\nselector.fit(features, labels)\r\nfeatures = selector.transform(features)\r\nfeature_scores = zip(features_list[1:],selector.scores_)\r\n\r\nsorted_scores = sorted(feature_scores, key=lambda feature: feature[1], reverse = True)\r\nfor item in sorted_scores:\r\n    print item[0], item[1]\r\n\'\'\'\r\n\r\n### Using Decision Tree\r\n\'\'\'\r\nclf = DecisionTreeClassifier()\r\nclf.fit(features, labels)\r\ndt_scores = zip(features_list[1:],clf.feature_importances_)\r\nsorted_dtscores = sorted(dt_scores, key=lambda feature: feature[1], reverse = True)\r\nfor item in sorted_dtscores:\r\n    print item[0], item[1]\r\n\'\'\'\r\n\r\n### Task 4: Try a varity of classifiers\r\n### Please name your classifier clf for easy export below.\r\n### Note that if you want to do PCA or other multi-stage operations,\r\n### you\'ll need to use Pipelines. For more info:\r\n### http://scikit-learn.org/stable/modules/pipeline.html\r\n\r\n# Provided to give you a starting point. Try a variety of classifiers.\r\n### Naive-Bayes Gaussian\r\n\'\'\'\r\nclf = GaussianNB()\r\n\'\'\'\r\n\r\n### Decision Tree\r\n#clf = tree.DecisionTreeClassifier()\r\n\r\n### AdaBoost Classifier\r\n#clf = AdaBoostClassifier()\r\n\r\n### K Nearest Neighbors Classifier\r\n#clf = KNeighborsClassifier()\r\n\r\n### Random Forest Classifier\r\n#clf = RandomForestClassifier()\r\n\r\n### Task 5: Tune your classifier to achieve better than .3 precision and recall \r\n### using our testing script. Check the tester.py script in the final project\r\n### folder for details on the evaluation method, especially the test_classifier\r\n### function. Because of the small size of the dataset, the script uses\r\n### stratified shuffle split cross validation. For more info: \r\n### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\r\n\r\n# Example starting point. Try investigating other evaluation techniques!\r\n\r\nfrom sklearn.cross_validation import train_test_split\r\n\r\nfeatures_train, features_test, labels_train, labels_test = \\\r\n    train_test_split(features, labels, test_size=0.3, random_state=42)\r\n\r\n\r\n### Decision Tree, need to change the clf to dt when tring to figure the optimal parameter values\r\n\'\'\'\r\nclf = tree.DecisionTreeClassifier(class_weight=None, criterion=\'gini\', max_depth=None,\r\n            max_features=None, max_leaf_nodes=None,\r\n            min_impurity_decrease=0.0, min_impurity_split=None,\r\n            min_samples_leaf=1, min_samples_split=2,\r\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\r\n            splitter=\'best\')\r\n\'\'\'\r\n#param_grid = {}\r\n#clf = GridSearchCV(dt, param_grid)\r\n#clf = clf.fit(features, labels)\r\n#print clf.best_estimator_\r\n\r\n### AdaBoost Classifier\r\n#clf = AdaBoostClassifier(algorithm=\'SAMME.R\', base_estimator=None,\r\n#          learning_rate=1.0, n_estimators=50, random_state=None)\r\n#param_grid = {}\r\n#clf = GridSearchCV(ada, param_grid)\r\n#clf = clf.fit(features, labels)\r\n#print clf.best_estimator_    \r\n    \r\n### K Nearest Neighbors Classifier\r\n#clf = KNeighborsClassifier(algorithm=\'auto\', leaf_size=30, metric=\'minkowski\',\r\n#           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\r\n#           weights=\'uniform\')\r\n#param_grid = {}\r\n#clf = GridSearchCV(knn, param_grid)\r\n#clf = clf.fit(features, labels)\r\n#print clf.best_estimator_\r\n\r\n### Random Forest Classifier\r\n#clf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion=\'gini\',\r\n#            max_depth=None, max_features=\'auto\', max_leaf_nodes=None,\r\n#            min_impurity_decrease=0.0, min_impurity_split=None,\r\n#            min_samples_leaf=1, min_samples_split=2,\r\n#            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\r\n#            oob_score=False, random_state=None, verbose=0,\r\n#            warm_start=False)\r\n#param_grid = {}\r\n#clf = GridSearchCV(rf, param_grid)\r\n#clf = clf.fit(features, labels)\r\n#print clf.best_estimator_\r\n\r\n### Task 6: Dump your classifier, dataset, and features_list so anyone can\r\n### check your results. You do not need to change anything below, but make sure\r\n### that the version of poi_id.py that you submit can be run on its own and\r\n### generates the necessary .pkl files for validating your results.\r\n\r\n\r\ndump_classifier_and_data(clf, my_dataset, features_list)\r\n'"
7-Intro-to-Machine-Learning/tester.py,0,"b'#!/usr/bin/pickle\r\n\r\n"""""" a basic script for importing student\'s POI identifier,\r\n    and checking the results that they get from it \r\n \r\n    requires that the algorithm, dataset, and features list\r\n    be written to my_classifier.pkl, my_dataset.pkl, and\r\n    my_feature_list.pkl, respectively\r\n\r\n    that process should happen at the end of poi_id.py\r\n""""""\r\n\r\nimport pickle\r\nimport sys\r\nfrom sklearn.cross_validation import StratifiedShuffleSplit\r\nsys.path.append(""../tools/"")\r\nfrom feature_format import featureFormat, targetFeatureSplit\r\n\r\nPERF_FORMAT_STRING = ""\\\r\n\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\r\nRecall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}""\r\nRESULTS_FORMAT_STRING = ""\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\\r\n\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}""\r\n\r\ndef test_classifier(clf, dataset, feature_list, folds = 1000):\r\n    data = featureFormat(dataset, feature_list, sort_keys = True)\r\n    labels, features = targetFeatureSplit(data)\r\n    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\r\n    true_negatives = 0\r\n    false_negatives = 0\r\n    true_positives = 0\r\n    false_positives = 0\r\n    for train_idx, test_idx in cv: \r\n        features_train = []\r\n        features_test  = []\r\n        labels_train   = []\r\n        labels_test    = []\r\n        for ii in train_idx:\r\n            features_train.append( features[ii] )\r\n            labels_train.append( labels[ii] )\r\n        for jj in test_idx:\r\n            features_test.append( features[jj] )\r\n            labels_test.append( labels[jj] )\r\n        \r\n        ### fit the classifier using training set, and test on test set\r\n        clf.fit(features_train, labels_train)\r\n        predictions = clf.predict(features_test)\r\n        for prediction, truth in zip(predictions, labels_test):\r\n            if prediction == 0 and truth == 0:\r\n                true_negatives += 1\r\n            elif prediction == 0 and truth == 1:\r\n                false_negatives += 1\r\n            elif prediction == 1 and truth == 0:\r\n                false_positives += 1\r\n            elif prediction == 1 and truth == 1:\r\n                true_positives += 1\r\n            else:\r\n                print ""Warning: Found a predicted label not == 0 or 1.""\r\n                print ""All predictions should take value 0 or 1.""\r\n                print ""Evaluating performance for processed predictions:""\r\n                break\r\n    try:\r\n        total_predictions = true_negatives + false_negatives + false_positives + true_positives\r\n        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\r\n        precision = 1.0*true_positives/(true_positives+false_positives)\r\n        recall = 1.0*true_positives/(true_positives+false_negatives)\r\n        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\r\n        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\r\n        print clf\r\n        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\r\n        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\r\n        print """"\r\n    except:\r\n        print ""Got a divide by zero when trying out:"", clf\r\n        print ""Precision or recall may be undefined due to a lack of true positive predicitons.""\r\n\r\nCLF_PICKLE_FILENAME = ""my_classifier.pkl""\r\nDATASET_PICKLE_FILENAME = ""my_dataset.pkl""\r\nFEATURE_LIST_FILENAME = ""my_feature_list.pkl""\r\n\r\ndef dump_classifier_and_data(clf, dataset, feature_list):\r\n    with open(CLF_PICKLE_FILENAME, ""w"") as clf_outfile:\r\n        pickle.dump(clf, clf_outfile)\r\n    with open(DATASET_PICKLE_FILENAME, ""w"") as dataset_outfile:\r\n        pickle.dump(dataset, dataset_outfile)\r\n    with open(FEATURE_LIST_FILENAME, ""w"") as featurelist_outfile:\r\n        pickle.dump(feature_list, featurelist_outfile)\r\n\r\ndef load_classifier_and_data():\r\n    with open(CLF_PICKLE_FILENAME, ""r"") as clf_infile:\r\n        clf = pickle.load(clf_infile)\r\n    with open(DATASET_PICKLE_FILENAME, ""r"") as dataset_infile:\r\n        dataset = pickle.load(dataset_infile)\r\n    with open(FEATURE_LIST_FILENAME, ""r"") as featurelist_infile:\r\n        feature_list = pickle.load(featurelist_infile)\r\n    return clf, dataset, feature_list\r\n\r\ndef main():\r\n    ### load up student\'s classifier, dataset, and feature_list\r\n    clf, dataset, feature_list = load_classifier_and_data()\r\n    ### Run testing script\r\n    test_classifier(clf, dataset, feature_list)\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
