file_path,api_count,code
fiml.py,27,"b'#\n# Copyright (c) 2016 KAMADA Ken\'ichi.\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n# 1. Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS\'\' AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n# OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n# OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n# SUCH DAMAGE.\n#\n\n""""""FIML estimation of the mean/covariance of data with missing values.\n\nThis is an implementation of full information maximum likelihood (FIML)\nmethod to estimate the mean and the covariance of data with missing\nvalues.\n""""""\n\nimport numpy as np\nimport scipy as sp\nimport scipy.optimize\n\n_log2pi = np.log(2 * np.pi)\n\ndef fiml(data, bias=False):\n    """"""FIML estimation of the mean/covariance of data with missing values.\n\n    Estimate the mean and the covariance of data with missing values by\n    full information maximum likelihood (FIML) method.\n\n    Parameters\n    ----------\n    data : ndarray\n        A 2-D array containing variables and observations.\n        Each row is an observation and each column is a variable.\n        A missing value is represented by `np.nan`.\n    bias : bool, optional\n        Must be True for now.\n\n    Returns\n    -------\n    mean : ndarray\n        Estimated means of the variables.\n    cov : ndarray\n        Estimated covariance of the variables.\n    """"""\n\n    if not bias:\n        raise NotImplementedError(""unbiased estimator is not yet implemented"")\n\n    size, dim = data.shape\n    mean0 = np.zeros(dim)\n    cov0 = np.eye(dim)\n    params0 = _pack_params(dim, mean0, cov0)\n    data_blocks = _sort_missing(data)\n    result = sp.optimize.fmin_slsqp(\n        _obj_func, params0, args=(dim, data_blocks), disp=False)\n    mean, cov = _unpack_params(dim, result)\n    return mean, cov\n\n# Sort data by the missing patterns.\n# The return value is in the following format.\n# Missing variables (columns) are removed from the data blocks.\n#  [(observation_pattern1, data_block1),\n#   (observation_pattern2, data_block2),\n#   ...]\ndef _sort_missing(data):\n    # Argsort the rows of obsmap.\n    obsmap = ~np.isnan(data)\n    sortedidx = sorted(range(data.shape[0]), key=lambda i: list(obsmap[i]))\n    # Split row indexes into blocks.\n    blocks = [[sortedidx[0]]]\n    for idx, prev in zip(sortedidx[1:], sortedidx[:-1]):\n        if (obsmap[prev] == obsmap[idx]).all():\n            blocks[-1].append(idx)\n        else:\n            blocks.append([idx])\n    return [(obsmap[b[0]], data[b][:, obsmap[b[0]]]) for b in blocks]\n\n# Pack the mean and the covariance into a 1-dimensional array.\n# - The first N values are the means.\n# - The remaining N (N + 1) / 2 values are the lower triangular matrix\n#   of the covariance.\ndef _pack_params(dim, mean, cov):\n    params = np.empty(dim + dim * (dim + 1) // 2)\n    params[:dim] = mean\n    params[dim:] = cov[np.tril_indices(dim)]\n    return params\n\n# Unpack the mean and the covariance from a 1-dimensional array.\ndef _unpack_params(dim, params):\n    mean = params[0:dim]\n    cov = np.empty((dim, dim))\n    ii, jj = np.tril_indices(dim)\n    cov[ii, jj] = params[dim:]\n    cov[jj, ii] = params[dim:]\n    return mean, cov\n\ndef _obj_func(params, dim, data_blocks):\n    mean, cov = _unpack_params(dim, params)\n    # Check if cov is positive semidefinite.\n    # A matrix has a Cholesky decomposition iff it is symmetric and\n    # positive semidefinite.  It is said that Cholesky decomposition is\n    # faster and more numerically stable than finding eigenvalues.\n    # However, numpy.linalg.cholesky() rejects singular matrices (i.e.,\n    # strictly ""semi""-definite ones).\n    # try:\n    #     _ = np.linalg.cholesky(cov)\n    # except np.linalg.LinAlgError:\n    #     return np.inf\n    if (np.linalg.eigvalsh(cov) < 0).any():\n        # XXX Returning inf is not a good idea, because many solvers\n        # cannot cope with it.\n        return np.inf\n    objval = 0.0\n    for obs, obs_data in data_blocks:\n        obs_mean = mean[obs]\n        obs_cov = cov[obs][:, obs]\n        objval += _log_likelihood_composed(obs_data, obs_mean, obs_cov)\n    return -objval\n\ndef _obj_func_1d(params, dim, data):\n    mean, cov = _unpack_params(dim, params)\n    objval = 0.0\n    for x in data:\n        obs = ~np.isnan(x)\n        objval += _log_likelihood_1d(x[obs], mean[obs], cov[obs][:, obs])\n    return -objval\n\n# Composite function of _log_likelihood() and _pdf_normal().\ndef _log_likelihood_composed(x, mean, cov):\n    xshift = x - mean\n    t1 = x.shape[-1] * _log2pi\n    sign, logdet = np.linalg.slogdet(cov)\n    t2 = logdet\n    t3 = xshift.dot(np.linalg.inv(cov)) * xshift\n    size = x.shape[0] if x.ndim == 2 else 1\n    return -0.5 * ((t1 + t2) * size + t3.sum())\n\n# Log likelihood function.\n# The input x can be one- or two-dimensional.\ndef _log_likelihood(x, mean, cov):\n    return np.log(_pdf_normal(x, mean, cov)).sum()\n\n# Log likelihood function.\ndef _log_likelihood_1d(x, mean, cov):\n    return np.log(_pdf_normal_1d(x, mean, cov))\n\n# Probability density function of multivariate normal distribution.\n# The input x can be one- or two-dimensional.\ndef _pdf_normal(x, mean, cov):\n    xshift = x - mean\n    t1 = (2 * np.pi) ** (-0.5 * x.shape[-1])\n    t2 = np.linalg.det(cov) ** (-0.5)\n    t3 = -0.5 * (xshift.dot(np.linalg.inv(cov)) * xshift).sum(axis=-1)\n    return t1 * t2 * np.exp(t3)\n\n# Probability density function of multivariate normal distribution.\ndef _pdf_normal_1d(x, mean, cov):\n    xshift = x - mean\n    t1 = (2 * np.pi) ** (-0.5 * len(x))\n    t2 = np.linalg.det(cov) ** (-0.5)\n    t3 = -0.5 * xshift.dot(np.linalg.inv(cov)).dot(xshift)\n    return t1 * t2 * np.exp(t3)\n'"
test_fiml.py,29,"b'#\n# Copyright (c) 2016 KAMADA Ken\'ichi.\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n# 1. Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS\'\' AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n# OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n# OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n# SUCH DAMAGE.\n#\n\n""""""Tests for fiml module.\n""""""\n\nimport unittest\n\nimport fiml\nimport numpy as np\n\nclass TestFIML(unittest.TestCase):\n    def test_sort_missing(self):\n        data = np.array(\n            [[1, 2, 3],\n             [np.nan, 5, 6],\n             [7, np.nan, np.nan],\n             [np.nan, 11, 12],\n             [13, np.nan, np.nan]])\n        ans = [(np.array([False, True, True]),\n                np.array([[5., 6.],\n                          [11., 12.]])),\n               (np.array([True, False, False]),\n                np.array([[7.],\n                          [13.]])),\n               (np.array([True, True, True]),\n                np.array([[1., 2., 3.]]))]\n        data_blocks = fiml._sort_missing(data)\n        self.assertNpSeqEqual(ans, data_blocks)\n\n    def test_pack_params(self):\n        dim = 5\n        template = fiml._pack_params(dim, np.zeros(dim), np.eye(dim))\n        params = np.random.randn(len(template))\n        mean, cov = fiml._unpack_params(dim, params)\n        self.assertNpEqual(cov, cov.T)\n        params2 = fiml._pack_params(dim, mean, cov)\n        self.assertClose(params, params2)\n\n    def test_missing_2d(self):\n        data = np.array(\n            ((0, 0.4, 0.5, 0.6, 1),\n             (0, 0.6, np.nan, 0.4, 1))).T\n        ans_mean = np.array((0.5, 0.5))\n        ans_cov = np.array(((0.104, 0.096), (0.096, 701.0 / 6500)))\n        mean, cov = fiml.fiml(data, bias=True)\n        self.assertModestlyClose(ans_mean, mean)\n        self.assertModestlyClose(ans_cov, cov)\n\n    def test_not_missing_1d(self):\n        self._test_not_missing(100, 1)\n\n    def test_not_missing_2d(self):\n        self._test_not_missing(100, 2)\n\n    def test_not_missing_3d(self):\n        self._test_not_missing(100, 3)\n\n    def _test_not_missing(self, size, dim):\n        data = np.random.randn(size, dim)\n        mean1 = data.mean(axis=0)\n        cov1 = np.cov(data, rowvar=False, bias=True)\n        mean2, cov2 = fiml.fiml(data, bias=True)\n        self.assertModestlyClose(mean1, mean2)\n        self.assertModestlyClose(cov1, cov2)\n\n    # Test if _pdf_normal() and _log_likelihood() accepts\n    # both a 2-D ndarray (multiple observations) and a 1-D ndarray.\n    def test_1d_and_2d(self):\n        for dim in range(2, 10):\n            data = np.random.randn(dim * 2, dim)\n            m = data.mean(axis=0)\n            c = np.cov(data, rowvar=False)\n\n            x = np.random.randn(dim)\n            r1 = fiml._pdf_normal_1d(x, m, c)\n            r2 = fiml._pdf_normal(x, m, c)\n            self.assertClose(r1, r2)\n            r1 = fiml._log_likelihood_1d(x, m, c)\n            r2 = fiml._log_likelihood(x, m, c)\n            r3 = fiml._log_likelihood_composed(x, m, c)\n            self.assertClose(r1, r2)\n            self.assertClose(r1, r3)\n\n            xx = np.random.randn(3, dim)\n            r1 = np.array([fiml._pdf_normal_1d(x, m, c) for x in xx])\n            r2 = fiml._pdf_normal(xx, m, c)\n            self.assertClose(r1, r2)\n            r1 = sum([fiml._log_likelihood_1d(x, m, c) for x in xx])\n            r2 = fiml._log_likelihood(xx, m, c)\n            r3 = fiml._log_likelihood_composed(xx, m, c)\n            self.assertClose(r1, r2)\n            self.assertClose(r1, r3)\n\n    def assertNpEqual(self, expected, actual):\n        if (expected != actual).any():\n            self.fail(""{} != {}"".format(expected, actual))\n\n    def assertClose(self, expected, actual):\n        #self.assertTrue(np.allclose(expected, actual))\n        if not np.allclose(expected, actual):\n            self.fail(""{} != {}"".format(expected, actual))\n\n    def assertModestlyClose(self, expected, actual):\n        # The default xtol of scipy.optimize.fmin is 1e-4.\n        if not np.allclose(expected, actual, atol=1e-4):\n            self.fail(""{} != {}"".format(expected, actual))\n\n    def assertNpSeqEqual(self, expected, actual):\n        def recursive(seq1, seq2):\n            if type(seq1) is not type(seq2):\n                return False\n            if isinstance(seq1, np.ndarray):\n                return np.array_equal(seq1, seq2)\n            if isinstance(seq1, (list, tuple)):\n                if len(seq1) != len(seq2):\n                    return False\n                for sub1, sub2 in zip(seq1, seq2):\n                    if not recursive(sub1, sub2):\n                        return False\n                return True\n            return seq1 == seq2\n        if not recursive(expected, actual):\n            self.fail(""{} != {}"".format(expected, actual))\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
