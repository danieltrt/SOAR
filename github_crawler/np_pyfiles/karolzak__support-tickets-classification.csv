file_path,api_count,code
0_preprocess_data.py,8,"b'import os\nfrom sklearn import preprocessing\nimport sys\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom azureml.dataprep import package\nsys.path.append(""."")\nsys.path.append("".."")\n\nremovedWordsList = ([\'xxxxx1\'])\n\n\ndef removeNonEnglish(text, englishWords):\n    global removedWordsList\n    wordList = text.split()\n    if len(wordList) == 0:\n        return "" ""\n    y = np.array(wordList)\n    x = np.array(englishWords)\n    index = np.arange(len(englishWords))\n    sorted_index = np.searchsorted(x, y)\n    yindex = np.take(index, sorted_index, mode=""clip"")\n    mask = x[yindex] != y\n    maskedArr = np.ma.array(yindex, mask=mask).compressed()\n    result = x[maskedArr]\n    text = np.array2string(result)\\\n        .replace(""\\\'"", """")\\\n        .replace(""["", """")\\\n        .replace(""]"", """")\\\n        .replace(""\\n"", """")\\\n        .replace(""\\r"", """")\n\n    # Logging removed words\n    removedWords = set(wordList)-set(result)\n    removedWordsList += set(list(removedWords))-set(removedWordsList)\n    return text\n\n\ndef encryptSingleColumn(data):\n    le = preprocessing.LabelEncoder()\n    le.fit(data)\n    return le.transform(data)\n\n\ndef encryptColumnsCollection(data, columnsToEncrypt):\n    for column in columnsToEncrypt:\n        data[column] = encryptSingleColumn(data[column])\n    return data\n\n\ndef removeString(data, regex):\n    return data.str.lower().str.replace(regex.lower(), \' \')\n\n\ndef cleanDataset(dataset, columnsToClean, regexList):\n    for column in columnsToClean:\n        for regex in regexList:\n            dataset[column] = removeString(dataset[column], regex)\n    return dataset\n\n\ndef getRegexList():\n    regexList = []\n    regexList += [\'From:(.*)\\r\\n\']  # from line\n    # regexList += [\'RITM[0-9]*\'] # request id\n    # regexList += [\'INC[0-9]*\'] # incident id\n    # regexList += [\'TKT[0-9]*\'] # ticket id\n    regexList += [\'Sent:(.*)\\r\\n\']  # sent to line\n    regexList += [\'Received:(.*)\\r\\n\']  # received data line\n    regexList += [\'To:(.*)\\r\\n\']  # to line\n    regexList += [\'CC:(.*)\\r\\n\']  # cc line\n    regexList += [\'The information(.*)infection\']  # footer\n    regexList += [\'Endava Limited is a company(.*)or omissions\']  # footer\n    regexList += [\'The information in this email is confidential and may be legally(.*)interference if you are not the intended recipient\']  # footer\n    regexList += [\'\\[cid:(.*)]\']  # images cid\n    regexList += [\'https?:[^\\]\\n\\r]+\']  # https & http\n    regexList += [\'Subject:\']\n    # regexList += [\'[\\w\\d\\-\\_\\.]+@[\\w\\d\\-\\_\\.]+\']  # emails\n    # regexList += [\'[0-9][\\-0\xe2\x80\x9390-9 ]+\']  # phones\n    # regexList += [\'[0-9]\']  # numbers\n    # regexList += [\'[^a-zA-z 0-9]+\']  # anything that is not a letter\n    # regexList += [\'[\\r\\n]\']  # \\r\\n\n    # regexList += [\' [a-zA-Z] \']  # single letters\n    # regexList += [\' [a-zA-Z][a-zA-Z] \']  # two-letter words\n    # regexList += [""  ""]  # double spaces\n\n    regexList += [\'^[_a-z0-9-]+(\\.[_a-z0-9-]+)*@[a-z0-9-]+(\\.[a-z0-9-]+)*(\\.[a-z]{2,4})$\']\n    regexList += [\'[\\w\\d\\-\\_\\.]+ @ [\\w\\d\\-\\_\\.]+\']\n    regexList += [\'Subject:\']\n    regexList += [\'[^a-zA-Z]\']\n\n    return regexList\n\n\nif __name__ == \'__main__\':\n    ####################\n    # Use this with AML Workbench to load data from data prep file\n    # dfIncidents = package.run(\'Incidents.dprep\', dataflow_idx=0)\n    # dfIncidents = pd.read_csv(\'allIncidents.csv\', encoding=""ISO-8859-1"")\n    # dfRequests = package.run(\'Requests.dprep\', dataflow_idx=0)\n    dfIncidents = package.run(\'IncidentsCleaned.dprep\', dataflow_idx=0)\n    dfRequests = package.run(\'RequestsCleaned.dprep\', dataflow_idx=0)\n\n    # Load dataset from file\n    # dfIncidents = pd.read_csv(\'./data/endava_tickets/all_incidents.csv\')\n    # dfRequests = pd.read_csv(\'./data/endava_tickets/all_requests.csv\')\n    #####################\n\n    # Reorder columns\n    columnsOrder = [\n        \'title\', \'body\', \'ticket_type\', \'category\',\n        \'sub_category1\', \'sub_category2\', \'business_service\',\n        \'urgency\', \'impact\'\n    ]\n    dfIncidents = dfIncidents[columnsOrder]\n    dfRequests = dfRequests[columnsOrder]\n    print(dfIncidents.shape)\n    print(dfRequests.shape)\n\n    # Merge incidents and requests datasets\n    dfTickets = dfRequests.append(\n        dfIncidents,\n        ignore_index=True)  # set True to avoid index duplicates\n    print(dfTickets.shape)\n\n    # Remove duplicates\n    columnsToDropDuplicates = [\'body\']\n    dfTickets = dfTickets.drop_duplicates(columnsToDropDuplicates)\n    print(dfTickets.shape)\n\n    # Merge \'title\' and \'body\' columns into single column \'body\'\n    # dfTickets[\'body\'] = (dfTickets[\'title\']+\n    #   "" "" + dfTickets[\'body\']).map(str)\n    # dfTickets = dfTickets.drop([\'title\'], axis=1)\n\n    # Select columns for cleaning\n    columnsToClean = [\'body\', \'title\']\n\n    # Create list of regex to remove sensitive data\n    # Clean dataset and remove sensitive data\n    cleanDataset(dfTickets, columnsToClean, getRegexList())\n\n    ########################################\n    # Remove all non english words + names #\n    ########################################\n    # Firstly load english words dataset and names dataset\n    # dfWordsEn = package.run(\'EnglishWords.dprep\', dataflow_idx=0)\n    # dfWordsEn = package.run(\'EnglishWordsAlpha.dprep\', dataflow_idx=0)\n    # dfWordsEn = package.run(\'EnglishWordsMerged.dprep\', dataflow_idx=0)\n    dfWordsEn = package.run(\'WordsEn.dprep\', dataflow_idx=0)\n    dfFirstNames = package.run(\'FirstNames.dprep\', dataflow_idx=0)\n    dfBlackListWords = package.run(\'WordsBlacklist.dprep\', dataflow_idx=0)\n\n    # Transform all words to lower case\n    dfWordsEn[\'Line\'] = dfWordsEn[\'Line\'].str.lower()\n    dfFirstNames[\'Line\'] = dfFirstNames[\'Line\'].str.lower()\n    dfBlackListWords[\'Line\'] = dfBlackListWords[\'Line\'].str.lower()\n\n    # Merge datasets removing names from English words dataset\n    print(""Shape before removing first names from\\\n        english words dataset: ""+str(dfWordsEn.shape))\n    dfWords = dfWordsEn.merge(\n        dfFirstNames.drop_duplicates(),\n        on=[\'Line\'], how=\'left\', indicator=True)\n    # Select words without names only\n    dfWords = dfWords.loc[dfWords[\'_merge\'] == \'left_only\']\n    print(""Shape after removing first names from\\\n        english words dataset: ""+str(dfWords.shape))\n    dfWords = dfWords.drop(""_merge"", axis=1)  # Drop merge indicator column\n\n    # Merge datasets removing blacklisted words\n    print(""Shape before removing blacklisted\\\n        words from english ords dataset: ""+str(dfWords.shape))\n    dfWords = dfWords.merge(\n        dfBlackListWords.drop_duplicates(),\n        on=[\'Line\'], how=\'left\', indicator=True\n    )\n    # Select words\n    dfWords = dfWords.loc[dfWords[\'_merge\'] == \'left_only\']\n    print(""Shape after removing blacklisted\\\n        words from english words dataset: ""+str(dfWords.shape))\n\n    # Remove non english words and names\n    dfTickets[\'body\'] = dfTickets[\'body\'].apply(\n        lambda emailBody: removeNonEnglish(emailBody, dfWords[\'Line\']))\n    dfTickets[\'title\'] = dfTickets[\'title\'].apply(\n        lambda emailBody: removeNonEnglish(emailBody, dfWords[\'Line\']))\n\n    # Remove empty strings and null rows after removing non english words\n    print(""Before removing empty: "" + str(dfTickets.shape))\n    dfTickets = dfTickets[dfTickets.body != "" ""]\n    dfTickets = dfTickets[dfTickets.body != """"]\n    dfTickets = dfTickets[~dfTickets.body.isnull()]\n    print(""After removing empty: "" + str(dfTickets.shape))\n\n    ########################################################\n    # Data encryption and anonymization using LabelEncoder #\n    ########################################################\n    # Select columns for encryption\n    columnsToEncrypt = [\n        \'category\', \'sub_category1\', \'sub_category2\',\n        \'business_service\', \'urgency\',\n        \'impact\', \'ticket_type\'\n    ]\n\n    # Encrypt data for each of selected columns\n    dfTickets = encryptColumnsCollection(dfTickets, columnsToEncrypt)\n\n    ##########################\n\n    # Remove duplicates x2\n    columnsToDropDuplicates = [\'body\']\n    dfTickets = dfTickets.drop_duplicates(columnsToDropDuplicates)\n    print(dfTickets.shape)\n\n    # Save cleaned and encrypted dataset back to csv without indexes\n    dfTickets.to_csv(\'all_tickets.csv\', index=False, index_label=False)\n    sortedRemovedWordsList = np.sort(removedWordsList)\n    dfx = pd.DataFrame(sortedRemovedWordsList)\n    dfx.to_csv(""removed_words.csv"", index=False, index_label=False)\n'"
1_download_dataset.py,0,"b'\nfrom __future__ import print_function\nimport os\ntry:\n    from urllib.request import urlretrieve\nexcept ImportError:\n    from urllib import urlretrieve\n\n\ndef download_file(file_url, folder_path, file_name):\n    file_path = os.path.join(folder_path, file_name)\n    if not os.path.exists(file_path):\n        print(\'Downloading file from \' + file_url + \'...\')\n        urlretrieve(file_url, file_path)\n        print(\'Done downloading file: \'+file_path)\n    else:\n        print(\'File: \' + file_path + \' already exists.\')\n\n\ndef download_dataset():\n    print(\'Downloading Endava support tickets dataset...\')\n    folder_path = os.path.join(\n        os.path.dirname(\n            os.path.abspath(__file__)\n        ),\n        \'datasets\'\n    )\n    url = ""https://privdatastorage.blob.core.windows.net/github/support-tickets-classification/datasets/all_tickets.csv""\n    file_name = \'all_tickets.csv\'\n    download_file(url, folder_path, file_name)\n\n\nif __name__ == ""__main__"":\n    download_dataset()\n'"
2_train_and_eval_model.py,8,"b'import sys\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport pickle\n# from helpers import *\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nimport os\nfrom matplotlib import pyplot as plt\nsys.path.append(""."")\nsys.path.append("".."")\n# Use the Azure Machine Learning data preparation package\n# from azureml.dataprep import package\n\n\nclass StemmedCountVectorizer(CountVectorizer):\n    def build_analyzer(self):\n        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n        stemmer = SnowballStemmer(""english"", ignore_stopwords=True)\n        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n\n\ncolumn_to_predict = ""ticket_type""\n# Supported datasets:\n# ticket_type\n# business_service\n# category\n# impact\n# urgency\n# sub_category1\n# sub_category2\n\nclassifier = ""NB""  # Supported algorithms # ""SVM"" # ""NB""\nuse_grid_search = False  # grid search is used to find hyperparameters. Searching for hyperparameters is time consuming\nremove_stop_words = True  # removes stop words from processed text\nstop_words_lang = \'english\'  # used with \'remove_stop_words\' and defines language of stop words collection\nuse_stemming = False  # word stemming using nltk\nfit_prior = True  # if use_stemming == True then it should be set to False ?? double check\nmin_data_per_class = 1  # used to determine number of samples required for each class.Classes with less than that will be excluded from the dataset. default value is 1\n\nif __name__ == \'__main__\':\n\n    # TODO Add download dataset\n     \n    # loading dataset from dprep in Workbench    \n    # dfTickets = package.run(\'AllTickets.dprep\', dataflow_idx=0) \n\n    # loading dataset from csv\n    dfTickets = pd.read_csv(\n        \'./datasets/all_tickets.csv\',\n        dtype=str\n    )  \n\n    text_columns = ""body""  # ""title"" - text columns used for TF-IDF\n    \n    # Removing rows related to classes represented by low amount of data\n    print(""Shape of dataset before removing classes with less then "" + str(min_data_per_class) + "" rows: ""+str(dfTickets.shape))\n    print(""Number of classes before removing classes with less then "" + str(min_data_per_class) + "" rows: ""+str(len(np.unique(dfTickets[column_to_predict]))))\n    bytag = dfTickets.groupby(column_to_predict).aggregate(np.count_nonzero)\n    tags = bytag[bytag.body > min_data_per_class].index\n    dfTickets = dfTickets[dfTickets[column_to_predict].isin(tags)]\n    print(\n        ""Shape of dataset after removing classes with less then ""\n        + str(min_data_per_class) + "" rows: ""\n        + str(dfTickets.shape)\n    )\n    print(\n        ""Number of classes after removing classes with less then ""\n        + str(min_data_per_class) + "" rows: ""\n        + str(len(np.unique(dfTickets[column_to_predict])))\n    )\n\n    labelData = dfTickets[column_to_predict]\n    data = dfTickets[text_columns]\n\n    # Split dataset into training and testing data\n    train_data, test_data, train_labels, test_labels = train_test_split(\n        data, labelData, test_size=0.2\n    )  # split data to train/test sets with 80:20 ratio\n\n    # Extracting features from text\n    # Count vectorizer\n    if remove_stop_words:\n        count_vect = CountVectorizer(stop_words=stop_words_lang)\n    elif use_stemming:\n        count_vect = StemmedCountVectorizer(stop_words=stop_words_lang)\n    else:\n        count_vect = CountVectorizer()\n\n    # Fitting the training data into a data processing pipeline and eventually into the model itself\n    if classifier == ""NB"":\n        print(""Training NB classifier"")\n        # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n        # The names \xe2\x80\x98vect\xe2\x80\x99 , \xe2\x80\x98tfidf\xe2\x80\x99 and \xe2\x80\x98clf\xe2\x80\x99 are arbitrary but will be used later.\n        # We will be using the \'text_clf\' going forward.\n\n        text_clf = Pipeline([\n            (\'vect\', count_vect),\n            (\'tfidf\', TfidfTransformer()),\n            (\'clf\', MultinomialNB(fit_prior=fit_prior))\n        ])\n        text_clf = text_clf.fit(train_data, train_labels)\n\n    elif classifier == ""SVM"":\n        print(""Training SVM classifier"")\n        # Training Support Vector Machines - SVM\n        text_clf = Pipeline([(\n            \'vect\', count_vect),\n            (\'tfidf\', TfidfTransformer()),\n            (\'clf\', SGDClassifier(\n                loss=\'hinge\', penalty=\'l2\', alpha=1e-3,\n                n_iter=5, random_state=42\n            )\n        )])\n        text_clf = text_clf.fit(train_data, train_labels)\n\n    if use_grid_search:\n        # Grid Search\n        # Here, we are creating a list of parameters for which we would like to do performance tuning.\n        # All the parameters name start with the classifier name (remember the arbitrary name we gave).\n        # E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.\n        \n        # NB parameters\n        parameters = {\n            \'vect__ngram_range\': [(1, 1), (1, 2)],\n            \'tfidf__use_idf\': (True, False),\n            \'clf__alpha\': (1e-2, 1e-3)\n        }\n\n        # SVM parameters\n        # parameters = {\n        #    \'vect__max_df\': (0.5, 0.75, 1.0),\n        #    \'vect__max_features\': (None, 5000, 10000, 50000),\n        #    \'vect__ngram_range\': ((1, 1), (1, 2)),  # unigrams or bigrams\n        #    \'tfidf__use_idf\': (True, False),\n        #    \'tfidf__norm\': (\'l1\', \'l2\'),\n        #    \'clf__alpha\': (0.00001, 0.000001),\n        #    \'clf__penalty\': (\'l2\', \'elasticnet\'),\n        #    \'clf__n_iter\': (10, 50, 80),\n        # }\n\n        # Next, we create an instance of the grid search by passing the classifier, parameters\n        # and n_jobs=-1 which tells to use multiple cores from user machine.\n        gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n        gs_clf = gs_clf.fit(train_data, train_labels)\n\n        # To see the best mean score and the params, run the following code\n        gs_clf.best_score_\n        gs_clf.best_params_\n\n    print(""Evaluating model"")\n    # Score and evaluate model on test data using model without hyperparameter tuning\n    predicted = text_clf.predict(test_data)\n    prediction_acc = np.mean(predicted == test_labels)\n    print(""Confusion matrix without GridSearch:"")\n    print(metrics.confusion_matrix(test_labels, predicted))\n    print(""Mean without GridSearch: "" + str(prediction_acc))\n\n    # Score and evaluate model on test data using model WITH hyperparameter tuning\n    if use_grid_search:\n        predicted = gs_clf.predict(test_data)\n        prediction_acc = np.mean(predicted == test_labels)\n        print(""Confusion matrix with GridSearch:"")\n        print(metrics.confusion_matrix(test_labels, predicted))\n        print(""Mean with GridSearch: "" + str(prediction_acc))\n\n    # Ploting confusion matrix with \'seaborn\' module\n    # Use below line only with Jupyter Notebook\n    # %matplotlib inline\n    import seaborn as sns\n    from sklearn.metrics import confusion_matrix\n    import matplotlib.pyplot as plt\n    import matplotlib\n    mat = confusion_matrix(test_labels, predicted)\n    plt.figure(figsize=(4, 4))\n    sns.set()\n    sns.heatmap(mat.T, square=True, annot=True, fmt=\'d\', cbar=False,\n                xticklabels=np.unique(test_labels),\n                yticklabels=np.unique(test_labels))\n    plt.xlabel(\'true label\')\n    plt.ylabel(\'predicted label\')\n    # Save confusion matrix to outputs in Workbench\n    # plt.savefig(os.path.join(\'.\', \'outputs\', \'confusion_matrix.png\'))\n    plt.show()\n\n    # Printing classification report\n    # Use below line only with Jupyter Notebook\n    from sklearn.metrics import classification_report\n    print(classification_report(test_labels, predicted,\n                                target_names=np.unique(test_labels)))\n\n    # Save trained models to /output folder\n    # Use with Workbench\n    if use_grid_search:\n        pickle.dump(\n            gs_clf,\n            open(os.path.join(\n                \'.\', \'outputs\', column_to_predict+"".model""),\n                \'wb\'\n            )\n        )\n    else:\n        pickle.dump(\n            text_clf,\n            open(os.path.join(\n                \'.\', \'outputs\', column_to_predict+"".model""),\n                \'wb\'\n            )\n        )\n'"
webservice/webservice.py,0,"b'import sys\nfrom flask import Flask, jsonify, request, make_response, abort\nimport os\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nimport time\nimport logging\nimport pickle\nimport re\n# Use with Azure Web Apps\nos.environ[\'PATH\'] = r\'D:\\home\\python354x64;\' + os.environ[\'PATH\']\nsys.path.append(""."")\nsys.path.append("".."")\nsys.path.append(""webservice/models"")\nsys.path.append(""wwwroot/models"")\napp = Flask(__name__)\n__location__ = os.path.realpath(os.path.join(\n    os.getcwd(), os.path.dirname(__file__), \'models\'))\n\n# Download models\nfrom models.download_models import download_file, download_models\ndownload_models()\n\n# Loading models\nmodel_impact = pickle.load(\n    open(\n        os.path.join(__location__, ""impact.model""), ""rb""\n    )\n)\nmodel_ticket_type = pickle.load(\n    open(\n        os.path.join(__location__, ""ticket_type.model""), ""rb""\n    )\n)\nmodel_category = pickle.load(\n    open(\n        os.path.join(__location__, ""category.model""), ""rb""\n    )\n)\n\n\n@app.errorhandler(404)\ndef not_found(error):\n    return make_response(jsonify({\'error\': \'Not found\'}), 404)\n\n\n@app.route(\'/\')\ndef index():\n    return """"""\n        <html>\n        <body>\n        Hello, World!<br>\n        This is a sample web service written in Python using <a href=""""http://flask.pocoo.org/"""">Flask</a> module.<br>\n        </body>\n        </html>\n        """"""\n\n\n@app.route(\'/endava/api/v1.0/predictall\', methods=[\'POST\'])\ndef predictall():\n    ts = time.gmtime()\n    logging.info(""Request received - %s"" % time.strftime(""%Y-%m-%d %H:%M:%S"", ts))\n    if (not request.json) or (\'description\' not in request.json):\n        abort(400)\n    description = request.json[\'description\']\n    description = preprocess_data(description)\n\n    predicted_ticket_type = model_ticket_type.predict([description])[0]\n    print(""predicted ticket_type: ""+str(predicted_ticket_type))\n\n    predicted_category = model_category.predict([description])[0]\n    print(""predicted category: ""+str(predicted_category))\n\n    predicted_impact = model_impact.predict([description])[0]\n    print(""predicted impact: ""+str(predicted_impact))\n\n    # predicted_business_service = model_business_service.predict([description])[0]\n    # print(""predicted business_service: ""+str(predicted_business_service))\n\n    # predicted_urgency = model_urgency.predict([description])[0]\n    # print(""predicted urgency: ""+str(predicted_urgency))\n\n    ts = time.gmtime()\n    logging.info(\n        ""Request sent to evaluation - %s""\n        % time.strftime(""%Y-%m-%d %H:%M:%S"", ts)\n    )\n    return jsonify({\n        ""description"": description,\n        ""ticket_type"": predicted_ticket_type,\n        # ""business_service"": predicted_business_service,\n        ""category"": predicted_category,\n        ""impact"": predicted_impact\n    })\n\n\n@app.route(\'/endava/api/v1.0/category\', methods=[\'POST\'])\ndef category1():\n    ts = time.gmtime()\n    logging.info(""Request received - %s"" % time.strftime(""%Y-%m-%d %H:%M:%S"", ts))\n    print(request)\n    print(request.json)\n    if not request.json or \'description\' not in request.json:\n        abort(400)\n    description = request.json[\'description\']\n    print(description)\n\n    predicted = model_category.predict([description])\n    print(""Predicted: ""+str(predicted))\n\n    ts = time.gmtime()\n    logging.info(""Request sent to evaluation - %s"" % time.strftime(""%Y-%m-%d %H:%M:%S"", ts))\n    return jsonify({""category"": predicted[0]})\n\n\n@app.route(\'/endava/api/v1.0/tickettype\', methods=[\'POST\'])\ndef tickettype():\n    ts = time.gmtime()\n    logging.info(""Request received - %s"" % time.strftime(""%Y-%m-%d %H:%M:%S"", ts))\n    print(request)\n    print(request.json)\n    if not request.json or \'description\' not in request.json:\n        abort(400)\n    description = request.json[\'description\']\n    print(description)\n\n    predicted = model_ticket_type.predict([description])\n    print(""Predicted: "" + str(predicted))\n\n    ts = time.gmtime()\n    logging.info(""Request sent to evaluation - %s"" % time.strftime(""%Y-%m-%d %H:%M:%S"", ts))\n    return jsonify({""ticket_type"": predicted[0]})\n\n\n# Data prep - much to improve :)\nregexArr1 = []\nregexArr2 = []\n\n\ndef getRegexList1():\n    regexList = []\n    regexList += [\'From:(.*)\']  # from line\n    regexList += [\'Sent:(.*)\']  # sent to line\n    regexList += [\'Received:(.*)\']  # received data line\n    regexList += [\'To:(.*)\']  # to line\n    regexList += [\'CC:(.*)\']  # cc line\n    regexList += [\'https?:[^\\]\\n\\r]+\']  # https & http\n    regexList += [\'Subject:\']\n    regexList += [\'[\\w\\d\\-\\_\\.]+@[\\w\\d\\-\\_\\.]+\']  # emails\n    return regexList\n\n\ndef getRegexList2():\n    regexList = []\n    regexList += [\'From:\']  # from line\n    regexList += [\'Sent:\']  # sent to line\n    regexList += [\'Received:\']  # received data line\n    regexList += [\'To:\']  # to line\n    regexList += [\'CC:\']  # cc line\n    regexList += [\'The information(.*)infection\']  # footer\n    regexList += [\'Endava Limited is a company(.*)or omissions\']  # footer\n    regexList += [\'The information in this email is confidential and may be legally(.*)interference if you are not the intended recipient\']  # footer\n    regexList += [\'\\[cid:(.*)]\']  # images cid\n    regexList += [\'https?:[^\\]\\n\\r]+\']  # https & http\n    regexList += [\'Subject:\']\n    regexList += [\'[\\w\\d\\-\\_\\.]+@[\\w\\d\\-\\_\\.]+\']  # emails\n    regexList += [\'[\\\\r]\']  # \\r\\n\n    regexList += [\'[\\\\n]\']  # \\r\\n\n\n    regexList += [\'^[_a-z0-9-]+(\\.[_a-z0-9-]+)*@[a-z0-9-]+(\\.[a-z0-9-]+)*(\\.[a-z]{2,4})$\']\n    regexList += [\'[^a-zA-Z]\']\n\n    return regexList\n\n\ndef preprocess_data(data):\n    print(data)\n    content = data.lower()\n    content = content.split(\'\\\\n\')\n\n    for word in content:\n        for regex in regexArr1:\n            word = re.sub(regex.lower(), \' \', word)\n\n    print(content)\n    content = """".join(content)\n    print(content)\n\n    for regex in regexArr2:\n        content = re.sub(regex.lower(), \' \', content)\n    print(content)\n\n    return content\n\n\nif __name__ == \'__main__\':\n    HOST = os.environ.get(\'SERVER_HOST\', \'localhost\')\n    try:\n        PORT = int(os.environ.get(\'SERVER_PORT\', \'5555\'))\n    except ValueError:\n        PORT = 5555\n    regexArr1 = getRegexList1()\n    regexArr2 = getRegexList2()\n\n    app.run(HOST, PORT)\n'"
webservice/models/download_models.py,0,"b'from __future__ import print_function\nimport os\ntry:\n    from urllib.request import urlretrieve\nexcept ImportError:\n    from urllib import urlretrieve\n\n\ndef download_file(file_url, folder_path, file_name):\n    file_path = os.path.join(folder_path, file_name)\n    if not os.path.exists(file_path):\n        print(\'Downloading file from \' + file_url + \'...\')\n        urlretrieve(file_url, file_path)\n        print(\'Done downloading file: \' + file_path)\n    else:\n        print(\'File: \' + file_path + \' already exists.\')\n\n\ndef download_models():\n    print(\'Downloading models for web service...\')\n    file_list = [\'category.model\', \'impact.model\', \'ticket_type.model\']\n    folder_path = os.path.dirname(os.path.abspath(__file__))\n    url = ""https://privdatastorage.blob.core.windows.net/github/support-tickets-classification/models/""\n    for file_name in file_list:\n        download_file(url + file_name, folder_path, file_name)\n\n\nif __name__ == ""__main__"":\n    download_models()\n'"
