file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n\nfrom setuptools import find_packages, setup\n\nwith open(""README.md"", ""r"") as fh:\n    long_description = fh.read()\n\nsetup(\n    name=""zappy"",\n    version=""0.2.0"",\n    description=""Distributed processing with NumPy and Zarr."",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    packages=find_packages(exclude=(""tests"",)),\n    classifiers=[\n        ""Development Status :: 3 - Alpha"",\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Information Technology"",\n        ""Intended Audience :: Science/Research"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Operating System :: OS Independent"",\n        ""Programming Language :: Python :: 2"",\n        ""Programming Language :: Python :: 2.7"",\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.6"",\n    ],\n    author=""Tom White"",\n    author_email=""tom.e.white@gmail.com"",\n    url=""https://github.com/lasersonlab/zappy"",\n)\n'"
pywren-scripts/analyze.py,0,"b'import glob\nimport matplotlib.pyplot as plt\nimport os\nimport pickle\n\nfrom timeline import plot_jobs\nfrom util import job_df_from_futures, normalize_times\n\nlogs_dir = os.path.expanduser(""~/.zappy/logs"")\nlog_files = glob.glob(os.path.join(logs_dir, ""*""))\nlog_files.sort()\n\nmost_recent_file = log_files[-1]\n\nprint(""Opening %s"" % most_recent_file)\n\nwith open(most_recent_file, ""rb"") as file:\n    indict = pickle.load(file)\n\n    func_size_bytes = indict[""futures""][0]._invoke_metadata[""func_module_str_len""]\n    print(""Function size in bytes: %d"" % func_size_bytes)\n\n    df = job_df_from_futures(\n        indict[""futures""], indict[""invoke_statuses""], indict[""run_statuses""]\n    )\n    df = normalize_times(df)\n    plot_jobs(df)\n    plt.show()\n'"
pywren-scripts/pywren_timing.py,1,"b'import pywren\nimport zappy.base as np\nimport zappy.executor\n\nexecutor = zappy.executor.PywrenExecutor()\na = zappy.executor.ones(executor, (10, 2), chunks=(2, 2), dtype=""i4"")\n\nimport s3fs.mapping\n\ns3 = s3fs.S3FileSystem()\npath = ""sc-tom-test-data/ones.zarr""\noutput_zarr = s3fs.mapping.S3Map(path, s3=s3)\n\nout = np.log1p(a)\nout.to_zarr(output_zarr, a.chunks)\n\ns3.ls(path)\n\ns3.rm(path, recursive=True)\n'"
pywren-scripts/pywren_timing_log1p.py,1,"b'import s3fs.mapping\nimport zappy.base as np\nimport zappy.executor\n\ns3 = s3fs.S3FileSystem()\n# using sc-tom-test-data/10x.zarr/X fails because the chunks sizes are too large for lambda memory\ninput_zarr = s3fs.mapping.S3Map(""sc-tom-test-data/10x/anndata_zarr_2000/10x.zarr/X"", s3=s3)\noutput_zarr = s3fs.mapping.S3Map(""sc-tom-test-data/10x-log1p.zarr"", s3=s3)\n\nexecutor = zappy.executor.PywrenExecutor()\nx = zappy.executor.from_zarr(executor, input_zarr)\n\nout = np.log1p(x)\nout.to_zarr(output_zarr, x.chunks)\n'"
pywren-scripts/pywren_timing_mem_error.py,1,"b'import pywren\nimport zappy.base as np\nimport zappy.executor\n\nexecutor = zappy.executor.PywrenExecutor()\na = zappy.executor.ones(executor, (20000, 28000), chunks=(10000, 28000), dtype=float)\n\nimport s3fs.mapping\n\ns3 = s3fs.S3FileSystem()\npath = ""sc-tom-test-data/ones.zarr""\noutput_zarr = s3fs.mapping.S3Map(path, s3=s3)\n\nnp.log1p(a, out=a)\na.to_zarr(output_zarr, a.chunks)\n\ns3.ls(path)\n\ns3.rm(path, recursive=True)\n'"
pywren-scripts/timeline.py,4,"b'import numpy as np\nimport pandas as pd\n\nimport matplotlib.patches as mpatches\nfrom matplotlib import pylab\nimport seaborn as sns\n\nimport util\n\n\ndef plot_jobs(normalized_times_df, point_size=4, plot_step=100, ax=None):\n    """"""\n    Plot basic history of job runtimes\n    """"""\n\n    palette = sns.color_palette(""hls"", len(util.DEFAULT_FIELDS))\n    create_fig = ax is None\n\n    if create_fig:\n        fig = pylab.figure(figsize=(10, 10))\n        ax = fig.add_subplot(1, 1, 1)\n    total_jobs = len(normalized_times_df)\n\n    y = np.arange(len(normalized_times_df))\n    point_size = 8\n\n    patches = []\n    for f_i, field_name in enumerate(util.DEFAULT_FIELDS):\n        ax.scatter(\n            normalized_times_df[field_name],\n            y,\n            c=palette[f_i],\n            edgecolor=""none"",\n            s=point_size,\n            alpha=0.8,\n        )\n\n        patches.append(mpatches.Patch(color=palette[f_i], label=field_name))\n    ax.set_xlabel(""wallclock time (sec)"")\n    ax.set_ylabel(""job"")\n\n    legend = pylab.legend(handles=patches, loc=""upper right"", frameon=True)\n    legend.get_frame().set_facecolor(""#FFFFFF"")\n\n    ax.xaxis.set_major_locator(pylab.MaxNLocator(min_n_ticks=10))\n\n    plot_step = 100  # int(np.min([128, total_jobs/32]))\n    y_ticks = np.arange(total_jobs // plot_step + 2) * plot_step\n    ax.set_yticks(y_ticks)\n    ax.set_ylim(-0.02 * total_jobs, total_jobs * 1.05)\n\n    ax.set_xlim(-5, np.max(normalized_times_df[""results returned""]) * 1.05)\n    for y in y_ticks:\n        ax.axhline(y, c=""k"", alpha=0.1, linewidth=1)\n\n    ax.grid(False)\n    if create_fig:\n        fig.tight_layout()\n    return ax\n'"
pywren-scripts/util.py,1,"b'import pandas as pd\nimport numpy as np\n\n\ndef job_df_from_futures(futures=None, invoke_statuses=None, run_statuses=None):\n    """"""\n    Extract all of the runtime info from a list of futures into\n    a single pandas dataframe\n    """"""\n\n    if invoke_statuses is None:\n        invoke_statuses = [f.invoke_status for f in futures]\n    if run_statuses is None:\n        run_statuses = [f.run_status for f in futures]\n\n    invoke_df = pd.DataFrame(invoke_statuses)\n    run_df = pd.DataFrame(run_statuses)\n    del run_df[""host_submit_time""]\n    results_df = pd.concat([run_df, invoke_df], axis=1)\n    return results_df\n\n\nDEFAULT_FIELDS = [\n    ""data uploaded"",\n    ""func uploaded"",\n    ""host submit"",\n    ""job start"",\n    ""setup done"",\n    ""job done"",\n    ""results returned"",\n]\n\n\ndef normalize_times(results_df, return_time_offset=False):\n    """"""\n    Normalize the times of a collection of job results, and return the \n    resulting dataframe\n    """"""\n\n    time_offset = np.min(results_df.data_upload_timestamp - results_df.data_upload_time)\n\n    data = {\n        ""data uploaded"": results_df.data_upload_timestamp - time_offset,\n        ""func uploaded"": results_df.func_upload_timestamp - time_offset,\n        ""host submit"": results_df.host_submit_time - time_offset,\n        ""job start"": results_df.start_time - time_offset,\n        ""setup done"": results_df.start_time + results_df.setup_time - time_offset,\n        ""job done"": results_df.end_time - time_offset,\n        ""results returned"": results_df.download_output_timestamp - time_offset,\n    }\n\n    df = pd.DataFrame(data)\n    if return_time_offset:\n        return df, time_offset\n    return df\n'"
tests/__init__.py,0,b''
tests/test_array.py,61,"b'import concurrent.futures\nimport logging\nimport pytest\nimport sys\nimport numpy as np\nimport zappy.executor\nimport zappy.direct\nimport zappy.spark\nimport zarr\n\nfrom numpy.testing import assert_allclose\nfrom pyspark.sql import SparkSession\n\n# add/change to ""pywren_ndarray"" to run the tests using Pywren (requires Pywren to be installed)\nTESTS = [\n    ""direct_ndarray"",\n    ""direct_zarr"",\n    ""executor_ndarray"",\n    ""executor_zarr"",\n    ""spark_ndarray"",\n    ""spark_zarr"",\n]\n\n# only run Beam tests on Python 2, and don\'t run executor tests\nif sys.version_info[0] == 2:\n    import apache_beam as beam\n    from apache_beam.options.pipeline_options import PipelineOptions\n    import zappy.beam\n\n    TESTS = [\n        ""direct_ndarray"",\n        ""direct_zarr"",\n        ""spark_ndarray"",\n        ""spark_zarr"",\n        ""beam_ndarray"",\n        ""beam_zarr"",\n    ]\n\n\nclass TestZappyArray:\n    @pytest.fixture()\n    def x(self):\n        return np.array(\n            [\n                [0.0, 1.0, 0.0, 3.0, 0.0],\n                [2.0, 0.0, 3.0, 4.0, 5.0],\n                [4.0, 0.0, 0.0, 6.0, 7.0],\n            ]\n        )\n\n    @pytest.fixture()\n    def chunks(self):\n        return (2, 5)\n\n    @pytest.fixture()\n    def xz(self, x, chunks, tmpdir):\n        input_file_zarr = str(tmpdir.join(""x.zarr""))\n        z = zarr.open(\n            input_file_zarr, mode=""w"", shape=x.shape, dtype=x.dtype, chunks=chunks\n        )\n        z[:] = x.copy()  # write as zarr locally\n        return input_file_zarr\n\n    @pytest.fixture(scope=""module"")\n    def sc(self):\n        logger = logging.getLogger(""py4j"")\n        logger.setLevel(logging.WARN)\n        spark = (\n            SparkSession.builder.master(""local[2]"")\n            .appName(""my-local-testing-pyspark-context"")\n            .getOrCreate()\n        )\n        yield spark.sparkContext\n        spark.stop()\n\n    @pytest.fixture(params=TESTS)\n    def xd(self, sc, x, xz, chunks, request):\n        if request.param == ""direct_ndarray"":\n            yield zappy.direct.from_ndarray(x.copy(), chunks)\n        elif request.param == ""direct_zarr"":\n            yield zappy.direct.from_zarr(xz)\n        elif request.param == ""executor_ndarray"":\n            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n                yield zappy.executor.from_ndarray(executor, x.copy(), chunks)\n        elif request.param == ""executor_zarr"":\n            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n                yield zappy.executor.from_zarr(executor, xz)\n        elif request.param == ""spark_ndarray"":\n            yield zappy.spark.from_ndarray(sc, x.copy(), chunks)\n        elif request.param == ""spark_zarr"":\n            yield zappy.spark.from_zarr(sc, xz)\n        elif request.param == ""beam_ndarray"":\n            pipeline_options = PipelineOptions()\n            pipeline = beam.Pipeline(options=pipeline_options)\n            yield zappy.beam.from_ndarray(pipeline, x.copy(), chunks)\n        elif request.param == ""beam_zarr"":\n            pipeline_options = PipelineOptions()\n            pipeline = beam.Pipeline(options=pipeline_options)\n            yield zappy.beam.from_zarr(pipeline, xz)\n        elif request.param == ""pywren_ndarray"":\n            executor = zappy.executor.PywrenExecutor()\n            yield zappy.executor.from_ndarray(executor, x.copy(), chunks)\n\n    @pytest.fixture(params=TESTS)\n    def xd_and_temp_store(self, sc, x, xz, chunks, request):\n        if request.param == ""direct_ndarray"":\n            yield zappy.direct.from_ndarray(x.copy(), chunks), zarr.TempStore()\n        elif request.param == ""direct_zarr"":\n            yield zappy.direct.from_zarr(xz), zarr.TempStore()\n        elif request.param == ""executor_ndarray"":\n            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n                yield zappy.executor.from_ndarray(\n                    executor, x.copy(), chunks\n                ), zarr.TempStore()\n        elif request.param == ""executor_zarr"":\n            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n                yield zappy.executor.from_zarr(executor, xz), zarr.TempStore()\n        elif request.param == ""spark_ndarray"":\n            yield zappy.spark.from_ndarray(sc, x.copy(), chunks), zarr.TempStore()\n        elif request.param == ""spark_zarr"":\n            yield zappy.spark.from_zarr(sc, xz), zarr.TempStore()\n        elif request.param == ""beam_ndarray"":\n            pipeline_options = PipelineOptions()\n            pipeline = beam.Pipeline(options=pipeline_options)\n            yield zappy.beam.from_ndarray(pipeline, x.copy(), chunks), zarr.TempStore()\n        elif request.param == ""beam_zarr"":\n            pipeline_options = PipelineOptions()\n            pipeline = beam.Pipeline(options=pipeline_options)\n            yield zappy.beam.from_zarr(pipeline, xz), zarr.TempStore()\n        elif request.param == ""pywren_ndarray"":\n            import s3fs.mapping\n\n            def create_unique_bucket_name(prefix):\n                import uuid\n\n                return ""%s-%s"" % (prefix, str(uuid.uuid4()).replace(""-"", """"))\n\n            s3 = s3fs.S3FileSystem()\n            bucket = create_unique_bucket_name(""zappy-test"")\n            s3.mkdir(bucket)\n            path = ""%s/%s"" % (bucket, ""test.zarr"")\n            s3store = s3fs.mapping.S3Map(path, s3=s3)\n            executor = zappy.executor.PywrenExecutor()\n            yield zappy.executor.from_ndarray(executor, x.copy(), chunks), s3store\n            s3.rm(bucket, recursive=True)\n\n    @pytest.fixture(params=[""direct"", ""executor"", ""spark""])  # TODO: beam\n    def zeros(self, sc, request):\n        if request.param == ""direct"":\n            yield zappy.direct.zeros((3, 5), chunks=(2, 5), dtype=int)\n        elif request.param == ""executor"":\n            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n                yield zappy.executor.zeros(executor, (3, 5), chunks=(2, 5), dtype=int)\n        elif request.param == ""spark"":\n            yield zappy.spark.zeros(sc, (3, 5), chunks=(2, 5), dtype=int)\n\n    @pytest.fixture(params=[""direct"", ""executor"", ""spark""])  # TODO: beam\n    def ones(self, sc, request):\n        if request.param == ""direct"":\n            yield zappy.direct.ones((3, 5), chunks=(2, 5), dtype=int)\n        elif request.param == ""executor"":\n            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n                yield zappy.executor.ones(executor, (3, 5), chunks=(2, 5), dtype=int)\n        elif request.param == ""spark"":\n            yield zappy.spark.ones(sc, (3, 5), chunks=(2, 5), dtype=int)\n\n    def test_identity(self, x, xd):\n        assert_allclose(np.asarray(xd), x)\n\n    def test_astype(self, x, xd):\n        xd = xd.astype(int)\n        x = x.astype(int)\n        assert xd.dtype == x.dtype\n        assert_allclose(np.asarray(xd), x)\n\n    def test_astype_inplace(self, x, xd):\n        original_id = id(xd)\n        xd = xd.astype(int, copy=False)\n        assert original_id == id(xd)\n        x = x.astype(int, copy=False)\n        assert xd.dtype == x.dtype\n        assert_allclose(np.asarray(xd), x)\n\n    def test_asarray(self, x, xd):\n        assert_allclose(np.asarray(xd), x)\n\n    def test_scalar_arithmetic(self, x, xd):\n        xd = (((xd + 1) * 2) - 4) / 1.1\n        x = (((x + 1) * 2) - 4) / 1.1\n        assert_allclose(np.asarray(xd), x)\n\n    def test_arithmetic(self, x, xd):\n        xd = xd * 2 + xd\n        x = x * 2 + x\n        assert_allclose(np.asarray(xd), x)\n\n    def test_broadcast_row(self, x, xd):\n        a = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n        xd = xd + a\n        x = x + a\n        assert_allclose(np.asarray(xd), x)\n\n    def test_broadcast_col(self, x, xd):\n        if sys.version_info[0] == 2 and isinstance(\n            xd, zappy.beam.array.BeamZappyArray\n        ):  # TODO: fix this\n            return\n        a = np.array([[1.0], [2.0], [3.0]])\n        xd = xd + a\n        x = x + a\n        assert_allclose(np.asarray(xd), x)\n\n    def test_eq(self, x, xd):\n        xd = xd == 0.0\n        x = x == 0.0\n        assert xd.dtype == x.dtype\n        assert_allclose(np.asarray(xd), x)\n\n    def test_ne(self, x, xd):\n        xd = xd != 0.0\n        x = x != 0.0\n        assert_allclose(np.asarray(xd), x)\n\n    def test_invert(self, x, xd):\n        xd = ~(xd == 0.0)\n        x = ~(x == 0.0)\n        assert_allclose(np.asarray(xd), x)\n\n    def test_inplace(self, x, xd):\n        original_id = id(xd)\n        xd += 1\n        assert original_id == id(xd)\n        x += 1\n        assert_allclose(np.asarray(xd), x)\n\n    def test_simple_index(self, x, xd):\n        xd = xd[0]\n        x = x[0]\n        assert_allclose(xd, x)\n\n    def test_boolean_index(self, x, xd):\n        xd = np.sum(xd, axis=1)  # sum rows\n        xd = xd[xd > 5]\n        x = np.sum(x, axis=1)  # sum rows\n        x = x[x > 5]\n        assert_allclose(np.asarray(xd), x)\n\n    def test_slice_cols(self, x, xd):\n        xd = xd[:, 1:3]\n        x = x[:, 1:3]\n        assert xd.shape == x.shape\n        assert_allclose(np.asarray(xd), x)\n\n    def test_slice_rows(self, x, xd):\n        xd = xd[1:3, :]\n        x = x[1:3, :]\n        assert xd.shape == x.shape\n        assert_allclose(np.asarray(xd), x)\n\n    def test_slice_rows_shrink_partitions(self, x, xd):\n        if sys.version_info[0] == 2 and isinstance(\n            xd, zappy.beam.array.BeamZappyArray\n        ):  # TODO: fix this\n            return\n        xd = xd[0:2, :]\n        x = x[0:2, :]\n        assert xd.shape == x.shape\n        assert_allclose(np.asarray(xd), x)\n\n    def test_subset_cols_boolean(self, x, xd):\n        subset = np.array([True, False, True, False, True])\n        xd = xd[:, subset]\n        x = x[:, subset]\n        assert xd.shape == x.shape\n        assert_allclose(np.asarray(xd), x)\n\n    def test_subset_rows_boolean(self, x, xd):\n        subset = np.array([True, False, True])\n        xd = xd[subset, :]\n        x = x[subset, :]\n        assert xd.shape == x.shape\n        assert_allclose(np.asarray(xd), x)\n\n    def test_subset_cols_int(self, x, xd):\n        subset = np.array([1, 3])\n        xd = xd[:, subset]\n        x = x[:, subset]\n        assert xd.shape == x.shape\n        assert_allclose(np.asarray(xd), x)\n\n    def test_subset_rows_int(self, x, xd):\n        subset = np.array([1, 2])\n        xd = xd[subset, :]\n        x = x[subset, :]\n        assert xd.shape == x.shape\n        assert_allclose(np.asarray(xd), x)\n\n    def test_newaxis(self, x, xd):\n        xd = np.sum(xd, axis=1)[:, np.newaxis]\n        x = np.sum(x, axis=1)[:, np.newaxis]\n        assert_allclose(np.asarray(xd), x)\n\n    def test_log1p(self, x, xd):\n        log1pnps = np.asarray(np.log1p(xd))\n        log1pnp = np.log1p(x)\n        assert_allclose(log1pnps, log1pnp)\n\n    def test_sum(self, x, xd):\n        if sys.version_info[0] == 2 and isinstance(\n            xd, zappy.beam.array.BeamZappyArray\n        ):  # TODO: fix this\n            return\n        totald = np.sum(xd)\n        total = np.sum(x)\n        assert totald == pytest.approx(total)\n\n    def test_sum_cols(self, x, xd):\n        xd = np.sum(xd, axis=0)\n        x = np.sum(x, axis=0)\n        assert_allclose(np.asarray(xd), x)\n\n    def test_sum_rows(self, x, xd):\n        xd = np.sum(xd, axis=1)\n        x = np.sum(x, axis=1)\n        assert_allclose(np.asarray(xd), x)\n\n    def test_mean(self, x, xd):\n        if sys.version_info[0] == 2 and isinstance(\n            xd, zappy.beam.array.BeamZappyArray\n        ):  # TODO: fix this\n            return\n        meand = np.mean(xd)\n        mean = np.mean(x)\n        assert meand == pytest.approx(mean)\n\n    def test_mean_cols(self, x, xd):\n        xd = np.mean(xd, axis=0)\n        x = np.mean(x, axis=0)\n        assert_allclose(np.asarray(xd), x)\n\n    def test_mean_rows(self, x, xd):\n        xd = np.mean(xd, axis=1)\n        x = np.mean(x, axis=1)\n        assert_allclose(np.asarray(xd), x)\n\n    def test_var(self, x, xd):\n        def var(x):\n            mean = x.mean(axis=0)\n            mean_sq = np.multiply(x, x).mean(axis=0)\n            return mean_sq - mean ** 2\n\n        varnps = np.asarray(var(xd))\n        varnp = var(x)\n        assert_allclose(varnps, varnp)\n\n    def test_median(self, x, xd):\n        mediand = np.median(xd)  # implicitly converts to np.array\n        median = np.median(x)\n        assert mediand == pytest.approx(median)\n\n    def test_write_zarr(self, x, xd_and_temp_store):\n        xd, temp_store = xd_and_temp_store\n        xd.to_zarr(temp_store, xd.chunks)\n        # read back as zarr directly and check it is the same as x\n        z = zarr.open(temp_store, mode=""r"", shape=x.shape, dtype=x.dtype, chunks=(2, 5))\n        arr = z[:]\n        assert_allclose(arr, x)\n\n    def test_write_zarr_ncopies(self, x, xd_and_temp_store):\n        xd, temp_store = xd_and_temp_store\n        if sys.version_info[0] == 2 and isinstance(\n            xd, zappy.beam.array.BeamZappyArray\n        ):  # TODO: fix this\n            return\n        xd = xd._repartition_chunks((3, 5))\n        ncopies = 3\n        xd.to_zarr(temp_store, xd.chunks, ncopies=ncopies)\n        # read back as zarr directly and check it is the same as x\n        z = zarr.open(\n            temp_store,\n            mode=""r"",\n            shape=(x.shape[0] * ncopies, x.shape[1]),\n            dtype=x.dtype,\n            chunks=(1, 5),\n        )\n        arr = z[:]\n        x_ncopies = np.vstack((x,) * ncopies)\n        assert_allclose(arr, x_ncopies)\n\n    def test_zeros(self, zeros):\n        totals = np.sum(zeros, axis=0)\n        x = np.array([0, 0, 0, 0, 0])\n        assert_allclose(np.asarray(totals), x)\n\n    def test_ones(self, ones):\n        totals = np.sum(ones, axis=0)\n        x = np.array([3, 3, 3, 3, 3])\n        assert_allclose(np.asarray(totals), x)\n\n    def test_asndarrays(self, x, xd):\n        if not isinstance(xd, zappy.executor.array.ExecutorZappyArray):\n            return\n        xd1, xd2 = zappy.executor.asndarrays((xd + 1, xd + 2))\n        assert_allclose(xd1, x + 1)\n        assert_allclose(xd2, x + 2)\n'"
tests/test_base.py,14,"b'import numpy as np\nfrom numpy.testing import assert_allclose, assert_array_equal\nimport pytest\n\nfrom zappy.base import ZappyArray\n\ntestdata = [\n    (\n        np.array([0, 1, 2]),\n        np.array([2, 1]),\n        np.array([np.array([0, 1]), np.array([0])]),\n    ),\n    (np.array([0, 1]), np.array([2, 1]), np.array([np.array([0, 1]), np.array([])])),\n    (np.array([1, 2]), np.array([2, 1]), np.array([np.array([1]), np.array([0])])),\n    (\n        np.array([1, 5]),\n        np.array([2, 3, 2, 1]),\n        np.array([np.array([1]), np.array([]), np.array([0]), np.array([])]),\n    ),\n    (\n        slice(None),\n        np.array([2, 3, 2, 1]),\n        np.array([slice(0, None), slice(None), slice(None), slice(1)]),\n    ),\n    (\n        slice(2, 5),\n        np.array([2, 3, 2, 1]),\n        np.array([slice(0, 0), slice(0, 3), slice(0, 0), slice(0, 0)]),\n    ),\n    (\n        slice(1, 6),\n        np.array([2, 3, 2, 1]),\n        np.array([slice(1, None), slice(None), slice(1), slice(0, 0)]),\n    ),\n]\n\n\n@pytest.mark.parametrize(""subset,partition_row_counts,expected"", testdata)\ndef test_copartition(subset, partition_row_counts, expected):\n    actual = ZappyArray._copartition(subset, partition_row_counts)\n    assert len(actual) == len(expected)\n    for i, arr in enumerate(actual):\n        assert_array_equal(arr, expected[i])\n'"
tests/test_dag.py,0,"b'import concurrent.futures\nimport pytest\nfrom zappy.executor.dag import DAG\n\n\ndef add_one(x):\n    return x + 1\n\n\ndef times_two(x):\n    return x * 2\n\n\ndef add(x, y):\n    return x + y\n\n\ndef test_dag_single_partition():\n    dag = DAG(concurrent.futures.ThreadPoolExecutor())\n    input = dag.add_input([2])\n    output = dag.transform(add_one, [input])\n    assert list(dag.compute(output)) == [3]\n\n\ndef test_dag_single_partition_serial_functions():\n    dag = DAG(concurrent.futures.ThreadPoolExecutor())\n    input = dag.add_input([2])\n    intermediate = dag.transform(add_one, [input])\n    output = dag.transform(times_two, [intermediate])\n    assert list(dag.compute(output)) == [6]\n\n\ndef test_dag_single_partition_binary_function():\n    dag = DAG(concurrent.futures.ThreadPoolExecutor())\n    input1 = dag.add_input([2])\n    input2 = dag.add_input([3])\n    output = dag.transform(add, [input1, input2])\n    assert list(dag.compute(output)) == [5]\n\n\ndef test_dag_multiple_partitions():\n    dag = DAG(concurrent.futures.ThreadPoolExecutor())\n    input = dag.add_input([2, 3, 5])\n    output = dag.transform(add_one, [input])\n    assert list(dag.compute(output)) == [3, 4, 6]\n\n\ndef test_dag_multiple_partitions_binary_function():\n    dag = DAG(concurrent.futures.ThreadPoolExecutor())\n    input1 = dag.add_input([2, 3, 5])\n    input2 = dag.add_input([7, 11, 13])\n    output = dag.transform(add, [input1, input2])\n    assert list(dag.compute(output)) == [9, 14, 18]\n\n\ndef test_incompatible_num_partitions():\n    dag = DAG(concurrent.futures.ThreadPoolExecutor())\n    dag.add_input([2])\n    with pytest.raises(AssertionError):\n        dag.add_input([1, 5])\n\n\ndef test_no_transform():\n    dag = DAG(concurrent.futures.ThreadPoolExecutor())\n    output = dag.add_input([2])\n    assert list(dag.compute(output)) == [2]\n\n\ndef test_dag_multiple_transforms():\n    dag = DAG(concurrent.futures.ThreadPoolExecutor())\n    input = dag.add_input([2, 3, 5])\n    inter = dag.transform(add_one, [input])\n    output = dag.transform(add_one, [inter])\n    assert list(dag.compute(output)) == [4, 5, 7]\n'"
tests/test_repartition_chunks.py,16,"b'import concurrent.futures\nimport logging\nimport numpy as np\nimport pytest\nimport zappy.executor\nimport zappy.direct\nimport zappy.spark\n\nfrom pyspark.sql import SparkSession\n\nTESTS = [0, 1, 2]\n\n\nclass TestZappyArray:\n    @pytest.fixture()\n    def x(self):\n        return np.array([[a] for a in range(12)])\n\n    @pytest.fixture(scope=""module"")\n    def sc(self):\n        logger = logging.getLogger(""py4j"")\n        logger.setLevel(logging.WARN)\n        spark = (\n            SparkSession.builder.master(""local[2]"")\n            .appName(""my-local-testing-pyspark-context"")\n            .getOrCreate()\n        )\n        yield spark.sparkContext\n        spark.stop()\n\n    @pytest.fixture(params=TESTS)\n    def xd(self, sc, x, request):\n        if request.param == 0:\n            yield zappy.direct.from_ndarray(x.copy(), (5, 1))\n        elif request.param == 1:\n            yield zappy.spark.from_ndarray(sc, x.copy(), (5, 1))\n        elif request.param == 2:\n            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n                yield zappy.executor.from_ndarray(executor, x.copy(), (5, 1))\n\n    @pytest.fixture(params=TESTS)\n    def xd34(self, sc, x, request):\n        if request.param == 0:\n            yield zappy.direct.from_ndarray(x.copy(), (3, 4))\n        elif request.param == 1:\n            yield zappy.spark.from_ndarray(sc, x.copy(), (3, 4))\n        elif request.param == 2:\n            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n                yield zappy.executor.from_ndarray(executor, x.copy(), (3, 4))\n\n    @pytest.fixture(params=TESTS)\n    def xd43(self, sc, x, request):\n        if request.param == 0:\n            yield zappy.direct.from_ndarray(x.copy(), (4, 3))\n        elif request.param == 1:\n            yield zappy.spark.from_ndarray(sc, x.copy(), (4, 3))\n        elif request.param == 2:\n            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n                yield zappy.executor.from_ndarray(executor, x.copy(), (4, 3))\n\n    def check(self, expected_rows, actual_rows):\n        assert len(actual_rows) == len(expected_rows)\n        for i in range(len(expected_rows)):\n            from numpy.testing import assert_array_equal\n\n            assert_array_equal(expected_rows[i], actual_rows[i])\n\n    def test_no_op(self, x, xd):\n        xd = xd._repartition_chunks((5, 1))\n        assert xd.partition_row_counts == [5, 5, 2]\n        expected_rows = [\n            np.array([[0], [1], [2], [3], [4]]),\n            np.array([[5], [6], [7], [8], [9]]),\n            np.array([[10], [11]]),\n        ]\n        actual_rows = xd._compute()\n        self.check(expected_rows, actual_rows)\n\n    def test_uneven(self, x, xd):\n        subset = np.array([True] * 12)\n        subset[7] = False  # drop a row\n        xd = xd[subset, :]\n        xd = xd._repartition_chunks((5, 1))\n        assert xd.partition_row_counts == [5, 5, 1]\n        expected_rows = [\n            np.array([[0], [1], [2], [3], [4]]),\n            np.array([[5], [6], [8], [9], [10]]),\n            np.array([[11]]),\n        ]\n        actual_rows = xd._compute()\n        self.check(expected_rows, actual_rows)\n\n    def test_subdivide(self, x, xd34):\n        xd = xd34._repartition_chunks((2, 1))\n        assert xd.partition_row_counts == [2, 2, 2, 2, 2, 2]\n        expected_rows = [\n            np.array([[0], [1]]),\n            np.array([[2], [3]]),\n            np.array([[4], [5]]),\n            np.array([[6], [7]]),\n            np.array([[8], [9]]),\n            np.array([[10], [11]]),\n        ]\n        actual_rows = xd._compute()\n        self.check(expected_rows, actual_rows)\n\n    def test_coalesce(self, x, xd43):\n        xd = xd43._repartition_chunks((6, 1))\n        assert xd.partition_row_counts == [6, 6]\n        expected_rows = [\n            np.array([[0], [1], [2], [3], [4], [5]]),\n            np.array([[6], [7], [8], [9], [10], [11]]),\n        ]\n        actual_rows = xd._compute()\n        self.check(expected_rows, actual_rows)\n'"
zappy/__init__.py,0,b''
zappy/base.py,38,"b'import builtins\nimport copy as cp\nimport numbers\nimport numpy as np\nimport zarr\n\nfrom functools import partial\n\nfrom zappy.zarr_util import (\n    get_chunk_indices,\n    read_zarr_chunk,\n    write_chunk,\n    write_chunk_gcs,\n    write_n_chunk_copies,\n    write_n_chunk_copies_gcs,\n)\n\n\nclass ZappyArray(np.lib.mixins.NDArrayOperatorsMixin):\n    def __init__(self, shape, chunks, dtype, partition_row_counts=None):\n        self.shape = shape\n        self.chunks = chunks\n        self.dtype = dtype if isinstance(dtype, np.dtype) else np.dtype(dtype)\n        if partition_row_counts is None:\n            partition_row_counts = [chunks[0]] * (shape[0] // chunks[0])\n            remaining = shape[0] % chunks[0]\n            if remaining != 0:\n                partition_row_counts.append(remaining)\n        self.partition_row_counts = partition_row_counts\n\n    def _new(self, **kwargs):\n        """"""Copy or update this object with the given keyword parameters.""""""\n        out = kwargs.get(""out"")\n        if isinstance(out, tuple) and len(out) > 0:\n            out = out[0]  # TODO: handle multiple values\n        obj = out if out else cp.copy(self)\n        for key, value in kwargs.items():\n            if key != ""out"" and not (key == ""dtype"" and value is None):\n                setattr(obj, key, value)\n        return obj\n\n    @property\n    def ndim(self):\n        return len(self.shape)\n\n    # Load and store methods\n\n    @classmethod\n    def from_ndarray(cls, sc, arr, chunks):\n        return NotImplemented\n\n    @classmethod\n    def from_zarr(cls, sc, zarr_file):\n        return NotImplemented\n\n    @staticmethod\n    def _read_chunks(arr, chunks):\n        shape = arr.shape\n        func = partial(read_zarr_chunk, arr, chunks)\n        chunk_indices = get_chunk_indices(shape, chunks)\n        return func, chunk_indices\n\n    @staticmethod\n    def _array_chunks_to_ndarray(array_chunks, partition_row_counts):\n        local_row_counts = [len(arr) for arr in array_chunks]\n        assert local_row_counts == list(partition_row_counts), (\n            ""Local row counts: %s; partition row counts: %s""\n            % (local_row_counts, partition_row_counts)\n        )\n        arr = np.concatenate(array_chunks)\n        assert arr.shape[0] == builtins.sum(partition_row_counts), (\n            ""Local #rows: %s; partition row counts total: %s""\n            % (arr.shape[0], builtins.sum(partition_row_counts))\n        )\n        return arr\n\n    def asndarray(self):\n        return ZappyArray._array_chunks_to_ndarray(\n            self._compute(), self.partition_row_counts\n        )\n\n    def __array__(self, dtype=None, **kwargs):\n        # respond to np.asarray\n        x = self.asndarray()\n        if dtype and x.dtype != dtype:\n            x = x.astype(dtype)\n        return x\n\n    def _compute(self):\n        """"""\n        :return: a list of array chunks\n        """"""\n        return NotImplemented\n\n    def _repartition_chunks(self, chunks):\n        # subclasses should implement this to repartition to equal-sized chunks (except the last partition, which may be smaller)\n        return NotImplemented\n\n    def _repartition_if_necessary(self, chunks):\n        # if all except last partition have c rows...\n        # ... then no need to shuffle, since already partitioned correctly\n        if all([count == chunks[0] for count in self.partition_row_counts[:-1]]):\n            return self\n        else:\n            return self._repartition_chunks(chunks)\n\n    def to_zarr(self, zarr_file, chunks, ncopies=1):\n        """"""\n        Write an ZappyArray object to a Zarr file.\n        """"""\n        if ncopies != 1:\n            assert self.shape[0] % chunks[0] == 0\n            shape = (self.shape[0] * ncopies, self.shape[1])\n            zarr.open(zarr_file, mode=""w"", shape=shape, chunks=chunks, dtype=self.dtype)\n            self._write_zarr(\n                zarr_file,\n                chunks,\n                write_n_chunk_copies(zarr_file, self.shape[0], ncopies),\n            )\n            return\n        zarr.open(\n            zarr_file, mode=""w"", shape=self.shape, chunks=chunks, dtype=self.dtype\n        )\n        repartitioned = self._repartition_if_necessary(chunks)\n        repartitioned._write_zarr(zarr_file, chunks, write_chunk(zarr_file))\n\n    def to_zarr_gcs(self, gcs_path, chunks, gcs_project, gcs_token=""cloud"", ncopies=1):\n        """"""\n        Write an ZappyArray object to a Zarr file on GCS.\n        """"""\n        if ncopies != 1:\n            assert self.shape[0] % chunks[0] == 0\n            shape = (self.shape[0] * ncopies, self.shape[1])\n            import gcsfs.mapping\n\n            gcs = gcsfs.GCSFileSystem(gcs_project, token=gcs_token)\n            store = gcsfs.mapping.GCSMap(gcs_path, gcs=gcs)\n            zarr.open(store, mode=""w"", shape=shape, chunks=chunks, dtype=self.dtype)\n            self._write_zarr(\n                store,\n                chunks,\n                write_n_chunk_copies_gcs(\n                    gcs_path, gcs_project, gcs_token, self.shape[0], ncopies\n                ),\n            )\n            return\n\n        repartitioned = self._repartition_if_necessary(chunks)\n        import gcsfs.mapping\n\n        gcs = gcsfs.GCSFileSystem(gcs_project, token=gcs_token)\n        store = gcsfs.mapping.GCSMap(gcs_path, gcs=gcs)\n        zarr.open(store, mode=""w"", shape=self.shape, chunks=chunks, dtype=self.dtype)\n        repartitioned._write_zarr(\n            store, chunks, write_chunk_gcs(gcs_path, gcs_project, gcs_token)\n        )\n\n    def _write_zarr(self, store, chunks, write_chunk_fn, ncopies=1):\n        return NotImplemented\n\n    # Array conversion (https://docs.scipy.org/doc/numpy-1.14.0/reference/arrays.ndarray.html#array-methods)\n\n    def astype(self, dtype, copy=True):\n        out = None if copy else self\n        dtype = dtype if isinstance(dtype, np.dtype) else np.dtype(dtype)\n        return self._unary_ufunc(lambda x: x.astype(dtype), out=out, dtype=dtype)\n\n    def copy(self):\n        return self._new()\n\n    # Calculation methods (https://docs.scipy.org/doc/numpy-1.14.0/reference/arrays.ndarray.html#calculation)\n\n    def mean(self, axis, out=None, dtype=None, **kwargs):\n        if axis == 1:\n            return self._calc_func_axis_rowwise(np.mean, axis)\n        return self._calc_mean(axis)\n\n    def argmax(self, axis, out=None, dtype=None, **kwargs):\n        if axis == 1:\n            return self._calc_func_axis_rowwise(np.argmax, axis)\n        return self._calc_func_axis_distributive(np.argmax, axis)\n\n    def min(self, axis, out=None, dtype=None, **kwargs):\n        if axis == 1:\n            return self._calc_func_axis_rowwise(np.amin, axis)\n        return self._calc_func_axis_distributive(np.amin, axis)\n\n    def argmin(self, axis, out=None, dtype=None, **kwargs):\n        if axis == 1:\n            return self._calc_func_axis_rowwise(np.argmin, axis)\n        return self._calc_func_axis_distributive(np.argmin, axis)\n\n    def sum(self, axis, out=None, dtype=None, **kwargs):\n        if axis == 1:\n            return self._calc_func_axis_rowwise(np.sum, axis)\n        return self._calc_func_axis_distributive(np.sum, axis)\n\n    def prod(self, axis, out=None, dtype=None, **kwargs):\n        if axis == 1:\n            return self._calc_func_axis_rowwise(np.prod, axis)\n        return self._calc_func_axis_distributive(np.prod, axis)\n\n    def all(self, axis, out=None, dtype=None, **kwargs):\n        if axis == 1:\n            return self._calc_func_axis_rowwise(np.all, axis)\n        return self._calc_func_axis_distributive(np.all, axis)\n\n    def any(self, axis, out=None, dtype=None, **kwargs):\n        if axis == 1:\n            return self._calc_func_axis_rowwise(np.any, axis)\n        return self._calc_func_axis_distributive(np.any, axis)\n\n    # TODO: more calculation methods here\n    # Not distributive: ptp, cumsum, var, std, cumprod\n    # Don\'t take an axis: clip, conj, round\n    # Other: trace (two axes!)\n\n    def _calc_mean(self, axis=None):\n        return NotImplemented\n\n    def _calc_func_axis_rowwise(self, func, axis):\n        # Calculation method that takes an axis argument and axis == 1\n        return NotImplemented\n\n    def _calc_func_axis_distributive(self, func, axis):\n        # Calculation method that takes an axis argument, and is distributive.\n        # Distributive in this context means that the result can be computed in pieces\n        # and combined using the function. So f(a, b, c, d) = f(f(a, b), f(c, d))\n        return NotImplemented\n\n    # Distributed ufunc internal implementation\n\n    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):\n        if method == ""__call__"":\n            # TODO: handle dtype generically\n            if ufunc.__name__ in (\n                ""greater"",\n                ""greater_equal"",\n                ""less"",\n                ""less_equal"",\n                ""not_equal"",\n                ""equal"",\n            ):\n                kwargs[""dtype""] = bool\n            if len(inputs) > 0 and isinstance(inputs[0], ZappyArray):\n                return inputs[0]._dist_ufunc(ufunc, inputs[1:], **kwargs)\n        return NotImplemented\n\n    def _dist_ufunc(self, func, args, out=None, dtype=None):\n        # unary ufunc\n        if len(args) == 0:\n            return self._unary_ufunc(func, out=out, dtype=dtype)\n        # binary ufunc\n        elif len(args) == 1:\n            other = args[0]\n            if self is other:\n                return self._binary_ufunc_self(func, out=out, dtype=dtype)\n            elif isinstance(other, numbers.Number) or other.ndim == 1:\n                return self._binary_ufunc_broadcast_single_row_or_value(\n                    func, other, out=out, dtype=dtype\n                )\n            elif self.shape[0] == other.shape[0] and other.shape[1] == 1:\n                return self._binary_ufunc_broadcast_single_column(\n                    func, other, out=out, dtype=dtype\n                )\n            elif self.shape == other.shape:\n                return self._binary_ufunc_same_shape(func, other, out=out, dtype=dtype)\n        else:\n            print(""_dist_ufunc %s not implemented for %s"" % (func, args))\n            return NotImplemented\n\n    def _unary_ufunc(self, func, out=None, dtype=None):\n        return NotImplemented\n\n    def _binary_ufunc_self(self, func, out=None, dtype=None):\n        return NotImplemented\n\n    def _binary_ufunc_broadcast_single_row_or_value(\n        self, func, other, out=None, dtype=None\n    ):\n        return NotImplemented\n\n    def _binary_ufunc_broadcast_single_column(self, func, other, out=None, dtype=None):\n        return NotImplemented\n\n    def _binary_ufunc_same_shape(self, func, other, out=None, dtype=None):\n        return NotImplemented\n\n    # Slicing implementation\n\n    def __getitem__(self, item):\n        all_indices = slice(None, None, None)\n        if isinstance(item, numbers.Number):\n            return self._integer_index(item)\n        elif isinstance(item, (np.ndarray, ZappyArray)) and item.dtype == bool:\n            return self._boolean_array_index_dist(item)\n        elif isinstance(item[0], slice) and item[0] == all_indices:\n            return self._column_subset(item)\n        elif isinstance(item[1], slice) and item[1] == all_indices:\n            return self._row_subset(item)\n        return NotImplemented\n\n    def _integer_index(self, item):\n        # TODO: not scalable for large arrays\n        return np.asarray(self).__getitem__(item)\n\n    def _boolean_array_index_dist(self, item):\n        return NotImplemented\n\n    def _column_subset(self, item):\n        return NotImplemented\n\n    def _row_subset(self, item):\n        return NotImplemented\n\n    # Utility methods\n    # TODO: document\n\n    @staticmethod\n    def _copartition(arr, partition_row_counts, shrink=False):\n        """"""Partition an array or slice according to the given row counts.\n        The array can be of indexes, or values.\n        """"""\n        if isinstance(arr, slice):  # slice\n            cum = np.cumsum(partition_row_counts)[0:-1]\n            offsets = np.insert(cum, 0, 0)  # add a leading 0\n            start = arr.start if arr.start is not None else 0\n            stop = arr.stop if arr.stop is not None else np.sum(partition_row_counts)\n            # find the partition index where the slice starts and stops\n            starti = np.searchsorted(cum, start, side=""right"")\n            stopi = np.searchsorted(cum, stop)\n            partition_row_subsets = [slice(0, 0)] * starti\n            for i in range(starti, stopi + 1):\n                subset_start = start - offsets[i] if i == starti else None\n                subset_stop = stop - offsets[i] if i == stopi else None\n                partition_row_subsets.append(slice(subset_start, subset_stop))\n            if not shrink:\n                partition_row_subsets.extend(\n                    [slice(0, 0)] * (len(partition_row_counts) - (stopi + 1))\n                )\n            return partition_row_subsets\n        elif arr.dtype == np.dtype(int):  # indexes\n            cum = np.cumsum(partition_row_counts)[0:-1]\n            offsets = np.insert(cum, 0, 0)  # add a leading 0\n            index_breaks = np.searchsorted(arr, cum)\n            splits = np.split(arr, index_breaks)\n            partition_row_subsets = []\n            for index, split in enumerate(splits):\n                if len(split) == 0:\n                    partition_row_subsets.append(split)\n                else:\n                    partition_row_subsets.append(split - offsets[index])\n            return partition_row_subsets\n        else:  # values\n            return ZappyArray._copartition_values(arr, partition_row_counts)\n\n    @staticmethod\n    def _copartition_values(arr, partition_row_counts):\n        return np.split(arr, np.cumsum(partition_row_counts)[0:-1])\n\n    @staticmethod\n    def _partition_row_counts(partition_row_subsets, partition_row_counts=None):\n        if isinstance(partition_row_subsets[0], slice):\n            counts = []\n            for i, subset in enumerate(partition_row_subsets):\n                start = subset.start if subset.start is not None else 0\n                stop = (\n                    subset.stop if subset.stop is not None else partition_row_counts[i]\n                )\n                counts.append(stop - start)\n            return counts\n        dtype = partition_row_subsets[0].dtype\n        if dtype == np.dtype(bool):\n            return [int(builtins.sum(s)) for s in partition_row_subsets]\n        elif dtype == np.dtype(int):\n            return [len(s) for s in partition_row_subsets]\n        return NotImplemented\n\n    @staticmethod\n    def _materialize_index(index):\n        """"""Materialize index as an ndarray, or leave as a slice.""""""\n        if isinstance(index, slice):\n            return index\n        return np.asarray(index)\n\n    @staticmethod\n    def _compute_dim(dim, subset):\n        """"""Compute the dimension of an index or slice.""""""\n        all_indices = slice(None, None, None)\n        if isinstance(subset, slice):\n            if subset == all_indices:\n                return dim\n            else:\n                return len(np.zeros((dim))[subset])\n        elif subset.dtype == np.dtype(int):\n            return len(subset)\n        return builtins.sum(subset)\n'"
zappy/zarr_util.py,0,"b'import itertools\nimport math\nimport zarr\n\ntry:\n    from itertools import accumulate\nexcept ImportError:\n    # see https://docs.python.org/dev/library/itertools.html#itertools.accumulate\n    import operator\n\n    def accumulate(iterable, func=operator.add):\n        ""Return running totals""\n        # accumulate([1,2,3,4,5]) --> 1 3 6 10 15\n        # accumulate([1,2,3,4,5], operator.mul) --> 1 2 6 24 120\n        it = iter(iterable)\n        try:\n            total = next(it)\n        except StopIteration:\n            return\n        yield total\n        for element in it:\n            total = func(total, element)\n            yield total\n\n\n# Utility functions for reading and writing a Zarr array chunk; these are designed to be run as Spark tasks.\n# Assume that the row lengths are small enough that the entire row fits into a Zarr chunk; in\n# other words, the chunk width is the same as the row width. Also each task reads/writes a single chunk.\n#\n# Possible matrix operations:\n# * Add or remove columns. Adjust chunk width. Easy to handle since row partitioning does not change.\n# * Add or remove rows. Changes row partitioning. Simplest way to handle is to shuffle with the chunk as the key.\n#   See repartition_chunks. May\n#   be able to be more sophisticated with a clever Spark coalescer that can read from other partitions.\n# * Matrix multiplication. Multiplying by a matrix on the right preserves partitioning, so only chunk width needs to\n#   change.\n\n\ndef get_chunk_indices(shape, chunks):\n    """"""\n    Return all the indices (coordinates) for the chunks in a zarr array, even empty ones.\n    """"""\n    return [\n        (i, j)\n        for i in range(int(math.ceil(float(shape[0]) / chunks[0])))\n        for j in range(int(math.ceil(float(shape[1]) / chunks[1])))\n    ]\n\n\ndef get_chunk_sizes(shape, chunks):\n    def sizes(length, chunk_length):\n        res = [chunk_length] * (length // chunk_length)\n        if length % chunk_length != 0:\n            res.append(length % chunk_length)\n        return res\n\n    return itertools.product(sizes(shape[0], chunks[0]), sizes(shape[1], chunks[1]))\n\n\ndef read_zarr_chunk(arr, chunks, chunk_index):\n    return arr[\n        chunks[0] * chunk_index[0] : chunks[0] * (chunk_index[0] + 1),\n        chunks[1] * chunk_index[1] : chunks[1] * (chunk_index[1] + 1),\n    ]\n\n\ndef read_chunk(file):\n    """"""\n    Return a function to read a chunk by coordinates from the given file.\n    """"""\n\n    def read_one_chunk(chunk_index):\n        """"""\n        Read a zarr chunk specified by coordinates chunk_index=(a,b).\n        """"""\n        z = zarr.open(file, mode=""r"")\n        return read_zarr_chunk(z, z.chunks, chunk_index)\n\n    return read_one_chunk\n\n\ndef write_chunk(file):\n    """"""\n    Return a function to write a chunk by index to the given file.\n    """"""\n\n    def write_one_chunk(index_arr):\n        """"""\n        Write a partition index and numpy array to a zarr store. The array must be the size of a chunk, and not\n        overlap other chunks.\n        """"""\n        index, arr = index_arr\n        z = zarr.open(file, mode=""r+"")\n        chunk_size = z.chunks\n        z[chunk_size[0] * index : chunk_size[0] * (index + 1), :] = arr\n\n    return write_one_chunk\n\n\ndef write_n_chunk_copies(file, size, ncopies):\n    """"""\n    Return a function to write a chunk by index to the given file to produce n copies of the array.\n    """"""\n\n    def write_n_chunks(index_arr):\n        index, arr = index_arr\n        z = zarr.open(file, mode=""r+"")\n        chunk_size = z.chunks\n        for i in range(ncopies):\n            effective_index = index + i * (size // chunk_size[0])\n            z[\n                chunk_size[0] * effective_index : chunk_size[0] * (effective_index + 1),\n                :,\n            ] = arr\n\n    return write_n_chunks\n\n\ndef write_chunk_gcs(gcs_path, gcs_project, gcs_token):\n    """"""\n    Return a function to write a chunk by index to the given file.\n    """"""\n\n    def write_one_chunk(index_arr):\n        """"""\n        Write a partition index and numpy array to a zarr store. The array must be the size of a chunk, and not\n        overlap other chunks.\n        """"""\n        import gcsfs.mapping\n\n        gcs = gcsfs.GCSFileSystem(gcs_project, token=gcs_token)\n        store = gcsfs.mapping.GCSMap(gcs_path, gcs=gcs)\n        index, arr = index_arr\n        z = zarr.open(store, mode=""r+"")\n        chunk_size = z.chunks\n        z[chunk_size[0] * index : chunk_size[0] * (index + 1), :] = arr\n\n    return write_one_chunk\n\n\ndef write_n_chunk_copies_gcs(gcs_path, gcs_project, gcs_token, size, ncopies):\n    """"""\n    Return a function to write a chunk by index to the given file to produce n copies of the array.\n    """"""\n\n    def write_n_chunks(index_arr):\n        import gcsfs.mapping\n\n        gcs = gcsfs.GCSFileSystem(gcs_project, token=gcs_token)\n        store = gcsfs.mapping.GCSMap(gcs_path, gcs=gcs)\n        index, arr = index_arr\n        z = zarr.open(store, mode=""r+"")\n        chunk_size = z.chunks\n        for i in range(ncopies):\n            effective_index = index + i * (size // chunk_size[0])\n            z[\n                chunk_size[0] * effective_index : chunk_size[0] * (effective_index + 1),\n                :,\n            ] = arr\n\n    return write_n_chunks\n\n\ndef calculate_partition_boundaries(chunks, partition_row_counts):\n    # Generate a list of offsets, so that k[i] is the number of rows before the i-th partition\n    # Then turn this into a row range for each partition\n    k = list(accumulate([0] + partition_row_counts))\n    partition_row_ranges = list(zip(k, k[1:]))\n    total_rows = k[-1]\n    new_num_partitions = ((total_rows - 1) // chunks[0]) + 1\n    return partition_row_ranges, total_rows, new_num_partitions\n\n\ndef extract_partial_chunks(iterator, chunks):\n    """"""\n    For a given partition, we now know the start and end row numbers, so use that along with the new chunk size\n    to break the rows into new (partial) chunks that are labelled with the new index number. Partial chunks will\n    be shuffled using the new index number as key to bring together all the partial chunks for a given new index\n    number.\n    """"""\n    # iterator is a single entry of ((row_start, row_end), array), where row_end is exclusive\n\n    c = chunks[0]  # the chunk size for rows\n    key, val = list(iterator)\n    k_i, k_i_next = key\n    tuples = []\n    for x in range(k_i - k_i % c, k_i_next, c):  # iterate over overlapping chunks\n        start, end = max(k_i, x), min(k_i_next, x + c)\n        start_offset, end_offset = start - k_i, end - k_i\n        partial_chunk = val[start_offset:end_offset]\n        new_index = start // c\n        new_start_offset, new_end_offset = (start - new_index * c, end - new_index * c)\n        tuples.append((new_index, ((new_start_offset, new_end_offset), partial_chunk)))\n    return tuples\n'"
zappy/beam/__init__.py,0,"b'from zappy.beam.array import from_ndarray, from_zarr\n'"
zappy/beam/array.py,8,"b'import apache_beam as beam\nimport builtins\nimport numpy as np\nimport zarr\n\nfrom apache_beam.pvalue import AsDict\nfrom zappy.base import *  # include everything in zappy.base and hence base numpy\n\n\ndef from_ndarray(pipeline, arr, chunks):\n    return BeamZappyArray.from_ndarray(pipeline, arr, chunks)\n\n\ndef from_zarr(pipeline, zarr_file):\n    return BeamZappyArray.from_zarr(pipeline, zarr_file)\n\n\nsym_counter = 0\n\n\ndef gensym(name):\n    global sym_counter\n    sym_counter += 1\n    return ""%s_%s"" % (name, sym_counter)\n\n\n# ndarray in Beam\n\n\nclass BeamZappyArray(ZappyArray):\n    """"""A numpy.ndarray backed by a Beam PCollection""""""\n\n    def __init__(\n        self, pipeline, pcollection, shape, chunks, dtype, partition_row_counts=None\n    ):\n        ZappyArray.__init__(self, shape, chunks, dtype, partition_row_counts)\n        self.pipeline = pipeline\n        self.pcollection = pcollection\n\n    # methods to convert to/from regular ndarray - mainly for testing\n    @classmethod\n    def from_ndarray(cls, pipeline, arr, chunks):\n        func, chunk_indices = ZappyArray._read_chunks(arr, chunks)\n        # use the first component of chunk index as an index (assumes rows are one chunk wide)\n        pcollection = (\n            pipeline\n            | beam.Create(chunk_indices)\n            | beam.Map(lambda chunk_index: (chunk_index[0], func(chunk_index)))\n        )\n        return cls(pipeline, pcollection, arr.shape, chunks, arr.dtype)\n\n    @classmethod\n    def from_zarr(cls, pipeline, zarr_file):\n        """"""\n        Read a Zarr file as a BeamZappyArray object.\n        """"""\n        arr = zarr.open(zarr_file, mode=""r"")\n        return cls.from_ndarray(pipeline, arr, arr.chunks)\n\n    def _compute(self):\n        # create a zarr groups to materialize arrays to\n        sym = gensym(""compute"")\n        store = zarr.TempStore()\n        root = zarr.open(store, mode=""w"")  # TODO: allow cloud storage\n\n        # save arrays\n        def save(indexed_row):\n            index, row = indexed_row\n            # remove array in case we are being materialized again\n            zarr.storage.rmdir(store, ""/{}"".format(index))\n            root = zarr.group(store)\n            root.array(str(index), row, chunks=False)\n\n        self.pcollection | sym >> beam.Map(save)\n        result = self.pipeline.run()\n        result.wait_until_finish()\n\n        # read back arrays\n        local_rows = [None] * len(self.partition_row_counts)\n        for (name, row) in root.arrays():\n            index = int(name)\n            local_rows[index] = row\n\n        return local_rows\n\n    def _write_zarr(self, store, chunks, write_chunk_fn):\n        self.pcollection | gensym(""write_zarr"") >> beam.Map(write_chunk_fn)\n\n        result = self.pipeline.run()\n        result.wait_until_finish()\n\n    # Calculation methods (https://docs.scipy.org/doc/numpy-1.14.0/reference/arrays.ndarray.html#calculation)\n\n    def _calc_func_axis_rowwise(self, func, axis):\n        new_pcollection = self.pcollection | gensym(func.__name__) >> beam.Map(\n            lambda pair: (pair[0], func(pair[1], axis=axis))\n        )\n        return self._new(\n            pcollection=new_pcollection,\n            shape=(self.shape[0],),\n            chunks=(self.chunks[0],),\n        )\n\n    def _calc_func_axis_distributive(self, func, axis):\n        if axis == 0:  # column-wise\n            # note that unlike in the Spark implementation, nothing is materialized here - the whole computation is deferred\n            # this should make things faster\n            new_pcollection = (\n                self.pcollection\n                | gensym(func.__name__)\n                >> beam.Map(lambda pair: func(pair[1], axis=0))  # drop indexes\n                | gensym(func.__name__ + ""_combine"")\n                >> beam.CombineGlobally(lambda values: func(values, axis=0))\n                | gensym(""add_index"") >> beam.Map(lambda elt: (0, elt))\n            )  # only one row\n            new_shape = (self.shape[1],)\n            return self._new(\n                pcollection=new_pcollection,\n                shape=new_shape,\n                chunks=new_shape,\n                partition_row_counts=new_shape,\n            )\n        return NotImplemented\n\n    def _calc_mean(self, axis=None):\n        class CountAndSumsFn(beam.DoFn):\n            def process(self, element):\n                (idx, row) = element\n                count = row.shape[0]\n                sum = np.sum(row, axis=0)\n                return [(count, sum)]\n\n        class MeanColsFn(beam.CombineFn):\n            def __init__(self, num_cols):\n                self.num_cols = num_cols\n\n            def create_accumulator(self):\n                return 0.0, np.zeros(self.num_cols)\n\n            def add_input(self, count_sum, input):\n                (count, sum) = count_sum\n                (input_count, input_sum) = input\n                return count + input_count, np.add(sum, input_sum)\n\n            def merge_accumulators(self, accumulators):\n                counts, sums = zip(*accumulators)\n                return builtins.sum(counts), np.sum(sums, axis=0)\n\n            def extract_output(self, count_sum):\n                (count, sum) = count_sum\n                return (\n                    0,\n                    sum / count if count else float(""NaN""),\n                )  # one element with index 0\n\n        if axis == 0:  # mean of each column\n            # note that unlike in the Spark implementation, nothing is materialized here - the whole computation is deferred\n            # this should make things faster\n            new_pcollection = (\n                self.pcollection\n                | gensym(""count_and_sum"") >> beam.ParDo(CountAndSumsFn())\n                | gensym(""mean_cols"") >> beam.CombineGlobally(MeanColsFn(self.shape[1]))\n            )\n            new_shape = (self.shape[1],)\n            return self._new(\n                pcollection=new_pcollection,\n                shape=new_shape,\n                chunks=new_shape,\n                partition_row_counts=new_shape,\n            )\n        return NotImplemented\n\n    # TODO: for Beam we should be able to avoid materializing everything - defer all the computations (even shapes, row partitions, etc)!\n\n    # Distributed ufunc internal implementation\n\n    def _unary_ufunc(self, func, out=None, dtype=None):\n        new_pcollection = self.pcollection | gensym(func.__name__) >> beam.Map(\n            lambda pair: (pair[0], func(pair[1]))\n        )\n        return self._new(pcollection=new_pcollection, out=out, dtype=dtype)\n\n    def _binary_ufunc_self(self, func, out=None, dtype=None):\n        new_pcollection = self.pcollection | gensym(func.__name__) >> beam.Map(\n            lambda pair: (pair[0], func(pair[1], pair[1]))\n        )\n        return self._new(pcollection=new_pcollection, out=out, dtype=dtype)\n\n    def _binary_ufunc_broadcast_single_row_or_value(\n        self, func, other, out=None, dtype=None\n    ):\n        other = np.asarray(other)  # materialize\n        # TODO: should send \'other\' as a Beam side input\n        new_pcollection = self.pcollection | gensym(func.__name__) >> beam.Map(\n            lambda pair: (pair[0], func(pair[1], other))\n        )\n        return self._new(pcollection=new_pcollection, out=out, dtype=dtype)\n\n    def _binary_ufunc_broadcast_single_column(self, func, other, out=None, dtype=None):\n        # TODO: Beam (side input)\n        return NotImplemented\n\n    def _binary_ufunc_same_shape(self, func, other, out=None, dtype=None):\n        if self.partition_row_counts == other.partition_row_counts:\n            # args have the same rows (and partitioning) so use zip to combine then apply the operator\n            # Beam doesn\'t have a direct equivalent of Spark\'s zip function, so we use CoGroupByKey (is this less efficient?)\n            def combine_indexed_dict(indexed_dict):\n                idx, dict = indexed_dict\n                return idx, func(dict[""self""][0], dict[""other""][0])\n\n            new_pcollection = (\n                {""self"": self.pcollection, ""other"": other.pcollection}\n                | beam.CoGroupByKey()\n                | gensym(func.__name__) >> beam.Map(combine_indexed_dict)\n            )\n            return self._new(pcollection=new_pcollection, out=out, dtype=dtype)\n        return NotImplemented\n\n    # Slicing\n\n    def _boolean_array_index_dist(self, item):\n        subset = np.asarray(item)  # materialize\n        partition_row_subsets = ZappyArray._copartition(\n            subset, self.partition_row_counts\n        )\n        new_partition_row_counts = ZappyArray._partition_row_counts(\n            partition_row_subsets\n        )\n        new_shape = (builtins.sum(new_partition_row_counts),)\n\n        # Beam doesn\'t have a direct equivalent of Spark\'s zip function, so we use a side input and join here\n        # See https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/snippets.py#L1295\n        subset_pcollection = self.pipeline | gensym(\n            ""partition_row_subsets""\n        ) >> beam.Create(enumerate(partition_row_subsets))\n\n        def join_row_with_subset(index_row, subset_dict):\n            index, row = index_row\n            return index, row[subset_dict[index]]\n\n        new_pcollection = self.pcollection | gensym(""row_subset"") >> beam.Map(\n            join_row_with_subset, AsDict(subset_pcollection)\n        )\n\n        return self._new(\n            pcollection=new_pcollection,\n            shape=new_shape,\n            partition_row_counts=new_partition_row_counts,\n        )\n\n    def _column_subset(self, item):\n        if item[1] is np.newaxis:  # add new col axis\n            new_num_cols = 1\n            new_shape = (self.shape[0], new_num_cols)\n            new_chunks = (self.chunks[0], new_num_cols)\n            new_pcollection = self.pcollection | gensym(\n                ""column_subset_newaxis""\n            ) >> beam.Map(lambda pair: (pair[0], pair[1][:, np.newaxis]))\n            return self._new(\n                pcollection=new_pcollection, shape=new_shape, chunks=new_chunks\n            )\n        new_pcollection = self.pcollection | gensym(""column_subset"") >> beam.Map(\n            lambda pair: (pair[0], pair[1][item])\n        )\n        subset = ZappyArray._materialize_index(item[1])\n        new_num_cols = ZappyArray._compute_dim(self.shape[1], subset)\n        new_shape = (self.shape[0], new_num_cols)\n        new_chunks = (self.chunks[0], new_num_cols)\n        return self._new(\n            pcollection=new_pcollection, shape=new_shape, chunks=new_chunks\n        )\n\n    def _row_subset(self, item):\n        subset = ZappyArray._materialize_index(item[0])  # materialize\n        partition_row_subsets = ZappyArray._copartition(\n            subset, self.partition_row_counts\n        )\n        new_partition_row_counts = ZappyArray._partition_row_counts(\n            partition_row_subsets, self.partition_row_counts\n        )\n        new_shape = (builtins.sum(new_partition_row_counts), self.shape[1])\n\n        # Beam doesn\'t have a direct equivalent of Spark\'s zip function, so we use a side input and join here\n        # See https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/snippets.py#L1295\n        subset_pcollection = self.pipeline | gensym(\n            ""partition_row_subsets""\n        ) >> beam.Create(enumerate(partition_row_subsets))\n\n        def join_row_with_subset(index_row, subset_dict):\n            index, row = index_row\n            return index, row[subset_dict[index], :]\n\n        new_pcollection = self.pcollection | gensym(""row_subset"") >> beam.Map(\n            join_row_with_subset, AsDict(subset_pcollection)\n        )\n\n        return self._new(\n            pcollection=new_pcollection,\n            shape=new_shape,\n            partition_row_counts=new_partition_row_counts,\n        )\n'"
zappy/direct/__init__.py,0,"b'from zappy.direct.array import from_ndarray, from_zarr, ones, zeros\n'"
zappy/direct/array.py,12,"b'import builtins\nimport numpy as np\nimport zarr\n\nfrom zappy.base import *  # include everything in zappy.base and hence base numpy\nfrom zappy.zarr_util import get_chunk_sizes\n\n\ndef from_ndarray(arr, chunks):\n    return DirectZappyArray.from_ndarray(arr, chunks)\n\n\ndef from_zarr(zarr_file):\n    return DirectZappyArray.from_zarr(zarr_file)\n\n\ndef zeros(shape, chunks, dtype=float):\n    return DirectZappyArray.zeros(shape, chunks, dtype)\n\n\ndef ones(shape, chunks, dtype=float):\n    return DirectZappyArray.ones(shape, chunks, dtype)\n\n\nclass DirectZappyArray(ZappyArray):\n    """"""A numpy.ndarray backed by chunked storage""""""\n\n    def __init__(self, local_rows, shape, chunks, dtype, partition_row_counts=None):\n        ZappyArray.__init__(self, shape, chunks, dtype, partition_row_counts)\n        self.local_rows = local_rows\n\n    # methods to convert to/from regular ndarray - mainly for testing\n    @classmethod\n    def from_ndarray(cls, arr, chunks):\n        func, chunk_indices = ZappyArray._read_chunks(arr, chunks)\n        local_rows = [func(i) for i in chunk_indices]\n        return cls(local_rows, arr.shape, chunks, arr.dtype)\n\n    @classmethod\n    def from_zarr(cls, zarr_file):\n        """"""\n        Read a Zarr file as a DirectZappyArray object.\n        """"""\n        arr = zarr.open(zarr_file, mode=""r"")\n        return cls.from_ndarray(arr, arr.chunks)\n\n    @classmethod\n    def zeros(cls, shape, chunks, dtype=float):\n        local_rows = [\n            np.zeros(chunk, dtype=dtype) for chunk in get_chunk_sizes(shape, chunks)\n        ]\n        return cls(local_rows, shape, chunks, dtype)\n\n    @classmethod\n    def ones(cls, shape, chunks, dtype=float):\n        local_rows = [\n            np.ones(chunk, dtype=dtype) for chunk in get_chunk_sizes(shape, chunks)\n        ]\n        return cls(local_rows, shape, chunks, dtype)\n\n    def _compute(self):\n        return self.local_rows\n\n    def _repartition_chunks(self, chunks):\n        arr = np.concatenate(self.local_rows)\n        partition_row_counts = [chunks[0]] * (self.shape[0] // chunks[0])\n        remaining = self.shape[0] % chunks[0]\n        if remaining != 0:\n            partition_row_counts.append(remaining)\n        return self._new(\n            local_rows=ZappyArray._copartition_values(arr, partition_row_counts),\n            chunks=chunks,\n            partition_row_counts=partition_row_counts,\n        )\n\n    def _write_zarr(self, store, chunks, write_chunk_fn):\n        for (idx, arr) in enumerate(self.local_rows):\n            write_chunk_fn((idx, arr))\n\n    # Calculation methods (https://docs.scipy.org/doc/numpy-1.14.0/reference/arrays.ndarray.html#calculation)\n\n    def _calc_mean(self, axis=None):\n        if axis is None:\n            result = [\n                (x.shape[0] * x.shape[1], np.sum(x, axis=axis)) for x in self.local_rows\n            ]\n            total_count = builtins.sum([res[0] for res in result])\n            mean = np.sum([res[1] for res in result], axis=axis) / total_count\n            return mean\n        elif axis == 0:  # mean of each column\n            result = [(x.shape[0], np.sum(x, axis=axis)) for x in self.local_rows]\n            total_count = builtins.sum([res[0] for res in result])\n            mean = np.sum([res[1] for res in result], axis=axis) / total_count\n            local_rows = [mean]\n            return self._new(\n                local_rows=local_rows,\n                shape=mean.shape,\n                chunks=mean.shape,\n                partition_row_counts=mean.shape,\n            )\n        return NotImplemented\n\n    def _calc_func_axis_rowwise(self, func, axis):\n        return self._new(\n            local_rows=[func(x, axis=axis) for x in self.local_rows],\n            shape=(self.shape[0],),\n            chunks=(self.chunks[0],),\n        )\n\n    def _calc_func_axis_distributive(self, func, axis):\n        per_chunk_result = [func(x, axis=axis) for x in self.local_rows]\n        result = func(per_chunk_result, axis=axis)\n        if axis is None:\n            return result\n        elif axis == 0:  # column-wise\n            local_rows = [result]\n            return self._new(\n                local_rows=local_rows,\n                shape=result.shape,\n                chunks=result.shape,\n                partition_row_counts=result.shape,\n            )\n        return NotImplemented\n\n    # Distributed ufunc internal implementation\n\n    def _unary_ufunc(self, func, out=None, dtype=None):\n        new_local_rows = [func(x) for x in self.local_rows]\n        return self._new(local_rows=new_local_rows, out=out, dtype=dtype)\n\n    def _binary_ufunc_self(self, func, out=None, dtype=None):\n        new_local_rows = [func(x, x) for x in self.local_rows]\n        return self._new(local_rows=new_local_rows, out=out, dtype=dtype)\n\n    def _binary_ufunc_broadcast_single_row_or_value(\n        self, func, other, out=None, dtype=None\n    ):\n        other = np.asarray(other)  # materialize\n        new_local_rows = [func(x, other) for x in self.local_rows]\n        return self._new(local_rows=new_local_rows, out=out, dtype=dtype)\n\n    def _binary_ufunc_broadcast_single_column(self, func, other, out=None, dtype=None):\n        other = np.asarray(other)  # materialize\n        partition_row_subsets = ZappyArray._copartition(\n            other, self.partition_row_counts\n        )\n        new_local_rows = [\n            func(p[0], p[1]) for p in zip(self.local_rows, partition_row_subsets)\n        ]\n        return self._new(local_rows=new_local_rows, out=out, dtype=dtype)\n\n    def _binary_ufunc_same_shape(self, func, other, out=None, dtype=None):\n        if self.partition_row_counts == other.partition_row_counts:\n            new_local_rows = [\n                func(p[0], p[1]) for p in zip(self.local_rows, other.local_rows)\n            ]\n            return self._new(local_rows=new_local_rows, out=out, dtype=dtype)\n        return NotImplemented\n\n    # Slicing\n\n    def _boolean_array_index_dist(self, item):\n        # almost identical to row subset below\n        subset = np.asarray(item)  # materialize\n        partition_row_subsets = ZappyArray._copartition(\n            subset, self.partition_row_counts\n        )\n        new_partition_row_counts = ZappyArray._partition_row_counts(\n            partition_row_subsets\n        )\n        new_shape = (builtins.sum(new_partition_row_counts),)\n        return self._new(\n            local_rows=[\n                p[0][p[1]] for p in zip(self.local_rows, partition_row_subsets)\n            ],\n            shape=new_shape,\n            partition_row_counts=new_partition_row_counts,\n        )\n\n    def _column_subset(self, item):\n        if item[1] is np.newaxis:  # add new col axis\n            new_num_cols = 1\n            new_shape = (self.shape[0], new_num_cols)\n            new_chunks = (self.chunks[0], new_num_cols)\n            return self._new(\n                local_rows=[x[:, np.newaxis] for x in self.local_rows],\n                shape=new_shape,\n                chunks=new_chunks,\n            )\n        subset = ZappyArray._materialize_index(item[1])\n        new_num_cols = ZappyArray._compute_dim(self.shape[1], subset)\n        new_shape = (self.shape[0], new_num_cols)\n        new_chunks = (self.chunks[0], new_num_cols)\n        return self._new(\n            local_rows=[x[item] for x in self.local_rows],\n            shape=new_shape,\n            chunks=new_chunks,\n        )\n\n    def _row_subset(self, item):\n        subset = ZappyArray._materialize_index(item[0])  # materialize\n        partition_row_subsets = ZappyArray._copartition(\n            subset, self.partition_row_counts\n        )\n        new_partition_row_counts = ZappyArray._partition_row_counts(\n            partition_row_subsets, self.partition_row_counts\n        )\n        new_shape = (builtins.sum(new_partition_row_counts), self.shape[1])\n        return self._new(\n            local_rows=[\n                p[0][p[1], :] for p in zip(self.local_rows, partition_row_subsets)\n            ],\n            shape=new_shape,\n            partition_row_counts=new_partition_row_counts,\n        )\n'"
zappy/executor/__init__.py,0,"b'from zappy.executor.array import (\n    from_ndarray,\n    from_zarr,\n    ones,\n    zeros,\n    asndarrays,\n    PywrenExecutor,\n)\n'"
zappy/executor/array.py,13,"b'import builtins\nimport concurrent.futures\nimport datetime\nimport os\nimport pickle\nimport uuid\n\nimport numpy as np\nimport zarr\n\nfrom zappy.base import *  # include everything in zappy.base and hence base numpy\nfrom zappy.executor.dag import DAG\nfrom zappy.zarr_util import (\n    calculate_partition_boundaries,\n    extract_partial_chunks,\n    get_chunk_sizes,\n)\n\n\ndef from_ndarray(executor, arr, chunks, intermediate_store=None):\n    return ExecutorZappyArray.from_ndarray(executor, arr, chunks, intermediate_store)\n\n\ndef from_zarr(executor, zarr_file, intermediate_store=None):\n    return ExecutorZappyArray.from_zarr(executor, zarr_file, intermediate_store)\n\n\ndef zeros(executor, shape, chunks, dtype=float, intermediate_store=None):\n    return ExecutorZappyArray.zeros(executor, shape, chunks, dtype, intermediate_store)\n\n\ndef ones(executor, shape, chunks, dtype=float, intermediate_store=None):\n    return ExecutorZappyArray.ones(executor, shape, chunks, dtype, intermediate_store)\n\n\ndef asndarrays(arrays):\n    return ExecutorZappyArray.asndarrays(arrays)\n\n\nclass PywrenExecutor(object):\n    """"""Small wrapper to make a Pywren executor behave like a concurrent.futures.Executor.""""""\n\n    def __init__(\n        self, pywren_executor=None, exclude_modules=None, record_job_history=True\n    ):\n        import pywren\n\n        self.pywren_executor = (\n            pywren_executor\n            if pywren_executor is not None\n            else pywren.default_executor()\n        )\n        self.exclude_modules = exclude_modules\n        self.record_job_history = record_job_history\n\n    def map(self, func, iterables):\n        import pywren\n\n        futures = self.pywren_executor.map(\n            func, iterables, exclude_modules=self.exclude_modules\n        )\n        pywren.wait(futures, return_when=pywren.ALL_COMPLETED)\n        results = [f.result() for f in futures]\n        if self.record_job_history:\n            run_statuses = [f.run_status for f in futures]\n            invoke_statuses = [f.invoke_status for f in futures]\n            outdict = {\n                ""futures"": futures,\n                ""run_statuses"": run_statuses,\n                ""invoke_statuses"": invoke_statuses,\n            }\n            logs_dir = os.path.expanduser(""~/.zappy/logs"")\n            os.makedirs(logs_dir, exist_ok=True)\n            timestamp = datetime.datetime.now().strftime(""%Y%m%d-%H%M%S.%f"")\n            filename = os.path.join(logs_dir, ""pywren-{}.pickle"".format(timestamp))\n            with open(filename, ""wb"") as file:\n                pickle.dump(outdict, file)\n        return results\n\n\nclass ExecutorZappyArray(ZappyArray):\n    """"""A numpy.ndarray backed by chunked storage""""""\n\n    def __init__(\n        self,\n        executor,\n        dag,\n        input,\n        shape,\n        chunks,\n        dtype,\n        partition_row_counts=None,\n        intermediate_store=None,\n    ):\n        ZappyArray.__init__(self, shape, chunks, dtype, partition_row_counts)\n        self.executor = executor\n        self.dag = dag\n        self.input = input\n        if intermediate_store is None:\n            self.intermediate_group = zarr.group()\n        else:\n            self.intermediate_group = zarr.group(intermediate_store)\n\n    # methods to convert to/from regular ndarray - mainly for testing\n    @classmethod\n    def from_ndarray(cls, executor, arr, chunks, intermediate_store=None):\n        func, chunk_indices = ZappyArray._read_chunks(arr, chunks)\n        dag = DAG(executor)\n        # the input is just the chunk indices\n        input = dag.add_input(chunk_indices)\n        # add a transform to read chunks\n        input = dag.transform(func, [input])\n        return cls(\n            executor,\n            dag,\n            input,\n            arr.shape,\n            chunks,\n            arr.dtype,\n            intermediate_store=intermediate_store,\n        )\n\n    @classmethod\n    def from_zarr(cls, executor, zarr_file, intermediate_store=None):\n        """"""\n        Read a Zarr file as an ExecutorZappyArray object.\n        """"""\n        arr = zarr.open(zarr_file, mode=""r"")\n        return cls.from_ndarray(executor, arr, arr.chunks, intermediate_store)\n\n    @classmethod\n    def zeros(cls, executor, shape, chunks, dtype=float, intermediate_store=None):\n        dag = DAG(executor)\n        input = dag.add_input(list(get_chunk_sizes(shape, chunks)))\n        input = dag.transform(lambda chunk: np.zeros(chunk, dtype=dtype), [input])\n        return cls(\n            executor,\n            dag,\n            input,\n            shape,\n            chunks,\n            dtype,\n            intermediate_store=intermediate_store,\n        )\n\n    @classmethod\n    def ones(cls, executor, shape, chunks, dtype=float, intermediate_store=None):\n        dag = DAG(executor)\n        input = dag.add_input(list(get_chunk_sizes(shape, chunks)))\n        input = dag.transform(lambda chunk: np.ones(chunk, dtype=dtype), [input])\n        return cls(\n            executor,\n            dag,\n            input,\n            shape,\n            chunks,\n            dtype,\n            intermediate_store=intermediate_store,\n        )\n\n    @classmethod\n    def asndarrays(cls, arrays):\n        """"""Compute all the input arrays in a single executor and return a tuple of ndarrays.""""""\n        # TODO: cache data read from zarr\n        if (\n            len(arrays) > 0 and len(set([arr.dag for arr in arrays])) == 1\n        ):  # all dags the same\n            array_chunk_tuples = arrays[0].dag.compute_multiple(\n                [arr.input for arr in arrays]\n            )\n            return tuple(\n                ZappyArray._array_chunks_to_ndarray(\n                    array_chunks, arrays[index].partition_row_counts\n                )\n                for (index, array_chunks) in enumerate(array_chunk_tuples)\n            )\n        else:\n            return tuple(arr.asndarray() for arr in arrays)\n\n    def _compute(self):\n        return list(self.dag.compute(self.input))\n\n    def _repartition_chunks(self, chunks):\n        c = chunks[0]\n        partition_row_ranges, total_rows, new_num_partitions = calculate_partition_boundaries(\n            chunks, self.partition_row_counts\n        )\n\n        # make a new zarr group in the intermediate store with a unique name\n        root = self.intermediate_group.create_group(str(uuid.uuid4()))\n        # make a zarr group for each partition\n        with concurrent.futures.ThreadPoolExecutor(max_workers=64) as executor:\n            executor.map(\n                lambda index: root.create_group(str(index)), range(new_num_partitions)\n            )\n\n        def tmp_store(pairs):\n            for pair in pairs:\n                index, offsets, partial_chunk = pair[0], pair[1][0], pair[1][1]\n                g = root.require_group(str(index))\n                g.array(""%s-%s"" % (offsets[0], offsets[1]), partial_chunk, chunks=False)\n\n        x1 = self.dag.add_input(partition_row_ranges)\n        x2 = self.dag.transform(\n            lambda x, y: extract_partial_chunks((x, y), chunks), [x1, self.input]\n        )\n\n        x3 = self.dag.transform(tmp_store, [x2])\n\n        # run computation to save partial chunks\n        list(self.dag.compute(x3))\n\n        # create a new computation to read and combine partial chunks\n        def tmp_load(new_index):\n            # last chunk has fewer than c rows\n            if new_index == new_num_partitions - 1 and total_rows % c != 0:\n                last_chunk_rows = total_rows % c\n                arr = np.zeros((last_chunk_rows, chunks[1]))\n            else:\n                arr = np.zeros(chunks)\n            g = root.require_group(str(new_index))\n            for (name, partial_chunk) in g.arrays():\n                new_start_offset, new_end_offset = [int(n) for n in name.split(""-"")]\n                arr[new_start_offset:new_end_offset] = partial_chunk\n            return arr\n\n        dag = DAG(self.executor)\n        input = dag.add_input(list(range(new_num_partitions)))\n        input = dag.transform(tmp_load, [input])\n\n        # TODO: delete intermediate store when dag is computed\n        return ExecutorZappyArray(\n            self.executor, dag, input, self.shape, chunks, self.dtype\n        )\n\n    def _write_zarr(self, store, chunks, write_chunk_fn):\n        indices = self.dag.add_input(list(range(len(self.partition_row_counts))))\n        output = self.dag.transform(\n            lambda x, y: write_chunk_fn((x, y)), [indices, self.input]\n        )\n        list(self.dag.compute(output))\n\n    # Calculation methods (https://docs.scipy.org/doc/numpy-1.14.0/reference/arrays.ndarray.html#calculation)\n\n    def _calc_mean(self, axis=None):\n        if axis is None:\n            result = self._new(\n                input=self.dag.transform(\n                    lambda x: (x.shape[0] * x.shape[1], np.sum(x, axis=axis)),\n                    [self.input],\n                )\n            )._compute()\n            total_count = builtins.sum([res[0] for res in result])\n            mean = np.sum([res[1] for res in result], axis=axis) / total_count\n            return mean\n        elif axis == 0:  # mean of each column\n            result = self._new(\n                input=self.dag.transform(\n                    lambda x: (x.shape[0], np.sum(x, axis=axis)), [self.input]\n                )\n            )._compute()\n            total_count = builtins.sum([res[0] for res in result])\n            mean = np.sum([res[1] for res in result], axis=axis) / total_count\n            # new dag\n            dag = DAG(self.executor)\n            partitioned_input = [mean]\n            input = dag.add_input(partitioned_input)\n            return self._new(\n                dag=dag,\n                input=input,\n                shape=mean.shape,\n                chunks=mean.shape,\n                partition_row_counts=mean.shape,\n            )\n        return NotImplemented\n\n    def _calc_func_axis_rowwise(self, func, axis):\n        input = self.dag.transform(lambda x: func(x, axis=axis), [self.input])\n        return self._new(input=input, shape=(self.shape[0],), chunks=(self.chunks[0],))\n\n    def _calc_func_axis_distributive(self, func, axis):\n        per_chunk_result = self._new(\n            input=self.dag.transform(lambda x: func(x, axis=axis), [self.input])\n        )._compute()\n        result = func(per_chunk_result, axis=axis)\n        if axis is None:\n            return result\n        elif axis == 0:  # column-wise\n            # new dag\n            dag = DAG(self.executor)\n            partitioned_input = [result]\n            input = dag.add_input(partitioned_input)\n            return self._new(\n                dag=dag,\n                input=input,\n                shape=result.shape,\n                chunks=result.shape,\n                partition_row_counts=result.shape,\n            )\n        return NotImplemented\n\n    # Distributed ufunc internal implementation\n\n    def _unary_ufunc(self, func, out=None, dtype=None):\n        input = self.dag.transform(func, [self.input])\n        return self._new(input=input, out=out, dtype=dtype)\n\n    def _binary_ufunc_self(self, func, out=None, dtype=None):\n        input = self.dag.transform(lambda x: func(x, x), [self.input])\n        return self._new(input=input, out=out, dtype=dtype)\n\n    def _binary_ufunc_broadcast_single_row_or_value(\n        self, func, other, out=None, dtype=None\n    ):\n        other = np.asarray(other)  # materialize\n        input = self.dag.transform(lambda x: func(x, other), [self.input])\n        return self._new(input=input, out=out, dtype=dtype)\n\n    def _binary_ufunc_broadcast_single_column(self, func, other, out=None, dtype=None):\n        other = np.asarray(other)  # materialize\n        partition_row_subsets = ZappyArray._copartition(\n            other, self.partition_row_counts\n        )\n        side_input = self.dag.add_input(partition_row_subsets)\n        input = self.dag.transform(func, [self.input, side_input])\n        return self._new(input=input, out=out, dtype=dtype)\n\n    def _binary_ufunc_same_shape(self, func, other, out=None, dtype=None):\n        if self.partition_row_counts == other.partition_row_counts:\n            input = self.dag.transform(func, [self.input, other.input])\n            return self._new(input=input, out=out, dtype=dtype)\n        return NotImplemented\n\n    # Slicing\n\n    def _boolean_array_index_dist(self, item):\n        # almost identical to row subset below (only lambda has different indexing)\n        subset = np.asarray(item)  # materialize\n        partition_row_subsets = ZappyArray._copartition(\n            subset, self.partition_row_counts\n        )\n        new_partition_row_counts = ZappyArray._partition_row_counts(\n            partition_row_subsets\n        )\n        new_shape = (builtins.sum(new_partition_row_counts),)\n        side_input = self.dag.add_input(partition_row_subsets)\n        input = self.dag.transform(lambda x, y: x[y], [self.input, side_input])\n        return self._new(\n            input=input, shape=new_shape, partition_row_counts=new_partition_row_counts\n        )\n\n    def _column_subset(self, item):\n        if item[1] is np.newaxis:  # add new col axis\n            new_num_cols = 1\n            new_shape = (self.shape[0], new_num_cols)\n            new_chunks = (self.chunks[0], new_num_cols)\n            input = self.dag.transform(lambda x: x[:, np.newaxis], [self.input])\n            return self._new(input=input, shape=new_shape, chunks=new_chunks)\n        subset = ZappyArray._materialize_index(item[1])\n        new_num_cols = ZappyArray._compute_dim(self.shape[1], subset)\n        new_shape = (self.shape[0], new_num_cols)\n        new_chunks = (self.chunks[0], new_num_cols)\n        input = self.dag.transform(lambda x: x[item], [self.input])\n        return self._new(input=input, shape=new_shape, chunks=new_chunks)\n\n    def _row_subset(self, item):\n        subset = ZappyArray._materialize_index(item[0])  # materialize\n        partition_row_subsets = ZappyArray._copartition(\n            subset, self.partition_row_counts, shrink=True\n        )\n        new_partition_row_counts = ZappyArray._partition_row_counts(\n            partition_row_subsets, self.partition_row_counts\n        )\n        new_shape = (builtins.sum(new_partition_row_counts), self.shape[1])\n        side_input = self.dag.add_input(partition_row_subsets)\n        input = self.dag.transform(lambda x, y: x[y, :], [self.input, side_input])\n        return self._new(\n            input=input, shape=new_shape, partition_row_counts=new_partition_row_counts\n        )\n'"
zappy/executor/dag.py,0,"b'import concurrent.futures\n\n\ndef unpack_args(f):\n    return lambda x: f(*x)\n\n\nclass Input:\n    def __init__(self, index):\n        self.index = index\n\n    def compute(self, input_values):\n        return input_values[self.index]\n\n    def node(self):\n        return \'""Input({}, {})""\'.format(id(self), self.index)\n\n    def dot(self, s=""""):\n        s += ""{}\\n"".format(self.node())\n        return s\n\n\nclass LazyVal:\n    def __init__(self, func, inputs):\n        """"""Inputs must be of type Input or LazyVal""""""\n        self.func = func\n        self.inputs = inputs\n\n    def compute(self, input_values):\n        computed_inputs = [input.compute(input_values) for input in self.inputs]\n        return unpack_args(self.func)(computed_inputs)\n\n    def node(self):\n        return \'""LazyVal({}, {})""\'.format(id(self), self.func)\n\n    def dot(self, s=""""):\n        for input in self.inputs:\n            s += ""{} -> {}\\n"".format(self.node(), input.node())\n        for input in self.inputs:\n            s = input.dot(s)\n        return s\n\n\nclass DAG:\n    def __init__(self, executor):\n        self.executor = executor\n        self.local_executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)\n        self.num_partitions = 0\n        self.partitioned_inputs = []\n\n    def add_input(self, partitioned_input):\n        assert len(partitioned_input) > 0\n        # first input sets the number of partitions\n        if self.num_partitions == 0:\n            self.num_partitions = len(partitioned_input)\n        # further inputs cannot grow the number of partitions\n        elif self.num_partitions < len(partitioned_input):\n            assert self.num_partitions >= len(partitioned_input)\n        # but they can shrink (e.g. when subsetting rows)\n        else:\n            self.num_partitions = len(partitioned_input)\n            self.partitioned_inputs = [\n                inputs[: self.num_partitions] for inputs in self.partitioned_inputs\n            ]\n        index = len(self.partitioned_inputs)\n        self.partitioned_inputs.append(partitioned_input)\n        return Input(index)\n\n    def transform(self, func, inputs):\n        assert len(inputs) > 0\n        return LazyVal(func, inputs)\n\n    def _get_zipped_inputs(self):\n        # iterate over inputs one partition at a time\n        return zip(*self.partitioned_inputs)\n\n    def compute(self, output):\n        # Uncomment to see the dot representation of the DAG\n        # print(""digraph compute {{\\n{}}}"".format(output.dot()))\n        if len(list(self._get_zipped_inputs())) == 1:\n            # run single partitions locally\n            return self.local_executor.map(output.compute, self._get_zipped_inputs())\n        return self.executor.map(output.compute, self._get_zipped_inputs())\n\n    def compute_multiple(self, outputs):\n        def apply_multiple(x):\n            return tuple(f(x) for f in tuple(output.compute for output in outputs))\n\n        return zip(*self.executor.map(apply_multiple, self._get_zipped_inputs()))\n'"
zappy/spark/__init__.py,0,"b'from zappy.spark.array import from_ndarray, from_zarr, ones, zeros\n'"
zappy/spark/array.py,14,"b'import builtins\nimport numpy as np\nimport zarr\n\nfrom zappy.base import *  # include everything in zappy.base and hence base numpy\nfrom zappy.zarr_util import (\n    calculate_partition_boundaries,\n    extract_partial_chunks,\n    get_chunk_sizes,\n)\n\n\ndef from_ndarray(sc, arr, chunks):\n    return SparkZappyArray.from_ndarray(sc, arr, chunks)\n\n\ndef from_zarr(sc, zarr_file):\n    return SparkZappyArray.from_zarr(sc, zarr_file)\n\n\ndef zeros(sc, shape, chunks, dtype=float):\n    return SparkZappyArray.zeros(sc, shape, chunks, dtype)\n\n\ndef ones(sc, shape, chunks, dtype=float):\n    return SparkZappyArray.ones(sc, shape, chunks, dtype)\n\n\n# ndarray in Spark\n\n\nclass SparkZappyArray(ZappyArray):\n    """"""A numpy.ndarray backed by a Spark RDD""""""\n\n    def __init__(self, sc, rdd, shape, chunks, dtype, partition_row_counts=None):\n        ZappyArray.__init__(self, shape, chunks, dtype, partition_row_counts)\n        self.sc = sc\n        self.rdd = rdd\n\n    # methods to convert to/from regular ndarray - mainly for testing\n    @classmethod\n    def from_ndarray(cls, sc, arr, chunks):\n        func, chunk_indices = ZappyArray._read_chunks(arr, chunks)\n        rdd = sc.parallelize(chunk_indices, len(chunk_indices)).map(func)\n        return cls(sc, rdd, arr.shape, chunks, arr.dtype)\n\n    @classmethod\n    def from_zarr(cls, sc, zarr_file):\n        """"""\n        Read a Zarr file as a SparkZappyArray object.\n        """"""\n        arr = zarr.open(zarr_file, mode=""r"")\n        return cls.from_ndarray(sc, arr, arr.chunks)\n\n    @classmethod\n    def zeros(cls, sc, shape, chunks, dtype=float):\n        chunk_sizes = list(get_chunk_sizes(shape, chunks))\n        rdd = sc.parallelize(chunk_sizes, len(chunk_sizes)).map(\n            lambda chunk: np.zeros(chunk, dtype=dtype)\n        )\n        return cls(sc, rdd, shape, chunks, dtype)\n\n    @classmethod\n    def ones(cls, sc, shape, chunks, dtype=float):\n        chunk_sizes = list(get_chunk_sizes(shape, chunks))\n        rdd = sc.parallelize(chunk_sizes, len(chunk_sizes)).map(\n            lambda chunk: np.ones(chunk, dtype=dtype)\n        )\n        return cls(sc, rdd, shape, chunks, dtype)\n\n    def _compute(self):\n        return self.rdd.collect()\n\n    def _repartition_chunks(self, chunks):\n        c = chunks[0]  # the chunk size for rows\n\n        partition_row_ranges, total_rows, new_num_partitions = calculate_partition_boundaries(\n            chunks, self.partition_row_counts\n        )\n\n        def extract(iterator):\n            # iterator has just a single element for map partitions\n            return extract_partial_chunks(list(iterator)[0], chunks)\n\n        def identity_partition_func(key):\n            return key\n\n        def combine_partial_chunks(pair):\n            """"""\n            Combine multiple non-overlapping parts of a new chunk into a single chunk.\n            """"""\n            new_index = pair[0]\n            # last chunk has fewer than c rows\n            if new_index == new_num_partitions - 1 and total_rows % c != 0:\n                last_chunk_rows = total_rows % c\n                arr = np.zeros((last_chunk_rows, chunks[1]))\n            else:\n                arr = np.zeros(chunks)\n            for ((new_start_offset, new_end_offset), partial_chunk) in pair[1]:\n                arr[new_start_offset:new_end_offset] = partial_chunk\n            return arr\n\n        partitioned_rdd = (\n            self.sc.parallelize(partition_row_ranges, len(partition_row_ranges))\n            .zip(self.rdd)\n            .mapPartitions(extract)\n            .groupByKey(new_num_partitions, identity_partition_func)\n            .map(combine_partial_chunks)\n        )\n\n        partition_row_counts = [chunks[0]] * (self.shape[0] // chunks[0])\n        remaining = self.shape[0] % chunks[0]\n        if remaining != 0:\n            partition_row_counts.append(remaining)\n        return self._new(\n            rdd=partitioned_rdd,\n            chunks=chunks,\n            partition_row_counts=partition_row_counts,\n        )\n\n    def _write_zarr(self, store, chunks, write_chunk_fn):\n        def index_partitions(index, iterator):\n            values = list(iterator)\n            assert len(values) == 1  # 1 numpy array per partition\n            return [(index, values[0])]\n\n        self.rdd.mapPartitionsWithIndex(index_partitions).foreach(write_chunk_fn)\n\n    # Calculation methods (https://docs.scipy.org/doc/numpy-1.14.0/reference/arrays.ndarray.html#calculation)\n\n    def _calc_mean(self, axis=None):\n        if axis is None:\n            result = self.rdd.map(\n                lambda x: (x.shape[0] * x.shape[1], np.sum(x, axis=axis))\n            ).collect()\n            total_count = builtins.sum([res[0] for res in result])\n            mean = np.sum([res[1] for res in result], axis=axis) / total_count\n            return mean\n        elif axis == 0:  # mean of each column\n            result = self.rdd.map(\n                lambda x: (x.shape[0], np.sum(x, axis=axis))\n            ).collect()\n            total_count = builtins.sum([res[0] for res in result])\n            mean = np.sum([res[1] for res in result], axis=axis) / total_count\n            rdd = self.rdd.ctx.parallelize([mean])\n            return self._new(\n                rdd=rdd,\n                shape=mean.shape,\n                chunks=mean.shape,\n                partition_row_counts=mean.shape,\n            )\n        return NotImplemented\n\n    def _calc_func_axis_rowwise(self, func, axis):\n        return self._new(\n            rdd=self.rdd.map(lambda x: func(x, axis=axis)),\n            shape=(self.shape[0],),\n            chunks=(self.chunks[0],),\n        )\n\n    def _calc_func_axis_distributive(self, func, axis):\n        per_chunk_result = self.rdd.map(lambda x: func(x, axis=axis)).collect()\n        result = func(per_chunk_result, axis=axis)\n        if axis is None:\n            return result\n        elif axis == 0:  # column-wise\n            rdd = self.rdd.ctx.parallelize([result])\n            return self._new(\n                rdd=rdd,\n                shape=result.shape,\n                chunks=result.shape,\n                partition_row_counts=result.shape,\n            )\n        return NotImplemented\n\n    # Distributed ufunc internal implementation\n\n    def _unary_ufunc(self, func, out=None, dtype=None):\n        new_rdd = self.rdd.map(lambda x: func(x))\n        return self._new(rdd=new_rdd, out=out, dtype=dtype)\n\n    def _binary_ufunc_self(self, func, out=None, dtype=None):\n        new_rdd = self.rdd.map(lambda x: func(x, x))\n        return self._new(rdd=new_rdd, out=out, dtype=dtype)\n\n    def _binary_ufunc_broadcast_single_row_or_value(\n        self, func, other, out=None, dtype=None\n    ):\n        other = np.asarray(other)  # materialize\n        # TODO: should send \'other\' as a Spark broadcast\n        new_rdd = self.rdd.map(lambda x: func(x, other))\n        return self._new(rdd=new_rdd, out=out, dtype=dtype)\n\n    def _binary_ufunc_broadcast_single_column(self, func, other, out=None, dtype=None):\n        other = np.asarray(other)  # materialize\n        partition_row_subsets = ZappyArray._copartition(\n            other, self.partition_row_counts\n        )\n        repartitioned_other_rdd = self.sc.parallelize(\n            partition_row_subsets, len(partition_row_subsets)\n        )\n        new_rdd = self.rdd.zip(repartitioned_other_rdd).map(lambda p: func(p[0], p[1]))\n        return self._new(rdd=new_rdd, out=out, dtype=dtype)\n\n    def _binary_ufunc_same_shape(self, func, other, out=None, dtype=None):\n        if self.partition_row_counts == other.partition_row_counts:\n            new_rdd = self.rdd.zip(other.rdd).map(lambda p: func(p[0], p[1]))\n            return self._new(rdd=new_rdd, out=out, dtype=dtype)\n        elif other.shape[1] == 1:\n            partition_row_subsets = self._copartition(\n                np.asarray(other), self.partition_row_counts\n            )\n            repartitioned_other_rdd = self.sc.parallelize(\n                partition_row_subsets, len(partition_row_subsets)\n            )\n            new_rdd = self.rdd.zip(repartitioned_other_rdd).map(\n                lambda p: func(p[0], p[1])\n            )\n            return self._new(rdd=new_rdd, out=out, dtype=dtype)\n        return NotImplemented\n\n    # Slicing\n\n    def _boolean_array_index_dist(self, item):\n        subset = np.asarray(item)  # materialize\n        partition_row_subsets = ZappyArray._copartition(\n            subset, self.partition_row_counts\n        )\n        new_partition_row_counts = ZappyArray._partition_row_counts(\n            partition_row_subsets\n        )\n        new_shape = (builtins.sum(new_partition_row_counts),)\n        subset_rdd = self.sc.parallelize(\n            partition_row_subsets, len(partition_row_subsets)\n        )\n        return self._new(\n            rdd=self.rdd.zip(subset_rdd).map(lambda p: p[0][p[1]]),\n            shape=new_shape,\n            partition_row_counts=new_partition_row_counts,\n        )\n\n    def _column_subset(self, item):\n        if item[1] is np.newaxis:  # add new col axis\n            new_num_cols = 1\n            new_shape = (self.shape[0], new_num_cols)\n            new_chunks = (self.chunks[0], new_num_cols)\n            return self._new(\n                rdd=self.rdd.map(lambda x: x[:, np.newaxis]),\n                shape=new_shape,\n                chunks=new_chunks,\n            )\n        subset = ZappyArray._materialize_index(item[1])\n        new_num_cols = ZappyArray._compute_dim(self.shape[1], subset)\n        new_shape = (self.shape[0], new_num_cols)\n        new_chunks = (self.chunks[0], new_num_cols)\n        return self._new(\n            rdd=self.rdd.map(lambda x: x[item]), shape=new_shape, chunks=new_chunks\n        )\n\n    def _row_subset(self, item):\n        subset = ZappyArray._materialize_index(item[0])  # materialize\n        partition_row_subsets = ZappyArray._copartition(\n            subset, self.partition_row_counts\n        )\n        new_partition_row_counts = ZappyArray._partition_row_counts(\n            partition_row_subsets, self.partition_row_counts\n        )\n        new_shape = (builtins.sum(new_partition_row_counts), self.shape[1])\n        subset_rdd = self.sc.parallelize(\n            partition_row_subsets, len(partition_row_subsets)\n        )\n        return self._new(\n            rdd=self.rdd.zip(subset_rdd).map(lambda p: p[0][p[1], :]),\n            shape=new_shape,\n            partition_row_counts=new_partition_row_counts,\n        )\n'"
