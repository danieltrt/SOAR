file_path,api_count,code
2_alchemist/createFakeHDF.py,4,"b""import numpy as np\nfrom pandas import HDFStore, DataFrame\n\n# create (or open) an hdf5 file and opens in append mode\nhdf = HDFStore('data/hdata.h5')\n\ndf = DataFrame(np.random.rand(1000, 3), columns=('A', 'B', 'C'))\n# put the dataset in the storage\nhdf.put('d1', df, format='table', data_columns=True)\nprint(hdf['d1'].shape)\n\nhdf.append('d1', DataFrame(np.random.rand(5, 3),\n                           columns=('A', 'B', 'C')),\n           format='table', data_columns=True)\n\ndf = DataFrame(np.random.rand(1000, 3), columns=('A', 'B', 'C'))\n# put the dataset in the storage\nhdf.put('d2', df, format='table', data_columns=True)\nprint(hdf['d2'].shape)\n\nhdf.append('d2', DataFrame(np.random.rand(5, 3),\n                           columns=('A', 'B', 'C')),\n           format='table', data_columns=True)\nhdf.close()  # closes the file\n"""
2_alchemist/helpers.py,0,"b'# helper to display Pandas Table output side by side\n\nfrom IPython.display import display_html\n\ndef highlight(data):\n    return [\'background-color: yellow\' for x in data]\n\ndef display_side_by_side(subset, *args):\n    html_str=\'\'\n    for i, df in enumerate(args):\n        if i:\n            html_str+=df.style.render()\n        else:\n            df.style.apply(highlight, subset)\n            html_str+=df.style.render()\n            \n    display_html(html_str.replace(\'table\',\'table style=""display:inline""\'),raw=True)\n    \n\n'"
2_alchemist/plot_clustering.py,0,"b""\nimport matplotlib.pyplot as plt\n\ndef plot_kmeans_clustering_results(c1, c2, c3, vq1, vq2, vq3):\n\n    # Setting plot limits\n    x1, x2 = -10, 10\n    y1, y2 = -10, 10\n\n    fig = plt.figure()\n    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n\n    ax1 = fig.add_subplot(121, aspect='equal')\n    ax1.scatter(c1[:, 0], c1[:, 1], lw=0.5, color='#00CC00')\n    ax1.scatter(c2[:, 0], c2[:, 1], lw=0.5, color='#028E9B')\n    ax1.scatter(c3[:, 0], c3[:, 1], lw=0.5, color='#FF7800')\n    ax1.xaxis.set_visible(False)\n    ax1.yaxis.set_visible(False)\n    ax1.set_xlim(x1, x2)\n    ax1.set_ylim(y1, y2)\n    ax1.text(-9, 8, 'Original')\n\n    ax2 = fig.add_subplot(122, aspect='equal')\n    ax2.scatter(vqc1[:, 0], vqc1[:, 1], lw=0.5, color='#00CC00')\n    ax2.scatter(vqc2[:, 0], vqc2[:, 1], lw=0.5, color='#028E9B')\n    ax2.scatter(vqc3[:, 0], vqc3[:, 1], lw=0.5, color='#FF7800')\n    ax2.xaxis.set_visible(False)\n    ax2.yaxis.set_visible(False)\n    ax2.set_xlim(x1, x2)\n    ax2.set_ylim(y1, y2)\n    ax2.text(-9, 8, 'VQ identified')\n\n    return fig"""
4_archmage/ann.py,15,"b'""""""Python Implementation of Multi-Layer Perceptron""""""\n\nimport numpy as np\nfrom numpy.random import seed\n\n# =================\n# Utility functions\n# =================\n\nimport random\nrandom.seed(123)\n\n# calculate a random number where:  a <= rand < b\ndef rand(a, b):\n    return (b-a)*random.random() + a\n\n# Make a matrix\ndef makeMatrix(I, J, fill=0.0):\n    return np.zeros([I,J])\n\n# our sigmoid function\ndef sigmoid(x):\n    #return math.tanh(x)\n    return 1/(1+np.exp(-x))\n\n# derivative of our sigmoid function, in terms of the output (i.e. y)\ndef dsigmoid(y):\n    return y - y**2\n\n# ==================\n\n\nclass MLP:\n    """"""Multi Layer Perceptron\n\n    Parameters\n    ------------\n    ni : int\n        Number of Input neurons\n    n_h : int\n        Number of Hidden neurons\n    n_o : int\n        Number of Output neurons\n\n    Attributes\n    -----------\n    ni : int\n        Number of Input neurons\n    n_h : int\n        Number of Hidden neurons\n    n_o : int\n        Number of Output neurons\n\n    ai : 1d-array (size: n_i)\n        Activations for Input layer\n    ah : 1d-array (size: n_h)\n        Activations for Hidden layer\n    ao : 1d-array (size: n_o)\n        Activations for Output layer\n\n    wi : 2d-array (shape n_i x n_h_)\n        Weight matrix between Input and Hidden Layer.\n    wo : 2d-array (shape n_h x n_o)\n        Weight matrix between Hidden and Output Layer.\n\n    """"""\n\n    def __init__(self, ni, nh, no):\n        # number of input, hidden, and output nodes\n        self.ni = ni + 1 # +1 for bias node\n        self.nh = nh\n        self.no = no\n\n        # activations for nodes\n        self.ai = [1.0]*self.ni\n        self.ah = [1.0]*self.nh\n        self.ao = [1.0]*self.no\n\n        # create weights\n        self.wi = makeMatrix(self.ni, self.nh)\n        self.wo = makeMatrix(self.nh, self.no)\n\n        # set them to random vaules\n        for i in range(self.ni):\n            for j in range(self.nh):\n                self.wi[i][j] = rand(-0.2, 0.2)\n        for j in range(self.nh):\n            for k in range(self.no):\n                self.wo[j][k] = rand(-2.0, 2.0)\n\n        # last change in weights for momentum\n        self.ci = makeMatrix(self.ni, self.nh)\n        self.co = makeMatrix(self.nh, self.no)\n\n\n    def backPropagate(self, targets, N, M):\n\n        if len(targets) != self.no:\n            print(targets)\n            raise ValueError(\'wrong number of target values\')\n\n        # calculate error terms for output\n        output_deltas = np.zeros(self.no)\n        for k in range(self.no):\n            error = targets[k]-self.ao[k]\n            output_deltas[k] = dsigmoid(self.ao[k]) * error\n\n        # calculate error terms for hidden\n        hidden_deltas = np.zeros(self.nh)\n        for j in range(self.nh):\n            error = 0.0\n            for k in range(self.no):\n                error += output_deltas[k]*self.wo[j][k]\n            hidden_deltas[j] = dsigmoid(self.ah[j]) * error\n\n        # update output weights\n        for j in range(self.nh):\n            for k in range(self.no):\n                change = output_deltas[k] * self.ah[j]\n                self.wo[j][k] += N*change + M*self.co[j][k]\n                self.co[j][k] = change\n\n        # update input weights\n        for i in range(self.ni):\n            for j in range(self.nh):\n                change = hidden_deltas[j]*self.ai[i]\n                self.wi[i][j] += N*change + M*self.ci[i][j]\n                self.ci[i][j] = change\n\n        # calculate error\n        error = 0.0\n        for k in range(len(targets)):\n            error += 0.5*(targets[k]-self.ao[k])**2\n        return error\n\n\n    def test(self, patterns):\n        self.predict = np.empty([len(patterns), self.no])\n        for i, p in enumerate(patterns):\n            self.predict[i] = self.activate(p)\n            #self.predict[i] = self.activate(p[0])\n\n    def activate(self, inputs):\n\n        if len(inputs) != self.ni-1:\n            print(inputs)\n            raise ValueError(\'wrong number of inputs\')\n\n        # input activations\n        for i in range(self.ni-1):\n            self.ai[i] = inputs[i]\n\n        # hidden activations\n        for j in range(self.nh):\n            sum_h = 0.0\n            for i in range(self.ni):\n                sum_h += self.ai[i] * self.wi[i][j]\n            self.ah[j] = sigmoid(sum_h)\n\n        # output activations\n        for k in range(self.no):\n            sum_o = 0.0\n            for j in range(self.nh):\n                sum_o += self.ah[j] * self.wo[j][k]\n            self.ao[k] = sigmoid(sum_o)\n\n        return self.ao[:]\n\n\n    def train(self, patterns, iterations=1000, N=0.5, M=0.1):\n        # N: learning rate\n        # M: momentum factor\n        patterns = list(patterns)\n        for i in range(iterations):\n            error = 0.0\n            for p in patterns:\n                inputs = p[0]\n                targets = p[1]\n                self.activate(inputs)\n                error += self.backPropagate([targets], N, M)\n            if i % 5 == 0:\n                print(\'error in interation %d : %-.5f\' % (i,error))\n            print(\'Final training error: %-.5f\' % error)\n\n\nclass Perceptron(object):\n    """"""Perceptron classifier.\n\n    Parameters\n    ------------\n    eta : float\n        Learning rate (between 0.0 and 1.0)\n    n_iter : int\n        Passes over the training dataset.\n\n    Attributes\n    -----------\n    w_ : 1d-array\n        Weights after fitting.\n    errors_ : list\n        Number of misclassifications in every epoch.\n\n    """"""\n    def __init__(self, eta=0.01, n_iter=10):\n        self.eta = eta\n        self.n_iter = n_iter\n\n    def fit(self, X, y):\n        """"""Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        Returns\n        -------\n        self : object\n\n        """"""\n        self.w_ = np.zeros(1 + X.shape[1])\n        self.errors_ = []\n\n        for _ in range(self.n_iter):\n            errors = 0\n            for xi, target in zip(X, y):\n                update = self.eta * (target - self.predict(xi))\n                self.w_[1:] += update * xi\n                self.w_[0] += update\n                errors += int(update != 0.0)\n            self.errors_.append(errors)\n        return self\n\n    def net_input(self, X):\n        """"""Calculate net input""""""\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n\n    def predict(self, X):\n        """"""Return class label after unit step""""""\n        return np.where(self.net_input(X) >= 0.0, 1, -1)\n\n\nclass AdalineGD:\n    """"""ADAptive LInear NEuron classifier.\n\n    Parameters\n    ------------\n    eta : float\n        Learning rate (between 0.0 and 1.0)\n    n_iter : int\n        Passes over the training dataset.\n\n    Attributes\n    -----------\n    w_ : 1d-array\n        Weights after fitting.\n    errors_ : list\n        Number of misclassifications in every epoch.\n\n    """"""\n    def __init__(self, eta=0.01, n_iter=50):\n        self.eta = eta\n        self.n_iter = n_iter\n\n    def fit(self, X, y):\n        """""" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        Returns\n        -------\n        self : object\n        """"""\n        self.w_ = np.zeros(1 + X.shape[1])\n        self.cost_ = []\n\n        for i in range(self.n_iter):\n            output = self.net_input(X)\n            errors = (y - output)\n            self.w_[1:] += self.eta * X.T.dot(errors)\n            self.w_[0] += self.eta * errors.sum()\n            cost = (errors**2).sum() / 2.0\n            self.cost_.append(cost)\n        return self\n\n    def net_input(self, X):\n        """"""Calculate net input""""""\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n\n    def activation(self, X):\n        """"""Compute linear activation""""""\n        return self.net_input(X)\n\n    def predict(self, X):\n        """"""Return class label after unit step""""""\n        return np.where(self.activation(X) >= 0.0, 1, -1)\n\nclass AdalineSGD(object):\n    """"""ADAptive LInear NEuron classifier.\n\n    Parameters\n    ------------\n    eta : float\n        Learning rate (between 0.0 and 1.0)\n    n_iter : int\n        Passes over the training dataset.\n\n    Attributes\n    -----------\n    w_ : 1d-array\n        Weights after fitting.\n    errors_ : list\n        Number of misclassifications in every epoch.\n    shuffle : bool (default: True)\n        Shuffles training data every epoch if True to prevent cycles.\n    random_state : int (default: None)\n        Set random state for shuffling and initializing the weights.\n\n    """"""\n    def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.w_initialized = False\n        self.shuffle = shuffle\n        if random_state:\n            seed(random_state)\n\n    def fit(self, X, y):\n        """""" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        Returns\n        -------\n        self : object\n\n        """"""\n        self._initialize_weights(X.shape[1])\n        self.cost_ = []\n        for i in range(self.n_iter):\n            if self.shuffle:\n                X, y = self._shuffle(X, y)\n            cost = []\n            for xi, target in zip(X, y):\n                cost.append(self._update_weights(xi, target))\n            avg_cost = sum(cost)/len(y)\n            self.cost_.append(avg_cost)\n        return self\n\n    def partial_fit(self, X, y):\n        """"""Fit training data without reinitializing the weights""""""\n        if not self.w_initialized:\n            self._initialize_weights(X.shape[1])\n        if y.ravel().shape[0] > 1:\n            for xi, target in zip(X, y):\n                self._update_weights(xi, target)\n        else:\n            self._update_weights(X, y)\n        return self\n\n    def _shuffle(self, X, y):\n        """"""Shuffle training data""""""\n        r = np.random.permutation(len(y))\n        return X[r], y[r]\n\n    def _initialize_weights(self, m):\n        """"""Initialize weights to zeros""""""\n        self.w_ = np.zeros(1 + m)\n        self.w_initialized = True\n\n    def _update_weights(self, xi, target):\n        """"""Apply Adaline learning rule to update the weights""""""\n        output = self.net_input(xi)\n        error = (target - output)\n        self.w_[1:] += self.eta * xi.dot(error)\n        self.w_[0] += self.eta * error\n        cost = 0.5 * error**2\n        return cost\n\n    def net_input(self, X):\n        """"""Calculate net input""""""\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n\n    def activation(self, X):\n        """"""Compute linear activation""""""\n        return self.net_input(X)\n\n    def predict(self, X):\n        """"""Return class label after unit step""""""\n        return np.where(self.activation(X) >= 0.0, 1, -1)\n'"
3_mage/utils/__init__.py,0,"b'from .plot_2d_separator import plot_2d_separator\nfrom .plot_kneighbors_regularization import plot_kneighbors_regularization, \\\n    plot_regression_datasets, make_dataset\nfrom .plot_linear_svc_regularization import plot_linear_svc_regularization\nfrom .plot_interactive_tree import plot_tree_interactive\nfrom .plot_interactive_forest import plot_forest_interactive\nfrom .plot_rbf_svm_parameters import plot_rbf_svm_parameters\nfrom .plot_rbf_svm_parameters import plot_svm_interactive\n\n__all__ = [\'plot_2d_separator\', \'plot_kneighbors_regularization\',\n           \'plot_linear_svc_regularization\', \'plot_tree_interactive\',\n           \'plot_regression_datasets\', \'make_dataset\',\n           ""plot_forest_interactive"", ""plot_rbf_svm_parameters"",\n           ""plot_svm_interactive""]\n'"
3_mage/utils/plot_2d_separator.py,4,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef plot_2d_separator(classifier, X, fill=False, ax=None, eps=None):\n    if eps is None:\n        eps = X.std() / 2.\n    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n    xx = np.linspace(x_min, x_max, 100)\n    yy = np.linspace(y_min, y_max, 100)\n\n    X1, X2 = np.meshgrid(xx, yy)\n    X_grid = np.c_[X1.ravel(), X2.ravel()]\n    try:\n        decision_values = classifier.decision_function(X_grid)\n        levels = [0]\n        fill_levels = [decision_values.min(), 0, decision_values.max()]\n    except AttributeError:\n        # no decision_function\n        decision_values = classifier.predict_proba(X_grid)[:, 1]\n        levels = [.5]\n        fill_levels = [0, .5, 1]\n\n    if ax is None:\n        ax = plt.gca()\n    if fill:\n        ax.contourf(X1, X2, decision_values.reshape(X1.shape),\n                    levels=fill_levels, colors=[\'blue\', \'red\'])\n    else:\n        ax.contour(X1, X2, decision_values.reshape(X1.shape), levels=levels,\n                   colors=""black"")\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n\nif __name__ == \'__main__\':\n    from sklearn.datasets import make_blobs\n    from sklearn.linear_model import LogisticRegression\n    X, y = make_blobs(centers=2, random_state=42)\n    clf = LogisticRegression().fit(X, y)\n    plot_2d_separator(clf, X, fill=True)\n    plt.scatter(X[:, 0], X[:, 1], c=y)\n    plt.show()\n'"
3_mage/utils/plot_interactive_forest.py,3,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nX, y = make_blobs(centers=[[0, 0], [1, 1]], random_state=61526, n_samples=50)\n\n\ndef plot_forest(max_depth=1):\n    plt.figure()\n    ax = plt.gca()\n    h = 0.02\n\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    if max_depth != 0:\n        forest = RandomForestClassifier(n_estimators=20, max_depth=max_depth,\n                                        random_state=1).fit(X, y)\n        Z = forest.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, alpha=.4)\n        ax.set_title(""max_depth = %d"" % max_depth)\n    else:\n        ax.set_title(""data set"")\n    ax.scatter(X[:, 0], X[:, 1], c=np.array([\'b\', \'r\'])[y], s=60)\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n\ndef plot_forest_interactive():\n    from IPython.html.widgets import interactive, IntSlider\n    slider = IntSlider(min=0, max=8, step=1, value=0)\n    return interactive(plot_forest, max_depth=slider)\n'"
3_mage/utils/plot_interactive_tree.py,5,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.externals.six import StringIO  # doctest: +SKIP\nfrom sklearn.tree import export_graphviz\nfrom scipy import ndimage\ntry: \n    from scipy.misc import imread\nexcept ImportError:\n    from scipy.ndimage import imread\n\nimport re\n\nX, y = make_blobs(centers=[[0, 0], [1, 1]], random_state=61526, n_samples=50)\n\n\ndef tree_image(tree, fout=None):\n    try:\n        import pydot\n    except ImportError:\n        # make a hacky white plot\n        x = np.ones((10, 10))\n        x[0, 0] = 0\n        return x\n    dot_data = StringIO()\n    export_graphviz(tree, out_file=dot_data)\n    data = re.sub(r""gini = 0\\.[0-9]+\\\\n"", """", dot_data.getvalue())\n    data = re.sub(r""samples = [0-9]+\\\\n"", """", data)\n    data = re.sub(r""\\\\nsamples = [0-9]+"", """", data)\n\n    graph = pydot.graph_from_dot_data(data)\n    if fout is None:\n        fout = ""tmp.png""\n    graph.write_png(fout)\n    return imread(fout)\n\n\ndef plot_tree(max_depth=1):\n    fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n    h = 0.02\n\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    if max_depth != 0:\n        tree = DecisionTreeClassifier(max_depth=max_depth, random_state=1).fit(X, y)\n        Z = tree.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n        Z = Z.reshape(xx.shape)\n        faces = tree.tree_.apply(np.c_[xx.ravel(), yy.ravel()].astype(np.float32))\n        faces = faces.reshape(xx.shape)\n        border = ndimage.laplace(faces) != 0\n        ax[0].contourf(xx, yy, Z, alpha=.4)\n        ax[0].scatter(xx[border], yy[border], marker=\'.\', s=1)\n        ax[0].set_title(""max_depth = %d"" % max_depth)\n        ax[1].imshow(tree_image(tree))\n        ax[1].axis(""off"")\n    else:\n        ax[0].set_title(""data set"")\n        ax[1].set_visible(False)\n    ax[0].scatter(X[:, 0], X[:, 1], c=np.array([\'b\', \'r\'])[y], s=60)\n    ax[0].set_xlim(x_min, x_max)\n    ax[0].set_ylim(y_min, y_max)\n    ax[0].set_xticks(())\n    ax[0].set_yticks(())\n\n\ndef plot_tree_interactive():\n    from IPython.html.widgets import interactive, IntSlider\n    slider = IntSlider(min=0, max=8, step=1, value=0)\n    return interactive(plot_tree, max_depth=slider)\n'"
3_mage/utils/plot_kneighbors_regularization.py,9,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\ndef make_dataset(n_samples=100):\n    rnd = np.random.RandomState(42)\n    x = np.linspace(-3, 3, n_samples)\n    y_no_noise = np.sin(4 * x) + x\n    y = y_no_noise + rnd.normal(size=len(x))\n    return x, y\n\n\ndef plot_regression_datasets():\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    for n_samples, ax in zip([10, 100, 1000], axes):\n        x, y = make_dataset(n_samples)\n        ax.plot(x, y, \'o\', alpha=.6)\n\n\ndef plot_kneighbors_regularization():\n    rnd = np.random.RandomState(42)\n    x = np.linspace(-3, 3, 100)\n    y_no_noise = np.sin(4 * x) + x\n    y = y_no_noise + rnd.normal(size=len(x))\n    X = x[:, np.newaxis]\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    x_test = np.linspace(-3, 3, 1000)\n\n    for n_neighbors, ax in zip([2, 5, 20], axes.ravel()):\n        kneighbor_regression = KNeighborsRegressor(n_neighbors=n_neighbors)\n        kneighbor_regression.fit(X, y)\n        ax.plot(x, y_no_noise, label=""true function"")\n        ax.plot(x, y, ""o"", label=""data"")\n        ax.plot(x_test, kneighbor_regression.predict(x_test[:, np.newaxis]),\n                label=""prediction"")\n        ax.legend()\n        ax.set_title(""n_neighbors = %d"" % n_neighbors)\n\nif __name__ == ""__main__"":\n    plot_kneighbors_regularization()\n    plt.show()\n'"
3_mage/utils/plot_linear_svc_regularization.py,1,"b'import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import make_blobs\nfrom .plot_2d_separator import plot_2d_separator\n\n\ndef plot_linear_svc_regularization():\n    X, y = make_blobs(centers=2, random_state=4, n_samples=30)\n    # a carefully hand-designed dataset lol\n    y[7] = 0\n    y[27] = 0\n\n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n\n    for ax, C in zip(axes, [1e-2, 1, 1e2]):\n        ax.scatter(X[:, 0], X[:, 1], s=150, c=np.array([\'red\', \'blue\'])[y])\n\n        svm = SVC(kernel=\'linear\', C=C).fit(X, y)\n        plot_2d_separator(svm, X, ax=ax, eps=.5)\n        ax.set_title(""C = %f"" % C)\n'"
3_mage/utils/plot_rbf_svm_parameters.py,6,"b'import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import make_blobs\nfrom .plot_2d_separator import plot_2d_separator\n\n\ndef make_handcrafted_dataset():\n    # a carefully hand-designed dataset lol\n    X, y = make_blobs(centers=2, random_state=4, n_samples=30)\n    y[np.array([7, 27])] = 0\n    mask = np.ones(len(X), dtype=np.bool)\n    mask[np.array([0, 1, 5, 26])] = 0\n    X, y = X[mask], y[mask]\n    return X, y\n\n\ndef plot_rbf_svm_parameters():\n    X, y = make_handcrafted_dataset()\n\n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n    for ax, C in zip(axes, [1e0, 5, 10, 100]):\n        ax.scatter(X[:, 0], X[:, 1], s=150, c=np.array([\'red\', \'blue\'])[y])\n\n        svm = SVC(kernel=\'rbf\', C=C).fit(X, y)\n        plot_2d_separator(svm, X, ax=ax, eps=.5)\n        ax.set_title(""C = %f"" % C)\n\n    fig, axes = plt.subplots(1, 4, figsize=(15, 3))\n    for ax, gamma in zip(axes, [0.1, .5, 1, 10]):\n        ax.scatter(X[:, 0], X[:, 1], s=150, c=np.array([\'red\', \'blue\'])[y])\n        svm = SVC(gamma=gamma, kernel=\'rbf\', C=1).fit(X, y)\n        plot_2d_separator(svm, X, ax=ax, eps=.5)\n        ax.set_title(""gamma = %f"" % gamma)\n\n\ndef plot_svm(log_C, log_gamma):\n    X, y = make_handcrafted_dataset()\n    C = 10. ** log_C\n    gamma = 10. ** log_gamma\n    svm = SVC(kernel=\'rbf\', C=C, gamma=gamma).fit(X, y)\n    ax = plt.gca()\n    plot_2d_separator(svm, X, ax=ax, eps=.5)\n    # plot data\n    ax.scatter(X[:, 0], X[:, 1], s=150, c=np.array([\'red\', \'blue\'])[y])\n    # plot support vectors\n    sv = svm.support_vectors_\n    ax.scatter(sv[:, 0], sv[:, 1], s=230, facecolors=\'none\', zorder=10, linewidth=3)\n    ax.set_title(""C = %.4f gamma = %.4f"" % (C, gamma))\n\n\ndef plot_svm_interactive():\n    from IPython.html.widgets import interactive, FloatSlider\n    C_slider = FloatSlider(min=-3, max=3, step=.1, value=0, readout=False)\n    gamma_slider = FloatSlider(min=-2, max=2, step=.1, value=0, readout=False)\n    return interactive(plot_svm, log_C=C_slider, log_gamma=gamma_slider)\n'"
