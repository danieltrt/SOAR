file_path,api_count,code
setup.py,0,"b'\xef\xbb\xbffrom setuptools import setup\n\nwith open(\'README.rst\') as readme:\n    long_description = readme.read()\n\nsetup(\n    name=\'PeakUtils\',\n    version=\'1.3.3\',\n    description=\'Peak detection utilities for 1D data\',\n    long_description=long_description,\n    long_description_content_type=""text/x-rst"",\n    author=\'Lucas Hermann Negri\',\n    author_email=\'lucashnegri@gmail.com\',\n    url=\'https://bitbucket.org/lucashnegri/peakutils\',\n    packages=[\'peakutils\'],\n    install_requires=[\'numpy\', \'scipy\'],\n    tests_require=[\'pandas\'],\n    classifiers=[\n        \'Development Status :: 5 - Production/Stable\',\n        \'License :: OSI Approved :: MIT License\',\n        \'Programming Language :: Python\',\n        \'Programming Language :: Python :: 3\',\n        \'Topic :: Scientific/Engineering\'\n    ],\n    license=\'MIT\',\n    test_suite=\'tests\',\n    keywords=\'peak detection search gaussian centroid baseline maximum\',\n)\n'"
docs/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n\nimport peakutils\nfrom better import better_theme_path\n\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.napoleon\'\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix of source filenames.\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'PeakUtils\'\ncopyright = \'2014 - 2020, Lucas Hermann Negri\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = peakutils.__version__\nrelease = version\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [\'_build\']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\nadd_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n#keep_warnings = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\nhtml_theme_path = [better_theme_path]\nhtml_theme = \'better\'\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#html_extra_path = []\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'PeakUtilsdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n# The paper size (\'letterpaper\' or \'a4paper\').\n#\'papersize\': \'letterpaper\',\n\n# The font size (\'10pt\', \'11pt\' or \'12pt\').\n#\'pointsize\': \'10pt\',\n\n# Additional stuff for the LaTeX preamble.\n#\'preamble\': \'\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n  (\'index\', \'PeakUtils.tex\', \'PeakUtils Documentation\',\n   \'Lucas Hermann Negri\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (\'index\', \'peakutils\', \'PeakUtils Documentation\',\n     [\'Lucas Hermann Negri\'], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n  (\'index\', \'PeakUtils\', \'PeakUtils Documentation\',\n   \'Lucas Hermann Negri\', \'PeakUtils\', \'Peak detection utilities\',\n   \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#texinfo_no_detailmenu = False\n'"
peakutils/__init__.py,0,"b""from .baseline import *\nfrom .peak import *\nfrom .prepare import *\n\n__version__ = '1.3.3'\n"""
peakutils/baseline.py,6,"b'""""""Baseline estimation algorithms.""""""\n\nimport numpy as np\nimport scipy.linalg as LA\nimport math\n\n\ndef baseline(y, deg=None, max_it=None, tol=None):\n    """"""\n    Computes the baseline of a given data.\n\n    Iteratively performs a polynomial fitting in the data to detect its\n    baseline. At every iteration, the fitting weights on the regions with\n    peaks are reduced to identify the baseline only.\n\n    Parameters\n    ----------\n    y : ndarray\n        Data to detect the baseline.\n    deg : int (default: 3)\n        Degree of the polynomial that will estimate the data baseline. A low\n        degree may fail to detect all the baseline present, while a high\n        degree may make the data too oscillatory, especially at the edges.\n    max_it : int (default: 100)\n        Maximum number of iterations to perform.\n    tol : float (default: 1e-3)\n        Tolerance to use when comparing the difference between the current\n        fit coefficients and the ones from the last iteration. The iteration\n        procedure will stop when the difference between them is lower than\n        *tol*.\n\n    Returns\n    -------\n    ndarray\n        Array with the baseline amplitude for every original point in *y*\n    """"""\n    # for not repeating ourselves in `envelope`\n    if deg is None: deg = 3\n    if max_it is None: max_it = 100\n    if tol is None: tol = 1e-3\n    \n    order = deg + 1\n    coeffs = np.ones(order)\n\n    # try to avoid numerical issues\n    cond = math.pow(abs(y).max(), 1. / order)\n    x = np.linspace(0., cond, y.size)\n    base = y.copy()\n\n    vander = np.vander(x, order)\n    vander_pinv = LA.pinv2(vander)\n\n    for _ in range(max_it):\n        coeffs_new = np.dot(vander_pinv, y)\n\n        if LA.norm(coeffs_new - coeffs) / LA.norm(coeffs) < tol:\n            break\n\n        coeffs = coeffs_new\n        base = np.dot(vander, coeffs)\n        y = np.minimum(y, base)\n\n    return base\n\ndef envelope(y, deg=None, max_it=None, tol=None):\n    """"""\n    Computes the upper envelope of a given data.\n    It is implemented in terms of the `baseline` function.\n    \n    Parameters\n    ----------\n    y : ndarray\n        Data to detect the baseline.\n    deg : int\n        Degree of the polynomial that will estimate the envelope.\n    max_it : int\n        Maximum number of iterations to perform.\n    tol : float\n        Tolerance to use when comparing the difference between the current\n        fit coefficients and the ones from the last iteration.\n\n    Returns\n    -------\n    ndarray\n        Array with the envelope amplitude for every original point in *y*\n    """"""\n    return y.max() - baseline(y.max() - y, deg, max_it, tol)\n'"
peakutils/peak.py,24,"b'""""""Peak detection algorithms.""""""\n\nimport warnings\n\nimport numpy as np\nfrom scipy import optimize\nfrom scipy.integrate import simps\n\neps = np.finfo(float).eps\n\n\ndef indexes(y, thres=0.3, min_dist=1, thres_abs=False):\n    """"""Peak detection routine.\n\n    Finds the numeric index of the peaks in *y* by taking its first order difference. By using\n    *thres* and *min_dist* parameters, it is possible to reduce the number of\n    detected peaks. *y* must be signed.\n\n    Parameters\n    ----------\n    y : ndarray (signed)\n        1D amplitude data to search for peaks.\n    thres : float between [0., 1.]\n        Normalized threshold. Only the peaks with amplitude higher than the\n        threshold will be detected.\n    min_dist : int\n        Minimum distance between each detected peak. The peak with the highest\n        amplitude is preferred to satisfy this constraint.\n    thres_abs: boolean\n        If True, the thres value will be interpreted as an absolute value, instead of\n        a normalized threshold.\n\n    Returns\n    -------\n    ndarray\n        Array containing the numeric indexes of the peaks that were detected.\n        When using with Pandas DataFrames, iloc should be used to access the values at the returned positions.\n    """"""\n    if isinstance(y, np.ndarray) and np.issubdtype(y.dtype, np.unsignedinteger):\n        raise ValueError(""y must be signed"")\n\n    if not thres_abs:\n        thres = thres * (np.max(y) - np.min(y)) + np.min(y)\n\n    min_dist = int(min_dist)\n\n    # compute first order difference\n    dy = np.diff(y)\n\n    # propagate left and right values successively to fill all plateau pixels (0-value)\n    zeros, = np.where(dy == 0)\n\n    # check if the signal is totally flat\n    if len(zeros) == len(y) - 1:\n        return np.array([])\n\n    if len(zeros):\n        # compute first order difference of zero indexes\n        zeros_diff = np.diff(zeros)\n        # check when zeros are not chained together\n        zeros_diff_not_one, = np.add(np.where(zeros_diff != 1), 1)\n        # make an array of the chained zero indexes\n        zero_plateaus = np.split(zeros, zeros_diff_not_one)\n\n        # fix if leftmost value in dy is zero\n        if zero_plateaus[0][0] == 0:\n            dy[zero_plateaus[0]] = dy[zero_plateaus[0][-1] + 1]\n            zero_plateaus.pop(0)\n\n        # fix if rightmost value of dy is zero\n        if len(zero_plateaus) and zero_plateaus[-1][-1] == len(dy) - 1:\n            dy[zero_plateaus[-1]] = dy[zero_plateaus[-1][0] - 1]\n            zero_plateaus.pop(-1)\n\n        # for each chain of zero indexes\n        for plateau in zero_plateaus:\n            median = np.median(plateau)\n            # set leftmost values to leftmost non zero values\n            dy[plateau[plateau < median]] = dy[plateau[0] - 1]\n            # set rightmost and middle values to rightmost non zero values\n            dy[plateau[plateau >= median]] = dy[plateau[-1] + 1]\n\n    # find the peaks by using the first order difference\n    peaks = np.where(\n        (np.hstack([dy, 0.0]) < 0.0)\n        & (np.hstack([0.0, dy]) > 0.0)\n        & (np.greater(y, thres))\n    )[0]\n\n    # handle multiple peaks, respecting the minimum distance\n    if peaks.size > 1 and min_dist > 1:\n        highest = peaks[np.argsort(y[peaks])][::-1]\n        rem = np.ones(y.size, dtype=bool)\n        rem[peaks] = False\n\n        for peak in highest:\n            if not rem[peak]:\n                sl = slice(max(0, peak - min_dist), peak + min_dist + 1)\n                rem[sl] = True\n                rem[peak] = False\n\n        peaks = np.arange(y.size)[~rem]\n\n    return peaks\n\n\ndef centroid(x, y):\n    """"""Computes the centroid for the specified data.\n    Refer to centroid2 for a more complete, albeit slower version.\n\n    Parameters\n    ----------\n    x : ndarray\n        Data on the x axis.\n    y : ndarray\n        Data on the y axis.\n\n    Returns\n    -------\n    float\n        Centroid of the data.\n    """"""\n    return np.sum(x * y) / np.sum(y)\n\n\ndef centroid2(y, x=None, dx=1.0):\n    """"""Computes the centroid for the specified data.\n    Not intended to be used\n\n    Parameters\n    ----------\n    y : array_like\n        Array whose centroid is to be calculated.\n    x : array_like, optional\n        The points at which y is sampled.\n    Returns\n    -------\n    (centroid, sd)\n        Centroid and standard deviation of the data.\n    """"""\n    yt = np.array(y)\n\n    if x is None:\n        x = np.arange(yt.size, dtype=""float"") * dx\n\n    normaliser = simps(yt, x)\n    centroid = simps(x * yt, x) / normaliser\n    var = simps((x - centroid) ** 2 * yt, x) / normaliser\n    return centroid, np.sqrt(var)\n\n\ndef gaussian(x, ampl, center, dev):\n    """"""Computes the Gaussian function.\n\n    Parameters\n    ----------\n    x : number\n        Point to evaluate the Gaussian for.\n    a : number\n        Amplitude.\n    b : number\n        Center.\n    c : number\n        Width.\n\n    Returns\n    -------\n    float\n        Value of the specified Gaussian at *x*\n    """"""\n    return ampl * np.exp(-(x - float(center)) ** 2 / (2.0 * dev ** 2 + eps))\n\n\ndef gaussian_fit(x, y, center_only=True):\n    """"""Performs a Gaussian fitting of the specified data.\n\n    Parameters\n    ----------\n    x : ndarray\n        Data on the x axis.\n    y : ndarray\n        Data on the y axis.\n    center_only: bool\n        If True, returns only the center of the Gaussian for `interpolate` compatibility\n\n    Returns\n    -------\n    ndarray or float\n        If center_only is `False`, returns the parameters of the Gaussian that fits the specified data\n        If center_only is `True`, returns the center position of the Gaussian\n    """"""\n    if len(x) < 3:\n        # used RuntimeError to match errors raised in scipy.optimize\n        raise RuntimeError(""At least 3 points required for Gaussian fitting"")\n\n    initial = [np.max(y), x[0], (x[1] - x[0]) * 5]\n    params, pcov = optimize.curve_fit(gaussian, x, y, initial)\n\n    if center_only:\n        return params[1]\n    else:\n        return params\n\n\ndef interpolate(x, y, ind=None, width=10, func=gaussian_fit):\n    """"""Tries to enhance the resolution of the peak detection by using\n    Gaussian fitting, centroid computation or an arbitrary function on the\n    neighborhood of each previously detected peak index.\n    \n    RuntimeErrors raised in the fitting function will be converted to warnings, with the peak\n    being mantained as the original one (in the ind array).\n\n    Parameters\n    ----------\n    x : ndarray\n        Data on the x dimension.\n    y : ndarray\n        Data on the y dimension.\n    ind : ndarray\n        Indexes of the previously detected peaks. If None, indexes() will be\n        called with the default parameters.\n    width : int\n        Number of points (before and after) each peak index to pass to *func*\n        in order to increase the resolution in *x*.\n    func : function(x,y)\n        Function that will be called to detect an unique peak in the x,y data.\n\n    Returns\n    -------\n    ndarray :\n        Array with the adjusted peak positions (in *x*)\n    """"""\n    assert x.shape == y.shape\n\n    if ind is None:\n        ind = indexes(y)\n\n    out = []\n\n    for i in ind:\n        slice_ = slice(i - width, i + width + 1)\n\n        try:\n            best_idx = func(x[slice_], y[slice_])\n        except RuntimeError as e:\n            warnings.warn(str(e))\n            best_idx = i\n\n        out.append(best_idx)\n\n    return np.array(out)\n'"
peakutils/plot.py,0,"b'import matplotlib.pyplot as plt\n\n\ndef plot(x, y, ind):\n    """"""\n    Plots the original data with the peaks that were identified\n\n    Parameters\n    ----------\n    x : array-like\n        Data on the x-axis\n    y : array-like\n        Data on the y-axis\n    ind : array-like\n        Indexes of the identified peaks\n    """"""\n    plt.plot(x, y, ""--"")\n\n    marker_x = x.iloc[ind] if hasattr(x, ""iloc"") else x[ind]\n    marker_y = y.iloc[ind] if hasattr(y, ""iloc"") else y[ind]\n\n    plt.plot(marker_x, marker_y, ""r+"", ms=5, mew=2, label=""{} peaks"".format(len(ind)))\n    plt.legend()\n'"
peakutils/prepare.py,1,"b'""""""Data preparation / preprocessing algorithms.""""""\n\nimport numpy as np\n\n\ndef scale(x, new_range=(0., 1.), eps=1e-9):\n    """"""Changes the scale of an array\n\n    Parameters\n    ----------\n    x : ndarray\n        1D array to change the scale (remains unchanged)\n    new_range : tuple (float, float)\n        Desired range of the array\n    eps: float\n        Numerical precision, to detect degenerate cases (for example, when\n        every value of *x* is equal)\n\n    Returns\n    -------\n    ndarray\n        Scaled array\n    tuple (float, float)\n        Previous data range, allowing a rescale to the old range\n    """"""\n    assert new_range[1] >= new_range[0]\n    range_ = (x.min(), x.max())\n\n    if (range_[1] - range_[0]) < eps:\n        mean = (new_range[0] + new_range[1]) / 2.0\n        xp = np.full(x.shape, mean)\n    else:\n        xp = (x - range_[0])\n        xp *= (new_range[1] - new_range[0]) / (range_[1] - range_[0])\n        xp += new_range[0]\n\n    return xp, range_\n'"
tests/__init__.py,0,b''
tests/peakutils_test.py,26,"b'import os\nimport unittest\nimport warnings\nimport itertools\n\nimport numpy\nfrom numpy.testing import assert_array_almost_equal\nimport scipy.signal\nimport numpy as np\nimport pandas as pd\nimport peakutils\n\ndef load(name):\n    p = os.path.join(os.path.dirname(__file__), name)\n    return numpy.loadtxt(p)\n\n\nclass LPGPeaks(unittest.TestCase):\n\n    """"""Tests with experimental data from long period gratings""""""\n\n    def test_peaks(self):\n        y = load(\'noise\')[:, 1]\n        filtered = scipy.signal.savgol_filter(y, 21, 1)\n        n_peaks = 8\n\n        idx = peakutils.indexes(filtered, thres=0.08, min_dist=50)\n\n        for p in range(idx.size, 1):\n            self.assertGreater(idx[p], 0)\n            self.assertLess(idx[p], idx.size - 1)\n            self.assertGreater(idx[p], idx[p - 1])\n\n        self.assertEqual(idx.size, n_peaks)\n\n\nclass FBGPeaks(unittest.TestCase):\n\n    """"""Tests with experimental data from fiber Bragg gratings""""""\n\n    def test_peaks(self):\n        data = load(\'baseline\')\n        x, y = data[:, 0], data[:, 1]\n        n_peaks = 2\n\n        prepared = y - peakutils.baseline(y, 3)\n        idx = peakutils.indexes(prepared, thres=0.03, min_dist=5)\n\n        for p in range(idx.size, 1):\n            self.assertGreater(idx[p], 0)\n            self.assertLess(idx[p], idx.size - 1)\n            self.assertGreater(idx[p], idx[p - 1])\n\n        self.assertEqual(idx.size, n_peaks)\n        assert_array_almost_equal(x[idx], numpy.array([1527.3, 1529.77]))\n\n\nclass SimulatedData(unittest.TestCase):\n\n    """"""Tests with simulated data""""""\n\n    def setUp(self):\n        self.near = numpy.array([0, 1, 0, 2, 0, 3, 0, 2, 0, 1, 0])\n\n    def aux_test_peaks(self, dtype):\n        """"""(3 peaks + baseline + noise)""""""\n        x = numpy.linspace(0, 100, 1000)\n        centers = (20, 40, 70)\n        y = (1000 * (peakutils.gaussian(x, 1, centers[0], 3) +\n             peakutils.gaussian(x, 2, centers[1], 5) +\n             peakutils.gaussian(x, 3, centers[2], 1) +\n             numpy.random.random(x.size) * 0.2)).astype(dtype)\n\n        filtered = scipy.signal.savgol_filter(y, 51, 3).astype(dtype)\n        \n        idx = peakutils.indexes(filtered, thres=0.3, min_dist=100)\n        peaks = peakutils.interpolate(x, y, idx, width=30)\n        self.assertEqual(idx.size, len(centers))\n        self.assertEqual(peaks.size, len(centers))\n\n        # interpolation should work!\n        for i in range(peaks.size):\n            self.assertAlmostEqual(peaks[i], centers[i], delta=0.5)\n\n    def test_peaks(self):\n        self.aux_test_peaks(\'float64\')\n        self.aux_test_peaks(\'float32\')\n        self.aux_test_peaks(\'int32\')\n        self.assertRaises(ValueError, self.aux_test_peaks, \'uint32\')\n\n    def test_near_peaks1(self):\n        out = peakutils.indexes(self.near, thres=0, min_dist=2)\n        expected = numpy.array([1, 5, 9])\n        assert_array_almost_equal(out, expected)\n\n    def test_near_peaks2(self):\n        out = peakutils.indexes(self.near, thres=0, min_dist=1)\n        expected = numpy.array([1, 3, 5, 7, 9])\n        assert_array_almost_equal(out, expected)\n\n    def test_list_peaks(self):\n        out = peakutils.indexes([1, 2, 1, 3, 5, 7, 4, 1], thres=0, min_dist=1)\n        expected = numpy.array([1, 5])\n        assert_array_almost_equal(out, expected)\n\n    def test_pandas_series(self):\n        x = [""a"", ""b"", ""c"", ""d"", ""e""]\n        y = [  0,   2,   0,  3,   0 ]\n        data = pd.Series(data=y, index=x)\n        out = peakutils.indexes(data, thres=0, min_dist=1)\n        expected = numpy.array([1, 3])\n        assert_array_almost_equal(out, expected)\n    \n    def test_absolute_threshold(self):\n        x = [0, 5, 0, 8, 0, 15, 0]\n        out1 = peakutils.indexes(x, thres=3, thres_abs=True)\n        assert_array_almost_equal(out1, [1, 3, 5])\n\n        out2 = peakutils.indexes(x, thres=5, thres_abs=True)\n        assert_array_almost_equal(out2, [3, 5])\n\n        out3 = peakutils.indexes(x, thres=7, thres_abs=True)\n        assert_array_almost_equal(out3, [3, 5])\n\n        out4 = peakutils.indexes(x, thres=14, thres_abs=True)\n        assert_array_almost_equal(out4, [5])\n\n        out5 = peakutils.indexes(x, thres=15, thres_abs=True)\n        assert_array_almost_equal(out5, [])\n\n        out6 = peakutils.indexes(x, thres=16, thres_abs=True)\n        assert_array_almost_equal(out6, [])\n\nclass Baseline(unittest.TestCase):\n\n    """"""Tests the conditioning of the lsqreg in the implementation""""""\n\n    def test_conditioning(self):\n        data = data = load(\'exp\')\n        y = data[:, 1]\n        mult = 1e-6\n\n        while mult < 100001:\n            ny = y * mult\n            base = peakutils.baseline(ny, 9) / mult\n            self.assertTrue(0.8 < base.max() < 1.0)\n            self.assertTrue(-0.1 <= base.min() < 0.1)\n            mult *= 10\n\n    def test_negative(self):\n        data = np.array([-1, -2, -3, -4, -3, -2, -1] * 10)\n        base = peakutils.baseline(data)\n        self.assertEqual(data.shape, base.shape)\n\nclass Prepare(unittest.TestCase):\n\n    """"""Tests the prepare module""""""\n\n    def test_scale(self):\n        orig = numpy.array([-2, -1, 0.5, 1, 3])\n        x1, range_old = peakutils.scale(orig, (-10, 8))\n        x2, range_new = peakutils.scale(x1, range_old)\n\n        assert_array_almost_equal(orig, x2)\n        self.assertTupleEqual(range_new, (-10, 8))\n\n    def test_scale_degenerate(self):\n        orig = numpy.array([-3, -3, -3])\n        x1, range_old = peakutils.scale(orig, (5, 7))\n        x2, range_new = peakutils.scale(x1, range_old)\n\n        assert_array_almost_equal(x1, [6, 6, 6])\n        assert_array_almost_equal(x2, orig)\n\nclass Centroid(unittest.TestCase):\n\n    """"""Tests the centroid implementations.""""""\n\n    def test_centroid(self):\n        y = np.ones(10)\n        x = np.arange(10)\n        self.assertEqual(peakutils.centroid(x, y), 4.5)\n\n    def test_centroid2(self):\n        y = np.ones(3)\n        x = np.array([0., 1., 9.])\n        c, v = peakutils.centroid2(y, x)\n        self.assertEqual(c, 4.5)\n\nclass GaussianFit(unittest.TestCase):\n\n    """""" Tests the Gaussian fit implementation """"""\n\n    def test_gaussian_fit(self):\n        params = np.array([0.5, 6, 2])\n        x = np.arange(10)\n        y = peakutils.gaussian(x, *params)\n        self.assertAlmostEqual(peakutils.gaussian_fit(x, y), params[1])\n\n        res = peakutils.gaussian_fit(x, y, center_only=False)\n        np.testing.assert_allclose(res, params)\n\nclass Plateau(unittest.TestCase):\n\n    """"""Issue #4""""""\n\n    def test_plateau1(self):\n        y = np.zeros(20)\n        y[1:6] = 1.0\n        y[8:9] = 2.0\n        y[11:19] = 3.0\n        idx = peakutils.indexes(y)\n        np.testing.assert_array_equal(idx, [3, 8, 14])\n\n    def test_plateau2(self):\n        y = np.zeros(20)\n        y[0:6] = 1.0\n        y[8:9] = 2.0\n        y[11:20] = 3.0\n        idx = peakutils.indexes(y)\n        np.testing.assert_array_equal(idx, [8])\n        # note: there are no peaks in the first and last series as the data\n        # to the left of 0 and right of 19 is unknown\n        \n    def test_flat(self):\n        ra = (0.2, 0.4, 0.6, 0.8, 0.95)\n        rb = (1, 2, 3, 4, 5, 6)\n        N = 20\n        \n        # all equal\n        for t, m in itertools.product(ra, rb):\n            y = np.ones(N)\n            peakutils.indexes(y, thres=t, min_dist=m)\n            \n        # a single peak\n        for t, m in itertools.product(ra, rb):\n            for z in range(m + 1, N - m):\n                y = np.ones(N)\n                y[z] = 1e3\n                p = peakutils.indexes(y, thres=t, min_dist=m)\n                self.assertEqual(p, np.array([z]))\n\nclass Float64(unittest.TestCase):\n\n    """"""Issue #11 (false alarm)""""""\n    def setUp(self):\n        self.col = [\n            u\'2161\', u\'183\', u\'167\', u\'270\', u\'164\', u\'475\', u\'327\', u\'279\', u\'0\', \n            u\'183\', u\'360\', u\'81\', u\'81\', u\'81\', u\'81\', u\'45\', u\'81\', u\'0\', u\'81\', u\'81\'\n        ]\n\n    def test_int_high_thres(self):\n        y = np.atleast_1d(self.col).astype(\'int\')\n        peaks = peakutils.indexes(y, thres=0.3)\n        np.testing.assert_array_almost_equal(peaks, [])\n\n    def test_float64_high_thres(self):\n        y = np.atleast_1d(self.col).astype(\'float64\')\n        peaks = peakutils.indexes(y, thres=0.3)\n        np.testing.assert_array_almost_equal(peaks, [])\n        \n    def test_int_low_thres(self):\n        y = np.atleast_1d(self.col).astype(\'int\')\n        peaks = peakutils.indexes(y, thres=0.01)\n        np.testing.assert_array_almost_equal(peaks, [3, 5, 10, 16])\n\n    def test_float64_low_thres(self):\n        y = np.atleast_1d(self.col).astype(\'float64\')\n        peaks = peakutils.indexes(y, thres=0.01)\n        np.testing.assert_array_almost_equal(peaks, [3, 5, 10, 16])\n\nclass InterpolateExceptions(unittest.TestCase):\n    \n    """""" Issue #14: convert fitting errors to warnings """"""\n    def test_interpolate_bounds(self):\n        x = np.arange(5)\n        y = np.array([0, 0, 1, 0, 0])\n        \n        with warnings.catch_warnings(record=True) as record:\n            for w in range(1, 10):\n                peakutils.interpolate(x, y, [2], width=w)\n        \n        self.assertGreater(len(record), 0)\n        \nclass HighEnvelope(unittest.TestCase):\n    \n    def test_up_envelope(self):\n        data = np.array([0, 2, 0, 0, 4, 0, 0, 0, 7, 0, 0, 0, 0, 9, 0, 0, 0, 11, 0, 0, 0, 0, 0, 9, 0,\n                         0, 0, 0, 7, 0, 0, 4, 0, 0, 3, 0, 0, 0, 1])\n        env = peakutils.envelope(data, 5)\n        tol = 1.05\n        \n        for a, b in zip(data, env):\n            self.assertLess(a, b * tol)\n\nif __name__ == \'__main__\':\n    numpy.random.seed(1997)\n    unittest.main()\n'"
tests/perf.py,4,"b'import numpy as np\nimport peakutils\nfrom timeit import default_timer as timer\n\n\ndef make_data(n, noise):\n    x = np.linspace(0, 100, n)\n    y = peakutils.gaussian(x, 5, 20, 10) + \\\n        peakutils.gaussian(x, 8, 70, 3) + \\\n        noise * np.random.rand(x.size) + np.polyval([3, 2], x)\n    return x, y\n\n\ndef benchit():\n    tests = [(""Small - Low noise"",  make_data(200, 1.), 100),\n             (""Small - High noise"", make_data(200, 3.), 100),\n             (""Big - Low noise"",    make_data(20000, 1), 5),\n             (""Big - High noise"",   make_data(20000, 2.), 5),\n             (""Plateaus"", (0, np.insert(np.ones(20000-3),\n                                        [1000, 10000, -1000], 3)), 5)]\n    for name, data, rep in tests:\n        begin = timer()\n\n        for _ in range(rep):\n            y = data[1] - peakutils.baseline(data[1])\n            if name is ""Plateaus"": y = data[1]\n            i = peakutils.indexes(y, thres=0.4, min_dist=y.size // 5)\n\n        end = timer()\n        each = (end - begin) / rep\n        print(""*{}* test took {:.3f} seconds each rep"".format(name, each))\n\n\nif __name__ == \'__main__\':\n    np.random.seed(1997)\n    benchit()\n'"
