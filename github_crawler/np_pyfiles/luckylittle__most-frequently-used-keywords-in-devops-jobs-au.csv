file_path,api_count,code
devops_jobs_keywords.py,0,"b""#!/bin/python3\n\nimport requests # used to grab data from the web\nfrom bs4 import BeautifulSoup # used to parse HTML\nfrom sklearn.feature_extraction.text import CountVectorizer # used to count number of words and phrases\n\ntexts = [] # initializing texts that will hold our job descriptions in this list\n\nfor index in range(0,1000,10): # go through 100 pages of Australian indeed\n    page = 'http://au.indeed.com/jobs?q=devops&start='+str(index) # identify the url of the job listings\n    web_result = requests.get(page).text # use requests to actually visit the url\n    soup = BeautifulSoup(web_result) # parse the html of the resulting page\n    for listing in soup.findAll('div', {'class':'summary'}): # for each listing on the page\n        texts.append(listing.text) # append the text of the listing to our list\n\ntype(texts) # == list\nvect = CountVectorizer(ngram_range=(1,2), stop_words='english') # get basic counts of one and two word phrases\nmatrix = vect.fit_transform(texts) # fit and learn to the vocabulary in the corpus\nprint(len(vect.get_feature_names())) # how many features are there\n\nfreqs = [(word, matrix.getcol(idx).sum()) for word, idx in vect.vocabulary_.items()] #sort from largest to smallest\nfor phrase, times in sorted (freqs, key = lambda x: -x[1])[:25]:\n    print(phrase, times) # print most used keywords and their frequency\n"""
solution_architect_jobs_keywords.py,0,"b""#!/bin/python3\n\nimport requests # used to grab data from the web\nfrom bs4 import BeautifulSoup # used to parse HTML\nfrom sklearn.feature_extraction.text import CountVectorizer # used to count number of words and phrases\n\ntexts = [] # initializing texts that will hold our job descriptions in this list\n\nfor index in range(0,1000,10): # go through 100 pages of Australian indeed\n    page = 'http://au.indeed.com/jobs?q=solution+architect&start='+str(index) # identify the url of the job listings\n    web_result = requests.get(page).text # use requests to actually visit the url\n    soup = BeautifulSoup(web_result, 'html.parser') # parse the html of the resulting page\n    for listing in soup.findAll('div', {'class':'summary'}): # for each listing on the page\n        texts.append(listing.text) # append the text of the listing to our list\n\ntype(texts) # == list\nvect = CountVectorizer(ngram_range=(1,2), stop_words='english') # get basic counts of one and two word phrases\nmatrix = vect.fit_transform(texts) # fit and learn to the vocabulary in the corpus\nprint(len(vect.get_feature_names())) # how many features are there\n\nfreqs = [(word, matrix.getcol(idx).sum()) for word, idx in vect.vocabulary_.items()] #sort from largest to smallest\nfor phrase, times in sorted (freqs, key = lambda x: -x[1])[:25]:\n    print(phrase, times) # print most used keywords and their frequency\n"""
