file_path,api_count,code
maml.py,56,"b'#!/usr/bin/env python3\nimport os\nimport pickle\nimport copy\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl; \nmpl.rcParams[""savefig.directory""] = \'~/Desktop\'#$os.chdir(os.path.dirname(__file__))\nimport argparse\nfrom collections import defaultdict\n\nfrom utils.optim import AdamOptimizer\nfrom utils.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array, rel_error\nfrom utils.data_generator import SinusoidGenerator\n\n""""""\nThis file contains logic for training a fully-connected neural network with\n2 hidden layers using the Model-Agnostic Meta-Learning (MAML) algorithm.\n\nIt is designed to solve the toy sinusoid meta-learning problem presented in the MAML paper, \nand uses the same architecture as presented in the paper.\n\nPassing the `--gradcheck=1` flag, will run finite differences gradient check\non the meta forward and backward to ensure correct implementation.\n\nAfter training a network, you can pass the `--test=1` flag to compare against\na joint-trained and random network baseline.\n""""""\n\n\n# special dictionary to return 0 if element does not exist (makes gradient code simpler)\nGradDict = lambda: defaultdict(lambda: 0) \nnormalize = lambda x: (x - x.mean()) / (x.std() + 1e-8)\n\n# weight util functions\ndef build_weights(hidden_dims=(64, 64)):\n    """"""Return dictionary on neural network weights""""""\n    # Initialize all weights (model params) with ""Xavier Initialization"" \n    # weight matrix init = uniform(-1, 1) / sqrt(layer_input)\n    # bias init = zeros()\n    H1, H2 = hidden_dims\n    w = {}\n    w[\'W1\'] = (-1 + 2*np.random.rand(1, H1)) / np.sqrt(1)\n    w[\'b1\'] = np.zeros(H1)\n    w[\'W2\'] = (-1 + 2*np.random.rand(H1, H2)) / np.sqrt(H1)\n    w[\'b2\'] = np.zeros(H2)\n    w[\'W3\'] = (-1 + 2*np.random.rand(H2, 1)) / np.sqrt(H2)\n    w[\'b3\'] = np.zeros(1)\n\n    # Cast all parameters to the correct datatype\n    for k, v in w.items():\n        w[k] = v.astype(np.float32)\n    return w\n\ndef save_weights(weights, filename, quiet=False):\n    with open(filename, \'wb\') as f:\n        pickle.dump(weights, f)\n    if not quiet:\n        print(\'weights saved to {}\'.format(filename))\n\ndef load_weights(filename, quiet=False):\n    with open(filename, \'rb\') as f:\n        weights = pickle.load(f)\n    if not quiet:\n        print(\'weights loaded from {}\'.format(filename))\n    return weights\n\nclass Network(object):\n    """"""\n    Forward and backward pass logic for 3 layer neural network\n    (see https://github.com/matwilso/maml_numpy#derivation for derivation)\n    """"""\n\n    def __init__(self, inner_lr=0.01, normalize=normalize):\n        self.inner_lr = inner_lr  # alpha in the paper\n        self.normalize = normalize  # function to normalize gradients before applying them to weights (helps with stability)\n\n    def inner_forward(self, x_a, weights, cache={}):\n        """"""Submodule for meta_forward. This is just a standard forward pass for a neural net.\n\n        Args:\n            x_a (ndarray): Example or examples of sinusoid from given phase, amplitude.  \n            weights (dict): Dictionary of weights and biases for neural net\n            cache (dict): Pass in dictionary to be updated with values needed in meta_backward\n\n        Returns:\n            pred_a (ndarray): Predicted values for example(s) x_a\n        """"""\n        w = weights\n        W1, b1, W2, b2, W3, b3 = w[\'W1\'], w[\'b1\'], w[\'W2\'], w[\'b2\'], w[\'W3\'], w[\'b3\']\n        # layer 1\n        affine1_a = x_a.dot(W1) + b1\n        relu1_a = np.maximum(0, affine1_a)\n        # layer 2\n        affine2_a = relu1_a.dot(W2) + b2 \n        relu2_a = np.maximum(0, affine2_a)\n        # layer 3\n        pred_a = relu2_a.dot(W3) + b3\n\n        cache.update(dict(x_a=x_a, affine1_a=affine1_a, relu1_a=relu1_a, affine2_a=affine2_a, relu2_a=relu2_a))\n        return pred_a\n\n    def inner_backward(self, dout_a, weights, cache, grads=GradDict(), lr=None):\n        """"""For fine-tuning network at meta-test time\n\n        (Although this has some repeated code from meta_backward, it was hard to \n        use as a subprocess for meta_backward.  It required several changes in \n        code and made things more confusing.)\n\n        Args:\n            dout_a (ndarray): Gradient of output (usually loss)\n            weights (dict): Dictionary of weights and biases for neural net\n            cache (dict): Dictionary of relevant values from forward pass\n\n        Returns:\n            dict: New dictionary, with updated weights\n        """"""\n        w = weights; c = cache\n        W1, b1, W2, b2, W3, b3 = w[\'W1\'], w[\'b1\'], w[\'W2\'], w[\'b2\'], w[\'W3\'], w[\'b3\']\n        lr = lr or self.inner_lr\n\n        drelu2_a = dout_a.dot(W3.T)\n        dW3 = c[\'relu2_a\'].T.dot(dout_a)\n        db3 = np.sum(dout_a, axis=0)\n\n        daffine2_a = np.where(c[\'affine2_a\'] > 0, drelu2_a, 0)\n\n        drelu1_a = daffine2_a.dot(W2.T)\n        dW2 = c[\'relu1_a\'].T.dot(dout_a)\n        db2 = np.sum(dout_a, axis=0)\n\n        daffine1_a = np.where(c[\'affine1_a\'] > 0, drelu1_a, 0)\n\n        dW1 = c[\'x_a\'].T.dot(daffine1_a)\n        db1 = np.sum(daffine1_a, axis=0)\n\n        grads[\'W1\'] += dW1\n        grads[\'b1\'] += db1\n        grads[\'W2\'] += dW2\n        grads[\'b2\'] += db2\n        grads[\'W3\'] += dW3\n        grads[\'b3\'] += db3\n\n        # Return new weights (for fine-tuning)\n        new_weights = {}\n        new_weights[\'W1\'] = W1 - lr*self.normalize(dW1)\n        new_weights[\'b1\'] = b1 - lr*self.normalize(db1)\n        new_weights[\'W2\'] = W2 - lr*self.normalize(dW2)\n        new_weights[\'b2\'] = b2 - lr*self.normalize(db2)\n        new_weights[\'W3\'] = W3 - lr*self.normalize(dW3)\n        new_weights[\'b3\'] = b3 - lr*self.normalize(db3)\n        return new_weights\n\n\n    def meta_forward(self, x_a, x_b, label_a, weights, cache={}):\n        """"""Full forward pass for MAML. Does a inner_forward, backprop, and gradient \n        update.  This will all be backpropped through w.r.t. weights in meta_backward\n\n        Args:\n            x_a (ndarray): Example or examples of sinusoid from given phase, amplitude.  \n            x_b (ndarray): Independent example(s) from same phase, amplitude as x_a\'s\n            label_a (ndarray): Ground truth labels for x_a\n            weights (dict): Dictionary of weights and biases for neural net\n            cache (dict): Pass in dictionary to be updated with values needed in meta_backward\n\n        Returns:\n            pred_b (ndarray): Predicted values for example(s) x_b\n        """"""\n        w = weights\n        W1, b1, W2, b2, W3, b3 = w[\'W1\'], w[\'b1\'], w[\'W2\'], w[\'b2\'], w[\'W3\'], w[\'b3\']\n\n        # A: inner\n        # standard forward and backward computations\n        inner_cache = {}\n        pred_a = self.inner_forward(x_a, w, inner_cache)\n\n        # inner loss\n        dout_a = 2*(pred_a - label_a) \n\n        # d 3rd layer\n        dW3 = inner_cache[\'relu2_a\'].T.dot(dout_a)\n        db3 = np.sum(dout_a, axis=0)\n        drelu2_a = dout_a.dot(W3.T)\n\n        daffine2_a = np.where(inner_cache[\'affine2_a\'] > 0, drelu2_a, 0)\n\n        # d 2nd layer\n        dW2 = inner_cache[\'relu1_a\'].T.dot(daffine2_a)\n        db2 = np.sum(daffine2_a, axis=0)\n        drelu1_a = daffine2_a.dot(W2.T)\n\n        daffine1_a = np.where(inner_cache[\'affine1_a\'] > 0, drelu1_a, 0)\n\n        # d 1st layer\n        dW1 = x_a.T.dot(daffine1_a)\n        db1 = np.sum(daffine1_a, axis=0)\n\n        # Forward on fast weights\n        # B: meta/outer\n        # SGD step is baked into forward pass, representing optimizing through fine-tuning\n        # Theta prime in the paper. Also called fast_weights in Finn\'s TF implementation\n        W1_prime = W1 - self.inner_lr*dW1\n        b1_prime = b1 - self.inner_lr*db1\n        W2_prime = W2 - self.inner_lr*dW2\n        b2_prime = b2 - self.inner_lr*db2\n        W3_prime = W3 - self.inner_lr*dW3\n        b3_prime = b3 - self.inner_lr*db3\n\n        # Do another forward pass with the fast weights, to predict B example\n        affine1_b = x_b.dot(W1_prime) + b1_prime\n        relu1_b = np.maximum(0, affine1_b)\n        affine2_b = relu1_b.dot(W2_prime) + b2_prime\n        relu2_b = np.maximum(0, affine2_b)\n        pred_b = relu2_b.dot(W3_prime) + b3_prime\n\n        # Cache relevant values for meta backpropping\n        outer_cache = dict(dout_a=dout_a, x_b=x_b, affine1_b=affine1_b, relu1_b=relu1_b, affine2_b=affine2_b, relu2_b=relu2_b, daffine2_a=daffine2_a, W2_prime=W2_prime, W3_prime=W3_prime)\n        cache.update(inner_cache)\n        cache.update(outer_cache)\n\n        return pred_b\n    \n    def meta_backward(self, dout_b, weights, cache, grads=GradDict()):\n        """"""Full backward pass for MAML. Through all operations from forward pass\n\n        Args:\n            dout_b (ndarray): Gradient signal of network output (usually loss gradient)\n            weights (dict): Dictionary of weights and biases used in forward pass\n            cache (dict): Dictionary of relevant values from forward pass\n            grads (dict): Pass in dictionary to be updated with weight gradients\n        """"""\n        c = cache; w = weights \n        W1, b1, W2, b2, W3, b3 = w[\'W1\'], w[\'b1\'], w[\'W2\'], w[\'b2\'], w[\'W3\'], w[\'b3\']\n\n        # First, backprop through the B network pass\n        # d 3rd layer\n        drelu2_b = dout_b.dot(c[\'W3_prime\'].T)\n        dW3_prime = c[\'relu2_b\'].T.dot(dout_b)\n        db3_prime = np.sum(dout_b, axis=0)\n\n        daffine2_b = np.where(c[\'affine2_b\'] > 0, drelu2_b, 0)\n\n        # d 2nd layer\n        drelu1_b = daffine2_b.dot(c[\'W2_prime\'].T)\n        dW2_prime = c[\'relu1_b\'].T.dot(daffine2_b)\n        db2_prime = np.sum(daffine2_b, axis=0)\n\n        daffine1_b = np.where(c[\'affine1_b\'] > 0, drelu1_b, 0)\n\n        # d 1st layer\n        dW1_prime = c[\'x_b\'].T.dot(daffine1_b)\n        db1_prime = np.sum(daffine1_b, axis=0)\n\n        # Next, backprop through the gradient descent step\n        dW1 = dW1_prime\n        db1 = db1_prime\n        dW2 = dW2_prime\n        db2 = db2_prime\n        dW3 = dW3_prime\n        db3 = db3_prime\n\n        ddW1 = dW1_prime * -self.inner_lr\n        ddb1 = db1_prime * -self.inner_lr\n        ddW2 = dW2_prime * -self.inner_lr\n        ddb2 = db2_prime * -self.inner_lr\n        ddW3 = dW3_prime * -self.inner_lr\n        ddb3 = db3_prime * -self.inner_lr\n\n        # Then, backprop through the first backprop\n        # start with dW1\'s\n        ddaffine1_a = c[\'x_a\'].dot(ddW1) \n        ddaffine1_a += ddb1\n\n        ddrelu1_a = np.where(c[\'affine1_a\'] > 0, ddaffine1_a, 0)\n\n        ddaffine2_a = ddrelu1_a.dot(W2)\n        dW2 += ddrelu1_a.T.dot(c[\'daffine2_a\'])\n\n        # dW2\'s\n        drelu1_a = c[\'daffine2_a\'].dot(ddW2.T) # shortcut back because of the grad dependency\n        ddaffine2_a += ddb2\n        ddaffine2_a += c[\'relu1_a\'].dot(ddW2)\n\n        ddrelu2_a = np.where(c[\'affine2_a\'] > 0, ddaffine2_a, 0)\n\n        ddout_a = ddrelu2_a.dot(W3)\n        dW3 += ddrelu2_a.T.dot(c[\'dout_a\'])\n\n        # dW3\'s\n        drelu2_a = c[\'dout_a\'].dot(ddW3.T) # shortcut back because of the grad dependency\n        ddout_a += ddb3\n        ddout_a += c[\'relu2_a\'].dot(ddW3)\n\n        # Finally, backprop through the first forward\n        dpred_a = ddout_a * 2 \n\n        drelu2_a += dpred_a.dot(W3.T)\n        db3 += np.sum(dpred_a, axis=0)\n        dW3 += c[\'relu2_a\'].T.dot(dpred_a)\n\n        daffine2_a = np.where(c[\'affine2_a\'] > 0, drelu2_a, 0)\n\n        drelu1_a += daffine2_a.dot(W2.T)\n        dW2 += c[\'relu1_a\'].T.dot(daffine2_a)\n        db2 += np.sum(daffine2_a, axis=0)\n\n        daffine1_a = np.where(c[\'affine1_a\'] > 0, drelu1_a, 0)\n\n        dW1 += c[\'x_a\'].T.dot(daffine1_a)\n        db1 += np.sum(daffine1_a, axis=0)\n\n        # update gradients \n        grads[\'W1\'] += self.normalize(dW1)\n        grads[\'b1\'] += self.normalize(db1)\n        grads[\'W2\'] += self.normalize(dW2)\n        grads[\'b2\'] += self.normalize(db2)\n        grads[\'W3\'] += self.normalize(dW3)\n        grads[\'b3\'] += self.normalize(db3)\n\n   \ndef gradcheck():\n    # Test the network gradient \n    nn = Network(normalize=lambda x: x) # don\'t normalize gradients so we can check validity \n    grads = GradDict()  # initialize grads to 0\n    # dummy inputs, labels, and fake backwards gradient signal\n    x_a = np.random.randn(15, 1)\n    x_b = np.random.randn(15, 1)\n    label = np.random.randn(15, 1)\n    dout = np.random.randn(15, 1)\n    # make weights. don\'t use build_weights here because this is more stable\n    W1 = np.random.randn(1, 40)\n    b1 = np.random.randn(40)\n    W2 = np.random.randn(40, 40)\n    b2 = np.random.randn(40)\n    W3 = np.random.randn(40, 1)\n    b3 = np.random.randn(1)\n    weights = dict(W1=W1, b1=b1, W2=W2, b2=b2, W3=W3, b3=b3)\n\n    # helper function to only change a single key of interest for independent finite differences\n    def rep_param(weights, name, val):\n        clean_params = copy.deepcopy(weights)\n        clean_params[name] = val\n        return clean_params\n\n    # Evaluate gradients numerically, using finite differences\n    numerical_grads = {}\n    for key in weights:\n        num_grad = eval_numerical_gradient_array(lambda w: nn.meta_forward(x_a, x_b, label, rep_param(weights, key, w)), weights[key], dout, h=1e-5)\n        numerical_grads[key] = num_grad\n\n    # Compute neural network gradients\n    cache = {}\n    out = nn.meta_forward(x_a, x_b, label, weights, cache=cache)\n    nn.meta_backward(dout, weights, cache, grads)\n\n    # The error should be around 1e-10\n    print()\n    for key in weights:\n        print(\'d{} error: {}\'.format(key, rel_error(numerical_grads[key], grads[key])))\n    print()\n\ndef test():\n    """"""Take one grad step using a minibatch of size 5 and see how well it works\n\n    Basically what they show in Figure 2 of the paper\n    """""" \n    nn = Network(inner_lr=FLAGS.inner_lr)\n\n    pre_weights = {}\n    pre_weights[\'maml\'] = load_weights(FLAGS.weight_path)\n    if FLAGS.use_baseline:\n        pre_weights[\'baseline\'] = load_weights(\'baseline_\'+FLAGS.weight_path)\n    pre_weights[\'random\'] = build_weights()\n\n    # Generate N batches of data, with same shape as training, but that all have the same amplitude and phase\n    N = 2\n    #sinegen = SinusoidGenerator(FLAGS.inner_bs*N, 1, config={\'input_range\':[1.0,5.0]}) \n    sinegen = SinusoidGenerator(FLAGS.inner_bs*N, 1)\n    x, y, amp, phase = map(lambda x: x[0], sinegen.generate()) # grab all the first elems\n    xs = np.split(x, N)\n    ys = np.split(y, N)\n\n    # Copy pre-update weights for later comparison\n    deepcopy = lambda weights: {key: weights[key].copy() for key in weights}\n    post_weights = {}\n    for key in pre_weights:\n        post_weights[key] = deepcopy(pre_weights[key])\n\n    T = 10\n    # Run fine-tuning \n    for key in post_weights:\n        for t in range(T):\n            for i in range(len(xs)):\n                x = xs[i]\n                y = ys[i]\n                grads = GradDict()\n                cache = {}\n                pred = nn.inner_forward(x, post_weights[key], cache)\n                loss = (pred - y)**2\n                dout = 2*(pred - y)\n                post_weights[key] = nn.inner_backward(dout, post_weights[key], cache)\n\n\n    colors = {\'maml\': \'r\', \'baseline\': \'b\', \'random\': \'g\'}\n    name = {\'maml\': \'MAML\', \'baseline\': \'joint training\', \'random\': \'random initialization\'}\n\n    sine_ground = lambda x: amp*np.sin(x - phase)\n    sine_pre_pred = lambda x, key: nn.inner_forward(x, pre_weights[key])[0]\n    sine_post_pred = lambda x, key: nn.inner_forward(x, post_weights[key])[0]\n\n    x_vals = np.linspace(-5, 5)\n    y_ground = np.apply_along_axis(sine_ground, 0, x_vals)\n\n\n    for key in post_weights:\n        y_pre = np.array([sine_pre_pred(np.array(x), key) for x in x_vals]).squeeze()\n        y_nn = np.array([sine_post_pred(np.array(x), key) for x in x_vals]).squeeze()\n        plt.plot(x_vals, y_ground, \'k\', label=\'{:.2f}sin(x - {:.2f})\'.format(amp, phase))\n        plt.plot(np.concatenate(xs), np.concatenate(ys), \'ok\', label=\'samples\')\n        plt.plot(x_vals, y_pre, colors[key]+\'--\', label=\'pre-update\')\n        plt.plot(x_vals, y_nn, colors[key]+\'-\', label=\'post-update\')\n\n        plt.legend()\n        plt.title(\'Fine-tuning performance {}\'.format(name[key]))\n        plt.savefig(key+\'.png\')\n        plt.show()\n\ndef train():\n    nn = Network(inner_lr=FLAGS.inner_lr)\n    weights = build_weights()\n    optimizer = AdamOptimizer(weights, learning_rate=FLAGS.meta_lr)\n    if FLAGS.use_baseline:\n        baseline_weights = build_weights()\n        baseline_optimizer = AdamOptimizer(baseline_weights, learning_rate=FLAGS.meta_lr)\n\n    sinegen = SinusoidGenerator(2*FLAGS.inner_bs, 25)  # update_batch * 2, meta batch size\n\n    try:\n        nitr = int(FLAGS.num_iter)\n        for itr in range(int(nitr)):\n            # create a minibatch of size 25, with 10 points\n            batch_x, batch_y, amp, phase = sinegen.generate()\n\n            inputa = batch_x[:, :FLAGS.inner_bs :]\n            labela = batch_y[:, :FLAGS.inner_bs :]\n            inputb = batch_x[:, FLAGS.inner_bs :] # b used for testing\n            labelb = batch_y[:, FLAGS.inner_bs :]\n            \n            # META BATCH\n            grads = GradDict() # zero grads\n            baseline_grads = GradDict() # zero grads\n            losses = []\n            baseline_losses = []\n            for batch_i in range(len(inputa)):\n                ia, la, ib, lb = inputa[batch_i], labela[batch_i], inputb[batch_i], labelb[batch_i]\n                cache = {}\n                pred_b = nn.meta_forward(ia, ib, la, weights, cache=cache)\n                losses.append((pred_b - lb)**2)\n                dout_b = 2*(pred_b - lb)\n                nn.meta_backward(dout_b, weights, cache, grads)\n\n\n                if FLAGS.use_baseline:\n                    baseline_cache = {}\n                    baseline_i = np.concatenate([ia,ib])\n                    baseline_l = np.concatenate([la,lb])\n                    baseline_pred = nn.inner_forward(baseline_i, baseline_weights, cache=baseline_cache)\n                    baseline_losses.append((baseline_pred - baseline_l)**2)\n                    dout_b = 2*(baseline_pred - baseline_l)\n                    nn.inner_backward(dout_b, baseline_weights, baseline_cache, baseline_grads)\n\n            optimizer.apply_gradients(weights, grads, learning_rate=FLAGS.meta_lr)\n            if FLAGS.use_baseline:\n                baseline_optimizer.apply_gradients(baseline_weights, baseline_grads, learning_rate=FLAGS.meta_lr)\n            if itr % 100 == 0:\n                if FLAGS.use_baseline:\n                    print(""[itr: {}] MAML loss = {} Baseline loss = {}"".format(itr, np.sum(losses), np.sum(baseline_losses)))\n                else:\n                    print(""[itr: {}] Loss = {}"".format(itr, np.sum(losses)))\n    except KeyboardInterrupt:\n        pass\n    save_weights(weights, FLAGS.weight_path)\n    if FLAGS.use_baseline:\n        save_weights(baseline_weights, ""baseline_""+FLAGS.weight_path)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'MAML\')\n    parser.add_argument(\'--seed\', type=int, default=2, help=\'\')\n    parser.add_argument(\'--gradcheck\', type=int, default=0, help=\'Run gradient check and other tests\')\n    parser.add_argument(\'--test\', type=int, default=0, help=\'Run test on trained network to see if it works\')\n    parser.add_argument(\'--meta_lr\', type=float, default=1e-3, help=\'Meta learning rate\')\n    parser.add_argument(\'--inner_lr\', type=float, default=1e-2, help=\'Inner learning rate\')\n    parser.add_argument(\'--inner_bs\', type=int, default=5, help=\'Inner batch size\')\n    parser.add_argument(\'--weight_path\', type=str, default=\'trained_maml_weights.pkl\', help=\'File name to save and load weights\')\n    parser.add_argument(\'--use_baseline\', type=int, default=1, help=\'Whether to train a baseline network\')\n    parser.add_argument(\'--num_iter\', type=float, default=1e4, help=\'Number of iterations\')\n    FLAGS = parser.parse_args()\n    np.random.seed(FLAGS.seed)\n    \n    if FLAGS.gradcheck:\n        gradcheck()\n    elif FLAGS.test:\n        test()\n    else:\n        train()\n'"
maml_1hidden.py,39,"b'#!/usr/bin/env python3\nimport pickle\nimport copy\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport argparse\nfrom collections import defaultdict\n\nfrom utils.optim import AdamOptimizer\nfrom utils.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array, rel_error\nfrom utils.data_generator import SinusoidGenerator\n\n\n# this will create a special dictionary that returns 0 if the element is not set, instead of error\n# (it makes the code for updating gradients simpler)\nGradDict = lambda: defaultdict(lambda: 0) \n\nnormalize = lambda x: (x - x.mean()) / (x.std() + 1e-8)\n\ndef build_weights(hidden_dim=200):\n    """"""Return weights to be used in forward pass""""""\n    # Initialize all weights (model params) with ""Xavier Initialization"" \n    # weight matrix init = uniform(-1, 1) / sqrt(layer_input)\n    # bias init = zeros()\n    H = hidden_dim\n    d = {}\n    d[\'W1\'] = (-1 + 2*np.random.rand(1, H)) / np.sqrt(1)\n    d[\'b1\'] = np.zeros(H)\n    d[\'W2\'] = (-1 + 2*np.random.rand(H, 1)) / np.sqrt(H)\n    d[\'b2\'] = np.zeros(1)\n\n    # Cast all parameters to the correct datatype\n    for k, v in d.items():\n        d[k] = v.astype(np.float32)\n    return d\n\ndef save_weights(weights, filename, quiet=False):\n    with open(filename, \'wb\') as f:\n        pickle.dump(weights, f)\n    if not quiet:\n        print(\'weights saved to {}\'.format(filename))\n\ndef load_weights(filename, quiet=False):\n    with open(filename, \'rb\') as f:\n        weights = pickle.load(f)\n    if not quiet:\n        print(\'weights loaded from {}\'.format(filename))\n    return weights\n\n\nclass Network(object):\n    """"""BYOW: Bring Your Own Weights\n\n    Hard-code operations for a 2 layer neural network\n    """"""\n    def __init__(self, alpha=0.01, normalized=normalize):\n        self.ALPHA = alpha\n        self.normalized = normalized\n\n    def inner_forward(self, x_a, w):\n        """"""submodule for forward pass""""""\n        W1, b1, W2, b2 = w[\'W1\'], w[\'b1\'], w[\'W2\'], w[\'b2\']\n\n        affine1_a = x_a.dot(W1) + b1\n        relu1_a = np.maximum(0, affine1_a)\n        pred_a = relu1_a.dot(W2) + b2 \n\n        cache = dict(x_a=x_a, affine1_a=affine1_a, relu1_a=relu1_a)\n        return pred_a, cache\n\n    def inner_backward(self, dout_a, weights, cache):\n        """"""just for fine-tuning at the end""""""\n        w = weights; c = cache\n        W1, b1, W2, b2 = w[\'W1\'], w[\'b1\'], w[\'W2\'], w[\'b2\']\n\n        drelu1_a = dout_a.dot(W2.T)\n        dW2 = cache[\'relu1_a\'].T.dot(dout_a)\n        db2 = np.sum(dout_a, axis=0)\n\n        daffine1_a = np.where(cache[\'affine1_a\'] > 0, drelu1_a, 0)\n\n        dW1 = c[\'x_a\'].T.dot(daffine1_a)\n        db1 = np.sum(daffine1_a, axis=0)\n\n        # grad steps\n        new_weights = {}\n        new_weights[\'W1\'] = W1 - self.ALPHA*self.normalized(dW1)\n        new_weights[\'b1\'] = b1 - self.ALPHA*self.normalized(db1)\n        new_weights[\'W2\'] = W2 - self.ALPHA*self.normalized(dW2)\n        new_weights[\'b2\'] = b2 - self.ALPHA*self.normalized(db2)\n        return new_weights\n\n\n    def meta_forward(self, x_a, x_b, label_a, weights, cache=None):\n        w = weights\n        W1, b1, W2, b2 = w[\'W1\'], w[\'b1\'], w[\'W2\'], w[\'b2\']\n\n        # standard forward and backward computations\n        # (a)\n        pred_a, inner_cache = self.inner_forward(x_a, w)\n\n        dout_a = 2*(pred_a - label_a)\n\n        drelu1_a = dout_a.dot(W2.T)\n        dW2 = inner_cache[\'relu1_a\'].T.dot(dout_a)\n        db2 = np.sum(dout_a, axis=0)\n\n        daffine1_a = np.where(inner_cache[\'affine1_a\'] > 0, drelu1_a, 0)\n\n        dW1 = x_a.T.dot(daffine1_a)\n        db1 = np.sum(daffine1_a, axis=0)\n\n        # Forward on fast weights\n        # (b)\n\n        # grad steps\n        W1_prime = W1 - self.ALPHA*dW1\n        b1_prime = b1 - self.ALPHA*db1\n        W2_prime = W2 - self.ALPHA*dW2\n        b2_prime = b2 - self.ALPHA*db2\n\n        affine1_b = x_b.dot(W1_prime) + b1_prime\n        relu1_b = np.maximum(0, affine1_b)\n        pred_b = relu1_b.dot(W2_prime) + b2_prime\n\n        if cache:\n            outer_cache = dict(dout_a=dout_a, x_b=x_b, affine1_b=affine1_b, relu1_b=relu1_b, W2_prime=W2_prime)\n            return pred_b, {**inner_cache, **outer_cache}\n        else:\n            return pred_b\n    \n    def meta_backward(self, dout_b, weights, cache, grads=None):\n        c = cache; w = weights # short \n        W1, b1, W2, b2 = w[\'W1\'], w[\'b1\'], w[\'W2\'], w[\'b2\']\n\n        # deriv w.r.t b (lower half)\n        # d 1st layer\n        dW2_prime = c[\'relu1_b\'].T.dot(dout_b)\n        db2_prime = np.sum(dout_b, axis=0)\n        drelu1_b = dout_b.dot(c[\'W2_prime\'].T)\n\n        daffine1_b = np.where(c[\'affine1_b\'] > 0, drelu1_b, 0)\n        # d 2nd layer\n        dW1_prime = c[\'x_b\'].T.dot(daffine1_b)\n        db1_prime = np.sum(daffine1_b, axis=0)\n\n        # deriv w.r.t a (upper half)\n\n        # going back through the gradient descent step\n        dW1 = dW1_prime\n        db1 = db1_prime\n        dW2 = dW2_prime\n        db2 = db2_prime\n\n        ddW1 = dW1_prime * -self.ALPHA\n        ddb1 = db1_prime * -self.ALPHA\n        ddW2 = dW2_prime * -self.ALPHA\n        ddb2 = db2_prime * -self.ALPHA\n\n        # backpropping through the first backprop\n        ddout_a = c[\'relu1_a\'].dot(ddW2)\n        ddout_a += ddb2\n        drelu1_a = c[\'dout_a\'].dot(ddW2.T) # shortcut back because of the grad dependency\n\n        ddaffine1_a = c[\'x_a\'].dot(ddW1) \n        ddaffine1_a += ddb1\n        ddrelu1_a = np.where(c[\'affine1_a\'] > 0, ddaffine1_a, 0)\n\n        dW2 += ddrelu1_a.T.dot(c[\'dout_a\'])\n\n        ddout_a += ddrelu1_a.dot(W2)\n\n        dpred_a = ddout_a * 2 # = dout_a\n\n        dW2 += c[\'relu1_a\'].T.dot(dpred_a)\n        db2 += np.sum(dpred_a, axis=0)\n\n        drelu1_a += dpred_a.dot(W2.T)\n\n        daffine1_a = np.where(c[\'affine1_a\'] > 0, drelu1_a, 0)\n\n        dW1 += c[\'x_a\'].T.dot(daffine1_a)\n        db1 += np.sum(daffine1_a, axis=0)\n\n        if grads is not None:\n            # update gradients \n            grads[\'W1\'] += self.normalized(dW1)\n            grads[\'b1\'] += self.normalized(db1)\n            grads[\'W2\'] += self.normalized(dW2)\n            grads[\'b2\'] += self.normalized(db2)\n\n   \ndef gradcheck():\n    # Test the network gradient \n    nn = Network(normalized=lambda x: x)\n    grads = GradDict()\n\n    np.random.seed(231)\n    x_a = np.random.randn(15, 1)\n    x_b = np.random.randn(15, 1)\n    label = np.random.randn(15, 1)\n    W1 = np.random.randn(1, 40)\n    b1 = np.random.randn(40)\n    W2 = np.random.randn(40, 1)\n    b2 = np.random.randn(1)\n\n    dout = np.random.randn(15, 1)\n\n    weights = w = {}\n    w[\'W1\'] = W1\n    w[\'b1\'] = b1\n    w[\'W2\'] = W2\n    w[\'b2\'] = b2\n\n    def rep_param(weights, name, val):\n        clean_params = copy.deepcopy(weights)\n        clean_params[name] = val\n        return clean_params\n\n    dW1_num = eval_numerical_gradient_array(lambda w: nn.meta_forward(x_a, x_b, label, rep_param(weights, \'W1\', w)), W1, dout)\n    db1_num = eval_numerical_gradient_array(lambda b: nn.meta_forward(x_a, x_b, label, rep_param(weights, \'b1\', b)), b1, dout)\n    dW2_num = eval_numerical_gradient_array(lambda w: nn.meta_forward(x_a, x_b, label, rep_param(weights, \'W2\', w)), W2, dout)\n    db2_num = eval_numerical_gradient_array(lambda b: nn.meta_forward(x_a, x_b, label, rep_param(weights, \'b2\', b)), b2, dout)\n\n    out, cache = nn.meta_forward(x_a, x_b, label, weights, cache=True)\n    nn.meta_backward(dout, weights, cache, grads)\n\n    # The error should be around 1e-10\n    print()\n    print(\'Testing meta-learning NN backward function:\')\n    print(\'dW1 error: \', rel_error(dW1_num, grads[\'W1\']))\n    print(\'db1 error: \', rel_error(db1_num, grads[\'b1\']))\n    print(\'dW2 error: \', rel_error(dW2_num, grads[\'W2\']))\n    print(\'db2 error: \', rel_error(db2_num, grads[\'b2\']))\n    print()\n\ndef test():\n    """"""take one grad step using a minibatch of size 5 and see how well it works\n\n    basically what they show in Figure 2 of:\n    https://arxiv.org/pdf/1703.03400.pdf\n    """""" \n    nn = Network()\n    pre_weights = load_weights(FLAGS.weight_path)\n    random_weights = build_weights()\n\n    # values for fine-tuning step\n    N = 10\n    sin_gen = SinusoidGenerator(5*N, 1) \n    x, y, amp, phase = map(lambda x: x[0], sin_gen.generate()) # grab all the first elems\n    xs = np.split(x, N)\n    ys = np.split(y, N)\n\n    new_weights = pre_weights.copy()\n    new_random_weights = random_weights.copy()\n    for i in range(len(xs)):\n        x = xs[i]\n        y = ys[i]\n        grads = GradDict()\n        pred, cache = nn.inner_forward(x, new_weights)\n        loss = (pred - y)**2\n        dout = 2*(pred - y)\n        new_weights = nn.inner_backward(dout, new_weights, cache)\n\n    for i in range(len(xs)):\n        x = xs[i]\n        y = ys[i]\n        grads = GradDict()\n        pred, cache = nn.inner_forward(x, new_random_weights)\n        loss = (pred - y)**2\n        dout = 2*(pred - y)\n        new_random_weights = nn.inner_backward(dout, new_random_weights, cache)\n\n\n    sine_true = lambda x: amp*np.sin(x - phase)\n    sine_nn = lambda x: nn.inner_forward(x, new_weights)[0]\n    sine_pre = lambda x: nn.inner_forward(x, pre_weights)[0]\n    sine_random = lambda x: nn.inner_forward(x, random_weights)[0]\n    sine_new_random = lambda x: nn.inner_forward(x, new_random_weights)[0]\n\n    x_vals = np.linspace(-5, 5)\n\n    y_true = np.apply_along_axis(sine_true, 0, x_vals)\n    y_nn = np.array([sine_nn(np.array(x)) for x in x_vals]).squeeze()\n    y_pre = np.array([sine_pre(np.array(x)) for x in x_vals]).squeeze()\n    y_random = np.array([sine_random(np.array(x)) for x in x_vals]).squeeze()\n    y_new_random = np.array([sine_new_random(np.array(x)) for x in x_vals]).squeeze()\n\n    plt.plot(x_vals, y_true, \'k\', label=\'{:.2f}sin(x - {:.2f})\'.format(amp, phase))\n    plt.plot(x_vals, y_pre, \'r--\', label=\'pre-update\')\n    plt.plot(x_vals, y_nn, \'r-\', label=\'post-update\')\n    plt.plot(x_vals, y_random, \'g--\', label=\'random\')\n    plt.plot(x_vals, y_new_random, \'g-\', label=\'new_random\')\n    plt.legend()\n    plt.show()\n\n\ndef main():\n    nn = Network()\n    weights = build_weights()\n    optimizer = AdamOptimizer(weights, learning_rate=FLAGS.learning_rate)\n\n    sin_gen = SinusoidGenerator(10, 25)  # update_batch * 2, meta batch size\n\n\n    lr = lambda x: x * FLAGS.learning_rate\n\n    nitr = 1e4\n    for itr in range(int(nitr)):\n        frac = 1.0 - (itr / nitr)\n\n        # create a minibatch of size 25, with 10 points\n        batch_x, batch_y, amp, phase = sin_gen.generate()\n\n        inputa = batch_x[:, :5, :]\n        labela = batch_y[:, :5, :]\n        inputb = batch_x[:, 5:, :] # b used for testing\n        labelb = batch_y[:, 5:, :]\n        \n        # META BATCH\n        grads = GradDict() # zero grads\n        losses = []\n        for batch_i in range(len(inputa)):\n            ia, la, ib, lb = inputa[batch_i], labela[batch_i], inputb[batch_i], labelb[batch_i]\n            pred_b, cache = nn.meta_forward(ia, ib, la, weights, cache=True)\n            losses.append((pred_b - lb)**2)\n            dout_b = 2*(pred_b - lb)\n            nn.meta_backward(dout_b, weights, cache, grads)\n        optimizer.apply_gradients(weights, grads, learning_rate=lr(frac))\n        if itr % 100 == 0:\n            print(""[itr: {}] Loss = {}"".format(itr, np.sum(losses)))\n\n    save_weights(weights, FLAGS.weight_path)\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'MAML\')\n    parser.add_argument(\'--gradcheck\', type=int, default=0, help=\'Run gradient check and other tests\')\n    parser.add_argument(\'--test\', type=int, default=0, help=\'Run test on trained network to see if it works\')\n    parser.add_argument(\'--learning_rate\', type=float, default=1e-3, help=\'Learning rate\')\n    parser.add_argument(\'--weight_path\', type=str, default=\'trained_maml_weights.pkl\', help=\'File name to save and load weights\')\n    FLAGS = parser.parse_args()\n    \n    if FLAGS.gradcheck:\n        gradcheck()\n        exit(0)\n\n    if FLAGS.test:\n        test()\n        exit(0)\n\n    main()\n\n\n'"
utils/data_generator.py,8,"b'"""""" \n\nTaken (and modified/hacked) from Chelsea Finn\'s MAML implementation\nhttps://github.com/cbfinn/maml\n\nCode for loading data. \n\n""""""\nimport numpy as np\nimport random\n\nclass SinusoidGenerator(object):\n    """"""\n    SinusoidGenerator capable of generating batches of sinusoid \n    A ""class"" is considered a particular sinusoid function.\n    """"""\n    def __init__(self, num_samples_per_class, batch_size, config={}):\n        """"""\n        Args:\n            num_samples_per_class: num samples to generate per class in one batch\n            batch_size: size of meta batch size (e.g. number of functions)\n        """"""\n        self.batch_size = batch_size\n        self.num_samples_per_class = num_samples_per_class\n        self.num_classes = 1  # by default 1 (only relevant for classification problems)\n\n        self.generate = self.generate_sinusoid_batch\n        self.amp_range = config.get(\'amp_range\', [0.1, 5.0])\n        self.phase_range = config.get(\'phase_range\', [0, np.pi])\n        self.input_range = config.get(\'input_range\', [-5.0, 5.0])\n        self.dim_input = 1\n        self.dim_output = 1\n\n    def generate_sinusoid_batch(self, train=True, input_idx=None):\n        # Note train arg is not used (but it is used for omniglot method.\n        # input_idx is used during qualitative testing --the number of examples used for the grad update\n        amp = np.random.uniform(self.amp_range[0], self.amp_range[1], [self.batch_size])\n        phase = np.random.uniform(self.phase_range[0], self.phase_range[1], [self.batch_size])\n        outputs = np.zeros([self.batch_size, self.num_samples_per_class, self.dim_output])\n        init_inputs = np.zeros([self.batch_size, self.num_samples_per_class, self.dim_input])\n        for func in range(self.batch_size):\n            init_inputs[func] = np.random.uniform(self.input_range[0], self.input_range[1], [self.num_samples_per_class, 1])\n            if input_idx is not None:\n                init_inputs[:,input_idx:,0] = np.linspace(self.input_range[0], self.input_range[1], num=self.num_samples_per_class-input_idx, retstep=False)\n            outputs[func] = amp[func] * np.sin(init_inputs[func]-phase[func])\n        return init_inputs, outputs, amp, phase\n'"
utils/gradient_check.py,11,"b'from __future__ import print_function\nfrom builtins import range\nfrom past.builtins import xrange\n\nimport numpy as np\nfrom random import randrange\n\n\n""""""\nTHIS IS FROM TAKEN FROM STANFORD\'S CS231N COURSE, WHICH I HIGHLY RECOMMEND \nhttp://cs231n.github.io/\n\nIt does numerical gradient checking\n""""""\n\ndef eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n    """"""\n    a naive implementation of numerical gradient of f at x\n    - f should be a function that takes a single argument\n    - x is the point (numpy array) to evaluate the gradient at\n    """"""\n\n    fx = f(x) # evaluate function value at original point\n    grad = np.zeros_like(x)\n    # iterate over all indexes in x\n    it = np.nditer(x, flags=[\'multi_index\'], op_flags=[\'readwrite\'])\n    while not it.finished:\n\n        # evaluate function at x+h\n        ix = it.multi_index\n        oldval = x[ix]\n        x[ix] = oldval + h # increment by h\n        fxph = f(x) # evalute f(x + h)\n        x[ix] = oldval - h\n        fxmh = f(x) # evaluate f(x - h)\n        x[ix] = oldval # restore\n\n        # compute the partial derivative with centered formula\n        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n        if verbose:\n            print(ix, grad[ix])\n        it.iternext() # step to next dimension\n\n    return grad\n\n\ndef eval_numerical_gradient_array(f, x, df, h=1e-5):\n    """"""\n    Evaluate a numeric gradient for a function that accepts a numpy\n    array and returns a numpy array.\n    """"""\n    grad = np.zeros_like(x)\n    it = np.nditer(x, flags=[\'multi_index\'], op_flags=[\'readwrite\'])\n    while not it.finished:\n        ix = it.multi_index\n\n        oldval = x[ix]\n        x[ix] = oldval + h\n        pos = f(x).copy()\n        x[ix] = oldval - h\n        neg = f(x).copy()\n        x[ix] = oldval\n\n        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n        it.iternext()\n    return grad\n\n\ndef eval_numerical_gradient_blobs(f, inputs, output, h=1e-5):\n    """"""\n    Compute numeric gradients for a function that operates on input\n    and output blobs.\n\n    We assume that f accepts several input blobs as arguments, followed by a\n    blob where outputs will be written. For example, f might be called like:\n\n    f(x, w, out)\n\n    where x and w are input Blobs, and the result of f will be written to out.\n\n    Inputs:\n    - f: function\n    - inputs: tuple of input blobs\n    - output: output blob\n    - h: step size\n    """"""\n    numeric_diffs = []\n    for input_blob in inputs:\n        diff = np.zeros_like(input_blob.diffs)\n        it = np.nditer(input_blob.vals, flags=[\'multi_index\'],\n                       op_flags=[\'readwrite\'])\n        while not it.finished:\n            idx = it.multi_index\n            orig = input_blob.vals[idx]\n\n            input_blob.vals[idx] = orig + h\n            f(*(inputs + (output,)))\n            pos = np.copy(output.vals)\n            input_blob.vals[idx] = orig - h\n            f(*(inputs + (output,)))\n            neg = np.copy(output.vals)\n            input_blob.vals[idx] = orig\n\n            diff[idx] = np.sum((pos - neg) * output.diffs) / (2.0 * h)\n\n            it.iternext()\n        numeric_diffs.append(diff)\n    return numeric_diffs\n\n\ndef eval_numerical_gradient_net(net, inputs, output, h=1e-5):\n    return eval_numerical_gradient_blobs(lambda *args: net.forward(),\n                inputs, output, h=h)\n\n\ndef grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5):\n    """"""\n    sample a few random elements and only return numerical\n    in this dimensions.\n    """"""\n\n    for i in range(num_checks):\n        ix = tuple([randrange(m) for m in x.shape])\n\n        oldval = x[ix]\n        x[ix] = oldval + h # increment by h\n        fxph = f(x) # evaluate f(x + h)\n        x[ix] = oldval - h # increment by h\n        fxmh = f(x) # evaluate f(x - h)\n        x[ix] = oldval # reset\n\n        grad_numerical = (fxph - fxmh) / (2 * h)\n        grad_analytic = analytic_grad[ix]\n        rel_error = (abs(grad_numerical - grad_analytic) /\n                    (abs(grad_numerical) + abs(grad_analytic)))\n        print(\'numerical: %f analytic: %f, relative error: %e\'\n              %(grad_numerical, grad_analytic, rel_error))\n\n\ndef rel_error(x, y):\n  """""" returns relative error """"""\n  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n'"
utils/optim.py,4,"b'import numpy as np\n""""""\nTHIS IS FROM TAKEN FROM STANFORD\'S CS231N COURSE, WHICH I HIGHLY RECOMMEND \nhttp://cs231n.github.io/\n\nThis file implements various first-order update rules that are commonly used for\ntraining neural networks. Each update rule accepts current weights and the\ngradient of the loss with respect to those weights and produces the next set of\nweights. Each update rule has the same interface:\n\ndef update(w, dw, config=None):\n\nInputs:\n  - w: A numpy array giving the current weights.\n  - dw: A numpy array of the same shape as w giving the gradient of the\n    loss with respect to w.\n  - config: A dictionary containing hyperparameter values such as learning rate,\n    momentum, etc. If the update rule requires caching values over many\n    iterations, then config will also hold these cached values.\n\nReturns:\n  - next_w: The next point after the update.\n  - config: The config dictionary to be passed to the next iteration of the\n    update rule.\n\nNOTE: For most update rules, the default learning rate will probably not perform\nwell; however the default values of the other hyperparameters should work well\nfor a variety of different problems.\n\nFor efficiency, update rules may perform in-place updates, mutating w and\nsetting next_w equal to w.\n""""""\n\n\n\n\n\ndef sgd(w, dw, config=None):\n    """"""\n    Performs vanilla stochastic gradient descent.\n\n    config format:\n    - learning_rate: Scalar learning rate.\n    """"""\n    if config is None: config = {}\n    config.setdefault(\'learning_rate\', 1e-2)\n\n    w -= config[\'learning_rate\'] * dw\n    return w, config\n\n\ndef adam(x, dx, config=None):\n    """"""\n    Uses the Adam update rule, which incorporates moving averages of both the\n    gradient and its square and a bias correction term.\n\n    config format:\n    - learning_rate: Scalar learning rate.\n    - beta1: Decay rate for moving average of first moment of gradient.\n    - beta2: Decay rate for moving average of second moment of gradient.\n    - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n    - m: Moving average of gradient.\n    - v: Moving average of squared gradient.\n    - t: Iteration number.\n    """"""\n    if config is None: config = {}\n    config.setdefault(\'learning_rate\', 1e-3)\n    config.setdefault(\'beta1\', 0.9)\n    config.setdefault(\'beta2\', 0.999)\n    config.setdefault(\'epsilon\', 1e-8)\n    config.setdefault(\'m\', np.zeros_like(x))\n    config.setdefault(\'v\', np.zeros_like(x))\n    config.setdefault(\'t\', 0)\n    \n    #print(config[\'learning_rate\'])\n\n    next_x = None\n    beta1, beta2, eps = config[\'beta1\'], config[\'beta2\'], config[\'epsilon\']\n    t, m, v = config[\'t\'], config[\'m\'], config[\'v\']\n    m = beta1 * m + (1 - beta1) * dx\n    v = beta2 * v + (1 - beta2) * (dx * dx)\n    t += 1\n    alpha = config[\'learning_rate\'] * np.sqrt(1 - beta2 ** t) / (1 - beta1 ** t)\n    x -= alpha * (m / (np.sqrt(v) + eps))\n    config[\'t\'] = t\n    config[\'m\'] = m\n    config[\'v\'] = v\n    next_x = x\n\n    return next_x, config\n\n\nclass AdamOptimizer():\n    def __init__(self, params, learning_rate=1e-3):\n        # Configuration for Adam optimization\n        self.optimization_config = {\'learning_rate\': learning_rate}\n        self.adam_configs = {}\n        for p in params:\n            d = {k: v for k, v in self.optimization_config.items()}\n            self.adam_configs[p] = d\n\n    def apply_gradients(self, params, grads, learning_rate=None):\n        for p in params: \n            if learning_rate is not None:\n                self.adam_configs[p][\'learning_rate\'] = learning_rate\n            next_w, self.adam_configs[p] = adam(params[p], grads[p], config=self.adam_configs[p])\n            params[p] = next_w\n\n\n\n\n'"
