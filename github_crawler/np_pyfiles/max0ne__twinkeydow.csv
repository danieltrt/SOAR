file_path,api_count,code
als_thing/compute_feature_vector.py,6,"b'import boto3\nimport pandas as pd\nimport os\nimport findspark\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import reverse\nfrom pyspark.mllib.recommendation import ALS\nfrom pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry, RowMatrix\nimport numpy as np\nfrom numpy import linalg as LA\nimport datetime\nfrom pymongo import MongoClient\nimport json\n\n# config_file = open(""/home/ec2-user/.config"",""r"")\n# config_json = json.loads(config_file.read())\n\nsecret_name = ""mongodb_credential""\nendpoint_url = ""https://secretsmanager.us-east-1.amazonaws.com""\nregion_name = ""us-east-1""\n\nsession = boto3.session.Session()\nclient = session.client(\n    service_name=\'secretsmanager\',\n    region_name=region_name,\n    endpoint_url=endpoint_url\n)\n\nget_secret_value_response = client.get_secret_value(\n    SecretId=secret_name\n)\n\nsecret = get_secret_value_response[\'SecretString\']\nconfig_json = json.loads(secret)\n\nMONGO_URL = config_json[""MONGO_URL""]\nMONGO_DB_NAME = config_json[""MONGO_DB_NAME""]\nclient = MongoClient(MONGO_URL)\ndb = client[MONGO_DB_NAME]\ncollection = db[\'global_config\']\nconfig = collection.find_one({""config_name"":""product_vector_config""})\n\nPARTITION_SIZE = config[""body""][""shard_size""]\nSPARK_HOME = ""/usr/lib/spark""\nOUTPUT_BUCKET_NAME = config[""body""][""bucketInfo""][""bucketName""]\nINPUT_BUCKET_NAME = config[""body""][""bucketInfo""][""inputBucketName""]\nKEY = \'16kUser.csv\'\ns3 = boto3.resource(\'s3\')\ns3.Bucket(INPUT_BUCKET_NAME).download_file(KEY, \'16kUser.csv\')\n\ntrain_file = pd.read_csv(\'16kUser.csv\', index_col=None, sep=\',\',header=None)\ntrain_file.columns = [\'uid\', \'repoid\']\ntrain_file = train_file.drop_duplicates()\ntrain_file.dropna(inplace=True)\ntrain_file.repoid = train_file.repoid.astype(int)\n# train_file.head()\n\nos.environ[""SPARK_HOME""] = SPARK_HOME\n# findspark.init(""/usr/lib/spark"")\n\nsc = SparkContext(appName=""RR"")\nsqlContext = SQLContext(sc)\n\ndef first_two_column(row):\n    return (int(row[0]), int(row[1]))\n\ntraining_rdd = sqlContext.createDataFrame(train_file).rdd.map(first_two_column)\n\ndef train(training_rdd):\n    model = ALS.trainImplicit( \\\n        training_rdd.map(lambda rr: (rr[0], rr[1], 1)),\n        rank=16,\n        iterations=10,\n        lambda_=0.1,\n        alpha=80.0\n    )\n    return model.productFeatures()\ndef similarity(feature_vecs, columnSimilarities_threshold):\n    # transpose `prod_features_rdd`\n    def transpose(rm):\n        cm = CoordinateMatrix(\n            rm.rows.zipWithIndex().flatMap(\n                lambda x: [MatrixEntry(x[1], j, v) for j, v in enumerate(x[0])]\n            )\n        )\n        return cm.transpose().toRowMatrix()\n    rowmat = RowMatrix(feature_vecs)\n    colmat = transpose(rowmat)\n    sims = colmat.columnSimilarities(columnSimilarities_threshold)\n    return sims\ndef most_similar(sims):\n    sim_entries = sims.entries\n    sim_entries_full = sim_entries.flatMap(lambda rr: ((rr.i, (rr.j, rr.value)), (rr.j, (rr.i, rr.value))))\n    def foldByKeyCompare(r1, r2):\n        if r1 is None:\n            return r2\n        return r1 if r1[1] > r2[1] else r2\n    sim_entries_highest = sim_entries_full.foldByKey(None, foldByKeyCompare)\n    return sim_entries_highest\ndef join_key(keys, most_similars):\n    orderedKeys = keys.zipWithIndex().map(lambda rr: (rr[1], rr[0]))\n    sim_entries_left_half = most_similars \\\n        .join(orderedKeys) \\\n        .map(lambda rr: (rr[1][0][0], (rr[1][1], rr[1][0][1])))\n    repo_pairs = sim_entries_left_half.join(orderedKeys).map(lambda rr: (rr[1][0][0], rr[1][1], rr[1][0][1]))\n    return repo_pairs\ndef timer(func, desc):\n    import time\n    start = time.time()\n    result = func()\n    end = time.time()\n    print(desc, "":"", end - start)\n    return result\ndef bind(func, *params):\n    def newfunc():\n        return func(*params)\n    return newfunc\n\nproductFeatures = timer(bind(train, training_rdd), ""ALS"")\nproductFeatures = productFeatures.cache()\npartition_num = productFeatures.count()/PARTITION_SIZE\n\n\n\ntimeString = datetime.datetime.now().strftime(\'%X-%m-%d-%Y\')\n\nif not os.path.exists(timeString):\n    os.makedirs(timeString)\n\nfor x in range(0, partition_num):\n    def filter(row):\n        return (row[0] % partition_num == x)\n    productFeatureShard = productFeatures.filter(filter)\n    np_features_shard = np.array(productFeatureShard.values().collect())\n    np_keys_shard = np.array(productFeatureShard.keys().collect())\n    np_la_norm_shard = np.array(list(map(LA.norm, np_features_shard)))\n    str_x = str(x)\n    productFeaturesFileName = \'productFeatures_\'+str_x+"".npy""\n    prod_feature_norm_FileName = \'prod_feature_norm_\'+str_x+"".npy""\n    repo_ids_FileName = \'repo_ids_\'+str_x+"".npy""\n    np.save(timeString+""/""+productFeaturesFileName, np_features_shard)\n    np.save(timeString+""/""+prod_feature_norm_FileName, list(np_la_norm_shard))\n    np.save(timeString+""/""+repo_ids_FileName, np_keys_shard)\n    s3.Bucket(OUTPUT_BUCKET_NAME).upload_file(timeString+""/""+productFeaturesFileName,timeString+""/""+productFeaturesFileName)\n    s3.Bucket(OUTPUT_BUCKET_NAME).upload_file(timeString+""/""+prod_feature_norm_FileName,timeString+""/""+prod_feature_norm_FileName)\n    s3.Bucket(OUTPUT_BUCKET_NAME).upload_file(timeString+""/""+repo_ids_FileName,timeString+""/""+repo_ids_FileName)\n\nconfig[""body""][""shardNumber""] = partition_num\nconfig[""body""][""bucketInfo""][""key""] = timeString\ncollection.update({""config_name"":""product_vector_config""},config,upsert=True)\n'"
als_thing/online_recommend_repo.py,5,"b""import numpy as np\nfrom numpy import linalg as LA\n\nnp_features = np.load('productFeatures.npy')\nnp_la_norm = np.load('prod_feature_norm.npy')\n\ndef similar(repoid):\n    if np_features:\n        pass\n\ndef handle(event, context):\n    np_features = np.load('productFeatures.npy')\n    np_la_norm = np.load('prod_feature_norm.npy')\n\n    print(np_features.shape)\n\n    def compute_recommend_repo(repo_idx):\n        return np.argsort(np_features.dot(np_features[repo_idx]) / (np_la_norm * np_la_norm[repo_idx]))\n\n    return compute_recommend_repo(2)[:5]\n\nif __name__ == '__main__':\n    handle(None, None)\n"""
als_thing/user_base_recommend.py,0,"b'import boto3\nimport pandas as pd\nimport os\nimport findspark\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import reverse\nfrom pyspark.mllib.recommendation import ALS\nfrom pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry, RowMatrix\nimport numpy as np\nfrom numpy import linalg as LA\nimport datetime\nfrom pymongo import MongoClient\nimport json\n\nsecret_name = ""mongodb_credential""\nendpoint_url = ""https://secretsmanager.us-east-1.amazonaws.com""\nregion_name = ""us-east-1""\n\nsession = boto3.session.Session()\nclient = session.client(\n    service_name=\'secretsmanager\',\n    region_name=region_name,\n    endpoint_url=endpoint_url\n)\n\nget_secret_value_response = client.get_secret_value(\n    SecretId=secret_name\n)\n\nsecret = get_secret_value_response[\'SecretString\']\nconfig_json = json.loads(secret)\n\nMONGO_URL = config_json[""MONGO_URL""]\nMONGO_DB_NAME = config_json[""MONGO_DB_NAME""]\nclient = MongoClient(MONGO_URL)\ndb = client[MONGO_DB_NAME]\ncollection = db[\'global_config\']\nconfig = collection.find_one({""config_name"":""product_vector_config""})\n\n# PARTITION_SIZE = config[""body""][""shard_size""]\n# SPARK_HOME = ""/usr/lib/spark""\nOUTPUT_BUCKET_NAME = config[""body""][""bucketInfo""][""bucketName""]\n\nINPUT_BUCKET_NAME = config[""body""][""bucketInfo""][""inputBucketName""]\nKEY = \'16kUser.csv\'\ns3 = boto3.resource(\'s3\')\ns3.Bucket(INPUT_BUCKET_NAME).download_file(KEY, \'16kUser.csv\')\n\ntrain_file = pd.read_csv(\'16kUser.csv\', index_col=None, sep=\',\',header=None)\ntrain_file.columns = [\'uid\', \'repoid\']\ntrain_file = train_file.drop_duplicates()\ntrain_file.dropna(inplace=True)\ntrain_file.repoid = train_file.repoid.astype(int)\n# train_file.head()\n\nSPARK_HOME = ""/usr/lib/spark""\nos.environ[""SPARK_HOME""] = SPARK_HOME\n# findspark.init(""/usr/lib/spark"")\n\n# train_file.columns = [\'uid\', \'repoid\', \'uname\', \'reponame\', \'date\']\ntrain_file.columns = [\'uid\', \'repoid\']\ntrain_file = train_file.drop_duplicates()\ntrain_file.dropna(inplace=True)\ntrain_file.repoid = train_file.repoid.astype(int)\n#train_file.head()\n\n#-------------------------------------------------\n#user rdd create\n\n\n\nconfig = collection.find_one({""config_name"":""register_user""})\n\n# PARTITION_SIZE = config[""body""][""shard_size""]\n# SPARK_HOME = ""/usr/lib/spark""\nOUTPUT_BUCKET_NAME = config[""body""][""bucketInfo""][""bucketName""]\n\nINPUT_BUCKET_NAME = config[""body""][""bucketInfo""][""inputBucketName""]\nKEY = \'registerd_user_star_repo.csv\'\ns3 = boto3.resource(\'s3\')\ns3.Bucket(INPUT_BUCKET_NAME).download_file(KEY, \'registerd_user_star_repo.csv\')\n\n\nuser_file = pd.read_csv(\'registerd_user_star_repo.csv\', index_col=None, sep=\',\',header=None)\nuser_file.columns = [\'uid\', \'repoid\']\nuser_file = user_file.drop_duplicates()\nuser_file.dropna(inplace=True)\nuser_file.repoid = user_file.repoid.astype(int)\n\n\n#the users that have alreday register out web app.\n\n\n\n#------------\ntrain_file = train_file[[\'uid\', \'repoid\']]\n#train_file.head()\nuser_file = user_file[[\'uid\', \'repoid\']]\n\n\n# import findspark\n# findspark.init(""/usr/local/spark-2.3.0"")\n\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\n\nfrom pyspark.mllib.recommendation import ALS\n\nsc = SparkContext(appName=""RR"")\nsqlContext = SQLContext(sc)\n\ndef first_two_column(row):\n    return (int(row[0]), int(row[1]), 1.0)\n\ntraining_rdd = sqlContext.createDataFrame(train_file).rdd\ntraining_rdd = training_rdd \\\n    .map(first_two_column)\n\n\nuser_rdd = sqlContext.createDataFrame(user_file).rdd\n#user_rdd.take(5)\n\n# this takes a short while too\n# model = ALS.trainImplicit( \\\n#     training_rdd,\n#     rank=16,\n#     iterations=10,\n#     lambda_=0.1,\n#     alpha=80.0\n# )\n\n# append user to training set and train again\ntraining_rdd = training_rdd.union(user_rdd.map(first_two_column))\n# this takes a short while too\nmodel = ALS.trainImplicit( \\\n    training_rdd,\n    rank=16,\n    iterations=10,\n    lambda_=0.1,\n    alpha=80.0\n)\n\nuser_id_file = user_file[[\'uid\']].drop_duplicates()\n#user_id_file.head()\nuser_id_rdd = sqlContext.createDataFrame(user_id_file).rdd\n#user_id_rdd.take(10)\n\n\n\n#--------------------------------------------\n#output to mongo database\n# from pymongo import MongoClient\n#mongodb://super:tKoJDjMtgcFPD_2Pfet@ds247587.mlab.com:47587/twinkeydow\n\n# try:\n#     conn = MongoClient(""mongodb://super:tKoJDjMtgcFPD_2Pfet@ds247587.mlab.com:47587/twinkeydow"")\n#     print(""Connected successfully!!!"")\n# except:\n#     print(""Could not connect to MongoDB"")\n\n# db = conn[\'twinkeydow\']\n\n# Created or Switched to collection names: my_gfg_collection\n# collection = db[\'userbaserec\']\ncollection = db[\'user_recommend\']\n\nrecomend_num = 5\nnum = user_id_rdd.count()\nfor i in range(num):\n    #print(user_id_rdd.take(num)[i].uid)\n    uid = user_id_rdd.take(num)[i].uid\n    user_products = model.recommendProducts(uid, recomend_num)\n    rec_id = []\n    for obj in user_products:\n        rec_id.append(obj.product)\n    record = {\n        ""_id"": uid,\n        ""rids"": rec_id,\n    }\n    rec = collection.update_one({""_id"": uid}, {""$set"":record}, upsert=True)\n'"
api/sim_repo/handler.py,4,"b'import json\nimport sys\nimport os\nfrom io import BytesIO\n\nimport boto3\nfrom pymongo import MongoClient\nimport numpy as np\nfrom numpy import linalg as LA\n\n# code generated from AWS Secret Manager\ndef get_db_credentials():\n    secret_name = ""mongodb_credential""\n    endpoint_url = ""https://secretsmanager.us-east-1.amazonaws.com""\n    region_name = ""us-east-1""\n\n    session = boto3.session.Session()\n    client = session.client(\n        service_name=\'secretsmanager\',\n        region_name=region_name,\n        endpoint_url=endpoint_url\n    )\n\n    get_secret_value_response = client.get_secret_value(\n        SecretId=secret_name\n    )\n    secret = get_secret_value_response[\'SecretString\']\n    config_json = json.loads(secret)\n    return config_json[""MONGO_URL""], config_json[""MONGO_DB_NAME""]\n\nMONGO_URL, MONGO_DB_NAME = get_db_credentials()\ndb = MongoClient(MONGO_URL).get_database(MONGO_DB_NAME)\n\n# env var -> mongo -> (bucketName, bucketKey, shardNumber)\ndef load_config():\n    # {\n    #   ""body"": {\n    #     ""bucketInfor"": {\n    #       ""bucketName"": ""cc-spark-product-feature-output"",\n    #       ""key"": ""02:15:25-04-12-2018 ""\n    #     },\n    #     ""shardNumber"": 6\n    #   },\n    #   ""config_name"": ""product_vector_config""\n    # }\n    try:\n        config = db.global_config.find_one({\'config_name\': \'product_vector_config\'}) or {}\n        body = config.get(\'body\', {})\n        bucketInfo = body.get(\'bucketInfo\', {})\n        bucketName = bucketInfo.get(\'bucketName\')\n        bucketKey = bucketInfo.get(\'key\')\n        shardNumber = int(body.get(\'shardNumber\'))\n        return bucketName, bucketKey, shardNumber\n    except Exception as err:\n        print(err)\n        return None, None, None\n\ndef load_feats(rid, bucketName, bucketKey, shardNumber):\n    try:\n        # read s3 file directly into memory\n        # code from https://dluo.me/s3databoto3\n        client = boto3.client(\'s3\')\n        def read_s3_into_np_arr(fname):\n            obj = client.get_object(Bucket=bucketName, Key=bucketKey + \'/\' + fname)\n            return np.load(BytesIO(obj[\'Body\'].read()))\n\n        # partition with rid is suppoed to be at `rid % shardNumber`\n        partition_id = str(rid % shardNumber)\n        prod_feature_norm = read_s3_into_np_arr(\'prod_feature_norm_\'+partition_id+"".npy"")\n        productFeatures = read_s3_into_np_arr(\'productFeatures_\'+partition_id+"".npy"")\n        repo_ids = read_s3_into_np_arr(\'repo_ids_\'+partition_id+"".npy"")\n\n        return prod_feature_norm, productFeatures, repo_ids\n    except Exception as err:\n        print(err)\n        return None\n\ndef compute_recommend_repo(repo_id, feats, limit):\n    prod_feature_norm, productFeatures, repo_ids = feats\n    repo_idies = np.where(repo_ids == repo_id)[0]\n    \n    if np.sum(repo_idies.shape) == 0:\n        print(\'untrained repo id\', repo_id)\n        return [], []\n    repo_idx = repo_idies[0]\n    sim_vals = productFeatures.dot(productFeatures[repo_idx]) / (prod_feature_norm * prod_feature_norm[repo_idx])\n    sim_vals_idx_sorted = np.argsort(-sim_vals)[1:limit+1]\n    return sim_vals[sim_vals_idx_sorted], repo_ids[sim_vals_idx_sorted]\n\ndef handle(event, context):\n    def response(status, body):\n        return {\n            ""headers"": { ""Access-Control-Allow-Origin"": ""*"" },\n            ""statusCode"": status,\n            ""body"": json.dumps(body)\n        }\n    \n    # 0. param\n    rid = event.get(\'queryStringParameters\', {}).get(\'rid\')\n    limit = event.get(\'queryStringParameters\', {}).get(\'limit\', 5)\n    if rid is None:\n        return response(400, ""rid required"")\n    rid = int(rid)\n    limit = int(limit)\n\n    # 1. load config from mongo\n    bucketName, bucketKey, shardNumber = load_config()\n    if bucketName is None or bucketKey is None or shardNumber is None:\n        return response(500, ""load config failed"")\n\n    # 2. download\n    feats = load_feats(rid, bucketName, bucketKey, shardNumber)\n    if feats is None:\n        return response(500, \'unable to load resources\')\n\n    sim_vals, sim_rids = compute_recommend_repo(rid, feats, limit)\n    return response(200, \\\n        list(map(lambda rr: { ""rid"": int(rr[0]), ""sim"": rr[1] }, zip(sim_rids, sim_vals))) \\\n        )\n\n\ndef main():\n    def eve(queryStringParameters):\n        return { \'queryStringParameters\': queryStringParameters }\n    print(handle(eve({ \'rid\': 1 }), None))\n    print(handle(eve({ \'rid\': u\'3605299\' }), None))\n    print(handle(eve({ \'rid\': u\'3605299\', \'limit\': 1 }), None))\n\nif __name__ == \'__main__\':\n    main()\n'"
