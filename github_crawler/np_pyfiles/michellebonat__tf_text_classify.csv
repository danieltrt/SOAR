file_path,api_count,code
ds_classify_3/classify.py,0,"b""# 1) Import libraries and modules\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.externals import joblib\n\n# 2) Load data from remote url\n\ndataset_url = 'https://s3.amazonaws.com/worldquant-dropbox/datasimply_export.csv'\ndata = pd.read_csv(dataset_url)\n\n# Uncomment this to print first 5 rows of data\n# print data.head()\n\n# Uncomment this to print the data shape\n# print data.shape\n\n# Uncomment this to print data overview summary stats\nprint data.describe()"""
tf_text_classifier/textclassifier.py,0,"b'import tensorflow as tf\nfrom tensorflow import keras\n\nimport numpy as np\n\nprint(tf.__version__)\n\n# Download IMDB dataset of 50k movie reviews\n# num_words=10000 keeps only the top 10,000 most frequently occurring words in the training data\nimdb = keras.datasets.imdb\n\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n\n# Explore the data\n# Convert text to integers\n# print(""Training entries: {}, labels: {}"".format(len(train_data), len(train_labels)))\n# Print first movie review\nprint(train_data[0])\n'"
tf_text_classify/classify.py,0,"b'import tensorflow as tf\nfrom tensorflow import keras\n\nimport numpy as np\n\n# print(tf.__version__)\n\n# Download the IMDB dataset, keeping the top 10,000 most frequently occurring words in the training data\nimdb = keras.datasets.imdb\n\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n\n# Explore the data\n# Training entries\n# print(""Training entries: {}, labels: {}"".format(len(train_data), len(train_labels)))\n\n# The first review as integers\n# print(train_data[0])\n\n# See how many words in the first and second reviews\n# Note: inputs to a neural network must be the same length, we\'ll need to resolve this later\n# len(train_data[0]), len(train_data[1])\n\n# Convert integers back to words\n# Creates a helper function to query a dictionary object that contains the integer to string mapping\n# A dictionary mapping words to an integer index\nword_index = imdb.get_word_index()\n\n# The first indices are reserved\nword_index = {k:(v+3) for k,v in word_index.items()}\nword_index[""<PAD>""] = 0\nword_index[""<START>""] = 1\nword_index[""<UNK>""] = 2  # unknown\nword_index[""<UNUSED>""] = 3\n\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return \' \'.join([reverse_word_index.get(i, \'?\') for i in text])\n\n# Use the decode_review function to display the text for the first review\ndecode_review(train_data[0])\n\n# Prepare the data\n# Convert the reviews (arrays of integers) into tensors to be fed into neural network using pad_sequences\ntrain_data = keras.preprocessing.sequence.pad_sequences(train_data,\n                                                        value=word_index[""<PAD>""],\n                                                        padding=\'post\',\n                                                        maxlen=256)\n\ntest_data = keras.preprocessing.sequence.pad_sequences(test_data,\n                                                       value=word_index[""<PAD>""],\n                                                       padding=\'post\',\n                                                       maxlen=256)\n\n# Look at the length of the examples now, they should be the same as each other\nlen(train_data[0]), len(train_data[1])\n\n# Look at the first review again, it is now padded, standardized length\nprint(train_data[0])\n\n# Build the model\n# Input data consists of array of word-indices. The labels to predict are either 0 or 1\n# Input shape is the vocabulary count used for the movie reviews (10,000 words)\nvocab_size = 10000\n\nmodel = keras.Sequential()\nmodel.add(keras.layers.Embedding(vocab_size, 16))\nmodel.add(keras.layers.GlobalAveragePooling1D())\nmodel.add(keras.layers.Dense(16, activation=tf.nn.relu))\nmodel.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\nmodel.summary()\n\n# Configure the model to use a loss function and optimizer\nmodel.compile(optimizer=tf.train.AdamOptimizer(),\n              loss=\'binary_crossentropy\',\n              metrics=[\'accuracy\'])\n\n# Create a validation set from the training data\nx_val = train_data[:10000]\npartial_x_train = train_data[10000:]\n\ny_val = train_labels[:10000]\npartial_y_train = train_labels[10000:]\n\n# Train the model\nhistory = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=40,\n                    batch_size=512,\n                    validation_data=(x_val, y_val),\n                    verbose=1)\n\n# Evaluate the model\n# results = model.evaluate(test_data, test_labels)\n\n# print(results)\n\n# Create a graph of accuracy and loss over time\n# Dictionary with an entry for each of the four metrics monitored during training and validation\nhistory_dict = history.history\nhistory_dict.keys()\n\n# Plot the training and validation loss for comparison, as well as the training and validation accuracy\nimport matplotlib.pyplot as plt\n\nacc = history.history[\'acc\']\nval_acc = history.history[\'val_acc\']\nloss = history.history[\'loss\']\nval_loss = history.history[\'val_loss\']\n\nepochs = range(1, len(acc) + 1)\n\n# Plot training and validation loss\n# ""bo"" is for ""blue dot""\n# plt.plot(epochs, loss, \'bo\', label=\'Training loss\')\n# b is for ""solid blue line""\n# plt.plot(epochs, val_loss, \'b\', label=\'Validation loss\')\n# plt.title(\'Training and validation loss\')\n# plt.xlabel(\'Epochs\')\n# plt.ylabel(\'Loss\')\n# plt.legend()\n\n# plt.show()\n\n# Plot training and validation accuracy\nplt.clf()   # clear figure\nacc_values = history_dict[\'acc\']\nval_acc_values = history_dict[\'val_acc\']\n\nplt.plot(epochs, acc, \'bo\', label=\'Training acc\')\nplt.plot(epochs, val_acc, \'b\', label=\'Validation acc\')\nplt.title(\'Training and validation accuracy\')\nplt.xlabel(\'Epochs\')\nplt.ylabel(\'Accuracy\')\nplt.legend()\n\nplt.show()'"
txt_classifier_tf/classifier_with_tf.py,0,b'import tensorflow as tf\nfrom tensorflow import keras\n\nimport numpy as np\n\nprint(tf.__version__)\n'
