file_path,api_count,code
algorithm/linear-regression/ScatterGraph.py,2,"b'# coding: utf-8\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef draw():\n    X = np.linspace(0, 10, 50)\n    noise = np.random.normal(0, 0.5, X.shape)\n    Y = X * 0.5 + 3 + noise\n\n    plt.scatter(X, Y)\n    plt.savefig(""scattergraph.png"")\n    plt.show()\n\nif __name__ == ""__main__"":\n    draw()'"
algorithm/linear-regression/__init__.py,0,b''
algorithm/linear-regression/gradientDescentLR.py,6,"b'# coding: utf-8\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nN = 200\n\nX = np.linspace(0, 10, N * 2)\nnoise = np.random.normal(0, 0.5, X.shape)\nY = X * 0.5 + 3 + noise\n\n\ndef calcLoss(train_X, train_Y, W, b):\n    return np.sum(np.square(train_Y - (train_X * W + b)))\n\ndef gradientDescent(train_X, train_Y, W, b, learningrate=0.001, trainingtimes=500):\n    global loss\n    global W_trace\n    global b_trace\n    size = train_Y.size\n    for _ in range(trainingtimes):\n        prediction = W * train_X + b\n        tempW = W + learningrate * np.sum(train_X * (train_Y - prediction)) / size\n        tempb = b + learningrate * np.sum(train_Y - prediction) / size\n        W = tempW\n        b = tempb\n        loss.append(calcLoss(train_X, train_Y, W, b))\n        W_trace.append(W)\n        b_trace.append(b)\n\n\nTraining_Times = 100\nLearning_Rate = 0.002\n\nloss = []\nW_trace = [-1]\nb_trace = [1]\ngradientDescent(X, Y, W_trace[0], b_trace[0], learningrate=Learning_Rate, trainingtimes=Training_Times)\nprint(W_trace[-1], b_trace[-1])\n\nfig = plt.figure()\nplt.title(r\'$loss\\ function\\ change\\ tendency$\')\nplt.xlabel(r\'$learning\\ times$\')\nplt.ylabel(r\'$loss\\ value$\')\nplt.plot(np.linspace(1, Training_Times, Training_Times), loss)\nplt.savefig(""gradientDescentLR.png"")\nplt.show()\n'"
algorithm/linear-regression/lossGraph.py,8,"b'# coding: utf-8\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure()\nax = Axes3D(fig)\nax.set_xlabel(r\'$Weight$\')\nax.set_ylabel(r\'$bias$\')\nax.set_zlabel(r\'$loss\\ value$\')\nN = 200\n\nX = np.linspace(0, 10, N*2)\nnoise = np.random.normal(0, 0.1, X.shape)\nY = X * 0.5 + 3 + noise\n\nW = np.linspace(-10, 10, N)\nb = np.linspace(-2, 8, N)\n\np = 0.6\nplt.xlim(W.min() * p, W.max() * p)\nplt.ylim(b.min() * p, b.max() * p)\nax.set_zlim(0, 1000)\n\nh = []\nfor i in W:\n    for j in b:\n        h.append(np.sum(np.square(Y - (X * i + j))))\n\nprint(np.min(h))\nh = np.asarray(h).reshape(N, N)\nW, b = np.meshgrid(W, b)\n\nax.scatter(W, b, h, c=\'b\')\n# plt.savefig(""lossgraph.png"")\n\nplt.show()\n'"
algorithm/linear-regression/matrixLR.py,7,"b'# coding: utf-8\n\nimport numpy as np\n\nX1 = np.asarray([2104, 1416, 1534, 852]).reshape(4, 1)\nX2 = np.asarray([5, 3, 3, 2]).reshape(4, 1)\nX3 = np.asarray([1, 2, 2, 1]).reshape(4, 1)\n\nX = np.mat(np.column_stack((X1, X2, X3, np.ones(shape=(4, 1)))))\nnoise = np.random.normal(0, 0.1, X1.shape)\nY = np.mat(2.5 * X1 - X2 + 2 * X3 + 4 + noise)\nYTwin = np.mat(2.5 * X1 - X2 + 2 * X3 + 4)\n\nW = (X.T * X).I * X.T * Y\nWTWin = (X.T * X).I * X.T * YTwin\nprint(W, ""\\n"", WTWin)\n'"
algorithm/linear-regression/quadraticFunction.py,4,"b'# coding: utf-8\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig = plt.figure()\n\nN = 200\n\nX = np.linspace(0, 10, N * 2)\nnoise = np.random.normal(0, 0.1, X.shape)\nY = X * 0.5 + 3 + noise\n\nW = 1\nb = np.linspace(-5, 6, N)\n\nh = []\nfor i in b:\n    h.append(np.sum(np.square(Y - (X * W + i))))\n\nplt.plot(b, h)\n\n# plt.savefig(""quadraticFunction.png"")\n\nplt.show()'"
algorithm/linear-regression/tensorflowLR.py,9,"b'# coding: utf-8\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nN = 1000\ntrain_X1 = np.linspace(1, 10, N).reshape(N, 1)\ntrain_X2 = np.linspace(1, 10, N).reshape(N, 1)\ntrain_X3 = np.linspace(1, 10, N).reshape(N, 1)\ntrain_X4 = np.linspace(1, 10, N).reshape(N, 1)\n\n# train_X = np.column_stack((train_X1, np.ones(shape=(N, 1))))\ntrain_X = np.column_stack((train_X1, train_X2, train_X3, train_X4, np.ones(shape=(N, 1))))\n\nnoise = np.random.normal(0, 0.5, train_X1.shape)\n# train_Y = 3 * train_X1 + 4\ntrain_Y = train_X1 + train_X2 + train_X3 + train_X4 + 4 + noise\n\nlength = len(train_X[0])\n\nX = tf.placeholder(tf.float32, [None, length], name=""X"")\nY = tf.placeholder(tf.float32, [None, 1], name=""Y"")\n\nW = tf.Variable(np.random.random(size=length).reshape(length, 1), dtype=tf.float32, name=""weight"")\n\nactivation = tf.matmul(X, W)\nlearning_rate = 0.006\n\nloss = tf.reduce_mean(tf.reduce_sum(tf.pow(activation - Y, 2), reduction_indices=[1]))\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n\ntraining_epochs = 2000\ndisplay_step = 100\n\nloss_trace = []\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(training_epochs):\n        sess.run(optimizer, feed_dict={X: train_X, Y: train_Y})\n        temp_loss = sess.run(loss, feed_dict={X: train_X, Y: train_Y})\n        loss_trace.append(temp_loss)\n        if 1 == epoch % display_step:\n            print(\'epoch: %4s\'%epoch, \'\\tloss: %s\'%temp_loss)\n    print(""\\nOptimization Finished!"")\n    print(""\\nloss = "", loss_trace[-1], ""\\nWeight =\\n"", sess.run(W, feed_dict={X: train_X, Y: train_Y}))\n\n\nplt.plot(np.linspace(0, 100, 100), loss_trace[:100])\n# plt.savefig(""tensorflowLR.png"")\nplt.show()\n\n# output:\n# epoch:    1 \tloss: 118.413925\n# epoch:  101 \tloss: 1.4500043\n# epoch:  201 \tloss: 1.0270562\n# epoch:  301 \tloss: 0.75373846\n# epoch:  401 \tloss: 0.5771168\n# epoch:  501 \tloss: 0.46298113\n# epoch:  601 \tloss: 0.38922414\n# epoch:  701 \tloss: 0.34156123\n# epoch:  801 \tloss: 0.31076077\n# epoch:  901 \tloss: 0.29085675\n# epoch: 1001 \tloss: 0.27799463\n# epoch: 1101 \tloss: 0.26968285\n# epoch: 1201 \tloss: 0.2643118\n# epoch: 1301 \tloss: 0.26084095\n# epoch: 1401 \tloss: 0.2585978\n# epoch: 1501 \tloss: 0.25714833\n# epoch: 1601 \tloss: 0.25621164\n# epoch: 1701 \tloss: 0.2556064\n# epoch: 1801 \tloss: 0.2552152\n# epoch: 1901 \tloss: 0.2549625\n# Optimization Finished!\n# loss =  0.25480175\n# Weight =\n#  [[1.0982682 ]\n#  [0.9760315 ]\n#  [1.0619627 ]\n#  [0.87049955]\n#  [3.9700394 ]]\n'"
algorithm/logistic-regression/__init__.py,0,b''
algorithm/logistic-regression/linear2LogisticreGressionGraph.py,8,"b'# coding: utf-8\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# x = [1, 2, 3, 4, 6, 7, 8, 9, 10]\n# y = [0, 0, 0, 0, 1, 1, 1, 1, 1]\n\n# train_X = np.asarray(x)\n# train_Y = np.asarray(y)\n\nfig = plt.figure()\nplt.xlim(-1, 12)\nplt.ylim(1, 5)\n# plt.scatter(train_X, train_Y)\n\n# s_X = np.linspace(-2, 12, 100)\n# s_Y = 1/(1 + np.power(np.e, -6*(s_X - 5)))\n\nx = np.asarray([1, 1, 2, 1.3, 2.4, 4, 3.4, 4.4, 6, 4, 5, 6.2, 7, 7.7])\ny = np.asarray([2, 3, 2.4, 3, 2.4, 4, 3, 2, 2.5, 4.4, 3.5, 3, 4, 2.5])\n\nfor _x, _y in zip(x, y):\n    if _x + _y > 7:\n        plt.plot(np.asarray([_x]), np.asarray([_y]), \'b^\')\n    else:\n        plt.plot(np.asarray([_x]), np.asarray([_y]), \'ro\')\n\n# plt.plot(s_X, s_Y)\n# plt.savefig(""linear2LogisticreGressionGraphAddScatter.png"")\n# plt.savefig(""linear2LogisticreGressionGraphAddSigmoid.png"")\n# plt.savefig(""linear2LogisticreGressionGraph.png"")\nplt.show()\n'"
algorithm/logistic-regression/linear2LogisticreGressionGraph2.py,4,"b'# coding: utf-8\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 6, 7, 8, 9, 10]\ny = [0, 0, 0, 0, 1, 1, 1, 1, 1]\n\ntrain_X = np.asarray(x)\ntrain_Y = np.asarray(y)\n\nfig = plt.figure()\nplt.xlim(-1, 12)\nplt.ylim(-0.5, 1.5)\nplt.scatter(train_X, train_Y)\n\ns_X = np.linspace(-2, 12, 100)\ns_Y = 1/(1 + np.power(np.e, -2*(s_X - 4)))\n\nplt.plot(s_X, s_Y)\n# plt.savefig(""linear2LogisticreGressionGraphAddSigmoid3.png"")\n# plt.savefig(""linear2LogisticreGressionGraphAddSigmoid2.png"")\nplt.show()\n'"
algorithm/logistic-regression/logisticreGression.py,12,"b'# coding: utf-8\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.animation as animation\n\nx = [1, 2, 3, 4, 6, 7, 8, 9, 10]\ny = [0, 0, 0, 0, 1, 1, 1, 1, 1]\n\ntrain_X = np.asarray(np.row_stack((np.ones(shape=(1, len(x))), x)), dtype=np.float64)\ntrain_Y = np.asarray(y, dtype=np.float64)\ntrain_W = np.asarray([-1, -1], dtype=np.float64).reshape(1, 2)\n\n\ndef sigmoid(X):\n    return 1 / (1 + np.power(np.e, -(X)))\n\n\ndef lossfunc(X, Y, W):\n    n = len(Y)\n    return (-1 / n) * np.sum(Y * np.log(sigmoid(np.matmul(W, X))) + (1 - Y) * np.log((1 - sigmoid(np.matmul(W, X)))))\n\n\nTraining_Times = 100000\nLearning_Rate = 0.3\n\nloss_Trace = []\nw_Trace = []\nb_Trace = []\n\n\ndef gradientDescent(X, Y, W, learningrate=0.001, trainingtimes=500):\n    n = len(Y)\n    for i in range(trainingtimes):\n        W = W - (learningrate / n) * np.sum((sigmoid(np.matmul(W, X)) - Y) * X, axis=1)\n\n        # for GIF\n        if 0 == i % 1000 or (100 > i and 0 == i % 2):\n            b_Trace.append(W[0, 0])\n            w_Trace.append(W[0, 1])\n            loss_Trace.append(lossfunc(X, Y, W))\n    return W\n\n\nfinal_W = gradientDescent(train_X, train_Y, train_W, learningrate=Learning_Rate, trainingtimes=Training_Times)\n\nprint(""Final Weight:"", final_W)\nprint(""Weight details trace: "", np.asarray([b_Trace, w_Trace]))\nprint(""Loss details trace: "", loss_Trace)\n\nfig, ax = plt.subplots()\nax.scatter(np.asarray(x), np.asarray(y))\nax.set_title(r\'$Fitting\\ line$\')\n\n\ndef update(i):\n    try:\n        ax.lines.pop(0)\n    except Exception:\n        pass\n    plot_X = np.linspace(-1, 12, 100)\n    W = np.asarray([b_Trace[i], w_Trace[i]]).reshape(1, 2)\n    X = np.row_stack((np.ones(shape=(1, len(plot_X))), plot_X))\n    plot_Y = sigmoid(np.matmul(W, X))\n    line = ax.plot(plot_X, plot_Y[0], \'r-\', lw=1)\n    ax.set_xlabel(r""$Cost\\ %.6s$"" % loss_Trace[i])\n    return line\n\n\nani = animation.FuncAnimation(fig, update, frames=len(w_Trace), interval=100)\nani.save(\'logisticregression.gif\', writer=\'imagemagick\')\n\nplt.show()\n'"
algorithm/logistic-regression/multiClassGraph.py,18,"b'# coding: utf-8\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.random.normal(10, 4, 100)\ny = np.random.normal(10, 4, 100)\n\nplt.figure()\nfor _x, _y in zip(x, y):\n    if _x ** 2 + _y ** 2 < 200:\n        plt.plot(np.asarray([_x]), np.asarray([_y]), \'b^\')\n    elif - _x + _y > 1:\n        plt.plot(np.asarray([_x]), np.asarray([_y]), \'ro\')\n    else:\n        plt.plot(np.asarray([_x]), np.asarray([_y]), \'gs\')\n# plt.savefig(""multiClass3ToAll.png"")\nplt.show()\n\nplt.figure()\nfor _x, _y in zip(x, y):\n    if _x ** 2 + _y ** 2 < 200:\n        plt.plot(np.asarray([_x]), np.asarray([_y]), \'b^\')\n    elif - _x + _y > 1:\n        plt.plot(np.asarray([_x]), np.asarray([_y]), \'ro\')\n    else:\n        plt.plot(np.asarray([_x]), np.asarray([_y]), \'rs\')\n# plt.savefig(""multiClass3To1.png"")\nplt.show()\n\nplt.figure()\nfor _x, _y in zip(x, y):\n    if _x ** 2 + _y ** 2 < 200:\n        plt.plot(np.asarray([_x]), np.asarray([_y]), \'g^\')\n    elif - _x + _y > 1:\n        plt.plot(np.asarray([_x]), np.asarray([_y]), \'ro\')\n    else:\n        plt.plot(np.asarray([_x]), np.asarray([_y]), \'gs\')\n# plt.savefig(""multiClass3To2.png"")\nplt.show()\n\nplt.figure()\nfor _x, _y in zip(x, y):\n    if _x ** 2 + _y ** 2 < 200:\n        plt.plot(np.asarray([_x]), np.asarray([_y]), \'b^\')\n    elif - _x + _y > 1:\n        plt.plot(np.asarray([_x]), np.asarray([_y]), \'bo\')\n    else:\n        plt.plot(np.asarray([_x]), np.asarray([_y]), \'gs\')\n# plt.savefig(""multiClass3To3.png"")\nplt.show()\n\nplt.figure()\nx = np.random.normal(0, 2, 200)\ny = np.random.normal(0, 2, 200)\nfor _x, _y in zip(x, y):\n    if _x ** 2 + _y ** 2 < 8:\n        plt.plot(np.asarray([_x]), np.asarray([_y]), \'b^\')\n    else:\n        plt.plot(np.asarray([_x]), np.asarray([_y]), \'ro\')\n\nboderparameter = plt.gca()\nboderparameter.spines[\'right\'].set_position((\'data\', 0))\nboderparameter.spines[\'top\'].set_position((\'data\', 0))\nplt.savefig(""circleGraph.png"")\nplt.show()\n'"
algorithm/neural-network/XOR.py,11,"b'# coding: utf-8\n\nimport numpy as np\n\nx1 = np.asarray([0, 0, 1, 1])\nx2 = np.asarray([0, 1, 0, 1])\nX = np.row_stack((np.ones(shape=(1, 4)), x1, x2))\nprint(""X:\\n%s"" % X)\ny = np.asarray([0, 1, 1, 0])\nW1 = np.asarray([[-1, 2, -2],\n                 [-1, -2, 2]])\nW2 = np.asarray([-1, 2, 2])\n\n\ndef sigmoid(input):\n    return 1 / (1 + np.power(np.e, -10 * (input)))\n\n\nnp.set_printoptions(precision=6, suppress=True)\nz1 = np.matmul(W1, X)\nprint(""W1*X = z1:\\n%s"" % z1)\na1 = np.row_stack((np.ones(shape=(1, 4)), sigmoid(z1)))\nprint(""sigmoid(z1) = a1:\\n%s"" % a1)\nz2 = np.matmul(W2, a1)\nprint(""W2*a1 = z2:\\n%s"" % z2)\na2 = sigmoid(z2)\nprint(""------------------------"")\nprint(""prediction: %s"" % a2)\nprint(""target: %s"" % y)\nprint(""------------------------"")\n\n# output:\n# X:\n# [[1. 1. 1. 1.]\n#  [0. 0. 1. 1.]\n#  [0. 1. 0. 1.]]\n# W1*X = z1:\n# [[-1. -3.  1. -1.]\n#  [-1.  1. -3. -1.]]\n# sigmoid(z1) = a1:\n# [[1.       1.       1.       1.      ]\n#  [0.000045 0.       0.999955 0.000045]\n#  [0.000045 0.999955 0.       0.000045]]\n# W2*a1 = z2:\n# [-0.999818  0.999909  0.999909 -0.999818]\n# ------------------------\n# prediction: [0.000045 0.999955 0.999955 0.000045]\n# target: [0 1 1 0]\n# ------------------------\n'"
algorithm/neural-network/XORDemo.py,21,"b'# coding: utf-8\n\nimport numpy as np\n\nk = 2\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.power(np.e, -k * (x)))\n\n\ndef actication(data):\n    return sigmoid(data)\n\n\ndef forward(W, data):\n    z, a = [], []\n    a.append(data)\n    data = np.row_stack(([1], data))\n    for w in W:\n        z.append(np.matmul(w, data))\n        a.append(actication(z[-1]))\n        data = np.row_stack(([1], a[-1]))\n    return z, a\n\n\ndef backward(y, W, z, a, learningrate):\n    length = len(z) + 1\n    Jtoz = k * (1 - y * (1 + np.power(np.e, -(k * z[-1])))) / np.power(np.e, -(k * z[-1]))\n    # print(""loss = %s"" % (-y * np.log(a[-1]) - (1 - y) * np.log(1 - a[-1])))\n    for layer in range(length - 1, 0, -1):\n        i = layer - length\n        if (i != -1):\n            Jtoz = np.matmul(W[i + 1][:, 1:].T, Jtoz) * k * np.power(np.e, -(k * z[i])) / np.power(\n                1 + np.power(np.e, -(k * z[i])), 2)\n        W[i] = W[i] - learningrate * np.matmul(Jtoz, np.row_stack(([1], a[i - 1])).T)\n    return W\n\n\ndef gradientDescent(X, y, shape, learningrate=0.001, trainingtimes=500):\n    W, z, a = [], [], []\n    W.append(np.asarray([[-1, 1, -1], [-1, -1, 1]]))\n    W.append(np.asarray([[-1, 1, 1]]))\n    # for layer in range(len(shape) - 1):\n    #     row = shape[layer + 1]\n    #     col = shape[layer] + 1\n    #     W.append(np.random.normal(0, 1, row * col).reshape(row, col))\n    for i in range(trainingtimes):\n        for x, j in zip(X.T, range(len(X[0]))):\n            z, a = forward(W, np.asarray([x]).T)\n            W = backward(y[j], W, z, a, learningrate)\n    return W\n\n\ndef train():\n    np.set_printoptions(precision=4, suppress=True)\n    x1 = np.asarray([0, 0, 1, 1])\n    x2 = np.asarray([0, 1, 0, 1])\n    X = np.row_stack((x1, x2))\n    y = np.asarray([0, 1, 1, 0])\n    shape = [2, 2, 1]\n    Learning_Rate = 0.1\n    Training_Times = 4000\n    W = gradientDescent(X, y, shape, learningrate=Learning_Rate, trainingtimes=Training_Times)\n\n    print(W)\n    testData = np.row_stack((np.ones(shape=(1, 4)), X))\n    for w in W:\n        testData = np.matmul(w, testData)\n        testData = np.row_stack((np.ones(shape=(1, 4)), actication(testData)))\n    print(testData[1])\n\n\nif __name__ == ""__main__"":\n    train()\n\n# output1:\n# [array([[-8.3273,  5.5208,  5.4758],\n#        [ 2.7417, -5.944 , -5.9745]]), array([[ 18.5644, -22.4426, -22.4217]])]\n# [0.0005 1.     1.     0.0005]\n# [array([[ 3.0903, -6.3961,  6.928 ],\n#        [ 3.0355,  6.7901, -6.2563]]), array([[ 41.2259, -22.2455, -22.0939]])]\n# [0.0024 1.     1.     0.0021]\n# [array([[ 5.3893,  4.913 , -7.0756],\n#        [ 6.2289, -1.3519, -4.7387]]), array([[  9.8004, -20.0023,  10.2014]])]\n# [0.5    1.     0.4995 0.0002]\n\n# output2:\n# [array([[-2.8868,  5.6614, -5.9766],\n#        [-2.9168, -5.9789,  5.6363]]), array([[-2.1866, 21.2065, 21.1815]])]\n# [0.016  1.     1.     0.0142]\n# [array([[-2.9942,  5.7925, -6.0901],\n#        [-3.0228, -6.0924,  5.7687]]), array([[-3.6425, 22.3914, 22.3658]])]\n# [0.0009 1.     1.     0.0008]'"
algorithm/neural-network/__init__.py,0,b''
algorithm/neural-network/tfXOR.py,5,"b'# coding: utf-8\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.power(np.e, -2 * (x)))\n\n\ndef add_layer(inputs, in_size, out_size, activation_function=None, ):\n    Weights = tf.Variable(tf.random_normal([in_size, out_size]))\n    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n    if activation_function is None:\n        outputs = Wx_plus_b\n    else:\n        outputs = activation_function(Wx_plus_b)\n    return outputs\n\n\nif __name__ == ""__main__"":\n    x1 = np.asarray([0, 0, 1, 1])\n    x2 = np.asarray([0, 1, 0, 1])\n    X = np.row_stack((x1, x2))\n    y = np.asarray([0, 1, 1, 0]).reshape(1, 4)\n    data_X = tf.placeholder(tf.float32, [None, 2])\n    data_y = tf.placeholder(tf.float32, [None, 1])\n\n\n    layer_one = add_layer(data_X, 2, 2, activation_function=sigmoid)\n    prediction = add_layer(layer_one, 2, 1, activation_function=sigmoid)\n    # layer_one = add_layer(data_X, 2, 2, activation_function=tf.nn.sigmoid)\n    # prediction = add_layer(layer_one, 2, 1, activation_function=tf.nn.sigmoid)\n\n    loss = tf.reduce_mean(tf.reduce_sum(- data_y * tf.log(prediction) - (1 - data_y) * tf.log(1 - prediction)))\n    train = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in range(4000):\n            sess.run(train, feed_dict={data_X: X.T, data_y: y.T})\n        print(sess.run(prediction, feed_dict={data_X: X.T, data_y: y.T}))\n\n# output:\n# [[0.00200064]\n#  [0.9985947 ]\n#  [0.9985983 ]\n#  [0.00144795]]\n# --------------\n# [[0.01765717]\n#  [0.98598236]\n#  [0.98598194]\n#  [0.0207849 ]]\n# --------------\n# [[0.00104381]\n#  [0.9991435 ]\n#  [0.49951136]\n#  [0.5003463 ]]\n'"
