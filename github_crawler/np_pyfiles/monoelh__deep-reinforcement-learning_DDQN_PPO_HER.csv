file_path,api_count,code
ddqn_framework.py,49,"b'#######################################################################################\r\n# Deep Q - Learning framework to play around with (dueling-, dense- and double q-learning )\r\n# Author: Manuel Hass\r\n# 2017\r\n# \r\n# *uses mlp_framework.py as model framework \r\n# *examples in the end\r\n#######################################################################################\r\n\r\n\r\n### imports\r\nimport numpy as np\r\nimport gym\r\nimport time \r\n\r\n# helper functions\r\ndef train_bellman(onlineDQN, targetDQN, batch, GAMMA):\r\n    \'\'\'\r\n    updates the onlineDQN with target Q values for the greedy action(choosen by onlineDQN)\r\n    \'\'\'\r\n    \r\n    state,action,reward,next_state,done = batch\r\n    Q = onlineDQN.infer(state)\r\n    t = targetDQN.infer(next_state)\r\n    a = np.argmax(onlineDQN.infer(next_state),axis=1)\r\n    Q[range(Q.shape[0]),action.astype(int)] = reward + np.logical_not(done)* GAMMA * t[range(t.shape[0]),a]\r\n    state_batch_ = state\r\n    target_batch_ = Q\r\n\r\n    onlineDQN.train(state_batch_, target_batch_)\r\n\r\ndef update_target(onlineDQN,targetDQN,duel=False):\r\n    \'\'\'\r\n    copies weights from onlineDQN to targetDQN\r\n    \'\'\'    \r\n    if duel:\r\n        for i in range(len(targetDQN.LL0)):\r\n            targetDQN.LL0[i].w = np.copy(onlineDQN.LL0[i].w)\r\n        for i in range(len(targetDQN.LLA)):\r\n            targetDQN.LLA[i].w = np.copy(onlineDQN.LLA[i].w)\r\n        for i in range(len(targetDQN.LLV)):\r\n            targetDQN.LLV[i].w = np.copy(onlineDQN.LLV[i].w)\r\n    else:    \r\n        for i in range(len(targetDQN.Layerlist)):\r\n            targetDQN.Layerlist[i].w = np.copy(onlineDQN.Layerlist[i].w)\r\n            \r\n\r\nclass ringbuffer:\r\n    \'\'\'\r\n    fast ringbuffer for the experience replay (numpy)\r\n    \'\'\'\r\n    def __init__(self,SIZE):\r\n        self.buffer_size = 0\r\n        self.SIZE = SIZE\r\n\r\n        #buffers\r\n        magic = np.ones((1,777))\r\n        self.state_buffer = magic\r\n        self.action_buffer = magic\r\n        self.reward_buffer = magic\r\n        self.next_state_buffer = magic\r\n        self.done_buffer = magic\r\n        self.priorities = magic\r\n\r\n\r\n    def add(self, sample):\r\n        init_flag = False\r\n        if np.array(self.state_buffer).shape[1] == 777: \r\n            self.state_buffer = np.empty((0,sample[0].shape[0]))#[1:]\r\n            self.action_buffer = np.empty((0,1))#[1:]\r\n            self.reward_buffer = np.empty((0,1))#[1:]\r\n            self.next_state_buffer = np.empty((0,sample[3].shape[0]))#[1:]\r\n            self.done_buffer = np.empty((0,1))#[1:]\r\n            self.priorities = np.empty((0,1))#[1:]\r\n            init_flag = True\r\n        self.state_buffer = np.append(self.state_buffer,sample[0][True,:],axis=0)\r\n        self.action_buffer = np.append(self.action_buffer,sample[1].reshape(1,1),axis=0)\r\n        self.reward_buffer = np.append(self.reward_buffer,sample[2].reshape(1,1),axis=0)\r\n        self.next_state_buffer = np.append(self.next_state_buffer,sample[3][True,:],axis=0)\r\n        self.done_buffer = np.append(self.done_buffer,sample[4].reshape(1,1),axis=0)\r\n        #print(np.max(self.priorities),\'maximum prio\')\r\n        new_sample_prio = np.max(self.priorities) if self.priorities.shape[0]>0 and np.max(np.abs(self.priorities))<1e10 else 1.\r\n        #print(new_sample_prio,\'new prio\')\r\n        self.priorities = np.append(self.priorities,np.array([new_sample_prio]).reshape(1,1),axis=0)\r\n        self.priorities /= np.sum(self.priorities)\r\n        #print(np.max(self.priorities),\'maximum prio after\')\r\n        \r\n        self.buffer_size += 1.\r\n        if self.buffer_size > self.SIZE or init_flag :\r\n            self.state_buffer =  self.state_buffer[1:]\r\n            self.action_buffer = self.action_buffer[1:]\r\n            self.reward_buffer = self.reward_buffer[1:]\r\n            self.next_state_buffer = self.next_state_buffer[1:]\r\n            self.done_buffer = self.done_buffer[1:]\r\n            self.priorities = self.priorities[1:]\r\n            init_flag = False\r\n\r\n    def get(self):\r\n        return [self.state_buffer,\r\n        self.action_buffer,\r\n        self.reward_buffer,\r\n        self.next_state_buffer,\r\n        self.done_buffer]\r\n\r\n    def sample(self,BATCHSIZE,prio=False):\r\n        if prio:\r\n            a = self.done_buffer.shape[0]\r\n            c = self.priorities.reshape((a))\r\n            b = c/np.sum(c)\r\n            ind = np.random.choice(np.arange(a),BATCHSIZE,replace=False,p=b).astype(int)\r\n        else: \r\n            ind = np.random.choice(np.arange(self.done_buffer.shape[0]),BATCHSIZE,replace=False).astype(int)\r\n        \r\n        return [self.state_buffer[ind],\r\n                self.action_buffer[ind].reshape(-1),\r\n                self.reward_buffer[ind].reshape(-1),\r\n                self.next_state_buffer[ind],\r\n                self.done_buffer[ind].reshape(-1) ]\r\n    \r\n    def prio_update(self, onlineDQN, targetDQN,epsilon=0.01,alpha=0.6,GAMMA=0.99,CHUNK = 5000.):\r\n        \r\n        #state,action,reward,next_state,done = self.get()\r\n        getbuffer = self.get()\r\n        #CHUNK = 5000. # max number of states used for inference at once\r\n        loops = int(getbuffer[0].shape[0]/CHUNK) # number of loops needed to update all prios\r\n        priobuffer = np.empty((0))\r\n        j = -1\r\n\r\n        for j in range(loops): # if replaybuffer size bigger than CHUNK size\r\n            state,action,reward,next_state,done = [x[int(j*CHUNK):int((j+1)*CHUNK)] for x in getbuffer]\r\n            Q = onlineDQN.infer(state)\r\n            Q_ = np.copy(Q)\r\n            t = targetDQN.infer(next_state)\r\n            a = np.argmax(onlineDQN.infer(next_state),axis=1)\r\n            Q[range(Q.shape[0]),action.astype(int)] = reward + np.logical_not(done)* GAMMA * t[range(t.shape[0]),a]\r\n            TD_loss = np.abs((Q_-Q))\r\n            TD_loss = TD_loss[range(TD_loss.shape[0]),a]\r\n            prio =  np.power((TD_loss+epsilon),alpha)\r\n            prio /= np.sum(prio)\r\n            priobuffer = np.append(priobuffer,prio)\r\n            \r\n        state,action,reward,next_state,done = [x[int((j+1)*CHUNK):] for x in getbuffer]\r\n        Q = onlineDQN.infer(state)\r\n        Q_ = np.copy(Q)\r\n        t = targetDQN.infer(next_state)\r\n        a = np.argmax(onlineDQN.infer(next_state),axis=1)\r\n        Q[range(Q.shape[0]),action.astype(int)] = reward + np.logical_not(done)* GAMMA * t[range(t.shape[0]),a]\r\n        TD_loss = np.abs((Q_-Q))\r\n        TD_loss = TD_loss[range(TD_loss.shape[0]),a]\r\n        prio =  np.power((TD_loss+epsilon),alpha)\r\n        prio /= np.sum(prio)\r\n\r\n        priobuffer = np.append(priobuffer,prio)\r\n        self.priorities = priobuffer[:,True]\r\n        \r\n\r\n\r\nclass trainer_config:\r\n    \'\'\'\r\n    configuration for the Q learner (trainer) for easy reuse\r\n    everything not model related goes here. maybe \r\n    \'\'\'\r\n    def __init__(self,\r\n        game_name=\'Acrobot-v1\',\r\n        BUFFER_SIZE = 50e3,\r\n        STEPS_PER_EPISODE = 500,\r\n        MAX_STEPS = 100000,   \r\n        UPDATE_TARGET_STEPS = 1000,\r\n        BATCH_SIZE = 32,\r\n        GAMMA = 0.99,\r\n        EXPLORATION = 100,\r\n        E_MIN = 0.01,\r\n        priority = False,\r\n        alpha = 0.6,\r\n        epsilon = 0.01\r\n\r\n        ):\r\n        ### game environment\r\n        self.game_name = game_name\r\n        \r\n        ### world variables for model building\r\n        env = gym.make(game_name).env\r\n        self.INPUT_SIZE = env.observation_space.shape[0]\r\n        self.OUTPUT_SIZE = env.action_space.n\r\n        env.close()\r\n        ### training variables\r\n        self.BUFFER_SIZE = BUFFER_SIZE\r\n        self.STEPS_PER_EPISODE = STEPS_PER_EPISODE   \r\n        self.MAX_STEPS = MAX_STEPS\r\n        self.UPDATE_TARGET_STEPS = UPDATE_TARGET_STEPS\r\n        self.BATCH_SIZE = BATCH_SIZE\r\n        self.GAMMA = GAMMA\r\n        self.EXPLORATION = EXPLORATION\r\n        self.E_MIN = E_MIN\r\n        #### PRIO MODULE ( default := alpha= 0.,epsilon=0.01)\r\n        self.priority = priority\r\n        self.alpha = alpha\r\n        self.epsilon = epsilon\r\n\r\nclass trainer:\r\n    \'\'\'\r\n    the actual DDQN-> 2 models, 1 config\r\n    train here, get your models and plots\r\n    \'\'\'\r\n    def __init__(self,onlineModel,targetModel,trainer_config):\r\n        ### load config \r\n        self.game_name = trainer_config.game_name\r\n        self.env = gym.make(self.game_name).env\r\n\r\n        ### training variables\r\n        self.BUFFER_SIZE = trainer_config.BUFFER_SIZE\r\n        self.STEPS_PER_EPISODE = trainer_config.STEPS_PER_EPISODE\r\n        self.MAX_STEPS = trainer_config.MAX_STEPS\r\n        self.UPDATE_TARGET_STEPS = trainer_config.UPDATE_TARGET_STEPS\r\n        self.BATCH_SIZE = trainer_config.BATCH_SIZE\r\n        self.GAMMA = trainer_config.GAMMA\r\n        self.EXPLORATION = trainer_config.EXPLORATION\r\n        self.E_MIN = trainer_config.E_MIN\r\n        self.priority = trainer_config.priority\r\n        self.alpha = trainer_config.alpha\r\n        self.epsilon = trainer_config.epsilon\r\n\r\n        ### models\r\n        self.onlineNet = onlineModel\r\n        self.targetNet = targetModel\r\n\r\n        ### logs\r\n        self.reward_plot = []\r\n        self.loss_plot = []\r\n\r\n        ### ringbuffer\r\n        self.REPLAY_BUFFER = ringbuffer(self.BUFFER_SIZE)\r\n        \r\n    def load_config(self,config):\r\n        \'\'\'\r\n        loads new config\r\n        \'\'\'\r\n        ### env\r\n        self.game_name = config.game_name\r\n        self.env = gym.make(self.game_name).env\r\n        ### training variables\r\n        self.BUFFER_SIZE = config.BUFFER_SIZE\r\n        self.STEPS_PER_EPISODE = config.STEPS_PER_EPISODE\r\n        self.MAX_STEPS = config.MAX_STEPS\r\n        self.UPDATE_TARGET_STEPS = config.UPDATE_TARGET_STEPS\r\n        self.BATCH_SIZE = config.BATCH_SIZE\r\n        self.GAMMA = config.GAMMA\r\n        self.EXPLORATION = config.EXPLORATION\r\n        self.E_MIN = config.E_MIN\r\n        self.priority = config.priority\r\n        self.alpha = config.alpha\r\n        self.epsilon = config.epsilon\r\n\r\n    def save_config(self):\r\n        \'\'\'\r\n        returns current config\r\n        \'\'\'\r\n        return trainer_config(self.game_name,\r\n                                self.BUFFER_SIZE,\r\n                                self.STEPS_PER_EPISODE,\r\n                                self.MAX_STEPS,   \r\n                                self.UPDATE_TARGET_STEPS,\r\n                                self.BATCH_SIZE,\r\n                                self.GAMMA,\r\n                                self.EXPLORATION,\r\n                                self.E_MIN,\r\n                                self.priority,\r\n                                self.alpha,\r\n                                self.epsilon\r\n                                )\r\n\r\n\r\n    def train(self,flag=False,log=False):\r\n        #### traincycles\r\n        start = time.perf_counter()\r\n        eps_rew = 0.\r\n        step_counter = 0.\r\n        current_state = self.env.reset()\r\n        for STEP in range(self.MAX_STEPS):\r\n            e = 1. / ((len(self.loss_plot)/self.EXPLORATION)+1)\r\n            if (STEP+1) % 2000 == 0. and log: \r\n                print(\'step \',STEP+1, \'exploration: {:.5}\'.format(max(0.01,e)),\'sec/step:\' ,(time.perf_counter()-start)/(STEP+1.))\r\n            if np.random.uniform(0,1) < max(self.E_MIN,e):\r\n                action = self.env.action_space.sample()\r\n            else:\r\n                action = np.argmax(self.onlineNet.infer(current_state[True,:]))\r\n            next_state, reward, done, _ = self.env.step(action)\r\n            eps_rew += reward\r\n            self.REPLAY_BUFFER.add([np.array(current_state), np.array(action), np.array(reward), np.array(next_state), np.array(done)])\r\n            step_counter += 1.\r\n            if step_counter > self.STEPS_PER_EPISODE: done = True\r\n            if done: \r\n                self.reward_plot += [eps_rew]\r\n                if flag:\r\n                    print(\'breaking after one episode with {} reward after {} steps\'.format(eps_rew,STEP+1))\r\n                    break\r\n                eps_rew = 0.\r\n                step_counter = 0.\r\n                \r\n                next_state = self.env.reset()\r\n            current_state = next_state\r\n            if STEP > self.BATCH_SIZE*2 or flag: \r\n                BATCH = self.REPLAY_BUFFER.sample(self.BATCH_SIZE, prio=self.priority)\r\n                train_bellman(self.onlineNet, self.targetNet, BATCH, self.GAMMA)\r\n                self.loss_plot += [self.onlineNet.loss]\r\n            if (STEP+1) % self.UPDATE_TARGET_STEPS == 0:\r\n                if self.priority: self.REPLAY_BUFFER.prio_update(self.onlineNet, self.targetNet, GAMMA=self.GAMMA,alpha=self.alpha,epsilon=self.epsilon)\r\n                if log: print(\'update ---- \', len(self.reward_plot),\' episodes played |||| 10 eps average reward: \',np.array(self.reward_plot)[-10:].mean())\r\n                update_target(self.onlineNet,self.targetNet,duel=False)\r\n\r\n\r\n\'\'\'\r\n#################### EXAMPLES ###################\r\n\r\n#################### train a ddqn:\r\nimport mlp_framework as nn  # mlp framework\r\n\r\n# STEP 1: create configuration\r\nconfiguration = trainer_config(game_name=\'CartPole-v0\',MAX_STEPS=100000)#game_name=\'CartPole-v0\', game_name=\'LunarLander-v2\'\r\n\r\n# STEP 2: build models (online & target)\r\nA1 = nn.layer(configuration.INPUT_SIZE,128)\r\nA2 = nn.layer(128,64)\r\nAOUT = nn.layer(64,configuration.OUTPUT_SIZE)\r\nAOUT.f = nn.f_iden\r\nL1 = nn.layer(configuration.INPUT_SIZE,128)\r\nL2 = nn.layer(128,64)\r\nLOUT = nn.layer(64,configuration.OUTPUT_SIZE)\r\nLOUT.f = nn.f_iden\r\nonlineNet = nn.mlp([A1,A2,AOUT])\r\ntargetNet = nn.mlp([L1,L2,LOUT])\r\n\r\n### dueling networks: experimental. not working yet. check loss fnct deriv !!!!\r\nA0 = nn.layer(configuration.INPUT_SIZE,64)\r\nA1 = nn.layer(64,64)\r\nA0t = nn.layer(configuration.INPUT_SIZE,64)\r\nA1t = nn.layer(64,64)\r\nAA = nn.layer(64,64)\r\nAAA = nn.layer(64,1)\r\nB = nn.layer(64,64)\r\nBB = nn.layer(64,configuration.OUTPUT_SIZE)\r\nAAt = nn.layer(64,64)\r\nAAAt = nn.layer(64,1)\r\nBt = nn.layer(64,64)\r\nBBt = nn.layer(64,configuration.OUTPUT_SIZE)\r\nLL0_ = [A0]\r\nLL0_t = [A0t]\r\nLLV_ = [AA,AAA]\r\nLLV_t = [AAt,AAAt]\r\nLLA_ = [B,BB]\r\nLLA_t = [Bt,BBt]\r\nonlineNet = nn.dueling_mlp(LL0_,LLV_,LLA_)\r\ntargetNet = nn.dueling_mlp(LL0_t,LLV_t,LLA_t)\r\n\r\n### dense mlp for DDQN: works quite well, even with 7 layers. despite fast convergence, instability dips can be seen early in training \r\n# regularozation helps a bit with instability ?! why is this happening in RL. maybe task dependent (CartPole-v0)\r\nINPUT_SIZE = configuration.INPUT_SIZE\r\nOUTPUT_SIZE = configuration.OUTPUT_SIZE\r\ngrowth = 20\r\nL1 = nn.layer(INPUT_SIZE,OUTPUT_SIZE*growth)\r\nL2 = nn.layer(INPUT_SIZE+OUTPUT_SIZE*1*growth,OUTPUT_SIZE*growth)\r\nL3 = nn.layer(INPUT_SIZE+OUTPUT_SIZE*2*growth,OUTPUT_SIZE*growth)\r\nL4 = nn.layer(INPUT_SIZE+OUTPUT_SIZE*3*growth,OUTPUT_SIZE*growth)\r\nL5 = nn.layer(INPUT_SIZE+OUTPUT_SIZE*4*growth,OUTPUT_SIZE*growth)\r\nL6 = nn.layer(INPUT_SIZE+OUTPUT_SIZE*5*growth,OUTPUT_SIZE*growth)\r\nL7 = nn.layer(INPUT_SIZE+OUTPUT_SIZE*6*growth,OUTPUT_SIZE*growth)\r\nLOUT = nn.layer(INPUT_SIZE+(OUTPUT_SIZE*7*growth),OUTPUT_SIZE)\r\nLOUT.f = nn.f_iden\r\nA1 = nn.layer(INPUT_SIZE,OUTPUT_SIZE*growth)\r\nA2 = nn.layer(INPUT_SIZE+OUTPUT_SIZE*1*growth,OUTPUT_SIZE*growth)\r\nA3 = nn.layer(INPUT_SIZE+OUTPUT_SIZE*2*growth,OUTPUT_SIZE*growth)\r\nA4 = nn.layer(INPUT_SIZE+OUTPUT_SIZE*3*growth,OUTPUT_SIZE*growth)\r\nA5 = nn.layer(INPUT_SIZE+OUTPUT_SIZE*4*growth,OUTPUT_SIZE*growth)\r\nA6 = nn.layer(INPUT_SIZE+OUTPUT_SIZE*5*growth,OUTPUT_SIZE*growth)\r\nA7 = nn.layer(INPUT_SIZE+OUTPUT_SIZE*6*growth,OUTPUT_SIZE*growth)\r\nAOUT = nn.layer(INPUT_SIZE+(OUTPUT_SIZE*7*growth),OUTPUT_SIZE)\r\nAOUT.f = nn.f_iden\r\nonlineNet = nn.dense_mlp([A1,A2,A3,A4,A5,A6,A7,AOUT])\r\ntargetNet = nn.dense_mlp([L1,L2,L3,L4,L5,L6,L7,LOUT])\r\n\r\n# STEP 3: create trainer\r\nddqn = trainer(onlineNet,targetNet,configuration)\r\n\r\n# STEP 5: train the trainer (ddqn) for configuration.MAX_STEPS:\r\nddqn.train(log=True)\r\n# OR: train the trainer (ddqn) for one episode by setting \'flag\' = True :\r\nddqn.train(True)\r\n\r\n#  STEP 6: \r\n# get your models, config and logs from the trainer (see \'some useful stuff\')\r\n\r\n#################### some usefull stuff:\r\n### save config\r\n# first_config = ddqn.save_config()\r\n### use new config (\'new_config\')\r\n# ddqn.load_config(new_config)\r\n### apply/get  model\r\n# ddqn.onlineNet = new_onlineNet\r\n# trained_targetNet  = ddqn.targetNet\r\n### clear REPLAY BUFFER\r\n# ddqn.REPLAY_BUFFER = ringbuffer(ddqn.BUFFER_SIZE)\r\n### get reward / loss logs\r\n# loss_list = ddqn.loss_plot\r\n# reward_list = ddqn.reward_plot\r\n\r\n\r\n#################### plotting and saving the model\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport pickle as pkl\r\nreward_plot = np.array(ddqn.reward_plot)\r\nloss_plot = np.array(ddqn.loss_plot)\r\n\r\nrewdata = pd.Series(reward_plot)\r\nlossdata = pd.Series(loss_plot)\r\n\r\nplt.figure(figsize=(14,8))\r\nrewdata.plot(alpha=0.1,color=\'b\')\r\nrewdata.rolling(window=100).mean().plot(style=\'g\',alpha=.9)\r\nrewdata.rolling(window=50).mean().plot(style=\'b\',alpha=.7)\r\nrewdata.rolling(window=20).mean().plot(style=\'r\',alpha=.5)\r\nplt.title(\'reward over episodes\')\r\nplt.figure(figsize=(14,8))\r\nlossdata.plot(alpha=0.1,color=\'b\')\r\nlossdata.rolling(window=500).mean().plot(style=\'b\')\r\nplt.title(\'loss over steps\')\r\nplt.show()\r\n\r\n#### pickling doesn\'t work with SwigPyObjects. In case you use Box2D environments do:\r\npkl.dump(ddqn.onlineNet,open(""ddqn_targetNet.p"",""wb""))\r\n#### else you can pickle the whole trainer object for convenience\r\n#pkl.dump(ddqn, open(""ddqn_trainer_model.p"",""wb""))\r\nprint(\'done dump\')\r\n\r\n\r\n\r\n#################### the agent in action (rendered games)\r\nenvX = gym.make(configuration.game_name).env#(\'MountainCar-v0\')#(\'Alien-v0\')#(\'LunarLander-v2\')\r\nfor i_episode in range(20):  ## number of games to be played\r\n    observation = envX.reset()\r\n    rew = 0.\r\n    for t in range(configuration.STEPS_PER_EPISODE):\r\n        envX.render()\r\n        action = np.argmax(ddqn.targetNet.infer(observation[True,:]))\r\n        observation, reward, done, info = envX.step(action)\r\n        rew += reward\r\n        if done or (t+1) >= configuration.STEPS_PER_EPISODE:\r\n            print(""Episode finished after {} timesteps with reward {}"".format(t+1,rew))\r\n            time.sleep(3)\r\n            break\r\n\r\n\r\n\r\n\r\n\r\n\'\'\' and None'"
mlp_framework.py,72,"b'#######################################################################################\r\n# Neural network framework to play around with\r\n# Author: Manuel Hass\r\n# 2017\r\n# \r\n# *examples in the end\r\n#######################################################################################\r\ntry:\r\n    import numpy as np\r\n    numpy = np\r\nexcept ImportError:\r\n    print (\'ERROR -> MODULE MISSING: numpy \')\r\n\r\n\r\n###################### loss ####################################################\r\ndef ce(y,yt,dev=False): ############ not robust !!\r\n    \'\'\'\r\n        cross entropy\r\n        argmax stuff maybe otherwise near zero logs are silly\r\n    \'\'\'\r\n    if (dev==True):\r\n        return (yt-y)\r\n    loss = [(yt[i]).dot(np.log(y[i])) for i in range(y.shape[0])]\r\n    loss = np.array(loss) * -1\r\n    return np.sum(loss)   /(loss.shape[0]*1.)\r\ndef bce(ya,yta,dev=False):  ############ not robust !!\r\n    \'\'\'\r\n        binary cross entropy\r\n    \'\'\'\r\n    if (dev==True):\r\n        return (yta-ya)/((1-yta)*yta)\r\n    return -(np.sum(ya*np.log(yta)+(1.-yta)*np.log(1.-yta))/(yta.shape[0]*2.0))\r\n\r\ndef qef(ya,yta,dev=False):\r\n    \'\'\'\r\n        quadratic error function ||prediction-target||\xc2\xb2\r\n    \'\'\'\r\n    if (dev==True):\r\n        return (yta-ya) \r\n    return np.sum((yta-ya)**2)/(yta.shape[0]*2.0)\r\n\r\ndef phl(y,yt,dev=False,delta=1.):\r\n    \'\'\'\r\n        subquadratic error function (pseudo huber loss)\r\n    \'\'\'\r\n    a = (yt-y)\r\n    if (dev==True):\r\n        return  a/( np.sqrt(a**2/delta**2 +1) ) \r\n    return np.sum((delta**2)*(np.sqrt(1+(a/delta)**2)-1)/(yt.shape[0]*2.0))\r\n\r\n\r\n###################### regularization ####################################################\r\ndef L2_norm(lam,a):  \r\n    \'\'\'\r\n        2-Norm regularizer\r\n    \'\'\'\r\n    return lam*a\r\n\r\ndef L1_norm(lam,a):\r\n    \'\'\'\r\n        1-Norm regularizer\r\n    \'\'\'\r\n    return lam*np.sign(a)\r\n\r\n\r\n###################### activation  ####################################################\r\ndef f_elu(a,dev=False):\r\n    \'\'\'\r\n        exponential linear unit\r\n            ~softplus [0,a]\r\n    \'\'\'\r\n    if dev:\r\n        return np.where(a>=0.,f_elu(a)+a,1)\r\n    return np.where(a>=0.,a*(np.exp(a)-1),a)\r\n\r\ndef f_softmax(a,dev=False):\r\n    \'\'\'\r\n        softmax transfer function \r\n            sigmoidal [0,1]\r\n    \'\'\'\r\n\r\n    if (dev==True):\r\n        return f_softmax(a)*(1-f_softmax(a))\r\n    return  np.exp(a)/ np.sum(np.exp(a))\r\n\r\n\r\ndef f_lgtr(a,dev=False):\r\n    \'\'\'\r\n        (robust) logistic transfer function \r\n            sigmoidal [0,1]\r\n    \'\'\'\r\n    if (dev==True):\r\n        return (1-np.tanh(a/2.)**2)/2.\r\n    return  (np.tanh(a/2.)+1)/2.\r\n\r\n \r\ndef f_stoch(a,dev=False):\r\n    \'\'\'\r\n        stochastic transfer function \r\n            activates if activated input > ~Uniform\r\n            binary [0,1]\r\n    \'\'\'\r\n    if (dev==True):\r\n        return np.zeros(a.shape)  \r\n    x = f_lgtr(a,dev=False)\r\n    rand = np.random.random(x.shape)\r\n    return  np.where(rand < x,1,0)\r\n\r\ndef f_tanh(a,dev=False):\r\n    \'\'\'\r\n        hyperbolic tangent transfer function \r\n            sigmoidal [-1,1]\r\n    \'\'\'\r\n    if (dev==True):\r\n        return (1-np.tanh(a)**2)\r\n    return  np.tanh(a)\r\n\r\ndef f_atan(a,dev=False):\r\n    \'\'\'\r\n        arcus tangent transfer function \r\n            sigmoidal [-pi/2, pi/2]\r\n    \'\'\'\r\n    if (dev==True):\r\n        return (1/(a**2+1))\r\n    return  np.arctan(a)\r\n\r\ndef f_sp(a,dev=False):\r\n    \'\'\'\r\n        softplus transfer function \r\n            [0,a]\r\n\r\n            ### kinda clip it...to make more robust\r\n    \'\'\'\r\n    if (dev==True):\r\n        return np.exp(a)/(np.exp(a)+1.)\r\n    return  np.log(np.exp(a)+1.)\r\n    \r\ndef f_relu(a,dev=False):\r\n    \'\'\'\r\n        rectified linear transfer function \r\n            [0,a]\r\n    \'\'\'\r\n    if (dev==True):\r\n        return np.maximum(0,np.sign(a)) \r\n    return  np.maximum(0.0,a)\r\n\r\ndef f_leaky(a,dev=False,leak=0.01):\r\n    \'\'\'\r\n       leaky rectified linear transfer function \r\n            [-leaky*a,a]\r\n    \'\'\'\r\n    if (dev==True):\r\n        signs = np.sign(a)\r\n\r\n        return np.where(signs>0.,signs,0.01*signs) \r\n    return  np.where(a>0.,a,leak*a)\r\n\r\n\r\n \r\ndef f_bi(a,dev=False):\r\n    \'\'\'\r\n        bent identity transfer function\r\n    \'\'\'\r\n    if (dev==True):\r\n         return a / ( 2.0*np.sqrt(a**2+1) ) + 1\r\n    return  (np.sqrt(a**2+1)-1)/2.0 + a\r\n\r\ndef f_iden(a,dev=False):\r\n    \'\'\'\r\n        identity transfer function \r\n    \'\'\'\r\n    if (dev==True):\r\n         return np.ones(a.shape)\r\n    return  a\r\n\r\ndef f_bin(a,dev=False):\r\n    \'\'\'\r\n        binary step transfer function \r\n    \'\'\'\r\n    if (dev==True):\r\n         return np.zeros(a.shape) \r\n    return  np.sign(f_relu(a))\r\n\r\n\r\n############################# utils ######################################\r\n### input / output processing\r\ndef one_hot(targets,smooth=False):\r\n    \'\'\'\r\n        input: discrete labels (number, string, etc.)\r\n        output: binary numpy array (size = #unique classes)\r\n    \'\'\'\r\n    classes =  np.unique(targets.T)\r\n    binarycoded = []\r\n    for i in classes:\r\n        binarycoded +=  [np.where(targets==i,1,0)[0]]\r\n    out = np.array(binarycoded).T\r\n    if smooth:\r\n        # one side label smoothind\r\n        out = out+.8 +.1\r\n    else:\r\n        return out\r\n\r\ndef hot_one(targets):\r\n    \'\'\'\r\n        input: binary array\r\n        output: discrete labels (numbers)\r\n    \'\'\'\r\n    return np.argmax(np.array(targets).T,axis=0).reshape(-1,1)\r\n\r\n\r\n############################ MLP ########################################### \r\n### network layer\r\nclass layer:\r\n    \'\'\'\r\n    actiavtion layer for model building:\r\n        layer(input_dimension,number_of_nodes)\r\n\r\n    parameters:\r\n        f   : activation function\r\n        w   : weights\r\n\r\n        reg : regularizer function\r\n        lam : regularizer lambda\r\n        eta : learning rate\r\n        \r\n        opt : optimizer (\'Adam\',\'RMSprop\',\'normal\')\r\n        eps : ""don\'t devide by zero!!""\r\n        b1  : momentumparameter for \'Adam\' optimizer\r\n        b2  : momentumparameter for \'RMSprop\' and \'Adam\' optimizer\r\n        m1  : momentum for \'Adam\' optimizer\r\n        m2  : momentum for \'RMSprop\' and \'Adam\' optimizer\r\n\r\n        count: number of updates \r\n\r\n    \'\'\'\r\n    def __init__(self,in_dim,nodes=32,no_bias=False): \r\n\r\n        #activation and weights\r\n        self.no_bias = no_bias\r\n        self.f = f_tanh\r\n        # by default Xavier init\r\n        #np.random.randn(nodes, in_dim) / np.sqrt(in_dim)#\r\n        #np.random.randn(nodes, in_dim+1) / np.sqrt(in_dim+1)#\r\n        if self.no_bias: self.w = np.random.uniform(-.1,.1,(nodes,in_dim))\r\n        else: self.w = np.random.uniform(-.1,.1,(nodes,in_dim+1))\r\n        ### Xavier init: \r\n        #w = np.random.randn(neurons, input_dimension) / np.sqrt(input_dimension)\r\n\r\n        #momentum \r\n        self.m1 = np.random.uniform(0.1,1,self.w.shape)\r\n        self.m2 = np.random.uniform(0.1,1,self.w.shape)\r\n        self.b1 = 0.9   # Adam, if b1 = 0. -> Adam = RMSprop\r\n        self.b2 = 0.99\r\n        self.opt = \'Adam\'\r\n        self.eps = 1e-10\r\n\r\n        #regularizer\r\n        self.reg = L2_norm\r\n        self.lam = 0.#1e-8\r\n\r\n        #learning\r\n        self.count = 0\r\n        self.eta = 1e-4\r\n\r\n    def forward(self,input_):\r\n        \'\'\'\r\n            forward pass (computes activation)\r\n                return: activation(input * weights[+ bias])\r\n        \'\'\'\r\n        ##### IF no_bias != True  :\r\n        if self.no_bias: self.x1 = input_\r\n        else: self.x1 = np.vstack((input_.T,np.ones(input_.shape[0]))).T\r\n        #print(\'fw x1: \',self.x1.shape)\r\n        self.h1 = np.dot(self.x1,self.w.T).T\r\n        #print(\'fw h1: \',self.h1.shape)\r\n        self.s = self.f(self.h1)\r\n        #print(\'fw s: \',self.s.shape)\r\n        return self.s\r\n\r\n    def backward(self,L_error):\r\n        \'\'\'\r\n            backward pass (computes gradient)\r\n                return: layer delta\r\n        \'\'\'\r\n        #print(\'L_error :  \',L_error.shape)\r\n        self.L_grad = L_error* self.f(self.h1,True).T\r\n        #print(\'L_grad :  \',self.L_grad.shape)\r\n        self.delta_W = -1./(self.x1).shape[0] * np.dot(self.L_grad.T,self.x1) - self.reg(self.lam,self.w)\r\n        if self.no_bias: \r\n            return np.dot(self.w.T,self.L_grad.T).T\r\n        else: return np.dot(self.w.T[1:],self.L_grad.T).T\r\n\r\n    def update(self):   \r\n        \'\'\'\r\n            update step (updates weights & momentum)\r\n        \'\'\'   \r\n        self.m1 = self.b1*self.m1 + (1-self.b1)*self.delta_W\r\n        self.m2 = self.b2*self.m2 + (1-self.b2)*self.delta_W**2\r\n        if(self.opt==\'RMSprop\'):\r\n            self.w += self.eta* self.delta_W / (np.sqrt(self.m2) +self.eps)\r\n        if (self.opt==\'Adam\'):\r\n            self.w += self.eta* self.m1 / (np.sqrt(self.m2) +self.eps)\r\n        if(self.opt==\'normal\'):\r\n            self.w += self.eta* self.delta_W\r\n        self.count += 1\r\n\r\n    def reset(self):\r\n        \'\'\'\r\n            weights & momentum reset\r\n        \'\'\'\r\n        self.w = np.random.uniform(-.7,.7,(nodes,in_dim+1))\r\n        self.m1 = np.random.uniform(0.,1,self.w.shape)\r\n        self.m2 = np.random.uniform(0.,1,self.w.shape)\r\n\r\n\r\n### (dropout layer) ## need to specify if not training... w*(1-droptrate)\r\nclass dropout:\r\n    \'\'\'\r\n        masks activations\r\n            dropout(input,drop)\r\n\r\n        parameter:\r\n            drop    : chance for dropping unit\r\n\r\n    \'\'\'\r\n    def __init__(self,in_dim,drop =.5,training=True): \r\n        self.training = training\r\n        # dropout mask\r\n        self.drop = drop\r\n        self.mask = np.random.choice([0, 1], size=(in_dim), p=[self.drop, 1-self.drop])\r\n\r\n\r\n    def forward(self,input_):\r\n        \'\'\'\r\n            masks input\r\n        \'\'\' \r\n        if not self.training: return (1.-self.drop)*input_.T\r\n        return (self.mask*input_).T\r\n\r\n    def backward(self,L_error):\r\n        \'\'\'\r\n            masks backward pass \r\n        \'\'\'\r\n        return self.mask * L_error\r\n\r\n    def update(self):   \r\n        \'\'\'\r\n            updates mask\r\n        \'\'\'   \r\n        self.mask = np.random.choice([0, 1], size=(self.mask.shape[0]), p=[self.drop, 1-self.drop])\r\n\r\n    def reset(self):\r\n        \'\'\'\r\n            also updates mask\r\n        \'\'\'   \r\n        self.update()\r\n\r\n\r\n### models\r\nclass mlp:\r\n    \'\'\'\r\n    multi layer perceptron model:\r\n        mlp(List_with_layers)\r\n\r\n    parameters:\r\n        Layerlist   : list of layers\r\n        erf         : errorfunction\r\n        loss        : last training loss\r\n        \r\n    \'\'\'\r\n    def __init__(self,Layerlist):\r\n        self.Layerlist = Layerlist\r\n        self.erf = qef\r\n\r\n    def infer(self, input_):\r\n        \'\'\'\r\n            compute full forward pass\r\n        \'\'\'            \r\n        out = input_\r\n        for L in self.Layerlist:\r\n            out = L.forward(out).T\r\n        return out\r\n\r\n    def train(self,input_,target_):\r\n        \'\'\'\r\n            training step\r\n        \'\'\'\r\n        outs = self.infer(input_)\r\n        self.loss = self.erf(target_,outs)       \r\n        grad = self.erf(target_,outs,True)        \r\n        for L in self.Layerlist[::-1]:\r\n            #print(grad.shape, \'before grad\')\r\n            grad = L.backward(grad)\r\n            #print(grad.shape, \'after grad\')\r\n\r\n            L.update()\r\n\r\nclass dense_mlp:\r\n    \'\'\'\r\n    dense multi layer perceptron model:\r\n        dense_mlp(List_with_layers)\r\n\r\n    parameters:\r\n        Layerlist   : list of layers\r\n        erf         : errorfunction\r\n        loss        : last training loss\r\n        \r\n    \'\'\'\r\n    def __init__(self,Layerlist):\r\n        self.Layerlist = Layerlist\r\n        self.erf = qef\r\n\r\n    def infer(self, input_):\r\n        \'\'\'\r\n            compute full forward pass\r\n            ---with dense looped connections !!!!!!!!!!!!!!!!\r\n        \'\'\'            \r\n        out = input_\r\n        out_list = [out] #list holding all layers activation\r\n        L = self.Layerlist\r\n        for i in range (len (L)):\r\n            #print(\'before shape \',out.shape)\r\n            #print(out_list[0].shape)\r\n            out = np.hstack((out_list[j] for j in range(i+1)))\r\n            \r\n            #print(\'after shape \',out.shape)\r\n            out = L[i].forward(out).T\r\n            #print(\'out \',out.shape)\r\n            out_list += [out]\r\n            #print(out_list[0].shape,out_list[1].shape)\r\n    \r\n        return out\r\n\r\n    def train(self,input_,target_):\r\n        \'\'\'\r\n            training step\r\n            ---nested backprop \r\n        \'\'\'\r\n        self.loss = self.erf(target_,self.infer(input_))       \r\n        grad = self.erf(target_,self.infer(input_),True)\r\n        L = self.Layerlist\r\n        #print(\'round ...........\')\r\n        for i in range(len(L)):\r\n            grad = L[-(i+1)].backward(grad)\r\n            L[-(i+1)].update()\r\n            grad_list = grad[:,:-L[-(i+1)].w.shape[0]]\r\n            if len(L) > (i+2):\r\n                for j in (np.arange(len(L)-(i+2))):\r\n                        grad_ = grad_list[:,:L[j].w.shape[0]]\r\n                        grad_list = grad_list[:,L[j].w.shape[0]:]\r\n                        _ = L[j].backward(grad_)\r\n                        L[j].update()\r\n\r\n            grad = grad[:,-L[max(-(i+2),-len(L))].w.shape[0]:]  \r\n\r\nclass dueling_mlp:\r\n    \'\'\' !!!!!!!!!!!!!!!! NOT WORKING YET !!!!!!!!!!!!!!!!\r\n    - not converging so far - think it\'s about target_updates:\r\n      double Q update :  Q_target = reward + gamma * tagetNet(state+1)[0,argmax(onlineNet(state+1))]\r\n      dueling Q update:  Q_target = reward + gamma * 1/number_of_actions * mean of maybe all Q action values?!\r\n    dueling mlp for Q learning:\r\n        dueling_mlp(LL0,LLA,LLB,model=mlp)\r\n        IN -> LL0 -> [LLV & LLA] => (LLV + (LLA-mean(LLA)))\r\n    parameters:\r\n        LL0         : list of layers for core model\r\n        LLV         : list of layers for value model (shape 1)\r\n        LLA         : list of layers for advantage model (shape actionspace)\r\n\r\n    \'\'\'\r\n    def __init__(self,LL0,LLV,LLA):\r\n        self.LL0 = LL0\r\n        self.LLV = LLV\r\n        self.LLA = LLA\r\n        self.erf = qef\r\n        \r\n\r\n    def infer(self, input_):\r\n        \'\'\'\r\n            compute full forward pass over both networks\r\n        \'\'\'      \r\n        out0 = input_\r\n        for L in self.LL0:\r\n            out0 = L.forward(out0).T\r\n        outV = out0\r\n        outA = out0\r\n        for L in self.LLA:\r\n            outA = L.forward(outA).T\r\n        for L in self.LLV:\r\n            outV = L.forward(outV).T\r\n       \r\n        outA_ = outA-outA.mean(0)\r\n        outQ = outA_ + outV\r\n\r\n        return outQ\r\n\r\n    def train(self,input_,target_):\r\n        \'\'\'\r\n            training step\r\n            think about the aggregation layer in the end ---> LLout maybe\r\n        \'\'\'\r\n        # calculating forward pass\r\n        out0 = input_\r\n        for L in self.LL0:\r\n            out0 = L.forward(out0).T\r\n        outV = out0\r\n        outA = out0\r\n        for L in self.LLA:\r\n            outA = L.forward(outA).T\r\n        for L in self.LLV:\r\n            outV = L.forward(outV).T\r\n        outA_ = outA-outA.mean(0)\r\n        outQ = outA_ + outV\r\n       \r\n\r\n        self.TD_loss = np.power((target_-outQ),2) \r\n        #print(self.TD_loss.shape,\'TD loss shape\')\r\n        self.loss = self.erf(target_,outQ)\r\n        #print(self.loss.shape,\'TD loss shape\')\r\n        \r\n        ###################################################\r\n        gradA = self.erf(target_-target_.mean(1)[:,True],outA,True)\r\n        gradV = self.erf(target_.mean(1)[:,True],outV,True)\r\n        # this might be the reason why it\'s not working. calculate a correct loss please...\r\n        ###################################################\r\n        \r\n        #print(gradV.shape, \'grad V\')\r\n        #print(gradA.shape, \'grad A\')\r\n        for L in self.LLA[::-1]:\r\n            #print(grad.shape, \'before grad\')\r\n            gradA = L.backward(gradA)\r\n            #print(grad.shape, \'after grad\')\r\n            L.update()\r\n        for L in self.LLV[::-1]:\r\n            #print(grad.shape, \'before grad\')\r\n            gradV = L.backward(gradV)\r\n            #print(grad.shape, \'after grad\')\r\n            L.update()\r\n        # 2 gradients arriving in feature extractor LL0\r\n        # update LL0 with the mean of both\r\n        grads = (gradA+gradV)/2.\r\n\r\n        for L in self.LL0[::-1]:\r\n            #print(grad.shape, \'before grad\')\r\n            gradA = L.backward(grads)\r\n            #print(grad.shape, \'after grad\')\r\n            L.update()\r\n\r\n\r\n\r\n\r\n\r\n\'\'\'\r\n### Xavier init: \r\nneurons = 256 #nodes\r\ninputs = 80*80 #input dimension\r\na = np.random.randn(neurons, inputs) / np.sqrt(inputs)\r\nprint(\'aaaa\',a.shape)\r\nprint(a[0,:20])\r\n#################### TESTING ###############################\r\nimport matplotlib.pyplot as plt\r\nnp.random.seed(123)\r\n### create testdata (2D gauss)\r\nD = np.random.multivariate_normal([-1,-1],np.diag([.4,.3]),size=100)\r\nD_ = np.random.multivariate_normal([0,0,],np.diag([.2,.4]),size=100)\r\nD__ = np.random.multivariate_normal([-1,1,],np.diag([.1,.5]),size=100)\r\nD___ = np.random.multivariate_normal([1,-1],np.diag([.2,.2]),size=100)\r\nD = np.concatenate((D,D_,D__,D___),0)\r\n#D = D + np.array([1,1])\r\nT = np.ones(D.shape[0])\r\nT[:100] *= 0\r\nT[200:300] *= 2\r\nT[300:] *= 3\r\nT1 = one_hot(T[:,True].T)\r\n\r\n### build model of layers\r\n\r\n\r\n\r\n#print(\'T1 / D \',T1.shape,D.shape)\r\nINPUT_SIZE = D.shape[1]\r\nOUTPUT_SIZE = T1.shape[1]\r\n\r\n######## MLP ###########\r\nA1 = layer(INPUT_SIZE,512,no_bias=False)\r\nA2 = layer(512,256)\r\nDO = dropout(256,.1)\r\nA3 = layer(256,128)\r\nA4 = layer(128,64)\r\nAOUT = layer(64,OUTPUT_SIZE)\r\nAOUT.f = f_lgtr\r\nmodel = mlp([A1,A2,DO,A3,A4,AOUT])\r\n\r\n\r\n######### DENSE MLP ########\r\n#L1 = layer(4,4)\r\n#L2 = layer(8,4)\r\n#L3 = layer(12,4)\r\n#L4 = layer(16,4)\r\n#OUT = layer(20,4)\r\n#OUT.f = f_lgtr\r\n#Llist = [L1,L2,L3,L4,OUT]\r\n#model = dense_mlp(Llist)\r\n\r\n###### LARGE DENSE MLP ###################\r\n#L1 = layer(INPUT_SIZE,OUTPUT_SIZE*2)\r\n#L2 = layer(INPUT_SIZE+OUTPUT_SIZE*2,OUTPUT_SIZE*2)\r\n#L3 = layer(INPUT_SIZE+OUTPUT_SIZE*4,OUTPUT_SIZE*2)\r\n#L4 = layer(INPUT_SIZE+OUTPUT_SIZE*6,OUTPUT_SIZE*2)\r\n#L5 = layer(INPUT_SIZE+OUTPUT_SIZE*8,OUTPUT_SIZE*2)\r\n#L6 = layer(INPUT_SIZE+OUTPUT_SIZE*10,OUTPUT_SIZE*2)\r\n#L7 = layer(INPUT_SIZE+OUTPUT_SIZE*12,OUTPUT_SIZE*2)\r\n#OUT = layer(INPUT_SIZE+(OUTPUT_SIZE*6),OUTPUT_SIZE)\r\n#OUT.f = f_lgtr\r\n#Llist = [L1,L2,L3,OUT]\r\n#model = dense_mlp(Llist)\r\n\r\nerrorlist = []\r\nnodrop = []\r\n### train model\r\nprint(D.shape)\r\nfor i in range(300):\r\n    model.train(D,T1)\r\n    errorlist += [model.loss]\r\n    model.Layerlist[2].training = False\r\n    nodrop += [qef(T1,model.infer(D))]\r\n    model.Layerlist[2].training = True\r\n    if (i+1) % 10 == 0:\r\n        print(\'MLP : \',model.loss, \' loss at \',i+1,\' steps \', )\r\n\r\nerrorlist = np.array(errorlist)\r\n\r\nplt.figure(figsize=(10,6))\r\nplt.plot(range(errorlist.shape[0]),errorlist,c=\'black\',label=\'training loss\')\r\nplt.plot(range(errorlist.shape[0]),nodrop,c=\'red\',label=\'inference loss\')\r\nplt.legend()\r\nplt.grid()\r\nplt.show()\r\n\r\n#------------------------------    \r\n### show prediction (area)\r\nmodel.Layerlist[2].training = False\r\nR = np.linspace(-2, 2, 100, endpoint=True) \r\nA,B = np.meshgrid(R,R)\r\nG = [] \r\nfor i in range(A.shape[0]):\r\n    for j in range(A.shape[1]):\r\n        G += [[A[i][i],A[i][j]]]\r\nG = np.array(G)\r\nplt.figure(figsize=(10,8))\r\nplt.scatter(D[:,0],D[:,1],c=T*255,edgecolors=None,s=20,cmap=\'rainbow\')\r\nplt.scatter(G[:,0],G[:,1],c=np.argmax(model.infer(G),1)*255,edgecolors=None,s=25,cmap=\'spectral\',alpha=.3)\r\nplt.xlim(-2,2)\r\nplt.ylim(-2,2)\r\n#plt.figure(figsize=(8,8))\r\n#a = np.flip(np.argmax(model.infer(G),1).T[::-1].reshape(100,100).T*255,1)\r\n#plt.imshow(a)\r\nplt.show()\r\n\r\n\r\n\'\'\'\r\n#\'\'\'\r\n'"
ppo_test.py,17,"b""#######################################################################################\r\n# PPO A2C-style- Learning framework to play around with \r\n# Author: Manuel Hass\r\n# 2018\r\n# \r\n# *uses mlp_framework.py as model framework \r\n#  \r\n#######################################################################################\r\n#!!!!!!!!!!! quite messy 'quick & dirty'  code !!!!!!!!!!!!!!!!!\r\n\r\n### imports\r\nimport numpy as np\r\nimport gym\r\nimport time \r\nimport mlp_framework as nn\r\n\r\nclass ppo:\r\n\t'''\r\n\tppo loss \r\n\t'''\r\n\tdef __init__(self):\r\n\t\tself.REWARD = 1\r\n\t\tself.PRED_REW = 1\r\n\t\tself.OLD_PROB = 1\r\n\r\n\t\tself.ADVANTAGE = self.REWARD - self.PRED_REW\r\n\t\tself.eps = 1e-10\r\n\r\n\t\tself.clipper = 0.2\r\n\r\n\tdef ppo_loss(self,yt,y,deriv=True):\r\n\t\tprobability = np.sum(yt*y)\r\n\t\told_probability = np.sum(yt*self.OLD_PROB)\r\n\t\t\r\n\t\tratio = probability / (old_probability+self.eps)\r\n\t\tclipped = np.clip(ratio,1-self.clipper,1+self.clipper)*self.ADVANTAGE\r\n\t\tterm2 = np.min((ratio*self.ADVANTAGE,clipped),axis=1)\r\n\t\tterm2 = np.mean(term2)\r\n\t\tloss = -np.log(probability+self.eps) * term2\r\n\t\t#print('loss: ',loss.shape)\r\n\t\treturn  loss\r\n\r\n'''\r\nloss_obj = ppo()\r\nloss = loss_obj.ppo_loss\r\nprint(loss(0,0))\r\nloss_obj.REWARD = 10\r\nprint(loss(0,0))\r\n''' and None\r\n\r\nstart = time.perf_counter()\r\n\r\n\r\n######################## model ####################################\r\n### create env\r\nenv = gym.make('CartPole-v0').env\r\n\r\n### create layers\r\nINPUT_SHAPE = env.observation_space.shape[0]\r\nOUTPUT_SHAPE = env.action_space.n\r\n\r\nA1 = nn.layer(INPUT_SHAPE,64,no_bias=True)\r\nA2 = nn.layer(64,64,no_bias=True)\r\nAOUT = nn.layer(64,OUTPUT_SHAPE,no_bias=True) # action out\r\nAOUT.f = nn.f_softmax\r\n\r\nL1 = nn.layer(INPUT_SHAPE,64)\r\nL2 = nn.layer(64,64)\r\nLOUT = nn.layer(64,1)# value out\r\nLOUT.f = nn.f_iden\r\n\r\n### create models\r\npolicy_model = nn.mlp([A1,A2,AOUT]) # policy model \r\npolicyloss = ppo()\r\npolicy_model.erf = policyloss.ppo_loss \r\n\r\nvalue_model = nn.mlp([L1,L2,LOUT]) # value model\r\n\r\n#use elu activation \r\nfor L in value_model.Layerlist:\r\n\tL.f = nn.f_elu\r\nvalue_model.Layerlist[-1].f = nn.f_iden\r\n\r\nMAX_EPISODES = 2000\r\nPOLICY_STEPS = 5\r\nVALUE_STEPS = 5\r\n\r\nGAMMA = 0.98\r\nLAMBDA = 0.96\r\n\r\n\r\n########################## training ###########################\r\nreward_log = []\r\nfor I in range(MAX_EPISODES):\r\n\tdone = False\r\n\tcurrent_state = env.reset()\r\n\tepisode_reward = []\r\n\tepisode_batch = [[],[],[]]\r\n\twhile not done:\r\n\t\tprediction = policy_model.infer(current_state[True,:])\t\r\n\t\tprobas = prediction[0]\r\n\t\ta = np.random.choice(env.action_space.n, p=probas)\r\n\t\tactions = np.zeros(prediction.shape[1])\r\n\t\tactions[a] = 1\r\n\r\n\t\tnext_state, reward, done, _ = env.step(a)\r\n\r\n\t\tepisode_reward += [reward]\r\n\t\tepisode_batch[0] += [current_state]\r\n\t\tepisode_batch[1] += [actions]\r\n\t\tepisode_batch[2] += [prediction]\r\n\t\tcurrent_state = next_state\r\n\r\n\t\tif done:\r\n\t\t\tvalues =  value_model.infer(np.array(episode_batch[0]))\r\n\t\t\tvalues = np.insert(values,0,np.zeros((1,1)))\r\n\t\t\treward_log += [np.sum(episode_reward)]\r\n\t\t\tif (I+1) % 10 ==  0 :\r\n\t\t\t\tprint((I+1),'th episode finished after {} timesteps with reward {} --- 10eps avg: {}'.format(len(episode_reward),np.sum(episode_reward),np.mean(reward_log[-50:])))\r\n\t\t\t## this computes targets, values and advantages. check back with loss and GAE\r\n\t\t\ttransformed_reward = []\r\n\t\t\tadvantage = 0.\r\n\t\t\tadvantage_list = []\r\n\t\t\tfor i in reversed(range(len(episode_reward))):\t\t\t\t\r\n\t\t\t\ttd =  episode_reward[i] * GAMMA * values[i+1] - values[i]\r\n\t\t\t\tadvantage = td + GAMMA * LAMBDA * advantage\r\n\t\t\t\tadvantage_list += [advantage]\r\n\t\t\t\ttransformed_reward += [(advantage + values[i])]\r\n\t\t\t\r\n\t\t\ttransformed_reward = np.array(list(reversed(transformed_reward)))\r\n\t\t\t\r\n\t\t\tadvantage_list = np.array(list(reversed(advantage_list)))\r\n\t\t\tadvantage_list -= np.mean(advantage_list)\r\n\t\t\tadvantage_list /= (np.std(advantage_list)+1e-10)\r\n\t\t\t'''\r\n\t\t\t# not GAE\r\n\t\t\tfor i in range(len(episode_reward)):\r\n\t\t\t\tRT = episode_reward[i]\r\n\r\n\t\t\t\tfor j in range(i+1,len(episode_reward)):\r\n\t\t\t\t\tRT += episode_reward[j] * (LAMBDA * GAMMA)**j \r\n\t\t\t\tepisode_reward[i] = RT #+  value_model.infer((episode_batch[0][j])[True,:]) *(LAMBDA * GAMMA)**(len(episode_reward)-1)\r\n\t\t\t'''\r\n\t\t\tepisode_reward = transformed_reward\r\n\t\t\tbreak\r\n\t\t#### \r\n\r\n\r\n\tSTATE, ACTION, PREDICTION, REWARD = np.array(episode_batch[0]),np.array(episode_batch[1]),np.array(episode_batch[2]),np.array(episode_reward)\r\n\r\n\tREWARD = REWARD.reshape(-1)\r\n\r\n\told_prob = PREDICTION\r\n\tpredicted_reward = value_model.infer(STATE)\r\n\tpolicyloss.REWARD = REWARD\r\n\tpolicyloss.PRED_REW = predicted_reward\r\n\tpolicyloss.OLD_PROB = old_prob\r\n\tpolicyloss.ADVANTAGE = advantage_list\r\n\tpolicy_model.erf = policyloss.ppo_loss\r\n\t\r\n\tcounter = int(reward_log[-1] * 2)\r\n\t\r\n\tfor step in range(POLICY_STEPS):\r\n\t\tpolicy_model.train(STATE,ACTION)\r\n\t#print('policy update')\r\n\tfor step in range(VALUE_STEPS):\r\n\t\tvalue_model.train(STATE,REWARD[:,True])\r\n\t#print('value update')\r\nprint('time elapsed: ',time.perf_counter()-start,'s')\r\n#########################################################################\r\n\r\n#################### plotting ###########################################\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\nrewdata = pd.Series(reward_log)\r\nplt.figure(figsize=(14,8))\r\nrewdata.plot(alpha=0.1,color='b')\r\nrewdata.rolling(window=100).mean().plot(style='g',alpha=.9)\r\n#rewdata.rolling(window=50).mean().plot(style='b',alpha=.7)\r\nrewdata.rolling(window=20).mean().plot(style='r',alpha=.5)\r\nplt.title('reward over episodes')\r\nplt.legend()\r\nplt.grid()\r\nplt.show()\r\n\r\n'''\r\nplt.figure()\r\nplt.plot(range(len(reward_log)),reward_log)\r\nplt.show()\r\n\r\n'''\r\n"""
