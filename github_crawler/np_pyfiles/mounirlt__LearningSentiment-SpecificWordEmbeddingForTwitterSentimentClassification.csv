file_path,api_count,code
src/__init__.py,0,b''
src/main.py,2,"b'from preprocess.preprocesstweets import *\nfrom Embedding.sswemodel.sswe_extractor import *\nfrom models import Dataset\nfrom classification.Classifier import Classifier\nfrom classification.NeuralNets import *\nfrom classification.Evaluator import Evaluator\nfrom feature_extractor import SennaFeatureExtractor\nfrom Visualisation.Visualiser import *\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import scale\nfrom tqdm import tqdm\nimport numpy as np\n\n\ndef main():\n    """""" Sentiment Specific Embedding for twitter classification """"""\n\n    embeddings_size = 50  # Embedding size for SSWE model\n    vocab_file = ""Embedding/features/semeval_vocabs_200.txt""  # path to the vocabulary file\n    vector_file = ""Embedding/features/semeval_vectors_200.txt""  # path to the vector file\n    stopwordsfile = ""preprocess/stopwords.txt""\n\n    """"""     Sentiment-Specific Word Embedding (SSWE)    """"""\n\n    if True:\n        # Load dataset\n        data_train = \'dataset/training1600000.csv\'  # training data set file path\n        pre_data_train = \'dataset/preprocessed_dataset1600000.csv\'  # file to save dataset after cleaning\n\n        if True:\n            print(""\\n **** Dataset cleaning ****"")\n            tweets_prepocess(data_train, pre_data_train, stopwordsfile)\n\n        if True:\n            print(""\\n **** SSWE model Trainig ****"")\n            train_model = None  # path to the file contains the trained model if it is already exist\n            save_model = ""Embedding/models/SSWE_model_1600000_200""  # path to the file where model will be saved\n            sswe = create_sswe_model(pre_data_train, vocab_file, vector_file, train_model,\n                                     save_model, embeddings_size)\n            sswe_trainer(sswe)\n\n    """"""     Embedding visualisation and Similarity computing    """"""\n\n    if True:\n        visualiser = Visualiser(sizeOfEmbedding=embeddings_size,\n                                VocabsFname=vocab_file,\n                                VectorsFname=vector_file,\n                                WVFilename=""Visualisation/data/w2vformat.txt"",\n                                visualizerHTMLfilename=""Visualisation/data/embedding.html"")\n        visualiser.visualize()\n\n    """""" Twitter Sentiment Classification """"""\n\n    if True:\n        # Data pre-processing\n\n        print(""\\n **** Training data cleaning ****"")\n        pre_processing_train = ""dataset/preprocessed_semeval_traindataset.csv""\n        # tweets_prepocess(train_set, pre_processing_train, stopwordsfile)\n\n        print(""\\n **** Test data cleaning ****"")\n        pre_processing_test = ""dataset/preprocessed_semeval_testdataset.csv""\n        # tweets_prepocess(test_set, pre_processing_test, stopwordsfile)\n\n        # LOAD TRAIN SET\n        dataset_train = Dataset.DatasetReview()\n        dataset_train.load_review_from_csv(pre_processing_train)\n\n        # LOAD TEST SET\n        dataset_test = Dataset.DatasetReview()\n        dataset_test.load_review_from_csv(pre_processing_test)\n\n        ################################### Neural Nets classifier ###########################\n\n        # Extract Features\n        tweet2v = get_sswe_features(vocab_file, vector_file)\n\n        # Extract samples and labels\n        x_train, y_train = split_data(dataset_train)\n        x_test, y_test = split_data(dataset_train)\n\n        tfidf = build_tfidf(x_train)\n\n        train_vecs_sswe = np.concatenate(\n            [buildWordVector(z.split(), embeddings_size,\n                             tweet2v, tfidf) for z in tqdm(map(lambda x: x, x_train))])\n\n        train_vecs_sswe = scale(train_vecs_sswe)\n\n        test_vecs_sswe = np.concatenate(\n            [buildWordVector(z.split(), embeddings_size,\n                             tweet2v, tfidf) for z in tqdm(map(lambda x: x, x_test))])\n        test_vecs_sswe = scale(test_vecs_sswe)\n\n        # neural network model\n        neuralnets = NeuralNets(input_size=embeddings_size, x_train=train_vecs_sswe, y_train=y_train,\n                                epochs=450, batch_size=32, x_test=test_vecs_sswe, y_test=y_test)\n        neuralnets.train_neural_nets()\n\n        ##########################################################################################\n        ########\n        ########        Classical classifiers with sklearn\n        ########\n        ##########################################################################################\n        print(""\\n**** CROSS VALIDATION EVALUATION (CORPUS: SemEval) ****\\n"")\n\n        fe_sswe = SennaFeatureExtractor(infile=vector_file, vocabfile=vocab_file, dimen=embeddings_size)\n        feature_extractors = [fe_sswe]\n        ev = Evaluator()\n\n        ################################# SVM ###################################################\n\n        print (""\\n**** CROSS VALIDATION EVALUATION (CORPUS: SemEval) ****\\n"")\n        model = Classifier(models=""svm"")\n        kfold = KFold(n_splits=10)\n        ev.eval_with_cross_validation(model, feature_extractors=feature_extractors,\n                                      training_set=dataset_train, num_fold=10, cv=kfold)\n        ev.create_evaluation_result(model, feature_extractors=feature_extractors,\n                                    training_set=dataset_train, num_fold=10, cv=kfold)\n\n        print (""\\n**** TEST SET EVALUATION (CORPUS: SemEval) ****\\n"")\n        ev.eval_with_test_set(model, feature_extractors=feature_extractors,\n                              training_set=dataset_train,\n                              test_set=dataset_test)\n\n        ################################### Naive bayes ##########################################\n\n        print (""\\n**** CROSS VALIDATION EVALUATION (CORPUS: SemEval) ****\\n"")\n        model = Classifier(models=""multinomial"")\n        kfold = KFold(n_splits=10)\n        ev.eval_with_cross_validation(model, feature_extractors=feature_extractors,\n                                      training_set=dataset_train, num_fold=10, cv=kfold)\n        ev.create_evaluation_result(model, feature_extractors=feature_extractors,\n                                    training_set=dataset_train, num_fold=10, cv=kfold)\n\n        print (""\\n**** TEST SET EVALUATION (CORPUS: DATASET) ****\\n"")\n        ev.eval_with_test_set(model, feature_extractors=feature_extractors,\n                              training_set=dataset_train,\n                              test_set=dataset_test)\n\n        #########################################  RandomForestClassifier #######################\n\n        print (""\\n**** CROSS VALIDATION EVALUATION (CORPUS: SemEval) ****\\n"")\n        model = Classifier(models=""rfc"")\n        kfold = KFold(n_splits=10)\n        ev.eval_with_cross_validation(model, feature_extractors=feature_extractors,\n                                      training_set=dataset_train, num_fold=10, cv=kfold)\n        ev.create_evaluation_result(model, feature_extractors=feature_extractors,\n                                    training_set=dataset_train, num_fold=10, cv=kfold)\n\n        print (""\\n**** TEST SET EVALUATION (CORPUS: SemEval) ****\\n"")\n        ev.eval_with_test_set(model, feature_extractors=feature_extractors,\n                              training_set=dataset_train,\n                              test_set=dataset_test)\n\n        #########################################  MLPClassifier #######################\n\n        print (""\\n**** CROSS VALIDATION EVALUATION (CORPUS: SemEval) ****\\n"")\n        model = Classifier(models=""nn"")\n        kfold = KFold(n_splits=10)\n        ev.eval_with_cross_validation(model, feature_extractors=feature_extractors,\n                                      training_set=dataset_train, num_fold=10, cv=kfold)\n        ev.create_evaluation_result(model, feature_extractors=feature_extractors,\n                                    training_set=dataset_train, num_fold=10, cv=kfold)\n\n        print (""\\n**** TEST SET EVALUATION (CORPUS: SemEval) ****\\n"")\n        ev.eval_with_test_set(model, feature_extractors=feature_extractors,\n                              training_set=dataset_train,\n                              test_set=dataset_test)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
src/Embedding/__init__.py,0,b''
src/Visualisation/Visualiser.py,0,"b'import pandas as pd  # provide sql-like data manipulation tools. very handy.\nfrom sklearn.manifold import TSNE\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom bokeh.models import HoverTool\nfrom bokeh.plotting import figure, show\nimport pprint\n\ndef WordVectorFileFormat(sizeOfEmbedding, VocabsFname, VectorsFname, wordVectorFileFormattedFname):\n    """"""\n    :return:\n    :param sizeOfEmbedding: Size of the words embedding in the vectors file (200)\n    :param VocabsFname: Filename where vocab words used for embedding are stored . txt Formatting of the file (\'vocabs.txt\')\n    :param VectorsFname: Filename where vector embedding of the vocab words is located . txt Formatting of the file (\'vectors.txt\')\n    :param wordVectorFileFormattedFname: Filename where WordVectorFile will be stored (  ""w2vVectors_jdid.txt"" )\n    :return:\n\n    """"""\n\n    def file_len(fname):\n        """"""\n        :param fname: Filename of a file with a NON-zero number of rows\n        :return: number of rows of the input file\n        """"""\n        ### TODO : set an exception when the file is empty.\n        with open(fname) as f:\n            for i, l in enumerate(f):\n                pass\n        f.close()\n        return i + 1\n\n    with open(VectorsFname, \'r\') as vectorsFile:\n\n        with open(VocabsFname, \'r\') as vocabsFile:\n            vocabLine = vocabsFile.readlines()\n            vectorLine = vectorsFile.readlines()\n            newline = list()\n            for i in range(vocabLine.__len__()):\n                newline.append(vocabLine[i].rstrip() + \' \' + vectorLine[i])\n            # del newline[17915]\n\n            with open(wordVectorFileFormattedFname, ""w"") as wordVectorFileFormatted:\n                wordVectorFileFormatted.write(str(file_len(VocabsFname) - 1) + \' \' + str(sizeOfEmbedding) + \'\\n\')\n                for l in newline:\n                    wordVectorFileFormatted.writelines(l)\n            wordVectorFileFormatted.close()\n        vocabsFile.close()\n    vectorsFile.close()\n\n\nclass Visualiser():\n    def __init__(self, sizeOfEmbedding=50, VocabsFname=None,\n                 VectorsFname=None, WVFilename=None,\n                 visualizerHTMLfilename=None):\n        # attributes used for the WordVectorFileFormat function.\n        self.sizeOfEmbedding = sizeOfEmbedding\n        self.VocabsFname = VocabsFname\n        self.VectorsFname = VectorsFname\n        self.WVFilename = WVFilename\n        self.visualizerHTMLfilename = visualizerHTMLfilename\n\n    def visualize(self):\n        # Constructing the wordvectors File from vectors.txt and vocabs.txt\n        sizeOfEmbedding = self.sizeOfEmbedding\n        VocabsFname = self.VocabsFname\n        VectorsFname = self.VectorsFname\n        WVFilename = self.WVFilename\n        visualizerHTMLfilename = self.visualizerHTMLfilename\n        WordVectorFileFormat(sizeOfEmbedding, VocabsFname, VectorsFname, WVFilename)\n\n        # Save the vectors using : word_vectors = KeyedVectors.\n        word_vectors = KeyedVectors.load_word2vec_format(fname=WVFilename, binary=False)\n\n        # defining the chart\n        visualizerHTMLfilename = open(visualizerHTMLfilename)\n        # Configuring the figure parameters.\n        plot_tfidf = figure(plot_width=700, plot_height=600, title=""A map of 10000 word vectors"",\n                                           tools=""pan,wheel_zoom,box_zoom,reset,hover,previewsave"", x_axis_type=None,\n                                           y_axis_type=None,\n                                           min_border=1)\n\n        #most_similar_to_happy = word_vectors.most_similar(\'happy\')\n        #print(""\\n The most similar to happy are: "")\n        #print(most_similar_to_happy)\n        pp = pprint.PrettyPrinter(indent=4)\n        var = raw_input(""Please enter a word OR press q to quit: "")\n        while var !=""q"":\n            print(""\\n The most similar words using the cosine similarity measure to ""+var+"" are: "")\n            most_sim = word_vectors.most_similar(var)\n            pp.pprint(word_vectors.most_similar(var))\n            var = raw_input(""\\n Please enter a word to compute its similarity OR press q to quit: "")\n        print(""\\n The most similar words using the MCO measure to"")\n        var = raw_input(""Please enter a word press q to quit: "")\n        while var !=""q"":\n            print(""\\n The most similar to ""+var+"" are: "")\n            most_sim_cum = word_vectors.most_similar_cosmul(var)\n            pp.pprint(word_vectors.most_similar_cosmul(var))\n            var = raw_input(""Please enter a word OR press q to quit: "")\n        #most_similar_to_sad = word_vectors.most_similar(\'sad\')\n        #most_similar_to_ugly = word_vectors.most_similar(\'ugly\')\n\n        #  define the word_vectors to plot in the figure\n        word_vectors_to_plot = [word_vectors.word_vec(w) for w in word_vectors.vocab.keys()[:5000]]\n        # dimensionality reduction. converting the vectors to 2d vectors\n        tsne_model = TSNE(n_components=2, verbose=1, random_state=0)\n        tsne_w2v = tsne_model.fit_transform(word_vectors_to_plot)\n\n        # putting everything in a dataframe\n        pd.options.mode.chained_assignment = None\n        tsne_df = pd.DataFrame(tsne_w2v, columns=[\'x\', \'y\'])\n        # tsne_df[\'words\'] = tweet_w2v.wv.vocab.keys()[:5000]\n        tsne_df[\'words\'] = word_vectors.vocab.keys()[:5000]\n\n        # plotting. the corresponding word appears when you hover on the data point.\n        plot_tfidf.scatter(x=\'x\', y=\'y\', source=tsne_df)\n        hover = plot_tfidf.select(dict(type=HoverTool))\n        hover.tooltips = {""word"": ""@words""}\n        show(plot_tfidf)\n'"
src/Visualisation/__init__.py,0,b''
src/Visualisation/core.py,0,"b'import pandas as pd # provide sql-like data manipulation tools. very handy.\npd.options.mode.chained_assignment = None\nimport numpy as np # high dimensional vector computing library.\nfrom copy import deepcopy\nfrom string import punctuation\nfrom random import shuffle\n\nimport gensim\nfrom gensim.models.word2vec import Word2Vec # the word2vec model gensim class\nLabeledSentence = gensim.models.doc2vec.LabeledSentence # we\'ll talk about this down below\n\nfrom tqdm import tqdm\ntqdm.pandas(desc=""progress-bar"")\n\nfrom nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\ntokenizer = TweetTokenizer()\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\ndef ingest():\n\n    data = pd.read_csv(\'/Users/montassar/PycharmProjects/visualization/Visualisation/trainingandtestdata/movie-review-sentiment-data/sample.csv\')\n    data.drop([\'ItemID\', \'SentimentSource\',\'Blank\',\'Date\'], axis=1, inplace=True)\n    data = data[data.Sentiment.isnull() == False]\n    data[\'Sentiment\'] = data[\'Sentiment\'].map(int)\n    data = data[data[\'SentimentText\'].isnull() == False]\n    data.reset_index(inplace=True)\n    data.drop(\'index\', axis=1, inplace=True)\n    print \'dataset loaded with shape\', data.shape\n    return data\n\n\n\ndef tokenize(tweet):\n    try:\n        tweet = unicode(tweet.decode(\'utf-8\').lower())\n        tokens = tokenizer.tokenize(tweet)\n        tokens = filter(lambda t: not t.startswith(\'@\'), tokens)\n        tokens = filter(lambda t: not t.startswith(\'#\'), tokens)\n        tokens = filter(lambda t: not t.startswith(\'http\'), tokens)\n        return tokens\n    except:\n        return \'NC\'\n\n\ndef postprocess(data, n=1000000):\n    data = data.head(n)\n    data[\'tokens\'] = data[\'SentimentText\'].progress_map(tokenize)  ## progress_map is a variant of the map function plus a progress bar. Handy to monitor DataFrame creations.\n    data = data[data.tokens != \'NC\']\n    data.reset_index(inplace=True)\n    data.drop(\'index\', inplace=True, axis=1)\n    return data\n\n\ndef labelizeTweets(tweets, label_type):\n    labelized = []\n    for i,v in tqdm(enumerate(tweets)):\n        label = \'%s_%s\'%(label_type,i)\n        labelized.append(LabeledSentence(v, [label]))\n    return labelized\n'"
src/analyzer/WordEmbeddingAnalyzer.py,0,"b'class WordEmbeddingAnalyzer(object):\n    """"""docstring for WordEmbeddingAnalyzer""""""\n    def __init__(self, model_path=None, senna=0):\n        super(WordEmbeddingAnalyzer, self).__init__()\n        if model_path:\n            self.model = model_path\n            self.is_senna = False\n\n    def load_model(self, model_path):\n        pass\n\n    def most_similar(self, word):\n        pass\n\n    def visualize(self):\n        pass\n'"
src/classification/Classifier.py,0,"b'import sys\n\nfrom sklearn import metrics\n\nfrom feature_extractor import FeatureExtractor\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nclass Classifier(object):\n    """"""docstring for Classifier""""""\n\n    def __init__(self, models=""multinomial""):\n        super(Classifier, self).__init__()\n        self.models = models\n        if models == ""multinomial"":\n            self.classifier = MultinomialNB()\n        elif models == ""svm"":\n            self.classifier = SVC(kernel=\'linear\')\n        elif models == ""rfc"":\n            self.classifier = RandomForestClassifier()\n        elif models == ""nn"":\n            self.classifier = MLPClassifier()\n\n    def classify(self, dataset):\n        contents = dataset.get_contents()\n        labels = dataset.get_labels()\n        return self.classify_raw(contents, labels)\n\n    def classify_raw(self, dataset, labels):\n        self.classifier = self.classifier.fit(dataset, labels)\n        return self.classifier\n\n    """"""Return predictions for dataset using Dataset class""""""\n\n    def test(self, dataset):\n        contents = dataset.get_contents()\n        return self.test_raw(contents)\n\n    """"""Return predictions for dataset using raw array dataset""""""\n\n    def test_raw(self, dataset):\n        predictions = self.classifier.predict(dataset)\n        return predictions\n\n    def get_classifier_type(self):\n        if self.models == ""multinomial"":\n            return ""Multinomial Naive-Bayes""\n        elif self.models == ""svm"":\n            return ""Support Vector Machine""\n        elif self.models == ""rfc"":\n            return ""Random Forest Classifier""\n        elif self.models == ""nn"":\n            return ""Multilayer Perceptron (Neural Network)""\n        else:\n            return ""Unknown classifier""\n\n\ndef main(filename):\n    fe = FeatureExtractor(""tfidf"", filename)\n    fe.load_dataset()\n    fe.load_labels()\n\n    bow = fe.build_bag()\n    bag = fe.build_tfidf()\n\n    print ""** Using Multinomial NB Models **""\n\n    # TFIDF\n    clf = Classifier(models=""multinomial"")\n    clf.classify(bag, fe.raw_labels)\n\n    preds = clf.test(bag)\n    # for doc, cat in zip(fe.dataset, preds):\n    # \tprint ""%r => %s"" % (doc, cat)\n\n    print ""TFIDF accuracy score: %f"" % (metrics.accuracy_score(fe.raw_labels, preds, normalize=True))\n    f1_pos = metrics.f1_score(fe.raw_labels, preds, pos_label=\'positive\')\n    f1_neg = metrics.f1_score(fe.raw_labels, preds, pos_label=\'negative\')\n    f1_neu = metrics.f1_score(fe.raw_labels, preds, pos_label=\'neutral\')\n    print ""TFIDF F1 score: %f"" % f1_pos\n    print ""TFIDF F1 negative score: %f"" % f1_neg\n    print ""TFIDF F1 neutral score: %f"" % f1_neu\n\n    print ""\\nAverage F-measure: %f"" % ((f1_pos + f1_neg + f1_neu ) / 2)\n\n    # bag of words\n    clf = Classifier(models=""multinomial"")\n    clf.classify(bow, fe.raw_labels)\n    preds = clf.test(bow)\n\n    print ""BOW accuracy score: %f"" % (metrics.accuracy_score(fe.raw_labels, preds, normalize=True))\n    print ""BOW F1 score: %f"" % (metrics.f1_score(fe.raw_labels, preds, pos_label=\'positive\'))\n\n    print ""\\n** Using SVM **""\n\n    # TFIDF\n    clf = Classifier(models=""svm"")\n    clf.classify(bag, fe.raw_labels)\n\n    preds = clf.test(bag)\n    # for doc, cat in zip(fe.dataset, preds):\n    # \tprint ""%r => %s"" % (doc, cat)\n\n    print ""TFIDF accuracy score: %f"" % (metrics.accuracy_score(fe.raw_labels, preds, normalize=True))\n\n    # bag of words\n    clf = Classifier(models=""svm"")\n    clf.classify(bow, fe.raw_labels)\n    preds = clf.test(bow)\n\n    print ""BOW accuracy score: %f"" % (metrics.accuracy_score(fe.raw_labels, preds, normalize=True))\n\n    X_train, X_test, y_train, y_test = train_test_split(bow, fe.raw_labels, test_size=0.4, random_state=0)\n    clf = Classifier(models=""svm"")\n    clf.classify(X_train, y_train)\n    preds = clf.test(X_test)\n\n    print ""Using 60/40, BOW accuracy: %f"" % (metrics.accuracy_score(y_test, preds, normalize=True))\n    print ""Using 60/40, BOW F1: %f"" % (metrics.f1_score(y_test, preds, pos_label=\'positive\'))\n\n\nif __name__ == \'__main__\':\n    main(sys.argv[1])\n'"
src/classification/Evaluator.py,1,"b'from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.model_selection import KFold\n\nimport copy\nimport csv\n\nimport numpy as np\n\nverbose_level = 0  # verbose level\nn_job = 3        # number of CPU used in evaluation\nseed = 7           # seed for our random state cross validation\n\nfrom nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\ntokenizer = TweetTokenizer()\nfrom sklearn.preprocessing import scale\n\nclass Evaluator(object):\n    """"""docstring for Evaluator""""""\n    def __init__(self):\n        super(Evaluator, self).__init__()\n\n    def eval_with_test_set(self, model, feature_extractors, training_set, test_set, outfile=""results_test.csv""):\n        if not isinstance(feature_extractors, list):\n            return\n\n        training_contents = training_set.get_contents()\n        training_labels = training_set.get_labels()\n\n        # build training features\n\n        # Print properties\n        print ""Evaluation method: Test Set""\n        print ""Classifier: %s"" % (model.get_classifier_type())\n\n        test_contents = test_set.get_contents()\n        test_labels = test_set.get_labels()\n\n        field_names = [""id"", ""content"", ""polarity""]\n        fe_predictions = dict()\n\n        for feature_extractor in feature_extractors:\n            fe = copy.copy(feature_extractor)\n            print ""\\nFeature Extractor: %s"" % (fe.get_name())\n            field_names.append(fe.get_name())\n\n            # build our feature extractor from the training dataset contents\n\n            fe.set_dataset(training_contents)\n            fe.build()\n            training_contents = [tweet.split() for tweet in training_contents]\n\n            training_features = fe.extract_features(training_contents)\n            #print(""training features :"")\n            #print(training_features)\n            # build features for our test dataset\n            test_contents = [tweet.split() for tweet in test_contents]\n            test_features = fe.extract_existing_features(test_contents)\n            #print(""test features :"")\n            #print(test_features)\n            # build training models\n            model.classify_raw(training_features, training_labels)\n\n            # start evaluating with test set\n            test_predictions = model.test_raw(test_features)\n            fe_predictions[fe.get_name()] = test_predictions\n\n            # evaluate confusion matrix\n            cnf_matrix = confusion_matrix(test_labels, test_predictions,labels=[\'positive\', \'negative\',\'neutral\'])\n\n            print ""Average F-measure: %f"" % (f1_score(test_labels, test_predictions, average=\'macro\'))\n            print ""Average accuracy : %f"" % (f1_score(test_labels, test_predictions, average=\'micro\'))\n            print ""\\nConfusion Matrix:""\n            print ""\\t\\tPositive\\tNegative\\tNeutral (predicted labels)""\n            print ""Positive\\t%d\\t\\t%d\\t\\t%d"" % (cnf_matrix[0][0], cnf_matrix[0][1],cnf_matrix[0][2])\n            print ""Negative\\t%d\\t\\t%d\\t\\t%d"" % (cnf_matrix[1][0], cnf_matrix[1][1],cnf_matrix[1][2])\n            print ""Neutral \\t%d\\t\\t%d\\t\\t%d"" % (cnf_matrix[2][0], cnf_matrix[2][1],cnf_matrix[2][2])\n            print ""(actual labels)\\n""\n\n        with open(outfile, ""wb"") as csvfile:\n            writer = csv.DictWriter(csvfile, fieldnames=field_names)\n            writer.writeheader()\n            for i in xrange(len(test_contents)):\n                row = {\n                    \'id\': i + 1,\n                    \'content\': test_contents[i],\n                    \'polarity\': test_labels[i],\n                }\n                # append results\n                for j in xrange(len(feature_extractors)):\n                    row[feature_extractors[j].get_name()] = fe_predictions[feature_extractors[j].get_name()][i]\n\n                writer.writerow(row)\n\n    def eval_with_cross_validation(self, model, feature_extractors, training_set, num_fold=10, cv=None):\n        if not isinstance(feature_extractors, list):\n            return\n\n        # if model\n        training_contents = training_set.get_contents()\n        training_labels = training_set.get_labels()\n\n        # Print properties\n        print ""Evaluation method: Cross Validation""\n        print ""Number of Folds: %d"" % (num_fold)\n        print ""Classifier: %s"" % (model.get_classifier_type())\n\n        if not cv:\n            kfold = KFold(n_splits=num_fold, random_state=seed)\n        else:\n            kfold = cv\n\n        for feature_extractor in feature_extractors:\n            fe = copy.copy(feature_extractor)\n            print ""\\nFeature Extractor: %s"" % (fe.get_name())\n\n            # build our feature extractor from the dataset contents\n            fe.set_dataset(training_contents)\n            fe.build()\n            training_contents = [tweet.split() for tweet in training_contents]\n            training_features = fe.extract_features(training_contents)\n            # obtain our classification results\n            # measure is done by using macro F1 score\n            scores = cross_val_score(model.classifier, X=training_features,\n                                    y=training_labels, cv=kfold, n_jobs=n_job,\n                                    scoring=\'f1_macro\', verbose=verbose_level)\n\n            # print each of the iteration scroe\n            for i in xrange(0, len(scores)):\n                print ""Iteration %d = %f"" % (i + 1, scores[i])\n\n            print ""Average score: %f"" % (scores.mean())\n            print ""Standard Deviation: %f"" % (scores.std())\n            print ""Maximum F1-score: %f"" % (np.amax(scores))\n\n\n    def create_evaluation_result(self, model, feature_extractors, training_set, num_fold=10, outfile=""results_cv.csv"", cv=None):\n        if not isinstance(feature_extractors, list):\n            return\n\n        # if model\n        training_contents = training_set.get_contents()\n        training_labels = training_set.get_labels()\n\n        # Print properties\n        print ""Evaluation method: Cross Validation""\n        print ""Number of Folds: %d"" % (num_fold)\n        print ""Classifier: %s"" % (model.get_classifier_type())\n\n        field_names = [""id"", ""content"", ""polarity""]\n        fe_predictions = dict()\n\n        if not cv:\n            kfold = KFold(n_splits=num_fold, random_state=seed)\n        else:\n            kfold = cv\n\n        for feature_extractor in feature_extractors:\n            fe = copy.copy(feature_extractor)\n            field_names.append(fe.get_name())\n\n            # build our feature extractor from the dataset contents\n            fe.set_dataset(training_contents)\n            fe.build()\n            training_contents = [tweet.split() for tweet in training_contents]\n            training_features = fe.extract_features(training_contents)\n            # obtain our classification results\n            # measure is done by using macro F1 score\n            predictions = cross_val_predict(model.classifier, X=training_features,\n                                            y=training_labels, cv=kfold, n_jobs=n_job,\n                                            verbose=verbose_level,fit_params={})\n            fe_predictions[fe.get_name()] = predictions\n\n        with open(outfile, ""wb"") as csvfile:\n            writer = csv.DictWriter(csvfile, fieldnames=field_names)\n            writer.writeheader()\n            for i in xrange(len(training_contents)):\n                row = {\n                    \'id\': i + 1,\n                    \'content\': training_contents[i],\n                    \'polarity\': training_labels[i],\n                }\n                # append results\n                for j in xrange(len(feature_extractors)):\n                    row[feature_extractors[j].get_name()] = fe_predictions[feature_extractors[j].get_name()][i]\n\n                writer.writerow(row)\n\n        return outfile\n'"
src/classification/NeuralNets.py,1,"b'from keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import scale\nfrom tqdm import tqdm\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\n\ndef split_data(dataset):\n\n    x_samples = dataset.get_contents()\n    y_labels = dataset.get_labels()\n    y = []\n    for x in y_labels:\n        if x==""positive"":\n            y.append(1)\n        elif x==""negative"":\n            y.append(-1)\n        else:\n            y.append(0)\n\n    return x_samples,y\n\n\ndef build_tfidf(x_train):\n    print \'building tf-idf matrix ...\'\n    vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n    vectorizer.fit_transform([x.split() for x in x_train])\n    tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n    print \'vocab size :\', len(tfidf)\n    return tfidf\n\n\nclass NeuralNets(object):\n    def __init__(self, input_size=100,x_train=None, y_train=None,\n                 epochs=20, batch_size=32,x_test=None, y_test=None):\n            self.inputdim = input_size\n            self.xtrain = x_train\n            self.ytrain = y_train\n            self.epochs = epochs\n            self.xtest = x_test\n            self.ytest = y_test\n            self.batchsize = batch_size\n\n\n    def train_neural_nets(self):\n\n        ""*** Train Neural Networks model ***""\n\n        model = Sequential()\n        model.add(Dense(200, activation=\'relu\', input_dim=self.inputdim))\n        #model.add(Dense(32, activation=\'softsign\'))\n        model.add(Dense(1, activation=\'sigmoid\'))\n        model.compile(optimizer=Adam(lr=0.01),loss=\'binary_crossentropy\',metrics=[\'accuracy\',\'mse\',\'mae\'])\n        print(""\\n Training Neural Network Classifier with Training dataset"")\n        model.fit(self.xtrain, self.ytrain, epochs=self.epochs, batch_size=self.batchsize, verbose=2)\n        print(""\\n Evaluating Neural Network Classifier on Test dataset"")\n        score = model.evaluate(self.xtest, self.ytest, batch_size=128, verbose=2)\n        print(""{} is {}"".format(""accuracy"",score[1]))\n        print(""{} is {}"".format(""mse: "",score[2]))\n        print(""{} is {}"".format(""mae: "", score[3]))\n        y_predictions = model.predict(self.xtest, batch_size=128, verbose=2)\n        y_pred = np.around(y_predictions)\n        y_pred = [int(x) for x in y_pred.flatten().tolist()]\n        cnf_matrix = confusion_matrix(self.ytest, y_pred,labels=[1,-1,0])\n        print ""Average F-measure: %f"" % (f1_score(self.ytest, y_pred, average=\'macro\'))\n        print ""\\n Confusion Matrix:""\n        print ""\\t\\tPositive\\tNegative\\tNeutral (predicted labels)""\n        print ""Positive\\t%d\\t\\t%d\\t\\t%d"" % (cnf_matrix[0][0], cnf_matrix[0][1], cnf_matrix[0][2])\n        print ""Negative\\t%d\\t\\t%d\\t\\t%d"" % (cnf_matrix[1][0], cnf_matrix[1][1], cnf_matrix[1][2])\n        print ""Neutral \\t%d\\t\\t%d\\t\\t%d"" % (cnf_matrix[2][0], cnf_matrix[2][1], cnf_matrix[2][2])\n        print ""(actual labels)\\n""\n'"
src/classification/__init__.py,0,b'from Classifier import Classifier\nfrom Evaluator import Evaluator\nfrom NeuralNets import NeuralNets\n'
src/feature_extractor/BagFeatureExtractor.py,0,"b'from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom FeatureExtractor import FeatureExtractor\n\n\nclass BagFeatureExtractor(FeatureExtractor):\n    """"""docstring for FeatureExtractor""""""\n    def __init__(self, dataset=None, size=0):\n        super(BagFeatureExtractor, self).__init__(dataset)\n        self.size = size\n        self.reducer = None\n\n    def build(self):\n        self.vectorizer = CountVectorizer()\n        self.vectorizer.fit(self.dataset)\n\n        # Build svd reducer\n        if self.size > 0:\n            self.reducer = TruncatedSVD(n_components=self.size)\n        return self\n\n    def extract_features(self, dataset):\n        features = super(BagFeatureExtractor, self).extract_features(dataset)\n        if self.reducer:\n            return self.reducer.fit_transform(features)\n        else:\n            return features\n\n    def extract_existing_features(self, dataset):\n        features = super(BagFeatureExtractor, self).extract_features(dataset)\n        if self.reducer:\n            return self.reducer.transform(features)\n        else:\n            return features\n\n    def get_name(self):\n        return ""Bag of Words""\n'"
src/feature_extractor/FeatureExtractor.py,0,"b'class FeatureExtractor(object):\n\t""""""docstring for FeatureExtractor""""""\n\tdef __init__(self, dataset=None):\n\t\tif dataset:\n\t\t\tself.dataset = dataset\n\t\tself.vectorizer = None\n\n\tdef set_dataset(self, dataset):\n\t\tself.dataset = dataset\n\n\tdef extract_features(self, dataset):\n\t\treturn self.vectorizer.transform(dataset)\n\n\tdef get_feature_size(self):\n\t\treturn len(self.vectorizer.get_feature_names())\n\n\tdef save_vocab(self, outfile):\n\t\twith open(outfile, ""wb"") as vocabfile:\n\t\t\tfor vocab in self.vectorizer.get_feature_names():\n\t\t\t\tvocabfile.write(vocab + ""\\n"")\n\n\t\treturn outfile\n'"
src/feature_extractor/SennaFeatureExtractor.py,1,"b'import numpy as np\n\nfrom FeatureExtractor import FeatureExtractor\nfrom vectorizer.WordEmbeddingVectorizer import WordEmbeddingVectorizer\n\nfrom models.SentenceIterator import SentenceIterator\n\n\nclass SennaFeatureExtractor(FeatureExtractor):\n    """"""docstring for FeatureExtractor""""""\n    def __init__(self, dataset=None, infile=None, vocabfile=None, binary=False, dimen=100):\n        self.model_file = infile\n        self.vocab_file = vocabfile\n        self.binary = binary\n        self.dataset = dataset\n        self.dimen = dimen\n\n    def build(self):\n        if self.model_file and self.vocab_file:\n            vocabs = []\n            models = []\n            with open(self.vocab_file, ""rb"") as vocablist:\n                for vocab in vocablist:\n                    vocabs.append(vocab.rstrip())\n\n            with open(self.model_file, ""rb"") as modellist:\n                for model in modellist:\n                    arr_model = model.split()\n                    models.append(np.array(map(float, arr_model)))\n                #modelss = models[:100]\n                #vocabss = vocabs[:100]\n            # build our word embedding model vectorizer\n            senna_dict = dict(zip(vocabs, models))\n            sentences = SentenceIterator(self.dataset)\n\n            self.vectorizer = WordEmbeddingVectorizer(senna_dict, self.dimen)\n            self.vectorizer.fit(sentences)\n        else:\n            pass\n\n        return self\n\n    def extract_existing_features(self, dataset):\n        return super(SennaFeatureExtractor, self).extract_features(dataset)\n\n    def get_name(self):\n        return ""SENNA C&W SSWE""\n'"
src/feature_extractor/TfidfFeatureExtractor.py,0,"b'from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom FeatureExtractor import FeatureExtractor\n\n\nclass TfidfFeatureExtractor(FeatureExtractor):\n    """"""docstring for FeatureExtractor""""""\n    def __init__(self, dataset=None, size=0):\n        super(TfidfFeatureExtractor, self).__init__(dataset)\n        self.size = size\n        self.reducer = None\n\n    def build(self):\n        self.vectorizer = TfidfVectorizer()\n        self.vectorizer.fit(self.dataset)\n\n        # Build svd reducer\n        if self.size > 0:\n            self.reducer = TruncatedSVD(n_components=self.size)\n        return self\n\n    def extract_features(self, dataset):\n        features = super(TfidfFeatureExtractor, self).extract_features(dataset)\n        if self.reducer:\n            return self.reducer.fit_transform(features)\n        else:\n            return features\n\n    def extract_existing_features(self, dataset):\n        features = super(TfidfFeatureExtractor, self).extract_features(dataset)\n        if self.reducer:\n            return self.reducer.transform(features)\n        else:\n            return features\n\n    def get_name(self):\n        return ""Term Frequency - Inverse Document Frequency""\n'"
src/feature_extractor/WordEmbeddingFeatureExtractor.py,0,"b'import gensim\n\nfrom FeatureExtractor import FeatureExtractor\nfrom vectorizer.WordEmbeddingVectorizer import WordEmbeddingVectorizer\n\nfrom models.SentenceIterator import SentenceIterator\n\n\nclass WordEmbeddingFeatureExtractor(FeatureExtractor):\n    """"""docstring for FeatureExtractor""""""\n    def __init__(self, dataset=None, infile=None, binary=False, dimen=100, sswe=0):\n        super(WordEmbeddingFeatureExtractor, self).__init__(dataset)\n        self.model_file = infile\n        self.binary = binary\n        self.dimen = dimen\n        self.sswe = sswe\n\n    def build(self):\n        if not self.model_file:\n            sentences = SentenceIterator(self.dataset)\n            w2v = gensim.models.Word2Vec(sentences, size=self.dimen, min_count=1)\n            word_vectors = w2v.wv\n            del w2v   # free memory\n        else:\n            word_vectors = gensim.models.KeyedVectors.load_word2vec_format(self.model_file, binary=self.binary)\n\n            # build our word embedding model vectorizer\n            # w2v_dict = dict(zip(w2v.index2word, w2v.syn0))\n            sentences = SentenceIterator(self.dataset)\n\n        self.vectorizer = WordEmbeddingVectorizer(word_vectors, self.dimen)\n        self.vectorizer.fit(sentences)\n\n        return self\n\n    def extract_existing_features(self, dataset):\n        return super(WordEmbeddingFeatureExtractor, self).extract_features(dataset)\n\n    def save_model_to_file(self, outfile, vocabfile=None, binary=True):\n        sentences = SentenceIterator(self.dataset)\n        w2v = gensim.models.Word2Vec(sentences, size=self.dimen, min_count=1, sg=1, workers=4, iter=10)\n\n        w2v.wv.save_word2vec_format(outfile, fvocab=vocabfile, binary=binary)\n\n    def get_name(self):\n        if self.sswe == 1:\n            return ""SSWE + Word2Vec""\n        else:\n            return ""Gensim Word2Vec""\n'"
src/feature_extractor/__init__.py,0,b'from FeatureExtractor import FeatureExtractor\nfrom BagFeatureExtractor import BagFeatureExtractor\nfrom TfidfFeatureExtractor import TfidfFeatureExtractor\nfrom WordEmbeddingFeatureExtractor import WordEmbeddingFeatureExtractor\nfrom SennaFeatureExtractor import SennaFeatureExtractor'
src/models/Dataset.py,0,"b'import csv\nimport sys\nimport random\n\nfrom Review import Review\nfrom sklearn.model_selection import train_test_split\n\n\nclass DatasetReview():\n    """"""docstring for Dataset""""""\n\n    def __init__(self):\n        self.dataset = []\n        self.field_names = []\n        self.label_values = []\n        self.column_label = """"\n\n    def load_review_from_csv(self, infile):\n        with open(infile, ""rb"") as csvfile:\n            reader = csv.DictReader(csvfile)\n\n            # init field names & label column\n            self.field_names = reader.fieldnames\n            self.column_label = self.field_names[-1]\n\n            for rows in reader:\n                review = Review(rows[self.field_names[0]], rows[self.field_names[1]])\n                self.dataset.append(review)\n                if self.label_values.count(rows[self.column_label]) == 0:\n                    self.label_values.append(rows[self.column_label])\n\n        return infile\n\n    def dataset_from_array(self, dataset):\n        n_dataset = DatasetReview()\n        n_dataset.dataset = dataset\n        n_dataset.field_names = self.field_names\n        n_dataset.label_values = self.label_values\n        n_dataset.column_label = self.column_label\n\n        return n_dataset\n\n    def dataset_from_contents_labels(self, contents, labels):\n        arr_dataset = []\n        for i in xrange(len(contents)):\n            dr = Review(contents[i], labels[i])\n            arr_dataset.append(dr)\n\n        return self.dataset_from_array(arr_dataset)\n\n    def get_dataset_size(self):\n        return len(self.dataset)\n\n    """"""get text content for datasets""""""\n\n    def get_contents(self):\n        res = []\n        for data in self.dataset:\n            res.append(data.content)\n\n        return res\n\n    """""" get labels for all datasets """"""\n\n    def get_labels(self):\n        res = []\n        for data in self.dataset:\n            res.append(data.polarity)\n\n        return res\n\n    def get_label_enum(self):\n        return self.label_values\n\n    def get_dataset(self, idx):\n        return self.dataset[idx]\n\n    def get_formatted_dataset(self):\n        res = []\n        for data in self.dataset:\n            res.append(data.to_string())\n\n        return res\n\n    def export_formatted_dataset(self, outfile):\n        res = self.get_formatted_dataset()\n        with open(outfile, ""wb"") as f:\n            for row in res:\n                f.write(row + ""\\n"")\n\n        return outfile\n\n    def export_to_csv(self, outfile):\n        with open(outfile, ""wb"") as csvfile:\n            writer = csv.DictWriter(csvfile, fieldnames=self.field_names)\n            writer.writeheader()\n            for data in self.dataset:\n                writer.writerow({\n                    \'content\': data.content,\n                    \'polarity\': data.polarity\n                })\n\n        return outfile\n\n    def get_data_label_size(self, label):\n        return sum(1 for x in self.dataset if x.polarity == label)\n\n    def get_data_label(self, label):\n        return [data for data in self.dataset if data.polarity == label]\n\n    def get_sample_to_minority(self):\n        if not self.dataset:\n            return []\n        else:\n            pos_sample = self.get_data_label_size(""positive"")\n            neg_sample = self.get_data_label_size(""negative"")\n            neu_sample = self.get_data_label_size(""neutral"")\n\n            print ""%d | %d | %d"" % (pos_sample, neg_sample, neu_sample)\n            t_dataset = []\n            if pos_sample > neg_sample:\n                temp = self.get_data_label(""positive"")\n                for x in xrange(0, neg_sample):\n                    idx = random.randint(0, len(temp) - 1)\n                    t_dataset.append(temp[idx])\n\n                # append the minority instance\n                t_dataset.extend(self.get_data_label(""negative""))\n                m_dts = self.dataset_from_array(t_dataset)\n                return m_dts\n\n            elif neg_sample > pos_sample:\n                temp = self.get_data_label(""negative"")\n                for x in xrange(1, pos_sample):\n                    idx = random.randint(0, len(temp) - 1)\n                    t_dataset.append(temp[idx])\n\n                # append the minority instance\n                t_dataset.extend(self.get_data_label(""positive""))\n                m_dts = self.dataset_from_array(t_dataset)\n                return m_dts\n\n            else:\n                return self\n\n    def split_to_ratio(self, ratio):\n        X_train, X_test, y_train, y_test = train_test_split(self.get_contents(), self.get_labels(), test_size=ratio)\n\n        dataset_train = self.dataset_from_contents_labels(X_train, y_train)\n        dataset_test = self.dataset_from_contents_labels(X_test, y_test)\n\n        return dataset_train, dataset_test\n\n    def export_only_contents(self, outfile):\n        with open(outfile, ""wb"") as ofile:\n            for data in self.dataset:\n                ofile.write(data.content + ""\\n"")\n\n        return outfile\n\n\ndef main(infile):\n    dataset = DatasetReview()\n    dataset.load_review_from_csv(infile)\n    print dataset.get_label_enum()\n    dataset.export_formatted_dataset(""formatted_dataset.txt"")\n\n    print ""Positive instances: %d"" % (dataset.get_data_label_size(""positive""))\n    print ""Negative instances: %d"" % (dataset.get_data_label_size(""negative""))\n\n    t_dataset = dataset.get_sample_to_minority()\n    print ""Positive instances: %d"" % (t_dataset.get_data_label_size(""positive""))\n    print ""Negative instances: %d"" % (t_dataset.get_data_label_size(""negative""))\n    t_dataset.export_to_csv(""sample.csv"")\n\n\nif __name__ == \'__main__\':\n    main(sys.argv[1])\n'"
src/models/Review.py,0,"b'class Review(object):\n    num_review = 0\n    uid_placeholder = 1\n    """"""docstring for Review""""""\n\n    def __init__(self, content, polarity):\n        super(Review, self).__init__()\n        self.content = content\n        self.polarity = polarity\n        Review.num_review += 1\n        self.id = Review.num_review\n\n    def to_string(self):\n        return str(self.id) + ""\\t"" + str(Review.uid_placeholder) + ""\\t"" + self.polarity + ""\\t"" + self.content\n'"
src/models/SentenceIterator.py,0,"b'class SentenceIterator(object):\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __iter__(self):\n        for sentence in self.dataset:\n            yield sentence.split()\n\n'"
src/models/__init__.py,0,b'from Dataset import DatasetReview\nfrom Review import Review\nfrom SentenceIterator import SentenceIterator'
src/preprocess/__init__.py,0,b'from preprocesstweets import tweets_prepocess'
src/preprocess/preprocesstweets.py,0,"b'import csv\nimport re\n\n\ndef replaceTwoOrMore(s):\n    # look for 2 or more repetitions of character\n    patt = re.compile(r""(.)\\1{1,}"", re.DOTALL)\n    return patt.sub(r""\\1\\1"", s)\n\n\ndef processTweet(tweet):\n    # process the tweets\n    # Convert to lower case\n    tweet = tweet.lower()\n    # Convert www.* or https?://* to URL\n    tweet = re.sub(\'((www\\.[^\\s]+)|(https?://[^\\s]+))\', \'url\', tweet)\n    # Convert @username to AT_USER\n    tweet = re.sub(\'@[^\\s]+\', \'at_user\', tweet)\n    # Remove additional white spaces\n    tweet = re.sub(\'[\\s]+\', \' \', tweet)\n    # Replace #word with word\n    tweet = re.sub(r\'#([^\\s]+)\', r\'\\1\', tweet)\n    # trim\n    tweet = tweet.strip(\'\\\'""\')\n    return tweet\n\n\ndef getStopWordList(fname):\n    # read the stopwords\n    stopWords = []\n    stopWords.append(\'rt\')\n    stopWords.append(\'url\')\n    stopWords.append(\'at_user\')\n\n    fp = open(fname, \'r\')\n    line = fp.readline()\n    while line:\n        word = line.strip()\n        stopWords.append(word)\n        line = fp.readline()\n    fp.close()\n    return stopWords\n\n\ndef getFeatureVector(tweet, stopWords):\n    featureVector = []\n    words = tweet.split()\n    for w in words:\n        # replace two or more with two occurrences\n        w = replaceTwoOrMore(w)\n        # strip punctuation\n        w = w.strip(\'\\\'""?,.\')\n        # check if it consists of only words\n        val = re.search(r""^[a-zA-Z][a-zA-Z0-9]*[a-zA-Z]+[a-zA-Z0-9]*$"", w)\n        # ignore if it is a stopWord\n        if (w in stopWords or val is None):\n            continue\n        else:\n            featureVector.append(w.lower())\n    return featureVector\n\n\ndef tweets_prepocess(inputfile, outputfile, stopwordfile):\n    with open(inputfile, \'r\') as f:\n        readerCSV = csv.reader(f, delimiter=\',\', dialect=\'excel\')\n        #print(readerCSV)\n        data = []\n        stopwords = getStopWordList(stopwordfile)\n        for row in readerCSV:\n            if len(row)>1:\n                tweet = row[0]\n                label = row[1]\n                #print(tweet)\n                tweet_processed = processTweet(replaceTwoOrMore(tweet))\n                tweet_features = getFeatureVector(tweet_processed, stopwords)\n                #print(tweet)\n                #print(""tweet"")\n                #print(tweet_features)\n                if len(tweet_features) > 3:\n                    tweet_tokens = \' \'.join(tweet_features)\n                    #data.append([\'"" {} ""\'.format(tweet_tokens),\'"" {} ""\'.format(label)])\n                    data.append([\'"" {} ""\'.format(tweet_tokens),label])\n                    #data.append([tweet_tokens, label])\n    f.close()\n\n    # preprocess and store in a new clean csv file\n    with open(outputfile, ""w"") as fw:\n        writer = csv.writer(fw, delimiter=\'\\t\', lineterminator=\'\\n\', dialect=\'excel\', quoting=csv.QUOTE_NONE,\n                            quotechar=None)\n        #writer = csv.writer(fw, delimiter=\'\\t\', lineterminator=\'\\n\', dialect=\'excel\', quoting=csv.QUOTE_NONE, quotechar=None)\n        #writer = csv.writer(fw, delimiter=\'\\t\', dialect=\'excel-tab\',quoting=csv.QUOTE_NONE, quotechar=None)\n        for x in data:\n            if len(x[1]) != 0:\n                writer.writerow(x)\n    fw.close()\n'"
src/Embedding/sswemodel/__init__.py,0,b'from sswe_extractor import *'
src/Embedding/sswemodel/sswe_extractor.py,3,"b'#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n""""""\nCreated on Fri Sep  8 15:43:24 2017\n\n@author: Mounir Ouled Ltaief\n""""""\n\nimport logging\nimport numpy as np\nfrom ConfigParser import ConfigParser\nfrom itertools import chain\n\n# allow executing from anywhere without installing the package\nimport sys\nimport os\nimport distutils.util\n\nbuilddir = os.path.dirname(os.path.realpath(__file__)) + \'/../build/lib.\'\nlibdir = builddir + distutils.util.get_platform() + \'-\' + \'.\'.join(map(str, sys.version_info[:2]))\n# sys.path.append(libdir)\nsys.path.insert(0, libdir)\n\n# local\nfrom deepnl import *\nfrom deepnl.extractors import *\nfrom deepnl.reader import TweetReader\nfrom deepnl.network import Network\nfrom deepnl.sentiwords import SentimentTrainer\n\n\n# ----------------------------------------------------------------------\n# ----------------------------------------------------------------------\nclass sswe_model(object):\n    def __init__(self, window=3, embeddings_size=50, epochs=100, learning_rate=0.001,\n                 eps=1e-8, ro=0.95, hidden=200, ngrams=2, textField=0,\n                 tagField=1, alpha=0.5, train=None, model=None,\n                 vocab=None, minOccurr=3, vocab_size=0, vectors=None, load=None,\n                 threads=5, variant=None, verbose=None, config_file=None):\n        self.window = window\n        self.embeddings_size = embeddings_size\n        self.iterations = epochs\n        self.learning_rate = learning_rate\n        self.eps = eps\n        self.ro = ro\n        self.hidden = hidden\n        self.ngrams = ngrams\n        self.textField = textField\n        self.tagField = tagField\n        self.alpha = alpha\n        self.train = train\n        self.vocab = vocab\n        self.minOccurr = minOccurr\n        self.vocab_size = vocab_size\n        self.vectors = vectors\n        self.load = load\n        self.variant = variant\n        self.verbose = verbose\n        self.model = model\n        self.threads = threads\n        self.config_file = config_file\n\n\n\ndef create_sswe_model(train_filename, vocab_file, vector_file, train_model, save_model, size):\n    """"""model parameters: you can customize other parameters in the class sswe_mode()""""""\n    emb_size = size  # Number of features per word\n    epochs = 100  # Number of training epochs\n    l_r = 0.1  # Learning rate for network weights\n    hidden = 200   # Number of hidden neurons\n    ngrams = 2  # Length of ngrams\n    text = 0  # field containing text\n    tag = 1  # field containing polarity\n    train = train_filename  # File with text corpus for training\n    model = save_model  # File where to save the model\n    vocab = vocab_file  # Vocabulary file, either read and updated or created\n    vectors = vector_file  # Embeddings file, either read and updated or created\n    load = train_model  # Load previously saved model\n    threads = 15  # Number of threads\n    variant = None\n\n    sswe = sswe_model(embeddings_size=emb_size, epochs=epochs, learning_rate=l_r, threads=threads,\n                      hidden=hidden, ngrams=ngrams, textField=text, tagField=tag, train=train,\n                      model=model, vocab=vocab, minOccurr=3, vectors=vectors, load=load, variant=variant)\n    return sswe\n\n\n\ndef sswe_trainer(model_parameters):\n    # set the seed for replicability\n    np.random.seed(42)\n    # args = parser.parse_args()\n    args = model_parameters\n    log_format = \'%(message)s\'\n    log_level = logging.DEBUG if args.verbose else logging.INFO\n    log_level = logging.INFO\n    logging.basicConfig(format=log_format, level=log_level)\n    logger = logging.getLogger(""Logger"")\n\n    config = ConfigParser()\n    if args.config_file:\n        config.read(args.config_file)\n    # merge args with config\n    reader = TweetReader(text_field=args.textField, label_field=args.tagField, ngrams=args.ngrams)\n    reader.read(args.train)\n    vocab, bigrams, trigrams = reader.create_vocabulary(reader.sentences, args.vocab_size,\n                                                        min_occurrences=args.minOccurr)\n    #print(""length vocab"")\n    #print(len(vocab))\n    if args.variant == \'word2vec\' and os.path.exists(args.vectors):\n        embeddings = Embeddings(vectors=args.vectors, variant=args.variant)\n        embeddings.merge(vocab)\n        logger.info(""Saving vocabulary in %s"" % args.vocab)\n        embeddings.save_vocabulary(args.vocab)\n    elif os.path.exists(args.vocab):\n        # start with the given vocabulary\n        b_vocab = reader.load_vocabulary(args.vocab)\n        bound = len(b_vocab)-len(bigrams)-len(trigrams)\n        base_vocab=b_vocab[:bound]\n        #print(""length base vocab :"")\n        #print(len(base_vocab))\n        if os.path.exists(args.vectors):\n            # load embeddings\n            embeddings = Embeddings(vectors=args.vectors, vocab=base_vocab, variant=args.variant)\n        else:\n            # create embeddings\n            embeddings = Embeddings(args.embeddings_size, vocab=base_vocab, variant=args.variant)\n            # add the ngrams from the corpus\n            embeddings.merge(vocab)\n            logger.info(""Overriding vocabulary in %s"" % args.vocab)\n            embeddings.save_vocabulary(args.vocab)\n    else:\n        embeddings = Embeddings(args.embeddings_size, vocab=vocab, variant=args.variant)\n        logger.info(""Saving vocabulary in %s"" % args.vocab)\n        embeddings.save_vocabulary(args.vocab)\n\n    # Assume bigrams are prefix of trigrams, or else we should put a terminator\n    # on trie\n    trie = {}\n    for b in chain(bigrams, trigrams):\n        tmp = trie\n        for w in b:\n            tmp = tmp.setdefault(embeddings.dict[w], {})\n\n    converter = Converter()\n    converter.add(embeddings)\n\n    trainer = create_trainer(args, converter)\n\n    report_intervals = max(args.iterations / 200, 1)\n    report_intervals = 10000  # DEBUG\n\n    logger.info(""Starting training"")\n\n    # a generator expression (can be iterated several times)\n    # It caches converted sentences, avoiding repeated conversions\n    converted_sentences = converter.generator(reader.sentences, cache=True)\n    trainer.train(converted_sentences, reader.polarities, trie,\n                  args.iterations, report_intervals)\n\n    logger.info(""Overriding vectors to %s"" % args.vectors)\n    embeddings.save_vectors(args.vectors, args.variant)\n    if args.model:\n        logger.info(""Saving trained model to %s"" % args.model)\n        trainer.save(args.model)\n\n\ndef create_trainer(args, converter):\n    """"""\n    Creates or loads a neural network according to the specified args.\n    """"""\n\n    logger = logging.getLogger(""Logger"")\n\n    if args.load:\n        logger.info(""Loading provided network..."")\n        trainer = SentimentTrainer.load(args.load)\n        # change learning rate\n        trainer.learning_rate = args.learning_rate\n    else:\n        logger.info(\'Creating new network...\')\n        # sum the number of features in all extractors\' tables\n        input_size = converter.size() * (args.window * 2 + 1)\n        nn = Network(input_size, args.hidden, 2)\n        options = {\n            \'learning_rate\': args.learning_rate,\n            \'eps\': args.eps,\n            \'ro\': args.ro,\n            \'verbose\': args.verbose,\n            \'left_context\': args.window,\n            \'right_context\': args.window,\n            \'ngram_size\': args.ngrams,\n            \'alpha\': args.alpha\n        }\n        trainer = SentimentTrainer(nn, converter, options)\n\n    trainer.saver = saver(args.model, args.vectors)\n\n    logger.info(""... with the following parameters:"")\n    logger.info(trainer.nn.description())\n\n    return trainer\n\n\ndef saver(model_file, vectors_file):\n    """"""Function for saving model periodically""""""\n\n    def save(trainer):\n        # save embeddings also separately\n        if vectors_file:\n            trainer.save_vectors(vectors_file)\n        if model_file:\n            trainer.save(model_file)\n\n    return save\n\n\ndef buildWordVector(tokens, size, tweet_w2v, tfidf):\n    vec = np.zeros(size).reshape((1, size))\n    count = 0.\n    for word in tokens:\n        try:\n            vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]\n            count += 1.\n        except KeyError:  # handling the case where the token is not\n            # in the corpus. useful for testing.\n            continue\n    if count != 0:\n        vec /= count\n    return vec\n\n\ndef get_sswe_features(vocab_file, model_file):\n    vocabs = []\n    models = []\n    with open(vocab_file, ""rb"") as vocablist:\n        for vocab in vocablist:\n            vocabs.append(vocab.rstrip())\n\n    with open(model_file, ""rb"") as modellist:\n        for model in modellist:\n            arr_model = model.split()\n            models.append(np.array(map(float, arr_model)))\n    # build our word embedding model vectorizer\n    sswe_dict = dict(zip(vocabs, models))\n    return sswe_dict\n'"
src/feature_extractor/vectorizer/WordEmbeddingVectorizer.py,3,"b'import numpy as np\n\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\ntokenizer = TweetTokenizer()\n\nclass WordEmbeddingVectorizer(object):\n    """"""docstring for WordEmbeddingVectorizer""""""\n    def __init__(self, model_library, size):\n        self.model_library = model_library\n        self.word_weight = None\n        self.dim = size\n\n    def set_model(self, model_library):\n        self.model_library = model_library\n        self.word_weight = None\n        self.dim = len(model_library.itervalues().next())\n\n    """"""Build TFIDF bags from train set text""""""\n    def fit(self, train_set):\n        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n        tfidf.fit(train_set)\n\n        max_idf = max(tfidf.idf_)\n        self.word_weight = defaultdict(\n            lambda: max_idf,\n            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n\n        self.feature_names = tfidf.get_feature_names()\n\n        return self\n\n    def transform(self, train_set):\n        return np.array([\n                np.sum([self.model_library[w]\n                    for w in words if w in self.model_library] or\n                    [np.zeros(self.dim)], axis=0)\n                for words in train_set])\n\n    def fit_transform(self, train_set):\n        vect = self.fit(train_set)\n        return vect.transform(train_set)\n\n    def get_feature_names(self):\n        return self.feature_names\n'"
src/feature_extractor/vectorizer/__init__.py,0,b'from WordEmbeddingVectorizer import WordEmbeddingVectorizer\n'
