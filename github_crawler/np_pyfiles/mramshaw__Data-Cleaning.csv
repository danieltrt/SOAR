file_path,api_count,code
winston_wolfe.py,2,"b'#!/usr/bin/env python\n\n""""""\nA quick and dirty \'cleaner\' for some data files.\n\nThree datasets will be cleaned, with cells reformatted as needed.\n""""""\n\nimport numpy as np\nimport pandas as pd\n\n# First dataset ====================================\n\nDF = pd.read_csv(\'Datasets/BL-Flickr-Images-Book.csv\', skipinitialspace=True)\n\nTO_DROP = [\'Edition Statement\',\n           \'Corporate Author\',\n           \'Corporate Contributors\',\n           \'Former owner\',\n           \'Engraver\',\n           \'Contributors\',\n           \'Issuance type\',\n           \'Shelfmarks\']\nDF.drop(TO_DROP, axis=1, inplace=True)\n\nDF.set_index(\'Identifier\', inplace=True)\n\n# Use a regular expression to extract a cleaned-up Date of Publication\nEXTRACT = DF[\'Date of Publication\'].str.extract(r\'^(\\d{4})\', expand=False)\nDF[\'Date of Publication\'] = pd.to_numeric(EXTRACT)\n\n# Use numpy to clean up Place of Publication\nPUB = DF[\'Place of Publication\']\nLONDON = PUB.str.contains(\'London\')\nOXFORD = PUB.str.contains(\'Oxford\')\nDF[\'Place of Publication\'] = np.where(LONDON, \'London\',\n                                      np.where(OXFORD, \'Oxford\',\n                                               PUB.str.replace(\'-\', \' \')))\n\nDF.to_csv(\'Output/BL-Flickr-Images-Book.csv\', header=\'column_names\')\n\n# Second dataset ===================================\n\nUNIVERSITY_TOWNS = []\nwith open(\'Datasets/university_towns.txt\') as towns:\n    for line in towns:\n        if \'[edit]\' in line:\n            # Remember this `state` until the next is found\n            state = line\n        else:\n            # Otherwise, we have a city; keep `state` as last-seen\n            UNIVERSITY_TOWNS.append((state, line))\n\nTOWNS_DF = pd.DataFrame(UNIVERSITY_TOWNS,\n                        columns=[\'State\', \'RegionName\'])\n\n\ndef get_citystate(item):\n    """"""Help for cleaning up data cells.""""""\n    if \' (\' in item:\n        return item[:item.find(\' (\')]\n    elif \'[\' in item:\n        return item[:item.find(\'[\')]\n    return item\n\n\n# Apply our function to each cell in our dataframe\nTOWNS_DF = TOWNS_DF.applymap(get_citystate)\n\n# Was TXT but probably CSV is a lot more useful\nTOWNS_DF.to_csv(\'Output/university_towns.csv\', header=\'column_names\')\n\n# Third dataset ====================================\n\n# Our real header line is the second one (offset 1)\nOLYMPICS_DF = pd.read_csv(\'Datasets/olympics.csv\', header=1)\n\n# The mapping of old -> new column names\nNEW_NAMES = {\'Unnamed: 0\': \'Country\',\n             \'? Summer\': \'Summer Olympics\',\n             \'01 !\': \'Gold\',\n             \'02 !\': \'Silver\',\n             \'03 !\': \'Bronze\',\n             \'? Winter\': \'Winter Olympics\',\n             \'01 !.1\': \'Gold.1\',\n             \'02 !.1\': \'Silver.1\',\n             \'03 !.1\': \'Bronze.1\',\n             \'? Games\': \'# Games\',\n             \'01 !.2\': \'Gold.2\',\n             \'02 !.2\': \'Silver.2\',\n             \'03 !.2\': \'Bronze.2\'}\n\n# Rename our columns\nOLYMPICS_DF.rename(columns=NEW_NAMES, inplace=True)\n\nOLYMPICS_DF.to_csv(\'Output/olympics.csv\', header=\'column_names\')\n'"
