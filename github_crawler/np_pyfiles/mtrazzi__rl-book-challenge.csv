file_path,api_count,code
chapter1/agents.py,5,"b'from board import TicTacToeBoard\nimport numpy as np\n\n\nclass RandomAgent:\n  def __init__(self, size=3, sym=\'x\'):\n    self.sym = sym\n    self.size = size\n\n  def best_move(self, board):\n    while True:\n      x, y = np.random.randint(self.size), np.random.randint(self.size)\n      if board.can_place(x, y):\n        return x, y\n\n  def train(self, opponent, n_episodes):\n    pass\n\n\nclass RLAgent:\n  """"""Offline training, assuming the RL Agent has its opponent as attribute.""""""\n  def __init__(self, size=3, sym=\'o\', step=0.2, eps=0.1, eps_decay=1):\n    self.step = step\n    self.V = {}\n    self.eps = eps\n    self.sym = sym\n    self.size = size\n    self.eps_decay = eps_decay\n    self.original_eps = eps\n\n  def get_board_id(self, board):\n    return board.__str__()\n\n  def random_move(self, board):\n    while True:\n      x, y = np.random.randint(board.size), np.random.randint(board.size)\n      if board.can_place(x, y):\n        return x, y\n\n  def best_move(self, board):\n    values = [self.get_move_value(board, i // self.size, i % self.size)\n              if board.can_place(i // self.size, i % self.size) else -np.inf\n              for i in range(self.size ** 2)]\n    idx = np.random.choice(np.flatnonzero(np.isclose(values, max(values))))\n    return idx // self.size, idx % self.size\n\n  def get_move_value(self, board, x, y):\n    board.do_move(x, y)\n    value = self.get_value(board)\n    board.undo_move(x, y)\n    return value\n\n  def get_value(self, board):\n    state_id = board.__str__()\n    if state_id in self.V:\n      return self.V[state_id]\n    self.V[state_id] = board.result(max_player=self.sym)\n    return self.V[state_id]\n\n  def update_value(self, s_t, s_tp1):\n    self.V[s_t] = self.V[s_t] + self.step * (self.V[s_tp1] - self.V[s_t])\n\n  def eps_greedy(self, board):\n    if np.random.random() < self.eps:\n      return ""eps"", self.random_move(board)\n    else:\n      return ""greedy"", self.best_move(board)\n\n  def train_one_step(self, board, opponent=RandomAgent()):\n    s_t = board.__str__()\n    self.get_value(board)  # to initialize V(s_t) if it doesn\'t exist yet\n    board.do_move(*opponent.best_move(board))\n    if not board.is_end_state():\n      case, move = self.eps_greedy(board)\n      board.do_move(*move)\n      s_tp1 = board.__str__()  # idem with V(s_{t+1})\n      self.get_value(board)\n      if case == ""greedy"":  # only update from non-exploratory moves\n        self.update_value(s_t, s_tp1)\n\n  def train(self, opponent=RandomAgent(), n_episodes=1000):\n    board = TicTacToeBoard(self.size)\n    for episode_nb in range(n_episodes):\n      board.reset()\n      while not board.is_end_state():\n        self.train_one_step(board, opponent)\n      if episode_nb % 100 == 0:\n        self.eps *= self.eps_decay\n        print(self.eps)\n\n  def get_possible_move_values(self, board):\n    d = {}\n    for x in range(board.size):\n      for y in range(board.size):\n        if board.can_place(x, y):\n          d[f""({x}, {y})""] = self.get_move_value(board, x, y)\n    return d\n'"
chapter1/board.py,1,"b'import numpy as np\n\nPLAYERS = [\'x\', \'o\']\n\n\nclass TicTacToeBoard:\n  def __init__(self, size=3):\n    self.size = size\n    self.board = self.new_board()\n    self.players = PLAYERS\n    self.curr_play_idx = 0\n\n  def is_valid_move(self, x, y):\n    return 0 <= x < self.size and 0 <= y < self.size and self.board[x][y] == \'.\'\n\n  def do_move(self, x, y):\n    if self.is_valid_move(x, y):\n      self.board[x][y] = self.players[self.curr_play_idx]\n      self.curr_play_idx = (self.curr_play_idx + 1) % 2\n    else:\n      exit(""Tried to do an invalid move"")\n\n  def undo_move(self, x, y):\n    self.board[x][y] = \'.\'\n    self.curr_play_idx = (self.curr_play_idx + 1) % 2\n\n  def transpose(self):\n    return np.array(self.board).T.tolist()\n\n  def diag(self):\n    return [[self.board[x_0 + i * dx][y_0 + i * dy] for i in (range(self.size))]\n            for (x_0, y_0, dx, dy) in [(0, 0, 1, 1), (self.size - 1, 0, -1, 1)]]\n\n  def has_won(self, sym):\n    for row in self.board + self.transpose() + self.diag():\n      if len(set(row)) == 1 and row[0] == sym:\n        return True\n    return False\n\n  def is_over(self):\n    for row in self.board:\n      if \'.\' in row:\n        return False\n    return True\n\n  def is_end_state(self):\n    return self.is_over() or self.has_won(\'x\') or self.has_won(\'o\')\n\n  def can_place(self, x, y):\n    return self.board[x][y] == \'.\'\n\n  def new_board(self):\n    return [[\'.\' for _ in range(self.size)] for _ in range(self.size)]\n\n  def reset(self):\n    self.board = self.new_board()\n\n  def result(self, max_player):\n    from utils import opposite\n    if self.has_won(max_player):\n      return 1\n    elif self.has_won(opposite(max_player)):\n      return 0\n    else:\n      return 0.5\n\n  def __str__(self):\n    return \'\\n\'.join([\' \'.join(row) for row in self.board])\n'"
chapter1/main.py,0,"b'from agents import RandomAgent, RLAgent\nfrom utils import benchmark, opposite_agent, play_against_agent\n\n\ndef self_play(n_iterations=10, ben_steps=1000, training_steps=int(1e4),\n              n_eval_episodes=100, **kwargs):\n  """"""\n  Returns an agent that learns from playing against himself from random to\n  optimal play.\n  """"""\n  agents = [RLAgent(**kwargs), RandomAgent()]\n  for _ in range(n_iterations):\n    benchmark(agents[0], agents[1], ben_steps, training_steps, n_eval_episodes)\n    # adding the trained agent as the new opponent to exploit\n    agents[1] = opposite_agent(agents[0])\n    agents[1].eps = agents[0].original_eps\n  return agents[0]\n\n\ndef main():\n  rl_agent_hyperparams = {""step"": 0.1, ""eps"": 0.5, ""eps_decay"": 0.99}\n  play_against_agent(self_play(n_iterations=4, training_steps=int(1e4),\n                     **rl_agent_hyperparams))\n\n\nif __name__ == \'__main__\':\n  main()\n'"
chapter1/utils.py,1,"b'import os\n\nfrom board import TicTacToeBoard\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef opposite(sym):\n  return \'x\' if sym == \'o\' else \'o\'\n\n\ndef play_against_agent(agent):\n  board = TicTacToeBoard(size=agent.size)\n  while True:\n    board.reset()\n    while not board.is_end_state():\n      os.system(\'cls\' if os.name == \'nt\' else \'clear\')\n      print(board)\n      x, y = [int(elt) for elt in input().split()]\n      board.do_move(x, y)\n      print(board)\n      print(agent.get_possible_move_values(board))\n      input(""..."")\n      board.do_move(*agent.best_move(board))\n\n\ndef test_agent(agent, opponent, n_episodes=10000):\n  from agents import RLAgent\n  board = TicTacToeBoard(agent.size)\n  n_wins = 0\n  for _ in range(n_episodes):\n    board.reset()\n    counter = 0\n    while not board.is_end_state():\n      to_play = opponent if counter % 2 == 0 else agent\n      if isinstance(to_play, RLAgent):\n        board.do_move(*(to_play.eps_greedy(board)[1]))\n      else:\n        board.do_move(*to_play.best_move(board))\n      counter += 1\n    n_wins += board.has_won(agent.sym)\n  print(f""{n_wins}/{n_episodes}"")\n  return n_wins / n_episodes\n\n\ndef weighted_averages(arr, alpha=0.9):\n  new_arr = np.zeros_like(arr)\n  avg = None\n  for i in range(len(arr)):\n    avg = alpha * avg + (1 - alpha) * arr[i] if avg is not None else arr[i]\n    new_arr[i] = avg\n  return new_arr\n\n\ndef benchmark(agent, opponent, step=10,\n              training_steps=100, n_eval_episodes=1000, alpha=0.9):\n  results = []\n  for _ in range(training_steps // step):\n    results.append(test_agent(agent, opponent, n_eval_episodes))\n    agent.train(opponent, step)\n  plt.plot(weighted_averages(results, alpha))\n  plt.show()\n\n\ndef opposite_agent(agent):\n  from agents import RLAgent\n  """"""Returns an agent who plays with the opposite symbol.""""""\n  new_agent = RLAgent(agent.size, opposite(agent.sym), agent.step, agent.eps,\n                      agent.eps_decay)\n  if isinstance(agent, RLAgent):\n    for state, val in agent.V.items():\n      new_agent.V[state] = 1 - val\n  return new_agent\n'"
chapter10/figures.py,2,"b'import argparse\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tiles_sutton import IHT, tiles\nfrom mountain_car import MountainCar, X_MIN, X_MAX, V_MIN, V_MAX\nfrom semi_grad_sarsa import EpisodicSemiGradientTD0\n\nTILE_SIZE = 4096\nN_TILES = 8\n\nFIG_10_2_ALP_L = [alpha / 8 for alpha in [0.1, 0.2, 0.5]]\nFIG_10_2_N_EP = 500\nFIG_10_2_G = 1\n\n\ndef get_idxs(iht, x, xdot, a):\n  return tiles(iht, N_TILES, [N_TILES * x / (X_MAX - X_MIN),\n               N_TILES * xdot / (V_MAX - V_MIN)], [a])\n\n\ndef fig_10_2():\n    env = MountainCar()\n    w_dim = TILE_SIZE * N_TILES\n    iht = IHT(TILE_SIZE)\n    def idxs(s, a): return get_idxs(iht, s[0], s[1], a)\n    def qhat(s, a, w): return np.sum(w[idxs(s, a)])\n\n    def nab_qhat(s, a, w):\n      res = np.zeros(len(w))\n      for idx in idxs(s, a):\n        res[idx] = True\n      return res\n\n    for alp in FIG_10_2_ALP_L:\n      print(alp)\n      alg = EpisodicSemiGradientTD0(env, alp, w_dim, eps=0)\n      n_steps = alg.pol_eva(qhat, nab_qhat, FIG_10_2_N_EP, FIG_10_2_G)\n      plt.plot(n_steps, label=f\'alpha={alp}\')\n    plt.legend()\n    plt.show()\n\n\nPLOT_FUNCTION = {\n  \'10.2\': fig_10_2,\n}\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n\n  parser.add_argument(\'figure\', type=str, default=None,\n                      help=\'Figure to reproduce.\',\n                      choices=list(PLOT_FUNCTION.keys()) + [\'all\'])\n  args = parser.parse_args()\n  if args.figure == \'all\':\n    for key, f in PLOT_FUNCTION.items():\n      print(f""[{key}]"")\n      f()\n  else:\n    print(f""[{args.figure}]"")\n    PLOT_FUNCTION[args.figure]()\n\n\nif __name__ == \'__main__\':\n  main()\n'"
chapter10/mountain_car.py,2,"b'import numpy as np\n\nX_0_MIN, X_0_MAX = [-0.6, -0.4]\nX_MIN, X_MAX = [-1.2, 0.5]\nV_0_MIN = 0\nV_MAX = 0.07\nV_MIN = -V_MAX\nV_NULL = 0\nR_STEP = -1\nTHR_FOR, THR_REV, ZER_THR = [1, -1, 0]\n\n\nclass MountainCar:\n  def __init__(self):\n    self.reset()\n    self.get_moves()\n\n  def bound(self, y_min, y, y_max):\n    return y_min if y < y_min else min(y, y_max)\n\n  def get_moves(self):\n    self.moves = [THR_FOR, THR_REV, ZER_THR]\n\n  def step(self, a):\n    self.state[1] = self.bound(V_MIN,\n                               (self.state[1] +\n                                0.001 * a - 0.0025 * np.cos(3 * self.state[1])),\n                               V_MAX)\n    self.state[0] = self.bound(X_MIN, self.state[0] + self.state[1], X_MAX)\n    if self.state[0] == X_MIN:\n      self.state[1] = V_NULL\n    return self.state, R_STEP, self.state[0] == X_MAX, {}\n\n  def reset(self):\n    x_0 = (X_0_MAX - X_0_MIN) * np.random.random() + X_0_MIN\n    self.state = [x_0, V_0_MIN]\n    return self.state\n\n  def __str__(self):\n    return f""x = {self.state[0]}, vx = {self.state[1]}""\n'"
chapter10/semi_grad_sarsa.py,6,"b'import numpy as np\nfrom utils import sample\n\n\nclass GradientAlg:\n  def __init__(self, env, alpha, w_dim, eps):\n    self.env = env\n    self.a = alpha\n    self.d = w_dim\n    self.eps = eps\n    self.qhat = None\n    self.reset()\n\n  def gre(self, s):\n    q_arr = np.array([self.qhat(s, a, self.w) for a in self.env.moves])\n    try:\n      best_move = np.random.choice(np.flatnonzero(q_arr == q_arr.max()))\n    except:\n      import ipdb; ipdb.set_trace()\n    return self.env.moves[best_move]\n\n  def eps_gre(self, s):\n    if np.random.random() < self.eps:\n      return sample(self.env.moves_d[s])\n    return self.gre(s)\n\n  def seed(self, seed):\n    self.env.seed(seed)\n    np.random.seed(seed)\n\n  def reset(self):\n    self.w = np.zeros(self.d)\n\n\nclass EpisodicSemiGradientTD0(GradientAlg):\n  def __init__(self, env, alpha, w_dim, eps):\n    super().__init__(env, alpha, w_dim, eps)\n\n  def pol_eva(self, qhat, nab_qhat, n_ep, gamma):\n    steps_per_ep = []\n    self.qhat, w = qhat, self.w\n    for ep in range(n_ep):\n      if ep > 0 and ep % 10 == 0:\n        print(f""ep #{ep}"")\n      s = self.env.reset()\n      a = self.eps_gre(s)\n      n_steps = 0\n      while True:\n        print(self.env)\n        print(np.mean(self.w))\n        s_p, r, d, _ = self.env.step(a)\n        n_steps += 1\n        if d:\n          self.w += self.a * (r - qhat(s, a, w)) * nab_qhat(s, a, w)\n          break\n        a_p = self.eps_gre(s_p)\n        self.w += self.a * (r + gamma * qhat(s_p, a_p, w) - (qhat(s, a, w)) *\n                            nab_qhat(s, a, w))\n        s, a = s_p, a_p\n      steps_per_ep.append(n_steps)\n    return steps_per_ep\n'"
chapter10/tiles_sutton.py,0,"b'""""""\nAllowing myself to use http://incompleteideas.net/tiles/tiles3.py-remove\nbecause the link is in the book page 246 and i\'ve already implemented\ntile coding in chapter 9.\n""""""\n\nfrom math import floor\nfrom itertools import zip_longest\n\nbasehash = hash\n\n\nclass IHT:\n  ""Structure to handle collisions""\n  def __init__(self, sizeval):\n    self.size = sizeval\n    self.overfullCount = 0\n    self.dictionary = {}\n\n  def __str__(self):\n    ""Prepares a string for printing whenever this object is printed""\n    return ""Collision table:"" + \\\n           "" size:"" + str(self.size) + \\\n           "" overfullCount:"" + str(self.overfullCount) + \\\n           "" dictionary:"" + str(len(self.dictionary)) + "" items""\n\n  def count(self):\n    return len(self.dictionary)\n\n  def fullp(self):\n    return len(self.dictionary) >= self.size\n\n  def getindex(self, obj, readonly=False):\n    d = self.dictionary\n    if obj in d:\n      return d[obj]\n    elif readonly:\n      return None\n    size = self.size\n    count = self.count()\n    if count >= size:\n      if self.overfullCount == 0:\n        print(\'IHT full, starting to allow collisions\')\n      self.overfullCount += 1\n      return basehash(obj) % self.size\n    else:\n      d[obj] = count\n      return count\n\n\ndef hashcoords(coordinates, m, readonly=False):\n  if type(m) == IHT:\n    return m.getindex(tuple(coordinates), readonly)\n  if type(m) == int:\n    return basehash(tuple(coordinates)) % m\n  if m is None:\n    return coordinates\n\n\ndef tiles(ihtORsize, numtilings, floats, ints=[], readonly=False):\n  """"""returns num-tilings tile indices corresponding to the floats and ints""""""\n  qfloats = [floor(f*numtilings) for f in floats]\n  Tiles = []\n  for tiling in range(numtilings):\n    tilingX2 = tiling*2\n    coords = [tiling]\n    b = tiling\n    for q in qfloats:\n      coords.append((q + b) // numtilings)\n      b += tilingX2\n    coords.extend(ints)\n    Tiles.append(hashcoords(coords, ihtORsize, readonly))\n  return Tiles\n\n\ndef tileswrap(ihtORsize, numtilings, floats, wrapwidths, ints=[],\n              readonly=False):\n  """"""\n  returns num-tilings tile indices corresponding to the floats and ints,\n  wrapping some floats\n  """"""\n  qfloats = [floor(f*numtilings) for f in floats]\n  Tiles = []\n  for tiling in range(numtilings):\n    tilingX2 = tiling*2\n    coords = [tiling]\n    b = tiling\n    for q, width in zip_longest(qfloats, wrapwidths):\n      c = (q + b % numtilings) // numtilings\n      coords.append(c % width if width else c)\n      b += tilingX2\n    coords.extend(ints)\n    Tiles.append(hashcoords(coords, ihtORsize, readonly))\n  return Tiles\n'"
chapter10/utils.py,1,b'import numpy as np\n\n\ndef sample(l):\n  return l[np.random.randint(len(l))]\n'
chapter2/bandit.py,3,"b'import numpy as np\n\n\nclass Bandit:\n  def __init__(self, k=10, mean=0):\n    self.k = k\n    self.q = np.random.randn(k) + mean\n    self.old_q = [q_val for q_val in self.q]  # copy\n\n  def max_action(self):\n    return np.argmax(self.q)\n\n  def reward(self, action):\n    return np.random.normal(self.q[action])\n\n  def reset(self):\n    self.q = self.old_q\n'"
chapter2/figures.py,13,"b'import argparse\n\nfrom bandit import Bandit\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmpl.use(\'TkAgg\')\n\nK = 10\n\n\ndef average_reward(Q, N):\n  return np.dot(Q, N) / sum(N)\n\n\ndef constant_alpha(alpha=0.1):\n  return lambda x: alpha\n\n\ndef sample_average(N):\n  return (1/N)\n\n\ndef action_selection(Q, eps=0.1, method=\'epsilon-greedy\', c=2, t=1, N=None):\n  if method == \'epsilon-greedy\':\n    if np.random.random() < eps:\n      return np.random.choice(Q.shape[0])\n    else:\n      return np.random.choice(np.flatnonzero(Q == Q.max()))\n  elif method == \'ucb\':\n    ucb = np.zeros_like(Q)\n    for a in range(Q.shape[0]):\n      ucb[a] = Q[a] + c * np.sqrt(np.log(t) / N[a]) if N[a] > 0 else np.inf\n    return np.random.choice(np.flatnonzero(ucb == ucb.max()))\n\n\ndef a_simple_bandit_algorithm(bandit, n_iterations=1000, eps=0.1,\n                              weight_fn=sample_average, random_walk=False,\n                              Q_1=0, method=\'epsilon-greedy\', c=2,\n                              start_timestep=np.inf):\n  """"""Returns the estimated Q-Values of the bandit problem.""""""\n  k = bandit.k\n  Q, N, R_log = np.zeros(k) + Q_1, np.zeros(k), np.zeros(k)\n  avg_rew, per_list = [], []\n  avg_r, per_max_act, avg_r_end = 0, 0, 0\n  for t in range(1, n_iterations + 1):\n    A = action_selection(Q, eps, method=method, c=c, t=t, N=N)\n    R = bandit.reward(A)\n    N[A] += 1\n    Q[A] += (R - Q[A]) * weight_fn(N[A])\n    R_log[A] += (R-R_log[A]) * (1 / N[A])\n    avg_r += (R - avg_r) / t\n    if t >= start_timestep:\n      avg_r_end += (R - avg_r_end) / (t - start_timestep + 1)\n    per_max_act += ((A == bandit.max_action()) - per_max_act) / t\n    per_list.append(per_max_act)\n    avg_rew.append(avg_r)\n    if random_walk:\n      bandit.q += 0.01 * np.random.randn(k)\n  return Q, np.array(per_list), np.array(avg_rew), [avg_r_end]\n\n\ndef plot_average(arr, eps_list, n_bandits, y_lim, show=True, extra_label=\'\',\n                 title=None, percentage=False):\n  for i, eps in enumerate(eps_list):\n    plt.plot((arr[i][1:] / n_bandits) * (100 if percentage else 1),\n             label=f""epsilon={eps} {extra_label}"")\n  axes = plt.gca()\n  axes.set_ylim(y_lim)\n  plt.xlabel(""Steps"")\n  plt.ylabel(""Optimal Action %"" if percentage else ""Average Reward"")\n  plt.legend()\n  if title is not None:\n    plt.title(title)\n  if show:\n    plt.show()\n\n\ndef plot_figures(k, n_bandits, n_steps, eps_list, weight_fn=sample_average,\n                 random_walk=False, y_bounds=[0, 1.5], Q_1=0, show=True,\n                 method=\'epsilon-greedy\', extra_label=\'\', title=None,\n                 percentage=False):\n  avg_rew_per_eps = [np.zeros(n_steps) for _ in range(len(eps_list))]\n  avg_rew_in_perc = [np.zeros(n_steps) for _ in range(len(eps_list))]\n  for i in range(n_bandits):\n    print(i)\n    bandit_pb = Bandit(k)\n    for i, eps in enumerate(eps_list):\n      _, per, avg_rew, _ = a_simple_bandit_algorithm(bandit_pb,\n                                                     n_iterations=n_steps,\n                                                     eps=eps,\n                                                     weight_fn=weight_fn,\n                                                     random_walk=random_walk,\n                                                     Q_1=Q_1,\n                                                     method=method)\n      avg_rew_per_eps[i] += avg_rew\n      avg_rew_in_perc[i] += per\n\n  to_plot = avg_rew_in_perc if percentage else avg_rew_per_eps\n  bounds = [0, 100] if percentage else y_bounds\n  plot_average(to_plot, eps_list, n_bandits, bounds, show, extra_label, title,\n               percentage)\n\n\ndef fig_2_2(n_bandits=2000, n_steps=1000, eps_list=[0, 0.1, 0.01]):\n  # reproducing figure 2.2\n  plot_figures(K, n_bandits, n_steps, eps_list, title=\'Figure 2.2\')\n  plot_figures(K, n_bandits, n_steps, eps_list, title=\'Figure 2.2\',\n               percentage=True)\n\n\ndef ex_2_5(n_bandits=100, n_steps=10000, eps_list=[0.1]):\n  # # exercise 2.5: difficulties of sample average on non-stationary problems\n  plot_figures(K, n_bandits, n_steps, eps_list, sample_average, True, [0, 3],\n               show=False, extra_label=\'sample average\',\n               title=""Exercise 2.5: sample-average vs.constant step size"")\n  plot_figures(K, n_bandits, n_steps, eps_list, constant_alpha(alpha=0.1), True,\n               [0, 3], extra_label=\'constant step size (alpha = 0.1)\')\n\n\ndef fig_2_3(n_bandits=2000, n_steps=1000):\n  # figure 2.3: optimistic greedy vs. realistic eps-greedy\n  for Q_1, eps, show in [(5, 0, False), (0, 0.1, True)]:\n    plot_figures(K, n_bandits, n_steps, [eps], constant_alpha(alpha=0.1),\n                 Q_1=Q_1, show=show, title=\'Figure 2.3\',\n                 extra_label=f\'Q_1={Q_1}\', percentage=True)\n\n\ndef fig_2_4(n_bandits=2000, n_steps=1000, eps_list=[0.1]):\n  # reproducing figure 2.4\n  plot_figures(K, n_bandits, n_steps, eps_list, sample_average, False, [0, 1.5],\n               show=False)\n  plot_figures(K, n_bandits, n_steps, eps_list, sample_average, False, [0, 1.5],\n               method=\'ucb\', extra_label=\'ucb\')\n\n\nPLOT_FUNCTION = {\n  \'2.2\': fig_2_2,\n  \'ex2.5\': ex_2_5,\n  \'2.3\': fig_2_3,\n  \'2.4\': fig_2_4,\n}\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n\n  parser.add_argument(\'figure\', type=str, default=None,\n                      help=\'Figure to reproduce.\',\n                      choices=[\'2.2\', \'ex2.5\', \'2.3\', \'2.4\'])\n  args = parser.parse_args()\n\n  PLOT_FUNCTION[args.figure]()\n\n\nif __name__ == ""__main__"":\n  main()\n'"
chapter2/gradient_bandit.py,8,"b'from bandit import Bandit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef softmax(H):\n  exp_val = np.exp(H)\n  p = exp_val / np.sum(exp_val)\n  return p\n\n\ndef gradient_update(H, pi, A, R, baseline=0, alpha=0.1):\n  for a in range(len(H)):\n    if a == A:\n      H[a] += alpha * (R - baseline) * (1 - pi[a])\n    else:\n      H[a] -= alpha * (R - baseline) * pi[a]\n\n\ndef gradient_bandit(bandit, n_steps=1000, alpha=0.1, baseline=False,\n                    percentage=True, start_timestep=np.inf, random_walk=False):\n  k, avg_r_end = bandit.k, 0\n  H, per_max_act_log, avg_rew, R_mean, per_max_act = np.zeros(k), [], [], 0, 0\n  max_action = bandit.max_action()\n  for t in range(1, n_steps + 1):\n    if random_walk:\n      bandit.q += 0.01 * np.random.randn(k)\n    pi = softmax(H)\n    A = np.random.choice(len(H), p=pi)\n    R = bandit.reward(A)\n    per_max_act += ((A == max_action) - per_max_act) / t\n    per_max_act_log.append(per_max_act)\n    overline_R_t = R_mean if baseline else 0\n    gradient_update(H, pi, A, R, overline_R_t, alpha)\n    R_mean += (R-R_mean) / t  # baseline \\overline{R_t} doesn\'t include R_t!\n    avg_rew.append(R_mean)\n    if t >= start_timestep:\n      avg_r_end += (R - avg_r_end) / (t - start_timestep + 1)\n  return (np.array(per_max_act_log) if percentage else np.array(avg_rew),\n          [avg_r_end])\n\n\ndef fig_2_5(n_bandits=2000, n_steps=1000, k=10, alpha_list=[0.1, 0.4]):\n  d = {}\n  for baseline in [False, True]:\n    for alpha in alpha_list:\n      d[(baseline, alpha)] = np.zeros(n_steps)\n  for n in range(n_bandits):\n    print(n)\n    bandit = Bandit(k, mean=4)\n    for baseline in [False, True]:\n      for alpha in alpha_list:\n        result_arr, _ = gradient_bandit(bandit, n_steps=n_steps,\n                                                alpha=alpha, baseline=baseline)\n        d[(baseline, alpha)] += result_arr\n\n  def label(baseline, alpha):\n    return (""with"" if baseline else ""without"") + f"" baseline, alpha={alpha}""\n  for key, avg_rew in d.items():\n    plt.plot((avg_rew / n_bandits) * 100, label=label(key[0], key[1]))\n  axes = plt.gca()\n  axes.set_ylim([0, 100])\n  plt.xlabel(""Steps"")\n  plt.ylabel(""Optimal Action %"")\n  plt.title(""Figure 2.5"")\n  plt.legend()\n  plt.show()\n\n\ndef main():\n  fig_2_5()\n\n\nif __name__ == ""__main__"":\n  main()\n'"
chapter2/summary.py,2,"b'import argparse\n\nfrom bandit import Bandit\nfrom figures import a_simple_bandit_algorithm, constant_alpha, sample_average\nfrom gradient_bandit import gradient_bandit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nHYPERPARMS = {\n  \'epsilon-greedy\': [1/128, 1/64, 1/32, 1/16, 1/8, 1/4],\n  \'gradient bandit\': [1/32, 1/16, 1/8, 1/4, 1/2, 1, 2],\n  \'ucb\': [1/16, 1/4, 1/2, 1, 2, 4],\n  \'optimistic greedy\': [1/4, 1/2, 1, 2, 4],\n}\n\nCOLORS = {\n  \'epsilon-greedy\': \'r\',\n  \'gradient bandit\': \'#1AD02B\',\n  \'ucb\': \'b\',\n  \'optimistic greedy\': \'k\',\n}\n\n\ndef apply_method(bandit, n_iterations, method_name, hyperparam, nonstat=False,\n                 start_timestep=np.inf):\n  if method_name in [\'epsilon-greedy\', \'ucb\', \'optimistic greedy\']:\n    Q_1 = hyperparam if method_name == \'optimistic greedy\' else 0\n    eps = hyperparam if method_name == \'epsilon-greedy\' else 0\n    c = hyperparam if method_name == \'ucb\' else None\n    weight_fn = (constant_alpha(0.1) if (method_name == \'optimistic greedy\' or\n                 (method_name == \'epsilon-greedy\' and nonstat))\n                 else sample_average)\n    method = \'ucb\' if method_name == \'ucb\' else \'epsilon-greedy\'\n    _, _, avg_rew, avg_rew_end = a_simple_bandit_algorithm(\n                                              bandit,\n                                              n_iterations=n_iterations,\n                                              eps=eps, weight_fn=weight_fn,\n                                              Q_1=Q_1, method=method, c=c,\n                                              random_walk=nonstat,\n                                              start_timestep=start_timestep)\n  else:\n    avg_rew, avg_rew_end = gradient_bandit(bandit, n_steps=n_iterations,\n                                           alpha=hyperparam,\n                                           baseline=True, percentage=False,\n                                           start_timestep=start_timestep,\n                                           random_walk=nonstat)\n  return avg_rew_end if nonstat else avg_rew\n\n\ndef plot_current(n_steps, results, iteration_nb=0,\n                 title=\'Figure 2.6\', fn=\'fig2_6\', y_label=\'\'):\n  _, ax = plt.subplots()\n  ax.set_xscale(\'log\', basex=2)\n  x = [2 ** i for i in range(-7, 3)]\n  x_name = ([f""1/{2**i}"" for i in range(7, 0, -1)] +\n            [str(2 ** i) for i in range(3)])\n  plt.xticks(x, x_name)\n  plt.ylabel(y_label)\n  plt.title(title)\n  for method, hyperparams in HYPERPARMS.items():\n    to_plot = [results[(method, hyper)] / (iteration_nb + 1) for hyper in\n               hyperparams]\n    plt.plot(hyperparams, to_plot, color=COLORS[method], label=method)\n  plt.legend()\n  plt.savefig(f""figs/{fn}_{iteration_nb}.png"")\n  plt.close()\n\n\ndef param_study(n_bandits=2000, n_steps=1000, title=\'Figure 2.6\',\n                fn=\'fig2_6\', nonstat=False, print_freq=10,\n                start_timestep=np.inf):\n  results = {(method, hyper): 0 for (method, hyperparams) in HYPERPARMS.items()\n             for hyper in hyperparams}\n  y_label = (f""Average Reward over last {n_steps-start_timestep} steps"" if\n             nonstat else f""Average Reward over first {n_steps} steps"")\n  for t in range(1, n_bandits + 1):\n    print(f""{t}/{n_bandits}"")\n    bandit = Bandit()\n    for method, hyperparams in HYPERPARMS.items():\n      for hyper in hyperparams:\n        results[(method, hyper)] += apply_method(bandit, n_steps, method, hyper,\n                                                 nonstat, start_timestep)[-1]\n        bandit.reset()  # need to reset q values after random walk\n    if (t % print_freq == 0):\n      plot_current(n_steps, results, t, title, fn, y_label)\n\n\ndef fig_2_6():\n  param_study()\n\n\ndef ex_2_11():\n  param_study(n_bandits=10, n_steps=int(2e5), title=\'Exercise 2.11\',\n              fn=\'ex2_11\', nonstat=True, start_timestep=int(1e5), print_freq=1)\n\n\nPLOT_FUNCTION = {\n  \'2.6\': fig_2_6,\n  \'ex2.11\': ex_2_11,\n}\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n\n  parser.add_argument(\'figure\', type=str, default=None,\n                      help=\'Figure to reproduce.\', choices=[\'2.6\', \'ex2.11\'])\n  args = parser.parse_args()\n\n  PLOT_FUNCTION[args.figure]()\n\n\nif __name__ == ""__main__"":\n  main()\n'"
chapter2/weights.py,0,"b""# for exercise 2.7\n\n\ndef weights(i, n, beta=0.5):\n  return (((1-beta) ** 2) * (beta ** (n-i)) * (1 / (1 - beta ** i)) *\n          (1 / (1 - beta ** (n-i+1))))\n\n\ndef sum_weights(n, beta=0.8):\n  return sum([weights(i, n, beta) for i in range(1, n+1)])\n\n\ndef main():\n  print(sum_weights(1000))\n\n\nif __name__ == '__main__':\n  main()\n"""
chapter4/car_rental.py,1,"b'from scipy.stats import poisson\nfrom mdp import MDP\nimport numpy as np\n\nREQUEST_LAMBDA = [3, 4]\nRETURNS_LAMBDA = [3, 2]\nCAR_MOVE_COST = 2\nRENT_BEN = 10\nNB_LOC = 2\nABSORBING_STATE = (-1, -1)\nPARKING_COST = 4\n\n\nclass CarRentalEnv(MDP):\n  def __init__(self, size, ex_4_7=False):\n    self.max_car_cap = size\n    self.max_car_moves = self.max_car_cap // 5 + 1\n    self.init_probs()\n    self.ex_4_7 = ex_4_7  # is the pb modified for exercise 4.7 or not\n    # possible nb of cars can go from 0 to max car capacity\n    self.size = self.max_car_cap + 1\n    self.get_moves()\n    self.get_states()\n    self.get_r()\n    super().__init__()\n\n  def get_moves(self):\n    self.moves = list(range(-self.max_car_moves, self.max_car_moves + 1))\n\n  def get_states(self):\n    self.states = [(x, y) for x in range(self.max_car_cap + 1)\n            for y in range(self.max_car_cap + 1)] + [ABSORBING_STATE]\n\n  def get_r(self):\n    if self.ex_4_7:\n      self.r = np.unique([self.compute_reward(n1, n2, m, car_sold)\n                        for n1 in range(self.max_car_cap + 1)\n                        for n2 in range(self.max_car_cap + 1)\n                        for m in range(-self.max_car_moves,\n                                       self.max_car_moves + 1)\n                        for car_sold in range(self.max_car_cap * NB_LOC + 1)])\n    self.r = [-CAR_MOVE_COST * car_moves + RENT_BEN * car_sold\n            for car_moves in range(self.max_car_moves + 1)\n            for car_sold in range(self.max_car_cap * NB_LOC + 1)]\n\n  def init_probs(self):\n    """"""Computing some probabilities in advance to make it go faster.""""""\n    sell_range = range(self.max_car_cap * NB_LOC + 1)\n    self.req_pmfs = {i: [poisson.pmf(j, REQUEST_LAMBDA[i])\n                     for j in sell_range] for i in range(NB_LOC)}\n    self.req_cdf = {i: [poisson.cdf(j, REQUEST_LAMBDA[i])\n                    for j in sell_range] for i in range(NB_LOC)}\n    self.ret_pmfs = {i: [poisson.pmf(j, RETURNS_LAMBDA[i])\n                     for j in sell_range] for i in range(NB_LOC)}\n    self.ret_sf_pmfs = {i: [poisson.sf(j, RETURNS_LAMBDA[i]) +\n                        self.ret_pmfs[i][j] for j in sell_range]\n                        for i in range(NB_LOC)}\n    self.req_pmfs_prod = {(k, n_sells - k): (self.req_pmfs[0][k] *\n                                             self.req_pmfs[1][n_sells - k])\n                          for n_sells in sell_range\n                          for k in range(n_sells + 1)}\n\n  def move_cost(self, m):\n    """"""Check the cost of doing m, depending on if we\'re in ex4.7.""""""\n    if not self.ex_4_7:\n      return abs(m) * CAR_MOVE_COST\n    return CAR_MOVE_COST * abs(m - 1 if m > 0 else m)\n\n  def is_possible_reward(self, n1, n2, m, r, move_cost, max_ben):\n    """"""Check if reward is possible for given state and problem.""""""\n    if not self.ex_4_7:\n      return r in range(-move_cost, -move_cost + max_ben + 1, RENT_BEN)\n    for car_sold in range(self.max_car_cap * NB_LOC + 1):\n      if self.compute_reward(n1, n2, m, car_sold) == r:\n        return True\n    return False\n\n  def park_cost(self, n1, n2, m):\n    if not self.ex_4_7:\n      return 0\n    return (PARKING_COST * ((n1 + m) >= self.max_car_cap or\n                            (n2 - m) >= self.max_car_cap))\n\n  def nb_car_sold(self, n1, n2, m, r):\n    """"""Compute the number of car sold, according to the given reward.""""""\n    return (r + self.park_cost(n1, n2, m) + self.move_cost(m)) // RENT_BEN\n\n  def compute_reward(self, n1, n2, m, car_sold):\n    """"""\n    Compute the reward associated with an action according to the description\n    of exercise 4.7.\n    """"""\n    return RENT_BEN * car_sold - (self.park_cost(n1, n2, m) + self.move_cost(m))\n\n  def _p(self, s_p, r, s, a):\n    """"""Transition function defined in private because p dictionary in mdp.py.""""""\n    (n1, n2), (n1_p, n2_p), m = s, s_p, a\n    move_cost = self.move_cost(m)\n    max_ben = (n1 + n2) * RENT_BEN\n    if self.is_terminal(s):\n      return (s_p == s) and (r == 0)\n    elif self.is_terminal(s_p) and m <= n1 and -m <= n2:\n      return (1 - self.req_cdf[0][n1 - m] * self.req_cdf[1][n2 + m]) * (r == 0)\n    elif not ((0 <= n1 - m <= self.max_car_cap)\n              and (0 <= n2 + m <= self.max_car_cap)\n              and self.is_possible_reward(n1, n2, m, r, move_cost, max_ben)):\n      return 0\n\n    nb_sells = self.nb_car_sold(n1, n2, m, r)\n\n    def p_ret(n_p, n, req, moved_cars, loc):\n      idx = n_p - (n - moved_cars) + req\n      if req > n:\n        return 0\n      # print(f""{idx} = {n_p} - ({n} - {moved_cars}) + {req}"")\n      return ((self.ret_sf_pmfs[loc][idx])\n              if n_p == self.max_car_cap else self.ret_pmfs[loc][idx])\n    return sum([p_ret(n1_p, n1, k, m, 0)\n                * p_ret(n2_p, n2, nb_sells - k, -m, 1)\n                * self.req_pmfs_prod[(k, nb_sells - k)]\n                for k in range(nb_sells + 1)])\n\n  def is_terminal(self, s):\n    return s == ABSORBING_STATE\n'"
chapter4/dynamic_programming.py,20,"b'import matplotlib.pyplot as plt\nimport numpy as np\nfrom car_rental import CarRentalEnv\nfrom gambler import GamblerEnv\n\n\nclass DynamicProgramming:\n  """"""\n  Dynamic Programming algorithms to run the gridworld and car_rental\n  examples (fig 4.1 and 4.2).\n  """"""\n  def __init__(self, env, pi=None, det_pi=None, theta=1e-4, gamma=0.9):\n    self.theta = theta\n    self.env = env\n    self.V = {s: 0 for s in env.states}\n    # vect for vectorized computation\n    self.V_vect = np.array([self.V[s] for s in self.env.states]).astype(float)\n    self.gamma = gamma\n    self.pi = self.initialize_deterministic_pi(det_pi) if pi is None else pi\n    self.compute_pi_vects()\n    # expected reward of s, a\n    self.er = {(s, a): np.dot(env.r, env.pr[(s, a)]) for s in env.states\n               for a in env.moves}\n    # Q is for if we want to use action values instead of state values\n    self.Q = {(s, a): 0 for s in self.env.states for a in self.env.moves}\n    self.Q_vect = {s: np.array([self.Q[(s, a)]\n                   for a in self.env.moves]).astype(float)\n                   for s in self.env.states}\n\n  def initialize_deterministic_pi(self, det_pi_dict=None):\n    """"""Initializes a deterministic policy pi.""""""\n    if det_pi_dict is None or not det_pi_dict:\n      det_pi_dict = {s: self.env.moves[np.random.randint(len(self.env.moves))]\n                     for s in self.env.states}\n    return {(a, s): int(a == det_pi_dict[s]) for a in self.env.moves\n            for s in self.env.states}\n\n  def compute_pi_vects(self):\n    """"""Initializing vectors for pi(.|s) for faster policy evaluation.""""""\n    self.pi_vect = {s: [self.pi[(a, s)] for a in self.env.moves]\n                    for s in self.env.states}\n\n  def print_policy_gridworld(self):\n    to_print = [[None] * self.env.size for _ in range(self.env.size)]\n    max_length = max([len(move_name) for move_name in self.env.moves])\n    for x in range(self.env.size):\n      for y in range(self.env.size):\n        to_print[x][y] = str(self.deterministic_pi((x, y))).ljust(max_length)\n    print(""printing policy gridworld"")\n    print(*to_print, sep=\'\\n\')\n\n  def print_policy_car_rental(self, title=\'Figure 4.2\'):\n    fig, ax = plt.subplots()\n    X = Y = list(range(self.env.size))\n    Z = [[self.deterministic_pi((x, y)) for y in Y] for x in X]\n    print(""printing policy car rental"")\n    transposed_Z = [[Z[self.env.size - x - 1][y] for y in Y] for x in X]\n    print(*transposed_Z, sep=\'\\n\')\n    pol_range = list(range(np.min(transposed_Z), np.max(transposed_Z) + 1))\n    CS = ax.contour(X, Y, Z, colors=\'k\', levels=pol_range)\n    ax.clabel(CS, inline=1, fontsize=10)\n    ax.set_title(title)\n    plt.show()\n\n  def print_policy_gambler(self, title=\'Figure 4.3\'):\n    plt.plot([self.deterministic_pi(s) for s in self.env.states[1:-1]])\n    plt.title(title)\n    plt.show()\n\n  def print_policy(self):\n    if isinstance(self.env, CarRentalEnv):\n      self.print_policy_car_rental()\n    elif isinstance(self.env, GamblerEnv):\n      self.print_policy_gambler()\n    else:\n      self.print_policy_gridworld()\n\n  def print_values(self, show_matplotlib=False, title=""Figure 4.3""):\n    if isinstance(self.env, GamblerEnv):\n      plt.plot([self.V[s] for s in self.env.states[1:-1]])\n      plt.title(title)\n      plt.show()\n      return\n    np.set_printoptions(2)\n    size = self.env.size\n    to_print = np.zeros((size, size))\n    idxs = list(range(size))\n    for x in idxs:\n      for y in idxs:\n        to_print[x][y] = self.V[(x, y)]\n    print(""printing value function V"")\n    if isinstance(self.env, CarRentalEnv):\n      to_print_term = [[to_print[size - x - 1][y] for y in idxs] for x in idxs]\n      if show_matplotlib:\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection=\'3d\')\n        (X, Y), Z = np.meshgrid(idxs, idxs), np.array(to_print).T\n        ax.set_xlabel(\'# of cars at second location\', fontsize=10)\n        ax.set_ylabel(\'# of cars at first location\', fontsize=10)\n        ax.set_xticks([idxs[0], idxs[-1]])\n        ax.set_yticks([idxs[0], idxs[-1]])\n        ax.set_zticks([np.min(Z), np.max(Z)])\n        ax.plot_surface(X, Y, Z)\n        plt.show()\n      print(np.array(to_print_term))\n    else:\n      print(np.array(to_print))\n\n  def print_Q_values(self):\n    for s in self.env.states:\n      self.V[s] = np.dot(self.pi_vect[s], self.Q_vect[s])\n      print(f""{s}:"", end=\' \')\n      print(*[f""{a}: {self.Q[(s, a)]}"" for a in self.env.moves])\n    self.print_values()\n    self.print_policy()\n\n  def expected_value(self, s, a, arr):\n    return self.er[(s, a)] + self.gamma * np.dot(arr, self.env.psp[(s, a)])\n\n  def policy_evaluation(self):\n    """"""Updates V according to current pi.""""""\n    self.compute_pi_vects()  # for faster policy evaluation\n    while True:\n      delta = 0\n      for s in self.env.states:\n        v = self.V[s]\n        expected_values = [self.expected_value(s, a, self.V_vect)\n                           for a in self.env.moves]\n        bellman_right_side = np.dot(self.pi_vect[s], expected_values)\n        self.V[s] = self.V_vect[self.env.states.index(s)] = bellman_right_side\n        delta = max(delta, abs(v-self.V[s]))\n      if delta < self.theta:\n        break\n\n  def deterministic_pi(self, s):\n    return self.env.moves[np.argmax([self.pi[(a, s)] for a in self.env.moves])]\n\n  def update_pi(self, s, a):\n    """"""Sets pi(a|s) = 1 and pi(a\'|s) = 0 for a\' != a.""""""\n    for a_p in self.env.moves:\n      self.pi[(a_p, s)] = (a == a_p)\n\n  def policy_improvement(self):\n    """"""Improves pi according to current V. Returns True if policy is stable.""""""\n    policy_stable = True\n    for s in self.env.states:\n      a_old = self.deterministic_pi(s)\n      ev = np.array([self.expected_value(s, a, self.V_vect)\n                    for a in self.env.moves])\n      a_new = self.env.moves[np.random.choice(np.flatnonzero(ev == ev.max()))]\n      self.update_pi(s, a_new)\n      policy_stable = policy_stable and (a_old == a_new)\n    return policy_stable\n\n  def policy_iteration(self):\n    while True:\n      self.policy_evaluation()\n      if self.policy_improvement():\n        return self.V, self.pi\n\n  def policy_iteration_improved(self):\n    past_policies_str = [str(self.pi)]\n    while True:\n      self.policy_evaluation()\n      policy_stable = self.policy_improvement()\n      pol_str = str(self.pi)\n      if policy_stable or pol_str in past_policies_str:\n        return self.V, self.pi\n      past_policies_str.append(pol_str)\n\n  def policy_evaluation_Q(self):\n    """"""Updates Q according to current pi.""""""\n    self.compute_pi_vects()  # for faster policy evaluation\n    while True:\n      delta = 0\n      for s in self.env.states:\n        # computing the sum over a_p of pi(a_p|s_p)Q(s_p, a_p)\n        # in advance so Q doesn\'t change between updates\n        expected_Q = [np.dot(self.pi_vect[s_p], self.Q_vect[s_p])\n                      for s_p in self.env.states]\n        for a in self.env.moves:\n          q = self.Q[(s, a)]\n          self.Q[(s, a)] = self.expected_value(s, a, expected_Q)\n          self.Q_vect[s][self.env.moves.index(a)] = self.Q[(s, a)]\n          delta = max(delta, abs(q-self.Q[(s, a)]))\n      if delta < self.theta:\n        break\n\n  def policy_improvement_Q(self):\n    """"""Improves pi according to current Q. Returns True if policy is stable.""""""\n    policy_stable = True\n    for s in self.env.states:\n      a_old = self.deterministic_pi(s)\n      best_actions = np.flatnonzero(self.Q_vect[s] == self.Q_vect[s].max())\n      a_new = self.env.moves[np.random.choice(best_actions)]\n      self.update_pi(s, a_new)\n      policy_stable = policy_stable and (a_old == a_new)\n    return policy_stable\n\n  def policy_iteration_Q(self):\n    """"""Policy iteration using Q values.""""""\n    while True:\n      self.policy_evaluation_Q()\n      if self.policy_improvement_Q():\n        return self.Q, self.pi\n\n  def value_iteration(self):\n    import time\n    start = time.time()\n    while True:\n      delta = 0\n      for s in self.env.states:\n        v = self.V[s]\n        expected_values = [self.expected_value(s, a, self.V_vect)\n                           for a in self.env.moves]\n        self.V[s] = self.V_vect[self.env.states.index(s)] = max(expected_values)\n        delta = max(delta, abs(v-self.V[s]))\n      if delta < self.theta:\n        break\n    print(f""finished value iteration after {time.time()-start}s"")\n    self.policy_improvement()\n'"
chapter4/figures.py,0,"b'import argparse\n\nfrom car_rental import CarRentalEnv\nfrom dynamic_programming import DynamicProgramming\nfrom gridworld import Gridworld\nfrom gambler import GamblerEnv\n\nDEF_FIG_4_1_SIZE = 4\nDEF_FIG_4_2_SIZE = 21\nDEF_FIG_4_3_SIZE = 100\nDEF_EX_4_4_SIZE = 3\n\n\ndef random_policy(env):\n  def pi(s, a):\n    return 1 / len(env.moves)\n  return pi\n\n\ndef fig_4_1(size=None):\n  if size is None:\n    size = DEF_FIG_4_1_SIZE\n  env = Gridworld(size)\n  pi_rand = random_policy(env)\n  pi_init = {(a, s): pi_rand(s, a) for s in env.states for a in env.moves}\n  alg = DynamicProgramming(env, pi=pi_init, theta=1e-4, gamma=1)  # undiscounted\n  alg.policy_evaluation()\n  alg.print_values()\n  # show the optimal policy\n  while not alg.policy_improvement():\n    pass\n  alg.print_policy()\n\n\ndef fig_4_2(size=None):\n  if size is None:\n    size = DEF_FIG_4_2_SIZE\n  # size - 1 because nb of cars from 0 to ""size"" param\n  env = CarRentalEnv(size - 1)\n  subject_policy = {s: 0 for s in env.states}\n  alg = DynamicProgramming(env, det_pi=subject_policy, gamma=0.9, theta=1e-4)\n  alg.policy_iteration()\n  alg.print_values(show_matplotlib=True)\n  alg.print_policy()\n\n\ndef ex_4_4(size=None):\n  """"""\n  Testing a policy iteration that stops when policy encountered twice on\n  environment where all policies are equally bad (gridworld with cost of move\n  equal to zero).\n  """"""\n  if size is None:\n    size = DEF_EX_4_4_SIZE\n  env = Gridworld(size, cost_move=0)\n  det_pi = {s: env.moves[0] for s in env.states}\n  alg = DynamicProgramming(env, det_pi=det_pi, theta=1e-7, gamma=1)\n  # uncomment/comment for the difference between improvement and not improvement\n  alg.policy_iteration_improved()\n  # alg.policy_iteration()  # only converge if lucky fixed point\n\n\ndef ex_4_5(size=None):\n  """"""\n  Testing policy evaluation and policy iteration on gridworld using Q values.\n  """"""\n  if size is None:\n    size = DEF_EX_4_4_SIZE\n  env = Gridworld(size)\n  pi_rand = random_policy(env)\n  pi_init = {(a, s): pi_rand(s, a) for s in env.states for a in env.moves}\n  alg = DynamicProgramming(env, pi=pi_init, theta=1e-4, gamma=1)\n  alg.policy_iteration_Q()\n  alg.print_policy()\n\n\ndef ex_4_7(size=None):\n  if size is None:\n    size = DEF_FIG_4_2_SIZE\n  # size - 1 because nb of cars from 0 to ""size"" param\n  env = CarRentalEnv(size - 1, ex_4_7=True)\n  subject_policy = {s: 0 for s in env.states}\n  alg = DynamicProgramming(env, det_pi=subject_policy, gamma=0.9, theta=1e-4)\n  alg.policy_iteration()\n  alg.print_values(show_matplotlib=True)\n  alg.print_policy_car_rental(\'Exercise 4.7\')\n\n\ndef run_gambler(size=None, title=\'Figure 4.3\', p_heads=0.4, theta=1e-4):\n  if size is None:\n    size = DEF_FIG_4_3_SIZE\n  env = GamblerEnv(size, p_heads=p_heads)\n  status_quo_policy = {s: 0 for s in env.states}\n  alg = DynamicProgramming(env, det_pi=status_quo_policy, gamma=1, theta=theta)\n  alg.value_iteration()\n  full_title = title + f"" p_h={p_heads}""\n  alg.print_values(title=full_title)\n  alg.print_policy_gambler(full_title)\n\n\ndef fig_4_3(size=None):\n  run_gambler(size, p_heads=0.4, title=\'Figure 4.3\', theta=1e-15)\n\n\ndef ex_4_9(size=None):\n  run_gambler(size, p_heads=0.25, title=\'Exercise 4.9\', theta=1e-15)\n  run_gambler(size, p_heads=0.55, title=\'Exercise 4.9\', theta=1e-15)\n\n\nPLOT_FUNCTION = {\n  \'4.1\': fig_4_1,\n  \'4.2\': fig_4_2,\n  \'ex4.4\': ex_4_4,\n  \'ex4.5\': ex_4_5,\n  \'ex4.7\': ex_4_7,\n  \'4.3\': fig_4_3,\n  \'ex4.9\': ex_4_9,\n}\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n\n  parser.add_argument(\'figure\', type=str, default=None,\n                      help=\'Figure to reproduce.\',\n                      choices=PLOT_FUNCTION.keys())\n  parser.add_argument(\'-s\', \'--size\', type=int, default=None,\n                      help=\'Size of the environment (size * size states).\')\n  args = parser.parse_args()\n\n  PLOT_FUNCTION[args.figure](args.size)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
chapter4/gambler.py,0,"b'from mdp import MDP\n\nLOSE = 0\nR_LOSE = 0\nR_WIN = 1\n\n\nclass GamblerEnv(MDP):\n  def __init__(self, size=100, p_heads=0.4):\n    # size of the problem is the goal in dollars\n    self._goal = size\n    self._p_heads = p_heads\n    super().__init__()\n\n  @property\n  def p_heads(self):\n    return self._p_heads\n\n  @property\n  def goal(self):\n    return self._goal\n\n  @property\n  def moves(self):\n    return list(range(self.goal + 1))\n\n  @property\n  def states(self):\n    return list(range(LOSE, self.goal + 1))\n\n  @property\n  def r(self):\n    return [R_LOSE, R_WIN]\n\n  def _p(self, s_p, r, s, a):\n    """"""Transition function defined in private because p dictionary in mdp.py.""""""\n    capital, stakes, target_capital = s, a, s_p\n    if self.is_terminal(s):\n      return float((r == 0) and (s_p == s))\n    if (stakes > min(capital, self.goal - capital) or\n        (target_capital not in [(s + stakes), (s - stakes)])\n        or (r == R_WIN and target_capital != self.goal)\n        or (r == R_LOSE) and target_capital == self.goal):\n        return 0\n    return (self.p_heads * (target_capital >= s) +\n            (1 - self.p_heads) * (target_capital <= s))\n\n  def is_terminal(self, s):\n    return s == LOSE or s == self.goal\n'"
chapter4/gridworld.py,2,"b'import numpy as np\nfrom mdp import MDP\n\nMOVES = {\n  ""UP"": (-1, 0),\n  ""DOWN"": (1, 0),\n  ""RIGHT"": (0, 1),\n  ""LEFT"": (0, -1),\n}\n\nUNIQUE_TERMINAL_STATE = (0, 0)\n\n\nclass Gridworld(MDP):\n  def __init__(self, size=4, cost_move=1):\n    self.size_val = size\n    self.cost_move = cost_move\n    super().__init__()\n\n  @property\n  def size(self):\n    return self.size_val\n\n  @property\n  def moves(self):\n    return list(MOVES.keys())\n\n  @property\n  def states(self):\n    return [(x, y) for x in range(self.size)\n            for y in range(self.size)]\n\n  @property\n  def r(self):\n    return [-1, 0]\n\n  def next_s(self, s, a):\n    move = MOVES[a]\n    candidate = s[0] + move[0], s[1] + move[1]\n    if self.is_terminal(s):\n      return s\n    elif self.is_terminal(candidate):\n      return UNIQUE_TERMINAL_STATE  # two grey cells are the same state\n    elif not self.is_valid(candidate):  # out of border move\n      return s\n    else:\n      return candidate\n\n  def reward(self, s, a):\n    return 0 if self.is_terminal(s) else -self.cost_move\n\n  def is_valid(self, s):\n    def is_valid_coord(x):\n      return 0 <= x < self.size\n    return np.all([is_valid_coord(s[i]) for i in range(len(s))])\n\n  def _p(self, s_p, r, s, a):\n    s_next = self.next_s(s, a)\n    r_next = self.reward(s, a)\n    return 1 if (self.is_valid(s_next)\n                 and np.all(s_next == s_p) and\n                 (r == r_next)) else 0\n\n  def is_terminal(self, s):\n    return ((s[0] == 0 and s[1] == 0) or\n            (s[0] == (self.size - 1) and s[1] == (self.size - 1)))\n'"
chapter4/mdp.py,3,"b'from abc import ABC, abstractmethod\nimport time\nimport numpy as np\n\n\nclass MDP(ABC):\n    """"""Base environment for the dynamic programming chapter.""""""\n\n    def __init__(self):\n        self.init_p()\n\n    def renormalize(self):\n        for s in self.states:\n            for a in self.moves:\n                p_sum = sum([self.p[(s_p, r, s, a)] for s_p in self.states\n                             for r in self.r])\n                if p_sum > 0:\n                    for s_p in self.states:\n                        for r in self.r:\n                            self.p[(s_p, r, s, a)] /= p_sum\n\n    def init_p(self):\n        print(""starting to compute transitions p..."")\n        start = time.time()\n        self.p = {(s_p, r, s, a): self._p(s_p, r, s, a)\n                  for s in self.states for a in self.moves\n                  for s_p in self.states for r in self.r}\n        # hardcoded normalization to avoid overflow\n        self.renormalize()\n\n        def p_sum(s_p_list, r_list, s_list, a_list):\n            return np.sum([self.p[(s_p, r, s, a)] for s_p in s_p_list\n                           for r in r_list for s in s_list for a in a_list])\n        self.pr = {(s, a): np.array([p_sum(self.states, [r], [s], [a])\n                   for r in self.r]) for s in self.states for a in self.moves}\n        self.psp = {(s, a): np.array([p_sum([s_p], self.r, [s], [a])\n                    for s_p in self.states])\n                    for s in self.states for a in self.moves}\n        print(f""finished after {time.time()-start}s"")\n'"
chapter4/utils.py,0,"b'def print_old_psums(env):\n    """"""Print sums of p(.,.|s,a) before normalization.""""""\n    for s in env.states:\n        for a in env.moves:\n            p_sum = sum([env._p(s_p, r, s, a) for s_p in env.states\n                        for r in env.r])\n            print(f""\\n### ({s}, {a}) ###\\n"")\n            print(*[f""({s_p}, {r}): {env._p(s_p, r, s, a)}"" for s_p in env.states for r in env.r], sep=\'\\n\')\n            print(f""\\nsum of p(., .| {s}, {a}) = {p_sum}"")\n'"
chapter5/blackjack.py,0,"b'from mdp import MDP\nimport random\n\nR_LOSE = -1\nR_DRAW = 0\nR_WIN = 1\nR_STEP = 0\nSTICK = 0\nHIT = 1\nMIN_PLAY_SUM = 12\nMIN_DEAL_CARD = 1\nBLACKJACK = 21\nN_DEAL_SCORES = 10\nACE_STATES = 2\nNUMBER_CARDS = 52\nNUMBER_COLORS = 4\nNB_VALUES = NUMBER_CARDS // NUMBER_COLORS\nACE_LOW = 1\nACE_HIGH = 11\nACE_DIFF = ACE_HIGH - ACE_LOW\nDEAL_THRES = 17\nPLAY_THRES = 12\nN_POSSIBLE_PLAY_SUMS = BLACKJACK - MIN_PLAY_SUM + 1\n\n\nclass Player:\n  def __init__(self, stick_threshold):\n    """"""\n    Deal cards so player sum is already greater than 12 (because it doesn\'t\n    make sense to stick when sum is lower) and dealer played his\n    policy of sticking when sum is lower than 17.\n    """"""\n    self.stick_threshold = stick_threshold\n\n  @property\n  def sum(self):\n    return self.card_sum + ACE_DIFF * self.usable_ace\n\n  @property\n  def usable_ace(self):\n    return (self.card_sum + ACE_DIFF <= BLACKJACK) and (ACE_LOW in self.cards)\n\n  @property\n  def card_sum(self):\n    return sum(self.card_value(i) for i in range(len(self.cards)))\n\n  @property\n  def bust(self):\n    return self.sum > BLACKJACK\n\n  def sample_card(self):\n    return random.randint(ACE_LOW, NB_VALUES)\n\n  def cards_to_values(self, card):\n    """"""Replace the value of kings, queens, jacks to 10.""""""\n    return min(card, 10)\n\n  def card_value(self, idx):\n    """"""Returns the value of the card #idx.""""""\n    return self.cards_to_values(self.cards[idx])\n\n  def new_card(self):\n    self.cards.append(self.sample_card())\n\n  def deal_cards(self):\n    while self.sum < self.stick_threshold:\n      self.new_card()\n\n  def reset(self, initial_cards=None):\n    self.cards = [] if initial_cards is None else initial_cards\n    self.deal_cards()\n\n  def __str__(self):\n    return (f""CARDS={self.cards}, "" +\n            f""SUM={self.sum}, "" +\n            f""USABLE_ACE={self.usable_ace}"")\n\n\nclass BlackjackEnv(MDP):\n  def __init__(self):\n    super().__init__()\n    self.players = {""dealer"": Player(DEAL_THRES),\n                    ""player"": Player(PLAY_THRES)}\n\n  def seed(self, seed=0):\n    random.seed(seed)\n\n  @property\n  def moves(self):\n    return [STICK, HIT]\n\n  @property\n  def states(self):\n    # states are encoded using: player\'s sum, dealer\'s showing card * ace or not\n    return list(range((BLACKJACK - MIN_PLAY_SUM + 1) *\n                      N_DEAL_SCORES * ACE_STATES))\n\n  @property\n  def r(self):\n    return [R_LOSE, R_DRAW, R_WIN]\n\n  def is_natural(self):\n    return (self.players[\'player\'].sum == BLACKJACK\n            and len(self.players[\'player\'].cards) == 2)\n\n  def get_result(self):\n    player, dealer = self.players[""player""], self.players[""dealer""]\n    sum_diff = player.sum - dealer.sum\n    if player.bust:\n      return R_LOSE\n    elif dealer.bust or sum_diff > 0:\n      return R_WIN\n    elif sum_diff == 0:\n      return R_DRAW\n    else:\n      return R_LOSE\n\n  def hit(self):\n    s = self.get_state()\n    self.players[\'player\'].new_card()\n    done = self.players[\'player\'].bust\n    if done:\n      return s, R_LOSE, done, {}  # returning s is arbitrary\n    return self.get_state(), R_STEP, done, {}\n\n  def stick(self):\n    return self.get_state(), self.get_result(), True, {}\n\n  def step(self, action):\n    if self.is_natural():\n      return self.get_state(), R_WIN, True, {}\n    return self.hit() if action == HIT else self.stick()\n\n  def compute_state(self, player_sum, usable_ace, dealer_card):\n    return (usable_ace * N_POSSIBLE_PLAY_SUMS * N_DEAL_SCORES +\n            N_POSSIBLE_PLAY_SUMS * (dealer_card - MIN_DEAL_CARD) +\n            (player_sum - MIN_PLAY_SUM))\n\n  def get_state(self):\n      return self.compute_state(self.players[\'player\'].sum,\n                                self.players[\'player\'].usable_ace,\n                                self.players[\'dealer\'].card_value(0))\n\n  def decode_state(self, s):\n    """"""Inverse of the function get_state but with state as input.""""""\n    player_sum = (s % N_POSSIBLE_PLAY_SUMS) + MIN_PLAY_SUM\n    s = (s - (player_sum - MIN_PLAY_SUM)) // N_POSSIBLE_PLAY_SUMS\n    dealer_card = (s % N_DEAL_SCORES) + MIN_DEAL_CARD\n    player_usable_ace = (s - (dealer_card - MIN_DEAL_CARD)) // N_DEAL_SCORES\n    return player_sum, player_usable_ace, dealer_card\n\n  def player_cards(self, player_sum, player_usable_ace):\n    cards = []\n    if player_usable_ace:\n      cards.append(ACE_LOW)\n      player_sum -= ACE_HIGH\n    while player_sum > 0:\n      card = min(player_sum, 10)\n      cards.append(card)\n      player_sum -= card\n    return cards\n\n  def force_state(self, s):\n    """"""Forces initial state to be s (e.g. for exploring starts).""""""\n    player_sum, usable_ace, dealer_card = self.decode_state(s)\n    self.players[\'player\'].reset(self.player_cards(player_sum, usable_ace))\n    self.players[\'dealer\'].reset([dealer_card])\n\n  def reset(self):\n    for player in self.players.values():\n      player.reset()\n    return self.get_state()\n\n  def _p(self, s_p, r, s, a):\n    """"""Transition function defined in private because p dictionary in mdp.py.""""""\n    pass\n\n  def __str__(self):\n    return (f""#####\\nPLAYER: {self.players[\'player\']}\\n"" +\n            f""DEALER: {self.players[\'dealer\']}"")\n'"
chapter5/figures.py,12,"b'import argparse\nimport copy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport time\n\n\nfrom blackjack import BlackjackEnv, HIT, STICK, N_POSSIBLE_PLAY_SUMS, MIN_DEAL_CARD\nfrom mc import (MonteCarloFirstVisit, MonteCarloES, OffPolicyMCControl,\n                OffPolicyMCPrediction, OnPolicyFirstVisitMonteCarlo)\nfrom one_state import LEFT, OneState, RIGHT, S_INIT\nfrom racetrack import RacetrackEnv, Position, Velocity, RaceState\n\nPOLICY_THRESHOLD = 20\nFIG_5_3_N_RUNS = 100\nFIG_5_3_STATE_VALUE = -0.27726\nFIG_5_3_PLAYER_SUM = 13\nFIG_5_3_USABLE_ACE = True\nFIG_5_3_DEALER_CARD = 2\nFIG_5_3_MAX_EP = 10 ** 4\nFIG_5_3_N_ESTIMATION_EP = 100000\nFIG_5_4_N_RUNS = 10\nFIG_5_4_MAX_EP = 10 ** 8\nFIG_5_5_MAX_EP = 10 ** 2\nFIG_5_5_MAX_PRINT_VEL = 1\nFIG_5_5_BLACK_COLOR = -1\nFIG_5_5_FINISH_COLOR = 0\nFIG_5_5_INIT_COLOR = 0\nFIG_5_5_N_INTERM_TRAJS = 100\nINITIAL_STATE_IDX = 0\n\ndef values_to_grid(env, V, usable_ace):\n  """"""Puts values V into a printable grid form depending on usable_ace.""""""\n  states = [env.decode_state(i) for i in V.keys()]\n  to_print = np.zeros((N_POSSIBLE_PLAY_SUMS, N_DEAL_SCORES))\n  for (i, (player_sum, usab, dealer_card)) in enumerate(states):\n    if usab == usable_ace:\n      to_print[player_sum - MIN_PLAY_SUM, dealer_card - MIN_DEAL_CARD] = V[i]\n  return to_print\n\n\ndef print_plot(to_print, title, fig, fig_id):\n  """"""Prints the grid `to_print` as presented in Figure 5.1. and 5.3.""""""\n  dealer_idxs = np.arange(MIN_DEAL_CARD, MIN_DEAL_CARD + N_DEAL_SCORES)\n  player_idxs = np.arange(MIN_PLAY_SUM, BLACKJACK + 1)\n  ax = fig.add_subplot(fig_id, projection=\'3d\')\n  ax.set_title(title, fontsize=10)\n  (X, Y), Z = np.meshgrid(dealer_idxs, player_idxs), to_print\n  ax.set_xlabel(\'Dealer showing\', fontsize=8)\n  ax.set_ylabel(\'Player Sum\', fontsize=8)\n  ax.set_xticks([dealer_idxs.min(), dealer_idxs.max()])\n  ax.set_yticks([player_idxs.min(), player_idxs.max()])\n  ax.set_zticks([-1, 1])\n  ax.plot_surface(X, Y, Z)\n\n\ndef print_policy(alg, usab_ace, title, fig, fig_id):\n  ax = fig.add_subplot(fig_id)\n  ax.set_title(title, fontsize=10)\n  to_print = np.zeros((N_POSSIBLE_PLAY_SUMS, N_DEAL_SCORES))\n  states = [alg.env.decode_state(i) for i in alg.V.keys()]\n  for (i, (player_sum, usab, dealer_card)) in enumerate(states):\n    if usab == usab_ace:\n      a = alg.sample_action(i, det=alg.det_pi is not None)\n      to_print[player_sum - MIN_PLAY_SUM, dealer_card - MIN_DEAL_CARD] = a\n  X = Y = list(range(to_print.shape[0]))\n  Z = [[to_print[x, y] for y in Y] for x in X]\n  dealer_idxs = np.arange(MIN_DEAL_CARD, MIN_DEAL_CARD + N_DEAL_SCORES)\n  player_idxs = np.arange(MIN_PLAY_SUM, BLACKJACK + 1)\n  sns.heatmap(Z, xticklabels=dealer_idxs, yticklabels=player_idxs,\n              cbar_kws={\'label\': \'0 = STICK, 1 = HIT\'})\n  ax.invert_yaxis()\n  ax.set_title(title)\n\ndef print_race_policy(alg):\n  env = alg.env\n  grid = env.race_map.grid\n  pi = alg.det_target\n\n  def print_speed_grid(fig, pol, grid, axis, vel, fig_id, fig_id_base):\n    ax = fig.add_subplot(str(fig_id_base) + str(fig_id))\n    ax.set_title(f\'axis = {""x"" if axis == 0 else ""y""}, vel = {str(vel)}\')\n    to_print = np.zeros_like(grid) - 2\n    for x in range(grid.shape[0]):\n      for y in range(grid.shape[1]):\n        pos = Position(x,y) \n        s = RaceState(pos, vel)\n        if grid[x, y] and s.is_valid(env.race_map):\n          a_best = pi[s]\n          to_print[x,y] = (a_best.x if axis == 0 else a_best.y)\n    sns.heatmap(to_print, xticklabels=[], yticklabels=[])\n    ax.invert_yaxis()\n\n  \n  x_vels = [Velocity(x, 0) for x in range(FIG_5_5_MAX_PRINT_VEL + 1)]\n  y_vels = [Velocity(0, y) for y in range(FIG_5_5_MAX_PRINT_VEL + 1)]\n  for (idx, vel) in enumerate(x_vels + y_vels):\n    fig = plt.figure()\n    for axis in [0, 1]:\n      print_speed_grid(fig, pi, grid, axis, vel, axis + 1, \'12\')\n    plt.show()\n\n\ndef plot_race_traj(alg, start_state, debug=True, max_steps=np.inf, eps=None, total_ep=None, title_fig=\'Fig 5.5\'):\n  # generating trajectories\n  alg.det_pi = alg.det_target\n  traj = alg.generate_trajectory(start_state=start_state, det=True, max_steps=max_steps, eps=eps)\n  \n  # initial map coloring\n  race_map = alg.env.race_map\n  color_grid = copy.copy(race_map.grid)\n  mask = copy.copy(1-race_map.grid)\n  for pos in race_map.finish_line:\n    color_grid[pos.x, pos.y] = FIG_5_5_FINISH_COLOR\n  for s_init in race_map.initial_states:\n    color_grid[s_init.p.x, s_init.p.y] = FIG_5_5_INIT_COLOR\n  backup_grid = copy.copy(color_grid) \n\n  def color_traj(s, a, color=None):\n    x, y = s.p.x, s.p.y\n    delta_x, delta_y = s.v.x + a.x, s.v.y + a.y\n    dx, dy = np.sign(delta_x), np.sign(delta_y) \n    color = color if color is not None else backup_grid[x, y]\n    while True:\n      color_grid[x, y] = color\n      if abs(delta_x) != abs(delta_y):   \n        if ((abs(x - s.p.x) + 1) / (abs(y - s.p.y) + 1)) > ((abs(delta_x) + 1) / (abs(delta_y) + 1)):\n          y += dy\n        else:\n          x += dx\n      else:\n        x, y = x + dx, y + dy\n      if (x == (s.p.x + delta_x) and y == (s.p.y + delta_y)) or not (0 <= x < color_grid.shape[0]) or not (0 <= y < color_grid.shape[1]):\n        if (0 <= x < color_grid.shape[0]) and (0 <= y < color_grid.shape[1]):\n          color_grid[x, y] = color\n        break\n\n  # plotting \n  def show_reverse(grid, mask_arr):\n      sns.heatmap(grid[::-1], mask=mask_arr[::-1],cbar_kws={\'label\': \'velocity norm\'})\n      plt.show()\n \n  fig, ax = plt.subplots()\n  nb_ep = \'\' if total_ep is None else f""after {total_ep} episodes: ""\n  ax.set_title(f""{title_fig} - speed heatmap - {nb_ep} optimal traj. takes {len(traj)} actions"")\n  ax.invert_xaxis()\n  for (s, a, _) in traj:\n    color = (s.v + a).norm()\n    color_traj(s, a, color) \n    if debug:\n      show_reverse(color_grid, mask)\n      color_traj(s, a) \n  if not debug:\n    show_reverse(color_grid, mask)\n\ndef random_policy(env):\n  p_uniform = 1 / len(env.moves)\n  return {(a, s): p_uniform for a in env.moves for s in env.states}\n\n\ndef blackjack_policy(env):\n  def policy(s, a):\n    player_sum, _, _ = env.decode_state(s)\n    return float(a == (STICK if player_sum >= POLICY_THRESHOLD else HIT))\n  return {(a, s): policy(s, a) for s in env.states for a in env.moves}\n\n\ndef blackjack_det_policy(env):\n  def policy(s):\n    player_sum, _, _ = env.decode_state(s)\n    return STICK if player_sum >= POLICY_THRESHOLD else HIT\n  return {s: policy(s) for s in env.states}\n\ndef generate_step_list(n_episodes):\n  step_list = []\n  step = 1\n  base = 10\n  while step < n_episodes:\n    for i in range(1, base):\n      step_list.append(i * step)\n    step *= base\n  return step_list + [n_episodes]\n\ndef fig_5_1():\n  env = BlackjackEnv()\n  fig = plt.figure()\n  fig.suptitle(\'Figure 5.1\')\n  for (i, n_episodes) in enumerate([10000, 500000]):\n    alg = MonteCarloFirstVisit(env, pi=blackjack_policy(env), gamma=1)\n    alg.first_visit_mc_prediction(n_episodes=n_episodes)\n    for (j, usable_ace) in enumerate([True, False]):\n      fig_id = \'22\' + str(2 * j + i + 1)\n      title = f""After {n_episodes} episodes ""\n      title += f""({\'No usable\' if not usable_ace else \'Usable\'} ace)""\n      print_plot(values_to_grid(env, alg.V, usable_ace=usable_ace),\n                 title=title, fig=fig, fig_id=fig_id)\n  plt.show()\n\n\ndef fig_5_2(n_episodes=int(1e5), on_policy_instead=False):\n  env = BlackjackEnv()\n  fig = plt.figure()\n  fig.suptitle(\'Figure 5.2\')\n  if on_policy_instead:\n    alg = OnPolicyFirstVisitMonteCarlo(env, pi=blackjack_policy(env),\n                                       det_pi=None, gamma=1, epsilon=1e-2)\n  else:\n    alg = MonteCarloES(env, pi=blackjack_policy(env),\n                       det_pi=blackjack_det_policy(env), gamma=1)\n  alg.estimate_optimal_policy(n_episodes=n_episodes)\n  alg.estimate_V_from_Q()\n  for (j, usable_ace) in enumerate([True, False]):\n    def fig_id(j, policy): return \'22\' + str(2 * (j + 1) - policy)\n    title = f""({\'No usable\' if not usable_ace else \'Usable\'} ace)""\n    print_policy(alg, usable_ace, title, fig, fig_id(j, policy=True))\n    print_plot(values_to_grid(env, alg.V, usable_ace=usable_ace),\n               title=\'v*\' + title, fig=fig, fig_id=fig_id(j, policy=False))\n  plt.show()\n\n\ndef fig_5_3(n_episodes):\n  n_episodes = FIG_5_3_MAX_EP if n_episodes == None else n_episodes\n  env = BlackjackEnv()\n  fig, ax = plt.subplots()\n  plt.title(\'Figure 5.3\')\n  fig_5_3_state = env.compute_state(FIG_5_3_PLAYER_SUM, FIG_5_3_USABLE_ACE,\n                                    FIG_5_3_DEALER_CARD)\n  step_list = generate_step_list(n_episodes)\n\n  # computing the value of the state from example 5.4 with first visit MC\n  # to check if we get ""-0.27726""\n  # estimat_alg = MonteCarloFirstVisit(env, pi=blackjack_policy(env), gamma=1)\n  # estimat_alg.first_visit_mc_prediction(FIG_5_3_N_ESTIMATION_EP, fig_5_3_state)\n  # print(estimat_alg.V[fig_5_3_state])\n  # -> result: turns out i get -0.28051 after 100000 episodes!\n\n  def compute_errors(alg, step_list, start_state):\n    errors = np.zeros(len(step_list))\n    all_estimates = []\n    for seed in range(FIG_5_3_N_RUNS):\n      print(f""\\n\\n@@@@@@@@@@@\\n\\n RUN #{seed} \\n\\n@@@@@@@@@@@\\n\\n"")\n      alg.reset()\n      estimates = alg.estimate_state(step_list, start_state, seed)\n      all_estimates.append(estimates)\n      errors = errors + (estimates - FIG_5_3_STATE_VALUE) ** 2\n    return (1 / FIG_5_3_N_RUNS) * errors\n  for weighted in [True, False]:\n    label = (\'Weighted\' if weighted else \'Ordinary\') + \' Importance Sampling\'\n    color = \'g\' if not weighted else \'r\'\n    alg = OffPolicyMCPrediction(env, pi=blackjack_policy(env),\n                                weighted=weighted, b=random_policy(env),\n                                gamma=1)\n    errors = compute_errors(alg, step_list, fig_5_3_state)\n    plt.plot(step_list, errors, color=color, label=label)\n  plt.xscale(\'log\')\n  ax.set_xticks(step_list)\n  ax.set_xlabel(\'Episodes (log scale)\')\n  ax.set_ylabel(f\'Mean square error (average over {FIG_5_3_N_RUNS} runs)\')\n  plt.legend()\n  plt.show()\n\ndef fig_5_4(n_episodes):\n  n_episodes = FIG_5_4_MAX_EP if n_episodes == None else n_episodes\n  # plot initialization\n  fig, ax = plt.subplots()\n  plt.title(\'Figure 5.4\')\n  fig_5_4_state = S_INIT\n\n  # algorithm initialization\n  env = OneState()\n  always_left_policy = {(a, s): float(a == LEFT) for a in env.moves for s in env.states}\n  alg = OffPolicyMCPrediction(env, pi=always_left_policy,\n                              weighted=False, b=random_policy(env),\n                              gamma=1)\n\n  # algorithm runs\n  step_list = generate_step_list(n_episodes)\n  for seed in range(FIG_5_4_N_RUNS):\n    alg.reset()\n    estimates = alg.estimate_state(step_list, fig_5_4_state, seed)\n    plt.plot(step_list, estimates)\n\n  # plotting\n  const = np.zeros_like(step_list)\n  for y in [1, 2]:\n    plt.plot(step_list, const + y, color=\'black\', linestyle=\'dashed\')\n  plt.xscale(\'log\')\n  ax.set_xticks(step_list)\n  ax.set_xlabel(\'Episodes (log scale)\')\n  ax.set_yticks([0, 1, 2])\n  ax.set_ylabel(\'MC estimate of v_pi(s) with ordinary \' + \n                f\'import. samp. ({FIG_5_4_N_RUNS} runs)\')\n  plt.show()\n\ndef fig_5_5(n_episodes, config_file, truncated_weighted_avg_est=False, title_fig=\'Fig 5.5\'): \n  n_episodes = FIG_5_5_MAX_EP if n_episodes == None else n_episodes\n  config_file = \'1.txt\' if config_file is None else config_file\n  env = RacetrackEnv(config_file)\n  gamma = 0.9 if truncated_weighted_avg_est else 1\n  for start_state in env.race_map.initial_states[INITIAL_STATE_IDX:]:\n    # training runs\n    env.seed(0)\n    env.noise = True\n    step_list = generate_step_list(n_episodes)\n    alg = OffPolicyMCControl(env, pi=random_policy(env),\n                             b=random_policy(env),\n                             gamma=gamma)\n \n    interm_trajs_length = n_episodes // FIG_5_5_N_INTERM_TRAJS\n    total_ep = 0\n    for i in range(FIG_5_5_N_INTERM_TRAJS):\n      alg.env.noise = True\n      optimisation_alg = alg.optimal_policy if not truncated_weighted_avg_est else alg.truncated_weighted_avg_est\n      optimisation_alg(n_episodes=interm_trajs_length, start_state=start_state, step_list=step_list)\n      total_ep += interm_trajs_length\n      alg.env.noise = False\n      plot_race_traj(alg, start_state, debug=False, max_steps=1000, total_ep=total_ep, eps=None, title_fig=title_fig)\n\ndef ex_5_14(n_episodes, config_file): \n  fig_5_5(n_episodes, config_file, truncated_weighted_avg_est=True, title_fig=\'Ex 5.14\')\n\nPLOT_FUNCTION = {\n  \'5.1\': fig_5_1,\n  \'5.2\': fig_5_2,\n  \'5.3\': fig_5_3,\n  \'5.4\': fig_5_4, \n  \'5.5\': fig_5_5, \n  \'ex5.14\': ex_5_14,\n}\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n\n  parser.add_argument(\'figure\', type=str, default=None,\n                      help=\'Figure to reproduce.\',\n                      choices=PLOT_FUNCTION.keys())\n  parser.add_argument(\'-n\', \'--n_ep\', type=int, default=None,\n                      help=\'Number of episodes.\')\n  parser.add_argument(\'-o\', \'--on_policy_instead\', type=bool, default=False,\n                      help=\'For testing on-policy first visit MC control.\')\n  parser.add_argument(\'-c\', \'--config\', type=str, default=\'configs/trivial.txt\',\n                      help=\'Config file for the maps of figure 5.5.\')\n  args = parser.parse_args()\n\n  if args.figure in [\'5.1\']:\n    PLOT_FUNCTION[args.figure](args.n_ep, args.on_policy_instead)\n  elif args.figure in [\'5.3\', \'5.4\']:\n    PLOT_FUNCTION[args.figure](args.n_ep)\n  elif args.figure in [\'5.5\', \'ex5.14\']:\n    PLOT_FUNCTION[args.figure](args.n_ep, args.config)\n\nif __name__ == ""__main__"":\n  main()\n'"
chapter5/mc.py,15,"b'import numpy as np\nimport time\n\nclass MonteCarlo:\n  def __init__(self, env, pi=None, det_pi=None, gamma=0.9):\n    self.env = env\n    self.pi = pi\n    self.det_pi = det_pi\n    self.gamma = gamma\n    self.reset()\n\n  def seed(self, seed):\n    self.env.seed(seed)\n    np.random.seed(seed)\n\n  def print_values(self):\n    print(self.V)\n\n  def sample_action(self, s, det=True, eps=None):\n    if eps is not None and np.random.random() < eps:\n      return self.env.moves[np.random.randint(len(self.env.moves))]\n    elif not det:\n      pi_dist = []\n      for a in self.env.moves:\n        pi_dist.append(self.pi[(a,s)])\n      pi_dist = [self.pi[(a, s)] for a in self.env.moves]\n      return self.env.moves[np.random.choice(np.arange(len(self.env.moves)), p=pi_dist)]\n    else:\n      return self.det_pi[s]\n\n  def generate_trajectory(self, start_state=None, det=True, max_steps=np.inf, eps=None): \n    trajs = []\n    s = self.env.reset() if start_state is None else start_state\n    if start_state is not None:\n      self.env.force_state(start_state)\n    n_steps = 0\n    while True and n_steps < max_steps:\n      a = self.sample_action(s, det, eps)\n      s_p, r, done, _ = self.env.step(a)\n      n_steps += 1\n      trajs.append((s, a, r))\n      s = s_p\n      if done:\n        break\n    return trajs\n\n  def update_pi(self, s, a):\n    """"""Sets pi(a|s) = 1 and pi(a\'|s) = 0 for a\' != a.""""""\n    for a_p in self.env.moves:\n      self.pi[(a_p, s)] = (a == a_p)\n\n  def update_det_pi(self, s, a):\n    self.det_pi[s] = a\n\n  def estimate_V_from_Q(self):\n    for s in self.env.states:\n      self.V[s] = max(self.Q[(s, a)] for a in self.env.moves)\n\n  def reset(self):\n    self.V = {s: 0 for s in self.env.states}\n    self.Q = {(s, a): 0 for s in self.env.states for a in self.env.moves}\n\n\nclass MonteCarloFirstVisit(MonteCarlo):\n  def __init__(self, env, pi=None, det_pi=None, gamma=0.9):\n    super().__init__(env, pi, det_pi, gamma)\n    self.returns = {s: [] for s in env.states}\n    self.return_counts = {key: 0 for key in self.returns.keys()}\n\n  def first_visit_mc_prediction(self, n_episodes, start_state=None):\n    for _ in range(n_episodes):\n      trajs = self.generate_trajectory(start_state=start_state, det=False)\n      G = 0\n      states = [s for (s, _, _) in trajs]\n      for (i, (s, a, r)) in enumerate(trajs[::-1]):\n        G = self.gamma * G + r\n        if s not in states[:-(i + 1)]:  # logging only first visits\n          self.returns[s].append(G)\n          self.return_counts[s] += 1\n          self.V[s] += (1 / self.return_counts[s]) * (G - self.V[s])\n\n\nclass MonteCarloES(MonteCarlo):\n  def __init__(self, env, pi=None, det_pi=None, gamma=0.9):\n    """"""Monte Carlo Exploring Starts (page 99).""""""\n    super().__init__(env, pi, det_pi, gamma)\n    self.returns = {(s, a): [] for s in env.states for a in env.moves}\n    self.return_counts = {key: 0 for key in self.returns.keys()}\n\n  def exploring_starts(self):\n    """"""Returns a state-action pair such that all have non-zero probability.""""""\n    def random_choice(l): return l[np.random.randint(len(l))]\n    return map(random_choice, (self.env.states, self.env.moves))\n\n  def generate_trajectory_exploring_starts(self, det=True):\n    s, a = self.exploring_starts()\n    self.env.force_state(s)\n    s_p, r, done, _ = self.env.step(a)\n    first_step = [(s, a, r)]\n    if done:\n      return first_step\n    return first_step + self.generate_trajectory(start_state=s_p, det=det)\n\n  def estimate_optimal_policy(self, n_episodes):\n    for _ in range(n_episodes):\n      trajs = self.generate_trajectory_exploring_starts(det=True)\n      G = 0\n      pairs = [(s, a) for (s, a, _) in trajs]\n      for (i, (s, a, r)) in enumerate(trajs[::-1]):\n        G = self.gamma * G + r\n        if (s, a) not in pairs[:-(i + 1)]:  # logging only first visits\n          self.returns[(s, a)].append(G)\n          self.return_counts[(s, a)] += 1\n          self.Q[(s, a)] += ((1 / self.return_counts[(s, a)]) *\n                             (G - self.Q[(s, a)]))\n          val = np.array([self.Q[(s, a)] for a in self.env.moves])\n          a_max_idx = np.random.choice(np.flatnonzero(val == val.max()))\n          self.update_det_pi(s, self.env.moves[a_max_idx])\n\n\nclass OnPolicyFirstVisitMonteCarlo(MonteCarlo):\n  def __init__(self, env, pi=None, det_pi=None, gamma=0.9, epsilon=0.1):\n    super().__init__(env, pi, None, gamma)\n    self.returns = {(s, a): [] for s in env.states for a in env.moves}\n    self.return_counts = {key: 0 for key in self.returns.keys()}\n    self.epsilon = epsilon\n\n  def update_pi_soft(self, s, a_max):\n    n_act = len(self.env.moves)\n    for a in self.env.moves:\n      self.pi[(a, s)] = self.epsilon / n_act + (1 - self.epsilon) * (a == a_max)\n\n  def estimate_optimal_policy(self, n_episodes):\n    for _ in range(n_episodes):\n      trajs = self.generate_trajectory(det=False)\n      G = 0\n      pairs = [(s, a) for (s, a, _) in trajs]\n      for (i, (s, a, r)) in enumerate(trajs[::-1]):\n        G = self.gamma * G + r\n        if (s, a) not in pairs[:-(i + 1)]:  # logging only first visits\n          self.returns[(s, a)].append(G)\n          self.return_counts[(s, a)] += 1\n          self.Q[(s, a)] += ((1 / self.return_counts[(s, a)]) *\n                             (G - self.Q[(s, a)]))\n          val = np.array([self.Q[(s, a)] for a in self.env.moves])\n          a_max_idx = np.random.choice(np.flatnonzero(val == val.max()))\n          self.update_pi_soft(s, self.env.moves[a_max_idx])\n\n\nclass OffPolicyMC(MonteCarlo):\n  def __init__(self, env, pi, weighted=True, b=None, gamma=0.9):\n    super().__init__(env, pi, None, gamma)\n    self.b = pi if b is None else b\n    self.pi = b  # because self.pi used in generate_trajectory\n    self.target = pi\n    self.weighted = weighted # weighted or ordinary important sampling\n    self.reset()\n\n  def target_estimate(self, s):\n    return sum(self.target[(a, s)] * self.Q[(s, a)] for a in self.env.moves)\n\n  def reset(self):\n    super().reset()\n    self.C = {(s, a): 0 for s in self.env.states for a in self.env.moves}\n    self.visit_counts = {key: 0 for key in self.C.keys()}\n\n\nclass OffPolicyMCPrediction(OffPolicyMC):\n  def __init__(self, env, pi, weighted=True, b=None, gamma=1):\n    super().__init__(env, pi, weighted, b, gamma)\n    self.reset()\n\n  def reset(self):\n    super().reset()\n    self.estimates = []\n    # returns scaled by importance sampling ratio\n    self.is_returns = {(s, a): [] for s in self.env.states for a in self.env.moves}\n\n  def ordinary_is(self, n_episodes, start_state=None, step_list=None):\n    """"""Ordinary Importance Sampling when start_state happens once per episode.""""""\n    step_list = [] if step_list is None else step_list\n    q_steps = []\n    for episode in range(n_episodes + 1):\n      trajs = self.generate_trajectory(start_state=start_state, det=False)\n      G = 0\n      W = 1\n      for (i, (s, a, r)) in enumerate(trajs[::-1]):\n        G = self.gamma * G + r\n        self.is_returns[(s, a)].append(W * G)\n        W *= self.target[(a, s)] / self.b[(a, s)]\n        if W == 0:\n          break\n      if episode in step_list:\n        for a in self.env.moves:\n          self.Q[(start_state, a)] = np.sum(self.is_returns[(s, a)]) / episode\n        self.estimates.append(self.target_estimate(start_state))\n\n  def weighted_is(self, n_episodes, start_state=None, step_list=None):\n    """"""Weighted Importance Sampling when start_state happens once per episode.""""""\n    step_list = [] if step_list is None else step_list\n    q_steps = []\n    for episode in range(n_episodes + 1):\n      trajs = self.generate_trajectory(start_state=start_state, det=False)\n      G = 0\n      W = 1\n      for (i, (s, a, r)) in enumerate(trajs[::-1]):\n        G = self.gamma * G + r\n        self.C[(s, a)] += W\n        self.Q[(s, a)] += (W / self.C[(s, a)]) * (G - self.Q[(s, a)])\n        W *= self.target[(a, s)] / self.b[(a, s)]\n        if W == 0:\n          break\n      if episode in step_list:\n        self.estimates.append(self.target_estimate(start_state))\n  \n  def importance_sampling(self, n_episodes, start_state=None, step_list=None):\n    algorithm = self.weighted_is if self.weighted else self.ordinary_is\n    algorithm(n_episodes, start_state, step_list)\n\n  def estimate_state(self, step_list, start_state=None, seed=0):\n    """"""Returns a list of state estimates at steps `step_list` for MSE.""""""\n    self.seed(seed)\n    self.importance_sampling(max(step_list), start_state=start_state,\n                             step_list=step_list)\n    estimates_arr = np.array(self.estimates)\n    self.estimates = []\n    return estimates_arr\n\nclass OffPolicyMCControl(OffPolicyMC):\n  def __init__(self, env, pi, b, gamma=1):\n    super().__init__(env, pi, True, b, gamma)\n    self.reset()\n    self.init_det_pi()\n\n\n  def init_det_pi(self):\n    self.det_target = {}\n    for s in self.env.states:\n      for a in self.env.moves:\n        # initializing Q to allow some randomness in deterministic policy\n        self.Q[(s,a)] = -int(1e5)\n      self.update_det_target(s)\n\n  def update_det_target(self, s):\n    best_move = self.env.moves[np.argmax([self.Q[(s, a)] for a in self.env.moves])]\n    self.det_target[s] = best_move\n\n  def det_target_estimate(self, s):\n    return self.Q[(s,self.det_target[s])]\n \n  def optimal_policy(self, n_episodes, start_state=None, step_list=None):\n    step_list = [] if step_list is None else step_list\n    for episode in range(1, n_episodes + 1):\n      start = time.time() \n      trajs = self.generate_trajectory(start_state=start_state, det=False)\n      print(f""generating trajectory took: {time.time() - start}s"")\n      if episode > 0 and episode % 10 == 0:\n        print(f""episode #{episode}"")\n      G = 0\n      W = 1\n      for (i, (s, a, r)) in enumerate(trajs[::-1]):\n        G = self.gamma * G + r\n        self.C[(s, a)] += W\n        self.Q[(s, a)] += (W / self.C[(s, a)]) * (G - self.Q[(s, a)])\n        self.update_det_target(s)\n        if not np.all(a == self.det_target[s]):\n          break\n        W *= 1 / self.b[(a, s)]\n      if episode in step_list:\n        self.estimates.append(self.det_target_estimate(start_state))\n  \n  def truncated_weighted_avg_est(self, n_episodes, start_state=None, step_list=None):\n    step_list = [] if step_list is None else step_list\n    for episode in range(1, n_episodes + 1):\n      start = time.time() \n      trajs = self.generate_trajectory(start_state=start_state, det=False)\n      print(f""generating trajectory took: {time.time() - start}s"")\n      if episode > 0 and episode % 10 == 0:\n        print(f""episode #{episode}"")\n      G = 0\n      W = 1\n      gamma_fact = 1\n      for (i, (s, a, r)) in enumerate(trajs[::-1]):\n        G = G + r\n        if i == 1 and self.gamma != 1:\n          gamma_fact *= (1 - self.gamma) / self.gamma\n        else:\n          gamma_fact *= self.gamma\n        actual_w = W * gamma_fact\n        self.C[(s, a)] += actual_w\n        self.Q[(s, a)] += (actual_w  / self.C[(s, a)]) * (G - self.Q[(s, a)])\n        self.update_det_target(s)\n        W *= 1 / self.b[(a, s)]\n        if not np.all(a == self.det_target[s]):\n          break\n      if episode in step_list:\n        self.estimates.append(self.det_target_estimate(start_state))\n  \n  def reset(self):\n    super().reset()\n    self.estimates = []\n\n  def __str__(self):\n    res = \'\'\n    for s in self.env.states:\n      res += f""{str(s)}: {self.det_target[s]}\\n""\n    return res\n'"
chapter5/mdp.py,0,"b'from abc import ABC, abstractmethod\n\n\nclass MDP(ABC):\n    """"""Base environment for the mdp envirnoment of the MC chapter.""""""\n\n    def __init__(self):\n        self.init_p()\n\n    def init_p(self):\n        self.p = {(s_p, r, s, a): self._p(s_p, r, s, a)\n                  for s in self.states for a in self.moves\n                  for s_p in self.states for r in self.r}\n\n    @abstractmethod\n    def _p(self, s_p, r, s, a):\n        """"""Specific transition probabilities for environment.""""""\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def states(self):\n        """"""List of possible states.""""""\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def r(self):\n        """"""List of possible rewards.""""""\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def moves(self):\n        """"""List of all available actions.""""""\n        raise NotImplementedError\n'"
chapter5/one_state.py,0,"b'from mdp import MDP\nimport random\n\nR_WIN = 1\nR_STEP = 0\nR_RIGHT = 0\nLEFT = 0\nRIGHT = 1\nS_INIT = 0\nS_ABS = 1\nP_STAY = 0.9\n\nclass OneState(MDP):\n  def __init__(self):\n    super().__init__()\n    self.reset()\n\n  def seed(self, seed=0):\n    random.seed(seed)\n\n  @property\n  def moves(self):\n    return [LEFT, RIGHT]\n\n  @property\n  def states(self):\n    return [S_INIT, S_ABS]\n\n  @property\n  def r(self):\n    return [R_STEP, R_WIN]\n\n  def step(self, action):\n    if self.state == S_ABS:\n      return S_ABS, R_STEP, True, {}\n    if action == LEFT:\n      if random.random() < P_STAY:\n        return S_INIT, R_STEP, False, {}\n      else:\n        return S_ABS, R_WIN, True, {}\n    else:\n      return S_ABS, R_RIGHT, True, {}\n\n  def force_state(self, s):\n    self.state = s\n    return\n\n  def reset(self):\n    self.state = S_INIT\n    return S_INIT\n\n  def _p(self, s_p, r, s, a):\n    """"""Transition function defined in private because p dictionary in mdp.py.""""""\n    pass\n\n  def __str__(self):\n    return f""{self.state}""\n'"
chapter5/racetrack.py,8,"b'from mdp import MDP\nimport numpy as np\nimport pandas as pd\nimport random\n\nVEL_CHANGES = [-1, 0, 1]\nVEL_MIN = 0\nVEL_MAX = 5\nVEL_RANGE = range(VEL_MIN, VEL_MAX + 1) \nVEL_LIST = [(x, y) for x in VEL_RANGE for y in VEL_RANGE]\nR_STEP = -1\nNOISE_PROB = 0.1\n\nclass Velocity:\n  def __init__(self, v_x, v_y):\n    self.x = v_x \n    self.y = v_y\n\n  def norm(self):\n    return np.linalg.norm([self.x, self.y])\n\n  def __eq__(self, other_vel):\n    return self.x == other_vel.x and self.y == other_vel.y\n\n  def __add__(self, other_vel):\n    return Velocity(self.x + other_vel.x, self.y + other_vel.y)\n\n  def __hash__(self):\n    return hash((self.x, self.y))\n\n  def __str__(self):\n    return f""({self.x}, {self.y})""\n\nclass Position:\n  def __init__(self, x, y):\n    self.x = x\n    self.y = y\n\n  def updated_pos(self, vel):\n    return Position(self.x + vel.x, self.y + vel.y)\n  \n  def is_valid_pos(self, grid):\n    return ((0 <= self.x < grid.shape[0]) and\n            (0 <= self.y < grid.shape[1]) and\n            grid[self.x, self.y])\n \n  def __eq__(self, other_pos):\n    return self.x == other_pos.x and self.y == other_pos.y\n\n  def __add__(self, other_pos):\n    return Position(self.x + other_pos.x, self.y + other_pos.y)\n\n  def __str__(self):\n    return f""({self.x}, {self.y})""\n\nclass RaceState:\n  def __init__(self, pos, vel):\n    self.p = pos\n    self.v = vel\n\n  def __str__(self): \n    return f""pos={self.p}, vel={self.v}""\n\n  def __eq__(self, other_state):\n    return self.p == other_state.p and self.v == other_state.v\n  \n  def __hash__(self):\n    return hash((self.p.x, self.p.y, self.v.x, self.v.y))\n\n  def is_valid(self, race_map):\n    return (self.p.is_valid_pos(race_map.grid) and (self.v.x > 0 or self.v.y > 0\n             or self in race_map.initial_states))\n\nclass RaceMap:\n  def __init__(self, filename):\n    """"""Reads file and builds corresponding map.""""""\n    self.file_arr = np.array(pd.read_csv(filename))\n    self.build_map() \n\n  def build_map(self):\n    self.y_min, self.y_max = self.get_extremes() \n    self.grid = np.zeros((self.file_arr[:,1].sum(), self.y_max - self.y_min))\n    self.fill_grid()\n    print(self.grid)\n    self.get_initial_states() \n    self.get_valid_pos_and_finish()\n\n  def get_valid_pos_and_finish(self):\n    self.valid_pos, self.finish_line = [], []\n    for x in range(self.grid.shape[0]):\n      for y in range(self.grid.shape[1]):\n        if self.grid[x, y]:\n          self.valid_pos.append(Position(x, y)) \n          if y == self.grid.shape[1] - 1:\n            self.finish_line.append(Position(x, y))\n\n  def get_initial_states(self):\n    y_0 = abs(self.y_min)\n    self.initial_states = [RaceState(Position(0, y_0 + i),Velocity(0, 0)) for i in range(self.file_arr[0, 2])]\n\n  def get_extremes(self):\n    """"""Returns position of extreme left from first rectangle.""""""\n    pos = min_y = 0\n    max_y = pos + self.file_arr[0, 2]\n    for (shift, _, n_cols) in self.file_arr:\n      pos += shift\n      if pos < min_y:\n        min_y = pos\n      if pos + n_cols > max_y:\n        max_y = pos + n_cols\n    return min_y, max_y\n\n  def fill_grid(self):\n    x, y = 0, abs(self.y_min)\n    for (shift, n_rows, n_cols) in self.file_arr:\n      y += shift\n      self.grid[x:x + n_rows, y:y + n_cols] = True\n      x += n_rows\n\nclass RacetrackEnv:\n  def __init__(self, filename, noise=True):\n    self.race_map = RaceMap(filename)\n    self.get_velocities()\n    self.get_states()\n    self.get_moves()\n    self.noise = noise\n    self.reset()\n\n  def seed(self, seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\n  def get_velocities(self):\n    self.velocities = [Velocity(*vel) for vel in VEL_LIST]\n\n  def get_moves(self):\n    self.moves = [Velocity(x, y) for x in VEL_CHANGES for y in VEL_CHANGES]\n\n  def get_states(self):\n    self.states = [RaceState(pos, vel) for pos in self.race_map.valid_pos for vel in self.velocities if RaceState(pos, vel).is_valid(self.race_map)]\n\n  @property\n  def r(self):\n    return [R_STEP]\n\n  def is_valid_vel(self, vel):\n    return VEL_MIN <= vel.x <= VEL_MAX and VEL_MIN <= vel.y <= VEL_MAX\n\n  def intersections(self, s_s, s_e):\n    # returns all the states intersected between start and end state\n    x, y = s_s.p.x, s_s.p.y\n    delta_x, delta_y = s_e.p.x - x, s_e.p.y - y\n    dx, dy = np.sign(delta_x), np.sign(delta_y) \n    if (abs(delta_x) == abs(delta_y)) or dx == 0 or dy == 0:\n      return [Position(x + i * dx, y + i * dy) for i in range(max(abs(delta_x), abs(delta_y)) + 1)]\n    else:\n      return [Position(x + i * dx, y + j * dy) for i in range(abs(delta_x) + 1) for j in range(abs(delta_y) + 1)]\n\n  def will_hit_boundary(self, s_s, s_e):\n    # returns True if car will hit boundaries between start and end state\n    return not np.all([p.is_valid_pos(self.race_map.grid) for p in self.intersections(s_s, s_e)])\n\n  def step(self, action):\n    # noise to make task more challenging (disabling at test time)\n    if self.noise and np.random.random() < NOISE_PROB:\n      action = Velocity(VEL_MIN, VEL_MIN)\n    # tolerance for wrong vel actions\n    new_vel = self.state.v + action\n    if not self.is_valid_vel(new_vel):\n      return self.state, R_STEP, False, {}\n    # if wrong end goal or will hit boundary, go to start line\n    new_pos = self.state.p.updated_pos(new_vel)\n    new_state = RaceState(new_pos, new_vel)\n    if (not new_state.is_valid(self.race_map)) or self.will_hit_boundary(self.state, new_state):\n      return self.reset(), R_STEP, False, {}\n    self.state = new_state\n    return new_state, R_STEP, new_pos in self.race_map.finish_line, {}\n\n  def force_state(self, s):\n    self.state = s\n    return\n\n  def sample_init_state(self):\n    init_states = self.race_map.initial_states\n    rand_idx = np.random.randint(len(init_states))\n    return init_states[rand_idx]\n\n  def reset(self):\n    self.state = self.sample_init_state()\n    return self.state\n\n  def __str__(self):\n    return f""{self.state}""\n'"
chapter6/car_rental_afterstate.py,9,"b'import numpy as np\n\nREQ_LAM = [3, 4]\nRET_LAM = [3, 2]\nCAR_MOVE_COST = 2\nRENT_BEN = 10\nNB_LOC = 2\nS_ABS = (-1, -1)\nPARKING_COST = 4\n\nclass CarRentalAfterstateEnv:\n  def __init__(self, size, ex_4_7=False):\n    self.max_car_cap = size\n    self.max_car_moves = self.max_car_cap // 5 + 1\n    self.init_probs()\n    self.ex_4_7 = ex_4_7\n    self.size = self.max_car_cap + 1\n    self.get_states()\n    self.get_moves()\n    self.get_moves_d()\n    self.get_r()\n    #self.compute_p()\n\n  def get_moves(self):\n    self.moves = list(range(-self.max_car_moves, self.max_car_moves + 1))\n\n  def get_moves_d(self):\n    self.moves_d = {}\n    for (n1, n2) in self.states:\n      if (n1, n2) == S_ABS:\n        self.moves_d[(n1, n2)] = [0]\n        continue\n      max_move_right = min([abs(n1), self.max_car_cap - n2, self.max_car_moves])\n      max_move_left = min([abs(n2), self.max_car_cap - n1, self.max_car_moves])\n      self.moves_d[(n1, n2)] = np.arange(-max_move_left, max_move_right + 1)\n\n  def get_states(self):\n    self.states = [(x, y) for x in range(self.max_car_cap + 1)\n            for y in range(self.max_car_cap + 1)] + [S_ABS]\n\n  def after_state(self, s, a):\n    (n1, n2) = s\n    #print(f""{str(s)} --({a})--> {str((n1 - a, n2 + a))}"")\n    return (n1 - a, n2 + a)\n\n  def get_r(self):\n    if self.ex_4_7:\n      self.r = np.unique([self.compute_reward(n1, n2, m, car_sold)\n                        for n1 in range(self.max_car_cap + 1)\n                        for n2 in range(self.max_car_cap + 1)\n                        for m in range(-self.max_car_moves,\n                                       self.max_car_moves + 1)\n                        for car_sold in range(self.max_car_cap * NB_LOC + 1)])\n    self.r = [-CAR_MOVE_COST * car_moves + RENT_BEN * car_sold\n            for car_moves in range(self.max_car_moves + 1)\n            for car_sold in range(self.max_car_cap * NB_LOC + 1)]\n\n  def poisson_dist(self, n, lamb):\n    exp_term = np.exp(-lamb)\n    mult = exp_term\n    fact = 1\n    dist = [mult / fact]\n    dist_sum = dist[0]\n    for i in range(1, n):\n      mult *= lamb\n      fact *= i\n      frac = mult / fact\n      dist.append(frac)\n      dist_sum += frac\n    dist.append(1 - dist_sum) \n    return dist\n\n  def init_probs(self):\n    self.ret_dis = {i: self.poisson_dist(self.max_car_cap, RET_LAM[i])\n                     for i in range(NB_LOC)}\n    self.req_dis = {i: self.poisson_dist(self.max_car_cap, REQ_LAM[i])\n                     for i in range(NB_LOC)}\n\n  def move_cost(self, m):\n    if not self.ex_4_7:\n      return abs(m) * CAR_MOVE_COST\n    return CAR_MOVE_COST * abs(m - 1 if m > 0 else m)\n\n  def park_cost(self, n1, n2, m):\n    if not self.ex_4_7:\n      return 0\n    return (PARKING_COST * ((n1 + m) >= self.max_car_cap or\n                            (n2 - m) >= self.max_car_cap))\n\n  def compute_reward(self, n1, n2, m, car_sold):\n    return RENT_BEN * car_sold - (self.park_cost(n1, n2, m) + self.move_cost(m))\n\n  def sample(self, distrib):\n    return np.random.choice(np.arange(len(distrib)), p=distrib) \n\n  def step(self, a):\n    (n1, n2), m = self.state, a\n    (ret_1, ret_2), (req_1, req_2) = [[self.sample(dis[i]) for i in [0, 1]] \n                                      for dis in [self.ret_dis, self.req_dis]]\n    car_sold = req_1 + req_2\n    n1_p, n2_p = n1 - m + ret_1 - req_1, n2 + m + ret_2 - req_2\n    done = n1_p < 0 or n2_p < 0\n    r = self.compute_reward(n1, n2, m, car_sold) if not done else 0\n    s_p = S_ABS if done else tuple(map(lambda x: min(x, self.max_car_cap), (n1_p, n2_p)))\n    self.state = s_p\n    return s_p, r, done, {}\n\n  def reset(self):\n    self.state = self.states[np.random.randint(len(self.states))]\n    return self.state\n\n  def seed(self, seed):\n    np.random.seed(seed)\n\n  def compute_p(self, n_iter=100):\n    self.counts = {(s_p, r, s, a): 0\n                  for s in self.states for a in self.moves_d[s]\n                  for s_p in self.states for r in self.r}\n    count = 0\n    to_do = sum(len(self.moves_d[s]) for s in self.states)\n    for s in self.states:\n      for a in self.moves_d[s]:\n        print(f""{int(100 * (count / to_do))}%"")\n        count += 1\n        for _ in range(n_iter):\n          self.state = s\n          s_p, r, _, _ = self.step(a)\n          self.counts[(s_p, r, s, a)] += 1\n\n    def p_sum(d, s_p_list, r_list, s_list, a_list):\n      return np.sum([d[(s_p, r, s, a)] for s_p in s_p_list\n                     for r in r_list for s in s_list for a in a_list])\n\n    self.psa = {(s, a): p_sum(self.counts, self.states, self.r, [s], [a]) for s in self.states for a in self.moves_d[s]}\n    self.p = {(s_p, r, s, a): (self.counts[(s_p, r, s, a)] / self.psa[(s, a)] if self.psa[(s, a)] != 0 else 0)\n               for s in self.states for a in self.moves_d[s]\n               for s_p in self.states for r in self.r}\n\n    self.pr = {(s, a): np.array([p_sum(self.p, self.states, [r], [s], [a])\n               for r in self.r]) for s in self.states for a in self.moves_d[s]}\n    self.psp = {(s, a): np.array([p_sum(self.p, [s_p], self.r, [s], [a])\n                    for s_p in self.states])\n                    for s in self.states for a in self.moves_d[s]}\n\n\n  def is_terminal(self, s):\n    return s == S_ABS\n'"
chapter6/cliff.py,0,"b'import numpy as np\n\nINIT_POS = (3, 0)\nGOAL_POS = (3, 11)\nGRID_SHAPE = (4, 12)\nR_STEP = -1\nR_CLIFF = -100\nKEY_ACTION_DICT = {\n  \'z\': (-1, 0),\n  \'q\': (0, -1),\n  \'d\': (0, 1),\n  \'s\': (1, 0),\n}\nPOS_CHAR_DICT = {\n  GOAL_POS: \'G\',\n  INIT_POS: \'S\',\n}\nAGENT_KEY = \'A\'\nCLIFF_KEY = \'C\'\n\nclass Position:\n  def __init__(self, x, y):\n    self.x = x\n    self.y = y\n  \n  def in_bounds(self, index, axis):\n    return max(0, min(index, GRID_SHAPE[axis] - 1))\n\n  def in_cliff(self):\n    return self.x == (GRID_SHAPE[0] - 1) and 0 < self.y < GRID_SHAPE[1] - 1\n  \n  def next_state(self, action): \n    s_p = Position(self.in_bounds(self.x + action[0], 0), self.in_bounds(self.y + action[1], 1))\n    return (Position(*INIT_POS), R_CLIFF) if s_p.in_cliff() else (s_p, R_STEP)\n\n  def __eq__(self, other_pos):\n    if isinstance(other_pos, tuple):\n      return self.x == other_pos[0] and self.y == other_pos[1]\n    return self.x == other_pos.x and self.y == other_pos.y\n\n  def __hash__(self):\n    return hash((self.x, self.y))\n\n  def __str__(self):\n    return f""({self.x}, {self.y})""\n\nclass TheCliff:\n  def __init__(self):\n    self.get_states()\n    self.get_moves()\n    self.get_moves_dict()\n    self.get_keys()\n\n  def get_moves(self):\n    self.moves = [(x, y) for x in [-1, 0, 1] for y in [-1, 0, 1] if (abs(x) + abs(y)) == 1]\n\n  def get_states(self):\n    self.states = [Position(x, y) for x in range(GRID_SHAPE[0]) for y in range(GRID_SHAPE[1])]\n\n  def get_moves_dict(self):\n    self.moves_d = {s: [a for a in self.moves if s.next_state(a)[0] in self.states] for s in self.states}\n\n  def step(self, action):\n    self.state, r = self.state.next_state(action)\n    return self.state, r, self.state == Position(*GOAL_POS), {}\n\n  def get_keys(self):\n    self.keys = KEY_ACTION_DICT.keys()\n\n  def step_via_key(self, key):\n    return self.step(KEY_ACTION_DICT[key])\n\n  def reset(self):\n    self.state = Position(*INIT_POS)\n    return self.state\n\n  def seed(self, seed):\n    pass\n\n  def __str__(self):\n    x_ag, y_ag = self.state.x, self.state.y\n    s = \'\'\n    s += \'\\n\'\n    for x in range(GRID_SHAPE[0]):\n      for y in range(GRID_SHAPE[1]):\n        if (x, y) == (x_ag, y_ag):\n          s += AGENT_KEY\n        elif (x, y) in POS_CHAR_DICT.keys():\n          s += POS_CHAR_DICT[(x, y)]\n        elif Position(x, y).in_cliff():\n          s += CLIFF_KEY\n        else:\n          s += \'.\'\n      s += \'\\n\'\n    return s\n'"
chapter6/double_expected_sarsa.py,3,"b'import copy\nimport numpy as np\nfrom expected_sarsa import ExpectedSarsa \n\nclass DoubleExpectedSarsa(ExpectedSarsa): \n  def __init__(self, env, step_size=0.1, gamma=1, eps=0.1, pol_deriv=None):\n    super().__init__(env, step_size, gamma, eps, pol_deriv) \n    self.greedy_pol = self.eps_gre(eps=0)\n    self.reset()\n    print(f""gamma={self.gamma}"") \n    print(f""eps={eps}"") \n    print(f""step_size={self.step_size}"") \n\n  def double_expected_sarsa_update(self, s, a, r, s_p):\n    (Q_1, Q_2), (pi_1, pi_2) = (self.Q_l, self.pi_l) if np.random.random() < 0.5 else (self.Q_l[::-1], self.pi_l[::-1])\n    # pi_dist_1 replace the a_max_Q_1\n    pi_dist_1 = np.array([self.pi[(a, s_p)] for a in self.env.moves_d[s_p]])\n    # update Q_1 accordingly\n    Q_1[(s, a)] += self.step_size * (r + np.dot(pi_dist_1, [Q_2[(s_p, a)] for a in self.env.moves_d[s_p]]) - Q_1[(s, a)])\n    # update corresponding pi\n    self.update_pi(pi_1, Q_1, s)\n    # update the Q sum\n    self.update_Q(s, a)\n    # update the sum policy\n    self.update_pi(self.pi, self.Q, s)\n\n  def double_expected_sarsa_log_actions(self, n_episodes, to_log_s, to_log_a):\n    per_l = []\n    for ep_nb in range(n_episodes):\n      s = self.env.reset()\n      nb_a, nb_s = 0, 0\n      while True:\n        a = self.pol_deriv(s)\n        s_p, r, d, _ = self.env.step(a) \n        nb_s += (s == to_log_s)\n        nb_a += (a == to_log_a) * (s == to_log_s)\n        self.double_expected_sarsa_update(s, a, r, s_p)\n        if d:\n          per_l.append(100 * (nb_a / nb_s))\n          break\n        s = s_p\n    return per_l\n\n  def update_Q(self, s, a):\n    self.Q[(s, a)] = sum(Q[(s, a)] for Q in self.Q_l)\n \n  def reset(self):\n    super().reset()\n    self.Q_l = [copy.deepcopy(self.Q) for _ in range(2)]\n    self.pi_l = [copy.deepcopy(self.pi) for _ in range(2)]\n'"
chapter6/double_qlearning.py,2,"b'import copy\nimport numpy as np\nfrom sarsa import Sarsa \n\nclass DoubleQLearning(Sarsa): \n  def __init__(self, env, step_size=0.1, gamma=1, eps=0.1, pol_deriv=None):\n    super().__init__(env, step_size, gamma, eps, pol_deriv) \n    self.greedy_pol = self.eps_gre(eps=0)\n    self.reset()\n    print(f""gamma={self.gamma}"") \n    print(f""eps={eps}"") \n    print(f""step_size={self.step_size}"") \n\n  def double_q_learning_update(self, s, a, r, s_p):\n    Q_1, Q_2 = self.Q_l if np.random.random() < 0.5 else self.Q_l[::-1]\n    a_max_Q_1 = self.best_action(self.env.moves_d[s_p], np.array([Q_1[(s_p, a)] for a in self.env.moves_d[s_p]]))\n    Q_1[(s, a)] += self.step_size * (r + self.gamma * Q_2[(s_p, a_max_Q_1)] - Q_1[(s, a)])\n\n  def double_q_learning_log_actions(self, n_episodes, to_log_s, to_log_a):\n    per_l = []\n    for ep_nb in range(n_episodes):\n      s = self.env.reset()\n      nb_a, nb_s = 0, 0\n      while True:\n        a = self.pol_deriv(s)\n        s_p, r, d, _ = self.env.step(a) \n        nb_s += (s == to_log_s)\n        nb_a += (a == to_log_a) * (s == to_log_s)\n        self.double_q_learning_update(s, a, r, s_p)\n        self.update_Q(s, a)\n        if d:\n          per_l.append(100 * (nb_a / nb_s))\n          break\n        s = s_p\n    return per_l\n\n  def update_Q(self, s, a):\n    self.Q[(s, a)] = sum(Q[(s, a)] for Q in self.Q_l)\n \n  def reset(self):\n    super().reset()\n    self.Q_l = [copy.deepcopy(self.Q) for _ in range(2)]\n'"
chapter6/driving.py,0,"b'GO_HOME = 0\nSTATES = [""leaving office"",\n          ""reach car"",\n          ""exiting highway"",\n          ""2ndary road"",\n          ""home street"",\n          ""arrive home""]\nTRAVEL_TIME = [5, 15, 10, 10, 3, 0]\n\nclass DrivingEnv:\n  def __init__(self):\n    self.reset()\n\n  @property\n  def moves(self):\n    return [GO_HOME]\n\n  @property\n  def states(self):\n    return STATES\n\n  def associated_reward(self, state):\n    return TRAVEL_TIME[self.states.index(state)]\n\n  def step(self, action):\n    state_idx = self.states.index(self.state) \n    done = state_idx == len(self.states) - 2\n    new_state = self.states[(state_idx + 1) % len(self.states)]\n    self.state = new_state\n    return new_state, TRAVEL_TIME[state_idx], done, {}\n\n  def reset(self):\n    self.state = self.states[0]\n    return self.state\n\n  def __str__(self):\n    return self.state\n'"
chapter6/expected_sarsa.py,3,"b'import numpy as np\nfrom sarsa import Sarsa\n\nclass ExpectedSarsa(Sarsa):\n  def __init__(self, env, step_size=0.1, gamma=1, eps=0.1, update_pi=None):\n    super().__init__(env, step_size, gamma, eps, None) \n    self.update_pi = self.update_policy(eps) if update_pi is None else update_pi\n    self.reset()\n    print(f""alpha={self.step_size}"")\n    print(f""gamm={gamma}"")\n    print(f""eps={eps}"")\n\n  def uniform_pol(self, env):\n    return {(a, s): 1 / len(env.moves_d[s]) for s in env.states for a in env.moves_d[s]}\n\n  def pi_dist(self, s):\n    return np.array([self.pi[(a, s)] for a in self.env.moves_d[s]])\n\n  def sample_action_d(self, s):\n    return self.env.moves_d[s][np.random.choice(np.arange(len(self.env.moves_d[s])), p=self.pi_dist(s))]\n\n  def expected_sarsa_update(self, s, a, r, s_p):\n    self.Q[(s, a)] += self.step_size * (r + np.dot(self.pi_dist(s_p), [self.Q[s_p, a] for a in self.env.moves_d[s_p]]) - self.Q[(s, a)])\n\n  def update_policy(self, eps):\n    def update_on_q_values(pi, Q, s):\n      best_a = self.eps_gre(0)(s)\n      moves = self.env.moves_d[s]\n      soft_min = eps / len(moves)\n      for a in moves:\n        pi[(a, s)] = soft_min + (1 - eps) * (a == best_a)\n    return update_on_q_values\n\n  def expected_sarsa(self, n_episodes): \n    r_sum_l = []\n    for ep_nb in range(n_episodes):\n      s = self.env.reset()\n      r_sum = 0\n      while True:\n        a = self.sample_action_d(s)\n        s_p, r, d, _ = self.env.step(a) \n        r_sum += r\n        a_p = self.sample_action_d(s)\n        self.expected_sarsa_update(s, a, r, s_p)\n        self.update_pi(self.pi, self.Q, s)\n        if d:\n          r_sum_l.append(r_sum)\n          break\n        s = s_p\n    return r_sum_l\n\n  def expected_sarsa_log_actions(self, n_episodes, to_log_s, to_log_a): \n    per_l = []\n    for ep_nb in range(n_episodes):\n      s = self.env.reset()\n      nb_a, nb_s = 0, 0\n      while True:\n        a = self.sample_action_d(s)\n        s_p, r, d, _ = self.env.step(a) \n        nb_s += (s == to_log_s)\n        nb_a += (a == to_log_a) * (s == to_log_s)\n        a_p = self.sample_action_d(s)\n        self.expected_sarsa_update(s, a, r, s_p)\n        self.update_pi(self.pi, self.Q, s)\n        if d:\n          per_l.append(100 * (nb_a / nb_s))\n          break\n        s = s_p\n    return per_l\n\n  def reset(self):\n    super().reset()\n    self.pi = self.uniform_pol(self.env)\n'"
chapter6/figures.py,19,"b'import argparse\nfrom td import OneStepTD\nfrom off_pol_td import OffPolicyTD\nfrom driving import DrivingEnv, TRAVEL_TIME\nfrom sarsa import Sarsa\nfrom windy_gridworld import WindyGridworld\nimport numpy as np\nfrom randomwalk import RandomWalk, NotSoRandomWalk, LEFT, RIGHT\nfrom cliff import TheCliff\nimport matplotlib.pyplot as plt\nfrom qlearning import QLearning\nfrom double_qlearning import DoubleQLearning\nfrom expected_sarsa import ExpectedSarsa\nfrom max_bias_mdp import MaxBiasMDP, S_A, LEFT\nfrom double_expected_sarsa import DoubleExpectedSarsa\nfrom car_rental_afterstate import CarRentalAfterstateEnv\nfrom td_afterstate import TDAfterstate\nfrom policy_iteration_afterstate import DynamicProgrammingAfterstate\nimport seaborn as sns\n\nN_EP_EX_6_2 = 100\nN_RUNS_EX_6_2 = 100\nTRUE_VALUES_EX_6_2 = [1/6, 2/6, 3/6, 4/6, 5/6]\nTD_STEPS_6_2 = [0.05, 0.1, 0.15]\nMC_STEPS_6_2 = [0.01, 0.02, 0.03, 0.04] \nTD_STEPS_6_4 = [0.025, 0.05, 0.1, 0.15, 0.2]\nMC_STEPS_6_4 = [0.005, 0.01, 0.02, 0.03, 0.04, 0.05] \nINIT_VAL_6_2 = 1/2\nLEFT_GRAPH_STEP_SIZE = 0.1\nDEFAULT_FONT = {\'fontsize\': 14}\nSMALL_FONT = {\'fontsize\': 10}\nUNDISCOUNTED = 1\nBATCH_ALPHA = {\'td\': 0.002, \'mc\': 0.001}\nNOT_SO_RW_ALPHA = 0.001\nEX_6_5_STEP_SIZE = 0.5\nEX_6_5_EPS = 0.1\nEX_6_5_XTICKS = [k * 1000 for k in range(9)]\nEX_6_5_YTICKS = [0, 50, 100, 150, 170]\nEX_6_9_XTICKS = [k * 2000 for k in range(20)]\nEX_6_9_YTICKS = [0, 50, 100, 150, 170, 200, 500, 1000]\nEX_6_10_N_SEEDS = 10\nEX_6_6_N_EPS = 500\nEX_6_6_YTICKS = [-100, -75, -50, -25]\nEX_6_6_N_SEEDS = 10\nEX_6_6_N_AVG = 50\nFIG_6_3_N_INT_RUNS = 250\nFIG_6_3_N_INT_EPS = 100\nFIG_6_3_N_ASY_RUNS = 5\nFIG_6_3_N_ASY_EPS = 1000\nFIG_6_5_ALPHA = 0.1\nFIG_6_5_N_RUNS = 100\nFIG_6_5_N_EPS = 300\nEX_6_13_N_EPS = 300\nEX_6_13_N_RUNS = 10000\nEX_6_14_SIZE = 4\nEX_6_14_ALPHA = 0.01\nEX_6_14_N_EPS = 1000\nEX_6_14_GAMMA = 0.9\n \ndef print_driving_home(states, V_old, V_new, fig, fig_id, ax_title):\n  ax = fig.add_subplot(fig_id)\n  ax.set_title(ax_title)\n  def pred(V):\n    return [V[idx] + sum(TRAVEL_TIME[:idx]) for idx in range(len(V))]\n  ax.set_xticklabels(states, fontdict={\'fontsize\': 8})\n  ax.set_xlabel(\'Situation\')\n  ax.set_ylabel(\'Predicted total travel time\')\n  plt.plot(pred(V_old), color=\'#000000\', label=\'actual outcome\')\n  plt.plot(pred(V_new), color=\'blue\', label=\'after update\')\n\ndef fig_6_1():\n  fig = plt.figure()\n  fig.suptitle(\'Figure 6.1\')\n  env = DrivingEnv()\n  pi = {(a, s): 1.0 for s in env.states for a in env.moves} \n  V_0 = [30, 35, 15, 10, 3, 0]\n  V_init = {s: V_0[idx] for (idx, s) in enumerate(env.states)}\n \n  # TD(0)\n  alg = OneStepTD(env, V_init=V_init, step_size=1, gamma=1)\n  alg.tabular_td_0(pi)\n  V_TD = alg.get_value_list()\n  print_driving_home(env.states, V_0, V_TD, fig, \'121\', \'one step TD\')\n \n  # constant step size mc\n  alg.reset()\n  alg.constant_step_size_mc(pi)\n  V_MC = alg.get_value_list()\n  print_driving_home(env.states, V_0, V_MC, fig, \'122\', \'constant step size mc\')\n\n  plt.legend()\n  plt.savefig(\'fig6.1.png\')\n  plt.show()\n\ndef print_random_walk(ax, state_labels, td_vals):\n  ax.set_xticklabels(state_labels, fontdict=DEFAULT_FONT)\n  x_ticks = np.arange(len(state_labels))\n  ax.set_xticks(x_ticks)\n  ax.set_xlabel(\'state\', fontdict=DEFAULT_FONT)\n  ax.set_ylabel(\'estimated value\', fontdict=DEFAULT_FONT) \n  plt.plot(x_ticks, TRUE_VALUES_EX_6_2, label=\'true values\')\n  for key,td_val in td_vals.items():\n    plt.plot(x_ticks, td_val[:-1], label=str(key) + \' episodes\')\n\ndef init_random_walk(init_value, step_size=None):\n  env = RandomWalk()\n  pi = {(a, s): 1.0 for s in env.states for a in env.moves} \n  V_0 = [init_value for s in env.states[:-1]] + [0]  # V = 0 for absorbing state\n  V_init = {s: V_0[idx] for (idx, s) in enumerate(env.states)}\n  alg = OneStepTD(env, V_init=V_init, step_size=step_size, gamma=UNDISCOUNTED)\n  return alg, pi\n\ndef left_graph(fig, fig_id, init_value):\n  alg, pi = init_random_walk(init_value, step_size=LEFT_GRAPH_STEP_SIZE)\n  tot_ep = 0\n  td_vals = {}\n  ax = fig.add_subplot(\'121\')\n  for n_episodes in [0, 1, 10, 100]:\n    alg.tabular_td_0(pi, n_episodes - tot_ep)\n    td_vals[n_episodes] = alg.get_value_list()\n  print_random_walk(ax, [""A"", ""B"", ""C"", ""D"", ""E""], td_vals)\n  plt.legend()\n\ndef right_graph(fig, fig_id, init_value, td_step_sizes, mc_step_sizes, font=DEFAULT_FONT, remove_x_label=False, batch=False): \n  ax = fig.add_subplot(fig_id)\n  ax.set_title(f\'V_init = {init_value}\', fontdict=font)\n  alg, pi = init_random_walk(init_value)\n  runs_dict = {alpha: np.zeros(N_EP_EX_6_2) for alpha in td_step_sizes + mc_step_sizes} \n  td_0 = alg.tabular_td_0 if not batch else alg.td_0_batch\n  mc = alg.constant_step_size_mc if not batch else alg.constant_step_size_mc_batch\n  to_compare_list = [(td_step_sizes, td_0), (mc_step_sizes, mc)]\n  for (step_size_list, algorithm) in to_compare_list:\n    for step_size in step_size_list:\n      alg.step_size = step_size\n      print(f""running step size {step_size}"")\n      for seed in range(N_RUNS_EX_6_2): \n        alg.reset()\n        alg.env.seed(seed)\n        err_l = []\n        for nb_ep in range(N_EP_EX_6_2):\n          algorithm(pi, 1)\n          v_arr = np.array(alg.get_value_list()[:-1])\n          err_l.append(np.linalg.norm(v_arr-TRUE_VALUES_EX_6_2))\n        runs_dict[step_size] += np.array(err_l)\n\n  for key in runs_dict.keys():\n    runs_dict[key] /= N_RUNS_EX_6_2\n  \n  if not remove_x_label:\n    ax.set_xlabel(\'walks / episodes\', fontdict=font)\n  ax.set_ylabel(\'empirical rms error averaged over states\', fontdict=font) \n  for key,err_run in runs_dict.items():\n    (color, alg_name) = (\'b\',\'td\') if key in td_step_sizes else (\'r\', \'mc\')\n    linewidth = max(int(100 * key) / 10 if key in td_step_sizes else int(200 * key) / 10, 10 / (len(runs_dict) * 10))\n    linestyle = \'dashed\' if key in [0.02, 0.03] else None\n    plt.plot(err_run, color=color, label=alg_name + \' (a=\' + str(key) + \')\', linewidth=linewidth, linestyle=linestyle)\n   \n  plt.legend()\n\ndef example_6_2():\n  fig = plt.figure()\n  fig.suptitle(\'Example 6.2\', fontdict=DEFAULT_FONT)\n  left_graph(fig, fig_id=\'121\', init_value=INIT_VAL_6_2)\n  right_graph(fig, \'122\', INIT_VAL_6_2, TD_STEPS_6_2, MC_STEPS_6_2)\n  plt.savefig(\'example6.2.png\')\n  plt.show()\n\ndef ex_6_4():\n  fig = plt.figure()\n  fig.suptitle(\'Exercise 6.4\', fontdict=DEFAULT_FONT)\n  right_graph(fig, \'111\', INIT_VAL_6_2, TD_STEPS_6_4, MC_STEPS_6_4, SMALL_FONT)\n  plt.savefig(\'ex6.4.png\')\n  plt.show()\n\ndef ex_6_5():\n  fig = plt.figure()\n  fig.suptitle(\'Exercise 6.5\', fontdict=SMALL_FONT)\n  for (idx, init_val) in enumerate([0, 0.25, 0.75, 1]):\n    right_graph(fig, \'22\' + str(idx + 1), init_val, TD_STEPS_6_2, MC_STEPS_6_2, SMALL_FONT, idx < 2)\n  plt.savefig(\'ex6.5.png\')\n  plt.show()\n\ndef fig_6_2():\n  fig = plt.figure()\n  fig.suptitle(\'Figure 6.2\', fontdict=SMALL_FONT)\n  right_graph(fig, \'111\', INIT_VAL_6_2, [BATCH_ALPHA[\'td\']], [BATCH_ALPHA[\'mc\']], batch=True, font=SMALL_FONT)\n  plt.savefig(\'fig6.2.png\')\n  plt.show()\n\ndef ex_6_7():\n  env = NotSoRandomWalk()\n  env.seed(0)\n  V_0 = [1/2 for s in env.states[:-1]] + [0]\n  V_init = {s: V_0[idx] for (idx, s) in enumerate(env.states)}\n  b = {(a, s): 1/2 for s in env.states for a in env.moves} \n  pi = {(a, s): float(a == RIGHT) for s in env.states for a in env.moves}\n  alg = OffPolicyTD(env, V_init, NOT_SO_RW_ALPHA, pi, b, UNDISCOUNTED)\n  alg.step_size = 0.01\n  alg.find_value_function(N_EP_EX_6_2 * 100)\n  print(alg.get_value_list())\n\ndef init_windy_gridworld_fig(title, xticks=None, yticks=None):\n  fig, ax = plt.subplots() \n  fig.suptitle(title)\n  ax.set_xlabel(\'Time steps\')\n  ax.set_ylabel(\'Episodes\')\n  if xticks is not None:\n    ax.set_xticks(xticks)\n  if yticks is not None:\n    ax.set_yticks(yticks)\n  return ax\n\ndef plot_sarsa(ax, n_ep, label=None, diags=False, stay=False, stoch=False, seed=0):\n  env = WindyGridworld(diags, stay, stoch)\n  alg = Sarsa(env, step_size=EX_6_5_STEP_SIZE, gamma=UNDISCOUNTED, eps=EX_6_5_EPS) \n  alg.seed(seed)\n  kwargs = {""label"": label} if label else {}\n  plt.plot(alg.on_policy_td_control(n_ep), **kwargs)\n\ndef example_6_5():\n  ax = init_windy_gridworld_fig(\'Example 6.5\', EX_6_5_XTICKS, EX_6_5_YTICKS)\n  plot_sarsa(ax, max(EX_6_5_YTICKS))\n  plt.savefig(\'example6.5.png\')\n  plt.show()\n\ndef ex_6_9():\n  ax = init_windy_gridworld_fig(\'Exercise 6.9\', EX_6_9_XTICKS, EX_6_9_YTICKS)\n  n_ep_urld, n_ep = EX_6_9_YTICKS[-2:]\n  plot_sarsa(ax, n_ep_urld, label=\'up right down left\')\n  plot_sarsa(ax, n_ep, label=\'with diags\', diags=True)\n  plot_sarsa(ax, n_ep, label=\'with diags and stay\', diags=True, stay=True)\n  plt.legend()\n  plt.savefig(\'ex6.9.png\')\n  plt.show()\n\ndef ex_6_10():\n  ax = init_windy_gridworld_fig(f\'Exercise 6.10 ({EX_6_10_N_SEEDS} seeds)\')\n  n_ep = max(EX_6_9_YTICKS)\n  for seed in range(EX_6_10_N_SEEDS):\n    plot_sarsa(ax, n_ep, diags=True, stay=True, stoch=True, seed=seed)\n  plt.savefig(\'ex6.10.png\')\n  plt.show()\n\ndef smooth_rewards(arr, to_avg=5):\n  nb_rew = len(arr)\n  new_arr = np.zeros(nb_rew - to_avg + 1) \n  for i in range(nb_rew - to_avg + 1):\n    new_arr[i] = np.mean([arr[i + j] for j in range(to_avg)])\n  return new_arr\n\ndef example_6_6():\n  fig, ax = plt.subplots() \n  fig.suptitle(f\'Example 6.6 (Averaged over {EX_6_6_N_SEEDS} seeds)\')\n  ax.set_xlabel(\'Episodes\')\n  ax.set_ylabel(f\'(Average of last {EX_6_6_N_AVG}) sum of rewards during episodes\')\n  ax.set_yticks(EX_6_6_YTICKS)\n  ax.set_ylim(bottom=min(EX_6_6_YTICKS))\n  n_ep = EX_6_6_N_EPS\n  env = TheCliff()\n  qlearning_alg = QLearning(env, step_size=EX_6_5_STEP_SIZE, gamma=UNDISCOUNTED, eps=EX_6_5_EPS) \n  sarsa_alg = Sarsa(env, step_size=EX_6_5_STEP_SIZE, gamma=UNDISCOUNTED, eps=EX_6_5_EPS) \n  qlearning_rew = np.zeros(n_ep)\n  sarsa_rew = np.zeros(n_ep)\n  for seed in range(EX_6_6_N_SEEDS):\n    print(f""seed={seed}"")\n    qlearning_alg.seed(seed)\n    qlearning_rew += qlearning_alg.q_learning(n_ep)\n    sarsa_alg.seed(seed)\n    sarsa_rew += sarsa_alg.on_policy_td_control(n_ep, rews=True)\n  plt.plot(smooth_rewards(qlearning_rew / EX_6_6_N_SEEDS, EX_6_6_N_AVG), color=\'r\', label=\'Q learning\')\n  plt.plot(smooth_rewards(sarsa_rew / EX_6_6_N_SEEDS, EX_6_6_N_AVG), color=\'b\', label=\'Sarsa\')\n  plt.legend()\n  plt.savefig(\'example6.6.png\')\n  plt.show()\n\ndef fig_6_3(): \n  fig, ax = plt.subplots() \n  fig.suptitle(f\'Figure 6.3\')\n  step_sizes = np.linspace(0.1, 1, 19)\n  ax.set_xlabel(f\'Step Sizes\')\n  ax.set_xticks(step_sizes)\n  ax.set_yticks([0, -40, -80, -120])\n  ax.set_ylim(bottom=-160, top=0)\n  ax.set_ylabel(\'Sum of rewards per episodes\')\n  env = TheCliff() \n  exp_sar_alg, sar_alg, qlear_alg = [name_alg(env, step_size=None, gamma=UNDISCOUNTED, eps=EX_6_5_EPS) for name_alg in [ExpectedSarsa, Sarsa, QLearning]]\n  exp_sar_opt, sar_opt, qlear_opt = exp_sar_alg.expected_sarsa, lambda n_ep: sar_alg.on_policy_td_control(n_ep, rews=True), qlear_alg.q_learning\n  for (alg, opt, alg_name, color, marker) in [(exp_sar_alg, exp_sar_opt, \'Expected Sarsa\', \'r\', \'x\'), (sar_alg, sar_opt, \'Sarsa\', \'b\', \'v\'), (qlear_alg, qlear_opt, \'Q-learning\', \'k\', \'s\')]:\n    print(f""\\n\\n\\n@@@@@@@@ {alg_name} @@@@@@@@\\n@@@@@@@@@@@@@@@@@@@@@@@@@\\n\\n\\n"")\n    for (n_ep, n_runs, run_type_name) in [(FIG_6_3_N_INT_EPS, FIG_6_3_N_INT_RUNS, \'Interim\'), (FIG_6_3_N_ASY_EPS, FIG_6_3_N_ASY_RUNS, \'Asymptotic\')]:\n      print(f""\\n######## {run_type_name} ########\\n"")\n      rew_l = []\n      for step_size in step_sizes: \n        print(f""alpha={step_size}"")\n        alg.step_size = step_size\n        rew_sum = 0\n        for seed in range(n_runs):\n          print(f""run #{seed}"")\n          alg.seed(seed)\n          alg.reset()\n          rew_sum += np.mean(opt(n_ep))\n        rew_l.append(rew_sum / n_runs)\n      label = f""{alg_name} ({run_type_name})""\n      plt.plot(step_sizes, rew_l, label=label, color=color, marker=marker, linestyle=\'-\' if run_type_name == \'Asymptotic\' else \'--\')\n  plt.legend()\n  plt.savefig(\'fig6.3.png\')\n  plt.show()\n\n\ndef plot_max_bias(title, filename, todo_list, n_runs, n_eps):\n  fig, ax = plt.subplots() \n  fig.suptitle(title)\n  ax.set_xlabel(f\'Episodes\')\n  xticks = [1, 100, 200, 300]\n  ax.set_xlim([min(xticks), max(xticks)])\n  ax.set_yticks([0, 5, 25, 50, 75, 100])\n  ax.set_ylim([0, 100])\n  ax.set_ylabel(\'% left actions from A\')\n  for (alg, opt, color, label) in todo_list:\n    perc_left = np.zeros(n_eps)\n    for seed in range(n_runs):\n      print(seed)\n      alg.seed(seed)\n      alg.reset()\n      perc_left += opt(n_eps)\n    plt.plot(perc_left / n_runs, label=label, color=color)\n  plt.plot(np.zeros(n_eps) + 5, color=\'k\', linestyle=\'--\', label=\'optimal\')\n  plt.legend()\n  plt.savefig(filename)\n  plt.show()\n\ndef fig_6_5():\n  env = MaxBiasMDP()\n  qlear_alg, dqlear_alg = [name_alg(env, step_size=FIG_6_5_ALPHA, gamma=UNDISCOUNTED, eps=EX_6_5_EPS) for name_alg in [QLearning, DoubleQLearning]]\n  qlear_opt = lambda n_ep: qlear_alg.q_learning_log_actions(n_ep, S_A, LEFT)\n  dqlear_opt = lambda n_ep: dqlear_alg.double_q_learning_log_actions(n_ep, S_A, LEFT)\n  todo = [(qlear_alg, qlear_opt, \'r\', \'Q-learning\'), (dqlear_alg, dqlear_opt, \'g\', \'Double Q-learning\')]\n  plot_max_bias(\'Figure 6.5\', \'fig6.5.png\', todo, FIG_6_5_N_RUNS, FIG_6_5_N_EPS)\n\ndef ex_6_13():\n  env = MaxBiasMDP()\n  esarsa_alg, desarsa_alg = [name_alg(env, step_size=FIG_6_5_ALPHA, gamma=UNDISCOUNTED, eps=EX_6_5_EPS) for name_alg in [ExpectedSarsa, DoubleExpectedSarsa]]\n  esarsa_opt = lambda n_ep: esarsa_alg.expected_sarsa_log_actions(n_ep, S_A, LEFT)\n  desarsa_opt = lambda n_ep: desarsa_alg.double_expected_sarsa_log_actions(n_ep, S_A, LEFT)\n  todo = [(desarsa_alg, desarsa_opt, \'g\', \'Double Expected Sarsa\'), (esarsa_alg, esarsa_opt, \'r\', \'Expected Sarsa\')]\n  plot_max_bias(f\'Exercise 6.13 ({EX_6_13_N_RUNS} runs)\', \'ex6.13.png\', todo, EX_6_13_N_RUNS, EX_6_13_N_EPS)\n\ndef print_car_rental_value_function(size, V): \n  to_print = np.zeros((size, size))\n  idxs = list(range(size))\n  for x in idxs:\n    for y in idxs:\n      to_print[x][y] = V[(x, y)]\n  to_print_term = [[to_print[size - x - 1][y] for y in idxs] for x in idxs]\n  print(f""#####\\n\\nV mean = {np.mean(to_print_term)}\\n\\n######"")\n  #fig = plt.figure()\n  #ax = fig.add_subplot(111, projection=\'3d\')\n  #plt.title(\'Exercise 6.14 (value function)\')\n  #(X, Y), Z = np.meshgrid(idxs, idxs), np.array(to_print).T\n  #ax.set_xlabel(\'# of cars at second location\', fontsize=10)\n  #ax.set_ylabel(\'# of cars at first location\', fontsize=10)\n  #ax.set_xticks([idxs[0], idxs[-1]])\n  #ax.set_yticks([idxs[0], idxs[-1]])\n  #ax.set_zticks([np.min(Z), np.max(Z)])\n  #ax.plot_surface(X, Y, Z)\n  #plt.show()\n  return np.mean(to_print_term)\n\ndef print_policy_car_rental(size, pi):\n  fig, ax = plt.subplots()\n  X = Y = list(range(size))\n  Z = [[pi[(x, y)] for y in Y] for x in X]\n  transposed_Z = [[Z[size - x - 1][y] for y in Y] for x in X]\n  sns.heatmap(transposed_Z)\n  print(*transposed_Z, sep=\'\\n\')\n  pol_range = list(range(np.min(transposed_Z), np.max(transposed_Z) + 1))\n  #CS = ax.contour(X, Y, Z, colors=\'k\', levels=pol_range)\n  #ax.clabel(CS, inline=1, fontsize=10)\n  ax.set_title(\'Exercise 6.14 (policy)\')\n  #plt.show()\n\ndef ex_6_14(size=None, ep_per_eval=None, alpha=None, max_ep=None):\n  size = EX_6_14_SIZE if size is None else size\n  env = CarRentalAfterstateEnv(size - 1)\n  env.seed(0)\n  #pi = {(a, s): (a == 0) for s in env.states for a in env.moves_d[s]}\n  pi = {s: 0 for s in env.states}\n  step_size_l = [0.003, 0.004, 0.005]\n  log_V_mean = {step_size: [] for step_size in step_size_l}\n  for step_size in step_size_l:\n    tot_ep = 0\n    alg = TDAfterstate(env, None, step_size=step_size, gamma=EX_6_14_GAMMA, pi_init=pi)\n    stable = False\n    while len(log_V_mean[step_size]) < 10:\n      print(f""tot_ep = {tot_ep}"")\n      V, pi, stable = alg.policy_iteration(ep_per_eval=ep_per_eval, batch=True, max_ep=max_ep)\n      tot_ep += ((ep_per_eval) * (ep_per_eval + 1)) // 2\n      mean = print_car_rental_value_function(size, V)\n      log_V_mean[step_size].append(mean)\n      plt.savefig(f\'ex6.14_val_{str(ep_per_eval)}_{str(alpha)[2:]}_{str(tot_ep)}ep.png\')\n      plt.close()\n  for step_size in step_size_l:\n    plt.plot(log_V_mean[step_size], label=f\'alpha={step_size}\')\n  plt.legend()\n  plt.savefig(\'learning_rates.png\')\n  plt.show()\n  #print_policy_car_rental(size, pi)\n  #plt.savefig(\'ex6.14_pol.png\')\n\nPLOT_FUNCTION = {\n  \'6.1\': fig_6_1,\n  \'example6.2\': example_6_2,\n  \'ex6.4\': ex_6_4,\n  \'ex6.5\': ex_6_5,\n  \'6.2\': fig_6_2,\n  \'ex6.7\': ex_6_7,\n  \'example6.5\': example_6_5,\n  \'ex6.9\': ex_6_9,\n  \'ex6.10\': ex_6_10, \n  \'example6.6\': example_6_6,\n  \'6.3\': fig_6_3,\n  \'6.5\': fig_6_5,\n  \'ex6.13\': ex_6_13,\n  \'ex6.14\': ex_6_14,\n}\n\ndef main():\n  parser = argparse.ArgumentParser()\n\n  parser.add_argument(\'figure\', type=str, default=None,\n                      help=\'Figure to reproduce.\',\n                      choices=PLOT_FUNCTION.keys())\n  parser.add_argument(\'-s\', \'--size\', type=int, default=None,\n                      help=\'Size of the environment (size * size states).\')\n  parser.add_argument(\'-e\', \'--ep\', type=int, default=None)\n  parser.add_argument(\'-a\', \'--alpha\', type=float, default=None)\n  parser.add_argument(\'-m\', \'--max_ep\', type=int, default=None)\n  args = parser.parse_args()\n\n  if args.figure == \'ex6.14\':\n    PLOT_FUNCTION[args.figure](args.size, args.ep, args.alpha, args.max_ep)\n  else:\n    PLOT_FUNCTION[args.figure]()\n\nif __name__ == ""__main__"":\n  main()\n'"
chapter6/max_bias_mdp.py,2,"b'import numpy as np\n\nR_FROM_A = 0\nR_ABS = 0\nLEFT = 0\nRIGHT = 1\nABS_ACT = 0\nS_A = 0\nS_B = 1\nS_INIT = S_A\nS_ABS = 2\nN_ACT_FROM_B = 100\n\nclass MaxBiasMDP:\n  def __init__(self):\n    self.reset()\n    self.moves_d = {S_A: [LEFT, RIGHT], S_B: [i for i in range(N_ACT_FROM_B)], S_ABS: [ABS_ACT]}\n    self.states = [S_A, S_B, S_ABS]\n\n  def seed(self, seed=0):\n    random.seed(seed)\n\n  def step(self, a):\n    if self.s == S_A:\n      r = R_FROM_A\n      s_p, d = (S_ABS, True) if a == RIGHT else (S_B, False)\n    elif self.s == S_B:\n      s_p, r, d = S_ABS, np.random.randn() - 1, True\n    else:\n      s_p, r, d = S_ABS, R_ABS, True\n    self.s = s_p\n    return s_p, r, d, {}\n\n  def reset(self):\n    self.s = S_INIT\n    return S_INIT\n\n  def seed(self, seed):\n    np.random.seed(seed) \n\n  def __str__(self):\n    return f""{self.state}""\n'"
chapter6/off_pol_td.py,0,"b'from td import TD\n\nclass OffPolicyTD(TD):\n  def __init__(self, env, V_init=None, step_size=0.1, pi=None, b=None, gamma=0.9):\n    super().__init__(env, V_init, step_size, gamma)\n    self.step_size = step_size\n    self.V_init = V_init\n    self.pi = pi\n    self.b = b\n    self.reset()\n\n  def reset(self):\n    super().reset()\n\n  def generate_episode(self):\n    return self.generate_traj(self.b, log_act=True)\n\n  def off_policy_td_update(self, s, a, r, s_p):\n    is_ratio = self.pi[(a, s)] / self.b[(a, s)] if self.pi[(a, s)] > 0 else 0\n    td_err = self.td_error(s, r, s_p)\n    self.V[s] += self.step_size * td_err * is_ratio\n\n  def find_value_function(self, n_episodes):\n    for episode in range(1, n_episodes + 1):\n      traj = self.generate_episode()\n      for i in range(len(traj) - 1):\n        s, a, r = traj[i]\n        s_p, _, _ = traj[i + 1]\n        self.off_policy_td_update(s, a, r, s_p)\n'"
chapter6/play.py,0,"b'import argparse\nimport os\nfrom windy_gridworld import WindyGridworld\nfrom cliff import TheCliff\n\nENV_DICT = {\n  \'windy_gridworld\': WindyGridworld(),\n  \'cliff\': TheCliff(),\n}\n\ndef play(env):\n  def refresh():\n    os.system(\'cls\' if os.name == \'nt\' else \'clear\')\n    print(env)\n  while True:\n    s = env.reset()\n    done = False\n    while not done:\n      key = \'\'\n      while key not in env.keys:\n        refresh()\n        key = input(""press key\\n$>"")\n        if key == ""exit()"":\n          exit()\n      _, _, done, _ = env.step_via_key(key)\n    again = input(""episode done, continue? (Y / n)"")\n    if again == \'n\':\n      break\n\ndef main():\n  parser = argparse.ArgumentParser()\n\n  parser.add_argument(\'env\', type=str, default=None,\n                      help=\'Env to play with.\',\n                      choices=ENV_DICT.keys())\n  args = parser.parse_args()\n  play(ENV_DICT[args.env])\n\nif __name__ == ""__main__"":\n  main()\n'"
chapter6/policy_iteration_afterstate.py,8,"b'import numpy as np\n\nclass DynamicProgrammingAfterstate:\n  def __init__(self, env, pi=None, det_pi=None, theta=1e-4, gamma=0.9):\n    self.theta = theta\n    self.env = env\n    self.V = {s: 0 for s in env.states}\n    # vect for vectorized computation\n    self.V_vect = np.array([self.V[s] for s in self.env.states]).astype(float)\n    self.gamma = gamma\n    self.pi = self.initialize_deterministic_pi(det_pi) if pi is None else pi\n    self.compute_pi_vects()\n    # expected reward of s, a\n    self.er = {(s, a): np.dot(env.r, env.pr[(s, a)]) for s in env.states\n               for a in env.moves_d[s]}\n    print(theta)\n    print(gamma)\n\n  def initialize_deterministic_pi(self, det_pi_dict=None):\n    """"""Initializes a deterministic policy pi.""""""\n    if det_pi_dict is None or not det_pi_dict:\n      det_pi_dict = {s: self.env.moves_d[s][np.random.randint(len(self.env.moves_d[s]))]\n                     for s in self.env.states}\n    return {(a, s): int(a == det_pi_dict[s]) for s in self.env.states\n             for a in self.env.moves_d[s]}\n\n  def compute_pi_vects(self):\n    """"""Initializing vectors for pi(.|s) for faster policy evaluation.""""""\n    self.pi_vect = {s: [self.pi[(a, s)] for a in self.env.moves_d[s]]\n                    for s in self.env.states}\n\n  def expected_value(self, s, a, arr):\n    return self.er[(s, a)] + self.gamma * np.dot(arr, self.env.psp[(s, a)])\n\n  def policy_evaluation(self):\n    """"""Updates V according to current pi.""""""\n    self.compute_pi_vects()  # for faster policy evaluation\n    while True:\n      delta = 0\n      for s in self.env.states:\n        v = self.V[s]\n        expected_values = [self.expected_value(s, a, self.V_vect)\n                           for a in self.env.moves_d[s]]\n        bellman_right_side = np.dot(self.pi_vect[s], expected_values)\n        self.V[s] = self.V_vect[self.env.states.index(s)] = bellman_right_side\n        delta = max(delta, abs(v-self.V[s]))\n      if delta < self.theta:\n        break\n\n  def deterministic_pi(self, s):\n    return self.env.moves_d[s][np.argmax([self.pi[(a, s)] for a in self.env.moves_d[s]])]\n\n  def update_pi(self, s, a):\n    """"""Sets pi(a|s) = 1 and pi(a\'|s) = 0 for a\' != a.""""""\n    for a_p in self.env.moves_d[s]:\n      self.pi[(a_p, s)] = (a == a_p)\n\n  def policy_improvement(self):\n    """"""Improves pi according to current V. Returns True if policy is stable.""""""\n    policy_stable = True\n    for s in self.env.states:\n      a_old = self.deterministic_pi(s)\n      ev = np.array([self.expected_value(s, a, self.V_vect)\n                    for a in self.env.moves_d[s]])\n      a_new = self.env.moves_d[s][np.random.choice(np.flatnonzero(ev == ev.max()))]\n      self.update_pi(s, a_new)\n      policy_stable = policy_stable and (a_old == a_new)\n    return policy_stable\n\n  def policy_iteration(self):\n    while True:\n      self.policy_evaluation()\n      if self.policy_improvement():\n        return self.V, self.pi\n\n'"
chapter6/qlearning.py,0,"b'from sarsa import Sarsa\n\nclass QLearning(Sarsa): \n  def __init__(self, env, step_size=0.1, gamma=1, eps=0.1, pol_deriv=None):\n    super().__init__(env, step_size, gamma, eps, pol_deriv) \n    self.greedy_pol = self.eps_gre(eps=0)\n    self.reset()\n    print(f""gamma={self.gamma}"") \n    print(f""eps={eps}"") \n    print(f""step_size={self.step_size}"") \n\n  def q_learning(self, n_episodes):\n    r_sum_l = []\n    for ep_nb in range(n_episodes):\n      s = self.env.reset()\n      r_sum = 0\n      while True:\n        a = self.pol_deriv(s)\n        s_p, r, d, _ = self.env.step(a) \n        r_sum += r\n        a_p = self.greedy_pol(s_p)\n        self.sarsa_update(s, a, r, s_p, a_p)\n        if d:\n          r_sum_l.append(r_sum)\n          break\n        s = s_p\n    return r_sum_l\n\n  def q_learning_log_actions(self, n_episodes, to_log_s, to_log_a):\n    per_l = []\n    for ep_nb in range(n_episodes):\n      s = self.env.reset()\n      nb_a, nb_s = 0, 0\n      while True:\n        a = self.pol_deriv(s)\n        s_p, r, d, _ = self.env.step(a) \n        nb_s += (s == to_log_s)\n        nb_a += (a == to_log_a) * (s == to_log_s)\n        a_p = self.greedy_pol(s_p)\n        self.sarsa_update(s, a, r, s_p, a_p)\n        if d:\n          per_l.append(100 * (nb_a / nb_s))\n          break\n        s = s_p\n    return per_l\n\n  def reset(self):\n    super().reset()\n'"
chapter6/randomwalk.py,4,"b'import numpy as np\n\nN_STATES = 5\nEMPTY_MOVE = 0\nSTATES = list(range(N_STATES))\nP_LEFT = 0.5\nP_LEFT_L = [0.9, 0.1]\nR_STEP = 0\nABSORBING_STATE = N_STATES\nLEFT = 0\nRIGHT = 1\n\nclass RandomWalk:\n  def __init__(self):\n    self.reset()\n\n  @property\n  def moves(self):\n    return [EMPTY_MOVE]\n\n  @property\n  def states(self):\n    return STATES + [ABSORBING_STATE]\n\n  def sample_shift(self):\n    return np.sign(np.random.random() - P_LEFT)\n\n  def step(self, action):\n    new_state = self.state + self.sample_shift() \n    if not (0 <= new_state < N_STATES):\n      return ABSORBING_STATE, float(new_state == N_STATES), True, {}\n    self.state = new_state\n    return self.state, R_STEP, False, {}\n\n  def reset(self):\n    self.state = N_STATES // 2\n    return self.state\n\n  def seed(self, seed):\n    np.random.seed(seed)\n\n  def __str__(self):\n    return self.state\n\nclass NotSoRandomWalk:\n  def __init__(self):\n    self.reset()\n\n  @property\n  def moves(self):\n    return [LEFT, RIGHT]\n\n  @property\n  def states(self):\n    return STATES + [ABSORBING_STATE]\n\n  def sample_shift(self, action):\n    return np.sign(np.random.random() - P_LEFT_L[action])\n\n  def step(self, action):\n    new_state = self.state + self.sample_shift(action) \n    if not (0 <= new_state < N_STATES):\n      return ABSORBING_STATE, float(new_state == N_STATES), True, {}\n    self.state = new_state\n    return self.state, R_STEP, False, {}\n\n  def reset(self):\n    self.state = N_STATES // 2\n    return self.state\n\n  def seed(self, seed):\n    np.random.seed(seed)\n\n  def __str__(self):\n    return self.state\n'"
chapter6/sarsa.py,4,"b'import numpy as np\nfrom td import TD\nimport time\n\nclass Sarsa(TD):\n  def __init__(self, env, step_size=0.1, gamma=1, eps=0.1, pol_deriv=None):\n    super().__init__(env, None, step_size, gamma) \n    self.pol_deriv = pol_deriv if pol_deriv is not None else self.eps_gre(eps)\n    self.reset() \n    #print(f""step size={self.step_size}"")\n    #print(f""epsilon={eps}"")\n    #print(f""gamma={self.gamma}"")\n\n  def best_action(self, moves, vals):\n    return moves[np.random.choice(np.flatnonzero(vals == vals.max()))]\n\n  def random_move(self, s):\n    return self.env.moves_d[s][np.random.randint(len(self.env.moves_d[s]))]\n\n  def eps_gre(self, eps):\n    def eps_gre_pol(s):\n      if np.random.random() < eps:\n        return self.random_move(s)\n      return self.best_action(self.env.moves_d[s], np.array([self.Q[(s, a)] for a in self.env.moves_d[s]]))\n    return eps_gre_pol \n\n  def sarsa_update(self, s, a, r, s_p, a_p):\n    self.Q[(s, a)] += self.step_size * (r + self.gamma * self.Q[(s_p, a_p)] - self.Q[(s, a)])\n\n  def on_policy_td_control(self, n_episodes, rews=False):\n    ep_per_timesteps = []\n    r_sum_l = []\n    for ep_nb in range(n_episodes):\n      ep_start = time.time()\n      s = self.env.reset()\n      a = self.pol_deriv(s)\n      r_sum = 0\n      while True:\n        ep_per_timesteps.append(ep_nb)\n        s_p, r, d, _ = self.env.step(a) \n        r_sum += r\n        a_p = self.pol_deriv(s_p)\n        self.sarsa_update(s, a, r, s_p, a_p)\n        if d:\n          r_sum_l.append(r_sum)\n          break\n        s, a = s_p, a_p\n    return ep_per_timesteps if not rews else r_sum_l\n\n  def reset(self):\n    self.Q = {(s,a): 0 for s in self.env.states for a in self.env.moves_d[s]}\n'"
chapter6/td.py,2,"b'import numpy as np\nimport copy\n\nclass TD:\n  def __init__(self, env, V_init=None, step_size=None, gamma=0.9):\n    self.env = env\n    self.gamma = gamma\n    self.V_init = V_init\n    self.step_size = step_size\n    self.reset()\n\n  def seed(self, seed):\n    self.env.seed(seed)\n    np.random.seed(seed)\n\n  def sample_action(self, pi, s):\n    pi_dist = [pi[(a, s)] for a in self.env.moves]\n    return self.env.moves[np.random.choice(np.arange(len(self.env.moves)), p=pi_dist)]\n  \n  def generate_traj(self, pi, log_act=False):\n    s = self.env.reset()\n    traj = []\n    while True:\n      a = self.sample_action(pi, s)\n      s_p, r, done, _ = self.env.step(a)\n      traj.append((s, r) if not log_act else (s, a, r))\n      s = s_p\n      if done:\n        return traj + [(s_p, 0) if not log_act else (s_p, a, 0)]\n\n  def td_update(self, s, r, s_p):\n    self.V[s] += self.step_size * self.td_error(s, r, s_p)\n\n  def td_error(self, s, r, s_p):\n    return r + self.gamma * self.V[s_p] - self.V[s]\n  \n  def mc_error(self, s, G):\n    return G - self.V[s]\n\n  def get_value_list(self):\n    return [val for key,val in self.V.items()]\n\n  def reset(self):\n    self.V = {s: 0 for s in self.env.states} if self.V_init is None else copy.deepcopy(self.V_init)\n\nclass OneStepTD(TD):\n  def __init__(self, env, V_init=None, step_size=0.1, gamma=0.9):\n    super().__init__(env, V_init, step_size, gamma)\n    self.reset()\n  \n  def tabular_td_0(self, pi, n_episodes=1):\n    for _ in range(n_episodes):\n      traj = self.generate_traj(pi)\n      for i in range(len(traj) - 1):\n        (s, r), (s_p, _) = traj[i], traj[i + 1]\n        self.td_update(s, r, s_p)\n\n  def td_0_batch(self, pi, n_episodes=1):\n    self.experience += [self.generate_traj(pi) for _ in range(n_episodes)]\n    td_error_sum = {s: 0 for s in self.V}\n    for traj in self.experience:\n      for i in range(len(traj) - 1):\n        (s, r), (s_p, _) = traj[i], traj[i + 1]\n        td_error_sum[s] += self.td_error(s, r, s_p)\n    for s in self.env.states[:-1]:\n      self.V[s] += self.step_size * td_error_sum[s]\n\n  def constant_step_size_mc(self, pi, n_episodes=1):\n    for _ in range(n_episodes): \n      traj = self.generate_traj(pi)\n      G = 0\n      for (s, r) in traj[::-1]:\n        G = r + self.gamma * G\n        self.V[s] += self.step_size * self.mc_error(s, G)\n\n  def constant_step_size_mc_batch(self, pi, n_episodes=1):\n    self.experience += [self.generate_traj(pi) for _ in range(n_episodes)]\n    def generate_G_traj(traj):\n      G = 0\n      G_traj = []\n      for (_, r) in traj[::-1]:\n        G = r + self.gamma * G\n        G_traj = [G] + G_traj\n      return G_traj\n    n_past_traj = len(self.G_trajs)\n    for i in range(n_episodes):\n      self.G_trajs[n_past_traj + i] = generate_G_traj(self.experience[n_past_traj + i])\n    mc_error_sum = {s: 0 for s in self.V}\n    for (traj_idx, traj) in enumerate(self.experience):\n      for i in range(len(traj) - 1):\n        (s, r), (s_p, _) = traj[i], traj[i + 1]\n        mc_error_sum[s] += self.mc_error(s, self.G_trajs[traj_idx][i])\n    for s in self.env.states:\n      self.V[s] += self.step_size * mc_error_sum[s]\n \n  def reset(self): \n    super().reset()\n    self.log = []\n    self.experience = []\n    self.G_trajs = {}\n'"
chapter6/td_afterstate.py,6,"b'import numpy as np\nfrom td import TD\nimport copy\n\nclass TDAfterstate(TD):\n  def __init__(self, env, V_init=None, step_size=None, gamma=0.9, eps=0.1, pi_init=None):\n    super().__init__(env, V_init, step_size, gamma)\n    self.eps = eps\n    self.pi = pi_init\n    self.b = {(a, s): 1 / len(env.moves_d[s]) for s in env.states for a in env.moves_d[s]}\n    self.reset()\n\n  def sample_action(self, pi, s):\n    pi_dist = [pi[(a, s)] for a in self.env.moves_d[s]]\n    return self.env.moves_d[s][np.random.choice(np.arange(len(self.env.moves_d[s])), p=pi_dist)]\n\n  def generate_traj(self):\n    s = self.env.reset()\n    traj = []\n    while True:\n      a = self.sample_action(self.b, s) if np.random.random() < self.eps else self.pi[s]\n      #a = self.sample_action(self.b, s)\n      s_p, r, done, _ = self.env.step(a)\n      traj.append((s, a, r))\n      s = s_p\n      if done:\n        return traj + [(s_p, 0, 0)]\n\n  def update_pi(self, s, a_0):\n    for a in self.env.moves_d[s]:\n      self.pi[(a, s)] = (a == a_0)\n\n  def td0_afterstate(self, n_episodes):\n    for ep_nb in range(n_episodes):\n      traj = self.generate_traj()\n      for i in range(len(traj) - 1):\n        (s, a, r), (s_p, a_p, _) = traj[i], traj[i + 1]\n        s_as, s_p_as = self.env.after_state(s, a), self.env.after_state(s_p, a_p)\n        self.V_as[s_as] += self.step_size * (r + self.gamma * self.V_as[s_p_as] - self.V_as[s_as])\n\n  def td0_afterstate_batch(self, n_episodes):\n    self.experience = []\n    for ep_nb in range(n_episodes):\n      td_error_sum = {s: 0 for s in self.V_as}\n      self.experience.append(self.generate_traj())\n      for traj in self.experience:\n        for i in range(len(traj) - 1):\n          (s, a, r), (s_p, a_p, _) = traj[i], traj[i + 1]\n          s_as, s_p_as = self.env.after_state(s, a), self.env.after_state(s_p, a_p)\n          is_ratio = (1 - self.eps) * ((a == self.pi[s]) / self.b[(a, s)]) + self.eps * (1 - (a == self.pi[s])) / self.b[(a, s)] \n          td_error_sum[s_as] += self.step_size * is_ratio * (r + self.gamma * self.V_as[s_p_as] - self.V_as[s_as])\n          #td_error_sum[s_as] += self.step_size * (r + self.gamma * self.V_as[s_p_as] - self.V_as[s_as])\n      for s in self.V_as:\n        self.V_as[s] += td_error_sum[s]\n  \n  def policy_improvement(self):\n    policy_stable = True\n    for s in self.env.states:\n      a_old = self.pi[s]\n      V_vect = np.array([self.V_as[self.env.after_state(s, a)] for a in self.env.moves_d[s]])\n      a_new = self.env.moves_d[s][np.random.choice(np.flatnonzero(V_vect == V_vect.max()))]\n      self.pi[s] =  a_new\n      policy_stable = policy_stable and (a_old == a_new)\n    return policy_stable\n\n  def policy_iteration(self, ep_per_eval=10, batch=True, max_ep=np.inf):\n    pi_log = [str(self.pi)]\n    pol_eval = self.td0_afterstate_batch if batch else self.td0_afterstate\n    while True: \n      print(len(pi_log))\n      pol_eval(ep_per_eval)\n      pol_stable = self.policy_improvement()\n      pi_str = str(self.pi)\n      if pol_stable or pi_str in pi_log or len(pi_log) >= max_ep:\n        if pol_stable:\n          print(""stable"")\n        return self.V_as, self.pi, pol_stable\n      pi_log.append(pi_str)\n\n  def reset(self):\n    super().reset()\n    self.V_as = {s: np.random.random() for s in self.env.states}\n'"
chapter6/windy_gridworld.py,2,"b'import numpy as np\n\nINIT_POS = (3, 0)\nGOAL_POS = (3, 7)\nWIND_ARR = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\nGRID_SHAPE = (7, 10)\nR_STEP = -1\nKEY_ACTION_DICT = {\n  \'a\': (-1, 0),\n  \'z\': (-1, 1),\n  \'e\': (-1, -1),\n  \'q\': (0, -1),\n  \'s\': (0, 0),\n  \'d\': (0, 1),\n  \'w\': (1, -1),\n  \'x\': (1, 0),\n  \'c\': (1, 1),\n}\nPOS_CHAR_DICT = {\n  GOAL_POS: \'G\',\n  INIT_POS: \'S\',\n}\nAGENT_KEY = \'A\'\n\nclass Position:\n  def __init__(self, x, y):\n    self.x = x\n    self.y = y\n  \n  def in_bounds(self, index, axis):\n    return max(0, min(index, GRID_SHAPE[axis] - 1))\n  \n  def get_wind(self, stoch):\n    return WIND_ARR[self.y] + stoch * np.random.randint(-1, 2)\n\n  def next_state(self, stoch, action): \n    wind = self.get_wind(stoch)\n    return Position(self.in_bounds(self.x + action[0] - wind, 0), \n                    self.in_bounds(self.y + action[1], 1))\n\n  def __eq__(self, other_pos):\n    return self.x == other_pos.x and self.y == other_pos.y\n\n  def __add__(self, other_pos):\n    return Position(self.x + other_pos.x, self.y + other_pos.y)\n\n  def __hash__(self):\n    return hash((self.x, self.y))\n\n  def __str__(self):\n    return f""({self.x}, {self.y})""\n\nclass WindyGridworld:\n  def __init__(self, diags=False, stay=False, stoch=False):\n    self.diags = diags\n    self.stay = stay\n    self.stoch = stoch\n    self.get_states()\n    self.get_moves()\n    self.get_moves_dict()\n    self.get_keys()\n    print(f""stoch: {self.stoch}"")\n    print(f""stay: {self.stay}"")\n    print(f""diags: {self.diags}"")\n\n  def seed(self, seed):\n    np.random.seed(seed)\n\n  def get_moves(self):\n    self.moves = [(x, y) for x in [-1, 0, 1] for y in [-1, 0, 1] if (abs(x) + abs(y)) == 1]\n    if self.diags:\n      self.moves += [(x, y) for x in [-1, 1] for y in [-1, 1]]\n    if self.stay:\n      self.moves += [(0, 0)]\n\n  def get_states(self):\n    self.states = [Position(x, y) for x in range(GRID_SHAPE[0]) for y in range(GRID_SHAPE[1])]\n\n  def get_moves_dict(self):\n    moves_d = {}\n    for s in self.states:\n      moves_d[s] = []\n      for a in self.moves:\n        if s.next_state(self.stoch, a) in self.states:\n          moves_d[s].append(a)\n    self.moves_d = moves_d\n\n  def step(self, action):\n    self.state = self.state.next_state(self.stoch, action)\n    return self.state, R_STEP, self.state == Position(*GOAL_POS), {}\n\n  def reset(self):\n    self.state = Position(*INIT_POS)\n    return self.state\n\n  def get_keys(self):\n    self.keys = KEY_ACTION_DICT.keys()\n\n  def step_via_key(self, key):\n    return self.step(KEY_ACTION_DICT[key])\n\n  def __str__(self):\n    x_ag, y_ag = self.state.x, self.state.y\n    s = \'\'\n    for wind in WIND_ARR:\n      s += str(wind)\n    s += \'\\n\'\n    for x in range(GRID_SHAPE[0]):\n      for y in range(GRID_SHAPE[1]):\n        if (x, y) == (x_ag, y_ag):\n          s += AGENT_KEY\n        elif (x, y) in POS_CHAR_DICT.keys():\n          s += POS_CHAR_DICT[(x, y)]\n        else:\n          s += \'.\'\n      s += \'\\n\'\n    return s\n'"
chapter7/figures.py,12,"b'import argparse\nimport matplotlib.pyplot as plt\nfrom nstep_td import nStepTD\nimport numpy as np\nfrom randomwalk import RandomWalk, NotSoRandomWalk, EMPTY_MOVE\nfrom windy_gridworld import WindyGridworld\nfrom nstep_sarsa import nStepSarsa\nfrom off_pol_nstep_exp_sarsa import OffPolnStepExpSarsa\nfrom off_pol_nstep_qsigma import OffPolnStepQSigma\nfrom off_pol_nstep_sarsa import OffPolnStepSarsa\nfrom off_pol_nstep_td import OffPolnStepTD\nfrom nstep_tree_backup import nStepTreeBackup\n\nUND = 1\nFIG_7_2_N_EP = 10\nFIG_7_2_N_STATES = 19\nFIG_7_2_N_RUNS = 100\nFIG_7_2_MAX_N = 512\nEX_7_2_N_RUNS = 1\nEX_7_3_N_RUNS = 1\nEX_7_3_N_STATES = 5\nFIG_7_4_STEPSIZE = 0.5\nFIG_7_4_N_EP = 170\nFIG_7_4_MAX_N = 16\nSECTION_7_3_STEPSIZE = 0.01\nSECTION_7_3_N_EP_TRAIN = 200\nSECTION_7_3_MAX_N = 8\nEX_7_7_N_EP_TRAIN = 1000\nEX_7_7_STEPSIZE = 0.001\nEX_7_7_MAX_N = 8\nEX_7_10_N_EP_TRAIN = 10000\nEX_7_10_STEPSIZE = 0.0001\nEX_7_10_MAX_N = 2\nEX_7_10_N_BATCHES = 5\nEX_7_10_N_STATES = 5\nSECTION_7_5_STEPSIZE = 0.01\nSECTION_7_5_N_EP_TRAIN = 200\nSECTION_7_5_MAX_N = 8\nSECTION_7_6_STEPSIZE = 0.01\nSECTION_7_6_N_EP_TRAIN = 100\nSECTION_7_6_N = 2\nSECTION_7_6_SIGMA_L = [0, 0.25, 0.5, 0.75, 1]\n\ndef run_random_walks(ax, ex_7_2=False, show=True, extra_label=\'\', dashed=False, n_runs=FIG_7_2_N_RUNS, n_states=FIG_7_2_N_STATES, left_rew=-1, true_vals=None, V_init=None):\n  n_l = [2 ** k for k in range(int(np.log(FIG_7_2_MAX_N) / np.log(2)) + 1)]\n  env = RandomWalk(n_states=n_states, r_l=left_rew)\n  pi = {(a, s): 1.0 for s in env.states for a in env.moves_d[s]}\n  true_vals = np.linspace(-1, 1, env.n_states + 2)[1:-1] if true_vals is None else true_vals \n  alg = nStepTD(env, V_init=V_init, step_size=None, gamma=UND, n=n_l[0], ex_7_2=ex_7_2)\n  for n in n_l:\n    alg.n = n\n    print(f"">> n={n}"")\n    err_l = []\n    alpha_max = 1 if (n <= 16 or ex_7_2) else 1 / (np.log(n // 8) / np.log(2))\n    alpha_l = np.linspace(0, alpha_max, 31)\n    for alpha in alpha_l:\n      alg.step_size = alpha\n      print(f""alpha={alpha}"")\n      err_sum = 0\n      for seed in range(n_runs):\n        alg.reset()\n        alg.seed(seed)\n        for ep in range(FIG_7_2_N_EP):\n          alg.pol_eval(pi, n_ep=1)\n          v_arr = np.array(alg.get_value_list()[:-1])\n          err_sum += np.sqrt(np.sum((v_arr-true_vals) ** 2) / env.n_states)\n      err_l.append(err_sum / (n_runs * FIG_7_2_N_EP))\n    plt.plot(alpha_l, err_l, label=f\'{extra_label} n={n}\', linestyle=\'dashed\' if dashed else None)\n  ax.set_xticks(np.linspace(0, 1, 6))\n  yticks = np.linspace(0.25, 0.55, 6)\n  ax.set_yticks(yticks)\n  ax.set_ylim([min(yticks), max(yticks)])\n  ax.set_xlabel(\'Stepsize\')\n  ax.set_ylabel(f\'Average RMS error ({env.n_states} states, first {FIG_7_2_N_EP} episodes)\')\n\ndef ex_7_2():\n  fig, ax = plt.subplots()\n  ax.set_title(\'Exercise 7.2\')\n  run_random_walks(ax, ex_7_2=True, extra_label=\'td error sum\', dashed=True, n_runs=EX_7_2_N_RUNS)\n  run_random_walks(ax, ex_7_2=False, extra_label=\'n-step td\', n_runs=EX_7_2_N_RUNS)\n  plt.legend(fontsize=\'x-small\')\n  plt.savefig(\'plots/ex7.2.png\')\n  plt.show()\n\ndef fig_7_2():\n  fig, ax = plt.subplots()\n  ax.set_title(\'Figure 7.2\')\n  run_random_walks(ax)\n  plt.legend(fontsize=\'x-small\')\n  plt.savefig(\'plots/fig7.2.png\')\n  plt.show()\n\ndef ex_7_3():\n  # testing with 5 states\n  fig = plt.figure()\n  fig.suptitle(\'Exercise 7.3\')\n  ax = fig.add_subplot(\'121\')\n  ax.set_title(f\'{EX_7_3_N_STATES} states\')\n  run_random_walks(ax, n_runs=EX_7_3_N_RUNS, n_states=EX_7_3_N_STATES)\n  yticks = np.linspace(0.15, 0.55, 8)\n  ax.set_yticks(yticks)\n  ax.set_ylim([min(yticks), max(yticks)]) \n  \n  # testing with a left reward of 0 with better true values\n  ax2 = fig.add_subplot(\'122\')\n  ax2.set_title(f\'r=0 left, {FIG_7_2_N_STATES} states\')\n  true_vals = np.linspace(0, 1, FIG_7_2_N_STATES + 2)[1:-1]\n  V_init = {s: 1/2 if s != FIG_7_2_N_STATES else 0 for s in range(FIG_7_2_N_STATES + 1)}\n  yticks = np.linspace(0.1, 0.3, 11)\n  run_random_walks(ax2, n_runs=EX_7_3_N_RUNS, n_states=FIG_7_2_N_STATES, left_rew=0, true_vals=true_vals, V_init=V_init)\n  ax2.set_yticks(yticks)\n  ax2.set_ylim([min(yticks), max(yticks)])\n  plt.legend(fontsize=\'x-small\')\n  plt.savefig(\'plots/ex7.3.png\')\n  plt.show()\n\ndef run_alg(alg, title, filename, n_ep, k_min, n_max, x_label=\'Timesteps\', y_label=\'Episodes\', show=True, extra_label=\'\', ax=None, reset=True, dashed=False):\n  if ax is None:\n    fig, ax = plt.subplots()\n    ax.set_title(title)\n  n_l = [2 ** k for k in range(k_min, int(np.log(n_max) / np.log(2)) + 1)]\n  alg.seed(0)\n  for n in n_l:\n    alg.n = n\n    if reset:\n      alg.reset()\n    plt.plot(alg.pol_eval(n_ep), label=f\'{extra_label}n={n}\', linestyle=\'dashed\' if dashed else None)\n  ax.set_xlabel(x_label)\n  ax.set_ylabel(y_label)\n  if show:\n    plt.legend()\n    plt.savefig(filename)\n    plt.show()\n\ndef fig_7_4():\n  env = WindyGridworld()\n  alg = nStepSarsa(env, step_size=FIG_7_4_STEPSIZE, gamma=UND, n=2)\n  run_alg(alg, f\'Figure 7.4 - n-step sarsa on windy gridworld (alpha={alg.step_size})\', \'plots/fig7.4.png\', FIG_7_4_N_EP, 0, FIG_7_4_MAX_N)\n\ndef section_7_3():\n  env = NotSoRandomWalk()\n  alg = OffPolnStepSarsa(env, b=None, step_size=SECTION_7_3_STEPSIZE, gamma=UND, n=2)\n  run_alg(alg, f\'Section 7.3 - off-policy n-step sarsa on (not so) random walk\\n({env.n_states} states, alpha={alg.step_size})\', \'plots/section7.3.png\', SECTION_7_3_N_EP_TRAIN, 1, SECTION_7_3_MAX_N, \'Train episodes\', \'avg episode length for 10 test episodes\\n (+ moving average)\')\n\ndef ex_7_7():\n  env = NotSoRandomWalk()\n  alg = OffPolnStepExpSarsa(env, b=None, step_size=EX_7_7_STEPSIZE, gamma=UND, n=2)\n  run_alg(alg, f\'Exercise 7.7 - off policy n-step expected sarsa on (not so) random walk\\n({env.n_states} states, alpha={alg.step_size})\', \'plots/ex7.7.png\', EX_7_7_N_EP_TRAIN, 1, EX_7_7_MAX_N, \'Train episodes\', \'avg episode length for 10 test episodes\\n(+ moving average)\')\n\ndef ex_7_10():\n  fig, ax = plt.subplots()\n  env = NotSoRandomWalk(n_states=EX_7_10_N_STATES, r_l=0)\n  alg_simple, alg_off_pol = [OffPolnStepTD(env, b=None, step_size=EX_7_10_STEPSIZE, gamma=UND, n=2, simple=is_simple) for is_simple in [True, False]]\n  for batch in range(EX_7_10_N_BATCHES):\n    for (alg, dashed, extra_lab) in [(alg_simple, True, \'(7.1) & (7.9)\'), (alg_off_pol, False, \'(7.2) & (7.13)\')]:\n      print((batch + 1) * EX_7_10_N_EP_TRAIN)\n      run_alg(alg, \'\', \'\', EX_7_10_N_EP_TRAIN, 1, EX_7_10_MAX_N, \'States\', \'Value\', show=False, ax=ax, extra_label=f\'{extra_lab} {(batch + 1) * EX_7_10_N_EP_TRAIN} ep. \', reset=False, dashed=dashed)\n  ax.set_title(f\'Exercise 7.10 - Off Pol. n-step TD on \\n(not so) random walk ({env.n_states} states, alpha={alg.step_size})\')\n  fig.set_size_inches(8, 6)\n  plt.legend()\n  plt.savefig(\'plots/ex7.10.png\', dpi=100)\n  plt.show()\n\ndef section_7_5():\n  env = NotSoRandomWalk(n_states=19)\n  alg = nStepTreeBackup(env, step_size=SECTION_7_5_STEPSIZE, gamma=UND, n=1)\n  run_alg(alg, f\'Section 7.5 - n-step tree backup on (not so) random walk\\n({env.n_states} states, alpha={alg.step_size})\', \'plots/section7.5.png\', SECTION_7_5_N_EP_TRAIN, 1, SECTION_7_5_MAX_N, \'Train episodes\', \'avg episode length for 10 test episodes\\n (+ moving average)\')\n\ndef section_7_6():\n  env = NotSoRandomWalk()\n  fig, ax = plt.subplots()\n  for sigma in SECTION_7_6_SIGMA_L:\n    alg = OffPolnStepQSigma(env, sigma_f=sigma, step_size=SECTION_7_6_STEPSIZE, gamma=UND, n=SECTION_7_6_N)\n    plt.plot(alg.pol_eval(SECTION_7_6_N_EP_TRAIN), label=f\'sigma={sigma}\')\n  ax.set_title(f\'Section 7.6 - Off Pol. n-step Q(sigma) on \\n(not so) random walk ({env.n_states} states, alpha={alg.step_size}, n={SECTION_7_6_N})\')\n  ax.set_xlabel(\'Train episodes\')\n  ax.set_ylabel(\'avg episode length for 10 test episodes\\n (+ moving average)\')\n  plt.legend()\n  plt.savefig(\'plots/section7.6.png\', dpi=100)\n  plt.show()\n\nPLOT_FUNCTION = {\n  \'ex7.2\': ex_7_2,\n  \'7.2\': fig_7_2,\n  \'ex7.3\': ex_7_3,\n  \'7.4\': fig_7_4,\n  \'section7.3\': section_7_3,\n  \'ex7.7\': ex_7_7,\n  \'ex7.10\': ex_7_10,\n  \'section7.5\': section_7_5,\n  \'section7.6\': section_7_6,\n}\n\ndef main():\n  parser = argparse.ArgumentParser()\n\n  parser.add_argument(\'figure\', type=str, default=None,\n                      help=\'Figure to reproduce.\',\n                      choices=list(PLOT_FUNCTION.keys()) + [\'all\'])\n  args = parser.parse_args()\n\n  if args.figure == \'all\':\n    for key, f in PLOT_FUNCTION.items():\n      if key not in [\'ex7.2\', \'7.2\']:\n        f()\n  else:\n    PLOT_FUNCTION[args.figure]()\n\nif __name__ == ""__main__"":\n  main()\n'"
chapter7/nstep_sarsa.py,5,"b'from nstep_td import nStepTD\nimport numpy as np\nimport time\n\nclass nStepSarsa(nStepTD):\n  def __init__(self, env, step_size=0.1, gamma=0.9, n=1, eps=0.1, exp_sar=False): \n    super().__init__(env, None, step_size, gamma, n)\n    self.update_pi = self.update_policy(eps)\n    self.exp_sar = exp_sar\n    self.reset()\n\n  def eps_gre(self, eps):\n    def eps_gre_pol(s):\n      if np.random.random() < eps:\n        return self.random_move(s)\n      q_arr = np.array([self.Q[(s, a)] for a in self.env.moves_d[s]])\n      return self.env.moves_d[s][np.random.choice(np.flatnonzero(q_arr == q_arr.max()))]\n    return eps_gre_pol\n \n  def update_policy(self, eps):\n    def update_on_q_values(s):\n      best_a = self.eps_gre(0)(s)\n      moves = self.env.moves_d[s]\n      soft_min = eps / len(moves)\n      for a in moves:\n        self.pi[(a, s)] = soft_min + (1 - eps) * (a == best_a)\n    return update_on_q_values\n\n  def initialize_pi(self):\n    self.pi = {}\n    for s in self.env.states:\n      self.update_pi(s)\n    return self.pi\n \n  def exp_val(self, s):\n    return sum(self.pi[(a, s)] * self.Q[(s, a)] for a in self.env.moves_d[s])\n\n  def n_step_return_q(self, tau, T):\n    n = self.n\n    max_idx = min(tau + n, T)\n    r_vals = self.get_r_values(self.R, tau + 1, max_idx + 1)\n    G = np.dot(self.gamma_l[:max_idx-tau], r_vals)\n    if tau + n < T:\n      tau_p_n = (tau + n) % (n + 1)\n      s, a = self.S[tau_p_n], self.A[tau_p_n]\n      last_term = self.Q[(s, a)] if not self.exp_sar else self.exp_val(s)\n      G = G + self.gamma_l[n] * last_term\n    return G\n\n  def pol_eval(self, n_ep=100, pi=None):\n    n, R, S, Q, A = self.n, self.R, self.S, self.Q, self.A\n    pi_learned = pi is None\n    self.pi = self.initialize_pi() if pi_learned else pi\n    ep_per_t = [] \n    for ep in range(n_ep):\n      S[0] = self.env.reset()\n      A[0] = self.sample_action(self.pi, S[0])\n      T = np.inf\n      t = 0\n      while True:\n        ep_per_t.append(ep)\n        tm, tp1m = t % (n + 1), (t + 1) % (n + 1)\n        if t < T:\n          S[tp1m], R[tp1m], d, _ = self.env.step(A[tm])\n          if d:\n            T = t + 1\n          else:\n            A[tp1m] = self.sample_action(self.pi, S[tp1m])\n        tau = t - n + 1\n        if tau >= 0:\n          G = self.n_step_return_q(tau, T)\n          taum = tau % (n + 1)\n          s, a = S[taum], A[taum]\n          Q[(s, a)] += self.step_size * (G - Q[(s, a)])\n          if pi_learned:\n            self.update_pi(s)\n        if tau == (T - 1):\n          break\n        t += 1\n    return ep_per_t\n\n\n  def get_v(self):\n    return {s: max(self.Q[(s, a)] for a in self.env.moves_d[s]) for s in self.env.states} \n\n  def reset(self):\n    super().reset()\n    self.Q = {(s, a): 0 for s in self.env.states for a in self.env.moves_d[s]}\n    self.A = [None for _ in range(self.n + 1)]\n'"
chapter7/nstep_td.py,4,"b'import numpy as np\nimport copy\n\nclass TD:\n  def __init__(self, env, V_init=None, step_size=None, gamma=0.9, n=1):\n    self.env = env\n    self.gamma = gamma\n    self.V_init = V_init\n    self.step_size = step_size\n    self.n = n\n    self.reset()\n\n  def sample_action(self, pi, s):\n    pi_dist = [pi[(a, s)] for a in self.env.moves_d[s]]\n    return self.env.moves_d[s][np.random.choice(np.arange(len(self.env.moves_d[s])), p=pi_dist)]\n  \n  def seed(self, seed):\n    self.env.seed(seed)\n    np.random.seed(seed)\n \n  def get_value_list(self):\n    return [val for key,val in self.V.items()]\n  \n  def reset(self):\n    self.V = {s: 0 for s in self.env.states} if self.V_init is None else copy.deepcopy(self.V_init)\n\nclass nStepTD(TD):\n  def __init__(self, env, V_init=None, step_size=None, gamma=0.9, n=1, ex_7_2=False):\n    super().__init__(env, V_init, step_size, gamma, n)\n    self.return_f = self.n_step_return if not ex_7_2 else self.td_err_sum\n    self.reset()\n\n  def get_r_values(self, R, i, j):\n    n = self.n\n    orig_mod = mod_idx = i % (n + 1)\n    goal = j % (n + 1)\n    R_vals = []\n    while True:\n      R_vals.append(R[mod_idx])\n      mod_idx = (mod_idx + 1) % (n + 1)\n      if mod_idx == goal:\n        return R_vals\n\n  def pol_eval(self, pi, n_ep):\n    n, R, S, V = self.n, self.R, self.S, self.V\n    for ep in range(n_ep):\n      S[0] = self.env.reset()\n      T = np.inf\n      t = 0\n      while True:\n        if t < T:\n          S[(t + 1) % (n + 1)], R[(t + 1) % (n + 1)], d, _ = self.env.step(self.sample_action(pi, S[t % (n + 1)]))\n          if d:\n            T = t + 1\n        tau = t - n + 1\n        if tau >= 0:\n          s = S[tau % (n + 1)]\n          G = self.return_f(tau, T)\n          V[s] += self.step_size * (G - V[s])\n        if tau == (T - 1):\n          break\n        t += 1\n\n  def n_step_return(self, tau, T):\n    n = self.n\n    max_idx = min(tau + n, T)\n    r_vals = self.get_r_values(self.R, tau + 1, max_idx + 1)\n    G = np.dot(self.gamma_l[:max_idx-tau], r_vals)\n    if tau + n < T:\n      G = G + self.gamma_l[n] * self.V[self.S[(tau + n) % (n + 1)]]\n    return G\n\n  def td_error(self, t):\n    n = self.n\n    s, s_p = self.S[t % (n + 1)], self.S[(t + 1) % (n + 1)]\n    r = self.R[(t + 1) % (n + 1)]\n    return r + self.gamma * self.V[s_p] - self.V[s]\n\n  def td_err_sum(self, tau, T):\n    max_idx = min(tau + self.n, T)\n    return sum(self.td_error(j) for j in range(tau, max_idx))\n\n  def reset(self):\n    self.gamma_l = [self.gamma ** k for k in range(self.n + 1)]\n    self.S = [None for _ in range(self.n + 1)]\n    self.R = [None for _ in range(self.n + 1)]\n    super().reset() \n'"
chapter7/nstep_tree_backup.py,1,"b'from off_pol_nstep_sarsa import OffPolnStepSarsa\nimport numpy as np\n\nclass nStepTreeBackup(OffPolnStepSarsa):\n  def __init__(self, env, step_size, gamma, n, b=None):\n    super().__init__(env, b, step_size, gamma, n)\n    self.reset()\n\n  def exp_Q(self, tm, a_rem=None):\n    s, a = self.S[tm], self.A[tm]\n    to_remove = 0 if a_rem is None else self.pi[(a, s)]\n    return self.exp_val(s) - to_remove\n\n  def pol_eval(self, n_ep=100, pi=None):\n    n, g, R, S, Q, A = self.n, self.gamma, self.R, self.S, self.Q, self.A\n    pi_learned = pi is None\n    avg = None\n    self.pi = self.initialize_pi() if pi_learned else pi\n    avg_length_l = []\n    for ep in range(n_ep):\n      ep_len = self.get_nb_timesteps(self.pi, 10)\n      avg = ep_len if avg is None else 0.2 * ep_len + 0.8 * avg\n      avg_length_l.append(avg)\n      print(f""nb_timesteps after {ep} train episodes ~= {avg} timesteps"")\n      S[0] = self.env.reset()\n      A[0] = self.sample_action(self.b, S[0])\n      T = np.inf\n      t = 0\n      while True:\n        tm, tp1m = t % (n + 1), (t + 1) % (n + 1)\n        if t < T:\n          S[tp1m], R[tp1m], d, _ = self.env.step(A[tm])\n          if d:\n            T = t + 1\n          else:\n            A[tp1m] = self.sample_action(self.b, S[tp1m])\n        tau = t - n + 1\n        if tau >= 0:\n          tmax = min(t + 1, T)\n          tmaxm = tmax % (n + 1)\n          G = R[tmaxm] + (tmax == T) * g * (self.exp_Q(tmaxm))\n          for k in range(tmax - 1, tau, -1):\n            km = k % (n + 1)\n            s, a, r = S[km], A[km], R[km]\n            G = r + g * (self.exp_Q(km, a) + self.pi[(a, s)] * G)\n          s, a = S[tau % (n + 1)], A[tau % (n + 1)]\n          Q[(s, a)] += self.step_size * (G - Q[(s, a)])\n          if pi_learned:\n            self.update_pi(s)\n        if tau == (T - 1):\n          break\n        t += 1\n    return avg_length_l\n\n  def reset(self):\n    super().reset()\n'"
chapter7/off_pol_nstep_exp_sarsa.py,2,"b'from off_pol_nstep_sarsa import OffPolnStepSarsa\nimport numpy as np\n\nclass OffPolnStepExpSarsa(OffPolnStepSarsa):\n  def __init__(self, env, b=None, step_size=None, gamma=0.9, n=1, eps=0.1):\n    super().__init__(env, b, step_size, gamma, n, eps)\n    self.exp_sar = True\n\n  def nstep_return_is(self, ro, tau, T):\n    return self.n_step_return_q(tau, T)\n    print(""not here"")\n    n, S, A, Q, R, g = self.n, self.S, self.A, self.Q, self.R, self.gamma\n    h = min(tau + n, T)\n    hm = h % (n + 1)\n    G = Q[(S[hm], A[hm])]\n    t = h - 1\n    if h >= T:\n      G = R[T % (n + 1)]\n      t -= 1\n    while t >= tau:\n      is_r = ro[(t + 1) % n]\n      tp1 = (t + 1) % (n + 1)\n      G = R[tp1] + g * is_r * (G - Q[(S[tp1], A[tp1])]) + g * self.exp_val(S[tp1])\n      t -= 1\n    return G\n\n  def pol_eval(self, n_ep_train=100, pi=None):\n    pi_learned = pi is None\n    n, R, S, A, Q = self.n, self.R, self.S, self.A, self.Q\n    avg = None\n    self.pi = self.initialize_pi() if pi_learned else pi\n    avg_length_l = []\n    for ep in range(n_ep_train):\n      ro = np.ones(n)\n      ep_len = self.get_nb_timesteps(self.pi, 1)\n      avg = ep_len if avg is None else 0.01 * ep_len + 0.99 * avg\n      avg_length_l.append(avg)\n      print(f""nb_timesteps after {ep} train episodes ~= {avg} timesteps"")\n      S[0] = self.env.reset()\n      A[0] = self.sample_action(self.b, S[0])\n      T = np.inf\n      t = 0\n      while True:\n        tm, tp1m = t % (n + 1), (t + 1) % (n + 1)\n        if t < T:\n          ro[t % n] = self.pi[(A[tm], S[tm])] / self.b[(A[tm], S[tm])]\n          S[tp1m], R[tp1m], d, _ = self.env.step(A[tm])\n          if d:\n            T = t + 1\n          else:\n            A[tp1m] = self.sample_action(self.b, S[tp1m])\n        tau = t - n + 1\n        if tau >= 0:\n          taum = tau % (n + 1)\n          G = self.nstep_return_is(ro, tau, T)\n          Q[(S[taum], A[taum])] += self.step_size * (G - Q[(S[taum], A[taum])])\n          if pi_learned:\n            self.update_pi(S[taum])\n        if tau == (T - 1):\n          break\n        t += 1\n    return avg_length_l\n'"
chapter7/off_pol_nstep_qsigma.py,2,"b'from off_pol_nstep_sarsa import OffPolnStepSarsa\nimport numpy as np\n\nclass OffPolnStepQSigma(OffPolnStepSarsa):\n  def __init__(self, env, sigma_f, step_size, gamma, n, b=None):\n    super().__init__(env, b, step_size, gamma, n)\n    self.sigma_f = lambda t, n: sigma_f if isinstance(sigma_f, float) else sigma_f\n    self.reset()\n\n  def pol_eval(self, n_ep=100, pi=None):\n    n, g, R, S, Q, A = self.n, self.gamma, self.R, self.S, self.Q, self.A\n    ro, sig = [np.ones(n + 1) for _ in range(2)]\n    pi_learned = pi is None\n    avg = None\n    self.pi = self.initialize_pi() if pi_learned else pi\n    avg_length_l = []\n    for ep in range(n_ep):\n      ep_len = self.get_nb_timesteps(self.pi, 10)\n      avg = ep_len if avg is None else 0.2 * ep_len + 0.8 * avg\n      avg_length_l.append(avg)\n      print(f""nb_timesteps after {ep} train episodes ~= {avg} timesteps"")\n      S[0] = self.env.reset()\n      A[0] = self.sample_action(self.b, S[0])\n      T = np.inf\n      t = 0\n      while True:\n        tm, tp1m = t % (n + 1), (t + 1) % (n + 1)\n        if t < T:\n          S[tp1m], R[tp1m], d, _ = self.env.step(A[tm])\n          if d:\n            T = t + 1\n          else:\n            A[tp1m] = self.sample_action(self.b, S[tp1m])\n            sig[tp1m] = self.sigma_f(t, n)\n            ro[tp1m] = self.pi[(A[tp1m], S[tp1m])] / self.b[(A[tp1m], S[tp1m])]\n        tau = t - n + 1\n        if tau >= 0:\n          tmax = min(t + 1, T)\n          if tmax == (t + 1):\n            G = Q[(S[tp1m], A[tp1m])]\n          for k in range(tmax, tau, -1):\n            km = k % (n + 1)\n            if k == T:\n              G = R[km]\n            else:\n              s, a, r = S[km], A[km], R[km]\n              expv, sigk, rok = self.exp_val(S[km]), sig[km], ro[km]\n              pi, q = self.pi[(a, s)], Q[(s, a)]\n              G = r + g * (sigk * rok + (1 - sigk) * pi) * (G - q) + g * expv\n          s, a = S[tau % (n + 1)], A[tau % (n + 1)]\n          Q[(s, a)] += self.step_size * (G - Q[(s, a)])\n          if pi_learned:\n            self.update_pi(s)\n        if tau == (T - 1):\n          break\n        t += 1\n    return avg_length_l\n\n  def reset(self):\n    super().reset()\n'"
chapter7/off_pol_nstep_sarsa.py,2,"b'from nstep_sarsa import nStepSarsa\nimport numpy as np\n\nclass OffPolnStepSarsa(nStepSarsa):\n  def __init__(self, env, b=None, step_size=0.1, gamma=0.9, n=1, eps=0.1, exp_sar=False):\n    super().__init__(env, step_size, gamma, n, eps, exp_sar)\n    self.b = self.uniform_pol() if b is None else b\n    assert(self.is_soft(self.b))\n    assert(0 < self.step_size <= 1)\n\n  def is_soft(self, pol):\n    for s in self.env.states:\n      for a in self.env.moves_d[s]:\n        if pol[(a, s)] == 0:\n          return False\n    return True \n\n  def uniform_pol(self):\n    return {(a, s): 1 / len(self.env.moves_d[s]) for s in self.env.states for a in self.env.moves_d[s]}\n\n  def get_nb_timesteps(self, pi, n_ep=1, max_steps=1000, debug=False):\n    count = 0\n    for ep in range(n_ep): \n      s = self.env.reset()\n      while True and count <= max_steps:\n        if debug:\n          import time\n          print(self.env)\n          time.sleep(0.01)\n        s, _, d, _ = self.env.step(self.sample_action(pi, s))\n        count += 1\n        if d:\n          break\n    return count / n_ep\n  \n  def pol_eval(self, n_ep_train=100, pi=None):\n    pi_learned = pi is None\n    n, R, S, Q, A = self.n, self.R, self.S, self.Q, self.A\n    avg = None\n    self.pi = self.initialize_pi() if pi_learned else pi\n    avg_length_l = []\n    for ep in range(n_ep_train):\n      ro = np.ones(n - 1)\n      ep_len = self.get_nb_timesteps(self.pi, 10)\n      avg = ep_len if avg is None else 0.2 * ep_len + 0.8 * avg\n      avg_length_l.append(avg)\n      print(f""nb_timesteps after {ep} train episodes ~= {avg} timesteps"")\n      S[0] = self.env.reset()\n      A[0] = self.sample_action(self.b, S[0])\n      T = np.inf\n      t = 0\n      while True:\n        tm, tp1m = t % (n + 1), (t + 1) % (n + 1)\n        if t < T:\n          S[tp1m], R[tp1m], d, _ = self.env.step(A[tm])\n          if d:\n            T = t + 1\n          else:\n            A[tp1m] = self.sample_action(self.b, S[tp1m])\n        tau = t - n + 1\n        if tau >= 0:\n          max_idx = min(tau + n - 1, T - 1) % (n - 1)\n          is_ratio = ro[(tau + 1) % (n-1):].prod() + ro[:max_idx + 1].prod()\n          G = self.n_step_return_q(tau, T)\n          taum = tau % (n + 1)\n          s, a = S[taum], A[taum]\n          Q[(s, a)] += self.step_size * is_ratio * (G - Q[(s, a)])\n          if pi_learned:\n            self.update_pi(s)\n        ro[(t + 1) % (n - 1)] = self.pi[(A[tp1m], S[tp1m])] / self.b[(A[tp1m], S[tp1m])]\n        if tau == (T - 1):\n          break\n        t += 1\n    return avg_length_l\n'"
chapter7/off_pol_nstep_td.py,2,"b'from off_pol_nstep_sarsa import OffPolnStepSarsa\nimport numpy as np\n\nclass OffPolnStepTD(OffPolnStepSarsa):\n  def __init__(self, env, b=None, step_size=None, gamma=0.9, n=1, eps=0.1, simple=False):\n    super().__init__(env, b, step_size, gamma, n, eps)\n    self.update_fn = self.simple_update if simple else self.off_pol_update\n    self.reset()\n\n  def off_pol_update(self, s, ro, tau, T):\n    G = self.nstep_return_is(ro, tau, T)\n    self.V[s] += self.step_size * (G - self.V[s])\n\n  def simple_ro(self, ro, t, h, T):\n    prod, A, S = 1, self.A, self.S\n    idx = min(h, T - 1)\n    while idx >= t:\n      idxm = idx % (self.n)\n      prod *= ro[idxm]\n      idx -= 1\n    return prod\n\n  def simple_update(self, s, ro, tau, T):\n    G = self.n_step_return(tau, T)\n    is_r = self.simple_ro(ro, tau, tau + self.n - 1, T)\n    self.V[s] += self.step_size * is_r * (G - self.V[s])\n\n  def nstep_return_is(self, ro, tau, T):\n    n, S, V, R, g = self.n, self.S, self.V, self.R, self.gamma\n    h = min(tau + n, T)\n    G = V[S[h % (n + 1)]] if tau + n < T else 0\n    t = h - 1\n    while t >= tau:\n      tm, tp1 = t % n, (t + 1) % (n + 1)\n      is_r = ro[tm]\n      G = is_r * (R[tp1] + g * G) + (1 - is_r) * self.V[S[tm]]\n      t -= 1\n    return G\n\n  def pol_eval(self, n_ep_train=100, pi=None):\n    pi_learned = pi is None\n    n, R, S, V = self.n, self.R, self.S, self.V\n    avg = None\n    self.pi = self.initialize_pi() if pi_learned else pi\n    for ep in range(n_ep_train):\n      ro = np.ones(n)\n      S[0] = self.env.reset()\n      T = np.inf\n      t = 0\n      while True:\n        tm, tp1m = t % (n + 1), (t + 1) % (n + 1)\n        if t < T:\n          a = self.sample_action(self.b, S[tm])\n          ro[t % n] = self.pi[(a, S[tm])] / self.b[(a, S[tm])]\n          S[tp1m], R[tp1m], d, _ = self.env.step(a)\n          if d:\n            T = t + 1\n        tau = t - n + 1\n        if tau >= 0:\n          s = S[tau % (n + 1)]\n          self.update_fn(s, ro, tau, T)\n          if pi_learned:\n            self.update_pi(s)\n        if tau == (T - 1):\n          break\n        t += 1\n    return self.get_value_list()\n\n  def reset(self):\n    super().reset()\n'"
chapter7/randomwalk.py,4,"b'import numpy as np\n\nN_STATES = 19\nEMPTY_MOVE = 0\nP_LEFT = 0.5\nP_LEFT_L = [0.9, 0.1]\nR_STEP = 0\nABSORBING_STATE = N_STATES\nLEFT = 0\nRIGHT = 1\nR_RIGHT = 1\n\nclass RandomWalk:\n  def __init__(self, n_states=None, r_l=-1):\n    self.n_states = N_STATES if n_states is None else n_states\n    self.absorbing_state = self.n_states\n    self.get_states()\n    self.get_moves()\n    self.get_moves_d()\n    self.reset()\n    self.r_l = r_l\n    self.r_r = R_RIGHT\n    print(self.n_states)\n\n  def get_moves(self):\n    self.moves = [EMPTY_MOVE]\n\n  def get_moves_d(self):\n    self.moves_d = {s: self.moves for s in self.states}\n\n  def get_states(self):\n    self.states = list(range(self.n_states)) + [self.absorbing_state]\n\n  def sample_shift(self):\n    return np.sign(np.random.random() - P_LEFT)\n\n  def step(self, action):\n    if self.state == self.n_states:\n      return self.state, R_STEP, True, {}\n    shift = self.sample_shift()\n    new_state = self.state + shift\n    if not (0 <= new_state < self.n_states):\n      r = self.r_r if (new_state == self.n_states) else self.r_l\n      return self.n_states, r, True, {}\n    self.state = new_state\n    return self.state, R_STEP, False, {}\n\n  def force_state(self, state):\n    self.state = state\n\n  def reset(self):\n    self.state = self.n_states // 2\n    return self.state\n\n  def seed(self, seed):\n    np.random.seed(seed)\n\n  def __str__(self):\n    return self.state\n\n\nclass NotSoRandomWalk:\n  def __init__(self, n_states=None, r_l=-1):\n    self.n_states = N_STATES if n_states is None else n_states\n    self.absorbing_state = self.n_states\n    self.get_states()\n    self.get_moves()\n    self.get_moves_d()\n    self.reset()\n    self.r_l = r_l\n    self.r_r = R_RIGHT\n\n  def get_moves(self):\n    self.moves = [LEFT, RIGHT]\n\n  def get_moves_d(self):\n    self.moves_d = {s: self.moves for s in self.states}\n\n  def get_states(self):\n    self.states = list(range(self.n_states)) + [self.absorbing_state]\n\n  def sample_shift(self, action):\n    return np.sign(np.random.random() - P_LEFT_L[action])\n\n  def step(self, action):\n    new_state = self.state + self.sample_shift(action) \n    if not (0 <= new_state < self.n_states):\n      r = self.r_r if (new_state == self.n_states) else self.r_l\n      return self.n_states, r, True, {}\n    self.state = new_state\n    return self.state, R_STEP, False, {}\n\n  def reset(self):\n    self.state = self.n_states // 2\n    return self.state\n\n  def seed(self, seed):\n    np.random.seed(seed)\n\n  def __str__(self):\n    return self.state\n'"
chapter7/windy_gridworld.py,2,"b'import numpy as np\n\nINIT_POS = (3, 0)\nGOAL_POS = (3, 7)\nWIND_ARR = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\nGRID_SHAPE = (7, 10)\nR_STEP = -1\nKEY_ACTION_DICT = {\n  \'a\': (-1, 0),\n  \'z\': (-1, 1),\n  \'e\': (-1, -1),\n  \'q\': (0, -1),\n  \'s\': (0, 0),\n  \'d\': (0, 1),\n  \'w\': (1, -1),\n  \'x\': (1, 0),\n  \'c\': (1, 1),\n}\nPOS_CHAR_DICT = {\n  GOAL_POS: \'G\',\n  INIT_POS: \'S\',\n}\nAGENT_KEY = \'A\'\n\nclass Position:\n  def __init__(self, x, y):\n    self.x = x\n    self.y = y\n  \n  def in_bounds(self, index, axis):\n    return max(0, min(index, GRID_SHAPE[axis] - 1))\n  \n  def get_wind(self, stoch):\n    return WIND_ARR[self.y] + stoch * np.random.randint(-1, 2)\n\n  def next_state(self, stoch, action): \n    wind = self.get_wind(stoch)\n    return Position(self.in_bounds(self.x + action[0] - wind, 0), \n                    self.in_bounds(self.y + action[1], 1))\n\n  def __eq__(self, other_pos):\n    return self.x == other_pos.x and self.y == other_pos.y\n\n  def __add__(self, other_pos):\n    return Position(self.x + other_pos.x, self.y + other_pos.y)\n\n  def __hash__(self):\n    return hash((self.x, self.y))\n\n  def __str__(self):\n    return f""({self.x}, {self.y})""\n\nclass WindyGridworld:\n  def __init__(self, diags=False, stay=False, stoch=False):\n    self.diags = diags\n    self.stay = stay\n    self.stoch = stoch\n    self.get_states()\n    self.get_moves()\n    self.get_moves_dict()\n    self.get_keys()\n\n  def seed(self, seed):\n    np.random.seed(seed)\n\n  def get_moves(self):\n    self.moves = [(x, y) for x in [-1, 0, 1] for y in [-1, 0, 1] if (abs(x) + abs(y)) == 1]\n    if self.diags:\n      self.moves += [(x, y) for x in [-1, 1] for y in [-1, 1]]\n    if self.stay:\n      self.moves += [(0, 0)]\n\n  def get_states(self):\n    self.states = [Position(x, y) for x in range(GRID_SHAPE[0]) for y in range(GRID_SHAPE[1])]\n\n  def get_moves_dict(self):\n    moves_d = {}\n    for s in self.states:\n      moves_d[s] = []\n      for a in self.moves:\n        if s.next_state(self.stoch, a) in self.states:\n          moves_d[s].append(a)\n    self.moves_d = moves_d\n\n  def step(self, action):\n    self.state = self.state.next_state(self.stoch, action)\n    return self.state, R_STEP, self.state == Position(*GOAL_POS), {}\n\n  def reset(self):\n    self.state = Position(*INIT_POS)\n    return self.state\n\n  def get_keys(self):\n    self.keys = KEY_ACTION_DICT.keys()\n\n  def step_via_key(self, key):\n    return self.step(KEY_ACTION_DICT[key])\n\n  def __str__(self):\n    x_ag, y_ag = self.state.x, self.state.y\n    s = \'\'\n    for wind in WIND_ARR:\n      s += str(wind)\n    s += \'\\n\'\n    for x in range(GRID_SHAPE[0]):\n      for y in range(GRID_SHAPE[1]):\n        if (x, y) == (x_ag, y_ag):\n          s += AGENT_KEY\n        elif (x, y) in POS_CHAR_DICT.keys():\n          s += POS_CHAR_DICT[(x, y)]\n        else:\n          s += \'.\'\n      s += \'\\n\'\n    return s\n'"
chapter8/dyna_maze.py,0,"b'import numpy as np\n\nINIT_POS = (2, 0)\nGOAL_POS_L = [(0, 8)]\nGRID_SHAPE = (6, 9)\nKEY_ACTION_DICT = {\n  \'z\': (-1, 0),\n  \'q\': (0, -1),\n  \'d\': (0, 1),\n  \'s\': (1, 0),\n}\nAGENT_KEY = \'A\'\nGOAL_KEY = \'G\'\nINIT_KEY = \'S\'\nWALL_KEY = \'W\'\nWALLS = [(1, 2), (2, 2), (3, 2), (4, 5), (0, 7), (1, 7), (2, 7)]\n\nclass Position:\n  def __init__(self, x, y, is_goal):\n    self.x = x\n    self.y = y\n    self.is_goal = is_goal\n\n  def in_bounds(self, grid_shape, index, axis):\n    return max(0, min(index, grid_shape[axis] - 1))\n \n  def move(self, action, grid_shape, goal_pos_l):\n    x_p, y_p = self.in_bounds(grid_shape, self.x + action[0], 0), self.in_bounds(grid_shape, self.y + action[1], 1)\n    return self if self.is_goal else Position(x_p, y_p, (x_p, y_p) in goal_pos_l)\n\n  def __eq__(self, other_pos):\n    if isinstance(other_pos, tuple):\n      return self.x == other_pos[0] and self.y == other_pos[1]\n    return self.x == other_pos.x and self.y == other_pos.y\n\n  def __hash__(self):\n    return hash((self.x, self.y))\n\n  def __str__(self):\n    return f""({self.x}, {self.y})""\n\nclass DynaMaze:\n  def __init__(self, init_pos=INIT_POS, goal_pos_l=GOAL_POS_L, grid_shape=GRID_SHAPE, walls1=WALLS, walls2=WALLS):\n    self.init_pos = init_pos\n    self.goal_pos_l = goal_pos_l\n    self.grid_shape = grid_shape\n    self.get_states()\n    self.get_moves()\n    self.get_moves_dict()\n    self.get_keys()\n    self.pos_char_dict = {pos: INIT_KEY if pos == self.init_pos else GOAL_KEY for pos in [self.init_pos] + self.goal_pos_l}\n    self.walls = walls1\n    self.walls1 = walls1\n    self.walls2 = walls2\n\n  def switch_walls(self):\n    if self.walls == self.walls1:\n      self.walls = self.walls2\n    else:\n      self.walls = self.walls1\n\n  def get_moves(self):\n    self.moves = [(x, y) for x in [-1, 0, 1] for y in [-1, 0, 1] if (abs(x) + abs(y)) == 1]\n\n  def get_states(self):\n    self.states = [Position(x, y, (x, y) in self.goal_pos_l) for x in range(self.grid_shape[0]) for y in range(self.grid_shape[1])]\n\n  def get_moves_dict(self):\n    self.moves_d = {s: self.moves for s in self.states}\n\n  def step(self, action):\n    s_curr = self.state\n    s_p = s_curr.move(action, self.grid_shape, self.goal_pos_l)\n    s_next = s_curr if (s_p.x, s_p.y) in self.walls else s_p\n    done = s_next.is_goal\n    r = float(done and not s_curr.is_goal)\n    self.state = s_next\n    return s_next, r, done, {}\n\n  def get_keys(self):\n    self.keys = KEY_ACTION_DICT.keys()\n\n  def step_via_key(self, key):\n    return self.step(KEY_ACTION_DICT[key])\n\n  def reset(self):\n    self.state = Position(*self.init_pos, self.init_pos in self.goal_pos_l)\n    return self.state\n\n  def seed(self, seed):\n    pass\n\n  def force_state(self, s):\n    self.state = s\n\n  def __str__(self):\n    x_ag, y_ag = self.state.x, self.state.y\n    s = \'\'\n    s += \'\\n\'\n    for x in range(self.grid_shape[0]):\n      for y in range(self.grid_shape[1]):\n        if (x, y) == (x_ag, y_ag):\n          s += AGENT_KEY\n        elif (x, y) in self.pos_char_dict.keys():\n          s += self.pos_char_dict[(x, y)]\n        elif (x, y) in self.walls:\n          s += WALL_KEY\n        else:\n          s += \'.\'\n      s += \'\\n\'\n    return s\n'"
chapter8/dyna_maze_part.py,0,"b'from dyna_maze import DynaMaze, INIT_POS, GOAL_POS_L, GRID_SHAPE, WALLS\n\nclass DynaMazePartitioned(DynaMaze):\n  def __init__(self, n_part, init_pos=INIT_POS, goal_pos_l=GOAL_POS_L, grid_shape=GRID_SHAPE, walls1=WALLS, walls2=WALLS):\n    self.n_part = n_part\n    self.row_part = (n_part + 1) // 2\n    self.col_part = n_part // 2\n    self.row_mul = 2 ** self.row_part\n    self.col_mul = 2 ** self.col_part\n    init_pos = self.expand(init_pos)\n    grid_shape = self.expand(grid_shape)\n    walls1, walls2, goal_pos_l  = [self.part_l(l) for l in [walls1, walls2, goal_pos_l]]\n    super().__init__(init_pos, goal_pos_l, grid_shape, walls1, walls2)\n\n  def expand(self, pos):\n    return pos[0] * self.row_mul, pos[1] * self.col_mul\n\n  def part(self, pos):\n    x_0, y_0 = self.expand(pos)\n    return [(_x + x_0, _y + y_0) for _x in range(self.row_mul) for _y in range(self.col_mul)]\n\n  def part_l(self, pos_list):\n    return sum([self.part(pos) for pos in pos_list], [])\n'"
chapter8/dyna_q.py,6,"b'import numpy as np\nfrom tabular_q import TabularQ\nfrom utils import sample\nfrom models import Model\nimport time\n\nclass DynaQ(TabularQ):\n  def __init__(self, env, alpha, gamma, eps):\n    super().__init__(Model(), alpha, gamma)\n    self.env = env\n    self.eps = eps\n    self.reset()\n\n  def best_actions(self, s):\n    q_max, a_max_l = -np.inf, []\n    for a in self.env.moves_d[s]:\n      q_val = self.Q[(s, a)]\n      if q_val >= q_max:\n        a_max_l = [a] if q_val > q_max else a_max_l + [a]\n        q_max = q_val\n    return a_max_l\n\n  def gre(self, s):\n    q_arr = np.array([self.Q[(s, a)] for a in self.env.moves_d[s]])\n    return self.env.moves_d[s][np.random.choice(np.flatnonzero(q_arr == q_arr.max()))]\n    \n\n  def eps_gre(self, s):\n    if np.random.random() < self.eps:\n      return sample(self.env.moves_d[s])\n    return self.gre(s)\n\n  def q_learning_update(self, s, a, r, s_p):\n    max_val = -np.inf\n    Q_max = max(self.Q[(s_p, a_p)] for a_p in self.env.moves_d[s])\n    self.Q[(s, a)] += self.a * (r + self.g * Q_max - self.Q[(s, a)])\n\n  def Q_reset(self):\n    for s in self.env.states:\n      for a in self.env.moves_d[s]:\n        self.Q[(s, a)] = 0\n\n  def tabular_dyna_q(self, n_eps, n_plan_steps=1):\n    ep_len_l = []\n    for ep in range(n_eps):\n      s = self.env.reset() \n      n_steps = 0\n      while True:\n        n_steps += 1\n        a = self.eps_gre(s)\n        s_p, r, d, _ = self.env.step(a)\n        self.q_learning_update(s, a, r, s_p)\n        self.model.add_transition(s, a, r, s_p)\n        self.rand_sam_one_step_pla(n_plan_steps)\n        s = s_p\n        if d:\n          ep_len_l.append(n_steps)\n          break\n    return ep_len_l\n\n  def tabular_dyna_q_step(self, n_steps=1, n_plan_steps=1):\n    cum_rew_l = []\n    cum_rew = 0\n    s = self.env.reset()\n    for step in range(n_steps):\n      a = self.eps_gre(s)\n      s_p, r, d, _ = self.env.step(a)\n      self.q_learning_update(s, a, r, s_p)\n      self.model.add_transition_cheat(s, a, r, s_p, self.env.moves_d[s])\n      self.rand_sam_one_step_pla(n_plan_steps)\n      s = self.env.reset() if d else s_p\n      cum_rew += r\n      cum_rew_l.append(cum_rew)\n    return cum_rew_l\n\n  def test_n_steps(self, max_steps, tol=0.1): \n    s = self.env.reset()\n    n_steps = 0\n    d = False\n    while n_steps < ((1 + tol) * max_steps) and not d:\n      s, _, d, _ = self.env.step(self.gre(s))\n      n_steps += 1\n    return d\n\n  def updates_until_optimal(self, n_steps_opt, n_plan_steps, tol=0.1):\n    n_updates = 0\n    s = self.env.reset()\n    while True:\n      a = self.eps_gre(s)\n      s_p, r, d, _ = self.env.step(a)\n      self.q_learning_update(s, a, r, s_p)\n      self.model.add_transition(s, a, r, s_p)\n      self.rand_sam_one_step_pla(n_plan_steps)\n      s = self.env.reset() if d else s_p\n      n_updates += n_plan_steps + 1\n      if d and self.test_n_steps(n_steps_opt, tol):\n        return n_updates\n\n  def seed(self, seed):\n    self.env.seed(seed)\n    np.random.seed(seed)\n\n  def reset(self): \n    self.model.reset()\n    self.Q_reset()\n'"
chapter8/dyna_q_plus.py,1,"b'from dyna_q import DynaQ\nfrom utils import sample\nimport numpy as np\nimport time\n\nclass DynaQPlus(DynaQ):\n  def __init__(self, env, alpha, gamma, eps, k):\n    super().__init__(env, alpha, gamma, eps)\n    self.k = k\n    self.reset()\n\n  def expl_term(self, s, a):\n    return self.k * np.sqrt(self.trans_count[(s, a)])\n\n  def plan_dyna_q_plus(self, n_updates=1, expl_term=True):\n    s_list = list(self.model.states)\n    a_dict = {s: list(self.env.moves_d[s]) for s in s_list}\n    for _ in range(n_updates):\n      s = sample(s_list)\n      a = sample(a_dict[s]) \n      s_p, r = self.model.sample_s_r(s, a) if a in self.model.moves_d[s] else (s, 0)\n      if expl_term:\n        r += self.expl_term(s, a)\n      Q_max = max(self.Q[(s_p, a_p)] for a_p in a_dict[s])\n      self.Q[(s, a)] += self.a * (r + self.g * Q_max - self.Q[(s, a)])\n\n  def upd_count(self, s_0, a_0):\n    for s in self.env.states:\n      for a in self.env.moves_d[s]:\n        self.trans_count[(s, a)] += 1\n    self.trans_count[(s_0, a_0)] = 0\n\n  def tabular_dyna_q_step(self, n_steps=1, n_plan_steps=1):\n    cum_rew_l = []\n    cum_rew = 0\n    s = self.env.reset()\n    for step in range(n_steps):\n      a = self.eps_gre(s)\n      self.upd_count(s, a)\n      s_p, r, d, _ = self.env.step(a)\n      self.q_learning_update(s, a, r, s_p)\n      self.model.add_transition(s, a, r, s_p)\n      self.plan_dyna_q_plus(n_plan_steps)\n      s = self.env.reset() if d else s_p\n      cum_rew += r\n      cum_rew_l.append(cum_rew)\n    return cum_rew_l\n\n  def add_expl_q_vals(self, s, remove=False):\n    sign = -1 if remove else 1\n    for a in self.env.moves_d[s]:\n      self.Q[(s, a)] += sign * self.expl_term(s, a)\n\n  def ex_8_4(self, n_steps=1, n_plan_steps=1):\n    cum_rew_l = []\n    cum_rew = 0\n    s = self.env.reset()\n    for step in range(n_steps):\n      self.add_expl_q_vals(s, remove=False)\n      a = self.eps_gre(s)\n      self.add_expl_q_vals(s, remove=True)\n      self.upd_count(s, a)\n      s_p, r, d, _ = self.env.step(a)\n      self.q_learning_update(s, a, r, s_p)\n      self.model.add_transition(s, a, r, s_p)\n      self.plan_dyna_q_plus(n_plan_steps, expl_term=False)\n      s = self.env.reset() if d else s_p\n      cum_rew += r\n      cum_rew_l.append(cum_rew)\n    return cum_rew_l\n\n  def print_trans_count(self):\n    for s in self.env.states:\n      print(s)\n      for a in self.env.moves_d[s]:\n        print(f""->{a}: count={self.trans_count[(s, a)]}, Q={self.Q[(s, a)]}"")\n\n  def reset(self):\n    super().reset()\n    self.trans_count = {(s, a): 0 for s in self.env.states for a in self.env.moves_d[s]}\n'"
chapter8/figures.py,16,"b'import argparse\nimport matplotlib.pyplot as plt\nfrom dyna_q import DynaQ\nfrom dyna_q_plus import DynaQPlus\nfrom dyna_maze import DynaMaze\nfrom dyna_maze_part import DynaMazePartitioned\nfrom models import FullModel\nfrom tabular_q import TabularQ\nfrom utils import sample, to_arr\nimport seaborn as sns\nimport numpy as np\nfrom nstep_sarsa import nStepSarsa\nfrom prior_sweep import PrioritizedSweeping\nfrom traj_sampling import TrajectorySampling\nfrom task import Task, START_STATE\nimport time\n\nSEC_8_1_ALP = 0.001\nSEC_8_1_N_STEPS = int(1e6)\nDYNA_MAZE_GAMMA = 0.95\nFIG_8_2_ALP = 0.1\nFIG_8_2_N_EP = 50\nFIG_8_2_EPS = 0.1\nFIG_8_2_PLAN_STEPS = [0, 5, 50]\nFIG_8_2_C_DIC = {0: \'b\', 5: \'g\', 50: \'r\'}\nFIG_8_2_N_RUNS = 30\nFIG_8_3_PLAN_STEPS = [0, 50]\nFIG_8_3_HEAT_LAB = {(0, -1): \'left\', (0, 1): \'right\', (-1, 0): \'up\', (1, 0): \'down\'}\nMED_FONT = 13\nBIG_FONT = 15\nEX_8_1_N_LIST = [5, 50]\nFIG_8_4_INIT_POS = (5, 3)\nFIG_8_4_GOAL_POS_L = [(0, 8)]\nFIG_8_4_GRID_SHAPE = (6, 9)\nFIG_8_4_WALLS = [(3, y) for y in range(FIG_8_4_GRID_SHAPE[1])]\nFIG_8_4_CHG_T = 1000\nFIG_8_4_FINAL_T = 3000\nFIG_8_4_PLAN_STEPS = 50\nFIG_8_4_N_RUNS = 5\nFIG_8_4_ALP = 0.5\nFIG_8_4_EPS = 0.1\nFIG_8_4_K = 0.001\nFIG_8_5_CHG_T = 3000\nFIG_8_5_FINAL_T = 6000\nEX_8_4_CHG_T = 6000\nEX_8_4_FINAL_T = 12000\nEXAMPLE_8_4_THETA = 1e-4\nEXAMPLE_8_4_N_PART = list(range(int(np.log(6016 // 47) / np.log(2)) + 1))\nEXAMPLE_8_4_N_RUNS = 4\nFIG_8_7_B_L = [2, 10, 100, 1000, 10000]\nFIG_8_7_N_RUNS = 100\nFIG_8_8_S_0 = START_STATE\nFIG_8_8_N_ST_LOWER = 10000\nFIG_8_8_N_UPD_LOWER = 200000\nFIG_8_8_B_L_LOWER = [1]\nFIG_8_8_LOG_FREQ_LOWER = 10000\nFIG_8_8_B_L_UPPER = [1, 3, 10]\nFIG_8_8_N_ST_UPPER = 1000\nFIG_8_8_N_UPD_UPPER = 20000\nFIG_8_8_LOG_FREQ_UPPER = 1000\nFIG_8_8_N_RUNS = 10\n\ndef save_plot(filename, dpi=None):\n  plt.savefig(\'plots/\' + filename + \'.png\', dpi=dpi)\n\ndef show_pol(alg, show_label=True):\n  action_dict = {move_id: alg.env.moves[move_id - 1] for move_id in range(1, len(alg.env.moves) + 1)}\n  heatmap_label = \'0 = all equal\'\n  for move_id, move in action_dict.items():\n    heatmap_label += f\', {move_id}: {FIG_8_3_HEAT_LAB[move]}\'\n  sns.heatmap(to_arr(get_dyna_maze_pol(alg.env, alg.Q)), cbar_kws={\'label\': heatmap_label if show_label else None}, xticklabels=False, yticklabels=False)\n\ndef section_8_1():\n  env = DynaMaze(FIG_8_2_INIT_POS, FIG_8_2_GOAL_POS_L, FIG_8_2_GRID_SHAPE, FIG_8_2_WALL)\n  alg = TabularQ(FullModel(env), SEC_8_1_ALP, DYNA_MAZE_GAMMA)\n  alg.seed(0)\n  alg.rand_sam_one_step_pla(SEC_8_1_N_STEPS, decay=True)\n  V = alg.get_V()\n  plt.title(\'Section 8.1 - tabular Q (1-step random sample, dyna maze)\')\n  sns.heatmap(to_arr(V), cbar_kws={\'label\': \'max(Q(s, a))\'})\n  save_plot(\'section8.1\')\n  plt.show()\n\ndef fig_8_2():\n  fig, ax = plt.subplots()\n  env = DynaMaze()\n  alg = DynaQ(env, FIG_8_2_ALP, DYNA_MAZE_GAMMA, FIG_8_2_EPS)\n  xticks = [2, 10, 20, 30, 40, 50]\n  yticks = [14, 200, 400, 600, 800]\n  ax.set_title(\'Figure 8.2\', fontsize=BIG_FONT)\n  ax.set_xlim([min(xticks), max(xticks)])\n  ax.set_ylim([0, max(yticks)])\n  ax.set_xticks(xticks)\n  ax.set_yticks(yticks)\n  ax.set_xlabel(\'Episodes\', fontsize=BIG_FONT)\n  ax.set_ylabel(\'Steps\\nper\\nepisode\', rotation=0, labelpad=25, fontsize=BIG_FONT)\n  ep_ticks = list(range(2, 51))\n  alg.seed(0)\n  for n_plan_steps in FIG_8_2_PLAN_STEPS:\n    arr_sum = np.zeros(FIG_8_2_N_EP)\n    for _ in range(FIG_8_2_N_RUNS):\n      alg.reset()\n      arr_sum += np.array(alg.tabular_dyna_q(FIG_8_2_N_EP, n_plan_steps))\n    plt.plot(ep_ticks, (arr_sum / FIG_8_2_N_RUNS)[1:], label=f\'{n_plan_steps} planning steps\', color=FIG_8_2_C_DIC[n_plan_steps])\n  plt.legend()\n  fig.set_size_inches(10, 8)\n  save_plot(\'fig8.2\', dpi=100)\n  plt.show()\n\ndef get_dyna_maze_pol(env, Q):\n  pi = {}\n  for s in env.states:\n    if (s, env.moves_d[s][0]) not in Q:\n      pi[s] = 0\n      continue\n    q_arr = np.array([Q[(s, a)] for a in env.moves_d[s]])\n    is_max = q_arr == q_arr.max()\n    arg_max_idx = np.flatnonzero(is_max)\n    if np.all(is_max):\n      pi[s] = 0\n    else:\n      pi[s] = arg_max_idx + 1\n  return pi\n\ndef fig_8_3():\n  fig = plt.figure()\n  fig.suptitle(\'Figure 8.3 - Policies found by Dyna-Q after 2 episodes\', fontsize=BIG_FONT)\n  env = DynaMaze()\n  alg = DynaQ(env, FIG_8_2_ALP, DYNA_MAZE_GAMMA, FIG_8_2_EPS)\n  action_dict = {move_id: env.moves[move_id - 1] for move_id in range(1, len(env.moves) + 1)}\n  heatmap_label = \'0 = all equal\'\n  for move_id, move in action_dict.items():\n    heatmap_label += f\', {move_id}: {FIG_8_3_HEAT_LAB[move]}\'\n  for (i, n_plan_steps) in enumerate(FIG_8_3_PLAN_STEPS): \n    alg.seed(0)\n    alg.reset()\n    ax = fig.add_subplot(f\'12{i + 1}\')\n    ax.set_title(f""with{\'\' if n_plan_steps > 0 else \'out\'} planning (n={n_plan_steps})"")\n    alg.tabular_dyna_q(2, n_plan_steps)\n    sns.heatmap(to_arr(get_dyna_maze_pol(env, alg.Q)), cbar_kws={\'label\': heatmap_label if i == 0 else None}, xticklabels=False, yticklabels=False)\n  fig.set_size_inches(10, 8)\n  save_plot(\'fig8.3\', dpi=100)\n  plt.show()\n\ndef ex_8_1():\n  fig = plt.figure()\n  fig.suptitle(\'Exercise 8.1 - Policies found by n-step sarsa after 2 episodes\', fontsize=BIG_FONT)\n  env = DynaMaze()\n  for (i, n) in enumerate(EX_8_1_N_LIST): \n    alg = nStepSarsa(env, step_size=FIG_8_2_ALP, gamma=DYNA_MAZE_GAMMA, n=n)\n    alg.seed(0)\n    alg.reset()\n    ax = fig.add_subplot(f\'12{i + 1}\')\n    ax.set_title(f\'n={n}\')\n    alg.pol_eval(n_ep=2)\n    show_pol(alg, i==0)\n  fig.set_size_inches(10, 8)\n  save_plot(\'ex8.1\', dpi=100)\n  plt.show()\n\ndef run_dynaq_dynaqp(title, filename, n_runs, xticks, yticks, change_t, final_t, walls1, walls2, plan_steps, alpha, eps, k, ex_8_4=False): \n  \n  # initialization\n  fig, ax = plt.subplots()\n  env = DynaMaze(FIG_8_4_INIT_POS, FIG_8_4_GOAL_POS, FIG_8_4_GRID_SHAPE, walls1, walls2)\n  dyna_q_alg = DynaQ(env, alpha, DYNA_MAZE_GAMMA, eps) if not ex_8_4 else DynaQPlus(env, alpha, DYNA_MAZE_GAMMA, eps, k)\n  dyna_qp_alg = DynaQPlus(env, alpha, DYNA_MAZE_GAMMA, eps, k)\n  for (alg, label) in [(dyna_q_alg, \'Dyna-Q\' + \'+ex8.4\' * ex_8_4), (dyna_qp_alg, \'Dyna-Q+\')]:\n    arr_sum = np.zeros(final_t)\n    alg.seed(0)\n    for run in range(n_runs):\n      alg.reset()\n      opt = alg.ex_8_4 if (ex_8_4 and label[-1] == \'4\') else alg.tabular_dyna_q_step\n      print(f""run {run + 1}/{n_runs}"")\n      cum_rew_l_left = opt(change_t, plan_steps)\n      alg.env.switch_walls()\n      cum_rew_l_right = np.array(opt(final_t - change_t, plan_steps)) + cum_rew_l_left[-1]\n      arr_sum += np.array(cum_rew_l_left + list(cum_rew_l_right))\n      alg.env.switch_walls()\n    plt.plot(arr_sum / n_runs, label=label)\n  \n  # plot \n  plt.legend()\n  ax.set_title(title + f"" ({n_runs} runs average, n={plan_steps} planning steps, a={alpha}, k={k})"", fontsize=MED_FONT)\n  ax.set_xticks(xticks)\n  ax.set_yticks(yticks)\n  ax.set_xlabel(\'Time Steps\', fontsize=BIG_FONT)\n  ax.set_ylabel(\'Cumulative\\nReward\', rotation=0, labelpad=15, fontsize=BIG_FONT-2)\n  fig.set_size_inches(10, 8)\n  save_plot(filename, dpi=100)\n  plt.show()\n\ndef fig_8_4():\n  run_dynaq_dynaqp(\'Figure 8.4\', \'fig8.4\', FIG_8_4_N_RUNS, [0, 1000, 2000, 3000], [0, 150], FIG_8_4_CHG_T, FIG_8_4_FINAL_T, FIG_8_4_WALLS[:-1], FIG_8_4_WALLS[1:], FIG_8_4_PLAN_STEPS, alpha=FIG_8_4_ALP, eps=FIG_8_4_EPS, k=FIG_8_4_K)\n\ndef fig_8_5():\n  run_dynaq_dynaqp(\'Figure 8.5\', \'fig8.5\', FIG_8_4_N_RUNS, [0, 3000, 6000], [0, 400], FIG_8_5_CHG_T, FIG_8_5_FINAL_T, FIG_8_4_WALLS[1:], FIG_8_4_WALLS[1:-1], FIG_8_4_PLAN_STEPS, alpha=FIG_8_4_ALP, eps=FIG_8_4_EPS, k=FIG_8_4_K)\n\ndef ex_8_4():\n  run_dynaq_dynaqp(\'Exercise 8.4\', \'ex8.4\', FIG_8_4_N_RUNS, [0, 6000, 12000], [0, 1000], EX_8_4_CHG_T, EX_8_4_FINAL_T, FIG_8_4_WALLS[1:], FIG_8_4_WALLS[:-1], FIG_8_4_PLAN_STEPS, alpha=FIG_8_4_ALP, eps=FIG_8_4_EPS, k=FIG_8_4_K, ex_8_4=True)\n\ndef example_8_4():\n  fig, ax = plt.subplots()  \n  ax.set_title(f\'Example 8.4 ({EXAMPLE_8_4_N_RUNS} runs per datapoint)\', fontsize=BIG_FONT)\n  n_upd_prio_l, n_upd_dyna_l, n_states_l = [], [], []\n  for n in EXAMPLE_8_4_N_PART:\n    env = DynaMazePartitioned(n)\n    n_moves_opt = sum(env.expand((6, 8)))\n    n_states_l.append(len(env.states)-len(env.walls))\n    print(f""n_states={n_states_l[-1]}"")\n    for (alg_class, param, n_upd_l) in [(PrioritizedSweeping, EXAMPLE_8_4_THETA, n_upd_prio_l), (DynaQ, FIG_8_4_EPS, n_upd_dyna_l)]:\n      print((""prio"" if alg_class == PrioritizedSweeping else ""dyna"") + ""..."")\n      n_upd_l.append(0)\n      alg = alg_class(env, FIG_8_4_ALP, DYNA_MAZE_GAMMA, param)\n      alg.seed(0)\n      for run in range(EXAMPLE_8_4_N_RUNS):\n        alg.reset()\n        start = time.time()\n        n_upd_l[-1] += (alg.updates_until_optimal(n_moves_opt, n_plan_steps=5, tol=0.5))\n        print(f""run #{run} took {time.time()-start:.2f}s"")\n      n_upd_l[-1] /= EXAMPLE_8_4_N_RUNS\n  ax.set_xlabel(\'Gridworld states (#states)\', fontsize=BIG_FONT-2)\n  ax.set_ylabel(\'Updates\\nuntil\\noptimal\\nsolution\', rotation=0, labelpad=25, fontsize=BIG_FONT-2)\n  ax.set_xscale(\'log\', basex=2)\n  x_name = [\'0\'] + [str(n_states) for n_states in n_states_l]\n  xticks = [2 ** k for k in range(len(n_states_l) + 1)]\n  plt.xticks(xticks, x_name)\n  ax.set_yscale(\'log\')\n  plt.plot(xticks, [10] + n_upd_prio_l, color=\'r\', label=\'Prioritized Sweeping\')\n  plt.plot(xticks, [10] + n_upd_dyna_l, color=\'b\', label=\'Dyna-Q\')\n  plt.legend()\n  fig.set_size_inches(10, 8)\n  save_plot(\'example8.4\', dpi=100)\n  plt.show()\n\ndef fig_8_7():\n  fig, ax = plt.subplots()  \n  ax.set_title(f\'Figure 8.7 ({FIG_8_7_N_RUNS} runs per b)\', fontsize=BIG_FONT)\n  R = 0\n  gamma = 1\n  def rms_error(vals, true_vals, n):\n    return np.sqrt(np.sum((vals - true_vals) ** 2) / n)\n  for b in FIG_8_7_B_L:\n    print(f""b={b}"")\n    true_q_vals = np.random.randn(b)\n    qstar = R + gamma * np.mean(true_q_vals)\n    estim = np.zeros((2 * b + 1, FIG_8_7_N_RUNS))\n    qhat_0 = qstar + np.random.choice([-1, 1])\n    for run in range(FIG_8_7_N_RUNS):\n      qhat = qhat_0\n      errors = []\n      for t in range(1, 2 * b + 1):\n        sampled_idx = np.random.randint(b)\n        qhat += (1 / (t + 1)) * (R + gamma * true_q_vals[sampled_idx] - qhat)\n        estim[t - 1, run] = qhat\n    xidxs = [x / (2 * b) for x in range(1, 2 * b + 1)]\n    rms_err = [rms_error(estim[t - 1, :], qstar, FIG_8_7_N_RUNS) for t in range(1, 2 * b + 1)]\n    plt.plot(xidxs,  rms_err, label=f\'b={b}\')\n  xname = [\'0\', \'1b\', \'2b\']\n  xticks = [0, 1 / 2, 1]\n  plt.xticks(xticks, xname)\n  plt.legend()\n  save_plot(\'fig8.7\')\n  plt.show()\n\ndef set_axis(ax, n_states, xticks, show_ylabel=True):\n  ax.set_title(f\'{n_states} states\')\n  xlabel = \'Computation time, in expected updates\'\n  ylabel = \'Value of\\nstart state\\nunder\\ngreedy\\npolicy\'\n  ax.set_xlabel(xlabel, fontsize=BIG_FONT-4)\n  if show_ylabel:\n    ax.set_ylabel(ylabel, rotation=0, labelpad=35, fontsize=BIG_FONT-4)\n  ax.set_xticks([xticks[k] for k in [0, 5, 10, 15, 20]])\n\ndef fig_8_8():\n  fig = plt.figure() \n  np.random.seed(0)\n  for (n_st, log_freq, n_upd, b_list, fig_id) in [(FIG_8_8_N_ST_UPPER, FIG_8_8_LOG_FREQ_UPPER, FIG_8_8_N_UPD_UPPER, FIG_8_8_B_L_UPPER, \'121\'),\n                                                  (FIG_8_8_N_ST_LOWER, FIG_8_8_LOG_FREQ_LOWER, FIG_8_8_N_UPD_LOWER, FIG_8_8_B_L_LOWER, \'122\')]:\n    xticks = [log_freq * k for k in range(n_upd // log_freq + 1)]\n    set_axis(fig.add_subplot(fig_id), n_st, xticks + [n_upd], fig_id == \'121\')\n    for b in b_list:\n      task_list = [Task(b, n_st) for _ in range(FIG_8_8_N_RUNS)]\n      print(f""b={b}"")\n      for label in [\'uniform\', \'on policy\']:\n        print(f""{label}.."")\n        vals = 0\n        for (run_id, task) in enumerate(task_list):\n          print(f""run #{run_id}"")\n          alg = TrajectorySampling(task) \n          updates = alg.uniform if label == \'uniform\' else alg.on_policy\n          vals += updates(FIG_8_8_S_0, n_upd, log_freq)\n          print(vals)\n        plt.plot(xticks,\n                 [0] + list(vals / FIG_8_8_N_RUNS),\n                 label=f\'b={b}, \' + label) \n      plt.legend()\n  fig.suptitle(f\'Figure 8.8 ({FIG_8_8_N_RUNS} sample tasks)\')\n  fig.set_size_inches(20, 16)\n  save_plot(\'fig8.8\')\n  plt.show()\n\nPLOT_FUNCTION = {\n  \'section8.1\': section_8_1,\n  \'8.2\': fig_8_2,\n  \'8.3\': fig_8_3, \n  \'8.4\': fig_8_4, \n  \'8.5\': fig_8_5, \n  \'ex8.1\': ex_8_1,\n  \'ex8.4\': ex_8_4,\n  \'example8.4\': example_8_4,\n  \'8.7\': fig_8_7,\n  \'8.8\': fig_8_8,\n}\n\ndef main():\n  parser = argparse.ArgumentParser()\n\n  parser.add_argument(\'figure\', type=str, default=None,\n                      help=\'Figure to reproduce.\',\n                      choices=list(PLOT_FUNCTION.keys()) + [\'all\'])\n  args = parser.parse_args()\n  if args.figure == \'all\':\n    for key, f in PLOT_FUNCTION.items():\n      print(f""[{key}]"")\n      f()\n  else:\n    print(f""[{args.figure}]"")\n    PLOT_FUNCTION[args.figure]()\n\nif __name__ == \'__main__\':\n  main()\n'"
chapter8/models.py,0,"b'import copy\n\nclass Model:\n  def __init__(self, states=None, moves_d=None):\n    self.states = set() if states is None else set(states)\n    self.moves_d = {s: set() for s in self.states} if moves_d is None else moves_d\n    self.trans = {}\n\n  def add_transition(self, s, a, r, s_p):\n    self.states.add(s)\n    if s in self.moves_d:\n      self.moves_d[s].add(a)\n    else:\n      self.moves_d[s] = set([a])\n    self.trans[(s, a)] = (s_p, r)\n\n  def add_transition_cheat(self, s, a, r, s_p, moves):\n    self.states.add(s)\n    self.moves_d[s] = set(moves)\n    for a_p in moves:\n      self.trans[(s, a_p)] = (s_p, r) if a == a_p else (s, 0)\n\n  def sample_s_r(self, s, a):\n    try:\n      return self.trans[(s, a)]\n    except KeyError:\n      print(f""transition {s}, {a} doesn\'t exist yet"")\n      import ipdb; ipdb.set_trace()\n\n  def __str__(self):\n    strg = \'\'\n    for s in self.states:\n      strg += str(s) + \'\\n\'\n      for a in self.moves_d[s]:\n        s_r = f""(s_p={str(self.trans[(s, a)][0])}, r={self.trans[(s, a)][1]})"" if (s, a) in self.trans else \'???\'\n        strg += f""-{a} --> {s_r}\\n""\n    return strg\n\n  def reset(self):\n    self.states = set()\n    self.moves_d = {}\n    self.trans = {}\n\nclass FullModel(Model):\n  def __init__(self, env):\n    super().__init__(env.states, env.moves_d)\n    self.env = copy.copy(env)\n\n  def sample_s_r(self, s, a):\n    self.env.force_state(s)\n    s_p, r, _, _ = self.env.step(a)\n    return s_p, r \n'"
chapter8/nstep_sarsa.py,7,"b'import numpy as np\nimport copy\n\nclass TD:\n  def __init__(self, env, V_init=None, step_size=None, gamma=0.9, n=1):\n    self.env = env\n    self.gamma = gamma\n    self.V_init = V_init\n    self.step_size = step_size\n    self.n = n\n    self.reset()\n\n  def sample_action(self, pi, s):\n    pi_dist = [pi[(a, s)] for a in self.env.moves_d[s]]\n    return self.env.moves_d[s][np.random.choice(np.arange(len(self.env.moves_d[s])), p=pi_dist)]\n  \n  def seed(self, seed):\n    self.env.seed(seed)\n    np.random.seed(seed)\n \n  def get_value_list(self):\n    return [val for key,val in self.V.items()]\n  \n  def reset(self):\n    self.V = {s: 0 for s in self.env.states} if self.V_init is None else copy.deepcopy(self.V_init)\n\nclass nStepTD(TD):\n  def __init__(self, env, V_init=None, step_size=None, gamma=0.9, n=1, ex_7_2=False):\n    super().__init__(env, V_init, step_size, gamma, n)\n    self.reset()\n\n  def get_r_values(self, R, i, j):\n    n = self.n\n    orig_mod = mod_idx = i % (n + 1)\n    goal = j % (n + 1)\n    R_vals = []\n    while True:\n      R_vals.append(R[mod_idx])\n      mod_idx = (mod_idx + 1) % (n + 1)\n      if mod_idx == goal:\n        return R_vals\n\n  def reset(self):\n    self.gamma_l = [self.gamma ** k for k in range(self.n + 1)]\n    self.S = [None for _ in range(self.n + 1)]\n    self.R = [None for _ in range(self.n + 1)]\n    super().reset() \n\n\nclass nStepSarsa(nStepTD):\n  def __init__(self, env, step_size=0.1, gamma=0.9, n=1, eps=0.1, exp_sar=False): \n    super().__init__(env, None, step_size, gamma, n)\n    self.update_pi = self.update_policy(eps)\n    self.exp_sar = exp_sar\n    self.reset()\n\n  def eps_gre(self, eps):\n    def eps_gre_pol(s):\n      if np.random.random() < eps:\n        return self.random_move(s)\n      q_arr = np.array([self.Q[(s, a)] for a in self.env.moves_d[s]])\n      return self.env.moves_d[s][np.random.choice(np.flatnonzero(q_arr == q_arr.max()))]\n    return eps_gre_pol\n \n  def update_policy(self, eps):\n    def update_on_q_values(s):\n      best_a = self.eps_gre(0)(s)\n      moves = self.env.moves_d[s]\n      soft_min = eps / len(moves)\n      for a in moves:\n        self.pi[(a, s)] = soft_min + (1 - eps) * (a == best_a)\n    return update_on_q_values\n\n  def initialize_pi(self):\n    self.pi = {}\n    for s in self.env.states:\n      self.update_pi(s)\n    return self.pi\n \n  def exp_val(self, s):\n    return sum(self.pi[(a, s)] * self.Q[(s, a)] for a in self.env.moves_d[s])\n\n  def n_step_return_q(self, tau, T):\n    n = self.n\n    max_idx = min(tau + n, T)\n    r_vals = self.get_r_values(self.R, tau + 1, max_idx + 1)\n    G = np.dot(self.gamma_l[:max_idx-tau], r_vals)\n    if tau + n < T:\n      tau_p_n = (tau + n) % (n + 1)\n      s, a = self.S[tau_p_n], self.A[tau_p_n]\n      last_term = self.Q[(s, a)] if not self.exp_sar else self.exp_val(s)\n      G = G + self.gamma_l[n] * last_term\n    return G\n\n  def pol_eval(self, n_ep=100, pi=None):\n    n, R, S, Q, A = self.n, self.R, self.S, self.Q, self.A\n    pi_learned = pi is None\n    self.pi = self.initialize_pi() if pi_learned else pi\n    ep_per_t = [] \n    for ep in range(n_ep):\n      S[0] = self.env.reset()\n      A[0] = self.sample_action(self.pi, S[0])\n      T = np.inf\n      t = 0\n      while True:\n        ep_per_t.append(ep)\n        tm, tp1m = t % (n + 1), (t + 1) % (n + 1)\n        if t < T:\n          S[tp1m], R[tp1m], d, _ = self.env.step(A[tm])\n          if d:\n            T = t + 1\n          else:\n            A[tp1m] = self.sample_action(self.pi, S[tp1m])\n        tau = t - n + 1\n        if tau >= 0:\n          G = self.n_step_return_q(tau, T)\n          taum = tau % (n + 1)\n          s, a = S[taum], A[taum]\n          Q[(s, a)] += self.step_size * (G - Q[(s, a)])\n          if pi_learned:\n            self.update_pi(s)\n        if tau == (T - 1):\n          break\n        t += 1\n    return ep_per_t\n\n\n  def get_v(self):\n    return {s: max(self.Q[(s, a)] for a in self.env.moves_d[s]) for s in self.env.states} \n\n  def reset(self):\n    super().reset()\n    self.Q = {(s, a): 0 for s in self.env.states for a in self.env.moves_d[s]}\n    self.A = [None for _ in range(self.n + 1)]\n\n'"
chapter8/play.py,0,"b'import argparse\nimport os\nfrom dyna_maze import DynaMaze\n\nENV_DICT = {\n  \'dyna_maze\': DynaMaze(),\n}\n\ndef play(env):\n  def refresh():\n    os.system(\'cls\' if os.name == \'nt\' else \'clear\')\n    print(env)\n  while True:\n    s = env.reset()\n    done = False\n    while not done:\n      key = \'\'\n      while key not in env.keys:\n        refresh()\n        key = input(""press key\\n$>"")\n        if key == ""exit()"":\n          exit()\n      _, _, done, _ = env.step_via_key(key)\n    again = input(""episode done, continue? (Y / n)"")\n    if again == \'n\':\n      break\n\ndef main():\n  parser = argparse.ArgumentParser()\n\n  parser.add_argument(\'-e\', \'--env\', type=str, default=\'dyna_maze\',\n                      help=\'Env to play with.\',\n                      choices=ENV_DICT.keys())\n  args = parser.parse_args()\n  play(ENV_DICT[args.env])\n\nif __name__ == ""__main__"":\n  main()\n'"
chapter8/prior_sweep.py,0,"b'from dyna_q import DynaQ\nfrom queue import PriorityQueue\nimport time\n\nclass Pair:\n  def __init__(self, P, s, a):\n    self.P = P\n    self.s = s\n    self.a = a\n\n  def get_s_a(self):\n    return (self.s, self.a)\n\n  def __lt__(self, other):\n    return self.P < other.P\n\nclass PrioritizedSweeping(DynaQ):\n  def __init__(self, env, alpha, gamma, theta, policy=None, eps=0.1):\n    super().__init__(env, alpha, gamma, eps)\n    self.theta = theta\n    self.policy = lambda s, Q: self.eps_gre(s) if policy is None else policy\n    self.reset()\n\n  def compute_p(self, s, a, r, s_p):\n    Q_max = max(self.Q[(s_p, a_p)] for a_p in self.env.moves_d[s])\n    return abs(r + self.g * Q_max - self.Q[(s, a)])\n\n  def PQueue_update(self, s, a, r, s_p):\n    P = self.compute_p(s, a, r, s_p)\n    if P > self.theta:\n      self.PQueue.put(Pair(P, s, a))\n\n  def updates_until_optimal(self, n_steps_opt, n_plan_steps, tol=0.1):\n    n_updates = 0\n    while True:\n      self.env.reset()\n      while True:\n        s = self.env.state\n        #print(self.env)\n        #time.sleep(0.01)\n        a = self.policy(s, self.Q)\n        s_p, r, d, _ = self.env.step(a)\n        self.model.add_transition(s, a, r, s_p)\n        self.predecessor[s_p].add((s, a))\n        self.PQueue_update(s, a, r, s_p)\n        for _ in range(n_plan_steps):\n          if self.PQueue.empty():\n            break\n          s, a = self.PQueue.get().get_s_a()\n          s_p, r = self.model.sample_s_r(s, a)\n          self.q_learning_update(s, a, r, s_p)\n          n_updates += 1\n          for (_s, _a) in self.predecessor[s]:\n            _, _r = self.model.sample_s_r(_s, _a)\n            self.PQueue_update(_s, _a, _r, s)\n        if d:\n          break\n      if self.test_n_steps(n_steps_opt, tol):\n        return n_updates\n\n  def reset(self):\n    super().reset()\n    self.PQueue = PriorityQueue()\n    self.predecessor = {s: set() for s in self.env.states}\n'"
chapter8/tabular_q.py,1,"b'from utils import sample\nimport numpy as np\n\nclass TabularQ:\n  def __init__(self, model, alpha, gamma):\n    self.model = model\n    self.a = alpha\n    self.g = gamma\n    self.Q = {}\n    self.reset_q()\n\n  def rand_sam_one_step_pla(self, n_updates=1, decay=False):\n    decay_rate = (1 - 1 / n_updates) if decay else None\n    S, A = self.model.states, self.model.moves_d\n    s_list = list(S)\n    a_dict = {s: list(A[s]) for s in s_list}\n    for _ in range(n_updates):\n      s = sample(s_list)\n      a = sample(a_dict[s]) \n      s_p, r = self.model.sample_s_r(s, a)\n      Q_max = max(self.Q[(s_p, a_p)] for a_p in a_dict[s])\n      self.Q[(s, a)] += self.a * (r + self.g * Q_max - self.Q[(s, a)])\n      if decay:\n        self.a *= decay_rate\n\n  def get_V(self):\n    return {s: max(self.Q[(s, a)] for a in self.model.moves_d[s]) for s in self.model.states}\n\n  def seed(self, seed):\n    np.random.seed(seed) \n\n  def reset_q(self):\n    for s in self.model.states:\n      for a in self.model.moves_d[s]:\n        self.Q[(s, a)] = 0\n'"
chapter8/task.py,4,"b'import copy\nimport numpy as np\nfrom utils import sample\n\nSTART_STATE = 0\nTERMINAL_STATE = -1\nLEFT = 0\nRIGHT = 1\nP_TERM = 0.1\nR_TERM = 0\n\nclass Task:\n  def __init__(self, b, n_states, eps=P_TERM):\n    self.b = b \n    self.n_states = n_states\n    self.get_states()\n    self.get_moves_d()\n    self.get_transitions()\n    self.eps = eps\n\n  def get_states(self):\n    self.states = [TERMINAL_STATE] + list(range(self.n_states))\n\n  def get_moves_d(self):\n    self.moves_d = {s: [LEFT, RIGHT] for s in self.states}\n\n  def sample_next_states(self):\n    states_no_term = self.states[1:]\n    states_copy = copy.copy(states_no_term)\n    next_states = []\n    n_states = len(states_no_term)\n    for i in range(self.b):\n      s_idx = np.random.randint(n_states - i)\n      next_states.append(states_copy[s_idx])\n      del states_copy[s_idx]\n    return next_states\n\n  def get_transitions(self): \n    self.trans = {}\n    for s in self.states:\n      for a in self.moves_d[s]:\n        if s == TERMINAL_STATE:\n          exp_rew, next_states = R_TERM, [s]\n        else:\n          exp_rew = np.random.randn()\n          next_states = self.sample_next_states()\n        self.trans[(s, a)] = exp_rew, next_states\n\n  def step(self, a):\n    s = self.state \n    exp_rew, next_states = self.trans[(s, a)]\n    done = np.random.random() < self.eps\n    s_p = TERMINAL_STATE if done else sample(next_states)\n    self.state = s_p\n    return s_p, exp_rew, done, {}\n\n  def seed(self, seed):\n    np.random.seed(seed)\n\n  def reset(self):\n    self.state = START_STATE\n    return self.state\n\n  def force_state(self, s):\n    self.state = s\n'"
chapter8/traj_sampling.py,5,"b'import numpy as np\nfrom utils import sample\nimport time\n\nclass TrajectorySampling:\n  def __init__(self, env, gamma=1, eps=0.1):\n    self.env = env\n    self.g = gamma\n    self.eps = eps\n    self.reset()\n\n  def gre(self, s):\n    q_arr = np.array([self.Q[(s, a)] for a in self.env.moves_d[s]])\n    return self.env.moves_d[s][np.random.choice(np.flatnonzero(q_arr == q_arr.max()))]\n\n  def gre_value_iteration(self, s_0, theta=1e-4, max_iter=1e2):\n    V = {s: 0 for s in self.env.states}\n    n_iter = 0\n    while True and n_iter < max_iter:\n      delta = 0\n      for s in self.env.states:\n        v = V[s]\n        r, next_states = self.env.trans[(s, self.gre(s))]\n        V[s] = r + ((1 - self.env.eps) / self.env.b) * sum(V[s_p] for s_p in next_states)\n        delta = max(delta, abs(v-V[s]))\n      if delta < theta:\n        break\n      n_iter += 1\n    return V[s_0]\n\n  def eps_gre(self, s):\n    if np.random.random() < self.eps:\n      return sample(self.env.moves_d[s])\n    return self.gre(s)\n\n  def exp_update(self, s, a):\n    exp_rew, next_states = self.env.trans[(s, a)]\n    self.Q[(s, a)] = exp_rew + self.g * (1 - self.eps) * sum(max(self.Q[(s_p, a_p)] for a_p in self.env.moves_d[s_p]) for s_p in next_states)\n\n  def uniform(self, start_state, n_updates, log_freq=100):\n    values = []\n    start = time.time()\n    n_upd = 0\n    while True:\n      for s in self.env.states:\n        for a in self.env.moves_d[s]:\n          self.exp_update(s, a) \n          n_upd += 1\n          if n_upd % log_freq == (log_freq - 1):\n            print(f""{n_upd + 1} updates (total of {time.time()-start:.2f}s)"")\n            values.append(self.gre_value_iteration(start_state, theta=0.1))\n          if n_upd == n_updates:\n            return np.array(values)\n\n  def on_policy(self, start_state, n_updates, log_freq=100):\n    values = []\n    start = time.time()\n    n_upd = 0\n    while n_upd < n_updates:\n      self.env.force_state(start_state)\n      s = start_state\n      d = False\n      while not d and n_upd < n_updates:\n        a = self.eps_gre(s)\n        self.exp_update(s, a)\n        n_upd += 1\n        s, r, d, _ = self.env.step(a)\n        if n_upd % log_freq == (log_freq - 1):\n          print(f""{n_upd + 1} updates (total of {time.time()-start:.2f}s)"")\n          values.append(self.gre_value_iteration(start_state, theta=0.1))\n        if d:\n          s = start_state\n          self.env.force_state(start_state)\n    return np.array(values)\n\n  def reset(self):\n    self.Q = {(s, a): 0 for s in self.env.states for a in self.env.moves_d[s]} \n'"
chapter8/utils.py,3,"b'import numpy as np\n\ndef sample(l): \n  return l[np.random.randint(len(l))]\n\ndef print_q_values(alg):\n  env, Q = alg.env, alg.Q\n  for s in env.states:\n    print(str(s))\n    for a in env.moves_d[s]:\n      print(f""->{a}: {Q[(s, a)]}"")\n\ndef to_arr(V):\n  (min_x, max_x) = (min_y, max_y) = np.inf, -np.inf\n  for pos in V.keys():\n    x, y = pos.x, pos.y\n    min_x, min_y = min(x, min_x), min(y, min_y)\n    max_x, max_y = max(x, max_x), max(y, max_y)\n  arr = np.zeros((max_x - min_x + 1, max_y - min_y + 1))\n  for pos, val in V.items():\n    arr[pos.x, pos.y] = val\n  return arr\n\n'"
chapter9/feat_const.py,7,"b'import numpy as np\n\ndef poly_feat(s, n): \n  if isinstance(s, float):\n    return np.array([s ** i for i in range(n + 1)])\n  # code below untested\n  k = s.shape[0]\n  n_feat = (n + 1) ** k\n  x_s = np.ones(n_feat)\n  pow_l = np.zeros(k)\n  s_pow = [[s[i] ** j for j in range(n + 1)] for i in range(k)]\n  idx = 0\n  while idx < n_feat:\n    for (i, cij) in enumerate(pow_l):\n      x_s[idx] *= s_pow[i][int(cij)]\n    for col in range(k - 1, -1, -1):\n      pow_l[col] = (pow_l[col] + 1) % (n + 1)\n      if pow_l[col] > 0:\n        break\n    idx += 1\n  return x_s\n\ndef four_feat(s, n):\n  if isinstance(s, float):\n    return np.array([np.cos(i * np.pi * s) for i in range(n + 1)])\n  # code below untested\n  k = s.shape[0]\n  n_feat = (n + 1) ** k\n  x_s = np.ones(n_feat)\n  pow_l = np.zeros(k)\n  s_pow = [[s[i] ** j for j in range(n + 1)] for i in range(k)]\n  idx = 0\n  while idx < n_feat:\n    for (i, cij) in enumerate(pow_l):\n      x_s[idx] = np.cos(np.pi * np.dot(pow_l, s))\n    for col in range(k - 1, -1, -1):\n      pow_l[col] = (pow_l[col] + 1) % (n + 1)\n      if pow_l[col] > 0:\n        break\n    idx += 1\n  return x_s\n'"
chapter9/figures.py,18,"b'import argparse\nimport matplotlib.pyplot as plt\nfrom randomwalk import RandomWalk, EMPTY_MOVE\nfrom gradient_methods import GradientMC, SemiGradientTD0\nfrom nstep_semi_grad import nStepSemiGrad\nfrom utils import est\nimport numpy as np\nfrom feat_const import poly_feat, four_feat\n\nMED_FONT = 13\n\nFIG_9_1_ALP = 2e-5\nFIG_9_1_W_DIM = 10\nFIG_9_1_N_EP = 10 ** 5\nFIG_9_1_N_EP_TR = 10 ** 3\nFIG_9_1_G = 1\n\nFIG_9_2_ALP = FIG_9_1_ALP\nFIG_9_2_W_DIM_L = FIG_9_1_W_DIM\nFIG_9_2_W_DIM_R = 20\nFIG_9_2_N_EP_L = FIG_9_1_N_EP\nFIG_9_2_N_EP_R = 10\nFIG_9_2_N_RUNS_R = 100\nFIG_9_2_N_EP_TR = FIG_9_1_N_EP_TR\nFIG_9_2_G = FIG_9_1_G\nFIG_9_2_MAX_N = 512\n\nFIG_9_5_BAS = [5, 10, 20]\nFIG_9_5_ALP_POL = 1e-4\nFIG_9_5_ALP_FOU = 5e-5\nFIG_9_5_N_EP = int(5e3)\nFIG_9_5_G = FIG_9_1_G\nFIG_9_5_N_RUNS = 30\n\nFIG_9_10_ALP_ST_AGG = 1e-4\nFIG_9_10_TIL_L = [50, 1]\nFIG_9_10_ALP_TIL_L = [FIG_9_10_ALP_ST_AGG / n_til for n_til in FIG_9_10_TIL_L]\nFIG_9_10_TOT_ST = 1000\nFIG_9_10_ST_AGG = 200\nFIG_9_10_N_EP = int(5e3)\nFIG_9_10_G = FIG_9_1_G\nFIG_9_10_N_RUNS = 30\n\ndef save_plot(filename, dpi=None):\n  plt.savefig(\'plots/\' + filename + \'.png\', dpi=dpi)\n\ndef plot_figure(ax, title, xticks, xnames, xlabel, yticks, ynames, ylabel,\n                labelpad=15, font=MED_FONT, loc=\'upper left\'):\n  ax.set_title(title, fontsize=font)\n  ax.set_xticks(xticks)\n  ax.set_xticklabels(xnames)\n  ax.set_yticks(yticks)\n  ax.set_yticklabels(ynames)\n  ax.set_xlim([min(xticks), max(xticks)])\n  ax.set_ylim([min(yticks), max(yticks)])\n  ax.set_xlabel(xlabel, fontsize=font)\n  ax.set_ylabel(ylabel, rotation=0, fontsize=font, labelpad=labelpad)\n  plt.legend(loc=loc)\n\ndef enc_st_agg(s, w, tot_st=1000):\n  return s // (tot_st // len(w))\n\ndef vhat_st_agg(s, w, tot_st=1000):\n  if s == tot_st:\n    return 0\n  return w[enc_st_agg(s, w, tot_st)]\n\ndef nab_vhat_st_agg(s, w, tot_st=1000):\n  if s == tot_st:\n    return 0\n  return np.array([i == enc_st_agg(s, w, tot_st) for i in range(len(w))])\n\ndef get_true_vals(env, pi):\n  #if input(""load true values? (Y/n)"") != ""n"":\n  print(""loading true vals"")\n  true_vals = np.load(\'true_vals.arr\', allow_pickle=True)\n  #else:\n  #  true_vals = np.array([est(env, pi, s, FIG_9_2_G, n_ep=FIG_9_2_N_EP_TR) for s in env.states])\n  #  if input(""save true values? (y/N)?"") != \'n\':\n  #    print(""saving true vals"")\n  #    true_vals.dump(\'true_vals.arr\')\n  return true_vals\n\ndef fig_9_1():\n  env = RandomWalk() \n  pi = {(EMPTY_MOVE, s): 1 for s in env.states}\n  true_vals = get_true_vals(env, pi)\n\n  grad_mc = GradientMC(env, FIG_9_1_ALP, FIG_9_1_W_DIM)\n  grad_mc.seed(0)\n  grad_mc.pol_eva(pi, vhat_st_agg, nab_vhat_st_agg, FIG_9_1_N_EP, FIG_9_1_G)\n  est_vals = [vhat_st_agg(s, grad_mc.w) for s in env.states][:-1]\n\n  fig, ax1 = plt.subplots()\n  ax1.plot(est_vals, \'b\', label=\'Approximate MC value vhat\')\n  ax1.plot(true_vals, \'r\', label=\'True value v_pi\')\n  plot_figure(ax1, \'Figure 9.1\', [0, 999], [1, 1000], \'State\',\n              [-1, 0, 1], [-1, 0, 1], \'\\n\\nValue\\nScale\')\n  ax2 = ax1.twinx()\n  ax2.set_yticks([0, 0.0017, 0.0137])\n  ax2.set_ylabel(\'Distribution\\nscale\', rotation=0, fontsize=MED_FONT)\n  ax2.plot(grad_mc.mu[:-1], \'m\', label=\'State distribution mu\')\n  plt.legend()\n  fig.set_size_inches(20, 14)\n  save_plot(\'fig9.1\', dpi=100)\n  plt.show()\n\ndef param_study(ax, alg, pi, vhat, nab_vhat, n_ep, n_runs, true_vals=None,\n                max_n=FIG_9_2_MAX_N, gamma=FIG_9_2_G):\n  n_l = [2 ** k for k in range(int(np.log(max_n) / np.log(2)) + 1)]\n  for n in n_l:\n    alg.n = n\n    print(f"">> n={n}"")\n    err_l = []\n    alpha_max = 1 if n <= 16 else 1 / (np.log(n // 8) / np.log(2))\n    alpha_l = np.linspace(0, alpha_max, 31)\n    for alpha in alpha_l:\n      alg.a = alpha\n      print(f""alpha={alpha}"")\n      err_sum = 0\n      for seed in range(n_runs):\n        alg.reset()\n        alg.seed(seed)\n        for ep in range(n_ep):\n          alg.pol_eva(pi, vhat, nab_vhat, n_ep=1, gamma=gamma)\n          v_arr = np.array(alg.get_value_list(vhat)[:-1]) # removes absorbing state\n          err_sum += np.sqrt(np.sum((v_arr-true_vals[:-1]) ** 2) / alg.env.n_states)\n      err_l.append(err_sum / (n_runs * n_ep))\n    plt.plot(alpha_l, err_l, label=f\'n={n}\')\n  ax.set_xticks(np.linspace(0, 1, 6))\n  yticks = np.linspace(0.25, 0.55, 6)\n  ax.set_yticks(yticks)\n  ax.set_ylim([min(yticks), max(yticks)])\n  ax.set_xlabel(\'Stepsize\')\n  ax.set_ylabel(f\'Average RMS error ({alg.env.n_states} states, first {n_ep} episodes)\')\n\ndef fig_9_2():\n  fig = plt.figure()\n  fig.suptitle(\'Figure 9.2\')\n  env = RandomWalk()\n  pi = {(EMPTY_MOVE, s): 1 for s in env.states}\n  true_vals = get_true_vals(env, pi)\n\n  semi_grad_td = SemiGradientTD0(env, FIG_9_2_ALP, FIG_9_2_W_DIM_L)\n  semi_grad_td.seed(0)\n  semi_grad_td.pol_eva(pi, vhat_st_agg, nab_vhat_st_agg, FIG_9_2_N_EP_L, FIG_9_2_G)\n  est_vals = [vhat_st_agg(s, semi_grad_td.w) for s in env.states][:-1]\n  ax1 = fig.add_subplot(\'121\')\n  ax1.plot(est_vals, \'b\', label=\'Approximate TD value vhat\')\n  ax1.plot(true_vals, \'r\', label=\'True value v_pi\')\n  plot_figure(ax1, \'\', [0, 999], [1, 1000], \'State\', [-1, 0, 1], [-1, 0, 1], \'\\n\\nValue\\nScale\')\n\n  nstep_semi_grad = nStepSemiGrad(env, None, FIG_9_2_W_DIM_R, FIG_9_2_G, 0)\n  ax2 = fig.add_subplot(\'122\')\n  param_study(ax2, nstep_semi_grad, pi, vhat_st_agg, nab_vhat_st_agg,\n              FIG_9_2_N_EP_R, FIG_9_2_N_RUNS_R, true_vals=true_vals,\n              max_n=FIG_9_2_MAX_N, gamma=FIG_9_2_G)\n  plt.legend()\n  fig.set_size_inches(20, 14)\n  save_plot(\'fig9.2\', dpi=100)\n  plt.show()\n\ndef fig_9_5():\n  fig, ax = plt.subplots()\n  env = RandomWalk()\n  pi = {(EMPTY_MOVE, s): 1 for s in env.states}\n  true_vals = get_true_vals(env, pi)\n\n  for (feat, alp, label) in [(poly_feat, FIG_9_5_ALP_POL, \'polynomial basis\'),\n                        (four_feat, FIG_9_5_ALP_FOU, \'fourier basis\')]:\n    for base in FIG_9_5_BAS:\n      def vhat(s, w): return np.dot(w, feat(s / 1000, base))\n      def nab_vhat(s, w): return feat(s / 1000, base)\n      w_dim = base + 1\n      grad_mc = GradientMC(env, alp, w_dim)\n      err_sum = np.zeros(FIG_9_5_N_EP)\n      for seed in range(FIG_9_5_N_RUNS):\n        print(f""seed={seed}"")\n        grad_mc.reset()\n        grad_mc.seed(seed)\n        err_per_ep = []\n        for ep in range(FIG_9_5_N_EP):\n          if ep % 100 == 0 and ep > 0:\n            print(ep)\n          grad_mc.pol_eva(pi, vhat, nab_vhat, n_ep=1, gamma=FIG_9_5_G)\n          est_vals = [vhat(s, grad_mc.w) for s in env.states][:-1]\n          err_per_ep.append(np.sqrt(np.dot(grad_mc.mu[:-1], (est_vals-true_vals[:-1]) ** 2)))\n        err_sum += err_per_ep\n      plt.plot(err_sum / FIG_9_5_N_RUNS, label=f\'{label}, n={base}\')\n  plt.legend()\n  plot_figure(ax, \'Figure 9.5\', [0, 5000], [0, 5000], ""Episodes"",\n             [0, 0.1, 0.2, 0.3, 0.4], [\'0\', \'0.1\', \'0.2\', \'0.3\', \'0.4\'],\n             f""Square-Root\\nValue Error\\n({FIG_9_5_N_RUNS} runs)"",\n             labelpad=30, font=MED_FONT, loc=\'lower left\')\n  fig.set_size_inches(20, 14)\n  save_plot(\'fig9.5\', dpi=100)\n  plt.show()\n\ndef fig_9_10():\n  fig, ax = plt.subplots()\n  env = RandomWalk()\n  pi = {(EMPTY_MOVE, s): 1 for s in env.states}\n  true_vals = get_true_vals(env, pi)\n\n  def feat_tile(s, offset, st_per_agg):\n    if s < offset:\n      return 0\n    return (s - offset) // st_per_agg + 1\n\n  def feat(s, st_per_agg, n_tiles):\n    dx = st_per_agg // n_tiles if n_tiles > 1 else 0\n    ft_per_til = FIG_9_10_TOT_ST // st_per_agg + (n_tiles > 1)\n    feat_arr = np.zeros(ft_per_til * n_tiles)\n    for n in range(n_tiles):\n      idx_min = n * ft_per_til\n      s_id = feat_tile(s, n * dx, st_per_agg) - (n_tiles == 1)\n      feat_arr[idx_min + s_id] = True\n    return feat_arr.astype(bool)\n\n  for (idx, n_tiles) in enumerate(FIG_9_10_TIL_L):\n    def feat_vec(s): return feat(s, FIG_9_10_ST_AGG, n_tiles)\n    def vhat(s, w): return np.sum(w[feat_vec(s)]) if s < FIG_9_10_TOT_ST else 0\n    def nab_vhat(s, w): return feat_vec(s) if s < FIG_9_10_TOT_ST else 0\n    w_dim = (FIG_9_10_TOT_ST // FIG_9_10_ST_AGG + (n_tiles > 1)) * n_tiles\n    grad_mc = GradientMC(env, FIG_9_10_ALP_TIL_L[idx], w_dim)\n    print(f""w_dim={w_dim}, alpha={grad_mc.a}, n_tiles={n_tiles}"")\n    err_sum = np.zeros(FIG_9_10_N_EP)\n    for seed in range(FIG_9_10_N_RUNS):\n      print(f""seed={seed}"")\n      grad_mc.reset()\n      grad_mc.seed(seed)\n      err_per_ep = []\n      for ep in range(FIG_9_10_N_EP):\n        if ep % 100 == 0 and ep > 0:\n          print(ep)\n        grad_mc.pol_eva(pi, vhat, nab_vhat, n_ep=1, gamma=FIG_9_10_G)\n        est_vals = [vhat(s, grad_mc.w) for s in env.states][:-1]\n        err_per_ep.append(np.sqrt(np.dot(grad_mc.mu[:-1], (est_vals-true_vals[:-1]) ** 2)))\n      err_sum += np.array(err_per_ep)\n    plt.plot(err_sum / FIG_9_10_N_RUNS, label=((\'State Aggregation\' if (n_tiles > 1) else \'Tile Coding\') +\n                                              f\' ({n_tiles} tile{""s"" * (n_tiles > 1)})\'))\n  plt.legend()\n  plot_figure(ax, \'Figure 9.10\', [0, 5000], [0, 5000], ""Episodes"",\n             [0, 0.1, 0.2, 0.3, 0.4], [\'0\', \'0.1\', \'0.2\', \'0.3\', \'0.4\'],\n             f""Square-Root\\nValue Error\\n({FIG_9_10_N_RUNS} runs)"",\n             labelpad=30, font=MED_FONT, loc=\'lower left\')\n  fig.set_size_inches(20, 14)\n  save_plot(\'fig9.10\', dpi=100)\n  plt.show()\n\nPLOT_FUNCTION = {\n  \'9.1\': fig_9_1,\n  \'9.2\': fig_9_2,\n  \'9.5\': fig_9_5,\n  \'9.10\': fig_9_10,\n}\n\ndef main():\n  parser = argparse.ArgumentParser()\n\n  parser.add_argument(\'figure\', type=str, default=None,\n                      help=\'Figure to reproduce.\',\n                      choices=list(PLOT_FUNCTION.keys()) + [\'all\'])\n  args = parser.parse_args()\n  if args.figure == \'all\':\n    for key, f in PLOT_FUNCTION.items():\n      print(f""[{key}]"")\n      f()\n  else:\n    print(f""[{args.figure}]"")\n    PLOT_FUNCTION[args.figure]()\n\nif __name__ == \'__main__\':\n  main()\n'"
chapter9/gradient_methods.py,3,"b'import numpy as np\nfrom utils import gen_traj, gen_traj_ret\n\n\nclass GradientAlg:\n  def __init__(self, env, alpha, w_dim):\n    self.env = env\n    self.a = alpha\n    self.d = w_dim\n    self.reset()\n\n  def seed(self, seed):\n    self.env.seed(seed)\n    np.random.seed(seed)\n\n\n  def reset(self):\n    self.w = np.zeros(self.d)\n    self.mu = np.zeros(len(self.env.states))\n\n\nclass GradientMC(GradientAlg):\n  def __init__(self, env, alpha, w_dim):\n    super().__init__(env, alpha, w_dim)\n\n  def pol_eva(self, pi, vhat, nab_vhat, n_ep, gamma):\n    for ep in range(n_ep):\n      if ep > 0 and ep % 1000 == 0:\n        print(f""ep #{ep}"")\n      for (s, G) in gen_traj_ret(self.env, pi, gamma):\n        self.mu[s] += 1\n        self.w += self.a * (G - vhat(s, self.w)) * nab_vhat(s, self.w)\n    self.mu /= self.mu.sum()\n\nclass SemiGradientTD0(GradientAlg):\n  def __init__(self, env, alpha, w_dim):\n    super().__init__(env, alpha, w_dim)\n\n  def pol_eva(self, pi, vhat, nab_vhat, n_ep, gamma):\n    for ep in range(n_ep):\n      if ep > 0 and ep % 100 == 0:\n        print(f""ep #{ep}"")\n      traj = gen_traj(self.env, pi, inc_term=True)\n      for i in range(len(traj) - 1):\n        (s, r), (s_p, _) = traj[i], traj[i + 1]\n        self.mu[s] += 1\n        self.w += self.a * (r + gamma * vhat(s_p, self.w) - vhat(s, self.w)) * nab_vhat(s, self.w)\n    self.mu /= self.mu.sum()\n'"
chapter9/nstep_semi_grad.py,5,"b'import numpy as np\n\nclass nStepSemiGrad:\n  def __init__(self, env, alpha, w_dim, gamma, n):\n    self.env = env\n    self.a = alpha\n    self.n = n\n    self.d = w_dim\n    self.g = gamma\n    self.reset()\n\n  def sample_action(self, pi, s):\n    pi_dist = [pi[(a, s)] for a in self.env.moves_d[s]]\n    return self.env.moves_d[s][np.random.choice(np.arange(len(self.env.moves_d[s])), p=pi_dist)]\n\n  def get_r_values(self, R, i, j):\n    n = self.n\n    orig_mod = mod_idx = i % (n + 1)\n    goal = j % (n + 1)\n    R_vals = []\n    while True:\n      R_vals.append(R[mod_idx])\n      mod_idx = (mod_idx + 1) % (n + 1)\n      if mod_idx == goal:\n        return R_vals\n\n  def n_step_return(self, vhat, tau, T):\n    S, R, n, g_l = self.S, self.R, self.n, self.g_l\n    max_idx = min(tau + n, T)\n    r_vals = self.get_r_values(R, tau + 1, max_idx + 1)\n    G = np.dot(g_l[:max_idx-tau], r_vals)\n    if tau + n < T:\n      G = G + g_l[n] * vhat(S[(tau + n) % (n + 1)], self.w)\n    return G\n\n  def pol_eva(self, pi, vhat, nab_vhat, n_ep, gamma):\n    self.g_l = [gamma ** k for k in range(self.n + 1)]\n    n, R, S = self.n, self.R, self.S\n    for ep in range(n_ep):\n      S[0] = self.env.reset()\n      T = np.inf\n      t = 0\n      while True:\n        if t < T:\n          S[(t + 1) % (n + 1)], R[(t + 1) % (n + 1)], d, _ = self.env.step(self.sample_action(pi, S[t % (n + 1)]))\n          if d:\n            T = t + 1\n        tau = t - n + 1\n        if tau >= 0:\n          s = S[tau % (n + 1)]\n          G = self.n_step_return(vhat, tau, T)\n          self.w += self.a * (G - vhat(s, self.w)) * nab_vhat(s, self.w)\n        if tau == (T - 1):\n          break\n        t += 1\n\n  def get_value_list(self, vhat):\n    return [vhat(s, self.w) for s in self.env.states]\n\n  def seed(self, seed):\n    self.env.seed(seed)\n    np.random.seed(seed)\n\n  def reset(self):\n    self.S = [None for _ in range(self.n + 1)]\n    self.R = [None for _ in range(self.n + 1)]\n    self.w = np.zeros(self.d)\n'"
chapter9/randomwalk.py,2,"b'import numpy as np\n\nN_STATES = 1000\nABSORBING_STATE = N_STATES\nEMPTY_MOVE = 0\nP_LEFT = 0.5\nR_STEP = 0\nR_LEFT = -1\nR_RIGHT = 1\nSHIFT = 100\n\nclass RandomWalk:\n  def __init__(self, n_states=None):\n    self.n_states = N_STATES if n_states is None else n_states\n    self.absorbing_state = self.n_states\n    self.get_states()\n    self.get_moves()\n    self.get_moves_d()\n    self.reset()\n    self.r_l = R_LEFT\n    self.r_r = R_RIGHT\n\n  def get_moves(self):\n    self.moves = [EMPTY_MOVE]\n\n  def get_moves_d(self):\n    self.moves_d = {s: self.moves for s in self.states}\n\n  def get_states(self):\n    self.states = list(range(self.n_states)) + [self.absorbing_state]\n\n  def sample_shift(self):\n    return int(np.sign(np.random.random() - P_LEFT) * np.random.randint(1, SHIFT + 1))\n\n  def step(self, action):\n    if self.state == self.n_states:\n      return self.state, R_STEP, True, {}\n    shift = self.sample_shift()\n    new_state = self.state + shift\n    if not (0 <= new_state < self.n_states):\n      r = self.r_r if (new_state >= self.n_states) else self.r_l\n      return self.n_states, r, True, {}\n    self.state = new_state\n    return self.state, R_STEP, False, {}\n\n  def force_state(self, state):\n    self.state = state\n\n  def reset(self):\n    self.state = self.n_states // 2\n    return self.state\n\n  def seed(self, seed):\n    np.random.seed(seed)\n\n  def __str__(self):\n    return self.state\n'"
chapter9/utils.py,1,"b'import numpy as np\n\nR_TERM = 0\n\ndef sample_action(env, pi, s):\n  moves = env.moves_d[s]\n  pi_dist = [pi[(a, s)] for a in moves]\n  return env.moves_d[s][np.random.choice(np.arange(len(moves)), p=pi_dist)]\n\ndef gen_traj(env, pi, s_0=None, inc_term=False): \n  if s_0 is None:\n    s = env.reset()\n  else:\n    s = s_0\n    env.force_state(s_0)\n  traj, d = [], False\n  while not d:\n    s_p, r, d, _ = env.step(sample_action(env, pi, s))\n    traj.append((s, r))\n    s = s_p\n  if inc_term:\n    traj.append((s, R_TERM))\n  return traj\n\ndef gen_traj_ret(env, pi, gamma, s_0=None):\n  traj = gen_traj(env, pi, s_0)\n  ret_traj, G = [], 0\n  for (t, (s, r)) in enumerate(traj[::-1]):\n    G = r + gamma * G\n    ret_traj.append((s, G))\n  return ret_traj[::-1]\n\ndef est(env, pi, s_0, gamma, n_ep):\n  v = 0\n  print(s_0)\n  for n in range(1, n_ep + 1):\n    _, G = gen_traj_ret(env, pi, gamma, s_0)[0]\n    v += (G - v) / n\n  return v\n'"
