file_path,api_count,code
setup.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom sys import version_info\nimport warnings\n\nfrom setuptools import setup\n\nwith open('README.rst', 'r') as fh:\n\treadme = fh.read()\n\n# with open('json_tricks/_version.py', 'r') as fh:\n# \tversion = fh.read().strip()\nfrom json_tricks._version import VERSION\n\nrequires = []\nif version_info < (2, 7, 0):\n\trequires.append('ordereddict')\n\nif (version_info[0] == 2 and version_info[1] < 7) or \\\n\t\t(version_info[0] == 3 and version_info[1] < 4) or \\\n\t\tversion_info[0] not in (2, 3):\n\traise warnings.warn('`json_tricks` does not support Python version {}.{}'\n\t\t.format(version_info[0], version_info[1]))\n\nsetup(\n\tname='json_tricks',\n\tdescription='Extra features for Python\\'s JSON: comments, order, numpy, '\n\t\t'pandas, datetimes, and many more! Simple but customizable.',\n\tlong_description=readme,\n\turl='https://github.com/mverleg/pyjson_tricks',\n\tauthor='Mark V',\n\tmaintainer='Mark V',\n\tauthor_email='markv.nl.dev@gmail.com',\n\tlicense='Revised BSD License (LICENSE.txt)',\n\tkeywords=['json', 'numpy', 'OrderedDict', 'comments', 'pandas', 'pytz',\n\t\t'enum', 'encode', 'decode', 'serialize', 'deserialize'],\n\tversion=VERSION,\n\tpackages=['json_tricks'],\n\tpackage_data=dict(\n\t\tjson_tricks=['LICENSE.txt', 'README.rst', 'VERSION'],\n\t\t# tests=['tests/*.py'],\n\t),\n\t# include_package_data=True,\n\tzip_safe=True,\n\tclassifiers=[\n\t\t'Development Status :: 5 - Production/Stable',\n\t\t'Development Status :: 6 - Mature',\n\t\t'Intended Audience :: Developers',\n\t\t'Natural Language :: English',\n\t\t'License :: OSI Approved :: BSD License',\n\t\t'Operating System :: OS Independent',\n\t\t'Programming Language :: Python',\n\t\t'Programming Language :: Python :: 2',\n\t\t'Programming Language :: Python :: 2.7',\n\t\t'Programming Language :: Python :: 3',\n\t\t'Programming Language :: Python :: 3.4',\n\t\t'Programming Language :: Python :: 3.5',\n\t\t'Programming Language :: Python :: 3.6',\n\t\t'Programming Language :: Python :: 3.7',\n\t\t'Programming Language :: Python :: 3.8',\n\t\t'Programming Language :: Python :: Implementation :: CPython',\n\t\t'Programming Language :: Python :: Implementation :: PyPy',\n\t\t'Topic :: Software Development :: Libraries :: Python Modules',\n\t\t# 'Topic :: Utilities',\n\t],\n\tinstall_requires=requires,\n)\n"""
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\nimport shlex\nfrom os.path import abspath\n\n\nsys.path.insert(0, abspath(\'..\'))\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#sys.path.insert(0, os.path.abspath(\'.\'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.viewcode\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'json-tricks\'\ncopyright = u\'2017, Mark\'\nauthor = u\'Mark\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'1.2\'\n# The full version, including alpha/beta/rc tags.\nrelease = version\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [\'_build\']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n#keep_warnings = False\n\n# If true, `to do` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \'alabaster\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#html_extra_path = []\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'hu\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'ru\', \'sv\', \'tr\'\n#html_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# Now only \'ja\' uses this config value\n#html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'json-tricksdoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n# The paper size (\'letterpaper\' or \'a4paper\').\n#\'papersize\': \'letterpaper\',\n\n# The font size (\'10pt\', \'11pt\' or \'12pt\').\n#\'pointsize\': \'10pt\',\n\n# Additional stuff for the LaTeX preamble.\n#\'preamble\': \'\',\n\n# Latex figure (float) alignment\n#\'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n  (master_doc, \'json-tricks.tex\', u\'json-tricks Documentation\',\n   u\'Mark\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'json-tricks\', u\'json-tricks Documentation\',\n     [author], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n  (master_doc, \'json-tricks\', u\'json-tricks Documentation\',\n   author, \'json-tricks\', \'One line description of project.\',\n   \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#texinfo_no_detailmenu = False\n'"
json_tricks/__init__.py,0,"b'\ntry:\n\tfrom json import JSONDecodeError  # imported for convenience\nexcept ImportError:\n\t"""""" Older versions of Python use ValueError, of which JSONDecodeError is a subclass; it\'s recommended to catch ValueError. """"""\nfrom .utils import hashodict, NoEnumException, NoNumpyException, NoPandasException, get_scalar_repr, encode_intenums_inplace, encode_scalars_inplace\nfrom .comment import strip_comment_line_with_symbol, strip_comments\nfrom .encoders import TricksEncoder, json_date_time_encode, class_instance_encode, json_complex_encode, \\\n\tnumeric_types_encode, ClassInstanceEncoder, json_set_encode, pandas_encode, nopandas_encode, \\\n\tnumpy_encode, NumpyEncoder, nonumpy_encode, NoNumpyEncoder, fallback_ignore_unknown, pathlib_encode\nfrom .decoders import DuplicateJsonKeyException, TricksPairHook, json_date_time_hook, json_complex_hook, \\\n\tnumeric_types_hook, ClassInstanceHook, json_set_hook, pandas_hook, nopandas_hook, json_numpy_obj_hook, \\\n\tjson_nonumpy_obj_hook, pathlib_hook\nfrom .nonp import dumps, dump, loads, load\nfrom ._version import VERSION\n\n__version__ = VERSION\n\n\ntry:\n\t# find_module takes just as long as importing, so no optimization possible\n\timport numpy\nexcept ImportError:\n\tNUMPY_MODE = False\n\t# from .nonp import dumps, dump, loads, load, nonumpy_encode as numpy_encode, json_nonumpy_obj_hook as json_numpy_obj_hook\nelse:\n\tNUMPY_MODE = True\n\t# from .np import dumps, dump, loads, load, numpy_encode, NumpyEncoder, json_numpy_obj_hook\n\t# from .np_utils import encode_scalars_inplace\n\n\n'"
json_tricks/_version.py,0,"b""\nVERSION = '3.15.2'\n\n"""
json_tricks/comment.py,0,"b'\nfrom re import findall\n\n\ndef strip_comment_line_with_symbol(line, start):\n\tparts = line.split(start)\n\tcounts = [len(findall(r\'(?:^|[^""\\\\]|(?:\\\\\\\\|\\\\"")+)("")(?!"")\', part)) for part in parts]\n\ttotal = 0\n\tfor nr, count in enumerate(counts):\n\t\ttotal += count\n\t\tif total % 2 == 0:\n\t\t\treturn start.join(parts[:nr+1]).rstrip()\n\telse:\n\t\treturn line.rstrip()\n\n\ndef strip_comments(string, comment_symbols=frozenset((\'#\', \'//\'))):\n\t""""""\n\t:param string: A string containing json with comments started by comment_symbols.\n\t:param comment_symbols: Iterable of symbols that start a line comment (default # or //).\n\t:return: The string with the comments removed.\n\t""""""\n\tlines = string.splitlines()\n\tfor k in range(len(lines)):\n\t\tfor symbol in comment_symbols:\n\t\t\tlines[k] = strip_comment_line_with_symbol(lines[k], start=symbol)\n\treturn \'\\n\'.join(lines)\n\n\n'"
json_tricks/decoders.py,0,"b'import warnings\nfrom collections import OrderedDict\nfrom datetime import datetime, date, time, timedelta\nfrom decimal import Decimal\nfrom fractions import Fraction\n\nfrom json_tricks import NoEnumException, NoPandasException, NoNumpyException\nfrom .utils import ClassInstanceHookBase, nested_index, str_type, gzip_decompress, filtered_wrapper\n\n\nclass DuplicateJsonKeyException(Exception):\n\t"""""" Trying to load a json map which contains duplicate keys, but allow_duplicates is False """"""\n\n\nclass TricksPairHook(object):\n\t""""""\n\tHook that converts json maps to the appropriate python type (dict or OrderedDict)\n\tand then runs any number of hooks on the individual maps.\n\t""""""\n\tdef __init__(self, ordered=True, obj_pairs_hooks=None, allow_duplicates=True, properties=None):\n\t\t""""""\n\t\t:param ordered: True if maps should retain their ordering.\n\t\t:param obj_pairs_hooks: An iterable of hooks to apply to elements.\n\t\t""""""\n\t\tself.properties = properties or {}\n\t\tself.map_type = OrderedDict\n\t\tif not ordered:\n\t\t\tself.map_type = dict\n\t\tself.obj_pairs_hooks = []\n\t\tif obj_pairs_hooks:\n\t\t\tself.obj_pairs_hooks = list(filtered_wrapper(hook) for hook in obj_pairs_hooks)\n\t\tself.allow_duplicates = allow_duplicates\n\n\tdef __call__(self, pairs):\n\t\tif not self.allow_duplicates:\n\t\t\tknown = set()\n\t\t\tfor key, value in pairs:\n\t\t\t\tif key in known:\n\t\t\t\t\traise DuplicateJsonKeyException((\'Trying to load a json map which contains a \' +\n\t\t\t\t\t\t\'duplicate key ""{0:}"" (but allow_duplicates is False)\').format(key))\n\t\t\t\tknown.add(key)\n\t\tmap = self.map_type(pairs)\n\t\tfor hook in self.obj_pairs_hooks:\n\t\t\tmap = hook(map, properties=self.properties)\n\t\treturn map\n\n\ndef json_date_time_hook(dct):\n\t""""""\n\tReturn an encoded date, time, datetime or timedelta to it\'s python representation, including optional timezone.\n\n\t:param dct: (dict) json encoded date, time, datetime or timedelta\n\t:return: (date/time/datetime/timedelta obj) python representation of the above\n\t""""""\n\tdef get_tz(dct):\n\t\tif not \'tzinfo\' in dct:\n\t\t\treturn None\n\t\ttry:\n\t\t\timport pytz\n\t\texcept ImportError as err:\n\t\t\traise ImportError((\'Tried to load a json object which has a timezone-aware (date)time. \'\n\t\t\t\t\'However, `pytz` could not be imported, so the object could not be loaded. \'\n\t\t\t\t\'Error: {0:}\').format(str(err)))\n\t\treturn pytz.timezone(dct[\'tzinfo\'])\n\n\tif not isinstance(dct, dict):\n\t\treturn dct\n\tif \'__date__\' in dct:\n\t\treturn date(year=dct.get(\'year\', 0), month=dct.get(\'month\', 0), day=dct.get(\'day\', 0))\n\telif \'__time__\' in dct:\n\t\ttzinfo = get_tz(dct)\n\t\treturn time(hour=dct.get(\'hour\', 0), minute=dct.get(\'minute\', 0), second=dct.get(\'second\', 0),\n\t\t\tmicrosecond=dct.get(\'microsecond\', 0), tzinfo=tzinfo)\n\telif \'__datetime__\' in dct:\n\t\ttzinfo = get_tz(dct)\n\t\tdt = datetime(year=dct.get(\'year\', 0), month=dct.get(\'month\', 0), day=dct.get(\'day\', 0),\n\t\t\thour=dct.get(\'hour\', 0), minute=dct.get(\'minute\', 0), second=dct.get(\'second\', 0),\n\t\t\tmicrosecond=dct.get(\'microsecond\', 0))\n\t\tif tzinfo is None:\n\t\t\treturn dt\n\t\treturn tzinfo.localize(dt)\n\telif \'__timedelta__\' in dct:\n\t\treturn timedelta(days=dct.get(\'days\', 0), seconds=dct.get(\'seconds\', 0),\n\t\t\tmicroseconds=dct.get(\'microseconds\', 0))\n\treturn dct\n\n\ndef json_complex_hook(dct):\n\t""""""\n\tReturn an encoded complex number to it\'s python representation.\n\n\t:param dct: (dict) json encoded complex number (__complex__)\n\t:return: python complex number\n\t""""""\n\tif not isinstance(dct, dict):\n\t\treturn dct\n\tif not \'__complex__\' in dct:\n\t\treturn dct\n\tparts = dct[\'__complex__\']\n\tassert len(parts) == 2\n\treturn parts[0] + parts[1] * 1j\n\n\ndef numeric_types_hook(dct):\n\tif not isinstance(dct, dict):\n\t\treturn dct\n\tif \'__decimal__\' in dct:\n\t\treturn Decimal(dct[\'__decimal__\'])\n\tif \'__fraction__\' in dct:\n\t\treturn Fraction(numerator=dct[\'numerator\'], denominator=dct[\'denominator\'])\n\treturn dct\n\n\ndef noenum_hook(dct):\n\tif isinstance(dct, dict) and \'__enum__\' in dct:\n\t\traise NoEnumException((\'Trying to decode a map which appears to represent a enum \'\n\t\t\t\'data structure, but enum support is not enabled, perhaps it is not installed.\'))\n\treturn dct\n\n\ndef pathlib_hook(dct):\n\tif not isinstance(dct, dict):\n\t\treturn dct\n\tif not \'__pathlib__\' in dct:\n\t\treturn dct\n\tfrom pathlib import Path\n\treturn Path(dct[\'__pathlib__\'])\n\n\ndef nopathlib_hook(dct):\n\tif isinstance(dct, dict) and \'__pathlib__\' in dct:\n\t\traise NoPathlib((\'Trying to decode a map which appears to represent a \'\n\t\t\t\t\t\t\'pathlib.Path data structure, but pathlib support \'\n\t\t\t\t\t\t\'is not enabled.\'))\n\treturn dct\n\n\nclass EnumInstanceHook(ClassInstanceHookBase):\n\t""""""\n\tThis hook tries to convert json encoded by enum_instance_encode back to it\'s original instance.\n\tIt only works if the environment is the same, e.g. the enum is similarly importable and hasn\'t changed.\n\t""""""\n\tdef __call__(self, dct, properties=None):\n\t\tif not isinstance(dct, dict):\n\t\t\treturn dct\n\t\tif \'__enum__\' not in dct:\n\t\t\treturn dct\n\t\tcls_lookup_map = properties.get(\'cls_lookup_map\', {})\n\t\tmod, name = dct[\'__enum__\'][\'__enum_instance_type__\']\n\t\tCls = self.get_cls_from_instance_type(mod, name, cls_lookup_map=cls_lookup_map)\n\t\treturn Cls[dct[\'__enum__\'][\'name\']]\n\n\nclass ClassInstanceHook(ClassInstanceHookBase):\n\t""""""\n\tThis hook tries to convert json encoded by class_instance_encoder back to it\'s original instance.\n\tIt only works if the environment is the same, e.g. the class is similarly importable and hasn\'t changed.\n\t""""""\n\tdef __call__(self, dct, properties=None):\n\t\tif not isinstance(dct, dict):\n\t\t\treturn dct\n\t\tif \'__instance_type__\' not in dct:\n\t\t\treturn dct\n\t\tcls_lookup_map = properties.get(\'cls_lookup_map\', {}) or {}\n\t\tmod, name = dct[\'__instance_type__\']\n\t\tCls = self.get_cls_from_instance_type(mod, name, cls_lookup_map=cls_lookup_map)\n\t\ttry:\n\t\t\tobj = Cls.__new__(Cls)\n\t\texcept TypeError:\n\t\t\traise TypeError((\'problem while decoding instance of ""{0:s}""; this instance has a special \'\n\t\t\t\t\'__new__ method and can\\\'t be restored\').format(name))\n\t\tif hasattr(obj, \'__json_decode__\'):\n\t\t\tproperties = {}\n\t\t\tif \'slots\' in dct:\n\t\t\t\tproperties.update(dct[\'slots\'])\n\t\t\tif \'attributes\' in dct:\n\t\t\t\tproperties.update(dct[\'attributes\'])\n\t\t\tobj.__json_decode__(**properties)\n\t\telse:\n\t\t\tif \'slots\' in dct:\n\t\t\t\tfor slot,value in dct[\'slots\'].items():\n\t\t\t\t\tsetattr(obj, slot, value)\n\t\t\tif \'attributes\' in dct:\n\t\t\t\tobj.__dict__ = dict(dct[\'attributes\'])\n\t\treturn obj\n\n\ndef json_set_hook(dct):\n\t""""""\n\tReturn an encoded set to it\'s python representation.\n\t""""""\n\tif not isinstance(dct, dict):\n\t\treturn dct\n\tif \'__set__\' not in dct:\n\t\treturn dct\n\treturn set((tuple(item) if isinstance(item, list) else item) for item in dct[\'__set__\'])\n\n\ndef pandas_hook(dct):\n\tif not isinstance(dct, dict):\n\t\treturn dct\n\tif \'__pandas_dataframe__\' not in dct and \'__pandas_series__\' not in dct:\n\t\treturn dct\n\tif \'__pandas_dataframe__\' in dct:\n\t\ttry:\n\t\t\tfrom pandas import DataFrame\n\t\texcept ImportError:\n\t\t\traise NoPandasException(\'Trying to decode a map which appears to repr esent a pandas data structure, but pandas appears not to be installed.\')\n\t\tfrom numpy import dtype, array\n\t\tmeta = dct.pop(\'__pandas_dataframe__\')\n\t\tindx = dct.pop(\'index\') if \'index\' in dct else None\n\t\tdtypes = dict((colname, dtype(tp)) for colname, tp in zip(meta[\'column_order\'], meta[\'types\']))\n\t\tdata = OrderedDict()\n\t\tfor name, col in dct.items():\n\t\t\tdata[name] = array(col, dtype=dtypes[name])\n\t\treturn DataFrame(\n\t\t\tdata=data,\n\t\t\tindex=indx,\n\t\t\tcolumns=meta[\'column_order\'],\n\t\t\t# mixed `dtypes` argument not supported, so use duct of numpy arrays\n\t\t)\n\telif \'__pandas_series__\' in dct:\n\t\tfrom pandas import Series\n\t\tfrom numpy import dtype, array\n\t\tmeta = dct.pop(\'__pandas_series__\')\n\t\tindx = dct.pop(\'index\') if \'index\' in dct else None\n\t\treturn Series(\n\t\t\tdata=dct[\'data\'],\n\t\t\tindex=indx,\n\t\t\tname=meta[\'name\'],\n\t\t\tdtype=dtype(meta[\'type\']),\n\t\t)\n\treturn dct\t# impossible\n\n\ndef nopandas_hook(dct):\n\tif isinstance(dct, dict) and (\'__pandas_dataframe__\' in dct or \'__pandas_series__\' in dct):\n\t\traise NoPandasException((\'Trying to decode a map which appears to represent a pandas \'\n\t\t\t\'data structure, but pandas support is not enabled, perhaps it is not installed.\'))\n\treturn dct\n\n\ndef json_numpy_obj_hook(dct):\n\t""""""\n\tReplace any numpy arrays previously encoded by NumpyEncoder to their proper\n\tshape, data type and data.\n\n\t:param dct: (dict) json encoded ndarray\n\t:return: (ndarray) if input was an encoded ndarray\n\t""""""\n\tif not isinstance(dct, dict):\n\t\treturn dct\n\tif not \'__ndarray__\' in dct:\n\t\treturn dct\n\ttry:\n\t\timport numpy\n\texcept ImportError:\n\t\traise NoNumpyException(\'Trying to decode a map which appears to represent a numpy \'\n\t\t\t\'array, but numpy appears not to be installed.\')\n\torder = None\n\tif \'Corder\' in dct:\n\t\torder = \'C\' if dct[\'Corder\'] else \'F\'\n\tdata_json = dct[\'__ndarray__\']\n\tshape = tuple(dct[\'shape\'])\n\tnptype = dct[\'dtype\']\n\tif shape:\n\t\tif nptype == \'object\':\n\t\t\treturn _lists_of_obj_to_ndarray(data_json, order, shape, nptype)\n\t\tif isinstance(data_json, str_type):\n\t\t\treturn _bin_str_to_ndarray(data_json, order, shape, nptype)\n\t\telse:\n\t\t\treturn _lists_of_numbers_to_ndarray(data_json, order, shape, nptype)\n\telse:\n\t\treturn _scalar_to_numpy(data_json, nptype)\n\n\ndef _bin_str_to_ndarray(data, order, shape, dtype):\n\t""""""\n\tFrom base64 encoded, gzipped binary data to ndarray.\n\t""""""\n\tfrom base64 import standard_b64decode\n\tfrom numpy import frombuffer\n\n\tassert order in [None, \'C\'], \'specifying different memory order is not (yet) supported \' \\\n\t\t\'for binary numpy format (got order = {})\'.format(order)\n\tif data.startswith(\'b64.gz:\'):\n\t\tdata = standard_b64decode(data[7:])\n\t\tdata = gzip_decompress(data)\n\telif data.startswith(\'b64:\'):\n\t\tdata = standard_b64decode(data[4:])\n\telse:\n\t\traise ValueError(\'found numpy array buffer, but did not understand header; supported: b64 or b64.gz\')\n\tdata = frombuffer(data, dtype=dtype)\n\treturn data.reshape(shape)\n\n\ndef _lists_of_numbers_to_ndarray(data, order, shape, dtype):\n\t""""""\n\tFrom nested list of numbers to ndarray.\n\t""""""\n\tfrom numpy import asarray\n\tarr = asarray(data, dtype=dtype, order=order)\n\tif shape != arr.shape:\n\t\twarnings.warn(\'size mismatch decoding numpy array: expected {}, got {}\'.format(shape, arr.shape))\n\treturn arr\n\n\ndef _lists_of_obj_to_ndarray(data, order, shape, dtype):\n\t""""""\n\tFrom nested list of objects (that aren\'t native numpy numbers) to ndarray.\n\t""""""\n\tfrom numpy import empty, ndindex\n\tarr = empty(shape, dtype=dtype, order=order)\n\tdec_data = data\n\tfor indx in ndindex(arr.shape):\n\t\tarr[indx] = nested_index(dec_data, indx)\n\treturn arr\n\n\ndef _scalar_to_numpy(data, dtype):\n\t""""""\n\tFrom scalar value to numpy type.\n\t""""""\n\timport numpy as nptypes\n\tdtype = getattr(nptypes, dtype)\n\treturn dtype(data)\n\n\ndef json_nonumpy_obj_hook(dct):\n\t""""""\n\tThis hook has no effect except to check if you\'re trying to decode numpy arrays without support, and give you a useful message.\n\t""""""\n\tif isinstance(dct, dict) and \'__ndarray__\' in dct:\n\t\traise NoNumpyException((\'Trying to decode a map which appears to represent a numpy array, \'\n\t\t\t\'but numpy support is not enabled, perhaps it is not installed.\'))\n\treturn dct\n\n\n'"
json_tricks/encoders.py,0,"b'import warnings\nfrom datetime import datetime, date, time, timedelta\nfrom decimal import Decimal\nfrom fractions import Fraction\nfrom functools import wraps\nfrom json import JSONEncoder\nfrom sys import version, stderr\n\nfrom .utils import hashodict, get_module_name_from_object, NoEnumException, NoPandasException, \\\n\tNoNumpyException, str_type, JsonTricksDeprecation, gzip_compress, filtered_wrapper\n\n\ndef _fallback_wrapper(encoder):\n\t""""""\n\tThis decorator makes an encoder run only if the current object hasn\'t been changed yet.\n\t(Changed-ness is checked with is_changed which is based on identity with `id`).\n\t""""""\n\t@wraps(encoder)\n\tdef fallback_encoder(obj, is_changed, **kwargs):\n\t\tif is_changed:\n\t\t\treturn obj\n\t\treturn encoder(obj, is_changed=is_changed, **kwargs)\n\treturn fallback_encoder\n\n\ndef fallback_ignore_unknown(obj, is_changed=None, fallback_value=None):\n\t""""""\n\tThis encoder returns None if the object isn\'t changed by another encoder and isn\'t a primitive.\n\t""""""\n\tif is_changed:\n\t\treturn obj\n\tif obj is None or isinstance(obj, (int, float, str_type, bool, list, dict)):\n\t\treturn obj\n\treturn fallback_value\n\n\nclass TricksEncoder(JSONEncoder):\n\t""""""\n\tEncoder that runs any number of encoder functions or instances on\n\tthe objects that are being encoded.\n\n\tEach encoder should make any appropriate changes and return an object,\n\tchanged or not. This will be passes to the other encoders.\n\t""""""\n\tdef __init__(self, obj_encoders=None, silence_typeerror=False, primitives=False, fallback_encoders=(), properties=None, **json_kwargs):\n\t\t""""""\n\t\t:param obj_encoders: An iterable of functions or encoder instances to try.\n\t\t:param silence_typeerror: DEPRECATED - If set to True, ignore the TypeErrors that Encoder instances throw (default False).\n\t\t""""""\n\t\tif silence_typeerror and not getattr(TricksEncoder, \'_deprecated_silence_typeerror\'):\n\t\t\tTricksEncoder._deprecated_silence_typeerror = True\n\t\t\tstderr.write(\'TricksEncoder.silence_typeerror is deprecated and may be removed in a future version\\n\')\n\t\tself.obj_encoders = []\n\t\tif obj_encoders:\n\t\t\tself.obj_encoders = list(obj_encoders)\n\t\tself.obj_encoders.extend(_fallback_wrapper(encoder) for encoder in list(fallback_encoders))\n\t\tself.obj_encoders = [filtered_wrapper(enc) for enc in self.obj_encoders]\n\t\tself.silence_typeerror = silence_typeerror\n\t\tself.properties = properties\n\t\tself.primitives = primitives\n\t\tsuper(TricksEncoder, self).__init__(**json_kwargs)\n\n\tdef default(self, obj, *args, **kwargs):\n\t\t""""""\n\t\tThis is the method of JSONEncoders that is called for each object; it calls\n\t\tall the encoders with the previous one\'s output used as input.\n\n\t\tIt works for Encoder instances, but they are expected not to throw\n\t\t`TypeError` for unrecognized types (the super method does that by default).\n\n\t\tIt never calls the `super` method so if there are non-primitive types\n\t\tleft at the end, you\'ll get an encoding error.\n\t\t""""""\n\t\tprev_id = id(obj)\n\t\tfor encoder in self.obj_encoders:\n\t\t\tobj = encoder(obj, primitives=self.primitives, is_changed=id(obj) != prev_id, properties=self.properties)\n\t\tif id(obj) == prev_id:\n\t\t\traise TypeError((\'Object of type {0:} could not be encoded by {1:} using encoders [{2:s}]. \'\n\t\t\t\t\'You can add an encoders for this type using `extra_obj_encoders`. If you want to \\\'skip\\\' this \'\n\t\t\t\t\'object, consider using `fallback_encoders` like `str` or `lambda o: None`.\').format(\n\t\t\t\t\ttype(obj), self.__class__.__name__, \', \'.join(str(encoder) for encoder in self.obj_encoders)))\n\t\treturn obj\n\n\ndef json_date_time_encode(obj, primitives=False):\n\t""""""\n\tEncode a date, time, datetime or timedelta to a string of a json dictionary, including optional timezone.\n\n\t:param obj: date/time/datetime/timedelta obj\n\t:return: (dict) json primitives representation of date, time, datetime or timedelta\n\t""""""\n\tif primitives and isinstance(obj, (date, time, datetime)):\n\t\treturn obj.isoformat()\n\tif isinstance(obj, datetime):\n\t\tdct = hashodict([(\'__datetime__\', None), (\'year\', obj.year), (\'month\', obj.month),\n\t\t\t(\'day\', obj.day), (\'hour\', obj.hour), (\'minute\', obj.minute),\n\t\t\t(\'second\', obj.second), (\'microsecond\', obj.microsecond)])\n\t\tif obj.tzinfo:\n\t\t\tdct[\'tzinfo\'] = obj.tzinfo.zone\n\telif isinstance(obj, date):\n\t\tdct = hashodict([(\'__date__\', None), (\'year\', obj.year), (\'month\', obj.month), (\'day\', obj.day)])\n\telif isinstance(obj, time):\n\t\tdct = hashodict([(\'__time__\', None), (\'hour\', obj.hour), (\'minute\', obj.minute),\n\t\t\t(\'second\', obj.second), (\'microsecond\', obj.microsecond)])\n\t\tif obj.tzinfo:\n\t\t\tdct[\'tzinfo\'] = obj.tzinfo.zone\n\telif isinstance(obj, timedelta):\n\t\tif primitives:\n\t\t\treturn obj.total_seconds()\n\t\telse:\n\t\t\tdct = hashodict([(\'__timedelta__\', None), (\'days\', obj.days), (\'seconds\', obj.seconds),\n\t\t\t\t(\'microseconds\', obj.microseconds)])\n\telse:\n\t\treturn obj\n\tfor key, val in tuple(dct.items()):\n\t\tif not key.startswith(\'__\') and not val:\n\t\t\tdel dct[key]\n\treturn dct\n\n\ndef enum_instance_encode(obj, primitives=False, with_enum_value=False):\n\t""""""Encodes an enum instance to json. Note that it can only be recovered if the environment allows the enum to be\n\timported in the same way.\n\t:param primitives: If true, encode the enum values as primitive (more readable, but cannot be restored automatically).\n\t:param with_enum_value: If true, the value of the enum is also exported (it is not used during import, as it should be constant).\n\t""""""\n\tfrom enum import Enum\n\tif not isinstance(obj, Enum):\n\t\treturn obj\n\tif primitives:\n\t\treturn {obj.name: obj.value}\n\tmod = get_module_name_from_object(obj)\n\trepresentation = dict(\n\t\t__enum__=dict(\n\t\t\t# Don\'t use __instance_type__ here since enums members cannot be created with __new__\n\t\t\t# Ie we can\'t rely on class deserialization to read them.\n\t\t\t__enum_instance_type__=[mod, type(obj).__name__],\n\t\t\tname=obj.name,\n\t\t),\n\t)\n\tif with_enum_value:\n\t\trepresentation[\'__enum__\'][\'value\'] = obj.value\n\treturn representation\n\n\ndef noenum_instance_encode(obj, primitives=False):\n\tif type(obj.__class__).__name__ == \'EnumMeta\':\n\t\traise NoEnumException((\'Trying to encode an object of type {0:} which appears to be \'\n\t\t\t\'an enum, but enum support is not enabled, perhaps it is not installed.\').format(type(obj)))\n\treturn obj\n\n\ndef class_instance_encode(obj, primitives=False):\n\t""""""\n\tEncodes a class instance to json. Note that it can only be recovered if the environment allows the class to be\n\timported in the same way.\n\t""""""\n\tif isinstance(obj, list) or isinstance(obj, dict):\n\t\treturn obj\n\tif hasattr(obj, \'__class__\') and (hasattr(obj, \'__dict__\') or hasattr(obj, \'__slots__\')):\n\t\tif not hasattr(obj, \'__new__\'):\n\t\t\traise TypeError(\'class ""{0:s}"" does not have a __new__ method; \'.format(obj.__class__) +\n\t\t\t\t(\'perhaps it is an old-style class not derived from `object`; add `object` as a base class to encode it.\'\n\t\t\t\t\tif (version[:2] == \'2.\') else \'this should not happen in Python3\'))\n\t\tif type(obj) == type(lambda: 0):\n\t\t\traise TypeError(\'instance ""{0:}"" of class ""{1:}"" cannot be encoded because it appears to be a lambda or function.\'\n\t\t\t\t.format(obj, obj.__class__))\n\t\ttry:\n\t\t\tobj.__new__(obj.__class__)\n\t\texcept TypeError:\n\t\t\traise TypeError((\'instance ""{0:}"" of class ""{1:}"" cannot be encoded, perhaps because it\\\'s __new__ method \'\n\t\t\t\t\'cannot be called because it requires extra parameters\').format(obj, obj.__class__))\n\t\tmod = get_module_name_from_object(obj)\n\t\tif mod == \'threading\':\n\t\t\t# In Python2, threading objects get serialized, which is probably unsafe\n\t\t\treturn obj\n\t\tname = obj.__class__.__name__\n\t\tif hasattr(obj, \'__json_encode__\'):\n\t\t\tattrs = obj.__json_encode__()\n\t\t\tif primitives:\n\t\t\t\treturn attrs\n\t\t\telse:\n\t\t\t\treturn hashodict(((\'__instance_type__\', (mod, name)), (\'attributes\', attrs)))\n\t\tdct = hashodict([(\'__instance_type__\',(mod, name))])\n\t\tif hasattr(obj, \'__slots__\'):\n\t\t\tslots = obj.__slots__\n\t\t\tif isinstance(slots, str):\n\t\t\t\tslots = [slots]\n\t\t\tslots = list(item for item in slots if item != \'__dict__\')\n\t\t\tdct[\'slots\'] = hashodict([])\n\t\t\tfor s in slots:\n\t\t\t\tdct[\'slots\'][s] = getattr(obj, s)\n\t\tif hasattr(obj, \'__dict__\'):\n\t\t\tdct[\'attributes\'] = hashodict(obj.__dict__)\n\t\tif primitives:\n\t\t\tattrs = dct.get(\'attributes\',{})\n\t\t\tattrs.update(dct.get(\'slots\',{}))\n\t\t\treturn attrs\n\t\telse:\n\t\t\treturn dct\n\treturn obj\n\n\ndef json_complex_encode(obj, primitives=False):\n\t""""""\n\tEncode a complex number as a json dictionary of it\'s real and imaginary part.\n\n\t:param obj: complex number, e.g. `2+1j`\n\t:return: (dict) json primitives representation of `obj`\n\t""""""\n\tif isinstance(obj, complex):\n\t\tif primitives:\n\t\t\treturn [obj.real, obj.imag]\n\t\telse:\n\t\t\treturn hashodict(__complex__=[obj.real, obj.imag])\n\treturn obj\n\n\ndef numeric_types_encode(obj, primitives=False):\n\t""""""\n\tEncode Decimal and Fraction.\n\n\t:param primitives: Encode decimals and fractions as standard floats. You may lose precision. If you do this, you may need to enable `allow_nan` (decimals always allow NaNs but floats do not).\n\t""""""\n\tif isinstance(obj, Decimal):\n\t\tif primitives:\n\t\t\treturn float(obj)\n\t\telse:\n\t\t\treturn {\n\t\t\t\t\'__decimal__\': str(obj.canonical()),\n\t\t\t}\n\tif isinstance(obj, Fraction):\n\t\tif primitives:\n\t\t\treturn float(obj)\n\t\telse:\n\t\t\treturn hashodict((\n\t\t\t\t(\'__fraction__\', True),\n\t\t\t\t(\'numerator\', obj.numerator),\n\t\t\t\t(\'denominator\', obj.denominator),\n\t\t\t))\n\treturn obj\n\n\ndef pathlib_encode(obj, primitives=False):\n    from pathlib import Path\n    if not isinstance(obj, Path):\n        return obj\n\n    if primitives:\n        return str(obj)\n\n    return {\'__pathlib__\': str(obj)}\n\n\nclass ClassInstanceEncoder(JSONEncoder):\n\t""""""\n\tSee `class_instance_encoder`.\n\t""""""\n\t# Not covered in tests since `class_instance_encode` is recommended way.\n\tdef __init__(self, obj, encode_cls_instances=True, **kwargs):\n\t\tself.encode_cls_instances = encode_cls_instances\n\t\tsuper(ClassInstanceEncoder, self).__init__(obj, **kwargs)\n\n\tdef default(self, obj, *args, **kwargs):\n\t\tif self.encode_cls_instances:\n\t\t\tobj = class_instance_encode(obj)\n\t\treturn super(ClassInstanceEncoder, self).default(obj, *args, **kwargs)\n\n\ndef json_set_encode(obj, primitives=False):\n\t""""""\n\tEncode python sets as dictionary with key __set__ and a list of the values.\n\n\tTry to sort the set to get a consistent json representation, use arbitrary order if the data is not ordinal.\n\t""""""\n\tif isinstance(obj, set):\n\t\ttry:\n\t\t\trepr = sorted(obj)\n\t\texcept Exception:\n\t\t\trepr = list(obj)\n\t\tif primitives:\n\t\t\treturn repr\n\t\telse:\n\t\t\treturn hashodict(__set__=repr)\n\treturn obj\n\n\ndef pandas_encode(obj, primitives=False):\n\tfrom pandas import DataFrame, Series\n\tif isinstance(obj, DataFrame):\n\t\trepr = hashodict()\n\t\tif not primitives:\n\t\t\trepr[\'__pandas_dataframe__\'] = hashodict((\n\t\t\t\t(\'column_order\', tuple(obj.columns.values)),\n\t\t\t\t(\'types\', tuple(str(dt) for dt in obj.dtypes)),\n\t\t\t))\n\t\trepr[\'index\'] = tuple(obj.index.values)\n\t\tfor k, name in enumerate(obj.columns.values):\n\t\t\trepr[name] = tuple(obj.iloc[:, k].values)\n\t\treturn repr\n\tif isinstance(obj, Series):\n\t\trepr = hashodict()\n\t\tif not primitives:\n\t\t\trepr[\'__pandas_series__\'] = hashodict((\n\t\t\t\t(\'name\', str(obj.name)),\n\t\t\t\t(\'type\', str(obj.dtype)),\n\t\t\t))\n\t\trepr[\'index\'] = tuple(obj.index.values)\n\t\trepr[\'data\'] = tuple(obj.values)\n\t\treturn repr\n\treturn obj\n\n\ndef nopandas_encode(obj):\n\tif (\'DataFrame\' in getattr(obj.__class__, \'__name__\', \'\') or \'Series\' in getattr(obj.__class__, \'__name__\', \'\')) \\\n\t\t\tand \'pandas.\' in getattr(obj.__class__, \'__module__\', \'\'):\n\t\traise NoPandasException((\'Trying to encode an object of type {0:} which appears to be \'\n\t\t\t\'a numpy array, but numpy support is not enabled, perhaps it is not installed.\').format(type(obj)))\n\treturn obj\n\n\ndef numpy_encode(obj, primitives=False, properties=None):\n\t""""""\n\tEncodes numpy `ndarray`s as lists with meta data.\n\n\tEncodes numpy scalar types as Python equivalents. Special encoding is not possible,\n\tbecause int64 (in py2) and float64 (in py2 and py3) are subclasses of primitives,\n\twhich never reach the encoder.\n\n\t:param primitives: If True, arrays are serialized as (nested) lists without meta info.\n\t""""""\n\tfrom numpy import ndarray, generic\n\tif isinstance(obj, ndarray):\n\t\tif primitives:\n\t\t\treturn obj.tolist()\n\t\telse:\n\t\t\tproperties = properties or {}\n\t\t\tuse_compact = properties.get(\'ndarray_compact\', None)\n\t\t\tjson_compression = bool(properties.get(\'compression\', False))\n\t\t\tif use_compact is None and json_compression and not getattr(numpy_encode, \'_warned_compact\', False):\n\t\t\t\tnumpy_encode._warned_compact = True\n\t\t\t\twarnings.warn(\'storing ndarray in text format while compression in enabled; in the next major version \'\n\t\t\t\t\t\'of json_tricks, the default when using compression will change to compact mode; to already use \'\n\t\t\t\t\t\'that smaller format, pass `properties={""ndarray_compact"": True}` to json_tricks.dump; \'\n\t\t\t\t\t\'to silence this warning, pass `properties={""ndarray_compact"": False}`; \'\n\t\t\t\t\t\'see issue https://github.com/mverleg/pyjson_tricks/issues/73\', JsonTricksDeprecation)\n\t\t\t# Property \'use_compact\' may also be an integer, in which case it\'s the number of\n\t\t\t# elements from which compact storage is used.\n\t\t\tif isinstance(use_compact, int) and not isinstance(use_compact, bool):\n\t\t\t\tuse_compact = obj.size >= use_compact\n\t\t\tif use_compact:\n\t\t\t\t# If the overall json file is compressed, then don\'t compress the array.\n\t\t\t\tdata_json = _ndarray_to_bin_str(obj, do_compress=not json_compression)\n\t\t\telse:\n\t\t\t\tdata_json = obj.tolist()\n\t\t\tdct = hashodict((\n\t\t\t\t(\'__ndarray__\', data_json),\n\t\t\t\t(\'dtype\', str(obj.dtype)),\n\t\t\t\t(\'shape\', obj.shape),\n\t\t\t))\n\t\t\tif len(obj.shape) > 1:\n\t\t\t\tdct[\'Corder\'] = obj.flags[\'C_CONTIGUOUS\']\n\t\t\treturn dct\n\telif isinstance(obj, generic):\n\t\tif NumpyEncoder.SHOW_SCALAR_WARNING:\n\t\t\tNumpyEncoder.SHOW_SCALAR_WARNING = False\n\t\t\twarnings.warn(\'json-tricks: numpy scalar serialization is experimental and may work differently in future versions\')\n\t\treturn obj.item()\n\treturn obj\n\n\ndef _ndarray_to_bin_str(array, do_compress):\n\t""""""\n\tFrom ndarray to base64 encoded, gzipped binary data.\n\t""""""\n\tfrom base64 import standard_b64encode\n\tassert array.flags[\'C_CONTIGUOUS\'], \'only C memory order is (currently) supported for compact ndarray format\'\n\n\toriginal_size = array.size * array.itemsize\n\theader = \'b64:\'\n\tdata = array.data\n\tif do_compress:\n\t\tsmall = gzip_compress(data, compresslevel=9)\n\t\tif len(small) < 0.9 * original_size and len(small) < original_size - 8:\n\t\t\theader = \'b64.gz:\'\n\t\t\tdata = small\n\tdata = standard_b64encode(data)\n\treturn header + data.decode(\'ascii\')\n\n\nclass NumpyEncoder(ClassInstanceEncoder):\n\t""""""\n\tJSON encoder for numpy arrays.\n\t""""""\n\tSHOW_SCALAR_WARNING = True\t# show a warning that numpy scalar serialization is experimental\n\n\tdef default(self, obj, *args, **kwargs):\n\t\t""""""\n\t\tIf input object is a ndarray it will be converted into a dict holding\n\t\tdata type, shape and the data. The object can be restored using json_numpy_obj_hook.\n\t\t""""""\n\t\twarnings.warn(\'`NumpyEncoder` is deprecated, use `numpy_encode`\', JsonTricksDeprecation)\n\t\tobj = numpy_encode(obj)\n\t\treturn super(NumpyEncoder, self).default(obj, *args, **kwargs)\n\n\ndef nonumpy_encode(obj):\n\t""""""\n\tRaises an error for numpy arrays.\n\t""""""\n\tif \'ndarray\' in getattr(obj.__class__, \'__name__\', \'\') and \'numpy.\' in getattr(obj.__class__, \'__module__\', \'\'):\n\t\traise NoNumpyException((\'Trying to encode an object of type {0:} which appears to be \'\n\t\t\t\'a pandas data stucture, but pandas support is not enabled, perhaps it is not installed.\').format(type(obj)))\n\treturn obj\n\n\nclass NoNumpyEncoder(JSONEncoder):\n\t""""""\n\tSee `nonumpy_encode`.\n\t""""""\n\tdef default(self, obj, *args, **kwargs):\n\t\twarnings.warn(\'`NoNumpyEncoder` is deprecated, use `nonumpy_encode`\', JsonTricksDeprecation)\n\t\tobj = nonumpy_encode(obj)\n\t\treturn super(NoNumpyEncoder, self).default(obj, *args, **kwargs)\n'"
json_tricks/nonp.py,0,"b'import warnings\nfrom json import loads as json_loads\nfrom os import fsync\nfrom sys import exc_info\n\nfrom json_tricks.utils import is_py3, dict_default, gzip_compress, gzip_decompress, JsonTricksDeprecation\nfrom .utils import str_type, NoNumpyException  # keep \'unused\' imports\nfrom .comment import strip_comments  # keep \'unused\' imports\n#TODO @mark: imports removed?\nfrom .encoders import TricksEncoder, json_date_time_encode, \\\n\tclass_instance_encode, json_complex_encode, json_set_encode, numeric_types_encode, numpy_encode, \\\n\tnonumpy_encode, nopandas_encode, pandas_encode, noenum_instance_encode, \\\n\tenum_instance_encode, pathlib_encode # keep \'unused\' imports\nfrom .decoders import TricksPairHook, \\\n\tjson_date_time_hook, ClassInstanceHook, \\\n\tjson_complex_hook, json_set_hook, numeric_types_hook, json_numpy_obj_hook, \\\n\tjson_nonumpy_obj_hook, \\\n\tnopandas_hook, pandas_hook, EnumInstanceHook, \\\n\tnoenum_hook, pathlib_hook, nopathlib_hook  # keep \'unused\' imports\n\n\nENCODING = \'UTF-8\'\n\n\n_cih_instance = ClassInstanceHook()\n_eih_instance = EnumInstanceHook()\nDEFAULT_ENCODERS = [json_date_time_encode, json_complex_encode, json_set_encode,\n\t\t\t\t\tnumeric_types_encode, class_instance_encode, ]\nDEFAULT_HOOKS = [json_date_time_hook, json_complex_hook, json_set_hook,\n\t\t\t\tnumeric_types_hook, _cih_instance, ]\n\n\n#TODO @mark: add properties to all built-in encoders (for speed - but it should keep working without)\ntry:\n\timport enum\nexcept ImportError:\n\tDEFAULT_ENCODERS = [noenum_instance_encode,] + DEFAULT_ENCODERS\n\tDEFAULT_HOOKS = [noenum_hook,] + DEFAULT_HOOKS\nelse:\n\tDEFAULT_ENCODERS = [enum_instance_encode,] + DEFAULT_ENCODERS\n\tDEFAULT_HOOKS = [_eih_instance,] + DEFAULT_HOOKS\n\ntry:\n\timport numpy\nexcept ImportError:\n\tDEFAULT_ENCODERS = [nonumpy_encode,] + DEFAULT_ENCODERS\n\tDEFAULT_HOOKS = [json_nonumpy_obj_hook,] + DEFAULT_HOOKS\nelse:\n\t# numpy encode needs to be before complex\n\tDEFAULT_ENCODERS = [numpy_encode,] + DEFAULT_ENCODERS\n\tDEFAULT_HOOKS = [json_numpy_obj_hook,] + DEFAULT_HOOKS\n\ntry:\n\timport pandas\nexcept ImportError:\n\tDEFAULT_ENCODERS = [nopandas_encode,] + DEFAULT_ENCODERS\n\tDEFAULT_HOOKS = [nopandas_hook,] + DEFAULT_HOOKS\nelse:\n\tDEFAULT_ENCODERS = [pandas_encode,] + DEFAULT_ENCODERS\n\tDEFAULT_HOOKS = [pandas_hook,] + DEFAULT_HOOKS\n\ntry:\n\timport pathlib\nexcept:\n\t# No need to include a ""nopathlib_encode"" hook since we would not encounter\n\t# the Path object if pathlib isn\'t available. However, we *could* encounter\n\t# a serialized Path object (produced by a version of Python with pathlib).\n\tDEFAULT_HOOKS = [nopathlib_hook,] + DEFAULT_HOOKS\nelse:\n\tDEFAULT_ENCODERS = [pathlib_encode,] + DEFAULT_ENCODERS\n\tDEFAULT_HOOKS = [pathlib_hook,] + DEFAULT_HOOKS\n\n\nDEFAULT_NONP_ENCODERS = [nonumpy_encode,] + DEFAULT_ENCODERS\t\t# DEPRECATED\nDEFAULT_NONP_HOOKS = [json_nonumpy_obj_hook,] + DEFAULT_HOOKS\t\t# DEPRECATED\n\n\ndef dumps(obj, sort_keys=None, cls=TricksEncoder, obj_encoders=DEFAULT_ENCODERS, extra_obj_encoders=(),\n\t\tprimitives=False, compression=None, allow_nan=False, conv_str_byte=False, fallback_encoders=(),\n\t\tproperties=None, **jsonkwargs):\n\t""""""\n\tConvert a nested data structure to a json string.\n\n\t:param obj: The Python object to convert.\n\t:param sort_keys: Keep this False if you want order to be preserved.\n\t:param cls: The json encoder class to use, defaults to NoNumpyEncoder which gives a warning for numpy arrays.\n\t:param obj_encoders: Iterable of encoders to use to convert arbitrary objects into json-able promitives.\n\t:param extra_obj_encoders: Like `obj_encoders` but on top of them: use this to add encoders without replacing defaults. Since v3.5 these happen before default encoders.\n\t:param fallback_encoders: These are extra `obj_encoders` that 1) are ran after all others and 2) only run if the object hasn\'t yet been changed.\n\t:param allow_nan: Allow NaN and Infinity values, which is a (useful) violation of the JSON standard (default False).\n\t:param conv_str_byte: Try to automatically convert between strings and bytes (assuming utf-8) (default False).\n\t:param properties: A dictionary of properties that is passed to each encoder that will accept it.\n\t:return: The string containing the json-encoded version of obj.\n\n\tOther arguments are passed on to `cls`. Note that `sort_keys` should be false if you want to preserve order.\n\t""""""\n\tif not hasattr(extra_obj_encoders, \'__iter__\'):\n\t\traise TypeError(\'`extra_obj_encoders` should be a tuple in `json_tricks.dump(s)`\')\n\tencoders = tuple(extra_obj_encoders) + tuple(obj_encoders)\n\tproperties = properties or {}\n\tdict_default(properties, \'primitives\', primitives)\n\tdict_default(properties, \'compression\', compression)\n\tdict_default(properties, \'allow_nan\', allow_nan)\n\ttxt = cls(sort_keys=sort_keys, obj_encoders=encoders, allow_nan=allow_nan,\n\t\tprimitives=primitives, fallback_encoders=fallback_encoders,\n\t  \tproperties=properties, **jsonkwargs).encode(obj)\n\tif not is_py3 and isinstance(txt, str):\n\t\ttxt = unicode(txt, ENCODING)\n\tif not compression:\n\t\treturn txt\n\tif compression is True:\n\t\tcompression = 5\n\ttxt = txt.encode(ENCODING)\n\tgzstring = gzip_compress(txt, compresslevel=compression)\n\treturn gzstring\n\n\ndef dump(obj, fp, sort_keys=None, cls=TricksEncoder, obj_encoders=DEFAULT_ENCODERS, extra_obj_encoders=(),\n\t\tprimitives=False, compression=None, force_flush=False, allow_nan=False, conv_str_byte=False,\n\t\tfallback_encoders=(), properties=None, **jsonkwargs):\n\t""""""\n\tConvert a nested data structure to a json string.\n\n\t:param fp: File handle or path to write to.\n\t:param compression: The gzip compression level, or None for no compression.\n\t:param force_flush: If True, flush the file handle used, when possibly also in the operating system (default False).\n\n\tThe other arguments are identical to `dumps`.\n\t""""""\n\tif (isinstance(obj, str_type) or hasattr(obj, \'write\')) and isinstance(fp, (list, dict)):\n\t\traise ValueError(\'json-tricks dump arguments are in the wrong order: provide the data to be serialized before file handle\')\n\ttxt = dumps(obj, sort_keys=sort_keys, cls=cls, obj_encoders=obj_encoders, extra_obj_encoders=extra_obj_encoders,\n\t\tprimitives=primitives, compression=compression, allow_nan=allow_nan, conv_str_byte=conv_str_byte,\n\t\tfallback_encoders=fallback_encoders, properties=properties, **jsonkwargs)\n\tif isinstance(fp, str_type):\n\t\tif compression:\n\t\t\tfh = open(fp, \'wb+\')\n\t\telse:\n\t\t\tfh = open(fp, \'w+\')\n\telse:\n\t\tfh = fp\n\t\tif conv_str_byte:\n\t\t\ttry:\n\t\t\t\tfh.write(b\'\')\n\t\t\texcept TypeError:\n\t\t\t\tpass\n\t\t\t\t# if not isinstance(txt, str_type):\n\t\t\t\t#\t\t# Cannot write bytes, so must be in text mode, but we didn\'t get a text\n\t\t\t\t#\t\tif not compression:\n\t\t\t\t#\t\t\ttxt = txt.decode(ENCODING)\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tfh.write(u\'\')\n\t\t\t\texcept TypeError:\n\t\t\t\t\tif isinstance(txt, str_type):\n\t\t\t\t\t\ttxt = txt.encode(ENCODING)\n\ttry:\n\t\tif compression and \'b\' not in getattr(fh, \'mode\', \'b?\') and not isinstance(txt, str_type):\n\t\t\traise IOError(\'If compression is enabled, the file must be opened in binary mode.\')\n\t\ttry:\n\t\t\tfh.write(txt)\n\t\texcept TypeError as err:\n\t\t\terr.args = (err.args[0] + \'. A possible reason is that the file is not opened in binary mode; \'\n\t\t\t\t\'be sure to set file mode to something like ""wb"".\',)\n\t\t\traise\n\tfinally:\n\t\tif force_flush:\n\t\t\tfh.flush()\n\t\t\ttry:\n\t\t\t\tif fh.fileno() is not None:\n\t\t\t\t\tfsync(fh.fileno())\n\t\t\texcept (ValueError,):\n\t\t\t\tpass\n\t\tif isinstance(fp, str_type):\n\t\t\tfh.close()\n\treturn txt\n\n\ndef loads(string, preserve_order=True, ignore_comments=None, decompression=None, obj_pairs_hooks=DEFAULT_HOOKS,\n\t\textra_obj_pairs_hooks=(), cls_lookup_map=None, allow_duplicates=True, conv_str_byte=False,\n\t\tproperties=None, **jsonkwargs):\n\t""""""\n\tConvert a nested data structure to a json string.\n\n\t:param string: The string containing a json encoded data structure.\n\t:param decode_cls_instances: True to attempt to decode class instances (requires the environment to be similar the the encoding one).\n\t:param preserve_order: Whether to preserve order by using OrderedDicts or not.\n\t:param ignore_comments: Remove comments (starting with # or //).\n\t:param decompression: True to use gzip decompression, False to use raw data, None to automatically determine (default). Assumes utf-8 encoding!\n\t:param obj_pairs_hooks: A list of dictionary hooks to apply.\n\t:param extra_obj_pairs_hooks: Like `obj_pairs_hooks` but on top of them: use this to add hooks without replacing defaults. Since v3.5 these happen before default hooks.\n\t:param cls_lookup_map: If set to a dict, for example ``globals()``, then classes encoded from __main__ are looked up this dict.\n\t:param allow_duplicates: If set to False, an error will be raised when loading a json-map that contains duplicate keys.\n\t:param parse_float: A function to parse strings to integers (e.g. Decimal). There is also `parse_int`.\n\t:param conv_str_byte: Try to automatically convert between strings and bytes (assuming utf-8) (default False).\n\t:return: The string containing the json-encoded version of obj.\n\n\tOther arguments are passed on to json_func.\n\t""""""\n\tif not hasattr(extra_obj_pairs_hooks, \'__iter__\'):\n\t\traise TypeError(\'`extra_obj_pairs_hooks` should be a tuple in `json_tricks.load(s)`\')\n\tif decompression is None:\n\t\tdecompression = isinstance(string, bytes) and string[:2] == b\'\\x1f\\x8b\'\n\tif decompression:\n\t\tstring = gzip_decompress(string).decode(ENCODING)\n\tif not isinstance(string, str_type):\n\t\tif conv_str_byte:\n\t\t\tstring = string.decode(ENCODING)\n\t\telse:\n\t\t\traise TypeError((\'The input was of non-string type ""{0:}"" in `json_tricks.load(s)`. \'\n\t\t\t\t\'Bytes cannot be automatically decoding since the encoding is not known. Recommended \'\n\t\t\t\t\'way is to instead encode the bytes to a string and pass that string to `load(s)`, \'\n\t\t\t\t\'for example bytevar.encode(""utf-8"") if utf-8 is the encoding. Alternatively you can \'\n\t\t\t\t\'force an attempt by passing conv_str_byte=True, but this may cause decoding issues.\')\n\t\t\t\t\t.format(type(string)))\n\tif ignore_comments or ignore_comments is None:\n\t\tnew_string = strip_comments(string)\n\t\tif ignore_comments is None and not getattr(loads, \'_ignore_comments_warned\', False) and string != new_string:\n\t\t\twarnings.warn(\'`json_tricks.load(s)` stripped some comments, but `ignore_comments` was \'\n\t\t\t\t\'not passed; in the next major release, the behaviour when `ignore_comments` is not \'\n\t\t\t\t\'passed will change; it is recommended to explicitly pass `ignore_comments=True` if \'\n\t\t\t\t\'you want to strip comments; see https://github.com/mverleg/pyjson_tricks/issues/74\',\n\t\t\t\tJsonTricksDeprecation)\n\t\t\tloads._ignore_comments_warned = True\n\t\tstring = new_string\n\tproperties = properties or {}\n\tdict_default(properties, \'preserve_order\', preserve_order)\n\tdict_default(properties, \'ignore_comments\', ignore_comments)\n\tdict_default(properties, \'decompression\', decompression)\n\tdict_default(properties, \'cls_lookup_map\', cls_lookup_map)\n\tdict_default(properties, \'allow_duplicates\', allow_duplicates)\n\thooks = tuple(extra_obj_pairs_hooks) + tuple(obj_pairs_hooks)\n\thook = TricksPairHook(ordered=preserve_order, obj_pairs_hooks=hooks, allow_duplicates=allow_duplicates, properties=properties)\n\treturn json_loads(string, object_pairs_hook=hook, **jsonkwargs)\n\n\ndef load(fp, preserve_order=True, ignore_comments=None, decompression=None, obj_pairs_hooks=DEFAULT_HOOKS,\n\t\textra_obj_pairs_hooks=(), cls_lookup_map=None, allow_duplicates=True, conv_str_byte=False,\n\t\tproperties=None, **jsonkwargs):\n\t""""""\n\tConvert a nested data structure to a json string.\n\n\t:param fp: File handle or path to load from.\n\n\tThe other arguments are identical to loads.\n\t""""""\n\ttry:\n\t\tif isinstance(fp, str_type):\n\t\t\tif decompression is not None:\n\t\t\t\topen_binary = bool(decompression)\n\t\t\telse:\n\t\t\t\twith open(fp, \'rb\') as fh:\n\t\t\t\t\t# This attempts to detect gzip mode; gzip should always\n\t\t\t\t\t# have this header, and text json can\'t have it.\n\t\t\t\t\topen_binary = (fh.read(2) == b\'\\x1f\\x8b\')\n\t\t\twith open(fp, \'rb\' if open_binary else \'r\') as fh:\n\t\t\t\tstring = fh.read()\n\t\telse:\n\t\t\tstring = fp.read()\n\texcept UnicodeDecodeError as err:\n\t\t# todo: not covered in tests, is it relevant?\n\t\traise Exception(\'There was a problem decoding the file content. A possible reason is that the file is not \' +\n\t\t\t\'opened  in binary mode; be sure to set file mode to something like ""rb"".\').with_traceback(exc_info()[2])\n\treturn loads(string, preserve_order=preserve_order, ignore_comments=ignore_comments, decompression=decompression,\n\t\tobj_pairs_hooks=obj_pairs_hooks, extra_obj_pairs_hooks=extra_obj_pairs_hooks, cls_lookup_map=cls_lookup_map,\n\t\tallow_duplicates=allow_duplicates, conv_str_byte=conv_str_byte, properties=properties, **jsonkwargs)\n\n\n'"
json_tricks/np.py,0,"b'\n""""""\nThis file exists for backward compatibility reasons.\n""""""\n\nimport warnings\nfrom .nonp import NoNumpyException, DEFAULT_ENCODERS, DEFAULT_HOOKS, dumps, dump, loads, load  # keep \'unused\' imports\nfrom .utils import hashodict, NoPandasException, JsonTricksDeprecation\nfrom .comment import strip_comment_line_with_symbol, strip_comments  # keep \'unused\' imports\nfrom .encoders import TricksEncoder, json_date_time_encode, class_instance_encode, ClassInstanceEncoder, \\\n\tnumpy_encode, NumpyEncoder # keep \'unused\' imports\nfrom .decoders import DuplicateJsonKeyException, TricksPairHook, json_date_time_hook, ClassInstanceHook, \\\n\tjson_complex_hook, json_set_hook, json_numpy_obj_hook  # keep \'unused\' imports\n\ntry:\n\timport numpy\nexcept ImportError:\n\traise NoNumpyException(\'Could not load numpy, maybe it is not installed? If you do not want to use numpy encoding \'\n\t\t\'or decoding, you can import the functions from json_tricks.nonp instead, which do not need numpy.\')\n\n\nwarnings.warn(\'`json_tricks.np` is deprecated, you can import directly from `json_tricks`\', JsonTricksDeprecation)\n\n\nDEFAULT_NP_ENCODERS = [numpy_encode,] + DEFAULT_ENCODERS    # DEPRECATED\nDEFAULT_NP_HOOKS = [json_numpy_obj_hook,] + DEFAULT_HOOKS   # DEPRECATED\n\n\n'"
json_tricks/np_utils.py,0,"b'\n""""""\nThis file exists for backward compatibility reasons.\n""""""\n\nfrom .utils import hashodict, get_scalar_repr, encode_scalars_inplace\nfrom .utils import NoNumpyException\nfrom . import np\n\n# try:\n# \tfrom numpy import generic, complex64, complex128\n# except ImportError:\n# \traise NoNumpyException(\'Could not load numpy, maybe it is not installed?\')\n\n\n'"
json_tricks/utils.py,0,"b'import gzip\nimport io\nimport warnings\nfrom collections import OrderedDict\nfrom functools import partial\nfrom importlib import import_module\nfrom sys import version_info, version\n\n\nclass JsonTricksDeprecation(UserWarning):\n\t"""""" Special deprecation warning because the built-in one is ignored by default """"""\n\tdef __init__(self, msg):\n\t\tsuper(JsonTricksDeprecation, self).__init__(msg)\n\n\nclass hashodict(OrderedDict):\n\t""""""\n\tThis dictionary is hashable. It should NOT be mutated, or all kinds of weird\n\tbugs may appear. This is not enforced though, it\'s only used for encoding.\n\t""""""\n\tdef __hash__(self):\n\t\treturn hash(frozenset(self.items()))\n\n\ntry:\n\tfrom inspect import signature\nexcept ImportError:\n\ttry:\n\t\tfrom inspect import getfullargspec\n\texcept ImportError:\n\t\tfrom inspect import getargspec, isfunction\n\t\tdef get_arg_names(callable):\n\t\t\tif type(callable) == partial and version_info[0] == 2:\n\t\t\t\tif not hasattr(get_arg_names, \'__warned_partial_argspec\'):\n\t\t\t\t\tget_arg_names.__warned_partial_argspec = True\n\t\t\t\t\twarnings.warn(""\'functools.partial\' and \'inspect.getargspec\' are not compatible in this Python version; ""\n\t\t\t\t\t\t""ignoring the \'partial\' wrapper when inspecting arguments of {}, which can lead to problems"".format(callable))\n\t\t\t\treturn set(getargspec(callable.func).args)\n\t\t\tif isfunction(callable):\n\t\t\t\targspec = getargspec(callable)\n\t\t\telse:\n\t\t\t\targspec = getargspec(callable.__call__)\n\t\t\treturn set(argspec.args)\n\telse:\n\t\t#todo: this is not covered in test case (py 3+ uses `signature`, py2 `getfullargspec`); consider removing it\n\t\tdef get_arg_names(callable):\n\t\t\targspec = getfullargspec(callable)\n\t\t\treturn set(argspec.args) | set(argspec.kwonlyargs)\nelse:\n\tdef get_arg_names(callable):\n\t\tsig = signature(callable)\n\t\treturn set(sig.parameters.keys())\n\n\ndef filtered_wrapper(encoder):\n\t""""""\n\tFilter kwargs passed to encoder.\n\t""""""\n\tif hasattr(encoder, ""default""):\n\t\tencoder = encoder.default\n\telif not hasattr(encoder, \'__call__\'):\n\t\traise TypeError(\'`obj_encoder` {0:} does not have `default` method and is not callable\'.format(enc))\n\tnames = get_arg_names(encoder)\n\n\tdef wrapper(*args, **kwargs):\n\t\treturn encoder(*args, **{k: v for k, v in kwargs.items() if k in names})\n\treturn wrapper\n\n\nclass NoNumpyException(Exception):\n\t"""""" Trying to use numpy features, but numpy cannot be found. """"""\n\n\nclass NoPandasException(Exception):\n\t"""""" Trying to use pandas features, but pandas cannot be found. """"""\n\n\nclass NoEnumException(Exception):\n\t"""""" Trying to use enum features, but enum cannot be found. """"""\n\n\nclass NoPathlibException(Exception):\n\t"""""" Trying to use pathlib features, but pathlib cannot be found. """"""\n\n\nclass ClassInstanceHookBase(object):\n\tdef get_cls_from_instance_type(self, mod, name, cls_lookup_map):\n\t\tCls = ValueError()\n\t\tif mod is None:\n\t\t\ttry:\n\t\t\t\tCls = getattr((__import__(\'__main__\')), name)\n\t\t\texcept (ImportError, AttributeError):\n\t\t\t\tif name not in cls_lookup_map:\n\t\t\t\t\traise ImportError((\'class {0:s} seems to have been exported from the main file, which means \'\n\t\t\t\t\t\t\'it has no module/import path set; you need to provide loads argument\'\n\t\t\t\t\t\t\'`cls_lookup_map={{""{0}"": Class}}` to locate the class\').format(name))\n\t\t\t\tCls = cls_lookup_map[name]\n\t\telse:\n\t\t\timp_err = None\n\t\t\ttry:\n\t\t\t\tmodule = import_module(\'{0:}\'.format(mod, name))\n\t\t\texcept ImportError as err:\n\t\t\t\timp_err = (\'encountered import error ""{0:}"" while importing ""{1:}"" to decode a json file; perhaps \'\n\t\t\t\t\t\'it was encoded in a different environment where {1:}.{2:} was available\').format(err, mod, name)\n\t\t\telse:\n\t\t\t\tif hasattr(module, name):\n\t\t\t\t\tCls = getattr(module, name)\n\t\t\t\telse:\n\t\t\t\t\timp_err = \'imported ""{0:}"" but could find ""{1:}"" inside while decoding a json file (found {2:})\'.format(\n\t\t\t\t\t\tmodule, name, \', \'.join(attr for attr in dir(module) if not attr.startswith(\'_\')))\n\t\t\tif imp_err:\n\t\t\t\tCls = cls_lookup_map.get(name, None)\n\t\t\t\tif Cls is None:\n\t\t\t\t\traise ImportError(\'{}; add the class to `cls_lookup_map={{""{}"": Class}}` argument\'.format(imp_err, name))\n\t\treturn Cls\n\n\ndef get_scalar_repr(npscalar):\n\treturn hashodict((\n\t\t(\'__ndarray__\', npscalar.item()),\n\t\t(\'dtype\', str(npscalar.dtype)),\n\t\t(\'shape\', ()),\n\t))\n\n\ndef encode_scalars_inplace(obj):\n\t""""""\n\tSearches a data structure of lists, tuples and dicts for numpy scalars\n\tand replaces them by their dictionary representation, which can be loaded\n\tby json-tricks. This happens in-place (the object is changed, use a copy).\n\t""""""\n\tfrom numpy import generic, complex64, complex128\n\tif isinstance(obj, (generic, complex64, complex128)):\n\t\treturn get_scalar_repr(obj)\n\tif isinstance(obj, dict):\n\t\tfor key, val in tuple(obj.items()):\n\t\t\tobj[key] = encode_scalars_inplace(val)\n\t\treturn obj\n\tif isinstance(obj, list):\n\t\tfor k, val in enumerate(obj):\n\t\t\tobj[k] = encode_scalars_inplace(val)\n\t\treturn obj\n\tif isinstance(obj, (tuple, set)):\n\t\treturn type(obj)(encode_scalars_inplace(val) for val in obj)\n\treturn obj\n\n\ndef encode_intenums_inplace(obj):\n\t""""""\n\tSearches a data structure of lists, tuples and dicts for IntEnum\n\tand replaces them by their dictionary representation, which can be loaded\n\tby json-tricks. This happens in-place (the object is changed, use a copy).\n\t""""""\n\tfrom enum import IntEnum\n\tfrom json_tricks import encoders\n\tif isinstance(obj, IntEnum):\n\t\treturn encoders.enum_instance_encode(obj)\n\tif isinstance(obj, dict):\n\t\tfor key, val in obj.items():\n\t\t\tobj[key] = encode_intenums_inplace(val)\n\t\treturn obj\n\tif isinstance(obj, list):\n\t\tfor index, val in enumerate(obj):\n\t\t\tobj[index] = encode_intenums_inplace(val)\n\t\treturn obj\n\tif isinstance(obj, (tuple, set)):\n\t\treturn type(obj)(encode_intenums_inplace(val) for val in obj)\n\treturn obj\n\n\ndef get_module_name_from_object(obj):\n\tmod = obj.__class__.__module__\n\tif mod == \'__main__\':\n\t\tmod = None\n\t\twarnings.warn((\'class {0:} seems to have been defined in the main file; unfortunately this means\'\n\t\t\t\' that it\\\'s module/import path is unknown, so you might have to provide cls_lookup_map when \'\n\t\t\t\'decoding\').format(obj.__class__))\n\treturn mod\n\n\ndef nested_index(collection, indices):\n\tfor i in indices:\n\t\tcollection = collection[i]\n\treturn collection\n\n\ndef dict_default(dictionary, key, default_value):\n\tif key not in dictionary:\n\t\tdictionary[key] = default_value\n\n\ndef gzip_compress(data, compresslevel):\n\t""""""\n\tDo gzip compression, without the timestamp. Similar to gzip.compress, but without timestamp, and also before py3.2.\n\t""""""\n\tbuf = io.BytesIO()\n\twith gzip.GzipFile(fileobj=buf, mode=\'wb\', compresslevel=compresslevel, mtime=0) as fh:\n\t\tfh.write(data)\n\treturn buf.getvalue()\n\n\ndef gzip_decompress(data):\n\t""""""\n\tDo gzip decompression, without the timestamp. Just like gzip.decompress, but that\'s py3.2+.\n\t""""""\n\twith gzip.GzipFile(fileobj=io.BytesIO(data)) as f:\n\t\treturn f.read()\n\n\nis_py3 = (version[:2] == \'3.\')\nstr_type = str if is_py3 else (basestring, unicode,)\n\n'"
tests/__init__.py,0,b''
tests/test_bare.py,4,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom collections import OrderedDict\nfrom datetime import datetime, time, date, timedelta\nfrom decimal import Decimal\nfrom fractions import Fraction\nfrom functools import partial\nfrom gzip import GzipFile\nfrom io import BytesIO, StringIO\nfrom math import pi, exp\nfrom os.path import join\nfrom tempfile import mkdtemp\n\nfrom _pytest.recwarn import warns\nfrom pytest import raises, fail\n\nfrom json_tricks import fallback_ignore_unknown, DuplicateJsonKeyException\nfrom json_tricks.nonp import strip_comments, dump, dumps, load, loads, \\\n\tENCODING\nfrom json_tricks.utils import is_py3, gzip_compress, JsonTricksDeprecation\nfrom .test_class import MyTestCls, CustomEncodeCls, SubClass, SuperClass, SlotsBase, SlotsDictABC, SlotsStr, \\\n\tSlotsABCDict, SlotsABC\n\nnonpdata = {\n\t\'my_array\': list(range(20)),\n\t\'my_map\': dict((chr(k), k) for k in range(97, 123)),\n\t\'my_string\': \'Hello world!\',\n\t\'my_float\': 3.1415,\n\t\'my_int\': 42\n}\n\n\ndef test_dumps_loads():\n\tjson = dumps(nonpdata)\n\tdata2 = loads(json)\n\tassert nonpdata == data2\n\n\ndef test_file_handle():\n\tpath = join(mkdtemp(), \'pytest-nonp.json\')\n\twith open(path, \'wb+\') as fh:\n\t\tdump(nonpdata, fh, compression=6)\n\twith open(path, \'rb\') as fh:\n\t\tdata2 = load(fh, decompression=True)\n\tassert data2 == nonpdata\n\twith open(path, \'rb\') as fh:\n\t\tdata3 = load(fh, decompression=None)  # test autodetect gzip\n\tassert data3 == nonpdata\n\n\ndef test_mix_handle_str_path():\n\t# Based on issue 68\n\tdata = {""fun"": 1.1234567891234567e-13}\n\tpath = join(mkdtemp(), \'test_mix_handle_str_path.json\')\n\tdump(data, open(path, ""w""))\n\tback = load(path)\n\tassert data == back\n\n\n\ndef test_wrong_arg_order():\n\t# Based on a problem from https://github.com/mverleg/array_storage_benchmark\n\tli = [[1.0, 2.0], [3.0, 4.0]]\n\tmap = {""a"": 1}\n\tpath = join(mkdtemp(), \'pytest-np.json.gz\')\n\tmsg = \'json-tricks dump arguments are in the wrong order: provide the data to be serialized before file handle\'\n\twith raises(ValueError) as ex:\n\t\twith open(path, \'wb+\') as fh:\n\t\t\tdump(fh, li)\n\tassert msg in ex.value.args[0]\n\twith raises(ValueError) as ex:\n\t\tdump(path, li)\n\tassert msg in ex.value.args[0]\n\twith raises(ValueError) as ex:\n\t\twith open(path, \'wb+\') as fh:\n\t\t\tdump(fh, map)\n\tassert msg in ex.value.args[0]\n\twith raises(ValueError) as ex:\n\t\tdump(path, map)\n\tassert msg in ex.value.args[0]\n\n\ndef test_mix_handle_bin_path():\n\t# Based on issue 68\n\tdata = {""fun"": 1.1234567891234567e-13}\n\tpath = join(mkdtemp(), \'test_mix_handle_bin_path.json\')\n\tif is_py3:\n\t\twith raises(TypeError):\n\t\t\tdump(data, open(path, ""wb""))\n\n\ndef test_mix_path_handle():\n\t# Based on issue 68\n\tdata = {""fun"": 1.1234567891234567e-13}\n\tpath = join(mkdtemp(), \'test_mix_path_handle.json\')\n\tdump(data, path)\n\n\ndef test_file_handle_types():\n\tpath = join(mkdtemp(), \'pytest-text.json\')\n\tfor conv_str_byte in [True, False]:\n\t\twith open(path, \'w+\') as fh:\n\t\t\tdump(nonpdata, fh, compression=False, conv_str_byte=conv_str_byte)\n\t\twith open(path, \'r\') as fh:\n\t\t\tassert load(fh, conv_str_byte=conv_str_byte) == nonpdata\n\t\twith StringIO() as fh:\n\t\t\tdump(nonpdata, fh, conv_str_byte=conv_str_byte)\n\t\t\tfh.seek(0)\n\t\t\tassert load(fh, conv_str_byte=conv_str_byte) == nonpdata\n\twith BytesIO() as fh:\n\t\twith raises(TypeError):\n\t\t\tdump(nonpdata, fh)\n\twith BytesIO() as fh:\n\t\tdump(nonpdata, fh, conv_str_byte=True)\n\t\tfh.seek(0)\n\t\tassert load(fh, conv_str_byte=True) == nonpdata\n\tif is_py3:\n\t\twith open(path, \'w+\') as fh:\n\t\t\twith raises(IOError):\n\t\t\t\tdump(nonpdata, fh, compression=6)\n\n\ndef test_file_path():\n\tpath = join(mkdtemp(), \'pytest-nonp.json\')\n\tdump(nonpdata, path, compression=6)\n\tdata2 = load(path, decompression=True)\n\tassert data2 == nonpdata\n\tdata3 = load(path, decompression=None)  # autodetect gzip\n\tassert data3 == nonpdata\n\n\ntest_json_with_comments = """"""{ # ""comment 1\n\t""hello"": ""Wor#d"", ""Bye"": ""\\\\""M#rk\\\\"""", ""yes\\\\\\\\\\\\"""": 5,# comment"" 2\n\t""quote"": ""\\\\""th#t\'s\\\\"" what she said"", # comment ""3""\n\t""list"": [1, 1, ""#"", ""\\\\"""", ""\\\\\\\\"", 8], ""dict"": {""q"": 7} #"" comment 4 with quotes\n}\n# comment 5""""""\n\ntest_json_without_comments = """"""{\n\t""hello"": ""Wor#d"", ""Bye"": ""\\\\""M#rk\\\\"""", ""yes\\\\\\\\\\\\"""": 5,\n\t""quote"": ""\\\\""th#t\'s\\\\"" what she said"",\n\t""list"": [1, 1, ""#"", ""\\\\"""", ""\\\\\\\\"", 8], ""dict"": {""q"": 7}\n}\n""""""\n\ntest_object_for_comment_strings = {\n\t""hello"": ""Wor#d"", ""Bye"": ""\\""M#rk\\"""", ""yes\\\\\\"""": 5,\n\t""quote"": ""\\""th#t\'s\\"" what she said"",\n\t""list"": [1, 1, ""#"", ""\\"""", ""\\\\"", 8], ""dict"": {""q"": 7}\n}\n\ntest_json_duplicates = """"""{""test"": 42, ""test"": 37}""""""\n\n\ndef test_strip_comments():\n\tvalid = strip_comments(test_json_with_comments)\n\tassert valid == test_json_without_comments\n\tvalid = strip_comments(test_json_with_comments.replace(\'#\', \'//\'))\n\tassert valid == test_json_without_comments.replace(\'#\', \'//\')\n\n\ndef test_ignore_comments_deprecation():\n\t# https://github.com/mverleg/pyjson_tricks/issues/74\n\n\t# First time should have deprecation warning\n\tloads._ignore_comments_warned_ = False\n\twith warns(JsonTricksDeprecation):\n\t\tloads(test_json_with_comments)\n\n\t# Second time there should be no warning\n\t# noinspection PyTypeChecker\n\twith warns(None) as captured:\n\t\tloaded = loads(test_json_with_comments)\n\tassert len(captured) == 0\n\tassert loaded == test_object_for_comment_strings\n\n\t# Passing a string without comments should not have a warning\n\tloads._ignore_comments_warned_ = False\n\t# noinspection PyTypeChecker\n\twith warns(None) as captured:\n\t\tloaded = loads(test_json_without_comments)\n\tassert len(captured) == 0\n\n\t# Passing True for argument explicitly should not have a warning\n\tloads._ignore_comments_warned_ = False\n\t# noinspection PyTypeChecker\n\twith warns(None) as captured:\n\t\tloaded = loads(test_json_with_comments, ignore_comments=True)\n\tassert len(captured) == 0\n\tassert loaded == test_object_for_comment_strings\n\n\t# Passing False for argument explicitly should not have a warning\n\tloads._ignore_comments_warned_ = False\n\t# noinspection PyTypeChecker\n\twith warns(None) as captured:\n\t\tloaded = loads(test_json_without_comments, ignore_comments=False)\n\tassert len(captured) == 0\n\tassert loaded == test_object_for_comment_strings\n\n\nordered_map = OrderedDict((\n\t(\'elephant\', None),\n\t(\'chicken\', None),\n\t(\'dolphin\', None),\n\t(\'wild boar\', None),\n\t(\'grasshopper\', None),\n\t(\'tiger\', None),\n\t(\'buffalo\', None),\n\t(\'killer whale\', None),\n\t(\'eagle\', None),\n\t(\'tortoise\', None),\n))\n\n\ndef test_string_compression():\n\tjson = dumps(ordered_map, compression=3)\n\tassert json[:2] == b\'\\x1f\\x8b\'\n\tdata2 = loads(json, decompression=True)\n\tassert ordered_map == data2\n\tdata3 = loads(json, decompression=None)\n\tassert ordered_map == data3\n\n\ndef test_flush_no_errors():\n\t# just tests that flush doesn\'t cause problems; checking actual flushing is too messy.\n\tpath = join(mkdtemp(), \'pytest-nonp.json\')\n\twith open(path, \'wb+\') as fh:\n\t\tdump(nonpdata, fh, compression=True, force_flush=True)\n\twith open(path, \'rb\') as fh:\n\t\tdata2 = load(fh, decompression=True)\n\tassert data2 == nonpdata\n\t# flush non-file IO\n\tsh = BytesIO()\n\ttry:\n\t\tdump(ordered_map, fp=sh, compression=True, force_flush=True)\n\tfinally:\n\t\tsh.close()\n\n\ndef test_compression_with_comments():\n\tif is_py3:\n\t\ttest_json = bytes(test_json_with_comments, encoding=ENCODING)\n\telse:\n\t\ttest_json = test_json_with_comments\n\tjson = gzip_compress(test_json, compresslevel=9)\n\tref = loads(test_json_without_comments)\n\tdata2 = loads(json, decompression=True)\n\tassert ref == data2\n\tdata3 = loads(json, decompression=None)\n\tassert ref == data3\n\n\ndef test_order():\n\tjson = dumps(ordered_map)\n\tdata2 = loads(json, preserve_order=True)\n\tassert tuple(ordered_map.keys()) == tuple(data2.keys())\n\treverse = OrderedDict(reversed(tuple(ordered_map.items())))\n\tjson = dumps(reverse)\n\tdata3 = loads(json, preserve_order=True)\n\tassert tuple(reverse.keys()) == tuple(data3.keys())\n\tjson = dumps(ordered_map)\n\tdata4 = loads(json, preserve_order=False)\n\tassert not isinstance(data4, OrderedDict)\n\n\ncls_instance = MyTestCls(s=\'ub\', dct={\'7\': 7})\ncls_instance_custom = CustomEncodeCls()\n\n\ndef test_cls_instance_default():\n\tjson = dumps(cls_instance)\n\tback = loads(json)\n\tassert (cls_instance.s == back.s)\n\tassert (cls_instance.dct == dict(back.dct))\n\tjson = dumps(cls_instance, primitives=True)\n\tback = loads(json)\n\tassert tuple(sorted(back.keys())) == (\'dct\', \'s\',)\n\tassert \'7\' in back[\'dct\']\n\n\ndef test_cls_instance_custom():\n\tjson = dumps(cls_instance_custom)\n\tback = loads(json)\n\tassert (cls_instance_custom.relevant == back.relevant)\n\tassert (cls_instance_custom.irrelevant == 37)\n\tassert (back.irrelevant == 12)\n\tjson = dumps(cls_instance_custom, primitives=True)\n\tback = loads(json)\n\tassert (cls_instance_custom.relevant == back[\'relevant\'])\n\tassert (cls_instance_custom.irrelevant == 37)\n\tassert \'irrelevant\' not in back\n\n\ndef test_cls_instance_local():\n\tjson = \'{""__instance_type__"": [null, ""CustomEncodeCls""], ""attributes"": {""relevant"": 137}}\'\n\tloads(json, cls_lookup_map=globals())\n\n\ndef test_cls_instance_inheritance():\n\tinst = SubClass()\n\tjson = dumps(inst)\n\tassert \'42\' not in json\n\tback = loads(json)\n\tassert inst == back\n\tinst.set_attr()\n\tjson = dumps(inst)\n\tassert \'42\' in json\n\tback = loads(json)\n\tassert inst == back\n\n\ndef test_cls_attributes_unchanged():\n\t""""""\n\tTest that class attributes are not restored. This would be undesirable,\n\tbecause deserializing one instance could impact all other existing ones.\n\t""""""\n\tSuperClass.cls_attr = 37\n\tinst = SuperClass()\n\tjson = dumps(inst)\n\tassert \'37\' not in json\n\tSuperClass.cls_attr = 42\n\tback = loads(json)\n\tassert inst == back\n\tassert inst.cls_attr == back.cls_attr == 42\n\tSuperClass.cls_attr = 37\n\n\ndef test_cls_lookup_map_fail():\n\tclass LocalCls(object):\n\t\tdef __init__(self, val):\n\t\t\tself.value = val\n\toriginal = [LocalCls(37), LocalCls(42)]\n\ttxt = dumps(original)\n\twith raises(ImportError) as err:\n\t\tloads(txt)\n\tassert \'LocalCls\' in str(err.value)\n\tassert \'cls_lookup_map\' in str(err.value)\n\twith raises(ImportError) as err:\n\t\tloads(txt, cls_lookup_map=globals())\n\tassert \'LocalCls\' in str(err.value)\n\tassert \'cls_lookup_map\' in str(err.value)\n\n\ndef test_cls_lookup_map_success():\n\tclass LocalCls(object):\n\t\tdef __init__(self, val):\n\t\t\tself.value = val\n\toriginal = [LocalCls(37), LocalCls(42)]\n\ttxt = dumps(original)\n\tback = loads(txt, cls_lookup_map=dict(LocalCls=LocalCls))\n\tassert len(original) == len(back) == 2\n\tassert original[0].value == back[0].value\n\tassert original[1].value == back[1].value\n\tback = loads(txt, properties=dict(cls_lookup_map=dict(LocalCls=LocalCls)))\n\tassert len(original) == len(back) == 2\n\tassert original[0].value == back[0].value\n\tassert original[1].value == back[1].value\n\n\ndef test_cls_slots():\n\tslots = [SlotsBase(), SlotsDictABC(), SlotsStr(), SlotsABCDict(), SlotsABC()]\n\ttxt = dumps(slots)\n\tres = loads(txt)\n\tfor inputobj, outputobj in zip(slots, res):\n\t\tassert isinstance(outputobj, SlotsBase)\n\t\tassert inputobj == outputobj\n\treferenceobj = SlotsBase()\n\tfor outputobj in res[1:]:\n\t\tassert outputobj != referenceobj\n\n\ndef test_duplicates():\n\tloads(test_json_duplicates, allow_duplicates=True)\n\twith raises(DuplicateJsonKeyException):\n\t\tloads(test_json_duplicates, allow_duplicates=False)\n\n\ndef test_complex_number():\n\tobjs = (\n\t\t4.2 + 3.7j,\n\t\t1j,\n\t\t1 + 0j,\n\t\t-999999.9999999 - 999999.9999999j,\n\t)\n\tfor obj in objs:\n\t\tjson = dumps(obj)\n\t\tback = loads(json)\n\t\tassert back == obj, \'json en/decoding failed for complex number {0:}\'.format(obj)\n\t\tjson = dumps(obj, primitives=True)\n\t\tback = loads(json)\n\t\tassert back == [obj.real, obj.imag]\n\t\tassert complex(*back) == obj\n\ttxt = \'{""__complex__"": [4.2, 3.7]}\'\n\tobj = loads(txt)\n\tassert obj == 4.2 + 3.7j\n\n\ndef test_float_precision():\n\tjson = dumps([pi])\n\tback = loads(json)\n\tassert back[0] - pi == 0, \'Precision lost while encoding and decoding float.\'\n\n\ndef test_set():\n\tsetdata = [{\'set\': set((3, exp(1), (-5, +7), False))}]\n\tjson = dumps(setdata)\n\tback = loads(json)\n\tassert isinstance(back[0][\'set\'], set)\n\tassert setdata == back\n\tjson = dumps(setdata, primitives=True)\n\tback = loads(json)\n\tassert isinstance(back[0][\'set\'], list)\n\tassert setdata[0][\'set\'] == set(tuple(q) if isinstance(q, list) else q for q in back[0][\'set\'])\n\n\ndef test_special_nr_parsing():\n\tnr_li_json = \'[1, 3.14]\'\n\tres = loads(nr_li_json,\n\t\tparse_int=lambda s: int(\'7\' + s),\n\t\tparse_float=lambda s: float(\'5\' + s)\n\t)\n\tassert res == [71, 53.14], \'Special integer and/or float parsing not working\'\n\tnr_li_json = \'[1, 3.14]\'\n\tres = loads(nr_li_json,\n\t\tparse_int=Decimal,\n\t\tparse_float=Decimal\n\t)\n\tassert isinstance(res[0], Decimal)\n\tassert isinstance(res[1], Decimal)\n\n\ndef test_special_floats():\n\t""""""\n\tThe official json standard doesn\'t support infinity or NaN, but the Python implementation does.\n\t""""""\n\tspecial_floats = [float(\'NaN\'), float(\'Infinity\'), -float(\'Infinity\'), float(\'+0\'), float(\'-0\')]\n\ttxt = dumps(special_floats, allow_nan=True)\n\tassert txt == ""[NaN, Infinity, -Infinity, 0.0, -0.0]""\n\tres = loads(txt)\n\tfor x, y in zip(special_floats, res):\n\t\t"""""" Use strings since `+0 == -1` and `NaN != NaN` """"""\n\t\tassert str(x) == str(y)\n\twith raises(ValueError):\n\t\tdumps(special_floats, allow_nan=False)\n\twith raises(ValueError):\n\t\tdumps(special_floats)\n\n\ndef test_decimal():\n\tdecimals = [Decimal(0), Decimal(-pi), Decimal(\'9999999999999999999999999999999999999999999999999999\'),\n\t\tDecimal(\'NaN\'), Decimal(\'Infinity\'), -Decimal(\'Infinity\'), Decimal(\'+0\'), Decimal(\'-0\')]\n\ttxt = dumps(decimals)\n\tres = loads(txt)\n\tfor x, y in zip(decimals, res):\n\t\tassert isinstance(y, Decimal)\n\t\tassert x == y or x.is_nan()\n\t\tassert str(x) == str(y)\n\n\ndef test_decimal_primitives():\n\tdecimals = [Decimal(0), Decimal(-pi), Decimal(\'9999999999999\')]\n\ttxt = dumps(decimals, primitives=True)\n\tres = loads(txt)\n\tfor x, y in zip(decimals, res):\n\t\tassert isinstance(y, float)\n\t\tassert x == y or x.is_nan()\n\n\ndef test_fraction():\n\tfractions = [Fraction(0), Fraction(1, 3), Fraction(-pi), Fraction(\'1/3\'), Fraction(\'1/3\') / Fraction(\'1/6\'),\n\t\tFraction(\'9999999999999999999999999999999999999999999999999999\'), Fraction(\'1/12345678901234567890123456789\'),]\n\ttxt = dumps(fractions)\n\tres = loads(txt)\n\tfor x, y in zip(fractions, res):\n\t\tassert isinstance(y, Fraction)\n\t\tassert x == y\n\t\tassert str(x) == str(y)\n\ttxt = dumps(fractions, primitives=True)\n\tres = loads(txt)\n\tfor x, y in zip(fractions, res):\n\t\tassert isinstance(y, float)\n\t\tassert abs(x - y) < 1e-10\n\n\nDTOBJ = [\n\tdatetime(year=1988, month=3, day=15, hour=8, minute=3, second=59, microsecond=7),\n\tdate(year=1988, month=3, day=15),\n\ttime(hour=8, minute=3, second=59, microsecond=123),\n\ttimedelta(days=2, seconds=3599),\n]\n\n\ndef test_naive_date_time():\n\tjson = dumps(DTOBJ)\n\tback = loads(json)\n\tassert DTOBJ == back\n\tfor orig, bck in zip(DTOBJ, back):\n\t\tassert orig == bck\n\t\tassert type(orig) == type(bck)\n\ttxt = \'{""__datetime__"": null, ""year"": 1988, ""month"": 3, ""day"": 15, ""hour"": 8, ""minute"": 3, \' \\\n\t\t\t\'""second"": 59, ""microsecond"": 7}\'\n\tobj = loads(txt)\n\tassert obj == datetime(year=1988, month=3, day=15, hour=8, minute=3, second=59, microsecond=7)\n\n\ndef test_primitive_naive_date_time():\n\tjson = dumps(DTOBJ, primitives=True)\n\tback = loads(json)\n\tfor orig, bck in zip(DTOBJ, back):\n\t\tif isinstance(bck, (date, time, datetime,)):\n\t\t\tassert isinstance(bck, str if is_py3 else (str, unicode))\n\t\t\tassert bck == orig.isoformat()\n\t\telif isinstance(bck, (timedelta,)):\n\t\t\tassert isinstance(bck, float)\n\t\t\tassert bck == orig.total_seconds()\n\tdt = datetime(year=1988, month=3, day=15, hour=8, minute=3, second=59, microsecond=7)\n\tassert dumps(dt, primitives=True).strip(\'""\') == \'1988-03-15T08:03:59.000007\'\n\n\ndef test_str_unicode_bytes():\n\ttext, obj = u\'{""mykey"": ""\xe4\xbd\xa0\xe5\xa5\xbd""}\', {""mykey"": u""\xe4\xbd\xa0\xe5\xa5\xbd""}\n\tassert loads(text) == obj\n\tif is_py3:\n\t\twith raises(TypeError) as err:\n\t\t\tloads(text.encode(\'utf-8\'))\n\t\tif \'ExceptionInfo\' in str(type(err)):\n\t\t\t# This check is needed because the type of err varies between versions\n\t\t\t# For some reason, isinstance(..., py.code.ExceptionInfo) does not work\n\t\t\terr = err.value\n\t\tassert \'The input was of non-string type\' in str(err)\n\t\tassert loads(text.encode(\'utf-8\'), conv_str_byte=True) == obj\n\telse:\n\t\tassert loads(\'{""mykey"": ""nihao""}\') == {\'mykey\': \'nihao\'}\n\n\ndef with_nondict_hook():\n\t"""""" Add a custom hook, to test that all future hooks handle non-dicts. """"""\n\t# Prevent issue 26 from coming back.\n\tdef test_hook(dct):\n\t\tif not isinstance(dct, dict):\n\t\t\treturn\n\t\treturn ValueError()\n\tloads(\'{""key"": 42}\', extra_obj_pairs_hooks=(test_hook,))\n\n\ndef test_custom_enc_dec():\n\t"""""" Test using a custom encoder/decoder. """"""\n\tdef silly_enc(obj):\n\t\treturn {""val"": 42}\n\tdef silly_dec(dct):\n\t\tif not isinstance(dct, dict):\n\t\t\treturn dct\n\t\treturn [37]\n\ttxt = dumps(lambda x: x * 2, extra_obj_encoders=(silly_enc,))\n\tassert txt == \'{""val"": 42}\'\n\tback = loads(txt, extra_obj_pairs_hooks=(silly_dec,))\n\tassert back == [37]\n\n\ndef test_lambda_partial():\n\t"""""" Test that a custom encoder/decoder works when wrapped in functools.partial,\n\t    which caused problems before because inspect.getargspec does not support it. """"""\n\tobj = dict(alpha=37.42, beta=[1, 2, 4, 8, 16, 32])\n\tenc_dec_lambda = partial(lambda x, y: x, y=0)\n\ttxt = dumps(obj, extra_obj_encoders=(enc_dec_lambda,))\n\tback = loads(txt, extra_obj_pairs_hooks=(enc_dec_lambda,))\n\tassert obj == back\n\tdef enc_dec_fun(obj, primitives=False, another=True):\n\t\treturn obj\n\ttxt = dumps(obj, extra_obj_encoders=(partial(enc_dec_fun, another=True),))\n\tback = loads(txt, extra_obj_pairs_hooks=(partial(enc_dec_fun, another=True),))\n\tassert obj == back\n\n\ndef test_hooks_not_too_eager():\n\tfrom threading import RLock\n\twith raises(TypeError):\n\t\tdumps([RLock()])\n\t\t# TypeError did not get raised, so show a message\n\t\t# (https://github.com/pytest-dev/pytest/issues/3974)\n\t\tfail(\'There is no hook to serialize RLock, so this should fail, \'\n\t\t\t\'otherwise some hook is too eager.\')\n\n\ndef test_fallback_hooks():\n\tfrom threading import RLock\n\n\tjson = dumps(OrderedDict((\n\t\t(\'li\', [1, 2, 3]),\n\t\t(\'lock\', RLock()),\n\t)), fallback_encoders=[fallback_ignore_unknown])\n\tbck = loads(json)\n\tassert bck == OrderedDict((\n\t\t(\'li\', [1, 2, 3]),\n\t\t(\'lock\', None),\n\t))\n\n\ndef test_empty_string_with_url():\n\t"""""" Originally for https://github.com/mverleg/pyjson_tricks/issues/51 """"""\n\ttxt = \'{""foo"": """", ""bar"": ""http://google.com""}\'\n\tassert txt == strip_comments(txt), strip_comments(txt)\n\ttxt = \'{""foo"": """", ""bar"": ""http://google.com""}\'\n\tassert txt == dumps(loads(txt, ignore_comments=False))\n\tassert txt == dumps(loads(txt, ignore_comments=True))\n\ttxt = \'{""a"": """", ""b"": ""//"", ""c"": """"}\'\n\tassert txt == dumps(loads(txt))\n\ttxt = \'{""a"": """", ""b"": ""/*"", ""c"": """"}\'\n\tassert txt == dumps(loads(txt))\n\ttxt = \'{""//"": ""//""}\'\n\tassert txt == dumps(loads(txt))\n\ttxt = \'{""///"": ""////*/*""}\'\n\tassert txt == dumps(loads(txt))\n\n'"
tests/test_class.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\nclass MyTestCls(object):\n\tdef __init__(self, **kwargs):\n\t\tfor k, v in kwargs.items():\n\t\t\tsetattr(self, k, v)\n\n\tdef __repr__(self):\n\t\treturn 'A<{0:}>'.format(', '.join('{0:s}={1:}'.format(k, v) for k, v in self.__dict__.items()))\n\n\nclass CustomEncodeCls(MyTestCls):\n\tdef __init__(self, **kwargs):\n\t\tsuper(CustomEncodeCls, self).__init__(**kwargs)\n\t\tself.relevant = 42\n\t\tself.irrelevant = 37\n\n\tdef __json_encode__(self):\n\t\treturn {'relevant': self.relevant}\n\n\tdef __json_decode__(self, **attrs):\n\t\tself.relevant = attrs['relevant']\n\t\tself.irrelevant = 12\n\n\nclass SuperClass(object):\n\tcls_attr = 37\n\t\n\tdef __init__(self):\n\t\tself.attr = None\n\t\n\tdef __eq__(self, other):\n\t\treturn self.__class__ == other.__class__ and self.__dict__ == other.__dict__\n\n\nclass SubClass(SuperClass):\n\tdef set_attr(self):\n\t\tself.attr = 42\n\n\nclass SlotsBase(object):\n\t__slots__ = []\n\t\n\tdef __eq__(self, other):\n\t\tif self.__class__ != other.__class__:\n\t\t\treturn False\n\t\tslots = self.__class__.__slots__\n\t\tif isinstance(slots,str):\n\t\t\tslots = [slots]\n\t\treturn all(getattr(self, i) == getattr(other, i) for i in slots)\n\n\nclass SlotsDictABC(SlotsBase):\n\t__slots__ = ['__dict__']\n\t\n\tdef __init__(self, a='a', b='b', c='c'):\n\t\tself.a = a\n\t\tself.b = b\n\t\tself.c = c\n\n\nclass SlotsStr(SlotsBase):\n\t__slots__ = 'name'\n\t\n\tdef __init__(self, name='name'):\n\t\tself.name = name\n\n\nclass SlotsABCDict(SlotsBase):\n\t__slots__ = ['a','b','c','__dict__']\n\t\n\tdef __init__(self, a='a', b='b', c='c'):\n\t\tself.a = a\n\t\tself.b = b\n\t\tself.c = c\n\n\nclass SlotsABC(SlotsBase):\n\t__slots__ = ['a','b','c']\n\t\n\tdef __init__(self, a='a', b='b', c='c'):\n\t\tself.a = a\n\t\tself.b = b\n\t\tself.c = c\n\n\n"""
tests/test_enum.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nfrom datetime import datetime\nfrom functools import partial\nfrom enum import Enum, IntEnum\nfrom json_tricks import dumps, loads, encode_intenums_inplace\nfrom json_tricks.encoders import enum_instance_encode\n\n\nPY2 = sys.version_info[0] == 2\n\n\nclass MyEnum(Enum):\n\tmember1 = \'VALUE1\'\n\tmember2 = \'VALUE2\'\n\n\nclass MyIntEnum(IntEnum):\n\tint_member = 1\n\n\ndef test_enum():\n\tmember = MyEnum.member1\n\ttxt = dumps(member)\n\tback = loads(txt)\n\n\tassert isinstance(back, MyEnum)\n\tassert back == member\n\n\ndef test_enum_instance_global():\n\tjson = \'{""__enum__"": {""__enum_instance_type__"": [null, ""MyEnum""], ""name"": ""member1""}}\'\n\tback = loads(json, cls_lookup_map=globals())\n\tassert isinstance(back, MyEnum)\n\tassert back == MyEnum.member1\n\n\ndef test_enum_primitives():\n\tmember = MyEnum.member1\n\ttxt = dumps(member, primitives=True)\n\tassert txt == \'{""member1"": ""VALUE1""}\'\n\n\ndef test_encode_int_enum():\n\tmember = MyIntEnum.int_member\n\ttxt = dumps(member)\n\t# IntEnum are serialized as strings in enum34 for python < 3.4. This comes from how the JSON serializer work. We can\'t do anything about this besides documenting.\n\t# See https://bitbucket.org/stoneleaf/enum34/issues/17/difference-between-enum34-and-enum-json\n\tif PY2:\n\t\tassert txt == u""MyIntEnum.int_member""\n\telse:\n\t\tassert txt == ""1""\n\n\ndef test_encode_int_enum_inplace():\n\tobj = {\n\t\t\'int_member\': MyIntEnum.int_member,\n\t\t\'list\': [MyIntEnum.int_member],\n\t\t\'nested\': {\n\t\t\t\'member\': MyIntEnum.int_member,\n\t\t}\n\t}\n\n\ttxt = dumps(encode_intenums_inplace(obj))\n\tdata = loads(txt)\n\n\tassert isinstance(data[\'int_member\'], MyIntEnum)\n\tassert data[\'int_member\'] == MyIntEnum.int_member\n\tassert isinstance(data[\'list\'][0], MyIntEnum)\n\tassert isinstance(data[\'nested\'][\'member\'], MyIntEnum)\n\n\nclass EnumValueTest(object):\n\talpha = 37\n\tdef __init__(self, beta):\n\t\tself.beta = beta\n\n\nclass CombineComplexTypesEnum(Enum):\n\tclass_inst = EnumValueTest(beta=42)\n\ttimepoint = datetime(year=1988, month=3, day=15, hour=8, minute=3, second=59, microsecond=7)\n\timg = 1j\n\n\ndef test_complex_types_enum():\n\tobj = [\n\t\tCombineComplexTypesEnum.timepoint,\n\t\tCombineComplexTypesEnum.img,\n\t\tCombineComplexTypesEnum.class_inst,\n\t]\n\ttxt = dumps(encode_intenums_inplace(obj))\n\tback = loads(txt)\n\tassert obj == back\n\n\ndef test_with_value():\n\tobj = [CombineComplexTypesEnum.class_inst, CombineComplexTypesEnum.timepoint]\n\tencoder = partial(enum_instance_encode, with_enum_value=True)\n\ttxt = dumps(obj, extra_obj_encoders=(encoder,))\n\tassert \'""value"":\' in txt\n\tback = loads(txt, obj_pairs_hooks=())\n\tclass_inst_encoding = loads(dumps(CombineComplexTypesEnum.class_inst.value), obj_pairs_hooks=())\n\ttimepoint_encoding = loads(dumps(CombineComplexTypesEnum.timepoint.value), obj_pairs_hooks=())\n\tassert back[0][\'__enum__\'][\'value\'] == class_inst_encoding\n\tassert back[1][\'__enum__\'][\'value\'] == timepoint_encoding\n\n\n'"
tests/test_meta.py,0,"b""\nimport re\n\n\ndef test_version():\n\timport json_tricks\n\tassert re.match(r'^\\d+\\.\\d+\\.\\d+$', json_tricks.__version__) is not None\n"""
tests/test_np.py,2,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom copy import deepcopy\nfrom os.path import join\nfrom tempfile import mkdtemp\n\nfrom _pytest.recwarn import warns\nfrom numpy import arange, ones, array, array_equal, finfo, iinfo, pi\nfrom numpy import int8, int16, int32, int64, uint8, uint16, uint32, uint64, \\\n\tfloat16, float32, float64, complex64, complex128, zeros, ndindex\nfrom numpy.core.umath import exp\nfrom numpy.testing import assert_equal\n\nfrom json_tricks import numpy_encode\nfrom json_tricks.np import dump, dumps, load, loads\nfrom json_tricks.np_utils import encode_scalars_inplace\nfrom json_tricks.utils import JsonTricksDeprecation, gzip_decompress\nfrom .test_bare import cls_instance\nfrom .test_class import MyTestCls\n\nDTYPES = (int8, int16, int32, int64, uint8, uint16, uint32, uint64,\n\tfloat16, float32, float64, complex64, complex128)\n\n\ndef get_lims(dtype):\n\ttry:\n\t\tinfo = finfo(dtype)\n\texcept ValueError:\n\t\tinfo = iinfo(dtype)\n\treturn dtype(info.min), dtype(info.max)\n\n\nnpdata = {\n\t\'vector\': arange(15, 70, 3, dtype=uint8),\n\t\'matrix\': ones((15, 10), dtype=float64),\n}\n\n\ndef _numpy_equality(d2):\n\tassert npdata.keys() == d2.keys()\n\tassert_equal(npdata[\'vector\'], d2[\'vector\'])\n\tassert_equal(npdata[\'matrix\'], d2[\'matrix\'])\n\tassert npdata[\'vector\'].dtype == d2[\'vector\'].dtype\n\tassert npdata[\'matrix\'].dtype == d2[\'matrix\'].dtype\n\n\ndef test_primitives():\n\ttxt = dumps(deepcopy(npdata), primitives=True)\n\tdata2 = loads(txt)\n\tassert isinstance(data2[\'vector\'], list)\n\tassert isinstance(data2[\'matrix\'], list)\n\tassert isinstance(data2[\'matrix\'][0], list)\n\tassert data2[\'vector\'] == npdata[\'vector\'].tolist()\n\tassert (abs(array(data2[\'vector\']) - npdata[\'vector\'])).sum() < 1e-10\n\tassert data2[\'matrix\'] == npdata[\'matrix\'].tolist()\n\tassert (abs(array(data2[\'matrix\']) - npdata[\'matrix\'])).sum() < 1e-10\n\n\ndef test_dumps_loads_numpy():\n\tjson = dumps(deepcopy(npdata))\n\tdata2 = loads(json)\n\t_numpy_equality(data2)\n\n\ndef test_file_numpy():\n\tpath = join(mkdtemp(), \'pytest-np.json\')\n\twith open(path, \'wb+\') as fh:\n\t\tdump(deepcopy(npdata), fh, compression=9)\n\twith open(path, \'rb\') as fh:\n\t\tdata2 = load(fh, decompression=True)\n\t_numpy_equality(data2)\n\n\ndef test_compressed_to_disk():\n\tarr = [array([[1.0, 2.0], [3.0, 4.0]])]\n\tpath = join(mkdtemp(), \'pytest-np.json.gz\')\n\twith open(path, \'wb+\') as fh:\n\t\tdump(arr, fh, compression=True, properties={\'ndarray_compact\': True})\n\n\nmixed_data = {\n\t\'vec\': array(range(10)),\n\t\'inst\': MyTestCls(\n\t\tnr=7, txt=\'yolo\',\n\t\tli=[1,1,2,3,5,8,12],\n\t\tvec=array(range(7,16,2)),\n\t\tinst=cls_instance\n\t),\n}\n\n\ndef test_mixed_cls_arr():\n\tjson = dumps(mixed_data)\n\tback = dict(loads(json))\n\tassert mixed_data.keys() == back.keys()\n\tassert (mixed_data[\'vec\'] == back[\'vec\']).all()\n\tassert (mixed_data[\'inst\'].vec == back[\'inst\'].vec).all()\n\tassert (mixed_data[\'inst\'].nr == back[\'inst\'].nr)\n\tassert (mixed_data[\'inst\'].li == back[\'inst\'].li)\n\tassert (mixed_data[\'inst\'].inst.s == back[\'inst\'].inst.s)\n\tassert (mixed_data[\'inst\'].inst.dct == dict(back[\'inst\'].inst.dct))\n\n\ndef test_memory_order():\n\tarrC = array([[1., 2.], [3., 4.]], order=\'C\')\n\tjson = dumps(arrC)\n\tarr = loads(json)\n\tassert array_equal(arrC, arr)\n\tassert arrC.flags[\'C_CONTIGUOUS\'] == arr.flags[\'C_CONTIGUOUS\'] and \\\n\t\tarrC.flags[\'F_CONTIGUOUS\'] == arr.flags[\'F_CONTIGUOUS\']\n\tarrF = array([[1., 2.], [3., 4.]], order=\'F\')\n\tjson = dumps(arrF)\n\tarr = loads(json)\n\tassert array_equal(arrF, arr)\n\tassert arrF.flags[\'C_CONTIGUOUS\'] == arr.flags[\'C_CONTIGUOUS\'] and \\\n\t\tarrF.flags[\'F_CONTIGUOUS\'] == arr.flags[\'F_CONTIGUOUS\']\n\n\ndef test_scalars_types():\n\t# from: https://docs.scipy.org/doc/numpy/user/basics.types.html\n\tencme = []\n\tfor dtype in DTYPES:\n\t\tfor val in (dtype(0),) + get_lims(dtype):\n\t\t\tassert isinstance(val, dtype)\n\t\t\tencme.append(val)\n\tjson = dumps(encme, indent=2)\n\trec = loads(json)\n\tassert encme == rec\n\n\ndef test_array_types():\n\t# from: https://docs.scipy.org/doc/numpy/user/basics.types.html\n\t# see also `test_scalars_types`\n\tfor dtype in DTYPES:\n\t\tvec = [array((dtype(0), dtype(exp(1))) + get_lims(dtype), dtype=dtype)]\n\t\tjson = dumps(vec)\n\t\tassert dtype.__name__ in json\n\t\trec = loads(json)\n\t\tassert rec[0].dtype == dtype\n\t\tassert array_equal(vec, rec)\n\n\ndef test_encode_scalar():\n\tencd = encode_scalars_inplace([complex128(1+2j)])\n\tassert isinstance(encd[0], dict)\n\tassert encd[0][\'__ndarray__\'] == 1+2j\n\tassert encd[0][\'shape\'] == ()\n\tassert encd[0][\'dtype\'] == complex128.__name__\n\n\ndef test_dump_np_scalars():\n\tdata = [\n\t\tint8(-27),\n\t\tcomplex64(exp(1)+37j),\n\t\t(\n\t\t\t{\n\t\t\t\t\'alpha\': float64(-exp(10)),\n\t\t\t\t\'str-only\': complex64(-1-1j),\n\t\t\t},\n\t\t\tuint32(123456789),\n\t\t\tfloat16(exp(-1)),\n\t\t\tset((\n\t\t\t\tint64(37),\n\t\t\t\tuint64(-0),\n\t\t\t)),\n\t\t),\n\t]\n\treplaced = encode_scalars_inplace(deepcopy(data))\n\tjson = dumps(replaced)\n\trec = loads(json)\n\tassert data[0] == rec[0]\n\tassert data[1] == rec[1]\n\tassert data[2][0] == rec[2][0]\n\tassert data[2][1] == rec[2][1]\n\tassert data[2][2] == rec[2][2]\n\tassert data[2][3] == rec[2][3]\n\tassert data[2] == tuple(rec[2])\n\n\ndef test_ndarray_object_nesting():\n\t# Based on issue 53\n\t# With nested ndarrays\n\tbefore = zeros((2, 2,), dtype=object)\n\tfor i in ndindex(before.shape):\n\t\tbefore[i] = array([1, 2, 3])\n\tafter = loads(dumps(before))\n\tassert before.shape == after.shape, \\\n\t\t\'shape of array changed for nested ndarrays:\\n{}\'.format(dumps(before, indent=2))\n\tassert before.dtype == before.dtype\n\tassert array_equal(before[0, 0], after[0, 0])\n\t# With nested lists\n\tbefore = zeros((2, 2,), dtype=object)\n\tfor i in ndindex(before.shape):\n\t\tbefore[i] = [1, 2, 3]\n\tafter = loads(dumps(before))\n\tassert before.shape == after.shape, \\\n\t\t\'shape of array changed for nested ndarrays:\\n{}\'.format(dumps(before, indent=2))\n\tassert before.dtype == before.dtype\n\tassert array_equal(before[0, 0], after[0, 0])\n\n\ndef test_dtype_object():\n\t# Based on issue 64\n\tarr = array([\'a\', \'b\', \'c\'], dtype=object)\n\tjson = dumps(arr)\n\tback = loads(json)\n\tassert array_equal(back, arr)\n\n\ndef test_compact_mode_unspecified():\n\t# Other tests may have raised deprecation warning, so reset the cache here\n\tnumpy_encode._warned_compact = False\n\tdata = [array([[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]]), array([pi, exp(1)])]\n\twith warns(JsonTricksDeprecation):\n\t\tgz_json_1 = dumps(data, compression=True)\n\t# noinspection PyTypeChecker\n\twith warns(None) as captured:\n\t\tgz_json_2 = dumps(data, compression=True)\n\tassert len(captured) == 0\n\tassert gz_json_1 == gz_json_2\n\tjson = gzip_decompress(gz_json_1).decode(\'ascii\')\n\tassert json == \'[{""__ndarray__"": [[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]], ""dtype"": ""float64"", ""shape"": [2, 4], ""Corder"": true}, \' \\\n\t\t\'{""__ndarray__"": [3.141592653589793, 2.718281828459045], ""dtype"": ""float64"", ""shape"": [2]}]\'\n\n\ndef test_compact():\n\tdata = [array(list(2**(x + 0.5) for x in range(-30, +31)))]\n\tjson = dumps(data, compression=True, properties={\'ndarray_compact\': True})\n\tback = loads(json)\n\tassert_equal(data, back)\n\n\ndef test_encode_disable_compact():\n\tdata = [array([[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]]), array([pi, exp(1)])]\n\tgz_json = dumps(data, compression=True, properties={\'ndarray_compact\': False})\n\tjson = gzip_decompress(gz_json).decode(\'ascii\')\n\tassert json == \'[{""__ndarray__"": [[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]], ""dtype"": ""float64"", ""shape"": [2, 4], ""Corder"": true}, \' \\\n\t\t\'{""__ndarray__"": [3.141592653589793, 2.718281828459045], ""dtype"": ""float64"", ""shape"": [2]}]\'\n\n\ndef test_encode_enable_compact():\n\tdata = [array([[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]]), array([pi, exp(1)])]\n\tgz_json = dumps(data, compression=True, properties={\'ndarray_compact\': True})\n\tjson = gzip_decompress(gz_json).decode(\'ascii\')\n\tassert json == \'[{""__ndarray__"": ""b64:AAAAAAAA8D8AAAAAAAAAQAAAAAAAAAhAAAAAAAAAEEAAAAAAAAA\' \\\n\t\t\'UQAAAAAAAABhAAAAAAAAAHEAAAAAAAAAgQA=="", ""dtype"": ""float64"", ""shape"": [2, 4], ""Corder"": \' \\\n\t\t\'true}, {""__ndarray__"": ""b64:GC1EVPshCUBpVxSLCr8FQA=="", ""dtype"": ""float64"", ""shape"": [2]}]\'\n\n\ndef test_encode_compact_cutoff():\n\tdata = [array([[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]]), array([pi, exp(1)])]\n\tgz_json = dumps(data, compression=True, properties={\'ndarray_compact\': 5})\n\tjson = gzip_decompress(gz_json).decode(\'ascii\')\n\tassert json == \'[{""__ndarray__"": ""b64:AAAAAAAA8D8AAAAAAAAAQAAAAAAAAAhAAAAAAAAAEEAAAAAAAAA\' \\\n\t\t\'UQAAAAAAAABhAAAAAAAAAHEAAAAAAAAAgQA=="", ""dtype"": ""float64"", ""shape"": [2, 4], ""Corder"": \' \\\n\t\t\'true}, {""__ndarray__"": [3.141592653589793, 2.718281828459045], ""dtype"": ""float64"", ""shape"": [2]}]\'\n\n\ndef test_encode_compact_inline_compression():\n\tdata = [array([[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0], [9.0, 10.0, 11.0, 12.0], [13.0, 14.0, 15.0, 16.0]])]\n\tjson = dumps(data, compression=False, properties={\'ndarray_compact\': True})\n\tassert \'b64.gz:\' in json, \'If the overall file is not compressed and there are significant savings, then do inline gzip compression.\'\n\tassert json == \'[{""__ndarray__"": ""b64.gz:H4sIAAAAAAAC/2NgAIEP9gwQ4AChOKC0AJQWgdISUFoGSitAaSUorQKl1aC0BpTWgtI6UFoPShs4AABmfqWAgAAAAA=="", ""dtype"": ""float64"", ""shape"": [4, 4], ""Corder"": true}]\'\n\n\ndef test_encode_compact_no_inline_compression():\n\tdata = [array([[1.0, 2.0], [3.0, 4.0]])]\n\tjson = dumps(data, compression=False, properties={\'ndarray_compact\': True})\n\tassert \'b64.gz:\' not in json, \'If the overall file is not compressed, but there are no significant savings, then do not do inline compression.\'\n\tassert json == \'[{""__ndarray__"": ""b64:AAAAAAAA8D8AAAAAAAAAQAAAAAAAAAhAAAAAAAAAEEA="", \' \\\n\t\t\'""dtype"": ""float64"", ""shape"": [2, 2], ""Corder"": true}]\'\n\n\ndef test_decode_compact_mixed_compactness():\n\tjson = \'[{""__ndarray__"": ""b64:AAAAAAAA8D8AAAAAAAAAQAAAAAAAAAhAAAAAAAAAEEAAAAAAAAA\' \\\n\t\t\'UQAAAAAAAABhAAAAAAAAAHEAAAAAAAAAgQA=="", ""dtype"": ""float64"", ""shape"": [2, 4], ""Corder"": \' \\\n\t\t\'true}, {""__ndarray__"": [3.141592653589793, 2.718281828459045], ""dtype"": ""float64"", ""shape"": [2]}]\'\n\tdata = loads(json)\n\tassert_equal(data[0], array([[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]]), array([pi, exp(1)]))\n\n\ndef test_decode_compact_inline_compression():\n\tjson = \'[{""__ndarray__"": ""b64.gz:H4sIAAAAAAAC/2NgAIEP9gwQ4AChOKC0AJQWgdISUFoGSitAaSUorQKl1aC0BpTWgtI6UFoPShs4AABmfqWAgAAAAA=="", ""dtype"": ""float64"", ""shape"": [4, 4], ""Corder"": true}]\'\n\tdata = loads(json)\n\tassert_equal(data[0], array([[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0], [9.0, 10.0, 11.0, 12.0], [13.0, 14.0, 15.0, 16.0]]))\n\n\ndef test_decode_compact_no_inline_compression():\n\tjson = \'[{""__ndarray__"": ""b64:AAAAAAAA8D8AAAAAAAAAQAAAAAAAAAhAAAAAAAAAEEA="", \' \\\n\t\t\'""dtype"": ""float64"", ""shape"": [2, 2], ""Corder"": true}]\'\n\tdata = loads(json)\n\tassert_equal(data[0], array([[1.0, 2.0], [3.0, 4.0]]))\n'"
tests/test_pandas.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom collections import OrderedDict\nfrom numpy import linspace, isnan\nfrom numpy.testing import assert_equal\nfrom pandas import DataFrame, Series\nfrom json_tricks import dumps, loads\nfrom tests.test_bare import nonpdata\n\n\nCOLUMNS = OrderedDict((\n\t('name', ('Alfa', 'Bravo', 'Charlie', 'Delta', 'Echo', 'Foxtrot', 'Golf',\n\t\t'Hotel', 'India', 'Juliett',)),\n\t('count', linspace(0, 10, 10, dtype=int)),\n\t('real', linspace(0, 7.5, 10, dtype=float)),\n\t('special', (float('NaN'), float('+inf'), float('-inf'), float('+0'),\n\t\tfloat('-0'), 1, 2, 3, 4, 5)),\n\t#todo: other types?\n))\n\n\ndef test_pandas_dataframe():\n\tdf = DataFrame(COLUMNS, columns=tuple(COLUMNS.keys()))\n\ttxt = dumps(df, allow_nan=True)\n\tback = loads(txt)\n\tassert isnan(back.iloc[0, -1])\n\tassert (df.equals(back))\n\tassert (df.dtypes == back.dtypes).all()\n\tdf = DataFrame(COLUMNS, columns=tuple(COLUMNS.keys()))\n\ttxt = dumps(df, primitives=True, allow_nan=True)\n\tback = loads(txt)\n\tassert isinstance(back, dict)\n\tassert isnan(back['special'][0])\n\tassert all(df.index.values == tuple(back.pop('index')))\n\tfor name, col in back.items():\n\t\tassert name in COLUMNS\n\t\tassert_equal(list(COLUMNS[name]), col)\n\n\ndef test_pandas_series():\n\tfor name, col in COLUMNS.items():\n\t\tds = Series(data=col, name=name)\n\t\ttxt = dumps(ds, allow_nan=True)\n\t\tback = loads(txt)\n\t\tassert (ds.equals(back))\n\t\tassert ds.dtype == back.dtype\n\tfor name, col in COLUMNS.items():\n\t\tds = Series(data=col, name=name)\n\t\ttxt = dumps(ds, primitives=True, allow_nan=True)\n\t\tback = loads(txt)\n\t\tassert isinstance(back, dict)\n\t\tassert_equal(ds.index.values, back['index'])\n\t\tassert_equal(ds.values, back['data'])\n\n\ndef test_pandas_mixed_with_other_types():\n\tdf = DataFrame(COLUMNS, columns=tuple(COLUMNS.keys()))\n\tmixed = dict(\n\t\tcomplex=1+42j,\n\t\tframes=[df, df],\n\t\t**nonpdata\n\t)\n\ttxt = dumps(mixed, allow_nan=True)\n\tback = loads(txt)\n\tassert mixed['frames'][0].equals(back['frames'][0]) and mixed['frames'][1].equals(back['frames'][1])\n\tdel mixed['frames'], back['frames']  # cannot compare dataframes with '=='\n\tassert mixed == back\n\n\n"""
tests/test_pathlib.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""\nThis tests Paths, which need pathlib.\n""""""\n\nfrom pathlib import Path\n\nfrom json_tricks import dumps, loads\n\n\n# These paths are not necessarily actual paths that exist, but are sufficient\n# for testing to ensure that we can properly serialize/deserialize them.\nPATHS = [\n    Path(),\n    Path(\'c:/users/pyjson_tricks\'),\n    Path(\'/home/users/pyjson_tricks\'),\n    Path(\'../\'),\n    Path(\'..\'),\n    Path(\'./\'),\n    Path(\'.\'),\n    Path(\'test_pathlib.py\'),\n    Path(\'/home/users/pyjson_tricks/test_pathlib.py\'),\n]\n\n\ndef test_path():\n    json = dumps(PATHS)\n    back = loads(json)\n    assert PATHS == back\n\n    for orig, bck in zip(PATHS, back):\n        assert orig == bck\n\n    txt = \'{""__pathlib__"": "".""}\'\n    obj = loads(txt)\n    assert obj == Path()\n\n'"
tests/test_tz.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""\nThis tests timezone-aware date/time objects, which need pytz. Naive date/times should\nwork with just Python code functionality, and are tested in `nonp`.\n""""""\n\nfrom datetime import datetime, date, time, timedelta\nfrom json_tricks import dumps, loads\nfrom json_tricks.utils import is_py3\nimport pytz\n\n\nDTOBJ = [\n\tdatetime(year=1988, month=3, day=15, hour=8, minute=3, second=59, microsecond=7),\n\tpytz.UTC.localize(datetime(year=1988, month=3, day=15, minute=3, second=59, microsecond=7)),\n\tpytz.timezone(\'Europe/Amsterdam\').localize(datetime(year=1988, month=3, day=15, microsecond=7)),\n\tdate(year=1988, month=3, day=15),\n\ttime(hour=8, minute=3, second=59, microsecond=123),\n\ttime(hour=8, second=59, microsecond=123, tzinfo=pytz.timezone(\'Europe/Amsterdam\')),\n\ttimedelta(days=2, seconds=3599),\n\ttimedelta(days=0, seconds=-42, microseconds=123),\n\t[{\'obj\': [pytz.timezone(\'Europe/Amsterdam\').localize(datetime(year=1988, month=3, day=15, microsecond=7))]}],\n]\n\n\ndef test_tzaware_date_time():\n\tjson = dumps(DTOBJ)\n\tback = loads(json)\n\tassert DTOBJ == back\n\tfor orig, bck in zip(DTOBJ, back):\n\t\tassert orig == bck\n\t\tassert type(orig) == type(bck)\n\ttxt = \'{""__datetime__"": null, ""year"": 1988, ""month"": 3, ""day"": 15, ""hour"": 8, ""minute"": 3, \' \\\n\t\t\t\'""second"": 59, ""microsecond"": 7, ""tzinfo"": ""Europe/Amsterdam""}\'\n\tobj = loads(txt)\n\tassert obj == pytz.timezone(\'Europe/Amsterdam\').localize(datetime(year=1988, month=3, day=15, hour=8, minute=3, second=59, microsecond=7))\n\n\ndef test_tzaware_naive_date_time():\n\tjson = dumps(DTOBJ, primitives=True)\n\tback = loads(json)\n\tfor orig, bck in zip(DTOBJ, back):\n\t\tif isinstance(bck, (date, time, datetime,)):\n\t\t\tassert isinstance(bck, str if is_py3 else (str, unicode))\n\t\t\tassert bck == orig.isoformat()\n\t\telif isinstance(bck, (timedelta,)):\n\t\t\tassert isinstance(bck, float)\n\t\t\tassert bck == orig.total_seconds()\n\tdt = pytz.timezone(\'Europe/Amsterdam\').localize(datetime(year=1988, month=3, day=15, hour=8, minute=3, second=59, microsecond=7))\n\tassert dumps(dt, primitives=True).strip(\'""\') == \'1988-03-15T08:03:59.000007+01:00\'\n\n\ndef test_avoiding_tz_datettime_problem():\n\t""""""\n\tThere\'s a weird problem (bug? feature?) when passing timezone object to datetime constructor. This tests checks that json_tricks doesn\'t suffer from this problem.\n\thttps://github.com/mverleg/pyjson_tricks/issues/41  /  https://stackoverflow.com/a/25390097/723090\n\t""""""\n\ttzdt = datetime(2007, 12, 5, 6, 30, 0, 1)\n\ttzdt = pytz.timezone(\'US/Pacific\').localize(tzdt)\n\tback = loads(dumps([tzdt]))[0]\n\tassert pytz.utc.normalize(tzdt) == pytz.utc.normalize(back), \\\n\t\t""Mismatch due to pytz localizing error {} != {}"".format(\n\t\t\tpytz.utc.normalize(tzdt), pytz.utc.normalize(back))\n\n\n'"
tests/test_utils.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom json_tricks.utils import hashodict, get_arg_names, nested_index\n\n\ndef test_hashodict():\n\tdata = hashodict((('alpha', 37), ('beta', 42), ('gamma', -99)))\n\tassert tuple(data.keys()) == ('alpha', 'beta', 'gamma',)\n\tassert isinstance(hash(data), int)\n\n\ndef test_get_args():\n\tdef get_my_args(hello, world=7):\n\t\tpass\n\targnames = get_arg_names(get_my_args)\n\tassert argnames == set(('hello', 'world'))\n\n\ndef test_nested_index():\n\tarr = [[[1, 2], [1, 2]], [[1, 2], [3, 3]]]\n\tassert 1 == nested_index(arr, (0, 0, 0,))\n\tassert 2 == nested_index(arr, (1, 0, 1,))\n\tassert [1, 2] == nested_index(arr, (1, 0,))\n\tassert [3, 3] == nested_index(arr, (1, 1,))\n\tassert [[1, 2], [1, 2]] == nested_index(arr, (0,))\n\tassert [[[1, 2], [1, 2]], [[1, 2], [3, 3]]] == nested_index(arr, ())\n\ttry:\n\t\tnested_index(arr, (0, 0, 0, 0,))\n\texcept TypeError:\n\t\tpass\n\telse:\n\t\traise AssertionError('indexing more than nesting level should yield IndexError')\n\n\ndef base85_vsbase64_performance():\n\tfrom base64 import b85encode, standard_b64encode, urlsafe_b64encode\n\tfrom random import getrandbits\n\ttest_data = bytearray(getrandbits(8) for _ in range(10000000))\n\tfrom timeit import default_timer\n\tprint('')\n\n\tstart = default_timer()\n\tfor _ in range(20):\n\t\tstandard_b64encode(test_data)\n\tend = default_timer()\n\tprint('standard_b64encode took {} s'.format(end - start))\n\n\tstart = default_timer()\n\tfor _ in range(20):\n\t\turlsafe_b64encode(test_data)\n\tend = default_timer()\n\tprint('urlsafe_b64encode took {} s'.format(end - start))\n\n\tstart = default_timer()\n\tfor _ in range(20):\n\t\tb85encode(test_data)\n\tend = default_timer()\n\tprint('b85encode took {} s'.format(end - start))\n\n\t# Result on local PC in 2020: base84 is 53x slower to encode\n\t# (urlsafe also costs a bit of performance, about 2x)\n"""
