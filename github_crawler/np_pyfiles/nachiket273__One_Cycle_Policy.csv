file_path,api_count,code
CLR.py,0,"b'import math\nimport matplotlib.pyplot as plt\n\n\nclass CLR(object):\n    """"""\n    The method is described in paper : https://arxiv.org/abs/1506.01186 to find out optimum \n    learning rate. The learning rate is increased from lower value to higher per iteration \n    for some iterations till loss starts exploding.The learning rate one power lower than \n    the one where loss is minimum is chosen as optimum learning rate for training.\n\n    Args:\n        optim   Optimizer used in training.\n\n        bn      Total number of iterations used for this test run.\n                The learning rate increasing factor is calculated based on this \n                iteration number.\n\n        base_lr The lower boundary for learning rate which will be used as\n                initial learning rate during test run. It is adviced to start from\n                small learning rate value like 1e-4.\n                Default value is 1e-5\n\n        max_lr  The upper boundary for learning rate. This value defines amplitude\n                for learning rate increase(max_lr-base_lr). max_lr value may not be \n                reached in test run as loss may explode before reaching max_lr.\n                It is adviced to use higher value like 10, 100.\n                Default value is 100.\n\n    """"""\n    def __init__(self, optim, bn, base_lr=1e-5, max_lr=100):\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.optim = optim\n        self.bn = bn - 1\n        ratio = self.max_lr/self.base_lr\n        self.mult = ratio ** (1/self.bn)\n        self.best_loss = 1e9\n        self.iteration = 0\n        self.lrs = []\n        self.losses = []\n        \n    def calc_lr(self, loss):\n        self.iteration +=1\n        if math.isnan(loss) or loss > 4 * self.best_loss:\n            return -1\n        if loss < self.best_loss and self.iteration > 1:\n            self.best_loss = loss\n            \n        mult = self.mult ** self.iteration\n        lr = self.base_lr * mult\n        \n        self.lrs.append(lr)\n        self.losses.append(loss)\n        \n        return lr\n        \n    def plot(self, start=10, end=-5):\n        plt.xlabel(""Learning Rate"")\n        plt.ylabel(""Losses"")\n        plt.plot(self.lrs[start:end], self.losses[start:end])\n        plt.xscale(\'log\')\n'"
OneCycle.py,0,"b'import math\n\nclass OneCycle(object):\n    """"""\n    In paper (https://arxiv.org/pdf/1803.09820.pdf), author suggests to do one cycle during \n    whole run with 2 steps of equal length. During first step, increase the learning rate \n    from lower learning rate to higher learning rate. And in second step, decrease it from \n    higher to lower learning rate. This is Cyclic learning rate policy. Author suggests one \n    addition to this. - During last few hundred/thousand iterations of cycle reduce the \n    learning rate to 1/100th or 1/1000th of the lower learning rate.\n\n    Also, Author suggests that reducing momentum when learning rate is increasing. So, we make \n    one cycle of momentum also with learning rate - Decrease momentum when learning rate is \n    increasing and increase momentum when learning rate is decreasing.\n\n    Args:\n        nb              Total number of iterations including all epochs\n\n        max_lr          The optimum learning rate. This learning rate will be used as highest \n                        learning rate. The learning rate will fluctuate between max_lr to\n                        max_lr/div and then (max_lr/div)/div.\n\n        momentum_vals   The maximum and minimum momentum values between which momentum will\n                        fluctuate during cycle.\n                        Default values are (0.95, 0.85)\n\n        prcnt           The percentage of cycle length for which we annihilate learning rate\n                        way below the lower learnig rate.\n                        The default value is 10\n\n        div             The division factor used to get lower boundary of learning rate. This\n                        will be used with max_lr value to decide lower learning rate boundary.\n                        This value is also used to decide how much we annihilate the learning \n                        rate below lower learning rate.\n                        The default value is 10.\n\n        use_cosine      Use cosine annealation instead of linear to change learning rate and \n                        momentum. \n                        The default value is False\n    """"""\n    def __init__(self, nb, max_lr, momentum_vals=(0.95, 0.85), prcnt= 10, div=10, use_cosine=False):\n        self.nb = nb\n        self.div = div\n        self.high_lr = max_lr\n        self.low_mom = momentum_vals[1]\n        self.high_mom = momentum_vals[0]\n        self.use_cosine = use_cosine\n        if self.use_cosine:\n            self.prcnt = 0\n        else:\n            self.prcnt = prcnt\n        self.iteration = 0\n        self.lrs = []\n        self.moms = []\n        if self.use_cosine:\n            self.step_len =  int(self.nb / 4)\n        else:\n            self.step_len =  int(self.nb * (1- prcnt/100)/2)\n        \n    def calc(self):\n        if self.use_cosine:\n            lr = self.calc_lr_cosine()\n            mom = self.calc_mom_cosine()\n        else:\n            lr = self.calc_lr()\n            mom = self.calc_mom()\n        self.iteration += 1\n        return (lr, mom)\n        \n    def calc_lr(self):\n        if self.iteration ==  0:\n            self.lrs.append(self.high_lr/self.div)\n            return self.high_lr/self.div\n        elif self.iteration == self.nb:\n            self.iteration = 0\n            self.lrs.append(self.high_lr/self.div)\n            return self.high_lr/self.div\n        elif self.iteration > 2 * self.step_len:\n            ratio = (self.iteration - 2 * self.step_len) / (self.nb - 2 * self.step_len)\n            #lr = self.high_lr * ( 1 - 0.99 * ratio)/self.div\n            lr = (self.high_lr / self.div) * (1- ratio * (1 - 1/self.div))\n        elif self.iteration > self.step_len:\n            ratio = 1- (self.iteration -self.step_len)/self.step_len\n            lr = self.high_lr * (1 + ratio * (self.div - 1)) / self.div\n        else :\n            ratio = self.iteration/self.step_len\n            lr = self.high_lr * (1 + ratio * (self.div - 1)) / self.div\n        self.lrs.append(lr)\n        return lr\n\n    def calc_mom(self):\n        if self.iteration == 0:\n            self.moms.append(self.high_mom)\n            return self.high_mom\n        elif self.iteration == self.nb:\n            self.iteration = 0\n            self.moms.append(self.high_mom)\n            return self.high_mom\n        elif self.iteration > 2 * self.step_len:\n            mom = self.high_mom\n        elif self.iteration > self.step_len:\n            ratio = (self.iteration -self.step_len)/self.step_len\n            mom = self.low_mom + ratio * (self.high_mom - self.low_mom)\n        else :\n            ratio = self.iteration/self.step_len\n            mom = self.high_mom - ratio * (self.high_mom - self.low_mom)\n        self.moms.append(mom)\n        return mom\n\n    def calc_lr_cosine(self):\n        if self.iteration ==  0:\n            self.lrs.append(self.high_lr/self.div)\n            return self.high_lr/self.div\n        elif self.iteration == self.nb:\n            self.iteration = 0\n            self.lrs.append(self.high_lr/self.div)\n            return self.high_lr/self.div\n        elif self.iteration > self.step_len:\n            ratio = (self.iteration -self.step_len)/(self.nb - self.step_len)\n            lr = (self.high_lr/self.div) + 0.5 * (self.high_lr - self.high_lr/self.div) * (1 + math.cos(math.pi * ratio))\n        else :\n            ratio = self.iteration/self.step_len\n            lr = self.high_lr - 0.5 * (self.high_lr - self.high_lr/self.div) * (1 + math.cos(math.pi * ratio))\n        self.lrs.append(lr)\n        return lr\n\n    def calc_mom_cosine(self):\n        if self.iteration == 0:\n            self.moms.append(self.high_mom)\n            return self.high_mom\n        elif self.iteration == self.nb:\n            self.iteration = 0\n            self.moms.append(self.high_mom)\n            return self.high_mom\n        elif self.iteration > self.step_len:\n            ratio = (self.iteration -self.step_len)/(self.nb - self.step_len)\n            mom = self.high_mom - 0.5 * (self.high_mom - self.low_mom) * (1 + math.cos(math.pi * ratio))\n        else :\n            ratio = self.iteration/self.step_len\n            mom = self.low_mom + 0.5 * (self.high_mom - self.low_mom) * (1 + math.cos(math.pi * ratio))\n        self.moms.append(mom)\n        return mom\n'"
