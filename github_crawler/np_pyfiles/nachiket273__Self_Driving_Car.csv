file_path,api_count,code
CarND-Behavioral-Cloning-P3/augment.py,11,"b'import numpy as np\nimport cv2\n\ndef random_shift(img, steering, shift_range=20):\n    ht, wd, ch = img.shape\n    \n    shift_x = shift_range * (np.random.rand()- 0.5)\n    shift_y = shift_range * (np.random.rand()- 0.5)\n    shift_m = np.float32([[1, 0, shift_x], [0, 1, shift_y]])\n    img = cv2.warpAffine(img, shift_m, (wd, ht))\n    \n    steering += shift_x * 0.002\n    return img, steering\n\ndef random_shadow(img):\n    ht, wd , ch = img.shape\n    x1, y1 = wd * np.random.rand(), 0\n    x2, y2 = wd * np.random.rand(), ht\n    xm, ym = np.mgrid[0:ht, 0:wd]\n    \n    mask = np.zeros_like(img[:, :, 1])\n    mask[(ym - y1) * (x2 - x1) - (y2 - y1) * (xm - x1) > 0] = 1\n\n    cond = mask == np.random.randint(2)\n    s_ratio = np.random.uniform(low=0.6, high=0.9)\n    \n    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n    hls[:, :, 1][cond] = hls[:, :, 1][cond] * s_ratio\n    return cv2.cvtColor(hls, cv2.COLOR_HLS2RGB)\n\ndef adjust_brightness(img):\n    hsv_img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n    ratio = 1.0 + 0.4 * (np.random.rand() - 0.5)\n    hsv_img[:,:,2] =  hsv_img[:,:,2] * ratio\n    return cv2.cvtColor(hsv_img, cv2.COLOR_HSV2RGB)\n\ndef flip(img, steering):\n    if np.random.rand() < 0.5:\n        img = cv2.flip(img, 1)\n        if(steering != 0):\n            steering = -steering \n    return img, steering'"
CarND-Behavioral-Cloning-P3/drive.py,1,"b'import argparse\nimport base64\nfrom datetime import datetime\nimport os\nimport shutil\n\nimport numpy as np\nimport socketio\nimport eventlet\nimport eventlet.wsgi\nfrom PIL import Image\nfrom flask import Flask\nfrom io import BytesIO\nimport cv2\nfrom keras.models import load_model\nimport h5py\nfrom keras import __version__ as keras_version\nimport utils\n\nsio = socketio.Server()\napp = Flask(__name__)\nmodel = None\nprev_image_array = None\n\n\nclass SimplePIController:\n    def __init__(self, Kp, Ki):\n        self.Kp = Kp\n        self.Ki = Ki\n        self.set_point = 0.\n        self.error = 0.\n        self.integral = 0.\n\n    def set_desired(self, desired):\n        self.set_point = desired\n\n    def update(self, measurement):\n        # proportional error\n        self.error = self.set_point - measurement\n\n        # integral error\n        self.integral += self.error\n\n        return self.Kp * self.error + self.Ki * self.integral\n\n\ncontroller = SimplePIController(0.1, 0.002)\nset_speed = 9\ncontroller.set_desired(set_speed)\n\n\n@sio.on(\'telemetry\')\ndef telemetry(sid, data):\n    if data:\n        # The current steering angle of the car\n        steering_angle = data[""steering_angle""]\n        # The current throttle of the car\n        throttle = data[""throttle""]\n        # The current speed of the car\n        speed = data[""speed""]\n        # The current image from the center camera of the car\n        imgString = data[""image""]\n        image = Image.open(BytesIO(base64.b64decode(imgString)))\n        image_array = np.asarray(image)\n        processed_array = utils.preprocess(image_array)\n        steering_angle = float(model.predict(processed_array[None, :, :, :], batch_size=1))\n\n        throttle = controller.update(float(speed))\n\n        print(steering_angle, throttle)\n        send_control(steering_angle, throttle)\n\n        # save frame\n        if args.image_folder != \'\':\n            timestamp = datetime.utcnow().strftime(\'%Y_%m_%d_%H_%M_%S_%f\')[:-3]\n            image_filename = os.path.join(args.image_folder, timestamp)\n            image.save(\'{}.jpg\'.format(image_filename))\n    else:\n        # NOTE: DON\'T EDIT THIS.\n        sio.emit(\'manual\', data={}, skip_sid=True)\n\n\n@sio.on(\'connect\')\ndef connect(sid, environ):\n    print(""connect "", sid)\n    send_control(0, 0)\n\n\ndef send_control(steering_angle, throttle):\n    sio.emit(\n        ""steer"",\n        data={\n            \'steering_angle\': steering_angle.__str__(),\n            \'throttle\': throttle.__str__()\n        },\n        skip_sid=True)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Remote Driving\')\n    parser.add_argument(\n        \'model\',\n        type=str,\n        help=\'Path to model h5 file. Model should be on the same path.\'\n    )\n    parser.add_argument(\n        \'image_folder\',\n        type=str,\n        nargs=\'?\',\n        default=\'\',\n        help=\'Path to image folder. This is where the images from the run will be saved.\'\n    )\n    args = parser.parse_args()\n\n    # check that model Keras version is same as local Keras version\n    f = h5py.File(args.model, mode=\'r\')\n    model_version = f.attrs.get(\'keras_version\')\n    keras_version = str(keras_version).encode(\'utf8\')\n\n    if model_version != keras_version:\n        print(\'You are using Keras version \', keras_version,\n              \', but the model was built using \', model_version)\n\n    model = load_model(args.model)\n\n    if args.image_folder != \'\':\n        print(""Creating image folder at {}"".format(args.image_folder))\n        if not os.path.exists(args.image_folder):\n            os.makedirs(args.image_folder)\n        else:\n            shutil.rmtree(args.image_folder)\n            os.makedirs(args.image_folder)\n        print(""RECORDING THIS RUN ..."")\n    else:\n        print(""NOT RECORDING THIS RUN ..."")\n\n    # wrap Flask application with engineio\'s middleware\n    app = socketio.Middleware(sio, app)\n\n    # deploy as an eventlet WSGI server\n    eventlet.wsgi.server(eventlet.listen((\'\', 4567)), app)\n'"
CarND-Behavioral-Cloning-P3/model.py,0,"b'from tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Lambda, Conv2D, Flatten, Dense, Activation, Cropping2D, Dropout\nimport utils\n\ndef get_model(optimizer, loss=\'mse\'):\n    model = Sequential()\n\n    model.add(Lambda(lambda x: x /127.5 - 1.0, input_shape = (utils.IMG_HT, utils.IMG_WIDTH, utils.IMG_CH)))\n\n    model.add(Conv2D(24, (5, 5), activation=""elu"", strides=(2, 2)))\n    model.add(Conv2D(36, (5, 5), activation=""elu"", strides=(2, 2)))\n    model.add(Conv2D(48, (5, 5), activation=""elu"", strides=(2, 2)))\n    model.add(Conv2D(64, (3, 3), activation=""elu"", strides=(1, 1)))\n    model.add(Conv2D(64, (3, 3), activation=""elu"", strides=(1, 1)))\n\n    model.add(Flatten())\n    model.add(Dropout(0.5))\n\n    model.add(Dense(100))\n    model.add(Activation(\'relu\'))\n    model.add(Dense(50))\n    model.add(Activation(\'relu\'))\n    model.add(Dense(10))\n    model.add(Dense(1))\n\n    model.compile(loss=loss, optimizer=optimizer)\n\n    return model'"
CarND-Behavioral-Cloning-P3/utils.py,7,"b""import cv2\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport sklearn\nimport tensorflow as tf\n\nIMG_HT, IMG_WIDTH, IMG_CH = 66, 200, 3\n\ndef read_img(img_path):\n    return cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n\ndef crop_and_resize(img, width, height):\n    img = img[40:-20, :, :]\n    return cv2.resize(img, (width, height), cv2.INTER_AREA)\n\ndef preprocess(img):\n    img = crop_and_resize(img, IMG_WIDTH, IMG_HT)\n    return cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n\ndef read_csv_and_rename_cols(file_path, columns=None):\n    path = Path(file_path)\n    if path.is_file() and path.exists():\n        if columns:\n            df = pd.read_csv(file_path, header=None)\n            df = df.rename(columns=columns)\n        else :\n            df = pd.read_csv(file_path)\n        return df\n\ndef get_relative_path(img_path):\n    relative_path = './' + img_path.split('/')[-3] + '/' + img_path.split('/')[-2] + '/' + img_path.split('/')[-1]\n    return relative_path\n\ndef get_kernel(no):\n    if no < 2 :\n        return tf.constant(np.ones((3,3,1,1)), tf.float32)\n    return tf.constant(np.ones((5,5,1,1)), tf.float32)\n\ndef get_stride(no):\n    if no < 2 :\n        return [1,1,1,1]\n    return [1,2,2,1]\n\ndef reverse(lst): \n    return [ele for ele in reversed(lst)]\n\ndef get_salient_feature_mask(ops):\n    i = 0\n    upscaled_conv = np.ones((1, 18))\n    layer_ops = reverse(ops)\n    for layer in layer_ops:\n        avg_actvn = np.mean(layer, axis=3).squeeze(axis=0)\n        avg_actvn = avg_actvn * upscaled_conv\n        if i == 4 :\n            output_shape = (IMG_HT, IMG_WIDTH)\n        else :\n            output_shape = (layer_ops[i+1].shape[1], layer_ops[i+1].shape[2])\n        x = tf.constant(np.reshape(avg_actvn, (1, avg_actvn.shape[0], avg_actvn.shape[1], 1)), tf.float32)\n        deconv = tf.nn.conv2d_transpose(x, get_kernel(i),(1, output_shape[0], output_shape[1], 1), get_stride(i), padding='VALID')\n        with tf.Session() as session:\n            res = session.run(deconv)\n        deconv_actn = np.reshape(res, output_shape)\n        upscaled_conv = deconv_actn\n        i += 1\n    mask = (upscaled_conv - np.min(upscaled_conv)) / (np.max(upscaled_conv) - np.min(upscaled_conv))\n    return mask\n\ndef display_multiple_images(img_list, label_list=[], cmap='', is_yuv=True):\n    assert(len(img_list) % 3 == 0)\n    if len(label_list) > 0 :\n        assert(len(img_list) == len(label_list))\n    \n    cols = 3\n    rows = int(len(img_list) / 3)\n    \n    fig = plt.figure(figsize=(16,16))\n    \n    for i in range(1, cols*rows +1):\n        fig.add_subplot(rows, cols, i)       \n        img = img_list[i-1]\n        if len(label_list) > 0:\n            title = label_list[i-1]\n            plt.title(title)\n        if is_yuv:\n            img = cv2.cvtColor(img, cv2.COLOR_YUV2RGB)\n        if cmap != '':\n            plt.imshow(img, cmap=cmap)\n        else:\n            plt.imshow(img)\n    plt.show()"""
CarND-Behavioral-Cloning-P3/video.py,0,"b'from moviepy.editor import ImageSequenceClip\nimport argparse\nimport os\n\nIMAGE_EXT = [\'jpeg\', \'gif\', \'png\', \'jpg\']\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Create driving video.\')\n    parser.add_argument(\n        \'image_folder\',\n        type=str,\n        default=\'\',\n        help=\'Path to image folder. The video will be created from these images.\'\n    )\n    parser.add_argument(\n        \'--fps\',\n        type=int,\n        default=60,\n        help=\'FPS (Frames per second) setting for the video.\')\n    args = parser.parse_args()\n\n    #convert file folder into list firltered for image file types\n    image_list = sorted([os.path.join(args.image_folder, image_file)\n                        for image_file in os.listdir(args.image_folder)])\n    \n    image_list = [image_file for image_file in image_list if os.path.splitext(image_file)[1][1:].lower() in IMAGE_EXT]\n\n    #two methods of naming output video to handle varying environemnts\n    video_file_1 = args.image_folder + \'.mp4\'\n    video_file_2 = args.image_folder + \'output_video.mp4\'\n\n    print(""Creating video {}, FPS={}"".format(args.image_folder, args.fps))\n    clip = ImageSequenceClip(image_list, fps=args.fps)\n    \n    try:\n        clip.write_videofile(video_file_1)\n    except:\n        clip.write_videofile(video_file_2)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
CarND-LaneLines-P1/helper.py,13,"b'import cv2\nimport math\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\nfrom PIL import Image\nimport os\n\ndef grayscale(img):\n    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    \ndef canny(img, low_threshold, high_threshold):\n    return cv2.Canny(img, low_threshold, high_threshold)\n\ndef gaussian_blur(img, kernel_size):\n    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n\ndef region_of_interest(img, vertices):\n    #defining a blank mask to start with\n    mask = np.zeros_like(img)   \n    \n    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n    if len(img.shape) > 2:\n        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n        ignore_mask_color = (255,) * channel_count\n    else:\n        ignore_mask_color = 255\n        \n    #filling pixels inside the polygon defined by ""vertices"" with the fill color    \n    cv2.fillPoly(mask, vertices, ignore_mask_color)\n    \n    #returning the image only where mask pixels are nonzero\n    masked_image = cv2.bitwise_and(img, mask)\n    return masked_image\n\n\ndef draw_lines(img, lines, color=[255, 0, 0], thickness=2):\n    for line in lines:\n        for x1,y1,x2,y2 in line:\n            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n            \n    return img\n\ndef hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n    draw_lines(line_img, lines)\n    return line_img\n\ndef weighted_img(img, initial_img, \xce\xb1=0.8, \xce\xb2=1., \xce\xb3=0.):\n    return cv2.addWeighted(initial_img, \xce\xb1, img, \xce\xb2, \xce\xb3)\n\ndef read_img(img):\n    return mpimg.imread(img)\n\ndef to_hls(img):\n    return cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n\ndef to_hsv(img):\n    return cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n\ndef isolate_color_mask(img, low_thresh, high_thresh):\n    assert(low_thresh.all() >=0  and low_thresh.all() <=255)\n    assert(high_thresh.all() >=0 and high_thresh.all() <=255)\n    return cv2.inRange(img, low_thresh, high_thresh)\n\ndef adjust_gamma(image, gamma=1.0):\n    invGamma = 1.0 / gamma\n    table = np.array([((i / 255.0) ** invGamma) * 255\n        for i in np.arange(0, 256)]).astype(""uint8"")\n \n    # apply gamma correction using the lookup table\n    return cv2.LUT(image, table)\n\ndef save_imgs(img_list, labels, prefix=""Test"", op_folder=""test_imgs_output""):\n    if not os.path.exists(op_folder):\n        os.mkdir(op_folder)\n    for img, label in zip(img_list, labels):\n        PATH = op_folder + ""/"" + prefix + ""_"" + label\n        Image.fromarray(img).save(PATH)\n\ndef display_imgs(img_list, labels=[],cols=2, fig_size=(15,15)):\n    if len(labels) > 0:\n        assert(len(img_list) == len(labels))\n    assert(len(img_list) > 0)\n    cmap = None\n    tot = len(img_list)\n    rows = tot / cols\n    plt.figure(figsize=fig_size)\n    for i in range(tot):\n        plt.subplot(rows, cols, i+1)\n        if len(img_list[i].shape) == 2:\n            cmap = \'gray\'\n        if len(labels) > 0:\n            plt.title(labels[i])\n        plt.imshow(img_list[i], cmap=cmap)\n        \n    plt.tight_layout()\n    plt.show()\n\ndef get_aoi(img):\n    rows, cols = img.shape[:2]\n    mask = np.zeros_like(img)\n    \n    left_bottom = [cols * 0.1, rows]\n    right_bottom = [cols * 0.95, rows]\n    left_top = [cols * 0.4, rows * 0.6]\n    right_top = [cols * 0.6, rows * 0.6]\n    \n    vertices = np.array([[left_bottom, left_top, right_top, right_bottom]], dtype=np.int32)\n    \n    if len(mask.shape) == 2:\n        cv2.fillPoly(mask, vertices, 255)\n    else:\n        cv2.fillPoly(mask, vertices, (255, ) * mask.shape[2])\n    return cv2.bitwise_and(img, mask)\n\ndef get_hough_lines(img, rho=1, theta=np.pi/180, threshold=20, min_line_len=20, max_line_gap=300):\n    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), \n                            minLineLength=min_line_len, maxLineGap=max_line_gap)\n    return lines\n\ndef get_line_length(line):\n    for x1, y1, x2, y2 in line:\n        return np.sqrt((y2-y1)**2 + (x2-x1)**2)\n\ndef get_line_slope_intercept(line):\n    for x1, y1, x2, y2 in line:\n        if x2-x1 == 0:\n            return math.inf, 0\n    slope = (y2-y1)/(x2-x1)\n    intercept = y1 - slope * x1\n    return slope, intercept\n        \ndef get_lines_slope_intecept(lines):\n    left_lines = []\n    right_lines = []\n    left_lengths = []\n    right_lengths = []\n    for line in lines:\n        slope, intercept = get_line_slope_intercept(line)\n        if slope == math.inf:\n            continue\n        line_len = get_line_length(line)\n        if slope < 0:\n            left_lines.append((slope, intercept))\n            left_lengths.append(line_len)\n        else :\n            right_lines.append((slope, intercept))\n            right_lengths.append(line_len)\n            \n    # average\n    left_avg = np.dot(left_lengths, left_lines)/np.sum(left_lengths) if len(left_lengths) > 0 else None\n    right_avg = np.dot(right_lengths, right_lines)/np.sum(right_lengths) if len(right_lengths) > 0 else None\n    \n    return left_avg, right_avg\n\ndef convert_slope_intercept_to_line(y1, y2 , line):\n    if line is None:\n        return None\n    \n    slope, intercept = line\n    x1 = int((y1- intercept)/slope)\n    y1 = int(y1)\n    x2 = int((y2- intercept)/slope)\n    y2 = int(y2)\n    return((x1, y1),(x2, y2))\n\ndef get_lane_lines(img, lines):\n    left_avg, right_avg = get_lines_slope_intecept(lines)\n    \n    y1 = img.shape[0]\n    y2 = img.shape[0] * 0.6\n    \n    left_lane = convert_slope_intercept_to_line(y1, y2, left_avg)\n    right_lane = convert_slope_intercept_to_line(y1, y2, right_avg)\n    return left_lane, right_lane\n\ndef draw_weighted_lines(img, lines, color=[255, 0, 0], thickness=2, alpha = 1.0, beta = 0.95, gamma= 0):\n    mask_img = np.zeros_like(img)\n    for line in lines:\n        if line is not None:\n            cv2.line(mask_img, *line, color, thickness)            \n    return weighted_img(mask_img, img, alpha, beta, gamma)'"
CarND-LaneLines-P1/pipeline.py,5,"b""from collections import deque\nfrom helper import *\nimport cv2\nimport numpy as np\n\n\nclass LaneMemory:\n    def __init__(self, max_entries=50):\n        self.max_entries = max_entries\n        self.left_lanes = deque(maxlen=self.max_entries)\n        self.right_lanes = deque(maxlen=self.max_entries)\n        \n    def process(self, img):\n        result, left_lane , right_lane = process_image(img, calc_mean=True, left_mem= self.left_lanes, right_mem = self.right_lanes)\n        self.left_lanes.append(left_lane)\n        self.right_lanes.append(right_lane)\n        return result\n\n\ndef process_image(image, calc_mean=False, **kwargs):\n    # NOTE: The output you return should be a color image (3 channel) for processing video below\n    # TODO: put your pipeline here,\n    # you should return the final output (image where lines are drawn on lanes)\n    if calc_mean:\n        assert('left_mem' in kwargs.keys())\n        assert('right_mem' in kwargs.keys())\n    \n    original_img = np.copy(image)\n    \n    # convert to grayscale\n    gray_img = grayscale(image)\n    \n    # darken the grayscale\n    darkened_img = adjust_gamma(gray_img, 0.5)\n    \n    # Color Selection\n    white_mask = isolate_color_mask(to_hls(image), np.array([0, 200, 0], dtype=np.uint8), np.array([200, 255, 255], dtype=np.uint8))\n    yellow_mask = isolate_color_mask(to_hls(image), np.array([10, 0, 100], dtype=np.uint8), np.array([40, 255, 255], dtype=np.uint8))\n    mask = cv2.bitwise_or(white_mask, yellow_mask)\n    colored_img = cv2.bitwise_and(darkened_img, darkened_img, mask=mask)\n    \n    # Apply Gaussian Blur\n    blurred_img = gaussian_blur(colored_img, kernel_size=7)\n    \n    # Apply Canny edge filter\n    canny_img = canny(blurred_img, low_threshold=70, high_threshold=140)\n    \n    # Get Area of Interest\n    aoi_img = get_aoi(canny_img)\n    \n    # Apply Hough lines\n    hough_lines = get_hough_lines(aoi_img)\n    hough_img = draw_lines(original_img, hough_lines)\n    \n    # Extrapolation and averaging\n    left_lane, right_lane = get_lane_lines(original_img, hough_lines)\n    \n    if calc_mean:\n        if left_lane is not None and right_lane is not None:\n            kwargs['left_mem'].append(left_lane)\n            kwargs['right_mem'].append(right_lane)\n        left_mean = np.mean(kwargs['left_mem'], axis=0, dtype=np.int32)\n        right_mean = np.mean(kwargs['right_mem'], axis=0, dtype=np.int32)\n        left_lane_avg = tuple(map(tuple, left_mean))\n        right_lane_avg = tuple(map(tuple, right_mean))\n        result = draw_weighted_lines(original_img, [left_lane_avg, right_lane_avg], thickness= 10)\n        return result, left_lane, right_lane\n    \n    result = draw_weighted_lines(original_img, [left_lane, right_lane], thickness= 10)\n       \n    return result, left_lane, right_lane\n\n\n\n"""
CarND-Advanced-Lane-Lines/examples/example.py,0,"b'def warper(img, src, dst):\n\n    # Compute and apply perpective transform\n    img_size = (img.shape[1], img.shape[0])\n    M = cv2.getPerspectiveTransform(src, dst)\n    warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_NEAREST)  # keep same size as input image\n\n    return warped\n'"
