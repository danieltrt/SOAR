file_path,api_count,code
model.py,8,"b'import random\nimport numpy as np\nimport pandas\nimport json\nimport cv2\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.layers import Dense, Dropout, Flatten, Activation\nfrom keras.layers.core import Lambda\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.convolutional import Convolution2D\nfrom keras.layers import Cropping2D\nfrom keras.utils.np_utils import to_categorical\n\nfrom utils import imageUtils\n\n#import image utils and set the image input shape\nimage_utils = imageUtils()\nim_x = image_utils.im_x\nim_y = image_utils.im_y\nim_z = image_utils.im_z\n\ndef get_model():\n    """"""\n        Defines the CNN model architecture and returns the model.\n        The architecture is the same as I developed for project 2\n        https://github.com/neerajdixit/Traffic-Sign-classifier-with-Deep-Learning\n        with an additional normalization layer in front and\n        a final fully connected layer of size 5 since we have 5 different type of objects in our data set.\n    """"""\n\n    # Create a Keras sequential model\n    model = Sequential()\n    #model.add(Cropping2D(cropping=((50,20), (0,0)), input_shape=(160,320,3)))\n    # Add a normalization layer to normalize between -0.5 and 0.5.\n    model.add(Lambda(lambda x: x / 255. - .5,input_shape=(im_x,im_y,im_z), name=\'norm\'))\n    # Add a convolution layer with Input = 32x32x3. Output = 30x30x6. Strides 1 and VALID padding.\n    # Perform RELU activation \n    model.add(Convolution2D(6, 3, 3, subsample=(1, 1), border_mode=""valid"", activation=\'relu\', name=\'conv1\'))\n    # Add a convolution layer with Input = 30x30x6. Output = 28x28x9. Strides 1 and VALID padding.\n    # Perform RELU activation \n    model.add(Convolution2D(9, 3, 3, subsample=(1, 1), border_mode=""valid"", activation=\'relu\', name=\'conv2\'))\n    # Add Pooling layer with Input = 28x28x9. Output = 14x14x9. 2x2 kernel, Strides 2 and VALID padding\n    model.add(MaxPooling2D(pool_size=(2, 2), border_mode=\'valid\', name=\'pool1\'))\n    # Add a convolution layer with Input 14x14x9. Output = 12x12x12. Strides 1 and VALID padding.\n    # Perform RELU activation \n    model.add(Convolution2D(12, 3, 3, subsample=(1, 1), border_mode=""valid"", activation=\'relu\', name=\'conv3\'))\n    # Add a convolution layer with Input = 30x30x6. Output = 28x28x9. Strides 1 and VALID padding.\n    # Perform RELU activation \n    model.add(Convolution2D(16, 3, 3, subsample=(1, 1), border_mode=""valid"", activation=\'relu\', name=\'conv4\'))\n    # Add Pooling layer with Input = 10x10x16. Output = 5x5x16. 2x2 kernel, Strides 2 and VALID padding\n    model.add(MaxPooling2D(pool_size=(2, 2), border_mode=\'valid\', name=\'pool2\'))\n    # Flatten. Input = 5x5x16. Output = 400.\n    model.add(Flatten(name=\'flat1\'))\n    # Add dropout layer with 0.2  \n    model.add(Dropout(0.2, name=\'dropout1\'))\n    # Add Fully Connected layer. Input = 400. Output = 220\n    # Perform RELU activation \n    model.add(Dense(220, activation=\'relu\', name=\'fc1\'))\n    # Add Fully Connected layer. Input = 220. Output = 43\n    # Perform RELU activation \n    model.add(Dense(43, activation=\'relu\', name=\'fc2\'))\n    # Add Fully Connected layer. Input = 43. Output = 5\n    # Perform RELU activation \n    model.add(Dense(5, name=\'fc3\'))\n    # Configure the model for training with Adam optimizer\n    # ""mean squared error"" loss objective and accuracy metrics\n    # Learning rate of 0.001 was chosen because this gave best performance after testing other values\n    model.compile(optimizer=Adam(lr=0.001), loss=""mse"", metrics=[\'accuracy\'])\n    return model\n\ndef data_generator(data_path, X_train, y_train, batch_size):\n    """"""\n        Data generator for kera fit_generator\n    """"""\n    while True:\n        # Select batch_size random indices from the image name array\n        indices = np.random.randint(len(X_train),size=batch_size)\n        # Get the corresponding steering angles\n        y = y_train[indices]\n        # Create empty numpy array of batch size for images\n        x=np.zeros((batch_size, im_x, im_y, im_z))\n        for i in range(batch_size):\n            # Read the image from data path using open cv\n            sample = X_train[indices[i]]\n            img = cv2.imread(data_path+sample[0].strip())\n            # pre process image and add to array\n            x[i] = image_utils.pre_process_image(img, sample[1], sample[2], sample[3], sample[4])\n        yield (x, y)\n\ndef setup_data(path):\n    """"""\n        Reads the log file from data_path and creates the data used by generators.\n        Takes in the log file location as parameter\n    """"""\n    # Read the csv file using pandas\n    train_data = pandas.read_csv(path+\'train.csv\', delim_whitespace=True, header = None)\n    # frame, xmin, ymin, xmax, ymax, occluded, label\n    X_train = np.dstack((np.array(train_data[0]),\n                         np.array(train_data[1]),\n                         np.array(train_data[2]),\n                         np.array(train_data[3]),\n                         np.array(train_data[4])))\n    y_train = np.array(train_data[6])\n    y_train = y_train.reshape(1,y_train.shape[0])\n    # binarize the labels\n    # car = 0,truck = 1,biker = 2,pedestrian = 3,trafficLight = 4\n    yy_train = to_categorical(y_train[0])\n    # Shuffle the data.\n    # Important before dividing the test data into training and validation sets. To avoid same data on multiple runs.\n    X_train, y_train = shuffle(X_train[0], yy_train)\n    # Split the test data in training and validation sets.\n    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train , test_size=0.3, random_state=0)\n    print(""Number of training examples ="", len(X_train))\n    print(""Number of validation examples ="", len(X_validation))\n    return X_train, X_validation, y_train, y_validation\n\n\n######## Processing ########\ndata_path = \'./object-dataset/\'\n# Setup data from drive log csv\nX_train, X_validation, y_train, y_validation = setup_data(\'./\')\n\n#Get model and print summary.\nmodel = get_model()\nprint(model.summary())\n\n# Set batch size and epocs\nper_epoch_samples=50000#len(X_train)\ngen_batch_size=500\nepochs=50\n\n# Fit the data on model and validate using data generators. \nmodel.fit_generator(data_generator(data_path, \n                                    X_train, \n                                    y_train, \n                                    gen_batch_size),\n                    samples_per_epoch=per_epoch_samples, \n                    nb_epoch=epochs,\n                    validation_data=data_generator(data_path, \n                        X_validation, \n                        y_validation, \n                        gen_batch_size),\n                    nb_val_samples=len(X_validation))\n\n# Save the model and weights\nprint(""Saving model weights and configuration file..."")\nmodel.save_weights(""model.h5"", True)\nwith open(\'model.json\', \'w\') as outfile:\n    json.dump(model.to_json(), outfile)\nprint(""model saved..."")\n'"
searchObject.py,14,"b'import json\r\nimport numpy as np\r\nimport cv2\r\nimport pandas\r\n\r\nfrom utils import imageUtils\r\nfrom keras.optimizers import Adam\r\nfrom keras.models import model_from_json\r\nfrom scipy.ndimage.measurements import label\r\n\r\nfrom moviepy.editor import VideoFileClip\r\n\r\nimage_utils = imageUtils()\r\n\r\ndef draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\r\n    """"""\r\n        Draw boxes on the detected objects.\r\n    """"""\r\n    # Make a copy of the image\r\n    imcopy = np.copy(img)\r\n    # Iterate through the bounding boxes\r\n    for bbox in bboxes:\r\n        # Draw a rectangle given bbox coordinates\r\n        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\r\n    # Return the image copy with boxes drawn\r\n    return imcopy\r\n\r\ndef slide_window(img, x_start_stop=[None, None],\r\n                      y_start_stop=[None, None],\r\n                      xy_window=(64, 64), xy_overlap=(0.5, 0.5)):\r\n    """"""\r\n        Function to get a list of windows to search on image.\r\n    """"""\r\n    # If x and/or y start/stop positions not defined, set to image size\r\n    if x_start_stop[0] == None:\r\n        x_start_stop[0] = 0\r\n    if x_start_stop[1] == None:\r\n        x_start_stop[1] = img.shape[1]\r\n    if y_start_stop[0] == None:\r\n        y_start_stop[0] = 0\r\n    if y_start_stop[1] == None:\r\n        y_start_stop[1] = img.shape[0]\r\n    # Compute the span of the region to be searched    \r\n    xspan = x_start_stop[1] - x_start_stop[0]\r\n    yspan = y_start_stop[1] - y_start_stop[0]\r\n    # Compute the number of pixels per step in x/y\r\n    nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))\r\n    ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))\r\n    # Compute the number of windows in x/y\r\n    nx_buffer = np.int(xy_window[0]*(xy_overlap[0]))\r\n    ny_buffer = np.int(xy_window[1]*(xy_overlap[1]))\r\n    nx_windows = np.int((xspan-nx_buffer)/nx_pix_per_step) \r\n    ny_windows = np.int((yspan-ny_buffer)/ny_pix_per_step) \r\n    # Initialize a list to append window positions to\r\n    window_list = []\r\n    # Loop through finding x and y window positions\r\n    # Note: you could vectorize this step, but in practice\r\n    # you\'ll be considering windows one by one with your\r\n    # classifier, so looping makes sense\r\n    for ys in range(ny_windows):\r\n        for xs in range(nx_windows):\r\n            # Calculate window position\r\n            startx = xs*nx_pix_per_step + x_start_stop[0]\r\n            endx = startx + xy_window[0]\r\n            starty = ys*ny_pix_per_step + y_start_stop[0]\r\n            endy = starty + xy_window[1]\r\n            # Append window position to list\r\n            window_list.append(((startx, starty), (endx, endy)))\r\n    return window_list\r\n\r\ndef add_heat(heatmap, bbox_list):\r\n    """"""\r\n       Iterate through list of boxes and mark the detected boxes.\r\n    """"""\r\n    for box in bbox_list:\r\n        # Add += 1 for all pixels inside each bbox\r\n        # Box form ((x1, y1), (x2, y2))\r\n        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\r\n    return heatmap\r\n\r\ndef apply_threshold(heatmap, threshold):\r\n    """"""\r\n        Threshhold the detections to reduce the number of detections.\r\n    """"""\r\n    # Zero out pixels below the threshold\r\n    heatmap[heatmap <= threshold] = 0\r\n    # Return thresholded map\r\n    return heatmap\r\n\r\ndef search_windows(img, windows):\r\n    """"""\r\n        use the model to predict the objects and return the windows.\r\n    """"""\r\n    #1) Create an empty list to receive positive detection windows\r\n    on_windows = []\r\n    #2) Iterate over all windows in the list\r\n    for window in windows:\r\n        #3) Extract the test window from original image\r\n        image_array = image_utils.pre_process_image(img[window[0][1]:window[1][1], window[0][0]:window[1][0]])\r\n        transformed_image_array = image_array[None, :, :, :]\r\n        pred = model.predict_classes(transformed_image_array, batch_size=1)\r\n        if pred == 0:\r\n            on_windows.append(window)\r\n        # print (pred)\r\n        # pred = model.predict(transformed_image_array, batch_size=1)\r\n        # max = np.argmax(pred)\r\n        # if max == 0 and pred[0][max] > 0.90:\r\n        #     on_windows.append(window)\r\n    #8) Return windows for positive detections\r\n    return on_windows\r\n\r\ndef draw_labeled_bboxes(img, labels):\r\n    """"""\r\n        Draw the boxes around detected object.\r\n    """"""\r\n    # Iterate through all detected cars\r\n    for car_number in range(1, labels[1]+1):\r\n        # Find pixels with each car_number label value\r\n        nonzero = (labels[0] == car_number).nonzero()\r\n        # Identify x and y values of those pixels\r\n        nonzeroy = np.array(nonzero[0])\r\n        nonzerox = np.array(nonzero[1])\r\n        # Define a bounding box based on min/max x and y\r\n        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\r\n        # Draw the box on the image\r\n        cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 6)\r\n    return img\r\n\r\ndef process_img(img):\r\n    """"""\r\n        Wrapper function.\r\n    """"""\r\n    heat = np.zeros_like(img[:,:,0]).astype(np.float)\r\n    xy_win=int((img.shape[0]*0.9 - img.shape[0]*0.6)/2)\r\n    windows = slide_window(img, x_start_stop=[0, img.shape[1]],\r\n                                y_start_stop=[int(img.shape[0]*0.6), int(img.shape[0]*0.9)], \r\n                                xy_window=(64, 64), xy_overlap=(0.5, 0.5))\r\n\r\n    hot_windows = search_windows(img, windows)\r\n    heat = add_heat(heat,hot_windows)\r\n    heat = apply_threshold(heat,2)\r\n    heatmap = np.clip(heat, 0, 255)\r\n    labels = label(heatmap)\r\n    return draw_labeled_bboxes(np.copy(img), labels)\r\n    \r\njfile = open(\'model.json\', \'r\')\r\nmodel = model_from_json(json.loads(jfile.read()))\r\nmodel.load_weights(\'model.h5\')\r\n\r\nimg = cv2.imread(\'test4.jpg\')\r\ndraw_img=process_img(img)\r\nimage_utils.plot_images([draw_img])\r\n\r\n# project_video_res = \'project_video_res.mp4\'\r\n# clip1 = VideoFileClip(""project_video.mp4"")\r\n# project_video_clip = clip1.fl_image(process_img)\r\n# project_video_clip.write_videofile(project_video_res, audio=False)\r\n\r\n# project_video_res = \'test_video_res.mp4\'\r\n# clip1 = VideoFileClip(""test_video.mp4"")\r\n# project_video_clip = clip1.fl_image(process_img)\r\n# project_video_clip.write_videofile(project_video_res, audio=False)'"
utils.py,0,"b'import cv2\nimport math\nimport matplotlib.pyplot as plt\n\nclass imageUtils(object):\n\t""""""utility class for image processing """"""\n\t\n\tdef __init__(self):\n\t\tpass\n\n\t# Final image size 32x32x3 to be used by the CNN\n\tim_x = 32\n\tim_y = 32\n\tim_z = 3\n\n\tdef plot_images(self, images):\n\t\t"""""" Function to plot test images """"""\n\t\tfor index in range(len(images)):\n\t\t    plt.imshow(images[index])\n\t\tplt.show()\n\n\tdef pre_process_image(self, img, xmin=None, ymin=None, xmax=None, ymax=None):\n\t\t""""""\n\t\t\tFunction to pre process images before feeding them into the network\n\t\t""""""\n\t\t\n\t\t#self.plot_images([img])\n\t\t# Get image shape\n\t\timg_shape = img.shape\n\t\t# Crop the top and bottom to remove unwanted features\n\t\tif xmin is not None:\n\t\t\timg = img[ymin:ymax, xmin:xmax]\n\t\timg = cv2.resize(img,(self.im_x,self.im_y), interpolation=cv2.INTER_AREA)\n\t\t#self.plot_images([img])\n\t\treturn img\n'"
