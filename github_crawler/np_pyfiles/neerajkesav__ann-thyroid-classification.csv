file_path,api_count,code
src/__init__.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Apr 24 12:43:30 2019\n\n@author: neeraj\n""""""\n\n'"
src/data_preprocessor.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nDataPreprocessor.\n@author: neeraj\n""""""\nimport pandas as pd\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nclass DataPreprocessor:\n    """"""Class DataPreprocessor. To perform preprocessing on dataset.\n     \n    Methods:\n        preprocess(): to get the clean data\n        split_predictors(): splits dataset to X and y variables.\n        scale_data(): Scales the train and test data\n        validation_split(): splits the train data to get train and validation data\n    """"""\n    \n    def preprocess(self, train, test):\n        """"""Takes arguments \'train\', \'test\'.\n        train: training data\n        test: testing data\n        \n        Returns clean \'train\' and \'test\' data.\n        """"""\n        #This dataset from UCI is almost a clean one. So nothing much to do.\n        column_list = [\'age\', \'sex\', \'on_thyroxine\', \'query_on_thyroxine\', \'on_antithyroid_medication\', \'sick\', \'pregnant\', \'thyroid_surgery\', \'I131_treatment\', \'query_hypothyroid\', \'query_hyperthyroid\', \'lithium\', \'goitre\', \'tumor\', \'hypopituitary\', \'psych\', \'TSH\', \'T3\', \'TT4\', \'T4U\', \'FTI\', \'Class\']\n        train = pd.DataFrame(train.iloc[:,0:22].values, columns=column_list)\n        test = pd.DataFrame(test.iloc[:,0:22].values, columns=column_list)\n        \n        return train, test\n    \n    def split_predictors(self, data):\n        """"""Takes argument \'data\'\n        data: data to split on predictors nd target variable.\n        \n        Returns the X and y variable wise parts of data\n        """"""\n        #Predictor data\n        data_X = data.drop([\'Class\'], axis=1)\n        \n        #Target data\n        data_y = data[\'Class\']\n        \n        return data_X, data_y\n    \n    def scale_data(self, train_X, test_X):\n        """"""Takes argument \'train_X\', \'test_X\'.\n        train_X: predictor data for training\n        test_X: predictor data for testing\n        \n        Returns the scaled train and test data.\n        """"""\n        sc = StandardScaler()\n        #scaling train data\n        train_X = sc.fit_transform(train_X)\n        \n        #scaling test data in the same sacle of train data.\n        test_X = sc.transform(test_X)\n        \n        return train_X, test_X\n    \n    def validation_split(self, train_X, train_y, test_size = 0.2, random_state = 1):\n        """"""Takes arguments \'train_X\', \'train_y\', \'test_size\', \'random_state\'.\n        train_X: predictor data of train dataset\n        train_y: target data of train dataset\n        test_size: fraction of test set to be splitted from train data. Default=0.2\n        random_state: Default= 1    \n        \n        Splits the train data to get validation dataset. \n        \n        Returns train and validation data as X and y parts.\n        """"""\n        #Spliting data\n        X_train, X_validation, y_train, y_validation = train_test_split(train_X, train_y, test_size = 0.2, random_state = 1)        \n        \n        return X_train, X_validation, y_train, y_validation\n        \n'"
src/dataset_loader.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nDatasetLoader.\n@author: neeraj kesavan\n""""""\nimport pandas\n\nclass DatasetLoader:\n    """""" Class DatasetLoader. To load dataset. DatasetLoader have the\n    following properties:\n    \n    Attributes:\n        path: path to the dataset.\n    \n    Methods:\n        __init__(): Constructor. initialize variable path.\n        load(): load datset to data from specified path and return data.\n        print_shape(): print the shape of data.\n    \n    """"""\n    \n    path = """"\n       \n    def __init__(self, path):\n        """"""Takes arguments \'path\'  and initializes class variable.        \n        """"""\n        self.path = path\n        \n    def load(self):\n        """"""Takes no arguments, load dataset to \'data\' from path\n        and returns data.\n        \n        data: loaded with dataset.       \n        """"""\n        data = pandas.read_csv(self.path, header=None, sep=\' \')\n        return data\n        \n    def print_shape(self, data):\n        """"""Takes argument \'data\', prints its shape.\n        data: contains dataset.         \n        """"""\n        print(data.shape)\n'"
src/main.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nMain.\n\n@author: neeraj\n""""""\nfrom dataset_loader import DatasetLoader\nfrom data_preprocessor import DataPreprocessor\nfrom model_builder import ModelBuilder\n\n      \n#Loading dataset  \ndata_loader = DatasetLoader(\'../resources/ann-train.data\')\ntrain = data_loader.load()\n\ndata_loader = DatasetLoader(\'../resources/ann-test.data\')\ntest = data_loader.load()\n\n#Preprocessing data\ndp = DataPreprocessor()\ntrain, test = dp.preprocess(train, test)\n\n#Splitting data to predictors and target vaiables\ntrain_X, train_y = dp.split_predictors(train)\ntest_X, test_y = dp.split_predictors(test)\n\n\n#splitting data for validation set\nX_train, X_val, y_train, y_val = dp.validation_split(train_X, train_y)\n\n#scaling train and validation data\nX_train, X_val = dp.scale_data(X_train, X_val)\n\n#model is defined in the ModelBuilder class.\nmb = ModelBuilder()\nclassifier = mb.get_classifier()        \n       \n#cross-validation on smaller set of training data\nmb.validate(classifier, X_train, y_train)\n##Cross Validation - Accuracy : 98.11% (1.13%)\n\n#evaluation model using validation set\nmb.evaluate(classifier, X_train, y_train, X_val, y_val)\n##Accuracy is 99.073% \n\n#scaling train and test data\ntrain_X, test_X = dp.scale_data(train_X, test_X)\n\n#cross-validation on complete train data\nmb.validate(classifier, train_X, train_y)\n##Cross Validation - Accuracy : 98.57% (0.41%)\n\n#Train with complete train data\nclassifier.fit(train_X, train_y, batch_size = 10, epochs = 100)\n\n#predicting on test data\nmb.check_prediction(classifier, test_X, test_y)\n#Test Data - Accuracy is 98.279% \n\n#Saving model to disk\nmb.save_model(classifier, \'../model/final_model1\')\n\n#Save predictions\nmb.save_predictions(classifier, test_X)     '"
src/model_builder.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nModelBuilder.\n\n@author: neeraj kesavan\n""""""\nimport pickle\nimport numpy\nimport pandas\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nclass ModelBuilder:\n    """"""Class ModelBuilder. Creates, validate, evaluate and saves the model.\n    \n    Methods:\n        classifier_model(): defines the neural network model to be used in the scikit-learn wrapper.\n        get_classifier(): gets the scikit-learn wrapped keras classifier model.\n        finalize_and_save(): fits the final model and save to disk.\n        save_model(): saves the model to disk.\n        load_model(): loads and returns model from disk.\n        check_prediction(): checks prediction accuracy.\n        save_predictions(): save predictions to disk.        \n    \n    """"""\n    \n    def classifier_model(self):\n        """"""Takes no arguments.\n        Defines the neural network model.\n        \n        Returns the \'model\'\n        """"""\n        #Sequential model\n        model = Sequential()\n        \n        #Input layer and first hidden layer.\n        model.add(Dense(48, kernel_initializer = \'uniform\', input_dim=21, activation=\'relu\'))\n        \n        #25% of neurons are droppedout to avoid over learning/fitting.\n        model.add(Dropout(0.25))\n        \n        #2nd hidden layer\n        model.add(Dense(48, kernel_initializer = \'uniform\', activation=\'relu\'))\n        \n        #25% of neurons are droppedout to avoid over learning/fitting.\n        model.add(Dropout(0.25))\n        \n        #3rd hidden layer\n        model.add(Dense(48, kernel_initializer = \'uniform\', activation=\'relu\'))\n        \n        #25% of neurons are droppedout to avoid over learning/fitting.\n        model.add(Dropout(0.25))\n        \n        #Output layer\n        model.add(Dense(3, kernel_initializer = \'uniform\', activation=\'softmax\'))\n        \n    \t#Compile model\n        model.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\n        \n        return model\n    \n    def get_classifier(self):\n        """"""Takes no argument.\n        Creates classifier model usig scikit-learn wrapper in Keras.\n        \n        Returns \'classifier\'.\n        """"""\n        #Creates model\n        classifier = KerasClassifier(build_fn = self.classifier_model, batch_size = 10, epochs = 100)\n        \n        return classifier\n    \n    def finalize_and_save(self, model, train_X, train_y, filename=\'../model/final_model\'):\n        """"""Takes arguments \'model\', \'filename\', \'train_X\', \'train_y\'.\n        model: finalized model.\n        filename: path+filename to which model to be saved.\n        train_X: input part of train_set.\n        train_y: output part of train_set.\n        \n        Saves the model to disk.\n        """"""\n        #Fits the model to train data\n        model.fit(train_X, train_y)\n        \n        #Saves the model to disk\n        self.save_model(model, filename)\n        \n    def save_model(self, model, filename=\'../model/saved_model\'):\n        """"""Takes arguments \'model\', \'filename\'.\n        model: finalized model.\n        filename: path+filename to which model to be saved.\n        \n        Saves the model to disk.\n        """"""\n        #Save the model to disk\n        pickle.dump(model, open(filename, \'wb\' ))\n        print(""\\nModel is saved..\\n"")\n    \n    def load_model(self, model_filename):\n        """"""Takes argument \'model_filename\'.\n        model_filename: path+filename of model to be loaded.\n        \n        Returns the loaded \'model\'\n        """"""\n        #Load the model from disk\n        loaded_model = pickle.load(open(model_filename, \'rb\' ))\n        \n        return loaded_model\n    \n    def validate(self, model, train_X, train_y):\n        """"""Takes arguments: \'model\', \'train_X\', \'train_y\'.\n        model: model to be validated.\n        train_X: input part of dataset.\n        train_y: output part of dataset.\n        \n        Perfoms cross-validation on the specified model and prints accuracy.\n        """"""\n        results = cross_val_score(estimator = model, X = train_X, y = train_y, cv = 10, n_jobs = 3)\n        \n        print(""\\nCross Validation - Accuracy : %.2f%% (%.2f%%)\\n"" % (results.mean()*100.0, results.std()*100.0))\n        \n    def evaluate(self, model, train_X, train_y, test_X, test_y):\n        """"""Takes arguments: \'model\', \'train_X\', \'train_y\', \'test_X\', \'test_y\'.\n        model: model to be evaluated.\n        train_X: input part of train data.\n        train_y: output part of train data.\n        test_X: input part of test data - validation.\n        test_y: output part of test data - validation.\n        \n        Perfoms evaluation on the specified model and prints accuracy.\n        """"""\n        #Fits the model to traindata\n        model.fit(train_X, train_y, batch_size = 10, epochs = 100) \n        \n        #prediction\n        y_test_pred = model.predict(test_X)\n        \n        #Confustion matrix from predictions\n        cm = confusion_matrix(test_y, y_test_pred)\n        \n        print(""\\nModel Evaluation - Accuracy is %.3f%% \\n"" % ((cm[0][0]+cm[1][1]+cm[2][2])*100/test_y.size))\n        \n    def check_prediction(self, model, test_X, test_y):\n        """"""Takes arguments: \'model\', \'test_X\', \'test_y\'.\n        model: model to be evaluated.\n        test_X: input part of test data\n        test_y: output part of test data\n\n        Perfoms predicton and prints accuracy.\n        """"""\n        #Prediction\n        y_test_pred = model.predict(test_X)\n        \n        #Confustion matrix from predictions\n        cm = confusion_matrix(test_y, y_test_pred)\n        \n        #Map predictions to class name\n        y_test_pred = self.map_pred_class(y_test_pred)\n        \n        print(""\\n............Predictions............\\n"")\n        print(y_test_pred.reshape(-1,1))\n        print(""\\nTest Data - Accuracy is %.3f%% \\n"" % ((cm[0][0]+cm[1][1]+cm[2][2])*100/test_y.size))\n    \n    def save_predictions(self, model, test_X):\n        """"""Takes arguments: \'model\', \'test_X\'.\n        model: model to be evaluated.\n        test_X: test data input to make prediction\n\n        Perfoms prediction and saves to disk.\n        """"""\n        #Prediction\n        predictions = model.predict(test_X)\n\n        #Map predictions to class name\n        predictions = self.map_pred_class(predictions)\n        \n        #Saves to disk\n        pandas.DataFrame(predictions).to_csv(\'../prediction/predictions.csv\', index=False)\n        print(""\\nPredictions are saved..\\n"")\n    \n    def map_pred_class(self, preditions):\n        """"""Takes argument \'predictions\'.\n        predictions: contains precited integer class values to be mapped to actual classes.\n        \n        Returns predicted class names.\n        """"""\n        \n        pred_map = [\'Normal\'  if(x==3) else \'Subnormal\' if (x==2) else \'HyperThyroid\'  for x in preditions]\n        \n        return numpy.array(pred_map)'"
