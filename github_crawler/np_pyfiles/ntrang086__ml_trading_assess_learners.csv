file_path,api_count,code
BagLearner.py,10,"b'""""""Implement Bag Learner""""""\n# TODO: Implement Boosting\n\nimport numpy as np\nimport LinRegLearner, DTLearner, RTLearner\n\n\nclass BagLearner(object):\n\n    def __init__(self, learner, bags=20, boost=False, verbose=False, **kwargs):\n        """"""Initalize a Bag Learner\n\n        Parameters:\n        learner: A LinRegLearner, DTLearner, or RTLearner\n        bags: The number of learners to be trained using Bootstrap Aggregation\n        boost: If true, boosting will be implemented\n        verbose: If True, information about the learner will be printed out\n        kwargs: Keyword arguments to be passed on to the learner\'s constructor\n        \n        Returns: An instance of Bag Learner\n        """"""\n        self.verbose = verbose\n        learners = []\n        for i in range(bags):\n            learners.append(learner(**kwargs))\n        self.learners = learners\n        self.kwargs = kwargs\n        self.bags = bags\n        self.boost = boost\n        if verbose:\n            self.get_learner_info()\n\n        \n    def addEvidence(self, dataX, dataY):\n        """"""Add training data to learner\n\n        Parameters:\n        dataX: A numpy ndarray of X values to add\n        dataY: A numpy 1D array of Y values to add\n\n        Returns: Updated individual learners in BagLearner\n        """"""\n        # Sample the data with replacement\n        num_samples = dataX.shape[0]\n        for learner in self.learners:\n            idx = np.random.choice(num_samples, num_samples)\n            bagX = dataX[idx]\n            bagY = dataY[idx]\n            learner.addEvidence(bagX, bagY)\n        if self.verbose:\n            self.get_learner_info()\n        \n        \n    def query(self, points):\n        """"""Estimates a set of test points given the model we built\n        \n        Parameters:\n        points: A numpy ndarray of test queries\n\n        Returns: \n        preds: A numpy 1D array of the estimated values\n        """"""\n        preds = np.array([learner.query(points) for learner in self.learners])\n        return np.mean(preds, axis=0)\n\n\n    def get_learner_info(self):\n        """"""Print out data for this BagLearner""""""\n        learner_name = str(type(self.learners[0]))[8:-2]\n        print (""This BagLearner is made up of {} {}:"".\n            format(self.bags, learner_name))\n\n        print (""kwargs ="", self.kwargs)\n        print (""boost ="", self.boost)\n\n        # Print out information for each learner within BagLearner\n        for i in range(1, self.bags + 1):\n            print (learner_name, ""#{}:"".format(i)); \n            self.learners[i-1].get_learner_info() \n\n\nif __name__==""__main__"":\n    print (""This is a Bag Learner\\n"")\n\n    # Some data to test the BagLearner\n    x0 = np.array([0.885, 0.725, 0.560, 0.735, 0.610, 0.260, 0.500, 0.320])\n    x1 = np.array([0.330, 0.390, 0.500, 0.570, 0.630, 0.630, 0.680, 0.780])\n    x2 = np.array([9.100, 10.900, 9.400, 9.800, 8.400, 11.800, 10.500, 10.000])\n    x = np.array([x0, x1, x2]).T\n    \n    y = np.array([4.000, 5.000, 6.000, 5.000, 3.000, 8.000, 7.000, 6.000])\n\n    # Create a BagLearner from given training x and y\n    bag_learner = BagLearner(DTLearner.DTLearner, verbose=True)\n    \n    print (""\\nAdd data"")\n    bag_learner.addEvidence(x, y)\n\n    # Query with dummy data\n    print (""Query with dummy data:\\n"", np.array([[1, 2, 3], [0.2, 12, 12]]))\n    print (""Query results:"", bag_learner.query(np.array([[1, 2, 3], [0.2, 12, 12]]))) '"
DTLearner.py,20,"b'""""""Implement a Decision Tree Learner""""""\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import pearsonr\nfrom copy import deepcopy\nfrom collections import Counter\nfrom operator import itemgetter\n\n\nclass DTLearner(object):\n\n    def __init__(self, leaf_size=1, verbose=False, tree=None):\n        """"""Initalize a Decision Tree Learner\n\n        Parameters:\n        leaf_size: The maximum number of samples to be aggregated at a leaf\n        verbose: If True, information about the learner will be printed out\n        tree: If None, the learner instance has no data. If not None, tree is a numpy ndarray. \n        Its columns are the features of data and its rows are the individual samples. The four \n        columns are feature indices (index for a leaf is -1), splitting values (or Y values for\n        leaves), and starting rows, from the current root, for its left and right subtrees (if any)\n        \n        Returns: An instance of Decision Tree Learner\n        """"""\n        self.leaf_size = leaf_size\n        self.verbose = verbose\n        self.tree = deepcopy(tree)\n        if verbose:\n            self.get_learner_info()\n        \n\n    def __build_tree(self, dataX, dataY):\n        """"""Builds the Decision Tree recursively by choosing the best feature to split on and \n        the splitting value. The best feature has the highest absolute correlation with dataY. \n        If all features have the same absolute correlation, choose the first feature. The \n        splitting value is the median of the data according to the best feature. \n        If the best feature doesn\'t split the data into two groups, choose the second best \n        one and so on; if none of the features does, return leaf\n\n        Parameters:\n        dataX: A numpy ndarray of X values at each node\n        dataY: A numpy 1D array of Y values at each node\n        \n        Returns:\n        tree: A numpy ndarray. Each row represents a node and four columns are feature indices \n        (int type; index for a leaf is -1), splitting values, and starting rows, from the current \n        root, for its left and right subtrees (if any)\n\n        """"""\n        # Get the number of samples (rows) and features (columns) of dataX\n        num_samples = dataX.shape[0]\n        num_feats = dataX.shape[1]\n\n        # Leaf value is the most common dataY\n        leaf = np.array([-1, Counter(dataY).most_common(1)[0][0], np.nan, np.nan])\n        \n        # If there are <= leaf_size samples or all data in dataY are the same, return leaf\n        if num_samples <= self.leaf_size or len(pd.unique(dataY)) == 1:\n            return leaf\n    \n        avail_feats_for_split = list(range(num_feats))\n\n        # Get a list of tuples of features and their correlations with dataY\n        feats_corrs = []\n        for feat_i in range(num_feats):\n            abs_corr = abs(pearsonr(dataX[:, feat_i], dataY)[0])\n            if np.isnan(abs_corr):\n                abs_corr = 0.0\n            feats_corrs.append((feat_i, abs_corr))\n        \n        # Sort the list in descending order by correlation\n        feats_corrs = sorted(feats_corrs, key=itemgetter(1), reverse=True)\n\n        # Choose the best feature, if any, by iterating over feats_corrs\n        feat_corr_i = 0\n        while len(avail_feats_for_split) > 0:\n            best_feat_i = feats_corrs[feat_corr_i][0]\n            best_abs_corr = feats_corrs[feat_corr_i][1]\n\n            # Split the data according to the best feature\n            split_val = np.median(dataX[:, best_feat_i])\n\n            # Logical arrays for indexing\n            left_index = dataX[:, best_feat_i] <= split_val\n            right_index = dataX[:, best_feat_i] > split_val\n\n            # If we can split the data into two groups, then break out of the loop            \n            if len(np.unique(left_index)) != 1:\n                break\n            \n            avail_feats_for_split.remove(best_feat_i)\n            feat_corr_i += 1\n        \n        # If we complete the while loop and run out of features to split, return leaf\n        if len(avail_feats_for_split) == 0:\n            return leaf\n\n        # Build left and right branches and the root                    \n        lefttree = self.__build_tree(dataX[left_index], dataY[left_index])\n        righttree = self.__build_tree(dataX[right_index], dataY[right_index])\n\n        # Set the starting row for the right subtree of the current root\n        if lefttree.ndim == 1:\n            righttree_start = 2 # The right subtree starts 2 rows down\n        elif lefttree.ndim > 1:\n            righttree_start = lefttree.shape[0] + 1\n        root = np.array([best_feat_i, split_val, 1, righttree_start])\n\n        return np.vstack((root, lefttree, righttree))\n    \n\n    def __tree_search(self, point, row):\n        """"""A private function to be used with query. It recursively searches \n        the decision tree matrix and returns a predicted value for point\n\n        Parameters:\n        point: A numpy 1D array of test query\n        row: The row of the decision tree matrix to search\n    \n        Returns \n        pred: The predicted value\n        """"""\n\n        # Get the feature on the row and its corresponding splitting value\n        feat, split_val = self.tree[row, 0:2]\n        \n        # If splitting value of feature is -1, we have reached a leaf so return it\n        if feat == -1:\n            return split_val\n\n        # If the corresponding feature\'s value from point <= split_val, go to the left tree\n        elif point[int(feat)] <= split_val:\n            pred = self.__tree_search(point, row + int(self.tree[row, 2]))\n\n        # Otherwise, go to the right tree\n        else:\n            pred = self.__tree_search(point, row + int(self.tree[row, 3]))\n        \n        return pred\n\n\n    def addEvidence(self, dataX, dataY):\n        """"""Add training data to learner\n\n        Parameters:\n        dataX: A numpy ndarray of X values of data to add\n        dataY: A numpy 1D array of Y training values\n\n        Returns: An updated tree matrix for DTLearner\n        """"""\n\n        new_tree = self.__build_tree(dataX, dataY)\n\n        # If self.tree is currently None, simply assign new_tree to it\n        if self.tree is None:\n            self.tree = new_tree\n\n        # Otherwise, append new_tree to self.tree\n        else:\n            self.tree = np.vstack((self.tree, new_tree))\n        \n        # If there is only a single row, expand tree to a numpy ndarray for consistency\n        if len(self.tree.shape) == 1:\n            self.tree = np.expand_dims(self.tree, axis=0)\n        \n        if self.verbose:\n            self.get_learner_info()\n        \n        \n    def query(self, points):\n        """"""Estimates a set of test points given the model we built\n        \n        Parameters:\n        points: A numpy ndarray of test queries\n\n        Returns: \n        preds: A numpy 1D array of the estimated values\n        """"""\n\n        preds = []\n        for point in points:\n            preds.append(self.__tree_search(point, row=0))\n        return np.asarray(preds)\n\n\n    def get_learner_info(self):\n        """"""Print out data for this learner""""""\n        print (""leaf_size ="", self.leaf_size)\n        if self.tree is not None:\n            print (""tree shape ="", self.tree.shape)\n            print (""tree as a matrix:"")\n            # Create a dataframe from tree for a user-friendly view\n            df_tree = pd.DataFrame(self.tree, \n                columns=[""factor"", ""split_val"", ""left"", ""right""])\n            df_tree.index.name = ""node""\n            print (df_tree)\n        else:\n            print (""Tree has no data"")\n        print ("""")\n\n\nif __name__==""__main__"":\n    print (""This is a Decision Tree Learner\\n"")\n\n    # Some data to test the DTLearner\n    x0 = np.array([0.885, 0.725, 0.560, 0.735, 0.610, 0.260, 0.500, 0.320])\n    x1 = np.array([0.330, 0.390, 0.500, 0.570, 0.630, 0.630, 0.680, 0.780])\n    x2 = np.array([9.100, 10.900, 9.400, 9.800, 8.400, 11.800, 10.500, 10.000])\n    x = np.array([x0, x1, x2]).T\n    \n    y = np.array([4.000, 5.000, 6.000, 5.000, 3.000, 8.000, 7.000, 6.000])\n\n    # Create a tree learner from given train X and y\n    dtl = DTLearner(verbose=True, leaf_size=1)\n    print (""\\nAdd data"")\n    dtl.addEvidence(x, y)\n\n    print (""\\nCreate another tree learner from an existing tree"")\n    dtl2 = DTLearner(tree=dtl.tree)\n\n    # dtl2 should have the same tree as dtl\n    assert np.any(dtl.tree == dtl2.tree)\n\n    dtl2.get_learner_info()\n\n    # Modify the dtl2.tree and assert that this doesn\'t affect dtl.tree\n    dtl2.tree[0] = np.arange(dtl2.tree.shape[1])\n    assert np.any(dtl.tree != dtl2.tree)\n\n    # Query with dummy data\n    dtl.query(np.array([[1, 2, 3], [0.2, 12, 12]]))\n\n    # Another dataset to test that ""If the best feature doesn\'t split the data into two\n    # groups, choose the second best one and so on; if none of the features does, return leaf""\n    print (""Another dataset for testing"")\n    x2 = np.array([\n     [  0.26,    0.63,   11.8  ],\n     [  0.26,    0.63,   11.8  ],\n     [  0.32,    0.78,   10.   ],\n     [  0.32,    0.78,   10.   ],\n     [  0.32,    0.78,   10.   ],\n     [  0.735,   0.57,    9.8  ],\n     [  0.26,    0.63,   11.8  ],\n     [  0.61,    0.63,    8.4  ]])\n        \n    y2 = np.array([ 8.,  8.,  6.,  6.,  6.,  5.,  8.,  3.])\n        \n    dtl = DTLearner(verbose=True)\n    dtl.addEvidence(x2, y2)'"
InsaneLearner.py,9,"b'""""""Implement Insane Learner""""""\n\nimport numpy as np\nimport LinRegLearner, DTLearner, RTLearner, BagLearner\n\n\nclass InsaneLearner(object):\n\n    def __init__(self, bag_learner=BagLearner.BagLearner, learner=DTLearner.DTLearner, \n        num_bag_learners=20, verbose=False, **kwargs):\n        """"""Initalize an Insane Learner\n\n        Parameters:\n        bag_learner: A BagLearner\n        learner: A LinRegLearner, DTLearner, or RTLearner to be called by bag_learner\n        num_bag_learners: The number of Bag learners to be trained\n        verbose: If True, information about the learner will be printed out\n        kwargs: Keyword arguments to be passed on to the learner\'s constructor\n        \n        Returns: An instance of Insane Learner\n        """"""\n        self.verbose = verbose\n        bag_learners = []\n        for i in range(num_bag_learners):\n            bag_learners.append(bag_learner(learner=learner, **kwargs))\n        self.bag_learners = bag_learners\n        self.kwargs = kwargs\n        self.num_bag_learners = num_bag_learners\n        if verbose:\n            self.get_learner_info()\n\n        \n    def addEvidence(self, dataX, dataY):\n        """"""Add training data to learner\n\n        Parameters:\n        dataX: A numpy ndarray of X values to add\n        dataY: A numpy 1D array of Y values to add\n\n        Returns: Updated individual bag learners within InsaneLearner\n        """"""\n        for bag_learner in self.bag_learners:\n            bag_learner.addEvidence(dataX, dataY)\n        if self.verbose:\n            self.get_learner_info()\n        \n        \n    def query(self, points):\n        """"""Estimates a set of test points given the model we built\n        \n        Parameters:\n        points: A numpy ndarray of test queries\n\n        Returns: \n        preds: A numpy 1D array of the estimated values\n        """"""\n        preds = np.array([learner.query(points) for learner in self.bag_learners])\n        return np.mean(preds, axis=0)\n\n\n    def get_learner_info(self):\n        """"""Print out data for this InsaneLearner""""""\n        bag_learner_name = str(type(self.bag_learners[0]))[8:-2]\n        print (""This InsaneLearner is made up of {} {}:"".\n            format(self.num_bag_learners, bag_learner_name))\n        print (""kwargs ="", self.kwargs)\n\n        # Print out information for each learner within InsaneLearner\n        for i in range(1, self.num_bag_learners + 1):\n            print (bag_learner_name, ""#{}:"".format(i)); \n            self.bag_learners[i-1].get_learner_info() \n\n\nif __name__==""__main__"":\n    print (""This is a Insane Learner\\n"")\n    \n    # Some data to test the InsaneLearner\n    x0 = np.array([0.885, 0.725, 0.560, 0.735, 0.610, 0.260, 0.500, 0.320])\n    x1 = np.array([0.330, 0.390, 0.500, 0.570, 0.630, 0.630, 0.680, 0.780])\n    x2 = np.array([9.100, 10.900, 9.400, 9.800, 8.400, 11.800, 10.500, 10.000])\n    x = np.array([x0, x1, x2]).T\n    \n    y = np.array([4.000, 5.000, 6.000, 5.000, 3.000, 8.000, 7.000, 6.000])\n\n    # Create an InsaneLearner from given training x and y\n    insane_learner = InsaneLearner(verbose=True)\n    \n    print (""\\nAdd data"")\n    insane_learner.addEvidence(x, y)\n    \n    # Query with dummy data\n    print (""Query with dummy data:\\n"", np.array([[1, 2, 3], [0.2, 12, 12]]))\n    print (""Query results:"", insane_learner.query(np.array([[1, 2, 3], [0.2, 12, 12]]))) \n    \n'"
LinRegLearner.py,4,"b'""""""A wrapper for linear regression""""""\n\nimport numpy as np\n\n\nclass LinRegLearner(object):\n\n    def __init__(self, verbose = False):\n        """"""Initalize a Linear Regression Learner\n\n        Parameters:\n        model_coefs: A nummpy ndarray of least-squares solution\n        residuals: A numpy ndarray of sums of residuals\n        rank: The rank of the first input matrix to np.linalg.lstsq()\n        s: An ndarray of singular values of the first input matrix to np.linalg.lstsq()\n\n        Returns: An instance of Linear Regression Learner\n        """"""\n        self.model_coefs = None\n        self.residuals = None\n        self.rank = None\n        self.s = None\n        self.verbose = verbose\n        if verbose:\n            self.get_learner_info()\n\n\n    def addEvidence(self, dataX, dataY):\n        """"""Add training data to learner\n\n        Parameters:\n        dataX: A numpy ndarray of X values of data to add\n        dataY: A numpy 1D array of Y values to add\n\n        Returns: Update the instance variables\n        """"""\n\n        # Add 1s column so linear regression finds a constant term\n        newdataX = np.ones([dataX.shape[0], dataX.shape[1] + 1])\n        newdataX[:,0:dataX.shape[1]] = dataX\n\n        # build and save the model\n        self.model_coefs, self.residuals, self.rank, self.s = np.linalg.lstsq(newdataX, dataY)\n\n        if self.verbose:\n            self.get_learner_info()\n        \n    \n    def query(self, points):\n        """"""Estimate a set of test points given the model we built\n\n        Parameters:\n        points: A numpy array with each row corresponding to a specific query\n\n        Returns: the estimated values according to the saved model\n        """"""\n        return (self.model_coefs[:-1] * points).sum(axis=1) + self.model_coefs[-1]\n\n\n    def get_learner_info(self):\n        """"""Print out data for this learner""""""\n        print (""Model coefficient matrix:"", self.model_coefs)\n        print (""Sums of residuals:"", self.residuals)\n        print ("""")\n\n\nif __name__==""__main__"":\n    print (""This is a Linear Regression Learner"")\n'"
RTLearner.py,23,"b'""""""A simple wrapper for Random Tree regression""""""\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import pearsonr\nfrom copy import deepcopy\nfrom collections import Counter\n\n\nclass RTLearner(object):\n\n    def __init__(self, leaf_size=1, verbose=False, tree=None):\n        """"""Initalize a Random Tree Learner\n\n        Parameters:\n        leaf_size: The maximum number of samples to be aggregated at a leaf. \n        verbose: If True, information about the learner will be printed out\n        tree: If None, the learner instance has no data. If not None, tree is a numpy ndarray. \n        Its columns are the features of data and its rows are the individual samples. The four \n        columns are feature indices (index for a leaf is -1), splitting values (or Y values for\n        leaves), and starting rows, from the current root, for its left and right subtrees (if any)\n        \n        Returns: A instance of Random Tree Learner\n        """"""\n        self.leaf_size = leaf_size\n        self.verbose = verbose\n        self.tree = deepcopy(tree)\n        if verbose:\n            self.get_learner_info()\n        \n\n    def __build_tree(self, dataX, dataY):\n        """"""Builds the Random Tree recursively by randomly choosing a feature to split on. \n        The splitting value is the mean of feature values of two random rows. If none of the \n        features can split the data into two groups, return leaf\n\n        Parameters:\n        dataX: A numpy ndarray of X values at each node\n        dataY: A numpy 1D array of Y values at each node\n        \n        Returns:\n        tree: A numpy ndarray. Its columns are the features of data and its rows are the \n        individual samples. The four columns are feature indices (index for a leaf is -1), \n        splitting values(or Y values for leaves), and starting rows, from the current root, \n        for its left and right subtrees (if any)\n\n        """"""\n        # Get the number of samples (rows) and features (columns) of dataX\n        num_samples = dataX.shape[0]\n        num_feats = dataX.shape[1]\n\n        # Leaf value is the most common dataY\n        leaf = np.array([-1, Counter(dataY).most_common(1)[0][0], np.nan, np.nan])\n        \n        # If there are <= leaf_size samples or all data in dataY are the same, return leaf\n        if num_samples <= self.leaf_size or len(np.unique(dataY)) == 1:\n            return leaf\n        \n        avail_feats_for_split = list(range(num_feats))\n\n        # If a randomly-chosen feature can\'t split the data, choose another one randomly\n        while len(avail_feats_for_split) > 0:\n            # Randomly choose a feature to split on\n            rand_feat_i = np.random.choice(avail_feats_for_split)\n\n            # Randomly choose two rows\n            rand_rows = [np.random.randint(0, num_samples), np.random.randint(0, num_samples)]\n            \n            # If the two rows are the same, reselect them until they are different\n            while rand_rows[0] == rand_rows[1] and num_samples > 1:\n                rand_rows = [np.random.randint(0, num_samples), np.random.randint(0, num_samples)]\n\n            # Split the data by computing the mean of feature values of two random rows\n            split_val = np.mean([dataX[rand_rows[0], rand_feat_i], \n                            dataX[rand_rows[1], rand_feat_i]])\n\n            # Logical arrays for indexing\n            left_index = dataX[:, rand_feat_i] <= split_val\n            right_index = dataX[:, rand_feat_i] > split_val\n\n            # If we can split the data into two groups, then break out of the loop            \n            if len(np.unique(left_index)) != 1:\n                break\n            \n            avail_feats_for_split.remove(rand_feat_i)\n        \n        # If we complete the while loop and run out of features to split, return leaf\n        if len(avail_feats_for_split) == 0:\n            return leaf\n\n        # Build left and right branches and the root\n        lefttree = self.__build_tree(dataX[left_index], dataY[left_index])\n        righttree = self.__build_tree(dataX[right_index], dataY[right_index])\n\n        # Set the starting row for the right subtree of the current root\n        if lefttree.ndim == 1:\n            righttree_start = 2 # The right subtree starts 2 rows down\n        elif lefttree.ndim > 1:\n            righttree_start = lefttree.shape[0] + 1\n        root = np.array([rand_feat_i, split_val, 1, righttree_start])\n\n        return np.vstack((root, lefttree, righttree))\n        \n\n    def __tree_search(self, point, row):\n        """"""A private function to be used with query. It recursively searches \n        the random tree matrix and returns a predicted value for point\n\n        Parameters:\n        point: A numpy 1D array of test query\n        row: The row of the random tree matrix to search\n    \n        Returns \n        pred: The predicted value\n        """"""\n\n        # Get the feature on the row and its corresponding splitting value\n        feat, split_val = self.tree[row, 0:2]\n        \n        # If splitting value of feature is -1, we have reached a leaf so return it\n        if feat == -1:\n            return split_val\n\n        # If the corresponding feature\'s value from point <= split_val, go to the left tree\n        elif point[int(feat)] <= split_val:\n            pred = self.__tree_search(point, row + int(self.tree[row, 2]))\n\n        # Otherwise, go to the right tree\n        else:\n            pred = self.__tree_search(point, row + int(self.tree[row, 3]))\n        \n        return pred\n\n\n    def addEvidence(self, dataX, dataY):\n        """"""Add training data to learner\n\n        Parameters:\n        dataX: A numpy ndarray of X values of data to add\n        dataY: A numpy 1D array of Y training values\n\n        Returns: An updated tree matrix for RTLearner\n        """"""\n\n        new_tree = self.__build_tree(dataX, dataY)\n\n        # If self.tree is currently None, simply assign new_tree to it\n        if self.tree is None:\n            self.tree = new_tree\n\n        # Otherwise, append new_tree to self.tree\n        else:\n            self.tree = np.vstack((self.tree, new_tree))\n\n        # If there is only a single row, expand tree to a numpy ndarray for consistency\n        if len(self.tree.shape) == 1:\n            self.tree = np.expand_dims(self.tree, axis=0)\n        \n        if self.verbose:\n            self.get_learner_info()\n        \n        \n    def query(self, points):\n        """"""Estimates a set of test points given the model we built\n        \n        Parameters:\n        points: A numpy ndarray of test queries\n\n        Returns: \n        preds: A numpy 1D array of the estimated values\n        """"""\n\n        preds = []\n        for point in points:\n            preds.append(self.__tree_search(point, row=0))\n        return np.asarray(preds)\n\n\n    def get_learner_info(self):\n        print (""leaf_size ="", self.leaf_size)\n        if self.tree is not None:\n            print (""tree shape ="", self.tree.shape)\n            print (""tree as a matrix:"")\n            # Create a dataframe from tree for a user-friendly view\n            df_tree = pd.DataFrame(self.tree, \n                columns=[""factor"", ""split_val"", ""left"", ""right""])\n            df_tree.index.name = ""node""\n            print (df_tree)\n        else:\n            print (""Tree has no data"")\n        print ("""")\n\n\nif __name__==""__main__"":\n    print (""This is a Random Tree Learner\\n"")\n\n    # Some data to test the RTLearner\n    x0 = np.array([0.885, 0.725, 0.560, 0.735, 0.610, 0.260, 0.500, 0.320])\n    x1 = np.array([0.330, 0.390, 0.500, 0.570, 0.630, 0.630, 0.680, 0.780])\n    x2 = np.array([9.100, 10.900, 9.400, 9.800, 8.400, 11.800, 10.500, 10.000])\n    x = np.array([x0, x1, x2]).T\n    \n    y = np.array([4.000, 5.000, 6.000, 5.000, 3.000, 8.000, 7.000, 6.000])\n\n    # Create a tree learner from given train X and y\n    rtl = RTLearner(verbose=True, leaf_size=1)\n    print (""\\nAdd data"")\n    rtl.addEvidence(x, y)\n\n    print (""\\nCreate another tree learner from an existing tree"")\n    rtl2 = RTLearner(tree=rtl.tree)\n\n    # rtl2 should have the same tree as rtl\n    assert np.any(rtl.tree == rtl2.tree)\n\n    rtl2.get_learner_info()\n\n    # Modify the rtl2.tree and assert that this doesn\'t affect rtl.tree\n    rtl2.tree[0] = np.arange(rtl2.tree.shape[1])\n    assert np.any(rtl.tree != rtl2.tree)\n\n    # Query with dummy data\n    rtl.query(np.array([[1, 2, 3], [0.2, 12, 12]]))\n\n    # Another dataset to test that ""If the best feature doesn\'t split the data into two\n    # groups, choose the second best one and so on; if none of the features does, return leaf""\n    print (""Another dataset for testing"")\n    x2 = np.array([\n     [  0.26,    0.63,   11.8  ],\n     [  0.26,    0.63,   11.8  ],\n     [  0.32,    0.78,   10.   ],\n     [  0.32,    0.78,   10.   ],\n     [  0.32,    0.78,   10.   ],\n     [  0.735,   0.57,    9.8  ],\n     [  0.26,    0.63,   11.8  ],\n     [  0.61,    0.63,    8.4  ]])\n        \n    y2 = np.array([ 8.,  8.,  6.,  6.,  6.,  5.,  8.,  3.])\n        \n    rtl3 = RTLearner(verbose=True)\n    rtl3.addEvidence(x2, y2)'"
analyze_learners_util.py,16,"b'""""""Utils for analyzing learners""""""\n\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n\ndef process_data(filename, train_size=0.6):\n    """"""Reads data from a file and split the data into training and test sets""""""\n    data = np.genfromtxt(filename, delimiter="","")\n    \n    # If the data has a header, remove it\n    if np.isnan(data[0]).all():\n        print (""Remove the header"")\n        data = data[1:]\n    \n    # If the data\'s first column is non-numerical (e.g. date), remove it\n    if np.isnan(data[:, 0]).all():\n        print (""Remove the non-numerical column (1st one)"")\n        data = data[:, 1:]\n\n    np.random.shuffle(data)\n\n    # Compute how much of the data is training and testing\n    train_rows = int(math.floor(train_size * data.shape[0]))\n    test_rows = data.shape[0] - train_rows\n\n    # Separate out training and testing data\n    trainX = data[:train_rows, 0:-1]\n    trainY = data[:train_rows, -1]\n    testX = data[train_rows:, 0:-1]\n    testY = data[train_rows:, -1]\n\n    return trainX, trainY, testX, testY\n\n\ndef train_test_learner(trainX, trainY, testX, testY, learner_arg, num_iterations=1, \n    max_leaf_size=None, max_bag_size=None, **kwargs):\n    \n    """"""Train and test a learner\n\n    Parameters:\n    trainX, trainY, testX, testY: Training and test data\n    learner_arg: A DTLearner, RTLearner or BagLearner\n    num_iterations: Number of times we train and test the data\n    max_leaf_size: The max value of the leaf size range on which we train a tree learner\n    max_bag_size: The max value of the bag size range on which we train a bag learner\n    kwargs: Keyword arguments to be passed on to the learner\'s constructor\n    \n    Returns:\n    RMSEin_mean: A numpy 1D array of means of root mean square errors (RMSEs) \n                for in-sample data\n    RMSEout_mean: A numpy 1D array of means of RMSEs for out-of-sample data\n    CORRin_mean: A numpy 1D array of medians of correlations \n                    between predicted and actual results for in-sample data\n    CORRout_mean: A numpy 1D array of medians of correlations for out-of-sample data\n    """"""\n\n    # Make sure that either of these variables is not None\n    if max_leaf_size is None and max_bag_size is None:\n        print (""Please specify the max_leaf_size or max_bag_size and try again;"")\n        print (""Returning fake data filled with zeros for now"")\n        return np.zeros((1, 1)), np.zeros((1, 1)), np.zeros((1, 1)), np.zeros((1, 1))\n\n    max_val = max_leaf_size or max_bag_size\n    # Initialize two ndarrays for in-sample and out-of-sample root mean squared errors\n    RMSEin = np.zeros((max_val, num_iterations))\n    RMSEout = np.zeros((max_val, num_iterations))\n\n    # Initialize two ndarrays for in-sample and out-of-sample correlations\n    CORRin = np.zeros((max_val, num_iterations))\n    CORRout = np.zeros((max_val, num_iterations))\n\n    # Train the learner and record RMSEs\n    for i in range(1, max_val):\n        for j in range(num_iterations):\n            # Create a learner and train it\n            if max_leaf_size is not None:\n                learner = learner_arg(leaf_size=i, **kwargs)\n            elif max_bag_size is not None:\n                learner = learner_arg(bags=i, **kwargs)\n            learner.addEvidence(trainX, trainY)\n\n            # Evaluate in-sample\n            predY = learner.query(trainX)\n            rmse = math.sqrt(((trainY - predY) ** 2).sum()/trainY.shape[0])\n            RMSEin[i, j] = rmse\n            c = np.corrcoef(predY, y=trainY)\n            CORRin[i, j] = c[0, 1]\n\n            # Evaluate out-of-sample\n            predY = learner.query(testX)\n            rmse = math.sqrt(((testY - predY) ** 2).sum()/testY.shape[0])\n            RMSEout[i, j] = rmse\n            c = np.corrcoef(predY, y=testY)\n            CORRout[i, j] = c[0, 1]\n    \n    # Get the means of RMSEs from all iterations\n    RMSEin_mean = np.mean(RMSEin, axis=1)\n    RMSEout_mean = np.mean(RMSEout, axis=1)\n\n    # Get the medians of correlations from all iterations\n    CORRin_mean = np.median(CORRin, axis=1)\n    CORRout_mean = np.median(CORRout, axis=1)\n\n    return RMSEin_mean, RMSEout_mean, CORRin_mean, CORRout_mean\n\n\ndef plot_results(in_sample, out_of_sample, title, xlabel, ylabel, \n    legend_loc=""lower right"", xaxis_length=1):\n    \n    """"""Plot the results, e.g. RMSEs or correlations from training and testing a learner\n    \n    Parameters:\n    in_sample: A numpy 1D array of in-sample data\n    out_of_sample: A numpy 1D array of out-of-sample data\n    title: The chart title\n    xlabel: x-axis label\n    ylabel: y-axis label\n    legend_loc: Location of legend\n    xaxis_length: The length of the x-axis\n\n    Returns: Plot the data\n    """"""\n\n    xaxis = np.arange(1, xaxis_length + 1)\n    plt.plot(xaxis, in_sample, label=""in-sample"", linewidth=2.0)\n    plt.plot(xaxis, out_of_sample, label=""out-of-sample"", linewidth=2.0)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.legend(loc=legend_loc)\n    plt.title(title)\n    plt.show()'"
