file_path,api_count,code
DTLearner.py,21,"b'""""""A simple wrapper for Decision Tree regression""""""\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import pearsonr\nfrom copy import deepcopy\nfrom collections import Counter\nfrom operator import itemgetter\n\n\nclass DTLearner(object):\n\n    def __init__(self, leaf_size=1, verbose=False, tree=None):\n        self.leaf_size = leaf_size\n        self.verbose = verbose\n        self.tree = deepcopy(tree)\n        if verbose:\n            self.get_learner_info()\n        \n\n    def __build_tree(self, dataX, dataY, rootX=[], rootY=[]):\n        """"""Builds the Decision Tree recursively by choosing the best feature to split on and \n        the splitting value. The best feature has the highest absolute correlation with dataY. \n        If all features have the same absolute correlation, choose the first feature. The \n        splitting value is the median of the data according to the best feature. \n        If the best feature doesn\'t split the data into two groups, choose the second best \n        one and so on; if none of the features does, return leaf\n\n        Parameters:\n        dataX: A numpy ndarray of X values at each node\n        dataY: A numpy 1D array of Y values at each node\n        rootX: A numpy ndarray of X values at the parent/root node of the current one\n        rootY: A numpy 1D array of Y values at the parent/root node of the current one\n        \n        Returns:\n        tree: A numpy ndarray. Each row represents a node and four columns are feature indices \n        (int type; index for a leaf is -1), splitting values, and starting rows, from the current \n        root, for its left and right subtrees (if any)\n\n        """"""\n        # Get the number of samples (rows) and features (columns) of dataX\n        num_samples = dataX.shape[0]\n        num_feats = dataX.shape[1]\n\n        # If there is no sample left, return the most common value from the root of current node\n        if num_samples == 0:\n            return np.array([-1, Counter(rootY).most_common(1)[0][0], np.nan, np.nan])\n\n        # If there are <= leaf_size samples or all data in dataY are the same, return leaf\n        if num_samples <= self.leaf_size or len(pd.unique(dataY)) == 1:\n            return np.array([-1, Counter(dataY).most_common(1)[0][0], np.nan, np.nan])\n    \n        avail_feats_for_split = list(range(num_feats))\n\n        # Get a list of tuples of features and their correlations with dataY\n        feats_corrs = []\n        for feat_i in range(num_feats):\n            abs_corr = abs(pearsonr(dataX[:, feat_i], dataY)[0])\n            feats_corrs.append((feat_i, abs_corr))\n        \n        # Sort the list in descending order by correlation\n        feats_corrs = sorted(feats_corrs, key=itemgetter(1), reverse=True)\n\n        # Choose the best feature, if any, by iterating over feats_corrs\n        feat_corr_i = 0\n        while len(avail_feats_for_split) > 0:\n            best_feat_i = feats_corrs[feat_corr_i][0]\n            best_abs_corr = feats_corrs[feat_corr_i][1]\n\n            # Split the data according to the best feature\n            split_val = np.median(dataX[:, best_feat_i])\n\n            # Logical arrays for indexing\n            left_index = dataX[:, best_feat_i] <= split_val\n            right_index = dataX[:, best_feat_i] > split_val\n\n            # If we can split the data into two groups, then break out of the loop            \n            if len(np.unique(left_index)) != 1:\n                break\n            \n            avail_feats_for_split.remove(best_feat_i)\n            feat_corr_i += 1\n        \n        # If we complete the while loop and run out of features to split, return leaf\n        if len(avail_feats_for_split) == 0:\n            return np.array([-1, Counter(dataY).most_common(1)[0][0], np.nan, np.nan])\n\n        # Build left and right branches and the root                    \n        lefttree = self.__build_tree(dataX[left_index], dataY[left_index], dataX, dataY)\n        righttree = self.__build_tree(dataX[right_index], dataY[right_index], dataX, dataY)\n\n        # Set the starting row for the right subtree of the current root\n        if lefttree.ndim == 1:\n            righttree_start = 2 # The right subtree starts 2 rows down\n        elif lefttree.ndim > 1:\n            righttree_start = lefttree.shape[0] + 1\n        root = np.array([best_feat_i, split_val, 1, righttree_start])\n\n        return np.vstack((root, lefttree, righttree))\n    \n\n    def __tree_search(self, point, row):\n        """"""A private function to be used with query. It recursively searches \n        the decision tree matrix and returns a predicted value for point\n\n        Parameters:\n        point: A numpy 1D array of test query\n        row: The row of the decision tree matrix to search\n    \n        Returns \n        pred: The predicted value\n        """"""\n\n        # Get the feature on the row and its corresponding splitting value\n        feat, split_val = self.tree[row, 0:2]\n        \n        # If splitting value of feature is -1, we have reached a leaf so return it\n        if feat == -1:\n            return split_val\n\n        # If the corresponding feature\'s value from point <= split_val, go to the left tree\n        elif point[int(feat)] <= split_val:\n            pred = self.__tree_search(point, row + int(self.tree[row, 2]))\n\n        # Otherwise, go to the right tree\n        else:\n            pred = self.__tree_search(point, row + int(self.tree[row, 3]))\n        \n        return pred\n\n\n    def addEvidence(self, dataX, dataY):\n        """"""Add training data to learner\n\n        Parameters:\n        dataX: A numpy ndarray of X values of data to add\n        dataY: A numpy 1D array of Y training values\n\n        Returns: An updated tree matrix for DTLearner\n        """"""\n\n        new_tree = self.__build_tree(dataX, dataY)\n\n        # If self.tree is currently None, simply assign new_tree to it\n        if self.tree is None:\n            self.tree = new_tree\n\n        # Otherwise, append new_tree to self.tree\n        else:\n            self.tree = np.vstack((self.tree, new_tree))\n        \n        # If there is only a single row, expand tree to a numpy ndarray for consistency\n        if len(self.tree.shape) == 1:\n            self.tree = np.expand_dims(self.tree, axis=0)\n        \n        if self.verbose:\n            self.get_learner_info()\n        \n        \n    def query(self, points):\n        """"""Estimates a set of test points given the model we built\n        \n        Parameters:\n        points: A numpy ndarray of test queries\n\n        Returns: \n        preds: A numpy 1D array of the estimated values\n        """"""\n\n        preds = []\n        for point in points:\n            preds.append(self.__tree_search(point, row=0))\n        return np.asarray(preds)\n\n\n    def get_learner_info(self):\n        print (""Info about this Decision Tree Learner:"")\n        print (""leaf_size ="", self.leaf_size)\n        if self.tree is not None:\n            print (""tree shape ="", self.tree.shape)\n            print (""tree as a matrix:"")\n            # Create a dataframe from tree for a user-friendly view\n            df_tree = pd.DataFrame(self.tree, columns=[""factor"", ""split_val"", ""left"", ""right""])\n            df_tree.index.name = ""node""\n            print (df_tree)\n        else:\n            print (""Tree has no data"")\n\n\nif __name__==""__main__"":\n    print (""This is a Decision Tree Learner\\n"")\n\n    # Some data to test the DTLearner\n    x0 = np.array([0.885, 0.725, 0.560, 0.735, 0.610, 0.260, 0.500, 0.320])\n    x1 = np.array([0.330, 0.390, 0.500, 0.570, 0.630, 0.630, 0.680, 0.780])\n    x2 = np.array([9.100, 10.900, 9.400, 9.800, 8.400, 11.800, 10.500, 10.000])\n    x = np.array([x0, x1, x2]).T\n    \n    y = np.array([4.000, 5.000, 6.000, 5.000, 3.000, 8.000, 7.000, 6.000])\n\n    # Create a tree learner from given train X and y\n    dtl = DTLearner(verbose=True, leaf_size=1)\n    print (""\\nAdd data"")\n    dtl.addEvidence(x, y)\n\n    print (""\\nCreate another tree learner from an existing tree"")\n    dtl2 = DTLearner(tree=dtl.tree)\n\n    # dtl2 should have the same tree as dtl\n    assert np.any(dtl.tree == dtl2.tree)\n\n    dtl2.get_learner_info()\n\n    # Modify the dtl2.tree and assert that this doesn\'t affect dtl.tree\n    dtl2.tree[0] = np.arange(dtl2.tree.shape[1])\n    assert np.any(dtl.tree != dtl2.tree)\n\n    # Query with dummy data\n    dtl.query(np.array([[1, 2, 3], [0.2, 12, 12]]))\n\n    # Another dataset to test that ""If the best feature doesn\'t split the data into two\n    # groups, choose the second best one and so on; if none of the features does, return leaf""\n    x2 = np.array([\n     [  0.26,    0.63,   11.8  ],\n     [  0.26,    0.63,   11.8  ],\n     [  0.32,    0.78,   10.   ],\n     [  0.32,    0.78,   10.   ],\n     [  0.32,    0.78,   10.   ],\n     [  0.735,   0.57,    9.8  ],\n     [  0.26,    0.63,   11.8  ],\n     [  0.61,    0.63,    8.4  ]])\n        \n    y2 = np.array([ 8.,  8.,  6.,  6.,  6.,  5.,  8.,  3.])\n        \n    dtl = DTLearner(verbose=True)\n    dtl.addEvidence(x2, y2)'"
LinRegLearner.py,2,"b'""""""A simple wrapper for linear regression""""""\n\nimport numpy as np\n\nclass LinRegLearner(object):\n\n    def __init__(self, verbose = False):\n        pass\n\n    def addEvidence(self, dataX, dataY):\n        """"""Add training data to learner\n        Parameters:\n        dataX: X values of data to add\n        dataY: The Y training values\n        """"""\n\n        # Add 1s column so linear regression finds a constant term\n        newdataX = np.ones([dataX.shape[0], dataX.shape[1] + 1])\n        newdataX[:,0:dataX.shape[1]] = dataX\n\n        # build and save the model\n        self.model_coefs, residuals, rank, s = np.linalg.lstsq(newdataX, dataY)\n        \n    def query(self, points):\n        """"""Estimate a set of test points given the model we built\n        Parameters:\n        points: A numpy array with each row corresponding to a specific query\n\n        Returns: the estimated values according to the saved model\n        """"""\n        return (self.model_coefs[:-1] * points).sum(axis=1) + self.model_coefs[-1]\n\nif __name__==""__main__"":\n    print (""This is a Linear Regression Learner"")\n'"
gen_data.py,12,"b'""""""Generate data to fool learners""""""\n\nimport numpy as np\n\n\ndef best4LinReg(seed=np.random.randint(1000000)):\n    """"""This function should return a dataset (X and Y) that will work\n    better for linear regression than decision trees\n\n    Parameters: \n    seed: Random integer used to initialize the pseudo-random number generator. \n    Whenever the seed is the same, the same data set will be returned. \n    Different seeds should result in different data sets.\n\n    Returns: \n    X: A numpy ndarray of X values\n    Y: A numpy 1D array of Y values\n    """"""\n\n    np.random.seed(seed)\n\n    # X and Y should each contain from 10 to 1000 rows\n    num_rows = np.random.randint(10, 1001)\n\n    # X should have from 2 to 1000 columns\n    num_X_cols = np.random.randint(2, 1001)\n\n    X = np.random.normal(size=(num_rows, num_X_cols))\n    Y = np.zeros(num_rows)\n    for col in range(num_X_cols):\n        Y += X[:, col]\n\n    return X, Y\n\n\ndef best4DT(seed=np.random.randint(1000000)):\n    """"""This function should return a dataset (X and Y) that will work\n    better for decision trees than linear regression\n\n    Parameters: \n    seed: Random seed used to initialize the pseudo-random number generator. \n    Whenever the seed is the same, the same data set will be returned. \n    Different seeds should result in different data sets.\n\n    Returns: \n    X: A numpy ndarray of X values\n    Y: A numpy 1D array of Y values\n    """"""\n\n    np.random.seed(seed)\n    \n    # X and Y should each contain from 10 to 1000 rows\n    num_rows = np.random.randint(10, 1001)\n\n    # X should have from 2 to 1000 columns\n    num_X_cols = np.random.randint(2, 1001)\n\n    X = np.random.normal(size=(num_rows, num_X_cols))\n    Y = np.zeros(num_rows)\n    for col in range(num_X_cols):\n        Y += X[:, col] ** 2\n    \n    return X, Y\n\n\nif __name__==""__main__"":\n    print (""Generate data that works better for one learner than the other"")'"
grade_best4.py,8,"b'""""""MC3-H1: Best4{LR,DT} - grading script.\n\nUsage:\n- Switch to a student feedback directory first (will write ""points.txt"" and ""comments.txt"" in pwd).\n- Run this script with both ml4t/ and student solution in PYTHONPATH, e.g.:\n    PYTHONPATH=ml4t:MC3-P1/jdoe7 python ml4t/mc3_p1_grading/grade_learners.py\n\nCopyright 2017, Georgia Tech Research Corporation\nAtlanta, Georgia 30332-0415\nAll Rights Reserved\n\nJan 2018: Trang Nguyen fixed imcompatibility issues for use in Python 3x and removed test for author()\n""""""\n\nimport pytest\nfrom grading import grader, GradeResult, time_limit, run_with_timeout, IncorrectOutput\n# These two lines will be commented out in the final grading script. \nfrom LinRegLearner import LinRegLearner\nfrom DTLearner import DTLearner\n\nimport os\nimport sys\nimport traceback as tb\n\nimport numpy as np\nimport pandas as pd\nfrom collections import namedtuple\n\nimport math\n\nimport time\n\nseconds_per_test_case = 5\n\nmax_points = 100.0 \nhtml_pre_block = True  # surround comments with HTML <pre> tag (for T-Square comments field)\n\n# Test cases\nBest4TestCase = namedtuple(\'Best4TestCase\', [\'description\', \'group\',\'max_tests\',\'needed_wins\',\'row_limits\',\'col_limits\',\'seed\'])\nbest4_test_cases = [\n    Best4TestCase(\n        description=""Test Case 1: Best4LinReg"",\n        group=""best4lr"",\n        max_tests=15,\n        needed_wins=10,\n        row_limits=(10,1000),\n        col_limits=(2,1000),\n        seed=1489683274\n        ),\n    Best4TestCase(\n        description=""Test Case 2: Best4DT"",\n        group=""best4dt"",\n        max_tests=15,\n        needed_wins=10,\n        row_limits=(10,1000),\n        col_limits=(2,1000),\n        seed=1489683274\n        )]\n\n# Test functon(s)\n@pytest.mark.parametrize(""description,group,max_tests,needed_wins,row_limits,col_limits,seed"", best4_test_cases)\ndef test_learners(description, group, max_tests, needed_wins, row_limits, col_limits, seed, grader):\n    """"""Test data generation methods beat given learner.\n\n    Requires test description, test case group, and a grader fixture.\n    """"""\n\n    points_earned = 0.0  # initialize points for this test case\n    incorrect = True\n    msgs = []\n    try:\n        dataX, dataY = None,None\n        same_dataX, same_dataY = None,None\n        diff_dataX, diff_dataY = None,None\n        betterLearner, worseLearner = None, None\n        if group==\'author\':\n            try:\n                from gen_data import author\n                auth_string = run_with_timeout(author,seconds_per_test_case,(),{})\n                if auth_string == \'tb34\':\n                    incorrect = True\n                    msgs.append(""   Incorrect author name (tb34)"")\n                    points_earned = -10\n                elif auth_string == \'\':\n                    incorrect = True\n                    msgs.append(""   Empty author name"")\n                    points_earned = -10\n                else:\n                    incorrect = False\n            except Exception as e:\n                incorrect = True\n                msgs.append(""   Exception occured when calling author() method: {}"".format(e))\n                points_earned = -10\n        else:\n            if group==""best4dt"":\n                from gen_data import best4DT\n                dataX, dataY = run_with_timeout(best4DT,seconds_per_test_case,(),{\'seed\':seed})\n                same_dataX,same_dataY = run_with_timeout(best4DT,seconds_per_test_case,(),{\'seed\':seed})\n                diff_dataX,diff_dataY = run_with_timeout(best4DT,seconds_per_test_case,(),{\'seed\':seed+1})\n                betterLearner = DTLearner\n                worseLearner = LinRegLearner\n            elif group==\'best4lr\':\n                from gen_data import best4LinReg\n                dataX, dataY = run_with_timeout(best4LinReg,seconds_per_test_case,(),{\'seed\':seed})\n                same_dataX, same_dataY = run_with_timeout(best4LinReg,seconds_per_test_case,(),{\'seed\':seed})\n                diff_dataX, diff_dataY = run_with_timeout(best4LinReg,seconds_per_test_case,(),{\'seed\':seed+1})\n                betterLearner = LinRegLearner\n                worseLearner = DTLearner\n\n            num_samples = dataX.shape[0]\n            cutoff = int(num_samples*0.6)\n            worse_better_err = []\n            for run in range(max_tests):\n                permutation = np.random.permutation(num_samples)\n                train_X,train_Y = dataX[permutation[:cutoff]], dataY[permutation[:cutoff]]\n                test_X,test_Y = dataX[permutation[cutoff:]], dataY[permutation[cutoff:]]\n                better = betterLearner()\n                worse = worseLearner()\n                better.addEvidence(train_X,train_Y)\n                worse.addEvidence(train_X,train_Y)\n                better_pred = better.query(test_X)\n                worse_pred = worse.query(test_X)\n                better_err = np.linalg.norm(test_Y-better_pred)\n                worse_err = np.linalg.norm(test_Y-worse_pred)\n                worse_better_err.append( (worse_err,better_err) )\n            worse_better_err.sort(key=cmp_to_key(keyfunction))\n            better_wins_count = 0\n            for worse_err,better_err in worse_better_err:\n                if better_err < 0.9*worse_err:\n                    better_wins_count = better_wins_count+1\n                    points_earned += 5.0\n                if better_wins_count >= needed_wins:\n                    break\n            incorrect = False\n            if (dataX.shape[0] < row_limits[0]) or (dataX.shape[0]>row_limits[1]):\n                incorrect = True\n                msgs.append(""    Invalid number of rows. Should be between {}, found {}"".format(row_limits,dataX.shape[0]))\n                points_earned = max(0,points_earned-20)\n            if (dataX.shape[1] < col_limits[0]) or (dataX.shape[1]>col_limits[1]):\n                incorrect = True\n                msgs.append(""    Invalid number of columns. Should be between {}, found {}"".format(col_limits,dataX.shape[1]))\n                points_earned = max(0,points_earned-20)\n            if better_wins_count < needed_wins:\n                incorrect = True\n                msgs.append(""    Better learner did not exceed worse learner. Expected {}, found {}"".format(needed_wins,better_wins_count))\n            if not(np.array_equal(same_dataY,dataY)) or not(np.array_equal(same_dataX,dataX)):\n                incorrect = True\n                msgs.append(""    Did not produce the same data with the same seed.\\n""+\\\n                            ""      First dataX:\\n{}\\n"".format(dataX)+\\\n                            ""      Second dataX:\\n{}\\n"".format(same_dataX)+\\\n                            ""      First dataY:\\n{}\\n"".format(dataY)+\\\n                            ""      Second dataY:\\n{}\\n"".format(same_dataY))\n                points_earned = max(0,points_earned-20)\n            if np.array_equal(diff_dataY,dataY) and np.array_equal(diff_dataX,dataX):\n                incorrect = True\n                msgs.append(""    Did not produce different data with different seeds.\\n""+\\\n                            ""      First dataX:\\n{}\\n"".format(dataX)+\\\n                            ""      Second dataX:\\n{}\\n"".format(diff_dataX)+\\\n                            ""      First dataY:\\n{}\\n"".format(dataY)+\\\n                            ""      Second dataY:\\n{}\\n"".format(diff_dataY))\n                points_earned = max(0,points_earned-20)            \n        if incorrect:\n            if group==\'author\':\n                raise (IncorrectOutput)\n            else:\n                inputs_str = ""    Residuals: {}"".format(worse_better_err)\n                raise (IncorrectOutput, ""Test failed on one or more output criteria.\\n  Inputs:\\n{}\\n  Failures:\\n{}"".format(inputs_str, ""\\n"".join(msgs)))\n        else:\n            if group != \'author\':\n                avg_ratio = 0.0\n                worse_better_err.sort(key=cmp_to_key(keyfunction_np_sign))\n                for we,be in worse_better_err[:10]:\n                    avg_ratio += (float(we) - float(be))\n                avg_ratio = avg_ratio/10.0\n                if group==""best4dt"":\n                    grader.add_performance(np.array([avg_ratio,0]))\n                else:\n                    grader.add_performance(np.array([0,avg_ratio]))\n    except Exception as e:\n        # Test result: failed\n        msg = ""Description: {} (group: {})\\n"".format(description, group)\n        \n        # Generate a filtered stacktrace, only showing erroneous lines in student file(s)\n        tb_list = tb.extract_tb(sys.exc_info()[2])\n        for i in range(len(tb_list)):\n            row = tb_list[i]\n            tb_list[i] = (os.path.basename(row[0]), row[1], row[2], row[3])  # show only filename instead of long absolute path\n        tb_list = [row for row in tb_list if (row[0] == \'gen_data.py\')]\n        if tb_list:\n            msg += ""Traceback:\\n""\n            msg += \'\'.join(tb.format_list(tb_list))  # contains newlines\n        elif \'grading_traceback\' in dir(e):\n            msg += ""Traceback:\\n""\n            msg += \'\'.join(tb.format_list(e.grading_traceback))\n        msg += ""{}: {}"".format(e.__class__.__name__, str(e))\n\n        # Report failure result to grader, with stacktrace\n        grader.add_result(GradeResult(outcome=\'failed\', points=points_earned, msg=msg))\n        raise\n    else:\n        # Test result: passed (no exceptions)\n        grader.add_result(GradeResult(outcome=\'passed\', points=points_earned, msg=None))\n\nfrom functools import cmp_to_key\n\ndef keyfunction(a, b):\n    return int((b[0]-b[1])-(a[0]-a[1]))\n\n\ndef keyfunction_np_sign(a, b):\n    return int(np.sign((b[0]-b[1])-(a[0]-a[1])))\n\n\nif __name__ == ""__main__"":\n    pytest.main([""-s"", __file__])\n'"
grading.py,0,"b'""""""MLT - Grading components (based on pytest fixtures).\n\nNote: Writes results to ""comments.txt"" in current working directory.\n\nCopyright 2017, Georgia Tech Research Corporation\nAtlanta, Georgia 30332-0415\nAll Rights Reserved\n""""""\n\nimport pytest\nimport signal\nfrom collections import namedtuple\nfrom contextlib import contextmanager\nimport multiprocessing\nimport sys,traceback\n\ntimeout_manager = multiprocessing.Manager()\n\nGradeResult = namedtuple(\'GradeResult\', [\'outcome\', \'points\', \'msg\'])\n\nclass IncorrectOutput(Exception): pass\n\nclass TimeoutException(Exception): pass\n\nclass Grader(object):\n    """"""Main grader class; an instance of this is passed in through a pytest fixture.""""""\n\n    def __init__(self, max_points=None, html_pre_block=False):\n        self.max_points = max_points\n        self.html_pre_block = html_pre_block\n        self.total_points = 0.0\n        self.results = []\n        self.performance = None\n\n    def add_result(self, result):\n        self.results.append(result)\n        self.add_points(result.points)\n\n    def add_points(self, points):\n        self.total_points += points\n\n    def add_performance(self,perf):\n        if self.performance is None:\n            self.performance = perf\n        else:\n            self.performance = self.performance + perf\n\n    def summary(self):\n        num_tests = len(self.results)\n        max_points = self.max_points if self.max_points is not None else float(num_tests)\n        tests_passed = len([result for result in self.results if result.outcome == \'passed\'])\n        # (BPH) return ""Total points: {} out of {}\\nTests passed: {} out of {}"".format(self.total_points, max_points, tests_passed, num_tests)\n        # (BPH) Removing points earned from comments.txt output as part\n        # (BPH) of the autograder -> ""test suite"" move\n        return ""Tests passed: {} out of {}"".format(tests_passed, num_tests)\n\n    def details(self):\n        # (BPH) return ""\\n"".join(""Test #{}: {}, points earned: {}{}"".format(i, self.results[i].outcome, self.results[i].points, ((""\\n"" + self.results[i].msg + ""\\n"") if self.results[i].msg is not None else """")) for i in range(len(self.results)))\n        # (BPH) Removing point\'s earned from comments.txt output as part\n        # (BPH) of the autograder->""test suite"" move\n        return ""\\n"".join(""Test #{}: {} {}"".format(i, self.results[i].outcome, ((""\\n"" + self.results[i].msg + ""\\n"") if self.results[i].msg is not None else """")) for i in range(len(self.results)))\n\n    def write_points(self, filename=""points.txt""):\n        print (""[GRADER] Writing points to \\""{}\\""..."".format(filename))  # [debug]\n        with open(filename, ""w"") as f:\n            f.write(""{}\\n"".format(self.total_points))\n    def write_performance(self,filename=""performance.txt""):\n        if self.performance is None:\n            print (""No performance metric collected, skipping"")\n        else:\n            print (""[GRADER] Writing performance to \\""{}\\""..."".format(filename))\n            with open(filename,""w"") as f:\n                f.write(""{}\\n"".format(self.performance))\n    def write_comments(self, filename=""comments.txt""):\n        # Build comments string\n        print (""[GRADER] Writing comments to \\""{}\\""..."".format(filename))  # [debug]\n        comments = ""--- Summary ---\\n"" + self.summary() + ""\\n""\n        details = self.details()\n        if details:\n            comments += ""\\n--- Details ---\\n"" + details + ""\\n""\n        print (""\\n{}"".format(comments))  # [debug]\n\n        # Write to file\n        with open(filename, ""w"") as f:\n            if self.html_pre_block:\n                f.write(""<pre>"")\n            f.write(comments)\n            if self.html_pre_block:\n                f.write(""</pre>\\n"")\n\n    def __str__(self):\n        return ""<{} at {:x}: total_points: {}, #results: {}>"".format(self.__class__.__name__, id(self), self.total_points, len(self.results))\n\n\n@contextmanager\ndef time_limit(seconds, msg=""Exceeded time limit!""):\n    """"""A contextmanager that raises a TimeoutException if execution takes longer than specified time.\n\n    Usage:\n        with time_limit(1):\n            # do stuff within 1 second\n\n    Note: seconds must be an integer.\n    Based on: http://stackoverflow.com/a/601168\n    """"""\n    def signal_handler(signum, frame):\n        raise (TimeoutException, msg)\n    signal.signal(signal.SIGALRM, signal_handler)\n    signal.alarm(seconds)\n    try:\n        yield\n    finally:\n        signal.alarm(0)\n\n\ndef proc_wrapper(func,rv,pos_args,keyword_args):\n    try:\n        rv[\'output\'] = func(*pos_args,**keyword_args)\n    except Exception as e:\n        rv[\'exception\'] = e\n        rv[\'traceback\'] = traceback.extract_tb(sys.exc_info()[2])\n\ndef run_with_timeout(func,timeout_seconds,pos_args,keyword_args):\n    rv_dict = timeout_manager.dict()\n    p = multiprocessing.Process(target=proc_wrapper,args=(func,rv_dict,pos_args,keyword_args))\n    p.start()\n    p.join(timeout_seconds)\n    if p.is_alive():\n        p.terminate()\n        raise TimeoutException(""Exceeded time limit!"")\n    if not(\'output\' in rv_dict):\n        if \'exception\' in rv_dict:\n            e = rv_dict[\'exception\']\n            e.grading_traceback=None\n            if \'traceback\' in rv_dict:\n                e.grading_traceback = rv_dict[\'traceback\']\n            raise e\n        raise Exception(\'Unknown Exception\')\n    return rv_dict[\'output\']\n\n# Test fixtures\n@pytest.fixture(scope=""module"")\ndef grader(request):\n    """"""A module-level grading fixture.""""""\n    max_points = getattr(request.module, ""max_points"", None)  # picked up from test module, if defined\n    html_pre_block = getattr(request.module, ""html_pre_block"", False)  # surround with HTML <pre> tag?\n    #print ""[GRADER] max_points: {}"".format(max_points)  # [debug]\n    _grader = Grader(max_points=max_points, html_pre_block=html_pre_block)  # singleton\n    def fin():\n        _grader.write_points()\n        _grader.write_comments()\n        _grader.write_performance()\n        print (""[GRADER] Done!"")  # [debug]\n    request.addfinalizer(fin)\n    return _grader\n'"
testbest4.py,2,"b'""""""\nTest best4 data generator.  (c) 2016 Tucker Balch\n""""""\n\nimport numpy as np\nimport math\nimport LinRegLearner as lrl\nimport DTLearner as dt\nfrom gen_data import best4LinReg, best4DT\n\n# compare two learners\' rmse out of sample\ndef compare_os_rmse(learner1, learner2, X, Y):\n\n    # compute how much of the data is training and testing\n    train_rows = int(math.floor(0.6* X.shape[0]))\n    test_rows = X.shape[0] - train_rows\n\n    # separate out training and testing data\n    train = np.random.choice(X.shape[0], size=train_rows, replace=False)\n    test = np.setdiff1d(np.array(range(X.shape[0])), train)\n    trainX = X[train, :] \n    trainY = Y[train]\n    testX = X[test, :]\n    testY = Y[test]\n\n    # train the learners\n    learner1.addEvidence(trainX, trainY) # train it\n    learner2.addEvidence(trainX, trainY) # train it\n\n    # evaluate learner1 out of sample\n    predY = learner1.query(testX) # get the predictions\n    rmse1 = math.sqrt(((testY - predY) ** 2).sum()/testY.shape[0])\n\n    # evaluate learner2 out of sample\n    predY = learner2.query(testX) # get the predictions\n    rmse2 = math.sqrt(((testY - predY) ** 2).sum()/testY.shape[0])\n\n    return rmse1, rmse2\n\ndef test_code():\n\n    # create two learners and get data\n    lrlearner = lrl.LinRegLearner(verbose = False)\n    dtlearner = dt.DTLearner(verbose = False, leaf_size = 1)\n    X, Y = best4LinReg()\n\n    # compare the two learners\n    rmseLR, rmseDT = compare_os_rmse(lrlearner, dtlearner, X, Y)\n\n    # share results\n    print\n    print (""best4LinReg() results"")\n    print (""RMSE LR    : "", rmseLR)\n    print (""RMSE DT    : "", rmseDT)\n    if rmseLR < 0.9 * rmseDT:\n        print (""LR < 0.9 DT:  pass"")\n    else:\n        print (""LR >= 0.9 DT:  fail"")\n    print\n\n    # get data that is best for a random tree\n    lrlearner = lrl.LinRegLearner(verbose = False)\n    dtlearner = dt.DTLearner(verbose = False, leaf_size = 1)\n    X, Y = best4DT()\n\n    # compare the two learners\n    rmseLR, rmseDT = compare_os_rmse(lrlearner, dtlearner, X, Y)\n\n    # share results\n    print\n    print (""best4RT() results"")\n    print (""RMSE LR    : "", rmseLR)\n    print (""RMSE RT    : "", rmseDT)\n    if rmseDT < 0.9 * rmseLR:\n        print (""DT < 0.9 LR:  pass"")\n    else:\n        print (""DT >= 0.9 LR:  fail"")\n    print\n\nif __name__==""__main__"":\n    test_code()\n'"
