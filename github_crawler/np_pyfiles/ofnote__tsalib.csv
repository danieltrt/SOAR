file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n\nimport os\nimport setuptools\n\ndef get_long_description():\n    filename = os.path.join(os.path.dirname(__file__), \'README.md\')\n    with open(filename) as f:\n        return f.read()\n\nsetuptools.setup(name=\'tsalib\',\n      version=\'0.2.2\',\n      description=""TSAlib: Support for Named Tensor Shapes"",\n      long_description=get_long_description(),\n      long_description_content_type=""text/markdown"",\n      author=\'Nishant Sinha\',\n      author_email=\'nishantsinha@acm.org\',\n      url=\'https://github.com/ofnote/tsalib\',\n      license=\'Apache 2.0\',\n      platforms=[\'POSIX\'],\n      packages=setuptools.find_packages(),\n      #entry_points={},\n      classifiers=[\n          \'Environment :: Console\',\n          \'Intended Audience :: Developers\',\n          \'License :: OSI Approved :: Apache Software License\',\n          \'Operating System :: OS Independent\',\n          \'Programming Language :: Python :: 3.6\',\n          \'Programming Language :: Python :: 3.7\',\n          \'Topic :: Software Development\',\n          \'Topic :: Scientific/Engineering :: Artificial Intelligence\'\n          ],\n      setup_requires=[\'sympy\'],\n      install_requires=[\'sympy\'],\n      )'"
models/allennlp_multi_head_similarity.py,0,"b'#Original file : https://github.com/allenai/allennlp/blob/master/allennlp/modules/similarity_functions/multiheaded.py\n\n# The annotations in the `forward` function are sufficient to explain the module\'s functionality\n\nimport sys\nsys.path.append(\'../\')\nfrom tsalib import dim_vars\n\n\nfrom overrides import overrides\nimport torch\nfrom torch.nn.parameter import Parameter\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.modules.similarity_functions.similarity_function import SimilarityFunction\nfrom allennlp.modules.similarity_functions.dot_product import DotProductSimilarity\n\n\n@SimilarityFunction.register(""multiheaded"")\nclass MultiHeadedSimilarity(SimilarityFunction):\n    """"""\n    This similarity function uses multiple ""heads"" to compute similarity.  That is, we take the\n    input tensors and project them into a number of new tensors, and compute similarities on each\n    of the projected tensors individually.  The result here has one more dimension than a typical\n    similarity function.\n    For example, say we have two input tensors, both of shape ``(batch_size, sequence_length,\n    100)``, and that we want 5 similarity heads.  We\'ll project these tensors with a ``100x100``\n    matrix, then split the resultant tensors to have shape ``(batch_size, sequence_length, 5,\n    20)``.  Then we call a wrapped similarity function on the result (by default just a dot\n    product), giving a tensor of shape ``(batch_size, sequence_length, 5)``.\n    Parameters\n    ----------\n    num_heads : ``int``\n        The number of similarity heads to compute.\n    tensor_1_dim : ``int``\n        The dimension of the first tensor described above.  This is ``tensor.size()[-1]`` - the\n        length of the vector `before` the multi-headed projection.  We need this so we can build\n        the weight matrix correctly.\n    tensor_1_projected_dim : ``int``, optional\n        The dimension of the first tensor `after` the multi-headed projection, `before` we split\n        into multiple heads.  This number must be divisible evenly by ``num_heads``.  If not given,\n        we default to ``tensor_1_dim``.\n    tensor_2_dim : ``int``, optional\n        The dimension of the second tensor described above.  This is ``tensor.size()[-1]`` - the\n        length of the vector `before` the multi-headed projection.  We need this so we can build\n        the weight matrix correctly.  If not given, we default to ``tensor_1_dim``.\n    tensor_2_projected_dim : ``int``, optional\n        The dimension of the second tensor `after` the multi-headed projection, `before` we split\n        into multiple heads.  This number must be divisible evenly by ``num_heads``.  If not given,\n        we default to ``tensor_2_dim``.\n    internal_similarity : ``SimilarityFunction``, optional\n        The ``SimilarityFunction`` to call on the projected, multi-headed tensors.  The default is\n        to use a dot product.\n    """"""\n    def __init__(self,\n                 num_heads: int,\n                 tensor_1_dim: int,\n                 tensor_1_projected_dim: int = None,\n                 tensor_2_dim: int = None,\n                 tensor_2_projected_dim: int = None,\n                 internal_similarity: SimilarityFunction = DotProductSimilarity()) -> None:\n        super(MultiHeadedSimilarity, self).__init__()\n        self.num_heads = num_heads\n        self._internal_similarity = internal_similarity\n        tensor_1_projected_dim = tensor_1_projected_dim or tensor_1_dim\n        tensor_2_dim = tensor_2_dim or tensor_1_dim\n        tensor_2_projected_dim = tensor_2_projected_dim or tensor_2_dim\n        if tensor_1_projected_dim % num_heads != 0:\n            raise ConfigurationError(""Projected dimension not divisible by number of heads: %d, %d""\n                                     % (tensor_1_projected_dim, num_heads))\n        if tensor_2_projected_dim % num_heads != 0:\n            raise ConfigurationError(""Projected dimension not divisible by number of heads: %d, %d""\n                                     % (tensor_2_projected_dim, num_heads))\n\n        # tsalib dim vars defined locally (to minimize changes from original implementation)\n        # better: define and store them in the config dictionary and use everywhere\n        self.D1, self.D2, self.D1p, self.D2p = dim_vars(\'D1:{0} D2:{1} D1p:{2} D2p:{3}\'\n                        .format(tensor_1_dim, tensor_2_dim, tensor_1_projected_dim, tensor_2_projected_dim))\n        \n        # original impl\n        self._tensor_1_projection = Parameter(torch.Tensor(tensor_1_dim, tensor_1_projected_dim))\n        self._tensor_2_projection = Parameter(torch.Tensor(tensor_2_dim, tensor_2_projected_dim))\n        \n        # with tsalib:\n        self._tensor_1_projection: (self.D1, self.D1p) = Parameter(torch.Tensor(self.D1, self.D1p))\n        self._tensor_2_projection: (self.D2, self.D2p) = Parameter(torch.Tensor(self.D2, self.D2p))\n\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        torch.nn.init.xavier_uniform_(self._tensor_1_projection)\n        torch.nn.init.xavier_uniform_(self._tensor_2_projection)\n\n    def forward_old(self, tensor_1: \'b,t,d1\', tensor_2: \'b,t,d2\') :\n        # This is the original `forward` implementation\n        # note the shape \'surgery\' below\n        H = self.num_heads\n        B, T = dim_vars(\'Batch(b):{tensor_1.shape(0)} T(t):{tensor_1.shape(1)}\')\n        D1, D2, D1p, D2p = self.D1, self.D2, self.D1p, self.D2p\n\n        projected_tensor_1: (B, T, D1p) = torch.matmul(tensor_1, self._tensor_1_projection)\n        projected_tensor_2: (B, T, D2p) = torch.matmul(tensor_2, self._tensor_2_projection)\n\n        # Here we split the last dimension of the tensors from (..., projected_dim) to\n        # (..., num_heads, projected_dim / num_heads), using tensor.view().\n        last_dim_size = projected_tensor_1.size(-1) // H\n        new_shape = list(projected_tensor_1.size())[:-1] + [H, last_dim_size]\n        split_tensor_1: (B, T, H, D1p // H) = projected_tensor_1.view(*new_shape)\n        \n        last_dim_size = projected_tensor_2.size(-1) // H\n        new_shape = list(projected_tensor_2.size())[:-1] + [H, last_dim_size]\n        split_tensor_2: (B, T, H, D2p // H) = projected_tensor_2.view(*new_shape)\n\n        # And then we pass this off to our internal similarity function.  Because the similarity\n        # functions don\'t care what dimension their input has, and only look at the last dimension,\n        # we don\'t need to do anything special here.  It will just compute similarity on the\n        # projection dimension for each head, returning a tensor of shape (..., num_heads).\n        ret : (B, T, H) = self._internal_similarity(split_tensor_1, split_tensor_2)\n        return ret\n\n    @overrides\n    def forward(self, tensor_1: \'b,t,d1\', tensor_2: \'b,t,d2\') :\n        # Cleaner implementation with tsalib\n\n        #B, T, H defined locally here (to minimize changes to original implementation)\n        # better: define and store them in the config dictionary and use everywhere\n        B, T, H = dim_vars(f\'Batch(b):{tensor_1.shape(0)} T(t):{tensor_1.shape(1)} H(h):{self.num_heads}\')\n        D1, D2, D1p, D2p = self.D1, self.D2, self.D1p, self.D2p\n\n        projected_tensor_1: (B, T, D1p) = torch.matmul(tensor_1, self._tensor_1_projection)\n        projected_tensor_2: (B, T, D2p) = torch.matmul(tensor_2, self._tensor_2_projection)\n\n        split_tensor_1 = projected_tensor_1.view(B, T, H, D1p // H)\n        split_tensor_2  = projected_tensor_2.view(B, T, H, D2p // H)\n\n        # And then we pass this off to our internal similarity function.  Because the similarity\n        # functions don\'t care what dimension their input has, and only look at the last dimension,\n        # we don\'t need to do anything special here.  It will just compute similarity on the\n        # projection dimension for each head, returning a tensor of shape (..., num_heads).\n        ret : (B, T, H) = self._internal_similarity(split_tensor_1, split_tensor_2)\n        return ret'"
models/openai_transformer.py,0,"b'# original file: https://github.com/allenai/allennlp/blob/master/allennlp/modules/openai_transformer.py\n# only selected modules copied and annotated in this file\n# some parts re-written to clarify tensor shapes\n\n# Note how TSAs help: a glance through the forward function exposes the functionality of the module\n\nfrom typing import NamedTuple, List\nimport copy\nimport io\nimport json\nimport logging\nimport math\nimport pathlib\nimport re\nimport tarfile\n\nimport numpy as np\nimport torch\nfrom torch.nn import Parameter\n\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.common.file_utils import cached_path\nfrom allennlp.common.from_params import FromParams\n\n\nimport sys\nsys.path.append(\'../\')\nfrom tsalib import dim_vars, warp\n\nB, T, D, H = dim_vars(\'Batch SeqLength EmbedDim(d):768 NumHeads(h):12\')\n\ndef gelu(x: torch.Tensor) -> torch.Tensor:\n    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n\ndef swish(x: torch.Tensor) -> torch.Tensor:\n    return x * torch.sigmoid(x)\n\n_ACTIVATION_FUNCTIONS = {\n        \'relu\': torch.nn.ReLU,\n        \'swish\': swish,\n        \'gelu\': gelu\n}\n\n\nclass TransformerConfig(NamedTuple):\n    """"""\n    The transformer has to pass a bunch of params to its submodules,\n    this bundles them together to make things easier.\n    """"""\n    embedding_dim: int = 768\n    num_heads: int = 12\n    embedding_dropout_probability: float = 0.1\n    attention_dropout_probability: float = 0.1\n    residual_dropout_probability: float = 0.1\n    activation_function: str = \'gelu\'\n\n\nclass LayerNorm(torch.nn.Module):\n    ""Construct a layernorm module in the OpenAI style (epsilon inside the square root).""\n\n    def __init__(self, n_state, e=1e-5):\n        super().__init__()\n        self.g = torch.nn.Parameter(torch.ones(n_state))\n        self.b = torch.nn.Parameter(torch.zeros(n_state))\n        self.e = e\n\n    def forward(self, x):\n        u = x.mean(-1, keepdim=True)\n        s = (x - u).pow(2).mean(-1, keepdim=True)\n        x = (x - u) / torch.sqrt(s + self.e)\n        return self.g * x + self.b\n\nclass Conv1D(torch.nn.Module):\n    def __init__(self, nf: int, rf: int, nx: int) -> None:\n        super().__init__()\n        self.rf = rf\n        self.nf = nf\n        self.nx = nx\n        if rf == 1:\n            w = torch.empty(nx, nf)\n            torch.nn.init.normal_(w, std=0.02)\n            self.w: (nx, nf) = Parameter(w)\n            self.b: (nf,) = Parameter(torch.zeros(nf))\n        else:\n            raise NotImplementedError\n\n    def forward(self, x) -> torch.Tensor:\n        x: (B, T, self.nx)\n\n        if self.rf == 1:\n            size_out: (B, T, self.nf) = x.size()[:-1] + (self.nf,)\n            x1: (B*T, nx) = x.view(-1, x.size(-1))\n            x: (B*T, self.nf) = torch.addmm(self.b, x1, self.w)\n            x: (B, T, self.nf) = x.view(*size_out)\n        else:\n            raise NotImplementedError\n        return x\n\nclass Attention(torch.nn.Module):\n    def __init__(self,\n                 nx: int,\n                 n_ctx: int,\n                 config: TransformerConfig,\n                 scale: bool = False) -> None:\n        super().__init__()\n        self.nx = nx\n        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n        assert n_state % config.num_heads == 0\n        self.register_buffer(\'b\', torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n        self.n_head = config.num_heads\n        self.split_size = n_state\n        self.scale = scale\n        self.c_attn = Conv1D(n_state * 3, 1, nx)\n        self.c_proj = Conv1D(n_state, 1, nx)\n        self.attn_dropout = torch.nn.Dropout(config.attention_dropout_probability)\n        self.resid_dropout = torch.nn.Dropout(config.residual_dropout_probability)\n\n    def _attn(self, q: (B,H,T,D), k: (B,H,D,T), v: (B,H,T,D)) -> torch.Tensor:\n        w: (B,H,T,T) = torch.matmul(q, k) #similarities\n        if self.scale:\n            w = w / math.sqrt(v.size(-1)) #scaled similarities\n\n        #adding positional encodings?\n        w: (B,H,T,T) = w * self.b + -1e9 * (1 - self.b)  # TF implem method: mask_attn_weights\n        w: (B,H,T,T) = torch.nn.Softmax(dim=-1)(w)\n        w: (B,H,T,T) = self.attn_dropout(w)\n        res: (B,H,T,D) = torch.matmul(w, v)\n        return res\n\n    def merge_heads_old(self, x: (B,H,T,D)):\n        # pylint: disable=no-self-use\n        x: (B,T,H,D) = x.permute(0, 2, 1, 3).contiguous()\n        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n        res: (B,T,H*D) = x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n        return res\n\n    def merge_heads(self, x: (B,H,T,D)):\n        # pylint: disable=no-self-use\n        res = warp(x, \'bhtd -> bthd -> b,t,h*d\', \'pcv\') #permute, then contiguous, then view transforms\n        return res\n\n    def split_heads(self, x: (B, T, D), k: bool = False):\n        H = self.n_head\n        new_x_shape = x.size()[:-1] + (H, x.size(-1) // H)\n        x: (B, T, H, D//H) = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n        if k:\n            ret: (B, H, D // H, T) = x.permute(0, 2, 3, 1)\n        else:\n            ret: (B, H, T, D // H) = x.permute(0, 2, 1, 3)\n\n        return ret\n\n    def forward(self, x) -> torch.Tensor:\n        x: (B, T, self.nx)\n\n        D = self.split_size\n        H = self.n_head\n        \n        x: (B, T, 3*D) = self.c_attn(x)\n        query: (B, T, D); key: (B, T, D); value: (B, T, D)\n        query, key, value = x.split(D, dim=2)\n\n        query: (B, H, T, D // H) = self.split_heads(query)\n        key: (B, H, D//H, T) = self.split_heads(key, k=True)\n        value: (B, H, T, D // H) = self.split_heads(value)\n        a: (B,H,T,D//H) = self._attn(query, key, value)\n        a: (B,T,D) = self.merge_heads(a)\n        a: (B,T,D) = self.c_proj(a)\n        a: (B,T,D) = self.resid_dropout(a)\n        return a\n\n\nclass MLP(torch.nn.Module):\n    def __init__(self, n_state: int, config: TransformerConfig) -> None:  # in MLP: n_state=3072 (4 * n_embd)\n        super().__init__()\n        self.c_fc = Conv1D(n_state, 1, config.embedding_dim)\n        self.c_proj = Conv1D(config.embedding_dim, 1, n_state)\n        self.act = _ACTIVATION_FUNCTIONS[config.activation_function]\n        self.dropout = torch.nn.Dropout(config.residual_dropout_probability)\n\n    def forward(self, x: (B,T,D)) -> torch.Tensor:\n        h: (B,T,4*D) = self.act(self.c_fc(x))\n        h2: (B, T, D) = self.c_proj(h)\n        return self.dropout(h2)\n\n\nclass Block(torch.nn.Module):\n    def __init__(self,\n                 n_ctx: int,\n                 config: TransformerConfig,\n                 scale: bool = False) -> None:\n        super().__init__()\n        nx = config.embedding_dim\n        self.attn = Attention(nx, n_ctx, config, scale)\n        self.ln_1 = LayerNorm(nx)\n        self.mlp = MLP(4 * nx, config)\n        self.ln_2 = LayerNorm(nx)\n\n    def forward(self, x: (B,T,D)) -> torch.Tensor:\n        a: (B,T,D) = self.attn(x)\n        n: (B,T,D) = self.ln_1(x + a)\n        m: (B,T,D) = self.mlp(n)\n        h: (B,T,D) = self.ln_2(n + m)\n        return h\n\nclass OpenaiTransformer(torch.nn.Module, FromParams):\n    """"""\n    Openai transformer, as per https://blog.openai.com/language-unsupervised/.\n    Default parameters are the ones for their pretrained model.\n    Parameters\n    ----------\n    vocab_size: ``int`` (optional, default: 40478)\n        The size of the vocabulary (number of byte pair embeddings)\n        excluding the n_special embeddings (if any), and the positional embeddings.\n    n_ctx: ``int`` (optional, default: 512)\n        The number of positional encodings to use for evaluation.\n    embedding_dim: ``int`` (optional, default: 768)\n        The dimension of the output embeddings.\n    num_heads: ``int`` (optional, default: 12)\n        How many ""heads"" the attention has.\n    num_layers: ``int`` (optional, default: 12)\n        How many layers of ""blocks"" the transformer has.\n    embedding_dropout_probability: ``float`` (optional, default: 0.1)\n        Dropout for the embedding.\n    attention_dropout_probability: ``float`` (optional, default: 0.1)\n        Dropout for attention.\n    residual_dropout_probability: ``float`` (optional, default: 0.1)\n        Dropout for residual\n    activation_function: ``str`` (optional, default: ``\'gelu\'``)\n        Activation function for the multi-layer perceptron.\n    model_path: ``str`` (optional, default: ``None``)\n        A tar.gz file containing serialized model weights. If supplied,\n        the weights will be loaded from that file.\n    requires_grad: ``bool`` (optional, default: ``False``)\n        If true, the transformer will be fine-tuneable.\n    n_special: ``int`` (optional, default: ``-1``)\n        The number of special tokens added to the byte pair vocabulary\n        (via ``OpenaiTransformerBytePairIndexer``).\n    """"""\n    def __init__(self,\n                 vocab_size: int = 40478,\n                 n_ctx: int = 512,\n                 embedding_dim: int = 768,\n                 num_heads: int = 12,\n                 num_layers: int = 12,\n                 embedding_dropout_probability: float = 0.1,\n                 attention_dropout_probability: float = 0.1,\n                 residual_dropout_probability: float = 0.1,\n                 activation_function: str = \'gelu\',\n                 model_path: str = None,\n                 requires_grad: bool = False,\n                 n_special: int = -1) -> None:\n        super().__init__()\n\n        config = TransformerConfig(\n                embedding_dim,\n                num_heads,\n                embedding_dropout_probability,\n                attention_dropout_probability,\n                residual_dropout_probability,\n                activation_function,\n        )\n\n        # the embedding size is vocab_size + n_special embeddings + n_ctx\n        embedding_size = vocab_size + max(n_special, 0) + n_ctx\n        self.vocab_size = embedding_size\n        self.n_ctx = n_ctx\n        self.n_special = n_special\n\n        self.num_output_layers = 1 + num_layers\n\n        self.embed = torch.nn.Embedding(embedding_size, embedding_dim)\n        self.drop = torch.nn.Dropout(embedding_dropout_probability)\n\n        block = Block(n_ctx, config, scale=True)\n        self.h = torch.nn.ModuleList([copy.deepcopy(block) for _ in range(num_layers)])\n        self.decoder = torch.nn.Linear(embedding_dim, embedding_size, bias=False)\n        self.decoder.weight = self.embed.weight  # Tied weights\n        # To reproduce the noise_shape parameter of TF implementation\n\n        torch.nn.init.normal_(self.embed.weight, std=0.02)\n\n        for parameter in self.parameters():\n            parameter.requires_grad = requires_grad\n\n        if model_path:\n            self.load_weights(model_path, n_special=n_special, n_ctx=n_ctx)\n\n    def forward(self, x: (B,T)) -> \'(btd)*\':\n        # x is (batch_size, sequence_length) tensor of byte-pair ids\n\n        # e is (batch_size, sequence_length, 2, embedding_dim) tensor of embeddings\n        e: (B, T, 2, D) = self.embed(x)\n\n        h: (B, T, D) = e.sum(dim=2)\n\n        all_layers: \'(btd)*\' = [h]\n        for block in self.h:\n            h = block(h)\n            all_layers.append(h)\n\n        return all_layers'"
models/resnet.py,0,"b'import torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n# Original file: https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n# Updated to add shape annotations (BasicBlock and ResNet modules)\n\nimport sys\nfrom tsalib import dim_vars\n\n\nB, C, Ci, Co = dim_vars(\'Batch(b):10 Channels(c):3 ChannelsIn(ci) ChannelsOut(co)\')\nH, W, Ex = dim_vars(\'Height(h):224 Width(w):224 BlockExpansion(e):1\')\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes: Ci, planes: Co, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x: (B, Ci, H, W)):\n        residual: (B, Ci, H, W) = x\n        O = (B, Co, H, W)\n\n        out: O = self.conv1(x)\n        out: O = self.bn1(out)\n        out: O = self.relu(out)\n        out: O = self.conv2(out)\n        out: O  = self.bn2(out)\n\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        #assert residual.size() == out.size()\n        out: O = out + residual\n        out: O = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        #print (f\'block expansion {block.expansion}\')\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x: (B, 3, H, W)): #H = W = 224\n        x: (B, 64, H//2, W//2) = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x: (B, 64, H//4, W//4) = self.maxpool(x)\n\n        x: (B, 64*Ex, H//4, W//4) = self.layer1(x)\n        x: (B, 128*Ex, H//8, W//8) = self.layer2(x)\n        x: (B, 256*Ex, H//16, W//16) = self.layer3(x)\n        x: (B, 512*Ex, H//32, W//32) = self.layer4(x)\n\n        x: (B, 512*Ex, 1, 1) = self.avgpool(x)\n        x: (B, 512*Ex) = x.view(x.size(0), -1)\n        x: (B, NC) = self.fc(x)\n\n        return x\n\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model\n\n\ndef test_resnet ():\n    bb = BasicBlock(64, 64)\n    x = torch.ones(10, 64, 256, 256)\n    out = bb.forward(x)\n    print (out.size())\n    \n    \n    rs18 = resnet18()\n    x = torch.ones(10, 3, 224, 224)\n    out = rs18.forward(x)\n    print (out.size())\n\nif __name__ == \'__main__\':\n    test_resnet()'"
models/snippets_pytorch.py,0,"b'import torch\nimport torch.nn.functional as F\n\n\nimport sys\nsys.path.append(\'../\')\nfrom tsalib import dim_vars as dvs, get_dim_vars\nfrom tsalib import permute_transform as pt, warp, dot, alignto\nfrom tsalib import reduce_dims as rd\n\nB, H, T, D = dvs(\'Batch(b):4 H(h):7 T(t):100 D(d):300\')\n\n\n# `merge_heads` function in Transformer network (original)\n\ndef merge_heads_old(x: (B,H,T,D)):\n  x = x.permute(0, 2, 1, 3).contiguous()\n  new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n  res = x.view(*new_x_shape)\n  return res\n\n\n# `merge_heads` using tsalib (weaker integration)\n\n\ndef merge_heads1(x: (B,H,T,D)):\n  x: (B,T,H,D) = x.permute(pt(\'bhtd -> bthd\')).contiguous()\n  res: (B,T,H*D) = x.view((B,T,H*D))\n  return res\n\n\n# \'merge_heads\' using tsalib\'s warp (deeper integration)\n\nfrom tsalib import warp\n\ndef merge_heads2(x: (B,H,T,D)):\n    res: (B,T,H*D) = warp(x, \'bhtd -> bthd -> b,t,h*d\', \'pcv\', debug=False)\n    return res\n\n\ndef test_merge_heads():\n    x = torch.randn( (B,H,T,D) )\n    y = merge_heads_old(x)\n    assert y.size() == (B,T,H*D)\n    y = merge_heads1(x)\n    assert y.size() == (B,T,H*D)\n    \n    y = merge_heads2(x)\n    assert y.size() == (B,T,H*D)\n    print (\'all merge_heads assertions hold\')\n\n\n\'\'\'\nEinsum attention \n\nOriginally from https://rockt.github.io/2018/04/30/einsum\n\nRevisited at http://nlp.seas.harvard.edu/NamedTensor\n\n\'\'\'\n\n\ndef random_tensors(shape, num=1):\n  tensors = [torch.randn(shape) for i in range(0, num)]\n  return tensors[0] if num == 1 else tensors\n\ndef make_params (l):\n    bM, br, w = random_tensors([l], num=3)\n    # -- [hidden_dimension x hidden_dimension]\n    WY, Wh, Wr, Wt = random_tensors([l, l], num=4)\n\n    return (bM, br, w), (WY, Wh, Wr, Wt)\n\ndef einsum_attn(Y, ht, rt1):\n    (bM, br, w), (WY, Wh, Wr, Wt) = make_params(7)\n\n    # -- [batch_size x hidden_dimension]\n    tmp = torch.einsum(""ik,kl->il"", [ht, Wh]) + \\\n          torch.einsum(""ik,kl->il"", [rt1, Wr])\n\n    Mt = torch.tanh(torch.einsum(""ijk,kl->ijl"", [Y, WY]) + \\\n                tmp.unsqueeze(1).expand_as(Y) + bM)\n    # -- [batch_size x sequence_length]\n    at = F.softmax(torch.einsum(""ijk,k->ij"", [Mt, w]), dim=-1)\n\n    # -- [batch_size x hidden_dimension]\n    rt = torch.einsum(""ijk,ij->ik"", [Y, at]) + \\\n         torch.tanh(torch.einsum(""ij,jk->ik"", [rt1, Wt]) + \n                    br)\n\n    # -- [batch_size x hidden_dimension], [batch_size x sequence_dimension]\n    return rt, at\n\ndef test_einsum_attn():\n    # -- [batch_size x sequence_length x hidden_dimension]\n    Y = random_tensors([3, 5, 7])\n    # -- [batch_size x hidden_dimension]\n    ht, rt1 = random_tensors([3, 7], num=2)\n\n    rt, at = einsum_attn(Y, ht, rt1)\n    assert rt.size() == (3, 7) and at.size() == (3, 5)\n\n    print (\'einsum attn: assertions hold\')\n\n\n\'\'\'\n========= With tsalib =====\n\'\'\'\n\ndef tsa_attn(Y, ht, rt1):\n    B, L, D = get_dim_vars(\'b l d\')\n    Y: \'bld\' ; ht: \'b,d\'; rt1: \'b,d\'\n\n    #bM, br, w: \'d,\'\n    #WY, Wh, Wr, Wt: \'d,d\' \n    (bM, br, w), (WY, Wh, Wr, Wt) = make_params(D)\n\n    tmp: \'bd\' = dot(\'_d.d_\', ht, Wh) + dot(\'_d.d_\', rt1, Wr)\n    tmpa: \'bld\' = alignto((tmp,\'bd\'), \'bld\')\n\n    Mt: \'bld\' = torch.tanh(dot(\'__d.d_\', Y, WY) + tmpa + bM)\n    at: \'bl\' = F.softmax(dot(\'__d.d\', Mt, w), dim=-1)\n    rt: \'bd\' = dot(\'bld,bl->bd\', Y, at) + torch.tanh(dot(\'_d.d_\', rt1, Wt) + br)\n\n    return rt, at\n\ndef test_tsa_attn():\n    B, L, D = dvs(\'Batch(b):3, sequence_length(l):5 hidden_dimension(d):300\', exists_ok=True)\n\n    Y = random_tensors([B, L, D])\n    ht, rt1 = random_tensors([B, D], num=2)\n    rt, at = tsa_attn(Y, ht, rt1)\n\n    assert rt.size() == (B, D) and at.size() == (B, L)\n    print (\'tsa attn: assertions hold\')\n\nif __name__ == \'__main__\':\n    test_merge_heads()\n    #test_einsum_attn()\n    test_tsa_attn()\n'"
models/snippets_tf.py,0,"b'\nfrom tsalib import dim_vars\nfrom tsalib.backend import get_shape_list\n\ndef modeling_embedding_lookup(input_ids: \'bti\'):\n    # illustrates local dim var usage, i is not declared globaly as dimvar\n    B, T, D = dim_vars(\'B(b):13 L(t):7 D(d):32\')\n    embedding_size = D\n    i = get_shape_list(input_ids)[-1] #num inputs\n\n    output: \'b*t*i,d\'\n\n    # OLD\n    \n    input_shape: \'bti\' = get_shape_list(input_ids)\n    output: \'btd\' = tf.reshape(output,\n                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n\n    # NEW: crisp one-liner\n\n    output: \'btd\' = warp(output, tfms=f\'b*t*{i},d -> b,t,d*{i}\', tfm_names=\'r\')\n  \n    assert output.get_shape() == (B, T, D)\n\n\n\n####################\n\n\ndef create_attention_mask_from_input_mask_old(from_tensor, to_mask):\n  """"""Create 3D attention mask from a 2D tensor mask.\n\n  Args:\n    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n  """"""\n  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n  batch_size = from_shape[0]\n  from_seq_length = from_shape[1]\n\n  to_shape = get_shape_list(to_mask, expected_rank=2)\n  to_seq_length = to_shape[1]\n\n  to_mask = tf.cast(\n      tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n  # We don\'t assume that `from_tensor` is a mask (although it could be). We\n  # don\'t actually care if we attend *from* padding tokens (only *to* padding)\n  # tokens so we create a tensor of all ones.\n  #\n  # `broadcast_ones` = [batch_size, from_seq_length, 1]\n  broadcast_ones = tf.ones(\n      shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n  # Here we broadcast along two dimensions to create the mask.\n  mask = broadcast_ones * to_mask\n\n  return mask\n\n\ndef create_attention_mask_from_input_mask(from_tensor: \'b,f,...\', to_mask: \'b,t\'):\n\n    B, F = get_shape_list(from_tensor, expected_rank=[2, 3])[:2]\n    To = get_shape_list(to_mask, expected_rank=2)[1]\n    to_mask = alignto((to_mask, \'bt\'), \'b_t\')\n    to_mask = tf.cast(to_mask, tf.float32)\n    broadcast_ones = tf.ones(shape=[B, F, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n    size_assert (get_shape_list(mask), (B, F, To))\n    print (f\'create attn: {get_shape_list(mask)}\')\n\n    return mask\n\n\n\n####################\n\ndef transpose_for_scores_old(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width):\n    print (f\'tran for sc: {get_shape_list(input_tensor)}\')\n\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\ndef transpose_for_scores(input_tensor: \'b*t,d\', batch_size: \'b\', num_attention_heads: \'n\', \n                            seq_length: \'t\', width: \'d\'):\n    return warp(input_tensor, [f\'b*t,d -> btnh\', \'_tn_ -> _nt_\'], \'vp\')\n\n####################\n\nif __name__ == \'__main__\':\n\n'"
tests/test.py,29,"b""\n# coding: utf-8\n\n# In[2]:\n\n\nimport sys\nimport numpy as np\nfrom tsalib import dim_vars, get_dim_vars, update_dim_vars_len\n\n\n# # Design Principles\n# **Dimension Variables** (DVs) are the core abstractions behind tsalib. \n# - They allow specifying and modifying shapes of tensors *symbolically*, i.e., using named symbols corresponding to different dimensions of tensor. \n# - Making dimension names explicit enables cleaner, DRY code, symbolic shape assertions, and faster debugging.\n# - **Symbolic** shapes or **annotations** are *tuples* over DVs and arithmetic expressions over DVs.\n# \n# The `tsalib` provides a collection of powerful APIs to handle all kinds of shape transformations using explicit dimension variables and shape annotations.  \n# \n# \n# - Designed to stay light, easy to incorporate into existing workflow with minimal code changes.\n# - The API includes both library-independent and dependent parts, giving developers flexibility in how they choose to incorporate `tsalib` in their workflow.\n# - Avoid deeper integration into popular tensor libraries to keep `tsalib` light-weight and avoid backend-inflicted bugs.\n# \n# Some popular models (resnet, transformer) annotated/re-written with tsalib can be found in the [models](models/) directory.\n# \n\n# ## Declare dimension variables\n# Dimension variables model both the `name` and the default `size` of a tensor.   \n# Format: **name(symbol):size**   --  `symbol` and `size` are optional\n# \n# We can declare dimension variables **globally** (Dimensions used in programs are known upfront and programs don't modify dimension names).  \n# Even better, we can put all these definitions in the Config dictionary.\n\n# In[3]:\n\n\n# globals variables prefixed with underscores\n_B, _T, _D, _K = dim_vars('Batch(b):20 SeqLength(t):10 EmbeddingDim(d):100 K(k):1')\n_C, _H, _W = dim_vars('Channels(c):3 Height(h):256 Width(w):256')\n\n\n# In[4]:\n\n\ndef test_decls():\n    print('\\nTest declarations ..')\n    #local declarations\n    print(f'B, C, D = {_B}, {_C}, {_D}')\n\n    #strict=False allows overwriting previous declarations\n    H, W = dim_vars ('Height(h):256 Width(w):256', exists_ok=True) \n    print(f'H, W = {H}, {W}')\n\n    # test update dim var len\n\n    H.update_len(1024)\n    print(f'H = {H}')\n\n    update_dim_vars_len({'h': 512, 'w': 128})\n    H, W = get_dim_vars('h w')\n    print(f'H, W = {H}, {W}')\n\n\n\n\n# Supports arithmetic over a combination of dim vars and other Python variables\ndef test_arith():\n    print('\\nTest arithmetic ..')\n    _K, _W, _B, _H = get_dim_vars('k w b h') \n    _K = _W * 2\n    h = 4\n    print((h, _H // h, _K, _B*2))\n\n# Use dimension variables in lieu of constant size values\n# note: dim_var declaration must include size of the variable\ndef test_cast_int():\n    print('\\nTest integer cast ..')\n    B, C = get_dim_vars('b c')\n    x = np.zeros((B, C))\n    print(f'shape of array: ({B},{C}): {x.shape}')\n    return x\n    \ndef basic_tests():\n    test_decls()\n    test_arith()\n    x = test_cast_int()\n    # Test assertions over symbolic shapes\n    assert x.shape == (_B,_C)\n    print ('assertions hold')\n\n\n# In[5]:\n\n\nbasic_tests()\n\n\n# ## Basic tsalib usage\n# Can be used to manage tensor shapes with **arbitrary** tensor libraries. Here, examples with *numpy* and *pytorch*.\n# - Create new tensors (independent of actual dimension sizes)\n# - **Annotate** tensor variables (widely considered best practice, otherwise done using adhoc comments)\n# - Check symbolic **assertions** (assertions **do not** change even if dimension size changes)\n\n# In[6]:\n\n\ndef test_numpy():\n    print('\\nTest usage with numpy ..')\n    B, D = get_dim_vars('b d')\n    import numpy as np\n    a: (B, D) = np.zeros((B,D))\n    print(f'original array: {(B,D)}: {a.shape}')\n\n    b: (2, B, D) = np.stack([a, a])\n    print(f'after stack: {(2,B,D)}: {b.shape}')\n\n    ax = (2,B,D).index(B)\n    c: (2, D) = np.mean(b, axis=ax)\n    print(f'after mean along axis = {ax}: {(2,D)}: {c.shape}')\n\ntest_numpy()\n\n\n# In[7]:\n\n\ndef test_pytorch():\n    print('\\nTest usage with pytorch ..')\n    B, D = get_dim_vars('b d')\n    B, D = dim_vars('Batch:2 EmbedDim:3', exists_ok=True)\n    import torch\n\n    a = torch.Tensor([[1., 2., 4.], [3., 6., 9.]])\n    assert a.size() == (B, D)\n\n    b = torch.stack([a, a])\n\n    print ('Asserting b.size() == (2,B,D)')\n    assert b.size() == (2, B, D)\n\n    c = torch.cat([a, a], dim=1)\n    print ('Assertion on c.size()')\n    assert c.size() == (B, D*2)\n\ntest_pytorch()\n\n\n# ## Shape Transformations with Dimensions Variables\n# To shape transform without `tsalib`, you either \n# -  **hard-code** integer constants for each dimension's position in shape transformations, or\n# - do shape tuple **surgeries** to compute the 'right' shape (for the general case)\n# \n# Instead, with `tsalib`, use dimension variables or the shorthand symbols directly. \n# \n# `tsalib` provides API for common shape transformations: **view** (reshape), **permute** (transpose) and **expand** (tile).  \n# These are *library-independent*, e.g., shorthand transformation -> target shape tuple -> reshape.\n# \n# One transformation to rule them all : **warp**. Do a sequence of transformations on a tensor.  \n# `warp` is implementated for several popular backend libraries.\n# \n# ## Work with Shorthand Shape Notation \n# Writing tuples of shape annotations can get cumbersome.\n# \n# So, instead of (B, T, D), write 'btd' (each dim gets a single char, concatenated together)\n# \n# Instead of (B \\* T, D // 2, T), write 'b * t, d//2, t' (arbitrary arithmetic expressions, comma-separated)\n# \n# Anonymous dimension variables : 'b,,d' omits naming dimension t.\n\n# ## Reshapes (view transformations) using dimension variables\n# These are library independent: `vt` returns target tensor shapes from shorthand transformation spec.\n\n# In[8]:\n\n\n# without tsalib, this is how we used to do it. See code from BERT.\ndef test_reshape_old ():\n    x = np.ones((20, 10, 100))\n    h = 4\n    new_shape = x.shape[:2] + (h, x.shape[2]//h) #shape surgery\n    x = x.reshape(new_shape)\n    print (x.shape)\n\nfrom tsalib import view_transform as vt    \n    \n# with tsalib, simply use dimension vars in-place\ndef test_reshape():\n    B, T, D = get_dim_vars('b t d')\n    x: (B,T,D) = np.ones((B, T, D))\n    h = 4\n    x: (B,T,h,D//h) = x.reshape((B, T, h, D//h))\n    assert x.shape == (B,T,h,D//h)\n    print ('test_reshape: all assertions hold')\n\n#using shorthand notation, omit dimensions not involved in transformation\ndef test_reshape_short():\n    B, T, D = get_dim_vars('b t d')\n    x: (B,T,D) = np.ones((B, T, D))\n    h = 4\n    x = x.reshape(vt(f'btd -> b,t,{h},d//{h}', x.shape))\n    assert x.shape == (B, T, h, D//h)\n\n    x1 = x.reshape(vt('b,t,4,k -> b*t,4,k', x.shape))\n    assert x1.shape == (B*T, h, D//h)\n    \n    x1 = x.reshape(vt('b,t,, -> b*t,,', x.shape))\n    assert x1.shape == (B*T, h, D//h)\n\n\n    print ('test_reshape_short: all assertions hold')\n\n\n#test_reshape_old()\ntest_reshape()\ntest_reshape_short()\n\n\n# ## Transpose/Permute transformations using dimension variables\n\n# In[9]:\n\n\nfrom tsalib import  permute_transform as pt\nfrom tsalib.transforms import _permute_transform as _pt\n\n# permute using dimension variables (internal, recommended to be not used)\ndef test_permute():\n    B, T, D, K = get_dim_vars('b t d k')\n    x: (B,T,D,K) = np.ones((B, T, D, K))\n    perm_indices = _pt(src=(B,T,D,K), to=(D,T,B,K))\n    assert perm_indices == (2,1,0,3)\n    x = x.transpose(perm_indices)\n    assert x.shape == (D,T,B,K)\n    print ('test_permute: all assertions hold')\n\n# shorthand permutes are snazzier (use '_' or ',' as placeholders)\ndef test_permute_short():\n    B, T, D, K, C, H, W = get_dim_vars('b t d k c h w')\n    x: (B,T,D,K) = np.ones((B, T, D, K))  \n    x = x.transpose(pt('btdk -> dtbk')) # (B, T, D, K) -> (D, T, B, K)\n    assert x.shape == (D,T,B,K)\n\n    x = x.transpose(pt('d_b_ -> b_d_')) # (D,T,B,K) -> (B, T, D, K)\n    assert x.shape == (B,T,D,K)\n\n    x: (B, C, H, W) = np.ones((B, C, H, W))\n    x1 = x.transpose(pt(',c,, -> ,,,c'))\n    assert x1.shape == (B, H, W, C)\n    print ('test_permute_short: all assertions hold')\ntest_permute()\ntest_permute_short()\n\n\n# ## Expand transformations\n\n# In[10]:\n\n\nfrom tsalib import _expand_transform as et\ndef test_expand():\n    B, T, D, K = get_dim_vars('b t d k')\n    \n    x: (B, T, D) = np.ones((B, T, D))\n    x: (B, K, T, D) = x[:, None]\n\n    expand_shape = et(src=(B,K,T,D), expansions=[(K, K*5)], in_shape=x.shape) #(B, K, T, D) -> (B, K*5, T, D)\n    assert expand_shape == (-1,5,-1,-1)\n    print ('test_expand: all assertions hold')\n\ndef test_expand_short():\n    B, T, D, K = get_dim_vars('b t d k')\n    \n    x: 'btd' = np.ones((B, T, D))\n    x: 'bktd' = x[:, None]\n    expand_shape = et(src=(B,K,T,D), expansions='k->k*5', in_shape=x.shape)\n    assert expand_shape == (-1,5,-1,-1)\n    print ('test_expand_short: all assertions hold')\ntest_expand()\ntest_expand_short()\n\n\n# ## *warp* : generalized shape transformations\n# \n# Writing a sequence of shape transformations in code can get cumbersome.  \n# `warp` enables specifying a sequence of transformations together **inline**.\n\n# In[11]:\n\n\nfrom tsalib import warp\ndef test_warp():\n    B, T, D = get_dim_vars('b t d')\n    \n    x: 'btd' = np.ones((B, T, D))\n    \n    # two view transformations (reshapes) in sequence\n    x1 = warp(x, 'btd -> b,t,4,d//4 -> b*t,4,d//4', 'vv', debug=False)\n    assert(x1.shape == (B*T,4,D//4))\n\n    # four reshapes in sequence\n    x2 = warp(x, 'btd -> b,t,4,d//4 -> b*t,4,d//4 -> b,t,4,d//4 -> btd', 'vvvv', debug=False)\n    assert(x2.shape == (B,T,D))\n    \n    # Same reshape sequence in shorthand, specified as list of transformations\n    x2 = warp(x, ['__d -> ,,4,d//4', 'b,t,, -> b*t,,', 'b*t,, -> b,t,,', ',,4,d//4 -> ,,d'], 'vvvv', debug=True)\n    assert(x2.shape == (B,T,D))\n    \n    print ('test_warp: all assertions hold')\n    \n\ndef test_warp_pytorch():\n    B, T, D = get_dim_vars('b t d')\n    \n    import torch\n    y: 'btd' = torch.randn(B, T, D)\n    #a reshape followed by permute\n    y = warp(y, 'btd -> b,t,4,d//4 -> b,4,t,d//4', 'vp', debug=False)\n    assert(y.shape == (B,4,T,D//4))\n\n    print ('test_warp_pytorch: all assertions hold')\n    \ntest_warp()\ntest_warp_pytorch()\n\n\n# ## Join: unified stack/concatenate for a list of tensors\n# Crisp shorthand : `'(b,t,d)* -> b,3*t,d'` (**concat**) or `'(b,t,d)* -> b,^,t,d'` (**stack**)\n\n# In[12]:\n\n\nfrom tsalib import join, join_transform\ndef test_join ():\n    B, T, D = get_dim_vars('b t d')\n    x1: 'btd' = np.ones((B, T, D))\n    x2: 'btd' = np.ones((B, T, D))\n    x3: 'btd' = np.ones((B, T, D))\n    \n    #concatenate along the (T) dimension: (b,t,d)* -> (b,3*t,d)\n    x = join([x1, x2, x3], dims=',*,') \n    assert x.shape == (B, 3*T, D)\n\n    \n    #stack: join by adding a new dimension to the front: (b,t,d)* -> (^,b,t,d)\n    x = join([x1, x2, x3], dims='^') \n    assert x.shape == (3, B, T, D)\n    \n    #stack by adding a new dimension at second position: (b,t,d)* -> b,^,t,d)\n    x = join([x1, x2, x3], dims=',^') \n    assert x.shape == (B, 3, T, D)\n    print ('test_join: all assertions passed')\n    \n    \ndef test_join_transform():\n    B, T, D = get_dim_vars('b t d')\n    x1: 'btd' = np.ones((B, T, D))\n    x2: 'btd' = np.ones((B, T, D))\n    x3: 'btd' = np.ones((B, T, D))\n    \n    dims = join_transform([x1,x2,x3], '(b,t,d)* -> b,3*t,d')\n    assert dims == ',*,'\n    #now use backend-dependent join\n    \n    dims = join_transform([x1,x2,x3], '(b,t,d)* -> b,^,t,d')\n    assert dims == ',^,,'\n    #now use backend-dependent join\n    \n    print ('test_join_transform: all assertions passed')\n    \ntest_join()\ntest_join_transform()\n\n\n# ## Align one tensor to another\n\n# In[13]:\n\n\nfrom tsalib import alignto\ndef test_align():\n    B, T, D = dim_vars('Batch(b):20 SeqLength(t):10 EmbeddingDim(d):100', exists_ok=True)\n    \n    x1 = np.random.randn(D,D)\n    x2 = np.random.randn(B,D,T,D)\n\n    x1_aligned = alignto( (x1, 'dd'), 'bdtd')\n    assert x1_aligned.shape == (1,D,1,D)\n    print ('test align: all assertion passed')\ntest_align()\n\n\n# ## Dot Product of two tensors (sharing exactly one dimension)\n\n# In[14]:\n\n\nfrom tsalib import dot\nimport torch\ndef test_dot():\n    B, C, T, D = get_dim_vars('b c t d')\n    #x = np.random.rand(B, C, T)\n    #y = np.random.rand(C, D)\n    x = torch.randn(B, C, T)\n    y = torch.randn(C, D)\n    z = dot('_c_.c_', x, y)\n    assert z.shape == (B, T, D)\n    print('test_dot: all assertions passed')\ntest_dot()\n\n\n# # Reduce ops (min, max, mean, ..) with tsalib\n# Reduction operators aggregate values over one or more tensor dimensions.  \n# `tsalib` provides `reduce_dims` to compute dimension ids using shorthand notation.\n\n# In[15]:\n\n\nfrom tsalib import reduce_dims as rd\n\ndef test_reduce ():\n    assert rd('2bd->2d') == (1,)\n    assert rd('2bd->2') == (1,2)\n    print ('test_reduce: all assertions hold')\ntest_reduce()\n\n\n# In[16]:\n\n\nx: 'btd' = np.random.rand(_B, _T, _D)\ny = np.mean(x, axis=rd('btd->b'))\nassert y.shape == (_B,)\n\n\n# ## Looong warps\n\n# In[17]:\n\n\ndef warp_long1 ():\n    B, T, D, C = get_dim_vars('b t d c')\n    x1: 'btd' = np.ones((B, T, D))\n    x2: 'btd' = np.ones((B, T, D))\n    x3: 'btd' = np.ones((B, T, D))\n    y = warp([x1,x2,x3], '(btd)* -> btdc -> bdtc -> b,d//2,t*2,c', 'jpv')\n    assert y.shape == (B, D//2, T*2, C)\n    print ('warp_long1: all assertions hold')\n    \ndef warp_long2 ():\n    B, T, D, C = get_dim_vars('b t d c')\n    x1: 'btd' = np.ones((B, T, D))\n    y = warp(x1, 'btd -> btd1 -> bdt1 -> b,d//2,t*2,1', 'apv')\n    assert y.shape == (B, D//2, T*2, 1)\n    print ('warp_long2: all assertions hold')\n    \n    \nwarp_long1()\n#warp_long2()\n\n"""
tsalib/__init__.py,0,"b'name = ""tsalib""\n\nfrom .ts import dim_var, dim_vars, get_dim_vars, update_dim_vars_len\nfrom .tsn import tsn_to_shape\nfrom .utils import select, reduce_dims, size_assert, int_shape\nfrom .transforms import view_transform, permute_transform, join_transform\nfrom .transforms import _expand_transform\nfrom .transforms import alignto\nfrom .tensor_ops import warp, join, dot\n'"
tsalib/backend.py,4,"b'from .utils import int_shape\n\nclass ABackend:\n    name = None\n\n    def shape(self, x): raise NotImplementedError\n    def contiguous(self, x): raise NotImplementedError\n    def view(self, x, shape): raise NotImplementedError\n    def transpose(self, x, dims): raise NotImplementedError\n    def expand(self, x, mul_shape): raise NotImplementedError\n    def stack(self, x, mul_shape): raise NotImplementedError\n    def concat(self, x, mul_shape): raise NotImplementedError\n    def einsum(self, eqn, args): raise NotImplementedError\n\n\nclass Numpy(ABackend):\n    name = \'numpy\'\n\n    def __init__(self):\n        import numpy\n        self.np = numpy\n    def shape(self, x): return x.shape\n    def contiguous(self, x): self.np.ascontiguousarray(x)\n    def view(self, x, shape): \n        #print (type(x), x.shape, shape)\n        return x.reshape(shape)\n    def transpose(self, x, dims): return x.transpose(dims)\n    def expand(self, x, mul_shape): return x.tile(mul_shape)\n    def stack(self, xlist, axis): return self.np.stack(xlist, axis=axis)\n    def concat(self, xlist, axis): return self.np.concatenate(xlist, axis=axis)\n    def einsum(self, eqn, args): return self.np.einsum(eqn, *args)\n\nclass PyTorch(ABackend):\n    name = \'pytorch\'\n    def __init__(self):\n        import torch\n        self.torch = torch\n    def shape(self, x): return x.size()\n    def contiguous(self, x): return x.contiguous()\n    def view(self, x, shape): return x.view(shape)\n    def transpose(self, x, dims): return x.permute(dims)\n    def expand(self, x, mul_shape): return x.expand(mul_shape)\n    def stack(self, xlist, axis): return self.torch.stack(xlist, dim=axis)\n    def concat(self, xlist, axis): return self.torch.cat(xlist, dim=axis)\n    def einsum(self, eqn, args): return self.torch.einsum(eqn, args)\n\n\ndef get_tf_shape(tf, tensor):\n  """"""Returns a list of the shape of tensor, preferring static dimensions.\n  (inspired by get_shape_list in BERT code)\n\n  Args:\n    tensor: A tf.Tensor object to find the shape of.\n  Returns:\n    A list of dimensions of the shape of tensor. All static dimensions will\n    be returned as python integers, and dynamic dimensions will be returned\n    as tf.Tensor scalars.\n  """"""\n  shape = tuple(tensor.shape.as_list())\n\n  non_static_indexes = []\n  for (index, dim) in enumerate(shape):\n    if dim is None:\n      non_static_indexes.append(index)\n\n  if not non_static_indexes:\n    return shape\n\n  dyn_shape = tf.shape(tensor)\n  for index in non_static_indexes:\n    shape[index] = dyn_shape[index]\n  return shape\n\nclass TF(ABackend):\n    name = \'tensorflow\'\n    def __init__(self):\n        import tensorflow\n        self.tf = tensorflow\n\n    def shape(self, x):\n        if self.tf.executing_eagerly():\n            return int_shape(x.shape)\n        else:\n            #return tuple(self.tf.unstack(self.tf.shape(x)))\n            return get_tf_shape(self.tf, x)\n\n    def contiguous(self, x): return x\n\n    def view(self, x, shape): return self.tf.reshape(x, shape)\n    def transpose(self, x, dims): return self.tf.transpose(x, dims)\n    def expand(self, x, mul_shape): return self.tf.tile(x, mul_shape)\n    def stack(self, xlist, axis): return self.tf.stack(xlist, axis=axis)\n    def concat(self, xlist, axis): return self.tf.concat(xlist, axis=axis)\n    def einsum(self, eqn, args): return self.tf.einsum(eqn, args)\n\n\nbecache  = {}\ndef from_cache(C):\n    s = str(C)\n    if s not in becache:\n        becache[s] = C()\n    return becache[s]\n\ndef get_str_type(x):\n    #print (x)\n    if isinstance(x, (tuple,list)):\n        if len(x) == 0: return \'\'\n        x = x[0]\n        \n    t = str(type(x))\n    return t\n\ndef get_tensor_lib(x):\n    t = get_str_type(x)\n\n    if \'numpy.\' in t: ret = Numpy\n    elif \'torch.\' in t: ret = PyTorch\n    elif \'tensorflow.\' in t: ret = TF\n    else: ret = None\n\n    return ret\n\ndef is_tensor(x):\n    t = get_str_type(x)\n\n    if \'numpy.\' in t: ret = (\'numpy.ndarray\' in t)\n    elif \'torch.\' in t: ret = (\'torch.Tensor\' in t)\n    elif \'tensorflow.\' in t: ret = (\'ops.Tensor\' in t)\n    else: ret = False\n\n    return ret\n\ndef get_backend_for_tensor(x):\n    \'\'\'\n    get backend for tensor x\n    \'\'\'\n    tlib = get_tensor_lib(x)\n\n    if tlib is None:\n        raise NotImplementedError(f\'Unsupported tensor type {type(x)}. Contributions welcome.\')\n    \n    ret = from_cache(tlib)\n\n    return ret\n\ndef get_backend_by_name (b):\n    if isinstance(b, (Numpy, TF, PyTorch)): return b\n    assert isinstance(b, str)\n    bemap = {\n        \'numpy\': Numpy,\n        \'np\': Numpy,\n        \'torch\': PyTorch,\n        \'pytorch\': PyTorch,\n        \'tensorflow\': TF,\n        \'tf\': TF\n    }\n    if b in bemap: return from_cache(bemap[b])\n    else:\n        raise NotImplementedError(f\'Unsupported backend {b}. Contributions welcome.\')\n\n\n\n\n'"
tsalib/tensor_ops.py,0,"b'from .tsn import tsn_to_tuple, tsn_to_str_list\nfrom .backend import get_backend_by_name, get_backend_for_tensor\nfrom .transforms import _view_transform, _permute_transform, _join_transform, _expand_transform\nfrom .transforms import alignto\nfrom .utils import get_lowercase_symbols, unify_tuples\n\ndef get_backend(backend, x):\n    if backend is not None:\n        be = get_backend_by_name(backend)\n    else:\n        be = get_backend_for_tensor(x)\n\n    return be\n\ndef join (tlist, dims, backend=None):\n    \'\'\'\n    tlist: List[tensor], list of tensors\n    dims: str = \'..,^,...\' or \'..,*,...\' \n    \'\'\'\n    assert isinstance(tlist, list), ""Can only group a list of tensors.""\n    assert len(tlist) > 1, ""Can only group more than one tensors.""\n\n    be = get_backend(backend, tlist[0])\n\n    if len(dims) > 1: assert \',\' in dims, \'Separate dimensions by "","" here.\'\n\n    out_shape = dims.strip().split(\',\')\n    if \'^\' in out_shape:\n        pos = out_shape.index(\'^\')\n        return be.stack(tlist, axis=pos)\n    else:\n        if \'*\' not in out_shape:\n            assert pos != \'-1\', \'Group specification does not contain ""^"" or ""*"".\'\n\n        pos = out_shape.index(\'*\')\n        return be.concat(tlist, axis=pos)\n\n\ndef tsnseq2shape_pairs (tfms):\n    res = [tsn_to_tuple(t.strip()) for t in tfms.split(\'->\')]\n    assert len(res) >= 2, \'At least 2 tfms required: {res}\'\n\n    shape_pairs = list(zip(res[:-1], res[1:]))\n    return shape_pairs\n\n\'\'\'\ndef unify_merge (res, tfms):\n    if len(res) == 0: return tfms\n\n    res_head, res_last = res[:-1], res[-1]\n    tfm_head, tfm_rest = tfms[0], tfms[1:]\n\n\n    subs_map = unify_tuples (res_last, tfm_head)\n    return res_head + [mid] + tfm_rest\n\'\'\'\n\ndef norm_tfms_to_shape_pairs (tfms):\n    \'\'\'\n    tfms  \'x -> y -> z -> u\' or [\'x -> y\', \'y -> z\', \'z -> u\'] or [\'x -> y -> z\', \'z -> u\']\n    \'x\', \'y\', \'z\' are tsns\n    returns: [(X, Y), (Y, Z), (Z, U)], where X is the tuple rep for tsn \'x\', ...\n    \'\'\'\n    res = []\n    if isinstance(tfms, str): \n        shape_pairs = tsnseq2shape_pairs(tfms)\n        res.extend(shape_pairs)\n\n    elif isinstance(tfms, (list, tuple)):\n        for pos, tfm in enumerate(tfms):\n            assert isinstance(tfm, str)\n            shape_pairs = tsnseq2shape_pairs(tfm)\n            res.extend(shape_pairs)\n    else:\n        raise ValueError(f\'unknown format: {tfms}\')\n\n    #print (\'norm_tfms\', tfms, \'\\n\', res)\n    return res\n\n\ndef norm_tfm_names (tfm_names):\n    \'\'\'\n    tfm_names: \'abc\' or [\'a\', \'bc\'] \n    returns: [\'a\', \'b\', \'c\']\n    \'\'\'\n    res = []\n    if isinstance(tfm_names, str): res = list(tfm_names)\n    elif isinstance(tfm_names, (list, tuple)):\n        for n in tfm_names:\n            assert isinstance(n, str)\n            res.extend(list(n))\n    return res\n\ndef tfm_seq_decompose (tfms, tfm_names):\n    \'\'\'\n    Decompose a multi-step transform into basic (view, permute, expand) transforms\n    tfms  \'btd -> b,t,2,d//2 -> b,2,t,d//2\'\n    tfm_names \'vp\' , i.e., view, then permute transform\n    \'\'\'\n    tfm_symbols = norm_tfm_names(tfm_names) # [\'v\', \'t\']\n    tfm_symbols_no_c = [n for n in tfm_symbols if n != \'c\'] #list without \'c\'\n    shape_pairs = norm_tfms_to_shape_pairs(tfms)\n\n    #print (len(tfm_symbols_no_c), len(shape_pairs))\n    assert len(tfm_symbols_no_c) == (len(shape_pairs)), \\\n            f""Num of transform steps {len(shape_pairs)} and names {len(tfm_symbols_no_c)} do not match""\n    \n    tfm_list = [] # (trf symbol, lhs_shape, rhs_shape)\n\n    curr_pos = 0 #count current tfm\'s position (handle implicit contiguous)\n    for sym in tfm_symbols:\n        if sym == \'c\':   #contiguous transform\n            tfm_list.append((sym, None, None))\n        else:\n            l, r = shape_pairs[curr_pos]\n            tfm_list.append((sym, l, r))\n            curr_pos += 1\n\n    return tfm_list\n\n\ndef warp (x, tfms, tfm_names, backend=None, debug=False):\n    \'\'\'\n    Perform a multi-step transform on the tensor x\n    x: tensor\n    tfms:  \'btd -> b,t,2,d//2 -> b,2,t,d//2 -> b,2,t,^n,d//2\'\n    tfm_names: \'vp\' [first (v)iew, then (p)ermute transform]\n    backend:  either a string(\'numpy\', \'tf\', \'torch\') or the corresponding backend.<class>\n    debug: prints per-step debugging information\n    \'\'\'\n\n    be = get_backend(backend, x)\n    tfm_list = tfm_seq_decompose(tfms, tfm_names)\n    #print (f\'tfm list {tfm_list}\')\n\n    ret = x\n    for sym, l, r in tfm_list:\n        if debug:\n            print(f\'*** processing transform.. {sym}\\n {l} -> {r}\')\n        if sym == \'v\' or sym == \'r\': #view transform\n            new_shape = _view_transform(l, r, be.shape(ret))\n            ret = be.view(ret, new_shape)\n        elif sym == \'p\' or sym == \'t\':\n            perm_indices = _permute_transform(l, r)\n            ret = be.transpose(ret, perm_indices)\n        #elif sym == \'e\':\n        #    expand_shape = _expand_transform(l, r)\n        #    ret = be.expand(ret, expand_shape)\n        elif sym == \'a\':\n            ret = alignto((ret, l), r)\n        elif sym == \'c\': \n            ret = be.contiguous(ret)\n        elif sym == \'j\':\n            dims = _join_transform(ret, l, r)\n            ret = join(ret, dims, backend=be)\n        else:\n            assert False, f\'Invalid transform symbol {sym}\'\n        if debug:\n            print (f\'after transform, shape is: {be.shape(ret)}\')\n    return ret\n\n\n\ndef tsn_fill_dot_eqn (lhs, placeholders=[\'_\',\'^\',\'\']):\n\n    \'\'\'\n    construct the full einsum equation for `dot` by adding new unicode symbols\n    lhs: [\'_d\', \'d__\']\n    returns: lhs2 = [\'ad\', \'dbc\'], rhs = \'abc\'\n    \'\'\'\n    assert len(lhs) == 2\n    lhs = [tsn_to_str_list(l)[0] for l in lhs] # [[\'a\', \'b\', \'c\'], ... ]\n\n    #sanity check\n    s1, s2 = set(lhs[0]), set(lhs[1])\n    common = list(s1.intersection(s2).difference(set(placeholders)))\n    assert len(common) == 1\n\n    chars = get_lowercase_symbols(len(s1)+len(s2)-1, common[0]) \n\n    rhs = []\n    cnt = 0\n    lhs2 = []\n    for l in lhs:\n        r = []\n        for c in l: \n            if c in placeholders:\n                o = chars[cnt]\n                rhs.append(o)\n                r.append(o)\n                cnt += 1\n            else: r.append(c)\n        r = \'\'.join(r)\n        lhs2.append(r)\n\n    return lhs2, \'\'.join(rhs), common[0]\n\n\ndef dot (tfm, x, y, backend=None):\n    if \'->\' in tfm: \n        eqn = tfm.replace(\'.\',\',\')\n        #call einsum\n    else:\n        if \'.\' not in tfm:\n            print (\'To avoid confusion, please separate the shorthand shapes by ""."", e.g., ""_d.d__""\')\n            if tfm.count(\',\') == 1:\n                tfm = tfm.replace(\',\',\'.\')\n            else:\n                raise ValueError(f\'Invalid dot transform spec {tfm}\')\n        \n        lp, r = tfm.split(\'.\')\n        lp, r, proj = tsn_fill_dot_eqn([lp, r])\n        eqn = \',\'.join(lp) + \'->\' + r\n\n    #print (f\'eqn: {eqn}, {x.size()} {y.size()}\')\n    be = get_backend(backend, x)\n    return be.einsum(eqn, (x, y))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tsalib/transforms.py,0,"b'from .ts import dim_var, DimExpr, dummy_dvar, TupleSeq\nfrom sympy import sympify, Integer\nfrom .tsn import _sexprs_to_ts, tsn_to_str_list, tsn_to_tuple, check_int_tuple, resolve_to_int_tuple\n\n\n\ndef _view_transform (src, to, in_shape, checkin=False):\n    \'\'\'\n    View Transform\n    src, to: Union[str, Tuple]\n    :src is the current view of the tensor\n    :to is the target view, may contain expressions over named dim variables\n    :in_shape is the shape (list/tuple) of the source tensor \n    :returns the new size of the tensor after view transformation (backend independent)\n    \'\'\'\n    if checkin: check_int_tuple(in_shape)\n\n    src = tsn_to_tuple(src)\n    if (len(src) != len(in_shape)):\n        print (f\'{src}, {in_shape}\')\n        raise ValueError(""Source DimExpr does not match input tensor\'s shape"")\n    to = tsn_to_tuple(to)\n    #print (src, to)\n    assert isinstance(in_shape, (list, tuple))\n    assert isinstance(src, tuple)\n    assert isinstance(to, tuple)\n\n    #sub_map = [(d.e, Symbol(f\'{i}\')) for i, d in enumerate(src)]\n    sub_map = [(d.exp, in_shape[i]) for i, d in enumerate(src)]\n    #print (to, sub_map)\n    out_shape = tuple([t.exp.subs(sub_map) if isinstance(t, DimExpr) else int(t) for t in to])\n    #print (out_shape)\n    out_shape = resolve_to_int_tuple(out_shape)\n    return out_shape\n\ndef view_transform (tfm, in_shape):\n    \'\'\'\n    View transform\n    :tfm:str is the shorthand representation of the transform (\'btd -> b,t*2,d//2\')\n    \'\'\'\n    l, r = tfm.split(\'->\')\n    return _view_transform(l.strip(), r.strip(), in_shape)\n\n\ndef _permute_transform(src, to):\n    \'\'\'\n    Permute Transform\n    src, to: Union[str, Tuple]\n\n    :src is the current dimension arrangement, list of named dim variables\n    :to is the target dimension arragement, list of named dim variables\n    :returns the index tuple for the permutation (backend independent)\n    \'\'\'\n    lhs = tsn_to_tuple(src, num_to_sym=True)\n    rhs = tsn_to_tuple(to, num_to_sym=True)\n    #print (src, lhs, to, rhs)\n    assert isinstance(lhs, tuple)\n    assert isinstance(rhs, tuple)\n\n    assert len(lhs) == len(rhs), ""Source and Target shapes for permutation are not same""\n\n    #map each lhs expression to a string \'0\', \'1\', \'2\', ... (sympy.Symbol)\n    #this avoids double substitution if lhs contains sympy.Integer values\n    sub_map = [(d.exp, f\'{i}\') for i, d in enumerate(lhs)]\n    #print (sub_map)\n    perm_indices = tuple([t.exp.subs(sub_map) for t in rhs])\n    # resolve symbols to integers\n    perm_indices = tuple([int(str(s)) for s in perm_indices])\n    #perm_indices = resolve_to_int_tuple(perm_indices)\n    #print (perm_indices)\n\n    return perm_indices\n\n\ndef permute_transform (tfm):\n    \'\'\'\n    Permute transform\n    :tfm: str\n    :tfm is the shorthand representation of the transform (\',t,d -> ,d,t\')\n    \'\'\'\n    l, r = tfm.split(\'->\')\n    return _permute_transform(l.strip(), r.strip())\n\n\n\n\n\ndef _join_transform (tlist, src, to):\n    \'\'\'\n    src: Union[str, TupleSeq]\n    to: Union[str, tuple]\n    src: (b,c,d)* , to: \'^, b, c, d\'    OR\n    src: (b,c,d)* , to: \'b, 3*c, d\'\n\n    returns the dims shorthand for joining (backend independent)\n\n    \'\'\'\n    lhs = tsn_to_tuple(src) #TupleSeq(B, C, D)\n    rhs = tsn_to_tuple(to) \n\n    assert isinstance(lhs, TupleSeq)\n    assert isinstance(rhs, tuple)\n\n    lhs = lhs.item() # (b, c, d)\n    int1 = Integer(1) # substitute all dim shorthands by \'1\'\n\n    sub_map = [(d.exp, int1) for d in lhs]\n    dims = tuple([t.exp.subs(sub_map) for t in rhs]) # (^, 1, 1, 1)\n\n\n    if len(rhs) == len(lhs): #concat, join by \'*\'\n        #join_dims = (1,3*1,1)\n        dims = \',\'.join(map(lambda x: \'\' if x == int1 else \'*\', dims))\n        #dims = \',*,\'\n    elif len(rhs) == (len(lhs) + 1): #stack, join by \'^\'\n        dims = \',\'.join(map(lambda x: \'\' if x == int1 else \'^\', dims))\n        #dims = \'^,,,\'\n    else:\n        raise ValueError(f\'Unable to join from {src} to {to}\')\n\n    return dims\n\n\ndef join_transform (tlist, tfm):\n    \'\'\'\n    Join transform (backend independent)\n    :tlist: List[tensor]\n    :tfm:str represents the transform ""(b,c,d)* -> ^,b,c,d""\n    returns the dims in TSN for joining \n\n    \'\'\'\n    l, r = tfm.split(\'->\')\n    return _join_transform(tlist, l.strip(), r.strip())\n\n\ndef align_transform (src, to, tile=False):\n    \'\'\'\n    src: tsn \'d,d\'\n    to: tsn \'6, d, t, d\'\n    tile: duplicate src values along new dimensions\n    return: \'^,,^,\' [expansion shorthand for src]\n        if tile is True: also return (6,1,int(T),1)\n    \'\'\'\n\n    lhs = tsn_to_tuple(src) #(D, D)\n    rhs = tsn_to_tuple(to)  #(6, D, T, D)\n\n    assert isinstance(lhs, tuple)\n    assert isinstance(rhs, tuple)\n\n    lhs_pos, rhs_pos = 0, 0\n    expand_dims = []\n    expand_ratio = []\n    for rhs_pos, S in enumerate(rhs):\n        if lhs_pos < len(lhs) and lhs[lhs_pos] == S: \n            #print (\'match\', d, lhs[lhs_pos])\n            expand_dims.append(\'\')\n            if tile: expand_ratio.append(1)\n            lhs_pos += 1\n        else:\n            expand_dims.append(\'^\')\n            if tile:\n                try:\n                    #may fail if S does not eval to a int value\n                    expand_ratio.append(int(S))\n                except:\n                    expand_ratio.append(S)\n    #print (lhs_pos, res)\n\n    if lhs_pos != len(lhs):\n        print (f\'Unable to align {src} to {to}: {src} not a subsequence of {to}\')\n        raise ValueError\n\n    return \',\'.join(expand_dims), expand_ratio\n\n\n\ndef _expansions_as_dict(expansions):\n    if isinstance(expansions, list): #[(T, T*5), (D, D*4)]\n        res = expansions\n    else:\n        assert isinstance(expansions, str) #\'k->k*5,t->t*10\'\n        expansions = expansions.strip().split(\',\')\n        res = []\n        for ex in expansions:\n            t = _sexprs_to_ts(ex.strip().split(\'->\'))\n            #print(t)\n            res.append(t)\n            #print (res)\n\n    exp_map = {l: r for (l, r) in res}\n    return exp_map\n\n\ndef _expand_transform (src, expansions, in_shape):\n    \'\'\'\n    :src        (B, T, D) = (10, 20, 300)\n    :expansions [(T, T*5), (D, D*4)]\n    :returns the expansion shape tuple (-1, 100, 1200)\n    \'\'\'\n    src = tsn_to_tuple(src)\n    exp_map = _expansions_as_dict(expansions)\n    #exp_map = {_sexpr_to_ts(s)[0]: _sexpr_to_ts(t)[0] for (s, t) in (expansions)}\n    sub_map = [(d.exp, in_shape[i]) for i, d in enumerate(src)] # (B, 10), (T, 20), (D, 300)\n\n    #print (expansions, exp_map)\n\n    res = []\n    for k in src:\n        if k not in exp_map: res.append(-1) #keep dim shape same\n        else:\n            v = exp_map[k]\n            assert isinstance(v, DimExpr)\n            res.append(v.exp.subs(sub_map)) #(T*5) -> 100, (D*4) -> 1200\n\n    res = tuple(res)\n    res = resolve_to_int_tuple(res)\n    return res\n\ndef expand_dims_transform(x, tfm):\n    \'\'\'\n    x: (backend) tensor, e.g., shape \'d,d\'\n    tfm: expand tsn: \'^,,^,\'\n    returns tensor of shape \'1,d,1,d\'\n    \'\'\'\n\n    #print (f\'expand: {tfm}\')\n    colon = slice(None)\n    expand_tup = tuple(None if c == \'^\' else colon for c in tfm.split(\',\'))\n    res = x[expand_tup]\n\n    return res\n\n\ndef alignto(x, ys, tile=False):\n    \'\'\'\n    Align tensor x\'s shape to y\'s shape.\n    Assume x\'s shape is a subsequence of y\'s shape\n    Assume tensors x and y support numpy\'s ""None, :"""" indexing notation\n    x: (tensor_var, x_shape)\n    ys: y_shape (tsn of target tensor)\n    \'\'\'\n\n    if tile:\n     raise NotImplementedError(\'tiling to be implemented.\')\n\n    assert isinstance(x, tuple), \'First argument is of form (tensor_var, tsn)\'\n    assert isinstance(ys, (str, tuple)) #TODO: tuple -> Shape\n    xt, xs = x\n    expand_tfm, expand_ratio = align_transform(xs, ys, tile)\n    exp_1 = expand_dims_transform(xt, expand_tfm)\n    return exp_1\n\n\n\n\n\n\n\n\n\n\n\n'"
tsalib/ts.py,0,"b""from sympy import symbols, Integer\nfrom sympy import Symbol, nan, simplify\nimport re\n\ndef arith_op (op, s1, s2):\n    assert isinstance(s1, DimExpr)\n    s2 = DimExpr(s2)\n\n    s1e = s1.exp\n    s2e = s2.exp\n\n    #print (f'arith_op: {op} {s1} {s2}')\n    if op == 'add':\n        se = s1e + s2e\n    elif op == 'mul':\n        se = s1e * s2e\n    elif op == 'truediv':\n        se = s1e / s2e  \n    elif op == 'floordiv':\n        se = s1e // s2e  \n    else:\n        raise NotImplementedError(f'{op}')\n\n    return DimExpr(se)\n\nclass TupleSeq:\n    def __init__(self, s):\n        self.s = s\n    def item(self): return self.s\n\nclass DimVar:\n    decls = {} #caches all dim var declarations\n    parse_regexp = r'(\\w+)(?:\\((\\w+)\\))?(?::(\\d+))?' #Height(h)?(:300)?\n\n    def __init__ (self, decl, exists_ok, cache):\n        '''\n        :decl: declaration string of variable ('Batch(b):20')\n        :exists_ok: if declared earlier, nop\n        :cache: store in `decls` cache\n        '''\n        assert isinstance(decl, str)\n        decl = decl.strip()\n\n        m = re.search(DimVar.parse_regexp, decl)\n        name, sname, val = m.groups()\n        #print (m.groups())\n\n        self._name = name\n        self._sname = sname if sname is not None else name\n        self._val = int(val) if val is not None else nan\n        \n        self._e = Symbol(self._sname)\n        if self._e in DimVar.decls:\n            prevd = DimVar.decls[self._e]\n            if not exists_ok:\n                raise ValueError(f'DimVar {self._sname} already declared as {prevd._name}({self._e}). Use exists_ok=True to skip check.')\n\n        else:\n            if cache: DimVar.decls[self._e] = self\n\n    @property\n    def exp(self): return self._e\n\n    @property\n    def size(self): return self._val\n\n    @property\n    def shortname(self): return self._sname\n\n    @property\n    def name(self):\n        ret = f'{self._name}'\n        if self._name != self._sname: ret += f'({self._sname})'\n        return ret\n\n    def update_len(self, new_val):\n        assert isinstance(new_val, int)\n        self._val = new_val\n\n    @staticmethod\n    def check_decl(sname):\n        return Symbol(sname) in DimVar.decls\n\n    @staticmethod\n    def lookup(sname):\n        #lookup by short name\n        sn = Symbol(sname)\n        #print (f'lookup: {sn} {len(DimVar.decls)}')\n        if len(DimVar.decls) == 0: \n            assert False\n        assert sn in DimVar.decls, f'DimVar short name {sn} not declared.'\n        return DimVar.decls[sn]\n\n    @staticmethod\n    def lookup2(name):\n        #lookup by (long) name\n        for k, decl in DimVar.decls.items():\n            #print ('** lookup2', name, decl._name)\n            if decl._name == name: return decl\n        assert False, f'DimVar full name {name} not declared.'\n\n    @staticmethod\n    def eval(e):\n        sub_map = [(e, dv._val) for e, dv in DimVar.decls.items()]\n        ret = e.subs(sub_map)\n        #print (e, sub_map)\n        #print (f'eval: {e} -> {ret}')\n        return ret\n\n    @staticmethod\n    def eval_name(e):\n        sub_map = [(e, dv.shortname) for e, dv in DimVar.decls.items()]\n        return str(e.subs(sub_map))\n\nclass DimExpr:\n    '''\n    Encapsulates the expression for a particular axis/dimension\n    '''\n    #DEFAULT_VALUE = 1\n\n    def __init__(self, t, is_dvar=False):\n        self._e = None\n        #self.is_dvar = is_dvar # a basic dimension var\n        self.dim_var = None\n        self._val = None #value of dimvar (nan if not set)\n\n        if isinstance(t, int):\n            self._e = Integer(t)\n            self._val = t\n        elif isinstance(t, DimVar):\n            self._e, self._val, self.dim_var = t.exp, t.size, t\n        elif isinstance(t, DimExpr):\n            self._e, self._val, self.dim_var = t._e, t._val, t.dim_var\n        else:\n            #print (f'test expr: {v} {repr(type(v))}')\n            self._e = t\n            self._val = DimVar.eval(t)\n            #self._val = int(v) if v is not nan else v\n\n    @property\n    def exp(self): return self._e\n    @property\n    def len(self): \n        return self._val if (self._val != nan) else None\n\n    def update_len(self, new_len):\n        if self.dim_var is None:\n            raise ValueError('Cannot update length of arbitrary dim expression.')\n        else:\n            self.dim_var.update_len(new_len)\n            self._val = new_len\n\n\n    def __int__(self): \n        #print(f'called int {self._val}')\n        if self._val != nan:\n            return int(self._val)\n        else: \n            #return DimExpr.DEFAULT_VALUE\n            raise ValueError(f'Cannot cast to integer: Default value of {self._e} not provided')\n    def __index__(self): return self.__int__()\n\n    def __add__(self, n): return arith_op('add', self, n)\n    def __radd__(self, n): return self.__add__(n)\n    def __mul__(self, n): return arith_op('mul', self, n)\n    def __rmul__(self, n): return self.__mul__(n)\n\n    def __floordiv__(self, n): return arith_op('floordiv', self, n)\n    def __rfloordiv__(self, n): return self.__floordiv__(n)\n\n    #truediv: '/' provided for convenience; prefer using '//'\n    def __truediv__(self, n): return arith_op('truediv', self, n)\n    def __rtruediv__(self, n): return self.__truediv__(n)\n\n    def __eq__(self, d):\n        #print (f'eq: {self}, {d}')\n        if isinstance(d, int):\n            #semantics: any integer matches nan\n            if self._val == nan: return True \n            else: return self._val == d\n        elif isinstance(d, DimExpr):\n            res = self._e == d._e \n            #print (res)\n            return res\n        else:\n            return False   \n\n    def __hash__(self):\n        return hash(self._e)\n\n    def __repr__(self):\n        s = DimVar.eval_name(self._e)\n        if self._val != nan:\n            s += f':{self._val}'\n        return s\n\n\ndef dim_var (name, exists_ok=False, cache=True):\n    '''\n    Declare a single dimension variable\n    '''\n    d = DimVar(name, exists_ok=exists_ok, cache=cache)\n    return DimExpr(d)\n\ndef dummy_dvar(pos):\n    '''\n    Declare a dummy dimension variable at a particular dim position. Do not cache.\n    '''\n    assert pos >= 0\n    name = f'_dm_{pos}'\n    d = dim_var(name, exists_ok=True, cache=False)\n    #print (f'dummy {d}')\n    return d\n\ndef is_dummy (dvar):\n    return '_dm_' in str(dvar.exp)\n\ndef dim_vars_from_shape(names, shape, exists_ok=False):\n    '''\n    Declare dim vars corresponding to dimensions of tensor\n    :names 'b t d'\n    :shape (10, 30, 300)\n    '''\n    names = names.strip().split(' ')\n    assert len(names) == len(shape), 'Number of Dimension Variables and Shape mismatch'\n\n    tss = [dim_var(f'{name}:{shape[i]}', exists_ok=exists_ok) for i, name in enumerate(names)]\n    if len(names) == 1: return tss[0]\n    else: return tss\n\n\ndef dim_vars(names, exists_ok=False, cache=True):\n    '''\n    Declare multiple dimension variables in one go\n    '''\n    names = names.split()\n    #print (repr(names))\n    tss = [dim_var(name, exists_ok=exists_ok, cache=cache) for name in names]\n\n    if len(names) == 1: return tss[0]\n    else: return tss\n\ndef get_dim_vars(names):\n    '''\n    names: 'b c h w', separated by spaces\n    '''\n    names = names.strip().split(' ')\n    res = [DimExpr(DimVar.lookup(name)) for name in names]\n    if len(names) == 1: return res[0]\n    else: return res\n\ndef get_dim_vars_by_long_name(names):\n    '''\n    names: 'B Channel D'\n    '''\n    names = names.strip().split(' ')\n    res = [DimExpr(DimVar.lookup2(name)) for name in names]\n    if len(names) == 1: return res[0]\n    else: return res\n\n\ndef get_decls (): return DimVar.decls\n\ndef update_dim_vars_len (name2len):\n    '''\n    name2len: dictionary with dim var name and new length pairs\n              e.g., {'t': 50, 'c': 256}\n    '''\n    for name, dimlen in name2len.items():\n        d = DimVar.lookup(name)\n        d.update_len(dimlen)\n\n\ndef declare_common_dim_vars ():\n    B, V, D, Dh = dim_vars('Batch Vocab EmbedDim HiddenDim')\n    C, Ci, Co = dim_vars('Channels InChannels OutChannels')\n    T, Te, Td = dim_vars('Time EncoderTime DecoderTime')\n\n    return B, D, V, Dh, T, Te, Td, C, Ci, Co\n"""
tsalib/ts_lite.py,0,"b""\n\nclass TSLite:\n    def __init__(self, name, w=1, b=0):\n        self.name = name\n        self.w = w\n        self.b = b\n\n    def __add__(self, n):\n        if isinstance(n, int):\n            return TSLite(self.name, self.w, self.b + n)\n        elif isinstance(n, TSLite):\n            return [self, n]\n        else:\n            assert False\n\n    def __mul__(self, n):\n        if isinstance(n, int):\n            return TSLite(self.name, self.w * n, self.b)\n        else:\n            assert False\n\n    def __div__(self, n):\n        if isinstance(n, int):\n            return TSLite(self.name, self.w / n, self.b)\n        else:\n            assert False\n\n    def __eq__(self, d):\n        return self.name == d.name and self.w == d.w and self.b == d.b\n\n    def __repr__(self):\n        s = f'({self.name}*{self.w}+{self.b})'\n        return s\n\n"""
tsalib/tsn.py,0,"b""'''\nUtilities for parsing, transforming tensor shorthand notation (TSN)\n'''\n\nfrom .ts import DimVar, DimExpr, dummy_dvar, TupleSeq\nfrom sympy import sympify, Symbol\nimport re\n\ndef _sexpr_to_ts (e, dummy_idx=0, strict=False, num_to_sym=False):\n    '''\n    A single string expression (sexpr) to Tensor Shape expressions (ts)\n    Converts shorthand dummy/empty placeholders to dummy TSs\n    '''\n    if isinstance(e, DimExpr):  \n        t = e\n    else: \n        assert isinstance(e, str)\n        if e.isdigit() and num_to_sym: e = '_' #convert to dummy var\n        if e == '' or e =='_':  \n            t = dummy_dvar(dummy_idx)\n            dummy_idx += 1\n        elif e == '^':\n            #TODO: better way to handle '^' ?\n            t = DimExpr(Symbol(e))\n        else: \n            #TODO: strict: check if all dim vars in e are previously declared?\n            t = DimExpr(sympify(e))\n\n    return t, dummy_idx\n\ndef _sexprs_to_ts(exprs, strict=False, num_to_sym=False):\n    '''\n    String expressions (sexprs) to Tensor Shape expressions (ts)\n    Converts shorthand dummy/empty placeholders to dummy TSs\n    Returns a tuple of TSs\n    '''\n    dummy_idx = 0\n    res = []\n    for e in exprs:\n        t, dummy_idx = _sexpr_to_ts(e, dummy_idx=dummy_idx, strict=strict, num_to_sym=num_to_sym)\n        res.append(t)\n\n    #print (exprs, res)\n    return tuple(res)\n\n\nseq_re = r'\\((.+)\\)\\*'\n\ndef tsn_to_str_list(ss: str):\n    # 'btd' -> ['b','t','d']\n\n    #remove all whitespace characters\n    ss = re.sub(r'\\s+', '', ss)\n\n    #check if shape corresponds to a sequence        \n    is_seq = False\n    m = re.search(seq_re, ss)\n    if m is not None:  # ss = '(b,t,d)*'\n        ss = m.groups()[0]  # ss = 'b,t,d'\n        #print (f'groups: {m.groups()}') \n        is_seq = True\n\n    if ',' in ss: exprs = ss.strip().split(',') #'b,t,d*2' -> ['b', 't', 'd*2']\n    else: exprs = list(ss)  \n\n    return exprs, is_seq \n\ndef tsn_to_tuple (ss, num_to_sym=False):\n    '''\n    :ss is shape string, e.g., 'btd' or 'b,t,d*2' or '(btd)*'\n    : num_to_sym : converts numeric values in tsn to anonymous symbols ('_')\n    :returns the shape representation in tuple/TupleSeq form\n    '''\n    if isinstance(ss, (list, tuple)):\n        for s in ss: assert isinstance(s, (DimExpr,int))\n        return tuple(ss)\n    elif isinstance(ss, TupleSeq):\n        return ss\n    elif isinstance(ss, str):\n        exprs, is_seq = tsn_to_str_list(ss)  # 'btd' -> 'b', 't', 'd'\n        exprs = _sexprs_to_ts(exprs, num_to_sym=num_to_sym)\n        for e in exprs:\n            assert isinstance(e, DimExpr)\n\n        exprs = tuple(exprs)\n\n        if is_seq:\n            exprs = TupleSeq(exprs)\n\n        return exprs\n\n    else:\n        raise ValueError('Unknown type of ss')\n\ndef check_int_tuple(s):\n    #print(f'int tuple? {s}')\n    for d in s:\n        try: d = int(d)\n        except:\n            raise ValueError(f'Unable to resolve expression {d}')\ndef is_int_tuple(s):\n    ret = all([isinstance(x, int) for x in s])\n    return ret\n\ndef resolve_to_int_tuple(s):\n    '''\n    resolve non-int elements by casting to int or looking up their DimVar values\n    '''\n    res = []\n    for d in s:\n        try: \n            #print (type(d), d)\n            d = int(d)\n            res.append(d)\n        except:\n            if isinstance(d, DimExpr):\n                e = d.exp\n            else:\n                e = d\n                #raise ValueError(f'Unknown item {d}: {type(d)}')\n\n            r = DimVar.eval(e)\n            #print('r is ', r)\n            try: \n                r = int(r)\n                res.append(r)\n            except:\n                raise ValueError(f'Unable to resolve {d}')\n\n    return tuple(res)\n\n\n\ndef tsn_to_shape (tsn):\n    '''\n    tsn: 'b,t,h*d'\n    Lookup each shorthand in cache. \n    returns: (B, T, H*D)\n    '''\n    assert isinstance(tsn, str)\n    return tsn_to_tuple(tsn)"""
tsalib/utils.py,0,"b'from .tsn import tsn_to_str_list, tsn_to_tuple\nfrom .ts import is_dummy\n\ndef get_nth_symbol(n, first=97):\n    #48 (0), 65(A), 97(a)\n    #first = 945 #alpha\n    return chr(first+n)\n\ndef get_lowercase_symbols(n, except_char=None):\n    symbols = [chr(97+i) for i in range(26)]\n    if except_char: symbols.remove(except_char)\n    return symbols[:n]\n\ndef unify_tuples (t1, t2):\n    \'\'\'\n    t1 and t2 can unifiable if \n    - lengths same\n    - at each i, t1[i] and t2[i] are same or one of them is a dummy\n    returns map from dummy symbols to actua dim vars\n    \'\'\'\n    assert isinstance(t1, tuple) and isinstance(t2, tuple)\n    assert len(t1) == len(t2), f\'Cannot match {t1} and {t2} of different lengths\'\n\n    dummy2dv = {}\n    for t1i, t2i in zip(t1, t2):\n        if t1i == t2i: \n            #res.append(t1i)\n            continue\n        assert not (is_dummy(t1i) and is_dummy(t2i)), f\'both dummies {t1i}, {t2i}\'\n\n        if is_dummy(t1i): dummy2dv[t1i] = t2i #res.append(t2i)\n        elif is_dummy(t2i): dummy2dv[t2i] = t1i #res.append(t1i)\n        else:\n            assert False, f""Cannot unify {t1i} and {t2i}""\n\n    return list(dummy2dv.items())\n\ndef int_shape(*s):\n    if len(s) == 1: \n        assert isinstance(s, (tuple,list))\n        s = s[0]\n    else: s = tuple(s)\n    return tuple([int(d) for d in s])\n\ndef select(x, dv_dict, squeeze=False):\n    \'\'\'\n    Index using dimension shorthands\n    \n    x: (t, \'bcd\') -- tensor, shape tuple (can be indexed in numpy notation : x[:,0,:])\n    dv_dict: {\'b\': 0, \'c\': 5} \n    squeeze: [True, False] or a tsn list (\'b,c\') of dims to be squeezed\n    \'\'\'\n    assert not squeeze, \'not implemented\'\n\n    assert isinstance(tuple), \'The first argument should be a tuple of (vector, shape)\'\n    xv, xs = x\n    shape, is_seq = tsn_to_str_list(xs)\n    if not is_seq:\n        raise NotImplementedError(f""get from shape {xs} not implemented"")\n\n    colon = slice(None)\n    slice_tuple = [colon] * len(shape)\n    for pos, sh in shape:\n        if sh in dv_dict:\n            slice_tuple[pos] = dv_dict[sh]\n\n    y = x[slice_tuple]\n    return y\n\ndef size_assert(x_size, sa, dims=None):\n    \'\'\'\n    x_size: integer tuple\n    sa: TSA\n    dims: None or Sequence[int], e.g., [0,1]\n    Check if size of tensor x matches TSA `sa` along `dims` axes\n    \'\'\'\n    x_size, sa = tuple(x_size), tuple(sa)\n    if dims is not None:\n        assert isinstance(dims, (list, tuple))\n        x_size = [x_size[d] for d in dims]\n        sa = [sa[d] for d in dims]\n\n    if x_size != sa:\n        print(f\'Size mismatch: size = {x_size}, expected: {sa}\')\n        assert False\n\n\ndef reduce_dims (tfm):\n    \'\'\'\n    tfm: str, \'btd->b\'\n    \'\'\'\n    src, to = tfm.split(\'->\')\n    src = tsn_to_tuple(src.strip())\n    to = tsn_to_tuple(to.strip())\n\n    assert isinstance(src, tuple)\n    assert isinstance(to, tuple)\n\n    drops = []\n    #check src includes all dims in to\n    for i, d in enumerate(src):\n        if d not in to:\n            drops.append(i)\n\n    return tuple(drops)\n\n'"
models/bert/modeling.py,0,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Adapted by Nishant Sinha for tsalib\n""""""The main BERT model and related functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport copy\nimport json\nimport math\nimport re\nimport six\nimport tensorflow as tf\n\n\nimport sys\nsys.path.append(\'../..\')\nfrom tsalib import int_shape, dim_vars, get_dim_vars, size_assert\nfrom tsalib import alignto, warp\n\n\nclass BertConfig(object):\n  """"""Configuration for `BertModel`.""""""\n\n  def __init__(self,\n               vocab_size: \'v\',\n               hidden_size: \'d\'=768,\n               num_hidden_layers: \'l\'=12,\n               num_attention_heads: \'n\'=4,\n               intermediate_size: \'s\'=3072,\n               hidden_act=""gelu"",\n               hidden_dropout_prob=0.1,\n               attention_probs_dropout_prob=0.1,\n               max_position_embeddings: \'p\'=512,\n               type_vocab_size: \'vt\'=16,\n               initializer_range=0.02):\n    """"""Constructs BertConfig.\n\n    Args:\n      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n      hidden_size: Size of the encoder layers and the pooler layer.\n      num_hidden_layers: Number of hidden layers in the Transformer encoder.\n      num_attention_heads: Number of attention heads for each attention layer in\n        the Transformer encoder.\n      intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n        layer in the Transformer encoder.\n      hidden_act: The non-linear activation function (function or string) in the\n        encoder and pooler.\n      hidden_dropout_prob: The dropout probability for all fully connected\n        layers in the embeddings, encoder, and pooler.\n      attention_probs_dropout_prob: The dropout ratio for the attention\n        probabilities.\n      max_position_embeddings: The maximum sequence length that this model might\n        ever be used with. Typically set this to something large just in case\n        (e.g., 512 or 1024 or 2048).\n      type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n        `BertModel`.\n      initializer_range: The stdev of the truncated_normal_initializer for\n        initializing all weight matrices.\n    """"""\n    batch_size, seq_length = 13, 7\n\n    B, T = dim_vars(f\'batch_size(b):{batch_size} seq_length(t):{seq_length}\', exists_ok=True)\n    V, D = dim_vars(f\'vocab_size(v):{vocab_size} hidden_size(d):{hidden_size}\', exists_ok=True)\n    Nl, N =  dim_vars(f\'num_hidden_layers(l):{num_hidden_layers} num_attention_heads(n):{num_attention_heads}\', exists_ok=True)\n    IS, P, Vt = dim_vars(f\'intermediate_size(s):{intermediate_size} max_position_embeddings(p):{max_position_embeddings} type_vocab_size(vt):{type_vocab_size}\', exists_ok=True)\n    H = dim_vars(f\'size_per_head(h):{hidden_size // num_attention_heads}\', exists_ok=True)\n\n    #print (f\'bert config: D = {D}\')\n\n    self.vocab_size: V = vocab_size\n    self.hidden_size: D = hidden_size\n    self.num_hidden_layers: Nl = num_hidden_layers\n    self.num_attention_heads: N = num_attention_heads\n    self.hidden_act = hidden_act\n    self.intermediate_size: IS = intermediate_size\n    self.hidden_dropout_prob = hidden_dropout_prob\n    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n    self.max_position_embeddings: P = max_position_embeddings\n    self.type_vocab_size: Vt = type_vocab_size\n    self.initializer_range = initializer_range\n\n  @classmethod\n  def from_dict(cls, json_object):\n    """"""Constructs a `BertConfig` from a Python dictionary of parameters.""""""\n    config = BertConfig(vocab_size=None)\n    for (key, value) in six.iteritems(json_object):\n      config.__dict__[key] = value\n    return config\n\n  @classmethod\n  def from_json_file(cls, json_file):\n    """"""Constructs a `BertConfig` from a json file of parameters.""""""\n    with tf.gfile.GFile(json_file, ""r"") as reader:\n      text = reader.read()\n    return cls.from_dict(json.loads(text))\n\n  def to_dict(self):\n    """"""Serializes this instance to a Python dictionary.""""""\n    output = copy.deepcopy(self.__dict__)\n    return output\n\n  def to_json_string(self):\n    """"""Serializes this instance to a JSON string.""""""\n    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n\nclass BertModel(object):\n  """"""BERT model (""Bidirectional Encoder Representations from Transformers"").\n\n  Example usage:\n\n  ```python\n  # Already been converted into WordPiece token ids\n  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n\n  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n\n  model = modeling.BertModel(config=config, is_training=True,\n    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n\n  label_embeddings = tf.get_variable(...)\n  pooled_output = model.get_pooled_output()\n  logits = tf.matmul(pooled_output, label_embeddings)\n  ...\n  ```\n  """"""\n\n  def __init__(self,\n               config,\n               is_training,\n               input_ids: \'bt\',\n               input_mask: \'bt\'=None,\n               token_type_ids: \'bt\'=None,\n               use_one_hot_embeddings=True,\n               scope=None):\n    """"""Constructor for BertModel.\n\n    Args:\n      config: `BertConfig` instance.\n      is_training: bool. true for training model, false for eval model. Controls\n        whether dropout will be applied.\n      input_ids: int32 Tensor of shape [batch_size, seq_length].\n      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n        embeddings or tf.embedding_lookup() for the word embeddings. On the TPU,\n        it is much faster if this is True, on the CPU or GPU, it is faster if\n        this is False.\n      scope: (optional) variable scope. Defaults to ""bert"".\n\n    Raises:\n      ValueError: The config is invalid or one of the input tensor shapes\n        is invalid.\n    """"""\n    config = copy.deepcopy(config)\n    if not is_training:\n      config.hidden_dropout_prob = 0.0\n      config.attention_probs_dropout_prob = 0.0\n\n    #input_shape = get_shape_list(input_ids, expected_rank=2)\n    #batch_size = input_shape[0]\n    #seq_length = input_shape[1]\n    B, T, D, N = get_dim_vars(\'b t d n\')\n\n    if input_mask is None:\n      input_mask = tf.ones(shape=[B, T], dtype=tf.int32)\n\n    if token_type_ids is None:\n      token_type_ids = tf.zeros(shape=[B, T], dtype=tf.int32)\n\n    with tf.variable_scope(scope, default_name=""bert""):\n      with tf.variable_scope(""embeddings""):\n        # Perform embedding lookup on the word ids.\n        self.embedding_output: \'btd\'; self.embedding_table: \'vd\'\n        (self.embedding_output, self.embedding_table) = embedding_lookup(\n            input_ids=input_ids,\n            vocab_size=config.vocab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=""word_embeddings"",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=""token_type_embeddings"",\n            use_position_embeddings=True,\n            position_embedding_name=""position_embeddings"",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(""encoder""):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask : \'btt\' = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers: \'(btd)*\' = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output: \'btd\' = self.all_encoder_layers[-1]\n      # The ""pooler"" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(""pooler""):\n        # We ""pool"" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        #TODO: select_squeeze\n        first_token_tensor: \'bd\' = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output: \'bd\' = tf.layers.dense(\n            first_token_tensor,\n            D,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self) -> \'bd\':\n    return self.pooled_output\n\n  def get_sequence_output(self) -> \'btd\':\n    """"""Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    """"""\n    return self.sequence_output\n\n  def get_all_encoder_layers(self) -> \'(btd)*\':\n    return self.all_encoder_layers\n\n  def get_embedding_output(self) -> \'btd\':\n    """"""Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    """"""\n    return self.embedding_output\n\n  def get_embedding_table(self) -> \'vd\':\n    return self.embedding_table\n\n\ndef gelu(input_tensor):\n  """"""Gaussian Error Linear Unit.\n\n  This is a smoother version of the RELU.\n  Original paper: https://arxiv.org/abs/1606.08415\n\n  Args:\n    input_tensor: float Tensor to perform activation.\n\n  Returns:\n    `input_tensor` with the GELU activation applied.\n  """"""\n  cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))\n  return input_tensor * cdf\n\n\ndef get_activation(activation_string):\n  """"""Maps a string to a Python function, e.g., ""relu"" => `tf.nn.relu`.\n\n  Args:\n    activation_string: String name of the activation function.\n\n  Returns:\n    A Python function corresponding to the activation function. If\n    `activation_string` is None, empty, or ""linear"", this will return None.\n    If `activation_string` is not a string, it will return `activation_string`.\n\n  Raises:\n    ValueError: The `activation_string` does not correspond to a known\n      activation.\n  """"""\n\n  # We assume that anything that""s not a string is already an activation\n  # function, so we just return it.\n  if not isinstance(activation_string, six.string_types):\n    return activation_string\n\n  if not activation_string:\n    return None\n\n  act = activation_string.lower()\n  if act == ""linear"":\n    return None\n  elif act == ""relu"":\n    return tf.nn.relu\n  elif act == ""gelu"":\n    return gelu\n  elif act == ""tanh"":\n    return tf.tanh\n  else:\n    raise ValueError(""Unsupported activation: %s"" % act)\n\n\ndef get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n  """"""Compute the union of the current variables and checkpoint variables.""""""\n  assignment_map = {}\n  initialized_variable_names = {}\n\n  name_to_variable = collections.OrderedDict()\n  for var in tvars:\n    name = var.name\n    m = re.match(""^(.*):\\\\d+$"", name)\n    if m is not None:\n      name = m.group(1)\n    name_to_variable[name] = var\n\n  init_vars = tf.train.list_variables(init_checkpoint)\n\n  assignment_map = collections.OrderedDict()\n  for x in init_vars:\n    (name, var) = (x[0], x[1])\n    if name not in name_to_variable:\n      continue\n    assignment_map[name] = name\n    initialized_variable_names[name] = 1\n    initialized_variable_names[name + "":0""] = 1\n\n  return (assignment_map, initialized_variable_names)\n\n\ndef dropout(input_tensor, dropout_prob):\n  """"""Perform dropout.\n\n  Args:\n    input_tensor: float Tensor.\n    dropout_prob: Python float. The probability of dropping out a value (NOT of\n      *keeping* a dimension as in `tf.nn.dropout`).\n\n  Returns:\n    A version of `input_tensor` with dropout applied.\n  """"""\n  if dropout_prob is None or dropout_prob == 0.0:\n    return input_tensor\n\n  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n  return output\n\n\ndef layer_norm(input_tensor, name=None):\n  """"""Run layer normalization on the last dimension of the tensor.""""""\n  return tf.contrib.layers.layer_norm(\n      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n\n\ndef layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n  """"""Runs layer normalization followed by dropout.""""""\n  output_tensor = layer_norm(input_tensor, name)\n  output_tensor = dropout(output_tensor, dropout_prob)\n  return output_tensor\n\n\ndef create_initializer(initializer_range=0.02):\n  """"""Creates a `truncated_normal_initializer` with the given range.""""""\n  return tf.truncated_normal_initializer(stddev=initializer_range)\n\n\ndef embedding_lookup(input_ids,\n                     vocab_size,\n                     embedding_size=128,\n                     initializer_range=0.02,\n                     word_embedding_name=""word_embeddings"",\n                     use_one_hot_embeddings=False):\n  """"""Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.nn.embedding_lookup()`. One hot is better\n      for TPUs.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  """"""\n  # This function assumes that the input is of shape [batch_size, seq_length,\n  # num_inputs].\n  #\n  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n  # reshape to [batch_size, seq_length, 1].\n  if input_ids.shape.ndims == 2:\n    input_ids = tf.expand_dims(input_ids, axis=[-1])\n\n  B, T, D = get_dim_vars(\'b t d\')\n\n  input_ids: \'bti\' #i : num of inputs\n  #TODO: define/pickup i from input_ids\n  i = get_shape_list(input_ids)[-1]\n\n  embedding_table: \'vd\' = tf.get_variable(\n      name=word_embedding_name,\n      shape=[vocab_size, embedding_size],\n      initializer=create_initializer(initializer_range))\n\n  if use_one_hot_embeddings:\n    flat_input_ids:\'b*t*i\' = tf.reshape(input_ids, [-1])\n    one_hot_input_ids: \'b*t*i,v\' = tf.one_hot(flat_input_ids, depth=vocab_size)\n    output: \'b*t*i,d\' = tf.matmul(one_hot_input_ids, embedding_table)\n  else:\n    output = tf.nn.embedding_lookup(embedding_table, input_ids)\n\n  #input_shape: \'bti\' = get_shape_list(input_ids)\n\n  output: \'btd\' = warp(output, tfms=f\'b*t*{i},d -> b,t,d*{i}\', tfm_names=\'r\')\n  \n  return (output, embedding_table)\n\n\ndef embedding_postprocessor(input_tensor: \'btd\',\n                            use_token_type=False,\n                            token_type_ids: \'bt\'=None,\n                            token_type_vocab_size=16,\n                            token_type_embedding_name=""token_type_embeddings"",\n                            use_position_embeddings=True,\n                            position_embedding_name=""position_embeddings"",\n                            initializer_range=0.02,\n                            max_position_embeddings=512,\n                            dropout_prob=0.1):\n  """"""Performs various post-processing on a word embedding tensor.\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length,\n      embedding_size].\n    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      Must be specified if `use_token_type` is True.\n    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n    token_type_embedding_name: string. The name of the embedding table variable\n      for token type ids.\n    use_position_embeddings: bool. Whether to add position embeddings for the\n      position of each token in the sequence.\n    position_embedding_name: string. The name of the embedding table variable\n      for positional embeddings.\n    initializer_range: float. Range of the weight initialization.\n    max_position_embeddings: int. Maximum sequence length that might ever be\n      used with this model. This can be longer than the sequence length of\n      input_tensor, but cannot be shorter.\n    dropout_prob: float. Dropout probability applied to the final output tensor.\n\n  Returns:\n    float tensor with same shape as `input_tensor`.\n\n  Raises:\n    ValueError: One of the tensor shapes or input values is invalid.\n  """"""\n  #input_shape = get_shape_list(input_tensor, expected_rank=3)\n  #batch_size = input_shape[0]\n  #seq_length = input_shape[1]\n  #width = input_shape[2]\n\n  B, T, D = int_shape(get_dim_vars(\'b t d\'))\n  batch_size, seq_length, width = B, T, D\n  size_assert(get_shape_list(input_tensor), (B,T,D))\n\n  output: \'btd\' = input_tensor\n\n  if use_token_type:\n    if token_type_ids is None:\n      raise ValueError(""`token_type_ids` must be specified if""\n                       ""`use_token_type` is True."")\n    token_type_table: \'tv,d\' = tf.get_variable(\n        name=token_type_embedding_name,\n        shape=[token_type_vocab_size, width],\n        initializer=create_initializer(initializer_range))\n    # This vocab will be small so we always do one-hot here, since it is always\n    # faster for a small vocabulary.\n    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)\n    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n    token_type_embeddings: \'btd\' = tf.reshape(token_type_embeddings, (B,T,D) )\n                                       #[batch_size, seq_length, width])\n    output += token_type_embeddings\n\n  if use_position_embeddings:\n    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n    with tf.control_dependencies([assert_op]):\n      full_position_embeddings = tf.get_variable(\n          name=position_embedding_name,\n          shape=[max_position_embeddings, width],\n          initializer=create_initializer(initializer_range))\n      # Since the position embedding table is a learned variable, we create it\n      # using a (long) sequence length `max_position_embeddings`. The actual\n      # sequence length might be shorter than this, for faster training of\n      # tasks that do not have long sequences.\n      #\n      # So `full_position_embeddings` is effectively an embedding table\n      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n      # perform a slice.\n      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n                                     [seq_length, -1])\n      num_dims = len(output.shape.as_list())\n\n      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n      # we broadcast among the first dimensions, which is typically just\n      # the batch size.\n      position_broadcast_shape = []\n      for _ in range(num_dims - 2):\n        position_broadcast_shape.append(1)\n      position_broadcast_shape.extend([T, D])\n      position_embeddings = tf.reshape(position_embeddings,\n                                       position_broadcast_shape)\n      output += position_embeddings\n\n  output: \'btd\' = layer_norm_and_dropout(output, dropout_prob)\n  return output\n\n\n\ndef create_attention_mask_from_input_mask(from_tensor: \'b,f,...\', to_mask: \'b,t\'):\n\n    #TODO: compact further by alignto(.., tile=True)\n    B, F = get_shape_list(from_tensor, expected_rank=[2, 3])[:2]\n    To = get_shape_list(to_mask, expected_rank=2)[1]\n    to_mask = tf.cast(alignto((to_mask, \'bt\'), \'b_t\'), tf.float32)\n    broadcast_ones = tf.ones(shape=[B, F, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n    size_assert (get_shape_list(mask), (B, F, To))\n\n    return mask\n\ndef attention_layer(from_tensor: \'b*t,d\',\n                    to_tensor: \'b*t,d\',\n                    attention_mask: \'b,t,t\'=None,\n                    num_attention_heads=1,\n                    size_per_head=512,\n                    query_act=None,\n                    key_act=None,\n                    value_act=None,\n                    attention_probs_dropout_prob=0.0,\n                    initializer_range=0.02,\n                    do_return_2d_tensor=False,\n                    batch_size=None,\n                    from_seq_length=None,\n                    to_seq_length=None):\n  """"""Performs multi-headed attention from `from_tensor` to `to_tensor`.\n\n  This is an implementation of multi-headed attention based on ""Attention\n  is all you Need"". If `from_tensor` and `to_tensor` are the same, then\n  this is self-attention. Each timestep in `from_tensor` attends to the\n  corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n\n  This function first projects `from_tensor` into a ""query"" tensor and\n  `to_tensor` into ""key"" and ""value"" tensors. These are (effectively) a list\n  of tensors of length `num_attention_heads`, where each tensor is of shape\n  [batch_size, seq_length, size_per_head].\n\n  Then, the query and key tensors are dot-producted and scaled. These are\n  softmaxed to obtain attention probabilities. The value tensors are then\n  interpolated by these probabilities, then concatenated back to a single\n  tensor and returned.\n\n  In practice, the multi-headed attention are done with transposes and\n  reshapes rather than actual separate tensors.\n\n  Args:\n    from_tensor: float Tensor of shape [batch_size, from_seq_length,\n      from_width].\n    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n    attention_mask: (optional) int32 Tensor of shape [batch_size,\n      from_seq_length, to_seq_length]. The values should be 1 or 0. The\n      attention scores will effectively be set to -infinity for any positions in\n      the mask that are 0, and will be unchanged for positions that are 1.\n    num_attention_heads: int. Number of attention heads.\n    size_per_head: int. Size of each attention head.\n    query_act: (optional) Activation function for the query transform.\n    key_act: (optional) Activation function for the key transform.\n    value_act: (optional) Activation function for the value transform.\n    attention_probs_dropout_prob: (optional) float. Dropout probability of the\n      attention probabilities.\n    initializer_range: float. Range of the weight initializer.\n    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n      * from_seq_length, num_attention_heads * size_per_head]. If False, the\n      output will be of shape [batch_size, from_seq_length, num_attention_heads\n      * size_per_head].\n    batch_size: (Optional) int. If the input is 2D, this might be the batch size\n      of the 3D version of the `from_tensor` and `to_tensor`.\n    from_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `from_tensor`.\n    to_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `to_tensor`.\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length,\n      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n      true, this will be of shape [batch_size * from_seq_length,\n      num_attention_heads * size_per_head]).\n\n  Raises:\n    ValueError: Any of the arguments or tensor shapes are invalid.\n  """"""\n\n  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n  \n  if len(from_shape) != len(to_shape):\n    raise ValueError(\n        ""The rank of `from_tensor` must match the rank of `to_tensor`."")\n\n  if len(from_shape) == 3:\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_seq_length = to_shape[1]\n  elif len(from_shape) == 2:\n    if (batch_size is None or from_seq_length is None or to_seq_length is None):\n      raise ValueError(\n          ""When passing in rank 2 tensors to attention_layer, the values ""\n          ""for `batch_size`, `from_seq_length`, and `to_seq_length` ""\n          ""must all be specified."")\n\n  # Scalar dimensions referenced here:\n  #   B(b) = batch size (number of sequences)\n  #   F(f) = `from_tensor` sequence length\n  #   T(t) = `to_tensor` sequence length\n  #   N(n) = `num_attention_heads`\n  #   H(h) = `size_per_head`\n\n  #from_tensor_2d: \'b*t,d\' = reshape_to_matrix(from_tensor)\n  #to_tensor_2d: \'b*t,d\' = reshape_to_matrix(to_tensor)\n  \n  from_tensor_2d: \'b*t,d\' = from_tensor\n  to_tensor_2d: \'b*t,d\' = to_tensor\n\n  query_layer: \'b*t,d\' = tf.layers.dense(\n      from_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=query_act,\n      name=""query"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  key_layer: \'b*t,d\' = tf.layers.dense(\n      to_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=key_act,\n      name=""key"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  value_layer: \'b*t,d\' = tf.layers.dense(\n      to_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=value_act,\n      name=""value"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  query_layer: \'bnth\' = warp(query_layer, \'b*t,d -> btnh -> bnth\', \'vp\')\n  key_layer: \'bnth\' = warp(key_layer, \'b*t,d -> btnh -> bnth\', \'vp\')\n\n  # Take the dot product between ""query"" and ""key"" to get the raw\n  # attention scores.\n  attention_scores: \'bntt\' = tf.matmul(query_layer, key_layer, transpose_b=True)\n  attention_scores: \'bntt\' = tf.multiply(attention_scores,\n                                 1.0 / math.sqrt(float(size_per_head)))\n\n  if attention_mask is not None:\n    # `attention_mask` = [B, 1, F, T]\n    #attention_mask = tf.expand_dims(attention_mask, axis=[1])\n    attention_mask = alignto((attention_mask,\'btt\'), \'bntt\')\n\n    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n    # masked positions, this operation will create a tensor which is 0.0 for\n    # positions we want to attend and -10000.0 for masked positions.\n    adder: \'bntt\' = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n\n    # Since we are adding it to the raw scores before the softmax, this is\n    # effectively the same as removing these entirely.\n    attention_scores += adder\n\n  # Normalize the attention scores to probabilities.\n  # `attention_probs` = [B, N, F, T]\n  attention_probs: \'bntt\' = tf.nn.softmax(attention_scores)\n\n  # This is actually dropping out entire tokens to attend to, which might\n  # seem a bit unusual, but is taken from the original Transformer paper.\n  attention_probs: \'bntt\' = dropout(attention_probs, attention_probs_dropout_prob)\n\n  value_layer: \'bnth\' = warp(value_layer, \'b*t,n*h -> btnh -> bnth\', \'vp\')\n\n  context_layer: \'bnth\' = tf.matmul(attention_probs, value_layer) #bntt,bnth->bnth OR ___t,__t_\n\n  if do_return_2d_tensor:\n    context_layer = warp(context_layer, \'bnth->btnh->b*t,n*h\', \'pv\')\n  else:\n    context_layer = warp(context_layer, \'bnth->btnh->b,t,n*h\', \'pv\')\n\n  return context_layer\n\n\n\ndef transformer_model(input_tensor: \'btd\',\n                      attention_mask: \'btt\'=None,\n                      hidden_size: \'d\'=768,\n                      num_hidden_layers: \'l\'=12,\n                      num_attention_heads: \'h\'=4,\n                      intermediate_size: \'s\'=3072,\n                      intermediate_act_fn=gelu,\n                      hidden_dropout_prob=0.1,\n                      attention_probs_dropout_prob=0.1,\n                      initializer_range=0.02,\n                      do_return_all_layers=False):\n  """"""Multi-headed, multi-layer Transformer from ""Attention is All You Need"".\n\n  This is almost an exact implementation of the original Transformer encoder.\n\n  See the original paper:\n  https://arxiv.org/abs/1706.03762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the ""intermediate"" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  """"""\n  if hidden_size % num_attention_heads != 0:\n    raise ValueError(\n        ""The hidden size (%d) is not a multiple of the number of attention ""\n        ""heads (%d)"" % (hidden_size, num_attention_heads))\n\n  attention_head_size = int(hidden_size / num_attention_heads)\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  #batch_size = input_shape[0]\n  #seq_length = input_shape[1]\n  #input_width = input_shape[2]\n\n  B, T, D = input_shape\n  batch_size, seq_length, input_width = B, T, D\n\n  # The Transformer performs sum residuals on all layers so the input needs\n  # to be the same as the hidden size.\n  if D != hidden_size:\n    raise ValueError(""The width of the input tensor (%d) != hidden size (%d)"" %\n                     (D, hidden_size))\n\n  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n  # help the optimizer.\n  #prev_output: \'b*t,d\' = reshape_to_matrix(input_tensor)\n  prev_output: \'b*t,d\' = warp(input_tensor, \'btd -> b*t,d\', \'v\')\n  size_assert(get_shape_list(prev_output), (B*T,D))\n\n  all_layer_outputs: \'(btd)*\' = []\n  for layer_idx in range(num_hidden_layers):\n    with tf.variable_scope(""layer_%d"" % layer_idx):\n      layer_input: \'b*t,d\' = prev_output\n\n      with tf.variable_scope(""attention""):\n        attention_heads: \'(b*t,d)*\' = []\n        with tf.variable_scope(""self""):\n          attention_head: \'b*t,d\' = attention_layer(\n              from_tensor=layer_input,\n              to_tensor=layer_input,\n              attention_mask=attention_mask,\n              num_attention_heads=num_attention_heads,\n              size_per_head=attention_head_size,\n              attention_probs_dropout_prob=attention_probs_dropout_prob,\n              initializer_range=initializer_range,\n              do_return_2d_tensor=True,\n              batch_size=batch_size,\n              from_seq_length=seq_length,\n              to_seq_length=seq_length)\n          attention_heads.append(attention_head)\n\n        attention_output = None\n        if len(attention_heads) == 1:\n          attention_output = attention_heads[0]\n        else:\n          # In the case where we have other sequences, we just concatenate\n          # them to the self-attention head before the projection.\n          attention_output: \'b*t,d\' = tf.concat(attention_heads, axis=-1)\n\n        # Run a linear projection of `hidden_size` then add a residual\n        # with `layer_input`.\n        with tf.variable_scope(""output""):\n          attention_output: \'b*t,d\' = tf.layers.dense(\n              attention_output,\n              hidden_size,\n              kernel_initializer=create_initializer(initializer_range))\n          attention_output = dropout(attention_output, hidden_dropout_prob)\n          attention_output = layer_norm(attention_output + layer_input)\n\n      # The activation is only applied to the ""intermediate"" hidden layer.\n      with tf.variable_scope(""intermediate""):\n        intermediate_output: \'b*t,s\' = tf.layers.dense(\n            attention_output,\n            intermediate_size,\n            activation=intermediate_act_fn,\n            kernel_initializer=create_initializer(initializer_range))\n\n      # Down-project back to `hidden_size` then add the residual.\n      with tf.variable_scope(""output""):\n        layer_output: \'b*t,d\' = tf.layers.dense(\n            intermediate_output,\n            hidden_size,\n            kernel_initializer=create_initializer(initializer_range))\n        layer_output = dropout(layer_output, hidden_dropout_prob)\n        layer_output = layer_norm(layer_output + attention_output)\n        prev_output: \'b*t,d\' = layer_output\n        all_layer_outputs.append(layer_output)\n\n  if do_return_all_layers:\n    final_outputs: \'(btd)*\' = []\n    for layer_output in all_layer_outputs:\n      #final_output: \'btd\' = reshape_from_matrix(layer_output, input_shape)\n      final_output: \'btd\' = warp(layer_output, \'b*t,d -> btd\', \'r\')\n      final_outputs.append(final_output)\n    return final_outputs\n  else:\n    final_output: \'btd\' = warp(layer_output, \'b*t,d -> btd\', \'r\') \n    return final_output\n\n\ndef get_shape_list(tensor, expected_rank=None, name=None):\n  """"""Returns a list of the shape of tensor, preferring static dimensions.\n\n  Args:\n    tensor: A tf.Tensor object to find the shape of.\n    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n      specified and the `tensor` has a different rank, and exception will be\n      thrown.\n    name: Optional name of the tensor for the error message.\n\n  Returns:\n    A list of dimensions of the shape of tensor. All static dimensions will\n    be returned as python integers, and dynamic dimensions will be returned\n    as tf.Tensor scalars.\n  """"""\n  if name is None:\n    name = tensor.name\n\n  if expected_rank is not None:\n    assert_rank(tensor, expected_rank, name)\n\n  shape = tensor.shape.as_list()\n\n  non_static_indexes = []\n  for (index, dim) in enumerate(shape):\n    if dim is None:\n      non_static_indexes.append(index)\n\n  if not non_static_indexes:\n    return shape\n\n  dyn_shape = tf.shape(tensor)\n  for index in non_static_indexes:\n    shape[index] = dyn_shape[index]\n  return shape\n\n\n\'\'\'\ndef reshape_to_matrix(input_tensor: \'x,y,...,w\') -> \'x*y*...,w\':\n  """"""Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).""""""\n  ndims = input_tensor.shape.ndims\n  if ndims < 2:\n    raise ValueError(""Input tensor must have at least rank 2. Shape = %s"" %\n                     (input_tensor.shape))\n  if ndims == 2:\n    return input_tensor\n\n  width = input_tensor.shape[-1]\n  output_tensor = tf.reshape(input_tensor, [-1, width])\n  return output_tensor\n\ndef reshape_from_matrix(output_tensor, orig_shape_list):\n  """"""Reshapes a rank 2 tensor back to its original rank >= 2 tensor.""""""\n  if len(orig_shape_list) == 2:\n    return output_tensor\n\n  output_shape = get_shape_list(output_tensor)\n\n  orig_dims = orig_shape_list[0:-1]\n  width = output_shape[-1]\n\n  return tf.reshape(output_tensor, orig_dims + [width])\n\'\'\'\n\ndef assert_rank(tensor, expected_rank, name=None):\n  """"""Raises an exception if the tensor rank is not of the expected rank.\n\n  Args:\n    tensor: A tf.Tensor to check the rank of.\n    expected_rank: Python integer or list of integers, expected rank.\n    name: Optional name of the tensor for the error message.\n\n  Raises:\n    ValueError: If the expected shape doesn\'t match the actual shape.\n  """"""\n  if name is None:\n    name = tensor.name\n\n  expected_rank_dict = {}\n  if isinstance(expected_rank, six.integer_types):\n    expected_rank_dict[expected_rank] = True\n  else:\n    for x in expected_rank:\n      expected_rank_dict[x] = True\n\n  actual_rank = tensor.shape.ndims\n  if actual_rank not in expected_rank_dict:\n    scope_name = tf.get_variable_scope().name\n    raise ValueError(\n        ""For the tensor `%s` in scope `%s`, the actual rank ""\n        ""`%d` (shape = %s) is not equal to the expected rank `%s`"" %\n        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))\n'"
models/bert/modeling_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport json\nimport random\nimport re\n\nimport six\nimport tensorflow as tf\nimport modeling\n\n\nimport sys\nsys.path.append(\'../..\')\nfrom tsalib import dim_vars\n\n\n\nclass BertModelTest(tf.test.TestCase):\n\n  class BertModelTester(object):\n\n    def __init__(self,\n                 parent,\n                 batch_size=13,\n                 seq_length=7,\n                 is_training=True,\n                 use_input_mask=True,\n                 use_token_type_ids=True,\n                 vocab_size=99,\n                 hidden_size=32,\n                 num_hidden_layers=5,\n                 num_attention_heads=4,\n                 intermediate_size=37,\n                 hidden_act=""gelu"",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 type_vocab_size=16,\n                 initializer_range=0.02,\n                 scope=None):\n\n\n      self.parent = parent\n      self.batch_size = batch_size\n      self.seq_length = seq_length\n      self.is_training = is_training\n      self.use_input_mask = use_input_mask\n      self.use_token_type_ids = use_token_type_ids\n      self.vocab_size = vocab_size\n      self.hidden_size = hidden_size\n      self.num_hidden_layers = num_hidden_layers\n      self.num_attention_heads = num_attention_heads\n      self.intermediate_size = intermediate_size\n      self.hidden_act = hidden_act\n      self.hidden_dropout_prob = hidden_dropout_prob\n      self.attention_probs_dropout_prob = attention_probs_dropout_prob\n      self.max_position_embeddings = max_position_embeddings\n      self.type_vocab_size = type_vocab_size\n      self.initializer_range = initializer_range\n      self.scope = scope\n\n\n      #B, T = dim_vars(f\'batch_size(b):{batch_size} seq_length(t):{seq_length}\', exists_ok=True)\n      #V, D = dim_vars(f\'vocab_size(v):{vocab_size} hidden_size(d):{hidden_size}\', exists_ok=True)\n      #Nl, H =  dim_vars(f\'num_hidden_layers(l):{num_hidden_layers} num_attention_heads(h):{num_attention_heads}\')\n      #IS, P, Vt = dim_vars(f\'intermediate_size(s):{intermediate_size} max_position_embeddings(p):{max_position_embeddings} type_vocab_size(vt):{type_vocab_size}\')\n\n\n    def create_model(self):\n      input_ids = BertModelTest.ids_tensor([self.batch_size, self.seq_length],\n                                           self.vocab_size)\n\n      input_mask = None\n      if self.use_input_mask:\n        input_mask = BertModelTest.ids_tensor(\n            [self.batch_size, self.seq_length], vocab_size=2)\n\n      token_type_ids = None\n      if self.use_token_type_ids:\n        token_type_ids = BertModelTest.ids_tensor(\n            [self.batch_size, self.seq_length], self.type_vocab_size)\n\n      config = modeling.BertConfig(\n          vocab_size=self.vocab_size,\n          hidden_size=self.hidden_size,\n          num_hidden_layers=self.num_hidden_layers,\n          num_attention_heads=self.num_attention_heads,\n          intermediate_size=self.intermediate_size,\n          hidden_act=self.hidden_act,\n          hidden_dropout_prob=self.hidden_dropout_prob,\n          attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n          max_position_embeddings=self.max_position_embeddings,\n          type_vocab_size=self.type_vocab_size,\n          initializer_range=self.initializer_range)\n\n      model = modeling.BertModel(\n          config=config,\n          is_training=self.is_training,\n          input_ids=input_ids,\n          input_mask=input_mask,\n          token_type_ids=token_type_ids,\n          scope=self.scope)\n\n      outputs = {\n          ""embedding_output"": model.get_embedding_output(),\n          ""sequence_output"": model.get_sequence_output(),\n          ""pooled_output"": model.get_pooled_output(),\n          ""all_encoder_layers"": model.get_all_encoder_layers(),\n      }\n      return outputs\n\n    def check_output(self, result):\n      self.parent.assertAllEqual(\n          result[""embedding_output""].shape,\n          [self.batch_size, self.seq_length, self.hidden_size])\n\n      self.parent.assertAllEqual(\n          result[""sequence_output""].shape,\n          [self.batch_size, self.seq_length, self.hidden_size])\n\n      self.parent.assertAllEqual(result[""pooled_output""].shape,\n                                 [self.batch_size, self.hidden_size])\n\n  def test_default(self):\n\n    self.run_tester(BertModelTest.BertModelTester(self))\n\n  def test_config_to_json_string(self):\n    config = modeling.BertConfig(vocab_size=99, hidden_size=32)\n    obj = json.loads(config.to_json_string())\n    self.assertEqual(obj[""vocab_size""], 99)\n    self.assertEqual(obj[""hidden_size""], 32)\n\n  def run_tester(self, tester):\n    with self.test_session() as sess:\n      ops = tester.create_model()\n      init_op = tf.group(tf.global_variables_initializer(),\n                         tf.local_variables_initializer())\n      sess.run(init_op)\n      output_result = sess.run(ops)\n      tester.check_output(output_result)\n\n      self.assert_all_tensors_reachable(sess, [init_op, ops])\n\n  @classmethod\n  def ids_tensor(cls, shape, vocab_size, rng=None, name=None):\n    """"""Creates a random int32 tensor of the shape within the vocab size.""""""\n    if rng is None:\n      rng = random.Random()\n\n    total_dims = 1\n    for dim in shape:\n      total_dims *= dim\n\n    values = []\n    for _ in range(total_dims):\n      values.append(rng.randint(0, vocab_size - 1))\n\n    return tf.constant(value=values, dtype=tf.int32, shape=shape, name=name)\n\n  def assert_all_tensors_reachable(self, sess, outputs):\n    """"""Checks that all the tensors in the graph are reachable from outputs.""""""\n    graph = sess.graph\n\n    ignore_strings = [\n        ""^.*/assert_less_equal/.*$"",\n        ""^.*/dilation_rate$"",\n        ""^.*/Tensordot/concat$"",\n        ""^.*/Tensordot/concat/axis$"",\n        ""^testing/.*$"",\n    ]\n\n    ignore_regexes = [re.compile(x) for x in ignore_strings]\n\n    unreachable = self.get_unreachable_ops(graph, outputs)\n    filtered_unreachable = []\n    for x in unreachable:\n      do_ignore = False\n      for r in ignore_regexes:\n        m = r.match(x.name)\n        if m is not None:\n          do_ignore = True\n      if do_ignore:\n        continue\n      filtered_unreachable.append(x)\n    unreachable = filtered_unreachable\n\n    self.assertEqual(\n        len(unreachable), 0, ""The following ops are unreachable: %s"" %\n        ("" "".join([x.name for x in unreachable])))\n\n  @classmethod\n  def get_unreachable_ops(cls, graph, outputs):\n    """"""Finds all of the tensors in graph that are unreachable from outputs.""""""\n    outputs = cls.flatten_recursive(outputs)\n    output_to_op = collections.defaultdict(list)\n    op_to_all = collections.defaultdict(list)\n    assign_out_to_in = collections.defaultdict(list)\n\n    for op in graph.get_operations():\n      for x in op.inputs:\n        op_to_all[op.name].append(x.name)\n      for y in op.outputs:\n        output_to_op[y.name].append(op.name)\n        op_to_all[op.name].append(y.name)\n      if str(op.type) == ""Assign"":\n        for y in op.outputs:\n          for x in op.inputs:\n            assign_out_to_in[y.name].append(x.name)\n\n    assign_groups = collections.defaultdict(list)\n    for out_name in assign_out_to_in.keys():\n      name_group = assign_out_to_in[out_name]\n      for n1 in name_group:\n        assign_groups[n1].append(out_name)\n        for n2 in name_group:\n          if n1 != n2:\n            assign_groups[n1].append(n2)\n\n    seen_tensors = {}\n    stack = [x.name for x in outputs]\n    while stack:\n      name = stack.pop()\n      if name in seen_tensors:\n        continue\n      seen_tensors[name] = True\n\n      if name in output_to_op:\n        for op_name in output_to_op[name]:\n          if op_name in op_to_all:\n            for input_name in op_to_all[op_name]:\n              if input_name not in stack:\n                stack.append(input_name)\n\n      expanded_names = []\n      if name in assign_groups:\n        for assign_name in assign_groups[name]:\n          expanded_names.append(assign_name)\n\n      for expanded_name in expanded_names:\n        if expanded_name not in stack:\n          stack.append(expanded_name)\n\n    unreachable_ops = []\n    for op in graph.get_operations():\n      is_unreachable = False\n      all_names = [x.name for x in op.inputs] + [x.name for x in op.outputs]\n      for name in all_names:\n        if name not in seen_tensors:\n          is_unreachable = True\n      if is_unreachable:\n        unreachable_ops.append(op)\n    return unreachable_ops\n\n  @classmethod\n  def flatten_recursive(cls, item):\n    """"""Flattens (potentially nested) a tuple/dictionary/list to a list.""""""\n    output = []\n    if isinstance(item, list):\n      output.extend(item)\n    elif isinstance(item, tuple):\n      output.extend(list(item))\n    elif isinstance(item, dict):\n      for (_, v) in six.iteritems(item):\n        output.append(v)\n    else:\n      return [item]\n\n    flat_output = []\n    for x in output:\n      flat_output.extend(cls.flatten_recursive(x))\n    return flat_output\n\n\nif __name__ == ""__main__"":\n\n    tf.test.main()\n'"
