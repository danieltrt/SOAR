file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n\nfrom setuptools import setup, find_packages\n\ntests_require = [\n    \'hypothesis\',\n    \'mock\',\n    \'pycodestyle\',\n    \'pylint\',\n    \'pytest\',\n    \'pytest-cov\',\n    \'pytest-timeout\',\n    \'pytest-httpserver\',\n    \'moto\',\n]\ndoc_require = [\n    \'Sphinx\',\n    \'sphinx_rtd_theme\',\n    \'sphinx_autodoc_typehints\',  # Propagate mypy info into docs\n    \'sphinx-click\',\n    \'recommonmark\',\n    \'setuptools\',  # version related dependencies\n    \'setuptools_scm[toml]\',\n]\n\nextras_require = {\n    \'performance\': [\'ciso8601\', \'bottleneck\'],\n    \'interactive\': [\'matplotlib\', \'fiona\'],\n    \'distributed\': [\'distributed\', \'dask[distributed]\'],\n    \'doc\': doc_require,\n    \'replicas\': [\'paramiko\', \'sshtunnel\', \'tqdm\'],\n    \'celery\': [\'celery>=4\', \'redis\'],\n    \'s3\': [\'boto3\'],\n    \'test\': tests_require,\n    \'cf\': [\'compliance-checker>=4.0.0\'],\n}\n\nextras_require[\'dev\'] = sorted(set(sum([extras_require[k] for k in [\n    \'test\',\n    \'doc\',\n    \'replicas\',\n    \'performance\',\n    \'s3\',\n    \'distributed\',\n]], [])))\n\n# An \'all\' option, following ipython naming conventions.\nextras_require[\'all\'] = sorted(set(sum(extras_require.values(), [])))\n\nextra_plugins = dict(read=[], write=[], index=[])\n\nsetup(\n    name=\'datacube\',\n    python_requires=\'>=3.6.0\',\n\n    url=\'https://github.com/opendatacube/datacube-core\',\n    author=\'Open Data Cube\',\n    maintainer=\'Open Data Cube\',\n    maintainer_email=\'\',\n    description=\'An analysis environment for satellite and other earth observation data\',\n    long_description=open(\'README.rst\').read(),\n    long_description_content_type=\'text/x-rst\',\n    license=\'Apache License 2.0\',\n    classifiers=[\n        ""Development Status :: 4 - Beta"",\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Science/Research"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Natural Language :: English"",\n        ""Operating System :: MacOS :: MacOS X"",\n        ""Operating System :: POSIX"",\n        ""Operating System :: POSIX :: BSD"",\n        ""Operating System :: POSIX :: Linux"",\n        ""Operating System :: Microsoft :: Windows"",\n        ""Programming Language :: Python"",\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n        ""Topic :: Scientific/Engineering :: GIS"",\n        ""Topic :: Scientific/Engineering :: Information Analysis"",\n    ],\n\n    packages=find_packages(\n        exclude=(\'tests\', \'tests.*\',\n                 \'integration_tests\', \'integration_tests.*\')\n    ),\n    package_data={\n        \'\': [\'*.yaml\', \'*/*.yaml\'],\n    },\n    scripts=[\n        \'datacube_apps/scripts/pbs_helpers.sh\'\n    ],\n    install_requires=[\n        \'affine\',\n        \'pyproj>=2.5\',\n        \'shapely>=1.6.4\',\n        \'cachetools\',\n        \'click>=5.0\',\n        \'cloudpickle>=0.4\',\n        \'dask[array]\',\n        \'distributed\',\n        \'jsonschema\',\n        \'netcdf4\',\n        \'numpy\',\n        \'psycopg2\',\n        \'lark-parser>=0.6.7\',\n        \'python-dateutil\',\n        \'pyyaml\',\n        \'rasterio>=1.0.2\',  # Multi-band re-project fixed in that version\n        \'sqlalchemy\',\n        \'toolz\',\n        \'xarray>=0.9\',  # >0.9 fixes most problems with `crs` attributes being lost\n    ],\n    extras_require=extras_require,\n    tests_require=tests_require,\n\n    entry_points={\n        \'console_scripts\': [\n            \'datacube = datacube.scripts.cli_app:cli\',\n            \'datacube-search = datacube.scripts.search_tool:cli\',\n            \'datacube-stacker = datacube_apps.stacker:main\',\n            \'datacube-worker = datacube.execution.worker:main\',\n            \'datacube-fixer = datacube_apps.stacker:fixer_main\',\n            \'datacube-ncml = datacube_apps.ncml:ncml_app\',\n            \'pixeldrill = datacube_apps.pixeldrill:main [interactive]\',\n            \'movie_generator = datacube_apps.movie_generator:main\',\n            \'datacube-simple-replica = datacube_apps.simple_replica:replicate [replicas]\'\n        ],\n        \'datacube.plugins.io.read\': [\n            \'netcdf = datacube.drivers.netcdf.driver:reader_driver_init\',\n            *extra_plugins[\'read\'],\n        ],\n        \'datacube.plugins.io.write\': [\n            \'netcdf = datacube.drivers.netcdf.driver:writer_driver_init\',\n            *extra_plugins[\'write\'],\n        ],\n        \'datacube.plugins.index\': [\n            \'default = datacube.index.index:index_driver_init\',\n            *extra_plugins[\'index\'],\n        ],\n    },\n)\n'"
datacube/__init__.py,0,"b'""""""\nDatacube\n========\n\nProvides access to multi-dimensional data, with a focus on Earth observations data such as LANDSAT.\n\nTo use this module, see the `Developer Guide <http://datacube-core.readthedocs.io/en/stable/dev/developer.html>`_.\n\nThe main class to access the datacube is :class:`datacube.Datacube`.\n\nTo initialise this class, you will need a config pointing to a database, such as a file with the following::\n\n    [datacube]\n    db_hostname: 130.56.244.227\n    db_database: democube\n    db_username: cube_user\n\n""""""\n\ntry:\n    from ._version import version as __version__\nexcept ImportError:\n    __version__ = \'Unknown/Not Installed\'\n\nfrom .api import Datacube\nimport warnings\nfrom .utils import xarray_geoextensions\n\n# Ensure deprecation warnings from datacube modules are shown\nwarnings.filterwarnings(\'always\', category=DeprecationWarning, module=r\'^datacube\\.\')\n\n__all__ = (\n    ""Datacube"",\n    ""__version__"",\n    ""xarray_geoextensions"",\n)\n'"
datacube/__main__.py,0,"b'if __name__ == ""__main__"":\n    from .config import auto_config\n    auto_config()\n'"
datacube/_celery_runner.py,0,"b'\nimport cloudpickle\nfrom celery import Celery\nfrom time import sleep\nimport redis\nimport os\nimport kombu.serialization\n\nfrom celery.backends import base as celery_base\n\n# This can be changed via environment variable `REDIS`\nREDIS_URL = \'redis://localhost:6379/0\'\n\nkombu.serialization.registry.register(\n    \'cloudpickle\',\n    cloudpickle.dumps, cloudpickle.loads,\n    content_type=\'application/x-python-cloudpickle\',\n    content_encoding=\'binary\'\n)\n\n# Tell celery that it\'s ok to serialise exceptions using cloudpickle.\ncelery_base.EXCEPTION_ABLE_CODECS = celery_base.EXCEPTION_ABLE_CODECS.union({\'cloudpickle\'})\n\n\ndef mk_celery_app(addr=None):\n    if addr is None:\n        url = os.environ.get(\'REDIS\', REDIS_URL)\n    else:\n        url = \'redis://{}:{}/0\'.format(*addr)\n\n    _app = Celery(\'datacube_task\', broker=url, backend=url)\n\n    _app.conf.update(\n        task_serializer=\'cloudpickle\',\n        result_serializer=\'cloudpickle\',\n        event_serializer=\'cloudpickle\',\n        accept_content=[\'cloudpickle\', \'json\', \'pickle\']\n    )\n\n    return _app\n\n\n# Celery worker launch script expects to see app object at the top level\n# pylint: disable=invalid-name\napp = mk_celery_app()\n\n\ndef set_address(host, port=6379, db=0, password=None):\n    if password is None:\n        url = \'redis://{}:{}/{}\'.format(host, port, db)\n    else:\n        url = \'redis://:{}@{}:{}/{}\'.format(password, host, port, db)\n\n    app.conf.update(result_backend=url,\n                    broker_url=url)\n\n\n@app.task()\ndef run_function(func, *args, **kwargs):\n    return func(*args, **kwargs)\n\n\ndef launch_worker(host, port=6379, password=None, nprocs=None):\n    if password == \'\':\n        password = get_redis_password(generate_if_missing=False)\n\n    set_address(host, port, password=password)\n\n    argv = [\'worker\', \'-A\', \'datacube._celery_runner\', \'-E\', \'-l\', \'INFO\']\n    if nprocs is not None:\n        argv.extend([\'-c\', str(nprocs)])\n\n    app.worker_main(argv)\n\n\ndef get_redis_password(generate_if_missing=False):\n    from .utils import write_user_secret_file, slurp, gen_password\n\n    REDIS_PASSWORD_FILE = \'.datacube-redis\'\n\n    password = slurp(REDIS_PASSWORD_FILE, in_home_dir=True)\n    if password is not None:\n        return password\n\n    if generate_if_missing:\n        password = gen_password(12)\n        write_user_secret_file(password, REDIS_PASSWORD_FILE, in_home_dir=True)\n\n    return password\n\n\nclass CeleryExecutor(object):\n    def __init__(self, host=None, port=None, password=None):\n        # print(\'Celery: {}:{}\'.format(host, port))\n        self._shutdown = None\n\n        if port or host or password:\n            if password == \'\':\n                password = get_redis_password(generate_if_missing=True)\n\n            set_address(host if host else \'localhost\',\n                        port if port else 6379,\n                        password=password)\n\n        host = host if host else \'localhost\'\n        port = port if port else 6379\n\n        if not check_redis(host, port, password):\n            if host in [\'localhost\', \'127.0.0.1\']:\n                self._shutdown = launch_redis(port if port else 6379, password=password)\n            else:\n                raise IOError(""Can\'t connect to redis server @ {}:{}"".format(host, port))\n\n    def __del__(self):\n        if self._shutdown:\n            app.control.shutdown()\n            sleep(1)\n            self._shutdown()\n\n    def __repr__(self):\n        return \'CeleryRunner\'\n\n    def submit(self, func, *args, **kwargs):\n        return run_function.delay(func, *args, **kwargs)\n\n    def map(self, func, iterable):\n        return [self.submit(func, data) for data in iterable]\n\n    @staticmethod\n    def get_ready(futures):\n        completed = []\n        failed = []\n        pending = []\n        for f in futures:\n            if f.ready():\n                if f.failed():\n                    failed.append(f)\n                else:\n                    completed.append(f)\n            else:\n                pending.append(f)\n        return completed, failed, pending\n\n    @staticmethod\n    def as_completed(futures):\n        while len(futures) > 0:\n            pending = []\n\n            for promise in futures:\n                if promise.ready():\n                    yield promise\n                else:\n                    pending.append(promise)\n\n            if len(pending) == len(futures):\n                # If no change detected sleep for a bit\n                # TODO: this is sub-optimal, not sure what other options are\n                #       though?\n                sleep(0.1)\n\n            futures = pending\n\n    @classmethod\n    def next_completed(cls, futures, default):\n        results = list(futures)\n        if not results:\n            return default, results\n        result = next(cls.as_completed(results), default)\n        results.remove(result)\n        return result, results\n\n    @staticmethod\n    def results(futures):\n        return [future.get() for future in futures]\n\n    @staticmethod\n    def result(future):\n        return future.get()\n\n    @staticmethod\n    def release(future):\n        future.forget()\n\n\ndef check_redis(host=\'localhost\', port=6379, password=None):\n    if password == \'\':\n        password = get_redis_password()\n\n    server = redis.Redis(host, port, password=password)\n    try:\n        server.ping()\n    except redis.exceptions.ConnectionError:\n        return False\n    except redis.exceptions.ResponseError as error:\n        print(\'Redis responded with an error: {}\'.format(error))\n        return False\n    return True\n\n\ndef launch_redis(port=6379, password=None, **kwargs):\n    import tempfile\n    from os import path\n    import subprocess\n    import shutil\n    from .utils import write_user_secret_file\n\n    def stringify(v):\n        if isinstance(v, str):\n            return \'""\' + v + \'""\' if v.find(\' \') >= 0 else v\n\n        if isinstance(v, bool):\n            return {True: \'yes\', False: \'no\'}[v]\n\n        return str(v)\n\n    def fix_key(k):\n        return k.replace(\'_\', \'-\')\n\n    def write_config(params, cfgfile):\n        lines = [\'{} {}\'.format(fix_key(k), stringify(v)) for k, v in params.items()]\n        cfg_txt = \'\\n\'.join(lines)\n        write_user_secret_file(cfg_txt, cfgfile)\n\n    workdir = tempfile.mkdtemp(prefix=\'redis-\')\n\n    defaults = dict(maxmemory_policy=\'noeviction\',\n                    daemonize=True,\n                    port=port,\n                    databases=4,\n                    maxmemory=""100mb"",\n                    hz=50,\n                    loglevel=\'notice\',\n                    pidfile=path.join(workdir, \'redis.pid\'),\n                    logfile=path.join(workdir, \'redis.log\'))\n\n    if password is not None:\n        if password == \'\':\n            password = get_redis_password(generate_if_missing=True)\n\n        defaults[\'requirepass\'] = password\n    else:\n        password = defaults.get(\'requirepass\', None)\n\n    defaults.update(kwargs)\n\n    cfgfile = path.join(workdir, \'redis.cfg\')\n    write_config(defaults, cfgfile)\n\n    def cleanup():\n        shutil.rmtree(workdir)\n\n    def shutdown():\n        server = redis.Redis(\'localhost\', port, password=password)\n        server.shutdown()\n        sleep(1)\n        cleanup()\n\n    try:\n        subprocess.check_call([\'redis-server\', cfgfile])\n    except subprocess.CalledProcessError:\n        cleanup()\n        return False\n\n    return shutdown\n'"
datacube/config.py,0,"b'# coding=utf-8\n""""""\nUser configuration.\n""""""\n\nimport os\nfrom pathlib import Path\nimport configparser\nfrom urllib.parse import unquote_plus, urlparse\nfrom typing import Optional, Iterable, Union, Any, Tuple, Dict\n\nPathLike = Union[str, \'os.PathLike[Any]\']\n\n\nENVIRONMENT_VARNAME = \'DATACUBE_CONFIG_PATH\'\n#: Config locations in order. Properties found in latter locations override\n#: earlier ones.\n#:\n#: - `/etc/datacube.conf`\n#: - file at `$DATACUBE_CONFIG_PATH` environment variable\n#: - `~/.datacube.conf`\n#: - `datacube.conf`\nDEFAULT_CONF_PATHS = tuple(p for p in [\'/etc/datacube.conf\',\n                                       os.environ.get(ENVIRONMENT_VARNAME, \'\'),\n                                       str(os.path.expanduser(""~/.datacube.conf"")),\n                                       \'datacube.conf\'] if len(p) > 0)\n\nDEFAULT_ENV = \'default\'\n\n# Default configuration options.\n_DEFAULT_CONF = """"""\n[DEFAULT]\n# Blank implies localhost\ndb_hostname:\ndb_database: datacube\nindex_driver: default\n# If a connection is unused for this length of time, expect it to be invalidated.\ndb_connection_timeout: 60\n\n[user]\n# Which environment to use when none is specified explicitly.\n#   note: will fail if default_environment points to non-existent section\n# default_environment: datacube\n""""""\n\n#: Used in place of None as a default, when None is a valid but not default parameter to a function\n_UNSET = object()\n\n\ndef read_config(default_text: Optional[str] = None) -> configparser.ConfigParser:\n    config = configparser.ConfigParser()\n    if default_text is not None:\n        config.read_string(default_text)\n    return config\n\n\nclass LocalConfig(object):\n    """"""\n    System configuration for the user.\n\n    This loads from a set of possible configuration files which define the available environments.\n    An environment contains connection details for a Data Cube Index, which provides access to\n    available data.\n\n    """"""\n\n    def __init__(self, config: configparser.ConfigParser,\n                 files_loaded: Optional[Iterable[str]] = None,\n                 env: Optional[str] = None):\n        """"""\n        Datacube environment resolution precedence is:\n          1. Supplied as a function argument `env`\n          2. DATACUBE_ENVIRONMENT environment variable\n          3. user.default_environment option in the config\n          4. \'default\' or \'datacube\' whichever is present\n\n        If environment is supplied by any of the first 3 methods is not present\n        in the config, then throw an exception.\n        """"""\n        self._config = config\n        self.files_loaded = [] if files_loaded is None else list(iter(files_loaded))\n\n        if env is None:\n            env = os.environ.get(\'DATACUBE_ENVIRONMENT\',\n                                 config.get(\'user\', \'default_environment\', fallback=None))\n\n        # If the user specifies a particular env, we either want to use it or Fail\n        if env:\n            if config.has_section(env):\n                self._env = env\n                # All is good\n                return\n            else:\n                raise ValueError(\'No config section found for environment %r\' % (env,))\n        else:\n            # If an env hasn\'t been specifically selected, we can fall back defaults\n            fallbacks = [DEFAULT_ENV, \'datacube\']\n            for fallback_env in fallbacks:\n                if config.has_section(fallback_env):\n                    self._env = fallback_env\n                    return\n            raise ValueError(\'No ODC environment, checked configurations for %s\' % fallbacks)\n\n    @classmethod\n    def find(cls,\n             paths: Optional[Union[str, Iterable[PathLike]]] = None,\n             env: Optional[str] = None) -> \'LocalConfig\':\n        """"""\n        Find config from environment variables or possible filesystem locations.\n\n        \'env\' is which environment to use from the config: it corresponds to the name of a\n        config section\n        """"""\n        config = read_config(_DEFAULT_CONF)\n\n        if paths is None:\n            if env is None:\n                env_opts = parse_env_params()\n                if env_opts:\n                    return _cfg_from_env_opts(env_opts, config)\n\n            paths = DEFAULT_CONF_PATHS\n\n        if isinstance(paths, str) or hasattr(paths, \'__fspath__\'):  # Use os.PathLike in 3.6+\n            paths = [str(paths)]\n\n        files_loaded = config.read(str(p) for p in paths if p)\n\n        return LocalConfig(\n            config,\n            files_loaded=files_loaded,\n            env=env,\n        )\n\n    def get(self, item: str, fallback=_UNSET):\n        if fallback is _UNSET:\n            return self._config.get(self._env, item)\n        else:\n            return self._config.get(self._env, item, fallback=fallback)\n\n    def __getitem__(self, item: str):\n        return self.get(item, fallback=None)\n\n    def __str__(self) -> str:\n        cfg = dict(self._config[self._env])\n        if \'db_password\' in cfg:\n            cfg[\'db_password\'] = \'***\'\n\n        return ""LocalConfig<loaded_from={}, environment={!r}, config={}>"".format(\n            self.files_loaded or \'defaults\',\n            self._env,\n            cfg)\n\n    def __repr__(self) -> str:\n        return str(self)\n\n\nDB_KEYS = (\'hostname\', \'port\', \'database\', \'username\', \'password\')\n\n\ndef parse_connect_url(url: str) -> Dict[str, str]:\n    """""" Extract database,hostname,port,username,password from db URL.\n\n    Example: postgresql://username:password@hostname:port/database\n\n    For local password-less db use `postgresql:///<your db>`\n    """"""\n    def split2(s: str, separator: str) -> Tuple[str, str]:\n        i = s.find(separator)\n        return (s, \'\') if i < 0 else (s[:i], s[i+1:])\n\n    _, netloc, path, *_ = urlparse(url)\n\n    db = path[1:] if path else \'\'\n    if \'@\' in netloc:\n        (user, password), (host, port) = (split2(p, \':\') for p in split2(netloc, \'@\'))\n    else:\n        user, password = \'\', \'\'\n        host, port = split2(netloc, \':\')\n\n    oo = dict(hostname=host, database=db)\n\n    if port:\n        oo[\'port\'] = port\n    if password:\n        oo[\'password\'] = unquote_plus(password)\n    if user:\n        oo[\'username\'] = user\n    return oo\n\n\ndef parse_env_params() -> Dict[str, str]:\n    """"""\n    - Extract parameters from DATACUBE_DB_URL if present\n    - Else look for DB_HOSTNAME, DB_USERNAME, DB_PASSWORD, DB_DATABASE\n    - Return {} otherwise\n    """"""\n\n    db_url = os.environ.get(\'DATACUBE_DB_URL\', None)\n    if db_url is not None:\n        return parse_connect_url(db_url)\n\n    params = {k: os.environ.get(\'DB_{}\'.format(k.upper()), None)\n              for k in DB_KEYS}\n    return {k: v\n            for k, v in params.items()\n            if v is not None and v != """"}\n\n\ndef _cfg_from_env_opts(opts: Dict[str, str],\n                       base: configparser.ConfigParser) -> LocalConfig:\n    base[\'default\'] = {\'db_\'+k: v for k, v in opts.items()}\n    return LocalConfig(base, files_loaded=[], env=\'default\')\n\n\ndef render_dc_config(params: Dict[str, Any],\n                     section_name: str = \'default\') -> str:\n    """""" Render output of parse_env_params to a string that can be written to config file.\n    """"""\n    oo = \'[{}]\\n\'.format(section_name)\n    for k in DB_KEYS:\n        v = params.get(k, None)\n        if v is not None:\n            oo += \'db_{k}: {v}\\n\'.format(k=k, v=v)\n    return oo\n\n\ndef auto_config() -> str:\n    """"""\n    Render config to $DATACUBE_CONFIG_PATH or ~/.datacube.conf, but only if doesn\'t exist.\n\n    option1:\n      DATACUBE_DB_URL  postgresql://user:password@host:port/database\n\n    option2:\n      DB_{HOSTNAME|PORT|USERNAME|PASSWORD|DATABASE}\n\n    option3:\n       default config\n    """"""\n    cfg_path = os.environ.get(\'DATACUBE_CONFIG_PATH\', None)\n    cfg_path = Path(cfg_path) if cfg_path else Path.home()/\'.datacube.conf\'\n\n    if cfg_path.exists():\n        return str(cfg_path)\n\n    opts = parse_env_params()\n\n    if len(opts) == 0:\n        opts[\'hostname\'] = \'\'\n        opts[\'database\'] = \'datacube\'\n\n    cfg_text = render_dc_config(opts)\n    with open(str(cfg_path), \'wt\') as f:\n        f.write(cfg_text)\n\n    return str(cfg_path)\n'"
datacube/executor.py,0,"b'#\n#    Licensed under the Apache License, Version 2.0 (the ""License"");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an ""AS IS"" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\nimport sys\n\n_REMOTE_LOG_FORMAT_STRING = \'%(asctime)s {} %(process)d %(name)s %(levelname)s %(message)s\'\n\n\nclass SerialExecutor(object):\n    def __repr__(self):\n        return \'SerialExecutor\'\n\n    @staticmethod\n    def submit(func, *args, **kwargs):\n        return func, args, kwargs\n\n    @staticmethod\n    def map(func, iterable):\n        return [SerialExecutor.submit(func, data) for data in iterable]\n\n    @staticmethod\n    def get_ready(futures):\n        def reraise(t, e, traceback):\n            raise t.with_traceback(e, traceback)\n\n        try:\n            result = SerialExecutor.result(futures[0])\n            return [(lambda x: x, [result], {})], [], futures[1:]\n        except Exception:  # pylint: disable=broad-except\n            exc_info = sys.exc_info()\n            return [], [(reraise, exc_info, {})], futures[1:]\n\n    @staticmethod\n    def as_completed(futures):\n        for future in futures:\n            yield future\n\n    @classmethod\n    def next_completed(cls, futures, default):\n        results = list(futures)\n        if not results:\n            return default, results\n        result = next(cls.as_completed(results), default)\n        results.remove(result)\n        return result, results\n\n    @staticmethod\n    def results(futures):\n        return [SerialExecutor.result(future) for future in futures]\n\n    @staticmethod\n    def result(future):\n        func, args, kwargs = future\n        return func(*args, **kwargs)\n\n    @staticmethod\n    def release(future):\n        pass\n\n\ndef setup_logging():\n    import logging\n    import socket\n\n    hostname = socket.gethostname()\n    log_format_string = _REMOTE_LOG_FORMAT_STRING.format(hostname)\n\n    handler = logging.StreamHandler()\n    handler.formatter = logging.Formatter(log_format_string)\n    logging.root.handlers = [handler]\n\n\ndef _get_distributed_executor(scheduler):\n    """"""\n    :param scheduler: Address of a scheduler\n    """"""\n    try:\n        import distributed\n    except ImportError:\n        return None\n\n    class DistributedExecutor(object):\n        def __init__(self, executor):\n            """"""\n            :type executor: distributed.Executor\n            :return:\n            """"""\n            self._executor = executor\n            self.setup_logging()\n\n        def setup_logging(self):\n            self._executor.run(setup_logging)\n\n        def submit(self, func, *args, **kwargs):\n            return self._executor.submit(func, *args, pure=False, **kwargs)\n\n        def map(self, func, iterable):\n            return self._executor.map(func, iterable)\n\n        @staticmethod\n        def get_ready(futures):\n            groups = {}\n            for f in futures:\n                groups.setdefault(f.status, []).append(f)\n            return groups.get(\'finished\', []), groups.get(\'error\', []), groups.get(\'pending\', [])\n\n        @staticmethod\n        def as_completed(futures):\n            return distributed.as_completed(futures)\n\n        @classmethod\n        def next_completed(cls, futures, default):\n            results = list(futures)\n            if not results:\n                return default, results\n            result = next(cls.as_completed(results), default)\n            results.remove(result)\n            return result, results\n\n        def results(self, futures):\n            return self._executor.gather(futures)\n\n        @staticmethod\n        def result(future):\n            return future.result()\n\n        @staticmethod\n        def release(future):\n            future.release()\n\n    try:\n        executor = DistributedExecutor(distributed.Client(scheduler))\n        return executor\n    except IOError:\n        return None\n\n\ndef _run_cloud_pickled_function(f_data, *args, **kwargs):\n    from cloudpickle import loads\n    func = loads(f_data)\n    return func(*args, **kwargs)\n\n\ndef _get_concurrent_executor(workers, use_cloud_pickle=False):\n    try:\n        from concurrent.futures import ProcessPoolExecutor, as_completed\n    except ImportError:\n        return None\n\n    def mk_submitter(pool, use_cloud_pickle):\n        def submit_direct(func, *args, **kwargs):\n            return pool.submit(func, *args, **kwargs)\n\n        def submit_cloud_pickle(func, *args, **kwargs):\n            from cloudpickle import dumps\n            return pool.submit(_run_cloud_pickled_function, dumps(func), *args, **kwargs)\n\n        return submit_cloud_pickle if use_cloud_pickle else submit_direct\n\n    class MultiprocessingExecutor(object):\n        def __init__(self, pool, use_cloud_pickle):\n            self._pool = pool\n            self._submitter = mk_submitter(pool, use_cloud_pickle)\n\n        def __repr__(self):\n            max_workers = self._pool.__dict__.get(\'_max_workers\', \'??\')\n            return \'Multiprocessing ({})\'.format(max_workers)\n\n        def submit(self, func, *args, **kwargs):\n            return self._submitter(func, *args, **kwargs)\n\n        def map(self, func, iterable):\n            return [self.submit(func, data) for data in iterable]\n\n        @staticmethod\n        def get_ready(futures):\n            completed = []\n            failed = []\n            pending = []\n            for f in futures:\n                if f.done():\n                    if f.exception():\n                        failed.append(f)\n                    else:\n                        completed.append(f)\n                else:\n                    pending.append(f)\n            return completed, failed, pending\n\n        @staticmethod\n        def as_completed(futures):\n            return as_completed(futures)\n\n        @classmethod\n        def next_completed(cls, futures, default):\n            results = list(futures)\n            if not results:\n                return default, results\n            result = next(cls.as_completed(results), default)\n            results.remove(result)\n            return result, results\n\n        @staticmethod\n        def results(futures):\n            return [future.result() for future in futures]\n\n        @staticmethod\n        def result(future):\n            return future.result()\n\n        @staticmethod\n        def release(future):\n            pass\n\n    if workers <= 0:\n        return None\n\n    return MultiprocessingExecutor(ProcessPoolExecutor(workers), use_cloud_pickle)\n\n\ndef get_executor(scheduler, workers, use_cloud_pickle=True):\n    """"""\n    Return a task executor based on input parameters. Falling back as required.\n\n    :param scheduler: IP address and port of a distributed.Scheduler, or a Scheduler instance\n    :param workers: Number of processes to start for process based parallel execution\n    :param use_cloud_pickle: Only applies when scheduler is None and workers > 0, default is True\n    """"""\n    if not workers:\n        return SerialExecutor()\n\n    if scheduler:\n        distributed_exec = _get_distributed_executor(scheduler)\n        if distributed_exec:\n            return distributed_exec\n\n    concurrent_exec = _get_concurrent_executor(workers, use_cloud_pickle=use_cloud_pickle)\n    if concurrent_exec:\n        return concurrent_exec\n\n    return SerialExecutor()\n\n\ndef mk_celery_executor(host, port, password=\'\'):\n    """"""\n    :param host: Address of the redis database server\n    :param port: Port of the redis database server\n    :password: Authentication for redis or None or \'\'\n               \'\' -- load from home folder, or generate if missing,\n               None -- no authentication\n    """"""\n    from ._celery_runner import CeleryExecutor\n    return CeleryExecutor(host, port, password=password)\n'"
datacube/helpers.py,2,"b'""""""\nUseful functions for Datacube users\n\nNot used internally, those should go in `utils.py`\n""""""\n\nimport numpy as np\nimport rasterio\nimport warnings\n\nDEFAULT_PROFILE = {\n    \'blockxsize\': 256,\n    \'blockysize\': 256,\n    \'compress\': \'lzw\',\n    \'driver\': \'GTiff\',\n    \'interleave\': \'band\',\n    \'nodata\': 0.0,\n    \'tiled\': True}\n\n\ndef write_geotiff(filename, dataset, profile_override=None):\n    """"""\n    DEPRECATED: use datacube.utils.cog.write_cog instead.\n\n    Write an ODC style xarray.Dataset to a GeoTIFF file.\n\n    :param filename: Output filename\n    :param dataset: xarray dataset containing one or more bands to write to a file.\n    :param profile_override: option dict, overrides rasterio file creation options.\n    :param time_index: DEPRECATED\n    """"""\n    warnings.warn(""""""Function datacube.helpers.write_geotiff is deprecated,\nplease use datacube.utils.cog.write_cog instead"""""",\n                  category=DeprecationWarning)\n\n    profile_override = profile_override or {}\n\n    geobox = getattr(dataset, \'geobox\', None)\n\n    if geobox is None:\n        raise ValueError(\'Can only write datasets with specified `crs` attribute\')\n\n    try:\n        dtypes = {val.dtype for val in dataset.data_vars.values()}\n        assert len(dtypes) == 1  # Check for multiple dtypes\n    except AttributeError:\n        dtypes = [dataset.dtype]\n\n    profile = DEFAULT_PROFILE.copy()\n    height, width = geobox.shape\n\n    profile.update({\n        \'width\': width,\n        \'height\': height,\n        \'transform\': geobox.affine,\n        \'crs\': str(geobox.crs),\n        \'count\': len(dataset.data_vars),\n        \'dtype\': str(dtypes.pop())\n    })\n    profile.update(profile_override)\n\n    _calculate_blocksize(profile)\n\n    with rasterio.open(str(filename), \'w\', **profile) as dest:\n        if hasattr(dataset, \'data_vars\'):\n            for bandnum, data in enumerate(dataset.data_vars.values(), start=1):\n                dest.write(data.data, bandnum)\n\n\ndef _calculate_blocksize(profile):\n    # Block size must be smaller than the image size, and for geotiffs must be divisible by 16\n    # Fix for small images.\n    if profile[\'blockxsize\'] > profile[\'width\']:\n        if profile[\'width\'] % 16 == 0 or profile[\'width\'] < 16:\n            profile[\'blockxsize\'] = profile[\'width\']\n        else:\n            profile[\'blockxsize\'] = 16\n\n    if profile[\'blockysize\'] > profile[\'height\']:\n        if profile[\'height\'] % 16 == 0 or profile[\'height\'] < 16:\n            profile[\'blockysize\'] = profile[\'height\']\n        else:\n            profile[\'blockysize\'] = 16\n\n\ndef ga_pq_fuser(dest, src):\n    """"""\n    Fuse two Geoscience Australia Pixel Quality ndarrays\n\n    To be used as a `fuse_func` when loaded `grouped` data, for example when grouping\n    by solar day to avoid duplicate data from scene overlaps.\n    """"""\n    valid_bit = 8\n    valid_val = (1 << valid_bit)\n\n    no_data_dest_mask = ~(dest & valid_val).astype(bool)\n    np.copyto(dest, src, where=no_data_dest_mask)\n\n    both_data_mask = (valid_val & dest & src).astype(bool)\n    np.copyto(dest, src & dest, where=both_data_mask)\n'"
datacube_apps/__init__.py,0,b''
datacube_apps/movie_generator.py,2,"b'""""""\nThis app creates time series movies\n\n\n""""""\n\n\nimport click\n\nimport fiona\nimport xarray as xr\nimport numpy as np\nimport rasterio\nimport subprocess\nfrom glob import glob\nfrom dateutil.parser import parse\nfrom datetime import datetime, timedelta, time, date\n\nfrom datacube.utils.masking import make_mask\nfrom datacube.ui import click as ui\nfrom datacube import Datacube\nfrom datacube.utils.dates import date_sequence\nfrom datacube.helpers import ga_pq_fuser\n\nDEFAULT_MEASUREMENTS = [\'red\', \'green\', \'blue\']\n\nDEFAULT_PRODUCTS = [\'ls5_nbar_albers\', \'ls7_nbar_albers\', \'ls8_nbar_albers\']\nDEFAULT_CRS = \'EPSG:3577\'\nFFMPEG_PATH = \'ffmpeg\'\nVALID_BIT = 8  # GA Landsat PQ Contiguity Bit\n\nSUBTITLE_FORMAT = \'%d %B %Y\'\nSRT_TIMEFMT = \'%H:%M:%S,%f\'\nSRT_FORMAT = """"""\n{i}\n{start} --> {end}\n{txt}""""""\n\n\ndef to_datetime(ctx, param, value):\n    if value:\n        return parse(value)\n    else:\n        return None\n\n\n# pylint: disable=too-many-arguments, too-many-locals\n@click.command(name=\'moviemaker\')\n@click.option(\'--load-bounds-from\', type=click.Path(exists=True, readable=True, dir_okay=False),\n              help=\'Shapefile to calculate boundary coordinates from.\')\n@click.option(\'--start-date\', callback=to_datetime, help=\'YYYY-MM-DD\')\n@click.option(\'--end-date\', callback=to_datetime, help=\'YYYY-MM-DD\')\n@click.option(\'--stats-duration\', default=\'1y\', help=\'eg. 1y, 3m\')\n@click.option(\'--step-size\', default=\'1y\', help=\'eg. 1y, 3m\')\n@click.option(\'--bounds\', nargs=4, help=\'LEFT, BOTTOM, RIGHT, TOP\')\n@click.option(\'--base-output-name\', default=\'output\', help=""Base name to use for images and video. Eg.  ""\n                                                           ""--base-output-name stromlo will produce ""\n                                                           ""stromlo_001_*.png and stromlo.mp4"")\n@click.option(\'--time-incr\', default=2, help=\'Time to display each image, in seconds\')\n@click.option(\'--product\', multiple=True, default=DEFAULT_PRODUCTS)\n@click.option(\'--measurement\', \'-m\', multiple=True, default=DEFAULT_MEASUREMENTS)\n@click.option(\'--ffmpeg-path\', default=FFMPEG_PATH, help=\'Path to ffmpeg executable\')\n@click.option(\'--crs\', default=DEFAULT_CRS, help=\'Used if specifying --bounds. eg. EPSG:3577. \')\n@ui.global_cli_options\n@ui.executor_cli_options\ndef main(bounds, base_output_name, load_bounds_from, start_date, end_date, product, measurement, executor,\n         step_size, stats_duration, time_incr, ffmpeg_path, crs):\n    """"""\n    Create an mp4 movie file based on datacube data\n\n    Use only clear pixels, and mosaic over time to produce full frames.\n\n    Can combine products, specify multiple --product\n\n    """"""\n    if load_bounds_from:\n        crs, (left, bottom, right, top) = bounds_from_file(load_bounds_from)\n    elif bounds:\n        left, bottom, right, top = bounds\n    else:\n        raise click.UsageError(\'Must specify one of --load-bounds-from or --bounds\')\n\n    tasks = []\n    for filenum, date_range in enumerate(date_sequence(start_date, end_date, stats_duration, step_size), start=1):\n        filename = ""{}_{:03d}_{:%Y-%m-%d}.png"".format(base_output_name, filenum, start_date)\n        task = dict(filename=filename, products=product, time=date_range, x=(left, right),\n                    y=(top, bottom), crs=crs, measurements=measurement)\n        tasks.append(task)\n\n    results = []\n    for task in tasks:\n        result_future = executor.submit(write_mosaic_to_file, **task)\n        results.append(result_future)\n\n    filenames = []\n    for result in executor.as_completed(results):\n        filenames.append(executor.result(result))\n\n    # Write subtitle file\n    subtitle_filename = ""{}.srt"".format(base_output_name)\n    write_subtitle_file(tasks, subtitle_filename=subtitle_filename, display_format=SUBTITLE_FORMAT,\n                        time_incr=time_incr)\n\n    # Write video file\n    filenames_pattern = \'%s*.png\' % base_output_name\n    video_filename = ""{}.mp4"".format(base_output_name)\n    write_video_file(filenames_pattern, video_filename, subtitle_filename, time_incr=time_incr, ffmpeg_path=ffmpeg_path)\n\n    click.echo(""Finished!"")\n\n\ndef bounds_from_file(filename):\n    with fiona.open(filename) as c:\n        return c.crs_wkt, c.bounds\n\n\ndef write_mosaic_to_file(filename, **expression):\n    image_data = compute_mosaic(**expression)\n    write_xarray_to_image(filename, image_data)\n    click.echo(\'Wrote {}.\'.format(filename))\n    return filename\n\n\ndef compute_mosaic(products, measurements, **parsed_expressions):\n    with Datacube() as dc:\n        acq_range = parsed_expressions[\'time\']\n        click.echo(""Processing time range {}"".format(acq_range))\n        datasets = []\n\n        for prodname in products:\n            dataset = dc.load(product=prodname,\n                              measurements=measurements,\n                              group_by=\'solar_day\',\n                              **parsed_expressions)\n            if len(dataset) == 0:\n                continue\n            else:\n                click.echo(""Found {} time slices of {} during {}."".format(len(dataset[\'time\']), prodname, acq_range))\n\n            pq = dc.load(product=prodname.replace(\'nbar\', \'pq\'),\n                         group_by=\'solar_day\',\n                         fuse_func=ga_pq_fuser,\n                         **parsed_expressions)\n\n            if len(pq) == 0:\n                click.echo(\'No PQ found, skipping\')\n                continue\n\n            crs = dataset.attrs[\'crs\']\n            dataset = dataset.where(dataset != -999)\n            dataset.attrs[\'product\'] = prodname\n            dataset.attrs[\'crs\'] = crs\n\n            cloud_free = make_mask(pq.pixelquality, ga_good_pixel=True)\n            dataset = dataset.where(cloud_free)\n\n            if len(dataset) == 0:\n                click.echo(""Nothing left after PQ masking"")\n                continue\n\n            datasets.append(dataset)\n\n    dataset = xr.concat(datasets, dim=\'time\')\n\n    return dataset.median(dim=\'time\')\n\n\ndef write_xarray_to_image(filename, dataset, dtype=\'uint16\'):\n    img = np.stack([dataset[colour].data for colour in DEFAULT_MEASUREMENTS])\n\n    maxvalue = 3000\n    nmask = np.isnan(img).any(axis=0)\n\n    mask = (img > maxvalue).any(axis=0)\n    img /= maxvalue\n    img[..., mask] = 1.0\n    img[..., nmask] = 1.0\n\n    img *= 2 ** 16\n\n    profile = {\n        \'driver\': \'PNG\',\n        \'height\': len(dataset[\'y\']),\n        \'width\': len(dataset[\'x\']),\n        \'count\': 3,\n        \'dtype\': dtype\n    }\n    click.echo(""Writing file: {}"".format(filename))\n    with rasterio.open(filename, \'w\', **profile) as dst:\n        dst.write(img.astype(dtype))\n\n\ndef write_subtitle_file(tasks, subtitle_filename, display_format, time_incr):\n    if time_incr < 1.0:\n        incr = timedelta(microseconds=time_incr * 1000000)\n    else:\n        incr = timedelta(seconds=time_incr)\n\n    with open(subtitle_filename, mode=\'w\') as output:\n        start_time_vid = time(0, 0, 0, 0)\n        for i, task in enumerate(tasks):\n            end_time_vid = (datetime.combine(date.today(), start_time_vid) + incr).time()\n\n            start_time_actual, _ = task[\'time\']\n\n            txt = start_time_actual.strftime(display_format)\n\n            output.write(\n                SRT_FORMAT.format(i=i, txt=txt,\n                                  start=start_time_vid.strftime(SRT_TIMEFMT)[:-3],\n                                  end=end_time_vid.strftime(SRT_TIMEFMT)[:-3]))\n            start_time_vid = end_time_vid\n\n\ndef write_video_file(filename_pattern, video_filename, subtitle_filename, time_incr, ffmpeg_path):\n    resize_images(filename_pattern)\n\n    # Run ffmpeg\n    movie_cmd = [ffmpeg_path, \'-framerate\', \'1/%s\' % time_incr, \'-pattern_type\', \'glob\',\n                 \'-i\', filename_pattern, \'-c:v\', \'libx264\', \'-pix_fmt\', \'yuv420p\', \'-r\', \'30\',\n                 \'-vf\', ""subtitles=\'%s\':force_style=\'FontName=DejaVu Sans\'"" % subtitle_filename, video_filename]\n\n    subprocess.check_call(movie_cmd)\n\n\ndef resize_images(filename_pattern):\n    """"""\n    Resize images files in place to a safe size for movie generation\n\n    - Maximum height of 1080\n    - Ensure dimensions are divisible by 2.\n\n    Uses the ImageMagick mogrify command.\n    """"""\n    sample_file = glob(filename_pattern)[0]\n    width, height = subprocess.check_output([\'identify\', sample_file]).decode(\'ascii\').split()[2].split(\'x\')\n    x, y = int(width), int(height)\n    if y > 1080:\n        scale = y / 1080\n        y = 1080\n        x = int(x / scale)\n    x = x + 1 if x % 2 == 1 else x\n    y = y + 1 if y % 2 == 1 else y\n    newdims = \'%sx%s!\' % (x, y)\n    resize_cmd = [\'mogrify\', \'-geometry\', newdims, filename_pattern]\n    subprocess.check_call(resize_cmd)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
datacube_apps/ncml.py,0,"b'""""""\nCreate statistical summaries command\n\n""""""\n\nimport logging\nimport os\nfrom datetime import datetime\nfrom functools import partial\nfrom itertools import groupby\n\nimport click\nfrom dateutil import tz\nfrom pathlib import Path\n\nimport datacube\nfrom datacube.ui import task_app\n\n_LOG = logging.getLogger(__name__)\n\nAPP_NAME = \'datacube-ncml\'\n\n\ndef get_filename(config, cell_index, year=None):\n    if year is None:\n        file_path_template = str(Path(config[\'location\'], config[\'ncml_path_template\']))\n    else:\n        file_path_template = str(Path(config[\'location\'], config[\'partial_ncml_path_template\']))\n    return file_path_template.format(tile_index=cell_index, start_time=year)\n\n\ndef make_ncml_tasks(index, config, cell_index=None, year=None, cell_index_list=None, **kwargs):\n    product = config[\'product\']\n\n    query = {}\n    if year is not None:\n        query[\'time\'] = datetime(year=year, month=1, day=1), datetime(year=year + 1, month=1, day=1)\n\n    gw = datacube.api.GridWorkflow(index=index, product=product.name)\n\n    if cell_index_list is None:\n        if cell_index is not None:\n            cell_index_list = [cell_index]\n        else:\n            cell_index_list = []\n\n    for requested_cell_index in cell_index_list:\n        cells = gw.list_cells(product=product.name, cell_index=requested_cell_index, **query)\n        for (found_cell_index, tile) in cells.items():\n            output_filename = get_filename(config, found_cell_index, year)\n            yield dict(tile=tile,\n                       cell_index=found_cell_index,\n                       output_filename=output_filename)\n\n\ndef make_ncml_config(index, config, export_path=None, nested_years=None, **query):\n    config[\'product\'] = index.products.get_by_name(config[\'output_type\'])\n\n    config[\'nested_years\'] = nested_years if nested_years is not None else []\n\n    if export_path is not None:\n        config[\'location\'] = export_path\n\n    if not os.access(config[\'location\'], os.W_OK):\n        _LOG.warning(\'Current user appears not have write access output location: %s\', config[\'location\'])\n    return config\n\n\ndef get_history_attribute(config, task):\n    return \'{dt} {user} {app} ({ver}) {args}  # {comment}\'.format(\n        dt=datetime.now(tz.tzlocal()).isoformat(),\n        user=os.environ.get(\'USERNAME\') or os.environ.get(\'USER\'),\n        app=APP_NAME,\n        ver=datacube.__version__,\n        args=\', \'.join([config[\'app_config_file\'],\n                        task[\'output_filename\'],\n                        str(task[\'cell_index\'])]),\n        comment=\'Created NCML file to aggregate multiple NetCDF files along the time dimension\'\n    )\n\n\ndef do_ncml_task(config, task):\n    tile = task[\'tile\']\n    nested_years = config[\'nested_years\']\n\n    def get_sources_filepath(sources):\n        year = int(str(sources.time.values.astype(\'datetime64[Y]\')))\n        if year in nested_years:\n            file_path_template = str(Path(config[\'location\'], config[\'partial_ncml_path_template\']))\n            return file_path_template.format(tile_index=task[\'cell_index\'], start_time=year), True\n\n        return str(sources.item()[0].local_path), False\n\n    header_attrs = dict(date_created=datetime.today().isoformat(),\n                        history=get_history_attribute(config, task))\n\n    file_locations = []\n    for (file_location, is_nested_ncml), sources in groupby(tile.sources, get_sources_filepath):\n        file_locations.append(file_location)\n        if is_nested_ncml:\n            write_ncml_file(file_location, [str(source.item()[0].local_path) for source in sources], header_attrs)\n\n    write_ncml_file(task[\'output_filename\'], file_locations, header_attrs)\n\n\ndef write_ncml_file(ncml_filename, file_locations, header_attrs):\n    filename = Path(ncml_filename)\n    temp_filename = Path().joinpath(*filename.parts[:-1]) / \'.tmp\' / filename.parts[-1]\n\n    if temp_filename.exists():\n        temp_filename.unlink()\n\n    try:\n        temp_filename.parent.mkdir(parents=True)\n    except OSError:\n        pass\n\n    netcdf_def = """"""\n        <netcdf xmlns=""http://www.unidata.ucar.edu/namespaces/netcdf/ncml-2.2"" location=""{path}"">\n            <remove name=""dataset"" type=""variable"" />\n            <remove name=""dataset_nchar"" type=""dimension"" />\n        </netcdf>""""""\n\n    with open(str(temp_filename), \'w\') as ncml_file:\n        ncml_file.write(\'<netcdf xmlns=""http://www.unidata.ucar.edu/namespaces/netcdf/ncml-2.2"">\\n\')\n\n        for key, value in header_attrs.items():\n            ncml_file.write(""  <attribute name=\'{key}\' type=\'string\' value=\'{value}\' />\\n"".format(key=key, value=value))\n\n        ncml_file.write(\'  <aggregation dimName=""time"" type=""joinExisting"">\\n\')\n\n        for file_location in file_locations:\n            ncml_file.write(netcdf_def.format(path=file_location))\n\n        ncml_file.write(\'  </aggregation>\\n\')\n        ncml_file.write(\'</netcdf>\\n\')\n\n    if filename.exists():\n        filename.unlink()\n\n    temp_filename.rename(filename)\n\n\n@click.group(name=APP_NAME, help=\'NCML creation utility\')\n@datacube.ui.click.version_option\ndef ncml_app():\n    pass\n\n\n#: pylint: disable=invalid-name\ncommand_options = datacube.ui.click.compose(\n    datacube.ui.click.config_option,\n    datacube.ui.click.pass_index(app_name=APP_NAME),\n    datacube.ui.click.logfile_option,\n    task_app.cell_index_option,\n    task_app.cell_index_list_option,\n    task_app.queue_size_option,\n    task_app.load_tasks_option,\n    task_app.save_tasks_option,\n    datacube.ui.click.executor_cli_options,\n    click.option(\'--export-path\', \'export_path\',\n                 help=\'Write the stacked files to an external location instead of the location in the app config\',\n                 default=None,\n                 type=click.Path(exists=True, writable=True, file_okay=False)),\n)\n\n\n@ncml_app.command(short_help=\'Create an ncml file\')\n@command_options\n@click.argument(\'app_config\')\n@task_app.task_app(make_config=make_ncml_config, make_tasks=make_ncml_tasks)\ndef full(index, config, tasks, executor, queue_size, **kwargs):\n    """"""Create ncml files for the full time depth of the product\n\n    e.g. datacube-ncml full <app_config_yaml>\n    """"""\n    click.echo(\'Starting datacube ncml utility...\')\n\n    task_func = partial(do_ncml_task, config)\n    task_app.run_tasks(tasks, executor, task_func, None, queue_size)\n\n\n@ncml_app.command(short_help=\'Create a full ncml file with nested ncml files for particular years\')\n@command_options\n@click.argument(\'app_config\')\n@click.argument(\'nested_years\', nargs=-1, type=click.INT)\n@task_app.task_app(make_config=make_ncml_config, make_tasks=make_ncml_tasks)\ndef nest(index, config, tasks, executor, queue_size, **kwargs):\n    """"""Create ncml files for the full time, with nested ncml files covering the given years\n\n    e.g. datacube-ncml nest <app_config_yaml> 2016 2017\n\n    This will refer to the actual files (hopefully stacked), and make ncml files for the given (ie unstacked) years.\n    Use the `update` command when new data is added to a year, without having to rerun for the entire time depth.\n    """"""\n    click.echo(\'Starting datacube ncml utility...\')\n\n    task_func = partial(do_ncml_task, config)\n    task_app.run_tasks(tasks, executor, task_func, None, queue_size)\n\n\n@ncml_app.command(short_help=\'Update a single year ncml file\')\n@command_options\n@click.argument(\'app_config\')\n@click.argument(\'year\', type=click.INT)\n@task_app.task_app(make_config=make_ncml_config, make_tasks=make_ncml_tasks)\ndef update(index, config, tasks, executor, queue_size, **kwargs):\n    """"""Update a single year ncml file\n\n    e.g datacube-ncml <app_config_yaml> 1996\n\n    This can be used to update an existing ncml file created with `nest` when new data is added.\n    """"""\n    click.echo(\'Starting datacube ncml utility...\')\n\n    task_func = partial(do_ncml_task, config)\n    task_app.run_tasks(tasks, executor, task_func, None, queue_size)\n\n\nif __name__ == \'__main__\':\n    ncml_app()\n'"
datacube_apps/pixeldrill.py,10,"b'#!/usr/bin/env python\n""""""\nInteractive Pixel Drill for AGDCv2.\n\n""""""\n# pylint: disable=import-error, wrong-import-position\n# Unavoidable with TK class hierarchy.\n# pylint: disable=too-many-ancestors, redefined-builtin\nimport argparse\nimport os\nimport sys\nimport warnings\n\nimport matplotlib\nimport numpy as np\nimport pandas as pd\nimport tkinter as tk\n\nimport datacube\n\nmatplotlib.use(\'TkAgg\')\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as anim\nfrom matplotlib.backends.backend_tkagg import NavigationToolbar2TkAgg, ToolTip\n\n\n# pylint: disable=invalid-name, too-many-locals, global-variable-undefined, too-many-statements, redefined-outer-name\n# pylint: disable=broad-except\n\nFONT = (""Helvetica"", 9)\n\n# Set our plot parameters\n\nplt.rcParams.update({\n    \'legend.fontsize\': 8,\n    \'legend.handlelength\': 3,\n    \'axes.titlesize\': 9,\n    \'axes.labelsize\': 9,\n    \'xtick.labelsize\': 9,\n    \'ytick.labelsize\': 9,\n    \'font.family\': \'sans\'})\n\n\nclass Toolbar(NavigationToolbar2TkAgg):\n    def __init__(self, canvas, parent):\n        self.toolitems = (\n            (\'Unzoom\', \'Reset original view\', \'home\', \'home\'),\n            (\'Zoom\', \'Zoom to rectangle\', \'zoom_to_rect\', \'zoom\'),\n            (\'Pan\', \'Pan axes with left mouse, zoom with right\', \'move\', \'pan\'),\n            (None, None, None, None),\n            (\'Save\', \'Save\', \'filesave\', \'save_movie\'),\n            (None, None, None, None),\n            (\'Prev\', \'Previous observation\', \'back\', \'backimg\'),\n            (\'Next\', \'Next observation\', \'forward\', \'fwdimg\'),\n            (None, None, None, None),\n        )\n        NavigationToolbar2TkAgg.__init__(self, canvas, parent)\n        self._init_toolbar()\n        self.configure(background=\'black\')\n\n    def _Button(self, text, file, command, extension=\'.gif\'):\n        b = tk.Button(master=self, text=text, padx=2, pady=2, command=command,\n                      relief=tk.FLAT, font=FONT, justify=tk.CENTER)\n        b.pack(side=tk.LEFT)\n        return b\n\n    def _init_toolbar(self):\n        xmin, xmax = self.canvas.figure.bbox.intervalx\n        height, width = 40, xmax - xmin\n        tk.Frame.__init__(self, master=self.window,\n                          width=int(width), height=int(height),\n                          borderwidth=2)\n        self.update()\n\n        for text, tooltip_text, image_file, callback in self.toolitems:\n            if text is None:\n                pass\n            else:\n                button = self._Button(text=text, file=image_file,\n                                      command=getattr(self, callback))\n                if tooltip_text is not None:\n                    ToolTip.createToolTip(button, tooltip_text)\n                button.configure(background=\'black\', foreground=\'white\')\n\n        self.message = tk.StringVar(master=self)\n        self._label = tk.Label(master=self, textvariable=self.message,\n                               font=FONT)\n        self._label.pack(side=tk.RIGHT)\n        self.pack(side=tk.BOTTOM, fill=tk.X)\n        self._label.configure(background=\'black\', foreground=\'white\')\n\n    def mouse_move(self, event):\n        self._set_cursor(event)\n        if event.inaxes and event.inaxes.get_navigate():\n            try:\n                s = event.inaxes.format_coord(event.xdata, event.ydata)\n                self.set_message(s)\n            except (ValueError, OverflowError):\n                pass\n\n    def fwdimg(self, *args):\n        fwdimg()\n\n    def backimg(self, *args):\n        backimg()\n\n    def save_movie(self, *args):\n        filetypes = self.canvas.get_supported_filetypes().copy()\n        default_filetype = self.canvas.get_default_filetype()\n\n        default_filetype_name = filetypes[default_filetype]\n        del filetypes[default_filetype]\n\n        sorted_filetypes = list(filetypes.items())\n        sorted_filetypes.sort()\n        sorted_filetypes.insert(0, (default_filetype, default_filetype_name))\n\n        defaultextension = \'\'\n        initialdir = plt.rcParams.get(\'savefig.directory\', \'\')\n        initialdir = os.path.expanduser(initialdir)\n        initialfile = \'movie.mp4\'\n        fname = tk.filedialog.asksaveasfilename(\n            master=self.window,\n            title=\'Save the stack\',\n            filetypes=[(\'MPEG 4\', \'*.mp4\')],\n            defaultextension=defaultextension,\n            initialdir=initialdir,\n            initialfile=initialfile,\n        )\n\n        if fname == """" or fname == ():\n            return\n        else:\n            if initialdir == \'\':\n                plt.rcParams[\'savefig.directory\'] = initialdir\n            else:\n                plt.rcParams[\'savefig.directory\'] = os.path.dirname(str(fname))\n            try:\n                writer = anim.writers[\'ffmpeg\']\n                mwriter = writer(fps=1,\n                                 bitrate=0,\n                                 codec=\'h264\',\n                                 # extra_args=[\'-crf\', \'23\', \'-pix_fmt\' \'yuv420p\'],\n                                 metadata={})\n                with mwriter.saving(mainfig, fname, 140):\n                    print(\' \'.join(mwriter._args()))  # pylint: disable=protected-access\n                    for i in range(ntime):\n                        changeimg(i)\n                        mwriter.grab_frame()\n            except Exception as e:\n                tk.messagebox.showerror(""Error saving file"", str(e))\n\n\nclass DrillToolbar(NavigationToolbar2TkAgg):\n    def __init__(self, canvas, parent):\n        self.toolitems = (\n            (\'CSV\', \'Save CSV\', \'filesave\', \'save_csv\'),\n            (\'FIG\', \'Save figure\', \'filesave\', \'save_figure\'),\n        )\n        NavigationToolbar2TkAgg.__init__(self, canvas, parent)\n        self._init_toolbar()\n        self.configure(background=\'black\')\n\n    def _Button(self, text, file, command, extension=\'.gif\'):\n        b = tk.Button(master=self, text=text, padx=2, pady=2, command=command,\n                      relief=tk.FLAT, font=FONT)\n        b.pack(side=tk.LEFT)\n        return b\n\n    def _init_toolbar(self):\n        xmin, xmax = self.canvas.figure.bbox.intervalx\n        height, width = 30, xmax - xmin\n        tk.Frame.__init__(self, master=self.window,\n                          width=int(width), height=int(height),\n                          borderwidth=2)\n        self.update()\n\n        for text, tooltip_text, image_file, callback in self.toolitems:\n            if text is None:\n                # spacer, unhandled in Tk\n                pass\n            else:\n                button = self._Button(text=text, file=image_file,\n                                      command=getattr(self, callback))\n                if tooltip_text is not None:\n                    ToolTip.createToolTip(button, tooltip_text)\n                button.configure(background=\'black\', foreground=\'white\')\n\n        self.message = tk.StringVar(master=self)\n\n    def save_csv(self, *args):\n        initialdir = plt.rcParams.get(\'savefig.directory\', \'\')\n        initialdir = os.path.expanduser(initialdir)\n        fname = tk.filedialog.asksaveasfilename(\n            master=self.window,\n            title=\'Save the pixel drill\',\n            filetypes=[(\'CSV\', \'*.csv\')],\n            defaultextension=\'\',\n            initialdir=initialdir,\n            initialfile=\'pixeldrill.csv\',\n        )\n\n        if fname == """" or fname == ():\n            return\n        else:\n            if initialdir == \'\':\n                plt.rcParams[\'savefig.directory\'] = initialdir\n            else:\n                plt.rcParams[\'savefig.directory\'] = os.path.dirname(str(fname))\n            try:\n                ds = pd.DataFrame(data=ts,\n                                  index=times,\n                                  columns=bands)\n                ds.to_csv(fname)\n\n            except Exception as e:\n                tk.messagebox.showerror(""Error saving file"", str(e))\n\n    def save_figure(self, *args):\n        filetypes = self.canvas.get_supported_filetypes().copy()\n        default_filetype = self.canvas.get_default_filetype()\n\n        default_filetype_name = filetypes[default_filetype]\n        del filetypes[default_filetype]\n\n        sorted_filetypes = list(filetypes.items())\n        sorted_filetypes.sort()\n        sorted_filetypes.insert(0, (default_filetype, default_filetype_name))\n\n        initialdir = plt.rcParams.get(\'savefig.directory\', \'\')\n        initialdir = os.path.expanduser(initialdir)\n        initialfile = \'pixeldrill.pdf\'\n        fname = tk.filedialog.asksaveasfilename(\n            master=self.window,\n            title=\'Save the pixel drill\',\n            filetypes=[(\'PNG\', \'*.png\'), (\'PDF\', \'*.pdf\')],\n            defaultextension=\'\',\n            initialdir=initialdir,\n            initialfile=initialfile,\n        )\n\n        if fname == """" or fname == ():\n            return\n        else:\n            if initialdir == \'\':\n                plt.rcParams[\'savefig.directory\'] = initialdir\n            else:\n                plt.rcParams[\'savefig.directory\'] = os.path.dirname(str(fname))\n            try:\n                fig = plt.figure(figsize=(6, 4.5))\n\n                ax3 = fig.add_subplot(211, xmargin=0, ymargin=0)\n                ax3.set_xticks(range(nband))\n                ax3.set_xticklabels(bands)\n                ax3.set_title(\'Spectral profiles through time\')\n                ax3.set_xlim((-0.2, nband - 0.8))\n                ax3.set_ylim((0, np.nanmax(data)))\n                ax3.xaxis.grid(color=\'black\', linestyle=\'dotted\')\n\n                box = ax3.get_position()\n                ax3.set_position([box.x0, box.y0 + box.height * 0.1,\n                                  box.width, box.height * 0.8])\n\n                tindex = range(1, len(times) + 1)\n\n                ax4 = fig.add_subplot(212, xmargin=0, ymargin=0)\n                ax4.set_title(\'Band time series\')\n\n                ax4.set_xticks(tindex)\n                ax4.set_xlim(0.9, tindex[-1] + 0.1)\n                ax4.set_ylim((0, np.nanmax(data)))\n\n                for i, p in enumerate(ts.T):\n                    ax3.plot(range(nband), p, c=\'k\')\n\n                for i in range(ts.shape[0]):\n                    tt = ts[i, :]\n                    ax4.plot(tindex, tt, lw=1,\n                             marker=\'.\', linestyle=\'-\', color=colors[i],\n                             label=bands[i])\n\n                ax4.legend(loc=\'upper center\', bbox_to_anchor=(0.5, -0.2),\n                           labelspacing=0.8, handletextpad=0, handlelength=2,\n                           borderaxespad=0, ncol=nband, columnspacing=0.5)\n\n                fig.savefig(fname, bbox_inches=\'tight\')\n\n                # plt.close(fig)\n\n            except Exception as e:\n                tk.messagebox.showerror(""Error saving file"", str(e))\n\n\nclass Formatter(object):\n    def __init__(self, vi, names, data):\n        self.vi = vi\n        self.names = names\n        self.data = data\n\n    def __call__(self, x, y):\n        xi, yi = int(round(x, 0)), int(round(y, 0))\n        values = \' \'.join([\'{}:{}\'.format(n, d) for n, d in\n                           zip(self.names, self.data[yi, xi, :, vi])])\n        return \'x:{} y:{}\\n{}\'.format(xi, yi, values)\n\n\ndef dcmap(length, base_cmap=None):\n    """"""Create an length-bin discrete colormap from the specified input map.""""""\n    base = plt.cm.get_cmap(base_cmap)\n    color_list = base(np.linspace(0, 1, length))\n    cmap_name = base.name + str(length)\n    return base.from_list(cmap_name, color_list, length)\n\n\ndef sizefmt(num, suffix=\'B\'):\n    for unit in [\'\', \'K\', \'M\', \'G\', \'T\', \'P\', \'E\', \'Z\']:\n        if abs(num) < 1024.0:\n            return ""%3.1f%s%s"" % (num, unit, suffix)\n        num /= 1024.0\n    return ""%.1f%s%s"" % (num, \'Yi\', suffix)\n\n\ndef setfg(ax, color):\n    """"""Set the color of the frame, major ticks, tick labels, axis labels,\n    title and legend.""""""\n    for tl in ax.get_xticklines() + ax.get_yticklines():\n        tl.set_color(color)\n    for spine in ax.spines:\n        ax.spines[spine].set_edgecolor(color)\n    for tick in ax.xaxis.get_major_ticks():\n        tick.label1.set_color(color)\n    for tick in ax.yaxis.get_major_ticks():\n        tick.label1.set_color(color)\n    ax.axes.xaxis.label.set_color(color)\n    ax.axes.yaxis.label.set_color(color)\n    ax.axes.xaxis.get_offset_text().set_color(color)\n    ax.axes.yaxis.get_offset_text().set_color(color)\n    ax.axes.title.set_color(color)\n    lh = ax.get_legend()\n    if lh is not None:\n        lh.get_title().set_color(color)\n        lh.legendPatch.set_edgecolor(\'none\')\n        labels = lh.get_texts()\n        for lab in labels:\n            lab.set_color(color)\n    for tl in ax.get_xticklabels():\n        tl.set_color(color)\n    for tl in ax.get_yticklabels():\n        tl.set_color(color)\n\n\ndef setbg(ax, color):\n    """"""Set the background color of the current axes (and legend).""""""\n    ax.patch.set_facecolor(color)\n    lh = ax.get_legend()\n    if lh is not None:\n        lh.legendPatch.set_facecolor(color)\n\n\ndef drill(x=0, y=0):\n    """"""Do the pixel drill.""""""\n\n    # Get slice\n\n    global ts\n    ts = data[y, x, :, :]\n\n    # Plot spectral profile\n\n    ax1.lines = []\n    for i, p in enumerate(ts.T):\n        ax1.plot(range(nband), p, c=\'w\')\n    # ax1.set_ylim((0, np.nanmax(ts)*1.2))\n\n    # Plot time series\n\n    ax2.lines = []\n    for i in range(ts.shape[0]):\n        tt = ts[i, :]\n        ax2.plot(tindex, tt, lw=1,\n                 marker=\'.\', linestyle=\'-\', color=colors[i],\n                 label=bands[i])\n    # ax2.set_xlim(-0.1, tindex[-1] + 0.1)\n    # ax2.set_xticks(tindex)\n\n    ax2.legend(loc=\'upper center\', bbox_to_anchor=(0.5, -0.2),\n               labelspacing=0.8, handletextpad=0, handlelength=2,\n               borderaxespad=0, ncol=nband, columnspacing=0.5)\n\n    setfg(ax2, \'white\')\n    setbg(ax2, \'black\')\n\n    # Update figure\n\n    drillfig.canvas.set_window_title(\'Pixel drill @ ({},{})\'.format(x, y))\n    drillfig.canvas.draw()\n\n\ndef changeimg(i):\n    """"""Change image shown.""""""\n    global vi\n\n    if vi == i:\n        return\n\n    # Scale and fix image\n    img = data[:, :, vbnds, i].copy()\n    mask = (img > maxvalue).any(axis=2)\n    img = img / maxvalue\n    img[mask] = 1.0\n    mask = np.isnan(img).any(axis=2)\n    img[mask] = 0.0\n\n    # Draw it\n    mainimg.set_data(img)\n    mainfig.canvas.set_window_title(\'[{}/{}] {}. Data mem usage: {}\'.format(i + 1, ntime, times[i], memusage))\n    mainfig.canvas.draw()\n\n    vi = i\n\n\ndef onclick(event):\n    """"""Handle a click event on the main image.""""""\n    global lastclick\n    try:\n        x = int(round(event.xdata))\n        y = int(round(event.ydata))\n        b = int(event.button)\n        if b in [2, 3]:\n            lastclick = (x, y)\n            drill(x, y)\n    except TypeError:\n        pass\n\n\ndef onclickpd(event):\n    """"""Handle a click event in the pixel drill.""""""\n    global vi\n    vi = int(round(event.xdata))\n    changeimg(vi)\n\n\ndef onpress(event):\n    """"""Handle a keyboard event.""""""\n\n    if event.key == \'right\':\n        fwdimg()\n        return\n\n    if event.key == \'left\':\n        backimg()\n        return\n\n\ndef fwdimg():\n    """"""Show next observation.""""""\n    i = min(vi + 1, data.shape[3] - 1)\n    changeimg(i)\n\n\ndef backimg():\n    """"""Show previous observation.""""""\n    i = max(0, vi - 1)\n    changeimg(i)\n\n\ndef run(latrange=None, lonrange=None, timerange=None, measurements=None,\n        valuemax=None, product=None, groupby=None, verbose=False):\n    """"""Do all the work.""""""\n\n    # Keep track of some variables globally instead of wrapping\n    # everything in a big object\n\n    global vi\n    global lastclick\n    global data\n    global ax, ax1, ax2\n    global nband, tindex, colors, bands, vbnds, ntime, times\n    global drillfig, mainfig, mainimg\n    global maxvalue, memusage\n\n    # Try to get data\n\n    try:\n        print(\'loading data from the datacube...\', end=\'\')\n\n        # Query the data\n\n        dc = datacube.Datacube()\n        dcdata = dc.load(product=product,\n                         measurements=measurements,\n                         time=timerange,\n                         latitude=latrange,\n                         longitude=lonrange,\n                         group_by=groupby)\n\n        # Check that we have data returned\n\n        if dcdata.data_vars == {}:\n            print(\'loading data failed, no data in that range.\')\n            sys.exit(1)\n\n        # Extract times and band information\n        dcdata = dcdata.to_array(dim=\'band\')\n\n        times = dcdata.coords[\'time\'].to_index().tolist()\n        bands = dcdata.coords[\'band\'].to_index().tolist()\n        bcols = {b: i for i, b in enumerate(bands)}\n\n        nband = len(bands)\n        ntime = len(times)\n\n        # Work out what to show for images\n\n        visible = [\'red\', \'green\', \'blue\']\n        if all([b in bands for b in visible]):\n            vbnds = [bcols[b] for b in visible]\n        elif len(bands) >= 3:\n            vbnds = [bcols[b] for b in bands[:3]]\n        else:\n            vbnds = [0, 0, 0]\n\n        print(\'done\')\n\n    except LookupError:\n        print(\'failed\')\n\n        # Display a list of valid products\n\n        if product is None:\n            print(\'valid products are:\')\n            prods = dc.list_products()[[\'name\', \'description\']]\n            print(prods.to_string(index=False,\n                                  justify=\'left\',\n                                  header=False,\n                                  formatters={\'description\': lambda s: \'(\' + s + \')\'}))\n        sys.exit(1)\n\n    except Exception:\n        print(\'failed\')\n        sys.exit(2)\n\n    # Nasty but it has to be done\n\n    data = dcdata.transpose(\'y\', \'x\', \'band\', \'time\').data.astype(np.float32)\n    data[data == -999] = np.nan\n\n    # Set variables\n\n    vi = 0\n    lastclick = (0, 0)\n    memusage = sizefmt(data.nbytes)\n    maxvalue = valuemax\n\n    # Setup the main figure\n\n    mainfig = plt.figure(figsize=(6, 6))\n    mainfig.canvas.set_window_title(\'[{}/{}] {}. Data mem usage: {}\'.format(1, ntime, times[0], memusage))\n    mainfig.patch.set_facecolor(\'black\')\n\n    ax = plt.Axes(mainfig, [0., 0., 1., 1.])\n    ax.format_coord = Formatter(vi, bands, data)\n    ax.set_axis_off()\n    ax.invert_yaxis()\n    mainfig.add_axes(ax)\n\n    # Surgery on the toolbar\n\n    canvas = mainfig.canvas\n    canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=1)\n\n    window = mainfig.canvas.toolbar.window\n    mainfig.canvas.toolbar.pack_forget()\n    mainfig.canvas.toolbar = Toolbar(mainfig.canvas, window)\n    mainfig.canvas.toolbar.update()\n    mainfig.canvas.toolbar.pack(side=tk.BOTTOM, fill=tk.BOTH, expand=0)\n    canvas.show()\n\n    # Scale and fix visible image\n\n    img = data[:, :, vbnds, 0].copy()\n    mask = (img > maxvalue).any(axis=2)\n    img = img / maxvalue\n    img[mask] = 1.0\n    mask = np.isnan(img).any(axis=2)\n    img[mask] = 0.0\n\n    # Show the image\n\n    mainimg = plt.imshow(img, interpolation=\'nearest\', origin=\'upper\', aspect=\'auto\', vmin=0, vmax=1)\n\n    # Setup the drill figure\n\n    drillfig = plt.figure(figsize=(4, 3))\n    drillfig.patch.set_facecolor(\'black\')\n    drillfig.canvas.toolbar.pack_forget()\n\n    # Surgery on the toolbar\n\n    canvas = drillfig.canvas\n    window = drillfig.canvas.toolbar.window\n    drillfig.canvas.toolbar.pack_forget()\n    drillfig.canvas.toolbar = DrillToolbar(drillfig.canvas, window)\n    drillfig.canvas.toolbar.update()\n    drillfig.canvas.toolbar.pack(side=tk.BOTTOM, fill=tk.BOTH, expand=0)\n    canvas.show()\n\n    # Spectral profile graph\n\n    ax1 = drillfig.add_subplot(211, xmargin=0)\n    ax1.set_xticks(range(nband))\n    ax1.set_xticklabels(bands)\n    ax1.set_title(\'Spectral profiles through time\')\n    ax1.set_xlim((-0.2, nband - 0.8))\n    ax1.set_ylim((0, np.nanmax(data)))\n    ax1.xaxis.grid(color=\'white\', linestyle=\'dotted\')\n\n    setfg(ax1, \'white\')\n    setbg(ax1, \'black\')\n\n    box = ax1.get_position()\n    ax1.set_position([box.x0, box.y0 + box.height * 0.2,\n                      box.width, box.height * 0.8])\n\n    # Time series graph\n\n    tindex = range(1, len(times) + 1)\n\n    ax2 = drillfig.add_subplot(212, xmargin=0)\n    ax2.set_title(\'Band time series\')\n\n    ax2.set_xticks(tindex)\n    # ax2.set_xticklabels(times)\n    ax2.set_xlim(0.9, tindex[-1] + 0.1)\n    ax2.set_ylim((0, np.nanmax(data)))\n\n    setfg(ax2, \'white\')\n    setbg(ax2, \'black\')\n\n    box = ax2.get_position()\n    ax2.set_position([box.x0, box.y0 + box.height * 0.2,\n                      box.width, box.height * 0.8])\n\n    # Work out colors for bands in time series\n\n    colors = [m[0] for m in bands if m[0] in [\'r\', \'g\', \'b\']]\n    ntoadd = max(0, len(bands) - len(colors))\n    cmap = dcmap(ntoadd, \'spring\')\n    colors = colors + [cmap(i) for i in range(ntoadd)]\n\n    drill(*lastclick)\n\n    # Hook up the event handlers\n\n    mainfig.canvas.mpl_connect(\'button_press_event\', onclick)\n    mainfig.canvas.mpl_connect(\'key_press_event\', onpress)\n    mainfig.canvas.mpl_connect(\'close_event\', lambda x: plt.close())\n\n    drillfig.canvas.mpl_connect(\'close_event\', lambda x: plt.close())\n    drillfig.canvas.mpl_connect(\'button_press_event\', onclickpd)\n\n    # Show it\n\n    plt.show()\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'-latrange\',\n                        help=\'latitude range\',\n                        nargs=2,\n                        default=[-34.5, -35],\n                        required=False)\n\n    parser.add_argument(\'-lonrange\',\n                        help=\'longitude range\',\n                        nargs=2,\n                        default=[148.5, 149],\n                        required=False)\n\n    parser.add_argument(\'-timerange\',\n                        help=\'time range\',\n                        nargs=2,\n                        default=[\'2011-3-2\', \'2011-6-5\'],\n                        type=str,\n                        required=False)\n\n    parser.add_argument(\'-measurements\',\n                        help=\'measurement\',\n                        action=\'append\',\n                        type=str,\n                        required=False)\n\n    parser.add_argument(\'-product\',\n                        help=\'product\',\n                        required=False)\n\n    parser.add_argument(\'-groupby\',\n                        help=\'groupby\',\n                        required=False)\n\n    parser.add_argument(\'-valuemax\',\n                        help=\'max value\',\n                        type=float,\n                        default=4000,\n                        required=False)\n\n    parser.add_argument(\'-verbose\',\n                        help=\'verbose output\',\n                        default=True,\n                        required=False)\n\n    args = parser.parse_args()\n    kwargs = vars(args)\n\n    if not args.product:\n        parser.print_help()\n        print(\'\\n\\nValid choices for PRODUCT are:\')\n        dc = datacube.Datacube()\n        prods = dc.list_products()[\'name\']\n        print(prods.to_string(index=False, header=False))\n        parser.exit()\n\n    if args.verbose:\n        print(kwargs)\n\n    run(**kwargs)\n\n\nif __name__ == \'__main__\':\n    try:\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\'ignore\', r\'All-NaN (slice|axis) encountered\')\n            main()\n    except KeyboardInterrupt:\n        pass\n'"
datacube_apps/simple_replica.py,0,"b'#!/usr/bin/env python\n""""""\nA Simple Data Cube Replication Tool\n\nConnects to a remote Data Cube via SSH, and downloads database records and files to a local file system and database.\n\nProvide a configuration file in ~/.datacube.replication.conf in YAML format, or specify an alternate location\non the command line.\n\nFor example, the following config will download 3 PQ products for the specified time and space range. Queries\nare specified the same as when using the API to search for datasets.\n\n.. code-block:: yaml\n\n    remote_host: raijin.nci.org.auo\n    remote_user: dra547\n    db_password: xxxxxxxxxxxx\n    remote_dir: /g/data/\n    local_dir: C:/datacube/\n\n    replicated_data:\n    - product: ls5_pq_albers\n      crs: EPSG:3577\n      x: [1200000, 1300000]\n      y: [-4200000, -4300000]\n      time: [2008-01-01, 2010-01-01]\n\n    - product: ls7_pq_albers\n      crs: EPSG:3577\n      x: [1200000, 1300000]\n      y: [-4200000, -4300000]\n      time: [2008-01-01, 2010-01-01]\n\n    - product: ls8_pq_albers\n      crs: EPSG:3577\n      x: [1200000, 1300000]\n      y: [-4200000, -4300000]\n      time: [2008-01-01, 2010-01-01]\n\n""""""\n\nimport logging\nimport os.path\nfrom configparser import ConfigParser\nfrom pathlib import Path\n\nimport click\nimport yaml\nfrom paramiko import SSHClient, WarningPolicy\nfrom sshtunnel import SSHTunnelForwarder\nfrom tqdm import tqdm\n\nfrom datacube import Datacube\nfrom datacube.config import LocalConfig, _DEFAULT_CONF\nfrom datacube.index import index_connect\nfrom datacube.ui.click import global_cli_options\n\nLOG = logging.getLogger(\'simple_replicator\')\n\nDEFAULT_REPLICATION_CONFIG = os.path.expanduser(\'~/.datacube.replication.conf\')\n\n\ndef uri_to_path(uri):\n    return uri.replace(\'file://\', \'\')\n\n\nclass DatacubeReplicator(object):\n    def __init__(self, config):\n        self.remote_host = config[\'remote_host\']\n        self.remote_user = config[\'remote_user\']\n        self.db_password = config[\'db_password\']\n        self.remote_dir = config[\'remote_dir\']\n        self.local_dir = config[\'local_dir\']\n        self.replication_defns = config[\'replicated_data\']\n\n        self.client = None\n        self.sftp = None\n        self.tunnel = None\n        self.remote_dc_config = None\n        self.remote_dc = None\n        self.local_index = index_connect()\n\n    def run(self):\n        self.connect()\n        self.read_remote_config()\n        self.connect_to_db()\n        self.replicate_all()\n        self.disconnect()\n\n    def connect(self):\n        client = SSHClient()\n        client.load_system_host_keys()\n        client.set_missing_host_key_policy(WarningPolicy())\n        client.connect(hostname=self.remote_host, username=self.remote_user)\n\n        LOG.debug(client)\n        self.client = client\n        self.sftp = client.open_sftp()\n\n    def disconnect(self):\n        self.client.close()\n        self.tunnel.stop()\n\n    def read_remote_config(self):\n        remote_config = ConfigParser()\n        remote_config.read_string(_DEFAULT_CONF)\n        with self.sftp.open(\'.datacube.conf\') as fin:\n            remote_config.read_file(fin)\n        self.remote_dc_config = LocalConfig(remote_config)\n\n    def connect_to_db(self):\n        self.tunnel = SSHTunnelForwarder(\n            self.remote_host,\n            ssh_username=self.remote_user,\n            remote_bind_address=(self.remote_dc_config.get(\'db_hostname\', \'127.0.0.1\'),\n                                 int(self.remote_dc_config.get(\'db_port\', 5432))))\n        self.tunnel.start()\n\n        # pylint: disable=protected-access\n        self.remote_dc_config._config[\'datacube\'][\'db_hostname\'] = \'127.0.0.1\'\n        self.remote_dc_config._config[\'datacube\'][\'db_port\'] = str(self.tunnel.local_bind_port)\n        self.remote_dc_config._config[\'datacube\'][\'db_username\'] = self.remote_user\n        self.remote_dc_config._config[\'datacube\'][\'db_password\'] = self.db_password\n\n        # This requires the password from somewhere\n        # Parsing it out of .pgpass sounds error prone and fragile\n        # Lets put it in the configuration for now\n        LOG.debug(\'Remote configuration loaded %s\', self.remote_dc_config)\n\n        self.remote_dc = Datacube(config=self.remote_dc_config)\n\n    def replicate_all(self):\n\n        for defn in tqdm(self.replication_defns, \'Replicating products\'):\n            self.replicate(defn)\n\n    def replicate_all_products(self):\n        products = self.remote_dc.index.products.get_all()\n        for product in products:\n            self.local_index.products.add(product)\n\n    def replicate(self, defn):\n        datasets = list(self.remote_dc.find_datasets(**defn))\n\n        if not datasets:\n            LOG.info(\'No remote datasets found matching %s\', defn)\n            return\n\n        # TODO: use generator not list\n        product = datasets[0].type\n        LOG.info(\'Ensuring remote product is in local index. %s\', product)\n\n        self.local_index.products.add(product)\n\n        for dataset in tqdm(datasets, \'Datasets\'):\n            # dataset = remote_dc.index.datasets.get(dataset.id, include_sources=True)\n            # We would need to pull the parent products down too\n            # TODO: Include parent source datasets + product definitions\n            dataset.sources = {}\n\n            LOG.debug(\'Replicating dataset %s\', dataset)\n            remote_path = uri_to_path(dataset.local_uri)\n            local_path = self.remote_to_local(uri_to_path(dataset.local_uri))\n\n            # Ensure local path exists\n            Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n\n            # Download file\n            self.sftp.get(remote_path, local_path)\n\n            # Add to local index\n            dataset.local_uri = \'file://\' + local_path\n            self.local_index.datasets.add(dataset)\n            LOG.debug(\'Downloaded to %s\', local_path)\n\n    def remote_to_local(self, remote):\n        return remote.replace(self.remote_dir, self.local_dir)\n\n\ndef replicate_data(config):\n    replicator = DatacubeReplicator(config)\n    replicator.run()\n\n\n@click.command(help=__doc__)\n@click.argument(\'config_path\', required=False)\n@global_cli_options\ndef replicate(config_path):\n    """"""\n    Connect to a remote Datacube, and replicate data locally.\n    """"""\n    if config_path is None:\n        config_path = DEFAULT_REPLICATION_CONFIG\n    LOG.debug(\'Config path: %s\', config_path)\n    with open(config_path) as fin:\n        config = yaml.load(fin)\n\n    replicate_data(config)\n\n\nif __name__ == \'__main__\':\n    replicate()\n'"
datacube_apps/worker.py,0,"b""from datacube.execution import worker\n\nimport warnings\n\n\ndef main():\n    warnings.warn('datacube_apps.worker should now '\n                  'be imported as datacube.execution.worker',\n                  DeprecationWarning, stacklevel=2)\n    return worker.main()\n\n\nif __name__ == '__main__':\n    main()\n"""
docs/click_utils.py,0,"b'import pkg_resources\nfrom docutils.nodes import literal_block, section, title, make_id\nfrom sphinx.domains import Domain\nfrom docutils.parsers.rst import Directive\nimport importlib\n\nimport click\n\n\nclass ClickHelpDirective(Directive):\n    has_content = True\n    required_arguments = 1\n\n    def run(self):\n        root_cmd = self.arguments[0]\n\n        env = self.state.document.settings.env\n\n        group = find_script_callable_from_env(root_cmd, env)\n\n        return [generate_help_text(group, [root_cmd])]\n\n\ndef find_script_callable_from_env(name, env):\n    commands = env.config.click_utils_commands\n\n    module, function_name = commands[name].split(\':\')\n    module = importlib.import_module(module)\n    return getattr(module, function_name)\n\n\ndef find_script_callable(name):\n    return list(pkg_resources.iter_entry_points(\n        \'console_scripts\', name))[0].load()\n\n\ndef generate_help_text(command, prefix):\n    ctx = click.Context(command)\n    help_opts = command.get_help_option(ctx).opts\n    full_cmd = \' \'.join(prefix)\n    block = section(None,\n                    title(None, full_cmd),\n                    ids=[make_id(full_cmd)], names=[full_cmd])\n    if help_opts:\n        h = ""$ {} {}\\n"".format(full_cmd, help_opts[0]) + command.get_help(ctx)\n        block.append(literal_block(None, h, language=\'console\'))\n\n    if isinstance(command, click.core.MultiCommand):\n        for c in command.list_commands(ctx):\n            c = command.resolve_command(ctx, [c])[1]\n            block.append(generate_help_text(c, prefix+[c.name]))\n\n    return block\n\n\ndef make_block(command, opt, content):\n    h = ""$ {} {}\\n"".format(command, opt) + content\n    return section(None,\n                   title(None, command),\n                   literal_block(None, h, language=\'console\'),\n                   ids=[make_id(command)], names=[command])\n\n\nclass DatacubeDomain(Domain):\n    name = \'datacube\'\n    label = \'Data Cube\'\n    directives = {\n        \'click-help\': ClickHelpDirective,\n    }\n\n\ndef setup(app):\n    app.add_config_value(\'click_utils_commands\', {}, \'html\')\n\n    app.add_domain(DatacubeDomain)\n'"
docs/conf.py,0,"b'import os\nimport sys\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.abspath(\'..\'))\nsys.path.insert(0, os.path.abspath(\'.\'))\nprint(sys.path)\n\non_rtd = os.environ.get(\'READTHEDOCS\', None) == \'True\'\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx_autodoc_typehints\',\n    \'sphinx.ext.graphviz\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.extlinks\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx_click.ext\',\n    \'click_utils\',\n    \'sphinx.ext.napoleon\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix of source filenames.\nsource_suffix = [\'.rst\', \'.md\']\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'Open Data Cube\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = ""1.8""\n# The full version, including alpha/beta/rc tags.\n# FIXME: obtain real version by running git\nrelease = version+""-FIXME""\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n# today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n# today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = []\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\nadd_function_parentheses = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'friendly\'\n\nautosummary_generate = True\n\nextlinks = {\'issue\': (\'https://github.com/opendatacube/datacube-core/issues/%s\', \'issue \'),\n            \'pull\': (\'https://github.com/opendatacube/datacube-core/pulls/%s\', \'PR \')}\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/\', None),\n    \'pandas\': (\'https://pandas.pydata.org/pandas-docs/stable/\', None),\n    \'numpy\': (\'https://docs.scipy.org/doc/numpy/\', None),\n    \'xarray\': (\'https://xarray.pydata.org/en/stable/\', None),\n}\n\ngraphviz_output_format = \'svg\'\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nif on_rtd:\n    html_theme = \'default\'\nelse:\n    import sphinx_rtd_theme\n\n    html_theme = \'sphinx_rtd_theme\'\n    html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\nhtml_theme_options = {\n    \'collapse_navigation\': False,\n    \'logo_only\': True,\n}\n\n\nhtml_logo = \'_static/odc-logo-central-blue.svg\'\nhtml_static_path = [\'_static\']\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n# html_favicon = None\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\nhtml_last_updated_fmt = \'%b %d, %Y\'\n\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\nhtml_show_sphinx = False\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'ODCdoc\'\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (\'index\', \'ODC.tex\', u\'Open Data Cube Documentation\', \'Open Data Cube\', \'manual\')\n]\n\nnumfig = True\n\n\ndef setup(app):\n    # Fix bug where code isn\'t being highlighted\n    app.add_stylesheet(\'pygments.css\')\n    app.add_stylesheet(\'custom.css\')\n\n\n# Clean up generated documentation files that RTD seems to be having trouble with\nif on_rtd:\n    import shutil\n\n    shutil.rmtree(\'./dev/generate\', ignore_errors=True)\n'"
integration_tests/__init__.py,0,"b'# coding=utf-8\n""""""\nModule\n""""""\n'"
integration_tests/conftest.py,0,"b'# coding=utf-8\n""""""\nCommon methods for index integration tests.\n""""""\nimport itertools\nimport os\nfrom copy import copy, deepcopy\nfrom datetime import timedelta\nfrom pathlib import Path\nfrom types import SimpleNamespace\nfrom uuid import uuid4\n\nimport pytest\nimport yaml\nfrom click.testing import CliRunner\nfrom hypothesis import HealthCheck, settings\n\nimport datacube.scripts.cli_app\nimport datacube.utils\nfrom datacube.drivers.postgres import _core\nfrom datacube.index import index_connect\nfrom datacube.index._metadata_types import default_metadata_type_docs\nfrom integration_tests.utils import _make_geotiffs, _make_ls5_scene_datasets, load_yaml_file, \\\n    GEOTIFF, load_test_products\n\nfrom datacube.config import LocalConfig\nfrom datacube.drivers.postgres import PostgresDb\n\n_SINGLE_RUN_CONFIG_TEMPLATE = """"""\n\n""""""\n\nINTEGRATION_TESTS_DIR = Path(__file__).parent\n\n_EXAMPLE_LS5_NBAR_DATASET_FILE = INTEGRATION_TESTS_DIR / \'example-ls5-nbar.yaml\'\n\n#: Number of time slices to create in sample data\nNUM_TIME_SLICES = 3\n\nPROJECT_ROOT = Path(__file__).parents[1]\nCONFIG_SAMPLES = PROJECT_ROOT / \'docs\' / \'config_samples\'\n\nCONFIG_FILE_PATHS = [str(INTEGRATION_TESTS_DIR / \'agdcintegration.conf\'),\n                     os.path.expanduser(\'~/.datacube_integration.conf\')]\n\n# Configure Hypothesis to allow slower tests, because we\'re testing datasets\n# and disk IO rather than scalar values in memory.  Ask @Zac-HD for details.\nsettings.register_profile(\n    \'opendatacube\', deadline=5000, max_examples=10,\n    suppress_health_check=[HealthCheck.too_slow]\n)\nsettings.load_profile(\'opendatacube\')\n\n\n@pytest.fixture\ndef global_integration_cli_args():\n    """"""\n    The first arguments to pass to a cli command for integration test configuration.\n    """"""\n    # List of a config files in order.\n    return list(itertools.chain(*((\'--config\', f) for f in CONFIG_FILE_PATHS)))\n\n\n@pytest.fixture\ndef datacube_env_name(request):\n    if hasattr(request, \'param\'):\n        return request.param\n    else:\n        return \'datacube\'\n\n\n@pytest.fixture\ndef local_config(datacube_env_name):\n    """"""Provides a :class:`LocalConfig` configured with suitable config file paths.\n\n    .. seealso::\n\n        The :func:`integration_config_paths` fixture sets up the config files.\n    """"""\n    return LocalConfig.find(CONFIG_FILE_PATHS, env=datacube_env_name)\n\n\n@pytest.fixture\ndef ingest_configs(datacube_env_name):\n    """""" Provides dictionary product_name => config file name\n    """"""\n    return {\n        \'ls5_nbar_albers\': \'ls5_nbar_albers.yaml\',\n        \'ls5_pq_albers\': \'ls5_pq_albers.yaml\',\n    }\n\n\n@pytest.fixture(params=[""US/Pacific"", ""UTC"", ])\ndef uninitialised_postgres_db(local_config, request):\n    """"""\n    Return a connection to an empty PostgreSQL database\n    """"""\n    timezone = request.param\n\n    db = PostgresDb.from_config(local_config,\n                                application_name=\'test-run\',\n                                validate_connection=False)\n\n    # Drop tables so our tests have a clean db.\n    # with db.begin() as c:  # Creates a new PostgresDbAPI, by passing a new connection to it\n    _core.drop_db(db._engine)\n    db._engine.execute(\'alter database %s set timezone = %r\' % (local_config[\'db_database\'], timezone))\n\n    # We need to run this as well, I think because SQLAlchemy grabs them into it\'s MetaData,\n    # and attempts to recreate them. WTF TODO FIX\n    remove_dynamic_indexes()\n\n    yield db\n    # with db.begin() as c:  # Drop SCHEMA\n    _core.drop_db(db._engine)\n    db.close()\n\n\n@pytest.fixture\ndef index(local_config,\n          uninitialised_postgres_db: PostgresDb):\n    index = index_connect(local_config, validate_connection=False)\n    index.init_db()\n    yield index\n    del index\n\n\n@pytest.fixture\ndef index_empty(local_config, uninitialised_postgres_db: PostgresDb):\n    index = index_connect(local_config, validate_connection=False)\n    index.init_db(with_default_types=False)\n    yield index\n    del index\n\n\n@pytest.fixture\ndef initialised_postgres_db(index):\n    """"""\n    Return a connection to an PostgreSQL database, initialised with the default schema\n    and tables.\n    """"""\n    return index._db\n\n\ndef remove_dynamic_indexes():\n    """"""\n    Clear any dynamically created postgresql indexes from the schema.\n    """"""\n    # Our normal indexes start with ""ix_"", dynamic indexes with ""dix_""\n    for table in _core.METADATA.tables.values():\n        table.indexes.intersection_update([i for i in table.indexes if not i.name.startswith(\'dix_\')])\n\n\n@pytest.fixture\ndef ls5_telem_doc(ga_metadata_type):\n    return {\n        ""name"": ""ls5_telem_test"",\n        ""description"": \'LS5 Test\',\n        ""license"": ""CC-BY-4.0"",\n        ""metadata"": {\n            ""platform"": {\n                ""code"": ""LANDSAT_5""\n            },\n            ""product_type"": ""satellite_telemetry_data"",\n            ""ga_level"": ""P00"",\n            ""format"": {\n                ""name"": ""RCC""\n            }\n        },\n        ""metadata_type"": ga_metadata_type.name\n    }\n\n\n@pytest.fixture\ndef ls5_telem_type(index, ls5_telem_doc):\n    return index.products.add_document(ls5_telem_doc)\n\n\n@pytest.fixture(scope=\'session\')\ndef geotiffs(tmpdir_factory):\n    """"""Create test geotiffs and corresponding yamls.\n\n    We create one yaml per time slice, itself comprising one geotiff\n    per band, each with specific custom data that can be later\n    tested. These are meant to be used by all tests in the current\n    session, by way of symlinking the yamls and tiffs returned by this\n    fixture, in order to save disk space (and potentially generation\n    time).\n\n    The yamls are customised versions of\n    :ref:`_EXAMPLE_LS5_NBAR_DATASET_FILE` shifted by 24h and with\n    spatial coords reflecting the size of the test geotiff, defined in\n    :ref:`GEOTIFF`.\n\n    :param tmpdir_fatory: pytest tmp dir factory.\n    :return: List of dictionaries like::\n\n        {\n            \'day\':..., # compact day string, e.g. `19900302`\n            \'uuid\':..., # a unique UUID for this dataset (i.e. specific day)\n            \'path\':..., # path to the yaml ingestion file\n            \'tiffs\':... # list of paths to the actual geotiffs in that dataset, one per band.\n        }\n\n    """"""\n    tiffs_dir = tmpdir_factory.mktemp(\'tiffs\')\n\n    config = load_yaml_file(_EXAMPLE_LS5_NBAR_DATASET_FILE)[0]\n\n    # Customise the spatial coordinates\n    ul = GEOTIFF[\'ul\']\n    lr = {\n        dim: ul[dim] + GEOTIFF[\'shape\'][dim] * GEOTIFF[\'pixel_size\'][dim]\n        for dim in (\'x\', \'y\')\n    }\n    config[\'grid_spatial\'][\'projection\'][\'geo_ref_points\'] = {\n        \'ul\': ul,\n        \'ur\': {\n            \'x\': lr[\'x\'],\n            \'y\': ul[\'y\']\n        },\n        \'ll\': {\n            \'x\': ul[\'x\'],\n            \'y\': lr[\'y\']\n        },\n        \'lr\': lr\n    }\n    # Generate the custom geotiff yamls\n    return [_make_tiffs_and_yamls(tiffs_dir, config, day_offset)\n            for day_offset in range(NUM_TIME_SLICES)]\n\n\ndef _make_tiffs_and_yamls(tiffs_dir, config, day_offset):\n    """"""Make a custom yaml and tiff for a day offset.\n\n    :param path-like tiffs_dir: The base path to receive the tiffs.\n    :param dict config: The yaml config to be cloned and altered.\n    :param int day_offset: how many days to offset the original yaml by.\n    """"""\n    config = deepcopy(config)\n\n    # Increment all dates by the day_offset\n    delta = timedelta(days=day_offset)\n    day_orig = config[\'acquisition\'][\'aos\'].strftime(\'%Y%m%d\')\n    config[\'acquisition\'][\'aos\'] += delta\n    config[\'acquisition\'][\'los\'] += delta\n    config[\'extent\'][\'from_dt\'] += delta\n    config[\'extent\'][\'center_dt\'] += delta\n    config[\'extent\'][\'to_dt\'] += delta\n    day = config[\'acquisition\'][\'aos\'].strftime(\'%Y%m%d\')\n\n    # Set the main UUID and assign random UUIDs where needed\n    uuid = uuid4()\n    config[\'id\'] = str(uuid)\n    level1 = config[\'lineage\'][\'source_datasets\'][\'level1\']\n    level1[\'id\'] = str(uuid4())\n    level1[\'lineage\'][\'source_datasets\'][\'satellite_telemetry_data\'][\'id\'] = str(uuid4())\n\n    # Alter band data\n    bands = config[\'image\'][\'bands\']\n    for band in bands.keys():\n        # Copy dict to avoid aliases in yaml output (for better legibility)\n        bands[band][\'shape\'] = copy(GEOTIFF[\'shape\'])\n        bands[band][\'cell_size\'] = {\n            dim: abs(GEOTIFF[\'pixel_size\'][dim]) for dim in (\'x\', \'y\')}\n        bands[band][\'path\'] = bands[band][\'path\'].replace(\'product/\', \'\').replace(day_orig, day)\n\n    dest_path = str(tiffs_dir.join(\'agdc-metadata_%s.yaml\' % day))\n    with open(dest_path, \'w\') as dest_yaml:\n        yaml.dump(config, dest_yaml)\n    return {\n        \'day\': day,\n        \'uuid\': uuid,\n        \'path\': dest_path,\n        \'tiffs\': _make_geotiffs(tiffs_dir, day_offset)  # make 1 geotiff per band\n    }\n\n\n@pytest.fixture\ndef example_ls5_dataset_path(example_ls5_dataset_paths):\n    """"""Create a single sample raw observation (dataset + geotiff).""""""\n    return list(example_ls5_dataset_paths.values())[0]\n\n\n@pytest.fixture\ndef example_ls5_dataset_paths(tmpdir, geotiffs):\n    """"""Create sample raw observations (dataset + geotiff).\n\n    This fixture should be used by eah test requiring a set of\n    observations over multiple time slices. The actual geotiffs and\n    corresponding yamls are symlinks to a set created for the whole\n    test session, in order to save disk and time.\n\n    :param tmpdir: The temp directory in which to create the datasets.\n    :param list geotiffs: List of session geotiffs and yamls, to be\n      linked from this unique observation set sample.\n    :return: dict: Dict of directories containing each observation,\n      indexed by dataset UUID.\n    """"""\n    dataset_dirs = _make_ls5_scene_datasets(geotiffs, tmpdir)\n    return dataset_dirs\n\n\n@pytest.fixture\ndef default_metadata_type_doc():\n    return [doc for doc in default_metadata_type_docs() if doc[\'name\'] == \'eo\'][0]\n\n\n@pytest.fixture\ndef telemetry_metadata_type_doc():\n    return [doc for doc in default_metadata_type_docs() if doc[\'name\'] == \'telemetry\'][0]\n\n\n@pytest.fixture\ndef ga_metadata_type_doc():\n    _FULL_EO_METADATA = Path(__file__).parent.joinpath(\'extensive-eo-metadata.yaml\')\n    [(path, eo_md_type)] = datacube.utils.read_documents(_FULL_EO_METADATA)\n    return eo_md_type\n\n\n@pytest.fixture\ndef default_metadata_types(index):\n    """"""Inserts the default metadata types into the Index""""""\n    for d in default_metadata_type_docs():\n        index.metadata_types.add(index.metadata_types.from_doc(d))\n    return index.metadata_types.get_all()\n\n\n@pytest.fixture\ndef ga_metadata_type(index, ga_metadata_type_doc):\n    return index.metadata_types.add(index.metadata_types.from_doc(ga_metadata_type_doc))\n\n\n@pytest.fixture\ndef default_metadata_type(index, default_metadata_types):\n    return index.metadata_types.get_by_name(\'eo\')\n\n\n@pytest.fixture\ndef telemetry_metadata_type(index, default_metadata_types):\n    return index.metadata_types.get_by_name(\'telemetry\')\n\n\n@pytest.fixture\ndef indexed_ls5_scene_products(index, ga_metadata_type):\n    """"""Add Landsat 5 scene Products into the Index""""""\n    products = load_test_products(\n        CONFIG_SAMPLES / \'dataset_types\' / \'ls5_scenes.yaml\',\n        # Use our larger metadata type with a more diverse set of field types.\n        metadata_type=ga_metadata_type\n    )\n\n    types = []\n    for product in products:\n        types.append(index.products.add_document(product))\n\n    return types\n\n\n@pytest.fixture\ndef example_ls5_nbar_metadata_doc():\n    return load_yaml_file(_EXAMPLE_LS5_NBAR_DATASET_FILE)[0]\n\n\n@pytest.fixture\ndef clirunner(global_integration_cli_args, datacube_env_name):\n    def _run_cli(opts, catch_exceptions=False,\n                 expect_success=True, cli_method=datacube.scripts.cli_app.cli,\n                 verbose_flag=\'-v\'):\n        exe_opts = list(itertools.chain(*((\'--config\', f) for f in CONFIG_FILE_PATHS)))\n        exe_opts += [\'--env\', datacube_env_name]\n        if verbose_flag:\n            exe_opts.append(verbose_flag)\n        exe_opts.extend(opts)\n\n        runner = CliRunner()\n        result = runner.invoke(\n            cli_method,\n            exe_opts,\n            catch_exceptions=catch_exceptions\n        )\n        if expect_success:\n            assert 0 == result.exit_code, ""Error for %r. output: %r"" % (opts, result.output)\n        return result\n\n    return _run_cli\n\n\n@pytest.fixture\ndef clirunner_raw():\n    def _run_cli(opts,\n                 catch_exceptions=False,\n                 expect_success=True,\n                 cli_method=datacube.scripts.cli_app.cli,\n                 verbose_flag=\'-v\'):\n        exe_opts = []\n        if verbose_flag:\n            exe_opts.append(verbose_flag)\n        exe_opts.extend(opts)\n\n        runner = CliRunner()\n        result = runner.invoke(\n            cli_method,\n            exe_opts,\n            catch_exceptions=catch_exceptions\n        )\n        if expect_success:\n            assert 0 == result.exit_code, ""Error for %r. output: %r"" % (opts, result.output)\n        return result\n\n    return _run_cli\n\n\n@pytest.fixture\ndef dataset_add_configs():\n    B = INTEGRATION_TESTS_DIR / \'data\' / \'dataset_add\'\n    return SimpleNamespace(metadata=str(B / \'metadata.yml\'),\n                           products=str(B / \'products.yml\'),\n                           datasets_bad1=str(B / \'datasets_bad1.yml\'),\n                           datasets_eo3=str(B / \'datasets_eo3.yml\'),\n                           datasets=str(B / \'datasets.yml\'))\n'"
integration_tests/data_utils.py,2,"b'""""""\nThe start of some general purpose utilities for generating test data.\n\nAt the moment it\'s not very generic, but can continue to be extended in that fashion.\n\nHypothesis is used for most of the data generation, which will hopefully improve the rigour\nof our tests when we can roll it into more tests.\n""""""\nimport string\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\n\nimport numpy as np\nimport rasterio\nimport yaml\nfrom hypothesis.strategies import sampled_from, datetimes, composite, floats, lists, text, uuids\n\nfrom datacube.utils.geometry import CRS, point\n\nDATE_FORMAT = \'%Y-%m-%d %H:%M:%S\'\n\n# Bounds retrieved from http://www.epsg-registry.org\n# Code, lat_bounds, lon_bounds\nEPSG_CODES = [(\'EPSG:32755\', (-80, 0), (144, 150))]\ncrses = sampled_from(EPSG_CODES)\nREASONABLE_DATES = datetimes(min_value=datetime(1980, 1, 1), max_value=datetime(2030, 1, 1))\nFILENAMES = text(alphabet=string.ascii_letters + string.digits + \'-_\', min_size=3)\n\n\n@composite\ndef bounds(draw, bounds):\n    return draw(\n        lists(\n            floats(*bounds), min_size=2, max_size=2\n        ).map(sorted).filter(lambda x: x[0] < x[1]))\n\n\n@composite\ndef bboxes(draw, lat_bounds=(-90, 90), lon_bounds=(-180, 180)):\n    """"""\n    https://tools.ietf.org/html/rfc7946#section-5\n    """"""\n    # Use 3 dim positions even if we only need 2\n    min_lat, max_lat = draw(bounds(lat_bounds))\n    min_lon, max_lon = draw(bounds(lon_bounds))\n\n    return min_lon, min_lat, max_lon, max_lat\n\n\n@composite\ndef extents(draw, lat_bounds, lon_bounds):\n    center_dt = draw(REASONABLE_DATES)\n\n    time_range = timedelta(seconds=12)\n    min_lon, min_lat, max_lon, max_lat = draw(bboxes(lat_bounds, lon_bounds))\n    return {\n        \'center_dt\': center_dt.strftime(DATE_FORMAT),\n        \'from_dt\': (center_dt - time_range).strftime(DATE_FORMAT),\n        \'to_dt\': (center_dt + time_range).strftime(DATE_FORMAT),\n        \'coord\': {\n            \'ll\': {\'lat\': min_lat, \'lon\': min_lon},\n            \'lr\': {\'lat\': min_lat, \'lon\': max_lon},\n            \'ul\': {\'lat\': max_lat, \'lon\': min_lon},\n            \'ur\': {\'lat\': max_lat, \'lon\': max_lon},\n        }\n    }\n\n\ndef _extent_point_projector(crs):\n    crs = CRS(crs)\n\n    def reproject_point(pos):\n        pos = point(pos[\'lon\'], pos[\'lat\'], CRS(\'EPSG:4326\'))\n        coords = pos.to_crs(crs).coords[0]\n        return {\'x\': coords[0], \'y\': coords[1]}\n\n    return reproject_point\n\n\ndef extent_to_grid_spatial(extent, crs):\n    """"""Convert an extent in WGS84 to a grid spatial in the supplied CRS""""""\n    reprojector = _extent_point_projector(crs)\n    return {\n        \'projection\': {\n            \'geo_ref_points\': {\n                corner: reprojector(pos)\n                for corner, pos in extent[\'coord\'].items()\n            },\n            \'spatial_reference\': crs\n        }\n    }\n\n\n@composite\ndef acquisition_details(draw):\n    return {\n        \'aos\': draw(REASONABLE_DATES),\n        \'groundstation\': {\n            \'code\': draw(text(alphabet=string.ascii_letters, min_size=3, max_size=5))\n        },\n        \'los\': draw(REASONABLE_DATES)\n    }\n\n\n@composite\ndef scene_datasets(draw):\n    """"""\n    Generate random test Landsat 5 Scene Datasets\n    """"""\n    crs, lat_bounds, lon_bounds = draw(crses)\n    extent = draw(extents(lat_bounds, lon_bounds))\n    return {\n        \'id\': str(draw(uuids())),\n        \'acquisition\': draw(acquisition_details()),\n        \'creation_dt\': draw(REASONABLE_DATES),\n        \'extent\': extent,\n        \'grid_spatial\': extent_to_grid_spatial(extent, crs),\n        \'image\': {\n            \'bands\': {\n                \'1\': {\n                    \'path\': draw(FILENAMES) + \'.tif\'\n                }\n            }\n        },\n        \'format\': {\n            \'name\': \'GeoTIFF\'},\n        \'instrument\': {\n            \'name\': \'TM\'},\n        \'lineage\': {\n            \'source_datasets\': {}},\n        \'platform\': {\n            \'code\': \'LANDSAT_5\'},\n        \'processing_level\': \'P54\',\n        \'product_type\': \'nbar\'\n    }\n\n\ndef write_test_scene_to_disk(dataset_dict, tmpdir):\n    tmpdir = Path(str(tmpdir))\n    # Make directory name\n    dir_name = dataset_dict[\'platform\'][\'code\'] + dataset_dict[\'id\']\n\n    # Create directory\n    new_dir = tmpdir / dir_name\n    new_dir.mkdir(exist_ok=True)\n\n    _make_geotiffs(new_dir, dataset_dict)\n    dataset_file = new_dir / \'agdc-metadata.yaml\'\n    with dataset_file.open(\'w\') as out:\n        yaml.safe_dump(dataset_dict, out)\n    return dataset_file\n\n\ndef _make_geotiffs(output_dir, dataset_dict, shape=(100, 100)):\n    """"""\n    Generate test GeoTIFF files into ``output_dir``, one per band, from a dataset dictionary\n\n    :return: a dictionary mapping band_number to filename, eg::\n\n        {\n            0: \'/tmp/tiffs/band01_time01.tif\',\n            1: \'/tmp/tiffs/band02_time01.tif\'\n        }\n    """"""\n    tiffs = {}\n    width, height = shape\n    pixel_width = (dataset_dict[\'grid_spatial\'][\'projection\'][\'geo_ref_points\'][\'ul\'][\'x\'] -\n                   dataset_dict[\'grid_spatial\'][\'projection\'][\'geo_ref_points\'][\'ur\'][\'x\']) / width\n    pixel_height = (dataset_dict[\'grid_spatial\'][\'projection\'][\'geo_ref_points\'][\'ul\'][\'y\'] -\n                    dataset_dict[\'grid_spatial\'][\'projection\'][\'geo_ref_points\'][\'ll\'][\'y\']) / height\n    metadata = {\'count\': 1,\n                \'crs\': dataset_dict[\'grid_spatial\'][\'projection\'][\'spatial_reference\'],\n                \'driver\': \'GTiff\',\n                \'dtype\': \'int16\',\n                \'width\': width,\n                \'height\': height,\n                \'nodata\': -999.0,\n                \'transform\': [pixel_width,\n                              0.0,\n                              dataset_dict[\'grid_spatial\'][\'projection\'][\'geo_ref_points\'][\'ul\'][\'x\'],\n                              0.0,\n                              pixel_height,\n                              dataset_dict[\'grid_spatial\'][\'projection\'][\'geo_ref_points\'][\'ul\'][\'y\']]}\n\n    for band_num, band_info in dataset_dict[\'image\'][\'bands\'].items():\n        path = Path(output_dir) / band_info[\'path\']\n        with rasterio.open(path, \'w\', **metadata) as dst:\n            # Write data in ""corners"" (rounded down by 100, for a size of 100x100)\n            data = np.zeros((height, width), dtype=np.int16)\n            data[:] = np.arange(height * width\n                                ).reshape((height, width)) + 10 * int(band_num)\n            dst.write(data, 1)\n        tiffs[band_num] = path\n    return tiffs\n'"
integration_tests/test_celery_runner.py,0,"b'""""""\nTests for datacube._celery_runner\n""""""\n\nfrom time import sleep\nimport subprocess\nimport pytest\nimport sys\n\ncr = pytest.importorskip(""datacube._celery_runner"")\n\nPORT = 29374\nPASS = \'dfhksdjh23iuervao\'\nWRONG_PASS = \'sdfghdfjsghjdfiueuiwei\'\nREDIS_WAIT = 0.5\n\n\ndef check_redis_binary():\n    try:\n        return subprocess.check_call([\'redis-server\', \'--version\']) == 0\n    except Exception:\n        return False\n\n\nhave_redis = check_redis_binary()\nskip_if_no_redis = pytest.mark.skipif(not have_redis, reason=""Needs redis-server to run"")\n\n\n@skip_if_no_redis\ndef test_launch_redis_no_password():\n    is_running = cr.check_redis(port=PORT)\n    assert is_running is False, ""Redis should not be running at the start of the test""\n\n    redis_stop = cr.launch_redis(PORT, password=None, loglevel=\'verbose\')\n    assert redis_stop is not None\n\n    sleep(REDIS_WAIT)\n    is_running = cr.check_redis(port=PORT)\n    assert is_running is True\n\n    redis_stop()\n    sleep(REDIS_WAIT)\n    is_running = cr.check_redis(port=PORT)\n    assert is_running is False\n\n\n@skip_if_no_redis\ndef test_launch_redis_with_config_password():\n    is_running = cr.check_redis(port=PORT)\n    assert is_running is False, ""Redis should not be running at the start of the test""\n\n    redis_stop = cr.launch_redis(PORT, password=\'\', loglevel=\'verbose\')\n    assert redis_stop is not None\n\n    sleep(REDIS_WAIT)\n    is_running = cr.check_redis(port=PORT, password=\'\')\n    assert is_running is True\n\n    redis_stop()\n    sleep(REDIS_WAIT)\n    is_running = cr.check_redis(port=PORT, password=\'\')\n    assert is_running is False\n\n\n@skip_if_no_redis\ndef test_launch_redis_with_custom_password():\n    is_running = cr.check_redis(port=PORT)\n    assert is_running is False, ""Redis should not be running at the start of the test""\n\n    redis_stop = cr.launch_redis(PORT, password=PASS, loglevel=\'verbose\')\n    assert redis_stop is not None\n\n    sleep(REDIS_WAIT)\n    is_running = cr.check_redis(port=PORT, password=PASS)\n    assert is_running is True\n\n    is_running = cr.check_redis(port=PORT, password=WRONG_PASS)\n    assert is_running is False\n\n    redis_stop()\n    sleep(REDIS_WAIT)\n    is_running = cr.check_redis(port=PORT, password=PASS)\n    assert is_running is False\n\n\ndef _echo(x, please_fail=False):\n    if please_fail:\n        raise IOError(\'Fake I/O error, cause you asked\')\n    return x\n\n\n@pytest.mark.timeout(30)\n@pytest.mark.skipif(sys.platform == \'win32\',\n                    reason=""does not run on Windows"")\n@skip_if_no_redis\ndef test_celery_with_worker():\n    DATA = [1, 2, 3, 4]\n\n    def launch_worker():\n        args = [\'bash\', \'-c\',\n                \'nohup {} -m datacube.execution.worker --executor celery localhost:{} --nprocs 1 &\'.format(\n                    sys.executable, PORT)]\n        try:\n            subprocess.check_call(args)\n        except subprocess.CalledProcessError:\n            return False\n\n        return True\n\n    assert cr.check_redis(port=PORT, password=\'\') is False, ""Redis should not be running at the start of the test""\n\n    runner = cr.CeleryExecutor(host=\'localhost\', port=PORT, password=\'\')\n    sleep(REDIS_WAIT)\n\n    assert cr.check_redis(port=PORT, password=\'\')\n\n    # no workers yet\n    future = runner.submit(_echo, 0)\n    assert future.ready() is False\n    runner.release(future)\n\n    futures = runner.map(_echo, DATA)\n    assert len(futures) == len(DATA)\n\n    completed, failed, pending = runner.get_ready(futures)\n\n    assert len(completed) == 0\n    assert len(failed) == 0\n    assert len(pending) == len(DATA)\n    # not worker test done\n\n    worker_started_ok = launch_worker()\n    assert worker_started_ok\n\n    futures = runner.map(_echo, DATA)\n    results = runner.results(futures)\n\n    assert len(results) == len(DATA)\n    assert set(results) == set(DATA)\n\n    # Test failure pass-through\n    future = runner.submit(_echo, """", please_fail=True)\n\n    for ff in runner.as_completed([future]):\n        assert ff.ready() is True\n        with pytest.raises(IOError):\n            runner.result(ff)\n\n    del runner\n\n    # Redis shouldn\'t be running now.\n    is_running = cr.check_redis(port=PORT)\n    assert is_running is False\n'"
integration_tests/test_config_tool.py,0,"b'# coding=utf-8\n""""""\nModule\n""""""\nimport logging\nimport random\nimport re\nfrom pathlib import Path\n\nimport pytest\n\nfrom datacube.drivers.postgres import _dynamic\nfrom datacube.drivers.postgres._core import drop_db, has_schema, SCHEMA_NAME\n\nEXAMPLE_DATASET_TYPE_DOCS = map(str, Path(__file__).parent.parent.\n                                joinpath(\'docs\', \'config_samples\', \'dataset_types\').glob(\'**/*.yaml\'))\n\n# Documents that shouldn\'t be accepted as mapping docs.\nINVALID_MAPPING_DOCS = map(str, Path(__file__).parent.parent.joinpath(\'docs\').glob(\'*\'))\n\n\ndef _dataset_type_count(db):\n    with db.connect() as connection:\n        return len(list(connection.get_all_products()))\n\n\ndef test_add_example_dataset_types(clirunner, initialised_postgres_db, default_metadata_type):\n    """"""\n    Add example mapping docs, to ensure they\'re valid and up-to-date.\n\n    We add them all to a single database to check for things like duplicate ids.\n\n    :type initialised_postgres_db: datacube.drivers.postgres._connections.PostgresDb\n    """"""\n    existing_mappings = _dataset_type_count(initialised_postgres_db)\n\n    print(\'{} mappings\'.format(existing_mappings))\n    for mapping_path in EXAMPLE_DATASET_TYPE_DOCS:\n        print(\'Adding mapping {}\'.format(mapping_path))\n\n        result = clirunner([\'-v\', \'product\', \'add\', mapping_path])\n        assert result.exit_code == 0\n\n        mappings_count = _dataset_type_count(initialised_postgres_db)\n        assert mappings_count > existing_mappings, ""Mapping document was not added: "" + str(mapping_path)\n        existing_mappings = mappings_count\n\n    result = clirunner([\'-v\', \'metadata\', \'list\'])\n    assert result.exit_code == 0\n\n    result = clirunner([\'-v\', \'metadata\', \'show\', \'-f\', \'json\', \'eo\'],\n                       expect_success=True)\n    assert result.exit_code == 0\n\n    result = clirunner([\'-v\', \'metadata\', \'show\'],\n                       expect_success=True)\n    assert result.exit_code == 0\n\n    result = clirunner([\'-v\', \'product\', \'list\'])\n    assert result.exit_code == 0\n\n    expect_result = 0 if existing_mappings > 0 else 1\n    result = clirunner([\'-v\', \'product\', \'show\'],\n                       expect_success=(expect_result == 0))\n    assert result.exit_code == expect_result\n\n    if existing_mappings > 1:\n        result = clirunner([\'-v\', \'product\', \'show\', \'-f\', \'json\'],\n                           expect_success=False)\n        assert result.exit_code == 1\n\n        result = clirunner([\'-v\', \'product\', \'show\', \'-f\', \'json\', \'ls8_level1_usgs\'],\n                           expect_success=False)\n        assert result.exit_code == 0\n\n        result = clirunner([\'-v\', \'product\', \'show\', \'-f\', \'yaml\', \'ls8_level1_usgs\'],\n                           expect_success=False)\n        assert result.exit_code == 0\n\n\ndef test_error_returned_on_invalid(clirunner, initialised_postgres_db):\n    """"""\n    :type initialised_postgres_db: datacube.drivers.postgres._connections.PostgresDb\n    """"""\n    assert _dataset_type_count(initialised_postgres_db) == 0\n\n    for mapping_path in INVALID_MAPPING_DOCS:\n        result = clirunner(\n            [\n                \'product\', \'add\', mapping_path\n            ],\n            # TODO: Make this false when the cli is updated to print errors (rather than uncaught exceptions).\n            catch_exceptions=True,\n            expect_success=False\n        )\n        assert result.exit_code != 0, ""Success return code for invalid document.""\n        assert _dataset_type_count(initialised_postgres_db) == 0, ""Invalid document was added to DB""\n\n\ndef test_config_check(clirunner, initialised_postgres_db, local_config):\n    """"""\n    :type local_config: datacube.config.LocalConfig\n    """"""\n\n    # This is not a very thorough check, we just check to see that\n    # it prints something vaguely related and does not error-out.\n    result = clirunner(\n        [\n            \'system\', \'check\'\n        ]\n    )\n\n    host_regex = re.compile(r\'.*Host:\\s+{}.*\'.format(local_config[\'db_hostname\']),\n                            flags=re.DOTALL)  # Match across newlines\n    user_regex = re.compile(r\'.*User:\\s+{}.*\'.format(local_config[\'db_username\']),\n                            flags=re.DOTALL)\n    assert host_regex.match(result.output)\n    assert user_regex.match(result.output)\n\n\ndef test_list_users_does_not_fail(clirunner, local_config, initialised_postgres_db):\n    """"""\n    :type local_config: datacube.config.LocalConfig\n    """"""\n    # We don\'t want to make assumptions about available users during test runs.\n    # (They are host-global, not specific to the database)\n    # So we\'re just checking that it doesn\'t fail (and the SQL etc is well formed)\n    result = clirunner(\n        [\n            \'user\', \'list\'\n        ]\n    )\n    assert result.exit_code == 0\n\n\ndef test_db_init_noop(clirunner, local_config, ls5_telem_type):\n    # Run on an existing database.\n    result = clirunner(\n        [\n            \'-v\', \'system\', \'init\'\n        ]\n    )\n    assert \'Updated.\' in result.output\n    # It should not rebuild indexes by default\n    assert \'Dropping index: dix_{}\'.format(ls5_telem_type.name) not in result.output\n\n    result = clirunner([\'metadata\', \'list\'])\n    assert ""eo3 "" in result.output\n\n\ndef test_db_init_rebuild(clirunner, local_config, ls5_telem_type):\n    # We set the field creation logging to debug, as we assert its logging output below.\n    _dynamic._LOG.setLevel(logging.DEBUG)\n\n    # Run on an existing database.\n    result = clirunner(\n        [\n            \'-v\', \'system\', \'init\', \'--rebuild\'\n        ]\n    )\n    assert \'Updated.\' in result.output\n    # It should have recreated views and indexes.\n    assert \'Dropping index: dix_{}\'.format(ls5_telem_type.name) in result.output\n    assert \'Creating index: dix_{}\'.format(ls5_telem_type.name) in result.output\n    assert \'Dropping view: {schema}.dv_{name}_dataset\'.format(\n        schema=SCHEMA_NAME, name=ls5_telem_type.name\n    ) in result.output\n    assert \'Creating view: {schema}.dv_{name}_dataset\'.format(\n        schema=SCHEMA_NAME, name=ls5_telem_type.name\n    ) in result.output\n\n\ndef test_db_init(clirunner, initialised_postgres_db):\n    with initialised_postgres_db.connect() as connection:\n        drop_db(connection._connection)\n\n        assert not has_schema(initialised_postgres_db._engine, connection._connection)\n\n    # Run on an empty database.\n    result = clirunner([\'system\', \'init\'])\n    assert \'Created.\' in result.output\n\n    with initialised_postgres_db.connect() as connection:\n        assert has_schema(initialised_postgres_db._engine, connection._connection)\n\n\ndef test_add_no_such_product(clirunner, initialised_postgres_db):\n    result = clirunner([\'dataset\', \'add\', \'--dtype\', \'no_such_product\'], expect_success=False)\n    assert result.exit_code != 0\n    assert ""DEPRECATED option detected"" in result.output\n    assert ""ERROR Supplied product name"" in result.output\n\n\n@pytest.fixture(params=[\n    (\'test_""user""_{n}\', None),\n    (\'test_""user""_{n}\', \'Test user description\'),\n    # Test that names are escaped\n    (\'test_user_""invalid+_chars_{n}\', None),\n    (\'test_user_invalid_desc_{n}\', \'Invalid ""\\\' chars in description\')])\ndef example_user(clirunner, initialised_postgres_db, request):\n    username, description = request.param\n\n    username = username.format(n=random.randint(111111, 999999))\n\n    # test_roles = (user_name for role_name, user_name, desc in roles if user_name.startswith(\'test_\'))\n    with initialised_postgres_db.connect() as connection:\n        users = (user_name for role_name, user_name, desc in connection.list_users())\n        if username in users:\n            connection.drop_users([username])\n\n    # No user exists.\n    assert_no_user(clirunner, username)\n\n    yield username, description\n\n    with initialised_postgres_db.connect() as connection:\n        users = (user_name for role_name, user_name, desc in connection.list_users())\n        if username in users:\n            connection.drop_users([username])\n\n\ndef test_user_creation(clirunner, example_user):\n    """"""\n    Add a user, grant them, delete them.\n\n    This test requires role creation privileges on the PostgreSQL instance used for testing...\n\n    :type db: datacube.drivers.postgres._connections.PostgresDb\n    """"""\n    username, user_description = example_user\n\n    # Create them\n    args = [\'user\', \'create\', \'ingest\', username]\n    if user_description:\n        args.extend([\'--description\', user_description])\n    clirunner(args)\n    assert_user_with_role(clirunner, \'ingest\', username)\n\n    # Grant them \'manage\' permission\n    clirunner([\'user\', \'grant\', \'manage\', username])\n    assert_user_with_role(clirunner, \'manage\', username)\n\n    # Delete them\n    clirunner([\'user\', \'delete\', username])\n    assert_no_user(clirunner, username)\n\n\ndef assert_user_with_role(clirunner, role, user_name):\n    result = clirunner([\'user\', \'list\'])\n    assert \'{}{}\'.format(\'user: \', user_name) in result.output\n\n\ndef assert_no_user(clirunner, username):\n    result = clirunner([\'user\', \'list\'])\n    assert username not in result.output\n'"
integration_tests/test_dataset_add.py,0,"b'import math\n\nimport toolz\nimport yaml\n\nfrom datacube.index import Index\nfrom datacube.index.hl import Doc2Dataset\nfrom datacube.model import MetadataType\nfrom datacube.testutils import gen_dataset_test_dag, load_dataset_definition, write_files, dataset_maker\nfrom datacube.utils import SimpleDocNav\n\n\ndef check_skip_lineage_test(clirunner, index):\n    ds = SimpleDocNav(gen_dataset_test_dag(11, force_tree=True))\n\n    prefix = write_files({\'agdc-metadata.yml\': yaml.safe_dump(ds.doc)})\n\n    clirunner([\'dataset\', \'add\', \'--confirm-ignore-lineage\', \'--product\', \'A\', str(prefix)])\n\n    ds_ = index.datasets.get(ds.id, include_sources=True)\n    assert ds_ is not None\n    assert str(ds_.id) == ds.id\n    assert ds_.sources == {}\n\n    assert index.datasets.get(ds.sources[\'ab\'].id) is None\n    assert index.datasets.get(ds.sources[\'ac\'].id) is None\n    assert index.datasets.get(ds.sources[\'ae\'].id) is None\n    assert index.datasets.get(ds.sources[\'ac\'].sources[\'cd\'].id) is None\n\n\ndef check_no_product_match(clirunner, index):\n    ds = SimpleDocNav(gen_dataset_test_dag(22, force_tree=True))\n\n    prefix = write_files({\'agdc-metadata.yml\': yaml.safe_dump(ds.doc)})\n\n    r = clirunner([\'dataset\', \'add\',\n                   \'--product\', \'A\',\n                   str(prefix)])\n    assert \'ERROR Dataset metadata did not match product signature\' in r.output\n\n    r = clirunner([\'dataset\', \'add\',\n                   \'--product\', \'A\',\n                   \'--product\', \'B\',\n                   str(prefix)])\n    assert \'ERROR No matching Product found for dataset\' in r.output\n\n    ds_ = index.datasets.get(ds.id, include_sources=True)\n    assert ds_ is None\n\n    # Ignore lineage but fail to match main dataset\n    r = clirunner([\'dataset\', \'add\',\n                   \'--product\', \'B\',\n                   \'--confirm-ignore-lineage\',\n                   str(prefix)])\n\n    assert \'ERROR Dataset metadata did not match product signature\' in r.output\n    assert index.datasets.has(ds.id) is False\n\n\ndef check_with_existing_lineage(clirunner, index):\n    """"""\n      A -> B\n      |    |\n      |    v\n      +--> C -> D\n      |\n      +--> E\n\n    Add nodes BCE(D) with auto-matching, then add node A with product restricted to A only.\n    """"""\n\n    ds = SimpleDocNav(gen_dataset_test_dag(33, force_tree=True))\n\n    child_docs = [ds.sources[x].doc for x in (\'ab\', \'ac\', \'ae\')]\n\n    prefix = write_files({\'lineage.yml\': yaml.safe_dump_all(child_docs),\n                          \'main.yml\': yaml.safe_dump(ds.doc),\n                          })\n\n    clirunner([\'dataset\', \'add\', str(prefix / \'lineage.yml\')])\n    assert index.datasets.get(ds.sources[\'ae\'].id) is not None\n    assert index.datasets.get(ds.sources[\'ab\'].id) is not None\n    assert index.datasets.get(ds.sources[\'ac\'].id) is not None\n\n    clirunner([\'dataset\', \'add\',\n               \'--no-auto-add-lineage\',\n               \'--product\', \'A\',\n               str(prefix / \'main.yml\')])\n\n    assert index.datasets.get(ds.id) is not None\n\n\ndef check_inconsistent_lineage(clirunner, index):\n    """"""\n      A -> B\n      |    |\n      |    v\n      +--> C -> D\n      |\n      +--> E\n\n    Add node E,\n    then try adding A with modified E in the lineage, should fail to add ABCD\n    """"""\n    ds = SimpleDocNav(gen_dataset_test_dag(1313, force_tree=True))\n\n    child_docs = [ds.sources[x].doc for x in (\'ae\',)]\n    modified_doc = toolz.assoc_in(ds.doc, \'lineage.source_datasets.ae.label\'.split(\'.\'), \'modified\')\n\n    prefix = write_files({\'lineage.yml\': yaml.safe_dump_all(child_docs),\n                          \'main.yml\': yaml.safe_dump(modified_doc),\n                          })\n\n    clirunner([\'dataset\', \'add\', str(prefix / \'lineage.yml\')])\n    assert index.datasets.get(ds.sources[\'ae\'].id) is not None\n\n    r = clirunner([\'dataset\', \'add\',\n                   str(prefix / \'main.yml\')])\n\n    assert \'ERROR Inconsistent lineage dataset\' in r.output\n\n    assert index.datasets.has(ds.id) is False\n    assert index.datasets.has(ds.sources[\'ab\'].id) is False\n    assert index.datasets.has(ds.sources[\'ac\'].id) is False\n    assert index.datasets.has(ds.sources[\'ac\'].sources[\'cd\'].id) is False\n\n    # now again but skipping verification check\n    r = clirunner([\'dataset\', \'add\', \'--no-verify-lineage\',\n                   str(prefix / \'main.yml\')])\n\n    assert index.datasets.has(ds.id)\n    assert index.datasets.has(ds.sources[\'ab\'].id)\n    assert index.datasets.has(ds.sources[\'ac\'].id)\n    assert index.datasets.has(ds.sources[\'ac\'].sources[\'cd\'].id)\n\n\ndef check_missing_lineage(clirunner, index):\n    """"""\n      A -> B\n      |    |\n      |    v\n      +--> C -> D\n      |\n      +--> E\n\n    Use --no-auto-add-lineage\n    """"""\n    ds = SimpleDocNav(gen_dataset_test_dag(44, force_tree=True))\n    child_docs = [ds.sources[x].doc for x in (\'ae\', \'ab\', \'ac\')]\n\n    prefix = write_files({\'lineage.yml\': yaml.safe_dump_all(child_docs),\n                          \'main.yml\': yaml.safe_dump(ds.doc),\n                          })\n\n    r = clirunner([\'dataset\', \'add\',\n                   \'--no-auto-add-lineage\',\n                   str(prefix / \'main.yml\')])\n\n    assert \'ERROR Following lineage datasets are missing\' in r.output\n    assert index.datasets.has(ds.id) is False\n\n    # now add lineage and try again\n    clirunner([\'dataset\', \'add\', str(prefix / \'lineage.yml\')])\n    assert index.datasets.has(ds.sources[\'ae\'].id)\n    r = clirunner([\'dataset\', \'add\',\n                   \'--no-auto-add-lineage\',\n                   str(prefix / \'main.yml\')])\n\n    assert index.datasets.has(ds.id)\n\n\ndef check_missing_metadata_doc(clirunner):\n    prefix = write_files({\'im.tiff\': \'\'})\n    r = clirunner([\'dataset\', \'add\', str(prefix / \'im.tiff\')])\n    assert ""ERROR No supported metadata docs found for dataset"" in r.output\n\n\ndef check_no_confirm(clirunner, path):\n    r = clirunner([\'dataset\', \'add\', \'--ignore-lineage\', str(path)], expect_success=False)\n    assert r.exit_code != 0\n    assert \'Use --confirm-ignore-lineage from non-interactive scripts\' in r.output\n\n\ndef check_bad_yaml(clirunner, index):\n    prefix = write_files({\'broken.yml\': \'""\'})\n    r = clirunner([\'dataset\', \'add\', str(prefix / \'broken.yml\')])\n    assert \'ERROR Failed reading documents from \' in r.output\n\n\ndef test_dataset_add(dataset_add_configs, index_empty, clirunner):\n    p = dataset_add_configs\n    index = index_empty\n    r = clirunner([\'dataset\', \'add\', p.datasets], expect_success=False)\n    assert r.exit_code != 0\n    assert \'Found no products\' in r.output\n\n    clirunner([\'metadata\', \'add\', p.metadata])\n    clirunner([\'product\', \'add\', p.products])\n    clirunner([\'dataset\', \'add\', p.datasets])\n    clirunner([\'dataset\', \'add\', p.datasets_bad1])\n    clirunner([\'dataset\', \'add\', p.datasets_eo3])\n\n    ds = load_dataset_definition(p.datasets)\n    ds_bad1 = load_dataset_definition(p.datasets_bad1)\n\n    # Check .hl.Doc2Dataset\n    doc2ds = Doc2Dataset(index)\n    _ds, _err = doc2ds(ds.doc, \'file:///something\')\n    assert _err is None\n    assert str(_ds.id) == ds.id\n    assert _ds.metadata_doc == ds.doc\n\n    # Check dataset search\n\n    r = clirunner([\'dataset\', \'search\'], expect_success=True)\n    assert ds.id in r.output\n    assert ds_bad1.id not in r.output\n    assert ds.sources[\'ab\'].id in r.output\n    assert ds.sources[\'ac\'].sources[\'cd\'].id in r.output\n\n    r = clirunner([\'dataset\', \'info\', \'-f\', \'csv\', ds.id])\n    assert ds.id in r.output\n\n    r = clirunner([\'dataset\', \'info\', \'-f\', \'yaml\', \'--show-sources\', ds.id])\n    assert ds.sources[\'ae\'].id in r.output\n\n    r = clirunner([\'dataset\', \'info\', \'-f\', \'yaml\', \'--show-derived\', ds.sources[\'ae\'].id])\n    assert ds.id in r.output\n\n    ds_ = SimpleDocNav(gen_dataset_test_dag(1, force_tree=True))\n    assert ds_.id == ds.id\n\n    x = index.datasets.get(ds.id, include_sources=True)\n    assert str(x.sources[\'ab\'].id) == ds.sources[\'ab\'].id\n    assert str(x.sources[\'ac\'].sources[\'cd\'].id) == ds.sources[\'ac\'].sources[\'cd\'].id\n\n    check_skip_lineage_test(clirunner, index)\n    check_no_product_match(clirunner, index)\n    check_with_existing_lineage(clirunner, index)\n    check_inconsistent_lineage(clirunner, index)\n    check_missing_metadata_doc(clirunner)\n    check_missing_lineage(clirunner, index)\n    check_no_confirm(clirunner, p.datasets)\n    check_bad_yaml(clirunner, index)\n\n    # check --product=nosuchproduct\n    r = clirunner([\'dataset\', \'add\', \'--product\', \'nosuchproduct\', p.datasets],\n                  expect_success=False)\n\n    assert ""ERROR Supplied product name"" in r.output\n    assert r.exit_code != 0\n\n    # Check that deprecated option is accepted\n    r = clirunner([\'dataset\', \'add\', \'--auto-match\', p.datasets])\n    assert \'WARNING --auto-match option is deprecated\' in r.output\n\n    # test dataset add eo3\n    r = clirunner([\'dataset\', \'add\', p.datasets_eo3])\n    assert r.exit_code == 0\n\n    ds_eo3 = load_dataset_definition(p.datasets_eo3)\n    _ds = index.datasets.get(ds_eo3.id, include_sources=True)\n    assert sorted(_ds.sources) == [\'a\', \'bc1\', \'bc2\']\n    assert _ds.crs == \'EPSG:3857\'\n    assert _ds.extent is not None\n    assert _ds.extent.crs == _ds.crs\n\n\ndef test_dataset_add_ambgious_products(dataset_add_configs, index_empty, clirunner):\n    p = dataset_add_configs\n    index = index_empty\n\n    dss = [SimpleDocNav(dataset_maker(i)(\n        \'A\',\n        product_type=\'eo\',\n        flag_a=\'a\',\n        flag_b=\'b\')) for i in [1, 2]]\n\n    prefix = write_files({\n        \'products.yml\': \'\'\'\nname: A\ndescription: test product A\nmetadata_type: minimal\nmetadata:\n    product_type: eo\n    flag_a: a\n\n---\nname: B\ndescription: test product B\nmetadata_type: minimal\nmetadata:\n    product_type: eo\n    flag_b: b\n    \'\'\',\n        \'dataset1.yml\': yaml.safe_dump(dss[0].doc),\n        \'dataset2.yml\': yaml.safe_dump(dss[1].doc),\n    })\n\n    clirunner([\'metadata\', \'add\', p.metadata])\n    clirunner([\'product\', \'add\', str(prefix / \'products.yml\')])\n\n    pp = list(index.products.get_all())\n    assert len(pp) == 2\n\n    for ds, i in zip(dss, (1, 2)):\n        r = clirunner([\'dataset\', \'add\', str(prefix / (\'dataset%d.yml\' % i))])\n        assert \'ERROR Auto match failed\' in r.output\n        assert \'matches several products\' in r.output\n        assert index.datasets.has(ds.id) is False\n\n    # check that forcing product works\n    ds, fname = dss[0], \'dataset1.yml\'\n    r = clirunner([\'dataset\', \'add\',\n                   \'--product\', \'A\',\n                   str(prefix / fname)])\n\n    assert index.datasets.has(ds.id) is True\n\n    # check that forcing via exclude works\n    ds, fname = dss[1], \'dataset2.yml\'\n    r = clirunner([\'dataset\', \'add\',\n                   \'--exclude-product\', \'B\',\n                   str(prefix / fname)])\n\n    assert index.datasets.has(ds.id) is True\n\n\ndef test_dataset_add_with_nans(dataset_add_configs, index_empty, clirunner):\n    p = dataset_add_configs\n    index = index_empty\n\n    clirunner([\'metadata\', \'add\', p.metadata])\n    clirunner([\'product\', \'add\', p.products])\n\n    mk = dataset_maker(0)\n\n    c = mk(\'C\', product_type=\'C\',\n           val_is_nan=math.nan,\n           val_is_inf=math.inf,\n           val_is_neginf=-math.inf)\n\n    b = mk(\'B\', sources={\'bc\': c}, product_type=\'B\')\n    a = mk(\'A\', sources={\'ac\': c}, product_type=\'A\')\n\n    prefix = write_files({\n        \'dataset.yml\': yaml.safe_dump_all([a, b]),\n    })\n\n    r = clirunner([\'dataset\', \'add\',\n                   \'--auto-add-lineage\',\n                   \'--verify-lineage\',\n                   str(prefix / \'dataset.yml\')])\n\n    assert ""ERROR"" not in r.output\n\n    a, b, c = [SimpleDocNav(v) for v in (a, b, c)]\n\n    assert index.datasets.bulk_has([a.id, b.id, c.id]) == [True, True, True]\n\n    c_doc = index.datasets.get(c.id).metadata_doc\n\n    assert c_doc[\'val_is_nan\'] == \'NaN\'\n    assert c_doc[\'val_is_inf\'] == \'Infinity\'\n    assert c_doc[\'val_is_neginf\'] == \'-Infinity\'\n\n\ndef test_dataset_add_inconsistent_measurements(dataset_add_configs, index_empty, clirunner):\n    p = dataset_add_configs\n    index = index_empty\n    mk = dataset_maker(0)\n\n    # not set, empty, subset, full set, super-set\n    ds1 = SimpleDocNav(mk(\'A\', product_type=\'eo\', ))\n    ds2 = SimpleDocNav(mk(\'B\', product_type=\'eo\', measurements={}))\n    ds3 = SimpleDocNav(mk(\'C\', product_type=\'eo\', measurements={\n        \'red\': {}\n    }))\n    ds4 = SimpleDocNav(mk(\'D\', product_type=\'eo\', measurements={\n        \'red\': {},\n        \'green\': {},\n    }))\n    ds5 = SimpleDocNav(mk(\'E\', product_type=\'eo\', measurements={\n        \'red\': {},\n        \'green\': {},\n        \'extra\': {},\n    }))\n\n    dss = (ds1, ds2, ds3, ds4, ds5)\n    docs = [ds.doc for ds in dss]\n\n    prefix = write_files({\n        \'products.yml\': \'\'\'\nname: eo\ndescription: test product\nmetadata_type: with_measurements\nmetadata:\n    product_type: eo\n\nmeasurements:\n    - name: red\n      dtype: int16\n      nodata: -999\n      units: \'1\'\n\n    - name: green\n      dtype: int16\n      nodata: -999\n      units: \'1\'\n    \'\'\',\n        \'dataset.yml\': yaml.safe_dump_all(docs),\n    })\n\n    clirunner([\'metadata\', \'add\', p.metadata])\n    r = clirunner([\'product\', \'add\', str(prefix / \'products.yml\')])\n\n    pp = list(index.products.get_all())\n    assert len(pp) == 1\n\n    r = clirunner([\'dataset\', \'add\', str(prefix / \'dataset.yml\')])\n    print(r.output)\n\n    r = clirunner([\'dataset\', \'search\', \'-f\', \'csv\'])\n    assert ds1.id not in r.output\n    assert ds2.id not in r.output\n    assert ds3.id not in r.output\n    assert ds4.id in r.output\n    assert ds5.id in r.output\n\n\ndef test_dataset_archive_restore(dataset_add_configs, index_empty, clirunner):\n    p = dataset_add_configs\n    index = index_empty\n\n    clirunner([\'metadata\', \'add\', p.metadata])\n    clirunner([\'product\', \'add\', p.products])\n    clirunner([\'dataset\', \'add\', p.datasets])\n\n    ds = load_dataset_definition(p.datasets)\n\n    assert index.datasets.has(ds.id) is True\n\n    # First do dry run\n    r = clirunner([\'dataset\', \'archive\', \'--dry-run\', ds.id])\n    r = clirunner([\'dataset\', \'archive\',\n                   \'--dry-run\',\n                   \'--archive-derived\',\n                   ds.sources[\'ae\'].id])\n    assert ds.id in r.output\n    assert ds.sources[\'ae\'].id in r.output\n\n    assert index.datasets.has(ds.id) is True\n\n    # Run for real\n    r = clirunner([\'dataset\', \'archive\', ds.id])\n    r = clirunner([\'dataset\', \'info\', ds.id])\n    assert \'status: archived\' in r.output\n\n    # restore dry run\n    r = clirunner([\'dataset\', \'restore\', \'--dry-run\', ds.id])\n    r = clirunner([\'dataset\', \'info\', ds.id])\n    assert \'status: archived\' in r.output\n\n    # restore for real\n    r = clirunner([\'dataset\', \'restore\', ds.id])\n    r = clirunner([\'dataset\', \'info\', ds.id])\n    assert \'status: active\' in r.output\n\n    # archive derived\n    d_id = ds.sources[\'ac\'].sources[\'cd\'].id\n    r = clirunner([\'dataset\', \'archive\', \'--archive-derived\', d_id])\n\n    r = clirunner([\'dataset\', \'info\', ds.id, ds.sources[\'ab\'].id, ds.sources[\'ac\'].id])\n    assert \'status: active\' not in r.output\n    assert \'status: archived\' in r.output\n\n    # restore derived\n    r = clirunner([\'dataset\', \'restore\', \'--restore-derived\', d_id])\n    r = clirunner([\'dataset\', \'info\', ds.id, ds.sources[\'ab\'].id, ds.sources[\'ac\'].id])\n    assert \'status: active\' in r.output\n    assert \'status: archived\' not in r.output\n\n\ndef test_dataset_add_http(dataset_add_configs, index: Index, default_metadata_type: MetadataType, httpserver,\n                          clirunner):\n    # pytest-localserver also looks good, it\'s been around for ages, but httpserver is the new cool\n    p = dataset_add_configs\n\n    httpserver.expect_request(\'/metadata_types.yaml\').respond_with_data(open(p.metadata).read())\n    httpserver.expect_request(\'/products.yaml\').respond_with_data(open(p.products).read())\n    httpserver.expect_request(\'/datasets.yaml\').respond_with_data(open(p.datasets).read())\n    # check that the request is served\n    #    assert requests.get(httpserver.url_for(""/dataset.yaml"")).yaml() == {\'foo\': \'bar\'}\n\n    clirunner([\'metadata\', \'add\', httpserver.url_for(\'/metadata_types.yaml\')])\n    clirunner([\'product\', \'add\', httpserver.url_for(\'/products.yaml\')])\n    # clirunner([\'dataset\', \'add\', p.datasets])\n    clirunner([\'dataset\', \'add\', httpserver.url_for(\'/datasets.yaml\')])\n\n    ds = load_dataset_definition(p.datasets)\n    assert index.datasets.has(ds.id)\n\n\ndef xtest_dataset_add_fails(clirunner, index):\n    result = clirunner([\'dataset\', \'add\', \'bad_path.yaml\'], expect_success=False)\n    assert result.exit_code != 0, ""Surely not being able to add a dataset when requested should return an error.""\n'"
integration_tests/test_double_ingestion.py,0,"b'import pytest\nimport netCDF4\n\nfrom integration_tests.utils import prepare_test_ingestion_configuration\nfrom integration_tests.test_full_ingestion import (check_open_with_api, check_data_with_api,\n                                                   ensure_datasets_are_indexed, check_data_shape,\n                                                   check_grid_mapping, check_cf_compliance, check_attributes,\n                                                   check_dataset_metadata_in_storage_unit,\n                                                   check_open_with_xarray)\n\nfrom integration_tests.test_end_to_end import INGESTER_CONFIGS\n\n\n@pytest.mark.parametrize(\'datacube_env_name\', (\'datacube\',), indirect=True)\n@pytest.mark.usefixtures(\'default_metadata_type\',\n                         \'indexed_ls5_scene_products\')\ndef test_double_ingestion(clirunner, index, tmpdir, ingest_configs, example_ls5_dataset_paths):\n    """"""\n    Test for the case where ingestor does not need to create a new product,\n    but should re-use an existing target product.\n\n    """"""\n    # Make a test ingestor configuration\n    config = INGESTER_CONFIGS / ingest_configs[\'ls5_nbar_albers\']\n    config_path, config = prepare_test_ingestion_configuration(tmpdir, None,\n                                                               config, mode=\'fast_ingest\')\n\n    def index_dataset(path):\n        return clirunner([\'dataset\', \'add\', str(path)])\n\n    def ingest_products():\n        valid_uuids = []\n        for uuid, ls5_dataset_path in example_ls5_dataset_paths.items():\n            valid_uuids.append(uuid)\n            index_dataset(ls5_dataset_path)\n\n        # Ensure that datasets are actually indexed\n        ensure_datasets_are_indexed(index, valid_uuids)\n\n        # Ingest them\n        clirunner([\n            \'ingest\',\n            \'--config-file\',\n            str(config_path)\n        ])\n\n        # Validate that the ingestion is working as expected\n        datasets = index.datasets.search_eager(product=\'ls5_nbar_albers\')\n        assert len(datasets) > 0\n        assert datasets[0].managed\n\n        check_open_with_api(index, len(valid_uuids))\n        check_data_with_api(index, len(valid_uuids))\n\n        # NetCDF specific checks, based on the saved NetCDF file\n        ds_path = str(datasets[0].local_path)\n        with netCDF4.Dataset(ds_path) as nco:\n            check_data_shape(nco)\n            check_grid_mapping(nco)\n            check_cf_compliance(nco)\n            check_dataset_metadata_in_storage_unit(nco, example_ls5_dataset_paths)\n            check_attributes(nco, config[\'global_attributes\'])\n\n            name = config[\'measurements\'][0][\'name\']\n            check_attributes(nco[name], config[\'measurements\'][0][\'attrs\'])\n        check_open_with_xarray(ds_path)\n\n    # Create and Index some example scene datasets\n    ingest_products()\n\n    ######################\n    #  Double Ingestion  #\n    ######################\n    # Create and Index some more scene datasets\n    ingest_products()\n'"
integration_tests/test_end_to_end.py,0,"b'import shutil\nfrom pathlib import Path\nimport numpy\nimport pytest\nimport rasterio\n\nfrom datacube.api.query import query_group_by\nfrom datacube.api.core import Datacube\n\nfrom integration_tests.utils import assert_click_command, prepare_test_ingestion_configuration\n\nPROJECT_ROOT = Path(__file__).parents[1]\nCONFIG_SAMPLES = PROJECT_ROOT / \'docs/config_samples/\'\nINGESTER_CONFIGS = CONFIG_SAMPLES / \'ingester\'\nLS5_DATASET_TYPES = CONFIG_SAMPLES / \'dataset_types/ls5_scenes.yaml\'\nTEST_DATA = PROJECT_ROOT / \'tests\' / \'data\' / \'lbg\'\nLBG_NBAR = \'LS5_TM_NBAR_P54_GANBAR01-002_090_084_19920323\'\nLBG_PQ = \'LS5_TM_PQ_P55_GAPQ01-002_090_084_19920323\'\nLBG_CELL = (15, -40)  # x,y\n\n\ndef custom_dumb_fuser(dst, src):\n    dst[:] = src[:]\n\n\n@pytest.fixture()\ndef testdata_dir(tmpdir, ingest_configs):\n    datadir = Path(str(tmpdir), \'data\')\n    datadir.mkdir()\n\n    shutil.copytree(str(TEST_DATA), str(tmpdir / \'lbg\'))\n\n    for file in ingest_configs.values():\n        prepare_test_ingestion_configuration(tmpdir, tmpdir, INGESTER_CONFIGS/file,\n                                             mode=\'end2end\')\n\n    return tmpdir\n\n\nignore_me = pytest.mark.xfail(True, reason=""get_data/get_description still to be fixed in Unification"")\n\n\n@pytest.mark.usefixtures(\'default_metadata_type\')\n@pytest.mark.parametrize(\'datacube_env_name\', (\'datacube\', ))\ndef test_end_to_end(clirunner, index, testdata_dir, ingest_configs, datacube_env_name):\n    """"""\n    Loads two dataset configurations, then ingests a sample Landsat 5 scene\n\n    One dataset configuration specifies Australian Albers Equal Area Projection,\n    the other is simply latitude/longitude.\n\n    The input dataset should be recorded in the index, and two sets of storage units\n    should be created on disk and recorded in the index.\n    """"""\n\n    lbg_nbar = testdata_dir / \'lbg\' / LBG_NBAR\n    lbg_pq = testdata_dir / \'lbg\' / LBG_PQ\n    ls5_nbar_albers_ingest_config = testdata_dir / ingest_configs[\'ls5_nbar_albers\']\n    ls5_pq_albers_ingest_config = testdata_dir / ingest_configs[\'ls5_pq_albers\']\n\n    # Add the LS5 Dataset Types\n    clirunner([\'-v\', \'product\', \'add\', str(LS5_DATASET_TYPES)])\n\n    # Index the Datasets\n    #  - do test run first to increase test coverage\n    clirunner([\'-v\', \'dataset\', \'add\', \'--dry-run\',\n               str(lbg_nbar), str(lbg_pq)])\n\n    #  - do actual indexing\n    clirunner([\'-v\', \'dataset\', \'add\',\n               str(lbg_nbar), str(lbg_pq)])\n\n    #  - this will be no-op but with ignore lineage\n    clirunner([\'-v\', \'dataset\', \'add\',\n               \'--confirm-ignore-lineage\',\n               str(lbg_nbar), str(lbg_pq)])\n\n    # Test no-op update\n    for policy in [\'archive\', \'forget\', \'keep\']:\n        clirunner([\'-v\', \'dataset\', \'update\',\n                   \'--dry-run\',\n                   \'--location-policy\', policy,\n                   str(lbg_nbar), str(lbg_pq)])\n\n        # Test no changes needed update\n        clirunner([\'-v\', \'dataset\', \'update\',\n                   \'--location-policy\', policy,\n                   str(lbg_nbar), str(lbg_pq)])\n\n    # TODO: test location update\n    # 1. Make a copy of a file\n    # 2. Call dataset update with archive/forget\n    # 3. Check location\n\n    # Ingest NBAR\n    clirunner([\'-v\', \'ingest\', \'-c\', str(ls5_nbar_albers_ingest_config)])\n\n    # Ingest PQ\n    clirunner([\'-v\', \'ingest\', \'-c\', str(ls5_pq_albers_ingest_config)])\n\n    dc = Datacube(index=index)\n    assert isinstance(str(dc), str)\n    assert isinstance(repr(dc), str)\n\n    with pytest.raises(ValueError):\n        dc.find_datasets(time=\'2019\')  # no product supplied, raises exception\n\n    check_open_with_dc(index)\n    check_open_with_grid_workflow(index)\n    check_load_via_dss(index)\n\n\ndef check_open_with_dc(index):\n    dc = Datacube(index=index)\n\n    data_array = dc.load(product=\'ls5_nbar_albers\', measurements=[\'blue\']).to_array(dim=\'variable\')\n    assert data_array.shape\n    assert (data_array != -999).any()\n\n    data_array = dc.load(product=\'ls5_nbar_albers\', measurements=[\'blue\'], time=\'1992-03-23T23:14:25.500000\')\n    assert data_array[\'blue\'].shape[0] == 1\n    assert (data_array.blue != -999).any()\n\n    data_array = dc.load(product=\'ls5_nbar_albers\', measurements=[\'blue\'], latitude=-35.3, longitude=149.1)\n    assert data_array[\'blue\'].shape[1:] == (1, 1)\n    assert (data_array.blue != -999).any()\n\n    data_array = dc.load(product=\'ls5_nbar_albers\', latitude=(-35, -36), longitude=(149, 150)).to_array(dim=\'variable\')\n\n    assert data_array.ndim == 4\n    assert \'variable\' in data_array.dims\n    assert (data_array != -999).any()\n\n    with rasterio.Env():\n        lazy_data_array = dc.load(product=\'ls5_nbar_albers\', latitude=(-35, -36), longitude=(149, 150),\n                                  dask_chunks={\'time\': 1, \'x\': 1000, \'y\': 1000}).to_array(dim=\'variable\')\n        assert lazy_data_array.data.dask\n        assert lazy_data_array.ndim == data_array.ndim\n        assert \'variable\' in lazy_data_array.dims\n        assert lazy_data_array[1, :2, 950:1050, 950:1050].equals(data_array[1, :2, 950:1050, 950:1050])\n\n    dataset = dc.load(product=\'ls5_nbar_albers\', measurements=[\'blue\'],\n                      fuse_func=custom_dumb_fuser)\n    assert dataset[\'blue\'].size\n\n    dataset = dc.load(product=\'ls5_nbar_albers\', latitude=(-35.2, -35.3), longitude=(149.1, 149.2))\n    assert dataset[\'blue\'].size\n\n    with rasterio.Env():\n        lazy_dataset = dc.load(product=\'ls5_nbar_albers\', latitude=(-35.2, -35.3), longitude=(149.1, 149.2),\n                               dask_chunks={\'time\': 1})\n        assert lazy_dataset[\'blue\'].data.dask\n        assert lazy_dataset.blue[:2, :100, :100].equals(dataset.blue[:2, :100, :100])\n        assert lazy_dataset.isel(time=slice(0, 2), x=slice(950, 1050), y=slice(950, 1050)).equals(\n            dataset.isel(time=slice(0, 2), x=slice(950, 1050), y=slice(950, 1050)))\n\n        # again but with larger time chunks\n        lazy_dataset = dc.load(product=\'ls5_nbar_albers\', latitude=(-35.2, -35.3), longitude=(149.1, 149.2),\n                               dask_chunks={\'time\': 2})\n        assert lazy_dataset[\'blue\'].data.dask\n        assert lazy_dataset.blue[:2, :100, :100].equals(dataset.blue[:2, :100, :100])\n        assert lazy_dataset.isel(time=slice(0, 2), x=slice(950, 1050), y=slice(950, 1050)).equals(\n            dataset.isel(time=slice(0, 2), x=slice(950, 1050), y=slice(950, 1050)))\n\n    dataset_like = dc.load(product=\'ls5_nbar_albers\', measurements=[\'blue\'], like=dataset)\n    assert (dataset.blue == dataset_like.blue).all()\n\n    solar_day_dataset = dc.load(product=\'ls5_nbar_albers\',\n                                latitude=(-35, -36), longitude=(149, 150),\n                                measurements=[\'blue\'], group_by=\'solar_day\')\n    assert 0 < solar_day_dataset.time.size <= dataset.time.size\n\n    dataset = dc.load(product=\'ls5_nbar_albers\', latitude=(-35.2, -35.3), longitude=(149.1, 149.2), align=(5, 20))\n    assert dataset.geobox.affine.f % abs(dataset.geobox.affine.e) == 5\n    assert dataset.geobox.affine.c % abs(dataset.geobox.affine.a) == 20\n    dataset_like = dc.load(product=\'ls5_nbar_albers\', measurements=[\'blue\'], like=dataset)\n    assert (dataset.blue == dataset_like.blue).all()\n\n    products_df = dc.list_products()\n    assert len(products_df)\n    assert len(products_df[products_df[\'name\'].isin([\'ls5_nbar_albers\'])])\n    assert len(products_df[products_df[\'name\'].isin([\'ls5_pq_albers\'])])\n\n    assert len(dc.list_measurements())\n    assert len(dc.list_measurements(with_pandas=False))\n    assert len(dc.list_products(with_pandas=False))\n\n    resamp = [\'nearest\', \'cubic\', \'bilinear\', \'cubic_spline\', \'lanczos\', \'average\']\n    results = {}\n\n    # WTF\n    def calc_max_change(da):\n        midline = int(da.shape[0] * 0.5)\n        a = int(abs(da[midline, :-1].data - da[midline, 1:].data).max())\n\n        centerline = int(da.shape[1] * 0.5)\n        b = int(abs(da[:-1, centerline].data - da[1:, centerline].data).max())\n        return a + b\n\n    for resamp_meth in resamp:\n        dataset = dc.load(product=\'ls5_nbar_albers\', measurements=[\'blue\'],\n                          latitude=(-35.28, -35.285), longitude=(149.15, 149.155),\n                          output_crs=\'EPSG:4326\', resolution=(-0.0000125, 0.0000125), resampling=resamp_meth)\n        results[resamp_meth] = calc_max_change(dataset.blue.isel(time=0))\n\n    assert results[\'cubic_spline\'] < results[\'nearest\']\n    assert results[\'lanczos\'] < results[\'average\']\n\n    # check empty result\n    dataset = dc.load(product=\'ls5_nbar_albers\',\n                      time=(\'1918\', \'1919\'),\n                      measurements=[\'blue\'],\n                      latitude=(-35.28, -35.285),\n                      longitude=(149.15, 149.155),\n                      output_crs=\'EPSG:4326\',\n                      resolution=(-0.0000125, 0.0000125))\n    assert len(dataset.data_vars) == 0\n\n\ndef check_open_with_grid_workflow(index):\n    type_name = \'ls5_nbar_albers\'\n    dt = index.products.get_by_name(type_name)\n\n    from datacube.api.grid_workflow import GridWorkflow\n    gw = GridWorkflow(index, dt.grid_spec)\n\n    cells = gw.list_cells(product=type_name, cell_index=LBG_CELL)\n    assert LBG_CELL in cells\n\n    cells = gw.list_cells(product=type_name)\n    assert LBG_CELL in cells\n\n    tile = cells[LBG_CELL]\n    assert \'x\' in tile.dims\n    assert \'y\' in tile.dims\n    assert \'time\' in tile.dims\n    assert tile.shape[1] == 4000\n    assert tile.shape[2] == 4000\n    assert tile[:1, :100, :100].shape == (1, 100, 100)\n    dataset_cell = gw.load(tile, measurements=[\'blue\'])\n    assert dataset_cell[\'blue\'].shape == tile.shape\n\n    for timestamp, tile_slice in tile.split(\'time\'):\n        assert tile_slice.shape == (1, 4000, 4000)\n\n    dataset_cell = gw.load(tile)\n    assert all(m in dataset_cell for m in [\'blue\', \'green\', \'red\', \'nir\', \'swir1\', \'swir2\'])\n\n    ts = numpy.datetime64(\'1992-03-23T23:14:25.500000000\')\n    tile_key = LBG_CELL + (ts,)\n    tiles = gw.list_tiles(product=type_name)\n    assert tiles\n    assert tile_key in tiles\n\n    tile = tiles[tile_key]\n    dataset_cell = gw.load(tile, measurements=[\'blue\'])\n    assert dataset_cell[\'blue\'].size\n\n    dataset_cell = gw.load(tile)\n    assert all(m in dataset_cell for m in [\'blue\', \'green\', \'red\', \'nir\', \'swir1\', \'swir2\'])\n\n\ndef check_load_via_dss(index):\n    dc = Datacube(index=index)\n\n    dss = dc.find_datasets(product=\'ls5_nbar_albers\')\n    assert len(dss) > 0\n\n    xx1 = dc.load(product=\'ls5_nbar_albers\', measurements=[\'blue\'])\n    xx2 = dc.load(datasets=dss, measurements=[\'blue\'])\n    assert xx1.blue.shape\n    assert (xx1.blue != -999).any()\n    assert (xx1.blue == xx2.blue).all()\n\n    xx2 = dc.load(datasets=iter(dss), measurements=[\'blue\'])\n    assert xx1.blue.shape\n    assert (xx1.blue != -999).any()\n    assert (xx1.blue == xx2.blue).all()\n\n    with pytest.raises(ValueError):\n        dc.load(measurements=[\'blue\'])\n\n\ndef check_legacy_open(index):\n    dc = Datacube(index=index)\n\n    data_array = dc.load(product=\'ls5_nbar_albers\',\n                         measurements=[\'blue\'],\n                         time=\'1992-03-23T23:14:25.500000\',\n                         use_threads=True)\n    assert data_array[\'blue\'].shape[0] == 1\n    assert (data_array.blue != -999).any()\n\n    # force fusing load by duplicating dataset\n    dss = dc.find_datasets(product=\'ls5_nbar_albers\',\n                           time=\'1992-03-23T23:14:25.500000\')\n\n    assert len(dss) == 1\n\n    dss = dss*2\n    sources = dc.group_datasets(dss, query_group_by(\'time\'))\n\n    gbox = data_array.geobox\n    mm = [dss[0].type.measurements[\'blue\']]\n    xx = dc.load_data(sources, gbox, mm)\n    assert (xx == data_array).all()\n\n    with rasterio.Env():\n        xx_lazy = dc.load_data(sources, gbox, mm, dask_chunks={\'time\': 1})\n        assert xx_lazy[\'blue\'].data.dask\n        assert xx_lazy.blue[0, :, :].equals(xx.blue[0, :, :])\n'"
integration_tests/test_environments.py,0,"b'import pytest\n\nfrom datacube import Datacube\nfrom datacube.config import LocalConfig\n\n\ndef test_multiple_environment_config(tmpdir):\n    config_path = tmpdir.join(\'second.conf\')\n\n    config_path.write(""""""\n[DEFAULT]\ndb_username: test_user\nindex_driver: default\n\n[default]\ndb_hostname: db.opendatacube.test\n\n[test_alt]\ndb_hostname: alt-db.opendatacube.test\n    """""")\n\n    config_path = str(config_path)\n\n    config = LocalConfig.find([config_path])\n    assert config[\'db_hostname\'] == \'db.opendatacube.test\'\n    alt_config = LocalConfig.find([config_path], env=\'test_alt\')\n    assert alt_config[\'db_hostname\'] == \'alt-db.opendatacube.test\'\n\n    # Make sure the correct config is passed through the API\n    # Parsed config:\n    db_url = \'postgresql://{user}@db.opendatacube.test:5432/datacube\'.format(user=config[\'db_username\'])\n    alt_db_url = \'postgresql://{user}@alt-db.opendatacube.test:5432/datacube\'.format(user=config[\'db_username\'])\n\n    with Datacube(config=config, validate_connection=False) as dc:\n        assert str(dc.index.url) == db_url\n\n    # When none specified, default environment is loaded\n    with Datacube(config=str(config_path), validate_connection=False) as dc:\n        assert str(dc.index.url) == db_url\n    # When specific environment is loaded\n    with Datacube(config=config_path, env=\'test_alt\', validate_connection=False) as dc:\n        assert str(dc.index.url) == alt_db_url\n\n    # An environment that isn\'t in any config files\n    with pytest.raises(ValueError):\n        with Datacube(config=config_path, env=\'undefined-env\', validate_connection=False) as dc:\n            pass\n\n\ndef test_wrong_env_error_message(clirunner_raw, monkeypatch):\n    from datacube import config\n    monkeypatch.setattr(config, \'DEFAULT_CONF_PATHS\', (\'/no/such/path-264619\',))\n\n    result = clirunner_raw([\'-E\', \'nosuch-env\', \'system\', \'check\'],\n                           expect_success=False)\n    assert ""No datacube config found for \'nosuch-env\'"" in result.output\n    assert result.exit_code != 0\n\n    result = clirunner_raw([\'system\', \'check\'],\n                           expect_success=False)\n    assert ""No datacube config found"" in result.output\n    assert result.exit_code != 0\n'"
integration_tests/test_full_ingestion.py,0,"b'import hashlib\nimport warnings\nfrom uuid import UUID\n\nimport netCDF4\nimport pytest\nimport yaml\nimport rasterio\nfrom affine import Affine\n\nfrom datacube.api.query import query_group_by\nfrom datacube.utils import geometry, read_documents, netcdf_extract_string\nfrom integration_tests.utils import prepare_test_ingestion_configuration, GEOTIFF\nfrom integration_tests.test_end_to_end import INGESTER_CONFIGS\n\nEXPECTED_STORAGE_UNIT_DATA_SHAPE = (1, 40, 40)\nCOMPLIANCE_CHECKER_NORMAL_LIMIT = 2\n\n\n@pytest.mark.timeout(20)\n@pytest.mark.parametrize(\'datacube_env_name\', (\'datacube\',), indirect=True)\n@pytest.mark.usefixtures(\'default_metadata_type\',\n                         \'indexed_ls5_scene_products\')\ndef test_full_ingestion(clirunner, index, tmpdir, example_ls5_dataset_paths, ingest_configs):\n    config = INGESTER_CONFIGS/ingest_configs[\'ls5_nbar_albers\']\n    config_path, config = prepare_test_ingestion_configuration(tmpdir, None, config, mode=\'fast_ingest\')\n    valid_uuids = []\n    for uuid, example_ls5_dataset_path in example_ls5_dataset_paths.items():\n        valid_uuids.append(uuid)\n        clirunner([\n            \'dataset\',\n            \'add\',\n            str(example_ls5_dataset_path)\n        ])\n\n    ensure_datasets_are_indexed(index, valid_uuids)\n\n    # TODO(csiro) Set time dimension when testing\n    # config[\'storage\'][\'tile_size\'][\'time\'] = 2\n\n    clirunner([\n        \'ingest\',\n        \'--config-file\',\n        str(config_path)\n    ])\n\n    datasets = index.datasets.search_eager(product=\'ls5_nbar_albers\')\n    assert len(datasets) > 0\n    assert datasets[0].managed\n\n    check_open_with_api(index, len(valid_uuids))\n    check_data_with_api(index, len(valid_uuids))\n\n    # NetCDF specific checks, based on the saved NetCDF file\n    ds_path = str(datasets[0].local_path)\n    with netCDF4.Dataset(ds_path) as nco:\n        check_data_shape(nco)\n        check_grid_mapping(nco)\n        check_cf_compliance(nco)\n        check_dataset_metadata_in_storage_unit(nco, example_ls5_dataset_paths)\n        check_attributes(nco, config[\'global_attributes\'])\n\n        name = config[\'measurements\'][0][\'name\']\n        check_attributes(nco[name], config[\'measurements\'][0][\'attrs\'])\n    check_open_with_xarray(ds_path)\n\n\n@pytest.mark.timeout(20)\n@pytest.mark.parametrize(\'datacube_env_name\', (\'datacube\',), indirect=True)\n@pytest.mark.usefixtures(\'default_metadata_type\',\n                         \'indexed_ls5_scene_products\')\ndef test_process_all_ingest_jobs(clirunner, index, tmpdir, example_ls5_dataset_paths, ingest_configs):\n    """"""\n    Test for the case where ingestor processes upto `--queue-size` number of tasks and not all the available scenes\n    """"""\n    # Make a test ingestor configuration\n    config = INGESTER_CONFIGS / ingest_configs[\'ls5_nbar_albers\']\n    config_path, config = prepare_test_ingestion_configuration(tmpdir, None,\n                                                               config, mode=\'fast_ingest\')\n\n    def index_dataset(path):\n        return clirunner([\'dataset\', \'add\', str(path)])\n\n    # Number of scenes generated is 3 (as per NUM_TIME_SLICES const from conftest.py)\n    # Set the queue size to process 2 tiles\n    queue_size = 2\n    valid_uuids = []\n    for uuid, ls5_dataset_path in example_ls5_dataset_paths.items():\n        valid_uuids.append(uuid)\n        index_dataset(ls5_dataset_path)\n\n    # Ensure that datasets are actually indexed\n    ensure_datasets_are_indexed(index, valid_uuids)\n\n    # Ingest all scenes (Though the queue size is 2, all 3 tiles will be ingested)\n    clirunner([\n        \'ingest\',\n        \'--config-file\',\n        str(config_path),\n        \'--queue-size\',\n        queue_size,\n        \'--allow-product-changes\',\n    ])\n\n    # Validate that the ingestion is working as expected\n    datasets = index.datasets.search_eager(product=\'ls5_nbar_albers\')\n    assert len(datasets) > 0\n    assert datasets[0].managed\n\n    check_open_with_api(index, len(valid_uuids))\n\n    # NetCDF specific checks, based on the saved NetCDF file\n    ds_path = str(datasets[0].local_path)\n    with netCDF4.Dataset(ds_path) as nco:\n        check_data_shape(nco)\n        check_grid_mapping(nco)\n        check_cf_compliance(nco)\n        check_dataset_metadata_in_storage_unit(nco, example_ls5_dataset_paths)\n        check_attributes(nco, config[\'global_attributes\'])\n\n        name = config[\'measurements\'][0][\'name\']\n        check_attributes(nco[name], config[\'measurements\'][0][\'attrs\'])\n    check_open_with_xarray(ds_path)\n\n\ndef ensure_datasets_are_indexed(index, valid_uuids):\n    datasets = index.datasets.search_eager(product=\'ls5_nbar_scene\')\n    assert len(datasets) == len(valid_uuids)\n    for dataset in datasets:\n        assert dataset.id in valid_uuids\n\n\ndef check_grid_mapping(nco):\n    assert \'grid_mapping\' in nco.variables[\'blue\'].ncattrs()\n    grid_mapping = nco.variables[\'blue\'].grid_mapping\n    assert grid_mapping in nco.variables\n    assert \'GeoTransform\' in nco.variables[grid_mapping].ncattrs()\n    assert \'spatial_ref\' in nco.variables[grid_mapping].ncattrs()\n\n\ndef check_data_shape(nco):\n    assert nco.variables[\'blue\'].shape == EXPECTED_STORAGE_UNIT_DATA_SHAPE\n\n\ndef check_cf_compliance(dataset):\n    try:\n        from compliance_checker.runner import CheckSuite, ComplianceChecker\n        import compliance_checker\n    except ImportError:\n        warnings.warn(\'compliance_checker unavailable, skipping NetCDF-CF Compliance Checks\')\n        return\n\n    if compliance_checker.__version__ < \'4.0.0\':\n        warnings.warn(\'Please upgrade compliance-checker to 4+ version\')\n        warnings.warn(\'compliance_checker version is too old, skipping NetCDF-CF Compliance Checks\')\n        return\n\n    skip = [\'check_dimension_order\',\n            \'check_all_features_are_same_type\',\n            \'check_conventions_version\',\n            \'check_appendix_a\']\n\n    cs = CheckSuite()\n    cs.load_all_available_checkers()\n    score_groups = cs.run(dataset, skip, \'cf\')\n    score_dict = {dataset.filepath(): score_groups}\n\n    groups = ComplianceChecker.stdout_output(cs, score_dict, verbose=1, limit=COMPLIANCE_CHECKER_NORMAL_LIMIT)\n    assert cs.passtree(groups, limit=COMPLIANCE_CHECKER_NORMAL_LIMIT)\n\n\ndef check_attributes(obj, attrs):\n    for k, v in attrs.items():\n        assert k in obj.ncattrs()\n        assert obj.getncattr(k) == v\n\n\ndef check_dataset_metadata_in_storage_unit(nco, dataset_dirs):\n    """"""Check one of the NetCDF files metadata against the original\n    metadata.""""""\n    assert len(nco.variables[\'dataset\']) == 1  # 1 time slice\n    stored_metadata = netcdf_extract_string(nco.variables[\'dataset\'][0])\n    stored = yaml.safe_load(stored_metadata)\n\n    assert \'lineage\' in stored\n    assert \'source_datasets\' in stored[\'lineage\']\n    assert \'0\' in stored[\'lineage\'][\'source_datasets\']\n    assert \'id\' in stored[\'lineage\'][\'source_datasets\'][\'0\']\n    source_uuid = UUID(stored[\'lineage\'][\'source_datasets\'][\'0\'][\'id\'])\n    assert source_uuid in dataset_dirs\n    ds_filename = dataset_dirs[source_uuid] / \'agdc-metadata.yaml\'\n    [(_, original)] = read_documents(ds_filename)\n    assert len(stored[\'lineage\'][\'source_datasets\']) == 1\n    assert next(iter(stored[\'lineage\'][\'source_datasets\'].values())) == original\n\n\ndef check_open_with_xarray(file_path):\n    import xarray\n    xarray.open_dataset(str(file_path))\n\n\ndef check_open_with_api(index, time_slices):\n    with rasterio.Env():\n        from datacube import Datacube\n        dc = Datacube(index=index)\n\n        input_type_name = \'ls5_nbar_albers\'\n        input_type = dc.index.products.get_by_name(input_type_name)\n        geobox = geometry.GeoBox(200, 200, Affine(25, 0.0, 638000, 0.0, -25, 6276000), geometry.CRS(\'EPSG:28355\'))\n        observations = dc.find_datasets(product=\'ls5_nbar_albers\', geopolygon=geobox.extent)\n        group_by = query_group_by(\'time\')\n        sources = dc.group_datasets(observations, group_by)\n        data = dc.load_data(sources, geobox, input_type.measurements.values())\n        assert data.blue.shape == (time_slices, 200, 200)\n\n        chunk_profile = {\'time\': 1, \'x\': 100, \'y\': 100}\n        lazy_data = dc.load_data(sources, geobox, input_type.measurements.values(), dask_chunks=chunk_profile)\n        assert lazy_data.blue.shape == (time_slices, 200, 200)\n        assert (lazy_data.blue.load() == data.blue).all()\n\n\ndef check_data_with_api(index, time_slices):\n    """"""Chek retrieved data for specific values.\n\n    We scale down by 100 and check for predefined values in the\n    corners.\n    """"""\n    from datacube import Datacube\n    dc = Datacube(index=index)\n\n    # TODO: this test needs to change, it tests that results are exactly the\n    #       same as some time before, but with the current zoom out factor it\'s\n    #       hard to verify that results are as expected even with human\n    #       judgement. What it should test is that reading native from the\n    #       ingested product gives exactly the same results as reading into the\n    #       same GeoBox from the original product. Separate to that there\n    #       should be a read test that confirms that what you read from native\n    #       product while changing projection is of expected value\n\n    # Make the retrieved data lower res\n    ss = 100\n    shape_x = int(GEOTIFF[\'shape\'][\'x\'] / ss)\n    shape_y = int(GEOTIFF[\'shape\'][\'y\'] / ss)\n    pixel_x = int(GEOTIFF[\'pixel_size\'][\'x\'] * ss)\n    pixel_y = int(GEOTIFF[\'pixel_size\'][\'y\'] * ss)\n\n    input_type_name = \'ls5_nbar_albers\'\n    input_type = dc.index.products.get_by_name(input_type_name)\n    geobox = geometry.GeoBox(shape_x + 2, shape_y + 2,\n                             Affine(pixel_x, 0.0, GEOTIFF[\'ul\'][\'x\'], 0.0, pixel_y, GEOTIFF[\'ul\'][\'y\']),\n                             geometry.CRS(GEOTIFF[\'crs\']))\n    observations = dc.find_datasets(product=\'ls5_nbar_albers\', geopolygon=geobox.extent)\n    group_by = query_group_by(\'time\')\n    sources = dc.group_datasets(observations, group_by)\n    data = dc.load_data(sources, geobox, input_type.measurements.values())\n    assert hashlib.md5(data.green.data).hexdigest() == \'0f64647bad54db4389fb065b2128025e\'\n    assert hashlib.md5(data.blue.data).hexdigest() == \'41a7b50dfe5c4c1a1befbc378225beeb\'\n    for time_slice in range(time_slices):\n        assert data.blue.values[time_slice][-1, -1] == -999\n'"
integration_tests/test_gridworkflow.py,0,"b'from datacube.api.grid_workflow import GridWorkflow\n\n\n# TODO: do we still need this now that driver manager is gone?\ndef test_create_gridworkflow_with_logging(index):\n    from logging import getLogger, StreamHandler\n\n    logger = getLogger(__name__)\n    handler = StreamHandler()\n    logger.addHandler(handler)\n\n    try:\n        gw = GridWorkflow(index)\n    finally:\n        logger.removeHandler(handler)\n'"
integration_tests/test_index_datasets_search.py,0,"b""import pytest\nfrom pathlib import PurePosixPath\n\nfrom integration_tests.test_full_ingestion import ensure_datasets_are_indexed\n\n\n@pytest.mark.parametrize('datacube_env_name', ('datacube',), indirect=True)\n@pytest.mark.usefixtures('default_metadata_type',\n                         'indexed_ls5_scene_products')\ndef test_index_datasets_search_light(index, tmpdir, clirunner,\n                                     example_ls5_dataset_paths):\n    def index_dataset(path):\n        return clirunner(['dataset', 'add', str(path)])\n\n    def index_products():\n        valid_uuids = []\n        for uuid, ls5_dataset_path in example_ls5_dataset_paths.items():\n            valid_uuids.append(uuid)\n            index_dataset(ls5_dataset_path)\n\n        # Ensure that datasets are actually indexed\n        ensure_datasets_are_indexed(index, valid_uuids)\n\n        return valid_uuids\n\n    valid_uuids = index_products()\n\n    # Test derived properties such as 'extent'\n    results = list(index.datasets.search_returning_datasets_light(field_names=('id', 'extent', 'time'),\n                                                                  product='ls5_nbar_scene'))\n    for dataset in results:\n        assert dataset.id in valid_uuids\n        # Assume projection is defined as\n        #         datum: GDA94\n        #         ellipsoid: GRS80\n        #         zone: -55\n        # for all datasets. This should give us epsg 28355\n        assert dataset.extent.crs.epsg == 28355\n\n    # test custom fields\n    results = list(index.datasets.search_returning_datasets_light(field_names=('id', 'zone'),\n                                                                  custom_offsets={'zone': ['grid_spatial',\n                                                                                           'projection', 'zone']},\n                                                                  product='ls5_nbar_scene'))\n    for dataset in results:\n        assert dataset.zone == -55\n\n    # Test conditional queries involving custom fields\n    results = list(index.datasets.search_returning_datasets_light(field_names=('id', 'zone'),\n                                                                  custom_offsets={'zone': ['grid_spatial',\n                                                                                           'projection', 'zone']},\n                                                                  product='ls5_nbar_scene',\n                                                                  zone='-55'))\n    assert len(results) > 0\n\n    results = list(index.datasets.search_returning_datasets_light(field_names=('id', 'zone'),\n                                                                  custom_offsets={'zone': ['grid_spatial',\n                                                                                           'projection', 'zone']},\n                                                                  product='ls5_nbar_scene',\n                                                                  zone='-65'))\n    assert len(results) == 0\n\n    # Test uris\n\n    # Test datasets with just one uri location\n    results_no_uri = list(index.datasets.search_returning_datasets_light(field_names=('id'),\n                                                                         product='ls5_nbar_scene'))\n    results_with_uri = list(index.datasets.search_returning_datasets_light(field_names=('id', 'uris'),\n                                                                           product='ls5_nbar_scene'))\n    assert len(results_no_uri) == len(results_with_uri)\n    for result in results_with_uri:\n        assert len(result.uris) == 1\n\n    # 'uri' field bahave same as 'uris' ('uri' could be deprecated!)\n    results_with_uri = list(index.datasets.search_returning_datasets_light(field_names=('id', 'uri'),\n                                                                           product='ls5_nbar_scene'))\n    assert len(results_no_uri) == len(results_with_uri)\n    for result in results_with_uri:\n        assert len(result.uri) == 1\n\n    # Add a new uri to a dataset\n    new_loc = PurePosixPath(tmpdir.strpath) / 'temp_location' / 'agdc-metadata.yaml'\n    index.datasets.add_location(valid_uuids[0], new_loc.as_uri())\n\n    results_with_uri = list(index.datasets.search_returning_datasets_light(field_names=('id', 'uris'),\n                                                                           product='ls5_nbar_scene',\n                                                                           id=valid_uuids[0]))\n    assert len(results_with_uri) == 1\n    assert len(results_with_uri[0].uris) == 2\n\n    results_with_uri = list(index.datasets.search_returning_datasets_light(field_names=('id', 'uri'),\n                                                                           product='ls5_nbar_scene',\n                                                                           id=valid_uuids[0]))\n    assert len(results_with_uri) == 1\n    assert len(results_with_uri[0].uri) == 2\n\n\n@pytest.mark.parametrize('datacube_env_name', ('datacube',), indirect=True)\n@pytest.mark.usefixtures('default_metadata_type',\n                         'indexed_ls5_scene_products')\ndef test_index_get_product_time_bounds(index, clirunner, example_ls5_dataset_paths):\n    def index_dataset(path):\n        return clirunner(['dataset', 'add', str(path)])\n\n    def index_products():\n        valid_uuids = []\n        for uuid, ls5_dataset_path in example_ls5_dataset_paths.items():\n            valid_uuids.append(uuid)\n            index_dataset(ls5_dataset_path)\n\n        # Ensure that datasets are actually indexed\n        ensure_datasets_are_indexed(index, valid_uuids)\n\n        return valid_uuids\n\n    valid_uuids = index_products()\n\n    # lets get time values\n    dataset_times = list(index.datasets.search_returning_datasets_light(field_names=('time',),\n                                                                        product='ls5_nbar_scene'))\n\n    # get time bounds\n    time_bounds = index.datasets.get_product_time_bounds(product='ls5_nbar_scene')\n    left = sorted(dataset_times, key=lambda dataset: dataset.time.lower)[0].time.lower\n    right = sorted(dataset_times, key=lambda dataset: dataset.time.upper)[-1].time.upper\n\n    assert left == time_bounds[0]\n    assert right == time_bounds[1]\n"""
integration_tests/test_index_out_of_bound.py,0,"b'# coding=utf-8\nimport pytest\n\nimport datacube\nfrom integration_tests.test_end_to_end import INGESTER_CONFIGS\nfrom integration_tests.test_full_ingestion import (check_open_with_api,\n                                                   ensure_datasets_are_indexed, check_data_shape,\n                                                   check_grid_mapping, check_cf_compliance, check_attributes,\n                                                   check_dataset_metadata_in_storage_unit,\n                                                   check_open_with_xarray)\nfrom integration_tests.utils import prepare_test_ingestion_configuration\nimport netCDF4\n\n\n@pytest.mark.timeout(20)\n@pytest.mark.parametrize(\'datacube_env_name\', (\'datacube\',), indirect=True)\n@pytest.mark.usefixtures(\'default_metadata_type\',\n                         \'indexed_ls5_scene_products\')\ndef test_index_out_of_bound_error(clirunner, index, tmpdir, example_ls5_dataset_paths, ingest_configs):\n    """"""\n    Test for the case where ingestor processes upto `--queue-size` number of tasks and not all the available scenes\n    """"""\n    # Make a test ingestor configuration\n    config = INGESTER_CONFIGS / ingest_configs[\'ls5_nbar_albers\']\n    config_path, config = prepare_test_ingestion_configuration(tmpdir, None,\n                                                               config, mode=\'fast_ingest\')\n\n    def index_dataset(path):\n        return clirunner([\'dataset\', \'add\', str(path)])\n\n    # Set the queue size to process 5 tiles\n    queue_size = 5\n    valid_uuids = []\n    for uuid, ls5_dataset_path in example_ls5_dataset_paths.items():\n        valid_uuids.append(uuid)\n        index_dataset(ls5_dataset_path)\n\n    # Ensure that datasets are actually indexed\n    ensure_datasets_are_indexed(index, valid_uuids)\n\n    # Locationless scenario within database arises when we run the sync tool (with --update-location option)\n    # on the disk where the actual file is removed and regenerated again with new dataset id.\n    for indexed_uuid in valid_uuids:\n        dc1 = datacube.Datacube(index=index)\n        datasets = dc1.find_datasets(product=\'ls5_nbar_scene\')\n        try:\n            # Remove location from the index, to simulate indexed out of range scenario\n            res = dc1.index.datasets.remove_location(indexed_uuid, datasets[0].local_uri)\n        except AttributeError:\n            # Do for one dataset, ignore any other attribute errors\n            pass\n        assert res is True, ""Error for %r. output: %r"" % (indexed_uuid, res)\n\n    # Ingest scenes with locationless dataset\n    clirunner([\n        \'ingest\',\n        \'--config-file\',\n        str(config_path),\n        \'--queue-size\',\n        queue_size,\n        \'--allow-product-changes\',\n    ])\n\n    # Validate that the ingestion is working as expected\n    datasets = index.datasets.search_eager(product=\'ls5_nbar_albers\')\n    assert len(datasets) > 0\n    assert datasets[0].managed\n\n    check_open_with_api(index, len(valid_uuids))\n\n    # NetCDF specific checks, based on the saved NetCDF file\n    ds_path = str(datasets[0].local_path)\n    with netCDF4.Dataset(ds_path) as nco:\n        check_data_shape(nco)\n        check_grid_mapping(nco)\n        check_cf_compliance(nco)\n        check_dataset_metadata_in_storage_unit(nco, example_ls5_dataset_paths)\n        check_attributes(nco, config[\'global_attributes\'])\n\n        name = config[\'measurements\'][0][\'name\']\n        check_attributes(nco[name], config[\'measurements\'][0][\'attrs\'])\n    check_open_with_xarray(ds_path)\n'"
integration_tests/test_model.py,0,"b'import pytest\nfrom datacube.model import Dataset, DatasetType\nfrom typing import List\n\n\ndef test_crs_parse(indexed_ls5_scene_products: List[DatasetType]) -> None:\n    product = indexed_ls5_scene_products[2]\n\n    # Explicit CRS, should load fine.\n    # Taken from LS8_OLI_NBAR_3577_-14_-11_20140601021126000000.nc\n    d = Dataset(product, {\n        ""grid_spatial"": {\n            ""projection"": {\n                ""valid_data"": {\n                    ""type"": ""Polygon"",\n                    ""coordinates"": [\n                        [[-1396453.986271351, -1100000.0], [-1400000.0, -1100000.0],\n                         [-1400000.0, -1053643.4714392645], [-1392296.4215373022, -1054399.795365491],\n                         [-1390986.9858215596, -1054531.808155645],\n                         [-1390806.366757733, -1054585.3982497198],\n                         [-1396453.986271351, -1100000.0]]\n                    ]\n                },\n                ""geo_ref_points"": {\n                    ""ll"": {""x"": -1400000.0, ""y"": -1100000.0},\n                    ""lr"": {""x"": -1300000.0, ""y"": -1100000.0},\n                    ""ul"": {""x"": -1400000.0, ""y"": -1000000.0},\n                    ""ur"": {""x"": -1300000.0, ""y"": -1000000.0}},\n                ""spatial_reference"": ""EPSG:3577""\n            }\n        }\n\n    })\n    assert str(d.crs) == \'EPSG:3577\'\n    assert d.extent is not None\n\n    def mk_ds(zone,  datum=""GDA94""):\n        return Dataset(product, {\n            ""grid_spatial"": {\n                ""projection"": {\n                    ""zone"": zone,\n                    ""datum"": datum,\n                    ""ellipsoid"": ""GRS80"",\n                    ""orientation"": ""NORTH_UP"",\n                    ""geo_ref_points"": {\n                        ""ll"": {""x"": 537437.5, ""y"": 5900512.5},\n                        ""lr"": {""x"": 781687.5, ""y"": 5900512.5},\n                        ""ul"": {""x"": 537437.5, ""y"": 6117112.5},\n                        ""ur"": {""x"": 781687.5, ""y"": 6117112.5}\n                    },\n                    ""map_projection"": ""UTM"",\n                    ""resampling_option"": ""CUBIC_CONVOLUTION""\n                }\n            }\n        })\n\n    # Valid datum/zone as seen on our LS5 scene, should infer crs.\n    ds = mk_ds(-51, ""GDA94"")\n    with pytest.warns(DeprecationWarning):\n        assert str(ds.crs) == \'EPSG:28351\'\n        assert ds.extent is not None\n\n    ds = mk_ds(""51S"", ""WGS84"")\n    with pytest.warns(DeprecationWarning):\n        assert str(ds.crs) == \'EPSG:32751\'\n        assert ds.extent is not None\n\n    ds = mk_ds(""51N"", ""WGS84"")\n    with pytest.warns(DeprecationWarning):\n        assert str(ds.crs) == \'EPSG:32651\'\n        assert ds.extent is not None\n\n    # Invalid datum/zone, can\'t infer\n    ds = mk_ds(-60, ""GDA94"")\n    # Prints warning: Can\'t figure out projection: possibly invalid zone (-60) for datum (\'GDA94\').""\n    # We still return None, rather than error, as they didn\'t specify a CRS explicitly\n    with pytest.warns(DeprecationWarning):\n        assert ds.crs is None\n\n    # No projection specified in the dataset\n    ds = Dataset(product, {})\n    assert ds.crs is None\n    assert ds.extent is None\n'"
integration_tests/test_validate_ingestion.py,0,"b'import pytest\n\nfrom integration_tests.utils import prepare_test_ingestion_configuration\nfrom integration_tests.test_end_to_end import PROJECT_ROOT\n\nCOMPLIANCE_CHECKER_NORMAL_LIMIT = 2\n\n\n@pytest.mark.timeout(20)\n@pytest.mark.parametrize(\'datacube_env_name\', (\'datacube\',), indirect=True)\n@pytest.mark.usefixtures(\'default_metadata_type\',\n                         \'indexed_ls5_scene_products\')\ndef test_invalid_ingestor_config(clirunner, index, tmpdir):\n    """"""\n    Test that the ingestor correctly identifies an invalid ingestor config file.\n\n    Note: We do not need to test valid config files as that is covered by the existing\n          ingestor tests.\n    """"""\n    base = PROJECT_ROOT / \'integration_tests/data/ingester/\'\n\n    for cfg, err in ((\'invalid_config.yaml\', ""\'src_varname\' is a required property""),\n                     (\'invalid_src_name.yaml\', \'No such variable in the source product:\')):\n        config = base / cfg\n        config_path, config = prepare_test_ingestion_configuration(tmpdir, None, config)\n\n        result = clirunner([\'ingest\', \'--config-file\', str(config_path)],\n                           expect_success=False)\n\n        assert result.exit_code != 0\n        assert err in result.output\n'"
integration_tests/utils.py,2,"b'import logging\nimport os\nimport shutil\nfrom contextlib import contextmanager\nfrom datetime import datetime\nfrom pathlib import Path\nimport numpy as np\nimport rasterio\nimport yaml\nfrom click.testing import CliRunner\n\nfrom datacube.utils.documents import load_from_yaml\n\n# On Windows, symlinks are not supported in Python 2 and require\n# specific privileges otherwise, so we copy instead of linking\nif os.name == \'nt\' or not hasattr(os, \'symlink\'):\n    symlink = shutil.copy\nelse:\n    symlink = os.symlink  # type: ignore\n\n#: Number of bands to place in generated GeoTIFFs\nNUM_BANDS = 3\n\n# Resolution and chunking shrink factors\nTEST_STORAGE_SHRINK_FACTORS = (100, 100)\nTEST_STORAGE_NUM_MEASUREMENTS = 2\nGEOGRAPHIC_VARS = (\'latitude\', \'longitude\')\nPROJECTED_VARS = (\'x\', \'y\')\n\nGEOTIFF = {\n    \'date\': datetime(1990, 3, 2),\n    \'shape\': {\n        \'x\': 432,\n        \'y\': 321\n    },\n    \'pixel_size\': {\n        \'x\': 25.0,\n        \'y\': -25.0\n    },\n    \'crs\': \'EPSG:28355\',  # \'EPSG:28355\'\n    \'ul\': {\n        \'x\': 638000.0,  # Coords must match crs\n        \'y\': 6276000.0  # Coords must match crs\n    }\n}\n\n\n@contextmanager\ndef alter_log_level(logger, level=logging.WARN):\n    previous_level = logger.getEffectiveLevel()\n    logger.setLevel(level)\n    yield\n    logger.setLevel(previous_level)\n\n\ndef assert_click_command(command, args):\n    result = CliRunner().invoke(\n        command,\n        args=args,\n        catch_exceptions=False\n    )\n    print(result.output)\n    assert not result.exception\n    assert result.exit_code == 0\n\n\ndef limit_num_measurements(dataset_type):\n    if \'measurements\' not in dataset_type:\n        return\n    measurements = dataset_type[\'measurements\']\n    if len(measurements) > TEST_STORAGE_NUM_MEASUREMENTS:\n        dataset_type[\'measurements\'] = measurements[:TEST_STORAGE_NUM_MEASUREMENTS]\n    return dataset_type\n\n\ndef prepare_test_ingestion_configuration(tmpdir,\n                                         output_dir,\n                                         filename,\n                                         mode=None):\n    customizers = {\n        \'fast_ingest\': edit_for_fast_ingest,\n        \'end2end\': edit_for_end2end,\n    }\n\n    filename = Path(filename)\n    if output_dir is None:\n        output_dir = tmpdir.ensure(filename.stem, dir=True)\n    config = load_yaml_file(filename)[0]\n\n    if mode is not None:\n        if mode not in customizers:\n            raise ValueError(\'Wrong mode: \' + mode)\n        config = customizers[mode](config)\n\n    config[\'location\'] = str(output_dir)\n\n    # If ingesting with the s3test driver\n    if \'bucket\' in config[\'storage\']:\n        config[\'storage\'][\'bucket\'] = str(output_dir)\n\n    config_path = tmpdir.join(filename.name)\n    with open(str(config_path), \'w\') as stream:\n        yaml.safe_dump(config, stream)\n    return config_path, config\n\n\ndef edit_for_end2end(config):\n    storage = config.get(\'storage\', {})\n\n    storage[\'crs\'] = \'EPSG:3577\'\n    storage[\'tile_size\'][\'x\'] = 100000.0\n    storage[\'tile_size\'][\'y\'] = 100000.0\n\n    config[\'storage\'] = storage\n    return config\n\n\ndef edit_for_fast_ingest(config):\n    config = alter_product_for_testing(config)\n    config[\'storage\'][\'crs\'] = \'EPSG:28355\'\n    config[\'storage\'][\'chunking\'][\'time\'] = 1\n    return config\n\n\ndef _make_geotiffs(tiffs_dir, day_offset, num_bands=NUM_BANDS):\n    """"""\n    Generate custom geotiff files, one per band.\n\n    Create ``num_bands`` TIFF files inside ``tiffs_dir``.\n\n    Return a dictionary mapping band_number to filename, eg::\n\n        {\n            0: \'/tmp/tiffs/band01_time01.tif\',\n            1: \'/tmp/tiffs/band02_time01.tif\'\n        }\n    """"""\n    tiffs = {}\n    width = GEOTIFF[\'shape\'][\'x\']\n    height = GEOTIFF[\'shape\'][\'y\']\n    metadata = {\'count\': 1,\n                \'crs\': GEOTIFF[\'crs\'],\n                \'driver\': \'GTiff\',\n                \'dtype\': \'int16\',\n                \'width\': width,\n                \'height\': height,\n                \'nodata\': -999.0,\n                \'transform\': [GEOTIFF[\'pixel_size\'][\'x\'],\n                              0.0,\n                              GEOTIFF[\'ul\'][\'x\'],\n                              0.0,\n                              GEOTIFF[\'pixel_size\'][\'y\'],\n                              GEOTIFF[\'ul\'][\'y\']]}\n\n    for band in range(num_bands):\n        path = str(tiffs_dir.join(\'band%02d_time%02d.tif\' % ((band + 1), day_offset)))\n        with rasterio.open(path, \'w\', **metadata) as dst:\n            # Write data in ""corners"" (rounded down by 100, for a size of 100x100)\n            data = np.zeros((height, width), dtype=np.int16)\n            data[:] = np.arange(height * width\n                                ).reshape((height, width)) + 10 * band + day_offset\n            \'\'\'\n            lr = (100 * int(GEOTIFF[\'shape\'][\'y\'] / 100.0),\n                  100 * int(GEOTIFF[\'shape\'][\'x\'] / 100.0))\n            data[0:100, 0:100] = 100 + day_offset\n            data[lr[0] - 100:lr[0], 0:100] = 200 + day_offset\n            data[0:100, lr[1] - 100:lr[1]] = 300 + day_offset\n            data[lr[0] - 100:lr[0], lr[1] - 100:lr[1]] = 400 + day_offset\n            \'\'\'\n            dst.write(data, 1)\n        tiffs[band] = path\n    return tiffs\n\n\ndef _make_ls5_scene_datasets(geotiffs, tmpdir):\n    """"""\n\n    Create directory structures like::\n\n        LS5_TM_NBAR_P54_GANBAR01-002_090_084_01\n        |---scene01\n        |     |----- report.txt\n        |     |----- LS5_TM_NBAR_P54_GANBAR01-002_090_084_01_B10.tif\n        |     |----- LS5_TM_NBAR_P54_GANBAR01-002_090_084_01_B20.tif\n        |     |----- LS5_TM_NBAR_P54_GANBAR01-002_090_084_01_B30.tif\n        |--- agdc-metadata.yaml\n\n    :param geotiffs: A list of dictionaries as output by :func:`geotiffs`\n    :param tmpdir:\n    :return:\n    """"""\n    dataset_dirs = {}\n    dataset_dir = tmpdir.mkdir(\'ls5_dataset\')\n    for geotiff in geotiffs:\n        # Make a directory for the dataset\n        obs_name = \'LS5_TM_NBAR_P54_GANBAR01-002_090_084_%s\' % geotiff[\'day\']\n        obs_dir = dataset_dir.mkdir(obs_name)\n        symlink(str(geotiff[\'path\']), str(obs_dir.join(\'agdc-metadata.yaml\')))\n\n        scene_dir = obs_dir.mkdir(\'scene01\')\n        scene_dir.join(\'report.txt\').write(\'Example\')\n        geotiff_name = \'%s_B{}0.tif\' % obs_name\n        for band in range(NUM_BANDS):\n            path = scene_dir.join(geotiff_name.format(band + 1))\n            symlink(str(geotiff[\'tiffs\'][band]), str(path))\n        dataset_dirs[geotiff[\'uuid\']] = Path(str(obs_dir))\n    return dataset_dirs\n\n\ndef load_yaml_file(filename):\n    with open(str(filename)) as f:\n        return list(load_from_yaml(f, parse_dates=True))\n\n\ndef is_geogaphic(storage_type):\n    return \'latitude\' in storage_type[\'storage\'][\'resolution\']\n\n\ndef shrink_storage_type(storage_type, variables, shrink_factors):\n    storage = storage_type[\'storage\']\n    for var in variables:\n        storage[\'resolution\'][var] = storage[\'resolution\'][var] * shrink_factors[0]\n        storage[\'chunking\'][var] = storage[\'chunking\'][var] / shrink_factors[1]\n    return storage_type\n\n\ndef load_test_products(filename, metadata_type=None):\n    dataset_types = load_yaml_file(filename)\n    return [alter_product_for_testing(dataset_type, metadata_type=metadata_type) for dataset_type in dataset_types]\n\n\ndef alter_product_for_testing(product, metadata_type=None):\n    limit_num_measurements(product)\n    if \'storage\' in product:\n        spatial_variables = GEOGRAPHIC_VARS if is_geogaphic(product) else PROJECTED_VARS\n        product = shrink_storage_type(product,\n                                      spatial_variables,\n                                      TEST_STORAGE_SHRINK_FACTORS)\n    if metadata_type:\n        product[\'metadata_type\'] = metadata_type.name\n    return product\n'"
tests/__init__.py,0,b''
tests/conftest.py,0,"b'""""""\npy.test configuration fixtures\n\nThis module defines any fixtures or other extensions to py.test to be used throughout the\ntests in this and sub packages.\n""""""\n\nimport os\n\nimport pytest\nfrom affine import Affine\n\nfrom datacube import Datacube\nfrom datacube.utils import geometry\nfrom datacube.utils.documents import read_documents\nfrom datacube.model import Measurement, MetadataType, DatasetType, Dataset\nfrom datacube.index.eo3 import prep_eo3\n\n\nAWS_ENV_VARS = (""AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN""\n                ""AWS_DEFAULT_REGION AWS_DEFAULT_OUTPUT AWS_PROFILE ""\n                ""AWS_ROLE_SESSION_NAME AWS_CA_BUNDLE ""\n                ""AWS_SHARED_CREDENTIALS_FILE AWS_CONFIG_FILE"").split("" "")\n\n\n@pytest.fixture\ndef example_gdal_path(data_folder):\n    """"""Return the pathname of a sample geotiff file\n\n    Use this fixture by specifiying an argument named \'example_gdal_path\' in your\n    test method.\n    """"""\n    return str(os.path.join(data_folder, \'sample_tile_151_-29.tif\'))\n\n\n@pytest.fixture\ndef no_crs_gdal_path(data_folder):\n    """"""Return the pathname of a GDAL file that doesn\'t contain a valid CRS.""""""\n    return str(os.path.join(data_folder, \'no_crs_ds.tif\'))\n\n\n@pytest.fixture\ndef data_folder():\n    """"""Return a string path to the location `test/data`""""""\n    return os.path.join(os.path.split(os.path.realpath(__file__))[0], \'data\')\n\n\n@pytest.fixture\ndef example_netcdf_path(request):\n    """"""Return a string path to `sample_tile.nc` in the test data dir""""""\n    return str(request.fspath.dirpath(\'data/sample_tile.nc\'))\n\n\n@pytest.fixture\ndef eo3_metadata_file(data_folder):\n    return os.path.join(data_folder, \'eo3.yaml\')\n\n\n@pytest.fixture\ndef eo3_metadata(eo3_metadata_file):\n    (_, doc), *_ = read_documents(eo3_metadata_file)\n    return MetadataType(doc)\n\n\n@pytest.fixture\ndef eo3_dataset_s2(eo3_metadata):\n    ds_doc = {\n        \'$schema\': \'https://schemas.opendatacube.org/dataset\',\n        \'id\': \'8b0e2770-5d4e-5238-8995-4aa91691ab85\',\n        \'product\': {\'name\': \'s2b_msil2a\'},\n        \'label\': \'S2B_MSIL2A_20200101T070219_N0213_R120_T39LVG_20200101T091825\',\n\n        \'crs\': \'epsg:32739\',\n        \'grids\': {\'g20m\': {\'shape\': [5490, 5490],\n                           \'transform\': [20, 0, 399960, 0, -20, 8700040, 0, 0, 1]},\n                  \'g60m\': {\'shape\': [1830, 1830],\n                           \'transform\': [60, 0, 399960, 0, -60, 8700040, 0, 0, 1]},\n                  \'default\': {\'shape\': [10980, 10980],\n                              \'transform\': [10, 0, 399960, 0, -10, 8700040, 0, 0, 1]}},\n        \'geometry\': {\'type\': \'Polygon\',\n                     \'coordinates\': [[[509759.0000000001, 8590241.0],\n                                      [399960.99999999977, 8590241.0],\n                                      [399960.99999999977, 8700039.0],\n                                      [509758.99999999965, 8700039.0],\n                                      [509759.0000000001, 8590241.0]]]},\n        \'properties\': {\'eo:gsd\': 10,\n                       \'datetime\': \'2020-01-01T07:02:54.188Z\',\n                       \'eo:platform\': \'sentinel-2b\',\n                       \'eo:instrument\': \'msi\',\n                       \'eo:cloud_cover\': 0,\n                       \'odc:file_format\': \'GeoTIFF\',\n                       \'odc:region_code\': \'39LVG\',\n                       \'odc:processing_datetime\': \'2020-01-01T07:02:54.188Z\'},\n\n        \'measurements\': {\'red\': {\'path\': \'B04.tif\'},\n                         \'scl\': {\'grid\': \'g20m\', \'path\': \'SCL.tif\'},\n                         \'blue\': {\'path\': \'B02.tif\'},\n                         \'green\': {\'path\': \'B03.tif\'},\n                         \'nir_1\': {\'path\': \'B08.tif\'},\n                         \'nir_2\': {\'grid\': \'g20m\', \'path\': \'B8A.tif\'},\n                         \'swir_1\': {\'grid\': \'g20m\', \'path\': \'B11.tif\'},\n                         \'swir_2\': {\'grid\': \'g20m\', \'path\': \'B12.tif\'},\n                         \'red_edge_1\': {\'grid\': \'g20m\', \'path\': \'B05.tif\'},\n                         \'red_edge_2\': {\'grid\': \'g20m\', \'path\': \'B06.tif\'},\n                         \'red_edge_3\': {\'grid\': \'g20m\', \'path\': \'B07.tif\'},\n                         \'water_vapour\': {\'grid\': \'g60m\', \'path\': \'B09.tif\'},\n                         \'coastal_aerosol\': {\'grid\': \'g60m\', \'path\': \'B01.tif\'}},\n        \'lineage\': {}}\n    product_doc = {\n        \'name\': \'s2b_msil2a\',\n        \'description\': \'Sentinel-2B Level 2 COGs\',\n        \'metadata_type\': \'eo3\',\n        \'metadata\': {\'product\': {\'name\': \'s2b_msil2a\'}},\n        \'measurements\':\n        [{\'name\': \'coastal_aerosol\', \'dtype\': \'uint16\', \'units\': \'1\', \'nodata\': 0, \'aliases\': [\'band_01\', \'B01\']},\n         {\'name\': \'blue\', \'dtype\': \'uint16\', \'units\': \'1\', \'nodata\': 0, \'aliases\': [\'band_02\', \'B02\']},\n         {\'name\': \'green\', \'dtype\': \'uint16\', \'units\': \'1\', \'nodata\': 0, \'aliases\': [\'band_03\', \'B03\']},\n         {\'name\': \'red\', \'dtype\': \'uint16\', \'units\': \'1\', \'nodata\': 0, \'aliases\': [\'band_04\', \'B04\']},\n         {\'name\': \'red_edge_1\', \'dtype\': \'uint16\', \'units\': \'1\', \'nodata\': 0, \'aliases\': [\'band_05\', \'B05\']},\n         {\'name\': \'red_edge_2\', \'dtype\': \'uint16\', \'units\': \'1\', \'nodata\': 0, \'aliases\': [\'band_06\', \'B06\']},\n         {\'name\': \'red_edge_3\', \'dtype\': \'uint16\', \'units\': \'1\', \'nodata\': 0, \'aliases\': [\'band_07\', \'B07\']},\n         {\'name\': \'nir_1\', \'dtype\': \'uint16\', \'units\': \'1\', \'nodata\': 0, \'aliases\': [\'band_08\', \'B08\']},\n         {\'name\': \'nir_2\', \'dtype\': \'uint16\', \'units\': \'1\', \'nodata\': 0, \'aliases\': [\'band_8a\', \'B8A\']},\n         {\'name\': \'water_vapour\', \'dtype\': \'uint16\', \'units\': \'1\', \'nodata\': 0, \'aliases\': [\'band_09\', \'B09\']},\n         {\'name\': \'swir_1\', \'dtype\': \'uint16\', \'units\': \'1\', \'nodata\': 0, \'aliases\': [\'band_11\', \'B11\']},\n         {\'name\': \'swir_2\', \'dtype\': \'uint16\', \'units\': \'1\', \'nodata\': 0, \'aliases\': [\'band_12\', \'B12\']},\n         {\'name\': \'scl\', \'dtype\': \'uint8\', \'units\': \'1\', \'nodata\': 0, \'aliases\': [\'mask\', \'qa\'],\n          \'flags_definition\': {\'sca\': {\'description\': \'Sen2Cor Scene Classification\',\n                                       \'bits\': [0, 1, 2, 3, 4, 5, 6, 7],\n                                       \'values\': {\n                                           \'0\': \'nodata\',\n                                           \'1\': \'defective\',\n                                           \'2\': \'dark\',\n                                           \'3\': \'shadow\',\n                                           \'4\': \'vegetation\',\n                                           \'5\': \'bare\',\n                                           \'6\': \'water\',\n                                           \'7\': \'unclassified\',\n                                           \'8\': \'cloud medium probability\',\n                                           \'9\': \'cloud high probability\',\n                                           \'10\': \'thin cirrus\',\n                                           \'11\': \'snow or ice\'}}}}]\n    }\n\n    return Dataset(DatasetType(eo3_metadata, product_doc), prep_eo3(ds_doc))\n\n\nnetcdf_num = 1\n\n\n@pytest.fixture\ndef tmpnetcdf_filename(tmpdir):\n    """"""Return a generated filename for a non-existant netcdf file""""""\n    global netcdf_num\n    filename = str(tmpdir.join(\'testfile_np_%s.nc\' % netcdf_num))\n    netcdf_num += 1\n    return filename\n\n\n@pytest.fixture\ndef odc_style_xr_dataset():\n    """"""An xarray.Dataset with ODC style coordinates and CRS, and no time dimension.\n\n    Contains an EPSG:4326, single variable \'B10\' of 100x100 int16 pixels.""""""\n    affine = Affine.scale(0.1, 0.1) * Affine.translation(20, 30)\n    geobox = geometry.GeoBox(100, 100, affine, geometry.CRS(GEO_PROJ))\n\n    return Datacube.create_storage({}, geobox, [Measurement(name=\'B10\', dtype=\'int16\', nodata=0, units=\'1\')])\n\n\n@pytest.fixture\ndef without_aws_env(monkeypatch):\n    for e in AWS_ENV_VARS:\n        monkeypatch.delenv(e, raising=False)\n\n\nGEO_PROJ = \'GEOGCS[""WGS 84"",DATUM[""WGS_1984"",SPHEROID[""WGS 84"",6378137,298.257223563,AUTHORITY[""EPSG"",""7030""]],\' \\\n           \'AUTHORITY[""EPSG"",""6326""]],PRIMEM[""Greenwich"",0],UNIT[""degree"",0.0174532925199433],\' \\\n           \'AUTHORITY[""EPSG"",""4326""]]\'\n\n\n@pytest.fixture(scope=""module"")\ndef dask_client():\n    from distributed import Client\n    client = Client(processes=False,\n                    threads_per_worker=1,\n                    dashboard_address=None)\n    yield client\n    client.close()\n    del client\n'"
tests/test_concurrent_executor.py,0,"b'""""""\nTests for MultiprocessingExecutor\n""""""\n\nfrom datacube.executor import get_executor\nfrom time import sleep\nimport pytest\n\nDATA = [1, 2, 3, 4]\nRETRIES = 5\n\n\ndef _echo(x, please_fail=False):\n    if please_fail or x == \'please fail\':\n        raise IOError(\'Fake I/O error, because you asked\')\n    return x\n\n\ndef run_executor_tests(executor, sleep_time=1):\n    # get_ready: mostly pending\n    futures = executor.map(_echo, DATA)\n    assert len(futures) == len(DATA)\n    completed, failed, pending = executor.get_ready(futures)\n    assert len(failed) == 0\n    assert len(completed) + len(pending) == len(DATA)\n\n    # get_ready: processed + failure\n    data = [\'please fail\'] + DATA\n    futures = executor.map(_echo, data)\n    assert len(futures) == len(data)\n\n    for _ in range(RETRIES):\n        sleep(sleep_time)  # give it ""enough"" time to finish\n        completed, failed, pending = executor.get_ready(futures)\n        if len(pending) == 0:\n            break\n\n    if sleep_time:\n        assert len(failed) == 1\n    else:\n        assert len(failed) in [0, 1]\n\n    assert len(completed) + len(pending) + len(failed) == len(data)\n\n    # test results\n    futures = executor.map(_echo, DATA)\n    results = executor.results(futures)\n\n    assert len(results) == len(DATA)\n    assert set(results) == set(DATA)\n\n    # Test failure pass-through\n    future = executor.submit(_echo, """", please_fail=True)\n\n    for ff in executor.as_completed([future]):\n        with pytest.raises(IOError):\n            executor.result(ff)\n\n    # Next completed with data\n    future = executor.submit(_echo, \'tt\')\n    futures = [future]\n    result, futures = executor.next_completed(futures, \'default\')\n    assert len(futures) == 0\n    print(type(result), result)\n\n    # Next completed with empty list\n    result, futures = executor.next_completed([], \'default\')\n    assert result == \'default\'\n    assert len(futures) == 0\n\n    executor.release(future)\n\n\ndef test_concurrent_executor():\n    executor = get_executor(None, 2)\n    assert \'Multiproc\' in str(executor)\n    run_executor_tests(executor)\n\n    executor = get_executor(None, 2, use_cloud_pickle=False)\n    assert \'Multiproc\' in str(executor)\n    run_executor_tests(executor)\n\n\ndef test_fallback_executor():\n    executor = get_executor(None, None)\n    assert \'Serial\' in str(executor)\n\n    run_executor_tests(executor, sleep_time=0)\n'"
tests/test_config.py,0,"b'# coding=utf-8\n""""""\nModule\n""""""\nimport pytest\nimport configparser\nfrom textwrap import dedent\n\nfrom datacube.config import LocalConfig, parse_connect_url, parse_env_params, auto_config\nfrom datacube.testutils import write_files\n\n\ndef test_find_config():\n    files = write_files({\n        \'base.conf\': dedent(""""""\\\n            [datacube]\n            db_hostname: fakehost.test.lan\n        """"""),\n        \'override.conf\': dedent(""""""\\\n            [datacube]\n            db_hostname: overridden.test.lan\n            db_database: overridden_db\n        """""")\n    })\n\n    # One config file\n    config = LocalConfig.find(paths=[str(files.joinpath(\'base.conf\'))])\n    assert config[\'db_hostname\'] == \'fakehost.test.lan\'\n    # Not set: uses default\n    assert config[\'db_database\'] == \'datacube\'\n\n    # Now two config files, with the latter overriding earlier options.\n    config = LocalConfig.find(paths=[str(files.joinpath(\'base.conf\')),\n                                     str(files.joinpath(\'override.conf\'))])\n    assert config[\'db_hostname\'] == \'overridden.test.lan\'\n    assert config[\'db_database\'] == \'overridden_db\'\n\n\nconfig_sample = """"""\n[default]\ndb_database: datacube\n\n# A blank host will use a local socket. Specify a hostname (such as localhost) to use TCP.\ndb_hostname:\n\n# Credentials are optional: you might have other Postgres authentication configured.\n# The default username is the current user id\n# db_username:\n# A blank password will fall back to default postgres driver authentication, such as reading your ~/.pgpass file.\n# db_password:\nindex_driver: pg\n\n\n## Development environment ##\n[dev]\n# These fields are all the defaults, so they could be omitted, but are here for reference\n\ndb_database: datacube\n\n# A blank host will use a local socket. Specify a hostname (such as localhost) to use TCP.\ndb_hostname:\n\n# Credentials are optional: you might have other Postgres authentication configured.\n# The default username is the current user id\n# db_username:\n# A blank password will fall back to default postgres driver authentication, such as reading your ~/.pgpass file.\n# db_password:\n\n## Staging environment ##\n[staging]\ndb_hostname: staging.dea.ga.gov.au\n""""""\n\n\ndef test_using_configparser(tmpdir):\n    sample_config = tmpdir.join(\'datacube.conf\')\n    sample_config.write(config_sample)\n\n    config = configparser.ConfigParser()\n    config.read(str(sample_config))\n\n\ndef test_empty_configfile(tmpdir):\n    default_only = """"""[default]""""""\n    sample_file = tmpdir.join(\'datacube.conf\')\n    sample_file.write(default_only)\n    config = configparser.ConfigParser()\n    config.read(str(sample_file))\n\n\ndef test_parse_db_url():\n\n    assert parse_connect_url(\'postgresql:///db\') == dict(database=\'db\', hostname=\'\')\n    assert parse_connect_url(\'postgresql://some.tld/db\') == dict(database=\'db\', hostname=\'some.tld\')\n    assert parse_connect_url(\'postgresql://some.tld:3344/db\') == dict(\n        database=\'db\',\n        hostname=\'some.tld\',\n        port=\'3344\')\n    assert parse_connect_url(\'postgresql://user@some.tld:3344/db\') == dict(\n        username=\'user\',\n        database=\'db\',\n        hostname=\'some.tld\',\n        port=\'3344\')\n    assert parse_connect_url(\'postgresql://user:pass@some.tld:3344/db\') == dict(\n        password=\'pass\',\n        username=\'user\',\n        database=\'db\',\n        hostname=\'some.tld\',\n        port=\'3344\')\n\n    # check urlencode is reversed for password field\n    assert parse_connect_url(\'postgresql://user:pass%40@some.tld:3344/db\') == dict(\n        password=\'pass@\',\n        username=\'user\',\n        database=\'db\',\n        hostname=\'some.tld\',\n        port=\'3344\')\n\n\ndef _clear_cfg_env(monkeypatch):\n    for e in (\'DATACUBE_DB_URL\',\n              \'DB_HOSTNAME\',\n              \'DB_PORT\',\n              \'DB_USERNAME\',\n              \'DB_PASSWORD\'):\n        monkeypatch.delenv(e, raising=False)\n\n\ndef test_parse_env(monkeypatch):\n    def set_env(**kw):\n        _clear_cfg_env(monkeypatch)\n        for e, v in kw.items():\n            monkeypatch.setenv(e, v)\n\n    def check_env(**kw):\n        set_env(**kw)\n        return parse_env_params()\n\n    assert check_env() == {}\n    assert check_env(DATACUBE_DB_URL=\'postgresql:///db\') == dict(\n        hostname=\'\',\n        database=\'db\'\n    )\n    assert check_env(DATACUBE_DB_URL=\'postgresql://uu:%20pass%40@host.tld:3344/db\') == dict(\n        username=\'uu\',\n        password=\' pass@\',\n        hostname=\'host.tld\',\n        port=\'3344\',\n        database=\'db\'\n    )\n    assert check_env(DB_DATABASE=\'db\') == dict(\n        database=\'db\'\n    )\n    assert check_env(DB_DATABASE=\'db\', DB_HOSTNAME=\'host.tld\') == dict(\n        database=\'db\',\n        hostname=\'host.tld\'\n    )\n    assert check_env(DB_DATABASE=\'db\',\n                     DB_HOSTNAME=\'host.tld\',\n                     DB_USERNAME=\'user\',\n                     DB_PASSWORD=\'pass@\') == dict(\n                         database=\'db\',\n                         hostname=\'host.tld\',\n                         username=\'user\',\n                         password=\'pass@\')\n\n    assert check_env(DB_DATABASE=\'db\',\n                     DB_HOSTNAME=\'host.tld\',\n                     DB_USERNAME=\'user\',\n                     DB_PORT=\'\',\n                     DB_PASSWORD=\'pass@\') == dict(\n                         database=\'db\',\n                         hostname=\'host.tld\',\n                         username=\'user\',\n                         password=\'pass@\')\n\n\ndef test_cfg_from_env(monkeypatch):\n    def set_env(**kw):\n        _clear_cfg_env(monkeypatch)\n        for e, v in kw.items():\n            monkeypatch.setenv(e, v)\n\n    set_env(DATACUBE_DB_URL=\'postgresql://uu:%20pass%40@host.tld:3344/db\')\n    cfg = LocalConfig.find()\n    assert \'3344\' in str(cfg)\n    assert \'3344\' in repr(cfg)\n    assert cfg[\'db_username\'] == \'uu\'\n    assert cfg[\'db_password\'] == \' pass@\'\n    assert cfg[\'db_hostname\'] == \'host.tld\'\n    assert cfg[\'db_database\'] == \'db\'\n    assert cfg[\'db_port\'] == \'3344\'\n\n    # check that password is redacted\n    assert "" pass@"" not in str(cfg)\n    assert ""***"" in str(cfg)\n    assert "" pass@"" not in repr(cfg)\n    assert ""***"" in repr(cfg)\n\n    set_env(DB_DATABASE=\'dc2\',\n            DB_HOSTNAME=\'remote.db\',\n            DB_PORT=\'4433\',\n            DB_USERNAME=\'dcu\',\n            DB_PASSWORD=\'gg\')\n    cfg = LocalConfig.find()\n    assert cfg[\'db_username\'] == \'dcu\'\n    assert cfg[\'db_password\'] == \'gg\'\n    assert cfg[\'db_hostname\'] == \'remote.db\'\n    assert cfg[\'db_database\'] == \'dc2\'\n    assert cfg[\'db_port\'] == \'4433\'\n\n\ndef test_auto_config(monkeypatch, tmpdir):\n    from pathlib import Path\n\n    cfg_file = Path(str(tmpdir/""dc.cfg""))\n    assert cfg_file.exists() is False\n    cfg_file_name = str(cfg_file)\n\n    _clear_cfg_env(monkeypatch)\n    monkeypatch.setenv(\'DATACUBE_CONFIG_PATH\', cfg_file_name)\n\n    assert auto_config() == cfg_file_name\n    assert cfg_file.exists() is True\n\n    monkeypatch.setenv(\'DB_HOSTNAME\', \'should-not-be-used.local\')\n    # second run should skip overwriting\n    assert auto_config() == cfg_file_name\n\n    config = LocalConfig.find(paths=cfg_file_name)\n    assert config[\'db_hostname\'] == \'\'\n    assert config[\'db_database\'] == \'datacube\'\n\n    cfg_file.unlink()\n    assert cfg_file.exists() is False\n    _clear_cfg_env(monkeypatch)\n\n    monkeypatch.setenv(\'DATACUBE_CONFIG_PATH\', cfg_file_name)\n    monkeypatch.setenv(\'DB_HOSTNAME\', \'some.db\')\n    monkeypatch.setenv(\'DB_USERNAME\', \'user\')\n\n    assert auto_config() == cfg_file_name\n    config = LocalConfig.find(paths=cfg_file_name)\n    assert config[\'db_hostname\'] == \'some.db\'\n    assert config[\'db_database\'] == \'datacube\'\n    assert config[\'db_username\'] == \'user\'\n\n    assert config.get(\'no_such_key\', 10) == 10\n    with pytest.raises(configparser.NoOptionError):\n        config.get(\'no_such_key\')\n'"
tests/test_driver.py,0,"b""import pytest\nimport yaml\n\nfrom types import SimpleNamespace\n\nfrom datacube.drivers import new_datasource, reader_drivers, writer_drivers\nfrom datacube.drivers import index_drivers, index_driver_by_name\nfrom datacube.drivers.indexes import IndexDriverCache\nfrom datacube.storage import BandInfo\nfrom datacube.storage._rio import RasterDatasetDataSource\nfrom datacube.testutils import mk_sample_dataset\nfrom datacube.model import MetadataType\n\n\ndef test_new_datasource_fallback():\n    bands = [dict(name='green',\n                  path='')]\n    dataset = mk_sample_dataset(bands, 'file:///foo', format='GeoTiff')\n\n    assert dataset.uri_scheme == 'file'\n\n    rdr = new_datasource(BandInfo(dataset, 'green'))\n    assert rdr is not None\n    assert isinstance(rdr, RasterDatasetDataSource)\n\n\ndef test_reader_drivers():\n    available_drivers = reader_drivers()\n    assert isinstance(available_drivers, list)\n\n\ndef test_writer_drivers():\n    available_drivers = writer_drivers()\n    assert 'netcdf' in available_drivers\n    assert 'NetCDF CF' in available_drivers\n\n\ndef test_index_drivers():\n    available_drivers = index_drivers()\n    assert 'default' in available_drivers\n\n\ndef test_default_injection():\n    cache = IndexDriverCache('datacube.plugins.index-no-such-prefix')\n    assert cache.drivers() == ['default']\n\n\ndef test_netcdf_driver_import():\n    try:\n        import datacube.drivers.netcdf.driver\n    except ImportError:\n        assert False and 'Failed to load netcdf writer driver'\n\n    assert datacube.drivers.netcdf.driver.reader_driver_init is not None\n\n\ndef test_writer_driver_mk_uri():\n    from datacube.drivers.netcdf.driver import NetcdfWriterDriver\n    writer_driver = NetcdfWriterDriver()\n\n    assert writer_driver.uri_scheme == 'file'\n\n    file_path = '/path/to/my_file.nc'\n    driver_alias = 'NetCDF CF'\n    storage_config = {'driver': driver_alias}\n    file_uri = writer_driver.mk_uri(file_path=file_path, storage_config=storage_config)\n    assert file_uri == f'file://{file_path}'\n\n\ndef test_metadata_type_from_doc():\n    metadata_doc = yaml.safe_load('''\nname: minimal\ndescription: minimal metadata definition\ndataset:\n    id: [id]\n    sources: [lineage, source_datasets]\n    label: [label]\n    creation_dt: [creation_dt]\n    search_fields:\n        some_custom_field:\n            description: some custom field\n            offset: [a,b,c,custom]\n    ''')\n\n    for name in index_drivers():\n        driver = index_driver_by_name(name)\n        metadata = driver.metadata_type_from_doc(metadata_doc)\n        assert isinstance(metadata, MetadataType)\n        assert metadata.id is None\n        assert metadata.name == 'minimal'\n        assert 'some_custom_field' in metadata.dataset_fields\n\n\ndef test_reader_cache_throws_on_missing_fallback():\n    from datacube.drivers.readers import rdr_cache\n\n    rdrs = rdr_cache()\n    assert rdrs is not None\n\n    with pytest.raises(KeyError):\n        rdrs('file', 'aint-such-format')\n\n\ndef test_driver_singleton():\n    from datacube.drivers._tools import singleton_setup\n    from unittest.mock import MagicMock\n\n    result = object()\n    factory = MagicMock(return_value=result)\n    obj = SimpleNamespace()\n\n    assert singleton_setup(obj, 'xx', factory) is result\n    assert singleton_setup(obj, 'xx', factory) is result\n    assert singleton_setup(obj, 'xx', factory) is result\n    assert obj.xx is result\n\n    factory.assert_called_once_with()\n"""
tests/test_eo3.py,0,"b'from affine import Affine\nimport pytest\nfrom datacube.utils.documents import parse_yaml\nfrom datacube.testutils import mk_sample_product\nfrom datacube.model import Dataset\n\nfrom datacube.index.eo3 import (\n    prep_eo3,\n    eo3_lonlat_bbox,\n    add_eo3_parts,\n    is_doc_eo3,\n    grid2points,\n)\n\nSAMPLE_DOC = \'\'\'---\n$schema: https://schemas.opendatacube.org/dataset\nid: 7d41a4d0-2ab3-4da1-a010-ef48662ae8ef\ncrs: ""EPSG:3857""\ngrids:\n    default:\n       shape: [100, 200]\n       transform: [10, 0, 100000, 0, -10, 200000, 0, 0, 1]\nlineage:\n  src_a: [\'7cf53cb3-5da7-483f-9f12-6056e3290b4e\']\n  src_b:\n    - \'f5b9f582-d5ff-43c0-a49b-ef175abe429c\'\n    - \'7f8c6e8e-6f6b-4513-a11c-efe466405509\'\n  src_empty: []\n...\n\'\'\'\n\n# Crosses lon=180 line in Pacific, taken from one the Landsat scenes\n# https://landsat-pds.s3.amazonaws.com/c1/L8/074/071/LC08_L1TP_074071_20190622_20190704_01_T1/index.html\n#\nSAMPLE_DOC_180 = \'\'\'---\n$schema: https://schemas.opendatacube.org/dataset\nid: f884df9b-4458-47fd-a9d2-1a52a2db8a1a\ncrs: ""EPSG:32660""\ngrids:\n    default:\n       shape: [7811, 7691]\n       transform: [30, 0, 618285, 0, -30, -1642485, 0, 0, 1]\n    pan:\n       shape: [15621, 15381]\n       transform: [15, 0, 618292.5, 0, -15, -1642492.5, 0, 0, 1]\nlineage: {}\n...\n\'\'\'\n\n\n@pytest.fixture\ndef sample_doc():\n    return parse_yaml(SAMPLE_DOC)\n\n\n@pytest.fixture\ndef sample_doc_180():\n    return parse_yaml(SAMPLE_DOC_180)\n\n\n@pytest.fixture\ndef eo3_product(eo3_metadata):\n    return mk_sample_product(""eo3_product"", metadata_type=eo3_metadata)\n\n\ndef test_grid2points():\n    identity = list(Affine.translation(0, 0))\n    grid = dict(shape=(11, 22),\n                transform=identity)\n\n    pts = grid2points(grid)\n    assert len(pts) == 4\n    assert pts == [(0, 0), (22, 0), (22, 11), (0, 11)]\n    pts_ = grid2points(grid, ring=True)\n    assert len(pts_) == 5\n    assert pts == pts_[:4]\n    assert pts_[0] == pts_[-1]\n\n    grid[\'transform\'] = tuple(Affine.translation(100, 0))\n    pts = grid2points(grid)\n    assert pts == [(100, 0), (122, 0), (122, 11), (100, 11)]\n\n    for bad in [{},\n                dict(shape=(1, 1)),\n                dict(transform=identity)]:\n        with pytest.raises(ValueError):\n            grid2points(bad)\n\n\ndef test_is_eo3(sample_doc, sample_doc_180):\n    identity = list(Affine.translation(0, 0))\n    assert is_doc_eo3(sample_doc) is True\n    assert is_doc_eo3(sample_doc_180) is True\n\n    # If there\'s no schema field at all, it\'s treated as legacy eo.\n    assert is_doc_eo3({}) is False\n    assert is_doc_eo3({\'crs\': \'EPSG:4326\'}) is False\n    assert is_doc_eo3({\'crs\': \'EPSG:4326\', \'grids\': {}}) is False\n\n    with pytest.raises(ValueError, match=""Unsupported dataset schema.*""):\n        is_doc_eo3({\'$schema\': \'https://schemas.opendatacube.org/eo4\'})\n\n\ndef test_add_eo3(sample_doc, sample_doc_180, eo3_product):\n    doc = add_eo3_parts(sample_doc)\n    assert doc is not sample_doc\n    ds = Dataset(eo3_product, doc)\n    assert ds.crs == \'EPSG:3857\'\n    assert ds.extent is not None\n    assert ds.extent.crs == \'EPSG:3857\'\n    assert ds.metadata.lat.begin < ds.metadata.lat.end\n    assert ds.metadata.lon.begin < ds.metadata.lon.end\n\n    doc = dict(**sample_doc,\n               geometry=ds.extent.buffer(-1).json)\n\n    ds2 = Dataset(eo3_product, add_eo3_parts(doc))\n    assert ds2.crs == \'EPSG:3857\'\n    assert ds2.extent is not None\n    assert ds2.extent.crs == \'EPSG:3857\'\n    assert ds2.metadata.lat.begin < ds2.metadata.lat.end\n    assert ds2.metadata.lon.begin < ds2.metadata.lon.end\n    assert ds.extent.contains(ds2.extent)\n\n    doc = add_eo3_parts(sample_doc_180)\n    assert doc is not sample_doc_180\n    ds = Dataset(eo3_product, doc)\n    assert ds.crs == \'EPSG:32660\'\n    assert ds.extent is not None\n    assert ds.extent.crs == \'EPSG:32660\'\n    assert ds.metadata.lat.begin < ds.metadata.lat.end\n    assert ds.metadata.lon.begin < 180 < ds.metadata.lon.end\n\n    doc = dict(**sample_doc)\n    doc.pop(\'crs\')\n    with pytest.raises(ValueError):\n        add_eo3_parts(doc)\n\n    doc = dict(**sample_doc)\n    doc.pop(\'grids\')\n    with pytest.raises(ValueError):\n        add_eo3_parts(doc)\n\n    with pytest.raises(ValueError):\n        eo3_lonlat_bbox({})\n\n\ndef test_prep_eo3(sample_doc, sample_doc_180, eo3_metadata):\n    rdr = eo3_metadata.dataset_reader(prep_eo3(sample_doc))\n    assert rdr.grid_spatial is not None\n    assert rdr.lat.end > rdr.lat.begin\n    assert rdr.lon.end > rdr.lon.begin\n    assert \'src_a\' in rdr.sources\n    assert \'src_b1\' in rdr.sources\n    assert \'src_b2\' in rdr.sources\n    assert \'src_empty\' not in rdr.sources\n\n    rdr = eo3_metadata.dataset_reader(prep_eo3(sample_doc_180))\n    assert rdr.grid_spatial is not None\n    assert rdr.sources == {}\n    assert rdr.lat.end > rdr.lat.begin\n    assert rdr.lon.end > rdr.lon.begin\n    assert rdr.lon.begin < 180 < rdr.lon.end\n\n    non_eo3_doc = {}\n    assert prep_eo3(None) is None\n    assert prep_eo3(non_eo3_doc, auto_skip=True) is non_eo3_doc\n\n    with pytest.raises(ValueError):\n        prep_eo3(non_eo3_doc)\n'"
tests/test_gbox_ops.py,3,"b'from affine import Affine\nimport numpy as np\nimport pytest\nfrom datacube.utils.geometry import gbox as gbx\nfrom datacube.utils import geometry\nfrom datacube.utils.geometry import GeoBox\n\nepsg3857 = geometry.CRS(\'EPSG:3857\')\n\n\ndef test_gbox_ops():\n    s = GeoBox(1000, 100, Affine(10, 0, 12340, 0, -10, 316770), epsg3857)\n    assert s.shape == (100, 1000)\n\n    d = gbx.flipy(s)\n    assert d.shape == s.shape\n    assert d.crs is s.crs\n    assert d.resolution == (-s.resolution[0], s.resolution[1])\n    assert d.extent.contains(s.extent)\n    with pytest.raises(ValueError):\n        # flipped grid\n        (s | d)\n    with pytest.raises(ValueError):\n        # flipped grid\n        (s & d)\n\n    d = gbx.flipx(s)\n    assert d.shape == s.shape\n    assert d.crs is s.crs\n    assert d.resolution == (s.resolution[0], -s.resolution[1])\n    assert d.extent.contains(s.extent)\n\n    assert gbx.flipy(gbx.flipy(s)).affine == s.affine\n    assert gbx.flipx(gbx.flipx(s)).affine == s.affine\n\n    d = gbx.zoom_out(s, 2)\n    assert d.shape == (50, 500)\n    assert d.crs is s.crs\n    assert d.extent.contains(s.extent)\n    assert d.resolution == (s.resolution[0]*2, s.resolution[1]*2)\n\n    d = gbx.zoom_out(s, 2*max(s.shape))\n    assert d.shape == (1, 1)\n    assert d.crs is s.crs\n    assert d.extent.contains(s.extent)\n\n    d = gbx.zoom_out(s, 1.33719)\n    assert d.crs is s.crs\n    assert d.extent.contains(s.extent)\n    assert all(ds < ss for ds, ss in zip(d.shape, s.shape))\n    with pytest.raises(ValueError):\n        # lower resolution grid\n        (s | d)\n    with pytest.raises(ValueError):\n        # lower resolution grid\n        (s & d)\n\n    d = gbx.zoom_to(s, s.shape)\n    assert d == s\n\n    d = gbx.zoom_to(s, (1, 3))\n    assert d.shape == (1, 3)\n    assert d.extent == s.extent\n\n    d = gbx.zoom_to(s, (10000, 10000))\n    assert d.shape == (10000, 10000)\n    assert d.extent == s.extent\n\n    d = gbx.pad(s, 1)\n    assert d.crs is s.crs\n    assert d.resolution == s.resolution\n    assert d.extent.contains(s.extent)\n    assert s.extent.contains(d.extent) is False\n    assert d[1:-1, 1:-1].affine == s.affine\n    assert d[1:-1, 1:-1].shape == s.shape\n    assert d == (s | d)\n    assert s == (s & d)\n\n    d = gbx.pad_wh(s, 10)\n    assert d == s\n\n    d = gbx.pad_wh(s, 100, 8)\n    assert d.width == s.width\n    assert d.height % 8 == 0\n    assert 0 < d.height - s.height < 8\n    assert d.affine == s.affine\n    assert d.crs is s.crs\n\n    d = gbx.pad_wh(s, 13, 17)\n    assert d.affine == s.affine\n    assert d.crs is s.crs\n    assert d.height % 17 == 0\n    assert d.width % 13 == 0\n    assert 0 < d.height - s.height < 17\n    assert 0 < d.width - s.width < 13\n\n    d = gbx.translate_pix(s, 1, 2)\n    assert d.crs is s.crs\n    assert d.resolution == s.resolution\n    assert d.extent != s.extent\n    assert s[2:3, 1:2].extent == d[:1, :1].extent\n\n    d = gbx.translate_pix(s, -10, -2)\n    assert d.crs is s.crs\n    assert d.resolution == s.resolution\n    assert d.extent != s.extent\n    assert s[:1, :1].extent == d[2:3, 10:11].extent\n\n    d = gbx.translate_pix(s, 0.1, 0)\n    assert d.crs is s.crs\n    assert d.shape == s.shape\n    assert d.resolution == s.resolution\n    assert d.extent != s.extent\n    assert d.extent.contains(s[:, 1:].extent)\n\n    d = gbx.translate_pix(s, 0, -0.5)\n    assert d.crs is s.crs\n    assert d.shape == s.shape\n    assert d.resolution == s.resolution\n    assert d.extent != s.extent\n    assert s.extent.contains(d[1:, :].extent)\n\n    d = gbx.affine_transform_pix(s, Affine(1, 0, 0,\n                                           0, 1, 0))\n    assert d.crs is s.crs\n    assert d.shape == s.shape\n    assert d.resolution == s.resolution\n    assert d.extent == s.extent\n\n    d = gbx.rotate(s, 180)\n    assert d.crs is s.crs\n    assert d.shape == s.shape\n    assert d.extent != s.extent\n    np.testing.assert_almost_equal(d.extent.area, s.extent.area, 1e-5)\n    assert s[49:52, 499:502].extent.contains(d[50:51, 500:501].extent), ""Check that center pixel hasn\'t moved""\n\n\ndef test_gbox_tiles():\n    A = Affine.identity()\n    H, W = (300, 200)\n    h, w = (10, 20)\n    gbox = GeoBox(W, H, A, epsg3857)\n    tt = gbx.GeoboxTiles(gbox, (h, w))\n    assert tt.shape == (300/10, 200/20)\n    assert tt.base is gbox\n\n    assert tt[0, 0] == gbox[0:h, 0:w]\n    assert tt[0, 1] == gbox[0:h, w:w+w]\n\n    assert tt[0, 0] is tt[0, 0]  # Should cache exact same object\n    assert tt[4, 1].shape == (h, w)\n\n    H, W = (11, 22)\n    h, w = (10, 9)\n    gbox = GeoBox(W, H, A, epsg3857)\n    tt = gbx.GeoboxTiles(gbox, (h, w))\n    assert tt.shape == (2, 3)\n    assert tt[1, 2] == gbox[10:11, 18:22]\n\n    for idx in [tt.shape, (-1, 0), (0, -1), (-33, 1)]:\n        with pytest.raises(IndexError):\n            tt[idx]\n\n        with pytest.raises(IndexError):\n            tt.chunk_shape(idx)\n\n    cc = np.zeros(tt.shape, dtype=\'int32\')\n    for idx in tt.tiles(gbox.extent):\n        cc[idx] += 1\n    np.testing.assert_array_equal(cc, np.ones(tt.shape))\n\n    assert list(tt.tiles(gbox[:h, :w].extent)) == [(0, 0)]\n\n    (H, W) = (11, 22)\n    (h, w) = (10, 20)\n    tt = gbx.GeoboxTiles(GeoBox(W, H, A, epsg3857), (h, w))\n    assert tt.chunk_shape((0, 0)) == (h, w)\n    assert tt.chunk_shape((0, 1)) == (h, 2)\n    assert tt.chunk_shape((1, 1)) == (1, 2)\n    assert tt.chunk_shape((1, 0)) == (1, w)\n'"
tests/test_geometry.py,54,"b'import math\nimport numpy as np\nfrom mock import MagicMock\nfrom affine import Affine\nimport pytest\nfrom pytest import approx\nimport pickle\n\nfrom datacube.utils import geometry\nfrom datacube.utils.geometry import (\n    GeoBox,\n    CRS,\n    CRSMismatchError,\n    BoundingBox,\n    common_crs,\n    multigeom,\n    bbox_union,\n    crs_units_per_degree,\n    decompose_rws,\n    affine_from_pts,\n    get_scale_at_point,\n    native_pix_transform,\n    scaled_down_geobox,\n    compute_reproject_roi,\n    roi_normalise,\n    roi_shape,\n    split_translation,\n    is_affine_st,\n    apply_affine,\n    compute_axis_overlap,\n    roi_is_empty,\n    clip_lon180,\n    chop_along_antimeridian,\n    projected_lon,\n    w_,\n)\nfrom datacube.utils.geometry._base import (\n    _mk_crs_coord,\n    bounding_box_in_pixel_domain,\n    geobox_intersection_conservative,\n    geobox_union_conservative,\n    _guess_crs_str,\n    force_2d,\n    _align_pix,\n    _round_to_res,\n    _norm_crs,\n    _norm_crs_or_error,\n)\nfrom datacube.testutils.geom import (\n    epsg4326,\n    epsg3577,\n    epsg3857,\n    AlbersGS,\n    mkA,\n    xy_from_gbox,\n    xy_norm,\n    to_fixed_point,\n    from_fixed_point,\n    gen_test_image_xy,\n    SAMPLE_WKT_WITHOUT_AUTHORITY,\n)\n\n\ndef test_pickleable():\n    poly = geometry.polygon([(10, 20), (20, 20), (20, 10), (10, 20)], crs=epsg4326)\n    pickled = pickle.dumps(poly, pickle.HIGHEST_PROTOCOL)\n    unpickled = pickle.loads(pickled)\n    assert poly == unpickled\n\n\ndef test_geobox_simple():\n    from affine import Affine\n    t = geometry.GeoBox(4000, 4000,\n                        Affine(0.00025, 0.0, 151.0, 0.0, -0.00025, -29.0),\n                        epsg4326)\n\n    expect_lon = np.asarray([151.000125, 151.000375, 151.000625, 151.000875, 151.001125,\n                             151.001375, 151.001625, 151.001875, 151.002125, 151.002375])\n\n    expect_lat = np.asarray([-29.000125, -29.000375, -29.000625, -29.000875, -29.001125,\n                             -29.001375, -29.001625, -29.001875, -29.002125, -29.002375])\n    expect_resolution = np.asarray([-0.00025, 0.00025])\n\n    assert t.coordinates[\'latitude\'].values.shape == (4000,)\n    assert t.coordinates[\'longitude\'].values.shape == (4000,)\n\n    np.testing.assert_almost_equal(t.resolution, expect_resolution)\n    np.testing.assert_almost_equal(t.coords[\'latitude\'].values[:10], expect_lat)\n    np.testing.assert_almost_equal(t.coords[\'longitude\'].values[:10], expect_lon)\n\n    assert (t == ""some random thing"") is False\n\n    # ensure GeoBox accepts string CRS\n    assert isinstance(geometry.GeoBox(4000, 4000,\n                                      Affine(0.00025, 0.0, 151.0, 0.0, -0.00025, -29.0),\n                                      \'epsg:4326\').crs, CRS)\n\n\ndef test_props():\n    crs = epsg4326\n\n    box1 = geometry.box(10, 10, 30, 30, crs=crs)\n    assert box1\n    assert box1.is_valid\n    assert not box1.is_empty\n    assert box1.area == 400.0\n    assert box1.boundary.length == 80.0\n    assert box1.centroid == geometry.point(20, 20, crs)\n\n    triangle = geometry.polygon([(10, 20), (20, 20), (20, 10), (10, 20)], crs=crs)\n    assert triangle.boundingbox == geometry.BoundingBox(10, 10, 20, 20)\n    assert triangle.envelope.contains(triangle)\n\n    assert box1.length == 80.0\n\n    box1copy = geometry.box(10, 10, 30, 30, crs=crs)\n    assert box1 == box1copy\n    assert box1.convex_hull == box1copy  # NOTE: this might fail because of point order\n\n    box2 = geometry.box(20, 10, 40, 30, crs=crs)\n    assert box1 != box2\n\n    bbox = geometry.BoundingBox(1, 0, 10, 13)\n    assert bbox.width == 9\n    assert bbox.height == 13\n    assert bbox.points == [(1, 0), (1, 13), (10, 0), (10, 13)]\n\n    assert bbox.transform(Affine.identity()) == bbox\n    assert bbox.transform(Affine.translation(1, 2)) == geometry.BoundingBox(2, 2, 11, 15)\n\n    pt = geometry.point(3, 4, crs)\n    assert pt.json[\'coordinates\'] == (3.0, 4.0)\n    assert \'Point\' in str(pt)\n    assert bool(pt) is True\n    assert pt.__nonzero__() is True\n\n    # check ""CRS as string is converted to class automatically""\n    assert isinstance(geometry.point(3, 4, \'epsg:3857\').crs, geometry.CRS)\n\n    # constructor with bad input should raise ValueError\n    with pytest.raises(ValueError):\n        geometry.Geometry(object())\n\n\ndef test_tests():\n    box1 = geometry.box(10, 10, 30, 30, crs=epsg4326)\n    box2 = geometry.box(20, 10, 40, 30, crs=epsg4326)\n    box3 = geometry.box(30, 10, 50, 30, crs=epsg4326)\n    box4 = geometry.box(40, 10, 60, 30, crs=epsg4326)\n    minibox = geometry.box(15, 15, 25, 25, crs=epsg4326)\n\n    assert not box1.touches(box2)\n    assert box1.touches(box3)\n    assert not box1.touches(box4)\n\n    assert box1.intersects(box2)\n    assert box1.intersects(box3)\n    assert not box1.intersects(box4)\n\n    assert not box1.crosses(box2)\n    assert not box1.crosses(box3)\n    assert not box1.crosses(box4)\n\n    assert not box1.disjoint(box2)\n    assert not box1.disjoint(box3)\n    assert box1.disjoint(box4)\n\n    assert box1.contains(minibox)\n    assert not box1.contains(box2)\n    assert not box1.contains(box3)\n    assert not box1.contains(box4)\n\n    assert minibox.within(box1)\n    assert not box1.within(box2)\n    assert not box1.within(box3)\n    assert not box1.within(box4)\n\n\ndef test_ops():\n    box1 = geometry.box(10, 10, 30, 30, crs=epsg4326)\n    box2 = geometry.box(20, 10, 40, 30, crs=epsg4326)\n    box3 = geometry.box(20, 10, 40, 30, crs=epsg4326)\n    box4 = geometry.box(40, 10, 60, 30, crs=epsg4326)\n    no_box = None\n\n    assert box1 != box2\n    assert box2 == box3\n    assert box3 != no_box\n\n    union1 = box1.union(box2)\n    assert union1.area == 600.0\n\n    with pytest.raises(geometry.CRSMismatchError):\n        box1.union(box2.to_crs(epsg3857))\n\n    inter1 = box1.intersection(box2)\n    assert bool(inter1)\n    assert inter1.area == 200.0\n\n    inter2 = box1.intersection(box4)\n    assert not bool(inter2)\n    assert inter2.is_empty\n    # assert not inter2.is_valid  TODO: what\'s going on here?\n\n    diff1 = box1.difference(box2)\n    assert diff1.area == 200.0\n\n    symdiff1 = box1.symmetric_difference(box2)\n    assert symdiff1.area == 400.0\n\n    # test segmented\n    line = geometry.line([(0, 0), (0, 5), (10, 5)], epsg4326)\n    line2 = line.segmented(2)\n    assert line.crs is line2.crs\n    assert line.length == line2.length\n    assert len(line.coords) < len(line2.coords)\n    poly = geometry.polygon([(0, 0), (0, 5), (10, 5)], epsg4326)\n    poly2 = poly.segmented(2)\n    assert poly.crs is poly2.crs\n    assert poly.length == poly2.length\n    assert poly.area == poly2.area\n    assert len(poly.geom.exterior.coords) < len(poly2.geom.exterior.coords)\n\n    poly2 = poly.exterior.segmented(2)\n    assert poly.crs is poly2.crs\n    assert poly.length == poly2.length\n    assert len(poly.geom.exterior.coords) < len(poly2.geom.coords)\n\n    # test interpolate\n    pt = line.interpolate(1)\n    assert pt.crs is line.crs\n    assert pt.coords[0] == (0, 1)\n    assert isinstance(pt.coords, list)\n\n    with pytest.raises(TypeError):\n        pt.interpolate(3)\n\n    # test array interface\n    assert line.__array_interface__ is not None\n    assert np.array(line).shape == (3, 2)\n\n    # test simplify\n    poly = geometry.polygon([(0, 0), (0, 5), (10, 5)], epsg4326)\n    assert poly.simplify(100) == poly\n\n    # test iteration\n    poly_2_parts = geometry.Geometry({\n        ""type"": ""MultiPolygon"",\n        ""coordinates"": [[[[102.0, 2.0], [103.0, 2.0], [103.0, 3.0], [102.0, 3.0], [102.0, 2.0]]],\n                        [[[100.0, 0.0], [101.0, 0.0], [101.0, 1.0], [100.0, 1.0], [100.0, 0.0]],\n                         [[100.2, 0.2], [100.8, 0.2], [100.8, 0.8], [100.2, 0.8], [100.2, 0.2]]]]},\n        \'EPSG:4326\')\n    pp = list(poly_2_parts)\n    assert len(pp) == 2\n    assert all(p.crs == poly_2_parts.crs for p in pp)\n\n    # test transform\n    assert geometry.point(0, 0, epsg4326).transform(lambda x, y: (x+1, y+2)) == geometry.point(1, 2, epsg4326)\n\n    # test sides\n    box = geometry.box(1, 2, 11, 22, epsg4326)\n    ll = list(geometry.sides(box))\n    assert all(l.crs is epsg4326 for l in ll)\n    assert len(ll) == 4\n    assert ll[0] == geometry.line([(1, 2), (1, 22)], epsg4326)\n    assert ll[1] == geometry.line([(1, 22), (11, 22)], epsg4326)\n    assert ll[2] == geometry.line([(11, 22), (11, 2)], epsg4326)\n    assert ll[3] == geometry.line([(11, 2), (1, 2)], epsg4326)\n\n\ndef test_geom_split():\n    box = geometry.box(0, 0, 10, 30, epsg4326)\n    line = geometry.line([(5, 0), (5, 30)], epsg4326)\n    bb = list(box.split(line))\n    assert len(bb) == 2\n    assert box.contains(bb[0] | bb[1])\n    assert (box ^ (bb[0] | bb[1])).is_empty\n\n    with pytest.raises(CRSMismatchError):\n        list(box.split(geometry.line([(5, 0), (5, 30)], epsg3857)))\n\n\ndef test_common_crs():\n    assert common_crs([]) is None\n    assert common_crs([geometry.point(0, 0, epsg4326),\n                       geometry.line([(0, 0), (1, 1)], epsg4326)]) is epsg4326\n\n    with pytest.raises(CRSMismatchError):\n        common_crs([geometry.point(0, 0, epsg4326),\n                    geometry.line([(0, 0), (1, 1)], epsg3857)])\n\n\ndef test_multigeom():\n    p1, p2 = (0, 0), (1, 2)\n    p3, p4 = (3, 4), (5, 6)\n    b1 = geometry.box(*p1, *p2, epsg4326)\n    b2 = geometry.box(*p3, *p4, epsg4326)\n    bb = multigeom([b1, b2])\n    assert bb.type == \'MultiPolygon\'\n    assert bb.crs is b1.crs\n    assert len(list(bb)) == 2\n\n    g1 = geometry.line([p1, p2], None)\n    g2 = geometry.line([p3, p4], None)\n    gg = multigeom(iter([g1, g2, g1]))\n    assert gg.type == \'MultiLineString\'\n    assert gg.crs is g1.crs\n    assert len(list(gg)) == 3\n\n    g1 = geometry.point(*p1, epsg3857)\n    g2 = geometry.point(*p2, epsg3857)\n    g3 = geometry.point(*p3, epsg3857)\n    gg = multigeom(iter([g1, g2, g3]))\n    assert gg.type == \'MultiPoint\'\n    assert gg.crs is g1.crs\n    assert len(list(gg)) == 3\n    assert list(gg)[0] == g1\n    assert list(gg)[1] == g2\n    assert list(gg)[2] == g3\n\n    # can\'t mix types\n    with pytest.raises(ValueError):\n        multigeom([geometry.line([p1, p2], None), geometry.point(*p1, None)])\n\n    # can\'t mix CRSs\n    with pytest.raises(CRSMismatchError):\n        multigeom([geometry.line([p1, p2], epsg4326),\n                   geometry.line([p3, p4], epsg3857)])\n\n    # only some types are supported on input\n    with pytest.raises(ValueError):\n        multigeom([gg])\n\n\ndef test_shapely_wrappers():\n    poly = geometry.polygon([(0, 0), (0, 5), (10, 5)], epsg4326)\n\n    assert isinstance(poly.svg(), str)\n    assert isinstance(poly._repr_svg_(), str)\n\n    with_hole = poly.buffer(1) - poly\n    assert len(poly.interiors) == 0\n    assert len(with_hole.interiors) == 1\n    assert isinstance(with_hole.interiors, list)\n    assert isinstance(with_hole.interiors[0], geometry.Geometry)\n    assert with_hole.interiors[0].crs == with_hole.crs\n    assert poly.exterior.crs == poly.crs\n\n    x, y = poly.exterior.xy\n    assert len(x) == len(y)\n    assert x.typecode == y.typecode\n    assert x.typecode == \'d\'\n\n    assert (poly | poly) == poly\n    assert (poly & poly) == poly\n    assert (poly ^ poly).is_empty\n    assert (poly - poly).is_empty\n\n\ndef test_to_crs():\n    poly = geometry.polygon([(0, 0), (0, 5), (10, 5)], epsg4326)\n    num_points = 3\n    assert poly.crs is epsg4326\n    assert poly.to_crs(epsg3857).crs is epsg3857\n    assert poly.to_crs(\'EPSG:3857\').crs == \'EPSG:3857\'\n    assert poly.to_crs(\'EPSG:3857\', 0.1).crs == epsg3857\n\n    assert poly.exterior.to_crs(epsg3857) == poly.to_crs(epsg3857).exterior\n\n    # test that by default segmentation happens\n    # +1 is because exterior loops back to start point\n    assert len(poly.to_crs(epsg3857).exterior.xy[0]) > num_points + 1\n\n    # test that +inf disables segmentation\n    # +1 is because exterior loops back to start point\n    assert len(poly.to_crs(epsg3857, float(\'+inf\')).exterior.xy[0]) == num_points + 1\n\n    # test the segmentation works on multi-polygons\n    mpoly = (geometry.box(0, 0, 1, 3, \'EPSG:4326\') |\n             geometry.box(2, 4, 3, 6, \'EPSG:4326\'))\n\n    assert mpoly.type == \'MultiPolygon\'\n    assert mpoly.to_crs(epsg3857).type == \'MultiPolygon\'\n\n    poly = geometry.polygon([(0, 0), (0, 5), (10, 5)], None)\n    assert poly.crs is None\n    with pytest.raises(ValueError):\n        poly.to_crs(epsg3857)\n\n\ndef test_boundingbox():\n    bb = BoundingBox(0, 3, 2, 4)\n    assert bb.width == 2\n    assert bb.height == 1\n    assert bb.width == bb.span_x\n    assert bb.height == bb.span_y\n\n    bb = BoundingBox(0, 3, 2.1, 4)\n    assert bb.width == 2\n    assert bb.height == 1\n    assert bb.span_x == 2.1\n    assert bb.width != bb.span_x\n    assert bb.height == bb.span_y\n\n    assert BoundingBox.from_xy(bb.range_x, bb.range_y) == bb\n\n    assert BoundingBox.from_xy((1, 2), (10, 20)) == (1, 10, 2, 20)\n    assert BoundingBox.from_xy((2, 1), (20, 10)) == (1, 10, 2, 20)\n    assert BoundingBox.from_points((1, 11), (2, 22)) == (1, 11, 2, 22)\n    assert BoundingBox.from_points((1, 22), (2, 11)) == (1, 11, 2, 22)\n\n\ndef test_densify():\n    from datacube.utils.geometry._base import densify\n\n    s_x10 = [(0, 0), (10, 0)]\n    assert densify(s_x10, 20) == s_x10\n    assert densify(s_x10, 200) == s_x10\n    assert densify(s_x10, 5) == [(0, 0), (5, 0), (10, 0)]\n    assert densify(s_x10, 4) == [(0, 0), (4, 0), (8, 0), (10, 0)]\n\n\ndef test_bbox_union():\n    b1 = BoundingBox(0, 1, 10, 20)\n    b2 = BoundingBox(5, 6, 11, 22)\n\n    assert bbox_union([b1]) == b1\n    assert bbox_union([b2]) == b2\n\n    bb = bbox_union(iter([b1, b2]))\n    assert bb == BoundingBox(0, 1, 11, 22)\n\n    bb = bbox_union(iter([b2, b1]*10))\n    assert bb == BoundingBox(0, 1, 11, 22)\n\n\ndef test_unary_union():\n    box1 = geometry.box(10, 10, 30, 30, crs=epsg4326)\n    box2 = geometry.box(20, 10, 40, 30, crs=epsg4326)\n    box3 = geometry.box(30, 10, 50, 30, crs=epsg4326)\n    box4 = geometry.box(40, 10, 60, 30, crs=epsg4326)\n\n    union0 = geometry.unary_union([box1])\n    assert union0 == box1\n\n    union1 = geometry.unary_union([box1, box4])\n    assert union1.type == \'MultiPolygon\'\n    assert union1.area == 2.0 * box1.area\n\n    union2 = geometry.unary_union([box1, box2])\n    assert union2.type == \'Polygon\'\n    assert union2.area == 1.5 * box1.area\n\n    union3 = geometry.unary_union([box1, box2, box3, box4])\n    assert union3.type == \'Polygon\'\n    assert union3.area == 2.5 * box1.area\n\n    union4 = geometry.unary_union([union1, box2, box3])\n    assert union4.type == \'Polygon\'\n    assert union4.area == 2.5 * box1.area\n\n    assert geometry.unary_union([]) is None\n\n    with pytest.raises(ValueError):\n        geometry.unary_union([box1, box1.to_crs(epsg3577)])\n\n\ndef test_unary_intersection():\n    box1 = geometry.box(10, 10, 30, 30, crs=epsg4326)\n    box2 = geometry.box(15, 10, 35, 30, crs=epsg4326)\n    box3 = geometry.box(20, 10, 40, 30, crs=epsg4326)\n    box4 = geometry.box(25, 10, 45, 30, crs=epsg4326)\n    box5 = geometry.box(30, 10, 50, 30, crs=epsg4326)\n    box6 = geometry.box(35, 10, 55, 30, crs=epsg4326)\n\n    inter1 = geometry.unary_intersection([box1])\n    assert bool(inter1)\n    assert inter1 == box1\n\n    inter2 = geometry.unary_intersection([box1, box2])\n    assert bool(inter2)\n    assert inter2.area == 300.0\n\n    inter3 = geometry.unary_intersection([box1, box2, box3])\n    assert bool(inter3)\n    assert inter3.area == 200.0\n\n    inter4 = geometry.unary_intersection([box1, box2, box3, box4])\n    assert bool(inter4)\n    assert inter4.area == 100.0\n\n    inter5 = geometry.unary_intersection([box1, box2, box3, box4, box5])\n    assert bool(inter5)\n    assert inter5.type == \'LineString\'\n    assert inter5.length == 20.0\n\n    inter6 = geometry.unary_intersection([box1, box2, box3, box4, box5, box6])\n    assert not bool(inter6)\n    assert inter6.is_empty\n\n\nclass TestCRSEqualityComparisons(object):\n    def test_comparison_edge_cases(self):\n        a = epsg4326\n        none_crs = None\n        assert a == a\n        assert a == str(a)\n        assert (a == none_crs) is False\n        assert (a == []) is False\n        assert (a == TestCRSEqualityComparisons) is False\n\n    def test_australian_albers_comparison(self):\n        a = geometry.CRS(""""""PROJCS[""GDA94_Australian_Albers"",GEOGCS[""GCS_GDA_1994"",\n                            DATUM[""Geocentric_Datum_of_Australia_1994"",SPHEROID[""GRS_1980"",6378137,298.257222101]],\n                            PRIMEM[""Greenwich"",0],UNIT[""Degree"",0.017453292519943295]],\n                            PROJECTION[""Albers_Conic_Equal_Area""],\n                            PARAMETER[""standard_parallel_1"",-18],\n                            PARAMETER[""standard_parallel_2"",-36],\n                            PARAMETER[""latitude_of_center"",0],\n                            PARAMETER[""longitude_of_center"",132],\n                            PARAMETER[""false_easting"",0],\n                            PARAMETER[""false_northing"",0],\n                            UNIT[""Meter"",1]]"""""")\n        b = epsg3577\n\n        assert a == b\n\n        assert a != epsg4326\n\n\ndef test_no_epsg():\n    c = geometry.CRS(\'+proj=longlat +no_defs +ellps=GRS80\')\n    b = geometry.CRS(""""""GEOGCS[""GRS 1980(IUGG, 1980)"",DATUM[""unknown"",SPHEROID[""GRS80"",6378137,298.257222101]],\n                        PRIMEM[""Greenwich"",0],UNIT[""degree"",0.0174532925199433]]"""""")\n\n    assert c.epsg is None\n    assert b.epsg is None\n\n\ndef test_xy_from_geobox():\n    gbox = GeoBox(3, 7, Affine.translation(10, 1000), epsg3857)\n    xx, yy = xy_from_gbox(gbox)\n\n    assert xx.shape == gbox.shape\n    assert yy.shape == gbox.shape\n    assert (xx[:, 0] == 10.5).all()\n    assert (xx[:, 1] == 11.5).all()\n    assert (yy[0, :] == 1000.5).all()\n    assert (yy[6, :] == 1006.5).all()\n\n    xx_, yy_, A = xy_norm(xx, yy)\n    assert xx_.shape == xx.shape\n    assert yy_.shape == yy.shape\n    np.testing.assert_almost_equal((xx_.min(), xx_.max()), (0, 1))\n    np.testing.assert_almost_equal((yy_.min(), yy_.max()), (0, 1))\n    assert (xx_[0] - xx_[1]).sum() != 0\n    assert (xx_[:, 0] - xx_[:, 1]).sum() != 0\n\n    XX, YY = apply_affine(A, xx_, yy_)\n    np.testing.assert_array_almost_equal(xx, XX)\n    np.testing.assert_array_almost_equal(yy, YY)\n\n\ndef test_gen_test_image_xy():\n    gbox = GeoBox(3, 7, Affine.translation(10, 1000), epsg3857)\n\n    xy, denorm = gen_test_image_xy(gbox, \'float64\')\n    assert xy.dtype == \'float64\'\n    assert xy.shape == (2,) + gbox.shape\n\n    x, y = denorm(xy)\n    x_, y_ = xy_from_gbox(gbox)\n\n    np.testing.assert_almost_equal(x, x_)\n    np.testing.assert_almost_equal(y, y_)\n\n    xy, denorm = gen_test_image_xy(gbox, \'uint16\')\n    assert xy.dtype == \'uint16\'\n    assert xy.shape == (2,) + gbox.shape\n\n    x, y = denorm(xy[0], xy[1])\n    assert x.shape == xy.shape[1:]\n    assert y.shape == xy.shape[1:]\n    assert x.dtype == \'float64\'\n\n    x_, y_ = xy_from_gbox(gbox)\n\n    np.testing.assert_almost_equal(x, x_, 4)\n    np.testing.assert_almost_equal(y, y_, 4)\n\n    for dt in (\'int8\', np.int16, np.dtype(np.uint64)):\n        xy, _ = gen_test_image_xy(gbox, dt)\n        assert xy.dtype == dt\n\n    # check no-data\n    xy, denorm = gen_test_image_xy(gbox, \'float32\')\n    assert xy.dtype == \'float32\'\n    assert xy.shape == (2,) + gbox.shape\n    xy[0, 0, :] = np.nan\n    xy[1, 1, :] = np.nan\n    xy_ = denorm(xy, nodata=np.nan)\n    assert np.isnan(xy_[:, :2]).all()\n    np.testing.assert_almost_equal(xy_[0][2:], x_[2:], 6)\n    np.testing.assert_almost_equal(xy_[1][2:], y_[2:], 6)\n\n    xy, denorm = gen_test_image_xy(gbox, \'int16\')\n    assert xy.dtype == \'int16\'\n    assert xy.shape == (2,) + gbox.shape\n    xy[0, 0, :] = -999\n    xy[1, 1, :] = -999\n    xy_ = denorm(xy, nodata=-999)\n    assert np.isnan(xy_[:, :2]).all()\n    np.testing.assert_almost_equal(xy_[0][2:], x_[2:], 4)\n    np.testing.assert_almost_equal(xy_[1][2:], y_[2:], 4)\n\n    # call without arguments should return linear mapping\n    A = denorm()\n    assert isinstance(A, Affine)\n\n\ndef test_fixed_point():\n    aa = np.asarray([0, 0.5, 1])\n    uu = to_fixed_point(aa, \'uint8\')\n    assert uu.dtype == \'uint8\'\n    assert aa.shape == uu.shape\n    assert tuple(uu.ravel()) == (0, 128, 255)\n\n    aa_ = from_fixed_point(uu)\n    assert aa_.dtype == \'float64\'\n    dd = np.abs(aa - aa_)\n    assert (dd < 1/255.0).all()\n\n    uu = to_fixed_point(aa, \'uint16\')\n    assert uu.dtype == \'uint16\'\n    assert tuple(uu.ravel()) == (0, 0x8000, 0xFFFF)\n\n    uu = to_fixed_point(aa, \'int16\')\n    assert uu.dtype == \'int16\'\n    assert tuple(uu.ravel()) == (0, 0x4000, 0x7FFF)\n\n    aa_ = from_fixed_point(uu)\n    dd = np.abs(aa - aa_)\n    assert (dd < 1.0/0x7FFF).all()\n\n\ndef test_geobox():\n    points_list = [\n        [(148.2697, -35.20111), (149.31254, -35.20111), (149.31254, -36.331431), (148.2697, -36.331431)],\n        [(148.2697, 35.20111), (149.31254, 35.20111), (149.31254, 36.331431), (148.2697, 36.331431)],\n        [(-148.2697, 35.20111), (-149.31254, 35.20111), (-149.31254, 36.331431), (-148.2697, 36.331431)],\n        [(-148.2697, -35.20111), (-149.31254, -35.20111), (-149.31254, -36.331431), (-148.2697, -36.331431),\n         (148.2697, -35.20111)],\n    ]\n    for points in points_list:\n        polygon = geometry.polygon(points, crs=epsg3577)\n        resolution = (-25, 25)\n        geobox = geometry.GeoBox.from_geopolygon(polygon, resolution)\n\n        assert abs(resolution[0]) > abs(geobox.extent.boundingbox.left - polygon.boundingbox.left)\n        assert abs(resolution[0]) > abs(geobox.extent.boundingbox.right - polygon.boundingbox.right)\n        assert abs(resolution[1]) > abs(geobox.extent.boundingbox.top - polygon.boundingbox.top)\n        assert abs(resolution[1]) > abs(geobox.extent.boundingbox.bottom - polygon.boundingbox.bottom)\n\n    A = mkA(0, scale=(10, -10),\n            translation=(-48800, -2983006))\n\n    w, h = 512, 256\n    gbox = GeoBox(w, h, A, epsg3577)\n\n    assert gbox.shape == (h, w)\n    assert gbox.transform == A\n    assert gbox.extent.crs == gbox.crs\n    assert gbox.geographic_extent.crs == epsg4326\n    assert gbox.extent.boundingbox.height == h*10.0\n    assert gbox.extent.boundingbox.width == w*10.0\n    assert isinstance(str(gbox), str)\n    assert \'EPSG:3577\' in repr(gbox)\n\n    assert GeoBox(1, 1, mkA(0), epsg4326).geographic_extent.crs == epsg4326\n    assert GeoBox(1, 1, mkA(0), None).dimensions == (\'y\', \'x\')\n\n    g2 = gbox[:-10, :-20]\n    assert g2.shape == (gbox.height - 10, gbox.width - 20)\n\n    # step of 1 is ok\n    g2 = gbox[::1, ::1]\n    assert g2.shape == gbox.shape\n\n    assert gbox[0].shape == (1, gbox.width)\n    assert gbox[:3].shape == (3, gbox.width)\n\n    with pytest.raises(NotImplementedError):\n        gbox[::2, :]\n\n    # too many slices\n    with pytest.raises(ValueError):\n        gbox[:1, :1, :]\n\n    assert gbox.buffered(10, 0).shape == (gbox.height + 2*1, gbox.width)\n    assert gbox.buffered(30, 20).shape == (gbox.height + 2*3, gbox.width + 2*2)\n\n    assert (gbox | gbox) == gbox\n    assert (gbox & gbox) == gbox\n    assert gbox.is_empty() is False\n    assert bool(gbox) is True\n\n    assert (gbox[:3, :4] & gbox[3:, 4:]).is_empty()\n    assert (gbox[:3, :4] & gbox[30:, 40:]).is_empty()\n\n    with pytest.raises(ValueError):\n        geobox_intersection_conservative([])\n\n    with pytest.raises(ValueError):\n        geobox_union_conservative([])\n\n    # can not combine across CRSs\n    with pytest.raises(ValueError):\n        bounding_box_in_pixel_domain(GeoBox(1, 1, mkA(0), epsg4326),\n                                     GeoBox(2, 3, mkA(0), epsg3577))\n\n\ndef test_geobox_xr_coords():\n    A = mkA(0, scale=(10, -10),\n            translation=(-48800, -2983006))\n\n    w, h = 512, 256\n    gbox = GeoBox(w, h, A, epsg3577)\n\n    cc = gbox.xr_coords()\n    assert list(cc) == [\'y\', \'x\']\n    assert cc[\'y\'].shape == (gbox.shape[0],)\n    assert cc[\'x\'].shape == (gbox.shape[1],)\n    assert \'crs\' in cc[\'y\'].attrs\n    assert \'crs\' in cc[\'x\'].attrs\n\n    cc = gbox.xr_coords(with_crs=True)\n    assert list(cc) == [\'y\', \'x\', \'spatial_ref\']\n    assert cc[\'spatial_ref\'].shape is ()\n    assert cc[\'spatial_ref\'].attrs[\'spatial_ref\'] == gbox.crs.wkt\n    assert isinstance(cc[\'spatial_ref\'].attrs[\'grid_mapping_name\'], str)\n\n    cc = gbox.xr_coords(with_crs=\'Albers\')\n    assert list(cc) == [\'y\', \'x\', \'Albers\']\n\n    # geographic CRS\n    A = mkA(0, scale=(0.1, -0.1), translation=(10, 30))\n    gbox = GeoBox(w, h, A, \'epsg:4326\')\n    cc = gbox.xr_coords(with_crs=True)\n    assert list(cc) == [\'latitude\', \'longitude\', \'spatial_ref\']\n    assert cc[\'spatial_ref\'].shape is ()\n    assert cc[\'spatial_ref\'].attrs[\'spatial_ref\'] == gbox.crs.wkt\n    assert isinstance(cc[\'spatial_ref\'].attrs[\'grid_mapping_name\'], str)\n\n    # missing CRS for GeoBox\n    gbox = GeoBox(w, h, A, None)\n    cc = gbox.xr_coords(with_crs=True)\n    assert list(cc) == [\'y\', \'x\']\n\n    # check CRS without name\n    crs = MagicMock()\n    crs.projected = True\n    crs.wkt = epsg3577.wkt\n    crs.epsg = epsg3577.epsg\n    crs._crs = MagicMock()\n    crs._crs.to_cf.return_value = {}\n    assert _mk_crs_coord(crs).attrs[\'grid_mapping_name\'] == \'??\'\n\n\ndef test_projected_lon():\n    assert projected_lon(epsg3857, 180).crs is epsg3857\n    assert projected_lon(\'EPSG:3577\', 100).crs == epsg3577\n\n\ndef test_chop():\n    poly = geometry.box(618300, -1876800,\n                        849000, -1642500, \'EPSG:32660\')\n\n    chopped = chop_along_antimeridian(poly)\n    assert chopped.crs is poly.crs\n    assert chopped.type == \'MultiPolygon\'\n    assert len([g for g in chopped]) == 2\n\n    poly = geometry.box(0, 0, 10, 20, \'EPSG:4326\')._to_crs(epsg3857)\n    assert poly.crs is epsg3857\n    assert chop_along_antimeridian(poly) is poly\n\n    with pytest.raises(ValueError):\n        chop_along_antimeridian(geometry.box(0, 1, 2, 3, None))\n\n\ndef test_clip_lon180():\n    err = 1e-9\n\n    def b(rside):\n        return geometry.box(170, 0, rside, 10, epsg4326)\n\n    def b_neg(lside):\n        return geometry.box(lside, 0, -170, 10, epsg4326)\n\n    assert clip_lon180(b(180 - err)) == b(180)\n    assert clip_lon180(b(-180 + err)) == b(180)\n\n    assert clip_lon180(b_neg(180 - err)) == b_neg(-180)\n    assert clip_lon180(b_neg(-180 + err)) == b_neg(-180)\n\n    bb = multigeom([b(180-err), b_neg(180-err)])\n    bb_ = [g for g in clip_lon180(bb)]\n    assert bb_[0] == b(180)\n    assert bb_[1] == b_neg(-180)\n\n\ndef test_wrap_dateline():\n    albers_crs = epsg3577\n    geog_crs = epsg4326\n\n    wrap = geometry.polygon([(3658653.1976781483, -4995675.379595791),\n                             (4025493.916030875, -3947239.249752495),\n                             (4912789.243100313, -4297237.125269571),\n                             (4465089.861944263, -5313778.16975072),\n                             (3658653.1976781483, -4995675.379595791)], crs=albers_crs)\n    wrapped = wrap.to_crs(geog_crs)\n    assert wrapped.type == \'Polygon\'\n    assert wrapped.intersects(geometry.line([(0, -90), (0, 90)], crs=geog_crs))\n    wrapped = wrap.to_crs(geog_crs, wrapdateline=True)\n    assert wrapped.type == \'MultiPolygon\'\n    assert not wrapped.intersects(geometry.line([(0, -90), (0, 90)], crs=geog_crs))\n\n\n@pytest.mark.parametrize(""pts"", [\n    [(12231455.716333, -5559752.598333),\n     (12231455.716333, -4447802.078667),\n     (13343406.236, -4447802.078667),\n     (13343406.236, -5559752.598333),\n     (12231455.716333, -5559752.598333)],\n    [(13343406.236, -5559752.598333),\n     (13343406.236, -4447802.078667),\n     (14455356.755667, -4447802.078667),\n     (14455356.755667, -5559752.598333),\n     (13343406.236, -5559752.598333)],\n    [(14455356.755667, -5559752.598333),\n     (14455356.755667, -4447802.078667),\n     (15567307.275333, -4447802.078667),\n     (15567307.275333, -5559752.598333),\n     (14455356.755667, -5559752.598333)],\n])\ndef test_wrap_dateline_sinusoidal(pts):\n    sinus_crs = geometry.CRS(""""""PROJCS[""unnamed"",\n                           GEOGCS[""Unknown datum based upon the custom spheroid"",\n                           DATUM[""Not specified (based on custom spheroid)"", SPHEROID[""Custom spheroid"",6371007.181,0]],\n                           PRIMEM[""Greenwich"",0],UNIT[""degree"",0.0174532925199433]],\n                           PROJECTION[""Sinusoidal""],\n                           PARAMETER[""longitude_of_center"",0],\n                           PARAMETER[""false_easting"",0],\n                           PARAMETER[""false_northing"",0],\n                           UNIT[""Meter"",1]]"""""")\n\n    wrap = geometry.polygon(pts, crs=sinus_crs)\n    wrapped = wrap.to_crs(epsg4326)\n    assert wrapped.type == \'Polygon\'\n    wrapped = wrap.to_crs(epsg4326, wrapdateline=True)\n    assert wrapped.type == \'MultiPolygon\'\n    assert not wrapped.intersects(geometry.line([(0, -90), (0, 90)], crs=epsg4326))\n\n\ndef test_wrap_dateline_utm():\n    poly = geometry.box(618300, -1876800,\n                        849000, -1642500, \'EPSG:32660\')\n\n    wrapped = poly.to_crs(epsg4326)\n    assert wrapped.type == \'Polygon\'\n    assert wrapped.intersects(geometry.line([(0, -90), (0, 90)], crs=epsg4326))\n    wrapped = poly.to_crs(epsg4326, wrapdateline=True)\n    assert wrapped.type == \'MultiPolygon\'\n    assert not wrapped.intersects(geometry.line([(0, -90), (0, 90)], crs=epsg4326))\n\n\ndef test_3d_geometry_converted_to_2d_geometry():\n    coordinates = [(115.8929714190001, -28.577007674999948, 0.0),\n                   (115.90275429200005, -28.57698532699993, 0.0),\n                   (115.90412631000004, -28.577577566999935, 0.0),\n                   (115.90157040700001, -28.58521105999995, 0.0),\n                   (115.89382838900008, -28.585473711999953, 0.0),\n                   (115.8929714190001, -28.577007674999948, 0.0)]\n    geom_3d = {\'coordinates\': [coordinates],\n               \'type\': \'Polygon\'}\n    geom_2d = {\'coordinates\': [[(x, y) for x, y, z in coordinates]],\n               \'type\': \'Polygon\'}\n\n    g_2d = geometry.Geometry(geom_2d)\n    g_3d = geometry.Geometry(geom_3d)\n\n    assert {2} == set(len(pt) for pt in g_3d.boundary.coords)  # All coordinates are 2D\n\n    assert g_2d == g_3d  # 3D geometry has been converted to a 2D by dropping the Z axis\n\n\ndef test_3d_point_converted_to_2d_point():\n    point = (-35.5029340, 145.9312455, 0.0)\n\n    point_3d = {\'coordinates\': point,\n                \'type\': \'Point\'}\n    point_2d = {\'coordinates\': (point[0], point[1]),\n                \'type\': \'Point\'}\n\n    p_2d = geometry.Geometry(point_2d)\n    p_3d = geometry.Geometry(point_3d)\n\n    assert len(p_3d.coords[0]) == 2\n\n    assert p_2d == p_3d\n\n\ndef test_crs():\n    CRS = geometry.CRS\n    custom_crs = geometry.CRS(""""""PROJCS[""unnamed"",\n                           GEOGCS[""Unknown datum based upon the custom spheroid"",\n                           DATUM[""Not specified (based on custom spheroid)"", SPHEROID[""Custom spheroid"",6371007.181,0]],\n                           PRIMEM[""Greenwich"",0],UNIT[""degree"",0.0174532925199433]],\n                           PROJECTION[""Sinusoidal""],\n                           PARAMETER[""longitude_of_center"",0],\n                           PARAMETER[""false_easting"",0],\n                           PARAMETER[""false_northing"",0],\n                           UNIT[""Meter"",1]]"""""")\n\n    crs = epsg3577\n    assert crs.geographic is False\n    assert crs.projected is True\n    assert crs.dimensions == (\'y\', \'x\')\n    assert crs.epsg == 3577\n    assert crs.units == (\'metre\', \'metre\')\n    assert isinstance(repr(crs), str)\n\n    crs = epsg4326\n    assert crs.geographic is True\n    assert crs.projected is False\n    assert crs.dimensions == (\'latitude\', \'longitude\')\n    assert crs.epsg == 4326\n\n    crs2 = CRS(crs)\n    assert crs2 == crs\n    assert crs.proj is crs2.proj\n\n    assert epsg4326.valid_region == geometry.box(-180, -90, 180, 90, epsg4326)\n    assert epsg3857.valid_region.crs == epsg4326\n    xmin, _, xmax, _ = epsg3857.valid_region.boundingbox\n    assert (xmin, xmax) == (-180, 180)\n    assert custom_crs.valid_region is None\n\n    assert epsg3577 == epsg3577\n    assert epsg3577 == \'EPSG:3577\'\n    assert (epsg3577 != epsg3577) is False\n    assert (epsg3577 == epsg4326) is False\n    assert (epsg3577 == \'EPSG:4326\') is False\n    assert epsg3577 != epsg4326\n    assert epsg3577 != \'EPSG:4326\'\n\n    bad_crs = [\'cupcakes\',\n               (\'PROJCS[""unnamed"",\'\n                \'GEOGCS[""WGS 84"", DATUM[""WGS_1984"", SPHEROID[""WGS 84"",6378137,298.257223563, AUTHORITY[""EPSG"",""7030""]],\'\n                \'AUTHORITY[""EPSG"",""6326""]], PRIMEM[""Greenwich"",0, AUTHORITY[""EPSG"",""8901""]],\'\n                \'UNIT[""degree"",0.0174532925199433, AUTHORITY[""EPSG"",""9122""]], AUTHORITY[""EPSG"",""4326""]]]\')]\n\n    for bad in bad_crs:\n        with pytest.raises(geometry.CRSError):\n            CRS(bad)\n\n    with pytest.warns(DeprecationWarning):\n        assert str(epsg3857) == epsg3857.crs_str\n\n\ndef test_polygon_path():\n    from datacube.utils.geometry.tools import polygon_path\n\n    pp = polygon_path([0, 1])\n    assert pp.shape == (2, 5)\n    assert set(pp.ravel()) == {0, 1}\n\n    pp2 = polygon_path([0, 1], [0, 1])\n    assert (pp2 == pp).all()\n\n    pp = polygon_path([0, 1], [2, 3])\n    assert set(pp[0].ravel()) == {0, 1}\n    assert set(pp[1].ravel()) == {2, 3}\n\n\ndef test_gbox_boundary():\n    from datacube.utils.geometry.tools import gbox_boundary\n    import numpy as np\n\n    xx = np.zeros((2, 6))\n\n    bb = gbox_boundary(xx, 3)\n\n    assert bb.shape == (4 + (3-2)*4, 2)\n    assert set(bb.T[0]) == {0.0, 3.0, 6.0}\n    assert set(bb.T[1]) == {0.0, 1.0, 2.0}\n\n\ndef test_geobox_scale_down():\n    from datacube.utils.geometry import GeoBox, CRS\n\n    crs = CRS(\'EPSG:3857\')\n\n    A = mkA(0, (111.2, 111.2), translation=(125671, 251465))\n    for s in [2, 3, 4, 8, 13, 16]:\n        gbox = GeoBox(233*s, 755*s, A, crs)\n        gbox_ = scaled_down_geobox(gbox, s)\n\n        assert gbox_.width == 233\n        assert gbox_.height == 755\n        assert gbox_.crs is crs\n        assert gbox_.extent.contains(gbox.extent)\n        assert gbox.extent.difference(gbox.extent).area == 0.0\n\n    gbox = GeoBox(1, 1, A, crs)\n    for s in [2, 3, 5]:\n        gbox_ = scaled_down_geobox(gbox, 3)\n\n        assert gbox_.shape == (1, 1)\n        assert gbox_.crs is crs\n        assert gbox_.extent.contains(gbox.extent)\n\n\ndef test_roi_tools():\n    from datacube.utils.geometry import (\n        roi_is_empty,\n        roi_is_full,\n        roi_shape,\n        roi_normalise,\n        roi_boundary,\n        roi_from_points,\n        roi_center,\n        roi_pad,\n        roi_intersect,\n        scaled_down_roi,\n        scaled_up_roi,\n        scaled_down_shape,\n    )\n    from numpy import s_\n\n    assert roi_shape(s_[2:4, 3:4]) == (2, 1)\n    assert roi_shape(s_[:4, :7]) == (4, 7)\n\n    assert roi_is_empty(s_[:4, :5]) is False\n    assert roi_is_empty(s_[1:1, :10]) is True\n    assert roi_is_empty(s_[7:3, :10]) is True\n\n    assert roi_is_empty(s_[:3]) is False\n    assert roi_is_empty(s_[4:4]) is True\n\n    assert roi_is_full(s_[:3], 3) is True\n    assert roi_is_full(s_[:3, 0:4], (3, 4)) is True\n    assert roi_is_full(s_[:, 0:4], (33, 4)) is True\n    assert roi_is_full(s_[1:3, 0:4], (3, 4)) is False\n    assert roi_is_full(s_[1:3, 0:4], (2, 4)) is False\n    assert roi_is_full(s_[0:4, 0:4], (3, 4)) is False\n\n    roi = s_[0:8, 0:4]\n    roi_ = scaled_down_roi(roi, 2)\n    assert roi_shape(roi_) == (4, 2)\n    assert scaled_down_roi(scaled_up_roi(roi, 3), 3) == roi\n\n    assert scaled_down_shape(roi_shape(roi), 2) == roi_shape(scaled_down_roi(roi, 2))\n\n    assert roi_shape(scaled_up_roi(roi, 10000, (40, 50))) == (40, 50)\n\n    assert roi_normalise(s_[3:4], 40) == s_[3:4]\n    assert roi_normalise(s_[:4], (40,)) == s_[0:4]\n    assert roi_normalise(s_[:], (40,)) == s_[0:40]\n    assert roi_normalise(s_[:-1], (3,)) == s_[0:2]\n    assert roi_normalise(s_[-2:-1, :], (10, 20)) == s_[8:9, 0:20]\n    assert roi_normalise(s_[-2:-1, :, 3:4], (10, 20, 100)) == s_[8:9, 0:20, 3:4]\n    assert roi_center(s_[0:3]) == 1.5\n    assert roi_center(s_[0:2, 0:6]) == (1, 3)\n\n    roi = s_[0:2, 4:13]\n    xy = roi_boundary(roi)\n\n    assert xy.shape == (4, 2)\n    assert roi_from_points(xy, (2, 13)) == roi\n\n    assert roi_intersect(roi, roi) == roi\n    assert roi_intersect(s_[0:3], s_[1:7]) == s_[1:3]\n    assert roi_intersect(s_[0:3], (s_[1:7],)) == s_[1:3]\n    assert roi_intersect((s_[0:3],), s_[1:7]) == (s_[1:3],)\n\n    assert roi_intersect(s_[4:7, 5:6], s_[0:1, 7:8]) == s_[4:4, 6:6]\n\n    assert roi_pad(s_[0:4], 1, 4) == s_[0:4]\n    assert roi_pad(s_[0:4, 1:5], 1, (4, 6)) == s_[0:4, 0:6]\n    assert roi_pad(s_[2:3, 1:5], 10, (7, 9)) == s_[0:7, 0:9]\n\n\ndef test_apply_affine():\n    A = mkA(rot=10, scale=(3, 1.3), translation=(-100, +2.3))\n    xx, yy = np.meshgrid(np.arange(13), np.arange(11))\n\n    xx_, yy_ = apply_affine(A, xx, yy)\n\n    assert xx_.shape == xx.shape\n    assert yy_.shape == xx.shape\n\n    xy_expect = [A*(x, y) for x, y in zip(xx.ravel(), yy.ravel())]\n    xy_got = [(x, y) for x, y in zip(xx_.ravel(), yy_.ravel())]\n\n    np.testing.assert_array_almost_equal(xy_expect, xy_got)\n\n\ndef test_point_transformer():\n    from datacube.utils.geometry import point\n\n    tr = epsg3857.transformer_to_crs(epsg4326)\n    tr_back = epsg4326.transformer_to_crs(epsg3857)\n\n    pts = [(0, 0), (0, 1),\n           (1, 2), (10, 11)]\n    x, y = np.vstack(pts).astype(\'float64\').T\n\n    pts_expect = [point(*pt, epsg3857).to_crs(epsg4326).points[0]\n                  for pt in pts]\n\n    x_expect = [pt[0] for pt in pts_expect]\n    y_expect = [pt[1] for pt in pts_expect]\n\n    x_, y_ = tr(x, y)\n    assert x_.shape == x.shape\n    np.testing.assert_array_almost_equal(x_, x_expect)\n    np.testing.assert_array_almost_equal(y_, y_expect)\n\n    x, y = (a.reshape(2, 2) for a in (x, y))\n    x_, y_ = tr(x, y)\n    assert x_.shape == x.shape\n\n    xb, yb = tr_back(x_, y_)\n    np.testing.assert_array_almost_equal(x, xb)\n    np.testing.assert_array_almost_equal(y, yb)\n\n    # check nans\n    x_, y_ = tr(np.asarray([np.nan, 0, np.nan]),\n                np.asarray([0, np.nan, np.nan]))\n\n    assert np.isnan(x_).all()\n    assert np.isnan(y_).all()\n\n\ndef test_split_translation():\n\n    def verify(a, b):\n        a = np.asarray(a)\n        b = np.asarray(b)\n        np.testing.assert_array_almost_equal(a, b)\n\n    def tt(tx, ty, *expect):\n        verify(split_translation((tx, ty)), expect)\n\n    assert split_translation((1, 2)) == ((1, 2), (0, 0))\n    assert split_translation((-1, -2)) == ((-1, -2), (0, 0))\n    tt(1.3, 2.5, (1, 2), (0.3, 0.5))\n    tt(1.1, 2.6, (1, 3), (0.1, -0.4))\n    tt(-1.1, 2.8, (-1, 3), (-0.1, -0.2))\n    tt(-1.9, 2.05, (-2, 2), (+0.1, 0.05))\n    tt(-1.5, 2.45, (-1, 2), (-0.5, 0.45))\n\n\ndef get_diff(A, B):\n    from math import sqrt\n    return sqrt(sum((a-b)**2 for a, b in zip(A, B)))\n\n\ndef test_affine_checks():\n    assert is_affine_st(mkA(scale=(1, 2), translation=(3, -10))) is True\n    assert is_affine_st(mkA(scale=(1, -2), translation=(-3, -10))) is True\n    assert is_affine_st(mkA(rot=0.1)) is False\n    assert is_affine_st(mkA(shear=0.4)) is False\n\n\ndef test_affine_rsw():\n\n    def run_test(a, scale, shear=0, translation=(0, 0), tol=1e-8):\n        A = mkA(a, scale=scale, shear=shear, translation=translation)\n\n        R, W, S = decompose_rws(A)\n\n        assert get_diff(A, R*W*S) < tol\n        assert get_diff(S, mkA(0, scale)) < tol\n        assert get_diff(R, mkA(a, translation=translation)) < tol\n\n    for a in (0, 12, 45, 33, 67, 89, 90, 120, 170):\n        run_test(a, (1, 1))\n        run_test(a, (0.5, 2))\n        run_test(-a, (0.5, 2))\n\n        run_test(a, (1, 2))\n        run_test(-a, (1, 2))\n\n        run_test(a, (2, -1))\n        run_test(-a, (2, -1))\n\n    run_test(0, (3, 4), 10)\n    run_test(-33, (3, -1), 10, translation=(100, -333))\n\n\ndef test_fit():\n    from random import uniform\n\n    def run_test(A, n, tol=1e-5):\n        X = [(uniform(0, 1), uniform(0, 1))\n             for _ in range(n)]\n        Y = [A*x for x in X]\n        A_ = affine_from_pts(X, Y)\n\n        assert get_diff(A, A_) < tol\n\n    A = mkA(13, scale=(3, 4), shear=3, translation=(100, -3000))\n\n    run_test(A, 3)\n    run_test(A, 10)\n\n    run_test(mkA(), 3)\n    run_test(mkA(), 10)\n\n\ndef test_scale_at_point():\n    def mk_transform(sx, sy):\n        A = mkA(37, scale=(sx, sy), translation=(2127, 93891))\n\n        def transofrom(pts):\n            return [A*x for x in pts]\n\n        return transofrom\n\n    tol = 1e-4\n    pt = (0, 0)\n    for sx, sy in [(3, 4), (0.4, 0.333)]:\n        tr = mk_transform(sx, sy)\n        sx_, sy_ = get_scale_at_point(pt, tr)\n        assert abs(sx - sx_) < tol\n        assert abs(sy - sy_) < tol\n\n        sx_, sy_ = get_scale_at_point(pt, tr, 0.1)\n        assert abs(sx - sx_) < tol\n        assert abs(sy - sy_) < tol\n\n\ndef test_pix_transform():\n    pt = tuple([int(x/10)*10 for x in\n                geometry.point(145, -35, epsg4326).to_crs(epsg3577).coords[0]])\n\n    A = mkA(scale=(20, -20), translation=pt)\n\n    src = geometry.GeoBox(1024, 512, A, epsg3577)\n    dst = geometry.GeoBox.from_geopolygon(src.geographic_extent,\n                                          (0.0001, -0.0001))\n\n    tr = native_pix_transform(src, dst)\n\n    pts_src = [(0, 0), (10, 20), (300, 200)]\n    pts_dst = tr(pts_src)\n    pts_src_ = tr.back(pts_dst)\n\n    np.testing.assert_almost_equal(pts_src, pts_src_)\n    assert tr.linear is None\n\n    # check identity transform\n    tr = native_pix_transform(src, src)\n\n    pts_src = [(0, 0), (10, 20), (300, 200)]\n    pts_dst = tr(pts_src)\n    pts_src_ = tr.back(pts_dst)\n\n    np.testing.assert_almost_equal(pts_src, pts_src_)\n    np.testing.assert_almost_equal(pts_src, pts_dst)\n    assert tr.linear is not None\n    assert tr.back.linear is not None\n    assert tr.back.back is tr\n\n    # check scale only change\n    tr = native_pix_transform(src, scaled_down_geobox(src, 2))\n    pts_dst = tr(pts_src)\n    pts_src_ = tr.back(pts_dst)\n\n    assert tr.linear is not None\n    assert tr.back.linear is not None\n    assert tr.back.back is tr\n\n    np.testing.assert_almost_equal(pts_dst,\n                                   [(x/2, y/2) for (x, y) in pts_src])\n\n    np.testing.assert_almost_equal(pts_src, pts_src_)\n\n\ndef test_compute_reproject_roi():\n    src = AlbersGS.tile_geobox((15, -40))\n    dst = geometry.GeoBox.from_geopolygon(src.extent.to_crs(epsg3857).buffer(10),\n                                          resolution=src.resolution)\n\n    rr = compute_reproject_roi(src, dst)\n\n    assert rr.roi_src == np.s_[0:src.height, 0:src.width]\n    assert 0 < rr.scale < 1\n    assert rr.is_st is False\n    assert rr.transform.linear is None\n    assert rr.scale in rr.scale2\n\n    # check pure translation case\n    roi_ = np.s_[113:-100, 33:-10]\n    rr = compute_reproject_roi(src, src[roi_])\n    assert rr.roi_src == roi_normalise(roi_, src.shape)\n    assert rr.scale == 1\n    assert rr.is_st is True\n\n    rr = compute_reproject_roi(src, src[roi_], padding=0, align=0)\n    assert rr.roi_src == roi_normalise(roi_, src.shape)\n    assert rr.scale == 1\n    assert rr.scale2 == (1, 1)\n\n    # check pure translation case\n    roi_ = np.s_[113:-100, 33:-10]\n    rr = compute_reproject_roi(src, src[roi_], align=256)\n\n    assert rr.roi_src == np.s_[0:src.height, 0:src.width]\n    assert rr.scale == 1\n\n    roi_ = np.s_[113:-100, 33:-10]\n    rr = compute_reproject_roi(src, src[roi_])\n\n    assert rr.scale == 1\n    assert roi_shape(rr.roi_src) == roi_shape(rr.roi_dst)\n    assert roi_shape(rr.roi_dst) == src[roi_].shape\n\n\ndef test_compute_reproject_roi_issue647():\n    """""" In some scenarios non-overlapping geoboxes will result in non-empty\n    `roi_dst` even though `roi_src` is empty.\n\n    Test this case separately.\n    """"""\n    from datacube.utils.geometry import CRS\n\n    src = GeoBox(10980, 10980,\n                 Affine(10, 0, 300000,\n                        0, -10, 5900020),\n                 CRS(\'epsg:32756\'))\n\n    dst = GeoBox(976, 976, Affine(10, 0, 1730240,\n                                  0, -10, -4170240),\n                 CRS(\'EPSG:3577\'))\n\n    assert src.extent.overlaps(dst.extent.to_crs(src.crs)) is False\n\n    rr = compute_reproject_roi(src, dst)\n\n    assert roi_is_empty(rr.roi_src)\n    assert roi_is_empty(rr.roi_dst)\n\n\ndef test_window_from_slice():\n    from numpy import s_\n\n    assert w_[None] is None\n    assert w_[s_[:3, 4:5]] == ((0, 3), (4, 5))\n    assert w_[s_[0:3, :5]] == ((0, 3), (0, 5))\n    assert w_[list(s_[0:3, :5])] == ((0, 3), (0, 5))\n\n    for roi in [s_[:3], s_[:3, :4, :5], 0]:\n        with pytest.raises(ValueError):\n            w_[roi]\n\n\ndef test_axis_overlap():\n    s_ = np.s_\n\n    # Source overlaps destination fully\n    #\n    # S: |<--------------->|\n    # D:      |<----->|\n    assert compute_axis_overlap(100, 20, 1, 10) == s_[10:30, 0:20]\n    assert compute_axis_overlap(100, 20, 2, 10) == s_[10:50, 0:20]\n    assert compute_axis_overlap(100, 20, 0.25, 10) == s_[10:15, 0:20]\n    assert compute_axis_overlap(100, 20, -1, 80) == s_[60:80, 0:20]\n    assert compute_axis_overlap(100, 20, -0.5, 50) == s_[40:50, 0:20]\n    assert compute_axis_overlap(100, 20, -2, 90) == s_[50:90, 0:20]\n\n    # Destination overlaps source fully\n    #\n    # S:      |<-------->|\n    # D: |<----------------->|\n    assert compute_axis_overlap(10, 100, 1, -10) == s_[0:10, 10:20]\n    assert compute_axis_overlap(10, 100, 2, -10) == s_[0:10, 5:10]\n    assert compute_axis_overlap(10, 100, 0.5, -10) == s_[0:10, 20:40]\n    assert compute_axis_overlap(10, 100, -1, 11) == s_[0:10, 1:11]\n\n    # Partial overlaps\n    #\n    # S: |<----------->|\n    # D:     |<----------->|\n    assert compute_axis_overlap(10, 10, 1, 3) == s_[3:10, 0:7]\n    assert compute_axis_overlap(10, 15, 1, 3) == s_[3:10, 0:7]\n\n    # S:     |<----------->|\n    # D: |<----------->|\n    assert compute_axis_overlap(10, 10, 1, -5) == s_[0:5, 5:10]\n    assert compute_axis_overlap(50, 10, 1, -5) == s_[0:5, 5:10]\n\n    # No overlaps\n    # S: |<--->|\n    # D:         |<--->|\n    assert compute_axis_overlap(10, 10, 1, 11) == s_[10:10, 0:0]\n    assert compute_axis_overlap(10, 40, 1, 11) == s_[10:10, 0:0]\n\n    # S:         |<--->|\n    # D: |<--->|\n    assert compute_axis_overlap(10, 10, 1, -11) == s_[0:0, 10:10]\n    assert compute_axis_overlap(40, 10, 1, -11) == s_[0:0, 10:10]\n\n\ndef test_crs_compat():\n    import rasterio.crs\n\n    crs = CRS(""epsg:3577"")\n    assert crs.epsg == 3577\n    crs2 = CRS(crs)\n    assert crs.epsg == crs2.epsg\n\n    crs_rio = rasterio.crs.CRS(init=\'epsg:3577\')\n    assert CRS(crs_rio).epsg == 3577\n\n    assert (CRS(crs_rio) == crs_rio) is True\n\n    with pytest.raises(geometry.CRSError):\n        CRS((""random"", ""tuple""))\n\n    with pytest.warns(UserWarning):\n        crs_dict = crs.proj.to_dict()\n\n    assert CRS(crs_dict) == crs\n\n\ndef test_crs_hash():\n    crs = CRS(""epsg:3577"")\n    crs2 = CRS(crs)\n\n    assert crs is not crs2\n    assert len(set([crs, crs2])) == 1\n\n\ndef test_base_internals():\n    assert _guess_crs_str(CRS(""epsg:3577"")) == \'EPSG:3577\'\n    no_epsg_crs = CRS(SAMPLE_WKT_WITHOUT_AUTHORITY)\n    assert _guess_crs_str(no_epsg_crs) == no_epsg_crs.to_wkt()\n\n    gjson_bad = {\'type\': \'a\', \'coordinates\': [1, [2, 3, 4]]}\n    assert force_2d(gjson_bad) == {\'type\': \'a\', \'coordinates\': [1, [2, 3]]}\n\n    with pytest.raises(ValueError):\n        force_2d({\'type\': \'a\', \'coordinates\': [set(""not a valid element"")]})\n\n    assert _round_to_res(0.2, 1.0) == 1\n    assert _round_to_res(0.0, 1.0) == 0\n    assert _round_to_res(0.05, 1.0) == 0\n\n    assert _norm_crs(None) is None\n\n    with pytest.raises(ValueError):\n        _norm_crs_or_error(None)\n\n\ndef test_geom_clone():\n    b = geometry.box(0, 0, 10, 20, epsg4326)\n    assert b == b.clone()\n    assert b.geom is not b.clone().geom\n\n    assert b == geometry.Geometry(b)\n    assert b.geom is not geometry.Geometry(b).geom\n\n\ndef test_crs_units_per_degree():\n    assert crs_units_per_degree(\'EPSG:3857\', (0, 0)) == crs_units_per_degree(\'EPSG:3857\', 0, 0)\n    assert crs_units_per_degree(\'EPSG:4326\', (120, -10)) == approx(1.0, 1e-6)\n\n    assert crs_units_per_degree(\'EPSG:3857\', 0, 0) == approx(111319.49, 0.5)\n    assert crs_units_per_degree(\'EPSG:3857\', 20, 0) == approx(111319.49, 0.5)\n    assert crs_units_per_degree(\'EPSG:3857\', 30, 0) == approx(111319.49, 0.5)\n    assert crs_units_per_degree(\'EPSG:3857\', 180, 0) == approx(111319.49, 0.5)\n    assert crs_units_per_degree(\'EPSG:3857\', -180, 0) == approx(111319.49, 0.5)\n\n\n@pytest.mark.parametrize(""left, right, off, res, expect"", [\n    (20, 30, 10, 0, (20, 1)),\n    (20, 30.5, 10, 0, (20, 1)),\n    (20, 31.5, 10, 0, (20, 2)),\n    (20, 30, 10, 3, (13, 2)),\n    (20, 30, 10, -3, (17, 2)),\n    (20, 30, -10, 0, (30, 1)),\n    (19.5, 30, -10, 0, (30, 1)),\n    (18.5, 30, -10, 0, (30, 2)),\n    (20, 30, -10, 3, (33, 2)),\n    (20, 30, -10, -3, (37, 2)),\n])\ndef test_align_pix(left, right, off, res, expect):\n    assert _align_pix(left, right, off, res) == expect\n\n\ndef test_lonlat_bounds():\n    # example from landsat scene: spans lon=180\n    poly = geometry.box(618300, -1876800, 849000, -1642500, \'EPSG:32660\')\n\n    bb = geometry.lonlat_bounds(poly)\n    assert bb.left < 180 < bb.right\n    assert geometry.lonlat_bounds(poly) == geometry.lonlat_bounds(poly, resolution=1e+8)\n\n    bb = geometry.lonlat_bounds(poly, mode=\'quick\')\n    assert bb.right - bb.left > 180\n\n    poly = geometry.box(1, -10, 2, 20, \'EPSG:4326\')\n    assert geometry.lonlat_bounds(poly) == poly.boundingbox\n\n    with pytest.raises(ValueError):\n        geometry.lonlat_bounds(geometry.box(0, 0, 1, 1, None))\n\n\n@pytest.mark.xfail(True, reason=""Bounds computation for large geometries in safe mode is broken"")\ndef test_lonalt_bounds_more_than_180():\n    poly = geometry.box(-150, -30, 150, 30, epsg4326).to_crs(epsg3857, math.inf)\n\n    assert geometry.lonlat_bounds(poly, ""quick"") == approx((-150, -30, 150, 30))\n    assert geometry.lonlat_bounds(poly, ""safe"") == approx((-150, -30, 150, 30))\n'"
tests/test_load_data.py,26,"b'from datacube import Datacube\nfrom datacube.api.query import query_group_by\nimport numpy as np\nfrom types import SimpleNamespace\nimport pytest\n\nfrom pathlib import Path\nfrom datacube.testutils import (\n    mk_sample_dataset,\n    mk_test_image,\n    gen_tiff_dataset,\n)\nfrom datacube.testutils.io import write_gtiff, rio_slurp, rio_slurp_xarray, get_raster_info\nfrom datacube.testutils.iodriver import NetCDF\nfrom datacube.utils import ignore_exceptions_if\n\n\ndef test_load_data(tmpdir):\n    tmpdir = Path(str(tmpdir))\n\n    group_by = query_group_by(\'time\')\n    spatial = dict(resolution=(15, -15),\n                   offset=(11230, 1381110),)\n\n    nodata = -999\n    aa = mk_test_image(96, 64, \'int16\', nodata=nodata)\n\n    ds, gbox = gen_tiff_dataset([SimpleNamespace(name=\'aa\', values=aa, nodata=nodata)],\n                                tmpdir,\n                                prefix=\'ds1-\',\n                                timestamp=\'2018-07-19\',\n                                **spatial)\n    assert ds.time is not None\n\n    ds2, _ = gen_tiff_dataset([SimpleNamespace(name=\'aa\', values=aa, nodata=nodata)],\n                              tmpdir,\n                              prefix=\'ds2-\',\n                              timestamp=\'2018-07-19\',\n                              **spatial)\n    assert ds.time is not None\n    assert ds.time == ds2.time\n\n    sources = Datacube.group_datasets([ds], \'time\')\n    sources2 = Datacube.group_datasets([ds, ds2], group_by)\n\n    mm = [\'aa\']\n    mm = [ds.type.measurements[k] for k in mm]\n\n    ds_data = Datacube.load_data(sources, gbox, mm)\n    assert ds_data.aa.nodata == nodata\n    np.testing.assert_array_equal(aa, ds_data.aa.values[0])\n\n    custom_fuser_call_count = 0\n\n    def custom_fuser(dest, delta):\n        nonlocal custom_fuser_call_count\n        custom_fuser_call_count += 1\n        dest[:] += delta\n\n    progress_call_data = []\n\n    def progress_cbk(n, nt):\n        progress_call_data.append((n, nt))\n\n    ds_data = Datacube.load_data(sources2, gbox, mm, fuse_func=custom_fuser,\n                                 progress_cbk=progress_cbk)\n    assert ds_data.aa.nodata == nodata\n    assert custom_fuser_call_count > 0\n    np.testing.assert_array_equal(nodata + aa + aa, ds_data.aa.values[0])\n\n    assert progress_call_data == [(1, 2), (2, 2)]\n\n\ndef test_load_data_cbk(tmpdir):\n    from datacube.api import TerminateCurrentLoad\n\n    tmpdir = Path(str(tmpdir))\n\n    spatial = dict(resolution=(15, -15),\n                   offset=(11230, 1381110),)\n\n    nodata = -999\n    aa = mk_test_image(96, 64, \'int16\', nodata=nodata)\n\n    bands = [SimpleNamespace(name=name, values=aa, nodata=nodata)\n             for name in [\'aa\', \'bb\']]\n\n    ds, gbox = gen_tiff_dataset(bands,\n                                tmpdir,\n                                prefix=\'ds1-\',\n                                timestamp=\'2018-07-19\',\n                                **spatial)\n    assert ds.time is not None\n\n    ds2, _ = gen_tiff_dataset(bands,\n                              tmpdir,\n                              prefix=\'ds2-\',\n                              timestamp=\'2018-07-19\',\n                              **spatial)\n    assert ds.time is not None\n    assert ds.time == ds2.time\n\n    sources = Datacube.group_datasets([ds, ds2], \'time\')\n    progress_call_data = []\n\n    def progress_cbk(n, nt):\n        progress_call_data.append((n, nt))\n\n    ds_data = Datacube.load_data(sources, gbox, ds.type.measurements,\n                                 progress_cbk=progress_cbk)\n\n    assert progress_call_data == [(1, 4), (2, 4), (3, 4), (4, 4)]\n    np.testing.assert_array_equal(aa, ds_data.aa.values[0])\n    np.testing.assert_array_equal(aa, ds_data.bb.values[0])\n\n    def progress_cbk_fail_early(n, nt):\n        progress_call_data.append((n, nt))\n        raise TerminateCurrentLoad()\n\n    def progress_cbk_fail_early2(n, nt):\n        progress_call_data.append((n, nt))\n        if n > 1:\n            raise KeyboardInterrupt()\n\n    progress_call_data = []\n    ds_data = Datacube.load_data(sources, gbox, ds.type.measurements,\n                                 progress_cbk=progress_cbk_fail_early)\n\n    assert progress_call_data == [(1, 4)]\n    assert ds_data.dc_partial_load is True\n    np.testing.assert_array_equal(aa, ds_data.aa.values[0])\n    np.testing.assert_array_equal(nodata, ds_data.bb.values[0])\n\n    progress_call_data = []\n    ds_data = Datacube.load_data(sources, gbox, ds.type.measurements,\n                                 progress_cbk=progress_cbk_fail_early2)\n\n    assert ds_data.dc_partial_load is True\n    assert progress_call_data == [(1, 4), (2, 4)]\n\n\ndef test_hdf5_lock_release_on_failure():\n    from datacube.storage._rio import RasterDatasetDataSource, HDF5_LOCK\n    from datacube.storage import BandInfo\n\n    band = dict(name=\'xx\',\n                layer=\'xx\',\n                dtype=\'uint8\',\n                units=\'K\',\n                nodata=33)\n\n    ds = mk_sample_dataset([band],\n                           uri=\'file:///tmp/this_probably_doesnot_exist_37237827513/xx.nc\',\n                           format=NetCDF)\n    src = RasterDatasetDataSource(BandInfo(ds, \'xx\'))\n\n    with pytest.raises(OSError):\n        with src.open():\n            assert False and ""Did not expect to get here""\n\n    assert not HDF5_LOCK._is_owned()\n\n\ndef test_rio_slurp(tmpdir):\n    w, h, dtype, nodata, ndw = 96, 64, \'int16\', -999, 7\n\n    pp = Path(str(tmpdir))\n\n    aa = mk_test_image(w, h, dtype, nodata, nodata_width=ndw)\n\n    assert aa.shape == (h, w)\n    assert aa.dtype.name == dtype\n    assert aa[10, 30] == (30 << 8) | 10\n    assert aa[10, 11] == nodata\n\n    aa0 = aa.copy()\n    mm0 = write_gtiff(pp/""rio-slurp-aa.tif"", aa, nodata=-999, overwrite=True)\n    mm00 = write_gtiff(pp/""rio-slurp-aa-missing-nodata.tif"", aa, nodata=None, overwrite=True)\n\n    aa, mm = rio_slurp(mm0.path)\n    np.testing.assert_array_equal(aa, aa0)\n    assert mm.gbox == mm0.gbox\n    assert aa.shape == mm.gbox.shape\n    xx = rio_slurp_xarray(mm0.path)\n    assert mm.gbox == xx.geobox\n    np.testing.assert_array_equal(xx.values, aa0)\n\n    aa, mm = rio_slurp(mm0.path, aa0.shape)\n    np.testing.assert_array_equal(aa, aa0)\n    assert aa.shape == mm.gbox.shape\n    assert mm.gbox is mm.src_gbox\n    xx = rio_slurp_xarray(mm0.path, aa0.shape)\n    assert mm.gbox == xx.geobox\n    np.testing.assert_array_equal(xx.values, aa0)\n\n    aa, mm = rio_slurp(mm0.path, (3, 7))\n    assert aa.shape == (3, 7)\n    assert aa.shape == mm.gbox.shape\n    assert mm.gbox != mm.src_gbox\n    assert mm.src_gbox == mm0.gbox\n    assert mm.gbox.extent == mm0.gbox.extent\n\n    aa, mm = rio_slurp(mm0.path, aa0.shape)\n    np.testing.assert_array_equal(aa, aa0)\n    assert aa.shape == mm.gbox.shape\n\n    aa, mm = rio_slurp(mm0.path, mm0.gbox, resampling=\'nearest\')\n    np.testing.assert_array_equal(aa, aa0)\n    xx = rio_slurp_xarray(mm0.path, mm0.gbox)\n    assert mm.gbox == xx.geobox\n    np.testing.assert_array_equal(xx.values, aa0)\n\n    aa, mm = rio_slurp(mm0.path, gbox=mm0.gbox, dtype=\'float32\')\n    assert aa.dtype == \'float32\'\n    np.testing.assert_array_equal(aa, aa0.astype(\'float32\'))\n    xx = rio_slurp_xarray(mm0.path, gbox=mm0.gbox)\n    assert mm.gbox == xx.geobox\n    assert mm.nodata == xx.nodata\n    np.testing.assert_array_equal(xx.values, aa0)\n\n    aa, mm = rio_slurp(mm0.path, mm0.gbox, dst_nodata=-33)\n    np.testing.assert_array_equal(aa == -33, aa0 == -999)\n\n    aa, mm = rio_slurp(mm00.path, mm00.gbox, dst_nodata=None)\n    np.testing.assert_array_equal(aa, aa0)\n\n\ndef test_rio_slurp_with_gbox(tmpdir):\n    w, h, dtype, nodata, ndw = 96, 64, \'int16\', -999, 7\n\n    pp = Path(str(tmpdir))\n    aa = mk_test_image(w, h, dtype, nodata, nodata_width=ndw)\n    assert aa.dtype.name == dtype\n    assert aa[10, 30] == (30 << 8) | 10\n    assert aa[10, 11] == nodata\n\n    aa = np.stack([aa, aa[::-1, ::-1]])\n    assert aa.shape == (2, h, w)\n    aa0 = aa.copy()\n\n    mm = write_gtiff(pp/""rio-slurp-aa.tif"", aa, nodata=-999, overwrite=True)\n    assert mm.count == 2\n\n    aa, mm = rio_slurp(mm.path, mm.gbox)\n    assert aa.shape == aa0.shape\n    np.testing.assert_array_equal(aa, aa0)\n\n\ndef test_missing_file_handling():\n    with pytest.raises(IOError):\n        rio_slurp(\'no-such-file.tiff\')\n\n    # by default should catch any exception\n    with ignore_exceptions_if(True):\n        rio_slurp(\'no-such-file.tiff\')\n\n    # this is equivalent to previous default behaviour, note that missing http\n    # resources are not OSError\n    with ignore_exceptions_if(True, (OSError,)):\n        rio_slurp(\'no-such-file.tiff\')\n\n    # check that only requested exceptions are caught\n    with pytest.raises(IOError):\n        with ignore_exceptions_if(True, (ValueError, ArithmeticError)):\n            rio_slurp(\'no-such-file.tiff\')\n\n\ndef test_native_load(tmpdir):\n    from datacube.testutils.io import native_load, native_geobox\n\n    tmpdir = Path(str(tmpdir))\n    spatial = dict(resolution=(15, -15),\n                   offset=(11230, 1381110),)\n    nodata = -999\n    aa = mk_test_image(96, 64, \'int16\', nodata=nodata)\n    cc = mk_test_image(32, 16, \'int16\', nodata=nodata)\n\n    bands = [SimpleNamespace(name=name, values=aa, nodata=nodata)\n             for name in [\'aa\', \'bb\']]\n    bands.append(SimpleNamespace(name=\'cc\', values=cc, nodata=nodata))\n\n    ds, gbox = gen_tiff_dataset(bands[:2],\n                                tmpdir,\n                                prefix=\'ds1-\',\n                                timestamp=\'2018-07-19\',\n                                **spatial)\n\n    assert set(get_raster_info(ds)) == set(ds.measurements)\n\n    xx = native_load(ds)\n    assert xx.geobox == gbox\n    np.testing.assert_array_equal(aa, xx.isel(time=0).aa.values)\n    np.testing.assert_array_equal(aa, xx.isel(time=0).bb.values)\n\n    ds, gbox_cc = gen_tiff_dataset(bands,\n                                   tmpdir,\n                                   prefix=\'ds2-\',\n                                   timestamp=\'2018-07-19\',\n                                   **spatial)\n\n    # cc is different size from aa,bb\n    with pytest.raises(ValueError):\n        xx = native_load(ds)\n\n    # cc is different size from aa,bb\n    with pytest.raises(ValueError):\n        xx = native_geobox(ds)\n\n    # aa and bb are the same\n    assert native_geobox(ds, [\'aa\', \'bb\']) == gbox\n    xx = native_load(ds, [\'aa\', \'bb\'])\n    assert xx.geobox == gbox\n    np.testing.assert_array_equal(aa, xx.isel(time=0).aa.values)\n    np.testing.assert_array_equal(aa, xx.isel(time=0).bb.values)\n\n    # cc will be reprojected\n    assert native_geobox(ds, basis=\'aa\') == gbox\n    xx = native_load(ds, basis=\'aa\')\n    assert xx.geobox == gbox\n    np.testing.assert_array_equal(aa, xx.isel(time=0).aa.values)\n    np.testing.assert_array_equal(aa, xx.isel(time=0).bb.values)\n\n    # cc is compatible with self\n    xx = native_load(ds, [\'cc\'])\n    assert xx.geobox == gbox_cc\n    np.testing.assert_array_equal(cc, xx.isel(time=0).cc.values)\n'"
tests/test_metadata_fields.py,0,"b'import yaml\nimport datetime\nimport decimal\nfrom textwrap import dedent\nimport pytest\n\nfrom datacube.model.fields import get_dataset_fields, parse_search_field, Expression\nfrom datacube.model import Range, metadata_from_doc\n\nMETADATA_DOC = yaml.safe_load(\'\'\'---\nname: test\ndescription: test all simple search field types\ndataset:\n  id: [id]\n  sources: [lineage, source_datasets]\n  label: [label]\n  creation_dt: [creation_dt]\n  search_fields:\n    x_default_type:\n       description: string type is assumed\n       offset: [some, path, x_default_type_path]\n\n    x_string:\n      type: string\n      description: field of type \'string\'\n      offset: [x_string_path]\n\n    x_double:\n      type: double\n      description: field of type \'double\'\n      offset: [x_double_path]\n\n    x_integer:\n      type: integer\n      description: field of type \'integer\'\n      offset: [x_integer_path]\n\n    x_numeric:\n      type: numeric\n      description: field of type \'numeric\'\n      offset: [x_numeric_path]\n\n    x_datetime:\n      type: datetime\n      description: field of type \'datetime\'\n      offset: [x_datetime_path]\n\'\'\')\n\nSAMPLE_DOC = yaml.safe_load(\'\'\'---\nx_string_path: some_string\nx_double_path: 6.283185307179586\nx_integer_path: 4466778\nx_numeric_path: \'100.33\'\nx_datetime_path: 1999-04-15 12:33:55.001\nsome:\n  path:\n    x_default_type_path: just_a_string\n\'\'\')\n\nMETADATA_DOC_RANGES = yaml.safe_load(\'\'\'---\nname: test\ndescription: test all simple search field types\ndataset:\n  id: [id]\n  sources: [lineage, source_datasets]\n  label: [label]\n  creation_dt: [creation_dt]\n  search_fields:\n     t_range:\n       type: datetime-range\n       min_offset: [[t,a], [t,b]]\n       max_offset: [[t,a], [t,b]]\n\n     x_range:\n       type: double-range\n       min_offset: [[x,a], [x,b], [x,c], [x,d]]\n       max_offset: [[x,a], [x,b], [x,c], [x,d]]\n\n     float_range:\n       type: float-range\n       description: float-range is alias for numeric-range\n       min_offset: [[a]]\n       max_offset: [[b]]\n\n     ab:\n       type: integer-range\n       min_offset: [[a]]\n       max_offset: [[b]]\n\'\'\')\n\nSAMPLE_DOC_RANGES = yaml.safe_load(\'\'\'---\nt:\n  a: 1999-04-15\n  b: 1999-04-16\nx:\n  a: 1\n  b: 2\n  c: 3\n  d: 4\n\'\'\')\n\n\ndef test_get_dataset_simple_fields():\n    xx = get_dataset_fields(METADATA_DOC)\n    assert xx[\'x_default_type\'].type_name == \'string\'\n\n    type_map = dict(\n        double=float,\n        integer=int,\n        string=str,\n        datetime=datetime.datetime,\n        numeric=decimal.Decimal,\n    )\n\n    for n, f in xx.items():\n        assert n == f.name\n        assert isinstance(f.description, str)\n\n        expected_type = type_map.get(f.type_name)\n        vv = f.extract(SAMPLE_DOC)\n        assert isinstance(vv, expected_type)\n\n        # missing data should return None\n        assert f.extract({}) is None\n\n\ndef test_get_dataset_range_fields():\n    xx = get_dataset_fields(METADATA_DOC_RANGES)\n    v = xx[\'x_range\'].extract(SAMPLE_DOC_RANGES)\n    assert v == Range(1, 4)\n\n    v = xx[\'t_range\'].extract(SAMPLE_DOC_RANGES)\n    assert v.begin.strftime(\'%Y-%m-%d\') == ""1999-04-15""\n    assert v.end.strftime(\'%Y-%m-%d\') == ""1999-04-16""\n\n    # missing range should return None\n    assert xx[\'ab\'].extract({}) is None\n\n    # partially missing Range\n    assert xx[\'ab\'].extract(dict(a=3)) == Range(3, None)\n    assert xx[\'ab\'].extract(dict(b=4)) == Range(None, 4)\n\n    # float-range conversion\n    assert xx[\'float_range\'].type_name == \'numeric-range\'\n\n\ndef test_metadata_from_doc():\n    mm = metadata_from_doc(METADATA_DOC)\n    assert mm.definition is METADATA_DOC\n\n    rdr = mm.dataset_reader(SAMPLE_DOC)\n    assert rdr.x_double == SAMPLE_DOC[\'x_double_path\']\n    assert rdr.x_integer == SAMPLE_DOC[\'x_integer_path\']\n    assert rdr.x_string == SAMPLE_DOC[\'x_string_path\']\n    assert rdr.x_numeric == decimal.Decimal(SAMPLE_DOC[\'x_numeric_path\'])\n\n\ndef test_bad_field_definition():\n    def doc(s):\n        return yaml.safe_load(dedent(s))\n\n    with pytest.raises(ValueError):\n        parse_search_field(doc(\'\'\'\n        type: bad_type\n        offset: [a]\n        \'\'\'))\n\n    with pytest.raises(ValueError):\n        parse_search_field(doc(\'\'\'\n        type: badtype-range\n        offset: [a]\n        \'\'\'))\n\n    with pytest.raises(ValueError):\n        parse_search_field(doc(\'\'\'\n        type: double\n        description: missing offset\n        \'\'\'))\n\n    with pytest.raises(ValueError):\n        parse_search_field(doc(\'\'\'\n        type: double-range\n        description: missing min_offset\n        max_offset: [[a]]\n        \'\'\'))\n\n    with pytest.raises(ValueError):\n        parse_search_field(doc(\'\'\'\n        type: double-range\n        description: missing max_offset\n        min_offset: [[a]]\n        \'\'\'))\n\n\ndef test_expression():\n    assert Expression() == Expression()\n    assert (Expression() == object()) is False\n'"
tests/test_model.py,0,"b'# coding=utf-8\nimport pytest\nimport numpy\nfrom datacube.testutils import mk_sample_dataset, mk_sample_product\nfrom datacube.model import GridSpec, Measurement, MetadataType\nfrom datacube.utils import geometry\nfrom datacube.storage import measurement_paths\n\n\ndef test_gridspec():\n    gs = GridSpec(crs=geometry.CRS(\'EPSG:4326\'), tile_size=(1, 1), resolution=(-0.1, 0.1), origin=(10, 10))\n    poly = geometry.polygon([(10, 12.2), (10.8, 13), (13, 10.8), (12.2, 10), (10, 12.2)], crs=geometry.CRS(\'EPSG:4326\'))\n    cells = {index: geobox for index, geobox in list(gs.tiles_from_geopolygon(poly))}\n    assert set(cells.keys()) == {(0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1)}\n    assert numpy.isclose(cells[(2, 0)].coordinates[\'longitude\'].values, numpy.linspace(12.05, 12.95, num=10)).all()\n    assert numpy.isclose(cells[(2, 0)].coordinates[\'latitude\'].values, numpy.linspace(10.95, 10.05, num=10)).all()\n\n    # check geobox_cache\n    cache = {}\n    poly = gs.tile_geobox((3, 4)).extent\n    (c1, gbox1),  = list(gs.tiles_from_geopolygon(poly, geobox_cache=cache))\n    (c2, gbox2),  = list(gs.tiles_from_geopolygon(poly, geobox_cache=cache))\n\n    assert c1 == (3, 4) and c2 == c1\n    assert gbox1 is gbox2\n\n    assert \'4326\' in str(gs)\n    assert \'4326\' in repr(gs)\n    assert (gs == gs)\n    assert (gs == {}) is False\n\n\ndef test_gridspec_upperleft():\n    """""" Test to ensure grid indexes can be counted correctly from bottom left or top left\n    """"""\n    tile_bbox = geometry.BoundingBox(left=1934400.0, top=2414800.0, right=2084400.000, bottom=2264800.000)\n    bbox = geometry.BoundingBox(left=1934615, top=2379460, right=1937615, bottom=2376460)\n    # Upper left - validated against WELD product tile calculator\n    # http://globalmonitoring.sdstate.edu/projects/weld/tilecalc.php\n    gs = GridSpec(crs=geometry.CRS(\'EPSG:5070\'), tile_size=(-150000, 150000), resolution=(-30, 30),\n                  origin=(3314800.0, -2565600.0))\n    cells = {index: geobox for index, geobox in list(gs.tiles(bbox))}\n    assert set(cells.keys()) == {(30, 6)}\n    assert cells[(30, 6)].extent.boundingbox == tile_bbox\n\n    gs = GridSpec(crs=geometry.CRS(\'EPSG:5070\'), tile_size=(150000, 150000), resolution=(-30, 30),\n                  origin=(14800.0, -2565600.0))\n    cells = {index: geobox for index, geobox in list(gs.tiles(bbox))}\n    assert set(cells.keys()) == {(30, 15)}  # WELD grid spec has 21 vertical cells -- 21 - 6 = 15\n    assert cells[(30, 15)].extent.boundingbox == tile_bbox\n\n\ndef test_dataset_basics():\n    ds = mk_sample_dataset([dict(name=\'a\')])\n    assert ds == ds\n    assert ds != ""33""\n    assert (ds == ""33"") is False\n    assert str(ds) == repr(ds)\n\n    ds = mk_sample_dataset([dict(name=\'a\')], uri=None, geobox=None)\n    assert ds.uris == []\n    assert ds.uri_scheme == \'\'\n    assert ds.crs is None\n    assert ds.bounds is None\n    assert ds.extent is None\n    assert ds.transform is None\n\n\ndef test_dataset_measurement_paths():\n    format = \'GeoTiff\'\n\n    ds = mk_sample_dataset([dict(name=n,\n                                 path=n+\'.tiff\')\n                            for n in \'a b c\'.split(\' \')],\n                           uri=\'file:///tmp/datataset.yml\',\n                           format=format)\n\n    assert ds.local_uri == ds.uris[0]\n    assert ds.uri_scheme == \'file\'\n    assert ds.format == format\n    paths = measurement_paths(ds)\n\n    for k, v in paths.items():\n        assert v == \'file:///tmp/\' + k + \'.tiff\'\n\n    ds.uris = None\n    assert ds.local_uri is None\n    with pytest.raises(ValueError):\n        measurement_paths(ds)\n\n\ndef test_product_basics():\n    product = mk_sample_product(\'test_product\')\n    assert product.name == \'test_product\'\n    assert \'test_product\' in str(product)\n    assert \'test_product\' in repr(product)\n    assert product == product\n    assert product == mk_sample_product(\'test_product\')\n    assert not (product == mk_sample_product(\'other\'))\n    assert not (product == [()])\n    assert hash(product) == hash(mk_sample_product(\'test_product\'))\n    assert \'time\' in dir(product.metadata)\n\n\ndef test_product_dimensions():\n    product = mk_sample_product(\'test_product\')\n    assert product.grid_spec is None\n    assert product.dimensions == (\'time\', \'y\', \'x\')\n\n    product = mk_sample_product(\'test_product\', with_grid_spec=True)\n    assert product.grid_spec is not None\n    assert product.dimensions == (\'time\', \'y\', \'x\')\n\n\ndef test_measurement():\n    # Can create a measurement\n    m = Measurement(name=\'t\', dtype=\'uint8\', nodata=255, units=\'1\')\n\n    # retrieve it\'s vital stats\n    assert m.name == \'t\'\n    assert m.dtype == \'uint8\'\n    assert m.nodata == 255\n    assert m.units == \'1\'\n\n    # retrieve the information required for filling a DataArray\n    assert m.dataarray_attrs() == {\'nodata\': 255, \'units\': \'1\'}\n\n    # Can add a new attribute by name and ensure it updates the DataArray attrs too\n    m[\'bob\'] = 10\n    assert m.bob == 10\n    assert m.dataarray_attrs() == {\'nodata\': 255, \'units\': \'1\', \'bob\': 10}\n\n    m[\'none\'] = None\n    assert m.none is None\n\n    # Resampling method is special and *not* needed for DataArray attrs\n    m[\'resampling_method\'] = \'cubic\'\n    assert \'resampling_method\' not in m.dataarray_attrs()\n\n    # It\'s possible to copy and update a Measurement instance\n    m2 = m.copy()\n    assert m2.bob == 10\n    assert m2.dataarray_attrs() == m.dataarray_attrs()\n\n    assert repr(m2) == repr(m)\n\n    # Must specify *all* required keys. name, dtype, nodata and units\n    with pytest.raises(ValueError) as e:\n        Measurement(name=\'x\', units=\'1\', nodata=0)\n\n    assert \'required keys missing:\' in str(e.value)\n    assert \'dtype\' in str(e.value)\n\n\ndef test_like_geobox():\n    from datacube.testutils.geom import AlbersGS\n    from datacube.api.core import output_geobox\n\n    geobox = AlbersGS.tile_geobox((15, -40))\n    assert output_geobox(like=geobox) is geobox\n\n\ndef test_output_geobox_fail_paths():\n    from datacube.api.core import output_geobox\n    gs_nores = GridSpec(crs=geometry.CRS(\'EPSG:4326\'),\n                        tile_size=None,\n                        resolution=None)\n\n    with pytest.raises(ValueError):\n        output_geobox()\n\n    with pytest.raises(ValueError):\n        output_geobox(output_crs=\'EPSG:4326\')  # need resolution as well\n\n    with pytest.raises(ValueError):\n        output_geobox(grid_spec=gs_nores)  # GridSpec with missing resolution\n\n\ndef test_metadata_type():\n    m = MetadataType({\'name\': \'eo\',\n                      \'dataset\': dict(\n                          id=[\'id\'],\n                          label=[\'ga_label\'],\n                          creation_time=[\'creation_dt\'],\n                          measurements=[\'image\', \'bands\'],\n                          sources=[\'lineage\', \'source_datasets\'],\n                          format=[\'format\', \'name\'])},\n                     dataset_search_fields={})\n\n    assert \'eo\' in str(m)\n    assert \'eo\' in repr(m)\n    assert m.name == \'eo\'\n    assert m.description is None\n    assert m.dataset_reader({}) is not None\n\n    # again but without dataset_search_fields\n    m = MetadataType(m.definition)\n    assert \'eo\' in str(m)\n    assert \'eo\' in repr(m)\n    assert m.name == \'eo\'\n    assert m.description is None\n    assert m.dataset_reader({}) is not None\n'"
tests/test_testutils.py,0,"b""import pytest\nfrom datacube.model import Dataset\nfrom datacube.testutils.threads import FakeThreadPoolExecutor\nfrom datacube.testutils import mk_sample_xr_dataset, mk_sample_product, mk_sample_dataset\nfrom datacube.testutils.io import native_geobox\n\n\ndef test_fakethreadpool():\n\n    def tfunc(a: int, b: int = 0, please_fail=False) -> int:\n        if please_fail:\n            raise ValueError('as you wish')\n        if a == 13:\n            raise ValueError('13')\n        return a + b\n\n    pool = FakeThreadPoolExecutor()\n\n    assert pool.submit(tfunc, 1).result() == 1\n    assert pool.submit(tfunc, 1, 2).result() == 3\n\n    fut = pool.submit(tfunc, 1, please_fail=True)\n    assert fut.done()\n    assert fut.exception() is not None\n\n    with pytest.raises(ValueError):\n        fut.result()\n\n    ff = list(pool.map(tfunc, range(14)))\n    assert len(ff) == 14\n    assert [f.result() for f in ff[:13]] == list(range(13))\n    assert ff[13].exception() is not None\n\n    aa = list(range(10))\n    bb = aa[::-1]\n    ff = list(pool.map(tfunc, aa, bb))\n    assert len(ff) == 10\n    assert [f.result() for f in ff[:13]] == [a+b for a, b in zip(aa, bb)]\n\n    pool.shutdown()\n\n\ndef test_mk_sample_xr():\n    xx = mk_sample_xr_dataset()\n    assert 'band' in xx.data_vars\n    assert list(xx.coords) == ['time', 'y', 'x', 'spatial_ref']\n    assert xx.band.dims == ('time', 'y', 'x')\n    assert xx.geobox is not None\n\n    assert mk_sample_xr_dataset(name='xx', shape=(3, 7)).xx.shape == (1, 3, 7)\n    assert mk_sample_xr_dataset(name='xx', time=None, shape=(3, 7)).xx.shape == (3, 7)\n    assert mk_sample_xr_dataset(name='xx', time=None).xx.dims == ('y', 'x')\n\n    assert mk_sample_xr_dataset(resolution=(1, 100)).geobox.resolution == (1, 100)\n    assert mk_sample_xr_dataset(resolution=(1, 100), xy=(3, 55)).geobox.transform*(0, 0) == (3, 55)\n    assert mk_sample_xr_dataset(crs=None).geobox is None\n\n\ndef test_native_geobox_ingested():\n    from datacube.testutils.io import native_geobox\n    from datacube.testutils.geom import AlbersGS\n\n    gbox = AlbersGS.tile_geobox((15, -40))\n    ds = mk_sample_dataset([dict(name='a')],\n                           geobox=gbox,\n                           product_opts=dict(with_grid_spec=True))\n\n    assert native_geobox(ds) == gbox\n\n    # check that dataset covering several tiles is detected as invalid\n    ds = mk_sample_dataset([dict(name='a')],\n                           geobox=gbox.buffered(10, 10),\n                           product_opts=dict(with_grid_spec=True))\n\n    with pytest.raises(ValueError):\n        native_geobox(ds)\n\n\ndef test_native_geobox_eo3(eo3_dataset_s2):\n    ds = eo3_dataset_s2\n    assert ds.crs is not None\n    assert 'blue' in ds.measurements\n\n    gb1 = native_geobox(ds, basis='blue')\n\n    assert gb1.width == 10980\n    assert gb1.height == 10980\n    assert gb1.crs == ds.crs\n\n    gb1_ = native_geobox(ds, ('blue', 'red', 'green'))\n    assert gb1_ == gb1\n\n    gb2 = native_geobox(ds, ['swir_1', 'swir_2'])\n    assert gb2 != gb1\n    assert gb2.width == 5490\n    assert gb2.height == 5490\n    assert gb2.crs == ds.crs\n\n    with pytest.raises(ValueError):\n        native_geobox(ds)\n\n    with pytest.raises(ValueError):\n        native_geobox(ds, ['no_such_band'])\n\n    ds.metadata_doc['measurements']['red_edge_1']['grid'] = 'no-such-grid'\n\n    with pytest.raises(ValueError):\n        native_geobox(ds, ['red_edge_1'])\n"""
tests/test_utils_aws.py,0,"b'import pytest\nimport mock\nimport json\nfrom botocore.credentials import ReadOnlyCredentials\n\nfrom datacube.testutils import write_files\nfrom datacube.utils.aws import (\n    _fetch_text,\n    ec2_current_region,\n    auto_find_region,\n    get_aws_settings,\n    mk_boto_session,\n    get_creds_with_retry,\n    s3_url_parse,\n    s3_fmt_range,\n    s3_client,\n    s3_dump,\n    s3_fetch,\n    _s3_cache_key,\n)\n\n\ndef _json(**kw):\n    return json.dumps(kw)\n\n\ndef mock_urlopen(text, code=200):\n    m = mock.MagicMock()\n    m.getcode.return_value = code\n    m.read.return_value = text.encode(\'utf8\')\n    m.__enter__.return_value = m\n    return m\n\n\ndef test_ec2_current_region():\n    tests = [(None, None),\n             (_json(region=\'TT\'), \'TT\'),\n             (_json(x=3), None),\n             (\'not valid json\', None)]\n\n    for (rv, expect) in tests:\n        with mock.patch(\'datacube.utils.aws._fetch_text\', return_value=rv):\n            assert ec2_current_region() == expect\n\n\n@mock.patch(\'datacube.utils.aws.botocore_default_region\',\n            return_value=None)\ndef test_auto_find_region(*mocks):\n    with mock.patch(\'datacube.utils.aws._fetch_text\', return_value=None):\n        with pytest.raises(ValueError):\n            auto_find_region()\n\n    with mock.patch(\'datacube.utils.aws._fetch_text\', return_value=_json(region=\'TT\')):\n        assert auto_find_region() == \'TT\'\n\n\n@mock.patch(\'datacube.utils.aws.botocore_default_region\',\n            return_value=\'tt-from-botocore\')\ndef test_auto_find_region_2(*mocks):\n    assert auto_find_region() == \'tt-from-botocore\'\n\n\ndef test_fetch_text():\n    with mock.patch(\'datacube.utils.aws.urlopen\',\n                    return_value=mock_urlopen(\'\', 505)):\n        assert _fetch_text(\'http://localhost:8817\') is None\n\n    with mock.patch(\'datacube.utils.aws.urlopen\',\n                    return_value=mock_urlopen(\'text\', 200)):\n        assert _fetch_text(\'http://localhost:8817\') == \'text\'\n\n    def fake_urlopen(*args, **kw):\n        raise IOError(""Always broken"")\n\n    with mock.patch(\'datacube.utils.aws.urlopen\', fake_urlopen):\n        assert _fetch_text(\'http://localhost:8817\') is None\n\n\ndef test_get_aws_settings(monkeypatch, without_aws_env):\n\n    pp = write_files({\n        ""config"": """"""\n[default]\nregion = us-west-2\n\n[profile east]\nregion = us-east-1\n[profile no_region]\n"""""",\n        ""credentials"": """"""\n[default]\naws_access_key_id = AKIAWYXYXYXYXYXYXYXY\naws_secret_access_key = fake-fake-fake\n[east]\naws_access_key_id = AKIAEYXYXYXYXYXYXYXY\naws_secret_access_key = fake-fake-fake\n""""""\n    })\n\n    assert (pp/""credentials"").exists()\n    assert (pp/""config"").exists()\n\n    monkeypatch.setenv(""AWS_CONFIG_FILE"", str(pp/""config""))\n    monkeypatch.setenv(""AWS_SHARED_CREDENTIALS_FILE"", str(pp/""credentials""))\n\n    aws, creds = get_aws_settings()\n    assert aws[\'region_name\'] == \'us-west-2\'\n    assert aws[\'aws_access_key_id\'] == \'AKIAWYXYXYXYXYXYXYXY\'\n    assert aws[\'aws_secret_access_key\'] == \'fake-fake-fake\'\n\n    sess = mk_boto_session(profile=""no_region"",\n                           creds=creds.get_frozen_credentials(),\n                           region_name=""mordor"")\n\n    assert sess.get_credentials().get_frozen_credentials() == creds.get_frozen_credentials()\n\n    aws, creds = get_aws_settings(profile=\'east\')\n    assert aws[\'region_name\'] == \'us-east-1\'\n    assert aws[\'aws_access_key_id\'] == \'AKIAEYXYXYXYXYXYXYXY\'\n    assert aws[\'aws_secret_access_key\'] == \'fake-fake-fake\'\n\n    aws, creds = get_aws_settings(aws_unsigned=True)\n    assert creds is None\n    assert aws[\'region_name\'] == \'us-west-2\'\n    assert aws[\'aws_unsigned\'] is True\n\n    aws, creds = get_aws_settings(profile=""no_region"",\n                                  region_name=""us-west-1"",\n                                  aws_unsigned=True)\n\n    assert creds is None\n    assert aws[\'region_name\'] == \'us-west-1\'\n    assert aws[\'aws_unsigned\'] is True\n\n    with mock.patch(\'datacube.utils.aws._fetch_text\',\n                    return_value=_json(region=""mordor"")):\n        aws, creds = get_aws_settings(profile=""no_region"",\n                                      aws_unsigned=True)\n\n        assert aws[\'region_name\'] == \'mordor\'\n        assert aws[\'aws_unsigned\'] is True\n\n\n@mock.patch(\'datacube.utils.aws.get_creds_with_retry\', return_value=None)\ndef test_get_aws_settings_no_credentials(without_aws_env):\n    # get_aws_settings should fail when credentials are not available\n    with pytest.raises(ValueError, match=""Couldn\'t get credentials""):\n        aws, creds = get_aws_settings(region_name=""fake"")\n\n\ndef test_creds_with_retry():\n    session = mock.MagicMock()\n    session.get_credentials = mock.MagicMock(return_value=None)\n\n    assert get_creds_with_retry(session, 2, 0.01) is None\n    assert session.get_credentials.call_count == 2\n\n\ndef test_s3_basics(without_aws_env):\n    from numpy import s_\n    from botocore.credentials import ReadOnlyCredentials\n\n    assert s3_url_parse(\'s3://bucket/key\') == (\'bucket\', \'key\')\n    assert s3_url_parse(\'s3://bucket/key/\') == (\'bucket\', \'key/\')\n    assert s3_url_parse(\'s3://bucket/k/k/key\') == (\'bucket\', \'k/k/key\')\n\n    with pytest.raises(ValueError):\n        s3_url_parse(""file://some/path"")\n\n    assert s3_fmt_range((0, 3)) == ""bytes=0-2""\n    assert s3_fmt_range(s_[4:10]) == ""bytes=4-9""\n    assert s3_fmt_range(s_[:10]) == ""bytes=0-9""\n    assert s3_fmt_range(None) is None\n\n    for bad in (s_[10:], s_[-2:3], s_[:-3], (-1, 3), (3, -1), s_[1:100:3]):\n        with pytest.raises(ValueError):\n            s3_fmt_range(bad)\n\n    creds = ReadOnlyCredentials(\'fake-key\', \'fake-secret\', None)\n\n    assert str(s3_client(region_name=\'kk\')._endpoint) == \'s3(https://s3.kk.amazonaws.com)\'\n    assert str(s3_client(region_name=\'kk\', use_ssl=False)._endpoint) == \'s3(http://s3.kk.amazonaws.com)\'\n\n    s3 = s3_client(region_name=\'us-west-2\', creds=creds)\n    assert s3 is not None\n\n\ndef test_s3_io(monkeypatch, without_aws_env):\n    import moto\n    from numpy import s_\n\n    url = ""s3://bucket/file.txt""\n    bucket, _ = s3_url_parse(url)\n    monkeypatch.setenv(""AWS_ACCESS_KEY_ID"", ""fake-key-id"")\n    monkeypatch.setenv(""AWS_SECRET_ACCESS_KEY"", ""fake-secret"")\n\n    with moto.mock_s3():\n        s3 = s3_client(region_name=\'kk\')\n        s3.create_bucket(Bucket=bucket)\n        assert s3_dump(b""33"", url, s3=s3) is True\n        assert s3_fetch(url, s3=s3) == b""33""\n\n        assert s3_dump(b""0123456789ABCDEF"", url, s3=s3) is True\n        assert s3_fetch(url, range=s_[:4], s3=s3) == b""0123""\n        assert s3_fetch(url, range=s_[3:8], s3=s3) == b""34567""\n\n        with pytest.raises(ValueError):\n            s3_fetch(url, range=s_[::2], s3=s3)\n\n\n@mock.patch(\'datacube.utils.aws.ec2_current_region\', return_value=""us-west-2"")\ndef test_s3_client_cache(monkeypatch, without_aws_env):\n    monkeypatch.setenv(""AWS_ACCESS_KEY_ID"", ""fake-key-id"")\n    monkeypatch.setenv(""AWS_SECRET_ACCESS_KEY"", ""fake-secret"")\n\n    s3 = s3_client(cache=True)\n    assert s3 is s3_client(cache=True)\n    assert s3 is s3_client(cache=\'purge\')\n    assert s3_client(cache=\'purge\') is None\n    assert s3 is not s3_client(cache=True)\n\n    opts = (dict(),\n            dict(region_name=""foo""),\n            dict(region_name=""bar""),\n            dict(profile=""foo""),\n            dict(profile=""foo"", region_name=""xxx""),\n            dict(profile=""bar""),\n            dict(creds=ReadOnlyCredentials(\'fake1\', \'...\', None)),\n            dict(creds=ReadOnlyCredentials(\'fake1\', \'...\', None), region_name=\'custom\'),\n            dict(creds=ReadOnlyCredentials(\'fake2\', \'...\', None)))\n\n    keys = set(_s3_cache_key(**o) for o in opts)\n    assert len(keys) == len(opts)\n'"
tests/test_utils_changes.py,0,"b'import pytest\nfrom datacube.utils.changes import (\n    contains,\n    classify_changes,\n    allow_any,\n    allow_removal,\n    allow_addition,\n    allow_extension,\n    allow_truncation,\n    MISSING,\n)\n\n\ndef test_changes_contains():\n    assert contains(""bob"", ""BOB"") is True\n    assert contains(""bob"", ""BOB"", case_sensitive=True) is False\n    assert contains(1, 1) is True\n    assert contains(1, {}) is False\n    # same as above, but with None interpreted as {}\n    assert contains(1, None) is False\n    assert contains({}, 1) is False\n    assert contains(None, 1) is False\n    assert contains({}, {}) is True\n    assert contains({}, None) is True\n\n    # this one is arguable...\n    assert contains(None, {}) is False\n    assert contains(None, None) is True\n    assert contains({""a"": 1, ""b"": 2}, {""a"": 1}) is True\n    assert contains({""a"": {""b"": ""BOB""}}, {""a"": {""b"": ""bob""}}) is True\n    assert (\n        contains({""a"": {""b"": ""BOB""}}, {""a"": {""b"": ""bob""}}, case_sensitive=True) is False\n    )\n    assert contains(""bob"", ""alice"") is False\n    assert contains({""a"": 1}, {""a"": 1, ""b"": 2}) is False\n    assert contains({""a"": {""b"": 1}}, {""a"": {}}) is True\n    assert contains({""a"": {""b"": 1}}, {""a"": None}) is True\n\n\ndef test_classify_changes():\n    assert classify_changes([], {}) == ([], [])\n    assert classify_changes([((\'a\',), 1, 2)], {}) == ([], [((\'a\',), 1, 2)])\n    assert classify_changes([((\'a\',), 1, 2)], {(\'a\',): allow_any}) == ([((\'a\',), 1, 2)], [])\n\n    changes = [((\'a2\',), {\'b1\': 1}, MISSING)]  # {\'a1\': 1, \'a2\': {\'b1\': 1}} \xe2\x86\x92 {\'a1\': 1}\n    good_change = (changes, [])\n    bad_change = ([], changes)\n    assert classify_changes(changes, {}) == bad_change\n    assert classify_changes(changes, {tuple(): allow_any}) == good_change\n    assert classify_changes(changes, {tuple(): allow_removal}) == bad_change\n    assert classify_changes(changes, {tuple(): allow_addition}) == bad_change\n    assert classify_changes(changes, {tuple(): allow_truncation}) == good_change\n    assert classify_changes(changes, {tuple(): allow_extension}) == bad_change\n    assert classify_changes(changes, {(\'a1\', ): allow_any}) == bad_change\n    assert classify_changes(changes, {(\'a1\', ): allow_removal}) == bad_change\n    assert classify_changes(changes, {(\'a1\', ): allow_addition}) == bad_change\n    assert classify_changes(changes, {(\'a1\', ): allow_truncation}) == bad_change\n    assert classify_changes(changes, {(\'a1\', ): allow_extension}) == bad_change\n    assert classify_changes(changes, {(\'a2\', ): allow_any}) == good_change\n    assert classify_changes(changes, {(\'a2\', ): allow_removal}) == good_change\n    assert classify_changes(changes, {(\'a2\', ): allow_addition}) == bad_change\n    assert classify_changes(changes, {(\'a2\', ): allow_truncation}) == bad_change\n    assert classify_changes(changes, {(\'a2\', ): allow_extension}) == bad_change\n\n    with pytest.raises(RuntimeError):\n        classify_changes(changes, {(\'a2\', ): object()})\n\n    assert str(MISSING) == repr(MISSING)\n'"
tests/test_utils_cog.py,13,"b'import pytest\nfrom pathlib import Path\nimport numpy as np\nimport xarray as xr\nfrom types import SimpleNamespace\nfrom dask.delayed import Delayed\nimport dask\n\nfrom datacube.testutils import (\n    mk_test_image,\n    gen_tiff_dataset,\n    remove_crs,\n)\nfrom datacube.testutils.io import native_load, rio_slurp_xarray, rio_slurp\nfrom datacube.utils.cog import write_cog, to_cog, _write_cog\n\n\ndef gen_test_data(prefix, dask=False):\n    w, h, dtype, nodata, ndw = 96, 64, \'int16\', -999, 7\n\n    aa = mk_test_image(w, h, dtype, nodata, nodata_width=ndw)\n\n    ds, gbox = gen_tiff_dataset(\n        SimpleNamespace(name=\'aa\', values=aa, nodata=nodata), prefix)\n    extras = {}\n\n    if dask:\n        extras.update(dask_chunks={\'time\': 1})\n\n    xx = native_load(ds, **extras)\n\n    return xx.aa.isel(time=0), ds\n\n\ndef test_cog_file(tmpdir):\n    pp = Path(str(tmpdir))\n    xx, ds = gen_test_data(pp)\n\n    # write to file\n    ff = write_cog(xx, pp / ""cog.tif"")\n    assert isinstance(ff, Path)\n    assert ff == pp / ""cog.tif""\n    assert ff.exists()\n\n    yy = rio_slurp_xarray(pp / ""cog.tif"")\n    np.testing.assert_array_equal(yy.values, xx.values)\n    assert yy.geobox == xx.geobox\n    assert yy.nodata == xx.nodata\n\n    _write_cog(np.stack([xx.values, xx.values]),\n               xx.geobox,\n               pp / ""cog-2-bands.tif"",\n               overview_levels=[])\n\n    yy, mm = rio_slurp(pp / ""cog-2-bands.tif"")\n    assert mm.gbox == xx.geobox\n    assert yy.shape == (2, *xx.shape)\n    np.testing.assert_array_equal(yy[0], xx.values)\n    np.testing.assert_array_equal(yy[1], xx.values)\n\n    with pytest.raises(ValueError, match=""Need 2d or 3d ndarray on input""):\n        _write_cog(xx.values.ravel(), xx.geobox, pp / ""wontwrite.tif"")\n\n    # sizes that are not multiples of 16\n    # also check that supplying `nodata=` doesn\'t break things\n    xx_odd = xx[:23, :63]\n    ff = write_cog(xx_odd, pp / ""cog_odd.tif"",\n                   nodata=xx_odd.attrs[""nodata""])\n    assert isinstance(ff, Path)\n    assert ff == pp / ""cog_odd.tif""\n    assert ff.exists()\n\n    yy = rio_slurp_xarray(pp / ""cog_odd.tif"")\n    np.testing.assert_array_equal(yy.values, xx_odd.values)\n    assert yy.geobox == xx_odd.geobox\n    assert yy.nodata == xx_odd.nodata\n\n    with pytest.warns(UserWarning):\n        write_cog(xx, pp / ""cog_badblocksize.tif"", blocksize=50)\n\n\ndef test_cog_file_dask(tmpdir):\n    pp = Path(str(tmpdir))\n    xx, ds = gen_test_data(pp, dask=True)\n    assert dask.is_dask_collection(xx)\n\n    path = pp / ""cog.tif""\n    ff = write_cog(xx, path)\n    assert isinstance(ff, Delayed)\n    assert path.exists() is False\n    assert ff.compute() == path\n    assert path.exists()\n\n    yy = rio_slurp_xarray(pp / ""cog.tif"")\n    np.testing.assert_array_equal(yy.values, xx.values)\n    assert yy.geobox == xx.geobox\n    assert yy.nodata == xx.nodata\n\n\ndef test_cog_mem(tmpdir):\n    pp = Path(str(tmpdir))\n    xx, ds = gen_test_data(pp)\n\n    # write to memory 1\n    bb = write_cog(xx, "":mem:"")\n    assert isinstance(bb, bytes)\n    path = pp / ""cog1.tiff""\n    with open(str(path), ""wb"") as f:\n        f.write(bb)\n\n    yy = rio_slurp_xarray(path)\n    np.testing.assert_array_equal(yy.values, xx.values)\n    assert yy.geobox == xx.geobox\n    assert yy.nodata == xx.nodata\n\n    # write to memory 2\n    bb = to_cog(xx)\n    assert isinstance(bb, bytes)\n    path = pp / ""cog2.tiff""\n    with open(str(path), ""wb"") as f:\n        f.write(bb)\n\n    yy = rio_slurp_xarray(path)\n    np.testing.assert_array_equal(yy.values, xx.values)\n    assert yy.geobox == xx.geobox\n    assert yy.nodata == xx.nodata\n\n    # write to memory 3 -- no overviews\n    bb = to_cog(xx, overview_levels=[])\n    assert isinstance(bb, bytes)\n    path = pp / ""cog3.tiff""\n    with open(str(path), ""wb"") as f:\n        f.write(bb)\n\n    yy = rio_slurp_xarray(path)\n    np.testing.assert_array_equal(yy.values, xx.values)\n    assert yy.geobox == xx.geobox\n    assert yy.nodata == xx.nodata\n\n\ndef test_cog_mem_dask(tmpdir):\n    pp = Path(str(tmpdir))\n    xx, ds = gen_test_data(pp, dask=True)\n\n    # write to memory 1\n    bb = write_cog(xx, "":mem:"")\n    assert isinstance(bb, Delayed)\n    bb = bb.compute()\n    assert isinstance(bb, bytes)\n\n    path = pp / ""cog1.tiff""\n    with open(str(path), ""wb"") as f:\n        f.write(bb)\n\n    yy = rio_slurp_xarray(path)\n    np.testing.assert_array_equal(yy.values, xx.values)\n    assert yy.geobox == xx.geobox\n    assert yy.nodata == xx.nodata\n\n    # write to memory 2\n    bb = to_cog(xx)\n    assert isinstance(bb, Delayed)\n    bb = bb.compute()\n    assert isinstance(bb, bytes)\n    path = pp / ""cog2.tiff""\n    with open(str(path), ""wb"") as f:\n        f.write(bb)\n\n    yy = rio_slurp_xarray(path)\n    np.testing.assert_array_equal(yy.values, xx.values)\n    assert yy.geobox == xx.geobox\n    assert yy.nodata == xx.nodata\n\n\n@pytest.mark.parametrize(""with_dask"", [True, False])\ndef test_cog_no_crs(tmpdir, with_dask):\n    pp = Path(str(tmpdir))\n\n    xx, ds = gen_test_data(pp, dask=with_dask)\n    xx = remove_crs(xx)\n\n    with pytest.raises(ValueError):\n        write_cog(xx, "":mem:"")\n\n    with pytest.raises(ValueError):\n        to_cog(xx)\n\n\ndef test_cog_rgba(tmpdir):\n    pp = Path(str(tmpdir))\n    xx, ds = gen_test_data(pp)\n    pix = np.dstack([xx.values] * 4)\n    rgba = xr.DataArray(pix,\n                        attrs=xx.attrs,\n                        dims=(\'y\', \'x\', \'band\'),\n                        coords=xx.coords)\n    assert(rgba.geobox == xx.geobox)\n    assert(rgba.shape[:2] == rgba.geobox.shape)\n\n    ff = write_cog(rgba, pp / ""cog.tif"")\n    yy = rio_slurp_xarray(ff)\n\n    assert(yy.geobox == rgba.geobox)\n    assert(yy.shape == rgba.shape)\n    np.testing.assert_array_equal(yy.values, rgba.values)\n\n    with pytest.raises(ValueError):\n        _write_cog(rgba.values[1:, :, :], rgba.geobox, \':mem:\')\n'"
tests/test_utils_dask.py,0,"b'import pytest\nimport moto\nfrom pathlib import Path\nimport dask\nimport dask.delayed\n\nfrom datacube.utils.io import slurp\n\nfrom datacube.utils.dask import (\n    start_local_dask,\n    get_total_available_memory,\n    compute_memory_per_worker,\n    compute_tasks,\n    pmap,\n    partition_map,\n    save_blob_to_file,\n    save_blob_to_s3,\n    _save_blob_to_file,\n    _save_blob_to_s3,\n)\n\nfrom datacube.utils.aws import (\n    s3_url_parse,\n    s3_fetch,\n    s3_client,\n)\n\n\ndef test_compute_tasks():\n    client = start_local_dask(threads_per_worker=1,\n                              dashboard_address=None)\n\n    tasks = (dask.delayed(x) for x in range(100))\n    xx = [x for x in compute_tasks(tasks, client)]\n    assert xx == [x for x in range(100)]\n\n    client.close()\n    del client\n\n\ndef test_partition_map():\n    tasks = partition_map(10, str, range(101))\n    tt = [t for t in tasks]\n    assert len(tt) == 11\n    lump = tt[0].compute()\n    assert len(lump) == 10\n    assert lump == [str(x) for x in range(10)]\n\n    lump = tt[-1].compute()\n    assert len(lump) == 1\n\n\ndef test_pmap():\n    client = start_local_dask(threads_per_worker=1,\n                              dashboard_address=None)\n\n    xx_it = pmap(str, range(101), client=client)\n    xx = [x for x in xx_it]\n\n    assert xx == [str(x) for x in range(101)]\n\n    client.close()\n    del client\n\n\n@pytest.mark.parametrize(""blob"", [\n    ""some utf8 string"",\n    b""raw bytes"",\n])\ndef test_save_blob_file_direct(tmpdir, blob):\n    tmpdir = Path(str(tmpdir))\n    fname = str(tmpdir/""file.txt"")\n    mode = ""rt"" if isinstance(blob, str) else ""rb""\n\n    assert _save_blob_to_file(blob, fname) == (fname, True)\n    assert slurp(fname, mode=mode) == blob\n\n    fname = str(tmpdir/""missing""/""file.txt"")\n    assert _save_blob_to_file(blob, fname) == (fname, False)\n\n\n@pytest.mark.parametrize(""blob"", [\n    ""some utf8 string"",\n    b""raw bytes"",\n])\ndef test_save_blob_file(tmpdir, blob, dask_client):\n    tmpdir = Path(str(tmpdir))\n    fname = str(tmpdir/""file.txt"")\n    dask_blob = dask.delayed(blob)\n    mode = ""rt"" if isinstance(blob, str) else ""rb""\n\n    rr = save_blob_to_file(dask_blob, fname)\n    assert dask_client.compute(rr).result() == (fname, True)\n    assert slurp(fname, mode=mode) == blob\n\n    fname = str(tmpdir/""missing""/""file.txt"")\n    rr = save_blob_to_file(dask_blob, fname)\n    assert dask_client.compute(rr).result() == (fname, False)\n\n\n@pytest.mark.parametrize(""blob"", [\n    ""some utf8 string"",\n    b""raw bytes"",\n])\ndef test_save_blob_s3_direct(blob, monkeypatch):\n    region_name = ""us-west-2""\n    blob2 = blob + blob\n\n    url = ""s3://bucket/file.txt""\n    url2 = ""s3://bucket/file-2.txt""\n\n    bucket, _ = s3_url_parse(url)\n    monkeypatch.setenv(""AWS_ACCESS_KEY_ID"", ""fake-key-id"")\n    monkeypatch.setenv(""AWS_SECRET_ACCESS_KEY"", ""fake-secret"")\n\n    with moto.mock_s3():\n        s3 = s3_client(region_name=region_name)\n        s3.create_bucket(Bucket=bucket)\n\n        assert _save_blob_to_s3(blob, url, region_name=region_name) == (url, True)\n        assert _save_blob_to_s3(blob2, url2, region_name=region_name) == (url2, True)\n\n        bb1 = s3_fetch(url, s3=s3)\n        bb2 = s3_fetch(url2, s3=s3)\n        if isinstance(blob, str):\n            bb1 = bb1.decode(""utf8"")\n            bb2 = bb2.decode(""utf8"")\n\n        assert bb1 == blob\n        assert bb2 == blob2\n\n        assert _save_blob_to_s3("""", ""s3://not-a-bucket/f.txt"") == (""s3://not-a-bucket/f.txt"", False)\n\n\n@pytest.mark.parametrize(""blob"", [\n    ""some utf8 string"",\n    b""raw bytes"",\n])\ndef test_save_blob_s3(blob, monkeypatch, dask_client):\n    region_name = ""us-west-2""\n\n    blob2 = blob + blob\n\n    dask_blob = dask.delayed(blob)\n    dask_blob2 = dask.delayed(blob2)\n\n    url = ""s3://bucket/file.txt""\n    url2 = ""s3://bucket/file-2.txt""\n\n    bucket, _ = s3_url_parse(url)\n    monkeypatch.setenv(""AWS_ACCESS_KEY_ID"", ""fake-key-id"")\n    monkeypatch.setenv(""AWS_SECRET_ACCESS_KEY"", ""fake-secret"")\n\n    with moto.mock_s3():\n        s3 = s3_client(region_name=region_name)\n        s3.create_bucket(Bucket=bucket)\n\n        rr = save_blob_to_s3(dask_blob, url, region_name=region_name)\n        assert rr.compute() == (url, True)\n\n        rr = save_blob_to_s3(dask_blob2, url2, region_name=region_name)\n        assert dask_client.compute(rr).result() == (url2, True)\n\n        bb1 = s3_fetch(url, s3=s3)\n        bb2 = s3_fetch(url2, s3=s3)\n        if isinstance(blob, str):\n            bb1 = bb1.decode(""utf8"")\n            bb2 = bb2.decode(""utf8"")\n\n        assert bb1 == blob\n        assert bb2 == blob2\n\n\ndef test_memory_functions(monkeypatch):\n    gig = 10**9\n\n    total_mem = get_total_available_memory()\n    default_safety = min(500*(1 << 20), total_mem//2)\n\n    assert total_mem - compute_memory_per_worker() == default_safety\n    assert total_mem - compute_memory_per_worker(2)*2 == default_safety\n\n    assert compute_memory_per_worker(mem_safety_margin=1) == total_mem - 1\n    assert compute_memory_per_worker(memory_limit=\'4G\') == 4*gig\n    assert compute_memory_per_worker(2, memory_limit=\'4G\') == 2*gig\n    assert compute_memory_per_worker(memory_limit=\'4G\',\n                                     mem_safety_margin=\'1G\') == 3*gig\n\n    total_mem = 1*gig\n    monkeypatch.setenv(\'MEM_LIMIT\', str(total_mem))\n    assert get_total_available_memory() == 1*gig\n    assert compute_memory_per_worker(mem_safety_margin=1) == total_mem - 1\n'"
tests/test_utils_dates.py,2,"b'import numpy as np\nimport pytest\nfrom datetime import datetime\nfrom datacube.utils.dates import (\n    parse_duration,\n    parse_interval,\n    parse_time,\n    _parse_time_generic,\n    mk_time_coord,\n    normalise_dt,\n)\n\nfrom dateutil.rrule import YEARLY, MONTHLY, DAILY\nfrom dateutil.relativedelta import relativedelta\n\n\ndef test_parse():\n    assert parse_duration(\'1y\') == relativedelta(years=1)\n    assert parse_duration(\'3m\') == relativedelta(months=3)\n    assert parse_duration(\'13d\') == relativedelta(days=13)\n\n    assert parse_interval(\'1y\') == (1, YEARLY)\n    assert parse_interval(\'3m\') == (3, MONTHLY)\n    assert parse_interval(\'13d\') == (13, DAILY)\n\n    with pytest.raises(ValueError):\n        parse_duration(\'1p\')\n\n    with pytest.raises(ValueError):\n        parse_interval(\'1g\')\n\n    assert _parse_time_generic(\'2020-01-01\') == parse_time(\'2020-01-01\')\n    date = datetime(2020, 2, 3)\n    assert _parse_time_generic(date) is date\n\n    # test fallback to python parser\n    assert parse_time(""3 Jan 2020"") == datetime(2020, 1, 3)\n\n\ndef test_normalise_dt():\n    dt_notz = datetime(2020, 2, 14, 10, 33, 11, tzinfo=None)\n    assert normalise_dt(dt_notz) is dt_notz\n\n    assert normalise_dt(""2020-01-20"") == datetime(2020, 1, 20)\n    assert normalise_dt(\'2020-03-26T10:15:32.556793+1:00\').tzinfo is None\n    assert normalise_dt(\'2020-03-26T10:15:32.556793+1:00\') == datetime(2020, 3, 26, 9, 15, 32, 556793)\n    assert normalise_dt(\'2020-03-26T10:15:32.556793+9:00\') == datetime(2020, 3, 26, 1, 15, 32, 556793)\n\n\ndef test_mk_time_coord():\n    t = mk_time_coord([\'2020-01-20\'])\n    assert t.shape == (1,)\n    assert isinstance(t.units, str)\n    assert t.name == \'time\'\n    assert list(t.coords) == [\'time\']\n    assert t.dtype == np.dtype(\'datetime64[ns]\')\n\n    some_dates = [\'2020-01-20\', datetime(2020, 2, 23)]\n    t = mk_time_coord(some_dates)\n    assert t.shape == (2,)\n    assert t.dtype == np.dtype(\'datetime64[ns]\')\n\n    t = mk_time_coord(some_dates, name=\'T\', units=\'ns\')\n    assert t.name == \'T\'\n    assert list(t.coords) == [\'T\']\n    assert t.units == \'ns\'\n    assert t.attrs[\'units\'] == \'ns\'\n'"
tests/test_utils_docs.py,2,"b'""""""\nTest utility functions from :module:`datacube.utils`\n\n\n""""""\nimport os\nfrom pathlib import Path\nfrom collections import OrderedDict\nfrom types import SimpleNamespace\nfrom typing import Tuple, Iterable\n\nimport numpy as np\nimport pytest\nimport toolz\n\nfrom datacube.model import MetadataType\nfrom datacube.model.utils import traverse_datasets, flatten_datasets, dedup_lineage, remap_lineage_doc\nfrom datacube.testutils import mk_sample_product, make_graph_abcde, gen_dataset_test_dag, dataset_maker\nfrom datacube.utils import (read_documents, InvalidDocException,\n                            SimpleDocNav)\nfrom datacube.utils.changes import check_doc_unchanged, get_doc_changes, MISSING, DocumentMismatchError\nfrom datacube.utils.documents import (\n    parse_yaml,\n    without_lineage_sources,\n    _open_from_s3,\n    netcdf_extract_string,\n    DocReader,\n    is_supported_document_type,\n    get_doc_offset,\n    get_doc_offset_safe,\n    _set_doc_offset,\n    transform_object_tree,\n)\nfrom datacube.utils.serialise import jsonify_document\nfrom datacube.utils.uris import as_url\n\n\ndoc_changes = [\n    (1, 1, []),\n    ({}, {}, []),\n    ({\'a\': 1}, {\'a\': 1}, []),\n    ({\'a\': {\'b\': 1}}, {\'a\': {\'b\': 1}}, []),\n    ([1, 2, 3], [1, 2, 3], []),\n    ([1, 2, [3, 4, 5]], [1, 2, [3, 4, 5]], []),\n    (1, 2, [((), 1, 2)]),\n    ([1, 2, 3], [2, 1, 3], [((0,), 1, 2), ((1,), 2, 1)]),\n    ([1, 2, [3, 4, 5]], [1, 2, [3, 6, 7]], [((2, 1), 4, 6), ((2, 2), 5, 7)]),\n    ({\'a\': 1}, {\'a\': 2}, [((\'a\',), 1, 2)]),\n    ({\'a\': 1}, {\'a\': 2}, [((\'a\',), 1, 2)]),\n    ({\'a\': 1}, {\'b\': 1}, [((\'a\',), 1, MISSING), ((\'b\',), MISSING, 1)]),\n    ({\'a\': {\'b\': 1}}, {\'a\': {\'b\': 2}}, [((\'a\', \'b\'), 1, 2)]),\n    ({}, {\'b\': 1}, [((\'b\',), MISSING, 1)]),\n    ({\'a\': {\'c\': 1}}, {\'a\': {\'b\': 1}}, [((\'a\', \'b\'), MISSING, 1), ((\'a\', \'c\'), 1, MISSING)])\n]\n\n\n@pytest.mark.parametrize(""v1, v2, expected"", doc_changes)\ndef test_get_doc_changes(v1, v2, expected):\n    rval = get_doc_changes(v1, v2)\n    assert rval == expected\n\n\ndef test_get_doc_changes_w_baseprefix():\n    rval = get_doc_changes({}, None, base_prefix=(\'a\',))\n    assert rval == [((\'a\',), {}, None)]\n\n\n@pytest.mark.parametrize(""v1, v2, expected"", doc_changes)\ndef test_check_doc_unchanged(v1, v2, expected):\n    if expected != []:\n        with pytest.raises(DocumentMismatchError):\n            check_doc_unchanged(v1, v2, \'name\')\n    else:\n        # No Error Raised\n        check_doc_unchanged(v1, v2, \'name\')\n\n\ndef test_more_check_doc_unchanged():\n    # No exception raised\n    check_doc_unchanged({\'a\': 1}, {\'a\': 1}, \'Letters\')\n\n    with pytest.raises(DocumentMismatchError, match=\'^Letters differs from stored.*a: 1!=2\'):\n        check_doc_unchanged({\'a\': 1}, {\'a\': 2}, \'Letters\')\n\n    with pytest.raises(DocumentMismatchError, match=\'^Letters differs from stored.*a.b: 1!=2\'):\n        check_doc_unchanged({\'a\': {\'b\': 1}}, {\'a\': {\'b\': 2}}, \'Letters\')\n\n\ndef test_without_lineage_sources():\n    def mk_sample(v):\n        return dict(lineage={\'source_datasets\': v, \'a\': \'a\', \'b\': \'b\'},\n                    aa=\'aa\',\n                    bb=dict(bb=\'bb\'))\n\n    spec = mk_sample_product(\'tt\')\n\n    x = {\'a\': 1}\n    assert without_lineage_sources(x, spec) == x\n    assert without_lineage_sources(x, spec, inplace=True) == x\n\n    x = {\'a\': 1, \'lineage\': {}}\n    assert without_lineage_sources(x, spec) == x\n    assert without_lineage_sources(x, spec, inplace=True) == x\n\n    x = mk_sample(1)\n    assert without_lineage_sources(x, spec) != x\n    assert x[\'lineage\'][\'source_datasets\'] == 1\n\n    x = mk_sample(2)\n    assert without_lineage_sources(x, spec, inplace=True) == x\n    assert x[\'lineage\'][\'source_datasets\'] == {}\n\n    assert mk_sample(10) != mk_sample({})\n    assert without_lineage_sources(mk_sample(10), spec) == mk_sample({})\n    assert without_lineage_sources(mk_sample(10), spec, inplace=True) == mk_sample({})\n\n    # check behaviour when `sources` is not defined for the type\n    no_sources_type = MetadataType({\n        \'name\': \'eo\',\n        \'description\': \'Sample\',\n        \'dataset\': dict(\n            id=[\'id\'],\n            label=[\'ga_label\'],\n            creation_time=[\'creation_dt\'],\n            measurements=[\'image\', \'bands\'],\n            format=[\'format\', \'name\'],\n        )\n    }, dataset_search_fields={})\n\n    assert without_lineage_sources(mk_sample(10), no_sources_type) == mk_sample(10)\n    assert without_lineage_sources(mk_sample(10), no_sources_type, inplace=True) == mk_sample(10)\n\n\ndef test_parse_yaml():\n    assert parse_yaml(\'a: 10\') == {\'a\': 10}\n\n\ndef test_read_docs_from_local_path(sample_document_files):\n    _test_read_docs_impl(sample_document_files)\n\n\ndef test_read_docs_from_file_uris(sample_document_files):\n    uris = [(\'file://\' + doc, ndocs) for doc, ndocs in sample_document_files]\n    _test_read_docs_impl(uris)\n\n\ndef test_read_docs_from_s3(sample_document_files, monkeypatch):\n    """"""\n    Use a mocked S3 bucket to test reading documents from S3\n    """"""\n    boto3 = pytest.importorskip(\'boto3\')\n    moto = pytest.importorskip(\'moto\')\n\n    monkeypatch.setenv(\'AWS_ACCESS_KEY_ID\', \'fake\')\n    monkeypatch.setenv(\'AWS_SECRET_ACCESS_KEY\', \'fake\')\n\n    with moto.mock_s3():\n        s3 = boto3.resource(\'s3\', region_name=\'us-east-1\')\n        bucket = s3.create_bucket(Bucket=\'mybucket\')\n\n        mocked_s3_objs = []\n        for abs_fname, ndocs in sample_document_files:\n            if abs_fname.endswith(\'gz\') or abs_fname.endswith(\'nc\'):\n                continue\n\n            fname = Path(abs_fname).name\n            bucket.upload_file(abs_fname, fname)\n\n            mocked_s3_objs.append((\'s3://mybucket/\' + fname, ndocs))\n\n        _test_read_docs_impl(mocked_s3_objs)\n\n    with pytest.raises(RuntimeError):\n        with _open_from_s3(""https://not-s3.ga/file.txt""):\n            pass\n\n\ndef test_read_docs_from_http(sample_document_files, httpserver):\n    http_docs = []\n    for abs_fname, ndocs in sample_document_files:\n        if abs_fname.endswith(\'gz\') or abs_fname.endswith(\'nc\'):\n            continue\n        path = ""/"" + Path(abs_fname).name\n\n        httpserver.expect_request(path).respond_with_data(open(abs_fname).read())\n        http_docs.append((httpserver.url_for(path), ndocs))\n\n    _test_read_docs_impl(http_docs)\n\n\ndef _test_read_docs_impl(sample_documents: Iterable[Tuple[str, int]]):\n    # Test case for returning URIs pointing to documents\n    for doc_url, num_docs in sample_documents:\n        all_docs = list(read_documents(doc_url, uri=True))\n        assert len(all_docs) == num_docs\n\n        for uri, doc in all_docs:\n            assert isinstance(doc, dict)\n            assert isinstance(uri, str)\n\n        url = as_url(doc_url)\n        if num_docs > 1:\n            expect_uris = [as_url(url) + \'#part={}\'.format(i) for i in range(num_docs)]\n        else:\n            expect_uris = [as_url(url)]\n\n        assert [f for f, _ in all_docs] == expect_uris\n\n\ndef test_dataset_maker():\n    mk = dataset_maker(0)\n    assert mk(\'aa\') == mk(\'aa\')\n\n    a = SimpleDocNav(mk(\'A\'))\n    b = SimpleDocNav(mk(\'B\'))\n\n    assert a.id != b.id\n    assert a.doc[\'creation_dt\'] == b.doc[\'creation_dt\']\n    assert isinstance(a.id, str)\n    assert a.sources == {}\n\n    a1, a2 = [dataset_maker(i)(\'A\', product_type=\'eo\') for i in (0, 1)]\n    assert a1[\'id\'] != a2[\'id\']\n    assert a1[\'creation_dt\'] != a2[\'creation_dt\']\n    assert a1[\'product_type\'] == \'eo\'\n\n    c = SimpleDocNav(mk(\'C\', sources=dict(a=a.doc, b=b.doc)))\n    assert c.sources[\'a\'].doc is a.doc\n    assert c.sources[\'b\'].doc is b.doc\n\n\ndef test_traverse_datasets():\n    """"""\n      A -> B\n      |    |\n      |    v\n      +--> C -> D\n      |\n      +--> E\n    """"""\n\n    def node(name, **kwargs):\n        return SimpleNamespace(id=name, sources=kwargs)\n\n    A, *_ = make_graph_abcde(node)\n\n    def visitor(node, name=None, depth=0, out=None):\n        s = \'{}:{}:{:d}\'.format(node.id, name if name else \'..\', depth)\n        out.append(s)\n\n    with pytest.raises(ValueError):\n        traverse_datasets(A, visitor, mode=\'not-a-real-mode\')\n\n    expect_preorder = \'\'\'\nA:..:0\nB:ab:1\nC:bc:2\nD:cd:3\nC:ac:1\nD:cd:2\nE:ae:1\n\'\'\'.lstrip().rstrip()\n\n    expect_postorder = \'\'\'\nD:cd:3\nC:bc:2\nB:ab:1\nD:cd:2\nC:ac:1\nE:ae:1\nA:..:0\n\'\'\'.lstrip().rstrip()\n\n    for mode, expect in zip([\'pre-order\', \'post-order\'],\n                            [expect_preorder, expect_postorder]):\n        out = []\n        traverse_datasets(A, visitor, mode=mode, out=out)\n        assert \'\\n\'.join(out) == expect\n\n    fv = flatten_datasets(A)\n\n    assert len(fv[\'A\']) == 1\n    assert len(fv[\'C\']) == 2\n    assert len(fv[\'E\']) == 1\n    assert set(fv.keys()) == set(\'ABCDE\')\n\n\ndef test_simple_doc_nav():\n    """"""\n      A -> B\n      |    |\n      |    v\n      +--> C -> D\n      |\n      +--> E\n    """"""\n\n    def node(name, **kwargs):\n        return dict(id=name, lineage=dict(source_datasets=kwargs))\n\n    A, _, C, _, _ = make_graph_abcde(node)\n    rdr = SimpleDocNav(A)\n\n    assert rdr.doc == A\n    assert rdr.doc_without_lineage_sources == node(\'A\')\n    assert isinstance(rdr.sources[\'ae\'], SimpleDocNav)\n    assert rdr.sources[\'ab\'].sources[\'bc\'].doc == C\n    assert rdr.doc_without_lineage_sources is rdr.doc_without_lineage_sources\n    assert rdr.sources is rdr.sources\n    assert isinstance(rdr.sources_path, tuple)\n\n    def visitor(node, name=None, depth=0, out=None):\n        s = \'{}:{}:{:d}\'.format(node.id, name if name else \'..\', depth)\n        out.append(s)\n\n    expect_preorder = \'\'\'\nA:..:0\nB:ab:1\nC:bc:2\nD:cd:3\nC:ac:1\nD:cd:2\nE:ae:1\n\'\'\'.lstrip().rstrip()\n\n    expect_postorder = \'\'\'\nD:cd:3\nC:bc:2\nB:ab:1\nD:cd:2\nC:ac:1\nE:ae:1\nA:..:0\n\'\'\'.lstrip().rstrip()\n\n    for mode, expect in zip([\'pre-order\', \'post-order\'],\n                            [expect_preorder, expect_postorder]):\n        out = []\n        traverse_datasets(rdr, visitor, mode=mode, out=out)\n        assert \'\\n\'.join(out) == expect\n\n    fv = flatten_datasets(rdr)\n\n    assert len(fv[\'A\']) == 1\n    assert len(fv[\'C\']) == 2\n    assert len(fv[\'E\']) == 1\n    assert set(fv.keys()) == set(\'ABCDE\')\n\n    fv, dg = flatten_datasets(rdr, with_depth_grouping=True)\n\n    assert len(fv[\'A\']) == 1\n    assert len(fv[\'C\']) == 2\n    assert len(fv[\'E\']) == 1\n    assert set(fv.keys()) == set(\'ABCDE\')\n    assert isinstance(dg, list)\n    assert len(dg) == 4\n    assert [len(l) for l in dg] == [1, 3, 2, 1]\n\n    def to_set(xx):\n        return set(x.id for x in xx)\n\n    assert [set(s) for s in (\'A\',\n                             \'BCE\',\n                             \'CD\',\n                             \'D\')] == [to_set(xx) for xx in dg]\n\n    with pytest.raises(ValueError):\n        SimpleDocNav([])\n\n\ndef test_dedup():\n    ds0 = SimpleDocNav(gen_dataset_test_dag(1, force_tree=True))\n\n    # make sure ds0 has duplicate C nodes with equivalent data\n    assert ds0.sources[\'ab\'].sources[\'bc\'].doc is not ds0.sources[\'ac\'].doc\n    assert ds0.sources[\'ab\'].sources[\'bc\'].doc == ds0.sources[\'ac\'].doc\n\n    ds = SimpleDocNav(dedup_lineage(ds0))\n    assert ds.sources[\'ab\'].sources[\'bc\'].doc is ds.sources[\'ac\'].doc\n    assert ds.sources[\'ab\'].sources[\'bc\'].sources[\'cd\'].doc is ds.sources[\'ac\'].sources[\'cd\'].doc\n\n    # again but with raw doc\n    ds = SimpleDocNav(dedup_lineage(ds0.doc))\n    assert ds.sources[\'ab\'].sources[\'bc\'].doc is ds.sources[\'ac\'].doc\n    assert ds.sources[\'ab\'].sources[\'bc\'].sources[\'cd\'].doc is ds.sources[\'ac\'].sources[\'cd\'].doc\n\n    # Test that we detect inconsistent metadata for duplicate entries (test 1)\n    # test: different values in the same spot\n    ds0 = SimpleDocNav(gen_dataset_test_dag(3, force_tree=True))\n    ds0.sources[\'ac\'].doc[\'label\'] = \'Modified\'\n    ds0 = SimpleDocNav(ds0.doc)\n    assert ds0.sources[\'ab\'].sources[\'bc\'].doc != ds0.sources[\'ac\'].doc\n\n    with pytest.raises(InvalidDocException, match=r\'Inconsistent metadata .*\'):\n        dedup_lineage(ds0)\n\n    # Test that we detect inconsistent metadata for duplicate entries (test 2)\n    # test: different sources structure\n    ds0 = SimpleDocNav(gen_dataset_test_dag(3, force_tree=True))\n    ds0.sources[\'ac\'].doc[\'lineage\'][\'source_datasets\'][\'extra\'] = ds0.sources[\'ae\'].doc.copy()\n    assert ds0.sources[\'ab\'].sources[\'bc\'].doc != ds0.sources[\'ac\'].doc\n\n    ds0 = SimpleDocNav(ds0.doc)\n\n    with pytest.raises(InvalidDocException, match=r\'Inconsistent lineage .*\'):\n        dedup_lineage(ds0)\n\n    # Test that we detect inconsistent lineage subtrees for duplicate entries\n\n    # Subtest 1: different set of keys\n    ds0 = SimpleDocNav(gen_dataset_test_dag(7, force_tree=True))\n    srcs = toolz.get_in(ds0.sources_path, ds0.sources[\'ac\'].doc)\n\n    assert \'cd\' in srcs\n    srcs[\'cd\'] = {}\n    ds0 = SimpleDocNav(ds0.doc)\n\n    with pytest.raises(InvalidDocException, match=r\'Inconsistent lineage .*\'):\n        dedup_lineage(ds0)\n\n    # Subtest 2: different values for ""child"" nodes\n    ds0 = SimpleDocNav(gen_dataset_test_dag(7, force_tree=True))\n    srcs = toolz.get_in(ds0.sources_path, ds0.sources[\'ac\'].doc)\n\n    assert \'cd\' in srcs\n    srcs[\'cd\'][\'id\'] = \'7fe57724-ed44-4beb-a3ab-c275339049be\'\n    ds0 = SimpleDocNav(ds0.doc)\n\n    with pytest.raises(InvalidDocException, match=r\'Inconsistent lineage .*\'):\n        dedup_lineage(ds0)\n\n    # Subtest 3: different name for child\n    ds0 = SimpleDocNav(gen_dataset_test_dag(7, force_tree=True))\n    srcs = toolz.get_in(ds0.sources_path, ds0.sources[\'ac\'].doc)\n\n    assert \'cd\' in srcs\n    srcs[\'CD\'] = srcs[\'cd\']\n    del srcs[\'cd\']\n    ds0 = SimpleDocNav(ds0.doc)\n\n    with pytest.raises(InvalidDocException, match=r\'Inconsistent lineage .*\'):\n        dedup_lineage(ds0)\n\n\ndef test_remap_lineage_doc():\n    def mk_node(ds, sources):\n        return dict(id=ds.id, **sources)\n\n    ds = SimpleDocNav(gen_dataset_test_dag(3, force_tree=True))\n    xx = remap_lineage_doc(ds, mk_node)\n    assert xx[\'id\'] == ds.id\n    assert xx[\'ac\'][\'id\'] == ds.sources[\'ac\'].id\n\n    xx = remap_lineage_doc(ds.doc, mk_node)\n    assert xx[\'id\'] == ds.id\n    assert xx[\'ac\'][\'id\'] == ds.sources[\'ac\'].id\n\n\ndef test_merge():\n    from datacube.model.utils import merge\n    assert merge(dict(a=1), dict(b=2)) == dict(a=1, b=2)\n    assert merge(dict(a=1, b=2), dict(b=2)) == dict(a=1, b=2)\n\n    with pytest.raises(Exception):\n        merge(dict(a=1, b=2), dict(b=3))\n\n\n@pytest.mark.xfail(True, reason=""Merging dictionaries with content of NaN doesn\'t work currently"")\ndef test_merge_with_nan():\n    from datacube.model.utils import merge\n\n    _nan = float(""nan"")\n    assert _nan != _nan\n    xx = merge(dict(a=_nan), dict(a=_nan))  # <- fails here because of simple equality check\n    assert xx[\'a\'] != xx[\'a\']\n\n\n@pytest.fixture\ndef sample_document_files(data_folder):\n    files = [(\'multi_doc.yml\', 3),\n             (\'multi_doc.yml.gz\', 3),\n             (\'multi_doc.nc\', 3),\n             (\'single_doc.yaml\', 1),\n             (\'sample.json\', 1)]\n\n    files = [(str(os.path.join(data_folder, f)), num_docs)\n             for f, num_docs in files]\n\n    return files\n\n\ndef test_jsonify():\n    from datetime import datetime\n    from uuid import UUID\n    from decimal import Decimal\n\n    assert sorted(jsonify_document({\'a\': (1.0, 2.0, 3.0),\n                                    \'b\': float(""inf""),\n                                    \'c\': datetime(2016, 3, 11),\n                                    \'d\': np.dtype(\'int16\'),\n                                    }).items()) == [\n                                    (\'a\', (1.0, 2.0, 3.0)),\n                                    (\'b\', \'Infinity\'),\n                                    (\'c\', \'2016-03-11T00:00:00\'),\n                                    (\'d\', \'int16\'),\n                                    ]\n\n    # Converts keys to strings:\n    assert sorted(jsonify_document({1: \'a\', \'2\': Decimal(\'2\')}).items()) == [\n        (\'1\', \'a\'), (\'2\', \'2\')]\n\n    assert jsonify_document({\'k\': UUID(""1f231570-e777-11e6-820f-185e0f80a5c0"")}) == {\n        \'k\': \'1f231570-e777-11e6-820f-185e0f80a5c0\'}\n\n\ndef test_netcdf_strings():\n    assert netcdf_extract_string(np.asarray([b\'a\', b\'b\'])) == ""ab""\n    txt = ""some string""\n    assert netcdf_extract_string(txt) is txt\n\n\ndef test_doc_reader():\n    d = DocReader({\'lat\': [\'extent\', \'lat\']}, {}, doc={\'extent\': {\'lat\': 4}})\n    assert hasattr(d, \'lat\')\n    assert d.lat == 4\n    assert d._doc == {\'extent\': {\'lat\': 4}}\n\n    d.lat = 5\n    assert d.lat == 5\n    assert d._doc == {\'extent\': {\'lat\': 5}}\n\n    assert d.search_fields == {}\n\n    assert not hasattr(d, \'no_such\')\n    with pytest.raises(AttributeError):\n        d.no_such\n\n    with pytest.raises(AttributeError):\n        d.no_such = 0\n\n    assert dir(d) == [\'lat\']\n\n    d = DocReader({\'platform\': [\'platform\', \'code\']}, {}, doc={})\n    assert d.platform is None\n\n\ndef test_is_supported_doc_type():\n    assert is_supported_document_type(Path(\'/tmp/something.yaml\'))\n    assert is_supported_document_type(Path(\'/tmp/something.YML\'))\n    assert is_supported_document_type(Path(\'/tmp/something.yaml.gz\'))\n    assert not is_supported_document_type(Path(\'/tmp/something.tif\'))\n    assert not is_supported_document_type(Path(\'/tmp/something.tif.gz\'))\n\n\ndef test_doc_offset():\n    assert get_doc_offset([\'a\'], {\'a\': 4}) == 4\n    assert get_doc_offset([\'a\', \'b\'], {\'a\': {\'b\': 4}}) == 4\n    with pytest.raises(KeyError):\n        get_doc_offset([\'a\'], {})\n\n    assert get_doc_offset_safe([\'a\'], {\'a\': 4}) == 4\n    assert get_doc_offset_safe([\'a\', \'b\'], {\'a\': {\'b\': 4}}) == 4\n    assert get_doc_offset_safe([\'a\'], {}) is None\n    assert get_doc_offset_safe([\'a\', \'b\', \'c\'], {\'a\': {\'b\': {}}}, 10) == 10\n    assert get_doc_offset_safe([\'a\', \'b\', \'c\'], {\'a\': {\'b\': []}}, 11) == 11\n\n    doc = {\'a\': 4}\n    _set_doc_offset([\'a\'], doc, 5)\n    assert doc == {\'a\': 5}\n    doc = {\'a\': {\'b\': 4}}\n    _set_doc_offset([\'a\', \'b\'], doc, \'c\')\n    assert doc == {\'a\': {\'b\': \'c\'}}\n\n\ndef test_transform_object_tree():\n    def add_one(a):\n        return a + 1\n    assert transform_object_tree(add_one, [1, 2, 3]) == [2, 3, 4]\n    assert transform_object_tree(add_one, {\'a\': 1, \'b\': 2, \'c\': 3}) == {\'a\': 2, \'b\': 3, \'c\': 4}\n    assert transform_object_tree(add_one, {\'a\': 1, \'b\': (2, 3), \'c\': [4, 5]}) == {\'a\': 2, \'b\': (3, 4), \'c\': [5, 6]}\n    assert transform_object_tree(add_one, {1: 1, \'2\': 2, 3.0: 3}, key_transform=float) == {1.0: 2, 2.0: 3, 3.0: 4}\n    # Order must be maintained\n    assert transform_object_tree(add_one, OrderedDict([(\'z\', 1), (\'w\', 2), (\'y\', 3), (\'s\', 7)])) \\\n        == OrderedDict([(\'z\', 2), (\'w\', 3), (\'y\', 4), (\'s\', 8)])\n'"
tests/test_utils_generic.py,0,"b'from queue import Queue\nfrom datacube.utils.generic import (\n    qmap,\n    it2q,\n    map_with_lookahead,\n    thread_local_cache,\n)\n\n\ndef test_map_with_lookahead():\n    def if_one(x):\n        return \'one\' + str(x)\n\n    def if_many(x):\n        return \'many\' + str(x)\n\n    assert list(map_with_lookahead(iter([]), if_one, if_many)) == []\n    assert list(map_with_lookahead(iter([1]), if_one, if_many)) == [if_one(1)]\n    assert list(map_with_lookahead(range(5), if_one, if_many)) == list(map(if_many, range(5)))\n    assert list(map_with_lookahead(range(10), if_one=if_one)) == list(range(10))\n    assert list(map_with_lookahead(iter([1]), if_many=if_many)) == [1]\n\n\ndef test_qmap():\n    q = Queue(maxsize=100)\n    it2q(range(10), q)\n    rr = [x for x in qmap(str, q)]\n    assert rr == [str(x) for x in range(10)]\n    q.join()  # should not block\n\n\ndef test_thread_local_cache():\n    name = ""test_0123394""\n    v = {}\n\n    assert thread_local_cache(name, v) is v\n    assert thread_local_cache(name) is v\n    assert thread_local_cache(name, purge=True) is v\n    assert thread_local_cache(name, 33) == 33\n    assert thread_local_cache(name, purge=True) == 33\n\n    assert thread_local_cache(""no_such_key"", purge=True) is None\n    assert thread_local_cache(""no_such_key"", 111, purge=True) == 111\n'"
tests/test_utils_other.py,44,"b'""""""\nTest utility functions from :module:`datacube.utils`\n\n\n""""""\nimport os\nimport pathlib\nimport string\nfrom pathlib import Path\n\nimport numpy as np\nimport pytest\nimport rasterio\nimport xarray as xr\nfrom dateutil.parser import parse\nfrom hypothesis import given\nfrom hypothesis.strategies import integers, text\nfrom pandas import to_datetime\n\nfrom datacube.helpers import write_geotiff\nfrom datacube.utils import gen_password, write_user_secret_file, slurp\nfrom datacube.model.utils import xr_apply\nfrom datacube.utils.dates import date_sequence\nfrom datacube.utils.math import (\n    num2numpy,\n    is_almost_int,\n    valid_mask,\n    invalid_mask,\n    clamp,\n    unsqueeze_data_array,\n    unsqueeze_dataset,\n    spatial_dims,\n    data_resolution_and_offset,\n    affine_from_axis,\n)\nfrom datacube.utils.py import sorted_items\nfrom datacube.utils.uris import (uri_to_local_path, mk_part_uri, get_part_from_uri, as_url, is_url,\n                                 pick_uri, uri_resolve, is_vsipath,\n                                 normalise_path, default_base_dir)\nfrom datacube.utils.io import check_write_path\nfrom datacube.testutils import mk_sample_product, remove_crs\n\n\ndef test_stats_dates():\n    # Winter for 1990\n    winter_1990 = list(date_sequence(start=to_datetime(\'1990-06-01\'), end=to_datetime(\'1990-09-01\'), step_size=\'3m\',\n                                     stats_duration=\'3m\'))\n    assert winter_1990 == [(parse(\'1990-06-01\'), parse(\'1990-09-01\'))]\n\n    # Every winter from 1990 - 1992\n    three_years_of_winter = list(date_sequence(start=to_datetime(\'1990-06-01\'), end=to_datetime(\'1992-09-01\'),\n                                               step_size=\'1y\',\n                                               stats_duration=\'3m\'))\n    assert three_years_of_winter == [(parse(\'1990-06-01\'), parse(\'1990-09-01\')),\n                                     (parse(\'1991-06-01\'), parse(\'1991-09-01\')),\n                                     (parse(\'1992-06-01\'), parse(\'1992-09-01\'))]\n\n    # Full years from 1990 - 1994\n    five_full_years = list(date_sequence(start=to_datetime(\'1990-01-01\'), end=to_datetime(\'1995\'), step_size=\'1y\',\n                                         stats_duration=\'1y\'))\n    assert five_full_years == [(parse(\'1990-01-01\'), parse(\'1991-01-01\')),\n                               (parse(\'1991-01-01\'), parse(\'1992-01-01\')),\n                               (parse(\'1992-01-01\'), parse(\'1993-01-01\')),\n                               (parse(\'1993-01-01\'), parse(\'1994-01-01\')),\n                               (parse(\'1994-01-01\'), parse(\'1995-01-01\'))]\n\n    # Every season (three months), starting in March, from 1990 until end 1992-02\n    two_years_of_seasons = list(date_sequence(start=to_datetime(\'1990-03-01\'), end=to_datetime(\'1992-03\'),\n                                              step_size=\'3m\',\n                                              stats_duration=\'3m\'))\n    assert len(two_years_of_seasons) == 8\n    assert two_years_of_seasons == [(parse(\'1990-03-01\'), parse(\'1990-06-01\')),\n                                    (parse(\'1990-06-01\'), parse(\'1990-09-01\')),\n                                    (parse(\'1990-09-01\'), parse(\'1990-12-01\')),\n                                    (parse(\'1990-12-01\'), parse(\'1991-03-01\')),\n                                    (parse(\'1991-03-01\'), parse(\'1991-06-01\')),\n                                    (parse(\'1991-06-01\'), parse(\'1991-09-01\')),\n                                    (parse(\'1991-09-01\'), parse(\'1991-12-01\')),\n                                    (parse(\'1991-12-01\'), parse(\'1992-03-01\'))]  # Leap year!\n\n    # Every month from 1990-01 to 1990-06\n    monthly = list(date_sequence(start=to_datetime(\'1990-01-01\'), end=to_datetime(\'1990-07-01\'), step_size=\'1m\',\n                                 stats_duration=\'1m\'))\n    assert len(monthly) == 6\n\n    # Complex\n    # I want the average over 5 years\n\n\ndef test_uri_to_local_path():\n    if os.name == \'nt\':\n        assert \'C:\\\\tmp\\\\test.tmp\' == str(uri_to_local_path(\'file:///C:/tmp/test.tmp\'))\n        assert \'\\\\\\\\remote\\\\path\\\\file.txt\' == str(uri_to_local_path(\'file://remote/path/file.txt\'))\n\n    else:\n        assert \'/tmp/something.txt\' == str(uri_to_local_path(\'file:///tmp/something.txt\'))\n\n        with pytest.raises(ValueError):\n            uri_to_local_path(\'file://remote/path/file.txt\')\n\n    assert uri_to_local_path(None) is None\n\n    with pytest.raises(ValueError):\n        uri_to_local_path(\'ftp://example.com/tmp/something.txt\')\n\n\n@pytest.mark.parametrize(""base"", [\n    ""s3://foo"",\n    ""gs://foo"",\n    ""wasb://foo"",\n    ""wasbs://foo"",\n    ""/vsizip//vsicurl/https://host.tld/some/path"",\n])\ndef test_uri_resolve(base):\n    abs_path = \'/abs/path/to/something\'\n    some_uri = \'http://example.com/file.txt\'\n\n    assert uri_resolve(base, abs_path) == ""file://"" + abs_path\n    assert uri_resolve(base, some_uri) is some_uri\n    assert uri_resolve(base, None) is base\n    assert uri_resolve(base, \'\') is base\n    assert uri_resolve(base, \'relative/path\') == base + \'/relative/path\'\n    assert uri_resolve(base + \'/\', \'relative/path\') == base + \'/relative/path\'\n    assert uri_resolve(base + \'/some/dir/\', \'relative/path\') == base + \'/some/dir/relative/path\'\n\n    if not is_vsipath(base):\n        assert uri_resolve(base + \'/some/dir/file.txt\', \'relative/path\') == base + \'/some/dir/relative/path\'\n\n\ndef test_pick_uri():\n    f, s, h = (\'file://a\', \'s3://b\', \'http://c\')\n\n    assert pick_uri([f, s, h]) is f\n    assert pick_uri([s, h, f]) is f\n    assert pick_uri([s, h]) is s\n    assert pick_uri([h, s]) is h\n    assert pick_uri([f, s, h], \'http:\') is h\n    assert pick_uri([f, s, h], \'s3:\') is s\n    assert pick_uri([f, s, h], \'file:\') is f\n\n    with pytest.raises(ValueError):\n        pick_uri([])\n\n    with pytest.raises(ValueError):\n        pick_uri([f, s, h], \'ftp:\')\n\n    with pytest.raises(ValueError):\n        pick_uri([s, h], \'file:\')\n\n\n@given(integers(), integers(), integers())\ndef test_clamp(x, lower_bound, upper_bound):\n    if lower_bound > upper_bound:\n        lower_bound, upper_bound = upper_bound, lower_bound\n    new_x = clamp(x, lower_bound, upper_bound)\n\n    # If x was already between the bounds, it shouldn\'t have changed\n    if lower_bound <= x <= upper_bound:\n        assert new_x == x\n    assert lower_bound <= new_x <= upper_bound\n\n\n@given(integers(min_value=10, max_value=30))\ndef test_gen_pass(n_bytes):\n    password1 = gen_password(n_bytes)\n    password2 = gen_password(n_bytes)\n    assert len(password1) >= n_bytes\n    assert len(password2) >= n_bytes\n    assert password1 != password2\n\n\n@given(text(alphabet=string.digits + string.ascii_letters + \' ,:.![]?\', max_size=20))\ndef test_write_user_secret_file(txt):\n    fname = u"".tst-datacube-uefvwr4cfkkl0ijk.txt""\n\n    write_user_secret_file(txt, fname)\n    txt_back = slurp(fname)\n    os.remove(fname)\n    assert txt == txt_back\n    assert slurp(fname) is None\n\n\ndef test_write_geotiff(tmpdir, odc_style_xr_dataset):\n    """"""Ensure the geotiff helper writer works, and supports datasets smaller than 256x256.""""""\n    filename = tmpdir + \'/test.tif\'\n\n    assert len(odc_style_xr_dataset.latitude) < 256\n\n    with pytest.warns(DeprecationWarning):\n        write_geotiff(filename, odc_style_xr_dataset)\n\n    assert filename.exists()\n\n    with rasterio.open(str(filename)) as src:\n        written_data = src.read(1)\n\n        assert (written_data == odc_style_xr_dataset[\'B10\']).all()\n\n\ndef test_write_geotiff_str_crs(tmpdir, odc_style_xr_dataset):\n    """"""Ensure the geotiff helper writer works, and supports crs as a string.""""""\n    filename = tmpdir + \'/test.tif\'\n\n    original_crs = odc_style_xr_dataset.crs\n\n    odc_style_xr_dataset.attrs[\'crs\'] = str(original_crs)\n\n    with pytest.warns(DeprecationWarning):\n        write_geotiff(filename, odc_style_xr_dataset)\n\n    assert filename.exists()\n\n    with rasterio.open(str(filename)) as src:\n        written_data = src.read(1)\n\n        assert (written_data == odc_style_xr_dataset[\'B10\']).all()\n\n    odc_style_xr_dataset = remove_crs(odc_style_xr_dataset)\n    with pytest.raises(ValueError):\n        with pytest.warns(DeprecationWarning):\n            write_geotiff(filename, odc_style_xr_dataset)\n\n\ndef test_testutils_mk_sample():\n    pp = mk_sample_product(\'tt\', measurements=[(\'aa\', \'int16\', -999),\n                                               (\'bb\', \'float32\', np.nan)])\n    assert set(pp.measurements) == {\'aa\', \'bb\'}\n\n    pp = mk_sample_product(\'tt\', measurements=[\'aa\', \'bb\'])\n    assert set(pp.measurements) == {\'aa\', \'bb\'}\n\n    pp = mk_sample_product(\'tt\', measurements=[dict(name=n) for n in [\'aa\', \'bb\']])\n    assert set(pp.measurements) == {\'aa\', \'bb\'}\n\n    with pytest.raises(ValueError):\n        mk_sample_product(\'tt\', measurements=[None])\n\n\ndef test_testutils_write_files():\n    from datacube.testutils import write_files, assert_file_structure\n\n    files = {\'a.txt\': \'string\',\n             \'aa.txt\': (\'line1\\n\', \'line2\\n\')}\n\n    pp = write_files(files)\n    assert pp.exists()\n    assert_file_structure(pp, files)\n\n    # test that we detect missing files\n    (pp / \'a.txt\').unlink()\n\n    with pytest.raises(AssertionError):\n        assert_file_structure(pp, files)\n\n    with pytest.raises(AssertionError):\n        assert_file_structure(pp, {\'aa.txt\': 3})\n\n    with pytest.raises(ValueError):\n        write_files({\'tt\': 3})\n\n\ndef test_part_uri():\n    base = \'file:///foo.txt\'\n\n    for i in range(10):\n        assert get_part_from_uri(mk_part_uri(base, i)) == i\n\n    assert get_part_from_uri(\'file:///f.txt\') is None\n    assert get_part_from_uri(\'file:///f.txt#something_else\') is None\n    assert get_part_from_uri(\'file:///f.txt#part=aa\') == \'aa\'\n    assert get_part_from_uri(\'file:///f.txt#part=111\') == 111\n\n\ndef test_xr_apply():\n    src = xr.DataArray(np.asarray([1, 2, 3], dtype=\'uint8\'), dims=[\'time\'])\n    dst = xr_apply(src, lambda _, v: v, dtype=\'float32\')\n\n    assert dst.dtype.name == \'float32\'\n    assert dst.shape == src.shape\n    assert dst.values.tolist() == [1, 2, 3]\n\n    dst = xr_apply(src, lambda _, v: v)\n    assert dst.dtype.name == \'uint8\'\n    assert dst.shape == src.shape\n    assert dst.values.tolist() == [1, 2, 3]\n\n    dst = xr_apply(src, lambda idx, _, v: idx[0] + v, with_numeric_index=True)\n    assert dst.dtype.name == \'uint8\'\n    assert dst.shape == src.shape\n    assert dst.values.tolist() == [0 + 1, 1 + 2, 2 + 3]\n\n\ndef test_sorted_items():\n    aa = dict(c=1, b={}, a=[])\n\n    assert \'\'.join(k for k, _ in sorted_items(aa)) == \'abc\'\n    assert \'\'.join(k for k, _ in sorted_items(aa, key=lambda x: x)) == \'abc\'\n    assert \'\'.join(k for k, _ in sorted_items(aa, reverse=True)) == \'cba\'\n\n    remap = dict(c=0, a=1, b=2)\n    assert \'\'.join(k for k, _ in sorted_items(aa, key=lambda x: remap[x])) == \'cab\'\n\n\ndef test_default_base_dir(monkeypatch):\n    def set_pwd(p):\n        if p is None:\n            monkeypatch.delenv(\'PWD\')\n        else:\n            monkeypatch.setenv(\'PWD\', str(p))\n\n    cwd = Path(\'.\').resolve()\n\n    # Default base dir (once resolved) will never be different from cwd\n    assert default_base_dir().resolve() == cwd\n\n    # should work when PWD is not set\n    set_pwd(None)\n    assert \'PWD\' not in os.environ\n    assert default_base_dir() == cwd\n\n    # should work when PWD is not absolute path\n    set_pwd(\'this/is/not/a/valid/path\')\n    assert default_base_dir() == cwd\n\n    # should be cwd when PWD points to some other dir\n    set_pwd(cwd / \'deeper\')\n    assert default_base_dir() == cwd\n\n    set_pwd(cwd.parent)\n    assert default_base_dir() == cwd\n\n    # PWD == cwd\n    set_pwd(cwd)\n    assert default_base_dir() == cwd\n\n    # TODO:\n    # - create symlink to current directory in temp\n    # - set PWD to that link\n    # - make sure that returned path is the same as symlink and different from cwd\n\n\ndef test_time_info():\n    from datacube.model.utils import time_info\n    from datetime import datetime\n\n    date = \'2019-03-03T00:00:00\'\n    ee = time_info(datetime(2019, 3, 3))\n    assert ee[\'extent\'][\'from_dt\'] == date\n    assert ee[\'extent\'][\'to_dt\'] == date\n    assert ee[\'extent\'][\'center_dt\'] == date\n    assert len(ee[\'extent\']) == 3\n\n    ee = time_info(datetime(2019, 3, 3), key_time=datetime(2019, 4, 4))\n    assert ee[\'extent\'][\'from_dt\'] == date\n    assert ee[\'extent\'][\'to_dt\'] == date\n    assert ee[\'extent\'][\'center_dt\'] == date\n    assert ee[\'extent\'][\'key_time\'] == \'2019-04-04T00:00:00\'\n    assert len(ee[\'extent\']) == 4\n\n\ndef test_normalise_path():\n    cwd = Path(\'.\').resolve()\n    assert normalise_path(\'.\').resolve() == cwd\n\n    p = Path(\'/a/b/c/d.txt\')\n    assert normalise_path(p) == Path(p)\n    assert normalise_path(str(p)) == Path(p)\n\n    base = Path(\'/a/b/\')\n    p = Path(\'c/d.txt\')\n    assert normalise_path(p, base) == (base / p)\n    assert normalise_path(str(p), str(base)) == (base / p)\n    assert normalise_path(p) == (cwd / p)\n\n    with pytest.raises(ValueError):\n        normalise_path(p, \'not/absolute/path\')\n\n\ndef test_testutils_testimage():\n    from datacube.testutils import mk_test_image, split_test_image\n\n    for dtype in (\'uint16\', \'uint32\', \'int32\', \'float32\'):\n        aa = mk_test_image(128, 64, dtype=dtype, nodata=None)\n        assert aa.shape == (64, 128)\n        assert aa.dtype == dtype\n\n        xx, yy = split_test_image(aa)\n        assert (xx[:, 33] == 33).all()\n        assert (xx[:, 127] == 127).all()\n        assert (yy[23, :] == 23).all()\n        assert (yy[63, :] == 63).all()\n\n\ndef test_testutils_gtif(tmpdir):\n    from datacube.testutils import mk_test_image\n    from datacube.testutils.io import write_gtiff, rio_slurp\n\n    w, h, dtype, nodata, ndw = 96, 64, \'int16\', -999, 7\n\n    aa = mk_test_image(w, h, dtype, nodata, nodata_width=ndw)\n    bb = mk_test_image(w, h, dtype, nodata=None)\n\n    assert aa.shape == (h, w)\n    assert aa.dtype.name == dtype\n    assert aa[10, 30] == (30 << 8) | 10\n    assert aa[10, 11] == nodata\n    assert bb[10, 11] == (11 << 8) | 10\n\n    aa5 = np.stack((aa,) * 5)\n\n    fname = pathlib.Path(str(tmpdir / ""aa.tiff""))\n    fname5 = pathlib.Path(str(tmpdir / ""aa5.tiff""))\n\n    aa_meta = write_gtiff(fname, aa, nodata=nodata,\n                          blocksize=128,\n                          resolution=(100, -100),\n                          offset=(12300, 11100),\n                          overwrite=True)\n\n    aa5_meta = write_gtiff(str(fname5), aa5, nodata=nodata,\n                           resolution=(100, -100),\n                           offset=(12300, 11100),\n                           overwrite=True)\n\n    assert fname.exists()\n    assert fname5.exists()\n\n    assert aa_meta.gbox.shape == (h, w)\n    assert aa_meta.path is fname\n\n    aa_, aa_meta_ = rio_slurp(fname)\n    aa5_, aa5_meta_ = rio_slurp(fname5)\n\n    assert aa_meta_.path is fname\n\n    (sx, _, tx,\n     _, sy, ty, *_) = aa5_meta_.transform\n\n    assert (tx, ty) == (12300, 11100)\n    assert (sx, sy) == (100, -100)\n\n    np.testing.assert_array_equal(aa, aa_)\n    np.testing.assert_array_equal(aa5, aa5_)\n\n    assert aa_meta_.transform == aa_meta.transform\n    assert aa5_meta_.transform == aa5_meta.transform\n\n    # check that overwrite is off by default\n    with pytest.raises(IOError):\n        write_gtiff(fname, aa, nodata=nodata,\n                    blocksize=128)\n\n    # check that overwrite re-writes file\n    write_gtiff(fname, bb[:32, :32],\n                gbox=aa_meta.gbox[:32, :32],\n                overwrite=True)\n\n    bb_, mm = rio_slurp(fname, (32, 32))\n    np.testing.assert_array_equal(bb[:32, :32], bb_)\n\n    assert mm.gbox == aa_meta.gbox[:32, :32]\n\n    with pytest.raises(ValueError):\n        write_gtiff(fname, np.zeros((3, 4, 5, 6)))\n\n\ndef test_testutils_geobox():\n    from datacube.testutils.io import dc_crs_from_rio, rio_geobox\n    from rasterio.crs import CRS\n    from affine import Affine\n\n    assert rio_geobox({}) is None\n\n    transform = Affine(10, 0, 4676,\n                       0, -10, 171878)\n\n    shape = (100, 640)\n    h, w = shape\n    crs = CRS.from_epsg(3578)\n\n    meta = dict(width=w, height=h, transform=transform, crs=crs)\n    gbox = rio_geobox(meta)\n\n    assert gbox.shape == shape\n    assert gbox.crs.epsg == 3578\n    assert gbox.transform == transform\n\n    wkt = \'\'\'PROJCS[""unnamed"",\n    GEOGCS[""NAD83"",\n       DATUM[""North_American_Datum_1983"",\n             SPHEROID[""GRS 1980"",6378137,298.257222101, AUTHORITY[""EPSG"",""7019""]],\n             TOWGS84[0,0,0,0,0,0,0],AUTHORITY[""EPSG"",""6269""]],\n       PRIMEM[""Greenwich"",0,AUTHORITY[""EPSG"",""8901""]],\n       UNIT[""degree"",0.0174532925199433,AUTHORITY[""EPSG"",""9122""]],\n       ],\n    PROJECTION[""Albers_Conic_Equal_Area""],\n    PARAMETER[""standard_parallel_1"",61.66666666666666],\n    PARAMETER[""standard_parallel_2"",68],\n    PARAMETER[""latitude_of_center"",59],\n    PARAMETER[""longitude_of_center"",-132.5],\n    PARAMETER[""false_easting"",500000],\n    PARAMETER[""false_northing"",500000],\n    UNIT[""Meter"",1]]\n    \'\'\'\n\n    crs_ = dc_crs_from_rio(CRS.from_wkt(wkt))\n    assert crs_.epsg is None\n\n\n@pytest.mark.parametrize(""test_input,expected"", [\n    (""/foo/bar/file.txt"", False),\n    (""file:///foo/bar/file.txt"", True),\n    (""test.bar"", False),\n    (""s3://mybucket/objname.tiff"", True),\n    (""gs://mybucket/objname.tiff"", True),\n    (""wasb://mybucket/objname.tiff"", True),\n    (""wasbs://mybucket/objname.tiff"", True),\n    (""ftp://host.name/filename.txt"", True),\n    (""https://host.name.com/path/file.txt"", True),\n    (""http://host.name.com/path/file.txt"", True),\n    (""sftp://user:pass@host.name.com/path/file.txt"", True),\n    (""file+gzip://host.name.com/path/file.txt"", True),\n    (""bongo:host.name.com/path/file.txt"", False),\n])\ndef test_is_url(test_input, expected):\n    assert is_url(test_input) == expected\n    if expected:\n        assert as_url(test_input) is test_input\n\n\ndef test_is_almost_int():\n    assert is_almost_int(1, 1e-10)\n    assert is_almost_int(1.001, .1)\n    assert is_almost_int(2 - 0.001, .1)\n    assert is_almost_int(-1.001, .1)\n\n\ndef test_valid_mask():\n    xx = np.zeros((4, 8), dtype=\'float32\')\n    mm = valid_mask(xx, 0)\n    assert mm.dtype == \'bool\'\n    assert mm.shape == xx.shape\n    assert not mm.all()\n    assert not mm.any()\n    nn = invalid_mask(xx, 0)\n    assert nn.dtype == \'bool\'\n    assert nn.shape == xx.shape\n    assert nn.all()\n    assert nn.any()\n\n    mm = valid_mask(xx, 13)\n    assert mm.dtype == \'bool\'\n    assert mm.shape == xx.shape\n    assert mm.all()\n    nn = invalid_mask(xx, 13)\n    assert nn.dtype == \'bool\'\n    assert nn.shape == xx.shape\n    assert not nn.any()\n\n    mm = valid_mask(xx, None)\n    assert mm.dtype == \'bool\'\n    assert mm.shape == xx.shape\n    assert mm.all()\n    nn = invalid_mask(xx, None)\n    assert nn.dtype == \'bool\'\n    assert nn.shape == xx.shape\n    assert not nn.any()\n\n    mm = valid_mask(xx, np.nan)\n    assert mm.dtype == \'bool\'\n    assert mm.shape == xx.shape\n    assert mm.all()\n    nn = invalid_mask(xx, np.nan)\n    assert nn.dtype == \'bool\'\n    assert nn.shape == xx.shape\n    assert not nn.any()\n\n    xx[0, 0] = np.nan\n    mm = valid_mask(xx, np.nan)\n    assert not mm[0, 0]\n    assert mm.sum() == (4 * 8 - 1)\n    nn = invalid_mask(xx, np.nan)\n    assert nn[0, 0]\n    assert nn.sum() == 1\n\n\ndef test_num2numpy():\n    assert num2numpy(None, \'int8\') is None\n    assert num2numpy(-1, \'int8\').dtype == np.dtype(\'int8\')\n    assert num2numpy(-1, \'int8\').dtype == np.int8(-1)\n\n    assert num2numpy(-1, \'uint8\') is None\n    assert num2numpy(256, \'uint8\') is None\n    assert num2numpy(-1, \'uint16\') is None\n    assert num2numpy(-1, \'uint32\') is None\n    assert num2numpy(-1, \'uint8\', ignore_range=True) == np.uint8(255)\n\n    assert num2numpy(0, \'uint8\') == 0\n    assert num2numpy(255, \'uint8\') == 255\n    assert num2numpy(-128, \'int8\') == -128\n    assert num2numpy(127, \'int8\') == 127\n    assert num2numpy(128, \'int8\') is None\n\n    assert num2numpy(3.3, np.dtype(\'float32\')).dtype == np.dtype(\'float32\')\n    assert num2numpy(3.3, np.float32).dtype == np.dtype(\'float32\')\n    assert num2numpy(3.3, np.float64).dtype == np.dtype(\'float64\')\n\n\ndef test_utils_datares():\n    assert data_resolution_and_offset(np.array([1.5, 2.5, 3.5])) == (1.0, 1.0)\n    assert data_resolution_and_offset(np.array([5, 3, 1])) == (-2.0, 6.0)\n    assert data_resolution_and_offset(np.array([5, 3])) == (-2.0, 6.0)\n    assert data_resolution_and_offset(np.array([1.5]), 1) == (1.0, 1.0)\n\n    with pytest.raises(ValueError):\n        data_resolution_and_offset(np.array([]))\n\n    with pytest.raises(ValueError):\n        data_resolution_and_offset(np.array([]), 10)\n\n    with pytest.raises(ValueError):\n        data_resolution_and_offset(np.array([1]))\n\n\ndef test_utils_affine_from_axis():\n    assert affine_from_axis(np.asarray([1.5, 2.5, 3.5]),\n                            np.asarray([10.5, 11.5])) * (0, 0) == (1.0, 10.0)\n\n    assert affine_from_axis(np.asarray([1.5, 2.5, 3.5]),\n                            np.asarray([10.5, 11.5])) * (2, 1) == (3, 11)\n\n    (sx, z1, tx,\n     z2, sy, ty, *_) = affine_from_axis(np.asarray([1, 2, 3]),\n                                        np.asarray([10, 20]))\n    assert z1 == 0 and z2 == 0\n    assert sx == 1 and sy == 10\n    assert tx == 0.5 and ty == 5\n\n    (sx, _, tx,\n     _, sy, ty, *_) = affine_from_axis(np.asarray([1]),\n                                       np.asarray([10, 20]), 1)\n    assert sx == 1 and sy == 10\n    assert tx == 0.5 and ty == 5\n\n    (sx, _, tx,\n     _, sy, ty, *_) = affine_from_axis(np.asarray([1]),\n                                       np.asarray([10, 20]), (1, 1))\n    assert sx == 1 and sy == 10\n    assert tx == 0.5 and ty == 5\n\n    (sx, _, tx,\n     _, sy, ty, *_) = affine_from_axis(np.asarray([1]),\n                                       np.asarray([10]), (2, 10))\n    assert sx == 2 and sy == 10\n    assert tx == 0 and ty == 5\n\n\ndef test_utils_math():\n    xx = xr.DataArray(np.zeros((3, 4)),\n                      name=\'xx\',\n                      dims=(\'y\', \'x\'),\n                      coords={\'x\': np.arange(4),\n                              \'y\': np.arange(3)})\n    xx_t = unsqueeze_data_array(xx, \'time\', 0)\n    assert xx_t.dims == (\'time\', \'y\', \'x\')\n    assert \'time\' in xx_t.coords\n    assert xx_t.data.shape == (1, 3, 4)\n\n    ds = unsqueeze_dataset(xx.to_dataset(), \'time\')\n    assert ds.xx.dims == (\'time\', \'y\', \'x\')\n    assert \'time\' in ds.xx.coords\n    assert ds.xx.data.shape == (1, 3, 4)\n\n\ndef test_spatial_dims():\n    def tt(d1, d2):\n        coords = {}\n        coords[d1] = np.arange(3)\n        coords[d2] = np.arange(4)\n        return xr.DataArray(np.zeros((3, 4)),\n                            name=\'xx\',\n                            dims=(d1, d2),\n                            coords=coords)\n\n    assert spatial_dims(tt(\'y\', \'x\')) == (\'y\', \'x\')\n    assert spatial_dims(tt(\'x\', \'y\')) == (\'y\', \'x\')\n    assert spatial_dims(tt(\'latitude\', \'longitude\')) == (\'latitude\', \'longitude\')\n    assert spatial_dims(tt(\'lat\', \'lon\')) == (\'lat\', \'lon\')\n    assert spatial_dims(tt(\'a\', \'b\')) is None\n    assert spatial_dims(tt(\'a\', \'b\'), relaxed=True) == (\'a\', \'b\')\n\n\ndef test_check_write_path(tmpdir):\n    tmpdir = Path(str(tmpdir))\n    some_path = tmpdir/""_should_not_exist-5125177.txt""\n    assert not some_path.exists()\n    assert check_write_path(some_path, overwrite=False) is some_path\n    assert check_write_path(str(some_path), overwrite=False) == some_path\n    assert isinstance(check_write_path(str(some_path), overwrite=False), Path)\n\n    p = tmpdir/""ttt.tmp""\n    with open(str(p), \'wt\') as f:\n        f.write(""text"")\n\n    assert p.exists()\n    with pytest.raises(IOError):\n        check_write_path(p, overwrite=False)\n\n    assert check_write_path(p, overwrite=True) == p\n    assert not p.exists()\n'"
tests/test_utils_rio.py,0,"b'import pytest\nimport mock\nimport os\n\nfrom datacube.testutils import write_files\nfrom datacube.utils.rio import (\n    activate_rio_env,\n    deactivate_rio_env,\n    get_rio_env,\n    set_default_rio_config,\n    activate_from_config,\n    configure_s3_access,\n)\n\n\ndef test_rio_env_no_aws():\n    deactivate_rio_env()\n\n    # make sure we start without env configured\n    assert get_rio_env() == {}\n\n    ee = activate_rio_env(FAKE_OPTION=1)\n    assert isinstance(ee, dict)\n    assert ee == get_rio_env()\n    assert \'GDAL_DISABLE_READDIR_ON_OPEN\' not in ee\n    if os.getenv(\'GDAL_DATA\', None):\n        assert \'GDAL_DATA\' in ee\n    assert ee.get(\'FAKE_OPTION\') == 1\n    assert \'AWS_ACCESS_KEY_ID\' not in ee\n\n    ee = activate_rio_env(cloud_defaults=True)\n    assert \'GDAL_DISABLE_READDIR_ON_OPEN\' in ee\n    assert ee == get_rio_env()\n\n    deactivate_rio_env()\n    assert get_rio_env() == {}\n\n\ndef test_rio_env_aws():\n    deactivate_rio_env()\n\n    # make sure we start without env configured\n    assert get_rio_env() == {}\n\n    with pytest.raises(ValueError):\n        activate_rio_env(aws=\'something\')\n\n    # note: setting region_name to avoid auto-lookup\n    ee = activate_rio_env(aws=dict(aws_unsigned=True,\n                                   region_name=\'us-west-1\'))\n\n    assert ee.get(\'AWS_NO_SIGN_REQUEST\') == \'YES\'\n\n    ee = activate_rio_env(cloud_defaults=True,\n                          aws=dict(aws_secret_access_key=\'blabla\',\n                                   aws_access_key_id=\'not a real one\',\n                                   aws_session_token=\'faketoo\',\n                                   region_name=\'us-west-1\'))\n\n    assert \'AWS_NO_SIGN_REQUEST\' not in ee\n    # check secrets are sanitized\n    assert ee.get(\'AWS_ACCESS_KEY_ID\') == \'xx..xx\'\n    assert ee.get(\'AWS_SECRET_ACCESS_KEY\') == \'xx..xx\'\n    assert ee.get(\'AWS_SESSION_TOKEN\') == \'xx..xx\'\n\n    assert ee.get(\'AWS_REGION\') == \'us-west-1\'\n\n    # check sanitize can be turned off\n    ee = get_rio_env(sanitize=False)\n    assert ee.get(\'AWS_SECRET_ACCESS_KEY\') == \'blabla\'\n    assert ee.get(\'AWS_ACCESS_KEY_ID\') == \'not a real one\'\n    assert ee.get(\'AWS_SESSION_TOKEN\') == \'faketoo\'\n\n    deactivate_rio_env()\n    assert get_rio_env() == {}\n\n\ndef test_rio_env_aws_auto_region(monkeypatch, without_aws_env):\n    import datacube.utils.aws\n\n    pp = write_files({\n        ""config"": """"""[default]\n""""""})\n\n    assert (pp/""config"").exists()\n    monkeypatch.setenv(""AWS_CONFIG_FILE"", str(pp/""config""))\n\n    assert datacube.utils.aws.botocore_default_region() is None\n\n    aws = dict(aws_secret_access_key=\'blabla\',\n               aws_access_key_id=\'not a real one\',\n               aws_session_token=\'faketoo\')\n\n    with mock.patch(\'datacube.utils.aws.ec2_current_region\',\n                    return_value=\'TT\'):\n        ee = activate_rio_env(aws=aws)\n        assert ee.get(\'AWS_REGION\') == \'TT\'\n\n    with mock.patch(\'datacube.utils.aws.ec2_current_region\',\n                    return_value=None):\n        ee = activate_rio_env(aws=aws)\n        assert \'AWS_REGION\' not in ee\n\n        with pytest.raises(ValueError):\n            activate_rio_env(aws=dict(region_name=\'auto\'))\n\n    deactivate_rio_env()\n    assert get_rio_env() == {}\n\n\ndef test_rio_env_aws_auto_region_dummy():\n    ""Just call it we don\'t know if it will succeed""\n\n    # at least it should not raise error since we haven\'t asked for region_name=\'auto\'\n    ee = activate_rio_env(aws={})\n    assert isinstance(ee, dict)\n\n    deactivate_rio_env()\n    assert get_rio_env() == {}\n\n\ndef test_rio_env_via_config():\n    ee = activate_from_config()\n    assert ee is not None\n\n    # Second call should not change anything\n    assert activate_from_config() is None\n\n    set_default_rio_config(aws=None, cloud_defaults=True)\n\n    # config change should activate new env\n    ee = activate_from_config()\n    assert ee is not None\n    assert \'GDAL_DISABLE_READDIR_ON_OPEN\' in ee\n\n    deactivate_rio_env()\n    assert get_rio_env() == {}\n\n\ndef test_rio_configure_aws_access(monkeypatch, without_aws_env, dask_client):\n    monkeypatch.setenv(""AWS_ACCESS_KEY_ID"", ""fake-key-id"")\n    monkeypatch.setenv(""AWS_SECRET_ACCESS_KEY"", ""fake-secret"")\n    monkeypatch.setenv(""AWS_DEFAULT_REGION"", ""fake-region"")\n\n    creds = configure_s3_access()\n    cc = creds.get_frozen_credentials()\n    assert cc.access_key == \'fake-key-id\'\n    assert cc.secret_key == \'fake-secret\'\n    assert cc.token is None\n\n    ee = activate_from_config()\n    assert ee is not None\n    assert \'AWS_ACCESS_KEY_ID\' in ee\n    assert \'AWS_SECRET_ACCESS_KEY\' in ee\n    assert \'AWS_REGION\' in ee\n    assert \'AWS_SESSION_TOKEN\' not in ee\n\n    ee = get_rio_env(sanitize=False)\n    assert ee is not None\n    assert ee[\'AWS_ACCESS_KEY_ID\'] == \'fake-key-id\'\n    assert ee[\'AWS_SECRET_ACCESS_KEY\'] == \'fake-secret\'\n    assert ee[\'AWS_REGION\'] == \'fake-region\'\n    assert ee[\'GDAL_DISABLE_READDIR_ON_OPEN\'] == \'EMPTY_DIR\'\n\n    ee_local = ee\n    client = dask_client\n\n    creds = configure_s3_access(client=client)\n    cc = creds.get_frozen_credentials()\n    assert cc.access_key == \'fake-key-id\'\n    assert cc.secret_key == \'fake-secret\'\n    assert cc.token is None\n\n    ee = client.submit(activate_from_config).result()\n    assert ee is not None\n    assert \'AWS_ACCESS_KEY_ID\' in ee\n    assert \'AWS_SECRET_ACCESS_KEY\' in ee\n    assert \'AWS_REGION\' in ee\n    assert \'AWS_SESSION_TOKEN\' not in ee\n\n    ee = client.submit(get_rio_env, sanitize=False).result()\n    assert ee == ee_local\n'"
tests/test_warp.py,8,"b""import numpy as np\nfrom affine import Affine\nimport rasterio\nfrom datacube.utils.geometry import warp_affine, rio_reproject, gbox as gbx\nfrom datacube.utils.geometry._warp import resampling_s2rio, is_resampling_nn\n\nfrom datacube.testutils.geom import (\n    AlbersGS,\n)\n\n\ndef test_rio_resampling_conversion():\n    import pytest\n\n    R = rasterio.warp.Resampling\n    assert resampling_s2rio('nearest') == R.nearest\n    assert resampling_s2rio('bilinear') == R.bilinear\n    assert resampling_s2rio('Bilinear') == R.bilinear\n    assert resampling_s2rio('mode') == R.mode\n    assert resampling_s2rio('average') == R.average\n\n    with pytest.raises(ValueError):\n        resampling_s2rio('no_such_mode')\n\n    # check is_resampling_nn\n    assert is_resampling_nn('nearest') is True\n    assert is_resampling_nn('Nearest') is True\n    assert is_resampling_nn('average') is False\n    assert is_resampling_nn('no_such_mode') is False\n\n    assert is_resampling_nn(R.nearest) is True\n    assert is_resampling_nn(0) is True\n    assert is_resampling_nn(R.mode) is False\n\n\ndef test_warp():\n    src = np.zeros((128, 256),\n                   dtype='int16')\n\n    src[10:20, 30:50] = 33\n\n    dst = np.zeros_like(src)\n    dst_ = warp_affine(src, dst, Affine.translation(+30, +10), resampling='nearest')\n    assert dst_ is dst\n    assert (dst[:10, :20] == 33).all()\n    assert (dst[10:, :] == 0).all()\n    assert (dst[:, 20:] == 0).all()\n\n    # check GDAL int8 limitation work-around\n    src = src.astype('int8')\n    dst = np.zeros_like(src)\n    dst_ = warp_affine(src, dst, Affine.translation(+30, +10), resampling='nearest')\n    assert dst_ is dst\n    assert (dst[:10, :20] == 33).all()\n    assert (dst[10:, :] == 0).all()\n    assert (dst[:, 20:] == 0).all()\n\n    # check GDAL int8 limitation work-around, with no-data\n    src = src.astype('int8')\n    dst = np.zeros_like(src)\n    dst_ = warp_affine(src, dst,\n                       Affine.translation(+30, +10),\n                       resampling='nearest',\n                       src_nodata=0,\n                       dst_nodata=-3)\n    assert dst_ is dst\n    assert (dst[:10, :20] == 33).all()\n    assert (dst[10:, :] == -3).all()\n    assert (dst[:, 20:] == -3).all()\n\n\ndef test_rio_reproject():\n    src = np.zeros((128, 256),\n                   dtype='int16')\n\n    src[10:20, 30:50] = 33\n\n    s_gbox = AlbersGS.tile_geobox((15, -40))[:src.shape[0], :src.shape[1]]\n\n    dst = np.zeros_like(src)\n    dst_ = rio_reproject(src, dst,\n                         s_gbox,\n                         gbx.translate_pix(s_gbox, 30, 10),\n                         resampling='nearest')\n    assert dst_ is dst\n    assert (dst[:10, :20] == 33).all()\n    assert (dst[10:, :] == 0).all()\n    assert (dst[:, 20:] == 0).all()\n\n    # check GDAL int8 limitation work-around\n    src = src.astype('int8')\n    dst = np.zeros_like(src)\n\n    dst_ = rio_reproject(src, dst,\n                         s_gbox,\n                         gbx.translate_pix(s_gbox, 30, 10),\n                         resampling='nearest')\n\n    assert dst_ is dst\n    assert (dst[:10, :20] == 33).all()\n    assert (dst[10:, :] == 0).all()\n    assert (dst[:, 20:] == 0).all()\n\n    # check GDAL int8 limitation work-around, with no-data\n    src = src.astype('int8')\n    dst = np.zeros_like(src)\n    dst_ = rio_reproject(src, dst,\n                         s_gbox,\n                         gbx.translate_pix(s_gbox, 30, 10),\n                         src_nodata=0,\n                         dst_nodata=-3,\n                         resampling='nearest')\n    assert dst_ is dst\n    assert (dst[:10, :20] == 33).all()\n    assert (dst[10:, :] == -3).all()\n    assert (dst[:, 20:] == -3).all()\n"""
tests/test_xarray_extension.py,0,"b'import pytest\nimport xarray as xr\nfrom datacube.testutils.geom import epsg4326, epsg3857\nfrom datacube.testutils import mk_sample_xr_dataset, remove_crs\nfrom datacube.utils.xarray_geoextensions import (\n    _norm_crs,\n    _xarray_affine,\n    _xarray_geobox,\n    _xarray_extent,\n    _get_crs_from_attrs,\n    _get_crs_from_coord,\n)\n\n\ndef test_xr_extension(odc_style_xr_dataset):\n    xx = odc_style_xr_dataset\n    assert _norm_crs(None) is None\n    assert _norm_crs(epsg4326) is epsg4326\n    assert _norm_crs(str(epsg4326)) == epsg4326\n\n    with pytest.raises(ValueError):\n        _norm_crs([])\n\n    assert xx.geobox.shape == xx.B10.shape\n\n    (sx, zz0, tx, zz1, sy, ty) = xx.affine[:6]\n    assert (zz0, zz1) == (0, 0)\n\n    xx = remove_crs(xx)\n\n    assert _xarray_geobox(xx) is None\n    assert _xarray_extent(xx) is None\n\n    # affine should still be valid\n    A = _xarray_affine(xx)\n    assert A is not None\n    assert A*(0.5, 0.5) == (xx.longitude[0], xx.latitude[0])\n    assert A*(0.5, 1.5) == (xx.longitude[0], xx.latitude[1])\n\n\ndef test_xr_geobox():\n    xy = (10, 111)\n    rxy = (10, -100)\n    resolution = rxy[::-1]\n\n    ds = mk_sample_xr_dataset(crs=epsg3857, xy=xy, resolution=resolution)\n\n    assert ds.geobox.crs == epsg3857\n    assert ds.band.geobox.crs == epsg3857\n    assert ds.band.affine*(0, 0) == xy\n    assert ds.band.affine*(1, 1) == tuple(a+b for a, b in zip(xy, rxy))\n\n    assert ds.band[:, :2, :2].affine*(0, 0) == xy\n    assert ds.band[:, :2, :2].affine*(1, 1) == tuple(a+b for a, b in zip(xy, rxy))\n    assert ds.band.isel(time=0, x=0).affine is None\n\n    xx = ds.band + 1000\n    assert xx.geobox is not None\n    assert xx.geobox == ds.band.geobox\n\n    assert mk_sample_xr_dataset(crs=epsg4326).geobox.crs == epsg4326\n    assert mk_sample_xr_dataset(crs=epsg4326).band.geobox.crs == epsg4326\n\n    assert mk_sample_xr_dataset(crs=None).geobox is None\n    assert mk_sample_xr_dataset(crs=None).affine is not None\n\n\ndef test_xr_geobox_unhappy():\n    xx = mk_sample_xr_dataset(crs=None)\n\n    # test that exceptions from get_crs_from_{coord,attrs} are caught\n    xx.band.attrs.update(grid_mapping=\'x\')  # force exception in coord\n    xx.x.attrs.update(crs=\'EPSG:4326\')      # force exception in attr\n    xx.y.attrs.update(crs=\'EPSG:3857\')\n    assert xx.band.geobox is None\n\n    # test _norm_crs exception is caught\n    xx = mk_sample_xr_dataset(crs=None)\n    xx.attrs[\'crs\'] = [\'this will not parse\']\n    assert xx.geobox is None\n\n\ndef test_crs_from_coord():\n    xx_none = mk_sample_xr_dataset(crs=None)\n    xx_3857 = mk_sample_xr_dataset(crs=epsg3857)\n    xx_4326 = mk_sample_xr_dataset(crs=epsg4326)\n    cc_4326 = xx_4326.geobox.xr_coords(with_crs=\'epsg_4326\')[\'epsg_4326\']\n    cc_3857 = xx_3857.geobox.xr_coords(with_crs=\'epsg_3857\')[\'epsg_3857\']\n\n    assert _get_crs_from_coord(xx_none.band) is None\n    assert _get_crs_from_coord(xx_none) is None\n    assert _get_crs_from_coord(xx_3857.band) == epsg3857\n    assert _get_crs_from_coord(xx_3857) == epsg3857\n    assert _get_crs_from_coord(xx_4326.band) == epsg4326\n    assert _get_crs_from_coord(xx_4326) == epsg4326\n\n    xx = xx_none.band.assign_attrs(grid_mapping=\'x\')\n    with pytest.raises(ValueError):\n        _get_crs_from_coord(xx)\n    xx = xx_none.band.assign_attrs(grid_mapping=\'no_such_coord\')\n    assert _get_crs_from_coord(xx) is None\n\n    xx_2crs = xx_none.assign_coords(cc_4326=cc_4326, cc_3857=cc_3857)\n    assert xx_2crs.geobox is None\n\n    # two coords, no grid mapping, strict mode\n    with pytest.raises(ValueError):\n        _get_crs_from_coord(xx_2crs)\n    with pytest.raises(ValueError):\n        _get_crs_from_coord(xx_2crs.band)\n\n    # any should just return ""first"" guess, we not sure which one\n    crs = _get_crs_from_coord(xx_2crs, \'any\')\n    assert epsg4326 == crs or epsg3857 == crs\n\n    # all should return a list of length 2\n    crss = _get_crs_from_coord(xx_2crs, \'all\')\n    assert len(crss) == 2\n    assert any(crs == epsg3857 for crs in crss)\n    assert any(crs == epsg4326 for crs in crss)\n\n    with pytest.raises(ValueError):\n        _get_crs_from_coord(xx_2crs, \'no-such-mode\')\n\n\ndef test_crs_from_attrs():\n    xx_none = mk_sample_xr_dataset(crs=None)\n    xx_3857 = mk_sample_xr_dataset(crs=epsg3857)\n    xx_4326 = mk_sample_xr_dataset(crs=epsg4326)\n\n    assert _get_crs_from_attrs(xx_none) is None\n    assert _get_crs_from_attrs(xx_none.band) is None\n    assert _get_crs_from_attrs(xx_3857.band) == epsg3857\n    assert _get_crs_from_attrs(xx_3857) == epsg3857\n    assert _get_crs_from_attrs(xx_4326.band) == epsg4326\n    assert _get_crs_from_attrs(xx_4326) == epsg4326\n\n    assert _get_crs_from_attrs(xr.Dataset()) is None\n\n    # check inconsistent CRSs\n    xx = xx_3857.copy()\n    xx.x.attrs[\'crs\'] = xx_4326.attrs[\'crs\']\n    with pytest.raises(ValueError):\n        _get_crs_from_attrs(xx)\n'"
datacube/api/__init__.py,0,"b'# coding=utf-8\n""""""\nModules for the Storage and Access Query API\n""""""\n\nfrom .core import Datacube, TerminateCurrentLoad\nfrom .grid_workflow import GridWorkflow, Tile\n\n__all__ = (\n    \'Datacube\',\n    \'GridWorkflow\',\n    \'Tile\',\n    \'TerminateCurrentLoad\',\n)\n'"
datacube/api/core.py,1,"b'import uuid\nimport collections\nfrom itertools import groupby\nfrom typing import Union, Optional, Dict, Tuple\nimport datetime\n\nimport numpy\nimport xarray\nfrom dask import array as da\n\nfrom datacube.config import LocalConfig\nfrom datacube.storage import reproject_and_fuse, BandInfo\nfrom datacube.utils import ignore_exceptions_if\nfrom datacube.utils import geometry\nfrom datacube.utils.dates import normalise_dt\nfrom datacube.utils.geometry import intersects, GeoBox\nfrom datacube.utils.geometry.gbox import GeoboxTiles\nfrom datacube.model.utils import xr_apply\n\nfrom .query import Query, query_group_by, query_geopolygon\nfrom ..index import index_connect\nfrom ..drivers import new_datasource\n\n\nclass TerminateCurrentLoad(Exception):\n    """""" This exception is raised by user code from `progress_cbk`\n        to terminate currently running `.load`\n    """"""\n    pass\n\n\nclass Datacube(object):\n    """"""\n    Interface to search, read and write a datacube.\n\n    :type index: datacube.index.index.Index\n    """"""\n\n    def __init__(self,\n                 index=None,\n                 config=None,\n                 app=None,\n                 env=None,\n                 validate_connection=True):\n        """"""\n        Create the interface for the query and storage access.\n\n        If no index or config is given, the default configuration is used for database connection.\n\n        :param Index index: The database index to use.\n        :type index: :py:class:`datacube.index.Index` or None.\n\n        :param Union[LocalConfig|str] config: A config object or a path to a config file that defines the connection.\n\n            If an index is supplied, config is ignored.\n        :param str app: A short, alphanumeric name to identify this application.\n\n            The application name is used to track down problems with database queries, so it is strongly\n            advised that be used.  Required if an index is not supplied, otherwise ignored.\n\n        :param str env: Name of the datacube environment to use.\n            ie. the section name in any config files. Defaults to \'datacube\' for backwards\n            compatibility with old config files.\n\n            Allows you to have multiple datacube instances in one configuration, specified on load,\n            eg. \'dev\', \'test\' or \'landsat\', \'modis\' etc.\n\n        :param bool validate_connection: Should we check that the database connection is available and valid\n\n        :return: Datacube object\n\n        """"""\n\n        def normalise_config(config):\n            if config is None:\n                return LocalConfig.find(env=env)\n            if isinstance(config, str):\n                return LocalConfig.find([config], env=env)\n            return config\n\n        if index is None:\n            index = index_connect(normalise_config(config),\n                                  application_name=app,\n                                  validate_connection=validate_connection)\n\n        self.index = index\n\n    def list_products(self, show_archived=False, with_pandas=True):\n        """"""\n        List products in the datacube\n\n        :param show_archived: include products that have been archived.\n        :param with_pandas: return the list as a Pandas DataFrame, otherwise as a list of dict.\n        :rtype: pandas.DataFrame or list(dict)\n        """"""\n        rows = [product.to_dict() for product in self.index.products.get_all()]\n        if not with_pandas:\n            return rows\n\n        import pandas\n        keys = set(k for r in rows for k in r)\n        main_cols = [\'id\', \'name\', \'description\']\n        grid_cols = [\'crs\', \'resolution\', \'tile_size\', \'spatial_dimensions\']\n        other_cols = list(keys - set(main_cols) - set(grid_cols))\n        cols = main_cols + other_cols + grid_cols\n        return pandas.DataFrame(rows, columns=cols).set_index(\'id\')\n\n    def list_measurements(self, show_archived=False, with_pandas=True):\n        """"""\n        List measurements for each product\n\n        :param show_archived: include products that have been archived.\n        :param with_pandas: return the list as a Pandas DataFrame, otherwise as a list of dict.\n        :rtype: pandas.DataFrame or list(dict)\n        """"""\n        measurements = self._list_measurements()\n        if not with_pandas:\n            return measurements\n\n        import pandas\n        return pandas.DataFrame.from_dict(measurements).set_index([\'product\', \'measurement\'])\n\n    def _list_measurements(self):\n        measurements = []\n        dts = self.index.products.get_all()\n        for dt in dts:\n            if dt.measurements:\n                for name, measurement in dt.measurements.items():\n                    row = {\n                        \'product\': dt.name,\n                        \'measurement\': name,\n                    }\n                    if \'attrs\' in measurement:\n                        row.update(measurement[\'attrs\'])\n                    row.update({k: v for k, v in measurement.items() if k != \'attrs\'})\n                    measurements.append(row)\n        return measurements\n\n    #: pylint: disable=too-many-arguments, too-many-locals\n    def load(self, product=None, measurements=None, output_crs=None, resolution=None, resampling=None,\n             skip_broken_datasets=False,\n             dask_chunks=None, like=None, fuse_func=None, align=None, datasets=None, progress_cbk=None,\n             **query):\n        """"""\n        Load data as an ``xarray`` object.  Each measurement will be a data variable in the :class:`xarray.Dataset`.\n\n        See the `xarray documentation <http://xarray.pydata.org/en/stable/data-structures.html>`_ for usage of the\n        :class:`xarray.Dataset` and :class:`xarray.DataArray` objects.\n\n        **Product and Measurements**\n            A product can be specified using the product name, or by search fields that uniquely describe a single\n            product.\n            ::\n\n                product=\'ls5_ndvi_albers\'\n\n            See :meth:`list_products` for the list of products with their names and properties.\n\n            A product can also be selected by searching using fields, but must only match one product.\n            For example::\n\n                platform=\'LANDSAT_5\',\n                product_type=\'ndvi\'\n\n            The ``measurements`` argument is a list of measurement names, as listed in :meth:`list_measurements`.\n            If not provided, all measurements for the product will be returned.\n            ::\n\n                measurements=[\'red\', \'nir\', \'swir2\']\n\n        **Dimensions**\n            Spatial dimensions can specified using the ``longitude``/``latitude`` and ``x``/``y`` fields.\n\n            The CRS of this query is assumed to be WGS84/EPSG:4326 unless the ``crs`` field is supplied,\n            even if the stored data is in another projection or the `output_crs` is specified.\n            The dimensions ``longitude``/``latitude`` and ``x``/``y`` can be used interchangeably.\n            ::\n\n                latitude=(-34.5, -35.2), longitude=(148.3, 148.7)\n\n            or ::\n\n                x=(1516200, 1541300), y=(-3867375, -3867350), crs=\'EPSG:3577\'\n\n            The ``time`` dimension can be specified using a tuple of datetime objects or strings with\n            `YYYY-MM-DD hh:mm:ss` format. E.g::\n\n                time=(\'2001-04\', \'2001-07\')\n\n            For EO-specific datasets that are based around scenes, the time dimension can be reduced to the day level,\n            using solar day to keep scenes together.\n            ::\n\n                group_by=\'solar_day\'\n\n            For data that has different values for the scene overlap the requires more complex rules for combining data,\n            such as GA\'s Pixel Quality dataset, a function can be provided to the merging into a single time slice.\n\n            See :func:`datacube.helpers.ga_pq_fuser` for an example implementation.\n\n\n        **Output**\n            To reproject or resample the data, supply the ``output_crs``, ``resolution``, ``resampling`` and ``align``\n            fields.\n\n            To reproject data to 25m resolution for EPSG:3577::\n\n                dc.load(product=\'ls5_nbar_albers\', x=(148.15, 148.2), y=(-35.15, -35.2), time=(\'1990\', \'1991\'),\n                        output_crs=\'EPSG:3577`, resolution=(-25, 25), resampling=\'cubic\')\n\n        :param str product: the product to be included.\n\n        :param measurements:\n            Measurements name or list of names to be included, as listed in :meth:`list_measurements`.\n\n            If a list is specified, the measurements will be returned in the order requested.\n            By default all available measurements are included.\n\n        :type measurements: list(str), optional\n\n        :param query:\n            Search parameters for products and dimension ranges as described above.\n\n        :param str output_crs:\n            The CRS of the returned data.  If no CRS is supplied, the CRS of the stored data is used.\n\n        :param (float,float) resolution:\n            A tuple of the spatial resolution of the returned data.\n            This includes the direction (as indicated by a positive or negative number).\n\n            Typically when using most CRSs, the first number would be negative.\n\n        :param str|dict resampling:\n            The resampling method to use if re-projection is required. This could be a string or\n            a dictionary mapping band name to resampling mode. When using a dict use ``\'*\'`` to\n            indicate ""apply to all other bands"", for example ``{\'*\': \'cubic\', \'fmask\': \'nearest\'}`` would\n            use `cubic` for all bands except ``fmask`` for which `nearest` will be used.\n\n            Valid values are: ``\'nearest\', \'cubic\', \'bilinear\', \'cubic_spline\', \'lanczos\', \'average\',\n            \'mode\', \'gauss\',  \'max\', \'min\', \'med\', \'q1\', \'q3\'``\n\n            Default is to use ``nearest`` for all bands.\n            .. seealso:: :meth:`load_data`\n\n        :param (float,float) align:\n            Load data such that point \'align\' lies on the pixel boundary.\n            Units are in the co-ordinate space of the output CRS.\n\n            Default is (0,0)\n\n        :param dict dask_chunks:\n            If the data should be lazily loaded using :class:`dask.array.Array`,\n            specify the chunking size in each output dimension.\n\n            See the documentation on using `xarray with dask <http://xarray.pydata.org/en/stable/dask.html>`_\n            for more information.\n\n        :param xarray.Dataset like:\n            Uses the output of a previous ``load()`` to form the basis of a request for another product.\n            E.g.::\n\n                pq = dc.load(product=\'ls5_pq_albers\', like=nbar_dataset)\n\n        :param str group_by:\n            When specified, perform basic combining/reducing of the data.\n\n        :param fuse_func:\n            Function used to fuse/combine/reduce data with the ``group_by`` parameter. By default,\n            data is simply copied over the top of each other, in a relatively undefined manner. This function can\n            perform a specific combining step, eg. for combining GA PQ data. This can be a dictionary if different\n            fusers are needed per band.\n\n        :param datasets:\n            Optional. If this is a non-empty list of :class:`datacube.model.Dataset` objects, these will be loaded\n            instead of performing a database lookup.\n\n        :param int limit:\n            Optional. If provided, limit the maximum number of datasets\n            returned. Useful for testing and debugging.\n\n        :param progress_cbk: Int, Int -> None\n            if supplied will be called for every file read with `files_processed_so_far, total_files`. This is\n            only applicable to non-lazy loads, ignored when using dask.\n\n        :return: Requested data in a :class:`xarray.Dataset`\n        :rtype: :class:`xarray.Dataset`\n        """"""\n        if product is None and datasets is None:\n            raise ValueError(""Must specify a product or supply datasets"")\n\n        if datasets is None:\n            datasets = self.find_datasets(product=product, like=like, ensure_location=True, **query)\n        elif isinstance(datasets, collections.Iterator):\n            datasets = list(datasets)\n\n        if len(datasets) == 0:\n            return xarray.Dataset()\n\n        ds, *_ = datasets\n        datacube_product = ds.type\n\n        geobox = output_geobox(like=like, output_crs=output_crs, resolution=resolution, align=align,\n                               grid_spec=datacube_product.grid_spec,\n                               datasets=datasets, **query)\n\n        group_by = query_group_by(**query)\n        grouped = self.group_datasets(datasets, group_by)\n\n        measurement_dicts = datacube_product.lookup_measurements(measurements)\n\n        result = self.load_data(grouped, geobox,\n                                measurement_dicts,\n                                resampling=resampling,\n                                fuse_func=fuse_func,\n                                dask_chunks=dask_chunks,\n                                skip_broken_datasets=skip_broken_datasets,\n                                progress_cbk=progress_cbk)\n\n        return result\n\n    def find_datasets(self, **search_terms):\n        """"""\n        Search the index and return all datasets for a product matching the search terms.\n\n        :param search_terms: see :class:`datacube.api.query.Query`\n        :return: list of datasets\n        :rtype: list[:class:`datacube.model.Dataset`]\n\n        .. seealso:: :meth:`group_datasets` :meth:`load_data` :meth:`find_datasets_lazy`\n        """"""\n        return list(self.find_datasets_lazy(**search_terms))\n\n    def find_datasets_lazy(self, limit=None, ensure_location=False, **kwargs):\n        """"""\n        Find datasets matching query.\n\n        :param kwargs: see :class:`datacube.api.query.Query`\n        :param ensure_location: only return datasets that have locations\n        :param limit: if provided, limit the maximum number of datasets returned\n        :return: iterator of datasets\n        :rtype: __generator[:class:`datacube.model.Dataset`]\n\n        .. seealso:: :meth:`group_datasets` :meth:`load_data` :meth:`find_datasets`\n        """"""\n        query = Query(self.index, **kwargs)\n        if not query.product:\n            raise ValueError(""must specify a product"")\n\n        datasets = self.index.datasets.search(limit=limit,\n                                              **query.search_terms)\n\n        if query.geopolygon is not None:\n            datasets = select_datasets_inside_polygon(datasets, query.geopolygon)\n\n        if ensure_location:\n            datasets = (dataset for dataset in datasets if dataset.uris)\n\n        return datasets\n\n    @staticmethod\n    def group_datasets(datasets, group_by):\n        """"""\n        Group datasets along defined non-spatial dimensions (ie. time).\n\n        :param datasets: a list of datasets, typically from :meth:`find_datasets`\n        :param GroupBy group_by: Contains:\n            - a function that returns a label for a dataset\n            - name of the new dimension\n            - unit for the new dimension\n            - function to sort by before grouping\n        :rtype: xarray.DataArray\n\n        .. seealso:: :meth:`find_datasets`, :meth:`load_data`, :meth:`query_group_by`\n        """"""\n        if isinstance(group_by, str):\n            group_by = query_group_by(group_by=group_by)\n\n        dimension, group_func, units, sort_key = group_by\n\n        def ds_sorter(ds):\n            return sort_key(ds), getattr(ds, \'id\', 0)\n\n        def norm_axis_value(x):\n            if isinstance(x, datetime.datetime):\n                # For datetime we convert to UTC, then strip timezone info\n                # to avoid numpy/pandas warning about timezones\n                return numpy.datetime64(normalise_dt(x), \'ns\')\n            return x\n\n        def mk_group(group):\n            dss = tuple(sorted(group, key=ds_sorter))\n            # TODO: decouple axis_value from group sorted order\n            axis_value = sort_key(dss[0])\n            return (norm_axis_value(axis_value), dss)\n\n        datasets = sorted(datasets, key=group_func)\n\n        groups = [mk_group(group)\n                  for _, group in groupby(datasets, group_func)]\n\n        groups.sort(key=lambda x: x[0])\n\n        coords = [coord for coord, _ in groups]\n        data = numpy.empty(len(coords), dtype=object)\n        for i, (_, dss) in enumerate(groups):\n            data[i] = dss\n\n        sources = xarray.DataArray(data, dims=[dimension], coords=[coords])\n        sources[dimension].attrs[\'units\'] = units\n        return sources\n\n    @staticmethod\n    def create_storage(coords, geobox, measurements, data_func=None):\n        """"""\n        Create a :class:`xarray.Dataset` and (optionally) fill it with data.\n\n        This function makes the in memory storage structure to hold datacube data.\n\n        :param dict coords:\n            OrderedDict holding `DataArray` objects defining the dimensions not specified by `geobox`\n\n        :param GeoBox geobox:\n            A GeoBox defining the output spatial projection and resolution\n\n        :param measurements:\n            list of :class:`datacube.model.Measurement`\n\n        :param data_func: Callable `Measurement -> np.ndarray`\n            function to fill the storage with data. It is called once for each measurement, with the measurement\n            as an argument. It should return an appropriately shaped numpy array. If not provided memory is\n            allocated an filled with `nodata` value defined on a given Measurement.\n\n        :rtype: :class:`xarray.Dataset`\n\n        .. seealso:: :meth:`find_datasets` :meth:`group_datasets`\n        """"""\n        from collections import OrderedDict\n        spatial_ref = \'spatial_ref\'\n\n        def empty_func(m, shape):\n            return numpy.full(shape, m.nodata, dtype=m.dtype)\n\n        crs_attrs = {}\n        if geobox.crs is not None:\n            crs_attrs[\'crs\'] = str(geobox.crs)\n            crs_attrs[\'grid_mapping\'] = spatial_ref\n\n        dims = tuple(coords) + geobox.dimensions\n        shape = tuple(c.size for c in coords.values()) + geobox.shape\n        coords = OrderedDict(**coords, **geobox.xr_coords(with_crs=spatial_ref))\n\n        data_func = data_func or (lambda m: empty_func(m, shape))\n\n        def mk_data_var(m, data_func):\n            data = data_func(m)\n            attrs = dict(**m.dataarray_attrs(),\n                         **crs_attrs)\n            return xarray.DataArray(data,\n                                    name=m.name,\n                                    coords=coords,\n                                    dims=dims,\n                                    attrs=attrs)\n\n        return xarray.Dataset({m.name: mk_data_var(m, data_func)\n                               for m in measurements},\n                              coords=coords,\n                              attrs=crs_attrs)\n\n    @staticmethod\n    def _dask_load(sources, geobox, measurements, dask_chunks,\n                   skip_broken_datasets=False):\n        needed_irr_chunks, grid_chunks = _calculate_chunk_sizes(sources, geobox, dask_chunks)\n        gbt = GeoboxTiles(geobox, grid_chunks)\n        dsk = {}\n\n        def chunk_datasets(dss, gbt):\n            out = {}\n            for ds in dss:\n                dsk[_tokenize_dataset(ds)] = ds\n                for idx in gbt.tiles(ds.extent):\n                    out.setdefault(idx, []).append(ds)\n            return out\n\n        chunked_srcs = xr_apply(sources,\n                                lambda _, dss: chunk_datasets(dss, gbt),\n                                dtype=object)\n\n        def data_func(measurement):\n            return _make_dask_array(chunked_srcs, dsk, gbt,\n                                    measurement,\n                                    chunks=needed_irr_chunks+grid_chunks,\n                                    skip_broken_datasets=skip_broken_datasets)\n\n        return Datacube.create_storage(sources.coords, geobox, measurements, data_func)\n\n    @staticmethod\n    def _xr_load(sources, geobox, measurements,\n                 skip_broken_datasets=False,\n                 progress_cbk=None):\n\n        def mk_cbk(cbk):\n            if cbk is None:\n                return None\n            n = 0\n            n_total = sum(len(x) for x in sources.values.ravel())*len(measurements)\n\n            def _cbk(*ignored):\n                nonlocal n\n                n += 1\n                return cbk(n, n_total)\n            return _cbk\n\n        data = Datacube.create_storage(sources.coords, geobox, measurements)\n        _cbk = mk_cbk(progress_cbk)\n\n        for index, datasets in numpy.ndenumerate(sources.values):\n            for m in measurements:\n                t_slice = data[m.name].values[index]\n\n                try:\n                    _fuse_measurement(t_slice, datasets, geobox, m,\n                                      skip_broken_datasets=skip_broken_datasets,\n                                      progress_cbk=_cbk)\n                except (TerminateCurrentLoad, KeyboardInterrupt):\n                    data.attrs[\'dc_partial_load\'] = True\n                    return data\n\n        return data\n\n    @staticmethod\n    def load_data(sources, geobox, measurements, resampling=None,\n                  fuse_func=None, dask_chunks=None, skip_broken_datasets=False,\n                  progress_cbk=None,\n                  **extra):\n        """"""\n        Load data from :meth:`group_datasets` into an :class:`xarray.Dataset`.\n\n        :param xarray.DataArray sources:\n            DataArray holding a list of :class:`datacube.model.Dataset`, grouped along the time dimension\n\n        :param GeoBox geobox:\n            A GeoBox defining the output spatial projection and resolution\n\n        :param measurements:\n            list of `Measurement` objects\n\n        :param str|dict resampling:\n            The resampling method to use if re-projection is required. This could be a string or\n            a dictionary mapping band name to resampling mode. When using a dict use ``\'*\'`` to\n            indicate ""apply to all other bands"", for example ``{\'*\': \'cubic\', \'fmask\': \'nearest\'}`` would\n            use `cubic` for all bands except ``fmask`` for which `nearest` will be used.\n\n            Valid values are: ``\'nearest\', \'cubic\', \'bilinear\', \'cubic_spline\', \'lanczos\', \'average\',\n            \'mode\', \'gauss\',  \'max\', \'min\', \'med\', \'q1\', \'q3\'``\n\n            Default is to use ``nearest`` for all bands.\n\n        :param fuse_func:\n            function to merge successive arrays as an output. Can be a dictionary just like resampling.\n\n        :param dict dask_chunks:\n            If provided, the data will be loaded on demand using using :class:`dask.array.Array`.\n            Should be a dictionary specifying the chunking size for each output dimension.\n            Unspecified dimensions will be auto-guessed, currently this means use chunk size of 1 for non-spatial\n            dimensions and use whole dimension (no chunking unless specified) for spatial dimensions.\n\n            See the documentation on using `xarray with dask <http://xarray.pydata.org/en/stable/dask.html>`_\n            for more information.\n\n        :param progress_cbk: Int, Int -> None\n            if supplied will be called for every file read with `files_processed_so_far, total_files`. This is\n            only applicable to non-lazy loads, ignored when using dask.\n\n        :rtype: xarray.Dataset\n\n        .. seealso:: :meth:`find_datasets` :meth:`group_datasets`\n        """"""\n        measurements = per_band_load_data_settings(measurements, resampling=resampling, fuse_func=fuse_func)\n\n        if dask_chunks is not None:\n            return Datacube._dask_load(sources, geobox, measurements, dask_chunks,\n                                       skip_broken_datasets=skip_broken_datasets)\n        else:\n            return Datacube._xr_load(sources, geobox, measurements,\n                                     skip_broken_datasets=skip_broken_datasets,\n                                     progress_cbk=progress_cbk)\n\n    def __str__(self):\n        return ""Datacube<index={!r}>"".format(self.index)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def close(self):\n        """"""\n        Close any open connections\n        """"""\n        self.index.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type_, value, traceback):\n        self.close()\n\n\ndef per_band_load_data_settings(measurements, resampling=None, fuse_func=None):\n    def with_resampling(m, resampling, default=None):\n        m = m.copy()\n        m[\'resampling_method\'] = resampling.get(m.name, default)\n        return m\n\n    def with_fuser(m, fuser, default=None):\n        m = m.copy()\n        m[\'fuser\'] = fuser.get(m.name, default)\n        return m\n\n    if isinstance(resampling, str):\n        resampling = {\'*\': resampling}\n\n    if not isinstance(fuse_func, dict):\n        fuse_func = {\'*\': fuse_func}\n\n    if isinstance(measurements, dict):\n        measurements = list(measurements.values())\n\n    if resampling is not None:\n        measurements = [with_resampling(m, resampling, default=resampling.get(\'*\'))\n                        for m in measurements]\n\n    if fuse_func is not None:\n        measurements = [with_fuser(m, fuse_func, default=fuse_func.get(\'*\'))\n                        for m in measurements]\n\n    return measurements\n\n\ndef output_geobox(like=None, output_crs=None, resolution=None, align=None,\n                  grid_spec=None, datasets=None, geopolygon=None, **query):\n    """""" Configure output geobox from user provided output specs. """"""\n\n    if like is not None:\n        assert output_crs is None, ""\'like\' and \'output_crs\' are not supported together""\n        assert resolution is None, ""\'like\' and \'resolution\' are not supported together""\n        assert align is None, ""\'like\' and \'align\' are not supported together""\n        if isinstance(like, GeoBox):\n            return like\n\n        return like.geobox\n\n    if output_crs is not None:\n        # user provided specifications\n        if resolution is None:\n            raise ValueError(""Must specify \'resolution\' when specifying \'output_crs\'"")\n        crs = geometry.CRS(output_crs)\n    else:\n        # specification from grid_spec\n        if grid_spec is None or grid_spec.crs is None:\n            raise ValueError(""Product has no default CRS. Must specify \'output_crs\' and \'resolution\'"")\n        crs = grid_spec.crs\n        if resolution is None:\n            if grid_spec.resolution is None:\n                raise ValueError(""Product has no default resolution. Must specify \'resolution\'"")\n            resolution = grid_spec.resolution\n        align = align or grid_spec.alignment\n\n    if geopolygon is None:\n        geopolygon = query_geopolygon(**query)\n\n        if geopolygon is None:\n            geopolygon = get_bounds(datasets, crs)\n\n    return geometry.GeoBox.from_geopolygon(geopolygon, resolution, crs, align)\n\n\ndef select_datasets_inside_polygon(datasets, polygon):\n    # Check against the bounding box of the original scene, can throw away some portions\n    assert polygon is not None\n    query_crs = polygon.crs\n    for dataset in datasets:\n        if intersects(polygon, dataset.extent.to_crs(query_crs)):\n            yield dataset\n\n\ndef fuse_lazy(datasets, geobox, measurement, skip_broken_datasets=False, prepend_dims=0):\n    prepend_shape = (1,) * prepend_dims\n    data = numpy.full(geobox.shape, measurement.nodata, dtype=measurement.dtype)\n    _fuse_measurement(data, datasets, geobox, measurement,\n                      skip_broken_datasets=skip_broken_datasets)\n    return data.reshape(prepend_shape + geobox.shape)\n\n\ndef _fuse_measurement(dest, datasets, geobox, measurement,\n                      skip_broken_datasets=False,\n                      progress_cbk=None):\n    srcs = []\n    for ds in datasets:\n        src = None\n        with ignore_exceptions_if(skip_broken_datasets):\n            src = new_datasource(BandInfo(ds, measurement.name))\n\n        if src is None:\n            if not skip_broken_datasets:\n                raise ValueError(f""Failed to load dataset: {ds.id}"")\n        else:\n            srcs.append(src)\n\n    reproject_and_fuse(srcs,\n                       dest,\n                       geobox,\n                       dest.dtype.type(measurement.nodata),\n                       resampling=measurement.get(\'resampling_method\', \'nearest\'),\n                       fuse_func=measurement.get(\'fuser\', None),\n                       skip_broken_datasets=skip_broken_datasets,\n                       progress_cbk=progress_cbk)\n\n\ndef get_bounds(datasets, crs):\n    bbox = geometry.bbox_union(ds.extent.to_crs(crs).boundingbox for ds in datasets)\n    return geometry.box(*bbox, crs=crs)\n\n\ndef _calculate_chunk_sizes(sources: xarray.DataArray,\n                           geobox: GeoBox,\n                           dask_chunks: Dict[str, Union[str, int]]):\n    valid_keys = sources.dims + geobox.dimensions\n    bad_keys = set(dask_chunks) - set(valid_keys)\n    if bad_keys:\n        raise KeyError(\'Unknown dask_chunk dimension {}. Valid dimensions are: {}\'.format(bad_keys, valid_keys))\n\n    chunk_maxsz = dict((dim, sz) for dim, sz in zip(sources.dims + geobox.dimensions,\n                                                    sources.shape + geobox.shape))\n\n    # defaults: 1 for non-spatial, whole dimension for Y/X\n    chunk_defaults = dict([(dim, 1) for dim in sources.dims] + [(dim, -1) for dim in geobox.dimensions])\n\n    def _resolve(k, v: Optional[Union[str, int]]) -> int:\n        if v is None or v == ""auto"":\n            v = _resolve(k, chunk_defaults[k])\n\n        if isinstance(v, int):\n            if v < 0:\n                return chunk_maxsz[k]\n            return v\n        raise ValueError(""Chunk should be one of int|\'auto\'"")\n\n    irr_chunks = tuple(_resolve(dim, dask_chunks.get(str(dim))) for dim in sources.dims)\n    grid_chunks = tuple(_resolve(dim, dask_chunks.get(str(dim))) for dim in geobox.dimensions)\n\n    return irr_chunks, grid_chunks\n\n\ndef _tokenize_dataset(dataset):\n    return \'dataset-{}\'.format(dataset.id.hex)\n\n\n# pylint: disable=too-many-locals\ndef _make_dask_array(chunked_srcs,\n                     dsk,\n                     gbt,\n                     measurement,\n                     chunks,\n                     skip_broken_datasets=False):\n    dsk = dsk.copy()  # this contains mapping from dataset id to dataset object\n\n    token = uuid.uuid4().hex\n    dsk_name = \'dc_load_{name}-{token}\'.format(name=measurement.name, token=token)\n\n    needed_irr_chunks, grid_chunks = chunks[:-2], chunks[-2:]\n    actual_irr_chunks = (1,) * len(needed_irr_chunks)\n\n    # we can have up to 4 empty chunk shapes: whole, right edge, bottom edge and\n    # bottom right corner\n    #  W R\n    #  B BR\n    empties = {}  # type Dict[Tuple[int,int], str]\n\n    def _mk_empty(shape: Tuple[int, int]) -> str:\n        name = empties.get(shape, None)\n        if name is not None:\n            return name\n\n        name = \'empty_{}x{}-{token}\'.format(*shape, token=token)\n        dsk[name] = (numpy.full, actual_irr_chunks + shape, measurement.nodata, measurement.dtype)\n        empties[shape] = name\n\n        return name\n\n    for irr_index, tiled_dss in numpy.ndenumerate(chunked_srcs.values):\n        key_prefix = (dsk_name, *irr_index)\n\n        # all spatial chunks\n        for idx in numpy.ndindex(gbt.shape):\n            dss = tiled_dss.get(idx, None)\n\n            if dss is None:\n                val = _mk_empty(gbt.chunk_shape(idx))\n            else:\n                val = (fuse_lazy,\n                       [_tokenize_dataset(ds) for ds in dss],\n                       gbt[idx],\n                       measurement,\n                       skip_broken_datasets,\n                       chunked_srcs.ndim)\n\n            dsk[key_prefix + idx] = val\n\n    y_shapes = [grid_chunks[0]]*gbt.shape[0]\n    x_shapes = [grid_chunks[1]]*gbt.shape[1]\n\n    y_shapes[-1], x_shapes[-1] = gbt.chunk_shape(tuple(n-1 for n in gbt.shape))\n\n    data = da.Array(dsk, dsk_name,\n                    chunks=actual_irr_chunks + (tuple(y_shapes), tuple(x_shapes)),\n                    dtype=measurement.dtype,\n                    shape=(chunked_srcs.shape + gbt.base.shape))\n\n    if needed_irr_chunks != actual_irr_chunks:\n        data = data.rechunk(chunks=chunks)\n    return data\n'"
datacube/api/grid_workflow.py,0,"b'\nimport logging\nimport numpy\nimport xarray\nfrom itertools import groupby\nfrom collections import OrderedDict\nimport pandas as pd\n\nfrom datacube.utils.geometry import intersects\nfrom .query import Query, query_group_by\nfrom .core import Datacube\n\n_LOG = logging.getLogger(__name__)\n\n\ndef _fast_slice(array, indexers):\n    data = array.values[indexers]\n    dims = [dim for dim, indexer in zip(array.dims, indexers) if isinstance(indexer, slice)]\n    variable = xarray.Variable(dims, data, attrs=array.attrs, fastpath=True)\n    coords = OrderedDict((dim,\n                          xarray.Variable((dim,),\n                                          array.coords[dim].values[indexer],\n                                          attrs=array.coords[dim].attrs,\n                                          fastpath=True))\n                         for dim, indexer in zip(array.dims, indexers) if isinstance(indexer, slice))\n    return xarray.DataArray(variable, coords=coords, fastpath=True)\n\n\nclass Tile(object):\n    """"""\n    The Tile object holds a lightweight representation of a datacube result.\n\n    It is produced by :meth:`.GridWorkflow.list_cells` or :meth:`.GridWorkflow.list_tiles`.\n\n    The Tile object can be passed to :meth:`GridWorkflow.load` to be loaded into memory as\n    an :class:`xarray.Dataset`.\n\n    A portion of a tile can be created by using index notation. eg:\n\n        tile[0:1, 0:1000, 0:1000]\n\n    This can be used to load small portions of data into memory, instead of having to access\n    the entire `Tile` at once.\n    """"""\n\n    def __init__(self, sources, geobox):\n        """"""Create a Tile representing a dataset that can be loaded.\n\n        :param xarray.DataArray sources: An array of non-spatial dimensions of the request, holding lists of\n            datacube.storage.DatasetSource objects.\n        :param model.GeoBox geobox: The spatial footprint of the Tile\n        """"""\n        self.sources = sources\n        self.geobox = geobox\n\n    @property\n    def dims(self):\n        """"""Names of the dimensions, eg ``(\'time\', \'y\', \'x\')``\n        :return: tuple(str)\n        """"""\n        return self.sources.dims + self.geobox.dimensions\n\n    @property\n    def shape(self):\n        """"""Lengths of each dimension, eg ``(285, 4000, 4000)``\n        :return: tuple(int)\n        """"""\n        return self.sources.shape + self.geobox.shape\n\n    @property\n    def product(self):\n        """"""\n        :rtype: datacube.model.DatasetType\n        """"""\n        return self.sources.values[0][0].type\n\n    def __getitem__(self, chunk):\n        sources = _fast_slice(self.sources, chunk[:len(self.sources.shape)])\n        geobox = self.geobox[chunk[len(self.sources.shape):]]\n        return Tile(sources, geobox)\n\n    # TODO(csiro) Split on time range\n    def split(self, dim, step=1):\n        """"""\n        Splits along a non-spatial dimension into Tile objects with a length of 1 or more in the `dim` dimension.\n\n        :param dim: Name of the non-spatial dimension to split\n        :param step: step size to split\n        :return: tuple(key, Tile)\n        """"""\n        axis = self.dims.index(dim)\n        indexer = [slice(None)] * len(self.dims)\n        size = self.sources[dim].size\n        for i in range(0, size, step):\n            indexer[axis] = slice(i, min(size, i + step))\n            yield self.sources[dim].values[i], self[tuple(indexer)]\n\n    def split_by_time(self, freq=\'A\', time_dim=\'time\', **kwargs):\n        """"""\n        Splits along the `time` dimension, into periods, using pandas offsets, such as:\n        :\n            \'A\': Annual\n            \'Q\': Quarter\n            \'M\': Month\n        See: http://pandas.pydata.org/pandas-docs/stable/timeseries.html?highlight=rollback#timeseries-offset-aliases\n\n        :param freq: time series frequency\n        :param time_dim: name of the time dimension\n        :param kwargs: other keyword arguments passed to ``pandas.period_range``\n        :return: Generator[tuple(str, Tile)] generator of the key string (eg \'1994\') and the slice of Tile\n        """"""\n        # extract first and last timestamps from the time axis, note this will\n        # work with 1 element arrays as well\n        start_range, end_range = self.sources[time_dim].data[[0, -1]]\n\n        for p in pd.period_range(start=start_range,\n                                 end=end_range,\n                                 freq=freq,\n                                 **kwargs):\n            sources_slice = self.sources.loc[{time_dim: slice(p.start_time, p.end_time)}]\n            yield str(p), Tile(sources=sources_slice, geobox=self.geobox)\n\n    def __str__(self):\n        return ""Tile<sources={!r},\\n\\tgeobox={!r}>"".format(self.sources, self.geobox)\n\n    def __repr__(self):\n        return self.__str__()\n\n\nclass GridWorkflow(object):\n    """"""\n    GridWorkflow deals with cell- and tile-based processing using a grid defining a projection and resolution.\n\n    Use GridWorkflow to specify your desired output grid.  The methods :meth:`list_cells` and :meth:`list_tiles`\n    query the index and return a dictionary of cell or tile keys, each mapping to a :class:`Tile` object.\n\n    The :class:`.Tile` object can then be used to load the data without needing the index,\n    and can be serialized for use with the `distributed` package.\n    """"""\n\n    def __init__(self, index, grid_spec=None, product=None):\n        """"""\n        Create a grid workflow tool.\n\n        Either grid_spec or product must be supplied.\n\n        :param datacube.index.Index index: The database index to use.\n        :param GridSpec grid_spec: The grid projection and resolution\n        :param str product: The name of an existing product, if no grid_spec is supplied.\n        """"""\n        self.index = index\n        if grid_spec is None:\n            product = self.index.products.get_by_name(product)\n            grid_spec = product and product.grid_spec\n        self.grid_spec = grid_spec\n\n    def cell_observations(self, cell_index=None, geopolygon=None, tile_buffer=None, **indexers):\n        """"""\n        List datasets, grouped by cell.\n\n        :param datacube.utils.Geometry geopolygon:\n            Only return observations with data inside polygon.\n        :param (float,float) tile_buffer:\n            buffer tiles by (y, x) in CRS units\n        :param (int,int) cell_index:\n            The cell index. E.g. (14, -40)\n        :param indexers:\n            Query to match the datasets, see :py:class:`datacube.api.query.Query`\n        :return: Datsets grouped by cell index\n        :rtype: dict[(int,int), list[:py:class:`datacube.model.Dataset`]]\n\n        .. seealso::\n            :meth:`datacube.Datacube.find_datasets`\n\n            :class:`datacube.api.query.Query`\n        """"""\n        # pylint: disable=too-many-locals\n        # TODO: split this method into 3: cell/polygon/unconstrained querying\n\n        if tile_buffer is not None and geopolygon is not None:\n            raise ValueError(\'Cannot process tile_buffering and geopolygon together.\')\n        cells = {}\n\n        def add_dataset_to_cells(tile_index, tile_geobox, dataset_):\n            cells.setdefault(tile_index, {\'datasets\': [], \'geobox\': tile_geobox})[\'datasets\'].append(dataset_)\n\n        if cell_index:\n            assert len(cell_index) == 2\n            cell_index = tuple(cell_index)\n            geobox = self.grid_spec.tile_geobox(cell_index)\n            geobox = geobox.buffered(*tile_buffer) if tile_buffer else geobox\n\n            datasets, query = self._find_datasets(geobox.extent, indexers)\n            for dataset in datasets:\n                if intersects(geobox.extent, dataset.extent.to_crs(self.grid_spec.crs)):\n                    add_dataset_to_cells(cell_index, geobox, dataset)\n            return cells\n        else:\n            datasets, query = self._find_datasets(geopolygon, indexers)\n            geobox_cache = {}\n\n            if query.geopolygon:\n                # Get a rough region of tiles\n                query_tiles = set(\n                    tile_index for tile_index, tile_geobox in\n                    self.grid_spec.tiles_from_geopolygon(query.geopolygon, geobox_cache=geobox_cache))\n\n                for dataset in datasets:\n                    # Go through our datasets and see which tiles each dataset produces, and whether they intersect\n                    # our query geopolygon.\n                    dataset_extent = dataset.extent.to_crs(self.grid_spec.crs)\n                    bbox = dataset_extent.boundingbox\n                    bbox = bbox.buffered(*tile_buffer) if tile_buffer else bbox\n\n                    for tile_index, tile_geobox in self.grid_spec.tiles(bbox, geobox_cache=geobox_cache):\n                        if tile_index in query_tiles and intersects(tile_geobox.extent, dataset_extent):\n                            add_dataset_to_cells(tile_index, tile_geobox, dataset)\n\n            else:\n                for dataset in datasets:\n                    for tile_index, tile_geobox in self.grid_spec.tiles_from_geopolygon(dataset.extent,\n                                                                                        tile_buffer=tile_buffer,\n                                                                                        geobox_cache=geobox_cache):\n                        add_dataset_to_cells(tile_index, tile_geobox, dataset)\n\n            return cells\n\n    def _find_datasets(self, geopolygon, indexers):\n        query = Query(index=self.index, geopolygon=geopolygon, **indexers)\n        if not query.product:\n            raise RuntimeError(\'must specify a product\')\n        datasets = self.index.datasets.search_eager(**query.search_terms)\n        return datasets, query\n\n    @staticmethod\n    def group_into_cells(observations, group_by):\n        """"""\n        Group observations into a stack of source tiles.\n\n        :param observations: datasets grouped by cell index, like from :py:meth:`cell_observations`\n        :param group_by: grouping method, as returned by :py:meth:`datacube.api.query.query_group_by`\n        :type group_by: :py:class:`datacube.api.query.GroupBy`\n        :return: tiles grouped by cell index\n        :rtype: dict[(int,int), :class:`.Tile`]\n\n        .. seealso::\n            :meth:`load`\n\n            :meth:`datacube.Datacube.group_datasets`\n        """"""\n        cells = {}\n        for cell_index, observation in observations.items():\n            sources = Datacube.group_datasets(observation[\'datasets\'], group_by)\n            cells[cell_index] = Tile(sources, observation[\'geobox\'])\n        return cells\n\n    @staticmethod\n    def tile_sources(observations, group_by):\n        """"""\n        Split observations into tiles and group into source tiles\n\n        :param observations: datasets grouped by cell index, like from :meth:`cell_observations`\n        :param group_by: grouping method, as returned by :py:meth:`datacube.api.query.query_group_by`\n        :type group_by: :py:class:`datacube.api.query.GroupBy`\n        :return: tiles grouped by cell index and time\n        :rtype: dict[tuple(int, int, numpy.datetime64), :py:class:`.Tile`]\n\n        .. seealso::\n            :meth:`load`\n\n            :meth:`datacube.Datacube.group_datasets`\n        """"""\n        tiles = {}\n        for cell_index, observation in observations.items():\n            observation[\'datasets\'].sort(key=group_by.group_by_func)\n            groups = [(key, tuple(group)) for key, group in groupby(observation[\'datasets\'], group_by.group_by_func)]\n\n            for key, datasets in groups:\n                data = numpy.empty(1, dtype=object)\n                data[0] = datasets\n                variable = xarray.Variable((group_by.dimension,), data,\n                                           fastpath=True)\n                coord = xarray.Variable((group_by.dimension,),\n                                        numpy.array([key], dtype=\'datetime64[ns]\'),\n                                        attrs={\'units\': group_by.units},\n                                        fastpath=True)\n                coords = OrderedDict([(group_by.dimension, coord)])\n                sources = xarray.DataArray(variable, coords=coords, fastpath=True)\n\n                tile_index = cell_index + (coord.values[0],)\n                tiles[tile_index] = Tile(sources, observation[\'geobox\'])\n        return tiles\n\n    def list_cells(self, cell_index=None, **query):\n        """"""\n        List cells that match the query.\n\n        Returns a dictionary of cell indexes to :class:`.Tile` objects.\n\n        Cells are included if they contain any datasets that match the query using the same format as\n        :meth:`datacube.Datacube.load`.\n\n        E.g.::\n\n            gw.list_cells(product=\'ls5_nbar_albers\',\n                          time=(\'2001-1-1 00:00:00\', \'2001-3-31 23:59:59\'))\n\n        :param (int,int) cell_index: The cell index. E.g. (14, -40)\n        :param query: see :py:class:`datacube.api.query.Query`\n        :rtype: dict[(int, int), :class:`.Tile`]\n        """"""\n        observations = self.cell_observations(cell_index, **query)\n        return self.group_into_cells(observations, query_group_by(**query))\n\n    def list_tiles(self, cell_index=None, **query):\n        """"""\n        List tiles of data, sorted by cell.\n        ::\n\n            tiles = gw.list_tiles(product=\'ls5_nbar_albers\',\n                                  time=(\'2001-1-1 00:00:00\', \'2001-3-31 23:59:59\'))\n\n        The values can be passed to :meth:`load`\n\n        :param (int,int) cell_index: The cell index (optional). E.g. (14, -40)\n        :param query: see :py:class:`datacube.api.query.Query`\n        :rtype: dict[(int, int, numpy.datetime64), :class:`.Tile`]\n\n        .. seealso:: :meth:`load`\n        """"""\n        observations = self.cell_observations(cell_index, **query)\n        return self.tile_sources(observations, query_group_by(**query))\n\n    @staticmethod\n    def load(tile, measurements=None, dask_chunks=None, fuse_func=None, resampling=None, skip_broken_datasets=False):\n        """"""\n        Load data for a cell/tile.\n\n        The data to be loaded is defined by the output of :meth:`list_tiles`.\n\n        This is a static function and does not use the index. This can be useful when running as a worker in a\n        distributed environment and you wish to minimize database connections.\n\n        See the documentation on using `xarray with dask <http://xarray.pydata.org/en/stable/dask.html>`_\n        for more information.\n\n        :param `.Tile` tile: The tile to load.\n\n        :param list(str) measurements: The names of measurements to load\n\n        :param dict dask_chunks: If the data should be loaded as needed using :py:class:`dask.array.Array`,\n            specify the chunk size in each output direction.\n\n            See the documentation on using `xarray with dask <http://xarray.pydata.org/en/stable/dask.html>`_\n            for more information.\n\n        :param fuse_func: Function to fuse together a tile that has been pre-grouped by calling\n            :meth:`list_cells` with a ``group_by`` parameter.\n\n        :param str|dict resampling:\n\n            The resampling method to use if re-projection is required, could be\n            configured per band using a dictionary (:meth: `load_data`)\n\n            Valid values are: ``\'nearest\', \'cubic\', \'bilinear\', \'cubic_spline\', \'lanczos\', \'average\'``\n\n            Defaults to ``\'nearest\'``.\n\n        :param bool skip_broken_datasets: If True, ignore broken datasets and continue processing with the data\n             that can be loaded. If False, an exception will be raised on a broken dataset. Defaults to False.\n\n        :rtype: :py:class:`xarray.Dataset`\n\n        .. seealso::\n            :meth:`list_tiles` :meth:`list_cells`\n        """"""\n        measurement_dicts = tile.product.lookup_measurements(measurements)\n\n        dataset = Datacube.load_data(tile.sources, tile.geobox,\n                                     measurement_dicts, resampling=resampling,\n                                     dask_chunks=dask_chunks, fuse_func=fuse_func,\n                                     skip_broken_datasets=skip_broken_datasets)\n\n        return dataset\n\n    def update_tile_lineage(self, tile):\n        for i in range(tile.sources.size):\n            sources = tile.sources.values[i]\n            tile.sources.values[i] = tuple(self.index.datasets.get(dataset.id, include_sources=True)\n                                           for dataset in sources)\n        return tile\n\n    def __str__(self):\n        return ""GridWorkflow<index={!r},\\n\\tgridspec={!r}>"".format(self.index, self.grid_spec)\n\n    def __repr__(self):\n        return self.__str__()\n'"
datacube/api/query.py,1,"b'#\n#    Licensed under the Apache License, Version 2.0 (the ""License"");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an ""AS IS"" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n""""""\nStorage Query and Access API module\n""""""\n\n\nimport logging\nimport datetime\nimport collections\nimport warnings\nimport pandas\n\nfrom dateutil import tz\nfrom pandas import to_datetime as pandas_to_datetime\nimport numpy as np\n\n\nfrom ..model import Range, Dataset\nfrom ..utils import geometry, datetime_to_seconds_since_1970\nfrom ..utils.dates import normalise_dt\n\n_LOG = logging.getLogger(__name__)\n\n\nGroupBy = collections.namedtuple(\'GroupBy\', [\'dimension\', \'group_by_func\', \'units\', \'sort_key\'])\n\nSPATIAL_KEYS = (\'latitude\', \'lat\', \'y\', \'longitude\', \'lon\', \'long\', \'x\')\nCRS_KEYS = (\'crs\', \'coordinate_reference_system\')\nOTHER_KEYS = (\'measurements\', \'group_by\', \'output_crs\', \'resolution\', \'set_nan\', \'product\', \'geopolygon\', \'like\',\n              \'source_filter\')\n\n\nclass Query(object):\n    def __init__(self, index=None, product=None, geopolygon=None, like=None, **search_terms):\n        """"""Parses search terms in preparation for querying the Data Cube Index.\n\n        Create a :class:`Query` object by passing it a set of search terms as keyword arguments.\n\n        >>> query = Query(product=\'ls5_nbar_albers\', time=(\'2001-01-01\', \'2002-01-01\'))\n\n        Use by accessing :attr:`search_terms`:\n\n        >>> query.search_terms[\'time\']  # doctest: +NORMALIZE_WHITESPACE\n        Range(begin=datetime.datetime(2001, 1, 1, 0, 0, tzinfo=<UTC>), \\\n        end=datetime.datetime(2002, 1, 1, 23, 59, 59, 999999, tzinfo=tzutc()))\n\n        By passing in an ``index``, the search parameters will be validated as existing on the ``product``.\n\n        Used by :meth:`datacube.Datacube.find_datasets` and :meth:`datacube.Datacube.load`.\n\n        :param datacube.index.Index index: An optional `index` object, if checking of field names is desired.\n        :param str product: name of product\n        :param geopolygon: spatial bounds of the search\n        :type geopolygon: geometry.Geometry or None\n        :param xarray.Dataset like: spatio-temporal bounds of `like` are used for the search\n        :param search_terms:\n         * `measurements` - list of measurements to retrieve\n         * `latitude`, `lat`, `y`, `longitude`, `lon`, `long`, `x` - tuples (min, max) bounding spatial dimensions\n         * `crs` - spatial coordinate reference system to interpret the spatial bounds\n         * `group_by` - observation grouping method. One of `time`, `solar_day`. Default is `time`\n        """"""\n        self.product = product\n        self.geopolygon = query_geopolygon(geopolygon=geopolygon, **search_terms)\n        if \'source_filter\' in search_terms and search_terms[\'source_filter\'] is not None:\n            self.source_filter = Query(**search_terms[\'source_filter\'])\n        else:\n            self.source_filter = None\n\n        remaining_keys = set(search_terms.keys()) - set(SPATIAL_KEYS + CRS_KEYS + OTHER_KEYS)\n        if index:\n            unknown_keys = remaining_keys - set(index.datasets.get_field_names())\n            # TODO: What about keys source filters, and what if the keys don\'t match up with this product...\n            if unknown_keys:\n                raise LookupError(\'Unknown arguments: \', unknown_keys)\n\n        self.search = {}\n        for key in remaining_keys:\n            self.search.update(_values_to_search(**{key: search_terms[key]}))\n\n        if like:\n            assert self.geopolygon is None, ""\'like\' with other spatial bounding parameters is not supported""\n            self.geopolygon = getattr(like, \'extent\', self.geopolygon)\n\n            if \'time\' not in self.search:\n                time_coord = like.coords.get(\'time\')\n                if time_coord is not None:\n                    self.search[\'time\'] = _time_to_search_dims(\n                        (pandas_to_datetime(time_coord.values[0]).to_pydatetime(),\n                         pandas_to_datetime(time_coord.values[-1]).to_pydatetime()\n                         + datetime.timedelta(milliseconds=1))  # TODO: inclusive time searches\n                    )\n\n    @property\n    def search_terms(self):\n        """"""\n        Access the search terms as a dictionary.\n\n        :type: dict\n        """"""\n        kwargs = {}\n        kwargs.update(self.search)\n        if self.geopolygon:\n            geo_bb = geometry.lonlat_bounds(self.geopolygon, resolution=100_000)  # TODO: pick resolution better\n            if geo_bb.bottom != geo_bb.top:\n                kwargs[\'lat\'] = Range(geo_bb.bottom, geo_bb.top)\n            else:\n                kwargs[\'lat\'] = geo_bb.bottom\n            if geo_bb.left != geo_bb.right:\n                kwargs[\'lon\'] = Range(geo_bb.left, geo_bb.right)\n            else:\n                kwargs[\'lon\'] = geo_bb.left\n        if self.product:\n            kwargs[\'product\'] = self.product\n        if self.source_filter:\n            kwargs[\'source_filter\'] = self.source_filter.search_terms\n        return kwargs\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return """"""Datacube Query:\n        type = {type}\n        search = {search}\n        geopolygon = {geopolygon}\n        """""".format(type=self.product,\n                   search=self.search,\n                   geopolygon=self.geopolygon)\n\n\ndef query_geopolygon(geopolygon=None, **kwargs):\n    spatial_dims = {dim: v for dim, v in kwargs.items() if dim in SPATIAL_KEYS}\n    crs = [v for k, v in kwargs.items() if k in CRS_KEYS]\n    if len(crs) == 1:\n        spatial_dims[\'crs\'] = crs[0]\n    elif len(crs) > 1:\n        raise ValueError(\'CRS is supplied twice\')\n\n    if geopolygon is not None and len(spatial_dims) > 0:\n        raise ValueError(\'Cannot specify ""geopolygon"" and one of %s at the same time\' % (SPATIAL_KEYS + CRS_KEYS,))\n\n    if geopolygon is None:\n        return _range_to_geopolygon(**spatial_dims)\n\n    return geopolygon\n\n\ndef _extract_time_from_ds(ds: Dataset) -> datetime.datetime:\n    return normalise_dt(ds.center_time)\n\n\ndef query_group_by(group_by=\'time\', **kwargs):\n    if not isinstance(group_by, str):\n        return group_by\n\n    time_grouper = GroupBy(dimension=\'time\',\n                           group_by_func=_extract_time_from_ds,\n                           units=\'seconds since 1970-01-01 00:00:00\',\n                           sort_key=_extract_time_from_ds)\n\n    solar_day_grouper = GroupBy(dimension=\'time\',\n                                group_by_func=solar_day,\n                                units=\'seconds since 1970-01-01 00:00:00\',\n                                sort_key=_extract_time_from_ds)\n\n    group_by_map = {\n        None: time_grouper,\n        \'time\': time_grouper,\n        \'solar_day\': solar_day_grouper\n    }\n\n    try:\n        return group_by_map[group_by]\n    except KeyError:\n        raise LookupError(\'No group by function for\', group_by)\n\n\ndef _range_to_geopolygon(**kwargs):\n    input_crs = None\n    input_coords = {\'left\': None, \'bottom\': None, \'right\': None, \'top\': None}\n    for key, value in kwargs.items():\n        if value is None:\n            continue\n        key = key.lower()\n        if key in [\'latitude\', \'lat\', \'y\']:\n            input_coords[\'top\'], input_coords[\'bottom\'] = _value_to_range(value)\n        if key in [\'longitude\', \'lon\', \'long\', \'x\']:\n            input_coords[\'left\'], input_coords[\'right\'] = _value_to_range(value)\n        if key in [\'crs\', \'coordinate_reference_system\']:\n            input_crs = geometry.CRS(value)\n    input_crs = input_crs or geometry.CRS(\'EPSG:4326\')\n    if any(v is not None for v in input_coords.values()):\n        if input_coords[\'left\'] == input_coords[\'right\']:\n            if input_coords[\'top\'] == input_coords[\'bottom\']:\n                return geometry.point(input_coords[\'left\'], input_coords[\'top\'], crs=input_crs)\n            else:\n                points = [(input_coords[\'left\'], input_coords[\'bottom\']),\n                          (input_coords[\'left\'], input_coords[\'top\'])]\n                return geometry.line(points, crs=input_crs)\n        else:\n            if input_coords[\'top\'] == input_coords[\'bottom\']:\n                points = [(input_coords[\'left\'], input_coords[\'top\']),\n                          (input_coords[\'right\'], input_coords[\'top\'])]\n                return geometry.line(points, crs=input_crs)\n            else:\n                points = [\n                    (input_coords[\'left\'], input_coords[\'top\']),\n                    (input_coords[\'right\'], input_coords[\'top\']),\n                    (input_coords[\'right\'], input_coords[\'bottom\']),\n                    (input_coords[\'left\'], input_coords[\'bottom\']),\n                    (input_coords[\'left\'], input_coords[\'top\'])\n                ]\n                return geometry.polygon(points, crs=input_crs)\n    return None\n\n\ndef _value_to_range(value):\n    if isinstance(value, (str, float, int)):\n        value = float(value)\n        return value, value\n    else:\n        return float(value[0]), float(value[-1])\n\n\ndef _values_to_search(**kwargs):\n    search = {}\n    for key, value in kwargs.items():\n        if key.lower() in (\'time\', \'t\'):\n            search[\'time\'] = _time_to_search_dims(value)\n        elif key not in [\'latitude\', \'lat\', \'y\'] + [\'longitude\', \'lon\', \'x\']:\n            if isinstance(value, collections.abc.Sequence) and len(value) == 2:\n                search[key] = Range(*value)\n            else:\n                search[key] = value\n    return search\n\n\ndef _datetime_to_timestamp(dt):\n    if not isinstance(dt, datetime.datetime) and not isinstance(dt, datetime.date):\n        dt = _to_datetime(dt)\n    return datetime_to_seconds_since_1970(dt)\n\n\ndef _to_datetime(t):\n    if isinstance(t, (float, int)):\n        t = datetime.datetime.fromtimestamp(t, tz=tz.tzutc())\n\n    if isinstance(t, tuple):\n        t = datetime.datetime(*t, tzinfo=tz.tzutc())\n    elif isinstance(t, str):\n        try:\n            t = datetime.datetime.strptime(t, ""%Y-%m-%dT%H:%M:%S.%fZ"")\n        except ValueError:\n            pass\n    elif isinstance(t, datetime.datetime):\n        if t.tzinfo is None:\n            t = t.replace(tzinfo=tz.tzutc())\n        return t\n\n    return pandas_to_datetime(t, utc=True, infer_datetime_format=True).to_pydatetime()\n\n\ndef _time_to_search_dims(time_range):\n    with warnings.catch_warnings():\n        warnings.simplefilter(""ignore"", UserWarning)\n\n        tr_start, tr_end = time_range, time_range\n\n        if hasattr(time_range, \'__iter__\') and not isinstance(time_range, str):\n            tmp = list(time_range)\n            tr_start, tr_end = tmp[0], tmp[-1]\n\n        # Attempt conversion to isoformat\n        # allows pandas.Period to handle\n        # date and datetime objects\n        if hasattr(tr_start, \'isoformat\'):\n            tr_start = tr_start.isoformat()\n        if hasattr(tr_end, \'isoformat\'):\n            tr_end = tr_end.isoformat()\n\n        start = _to_datetime(tr_start)\n        end = _to_datetime(pandas.Period(tr_end)\n                           .end_time\n                           .to_pydatetime())\n\n        tr = Range(start, end)\n        if start == end:\n            return tr[0]\n\n        return tr\n\n\ndef _convert_to_solar_time(utc, longitude):\n    seconds_per_degree = 240\n    offset_seconds = int(longitude * seconds_per_degree)\n    offset = datetime.timedelta(seconds=offset_seconds)\n    return utc + offset\n\n\ndef solar_day(dataset, longitude=None):\n    utc = dataset.center_time\n\n    if longitude is None:\n        m = dataset.metadata\n        if hasattr(m, \'lon\'):\n            lon = m.lon\n            longitude = (lon.begin + lon.end)*0.5\n        else:\n            raise ValueError(\'Cannot compute solar_day: dataset is missing spatial info\')\n\n    solar_time = _convert_to_solar_time(utc, longitude)\n    return np.datetime64(solar_time.date(), \'D\')\n'"
datacube/drivers/__init__.py,0,"b'""""""\nThis module implements a simple plugin manager for storage and index drivers.\n""""""\n\nfrom .indexes import index_driver_by_name, index_drivers\nfrom .readers import new_datasource, reader_drivers\nfrom .writers import storage_writer_by_name, writer_drivers\n\n__all__ = [\'new_datasource\', \'storage_writer_by_name\',\n           \'index_driver_by_name\', \'index_drivers\',\n           \'reader_drivers\', \'writer_drivers\']\n'"
datacube/drivers/_tools.py,0,"b'from threading import Lock\nfrom typing import Any\n\nDRIVER_CACHE_LOCK = Lock()\nNOT_SET_MARKER = object()\n\n\ndef singleton_setup(obj: object,\n                    key: str,\n                    factory,\n                    *args,\n                    **kwargs) -> Any:\n    """"""\n    Does:\n      obj.key = factory(*args, **kwargs)  # but only once and in a thread safe manner\n      return obj.key\n    """"""\n    v = getattr(obj, key, NOT_SET_MARKER)\n    if v is not NOT_SET_MARKER:\n        return v\n\n    with DRIVER_CACHE_LOCK:\n        v = getattr(obj, key, NOT_SET_MARKER)\n        if v is not NOT_SET_MARKER:\n            # very rare multi-thread only event.\n            # Other thread did it first but after this thread\'s initial check above.\n            # Disable test cover\n            return v  # pragma: no cover\n        v = factory(*args, **kwargs)\n        setattr(obj, key, v)\n        return v\n'"
datacube/drivers/_types.py,2,"b'"""""" Defines abstract types for IO drivers.\n""""""\nfrom typing import (\n    List, Tuple, Optional, Union, Any, Iterable,\n    TYPE_CHECKING\n)\n\nfrom abc import ABCMeta, abstractmethod\nimport numpy as np\nfrom affine import Affine\nfrom concurrent.futures import Future\nfrom datacube.storage import BandInfo\nfrom datacube.utils.geometry import CRS\n\n# pylint: disable=invalid-name,unsubscriptable-object,pointless-statement\n\nif TYPE_CHECKING:\n    FutureGeoRasterReader = Future[\'GeoRasterReader\']  # pragma: no cover\n    FutureNdarray = Future[np.ndarray]                 # pragma: no cover\nelse:\n    FutureGeoRasterReader = Future\n    FutureNdarray = Future\n\n\nRasterShape = Tuple[int, int]\nRasterWindow = Tuple[slice, slice]\n\n\nclass GeoRasterReader(object, metaclass=ABCMeta):\n    """""" Abstract base class for dataset reader.\n    """"""\n\n    @property\n    @abstractmethod\n    def crs(self) -> Optional[CRS]:\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def transform(self) -> Optional[Affine]:\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def dtype(self) -> np.dtype:\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def shape(self) -> RasterShape:\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def nodata(self) -> Optional[Union[int, float]]:\n        ...  # pragma: no cover\n\n    @abstractmethod\n    def read(self,\n             window: Optional[RasterWindow] = None,\n             out_shape: Optional[RasterShape] = None) -> FutureNdarray:\n        ...  # pragma: no cover\n\n\nclass ReaderDriver(object, metaclass=ABCMeta):\n    """""" Interface for Reader Driver\n    """"""\n\n    @abstractmethod\n    def new_load_context(self,\n                         bands: Iterable[BandInfo],\n                         old_ctx: Optional[Any]) -> Any:\n        """"""Recycle old context if available/possible and create new context.\n           ``old_ctx`` won\'t be used after this call.\n\n           Same context object is passed to all calls to ``open`` function that\n           happen within the same ``dc.load``.\n\n           If your driver doesn\'t need it just return ``None``\n        """"""\n        ...  # pragma: no cover\n\n    @abstractmethod\n    def open(self, band: BandInfo, ctx: Any) -> FutureGeoRasterReader:\n        ...  # pragma: no cover\n\n\nclass ReaderDriverEntry(object, metaclass=ABCMeta):\n    @property\n    @abstractmethod\n    def protocols(self) -> List[str]:\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def formats(self) -> List[str]:\n        ...  # pragma: no cover\n\n    @abstractmethod\n    def supports(self, protocol: str, fmt: str) -> bool:\n        ...  # pragma: no cover\n\n    def new_instance(self, cfg: dict) -> ReaderDriver:\n        ...  # pragma: no cover\n'"
datacube/drivers/datasource.py,2,"b'"""""" Defines abstract types for IO reader drivers.\n""""""\nfrom abc import ABCMeta, abstractmethod\nfrom contextlib import contextmanager\nimport numpy as np\nfrom affine import Affine\nfrom typing import Tuple, Iterator, Optional, Union\n\n\nRasterShape = Tuple[int, int]                 # pylint: disable=invalid-name\nRasterWindow = Union[                         # pylint: disable=invalid-name\n    Tuple[Tuple[int, int], Tuple[int, int]],\n    Tuple[slice, slice]]\n\n# pylint: disable=pointless-statement\n\n\nclass GeoRasterReader(object, metaclass=ABCMeta):\n    """""" Abstract base class for dataset reader.\n    """"""\n\n    @property\n    @abstractmethod\n    def crs(self):\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def transform(self) -> Optional[Affine]:\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def dtype(self) -> Union[str, np.dtype]:\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def shape(self) -> RasterShape:\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def nodata(self) -> Optional[Union[int, float]]:\n        ...  # pragma: no cover\n\n    @abstractmethod\n    def read(self,\n             window: Optional[RasterWindow] = None,\n             out_shape: Optional[RasterShape] = None) -> Optional[np.ndarray]:\n        ...  # pragma: no cover\n\n\nclass DataSource(object, metaclass=ABCMeta):\n    """""" Abstract base class for dataset source.\n    """"""\n\n    @abstractmethod\n    @contextmanager\n    def open(self) -> Iterator[GeoRasterReader]:\n        ...  # pragma: no cover\n'"
datacube/drivers/driver_cache.py,0,"b'import logging\nfrom typing import Dict, Any, Tuple, Iterable\n\n_LOG = logging.getLogger(__name__)\n\n\ndef load_drivers(group: str) -> Dict[str, Any]:\n    """"""\n    Load available drivers for a given group name.\n\n    Gracefully handles:\n\n     - Driver module not able to be imported\n     - Driver init function throwing an exception or returning None\n\n     By having driver entry_points pointing to a function, we defer loading the driver\n     module or running any code until required.\n\n    :param group: Name of the entry point group e.g. ""datacube.plugins.io.read""\n\n    :returns: Dictionary String -> Driver Object\n    """"""\n\n    def safe_load(ep):\n        from pkg_resources import DistributionNotFound\n        # pylint: disable=broad-except,bare-except\n        try:\n            driver_init = ep.load()\n        except DistributionNotFound:\n            # This happens when entry points were marked with extra features,\n            # but extra feature were not requested for installation\n            return None\n        except Exception as e:\n            _LOG.warning(\'Failed to resolve driver %s::%s\', group, ep.name)\n            _LOG.warning(\'Error was: %s\', repr(e))\n            return None\n\n        try:\n            driver = driver_init()\n        except Exception:\n            _LOG.warning(\'Exception during driver init, driver name: %s::%s\', group, ep.name)\n            return None\n\n        if driver is None:\n            _LOG.warning(\'Driver init returned None, driver name: %s::%s\', group, ep.name)\n\n        return driver\n\n    def resolve_all(group: str) -> Iterable[Tuple[str, Any]]:\n        from pkg_resources import iter_entry_points\n        for ep in iter_entry_points(group=group, name=None):\n            driver = safe_load(ep)\n            if driver is not None:\n                yield (ep.name, driver)\n\n    return dict((name, driver) for name, driver in resolve_all(group))\n'"
datacube/drivers/indexes.py,0,"b'from typing import List\n\nfrom ._tools import singleton_setup\nfrom .driver_cache import load_drivers\n\n\nclass IndexDriverCache(object):\n    def __init__(self, group: str):\n        self._drivers = load_drivers(group)\n\n        if len(self._drivers) == 0:\n            from datacube.index.index import index_driver_init\n            self._drivers = dict(default=index_driver_init())\n\n        for driver in list(self._drivers.values()):\n            if hasattr(driver, \'aliases\'):\n                for alias in driver.aliases:\n                    self._drivers[alias] = driver\n\n    def __call__(self, name: str):\n        """"""\n        :returns: None if driver with a given name is not found\n\n        :param name: Driver name\n        :return: Returns IndexDriver\n        """"""\n        return self._drivers.get(name, None)\n\n    def drivers(self) -> List[str]:\n        """""" Returns list of driver names\n        """"""\n        return list(self._drivers.keys())\n\n\ndef index_cache() -> IndexDriverCache:\n    """""" Singleton for IndexDriverCache\n    """"""\n    return singleton_setup(index_cache, \'_instance\',\n                           IndexDriverCache,\n                           \'datacube.plugins.index\')\n\n\ndef index_drivers():\n    """""" Returns list driver names\n    """"""\n    return index_cache().drivers()\n\n\ndef index_driver_by_name(name):\n    """""" Lookup writer driver by name\n\n    :returns: Initialised writer driver instance\n    :returns: None if driver with this name doesn\'t exist\n    """"""\n    return index_cache()(name)\n'"
datacube/drivers/readers.py,0,"b'from typing import List, Optional, Callable\nfrom .driver_cache import load_drivers\nfrom .datasource import DataSource\nfrom ._tools import singleton_setup\nfrom datacube.storage._base import BandInfo\n\nDatasourceFactory = Callable[[BandInfo], DataSource]  # pylint: disable=invalid-name\n\n\nclass ReaderDriverCache(object):\n    def __init__(self, group: str):\n        self._drivers = load_drivers(group)\n\n        lookup = {}\n        for driver in self._drivers.values():\n            for uri_scheme in driver.protocols:\n                for fmt in driver.formats:\n                    if driver.supports(uri_scheme, fmt):\n                        key = (uri_scheme.lower(), fmt.lower())\n                        lookup[key] = driver\n\n        self._lookup = lookup\n\n    def _find_driver(self, uri_scheme: str, fmt: str):\n        key = (uri_scheme.lower(), fmt.lower())\n        return self._lookup.get(key)\n\n    def __call__(self, uri_scheme: str, fmt: str,\n                 fallback: Optional[DatasourceFactory] = None) -> DatasourceFactory:\n        """"""Lookup `new_datasource` constructor method from the driver. Returns\n        `fallback` method if no driver is found.\n\n        :param uri_scheme: Protocol part of the Dataset uri\n        :param fmt: Dataset format\n        :return: Returns function `(DataSet, band_name:str) => DataSource`\n        """"""\n        driver = self._find_driver(uri_scheme, fmt)\n        if driver is not None:\n            return driver.new_datasource\n        if fallback is not None:\n            return fallback\n        else:\n            raise KeyError(""No driver found and no fallback provided"")\n\n    def drivers(self) -> List[str]:\n        """""" Returns list of driver names\n        """"""\n        return list(self._drivers.keys())\n\n\ndef rdr_cache() -> ReaderDriverCache:\n    """""" Singleton for ReaderDriverCache\n    """"""\n    return singleton_setup(rdr_cache, \'_instance\',\n                           ReaderDriverCache,\n                           \'datacube.plugins.io.read\')\n\n\ndef reader_drivers() -> List[str]:\n    """""" Returns list driver names\n    """"""\n    return rdr_cache().drivers()\n\n\ndef choose_datasource(band: \'BandInfo\') -> DatasourceFactory:\n    """"""Returns appropriate `DataSource` class (or a constructor method) for loading\n    given `dataset`.\n\n    An appropriate `DataSource` implementation is chosen based on:\n\n    - Dataset URI (protocol part)\n    - Dataset format\n    - Current system settings\n    - Available IO plugins\n\n    NOTE: we assume that all bands can be loaded with the same implementation.\n\n    """"""\n    from datacube.storage._rio import RasterDatasetDataSource\n    return rdr_cache()(band.uri_scheme, band.format, fallback=RasterDatasetDataSource)\n\n\ndef new_datasource(band: BandInfo) -> Optional[DataSource]:\n    """"""Returns a newly constructed data source to read dataset band data.\n\n    An appropriate `DataSource` implementation is chosen based on:\n\n    - Dataset URI (protocol part)\n    - Dataset format\n    - Current system settings\n    - Available IO plugins\n\n    This function will return the default :class:`RasterDatasetDataSource` if no more specific\n    ``DataSource`` can be found.\n\n    :param dataset: The dataset to read.\n    :param str band_name: the name of the band to read.\n\n    """"""\n\n    source_type = choose_datasource(band)\n\n    if source_type is None:\n        return None\n\n    return source_type(band)\n'"
datacube/drivers/writers.py,0,"b'from typing import List\n\nfrom ._tools import singleton_setup\nfrom .driver_cache import load_drivers\n\n\nclass WriterDriverCache(object):\n    def __init__(self, group: str):\n        self._drivers = load_drivers(group)\n\n        for driver in list(self._drivers.values()):\n            if hasattr(driver, \'aliases\'):\n                for alias in driver.aliases:\n                    self._drivers[alias] = driver\n\n    def __call__(self, name: str):\n        """"""\n        :returns: None if driver with a given name is not found\n\n        :param name: Driver name\n        :return: Returns WriterDriver\n        """"""\n        return self._drivers.get(name, None)\n\n    def drivers(self) -> List[str]:\n        """""" Returns list of driver names\n        """"""\n        return list(self._drivers.keys())\n\n\ndef writer_cache() -> WriterDriverCache:\n    """""" Singleton for WriterDriverCache\n    """"""\n    return singleton_setup(writer_cache, \'_instance\',\n                           WriterDriverCache,\n                           \'datacube.plugins.io.write\')\n\n\ndef writer_drivers() -> List[str]:\n    """""" Returns list driver names\n    """"""\n    return writer_cache().drivers()\n\n\ndef storage_writer_by_name(name):\n    """""" Lookup writer driver by name\n\n    :returns: Initialised writer driver instance\n    :returns: None if driver with this name doesn\'t exist\n    """"""\n    return writer_cache()(name)\n'"
datacube/execution/__init__.py,0,b''
datacube/execution/worker.py,0,"b'""""""\n  This app launches workers for distributed work loads\n""""""\n\nimport click\n\nKNOWN_WORKER_TYPES = [\'distributed\', \'dask\', \'celery\']\n\n\ndef parse_executor_opt(ctx, param, value):\n    from datacube.ui.click import parse_endpoint\n    (ex_type, host_port) = value\n    if ex_type is None:\n        ctx.fail(\'Need to provide valid --executor argument\')\n\n    try:\n        host, port = parse_endpoint(host_port)\n    except ValueError:\n        ctx.fail(\'Expected host:port, got `{}`\'.format(host_port))\n\n    return ex_type, host, port\n\n\ndef launch_celery_worker(host, port, nprocs, password=\'\'):\n    from datacube import _celery_runner as cr\n    cr.launch_worker(host, port, password=password, nprocs=nprocs)\n\n\ndef launch_distributed_worker(host, port, nprocs, nthreads=1):\n    import subprocess\n\n    addr = \'{}:{}\'.format(host, port)\n    dask_worker = [\'dask-worker\', addr,\n                   \'--nthreads\', str(nthreads)]\n\n    if nprocs > 0:\n        dask_worker.extend([\'--nprocs\', str(nprocs)])\n\n    subprocess.check_call(dask_worker)\n\n\n@click.command(name=\'worker\')\n@click.option(\'--executor\', type=(click.Choice(KNOWN_WORKER_TYPES), str),  # type: ignore\n              help=""(distributed|dask(alias for distributed)|celery) host:port"",\n              default=(None, None),\n              callback=parse_executor_opt)\n@click.option(\'--nprocs\', type=int, default=0, help=\'Number of worker processes to launch\')\ndef main(executor, nprocs):\n    launchers = dict(celery=launch_celery_worker,\n                     dask=launch_distributed_worker,\n                     distributed=launch_distributed_worker)\n    ex_type, host, port = executor\n    return launchers[ex_type](host, port, nprocs)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
datacube/index/__init__.py,0,"b'# coding=utf-8\n""""""\nModules for interfacing with the index/database.\n""""""\n\nfrom ._api import index_connect\nfrom .fields import UnknownFieldError\nfrom .exceptions import DuplicateRecordError, MissingRecordError, IndexSetupError\nfrom .index import Index\n\n__all__ = [\n    \'index_connect\',\n    \'Index\',\n\n    \'DuplicateRecordError\',\n    \'IndexSetupError\',\n    \'MissingRecordError\',\n    \'UnknownFieldError\',\n]\n'"
datacube/index/_api.py,0,"b'# coding=utf-8\n""""""\nAccess methods for indexing datasets & products.\n""""""\n\nimport logging\n\nfrom datacube.config import LocalConfig\nfrom datacube.drivers import index_driver_by_name, index_drivers\nfrom .index import Index\n\n_LOG = logging.getLogger(__name__)\n\n\ndef index_connect(local_config: LocalConfig = None,\n                  application_name: str = None,\n                  validate_connection: bool = True) -> Index:\n    """"""\n    Create a Data Cube Index that can connect to a PostgreSQL server\n\n    It contains all the required connection parameters, but doesn\'t actually\n    check that the server is available.\n\n    :param application_name: A short, alphanumeric name to identify this application.\n    :param local_config: Config object to use. (optional)\n    :param validate_connection: Validate database connection and schema immediately\n    :raises datacube.index.Exceptions.IndexSetupError:\n    """"""\n    if local_config is None:\n        local_config = LocalConfig.find()\n\n    driver_name = local_config.get(\'index_driver\', \'default\')\n    index_driver = index_driver_by_name(driver_name)\n    if not index_driver:\n        raise RuntimeError(\n            ""No index driver found for %r. %s available: %s"" % (\n                driver_name, len(index_drivers()), \', \'.join(index_drivers())\n            )\n        )\n\n    return index_driver.connect_to_index(local_config,\n                                         application_name=application_name,\n                                         validate_connection=validate_connection)\n'"
datacube/index/_datasets.py,0,"b'# type: ignore\n""""""\nAPI for dataset indexing, access and search.\n""""""\nimport logging\nimport warnings\nfrom collections import namedtuple\nfrom typing import Any, Iterable, Set, Tuple, Union, List\nfrom uuid import UUID\n\nfrom datacube.model import Dataset, DatasetType\nfrom datacube.model.utils import flatten_datasets\nfrom datacube.utils import jsonify_document, changes, cached_property\nfrom datacube.utils.changes import get_doc_changes\nfrom . import fields\n\nimport json\nfrom datacube.drivers.postgres._fields import SimpleDocField, DateDocField\nfrom datacube.drivers.postgres._schema import DATASET\nfrom sqlalchemy import select, func\nfrom datacube.model.fields import Field\n\n_LOG = logging.getLogger(__name__)\n\n\n# It\'s a public api, so we can\'t reorganise old methods.\n# pylint: disable=too-many-public-methods, too-many-lines\n\nclass DatasetSpatialMixin(object):\n    __slots__ = ()\n\n    @property\n    def _gs(self):\n        return self.grid_spatial\n\n    @property\n    def crs(self):\n        return Dataset.crs.__get__(self)\n\n    @cached_property\n    def extent(self):\n        return Dataset.extent.func(self)\n\n    @property\n    def transform(self):\n        return Dataset.transform.__get__(self)\n\n    @property\n    def bounds(self):\n        return Dataset.bounds.__get__(self)\n\n\nclass DatasetResource(object):\n    """"""\n    :type _db: datacube.drivers.postgres._connections.PostgresDb\n    :type types: datacube.index._products.ProductResource\n    """"""\n\n    def __init__(self, db, dataset_type_resource):\n        """"""\n        :type db: datacube.drivers.postgres._connections.PostgresDb\n        :type dataset_type_resource: datacube.index._products.ProductResource\n        """"""\n        self._db = db\n        self.types = dataset_type_resource\n\n    def get(self, id_, include_sources=False):\n        """"""\n        Get dataset by id\n\n        :param UUID id_: id of the dataset to retrieve\n        :param bool include_sources: get the full provenance graph?\n        :rtype: Dataset\n        """"""\n        if isinstance(id_, str):\n            id_ = UUID(id_)\n\n        with self._db.connect() as connection:\n            if not include_sources:\n                dataset = connection.get_dataset(id_)\n                return self._make(dataset, full_info=True) if dataset else None\n\n            datasets = {result[\'id\']: (self._make(result, full_info=True), result)\n                        for result in connection.get_dataset_sources(id_)}\n\n        if not datasets:\n            # No dataset found\n            return None\n\n        for dataset, result in datasets.values():\n            dataset.metadata.sources = {\n                classifier: datasets[source][0].metadata_doc\n                for source, classifier in zip(result[\'sources\'], result[\'classes\']) if source\n            }\n            dataset.sources = {\n                classifier: datasets[source][0]\n                for source, classifier in zip(result[\'sources\'], result[\'classes\']) if source\n            }\n        return datasets[id_][0]\n\n    def bulk_get(self, ids):\n        def to_uuid(x):\n            return x if isinstance(x, UUID) else UUID(x)\n\n        ids = [to_uuid(i) for i in ids]\n\n        with self._db.connect() as connection:\n            rows = connection.get_datasets(ids)\n            return [self._make(r, full_info=True) for r in rows]\n\n    def get_derived(self, id_):\n        """"""\n        Get all derived datasets\n\n        :param UUID id_: dataset id\n        :rtype: list[Dataset]\n        """"""\n        with self._db.connect() as connection:\n            return [\n                self._make(result, full_info=True)\n                for result in connection.get_derived_datasets(id_)\n            ]\n\n    def has(self, id_):\n        """"""\n        Have we already indexed this dataset?\n\n        :param typing.Union[UUID, str] id_: dataset id\n        :rtype: bool\n        """"""\n        with self._db.connect() as connection:\n            return connection.contains_dataset(id_)\n\n    def bulk_has(self, ids_):\n        """"""\n        Like `has` but operates on a list of ids.\n\n        For every supplied id check if database contains a dataset with that id.\n\n        :param [typing.Union[UUID, str]] ids_: list of dataset ids\n\n        :rtype: [bool]\n        """"""\n        with self._db.connect() as connection:\n            existing = set(connection.datasets_intersection(ids_))\n\n        return [x in existing for x in\n                map((lambda x: UUID(x) if isinstance(x, str) else x), ids_)]\n\n    def add(self, dataset, with_lineage=None, **kwargs):\n        """"""\n        Add ``dataset`` to the index. No-op if it is already present.\n\n        :param Dataset dataset: dataset to add\n        :param bool with_lineage: True -- attempt adding lineage if it\'s missing, False don\'t\n        :rtype: Dataset\n        """"""\n\n        def process_bunch(dss, main_ds, transaction):\n            edges = []\n\n            # First insert all new datasets\n            for ds in dss:\n                is_new = transaction.insert_dataset(ds.metadata_doc_without_lineage(), ds.id, ds.type.id)\n                if is_new:\n                    edges.extend((name, ds.id, src.id)\n                                 for name, src in ds.sources.items())\n\n            # Second insert lineage graph edges\n            for ee in edges:\n                transaction.insert_dataset_source(*ee)\n\n            # Finally update location for top-level dataset only\n            if main_ds.uris is not None:\n                self._ensure_new_locations(main_ds, transaction=transaction)\n\n        if with_lineage is None:\n            policy = kwargs.pop(\'sources_policy\', None)\n            if policy is not None:\n                _LOG.debug(\'Use of sources_policy is deprecated\')\n                with_lineage = (policy != ""skip"")\n                if policy == \'verify\':\n                    _LOG.debug(\'Verify is no longer done inside add\')\n            else:\n                with_lineage = True\n\n        _LOG.info(\'Indexing %s\', dataset.id)\n\n        if with_lineage:\n            ds_by_uuid = flatten_datasets(dataset)\n            all_uuids = list(ds_by_uuid)\n\n            present = {k: v for k, v in zip(all_uuids, self.bulk_has(all_uuids))}\n\n            if present[dataset.id]:\n                _LOG.warning(\'Dataset %s is already in the database\', dataset.id)\n                return dataset\n\n            dss = [ds for ds in [dss[0] for dss in ds_by_uuid.values()] if not present[ds.id]]\n        else:\n            if self.has(dataset.id):\n                _LOG.warning(\'Dataset %s is already in the database\', dataset.id)\n                return dataset\n\n            dss = [dataset]\n\n        with self._db.begin() as transaction:\n            process_bunch(dss, dataset, transaction)\n\n        return dataset\n\n    def search_product_duplicates(self, product: DatasetType, *args):\n        """"""\n        Find dataset ids who have duplicates of the given set of field names.\n\n        Product is always inserted as the first grouping field.\n\n        Returns each set of those field values and the datasets that have them.\n        """"""\n\n        def load_field(f: Union[str, fields.Field]) -> fields.Field:\n            if isinstance(f, str):\n                return product.metadata_type.dataset_fields[f]\n            assert isinstance(f, fields.Field), ""Not a field: %r"" % (f,)\n            return f\n\n        group_fields = [load_field(f) for f in args]  # type: List[fields.Field]\n        result_type = namedtuple(\'search_result\', list(f.name for f in group_fields))  # type: ignore\n\n        expressions = [product.metadata_type.dataset_fields.get(\'product\') == product.name]\n\n        with self._db.connect() as connection:\n            for record in connection.get_duplicates(group_fields, expressions):\n                dataset_ids = set(record[0])\n                grouped_fields = tuple(record[1:])\n                yield result_type(*grouped_fields), dataset_ids\n\n    def can_update(self, dataset, updates_allowed=None):\n        """"""\n        Check if dataset can be updated. Return bool,safe_changes,unsafe_changes\n\n        :param Dataset dataset: Dataset to update\n        :param dict updates_allowed: Allowed updates\n        :rtype: bool,list[change],list[change]\n        """"""\n        need_sources = dataset.sources is not None\n        existing = self.get(dataset.id, include_sources=need_sources)\n        if not existing:\n            raise ValueError(\'Unknown dataset %s, cannot update \xe2\x80\x93 did you intend to add it?\' % dataset.id)\n\n        if dataset.type.name != existing.type.name:\n            raise ValueError(\'Changing product is not supported. From %s to %s in %s\' % (existing.type.name,\n                                                                                         dataset.type.name,\n                                                                                         dataset.id))\n\n        # TODO: figure out (un)safe changes from metadata type?\n        allowed = {\n            # can always add more metadata\n            tuple(): changes.allow_extension,\n        }\n        allowed.update(updates_allowed or {})\n\n        doc_changes = get_doc_changes(existing.metadata_doc, jsonify_document(dataset.metadata_doc))\n        good_changes, bad_changes = changes.classify_changes(doc_changes, allowed)\n\n        return not bad_changes, good_changes, bad_changes\n\n    def update(self, dataset, updates_allowed=None):\n        """"""\n        Update dataset metadata and location\n        :param Dataset dataset: Dataset to update\n        :param updates_allowed: Allowed updates\n        :rtype: Dataset\n        """"""\n        existing = self.get(dataset.id)\n        can_update, safe_changes, unsafe_changes = self.can_update(dataset, updates_allowed)\n\n        if not safe_changes and not unsafe_changes:\n            self._ensure_new_locations(dataset, existing)\n            _LOG.info(""No changes detected for dataset %s"", dataset.id)\n            return dataset\n\n        if not can_update:\n            full_message = ""Unsafe changes at "" + "", "".join(""."".join(offset) for offset, _, _ in unsafe_changes)\n            raise ValueError(full_message)\n\n        _LOG.info(""Updating dataset %s"", dataset.id)\n\n        for offset, old_val, new_val in safe_changes:\n            _LOG.info(""Safe change from %r to %r"", old_val, new_val)\n\n        for offset, old_val, new_val in unsafe_changes:\n            _LOG.info(""Unsafe change from %r to %r"", old_val, new_val)\n\n        product = self.types.get_by_name(dataset.type.name)\n        with self._db.begin() as transaction:\n            if not transaction.update_dataset(dataset.metadata_doc_without_lineage(), dataset.id, product.id):\n                raise ValueError(""Failed to update dataset %s..."" % dataset.id)\n\n        self._ensure_new_locations(dataset, existing)\n\n        return dataset\n\n    def _ensure_new_locations(self, dataset, existing=None, transaction=None):\n        skip_set = set([None] + existing.uris if existing is not None else [])\n        new_uris = [uri for uri in dataset.uris if uri not in skip_set]\n\n        def insert_one(uri, transaction):\n            return transaction.insert_dataset_location(dataset.id, uri)\n\n        # process in reverse order, since every add is essentially append to\n        # front of a stack\n        for uri in new_uris[::-1]:\n            if transaction is None:\n                with self._db.begin() as tr:\n                    insert_one(uri, tr)\n            else:\n                insert_one(uri, transaction)\n\n    def archive(self, ids):\n        """"""\n        Mark datasets as archived\n\n        :param list[UUID] ids: list of dataset ids to archive\n        """"""\n        with self._db.begin() as transaction:\n            for id_ in ids:\n                transaction.archive_dataset(id_)\n\n    def restore(self, ids):\n        """"""\n        Mark datasets as not archived\n\n        :param Iterable[UUID] ids: list of dataset ids to restore\n        """"""\n        with self._db.begin() as transaction:\n            for id_ in ids:\n                transaction.restore_dataset(id_)\n\n    def get_field_names(self, product_name=None):\n        """"""\n        Get the list of possible search fields for a Product\n\n        :param str product_name:\n        :rtype: set[str]\n        """"""\n        if product_name is None:\n            types = self.types.get_all()\n        else:\n            types = [self.types.get_by_name(product_name)]\n\n        out = set()\n        for type_ in types:\n            out.update(type_.metadata_type.dataset_fields)\n        return out\n\n    def get_locations(self, id_):\n        """"""\n        Get the list of storage locations for the given dataset id\n\n        :param typing.Union[UUID, str] id_: dataset id\n        :rtype: list[str]\n        """"""\n        with self._db.connect() as connection:\n            return connection.get_locations(id_)\n\n    def get_archived_locations(self, id_):\n        """"""\n        Find locations which have been archived for a dataset\n\n        :param typing.Union[UUID, str] id_: dataset id\n        :rtype: list[str]\n        """"""\n        with self._db.connect() as connection:\n            return [uri for uri, archived_dt in connection.get_archived_locations(id_)]\n\n    def get_archived_location_times(self, id_):\n        """"""\n        Get each archived location along with the time it was archived.\n\n        :param typing.Union[UUID, str] id_: dataset id\n        :rtype: List[Tuple[str, datetime.datetime]]\n        """"""\n        with self._db.connect() as connection:\n            return list(connection.get_archived_locations(id_))\n\n    def add_location(self, id_, uri):\n        """"""\n        Add a location to the dataset if it doesn\'t already exist.\n\n        :param typing.Union[UUID, str] id_: dataset id\n        :param str uri: fully qualified uri\n        :returns bool: Was one added?\n        """"""\n        if not uri:\n            warnings.warn(""Cannot add empty uri. (dataset %s)"" % id_)\n            return False\n\n        with self._db.connect() as connection:\n            return connection.insert_dataset_location(id_, uri)\n\n    def get_datasets_for_location(self, uri, mode=None):\n        """"""\n        Find datasets that exist at the given URI\n\n        :param uri: search uri\n        :param str mode: \'exact\' or \'prefix\'\n        :return:\n        """"""\n        with self._db.connect() as connection:\n            return (self._make(row) for row in connection.get_datasets_for_location(uri, mode=mode))\n\n    def remove_location(self, id_, uri):\n        """"""\n        Remove a location from the dataset if it exists.\n\n        :param typing.Union[UUID, str] id_: dataset id\n        :param str uri: fully qualified uri\n        :returns bool: Was one removed?\n        """"""\n        with self._db.connect() as connection:\n            was_removed = connection.remove_location(id_, uri)\n            return was_removed\n\n    def archive_location(self, id_, uri):\n        """"""\n        Archive a location of the dataset if it exists.\n\n        :param typing.Union[UUID, str] id_: dataset id\n        :param str uri: fully qualified uri\n        :return bool: location was able to be archived\n        """"""\n        with self._db.connect() as connection:\n            was_archived = connection.archive_location(id_, uri)\n            return was_archived\n\n    def restore_location(self, id_, uri):\n        """"""\n        Un-archive a location of the dataset if it exists.\n\n        :param typing.Union[UUID, str] id_: dataset id\n        :param str uri: fully qualified uri\n        :return bool: location was able to be restored\n        """"""\n        with self._db.connect() as connection:\n            was_restored = connection.restore_location(id_, uri)\n            return was_restored\n\n    def _make(self, dataset_res, full_info=False, product=None):\n        """"""\n        :rtype Dataset\n\n        :param bool full_info: Include all available fields\n        """"""\n        if dataset_res.uris:\n            uris = [uri for uri in dataset_res.uris if uri]\n        else:\n            uris = []\n\n        product = product or self.types.get(dataset_res.dataset_type_ref)\n\n        return Dataset(\n            type_=product,\n            metadata_doc=dataset_res.metadata,\n            uris=uris,\n            indexed_by=dataset_res.added_by if full_info else None,\n            indexed_time=dataset_res.added if full_info else None,\n            archived_time=dataset_res.archived\n        )\n\n    def _make_many(self, query_result, product=None):\n        """"""\n        :rtype list[Dataset]\n        """"""\n        return (self._make(dataset, product=product) for dataset in query_result)\n\n    def search_by_metadata(self, metadata):\n        """"""\n        Perform a search using arbitrary metadata, returning results as Dataset objects.\n\n        Caution \xe2\x80\x93 slow! This will usually not use indexes.\n\n        :param dict metadata:\n        :rtype: list[Dataset]\n        """"""\n        with self._db.connect() as connection:\n            for dataset in self._make_many(connection.search_datasets_by_metadata(metadata)):\n                yield dataset\n\n    def search(self, limit=None, **query):\n        """"""\n        Perform a search, returning results as Dataset objects.\n\n        :param Union[str,float,Range,list] query:\n        :param int limit: Limit number of datasets\n        :rtype: __generator[Dataset]\n        """"""\n        source_filter = query.pop(\'source_filter\', None)\n        for product, datasets in self._do_search_by_product(query,\n                                                            source_filter=source_filter,\n                                                            limit=limit):\n            yield from self._make_many(datasets, product)\n\n    def search_by_product(self, **query):\n        """"""\n        Perform a search, returning datasets grouped by product type.\n\n        :param dict[str,str|float|datacube.model.Range] query:\n        :rtype: __generator[(DatasetType,  __generator[Dataset])]]\n        """"""\n        for product, datasets in self._do_search_by_product(query):\n            yield product, self._make_many(datasets, product)\n\n    def search_returning(self, field_names, limit=None, **query):\n        """"""\n        Perform a search, returning only the specified fields.\n\n        This method can be faster than normal search() if you don\'t need all fields of each dataset.\n\n        It also allows for returning rows other than datasets, such as a row per uri when requesting field \'uri\'.\n\n        :param tuple[str] field_names:\n        :param Union[str,float,Range,list] query:\n        :param int limit: Limit number of datasets\n        :returns __generator[tuple]: sequence of results, each result is a namedtuple of your requested fields\n        """"""\n        result_type = namedtuple(\'search_result\', field_names)\n\n        for _, results in self._do_search_by_product(query,\n                                                     return_fields=True,\n                                                     select_field_names=field_names,\n                                                     limit=limit):\n\n            for columns in results:\n                yield result_type(*columns)\n\n    def count(self, **query):\n        """"""\n        Perform a search, returning count of results.\n\n        :param dict[str,str|float|datacube.model.Range] query:\n        :rtype: int\n        """"""\n        # This may be optimised into one query in the future.\n        result = 0\n        for product_type, count in self._do_count_by_product(query):\n            result += count\n\n        return result\n\n    def count_by_product(self, **query):\n        """"""\n        Perform a search, returning a count of for each matching product type.\n\n        :param dict[str,str|float|datacube.model.Range] query:\n        :returns: Sequence of (product, count)\n        :rtype: __generator[(DatasetType,  int)]]\n        """"""\n        return self._do_count_by_product(query)\n\n    def count_by_product_through_time(self, period, **query):\n        """"""\n        Perform a search, returning counts for each product grouped in time slices\n        of the given period.\n\n        :param dict[str,str|float|datacube.model.Range] query:\n        :param str period: Time range for each slice: \'1 month\', \'1 day\' etc.\n        :returns: For each matching product type, a list of time ranges and their count.\n        :rtype: __generator[(DatasetType, list[(datetime.datetime, datetime.datetime), int)]]\n        """"""\n        return self._do_time_count(period, query)\n\n    def count_product_through_time(self, period, **query):\n        """"""\n        Perform a search, returning counts for a single product grouped in time slices\n        of the given period.\n\n        Will raise an error if the search terms match more than one product.\n\n        :param dict[str,str|float|datacube.model.Range] query:\n        :param str period: Time range for each slice: \'1 month\', \'1 day\' etc.\n        :returns: For each matching product type, a list of time ranges and their count.\n        :rtype: list[(str, list[(datetime.datetime, datetime.datetime), int)]]\n        """"""\n        return next(self._do_time_count(period, query, ensure_single=True))[1]\n\n    def _get_dataset_types(self, q):\n        types = set()\n        if \'product\' in q.keys():\n            types.add(self.types.get_by_name(q[\'product\']))\n        else:\n            # Otherwise search any metadata type that has all the given search fields.\n            types = self.types.get_with_fields(tuple(q.keys()))\n            if not types:\n                raise ValueError(\'No type of dataset has fields: {}\'.format(q.keys()))\n\n        return types\n\n    def _get_product_queries(self, query):\n        for product, q in self.types.search_robust(**query):\n            q[\'dataset_type_id\'] = product.id\n            yield q, product\n\n    # pylint: disable=too-many-locals\n    def _do_search_by_product(self, query, return_fields=False, select_field_names=None,\n                              with_source_ids=False, source_filter=None,\n                              limit=None):\n        if source_filter:\n            product_queries = list(self._get_product_queries(source_filter))\n            if not product_queries:\n                # No products match our source filter, so there will be no search results regardless.\n                raise ValueError(\'No products match source filter: \' % source_filter)\n            if len(product_queries) > 1:\n                raise RuntimeError(""Multi-product source filters are not supported. Try adding \'product\' field"")\n\n            source_queries, source_product = product_queries[0]\n            dataset_fields = source_product.metadata_type.dataset_fields\n            source_exprs = tuple(fields.to_expressions(dataset_fields.get, **source_queries))\n        else:\n            source_exprs = None\n\n        product_queries = list(self._get_product_queries(query))\n        if not product_queries:\n            raise ValueError(\'No products match search terms: %r\' % query)\n\n        for q, product in product_queries:\n            dataset_fields = product.metadata_type.dataset_fields\n            query_exprs = tuple(fields.to_expressions(dataset_fields.get, **q))\n            select_fields = None\n            if return_fields:\n                # if no fields specified, select all\n                if select_field_names is None:\n                    select_fields = tuple(field for name, field in dataset_fields.items()\n                                          if not field.affects_row_selection)\n                else:\n                    select_fields = tuple(dataset_fields[field_name]\n                                          for field_name in select_field_names)\n            with self._db.connect() as connection:\n                yield (product,\n                       connection.search_datasets(\n                           query_exprs,\n                           source_exprs,\n                           select_fields=select_fields,\n                           limit=limit,\n                           with_source_ids=with_source_ids\n                       ))\n\n    def _do_count_by_product(self, query):\n        product_queries = self._get_product_queries(query)\n\n        for q, product in product_queries:\n            dataset_fields = product.metadata_type.dataset_fields\n            query_exprs = tuple(fields.to_expressions(dataset_fields.get, **q))\n            with self._db.connect() as connection:\n                count = connection.count_datasets(query_exprs)\n            if count > 0:\n                yield product, count\n\n    def _do_time_count(self, period, query, ensure_single=False):\n        if \'time\' not in query:\n            raise ValueError(\'Counting through time requires a ""time"" range query argument\')\n\n        query = dict(query)\n\n        start, end = query[\'time\']\n        del query[\'time\']\n\n        product_queries = list(self._get_product_queries(query))\n        if ensure_single:\n            if len(product_queries) == 0:\n                raise ValueError(\'No products match search terms: %r\' % query)\n            if len(product_queries) > 1:\n                raise ValueError(\'Multiple products match single query search: %r\' %\n                                 ([dt.name for q, dt in product_queries],))\n\n        for q, product in product_queries:\n            dataset_fields = product.metadata_type.dataset_fields\n            query_exprs = tuple(fields.to_expressions(dataset_fields.get, **q))\n            with self._db.connect() as connection:\n                yield product, list(connection.count_datasets_through_time(\n                    start,\n                    end,\n                    period,\n                    dataset_fields.get(\'time\'),\n                    query_exprs\n                ))\n\n    def search_summaries(self, **query):\n        """"""\n        Perform a search, returning just the search fields of each dataset.\n\n        :param dict[str,str|float|datacube.model.Range] query:\n        :rtype: __generator[dict]\n        """"""\n        for _, results in self._do_search_by_product(query, return_fields=True):\n            for columns in results:\n                yield dict(columns)\n\n    def search_eager(self, **query):\n        """"""\n        Perform a search, returning results as Dataset objects.\n\n        :param dict[str,str|float|datacube.model.Range] query:\n        :rtype: list[Dataset]\n        """"""\n        return list(self.search(**query))\n\n    def get_product_time_bounds(self, product: str):\n        """"""\n        Returns the minimum and maximum acquisition time of the product.\n        """"""\n\n        # Get the offsets from dataset doc\n        product = self.types.get_by_name(product)\n        dataset_section = product.metadata_type.definition[\'dataset\']\n        min_offset = dataset_section[\'search_fields\'][\'time\'][\'min_offset\']\n        max_offset = dataset_section[\'search_fields\'][\'time\'][\'max_offset\']\n\n        time_min = DateDocField(\'aquisition_time_min\',\n                                \'Min of time when dataset was acquired\',\n                                DATASET.c.metadata,\n                                False,  # is it indexed\n                                offset=min_offset,\n                                selection=\'least\')\n\n        time_max = DateDocField(\'aquisition_time_max\',\n                                \'Max of time when dataset was acquired\',\n                                DATASET.c.metadata,\n                                False,  # is it indexed\n                                offset=max_offset,\n                                selection=\'greatest\')\n\n        with self._db.connect() as connection:\n            result = connection.execute(\n                select(\n                    [func.min(time_min.alchemy_expression), func.max(time_max.alchemy_expression)]\n                ).where(\n                    DATASET.c.dataset_type_ref == product.id\n                )\n            ).first()\n\n        return result\n\n    # pylint: disable=redefined-outer-name\n    def search_returning_datasets_light(self, field_names: tuple, custom_offsets=None, limit=None, **query):\n        """"""\n        This is a dataset search function that returns the results as objects of a dynamically\n        generated Dataset class that is a subclass of tuple.\n\n        Only the requested fields will be returned together with related derived attributes as property functions\n        similer to the datacube.model.Dataset class. For example, if \'extent\'is requested all of\n        \'crs\', \'extent\', \'transform\', and \'bounds\' are available as property functions.\n\n        The field_names can be custom fields in addition to those specified in metadata_type, fixed fields, or\n        native fields. The field_names can also be derived fields like \'extent\', \'crs\', \'transform\',\n        and \'bounds\'. The custom fields require custom offsets of the metadata doc be provided.\n\n        The datasets can be selected based on values of custom fields as long as relevant custom\n        offsets are provided. However custom field values are not transformed so must match what is\n        stored in the database.\n\n        :param field_names: A tuple of field names that would be returned including derived fields\n                            such as extent, crs\n        :param custom_offsets: A dictionary of offsets in the metadata doc for custom fields\n        :param limit: Number of datasets returned per product.\n        :param query: key, value mappings of query that will be processed against metadata_types,\n                      product definitions and/or dataset table.\n        :return: A Dynamically generated DatasetLight (a subclass of namedtuple and possibly with\n        property functions).\n        """"""\n\n        assert field_names\n\n        for product, query_exprs in self.make_query_expr(query, custom_offsets):\n\n            select_fields = self.make_select_fields(product, field_names, custom_offsets)\n            select_field_names = tuple(field.name for field in select_fields)\n            result_type = namedtuple(\'DatasetLight\', select_field_names)  # type: ignore\n\n            if \'grid_spatial\' in select_field_names:\n                class DatasetLight(result_type, DatasetSpatialMixin):\n                    pass\n            else:\n                class DatasetLight(result_type):  # type: ignore\n                    __slots__ = ()\n\n            with self._db.connect() as connection:\n                results = connection.search_unique_datasets(\n                    query_exprs,\n                    select_fields=select_fields,\n                    limit=limit\n                )\n\n            for result in results:\n                field_values = dict()\n                for i_, field in enumerate(select_fields):\n                    # We need to load the simple doc fields\n                    if isinstance(field, SimpleDocField):\n                        field_values[field.name] = json.loads(result[i_])\n                    else:\n                        field_values[field.name] = result[i_]\n\n                yield DatasetLight(**field_values)  # type: ignore\n\n    def make_select_fields(self, product, field_names, custom_offsets):\n        """"""\n        Parse and generate the list of select fields to be passed to the database API.\n        """"""\n\n        assert product and field_names\n\n        dataset_fields = product.metadata_type.dataset_fields\n        dataset_section = product.metadata_type.definition[\'dataset\']\n\n        select_fields = []\n        for field_name in field_names:\n            if dataset_fields.get(field_name):\n                select_fields.append(dataset_fields[field_name])\n            else:\n                # try to construct the field\n                if field_name in {\'transform\', \'extent\', \'crs\', \'bounds\'}:\n                    grid_spatial = dataset_section.get(\'grid_spatial\')\n                    if grid_spatial:\n                        select_fields.append(SimpleDocField(\n                            \'grid_spatial\', \'grid_spatial\', DATASET.c.metadata,\n                            False,\n                            offset=grid_spatial\n                        ))\n                elif custom_offsets and field_name in custom_offsets:\n                    select_fields.append(SimpleDocField(\n                        field_name, field_name, DATASET.c.metadata,\n                        False,\n                        offset=custom_offsets[field_name]\n                    ))\n                elif field_name == \'uris\':\n                    select_fields.append(Field(\'uris\', \'uris\'))\n\n        return select_fields\n\n    def make_query_expr(self, query, custom_offsets):\n        """"""\n        Generate query expressions including queries based on custom fields\n        """"""\n\n        product_queries = list(self._get_product_queries(query))\n        custom_query = dict()\n        if not product_queries:\n            # The key, values in query that are un-machable with info\n            # in metadata types and product definitions, perhaps there are custom\n            # fields, will need to handle custom fields separately\n\n            canonical_query = query.copy()\n            custom_query = {key: canonical_query.pop(key) for key in custom_offsets\n                            if key in canonical_query}\n            product_queries = list(self._get_product_queries(canonical_query))\n\n            if not product_queries:\n                raise ValueError(\'No products match search terms: %r\' % query)\n\n        for q, product in product_queries:\n            dataset_fields = product.metadata_type.dataset_fields\n            query_exprs = tuple(fields.to_expressions(dataset_fields.get, **q))\n            custom_query_exprs = tuple(self.get_custom_query_expressions(custom_query, custom_offsets))\n\n            yield product, query_exprs + custom_query_exprs\n\n    def get_custom_query_expressions(self, custom_query, custom_offsets):\n        """"""\n        Generate query expressions for custom fields. it is assumed that custom fields are to be found\n        in metadata doc and their offsets are provided. custom_query is a dict of key fields involving\n        custom fields.\n        """"""\n\n        custom_exprs = []\n        for key in custom_query:\n            # for now we assume all custom query fields are SimpleDocFields\n            custom_field = SimpleDocField(\n                custom_query[key], custom_query[key], DATASET.c.metadata,\n                False, offset=custom_offsets[key]\n            )\n            custom_exprs.append(fields.as_expression(custom_field, custom_query[key]))\n\n        return custom_exprs\n'"
datacube/index/_metadata_types.py,0,"b'# coding=utf-8\n\nimport logging\nimport warnings\nfrom pathlib import Path\n\nfrom cachetools.func import lru_cache\n\nfrom datacube.model import MetadataType\nfrom datacube.utils import jsonify_document, changes, _readable_offset, read_documents\nfrom datacube.utils.changes import check_doc_unchanged, get_doc_changes\n\n_LOG = logging.getLogger(__name__)\n\n_DEFAULT_METADATA_TYPES_PATH = Path(__file__).parent.joinpath(\'default-metadata-types.yaml\')\n\n\ndef default_metadata_type_docs():\n    """"""A list of the bare dictionary format of default :class:`datacube.model.MetadataType`""""""\n    return [doc for (path, doc) in read_documents(_DEFAULT_METADATA_TYPES_PATH)]\n\n\nclass MetadataTypeResource(object):\n    def __init__(self, db):\n        """"""\n        :type db: datacube.drivers.postgres._connections.PostgresDb\n        """"""\n        self._db = db\n\n        self.get_unsafe = lru_cache()(self.get_unsafe)\n        self.get_by_name_unsafe = lru_cache()(self.get_by_name_unsafe)\n\n    def __getstate__(self):\n        """"""\n        We define getstate/setstate to avoid pickling the caches\n        """"""\n        return (self._db,)\n\n    def __setstate__(self, state):\n        """"""\n        We define getstate/setstate to avoid pickling the caches\n        """"""\n        self.__init__(*state)\n\n    def from_doc(self, definition):\n        """"""\n        :param dict definition:\n        :rtype: datacube.model.MetadataType\n        """"""\n        MetadataType.validate(definition)\n        return self._make(definition)\n\n    def add(self, metadata_type, allow_table_lock=False):\n        """"""\n        :param datacube.model.MetadataType metadata_type:\n        :param allow_table_lock:\n            Allow an exclusive lock to be taken on the table while creating the indexes.\n            This will halt other user\'s requests until completed.\n\n            If false, creation will be slightly slower and cannot be done in a transaction.\n        :rtype: datacube.model.MetadataType\n        """"""\n        # This column duplication is getting out of hand:\n        MetadataType.validate(metadata_type.definition)\n\n        existing = self.get_by_name(metadata_type.name)\n        if existing:\n            # They\'ve passed us the same one again. Make sure it matches what is stored.\n            check_doc_unchanged(\n                existing.definition,\n                jsonify_document(metadata_type.definition),\n                \'Metadata Type {}\'.format(metadata_type.name)\n            )\n        else:\n            with self._db.connect() as connection:\n                connection.insert_metadata_type(\n                    name=metadata_type.name,\n                    definition=metadata_type.definition,\n                    concurrently=not allow_table_lock\n                )\n        return self.get_by_name(metadata_type.name)\n\n    def can_update(self, metadata_type, allow_unsafe_updates=False):\n        """"""\n        Check if metadata type can be updated. Return bool,safe_changes,unsafe_changes\n\n        Safe updates currently allow new search fields to be added, description to be changed.\n\n        :param datacube.model.MetadataType metadata_type: updated MetadataType\n        :param bool allow_unsafe_updates: Allow unsafe changes. Use with caution.\n        :rtype: bool,list[change],list[change]\n        """"""\n        MetadataType.validate(metadata_type.definition)\n\n        existing = self.get_by_name(metadata_type.name)\n        if not existing:\n            raise ValueError(\'Unknown metadata type %s, cannot update \xe2\x80\x93 \'\n                             \'did you intend to add it?\' % metadata_type.name)\n\n        updates_allowed = {\n            (\'description\',): changes.allow_any,\n            # You can add new fields safely but not modify existing ones.\n            (\'dataset\',): changes.allow_extension,\n            (\'dataset\', \'search_fields\'): changes.allow_extension\n        }\n\n        doc_changes = get_doc_changes(existing.definition, jsonify_document(metadata_type.definition))\n        good_changes, bad_changes = changes.classify_changes(doc_changes, updates_allowed)\n\n        for offset, old_val, new_val in good_changes:\n            _LOG.info(""Safe change in %s from %r to %r"", _readable_offset(offset), old_val, new_val)\n\n        for offset, old_val, new_val in bad_changes:\n            _LOG.info(""Unsafe change in %s from %r to %r"", _readable_offset(offset), old_val, new_val)\n\n        return allow_unsafe_updates or not bad_changes, good_changes, bad_changes\n\n    def update(self, metadata_type, allow_unsafe_updates=False, allow_table_lock=False):\n        """"""\n        Update a metadata type from the document. Unsafe changes will throw a ValueError by default.\n\n        Safe updates currently allow new search fields to be added, description to be changed.\n\n        :param datacube.model.MetadataType metadata_type: updated MetadataType\n        :param bool allow_unsafe_updates: Allow unsafe changes. Use with caution.\n        :param allow_table_lock:\n            Allow an exclusive lock to be taken on the table while creating the indexes.\n            This will halt other user\'s requests until completed.\n\n            If false, creation will be slower and cannot be done in a transaction.\n        :rtype: datacube.model.MetadataType\n        """"""\n        can_update, safe_changes, unsafe_changes = self.can_update(metadata_type, allow_unsafe_updates)\n\n        if not safe_changes and not unsafe_changes:\n            _LOG.info(""No changes detected for metadata type %s"", metadata_type.name)\n            return self.get_by_name(metadata_type.name)\n\n        if not can_update:\n            full_message = ""Unsafe changes at "" + "", "".join(""."".join(map(str, offset))\n                                                            for offset, _, _ in unsafe_changes)\n            raise ValueError(full_message)\n\n        _LOG.info(""Updating metadata type %s"", metadata_type.name)\n\n        with self._db.connect() as connection:\n            connection.update_metadata_type(\n                name=metadata_type.name,\n                definition=metadata_type.definition,\n                concurrently=not allow_table_lock\n            )\n\n        self.get_by_name_unsafe.cache_clear()\n        self.get_unsafe.cache_clear()\n        return self.get_by_name(metadata_type.name)\n\n    def update_document(self, definition, allow_unsafe_updates=False):\n        """"""\n        Update a metadata type from the document. Unsafe changes will throw a ValueError by default.\n\n        Safe updates currently allow new search fields to be added, description to be changed.\n\n        :param dict definition: Updated definition\n        :param bool allow_unsafe_updates: Allow unsafe changes. Use with caution.\n        :rtype: datacube.model.MetadataType\n        """"""\n        return self.update(self.from_doc(definition), allow_unsafe_updates=allow_unsafe_updates)\n\n    def get(self, id_):\n        """"""\n        :rtype: datacube.model.MetadataType\n        """"""\n        try:\n            return self.get_unsafe(id_)\n        except KeyError:\n            return None\n\n    def get_by_name(self, name):\n        """"""\n        :rtype: datacube.model.MetadataType\n        """"""\n        try:\n            return self.get_by_name_unsafe(name)\n        except KeyError:\n            return None\n\n    # This is memoized in the constructor\n    # pylint: disable=method-hidden\n    def get_unsafe(self, id_):  # type: ignore\n        with self._db.connect() as connection:\n            record = connection.get_metadata_type(id_)\n        if record is None:\n            raise KeyError(\'%s is not a valid MetadataType id\')\n        return self._make_from_query_row(record)\n\n    # This is memoized in the constructor\n    # pylint: disable=method-hidden\n    def get_by_name_unsafe(self, name):  # type: ignore\n        with self._db.connect() as connection:\n            record = connection.get_metadata_type_by_name(name)\n        if not record:\n            raise KeyError(\'%s is not a valid MetadataType name\' % name)\n        return self._make_from_query_row(record)\n\n    def check_field_indexes(self, allow_table_lock=False, rebuild_all=None,\n                            rebuild_views=False, rebuild_indexes=False):\n        """"""\n        Create or replace per-field indexes and views.\n        :param allow_table_lock:\n            Allow an exclusive lock to be taken on the table while creating the indexes.\n            This will halt other user\'s requests until completed.\n\n            If false, creation will be slightly slower and cannot be done in a transaction.\n        """"""\n        if rebuild_all is not None:\n            warnings.warn(\n                ""The rebuild_all option of check_field_indexes() is deprecated."",\n                ""Instead, use rebuild_views=True or rebuild_indexes=True as needed."",\n                DeprecationWarning)\n            rebuild_views = rebuild_indexes = rebuild_all\n\n        with self._db.connect() as connection:\n            connection.check_dynamic_fields(\n                concurrently=not allow_table_lock,\n                rebuild_indexes=rebuild_indexes,\n                rebuild_views=rebuild_views,\n            )\n\n    def get_all(self):\n        """"""\n        Retrieve all Metadata Types\n\n        :rtype: iter[datacube.model.MetadataType]\n        """"""\n        with self._db.connect() as connection:\n            return self._make_many(connection.get_all_metadata_types())\n\n    def _make_many(self, query_rows):\n        """"""\n        :rtype: list[datacube.model.MetadataType]\n        """"""\n        return (self._make_from_query_row(c) for c in query_rows)\n\n    def _make_from_query_row(self, query_row):\n        """"""\n        :rtype: datacube.model.MetadataType\n        """"""\n        return self._make(query_row[\'definition\'], query_row[\'id\'])\n\n    def _make(self, definition, id_=None):\n        """"""\n        :param dict definition:\n        :param int id_:\n        :rtype: datacube.model.MetadataType\n        """"""\n        return MetadataType(\n            definition,\n            dataset_search_fields=self._db.get_dataset_fields(definition),\n            id_=id_\n        )\n'"
datacube/index/_products.py,0,"b'# coding=utf-8\n\nimport logging\n\nfrom cachetools.func import lru_cache\n\nfrom datacube.index import fields\nfrom datacube.model import DatasetType\nfrom datacube.utils import InvalidDocException, jsonify_document, changes, _readable_offset\nfrom datacube.utils.changes import check_doc_unchanged, get_doc_changes\n\nfrom typing import Iterable\n\n_LOG = logging.getLogger(__name__)\n\n\nclass ProductResource(object):\n    """"""\n    :type _db: datacube.drivers.postgres._connections.PostgresDb\n    :type metadata_type_resource: datacube.index._metadata_types.MetadataTypeResource\n    """"""\n\n    def __init__(self, db, metadata_type_resource):\n        """"""\n        :type db: datacube.drivers.postgres._connections.PostgresDb\n        :type metadata_type_resource: datacube.index._metadata_types.MetadataTypeResource\n        """"""\n        self._db = db\n        self.metadata_type_resource = metadata_type_resource\n\n        self.get_unsafe = lru_cache()(self.get_unsafe)\n        self.get_by_name_unsafe = lru_cache()(self.get_by_name_unsafe)\n\n    def __getstate__(self):\n        """"""\n        We define getstate/setstate to avoid pickling the caches\n        """"""\n        return self._db, self.metadata_type_resource\n\n    def __setstate__(self, state):\n        """"""\n        We define getstate/setstate to avoid pickling the caches\n        """"""\n        self.__init__(*state)\n\n    def from_doc(self, definition):\n        """"""\n        Create a Product from its definitions\n\n        :param dict definition: product definition document\n        :rtype: DatasetType\n        """"""\n        # This column duplication is getting out of hand:\n        DatasetType.validate(definition)\n\n        metadata_type = definition[\'metadata_type\']\n\n        # They either specified the name of a metadata type, or specified a metadata type.\n        # Is it a name?\n        if isinstance(metadata_type, str):\n            metadata_type = self.metadata_type_resource.get_by_name(metadata_type)\n        else:\n            # Otherwise they embedded a document, add it if needed:\n            metadata_type = self.metadata_type_resource.from_doc(metadata_type)\n            definition = definition.copy()\n            definition[\'metadata_type\'] = metadata_type.name\n\n        if not metadata_type:\n            raise InvalidDocException(\'Unknown metadata type: %r\' % definition[\'metadata_type\'])\n\n        return DatasetType(metadata_type, definition)\n\n    def add(self, product, allow_table_lock=False):\n        """"""\n        Add a Product.\n\n        :param allow_table_lock:\n            Allow an exclusive lock to be taken on the table while creating the indexes.\n            This will halt other user\'s requests until completed.\n\n            If false, creation will be slightly slower and cannot be done in a transaction.\n        :param DatasetType product: Product to add\n        :rtype: DatasetType\n        """"""\n        DatasetType.validate(product.definition)\n\n        existing = self.get_by_name(product.name)\n        if existing:\n            check_doc_unchanged(\n                existing.definition,\n                jsonify_document(product.definition),\n                \'Metadata Type {}\'.format(product.name)\n            )\n        else:\n            metadata_type = self.metadata_type_resource.get_by_name(product.metadata_type.name)\n            if metadata_type is None:\n                _LOG.warning(\'Adding metadata_type ""%s"" as it doesn\\\'t exist.\', product.metadata_type.name)\n                metadata_type = self.metadata_type_resource.add(product.metadata_type,\n                                                                allow_table_lock=allow_table_lock)\n            with self._db.connect() as connection:\n                connection.insert_product(\n                    name=product.name,\n                    metadata=product.metadata_doc,\n                    metadata_type_id=metadata_type.id,\n                    search_fields=metadata_type.dataset_fields,\n                    definition=product.definition,\n                    concurrently=not allow_table_lock,\n                )\n        return self.get_by_name(product.name)\n\n    def can_update(self, product, allow_unsafe_updates=False):\n        """"""\n        Check if product can be updated. Return bool,safe_changes,unsafe_changes\n\n        (An unsafe change is anything that may potentially make the product\n        incompatible with existing datasets of that type)\n\n        :param DatasetType product: Product to update\n        :param bool allow_unsafe_updates: Allow unsafe changes. Use with caution.\n        :rtype: bool,list[change],list[change]\n        """"""\n        DatasetType.validate(product.definition)\n\n        existing = self.get_by_name(product.name)\n        if not existing:\n            raise ValueError(\'Unknown product %s, cannot update \xe2\x80\x93 did you intend to add it?\' % product.name)\n\n        updates_allowed = {\n            (\'description\',): changes.allow_any,\n            (\'metadata_type\',): changes.allow_any,\n\n            # You can safely make the match rules looser but not tighter.\n            # Tightening them could exclude datasets already matched to the product.\n            # (which would make search results wrong)\n            (\'metadata\',): changes.allow_truncation,\n\n            # Some old storage fields should not be in the product definition any more: allow removal.\n            (\'storage\', \'chunking\'): changes.allow_removal,\n            (\'storage\', \'driver\'): changes.allow_removal,\n            (\'storage\', \'dimension_order\'): changes.allow_removal,\n        }\n\n        doc_changes = get_doc_changes(existing.definition, jsonify_document(product.definition))\n        good_changes, bad_changes = changes.classify_changes(doc_changes, updates_allowed)\n\n        for offset, old_val, new_val in good_changes:\n            _LOG.info(""Safe change in %s from %r to %r"", _readable_offset(offset), old_val, new_val)\n\n        for offset, old_val, new_val in bad_changes:\n            _LOG.info(""Unsafe change in %s from %r to %r"", _readable_offset(offset), old_val, new_val)\n\n        return allow_unsafe_updates or not bad_changes, good_changes, bad_changes\n\n    def update(self, product, allow_unsafe_updates=False, allow_table_lock=False):\n        """"""\n        Update a product. Unsafe changes will throw a ValueError by default.\n\n        (An unsafe change is anything that may potentially make the product\n        incompatible with existing datasets of that type)\n\n        :param DatasetType product: Product to update\n        :param bool allow_unsafe_updates: Allow unsafe changes. Use with caution.\n        :param allow_table_lock:\n            Allow an exclusive lock to be taken on the table while creating the indexes.\n            This will halt other user\'s requests until completed.\n\n            If false, creation will be slower and cannot be done in a transaction.\n        :rtype: DatasetType\n        """"""\n\n        can_update, safe_changes, unsafe_changes = self.can_update(product, allow_unsafe_updates)\n\n        if not safe_changes and not unsafe_changes:\n            _LOG.info(""No changes detected for product %s"", product.name)\n            return self.get_by_name(product.name)\n\n        if not can_update:\n            full_message = ""Unsafe changes at "" + (\n                "", "".join(\n                    _readable_offset(offset)\n                    for offset, _, _ in unsafe_changes\n                )\n            )\n            raise ValueError(full_message)\n\n        _LOG.info(""Updating product %s"", product.name)\n\n        existing = self.get_by_name(product.name)\n        changing_metadata_type = product.metadata_type.name != existing.metadata_type.name\n        if changing_metadata_type:\n            raise ValueError(""Unsafe change: cannot (currently) switch metadata types for a product"")\n            # TODO: Ask Jeremy WTF is going on here\n            # If the two metadata types declare the same field with different postgres expressions\n            # we can\'t safely change it.\n            # (Replacing the index would cause all existing users to have no effective index)\n            # for name, field in existing.metadata_type.dataset_fields.items():\n            #     new_field = type_.metadata_type.dataset_fields.get(name)\n            #     if new_field and (new_field.sql_expression != field.sql_expression):\n            #         declare_unsafe(\n            #             (\'metadata_type\',),\n            #             \'Metadata type change results in incompatible index \'\n            #             \'for {!r} ({!r} \xe2\x86\x92 {!r})\'.format(\n            #                 name, field.sql_expression, new_field.sql_expression\n            #             )\n            #         )\n        metadata_type = self.metadata_type_resource.get_by_name(product.metadata_type.name)\n        # TODO: should we add metadata type here?\n        assert metadata_type, ""TODO: should we add metadata type here?""\n        with self._db.connect() as conn:\n            conn.update_product(\n                name=product.name,\n                metadata=product.metadata_doc,\n                metadata_type_id=metadata_type.id,\n                search_fields=metadata_type.dataset_fields,\n                definition=product.definition,\n                update_metadata_type=changing_metadata_type,\n                concurrently=not allow_table_lock\n            )\n\n        self.get_by_name_unsafe.cache_clear()\n        self.get_unsafe.cache_clear()\n        return self.get_by_name(product.name)\n\n    def update_document(self, definition, allow_unsafe_updates=False, allow_table_lock=False):\n        """"""\n        Update a Product using its definition\n\n        :param bool allow_unsafe_updates: Allow unsafe changes. Use with caution.\n        :param dict definition: product definition document\n        :param allow_table_lock:\n            Allow an exclusive lock to be taken on the table while creating the indexes.\n            This will halt other user\'s requests until completed.\n\n            If false, creation will be slower and cannot be done in a transaction.\n        :rtype: DatasetType\n        """"""\n        type_ = self.from_doc(definition)\n        return self.update(\n            type_,\n            allow_unsafe_updates=allow_unsafe_updates,\n            allow_table_lock=allow_table_lock,\n        )\n\n    def add_document(self, definition):\n        """"""\n        Add a Product using its definition\n\n        :param dict definition: product definition document\n        :rtype: DatasetType\n        """"""\n        type_ = self.from_doc(definition)\n        return self.add(type_)\n\n    def get(self, id_):\n        """"""\n        Retrieve Product by id\n\n        :param int id_: id of the Product\n        :rtype: DatasetType\n        """"""\n        try:\n            return self.get_unsafe(id_)\n        except KeyError:\n            return None\n\n    def get_by_name(self, name):\n        """"""\n        Retrieve Product by name\n\n        :param str name: name of the Product\n        :rtype: DatasetType\n        """"""\n        try:\n            return self.get_by_name_unsafe(name)\n        except KeyError:\n            return None\n\n    # This is memoized in the constructor\n    # pylint: disable=method-hidden\n    def get_unsafe(self, id_):  # type: ignore\n        with self._db.connect() as connection:\n            result = connection.get_product(id_)\n        if not result:\n            raise KeyError(\'""%s"" is not a valid Product id\' % id_)\n        return self._make(result)\n\n    # This is memoized in the constructor\n    # pylint: disable=method-hidden\n    def get_by_name_unsafe(self, name):  # type: ignore\n        with self._db.connect() as connection:\n            result = connection.get_product_by_name(name)\n        if not result:\n            raise KeyError(\'""%s"" is not a valid Product name\' % name)\n        return self._make(result)\n\n    def get_with_fields(self, field_names):\n        """"""\n        Return dataset types that have all the given fields.\n\n        :param tuple[str] field_names:\n        :rtype: __generator[DatasetType]\n        """"""\n        for type_ in self.get_all():\n            for name in field_names:\n                if name not in type_.metadata_type.dataset_fields:\n                    break\n            else:\n                yield type_\n\n    def search(self, **query):\n        """"""\n        Return dataset types that have all the given fields.\n\n        :param dict query:\n        :rtype: __generator[DatasetType]\n        """"""\n        for type_, q in self.search_robust(**query):\n            if not q:\n                yield type_\n\n    def search_robust(self, **query):\n        """"""\n        Return dataset types that match match-able fields and dict of remaining un-matchable fields.\n\n        :param dict query:\n        :rtype: __generator[(DatasetType, dict)]\n        """"""\n\n        def _listify(v):\n            return v if isinstance(v, list) else [v]\n\n        for type_ in self.get_all():\n            remaining_matchable = query.copy()\n            # If they specified specific product/metadata-types, we can quickly skip non-matches.\n            if type_.name not in _listify(remaining_matchable.pop(\'product\', type_.name)):\n                continue\n            if type_.metadata_type.name not in _listify(remaining_matchable.pop(\'metadata_type\',\n                                                                                type_.metadata_type.name)):\n                continue\n\n            # Check that all the keys they specified match this product.\n            for key, value in list(remaining_matchable.items()):\n                field = type_.metadata_type.dataset_fields.get(key)\n                if not field:\n                    # This type doesn\'t have that field, so it cannot match.\n                    break\n                if not hasattr(field, \'extract\'):\n                    # non-document/native field\n                    continue\n                if field.extract(type_.metadata_doc) is None:\n                    # It has this field but it\'s not defined in the type doc, so it\'s unmatchable.\n                    continue\n\n                expr = fields.as_expression(field, value)\n                if expr.evaluate(type_.metadata_doc):\n                    remaining_matchable.pop(key)\n                else:\n                    # A property doesn\'t match this type, skip to next type.\n                    break\n\n            else:\n                yield type_, remaining_matchable\n\n    def get_all(self) -> Iterable[DatasetType]:\n        """"""\n        Retrieve all Products\n        """"""\n        with self._db.connect() as connection:\n            return (self._make(record) for record in connection.get_all_products())\n\n    def _make_many(self, query_rows):\n        return (self._make(c) for c in query_rows)\n\n    def _make(self, query_row) -> DatasetType:\n        return DatasetType(\n            definition=query_row[\'definition\'],\n            metadata_type=self.metadata_type_resource.get(query_row[\'metadata_type_ref\']),\n            id_=query_row[\'id\'],\n        )\n'"
datacube/index/_users.py,0,"b'# coding=utf-8\n\n\nclass UserResource(object):\n    def __init__(self, db):\n        """"""\n        :type db: datacube.drivers.postgres._connections.PostgresDb\n        """"""\n        self._db = db\n\n    def grant_role(self, role, *usernames):\n        """"""\n        Grant a role to users\n        """"""\n        with self._db.connect() as connection:\n            connection.grant_role(role, usernames)\n\n    def create_user(self, username, password, role, description=None):\n        """"""\n        Create a new user.\n        """"""\n        with self._db.connect() as connection:\n            connection.create_user(username, password, role, description=description)\n\n    def delete_user(self, *usernames):\n        """"""\n        Delete a user\n        """"""\n        with self._db.connect() as connection:\n            connection.drop_users(usernames)\n\n    def list_users(self):\n        """"""\n        :return: list of (role, user, description)\n        :rtype: list[(str, str, str)]\n        """"""\n        with self._db.connect() as connection:\n            for role, user, description in connection.list_users():\n                yield role, user, description\n'"
datacube/index/eo3.py,0,"b'"""""" Tools for working with EO3 metadata\n""""""\nimport collections.abc\nfrom types import SimpleNamespace\nfrom affine import Affine\nimport toolz\nfrom typing import Dict, Any, Optional\n\nfrom datacube.utils.geometry import (\n    SomeCRS,\n    CRS,\n    Geometry,\n    polygon,\n    bbox_union,\n    CoordList,\n    BoundingBox,\n    lonlat_bounds,\n)\n\nEO3_SCHEMA = ""https://schemas.opendatacube.org/dataset""\n\n\ndef _norm_grid(grid: Dict[str, Any]) -> Any:\n    shape = grid.get(\'shape\')\n    transform = grid.get(\'transform\')\n    if shape is None or transform is None:\n        raise ValueError(""Each grid must have .shape and .transform"")\n    return SimpleNamespace(shape=shape,\n                           transform=Affine(*transform[:6]))\n\n\ndef grid2points(grid: Dict[str, Any],\n                ring: bool = False) -> CoordList:\n    grid = _norm_grid(grid)\n\n    ny, nx = (float(dim) for dim in grid.shape)\n    transform = grid.transform\n    pts = [(0.0, 0.0), (nx, 0.0), (nx, ny), (0.0, ny)]\n    if ring:\n        pts += pts[:1]\n    return [transform*pt for pt in pts]\n\n\ndef grid2ref_points(grid: Dict[str, Any]) -> Dict[str, Any]:\n    nn = [\'ul\', \'ur\', \'lr\', \'ll\']\n    return {n: dict(x=x, y=y)\n            for n, (x, y) in zip(nn, grid2points(grid))}\n\n\ndef grid2polygon(grid: Dict[str, Any], crs: SomeCRS) -> Geometry:\n    return polygon(grid2points(grid, ring=True), crs)\n\n\ndef eo3_lonlat_bbox(doc: Dict[str, Any],\n                    resolution: Optional[float] = None) -> BoundingBox:\n    """""" Compute bounding box in Lon/Lat for a given EO3 document.\n    """"""\n    crs = doc.get(\'crs\')\n    grids = doc.get(\'grids\')\n\n    if crs is None or grids is None:\n        raise ValueError(""Input must have crs and grids"")\n\n    crs = CRS(crs)\n    geom = doc.get(\'geometry\', None)\n    if geom is not None:\n        geom = Geometry(geom, crs)\n        return lonlat_bounds(geom, resolution=resolution)\n\n    bounds = [lonlat_bounds(grid2polygon(grid, crs), resolution=resolution)\n              for grid in grids.values()]\n\n    return bbox_union(bounds)\n\n\ndef eo3_grid_spatial(doc: Dict[str, Any],\n                     resolution: Optional[float] = None) -> Dict[str, Any]:\n    """"""Using doc[grids|crs|geometry] compute EO3 style grid spatial:\n\n    Note that `geo_ref_points` are set to the 4 corners of the default grid\n    only, while lon/lat bounds are computed using all the grids, unless tighter\n    valid region is defined via `geometry` key, in which case it is used to\n    determine lon/lat bounds instead.\n\n    inputs:\n    ```\n    crs: ""<:str>""\n    geometry: <:GeoJSON object>  # optional\n    grids:\n       default:\n          shape: [ny: int, nx: int]\n          transform: [a0, a1, a2, a3, a4, a5, 0, 0, 1]\n       <...> # optionally more grids\n    ```\n\n    Where transform is a linear mapping matrix from pixel space to projected\n    space encoded in row-major order:\n\n       [X]   [a0, a1, a2] [ Pixel]\n       [Y] = [a3, a4, a5] [ Line ]\n       [1]   [ 0,  0,  1] [  1   ]\n\n    outputs:\n    ```\n      extent:\n        lat: {begin=<>, end=<>}\n        lon: {begin=<>, end=<>}\n\n      grid_spatial:\n        projection:\n          spatial_reference: ""<crs>""\n          geo_ref_points: {ll: {x:<>, y:<>}, ...}\n          valid_data: {...}\n    ```\n\n    """"""\n    grid = toolz.get_in([\'grids\', \'default\'], doc, None)\n    crs = doc.get(\'crs\', None)\n    if crs is None or grid is None:\n        raise ValueError(""Input must have crs and grids.default"")\n\n    geometry = doc.get(\'geometry\')\n\n    if geometry is not None:\n        valid_data = dict(valid_data=geometry)\n    else:\n        valid_data = {}\n\n    oo = dict(grid_spatial=dict(projection={\n        \'spatial_reference\': crs,\n        \'geo_ref_points\': grid2ref_points(grid),\n        **valid_data,\n    }))\n\n    x1, y1, x2, y2 = eo3_lonlat_bbox(doc, resolution=resolution)\n    oo[\'extent\'] = dict(lon=dict(begin=x1, end=x2),\n                        lat=dict(begin=y1, end=y2))\n    return oo\n\n\ndef add_eo3_parts(doc: Dict[str, Any],\n                  resolution: Optional[float] = None) -> Dict[str, Any]:\n    """"""Add spatial keys the DB requires to eo3 metadata\n    """"""\n    return dict(**doc,\n                **eo3_grid_spatial(doc, resolution=resolution))\n\n\ndef is_doc_eo3(doc: Dict[str, Any]) -> bool:\n    """""" Is this document eo3?\n\n    :param doc: Parsed ODC Dataset metadata document\n\n    :returns:\n        False if this document is a legacy dataset\n        True if this document is eo3\n\n    :raises ValueError: For an unsupported document\n    """"""\n    schema = doc.get(\'$schema\')\n    # All legacy documents had no schema at all.\n    if schema is None:\n        return False\n\n    if schema == EO3_SCHEMA:\n        return True\n\n    # Otherwise it has an unknown schema.\n    #\n    # Reject it for now.\n    # We don\'t want future documents (like Stac items, or ""eo4"") to be quietly\n    # accepted as legacy eo.\n    raise ValueError(f\'Unsupported dataset schema: {schema!r}\')\n\n\ndef prep_eo3(doc: Dict[str, Any],\n             auto_skip: bool = False,\n             resolution: Optional[float] = None) -> Dict[str, Any]:\n    """""" Modify spatial and lineage sections of eo3 metadata\n    :param doc: input document\n    :param auto_skip: If true check if dataset is EO3 and if not\n                      silently return input dataset without modifications\n    """"""\n    if doc is None:\n        return None\n\n    if auto_skip:\n        if not is_doc_eo3(doc):\n            return doc\n\n    doc = add_eo3_parts(doc, resolution=resolution)\n    lineage = doc.pop(\'lineage\', {})\n\n    def remap_lineage(name, uuids) -> Dict[str, Any]:\n        """""" Turn name, [uuid] -> {name: {id: uuid}}\n        """"""\n        if len(uuids) == 0:\n            return {}\n        if len(uuids) == 1:\n            return {name: {\'id\': uuids[0]}}\n\n        out = {}\n        for idx, uuid in enumerate(uuids, start=1):\n            out[name+str(idx)] = {\'id\': uuid}\n        return out\n\n    sources = {}\n    for name, uuids in lineage.items():\n        sources.update(remap_lineage(name, uuids))\n\n    doc[\'lineage\'] = dict(source_datasets=sources)\n    return doc\n'"
datacube/index/exceptions.py,0,b'\n\nclass DuplicateRecordError(Exception):\n    pass\n\n\nclass MissingRecordError(Exception):\n    pass\n\n\nclass IndexSetupError(Exception):\n    pass\n'
datacube/index/fields.py,0,"b'# coding=utf-8\n""""""\nCommon datatypes for DB drivers.\n""""""\n\nfrom datetime import date, datetime, time\nfrom dateutil.tz import tz\nfrom typing import List\n\nfrom datacube.model import Range\nfrom datacube.model.fields import Expression, Field\n\n__all__ = [\'Field\',\n           \'Expression\',\n           \'OrExpression\',\n           \'UnknownFieldError\',\n           \'to_expressions\',\n           \'as_expression\']\n\n\nclass UnknownFieldError(Exception):\n    pass\n\n\nclass OrExpression(Expression):\n    def __init__(self, *exprs):\n        super(OrExpression, self).__init__()\n        self.exprs = exprs\n\n    def evaluate(self, ctx):\n        return any(expr.evaluate(ctx) for expr in self.exprs)\n\n\ndef as_expression(field: Field, value) -> Expression:\n    """"""\n    Convert a single field/value to expression, following the ""simple"" convensions.\n    """"""\n    if isinstance(value, Range):\n        return field.between(value.begin, value.end)\n    elif isinstance(value, list):\n        return OrExpression(*(as_expression(field, val) for val in value))\n    # Treat a date (day) as a time range.\n    elif isinstance(value, date) and not isinstance(value, datetime):\n        return as_expression(\n            field,\n            Range(\n                datetime.combine(value, time.min.replace(tzinfo=tz.tzutc())),\n                datetime.combine(value, time.max.replace(tzinfo=tz.tzutc()))\n            )\n        )\n    return field == value\n\n\ndef _to_expression(get_field, name: str, value) -> Expression:\n    field = get_field(name)\n    if field is None:\n        raise UnknownFieldError(\'Unknown field %r\' % name)\n\n    return as_expression(field, value)\n\n\ndef to_expressions(get_field, **query) -> List[Expression]:\n    """"""\n    Convert a simple query (dict of param names and values) to expression objects.\n    :type get_field: (str) -> Field\n    :type query: dict[str,str|float|datacube.model.Range]\n    """"""\n    return [_to_expression(get_field, name, value) for name, value in query.items()]\n'"
datacube/index/hl.py,0,"b'""""""\nHigh level indexing operations/utilities\n""""""\nimport json\nimport toolz\nfrom types import SimpleNamespace\n\nfrom datacube.model import Dataset\nfrom datacube.utils import changes, InvalidDocException, SimpleDocNav, jsonify_document\nfrom datacube.model.utils import dedup_lineage, remap_lineage_doc, flatten_datasets\nfrom datacube.utils.changes import get_doc_changes\nfrom .eo3 import prep_eo3, is_doc_eo3\n\n\nclass BadMatch(Exception):\n    pass\n\n\ndef load_rules_from_types(index, product_names=None, excluding=None):\n    products = []\n    if product_names:\n        for name in product_names:\n            product = index.products.get_by_name(name)\n            if not product:\n                return None, \'Supplied product name ""%s"" not present in the database\' % name\n            products.append(product)\n    else:\n        products += index.products.get_all()\n\n    if excluding is not None:\n        excluding = set(excluding)\n        products = [p for p in products if p.name not in excluding]\n\n    if len(products) == 0:\n        return None, \'Found no products in the database\'\n\n    return [SimpleNamespace(product=p, signature=p.metadata_doc) for p in products], None\n\n\ndef product_matcher(rules):\n    """"""Given product matching rules return a function mapping a document to a\n    matching product.\n\n    """"""\n    assert len(rules) > 0\n\n    def matches(doc, rule):\n        return changes.contains(doc, rule.signature)\n\n    def single_product_matcher(rule):\n        def match(doc):\n            if matches(doc, rule):\n                return rule.product\n\n            relevant_doc = {k: v for k, v in doc.items() if k in rule.signature}\n            raise BadMatch(\'Dataset metadata did not match product signature.\'\n                           \'\\nDataset definition:\\n %s\\n\'\n                           \'\\nProduct signature:\\n %s\\n\'\n                           % (json.dumps(relevant_doc, indent=4),\n                              json.dumps(rule.signature, indent=4)))\n\n        return match\n\n    if len(rules) == 1:\n        return single_product_matcher(rules[0])\n\n    def match(doc):\n        matched = [rule.product for rule in rules if changes.contains(doc, rule.signature)]\n\n        if len(matched) == 1:\n            return matched[0]\n\n        doc_id = doc.get(\'id\', \'<missing id>\')\n\n        if len(matched) == 0:\n            raise BadMatch(\'No matching Product found for dataset %s\' % doc_id)\n        else:\n            raise BadMatch(\'Auto match failed, dataset %s matches several products:\\n  %s\' % (\n                doc_id,\n                \',\'.join(p.name for p in matched)))\n\n    return match\n\n\ndef check_dataset_consistent(dataset):\n    """"""\n    :type dataset: datacube.model.Dataset\n    :return: (Is consistent, [error message|None])\n    :rtype: (bool, str or None)\n    """"""\n    product_measurements = set(dataset.type.measurements.keys())\n\n    if len(product_measurements) == 0:\n        return True, None\n\n    if dataset.measurements is None:\n        return False, ""No measurements defined for a dataset""\n\n    # It the type expects measurements, ensure our dataset contains them all.\n    if not product_measurements.issubset(dataset.measurements.keys()):\n        not_measured = str(product_measurements - set(dataset.measurements.keys()))\n        msg = ""The dataset is not specifying all of the measurements in this product.\\n""\n        msg += ""Missing fields are;\\n"" + not_measured\n        return False, msg\n\n    return True, None\n\n\ndef check_consistent(a, b):\n    diffs = get_doc_changes(a, b)\n    if len(diffs) == 0:\n        return True, None\n\n    def render_diff(offset, a, b):\n        offset = \'.\'.join(map(str, offset))\n        return \'{}: {!r}!={!r}\'.format(offset, a, b)\n\n    return False, "", "".join([render_diff(offset, a, b) for offset, a, b in diffs])\n\n\ndef dataset_resolver(index,\n                     product_matching_rules,\n                     fail_on_missing_lineage=False,\n                     verify_lineage=True,\n                     skip_lineage=False):\n    match_product = product_matcher(product_matching_rules)\n\n    def resolve_no_lineage(ds, uri):\n        doc = ds.doc_without_lineage_sources\n        try:\n            product = match_product(doc)\n        except BadMatch as e:\n            return None, e\n\n        return Dataset(product, doc, uris=[uri], sources={}), None\n\n    def resolve(main_ds, uri):\n        try:\n            main_ds = SimpleDocNav(dedup_lineage(main_ds))\n        except InvalidDocException as e:\n            return None, e\n\n        main_uuid = main_ds.id\n\n        ds_by_uuid = toolz.valmap(toolz.first, flatten_datasets(main_ds))\n        all_uuid = list(ds_by_uuid)\n        db_dss = {str(ds.id): ds for ds in index.datasets.bulk_get(all_uuid)}\n\n        lineage_uuids = set(filter(lambda x: x != main_uuid, all_uuid))\n        missing_lineage = lineage_uuids - set(db_dss)\n\n        if missing_lineage and fail_on_missing_lineage:\n            return None, ""Following lineage datasets are missing from DB: %s"" % (\',\'.join(missing_lineage))\n\n        if verify_lineage and not is_doc_eo3(main_ds.doc):\n            bad_lineage = []\n\n            for uuid in lineage_uuids:\n                if uuid in db_dss:\n                    ok, err = check_consistent(jsonify_document(ds_by_uuid[uuid].doc_without_lineage_sources),\n                                               db_dss[uuid].metadata_doc)\n                    if not ok:\n                        bad_lineage.append((uuid, err))\n\n            if len(bad_lineage) > 0:\n                error_report = \'\\n\'.join(\'Inconsistent lineage dataset {}:\\n> {}\'.format(uuid, err)\n                                         for uuid, err in bad_lineage)\n                return None, error_report\n\n        def with_cache(v, k, cache):\n            cache[k] = v\n            return v\n\n        def resolve_ds(ds, sources, cache=None):\n            cached = cache.get(ds.id)\n            if cached is not None:\n                return cached\n\n            uris = [uri] if ds.id == main_uuid else []\n\n            doc = ds.doc\n\n            db_ds = db_dss.get(ds.id)\n            if db_ds:\n                product = db_ds.type\n            else:\n                product = match_product(doc)\n\n            return with_cache(Dataset(product, doc, uris=uris, sources=sources), ds.id, cache)\n\n        try:\n            return remap_lineage_doc(main_ds, resolve_ds, cache={}), None\n        except BadMatch as e:\n            return None, e\n\n    return resolve_no_lineage if skip_lineage else resolve\n\n\nclass Doc2Dataset:\n    """"""Used for constructing `Dataset` objects from plain metadata documents.\n\n    This requires a database connection to perform the automatic matching against\n    available products.\n\n    There are options for including and excluding the products to match against,\n    as well as how to deal with source lineage.\n\n    Once constructed, call with a dictionary object and location URI, eg::\n\n        resolver = Doc2Dataset(index)\n        dataset = resolver(dataset_dictionary, \'file:///tmp/test-dataset.json\')\n        index.dataset.add(dataset)\n\n\n    :param index: an open Database connection\n\n    :param list products: List of product names against which to match datasets\n                          (including lineage datasets). If not supplied we will\n                          consider all products.\n\n    :param list exclude_products: List of products to exclude from matching\n\n    :param fail_on_missing_lineage: If True fail resolve if any lineage\n                                    datasets are missing from the DB\n\n    :param verify_lineage: If True check that lineage datasets in the\n                           supplied document are identical to DB versions\n\n    :param skip_lineage: If True ignore lineage sub-tree in the supplied\n                         document and construct dataset without lineage datasets\n    :param eo3: \'auto\'/True/False by default auto-detect EO3 datasets and pre-process them\n    """"""\n    def __init__(self,\n                 index,\n                 products=None,\n                 exclude_products=None,\n                 fail_on_missing_lineage=False,\n                 verify_lineage=True,\n                 skip_lineage=False,\n                 eo3=\'auto\'):\n        rules, err_msg = load_rules_from_types(index,\n                                               product_names=products,\n                                               excluding=exclude_products)\n        if rules is None:\n            raise ValueError(err_msg)\n\n        self._eo3 = eo3\n        self._ds_resolve = dataset_resolver(index,\n                                            rules,\n                                            fail_on_missing_lineage=fail_on_missing_lineage,\n                                            verify_lineage=verify_lineage,\n                                            skip_lineage=skip_lineage)\n\n    def __call__(self, doc, uri):\n        """"""Attempt to construct dataset from metadata document and a uri.\n\n        :param doc: Dictionary or SimpleDocNav object\n        :param uri: String ""location"" property of the Dataset\n\n        :return: (dataset, None) is successful,\n        :return: (None, ErrorMessage) on failure\n        """"""\n        if not isinstance(doc, SimpleDocNav):\n            doc = SimpleDocNav(doc)\n\n        if self._eo3:\n            auto_skip = self._eo3 == \'auto\'\n            doc = SimpleDocNav(prep_eo3(doc.doc, auto_skip=auto_skip))\n\n        dataset, err = self._ds_resolve(doc, uri)\n        if dataset is None:\n            return None, err\n\n        is_consistent, reason = check_dataset_consistent(dataset)\n        if not is_consistent:\n            return None, reason\n\n        return dataset, None\n'"
datacube/index/index.py,0,"b'import logging\n\nfrom datacube.drivers.postgres import PostgresDb\nfrom datacube.index._datasets import DatasetResource  # type: ignore\nfrom datacube.index._metadata_types import MetadataTypeResource, default_metadata_type_docs\nfrom datacube.index._products import ProductResource\nfrom datacube.index._users import UserResource\nfrom datacube.model import MetadataType\n\n_LOG = logging.getLogger(__name__)\n\n\nclass Index(object):\n    """"""\n    Access to the datacube index.\n\n    DON\'T INITIALISE THIS DIRECTLY (it will break in the future). Use `datacube.index.index_connect()` or\n    access property ``.index`` on your existing :class:`datacube.api.core.Datacube`.\n\n    These are thread safe. But not multiprocess safe once a connection is made (db connections cannot be shared\n    between processes) You can close idle connections before forking by calling close(), provided you know no\n    other connections are active. Or else use a separate instance of this class in each process.\n\n    :ivar datacube.index._datasets.DatasetResource datasets: store and retrieve :class:`datacube.model.Dataset`\n    :ivar datacube.index._products.ProductResource products: store and retrieve :class:`datacube.model.DatasetType`\\\n    (should really be called Product)\n    :ivar datacube.index._metadata_types.MetadataTypeResource metadata_types: store and retrieve \\\n    :class:`datacube.model.MetadataType`\n    :ivar UserResource users: user management\n\n    :type users: datacube.index._users.UserResource\n    :type datasets: datacube.index._datasets.DatasetResource\n    :type products: datacube.index._products.ProductResource\n    :type metadata_types: datacube.index._metadata_types.MetadataTypeResource\n    """"""\n\n    def __init__(self, db: PostgresDb) -> None:\n        self._db = db\n\n        self.users = UserResource(db)\n        self.metadata_types = MetadataTypeResource(db)\n        self.products = ProductResource(db, self.metadata_types)\n        self.datasets = DatasetResource(db, self.products)\n\n    @property\n    def url(self) -> str:\n        return self._db.url\n\n    @classmethod\n    def from_config(cls, config, application_name=None, validate_connection=True):\n        db = PostgresDb.from_config(config, application_name=application_name,\n                                    validate_connection=validate_connection)\n        return cls(db)\n\n    @classmethod\n    def get_dataset_fields(cls, doc):\n        return PostgresDb.get_dataset_fields(doc)\n\n    def init_db(self, with_default_types=True, with_permissions=True):\n        is_new = self._db.init(with_permissions=with_permissions)\n\n        if is_new and with_default_types:\n            _LOG.info(\'Adding default metadata types.\')\n            for doc in default_metadata_type_docs():\n                self.metadata_types.add(self.metadata_types.from_doc(doc), allow_table_lock=True)\n\n        return is_new\n\n    def close(self):\n        """"""\n        Close any idle connections database connections.\n\n        This is good practice if you are keeping the Index instance in scope\n        but wont be using it for a while.\n\n        (Connections are normally closed automatically when this object is deleted: ie. no references exist)\n        """"""\n        self._db.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type_, value, traceback):\n        self.close()\n\n    def __repr__(self):\n        return ""Index<db={!r}>"".format(self._db)\n\n\nclass DefaultIndexDriver(object):\n    @staticmethod\n    def connect_to_index(config, application_name=None, validate_connection=True):\n        return Index.from_config(config, application_name, validate_connection)\n\n    @staticmethod\n    def metadata_type_from_doc(definition: dict) -> MetadataType:\n        """"""\n        :param definition:\n        """"""\n        MetadataType.validate(definition)  # type: ignore\n        return MetadataType(definition,\n                            dataset_search_fields=Index.get_dataset_fields(definition))\n\n\ndef index_driver_init():\n    return DefaultIndexDriver()\n'"
datacube/model/__init__.py,0,"b'# coding=utf-8\n""""""\nCore classes used across modules.\n""""""\nimport logging\nimport math\nimport warnings\nfrom collections import OrderedDict\nfrom datetime import datetime\nfrom pathlib import Path\nfrom uuid import UUID\n\nfrom affine import Affine\nfrom typing import Optional, List, Mapping, Any, Dict, Tuple, Iterator\n\nfrom urllib.parse import urlparse\nfrom datacube.utils import geometry, without_lineage_sources, parse_time, cached_property, uri_to_local_path, \\\n    schema_validated, DocReader\nfrom .fields import Field, get_dataset_fields\nfrom ._base import Range\n\n_LOG = logging.getLogger(__name__)\n\nDEFAULT_SPATIAL_DIMS = (\'y\', \'x\')  # Used when product lacks grid_spec\n\nSCHEMA_PATH = Path(__file__).parent / \'schema\'\n\n\nclass Dataset:\n    """"""\n    A Dataset. A container of metadata, and refers typically to a multi-band raster on disk.\n\n    Most important parts are the metadata_doc and uri.\n\n    :param metadata_doc: the document (typically a parsed json/yaml)\n    :param uris: All active uris for the dataset\n    """"""\n\n    def __init__(self,\n                 type_: \'DatasetType\',\n                 metadata_doc: Dict[str, Any],\n                 uris: Optional[List[str]] = None,\n                 sources: Optional[Mapping[str, \'Dataset\']] = None,\n                 indexed_by: Optional[str] = None,\n                 indexed_time: Optional[datetime] = None,\n                 archived_time: Optional[datetime] = None):\n        assert isinstance(type_, DatasetType)\n\n        self.type = type_\n\n        #: The document describing the dataset as a dictionary. It is often serialised as YAML on disk\n        #: or inside a NetCDF file, and as JSON-B inside the database index.\n        self.metadata_doc = metadata_doc\n\n        #: Active URIs in order from newest to oldest\n        self.uris = uris\n\n        #: The datasets that this dataset is derived from (if requested on load).\n        self.sources = sources\n\n        if self.sources is not None:\n            assert set(self.metadata.sources.keys()) == set(self.sources.keys())\n\n        #: The User who indexed this dataset\n        self.indexed_by = indexed_by\n        self.indexed_time = indexed_time\n        # When the dataset was archived. Null it not archived.\n        self.archived_time = archived_time\n\n    @property\n    def metadata_type(self) -> \'MetadataType\':\n        return self.type.metadata_type\n\n    @property\n    def local_uri(self) -> Optional[str]:\n        """"""\n        The latest local file uri, if any.\n        """"""\n        if self.uris is None:\n            return None\n\n        local_uris = [uri for uri in self.uris if uri.startswith(\'file:\')]\n        if local_uris:\n            return local_uris[0]\n\n        return None\n\n    @property\n    def local_path(self) -> Optional[Path]:\n        """"""\n        A path to this dataset on the local filesystem (if available).\n        """"""\n        return uri_to_local_path(self.local_uri)\n\n    @property\n    def id(self) -> UUID:\n        """""" UUID of a dataset\n        """"""\n        # This is a string in a raw document.\n        return UUID(self.metadata.id)\n\n    @property\n    def managed(self) -> bool:\n        return self.type.managed\n\n    @property\n    def format(self) -> str:\n        return self.metadata.format\n\n    @property\n    def uri_scheme(self) -> str:\n        if self.uris is None or len(self.uris) == 0:\n            return \'\'\n\n        url = urlparse(self.uris[0])\n        if url.scheme == \'\':\n            return \'file\'\n        return url.scheme\n\n    @property\n    def measurements(self) -> Dict[str, Any]:\n        # It\'s an optional field in documents.\n        # Dictionary of key -> measurement descriptor\n        if not hasattr(self.metadata, \'measurements\'):\n            return {}\n        return self.metadata.measurements\n\n    @cached_property\n    def center_time(self) -> Optional[datetime]:\n        """""" mid-point of time range\n        """"""\n        time = self.time\n        if time is None:\n            return None\n        return time.begin + (time.end - time.begin) // 2\n\n    @property\n    def time(self) -> Optional[Range]:\n        try:\n            time = self.metadata.time\n            return Range(parse_time(time.begin), parse_time(time.end))\n        except AttributeError:\n            return None\n\n    @cached_property\n    def key_time(self):\n        """"""\n        :rtype: datetime.datetime\n        """"""\n        if \'key_time\' in self.metadata.fields:\n            return self.metadata.key_time\n\n        # Existing datasets are already using the computed ""center_time"" for their storage index key\n        # if \'center_time\' in self.metadata.fields:\n        #     return self.metadata.center_time\n\n        return self.center_time\n\n    @property\n    def bounds(self) -> Optional[geometry.BoundingBox]:\n        """""" :returns: bounding box of the dataset in the native crs\n        """"""\n        gs = self._gs\n        if gs is None:\n            return None\n\n        bounds = gs[\'geo_ref_points\']\n        return geometry.BoundingBox(left=min(bounds[\'ur\'][\'x\'], bounds[\'ll\'][\'x\']),\n                                    right=max(bounds[\'ur\'][\'x\'], bounds[\'ll\'][\'x\']),\n                                    top=max(bounds[\'ur\'][\'y\'], bounds[\'ll\'][\'y\']),\n                                    bottom=min(bounds[\'ur\'][\'y\'], bounds[\'ll\'][\'y\']))\n\n    @property\n    def transform(self) -> Optional[Affine]:\n        geo = self._gs\n        if geo is None:\n            return None\n\n        bounds = geo.get(\'geo_ref_points\')\n        if bounds is None:\n            return None\n\n        return Affine(bounds[\'lr\'][\'x\'] - bounds[\'ul\'][\'x\'], 0, bounds[\'ul\'][\'x\'],\n                      0, bounds[\'lr\'][\'y\'] - bounds[\'ul\'][\'y\'], bounds[\'ul\'][\'y\'])\n\n    @property\n    def is_archived(self) -> bool:\n        """"""\n        Is this dataset archived?\n\n        (an archived dataset is one that is not intended to be used by users anymore: eg. it has been\n        replaced by another dataset. It will not show up in search results, but still exists in the\n        system via provenance chains or through id lookup.)\n\n        """"""\n        return self.archived_time is not None\n\n    @property\n    def is_active(self) -> bool:\n        """"""\n        Is this dataset active?\n\n        (ie. dataset hasn\'t been archived)\n\n        """"""\n        return not self.is_archived\n\n    @property\n    def _gs(self) -> Optional[Dict[str, Any]]:\n        try:\n            return self.metadata.grid_spatial\n        except AttributeError:\n            return None\n\n    @property\n    def crs(self) -> Optional[geometry.CRS]:\n        """""" Return CRS if available\n        """"""\n        projection = self._gs\n\n        if not projection:\n            return None\n\n        crs = projection.get(\'spatial_reference\', None)\n        if crs:\n            return geometry.CRS(str(crs))\n\n        # Try to infer CRS\n        zone_ = projection.get(\'zone\')\n        datum_ = projection.get(\'datum\')\n        if zone_ and datum_:\n            warnings.warn(""Using zone/datum to specify CRS is deprecated"",\n                          category=DeprecationWarning)\n            try:\n                # TODO: really need CRS specified properly in agdc-metadata.yaml\n                if datum_ == \'GDA94\':\n                    return geometry.CRS(\'EPSG:283\' + str(abs(zone_)))\n                if datum_ == \'WGS84\':\n                    if zone_[-1] == \'S\':\n                        return geometry.CRS(\'EPSG:327\' + str(abs(int(zone_[:-1]))))\n                    else:\n                        return geometry.CRS(\'EPSG:326\' + str(abs(int(zone_[:-1]))))\n            except geometry.CRSError:\n                # We still return None, as they didn\'t specify a CRS explicitly...\n                _LOG.warning(\n                    ""Can\'t figure out projection: possibly invalid zone (%r) for datum (%r)."", zone_, datum_\n                )\n\n        return None\n\n    @cached_property\n    def extent(self) -> Optional[geometry.Geometry]:\n        """""" :returns: valid extent of the dataset or None\n        """"""\n\n        def xytuple(obj):\n            return obj[\'x\'], obj[\'y\']\n\n        # If no projection or crs, they have no extent.\n        projection = self._gs\n        if not projection:\n            return None\n        crs = self.crs\n        if not crs:\n            _LOG.debug(""No CRS, assuming no extent (dataset %s)"", self.id)\n            return None\n\n        valid_data = projection.get(\'valid_data\')\n        geo_ref_points = projection.get(\'geo_ref_points\')\n        if valid_data:\n            return geometry.Geometry(valid_data, crs=crs)\n        elif geo_ref_points:\n            return geometry.polygon([xytuple(geo_ref_points[key]) for key in (\'ll\', \'ul\', \'ur\', \'lr\', \'ll\')],\n                                    crs=crs)\n\n        return None\n\n    def __eq__(self, other) -> bool:\n        if isinstance(other, Dataset):\n            return self.id == other.id\n        return False\n\n    def __hash__(self):\n        return hash(self.id)\n\n    def __str__(self):\n        str_loc = \'not available\' if not self.uris else self.uris[0]\n        return ""Dataset <id={id} product={type} location={loc}>"".format(id=self.id,\n                                                                        type=self.type.name,\n                                                                        loc=str_loc)\n\n    def __repr__(self) -> str:\n        return self.__str__()\n\n    @property\n    def metadata(self) -> DocReader:\n        return self.metadata_type.dataset_reader(self.metadata_doc)\n\n    def metadata_doc_without_lineage(self) -> Dict[str, Any]:\n        """""" Return metadata document without nested lineage datasets\n        """"""\n        return without_lineage_sources(self.metadata_doc, self.metadata_type)\n\n\nclass Measurement(dict):\n    """"""\n    Describes a single data variable of a Product or Dataset.\n\n    Must include, which can be used when loading and interpreting data:\n\n     - name\n     - dtype - eg: int8, int16, float32\n     - nodata - What value represent No Data\n     - units\n\n    Attributes can be accessed using ``dict []`` syntax.\n\n    Can also include attributes like alternative names \'aliases\', and spectral and bit flags\n    definitions to aid with interpreting the data.\n\n    """"""\n    REQUIRED_KEYS = (\'name\', \'dtype\', \'nodata\', \'units\')\n    OPTIONAL_KEYS = (\'aliases\', \'spectral_definition\', \'flags_definition\')\n    ATTR_BLACKLIST = set([\'name\', \'dtype\', \'aliases\', \'resampling_method\', \'fuser\'])\n\n    def __init__(self, **kwargs):\n        missing_keys = set(self.REQUIRED_KEYS) - set(kwargs)\n        if missing_keys:\n            raise ValueError(""Measurement required keys missing: {}"".format(missing_keys))\n\n        super().__init__(**kwargs)\n\n    def __getattr__(self, key: str) -> Any:\n        """""" Allow access to items as attributes. """"""\n        v = self.get(key, self)\n        if v is self:\n            raise AttributeError(""\'Measurement\' object has no attribute \'{}\'"".format(key))\n        return v\n\n    def __repr__(self) -> str:\n        return ""Measurement({})"".format(super(Measurement, self).__repr__())\n\n    def copy(self) -> \'Measurement\':\n        """"""Required as the super class `dict` method returns a `dict`\n           and does not preserve Measurement class""""""\n        return Measurement(**self)\n\n    def dataarray_attrs(self) -> Dict[str, Any]:\n        """"""This returns attributes filtered for display in a dataarray.""""""\n        return {key: value for key, value in self.items() if key not in self.ATTR_BLACKLIST}\n\n\n@schema_validated(SCHEMA_PATH / \'metadata-type-schema.yaml\')\nclass MetadataType:\n    """"""Metadata Type definition""""""\n\n    def __init__(self,\n                 definition: Mapping[str, Any],\n                 dataset_search_fields: Optional[Mapping[str, Field]] = None,\n                 id_: Optional[int] = None):\n        if dataset_search_fields is None:\n            dataset_search_fields = get_dataset_fields(definition)\n        self.definition = definition\n        self.dataset_fields = dataset_search_fields\n        self.id = id_\n\n    @property\n    def name(self) -> str:\n        return self.definition.get(\'name\', None)\n\n    @property\n    def description(self) -> str:\n        return self.definition.get(\'description\', None)\n\n    def dataset_reader(self, dataset_doc: Mapping[str, Field]) -> DocReader:\n        return DocReader(self.definition[\'dataset\'], self.dataset_fields, dataset_doc)\n\n    def __str__(self) -> str:\n        return ""MetadataType(name={name!r}, id_={id!r})"".format(id=self.id, name=self.name)\n\n    def __repr__(self) -> str:\n        return str(self)\n\n\n@schema_validated(SCHEMA_PATH / \'dataset-type-schema.yaml\')\nclass DatasetType:\n    """"""\n    Product definition\n\n    :param MetadataType metadata_type:\n    :param dict definition:\n    """"""\n\n    def __init__(self,\n                 metadata_type: MetadataType,\n                 definition: Mapping[str, Any],\n                 id_: Optional[int] = None):\n        assert isinstance(metadata_type, MetadataType)\n        self.id = id_\n        self.metadata_type = metadata_type\n        #: product definition document\n        self.definition = definition\n\n    @property\n    def name(self) -> str:\n        return self.definition[\'name\']\n\n    @property\n    def managed(self) -> bool:\n        return self.definition.get(\'managed\', False)\n\n    @property\n    def metadata_doc(self) -> Mapping[str, Any]:\n        return self.definition[\'metadata\']\n\n    @property\n    def metadata(self) -> DocReader:\n        return self.metadata_type.dataset_reader(self.metadata_doc)\n\n    @property\n    def fields(self):\n        return self.metadata_type.dataset_reader(self.metadata_doc).fields\n\n    @property\n    def measurements(self) -> Mapping[str, Measurement]:\n        """"""\n        Dictionary of measurements in this product\n        """"""\n        return OrderedDict((m[\'name\'], Measurement(**m)) for m in self.definition.get(\'measurements\', []))\n\n    @property\n    def dimensions(self) -> Tuple[str, str, str]:\n        """"""\n        List of dimension labels for data in this product\n        """"""\n        if self.grid_spec is not None:\n            spatial_dims = self.grid_spec.dimensions\n        else:\n            spatial_dims = DEFAULT_SPATIAL_DIMS\n\n        return (\'time\',) + spatial_dims\n\n    @cached_property\n    def grid_spec(self) -> Optional[\'GridSpec\']:\n        """"""\n        Grid specification for this product\n        """"""\n        storage = self.definition.get(\'storage\')\n        if storage is None:\n            return None\n\n        crs = storage.get(\'crs\')\n        if crs is None:\n            return None\n\n        crs = geometry.CRS(str(crs).strip())\n\n        def extract_point(name):\n            xx = storage.get(name, None)\n            return None if xx is None else tuple(xx[dim] for dim in crs.dimensions)\n\n        gs_params = {name: extract_point(name)\n                     for name in (\'tile_size\', \'resolution\', \'origin\')}\n\n        return GridSpec(crs=crs, **gs_params)\n\n    def canonical_measurement(self, measurement: str) -> str:\n        """""" resolve measurement alias into canonical name\n        """"""\n        for m in self.measurements:\n            if measurement == m:\n                return measurement\n            elif measurement in self.measurements[m].get(\'aliases\', []):\n                return m\n        raise KeyError(measurement)\n\n    def lookup_measurements(self, measurements: Optional[List[str]] = None) -> Mapping[str, Measurement]:\n        """"""\n        Find measurements by name\n\n        :param measurements: list of measurement names\n        """"""\n        my_measurements = self.measurements\n        if measurements is None:\n            return my_measurements\n\n        def fix_alias(measurement):\n            result = my_measurements[self.canonical_measurement(measurement)].copy()\n            result[\'name\'] = measurement\n            return result\n\n        return OrderedDict((measurement, fix_alias(measurement))\n                           for measurement in measurements)\n\n    def dataset_reader(self, dataset_doc):\n        return self.metadata_type.dataset_reader(dataset_doc)\n\n    def to_dict(self) -> Mapping[str, Any]:\n        """"""\n        Convert to a dictionary representation of the available fields\n        """"""\n        row = {\n            \'id\': self.id,\n            \'name\': self.name,\n            \'description\': self.definition[\'description\'],\n        }\n        row.update(self.fields)\n        if self.grid_spec is not None:\n            row.update({\n                \'crs\': str(self.grid_spec.crs),\n                \'spatial_dimensions\': self.grid_spec.dimensions,\n                \'tile_size\': self.grid_spec.tile_size,\n                \'resolution\': self.grid_spec.resolution,\n            })\n        return row\n\n    def __str__(self) -> str:\n        return ""DatasetType(name={name!r}, id_={id!r})"".format(id=self.id, name=self.name)\n\n    def __repr__(self) -> str:\n        return self.__str__()\n\n    # Types are uniquely identifiable by name:\n\n    def __eq__(self, other) -> bool:\n        if self is other:\n            return True\n\n        if self.__class__ != other.__class__:\n            return False\n\n        return self.name == other.name\n\n    def __hash__(self):\n        return hash(self.name)\n\n\n@schema_validated(SCHEMA_PATH / \'ingestor-config-type-schema.yaml\')\nclass IngestorConfig:\n    """"""\n    Ingestor configuration definition\n    """"""\n    pass\n\n\nclass GridSpec:\n    """"""\n    Definition for a regular spatial grid\n\n    >>> gs = GridSpec(crs=geometry.CRS(\'EPSG:4326\'), tile_size=(1, 1), resolution=(-0.1, 0.1), origin=(-50.05, 139.95))\n    >>> gs.tile_resolution\n    (10, 10)\n    >>> list(gs.tiles(geometry.BoundingBox(140, -50, 141.5, -48.5)))\n    [((0, 0), GeoBox(10, 10, Affine(0.1, 0.0, 139.95,\n           0.0, -0.1, -49.05), EPSG:4326)), ((1, 0), GeoBox(10, 10, Affine(0.1, 0.0, 140.95,\n           0.0, -0.1, -49.05), EPSG:4326)), ((0, 1), GeoBox(10, 10, Affine(0.1, 0.0, 139.95,\n           0.0, -0.1, -48.05), EPSG:4326)), ((1, 1), GeoBox(10, 10, Affine(0.1, 0.0, 140.95,\n           0.0, -0.1, -48.05), EPSG:4326))]\n\n    :param geometry.CRS crs: Coordinate System used to define the grid\n    :param [float,float] tile_size: (Y, X) size of each tile, in CRS units\n    :param [float,float] resolution: (Y, X) size of each data point in the grid, in CRS units. Y will\n                                   usually be negative.\n    :param [float,float] origin: (Y, X) coordinates of a corner of the (0,0) tile in CRS units. default is (0.0, 0.0)\n    """"""\n\n    def __init__(self,\n                 crs: geometry.CRS,\n                 tile_size: Tuple[float, float],\n                 resolution: Tuple[float, float],\n                 origin: Optional[Tuple[float, float]] = None):\n        self.crs = crs\n        self.tile_size = tile_size\n        self.resolution = resolution\n        self.origin = origin or (0.0, 0.0)\n\n    def __eq__(self, other):\n        if not isinstance(other, GridSpec):\n            return False\n\n        return (self.crs == other.crs\n                and self.tile_size == other.tile_size\n                and self.resolution == other.resolution\n                and self.origin == other.origin)\n\n    @property\n    def dimensions(self) -> Tuple[str, str]:\n        """"""\n        List of dimension names of the grid spec\n\n        """"""\n        return self.crs.dimensions\n\n    @property\n    def alignment(self) -> Tuple[float, float]:\n        """"""\n        Pixel boundary alignment\n        """"""\n        y, x = (orig % abs(res) for orig, res in zip(self.origin, self.resolution))\n        return (y, x)\n\n    @property\n    def tile_resolution(self) -> Tuple[int, int]:\n        """"""\n        Tile size in pixels in CRS dimension order (Usually y,x or lat,lon)\n        """"""\n        y, x = (int(abs(ts / res)) for ts, res in zip(self.tile_size, self.resolution))\n        return (y, x)\n\n    def tile_coords(self, tile_index: Tuple[int, int]) -> Tuple[float, float]:\n        """"""\n        Tile coordinates in (Y,X) order\n\n        :param tile_index: in X,Y order\n        """"""\n\n        def coord(index: int,\n                  resolution: float,\n                  size: float,\n                  origin: float) -> float:\n            return (index + (1 if resolution < 0 < size else 0)) * size + origin\n\n        y, x = (coord(index, res, size, origin)\n                for index, res, size, origin in zip(tile_index[::-1], self.resolution, self.tile_size, self.origin))\n        return (y, x)\n\n    def tile_geobox(self, tile_index: Tuple[int, int]) -> geometry.GeoBox:\n        """"""\n        Tile geobox.\n\n        :param (int,int) tile_index:\n        """"""\n        res_y, res_x = self.resolution\n        y, x = self.tile_coords(tile_index)\n        h, w = self.tile_resolution\n        geobox = geometry.GeoBox(crs=self.crs, affine=Affine(res_x, 0.0, x, 0.0, res_y, y), width=w, height=h)\n        return geobox\n\n    def tiles(self, bounds: geometry.BoundingBox,\n              geobox_cache: Optional[dict] = None) -> Iterator[Tuple[Tuple[int, int],\n                                                                     geometry.GeoBox]]:\n        """"""\n        Returns an iterator of tile_index, :py:class:`GeoBox` tuples across\n        the grid and overlapping with the specified `bounds` rectangle.\n\n        .. note::\n\n           Grid cells are referenced by coordinates `(x, y)`, which is the opposite to the usual CRS\n           dimension order.\n\n        :param BoundingBox bounds: Boundary coordinates of the required grid\n        :param dict geobox_cache: Optional cache to re-use geoboxes instead of creating new one each time\n        :return: iterator of grid cells with :py:class:`GeoBox` tiles\n        """"""\n        def geobox(tile_index):\n            if geobox_cache is None:\n                return self.tile_geobox(tile_index)\n\n            gbox = geobox_cache.get(tile_index)\n            if gbox is None:\n                gbox = self.tile_geobox(tile_index)\n                geobox_cache[tile_index] = gbox\n            return gbox\n\n        tile_size_y, tile_size_x = self.tile_size\n        tile_origin_y, tile_origin_x = self.origin\n        for y in GridSpec.grid_range(bounds.bottom - tile_origin_y, bounds.top - tile_origin_y, tile_size_y):\n            for x in GridSpec.grid_range(bounds.left - tile_origin_x, bounds.right - tile_origin_x, tile_size_x):\n                tile_index = (x, y)\n                yield tile_index, geobox(tile_index)\n\n    def tiles_from_geopolygon(self, geopolygon: geometry.Geometry,\n                              tile_buffer: Optional[Tuple[float, float]] = None,\n                              geobox_cache: Optional[dict] = None) -> Iterator[Tuple[Tuple[int, int],\n                                                                                     geometry.GeoBox]]:\n        """"""\n        Returns an iterator of tile_index, :py:class:`GeoBox` tuples across\n        the grid and overlapping with the specified `geopolygon`.\n\n        .. note::\n\n           Grid cells are referenced by coordinates `(x, y)`, which is the opposite to the usual CRS\n           dimension order.\n\n        :param geometry.Geometry geopolygon: Polygon to tile\n        :param tile_buffer: Optional <float,float> tuple, (extra padding for the query\n                            in native units of this GridSpec)\n        :param dict geobox_cache: Optional cache to re-use geoboxes instead of creating new one each time\n        :return: iterator of grid cells with :py:class:`GeoBox` tiles\n        """"""\n        geopolygon = geopolygon.to_crs(self.crs)\n        bbox = geopolygon.boundingbox\n        bbox = bbox.buffered(*tile_buffer) if tile_buffer else bbox\n\n        for tile_index, tile_geobox in self.tiles(bbox, geobox_cache):\n            tile_geobox = tile_geobox.buffered(*tile_buffer) if tile_buffer else tile_geobox\n\n            if geometry.intersects(tile_geobox.extent, geopolygon):\n                yield (tile_index, tile_geobox)\n\n    @staticmethod\n    def grid_range(lower: float, upper: float, step: float) -> range:\n        """"""\n        Returns the indices along a 1D scale.\n\n        Used for producing 2D grid indices.\n\n        >>> list(GridSpec.grid_range(-4.0, -1.0, 3.0))\n        [-2, -1]\n        >>> list(GridSpec.grid_range(1.0, 4.0, -3.0))\n        [-2, -1]\n        >>> list(GridSpec.grid_range(-3.0, 0.0, 3.0))\n        [-1]\n        >>> list(GridSpec.grid_range(-2.0, 1.0, 3.0))\n        [-1, 0]\n        >>> list(GridSpec.grid_range(-1.0, 2.0, 3.0))\n        [-1, 0]\n        >>> list(GridSpec.grid_range(0.0, 3.0, 3.0))\n        [0]\n        >>> list(GridSpec.grid_range(1.0, 4.0, 3.0))\n        [0, 1]\n        """"""\n        if step < 0.0:\n            lower, upper, step = -upper, -lower, -step\n        assert step > 0.0\n        return range(int(math.floor(lower / step)), int(math.ceil(upper / step)))\n\n    def __str__(self) -> str:\n        return ""GridSpec(crs=%s, tile_size=%s, resolution=%s)"" % (\n            self.crs, self.tile_size, self.resolution)\n\n    def __repr__(self) -> str:\n        return self.__str__()\n\n\ndef metadata_from_doc(doc: Mapping[str, Any]) -> MetadataType:\n    """"""Construct MetadataType that is not tied to any particular db index. This is\n    useful when there is a need to interpret dataset metadata documents\n    according to metadata spec.\n    """"""\n    from .fields import get_dataset_fields\n    MetadataType.validate(doc)  # type: ignore\n    return MetadataType(doc, get_dataset_fields(doc))\n'"
datacube/model/_base.py,0,"b""from collections import namedtuple\n\nRange = namedtuple('Range', ('begin', 'end'))\n"""
datacube/model/fields.py,0,"b'""""""Non-db specific implementation of metadata search fields.\n\nThis allows extraction of fields of interest from dataset metadata document.\n""""""\nfrom typing import Mapping, Dict, Any\nimport toolz\nimport decimal\nfrom datacube.utils import parse_time\nfrom ._base import Range\n\n# Allowed values for field \'type\' (specified in a metadata type docuemnt)\n_AVAILABLE_TYPE_NAMES = (\n    \'numeric-range\',\n    \'double-range\',\n    \'integer-range\',\n    \'datetime-range\',\n\n    \'string\',\n    \'numeric\',\n    \'double\',\n    \'integer\',\n    \'datetime\',\n\n    # For backwards compatibility (alias for numeric-range)\n    \'float-range\',\n)\n\n\nclass Expression:\n    # No properties at the moment. These are built and returned by the\n    # DB driver (from Field methods), so they\'re mostly an opaque token.\n\n    # A simple equals implementation for comparison in test code.\n    def __eq__(self, other) -> bool:\n        if self.__class__ != other.__class__:\n            return False\n        return self.__dict__ == other.__dict__\n\n\nclass Field:\n    """"""\n    A searchable field within a dataset/storage metadata document.\n    """"""\n    # type of field.\n    # If type is not specified, the field is a string\n    # This should always be one of _AVAILABLE_TYPE_NAMES\n    type_name = \'string\'\n\n    def __init__(self, name: str, description: str):\n        self.name = name\n\n        self.description = description\n\n        # Does selecting this affect the output rows?\n        # (eg. Does this join other tables that aren\'t 1:1 with datasets.)\n        self.affects_row_selection = False\n\n        assert self.type_name in _AVAILABLE_TYPE_NAMES, ""Invalid type name %r"" % (self.type_name,)\n\n    def __eq__(self, value) -> Expression:  # type: ignore\n        """"""\n        Is this field equal to a value?\n\n        this returns an Expression object (hence type ignore above)\n        """"""\n        raise NotImplementedError(\'equals expression\')\n\n    def between(self, low, high) -> Expression:\n        """"""\n        Is this field in a range?\n        """"""\n        raise NotImplementedError(\'between expression\')\n\n\nclass SimpleField:\n    def __init__(self,\n                 offset,\n                 converter,\n                 type_name,\n                 name=\'\',\n                 description=\'\'):\n        self._offset = offset\n        self._converter = converter\n        self.type_name = type_name\n        self.description = description\n        self.name = name\n\n    def extract(self, doc):\n        v = toolz.get_in(self._offset, doc, default=None)\n        if v is None:\n            return None\n        return self._converter(v)\n\n\nclass RangeField:\n    def __init__(self,\n                 min_offset,\n                 max_offset,\n                 base_converter,\n                 type_name,\n                 name=\'\',\n                 description=\'\'):\n        self.type_name = type_name\n        self.description = description\n        self.name = name\n        self._converter = base_converter\n        self._min_offset = min_offset\n        self._max_offset = max_offset\n\n    def extract(self, doc):\n        def extract_raw(paths):\n            vv = [toolz.get_in(p, doc, default=None) for p in paths]\n            return [self._converter(v) for v in vv if v is not None]\n\n        v_min = extract_raw(self._min_offset)\n        v_max = extract_raw(self._max_offset)\n\n        v_min = None if len(v_min) == 0 else min(v_min)\n        v_max = None if len(v_max) == 0 else max(v_max)\n\n        if v_min is None and v_max is None:\n            return None\n\n        return Range(v_min, v_max)\n\n\ndef parse_search_field(doc, name=\'\'):\n    parsers = {\n        \'string\': str,\n        \'double\': float,\n        \'integer\': int,\n        \'numeric\': decimal.Decimal,\n        \'datetime\': parse_time,\n        \'object\': lambda x: x,\n    }\n    _type = doc.get(\'type\', \'string\')\n\n    if _type in parsers:\n        offset = doc.get(\'offset\', None)\n        if offset is None:\n            raise ValueError(\'Missing offset\')\n\n        return SimpleField(offset,\n                           parsers[_type],\n                           _type,\n                           name=name,\n                           description=doc.get(\'description\', \'\'))\n\n    if not _type.endswith(\'-range\'):\n        raise ValueError(\'Unsupported search field type: \' + str(_type))\n\n    raw_type = _type.split(\'-\')[0]\n\n    if raw_type == \'float\':  # float-range is supposed to be supported, but not just float?\n        raw_type = \'numeric\'\n        _type = \'numeric-range\'\n\n    if raw_type not in parsers:\n        raise ValueError(\'Unsupported search field type: \' + str(_type))\n\n    min_offset = doc.get(\'min_offset\', None)\n    max_offset = doc.get(\'max_offset\', None)\n\n    if min_offset is None or max_offset is None:\n        raise ValueError(\'Need to specify both min_offset and max_offset\')\n\n    return RangeField(min_offset,\n                      max_offset,\n                      parsers[raw_type],\n                      _type,\n                      name=name,\n                      description=doc.get(\'description\', \'\'))\n\n\ndef get_dataset_fields(metadata_definition: Mapping[str, Any]) -> Dict[str, Field]:\n    """"""Construct search fields dictionary not tied to any specific db\n    implementation.\n\n    """"""\n    fields = toolz.get_in([\'dataset\', \'search_fields\'], metadata_definition, {})\n    return {n: parse_search_field(doc, name=n) for n, doc in fields.items()}\n'"
datacube/model/utils.py,0,"b'\nimport datetime\nimport os\nimport platform\nimport sys\nimport uuid\nimport toolz\n\nimport numpy\nimport xarray\nimport yaml\nfrom pandas import to_datetime\n\nimport datacube\nfrom datacube.model import Dataset\nfrom datacube.utils import geometry, SimpleDocNav, InvalidDocException\nfrom datacube.utils.py import sorted_items\n\ntry:\n    from yaml import CSafeDumper as SafeDumper  # type: ignore\nexcept ImportError:\n    from yaml import SafeDumper  # type: ignore\n\n\ndef machine_info():\n    info = {\n        \'software_versions\': {\n            \'python\': {\'version\': sys.version},\n            \'datacube\': {\'version\': datacube.__version__,\n                         \'repo_url\': \'https://github.com/opendatacube/datacube-core.git\'},\n        },\n        \'hostname\': platform.node(),\n    }\n\n    if hasattr(os, \'uname\'):\n        info[\'uname\'] = \' \'.join(os.uname())\n    else:\n        info[\'uname\'] = \' \'.join([platform.system(),\n                                  platform.node(),\n                                  platform.release(),\n                                  platform.version(),\n                                  platform.machine()])\n\n    return {\'lineage\': {\'machine\': info}}\n\n\ndef geobox_info(extent, valid_data=None):\n    image_bounds = extent.boundingbox\n    data_bounds = valid_data.boundingbox if valid_data else image_bounds\n    ul = geometry.point(data_bounds.left, data_bounds.top, crs=extent.crs).to_crs(geometry.CRS(\'EPSG:4326\'))\n    ur = geometry.point(data_bounds.right, data_bounds.top, crs=extent.crs).to_crs(geometry.CRS(\'EPSG:4326\'))\n    lr = geometry.point(data_bounds.right, data_bounds.bottom, crs=extent.crs).to_crs(geometry.CRS(\'EPSG:4326\'))\n    ll = geometry.point(data_bounds.left, data_bounds.bottom, crs=extent.crs).to_crs(geometry.CRS(\'EPSG:4326\'))\n    doc = {\n        \'extent\': {\n            \'coord\': {\n                \'ul\': {\'lon\': ul.points[0][0], \'lat\': ul.points[0][1]},\n                \'ur\': {\'lon\': ur.points[0][0], \'lat\': ur.points[0][1]},\n                \'lr\': {\'lon\': lr.points[0][0], \'lat\': lr.points[0][1]},\n                \'ll\': {\'lon\': ll.points[0][0], \'lat\': ll.points[0][1]},\n            }\n        },\n        \'grid_spatial\': {\n            \'projection\': {\n                \'spatial_reference\': str(extent.crs),\n                \'geo_ref_points\': {\n                    \'ul\': {\'x\': image_bounds.left, \'y\': image_bounds.top},\n                    \'ur\': {\'x\': image_bounds.right, \'y\': image_bounds.top},\n                    \'ll\': {\'x\': image_bounds.left, \'y\': image_bounds.bottom},\n                    \'lr\': {\'x\': image_bounds.right, \'y\': image_bounds.bottom},\n                }\n            }\n        }\n    }\n    if valid_data:\n        doc[\'grid_spatial\'][\'projection\'][\'valid_data\'] = valid_data.__geo_interface__\n    return doc\n\n\ndef new_dataset_info():\n    return {\n        \'id\': str(uuid.uuid4()),\n        \'creation_dt\': datetime.datetime.utcnow().isoformat(),\n    }\n\n\ndef band_info(band_names, band_uris=None):\n    """"""\n    :param list band_names: names of the bands\n    :param dict band_uris: mapping from names to dicts with \'path\' and \'layer\' specs\n    """"""\n    if band_uris is None:\n        band_uris = {name: {\'path\': \'\', \'layer\': name} for name in band_names}\n\n    return {\n        \'image\': {\n            \'bands\': {name: band_uris[name] for name in band_names}\n        }\n    }\n\n\ndef time_info(time, start_time=None, end_time=None, key_time=None):\n    time_str = to_datetime(time).isoformat()\n    start_time_str = to_datetime(start_time).isoformat() if start_time else time_str\n    end_time_str = to_datetime(end_time).isoformat() if end_time else time_str\n    extent = {\n        \'extent\': {\n            \'from_dt\': start_time_str,\n            \'to_dt\': end_time_str,\n            \'center_dt\': time_str,\n        }\n    }\n    if key_time is not None:\n        extent[\'extent\'][\'key_time\'] = to_datetime(key_time).isoformat()\n    return extent\n\n\ndef source_info(source_datasets):\n    return {\n        \'lineage\': {\n            \'source_datasets\': {str(idx): dataset.metadata_doc for idx, dataset in enumerate(source_datasets)}\n        }\n    }\n\n\ndef datasets_to_doc(output_datasets):\n    """"""\n    Create a yaml document version of every dataset\n\n    :param output_datasets: An array of :class:`datacube.model.Dataset`\n    :type output_datasets: :py:class:`xarray.DataArray`\n    :return: An array of yaml document strings\n    :rtype: :py:class:`xarray.DataArray`\n    """"""\n\n    def dataset_to_yaml(index, dataset):\n        return yaml.dump(dataset.metadata_doc, Dumper=SafeDumper, encoding=\'utf-8\')\n\n    return xr_apply(output_datasets, dataset_to_yaml, dtype=\'O\').astype(\'S\')\n\n\ndef xr_iter(data_array):\n    """"""\n    Iterate over every element in an xarray, returning::\n\n        * the numerical index eg ``(10, 1)``\n        * the labeled index eg ``{\'time\': datetime(), \'band\': \'red\'}``\n        * the element (same as ``da[10, 1].item()``)\n\n    :param data_array: Array to iterate over\n    :type data_array: xarray.DataArray\n    :return: i-index, label-index, value of da element\n    :rtype tuple, dict, da.dtype\n    """"""\n    values = data_array.values\n    coords = {coord_name: v.values for coord_name, v in data_array.coords.items()}\n    for i in numpy.ndindex(data_array.shape):\n        entry = values[i]\n        index = {coord_name: v[i] for coord_name, v in coords.items()}\n        yield i, index, entry\n\n\ndef xr_apply(data_array, func, dtype=None, with_numeric_index=False):\n    """"""\n    Apply a function to every element of a :class:`xarray.DataArray`\n\n    :type data_array: xarray.DataArray\n    :param func: function that takes a dict of labels and an element of the array,\n        and returns a value of the given dtype\n    :param dtype: The dtype of the returned array, default to the same as original\n    :param with_numeric_index Bool: If true include numeric index: func(index, labels, value)\n    :return: The array with output of the function for every element.\n    :rtype: xarray.DataArray\n    """"""\n    if dtype is None:\n        dtype = data_array.dtype\n\n    data = numpy.empty(shape=data_array.shape, dtype=dtype)\n    for i, index, entry in xr_iter(data_array):\n        if with_numeric_index:\n            v = func(i, index, entry)\n        else:\n            v = func(index, entry)\n        data[i] = v\n    return xarray.DataArray(data, coords=data_array.coords, dims=data_array.dims)\n\n\ndef make_dataset(product, sources, extent, center_time, valid_data=None, uri=None, app_info=None,\n                 band_uris=None, start_time=None, end_time=None):\n    """"""\n    Create :class:`datacube.model.Dataset` for the data\n\n    :param DatasetType product: Product the dataset is part of\n    :param list[:class:`Dataset`] sources: datasets used to produce the dataset\n    :param Geometry extent: extent of the dataset\n    :param Geometry valid_data: extent of the valid data\n    :param center_time: time of the central point of the dataset\n    :param str uri: The uri of the dataset\n    :param dict app_info: Additional metadata to be stored about the generation of the product\n    :param dict band_uris: band name to uri mapping\n    :param start_time: start time of the dataset (defaults to `center_time`)\n    :param end_time: end time of the dataset (defaults to `center_time`)\n    :rtype: class:`Dataset`\n    """"""\n    document = {}\n    merge(document, product.metadata_doc)\n    merge(document, new_dataset_info())\n    merge(document, machine_info())\n    merge(document, band_info(product.measurements.keys(), band_uris=band_uris))\n    merge(document, source_info(sources))\n    merge(document, geobox_info(extent, valid_data))\n    merge(document, time_info(center_time, start_time, end_time))\n    merge(document, app_info or {})\n\n    return Dataset(product,\n                   document,\n                   uris=[uri] if uri else None,\n                   sources={str(idx): dataset for idx, dataset in enumerate(sources)})\n\n\ndef merge(a, b, path=None):\n    """"""\n    Merge dictionary `b` into dictionary `a`\n\n    See: http://stackoverflow.com/a/7205107/5262498\n\n    :type a: dict\n    :type b: dict\n    :rtype: dict\n    """"""\n    if path is None:\n        path = []\n    for key in b:\n        if key in a:\n            if isinstance(a[key], dict) and isinstance(b[key], dict):\n                merge(a[key], b[key], path + [str(key)])\n            elif a[key] == b[key]:\n                pass  # same leaf value\n            else:\n                raise Exception(\'Conflict at %s\' % \'.\'.join(path + [str(key)]))\n        else:\n            a[key] = b[key]\n    return a\n\n\ndef traverse_datasets(ds, cbk, mode=\'post-order\', **kwargs):\n    """"""Perform depth first traversal of lineage tree. Note that we assume it\'s a\n    tree, even though it might be a DAG (Directed Acyclic Graph). If it is a\n    DAG it will be treated as if it was a tree with some nodes appearing twice or more\n    times in this tree.\n\n    Order of traversal of nodes on the same level is in default sort order for\n    strings (assuming your keys are strings, which is the case for Dataset\n    object). NOTE: this could be problematic as order might be dependent on\n    locale settings.\n\n    If given a graph with cycles this will blow the stack, so don\'t do that.\n\n    ds -- Dataset with lineage to iterate over, but really anything that has\n          `sources` attribute which contains a dict from string to the same\n          thing.\n\n    cbk :: (Dataset, depth=0, name=None, **kwargs) -> None\n\n    mode: post-order | pre-order\n\n    mode=post-order -- Visit all lineage first, only then visit top level\n    mode=pre-order --  Visit top level first, only then visit lineage\n\n    """"""\n\n    def visit_pre_order(ds, func, depth=0, name=None):\n        func(ds, depth=depth, name=name, **kwargs)\n\n        for k, v in sorted_items(ds.sources):\n            visit_pre_order(v, func, depth=depth+1, name=k)\n\n    def visit_post_order(ds, func, depth=0, name=None):\n        for k, v in sorted_items(ds.sources):\n            visit_post_order(v, func, depth=depth+1, name=k)\n\n        func(ds, depth=depth, name=name, **kwargs)\n\n    proc = {\'post-order\': visit_post_order,\n            \'pre-order\': visit_pre_order}.get(mode, None)\n\n    if proc is None:\n        raise ValueError(\'Unsupported traversal mode: {}\'.format(mode))\n\n    proc(ds, cbk)\n\n\ndef flatten_datasets(ds, with_depth_grouping=False):\n    """"""Build a dictionary mapping from dataset.id to a list of datasets with that\n    id appearing in the lineage DAG. When DAG is unrolled into a tree, some\n    datasets will be reachable by multiple paths, sometimes these would be\n    exactly the same python object, other times they will be duplicate views of\n    the same ""conceptual dataset object"". If the same dataset is reachable by\n    three possible paths from the root, it will appear three times in the\n    flattened view.\n\n    ds could be a Dataset object read from DB with `include_sources=True`, or\n    it could be `SimpleDocNav` object created from a dataset metadata document\n    read from a file.\n\n    If with_depth_grouping=True, also build depth -> [Ds] mapping and return it\n    along with Id -> [Ds] mapping. In this case top level is depth=0.\n    """"""\n    def get_list(out, k):\n        if k not in out:\n            out[k] = []\n        return out[k]\n\n    def proc(ds, depth=0, name=None, id_map=None, depth_map=None):\n        k = ds.id\n\n        get_list(id_map, k).append(ds)\n        if depth_map is not None:\n            get_list(depth_map, depth).append(ds)\n\n    id_map = {}\n    depth_map = {} if with_depth_grouping else None\n\n    traverse_datasets(ds, proc, id_map=id_map, depth_map=depth_map)\n\n    if depth_map:\n        # convert dict Int->V to just a list\n        dout = [None]*len(depth_map)\n        for k, v in depth_map.items():\n            dout[k] = v\n\n        return id_map, dout\n\n    return id_map\n\n\ndef remap_lineage_doc(root, mk_node, **kwargs):\n    def visit(ds):\n        return mk_node(ds,\n                       {k: visit(v) for k, v in sorted_items(ds.sources)},\n                       **kwargs)\n\n    if not isinstance(root, SimpleDocNav):\n        root = SimpleDocNav(root)\n\n    return visit(root)\n\n\ndef dedup_lineage(root):\n    """"""Find duplicate nodes in the lineage tree and replace them with references.\n\n    Will raise `ValueError` when duplicate dataset (same uuid, but different\n    path from root) has either conflicting metadata or conflicting lineage\n    data.\n\n    :param dict|SimpleDocNav root:\n\n    Returns a new document that has the same structure as input document, but\n    with duplicate entries now being aliases rather than copies.\n    """"""\n\n    def check_sources(a, b):\n        """""" True if two dictionaries contain same objects under the same names.\n        same, not just equivalent.\n        """"""\n        if len(a) != len(b):\n            return False\n\n        for ((ak, av), (bk, bv)) in zip(sorted_items(a), sorted_items(b)):\n            if ak != bk:\n                return False\n            if av is not bv:\n                return False\n\n        return True\n\n    def mk_node(ds, sources, cache, sources_path):\n        existing = cache.get(ds.id, None)\n        doc = ds.doc_without_lineage_sources\n\n        if existing is not None:\n            _ds, _doc, _sources = existing\n\n            if not check_sources(sources, _sources):\n                raise InvalidDocException(\'Inconsistent lineage for repeated dataset with _id: {}\'.format(ds.id))\n\n            if doc != _doc:\n                raise InvalidDocException(\'Inconsistent metadata for repeated dataset with _id: {}\'.format(ds.id))\n\n            return _ds\n\n        out_ds = toolz.assoc_in(doc, sources_path, sources)\n        cache[ds.id] = (out_ds, doc, sources)\n        return out_ds\n\n    if not isinstance(root, SimpleDocNav):\n        root = SimpleDocNav(root)\n\n    return remap_lineage_doc(root, mk_node, cache={}, sources_path=root.sources_path)\n'"
datacube/scripts/__init__.py,0,b''
datacube/scripts/cli_app.py,0,"b'#!/usr/bin/env python\n# coding=utf-8\n""""""\nDatacube command-line interface\n""""""\n\n\nfrom datacube.ui.click import cli\nimport datacube.scripts.dataset\nimport datacube.scripts.ingest\nimport datacube.scripts.product\nimport datacube.scripts.metadata\nimport datacube.scripts.system\nimport datacube.scripts.user\n\n\nif __name__ == \'__main__\':\n    cli()\n'"
datacube/scripts/dataset.py,0,"b'import csv\nimport datetime\nimport logging\nimport sys\nfrom collections import OrderedDict\nfrom typing import Iterable, Mapping, MutableMapping, Any\n\nimport click\nimport yaml\nimport yaml.resolver\nfrom click import echo\n\nfrom datacube.index.exceptions import MissingRecordError\nfrom datacube.index.hl import Doc2Dataset, check_dataset_consistent\nfrom datacube.index.eo3 import prep_eo3\nfrom datacube.index.index import Index\nfrom datacube.model import Dataset\nfrom datacube.ui import click as ui\nfrom datacube.ui.click import cli\nfrom datacube.ui.common import ui_path_doc_stream\nfrom datacube.utils import changes, SimpleDocNav\nfrom datacube.utils.serialise import SafeDatacubeDumper\n\n_LOG = logging.getLogger(\'datacube-dataset\')\n\n\ndef report_old_options(mapping):\n    def maybe_remap(s):\n        if s in mapping:\n            _LOG.warning(""DEPRECATED option detected: --%s use --%s instead"", s, mapping[s])\n            return mapping[s]\n        else:\n            return s\n\n    return maybe_remap\n\n\n@cli.group(name=\'dataset\', help=\'Dataset management commands\')\ndef dataset_cmd():\n    pass\n\n\ndef dataset_stream(doc_stream, ds_resolve):\n    """""" Convert a stream `(uri, doc)` pairs into a stream of resolved datasets\n\n        skips failures with logging\n    """"""\n    for uri, ds in doc_stream:\n        dataset, err = ds_resolve(ds, uri)\n\n        if dataset is None:\n            _LOG.error(\'%s\', str(err))\n            continue\n\n        yield dataset\n\n\ndef load_datasets_for_update(doc_stream, index):\n    """"""Consume stream of dataset documents, associate each to a product by looking\n    up existing dataset in the index. Datasets not in the database will be\n    logged.\n\n    Doesn\'t load lineage information\n\n    Generates tuples in the form (new_dataset, existing_dataset)\n    """"""\n\n    def mk_dataset(ds, uri):\n        uuid = ds.id\n\n        if uuid is None:\n            return None, None, ""Metadata document it missing id field""\n\n        existing = index.datasets.get(uuid)\n        if existing is None:\n            return None, None, ""No such dataset in the database: {}"".format(uuid)\n\n        ds = SimpleDocNav(prep_eo3(ds.doc, auto_skip=True))\n\n        # TODO: what about sources=?\n        return Dataset(existing.type,\n                       ds.doc_without_lineage_sources,\n                       uris=[uri]), existing, None\n\n    for uri, doc in doc_stream:\n        dataset, existing, error_msg = mk_dataset(doc, uri)\n\n        if dataset is None:\n            _LOG.error(""Failure while processing: %s\\n > Reason: %s"", uri, error_msg)\n        else:\n            is_consistent, reason = check_dataset_consistent(dataset)\n            if is_consistent:\n                yield dataset, existing\n            else:\n                _LOG.error(""Dataset %s inconsistency: %s"", dataset.id, reason)\n\n\n@dataset_cmd.command(\'add\',\n                     help=""Add datasets to the Data Cube"",\n                     context_settings=dict(token_normalize_func=report_old_options({\n                         \'dtype\': \'product\',\n                         \'t\': \'p\'\n                     })))\n@click.option(\'--product\', \'-p\', \'product_names\',\n              help=(\'Only match against products specified with this option, \'\n                    \'you can supply several by repeating this option with a new product name\'),\n              multiple=True)\n@click.option(\'--exclude-product\', \'-x\', \'exclude_product_names\',\n              help=(\'Attempt to match to all products in the DB except for products \'\n                    \'specified with this option, \'\n                    \'you can supply several by repeating this option with a new product name\'),\n              multiple=True)\n@click.option(\'--auto-match\', \'-a\', help=""Deprecated don\'t use it, it\'s a no-op"",\n              is_flag=True, default=False)\n@click.option(\'--auto-add-lineage/--no-auto-add-lineage\', is_flag=True, default=True,\n              help=(\'Default behaviour is to automatically add lineage datasets if they are missing from the database, \'\n                    \'but this can be disabled if lineage is expected to be present in the DB, \'\n                    \'in this case add will abort when encountering missing lineage dataset\'))\n@click.option(\'--verify-lineage/--no-verify-lineage\', is_flag=True, default=True,\n              help=(\'Lineage referenced in the metadata document should be the same as in DB, \'\n                    \'default behaviour is to skip those top-level datasets that have lineage data \'\n                    \'different from the version in the DB. This option allows omitting verification step.\'))\n@click.option(\'--dry-run\', help=\'Check if everything is ok\', is_flag=True, default=False)\n@click.option(\'--ignore-lineage\',\n              help=""Pretend that there is no lineage data in the datasets being indexed"",\n              is_flag=True, default=False)\n@click.option(\'--confirm-ignore-lineage\',\n              help=""Pretend that there is no lineage data in the datasets being indexed, without confirmation"",\n              is_flag=True, default=False)\n@click.argument(\'dataset-paths\', type=str, nargs=-1)\n@ui.pass_index()\ndef index_cmd(index, product_names,\n              exclude_product_names,\n              auto_match,\n              auto_add_lineage,\n              verify_lineage,\n              dry_run,\n              ignore_lineage,\n              confirm_ignore_lineage,\n              dataset_paths):\n    if confirm_ignore_lineage is False and ignore_lineage is True:\n        if sys.stdin.isatty():\n            confirmed = click.confirm(""Requested to skip lineage information, Are you sure?"", default=False)\n            if not confirmed:\n                click.echo(\'OK aborting\', err=True)\n                sys.exit(1)\n        else:\n            click.echo(""Use --confirm-ignore-lineage from non-interactive scripts. Aborting."")\n            sys.exit(1)\n\n        confirm_ignore_lineage = True\n\n    if auto_match is True:\n        _LOG.warning(""--auto-match option is deprecated, update your scripts, behaviour is the same without it"")\n\n    try:\n        ds_resolve = Doc2Dataset(index,\n                                 product_names,\n                                 exclude_products=exclude_product_names,\n                                 skip_lineage=confirm_ignore_lineage,\n                                 fail_on_missing_lineage=not auto_add_lineage,\n                                 verify_lineage=verify_lineage)\n    except ValueError as e:\n        _LOG.error(e)\n        sys.exit(2)\n\n    def run_it(dataset_paths):\n        doc_stream = ui_path_doc_stream(dataset_paths, logger=_LOG, uri=True)\n        dss = dataset_stream(doc_stream, ds_resolve)\n        index_datasets(dss,\n                       index,\n                       auto_add_lineage=auto_add_lineage,\n                       dry_run=dry_run)\n\n    # If outputting directly to terminal, show a progress bar.\n    if sys.stdout.isatty():\n        with click.progressbar(dataset_paths, label=\'Indexing datasets\') as pp:\n            run_it(pp)\n    else:\n        run_it(dataset_paths)\n\n\ndef index_datasets(dss, index, auto_add_lineage, dry_run):\n    for dataset in dss:\n        _LOG.info(\'Matched %s\', dataset)\n        if not dry_run:\n            try:\n                index.datasets.add(dataset, with_lineage=auto_add_lineage)\n            except (ValueError, MissingRecordError) as e:\n                _LOG.error(\'Failed to add dataset %s: %s\', dataset.local_uri, e)\n\n\ndef parse_update_rules(keys_that_can_change):\n    updates_allowed = {}\n    for key_str in keys_that_can_change:\n        updates_allowed[tuple(key_str.split(\'.\'))] = changes.allow_any\n    return updates_allowed\n\n\n@dataset_cmd.command(\'update\', help=""Update datasets in the Data Cube"")\n@click.option(\'--allow-any\', \'keys_that_can_change\',\n              help=""Allow any changes to the specified key (a.b.c)"",\n              multiple=True)\n@click.option(\'--dry-run\', help=\'Check if everything is ok\', is_flag=True, default=False)\n@click.option(\'--location-policy\',\n              type=click.Choice([\'keep\', \'archive\', \'forget\']),\n              default=\'keep\',\n              help=\'\'\'What to do with previously recorded dataset location\n\'keep\' - keep as alternative location [default]\n\'archive\' - mark as archived\n\'forget\' - remove from the index\n\'\'\')\n@click.argument(\'dataset-paths\', nargs=-1)\n@ui.pass_index()\ndef update_cmd(index, keys_that_can_change, dry_run, location_policy, dataset_paths):\n    def loc_action(action, new_ds, existing_ds, action_name):\n        if len(existing_ds.uris) == 0:\n            return None\n\n        if len(existing_ds.uris) > 1:\n            _LOG.warning(""Refusing to %s old location, there are several"", action_name)\n            return None\n\n        new_uri, = new_ds.uris\n        old_uri, = existing_ds.uris\n\n        if new_uri == old_uri:\n            return None\n\n        if dry_run:\n            echo(\'Will {} old location {}, and add new one {}\'.format(action_name, old_uri, new_uri))\n            return True\n\n        return action(existing_ds.id, old_uri)\n\n    def loc_archive(new_ds, existing_ds):\n        return loc_action(index.datasets.archive_location, new_ds, existing_ds, \'archive\')\n\n    def loc_forget(new_ds, existing_ds):\n        return loc_action(index.datasets.remove_location, new_ds, existing_ds, \'forget\')\n\n    def loc_keep(new_ds, existing_ds):\n        return None\n\n    update_loc = dict(archive=loc_archive,\n                      forget=loc_forget,\n                      keep=loc_keep)[location_policy]\n\n    updates_allowed = parse_update_rules(keys_that_can_change)\n\n    success, fail = 0, 0\n\n    for dataset, existing_ds in load_datasets_for_update(\n            ui_path_doc_stream(dataset_paths, logger=_LOG, uri=True), index):\n        _LOG.info(\'Matched %s\', dataset)\n\n        if location_policy != \'keep\':\n            if len(existing_ds.uris) > 1:\n                # TODO:\n                pass\n\n        if not dry_run:\n            try:\n                index.datasets.update(dataset, updates_allowed=updates_allowed)\n                update_loc(dataset, existing_ds)\n                success += 1\n                echo(\'Updated %s\' % dataset.id)\n            except ValueError as e:\n                fail += 1\n                echo(\'Failed to update %s: %s\' % (dataset.id, e))\n        else:\n            if update_dry_run(index, updates_allowed, dataset):\n                update_loc(dataset, existing_ds)\n                success += 1\n            else:\n                fail += 1\n    echo(\'%d successful, %d failed\' % (success, fail))\n\n\ndef update_dry_run(index, updates_allowed, dataset):\n    try:\n        can_update, safe_changes, unsafe_changes = index.datasets.can_update(dataset, updates_allowed=updates_allowed)\n    except ValueError as e:\n        echo(\'Cannot update %s: %s\' % (dataset.id, e))\n        return False\n\n    if can_update:\n        echo(\'Can update %s: %s unsafe changes, %s safe changes\' % (dataset.id,\n                                                                    len(unsafe_changes),\n                                                                    len(safe_changes)))\n    else:\n        echo(\'Cannot update %s: %s unsafe changes, %s safe changes\' % (dataset.id,\n                                                                       len(unsafe_changes),\n                                                                       len(safe_changes)))\n    return can_update\n\n\ndef build_dataset_info(index: Index, dataset: Dataset,\n                       show_sources: bool = False,\n                       show_derived: bool = False,\n                       depth: int = 1,\n                       max_depth: int = 99) -> Mapping[str, Any]:\n    info = OrderedDict((\n        (\'id\', str(dataset.id)),\n        (\'product\', dataset.type.name),\n        (\'status\', \'archived\' if dataset.is_archived else \'active\')\n    ))  # type: MutableMapping[str, Any]\n\n    # Optional when loading a dataset.\n    if dataset.indexed_time is not None:\n        info[\'indexed\'] = dataset.indexed_time\n\n    info[\'locations\'] = dataset.uris\n    info[\'fields\'] = dataset.metadata.search_fields\n\n    if depth < max_depth:\n        if show_sources and dataset.sources is not None:\n            info[\'sources\'] = {key: build_dataset_info(index, source,\n                                                       show_sources=True, show_derived=False,\n                                                       depth=depth + 1, max_depth=max_depth)\n                               for key, source in dataset.sources.items()}\n\n        if show_derived:\n            info[\'derived\'] = [build_dataset_info(index, derived,\n                                                  show_sources=False, show_derived=True,\n                                                  depth=depth + 1, max_depth=max_depth)\n                               for derived in index.datasets.get_derived(dataset.id)]\n\n    return info\n\n\ndef _write_csv(infos):\n    writer = csv.DictWriter(sys.stdout, [\'id\', \'status\', \'product\', \'location\'], extrasaction=\'ignore\')\n    writer.writeheader()\n\n    def add_first_location(row):\n        locations_ = row[\'locations\']\n        row[\'location\'] = locations_[0] if locations_ else None\n        return row\n\n    writer.writerows(add_first_location(row) for row in infos)\n\n\ndef _write_yaml(infos):\n    """"""\n    Dump yaml data with support for OrderedDicts.\n\n    Allows for better human-readability of output: such as dataset ID field first, sources last.\n\n    (Ordered dicts are output identically to normal yaml dicts: their order is purely for readability)\n    """"""\n\n    return yaml.dump_all(infos, sys.stdout, SafeDatacubeDumper, default_flow_style=False, indent=4)\n\n\n_OUTPUT_WRITERS = {\n    \'csv\': _write_csv,\n    \'yaml\': _write_yaml,\n}\n\n\n@dataset_cmd.command(\'info\', help=""Display dataset information"")\n@click.option(\'--show-sources\', help=\'Also show source datasets\', is_flag=True, default=False)\n@click.option(\'--show-derived\', help=\'Also show derived datasets\', is_flag=True, default=False)\n@click.option(\'-f\', help=\'Output format\',\n              type=click.Choice(list(_OUTPUT_WRITERS)), default=\'yaml\', show_default=True)\n@click.option(\'--max-depth\',\n              help=\'Maximum sources/derived depth to travel\',\n              type=int,\n              # Unlikely to be hit, but will avoid total-death by circular-references.\n              default=99)\n@click.argument(\'ids\', nargs=-1)\n@ui.pass_index()\ndef info_cmd(index: Index, show_sources: bool, show_derived: bool,\n             f: str,\n             max_depth: int,\n             ids: Iterable[str]) -> None:\n    # Using an array wrapper to get around the lack of ""nonlocal"" in py2\n    missing_datasets = [0]\n\n    def get_datasets(ids):\n        for id_ in ids:\n            dataset = index.datasets.get(id_, include_sources=show_sources)\n            if dataset:\n                yield dataset\n            else:\n                click.echo(\'%s missing\' % id_, err=True)\n                missing_datasets[0] += 1\n\n    _OUTPUT_WRITERS[f](\n        build_dataset_info(index,\n                           dataset,\n                           show_sources=show_sources,\n                           show_derived=show_derived,\n                           max_depth=max_depth)\n        for dataset in get_datasets(ids)\n    )\n    sys.exit(missing_datasets[0])\n\n\n@dataset_cmd.command(\'search\')\n@click.option(\'--limit\', help=\'Limit the number of results\',\n              type=int, default=None)\n@click.option(\'-f\', help=\'Output format\',\n              type=click.Choice(list(_OUTPUT_WRITERS)), default=\'yaml\', show_default=True)\n@ui.parsed_search_expressions\n@ui.pass_index()\ndef search_cmd(index, limit, f, expressions):\n    """"""\n    Search available Datasets\n    """"""\n    datasets = index.datasets.search(limit=limit, **expressions)\n    _OUTPUT_WRITERS[f](\n        build_dataset_info(index, dataset)\n        for dataset in datasets\n    )\n\n\ndef _get_derived_set(index, id_):\n    """"""\n    Get a single flat set of all derived datasets.\n    (children, grandchildren, great-grandchildren...)\n    """"""\n    derived_set = {index.datasets.get(id_)}\n    to_process = {id_}\n    while to_process:\n        derived = index.datasets.get_derived(to_process.pop())\n        to_process.update(d.id for d in derived)\n        derived_set.update(derived)\n    return derived_set\n\n\n@dataset_cmd.command(\'archive\', help=""Archive datasets"")\n@click.option(\'--archive-derived\', \'-d\', help=\'Also recursively archive derived datasets\', is_flag=True, default=False)\n@click.option(\'--dry-run\', help=""Don\'t archive. Display datasets that would get archived"",\n              is_flag=True, default=False)\n@click.argument(\'ids\', nargs=-1)\n@ui.pass_index()\ndef archive_cmd(index, archive_derived, dry_run, ids):\n    for id_ in ids:\n        to_process = _get_derived_set(index, id_) if archive_derived else [index.datasets.get(id_)]\n        for d in to_process:\n            click.echo(\'archiving %s %s %s\' % (d.type.name, d.id,\n                                               d.local_uri if d.local_uri is not None else ""<no location>""))\n        if not dry_run:\n            index.datasets.archive(d.id for d in to_process)\n\n\n@dataset_cmd.command(\'restore\', help=""Restore datasets"")\n@click.option(\'--restore-derived\', \'-d\', help=\'Also recursively restore derived datasets\', is_flag=True, default=False)\n@click.option(\'--dry-run\', help=""Don\'t restore. Display datasets that would get restored"",\n              is_flag=True, default=False)\n@click.option(\'--derived-tolerance-seconds\',\n              help=""Only restore derived datasets that were archived ""\n                   ""this recently to the original dataset"",\n              default=10 * 60)\n@click.argument(\'ids\', nargs=-1)\n@ui.pass_index()\ndef restore_cmd(index, restore_derived, derived_tolerance_seconds, dry_run, ids):\n    tolerance = datetime.timedelta(seconds=derived_tolerance_seconds)\n\n    for id_ in ids:\n        _restore_one(dry_run, id_, index, restore_derived, tolerance)\n\n\ndef _restore_one(dry_run: bool,\n                 id_: str,\n                 index: Index,\n                 restore_derived: bool,\n                 tolerance: datetime.timedelta) -> None:\n    target_dataset = index.datasets.get(id_)\n    to_process = _get_derived_set(index, id_) if restore_derived else {target_dataset}\n    _LOG.debug(""%s selected"", len(to_process))\n\n    # Only the already-archived ones.\n    to_process = {d for d in to_process if d.is_archived}\n    _LOG.debug(""%s selected are archived"", len(to_process))\n\n    def within_tolerance(dataset):\n        if not dataset.is_archived:\n            return False\n        t = target_dataset.archived_time\n        return (t - tolerance) <= dataset.archived_time <= (t + tolerance)\n\n    # Only those archived around the same time as the target.\n    if restore_derived and target_dataset.is_archived:\n        to_process = set(filter(within_tolerance, to_process))\n        _LOG.debug(""%s selected were archived within the tolerance"", len(to_process))\n\n    for d in to_process:\n        click.echo(\'restoring %s %s %s\' % (d.type.name, d.id, d.local_uri))\n    if not dry_run:\n        index.datasets.restore(d.id for d in to_process)\n'"
datacube/scripts/ingest.py,0,"b'import time\nimport logging\nimport click\nimport cachetools\nimport itertools\nimport sys\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom pandas import to_datetime\nfrom datetime import datetime\nfrom typing import Tuple\n\nimport datacube\nfrom datacube.api.core import Datacube\nfrom datacube.index.index import Index\nfrom datacube.model import DatasetType, Range, Measurement, IngestorConfig\nfrom datacube.utils import geometry\nfrom datacube.model.utils import make_dataset, xr_apply, datasets_to_doc\nfrom datacube.ui import click as ui\nfrom datacube.utils import read_documents\nfrom datacube.utils.documents import InvalidDocException\nfrom datacube.utils.uris import normalise_path\nfrom datacube.ui.task_app import check_existing_files, load_tasks as load_tasks_, save_tasks as save_tasks_\nfrom datacube.drivers import storage_writer_by_name\n\nfrom datacube.ui.click import cli\n\n_LOG = logging.getLogger(\'datacube-ingest\')\n\nFUSER_KEY = \'fuse_data\'\n\n\ndef polygon_from_sources_extents(sources, geobox):\n    sources_union = geometry.unary_union(source.extent.to_crs(geobox.crs) for source in sources)\n    valid_data = geobox.extent.intersection(sources_union)\n    resolution = min([abs(x) for x in geobox.resolution])\n    return valid_data.simplify(tolerance=resolution * 0.01)\n\n\ndef find_diff(input_type, output_type, index, **query):\n    from datacube.api.grid_workflow import GridWorkflow\n    workflow = GridWorkflow(index, output_type.grid_spec)\n\n    tiles_in = workflow.list_tiles(product=input_type.name, **query)\n    tiles_out = workflow.list_tiles(product=output_type.name, **query)\n\n    tasks = [{\'tile\': tile, \'tile_index\': key} for key, tile in tiles_in.items() if key not in tiles_out]\n    return tasks\n\n\ndef morph_dataset_type(source_type, config, index, storage_format):\n    output_metadata_type = source_type.metadata_type\n    if \'metadata_type\' in config:\n        output_metadata_type = index.metadata_types.get_by_name(config[\'metadata_type\'])\n\n    output_type = DatasetType(output_metadata_type, deepcopy(source_type.definition))\n    output_type.definition[\'name\'] = config[\'output_type\']\n    output_type.definition[\'managed\'] = True\n    output_type.definition[\'description\'] = config[\'description\']\n    output_type.definition[\'storage\'] = {k: v for (k, v) in config[\'storage\'].items()\n                                         if k in (\'crs\', \'tile_size\', \'resolution\', \'origin\')}\n\n    output_type.metadata_doc[\'format\'] = {\'name\': storage_format}\n\n    if \'metadata_type\' in config:\n        output_type.definition[\'metadata_type\'] = config[\'metadata_type\']\n\n    def morph_measurement(src_measurements, spec):\n        src_varname = spec.get(\'src_varname\',\n                               spec.get(\'name\', None))\n        assert src_varname is not None\n\n        measurement = src_measurements.get(src_varname, None)\n        if measurement is None:\n            raise ValueError(""No such variable in the source product: {}"".format(src_varname))\n\n        measurement.update({k: spec.get(k, measurement[k]) for k in (\'name\', \'nodata\', \'dtype\')})\n        return Measurement(**measurement)\n\n    output_type.definition[\'measurements\'] = [morph_measurement(output_type.measurements, spec)\n                                              for spec in config[\'measurements\']]\n    return output_type\n\n\ndef get_variable_params(config):\n    chunking = config[\'storage\'][\'chunking\']\n    chunking = [chunking[dim] for dim in config[\'storage\'][\'dimension_order\']]\n\n    variable_params = {}\n    for mapping in config[\'measurements\']:\n        varname = mapping[\'name\']\n        variable_params[varname] = {k: v for k, v in mapping.items() if k in {\'zlib\',\n                                                                              \'complevel\',\n                                                                              \'shuffle\',\n                                                                              \'fletcher32\',\n                                                                              \'contiguous\',\n                                                                              \'attrs\'}}\n        variable_params[varname][\'chunksizes\'] = chunking\n\n    return variable_params\n\n\ndef get_app_metadata(config_file):\n    doc = {\n        \'lineage\': {\n            \'algorithm\': {\n                \'name\': \'datacube-ingest\',\n                \'repo_url\': \'https://github.com/opendatacube/datacube-core.git\',\n                \'parameters\': {\'configuration_file\': config_file},\n                \'version\': datacube.__version__,\n            },\n        }\n    }\n    return doc\n\n\ndef get_filename(config, tile_index, sources, **kwargs):\n    file_path_template = str(Path(config[\'location\'], config[\'file_path_template\']))\n    time_format = \'%Y%m%d%H%M%S%f\'\n    return Path(file_path_template.format(\n        tile_index=tile_index,\n        start_time=to_datetime(sources.time.values[0]).strftime(time_format),\n        end_time=to_datetime(sources.time.values[-1]).strftime(time_format),\n        version=config[\'taskfile_utctime\'],\n        **kwargs))\n\n\ndef get_measurements(source_type, config):\n    def merge_measurement(measurement, spec):\n        measurement.update({k: spec.get(k) or measurement[k] for k in (\'nodata\', \'dtype\')})\n        return Measurement(**measurement)\n\n    return [merge_measurement(source_type.measurements[spec[\'src_varname\']].copy(), spec)\n            for spec in config[\'measurements\']]\n\n\ndef get_namemap(config):\n    return {spec[\'src_varname\']: spec[\'name\'] for spec in config[\'measurements\']}\n\n\ndef get_resampling(config):\n    """""" What resampling strategy to use for each input band\n    """"""\n    return {spec[\'src_varname\']: spec.get(\'resampling_method\') for spec in config[\'measurements\']}\n\n\ndef ensure_output_type(index: Index,\n                       config: dict,\n                       storage_format: str,\n                       allow_product_changes: bool = False) -> Tuple[DatasetType, DatasetType]:\n    """"""\n    Create the output product for the given ingest config if it doesn\'t already exist.\n\n    It will throw a ValueError if the config already exists but differs from the existing.\n    Set allow_product_changes=True to allow changes.\n    """"""\n    source_type = index.products.get_by_name(config[\'source_type\'])\n    if not source_type:\n        click.echo(""Source DatasetType %s does not exist"" % config[\'source_type\'])\n        click.get_current_context().exit(1)\n\n    output_type = morph_dataset_type(source_type, config, index, storage_format)\n    _LOG.info(\'Created DatasetType %s\', output_type.name)\n\n    existing = index.products.get_by_name(output_type.name)\n    if existing:\n        can_update, safe_changes, unsafe_changes = index.products.can_update(output_type)\n        if safe_changes or unsafe_changes:\n            if not allow_product_changes:\n                raise ValueError(""Ingest config differs from the existing output product, ""\n                                 ""but allow_product_changes=False"")\n            output_type = index.products.update(output_type)\n        else:\n            output_type = existing\n    else:\n        output_type = index.products.add(output_type)\n\n    return source_type, output_type\n\n\n@cachetools.cached(cache={}, key=lambda index, id_: id_)\ndef get_full_lineage(index, id_):\n    return index.datasets.get(id_, include_sources=True)\n\n\ndef load_config_from_file(path):\n    config_file = Path(path)\n    _, config = next(read_documents(config_file))\n    IngestorConfig.validate(config)\n    config[\'filename\'] = str(normalise_path(config_file))\n\n    return config\n\n\ndef create_task_list(index, output_type, year, source_type, config):\n    config[\'taskfile_utctime\'] = int(time.time())\n\n    query = {}\n    if year:\n        query[\'time\'] = Range(datetime(year=year[0], month=1, day=1), datetime(year=year[1] + 1, month=1, day=1))\n    if \'ingestion_bounds\' in config:\n        bounds = config[\'ingestion_bounds\']\n        query[\'x\'] = Range(bounds[\'left\'], bounds[\'right\'])\n        query[\'y\'] = Range(bounds[\'bottom\'], bounds[\'top\'])\n\n    tasks = find_diff(source_type, output_type, index, **query)\n    _LOG.info(\'%s tasks discovered\', len(tasks))\n\n    def check_valid(tile, tile_index):\n        if FUSER_KEY in config:\n            return True\n\n        require_fusing = [source for source in tile.sources.values if len(source) > 1]\n        if require_fusing:\n            _LOG.warning(\'Skipping %s - no ""%s"" specified in config: %s\', tile_index, FUSER_KEY, require_fusing)\n\n        return not require_fusing\n\n    def update_sources(sources):\n        return tuple(get_full_lineage(index, dataset.id) for dataset in sources)\n\n    def update_task(task):\n        tile = task[\'tile\']\n        for i in range(tile.sources.size):\n            tile.sources.values[i] = update_sources(tile.sources.values[i])\n        return task\n\n    tasks = (update_task(task) for task in tasks if check_valid(**task))\n    return tasks\n\n\ndef ingest_work(config, source_type, output_type, tile, tile_index):\n    # pylint: disable=too-many-locals\n    _LOG.info(\'Starting task %s\', tile_index)\n    driver = storage_writer_by_name(config[\'storage\'][\'driver\'])\n\n    if driver is None:\n        _LOG.error(\'Failed to load storage driver %s\', config[\'storage\'][\'driver\'])\n        raise ValueError(\'Something went wrong: no longer can find driver pointed by storage.driver option\')\n\n    namemap = get_namemap(config)\n    # TODO: get_measurements possibly changes dtype, not sure load_data would like that\n    measurements = get_measurements(source_type, config)\n    resampling = get_resampling(config)\n    variable_params = get_variable_params(config)\n    global_attributes = config[\'global_attributes\']\n\n    fuse_func = {\'copy\': None}[config.get(FUSER_KEY, \'copy\')]\n\n    datasets = tile.sources.sum().item()\n    for dataset in datasets:\n        if not dataset.uris:\n            _LOG.error(\'Locationless dataset found in the database: %r\', dataset)\n\n    data = Datacube.load_data(tile.sources, tile.geobox, measurements,\n                              resampling=resampling,\n                              fuse_func=fuse_func)\n\n    nudata = data.rename(namemap)\n    file_path = get_filename(config, tile_index, tile.sources)\n    file_uri = driver.mk_uri(file_path, config[\'storage\'])\n\n    def _make_dataset(labels, sources):\n        return make_dataset(product=output_type,\n                            sources=sources,\n                            extent=tile.geobox.extent,\n                            center_time=labels[\'time\'],\n                            uri=file_uri,\n                            app_info=get_app_metadata(config[\'filename\']),\n                            valid_data=polygon_from_sources_extents(sources, tile.geobox))\n\n    datasets = xr_apply(tile.sources, _make_dataset, dtype=\'O\')  # Store in Dataarray to associate Time -> Dataset\n    nudata[\'dataset\'] = datasets_to_doc(datasets)\n\n    variable_params[\'dataset\'] = {\n        \'chunksizes\': (1,),\n        \'zlib\': True,\n        \'complevel\': 9,\n    }\n\n    driver_data = driver.write_dataset_to_storage(nudata, file_uri,\n                                                  global_attributes=global_attributes,\n                                                  variable_params=variable_params,\n                                                  storage_config=config[\'storage\'])\n\n    if (driver_data is not None) and len(driver_data) > 0:\n        datasets.attrs[\'driver_data\'] = driver_data\n\n    _LOG.info(\'Finished task %s\', tile_index)\n\n    return datasets\n\n\ndef _index_datasets(index, results):\n    n = 0\n    for datasets in results:\n        extra_args = {}\n        # datasets is an xarray.DataArray\n        if \'driver_data\' in datasets.attrs:\n            extra_args[\'driver_data\'] = datasets.attrs[\'driver_data\']\n\n        for dataset in datasets.values:\n            if \'driver_data\' in extra_args:\n                dataset.metadata_doc[\'driver_data\'] = extra_args[\'driver_data\']\n            index.datasets.add(dataset, with_lineage=False, **extra_args)\n            n += 1\n    return n\n\n\ndef process_tasks(index, config, source_type, output_type, tasks, queue_size, executor):\n    # pylint: disable=too-many-locals\n    def submit_task(task):\n        _LOG.info(\'Submitting task: %s\', task[\'tile_index\'])\n        return executor.submit(ingest_work,\n                               config=config,\n                               source_type=source_type,\n                               output_type=output_type,\n                               **task)\n\n    pending = []\n\n    # Count of storage unit/s creation successful/failed\n    nc_successful = nc_failed = 0\n\n    # Count of storage unit/s indexed successfully or failed to index\n    index_successful = index_failed = 0\n\n    tasks = iter(tasks)\n\n    while True:\n        pending += [submit_task(task) for task in itertools.islice(tasks, queue_size)]\n        if len(pending) == 0:\n            break\n\n        nc_completed, failed, pending = executor.get_ready(pending)\n        nc_successful += len(nc_completed)\n\n        for future in failed:\n            try:\n                executor.result(future)\n            except Exception as err:  # pylint: disable=broad-except\n                _LOG.exception(\'Failed to create storage unit file (Exception: %s) \', str(err), exc_info=True)\n                nc_failed += 1\n\n        _LOG.info(\'Storage unit file creation status (Created_Count: %s, Failed_Count: %s)\',\n                  nc_successful,\n                  nc_failed)\n\n        if not nc_completed:\n            time.sleep(1)\n            continue\n\n        try:\n            # TODO: ideally we wouldn\'t block here indefinitely\n            # maybe limit gather to 50-100 results and put the rest into a index backlog\n            # this will also keep the queue full\n            results = executor.results(nc_completed)\n            index_successful += _index_datasets(index, results)\n        except Exception as err:  # pylint: disable=broad-except\n            _LOG.exception(\'Failed to index storage unit file (Exception: %s)\', str(err), exc_info=True)\n            index_failed += 1\n\n        _LOG.info(\'Storage unit files indexed (Successful: %s, Failed: %s)\', index_successful, index_failed)\n\n    return index_successful, index_failed\n\n\ndef _validate_year(ctx, param, value):\n    try:\n        if value is None:\n            return None\n        years = list(map(int, value.split(\'-\', 2)))\n        if len(years) == 1:\n            return years[0], years[0]\n        return tuple(years)\n    except ValueError:\n        raise click.BadParameter(\'year must be specified as a single year (eg 1996) \'\n                                 \'or as an inclusive range (eg 1996-2001)\')\n\n\ndef get_driver_from_config(config):\n    driver_name = config[\'storage\'][\'driver\']\n    driver = storage_writer_by_name(driver_name)\n    if driver is None:\n        click.echo(\'Failed to load requested storage driver: \' + driver_name)\n        sys.exit(2)\n    return driver\n\n\n@cli.command(\'ingest\', help=""Ingest datasets"")\n@click.option(\'--config-file\', \'-c\',\n              type=click.Path(exists=True, readable=True, writable=False, dir_okay=False),\n              help=\'Ingest configuration file\')\n@click.option(\'--year\', callback=_validate_year, help=\'Limit the process to a particular year\')\n@click.option(\'--queue-size\', type=click.IntRange(1, 100000), default=3200, help=\'Task queue size\')\n@click.option(\'--save-tasks\', help=\'Save tasks to the specified file\',\n              type=click.Path(exists=False))\n@click.option(\'--load-tasks\', help=\'Load tasks from the specified file\',\n              type=click.Path(exists=True, readable=True, writable=False, dir_okay=False))\n@click.option(\'--dry-run\', \'-d\', is_flag=True, default=False, help=\'Check if everything is ok\')\n@click.option(\'--allow-product-changes\', is_flag=True, default=False,\n              help=\'Allow the output product definition to be updated if it differs.\')\n@ui.executor_cli_options\n@ui.pass_index(app_name=\'datacube-ingest\')\ndef ingest_cmd(index,\n               config_file,\n               year,\n               queue_size,\n               save_tasks,\n               load_tasks,\n               dry_run,\n               allow_product_changes,\n               executor):\n    # pylint: disable=too-many-locals\n\n    try:\n        if config_file:\n            config, tasks = load_config_from_file(config_file), None\n        elif load_tasks:\n            config, tasks = load_tasks_(load_tasks)\n        else:\n            click.echo(\'Must specify exactly one of --config-file, --load-tasks\')\n            sys.exit(-1)\n\n    except InvalidDocException as e:\n        exception, = e.args\n        _LOG.error(exception.message)\n        sys.exit(-1)\n\n    driver = get_driver_from_config(config)\n\n    try:\n        source_type, output_type = ensure_output_type(index, config, driver.format,\n                                                      allow_product_changes=allow_product_changes)\n    except ValueError as e:\n        _LOG.error(str(e))\n        sys.exit(-1)\n\n    if tasks is None:\n        tasks = create_task_list(index, output_type, year, source_type, config)\n\n    if dry_run:\n        check_existing_files(get_filename(config, task[\'tile_index\'], task[\'tile\'].sources) for task in tasks)\n    elif save_tasks:\n        save_tasks_(config, tasks, save_tasks)\n    else:\n        successful, failed = process_tasks(index, config, source_type, output_type, tasks, queue_size, executor)\n        click.echo(\'%d successful, %d failed\' % (successful, failed))\n\n        sys.exit(failed)\n'"
datacube/scripts/metadata.py,0,"b'\nimport json\nimport logging\nimport sys\nfrom typing import List\n\nimport yaml\nfrom pathlib import Path\n\nimport click\nfrom click import echo, style\n\nfrom datacube.index.index import Index\nfrom datacube.ui import click as ui\nfrom datacube.ui.click import cli\nfrom datacube.utils import read_documents, InvalidDocException\nfrom datacube.utils.serialise import SafeDatacubeDumper\n\n_LOG = logging.getLogger(\'datacube-md-type\')\n\n\n@cli.group(name=\'metadata\', help=\'Metadata type commands\')\ndef this_group():\n    pass\n\n\n@this_group.command(\'add\')\n@click.option(\'--allow-exclusive-lock/--forbid-exclusive-lock\', is_flag=True, default=False,\n              help=\'Allow index to be locked from other users while updating (default: false)\')\n@click.argument(\'files\',\n                type=str,\n                nargs=-1)\n@ui.pass_index()\ndef add_metadata_types(index, allow_exclusive_lock, files):\n    # type: (Index, bool, list) -> None\n    """"""\n    Add or update metadata types in the index\n    """"""\n    for descriptor_path, parsed_doc in read_documents(*files):\n        try:\n            type_ = index.metadata_types.from_doc(parsed_doc)\n            index.metadata_types.add(type_, allow_table_lock=allow_exclusive_lock)\n        except InvalidDocException as e:\n            _LOG.exception(e)\n            _LOG.error(\'Invalid metadata type definition: %s\', descriptor_path)\n            continue\n\n\n@this_group.command(\'update\')\n@click.option(\n    \'--allow-unsafe/--forbid-unsafe\', is_flag=True, default=False,\n    help=""Allow unsafe updates (default: false)""\n)\n@click.option(\'--allow-exclusive-lock/--forbid-exclusive-lock\', is_flag=True, default=False,\n              help=\'Allow index to be locked from other users while updating (default: false)\')\n@click.option(\'--dry-run\', \'-d\', is_flag=True, default=False,\n              help=\'Check if everything is ok\')\n@click.argument(\'files\', type=str, nargs=-1)\n@ui.pass_index()\ndef update_metadata_types(index: Index, allow_unsafe: bool, allow_exclusive_lock: bool, dry_run: bool, files: List):\n    """"""\n    Update existing metadata types.\n\n    An error will be thrown if a change is potentially unsafe.\n\n    (An unsafe change is anything that may potentially make the metadata type\n    incompatible with existing ones of the same name)\n    """"""\n    for descriptor_path, parsed_doc in read_documents(*files):\n        try:\n            type_ = index.metadata_types.from_doc(parsed_doc)\n        except InvalidDocException as e:\n            _LOG.exception(e)\n            _LOG.error(\'Invalid metadata type definition: %s\', descriptor_path)\n            continue\n\n        if not dry_run:\n            index.metadata_types.update(\n                type_,\n                allow_unsafe_updates=allow_unsafe,\n                allow_table_lock=allow_exclusive_lock,\n            )\n            echo(\'Updated ""%s""\' % type_.name)\n        else:\n            can_update, safe_changes, unsafe_changes = index.metadata_types.can_update(\n                type_, allow_unsafe_updates=allow_unsafe\n            )\n            if can_update:\n                echo(\'Can update ""%s"": %s unsafe changes, %s safe changes\' % (type_.name,\n                                                                              len(unsafe_changes),\n                                                                              len(safe_changes)))\n            else:\n                echo(\'Cannot update ""%s"": %s unsafe changes, %s safe changes\' % (type_.name,\n                                                                                 len(unsafe_changes),\n                                                                                 len(safe_changes)))\n\n\n@this_group.command(\'show\')\n@click.option(\'-f\', \'output_format\', help=\'Output format\',\n              type=click.Choice([\'yaml\', \'json\']), default=\'yaml\', show_default=True)\n@click.argument(\'metadata_type_name\', nargs=-1)\n@ui.pass_index()\ndef show_metadata_type(index, metadata_type_name, output_format):\n    """"""\n    Show information about a metadata type.\n    """"""\n\n    if len(metadata_type_name) == 0:\n        mm = list(index.metadata_types.get_all())\n    else:\n        mm = []\n        for name in metadata_type_name:\n            m = index.metadata_types.get_by_name(name)\n            if m is None:\n                echo(\'No such metadata: {!r}\'.format(name), err=True)\n                sys.exit(1)\n            else:\n                mm.append(m)\n\n    if len(mm) == 0:\n        echo(\'No metadata\')\n        sys.exit(1)\n\n    if output_format == \'yaml\':\n        yaml.dump_all((m.definition for m in mm),\n                      sys.stdout,\n                      Dumper=SafeDatacubeDumper,\n                      default_flow_style=False,\n                      indent=4)\n    elif output_format == \'json\':\n        if len(mm) > 1:\n            echo(\'Can not output more than 1 metadata document in json format\', err=True)\n            sys.exit(1)\n        m = mm[0]\n        echo(json.dumps(m.definition, indent=4))\n\n\n@this_group.command(\'list\')\n@ui.pass_index()\ndef list_metadata_types(index):\n    """"""\n    List metadata types that are defined in the generic index.\n    """"""\n    metadata_types = list(index.metadata_types.get_all())\n\n    if not metadata_types:\n        echo(\'No metadata types found :(\', err=True)\n        sys.exit(1)\n\n    max_w = max(len(m.name) for m in metadata_types)\n    for m in metadata_types:\n        description_short = m.definition.get(\'description\', \'\').split(\'\\n\')[0]\n        name = \'{s:<{n}}\'.format(s=m.name, n=max_w)\n        echo(style(name, fg=\'green\') + \'  \' + description_short)\n'"
datacube/scripts/product.py,0,"b'import csv\nimport json\nimport logging\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport click\nimport signal\nimport pandas as pd\nimport yaml\nimport yaml.resolver\nfrom click import echo, style\n\nfrom datacube.index.index import Index\nfrom datacube.ui import click as ui\nfrom datacube.ui.click import cli\nfrom datacube.utils import read_documents, InvalidDocException\nfrom datacube.utils.serialise import SafeDatacubeDumper\n\n_LOG = logging.getLogger(\'datacube-product\')\n\n\n@cli.group(name=\'product\', help=\'Product commands\')\ndef product_cli():\n    pass\n\n\n@product_cli.command(\'add\')\n@click.option(\'--allow-exclusive-lock/--forbid-exclusive-lock\', is_flag=True, default=False,\n              help=\'Allow index to be locked from other users while updating (default: false)\')\n@click.argument(\'files\',\n                type=str,\n                nargs=-1)\n@ui.pass_index()\ndef add_products(index, allow_exclusive_lock, files):\n    # type: (Index, bool, list) -> None\n    """"""\n    Add or update products in the generic index.\n    """"""\n    def on_ctrlc(sig, frame):\n        echo(f\'\'\'Can not abort `product add` without leaving database in bad state.\n\nThis operation requires constructing a bunch of indexes and this takes time, the\nbigger your database the longer it will take. Just wait a bit.\'\'\')\n\n    signal.signal(signal.SIGINT, on_ctrlc)\n\n    for descriptor_path, parsed_doc in read_documents(*files):\n        try:\n            type_ = index.products.from_doc(parsed_doc)\n            echo(f\'Adding ""{type_.name}"" (this might take a while)\', nl=False)\n            index.products.add(type_, allow_table_lock=allow_exclusive_lock)\n            echo(\' DONE\')\n        except InvalidDocException as e:\n            _LOG.exception(e)\n            _LOG.error(\'Invalid product definition: %s\', descriptor_path)\n            sys.exit(1)\n\n\n@product_cli.command(\'update\')\n@click.option(\n    \'--allow-unsafe/--forbid-unsafe\', is_flag=True, default=False,\n    help=""Allow unsafe updates (default: false)""\n)\n@click.option(\'--allow-exclusive-lock/--forbid-exclusive-lock\', is_flag=True, default=False,\n              help=\'Allow index to be locked from other users while updating (default: false)\')\n@click.option(\'--dry-run\', \'-d\', is_flag=True, default=False,\n              help=\'Check if everything is ok\')\n@click.argument(\'files\', type=str, nargs=-1)\n@ui.pass_index()\ndef update_products(index: Index, allow_unsafe: bool, allow_exclusive_lock: bool, dry_run: bool, files: List):\n    """"""\n    Update existing products.\n\n    An error will be thrown if a change is potentially unsafe.\n\n    (An unsafe change is anything that may potentially make the product\n    incompatible with existing datasets of that type)\n    """"""\n    failures = 0\n    for descriptor_path, parsed_doc in read_documents(*files):\n        try:\n            type_ = index.products.from_doc(parsed_doc)\n        except InvalidDocException as e:\n            _LOG.exception(e)\n            _LOG.error(\'Invalid product definition: %s\', descriptor_path)\n            failures += 1\n            continue\n\n        if not dry_run:\n            try:\n                index.products.update(\n                    type_,\n                    allow_unsafe_updates=allow_unsafe,\n                    allow_table_lock=allow_exclusive_lock,\n                )\n                echo(\'Updated ""%s""\' % type_.name)\n            except ValueError as e:\n                echo(\'Failed to update ""%s"": %s\' % (type_.name, e))\n                failures += 1\n        else:\n            can_update, safe_changes, unsafe_changes = index.products.can_update(\n                type_,\n                allow_unsafe_updates=allow_unsafe\n            )\n\n            if can_update:\n                echo(\'Can update ""%s"": %s unsafe changes, %s safe changes\' % (type_.name,\n                                                                              len(unsafe_changes),\n                                                                              len(safe_changes)))\n            else:\n                echo(\'Cannot update ""%s"": %s unsafe changes, %s safe changes\' % (type_.name,\n                                                                                 len(unsafe_changes),\n                                                                                 len(safe_changes)))\n    sys.exit(failures)\n\n\ndef _write_csv(products):\n    product_dicts = [prod.to_dict() for prod in products]\n    writer = csv.DictWriter(sys.stdout, [\'id\', \'name\', \'description\',\n                                         \'ancillary_quality\', \'latgqa_cep90\', \'product_type\',\n                                         \'gqa_abs_iterative_mean_xy\', \'gqa_ref_source\', \'sat_path\',\n                                         \'gqa_iterative_stddev_xy\', \'time\', \'sat_row\', \'orbit\', \'gqa\',\n                                         \'instrument\', \'gqa_abs_xy\', \'crs\', \'resolution\', \'tile_size\',\n                                         \'spatial_dimensions\'], extrasaction=\'ignore\')\n    writer.writeheader()\n    writer.writerows(product_dicts)\n\n\ndef _write_yaml(products):\n    """"""\n    Dump yaml data with support for OrderedDicts.\n\n    Allows for better human-readability of output: such as dataset ID field first, sources last.\n\n    (Ordered dicts are output identically to normal yaml dicts: their order is purely for readability)\n    """"""\n    product_dicts = [prod.to_dict() for prod in products]\n\n    return yaml.dump_all(product_dicts, sys.stdout, Dumper=SafeDatacubeDumper, default_flow_style=False, indent=4)\n\n\ndef _write_tab(products):\n    df = pd.DataFrame(prod.to_dict() for prod in products)\n\n    if df.empty:\n        echo(\'No products discovered :(\')\n        return\n\n    output_columns=(\'id\', \'name\', \'description\', \'ancillary_quality\',\n                    \'product_type\', \'gqa_abs_iterative_mean_xy\',\n                    \'gqa_ref_source\', \'sat_path\',\n                    \'gqa_iterative_stddev_xy\', \'time\', \'sat_row\',\n                    \'orbit\', \'gqa\', \'instrument\', \'gqa_abs_xy\', \'crs\',\n                    \'resolution\', \'tile_size\', \'spatial_dimensions\')\n    # If the intersection of desired columns with available columns is empty, just use whatever IS in df\n    output_columns=tuple(col for col in output_columns if col in df.columns) or df.columns\n    echo(df.to_string(columns=output_columns,justify=\'left\',index=False))\n\n\ndef _default_lister(products):\n    products = list(products)\n    if len(products) == 0:\n        return\n\n    max_w = max(len(p.name) for p in products)\n\n    for prod in products:\n        name = \'{s:<{n}}\'.format(s=prod.name, n=max_w)\n        echo(style(name, fg=\'green\') + \'  \' + prod.definition.get(\'description\', \'\'))\n\n\nLIST_OUTPUT_WRITERS = {\n    \'default\': _default_lister,\n    \'csv\': _write_csv,\n    \'yaml\': _write_yaml,\n    \'tab\': _write_tab,\n}\n\n\n@product_cli.command(\'list\')\n@click.option(\'-f\', \'output_format\', help=\'Output format\',\n              type=click.Choice(list(LIST_OUTPUT_WRITERS)), default=\'default\', show_default=True)\n@ui.pass_datacube()\ndef list_products(dc, output_format):\n    """"""\n    List products that are defined in the generic index.\n    """"""\n    products = dc.index.products.search()\n\n    writer = LIST_OUTPUT_WRITERS[output_format]\n\n    writer(products)\n\n\n@product_cli.command(\'show\')\n@click.option(\'-f\', \'output_format\', help=\'Output format\',\n              type=click.Choice([\'yaml\', \'json\']), default=\'yaml\', show_default=True)\n@click.argument(\'product_name\', nargs=-1)\n@ui.pass_datacube()\ndef show_product(dc, product_name, output_format):\n    """"""\n    Show details about a product in the generic index.\n    """"""\n\n    if len(product_name) == 0:\n        products = list(dc.index.products.get_all())\n    else:\n        products = []\n        for name in product_name:\n            p = dc.index.products.get_by_name(name)\n            if p is None:\n                echo(\'No such product: {!r}\'.format(name), err=True)\n                sys.exit(1)\n            else:\n                products.append(p)\n\n    if len(products) == 0:\n        echo(\'No products\', err=True)\n        sys.exit(1)\n\n    if output_format == \'yaml\':\n        yaml.dump_all((p.definition for p in products),\n                      sys.stdout,\n                      Dumper=SafeDatacubeDumper,\n                      default_flow_style=False,\n                      indent=4)\n    elif output_format == \'json\':\n        if len(products) > 1:\n            echo(\'Can not output more than 1 product in json format\', err=True)\n            sys.exit(1)\n        product, *_ = products\n        click.echo_via_pager(json.dumps(product.definition, indent=4))\n'"
datacube/scripts/search_tool.py,0,"b'#!/usr/bin/env python\n# coding=utf-8\n""""""\nQuery datasets.\n""""""\n\nimport csv\nimport datetime\nimport sys\nfrom functools import partial\n\nimport click\nfrom dateutil import tz\nfrom psycopg2._range import Range\nfrom functools import singledispatch\n\nfrom datacube.ui import click as ui\nfrom datacube.ui.click import CLICK_SETTINGS\n\nPASS_INDEX = ui.pass_index(\'datacube-search\')\n\n\ndef printable_values(d):\n    return {k: printable(v) for k, v in d.items()}\n\n\ndef write_pretty(out_f, field_names, search_results, terminal_size=click.get_terminal_size()):\n    """"""\n    Output in a human-readable text format. Inspired by psql\'s expanded output.\n    """"""\n    terminal_width = terminal_size[0]\n    record_num = 1\n\n    field_header_width = max(len(name) for name in field_names)\n    field_output_format = \'{:<\' + str(field_header_width) + \'} | {}\'\n\n    for result in search_results:\n        separator_line = \'-[ {} ]\'.format(record_num)\n        separator_line += \'-\' * (terminal_width - len(separator_line) - 1)\n        click.echo(separator_line, file=out_f)\n\n        for name, value in sorted(result.items()):\n            click.echo(\n                field_output_format.format(name, printable(value)),\n                file=out_f\n            )\n\n        record_num += 1\n\n\ndef write_csv(out_f, field_names, search_results):\n    """"""\n    Output as a CSV.\n    """"""\n    writer = csv.DictWriter(out_f, tuple(sorted(field_names)))\n    writer.writeheader()\n    writer.writerows(\n        (\n            printable_values(d) for d in\n            search_results\n        )\n    )\n\n\nOUTPUT_FORMATS = {\n    \'csv\': write_csv,\n    \'pretty\': write_pretty\n}\n\n\n@click.group(help=""Search the Data Cube"", context_settings=CLICK_SETTINGS)\n@ui.global_cli_options\n@click.option(\'-f\',\n              type=click.Choice(list(OUTPUT_FORMATS)),\n              default=\'pretty\', show_default=True,\n              help=\'Output format\')\n@click.pass_context\ndef cli(ctx, f):\n    ctx.obj[\'write_results\'] = partial(OUTPUT_FORMATS[f], sys.stdout)\n\n\n@cli.command()\n@ui.parsed_search_expressions\n@PASS_INDEX\n@click.pass_context\ndef datasets(ctx, index, expressions):\n    """"""\n    Search available Datasets\n    """"""\n    ctx.obj[\'write_results\'](\n        sorted(index.datasets.get_field_names()),\n        index.datasets.search_summaries(**expressions)\n    )\n\n\n@cli.command(\'product-counts\')\n@click.argument(\'period\', nargs=1)\n@ui.parsed_search_expressions\n@PASS_INDEX\ndef product_counts(index, period, expressions):\n    """"""\n    Count product Datasets available by period\n\n    PERIOD: eg. 1 month, 6 months, 1 year\n    """"""\n    for product, series in index.datasets.count_by_product_through_time(period, **expressions):\n        click.echo(product.name)\n        for timerange, count in series:\n            formatted_dt = _assume_utc(timerange[0]).strftime(""%Y-%m-%d"")\n            click.echo(\'    {}: {}\'.format(formatted_dt, count))\n\n\n@singledispatch\ndef printable(val):\n    return val\n\n\n@printable.register(type(None))\ndef printable_none(val):\n    return \'\'\n\n\n@printable.register(datetime.datetime)\ndef printable_dt(val):\n    """"""\n    :type val: datetime.datetime\n    """"""\n    return _assume_utc(val).isoformat()\n\n\ndef _assume_utc(val):\n    if val.tzinfo is None:\n        return val.replace(tzinfo=tz.tzutc())\n    else:\n        return val.astimezone(tz.tzutc())\n\n\n@printable.register(Range)\ndef printable_r(val):\n    """"""\n    :type val: psycopg2._range.Range\n    """"""\n    if val.lower_inf:\n        return printable(val.upper)\n    if val.upper_inf:\n        return printable(val.lower)\n\n    return \'{} to {}\'.format(printable(val.lower), printable(val.upper))\n\n\nif __name__ == \'__main__\':\n    cli()\n'"
datacube/scripts/system.py,0,"b'import logging\n\nimport click\nfrom click import echo, style\nfrom sqlalchemy.exc import OperationalError\n\nimport datacube\nfrom datacube.index import index_connect\nfrom datacube.drivers.postgres._connections import IndexSetupError\nfrom datacube.ui import click as ui\nfrom datacube.ui.click import cli, handle_exception\nfrom datacube.config import LocalConfig\n\n_LOG = logging.getLogger(\'datacube-system\')\n\n\n@cli.group(name=\'system\', help=\'System commands\')\ndef system():\n    pass\n\n\n@system.command(\'init\', help=\'Initialise the database\')\n@click.option(\n    \'--default-types/--no-default-types\', is_flag=True, default=True,\n    help=""Add default types? (default: true)""\n)\n@click.option(\n    \'--init-users/--no-init-users\', is_flag=True, default=True,\n    help=""Include user roles and grants. (default: true)""\n)\n@click.option(\n    \'--recreate-views/--no-recreate-views\', is_flag=True, default=True,\n    help=""Recreate dynamic views""\n)\n@click.option(\n    \'--rebuild/--no-rebuild\', is_flag=True, default=False,\n    help=""Rebuild all dynamic fields (caution: slow)""\n)\n@click.option(\n    \'--lock-table/--no-lock-table\', is_flag=True, default=False,\n    help=""Allow table to be locked (eg. while creating missing indexes)""\n)\n@ui.pass_index(expect_initialised=False)\ndef database_init(index, default_types, init_users, recreate_views, rebuild, lock_table):\n    echo(\'Initialising database...\')\n\n    was_created = index.init_db(with_default_types=default_types,\n                                with_permissions=init_users)\n\n    if was_created:\n        echo(style(\'Created.\', bold=True))\n    else:\n        echo(style(\'Updated.\', bold=True))\n\n    echo(\'Checking indexes/views.\')\n    index.metadata_types.check_field_indexes(\n        allow_table_lock=lock_table,\n        rebuild_indexes=rebuild,\n        rebuild_views=recreate_views or rebuild,\n    )\n    echo(\'Done.\')\n\n\n@system.command(\'check\', help=\'Check and display current configuration\')\n@ui.pass_config\ndef check(local_config: LocalConfig):\n    """"""\n    Verify & view current configuration\n    """"""\n\n    def echo_field(name, value):\n        echo(\'{:<15}\'.format(name + \':\') + style(str(value), bold=True))\n\n    echo_field(\'Version\', datacube.__version__)\n    echo_field(\'Config files\', \',\'.join(local_config.files_loaded))\n    echo_field(\'Host\',\n               \'{}:{}\'.format(local_config[\'db_hostname\'] or \'localhost\', local_config.get(\'db_port\', None) or \'5432\'))\n\n    echo_field(\'Database\', local_config[\'db_database\'])\n    echo_field(\'User\', local_config[\'db_username\'])\n    echo_field(\'Environment\', local_config[\'env\'])\n    echo_field(\'Index Driver\', local_config[\'index_driver\'])\n\n    echo()\n    echo(\'Valid connection:\\t\', nl=False)\n    try:\n        index = index_connect(local_config=local_config)\n        echo(style(\'YES\', bold=True))\n        for role, user, description in index.users.list_users():\n            if user == local_config[\'db_username\']:\n                echo(\'You have %s privileges.\' % style(role.upper(), bold=True))\n    except OperationalError as e:\n        handle_exception(\'Error Connecting to Database: %s\', e)\n    except IndexSetupError as e:\n        handle_exception(\'Database not initialised: %s\', e)\n'"
datacube/scripts/user.py,0,"b'\nimport logging\nimport click\nimport csv\nimport sys\nimport yaml\nimport yaml.resolver\n\nfrom collections import OrderedDict\n\nfrom datacube.utils import gen_password\nfrom datacube.config import LocalConfig\nfrom datacube.index.index import Index\nfrom datacube.ui import click as ui\nfrom datacube.ui.click import cli\nfrom datacube.utils.serialise import SafeDatacubeDumper\n\n_LOG = logging.getLogger(\'datacube-user\')\nUSER_ROLES = (\'user\', \'ingest\', \'manage\', \'admin\')\n\n\n@cli.group(name=\'user\', help=\'User management commands\')\ndef user_cmd():\n    pass\n\n\ndef build_user_list(index):\n    lstdct = []\n    for role, user, description in index.users.list_users():\n        info = OrderedDict((\n            (\'role\', role),\n            (\'user\', user),\n            (\'description\', description)\n        ))\n        lstdct.append(info)\n    return lstdct\n\n\ndef _write_csv(index):\n    writer = csv.DictWriter(sys.stdout, [\'role\', \'user\', \'description\'], extrasaction=\'ignore\')\n    writer.writeheader()\n\n    def add_first_role(row):\n        roles_ = row[\'role\']\n        row[\'role\'] = roles_ if roles_ else None\n        return row\n\n    writer.writerows(add_first_role(row) for row in index)\n\n\ndef _write_yaml(index):\n    """"""\n    Dump yaml data with support for OrderedDicts.\n\n    Allows for better human-readability of output: such as dataset ID field first, sources last.\n\n    (Ordered dicts are output identically to normal yaml dicts: their order is purely for readability)\n    """"""\n\n    return yaml.dump_all(index, sys.stdout, SafeDatacubeDumper, default_flow_style=False, indent=4)\n\n\n_OUTPUT_WRITERS = {\n    \'csv\': _write_csv,\n    \'yaml\': _write_yaml,\n}\n\n\n@user_cmd.command(\'list\')\n@click.option(\'-f\', help=\'Output format\',\n              type=click.Choice(list(_OUTPUT_WRITERS)), default=\'yaml\', show_default=True)\n@ui.pass_index()\ndef list_users(index, f):\n    """"""\n    List users\n    """"""\n    _OUTPUT_WRITERS[f](build_user_list(index))\n\n\n@user_cmd.command(\'grant\')\n@click.argument(\'role\',\n                type=click.Choice(USER_ROLES),\n                nargs=1)\n@click.argument(\'users\', nargs=-1)\n@ui.pass_index()\ndef grant(index, role, users):\n    """"""\n    Grant a role to users\n    """"""\n    index.users.grant_role(role, *users)\n\n\n@user_cmd.command(\'create\')\n@click.argument(\'role\',\n                type=click.Choice(USER_ROLES), nargs=1)\n@click.argument(\'user\', nargs=1)\n@click.option(\'--description\')\n@ui.pass_index()\n@ui.pass_config\ndef create_user(config, index, role, user, description):\n    # type: (LocalConfig, Index, str, str, str) -> None\n    """"""\n    Create a User\n    """"""\n    password = gen_password(12)\n    index.users.create_user(user, password, role, description=description)\n\n    click.echo(\'{host}:{port}:*:{username}:{password}\'.format(\n        host=config.get(\'db_hostname\', None) or \'localhost\',\n        port=config.get(\'db_port\', None),\n        username=user,\n        password=password\n    ))\n\n\n@user_cmd.command(\'delete\')\n@click.argument(\'users\', nargs=-1)\n@ui.pass_index()\n@ui.pass_config\ndef delete_user(config, index, users):\n    """"""\n    Delete a User\n    """"""\n    index.users.delete_user(*users)\n'"
datacube/storage/__init__.py,0,"b'# coding=utf-8\n""""""\nModules for creating and accessing Data Store Units\n\n\n""""""\n\nfrom ..drivers.datasource import (\n    DataSource,\n    GeoRasterReader,\n    RasterShape,\n    RasterWindow)\n\nfrom ._base import BandInfo, measurement_paths\nfrom ._load import reproject_and_fuse\n\n__all__ = (\n    \'BandInfo\',\n    \'DataSource\',\n    \'GeoRasterReader\',\n    \'RasterShape\',\n    \'RasterWindow\',\n    \'measurement_paths\',\n    \'reproject_and_fuse\',\n)\n'"
datacube/storage/_base.py,0,"b'from typing import Optional, Dict, Any, Tuple\nfrom urllib.parse import urlparse\n\nfrom datacube.model import Dataset\nfrom datacube.utils.uris import uri_resolve, pick_uri\n\n\ndef _get_band_and_layer(b: Dict[str, Any]) -> Tuple[Optional[int], Optional[str]]:\n    """""" Encode legacy logic for extracting band/layer:\n\n        on input:\n        band -- Int | Nothing\n        layer -- Str | Int | Nothing\n\n    Valid combinations are:\n        band  layer  Output\n    ---------------------------\n          -     -    ( - ,  - )\n          -    int   (int,  - )\n         int    -    (int,  - )\n         int   str   (int, str)\n          -    str   ( - , str)\n\n    """"""\n    band = b.get(\'band\')\n    layer = b.get(\'layer\')\n\n    if band is None:\n        if isinstance(layer, int):\n            return (layer, None)\n        if layer is None or isinstance(layer, str):\n            return (None, layer)\n\n        raise ValueError(\'Expect `layer` to be one of None,int,str but it is {}\'.format(type(layer)))\n    else:\n        if not isinstance(band, int):\n            raise ValueError(\'Expect `band` to be an integer (it is {})\'.format(type(band)))\n        if layer is not None and not isinstance(layer, str):\n            raise ValueError(\'Expect `layer` to be one of None,str but it is {}\'.format(type(layer)))\n\n        return (band, layer)\n\n\ndef _extract_driver_data(ds: Dataset) -> Optional[Any]:\n    return ds.metadata_doc.get(\'driver_data\', None)\n\ndef measurement_paths(ds: Dataset) -> Dict[str, str]:\n    """"""\n    Returns a dictionary mapping from band name to url pointing to band storage\n    resource.\n\n    :return: Band Name => URL\n    """"""\n    if ds.uris is None:\n        raise ValueError(\'No locations on this dataset\')\n\n    base = pick_uri(ds.uris)\n    return dict((k, uri_resolve(base, m.get(\'path\')))\n                for k, m in ds.measurements.items())\n\n\nclass BandInfo:\n    __slots__ = (\'name\',\n                 \'uri\',\n                 \'band\',\n                 \'layer\',\n                 \'dtype\',\n                 \'nodata\',\n                 \'units\',\n                 \'crs\',\n                 \'transform\',\n                 \'center_time\',\n                 \'format\',\n                 \'driver_data\')\n\n    def __init__(self,\n                 ds: Dataset,\n                 band: str,\n                 uri_scheme: Optional[str] = None):\n        try:\n            canonical_name = ds.type.canonical_measurement(band)\n        except KeyError:\n            raise ValueError(\'No such band: {}\'.format(band))\n\n        mm = ds.measurements.get(canonical_name)\n        mp = ds.type.measurements.get(canonical_name)\n\n        if mm is None or mp is None:\n            raise ValueError(\'No such band: {}\'.format(band))\n\n        if ds.uris is None:\n            raise ValueError(\'No uris defined on a dataset\')\n\n        base_uri = pick_uri(ds.uris, uri_scheme)\n\n        bint, layer = _get_band_and_layer(mm)\n\n        self.name = band\n        self.uri = uri_resolve(base_uri, mm.get(\'path\'))\n        self.band = bint\n        self.layer = layer\n        self.dtype = mp.dtype\n        self.nodata = mp.nodata\n        self.units = mp.units\n        self.crs = ds.crs\n        self.transform = ds.transform\n        self.format = ds.format\n        self.driver_data = _extract_driver_data(ds)\n\n    @property\n    def uri_scheme(self) -> str:\n        return urlparse(self.uri).scheme\n'"
datacube/storage/_hdf5.py,0,b'from threading import RLock\nHDF5_LOCK = RLock()\n'
datacube/storage/_load.py,8,"b'""""""\nImportant functions are:\n\n* :func:`reproject_and_fuse`\n\n""""""\nimport logging\nfrom collections import OrderedDict\nimport numpy as np\nfrom xarray.core.dataarray import DataArray as XrDataArray, DataArrayCoordinates\nfrom xarray.core.dataset import Dataset as XrDataset\nfrom typing import (\n    Union, Optional, Callable,\n    List, Any, Iterator, Iterable, Mapping, Tuple, Hashable, cast\n)\n\nfrom datacube.utils import ignore_exceptions_if\nfrom datacube.utils.math import invalid_mask\nfrom datacube.utils.geometry import GeoBox, roi_is_empty\nfrom datacube.model import Measurement\nfrom datacube.drivers._types import ReaderDriver\nfrom . import DataSource, BandInfo\n\n_LOG = logging.getLogger(__name__)\n\nFuserFunction = Callable[[np.ndarray, np.ndarray], Any]  # pylint: disable=invalid-name\nProgressFunction = Callable[[int, int], Any]  # pylint: disable=invalid-name\n\n\ndef _default_fuser(dst: np.ndarray, src: np.ndarray, dst_nodata) -> None:\n    """""" Overwrite only those pixels in `dst` with `src` that are ""not valid""\n\n        For every pixel in dst that equals to dst_nodata replace it with pixel\n        from src.\n    """"""\n    np.copyto(dst, src, where=invalid_mask(dst, dst_nodata))\n\n\ndef reproject_and_fuse(datasources: List[DataSource],\n                       destination: np.ndarray,\n                       dst_gbox: GeoBox,\n                       dst_nodata: Optional[Union[int, float]],\n                       resampling: str = \'nearest\',\n                       fuse_func: Optional[FuserFunction] = None,\n                       skip_broken_datasets: bool = False,\n                       progress_cbk: Optional[ProgressFunction] = None):\n    """"""\n    Reproject and fuse `sources` into a 2D numpy array `destination`.\n\n    :param datasources: Data sources to open and read from\n    :param destination: ndarray of appropriate size to read data into\n    :param dst_gbox: GeoBox defining destination region\n    :param skip_broken_datasets: Carry on in the face of adversity and failing reads.\n    :param progress_cbk: If supplied will be called with 2 integers `Items processed, Total Items`\n                         after reading each file.\n    """"""\n    # pylint: disable=too-many-locals\n    from ._read import read_time_slice\n    assert len(destination.shape) == 2\n\n    def copyto_fuser(dest: np.ndarray, src: np.ndarray) -> None:\n        _default_fuser(dest, src, dst_nodata)\n\n    fuse_func = fuse_func or copyto_fuser\n\n    destination.fill(dst_nodata)\n    if len(datasources) == 0:\n        return destination\n    elif len(datasources) == 1:\n        with ignore_exceptions_if(skip_broken_datasets):\n            with datasources[0].open() as rdr:\n                read_time_slice(rdr, destination, dst_gbox, resampling, dst_nodata)\n\n        if progress_cbk:\n            progress_cbk(1, 1)\n\n        return destination\n    else:\n        # Multiple sources, we need to fuse them together into a single array\n        buffer_ = np.full(destination.shape, dst_nodata, dtype=destination.dtype)\n        for n_so_far, source in enumerate(datasources, 1):\n            with ignore_exceptions_if(skip_broken_datasets):\n                with source.open() as rdr:\n                    roi = read_time_slice(rdr, buffer_, dst_gbox, resampling, dst_nodata)\n\n                if not roi_is_empty(roi):\n                    fuse_func(destination[roi], buffer_[roi])\n                    buffer_[roi] = dst_nodata  # clean up for next read\n\n            if progress_cbk:\n                progress_cbk(n_so_far, len(datasources))\n\n        return destination\n\n\ndef _mk_empty_ds(coords: DataArrayCoordinates, geobox: GeoBox) -> XrDataset:\n    cc = OrderedDict(coords.items())\n    cc.update(geobox.xr_coords())\n    return XrDataset(coords=cast(Mapping[Hashable, Any], cc), attrs={\'crs\': geobox.crs})\n\n\ndef _allocate_storage(coords: DataArrayCoordinates,\n                      geobox: GeoBox,\n                      measurements: Iterable[Measurement]) -> XrDataset:\n    xx = _mk_empty_ds(coords, geobox)\n    dims = list(xx.coords.keys())\n    shape = tuple(xx.sizes[k] for k in dims)\n\n    for m in measurements:\n        name, dtype, attrs = m.name, m.dtype, m.dataarray_attrs()\n        attrs[\'crs\'] = geobox.crs\n        data = np.empty(shape, dtype=dtype)\n        xx[name] = XrDataArray(data, coords=xx.coords, dims=dims, name=name, attrs=attrs)\n\n    return xx\n\n\ndef xr_load(sources: XrDataArray,\n            geobox: GeoBox,\n            measurements: List[Measurement],\n            driver: ReaderDriver,\n            driver_ctx_prev: Optional[Any] = None,\n            skip_broken_datasets: bool = False) -> Tuple[XrDataset, Any]:\n    # pylint: disable=too-many-locals\n    from ._read import read_time_slice_v2\n\n    out = _allocate_storage(sources.coords, geobox, measurements)\n\n    def all_groups() -> Iterator[Tuple[Measurement, int, List[BandInfo]]]:\n        for idx, dss in np.ndenumerate(sources.values):\n            for m in measurements:\n                bbi = [BandInfo(ds, m.name) for ds in dss]\n                yield (m, idx, bbi)\n\n    def just_bands(groups) -> Iterator[BandInfo]:\n        for _, _, bbi in groups:\n            yield from bbi\n\n    groups = list(all_groups())\n    ctx = driver.new_load_context(just_bands(groups), driver_ctx_prev)\n\n    # TODO: run upto N concurrently\n    for m, idx, bbi in groups:\n        dst = out.data_vars[m.name].values[idx]\n        dst[:] = m.nodata\n        resampling = m.get(\'resampling_method\', \'nearest\')\n        fuse_func = m.get(\'fuser\', None)\n\n        for band in bbi:\n            rdr = driver.open(band, ctx).result()\n\n            pix, roi = read_time_slice_v2(rdr, geobox, resampling, m.nodata)\n\n            if pix is not None:\n                if fuse_func:\n                    fuse_func(dst[roi], pix)\n                else:\n                    _default_fuser(dst[roi], pix, m.nodata)\n\n    return out, ctx\n'"
datacube/storage/_read.py,7,"b'"""""" Dataset -> Raster\n""""""\nfrom affine import Affine\nimport numpy as np\nfrom typing import Tuple\n\nfrom ..utils.math import is_almost_int, valid_mask\n\nfrom ..utils.geometry import (\n    roi_shape,\n    roi_is_empty,\n    roi_is_full,\n    roi_pad,\n    GeoBox,\n    w_,\n    warp_affine,\n    rio_reproject,\n    compute_reproject_roi)\n\nfrom ..utils.geometry._warp import is_resampling_nn, Resampling, Nodata\nfrom ..utils.geometry import gbox as gbx\n\n\ndef rdr_geobox(rdr) -> GeoBox:\n    """""" Construct GeoBox from opened dataset reader.\n    """"""\n    h, w = rdr.shape\n    return GeoBox(w, h, rdr.transform, rdr.crs)\n\n\ndef can_paste(rr, stol=1e-3, ttol=1e-2):\n    """"""\n    Take result of compute_reproject_roi and check if can read(possibly with scale) and paste,\n    or do we need to read then reproject.\n\n    :returns: (True, None) if one can just read and paste\n    :returns: (False, Reason) if pasting is not possible, so need to reproject after reading\n    """"""\n    if not rr.is_st:  # not linear or not Scale + Translation\n        return False, ""not ST""\n\n    scale = rr.scale\n    if not is_almost_int(scale, stol):  # non-integer scaling\n        return False, ""non-integer scale""\n\n    scale = np.round(scale)\n    A = rr.transform.linear           # src -> dst\n    A = A*Affine.scale(scale, scale)  # src.overview[scale] -> dst\n\n    (sx, _, tx,  # tx, ty are in dst pixel space\n     _, sy, ty,\n     *_) = A\n\n    if any(abs(abs(s) - 1) > stol\n           for s in (sx, sy)):  # not equal scaling across axis?\n        return False, ""sx!=sy, probably""\n\n    ny, nx = (n/scale\n              for n in roi_shape(rr.roi_src))\n\n    # src_roi doesn\'t divide by scale properly:\n    #  example 3x7 scaled down by factor of 2\n    if not all(is_almost_int(n, stol) for n in (nx, ny)):\n        return False, ""src_roi doesn\'t align for scale""\n\n    # TODO: probably need to deal with sub-pixel translation here, if we want\n    # to ignore sub-pixel translation and dst roi is 1 pixel bigger than src it\n    # should still be ok to paste after cropping dst roi by one pixel on the\n    # appropriate side. As it stands sub-pixel translation will be ignored only\n    # in some cases.\n\n    # scaled down shape doesn\'t match dst shape\n    s_shape = (int(ny), int(nx))\n    if s_shape != roi_shape(rr.roi_dst):\n        return False, ""src_roi/scale != dst_roi""\n\n    # final check: sub-pixel translation\n    if not all(is_almost_int(t, ttol) for t in (tx, ty)):\n        return False, ""sub-pixel translation""\n\n    return True, None\n\n\ndef pick_read_scale(scale: float, rdr=None, tol=1e-3):\n    assert scale > 0\n    # First find nearest integer scale\n    #    Scale down to nearest integer, unless we can scale up by less than tol\n    #\n    # 2.999999 -> 3\n    # 2.8 -> 2\n    # 0.3 -> 1\n\n    if scale < 1:\n        return 1\n\n    if is_almost_int(scale, tol):\n        scale = np.round(scale)\n\n    scale = int(scale)\n\n    if rdr is not None:\n        # TODO: check available overviews in rdr\n        pass\n\n    return scale\n\n\ndef read_time_slice(rdr,\n                    dst: np.ndarray,\n                    dst_gbox: GeoBox,\n                    resampling: Resampling,\n                    dst_nodata: Nodata) -> Tuple[slice, slice]:\n    """""" From opened reader object read into `dst`\n\n    :returns: affected destination region\n    """"""\n    assert dst.shape == dst_gbox.shape\n    src_gbox = rdr_geobox(rdr)\n\n    rr = compute_reproject_roi(src_gbox, dst_gbox)\n\n    if roi_is_empty(rr.roi_dst):\n        return rr.roi_dst\n\n    is_nn = is_resampling_nn(resampling)\n    scale = pick_read_scale(rr.scale, rdr)\n\n    paste_ok, _ = can_paste(rr, ttol=0.9 if is_nn else 0.01)\n\n    def norm_read_args(roi, shape):\n        if roi_is_full(roi, rdr.shape):\n            roi = None\n\n        if roi is None and shape == rdr.shape:\n            shape = None\n\n        return w_[roi], shape\n\n    if paste_ok:\n        A = rr.transform.linear\n        sx, sy = A.a, A.e\n\n        dst = dst[rr.roi_dst]\n        pix = rdr.read(*norm_read_args(rr.roi_src, dst.shape))\n\n        if sx < 0:\n            pix = pix[:, ::-1]\n        if sy < 0:\n            pix = pix[::-1, :]\n\n        if rdr.nodata is None:\n            np.copyto(dst, pix)\n        else:\n            np.copyto(dst, pix, where=valid_mask(pix, rdr.nodata))\n    else:\n        if rr.is_st:\n            # add padding on src/dst ROIs, it was set to tight bounds\n            # TODO: this should probably happen inside compute_reproject_roi\n            rr.roi_dst = roi_pad(rr.roi_dst, 1, dst_gbox.shape)\n            rr.roi_src = roi_pad(rr.roi_src, 1, src_gbox.shape)\n\n        dst = dst[rr.roi_dst]\n        dst_gbox = dst_gbox[rr.roi_dst]\n        src_gbox = src_gbox[rr.roi_src]\n        if scale > 1:\n            src_gbox = gbx.zoom_out(src_gbox, scale)\n\n        pix = rdr.read(*norm_read_args(rr.roi_src, src_gbox.shape))\n\n        if rr.transform.linear is not None:\n            A = (~src_gbox.transform)*dst_gbox.transform\n            warp_affine(pix, dst, A, resampling,\n                        src_nodata=rdr.nodata, dst_nodata=dst_nodata)\n        else:\n            rio_reproject(pix, dst, src_gbox, dst_gbox, resampling,\n                          src_nodata=rdr.nodata, dst_nodata=dst_nodata)\n\n    return rr.roi_dst\n\n\ndef read_time_slice_v2(rdr,\n                       dst_gbox: GeoBox,\n                       resampling: Resampling,\n                       dst_nodata: Nodata) -> Tuple[np.ndarray,\n                                                    Tuple[slice, slice]]:\n    """""" From opened reader object read into `dst`\n\n    :returns: pixels read and ROI of dst_gbox that was affected\n    """"""\n    # pylint: disable=too-many-locals\n    src_gbox = rdr_geobox(rdr)\n\n    rr = compute_reproject_roi(src_gbox, dst_gbox)\n\n    if roi_is_empty(rr.roi_dst):\n        return None, rr.roi_dst\n\n    is_nn = is_resampling_nn(resampling)\n    scale = pick_read_scale(rr.scale, rdr)\n\n    paste_ok, _ = can_paste(rr, ttol=0.9 if is_nn else 0.01)\n\n    def norm_read_args(roi, shape):\n        if roi_is_full(roi, rdr.shape):\n            roi = None\n\n        if roi is None and shape == rdr.shape:\n            shape = None\n\n        return roi, shape\n\n    if paste_ok:\n        read_shape = roi_shape(rr.roi_dst)\n        A = rr.transform.linear\n        sx, sy = A.a, A.e\n\n        pix = rdr.read(*norm_read_args(rr.roi_src, read_shape)).result()\n\n        if sx < 0:\n            pix = pix[:, ::-1]\n        if sy < 0:\n            pix = pix[::-1, :]\n\n        # normalise nodata to be equal to `dst_nodata`\n        if rdr.nodata is not None and rdr.nodata != dst_nodata:\n            pix[pix == rdr.nodata] = dst_nodata\n\n        dst = pix\n    else:\n        if rr.is_st:\n            # add padding on src/dst ROIs, it was set to tight bounds\n            # TODO: this should probably happen inside compute_reproject_roi\n            rr.roi_dst = roi_pad(rr.roi_dst, 1, dst_gbox.shape)\n            rr.roi_src = roi_pad(rr.roi_src, 1, src_gbox.shape)\n\n        dst_gbox = dst_gbox[rr.roi_dst]\n        src_gbox = src_gbox[rr.roi_src]\n        if scale > 1:\n            src_gbox = gbx.zoom_out(src_gbox, scale)\n\n        dst = np.full(dst_gbox.shape, dst_nodata, dtype=rdr.dtype)\n        pix = rdr.read(*norm_read_args(rr.roi_src, src_gbox.shape)).result()\n\n        if rr.transform.linear is not None:\n            A = (~src_gbox.transform)*dst_gbox.transform\n            warp_affine(pix, dst, A, resampling,\n                        src_nodata=rdr.nodata, dst_nodata=dst_nodata)\n        else:\n            rio_reproject(pix, dst, src_gbox, dst_gbox, resampling,\n                          src_nodata=rdr.nodata, dst_nodata=dst_nodata)\n\n    return dst, rr.roi_dst\n'"
datacube/storage/_rio.py,6,"b'# coding=utf-8\n""""""\nDriver implementation for Rasterio based reader.\n""""""\nimport logging\nimport warnings\nimport contextlib\nfrom contextlib import contextmanager\nfrom threading import RLock\nimport numpy as np\nfrom affine import Affine\nimport rasterio\nfrom urllib.parse import urlparse\nfrom typing import Optional, Iterator\n\nfrom datacube.utils import geometry\nfrom datacube.utils.math import num2numpy\nfrom datacube.utils import uri_to_local_path, get_part_from_uri, is_vsipath\nfrom datacube.utils.rio import activate_from_config\nfrom . import DataSource, GeoRasterReader, RasterShape, RasterWindow, BandInfo\nfrom ._hdf5 import HDF5_LOCK\n\n_LOG = logging.getLogger(__name__)\n\n\ndef _rasterio_crs(src):\n    if src.crs is None:\n        raise ValueError(\'no CRS\')\n\n    return geometry.CRS(src.crs)\n\n\ndef maybe_lock(lock):\n    if lock is None:\n        return contextlib.suppress()\n    return lock\n\n\nclass BandDataSource(GeoRasterReader):\n    """"""\n    Wrapper for a :class:`rasterio.Band` object\n\n    :type source: rasterio.Band\n    """"""\n\n    def __init__(self, source, nodata=None,\n                 lock: Optional[RLock] = None):\n        self.source = source\n        if nodata is None:\n            nodata = self.source.ds.nodatavals[self.source.bidx-1]\n\n        self._nodata = num2numpy(nodata, source.dtype)\n        self._lock = lock\n\n    @property\n    def nodata(self):\n        return self._nodata\n\n    @property\n    def crs(self) -> geometry.CRS:\n        return _rasterio_crs(self.source.ds)\n\n    @property\n    def transform(self) -> Affine:\n        return self.source.ds.transform\n\n    @property\n    def dtype(self) -> np.dtype:\n        return np.dtype(self.source.dtype)\n\n    @property\n    def shape(self) -> RasterShape:\n        return self.source.shape\n\n    def read(self, window: Optional[RasterWindow] = None,\n             out_shape: Optional[RasterShape] = None) -> Optional[np.ndarray]:\n        """"""Read data in the native format, returning a numpy array\n        """"""\n        with maybe_lock(self._lock):\n            return self.source.ds.read(indexes=self.source.bidx, window=window, out_shape=out_shape)\n\n\nclass OverrideBandDataSource(GeoRasterReader):\n    """"""Wrapper for a rasterio.Band object that overrides nodata, CRS and transform\n\n    This is useful for files with malformed or missing properties.\n\n\n    :type source: rasterio.Band\n    """"""\n\n    def __init__(self,\n                 source: rasterio.Band,\n                 nodata,\n                 crs: geometry.CRS,\n                 transform: Affine,\n                 lock: Optional[RLock] = None):\n        self.source = source\n        self._nodata = num2numpy(nodata, source.dtype)\n        self._crs = crs\n        self._transform = transform\n        self._lock = lock\n\n    @property\n    def crs(self) -> geometry.CRS:\n        return self._crs\n\n    @property\n    def transform(self) -> Affine:\n        return self._transform\n\n    @property\n    def nodata(self):\n        return self._nodata\n\n    @property\n    def dtype(self) -> np.dtype:\n        return np.dtype(self.source.dtype)\n\n    @property\n    def shape(self) -> RasterShape:\n        return self.source.shape\n\n    def read(self, window: Optional[RasterWindow] = None,\n             out_shape: Optional[RasterShape] = None) -> Optional[np.ndarray]:\n        """"""Read data in the native format, returning a native array\n        """"""\n        with maybe_lock(self._lock):\n            return self.source.ds.read(indexes=self.source.bidx, window=window, out_shape=out_shape)\n\n\nclass RasterioDataSource(DataSource):\n    """"""\n    Abstract class used by fuse_sources and :func:`read_from_source`\n\n    """"""\n\n    def __init__(self, filename, nodata, lock=None):\n        self.filename = filename\n        self.nodata = nodata\n        self._lock = lock\n\n    def get_bandnumber(self, src):\n        raise NotImplementedError()\n\n    def get_transform(self, shape):\n        raise NotImplementedError()\n\n    def get_crs(self):\n        raise NotImplementedError()\n\n    @contextmanager\n    def open(self) -> Iterator[GeoRasterReader]:\n        """"""Context manager which returns a :class:`BandDataSource`""""""\n\n        activate_from_config()  # check if settings changed and apply new\n\n        lock = self._lock\n        locked = False if lock is None else lock.acquire(blocking=True)\n\n        try:\n            _LOG.debug(""opening %s"", self.filename)\n            with rasterio.open(self.filename, sharing=False) as src:\n                override = False\n\n                transform = src.transform\n                if transform.is_identity:\n                    override = True\n                    transform = self.get_transform(src.shape)\n\n                try:\n                    crs = _rasterio_crs(src)\n                except ValueError:\n                    override = True\n                    crs = self.get_crs()\n\n                bandnumber = self.get_bandnumber(src)\n                band = rasterio.band(src, bandnumber)\n                nodata = src.nodatavals[band.bidx-1] if src.nodatavals[band.bidx-1] is not None else self.nodata\n                nodata = num2numpy(nodata, band.dtype)\n\n                if locked:\n                    locked = False\n                    lock.release()\n\n                if override:\n                    warnings.warn(f""""""Broken/missing geospatial data was found in file:\n""{self.filename}""\nWill use approximate metadata for backwards compatibility reasons (#673).\nThis behaviour is deprecated. Future versions will raise an error."""""",\n                                  category=DeprecationWarning)\n                    yield OverrideBandDataSource(band, nodata=nodata, crs=crs, transform=transform, lock=lock)\n                else:\n                    yield BandDataSource(band, nodata=nodata, lock=lock)\n\n        except Exception as e:\n            _LOG.error(""Error opening source dataset: %s"", self.filename)\n            raise e\n        finally:\n            if locked:\n                lock.release()\n\n\nclass RasterDatasetDataSource(RasterioDataSource):\n    """"""Data source for reading from a Data Cube Dataset""""""\n\n    def __init__(self, band: BandInfo):\n        """"""\n        Initialise for reading from a Data Cube Dataset.\n\n        :param dataset: dataset to read from\n        :param measurement_id: measurement to read. a single \'band\' or \'slice\'\n        """"""\n        self._band_info = band\n        self._hdf = _is_hdf(band.format)\n        self._part = get_part_from_uri(band.uri)\n        filename = _url2rasterio(band.uri, band.format, band.layer)\n        lock = HDF5_LOCK if self._hdf else None\n        super(RasterDatasetDataSource, self).__init__(filename, nodata=band.nodata, lock=lock)\n\n    def get_bandnumber(self, src=None) -> Optional[int]:\n\n        # If `band` property is set to an integer it overrides any other logic\n        bi = self._band_info\n        if bi.band is not None:\n            return bi.band\n\n        if not self._hdf:\n            return 1\n\n        # Netcdf/hdf only below\n        if self._part is not None:\n            return self._part + 1  # Convert to rasterio 1-based indexing\n\n        if src is None:\n            # File wasnt\' open, could be unstacked file in a new format, or\n            # stacked/unstacked in old. We assume caller knows what to do\n            # (maybe based on some side-channel information), so just report\n            # undefined.\n            return None\n\n        if src.count == 1:  # Single-slice netcdf file\n            return 1\n\n        raise DeprecationWarning(""Stacked netcdf without explicit time index is not supported anymore"")\n\n    def get_transform(self, shape: RasterShape) -> Affine:\n        return self._band_info.transform * Affine.scale(1 / shape[1], 1 / shape[0])\n\n    def get_crs(self):\n        return self._band_info.crs\n\n\ndef _is_hdf(fmt: str) -> bool:\n    """""" Check if format is of HDF type (this includes netcdf variants)\n    """"""\n    fmt = fmt.lower()\n    return any(f in fmt for f in (\'netcdf\', \'hdf\'))\n\n\ndef _build_hdf_uri(url_str: str, fmt: str, layer: str) -> str:\n    if is_vsipath(url_str):\n        base = url_str\n    else:\n        url = urlparse(url_str)\n        if url.scheme in (None, \'\'):\n            raise ValueError(""Expect either URL or /vsi path"")\n\n        if url.scheme != \'file\':\n            raise RuntimeError(""Can\'t access %s over %s"" % (fmt, url.scheme))\n        base = str(uri_to_local_path(url_str))\n\n    return \'{}:""{}"":{}\'.format(fmt, base, layer)\n\n\ndef _url2rasterio(url_str: str, fmt: str, layer: Optional[str]) -> str:\n    """"""\n    turn URL into a string that could be passed to raterio.open\n    """"""\n    if _is_hdf(fmt):\n        if layer is None:\n            raise ValueError(""Missing layer for hdf/netcdf format dataset"")\n\n        return _build_hdf_uri(url_str, fmt, layer)\n\n    if is_vsipath(url_str):\n        return url_str\n\n    url = urlparse(url_str)\n    if url.scheme in (None, \'\'):\n        raise ValueError(""Expect either URL or /vsi path"")\n\n    if url.scheme == \'file\':\n        # if local path strip scheme and other gunk\n        return str(uri_to_local_path(url_str))\n\n    return url_str\n'"
datacube/storage/masking.py,0,"b'import warnings\n\nwarnings.warn(""datacube.storage.masking has moved to datacube.utils.masking"",\n              category=DeprecationWarning)\n\nfrom datacube.utils.masking import *\n'"
datacube/testutils/__init__.py,5,"b'# coding=utf-8\n""""""\nUseful methods for tests (particularly: reading/writing and checking files)\n""""""\nimport atexit\nimport os\nimport shutil\nimport tempfile\nimport json\nimport uuid\nimport numpy as np\nimport xarray as xr\nfrom datetime import datetime\nfrom collections.abc import Sequence, Mapping\nimport pathlib\n\nfrom affine import Affine\nfrom datacube import Datacube\nfrom datacube.model import Measurement\nfrom datacube.utils.dates import mk_time_coord\nfrom datacube.utils.documents import parse_yaml\nfrom datacube.model import Dataset, DatasetType, MetadataType\nfrom datacube.ui.common import get_metadata_path\nfrom datacube.utils import read_documents, SimpleDocNav\nfrom datacube.utils.geometry import GeoBox, CRS\n\nfrom datacube.model.fields import parse_search_field\n\n_DEFAULT = object()\n\n\ndef assert_file_structure(folder, expected_structure, root=\'\'):\n    """"""\n    Assert that the contents of a folder (filenames and subfolder names recursively)\n    match the given nested dictionary structure.\n\n    :type folder: pathlib.Path\n    :type expected_structure: dict[str,str|dict]\n    """"""\n\n    expected_filenames = set(expected_structure.keys())\n    actual_filenames = {f.name for f in folder.iterdir()}\n\n    if expected_filenames != actual_filenames:\n        missing_files = expected_filenames - actual_filenames\n        missing_text = \'Missing: %r\' % (sorted(list(missing_files)))\n        extra_files = actual_filenames - expected_filenames\n        added_text = \'Extra  : %r\' % (sorted(list(extra_files)))\n        raise AssertionError(\'Folder mismatch of %r\\n\\t%s\\n\\t%s\' % (root, missing_text, added_text))\n\n    for k, v in expected_structure.items():\n        id_ = \'%s/%s\' % (root, k) if root else k\n\n        f = folder.joinpath(k)\n        if isinstance(v, Mapping):\n            assert f.is_dir(), ""%s is not a dir"" % (id_,)\n            assert_file_structure(f, v, id_)\n        elif isinstance(v, (str, Sequence)):\n            assert f.is_file(), ""%s is not a file"" % (id_,)\n        else:\n            assert False, ""Only strings|[strings] and dicts expected when defining a folder structure.""\n\n\ndef write_files(file_dict):\n    """"""\n    Convenience method for writing a bunch of files to a temporary directory.\n\n    Dict format is ""filename"": ""text content""\n\n    If content is another dict, it is created recursively in the same manner.\n\n    writeFiles({\'test.txt\': \'contents of text file\'})\n\n    :type file_dict: dict\n    :rtype: pathlib.Path\n    :return: Created temporary directory path\n    """"""\n    containing_dir = tempfile.mkdtemp(suffix=\'neotestrun\')\n    _write_files_to_dir(containing_dir, file_dict)\n\n    def remove_if_exists(path):\n        if os.path.exists(path):\n            shutil.rmtree(path)\n\n    atexit.register(remove_if_exists, containing_dir)\n    return pathlib.Path(containing_dir)\n\n\ndef _write_files_to_dir(directory_path, file_dict):\n    """"""\n    Convenience method for writing a bunch of files to a given directory.\n\n    :type directory_path: str\n    :type file_dict: dict\n    """"""\n    for filename, contents in file_dict.items():\n        path = os.path.join(directory_path, filename)\n        if isinstance(contents, Mapping):\n            os.mkdir(path)\n            _write_files_to_dir(path, contents)\n        else:\n            with open(path, \'w\') as f:\n                if isinstance(contents, str):\n                    f.write(contents)\n                elif isinstance(contents, Sequence):\n                    f.writelines(contents)\n                else:\n                    raise ValueError(\'Unexpected file contents: %s\' % type(contents))\n\n\ndef isclose(a, b, rel_tol=1e-09, abs_tol=0.0):\n    """"""\n    Testing aproximate equality for floats\n    See https://docs.python.org/3/whatsnew/3.5.html#pep-485-a-function-for-testing-approximate-equality\n    """"""\n    return abs(a - b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)\n\n\ndef geobox_to_gridspatial(geobox):\n    if geobox is None:\n        return {}\n\n    l, b, r, t = geobox.extent.boundingbox\n    return {""grid_spatial"": {\n        ""projection"": {\n            ""geo_ref_points"": {\n                ""ll"": {""x"": l, ""y"": b},\n                ""lr"": {""x"": r, ""y"": b},\n                ""ul"": {""x"": l, ""y"": t},\n                ""ur"": {""x"": r, ""y"": t}},\n            ""spatial_reference"": str(geobox.crs)}}}\n\n\ndef mk_sample_eo(name=\'eo\'):\n    eo_yaml = f""""""\nname: {name}\ndescription: Sample\ndataset:\n    id: [\'id\']\n    label: [\'ga_label\']\n    creation_time: [\'creation_dt\']\n    measurements: [\'image\', \'bands\']\n    sources: [\'lineage\', \'source_datasets\']\n    format: [\'format\', \'name\']\n    grid_spatial: [\'grid_spatial\', \'projection\']\n    search_fields:\n       time:\n         type: \'datetime-range\'\n         min_offset: [[\'time\']]\n         max_offset: [[\'time\']]\n    """"""\n    return MetadataType(parse_yaml(eo_yaml))\n\n\ndef mk_sample_product(name,\n                      description=\'Sample\',\n                      measurements=(\'red\', \'green\', \'blue\'),\n                      with_grid_spec=False,\n                      metadata_type=None,\n                      storage=None):\n\n    if storage is None and with_grid_spec is True:\n        storage = {\'crs\': \'EPSG:3577\',\n                   \'resolution\': {\'x\': 25, \'y\': -25},\n                   \'tile_size\': {\'x\': 100000.0, \'y\': 100000.0}}\n\n    common = dict(dtype=\'int16\',\n                  nodata=-999,\n                  units=\'1\',\n                  aliases=[])\n\n    if metadata_type is None:\n        metadata_type = mk_sample_eo(\'eo\')\n\n    def mk_measurement(m):\n        if isinstance(m, str):\n            return dict(name=m, **common)\n        elif isinstance(m, tuple):\n            name, dtype, nodata = m\n            m = common.copy()\n            m.update(name=name, dtype=dtype, nodata=nodata)\n            return m\n        elif isinstance(m, dict):\n            m_merged = common.copy()\n            m_merged.update(m)\n            return m_merged\n        else:\n            raise ValueError(\'Only support str|dict|(name, dtype, nodata)\')\n\n    measurements = [mk_measurement(m) for m in measurements]\n\n    definition = dict(\n        name=name,\n        description=description,\n        metadata_type=metadata_type.name,\n        metadata={},\n        measurements=measurements\n    )\n\n    if storage is not None:\n        definition[\'storage\'] = storage\n\n    return DatasetType(metadata_type, definition)\n\n\ndef mk_sample_dataset(bands,\n                      uri=\'file:///tmp\',\n                      product_name=\'sample\',\n                      format=\'GeoTiff\',\n                      timestamp=None,\n                      id=\'3a1df9e0-8484-44fc-8102-79184eab85dd\',\n                      geobox=None,\n                      product_opts=None):\n    # pylint: disable=redefined-builtin\n    image_bands_keys = \'path layer band\'.split(\' \')\n    measurement_keys = \'dtype units nodata aliases name\'.split(\' \')\n\n    def with_keys(d, keys):\n        return dict((k, d[k]) for k in keys if k in d)\n\n    measurements = [with_keys(m, measurement_keys) for m in bands]\n    image_bands = dict((m[\'name\'], with_keys(m, image_bands_keys)) for m in bands)\n\n    if product_opts is None:\n        product_opts = {}\n\n    ds_type = mk_sample_product(product_name,\n                                measurements=measurements,\n                                **product_opts)\n\n    if timestamp is None:\n        timestamp = \'2018-06-29\'\n    if uri is None:\n        uris = []\n    elif isinstance(uri, list):\n        uris = uri.copy()\n    else:\n        uris = [uri]\n\n    return Dataset(ds_type, {\n        \'id\': id,\n        \'format\': {\'name\': format},\n        \'image\': {\'bands\': image_bands},\n        \'time\': timestamp,\n        **geobox_to_gridspatial(geobox),\n    }, uris=uris)\n\n\ndef make_graph_abcde(node):\n    """"""\n      A -> B\n      |    |\n      |    v\n      +--> C -> D\n      |\n      +--> E\n    """"""\n    d = node(\'D\')\n    e = node(\'E\')\n    c = node(\'C\', cd=d)\n    b = node(\'B\', bc=c)\n    a = node(\'A\', ab=b, ac=c, ae=e)\n    return a, b, c, d, e\n\n\ndef dataset_maker(idx, t=None):\n    """""" Return function that generates ""dataset documents""\n\n    (name, sources={}, **kwargs) -> dict\n    """"""\n    ns = uuid.UUID(\'c0fefefe-2470-3b03-803f-e7599f39ceff\')\n    postfix = \'\' if idx is None else \'{:04d}\'.format(idx)\n\n    if t is None:\n        t = datetime.fromordinal(736637 + (0 if idx is None else idx))\n\n    t = t.isoformat()\n\n    def make(name, sources=_DEFAULT, **kwargs):\n        if sources is _DEFAULT:\n            sources = {}\n\n        return dict(id=str(uuid.uuid5(ns, name + postfix)),\n                    label=name+postfix,\n                    creation_dt=t,\n                    n=idx,\n                    lineage=dict(source_datasets=sources),\n                    **kwargs)\n\n    return make\n\n\ndef gen_dataset_test_dag(idx, t=None, force_tree=False):\n    """"""Build document suitable for consumption by dataset add\n\n    when force_tree is True pump the object graph through json\n    serialise->deserialise, this converts DAG to a tree (no object sharing,\n    copies instead).\n    """"""\n    def node_maker(n, t):\n        mk = dataset_maker(n, t)\n\n        def node(name, **kwargs):\n            return mk(name,\n                      product_type=name,\n                      sources=kwargs)\n\n        return node\n\n    def deref(a):\n        return json.loads(json.dumps(a))\n\n    root, *_ = make_graph_abcde(node_maker(idx, t))\n    return deref(root) if force_tree else root\n\n\ndef load_dataset_definition(path):\n    if not isinstance(path, pathlib.Path):\n        path = pathlib.Path(path)\n\n    fname = get_metadata_path(path)\n    for _, doc in read_documents(fname):\n        return SimpleDocNav(doc)\n\n\ndef mk_test_image(w, h,\n                  dtype=\'int16\',\n                  nodata=-999,\n                  nodata_width=4):\n    """"""\n    Create 2d ndarray where each pixel value is formed by packing x coordinate in\n    to the upper half of the pixel value and y coordinate is in the lower part.\n\n    So for uint16: im[y, x] == (x<<8) | y IF abs(x-y) >= nodata_width\n                   im[y, x] == nodata     IF abs(x-y) < nodata_width\n\n    really it\'s actually: im[y, x] == ((x & 0xFF ) <<8) | (y & 0xFF)\n\n    If dtype is of floating point type:\n       im[y, x] = (x + ((y%1024)/1024))\n\n    Pixels along the diagonal are set to nodata values (to disable set nodata_width=0)\n    """"""\n\n    dtype = np.dtype(dtype)\n\n    xx, yy = np.meshgrid(np.arange(w),\n                         np.arange(h))\n    if dtype.kind == \'f\':\n        aa = xx.astype(dtype) + (yy.astype(dtype) % 1024.0) / 1024.0\n    else:\n        nshift = dtype.itemsize*8//2\n        mask = (1 << nshift) - 1\n        aa = ((xx & mask) << nshift) | (yy & mask)\n        aa = aa.astype(dtype)\n\n    if nodata is not None:\n        aa[abs(xx-yy) < nodata_width] = nodata\n    return aa\n\n\ndef split_test_image(aa):\n    """"""\n    Separate image created by mk_test_image into x,y components\n    """"""\n    if aa.dtype.kind == \'f\':\n        y = np.round((aa % 1)*1024)\n        x = np.floor(aa)\n    else:\n        nshift = (aa.dtype.itemsize*8)//2\n        mask = (1 << nshift) - 1\n        y = aa & mask\n        x = aa >> nshift\n    return x, y\n\n\ndef gen_tiff_dataset(bands,\n                     base_folder,\n                     prefix=\'\',\n                     timestamp=\'2018-07-19\',\n                     **kwargs):\n    """"""\n       each band:\n         .name    - string\n         .values  - ndarray\n         .nodata  - numeric|None\n\n    :returns:  (Dataset, GeoBox)\n    """"""\n    from .io import write_gtiff\n    from pathlib import Path\n\n    if not isinstance(bands, Sequence):\n        bands = (bands,)\n\n    # write arrays to disk and construct compatible measurement definitions\n    gbox = None\n    mm = []\n    for band in bands:\n        name = band.name\n        fname = prefix + name + \'.tiff\'\n        meta = write_gtiff(base_folder/fname, band.values,\n                           nodata=band.nodata,\n                           overwrite=True,\n                           **kwargs)\n\n        gbox = meta.gbox\n\n        mm.append(dict(name=name,\n                       path=fname,\n                       layer=1,\n                       dtype=meta.dtype))\n\n    uri = Path(base_folder/\'metadata.yaml\').absolute().as_uri()\n    ds = mk_sample_dataset(mm,\n                           uri=uri,\n                           timestamp=timestamp,\n                           geobox=gbox)\n    return ds, gbox\n\n\ndef mk_sample_xr_dataset(crs=""EPSG:3578"",\n                         shape=(33, 74),\n                         resolution=None,\n                         xy=(0, 0),\n                         time=\'2020-02-13T11:12:13.1234567Z\',\n                         name=\'band\',\n                         dtype=\'int16\',\n                         nodata=-999,\n                         units=\'1\'):\n    """""" Note that resolution is in Y,X order to match that of GeoBox.\n\n        shape (height, width)\n        resolution (y: float, x: float) - in YX, to match GeoBox/shape notation\n\n        xy (x: float, y: float) -- location of the top-left corner of the top-left pixel in CRS units\n    """"""\n\n    if isinstance(crs, str):\n        crs = CRS(crs)\n\n    if resolution is None:\n        resolution = (-10, 10) if crs is None or crs.projected else (-0.01, 0.01)\n\n    t_coords = {}\n    if time is not None:\n        t_coords[\'time\'] = mk_time_coord([time])\n\n    transform = Affine.translation(*xy)*Affine.scale(*resolution[::-1])\n    h, w = shape\n    geobox = GeoBox(w, h, transform, crs)\n\n    return Datacube.create_storage(t_coords, geobox, [Measurement(name=name, dtype=dtype, nodata=nodata, units=units)])\n\n\ndef remove_crs(xx):\n    xx = xx.reset_coords([\'spatial_ref\'], drop=True)\n\n    xx.attrs.pop(\'crs\', None)\n    for x in xx.coords.values():\n        x.attrs.pop(\'crs\', None)\n\n    if isinstance(xx, xr.Dataset):\n        for x in xx.data_vars.values():\n            x.attrs.pop(\'crs\', None)\n\n    return xx\n'"
datacube/testutils/geom.py,17,"b'import numpy as np\nfrom affine import Affine\nfrom typing import Callable, Union, Tuple\n\nfrom datacube.utils.geometry import (\n    CRS,\n    GeoBox,\n    apply_affine,\n)\nfrom datacube.model import GridSpec\n\n# pylint: disable=invalid-name\n\nepsg4326 = CRS(\'EPSG:4326\')\nepsg3577 = CRS(\'EPSG:3577\')\nepsg3857 = CRS(\'EPSG:3857\')\n\nAlbersGS = GridSpec(crs=epsg3577,\n                    tile_size=(100000.0, 100000.0),\n                    resolution=(-25, 25),\n                    origin=(0.0, 0.0))\n\nSAMPLE_WKT_WITHOUT_AUTHORITY = \'\'\'PROJCS[""unnamed"",\n       GEOGCS[""unnamed ellipse"",\n              DATUM[""unknown"",\n                    SPHEROID[""unnamed"",6378137,0],\n                    EXTENSION[""PROJ4_GRIDS"",""@null""]],\n              PRIMEM[""Greenwich"",0],\n              UNIT[""degree"",0.0174532925199433]],\n       PROJECTION[""Mercator_2SP""],\n       PARAMETER[""standard_parallel_1"",0],\n       PARAMETER[""central_meridian"",0],\n       PARAMETER[""false_easting"",0],\n       PARAMETER[""false_northing"",0],\n       UNIT[""Meter"",1]\n]\n\'\'\'\n\n\ndef mkA(rot=0, scale=(1, 1), shear=0, translation=(0, 0)):\n    return Affine.translation(*translation)*Affine.rotation(rot)*Affine.shear(shear)*Affine.scale(*scale)\n\n\ndef xy_from_gbox(gbox: GeoBox) -> Tuple[np.ndarray, np.ndarray]:\n    """"""\n    :returns: Two images with X and Y coordinates for centers of pixels\n    """"""\n    h, w = gbox.shape\n\n    xx, yy = np.meshgrid(np.arange(w, dtype=\'float64\') + 0.5,\n                         np.arange(h, dtype=\'float64\') + 0.5)\n\n    return apply_affine(gbox.transform, xx, yy)\n\n\ndef xy_norm(x: np.ndarray, y: np.ndarray,\n            deg: float = 33.0) -> Tuple[np.ndarray, np.ndarray, Affine]:\n    """"""\n    Transform output of xy_from_geobox with a reversible linear transform. On\n    output x,y are in [0,1] range. Reversible Affine transform includes\n    rotation by default, this is to ensure that test images don\'t have\n    symmetries that are aligned to X/Y axis.\n\n    1. Rotate x,y by ``deg``\n    2. Offset and scale such that values are in [0, 1] range\n\n\n    :returns: (x\', y\', A)\n\n    - (x, y) == A*(x\', y\')\n    - [x|y]\'.min() == 0\n    - [x|y]\'.max() == 1\n\n    """"""\n\n    def norm_v(v):\n        vmin = v.min()\n        v -= vmin\n        s = 1.0/v.max()\n        v *= s\n\n        return (s, -vmin*s)\n\n    A_rot = Affine.rotation(deg)\n    x, y = apply_affine(A_rot, x, y)\n\n    sx, tx = norm_v(x)\n    sy, ty = norm_v(y)\n\n    A = Affine(sx, 0, tx,\n               0, sy, ty)*A_rot\n\n    return x, y, ~A\n\n\ndef to_fixed_point(a, dtype=\'uint16\'):\n    """"""\n    Convert normalised ([0,1]) floating point image to integer fixed point fractional.\n\n    Note for signed types: there is no offset, 0 -> 0, 1 -> (2**(nbits - 1) - 1).\n\n    Reverse is provided by: ``from_fixed_point``\n    """"""\n    ii = np.iinfo(dtype)\n    a = a*ii.max + 0.5\n    a = np.clip(a, 0, ii.max, out=a)\n    return a.astype(ii.dtype)\n\n\ndef from_fixed_point(a):\n    """"""\n    Convert fixed point image to floating point\n\n    This is reverse of ``to_fixed_point``\n    """"""\n    ii = np.iinfo(a.dtype)\n    return a.astype(\'float64\')*(1.0/ii.max)\n\n\ndef gen_test_image_xy(gbox: GeoBox,\n                      dtype: Union[str, np.dtype, type] = \'float32\',\n                      deg: float = 33.0) -> Tuple[np.ndarray, Callable]:\n    """"""\n    Generate test image that captures pixel coordinates in pixel values.\n    Useful for testing reprojections/reads.\n\n    :param gbox: GeoBox defining pixel plane\n\n    :dtype: data type of the image, defaults to `float32`, but it can be an\n            integer type in which case normalised coordinates will be\n            quantised increasing error.\n\n    :returns: 2xWxH ndarray encoding X,Y coordinates of pixel centers in some\n              normalised space, and a callable that can convert from normalised\n              space back to coordinate space.\n\n    """"""\n    dtype = np.dtype(dtype)\n\n    x, y = xy_from_gbox(gbox)\n    x, y, A = xy_norm(x, y, deg)\n\n    xy = np.stack([x, y])\n\n    if dtype.kind == \'f\':\n        xy = xy.astype(dtype)\n    else:\n        xy = to_fixed_point(xy, dtype)\n\n    def denorm(xy=None, y=None, nodata=None):\n        if xy is None:\n            return A\n\n        stacked = y is None\n        x, y = xy if stacked else (xy, y)\n        missing_mask = None\n\n        if nodata is not None:\n            if np.isnan(nodata):\n                missing_mask = np.isnan(x) + np.isnan(y)\n            else:\n                missing_mask = (x == nodata) + (y == nodata)\n\n        if x.dtype.kind != \'f\':\n            x = from_fixed_point(x)\n            y = from_fixed_point(y)\n\n        x, y = apply_affine(A, x, y)\n\n        if missing_mask is not None:\n            x[missing_mask] = np.nan\n            y[missing_mask] = np.nan\n\n        if stacked:\n            return np.stack([x, y])\n        else:\n            return x, y\n\n    return xy, denorm\n'"
datacube/testutils/io.py,2,"b'import numpy as np\nimport toolz\n\nfrom ..model import Dataset\nfrom ..storage import reproject_and_fuse, BandInfo\nfrom ..storage._rio import RasterioDataSource, RasterDatasetDataSource\nfrom ..utils.geometry._warp import resampling_s2rio\nfrom ..storage._read import rdr_geobox\nfrom ..utils.geometry import GeoBox\nfrom ..utils.geometry import gbox as gbx\nfrom ..index.eo3 import is_doc_eo3, _norm_grid\nfrom types import SimpleNamespace\n\n\nclass RasterFileDataSource(RasterioDataSource):\n    """""" This is only used in test code\n    """"""\n    def __init__(self, filename, bandnumber, nodata=None, crs=None, transform=None, lock=None):\n        super(RasterFileDataSource, self).__init__(filename, nodata, lock=lock)\n        self.bandnumber = bandnumber\n        self.crs = crs\n        self.transform = transform\n\n    def get_bandnumber(self, src):\n        return self.bandnumber\n\n    def get_transform(self, shape):\n        if self.transform is None:\n            raise RuntimeError(\'No transform in the data and no fallback\')\n        return self.transform\n\n    def get_crs(self):\n        if self.crs is None:\n            raise RuntimeError(\'No CRS in the data and no fallback\')\n        return self.crs\n\n\ndef _raster_metadata(band):\n    source = RasterDatasetDataSource(band)\n    with source.open() as rdr:\n        return SimpleNamespace(dtype=rdr.dtype.name,\n                               nodata=rdr.nodata,\n                               geobox=rdr_geobox(rdr))\n\n\ndef get_raster_info(ds: Dataset, measurements=None):\n    """"""\n    :param ds: Dataset\n    :param measurements: List of band names to load\n    """"""\n    if measurements is None:\n        measurements = list(ds.type.measurements)\n\n    return {n: _raster_metadata(BandInfo(ds, n))\n            for n in measurements}\n\n\ndef eo3_geobox(ds: Dataset, band: str) -> GeoBox:\n    mm = ds.measurements.get(band, None)\n    if mm is None:\n        raise ValueError(f""No such band: {band}"")\n\n    crs = ds.crs\n    doc_path = (\'grids\', mm.get(\'grid\', \'default\'))\n\n    grid = toolz.get_in(doc_path, ds.metadata_doc)\n    if crs is None or grid is None:\n        raise ValueError(\'Not a valid EO3 dataset\')\n\n    grid = _norm_grid(grid)\n    h, w = grid.shape\n\n    return GeoBox(w, h, grid.transform, crs)\n\n\ndef native_geobox(ds, measurements=None, basis=None):\n    """"""Compute native GeoBox for a set of bands for a given dataset\n\n    :param ds: Dataset\n    :param measurements: List of band names to consider\n    :param basis: Name of the band to use for computing reference frame, other\n    bands might be reprojected if they use different pixel grid\n\n    :return: GeoBox describing native storage coordinates.\n    """"""\n    gs = ds.type.grid_spec\n    if gs is not None:\n        # Dataset is from ingested product, figure out GeoBox of the tile this dataset covers\n        bb = [gbox for _, gbox in gs.tiles(ds.bounds)]\n        if len(bb) != 1:\n            # Ingested product but dataset overlaps several/none tiles -- no good\n            raise ValueError(\'Broken GridSpec detected\')\n        return bb[0]\n\n    if measurements is None and basis is None:\n        measurements = list(ds.type.measurements)\n\n    if is_doc_eo3(ds.metadata_doc):\n        if basis is not None:\n            return eo3_geobox(ds, basis)\n\n        gboxes = [eo3_geobox(ds, band) for band in measurements]\n    else:\n        if basis is not None:\n            return get_raster_info(ds, [basis])[basis].geobox\n\n        ii = get_raster_info(ds, measurements)\n        gboxes = [info.geobox for info in ii.values()]\n\n    geobox = gboxes[0]\n    consistent = all(geobox == gbox for gbox in gboxes)\n    if not consistent:\n        raise ValueError(\'Not all bands share the same pixel grid\')\n    return geobox\n\n\ndef native_load(ds, measurements=None, basis=None, **kw):\n    """"""Load single dataset in native resolution.\n\n    :param ds: Dataset\n    :param measurements: List of band names to load\n    :param basis: Name of the band to use for computing reference frame, other\n    bands might be reprojected if they use different pixel grid\n\n    :param **kw: Any other parameter load_data accepts\n\n    :return: Xarray dataset\n    """"""\n    from datacube import Datacube\n    geobox = native_geobox(ds, measurements, basis)  # early exit via exception if no compatible grid exists\n    if measurements is not None:\n        mm = [ds.type.measurements[n] for n in measurements]\n    else:\n        mm = ds.type.measurements\n\n    return Datacube.load_data(Datacube.group_datasets([ds], \'time\'),\n                              geobox,\n                              measurements=mm, **kw)\n\n\ndef dc_read(path,\n            band=1,\n            gbox=None,\n            resampling=\'nearest\',\n            dtype=None,\n            dst_nodata=None,\n            fallback_nodata=None):\n    """"""\n    Use default io driver to read file without constructing Dataset object.\n    """"""\n    source = RasterFileDataSource(path, band, nodata=fallback_nodata)\n    with source.open() as rdr:\n        dtype = rdr.dtype if dtype is None else dtype\n        if gbox is None:\n            gbox = rdr_geobox(rdr)\n        if dst_nodata is None:\n            dst_nodata = rdr.nodata\n\n    # currently dst_nodata = None case is not supported. So if fallback_nodata\n    # was None and file had none set, then use 0 as default output fill value\n    if dst_nodata is None:\n        dst_nodata = 0\n\n    im = np.full(gbox.shape, dst_nodata, dtype=dtype)\n    reproject_and_fuse([source], im, gbox, dst_nodata, resampling=resampling)\n    return im\n\n\ndef write_gtiff(fname,\n                pix,\n                crs=\'epsg:3857\',\n                resolution=(10, -10),\n                offset=(0.0, 0.0),\n                nodata=None,\n                overwrite=False,\n                blocksize=None,\n                gbox=None,\n                **extra_rio_opts):\n    """""" Write ndarray to GeoTiff file.\n\n    Geospatial info can be supplied either via\n    - resolution, offset, crs\n    or\n    - gbox (takes precedence if supplied)\n    """"""\n    # pylint: disable=too-many-locals\n\n    from affine import Affine\n    import rasterio\n    from pathlib import Path\n\n    if pix.ndim == 2:\n        h, w = pix.shape\n        nbands = 1\n        band = 1\n    elif pix.ndim == 3:\n        nbands, h, w = pix.shape\n        band = tuple(i for i in range(1, nbands+1))\n    else:\n        raise ValueError(\'Need 2d or 3d ndarray on input\')\n\n    if not isinstance(fname, Path):\n        fname = Path(fname)\n\n    if fname.exists():\n        if overwrite:\n            fname.unlink()\n        else:\n            raise IOError(""File exists"")\n\n    if gbox is not None:\n        assert gbox.shape == (h, w)\n\n        A = gbox.transform\n        crs = str(gbox.crs)\n    else:\n        sx, sy = resolution\n        tx, ty = offset\n\n        A = Affine(sx, 0, tx,\n                   0, sy, ty)\n\n    rio_opts = dict(width=w,\n                    height=h,\n                    count=nbands,\n                    dtype=pix.dtype.name,\n                    crs=crs,\n                    transform=A,\n                    predictor=2,\n                    compress=\'DEFLATE\')\n\n    if blocksize is not None:\n        rio_opts.update(tiled=True,\n                        blockxsize=min(blocksize, w),\n                        blockysize=min(blocksize, h))\n\n    if nodata is not None:\n        rio_opts.update(nodata=nodata)\n\n    rio_opts.update(extra_rio_opts)\n\n    with rasterio.open(str(fname), \'w\', driver=\'GTiff\', **rio_opts) as dst:\n        dst.write(pix, band)\n        meta = dst.meta\n\n    meta[\'gbox\'] = gbox if gbox is not None else rio_geobox(meta)\n    meta[\'path\'] = fname\n    return SimpleNamespace(**meta)\n\n\ndef dc_crs_from_rio(crs):\n    from datacube.utils.geometry import CRS\n\n    if crs.is_epsg_code:\n        return CRS(\'EPSG:{}\'.format(crs.to_epsg()))\n    return CRS(crs.wkt)\n\n\ndef rio_geobox(meta):\n    """""" Construct geobox from src.meta of opened rasterio dataset\n    """"""\n    if \'crs\' not in meta or \'transform\' not in meta:\n        return None\n\n    h, w = (meta[\'height\'], meta[\'width\'])\n    crs = dc_crs_from_rio(meta[\'crs\'])\n    transform = meta[\'transform\']\n\n    return GeoBox(w, h, transform, crs)\n\n\ndef _fix_resampling(kw):\n    r = kw.get(\'resampling\', None)\n    if isinstance(r, str):\n        kw[\'resampling\'] = resampling_s2rio(r)\n\n\ndef rio_slurp_reproject(fname, gbox, dtype=None, dst_nodata=None, **kw):\n    """"""\n    Read image with reprojection\n    """"""\n    import rasterio\n    from rasterio.warp import reproject\n\n    _fix_resampling(kw)\n\n    with rasterio.open(str(fname), \'r\') as src:\n        if src.count == 1:\n            shape = gbox.shape\n            src_band = rasterio.band(src, 1)\n        else:\n            shape = (src.count, *gbox.shape)\n            src_band = rasterio.band(src, tuple(range(1, src.count+1)))\n\n        if dtype is None:\n            dtype = src.dtypes[0]\n        if dst_nodata is None:\n            dst_nodata = src.nodata\n        if dst_nodata is None:\n            dst_nodata = 0\n\n        pix = np.full(shape, dst_nodata, dtype=dtype)\n\n        reproject(src_band, pix,\n                  dst_nodata=dst_nodata,\n                  dst_transform=gbox.transform,\n                  dst_crs=str(gbox.crs),\n                  **kw)\n\n        meta = src.meta\n        meta[\'src_gbox\'] = rio_geobox(meta)\n        meta[\'path\'] = fname\n        meta[\'gbox\'] = gbox\n\n        return pix, SimpleNamespace(**meta)\n\n\ndef rio_slurp_read(fname, out_shape=None, **kw):\n    """"""\n    Read whole image file using rasterio.\n\n    :returns: ndarray (2d or 3d if multi-band), dict (rasterio meta)\n    """"""\n    import rasterio\n\n    _fix_resampling(kw)\n\n    if out_shape is not None:\n        kw.update(out_shape=out_shape)\n\n    with rasterio.open(str(fname), \'r\') as src:\n        data = src.read(1, **kw) if src.count == 1 else src.read(**kw)\n        meta = src.meta\n        src_gbox = rio_geobox(meta)\n\n        same_gbox = out_shape is None or out_shape == src_gbox.shape\n        gbox = src_gbox if same_gbox else gbx.zoom_to(src_gbox, out_shape)\n\n        meta[\'src_gbox\'] = src_gbox\n        meta[\'gbox\'] = gbox\n        meta[\'path\'] = fname\n        return data, SimpleNamespace(**meta)\n\n\ndef rio_slurp(fname, *args, **kw):\n    """"""\n    Dispatches to either:\n\n    rio_slurp_read(fname, out_shape, ..)\n    rio_slurp_reproject(fname, gbox, ...)\n\n    """"""\n    if len(args) == 0:\n        if \'gbox\' in kw:\n            return rio_slurp_reproject(fname, **kw)\n        else:\n            return rio_slurp_read(fname, **kw)\n\n    if isinstance(args[0], GeoBox):\n        return rio_slurp_reproject(fname, *args, **kw)\n    else:\n        return rio_slurp_read(fname, *args, **kw)\n\n\ndef rio_slurp_xarray(fname, *args, rgb=\'auto\', **kw):\n    """"""\n    Dispatches to either:\n\n    rio_slurp_read(fname, out_shape, ..)\n    rio_slurp_reproject(fname, gbox, ...)\n\n    then wraps it all in xarray.DataArray with .crs,.nodata etc.\n    """"""\n    from xarray import DataArray\n\n    if len(args) == 0:\n        if \'gbox\' in kw:\n            im, mm = rio_slurp_reproject(fname, **kw)\n        else:\n            im, mm = rio_slurp_read(fname, **kw)\n    else:\n        if isinstance(args[0], GeoBox):\n            im, mm = rio_slurp_reproject(fname, *args, **kw)\n        else:\n            im, mm = rio_slurp_read(fname, *args, **kw)\n\n    if im.ndim == 3:\n        dims = (\'band\', *mm.gbox.dims)\n        if rgb and im.shape[0] in (3, 4):\n            im = im.transpose([1, 2, 0])\n            dims = tuple(dims[i] for i in [1, 2, 0])\n    else:\n        dims = mm.gbox.dims\n\n    return DataArray(im,\n                     dims=dims,\n                     coords=mm.gbox.xr_coords(with_crs=True),\n                     attrs=dict(\n                         nodata=mm.nodata))\n'"
datacube/testutils/iodriver.py,0,"b'"""""" Reader driver construction for tests\n""""""\nfrom pathlib import Path\nfrom datacube.testutils import mk_sample_dataset\nfrom datacube.drivers.rio._reader import (\n    RDEntry,\n)\nfrom datacube.storage import BandInfo\nfrom datacube.testutils.threads import FakeThreadPoolExecutor\n\nNetCDF = \'NetCDF\'    # pylint: disable=invalid-name\nGeoTIFF = \'GeoTIFF\'  # pylint: disable=invalid-name\n\n\ndef mk_rio_driver():\n    pool = FakeThreadPoolExecutor()\n    rde = RDEntry()\n    return rde.new_instance({\'pool\': pool,\n                             \'allow_custom_pool\': True})\n\n\ndef mk_band(name: str,\n            base_uri: str,\n            path: str = \'\',\n            format: str = GeoTIFF,  # pylint: disable=redefined-builtin\n            **extras) -> BandInfo:\n    """"""\n    **extras**:\n       layer, band, nodata, dtype, units, aliases\n    """"""\n    band_opts = {k: extras.pop(k)\n                 for k in \'path layer band nodata dtype units aliases\'.split() if k in extras}\n\n    band = dict(name=name, path=path, **band_opts)\n    ds = mk_sample_dataset([band], base_uri, format=format, **extras)\n    return BandInfo(ds, name)\n\n\ndef open_reader(path: str,\n                band_name: str = \'b1\',\n                format: str = GeoTIFF,  # pylint: disable=redefined-builtin\n                **extras):\n    """"""\n    **extras**:\n       layer, band, nodata, dtype, units, aliases\n    """"""\n    rdr = mk_rio_driver()\n    base_uri = Path(path).absolute().as_uri()\n    bi = mk_band(band_name, base_uri, format=format, **extras)\n    load_ctx = rdr.new_load_context(iter([bi]), None)\n    fut = rdr.open(bi, load_ctx)\n    return fut.result()\n\n\ndef tee_new_load_context(rdr, new_impl):\n    """""" When calling rdr.new_load_context(bands, old_ctx) tee data to new_impl\n    """"""\n    _real_impl = rdr.new_load_context\n\n    def patched(bands, old_ctx):\n        bands = list(bands)\n        new_impl(iter(bands), old_ctx)\n        return _real_impl(iter(bands), old_ctx)\n\n    rdr.new_load_context = patched\n'"
datacube/testutils/threads.py,0,"b'"""""" threads related stuff\n""""""\n\nfrom concurrent.futures import Future\nfrom functools import partial\n\n\nclass FakeThreadPoolExecutor():\n    """""" Limited version of ThreadPool that executes in the current thread.\n    """"""\n\n    def submit(self, fn, *args, **kwargs):\n        f = Future()\n        try:\n            f.set_result(fn(*args, **kwargs))\n        except Exception as e:  # pylint: disable=broad-except\n            f.set_exception(e)\n\n        return f\n\n    def map(self, fn, *iterables, timeout=None, chunksize=1):\n        return map(partial(self.submit, fn), *iterables)\n\n    def shutdown(self, wait=True):\n        pass\n'"
datacube/ui/__init__.py,0,"b'""""""\nUser Interface Utilities\n""""""\nfrom .expression import parse_expressions\nfrom .common import get_metadata_path\nfrom datacube.utils import read_documents\n\n__all__ = [\n    \'parse_expressions\',\n    \'get_metadata_path\',\n    ""read_documents"",\n]\n'"
datacube/ui/click.py,0,"b'""""""\nCommon functions for click-based cli scripts.\n""""""\nimport functools\nimport logging\nimport os\nimport copy\nimport sys\n\nimport click\n\nfrom datacube import config, __version__\nfrom datacube.api.core import Datacube\n\nfrom datacube.executor import get_executor, mk_celery_executor\nfrom datacube.index import index_connect\nfrom pathlib import Path\n\nfrom datacube.ui.expression import parse_expressions\nfrom sqlalchemy.exc import OperationalError, ProgrammingError\n\n_LOG_FORMAT_STRING = \'%(asctime)s %(process)d %(name)s %(levelname)s %(message)s\'\nCLICK_SETTINGS = dict(help_option_names=[\'-h\', \'--help\'])\n_LOG = logging.getLogger(__name__)\n\n\ndef _print_version(ctx, param, value):\n    if not value or ctx.resilient_parsing:\n        return\n\n    click.echo(\n        \'{prog}, version {version}\'.format(\n            prog=\'Open Data Cube core\',\n            version=__version__\n        )\n    )\n    ctx.exit()\n\n\ndef compose(*functions):\n    """"""\n    >>> compose(\n    ...     lambda x: x+1,\n    ...     lambda y: y+2\n    ... )(1)\n    4\n    """"""\n\n    def compose2(f, g):\n        return lambda x: f(g(x))\n\n    return functools.reduce(compose2, functions, lambda x: x)\n\n\nclass ColorFormatter(logging.Formatter):\n    colors = {\n        \'info\': dict(fg=\'white\'),\n        \'error\': dict(fg=\'red\'),\n        \'exception\': dict(fg=\'red\'),\n        \'critical\': dict(fg=\'red\'),\n        \'debug\': dict(fg=\'blue\'),\n        \'warning\': dict(fg=\'yellow\')\n    }\n\n    def format(self, record):\n        if not record.exc_info:\n            record = copy.copy(record)\n            record.levelname = click.style(record.levelname, **self.colors.get(record.levelname.lower(), {}))\n        return logging.Formatter.format(self, record)\n\n\nclass ClickHandler(logging.Handler):\n    def emit(self, record):\n        try:\n            msg = self.format(record)\n            click.echo(msg, err=True)\n        except:  # pylint: disable=bare-except\n            self.handleError(record)\n\n\ndef remove_handlers_of_type(logger, handler_type):\n    for handler in logger.handlers:\n        if isinstance(handler, handler_type):\n            logger.removeHandler(handler)\n\n\ndef _init_logging(ctx, param, value):\n    # When running in tests, we don\'t want to keep adding log handlers. It creates duplicate log messages up the wahoo.\n    remove_handlers_of_type(logging.root, ClickHandler)\n    handler = ClickHandler()\n    handler.formatter = ColorFormatter(_LOG_FORMAT_STRING)\n    logging.root.addHandler(handler)\n\n    logging_level = logging.WARN - 10 * value\n    logging.root.setLevel(logging_level)\n    logging.getLogger(\'datacube\').setLevel(logging_level)\n\n    if logging_level <= logging.INFO:\n        logging.getLogger(\'rasterio\').setLevel(logging.INFO)\n\n    logging.getLogger(\'datacube\').info(\'Running datacube command: %s\', \' \'.join(sys.argv))\n\n    if not ctx.obj:\n        ctx.obj = {}\n\n    ctx.obj[\'verbosity\'] = value\n\n\ndef _add_logfile(ctx, param, value):\n    formatter = logging.Formatter(_LOG_FORMAT_STRING)\n    for logfile in value:\n        handler = logging.FileHandler(logfile)\n        handler.formatter = formatter\n        logging.root.addHandler(handler)\n\n\ndef _log_queries(ctx, param, value):\n    if value:\n        logging.getLogger(\'sqlalchemy.engine\').setLevel(\'INFO\')\n\n\ndef _set_config(ctx, param, value):\n    if value:\n        if not any(os.path.exists(p) for p in value):\n            raise ValueError(\'No specified config paths exist: {}\'.format(value))\n\n        if not ctx.obj:\n            ctx.obj = {}\n        paths = value\n        ctx.obj[\'config_files\'] = paths\n    return value\n\n\ndef _set_environment(ctx, param, value):\n    if not ctx.obj:\n        ctx.obj = {}\n    ctx.obj[\'config_environment\'] = value\n\n\n#: pylint: disable=invalid-name\nversion_option = click.option(\'--version\', is_flag=True, callback=_print_version,\n                              expose_value=False, is_eager=True)\n#: pylint: disable=invalid-name\nverbose_option = click.option(\'--verbose\', \'-v\', count=True, callback=_init_logging,\n                              is_eager=True, expose_value=False, help=""Use multiple times for more verbosity"")\n#: pylint: disable=invalid-name\nlogfile_option = click.option(\'--log-file\', multiple=True, callback=_add_logfile,\n                              is_eager=True, expose_value=False, help=""Specify log file"")\n#: pylint: disable=invalid-name\nconfig_option = click.option(\'--config\', \'--config_file\', \'-C\', multiple=True, default=\'\', callback=_set_config,\n                             expose_value=False)\nconfig_option_exposed = click.option(\'--config\', \'--config_file\', \'-C\', multiple=True, default=\'\', callback=_set_config)\n\nenvironment_option = click.option(\'--env\', \'-E\', callback=_set_environment,\n                                  expose_value=False)\n#: pylint: disable=invalid-name\nlog_queries_option = click.option(\'--log-queries\', is_flag=True, callback=_log_queries,\n                                  expose_value=False, help=""Print database queries."")\n\n# This is a function, so it\'s valid to be lowercase.\n#: pylint: disable=invalid-name\nglobal_cli_options = compose(\n    version_option,\n    verbose_option,\n    logfile_option,\n    environment_option,\n    config_option,\n    log_queries_option\n)\n\n\n@click.group(help=""Data Cube command-line interface"", context_settings=CLICK_SETTINGS)\n@global_cli_options\ndef cli():\n    pass\n\n\ndef pass_config(f):\n    """"""Get a datacube config as the first argument. """"""\n\n    def new_func(*args, **kwargs):\n        obj = click.get_current_context().obj\n\n        paths = obj.get(\'config_files\', None)\n        # If the user is overriding the defaults\n        specific_environment = obj.get(\'config_environment\')\n\n        try:\n            parsed_config = config.LocalConfig.find(paths=paths, env=specific_environment)\n        except ValueError:\n            if specific_environment:\n                raise click.ClickException(""No datacube config found for \'{}\'"".format(specific_environment))\n            else:\n                raise click.ClickException(""No datacube config found"")\n\n        _LOG.debug(""Loaded datacube config: %r"", parsed_config)\n        return f(parsed_config, *args, **kwargs)\n\n    return functools.update_wrapper(new_func, f)\n\n\ndef pass_index(app_name=None, expect_initialised=True):\n    """"""Get a connection to the index as the first argument.\n\n    :param str app_name:\n        A short name of the application for logging purposes.\n    :param bool expect_initialised:\n        Whether to connect immediately on startup. Useful to catch connection config issues immediately,\n        but if you\'re planning to fork before any usage (such as in the case of some web servers),\n        you may not want this. For more information on thread/process usage, see datacube.index.Index\n    """"""\n\n    def decorate(f):\n        @pass_config\n        def with_index(local_config: config.LocalConfig,\n                       *args,\n                       **kwargs):\n            command_path = click.get_current_context().command_path\n            try:\n                index = index_connect(local_config,\n                                      application_name=app_name or command_path,\n                                      validate_connection=expect_initialised)\n                _LOG.debug(""Connected to datacube index: %s"", index)\n            except (OperationalError, ProgrammingError) as e:\n                handle_exception(\'Error Connecting to database: %s\', e)\n                return\n\n            try:\n                return f(index, *args, **kwargs)\n            finally:\n                index.close()\n                del index\n\n        return functools.update_wrapper(with_index, f)\n\n    return decorate\n\n\ndef pass_datacube(app_name=None, expect_initialised=True):\n    """"""\n    Get a DataCube from the current or default local settings.\n\n    :param str app_name:\n        A short name of the application for logging purposes.\n    :param bool expect_initialised:\n        Whether to connect immediately on startup. Useful to catch connection config issues immediately,\n        but if you\'re planning to fork before any usage (such as in the case of some web servers),\n        you may not want this. For More information on thread/process usage, see datacube.index.Index\n    """"""\n\n    def decorate(f):\n        @pass_index(app_name=app_name, expect_initialised=expect_initialised)\n        def with_datacube(index, *args, **kwargs):\n            return f(Datacube(index=index), *args, **kwargs)\n\n        return functools.update_wrapper(with_datacube, f)\n\n    return decorate\n\n\ndef parse_endpoint(value):\n    ip, port = tuple(value.split(\':\'))\n    return ip, int(port)\n\n\nEXECUTOR_TYPES = {\n    \'serial\': lambda _: get_executor(None, None),\n    \'multiproc\': lambda workers: get_executor(None, int(workers)),\n    \'distributed\': lambda addr: get_executor(parse_endpoint(addr), True),\n    \'celery\': lambda addr: mk_celery_executor(*parse_endpoint(addr))\n}\n\nEXECUTOR_TYPES[\'dask\'] = EXECUTOR_TYPES[\'distributed\']  # Add alias ""dask"" for distributed\n\n\ndef _setup_executor(ctx, param, value):\n    try:\n        return EXECUTOR_TYPES[value[0]](value[1])\n    except ValueError:\n        ctx.fail(""Failed to create \'%s\' executor with \'%s\'"" % value)\n\n\nexecutor_cli_options = click.option(\'--executor\',  # type: ignore\n                                    type=(click.Choice(list(EXECUTOR_TYPES)), str),\n                                    default=(\'serial\', None),\n                                    help=""Run parallelized, either locally or distributed. eg:\\n""\n                                         ""--executor multiproc 4 (OR)\\n""\n                                         ""--executor distributed 10.0.0.8:8888"",\n                                    callback=_setup_executor)\n\n\ndef handle_exception(msg, e):\n    """"""\n    Exit following an exception in a CLI app\n\n    If verbosity (-v flag) specified, dump out a stack trace. Otherwise,\n    simply print the given error message.\n\n    Include a \'%s\' in the message to print the single line message from the\n    exception.\n\n    :param e: caught Exception\n    :param msg: Message to User with optional %s\n    """"""\n    ctx = click.get_current_context()\n    if ctx.obj[\'verbosity\'] >= 1:\n        raise e\n    else:\n        if \'%s\' in msg:\n            click.echo(msg % e)\n        else:\n            click.echo(msg)\n        ctx.exit(1)\n\n\ndef parsed_search_expressions(f):\n    """"""\n    Add [EXPRESSIONs] arguments to a click application\n\n    Passes a parsed dict of search expressions to the `expressions` argument\n    of the command.\n\n    Also appends documentation on using search expressions to the command.\n\n    WARNING: This wrapped expects an unlimited number of search expressions\n    as click arguments, which means your command must take only click options\n    or a specified number of arguments.\n    """"""\n    if not f.__doc__:\n        f.__doc__ = """"\n    f.__doc__ += """"""\n    EXPRESSIONS\n    \n    Select datasets using [EXPRESSIONS] to filter by date, product type,\n    spatial extents or other searchable fields.\n\n    \\b\n        FIELD = VALUE\n        FIELD in DATE-RANGE\n        FIELD in [START, END]\n\n    \\b\n    DATE-RANGE is one of YYYY, YYYY-MM or YYYY-MM-DD\n    START and END can be either numbers or dates\n\n    FIELD: x, y, lat, lon, time, product, ... \n\n    \\b\n    eg. \'time in [1996-01-01, 1996-12-31]\'\n        \'time in 1996\'\n        \'lon in [130, 140]\' \'lat in [-40, -30]\'\n        product=ls5_nbar_albers\n\n    """"""\n\n    def my_parse(ctx, param, value):\n        return parse_expressions(*list(value))\n\n    f = click.argument(\'expressions\', callback=my_parse, nargs=-1)(f)\n    return f\n'"
datacube/ui/common.py,0,"b'# coding=utf-8\n""""""\nCommon methods for UI code.\n""""""\nfrom pathlib import Path\nfrom typing import Union, Optional\n\nfrom toolz.functoolz import identity\n\nfrom datacube.utils import read_documents, InvalidDocException, SimpleDocNav, is_supported_document_type, is_url\n\n\ndef get_metadata_path(possible_path: Union[str, Path]) -> str:\n    """"""\n    Find a metadata path for a given input/dataset path.\n\n    Needs to handle local files as well as remote URLs\n    """"""\n    # We require exact URLs, lets skip any sort of fancy investigation and mapping\n    if isinstance(possible_path, str) and is_url(possible_path):\n        return possible_path\n\n    dataset_path = Path(possible_path)\n\n    # They may have given us a metadata file directly.\n    if dataset_path.is_file() and is_supported_document_type(dataset_path):\n        return str(dataset_path)\n\n    # Otherwise there may be a sibling file with appended suffix \'.agdc-md.yaml\'.\n    expected_name = dataset_path.parent.joinpath(\'{}.agdc-md\'.format(dataset_path.name))\n    found = _find_any_metadata_suffix(expected_name)\n    if found:\n        return str(found)\n\n    # Otherwise if it\'s a directory, there may be an \'agdc-metadata.yaml\' file describing all contained datasets.\n    if dataset_path.is_dir():\n        expected_name = dataset_path.joinpath(\'agdc-metadata\')\n        found = _find_any_metadata_suffix(expected_name)\n        if found:\n            return str(found)\n\n    raise ValueError(\'No metadata found for input %r\' % dataset_path)\n\n\ndef _find_any_metadata_suffix(path: Path) -> Optional[Path]:\n    """"""\n    Find any supported metadata files that exist with the given file path stem.\n    (supported suffixes are tried on the name)\n\n    Eg. searching for \'/tmp/ga-metadata\' will find if any files such as \'/tmp/ga-metadata.yaml\' or\n    \'/tmp/ga-metadata.json\', or \'/tmp/ga-metadata.yaml.gz\' etc that exist: any suffix supported by read_documents()\n\n    :type path: pathlib.Path\n    """"""\n    existing_paths = list(filter(is_supported_document_type, path.parent.glob(path.name + \'*\')))\n    if not existing_paths:\n        return None\n\n    if len(existing_paths) > 1:\n        raise ValueError(\'Multiple matched metadata files: {!r}\'.format(existing_paths))\n\n    return existing_paths[0]\n\n\ndef ui_path_doc_stream(paths, logger=None, uri=True, raw=False):\n    """"""Given a stream of URLs, or Paths that could be directories, generate a stream of\n    (path, doc) tuples.\n\n    For every path:\n    1. If directory find the metadata file or log error if not found\n\n    2. Load all documents from that path and return one at a time (parsing\n    errors are logged, but processing should continue)\n\n    :param paths: Filesystem paths\n\n    :param logger: Logger to use to report errors\n\n    :param uri: If True return path in uri format, else return it as filesystem path\n\n    :param raw: By default docs are wrapped in :class:`SimpleDocNav`, but you can\n    instead request them to be raw dictionaries\n\n    """"""\n\n    def on_error1(p, e):\n        if logger is not None:\n            logger.error(\'No supported metadata docs found for dataset %s\', str(p))\n\n    def on_error2(p, e):\n        if logger is not None:\n            logger.error(\'Failed reading documents from %s\', str(p))\n\n    yield from _path_doc_stream(_resolve_doc_files(paths, on_error=on_error1),\n                                on_error=on_error2, uri=uri, raw=raw)\n\n\ndef _resolve_doc_files(paths, on_error):\n    for p in paths:\n        try:\n            yield get_metadata_path(p)\n        except ValueError as e:\n            on_error(p, e)\n\n\ndef _path_doc_stream(files, on_error, uri=True, raw=False):\n    """"""See :func:`ui_path_doc_stream` for documentation""""""\n    maybe_wrap = identity if raw else SimpleDocNav\n\n    for fname in files:\n        try:\n            for p, doc in read_documents(fname, uri=uri):\n                yield p, maybe_wrap(doc)\n\n        except InvalidDocException as e:\n            on_error(fname, e)\n'"
datacube/ui/expression.py,0,"b'""""""\nSearch expression parsing for command line applications.\n\nThree types of expressions are available:\n\n    FIELD = VALUE\n    FIELD in DATE-RANGE\n    FIELD in [START, END]\n\nWhere DATE-RANGE is one of YYYY, YYYY-MM or YYYY-MM-DD\nand START, END are either numbers or dates.\n""""""\n# flake8: noqa\n\nfrom lark import Lark, v_args, Transformer\n\nfrom datacube.api.query import _time_to_search_dims\nfrom datacube.model import Range\n\n\nsearch_grammar = r""""""\n    start: expression*\n    ?expression: equals_expr\n               | time_in_expr\n               | field_in_expr\n\n    equals_expr: field ""="" value\n    time_in_expr: time ""in"" date_range\n    field_in_expr: field ""in"" ""["" orderable "","" orderable ""]""\n\n    field: FIELD\n    time: TIME\n\n    ?orderable: INT -> integer\n              | SIGNED_NUMBER -> number\n\n    ?value: INT -> integer\n          | SIGNED_NUMBER -> number\n          | ESCAPED_STRING -> string\n          | SIMPLE_STRING -> simple_string\n          | URL_STRING -> url_string\n\n\n    ?date_range: date -> single_date\n               | ""["" date "","" date ""]"" -> date_pair\n\n    date: YEAR [""-"" MONTH [""-"" DAY ]]\n\n    TIME: ""time""\n    FIELD: /[a-zA-Z][\\w\\d_]*/\n    YEAR: DIGIT ~ 4\n    MONTH: DIGIT ~ 1..2\n    DAY: DIGIT ~ 1..2\n    SIMPLE_STRING: /[a-zA-Z][\\w._-]*/\n    URL_STRING: /[a-z0-9+.-]+:\\/\\/([:\\/\\w._-])*/\n\n\n    %import common.ESCAPED_STRING\n    %import common.SIGNED_NUMBER\n    %import common.INT\n    %import common.DIGIT\n    %import common.CNAME\n    %import common.WS\n    %ignore WS\n""""""\n\n\ndef identity(x):\n    return x\n\n\n@v_args(inline=True)\nclass TreeToSearchExprs(Transformer):\n    # Convert the expressions\n    def equals_expr(self, field, value):\n        return {str(field): value}\n\n    def field_in_expr(self, field, lower, upper):\n        return {str(field): Range(lower, upper)}\n\n    def time_in_expr(self, time_field, date_range):\n        return {str(time_field): date_range}\n\n    # Convert the literals\n    def string(self, val):\n        return str(val[1:-1])\n\n    simple_string = url_string = field = time = str\n    number = float\n    integer = int\n    value = identity\n\n    def single_date(self, date):\n        return _time_to_search_dims(date)\n\n    def date_pair(self, start, end):\n        return _time_to_search_dims((start, end))\n\n    def date(self, y, m=None, d=None):\n        return ""-"".join(x for x in [y, m, d] if x is not None)\n\n    # Merge everything into a single dict\n    def start(self, *search_exprs):\n        combined = {}\n        for expr in search_exprs:\n            combined.update(expr)\n        return combined\n\n\ndef parse_expressions(*expression_text):\n    expr_parser = Lark(search_grammar)\n    tree = expr_parser.parse(\' \'.join(expression_text))\n    return TreeToSearchExprs().transform(tree)\n\n\ndef main():\n    expr_parser = Lark(search_grammar)\n\n    sample_inputs = """"""platform = ""LANDSAT_8""\n    platform = ""LAND SAT_8""\n    platform = 4\n    lat in [4, 6]\n    time in [2014, 2014]\n    time in [2014-03-01, 2014-04-01]\n    time in 2014-03-02\n    time in 2014-3-2\n    time in 2014-3\n    time in 2014\n    platform = LANDSAT_8\n    lat in [4, 6] time in 2014-03-02\n    platform=LS8 lat in [-14, -23.5] instrument=""OTHER""\n    """""".strip().split(\'\\n\')\n\n    for sample in sample_inputs:\n        transformer = TreeToSearchExprs()\n        tree = expr_parser.parse(sample)\n\n        print(sample)\n        print(tree)\n        print(transformer.transform(tree))\n        print()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
datacube/ui/task_app.py,0,"b'\nimport logging\nimport os\nimport time\nimport click\nimport functools\nimport itertools\nimport re\nfrom pathlib import Path\nimport pandas as pd\nimport pickle\n\nfrom datacube.ui import click as dc_ui\nfrom datacube.utils import read_documents\n\n\n_LOG = logging.getLogger(__name__)\n\n\ndef load_config(index, app_config_file, make_config, make_tasks, *args, **kwargs):\n    app_config_path = Path(app_config_file)\n    _, config = next(read_documents(app_config_path))\n    config[\'app_config_file\'] = app_config_path.name\n    config[\'task_timestamp\'] = int(time.time())\n\n    config = make_config(index, config, **kwargs)\n\n    tasks = make_tasks(index, config, **kwargs)\n\n    return config, iter(tasks)\n\n\ndef pickle_stream(objs, filename):\n    idx = 0\n    with open(filename, \'wb\') as stream:\n        for idx, obj in enumerate(objs, start=1):\n            pickle.dump(obj, stream, pickle.HIGHEST_PROTOCOL)\n    return idx\n\n\ndef unpickle_stream(filename):\n    with open(filename, \'rb\') as stream:\n        while True:\n            try:\n                yield pickle.load(stream)\n            except EOFError:\n                break\n\n\ndef save_tasks(config, tasks, taskfile):\n    """"""Saves the config\n\n    :param config: dict of configuration options common to all tasks\n    :param tasks:\n    :param str taskfile: Name of output file\n    :return: Number of tasks saved to the file\n    """"""\n    i = pickle_stream(itertools.chain([config], tasks), taskfile)\n    if i <= 1:\n        # Only saved the config, no tasks!\n        os.remove(taskfile)\n        return 0\n    else:\n        _LOG.info(\'Saved config and %d tasks to %s\', i - 1, taskfile)\n    return i - 1\n\n\ndef load_tasks(taskfile):\n    stream = unpickle_stream(taskfile)\n    config = next(stream)\n    return config, stream\n\n\n# This is a function, so it\'s valid to be lowercase.\n#: pylint: disable=invalid-name\napp_config_option = click.option(\'--app-config\', help=\'App configuration file\',\n                                 type=click.Path(exists=True, readable=True, writable=False, dir_okay=False))\n#: pylint: disable=invalid-name\nload_tasks_option = click.option(\'--load-tasks\', \'input_tasks_file\', help=\'Load tasks from the specified file\',\n                                 type=click.Path(exists=True, readable=True, writable=False, dir_okay=False))\n#: pylint: disable=invalid-name\nsave_tasks_option = click.option(\'--save-tasks\', \'output_tasks_file\', help=\'Save tasks to the specified file\',\n                                 type=click.Path(exists=False))\n#: pylint: disable=invalid-name\nqueue_size_option = click.option(\'--queue-size\', help=\'Number of tasks to queue at the start\',\n                                 type=click.IntRange(1, 100000), default=3200)\n\n#: pylint: disable=invalid-name\ntask_app_options = dc_ui.compose(\n    app_config_option,\n    load_tasks_option,\n    save_tasks_option,\n\n    dc_ui.config_option,\n    dc_ui.verbose_option,\n    dc_ui.log_queries_option,\n    dc_ui.executor_cli_options,\n)\n\n\ndef _cell_list_from_file(filename):\n    cell_matcher = re.compile(r\'(-?\\d+)(?:\\s*(?:,|_|\\s)\\s*)(-?\\d+)\')\n    with open(filename) as cell_file:\n        for line in cell_file:\n            match = cell_matcher.match(line)\n            if match:\n                yield tuple(int(i) for i in match.groups())\n\n\ndef cell_list_to_file(filename, cell_list):\n    with open(filename, \'w\') as cell_file:\n        for cell in cell_list:\n            cell_file.write(\'{0},{1}\\n\'.format(*cell))\n\n\ndef validate_cell_list(ctx, param, value):\n    try:\n        if value is None:\n            return None\n        return list(_cell_list_from_file(value))\n    except ValueError:\n        raise click.BadParameter(\'cell_index_list must be a file with lines in the form ""14,-11""\')\n\n\ndef validate_cell_index(ctx, param, value):\n    try:\n        if value is None:\n            return None\n        return tuple(int(i) for i in value.split(\',\', 2))\n    except ValueError:\n        raise click.BadParameter(\'cell_index must be specified in the form ""14,-11""\')\n\n\ndef validate_year(ctx, param, value):\n    try:\n        if value is None:\n            return None\n        years = [pd.Period(y) for y in value.split(\'-\', 2)]\n        return years[0].start_time.to_pydatetime(warn=False), years[-1].end_time.to_pydatetime(warn=False)\n    except ValueError:\n        raise click.BadParameter(\'year must be specified as a single year (eg 1996) \'\n                                 \'or as an inclusive range (eg 1996-2001)\')\n\n\ndef break_query_into_years(time_query, **kwargs):\n    if time_query is None:\n        return [kwargs]\n    return [dict(time=time_range, **kwargs) for time_range in year_splitter(*time_query)]\n\n\ndef year_splitter(start, end):\n    """"""\n    Produces a list of time ranges based that represent each year in the range.\n\n    `year_splitter(\'1992\', \'1993\')` returns:\n\n    ::\n        [(\'1992-01-01 00:00:00\', \'1992-12-31 23:59:59.9999999\'),\n         (\'1993-01-01 00:00:00\', \'1993-12-31 23:59:59.9999999\')]\n\n    :param str start: start year\n    :param str end: end year\n    :return Generator[tuple(str, str)]: strings representing the ranges\n    """"""\n    start_ts = pd.Timestamp(start)\n    end_ts = pd.Timestamp(end)\n    for p in pd.period_range(start=start_ts, end=end_ts, freq=\'A\'):\n        yield str(p.start_time), str(p.end_time)\n\n\n#: pylint: disable=invalid-name\ncell_index_option = click.option(\'--cell-index\', \'cell_index\',\n                                 help=\'Limit the process to a particular cell (e.g. 14,-11)\',\n                                 callback=validate_cell_index, default=None)\n#: pylint: disable=invalid-name\ncell_index_list_option = click.option(\'--cell-index-list\', \'cell_index_list\',\n                                      help=\'Limit the process to a file of cells indexes (e.g. 14,-11)\',\n                                      callback=validate_cell_list, default=None)\n#: pylint: disable=invalid-name\nyear_option = click.option(\'--year\', \'time\', help=\'Limit the process to a particular year\',\n                           callback=validate_year)\n\n\ndef task_app(make_config, make_tasks):\n    """"""\n    Create a `Task App` from a function\n\n    Decorates a function\n    :param make_config: callable(index, config, **query)\n    :param make_tasks: callable(index, config, **kwargs)\n    :return:\n    """"""\n    def decorate(app_func):\n        def with_app_args(index, app_config=None, input_tasks_file=None, output_tasks_file=None, *args, **kwargs):\n            if (app_config is None) == (input_tasks_file is None):\n                click.echo(\'Must specify exactly one of --app-config, --load-tasks\')\n                click.get_current_context().exit(1)\n\n            if app_config is not None:\n                config, tasks = load_config(index, app_config, make_config, make_tasks, *args, **kwargs)\n\n            if input_tasks_file:\n                config, tasks = load_tasks(input_tasks_file)\n\n            if output_tasks_file:\n                num_tasks_saved = save_tasks(config, tasks, output_tasks_file)\n                return num_tasks_saved != 0\n\n            return app_func(index, config, tasks, *args, **kwargs)\n\n        return functools.update_wrapper(with_app_args, app_func)\n\n    return decorate\n\n\ndef check_existing_files(paths):\n    """"""Check for existing files and optionally delete them.\n\n    :param paths: sequence of path strings or path objects\n    """"""\n    click.echo(\'Files to be created:\')\n    existing_files = []\n    total = 0\n    for path in paths:\n        total += 1\n        file_path = Path(path)\n        file_info = \'\'\n        if file_path.exists():\n            existing_files.append(file_path)\n            file_info = \' - ALREADY EXISTS\'\n        click.echo(\'{}{}\'.format(path, file_info))\n\n    if existing_files:\n        if click.confirm(\'There were {} existing files found that are not indexed. Delete those files now?\'.format(\n                len(existing_files))):\n            for file_path in existing_files:\n                file_path.unlink()\n\n    click.echo(\'{total} tasks files to be created ({valid} valid files, {invalid} existing paths)\'.format(\n        total=total, valid=total - len(existing_files), invalid=len(existing_files)\n    ))\n\n\ndef do_nothing(result):\n    pass\n\n\ndef _wrap_impl(f, args, kwargs, task):\n    """"""\n    Helper method, needs to be at the top level\n    """"""\n    return f(task, *args, **kwargs)\n\n\ndef wrap_task(f, *args, **kwargs):\n    """"""\n    Turn function `f(task, *args, **kwargs)` into `g(task)` in pickle-able fashion\n    """"""\n    return functools.partial(_wrap_impl, f, args, kwargs)\n\n\ndef run_tasks(tasks, executor, run_task, process_result=None, queue_size=50):\n    """"""\n    :param tasks: iterable of tasks. Usually a generator to create them as required.\n    :param executor: a datacube executor, similar to `distributed.Client` or `concurrent.futures`\n    :param run_task: the function used to run a task. Expects a single argument of one of the tasks\n    :param process_result: a function to do something based on the result of a completed task. It\n                           takes a single argument, the return value from `run_task(task)`\n    :param queue_size: How large the queue of tasks should be. Will depend on how fast tasks are\n                       processed, and how much memory is available to buffer them.\n    """"""\n    click.echo(\'Starting processing...\')\n    process_result = process_result or do_nothing\n    results = []\n    task_queue = itertools.islice(tasks, queue_size)\n    for task in task_queue:\n        _LOG.info(\'Running task: %s\', task.get(\'tile_index\', str(task)))\n        results.append(executor.submit(run_task, task=task))\n\n    click.echo(\'Task queue filled, waiting for first result...\')\n\n    successful = failed = 0\n    while results:\n        result, results = executor.next_completed(results, None)\n\n        # submit a new _task to replace the one we just finished\n        task = next(tasks, None)\n        if task:\n            _LOG.info(\'Running task: %s\', task.get(\'tile_index\', str(task)))\n            results.append(executor.submit(run_task, task=task))\n\n        # Process the result\n        try:\n            actual_result = executor.result(result)\n            process_result(actual_result)\n            successful += 1\n        except Exception as err:  # pylint: disable=broad-except\n            _LOG.exception(\'Task failed: %s\', err)\n            failed += 1\n            continue\n        finally:\n            # Release the _task to free memory so there is no leak in executor/scheduler/worker process\n            executor.release(result)\n\n    click.echo(\'%d successful, %d failed\' % (successful, failed))\n'"
datacube/utils/__init__.py,0,"b'""""""\nUtility functions\n""""""\n\nfrom .dates import datetime_to_seconds_since_1970, parse_time\nfrom .py import cached_property, ignore_exceptions_if, import_function\nfrom .serialise import jsonify_document\nfrom .uris import is_url, uri_to_local_path, get_part_from_uri, mk_part_uri, is_vsipath\nfrom .io import slurp, check_write_path, write_user_secret_file\nfrom .documents import (\n    InvalidDocException,\n    SimpleDocNav,\n    DocReader,\n    is_supported_document_type,\n    read_strings_from_netcdf,\n    read_documents,\n    validate_document,\n    NoDatesSafeLoader,\n    get_doc_offset,\n    get_doc_offset_safe,\n    netcdf_extract_string,\n    without_lineage_sources,\n    schema_validated,\n    _readable_offset,\n)\nfrom .math import (\n    unsqueeze_dataset,\n    unsqueeze_data_array,\n    spatial_dims,\n    iter_slices,\n    data_resolution_and_offset,\n)\nfrom ._misc import (\n    DatacubeException,\n    gen_password,\n)\n\n\n__all__ = (\n    ""datetime_to_seconds_since_1970"",\n    ""parse_time"",\n    ""cached_property"",\n    ""ignore_exceptions_if"",\n    ""import_function"",\n    ""jsonify_document"",\n    ""is_url"",\n    ""is_vsipath"",\n    ""uri_to_local_path"",\n    ""get_part_from_uri"",\n    ""mk_part_uri"",\n    ""InvalidDocException"",\n    ""SimpleDocNav"",\n    ""DocReader"",\n    ""is_supported_document_type"",\n    ""read_strings_from_netcdf"",\n    ""read_documents"",\n    ""validate_document"",\n    ""NoDatesSafeLoader"",\n    ""get_doc_offset"",\n    ""get_doc_offset_safe"",\n    ""netcdf_extract_string"",\n    ""without_lineage_sources"",\n    ""unsqueeze_data_array"",\n    ""unsqueeze_dataset"",\n    ""spatial_dims"",\n    ""iter_slices"",\n    ""data_resolution_and_offset"",\n    ""DatacubeException"",\n    ""schema_validated"",\n    ""write_user_secret_file"",\n    ""slurp"",\n    ""check_write_path"",\n    ""gen_password"",\n    ""_readable_offset"",\n)\n'"
datacube/utils/_misc.py,0,"b'""""""\nUtility functions\n""""""\nimport os\n\n\nclass DatacubeException(Exception):\n    """"""Your Data Cube has malfunctioned""""""\n    pass\n\n\ndef gen_password(num_random_bytes=12):\n    """"""\n    Generate random password\n    """"""\n    import base64\n    return base64.urlsafe_b64encode(os.urandom(num_random_bytes)).decode(\'utf-8\')\n'"
datacube/utils/changes.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nValidation of document/dictionary changes.\n""""""\nfrom itertools import zip_longest\n\n\ndef contains(v1, v2, case_sensitive=False):\n    """"""\n    Check that v1 is a superset of v2.\n\n    For dicts contains(v1[k], v2[k]) for all k in v2\n    For other types v1 == v2\n    v2 None is interpreted as {}\n\n    """"""\n    if not case_sensitive:\n        if isinstance(v1, str):\n            return isinstance(v2, str) and v1.lower() == v2.lower()\n\n    if isinstance(v1, dict):\n        return v2 is None or (isinstance(v2, dict) and\n                              all(contains(v1.get(k, object()), v, case_sensitive=case_sensitive)\n                                  for k, v in v2.items()))\n\n    return v1 == v2\n\n\nclass MissingSentinel(object):\n    def __str__(self):\n        return ""missing""\n\n    def __repr__(self):\n        return ""missing""\n\n\nMISSING = MissingSentinel()\n\n\ndef get_doc_changes(original, new, base_prefix=()):\n    """"""\n    Return a list of `changed fields` between two dict structures.\n\n    A `changed field` is represented by a 3-tuple made up of:\n\n    1. `offset` to the change - a tuple of `item` accessors on the document.\n    2. What is in `original` - Either a single value, a dict or list, or :data:`MISSING`.\n    3. What is in `new`\n\n    If the documents are identical, an empty list is returned.\n\n    :type original: Union[dict, list, int]\n    :rtype: list[(tuple, object, object)]\n\n\n    """"""\n    changed_fields = []\n    if original == new:\n        return changed_fields\n\n    if isinstance(original, dict) and isinstance(new, dict):\n        all_keys = set(original.keys()).union(new.keys())\n        for key in all_keys:\n            changed_fields.extend(get_doc_changes(original.get(key, MISSING),\n                                                  new.get(key, MISSING),\n                                                  base_prefix + (key,)))\n    elif isinstance(original, list) and isinstance(new, list):\n        for idx, (orig_item, new_item) in enumerate(zip_longest(original, new)):\n            changed_fields.extend(get_doc_changes(orig_item, new_item, base_prefix + (idx, )))\n    else:\n        changed_fields.append((base_prefix, original, new))\n\n    return sorted(changed_fields, key=lambda a: a[0])\n\n\nclass DocumentMismatchError(Exception):\n    pass\n\n\ndef check_doc_unchanged(original, new, doc_name):\n    """"""\n    Raise an error if any fields have been modified on a document.\n\n    :param original: original document\n    :param new: new document to compare against the original\n    :param doc_name: Label used to name the document\n    """"""\n    changes = get_doc_changes(original, new)\n\n    if changes:\n        raise DocumentMismatchError(\n            \'{} differs from stored ({})\'.format(\n                doc_name,\n                \', \'.join([\'{}: {!r}!={!r}\'.format(\'.\'.join(map(str, offset)), v1, v2) for offset, v1, v2 in changes])\n            )\n        )\n\n\ndef allow_truncation(key, offset, old_value, new_value):\n    return offset and key == offset[:-1] and new_value == MISSING\n\n\ndef allow_extension(key, offset, old_value, new_value):\n    return offset and key == offset[:-1] and old_value == MISSING\n\n\ndef allow_addition(key, offset, old_value, new_value):\n    return key == offset and old_value == MISSING\n\n\ndef allow_removal(key, offset, old_value, new_value):\n    return key == offset and new_value == MISSING\n\n\ndef allow_any(key, offset, old, new):\n    return True, None\n\n\ndef classify_changes(changes, allowed_changes):\n    """"""\n    Classify list of changes into good(allowed) and bad(not allowed) based on allowed changes.\n\n    :param list[(tuple,object,object)] changes: result of get_doc_changes\n    :param allowed_changes: mapping from key to change policy (subset, superset, any)\n    :return: good_changes, bad_chages\n    """"""\n    allowed_changes_index = dict(allowed_changes)\n\n    good_changes = []\n    bad_changes = []\n\n    for offset, old_val, new_val in changes:\n        allowance = allowed_changes_index.get(offset)\n        allowance_offset = offset\n        # If no allowance on this leaf, find if any parents have allowances.\n        while allowance is None:\n            if not allowance_offset:\n                break\n\n            allowance_offset = allowance_offset[:-1]\n            allowance = allowed_changes_index.get(allowance_offset)\n\n        if allowance is None:\n            bad_changes.append((offset, old_val, new_val))\n        elif hasattr(allowance, \'__call__\'):\n            if allowance(allowance_offset, offset, old_val, new_val):\n                good_changes.append((offset, old_val, new_val))\n            else:\n                bad_changes.append((offset, old_val, new_val))\n        else:\n            raise RuntimeError(\'Unknown change type: expecting validation function at %r\' % offset)\n\n    return good_changes, bad_changes\n'"
datacube/utils/cog.py,1,"b'import warnings\nimport rasterio\nfrom rasterio.shutil import copy as rio_copy\nimport numpy as np\nimport xarray as xr\nimport dask\nfrom dask.delayed import Delayed\nfrom pathlib import Path\nfrom typing import Union, Optional, List, Any\n\nfrom .io import check_write_path\nfrom .geometry import GeoBox\nfrom .geometry.tools import align_up\n\n__all__ = (""write_cog"", ""to_cog"")\n\n\ndef _adjust_blocksize(block, dim):\n    if block > dim:\n        return align_up(dim, 16)\n    return align_up(block, 16)\n\n\ndef _write_cog(pix: np.ndarray,\n               geobox: GeoBox,\n               fname: Union[Path, str],\n               nodata: Optional[float] = None,\n               overwrite: bool = False,\n               blocksize: Optional[int] = None,\n               overview_resampling: Optional[str] = None,\n               overview_levels: Optional[List[int]] = None,\n               ovr_blocksize: Optional[int] = None,\n               **extra_rio_opts) -> Union[Path, bytes]:\n    """"""Write geo-registered ndarray to GeoTiff file or RAM.\n\n    :param pix: ``xarray.DataArray`` with crs or (ndarray, geobox, nodata) triple\n    :param fname:  Output file or "":mem:""\n    :param nodata: Set ``nodata`` flag to this value if supplied\n    :param overwrite: True -- replace existing file, False -- abort with IOError exception\n    :param blocksize: Size of internal tiff tiles (512x512 pixels)\n    :param ovr_blocksize: Size of internal tiles in overview images (defaults to blocksize)\n    :param overview_resampling: Use this resampling when computing overviews\n    :param overview_levels: List of shrink factors to compute overiews for: [2,4,8,16,32]\n                            to disable overviews supply empty list ``[]``\n    :param extra_rio_opts: Any other option is passed to ``rasterio.open``\n\n    When fname="":mem:"" write COG to memory rather than to a file and return it\n    as memoryview object.\n\n    NOTE: about memory requirements\n\n    This function generates temporary in memory tiff file without compression\n    to speed things up. It then adds overviews to this file and only then\n    copies it to the final destination with requested compression settings.\n    This is necessary to produce compliant COG, since COG standard demands\n    overviews to be placed before native resolution data and double pass is the\n    only way to achieve this currently.\n\n    This means that this function will use about 1.5 to 2 times memory taken by `pix`.\n    """"""\n    # pylint: disable=too-many-locals\n    if blocksize is None:\n        blocksize = 512\n    if ovr_blocksize is None:\n        ovr_blocksize = blocksize\n    if overview_levels is None:\n        overview_levels = [2 ** i for i in range(1, 6)]\n    if overview_resampling is None:\n        overview_resampling = ""nearest""\n\n    if pix.ndim == 2:\n        h, w = pix.shape\n        nbands = 1\n        band = 1  # type: Any\n    elif pix.ndim == 3:\n        if pix.shape[:2] == geobox.shape:\n            pix = pix.transpose([2, 0, 1])\n        elif pix.shape[-2:] != geobox.shape:\n            raise ValueError(\'GeoBox shape does not match image shape\')\n\n        nbands, h, w = pix.shape\n        band = tuple(i for i in range(1, nbands + 1))\n    else:\n        raise ValueError(""Need 2d or 3d ndarray on input"")\n\n    assert geobox.shape == (h, w)\n\n    if fname != "":mem:"":\n        path = check_write_path(\n            fname, overwrite\n        )  # aborts if overwrite=False and file exists already\n\n    resampling = rasterio.enums.Resampling[overview_resampling]\n\n    if (blocksize % 16) != 0:\n        warnings.warn(f""Block size must be a multiple of 16, will be adjusted"")\n\n    rio_opts = dict(\n        width=w,\n        height=h,\n        count=nbands,\n        dtype=pix.dtype.name,\n        crs=str(geobox.crs),\n        transform=geobox.transform,\n        tiled=True,\n        blockxsize=_adjust_blocksize(blocksize, w),\n        blockysize=_adjust_blocksize(blocksize, h),\n        zlevel=6,\n        predictor=3 if pix.dtype.kind == ""f"" else 2,\n        compress=""DEFLATE"",\n    )\n\n    if nodata is not None:\n        rio_opts.update(nodata=nodata)\n\n    rio_opts.update(extra_rio_opts)\n\n    # Deal efficiently with ""no overviews needed case""\n    if len(overview_levels) == 0:\n        if fname == "":mem:"":\n            with rasterio.MemoryFile() as mem:\n                with mem.open(driver=""GTiff"", **rio_opts) as dst:\n                    dst.write(pix, band)\n                return bytes(mem.getbuffer())\n        else:\n            with rasterio.open(path, mode=\'w\', driver=\'GTiff\', **rio_opts) as dst:\n                dst.write(pix, band)\n            return path\n\n    # copy re-compresses anyway so skip compression for temp image\n    tmp_opts = rio_opts.copy()\n    tmp_opts.pop(""compress"")\n    tmp_opts.pop(""predictor"")\n    tmp_opts.pop(""zlevel"")\n\n    with rasterio.Env(GDAL_TIFF_OVR_BLOCKSIZE=ovr_blocksize):\n        with rasterio.MemoryFile() as mem:\n            with mem.open(driver=""GTiff"", **tmp_opts) as tmp:\n                tmp.write(pix, band)\n                tmp.build_overviews(overview_levels, resampling)\n\n                if fname == "":mem:"":\n                    with rasterio.MemoryFile() as mem2:\n                        rio_copy(\n                            tmp,\n                            mem2.name,\n                            driver=""GTiff"",\n                            copy_src_overviews=True,\n                            **rio_opts\n                        )\n                        return bytes(mem2.getbuffer())\n\n                rio_copy(\n                    tmp, path, driver=""GTiff"", copy_src_overviews=True, **rio_opts\n                )\n\n    return path\n\n\n_delayed_write_cog_to_mem = dask.delayed(  # pylint: disable=invalid-name\n    _write_cog,\n    name=""compress-cog"", pure=True, nout=1\n)\n\n_delayed_write_cog_to_file = dask.delayed(   # pylint: disable=invalid-name\n    _write_cog,\n    name=""save-cog"", pure=False, nout=1\n)\n\n\ndef write_cog(geo_im: xr.DataArray,\n              fname: Union[str, Path],\n              blocksize: Optional[int] = None,\n              ovr_blocksize: Optional[int] = None,\n              overview_resampling: Optional[str] = None,\n              overview_levels: Optional[List[int]] = None,\n              **extra_rio_opts) -> Union[Path, bytes, Delayed]:\n    """"""\n    Save ``xarray.DataArray`` to a file in Cloud Optimized GeoTiff format.\n\n    This function is ""Dask aware"". If ``geo_im`` is a Dask array, then the\n    output of this function is also a Dask Delayed object. This allows us to\n    save multiple images concurrently across a Dask cluster. If you are not\n    familiar with Dask this can be confusing, as no operation is performed until\n    ``.compute()`` method is called, so if you call this function with Dask\n    array it will return immediately without writing anything to disk.\n\n    If you are using Dask to speed up data loading, follow example below:\n\n    .. code-block:: python\n\n       # Example: save red band from first time slice to file ""red.tif""\n       xx = dc.load(.., dask_chunks=dict(x=1024, y=1024))\n       write_cog(xx.isel(time=0).red, ""red.tif"").compute()\n       # or compute input first instead\n       write_cog(xx.isel(time=0).red.compute(), ""red.tif"")\n\n    :param geo_im: ``xarray.DataArray`` with crs\n    :param fname: Output path or ``"":mem:""`` in which case compress to RAM and return bytes\n    :param blocksize: Size of internal tiff tiles (512x512 pixels)\n    :param ovr_blocksize: Size of internal tiles in overview images (defaults to blocksize)\n    :param overview_resampling: Use this resampling when computing overviews\n    :param overview_levels: List of shrink factors to compute overiews for: [2,4,8,16,32],\n                            to disable overviews supply empty list ``[]``\n    :param extra_rio_opts: Any other option is passed to ``rasterio.open``\n\n    :returns: Path to which output was written\n    :returns: Bytes if ``fname="":mem:""``\n    :returns: ``dask.Delayed`` object if input is a Dask array\n\n    .. note ::\n\n       **memory requirements**\n\n       This function generates temporary in memory tiff file without\n       compression to speed things up. It then adds overviews to this file and\n       only then copies it to the final destination with requested compression\n       settings. This is necessary to produce compliant COG, since COG standard\n       demands overviews to be placed before native resolution data and double\n       pass is the only way to achieve this currently.\n\n       This means that this function will use about 1.5 to 2 times memory taken by ``geo_im``.\n    """"""\n    pix = geo_im.data\n    geobox = getattr(geo_im, \'geobox\', None)\n    nodata = extra_rio_opts.pop(\'nodata\', None)\n    if nodata is None:\n        nodata = geo_im.attrs.get(""nodata"", None)\n\n    if geobox is None:\n        raise ValueError(""Need geo-registered array on input"")\n\n    if dask.is_dask_collection(pix):\n        real_op = _delayed_write_cog_to_mem if fname == "":mem:"" else _delayed_write_cog_to_file\n    else:\n        real_op = _write_cog\n\n    return real_op(\n        pix,\n        geobox,\n        fname,\n        nodata=nodata,\n        blocksize=blocksize,\n        ovr_blocksize=ovr_blocksize,\n        overview_resampling=overview_resampling,\n        overview_levels=overview_levels,\n        **extra_rio_opts)\n\n\ndef to_cog(geo_im: xr.DataArray,\n           blocksize: Optional[int] = None,\n           ovr_blocksize: Optional[int] = None,\n           overview_resampling: Optional[str] = None,\n           overview_levels: Optional[List[int]] = None,\n           **extra_rio_opts) -> Union[bytes, Delayed]:\n    """"""\n    Compress ``xarray.DataArray`` into Cloud Optimized GeoTiff bytes in memory.\n\n    This function doesn\'t write to disk, it compresses in RAM, this is useful\n    for saving data to S3 or other cloud object stores.\n\n    This function is ""Dask aware"". If ``geo_im`` is a Dask array, then the\n    output of this function is also a Dask Delayed object. This allows us to\n    compress multiple images concurrently across a Dask cluster. If you are not\n    familiar with Dask this can be confusing, as no operation is performed until\n    ``.compute()`` method is called, so if you call this function with Dask\n    array it will return immediately without compressing any data.\n\n    :param geo_im: ``xarray.DataArray`` with crs\n    :param blocksize: Size of internal tiff tiles (512x512 pixels)\n    :param ovr_blocksize: Size of internal tiles in overview images (defaults to blocksize)\n    :param overview_resampling: Use this resampling when computing overviews\n    :param overview_levels: List of shrink factors to compute overiews for: [2,4,8,16,32]\n    :param extra_rio_opts: Any other option is passed to ``rasterio.open``\n\n    :returns: In-memory GeoTiff file as bytes\n    :returns: ``dask.Delayed`` object if input is a Dask array\n\n    Also see :py:meth:`~datacube.utils.cog.write_cog`\n\n    """"""\n    bb = write_cog(geo_im,\n                   "":mem:"",\n                   blocksize=blocksize,\n                   ovr_blocksize=ovr_blocksize,\n                   overview_resampling=overview_resampling,\n                   overview_levels=overview_levels,\n                   **extra_rio_opts)\n\n    assert isinstance(bb, (bytes, Delayed))  # for mypy sake for :mem: output it bytes or delayed bytes\n    return bb\n'"
datacube/utils/dask.py,0,"b'"""""" Dask Distributed Tools\n\n""""""\nfrom typing import Any, Iterable, Optional, Union, Tuple\nfrom random import randint\nimport toolz\nimport queue\nfrom dask.distributed import Client\nimport dask\nimport threading\nimport logging\nimport os\nfrom botocore.credentials import ReadOnlyCredentials\nfrom botocore.exceptions import BotoCoreError\nfrom .aws import s3_dump, s3_client\n\n\n__all__ = (\n    ""start_local_dask"",\n    ""pmap"",\n    ""compute_tasks"",\n    ""partition_map"",\n    ""save_blob_to_file"",\n    ""save_blob_to_s3"",\n)\n\n_LOG = logging.getLogger(__name__)\n\n\ndef get_total_available_memory(check_jupyter_hub=True):\n    """""" Figure out how much memory is available\n        1. Check MEM_LIMIT environment variable, set by jupyterhub\n        2. Use hardware information if that not set\n    """"""\n    if check_jupyter_hub:\n        mem_limit = os.environ.get(\'MEM_LIMIT\', None)\n        if mem_limit is not None:\n            return int(mem_limit)\n\n    from psutil import virtual_memory\n    return virtual_memory().total\n\n\ndef compute_memory_per_worker(n_workers: int = 1,\n                              mem_safety_margin: Optional[Union[str, int]] = None,\n                              memory_limit: Optional[Union[str, int]] = None) -> int:\n    """""" Figure out how much memory to assign per worker.\n\n        result can be passed into `memory_limit=` parameter of dask worker/cluster/client\n    """"""\n    from dask.utils import parse_bytes\n\n    if isinstance(memory_limit, str):\n        memory_limit = parse_bytes(memory_limit)\n\n    if isinstance(mem_safety_margin, str):\n        mem_safety_margin = parse_bytes(mem_safety_margin)\n\n    if memory_limit is None and mem_safety_margin is None:\n        total_bytes = get_total_available_memory()\n        # leave 500Mb or half of all memory if RAM is less than 1 Gb\n        mem_safety_margin = min(500*(1024*1024), total_bytes//2)\n    elif memory_limit is None:\n        total_bytes = get_total_available_memory()\n    elif mem_safety_margin is None:\n        total_bytes = memory_limit\n        mem_safety_margin = 0\n    else:\n        total_bytes = memory_limit\n\n    return (total_bytes - mem_safety_margin)//n_workers\n\n\ndef start_local_dask(n_workers: int = 1,\n                     threads_per_worker: Optional[int] = None,\n                     mem_safety_margin: Optional[Union[str, int]] = None,\n                     memory_limit: Optional[Union[str, int]] = None,\n                     **kw):\n    """"""Wrapper around `distributed.Client(..)` constructor that deals with memory better.\n\n    :param n_workers: number of worker processes to launch\n    :param threads_per_worker: number of threads per worker, default is as many as there are CPUs\n    :param mem_safety_margin: bytes to reserve for the rest of the system, only applicable\n                              if `memory_limit=` is not supplied.\n\n    NOTE: if `memory_limit` is supplied, it will be parsed and divided equally between workers.\n    """"""\n    memory_limit = compute_memory_per_worker(n_workers=n_workers,\n                                             memory_limit=memory_limit,\n                                             mem_safety_margin=mem_safety_margin)\n\n    client = Client(n_workers=n_workers,\n                    threads_per_worker=threads_per_worker,\n                    memory_limit=memory_limit,\n                    **kw)\n\n    return client\n\n\ndef _randomize(prefix):\n    return \'{}-{:08x}\'.format(prefix, randint(0, 0xFFFFFFFF))\n\n\ndef partition_map(n: int, func: Any, its: Iterable[Any],\n                  name: str = \'compute\') -> Iterable[Any]:\n    """""" Partition sequence into lumps of size `n`, then construct dask delayed computation evaluating to:\n\n    [func(x) for x in its[0:1n]],\n    [func(x) for x in its[n:2n]],\n    ...\n    [func(x) for x in its[]],\n\n    :param n: number of elements to process in one go\n    :param func: Function to apply (non-dask)\n    :param its:  Values to feed to fun\n    :param name: How the computation should be named in dask visualizations\n    """"""\n    def lump_proc(dd):\n        return [func(d) for d in dd]\n\n    proc = dask.delayed(lump_proc, nout=1, pure=True)\n    data_name = _randomize(\'data_\' + name)\n    name = _randomize(name)\n\n    for i, dd in enumerate(toolz.partition_all(n, its)):\n        lump = dask.delayed(dd,\n                            pure=True,\n                            traverse=False,\n                            name=data_name + str(i))\n        yield proc(lump, dask_key_name=name + str(i))\n\n\ndef compute_tasks(tasks: Iterable[Any], client: Client,\n                  max_in_flight: int = 3) -> Iterable[Any]:\n    """""" Parallel compute stream with back pressure.\n\n        Equivalent to:\n\n        (client.compute(task).result()\n          for task in tasks)\n\n        but with up to `max_in_flight` tasks being processed at the same time.\n        Input/Output order is preserved, so there is a possibility of head of\n        line blocking.\n\n        NOTE: lower limit is 3 concurrent tasks to simplify implementation,\n              there is no point calling this function if you want one active\n              task and supporting exactly 2 active tasks is not worth the complexity,\n              for now. We might special-case `2` at some point.\n\n    """"""\n    # New thread:\n    #    1. Take dask task from iterator\n    #    2. Submit to client for processing\n    #    3. Send it of to wrk_q\n    #\n    # Calling thread:\n    #    1. Pull scheduled future from wrk_q\n    #    2. Wait for result of the future\n    #    3. yield result to calling code\n    from .generic import it2q, qmap\n\n    # (max_in_flight - 2) -- one on each side of queue\n    wrk_q = queue.Queue(maxsize=max(1, max_in_flight - 2))  # type: queue.Queue\n\n    # fifo_timeout=\'0ms\' ensures that priority of later tasks is lower\n    futures = (client.compute(task, fifo_timeout=\'0ms\') for task in tasks)\n\n    in_thread = threading.Thread(target=it2q, args=(futures, wrk_q))\n    in_thread.start()\n\n    yield from qmap(lambda f: f.result(), wrk_q)\n\n    in_thread.join()\n\n\ndef pmap(func: Any,\n         its: Iterable[Any],\n         client: Client,\n         lump: int = 1,\n         max_in_flight: int = 3,\n         name: str = \'compute\') -> Iterable[Any]:\n    """""" Parallel map with back pressure.\n\n    Equivalent to this:\n\n       (func(x) for x in its)\n\n    Except that ``func(x)`` runs concurrently on dask cluster.\n\n    :param func:   Method that will be applied concurrently to data from ``its``\n    :param its:    Iterator of input values\n    :param client: Connected dask client\n    :param lump:   Group this many datasets into one task\n    :param max_in_flight: Maximum number of active tasks to submit\n    :param name:   Dask name for computation\n    """"""\n    max_in_flight = max_in_flight // lump\n\n    tasks = partition_map(lump, func, its, name=name)\n\n    for xx in compute_tasks(tasks, client=client, max_in_flight=max_in_flight):\n        yield from xx\n\n\ndef _save_blob_to_file(data: Union[bytes, str],\n                       fname: str) -> Tuple[str, bool]:\n    if isinstance(data, str):\n        data = data.encode(\'utf8\')\n\n    try:\n        with open(fname, \'wb\') as f:\n            f.write(data)\n    except IOError:\n        return (fname, False)\n\n    return (fname, True)\n\n\n@dask.delayed(name=\'save-to-disk\', pure=False)\ndef save_blob_to_file(data,\n                      fname,\n                      with_deps=None):\n    """""" Dump from memory to local filesystem as a dask delayed operation.\n\n    NOTE: dask workers better be local or have network filesystem mounted in\n    the same path as calling code.\n\n    :param data     : Data blob to save to file (have to fit into memory all at once),\n                      strings will be saved in UTF8 format.\n    :param fname    : Path to file\n    :param with_deps: Useful for introducing dependencies into dask graph,\n                      for example save yaml file after saving all tiff files.\n\n    Returns\n    -------\n    (File Path, True) tuple on success\n    (File Path, False) on any error\n    """"""\n    return _save_blob_to_file(data, fname)\n\n\ndef _save_blob_to_s3(data: Union[bytes, str],\n                     url: str,\n                     profile: Optional[str] = None,\n                     creds: Optional[ReadOnlyCredentials] = None,\n                     region_name: Optional[str] = None,\n                     **kw) -> Tuple[str, bool]:\n    """""" Dump from memory to S3 as a dask delayed operation.\n\n    :param data       : Data blob to save to file (have to fit into memory all at once)\n    :param url        : Url in a form s3://bucket/path/to/file\n\n    :param profile    : Profile name to lookup (only used if session is not supplied)\n    :param creds      : Override credentials with supplied data\n    :param region_name: Region name to use, overrides session setting\n\n    Returns\n    -------\n    (url, True) tuple on success\n    (url, False) on any error\n    """"""\n    from botocore.errorfactory import ClientError\n    try:\n        s3 = s3_client(profile=profile,\n                       creds=creds,\n                       region_name=region_name,\n                       cache=True)\n\n        result = s3_dump(data, url, s3=s3, **kw)\n    except (IOError, BotoCoreError, ClientError):\n        result = False\n\n    return url, result\n\n\n@dask.delayed(name=\'save-to-s3\', pure=False)\ndef save_blob_to_s3(data,\n                    url,\n                    profile=None,\n                    creds=None,\n                    region_name=None,\n                    with_deps=None,\n                    **kw):\n    """""" Dump from memory to S3 as a dask delayed operation.\n\n    :param data       : Data blob to save to file (have to fit into memory all at once)\n    :param url        : Url in a form s3://bucket/path/to/file\n\n    :param profile    : Profile name to lookup (only used if session is not supplied)\n    :param creds      : Override credentials with supplied data\n    :param region_name: Region name to use, overrides session setting\n\n    :param with_deps  : Useful for introducing dependencies into dask graph,\n                        for example save yaml file after saving all tiff files.\n\n    Returns\n    -------\n    (url, True) tuple on success\n    (url, False) on any error\n    """"""\n    return _save_blob_to_s3(data, url,\n                            profile=profile,\n                            creds=creds,\n                            region_name=region_name,\n                            **kw)\n'"
datacube/utils/dates.py,1,"b'""""""\nDate and time utility functions\n\nIncludes sequence generation functions to be used by statistics apps\n\n""""""\nfrom typing import Union, Optional, Callable\nfrom datetime import datetime\n\nimport dateutil\nimport dateutil.parser\nfrom dateutil.relativedelta import relativedelta\nfrom dateutil.rrule import YEARLY, MONTHLY, DAILY, rrule\nfrom dateutil.tz import tzutc\nimport numpy as np\nimport xarray as xr\n\n\nFREQS = {\'y\': YEARLY, \'m\': MONTHLY, \'d\': DAILY}\nDURATIONS = {\'y\': \'years\', \'m\': \'months\', \'d\': \'days\'}\n\n\ndef date_sequence(start, end, stats_duration, step_size):\n    """"""\n    Generate a sequence of time span tuples\n\n    :seealso:\n        Refer to `dateutil.parser.parse` for details on date parsing.\n\n    :param str start: Start date of first interval\n    :param str end: End date. The end of the last time span may extend past this date.\n    :param str stats_duration: What period of time should be grouped\n    :param str step_size: How far apart should the start dates be\n    :return: sequence of (start_date, end_date) tuples\n    """"""\n    step_size, freq = parse_interval(step_size)\n    stats_duration = parse_duration(stats_duration)\n    for start_date in rrule(freq, interval=step_size, dtstart=start, until=end):\n        end_date = start_date + stats_duration\n        if end_date <= end:\n            yield start_date, start_date + stats_duration\n\n\ndef parse_interval(interval):\n    count, units = split_duration(interval)\n    try:\n        return count, FREQS[units]\n    except KeyError:\n        raise ValueError(\'Invalid interval ""{}"", units not in of: {}\'.format(interval, FREQS.keys))\n\n\ndef parse_duration(duration):\n    count, units = split_duration(duration)\n    try:\n        delta = {DURATIONS[units]: count}\n    except KeyError:\n        raise ValueError(\'Duration ""{}"" not in months or years\'.format(duration))\n\n    return relativedelta(**delta)\n\n\ndef split_duration(duration):\n    return int(duration[:-1]), duration[-1:]\n\n\ndef datetime_to_seconds_since_1970(dt):\n    epoch = datetime(1970, 1, 1, 0, 0, 0, tzinfo=tzutc() if dt.tzinfo else None)\n    return (dt - epoch).total_seconds()\n\n\ndef _parse_time_generic(time: Union[str, datetime]) -> datetime:\n    """"""Convert string to datetime object\n\n    Calling this on datetime object is a no-op.\n    """"""\n    if isinstance(time, str):\n        return dateutil.parser.parse(time)\n    return time\n\n\ndef _parse_time_ciso8601(time: Union[str, datetime]) -> datetime:\n    """"""Convert string to datetime object\n\n    This function deals with ISO8601 dates fast, and fallbacks to python for\n    other formats.\n\n    Calling this on datetime object is a no-op.\n    """"""\n    from ciso8601 import parse_datetime\n\n    if isinstance(time, datetime):\n        return time\n\n    try:\n        return parse_datetime(time)\n    except Exception:  # pylint: disable=broad-except\n        return _parse_time_generic(time)\n\n\ndef normalise_dt(dt: Union[str, datetime]) -> datetime:\n    """""" Turn strings into dates, turn timestamps with timezone info into UTC and remove timezone info.\n    """"""\n    if isinstance(dt, str):\n        dt = parse_time(dt)\n    if dt.tzinfo is not None:\n        dt = dt.astimezone(tzutc()).replace(tzinfo=None)\n    return dt\n\n\ndef mk_time_coord(dts, name=\'time\', units=\'seconds since 1970-01-01 00:00:00\'):\n    """""" List[datetime] -> time coordinate for xarray\n    """"""\n\n    dts = [normalise_dt(dt) for dt in dts]\n    data = np.asarray(dts, dtype=\'datetime64\')\n    return xr.DataArray(data,\n                        name=name,\n                        coords={name: data},\n                        dims=(name,),\n                        attrs={\'units\': units})\n\ndef _mk_parse_time()->Callable[[Union[str, datetime]], datetime]:\n    try:\n        import ciso8601             # pylint: disable=wrong-import-position\n        return _parse_time_ciso8601\n    except ImportError:             # pragma: no cover\n        return _parse_time_generic  # pragma: no cover\n\nparse_time = _mk_parse_time()  # pylint: disable=invalid-name\n'"
datacube/utils/documents.py,0,"b'###\n# Functions for working with YAML documents and configurations\n###\nimport gzip\nimport json\nimport logging\nimport sys\nfrom collections import OrderedDict, Mapping\nfrom contextlib import contextmanager\nfrom itertools import chain\nfrom pathlib import Path\nfrom urllib.parse import urlparse\nfrom urllib.request import urlopen\nimport typing\nfrom typing import Dict, Any\nfrom copy import deepcopy\n\nimport numpy\nimport toolz\nimport yaml\n\ntry:\n    from yaml import CSafeLoader as SafeLoader  # type: ignore\nexcept ImportError:\n    from yaml import SafeLoader  # type: ignore\n\nfrom datacube.utils.generic import map_with_lookahead\nfrom datacube.utils.uris import mk_part_uri, as_url, uri_to_local_path\n\nPY35 = sys.version_info <= (3, 6)\n_LOG = logging.getLogger(__name__)\n\n\n@contextmanager\ndef _open_from_s3(url):\n    o = urlparse(url)\n    if o.scheme != \'s3\':\n        raise RuntimeError(""Abort abort I don\'t know how to open non s3 urls"")\n\n    import boto3\n    s3 = boto3.resource(""s3"")\n\n    bucket = o.netloc\n    key = o.path[1:]\n    obj = s3.Object(bucket, key).get(ResponseCacheControl=\'no-cache\')\n\n    yield obj[\'Body\']\n    _LOG.debug(""Closing %s"", obj)\n\n\ndef _open_with_urllib(url):\n    return urlopen(url)\n\n\n_PROTOCOL_OPENERS = {\n    \'s3\': _open_from_s3,\n    \'ftp\': _open_with_urllib,\n    \'http\': _open_with_urllib,\n    \'https\': _open_with_urllib,\n    \'file\': _open_with_urllib\n}\n\n\ndef load_from_yaml(handle, parse_dates=False):\n    loader = SafeLoader if parse_dates else NoDatesSafeLoader\n    yield from yaml.load_all(handle, Loader=loader)\n\n\ndef parse_yaml(doc: str) -> typing.Mapping[str, Any]:\n    """""" Convert a single document yaml string into a parsed document\n    """"""\n    return yaml.load(doc, Loader=SafeLoader)\n\n\ndef load_from_json(handle):\n    yield json.load(handle)\n\n\ndef load_from_netcdf(path):\n    for doc in read_strings_from_netcdf(path, variable=\'dataset\'):\n        yield yaml.load(doc, Loader=NoDatesSafeLoader)\n\n\n_PARSERS = {\n    \'.yaml\': load_from_yaml,\n    \'.yml\': load_from_yaml,\n    \'.json\': load_from_json,\n}\n\n\ndef load_documents(path):\n    """"""\n    Load document/s from the specified path.\n\n    At the moment can handle:\n\n     - JSON and YAML locally and remotely.\n     - Compressed JSON and YAML locally\n     - Data Cube Dataset Documents inside local NetCDF files.\n\n    :param path: path or URI to load documents from\n    :return: generator of dicts\n    """"""\n    path = str(path)\n    url = as_url(path)\n    scheme = urlparse(url).scheme\n    compressed = url[-3:] == \'.gz\'\n\n    if scheme == \'file\' and path[-3:] == \'.nc\':\n        path = uri_to_local_path(url)\n        yield from load_from_netcdf(path)\n    else:\n        with _PROTOCOL_OPENERS[scheme](url) as fh:\n            if compressed:\n                fh = gzip.open(fh)\n                path = path[:-3]\n\n            suffix = Path(path).suffix\n\n            parser = _PARSERS[suffix]\n\n            yield from parser(fh)\n\n\ndef read_documents(*paths, uri=False):\n    """"""\n    Read and parse documents from the filesystem or remote URLs (yaml or json).\n\n    Note that a single yaml file can contain multiple documents.\n\n    This function will load any dates in the documents as strings. In\n    Data Cube we store JSONB in PostgreSQL and it will turn our dates\n    into strings anyway.\n\n    :param uri: When True yield URIs instead of Paths\n    :param paths: input Paths or URIs\n    :type uri: Bool\n    :rtype: tuple[(str, dict)]\n    """"""\n\n    def process_file(path):\n        docs = load_documents(path)\n\n        if not uri:\n            for doc in docs:\n                yield path, doc\n        else:\n            url = as_url(path)\n\n            def add_uri_no_part(x):\n                idx, doc = x\n                return url, doc\n\n            def add_uri_with_part(x):\n                idx, doc = x\n                return mk_part_uri(url, idx), doc\n\n            yield from map_with_lookahead(enumerate(docs),\n                                          if_one=add_uri_no_part,\n                                          if_many=add_uri_with_part)\n\n    for path in paths:\n        try:\n            yield from process_file(path)\n        except InvalidDocException as e:\n            raise e\n        except (yaml.YAMLError, ValueError) as e:\n            raise InvalidDocException(\'Failed to load %s: %s\' % (path, e))\n        except Exception as e:\n            raise InvalidDocException(\'Failed to load %s: %s\' % (path, e))\n\n\ndef netcdf_extract_string(chars):\n    """"""\n    Convert netcdf S|U chars to Unicode string.\n    """"""\n    import netCDF4\n\n    if isinstance(chars, str):\n        return chars\n\n    chars = netCDF4.chartostring(chars)\n    if chars.dtype.kind == \'U\':\n        return str(chars)\n    else:\n        return str(numpy.char.decode(chars))\n\n\ndef read_strings_from_netcdf(path, variable):\n    """"""\n    Load all of the string encoded data from a variable in a NetCDF file.\n\n    By \'string\', the CF conventions mean ascii.\n\n    Useful for loading dataset metadata information.\n    """"""\n    import netCDF4\n\n    with netCDF4.Dataset(str(path)) as ds:\n        for chars in ds[variable]:\n            yield netcdf_extract_string(chars)\n\n\ndef validate_document(document, schema, schema_folder=None):\n    import jsonschema\n\n    try:\n        # Allow schemas to reference other schemas in the given folder.\n        def doc_reference(path):\n            path = Path(schema_folder).joinpath(path)\n            if not path.exists():\n                raise ValueError(""Reference not found: %s"" % path)\n            referenced_schema = next(iter(read_documents(path)))[1]\n            return referenced_schema\n\n        jsonschema.Draft4Validator.check_schema(schema)\n        ref_resolver = jsonschema.RefResolver.from_schema(\n            schema,\n            handlers={\'\': doc_reference} if schema_folder else ()\n        )\n        validator = jsonschema.Draft4Validator(schema, resolver=ref_resolver)\n        validator.validate(document)\n    except jsonschema.ValidationError as e:\n        raise InvalidDocException(e)\n\n\n_DOCUMENT_EXTENSIONS = (\'.yaml\', \'.yml\', \'.json\', \'.nc\')\n_COMPRESSION_EXTENSIONS = (\'\', \'.gz\')\n_ALL_SUPPORTED_EXTENSIONS = tuple(doc_type + compression_type\n                                  for doc_type in _DOCUMENT_EXTENSIONS\n                                  for compression_type in _COMPRESSION_EXTENSIONS)\n\n\ndef is_supported_document_type(path):\n    """"""\n    Does a document path look like a supported type?\n\n    :type path: Union[Path, str]\n    :rtype: bool\n    """"""\n    return any([str(path).lower().endswith(suffix) for suffix in _ALL_SUPPORTED_EXTENSIONS])\n\n\nclass NoDatesSafeLoader(SafeLoader):  # pylint: disable=too-many-ancestors\n    @classmethod\n    def remove_implicit_resolver(cls, tag_to_remove):\n        """"""\n        Removes implicit resolvers for a particular tag\n\n        Takes care not to modify resolvers in super classes.\n\n        We want to load datetimes as strings, not dates. We go on to\n        serialise as json which doesn\'t have the advanced types of\n        yaml, and leads to slightly different objects down the track.\n        """"""\n        if \'yaml_implicit_resolvers\' not in cls.__dict__:\n            cls.yaml_implicit_resolvers = cls.yaml_implicit_resolvers.copy()\n\n        for first_letter, mappings in cls.yaml_implicit_resolvers.items():\n            cls.yaml_implicit_resolvers[first_letter] = [(tag, regexp)\n                                                         for tag, regexp in mappings\n                                                         if tag != tag_to_remove]\n\n\nNoDatesSafeLoader.remove_implicit_resolver(\'tag:yaml.org,2002:timestamp\')\n\n\nclass InvalidDocException(Exception):\n    pass\n\n\ndef get_doc_offset(offset, document):\n    """"""\n    :type offset: list[str]\n    :type document: dict\n\n    """"""\n    return toolz.get_in(offset, document, no_default=True)\n\n\ndef get_doc_offset_safe(offset, document, value_if_missing=None):\n    """"""\n    :type offset: list[str]\n    :type document: dict\n\n    """"""\n    return toolz.get_in(offset, document, default=value_if_missing)\n\n\ndef transform_object_tree(f, o, key_transform=lambda k: k):\n    """"""\n    Apply a function (f) on all the values in the given document tree (o), returning a new document of\n    the results.\n\n    Recurses through container types (dicts, lists, tuples).\n\n    Returns a new instance (deep copy) without modifying the original.\n\n    :param f: Function to apply on values.\n    :param o: document/object\n    :param key_transform: Optional function to apply on any dictionary keys.\n\n    """"""\n\n    def recur(o_):\n        return transform_object_tree(f, o_, key_transform=key_transform)\n\n    if isinstance(o, OrderedDict):\n        return OrderedDict((key_transform(k), recur(v)) for k, v in o.items())\n    if isinstance(o, dict):\n        return {key_transform(k): recur(v) for k, v in o.items()}\n    if isinstance(o, list):\n        return [recur(v) for v in o]\n    if isinstance(o, tuple):\n        return tuple(recur(v) for v in o)\n    return f(o)\n\n\nclass SimpleDocNav(object):\n    """"""\n    Allows navigation of Dataset metadata document lineage tree without\n    creating full Dataset objects.\n\n    This has the assumption that a dictionary of source datasets is\n    found at the offset ``lineage -> source_datasets`` inside each\n    dataset dictionary.\n\n    """"""\n\n    def __init__(self, doc):\n        if not isinstance(doc, Mapping):\n            raise ValueError("""")\n\n        self._doc = doc\n        self._doc_without = None\n        self._sources_path = (\'lineage\', \'source_datasets\')\n        self._sources = None\n\n    @property\n    def doc(self):\n        return self._doc\n\n    @property\n    def doc_without_lineage_sources(self):\n        if self._doc_without is None:\n            self._doc_without = toolz.assoc_in(self._doc, self._sources_path, {})\n\n        return self._doc_without\n\n    @property\n    def id(self):\n        return self._doc.get(\'id\', None)\n\n    @property\n    def sources(self):\n        if self._sources is None:\n            self._sources = {k: SimpleDocNav(v)\n                             for k, v in get_doc_offset_safe(self._sources_path, self._doc, {}).items()}\n        return self._sources\n\n    @property\n    def sources_path(self):\n        return self._sources_path\n\n\ndef _set_doc_offset(offset, document, value):\n    """"""\n    :type offset: list[str]\n    :type document: dict\n\n    """"""\n    read_offset = offset[:-1]\n    sub_doc = get_doc_offset(read_offset, document)\n    sub_doc[offset[-1]] = value\n\n\nclass DocReader(object):\n    def __init__(self, type_definition, search_fields, doc):\n        """"""\n        :type system_offsets: dict[str,list[str]]\n        :type doc: dict\n        """"""\n        self.__dict__[\'_doc\'] = doc\n\n        # The user-configurable search fields for this dataset type.\n        self.__dict__[\'_search_fields\'] = {name: field\n                                           for name, field in search_fields.items()\n                                           if hasattr(field, \'extract\')}\n\n        # The field offsets that the datacube itself understands: id, format, sources etc.\n        # (See the metadata-type-schema.yaml or the comments in default-metadata-types.yaml)\n        self.__dict__[\'_system_offsets\'] = {name: field\n                                            for name, field in type_definition.items()\n                                            if name != \'search_fields\'}\n\n    def __getattr__(self, name):\n        offset = self._system_offsets.get(name)\n        field = self._search_fields.get(name)\n        if offset:\n            return get_doc_offset_safe(offset, self._doc)\n        elif field:\n            return field.extract(self._doc)\n        else:\n            raise AttributeError(\n                \'Unknown field %r. Expected one of %r\' % (\n                    name, list(chain(self._system_offsets.keys(), self._search_fields.keys()))\n                )\n            )\n\n    def __setattr__(self, name, val):\n        offset = self._system_offsets.get(name)\n        if offset is None:\n            raise AttributeError(\n                \'Unknown field offset %r. Expected one of %r\' % (\n                    name, list(self._fields.keys())\n                )\n            )\n        return _set_doc_offset(offset, self._doc, val)\n\n    @property\n    def fields(self):\n        fields = {}\n        fields.update(self.search_fields)\n        fields.update(self.system_fields)\n        return fields\n\n    @property\n    def search_fields(self):\n        fields = {}\n        for name, field in self._search_fields.items():\n            try:\n                fields[name] = field.extract(self._doc)\n            except (AttributeError, KeyError, ValueError):\n                continue\n        return fields\n\n    @property\n    def system_fields(self):\n        fields = {}\n        for name, offset in self._system_offsets.items():\n            try:\n                fields[name] = get_doc_offset(offset, self._doc)\n            except (AttributeError, KeyError, ValueError):\n                continue\n        return fields\n\n    def __dir__(self):\n        return list(self.fields)\n\n\ndef without_lineage_sources(doc: Dict[str, Any],\n                            spec,\n                            inplace: bool = False) -> Dict[str, Any]:\n    """""" Replace lineage.source_datasets with {}\n\n    :param dict doc: parsed yaml/json document describing dataset\n    :param spec: Product or MetadataType according to which `doc` to be interpreted\n    :param bool inplace: If True modify `doc` in place\n    """"""\n\n    if not inplace:\n        doc = deepcopy(doc)\n\n    doc_view = spec.dataset_reader(doc)\n\n    if \'sources\' in doc_view.fields:\n        doc_view.sources = {}\n\n    return doc\n\n\ndef schema_validated(schema):\n    """"""\n    Decorate a class to enable validating its definition against a JSON Schema file.\n\n    Adds a self.validate() method which takes a dict used to populate the instantiated class.\n\n    :param pathlib.Path schema: filename of the json schema, relative to `SCHEMA_PATH`\n    :return: wrapped class\n    """"""\n\n    def validate(cls, document):\n        return validate_document(document, cls.schema, schema.parent)\n\n    def decorate(cls):\n        cls.schema = next(iter(read_documents(schema)))[1]\n        cls.validate = classmethod(validate)\n        return cls\n\n    return decorate\n\n\ndef _readable_offset(offset):\n    return \'.\'.join(map(str, offset))\n'"
datacube/utils/generic.py,0,"b'import itertools\nimport threading\nfrom typing import Any\n\nEOS = object()\n_LCL = threading.local()\n\n__all__ = (\n    ""EOS"",\n    ""map_with_lookahead"",\n    ""qmap"",\n    ""it2q"",\n    ""thread_local_cache"",\n)\n\n\ndef map_with_lookahead(it, if_one=None, if_many=None):\n    """"""\n    It\'s like normal map: creates a new generator by applying a function to every\n    element of the original generator, but it applies `if_one` transform for\n    single element sequences and `if_many` transform for multi-element sequences.\n\n    If iterators supported `len` it would be equivalent to the code below::\n\n        proc = if_many if len(it) > 1 else if_one\n        return map(proc, it)\n\n    :param it: Sequence to iterate over\n    :param if_one: Function to apply for single element sequences\n    :param if_many: Function to apply for multi-element sequences\n\n    """"""\n    if_one = if_one or (lambda x: x)\n    if_many = if_many or (lambda x: x)\n\n    it = iter(it)\n    p1 = list(itertools.islice(it, 2))\n    proc = if_many if len(p1) > 1 else if_one\n\n    for v in itertools.chain(iter(p1), it):\n        yield proc(v)\n\n\ndef qmap(func, q, eos_marker=EOS):\n    """""" Converts queue to an iterator.\n\n    For every `item` in the `q` that is not `eos_marker`, `yield proc(item)`\n\n    Takes care of calling `.task_done()` on every item extracted from the queue.\n    """"""\n    while True:\n        item = q.get(block=True)\n        if item is eos_marker:\n            q.task_done()\n            break\n        else:\n            try:\n                yield func(item)\n            finally:\n                q.task_done()\n\n\ndef it2q(its, q, eos_marker=EOS):\n    """""" Convert iterator into a Queue\n\n        [1, 2, 3] => [1, 2, 3, eos_marker]\n    """"""\n    try:\n        for x in its:\n            q.put(x, block=True)\n    finally:\n        q.put(eos_marker, block=True)\n\n\ndef thread_local_cache(name: str,\n                       initial_value: Any = None,\n                       purge: bool = False) -> Any:\n    """""" Define/get thread local object with a given name.\n\n    :param name:          name for this cache\n    :param initial_value: Initial value if not set for this thread\n    :param purge:         If True delete from cache (returning what was there previously)\n\n    Returns\n    -------\n    value previously set in the thread or `initial_value`\n    """"""\n    absent = object()\n    cc = getattr(_LCL, name, absent)\n    absent = cc is absent\n\n    if absent:\n        cc = initial_value\n\n    if purge:\n        if not absent:\n            delattr(_LCL, name)\n    else:\n        if absent:\n            setattr(_LCL, name, cc)\n\n    return cc\n'"
datacube/utils/io.py,0,"b'import os\nfrom pathlib import Path\nfrom typing import Union, Optional\n\n\ndef _norm_path(path: Union[str, Path], in_home_dir: bool = False) -> Path:\n    if isinstance(path, str):\n        path = Path(path)\n    if in_home_dir:\n        path = Path.home()/path\n    return path\n\n\ndef check_write_path(fname: Union[Path, str], overwrite: bool) -> Path:\n    """""" Check is output file exists and either remove it first or raise IOError.\n\n    :param fname: string or Path object\n    :param overwrite: Whether to remove file when it exists\n\n    exists   overwrite   Action\n    ----------------------------------------------\n    T            T       delete file, return Path\n    T            F       raise IOError\n    F            T       return Path\n    F            F       return Path\n    """"""\n    if not isinstance(fname, Path):\n        fname = Path(fname)\n\n    if fname.exists():\n        if overwrite:\n            fname.unlink()\n        else:\n            raise IOError(""File exists"")\n    return fname\n\n\ndef write_user_secret_file(text: Union[str, bytes],\n                           fname: Union[str, Path],\n                           in_home_dir: bool = False,\n                           mode: str = \'w\'):\n    """"""Write file only readable/writeable by the user""""""\n\n    fname = _norm_path(fname, in_home_dir)\n    open_flags = os.O_WRONLY | os.O_CREAT | os.O_TRUNC\n    access = 0o600  # Make sure file is readable by current user only\n    with os.fdopen(os.open(str(fname), open_flags, access), mode) as handle:\n        handle.write(text)\n        handle.close()\n\n\ndef slurp(fname: Union[str, Path],\n          in_home_dir: bool = False,\n          mode: str = \'r\') -> Optional[Union[bytes, str]]:\n    """"""\n    Read an entire file into a string\n\n    :param fname: file path\n    :param in_home_dir: if True treat fname as a path relative to $HOME folder\n    :return: Content of a file or None if file doesn\'t exist or can not be read for any other reason\n    """"""\n    fname = _norm_path(fname, in_home_dir)\n    try:\n        with open(str(fname), mode) as handle:\n            return handle.read()\n    except IOError:\n        return None\n'"
datacube/utils/masking.py,0,"b'""""""\nTools for masking data based on a bit-mask variable with attached definition.\n\nThe main functions are `make_mask(variable)` `describe_flags(variable)`\n""""""\n\nimport collections\n\nimport pandas\nimport numpy\nimport xarray\nfrom xarray import DataArray, Dataset\n\nfrom datacube.utils.math import valid_mask\n\n\nFLAGS_ATTR_NAME = \'flags_definition\'\n\n\ndef list_flag_names(variable):\n    """"""\n    Returns the available masking flags for the variable\n\n    :param variable: Masking xarray.Dataset or xarray.DataArray\n    :return: list\n    """"""\n    flags_def = get_flags_def(variable)\n    return sorted(list(flags_def.keys()))\n\n\ndef describe_variable_flags(variable, with_pandas=True):\n    """"""\n    Returns either a Pandas Dataframe (with_pandas=True - default) or a string\n    (with_pandas=False) describing the available flags for a masking variable\n\n    Interprets the `flags_definition` attribute on the provided variable and\n    returns a Pandas Dataframe or string like::\n\n        Bits are listed from the MSB (bit 13) to the LSB (bit 0)\n        Bit     Value   Flag Name            Description\n        13      0       cloud_shadow_fmask   Cloud Shadow (Fmask)\n        12      0       cloud_shadow_acca    Cloud Shadow (ACCA)\n        11      0       cloud_fmask          Cloud (Fmask)\n        10      0       cloud_acca           Cloud (ACCA)\n\n    :param variable: Masking xarray.Dataset or xarray.DataArray\n    :return: Pandas Dataframe or str\n    """"""\n    flags_def = get_flags_def(variable)\n\n    if not with_pandas:\n        return describe_flags_def(flags_def)\n\n    return pandas.DataFrame.from_dict(flags_def, orient=\'index\')\n\n\ndef describe_flags_def(flags_def):\n    return \'\\n\'.join(generate_table(list(_table_contents(flags_def))))\n\n\ndef _table_contents(flags_def):\n    yield \'Flag name\', \'Description\', \'Bit. No\', \'Value\', \'Meaning\'\n    for name, defn in sorted(flags_def.items(), key=_order_bitdefs_by_bits):\n        name, desc = name, defn[\'description\']\n        for value, meaning in defn[\'values\'].items():\n            yield name, desc, str(defn[\'bits\']), str(value), str(meaning)\n            name, desc = \'\', \'\'\n\n\ndef _order_bitdefs_by_bits(bitdef):\n    name, defn = bitdef\n    try:\n        return min(defn[\'bits\'])\n    except TypeError:\n        return defn[\'bits\']\n\n\ndef make_mask(variable, **flags):\n    """"""\n    Returns a mask array, based on provided flags\n\n    When multiple flags are provided, they will be combined in a logical AND fashion.\n\n    For example:\n\n    >>> make_mask(pqa, cloud_acca=False, cloud_fmask=False, land_obs=True) # doctest: +SKIP\n\n    OR\n\n    >>> make_mask(pqa, **GOOD_PIXEL_FLAGS) # doctest: +SKIP\n\n    where `GOOD_PIXEL_FLAGS` is a dict of flag_name to True/False\n\n    :param variable:\n    :type variable: xarray.Dataset or xarray.DataArray\n    :param flags: list of boolean flags\n    :return: boolean xarray.DataArray or xarray.Dataset\n    """"""\n    flags_def = get_flags_def(variable)\n\n    mask, mask_value = create_mask_value(flags_def, **flags)\n\n    return variable & mask == mask_value\n\n\ndef valid_data_mask(data):\n    """"""\n    Returns bool arrays where the data is not `nodata`\n\n    :param Dataset or DataArray data:\n    :return: Dataset or DataArray\n    """"""\n    if isinstance(data, Dataset):\n        return data.apply(valid_data_mask)\n\n    if not isinstance(data, DataArray):\n        raise TypeError(\'valid_data_mask not supported for type {}\'.format(type(data)))\n\n    nodata = data.attrs.get(\'nodata\', None)\n\n    return xarray.apply_ufunc(valid_mask, data, nodata,\n                              dask=\'parallelized\',\n                              output_dtypes=[numpy.bool])\n\n\ndef mask_invalid_data(data, keep_attrs=True):\n    """"""\n    Sets all `nodata` values to ``nan``.\n\n    This will convert numeric data to type `float`.\n\n    :param Dataset or DataArray data:\n    :param bool keep_attrs: If the attributes of the data should be included in the returned .\n    :return: Dataset or DataArray\n    """"""\n    if isinstance(data, Dataset):\n        # Pass keep_attrs as a positional arg to the DataArray func\n        return data.apply(mask_invalid_data, keep_attrs=keep_attrs, args=(keep_attrs,))\n\n    if isinstance(data, DataArray):\n        if \'nodata\' not in data.attrs:\n            return data\n        out_data_array = data.where(data != data.nodata)\n        if keep_attrs:\n            out_data_array.attrs = {key: value\n                                    for key, value in data.attrs.items()\n                                    if key != \'nodata\'}\n        return out_data_array\n\n    raise TypeError(\'mask_invalid_data not supported for type {}\'.format(type(data)))\n\n\ndef create_mask_value(bits_def, **flags):\n    mask = 0\n    value = 0\n\n    for flag_name, flag_ref in flags.items():\n        defn = bits_def.get(flag_name, None)\n        if defn is None:\n            raise ValueError(f\'Unknown flag: ""{flag_name}""\')\n\n        try:\n            [flag_value] = (bit_val\n                            for bit_val, val_ref in defn[\'values\'].items()\n                            if val_ref == flag_ref)\n            flag_value = int(flag_value)  # Might be string if coming from DB\n        except ValueError:\n            raise ValueError(\'Unknown value %s specified for flag %s\' %\n                             (flag_ref, flag_name))\n\n        if isinstance(defn[\'bits\'], collections.abc.Iterable):  # Multi-bit flag\n            # Set mask\n            for bit in defn[\'bits\']:\n                mask = set_value_at_index(mask, bit, True)\n\n            shift = min(defn[\'bits\'])\n            real_val = flag_value << shift\n\n            value |= real_val\n\n        else:\n            bit = defn[\'bits\']\n            mask = set_value_at_index(mask, bit, True)\n            value = set_value_at_index(value, bit, flag_value)\n\n    return mask, value\n\n\ndef mask_to_dict(bits_def, mask_value):\n    """"""\n    Describes which flags are set for a mask value\n\n    :param bits_def:\n    :param mask_value:\n    :return: Mapping of flag_name -> set_value\n    :rtype: dict\n    """"""\n    return_dict = {}\n    for flag_name, flag_defn in bits_def.items():\n\n        # Make bits a list, even if there is only one\n        flag_bits = flag_defn[\'bits\']\n        if not isinstance(flag_defn[\'bits\'], list):\n            flag_bits = [flag_bits]\n\n        # The amount to shift flag_value to line up with mask_value\n        flag_shift = min(flag_bits)\n\n        # Mask our mask_value, we are only interested in the bits for this flag\n        flag_mask = 0\n        for i in flag_bits:\n            flag_mask |= (1 << i)\n        masked_mask_value = mask_value & flag_mask\n\n        for flag_value, value in flag_defn[\'values\'].items():\n            shifted_value = int(flag_value) << flag_shift\n            if shifted_value == masked_mask_value:\n                assert flag_name not in return_dict\n                return_dict[flag_name] = value\n    return return_dict\n\n\ndef get_flags_def(variable):\n    flags = getattr(variable, FLAGS_ATTR_NAME, None)\n    if flags is not None:\n        return flags\n\n    data_vars = getattr(variable, \'data_vars\', None)\n\n    if data_vars is not None:\n        # Maybe we have a DataSet, not a DataArray\n        for var in data_vars.values():\n            flags = getattr(var, FLAGS_ATTR_NAME, None)\n            if flags is not None:\n                return flags\n\n    raise ValueError(\'No masking variable found\')\n\n\ndef set_value_at_index(bitmask, index, value):\n    """"""\n    Set a bit value onto an integer bitmask\n\n    eg. set bits 2 and 4 to True\n    >>> mask = 0\n    >>> mask = set_value_at_index(mask, 2, True)\n    >>> mask = set_value_at_index(mask, 4, True)\n    >>> print(bin(mask))\n    0b10100\n    >>> mask = set_value_at_index(mask, 2, False)\n    >>> print(bin(mask))\n    0b10000\n\n    :param bitmask: existing int bitmask to alter\n    :type bitmask: int\n    :type index: int\n    :type value: bool\n    """"""\n    bit_val = 2 ** index\n    if value:\n        bitmask |= bit_val\n    else:\n        bitmask &= (~bit_val)\n    return bitmask\n\n\ndef generate_table(rows):\n    """"""\n    Yield strings to print a table using the data in `rows`.\n\n    TODO: Maybe replace with Pandas\n\n    :param rows: A sequence of sequences with the 0th element being the table\n                 header\n    """"""\n\n    # - figure out column widths\n    widths = [len(max(columns, key=len)) for columns in zip(*rows)]\n\n    # - print the header\n    header, data = rows[0], rows[1:]\n    yield (\n        \' | \'.join(format(title, ""%ds"" % width) for width, title in zip(widths, header))\n    )\n\n    # Print the separator\n    first_col = \'\'\n    # - print the data\n    for row in data:\n        if first_col == \'\' and row[0] != \'\':\n            # - print the separator\n            yield \'-+-\'.join(\'-\' * width for width in widths)\n        first_col = row[0]\n\n        yield (\n            "" | "".join(format(cdata, ""%ds"" % width) for width, cdata in zip(widths, row))\n        )\n'"
datacube/utils/math.py,0,"b'from typing import Tuple, Optional, Any, cast\nfrom math import ceil, fmod\n\nimport numpy\nimport xarray as xr\nfrom affine import Affine\n\n\ndef unsqueeze_data_array(da: xr.DataArray,\n                         dim: str,\n                         pos: int,\n                         coord: Any = 0,\n                         attrs: Optional[dict] = None) -> xr.DataArray:\n    """"""\n    Add a 1-length dimension to a data array.\n\n    :param da: array to add a 1-length dimension\n    :param dim: name of new dimension\n    :param pos: position of dim\n    :param coord: label of the coordinate on the unsqueezed dimension\n    :param attrs: attributes for the coordinate dimension\n    :return: A new xarray with a dimension added\n    """"""\n    new_dims = list(da.dims)\n    new_dims.insert(pos, dim)\n    new_shape = da.data.shape[:pos] + (1,) + da.data.shape[pos:]\n    new_data = da.data.reshape(new_shape)\n    new_coords = {k: v for k, v in da.coords.items()}\n    new_coords[dim] = xr.DataArray([coord], dims=[dim], attrs=attrs)\n    return xr.DataArray(new_data, dims=new_dims, coords=new_coords, attrs=da.attrs)\n\n\ndef unsqueeze_dataset(ds: xr.Dataset, dim: str, coord: int = 0, pos: int = 0) -> xr.Dataset:\n    ds = ds.apply(unsqueeze_data_array, dim=dim, pos=pos, keep_attrs=True, coord=coord)\n    return ds\n\n\ndef spatial_dims(xx: xr.DataArray, relaxed: bool = False) -> Optional[Tuple[str, str]]:\n    """""" Find spatial dimensions of `xx`.\n\n        Checks for presence of dimensions named:\n          y, x | latitude, longitude | lat, lon\n\n        Returns\n        =======\n        None -- if no dimensions with expected names are found\n        (\'y\', \'x\') | (\'latitude\', \'longitude\') | (\'lat\', \'lon\')\n\n        If *relaxed* is True and none of the above dimension names are found,\n        assume that last two dimensions are spatial dimensions.\n    """"""\n    guesses = [(\'y\', \'x\'),\n               (\'latitude\', \'longitude\'),\n               (\'lat\', \'lon\')]\n\n    dims = set(xx.dims)\n    for guess in guesses:\n        if dims.issuperset(guess):\n            return guess\n\n    if relaxed and len(xx.dims) >= 2:\n        return cast(Tuple[str, str], xx.dims[-2:])\n\n    return None\n\n\ndef clamp(x, l, u):\n    """"""\n    clamp x to be l <= x <= u\n\n    >>> clamp(5, 1, 10)\n    5\n    >>> clamp(-1, 1, 10)\n    1\n    >>> clamp(12, 1, 10)\n    10\n    """"""\n    assert l <= u\n    return l if x < l else u if x > u else x\n\n\ndef is_almost_int(x: float, tol: float):\n    """"""\n    Check if number is close enough to an integer\n    """"""\n    x = abs(fmod(x, 1))\n    if x > 0.5:\n        x = 1 - x\n    return x < tol\n\n\ndef dtype_is_float(dtype) -> bool:\n    """"""\n    Check if `dtype` is floating-point.\n    """"""\n    return numpy.dtype(dtype).kind == \'f\'\n\n\ndef valid_mask(xx, nodata):\n    """"""\n    Compute mask such that xx[mask] contains only valid pixels.\n    """"""\n    if dtype_is_float(xx.dtype):\n        if nodata is None or numpy.isnan(nodata):\n            return ~numpy.isnan(xx)\n        return ~numpy.isnan(xx) & (xx != nodata)\n\n    if nodata is None:\n        return numpy.full_like(xx, True, dtype=numpy.bool)\n    return xx != nodata\n\n\ndef invalid_mask(xx, nodata):\n    """"""\n    Compute mask such that xx[mask] contains only invalid pixels.\n    """"""\n    if dtype_is_float(xx.dtype):\n        if nodata is None or numpy.isnan(nodata):\n            return numpy.isnan(xx)\n        return numpy.isnan(xx) | (xx == nodata)\n\n    if nodata is None:\n        return numpy.full_like(xx, False, dtype=numpy.bool)\n    return xx == nodata\n\n\ndef num2numpy(x, dtype, ignore_range=None):\n    """"""\n    Cast python numeric value to numpy.\n\n    :param x int|float: Numerical value to convert to numpy.type\n    :param dtype str|numpy.dtype|numpy.type: Destination dtype\n    :param ignore_range: If set to True skip range check and cast anyway (for example: -1 -> 255)\n\n    :returns: None if x is None\n    :returns: None if x is outside the valid range of dtype and ignore_range is not set\n    :returns: dtype.type(x) if x is within range or ignore_range=True\n    """"""\n    if x is None:\n        return None\n\n    if isinstance(dtype, (str, type)):\n        dtype = numpy.dtype(dtype)\n\n    if ignore_range or dtype.kind == \'f\':\n        return dtype.type(x)\n\n    info = numpy.iinfo(dtype)\n    if info.min <= x <= info.max:\n        return dtype.type(x)\n\n    return None\n\n\ndef data_resolution_and_offset(data, fallback_resolution=None):\n    """""" Compute resolution and offset from x/y axis data.\n\n        Only uses first two coordinate values, assumes that data is regularly\n        sampled.\n\n        Returns\n        =======\n        (resolution: float, offset: float)\n    """"""\n    if data.size < 2:\n        if data.size < 1:\n            raise ValueError(""Can\'t calculate resolution for empty data"")\n        if fallback_resolution is None:\n            raise ValueError(""Can\'t calculate resolution with data size < 2"")\n        res = fallback_resolution\n    else:\n        res = (data[data.size - 1] - data[0]) / (data.size - 1.0)\n        res = res.item()\n\n    off = data[0] - 0.5 * res\n    return res, off.item()\n\n\ndef affine_from_axis(xx, yy, fallback_resolution=None):\n    """""" Compute Affine transform from pixel to real space given X,Y coordinates.\n\n        :param xx: X axis coordinates\n        :param yy: Y axis coordinates\n        :param fallback_resolution: None|float|(resx:float, resy:float) resolution to\n                                    assume for single element axis.\n\n        (0, 0) in pixel space is defined as top left corner of the top left pixel\n            \\\n            `` 0   1\n             +---+---+\n           0 |   |   |\n             +---+---+\n           1 |   |   |\n             +---+---+\n\n        Only uses first two coordinate values, assumes that data is regularly\n        sampled.\n\n        raises ValueError when any axis is empty\n        raises ValueError when any axis has single value and fallback resolution was not supplied.\n    """"""\n    if fallback_resolution is not None:\n        if isinstance(fallback_resolution, (float, int)):\n            frx, fry = fallback_resolution, fallback_resolution\n        else:\n            frx, fry = fallback_resolution\n    else:\n        frx, fry = None, None\n\n    xres, xoff = data_resolution_and_offset(xx, frx)\n    yres, yoff = data_resolution_and_offset(yy, fry)\n\n    return Affine.translation(xoff, yoff) * Affine.scale(xres, yres)\n\n\ndef iter_slices(shape, chunk_size):\n    """"""\n    Generate slices for a given shape.\n\n    E.g. ``shape=(4000, 4000), chunk_size=(500, 500)``\n    Would yield 64 tuples of slices, each indexing 500x500.\n\n    If the shape is not divisible by the chunk_size, the last chunk in each dimension will be smaller.\n\n    :param tuple(int) shape: Shape of an array\n    :param tuple(int) chunk_size: length of each slice for each dimension\n    :return: Yields slices that can be used on an array of the given shape\n\n    >>> list(iter_slices((5,), (2,)))\n    [(slice(0, 2, None),), (slice(2, 4, None),), (slice(4, 5, None),)]\n    """"""\n    assert len(shape) == len(chunk_size)\n    num_grid_chunks = [int(ceil(s / float(c))) for s, c in zip(shape, chunk_size)]\n    for grid_index in numpy.ndindex(*num_grid_chunks):\n        yield tuple(\n            slice(min(d * c, stop), min((d + 1) * c, stop)) for d, c, stop in zip(grid_index, chunk_size, shape))\n'"
datacube/utils/py.py,0,"b'import importlib\nimport logging\nfrom contextlib import contextmanager\n\nimport toolz\n\n_LOG = logging.getLogger(__name__)\n\n\ndef import_function(func_ref):\n    """"""\n    Import a function available in the python path.\n\n    Expects at least one \'.\' in the `func_ref`,\n    eg:\n        `module.function_name`\n        `package.module.function_name`\n\n    :param func_ref:\n    :return: function\n    """"""\n    module_name, _, func_name = func_ref.rpartition(\'.\')\n    module = importlib.import_module(module_name)\n    return getattr(module, func_name)\n\n\n@contextmanager\ndef ignore_exceptions_if(ignore_errors, errors=None):\n    """"""Ignore Exceptions raised within this block if ignore_errors is True""""""\n    if errors is None:\n        errors = (Exception,)\n\n    if ignore_errors:\n        try:\n            yield\n        except errors as e:\n            _LOG.warning(\'Ignoring Exception: %s\', e)\n    else:\n        yield\n\n\nclass cached_property(object):  # pylint: disable=invalid-name\n    """"""\n    A property that is only computed once per instance and then replaces\n    itself with an ordinary attribute. Deleting the attribute resets the\n    property.\n\n    Source: https://github.com/bottlepy/bottle/commit/fa7733e075da0d790d809aa3d2f53071897e6f76\n    """"""\n\n    def __init__(self, func):\n        self.__doc__ = getattr(func, \'__doc__\')\n        self.func = func\n\n    def __get__(self, obj, cls):\n        if obj is None:\n            return self\n        value = obj.__dict__[self.func.__name__] = self.func(obj)\n        return value\n\n\ndef sorted_items(d, key=None, reverse=False):\n    """"""Given a dictionary `d` return items: (k1, v1), (k2, v2)... sorted in\n    ascending order according to key.\n\n    :param dict d: dictionary\n    :param key: optional function remapping key\n    :param bool reverse: If True return in descending order instead of default ascending\n\n    """"""\n    key = toolz.first if key is None else toolz.comp(key, toolz.first)\n    return sorted(d.items(), key=key, reverse=reverse)\n'"
datacube/utils/serialise.py,0,"b'# coding=utf-8\n""""""\nSerialise function used in YAML output\n""""""\n\nimport math\nfrom collections import OrderedDict\nfrom datetime import datetime, date\nfrom decimal import Decimal\nfrom uuid import UUID\n\nimport numpy\nimport yaml\n\nfrom datacube.utils.documents import transform_object_tree\nfrom datacube.model._base import Range\n\n\nclass SafeDatacubeDumper(yaml.SafeDumper):  # pylint: disable=too-many-ancestors\n    pass\n\n\ndef _dict_representer(dumper, data):\n    return dumper.represent_mapping(yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG, data.items())\n\n\ndef _reduced_accuracy_decimal_representer(dumper: yaml.Dumper, data: Decimal) -> yaml.Node:\n    return dumper.represent_float(float(data))\n\n\ndef _range_representer(dumper: yaml.Dumper, data: Range) -> yaml.Node:\n    begin, end = data\n\n    # pyyaml doesn\'t output timestamps in flow style as timestamps(?)\n    if isinstance(begin, datetime):\n        begin = begin.isoformat()\n    if isinstance(end, datetime):\n        end = end.isoformat()\n\n    return dumper.represent_mapping(\n        yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG,\n        ((\'begin\', begin), (\'end\', end)),\n        flow_style=True\n    )\n\n\nSafeDatacubeDumper.add_representer(OrderedDict, _dict_representer)\nSafeDatacubeDumper.add_representer(Decimal, _reduced_accuracy_decimal_representer)\nSafeDatacubeDumper.add_representer(Range, _range_representer)\n\n\ndef jsonify_document(doc):\n    """"""\n    Make a document ready for serialisation as JSON.\n\n    Returns the new document, leaving the original unmodified.\n\n    """"""\n\n    def fixup_value(v):\n        if isinstance(v, float):\n            if math.isfinite(v):\n                return v\n            if math.isnan(v):\n                return ""NaN""\n            return ""-Infinity"" if v < 0 else ""Infinity""\n        if isinstance(v, (datetime, date)):\n            return v.isoformat()\n        if isinstance(v, numpy.dtype):\n            return v.name\n        if isinstance(v, UUID):\n            return str(v)\n        if isinstance(v, Decimal):\n            return str(v)\n        return v\n\n    return transform_object_tree(fixup_value, doc, key_transform=str)\n'"
datacube/utils/uris.py,0,"b'import os\n\nimport pathlib\nimport re\nfrom typing import Optional, List, Union\nimport urllib.parse\nfrom urllib.parse import urlparse, parse_qsl, urljoin\nfrom urllib.request import url2pathname\nfrom pathlib import Path\n\nURL_RE = re.compile(r\'\\A\\s*[\\w\\d\\+]+://\')\n\n\ndef is_url(url_str: str) -> bool:\n    """"""\n    Check if url_str tastes like a url (starts with blah://)\n    """"""\n    try:\n        return URL_RE.match(url_str) is not None\n    except TypeError:\n        return False\n\n\ndef is_vsipath(path: str) -> bool:\n    """""" Check if string is a GDAL ""/vsi.*"" path\n    """"""\n    path = path.lower()\n    return path.startswith(""/vsi"")\n\n\ndef vsi_join(base: str, path: str) -> str:\n    """""" Extend GDAL\'s vsi path\n\n        Basically just base/path, but taking care of trailing `/` in base\n    """"""\n    return base.rstrip(\'/\') + \'/\' + path\n\n\ndef uri_to_local_path(local_uri: Optional[str]) -> Optional[pathlib.Path]:\n    """"""\n    Transform a URI to a platform dependent Path.\n\n    For example on Unix:\n    \'file:///tmp/something.txt\' -> \'/tmp/something.txt\'\n\n    On Windows:\n    \'file:///C:/tmp/something.txt\' -> \'C:\\\\tmp\\\\test.tmp\'\n\n    .. note:\n        Only supports file:// schema URIs\n    """"""\n    if not local_uri:\n        return None\n\n    components = urlparse(local_uri)\n    if components.scheme != \'file\':\n        raise ValueError(\'Only file URIs currently supported. Tried %r.\' % components.scheme)\n\n    path = url2pathname(components.path)\n\n    if components.netloc:\n        if os.name == \'nt\':\n            path = \'//{}{}\'.format(components.netloc, path)\n        else:\n            raise ValueError(\'Only know how to use `netloc` urls on Windows\')\n\n    return pathlib.Path(path)\n\n\ndef mk_part_uri(uri: str, idx: int) -> str:\n    """""" Appends fragment part to the uri recording index of the part\n    """"""\n    return \'{}#part={:d}\'.format(uri, idx)\n\n\ndef get_part_from_uri(uri: str) -> Optional[int]:\n    """"""\n    Reverse of mk_part_uri\n\n    returns None|int|string\n    """"""\n\n    def maybe_int(v):\n        if v is None:\n            return None\n        try:\n            return int(v)\n        except ValueError:\n            return v\n\n    opts = dict(parse_qsl(urlparse(uri).fragment))\n    return maybe_int(opts.get(\'part\', None))\n\n\ndef as_url(maybe_uri: str) -> str:\n    if is_url(maybe_uri):\n        return maybe_uri\n    else:\n        return pathlib.Path(maybe_uri).absolute().as_uri()\n\n\ndef default_base_dir() -> pathlib.Path:\n    """"""Return absolute path to current directory. If PWD environment variable is\n       set correctly return that, note that PWD might be set to ""symlinked""\n       path instead of ""real"" path.\n\n       Only return PWD instead of cwd when:\n\n       1. PWD exists (i.e. launched from interactive shell)\n       2. Contains Absolute path (sanity check)\n       3. Absolute ath in PWD resolves to the same directory as cwd (process didn\'t call chdir after starting)\n    """"""\n    cwd = pathlib.Path(\'.\').resolve()\n\n    _pwd = os.environ.get(\'PWD\')\n    if _pwd is None:\n        return cwd\n\n    pwd = pathlib.Path(_pwd)\n    if not pwd.is_absolute():\n        return cwd\n\n    try:\n        pwd_resolved = pwd.resolve()\n    except IOError:\n        return cwd\n\n    if cwd != pwd_resolved:\n        return cwd\n\n    return pwd\n\n\ndef normalise_path(p: Union[str, pathlib.Path],\n                   base: Optional[Union[str, pathlib.Path]] = None) -> pathlib.Path:\n    """"""Turn path into absolute path resolving any `../` and `.`\n\n       If path is relative pre-pend `base` path to it, `base` if set should be\n       an absolute path. If not set, current working directory (as seen by the\n       user launching the process, including any possible symlinks) will be\n       used.\n    """"""\n    assert isinstance(p, (str, pathlib.Path))\n    assert isinstance(base, (str, pathlib.Path, type(None)))\n\n    def norm(p):\n        return pathlib.Path(os.path.normpath(str(p)))\n\n    if isinstance(p, str):\n        p = pathlib.Path(p)\n\n    if isinstance(base, str):\n        base = pathlib.Path(base)\n\n    if p.is_absolute():\n        return norm(p)\n\n    if base is None:\n        base = default_base_dir()\n    elif not base.is_absolute():\n        raise ValueError(""Expect base to be an absolute path"")\n\n    return norm(base / p)\n\n\ndef uri_resolve(base: str, path: Optional[str]) -> str:\n    """"""\n    path                  -- if path is a uri or /vsi.* style path\n    Path(path).as_uri()   -- if path is absolute filename\n    base/path             -- in all other cases\n    """"""\n    if not path:\n        return base\n\n    if is_vsipath(path) or is_url(path):\n        return path\n\n    p = Path(path)\n    if p.is_absolute():\n        return p.as_uri()\n\n    if is_vsipath(base):\n        return vsi_join(base, path)\n    else:\n        return urljoin(base, path)\n\n\ndef pick_uri(uris: List[str], scheme: Optional[str] = None) -> str:\n    """""" If scheme is supplied:\n          Return first uri matching the scheme or raises Exception\n        If scheme is not supplied:\n          Return first `file:` uri, or failing that the very first uri\n    """"""\n\n    def pick(uris: List[str], scheme: str) -> Optional[str]:\n        for uri in uris:\n            if uri.startswith(scheme):\n                return uri\n        return None\n\n    if len(uris) < 1:\n        raise ValueError(\'No uris on a dataset\')\n\n    base_uri = pick(uris, scheme or \'file:\')\n\n    if base_uri is not None:\n        return base_uri\n\n    if scheme is not None:\n        raise ValueError(\'No uri with required scheme was found\')\n\n    return uris[0]\n\n\ndef register_scheme(*schemes):\n    """"""\n    Register additional uri schemes as supporting relative offsets (etc), so that band/measurement paths can be\n    calculated relative to the base uri.\n    """"""\n    urllib.parse.uses_netloc.extend(schemes)\n    urllib.parse.uses_relative.extend(schemes)\n    urllib.parse.uses_params.extend(schemes)\n\n\n# `urljoin`, that we use for relative path computation, needs to know which url\n# schemes support relative offsets. By default only well known types are\n# understood. So here we register more common blob store url protocols.\nregister_scheme(\n    \'s3\',         # `s3://...`      -- AWS S3 Object Store\n    \'gs\',         # `gs://...`      -- Google Cloud Storage\n    \'wasb\',       # `wasb[s]://...` -- Windows Azure Storage Blob\n    \'wasbs\'\n)\n'"
datacube/utils/xarray_geoextensions.py,0,"b'""""""\nAdd geometric extensions to :class:`xarray.Dataset` and :class:`xarray.DataArray` for use\nwith Data Cube by Monkey Patching those classes.\n\nThis extension is reliant on an `xarray` object having a `.crs` property of type\n:class:`datacube.utils.geometry.CRS`. This is used to inspect the spatial dimensions of the\n:class:`Dataset` or :class:`DataArray`, and provide new attributes for accessing a\n:class:`datacube.utils.geometry.GeoBox`, affine transform and extent for the dataset as\n`.geobox`, `.affine` and `.extent` respectively.\n\n""""""\n\nimport xarray\n\nfrom datacube.utils import geometry, spatial_dims\nfrom datacube.utils.math import affine_from_axis\n\n\ndef _norm_crs(crs):\n    if crs is None or isinstance(crs, geometry.CRS):\n        return crs\n    elif isinstance(crs, str):\n        return geometry.CRS(crs)\n    else:\n        raise ValueError(\'Can not interpret {} as CRS\'.format(type(crs)))\n\n\ndef _get_crs_from_attrs(obj):\n    """""" Looks for attribute named `crs` containing CRS string\n        1. Checks spatials coords attrs\n        2. Checks data variable attrs\n        3. Checks dataset attrs\n\n        Returns\n        =======\n        Content for `.attrs[crs]` usually it\'s a string\n        None if not present in any of the places listed above\n    """"""\n    if isinstance(obj, xarray.Dataset):\n        if len(obj.data_vars) > 0:\n            data_array = next(iter(obj.data_vars.values()))\n        else:\n            # fall back option\n            return obj.attrs.get(\'crs\', None)\n    else:\n        data_array = obj\n\n    sdims = spatial_dims(data_array, relaxed=True)\n    if sdims is not None:\n        crs_set = set(data_array[d].attrs.get(\'crs\', None) for d in sdims)\n        crs = None\n        if len(crs_set) > 1:\n            raise ValueError(\'Spatial dimensions have different crs.\')\n        elif len(crs_set) == 1:\n            crs = crs_set.pop()\n    else:\n        crs = None\n\n    if crs is None:\n        # fall back option\n        crs = data_array.attrs.get(\'crs\', None) or obj.attrs.get(\'crs\', None)\n    return crs\n\n\ndef _get_crs_from_coord(obj, mode=\'strict\'):\n    """""" Looks for dimensionless coordinate with `spatial_ref` attribute.\n\n        obj: Dataset | DataArray\n        mode: strict|any|all\n           strict -- raise Error if multiple candidates\n           any    -- return first one\n           all    -- return a list of all found CRSs\n\n       Returns\n       =======\n       None     - if none found\n       crs:str  - if found one\n       crs:str  - if found several but mode is any\n\n       (crs: str, crs: str) - if found several and mode=all\n    """"""\n    grid_mapping = obj.attrs.get(\'grid_mapping\', None)\n\n    # First check CF convention ""pointer""\n    if grid_mapping is not None and grid_mapping in obj.coords:\n        coord = obj.coords[grid_mapping]\n        spatial_ref = coord.attrs.get(\'spatial_ref\', None)\n        if spatial_ref is not None:\n            return spatial_ref\n        else:\n            raise ValueError(f""Coordinate \'{grid_mapping}\' has no `spatial_ref` attribute"")\n\n    # No explicit `grid_mapping` find some ""CRS"" coordinate\n    candidates = tuple(coord.attrs[\'spatial_ref\'] for coord in obj.coords.values()\n                       if coord.ndim == 0 and \'spatial_ref\' in coord.attrs)\n\n    if len(candidates) == 0:\n        return None\n    if len(candidates) == 1:\n        return candidates[0]\n\n    if mode == \'strict\':\n        raise ValueError(""Too many candidates when looking for CRS"")\n    elif mode == \'all\':\n        return candidates\n    elif mode == \'any\':\n        return candidates[0]\n    else:\n        raise ValueError(f""Mode needs to be: strict|any|all got {mode}"")\n\n\ndef _xarray_affine(obj):\n    sdims = spatial_dims(obj, relaxed=True)\n    if sdims is None:\n        return None\n\n    yy, xx = (obj[dim] for dim in sdims)\n    fallback_res = (coord.attrs.get(\'resolution\', None) for coord in (xx, yy))\n\n    return affine_from_axis(xx.values, yy.values, fallback_res)\n\n\ndef _xarray_extent(obj):\n    geobox = obj.geobox\n    return None if geobox is None else geobox.extent\n\n\ndef _xarray_geobox(obj):\n    crs = None\n    try:\n        crs = _get_crs_from_coord(obj)\n    except ValueError:\n        pass\n\n    if crs is None:\n        try:\n            crs = _get_crs_from_attrs(obj)\n        except ValueError:\n            pass\n\n    if crs is None:\n        return None\n\n    try:\n        crs = _norm_crs(crs)\n    except ValueError:\n        return None\n\n    dims = crs.dimensions\n    return geometry.GeoBox(obj[dims[1]].size, obj[dims[0]].size, obj.affine, crs)\n\n\nxarray.Dataset.geobox = property(_xarray_geobox)   # type: ignore\nxarray.Dataset.affine = property(_xarray_affine)   # type: ignore\nxarray.Dataset.extent = property(_xarray_extent)   # type: ignore\nxarray.DataArray.geobox = property(_xarray_geobox) # type: ignore\nxarray.DataArray.affine = property(_xarray_affine) # type: ignore\nxarray.DataArray.extent = property(_xarray_extent) # type: ignore\n'"
datacube/virtual/__init__.py,0,"b'from typing import Mapping, Any\nimport copy\n\nfrom .impl import VirtualProduct, Transformation, VirtualProductException\nfrom .impl import from_validated_recipe, virtual_product_kind\nfrom .transformations import MakeMask, ApplyMask, ToFloat, Rename, Select, Expressions\nfrom .transformations import XarrayReduction, year, month, week, day, earliest_time\nfrom .catalog import Catalog\nfrom .utils import reject_keys\n\nfrom datacube.model import Measurement\nfrom datacube.utils import import_function\nfrom datacube.utils.documents import parse_yaml\n\n__all__ = [\'construct\', \'Transformation\', \'Measurement\']\n\n\nclass NameResolver:\n    """""" Apply a mapping from name to callable objects in a recipe. """"""\n\n    def __init__(self, lookup_table):\n        self.lookup_table = lookup_table\n\n    def clone(self):\n        """""" Safely copy the resolver in order to possibly extend it. """"""\n        return NameResolver(copy.deepcopy(self.lookup_table))\n\n    @staticmethod\n    def _assert(cond, msg):\n        if not cond:\n            raise VirtualProductException(msg)\n\n    def construct(self, **recipe) -> VirtualProduct:\n        """""" Validate recipe and construct virtual product. """"""\n\n        get = recipe.get\n\n        def lookup(name, namespace=None, kind=\'transformation\'):\n            if callable(name):\n                return name\n\n            if namespace is not None and namespace in self.lookup_table and name in self.lookup_table[namespace]:\n                result = self.lookup_table[namespace][name]\n            else:\n                try:\n                    result = import_function(name)\n                except (ImportError, AttributeError):\n                    msg = ""could not resolve {} {} in {}"".format(kind, name, recipe)\n                    raise VirtualProductException(msg)\n\n            self._assert(callable(result), ""{} not callable in {}"".format(kind, recipe))\n\n            return result\n\n        kind = virtual_product_kind(recipe)\n\n        if kind == \'product\':\n            func_keys = [\'fuse_func\', \'dataset_predicate\']\n            return from_validated_recipe({key: value if key not in func_keys else lookup(value, kind=\'function\')\n                                          for key, value in recipe.items()})\n\n        if kind == \'transform\':\n            cls_name = recipe[\'transform\']\n            input_product = get(\'input\')\n\n            self._assert(input_product is not None, ""no input for transformation in {}"".format(recipe))\n\n            return from_validated_recipe(dict(transform=lookup(cls_name, \'transform\'),\n                                              input=self.construct(**input_product),\n                                              **reject_keys(recipe, [\'transform\', \'input\'])))\n\n        if kind == \'collate\':\n            self._assert(len(recipe[\'collate\']) > 0, ""no children for collate in {}"".format(recipe))\n\n            return from_validated_recipe(dict(collate=[self.construct(**child) for child in recipe[\'collate\']],\n                                              **reject_keys(recipe, [\'collate\'])))\n\n        if kind == \'juxtapose\':\n            self._assert(len(recipe[\'juxtapose\']) > 0, ""no children for juxtapose in {}"".format(recipe))\n\n            return from_validated_recipe(dict(juxtapose=[self.construct(**child) for child in recipe[\'juxtapose\']],\n                                              **reject_keys(recipe, [\'juxtapose\'])))\n\n        if kind == \'aggregate\':\n            cls_name = recipe[\'aggregate\']\n            input_product = get(\'input\')\n            group_by = get(\'group_by\')\n\n            self._assert(input_product is not None, ""no input for aggregate in {}"".format(recipe))\n            self._assert(group_by is not None, ""no group_by for aggregate in {}"".format(recipe))\n\n            return from_validated_recipe(dict(aggregate=lookup(cls_name, \'aggregate\'),\n                                              group_by=lookup(group_by, \'aggregate/group_by\', kind=\'group_by\'),\n                                              input=self.construct(**input_product),\n                                              **reject_keys(recipe, [\'aggregate\', \'input\', \'group_by\'])))\n\n        if kind == \'reproject\':\n            input_product = get(\'input\')\n            output_crs = recipe[\'reproject\'].get(\'output_crs\')\n            resolution = recipe[\'reproject\'].get(\'resolution\')\n            align = recipe[\'reproject\'].get(\'align\')\n\n            self._assert(input_product is not None, ""no input for reproject in {}"".format(recipe))\n            self._assert(output_crs is not None, ""no output_crs for reproject in {}"".format(recipe))\n            self._assert(resolution is not None, ""no resolution for reproject in {}"".format(recipe))\n\n            return from_validated_recipe(dict(reproject=recipe[\'reproject\'],\n                                              input=self.construct(**input_product),\n                                              **reject_keys(recipe, [\'reproject\', \'input\'])))\n\n        raise VirtualProductException(""could not understand virtual product recipe: {}"".format(recipe))\n\n    def register(self, namespace: str, name: str, callable_obj):\n        """""" Register a callable to the name resolver. """"""\n        if namespace not in self.lookup_table:\n            self.lookup_table[namespace] = {}\n\n        if name in self.lookup_table[namespace]:\n            raise VirtualProductException(""name {} under {} is already registered"".format(name, namespace))\n\n        self.lookup_table[namespace][name] = callable_obj\n\n\nDEFAULT_RESOLVER = NameResolver({\'transform\': dict(make_mask=MakeMask,\n                                                   apply_mask=ApplyMask,\n                                                   to_float=ToFloat,\n                                                   rename=Rename,\n                                                   select=Select,\n                                                   expressions=Expressions),\n                                 \'aggregate\': dict(xarray_reduction=XarrayReduction),\n                                 \'aggregate/group_by\': dict(year=year,\n                                                            month=month,\n                                                            week=week,\n                                                            day=day,\n                                                            earliest_time=earliest_time)})\n\n\ndef construct(name_resolver=None, **recipe: Mapping[str, Any]) -> VirtualProduct:\n    """"""\n    Create a virtual product from a specification dictionary.\n    """"""\n    if name_resolver is None:\n        name_resolver = DEFAULT_RESOLVER\n\n    return name_resolver.construct(**recipe)\n\n\ndef construct_from_yaml(recipe: str, name_resolver=None) -> VirtualProduct:\n    """"""\n    Create a virtual product from a yaml recipe.\n    """"""\n    if name_resolver is None:\n        name_resolver = DEFAULT_RESOLVER\n\n    return construct(name_resolver=name_resolver, **parse_yaml(recipe))\n\n\ndef catalog_from_yaml(catalog_body: str, name_resolver=None) -> Catalog:\n    """"""\n    Load a catalog of virtual products from a yaml document.\n    """"""\n    if name_resolver is None:\n        name_resolver = DEFAULT_RESOLVER\n\n    return Catalog(name_resolver, parse_yaml(catalog_body))\n\n\ndef catalog_from_file(filename: str, name_resolver=None) -> Catalog:\n    """"""\n    Load a catalog of virtual products from a yaml file.\n    """"""\n    with open(filename) as fl:\n        return catalog_from_yaml(fl.read(), name_resolver=name_resolver)\n'"
datacube/virtual/catalog.py,0,"b'""""""\nCatalog of virtual products.\n""""""\n\nfrom collections.abc import Mapping\nfrom itertools import chain\n\nimport yaml\n\nfrom datacube.model.utils import SafeDumper\n\n\nclass UnappliedTransform:\n    def __init__(self, name_resolver, recipe):\n        self.name_resolver = name_resolver\n        self.recipe = recipe\n\n    def __call__(self, input):\n        return self.name_resolver.construct(**self.recipe, input=input)\n\n    def __repr__(self):\n        return yaml.dump(self.recipe, Dumper=SafeDumper,\n                         default_flow_style=False, indent=2)\n\n\nclass Catalog(Mapping):\n    """"""\n    A catalog of virtual products specified in a yaml document.\n    """"""\n\n    def __init__(self, name_resolver, contents):\n        self.name_resolver = name_resolver\n        self.contents = contents\n        common = set(self._names(\'products\')) & set(self._names(\'transforms\'))\n        assert not common, f""common names found in products and transforms {common}""\n\n    def _names(self, section):\n        """""" List of names under a section (products or transforms). """"""\n        if section not in self.contents:\n            return []\n        return list(self.contents[section])\n\n    def __getitem__(self, name):\n        """"""\n        Looks up a virtual product or transform by name.\n        Returns `None` if not found.\n        """"""\n        if name in self._names(\'products\'):\n            return self.name_resolver.construct(**self.contents[\'products\'][name][\'recipe\'])\n        if name in self._names(\'transforms\'):\n            return UnappliedTransform(self.name_resolver, self.contents[\'transforms\'][name][\'recipe\'])\n\n        # raising a `KeyError` here stops autocompletion from working\n        return None\n\n    def __getattr__(self, name):\n        return self[name]\n\n    def __len__(self):\n        return len(self._names(\'products\')) + len(self._names(\'transforms\'))\n\n    def __iter__(self):\n        return chain(iter(self._names(\'products\')), iter(self._names(\'transforms\')))\n\n    def __dir__(self):\n        """"""\n        Override to provide autocompletion of products and transforms.\n        """"""\n        return sorted(dir(Mapping) + list(self.__dict__) + self._names(\'products\') + self._names(\'transforms\'))\n'"
datacube/virtual/impl.py,0,"b'""""""\nImplementation of virtual products. Provides an interface for the products in the datacube\nto query and to load data, and combinators to combine multiple products into ""virtual""\nproducts implementing the same interface.\n""""""\n\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Mapping, Sequence\nfrom functools import reduce\nfrom typing import Any, Dict, List, cast\n\nimport uuid\nimport numpy\nimport xarray\nimport dask.array\nfrom dask.core import flatten\nimport yaml\n\nfrom datacube import Datacube\nfrom datacube.api.core import select_datasets_inside_polygon, output_geobox\nfrom datacube.api.grid_workflow import _fast_slice\nfrom datacube.api.query import Query, query_group_by\nfrom datacube.model import Measurement, DatasetType\nfrom datacube.model.utils import xr_apply, xr_iter, SafeDumper\nfrom datacube.testutils.io import native_geobox\nfrom datacube.utils.geometry import GeoBox, rio_reproject, geobox_union_conservative\nfrom datacube.utils.geometry import compute_reproject_roi\nfrom datacube.utils.geometry.gbox import GeoboxTiles\nfrom datacube.utils.geometry._warp import resampling_s2rio\nfrom datacube.api.core import per_band_load_data_settings\n\nfrom .utils import qualified_name, merge_dicts\nfrom .utils import select_unique, select_keys, reject_keys, merge_search_terms\n\n\nclass VirtualProductException(Exception):\n    """""" Raised if the construction of the virtual product cannot be validated. """"""\n\n\nclass VirtualDatasetBag:\n    """""" Result of `VirtualProduct.query`. """"""\n    def __init__(self, bag, geopolygon, product_definitions):\n        self.bag = bag\n        self.geopolygon = geopolygon\n        self.product_definitions = product_definitions\n\n    def contained_datasets(self):\n        def worker(bag):\n            if isinstance(bag, Sequence):\n                for child in bag:\n                    yield child\n\n            elif isinstance(bag, Mapping):\n                # there should only be one key (collate or juxtapose)\n                for key in bag:\n                    # bag[key] should be a list\n                    for child in bag[key]:\n                        yield from worker(child)\n\n            else:\n                raise VirtualProductException(""unexpected bag"")\n\n        return worker(self.bag)\n\n    def explode(self):\n        def worker(bag):\n            if isinstance(bag, Sequence):\n                for child in bag:\n                    yield [child]\n\n            elif isinstance(bag, Mapping):\n                if \'juxtapose\' in bag:\n                    # too hard, giving up\n                    raise NotImplementedError\n\n                for index, child_bag in enumerate(bag[\'collate\']):\n                    for child in worker(child_bag):\n                        yield {\'collate\': [child if i == index else []\n                                           for i, _ in enumerate(bag[\'collate\'])]}\n\n            else:\n                raise VirtualProductException(""unexpected bag"")\n\n        for child in worker(self.bag):\n            yield VirtualDatasetBag(child, self.geopolygon, self.product_definitions)\n\n    def __repr__(self):\n        return ""<VirtualDatasetBag of {} datacube datasets>"".format(len(list(self.contained_datasets())))\n\n\nclass VirtualDatasetBox:\n    """""" Result of `VirtualProduct.group`. """"""\n\n    def __init__(self, box, geobox, load_natively, product_definitions, geopolygon=None):\n        if not load_natively and geobox is None:\n            raise VirtualProductException(""VirtualDatasetBox has no geobox"")\n        if not load_natively and geopolygon is not None:\n            raise VirtualProductException(""unexpected geopolygon for VirtualDatasetBox"")\n\n        self.box = box\n        self.geobox = geobox\n        self.load_natively = load_natively\n        self.product_definitions = product_definitions\n        self.geopolygon = geopolygon\n\n    def __repr__(self):\n        if not self.load_natively:\n            return ""<VirtualDatasetBox of shape {}>"".format(dict(zip(self.dims, self.shape)))\n\n        return ""<natively loaded VirtualDatasetBox>""\n\n    @property\n    def dims(self):\n        """"""\n        Names of the dimensions, e.g., ``(\'time\', \'y\', \'x\')``.\n        :return: tuple(str)\n        """"""\n        if self.load_natively:\n            raise VirtualProductException(""dims requires known geobox"")\n\n        return self.box.dims + self.geobox.dimensions\n\n    @property\n    def shape(self):\n        """"""\n        Lengths of each dimension, e.g., ``(285, 4000, 4000)``.\n        :return: tuple(int)\n        """"""\n        if self.load_natively:\n            raise VirtualProductException(""shape requires known geobox"")\n\n        return self.box.shape + self.geobox.shape\n\n    def __getitem__(self, chunk):\n        if self.load_natively:\n            raise VirtualProductException(""slicing requires known geobox"")\n\n        # TODO implement this properly\n        box = self.box\n\n        return VirtualDatasetBox(_fast_slice(box, chunk[:len(box.shape)]),\n                                 self.geobox[chunk[len(box.shape):]],\n                                 self.load_natively,\n                                 self.product_definitions,\n                                 geopolygon=self.geopolygon)\n\n    def map(self, func, dtype=\'O\'):\n        return VirtualDatasetBox(xr_apply(self.box, func, dtype=dtype),\n                                 self.geobox,\n                                 self.load_natively,\n                                 self.product_definitions,\n                                 geopolygon=self.geopolygon)\n\n    def filter(self, predicate):\n        mask = self.map(predicate, dtype=\'bool\')\n\n        # NOTE: this could possibly result in an empty box\n        return VirtualDatasetBox(self.box[mask.box], self.geobox, self.load_natively, self.product_definitions,\n                                 geopolygon=self.geopolygon)\n\n    def split(self, dim=\'time\'):\n        box = self.box\n\n        [length] = box[dim].shape\n        for i in range(length):\n            yield VirtualDatasetBox(box.isel(**{dim: slice(i, i + 1)}),\n                                    self.geobox,\n                                    self.load_natively,\n                                    self.product_definitions,\n                                    geopolygon=self.geopolygon)\n\n    def input_datasets(self):\n        def traverse(entry):\n            if isinstance(entry, Mapping):\n                if \'collate\' in entry:\n                    _, child = entry[\'collate\']\n                    yield from traverse(child)\n                elif \'juxtapose\' in entry:\n                    for child in entry[\'juxtapose\']:\n                        yield from traverse(child)\n                else:\n                    raise VirtualProductException(""malformed box"")\n\n            elif isinstance(entry, Sequence):\n                yield from entry\n\n            elif isinstance(entry, VirtualDatasetBox):\n                for _, _, child in xr_iter(entry.input_datasets()):\n                    yield from child\n\n            else:\n                raise VirtualProductException(""malformed box"")\n\n        def worker(index, entry):\n            return set(traverse(entry))\n\n        return self.map(worker).box\n\n\nclass Transformation(ABC):\n    """"""\n    A user-defined on-the-fly data transformation.\n\n    The data coming in and out of the `compute` method are `xarray.Dataset` objects.\n    The measurements are stored as `xarray.DataArray` objects inside it.\n\n    The `measurements` method transforms the dictionary mapping measurement names\n    to `datacube.model.Measurement` objects describing the input data\n    into a dictionary describing the measurements of the output data\n    produced by the `compute` method.\n    """"""\n\n    @abstractmethod\n    def measurements(self, input_measurements) -> Dict[str, Measurement]:\n        """"""\n        Returns the dictionary describing the output measurements from this transformation.\n        Assumes the `data` provided to `compute` will have measurements\n        given by the dictionary `input_measurements`.\n        """"""\n\n    @abstractmethod\n    def compute(self, data):\n        """"""\n        Perform computation on `data` that results in an `xarray.Dataset`\n        having measurements reported by the `measurements` method.\n        """"""\n\n\nclass VirtualProduct(Mapping):\n    """"""\n    A recipe for combining loaded data from multiple datacube products.\n\n    Basic combinators are:\n        - product: an existing datacube product\n        - transform: on-the-fly computation on data being loaded\n        - collate: stack observations from products with the same set of measurements\n        - juxtapose: put measurements from different products side-by-side\n        - aggregate: take (non-spatial) statistics of grouped data\n        - reproject: on-the-fly reprojection of raster data\n    """"""\n\n    _GEOBOX_KEYS = {\'output_crs\', \'resolution\', \'align\'}\n    _GROUPING_KEYS = {\'group_by\'}\n    _LOAD_KEYS = {\'measurements\', \'fuse_func\', \'resampling\', \'dask_chunks\', \'like\'}\n    _ADDITIONAL_KEYS = {\'dataset_predicate\'}\n\n    _NON_SPATIAL_KEYS = _GEOBOX_KEYS | _GROUPING_KEYS\n    _NON_QUERY_KEYS = _NON_SPATIAL_KEYS | _LOAD_KEYS | _ADDITIONAL_KEYS\n\n    # helper methods\n\n    def __getitem__(self, key):\n        return self._settings[key]\n\n    def __len__(self):\n        return len(self._settings)\n\n    def __iter__(self):\n        return iter(self._settings)\n\n    @staticmethod\n    def _assert(cond, msg):\n        if not cond:\n            raise VirtualProductException(msg)\n\n    # public interface\n\n    def __init__(self, settings: Dict[str, Any]) -> None:\n        """"""\n        :param settings: validated and reference-resolved recipe\n        """"""\n        self._settings = settings\n\n    def output_measurements(self, product_definitions: Dict[str, DatasetType]) -> Dict[str, Measurement]:\n        """"""\n        A dictionary mapping names to measurement metadata.\n        :param product_definitions: a dictionary mapping product names to products (`DatasetType` objects)\n        """"""\n        raise NotImplementedError\n\n    def query(self, dc: Datacube, **search_terms: Dict[str, Any]) -> VirtualDatasetBag:\n        """""" Collection of datasets that match the query. """"""\n        raise NotImplementedError\n\n    # no index access below this line\n\n    def group(self, datasets: VirtualDatasetBag, **group_settings: Dict[str, Any]) -> VirtualDatasetBox:\n        """"""\n        Datasets grouped by their timestamps.\n        :param datasets: the `VirtualDatasetBag` to fetch data from\n        """"""\n        raise NotImplementedError\n\n    def fetch(self, grouped: VirtualDatasetBox, **load_settings: Dict[str, Any]) -> xarray.Dataset:\n        """""" Convert grouped datasets to `xarray.Dataset`. """"""\n        raise NotImplementedError\n\n    def __repr__(self):\n        return yaml.dump(self._reconstruct(), Dumper=SafeDumper,\n                         default_flow_style=False, indent=2)\n\n    def load(self, dc: Datacube, **query: Dict[str, Any]) -> xarray.Dataset:\n        """""" Mimic `datacube.Datacube.load`. For illustrative purposes. May be removed in the future. """"""\n        datasets = self.query(dc, **query)\n        grouped = self.group(datasets, **query)\n        return self.fetch(grouped, **query)\n\n\nclass Product(VirtualProduct):\n    """""" An existing datacube product. """"""\n\n    @property\n    def _product(self):\n        """""" The name of an existing datacube product. """"""\n        return self[\'product\']\n\n    def _reconstruct(self):\n        return {key: value if key not in [\'fuse_func\', \'dataset_predicate\'] else qualified_name(value)\n                for key, value in self.items()}\n\n    def output_measurements(self, product_definitions: Dict[str, DatasetType],\n                            measurements: List[str] = None) -> Dict[str, Measurement]:\n        self._assert(self._product in product_definitions,\n                     ""product {} not found in definitions"".format(self._product))\n\n        if measurements is None:\n            measurements = self.get(\'measurements\')\n\n        product = product_definitions[self._product]\n        return product.lookup_measurements(measurements)\n\n    def query(self, dc: Datacube, **search_terms: Dict[str, Any]) -> VirtualDatasetBag:\n        product = dc.index.products.get_by_name(self._product)\n        if product is None:\n            raise VirtualProductException(""could not find product {}"".format(self._product))\n\n        originals = reject_keys(self, self._NON_QUERY_KEYS)\n        overrides = reject_keys(search_terms, self._NON_QUERY_KEYS)\n\n        query = Query(dc.index, **merge_search_terms(originals, overrides))\n        self._assert(query.product == self._product,\n                     ""query for {} returned another product {}"".format(self._product, query.product))\n\n        # find the datasets\n        datasets = (dataset for dataset in dc.index.datasets.search(**query.search_terms) if dataset.uris)\n\n        if query.geopolygon is not None:\n            datasets = select_datasets_inside_polygon(datasets, query.geopolygon)\n\n        # should we put it in the Transformation class?\n        if self.get(\'dataset_predicate\') is not None:\n            datasets = [dataset\n                        for dataset in datasets\n                        if self[\'dataset_predicate\'](dataset)]\n\n        return VirtualDatasetBag(list(datasets), query.geopolygon,\n                                 {product.name: product})\n\n    def group(self, datasets: VirtualDatasetBag, **group_settings: Dict[str, Any]) -> VirtualDatasetBox:\n        geopolygon = datasets.geopolygon\n        selected = list(datasets.bag)\n\n        # geobox\n        merged = merge_search_terms(self, group_settings)\n\n        try:\n            geobox = output_geobox(datasets=selected,\n                                   grid_spec=datasets.product_definitions[self._product].grid_spec,\n                                   geopolygon=geopolygon, **select_keys(merged, self._GEOBOX_KEYS))\n            load_natively = False\n\n        except ValueError:\n            # we are not calculating geoboxes here for the moment\n            # since it may require filesystem access\n            # in ODC 2.0 the dataset should know the information required\n            geobox = None\n            load_natively = True\n\n        # group by time\n        group_query = query_group_by(**select_keys(merged, self._GROUPING_KEYS))\n\n        # information needed for Datacube.load_data\n        return VirtualDatasetBox(Datacube.group_datasets(selected, group_query),\n                                 geobox,\n                                 load_natively,\n                                 datasets.product_definitions,\n                                 geopolygon=None if not load_natively else geopolygon)\n\n    def fetch(self, grouped: VirtualDatasetBox, **load_settings: Dict[str, Any]) -> xarray.Dataset:\n        """""" Convert grouped datasets to `xarray.Dataset`. """"""\n\n        load_keys = self._LOAD_KEYS - {\'measurements\'}\n        merged = merge_search_terms(select_keys(self, load_keys),\n                                    select_keys(load_settings, load_keys))\n\n        product = grouped.product_definitions[self._product]\n\n        if \'measurements\' in self and \'measurements\' in load_settings:\n            for measurement in load_settings[\'measurements\']:\n                self._assert(measurement in self[\'measurements\'],\n                             \'{} not found in {}\'.format(measurement, self._product))\n\n        measurement_dicts = self.output_measurements(grouped.product_definitions,\n                                                     load_settings.get(\'measurements\'))\n\n        if grouped.load_natively:\n            canonical_names = [product.canonical_measurement(measurement) for measurement in measurement_dicts]\n            dataset_geobox = geobox_union_conservative([native_geobox(ds,\n                                                                      measurements=canonical_names,\n                                                                      basis=merged.get(\'like\'))\n                                                        for ds in grouped.box.sum().item()])\n\n            if grouped.geopolygon is not None:\n                reproject_roi = compute_reproject_roi(dataset_geobox,\n                                                      GeoBox.from_geopolygon(grouped.geopolygon,\n                                                                             crs=dataset_geobox.crs,\n                                                                             align=dataset_geobox.alignment,\n                                                                             resolution=dataset_geobox.resolution))\n\n                self._assert(reproject_roi.is_st, ""native load is not axis-aligned"")\n                self._assert(numpy.isclose(reproject_roi.scale, 1.0), ""native load should not require scaling"")\n\n                geobox = dataset_geobox[reproject_roi.roi_src]\n            else:\n                geobox = dataset_geobox\n        else:\n            geobox = grouped.geobox\n\n        result = Datacube.load_data(grouped.box,\n                                    geobox, list(measurement_dicts.values()),\n                                    fuse_func=merged.get(\'fuse_func\'),\n                                    dask_chunks=merged.get(\'dask_chunks\'),\n                                    resampling=merged.get(\'resampling\', \'nearest\'))\n\n        return result\n\n\nclass Transform(VirtualProduct):\n    """""" An on-the-fly transformation. """"""\n\n    @property\n    def _transformation(self) -> Transformation:\n        """""" The `Transformation` object associated with a transform product. """"""\n        cls = self[\'transform\']\n\n        try:\n            obj = cls(**{key: value for key, value in self.items() if key not in [\'transform\', \'input\']})\n        except TypeError:\n            raise VirtualProductException(""transformation {} could not be instantiated"".format(cls))\n\n        self._assert(isinstance(obj, Transformation), ""not a transformation object: {}"".format(obj))\n\n        return cast(Transformation, obj)\n\n    @property\n    def _input(self) -> VirtualProduct:\n        """""" The input product of a transform product. """"""\n        return from_validated_recipe(self[\'input\'])\n\n    def _reconstruct(self):\n        # pylint: disable=protected-access\n        return dict(transform=qualified_name(self[\'transform\']),\n                    input=self._input._reconstruct(), **reject_keys(self, [\'input\', \'transform\']))\n\n    def output_measurements(self, product_definitions: Dict[str, DatasetType]) -> Dict[str, Measurement]:\n        input_measurements = self._input.output_measurements(product_definitions)\n\n        return self._transformation.measurements(input_measurements)\n\n    def query(self, dc: Datacube, **search_terms: Dict[str, Any]) -> VirtualDatasetBag:\n        return self._input.query(dc, **search_terms)\n\n    def group(self, datasets: VirtualDatasetBag, **group_settings: Dict[str, Any]) -> VirtualDatasetBox:\n        return self._input.group(datasets, **group_settings)\n\n    def fetch(self, grouped: VirtualDatasetBox, **load_settings: Dict[str, Any]) -> xarray.Dataset:\n        input_data = self._input.fetch(grouped, **load_settings)\n        output_data = self._transformation.compute(input_data)\n        output_data.attrs[\'crs\'] = input_data.attrs[\'crs\']\n        for data_var in output_data.data_vars:\n            output_data[data_var].attrs[\'crs\'] = input_data.attrs[\'crs\']\n        return output_data\n\n\nclass Aggregate(VirtualProduct):\n    """""" A (non-spatial) statistic of grouped data. """"""\n\n    @property\n    def _statistic(self) -> Transformation:\n        """""" The `Transformation` object associated with an aggregate product. """"""\n        cls = self[\'aggregate\']\n\n        try:\n            obj = cls(**{key: value for key, value in self.items()\n                         if key not in [\'aggregate\', \'input\', \'group_by\']})\n        except TypeError:\n            raise VirtualProductException(""transformation {} could not be instantiated"".format(cls))\n\n        self._assert(isinstance(obj, Transformation), ""not a transformation object: {}"".format(obj))\n\n        return cast(Transformation, obj)\n\n    @property\n    def _input(self) -> VirtualProduct:\n        """""" The input product of a transform product. """"""\n        return from_validated_recipe(self[\'input\'])\n\n    def _reconstruct(self):\n        # pylint: disable=protected-access\n        return dict(aggregate=qualified_name(self[\'aggregate\']),\n                    group_by=qualified_name(self[\'group_by\']),\n                    input=self._input._reconstruct(),\n                    **reject_keys(self, [\'input\', \'aggregate\', \'group_by\']))\n\n    def output_measurements(self, product_definitions: Dict[str, DatasetType]) -> Dict[str, Measurement]:\n        input_measurements = self._input.output_measurements(product_definitions)\n\n        return self._statistic.measurements(input_measurements)\n\n    def query(self, dc: Datacube, **search_terms: Dict[str, Any]) -> VirtualDatasetBag:\n        return self._input.query(dc, **search_terms)\n\n    def group(self, datasets: VirtualDatasetBag, **group_settings: Dict[str, Any]) -> VirtualDatasetBox:\n        grouped = self._input.group(datasets, **group_settings)\n        dim = self.get(\'dim\', \'time\')\n\n        def to_box(value):\n            return xarray.DataArray([VirtualDatasetBox(value, grouped.geobox,\n                                                       grouped.load_natively, grouped.product_definitions,\n                                                       geopolygon=grouped.geopolygon)],\n                                    dims=[\'_fake_\'])\n\n        result = grouped.box.groupby(self[\'group_by\'](grouped.box[dim]), squeeze=False).apply(to_box).squeeze(\'_fake_\')\n        result[dim].attrs.update(grouped.box[dim].attrs)\n\n        return VirtualDatasetBox(result, grouped.geobox, grouped.load_natively, grouped.product_definitions,\n                                 geopolygon=grouped.geopolygon)\n\n    def fetch(self, grouped: VirtualDatasetBox, **load_settings: Dict[str, Any]) -> xarray.Dataset:\n        dim = self.get(\'dim\', \'time\')\n\n        def xr_map(array, func):\n            # convenient function close to `xr_apply` in spirit\n            coords = {key: value.values for key, value in array.coords.items()}\n            for i in numpy.ndindex(array.shape):\n                yield func({key: value[i] for key, value in coords.items()}, array.values[i])\n\n        def statistic(coords, value):\n            data = self._input.fetch(value, **load_settings)\n            result = self._statistic.compute(data)\n            result.coords[dim] = coords[dim]\n            return result\n\n        groups = list(xr_map(grouped.box, statistic))\n        result = xarray.concat(groups, dim=dim).assign_attrs(**select_unique([g.attrs for g in groups]))\n        result.coords[dim].attrs.update(grouped.box[dim].attrs)\n        return result\n\n\nclass Collate(VirtualProduct):\n    """""" Stack observations from products with the same set of measurements. """"""\n\n    @property\n    def _children(self) -> List[VirtualProduct]:\n        """""" The children of a collate product. """"""\n        return [from_validated_recipe(child) for child in self[\'collate\']]\n\n    def _reconstruct(self):\n        # pylint: disable=protected-access\n        children = [child._reconstruct() for child in self._children]\n        return dict(collate=children, **reject_keys(self, [\'collate\']))\n\n    def output_measurements(self, product_definitions: Dict[str, DatasetType]) -> Dict[str, Measurement]:\n        input_measurement_list = [child.output_measurements(product_definitions)\n                                  for child in self._children]\n\n        first, *rest = input_measurement_list\n\n        for child in rest:\n            self._assert(set(child) == set(first),\n                         ""child datasets do not all have the same set of measurements"")\n\n        name = self.get(\'index_measurement_name\')\n        if name is None:\n            return first\n\n        self._assert(name not in first, ""source index measurement \'{}\' already present"".format(name))\n\n        first.update({name: Measurement(name=name, dtype=\'int8\', nodata=-1, units=\'1\')})\n        return first\n\n    def query(self, dc: Datacube, **search_terms: Dict[str, Any]) -> VirtualDatasetBag:\n        result = [child.query(dc, **search_terms) for child in self._children]\n\n        return VirtualDatasetBag({\'collate\': [datasets.bag for datasets in result]},\n                                 select_unique([datasets.geopolygon for datasets in result]),\n                                 merge_dicts([datasets.product_definitions for datasets in result]))\n\n    def group(self, datasets: VirtualDatasetBag, **group_settings: Dict[str, Any]) -> VirtualDatasetBox:\n        self._assert(\'collate\' in datasets.bag and len(datasets.bag[\'collate\']) == len(self._children),\n                     ""invalid dataset bag"")\n\n        def build(source_index, product, dataset_bag):\n            grouped = product.group(VirtualDatasetBag(dataset_bag,\n                                                      datasets.geopolygon, datasets.product_definitions),\n                                    **group_settings)\n\n            def tag(_, value):\n                return {\'collate\': (source_index, value)}\n\n            return grouped.map(tag)\n\n        groups = [build(source_index, product, dataset_bag)\n                  for source_index, (product, dataset_bag)\n                  in enumerate(zip(self._children, datasets.bag[\'collate\']))]\n\n        dim = self.get(\'dim\', \'time\')\n        return VirtualDatasetBox(xarray.concat([grouped.box for grouped in groups], dim=dim).sortby(dim),\n                                 select_unique([grouped.geobox for grouped in groups]),\n                                 select_unique([grouped.load_natively for grouped in groups]),\n                                 merge_dicts([grouped.product_definitions for grouped in groups]),\n                                 geopolygon=select_unique([grouped.geopolygon for grouped in groups]))\n\n    def fetch(self, grouped: VirtualDatasetBox, **load_settings: Dict[str, Any]) -> xarray.Dataset:\n        def is_from(source_index):\n            def result(_, value):\n                self._assert(\'collate\' in value, ""malformed dataset box in collate"")\n                return value[\'collate\'][0] == source_index\n\n            return result\n\n        def strip_source(_, value):\n            return value[\'collate\'][1]\n\n        def fetch_child(child, source_index, r):\n            if any([x == 0 for x in r.box.shape]):\n                # empty raster\n                return None\n            else:\n                result = child.fetch(r, **load_settings)\n                name = self.get(\'index_measurement_name\')\n\n                if name is None:\n                    return result\n\n                # implication for dask?\n                measurement = Measurement(name=name, dtype=\'int8\', nodata=-1, units=\'1\')\n                shape = select_unique([result[band].shape for band in result.data_vars])\n                array = numpy.full(shape, source_index, dtype=measurement.dtype)\n                first = result[list(result.data_vars)[0]]\n                result[name] = xarray.DataArray(array, dims=first.dims, coords=first.coords,\n                                                name=name).assign_attrs(units=measurement.units,\n                                                                        nodata=measurement.nodata)\n                return result\n\n        groups = [fetch_child(child, source_index, grouped.filter(is_from(source_index)).map(strip_source))\n                  for source_index, child in enumerate(self._children)]\n\n        non_empty = [g for g in groups if g is not None]\n\n        dim = self.get(\'dim\', \'time\')\n\n        result = xarray.concat(non_empty,\n                               dim=dim).sortby(dim).assign_attrs(**select_unique([g.attrs\n                                                                                  for g in non_empty]))\n\n        # concat and sortby mess up chunking\n        if \'dask_chunks\' not in load_settings or dim not in load_settings[\'dask_chunks\']:\n            return result\n        return result.apply(lambda x: x.chunk({dim: load_settings[\'dask_chunks\'][dim]}), keep_attrs=True)\n\n\nclass Juxtapose(VirtualProduct):\n    """""" Put measurements from different products side-by-side. """"""\n\n    @property\n    def _children(self) -> List[VirtualProduct]:\n        """""" The children of a juxtapose product. """"""\n        return [from_validated_recipe(child) for child in self[\'juxtapose\']]\n\n    def _reconstruct(self):\n        # pylint: disable=protected-access\n        children = [child._reconstruct() for child in self._children]\n        return dict(juxtapose=children, **reject_keys(self, [\'juxtapose\']))\n\n    def output_measurements(self, product_definitions: Dict[str, DatasetType]) -> Dict[str, Measurement]:\n        input_measurement_list = [child.output_measurements(product_definitions)\n                                  for child in self._children]\n\n        result = cast(Dict[str, Measurement], {})\n        for measurements in input_measurement_list:\n            common = set(result) & set(measurements)\n            self._assert(not common, ""common measurements {} between children"".format(common))\n\n            result.update(measurements)\n\n        return result\n\n    def query(self, dc: Datacube, **search_terms: Dict[str, Any]) -> VirtualDatasetBag:\n        result = [child.query(dc, **search_terms)\n                  for child in self._children]\n\n        return VirtualDatasetBag({\'juxtapose\': [datasets.bag for datasets in result]},\n                                 select_unique([datasets.geopolygon for datasets in result]),\n                                 merge_dicts([datasets.product_definitions for datasets in result]))\n\n    def group(self, datasets: VirtualDatasetBag, **group_settings: Dict[str, Any]) -> VirtualDatasetBox:\n        self._assert(\'juxtapose\' in datasets.bag and len(datasets.bag[\'juxtapose\']) == len(self._children),\n                     ""invalid dataset bag"")\n\n        groups = [product.group(VirtualDatasetBag(dataset_bag, datasets.geopolygon, datasets.product_definitions),\n                                **group_settings)\n                  for product, dataset_bag in zip(self._children, datasets.bag[\'juxtapose\'])]\n\n        aligned_boxes = xarray.align(*[grouped.box for grouped in groups])\n\n        def tuplify(indexes, _):\n            return {\'juxtapose\': [box.sel(**indexes).item() for box in aligned_boxes]}\n\n        return VirtualDatasetBox(xr_apply(aligned_boxes[0], tuplify),\n                                 select_unique([grouped.geobox for grouped in groups]),\n                                 select_unique([grouped.load_natively for grouped in groups]),\n                                 merge_dicts([grouped.product_definitions for grouped in groups]),\n                                 geopolygon=select_unique([grouped.geopolygon for grouped in groups]))\n\n    def fetch(self, grouped: VirtualDatasetBox, **load_settings: Dict[str, Any]) -> xarray.Dataset:\n        def select_child(source_index):\n            def result(_, value):\n                self._assert(\'juxtapose\' in value, ""malformed dataset box in juxtapose"")\n                return value[\'juxtapose\'][source_index]\n\n            return result\n\n        def fetch_recipe(source_index):\n            child_groups = grouped.map(select_child(source_index))\n            return VirtualDatasetBox(child_groups.box, grouped.geobox,\n                                     grouped.load_natively, grouped.product_definitions,\n                                     geopolygon=grouped.geopolygon)\n\n        groups = [child.fetch(fetch_recipe(source_index), **load_settings)\n                  for source_index, child in enumerate(self._children)]\n\n        return xarray.merge(groups).assign_attrs(**select_unique([g.attrs for g in groups]))\n\n\nclass Reproject(VirtualProduct):\n    """"""\n    On-the-fly reprojection of raster data.\n    """"""\n\n    @property\n    def _input(self) -> ""VirtualProduct"":\n        """""" The input product of a transform product. """"""\n        return from_validated_recipe(self[""input""])\n\n    def _reconstruct(self):\n        # pylint: disable=protected-access\n        return dict(input=self._input._reconstruct(), **reject_keys(self, [""input""]))\n\n    def output_measurements(self, product_definitions: Dict[str, DatasetType]) -> Dict[str, Measurement]:\n        """"""\n        A dictionary mapping names to measurement metadata.\n        :param product_definitions: a dictionary mapping product names to products (`DatasetType` objects)\n        """"""\n        return self._input.output_measurements(product_definitions)\n\n    def query(self, dc: Datacube, **search_terms: Dict[str, Any]) -> VirtualDatasetBag:\n        """""" Collection of datasets that match the query. """"""\n        return self._input.query(dc, **reject_keys(search_terms, self._GEOBOX_KEYS))\n\n    def group(self, datasets: VirtualDatasetBag, **group_settings: Dict[str, Any]) -> VirtualDatasetBox:\n        """"""\n        Datasets grouped by their timestamps.\n        :param datasets: the `VirtualDatasetBag` to fetch data from\n        """"""\n        geopolygon = datasets.geopolygon\n\n        merged = merge_search_terms(self, group_settings)\n        if geopolygon is None:\n            selected = list(datasets.contained_datasets())\n        else:\n            selected = None\n\n        geobox = output_geobox(datasets=selected,\n                               output_crs=self[\'reproject\'][\'output_crs\'],\n                               resolution=self[\'reproject\'][\'resolution\'],\n                               align=self[\'reproject\'].get(\'align\'),\n                               geopolygon=geopolygon)\n\n        # load natively\n        input_box = self._input.group(datasets, **reject_keys(merged, self._GEOBOX_KEYS))\n\n        return VirtualDatasetBox(input_box.box,\n                                 geobox,\n                                 True,\n                                 datasets.product_definitions,\n                                 geopolygon=geopolygon)\n\n    def fetch(self, grouped: VirtualDatasetBox, **load_settings: Dict[str, Any]) -> xarray.Dataset:\n        """""" Convert grouped datasets to `xarray.Dataset`. """"""\n        geobox = grouped.geobox\n\n        measurements = self.output_measurements(grouped.product_definitions)\n\n        band_settings = dict(zip(list(measurements),\n                                 per_band_load_data_settings(measurements,\n                                                             resampling=self.get(\'resampling\', \'nearest\'))))\n\n        boxes = [VirtualDatasetBox(box_slice.box, None, True, box_slice.product_definitions, geopolygon=geobox.extent)\n                 for box_slice in grouped.split()]\n\n        dask_chunks = load_settings.get(\'dask_chunks\')\n        if dask_chunks is None:\n            rasters = [self._input.fetch(box, **load_settings) for box in boxes]\n        else:\n            rasters = [self._input.fetch(box, dask_chunks={key: 1 for key in dask_chunks if key not in geobox.dims},\n                                         **reject_keys(load_settings, [\'dask_chunks\']))\n                       for box in boxes]\n\n        result = xarray.Dataset()\n        result.coords[\'time\'] = grouped.box.coords[\'time\']\n\n        for name, coord in grouped.geobox.coordinates.items():\n            result.coords[name] = (name, coord.values, {\'units\': coord.units, \'resolution\': coord.resolution})\n\n        for measurement in measurements:\n            result[measurement] = xarray.concat([reproject_band(raster[measurement],\n                                                                geobox,\n                                                                band_settings[measurement][\'resampling_method\'],\n                                                                grouped.box.dims + geobox.dims,\n                                                                dask_chunks)\n                                                 for raster in rasters], dim=\'time\')\n\n        result.attrs[\'crs\'] = geobox.crs\n        return result\n\n\ndef reproject_band(band, geobox, resampling, dims, dask_chunks=None):\n    """""" Reproject a single measurement to the geobox. """"""\n    if not hasattr(band.data, \'dask\') or dask_chunks is None:\n        data = reproject_array(band.data, band.nodata, band.geobox, geobox, resampling)\n        return wrap_in_dataarray(data, band, geobox, dims)\n\n    dask_name = \'warp_{name}-{token}\'.format(name=band.name, token=uuid.uuid4().hex)\n    dependencies = [band.data]\n\n    spatial_chunks = tuple(dask_chunks.get(k, geobox.shape[i])\n                           for i, k in enumerate(geobox.dims))\n\n    gt = GeoboxTiles(geobox, spatial_chunks)\n    new_layer = {}\n\n    for tile_index in numpy.ndindex(gt.shape):\n        sub_geobox = gt[tile_index]\n        # find the input array slice from the output geobox\n        reproject_roi = compute_reproject_roi(band.geobox, sub_geobox, padding=1)\n\n        # find the chunk from the input array with the slice index\n        subset_band = band[(...,) + reproject_roi.roi_src].chunk(-1)\n\n        if min(subset_band.shape) == 0:\n            # pad the empty chunk\n            new_layer[(dask_name,) + tile_index] = (numpy.full, sub_geobox.shape, band.nodata, band.dtype)\n        else:\n            # next 3 lines to generate the new graph\n            dependencies.append(subset_band.data)\n            # get the input dask array for the function `reproject_array`\n            band_key = list(flatten(subset_band.data.__dask_keys__()))[0]\n            # generate a new layer of dask graph with reroject\n            new_layer[(dask_name,) + tile_index] = (reproject_array,\n                                                    band_key, band.nodata, subset_band.geobox, sub_geobox, resampling)\n\n    # create a new graph with the additional layer and pack the graph into dask.array\n    # since only regular chunking is allowed at the higher level dask.array interface,\n    # to manipulate the graph seems to be the easiest way to obtain a dask.array with irregular chunks after reproject\n    data = dask.array.Array(band.data.dask.from_collections(dask_name, new_layer, dependencies=dependencies),\n                            dask_name,\n                            chunks=spatial_chunks,\n                            dtype=band.dtype,\n                            shape=gt.base.shape)\n\n    return wrap_in_dataarray(data, band, geobox, dims)\n\n\ndef reproject_array(src, nodata, s_geobox, d_geobox, resampling):\n    """""" Reproject a numpy array. """"""\n    dst = numpy.full(d_geobox.shape, fill_value=nodata, dtype=src.dtype)\n    rio_reproject(src=src, dst=dst,\n                  s_gbox=s_geobox, d_gbox=d_geobox,\n                  resampling=resampling_s2rio(resampling),\n                  src_nodata=nodata,\n                  dst_nodata=nodata)\n    return dst\n\n\ndef wrap_in_dataarray(reprojected_data, src_band, dst_geobox, dims):\n    """""" Wrap the reproject numpy array in a `xarray.DataArray` with relevant metadata. """"""\n    non_spatial_shape = src_band.shape[:-2]\n    assert all(x == 1 for x in non_spatial_shape)\n\n    result = xarray.DataArray(data=reprojected_data.reshape(non_spatial_shape + dst_geobox.shape),\n                              dims=dims, attrs=src_band.attrs)\n    result.coords[\'time\'] = src_band.coords[\'time\']\n\n    for name, coord in dst_geobox.coordinates.items():\n        result.coords[name] = (name, coord.values, {\'units\': coord.units, \'resolution\': coord.resolution})\n\n    result.attrs[\'crs\'] = dst_geobox.crs\n    return result\n\n\ndef virtual_product_kind(recipe):\n    """""" One of product, transform, collate, juxtapose, aggregate, or reproject. """"""\n    candidates = [key for key in list(recipe)\n                  if key in [\'product\', \'transform\', \'collate\', \'juxtapose\', \'aggregate\', \'reproject\']]\n    if len(candidates) > 1:\n        raise VirtualProductException(""ambiguous kind in recipe: {}"".format(recipe))\n    if len(candidates) < 1:\n        raise VirtualProductException(""virtual product kind not specified in recipe: {}"".format(recipe))\n    return candidates[0]\n\n\ndef from_validated_recipe(recipe):\n    lookup = dict(product=Product, transform=Transform, collate=Collate,\n                  juxtapose=Juxtapose, aggregate=Aggregate, reproject=Reproject)\n    return lookup[virtual_product_kind(recipe)](recipe)\n'"
datacube/virtual/transformations.py,0,"b'from typing import Optional, Collection\n\nimport numpy\nimport xarray\nimport lark\n\nfrom datacube.utils.masking import make_mask as make_mask_prim\nfrom datacube.utils.masking import mask_invalid_data as mask_invalid_data_prim\nfrom datacube.utils.masking import valid_data_mask\n\nfrom datacube.utils.math import dtype_is_float\n\nfrom .impl import VirtualProductException, Transformation, Measurement\n\n\ndef selective_apply_dict(dictionary, apply_to=None, key_map=None, value_map=None):\n    def skip(key):\n        return apply_to is not None and key not in apply_to\n\n    def key_worker(key):\n        if key_map is None or skip(key):\n            return key\n\n        return key_map(key)\n\n    def value_worker(key, value):\n        if value_map is None or skip(key):\n            return value\n\n        return value_map(key, value)\n\n    return {key_worker(key): value_worker(key, value)\n            for key, value in dictionary.items()}\n\n\ndef selective_apply(data, apply_to=None, key_map=None, value_map=None):\n    return xarray.Dataset(data_vars=selective_apply_dict(data.data_vars, apply_to=apply_to,\n                                                         key_map=key_map, value_map=value_map),\n                          coords=data.coords, attrs=data.attrs)\n\n\nclass MakeMask(Transformation):\n    """"""\n    Create a mask that would only keep pixels for which the measurement with `mask_measurement_name`\n    of the `product` satisfies `flags`.\n\n    Alias in recipe: ``make_mask``.\n\n    :param mask_measurement_name: the name of the measurement to create the mask from\n    :param flags: definition of the flags for the mask\n    """"""\n\n    def __init__(self, mask_measurement_name, flags):\n        self.mask_measurement_name = mask_measurement_name\n        self.flags = flags\n\n    def measurements(self, input_measurements):\n        if self.mask_measurement_name not in input_measurements:\n            raise VirtualProductException(""required measurement {} not found""\n                                          .format(self.mask_measurement_name))\n\n        def worker(_, value):\n            result = value.copy()\n            result[\'dtype\'] = \'bool\'\n            return Measurement(**result)\n\n        return selective_apply_dict(input_measurements,\n                                    apply_to=[self.mask_measurement_name], value_map=worker)\n\n    def compute(self, data):\n        def worker(_, value):\n            return make_mask_prim(value, **self.flags)\n\n        return selective_apply(data, apply_to=[self.mask_measurement_name], value_map=worker)\n\n\nclass ApplyMask(Transformation):\n    """"""\n    Apply a boolean mask to other measurements.\n\n    Alias in recipe: ``apply_mask``.\n\n    :param mask_measurement_name: name of the measurement to use as a mask\n    :param apply_to: list of names of measurements to apply the mask to\n    :param preserve_dtype: whether to cast back to original ``dtype`` after masking\n    :param fallback_dtype: default ``dtype`` for masked measurements\n    :param dilation: the dilation to apply to mask in pixels\n    """"""\n    def __init__(self, mask_measurement_name, apply_to: Optional[Collection[str]] = None,\n                 preserve_dtype=True, fallback_dtype=\'float32\', dilation: int = 0):\n        self.mask_measurement_name = mask_measurement_name\n        self.apply_to = apply_to\n        self.preserve_dtype = preserve_dtype\n        self.fallback_dtype = fallback_dtype\n        self.dilation = int(dilation)\n\n    def measurements(self, input_measurements):\n        rest = {key: value\n                for key, value in input_measurements.items()\n                if key != self.mask_measurement_name}\n\n        def worker(_, value):\n            if self.preserve_dtype:\n                return value\n\n            result = value.copy()\n            result[\'dtype\'] = self.fallback_dtype\n            result[\'nodata\'] = float(\'nan\')\n            return Measurement(**result)\n\n        return selective_apply_dict(rest, apply_to=self.apply_to, value_map=worker)\n\n    def compute(self, data):\n        mask = data[self.mask_measurement_name]\n        rest = data.drop(self.mask_measurement_name)\n\n        def dilate(array):\n            """"""Dilation e.g. for the mask""""""\n            # e.g. kernel = [[1] * 7] * 7 # blocky 3-pixel dilation\n            # pylint: disable=invalid-unary-operand-type\n            y, x = numpy.ogrid[-self.dilation:(self.dilation+1), -self.dilation:(self.dilation+1)]\n            kernel = ((x * x) + (y * y) <= (self.dilation + 0.5) ** 2)  # disk-like `self.dilation` radial dilation\n            return ~scipy.ndimage.binary_dilation(~array.astype(numpy.bool),\n                                                  structure=kernel.reshape((1, )+kernel.shape))\n\n        if self.dilation > 0:\n            import scipy.ndimage\n            mask = xarray.apply_ufunc(dilate, mask, output_dtypes=[numpy.bool], dask=\'parallelized\',\n                                      keep_attrs=True)\n\n        def worker(key, value):\n            if self.preserve_dtype:\n                if \'nodata\' not in value.attrs:\n                    raise VirtualProductException(""measurement {} has no nodata value"".format(key))\n                return value.where(mask, value.attrs[\'nodata\'])\n\n            result = value.where(mask).astype(self.fallback_dtype)\n            result.attrs[\'nodata\'] = float(\'nan\')\n            return result\n\n        return selective_apply(rest, apply_to=self.apply_to, value_map=worker)\n\n\nclass ToFloat(Transformation):\n    """"""\n    Convert measurements to floats and mask invalid data.\n\n    Alias in recipe: ``to_float``.\n\n    :param apply_to: list of names of measurements to apply conversion to\n    :param dtype: default ``dtype`` for conversion\n    """"""\n    def __init__(self, apply_to=None, dtype=\'float32\'):\n        self.apply_to = apply_to\n        self.dtype = dtype\n\n    def measurements(self, input_measurements):\n        def worker(_, value):\n            result = value.copy()\n            result[\'dtype\'] = self.dtype\n            return Measurement(**result)\n\n        return selective_apply_dict(input_measurements, apply_to=self.apply_to, value_map=worker)\n\n    def compute(self, data):\n        def worker(_, value):\n            if hasattr(value, \'dtype\') and value.dtype == self.dtype:\n                return value\n\n            return mask_invalid_data_prim(value).astype(self.dtype)\n\n        return selective_apply(data, apply_to=self.apply_to, value_map=worker)\n\n\nclass Rename(Transformation):\n    """"""\n    Rename measurements.\n\n    Alias in recipe: ``rename``.\n\n    :param measurement_names: mapping from INPUT NAME to OUTPUT NAME\n    """"""\n    def __init__(self, measurement_names):\n        self.measurement_names = measurement_names\n\n    def measurements(self, input_measurements):\n        def key_map(key):\n            return self.measurement_names[key]\n\n        def value_map(key, value):\n            result = value.copy()\n            result[\'name\'] = self.measurement_names[key]\n            return Measurement(**result)\n\n        return selective_apply_dict(input_measurements, apply_to=self.measurement_names,\n                                    key_map=key_map, value_map=value_map)\n\n    def compute(self, data):\n        return data.rename(self.measurement_names)\n\n\nclass Select(Transformation):\n    """"""\n    Keep only specified measurements.\n\n    Alias in recipe: ``select``.\n\n    :param measurement_names: list of measurements to keep\n    """"""\n    def __init__(self, measurement_names):\n        self.measurement_names = measurement_names\n\n    def measurements(self, input_measurements):\n        return {key: value\n                for key, value in input_measurements.items()\n                if key in self.measurement_names}\n\n    def compute(self, data):\n        return data.drop([measurement\n                          for measurement in data.data_vars\n                          if measurement not in self.measurement_names])\n\n\ndef formula_parser():\n    return lark.Lark(""""""\n                ?expr: num_expr | bool_expr\n\n                ?bool_expr: or_clause | comparison_clause\n\n                ?or_clause: or_clause ""|"" and_clause -> or_\n                          | or_clause ""^"" and_clause -> xor\n                          | and_clause\n                ?and_clause: and_clause ""&"" term -> and_\n                           | term\n                ?term: ""not"" term -> not_\n                     | ""("" bool_expr "")""\n\n                ?comparison_clause: eq | ne | le | ge | lt | gt\n\n                eq: num_expr ""=="" num_expr\n                ne: num_expr ""!="" num_expr\n                le: num_expr ""<="" num_expr\n                ge: num_expr "">="" num_expr\n                lt: num_expr ""<"" num_expr\n                gt: num_expr "">"" num_expr\n\n\n                ?num_expr: shift\n\n                ?shift: shift ""<<"" sum -> lshift\n                      | shift "">>"" sum -> rshift\n                      | sum\n\n                ?sum: sum ""+"" product -> add\n                    | sum ""-"" product -> sub\n                    | product\n\n                ?product: product ""*"" atom -> mul\n                        | product ""/"" atom -> truediv\n                        | product ""//"" atom -> floordiv\n                        | product ""%"" atom -> mod\n                        | atom\n\n                ?atom: ""-"" subatom -> neg\n                     | ""+"" subatom -> pos\n                     | ""~"" subatom -> inv\n                     | subatom ""**"" atom -> pow\n                     | subatom\n\n                ?subatom: NAME -> var_name\n                        | FLOAT -> float_literal\n                        | INT -> int_literal\n                        | ""("" num_expr "")""\n\n\n                %import common.FLOAT\n                %import common.INT\n                %import common.WS_INLINE\n                %import common.CNAME -> NAME\n\n                %ignore WS_INLINE\n                """""", start=\'expr\')\n\n\n@lark.v_args(inline=True)\nclass EvaluateTree(lark.Transformer):\n    from operator import not_, or_, and_, xor\n    from operator import eq, ne, le, ge, lt, gt\n    from operator import add, sub, mul, truediv, floordiv, neg, pos, inv, mod, pow, lshift, rshift\n\n    float_literal = float\n    int_literal = int\n\n\nclass Expressions(Transformation):\n    """"""\n    Calculate measurements on-the-fly using arithmetic expressions.\n\n    Alias in recipe: ``expressions``. For example,\n\n    .. code-block:: yaml\n\n       transform: expressions\n       output:\n           ndvi:\n               formula: (nir - red) / (nir + red)\n\n       input:\n           product: example_surface_reflectance_product\n           measurements: [nir, red]\n\n    """"""\n    def __init__(self, output, masked=True):\n        """"""\n        Initialize transformation.\n\n        :param output:\n            A dictionary mapping output measurement names to specifications.\n            That specification can be one of:\n\n            - a measurement name from the input product in which case it is copied over\n            - a dictionary containing a ``formula``,\n              and optionally a ``dtype``, a new ``nodata`` value, and a ``units`` specification\n\n        :param masked:\n            Defaults to ``True``. If set to ``False``, the inputs and outputs are not masked for no data.\n        """"""\n        self.output = output\n        self.masked = masked\n\n    def measurements(self, input_measurements):\n        parser = formula_parser()\n\n        @lark.v_args(inline=True)\n        class EvaluateType(EvaluateTree):\n            def var_name(self, key):\n                return numpy.array([], dtype=input_measurements[key.value].dtype)\n\n        ev = EvaluateType()\n\n        def deduce_type(output_var, output_desc):\n            if \'dtype\' in output_desc:\n                return numpy.dtype(output_desc[\'dtype\'])\n\n            formula = output_desc[\'formula\']\n            tree = parser.parse(formula)\n\n            result = ev.transform(tree)\n            return result.dtype\n\n        def measurement(output_var, output_desc):\n            if isinstance(output_desc, str):\n                # copy measurement over\n                return input_measurements[output_desc]\n\n            return Measurement(name=output_var, dtype=deduce_type(output_var, output_desc),\n                               nodata=output_desc.get(\'nodata\', float(\'nan\')),\n                               units=output_desc.get(\'units\', \'1\'))\n\n        return {output_var: measurement(output_var, output_desc)\n                for output_var, output_desc in self.output.items()}\n\n    def compute(self, data):\n        parser = formula_parser()\n\n        @lark.v_args(inline=True)\n        class EvaluateData(EvaluateTree):\n            def var_name(self, key):\n                return data[key.value]\n\n        @lark.v_args(inline=True)\n        class EvaluateNodataMask(lark.Transformer):\n            # the result of an expression is nodata whenever any of its subexpressions is nodata\n            from operator import or_\n\n            # pylint: disable=invalid-name\n            and_ = _xor = or_\n            eq = ne = le = ge = lt = gt = or_\n            add = sub = mul = truediv = floordiv = mod = pow = lshift = rshift = or_\n\n            def not_(self, value):\n                return value\n\n            neg = pos = inv = not_\n\n            @staticmethod\n            def float_literal(value):\n                return False\n\n            @staticmethod\n            def int_literal(value):\n                return False\n\n            def var_name(self, key):\n                # pylint: disable=invalid-unary-operand-type\n                return ~valid_data_mask(data[key.value])\n\n        ev_data = EvaluateData()\n        ev_mask = EvaluateNodataMask()\n\n        def result(output_var, output_desc):\n            # pylint: disable=invalid-unary-operand-type\n\n            if isinstance(output_desc, str):\n                # copy measurement over\n                return data[output_desc]\n\n            nodata = output_desc.get(\'nodata\')\n            dtype = output_desc.get(\'dtype\')\n\n            formula = output_desc[\'formula\']\n            tree = parser.parse(formula)\n            result = ev_data.transform(tree)\n            result.attrs[\'crs\'] = data.attrs[\'crs\']\n            if nodata is not None:\n                result.attrs[\'nodata\'] = nodata\n            result.attrs[\'units\'] = output_desc.get(\'units\', \'1\')\n\n            if \'masked\' in output_desc:\n                masked = output_desc[\'masked\']\n            else:\n                masked = self.masked\n\n            if not masked:\n                if dtype is None:\n                    return result\n                return result.astype(dtype)\n\n            # masked output\n            if dtype is not None:\n                result = result.astype(dtype)\n\n            dtype = result.dtype\n            mask = ev_mask.transform(tree)\n\n            if numpy.dtype(dtype) == numpy.bool:\n                # any operation on nodata should evaluate to False\n                # omission of attrs[\'nodata\'] is deliberate\n                result = result.where(~mask, False)\n\n            elif nodata is None:\n                if not dtype_is_float(dtype):\n                    raise VirtualProductException(""cannot mask without specified nodata"")\n\n                result = result.where(~mask)\n                result.attrs[\'nodata\'] = numpy.nan\n\n            else:\n                result = result.where(~mask, nodata)\n                result.attrs[\'nodata\'] = nodata\n\n            return result\n\n        return xarray.Dataset(data_vars={output_var: result(output_var, output_desc)\n                                         for output_var, output_desc in self.output.items()},\n                              coords=data.coords, attrs=data.attrs)\n\n\ndef year(time):\n    return time.astype(\'datetime64[Y]\')\n\n\ndef month(time):\n    return time.astype(\'datetime64[M]\')\n\n\ndef week(time):\n    return time.astype(\'datetime64[W]\')\n\n\ndef day(time):\n    return time.astype(\'datetime64[D]\')\n\n\ndef earliest_time(time):\n    earliest_time = time.copy()\n    earliest_time.data[0:] = year(time).data[0]\n    return earliest_time\n\n\nclass XarrayReduction(Transformation):\n    """"""\n    Apply an `xarray` reduction method to the data.\n    """"""\n\n    def __init__(self, method=None, apply_to=None, dtype=None, dim=\'time\', **kwargs):\n        if method is None:\n            raise VirtualProductException(""no method specified in xarray reduction"")\n\n        self.method = method\n        self.kwargs = kwargs\n        self.apply_to = apply_to\n        self.dtype = dtype\n        self.dim = dim\n\n    def measurements(self, input_measurements):\n        def worker(_, value):\n            if self.dtype is None:\n                return value\n\n            result = value.copy()\n            result[\'dtype\'] = self.dtype\n            return Measurement(**result)\n\n        return selective_apply_dict(input_measurements,\n                                    apply_to=self.apply_to, value_map=worker)\n\n    def compute(self, data):\n        func = getattr(xarray.DataArray, self.method)\n\n        def worker(_, value):\n            return func(value, dim=self.dim, **self.kwargs)\n\n        return selective_apply(data, apply_to=self.apply_to, value_map=worker)\n'"
datacube/virtual/utils.py,0,"b'"""""" Utilities to facilitate virtual product implementation. """"""\n\nimport warnings\nimport math\n\n\ndef select_unique(things):\n    """""" Checks that all the members of `things` are equal, and then returns it. """"""\n    first, *rest = things\n    for other in rest:\n        if first != other:\n            warnings.warn(""select_unique may have failed: {} is not the same as {}""\n                          .format(first, other))\n            break\n\n    return first\n\n\ndef select_keys(settings, keys):\n    return {key: value\n            for key, value in settings.items() if key in keys}\n\n\ndef reject_keys(settings, keys):\n    return {key: value\n            for key, value in settings.items() if key not in keys}\n\n\ndef merge_dicts(dicts):\n    """"""\n    Merge a list of dictionaries into one.\n    Later entries override the earlier ones.\n    """"""\n    if len(dicts) == 0:\n        return {}\n    if len(dicts) == 1:\n        return dicts[0]\n\n    first, *rest = dicts\n    result = dict(first)\n    for other in rest:\n        result.update(other)\n    return result\n\n\ndef merge_search_terms(original, override, keys=None):\n    def pick(key, a, b):\n        if b is None:\n            return a\n\n        # trust the override\n        return b\n\n    return {key: pick(key, original.get(key), override.get(key))\n            for key in list(original.keys()) + list(override.keys())\n            if keys is None or key in keys}\n\n\ndef qualified_name(func):\n    return func.__module__ + \'.\' + func.__qualname__\n'"
datacube_apps/stacker/__init__.py,0,"b'# coding=utf-8\n""""""\nModule for stacking datasets along the time dimension in a single file.\n""""""\nfrom .stacker import main\nfrom .fixer import fixer as fixer_main\n\n__all__ = [\'main\', \'fixer_main\']\n'"
datacube_apps/stacker/fixer.py,0,"b'""""""\nFinds single timeslice files that have not been stacked (based on filename), and rewrites them\n\nThis tool is used to update NetCDF metadata for files that are not picked up by the stacker\n\n""""""\n\nimport copy\nimport datetime\nimport itertools\nimport logging\nimport os\nimport re\nimport socket\nfrom functools import partial\nfrom collections import Counter\nfrom typing import List\n\nimport click\nimport dask.array as da\nimport dask\nfrom dateutil import tz\nfrom pathlib import Path\nimport pandas as pd\nimport xarray as xr\n\nimport datacube\nfrom datacube.model import Dataset\nfrom datacube.model.utils import xr_apply, datasets_to_doc\nfrom datacube.drivers.netcdf import create_netcdf_storage_unit, netcdf_writer\nfrom datacube.ui import task_app\n\n_LOG = logging.getLogger(__name__)\n\nAPP_NAME = \'datacube-fixer\'\n\n\ndef make_filename(config, cell_index, start_time):\n    file_path_template = str(Path(config[\'location\'], config[\'file_path_template\']))\n    return file_path_template.format(tile_index=cell_index, start_time=start_time, version=config[\'taskfile_utctime\'])\n\n\ndef get_temp_file(final_output_path):\n    """"""\n    Get a temp file path\n    Changes ""/path/file.nc"" to ""/path/.tmp/file.nc.host.pid.tmp""\n    :param Path final_output_path:\n    :return: Path to temporarily write output\n    :rtype: Path\n    """"""\n\n    tmp_folder = final_output_path.parent / \'.tmp\'\n    id_file = \'{host}.{pid}\'.format(host=socket.gethostname(), pid=os.getpid())\n    tmp_path = (tmp_folder / final_output_path.stem).with_suffix(final_output_path.suffix + id_file + \'.tmp\')\n    try:\n        tmp_folder.mkdir(parents=True)\n    except OSError:\n        pass\n    if tmp_path.exists():\n        tmp_path.unlink()\n    return tmp_path\n\n\nFIND_TIME_RE = re.compile(r\'.+_(?P<start_time>\\d+)(?:_v\\d+)?\\.nc\\Z\')\n\n\ndef get_single_dataset_paths(cell):\n    cnt = Counter(ds.local_path for ds in itertools.chain(*cell.sources.values))\n    files_to_fix = [local_path for local_path, count in cnt.items()\n                    if count == 1 and FIND_TIME_RE.search(str(local_path)).groups()]\n    return files_to_fix\n\n\ndef make_fixer_tasks(index, config, time=None, cell_index=None, **kwargs):\n    """"""Find datasets that have a location not shared by other datasets and make it into a task\n    """"""\n    gw = datacube.api.GridWorkflow(index=index, product=config[\'product\'].name)\n\n    for query in task_app.break_query_into_years(time):\n        cells = gw.list_cells(product=config[\'product\'].name, cell_index=cell_index, **query)\n\n        for cell_index_key, cell in cells.items():\n            files_to_fix = get_single_dataset_paths(cell)\n            if files_to_fix:\n                for cell_time, tile in cell.split(\'time\'):\n                    source_path = tile.sources.values.item()[0].local_path\n                    if source_path in files_to_fix:\n                        tile = gw.update_tile_lineage(tile)\n                        start_time = \'{0:%Y%m%d%H%M%S%f}\'.format(pd.Timestamp(cell_time).to_datetime())\n                        output_filename = make_filename(config, cell_index_key, start_time)\n                        _LOG.info(\'Fixing required for: time=%s, cell=%s. Output=%s\',\n                                  start_time, cell_index_key, output_filename)\n                        yield dict(start_time=cell_time,\n                                   tile=tile,\n                                   cell_index=cell_index_key,\n                                   output_filename=output_filename)\n\n\ndef make_fixer_config(index, config, export_path=None, **query):\n    config[\'product\'] = index.products.get_by_name(config[\'output_type\'])\n\n    if export_path is not None:\n        config[\'location\'] = export_path\n        config[\'index_datasets\'] = False\n    else:\n        config[\'index_datasets\'] = True\n\n    if not os.access(config[\'location\'], os.W_OK):\n        _LOG.warning(\'Current user appears not have write access output location: %s\', config[\'location\'])\n\n    chunking = config[\'storage\'][\'chunking\']\n    chunking = [chunking[dim] for dim in config[\'storage\'][\'dimension_order\']]\n\n    var_param_keys = {\'zlib\', \'complevel\', \'shuffle\', \'fletcher32\', \'contiguous\', \'attrs\'}\n    variable_params = {}\n    for mapping in config[\'measurements\']:\n        varname = mapping[\'name\']\n        variable_params[varname] = {k: v for k, v in mapping.items() if k in var_param_keys}\n        variable_params[varname][\'chunksizes\'] = chunking\n\n    config[\'variable_params\'] = variable_params\n\n    config[\'taskfile_utctime\'] = int(datacube.utils.datetime_to_seconds_since_1970(datetime.datetime.now()))\n\n    return config\n\n\ndef build_history_string(config, task, keep_original=True):\n    tile = task[\'tile\']\n    input_path = str(tile.sources[0].item()[0].local_path)\n    if keep_original:\n        original_dataset = xr.open_dataset(input_path)\n        original_history = original_dataset.attrs.get(\'history\', \'\')\n    else:\n        original_history = \'Original file at {}\'.format(input_path)\n\n    if original_history:\n        original_history += \'\\n\'\n\n    new_history = \'{dt} {user} {app} ({ver}) {args}  # {comment}\'.format(\n        dt=datetime.datetime.now(tz.tzlocal()).isoformat(),\n        user=os.environ.get(\'USERNAME\') or os.environ.get(\'USER\'),\n        app=APP_NAME,\n        ver=datacube.__version__,\n        args=\', \'.join([config[\'app_config_file\'],\n                        str(config[\'taskfile_utctime\']),\n                        task[\'output_filename\'],\n                        str(task[\'start_time\']),\n                        str(task[\'cell_index\'])\n                       ]),\n        comment=\'Updating NetCDF metadata from config file\'\n    )\n    return original_history + new_history\n\n\ndef _unwrap_dataset_list(labels, dataset_list):\n    return dataset_list[0]\n\n\ndef do_fixer_task(config, task):\n    global_attributes = config[\'global_attributes\']\n\n    # Don\'t keep the original history if we are trying to fix it\n    global_attributes[\'history\'] = build_history_string(config, task, keep_original=False)\n\n    variable_params = config[\'variable_params\']\n\n    output_filename = Path(task[\'output_filename\'])\n    output_uri = output_filename.absolute().as_uri()\n    temp_filename = get_temp_file(output_filename)\n    tile = task[\'tile\']\n\n    # Only use the time chunk size (eg 5), but not spatial chunks\n    # This means the file only gets opened once per band, and all data is available when compressing on write\n    # 5 * 4000 * 4000 * 2bytes == 152MB, so mem usage is not an issue\n    chunk_profile = {\'time\': config[\'storage\'][\'chunking\'][\'time\']}\n\n    data = datacube.api.GridWorkflow.load(tile, dask_chunks=chunk_profile)\n\n    unwrapped_datasets = xr_apply(tile.sources, _unwrap_dataset_list, dtype=\'O\')\n    data[\'dataset\'] = datasets_to_doc(unwrapped_datasets)\n\n    try:\n        if data.geobox is None:\n            raise DatacubeException(\'Dataset geobox property is None, cannot write to NetCDF file.\')\n\n        if data.geobox.crs is None:\n            raise DatacubeException(\'Dataset geobox.crs property is None, cannot write to NetCDF file.\')\n\n        nco = create_netcdf_storage_unit(temp_filename,\n                                         data.geobox.crs,\n                                         data.coords,\n                                         data.data_vars,\n                                         variable_params,\n                                         global_attributes)\n        write_data_variables(data.data_vars, nco)\n        nco.close()\n\n        temp_filename.rename(output_filename)\n\n        if config.get(\'check_data_identical\', False):\n            new_tile = make_updated_tile(unwrapped_datasets, output_uri, tile.geobox)\n            new_data = datacube.api.GridWorkflow.load(new_tile, dask_chunks=chunk_profile)\n            check_identical(data, new_data, output_filename)\n\n    except Exception as e:\n        if temp_filename.exists():\n            temp_filename.unlink()\n        raise e\n\n    return unwrapped_datasets, output_uri\n\n\ndef write_data_variables(data_vars, nco):\n    for name, variable in data_vars.items():\n        try:\n            with dask.set_options(get=dask.local.get_sync):\n                da.store(variable.data, nco[name], lock=True)\n        except ValueError:\n            nco[name][:] = netcdf_writer.netcdfy_data(variable.values)\n        nco.sync()\n\n\ndef check_identical(data1, data2, output_filename):\n    with dask.set_options(get=dask.local.get_sync):\n        if not all((data1 == data2).all().values()):\n            _LOG.error(""Mismatch found for %s, not indexing"", output_filename)\n            raise ValueError(""Mismatch found for %s, not indexing"" % output_filename)\n    return True\n\n\ndef make_updated_tile(old_datasets, new_uri, geobox):\n    def update_dataset_location(labels, dataset: Dataset) -> List[Dataset]:\n        new_dataset = copy.copy(dataset)\n        new_dataset.uris = [new_uri]\n        return [new_dataset]\n\n    updated_datasets = xr_apply(old_datasets, update_dataset_location, dtype=\'O\')\n    return datacube.api.Tile(sources=updated_datasets, geobox=geobox)\n\n\ndef process_result(index, result):\n    datasets, new_uri = result\n    for dataset in datasets.values:\n        _LOG.info(\'Updating dataset location: %s\', dataset.local_path)\n        old_uri = dataset.local_uri\n        index.datasets.add_location(dataset.id, new_uri)\n        index.datasets.archive_location(dataset.id, old_uri)\n\n\n@click.command(name=APP_NAME)\n@datacube.ui.click.pass_index(app_name=APP_NAME)\n@datacube.ui.click.global_cli_options\n@click.option(\'--cell-index\', \'cell_index\', help=\'Limit the process to a particular cell (e.g. 14,-11)\',\n              callback=task_app.validate_cell_index, default=None)\n@click.option(\'--year\', \'time\', callback=task_app.validate_year, help=\'Limit the process to a particular year\')\n@click.option(\'--export-path\', \'export_path\',\n              help=\'Write the stacked files to an external location without updating the index\',\n              default=None,\n              type=click.Path(exists=True, writable=True, file_okay=False))\n@task_app.queue_size_option\n@task_app.task_app_options\n@task_app.task_app(make_config=make_fixer_config, make_tasks=make_fixer_tasks)\ndef fixer(index, config, tasks, executor, queue_size, **kwargs):\n    """"""This script rewrites unstacked dataset files to correct their NetCDF metadata.""""""\n    click.echo(\'Starting fixer utility...\')\n\n    task_func = partial(do_fixer_task, config)\n    process_func = partial(process_result, index) if config[\'index_datasets\'] else None\n    task_app.run_tasks(tasks, executor, task_func, process_func, queue_size)\n\n\nif __name__ == \'__main__\':\n    fixer()\n'"
datacube_apps/stacker/stacker.py,0,"b'""""""\nCreate time-stacked NetCDF files\n\n""""""\n\nimport copy\nimport datetime\nimport itertools\nimport logging\nimport os\nimport socket\nfrom functools import partial\nfrom typing import List\n\nimport click\nimport dask.array as da\nimport dask\nfrom dateutil import tz\nfrom pathlib import Path\n\nimport datacube\nfrom datacube.model import Dataset\nfrom datacube.model.utils import xr_apply, datasets_to_doc\nfrom datacube.utils import mk_part_uri\nfrom datacube.drivers.netcdf import create_netcdf_storage_unit, netcdf_writer\nfrom datacube.ui import task_app\n\n_LOG = logging.getLogger(__name__)\n\nAPP_NAME = \'datacube-stacker\'\n\n\ndef get_filename(config, cell_index, year):\n    file_path_template = str(Path(config[\'location\'], config[\'file_path_template\']))\n    return file_path_template.format(tile_index=cell_index, start_time=year, version=config[\'taskfile_utctime\'])\n\n\ndef get_temp_file(final_output_path) -> Path:\n    """"""\n    Get a temp file path\n    Changes ""/path/file.nc"" to ""/path/.tmp/file.nc.host.pid.tmp""\n    :param Path final_output_path:\n    :return: Path to temporarily write output\n    """"""\n\n    tmp_folder = final_output_path.parent / \'.tmp\'\n    id_file = \'{host}.{pid}\'.format(host=socket.gethostname(), pid=os.getpid())\n    tmp_path = (tmp_folder / final_output_path.stem).with_suffix(final_output_path.suffix + id_file + \'.tmp\')\n    try:\n        tmp_folder.mkdir(parents=True)\n    except OSError:\n        pass\n    if tmp_path.exists():\n        tmp_path.unlink()\n    return tmp_path\n\n\ndef make_stacker_tasks(index, config, cell_index=None, time=None, **kwargs):\n    gw = datacube.api.GridWorkflow(index=index, product=config[\'product\'].name)\n\n    for query in task_app.break_query_into_years(time):\n        cells = gw.list_cells(product=config[\'product\'].name, cell_index=cell_index, **query)\n        for cell_index_key, tile in cells.items():\n            for year, year_tile in tile.split_by_time(freq=\'A\'):\n                storage_files = set(ds.local_path for ds in itertools.chain(*year_tile.sources.values))\n                if len(storage_files) > 1:\n                    year_tile = gw.update_tile_lineage(year_tile)\n                    output_filename = get_filename(config, cell_index_key, year)\n                    _LOG.info(\'Stacking required for: year=%s, cell=%s. Output=%s\',\n                              year, cell_index_key, output_filename)\n                    yield dict(year=year,\n                               tile=year_tile,\n                               cell_index=cell_index_key,\n                               output_filename=output_filename)\n                elif len(storage_files) == 1:\n                    [only_filename] = storage_files\n                    _LOG.info(\'Stacking not required for: year=%s, cell=%s. existing=%s\',\n                              year, cell_index_key, only_filename)\n\n\ndef make_stacker_config(index, config, export_path=None, check_data=None, **query):\n    config[\'product\'] = index.products.get_by_name(config[\'output_type\'])\n\n    if export_path is not None:\n        config[\'location\'] = export_path\n        config[\'index_datasets\'] = False\n    else:\n        config[\'index_datasets\'] = True\n\n    if check_data is not None:\n        config[\'check_data_identical\'] = check_data\n\n    if not os.access(config[\'location\'], os.W_OK):\n        _LOG.warning(\'Current user appears not have write access output location: %s\', config[\'location\'])\n\n    chunking = config[\'storage\'][\'chunking\']\n    chunking = [chunking[dim] for dim in config[\'storage\'][\'dimension_order\']]\n\n    var_param_keys = {\'zlib\', \'complevel\', \'shuffle\', \'fletcher32\', \'contiguous\', \'attrs\'}\n    variable_params = {}\n    for mapping in config[\'measurements\']:\n        varname = mapping[\'name\']\n        variable_params[varname] = {k: v for k, v in mapping.items() if k in var_param_keys}\n        variable_params[varname][\'chunksizes\'] = chunking\n\n    config[\'variable_params\'] = variable_params\n\n    config[\'taskfile_utctime\'] = int(datacube.utils.datetime_to_seconds_since_1970(datetime.datetime.now()))\n\n    return config\n\n\ndef get_history_attribute(config, task):\n    return \'{dt} {user} {app} ({ver}) {args}  # {comment}\'.format(\n        dt=datetime.datetime.now(tz.tzlocal()).isoformat(),\n        user=os.environ.get(\'USERNAME\') or os.environ.get(\'USER\'),\n        app=APP_NAME,\n        ver=datacube.__version__,\n        args=\', \'.join([config[\'app_config_file\'],\n                        str(config[\'taskfile_utctime\']),\n                        task[\'output_filename\'],\n                        str(task[\'year\']),\n                        str(task[\'cell_index\'])\n                       ]),\n        comment=\'Stacking datasets for a year into a single NetCDF file\'\n    )\n\n\ndef _unwrap_dataset_list(labels, dataset_list):\n    return dataset_list[0]\n\n\ndef do_stack_task(config, task):\n    global_attributes = config[\'global_attributes\']\n    global_attributes[\'history\'] = get_history_attribute(config, task)\n\n    variable_params = config[\'variable_params\']\n\n    variable_params[\'dataset\'] = {\n        \'chunksizes\': (1,),\n        \'zlib\': True,\n        \'complevel\': 9,\n    }\n\n    output_filename = Path(task[\'output_filename\'])\n    output_uri = output_filename.absolute().as_uri()\n    temp_filename = get_temp_file(output_filename)\n    tile = task[\'tile\']\n\n    # Only use the time chunk size (eg 5), but not spatial chunks\n    # This means the file only gets opened once per band, and all data is available when compressing on write\n    # 5 * 4000 * 4000 * 2bytes == 152MB, so mem usage is not an issue\n    chunk_profile = {\'time\': config[\'storage\'][\'chunking\'][\'time\']}\n\n    data = datacube.api.GridWorkflow.load(tile, dask_chunks=chunk_profile)\n\n    unwrapped_datasets = xr_apply(tile.sources, _unwrap_dataset_list, dtype=\'O\')\n    data[\'dataset\'] = datasets_to_doc(unwrapped_datasets)\n\n    try:\n        if data.geobox is None:\n            raise DatacubeException(\'Dataset geobox property is None, cannot write to NetCDF file.\')\n\n        if data.geobox.crs is None:\n            raise DatacubeException(\'Dataset geobox.crs property is None, cannot write to NetCDF file.\')\n\n        nco = create_netcdf_storage_unit(temp_filename,\n                                         data.geobox.crs,\n                                         data.coords,\n                                         data.data_vars,\n                                         variable_params,\n                                         global_attributes)\n        write_data_variables(data.data_vars, nco)\n        nco.close()\n\n        temp_filename.rename(output_filename)\n\n        if config.get(\'check_data_identical\', False):\n            new_tile = make_updated_tile(unwrapped_datasets, output_uri, tile.geobox)\n            new_data = datacube.api.GridWorkflow.load(new_tile, dask_chunks=chunk_profile)\n            check_identical(data, new_data, output_filename)\n\n    except Exception as e:\n        if temp_filename.exists():\n            temp_filename.unlink()\n        raise e\n\n    return unwrapped_datasets, output_uri\n\n\ndef write_data_variables(data_vars, nco):\n    for name, variable in data_vars.items():\n        try:\n            with dask.set_options(get=dask.local.get_sync):\n                da.store(variable.data, nco[name], lock=True)\n        except ValueError:\n            nco[name][:] = netcdf_writer.netcdfy_data(variable.values)\n        nco.sync()\n\n\ndef check_identical(data1, data2, output_filename):\n    _LOG.debug(\'Verifying file: ""%s""\', output_filename)\n    with dask.set_options(get=dask.local.get_sync):\n        if not all((data1 == data2).all().values()):\n            _LOG.error(""Mismatch found for %s, not indexing"", output_filename)\n            raise ValueError(""Mismatch found for %s, not indexing"" % output_filename)\n    return True\n\n\ndef make_updated_tile(old_datasets, new_uri, geobox):\n    def update_dataset_location(idx, labels, dataset: Dataset) -> List[Dataset]:\n        idx, = idx\n        new_dataset = copy.copy(dataset)\n        new_dataset.uris = [mk_part_uri(new_uri, idx)]\n        return [new_dataset]\n\n    updated_datasets = xr_apply(old_datasets, update_dataset_location, with_numeric_index=True)\n    return datacube.api.Tile(sources=updated_datasets, geobox=geobox)\n\n\ndef process_result(index, result):\n    datasets, new_common_uri = result\n    for idx, dataset in enumerate(datasets.values):\n        new_uri = mk_part_uri(new_common_uri, idx)\n        _LOG.info(\'Updating dataset location: %s\', dataset.local_path)\n        old_uri = dataset.local_uri\n        index.datasets.add_location(dataset.id, new_uri)\n        index.datasets.archive_location(dataset.id, old_uri)\n\n\n@click.command(name=APP_NAME)\n@datacube.ui.click.pass_index(app_name=APP_NAME)\n@datacube.ui.click.global_cli_options\n@click.option(\'--cell-index\', \'cell_index\', help=\'Limit the process to a particular cell (e.g. 14,-11)\',\n              callback=task_app.validate_cell_index, default=None)\n@click.option(\'--year\', \'time\', callback=task_app.validate_year, help=\'Limit the process to a particular year\')\n@click.option(\'--export-path\', \'export_path\',\n              help=\'Write the stacked files to an external location without updating the index\',\n              default=None,\n              type=click.Path(exists=True, writable=True, file_okay=False))\n@click.option(\'--check-data/--no-check-data\', is_flag=True, default=None,\n              help=""Overrides config option: check_data_identical"")\n@task_app.queue_size_option\n@task_app.task_app_options\n@task_app.task_app(make_config=make_stacker_config, make_tasks=make_stacker_tasks)\ndef main(index, config, tasks, executor, queue_size, **kwargs):\n    """"""Store datasets into NetCDF files containing an entire year in the same file.\n\n    - Uses the same configuration format as the `ingest` tool.\n    - However, does not create new datasets, but instead updates dataset locations then archives the original location.\n    """"""\n    click.echo(\'Starting stacking utility...\')\n\n    task_func = partial(do_stack_task, config)\n    process_func = partial(process_result, index) if config[\'index_datasets\'] else None\n    task_app.run_tasks(tasks, executor, task_func, process_func, queue_size)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/dummy_task_app/dummy_task_app.py,0,"b'#!/usr/bin/env python\n"""""" Sample task app\n""""""\n\n\nimport random\nimport click\nfrom datacube.ui import click as ui\nfrom datacube.ui.task_app import task_app, task_app_options, run_tasks, wrap_task\n\nAPP_NAME = \'dummy\'\n\n\ndef random_sleep(amount_secs=0.1, prop=0.5):\n    """"""emulate processing time variance""""""\n    from time import sleep\n    from random import uniform\n\n    if uniform(0, 1) < prop:\n        sleep(amount_secs)\n\n\ndef unused(*_, **_unused):\n    """"""Used to silence pylint warnings""""""\n    pass\n\n\ndef make_config(db_index, config, **opts):\n    """"""Called after parsing command line arguments and initialising database index.\n\n    The idea is to inject extra configs based on the content of the database,\n    app config file and command line arguments.\n\n    """"""\n\n    click.echo(""------------------------------"")\n    click.echo(opts)\n    click.echo(""------------------------------"")\n\n    # in real program you might need these\n    unused(db_index)\n\n    # Override config value with command line value, or set to default value of 10\n    num_tasks = opts.get(\'num_tasks\')\n    if num_tasks is None:\n        num_tasks = config.get(\'num_tasks\', 10)\n    config[\'num_tasks\'] = num_tasks\n\n    return config\n\n\ndef make_tasks(db_index, config, **opts):\n    """""" Generate a task list.\n\n    This function receives config created by `make_config` as well as database index\n    """"""\n    num_tasks = config[\'num_tasks\']\n\n    unused(db_index, opts)\n\n    for i in range(num_tasks):\n        print(\'Generating task: {}\'.format(i))\n        yield {\'val\': i}\n\n\ndef run_task(task, op):\n    """""" Runs across multiple cpus/nodes\n    """"""\n    from math import sqrt\n\n    ops = {\'sqrt\': sqrt,\n           \'pow2\': lambda x: x*x}\n\n    random_sleep(1, 0.1)  # Sleep for 1 second 10% of the time\n\n    val = task[\'val\']\n\n    if val == 666:\n        click.echo(\'Injecting failure\')\n        raise IOError(\'Fake IO Error\')\n\n    result = ops[op](val)\n    click.echo(\'{} => {}\'.format(val, result))\n\n    return result\n\n\n@click.command(name=APP_NAME)\n@ui.pass_index(app_name=APP_NAME)\n@task_app_options\n@click.option(\'--num-tasks\', type=int, help=\'Sample argument: number of tasks to generate\')\n@task_app(make_config=make_config, make_tasks=make_tasks)\ndef app_main(db_index, config, tasks, executor, **opts):\n    """"""\n    make_config => config\n    config => make_tasks => tasks\n    """"""\n    from pickle import dumps\n\n    unused(db_index, opts, config)\n\n    click.echo(\'Using executor {}\'.format(repr(executor)))\n    task_runner = wrap_task(run_task, config[\'op\'])\n\n    click.echo(\'Task function size: {}\'.format(\n        len(dumps(task_runner))\n    ))\n\n    run_tasks(tasks, executor, task_runner, queue_size=10)\n\n    return 0\n\n\nif __name__ == \'__main__\':\n    random.seed()\n    app_main()\n'"
examples/io_plugin/setup.py,0,"b'from setuptools import setup, find_packages\n\nsetup(\n    name=\'dcio_example\',\n    version=""1.0"",\n    description=""Test IO plugins for datacube"",\n    author=\'AGDC Collaboration\',\n    packages=find_packages(),\n\n    entry_points={\n        \'datacube.plugins.io.read\': [\n            \'pickle=dcio_example.pickles:rdr_driver_init\',\n            \'zeros=dcio_example.zeros:init_driver\'\n        ],\n        \'datacube.plugins.io.write\': [\n            \'pickle=dcio_example.pickles:writer_driver_init\',\n        ]\n\n    }\n)\n'"
integration_tests/index/__init__.py,0,"b'# coding=utf-8\n""""""\nModule\n""""""\n'"
integration_tests/index/test_config_docs.py,0,"b'# coding=utf-8\n""""""\nModule\n""""""\n\nimport copy\nimport pytest\nimport yaml\n\nfrom datacube.drivers.postgres._fields import NumericRangeDocField, PgField\nfrom datacube.index.index import Index\nfrom datacube.index._metadata_types import default_metadata_type_docs\nfrom datacube.model import MetadataType, DatasetType\nfrom datacube.model import Range, Dataset\nfrom datacube.utils import changes\n\n_DATASET_METADATA = {\n    \'id\': \'f7018d80-8807-11e5-aeaa-1040f381a756\',\n    \'instrument\': {\'name\': \'TM\'},\n    \'platform\': {\n        \'code\': \'LANDSAT_5\',\n        \'label\': \'Landsat 5\'\n    },\n    \'size_bytes\': 4550,\n    \'product_type\': \'NBAR\',\n    \'bands\': {\n        \'1\': {\n            \'type\': \'reflective\',\n            \'cell_size\': 25.0,\n            \'path\': \'product/LS8_OLITIRS_NBAR_P54_GALPGS01-002_112_079_20140126_B1.tif\',\n            \'label\': \'Coastal Aerosol\',\n            \'number\': \'1\'\n        },\n        \'2\': {\n            \'type\': \'reflective\',\n            \'cell_size\': 25.0,\n            \'path\': \'product/LS8_OLITIRS_NBAR_P54_GALPGS01-002_112_079_20140126_B2.tif\',\n            \'label\': \'Visible Blue\',\n            \'number\': \'2\'\n        },\n        \'3\': {\n            \'type\': \'reflective\',\n            \'cell_size\': 25.0,\n            \'path\': \'product/LS8_OLITIRS_NBAR_P54_GALPGS01-002_112_079_20140126_B3.tif\',\n            \'label\': \'Visible Green\',\n            \'number\': \'3\'\n        },\n    }\n}\n\n\ndef test_metadata_indexes_views_exist(initialised_postgres_db, default_metadata_type):\n    """"""\n    :type initialised_postgres_db: datacube.drivers.postgres._connections.PostgresDb\n    :type default_metadata_type: datacube.model.MetadataType\n    """"""\n    # Metadata indexes should no longer exist.\n    assert not _object_exists(initialised_postgres_db, \'dix_eo_platform\')\n\n    # Ensure view was created (following naming conventions)\n    assert _object_exists(initialised_postgres_db, \'dv_eo_dataset\')\n\n\ndef test_dataset_indexes_views_exist(initialised_postgres_db, ls5_telem_type):\n    """"""\n    :type initialised_postgres_db: datacube.drivers.postgres._connections.PostgresDb\n    :type ls5_telem_type: datacube.model.DatasetType\n    """"""\n    assert ls5_telem_type.name == \'ls5_telem_test\'\n\n    # Ensure field indexes were created for the dataset type (following the naming conventions):\n    assert _object_exists(initialised_postgres_db, ""dix_ls5_telem_test_orbit"")\n\n    # Ensure it does not create a \'platform\' index, because that\'s a fixed field\n    # (ie. identical in every dataset of the type)\n    assert not _object_exists(initialised_postgres_db, ""dix_ls5_telem_test_platform"")\n\n    # Ensure view was created (following naming conventions)\n    assert _object_exists(initialised_postgres_db, \'dv_ls5_telem_test_dataset\')\n\n    # Ensure view was created (following naming conventions)\n    assert not _object_exists(initialised_postgres_db,\n                              \'dix_ls5_telem_test_gsi\'), ""indexed=false field gsi shouldn\'t have an index""\n\n\ndef test_dataset_composite_indexes_exist(initialised_postgres_db, ls5_telem_type):\n    # This type has fields named lat/lon/time, so composite indexes should now exist for them:\n    # (following the naming conventions)\n    assert _object_exists(initialised_postgres_db, ""dix_ls5_telem_test_sat_path_sat_row_time"")\n\n    # But no individual field indexes for these\n    assert not _object_exists(initialised_postgres_db, ""dix_ls5_telem_test_sat_path"")\n    assert not _object_exists(initialised_postgres_db, ""dix_ls5_telem_test_sat_row"")\n    assert not _object_exists(initialised_postgres_db, ""dix_ls5_telem_test_time"")\n\n\ndef test_field_expression_unchanged(default_metadata_type, telemetry_metadata_type):\n    # type: (MetadataType, MetadataType) -> None\n\n    # We\'re checking for accidental changes here in our field-to-SQL code\n\n    # If we started outputting a different expression they would quietly no longer match the expression\n    # indexes that exist in our DBs.\n\n    # The time field on the default \'eo\' metadata type.\n    field = default_metadata_type.dataset_fields[\'time\']\n    assert isinstance(field, PgField)\n    assert field.sql_expression == (\n        ""tstzrange(""\n        ""least(""\n        ""agdc.common_timestamp(agdc.dataset.metadata #>> \'{extent, from_dt}\'), ""\n        ""agdc.common_timestamp(agdc.dataset.metadata #>> \'{extent, center_dt}\')""\n        ""), greatest(""\n        ""agdc.common_timestamp(agdc.dataset.metadata #>> \'{extent, to_dt}\'), ""\n        ""agdc.common_timestamp(agdc.dataset.metadata #>> \'{extent, center_dt}\')""\n        ""), \'[]\')""\n    )\n\n    field = default_metadata_type.dataset_fields[\'lat\']\n    assert isinstance(field, PgField)\n    assert field.sql_expression == (\n        ""agdc.float8range(""\n        ""least(""\n        ""CAST(agdc.dataset.metadata #>> \'{extent, coord, ur, lat}\' AS DOUBLE PRECISION), ""\n        ""CAST(agdc.dataset.metadata #>> \'{extent, coord, lr, lat}\' AS DOUBLE PRECISION), ""\n        ""CAST(agdc.dataset.metadata #>> \'{extent, coord, ul, lat}\' AS DOUBLE PRECISION), ""\n        ""CAST(agdc.dataset.metadata #>> \'{extent, coord, ll, lat}\' AS DOUBLE PRECISION)), ""\n        ""greatest(""\n        ""CAST(agdc.dataset.metadata #>> \'{extent, coord, ur, lat}\' AS DOUBLE PRECISION), ""\n        ""CAST(agdc.dataset.metadata #>> \'{extent, coord, lr, lat}\' AS DOUBLE PRECISION), ""\n        ""CAST(agdc.dataset.metadata #>> \'{extent, coord, ul, lat}\' AS DOUBLE PRECISION), ""\n        ""CAST(agdc.dataset.metadata #>> \'{extent, coord, ll, lat}\' AS DOUBLE PRECISION)""\n        ""), \'[]\')""\n    )\n\n    # A single string value\n    field = default_metadata_type.dataset_fields[\'platform\']\n    assert isinstance(field, PgField)\n    assert field.sql_expression == (\n        ""agdc.dataset.metadata #>> \'{platform, code}\'""\n    )\n\n    # A single integer value\n    field = telemetry_metadata_type.dataset_fields[\'orbit\']\n    assert isinstance(field, PgField)\n    assert field.sql_expression == (\n        ""CAST(agdc.dataset.metadata #>> \'{acquisition, platform_orbit}\' AS INTEGER)""\n    )\n\n\ndef _object_exists(db, index_name):\n    with db.connect() as connection:\n        val = connection._connection.execute(""SELECT to_regclass(\'agdc.%s\')"" % index_name).scalar()\n    return val == (\'agdc.%s\' % index_name)\n\n\ndef test_idempotent_add_dataset_type(index, ls5_telem_type, ls5_telem_doc):\n    """"""\n    :type ls5_telem_type: datacube.model.DatasetType\n    :type index: datacube.index._api.Index\n    """"""\n    assert index.products.get_by_name(ls5_telem_type.name) is not None\n\n    # Re-add should have no effect, because it\'s equal to the current one.\n    index.products.add_document(ls5_telem_doc)\n\n    # But if we add the same type with differing properties we should get an error:\n    different_telemetry_type = copy.deepcopy(ls5_telem_doc)\n    different_telemetry_type[\'metadata\'][\'ga_label\'] = \'something\'\n    with pytest.raises(changes.DocumentMismatchError):\n        index.products.add_document(different_telemetry_type)\n\n        # TODO: Support for adding/changing search fields?\n\n\ndef test_update_dataset(index, ls5_telem_doc, example_ls5_nbar_metadata_doc):\n    """"""\n    :type index: datacube.index._api.Index\n    """"""\n    ls5_telem_type = index.products.add_document(ls5_telem_doc)\n    assert ls5_telem_type\n\n    example_ls5_nbar_metadata_doc[\'lineage\'][\'source_datasets\'] = {}\n    dataset = Dataset(ls5_telem_type, example_ls5_nbar_metadata_doc, uris=[\'file:///test/doc.yaml\'], sources={})\n    dataset = index.datasets.add(dataset)\n    assert dataset\n\n    # update with the same doc should do nothing\n    index.datasets.update(dataset)\n    updated = index.datasets.get(dataset.id)\n    assert updated.local_uri == \'file:///test/doc.yaml\'\n    assert updated.uris == [\'file:///test/doc.yaml\']\n\n    # update location\n    assert index.datasets.get(dataset.id).local_uri == \'file:///test/doc.yaml\'\n    update = Dataset(ls5_telem_type, example_ls5_nbar_metadata_doc, uris=[\'file:///test/doc2.yaml\'], sources={})\n    index.datasets.update(update)\n    updated = index.datasets.get(dataset.id)\n\n    # New locations are appended on update.\n    # They may be indexing the same dataset from a different location: we don\'t want to remove the original location.\n    # Returns the most recently added\n    assert updated.local_uri == \'file:///test/doc2.yaml\'\n    # But both still exist (newest-to-oldest order)\n    assert updated.uris == [\'file:///test/doc2.yaml\', \'file:///test/doc.yaml\']\n\n    # adding more metadata should always be allowed\n    doc = copy.deepcopy(updated.metadata_doc)\n    doc[\'test1\'] = {\'some\': \'thing\'}\n    update = Dataset(ls5_telem_type, doc, uris=updated.uris)\n    index.datasets.update(update)\n    updated = index.datasets.get(dataset.id)\n    assert updated.metadata_doc[\'test1\'] == {\'some\': \'thing\'}\n    assert updated.local_uri == \'file:///test/doc2.yaml\'\n    assert len(updated.uris) == 2\n\n    # adding more metadata and changing location\n    doc = copy.deepcopy(updated.metadata_doc)\n    doc[\'test2\'] = {\'some\': \'other thing\'}\n    update = Dataset(ls5_telem_type, doc, uris=[\'file:///test/doc3.yaml\'])\n    index.datasets.update(update)\n    updated = index.datasets.get(dataset.id)\n    assert updated.metadata_doc[\'test1\'] == {\'some\': \'thing\'}\n    assert updated.metadata_doc[\'test2\'] == {\'some\': \'other thing\'}\n    assert updated.local_uri == \'file:///test/doc3.yaml\'\n    assert len(updated.uris) == 3\n\n    # changing existing metadata fields isn\'t allowed by default\n    doc = copy.deepcopy(updated.metadata_doc)\n    doc[\'product_type\'] = \'foobar\'\n    update = Dataset(ls5_telem_type, doc, uris=[\'file:///test/doc4.yaml\'])\n    with pytest.raises(ValueError):\n        index.datasets.update(update)\n    updated = index.datasets.get(dataset.id)\n    assert updated.metadata_doc[\'test1\'] == {\'some\': \'thing\'}\n    assert updated.metadata_doc[\'test2\'] == {\'some\': \'other thing\'}\n    assert updated.metadata_doc[\'product_type\'] == \'nbar\'\n    assert updated.local_uri == \'file:///test/doc3.yaml\'\n    assert len(updated.uris) == 3\n\n    # allowed changes go through\n    doc = copy.deepcopy(updated.metadata_doc)\n    doc[\'product_type\'] = \'foobar\'\n\n\ndef test_update_dataset_type(index, ls5_telem_type, ls5_telem_doc, ga_metadata_type_doc):\n    """"""\n    :type ls5_telem_type: datacube.model.DatasetType\n    :type index: datacube.index._api.Index\n    """"""\n    assert index.products.get_by_name(ls5_telem_type.name) is not None\n\n    # Update with a new description\n    ls5_telem_doc[\'description\'] = ""New description""\n    index.products.update_document(ls5_telem_doc)\n    # Ensure was updated\n    assert index.products.get_by_name(ls5_telem_type.name).definition[\'description\'] == ""New description""\n\n    # Remove some match rules (looser rules -- that match more datasets -- should be allowed)\n    assert \'format\' in ls5_telem_doc[\'metadata\']\n    del ls5_telem_doc[\'metadata\'][\'format\'][\'name\']\n    del ls5_telem_doc[\'metadata\'][\'format\']\n    index.products.update_document(ls5_telem_doc)\n    # Ensure was updated\n    updated_type = index.products.get_by_name(ls5_telem_type.name)\n    assert updated_type.definition[\'metadata\'] == ls5_telem_doc[\'metadata\']\n\n    # Specifying metadata type definition (rather than name) should be allowed\n    full_doc = copy.deepcopy(ls5_telem_doc)\n    full_doc[\'metadata_type\'] = ga_metadata_type_doc\n    index.products.update_document(full_doc)\n\n    # Remove fixed field, forcing a new index to be created (as datasets can now differ for the field).\n    assert not _object_exists(index._db, \'dix_ls5_telem_test_product_type\')\n    del ls5_telem_doc[\'metadata\'][\'product_type\']\n    index.products.update_document(ls5_telem_doc)\n    # Ensure was updated\n    assert _object_exists(index._db, \'dix_ls5_telem_test_product_type\')\n    updated_type = index.products.get_by_name(ls5_telem_type.name)\n    assert updated_type.definition[\'metadata\'] == ls5_telem_doc[\'metadata\']\n\n    # But if we make metadata more restrictive we get an error:\n    different_telemetry_type = copy.deepcopy(ls5_telem_doc)\n    assert \'ga_label\' not in different_telemetry_type[\'metadata\']\n    different_telemetry_type[\'metadata\'][\'ga_label\'] = \'something\'\n    with pytest.raises(ValueError):\n        index.products.update_document(different_telemetry_type)\n    # Check was not updated.\n    updated_type = index.products.get_by_name(ls5_telem_type.name)\n    assert \'ga_label\' not in updated_type.definition[\'metadata\']\n\n    # But works when unsafe updates are allowed.\n    index.products.update_document(different_telemetry_type, allow_unsafe_updates=True)\n    updated_type = index.products.get_by_name(ls5_telem_type.name)\n    assert updated_type.definition[\'metadata\'][\'ga_label\'] == \'something\'\n\n\ndef test_product_update_cli(index: Index,\n                            clirunner,\n                            ls5_telem_type: DatasetType,\n                            ls5_telem_doc: dict,\n                            ga_metadata_type: MetadataType,\n                            tmpdir) -> None:\n    """"""\n    Test updating products via cli\n    """"""\n\n    def run_update_product(file_path, allow_unsafe=False):\n        if allow_unsafe:\n            allow_unsafe = [\'--allow-unsafe\']\n        else:\n            allow_unsafe = []\n\n        return clirunner(\n            [\n                \'product\', \'update\', str(file_path)\n            ] + allow_unsafe, catch_exceptions=False,\n            expect_success=False\n        )\n\n    def get_current(index, product_doc):\n        # It\'s calling out to a separate instance to update the product (through the cli),\n        # so we need to clear our local index object\'s cache to get the updated one.\n        index.products.get_by_name_unsafe.cache_clear()\n\n        return index.products.get_by_name(product_doc[\'name\']).definition\n\n    # Update an unchanged file, should be unchanged.\n    file_path = tmpdir.join(\'unmodified-product.yaml\')\n    file_path.write(_to_yaml(ls5_telem_doc))\n    result = run_update_product(file_path)\n    assert str(\'Updated ""ls5_telem_test""\') in result.output\n    assert get_current(index, ls5_telem_doc) == ls5_telem_doc\n    assert result.exit_code == 0\n\n    # Try to add an unknown property: this should be forbidden by validation of dataset-type-schema.yaml\n    modified_doc = copy.deepcopy(ls5_telem_doc)\n    modified_doc[\'newly_added_property\'] = {}\n    file_path = tmpdir.join(\'invalid-product.yaml\')\n    file_path.write(_to_yaml(modified_doc))\n    result = run_update_product(file_path)\n\n    # The error message differs between jsonschema versions, but should always mention the invalid property name.\n    assert ""newly_added_property"" in result.output\n    # Return error code for failure!\n    assert result.exit_code == 1\n    assert get_current(index, ls5_telem_doc) == ls5_telem_doc\n\n    # Use of a numeric key in the document\n    # (This has thrown errors in the past. all dict keys are strings after json conversion, but some old docs use\n    # numbers as keys in yaml)\n    modified_doc = copy.deepcopy(ls5_telem_doc)\n    modified_doc[\'metadata\'][42] = \'hello\'\n    file_path = tmpdir.join(\'unsafe-change-to-product.yaml\')\n    file_path.write(_to_yaml(modified_doc))\n    result = run_update_product(file_path)\n    assert ""Unsafe change in metadata.42 from missing to \'hello\'"" in result.output\n    # Return error code for failure!\n    assert result.exit_code == 1\n    # Unchanged\n    assert get_current(index, ls5_telem_doc) == ls5_telem_doc\n\n    # But if we set allow-unsafe==True, this one will work.\n    result = run_update_product(file_path, allow_unsafe=True)\n    assert ""Unsafe change in metadata.42 from missing to \'hello\'"" in result.output\n    assert result.exit_code == 0\n    # Has changed, and our key is now a string (json only allows string keys)\n    modified_doc = copy.deepcopy(ls5_telem_doc)\n    modified_doc[\'metadata\'][\'42\'] = \'hello\'\n    assert get_current(index, ls5_telem_doc) == modified_doc\n\n\ndef _to_yaml(ls5_telem_doc):\n    # Need to explicitly allow unicode in Py2\n    return yaml.safe_dump(ls5_telem_doc, allow_unicode=True)\n\n\ndef test_update_metadata_type(index, default_metadata_type):\n    """"""\n    :type default_metadata_type_docs: list[dict]\n    :type index: datacube.index._api.Index\n    """"""\n    mt_doc = [d for d in default_metadata_type_docs() if d[\'name\'] == default_metadata_type.name][0]\n\n    assert index.metadata_types.get_by_name(mt_doc[\'name\']) is not None\n\n    # Update with no changes should work.\n    index.metadata_types.update_document(mt_doc)\n\n    # Add search field\n    mt_doc[\'dataset\'][\'search_fields\'][\'testfield\'] = {\n        \'description\': ""Field added for testing"",\n        \'offset\': [\'test\']\n    }\n\n    # TODO: Able to remove fields?\n    # Indexes will be difficult to handle, as dropping them may affect other users. But leaving them there may\n    # lead to issues if a different field is created with the same name.\n\n    index.metadata_types.update_document(mt_doc)\n    # Ensure was updated\n    updated_type = index.metadata_types.get_by_name(mt_doc[\'name\'])\n    assert \'testfield\' in updated_type.dataset_fields\n\n    # But if we change an existing field type we get an error:\n    different_mt_doc = copy.deepcopy(mt_doc)\n    different_mt_doc[\'dataset\'][\'search_fields\'][\'time\'][\'type\'] = \'numeric-range\'\n    with pytest.raises(ValueError):\n        index.metadata_types.update_document(different_mt_doc)\n\n    # But works when unsafe updates are allowed.\n    index.metadata_types.update_document(different_mt_doc, allow_unsafe_updates=True)\n    updated_type = index.metadata_types.get_by_name(mt_doc[\'name\'])\n    assert isinstance(updated_type.dataset_fields[\'time\'], NumericRangeDocField)\n\n\ndef test_filter_types_by_fields(index, ls5_telem_type):\n    """"""\n    :type ls5_telem_type: datacube.model.DatasetType\n    :type index: datacube.index._api.Index\n    """"""\n    assert index.products\n    res = list(index.products.get_with_fields([\'sat_path\', \'sat_row\', \'platform\']))\n    assert res == [ls5_telem_type]\n\n    res = list(index.products.get_with_fields([\'sat_path\', \'sat_row\', \'platform\', \'favorite_icecream\']))\n    assert len(res) == 0\n\n\ndef test_filter_types_by_search(index, ls5_telem_type):\n    """"""\n    :type ls5_telem_type: datacube.model.DatasetType\n    :type index: datacube.index._api.Index\n    """"""\n    assert index.products\n\n    # No arguments, return all.\n    res = list(index.products.search())\n    assert res == [ls5_telem_type]\n\n    # Matching fields\n    res = list(index.products.search(\n        product_type=\'satellite_telemetry_data\',\n        product=\'ls5_telem_test\'\n    ))\n    assert res == [ls5_telem_type]\n\n    # Matching fields and non-available fields\n    res = list(index.products.search(\n        product_type=\'satellite_telemetry_data\',\n        product=\'ls5_telem_test\',\n        lat=Range(142.015625, 142.015625),\n        lon=Range(-12.046875, -12.046875)\n    ))\n    assert res == []\n\n    # Matching fields and available fields\n    [(res, q)] = list(index.products.search_robust(\n        product_type=\'satellite_telemetry_data\',\n        product=\'ls5_telem_test\',\n        sat_path=Range(142.015625, 142.015625),\n        sat_row=Range(-12.046875, -12.046875)\n    ))\n    assert res == ls5_telem_type\n    assert \'sat_path\' in q\n    assert \'sat_row\' in q\n\n    # Or expression test\n    res = list(index.products.search(\n        product_type=[\'satellite_telemetry_data\', \'nbar\'],\n    ))\n    assert res == [ls5_telem_type]\n\n    # Mismatching fields\n    res = list(index.products.search(\n        product_type=\'nbar\',\n    ))\n    assert res == []\n\n\ndef test_update_metadata_type_doc(initialised_postgres_db, index, ls5_telem_type):\n    type_doc = copy.deepcopy(ls5_telem_type.metadata_type.definition)\n    type_doc[\'dataset\'][\'search_fields\'][\'test_indexed\'] = {\n        \'description\': \'indexed test field\',\n        \'offset\': [\'test\', \'indexed\']\n    }\n    type_doc[\'dataset\'][\'search_fields\'][\'test_not_indexed\'] = {\n        \'description\': \'not indexed test field\',\n        \'offset\': [\'test\', \'not\', \'indexed\'],\n        \'indexed\': False\n    }\n\n    index.metadata_types.update_document(type_doc)\n\n    assert ls5_telem_type.name == \'ls5_telem_test\'\n    assert _object_exists(initialised_postgres_db, ""dix_ls5_telem_test_test_indexed"")\n    assert not _object_exists(initialised_postgres_db, ""dix_ls5_telem_test_test_not_indexed"")\n'"
integration_tests/index/test_index_data.py,0,"b'# coding=utf-8\n""""""\nTest database methods.\n\nIntegration tests: these depend on a local Postgres instance.\n""""""\nimport copy\nimport datetime\nimport sys\nfrom pathlib import Path\nfrom uuid import UUID\n\nimport pytest\nfrom dateutil import tz\n\nfrom datacube.drivers.postgres import PostgresDb\nfrom datacube.index.exceptions import MissingRecordError\nfrom datacube.index.index import Index\nfrom datacube.model import Dataset, MetadataType\n\n_telemetry_uuid = UUID(\'4ec8fe97-e8b9-11e4-87ff-1040f381a756\')\n_telemetry_dataset = {\n    \'product_type\': \'satellite_telemetry_data\',\n    \'checksum_path\': \'package.sha1\',\n    \'id\': str(_telemetry_uuid),\n    \'ga_label\': \'LS8_OLITIRS_STD-MD_P00_LC81160740742015089ASA00_\'\n                \'116_074_20150330T022553Z20150330T022657\',\n\n    \'ga_level\': \'P00\',\n    \'size_bytes\': 637660782,\n    \'platform\': {\n        \'code\': \'LANDSAT_8\'\n    },\n    # We\'re unlikely to have extent info for a raw dataset, we\'ll use it for search tests.\n    \'extent\': {\n        \'center_dt\': datetime.datetime(2014, 7, 26, 23, 49, 0, 343853).isoformat(),\n        \'coord\': {\n            \'ll\': {\'lat\': -31.33333, \'lon\': 149.78434},\n            \'lr\': {\'lat\': -31.37116, \'lon\': 152.20094},\n            \'ul\': {\'lat\': -29.23394, \'lon\': 149.85216},\n            \'ur\': {\'lat\': -29.26873, \'lon\': 152.21782}\n        }\n    },\n    \'creation_dt\': datetime.datetime(2015, 4, 22, 6, 32, 4).isoformat(),\n    \'instrument\': {\'name\': \'OLI_TIRS\'},\n    \'format\': {\n        \'name\': \'MD\'\n    },\n    \'lineage\': {\n        \'source_datasets\': {},\n        \'blah\': float(\'NaN\')\n    }\n}\n\n_pseudo_telemetry_dataset_type = {\n    \'name\': \'ls8_telemetry\',\n    \'description\': \'LS8 test\',\n    \'metadata\': {\n        \'product_type\': \'satellite_telemetry_data\',\n        \'platform\': {\n            \'code\': \'LANDSAT_8\'\n        },\n        \'format\': {\n            \'name\': \'MD\'\n        }\n    },\n    \'metadata_type\': \'eo\'\n}\n\n\ndef test_archive_datasets(index, initialised_postgres_db, local_config, default_metadata_type):\n    dataset_type = index.products.add_document(_pseudo_telemetry_dataset_type)\n    with initialised_postgres_db.begin() as transaction:\n        was_inserted = transaction.insert_dataset(\n            _telemetry_dataset,\n            _telemetry_uuid,\n            dataset_type.id\n        )\n\n    assert was_inserted\n    assert index.datasets.has(_telemetry_uuid)\n\n    datasets = index.datasets.search_eager()\n    assert len(datasets) == 1\n    assert datasets[0].is_active\n\n    index.datasets.archive([_telemetry_uuid])\n    datasets = index.datasets.search_eager()\n    assert len(datasets) == 0\n\n    # The model should show it as archived now.\n    indexed_dataset = index.datasets.get(_telemetry_uuid)\n    assert indexed_dataset.is_archived\n    assert not indexed_dataset.is_active\n\n    index.datasets.restore([_telemetry_uuid])\n    datasets = index.datasets.search_eager()\n    assert len(datasets) == 1\n\n    # And now active\n    indexed_dataset = index.datasets.get(_telemetry_uuid)\n    assert indexed_dataset.is_active\n    assert not indexed_dataset.is_archived\n\n\n@pytest.fixture\ndef telemetry_dataset(index: Index, initialised_postgres_db: PostgresDb, default_metadata_type) -> Dataset:\n    dataset_type = index.products.add_document(_pseudo_telemetry_dataset_type)\n    assert not index.datasets.has(_telemetry_uuid)\n\n    with initialised_postgres_db.begin() as transaction:\n        was_inserted = transaction.insert_dataset(\n            _telemetry_dataset,\n            _telemetry_uuid,\n            dataset_type.id\n        )\n    assert was_inserted\n\n    return index.datasets.get(_telemetry_uuid)\n\n\ndef test_index_duplicate_dataset(index: Index, initialised_postgres_db: PostgresDb,\n                                 local_config,\n                                 default_metadata_type) -> None:\n    dataset_type = index.products.add_document(_pseudo_telemetry_dataset_type)\n    assert not index.datasets.has(_telemetry_uuid)\n\n    with initialised_postgres_db.begin() as transaction:\n        was_inserted = transaction.insert_dataset(\n            _telemetry_dataset,\n            _telemetry_uuid,\n            dataset_type.id\n        )\n\n    assert was_inserted\n    assert index.datasets.has(_telemetry_uuid)\n\n    # Insert again.\n    with initialised_postgres_db.connect() as connection:\n        was_inserted = connection.insert_dataset(\n            _telemetry_dataset,\n            _telemetry_uuid,\n            dataset_type.id\n        )\n        assert was_inserted is False\n\n    assert index.datasets.has(_telemetry_uuid)\n\n\ndef test_has_dataset(index: Index, telemetry_dataset: Dataset) -> None:\n    assert index.datasets.has(_telemetry_uuid)\n    assert index.datasets.has(str(_telemetry_uuid))\n\n    assert not index.datasets.has(UUID(\'f226a278-e422-11e6-b501-185e0f80a5c0\'))\n    assert not index.datasets.has(\'f226a278-e422-11e6-b501-185e0f80a5c0\')\n\n    assert index.datasets.bulk_has([_telemetry_uuid, UUID(\'f226a278-e422-11e6-b501-185e0f80a5c0\')]) == [True, False]\n    assert index.datasets.bulk_has([str(_telemetry_uuid), \'f226a278-e422-11e6-b501-185e0f80a5c0\']) == [True, False]\n\n\ndef test_get_dataset(index: Index, telemetry_dataset: Dataset) -> None:\n    assert index.datasets.has(_telemetry_uuid)\n    assert index.datasets.has(str(_telemetry_uuid))\n\n    assert index.datasets.bulk_has([_telemetry_uuid, \'f226a278-e422-11e6-b501-185e0f80a5c0\']) == [True, False]\n\n    for tr in (lambda x: x, lambda x: str(x)):\n        ds = index.datasets.get(tr(_telemetry_uuid))\n        assert ds.id == _telemetry_uuid\n\n        ds, = index.datasets.bulk_get([tr(_telemetry_uuid)])\n        assert ds.id == _telemetry_uuid\n\n    assert index.datasets.bulk_get([\'f226a278-e422-11e6-b501-185e0f80a5c0\',\n                                    \'f226a278-e422-11e6-b501-185e0f80a5c1\']) == []\n\n\ndef test_transactions(index: Index,\n                      initialised_postgres_db: PostgresDb,\n                      local_config,\n                      default_metadata_type) -> None:\n    assert not index.datasets.has(_telemetry_uuid)\n\n    dataset_type = index.products.add_document(_pseudo_telemetry_dataset_type)\n    with initialised_postgres_db.begin() as transaction:\n        was_inserted = transaction.insert_dataset(\n            _telemetry_dataset,\n            _telemetry_uuid,\n            dataset_type.id\n        )\n        assert was_inserted\n        assert transaction.contains_dataset(_telemetry_uuid)\n        # Normal DB uses a separate connection: No dataset visible yet.\n        assert not index.datasets.has(_telemetry_uuid)\n\n        transaction.rollback()\n\n    # Should have been rolled back.\n    assert not index.datasets.has(_telemetry_uuid)\n\n\ndef test_get_missing_things(index: Index) -> None:\n    """"""\n    The get(id) methods should return None if the object doesn\'t exist.\n    """"""\n    uuid_ = UUID(\'18474b58-c8a6-11e6-a4b3-185e0f80a5c0\')\n    missing_thing = index.datasets.get(uuid_, include_sources=False)\n    assert missing_thing is None, ""get() should return none when it doesn\'t exist""\n\n    missing_thing = index.datasets.get(uuid_, include_sources=True)\n    assert missing_thing is None, ""get() should return none when it doesn\'t exist""\n\n    id_ = sys.maxsize\n    missing_thing = index.metadata_types.get(id_)\n    assert missing_thing is None, ""get() should return none when it doesn\'t exist""\n\n    missing_thing = index.products.get(id_)\n    assert missing_thing is None, ""get() should return none when it doesn\'t exist""\n\n\ndef test_index_dataset_with_sources(index, default_metadata_type):\n    type_ = index.products.add_document(_pseudo_telemetry_dataset_type)\n\n    parent_doc = _telemetry_dataset.copy()\n    parent = Dataset(type_, parent_doc, None, sources={})\n    child_doc = _telemetry_dataset.copy()\n    child_doc[\'lineage\'] = {\'source_datasets\': {\'source\': _telemetry_dataset}}\n    child_doc[\'id\'] = \'051a003f-5bba-43c7-b5f1-7f1da3ae9cfb\'\n    child = Dataset(type_, child_doc, sources={\'source\': parent})\n\n    with pytest.raises(MissingRecordError):\n        index.datasets.add(child, with_lineage=False)\n\n    index.datasets.add(child)\n    assert index.datasets.get(parent.id)\n    assert index.datasets.get(child.id)\n\n    assert len(index.datasets.bulk_get([parent.id, child.id])) == 2\n\n    index.datasets.add(child, with_lineage=False)\n    index.datasets.add(child, with_lineage=True)\n\n    parent_doc[\'platform\'] = {\'code\': \'LANDSAT_9\'}\n    index.datasets.add(child, with_lineage=True)\n    index.datasets.add(child, with_lineage=False)\n\n    # backwards compatibility code path checks, don\'t use this in normal code\n    for p in (\'skip\', \'ensure\', \'verify\'):\n        index.datasets.add(child, sources_policy=p)\n\n\n@pytest.mark.parametrize(\'datacube_env_name\', (\'datacube\', ), indirect=True)\ndef test_index_dataset_with_location(index: Index, default_metadata_type: MetadataType):\n    first_file = Path(\'/tmp/first/something.yaml\').absolute()\n    second_file = Path(\'/tmp/second/something.yaml\').absolute()\n\n    type_ = index.products.add_document(_pseudo_telemetry_dataset_type)\n    dataset = Dataset(type_, _telemetry_dataset, uris=[first_file.as_uri()], sources={})\n    index.datasets.add(dataset)\n    stored = index.datasets.get(dataset.id)\n\n    assert stored.id == _telemetry_uuid\n    # TODO: Dataset types?\n    assert stored.type.id == type_.id\n    assert stored.metadata_type.id == default_metadata_type.id\n    assert stored.local_path == Path(first_file)\n\n    # Ingesting again should have no effect.\n    index.datasets.add(dataset)\n    stored = index.datasets.get(dataset.id)\n    locations = index.datasets.get_locations(dataset.id)\n    assert len(locations) == 1\n    # Remove the location\n    was_removed = index.datasets.remove_location(dataset.id, first_file.as_uri())\n    assert was_removed\n    was_removed = index.datasets.remove_location(dataset.id, first_file.as_uri())\n    assert not was_removed\n    locations = index.datasets.get_locations(dataset.id)\n    assert len(locations) == 0\n    # Re-add the location\n    was_added = index.datasets.add_location(dataset.id, first_file.as_uri())\n    assert was_added\n    was_added = index.datasets.add_location(dataset.id, first_file.as_uri())\n    assert not was_added\n    locations = index.datasets.get_locations(dataset.id)\n    assert len(locations) == 1\n\n    # A rough date is ok: 1:01 beforehand just in case someone runs this during daylight savings time conversion :)\n    # (any UTC conversion errors will be off by much more than this for PST/AEST)\n    before_archival_dt = utc_now() - datetime.timedelta(hours=1, minutes=1)\n\n    was_archived = index.datasets.archive_location(dataset.id, first_file.as_uri())\n    assert was_archived\n    locations = index.datasets.get_locations(dataset.id)\n    assert locations == []\n    locations = index.datasets.get_archived_locations(dataset.id)\n    assert locations == [first_file.as_uri()]\n\n    # It should return the time archived.\n    location_times = index.datasets.get_archived_location_times(dataset.id)\n    assert len(location_times) == 1\n    location, archived_time = location_times[0]\n    assert location == first_file.as_uri()\n    assert utc_now() > archived_time > before_archival_dt\n\n    was_restored = index.datasets.restore_location(dataset.id, first_file.as_uri())\n    assert was_restored\n    locations = index.datasets.get_locations(dataset.id)\n    assert len(locations) == 1\n\n    # Indexing with a new path should NOT add the second one.\n    dataset.uris = [second_file.as_uri()]\n    index.datasets.add(dataset)\n    stored = index.datasets.get(dataset.id)\n    locations = index.datasets.get_locations(dataset.id)\n    assert len(locations) == 1\n\n    # Add location manually instead\n    index.datasets.add_location(dataset.id, second_file.as_uri())\n    stored = index.datasets.get(dataset.id)\n    assert len(stored.uris) == 2\n\n    # Newest to oldest.\n    assert stored.uris == [second_file.as_uri(), first_file.as_uri()]\n    # And the second one is newer, so it should be returned as the default local path:\n    assert stored.local_path == Path(second_file)\n\n    # Can archive and restore the first file, and location order is preserved\n    was_archived = index.datasets.archive_location(dataset.id, first_file.as_uri())\n    assert was_archived\n    locations = index.datasets.get_locations(dataset.id)\n    assert locations == [second_file.as_uri()]\n    was_restored = index.datasets.restore_location(dataset.id, first_file.as_uri())\n    assert was_restored\n    locations = index.datasets.get_locations(dataset.id)\n    assert locations == [second_file.as_uri(), first_file.as_uri()]\n\n    # Can archive and restore the second file, and location order is preserved\n    was_archived = index.datasets.archive_location(dataset.id, second_file.as_uri())\n    assert was_archived\n    locations = index.datasets.get_locations(dataset.id)\n    assert locations == [first_file.as_uri()]\n    was_restored = index.datasets.restore_location(dataset.id, second_file.as_uri())\n    assert was_restored\n    locations = index.datasets.get_locations(dataset.id)\n    assert locations == [second_file.as_uri(), first_file.as_uri()]\n\n    # Indexing again without location should have no effect.\n    dataset.uris = []\n    index.datasets.add(dataset)\n    stored = index.datasets.get(dataset.id)\n    locations = index.datasets.get_locations(dataset.id)\n    assert len(locations) == 2\n    # Newest to oldest.\n    assert locations == [second_file.as_uri(), first_file.as_uri()]\n    # And the second one is newer, so it should be returned as the default local path:\n    assert stored.local_path == Path(second_file)\n\n    # Check order of uris is preserved when indexing with more than one\n    second_ds_doc = copy.deepcopy(_telemetry_dataset)\n    second_ds_doc[\'id\'] = \'366f32d8-e1f8-11e6-94b4-185e0f80a589\'\n    index.datasets.add(Dataset(type_, second_ds_doc, uris=[\'file:///a\', \'file:///b\'], sources={}))\n\n    # test order using get_locations function\n    assert index.datasets.get_locations(second_ds_doc[\'id\']) == [\'file:///a\', \'file:///b\']\n\n    # test order using datasets.get(), it has custom query as it turns out\n    assert index.datasets.get(second_ds_doc[\'id\']).uris == [\'file:///a\', \'file:///b\']\n\n    # test update, this should prepend file:///c, file:///d to the existing list\n    index.datasets.update(Dataset(type_, second_ds_doc, uris=[\'file:///a\', \'file:///c\', \'file:///d\'], sources={}))\n    assert index.datasets.get_locations(second_ds_doc[\'id\']) == [\'file:///c\', \'file:///d\', \'file:///a\', \'file:///b\']\n    assert index.datasets.get(second_ds_doc[\'id\']).uris == [\'file:///c\', \'file:///d\', \'file:///a\', \'file:///b\']\n\n    # Ability to get datasets for a location\n    # Add a second dataset with a different location (to catch lack of joins, filtering etc)\n    second_ds_doc = copy.deepcopy(_telemetry_dataset)\n    second_ds_doc[\'id\'] = \'366f32d8-e1f8-11e6-94b4-185e0f80a5c0\'\n    index.datasets.add(Dataset(type_, second_ds_doc, uris=[second_file.as_uri()], sources={}))\n    for mode in (\'exact\', \'prefix\', None):\n        dataset_ids = [d.id for d in index.datasets.get_datasets_for_location(first_file.as_uri(), mode=mode)]\n        assert dataset_ids == [dataset.id]\n\n    assert list(index.datasets.get_datasets_for_location(first_file.as_uri() + ""#part=100"")) == []\n\n    with pytest.raises(ValueError):\n        list(index.datasets.get_datasets_for_location(first_file.as_uri(), mode=""nosuchmode""))\n\n\ndef utc_now():\n    # utcnow() doesn\'t include a tzinfo.\n    return datetime.datetime.utcnow().replace(tzinfo=tz.tzutc())\n'"
integration_tests/index/test_pluggable_indexes.py,0,"b""from configparser import ConfigParser\n\nfrom datacube.index.index import Index\n\n\ndef test_with_standard_index(uninitialised_postgres_db):\n    index = Index(uninitialised_postgres_db)\n    index.init_db()\n\n\ndef create_sample_config():\n    parser = ConfigParser()\n    parser.add_section('test_env')\n    parser.set('test_env', 'index_driver', 'default')\n\n\ndef test_system_init(uninitialised_postgres_db, clirunner):\n    result = clirunner(['system', 'init'], catch_exceptions=False)\n\n    # Question: Should the Index be able to be specified on the command line, or should it come from the config file?\n\n    if result.exit_code != 0:\n        print(result.output)\n        assert False\n"""
integration_tests/index/test_search.py,0,"b'# coding=utf-8\n""""""\nModule\n""""""\nimport copy\nimport csv\nimport datetime\nimport io\nimport uuid\nfrom decimal import Decimal\nfrom uuid import UUID\nfrom typing import List, Iterable, Dict, Any, Tuple\n\nimport pytest\nimport yaml\nfrom dateutil import tz\nfrom psycopg2._range import NumericRange\n\nimport datacube.scripts.cli_app\nimport datacube.scripts.search_tool\nfrom datacube.config import LocalConfig\nfrom datacube.drivers.postgres import PostgresDb\nfrom datacube.drivers.postgres._connections import DEFAULT_DB_USER\nfrom datacube.index.index import Index\nfrom datacube.model import Dataset\nfrom datacube.model import DatasetType\nfrom datacube.model import MetadataType\nfrom datacube.model import Range\n\nfrom datacube.testutils import load_dataset_definition\n\n\n@pytest.fixture\ndef pseudo_ls8_type(index, ga_metadata_type):\n    index.products.add_document({\n        \'name\': \'ls8_telemetry\',\n        \'description\': \'telemetry test\',\n        \'metadata\': {\n            \'product_type\': \'pseudo_ls8_data\',\n            \'platform\': {\n                \'code\': \'LANDSAT_8\'\n            },\n            \'instrument\': {\n                \'name\': \'OLI_TIRS\'\n            },\n            \'format\': {\n                \'name\': \'PSEUDOMD\'\n            }\n        },\n        \'metadata_type\': ga_metadata_type.name\n    })\n    return index.products.get_by_name(\'ls8_telemetry\')\n\n\n@pytest.fixture\ndef pseudo_ls8_dataset(index, initialised_postgres_db, pseudo_ls8_type):\n    id_ = str(uuid.uuid4())\n    with initialised_postgres_db.connect() as connection:\n        was_inserted = connection.insert_dataset(\n            {\n                \'id\': id_,\n                \'product_type\': \'pseudo_ls8_data\',\n                \'checksum_path\': \'package.sha1\',\n                \'ga_label\': \'LS8_OLITIRS_STD-MD_P00_LC81160740742015089ASA00_\'\n                            \'116_074_20150330T022553Z20150330T022657\',\n\n                \'ga_level\': \'P00\',\n                \'size_bytes\': 637660782,\n                \'platform\': {\n                    \'code\': \'LANDSAT_8\'\n                },\n                # We\'re unlikely to have extent info for a raw dataset, we\'ll use it for search tests.\n                \'extent\': {\n                    \'from_dt\': datetime.datetime(2014, 7, 26, 23, 48, 0, 343853),\n                    \'to_dt\': datetime.datetime(2014, 7, 26, 23, 52, 0, 343853),\n                    \'coord\': {\n                        \'ll\': {\'lat\': -31.33333, \'lon\': 149.78434},\n                        \'lr\': {\'lat\': -31.37116, \'lon\': 152.20094},\n                        \'ul\': {\'lat\': -29.23394, \'lon\': 149.85216},\n                        \'ur\': {\'lat\': -29.26873, \'lon\': 152.21782}\n                    }\n                },\n                \'image\': {\n                    \'satellite_ref_point_start\': {\'x\': 116, \'y\': 74},\n                    \'satellite_ref_point_end\': {\'x\': 116, \'y\': 84},\n                },\n                \'creation_dt\': datetime.datetime(2015, 4, 22, 6, 32, 4),\n                \'instrument\': {\'name\': \'OLI_TIRS\'},\n                \'format\': {\n                    \'name\': \'PSEUDOMD\'\n                },\n                \'lineage\': {\n                    \'source_datasets\': {}\n                }\n            },\n            id_,\n            pseudo_ls8_type.id\n        )\n    assert was_inserted\n    d = index.datasets.get(id_)\n    # The dataset should have been matched to the telemetry type.\n    assert d.type.id == pseudo_ls8_type.id\n\n    return d\n\n\n@pytest.fixture\ndef pseudo_ls8_dataset2(index, initialised_postgres_db, pseudo_ls8_type):\n    # Like the previous dataset, but a day later in time.\n    id_ = str(uuid.uuid4())\n    with initialised_postgres_db.connect() as connection:\n        was_inserted = connection.insert_dataset(\n            {\n                \'id\': id_,\n                \'product_type\': \'pseudo_ls8_data\',\n                \'checksum_path\': \'package.sha1\',\n                \'ga_label\': \'LS8_OLITIRS_STD-MD_P00_LC81160740742015089ASA00_\'\n                            \'116_074_20150330T022553Z20150330T022657\',\n\n                \'ga_level\': \'P00\',\n                \'size_bytes\': 637660782,\n                \'platform\': {\n                    \'code\': \'LANDSAT_8\'\n                },\n                \'image\': {\n                    \'satellite_ref_point_start\': {\'x\': 116, \'y\': 74},\n                    \'satellite_ref_point_end\': {\'x\': 116, \'y\': 84},\n                },\n                # We\'re unlikely to have extent info for a raw dataset, we\'ll use it for search tests.\n                \'extent\': {\n                    \'from_dt\': datetime.datetime(2014, 7, 27, 23, 48, 0, 343853),\n                    \'to_dt\': datetime.datetime(2014, 7, 27, 23, 52, 0, 343853),\n                    \'coord\': {\n                        \'ll\': {\'lat\': -31.33333, \'lon\': 149.78434},\n                        \'lr\': {\'lat\': -31.37116, \'lon\': 152.20094},\n                        \'ul\': {\'lat\': -29.23394, \'lon\': 149.85216},\n                        \'ur\': {\'lat\': -29.26873, \'lon\': 152.21782}\n                    }\n                },\n                \'creation_dt\': datetime.datetime(2015, 4, 22, 6, 32, 4),\n                \'instrument\': {\'name\': \'OLI_TIRS\'},\n                \'format\': {\n                    \'name\': \'PSEUDOMD\'\n                },\n                \'lineage\': {\n                    \'source_datasets\': {}\n                }\n            },\n            id_,\n            pseudo_ls8_type.id\n        )\n    assert was_inserted\n    d = index.datasets.get(id_)\n    # The dataset should have been matched to the telemetry type.\n    assert d.type.id == pseudo_ls8_type.id\n\n    return d\n\n\n# Datasets 3 and 4 mirror 1 and 2 but have a different path/row.\n@pytest.fixture\ndef pseudo_ls8_dataset3(index: Index,\n                        initialised_postgres_db: PostgresDb,\n                        pseudo_ls8_type: DatasetType,\n                        pseudo_ls8_dataset: Dataset) -> Dataset:\n    # Same as 1, but a different path/row\n    id_ = str(uuid.uuid4())\n    dataset_doc = copy.deepcopy(pseudo_ls8_dataset.metadata_doc)\n    dataset_doc[\'id\'] = id_\n    dataset_doc[\'image\'] = {\n        \'satellite_ref_point_start\': {\'x\': 116, \'y\': 85},\n        \'satellite_ref_point_end\': {\'x\': 116, \'y\': 87},\n    }\n\n    with initialised_postgres_db.connect() as connection:\n        was_inserted = connection.insert_dataset(\n            dataset_doc,\n            id_,\n            pseudo_ls8_type.id\n        )\n    assert was_inserted\n    d = index.datasets.get(id_)\n    # The dataset should have been matched to the telemetry type.\n    assert d.type.id == pseudo_ls8_type.id\n    return d\n\n\n@pytest.fixture\ndef pseudo_ls8_dataset4(index: Index,\n                        initialised_postgres_db: PostgresDb,\n                        pseudo_ls8_type: DatasetType,\n                        pseudo_ls8_dataset2: Dataset) -> Dataset:\n    # Same as 2, but a different path/row\n    id_ = str(uuid.uuid4())\n    dataset_doc = copy.deepcopy(pseudo_ls8_dataset2.metadata_doc)\n    dataset_doc[\'id\'] = id_\n    dataset_doc[\'image\'] = {\n        \'satellite_ref_point_start\': {\'x\': 116, \'y\': 85},\n        \'satellite_ref_point_end\': {\'x\': 116, \'y\': 87},\n    }\n\n    with initialised_postgres_db.connect() as connection:\n        was_inserted = connection.insert_dataset(\n            dataset_doc,\n            id_,\n            pseudo_ls8_type.id\n        )\n        assert was_inserted\n        d = index.datasets.get(id_)\n        # The dataset should have been matched to the telemetry type.\n        assert d.type.id == pseudo_ls8_type.id\n        return d\n\n\n@pytest.fixture\ndef ls5_dataset_w_children(index, clirunner, example_ls5_dataset_path, indexed_ls5_scene_products):\n    clirunner([\'dataset\', \'add\', str(example_ls5_dataset_path)])\n    doc = load_dataset_definition(example_ls5_dataset_path)\n    return index.datasets.get(doc.id, include_sources=True)\n\n\n@pytest.fixture\ndef ls5_dataset_nbar_type(ls5_dataset_w_children: Dataset,\n                          indexed_ls5_scene_products: List[DatasetType]) -> DatasetType:\n    for dataset_type in indexed_ls5_scene_products:\n        if dataset_type.name == ls5_dataset_w_children.type.name:\n            return dataset_type\n    else:\n        raise RuntimeError(""LS5 type was not among types"")\n\n\ndef test_search_dataset_equals(index: Index, pseudo_ls8_dataset: Dataset):\n    datasets = index.datasets.search_eager(\n        platform=\'LANDSAT_8\'\n    )\n    assert len(datasets) == 1\n    assert datasets[0].id == pseudo_ls8_dataset.id\n\n    datasets = index.datasets.search_eager(\n        platform=\'LANDSAT_8\',\n        instrument=\'OLI_TIRS\'\n    )\n    assert len(datasets) == 1\n    assert datasets[0].id == pseudo_ls8_dataset.id\n\n    # Wrong sensor name\n    with pytest.raises(ValueError):\n        datasets = index.datasets.search_eager(\n            platform=\'LANDSAT-8\',\n            instrument=\'TM\',\n        )\n\n\ndef test_search_dataset_by_metadata(index: Index, pseudo_ls8_dataset: Dataset) -> None:\n    datasets = index.datasets.search_by_metadata(\n        {""platform"": {""code"": ""LANDSAT_8""}, ""instrument"": {""name"": ""OLI_TIRS""}}\n    )\n    datasets = list(datasets)\n    assert len(datasets) == 1\n    assert datasets[0].id == pseudo_ls8_dataset.id\n\n    datasets = index.datasets.search_by_metadata(\n        {""platform"": {""code"": ""LANDSAT_5""}, ""instrument"": {""name"": ""TM""}}\n    )\n    datasets = list(datasets)\n    assert len(datasets) == 0\n\n\ndef test_search_day(index: Index, pseudo_ls8_dataset: Dataset) -> None:\n    # Matches day\n    datasets = index.datasets.search_eager(\n        time=datetime.date(2014, 7, 26)\n    )\n    assert len(datasets) == 1\n    assert datasets[0].id == pseudo_ls8_dataset.id\n\n    # Different day: no match\n    datasets = index.datasets.search_eager(\n        time=datetime.date(2014, 7, 27)\n    )\n    assert len(datasets) == 0\n\n\ndef test_search_dataset_ranges(index: Index, pseudo_ls8_dataset: Dataset) -> None:\n    # In the lat bounds.\n    datasets = index.datasets.search_eager(\n        lat=Range(-30.5, -29.5),\n        time=Range(\n            datetime.datetime(2014, 7, 26, 23, 0, 0),\n            datetime.datetime(2014, 7, 26, 23, 59, 0)\n        )\n    )\n    assert len(datasets) == 1\n    assert datasets[0].id == pseudo_ls8_dataset.id\n\n    # Out of the lat bounds.\n    datasets = index.datasets.search_eager(\n        lat=Range(28, 32),\n        time=Range(\n            datetime.datetime(2014, 7, 26, 23, 48, 0),\n            datetime.datetime(2014, 7, 26, 23, 50, 0)\n        )\n    )\n    assert len(datasets) == 0\n\n    # Out of the time bounds\n    datasets = index.datasets.search_eager(\n        lat=Range(-30.5, -29.5),\n        time=Range(\n            datetime.datetime(2014, 7, 26, 21, 48, 0),\n            datetime.datetime(2014, 7, 26, 21, 50, 0)\n        )\n    )\n    assert len(datasets) == 0\n\n    # A dataset that overlaps but is not fully contained by the search bounds.\n    # TODO: Do we want overlap as the default behaviour?\n    # Should we distinguish between \'contains\' and \'overlaps\'?\n    datasets = index.datasets.search_eager(\n        lat=Range(-40, -30)\n    )\n    assert len(datasets) == 1\n    assert datasets[0].id == pseudo_ls8_dataset.id\n\n    # Single point search\n    datasets = index.datasets.search_eager(\n        lat=-30.0,\n        time=Range(\n            datetime.datetime(2014, 7, 26, 23, 0, 0),\n            datetime.datetime(2014, 7, 26, 23, 59, 0)\n        )\n    )\n    assert len(datasets) == 1\n    assert datasets[0].id == pseudo_ls8_dataset.id\n\n    datasets = index.datasets.search_eager(\n        lat=30.0,\n        time=Range(\n            datetime.datetime(2014, 7, 26, 23, 0, 0),\n            datetime.datetime(2014, 7, 26, 23, 59, 0)\n        )\n    )\n    assert len(datasets) == 0\n\n    # Single timestamp search\n    datasets = index.datasets.search_eager(\n        lat=Range(-30.5, -29.5),\n        time=datetime.datetime(2014, 7, 26, 23, 50, 0)\n    )\n    assert len(datasets) == 1\n    assert datasets[0].id == pseudo_ls8_dataset.id\n\n    datasets = index.datasets.search_eager(\n        lat=Range(-30.5, -29.5),\n        time=datetime.datetime(2014, 7, 26, 23, 30, 0)\n    )\n    assert len(datasets) == 0\n\n\ndef test_search_globally(index: Index, pseudo_ls8_dataset: Dataset) -> None:\n    # Insert dataset. It should be matched to the telemetry collection.\n    # No expressions means get all.\n    results = list(index.datasets.search())\n    assert len(results) == 1\n\n    # Dataset sources aren\'t loaded by default\n    assert results[0].sources is None\n\n\ndef _load_product_query(\n        lazy_results: Iterable[Tuple[DatasetType, Iterable[Dataset]]]\n) -> Dict[str, List[Dataset]]:\n    """"""\n    search_by_product() returns two levels of laziness. load them all into memory\n    for easy comparison/counts\n    """"""\n    products = {}  # type: Dict[str, List[Dataset]]\n    for product, datasets in lazy_results:\n        assert product.name not in products, ""search_by_product() returned a product twice""\n        products[product.name] = list(datasets)\n    return products\n\n\ndef test_search_by_product(index: Index,\n                           pseudo_ls8_type: DatasetType,\n                           pseudo_ls8_dataset: Dataset,\n                           indexed_ls5_scene_products,\n                           ls5_dataset_w_children: Dataset) -> None:\n    # Query all the test data, the counts should match expected\n    results = _load_product_query(index.datasets.search_by_product())\n    assert len(results) == 7\n    dataset_count = sum(len(ds) for ds in results.values())\n    assert dataset_count == 4\n\n    # Query one product\n    products = _load_product_query(index.datasets.search_by_product(\n        platform=\'LANDSAT_8\',\n        instrument=\'OLI_TIRS\',\n    ))\n    assert len(products) == 1\n    [dataset] = products[pseudo_ls8_type.name]\n    assert dataset.id == pseudo_ls8_dataset.id\n\n\ndef test_search_limit(index, pseudo_ls8_dataset, pseudo_ls8_dataset2):\n    datasets = list(index.datasets.search())\n    assert len(datasets) == 2\n    datasets = list(index.datasets.search(limit=1))\n    assert len(datasets) == 1\n    datasets = list(index.datasets.search(limit=0))\n    assert len(datasets) == 0\n    datasets = list(index.datasets.search(limit=5))\n    assert len(datasets) == 2\n\n    datasets = list(index.datasets.search_returning((\'id\',)))\n    assert len(datasets) == 2\n    datasets = list(index.datasets.search_returning((\'id\',), limit=1))\n    assert len(datasets) == 1\n    datasets = list(index.datasets.search_returning((\'id\',), limit=0))\n    assert len(datasets) == 0\n    datasets = list(index.datasets.search_returning((\'id\',), limit=5))\n    assert len(datasets) == 2\n\n\ndef test_search_or_expressions(index: Index,\n                               pseudo_ls8_type: DatasetType,\n                               pseudo_ls8_dataset: Dataset,\n                               ls5_dataset_nbar_type: DatasetType,\n                               ls5_dataset_w_children: Dataset,\n                               default_metadata_type: MetadataType,\n                               telemetry_metadata_type: MetadataType) -> None:\n    # Four datasets:\n    # Our standard LS8\n    # - type=ls8_telemetry\n    # LS5 with children:\n    # - type=ls5_nbar_scene\n    # - type=ls5_level1_scene\n    # - type=ls5_satellite_telemetry_data\n\n    all_datasets = index.datasets.search_eager()\n    assert len(all_datasets) == 4\n    all_ids = set(dataset.id for dataset in all_datasets)\n\n    # OR all platforms: should return all datasets\n    datasets = index.datasets.search_eager(\n        platform=[\'LANDSAT_5\', \'LANDSAT_7\', \'LANDSAT_8\']\n    )\n    assert len(datasets) == 4\n    ids = set(dataset.id for dataset in datasets)\n    assert ids == all_ids\n\n    # OR expression with only one clause.\n    datasets = index.datasets.search_eager(\n        platform=[\'LANDSAT_8\']\n    )\n    assert len(datasets) == 1\n    assert datasets[0].id == pseudo_ls8_dataset.id\n\n    # OR two products: return two\n    datasets = index.datasets.search_eager(\n        product=[pseudo_ls8_type.name, ls5_dataset_nbar_type.name]\n    )\n    assert len(datasets) == 2\n    ids = set(dataset.id for dataset in datasets)\n    assert ids == {pseudo_ls8_dataset.id, ls5_dataset_w_children.id}\n\n    # eo OR telemetry: return all\n    datasets = index.datasets.search_eager(\n        metadata_type=[\n            # LS5 + children\n            default_metadata_type.name,\n            # Nothing\n            telemetry_metadata_type.name,\n            # LS8 dataset\n            pseudo_ls8_type.metadata_type.name\n        ]\n    )\n    assert len(datasets) == 4\n    ids = set(dataset.id for dataset in datasets)\n    assert ids == all_ids\n\n    # Redundant ORs should have no effect.\n    datasets = index.datasets.search_eager(\n        product=[pseudo_ls8_type.name, pseudo_ls8_type.name, pseudo_ls8_type.name]\n    )\n    assert len(datasets) == 1\n    assert datasets[0].id == pseudo_ls8_dataset.id\n\n\ndef test_search_returning(index: Index,\n                          local_config: LocalConfig,\n                          pseudo_ls8_type: DatasetType,\n                          pseudo_ls8_dataset: Dataset,\n                          ls5_dataset_w_children) -> None:\n\n    assert index.datasets.count() == 4, ""Expected four test datasets""\n\n    # Expect one product with our one dataset.\n    results = list(index.datasets.search_returning(\n        (\'id\', \'sat_path\', \'sat_row\'),\n        platform=\'LANDSAT_8\',\n        instrument=\'OLI_TIRS\',\n    ))\n    assert len(results) == 1\n    id_, path_range, sat_range = results[0]\n    assert id_ == pseudo_ls8_dataset.id\n    # TODO: output nicer types?\n    assert path_range == NumericRange(Decimal(\'116\'), Decimal(\'116\'), \'[]\')\n    assert sat_range == NumericRange(Decimal(\'74\'), Decimal(\'84\'), \'[]\')\n\n    results = list(index.datasets.search_returning(\n        (\'id\', \'metadata_doc\',),\n        platform=\'LANDSAT_8\',\n        instrument=\'OLI_TIRS\',\n    ))\n    assert len(results) == 1\n    id_, document = results[0]\n    assert id_ == pseudo_ls8_dataset.id\n    assert document == pseudo_ls8_dataset.metadata_doc\n\n    my_username = local_config.get(\'db_username\', DEFAULT_DB_USER)\n\n    # Mixture of document and native fields\n    results = list(index.datasets.search_returning(\n        (\'id\', \'creation_time\', \'format\', \'label\'),\n        platform=\'LANDSAT_8\',\n        indexed_by=my_username,\n    ))\n    assert len(results) == 1\n\n    id_, creation_time, format_, label = results[0]\n\n    assert id_ == pseudo_ls8_dataset.id\n    assert format_ == \'PSEUDOMD\'\n\n    # It\'s always UTC in the document\n    expected_time = creation_time.astimezone(tz.tzutc()).replace(tzinfo=None)\n    assert expected_time.isoformat() == pseudo_ls8_dataset.metadata_doc[\'creation_dt\']\n    assert label == pseudo_ls8_dataset.metadata_doc[\'ga_label\']\n\n\ndef test_search_returning_rows(index, pseudo_ls8_type,\n                               pseudo_ls8_dataset, pseudo_ls8_dataset2,\n                               indexed_ls5_scene_products):\n    dataset = pseudo_ls8_dataset\n\n    # If returning a field like uri, there will be one result per location.\n\n    # No locations\n    results = list(index.datasets.search_returning(\n        (\'id\', \'uri\'),\n        platform=\'LANDSAT_8\',\n        instrument=\'OLI_TIRS\',\n    ))\n    assert len(results) == 0\n\n    # Add a location to the dataset and we should get one result\n    test_uri = \'file:///tmp/test1\'\n    index.datasets.add_location(dataset.id, test_uri)\n    results = list(index.datasets.search_returning(\n        (\'id\', \'uri\'),\n        platform=\'LANDSAT_8\',\n        instrument=\'OLI_TIRS\',\n    ))\n    assert len(results) == 1\n    assert results == [(dataset.id, test_uri)]\n\n    # Add a second location and we should get two results\n    test_uri2 = \'file:///tmp/test2\'\n    index.datasets.add_location(dataset.id, test_uri2)\n    results = set(index.datasets.search_returning(\n        (\'id\', \'uri\'),\n        platform=\'LANDSAT_8\',\n        instrument=\'OLI_TIRS\',\n    ))\n    assert len(results) == 2\n    assert results == {\n        (dataset.id, test_uri),\n        (dataset.id, test_uri2)\n    }\n\n    # A second dataset now has a location too:\n    test_uri3 = \'mdss://c10/tmp/something\'\n    index.datasets.add_location(pseudo_ls8_dataset2.id, test_uri3)\n    # Datasets and locations should still correctly match up...\n    results = set(index.datasets.search_returning(\n        (\'id\', \'uri\'),\n        platform=\'LANDSAT_8\',\n        instrument=\'OLI_TIRS\',\n    ))\n    assert len(results) == 3\n    assert results == {\n        (dataset.id, test_uri),\n        (dataset.id, test_uri2),\n        (pseudo_ls8_dataset2.id, test_uri3),\n    }\n\n\ndef test_searches_only_type(index: Index,\n                            pseudo_ls8_type: DatasetType,\n                            pseudo_ls8_dataset: Dataset,\n                            ls5_telem_type) -> None:\n    # The dataset should have been matched to the telemetry type.\n    assert pseudo_ls8_dataset.type.id == pseudo_ls8_type.id\n    assert index.datasets.search_eager()\n\n    # One result in the telemetry type\n    datasets = index.datasets.search_eager(\n        product=pseudo_ls8_type.name,\n        platform=\'LANDSAT_8\',\n        instrument=\'OLI_TIRS\',\n    )\n    assert len(datasets) == 1\n    assert datasets[0].id == pseudo_ls8_dataset.id\n\n    # One result in the metadata type\n    datasets = index.datasets.search_eager(\n        metadata_type=pseudo_ls8_type.metadata_type.name,\n        platform=\'LANDSAT_8\',\n        instrument=\'OLI_TIRS\',\n    )\n    assert len(datasets) == 1\n    assert datasets[0].id == pseudo_ls8_dataset.id\n\n    # No results when searching for a different dataset type.\n    with pytest.raises(ValueError):\n        datasets = index.datasets.search_eager(\n            product=ls5_telem_type.name,\n            platform=\'LANDSAT_8\',\n            instrument=\'OLI_TIRS\'\n        )\n\n    # One result when no types specified.\n    datasets = index.datasets.search_eager(\n        platform=\'LANDSAT_8\',\n        instrument=\'OLI_TIRS\'\n    )\n    assert len(datasets) == 1\n    assert datasets[0].id == pseudo_ls8_dataset.id\n\n    # No results for different metadata type.\n    with pytest.raises(ValueError):\n        datasets = index.datasets.search_eager(\n            metadata_type=\'telemetry\',\n            platform=\'LANDSAT_8\',\n            instrument=\'OLI_TIRS\'\n        )\n\n\ndef test_search_special_fields(index: Index,\n                               pseudo_ls8_type: DatasetType,\n                               pseudo_ls8_dataset: Dataset,\n                               ls5_dataset_w_children) -> None:\n    # \'product\' is a special case\n    datasets = index.datasets.search_eager(\n        product=pseudo_ls8_type.name\n    )\n    assert len(datasets) == 1\n    assert datasets[0].id == pseudo_ls8_dataset.id\n\n    # Unknown field: no results\n    with pytest.raises(ValueError):\n        datasets = index.datasets.search_eager(\n            platform=\'LANDSAT_8\',\n            flavour=\'chocolate\',\n        )\n\n\ndef test_search_by_uri(index, ls5_dataset_w_children):\n    datasets = index.datasets.search_eager(product=ls5_dataset_w_children.type.name,\n                                           uri=ls5_dataset_w_children.local_uri)\n    assert len(datasets) == 1\n\n    datasets = index.datasets.search_eager(product=ls5_dataset_w_children.type.name,\n                                           uri=\'file:///x/yz\')\n    assert len(datasets) == 0\n\n\ndef test_search_conflicting_types(index, pseudo_ls8_dataset, pseudo_ls8_type):\n    # Should return no results.\n    with pytest.raises(ValueError):\n        index.datasets.search_eager(\n            product=pseudo_ls8_type.name,\n            # The telemetry type is not of type storage_unit.\n            metadata_type=\'storage_unit\'\n        )\n\n\ndef test_fetch_all_of_md_type(index: Index, pseudo_ls8_dataset: Dataset) -> None:\n    # Get every dataset of the md type.\n    assert pseudo_ls8_dataset.metadata_type is not None  # to shut up mypy\n    results = index.datasets.search_eager(\n        metadata_type=pseudo_ls8_dataset.metadata_type.name\n    )\n    assert len(results) == 1\n    assert results[0].id == pseudo_ls8_dataset.id\n    # Get every dataset of the type.\n    results = index.datasets.search_eager(\n        product=pseudo_ls8_dataset.type.name\n    )\n    assert len(results) == 1\n    assert results[0].id == pseudo_ls8_dataset.id\n\n    # No results for another.\n    with pytest.raises(ValueError):\n        results = index.datasets.search_eager(\n            metadata_type=\'telemetry\'\n        )\n\n\ndef test_count_searches(index: Index,\n                        pseudo_ls8_type: DatasetType,\n                        pseudo_ls8_dataset: Dataset,\n                        ls5_telem_type) -> None:\n    # The dataset should have been matched to the telemetry type.\n    assert pseudo_ls8_dataset.type.id == pseudo_ls8_type.id\n    assert index.datasets.search_eager()\n\n    # One result in the telemetry type\n    datasets = index.datasets.count(\n        product=pseudo_ls8_type.name,\n        platform=\'LANDSAT_8\',\n        instrument=\'OLI_TIRS\',\n    )\n    assert datasets == 1\n\n    # One result in the metadata type\n    datasets = index.datasets.count(\n        metadata_type=pseudo_ls8_type.metadata_type.name,\n        platform=\'LANDSAT_8\',\n        instrument=\'OLI_TIRS\',\n    )\n    assert datasets == 1\n\n    # No results when searching for a different dataset type.\n    datasets = index.datasets.count(\n        product=ls5_telem_type.name,\n        platform=\'LANDSAT_8\',\n        instrument=\'OLI_TIRS\'\n    )\n    assert datasets == 0\n\n    # One result when no types specified.\n    datasets = index.datasets.count(\n        platform=\'LANDSAT_8\',\n        instrument=\'OLI_TIRS\',\n    )\n    assert datasets == 1\n\n    # No results for different metadata type.\n    datasets = index.datasets.count(\n        metadata_type=\'telemetry\',\n        platform=\'LANDSAT_8\',\n        instrument=\'OLI_TIRS\'\n    )\n    assert datasets == 0\n\n\ndef test_get_dataset_with_children(index: Index, ls5_dataset_w_children: Dataset) -> None:\n    id_ = ls5_dataset_w_children.id\n    assert isinstance(id_, UUID)\n\n    # Sources not loaded by default\n    d = index.datasets.get(id_)\n    assert d.sources is None\n\n    # Ask for all sources\n    d = index.datasets.get(id_, include_sources=True)\n    assert list(d.sources.keys()) == [\'level1\']\n    level1 = d.sources[\'level1\']\n    assert list(level1.sources.keys()) == [\'satellite_telemetry_data\']\n    assert list(level1.sources[\'satellite_telemetry_data\'].sources) == []\n\n    # It should also work with a string id\n    d = index.datasets.get(str(id_), include_sources=True)\n    assert list(d.sources.keys()) == [\'level1\']\n    level1 = d.sources[\'level1\']\n    assert list(level1.sources.keys()) == [\'satellite_telemetry_data\']\n    assert list(level1.sources[\'satellite_telemetry_data\'].sources) == []\n\n\ndef test_count_by_product_searches(index: Index,\n                                   pseudo_ls8_type: DatasetType,\n                                   pseudo_ls8_dataset: Dataset,\n                                   ls5_telem_type: DatasetType) -> None:\n    # The dataset should have been matched to the telemetry type.\n    assert pseudo_ls8_dataset.type.id == pseudo_ls8_type.id\n    assert index.datasets.search_eager()\n\n    # One result in the telemetry type\n    products = tuple(index.datasets.count_by_product(\n        product=pseudo_ls8_type.name,\n        platform=\'LANDSAT_8\',\n        instrument=\'OLI_TIRS\',\n    ))\n    assert products == ((pseudo_ls8_type, 1),)\n\n    # One result in the metadata type\n    products = tuple(index.datasets.count_by_product(\n        metadata_type=pseudo_ls8_type.metadata_type.name,\n        platform=\'LANDSAT_8\',\n        instrument=\'OLI_TIRS\',\n    ))\n    assert products == ((pseudo_ls8_type, 1),)\n\n    # No results when searching for a different dataset type.\n    products = tuple(index.datasets.count_by_product(\n        product=ls5_telem_type.name,\n        platform=\'LANDSAT_8\',\n        instrument=\'OLI_TIRS\'\n    ))\n    assert products == ()\n\n    # One result when no types specified.\n    products = tuple(index.datasets.count_by_product(\n        platform=\'LANDSAT_8\',\n        instrument=\'OLI_TIRS\',\n    ))\n    assert products == ((pseudo_ls8_type, 1),)\n\n    # Only types with datasets should be returned (these params match ls5_gtiff too)\n    products = tuple(index.datasets.count_by_product())\n    assert products == ((pseudo_ls8_type, 1),)\n\n    # No results for different metadata type.\n    products = tuple(index.datasets.count_by_product(\n        metadata_type=\'telemetry\',\n    ))\n    assert products == ()\n\n\ndef test_count_time_groups(index: Index,\n                           pseudo_ls8_type: DatasetType,\n                           pseudo_ls8_dataset: Dataset) -> None:\n    # \'from_dt\': datetime.datetime(2014, 7, 26, 23, 48, 0, 343853),\n    # \'to_dt\': datetime.datetime(2014, 7, 26, 23, 52, 0, 343853),\n    timeline = list(index.datasets.count_product_through_time(\n        \'1 day\',\n        product=pseudo_ls8_type.name,\n        time=Range(\n            datetime.datetime(2014, 7, 25, tzinfo=tz.tzutc()),\n            datetime.datetime(2014, 7, 27, tzinfo=tz.tzutc())\n        )\n    ))\n\n    assert len(timeline) == 2\n    assert timeline == [\n        (\n            Range(datetime.datetime(2014, 7, 25, tzinfo=tz.tzutc()),\n                  datetime.datetime(2014, 7, 26, tzinfo=tz.tzutc())),\n            0\n        ),\n        (\n            Range(datetime.datetime(2014, 7, 26, tzinfo=tz.tzutc()),\n                  datetime.datetime(2014, 7, 27, tzinfo=tz.tzutc())),\n            1\n        )\n    ]\n\n\n@pytest.mark.usefixtures(\'ga_metadata_type\',\n                         \'indexed_ls5_scene_products\')\ndef test_source_filter(clirunner, index, example_ls5_dataset_path):\n    clirunner(\n        [\n            \'dataset\',\n            \'add\',\n            str(example_ls5_dataset_path)\n        ]\n    )\n\n    all_nbar = index.datasets.search_eager(product=\'ls5_nbar_scene\')\n    assert len(all_nbar) == 1\n    all_level1 = index.datasets.search_eager(product=\'ls5_level1_scene\')\n    assert len(all_level1) == 1\n    assert all_level1[0].metadata.gsi == \'ASA\'\n\n    dss = index.datasets.search_eager(\n        product=\'ls5_nbar_scene\',\n        source_filter={\'product\': \'ls5_level1_scene\', \'gsi\': \'ASA\'}\n    )\n    assert dss == all_nbar\n    dss = index.datasets.search_eager(\n        product=\'ls5_nbar_scene\',\n        source_filter={\'product\': \'ls5_level1_scene\', \'gsi\': \'GREG\'}\n    )\n    assert dss == []\n\n    with pytest.raises(RuntimeError):\n        dss = index.datasets.search_eager(\n            product=\'ls5_nbar_scene\',\n            source_filter={\'gsi\': \'ASA\'}\n        )\n\n\ndef test_count_time_groups_cli(clirunner: Any,\n                               pseudo_ls8_type: DatasetType,\n                               pseudo_ls8_dataset: Dataset) -> None:\n    result = clirunner(\n        [\n            \'product-counts\',\n            \'1 day\',\n            \'time in [2014-07-25, 2014-07-27]\'\n        ], cli_method=datacube.scripts.search_tool.cli,\n        verbose_flag=\'\'\n    )\n\n    expected_out = (\n        \'{}\\n\'\n        \'    2014-07-25: 0\\n\'\n        \'    2014-07-26: 1\\n\'\n    ).format(pseudo_ls8_type.name)\n\n    assert result.output == expected_out\n\n\ndef test_search_cli_basic(clirunner: Any,\n                          telemetry_metadata_type: MetadataType,\n                          pseudo_ls8_dataset: Dataset) -> None:\n    """"""\n    Search datasets using the cli.\n    """"""\n    result = clirunner(\n        [\n            # No search arguments: return all datasets.\n            \'datasets\'\n        ], cli_method=datacube.scripts.search_tool.cli\n    )\n\n    assert str(pseudo_ls8_dataset.id) in result.output\n    assert str(telemetry_metadata_type.name) in result.output\n\n    assert result.exit_code == 0\n\n\ndef test_cli_info(index: Index,\n                  clirunner: Any,\n                  pseudo_ls8_dataset: Dataset,\n                  pseudo_ls8_dataset2: Dataset) -> None:\n    """"""\n    Search datasets using the cli.\n    """"""\n    index.datasets.add_location(pseudo_ls8_dataset.id, \'file:///tmp/location1\')\n    index.datasets.add_location(pseudo_ls8_dataset.id, \'file:///tmp/location2\')\n\n    opts = [\n        \'dataset\', \'info\', str(pseudo_ls8_dataset.id)\n    ]\n    result = clirunner(opts, verbose_flag=\'\')\n\n    output = result.output\n\n    # Should be a valid yaml\n    yaml_docs = list(yaml.safe_load_all(output))\n    assert len(yaml_docs) == 1\n\n    # We output properties in order for readability:\n    output_lines = [l for l in output.splitlines() if not l.startswith(\'indexed:\')]\n    assert output_lines == [\n        ""id: "" + str(pseudo_ls8_dataset.id),\n        \'product: ls8_telemetry\',\n        \'status: active\',\n        # Newest location first\n        \'locations:\',\n        \'- file:///tmp/location2\',\n        \'- file:///tmp/location1\',\n        \'fields:\',\n        \'    creation_time: 2015-04-22 06:32:04\',\n        \'    format: PSEUDOMD\',\n        \'    gsi: null\',\n        \'    instrument: OLI_TIRS\',\n        \'    label: LS8_OLITIRS_STD-MD_P00_LC81160740742015089ASA00_116_074_20150330T022553Z20150330T022657\',\n        \'    lat: {begin: -31.37116, end: -29.23394}\',\n        \'    lon: {begin: 149.78434, end: 152.21782}\',\n        \'    orbit: null\',\n        \'    platform: LANDSAT_8\',\n        \'    product_type: pseudo_ls8_data\',\n        \'    sat_path: {begin: 116, end: 116}\',\n        \'    sat_row: {begin: 74, end: 84}\',\n        ""    time: {begin: \'2014-07-26T23:48:00.343853\', end: \'2014-07-26T23:52:00.343853\'}"",\n    ]\n\n    # Check indexed time separately, as we don\'t care what timezone it\'s displayed in.\n    indexed_time = yaml_docs[0][\'indexed\']\n    assert isinstance(indexed_time, datetime.datetime)\n    assert assume_utc(indexed_time) == assume_utc(pseudo_ls8_dataset.indexed_time)\n\n    # Request two, they should have separate yaml documents\n    opts.append(str(pseudo_ls8_dataset2.id))\n\n    result = clirunner(opts)\n    yaml_docs = list(yaml.safe_load_all(result.output))\n    assert len(yaml_docs) == 2, ""Two datasets should produce two sets of info""\n    assert yaml_docs[0][\'id\'] == str(pseudo_ls8_dataset.id)\n    assert yaml_docs[1][\'id\'] == str(pseudo_ls8_dataset2.id)\n\n\ndef assume_utc(d):\n    if d.tzinfo is None:\n        return d.replace(tzinfo=tz.tzutc())\n    else:\n        return d.astimezone(tz.tzutc())\n\n\ndef test_cli_missing_info(clirunner, initialised_postgres_db):\n    id_ = str(uuid.uuid4())\n    result = clirunner(\n        [\n            \'dataset\', \'info\', id_\n        ],\n        catch_exceptions=False,\n        expect_success=False,\n        verbose_flag=False\n    )\n\n    assert result.exit_code == 1, ""Should return exit status when dataset is missing""\n    # This should have been output to stderr, but the CliRunner doesnit distinguish\n    assert result.output == ""{id} missing\\n"".format(id=id_)\n\n\ndef test_find_duplicates(index, pseudo_ls8_type,\n                         pseudo_ls8_dataset, pseudo_ls8_dataset2, pseudo_ls8_dataset3, pseudo_ls8_dataset4,\n                         ls5_dataset_w_children):\n    # type: (Index, DatasetType, Dataset, Dataset, Dataset, Dataset, Dataset) -> None\n\n    # Our four ls8 datasets and three ls5.\n    all_datasets = index.datasets.search_eager()\n    assert len(all_datasets) == 7\n\n    # First two ls8 datasets have the same path/row, last two have a different row.\n    expected_ls8_path_row_duplicates = [\n        (\n            (\n                NumericRange(Decimal(\'116\'), Decimal(\'116\'), \'[]\'),\n                NumericRange(Decimal(\'74\'), Decimal(\'84\'), \'[]\')\n            ),\n            {pseudo_ls8_dataset.id, pseudo_ls8_dataset2.id}\n        ),\n        (\n            (\n                NumericRange(Decimal(\'116\'), Decimal(\'116\'), \'[]\'),\n                NumericRange(Decimal(\'85\'), Decimal(\'87\'), \'[]\')\n            ),\n            {pseudo_ls8_dataset3.id, pseudo_ls8_dataset4.id}\n        ),\n\n    ]\n\n    # Specifying groups as fields:\n    f = pseudo_ls8_type.metadata_type.dataset_fields.get\n    field_res = sorted(index.datasets.search_product_duplicates(\n        pseudo_ls8_type,\n        f(\'sat_path\'), f(\'sat_row\')\n    ))\n    assert field_res == expected_ls8_path_row_duplicates\n    # Field names as strings\n    product_res = sorted(index.datasets.search_product_duplicates(\n        pseudo_ls8_type,\n        \'sat_path\', \'sat_row\'\n    ))\n    assert product_res == expected_ls8_path_row_duplicates\n\n    # Get duplicates that start on the same day\n    f = pseudo_ls8_type.metadata_type.dataset_fields.get\n    field_res = sorted(index.datasets.search_product_duplicates(\n        pseudo_ls8_type,\n        f(\'time\').lower.day  # type: ignore\n    ))\n\n    # Datasets 1 & 3 are on the 26th.\n    # Datasets 2 & 4 are on the 27th.\n    assert field_res == [\n        (\n            (\n                datetime.datetime(2014, 7, 26, 0, 0),\n            ),\n            {pseudo_ls8_dataset.id, pseudo_ls8_dataset3.id}\n        ),\n        (\n            (\n                datetime.datetime(2014, 7, 27, 0, 0),\n            ),\n            {pseudo_ls8_dataset2.id, pseudo_ls8_dataset4.id}\n        ),\n\n    ]\n\n    # No LS5 duplicates: there\'s only one of each\n    sat_res = sorted(index.datasets.search_product_duplicates(\n        ls5_dataset_w_children.type,\n        \'sat_path\', \'sat_row\'\n    ))\n    assert sat_res == []\n\n\ndef test_csv_search_via_cli(clirunner: Any,\n                            pseudo_ls8_type: DatasetType,\n                            pseudo_ls8_dataset: Dataset,\n                            pseudo_ls8_dataset2: Dataset) -> None:\n    """"""\n    Search datasets via the cli with csv output\n    """"""\n\n    # Test dataset is:\n    # platform: LANDSAT_8\n    # from: 2014-7-26  23:48:00\n    # to:   2014-7-26  23:52:00\n    # coords:\n    #     ll: (-31.33333, 149.78434)\n    #     lr: (-31.37116, 152.20094)\n    #     ul: (-29.23394, 149.85216)\n    #     ur: (-29.26873, 152.21782)\n\n    # Dataset 2 is the same but on day 2014-7-27\n\n    def matches_both(*args):\n        rows = _cli_csv_search((\'datasets\',) + args, clirunner)\n        assert len(rows) == 2\n        assert {rows[0][\'id\'], rows[1][\'id\']} == {str(pseudo_ls8_dataset.id), str(pseudo_ls8_dataset2.id)}\n\n    def matches_1(*args):\n        rows = _cli_csv_search((\'datasets\',) + args, clirunner)\n        assert len(rows) == 1\n        assert rows[0][\'id\'] == str(pseudo_ls8_dataset.id)\n\n    def matches_none(*args):\n        rows = _cli_csv_search((\'datasets\',) + args, clirunner)\n        assert len(rows) == 0\n\n    def no_such_product(*args):\n        with pytest.raises(ValueError):\n            _cli_csv_search((\'datasets\',) + args, clirunner)\n\n    matches_both(\'lat in [-40, -10]\')\n    matches_both(\'product=\' + pseudo_ls8_type.name)\n\n    # Don\'t return on a mismatch\n    matches_none(\'lat in [150, 160]\')\n\n    # Match only a single dataset using multiple fields\n    matches_1(\'platform=LANDSAT_8\', \'time in [2014-07-24, 2014-07-26]\')\n\n    # One matching field, one non-matching\n    no_such_product(\'time in [2014-07-24, 2014-07-26]\', \'platform=LANDSAT_5\')\n\n    # Test date shorthand\n    matches_both(\'time in [2014-07, 2014-07]\')\n    matches_none(\'time in [2014-06, 2014-06]\')\n\n    matches_both(\'time in 2014-07\')\n    matches_none(\'time in 2014-08\')\n    matches_both(\'time in 2014\')\n    matches_none(\'time in 2015\')\n\n    matches_both(\'time in [2014, 2014]\')\n    matches_both(\'time in [2013, 2014]\')\n    matches_none(\'time in [2015, 2015]\')\n    matches_none(\'time in [2013, 2013]\')\n\n    matches_both(\'time in [2014-7, 2014-8]\')\n    matches_none(\'time in [2014-6, 2014-6]\')\n    matches_both(\'time in [2005, 2015]\')\n\n\n# Headers are currently in alphabetical order.\n_EXPECTED_OUTPUT_HEADER = \'creation_time,dataset_type_id,format,gsi,id,indexed_by,indexed_time,\' \\\n                          \'instrument,label,lat,lon,metadata_doc,metadata_type,metadata_type_id,\' \\\n                          \'orbit,platform,product,product_type,sat_path,sat_row,time,uri\'\n\n\ndef test_csv_structure(clirunner, pseudo_ls8_type, ls5_telem_type,\n                       pseudo_ls8_dataset, pseudo_ls8_dataset2):\n    output = _csv_search_raw([\'datasets\', \' lat in [-40, -10]\'], clirunner)\n    lines = [line.strip() for line in output.split(\'\\n\') if line]\n    # A header and two dataset rows\n    assert len(lines) == 3\n\n    assert lines[0] == _EXPECTED_OUTPUT_HEADER\n\n\ndef _cli_csv_search(args, clirunner):\n    # Do a CSV search from the cli, returning results as a list of dictionaries\n    output = _csv_search_raw(args, clirunner)\n    return list(csv.DictReader(io.StringIO(output)))\n\n\ndef _csv_search_raw(args, clirunner):\n    # Do a CSV search from the cli, returning output as a string\n    result = clirunner([\'-f\', \'csv\'] + list(args), cli_method=datacube.scripts.search_tool.cli, verbose_flag=False)\n    return result.output\n'"
tests/api/__init__.py,0,b''
tests/api/test_core.py,1,"b'import xarray as xr\nimport numpy as np\nimport datetime\nfrom uuid import UUID\nfrom types import SimpleNamespace\nimport pytest\n\nfrom datacube.api.query import GroupBy\nfrom datacube.api.core import _calculate_chunk_sizes\nfrom datacube import Datacube\nfrom datacube.testutils.geom import AlbersGS\nfrom datacube.testutils import mk_sample_dataset\n\n\ndef test_grouping_datasets():\n    def group_func(d):\n        return d.time\n    dimension = \'time\'\n    units = None\n    datasets = [\n        SimpleNamespace(time=datetime.datetime(2016, 1, 1), value=\'foo\', id=UUID(int=10)),\n        SimpleNamespace(time=datetime.datetime(2016, 2, 1), value=\'bar\', id=UUID(int=1)),\n        SimpleNamespace(time=datetime.datetime(2016, 1, 1), value=\'flim\', id=UUID(int=9)),\n    ]\n\n    group_by = GroupBy(dimension, group_func, units, sort_key=group_func)\n    grouped = Datacube.group_datasets(datasets, group_by)\n    dss = grouped.isel(time=0).values[()]\n    assert isinstance(dss, tuple)\n    assert len(dss) == 2\n    assert [ds.value for ds in dss] == [\'flim\', \'foo\']\n\n    dss = grouped.isel(time=1).values[()]\n    assert isinstance(dss, tuple)\n    assert len(dss) == 1\n    assert [ds.value for ds in dss] == [\'bar\']\n\n    assert str(grouped.time.dtype) == \'datetime64[ns]\'\n    assert grouped.loc[\'2016-01-01\':\'2016-01-15\']\n\n\ndef test_group_datasets_by_time():\n    bands = [dict(name=\'a\')]\n    # Same time instant but one explicitly marked as UTC\n    ds1 = mk_sample_dataset(bands, timestamp=""2019-01-01T23:24:00Z"")\n    ds2 = mk_sample_dataset(bands, timestamp=""2019-01-01T23:24:00"")\n    # Same ""time"" but in a different timezone, and actually later\n    ds3 = mk_sample_dataset(bands, timestamp=""2019-01-01T23:24:00-1"")\n    assert ds1.center_time.tzinfo is not None\n    assert ds2.center_time.tzinfo is None\n    assert ds3.center_time.tzinfo is not None\n\n    xx = Datacube.group_datasets([ds1, ds2, ds3], \'time\')\n    assert xx.time.shape == (2,)\n    assert len(xx.data[0]) == 2\n    assert len(xx.data[1]) == 1\n\n\ndef test_grouped_datasets_should_be_in_consistent_order():\n    datasets = [\n        {\'time\': datetime.datetime(2016, 1, 1, 0, 1), \'value\': \'foo\'},\n        {\'time\': datetime.datetime(2016, 1, 1, 0, 2), \'value\': \'flim\'},\n        {\'time\': datetime.datetime(2016, 2, 1, 0, 1), \'value\': \'bar\'}\n    ]\n\n    grouped = _group_datasets_by_date(datasets)\n\n    # Swap the two elements which get grouped together\n    datasets[0], datasets[1] = datasets[1], datasets[0]\n    grouped_2 = _group_datasets_by_date(datasets)\n\n    assert len(grouped) == len(grouped_2) == 2\n    assert all(grouped.values == grouped_2.values)\n\n\ndef _group_datasets_by_date(datasets):\n    def group_func(d):\n        return d[\'time\'].date()\n\n    def sort_key(d):\n        return d[\'time\']\n    dimension = \'time\'\n    units = None\n\n    group_by = GroupBy(dimension, group_func, units, sort_key)\n    return Datacube.group_datasets(datasets, group_by)\n\n\ndef test_dask_chunks():\n    coords = {\'time\': np.arange(10)}\n\n    sources = xr.DataArray(coords[\'time\'],\n                           coords=coords,\n                           dims=list(coords))\n    geobox = AlbersGS.tile_geobox((0, 0))[:6, :7]\n\n    assert geobox.dimensions == (\'y\', \'x\')\n    assert sources.dims == (\'time\',)\n\n    assert _calculate_chunk_sizes(sources, geobox, {}) == ((1,), (6, 7))\n    assert _calculate_chunk_sizes(sources, geobox, {\'time\': -1}) == ((10,), (6, 7))\n    assert _calculate_chunk_sizes(sources, geobox, {\'time\': \'auto\', \'x\': \'auto\'}) == ((1,), (6, 7))\n    assert _calculate_chunk_sizes(sources, geobox, {\'y\': -1, \'x\': 3}) == ((1,), (6, 3))\n    assert _calculate_chunk_sizes(sources, geobox, {\'y\': 2, \'x\': 3}) == ((1,), (2, 3))\n\n    with pytest.raises(ValueError):\n        _calculate_chunk_sizes(sources, geobox, {\'x\': ""aouto""})\n\n    with pytest.raises(KeyError):\n        _calculate_chunk_sizes(sources, geobox, {\'zz\': 1})\n'"
tests/api/test_grid_workflow.py,0,"b'import pytest\nimport numpy\nfrom datacube.model import GridSpec\nfrom datacube.utils import geometry\nfrom mock import MagicMock\n\n\nclass PickableMock(MagicMock):\n    def __reduce__(self):\n        return (MagicMock, ())\n\n\ndef test_gridworkflow():\n    """""" Test GridWorkflow with padding option. """"""\n    from mock import MagicMock\n    import datetime\n\n    # ----- fake a datacube -----\n    # e.g. let there be a dataset that coincides with a grid cell\n\n    fakecrs = geometry.CRS(\'EPSG:4326\')\n\n    grid = 100  # spatial frequency in crs units\n    pixel = 10  # square pixel linear dimension in crs units\n    # if cell(0,0) has lower left corner at grid origin,\n    # and cell indices increase toward upper right,\n    # then this will be cell(1,-2).\n    gridspec = GridSpec(crs=fakecrs, tile_size=(grid, grid), resolution=(-pixel, pixel))  # e.g. product gridspec\n\n    fakedataset = MagicMock()\n    fakedataset.extent = geometry.box(left=grid, bottom=-grid, right=2*grid, top=-2*grid, crs=fakecrs)\n    fakedataset.center_time = t = datetime.datetime(2001, 2, 15)\n\n    fakeindex = PickableMock()\n    fakeindex._db = None\n    fakeindex.datasets.get_field_names.return_value = [\'time\']  # permit query on time\n    fakeindex.datasets.search_eager.return_value = [fakedataset]\n\n    # ------ test without padding ----\n\n    from datacube.api.grid_workflow import GridWorkflow\n    gw = GridWorkflow(fakeindex, gridspec)\n    # Need to force the fake index otherwise the driver manager will\n    # only take its _db\n    gw.index = fakeindex\n    query = dict(product=\'fake_product_name\', time=(\'2001-1-1 00:00:00\', \'2001-3-31 23:59:59\'))\n\n    # test backend : that it finds the expected cell/dataset\n    assert list(gw.cell_observations(**query).keys()) == [(1, -2)]\n\n    # again but with geopolygon\n    assert list(gw.cell_observations(**query,\n                                     geopolygon=gridspec.tile_geobox((1, -2)).extent).keys()) == [(1, -2)]\n\n    with pytest.raises(ValueError) as e:\n        list(gw.cell_observations(**query, tile_buffer=(1, 1),\n                                  geopolygon=gridspec.tile_geobox((1, -2)).extent).keys())\n    assert str(e.value) == \'Cannot process tile_buffering and geopolygon together.\'\n\n    # test frontend\n    assert len(gw.list_tiles(**query)) == 1\n\n    # ------ introduce padding --------\n\n    assert len(gw.list_tiles(tile_buffer=(20, 20), **query)) == 9\n\n    # ------ add another dataset (to test grouping) -----\n\n    # consider cell (2,-2)\n    fakedataset2 = MagicMock()\n    fakedataset2.extent = geometry.box(left=2*grid, bottom=-grid, right=3*grid, top=-2*grid, crs=fakecrs)\n    fakedataset2.center_time = t\n\n    def search_eager(lat=None, lon=None, **kwargs):\n        return [fakedataset, fakedataset2]\n\n    fakeindex.datasets.search_eager = search_eager\n\n    # unpadded\n    assert len(gw.list_tiles(**query)) == 2\n    ti = numpy.datetime64(t, \'ns\')\n    assert set(gw.list_tiles(**query).keys()) == {(1, -2, ti), (2, -2, ti)}\n\n    # padded\n    assert len(gw.list_tiles(tile_buffer=(20, 20), **query)) == 12  # not 18=2*9 because of grouping\n\n    # -------- inspect particular returned tile objects --------\n\n    # check the array shape\n\n    tile = gw.list_tiles(**query)[1, -2, ti]  # unpadded example\n    assert grid/pixel == 10\n    assert tile.shape == (1, 10, 10)\n\n    padded_tile = gw.list_tiles(tile_buffer=(20, 20), **query)[1, -2, ti]  # padded example\n    # assert grid/pixel + 2*gw2.grid_spec.padding == 14  # GREG: understand this\n    assert padded_tile.shape == (1, 14, 14)\n\n    # count the sources\n\n    assert len(tile.sources.isel(time=0).item()) == 1\n    assert len(padded_tile.sources.isel(time=0).item()) == 2\n\n    # check the geocoding\n\n    assert tile.geobox.alignment == padded_tile.geobox.alignment\n    assert tile.geobox.affine * (0, 0) == padded_tile.geobox.affine * (2, 2)\n    assert tile.geobox.affine * (10, 10) == padded_tile.geobox.affine * (10+2, 10+2)\n\n    # ------- check loading --------\n    # GridWorkflow accesses the load_data API\n    # to ultimately convert geobox,sources,measurements to xarray,\n    # so only thing to check here is the call interface.\n\n    measurement = dict(nodata=0, dtype=numpy.int)\n    fakedataset.type.lookup_measurements.return_value = {\'dummy\': measurement}\n    fakedataset2.type = fakedataset.type\n\n    from mock import patch\n    with patch(\'datacube.api.core.Datacube.load_data\') as loader:\n\n        data = GridWorkflow.load(tile)\n        data2 = GridWorkflow.load(padded_tile)\n        # Note, could also test Datacube.load for consistency (but may require more patching)\n\n    assert data is data2 is loader.return_value\n    assert loader.call_count == 2\n\n    # Note, use of positional arguments here is not robust, could spec mock etc.\n    for (args, kwargs), loadable in zip(loader.call_args_list, [tile, padded_tile]):\n        args = list(args)\n        assert args[0] is loadable.sources\n        assert args[1] is loadable.geobox\n        assert list(args[2].values())[0] is measurement\n        assert \'resampling\' in kwargs\n\n    # ------- check single cell index extract -------\n    tile = gw.list_tiles(cell_index=(1, -2), **query)\n    assert len(tile) == 1\n    assert tile[1, -2, ti].shape == (1, 10, 10)\n    assert len(tile[1, -2, ti].sources.values[0]) == 1\n\n    padded_tile = gw.list_tiles(cell_index=(1, -2), tile_buffer=(20, 20), **query)\n    assert len(padded_tile) == 1\n    assert padded_tile[1, -2, ti].shape == (1, 14, 14)\n    assert len(padded_tile[1, -2, ti].sources.values[0]) == 2\n\n\ndef test_gridworkflow_with_time_depth():\n    """"""Test GridWorkflow with time series.\n    Also test `Tile` methods `split` and `split_by_time`\n    """"""\n    from mock import MagicMock\n    import datetime\n\n    fakecrs = geometry.CRS(\'EPSG:4326\')\n\n    grid = 100  # spatial frequency in crs units\n    pixel = 10  # square pixel linear dimension in crs units\n    # if cell(0,0) has lower left corner at grid origin,\n    # and cell indices increase toward upper right,\n    # then this will be cell(1,-2).\n    gridspec = GridSpec(crs=fakecrs, tile_size=(grid, grid), resolution=(-pixel, pixel))  # e.g. product gridspec\n\n    def make_fake_datasets(num_datasets):\n        start_time = datetime.datetime(2001, 2, 15)\n        delta = datetime.timedelta(days=16)\n        for i in range(num_datasets):\n            fakedataset = MagicMock()\n            fakedataset.extent = geometry.box(left=grid, bottom=-grid, right=2*grid, top=-2*grid, crs=fakecrs)\n            fakedataset.center_time = start_time + (delta * i)\n            yield fakedataset\n\n    fakeindex = PickableMock()\n    fakeindex.datasets.get_field_names.return_value = [\'time\']  # permit query on time\n    fakeindex.datasets.search_eager.return_value = list(make_fake_datasets(100))\n\n    # ------ test with time dimension ----\n\n    from datacube.api.grid_workflow import GridWorkflow\n    gw = GridWorkflow(fakeindex, gridspec)\n    query = dict(product=\'fake_product_name\')\n\n    cells = gw.list_cells(**query)\n    for cell_index, cell in cells.items():\n\n        #  test Tile.split()\n        for label, tile in cell.split(\'time\'):\n            assert tile.shape == (1, 10, 10)\n\n        #  test Tile.split_by_time()\n        for year, year_cell in cell.split_by_time(freq=\'A\'):\n            for t in year_cell.sources.time.values:\n                assert str(t)[:4] == year\n'"
tests/api/test_masking.py,7,"b'# coding=utf-8\nimport yaml\nimport pytest\nfrom xarray import DataArray, Dataset\nimport numpy as np\n\nfrom datacube.utils.masking import (\n    list_flag_names,\n    create_mask_value,\n    describe_variable_flags,\n    mask_to_dict,\n    mask_invalid_data,\n    valid_data_mask,\n)\n\n\n@pytest.fixture\ndef simple_var():\n    flags = SimpleVariableWithFlagsDef().flags_definition\n    return DataArray(np.zeros((2, 3)),\n                     dims=(\'y\', \'x\'),\n                     name=\'simple_var\',\n                     attrs={\'flags_definition\': flags})\n\n\ndef test_list_flag_names(simple_var):\n    flags = list_flag_names(simple_var)\n    for flag_name in simple_var.flags_definition.keys():\n        assert flag_name in flags\n\n    with pytest.raises(ValueError):\n        list_flag_names(([], {}))\n\n\ndef test_create_mask_value(simple_var):\n    bits_def = simple_var.flags_definition\n\n    assert create_mask_value(bits_def, contiguous=True) == (256, 256)\n    assert create_mask_value(bits_def, contiguous=False) == (256, 0)\n    assert create_mask_value(bits_def, contiguous=True, land_sea=\'land\') == (\n        768, 768)\n    assert create_mask_value(bits_def, contiguous=False, land_sea=\'land\') == (768, 512)\n\n\ndef test_create_multi_mask_value():\n    multi_var = VariableWithMultiBitFlags()\n    multi_flags_def = multi_var.flags_definition\n\n    assert create_mask_value(multi_flags_def, filled=True) == (1, 1)\n    assert create_mask_value(multi_flags_def, water_confidence=\'water\') == (0b011000, 0b011000)\n\n    assert create_mask_value(multi_flags_def, water_confidence=\'water\', filled=True) == (\n        0b011001, 0b011001)\n    assert create_mask_value(multi_flags_def, water_confidence=\'not_determined\') == (0b011000, 0b0)\n    assert create_mask_value(multi_flags_def, water_confidence=\'no_water\') == (0b11000, 0b01000)\n    assert create_mask_value(multi_flags_def, veg_confidence=\'maybe_veg\') == (\n        0b110000000, 0b100000000)\n    assert create_mask_value(multi_flags_def,\n                             veg_confidence=\'maybe_veg\',\n                             water_confidence=\'water\') == (0b110011000, 0b100011000)\n    assert create_mask_value(multi_flags_def,\n                             veg_confidence=\'maybe_veg\',\n                             water_confidence=\'water\', filled=True) == (0b110011001, 0b100011001)\n\n    assert create_mask_value(multi_flags_def, water_confidence=\'maybe_water\') == (0b011000, 0b10000)\n\n    with pytest.raises(ValueError):\n        create_mask_value(multi_flags_def, this_flag_doesnot_exist=9)\n\n    with pytest.raises(ValueError):\n        create_mask_value(multi_flags_def, water_confidence=\'invalid enum value\')\n\n\ndef test_ga_good_pixel(simple_var):\n    bits_def = simple_var.flags_definition\n\n    assert create_mask_value(bits_def, ga_good_pixel=True) == (16383, 16383)\n\n\ndef test_describe_flags(simple_var):\n    describe_variable_flags(simple_var)\n    describe_variable_flags(simple_var, with_pandas=False)\n\n    describe_variable_flags(simple_var.to_dataset())\n    describe_variable_flags(simple_var.to_dataset(), with_pandas=False)\n\n\nclass SimpleVariableWithFlagsDef(object):\n    bits_def_yaml = """"""\n        cloud_shadow_fmask:\n          bits: 13\n          description: Cloud Shadow (Fmask)\n          values:\n            0: cloud_shadow\n            1: no_cloud_shadow\n        cloud_shadow_acca:\n          bits: 12\n          description: Cloud Shadow (ACCA)\n          values:\n            0: cloud_shadow\n            1: no_cloud_shadow\n        cloud_fmask:\n          bits: 11\n          description: Cloud (Fmask)\n          values:\n            0: cloud\n            1: no_cloud\n        cloud_acca:\n          bits: 10\n          description: Cloud Shadow (ACCA)\n          values:\n            0: cloud\n            1: no_cloud\n        land_sea:\n          bits: 9\n          description: Land or Sea\n          values:\n            0: sea\n            1: land\n        contiguous:\n          bits: 8\n          description: All bands for this pixel contain non-null values\n          values:\n            0: false\n            1: true\n        swir2_saturated:\n          bits: 7\n          description: SWIR2 is saturated\n          values:\n            0: true\n            1: false\n\n        swir2_saturated:\n          bits: 6\n          description: SWIR2 band is saturated\n          values:\n            0: true\n            1: false\n        swir1_saturated:\n          bits: 4\n          description: SWIR1 band is saturated\n          values:\n            0: true\n            1: false\n        nir_saturated:\n          bits: 3\n          description: NIR band is saturated\n          values:\n            0: true\n            1: false\n\n        red_saturated:\n          bits: 2\n          description: Red band is saturated\n          values:\n            0: true\n            1: false\n\n        green_saturated:\n          bits: 1\n          description: Green band is saturated\n          values:\n            0: true\n            1: false\n\n        blue_saturated:\n          bits: 0\n          description: Blue band is saturated\n          values:\n            0: true\n            1: false\n\n        ga_good_pixel:\n          bits: [13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n          description: Best Quality Pixel\n          values:\n            16383: true\n        """"""\n\n    flags_definition = yaml.safe_load(bits_def_yaml)\n\n\nclass VariableWithMultiBitFlags(object):\n    bits_def_yaml = """"""\n        cloud_confidence:\n          bits: [13, 14]\n          description: Cloud Confidence\n          values:\n            0: not_determined\n            1: no_cloud\n            2: maybe_cloud\n            3: cloud\n        cirrus_confidence:\n          bits: [11, 12]\n          description: Cirrus Confidence\n          values:\n            0: not_determined\n            1: no_cirrus\n            2: maybe_cirrus\n            3: cirrus\n\n        snowice_confidence:\n          bits: [9, 10]\n          description: Snow/Ice Confidence\n          values:\n            0: not_determined\n            1: no_snowice\n            2: maybe_snowice\n            3: snowice\n\n        veg_confidence:\n          bits: [7, 8]\n          description: Vegetation Confidence\n          values:\n            0: not_determined\n            1: no_veg\n            2: maybe_veg\n            3: veg\n\n\n\n        water_confidence:\n          bits: [3, 4]\n          description: Water Confidence\n          values:\n            0: not_determined\n            1: no_water\n            2: maybe_water\n            3: water\n\n        terrain_occluded:\n          bits: 2\n          description: Terrain occluded\n          values:\n            0: False\n            1: True\n\n        frame_dropped:\n          bits: 1\n          description: Frame dropped\n          values:\n            0: False\n            1: True\n\n        filled:\n          bits: 0\n          description: Filled\n          values:\n            0: False\n            1: True\n    """"""\n\n    flags_definition = yaml.safe_load(bits_def_yaml)\n\n\nbits_def_json = {\n    \'blue_saturated\': {\n        \'bits\': 0,\n        \'description\': \'Blue band is saturated\',\n        \'values\': {\'0\': True, \'1\': False}},\n    \'cloud_acca\': {\n        \'bits\': 10,\n        \'description\': \'Cloud Shadow (ACCA)\',\n        \'values\': {\'0\': \'cloud\', \'1\': \'no_cloud\'}},\n    \'cloud_fmask\': {\n        \'bits\': 11,\n        \'description\': \'Cloud (Fmask)\',\n        \'values\': {\'0\': \'cloud\', \'1\': \'no_cloud\'}},\n    \'cloud_shadow_acca\': {\n        \'bits\': 12,\n        \'description\': \'Cloud Shadow (ACCA)\',\n        \'values\': {\'0\': \'cloud_shadow\', \'1\': \'no_cloud_shadow\'}},\n    \'cloud_shadow_fmask\': {\n        \'bits\': 13,\n        \'description\': \'Cloud Shadow (Fmask)\',\n        \'values\': {\'0\': \'cloud_shadow\', \'1\': \'no_cloud_shadow\'}},\n    \'contiguous\': {\n        \'bits\': 8,\n        \'description\': \'All bands for this pixel contain non-null values\',\n        \'values\': {\'0\': False, \'1\': True}},\n    \'ga_good_pixel\': {\n        \'bits\': [13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n        \'description\': \'Best Quality Pixel\',\n        \'values\': {\'16383\': True}},\n    \'green_saturated\': {\n        \'bits\': 1,\n        \'description\': \'Green band is saturated\',\n        \'values\': {\'0\': True, \'1\': False}},\n    \'land_sea\': {\n        \'bits\': 9,\n        \'description\': \'Land or Sea\',\n        \'values\': {\'0\': \'sea\', \'1\': \'land\'}},\n    \'nir_saturated\': {\n        \'bits\': 3,\n        \'description\': \'NIR band is saturated\',\n        \'values\': {\'0\': True, \'1\': False}},\n    \'red_saturated\': {\n        \'bits\': 2,\n        \'description\': \'Red band is saturated\',\n        \'values\': {\'0\': True, \'1\': False}},\n    \'swir1_saturated\': {\n        \'bits\': 4,\n        \'description\': \'SWIR1 band is saturated\',\n        \'values\': {\'0\': True, \'1\': False}},\n    \'swir2_saturated\': {\n        \'bits\': 7,\n        \'description\': \'SWIR2 band is saturated\',\n        \'values\': {\'0\': True, \'1\': False}},\n    \'tir_saturated\': {\n        \'bits\': 5,\n        \'description\': \'Thermal Infrared band is saturated\',\n        \'values\': {\'0\': True, \'1\': False}}}\n\n\n@pytest.mark.parametrize(\'bits_def\', [SimpleVariableWithFlagsDef.flags_definition,\n                                      bits_def_json])\ndef test_simple_mask_to_dict(bits_def):\n    # All 0. Contiguous should be False, Saturated bits should be true\n    test_dict = mask_to_dict(bits_def, 0)\n    assert not test_dict[\'contiguous\']\n    assert test_dict[\'blue_saturated\']\n\n    # Only contiguous (bit 8) set\n    test_dict = mask_to_dict(bits_def, 256)\n    assert test_dict[\'contiguous\']\n    assert test_dict[\'blue_saturated\']\n    assert test_dict[\'land_sea\'] == \'sea\'\n\n    # Contiguous and land_sea bits set to 1. (bits 7 and 8)\n    test_dict = mask_to_dict(bits_def, 768)\n    assert test_dict[\'contiguous\']\n    assert test_dict[\'land_sea\'] == \'land\'\n    assert test_dict[\'blue_saturated\']\n\n    # All GA PQ bits set to 1\n    test_dict = mask_to_dict(bits_def, 16383)\n    assert test_dict[\'ga_good_pixel\']\n    assert test_dict[\'contiguous\']\n    assert test_dict[\'land_sea\'] == \'land\'\n    assert not test_dict[\'blue_saturated\']\n\n\ndef test_mask_valid_data():\n    test_attrs = {\n        \'one\': 1,\n        \'nodata\': -999,\n    }\n\n    expected_data_array = DataArray(np.array([[1., np.nan, np.nan], [2, 3, np.nan], [np.nan, np.nan, np.nan]],\n                                             dtype=\'float\'),\n                                    attrs=test_attrs, name=\'var_one\')\n\n    data_array = DataArray([[1, -999, -999], [2, 3, -999], [-999, -999, -999]], attrs=test_attrs)\n    dataset = Dataset(data_vars={\'var_one\': data_array}, attrs={\'ds_attr\': \'still here\'})\n\n    # Make sure test is actually changing something\n    assert not data_array.equals(expected_data_array)\n\n    output_ds = mask_invalid_data(dataset, keep_attrs=True)\n    assert output_ds.attrs[\'ds_attr\'] == \'still here\'\n    assert output_ds.data_vars[\'var_one\'].equals(expected_data_array)\n    assert output_ds.data_vars[\'var_one\'].attrs[\'one\'] == 1\n\n    output_da = mask_invalid_data(data_array, keep_attrs=True)\n    assert output_da.equals(expected_data_array)\n    assert output_da.attrs[\'one\'] == 1\n\n    missing_nodata = data_array.copy()\n    del missing_nodata.attrs[\'nodata\']\n    assert not hasattr(missing_nodata, \'nodata\')\n    np.testing.assert_array_equal(missing_nodata, mask_invalid_data(missing_nodata))\n\n    with pytest.raises(TypeError):\n        mask_invalid_data({})\n\n\ndef test_valid_data_mask():\n    attrs = {\n        \'nodata\': -999,\n    }\n\n    expected_data_array = DataArray(np.array([[True, False, False], [True, True, False], [False, False, False]],\n                                             dtype=\'bool\'))\n\n    data_array = DataArray([[1, -999, -999], [2, 3, -999], [-999, -999, -999]], attrs=attrs)\n    dataset = Dataset(data_vars={\'var_one\': data_array})\n\n    output_ds = valid_data_mask(dataset)\n    assert output_ds.data_vars[\'var_one\'].equals(expected_data_array)\n\n    output_da = valid_data_mask(data_array)\n    assert output_da.equals(expected_data_array)\n\n    expected_data_array = DataArray(np.array([[True, True, True], [True, True, True], [True, True, True]],\n                                             dtype=\'bool\'))\n    data_array = DataArray([[1, -999, -999], [2, 3, -999], [-999, -999, -999]])\n    dataset = Dataset(data_vars={\'var_one\': data_array})\n\n    output_ds = valid_data_mask(dataset)\n    assert output_ds.data_vars[\'var_one\'].equals(expected_data_array)\n\n    output_da = valid_data_mask(data_array)\n    assert output_da.equals(expected_data_array)\n\n    expected_data_array = DataArray(np.array([[True, False, False], [True, True, False], [False, False, False]],\n                                             dtype=\'bool\'))\n\n    data_array = DataArray([[1, -999, -999], [2, 3, -999], [-999, -999, float(\'nan\')]], attrs=attrs)\n    dataset = Dataset(data_vars={\'var_one\': data_array})\n\n    output_ds = valid_data_mask(dataset)\n    assert output_ds.data_vars[\'var_one\'].equals(expected_data_array)\n\n    output_da = valid_data_mask(data_array)\n    assert output_da.equals(expected_data_array)\n\n    expected_data_array = DataArray(np.array([[True, True, True], [True, True, True], [True, True, False]],\n                                             dtype=\'bool\'))\n\n    data_array = DataArray([[1, -999, -999], [2, 3, -999], [-999, -999, float(\'nan\')]])\n    dataset = Dataset(data_vars={\'var_one\': data_array})\n\n    output_ds = valid_data_mask(dataset)\n    assert output_ds.data_vars[\'var_one\'].equals(expected_data_array)\n\n    output_da = valid_data_mask(data_array)\n    assert output_da.equals(expected_data_array)\n\n    with pytest.raises(TypeError):\n        valid_data_mask(([], []))\n\n\ndef test_deprecation():\n    from datacube.storage.masking import make_mask as a\n    from datacube.utils.masking import make_mask as b\n    assert a is b\n'"
tests/api/test_query.py,2,"b'#\n#    Licensed under the Apache License, Version 2.0 (the ""License"");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an ""AS IS"" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\nimport datetime\nimport pandas\nimport numpy as np\nfrom types import SimpleNamespace\n\nimport pytest\n\nfrom datacube.api.query import Query, _datetime_to_timestamp, query_group_by, solar_day, GroupBy\nfrom datacube.model import Range\nfrom datacube.utils import parse_time\nfrom datacube.utils.geometry import CRS\n\n\ndef test_datetime_to_timestamp():\n    assert _datetime_to_timestamp((1990, 1, 7)) == 631670400\n    assert _datetime_to_timestamp(datetime.datetime(1990, 1, 7)) == 631670400\n    assert _datetime_to_timestamp(631670400) == 631670400\n    assert _datetime_to_timestamp(\'1990-01-07T00:00:00.0Z\') == 631670400\n\n\ndef test_query_kwargs():\n    from mock import MagicMock\n\n    mock_index = MagicMock()\n    mock_index.datasets.get_field_names = lambda: {u\'product\', u\'lat\', u\'sat_path\', \'type_id\', u\'time\', u\'lon\',\n                                                   u\'orbit\', u\'instrument\', u\'sat_row\', u\'platform\', \'metadata_type\',\n                                                   u\'gsi\', \'type\', \'id\'}\n\n    query = Query(index=mock_index, product=\'ls5_nbar_albers\')\n    assert str(query)\n    assert query.product == \'ls5_nbar_albers\'\n    assert query.search_terms[\'product\'] == \'ls5_nbar_albers\'\n\n    query = Query(index=mock_index, latitude=(-35, -36), longitude=(148, 149))\n    assert query.geopolygon\n    assert \'lat\' in query.search_terms\n    assert \'lon\' in query.search_terms\n\n    query = Query(index=mock_index, latitude=-35, longitude=148)\n    assert query.geopolygon\n    assert \'lat\' in query.search_terms\n    assert \'lon\' in query.search_terms\n\n    query = Query(index=mock_index, y=(-4174726, -4180011), x=(1515184, 1523263), crs=\'EPSG:3577\')\n    assert query.geopolygon\n    assert \'lat\' in query.search_terms\n    assert \'lon\' in query.search_terms\n\n    query = Query(index=mock_index, y=-4174726, x=1515184, crs=\'EPSG:3577\')\n    assert query.geopolygon\n    assert \'lat\' in query.search_terms\n    assert \'lon\' in query.search_terms\n\n    query = Query(index=mock_index, y=-4174726, x=1515184, crs=\'EPSG:3577\')\n    assert query.geopolygon\n    assert \'lat\' in query.search_terms\n    assert \'lon\' in query.search_terms\n\n    query = Query(index=mock_index, y=-4174726, x=1515184, crs=CRS(\'EPSG:3577\'))\n    assert query.geopolygon\n    assert \'lat\' in query.search_terms\n    assert \'lon\' in query.search_terms\n\n    query = Query(index=mock_index, time=\'2001\')\n    assert \'time\' in query.search\n\n    query = Query(index=mock_index, time=(\'2001\', \'2002\'))\n    assert \'time\' in query.search\n\n    with pytest.raises(ValueError):\n        Query(index=mock_index,\n              y=-4174726, coordinate_reference_system=\'WGS84\',\n              x=1515184, crs=\'EPSG:3577\')\n\n    with pytest.raises(LookupError):\n        Query(index=mock_index, y=-4174726, x=1515184, crs=\'EPSG:3577\', made_up_key=\'NotReal\')\n\n    with pytest.raises(LookupError):\n        query_group_by(group_by=\'magic\')\n\n    gb = query_group_by(\'time\')\n    assert isinstance(gb, GroupBy)\n    assert query_group_by(group_by=gb) is gb\n\n\ndef format_test(start_out, end_out):\n    return Range(pandas.to_datetime(start_out, utc=True).to_pydatetime(),\n                 pandas.to_datetime(end_out, utc=True).to_pydatetime())\n\n\ntestdata = [\n    ((datetime.datetime(2008, 1, 1), datetime.datetime(2008, 1, 10)),\n     format_test(\'2008-01-01T00:00:00\', \'2008-01-10T00:00:00.999999\')),\n    ((datetime.datetime(2008, 1, 1), datetime.datetime(2008, 1, 10, 23, 0, 0)),\n     format_test(\'2008-01-01T00:00:00\', \'2008-01-10T23:00:00.999999\')),\n    ((datetime.datetime(2008, 1, 1), datetime.datetime(2008, 1, 10, 23, 59, 40)),\n     format_test(\'2008-01-01T00:00:00\', \'2008-01-10T23:59:40.999999\')),\n    ((\'2008\'),\n     format_test(\'2008-01-01T00:00:00\', \'2008-12-31T23:59:59.999999\')),\n    ((\'2008\', \'2008\'),\n     format_test(\'2008-01-01T00:00:00\', \'2008-12-31T23:59:59.999999\')),\n    ((\'2008\', \'2009\'),\n     format_test(\'2008-01-01T00:00:00\', \'2009-12-31T23:59:59.999999\')),\n    ((\'2008-03\', \'2009\'),\n     format_test(\'2008-03-01T00:00\', \'2009-12-31T23:59:59.999999\')),\n    ((\'2008-03\', \'2009-10\'),\n     format_test(\'2008-03-01T00:00\', \'2009-10-31T23:59:59.999999\')),\n    ((\'2008\', \'2009-10\'),\n     format_test(\'2008-01-01T00:00\', \'2009-10-31T23:59:59.999999\')),\n    ((\'2008-03-03\', \'2008-11\'),\n     format_test(\'2008-03-03T00:00:00\', \'2008-11-30T23:59:59.999999\')),\n    ((\'2008-11-14\', \'2008-11-30\'),\n     format_test(\'2008-11-14T00:00:00\', \'2008-11-30T23:59:59.999999\')),\n    ((\'2008-11-14\', \'2008-11-29\'),\n     format_test(\'2008-11-14T00:00:00\', \'2008-11-29T23:59:59.999999\')),\n    ((\'2008-11-14\', \'2008-11\'),\n     format_test(\'2008-11-14T00:00:00\', \'2008-11-30T23:59:59.999999\')),\n    ((\'2008-11-14\', \'2008\'),\n     format_test(\'2008-11-14T00:00:00\', \'2008-12-31T23:59:59.999999\')),\n    ((\'2008-11-14\'),\n     format_test(\'2008-11-14T00:00:00\', \'2008-11-14T23:59:59.999999\')),\n    ((\'2008-11-14\', \'2009-02-02\'),\n     format_test(\'2008-11-14T00:00:00\', \'2009-02-02T23:59:59.999999\')),\n    ((\'2008-11-14T23:33:57\', \'2008-11-14 23:33:57\'),\n     format_test(\'2008-11-14T23:33:57\', \'2008-11-14T23:33:57.999999\')),\n    ((\'2008-11-14 23:33\', \'2008-11-14 23:34\'),\n     format_test(\'2008-11-14T23:33:00\', \'2008-11-14T23:34:59.999999\')),\n    ((\'2008-11-14T23:00:00\', \'2008-11-14 23:35\'),\n     format_test(\'2008-11-14T23:00\', \'2008-11-14T23:35:59.999999\')),\n    ((\'2008-11-10T11\', \'2008-11-16 14:01\'),\n     format_test(\'2008-11-10T11:00\', \'2008-11-16T14:01:59.999999\')),\n    ((datetime.date(1995, 1, 1), datetime.date(1999, 1, 1)),\n     format_test(\'1995-01-01T00:00:00\', \'1999-01-01T23:59:59.999999\')),\n    ((datetime.datetime(2008, 1, 1), datetime.date(2008, 1, 4), datetime.datetime(2008, 1, 10, 23, 59, 40)),\n     format_test(\'2008-01-01T00:00:00\', \'2008-01-10T23:59:40.999999\')),\n    ((datetime.date(2008, 1, 1)),\n     format_test(\'2008-01-01T00:00:00\', \'2008-01-01T23:59:59.999999\'))\n]\n\n\n@pytest.mark.parametrize(\'time_param,expected\', testdata)\ndef test_time_handling(time_param, expected):\n    query = Query(time=time_param)\n    assert \'time\' in query.search_terms\n    assert query.search_terms[\'time\'] == expected\n\n\ndef test_solar_day():\n    _s = SimpleNamespace\n    ds = _s(center_time=parse_time(\'1987-05-22 23:07:44.2270250Z\'),\n            metadata=_s(lon=Range(begin=150.415,\n                                  end=152.975)))\n\n    assert solar_day(ds) == np.datetime64(\'1987-05-23\', \'D\')\n    assert solar_day(ds, longitude=0) == np.datetime64(\'1987-05-22\', \'D\')\n\n    ds.metadata = _s()\n\n    with pytest.raises(ValueError) as e:\n        solar_day(ds)\n\n    assert \'Cannot compute solar_day: dataset is missing spatial info\' in str(e.value)\n\n\ndef test_dateline_query_building():\n    lon = Query(x=(618300, 849000),\n                y=(-1876800, -1642500),\n                crs=\'EPSG:32660\').search_terms[\'lon\']\n\n    assert lon.begin < 180 < lon.end\n'"
tests/api/test_virtual.py,0,"b'from collections import OrderedDict\nfrom datetime import datetime\nfrom copy import deepcopy\nimport warnings\n\nimport pytest\nimport mock\nimport numpy\n\nfrom datacube.model import DatasetType, MetadataType, Dataset, GridSpec\nfrom datacube.utils import geometry\nfrom datacube.virtual import construct_from_yaml, catalog_from_yaml, VirtualProductException\nfrom datacube.virtual import DEFAULT_RESOLVER, Transformation\nfrom datacube.virtual.impl import Datacube\n\n\n##########################################\n# Set up some common test data and fixtures\n\nPRODUCT_LIST = [\'ls7_pq_albers\', \'ls8_pq_albers\', \'ls7_nbar_albers\', \'ls8_nbar_albers\']\n\n\ndef example_metadata_type():\n    return MetadataType(dict(name=\'eo\',\n                             dataset=dict(id=[\'id\'],\n                                          label=[\'ga_label\'],\n                                          creation_time=[\'creation_dt\'],\n                                          measurements=[\'image\', \'bands\'],\n                                          grid_spatial=[\'grid_spatial\', \'projection\'],\n                                          sources=[\'lineage\', \'source_datasets\'])),\n                        dataset_search_fields={})\n\n\ndef example_product(name):\n    if name not in PRODUCT_LIST:\n        return None\n\n    blue = dict(name=\'blue\', dtype=\'int16\', nodata=-999, units=\'1\')\n    green = dict(name=\'green\', dtype=\'int16\', nodata=-999, units=\'1\', aliases=[\'verde\'])\n    flags = {""cloud_acca"": {""bits"": 10, ""values"": {""0"": ""cloud"", ""1"": ""no_cloud""}},\n             ""contiguous"": {""bits"": 8, ""values"": {""0"": False, ""1"": True}},\n             ""cloud_fmask"": {""bits"": 11, ""values"": {""0"": ""cloud"", ""1"": ""no_cloud""}},\n             ""nir_saturated"": {""bits"": 3, ""values"": {""0"": True, ""1"": False}},\n             ""red_saturated"": {""bits"": 2, ""values"": {""0"": True, ""1"": False}},\n             ""blue_saturated"": {""bits"": 0, ""values"": {""0"": True, ""1"": False}},\n             ""green_saturated"": {""bits"": 1, ""values"": {""0"": True, ""1"": False}},\n             ""swir1_saturated"": {""bits"": 4, ""values"": {""0"": True, ""1"": False}},\n             ""swir2_saturated"": {""bits"": 7, ""values"": {""0"": True, ""1"": False}},\n             ""cloud_shadow_acca"": {""bits"": 12, ""values"": {""0"": ""cloud_shadow"", ""1"": ""no_cloud_shadow""}},\n             ""cloud_shadow_fmask"": {""bits"": 13, ""values"": {""0"": ""cloud_shadow"", ""1"": ""no_cloud_shadow""}}}\n\n    pixelquality = dict(name=\'pixelquality\', dtype=\'int16\', nodata=0, units=\'1\',\n                        flags_definition=flags)\n\n    result = DatasetType(example_metadata_type(),\n                         dict(name=name, description="""", metadata_type=\'eo\', metadata={}))\n    result.grid_spec = GridSpec(crs=geometry.CRS(\'EPSG:3577\'),\n                                tile_size=(100000., 100000.),\n                                resolution=(-25, 25))\n    if \'_pq_\' in name:\n        result.definition = {\'name\': name, \'measurements\': [pixelquality]}\n    else:\n        result.definition = {\'name\': name, \'measurements\': [blue, green]}\n    return result\n\n\ndef example_grid_spatial():\n    return {\n        ""projection"": {\n            ""valid_data"": {\n                ""type"": ""Polygon"",\n                ""coordinates"": [[[1500000.0, -4000000.0],\n                                 [1500000.0, -3900000.0],\n                                 [1600000.0, -3900000.0],\n                                 [1600000.0, -4000000.0],\n                                 [1500000.0, -4000000.0]]]\n            },\n            ""geo_ref_points"": {\n                ""ll"": {""x"": 1500000.0, ""y"": -4000000.0},\n                ""lr"": {""x"": 1600000.0, ""y"": -4000000.0},\n                ""ul"": {""x"": 1500000.0, ""y"": -3900000.0},\n                ""ur"": {""x"": 1600000.0, ""y"": -3900000.0}\n            },\n            ""spatial_reference"": ""EPSG:3577""\n        }\n    }\n\n\n@pytest.fixture\ndef catalog():\n    return catalog_from_yaml(""""""\n        about: this is a test catalog of virtual products\n        products:\n            cloud_free_ls8_nbar:\n                tags: [nbar, landsat-8]\n                recipe:\n                    &cloud_free_ls8_nbar_recipe\n                    transform: apply_mask\n                    mask_measurement_name: pixelquality\n                    input:\n                        &cloud_mask_recipe\n                        transform: make_mask\n                        flags:\n                            blue_saturated: false\n                            cloud_acca: no_cloud\n                            cloud_fmask: no_cloud\n                            cloud_shadow_acca: no_cloud_shadow\n                            cloud_shadow_fmask: no_cloud_shadow\n                            contiguous: true\n                            green_saturated: false\n                            nir_saturated: false\n                            red_saturated: false\n                            swir1_saturated: false\n                            swir2_saturated: false\n                        mask_measurement_name: pixelquality\n                        input:\n                            juxtapose:\n                              - product: ls8_nbar_albers\n                                measurements: [\'blue\', \'green\']\n                              - product: ls8_pq_albers\n\n            cloud_free_ls7_nbar:\n                tags: [nbar, landsat-7]\n                recipe:\n                    &cloud_free_ls7_nbar_recipe\n                    transform: datacube.virtual.transformations.ApplyMask\n                    mask_measurement_name: pixelquality\n                    input:\n                      <<: *cloud_mask_recipe\n                      input:\n                        juxtapose:\n                          - product: ls7_nbar_albers\n                            measurements: [\'blue\', \'green\']\n                          - product: ls7_pq_albers\n\n            cloud_free_nbar:\n                description: cloud free NBAR from Landsat-7 and Landsat-8\n                tags: [nbar, landsat-7, landsat-8]\n                recipe:\n                    collate:\n                        - *cloud_free_ls8_nbar_recipe\n                        - *cloud_free_ls7_nbar_recipe\n\n                    index_measurement_name: source_index\n\n            mean_blue:\n                recipe:\n                    aggregate: xarray_reduction\n                    method: mean\n                    group_by: month\n                    input:\n                        transform: to_float\n                        input:\n                            collate:\n                              - product: ls7_nbar_albers\n                                measurements: [blue]\n                              - product: ls8_nbar_albers\n                                measurements: [blue]\n    """""")\n\n\n@pytest.fixture\ndef cloud_free_nbar(catalog):\n    return catalog[\'cloud_free_nbar\']\n\n\ndef load_data(*args, **kwargs):\n    sources, geobox, measurements = args\n\n    # this returns nodata bands which are good enough for this test\n    result = Datacube.create_storage(OrderedDict((dim, sources.coords[dim]) for dim in sources.dims),\n                                     geobox, measurements)\n    return result\n\n\ndef group_datasets(*args, **kwargs):\n    return Datacube.group_datasets(*args, **kwargs)\n\n\n@pytest.fixture\ndef dc():\n    result = mock.MagicMock()\n    result.index.datasets.get_field_names.return_value = {\'id\', \'product\', \'time\'}\n\n    ids = [\'87a68652-b76a-450f-b44f-e5192243218e\',\n           \'af9deddf-daf3-4e93-8f36-21437b52817c\',\n           \'0c5d304f-7cd8-425e-8b0f-0f72b4fc4e6e\',\n           \'9dcfc6f6-aa36-47ef-9501-177fa39e7e7d\',\n           \'dda8b22e-27f5-40a5-99d4-b94810f545d0\']\n\n    def example_dataset(product, id, center_time):\n        result = Dataset(example_product(product),\n                         dict(id=id, grid_spatial=example_grid_spatial()),\n                         uris=[\'file://test.zzz\'])\n        result.center_time = center_time\n        return result\n\n    def search(*args, **kwargs):\n        product = kwargs[\'product\']\n        if product == \'ls8_nbar_albers\':\n            return [example_dataset(product, ids[0], datetime(2014, 2, 7, 23, 57, 26)),\n                    example_dataset(product, ids[1], datetime(2014, 1, 1, 23, 57, 26))]\n        elif product == \'ls8_pq_albers\':\n            return [example_dataset(product, ids[2], datetime(2014, 2, 7, 23, 57, 26))]\n        elif product == \'ls7_nbar_albers\':\n            return [example_dataset(product, ids[3], datetime(2014, 1, 22, 23, 57, 36))]\n        elif product == \'ls7_pq_albers\':\n            return [example_dataset(product, ids[4], datetime(2014, 1, 22, 23, 57, 36))]\n        else:\n            return []\n\n    result.index.products.get_all = lambda: [example_product(x) for x in PRODUCT_LIST]\n    result.index.products.get_by_name = example_product\n    result.index.datasets.search = search\n    return result\n\n\n@pytest.fixture\ndef query():\n    return {\n        \'time\': (\'2014-01-01\', \'2014-03-01\'),\n        \'lat\': (-35.2, -35.21),\n        \'lon\': (149.0, 149.01)\n    }\n\n\n##################################\n# Virtual Product Tests Start Here\n\ndef test_name_resolution(cloud_free_nbar):\n    for prod in cloud_free_nbar[\'collate\']:\n        assert callable(prod[\'transform\'])\n\n\ndef test_str(cloud_free_nbar):\n    assert str(cloud_free_nbar)\n\n\ndef test_output_measurements(cloud_free_nbar, dc):\n    measurements = cloud_free_nbar.output_measurements({product.name: product\n                                                        for product in dc.index.products.get_all()})\n    assert \'blue\' in measurements\n    assert \'green\' in measurements\n    assert \'source_index\' in measurements\n    assert \'pixelquality\' not in measurements\n\n\ndef test_group_datasets(cloud_free_nbar, dc, query):\n    bag = cloud_free_nbar.query(dc, **query)\n    box = cloud_free_nbar.group(bag, **query)\n\n    [time] = box.box.shape\n    assert time == 2\n\n\ndef test_explode(dc, query):\n    collate = construct_from_yaml(""""""\n        collate:\n            - product: ls8_nbar_albers\n            - product: ls7_nbar_albers\n    """""")\n\n    bag = collate.query(dc, **query)\n\n    # three datasets in two products\n    assert len(list(bag.contained_datasets())) == 3\n\n    bags = list(bag.explode())\n\n    # each element should contain one dataset\n    assert len(bags) == 3\n\n    for bag in bags:\n        assert len(list(bag.contained_datasets())) == 1\n\n        # the smaller bags should have the same structure\n        assert \'collate\' in bag.bag\n\n        # there were two products (only one of them should have the single dataset)\n        assert len(bag.bag[\'collate\']) == 2\n\n\ndef test_load_data(cloud_free_nbar, dc, query):\n    with mock.patch(\'datacube.virtual.impl.Datacube\') as mock_datacube:\n        mock_datacube.load_data = load_data\n        mock_datacube.group_datasets = group_datasets\n        data = cloud_free_nbar.load(dc, **query)\n\n    assert \'blue\' in data\n    assert \'green\' in data\n    assert \'source_index\' in data\n    assert \'pixelquality\' not in data\n\n    assert numpy.array_equal(numpy.unique(data.blue.values), numpy.array([-999]))\n    assert numpy.array_equal(numpy.unique(data.green.values), numpy.array([-999]))\n    assert numpy.array_equal(numpy.unique(data.source_index.values), numpy.array([0, 1]))\n\n\ndef test_misspelled_product(dc, query):\n    ls8_nbar = construct_from_yaml(""product: ls8_nbar"")\n\n    with pytest.raises(VirtualProductException):\n        ls8_nbar.query(dc, **query)\n\n#####################################\n# Virtual Product Transform Tests\n#####################################\n\n\ndef test_select_transform(dc, query):\n    select = construct_from_yaml(""""""\n        transform: select\n        measurement_names:\n            - green\n        input:\n            product: ls8_nbar_albers\n            measurements: [blue, green]\n    """""")\n\n    with mock.patch(\'datacube.virtual.impl.Datacube\') as mock_datacube:\n        mock_datacube.load_data = load_data\n        mock_datacube.group_datasets = group_datasets\n        data = select.load(dc, **query)\n\n    assert \'green\' in data\n    assert \'blue\' not in data\n\n\ndef test_rename_transform(dc, query):\n    rename = construct_from_yaml(""""""\n        transform: rename\n        measurement_names:\n            green: verde\n        input:\n            product: ls8_nbar_albers\n            measurements: [blue, green]\n    """""")\n\n    with mock.patch(\'datacube.virtual.impl.Datacube\') as mock_datacube:\n        mock_datacube.load_data = load_data\n        mock_datacube.group_datasets = group_datasets\n        data = rename.load(dc, **query)\n\n    assert \'verde\' in data\n    assert \'blue\' in data\n    assert \'green\' not in data\n\n\ndef test_to_float_transform(dc, query):\n    to_float = construct_from_yaml(""""""\n        transform: to_float\n        input:\n            product: ls8_nbar_albers\n            measurements: [blue]\n    """""")\n\n    with mock.patch(\'datacube.virtual.impl.Datacube\') as mock_datacube:\n        mock_datacube.load_data = load_data\n        mock_datacube.group_datasets = group_datasets\n        data = to_float.load(dc, **query)\n\n    assert numpy.all(numpy.isnan(data.blue.values))\n    assert data.blue.dtype == \'float32\'\n\n\ndef test_vp_handles_product_aliases(dc, query):\n    verde = construct_from_yaml(""""""\n        product: ls8_nbar_albers\n        measurements: [verde]\n    """""")\n\n    measurements = verde.output_measurements({product.name: product\n                                              for product in dc.index.products.get_all()})\n    assert \'verde\' in measurements\n    assert \'green\' not in measurements\n\n    with mock.patch(\'datacube.virtual.impl.Datacube\') as mock_datacube:\n        mock_datacube.load_data = load_data\n        mock_datacube.group_datasets = group_datasets\n        data = verde.load(dc, **query)\n\n    assert \'verde\' in data\n    assert \'green\' not in data\n\n\ndef test_expressions_transform(dc, query):\n    bluegreen = construct_from_yaml(""""""\n        transform: expressions\n        output:\n            bluegreen:\n                formula: blue + green\n                nodata: -999\n            blue: blue\n        input:\n            product: ls8_nbar_albers\n            measurements: [blue, green]\n    """""")\n\n    measurements = bluegreen.output_measurements({product.name: product\n                                                  for product in dc.index.products.get_all()})\n    assert \'bluegreen\' in measurements\n\n    with mock.patch(\'datacube.virtual.impl.Datacube\') as mock_datacube:\n        mock_datacube.load_data = load_data\n        mock_datacube.group_datasets = group_datasets\n        data = bluegreen.load(dc, **query)\n\n    assert \'bluegreen\' in data\n    assert numpy.all((data.bluegreen == -999).values)\n    assert \'blue\' in data\n    assert numpy.all((data.blue == -999).values)\n    assert \'green\' not in data\n\n    bluegreen = construct_from_yaml(""""""\n        transform: expressions\n        output:\n            bluegreen:\n                formula: blue + green\n                dtype: float32\n            blue: blue\n        input:\n            product: ls8_nbar_albers\n            measurements: [blue, green]\n    """""")\n\n    measurements = bluegreen.output_measurements({product.name: product\n                                                  for product in dc.index.products.get_all()})\n    assert \'bluegreen\' in measurements\n    assert measurements[\'bluegreen\'].dtype == numpy.dtype(\'float32\')\n    assert numpy.isnan(measurements[\'bluegreen\'].nodata)\n\n    with mock.patch(\'datacube.virtual.impl.Datacube\') as mock_datacube:\n        mock_datacube.load_data = load_data\n        mock_datacube.group_datasets = group_datasets\n        data = bluegreen.load(dc, **query)\n\n    assert \'bluegreen\' in data\n    assert numpy.all(numpy.isnan(data.bluegreen.values))\n    assert \'blue\' in data\n    assert numpy.all((data.blue == -999).values)\n    assert \'green\' not in data\n\n\ndef test_aggregate(dc, query, catalog):\n    aggr = catalog[\'mean_blue\']\n\n    measurements = aggr.output_measurements({product.name: product\n                                             for product in dc.index.products.get_all()})\n    assert \'blue\' in measurements\n\n    with mock.patch(\'datacube.virtual.impl.Datacube\') as mock_datacube, warnings.catch_warnings():\n        warnings.simplefilter(""ignore"")\n        mock_datacube.load_data = load_data\n        mock_datacube.group_datasets = group_datasets\n        data = aggr.load(dc, **query)\n\n    assert data.time.shape == (2,)\n\n\ndef test_register(dc, query):\n    class BlueGreen(Transformation):\n        def compute(self, data):\n            return (data.blue + data.green).to_dataset(name=\'bluegreen\').assign_attrs(data.blue.attrs)\n\n        def measurements(self, input_measurements):\n            bluegreen = deepcopy(input_measurements[\'blue\'])\n            bluegreen.name = \'bluegreen\'\n            return {\'bluegreen\': bluegreen}\n\n    resolver = deepcopy(DEFAULT_RESOLVER)\n    resolver.register(\'transform\', \'bluegreen\', BlueGreen)\n\n    bluegreen = construct_from_yaml(""""""\n        transform: bluegreen\n        input:\n            product: ls8_nbar_albers\n            measurements: [blue, green]\n    """""", name_resolver=resolver)\n\n    measurements = bluegreen.output_measurements({product.name: product\n                                                  for product in dc.index.products.get_all()})\n    assert \'bluegreen\' in measurements\n\n    with mock.patch(\'datacube.virtual.impl.Datacube\') as mock_datacube:\n        mock_datacube.load_data = load_data\n        mock_datacube.group_datasets = group_datasets\n        data = bluegreen.load(dc, **query)\n\n    assert \'bluegreen\' in data\n'"
tests/drivers/test_rio_reader.py,3,"b'"""""" Tests for new RIO reader driver\n""""""\nfrom datetime import datetime\nfrom concurrent.futures import ThreadPoolExecutor, Future\nimport numpy as np\nimport rasterio\nfrom affine import Affine\nimport pytest\nimport warnings\n\nfrom datacube.drivers.rio._reader import (\n    RDEntry,\n    _dc_crs,\n    _rio_uri,\n    _rio_band_idx,\n    _roi_to_window,\n)\nfrom datacube.testutils.geom import SAMPLE_WKT_WITHOUT_AUTHORITY, epsg3857\nfrom datacube.testutils.iodriver import (\n    NetCDF, GeoTIFF, mk_band, mk_rio_driver, open_reader\n)\n\n\ndef test_rio_rd_entry():\n    rde = RDEntry()\n\n    assert \'file\' in rde.protocols\n    assert \'s3\' in rde.protocols\n\n    assert GeoTIFF in rde.formats\n    assert NetCDF in rde.formats\n\n    assert rde.supports(\'file\', NetCDF) is True\n    assert rde.supports(\'s3\', NetCDF) is False\n\n    assert rde.supports(\'file\', GeoTIFF) is True\n    assert rde.supports(\'s3\', GeoTIFF) is True\n\n    assert rde.new_instance({}) is not None\n    assert rde.new_instance({\'max_workers\': 2}) is not None\n\n    with pytest.raises(ValueError):\n        rde.new_instance({\'pool\': []})\n\n    # check pool re-use\n    pool = ThreadPoolExecutor(max_workers=1)\n    rdr = rde.new_instance({\'pool\': pool})\n    assert rdr._pool is pool\n\n\ndef test_rd_internals_crs():\n    from rasterio.crs import CRS as RioCRS\n\n    assert _dc_crs(None) is None\n    assert _dc_crs(RioCRS()) is None\n    assert _dc_crs(RioCRS.from_epsg(3857)).epsg == 3857\n    assert _dc_crs(RioCRS.from_wkt(SAMPLE_WKT_WITHOUT_AUTHORITY)).epsg is None\n\n\ndef test_rd_internals_roi():\n    s_ = np.s_\n\n    assert _roi_to_window(None, (1, 1)) is None\n    assert _roi_to_window(s_[:, :], (1, 10)) == ((0, 1), (0, 10))\n    assert _roi_to_window(s_[1:, -1:], (5, 10)) == ((1, 5), (9, 10))\n    assert _roi_to_window(s_[:3, 3:-1], (5, 10)) == ((0, 3), (3, 9))\n\n\ndef test_rd_internals_bidx(data_folder):\n    base = ""file://"" + str(data_folder) + ""/metadata.yml""\n    bi = mk_band(\'a\',\n                 base,\n                 path=""multi_doc.nc"",\n                 format=NetCDF,\n                 timestamp=datetime.utcfromtimestamp(1),\n                 layer=\'a\')\n    assert bi.uri.endswith(\'multi_doc.nc\')\n\n    rio_fname = _rio_uri(bi)\n    assert rio_fname.startswith(\'NETCDF:\')\n\n    with rasterio.open(rio_fname) as src:\n        # timestamp search was removed\n        with pytest.raises(DeprecationWarning):\n            _rio_band_idx(bi, src)\n\n        # extract from .uri\n        bi.uri = bi.uri + ""#part=5""\n        assert _rio_band_idx(bi, src) == 5\n\n        # extract from .band\n        bi.band = 33\n        assert _rio_band_idx(bi, src) == 33\n\n    bi = mk_band(\'a\', base, path=""test.tif"", format=GeoTIFF)\n\n    with rasterio.open(_rio_uri(bi), \'r\') as src:\n        # should default to 1\n        assert _rio_band_idx(bi, src) == 1\n\n        # layer containing int should become index\n        bi = mk_band(\'a\', base, path=""test.tif"", format=GeoTIFF, layer=2)\n        assert _rio_band_idx(bi, src) == 2\n\n        # band is the keyword\n        bi = mk_band(\'a\', base, path=""test.tif"", format=GeoTIFF, band=3)\n        assert _rio_band_idx(bi, src) == 3\n\n    # TODO: make single time-slice netcdf, for now pretend that this tiff is netcdf\n    with rasterio.open(str(data_folder)+""/sample_tile_151_-29.tif"", \'r\') as src:\n        bi = mk_band(\'a\', base, path=\'sample_tile_151_-29.tif\', format=NetCDF)\n        assert src.count == 1\n        assert _rio_band_idx(bi, src) == 1\n\n\ndef test_rd_internals_uri():\n    base = ""file:///some/path/""\n\n    bi = mk_band(\'green\', base, path=""f.tiff"", format=GeoTIFF)\n    assert _rio_uri(bi) == \'/some/path/f.tiff\'\n\n    bi = mk_band(\'x\', base, path=""x.nc"", layer=\'x\', format=NetCDF)\n    assert _rio_uri(bi) == \'NETCDF:""/some/path/x.nc"":x\'\n\n    bi = mk_band(\'jj\', \'s3://some/path/config.yml\', ""jj.tiff"")\n    assert _rio_uri(bi) == \'s3://some/path/jj.tiff\'\n    assert _rio_uri(bi) is bi.uri\n\n\ndef test_rio_driver_fail_to_open():\n    nosuch_uri = \'file:///this-file-hopefully/doesnot/exist-4718193.tiff\'\n    rde = RDEntry()\n    rdr = rde.new_instance({})\n\n    assert rdr is not None\n\n    load_ctx = rdr.new_load_context(iter([]), None)\n    load_ctx = rdr.new_load_context(iter([]), load_ctx)\n\n    bi = mk_band(\'green\', nosuch_uri)\n    assert bi.uri == nosuch_uri\n    fut = rdr.open(bi, load_ctx)\n\n    assert isinstance(fut, Future)\n\n    with pytest.raises(IOError):\n        fut.result()\n\n\ndef test_rio_driver_open(data_folder):\n    base = ""file://"" + str(data_folder) + ""/metadata.yml""\n\n    rdr = mk_rio_driver()\n    assert rdr is not None\n\n    load_ctx = rdr.new_load_context(iter([]), None)\n    bi = mk_band(\'b1\', base, path=""test.tif"", format=GeoTIFF)\n    load_ctx = rdr.new_load_context(iter([bi]), load_ctx)\n    fut = rdr.open(bi, load_ctx)\n    assert isinstance(fut, Future)\n\n    src = fut.result()\n    assert src.crs is not None\n    assert src.transform is not None\n    assert src.crs.epsg == 4326\n    assert src.shape == (2000, 4000)\n    assert src.nodata == -999\n    assert src.dtype == np.dtype(np.int16)\n\n    xx = src.read().result()\n    assert xx.shape == src.shape\n    assert xx.dtype == src.dtype\n\n    # check overrides\n    bi = mk_band(\'b1\', base, path=""zeros_no_geo_int16_7x3.tif"", format=GeoTIFF, nodata=None)\n\n    # First verify that missing overrides in the band doesn\'t cause issues\n    assert bi.crs is None\n    assert bi.transform is None\n    assert bi.nodata is None\n\n    load_ctx = rdr.new_load_context(iter([bi]), load_ctx)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\'ignore\', rasterio.errors.NotGeoreferencedWarning)\n        src = rdr.open(bi, load_ctx).result()\n\n    assert src.crs is None\n    assert src.transform is None\n    assert src.nodata is None\n\n    # Now test that overrides work\n    bi.crs = epsg3857\n    bi.transform = Affine.translation(10, 100)\n    bi.nodata = -33\n\n    load_ctx = rdr.new_load_context(iter([bi]), load_ctx)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\'ignore\', rasterio.errors.NotGeoreferencedWarning)\n        src = rdr.open(bi, load_ctx).result()\n\n    assert src.crs == bi.crs\n    assert src.transform == bi.transform\n    assert src.nodata == bi.nodata\n\n\ndef test_testutils_iodriver(data_folder):\n    fpath = str(data_folder) + \'/test.tif\'\n    src = open_reader(fpath)\n    assert src is not None\n    assert src.crs is not None\n    assert src.transform is not None\n    assert src.crs.epsg == 4326\n    assert src.shape == (2000, 4000)\n    assert src.nodata == -999\n    assert src.dtype == np.dtype(np.int16)\n'"
tests/index/__init__.py,0,"b'# coding=utf-8\n""""""\nModule\n""""""\n'"
tests/index/test_api_index_dataset.py,0,"b'# coding=utf-8\n\n\nimport datetime\nfrom collections import namedtuple\nfrom contextlib import contextmanager\nfrom copy import deepcopy\n\nimport pytest\nfrom uuid import UUID\n\nfrom datacube.index._datasets import DatasetResource\nfrom datacube.index.exceptions import DuplicateRecordError\nfrom datacube.model import DatasetType, MetadataType, Dataset\nfrom datacube.utils.changes import DocumentMismatchError\n\n_nbar_uuid = UUID(\'f2f12372-8366-11e5-817e-1040f381a756\')\n_ortho_uuid = UUID(\'5cf41d98-eda9-11e4-8a8e-1040f381a756\')\n_telemetry_uuid = UUID(\'4ec8fe97-e8b9-11e4-87ff-1040f381a756\')\n\n# An NBAR with source datasets. Many fields have been removed to keep it semi-focused to our ingest test.\n_EXAMPLE_NBAR = {\n    \'id\': str(_nbar_uuid),\n    \'product_type\': \'nbar_brdf\',\n    \'checksum_path\': \'package.sha1\',\n    \'ga_label\': \'LS8_OLITIRS_NBAR_P54_GALPGS01-002_112_079_20140126\',\n    \'ga_level\': \'P54\',\n    \'size_bytes\': 4550,\n    \'platform\': {\'code\': \'LANDSAT_8\'},\n    \'creation_dt\': datetime.datetime(2014, 1, 26, 2, 5, 23, 126373).isoformat(),\n    \'instrument\': {\'name\': \'OLI_TIRS\'},\n    \'format\': {\'name\': \'GeoTIFF\'},\n    \'extent\': {\n        \'center_dt\': datetime.datetime(2014, 1, 26, 2, 5, 23, 126373).isoformat(),\n        \'coord\': {\n            \'ul\': {\'lat\': -26.37259, \'lon\': 116.58914},\n            \'lr\': {\'lat\': -28.48062, \'lon\': 118.96145},\n            \'ur\': {\'lat\': -26.36025, \'lon\': 118.92432},\n            \'ll\': {\'lat\': -28.49412, \'lon\': 116.58121}\n        }\n    },\n    \'lineage\': {\n        \'machine\': {},\n        \'source_datasets\': {\n            \'ortho\': {\n                \'product_level\': \'L1T\',\n                \'product_type\': \'ortho\',\n                \'id\': str(_ortho_uuid),\n                \'usgs\': {\n                    \'scene_id\': \'LC81120792014026ASA00\'\n                },\n                \'extent\': {\n                    \'center_dt\': datetime.datetime(2014, 1, 26, 2, 5, 23, 126373).isoformat(),\n                    \'coord\': {\n                        \'ul\': {\'lat\': -26.37259, \'lon\': 116.58914},\n                        \'lr\': {\'lat\': -28.48062, \'lon\': 118.96145},\n                        \'ur\': {\'lat\': -26.36025, \'lon\': 118.92432},\n                        \'ll\': {\'lat\': -28.49412, \'lon\': 116.58121}\n                    }\n                },\n                \'size_bytes\': 1854924494,\n                \'platform\': {\n                    \'code\': \'LANDSAT_8\'},\n                \'creation_dt\': datetime.datetime(2015, 4, 7, 0, 58, 8).isoformat(),\n                \'instrument\': {\'name\': \'OLI_TIRS\'},\n                \'checksum_path\': \'package.sha1\',\n                \'ga_label\': \'LS8_OLITIRS_OTH_P51_GALPGS01-002_112_079_20140126\',\n                \'grid_spatial\': {\n                    \'projection\': {\n                        \'spatial_reference\': \'EPSG:28350\',\n                        \'resampling_option\': \'CUBIC_CONVOLUTION\',\n                        \'geo_ref_points\': {\n                            \'ul\': {\'y\': 7082987.5, \'x\': 459012.5},\n                            \'lr\': {\'y\': 6847987.5, \'x\': 692012.5},\n                            \'ur\': {\'y\': 7082987.5, \'x\': 692012.5},\n                            \'ll\': {\'y\': 6847987.5, \'x\': 459012.5}\n                        },\n                        \'orientation\': \'NORTH_UP\',\n                    }\n                },\n                \'acquisition\': {\n                    \'groundstation\': {\n                        \'code\': \'ASA\',\n                        \'eods_domain_code\': \'002\',\n                        \'label\': \'Alice Springs\'\n                    }\n                },\n                \'format\': {\'name\': \'GEOTIFF\'},\n                \'lineage\': {\n                    \'algorithm\': {\n                        \'name\': \'LPGS\',\n                        \'parameters\': {},\n                        \'version\': \'2.4.0\'\n                    },\n                    \'machine\': {},\n                    \'source_datasets\': {\n                        \'satellite_telemetry_data\': {\n                            \'product_type\': \'satellite_telemetry_data\',\n                            \'checksum_path\': \'package.sha1\',\n                            \'id\': str(_telemetry_uuid),\n                            \'ga_label\': \'LS8_OLITIRS_STD-MD_P00_LC81160740742015089ASA00_\'\n                                        \'116_074_20150330T022553Z20150330T022657\',\n\n                            \'ga_level\': \'P00\',\n                            \'size_bytes\': 637660782,\n                            \'platform\': {\n                                \'code\': \'LANDSAT_8\'},\n                            \'creation_dt\': datetime.datetime(2015, 4, 22, 6, 32, 4).isoformat(),\n                            \'instrument\': {\'name\': \'OLI_TIRS\'},\n                            \'format\': {\n                                \'name\': \'MD\'},\n                            \'lineage\': {\n                                \'source_datasets\': {}\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\n_EXAMPLE_METADATA_TYPE = MetadataType(\n    {\n        \'name\': \'eo\',\n        \'dataset\': dict(\n            id=[\'id\'],\n            label=[\'ga_label\'],\n            creation_time=[\'creation_dt\'],\n            measurements=[\'image\', \'bands\'],\n            sources=[\'lineage\', \'source_datasets\']\n        )\n    },\n    dataset_search_fields={}\n)\n\n_EXAMPLE_DATASET_TYPE = DatasetType(\n    _EXAMPLE_METADATA_TYPE,\n    {\n        \'name\': \'eo\',\n        \'description\': """",\n        \'metadata_type\': \'eo\',\n        \'metadata\': {}\n    }\n)\n\n\ndef _build_dataset(doc):\n    sources = {name: _build_dataset(src) for name, src in doc[\'lineage\'][\'source_datasets\'].items()}\n    return Dataset(_EXAMPLE_DATASET_TYPE, doc, uris=[\'file://test.zzz\'], sources=sources)\n\n\n_EXAMPLE_NBAR_DATASET = _build_dataset(_EXAMPLE_NBAR)\n\nDatasetRecord = namedtuple(\'DatasetRecord\', [\'id\', \'metadata\', \'dataset_type_ref\', \'uris\',\n                                             \'added\', \'added_by\', \'archived\'])\n\n\nclass MockIndex(object):\n    def __init__(self, db):\n        self._db = db\n\n\nclass MockDb(object):\n    def __init__(self):\n        self.dataset = {}\n        self.dataset_source = set()\n\n    @contextmanager\n    def begin(self):\n        yield self\n\n    @contextmanager\n    def connect(self):\n        yield self\n\n    def get_dataset(self, id):\n        return self.dataset.get(id, None)\n\n    def get_locations(self, dataset):\n        return [\'file:xxx\']\n\n    def datasets_intersection(self, ids):\n        return [k for k in ids if k in self.dataset]\n\n    def insert_dataset_location(self, *args, **kwargs):\n        return\n\n    def insert_dataset(self, metadata_doc, dataset_id, dataset_type_id):\n        # Will we pretend this one was already ingested?\n        if dataset_id in self.dataset:\n            raise DuplicateRecordError(\'already ingested\')\n\n        self.dataset[dataset_id] = DatasetRecord(dataset_id, deepcopy(metadata_doc), dataset_type_id,\n                                                 None, None, None, None)\n        return True\n\n    def insert_dataset_source(self, classifier, dataset_id, source_dataset_id):\n        self.dataset_source.add((classifier, dataset_id, source_dataset_id))\n\n\nclass MockTypesResource(object):\n    def __init__(self, type_):\n        self.type = type_\n\n    def get(self, *args, **kwargs):\n        return self.type\n\n    def get_by_name(self, *args, **kwargs):\n        return self.type\n\n\ndef test_index_dataset():\n    mock_db = MockDb()\n    mock_types = MockTypesResource(_EXAMPLE_DATASET_TYPE)\n    datasets = DatasetResource(mock_db, mock_types)\n    dataset = datasets.add(_EXAMPLE_NBAR_DATASET)\n\n    ids = {d.id for d in mock_db.dataset.values()}\n    assert ids == {_nbar_uuid, _ortho_uuid, _telemetry_uuid}\n\n    # Three datasets (ours and the two embedded source datasets)\n    assert len(mock_db.dataset) == 3\n\n    # Our three datasets should be linked together\n    # Nbar -> Ortho -> Telemetry\n    assert len(mock_db.dataset_source) == 2\n    assert mock_db.dataset_source == {\n        (\'ortho\', _nbar_uuid, _ortho_uuid),\n        (\'satellite_telemetry_data\', _ortho_uuid, _telemetry_uuid)\n    }\n\n    # Nothing ingested, because we reported the first as already ingested.\n    dataset = datasets.add(_EXAMPLE_NBAR_DATASET)\n    assert len(mock_db.dataset) == 3\n    assert len(mock_db.dataset_source) == 2\n\n\ndef test_index_already_ingested_source_dataset():\n    mock_db = MockDb()\n    mock_types = MockTypesResource(_EXAMPLE_DATASET_TYPE)\n    datasets = DatasetResource(mock_db, mock_types)\n    dataset = datasets.add(_EXAMPLE_NBAR_DATASET.sources[\'ortho\'])\n\n    assert len(mock_db.dataset) == 2\n    assert len(mock_db.dataset_source) == 1\n\n    dataset = datasets.add(_EXAMPLE_NBAR_DATASET)\n    assert len(mock_db.dataset) == 3\n    assert len(mock_db.dataset_source) == 2\n\n\ndef test_index_two_levels_already_ingested():\n    mock_db = MockDb()\n    mock_types = MockTypesResource(_EXAMPLE_DATASET_TYPE)\n    datasets = DatasetResource(mock_db, mock_types)\n    dataset = datasets.add(_EXAMPLE_NBAR_DATASET.sources[\'ortho\'].sources[\'satellite_telemetry_data\'])\n\n    assert len(mock_db.dataset) == 1\n    assert len(mock_db.dataset_source) == 0\n\n    dataset = datasets.add(_EXAMPLE_NBAR_DATASET)\n    assert len(mock_db.dataset) == 3\n    assert len(mock_db.dataset_source) == 2\n'"
tests/index/test_fields.py,0,"b'# coding=utf-8\n""""""\nModule\n""""""\n\nfrom datacube.drivers.postgres._fields import SimpleDocField, NumericRangeDocField, parse_fields, RangeDocField, \\\n    IntDocField\nfrom datacube.drivers.postgres._api import _split_uri\nfrom datacube.drivers.postgres._schema import DATASET\nfrom datacube.model import Range\nimport pytest\n\n\ndef _assert_same(obj1, obj2):\n    assert obj1.__class__ == obj2.__class__\n    assert obj1.__dict__ == obj2.__dict__\n\n\ndef test_split_uri():\n    assert _split_uri(\'http://test.com/something.txt\') == (\'http\', \'//test.com/something.txt\')\n    assert _split_uri(\'eods:LS7_ETM_SYS_P31_GALPGS01-002_101_065_20160127\') == (\n        \'eods\', \'LS7_ETM_SYS_P31_GALPGS01-002_101_065_20160127\')\n    assert _split_uri(\'file://rhe-test-dev.prod.lan/data/fromASA/LANDSAT-7.89274.S4A2C1D3R3\') == (\n        \'file\', \'//rhe-test-dev.prod.lan/data/fromASA/LANDSAT-7.89274.S4A2C1D3R3\')\n    assert _split_uri(\'file:///C:/tmp/first/something.yaml\') == (\'file\', \'///C:/tmp/first/something.yaml\')\n\n    with pytest.raises(ValueError):\n        _split_uri(\'/no/semicolon\')\n\n\ndef test_get_single_field():\n    fields = parse_fields({\n        \'platform\': {\n            \'description\': \'Satellite\',\n            \'offset\': [\'platform\', \'code\']\n        },\n        \'instrument\': {\n            \'offset\': [\'instrument\', \'name\']\n        }\n    }, DATASET.c.metadata)\n    assert set(fields.keys()) == {\'platform\', \'instrument\'}\n    field = fields[\'platform\']\n    _assert_same(\n        field,\n        SimpleDocField(\n            \'platform\', \'Satellite\',\n            DATASET.c.metadata,\n            True,\n            offset=[\'platform\', \'code\']\n        )\n    )\n    assert isinstance(field, SimpleDocField)\n    assert field.extract({\'platform\': {\'code\': \'turtle\'}}) == \'turtle\'\n    assert field.extract({\'platform\': {\'code\': None}}) is None\n    assert field.extract({}) is None\n\n\ndef test_get_multi_field():\n    fields = parse_fields({\n        \'orbit\': {\n            \'description\': \'Orbit number\',\n            \'type\': \'integer\',\n            \'offset\': [\n                [\'acquisition\', \'platform_orbit\'],\n                [\'orbit\']\n            ]\n        }\n    }, DATASET.c.metadata)\n    assert set(fields.keys()) == {\'orbit\'}\n\n    field = fields[\'orbit\']\n    _assert_same(\n        field,\n        IntDocField(\n            \'orbit\', \'Orbit number\',\n            DATASET.c.metadata,\n            True,\n            offset=[\n                [\'acquisition\', \'platform_orbit\'],\n                [\'orbit\']\n            ]\n        )\n    )\n    assert isinstance(field, SimpleDocField)\n    assert field.extract({\'platform\': {\'code\': \'turtle\'}}) is None\n    assert field.extract({\'acquisition\': {\'platform_orbit\': 5}}) == 5\n    assert field.extract({\'orbit\': 10}) == 10\n    # It chooses the first listed field with a non-null value\n    assert field.extract({\'orbit\': 10, \'acquisition\': {\'platform_orbit\': 5}}) == 5\n\n\ndef test_get_range_field():\n    storage_fields = parse_fields({\n        \'lat\': {\n            \'type\': \'float-range\',\n            \'max_offset\': [[\'extents\', \'geospatial_lat_max\']],\n            \'min_offset\': [\n                [\'extents\', \'geospatial_lat_other\'],\n                [\'extents\', \'geospatial_lat_min\']\n            ],\n        },\n    }, DATASET.c.metadata)\n    field = storage_fields[\'lat\']\n    _assert_same(\n        field,\n        NumericRangeDocField(\n            \'lat\', None,\n            DATASET.c.metadata,\n            True,\n            max_offset=[\n                [\'extents\', \'geospatial_lat_max\']\n            ],\n            min_offset=[\n                [\'extents\', \'geospatial_lat_other\'],\n                [\'extents\', \'geospatial_lat_min\']\n            ],\n        )\n    )\n    assert isinstance(field, RangeDocField)\n    extracted = field.extract({\'extents\': {\'geospatial_lat_min\': 2, \'geospatial_lat_max\': 4}})\n    assert extracted == Range(begin=2, end=4)\n'"
tests/index/test_query.py,0,"b'# coding=utf-8\n""""""\nModule\n""""""\n\nfrom psycopg2.extras import NumericRange\n\nfrom datacube.drivers.postgres._fields import SimpleDocField, RangeBetweenExpression, EqualsExpression, \\\n    NumericRangeDocField\nfrom datacube.index.fields import to_expressions\nfrom datacube.model import Range\n\n\ndef test_build_query_expressions():\n    _sat_field = SimpleDocField(\'platform\', None, None, None)\n    _sens_field = SimpleDocField(\'instrument\', None, None, None)\n    _lat_field = NumericRangeDocField(\'lat\', None, None, None)\n    _fields = {\n        \'platform\': _sat_field,\n        \'instrument\': _sens_field,\n        \'lat\': _lat_field\n    }\n\n    assert [EqualsExpression(_sat_field, ""LANDSAT_8"")] == to_expressions(_fields.get, platform=""LANDSAT_8"")\n    assert [RangeBetweenExpression(\n        _lat_field, 4, 23.0, _range_class=NumericRange\n    )] == to_expressions(_fields.get, lat=Range(4, 23))\n'"
tests/index/test_validate_dataset_type.py,0,"b'# coding=utf-8\n""""""\nModule\n""""""\n\nfrom copy import deepcopy\n\nimport pytest\n\nfrom datacube.model import DatasetType\nfrom datacube.utils import InvalidDocException\n\nonly_mandatory_fields = {\n    \'name\': \'ls7_nbar\',\n    \'description\': \'description\',\n    \'metadata_type\': \'eo\',\n    \'metadata\': {\'product_type\': \'test\'}\n}\n\n\n@pytest.mark.parametrize(""valid_dataset_type_update"", [\n    {},\n    {\'storage\': {\'crs\': \'EPSG:3577\'}},\n    # With the optional properties\n    {\'measurements\': [{\'name\': \'band_70\', \'dtype\': \'int16\', \'nodata\': -999, \'units\': \'1\'}]}\n])\ndef test_accepts_valid_docs(valid_dataset_type_update):\n    doc = deepcopy(only_mandatory_fields)\n    doc.update(valid_dataset_type_update)\n    # Should have no errors.\n    DatasetType.validate(doc)\n\n\ndef test_incomplete_dataset_type_invalid():\n    # Invalid: An empty doc.\n    with pytest.raises(InvalidDocException) as e:\n        DatasetType.validate({})\n\n\n# Changes to the above dict that should render it invalid.\n@pytest.mark.parametrize(""invalid_dataset_type_update"", [\n    # Mandatory\n    {\'name\': None},\n    # Should be an object\n    {\'storage\': \'s\'},\n    # Should be a string\n    {\'description\': 123},\n    # Unknown property\n    {\'asdf\': \'asdf\'},\n    # Name must have alphanumeric & underscores only.\n    {\'name\': \' whitespace \'},\n    {\'name\': \'with-dashes\'},\n    # Mappings\n    {\'mappings\': {}},\n    {\'mappings\': \'\'}\n])\ndef test_rejects_invalid_docs(invalid_dataset_type_update):\n    mapping = deepcopy(only_mandatory_fields)\n    mapping.update(invalid_dataset_type_update)\n    with pytest.raises(InvalidDocException) as e:\n        DatasetType.validate(mapping)\n\n\n@pytest.mark.parametrize(""valid_dataset_type_measurement"", [\n    {\n        \'name\': \'1\',\n        \'dtype\': \'int16\',\n        \'units\': \'1\',\n        \'nodata\': -999\n    },\n    # With the optional properties\n    {\n        \'name\': \'red\',\n        \'nodata\': -999,\n        \'units\': \'1\',\n        \'dtype\': \'int16\',\n        # TODO: flags/spectral\n    },\n])\ndef test_accepts_valid_measurements(valid_dataset_type_measurement):\n    mapping = deepcopy(only_mandatory_fields)\n    mapping[\'measurements\'] = [valid_dataset_type_measurement]\n    # Should have no errors.\n    DatasetType.validate(mapping)\n\n\n# Changes to the above dict that should render it invalid.\n@pytest.mark.parametrize(""invalid_dataset_type_measurement"", [\n    # no name\n    {\'nodata\': -999},\n    # nodata must be numeric\n    {\'name\': \'red\', \'nodata\': \'-999\'},\n    # Limited dtype options\n    {\'name\': \'red\', \'dtype\': \'asdf\'},\n    {\'name\': \'red\', \'dtype\': \'intt13\'},\n    {\'name\': \'red\', \'dtype\': 13},\n    # Unknown property\n    {\'name\': \'red\', \'asdf\': \'asdf\'},\n])\ndef test_rejects_invalid_measurements(invalid_dataset_type_measurement):\n    mapping = deepcopy(only_mandatory_fields)\n    mapping[\'measurements\'] = {\'10\': invalid_dataset_type_measurement}\n    with pytest.raises(InvalidDocException) as e:\n        DatasetType.validate(mapping)\n'"
tests/scripts/__init__.py,0,"b'# coding=utf-8\n""""""\nModule\n""""""\n'"
tests/scripts/test_search_tool.py,0,"b'# coding=utf-8\n""""""\nModule\n""""""\n\nimport datetime\n\nfrom dateutil import tz\nfrom psycopg2._range import Range, NumericRange\n\nfrom datacube.index.fields import Field\nfrom datacube.scripts.search_tool import write_csv, write_pretty\n\n\nclass MockFile():\n    def __init__(self):\n        self.vals = []\n\n    def write(self, s):\n        self.vals.append(s)\n\n    def flush(self):\n        pass\n\n    def getvalue(self):\n        return \'\'.join(self.vals)\n\n\ndef test_csv_serialise():\n    m = MockFile()\n    write_csv(\n        m,\n        {""f1"": Field(""f1"", """"), ""f2"": Field(""f2"", \'\')},\n        [\n            {""f1"": 12, ""f2"": NumericRange(1.0, 2.0)},\n            {""f1"": datetime.datetime(2014, 7, 26, 23, 48, 0, tzinfo=tz.tzutc()), ""f2"": Range(-1.0, 2.0)},\n            {""f1"": datetime.datetime(2014, 7, 26, 23, 48, 0), ""f2"": ""landsat""}\n        ]\n    )\n\n    assert m.getvalue() == \'\\r\\n\'.join(\n        [\n            \'f1,f2\',\n            \'12,1.0 to 2.0\',\n            \'2014-07-26T23:48:00+00:00,-1.0 to 2.0\',\n            \'2014-07-26T23:48:00+00:00,landsat\',\n            \'\'\n        ]\n    )\n\n\ndef test_pretty_serialise():\n    m = MockFile()\n    write_pretty(\n        m,\n        {""f1"": Field(""f1"", """"), ""field 2"": Field(""f2"", \'\')},\n        [\n            {""f1"": 12, ""field 2"": NumericRange(1.0, 2.0)},\n            {""f1"": datetime.datetime(2014, 7, 26, 23, 48, 0, tzinfo=tz.tzutc()), ""field 2"": Range(-1.0, 2.0)},\n            {""f1"": datetime.datetime(2014, 7, 26, 23, 48, 0), ""field 2"": ""landsat""}\n        ],\n        terminal_size=(12, 12)\n    )\n\n    assert m.getvalue() == \'\\n\'.join(\n        [\n            \'-[ 1 ]-----\',\n            \'f1      | 12\',\n            \'field 2 | 1.0 to 2.0\',\n            \'-[ 2 ]-----\',\n            \'f1      | 2014-07-26T23:48:00+00:00\',\n            \'field 2 | -1.0 to 2.0\',\n            \'-[ 3 ]-----\',\n            \'f1      | 2014-07-26T23:48:00+00:00\',\n            \'field 2 | landsat\',\n            \'\'\n        ]\n    )\n'"
tests/storage/test_base.py,0,"b""import pytest\nfrom datacube.storage import BandInfo\nfrom datacube.testutils import mk_sample_dataset\nfrom datacube.storage._base import _get_band_and_layer\n\n\ndef test_band_layer():\n    def t(band=None, layer=None):\n        return _get_band_and_layer(dict(band=band, layer=layer))\n\n    assert t() == (None, None)\n    assert t(1) == (1, None)\n    assert t(None, 3) == (3, None)\n    assert t(1, 'foo') == (1, 'foo')\n    assert t(None, 'foo') == (None, 'foo')\n\n    bad_inputs = [('string', None),  # band has to be int|None\n                  (None, {}),  # layer has to be int|str|None\n                  (1, 3)]  # if band is set layer should be str|None\n\n    for bad in bad_inputs:\n        with pytest.raises(ValueError):\n            t(*bad)\n\n\ndef test_band_info():\n    bands = [dict(name=n,\n                  dtype='uint8',\n                  units='K',\n                  nodata=33,\n                  path=n+'.tiff')\n             for n in 'a b c'.split(' ')]\n\n    ds = mk_sample_dataset(bands,\n                           uri='file:///tmp/datataset.yml',\n                           format='GeoTIFF')\n\n    binfo = BandInfo(ds, 'b')\n    assert binfo.name == 'b'\n    assert binfo.band is None\n    assert binfo.layer is None\n    assert binfo.dtype == 'uint8'\n    assert binfo.transform is None\n    assert binfo.crs is None\n    assert binfo.units == 'K'\n    assert binfo.nodata == 33\n    assert binfo.uri == 'file:///tmp/b.tiff'\n    assert binfo.format == ds.format\n    assert binfo.driver_data is None\n    assert binfo.uri_scheme == 'file'\n\n    with pytest.raises(ValueError):\n        BandInfo(ds, 'no_such_band')\n\n    # Check case where dataset is missing band that is present in the product\n    del ds.metadata_doc['image']['bands']['c']\n    with pytest.raises(ValueError):\n        BandInfo(ds, 'c')\n\n    ds.uris = []\n    with pytest.raises(ValueError):\n        BandInfo(ds, 'a')\n\n    ds.uris = None\n    with pytest.raises(ValueError):\n        BandInfo(ds, 'a')\n"""
tests/storage/test_netcdfwriter.py,0,"b'\nimport netCDF4\nimport numpy\nimport xarray as xr\nimport pytest\nfrom hypothesis import given\nfrom hypothesis.strategies import text\nimport string\n\nfrom datacube.drivers.netcdf.writer import create_netcdf, create_coordinate, create_variable, netcdfy_data, \\\n    create_grid_mapping_variable, flag_mask_meanings, Variable\nfrom datacube.drivers.netcdf import write_dataset_to_netcdf\nfrom datacube.drivers.netcdf.writer import DEFAULT_GRID_MAPPING\nfrom datacube.utils import geometry, DatacubeException, read_strings_from_netcdf\n\n\ncrs_var = DEFAULT_GRID_MAPPING\n\nGEO_PROJ = geometry.CRS(""""""GEOGCS[""WGS 84"",\n                           DATUM[""WGS_1984"",SPHEROID[""WGS 84"",6378137,298.257223563,AUTHORITY[""EPSG"",""7030""]],\n                           AUTHORITY[""EPSG"",""6326""]],PRIMEM[""Greenwich"",0],UNIT[""degree"",0.0174532925199433],\n                           AUTHORITY[""EPSG"",""4326""]]"""""")\n\nALBERS_PROJ = geometry.CRS(""""""PROJCS[""GDA94 / Australian Albers"",\n                                GEOGCS[""GDA94"",\n                                    DATUM[""Geocentric_Datum_of_Australia_1994"",\n                                        SPHEROID[""GRS 1980"",6378137,298.257222101,\n                                            AUTHORITY[""EPSG"",""7019""]],\n                                        TOWGS84[0,0,0,0,0,0,0],\n                                        AUTHORITY[""EPSG"",""6283""]],\n                                    PRIMEM[""Greenwich"",0,\n                                        AUTHORITY[""EPSG"",""8901""]],\n                                    UNIT[""degree"",0.01745329251994328,\n                                        AUTHORITY[""EPSG"",""9122""]],\n                                    AUTHORITY[""EPSG"",""4283""]],\n                                UNIT[""metre"",1,\n                                    AUTHORITY[""EPSG"",""9001""]],\n                                PROJECTION[""Albers_Conic_Equal_Area""],\n                                PARAMETER[""standard_parallel_1"",-18],\n                                PARAMETER[""standard_parallel_2"",-36],\n                                PARAMETER[""latitude_of_center"",0],\n                                PARAMETER[""longitude_of_center"",132],\n                                PARAMETER[""false_easting"",0],\n                                PARAMETER[""false_northing"",0],\n                                AUTHORITY[""EPSG"",""3577""],\n                                AXIS[""Easting"",EAST],\n                                AXIS[""Northing"",NORTH]]"""""")\n\nSINIS_PROJ = geometry.CRS(""""""PROJCS[""Sinusoidal"",\n                                GEOGCS[""GCS_Undefined"",\n                                        DATUM[""Undefined"",\n                                        SPHEROID[""User_Defined_Spheroid"",6371007.181,0.0]],\n                                        PRIMEM[""Greenwich"",0.0],\n                                        UNIT[""Degree"",0.0174532925199433]],\n                                PROJECTION[""Sinusoidal""],\n                                PARAMETER[""False_Easting"",0.0],\n                                PARAMETER[""False_Northing"",0.0],\n                                PARAMETER[""Central_Meridian"",0.0],\n                                UNIT[""Meter"",1.0]]"""""")\n\nLCC2_PROJ = geometry.CRS(""""""PROJCS[""unnamed"",\n                               GEOGCS[""WGS 84"",\n                                       DATUM[""unknown"",\n                                       SPHEROID[""WGS84"",6378137,6556752.3141]],\n                                       PRIMEM[""Greenwich"",0],\n                                       UNIT[""degree"",0.0174532925199433]],\n                               UNIT[""metre"",1, AUTHORITY[""EPSG"",""9001""]],\n                               PROJECTION[""Lambert_Conformal_Conic_2SP""],\n                               PARAMETER[""standard_parallel_1"",17.5],\n                               PARAMETER[""standard_parallel_2"",29.5],\n                               PARAMETER[""latitude_of_origin"",12],\n                               PARAMETER[""central_meridian"",-102],\n                               PARAMETER[""false_easting"",2500000],\n                               PARAMETER[""false_northing"",0]]"""""")\n\n\ndef _ensure_spheroid(var):\n    assert \'semi_major_axis\' in var.ncattrs()\n    assert \'semi_minor_axis\' in var.ncattrs()\n    assert \'inverse_flattening\' in var.ncattrs()\n\n\ndef _ensure_gdal(var):\n    assert \'GeoTransform\' in var.ncattrs()\n    assert \'spatial_ref\' in var.ncattrs()\n\n\ndef _ensure_geospatial(nco):\n    assert \'geospatial_bounds\' in nco.ncattrs()\n    assert \'geospatial_bounds_crs\' in nco.ncattrs()\n    assert nco.getncattr(\'geospatial_bounds_crs\') == ""EPSG:4326""\n\n    assert \'geospatial_lat_min\' in nco.ncattrs()\n    assert \'geospatial_lat_max\' in nco.ncattrs()\n    assert \'geospatial_lat_units\' in nco.ncattrs()\n    assert nco.getncattr(\'geospatial_lat_units\') == ""degrees_north""\n\n    assert \'geospatial_lon_min\' in nco.ncattrs()\n    assert \'geospatial_lon_max\' in nco.ncattrs()\n    assert \'geospatial_lon_units\' in nco.ncattrs()\n    assert nco.getncattr(\'geospatial_lon_units\') == ""degrees_east""\n\n\ndef test_create_albers_projection_netcdf(tmpnetcdf_filename):\n    nco = create_netcdf(tmpnetcdf_filename)\n    create_coordinate(nco, \'x\', numpy.array([1., 2., 3.]), \'m\')\n    create_coordinate(nco, \'y\', numpy.array([1., 2., 3.]), \'m\')\n    create_grid_mapping_variable(nco, ALBERS_PROJ)\n    nco.close()\n\n    with netCDF4.Dataset(tmpnetcdf_filename) as nco:\n        assert crs_var in nco.variables\n        assert nco[crs_var].grid_mapping_name == \'albers_conical_equal_area\'\n        assert \'standard_parallel\' in nco[crs_var].ncattrs()\n        assert \'longitude_of_central_meridian\' in nco[crs_var].ncattrs()\n        assert \'latitude_of_projection_origin\' in nco[crs_var].ncattrs()\n        _ensure_spheroid(nco[crs_var])\n        _ensure_gdal(nco[crs_var])\n        _ensure_geospatial(nco)\n\n\ndef test_create_lambert_conformal_conic_2sp_projection_netcdf(tmpnetcdf_filename):\n    nco = create_netcdf(tmpnetcdf_filename)\n    create_coordinate(nco, \'x\', numpy.array([1., 2., 3.]), \'m\')\n    create_coordinate(nco, \'y\', numpy.array([1., 2., 3.]), \'m\')\n    create_grid_mapping_variable(nco, LCC2_PROJ)\n    nco.close()\n\n    with netCDF4.Dataset(tmpnetcdf_filename) as nco:\n        assert crs_var in nco.variables\n        assert nco[crs_var].grid_mapping_name == \'lambert_conformal_conic\'\n        assert \'standard_parallel\' in nco[crs_var].ncattrs()\n        assert \'longitude_of_central_meridian\' in nco[crs_var].ncattrs()\n        assert \'latitude_of_projection_origin\' in nco[crs_var].ncattrs()\n        assert \'false_easting\' in nco[crs_var].ncattrs()\n        assert \'false_northing\' in nco[crs_var].ncattrs()\n        _ensure_spheroid(nco[crs_var])\n        _ensure_gdal(nco[crs_var])\n        _ensure_geospatial(nco)\n\n\ndef test_create_epsg4326_netcdf(tmpnetcdf_filename):\n    nco = create_netcdf(tmpnetcdf_filename)\n    create_coordinate(nco, \'latitude\', numpy.array([1., 2., 3.]), \'m\')\n    create_coordinate(nco, \'longitude\', numpy.array([1., 2., 3.]), \'m\')\n    create_grid_mapping_variable(nco, GEO_PROJ)\n    nco.close()\n\n    with netCDF4.Dataset(tmpnetcdf_filename) as nco:\n        assert crs_var in nco.variables\n        assert nco[crs_var].grid_mapping_name == \'latitude_longitude\'\n        _ensure_spheroid(nco[crs_var])\n        _ensure_geospatial(nco)\n\n\ndef test_create_sinus_netcdf(tmpnetcdf_filename):\n    nco = create_netcdf(tmpnetcdf_filename)\n    create_coordinate(nco, \'x\', numpy.array([1., 2., 3.]), \'m\')\n    create_coordinate(nco, \'y\', numpy.array([1., 2., 3.]), \'m\')\n    create_grid_mapping_variable(nco, SINIS_PROJ)\n    nco.close()\n\n    with netCDF4.Dataset(tmpnetcdf_filename) as nco:\n        assert crs_var in nco.variables\n        assert nco[crs_var].grid_mapping_name == \'sinusoidal\'\n        assert \'longitude_of_central_meridian\' in nco[crs_var].ncattrs()\n        _ensure_spheroid(nco[crs_var])\n        _ensure_geospatial(nco)\n\n\n# Work around outstanding bug with hypothesis/pytest, where function level fixtures are only run once.\n# Generate a new netcdf filename for each run, so that old files don\'t cause permission errors on windows\n# due to antivirus software filesystem lag.\n# See https://github.com/HypothesisWorks/hypothesis-python/issues/377\n@given(s1=text(alphabet=string.printable, max_size=100),\n       s2=text(alphabet=string.printable, max_size=100),\n       s3=text(alphabet=string.printable, max_size=100))\ndef test_create_string_variable(tmpnetcdf_filename, s1, s2, s3):\n    str_var = \'str_var\'\n    nco = create_netcdf(tmpnetcdf_filename)\n    coord = create_coordinate(nco, \'greg\', numpy.array([1.0, 3.0, 9.0]), \'cubic gregs\')\n    assert coord is not None\n\n    dtype = numpy.dtype(\'S100\')\n    data = numpy.array([s1, s2, s3], dtype=dtype)\n\n    var = create_variable(nco, str_var, Variable(dtype, None, (\'greg\',), None))\n    var[:] = netcdfy_data(data)\n    nco.close()\n\n    with netCDF4.Dataset(tmpnetcdf_filename) as nco:\n        assert str_var in nco.variables\n\n    for returned, expected in zip(read_strings_from_netcdf(tmpnetcdf_filename, variable=str_var), (s1, s2, s3)):\n        assert returned == expected\n\n\ndef test_chunksizes(tmpnetcdf_filename):\n    nco = create_netcdf(tmpnetcdf_filename)\n\n    x = numpy.arange(3, dtype=\'float32\')\n    y = numpy.arange(5, dtype=\'float32\')\n\n    coord1 = create_coordinate(nco, \'x\', x, \'m\')\n    coord2 = create_coordinate(nco, \'y\', y, \'m\')\n\n    assert coord1 is not None and coord2 is not None\n\n    no_chunks = create_variable(nco, \'no_chunks\',\n                                Variable(numpy.dtype(\'int16\'), None, (\'x\', \'y\'), None))\n\n    min_max_chunks = create_variable(nco, \'min_max_chunks\',\n                                     Variable(numpy.dtype(\'int16\'), None, (\'x\', \'y\'), None),\n                                     chunksizes=(2, 50))\n\n    assert no_chunks is not None\n    assert min_max_chunks is not None\n\n    strings = numpy.array([""AAa"", \'bbb\', \'CcC\'], dtype=\'S\')\n    strings = xr.DataArray(strings, dims=[\'x\'], coords={\'x\': x})\n    create_variable(nco, \'strings_unchunked\', strings)\n    create_variable(nco, \'strings_chunked\', strings, chunksizes=(1,))\n\n    nco.close()\n\n    with netCDF4.Dataset(tmpnetcdf_filename) as nco:\n        assert nco[\'no_chunks\'].chunking() == \'contiguous\'\n        assert nco[\'min_max_chunks\'].chunking() == [2, 5]\n        assert nco[\'strings_unchunked\'].chunking() == \'contiguous\'\n        assert nco[\'strings_chunked\'].chunking() == [1, 3]\n\n\nEXAMPLE_FLAGS_DEF = {\n        \'band_1_saturated\': {\n            \'bits\': 0,\n            \'values\': {\n                0: True,\n                1: False\n            },\n            \'description\': \'Band 1 is saturated\'},\n        \'band_2_saturated\': {\n            \'bits\': 1,\n            \'values\': {\n                0: True,\n                1: False\n            },\n            \'description\': \'Band 2 is saturated\'},\n        \'band_3_saturated\': {\n            \'bits\': 2,\n            \'values\': {\n                0: True,\n                1: False\n            },\n            \'description\': \'Band 3 is saturated\'},\n        \'land_sea\': {\n            \'bits\': 9,\n            \'values\': {\n                0: \'sea\',\n                1: \'land\'\n            },\n            \'description\': \'Land/Sea observation\'},\n    }\n\n\ndef test_measurements_model_netcdfflags():\n    masks, valid_range, meanings = flag_mask_meanings(EXAMPLE_FLAGS_DEF)\n    assert ([0, 1023] == valid_range).all()\n    assert ([1, 2, 4, 512] == masks).all()\n    assert \'no_band_1_saturated no_band_2_saturated no_band_3_saturated land\' == meanings\n\n\ndef test_useful_error_on_write_empty_dataset(tmpnetcdf_filename):\n    with pytest.raises(DatacubeException) as excinfo:\n        ds = xr.Dataset()\n        write_dataset_to_netcdf(ds, tmpnetcdf_filename)\n    assert \'empty\' in str(excinfo.value)\n\n    with pytest.raises(DatacubeException) as excinfo:\n        ds = xr.Dataset(data_vars={\'blue\': ((\'time\',), numpy.array([0, 1, 2]))})\n        write_dataset_to_netcdf(ds, tmpnetcdf_filename)\n    assert \'geobox\' in str(excinfo.value)\n\n\ndef test_write_dataset_to_netcdf(tmpnetcdf_filename, odc_style_xr_dataset):\n    write_dataset_to_netcdf(odc_style_xr_dataset, tmpnetcdf_filename, global_attributes={\'foo\': \'bar\'},\n                            variable_params={\'B10\': {\'attrs\': {\'abc\': \'xyz\'}}})\n\n    with netCDF4.Dataset(tmpnetcdf_filename) as nco:\n        nco.set_auto_mask(False)\n        assert \'B10\' in nco.variables\n        var = nco.variables[\'B10\']\n        assert (var[:] == odc_style_xr_dataset[\'B10\'].values).all()\n\n        assert \'foo\' in nco.ncattrs()\n        assert nco.getncattr(\'foo\') == \'bar\'\n\n        assert \'abc\' in var.ncattrs()\n        assert var.getncattr(\'abc\') == \'xyz\'\n\n    with pytest.raises(RuntimeError):\n        write_dataset_to_netcdf(odc_style_xr_dataset, tmpnetcdf_filename)\n\n    # Check grid_mapping is a coordinate\n    xx = xr.open_dataset(tmpnetcdf_filename)\n    assert crs_var in xx.coords\n    assert crs_var not in xx.data_vars\n'"
tests/storage/test_storage.py,44,"b'from contextlib import contextmanager\n\nimport mock\nimport numpy as np\nimport pytest\nimport rasterio.warp\nfrom affine import Affine, identity\nfrom rasterio.warp import Resampling\n\nfrom datacube.drivers.datasource import DataSource\nfrom datacube.model import Dataset, DatasetType, MetadataType\nfrom datacube.testutils.io import RasterFileDataSource\nfrom datacube.storage import BandInfo\nfrom datacube.drivers.netcdf import create_netcdf_storage_unit, Variable\nfrom datacube.storage import reproject_and_fuse\nfrom datacube.storage._rio import RasterDatasetDataSource, _url2rasterio\nfrom datacube.storage._read import read_time_slice\nfrom datacube.utils.geometry import GeoBox\n\nfrom datacube.testutils.geom import epsg4326, epsg3577\n\n\ndef mk_gbox(shape=(2, 2), transform=identity, crs=epsg4326):\n    H, W = shape\n    return GeoBox(W, H, transform, crs)\n\n\ndef test_first_source_is_priority_in_reproject_and_fuse():\n    crs = epsg4326\n    shape = (2, 2)\n    no_data = -1\n\n    source1 = FakeDatasetSource([[1, 1], [1, 1]], crs=crs, shape=shape)\n    source2 = FakeDatasetSource([[2, 2], [2, 2]], crs=crs, shape=shape)\n    sources = [source1, source2]\n\n    output_data = np.full(shape, fill_value=no_data, dtype=\'int16\')\n    reproject_and_fuse(sources, output_data, mk_gbox(shape, crs=crs), dst_nodata=no_data)\n\n    assert (output_data == 1).all()\n\n\ndef test_second_source_used_when_first_is_empty():\n    crs = epsg4326\n    shape = (2, 2)\n    no_data = -1\n\n    source1 = FakeDatasetSource([[-1, -1], [-1, -1]], crs=crs, shape=shape)\n    source2 = FakeDatasetSource([[2, 2], [2, 2]], crs=crs, shape=shape)\n    sources = [source1, source2]\n\n    output_data = np.full(shape, fill_value=no_data, dtype=\'int16\')\n    reproject_and_fuse(sources, output_data, mk_gbox(shape, crs=crs), dst_nodata=no_data)\n\n    assert (output_data == 2).all()\n\n\ndef test_progress_cbk():\n    crs = epsg4326\n    shape = (2, 2)\n    no_data = -1\n    output_data = np.full(shape, fill_value=no_data, dtype=\'int16\')\n\n    src = FakeDatasetSource([[2, 2], [2, 2]], crs=crs, shape=shape)\n\n    def _cbk(n_so_far, n_total, out):\n        out.append((n_so_far, n_total))\n\n    cbk_args = []\n    reproject_and_fuse([src], output_data,\n                       mk_gbox(shape, crs=crs),\n                       dst_nodata=no_data, progress_cbk=lambda *a: _cbk(*a, cbk_args))\n\n    assert cbk_args == [(1, 1)]\n\n    cbk_args = []\n    reproject_and_fuse([src, src], output_data,\n                       mk_gbox(shape, crs=crs),\n                       dst_nodata=no_data, progress_cbk=lambda *a: _cbk(*a, cbk_args))\n\n    assert cbk_args == [(1, 2), (2, 2)]\n\n\ndef test_mixed_result_when_first_source_partially_empty():\n    crs = epsg4326\n    shape = (2, 2)\n    no_data = -1\n\n    source1 = FakeDatasetSource([[1, 1], [no_data, no_data]], crs=crs)\n    source2 = FakeDatasetSource([[2, 2], [2, 2]], crs=crs)\n    sources = [source1, source2]\n\n    output_data = np.full(shape, fill_value=no_data, dtype=\'int16\')\n    reproject_and_fuse(sources, output_data, mk_gbox(shape, crs=crs), dst_nodata=no_data)\n\n    assert (output_data == [[1, 1], [2, 2]]).all()\n\n\ndef test_when_input_empty():\n    shape = (2, 2)\n    no_data = -1\n    out = np.full(shape, fill_value=no_data, dtype=\'int16\')\n    reproject_and_fuse([], out, mk_gbox(shape, crs=epsg4326), dst_nodata=no_data)\n    assert (out == no_data).all()\n\n\ndef test_mixed_result_when_first_source_partially_empty_with_nan_nodata():\n    crs = epsg4326\n    shape = (2, 2)\n    no_data = np.nan\n\n    source1 = FakeDatasetSource([[1, 1], [no_data, no_data]], crs=crs)\n    source2 = FakeDatasetSource([[2, 2], [2, 2]], crs=crs)\n    sources = [source1, source2]\n\n    output_data = np.full(shape, fill_value=no_data, dtype=\'float64\')\n    reproject_and_fuse(sources, output_data, mk_gbox(shape, crs=crs), dst_nodata=no_data)\n\n    assert (output_data == [[1, 1], [2, 2]]).all()\n\n\nclass FakeBandDataSource(object):\n    def __init__(self, value, nodata, shape=(2, 2), *args, **kwargs):\n        self.value = value\n        self.crs = epsg4326\n        self.transform = Affine.identity()\n        self.dtype = np.int16 if not np.isnan(nodata) else np.float64\n        self.shape = shape\n        self.nodata = nodata\n\n    def read(self, window=None, out_shape=None):\n        """"""Read data in the native format, returning a numpy array\n        """"""\n        return np.array(self.value)\n\n\nclass FakeDatasetSource(DataSource):\n    def __init__(self, value, bandnumber=1, nodata=-999, shape=(2, 2), crs=None, transform=None,\n                 band_source_class=FakeBandDataSource):\n        super(FakeDatasetSource, self).__init__()\n        self.value = value\n        self.bandnumber = bandnumber\n        self.crs = crs\n        self.transform = transform\n        self.band_source_class = band_source_class\n        self.shape = shape\n        self.nodata = nodata\n\n    def get_bandnumber(self, src):\n        return self.bandnumber\n\n    def get_transform(self, shape):\n        if self.transform is None:\n            raise RuntimeError(\'No transform in the data and no fallback\')\n        return self.transform\n\n    def get_crs(self):\n        if self.crs is None:\n            raise RuntimeError(\'No CRS in the data and no fallback\')\n        return self.crs\n\n    @contextmanager\n    def open(self):\n        """"""Context manager which returns a :class:`BandDataSource`""""""\n        yield self.band_source_class(value=self.value, nodata=self.nodata, shape=self.shape)\n\n\nclass BrokenBandDataSource(FakeBandDataSource):\n    def read(self, window=None, out_shape=None):\n        raise OSError(\'Read or write failed\')\n\n\ndef test_read_from_broken_source():\n    crs = epsg4326\n    shape = (2, 2)\n    no_data = -1\n\n    source1 = FakeDatasetSource(value=[[1, 1], [no_data, no_data]], crs=crs, band_source_class=BrokenBandDataSource)\n    source2 = FakeDatasetSource(value=[[2, 2], [2, 2]], crs=crs)\n    sources = [source1, source2]\n\n    output_data = np.full(shape, fill_value=no_data, dtype=\'int16\')\n\n    gbox = mk_gbox(shape, crs=crs)\n\n    # Check exception is raised\n    with pytest.raises(OSError):\n        reproject_and_fuse(sources, output_data, gbox, dst_nodata=no_data)\n\n    # Check can ignore errors\n    reproject_and_fuse(sources, output_data, gbox, dst_nodata=no_data,\n                       skip_broken_datasets=True)\n\n    assert (output_data == [[2, 2], [2, 2]]).all()\n\n\nclass FakeDataSource(object):\n    def __init__(self):\n        self.crs = epsg4326\n        self.transform = Affine(0.25, 0, 100, 0, -0.25, -30)\n        self.nodata = -999\n        self.shape = (613, 597)\n\n        self.data = np.full(self.shape, self.nodata, dtype=\'int16\')\n        self.data[:512, :512] = np.arange(512) + np.arange(512).reshape((512, 1))\n\n    def read(self, window=None, out_shape=None):\n        data = self.data\n\n        if window:\n            data = data[slice(*window[0]), slice(*window[1])]\n\n        if out_shape is not None and out_shape != data.shape:\n            xidx = ((np.arange(out_shape[1]) + 0.5) * (data.shape[1] / out_shape[1]) - 0.5).round().astype(\'int\')\n            yidx = ((np.arange(out_shape[0]) + 0.5) * (data.shape[0] / out_shape[0]) - 0.5).round().astype(\'int\')\n            data = data[np.meshgrid(yidx, xidx, indexing=\'ij\')]\n\n        return data.copy()\n\n\ndef assert_same_read_results(source, dst_shape, dst_dtype, dst_transform, dst_nodata, dst_projection, resampling):\n    expected = np.empty(dst_shape, dtype=dst_dtype)\n    with source.open() as src:\n        rasterio.warp.reproject(src.data,\n                                expected,\n                                src_transform=src.transform,\n                                src_crs=str(src.crs),\n                                src_nodata=src.nodata,\n                                dst_transform=dst_transform,\n                                dst_crs=str(dst_projection),\n                                dst_nodata=dst_nodata,\n                                resampling=resampling)\n\n    result = np.full(dst_shape, dst_nodata, dtype=dst_dtype)\n    H, W = dst_shape\n    dst_gbox = GeoBox(W, H, dst_transform, dst_projection)\n    with source.open() as rdr:\n        read_time_slice(rdr,\n                        result,\n                        dst_gbox,\n                        dst_nodata=dst_nodata,\n                        resampling=resampling)\n\n    assert np.isclose(result, expected, atol=0, rtol=0.05, equal_nan=True).all()\n    return result\n\n\ndef test_read_from_fake_source():\n    data_source = FakeDataSource()\n\n    @contextmanager\n    def fake_open():\n        yield data_source\n\n    source = mock.Mock()\n    source.open = fake_open\n\n    # one-to-one copy\n    assert_same_read_results(\n        source,\n        dst_shape=data_source.shape,\n        dst_dtype=data_source.data.dtype,\n        dst_transform=data_source.transform,\n        dst_nodata=data_source.nodata,\n        dst_projection=data_source.crs,\n        resampling=Resampling.nearest)\n\n    # change dtype\n    assert_same_read_results(\n        source,\n        dst_shape=data_source.shape,\n        dst_dtype=\'int32\',\n        dst_transform=data_source.transform,\n        dst_nodata=data_source.nodata,\n        dst_projection=data_source.crs,\n        resampling=Resampling.nearest)\n\n    # change nodata\n    assert_same_read_results(\n        source,\n        dst_shape=data_source.shape,\n        dst_dtype=\'float32\',\n        dst_transform=data_source.transform,\n        dst_nodata=float(\'nan\'),\n        dst_projection=data_source.crs,\n        resampling=Resampling.nearest)\n\n    # different offsets/sizes\n    assert_same_read_results(\n        source,\n        dst_shape=(517, 557),\n        dst_dtype=\'float32\',\n        dst_transform=data_source.transform * Affine.translation(-200, -200),\n        dst_nodata=float(\'nan\'),\n        dst_projection=data_source.crs,\n        resampling=Resampling.nearest)\n\n    assert_same_read_results(\n        source,\n        dst_shape=(807, 879),\n        dst_dtype=\'float32\',\n        dst_transform=data_source.transform * Affine.translation(200, 200),\n        dst_nodata=float(\'nan\'),\n        dst_projection=data_source.crs,\n        resampling=Resampling.nearest)\n\n    assert_same_read_results(\n        source,\n        dst_shape=(807, 879),\n        dst_dtype=\'float32\',\n        dst_transform=data_source.transform * Affine.translation(1500, -1500),\n        dst_nodata=float(\'nan\'),\n        dst_projection=data_source.crs,\n        resampling=Resampling.nearest)\n\n    # flip axis\n    assert_same_read_results(\n        source,\n        dst_shape=(517, 557),\n        dst_dtype=\'float32\',\n        dst_transform=data_source.transform * Affine.translation(0, 512) * Affine.scale(1, -1),\n        dst_nodata=float(\'nan\'),\n        dst_projection=data_source.crs,\n        resampling=Resampling.nearest)\n\n    assert_same_read_results(\n        source,\n        dst_shape=(517, 557),\n        dst_dtype=\'float32\',\n        dst_transform=data_source.transform * Affine.translation(512, 0) * Affine.scale(-1, 1),\n        dst_nodata=float(\'nan\'),\n        dst_projection=data_source.crs,\n        resampling=Resampling.nearest)\n\n    # scale\n    assert_same_read_results(\n        source,\n        dst_shape=(250, 500),\n        dst_dtype=\'float32\',\n        dst_transform=data_source.transform * Affine.scale(1.2, 1.4),\n        dst_nodata=float(\'nan\'),\n        dst_projection=data_source.crs,\n        resampling=Resampling.nearest)\n\n    assert_same_read_results(\n        source,\n        dst_shape=(500, 250),\n        dst_dtype=\'float32\',\n        dst_transform=data_source.transform * Affine.scale(1.4, 1.2),\n        dst_nodata=float(\'nan\'),\n        dst_projection=data_source.crs,\n        resampling=Resampling.cubic)\n\n    assert_same_read_results(\n        source,\n        dst_shape=(67, 35),\n        dst_dtype=\'float32\',\n        dst_transform=data_source.transform * Affine.scale(1.16, 1.8),\n        dst_nodata=float(\'nan\'),\n        dst_projection=data_source.crs,\n        resampling=Resampling.cubic)\n\n    assert_same_read_results(\n        source,\n        dst_shape=(35, 67),\n        dst_dtype=\'float32\',\n        dst_transform=data_source.transform * Affine.translation(27, 35) * Affine.scale(1.8, 1.16),\n        dst_nodata=float(\'nan\'),\n        dst_projection=data_source.crs,\n        resampling=Resampling.cubic)\n\n    assert_same_read_results(\n        source,\n        dst_shape=(35, 67),\n        dst_dtype=\'float32\',\n        dst_transform=data_source.transform * Affine.translation(-13, -27) * Affine.scale(1.8, 1.16),\n        dst_nodata=float(\'nan\'),\n        dst_projection=data_source.crs,\n        resampling=Resampling.average)\n\n    # scale + flip\n    assert_same_read_results(\n        source,\n        dst_shape=(35, 67),\n        dst_dtype=\'float32\',\n        dst_transform=data_source.transform * Affine.translation(15, 512 + 17) * Affine.scale(1.8, -1.16),\n        dst_nodata=float(\'nan\'),\n        dst_projection=data_source.crs,\n        resampling=Resampling.cubic)\n\n    assert_same_read_results(\n        source,\n        dst_shape=(67, 35),\n        dst_dtype=\'float32\',\n        dst_transform=data_source.transform * Affine.translation(512 - 23, -29) * Affine.scale(-1.16, 1.8),\n        dst_nodata=float(\'nan\'),\n        dst_projection=data_source.crs,\n        resampling=Resampling.cubic)\n\n    # TODO: crs change\n\n\ndef _read_from_source(source, dest, dst_transform, dst_nodata, dst_projection, resampling):\n    """"""\n    Adapt old signature to new function, so that we can keep old tests at least for now\n    """"""\n    H, W = dest.shape\n    gbox = GeoBox(W, H, dst_transform, dst_projection)\n    dest[:] = dst_nodata  # new code assumes pre-populated image\n    with source.open() as rdr:\n        read_time_slice(rdr, dest, gbox, resampling=resampling, dst_nodata=dst_nodata)\n\n\nclass TestRasterDataReading(object):\n    @pytest.mark.parametrize(""dst_nodata"", [\n        np.nan, float(""nan""), -999\n    ])\n    def xtest_failed_data_read(self, make_sample_geotiff, dst_nodata):\n        sample_geotiff_path, geobox, written_data = make_sample_geotiff(dst_nodata)\n\n        src_transform = Affine(25.0, 0.0, 1200000.0,\n                               0.0, -25.0, -4200000.0)\n        source = RasterFileDataSource(sample_geotiff_path, 1, transform=src_transform)\n\n        dest = np.zeros((20, 100))\n        dst_nodata = -999\n        dst_projection = epsg3577\n        dst_resampling = Resampling.nearest\n\n        # Read exactly the hunk of data that we wrote\n        dst_transform = Affine(25.0, 0.0, 127327.0,\n                               0.0, -25.0, -417232.0)\n        _read_from_source(source, dest, dst_transform, dst_nodata, dst_projection, dst_resampling)\n\n        assert np.all(written_data == dest)\n\n    @pytest.mark.parametrize(""dst_nodata"", [\n        np.nan, float(""nan""), -999\n    ])\n    def test_read_with_rasterfiledatasource(self, make_sample_geotiff, dst_nodata):\n        sample_geotiff_path, geobox, written_data = make_sample_geotiff(dst_nodata)\n\n        source = RasterFileDataSource(str(sample_geotiff_path), 1)\n\n        dest = np.zeros_like(written_data)\n        dst_transform = geobox.transform\n        dst_projection = epsg3577\n        dst_resampling = Resampling.nearest\n\n        # Read exactly the hunk of data that we wrote\n        _read_from_source(source, dest, dst_transform, dst_nodata, dst_projection, dst_resampling)\n\n        assert np.all(written_data == dest)\n\n        # Try reading from partially outside of our area\n        xoff = 50\n        offset_transform = dst_transform * Affine.translation(xoff, 0)\n        dest = np.zeros_like(written_data)\n\n        _read_from_source(source, dest, offset_transform, dst_nodata, dst_projection, dst_resampling)\n        assert np.all(written_data[:, xoff:] == dest[:, :xoff])\n\n        # Try reading from complete outside of our area, should return nodata\n        xoff = 300\n        offset_transform = dst_transform * Affine.translation(xoff, 0)\n        dest = np.zeros_like(written_data)\n\n        _read_from_source(source, dest, offset_transform, dst_nodata, dst_projection, dst_resampling)\n        if np.isnan(dst_nodata):\n            assert np.all(np.isnan(dest))\n        else:\n            assert np.all(dst_nodata == dest)\n\n    @pytest.mark.parametrize(""dst_transform"", [\n        Affine(25.0, 0.0, 1273275.0, 0.0, -25.0, -4172325.0),\n        Affine(25.0, 0.0, 127327.0, 0.0, -25.0, -417232.0)\n    ])\n    def test_read_data_from_outside_file_region(self, make_sample_netcdf, dst_transform):\n        sample_nc, geobox, written_data = make_sample_netcdf\n\n        source = RasterFileDataSource(sample_nc, 1)\n\n        dest = np.zeros((200, 1000))\n        dst_nodata = -999\n        dst_projection = epsg3577\n        dst_resampling = Resampling.nearest\n\n        # Read exactly the hunk of data that we wrote\n        _read_from_source(source, dest, dst_transform, dst_nodata, dst_projection, dst_resampling)\n\n        assert np.all(dest == -999)\n\n    def test_read_from_file_with_missing_crs(self, no_crs_gdal_path):\n        """"""\n        We need to be able to read from data files even when GDAL can\'t automatically gather all the metdata.\n\n        The :class:`RasterFileDataSource` is able to override the nodata, CRS and transform attributes if necessary.\n        """"""\n        crs = epsg4326\n        nodata = -999\n        transform = Affine(0.01, 0.0, 111.975,\n                           0.0, 0.01, -9.975)\n        data_source = RasterFileDataSource(no_crs_gdal_path, bandnumber=1, nodata=nodata, crs=crs, transform=transform)\n        with pytest.warns(DeprecationWarning):\n            with data_source.open() as src:\n                dest1 = src.read()\n                assert dest1.shape == (10, 10)\n\n\n@pytest.fixture\ndef make_sample_netcdf(tmpdir):\n    """"""Make a test Geospatial NetCDF file, 4000x4000 int16 random data, in a variable named `sample`.\n    Return the GDAL access string.""""""\n    sample_nc = str(tmpdir.mkdir(\'netcdfs\').join(\'sample.nc\'))\n    geobox = GeoBox(4000, 4000, affine=Affine(25.0, 0.0, 1200000, 0.0, -25.0, -4200000), crs=epsg3577)\n\n    sample_data = np.random.randint(10000, size=(4000, 4000), dtype=np.int16)\n\n    variables = {\'sample\': Variable(sample_data.dtype, nodata=-999, dims=geobox.dimensions, units=1)}\n    nco = create_netcdf_storage_unit(sample_nc, geobox.crs, geobox.coordinates, variables=variables, variable_params={})\n\n    nco[\'sample\'][:] = sample_data\n\n    nco.close()\n\n    return \'NetCDF:""%s"":sample\' % sample_nc, geobox, sample_data\n\n\n@pytest.fixture\ndef make_sample_geotiff(tmpdir):\n    """""" Make a sample geotiff, filled with random data, and twice as tall as it is wide. """"""\n    def internal_make_sample_geotiff(nodata=-999):\n        sample_geotiff = str(tmpdir.mkdir(\'tiffs\').join(\'sample.tif\'))\n\n        geobox = GeoBox(100, 200, affine=Affine(25.0, 0.0, 0, 0.0, -25.0, 0), crs=epsg3577)\n        if np.isnan(nodata):\n            out_dtype = \'float64\'\n            sample_data = 10000 * np.random.random_sample(size=geobox.shape)\n        else:\n            out_dtype = \'int16\'\n            sample_data = np.random.randint(10000, size=geobox.shape, dtype=out_dtype)\n        rio_args = {\n            \'height\': geobox.height,\n            \'width\': geobox.width,\n            \'count\': 1,\n            \'dtype\': out_dtype,\n            \'crs\': \'EPSG:3577\',\n            \'transform\': geobox.transform,\n            \'nodata\': nodata\n        }\n        with rasterio.open(sample_geotiff, \'w\', driver=\'GTiff\', **rio_args) as dst:\n            dst.write(sample_data, 1)\n\n        return sample_geotiff, geobox, sample_data\n    return internal_make_sample_geotiff\n\n\n_EXAMPLE_METADATA_TYPE = MetadataType(\n    {\n        \'name\': \'eo\',\n        \'dataset\': dict(\n            id=[\'id\'],\n            label=[\'ga_label\'],\n            creation_time=[\'creation_dt\'],\n            measurements=[\'image\', \'bands\'],\n            sources=[\'lineage\', \'source_datasets\'],\n            format=[\'format\', \'name\'],\n        )\n    },\n    dataset_search_fields={}\n)\n\n_EXAMPLE_DATASET_TYPE = DatasetType(\n    _EXAMPLE_METADATA_TYPE,\n    {\n        \'name\': \'ls5_nbar_scene\',\n        \'description\': ""Landsat 5 NBAR 25 metre"",\n        \'metadata_type\': \'eo\',\n        \'metadata\': {},\n        \'measurements\': [\n            {\'aliases\': [\'band_2\', \'2\'],\n             \'dtype\': \'int16\',\n             \'name\': \'green\',\n             \'nodata\': -999,\n             \'units\': \'1\'}],\n    }\n)\n\n\ndef test_multiband_support_in_datasetsource(example_gdal_path):\n    defn = {\n        ""id"": \'12345678123456781234567812345678\',\n        ""format"": {""name"": ""GeoTiff""},\n        ""image"": {\n            ""bands"": {\n                \'green\': {\n                    \'type\': \'reflective\',\n                    \'cell_size\': 25.0,\n                    \'path\': example_gdal_path,\n                    \'label\': \'Coastal Aerosol\',\n                    \'number\': \'1\',\n                },\n            }\n        }\n    }\n\n    # Without new band attribute, default to band number 1\n    d = Dataset(_EXAMPLE_DATASET_TYPE, defn, uris=[\'file:///tmp\'])\n\n    ds = RasterDatasetDataSource(BandInfo(d, \'green\'))\n\n    bandnum = ds.get_bandnumber(None)\n\n    assert bandnum == 1\n\n    with ds.open() as foo:\n        data = foo.read()\n        assert isinstance(data, np.ndarray)\n\n    #############\n    # With new \'image.bands.[band].band\' attribute\n    band_num = 3\n    defn[\'image\'][\'bands\'][\'green\'][\'band\'] = band_num\n    d = Dataset(_EXAMPLE_DATASET_TYPE, defn, uris=[\'file:///tmp\'])\n\n    ds = RasterDatasetDataSource(BandInfo(d, \'green\'))\n\n    assert ds.get_bandnumber(None) == band_num\n\n\ndef test_netcdf_multi_part():\n    defn = {\n        ""id"": \'12345678123456781234567812345678\',\n        ""format"": {""name"": ""NetCDF CF""},\n        ""image"": {\n            ""bands"": {\n                \'green\': {\n                    \'type\': \'reflective\',\n                    \'cell_size\': 25.0,\n                    \'layer\': \'green\',\n                    \'path\': \'\',\n                    \'label\': \'Coastal Aerosol\',\n                },\n            }\n        }\n    }\n\n    def ds(uri):\n        d = Dataset(_EXAMPLE_DATASET_TYPE, defn, uris=[uri])\n        return RasterDatasetDataSource(BandInfo(d, \'green\'))\n\n    for i in range(3):\n        assert ds(\'file:///tmp.nc#part=%d\' % i).get_bandnumber() == (i+1)\n\n    # can\'t tell without opening file\n    assert ds(\'file:///tmp.nc\').get_bandnumber() is None\n\n\ndef test_rasterio_nodata(tmpdir):\n    from datacube.testutils.io import dc_read, write_gtiff\n    from pathlib import Path\n\n    roi = np.s_[10:20, 20:30]\n    xx = np.zeros((64, 64), dtype=\'uint8\')\n    xx[roi] = 255\n\n    pp = Path(str(tmpdir))\n\n    mm = write_gtiff(pp/\'absent_nodata.tiff\', xx, nodata=None)\n\n    yy = dc_read(mm.path, gbox=mm.gbox, fallback_nodata=None)\n    np.testing.assert_array_equal(xx, yy)\n\n    # fallback nodata is outside source range so it shouldn\'t be used\n    yy = dc_read(mm.path, gbox=mm.gbox, fallback_nodata=-1, dst_nodata=-999, dtype=\'int16\')\n    np.testing.assert_array_equal(xx.astype(\'int16\'), yy)\n\n    # treat zeros as no-data + type conversion while reading\n    yy_expect = xx.copy().astype(\'int16\')\n    yy_expect[xx == 0] = -999\n    assert set(yy_expect.ravel()) == {-999, 255}\n\n    yy = dc_read(mm.path, fallback_nodata=0, dst_nodata=-999, dtype=\'int16\')\n    np.testing.assert_array_equal(yy_expect, yy)\n\n    # now check that file nodata is used instead of fallback\n    mm = write_gtiff(pp/\'with_nodata.tiff\', xx, nodata=33)\n    yy = dc_read(mm.path, fallback_nodata=0, dst_nodata=-999, dtype=\'int16\')\n\n    np.testing.assert_array_equal(xx, yy)\n\n    yy = dc_read(mm.path)\n    np.testing.assert_array_equal(xx, yy)\n\n\ndef test_rio_driver_specifics():\n    assert _url2rasterio(\'file:///f.nc\', \'NetCDF\', \'band\') == \'NetCDF:""/f.nc"":band\'\n    assert _url2rasterio(\'file:///f.nc\', \'HDF5\', \'band\') == \'HDF5:""/f.nc"":band\'\n    assert _url2rasterio(\'file:///f.nc\', \'HDF4_EOS:EOS_GRID\', \'band\') == \'HDF4_EOS:EOS_GRID:""/f.nc"":band\'\n    assert _url2rasterio(\'file:///f.tiff\', \'GeoTIFF\', None) == \'/f.tiff\'\n    s3_url = \'s3://bucket/file\'\n    assert _url2rasterio(s3_url, \'GeoTIFF\', None) is s3_url\n\n    vsi_url = \'/vsicurl/https://host.tld/path\'\n    assert _url2rasterio(vsi_url, \'GeoTIFF\', None) is vsi_url\n    assert _url2rasterio(vsi_url, \'NetCDF\', \'aa\') == \'NetCDF:""{}"":aa\'.format(vsi_url)\n    assert _url2rasterio(vsi_url, \'HDF5\', \'aa\') == \'HDF5:""{}"":aa\'.format(vsi_url)\n\n    with pytest.raises(ValueError):\n        _url2rasterio(\'file:///f.nc\', \'NetCDF\', None)\n\n    with pytest.raises(RuntimeError):\n        _url2rasterio(\'http://example.com/f.nc\', \'NetCDF\', \'aa\')\n\n    with pytest.raises(ValueError):\n        _url2rasterio(\'/some/path/\', \'GeoTIFF\', None)\n\n    with pytest.raises(ValueError):\n        _url2rasterio(\'/some/path/\', \'NetCDF\', \'aa\')\n'"
tests/storage/test_storage_load.py,14,"b'"""""" Test New IO driver loading\n""""""\n\nimport numpy as np\n\nfrom datacube.storage._load import (\n    xr_load, _default_fuser\n)\n\nfrom datacube.api.core import Datacube\nfrom datacube.testutils import mk_sample_dataset\nfrom datacube.testutils.io import rio_slurp\nfrom datacube.testutils.iodriver import mk_rio_driver, tee_new_load_context\n\n\ndef test_default_fuser():\n    dest = np.full((2, 2), -1.0)\n    src1 = np.array([[0.0, -1.0], [-1.0, 6.0]])\n\n    _default_fuser(dest, src1, -1.0)\n    assert np.all(dest == src1)\n\n    src2 = np.array([[9.0, np.nan], [-1.0, 3.0]])\n    _default_fuser(dest, src2, -1.0)\n    assert np.allclose(dest, np.array([[0.0, np.nan], [-1.0, 6.0]]), equal_nan=True)\n\n    src3 = np.full((2, 2), 55.0)\n    _default_fuser(dest, src3, -1.0)\n    assert np.all(dest == np.array([[0.0, 55.0], [55.0, 6.0]]))\n\n    dest = np.full((2, 2), -1)\n    src1 = np.array([[0, -1], [-1, 6]])\n\n    _default_fuser(dest, src1, -1)\n    assert np.all(dest == src1)\n\n    src2 = np.array([[9, 4], [-1, 3]])\n    _default_fuser(dest, src1, None)\n    assert np.all(dest == src1)\n\n\ndef test_new_xr_load(data_folder):\n    base = ""file://"" + str(data_folder) + ""/metadata.yml""\n\n    rdr = mk_rio_driver()\n    assert rdr is not None\n\n    _bands = []\n\n    def band_info_collector(bands, ctx):\n        for b in bands:\n            _bands.append(b)\n\n    tee_new_load_context(rdr, band_info_collector)\n\n    band_a = dict(name=\'a\',\n                  path=\'test.tif\')\n\n    band_b = dict(name=\'b\',\n                  band=2,\n                  path=\'test.tif\')\n\n    ds = mk_sample_dataset([band_a, band_b], base)\n\n    sources = Datacube.group_datasets([ds], \'time\')\n\n    im, meta = rio_slurp(str(data_folder) + \'/test.tif\')\n    measurements = [ds.type.measurements[n] for n in (\'a\', \'b\')]\n    measurements[1][\'fuser\'] = lambda dst, src: _default_fuser(dst, src, measurements[1].nodata)\n\n    xx, _ = xr_load(sources, meta.gbox, measurements, rdr)\n\n    assert len(_bands) == 2\n\n    assert im[0].shape == xx.a.isel(time=0).shape\n    assert im[1].shape == xx.b.isel(time=0).shape\n\n    np.testing.assert_array_equal(im[0], xx.a.values[0])\n    np.testing.assert_array_equal(im[1], xx.b.values[0])\n'"
tests/storage/test_storage_read.py,42,"b'from affine import Affine\nimport numpy as np\n\nfrom datacube.storage._read import (\n    can_paste,\n    read_time_slice,\n    read_time_slice_v2,\n    pick_read_scale,\n    rdr_geobox)\n\nfrom datacube.testutils.io import RasterFileDataSource\nfrom datacube.utils.geometry import (\n    compute_reproject_roi,\n    GeoBox,\n    roi_shape,\n    roi_is_empty,\n)\n\nfrom datacube.utils.geometry import gbox as gbx\n\nfrom datacube.testutils.io import (\n    rio_slurp\n)\n\nfrom datacube.testutils.geom import (\n    epsg3857,\n    AlbersGS,\n)\n\n\ndef test_pick_read_scale():\n    assert pick_read_scale(0.7) == 1\n    assert pick_read_scale(1.3) == 1\n    assert pick_read_scale(2.3) == 2\n    assert pick_read_scale(1.99999) == 2\n\n\ndef test_can_paste():\n    src = AlbersGS.tile_geobox((17, -40))\n\n    def check_true(dst, **kwargs):\n        ok, reason = can_paste(compute_reproject_roi(src, dst), **kwargs)\n        if not ok:\n            assert ok is True, reason\n\n    def check_false(dst, **kwargs):\n        ok, reason = can_paste(compute_reproject_roi(src, dst), **kwargs)\n        if ok:\n            assert ok is False, ""Expected can_paste to return False, but got True""\n\n    check_true(gbx.pad(src, 100))\n    check_true(src[:10, :10])\n    check_true(gbx.translate_pix(src, 0.1, 0.3), ttol=0.5)\n    check_true(gbx.translate_pix(src, 3, -4))\n    check_true(gbx.flipx(src))\n    check_true(gbx.flipy(src))\n    check_true(gbx.flipx(gbx.flipy(src)))\n    check_true(gbx.zoom_out(src, 2))\n    check_true(gbx.zoom_out(src, 4))\n    check_true(gbx.zoom_out(src[:9, :9], 3))\n\n    # Check False code paths\n    dst = GeoBox.from_geopolygon(src.extent.to_crs(epsg3857).buffer(10),\n                                 resolution=src.resolution)\n    check_false(dst)  # non ST\n\n    check_false(gbx.zoom_out(src, 1.9))   # non integer scale\n    check_false(gbx.affine_transform_pix(src, Affine.scale(1, 2)))  # sx != sy\n\n    dst = gbx.translate_pix(src, -1, -1)\n    dst = gbx.zoom_out(dst, 2)\n    check_false(dst)  # src_roi doesn\'t align for scale\n\n    check_false(dst, stol=0.7)  # src_roi/scale != dst_roi\n    check_false(gbx.translate_pix(src, 0.3, 0.4))  # sub-pixel translation\n\n\ndef test_read_paste(tmpdir):\n    from datacube.testutils import mk_test_image\n    from datacube.testutils.io import write_gtiff\n    from pathlib import Path\n\n    pp = Path(str(tmpdir))\n\n    xx = mk_test_image(128, 64, nodata=None)\n    assert (xx != -999).all()\n\n    mm = write_gtiff(pp/\'tst-read-paste-128x64-int16.tif\', xx, nodata=None)\n\n    def _read(gbox, resampling=\'nearest\',\n              fallback_nodata=-999,\n              dst_nodata=-999,\n              check_paste=False):\n        with RasterFileDataSource(mm.path, 1, nodata=fallback_nodata).open() as rdr:\n            if check_paste:\n                # check that we are using paste\n                paste_ok, reason = can_paste(compute_reproject_roi(rdr_geobox(rdr), gbox))\n                assert paste_ok is True, reason\n\n            yy = np.full(gbox.shape, dst_nodata, dtype=rdr.dtype)\n            roi = read_time_slice(rdr, yy, gbox, resampling, dst_nodata)\n            return yy, roi\n\n    # read native whole\n    yy, roi = _read(mm.gbox)\n    np.testing.assert_array_equal(xx, yy)\n    assert roi == np.s_[0:64, 0:128]\n\n    # read native whole, no nodata case\n    yy, roi = _read(mm.gbox, fallback_nodata=None)\n    np.testing.assert_array_equal(xx, yy)\n    assert roi == np.s_[0:64, 0:128]\n\n    # read native whole, ignoring small sub-pixel translation\n    yy, roi = _read(gbx.translate_pix(mm.gbox, 0.3, -0.4), fallback_nodata=-33)\n    np.testing.assert_array_equal(xx, yy)\n    assert roi == np.s_[0:64, 0:128]\n\n    # no overlap between src and dst\n    yy, roi = _read(gbx.translate_pix(mm.gbox, 10000, -10000))\n    assert roi_is_empty(roi)\n\n    # read with Y flipped\n    yy, roi = _read(gbx.flipy(mm.gbox))\n    np.testing.assert_array_equal(xx[::-1, :], yy)\n    assert roi == np.s_[0:64, 0:128]\n\n    # read with X flipped\n    yy, roi = _read(gbx.flipx(mm.gbox))\n    np.testing.assert_array_equal(xx[:, ::-1], yy)\n    assert roi == np.s_[0:64, 0:128]\n\n    # read with X and Y flipped\n    yy, roi = _read(gbx.flipy(gbx.flipx(mm.gbox)))\n    assert roi == np.s_[0:64, 0:128]\n    np.testing.assert_array_equal(xx[::-1, ::-1], yy[roi])\n\n    # dst is fully inside src\n    sroi = np.s_[10:19, 31:47]\n    yy, roi = _read(mm.gbox[sroi])\n    np.testing.assert_array_equal(xx[sroi], yy[roi])\n\n    # partial overlap\n    yy, roi = _read(gbx.translate_pix(mm.gbox, -3, -10))\n    assert roi == np.s_[10:64, 3:128]\n    np.testing.assert_array_equal(xx[:-10, :-3], yy[roi])\n    assert (yy[:10, :] == -999).all()\n    assert (yy[:, :3] == -999).all()\n\n    # scaling paste\n    yy, roi = _read(gbx.zoom_out(mm.gbox, 2), check_paste=True)\n    assert roi == np.s_[0:32, 0:64]\n    np.testing.assert_array_equal(xx[1::2, 1::2], yy)\n\n\ndef test_read_with_reproject(tmpdir):\n    from datacube.testutils import mk_test_image\n    from datacube.testutils.io import write_gtiff\n    from pathlib import Path\n\n    pp = Path(str(tmpdir))\n\n    xx = mk_test_image(128, 64, nodata=None)\n    assert (xx != -999).all()\n    tile = AlbersGS.tile_geobox((17, -40))[:64, :128]\n\n    mm = write_gtiff(pp/\'tst-read-with-reproject-128x64-int16.tif\', xx,\n                     crs=str(tile.crs),\n                     resolution=tile.resolution[::-1],\n                     offset=tile.transform*(0, 0),\n                     nodata=-999)\n    assert mm.gbox == tile\n\n    def _read(gbox,\n              resampling=\'nearest\',\n              fallback_nodata=None,\n              dst_nodata=-999):\n        with RasterFileDataSource(mm.path, 1, nodata=fallback_nodata).open() as rdr:\n            yy = np.full(gbox.shape, dst_nodata, dtype=rdr.dtype)\n            roi = read_time_slice(rdr, yy, gbox, resampling, dst_nodata)\n            return yy, roi\n\n    gbox = gbx.pad(mm.gbox, 10)\n    gbox = gbx.zoom_out(gbox, 0.873)\n    yy, roi = _read(gbox)\n\n    assert roi[0].start > 0 and roi[1].start > 0\n    assert (yy[0] == -999).all()\n\n    yy_expect, _ = rio_slurp(mm.path, gbox)\n    np.testing.assert_array_equal(yy, yy_expect)\n\n    gbox = gbx.zoom_out(mm.gbox[3:-3, 10:-10], 2.1)\n    yy, roi = _read(gbox)\n\n    assert roi_shape(roi) == gbox.shape\n    assert not (yy == -999).any()\n\n    gbox = GeoBox.from_geopolygon(mm.gbox.extent.to_crs(epsg3857).buffer(50),\n                                  resolution=mm.gbox.resolution)\n\n    assert gbox.extent.contains(mm.gbox.extent.to_crs(epsg3857))\n    assert gbox.crs != mm.gbox.crs\n    yy, roi = _read(gbox)\n    assert roi[0].start > 0 and roi[1].start > 0\n    assert (yy[0] == -999).all()\n\n    gbox = gbx.zoom_out(gbox, 4)\n    yy, roi = _read(gbox, resampling=\'average\')\n    nvalid = (yy != -999).sum()\n    nempty = (yy == -999).sum()\n    assert nvalid > nempty\n\n\ndef test_read_paste_v2(tmpdir):\n    from datacube.testutils import mk_test_image\n    from datacube.testutils.io import write_gtiff\n    from datacube.testutils.iodriver import open_reader\n    from pathlib import Path\n\n    pp = Path(str(tmpdir))\n\n    xx = mk_test_image(128, 64, nodata=None)\n    assert (xx != -999).all()\n\n    mm = write_gtiff(pp/\'tst-read-paste-128x64-int16.tif\', xx, nodata=None)\n\n    def _read(gbox, resampling=\'nearest\',\n              fallback_nodata=-999,\n              dst_nodata=-999,\n              check_paste=False):\n\n        rdr = open_reader(mm.path,\n                          nodata=fallback_nodata)\n        if check_paste:\n            # check that we are using paste\n            paste_ok, reason = can_paste(compute_reproject_roi(rdr_geobox(rdr), gbox))\n            assert paste_ok is True, reason\n\n        yy = np.full(gbox.shape, dst_nodata, dtype=rdr.dtype)\n        yy_, roi = read_time_slice_v2(rdr, gbox, resampling, dst_nodata)\n        yy[roi] = yy_\n        return yy, roi\n\n    # read native whole\n    yy, roi = _read(mm.gbox)\n    np.testing.assert_array_equal(xx, yy)\n    assert roi == np.s_[0:64, 0:128]\n\n    # read native whole, no nodata case\n    yy, roi = _read(mm.gbox, fallback_nodata=None)\n    np.testing.assert_array_equal(xx, yy)\n    assert roi == np.s_[0:64, 0:128]\n\n    # read native whole, ignoring small sub-pixel translation\n    yy, roi = _read(gbx.translate_pix(mm.gbox, 0.3, -0.4), fallback_nodata=-33)\n    np.testing.assert_array_equal(xx, yy)\n    assert roi == np.s_[0:64, 0:128]\n\n    # no overlap between src and dst\n    yy, roi = _read(gbx.translate_pix(mm.gbox, 10000, -10000))\n    assert roi_is_empty(roi)\n\n    # read with Y flipped\n    yy, roi = _read(gbx.flipy(mm.gbox))\n    np.testing.assert_array_equal(xx[::-1, :], yy)\n    assert roi == np.s_[0:64, 0:128]\n\n    # read with X flipped\n    yy, roi = _read(gbx.flipx(mm.gbox))\n    np.testing.assert_array_equal(xx[:, ::-1], yy)\n    assert roi == np.s_[0:64, 0:128]\n\n    # read with X and Y flipped\n    yy, roi = _read(gbx.flipy(gbx.flipx(mm.gbox)))\n    assert roi == np.s_[0:64, 0:128]\n    np.testing.assert_array_equal(xx[::-1, ::-1], yy[roi])\n\n    # dst is fully inside src\n    sroi = np.s_[10:19, 31:47]\n    yy, roi = _read(mm.gbox[sroi])\n    np.testing.assert_array_equal(xx[sroi], yy[roi])\n\n    # partial overlap\n    yy, roi = _read(gbx.translate_pix(mm.gbox, -3, -10))\n    assert roi == np.s_[10:64, 3:128]\n    np.testing.assert_array_equal(xx[:-10, :-3], yy[roi])\n    assert (yy[:10, :] == -999).all()\n    assert (yy[:, :3] == -999).all()\n\n    # scaling paste\n    yy, roi = _read(gbx.zoom_out(mm.gbox, 2), check_paste=True)\n    assert roi == np.s_[0:32, 0:64]\n    np.testing.assert_array_equal(xx[1::2, 1::2], yy)\n\n\ndef test_read_with_reproject_v2(tmpdir):\n    from datacube.testutils import mk_test_image\n    from datacube.testutils.io import write_gtiff\n    from datacube.testutils.iodriver import open_reader\n    from pathlib import Path\n\n    pp = Path(str(tmpdir))\n\n    xx = mk_test_image(128, 64, nodata=None)\n    assert (xx != -999).all()\n    tile = AlbersGS.tile_geobox((17, -40))[:64, :128]\n\n    def _read(gbox, resampling=\'nearest\',\n              fallback_nodata=-999,\n              dst_nodata=-999):\n\n        rdr = open_reader(mm.path,\n                          nodata=fallback_nodata)\n\n        yy = np.full(gbox.shape, dst_nodata, dtype=rdr.dtype)\n        yy_, roi = read_time_slice_v2(rdr, gbox, resampling, dst_nodata)\n        yy[roi] = yy_\n        return yy, roi\n\n    mm = write_gtiff(pp/\'tst-read-with-reproject-128x64-int16.tif\', xx,\n                     crs=str(tile.crs),\n                     resolution=tile.resolution[::-1],\n                     offset=tile.transform*(0, 0),\n                     nodata=-999)\n    assert mm.gbox == tile\n\n    gbox = gbx.pad(mm.gbox, 10)\n    gbox = gbx.zoom_out(gbox, 0.873)\n    yy, roi = _read(gbox)\n\n    assert roi[0].start > 0 and roi[1].start > 0\n    assert (yy[0] == -999).all()\n\n    yy_expect, _ = rio_slurp(mm.path, gbox)\n    np.testing.assert_array_equal(yy, yy_expect)\n\n    gbox = gbx.zoom_out(mm.gbox[3:-3, 10:-10], 2.1)\n    yy, roi = _read(gbox)\n\n    assert roi_shape(roi) == gbox.shape\n    assert not (yy == -999).any()\n\n    gbox = GeoBox.from_geopolygon(mm.gbox.extent.to_crs(epsg3857).buffer(50),\n                                  resolution=mm.gbox.resolution)\n\n    assert gbox.extent.contains(mm.gbox.extent.to_crs(epsg3857))\n    assert gbox.crs != mm.gbox.crs\n    yy, roi = _read(gbox)\n    assert roi[0].start > 0 and roi[1].start > 0\n    assert (yy[0] == -999).all()\n\n    gbox = gbx.zoom_out(gbox, 4)\n    yy, roi = _read(gbox, resampling=\'average\')\n    nvalid = (yy != -999).sum()\n    nempty = (yy == -999).sum()\n    assert nvalid > nempty\n'"
tests/ui/__init__.py,0,b''
tests/ui/test_common.py,0,"b'""""""\nModule\n""""""\nfrom pathlib import Path\n\nimport pytest\n\nfrom datacube.testutils import write_files, assert_file_structure\nfrom datacube.ui.common import get_metadata_path, _find_any_metadata_suffix, ui_path_doc_stream\n\n\ndef test_get_metadata_path():\n    test_file_structure = {\n        \'directory_dataset\': {\n            \'file1.txt\': \'\',\n            \'file2.txt\': \'\',\n            \'agdc-metadata.yaml.gz\': \'\'\n        },\n        \'file_dataset.tif\': \'\',\n        \'file_dataset.tif.agdc-md.yaml\': \'\',\n        \'dataset_metadata.yaml\': \'\',\n        \'no_metadata.tif\': \'\',\n    }\n\n    out_dir = write_files(test_file_structure)\n\n    assert_file_structure(out_dir, test_file_structure)\n\n    # A metadata file can be specified directly.\n    path = get_metadata_path(out_dir.joinpath(\'dataset_metadata.yaml\'))\n    assert Path(path).absolute() == out_dir.joinpath(\'dataset_metadata.yaml\').absolute()\n\n    # A dataset directory will have an internal \'agdc-metadata\' file.\n    path = get_metadata_path(out_dir.joinpath(\'directory_dataset\'))\n    assert Path(path).absolute() == out_dir.joinpath(\'directory_dataset\', \'agdc-metadata.yaml.gz\').absolute()\n\n    # Other out_dir can have a sibling file ending in \'agdc-md.yaml\'\n    path = get_metadata_path(out_dir.joinpath(\'file_dataset.tif\'))\n    assert Path(path).absolute() == out_dir.joinpath(\'file_dataset.tif.agdc-md.yaml\').absolute()\n\n    # URLs are always themselves\n    example_url = \'http://localhost/dataset.yaml\'\n    url = get_metadata_path(example_url)\n    assert url == example_url\n\n    # Lack of metadata raises an error.\n    with pytest.raises(ValueError):\n        get_metadata_path(out_dir.joinpath(\'no_metadata.tif\'))\n\n    # Nonexistent dataset raises a ValueError.\n    with pytest.raises(ValueError):\n        get_metadata_path(out_dir.joinpath(\'missing-dataset.tif\'))\n\n\ndef test_find_any_metatadata_suffix():\n    files = write_files({\n        \'directory_dataset\': {\n            \'file1.txt\': \'\',\n            \'file2.txt\': \'\',\n            \'agdc-metadata.json.gz\': \'\'\n        },\n        \'file_dataset.tif.agdc-md.yaml\': \'\',\n        \'dataset_metadata.YAML\': \'\',\n        \'no_metadata.tif\': \'\',\n        \'ambigous.yml\': \'\',\n        \'ambigous.yaml\': \'\',\n    })\n\n    path = _find_any_metadata_suffix(files.joinpath(\'dataset_metadata\'))\n    assert Path(path).absolute() == files.joinpath(\'dataset_metadata.YAML\').absolute()\n\n    path = _find_any_metadata_suffix(files.joinpath(\'directory_dataset\', \'agdc-metadata\'))\n    assert Path(path).absolute() == files.joinpath(\'directory_dataset\', \'agdc-metadata.json.gz\').absolute()\n\n    path = _find_any_metadata_suffix(files.joinpath(\'file_dataset.tif.agdc-md\'))\n    assert Path(path).absolute() == files.joinpath(\'file_dataset.tif.agdc-md.yaml\').absolute()\n\n    # Returns none if none exist\n    path = _find_any_metadata_suffix(files.joinpath(\'no_metadata\'))\n    assert path is None\n\n    with pytest.raises(ValueError):\n        _find_any_metadata_suffix(files.joinpath(\'ambigous\'))\n\n\ndef test_ui_path_doc_stream(httpserver):\n    filename = \'dataset_metadata.yaml\'\n    file_content = \'\'\n    out_dir = write_files({filename: file_content})\n\n    httpserver.expect_request(filename).respond_with_data(file_content)\n\n    input_paths = [Path(out_dir) / \'dataset_metadata.yaml\', httpserver.url_for(filename)]\n\n    for input_path, (doc, resolved_path) in zip(input_paths, ui_path_doc_stream(input_paths)):\n        assert doc == {}\n        assert input_path == resolved_path\n'"
tests/ui/test_expression_parsing.py,0,"b'from datetime import datetime\nfrom dateutil.tz import tzutc\n\nfrom datacube.model import Range\nfrom datacube.ui import parse_expressions\n\n\ndef test_parse_empty_str():\n    q = parse_expressions(\'\')\n    assert q == {}\n\n\ndef test_between_expression():\n    q = parse_expressions(\'time in [2014, 2015]\')\n    assert \'time\' in q\n    r = q[\'time\']\n    assert isinstance(r, Range)\n    assert isinstance(r.begin, datetime)\n    assert isinstance(r.end, datetime)\n\n    for k in (\'lon\', \'lat\', \'x\', \'y\'):\n        q = parse_expressions(\'{} in [10, 11.3]\'.format(k))\n        assert k in q\n        r = q[k]\n        assert isinstance(r, Range)\n        assert isinstance(r.begin, (int, float))\n        assert isinstance(r.end, (int, float))\n        assert r == Range(10, 11.3)\n\n\ndef test_parse_simple_expression():\n    assert parse_expressions(\'platform = 4\') == {\'platform\': 4}\n    assert parse_expressions(\'platform = ""LANDSAT_8""\') == {\'platform\': \'LANDSAT_8\'}\n    assert parse_expressions(\'platform = LANDSAT_8\') == {\'platform\': \'LANDSAT_8\'}\n    assert parse_expressions(\'platform = ""LAND SAT_8""\') == {\'platform\': \'LAND SAT_8\'}\n\n    assert parse_expressions(\'lat in [4, 6]\') == {\'lat\': Range(4, 6)}\n\n\ndef test_parse_uri_expression():\n    assert parse_expressions(\'uri = file:///f/data/test.nc\') == {\'uri\': \'file:///f/data/test.nc\'}\n    assert parse_expressions(\'uri = ""file:///f/data/test.nc""\') == {\'uri\': \'file:///f/data/test.nc\'}\n    assert parse_expressions(\'uri = ""file:///f/data/test me.nc""\') == {\'uri\': \'file:///f/data/test me.nc\'}\n    assert parse_expressions(\'uri = file:///C:/f/data/test.nc\') == {\'uri\': \'file:///C:/f/data/test.nc\'}\n    assert parse_expressions(\'uri = ""file:///C:/f/data/test.nc""\') == {\'uri\': \'file:///C:/f/data/test.nc\'}\n    assert parse_expressions(\'uri = ""file:///C:/f/data/test me.nc""\') == {\'uri\': \'file:///C:/f/data/test me.nc\'}\n\n\ndef test_parse_dates():\n    assert parse_expressions(\'time in 2014-03-02\') == {\n        \'time\': Range(begin=datetime(2014, 3, 2, 0, 0, tzinfo=tzutc()),\n                      end=datetime(2014, 3, 2, 23, 59, 59, 999999, tzinfo=tzutc()))\n    }\n\n    assert parse_expressions(\'time in 2014-3-2\') == {\n        \'time\': Range(begin=datetime(2014, 3, 2, 0, 0, tzinfo=tzutc()),\n                      end=datetime(2014, 3, 2, 23, 59, 59, 999999, tzinfo=tzutc()))\n    }\n\n    # A missing day defaults to the first of the month.\n    # They are probably better off using in-expessions in these cases (eg. ""time in 2013-01""), but it\'s here\n    # for backwards compatibility.\n    march_2014 = {\n        \'time\': Range(begin=datetime(2014, 3, 1, 0, 0, tzinfo=tzutc()),\n                      end=datetime(2014, 3, 31, 23, 59, 59, 999999, tzinfo=tzutc()))\n    }\n    assert parse_expressions(\'time in 2014-03\') == march_2014\n    assert parse_expressions(\'time in 2014-3\') == march_2014\n\n    implied_feb_march_2014 = {\n        \'time\': Range(begin=datetime(2014, 2, 1, 0, 0, tzinfo=tzutc()),\n                      end=datetime(2014, 3, 31, 23, 59, 59, 999999, tzinfo=tzutc()))\n    }\n    assert parse_expressions(\'time in [2014-02, 2014-03]\') == implied_feb_march_2014\n\n\ndef test_parse_date_ranges():\n    eighth_march_2014 = {\n        \'time\': Range(datetime(2014, 3, 8, tzinfo=tzutc()), datetime(2014, 3, 8, 23, 59, 59, 999999, tzinfo=tzutc()))\n    }\n    assert parse_expressions(\'time in 2014-03-08\') == eighth_march_2014\n    assert parse_expressions(\'time in 2014-03-8\') == eighth_march_2014\n\n    march_2014 = {\n        \'time\': Range(datetime(2014, 3, 1, tzinfo=tzutc()), datetime(2014, 3, 31, 23, 59, 59, 999999, tzinfo=tzutc()))\n    }\n    assert parse_expressions(\'time in 2014-03\') == march_2014\n    assert parse_expressions(\'time in 2014-3\') == march_2014\n    # Leap year, 28 days\n    feb_2014 = {\n        \'time\': Range(datetime(2014, 2, 1, tzinfo=tzutc()), datetime(2014, 2, 28, 23, 59, 59, 999999, tzinfo=tzutc()))\n    }\n    assert parse_expressions(\'time in 2014-02\') == feb_2014\n    assert parse_expressions(\'time in 2014-2\') == feb_2014\n\n    # Entire year\n    year_2014 = {\n        \'time\': Range(datetime(2014, 1, 1, tzinfo=tzutc()), datetime(2014, 12, 31, 23, 59, 59, 999999, tzinfo=tzutc()))\n    }\n    assert parse_expressions(\'time in 2014\') == year_2014\n\n\ndef test_parse_multiple_simple_expressions():\n    # Multiple expressions in one command-line statement.\n    # Mixed whitespace:\n    between_exp = parse_expressions(\'platform=LS8 lat in [-4, 23.5] instrument=""OTHER""\')\n    assert between_exp == {\'platform\': \'LS8\', \'lat\': Range(-4, 23.5), \'instrument\': \'OTHER\'}\n    # Range(x,y) is ""equal"" to (x, y). Check explicitly that it\'s a range:\n    assert between_exp[\'lat\'].begin == -4\n'"
tests/ui/test_task_app.py,0,"b'# coding=utf-8\n""""""\nModule\n""""""\n\nfrom datacube.ui.task_app import task_app, run_tasks, wrap_task\nimport datacube.executor\n\n\ndef make_test_config(index, config, **kwargs):\n    assert index == \'Fake Index\'\n    assert \'config_arg\' in kwargs\n\n    config[\'some_item\'] = \'make_test_config\'\n    config[\'num_tasks\'] = config.get(\'num_tasks\', 3)\n    return config\n\n\ndef make_test_tasks(index, config, **kwargs):\n    assert index == \'Fake Index\'\n    assert \'task_arg\' in kwargs\n\n    num_tasks = config[\'num_tasks\']\n    for i in range(0, num_tasks):\n        yield \'Task: {}\'.format(i)\n\n\n@task_app(make_config=make_test_config, make_tasks=make_test_tasks)\ndef my_test_app(index, config, tasks, **kwargs):\n    assert index == \'Fake Index\'\n    assert config[\'some_item\'] == \'make_test_config\'\n    assert \'app_arg\' in kwargs\n\n    task_list = list(tasks)\n    assert len(task_list) == config[\'num_tasks\']\n\n\ndef test_task_app(tmpdir):\n    index = \'Fake Index\'\n\n    app_config = tmpdir.join(""app_config.yaml"")\n    app_config.write(\'name: Test Config\\r\\n\'\n                     \'description: This is my test app config file\')\n\n    my_test_app(index, str(app_config), app_arg=True, config_arg=True, task_arg=True)\n\n\ndef test_task_app_with_task_file(tmpdir):\n    index = \'Fake Index\'\n\n    app_config = tmpdir.join(""app_config.yaml"")\n    app_config.write(\'name: Test Config\\r\\n\'\n                     \'description: This is my test app config file\')\n\n    taskfile = tmpdir.join(""tasks.bin"")\n    assert not taskfile.check()\n\n    my_test_app(index, app_config=str(app_config), output_tasks_file=str(taskfile), config_arg=True, task_arg=True)\n\n    assert taskfile.check()\n\n    my_test_app(index, input_tasks_file=str(taskfile), app_arg=True)\n\n\ndef test_task_app_with_no_tasks(tmpdir):\n    index = \'Fake Index\'\n\n    app_config = tmpdir.join(""app_config.yaml"")\n    app_config.write(\'name: Test Config\\r\\n\'\n                     \'description: This is my test app config file\\r\\n\'\n                     \'num_tasks: 0\')\n\n    taskfile = tmpdir.join(""tasks.bin"")\n    assert not taskfile.check()\n\n    my_test_app(index, app_config=str(app_config), output_tasks_file=str(taskfile), config_arg=True, task_arg=True)\n\n    assert not taskfile.check()\n\n\ndef test_task_app_year_splitting():\n    import pandas as pd\n    from datacube.ui.task_app import validate_year, break_query_into_years\n    one_millisecond = pd.Timedelta(\'1 ms\')\n\n    def is_close(ts1, ts2, max_delta=one_millisecond):\n        return abs(pd.Timestamp(ts1) - pd.Timestamp(ts2)) < max_delta\n\n    assert validate_year(None, None, None) is None\n    year_range = validate_year(None, None, \'1996-2004\')\n    assert is_close(year_range[0], \'1996-01-01 00:00:00\')\n    assert is_close(year_range[1], \'2004-12-31 23:59:59.999999999\')\n\n    year_range = validate_year(None, None, \'2003\')\n    assert is_close(year_range[0], \'2003-01-01 00:00:00\')\n    assert is_close(year_range[1], \'2003-12-31 23:59:59.999999999\')\n\n    # Test that a no year range makes a single query\n    year_range = None\n    query = break_query_into_years(year_range)\n    assert len(query) == 1\n    assert query[0] == {}\n\n    # Test that a no year range makes a single query with additional params\n    year_range = None\n    test_cell_index = (11, 12)\n    query = break_query_into_years(year_range, cell_index=test_cell_index)\n    assert len(query) == 1\n    assert query[0] == {\'cell_index\': test_cell_index}\n\n    # Test that a single year makes a single query\n    year_range = (""1996"", ""1996"")\n    query = break_query_into_years(year_range)\n    assert len(query) == 1\n    assert is_close(query[0][\'time\'][0], \'1996-01-01 00:00:00\')\n    assert is_close(query[0][\'time\'][1], \'1996-12-31 23:59:59.999999999\')\n\n    # Test that a multiple years makes multiple queries\n    year_range = (""1996"", ""1997"")\n    query = break_query_into_years(year_range)\n    assert len(query) == 2\n    assert is_close(query[0][\'time\'][0], \'1996-01-01 00:00:00\')\n    assert is_close(query[0][\'time\'][1], \'1996-12-31 23:59:59.999999999\')\n    assert is_close(query[1][\'time\'][0], \'1997-01-01 00:00:00\')\n    assert is_close(query[1][\'time\'][1], \'1997-12-31 23:59:59.999999999\')\n\n    # Check that additional kwargs can be used in the query\n    year_range = (""1996"", ""1997"")\n    test_cell_index = (11, 12)\n    query = break_query_into_years(year_range, cell_index=test_cell_index)\n    assert len(query) == 2\n    assert is_close(query[0][\'time\'][0], \'1996-01-01 00:00:00\')\n    assert is_close(query[0][\'time\'][1], \'1996-12-31 23:59:59.999999999\')\n    assert query[0][\'cell_index\'] == test_cell_index\n    assert is_close(query[1][\'time\'][0], \'1997-01-01 00:00:00\')\n    assert is_close(query[1][\'time\'][1], \'1997-12-31 23:59:59.999999999\')\n    assert query[1][\'cell_index\'] == test_cell_index\n\n\ndef test_task_app_cell_index(tmpdir):\n    from datacube.ui.task_app import validate_cell_index, validate_cell_list, cell_list_to_file\n\n    assert validate_cell_index(None, None, None) is None\n    assert validate_cell_index(None, None, \'17,-12\') == (17, -12)\n\n    cell_list = [(17, 12), (16, -10), (-23, 0)]\n\n    cell_list_file = tmpdir.join(\'cell_list.txt\')\n    if cell_list_file.exists():\n        cell_list_file.unlink()\n    assert not cell_list_file.check()\n\n    cell_list_to_file(str(cell_list_file), cell_list)\n\n    assert cell_list_file.check()\n\n    assert validate_cell_list(None, None, None) is None\n    assert validate_cell_list(None, None, str(cell_list_file)) == cell_list\n\n\ndef test_run_tasks():\n    executor = datacube.executor.SerialExecutor()\n    tasks = ({\'val\': i} for i in range(3))\n    tasks_to_do = list(range(3))\n\n    def task_func(task):\n        x = task[\'val\']\n        return (x, x**2)\n\n    def process_result_func(result):\n        assert result[0]**2 == result[1]\n        tasks_to_do.remove(result[0])\n\n    run_tasks(tasks, executor, task_func, process_result_func)\n    assert not tasks_to_do\n\n    # no task proc specified\n    tasks = ({\'val\': i} for i in range(3))\n    run_tasks(tasks, executor, task_func)\n\n\ndef test_wrap_task():\n    def task_with_args(task, a, b):\n        return (task, a, b)\n\n    assert task_with_args(1, 2, \'a\') == (1, 2, \'a\')\n    assert wrap_task(task_with_args, \'a\', \'b\')(0) == (0, \'a\', \'b\')\n'"
datacube/drivers/netcdf/__init__.py,0,"b""from ._write import write_dataset_to_netcdf, create_netcdf_storage_unit\nfrom . import writer as netcdf_writer\nfrom .writer import Variable\n\n__all__ = (\n    'create_netcdf_storage_unit',\n    'write_dataset_to_netcdf',\n    'netcdf_writer',\n    'Variable',\n)\n"""
datacube/drivers/netcdf/_safestrings.py,0,"b'""""""\nProvides `SafeStringsDataset`, a replacement netCDF4.Dataset class which works\naround a bug in NetCDF4 which causes attribute strings written to files to\nbe incompatible with older NetCDF software. Ensures that strings are only\nwritten as UTF-8 encoded bytes.\n\nFor more information see https://github.com/Unidata/netcdf4-python/issues/448\n""""""\nimport netCDF4\n\n\nclass _VariableProxy(object):\n    """"""\n    Wraps a netCDF4 Variable object, ensuring that any attributes are written\n    as bytes and not unicode strings.\n    """"""\n    __initialized = False\n\n    def __init__(self, wrapped):\n        self._wrapped = wrapped\n        self.__initialized = True\n\n    @property  # type: ignore\n    def __class__(self):\n        return self._wrapped.__class__\n\n    def __getattr__(self, name):\n        return getattr(self._wrapped, name)\n\n    def __setattr__(self, name, value):\n        if self.__initialized:\n            if isinstance(value, str):\n                value = value.encode(\'ascii\')\n            setattr(self._wrapped, name, value)\n        else:\n            super(_VariableProxy, self).__setattr__(name, value)\n\n    def __getitem__(self, key):\n        return self._wrapped.__getitem__(key)\n\n    def __setitem__(self, key, value):\n        self._wrapped.__setitem__(key, value)\n\n    def setncattr(self, name, value):\n        self._wrapped.setncattr(name, value)\n\n\nclass _NC4DatasetProxy(object):\n    """"""\n    Mixin to the NetCDF4.Dataset, ensuring that attributes are written as bytes\n    and not unicode strings, and that created and accessed variables are\n    wrapped in _VariableProxy objects.\n\n    Overrides the `createVariable()` method, and the `nco[varname]` style access.\n\n    Doesn\'t yet support `nco.variables[varname]` style access of variables.\n    """"""\n\n    def __setattr__(self, name, value):\n        if isinstance(value, str):\n            value = value.encode(\'ascii\')\n        super(_NC4DatasetProxy, self).__setattr__(name, value)\n\n    def setncattr(self, name, value):\n        if isinstance(value, str):\n            value = value.encode(\'ascii\')\n        super(_NC4DatasetProxy, self).setncattr(name, value)\n\n    def __getitem__(self, name):\n        var = super(_NC4DatasetProxy, self).__getitem__(name)\n        return _VariableProxy(var)\n\n    #: pylint: disable=invalid-name\n    def createVariable(self, *args, **kwargs):\n        new_var = super(_NC4DatasetProxy, self).createVariable(*args, **kwargs)\n        return _VariableProxy(new_var)\n\n\nclass SafeStringsDataset(_NC4DatasetProxy, netCDF4.Dataset):\n    """"""\n    A wrapper for NetCDF4.Dataset, which ensures all attributes and variable attributes\n    are stored using encoded bytes and not unicode strings.\n\n    Unicode strings cause a bug in the NetCDF4 library which make them unreadable by\n    some older software.\n    """"""\n    pass\n'"
datacube/drivers/netcdf/_write.py,0,"b'from pathlib import Path\nimport logging\n\nfrom . import writer as netcdf_writer\nfrom datacube.utils import DatacubeException\nfrom datacube.storage._hdf5 import HDF5_LOCK\n\n\n_LOG = logging.getLogger(__name__)\n\n\ndef create_netcdf_storage_unit(filename,\n                               crs, coordinates, variables, variable_params, global_attributes=None,\n                               netcdfparams=None):\n    """"""\n    Create a NetCDF file on disk.\n\n    :param pathlib.Path filename: filename to write to\n    :param datacube.utils.geometry.CRS crs: Datacube CRS object defining the spatial projection\n    :param dict coordinates: Dict of named `datacube.model.Coordinate`s to create\n    :param dict variables: Dict of named `datacube.model.Variable`s to create\n    :param dict variable_params:\n        Dict of dicts, with keys matching variable names, of extra parameters for variables\n    :param dict global_attributes: named global attributes to add to output file\n    :param dict netcdfparams: Extra parameters to use when creating netcdf file\n    :return: open netCDF4.Dataset object, ready for writing to\n    """"""\n    filename = Path(filename)\n    if filename.exists():\n        raise RuntimeError(\'Storage Unit already exists: %s\' % filename)\n\n    try:\n        filename.parent.mkdir(parents=True)\n    except OSError:\n        pass\n\n    _LOG.info(\'Creating storage unit: %s\', filename)\n\n    nco = netcdf_writer.create_netcdf(str(filename), **(netcdfparams or {}))\n\n    for name, coord in coordinates.items():\n        if coord.values.ndim > 0:  # skip CRS coordinate\n            netcdf_writer.create_coordinate(nco, name, coord.values, coord.units)\n\n    grid_mapping = netcdf_writer.DEFAULT_GRID_MAPPING\n    netcdf_writer.create_grid_mapping_variable(nco, crs, name=grid_mapping)\n\n    for name, variable in variables.items():\n        has_crs = all(dim in variable.dims for dim in crs.dimensions)\n        var_params = variable_params.get(name, {})\n        data_var = netcdf_writer.create_variable(nco, name, variable,\n                                                 grid_mapping=grid_mapping if has_crs else None,\n                                                 **var_params)\n\n        for key, value in var_params.get(\'attrs\', {}).items():\n            setattr(data_var, key, value)\n\n    for key, value in (global_attributes or {}).items():\n        setattr(nco, key, value)\n\n    return nco\n\n\ndef write_dataset_to_netcdf(dataset, filename, global_attributes=None, variable_params=None,\n                            netcdfparams=None):\n    """"""\n    Write a Data Cube style xarray Dataset to a NetCDF file\n\n    Requires a spatial Dataset, with attached coordinates and global crs attribute.\n\n    :param `xarray.Dataset` dataset:\n    :param filename: Output filename\n    :param global_attributes: Global file attributes. dict of attr_name: attr_value\n    :param variable_params: dict of variable_name: {param_name: param_value, [...]}\n                            Allows setting storage and compression options per variable.\n                            See the `netCDF4.Dataset.createVariable` for available\n                            parameters.\n    :param netcdfparams: Optional params affecting netCDF file creation\n    """"""\n    global_attributes = global_attributes or {}\n    variable_params = variable_params or {}\n    filename = Path(filename)\n\n    if not dataset.data_vars.keys():\n        raise DatacubeException(\'Cannot save empty dataset to disk.\')\n\n    if dataset.geobox is None:\n        raise DatacubeException(\'Dataset geobox property is None, cannot write to NetCDF file.\')\n\n    try:\n        HDF5_LOCK.acquire(blocking=True)\n        nco = create_netcdf_storage_unit(filename,\n                                         dataset.geobox.crs,\n                                         dataset.coords,\n                                         dataset.data_vars,\n                                         variable_params,\n                                         global_attributes,\n                                         netcdfparams)\n\n        for name, variable in dataset.data_vars.items():\n            nco[name][:] = netcdf_writer.netcdfy_data(variable.values)\n\n        nco.close()\n    finally:\n        HDF5_LOCK.release()\n'"
datacube/drivers/netcdf/driver.py,0,"b'from urllib.parse import urlsplit\n\nfrom datacube.storage._rio import RasterDatasetDataSource\nfrom datacube.utils.uris import normalise_path\nfrom ._write import write_dataset_to_netcdf\n\nPROTOCOL = \'file\'\nFORMAT = \'NetCDF\'\n\n\nclass NetcdfReaderDriver(object):\n    def __init__(self):\n        self.name = \'NetcdfReader\'\n        self.protocols = [PROTOCOL]\n        self.formats = [FORMAT]\n\n    def supports(self, protocol, fmt):\n        return (protocol in self.protocols and\n                fmt in self.formats)\n\n    def new_datasource(self, band):\n        return RasterDatasetDataSource(band)\n\n\ndef reader_driver_init():\n    return NetcdfReaderDriver()\n\n\nclass NetcdfWriterDriver(object):\n    def __init__(self):\n        pass\n\n    @property\n    def aliases(self):\n        return [\'NetCDF CF\']\n\n    @property\n    def format(self):\n        return FORMAT\n\n    @property\n    def uri_scheme(self):\n        return PROTOCOL\n\n    def mk_uri(self, file_path, storage_config):\n        """"""\n        Constructs a URI from the file_path and storage config.\n\n        A typical implementation should return f\'{scheme}://{file_path}\'\n\n        Example:\n            file_path = \'/path/to/my_file.nc\'\n            storage_config = {\'driver\': \'NetCDF CF\'}\n\n            mk_uri(file_path, storage_config) should return \'file:///path/to/my_file.nc\'\n\n        :param Path file_path: The file path of the file to be converted into a URI.\n        :param dict storage_config: The dict holding the storage config found in the ingest definition.\n        :return: file_path as a URI that the Driver understands.\n        :rtype: str\n        """"""\n        return normalise_path(file_path).as_uri()\n\n    def write_dataset_to_storage(self, dataset, file_uri,\n                                 global_attributes=None,\n                                 variable_params=None,\n                                 storage_config=None,\n                                 **kwargs):\n        # TODO: Currently ingestor copies chunking info from storage_config to\n        # variable_params, this logic should probably happen here.\n\n        write_dataset_to_netcdf(dataset, urlsplit(file_uri).path,\n                                global_attributes=global_attributes,\n                                variable_params=variable_params,\n                                **kwargs)\n\n        return {}\n\n\ndef writer_driver_init():\n    return NetcdfWriterDriver()\n'"
datacube/drivers/netcdf/writer.py,0,"b'# coding=utf-8\n""""""\nCreate netCDF4 Storage Units and write data to them\n""""""\n\nimport logging\nimport numbers\nfrom datetime import datetime\nfrom collections import namedtuple\nimport numpy\n\nfrom datacube.utils.masking import describe_flags_def\nfrom datacube.utils import geometry, data_resolution_and_offset\nfrom ._safestrings import SafeStringsDataset as Dataset\n\nfrom datacube import __version__\n\nVariable = namedtuple(\'Variable\', (\'dtype\', \'nodata\', \'dims\', \'units\'))\n_LOG = logging.getLogger(__name__)\nDEFAULT_GRID_MAPPING = \'spatial_ref\'\n\n_STANDARD_COORDINATES = {\n    \'longitude\': {\n        \'standard_name\': \'longitude\',\n        \'long_name\': \'longitude\',\n        \'axis\': \'X\'\n    },\n    \'latitude\': {\n        \'standard_name\': \'latitude\',\n        \'long_name\': \'latitude\',\n        \'axis\': \'Y\'\n    },\n    \'x\': {\n        \'standard_name\': \'projection_x_coordinate\',\n        \'long_name\': \'x coordinate of projection\',\n        # \'axis\': \'X\'  # this makes gdal (2.0.0) think x is longitude and it does bad things to it (subtract 360)\n    },\n    \'y\': {\n        \'standard_name\': \'projection_y_coordinate\',\n        \'long_name\': \'y coordinate of projection\',\n        # \'axis\': \'Y\'  # see x\'s axis comment above\n    },\n    \'time\': {\n        \'standard_name\': \'time\',\n        \'long_name\': \'Time, unix time-stamp\',\n        \'axis\': \'T\',\n        \'calendar\': \'standard\'\n    }\n}\n\n\ndef create_netcdf(netcdf_path, **kwargs):\n    """"""\n    Create and return an empty NetCDF file\n\n    :param netcdf_path: File path to write to\n    :param kwargs: See :class:`Dataset` for more information\n    :return: open NetCDF Dataset\n    """"""\n    nco = Dataset(netcdf_path, \'w\', **kwargs)\n    nco.date_created = datetime.today().isoformat()\n    nco.setncattr(\'Conventions\', \'CF-1.6, ACDD-1.3\')\n    nco.history = (""NetCDF-CF file created by ""\n                   ""datacube version \'{}\' at {:%Y%m%d}.""\n                   .format(__version__, datetime.utcnow()))\n    return nco\n\n\ndef append_netcdf(netcdf_path):\n    """"""\n    Open a NetCDF file in append mode\n\n    :param netcdf_path:\n    :return: open NetCDF Dataset\n    """"""\n    return Dataset(netcdf_path, \'a\')\n\n\ndef create_coordinate(nco, name, labels, units):\n    """"""\n    :type nco: netCDF4.Dataset\n    :type name: str\n    :type labels: numpy.array\n    :type units: str\n    :rtype: netCDF4.Variable\n    """"""\n    labels = netcdfy_coord(labels)\n\n    nco.createDimension(name, labels.size)\n    var = nco.createVariable(name, labels.dtype, name)\n    var[:] = labels\n\n    var.units = units\n    for key, value in _STANDARD_COORDINATES.get(name, {}).items():\n        setattr(var, key, value)\n\n    return var\n\n\ndef create_variable(nco, name, var, grid_mapping=None, attrs=None, **kwargs):\n    """"""\n    :param nco:\n    :param name:\n    :param datacube.model.Variable var:\n    :param kwargs:\n    :return:\n    """"""\n    assert var.dtype.kind != \'U\'  # Creates Non CF-Compliant NetCDF File\n\n    def clamp_chunksizes(chunksizes, dim_names):\n        if chunksizes is None:\n            return None\n\n        maxsizes = [len(nco.dimensions[dim]) for dim in dim_names]\n\n        # pad chunksizes to new dimension length if too short\n        chunksizes = tuple(chunksizes) + tuple(maxsizes[len(chunksizes):])\n\n        # clamp\n        return [min(sz, maxsz) for sz, maxsz in zip(chunksizes, maxsizes)]\n\n    if var.dtype.kind == \'S\' and var.dtype.itemsize > 1:\n        new_dim_name = name + \'_nchar\'\n        nco.createDimension(new_dim_name, size=var.dtype.itemsize)\n\n        dims = tuple(var.dims) + (new_dim_name,)\n        datatype = numpy.dtype(\'S1\')\n    else:\n        dims = var.dims\n        datatype = var.dtype\n\n    chunksizes = clamp_chunksizes(kwargs.pop(\'chunksizes\', None), dims)\n\n    data_var = nco.createVariable(varname=name,\n                                  datatype=datatype,\n                                  dimensions=dims,\n                                  fill_value=getattr(var, \'nodata\', None),\n                                  chunksizes=chunksizes,\n                                  **kwargs)\n    if grid_mapping is not None:\n        data_var.grid_mapping = grid_mapping\n    if getattr(var, \'units\', None):\n        data_var.units = var.units\n    data_var.set_auto_maskandscale(False)\n    return data_var\n\n\ndef _create_latlon_grid_mapping_variable(nco, crs, name=DEFAULT_GRID_MAPPING):\n    crs_var = nco.createVariable(name, \'i4\')\n    crs_var.long_name = crs._crs.name  # ""Lon/Lat Coords in WGS84""\n\n    # also available as crs._crs.to_cf()[\'grid_mapping_name\']\n    crs_var.grid_mapping_name = \'latitude_longitude\'\n\n    crs_var.longitude_of_prime_meridian = 0.0\n    return crs_var\n\n\ndef _write_albers_params(crs_var, crs):\n    # http://spatialreference.org/ref/epsg/gda94-australian-albers/html/\n    # http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/build/cf-conventions.html#appendix-grid-mappings\n    cf = crs._crs.to_cf()\n\n    crs_var.grid_mapping_name = cf[\'grid_mapping_name\']\n    crs_var.standard_parallel = tuple(cf[\'standard_parallel\'])\n    crs_var.longitude_of_central_meridian = cf[\'longitude_of_central_meridian\']\n    crs_var.latitude_of_projection_origin = cf[\'latitude_of_projection_origin\']\n\n\ndef _write_sinusoidal_params(crs_var, crs):\n    cf = crs._crs.to_cf()\n\n    crs_var.grid_mapping_name = cf[\'grid_mapping_name\']\n    crs_var.longitude_of_central_meridian = cf[\'longitude_of_projection_origin\']\n\n\ndef _write_transverse_mercator_params(crs_var, crs):\n    cf = crs._crs.to_cf()\n\n    # http://spatialreference.org/ref/epsg/wgs-84-utm-zone-54s/\n    crs_var.grid_mapping_name = cf[\'grid_mapping_name\']\n    crs_var.scale_factor_at_central_meridian = cf[\'scale_factor_at_central_meridian\']\n    crs_var.longitude_of_central_meridian = cf[\'longitude_of_central_meridian\']\n    crs_var.latitude_of_projection_origin = cf[\'latitude_of_projection_origin\']\n\n\ndef _write_lcc2_params(crs_var, crs):\n    cf = crs._crs.to_cf()\n\n    # e.g. http://spatialreference.org/ref/sr-org/mexico-inegi-lambert-conformal-conic/\n    crs_var.grid_mapping_name = cf[\'grid_mapping_name\']\n    crs_var.standard_parallel =  cf[\'standard_parallel\']\n    crs_var.latitude_of_projection_origin = cf[\'latitude_of_projection_origin\']\n    crs_var.longitude_of_central_meridian = cf[\'longitude_of_central_meridian\']\n    crs_var.false_easting = cf[\'false_easting\']\n    crs_var.false_northing = cf[\'false_northing\']\n    crs_var.semi_major_axis = crs.semi_major_axis\n    crs_var.semi_minor_axis = crs.semi_minor_axis\n\n\nCRS_PARAM_WRITERS = {\n    \'albers_conic_equal_area\': _write_albers_params,\n    \'albers_conical_equal_area\': _write_albers_params,\n    \'sinusoidal\': _write_sinusoidal_params,\n    \'transverse_mercator\': _write_transverse_mercator_params,\n    \'lambert_conformal_conic_2sp\': _write_lcc2_params,\n    \'lambert_conformal_conic\': _write_lcc2_params,\n}\n\n\ndef _create_projected_grid_mapping_variable(nco, crs, name=DEFAULT_GRID_MAPPING):\n    cf = crs._crs.to_cf()\n    grid_mapping_name = cf[\'grid_mapping_name\']\n    if grid_mapping_name not in CRS_PARAM_WRITERS:\n        raise ValueError(\'{} CRS is not supported\'.format(grid_mapping_name))\n\n    crs_var = nco.createVariable(name, \'i4\')\n    CRS_PARAM_WRITERS[grid_mapping_name](crs_var, crs)\n\n    crs_var.false_easting = cf[\'false_easting\']\n    crs_var.false_northing = cf[\'false_northing\']\n    crs_var.long_name = crs._crs.name\n\n    return crs_var\n\n\ndef _write_geographical_extents_attributes(nco, extent):\n    geo_extents = extent.to_crs(geometry.CRS(""EPSG:4326""))\n    nco.geospatial_bounds = geo_extents.wkt\n    nco.geospatial_bounds_crs = ""EPSG:4326""\n\n    geo_bounds = geo_extents.boundingbox\n    nco.geospatial_lat_min = geo_bounds.bottom\n    nco.geospatial_lat_max = geo_bounds.top\n    nco.geospatial_lat_units = ""degrees_north""\n    nco.geospatial_lon_min = geo_bounds.left\n    nco.geospatial_lon_max = geo_bounds.right\n    nco.geospatial_lon_units = ""degrees_east""\n\n    # TODO: broken anyway...\n    # nco.geospatial_lat_resolution = ""{} degrees"".format(abs(geobox.affine.e))\n    # nco.geospatial_lon_resolution = ""{} degrees"".format(abs(geobox.affine.a))\n\n\ndef create_grid_mapping_variable(nco, crs, name=DEFAULT_GRID_MAPPING):\n    if crs.geographic:\n        crs_var = _create_latlon_grid_mapping_variable(nco, crs, name)\n    elif crs.projected:\n        crs_var = _create_projected_grid_mapping_variable(nco, crs, name)\n    else:\n        raise ValueError(\'Unknown CRS\')\n\n    # mark crs variable as a coordinate\n    coords = getattr(nco, \'coordinates\', None)\n    coords = [] if coords is None else coords.split(\',\')\n    if name not in coords:\n        coords.append(name)\n    nco.coordinates = \',\'.join(coords)\n\n    crs_var.semi_major_axis = crs.semi_major_axis\n    crs_var.semi_minor_axis = crs.semi_minor_axis\n    crs_var.inverse_flattening = crs.inverse_flattening\n    crs_var.crs_wkt = crs.wkt\n\n    crs_var.spatial_ref = crs.wkt\n\n    dims = crs.dimensions\n    xres, xoff = data_resolution_and_offset(nco[dims[1]])\n    yres, yoff = data_resolution_and_offset(nco[dims[0]])\n    crs_var.GeoTransform = [xoff, xres, 0.0, yoff, 0.0, yres]\n\n    left, right = nco[dims[1]][0] - 0.5 * xres, nco[dims[1]][-1] + 0.5 * xres\n    bottom, top = nco[dims[0]][0] - 0.5 * yres, nco[dims[0]][-1] + 0.5 * yres\n    _write_geographical_extents_attributes(nco, geometry.box(left, bottom, right, top, crs=crs))\n\n    return crs_var\n\n\ndef write_flag_definition(variable, flags_definition):\n    # write bitflag info\n    # Functions for this are stored in Measurements\n    variable.QA_index = describe_flags_def(flags_def=flags_definition)\n    variable.flag_masks, variable.valid_range, variable.flag_meanings = flag_mask_meanings(flags_def=flags_definition)\n\n\ndef netcdfy_coord(data):\n    return netcdfy_data(data)\n\n\ndef netcdfy_data(data):\n    # NetCDF/CF Conventions only seem to allow storing ascii, not unicode\n    if data.dtype.kind == \'S\' and data.dtype.itemsize > 1:\n        return data.view(\'S1\').reshape(data.shape + (-1,))\n    if data.dtype.kind == \'M\':\n        return data.astype(\'<M8[s]\').astype(\'double\')\n    else:\n        return data\n\n\ndef flag_mask_meanings(flags_def):\n    # Filter out any multi-bit mask values since we can\'t handle them yet\n    flags_def = {k: v for k, v in flags_def.items() if isinstance(v[\'bits\'], numbers.Integral)}\n    max_bit = max([bit_def[\'bits\'] for bit_def in flags_def.values()])\n\n    if max_bit >= 32:\n        # GDAL upto and including 2.0 can\'t support int64 attributes...\n        raise RuntimeError(\'Bit index too high: %s\' % max_bit)\n\n    valid_range = numpy.array([0, (2 ** max_bit - 1) + 2 ** max_bit], dtype=\'int32\')\n\n    masks = []\n    meanings = []\n\n    def by_bits(i):\n        _, v = i\n        return v[\'bits\']\n\n    for name, bitdef in sorted(flags_def.items(), key=by_bits):\n        try:\n            true_value = bitdef[\'values\'][1]\n\n            if true_value is True:\n                meaning = name\n            elif true_value is False:\n                meaning = \'no_\' + name\n            else:\n                meaning = true_value\n\n            masks.append(2 ** bitdef[\'bits\'])\n            meanings.append(str(meaning))\n        except KeyError:\n            continue\n\n    return numpy.array(masks, dtype=\'int32\'), valid_range, \' \'.join(meanings)\n'"
datacube/drivers/postgres/__init__.py,0,"b'# coding=utf-8\n""""""\nLower-level database access.\n\nThis package tries to contain any SQLAlchemy and database-specific code.\n""""""\n\nfrom ._connections import PostgresDb\n\n__all__ = [\'PostgresDb\']\n'"
datacube/drivers/postgres/_api.py,0,"b'# coding=utf-8\n\n# We often have one-arg-per column, so these checks aren\'t so useful.\n# pylint: disable=too-many-arguments,too-many-public-methods,too-many-lines\n\n# SQLAlchemy queries require ""column == None"", not ""column is None"" due to operator overloading:\n# pylint: disable=singleton-comparison\n\n""""""\nPersistence API implementation for postgres.\n""""""\n\nimport logging\nimport uuid  # noqa: F401\nfrom sqlalchemy import cast\nfrom sqlalchemy import delete\nfrom sqlalchemy import select, text, bindparam, and_, or_, func, literal, distinct\nfrom sqlalchemy.dialects.postgresql import INTERVAL\nfrom sqlalchemy.dialects.postgresql import JSONB, insert\nfrom sqlalchemy.exc import IntegrityError\nfrom typing import Iterable, Tuple\n\nfrom datacube.index.exceptions import MissingRecordError\nfrom datacube.index.fields import OrExpression\nfrom datacube.model import Range\nfrom . import _core\nfrom . import _dynamic as dynamic\nfrom ._fields import parse_fields, Expression, PgField, PgExpression  # noqa: F401\nfrom ._fields import NativeField, DateDocField, SimpleDocField\nfrom ._schema import DATASET, DATASET_SOURCE, METADATA_TYPE, DATASET_LOCATION, PRODUCT\nfrom .sql import escape_pg_identifier\n\n\ndef _dataset_uri_field(table):\n    return table.c.uri_scheme + \':\' + table.c.uri_body\n\n\n# Fields for selecting dataset with uris\n# Need to alias the table, as queries may join the location table for filtering.\nSELECTED_DATASET_LOCATION = DATASET_LOCATION.alias(\'selected_dataset_location\')\n_DATASET_SELECT_FIELDS = (\n    DATASET,\n    # All active URIs, from newest to oldest\n    func.array(\n        select([\n            _dataset_uri_field(SELECTED_DATASET_LOCATION)\n        ]).where(\n            and_(\n                SELECTED_DATASET_LOCATION.c.dataset_ref == DATASET.c.id,\n                SELECTED_DATASET_LOCATION.c.archived == None\n            )\n        ).order_by(\n            SELECTED_DATASET_LOCATION.c.added.desc(),\n            SELECTED_DATASET_LOCATION.c.id.desc()\n        ).label(\'uris\')\n    ).label(\'uris\')\n)\n\nPGCODE_UNIQUE_CONSTRAINT = \'23505\'\nPGCODE_FOREIGN_KEY_VIOLATION = \'23503\'\n\n_LOG = logging.getLogger(__name__)\n\n\ndef _split_uri(uri):\n    """"""\n    Split the scheme and the remainder of the URI.\n\n    """"""\n    idx = uri.find(\':\')\n    if idx < 0:\n        raise ValueError(""Not a URI"")\n\n    return uri[:idx], uri[idx+1:]\n\n\ndef get_native_fields():\n    # Native fields (hard-coded into the schema)\n    fields = {\n        \'id\': NativeField(\n            \'id\',\n            \'Dataset UUID\',\n            DATASET.c.id\n        ),\n        \'indexed_time\': NativeField(\n            \'indexed_time\',\n            \'When dataset was indexed\',\n            DATASET.c.added\n        ),\n        \'indexed_by\': NativeField(\n            \'indexed_by\',\n            \'User who indexed the dataset\',\n            DATASET.c.added_by\n        ),\n        \'product\': NativeField(\n            \'product\',\n            \'Product name\',\n            PRODUCT.c.name\n        ),\n        \'dataset_type_id\': NativeField(\n            \'dataset_type_id\',\n            \'ID of a dataset type\',\n            DATASET.c.dataset_type_ref\n        ),\n        \'metadata_type\': NativeField(\n            \'metadata_type\',\n            \'Metadata type name of dataset\',\n            METADATA_TYPE.c.name\n        ),\n        \'metadata_type_id\': NativeField(\n            \'metadata_type_id\',\n            \'ID of a metadata type\',\n            DATASET.c.metadata_type_ref\n        ),\n        \'metadata_doc\': NativeField(\n            \'metadata_doc\',\n            \'Full metadata document\',\n            DATASET.c.metadata\n        ),\n        # Fields that can affect row selection\n\n        # Note that this field is a single uri: selecting it will result in one-result per uri.\n        # (ie. duplicate datasets if multiple uris, no dataset if no uris)\n        \'uri\': NativeField(\n            \'uri\',\n            ""Dataset URI"",\n            DATASET_LOCATION.c.uri_body,\n            alchemy_expression=_dataset_uri_field(DATASET_LOCATION),\n            affects_row_selection=True\n        ),\n    }\n    return fields\n\n\ndef get_dataset_fields(metadata_type_definition):\n    dataset_section = metadata_type_definition[\'dataset\']\n\n    fields = get_native_fields()\n    # ""Fixed fields"" (not dynamic: defined in metadata type schema)\n    fields.update(dict(\n        creation_time=DateDocField(\n            \'creation_time\',\n            \'Time when dataset was created (processed)\',\n            DATASET.c.metadata,\n            False,\n            offset=dataset_section.get(\'creation_dt\') or [\'creation_dt\']\n        ),\n        format=SimpleDocField(\n            \'format\',\n            \'File format (GeoTiff, NetCDF)\',\n            DATASET.c.metadata,\n            False,\n            offset=dataset_section.get(\'format\') or [\'format\', \'name\']\n        ),\n        label=SimpleDocField(\n            \'label\',\n            \'Label\',\n            DATASET.c.metadata,\n            False,\n            offset=dataset_section.get(\'label\') or [\'label\']\n        ),\n    ))\n\n    # noinspection PyTypeChecker\n    fields.update(\n        parse_fields(\n            dataset_section[\'search_fields\'],\n            DATASET.c.metadata\n        )\n    )\n    return fields\n\n\nclass PostgresDbAPI(object):\n    def __init__(self, connection):\n        self._connection = connection\n\n    @property\n    def in_transaction(self):\n        return self._connection.in_transaction()\n\n    def rollback(self):\n        self._connection.execute(text(\'ROLLBACK\'))\n\n    def execute(self, command):\n        return self._connection.execute(command)\n\n    def insert_dataset(self, metadata_doc, dataset_id, product_id):\n        """"""\n        Insert dataset if not already indexed.\n        :type metadata_doc: dict\n        :type dataset_id: str or uuid.UUID\n        :type product_id: int\n        :return: whether it was inserted\n        :rtype: bool\n        """"""\n        dataset_type_ref = bindparam(\'dataset_type_ref\')\n        ret = self._connection.execute(\n            insert(DATASET).from_select(\n                [\'id\', \'dataset_type_ref\', \'metadata_type_ref\', \'metadata\'],\n                select([\n                    bindparam(\'id\'), dataset_type_ref,\n                    select([\n                        PRODUCT.c.metadata_type_ref\n                    ]).where(\n                        PRODUCT.c.id == dataset_type_ref\n                    ).label(\'metadata_type_ref\'),\n                    bindparam(\'metadata\', type_=JSONB)\n                ])\n            ).on_conflict_do_nothing(\n                index_elements=[\'id\']\n            ),\n            id=dataset_id,\n            dataset_type_ref=product_id,\n            metadata=metadata_doc\n        )\n        return ret.rowcount > 0\n\n    def update_dataset(self, metadata_doc, dataset_id, product_id):\n        """"""\n        Update dataset\n        :type metadata_doc: dict\n        :type dataset_id: str or uuid.UUID\n        :type product_id: int\n        """"""\n        res = self._connection.execute(\n            DATASET.update().returning(DATASET.c.id).where(\n                and_(\n                    DATASET.c.id == dataset_id,\n                    DATASET.c.dataset_type_ref == product_id\n                )\n            ).values(\n                metadata=metadata_doc\n            )\n        )\n        return res.rowcount > 0\n\n    def insert_dataset_location(self, dataset_id, uri):\n        """"""\n        Add a location to a dataset if it is not already recorded.\n\n        Returns True if success, False if this location already existed\n\n        :type dataset_id: str or uuid.UUID\n        :type uri: str\n        :rtype bool:\n        """"""\n\n        scheme, body = _split_uri(uri)\n\n        r = self._connection.execute(\n            insert(DATASET_LOCATION).on_conflict_do_nothing(\n                index_elements=[\'uri_scheme\', \'uri_body\', \'dataset_ref\']\n            ),\n            dataset_ref=dataset_id,\n            uri_scheme=scheme,\n            uri_body=body,\n        )\n\n        return r.rowcount > 0\n\n    def contains_dataset(self, dataset_id):\n        return bool(\n            self._connection.execute(\n                select(\n                    [DATASET.c.id]\n                ).where(\n                    DATASET.c.id == dataset_id\n                )\n            ).fetchone()\n        )\n\n    def datasets_intersection(self, dataset_ids):\n        """""" Compute set intersection: db_dataset_ids & dataset_ids\n        """"""\n        return [r[0]\n                for r in self._connection.execute(select(\n                    [DATASET.c.id]\n                ).where(\n                    DATASET.c.id.in_(dataset_ids)\n                )).fetchall()]\n\n    def get_datasets_for_location(self, uri, mode=None):\n        scheme, body = _split_uri(uri)\n\n        if mode is None:\n            mode = \'exact\' if body.count(\'#\') > 0 else \'prefix\'\n\n        if mode == \'exact\':\n            body_query = DATASET_LOCATION.c.uri_body == body\n        elif mode == \'prefix\':\n            body_query = DATASET_LOCATION.c.uri_body.startswith(body)\n        else:\n            raise ValueError(\'Unsupported query mode {}\'.format(mode))\n\n        return self._connection.execute(\n            select(\n                _DATASET_SELECT_FIELDS\n            ).select_from(\n                DATASET_LOCATION.join(DATASET)\n            ).where(\n                and_(DATASET_LOCATION.c.uri_scheme == scheme, body_query)\n            )\n        ).fetchall()\n\n    def insert_dataset_source(self, classifier, dataset_id, source_dataset_id):\n        try:\n            r = self._connection.execute(\n                insert(DATASET_SOURCE).on_conflict_do_nothing(\n                    index_elements=[\'classifier\', \'dataset_ref\']\n                ),\n                classifier=classifier,\n                dataset_ref=dataset_id,\n                source_dataset_ref=source_dataset_id\n            )\n            return r.rowcount > 0\n        except IntegrityError as e:\n            if e.orig.pgcode == PGCODE_FOREIGN_KEY_VIOLATION:\n                raise MissingRecordError(""Referenced source dataset doesn\'t exist"")\n            raise\n\n    def archive_dataset(self, dataset_id):\n        self._connection.execute(\n            DATASET.update().where(\n                DATASET.c.id == dataset_id\n            ).where(\n                DATASET.c.archived == None\n            ).values(\n                archived=func.now()\n            )\n        )\n\n    def restore_dataset(self, dataset_id):\n        self._connection.execute(\n            DATASET.update().where(\n                DATASET.c.id == dataset_id\n            ).values(\n                archived=None\n            )\n        )\n\n    def delete_dataset(self, dataset_id):\n        self._connection.execute(\n            DATASET.delete().where(\n                DATASET.c.id == dataset_id\n            )\n        )\n\n    def get_dataset(self, dataset_id):\n        return self._connection.execute(\n            select(_DATASET_SELECT_FIELDS).where(DATASET.c.id == dataset_id)\n        ).first()\n\n    def get_datasets(self, dataset_ids):\n        return self._connection.execute(\n            select(_DATASET_SELECT_FIELDS).where(DATASET.c.id.in_(dataset_ids))\n        ).fetchall()\n\n    def get_derived_datasets(self, dataset_id):\n        return self._connection.execute(\n            select(\n                _DATASET_SELECT_FIELDS\n            ).select_from(\n                DATASET.join(DATASET_SOURCE, DATASET.c.id == DATASET_SOURCE.c.dataset_ref)\n            ).where(\n                DATASET_SOURCE.c.source_dataset_ref == dataset_id\n            )\n        ).fetchall()\n\n    def get_dataset_sources(self, dataset_id):\n        # recursively build the list of (dataset_ref, source_dataset_ref) pairs starting from dataset_id\n        # include (dataset_ref, NULL) [hence the left join]\n        sources = select(\n            [DATASET.c.id.label(\'dataset_ref\'),\n             DATASET_SOURCE.c.source_dataset_ref,\n             DATASET_SOURCE.c.classifier]\n        ).select_from(\n            DATASET.join(DATASET_SOURCE,\n                         DATASET.c.id == DATASET_SOURCE.c.dataset_ref,\n                         isouter=True)\n        ).where(\n            DATASET.c.id == dataset_id\n        ).cte(name=""sources"", recursive=True)\n\n        sources = sources.union_all(\n            select(\n                [sources.c.source_dataset_ref.label(\'dataset_ref\'),\n                 DATASET_SOURCE.c.source_dataset_ref,\n                 DATASET_SOURCE.c.classifier]\n            ).select_from(\n                sources.join(DATASET_SOURCE,\n                             sources.c.source_dataset_ref == DATASET_SOURCE.c.dataset_ref,\n                             isouter=True)\n            ).where(sources.c.source_dataset_ref != None))\n\n        # turn the list of pairs into adjacency list (dataset_ref, [source_dataset_ref, ...])\n        # some source_dataset_ref\'s will be NULL\n        aggd = select(\n            [sources.c.dataset_ref,\n             func.array_agg(sources.c.source_dataset_ref).label(\'sources\'),\n             func.array_agg(sources.c.classifier).label(\'classes\')]\n        ).group_by(sources.c.dataset_ref).alias(\'aggd\')\n\n        # join the adjacency list with datasets table\n        query = select(\n            _DATASET_SELECT_FIELDS + (aggd.c.sources, aggd.c.classes)\n        ).select_from(aggd.join(DATASET, DATASET.c.id == aggd.c.dataset_ref))\n\n        return self._connection.execute(query).fetchall()\n\n    def search_datasets_by_metadata(self, metadata):\n        """"""\n        Find any datasets that have the given metadata.\n\n        :type metadata: dict\n        :rtype: dict\n        """"""\n        # Find any storage types whose \'dataset_metadata\' document is a subset of the metadata.\n        return self._connection.execute(\n            select(_DATASET_SELECT_FIELDS).where(DATASET.c.metadata.contains(metadata))\n        ).fetchall()\n\n    @staticmethod\n    def _alchemify_expressions(expressions):\n        def raw_expr(expression):\n            if isinstance(expression, OrExpression):\n                return or_(raw_expr(expr) for expr in expression.exprs)\n            return expression.alchemy_expression\n\n        return [raw_expr(expression) for expression in expressions]\n\n    @staticmethod\n    def search_datasets_query(expressions, source_exprs=None,\n                              select_fields=None, with_source_ids=False, limit=None):\n        """"""\n        :type expressions: Tuple[Expression]\n        :type source_exprs: Tuple[Expression]\n        :type select_fields: Iterable[PgField]\n        :type with_source_ids: bool\n        :type limit: int\n        :rtype: sqlalchemy.Expression\n        """"""\n\n        if select_fields:\n            select_columns = tuple(\n                f.alchemy_expression.label(f.name)\n                for f in select_fields\n            )\n        else:\n            select_columns = _DATASET_SELECT_FIELDS\n\n        if with_source_ids:\n            # Include the IDs of source datasets\n            select_columns += (\n                select(\n                    (func.array_agg(DATASET_SOURCE.c.source_dataset_ref),)\n                ).select_from(\n                    DATASET_SOURCE\n                ).where(\n                    DATASET_SOURCE.c.dataset_ref == DATASET.c.id\n                ).group_by(\n                    DATASET_SOURCE.c.dataset_ref\n                ).label(\'dataset_refs\'),\n            )\n\n        raw_expressions = PostgresDbAPI._alchemify_expressions(expressions)\n        from_expression = PostgresDbAPI._from_expression(DATASET, expressions, select_fields)\n        where_expr = and_(DATASET.c.archived == None, *raw_expressions)\n\n        if not source_exprs:\n            return (\n                select(\n                    select_columns\n                ).select_from(\n                    from_expression\n                ).where(\n                    where_expr\n                ).limit(\n                    limit\n                )\n            )\n        base_query = (\n            select(\n                select_columns + (DATASET_SOURCE.c.source_dataset_ref,\n                                  literal(1).label(\'distance\'),\n                                  DATASET_SOURCE.c.classifier.label(\'path\'))\n            ).select_from(\n                from_expression.join(DATASET_SOURCE, DATASET.c.id == DATASET_SOURCE.c.dataset_ref)\n            ).where(\n                where_expr\n            )\n        ).cte(name=""base_query"", recursive=True)\n\n        recursive_query = base_query.union_all(\n            select(\n                [col for col in base_query.columns\n                 if col.name not in [\'source_dataset_ref\', \'distance\', \'path\']\n                 ] + [\n                    DATASET_SOURCE.c.source_dataset_ref,\n                    (base_query.c.distance + 1).label(\'distance\'),\n                    (base_query.c.path + \'.\' + DATASET_SOURCE.c.classifier).label(\'path\')\n                ]\n            ).select_from(\n                base_query.join(\n                    DATASET_SOURCE, base_query.c.source_dataset_ref == DATASET_SOURCE.c.dataset_ref\n                )\n            )\n        )\n\n        return (\n            select(\n                [distinct(recursive_query.c.id)\n                 ] + [\n                    col for col in recursive_query.columns\n                    if col.name not in [\'id\', \'source_dataset_ref\', \'distance\', \'path\']]\n            ).select_from(\n                recursive_query.join(DATASET, DATASET.c.id == recursive_query.c.source_dataset_ref)\n            ).where(\n                and_(DATASET.c.archived == None, *PostgresDbAPI._alchemify_expressions(source_exprs))\n            ).limit(\n                limit\n            )\n        )\n\n    def search_datasets(self, expressions,\n                        source_exprs=None, select_fields=None,\n                        with_source_ids=False, limit=None):\n        """"""\n        :type with_source_ids: bool\n        :type select_fields: tuple[datacube.drivers.postgres._fields.PgField]\n        :type expressions: tuple[datacube.drivers.postgres._fields.PgExpression]\n        """"""\n        select_query = self.search_datasets_query(expressions, source_exprs,\n                                                  select_fields, with_source_ids, limit)\n        return self._connection.execute(select_query)\n\n    @staticmethod\n    def search_unique_datasets_query(expressions, select_fields, limit):\n        """"""\n        \'unique\' here refer to that the query results do not contain datasets\n        having the same \'id\' more than once.\n\n        We are not dealing with dataset_source table here and we are not joining\n        dataset table with dataset_location table. We are aggregating stuff\n        in dataset_location per dataset basis if required. It returns the construted\n        query.\n        """"""\n\n        # expressions involving DATASET_SOURCE cannot not done for now\n        for expression in expressions:\n            assert expression.field.required_alchemy_table != DATASET_SOURCE, \\\n                \'Joins with dataset_source cannot be done for this query\'\n\n        # expressions involving \'uri\' and \'uris\' will be handled different\n        expressions = [expression for expression in expressions\n                       if expression.field.required_alchemy_table != DATASET_LOCATION]\n\n        if select_fields:\n            select_columns = []\n            for field in select_fields:\n                if field.name in {\'uri\', \'uris\'}:\n                    # All active URIs, from newest to oldest\n                    uris_field = func.array(\n                        select([\n                            _dataset_uri_field(SELECTED_DATASET_LOCATION)\n                        ]).where(\n                            and_(\n                                SELECTED_DATASET_LOCATION.c.dataset_ref == DATASET.c.id,\n                                SELECTED_DATASET_LOCATION.c.archived == None\n                            )\n                        ).order_by(\n                            SELECTED_DATASET_LOCATION.c.added.desc(),\n                            SELECTED_DATASET_LOCATION.c.id.desc()\n                        ).label(\'uris\')\n                    ).label(\'uris\')\n                    select_columns.append(uris_field)\n                else:\n                    select_columns.append(field.alchemy_expression.label(field.name))\n        else:\n            select_columns = _DATASET_SELECT_FIELDS\n\n        raw_expressions = PostgresDbAPI._alchemify_expressions(expressions)\n\n        # We don\'t need \'DATASET_LOCATION table in the from expression\n        select_fields_ = [field for field in select_fields if field.name not in {\'uri\', \'uris\'}]\n\n        from_expression = PostgresDbAPI._from_expression(DATASET, expressions, select_fields_)\n        where_expr = and_(DATASET.c.archived == None, *raw_expressions)\n\n        return (\n            select(\n                select_columns\n            ).select_from(\n                from_expression\n            ).where(\n                where_expr\n            ).limit(\n                limit\n            )\n        )\n\n    def search_unique_datasets(self, expressions, select_fields=None, limit=None):\n        """"""\n        Processes a search query without duplicating datasets.\n\n        \'unique\' here refer to that the results do not contain datasets having the same \'id\'\n        more than once. we achieve this by not allowing dataset table to join with\n        dataset_location or dataset_source tables. Joining with other tables would not\n        result in multiple records per dataset due to the direction of cardinality.\n        """"""\n\n        select_query = self.search_unique_datasets_query(expressions, select_fields, limit)\n\n        return self._connection.execute(select_query)\n\n    def get_duplicates(self, match_fields, expressions):\n        # type: (Tuple[PgField], Tuple[PgExpression]) -> Iterable[tuple]\n        group_expressions = tuple(f.alchemy_expression for f in match_fields)\n\n        select_query = select(\n            (func.array_agg(DATASET.c.id),) + group_expressions\n        ).select_from(\n            PostgresDbAPI._from_expression(DATASET, expressions, match_fields)\n        ).where(\n            and_(DATASET.c.archived == None, *(PostgresDbAPI._alchemify_expressions(expressions)))\n        ).group_by(\n            *group_expressions\n        ).having(\n            func.count(DATASET.c.id) > 1\n        )\n        return self._connection.execute(select_query)\n\n    def count_datasets(self, expressions):\n        """"""\n        :type expressions: tuple[datacube.drivers.postgres._fields.PgExpression]\n        :rtype: int\n        """"""\n\n        raw_expressions = self._alchemify_expressions(expressions)\n\n        select_query = (\n            select(\n                [func.count(\'*\')]\n            ).select_from(\n                self._from_expression(DATASET, expressions)\n            ).where(\n                and_(DATASET.c.archived == None, *raw_expressions)\n            )\n        )\n\n        return self._connection.scalar(select_query)\n\n    def count_datasets_through_time(self, start, end, period, time_field, expressions):\n        """"""\n        :type period: str\n        :type start: datetime.datetime\n        :type end: datetime.datetime\n        :type expressions: tuple[datacube.drivers.postgres._fields.PgExpression]\n        :rtype: list[((datetime.datetime, datetime.datetime), int)]\n        """"""\n\n        results = self._connection.execute(\n            self.count_datasets_through_time_query(start, end, period, time_field, expressions)\n        )\n\n        for time_period, dataset_count in results:\n            # if not time_period.upper_inf:\n            yield Range(time_period.lower, time_period.upper), dataset_count\n\n    def count_datasets_through_time_query(self, start, end, period, time_field, expressions):\n        raw_expressions = self._alchemify_expressions(expressions)\n\n        start_times = select((\n            func.generate_series(start, end, cast(period, INTERVAL)).label(\'start_time\'),\n        )).alias(\'start_times\')\n\n        time_range_select = (\n            select((\n                func.tstzrange(\n                    start_times.c.start_time,\n                    func.lead(start_times.c.start_time).over()\n                ).label(\'time_period\'),\n            ))\n        ).alias(\'all_time_ranges\')\n\n        # Exclude the trailing (end time to infinite) row. Is there a simpler way?\n        time_ranges = (\n            select((\n                time_range_select,\n            )).where(\n                ~func.upper_inf(time_range_select.c.time_period)\n            )\n        ).alias(\'time_ranges\')\n\n        count_query = (\n            select(\n                (func.count(\'*\'),)\n            ).select_from(\n                self._from_expression(DATASET, expressions)\n            ).where(\n                and_(\n                    time_field.alchemy_expression.overlaps(time_ranges.c.time_period),\n                    DATASET.c.archived == None,\n                    *raw_expressions\n                )\n            )\n        )\n\n        return select((time_ranges.c.time_period, count_query.label(\'dataset_count\')))\n\n    @staticmethod\n    def _from_expression(source_table, expressions=None, fields=None):\n        join_tables = set()\n        if expressions:\n            join_tables.update(expression.field.required_alchemy_table for expression in expressions)\n        if fields:\n            join_tables.update(field.required_alchemy_table for field in fields)\n        join_tables.discard(source_table)\n\n        table_order_hack = [DATASET_SOURCE, DATASET_LOCATION, DATASET, PRODUCT, METADATA_TYPE]\n\n        from_expression = source_table\n        for table in table_order_hack:\n            if table in join_tables:\n                from_expression = from_expression.join(table)\n        return from_expression\n\n    def get_product(self, id_):\n        return self._connection.execute(\n            PRODUCT.select().where(PRODUCT.c.id == id_)\n        ).first()\n\n    def get_metadata_type(self, id_):\n        return self._connection.execute(\n            METADATA_TYPE.select().where(METADATA_TYPE.c.id == id_)\n        ).first()\n\n    def get_product_by_name(self, name):\n        return self._connection.execute(\n            PRODUCT.select().where(PRODUCT.c.name == name)\n        ).first()\n\n    def get_metadata_type_by_name(self, name):\n        return self._connection.execute(\n            METADATA_TYPE.select().where(METADATA_TYPE.c.name == name)\n        ).first()\n\n    def insert_product(self,\n                       name,\n                       metadata,\n                       metadata_type_id,\n                       search_fields,\n                       definition,\n                       concurrently=True):\n\n        res = self._connection.execute(\n            PRODUCT.insert().values(\n                name=name,\n                metadata=metadata,\n                metadata_type_ref=metadata_type_id,\n                definition=definition\n            )\n        )\n\n        type_id = res.inserted_primary_key[0]\n\n        # Initialise search fields.\n        self._setup_product_fields(type_id, name, search_fields, definition[\'metadata\'],\n                                   concurrently=concurrently)\n        return type_id\n\n    def update_product(self,\n                       name,\n                       metadata,\n                       metadata_type_id,\n                       search_fields,\n                       definition, update_metadata_type=False, concurrently=False):\n        res = self._connection.execute(\n            PRODUCT.update().returning(PRODUCT.c.id).where(\n                PRODUCT.c.name == name\n            ).values(\n                metadata=metadata,\n                metadata_type_ref=metadata_type_id,\n                definition=definition\n            )\n        )\n        type_id = res.first()[0]\n\n        if update_metadata_type:\n            if not self._connection.in_transaction():\n                raise RuntimeError(\'Must update metadata types in transaction\')\n\n            self._connection.execute(\n                DATASET.update().where(\n                    DATASET.c.dataset_type_ref == type_id\n                ).values(\n                    metadata_type_ref=metadata_type_id,\n                )\n            )\n\n        # Initialise search fields.\n        self._setup_product_fields(type_id, name, search_fields, definition[\'metadata\'],\n                                   concurrently=concurrently,\n                                   rebuild_view=True)\n        return type_id\n\n    def insert_metadata_type(self, name, definition, concurrently=False):\n        res = self._connection.execute(\n            METADATA_TYPE.insert().values(\n                name=name,\n                definition=definition\n            )\n        )\n        type_id = res.inserted_primary_key[0]\n\n        search_fields = get_dataset_fields(definition)\n        self._setup_metadata_type_fields(\n            type_id, name, search_fields, concurrently=concurrently\n        )\n\n    def update_metadata_type(self, name, definition, concurrently=False):\n        res = self._connection.execute(\n            METADATA_TYPE.update().returning(METADATA_TYPE.c.id).where(\n                METADATA_TYPE.c.name == name\n            ).values(\n                name=name,\n                definition=definition\n            )\n        )\n        type_id = res.first()[0]\n\n        search_fields = get_dataset_fields(definition)\n        self._setup_metadata_type_fields(\n            type_id, name, search_fields,\n            concurrently=concurrently,\n            rebuild_views=True,\n        )\n\n        return type_id\n\n    def check_dynamic_fields(self, concurrently=False, rebuild_views=False, rebuild_indexes=False):\n        _LOG.info(\'Checking dynamic views/indexes. (rebuild views=%s, indexes=%s)\', rebuild_views, rebuild_indexes)\n\n        search_fields = {}\n\n        for metadata_type in self.get_all_metadata_types():\n            fields = get_dataset_fields(metadata_type[\'definition\'])\n            search_fields[metadata_type[\'id\']] = fields\n            self._setup_metadata_type_fields(\n                metadata_type[\'id\'],\n                metadata_type[\'name\'],\n                fields,\n                rebuild_indexes=rebuild_indexes,\n                rebuild_views=rebuild_views,\n                concurrently=concurrently,\n            )\n\n    def _setup_metadata_type_fields(self, id_, name, fields,\n                                    rebuild_indexes=False, rebuild_views=False, concurrently=True):\n        # Metadata fields are no longer used (all queries are per-dataset-type): exclude all.\n        # This will have the effect of removing any old indexes that still exist.\n        exclude_fields = tuple(fields)\n\n        dataset_filter = and_(DATASET.c.archived == None, DATASET.c.metadata_type_ref == id_)\n        dynamic.check_dynamic_fields(self._connection, concurrently, dataset_filter,\n                                     exclude_fields, fields, name,\n                                     rebuild_indexes=rebuild_indexes, rebuild_view=rebuild_views)\n\n        for product in self._get_products_for_metadata_type(id_):\n            self._setup_product_fields(\n                product[\'id\'],\n                product[\'name\'],\n                fields,\n                product[\'definition\'][\'metadata\'],\n                rebuild_view=rebuild_views,\n                rebuild_indexes=rebuild_indexes,\n                concurrently=concurrently\n            )\n\n    def _setup_product_fields(self, id_, name, fields, metadata_doc,\n                              rebuild_indexes=False, rebuild_view=False, concurrently=True):\n        dataset_filter = and_(DATASET.c.archived == None, DATASET.c.dataset_type_ref == id_)\n        excluded_field_names = tuple(self._get_active_field_names(fields, metadata_doc))\n\n        dynamic.check_dynamic_fields(self._connection, concurrently, dataset_filter,\n                                     excluded_field_names, fields, name,\n                                     rebuild_indexes=rebuild_indexes, rebuild_view=rebuild_view)\n\n    @staticmethod\n    def _get_active_field_names(fields, metadata_doc):\n        for field in fields.values():\n            if hasattr(field, \'extract\'):\n                try:\n                    value = field.extract(metadata_doc)\n                    if value is not None:\n                        yield field.name\n                except (AttributeError, KeyError, ValueError):\n                    continue\n\n    def get_all_products(self):\n        return self._connection.execute(\n            PRODUCT.select().order_by(PRODUCT.c.name.asc())\n        ).fetchall()\n\n    def _get_products_for_metadata_type(self, id_):\n        return self._connection.execute(\n            PRODUCT.select(\n            ).where(\n                PRODUCT.c.metadata_type_ref == id_\n            ).order_by(\n                PRODUCT.c.name.asc()\n            )).fetchall()\n\n    def get_all_metadata_types(self):\n        return self._connection.execute(METADATA_TYPE.select().order_by(METADATA_TYPE.c.name.asc())).fetchall()\n\n    def get_locations(self, dataset_id):\n        return [\n            record[0]\n            for record in self._connection.execute(\n                select([\n                    _dataset_uri_field(DATASET_LOCATION)\n                ]).where(\n                    and_(DATASET_LOCATION.c.dataset_ref == dataset_id, DATASET_LOCATION.c.archived == None)\n                ).order_by(\n                    DATASET_LOCATION.c.added.desc(),\n                    DATASET_LOCATION.c.id.desc()\n                )\n            ).fetchall()\n        ]\n\n    def get_archived_locations(self, dataset_id):\n        """"""\n        Return a list of uris and archived_times for a dataset\n        """"""\n        return [\n            (location_uri, archived_time)\n            for location_uri, archived_time in self._connection.execute(\n                select([\n                    _dataset_uri_field(DATASET_LOCATION), DATASET_LOCATION.c.archived\n                ]).where(\n                    and_(DATASET_LOCATION.c.dataset_ref == dataset_id, DATASET_LOCATION.c.archived != None)\n                ).order_by(\n                    DATASET_LOCATION.c.added.desc()\n                )\n            ).fetchall()\n        ]\n\n    def remove_location(self, dataset_id, uri):\n        """"""\n        Remove the given location for a dataset\n\n        :returns bool: Was the location deleted?\n        """"""\n        scheme, body = _split_uri(uri)\n        res = self._connection.execute(\n            delete(DATASET_LOCATION).where(\n                and_(\n                    DATASET_LOCATION.c.dataset_ref == dataset_id,\n                    DATASET_LOCATION.c.uri_scheme == scheme,\n                    DATASET_LOCATION.c.uri_body == body,\n                )\n            )\n        )\n        return res.rowcount > 0\n\n    def archive_location(self, dataset_id, uri):\n        scheme, body = _split_uri(uri)\n        res = self._connection.execute(\n            DATASET_LOCATION.update().where(\n                and_(\n                    DATASET_LOCATION.c.dataset_ref == dataset_id,\n                    DATASET_LOCATION.c.uri_scheme == scheme,\n                    DATASET_LOCATION.c.uri_body == body,\n                    DATASET_LOCATION.c.archived == None,\n                )\n            ).values(\n                archived=func.now()\n            )\n        )\n        return res.rowcount > 0\n\n    def restore_location(self, dataset_id, uri):\n        scheme, body = _split_uri(uri)\n        res = self._connection.execute(\n            DATASET_LOCATION.update().where(\n                and_(\n                    DATASET_LOCATION.c.dataset_ref == dataset_id,\n                    DATASET_LOCATION.c.uri_scheme == scheme,\n                    DATASET_LOCATION.c.uri_body == body,\n                    DATASET_LOCATION.c.archived != None,\n                )\n            ).values(\n                archived=None\n            )\n        )\n        return res.rowcount > 0\n\n    def __repr__(self):\n        return ""PostgresDb<connection={!r}>"".format(self._connection)\n\n    def list_users(self):\n        result = self._connection.execute(""""""\n            select\n                group_role.rolname as role_name,\n                user_role.rolname as user_name,\n                pg_catalog.shobj_description(user_role.oid, \'pg_authid\') as description\n            from pg_roles group_role\n            inner join pg_auth_members am on am.roleid = group_role.oid\n            inner join pg_roles user_role on am.member = user_role.oid\n            where (group_role.rolname like \'agdc_%%\') and not (user_role.rolname like \'agdc_%%\')\n            order by group_role.oid asc, user_role.oid asc;\n        """""")\n        for row in result:\n            yield _core.from_pg_role(row[\'role_name\']), row[\'user_name\'], row[\'description\']\n\n    def create_user(self, username, password, role, description=None):\n        pg_role = _core.to_pg_role(role)\n        username = escape_pg_identifier(self._connection, username)\n        sql = text(\'create user {username} password :password in role {role}\'.format(username=username, role=pg_role))\n        self._connection.execute(sql,\n                                 password=password)\n        if description:\n            sql = text(\'comment on role {username} is :description\'.format(username=username))\n            self._connection.execute(sql,\n                                     description=description)\n\n    def drop_users(self, users):\n        # type: (Iterable[str]) -> None\n        for username in users:\n            sql = text(\'drop role {username}\'.format(username=escape_pg_identifier(self._connection, username)))\n            self._connection.execute(sql)\n\n    def grant_role(self, role, users):\n        # type: (str, Iterable[str]) -> None\n        """"""\n        Grant a role to a user.\n        """"""\n        pg_role = _core.to_pg_role(role)\n\n        for user in users:\n            if not _core.has_role(self._connection, user):\n                raise ValueError(\'Unknown user %r\' % user)\n\n        _core.grant_role(self._connection, pg_role, users)\n'"
datacube/drivers/postgres/_connections.py,0,"b'# coding=utf-8\n\n# We often have one-arg-per column, so these checks aren\'t so useful.\n# pylint: disable=too-many-arguments,too-many-public-methods\n\n# SQLAlchemy queries require ""column == None"", not ""column is None"" due to operator overloading:\n# pylint: disable=singleton-comparison\n\n""""""\nPostgres connection and setup\n""""""\nimport json\nimport logging\nimport os\nimport re\nfrom contextlib import contextmanager\nfrom typing import Optional\n\nfrom sqlalchemy import create_engine, text\nfrom sqlalchemy.engine.url import URL as EngineUrl\n\nimport datacube\nfrom datacube.index.exceptions import IndexSetupError\nfrom datacube.utils import jsonify_document\nfrom . import _api\nfrom . import _core\n\n_LIB_ID = \'agdc-\' + str(datacube.__version__)\n\n_LOG = logging.getLogger(__name__)\n\ntry:\n    import pwd\n\n    DEFAULT_DB_USER = pwd.getpwuid(os.geteuid()).pw_name  # type: Optional[str]\nexcept (ImportError, KeyError):\n    # No default on Windows and some other systems\n    DEFAULT_DB_USER = None\nDEFAULT_DB_PORT = 5432\n\n\nclass PostgresDb(object):\n    """"""\n    A thin database access api.\n\n    It exists so that higher level modules are not tied to SQLAlchemy, connections or specifics of database-access.\n\n    (and can be unit tested without any actual databases)\n\n    Thread safe: the only shared state is the (thread-safe) sqlalchemy connection pool.\n\n    But not multiprocess safe once the first connections are made! A connection must not be shared between multiple\n    processes. You can call close() before forking if you know no other threads currently hold connections,\n    or else use a separate instance of this class in each process.\n    """"""\n\n    def __init__(self, engine):\n        # We don\'t recommend using this constructor directly as it may change.\n        # Use static methods PostgresDb.create() or PostgresDb.from_config()\n        self._engine = engine\n\n    @classmethod\n    def from_config(cls, config, application_name=None, validate_connection=True):\n        app_name = cls._expand_app_name(application_name)\n\n        return PostgresDb.create(\n            config[\'db_hostname\'],\n            config[\'db_database\'],\n            config.get(\'db_username\', DEFAULT_DB_USER),\n            config.get(\'db_password\', None),\n            config.get(\'db_port\', DEFAULT_DB_PORT),\n            application_name=app_name,\n            validate=validate_connection,\n            pool_timeout=int(config.get(\'db_connection_timeout\', 60))\n        )\n\n    @classmethod\n    def create(cls, hostname, database, username=None, password=None, port=None,\n               application_name=None, validate=True, pool_timeout=60):\n        engine = cls._create_engine(\n            EngineUrl(\n                \'postgresql\',\n                host=hostname, database=database, port=port,\n                username=username, password=password,\n            ),\n            application_name=application_name,\n            pool_timeout=pool_timeout)\n        if validate:\n            if not _core.database_exists(engine):\n                raise IndexSetupError(\'\\n\\nNo DB schema exists. Have you run init?\\n\\t{init_command}\'.format(\n                    init_command=\'datacube system init\'\n                ))\n\n            if not _core.schema_is_latest(engine):\n                raise IndexSetupError(\n                    \'\\n\\nDB schema is out of date. \'\n                    \'An administrator must run init:\\n\\t{init_command}\'.format(\n                        init_command=\'datacube -v system init\'\n                    ))\n        return PostgresDb(engine)\n\n    @staticmethod\n    def _create_engine(url, application_name=None, pool_timeout=60):\n        return create_engine(\n            url,\n            echo=False,\n            echo_pool=False,\n\n            # \'AUTOCOMMIT\' here means READ-COMMITTED isolation level with autocommit on.\n            # When a transaction is needed we will do an explicit begin/commit.\n            isolation_level=\'AUTOCOMMIT\',\n\n            json_serializer=_to_json,\n            # If a connection is idle for this many seconds, SQLAlchemy will renew it rather\n            # than assuming it\'s still open. Allows servers to close idle connections without clients\n            # getting errors.\n            pool_recycle=pool_timeout,\n            connect_args={\'application_name\': application_name}\n        )\n\n    @property\n    def url(self) -> str:\n        return self._engine.url\n\n    @staticmethod\n    def get_db_username(config):\n        try:\n            return config[\'db_username\']\n        except KeyError:\n            return DEFAULT_DB_USER\n\n    def close(self):\n        """"""\n        Close any idle connections in the pool.\n\n        This is good practice if you are keeping this object in scope\n        but wont be using it for a while.\n\n        Connections should not be shared between processes, so this should be called\n        before forking if the same instance will be used.\n\n        (connections are normally closed automatically when this object is\n         garbage collected)\n        """"""\n        self._engine.dispose()\n\n    @classmethod\n    def _expand_app_name(cls, application_name):\n        """"""\n        >>> PostgresDb._expand_app_name(None) #doctest: +ELLIPSIS\n        \'agdc-...\'\n        >>> PostgresDb._expand_app_name(\'\') #doctest: +ELLIPSIS\n        \'agdc-...\'\n        >>> PostgresDb._expand_app_name(\'cli\') #doctest: +ELLIPSIS\n        \'cli agdc-...\'\n        >>> PostgresDb._expand_app_name(\'a b.c/d\')\n        \'a-b-c-d agdc-...\'\n        >>> PostgresDb._expand_app_name(5)\n        Traceback (most recent call last):\n        ...\n        TypeError: Application name must be a string\n        """"""\n        full_name = _LIB_ID\n        if application_name:\n            if not isinstance(application_name, str):\n                raise TypeError(\'Application name must be a string\')\n\n            full_name = re.sub(\'[^0-9a-zA-Z]+\', \'-\', application_name) + \' \' + full_name\n\n        if len(full_name) > 64:\n            _LOG.warning(\'Application name is too long: Truncating to %s chars\', (64 - len(_LIB_ID) - 1))\n        return full_name[-64:]\n\n    def init(self, with_permissions=True):\n        """"""\n        Init a new database (if not already set up).\n\n        :return: If it was newly created.\n        """"""\n        is_new = _core.ensure_db(self._engine, with_permissions=with_permissions)\n        if not is_new:\n            _core.update_schema(self._engine)\n\n        return is_new\n\n    @contextmanager\n    def connect(self):\n        """"""\n        Borrow a connection from the pool.\n\n        The name connect() is misleading: it will not create a new connection if one is already available in the pool.\n\n        Callers should minimise the amount of time they hold onto their connections. If they\'re doing anything between\n        calls to the DB (such as opening files, or waiting on user input), it\'s better to return the connection\n        to the pool beforehand.\n\n        The connection can raise errors if not following this advice (""server closed the connection unexpectedly""),\n        as some servers will aggressively close idle connections (eg. DEA\'s NCI servers). It also prevents the\n        connection from being reused while borrowed.\n        """"""\n        with self._engine.connect() as connection:\n            yield _api.PostgresDbAPI(connection)\n            connection.close()\n\n    @contextmanager\n    def begin(self):\n        """"""\n        Start a transaction.\n\n        Returns an instance that will maintain a single connection in a transaction.\n\n        Call commit() or rollback() to complete the transaction or use a context manager:\n\n            with db.begin() as trans:\n                trans.insert_dataset(...)\n\n        (Don\'t share an instance between threads)\n\n        :rtype: PostgresDBAPI\n        """"""\n        with self._engine.connect() as connection:\n            connection.execute(text(\'BEGIN\'))\n            try:\n                yield _api.PostgresDbAPI(connection)\n                connection.execute(text(\'COMMIT\'))\n            except Exception:  # pylint: disable=broad-except\n                connection.execute(text(\'ROLLBACK\'))\n                raise\n            finally:\n                connection.close()\n\n    def give_me_a_connection(self):\n        return self._engine.connect()\n\n    @classmethod\n    def get_dataset_fields(cls, metadata_type_definition):\n        return _api.get_dataset_fields(metadata_type_definition)\n\n    def __repr__(self):\n        return ""PostgresDb<engine={!r}>"".format(self._engine)\n\n\ndef _to_json(o):\n    # Postgres <=9.5 doesn\'t support NaN and Infinity\n    fixedup = jsonify_document(o)\n    return json.dumps(fixedup, default=_json_fallback)\n\n\ndef _json_fallback(obj):\n    """"""Fallback json serialiser.""""""\n    raise TypeError(""Type not serializable: {}"".format(type(obj)))\n'"
datacube/drivers/postgres/_core.py,0,"b'# coding=utf-8\n""""""\nCore SQL schema settings.\n""""""\n\nimport logging\n\nfrom sqlalchemy import MetaData\nfrom sqlalchemy.engine import Engine\nfrom sqlalchemy.schema import CreateSchema\n\nfrom datacube.drivers.postgres.sql import TYPES_INIT_SQL, pg_exists, pg_column_exists, escape_pg_identifier\n\nUSER_ROLES = (\'agdc_user\', \'agdc_ingest\', \'agdc_manage\', \'agdc_admin\')\n\nSQL_NAMING_CONVENTIONS = {\n    ""ix"": \'ix_%(column_0_label)s\',\n    ""uq"": ""uq_%(table_name)s_%(column_0_name)s"",\n    ""ck"": ""ck_%(table_name)s_%(constraint_name)s"",\n    ""fk"": ""fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s"",\n    ""pk"": ""pk_%(table_name)s"",\n    # Other prefixes handled outside of sqlalchemy:\n    # dix: dynamic-index, those indexes created automatically based on search field configuration.\n    # tix: test-index, created by hand for testing, particularly in dev.\n}\nSCHEMA_NAME = \'agdc\'\n\nMETADATA = MetaData(naming_convention=SQL_NAMING_CONVENTIONS, schema=SCHEMA_NAME)\n\n_LOG = logging.getLogger(__name__)\n\n\ndef schema_qualified(name):\n    """"""\n    >>> schema_qualified(\'dataset\')\n    \'agdc.dataset\'\n    """"""\n    return \'{}.{}\'.format(SCHEMA_NAME, name)\n\n\ndef _get_quoted_connection_info(connection):\n    db, user = connection.execute(""select quote_ident(current_database()), quote_ident(current_user)"").fetchone()\n    return db, user\n\n\ndef ensure_db(engine, with_permissions=True):\n    """"""\n    Initialise the db if needed.\n\n    Ensures standard users exist.\n\n    Create the schema if it doesn\'t exist.\n    """"""\n    is_new = False\n    c = engine.connect()\n\n    quoted_db_name, quoted_user = _get_quoted_connection_info(c)\n\n    if with_permissions:\n        _LOG.info(\'Ensuring user roles.\')\n        _ensure_role(c, \'agdc_user\')\n        _ensure_role(c, \'agdc_ingest\', inherits_from=\'agdc_user\')\n        _ensure_role(c, \'agdc_manage\', inherits_from=\'agdc_ingest\')\n        _ensure_role(c, \'agdc_admin\', inherits_from=\'agdc_manage\', add_user=True)\n\n        c.execute(""""""\n        grant all on database {db} to agdc_admin;\n        """""".format(db=quoted_db_name))\n\n    if not has_schema(engine, c):\n        is_new = True\n        try:\n            c.execute(\'begin\')\n            if with_permissions:\n                # Switch to \'agdc_admin\', so that all items are owned by them.\n                c.execute(\'set role agdc_admin\')\n            _LOG.info(\'Creating schema.\')\n            c.execute(CreateSchema(SCHEMA_NAME))\n            _LOG.info(\'Creating tables.\')\n            c.execute(TYPES_INIT_SQL)\n            METADATA.create_all(c)\n            c.execute(\'commit\')\n        except:\n            c.execute(\'rollback\')\n            raise\n        finally:\n            if with_permissions:\n                c.execute(\'set role ""{}""\'.format(quoted_user))\n\n    if with_permissions:\n        _LOG.info(\'Adding role grants.\')\n        c.execute(""""""\n        grant usage on schema {schema} to agdc_user;\n        grant select on all tables in schema {schema} to agdc_user;\n        grant execute on function {schema}.common_timestamp(text) to agdc_user;\n\n        grant insert on {schema}.dataset,\n                        {schema}.dataset_location,\n                        {schema}.dataset_source to agdc_ingest;\n        grant usage, select on all sequences in schema {schema} to agdc_ingest;\n\n        -- (We\'re only granting deletion of types that have nothing written yet: they can\'t delete the data itself)\n        grant insert, delete on {schema}.dataset_type,\n                                {schema}.metadata_type to agdc_manage;\n        -- Allow creation of indexes, views\n        grant create on schema {schema} to agdc_manage;\n        """""".format(schema=SCHEMA_NAME))\n\n    c.close()\n\n    return is_new\n\n\ndef database_exists(engine):\n    """"""\n    Have they init\'d this database?\n    """"""\n    return has_schema(engine, engine)\n\n\ndef schema_is_latest(engine: Engine) -> bool:\n    """"""\n    Is the current schema up-to-date?\n\n    This is run when a new connection is established to see if it\'s compatible.\n\n    It should be runnable by unprivileged users. If it returns false, their\n    connection will be rejected and they will be told to get an administrator\n    to apply updates.\n\n    See the ``update_schema()`` function below for actually applying the updates.\n    """"""\n    # In lieu of a versioned schema, we typically check by seeing if one of the objects\n    # from the change exists.\n    #\n    # Eg.\n    #     return pg_column_exists(engine, schema_qualified(\'dataset_location\'), \'archived\')\n    #\n    # ie. Does the \'archived\' column exist? If so, we know the related schema was applied.\n\n    # No schema changes recently. Everything is perfect.\n    return True\n\n\ndef update_schema(engine: Engine):\n    """"""\n    Check and apply any missing schema changes to the database.\n\n    This is run by an administrator.\n\n    See the `schema_is_latest()` function above: this should apply updates\n    that it requires.\n    """"""\n    # This will typically check if something exists (like a newly added column), and\n    # run the SQL of the change inside a single transaction.\n\n    # Empty, as no schema changes have been made recently.\n    # -> If you need to write one, look at the Git history of this\n    #    function for some examples.\n    \n    # Post 1.8 DB Federation triggers\n    from datacube.drivers.postgres._triggers import install_timestamp_trigger\n    _LOG.info(""Adding Update Triggers"")\n    c = engine.connect()\n    c.execute(\'begin\')\n    install_timestamp_trigger(c)\n    c.execute(\'commit\')\n    c.close()\n\n\ndef _ensure_role(engine, name, inherits_from=None, add_user=False, create_db=False):\n    if has_role(engine, name):\n        _LOG.debug(\'Role exists: %s\', name)\n        return\n\n    sql = [\n        \'create role %s nologin inherit\' % name,\n        \'createrole\' if add_user else \'nocreaterole\',\n        \'createdb\' if create_db else \'nocreatedb\'\n    ]\n    if inherits_from:\n        sql.append(\'in role \' + inherits_from)\n    engine.execute(\' \'.join(sql))\n\n\ndef grant_role(engine, role, users):\n    if role not in USER_ROLES:\n        raise ValueError(\'Unknown role %r. Expected one of %r\' % (role, USER_ROLES))\n\n    users = [escape_pg_identifier(engine, user) for user in users]\n    with engine.begin():\n        engine.execute(\'revoke {roles} from {users}\'.format(users=\', \'.join(users), roles=\', \'.join(USER_ROLES)))\n        engine.execute(\'grant {role} to {users}\'.format(users=\', \'.join(users), role=role))\n\n\ndef has_role(conn, role_name):\n    return bool(conn.execute(\'SELECT rolname FROM pg_roles WHERE rolname=%s\', role_name).fetchall())\n\n\ndef has_schema(engine, connection):\n    return engine.dialect.has_schema(connection, SCHEMA_NAME)\n\n\ndef drop_db(connection):\n    connection.execute(\'drop schema if exists %s cascade;\' % SCHEMA_NAME)\n\n\ndef to_pg_role(role):\n    """"""\n    Convert a role name to a name for use in PostgreSQL\n\n    There is a short list of valid ODC role names, and they are given\n    a prefix inside of PostgreSQL.\n\n    Why are we even doing this? Can\'t we use the same names internally and externally?\n\n    >>> to_pg_role(\'ingest\')\n    \'agdc_ingest\'\n    >>> to_pg_role(\'fake\')\n    Traceback (most recent call last):\n    ...\n    ValueError: Unknown role \'fake\'. Expected one of ...\n    """"""\n    pg_role = \'agdc_\' + role.lower()\n    if pg_role not in USER_ROLES:\n        raise ValueError(\n            \'Unknown role %r. Expected one of %r\' %\n            (role, [r.split(\'_\')[1] for r in USER_ROLES])\n        )\n    return pg_role\n\n\ndef from_pg_role(pg_role):\n    """"""\n    Convert a PostgreSQL role name back to an ODC name.\n\n    >>> from_pg_role(\'agdc_admin\')\n    \'admin\'\n    >>> from_pg_role(\'fake\')\n    Traceback (most recent call last):\n    ...\n    ValueError: Not a pg role: \'fake\'. Expected one of ...\n    """"""\n    if pg_role not in USER_ROLES:\n        raise ValueError(\'Not a pg role: %r. Expected one of %r\' % (pg_role, USER_ROLES))\n\n    return pg_role.split(\'_\')[1]\n'"
datacube/drivers/postgres/_dynamic.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nMethods for managing dynamic dataset field indexes and views.\n""""""\n\nimport logging\n\nfrom sqlalchemy import Index\nfrom sqlalchemy import select\n\nfrom ._core import schema_qualified\nfrom ._schema import DATASET, PRODUCT, METADATA_TYPE\nfrom .sql import pg_exists, CreateView\n\n_LOG = logging.getLogger(__name__)\n\n\ndef contains_all(d_, *keys):\n    """"""\n    Does the dictionary have values for all of the given keys?\n\n    >>> contains_all({\'a\': 4}, \'a\')\n    True\n    >>> contains_all({\'a\': 4, \'b\': 5}, \'a\', \'b\')\n    True\n    >>> contains_all({\'b\': 5}, \'a\')\n    False\n    """"""\n    return all([d_.get(key) for key in keys])\n\n\ndef _ensure_view(conn, fields, name, replace_existing, where_expression):\n    """"""\n    Ensure a view exists for the given fields\n    """"""\n    # Create a view of search fields (for debugging convenience).\n    # \'dv_\' prefix: dynamic view. To distinguish from views that are created as part of the schema itself.\n    view_name = schema_qualified(\'dv_{}_dataset\'.format(name))\n    exists = pg_exists(conn, view_name)\n    # This currently leaves a window of time without the views: it\'s primarily intended for development.\n    if exists and replace_existing:\n        _LOG.debug(\'Dropping view: %s (replace=%r)\', view_name, replace_existing)\n        conn.execute(\'drop view %s\' % view_name)\n        exists = False\n    if not exists:\n        _LOG.debug(\'Creating view: %s\', view_name)\n        conn.execute(\n            CreateView(\n                view_name,\n                select(\n                    [field.alchemy_expression.label(field.name) for field in fields.values()\n                     if not field.affects_row_selection]\n                ).select_from(\n                    DATASET.join(PRODUCT).join(METADATA_TYPE)\n                ).where(where_expression)\n            )\n        )\n    else:\n        _LOG.debug(\'View exists: %s (replace=%r)\', view_name, replace_existing)\n    legacy_name = schema_qualified(\'{}_dataset\'.format(name))\n    if pg_exists(conn, legacy_name):\n        _LOG.debug(\'Dropping legacy view: %s\', legacy_name)\n        conn.execute(\'drop view %s\' % legacy_name)\n\n\ndef check_dynamic_fields(conn, concurrently, dataset_filter, excluded_field_names, fields, name,\n                         rebuild_indexes=False, rebuild_view=False):\n    """"""\n    Check that we have expected indexes and views for the given fields\n    """"""\n\n    # If this type has time/space fields, create composite indexes (as they are often searched together)\n    # We will probably move these into product configuration in the future.\n    composite_indexes = (\n        (\'lat\', \'lon\', \'time\'),\n        (\'time\', \'lat\', \'lon\'),\n        (\'sat_path\', \'sat_row\', \'time\')\n    )\n\n    all_exclusions = tuple(excluded_field_names)\n    for composite_names in composite_indexes:\n        # If all of the fields are available in this product, we\'ll create a composite index\n        # for them instead of individual indexes.\n        if contains_all(fields, *composite_names):\n            all_are_excluded = set(excluded_field_names) >= set(composite_names)\n            _check_field_index(\n                conn,\n                [fields.get(f) for f in composite_names],\n                name, dataset_filter,\n                concurrently=concurrently,\n                replace_existing=rebuild_indexes,\n                # If all fields were excluded individually it should be removed.\n                should_exist=not all_are_excluded,\n                index_type=\'gist\'\n            )\n            all_exclusions += composite_names\n\n    # Create indexes for the individual fields.\n    for field in fields.values():\n        if not field.postgres_index_type:\n            continue\n        _check_field_index(\n            conn, [field],\n            name, dataset_filter,\n            should_exist=field.indexed and (field.name not in all_exclusions),\n            concurrently=concurrently,\n            replace_existing=rebuild_indexes,\n        )\n    # A view of all fields\n    _ensure_view(conn, fields, name, rebuild_view, dataset_filter)\n\n\ndef _check_field_index(conn, fields, name_prefix, filter_expression,\n                       should_exist=True, concurrently=False,\n                       replace_existing=False, index_type=None):\n    """"""\n    Check the status of a given index: add or remove it as needed\n    """"""\n    if index_type is None:\n        if len(fields) > 1:\n            raise ValueError(\'Must specify index type for composite indexes.\')\n        index_type = fields[0].postgres_index_type\n\n    field_name = \'_\'.join([f.name.lower() for f in fields])\n    # Our normal indexes start with ""ix_"", dynamic indexes with ""dix_""\n    index_name = \'dix_{prefix}_{field_name}\'.format(\n        prefix=name_prefix.lower(),\n        field_name=field_name\n    )\n    # Previous naming scheme\n    legacy_name = \'dix_field_{prefix}_dataset_{field_name}\'.format(\n        prefix=name_prefix.lower(),\n        field_name=field_name,\n    )\n    indexed_expressions = [f.alchemy_expression for f in fields]\n    index = Index(\n        index_name,\n        *indexed_expressions,\n        postgresql_where=filter_expression,\n        postgresql_using=index_type,\n        # Don\'t lock the table (in the future we\'ll allow indexing new fields...)\n        postgresql_concurrently=concurrently\n    )\n    exists = pg_exists(conn, schema_qualified(index_name))\n    legacy_exists = pg_exists(conn, schema_qualified(legacy_name))\n\n    # This currently leaves a window of time without indexes: it\'s primarily intended for development.\n    if replace_existing or (not should_exist):\n        if exists:\n            _LOG.debug(\'Dropping index: %s (replace=%r)\', index_name, replace_existing)\n            index.drop(conn)\n            exists = False\n        if legacy_exists:\n            _LOG.debug(\'Dropping legacy index: %s (replace=%r)\', legacy_name, replace_existing)\n            Index(legacy_name, *indexed_expressions).drop(conn)\n            legacy_exists = False\n\n    if should_exist:\n        if not (exists or legacy_exists):\n            _LOG.info(\'Creating index: %s\', index_name)\n            index.create(conn)\n        else:\n            _LOG.debug(\'Index exists: %s  (replace=%r)\', index_name, replace_existing)\n'"
datacube/drivers/postgres/_fields.py,0,"b'# coding=utf-8\n# pylint: disable=abstract-method\n""""""\nBuild and index fields within documents.\n""""""\nfrom collections import namedtuple\nfrom datetime import datetime, date\nfrom decimal import Decimal\n\nfrom dateutil import tz\nfrom psycopg2.extras import NumericRange, DateTimeTZRange\nfrom sqlalchemy import cast, func, and_\nfrom sqlalchemy.dialects import postgresql as postgres\nfrom sqlalchemy.dialects.postgresql import INT4RANGE\nfrom sqlalchemy.dialects.postgresql import NUMRANGE, TSTZRANGE\nfrom sqlalchemy.sql import ColumnElement\n\nfrom datacube import utils\nfrom datacube.model.fields import Expression, Field\nfrom datacube.model import Range\nfrom datacube.utils import get_doc_offset_safe\nfrom .sql import FLOAT8RANGE\n\nfrom typing import Any, Callable, Tuple, Union\n\n\nclass PgField(Field):\n    """"""\n    Postgres implementation of a searchable field. May be a value inside\n    a JSONB column.\n    """"""\n\n    def __init__(self, name, description, alchemy_column, indexed):\n        super(PgField, self).__init__(name, description)\n\n        # The underlying SQLAlchemy column. (eg. DATASET.c.metadata)\n        self.alchemy_column = alchemy_column\n        self.indexed = indexed\n\n    @property\n    def required_alchemy_table(self):\n        return self.alchemy_column.table\n\n    @property\n    def alchemy_expression(self):\n        """"""\n        Get an SQLAlchemy expression for accessing this field.\n        :return:\n        """"""\n        raise NotImplementedError(\'alchemy expression\')\n\n    @property\n    def sql_expression(self):\n        """"""\n        Get the raw SQL expression for this field as a string.\n        :rtype: str\n        """"""\n        return str(self.alchemy_expression.compile(\n            dialect=postgres.dialect(),\n            compile_kwargs={""literal_binds"": True}\n        ))\n\n    @property\n    def postgres_index_type(self):\n        return \'btree\'\n\n    def __eq__(self, value):\n        """"""\n        :rtype: Expression\n        """"""\n        return EqualsExpression(self, value)\n\n    def between(self, low, high):\n        """"""\n        :rtype: Expression\n        """"""\n        raise NotImplementedError(\'between expression\')\n\n\nclass NativeField(PgField):\n    """"""\n    Fields hard-coded into the schema. (not user configurable)\n    """"""\n\n    def __init__(self, name, description, alchemy_column, alchemy_expression=None,\n                 # Should this be selected by default when selecting all fields?\n                 affects_row_selection=False):\n        super(NativeField, self).__init__(name, description, alchemy_column, False)\n        self._expression = alchemy_expression\n        self.affects_row_selection = affects_row_selection\n\n    @property\n    def alchemy_expression(self):\n        expression = self._expression if self._expression is not None else self.alchemy_column\n        return expression.label(self.name)\n\n    @property\n    def postgres_index_type(self):\n        # Don\'t add extra indexes for native fields.\n        return None\n\n\nclass PgDocField(PgField):\n    """"""\n    A field extracted from inside a (jsonb) document.\n    """"""\n\n    def extract(self, document):\n        """"""\n        Extract a value from the given document in pure python (no postgres).\n        """"""\n        raise NotImplementedError(""extract()"")\n\n    def value_to_alchemy(self, value):\n        """"""\n        Wrap the given value with any necessary type casts/conversions for this field.\n\n        Overridden by other classes as needed.\n        """"""\n        # Default do nothing (eg. string datatypes)\n        return value\n\n    def parse_value(self, value):\n        """"""\n        Parse the value from a string. May be overridden by subclasses.\n        """"""\n        return value\n\n    def _alchemy_offset_value(self, doc_offsets, agg_function):\n        # type: (Tuple[Tuple[str]], Callable[[Any], ColumnElement]) -> ColumnElement\n        """"""\n        Get an sqlalchemy value for the given offsets of this field\'s sqlalchemy column.\n        If there are multiple they will be combined using the given aggregate function.\n\n        Offsets can either be single:\n            (\'platform\', \'code\')\n        Or multiple:\n            ((\'platform\', \'code\'), (\'satellite\', \'name\'))\n\n        In the latter case, the multiple values are combined using the given aggregate function\n        (defaults to using coalesce: grab the first non-null value)\n        """"""\n        if not doc_offsets:\n            raise ValueError(""Value requires at least one offset"")\n\n        if isinstance(doc_offsets[0], str):\n            # It\'s a single offset.\n            doc_offsets = [doc_offsets]\n\n        alchemy_values = [self.value_to_alchemy(self.alchemy_column[offset].astext) for offset in doc_offsets]\n        # If there\'s multiple fields, we aggregate them (eg. ""min()""). Otherwise use the one.\n        return agg_function(*alchemy_values) if len(alchemy_values) > 1 else alchemy_values[0]\n\n    def _extract_offset_value(self, doc, doc_offsets, agg_function):\n        """"""\n        Extract a value for the given document offsets.\n\n        Same as _alchemy_offset_value(), but returns the value instead of an sqlalchemy expression to calc the value.\n        """"""\n        if not doc_offsets:\n            raise ValueError(""Value requires at least one offset"")\n\n        if isinstance(doc_offsets[0], str):\n            # It\'s a single offset.\n            doc_offsets = [doc_offsets]\n\n        values = (get_doc_offset_safe(offset, doc) for offset in doc_offsets)\n        values = [self.parse_value(v) for v in values if v is not None]\n\n        if not values:\n            return None\n        if len(values) == 1:\n            return values[0]\n        return agg_function(*values)\n\n\nclass SimpleDocField(PgDocField):\n    """"""\n    A field with a single value (eg. String, int) calculated as an offset inside a (jsonb) document.\n    """"""\n\n    def __init__(self, name, description, alchemy_column, indexed, offset=None, selection=\'first\'):\n        super(SimpleDocField, self).__init__(name, description, alchemy_column, indexed)\n        self.offset = offset\n        if selection not in SELECTION_TYPES:\n            raise ValueError(\n                ""Unknown field selection type %s. Expected one of: %r"" % (selection, (SELECTION_TYPES,),)\n            )\n        self.aggregation = SELECTION_TYPES[selection]\n\n    @property\n    def alchemy_expression(self):\n        return self._alchemy_offset_value(self.offset, self.aggregation.pg_calc)\n\n    def __eq__(self, value):\n        """"""\n        :rtype: Expression\n        """"""\n        return EqualsExpression(self, value)\n\n    def between(self, low, high):\n        """"""\n        :rtype: Expression\n        """"""\n        raise NotImplementedError(\'Simple field between expression\')\n\n    def extract(self, document):\n        return self._extract_offset_value(document, self.offset, self.aggregation.calc)\n\n    def evaluate(self, ctx):\n        return self.extract(ctx)\n\n\nclass IntDocField(SimpleDocField):\n    type_name = \'integer\'\n\n    def value_to_alchemy(self, value):\n        return cast(value, postgres.INTEGER)\n\n    def between(self, low, high):\n        return ValueBetweenExpression(self, low, high)\n\n    def parse_value(self, value):\n        return int(value)\n\n\nclass NumericDocField(SimpleDocField):\n    type_name = \'numeric\'\n\n    def value_to_alchemy(self, value):\n        return cast(value, postgres.NUMERIC)\n\n    def between(self, low, high):\n        return ValueBetweenExpression(self, low, high)\n\n    def parse_value(self, value):\n        return Decimal(value)\n\n\nclass DoubleDocField(SimpleDocField):\n    type_name = \'double\'\n\n    def value_to_alchemy(self, value):\n        return cast(value, postgres.DOUBLE_PRECISION)\n\n    def between(self, low, high):\n        return ValueBetweenExpression(self, low, high)\n\n    def parse_value(self, value):\n        return float(value)\n\n\nclass DateDocField(SimpleDocField):\n    type_name = \'datetime\'\n\n    def value_to_alchemy(self, value):\n        # type: (Union[datetime, date, str, ColumnElement]) -> Union[datetime, date, str, ColumnElement]\n        """"""\n        Wrap a value as needed for this field type.\n        """"""\n        if isinstance(value, datetime):\n            return _default_utc(value)\n        # SQLAlchemy expression or string are parsed in pg as dates.\n        elif isinstance(value, (ColumnElement, str)):\n            return func.agdc.common_timestamp(value)\n        else:\n            raise ValueError(""Value not readable as date: %r"" % (value,))\n\n    def between(self, low, high):\n        return ValueBetweenExpression(self, low, high)\n\n    def parse_value(self, value):\n        return utils.parse_time(value)\n\n    @property\n    def day(self):\n        """"""Get field truncated to the day""""""\n        return NativeField(\n            \'{}_day\'.format(self.name),\n            \'Day of {}\'.format(self.description),\n            self.alchemy_column,\n            alchemy_expression=cast(func.date_trunc(\'day\', self.alchemy_expression), postgres.TIMESTAMP)\n        )\n\n\nclass RangeDocField(PgDocField):\n    """"""\n    A range of values. Has min and max values, which may be calculated from multiple\n    values in the document.\n    """"""\n    FIELD_CLASS = SimpleDocField\n\n    def __init__(self, name, description, alchemy_column, indexed, min_offset=None, max_offset=None):\n        super(RangeDocField, self).__init__(name, description, alchemy_column, indexed)\n        self.lower = self.FIELD_CLASS(\n            name + \'_lower\',\n            description,\n            alchemy_column,\n            indexed=False,\n            offset=min_offset,\n            selection=\'least\'\n        )\n        self.greater = self.FIELD_CLASS(\n            name + \'_greater\',\n            description,\n            alchemy_column,\n            indexed=False,\n            offset=max_offset,\n            selection=\'greatest\'\n        )\n\n    def value_to_alchemy(self, value):\n        raise NotImplementedError(\'range type\')\n\n    @property\n    def postgres_index_type(self):\n        return \'gist\'\n\n    @property\n    def alchemy_expression(self):\n        return self.value_to_alchemy((self.lower.alchemy_expression, self.greater.alchemy_expression))\n\n    def __eq__(self, value):\n        """"""\n        :rtype: Expression\n        """"""\n        # Lower and higher are interchangeable here: they\'re the same type.\n        casted_val = self.lower.value_to_alchemy(value)\n        return RangeContainsExpression(self, casted_val)\n\n    def extract(self, document):\n        min_val = self.lower.extract(document)\n        max_val = self.greater.extract(document)\n        if not min_val and not max_val:\n            return None\n        return Range(min_val, max_val)\n\n\nclass NumericRangeDocField(RangeDocField):\n    FIELD_CLASS = NumericDocField\n    type_name = \'numeric-range\'\n\n    def value_to_alchemy(self, value):\n        low, high = value\n        return func.numrange(\n            low, high,\n            # Inclusive on both sides.\n            \'[]\',\n            type_=NUMRANGE,\n        )\n\n    def between(self, low, high):\n        """"""\n        :rtype: Expression\n        """"""\n        return RangeBetweenExpression(self, low, high, _range_class=NumericRange)\n\n\nclass IntRangeDocField(RangeDocField):\n    FIELD_CLASS = IntDocField\n    type_name = \'integer-range\'\n\n    def value_to_alchemy(self, value):\n        low, high = value\n        return func.numrange(\n            low, high,\n            # Inclusive on both sides.\n            \'[]\',\n            type_=INT4RANGE,\n        )\n\n    def between(self, low, high):\n        """"""\n        :rtype: Expression\n        """"""\n        return RangeBetweenExpression(self, low, high, _range_class=NumericRange)\n\n\nclass DoubleRangeDocField(RangeDocField):\n    FIELD_CLASS = DoubleDocField\n    type_name = \'double-range\'\n\n    def value_to_alchemy(self, value):\n        low, high = value\n        return func.agdc.float8range(\n            low, high,\n            # Inclusive on both sides.\n            \'[]\',\n            type_=FLOAT8RANGE,\n        )\n\n    def between(self, low, high):\n        """"""\n        :rtype: Expression\n        """"""\n        return RangeBetweenExpression(self, low, high, _range_class=NumericRange)\n\n\nclass DateRangeDocField(RangeDocField):\n    FIELD_CLASS = DateDocField\n    type_name = \'datetime-range\'\n\n    def value_to_alchemy(self, value):\n        low, high = value\n        return func.tstzrange(\n            low, high,\n            # Inclusive on both sides.\n            \'[]\',\n            type_=TSTZRANGE,\n        )\n\n    def between(self, low, high):\n        """"""\n        :rtype: Expression\n        """"""\n        low = _number_implies_year(low)\n        high = _number_implies_year(high)\n\n        if isinstance(low, datetime) and isinstance(high, datetime):\n            return RangeBetweenExpression(\n                self,\n                _default_utc(low),\n                _default_utc(high),\n                _range_class=DateTimeTZRange\n            )\n        else:\n            raise ValueError(""Unknown comparison type for date range: ""\n                             ""expecting datetimes, got: (%r, %r)"" % (low, high))\n\n\ndef _number_implies_year(v):\n    # type: (Union[int, datetime]) -> datetime\n    """"""\n    >>> _number_implies_year(1994)\n    datetime.datetime(1994, 1, 1, 0, 0)\n    >>> _number_implies_year(datetime(1994, 4, 4))\n    datetime.datetime(1994, 4, 4, 0, 0)\n    """"""\n    if isinstance(v, int):\n        return datetime(v, 1, 1)\n    # The expression module parses all number ranges as floats.\n    if isinstance(v, float):\n        return datetime(int(v), 1, 1)\n\n    return v\n\n\nclass PgExpression(Expression):\n    def __init__(self, field):\n        super(PgExpression, self).__init__()\n        #: :type: PgField\n        self.field = field\n\n    @property\n    def alchemy_expression(self):\n        """"""\n        Get an SQLAlchemy expression for accessing this field.\n        :return:\n        """"""\n        raise NotImplementedError(\'alchemy expression\')\n\n\nclass ValueBetweenExpression(PgExpression):\n    def __init__(self, field, low_value, high_value):\n        super(ValueBetweenExpression, self).__init__(field)\n        self.low_value = low_value\n        self.high_value = high_value\n\n    @property\n    def alchemy_expression(self):\n        if self.low_value is not None and self.high_value is not None:\n            return and_(self.field.alchemy_expression >= self.low_value,\n                        self.field.alchemy_expression < self.high_value)\n        if self.low_value is not None:\n            return self.field.alchemy_expression >= self.low_value\n        if self.high_value is not None:\n            return self.field.alchemy_expression < self.high_value\n\n        raise ValueError(\'Expect at least one of [low,high] to be set\')\n\n\nclass RangeBetweenExpression(PgExpression):\n    def __init__(self, field, low_value, high_value, _range_class):\n        super(RangeBetweenExpression, self).__init__(field)\n        self.low_value = low_value\n        self.high_value = high_value\n        self._range_class = _range_class\n\n    @property\n    def alchemy_expression(self):\n        return self.field.alchemy_expression.overlaps(\n            self._range_class(self.low_value, self.high_value)\n        )\n\n\nclass RangeContainsExpression(PgExpression):\n    def __init__(self, field, value):\n        super(RangeContainsExpression, self).__init__(field)\n        self.value = value\n\n    @property\n    def alchemy_expression(self):\n        return self.field.alchemy_expression.contains(self.value)\n\n\nclass EqualsExpression(PgExpression):\n    def __init__(self, field, value):\n        super(EqualsExpression, self).__init__(field)\n        self.value = value\n\n    @property\n    def alchemy_expression(self):\n        return self.field.alchemy_expression == self.value\n\n    def evaluate(self, ctx):\n        return self.field.evaluate(ctx) == self.value\n\n\ndef parse_fields(doc, table_column):\n    """"""\n    Parse a field spec document into objects.\n\n    Example document:\n\n    ::\n\n        {\n            # Field name:\n            \'lat\': {\n                # Field type & properties.\n                \'type\': \'float-range\',\n                \'min_offset\': [\n                    # Offsets within a dataset document for this field.\n                    [\'extent\', \'coord\', \'ul\', \'lat\'],\n                    [\'extent\', \'coord\', \'ll\', \'lat\']\n                ],\n                \'max_offset\': [\n                    [\'extent\', \'coord\', \'ur\', \'lat\'],\n                    [\'extent\', \'coord\', \'lr\', \'lat\']\n                ]\n            }\n        }\n\n    :param table_column: SQLAlchemy jsonb column for the document we\'re reading fields from.\n    :type doc: dict\n    :rtype: dict[str, PgField]\n    """"""\n\n    # Implementations of fields for this driver\n    types = {\n        SimpleDocField,\n        IntDocField,\n        DoubleDocField,\n        DateDocField,\n\n        NumericRangeDocField,\n        IntRangeDocField,\n        DoubleRangeDocField,\n        DateRangeDocField,\n    }\n    type_map = {f.type_name: f for f in types}\n    # An alias for backwards compatibility\n    type_map[\'float-range\'] = NumericRangeDocField\n\n    # No later field should have overridden string\n    assert type_map[\'string\'] == SimpleDocField\n\n    def _get_field(name, descriptor, column):\n        """"""\n        :type name: str\n        :type descriptor: dict\n        :param column: SQLAlchemy table column\n        :rtype: PgField\n        """"""\n        ctorargs = descriptor.copy()\n        type_name = ctorargs.pop(\'type\', \'string\')\n        description = ctorargs.pop(\'description\', None)\n        indexed_val = ctorargs.pop(\'indexed\', ""true"")\n        indexed = indexed_val.lower() == \'true\' if isinstance(indexed_val, str) else indexed_val\n\n        field_class = type_map.get(type_name)\n        if not field_class:\n            raise ValueError((\'Field %r has unknown type %r.\'\n                              \' Available types are: %r\') % (name, type_name, list(type_map.keys())))\n        try:\n            return field_class(name, description, column, indexed, **ctorargs)\n        except TypeError as e:\n            raise RuntimeError(\n                \'Field {name} has unexpected argument for a {type}\'.format(\n                    name=name, type=type_name\n                ), e\n            )\n\n    return {name: _get_field(name, descriptor, table_column) for name, descriptor in doc.items()}\n\n\ndef _coalesce(*values):\n    """"""\n    Return first non-none value.\n    Return None if all values are None, or there are no values passed in.\n\n    >>> _coalesce(1, 2)\n    1\n    >>> _coalesce(None, 2, 3)\n    2\n    >>> _coalesce(None, None, 3, None, 5)\n    3\n    """"""\n    for v in values:\n        if v is not None:\n            return v\n    return None\n\n\ndef _default_utc(d):\n    if d.tzinfo is None:\n        return d.replace(tzinfo=tz.tzutc())\n    return d\n\n\n# How to choose/combine multiple doc values.\nValueAggregation = namedtuple(\'ValueAggregation\', (\'calc\', \'pg_calc\'))\nSELECTION_TYPES = {\n    # First non-null\n    \'first\': ValueAggregation(_coalesce, func.coalesce),\n    # min/max\n    \'least\': ValueAggregation(min, func.least),\n    \'greatest\': ValueAggregation(max, func.greatest),\n}\n'"
datacube/drivers/postgres/_schema.py,0,"b'# coding=utf-8\n""""""\nTables for indexing the datasets which were ingested into the AGDC.\n""""""\n\nimport logging\n\nfrom sqlalchemy import ForeignKey, UniqueConstraint, PrimaryKeyConstraint, CheckConstraint, SmallInteger\nfrom sqlalchemy import Table, Column, Integer, String, DateTime\nfrom sqlalchemy.dialects import postgresql as postgres\nfrom sqlalchemy.sql import func\n\nfrom . import sql\nfrom . import _core\n\n_LOG = logging.getLogger(__name__)\n\nMETADATA_TYPE = Table(\n    \'metadata_type\', _core.METADATA,\n    Column(\'id\', SmallInteger, primary_key=True, autoincrement=True),\n\n    Column(\'name\', String, unique=True, nullable=False),\n\n    Column(\'definition\', postgres.JSONB, nullable=False),\n\n    # When it was added and by whom.\n    Column(\'added\', DateTime(timezone=True), server_default=func.now(), nullable=False),\n    Column(\'added_by\', sql.PGNAME, server_default=func.current_user(), nullable=False),\n\n    # Name must be alphanumeric + underscores.\n    CheckConstraint(r""name ~* \'^\\w+$\'"", name=\'alphanumeric_name\'),\n)\n\nPRODUCT = Table(\n    \'dataset_type\', _core.METADATA,\n    Column(\'id\', SmallInteger, primary_key=True, autoincrement=True),\n\n    # A name/label for this type (eg. \'ls7_nbar\'). Specified by users.\n    Column(\'name\', String, unique=True, nullable=False),\n\n    # All datasets of this type should contain these fields.\n    # (newly-ingested datasets may be matched against these fields to determine the dataset type)\n    Column(\'metadata\', postgres.JSONB, nullable=False),\n\n    # The metadata format expected (eg. what fields to search by)\n    Column(\'metadata_type_ref\', None, ForeignKey(METADATA_TYPE.c.id), nullable=False),\n\n    Column(\'definition\', postgres.JSONB, nullable=False),\n\n    # When it was added and by whom.\n    Column(\'added\', DateTime(timezone=True), server_default=func.now(), nullable=False),\n    Column(\'added_by\', sql.PGNAME, server_default=func.current_user(), nullable=False),\n\n    # Name must be alphanumeric + underscores.\n    CheckConstraint(r""name ~* \'^\\w+$\'"", name=\'alphanumeric_name\'),\n)\n\nDATASET = Table(\n    \'dataset\', _core.METADATA,\n    Column(\'id\', postgres.UUID(as_uuid=True), primary_key=True),\n\n    Column(\'metadata_type_ref\', None, ForeignKey(METADATA_TYPE.c.id), nullable=False),\n    Column(\'dataset_type_ref\', None, ForeignKey(PRODUCT.c.id), index=True, nullable=False),\n\n    Column(\'metadata\', postgres.JSONB, index=False, nullable=False),\n\n    # Date it was archived. Null for active datasets.\n    Column(\'archived\', DateTime(timezone=True), default=None, nullable=True),\n\n    # When it was added and by whom.\n    Column(\'added\', DateTime(timezone=True), server_default=func.now(), nullable=False),\n    Column(\'added_by\', sql.PGNAME, server_default=func.current_user(), nullable=False),\n)\n\nDATASET_LOCATION = Table(\n    \'dataset_location\', _core.METADATA,\n    Column(\'id\', Integer, primary_key=True, autoincrement=True),\n    Column(\'dataset_ref\', None, ForeignKey(DATASET.c.id), index=True, nullable=False),\n\n    # The base URI to find the dataset.\n    #\n    # All paths in the dataset metadata can be computed relative to this.\n    # (it is often the path of the source metadata file)\n    #\n    # eg \'file:///g/data/datasets/LS8_NBAR/agdc-metadata.yaml\' or \'ftp://eo.something.com/dataset\'\n    # \'file\' is a scheme, \'///g/data/datasets/LS8_NBAR/agdc-metadata.yaml\' is a body.\n    Column(\'uri_scheme\', String, nullable=False),\n    Column(\'uri_body\', String, nullable=False),\n\n    # When it was added and by whom.\n    Column(\'added\', DateTime(timezone=True), server_default=func.now(), nullable=False),\n    Column(\'added_by\', sql.PGNAME, server_default=func.current_user(), nullable=False),\n\n    # Date it was archived. Null for active locations.\n    Column(\'archived\', DateTime(timezone=True), default=None, nullable=True),\n\n    UniqueConstraint(\'uri_scheme\', \'uri_body\', \'dataset_ref\'),\n)\n\n# Link datasets to their source datasets.\nDATASET_SOURCE = Table(\n    \'dataset_source\', _core.METADATA,\n    Column(\'dataset_ref\', None, ForeignKey(DATASET.c.id), nullable=False),\n\n    # An identifier for this source dataset.\n    #    -> Usually it\'s the dataset type (\'ortho\', \'nbar\'...), as there\'s typically only one source\n    #       of each type.\n    Column(\'classifier\', String, nullable=False),\n    Column(\'source_dataset_ref\', None, ForeignKey(DATASET.c.id), nullable=False),\n\n    PrimaryKeyConstraint(\'dataset_ref\', \'classifier\'),\n    UniqueConstraint(\'source_dataset_ref\', \'dataset_ref\'),\n)\n'"
datacube/drivers/postgres/_triggers.py,0,"b'# coding=utf-8\n""""""\nMethods for adding triggers to capture row update time-stamps\n""""""\n\nfrom .sql import SCHEMA_NAME\nfrom ._schema import (\n    METADATA_TYPE, DATASET, DATASET_LOCATION, PRODUCT, DATASET_SOURCE\n)\n\nUPDATE_TIMESTAMP_SQL = """"""\ncreate or replace function {schema}.set_row_update_time()\nreturns trigger as $$\nbegin\n  new.updated = now();\n  return new;\nend;\n$$ language plpgsql;\n"""""".format(schema=SCHEMA_NAME)\n\nUPDATE_COLUMN_MIGRATE_SQL_TEMPLATE = """"""\nalter table {schema}.{table} add column if not exists updated\ntimestamptz not null default now();\n""""""\n\nINSTALL_TRIGGER_SQL_TEMPLATE = """"""\ndrop trigger if exists row_update_time_{table} on {schema}.{table};\ncreate trigger row_update_time_{table}\nbefore update on {schema}.{table}\nfor each row\nexecute procedure {schema}.set_row_update_time();\n""""""\n\nTABLE_NAMES = [\n    METADATA_TYPE.name,\n    PRODUCT.name,\n    DATASET_SOURCE.name,\n    DATASET.name,\n    DATASET_LOCATION.name\n]\n\n\ndef install_timestamp_trigger(conn):\n    # Create trigger capture function\n    conn.execute(UPDATE_TIMESTAMP_SQL)\n\n    for name in TABLE_NAMES:\n        # Add update_at columns\n        # HACK: Make this more SQLAlchemy with add_column on Table objects\n        conn.execute(UPDATE_COLUMN_MIGRATE_SQL_TEMPLATE.format(schema=SCHEMA_NAME, table=name))\n        conn.execute(INSTALL_TRIGGER_SQL_TEMPLATE.format(schema=SCHEMA_NAME, table=name))\n'"
datacube/drivers/postgres/sql.py,0,"b'# coding=utf-8\n""""""\nCustom types for postgres & sqlalchemy\n""""""\n\nfrom sqlalchemy import TIMESTAMP\nfrom sqlalchemy.dialects.postgresql.ranges import RangeOperators\nfrom sqlalchemy.ext.compiler import compiles\nfrom sqlalchemy.sql import sqltypes\nfrom sqlalchemy.sql.expression import Executable, ClauseElement\nfrom sqlalchemy.sql.functions import GenericFunction\n\nSCHEMA_NAME = \'agdc\'\n\n\nclass CreateView(Executable, ClauseElement):\n    def __init__(self, name, select):\n        self.name = name\n        self.select = select\n\n\n@compiles(CreateView)\ndef visit_create_view(element, compiler, **kw):\n    return ""CREATE VIEW %s AS %s"" % (\n        element.name,\n        compiler.process(element.select, literal_binds=True)\n    )\n\n\nTYPES_INIT_SQL = """"""\ncreate or replace function {schema}.common_timestamp(text)\nreturns timestamp with time zone as $$\nselect ($1)::timestamp at time zone \'utc\';\n$$ language sql immutable returns null on null input;\n\ncreate type {schema}.float8range as range (\n    subtype = float8,\n    subtype_diff = float8mi\n);\n"""""".format(schema=SCHEMA_NAME)\n\n\n# pylint: disable=abstract-method\nclass FLOAT8RANGE(RangeOperators, sqltypes.TypeEngine):\n    __visit_name__ = \'FLOAT8RANGE\'\n\n\n@compiles(FLOAT8RANGE)\ndef visit_float8range(element, compiler, **kw):\n    return ""FLOAT8RANGE""\n\n\n# Register the function with SQLAlchemhy.\n# pylint: disable=too-many-ancestors\nclass CommonTimestamp(GenericFunction):\n    type = TIMESTAMP(timezone=True)\n    package = \'agdc\'\n    identifier = \'common_timestamp\'\n\n    name = \'common_timestamp\'\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.packagenames = [\'%s\' % SCHEMA_NAME]\n\n\n# pylint: disable=too-many-ancestors\nclass Float8Range(GenericFunction):\n    type = FLOAT8RANGE\n    package = \'agdc\'\n    identifier = \'float8range\'\n\n    name = \'float8range\'\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.packagenames = [\'%s\' % SCHEMA_NAME]\n\n\nclass PGNAME(sqltypes.Text):\n    """"""Postgres \'NAME\' type.""""""\n    __visit_name__ = \'NAME\'\n\n\n@compiles(PGNAME)\ndef visit_name(element, compiler, **kw):\n    return ""NAME""\n\n\ndef pg_exists(conn, name):\n    """"""\n    Does a postgres object exist?\n    :rtype bool\n    """"""\n    return conn.execute(""SELECT to_regclass(%s)"", name).scalar() is not None\n\n\ndef pg_column_exists(conn, table, column):\n    """"""\n    Does a postgres object exist?\n    :rtype bool\n    """"""\n    return conn.execute(""""""\n                        SELECT 1 FROM pg_attribute\n                        WHERE attrelid = to_regclass(%s)\n                        AND attname = %s\n                        AND NOT attisdropped\n                        """""", table, column).scalar() is not None\n\n\ndef escape_pg_identifier(engine, name):\n    """"""\n    Escape identifiers (tables, fields, roles, etc) for inclusion in SQL statements.\n\n    psycopg2 can safely merge query arguments, but cannot do the same for dynamically\n    generating queries.\n\n    See http://initd.org/psycopg/docs/sql.html for more information.\n    """"""\n    # New (2.7+) versions of psycopg2 have function: extensions.quote_ident()\n    # But it\'s too bleeding edge right now. We\'ll ask the server to escape instead, as\n    # these are not performance sensitive.\n    return engine.execute(""select quote_ident(%s)"", name).scalar()\n'"
datacube/drivers/rio/__init__.py,0,"b'"""""" RasterIO based driver\n""""""\n'"
datacube/drivers/rio/_reader.py,3,"b'"""""" reader\n""""""\nfrom typing import (\n    List, Optional, Union, Any, Iterable,\n    Tuple, NamedTuple, TypeVar\n)\nimport numpy as np\nfrom affine import Affine\nfrom concurrent.futures import ThreadPoolExecutor\nimport rasterio\nfrom rasterio.io import DatasetReader\nimport rasterio.crs\n\nfrom datacube.storage import BandInfo\nfrom datacube.utils.geometry import CRS\nfrom datacube.utils import (\n    uri_to_local_path,\n    get_part_from_uri,\n)\nfrom datacube.drivers._types import (\n    ReaderDriverEntry,\n    ReaderDriver,\n    GeoRasterReader,\n    FutureGeoRasterReader,\n    FutureNdarray,\n    RasterShape,\n    RasterWindow,\n)\n\nOverrides = NamedTuple(\'Overrides\', [(\'crs\', Optional[CRS]),\n                                     (\'transform\', Optional[Affine]),\n                                     (\'nodata\', Optional[Union[float, int]])])\n\nRioWindow = Tuple[Tuple[int, int], Tuple[int, int]]  # pylint: disable=invalid-name\nT = TypeVar(\'T\')\n\n\ndef pick(a: Optional[T], b: Optional[T]) -> Optional[T]:\n    """""" Return first non-None value or None if all are None\n    """"""\n    return b if a is None else a\n\n\ndef _is_netcdf(fmt: str) -> bool:\n    return fmt == \'NetCDF\'\n\n\ndef _roi_to_window(roi: Optional[RasterWindow], shape: RasterShape) -> Optional[RioWindow]:\n    if roi is None:\n        return None\n\n    def s2t(s: slice, n: int) -> Tuple[int, int]:\n        _in = 0 if s.start is None else s.start\n        _out = n if s.stop is None else s.stop\n\n        if _in < 0:\n            _in += n\n        if _out < 0:\n            _out += n\n\n        return (_in, _out)\n\n    s1, s2 = (s2t(s, n)\n              for s, n in zip(roi, shape))\n    return (s1, s2)\n\n\ndef _dc_crs(crs: Optional[rasterio.crs.CRS]) -> Optional[CRS]:\n    """""" Convert RIO version of CRS to datacube\n    """"""\n    if crs is None:\n        return None\n\n    if not crs.is_valid:\n        return None\n\n    if crs.is_epsg_code:\n        return CRS(\'epsg:{}\'.format(crs.to_epsg()))\n    return CRS(crs.wkt)\n\n\ndef _read(src: DatasetReader,\n          bidx: int,\n          window: Optional[RasterWindow],\n          out_shape: Optional[RasterShape]) -> np.ndarray:\n    return src.read(bidx,\n                    window=_roi_to_window(window, src.shape),\n                    out_shape=out_shape)\n\n\ndef _rio_uri(band: BandInfo) -> str:\n    """"""\n    - file uris are converted to file names\n       - if also netcdf wrap in NETCDF:""${filename}"":${layer}\n    - All other protocols go through unmodified\n    """"""\n    if band.uri_scheme == \'file\':\n        fname = str(uri_to_local_path(band.uri))\n\n        if _is_netcdf(band.format):\n            fname = \'NETCDF:""{}"":{}\'.format(fname, band.layer)\n\n        return fname\n\n    return band.uri\n\n\ndef _rio_band_idx(band: BandInfo, src: DatasetReader) -> int:\n    if band.band is not None:\n        return band.band\n\n    if not _is_netcdf(band.format):\n        return 1\n\n    bidx = get_part_from_uri(band.uri)\n    if bidx is not None:\n        return bidx\n\n    if src.count == 1:  # Single-slice netcdf file\n        return 1\n\n    raise DeprecationWarning(""Stacked netcdf without explicit time index is not supported anymore"")\n\n\nclass RIOReader(GeoRasterReader):\n    def __init__(self,\n                 src: DatasetReader,\n                 band_idx: int,\n                 pool: ThreadPoolExecutor,\n                 overrides: Overrides = Overrides(None, None, None)):\n\n        transform = pick(overrides.transform, src.transform)\n        if transform is not None and transform.is_identity:\n            transform = None\n\n        self._src = src\n        self._crs = overrides.crs or _dc_crs(src.crs)\n        self._transform = transform\n        self._nodata = pick(overrides.nodata, src.nodatavals[band_idx-1])\n        self._band_idx = band_idx\n        self._dtype = src.dtypes[band_idx-1]\n        self._pool = pool\n\n    @property\n    def crs(self) -> Optional[CRS]:\n        return self._crs\n\n    @property\n    def transform(self) -> Optional[Affine]:\n        return self._transform\n\n    @property\n    def dtype(self) -> np.dtype:\n        return np.dtype(self._dtype)\n\n    @property\n    def shape(self) -> RasterShape:\n        return self._src.shape\n\n    @property\n    def nodata(self) -> Optional[Union[int, float]]:\n        return self._nodata\n\n    def read(self,\n             window: Optional[RasterWindow] = None,\n             out_shape: Optional[RasterShape] = None) -> FutureNdarray:\n        return self._pool.submit(_read, self._src, self._band_idx, window, out_shape)\n\n\ndef _compute_overrides(src: DatasetReader, bi: BandInfo) -> Overrides:\n    """""" If dataset is missing nodata, crs or transform.\n    """"""\n    crs, transform, nodata = None, None, None\n\n    if src.crs is None or not src.crs.is_valid:\n        crs = bi.crs\n\n    if src.transform.is_identity:\n        transform = bi.transform\n\n    if src.nodata is None:\n        nodata = bi.nodata\n\n    return Overrides(crs=crs, transform=transform, nodata=nodata)\n\n\ndef _rdr_open(band: BandInfo, ctx: Any, pool: ThreadPoolExecutor) -> RIOReader:\n    """""" Open file pointed by BandInfo and return RIOReader instance.\n\n        raises Exception on failure\n\n        TODO: start using ctx for handle cache\n    """"""\n    normalised_uri = _rio_uri(band)\n    src = rasterio.open(normalised_uri, \'r\')\n    bidx = _rio_band_idx(band, src)\n\n    return RIOReader(src, bidx, pool, _compute_overrides(src, band))\n\n\nclass RIORdrDriver(ReaderDriver):\n    def __init__(self, pool: ThreadPoolExecutor, cfg: dict):\n        self._pool = pool\n        self._cfg = cfg\n\n    def new_load_context(self,\n                         bands: Iterable[BandInfo],\n                         old_ctx: Optional[Any]) -> Any:\n        return None  # TODO: implement file handle cache with this\n\n    def open(self, band: BandInfo, ctx: Any) -> FutureGeoRasterReader:\n        return self._pool.submit(_rdr_open, band, ctx, self._pool)\n\n\nclass RDEntry(ReaderDriverEntry):\n    PROTOCOLS = [\'file\', \'http\', \'https\', \'s3\', \'ftp\', \'zip\']\n    FORMATS = [\'GeoTIFF\', \'NetCDF\', \'JPEG2000\']\n\n    @property\n    def protocols(self) -> List[str]:\n        return RDEntry.PROTOCOLS\n\n    @property\n    def formats(self) -> List[str]:\n        return RDEntry.FORMATS\n\n    def supports(self, protocol: str, fmt: str) -> bool:\n        # TODO: might need better support matrix structures\n\n        if fmt == \'NetCDF\':\n            return protocol == \'file\'\n\n        return True\n\n    def new_instance(self, cfg: dict) -> ReaderDriver:\n        cfg = cfg.copy()\n        pool = cfg.pop(\'pool\', None)\n        if pool is None:\n            max_workers = cfg.pop(\'max_workers\', 1)\n            pool = ThreadPoolExecutor(max_workers=max_workers)\n        elif not isinstance(pool, ThreadPoolExecutor):\n            if not cfg.pop(\'allow_custom_pool\', False):\n                raise ValueError(""External `pool` should be a `ThreadPoolExecutor`"")\n\n        return RIORdrDriver(pool, cfg)\n'"
datacube/utils/aws/__init__.py,0,"b'""""""\nHelper methods for working with AWS\n""""""\nimport botocore\nimport botocore.session\nfrom botocore.credentials import Credentials, ReadOnlyCredentials\nfrom botocore.session import Session\nimport time\nfrom urllib.request import urlopen\nfrom urllib.parse import urlparse\nfrom typing import Optional, Dict, Tuple, Any, Union, IO\nfrom datacube.utils.generic import thread_local_cache\n\nByteRange = Union[slice, Tuple[int, int]]       # pylint: disable=invalid-name\nMaybeS3 = Optional[botocore.client.BaseClient]  # pylint: disable=invalid-name\n\n\ndef _fetch_text(url: str, timeout: float = 0.1) -> Optional[str]:\n    try:\n        with urlopen(url, timeout=timeout) as resp:\n            if 200 <= resp.getcode() < 300:\n                return resp.read().decode(\'utf8\')\n            else:\n                return None\n    except IOError:\n        return None\n\n\ndef s3_url_parse(url: str) -> Tuple[str, str]:\n    """""" Return Bucket, Key tuple\n    """"""\n    uu = urlparse(url)\n    if uu.scheme != ""s3"":\n        raise ValueError(""Not a valid s3 url"")\n    return uu.netloc, uu.path.lstrip(\'/\')\n\n\ndef s3_fmt_range(r: Optional[ByteRange]):\n    """""" None -> None\n        (in, out) -> ""bytes={in}-{out-1}""\n    """"""\n    if r is None:\n        return None\n\n    if isinstance(r, slice):\n        if r.step not in [1, None]:\n            raise ValueError(""Can not process decimated slices"")\n        if r.stop is None:\n            raise ValueError(""Can not process open ended slices"")\n\n        _in = 0 if r.start is None else r.start\n        _out = r.stop\n    else:\n        _in, _out = r\n\n    if _in < 0 or _out < 0:\n        raise ValueError(""Slice has to be positive"")\n\n    return \'bytes={:d}-{:d}\'.format(_in, _out-1)\n\n\ndef ec2_metadata(timeout: float = 0.1) -> Optional[Dict[str, Any]]:\n    """""" When running inside AWS returns dictionary describing instance identity.\n        Returns None when not inside AWS\n    """"""\n    import json\n    txt = _fetch_text(\'http://169.254.169.254/latest/dynamic/instance-identity/document\', timeout)\n\n    if txt is None:\n        return None\n\n    try:\n        return json.loads(txt)\n    except json.JSONDecodeError:\n        return None\n\n\ndef ec2_current_region() -> Optional[str]:\n    """""" Returns name of the region  this EC2 instance is running in.\n    """"""\n    cfg = ec2_metadata()\n    if cfg is None:\n        return None\n    return cfg.get(\'region\', None)\n\n\ndef botocore_default_region(session: Optional[Session] = None) -> Optional[str]:\n    """""" Returns default region name as configured on the system.\n    """"""\n    if session is None:\n        session = botocore.session.get_session()\n    return session.get_config_variable(\'region\')\n\n\ndef auto_find_region(session: Optional[Session] = None) -> str:\n    """"""\n    Try to figure out which region name to use\n\n    1. Region as configured for this/default session\n    2. Region this EC2 instance is running in\n    3. None\n    """"""\n    region_name = botocore_default_region(session)\n\n    if region_name is None:\n        region_name = ec2_current_region()\n\n    if region_name is None:\n        raise ValueError(\'Region name is not supplied and default can not be found\')\n\n    return region_name\n\n\ndef get_creds_with_retry(session: Session,\n                         max_tries: int = 10,\n                         sleep: float = 0.1) -> Optional[Credentials]:\n    """""" Attempt to obtain credentials upto `max_tries` times with back off\n    :param session: botocore session, see get_boto_session\n    :param max_tries: number of attempt before failing and returing None\n    :param sleep: number of seconds to sleep after first failure (doubles on every consecutive failure)\n    """"""\n    for i in range(max_tries):\n        if i > 0:\n            time.sleep(sleep)\n            sleep = min(sleep*2, 10)\n\n        creds = session.get_credentials()\n        if creds is not None:\n            return creds\n\n    return None\n\n\ndef mk_boto_session(profile: Optional[str] = None,\n                    creds: Optional[ReadOnlyCredentials] = None,\n                    region_name: Optional[str] = None) -> Session:\n    """""" Get botocore session with correct `region` configured\n\n    :param profile: profile name to lookup\n    :param creds: Override credentials with supplied data\n    :param region_name: default region_name to use if not configured for a given profile\n    """"""\n    session = botocore.session.Session(profile=profile)\n\n    if creds is not None:\n        session.set_credentials(creds.access_key,\n                                creds.secret_key,\n                                creds.token)\n\n    _region = session.get_config_variable(""region"")\n    if _region is None:\n        if region_name is None or region_name == ""auto"":\n            _region = auto_find_region(session)\n        else:\n            _region = region_name\n        session.set_config_variable(""region"", _region)\n\n    return session\n\n\ndef get_aws_settings(profile: Optional[str] = None,\n                     region_name: str = ""auto"",\n                     aws_unsigned: bool = False,\n                     requester_pays: bool = False) -> Tuple[Dict[str, Any], Credentials]:\n    """"""Compute `aws=` parameter for `set_default_rio_config`\n\n    see also `datacube.utils.rio.set_default_rio_config`\n\n    Returns a tuple of:\n      (aws: Dictionary,\n       creds: session credentials from botocore).\n\n    Note that credentials are baked in to `aws` setting dictionary,\n    however since those might be STS credentials they might require refresh\n    hence they are returned from this function separately as well.\n    """"""\n    session = mk_boto_session(profile=profile,\n                              region_name=region_name)\n\n    region_name = session.get_config_variable(""region"")\n\n    if aws_unsigned:\n        return (dict(region_name=region_name,\n                     aws_unsigned=True), None)\n\n    creds = get_creds_with_retry(session)\n    if creds is None:\n        raise ValueError(""Couldn\'t get credentials"")\n\n    cc = creds.get_frozen_credentials()\n\n    return (dict(region_name=region_name,\n                 aws_access_key_id=cc.access_key,\n                 aws_secret_access_key=cc.secret_key,\n                 aws_session_token=cc.token,\n                 requester_pays=requester_pays), creds)\n\n\ndef _s3_cache_key(profile: Optional[str] = None,\n                  creds: Optional[ReadOnlyCredentials] = None,\n                  region_name: Optional[str] = None,\n                  prefix: str = ""s3"") -> str:\n    parts = [prefix,\n             """" if creds is None else creds.access_key,\n             profile or """",\n             region_name or """"]\n    return "":"".join(parts)\n\n\ndef _mk_s3_client(profile: Optional[str] = None,\n                  creds: Optional[ReadOnlyCredentials] = None,\n                  region_name: Optional[str] = None,\n                  session: Optional[Session] = None,\n                  use_ssl: bool = True,\n                  **cfg) -> botocore.client.BaseClient:\n    """""" Construct s3 client with configured region_name.\n\n    :param profile    : profile name to lookup (only used if session is not supplied)\n    :param creds      : Override credentials with supplied data\n    :param region_name: region_name to use, overrides session setting\n    :param session    : botocore session to use\n    :param use_ssl    : Whether to connect via http or https\n\n    **cfg: passed on to botocore.client.Config(..)\n       max_pool_connections\n       connect_timeout\n       read_timeout\n       parameter_validation\n       ...\n    """"""\n    if session is None:\n        session = mk_boto_session(profile=profile,\n                                  creds=creds,\n                                  region_name=region_name)\n\n    extras = {}  # type: Dict[str, Any]\n    if creds is not None:\n        extras.update(aws_access_key_id=creds.access_key,\n                      aws_secret_access_key=creds.secret_key,\n                      aws_session_token=creds.token)\n    if region_name is not None:\n        extras[\'region_name\'] = region_name\n\n    return session.create_client(\'s3\',\n                                 use_ssl=use_ssl,\n                                 **extras,\n                                 config=botocore.client.Config(**cfg))\n\n\ndef s3_client(profile: Optional[str] = None,\n              creds: Optional[ReadOnlyCredentials] = None,\n              region_name: Optional[str] = None,\n              session: Optional[Session] = None,\n              use_ssl: bool = True,\n              cache: Union[bool, str] = False,\n              **cfg) -> botocore.client.BaseClient:\n    """""" Construct s3 client with configured region_name.\n\n    :param profile    : profile name to lookup (only used if session is not supplied)\n    :param creds      : Override credentials with supplied data\n    :param region_name: region_name to use, overrides session setting\n    :param session    : botocore session to use\n    :param use_ssl    : Whether to connect via http or https\n    :param cache      : True -- Store/lookup s3 client in thread local cache\n                        ""purge"" -- delete from cache and return what was there to begin with\n\n    **cfg: passed on to botocore.client.Config(..)\n       max_pool_connections\n       connect_timeout\n       read_timeout\n       parameter_validation\n       ...\n    """"""\n    if not cache:\n        return _mk_s3_client(profile,\n                             creds=creds,\n                             region_name=region_name,\n                             session=session,\n                             use_ssl=use_ssl,\n                             **cfg)\n\n    _cache = thread_local_cache(""__aws_s3_cache"", {})\n\n    key = _s3_cache_key(profile=profile,\n                        region_name=region_name,\n                        creds=creds)\n\n    if cache == ""purge"":\n        return _cache.pop(key, None)\n\n    s3 = _cache.get(key, None)\n\n    if s3 is None:\n        s3 = _mk_s3_client(profile,\n                           creds=creds,\n                           region_name=region_name,\n                           session=session,\n                           use_ssl=use_ssl,\n                           **cfg)\n        _cache[key] = s3\n\n    return s3\n\n\ndef s3_fetch(url: str,\n             s3: MaybeS3 = None,\n             range: Optional[ByteRange] = None,  # pylint: disable=redefined-builtin\n             **kwargs):\n    """""" Read entire or part of object into memory and return as bytes\n\n    :param url: s3://bucket/path/to/object\n    :param s3: pre-configured s3 client, see make_s3_client()\n    :param range: Byte range to read (first_byte, one_past_last_byte), default is whole object\n    """"""\n    if range is not None:\n        try:\n            kwargs[\'Range\'] = s3_fmt_range(range)\n        except Exception:\n            raise ValueError(\'Bad range passed in: \' + str(range))\n\n    s3 = s3 or s3_client()\n    bucket, key = s3_url_parse(url)\n    oo = s3.get_object(Bucket=bucket, Key=key, **kwargs)\n    return oo[\'Body\'].read()\n\n\ndef s3_dump(data: Union[bytes, str, IO],\n            url: str,\n            s3: MaybeS3 = None,\n            **kwargs):\n    """""" Write data to s3 object.\n\n    :param data: bytes to write\n    :param url: s3://bucket/path/to/object\n    :param s3: pre-configured s3 client, see s3_client()\n    **kwargs -- Are passed on to `s3.put_object(..)`\n\n    ContentType\n    ACL\n    """"""\n\n    s3 = s3 or s3_client()\n    bucket, key = s3_url_parse(url)\n\n    r = s3.put_object(Bucket=bucket,\n                      Key=key,\n                      Body=data,\n                      **kwargs)\n    code = r[\'ResponseMetadata\'][\'HTTPStatusCode\']\n    return 200 <= code < 300\n'"
datacube/utils/geometry/__init__.py,0,"b'"""""" Geometric shapes and operations on them\n""""""\n\nfrom ._base import (\n    Coordinate,\n    BoundingBox,\n    CRSError,\n    CRSMismatchError,\n    CRS,\n    MaybeCRS,\n    SomeCRS,\n    CoordList,\n    Geometry,\n    GeoBox,\n    common_crs,\n    bbox_union,\n    bbox_intersection,\n    crs_units_per_degree,\n    geobox_union_conservative,\n    geobox_intersection_conservative,\n    intersects,\n    scaled_down_geobox,\n    point,\n    multipoint,\n    line,\n    multiline,\n    polygon,\n    multipolygon,\n    multigeom,\n    box,\n    sides,\n    polygon_from_transform,\n    unary_union,\n    unary_intersection,\n    lonlat_bounds,\n    projected_lon,\n    clip_lon180,\n    chop_along_antimeridian,\n)\n\nfrom .tools import (\n    is_affine_st,\n    apply_affine,\n    roi_boundary,\n    roi_is_empty,\n    roi_is_full,\n    roi_intersect,\n    roi_shape,\n    roi_normalise,\n    roi_from_points,\n    roi_center,\n    roi_pad,\n    scaled_down_shape,\n    scaled_down_roi,\n    scaled_up_roi,\n    decompose_rws,\n    affine_from_pts,\n    get_scale_at_point,\n    native_pix_transform,\n    compute_reproject_roi,\n    split_translation,\n    compute_axis_overlap,\n    w_,\n)\n\nfrom ._warp import (\n    warp_affine,\n    rio_reproject,\n)\n\n__all__ = [\n    ""Coordinate"",\n    ""BoundingBox"",\n    ""CRSError"",\n    ""CRSMismatchError"",\n    ""CRS"",\n    ""MaybeCRS"",\n    ""SomeCRS"",\n    ""CoordList"",\n    ""Geometry"",\n    ""GeoBox"",\n    ""common_crs"",\n    ""bbox_union"",\n    ""bbox_intersection"",\n    ""crs_units_per_degree"",\n    ""geobox_union_conservative"",\n    ""geobox_intersection_conservative"",\n    ""intersects"",\n    ""point"",\n    ""multipoint"",\n    ""line"",\n    ""multiline"",\n    ""polygon"",\n    ""multipolygon"",\n    ""multigeom"",\n    ""box"",\n    ""sides"",\n    ""polygon_from_transform"",\n    ""unary_union"",\n    ""unary_intersection"",\n    ""lonlat_bounds"",\n    ""projected_lon"",\n    ""clip_lon180"",\n    ""chop_along_antimeridian"",\n    ""is_affine_st"",\n    ""apply_affine"",\n    ""compute_axis_overlap"",\n    ""roi_boundary"",\n    ""roi_is_empty"",\n    ""roi_is_full"",\n    ""roi_intersect"",\n    ""roi_shape"",\n    ""roi_normalise"",\n    ""roi_from_points"",\n    ""roi_center"",\n    ""roi_pad"",\n    ""scaled_down_geobox"",\n    ""scaled_down_shape"",\n    ""scaled_down_roi"",\n    ""scaled_up_roi"",\n    ""decompose_rws"",\n    ""affine_from_pts"",\n    ""get_scale_at_point"",\n    ""native_pix_transform"",\n    ""compute_reproject_roi"",\n    ""split_translation"",\n    ""warp_affine"",\n    ""rio_reproject"",\n    ""w_"",\n]\n'"
datacube/utils/geometry/_base.py,0,"b'import functools\nimport itertools\nimport math\nimport array\nimport warnings\nfrom collections import namedtuple, OrderedDict\nfrom typing import Tuple, Iterable, List, Union, Optional, Any, Callable, Hashable, Dict, Iterator\nfrom collections.abc import Sequence\nfrom distutils.version import LooseVersion\n\nimport cachetools\nimport numpy\nimport xarray as xr\nfrom affine import Affine\nimport rasterio\nfrom shapely import geometry, ops\nfrom shapely.geometry import base\nfrom pyproj import CRS as _CRS\nfrom pyproj.enums import WktVersion\nfrom pyproj.transformer import Transformer\nfrom pyproj.exceptions import CRSError\n\nfrom .tools import roi_normalise, roi_shape, is_affine_st\nfrom ..math import is_almost_int\n\nCoordinate = namedtuple(\'Coordinate\', (\'values\', \'units\', \'resolution\'))\n_BoundingBox = namedtuple(\'BoundingBox\', (\'left\', \'bottom\', \'right\', \'top\'))\nSomeCRS = Union[str, \'CRS\', _CRS, Dict[str, Any]]\nMaybeCRS = Optional[SomeCRS]\nCoordList = List[Tuple[float, float]]\n\n# pylint: disable=too-many-lines\n\n\nclass BoundingBox(_BoundingBox):\n    """"""Bounding box, defining extent in cartesian coordinates.\n    """"""\n\n    def buffered(self, ybuff: float, xbuff: float) -> \'BoundingBox\':\n        """"""\n        Return a new BoundingBox, buffered in the x and y dimensions.\n\n        :param ybuff: Y dimension buffering amount\n        :param xbuff: X dimension buffering amount\n        :return: new BoundingBox\n        """"""\n        return BoundingBox(left=self.left - xbuff, right=self.right + xbuff,\n                           top=self.top + ybuff, bottom=self.bottom - ybuff)\n\n    @property\n    def span_x(self) -> float:\n        return self.right - self.left\n\n    @property\n    def span_y(self) -> float:\n        return self.top - self.bottom\n\n    @property\n    def width(self) -> int:\n        return int(self.right - self.left)\n\n    @property\n    def height(self) -> int:\n        return int(self.top - self.bottom)\n\n    @property\n    def range_x(self) -> Tuple[float, float]:\n        return (self.left, self.right)\n\n    @property\n    def range_y(self) -> Tuple[float, float]:\n        return (self.bottom, self.top)\n\n    @property\n    def points(self) -> CoordList:\n        """"""Extract four corners of the bounding box\n        """"""\n        x0, y0, x1, y1 = self\n        return list(itertools.product((x0, x1), (y0, y1)))\n\n    def transform(self, transform: Affine) -> \'BoundingBox\':\n        """"""Transform bounding box through a linear transform\n\n           Apply linear transform on 4 points of the bounding box and compute\n           bounding box of these four points.\n        """"""\n        pts = [transform*pt for pt in self.points]\n        xx = [x for x, _ in pts]\n        yy = [y for _, y in pts]\n        return BoundingBox(min(xx), min(yy), max(xx), max(yy))\n\n    @staticmethod\n    def from_xy(x: Tuple[float, float],\n                y: Tuple[float, float]) -> \'BoundingBox\':\n        """""" BoundingBox from x and y ranges\n\n        :param x: (left, right)\n        :param y: (bottom, top)\n        """"""\n        x1, x2 = sorted(x)\n        y1, y2 = sorted(y)\n        return BoundingBox(x1, y1, x2, y2)\n\n    @staticmethod\n    def from_points(p1: Tuple[float, float],\n                    p2: Tuple[float, float]) -> \'BoundingBox\':\n        """""" BoundingBox from 2 points\n        :param p1: (x, y)\n        :param p2: (x, y)\n        """"""\n        return BoundingBox.from_xy((p1[0], p2[0]),\n                                   (p1[1], p2[1]))\n\n\n@cachetools.cached({})\ndef _make_crs(crs_str: str) -> Tuple[_CRS, Optional[int]]:\n    crs = _CRS.from_user_input(crs_str)\n    return (crs, crs.to_epsg())\n\n\ndef _make_crs_transform_key(from_crs, to_crs, always_xy):\n    return (id(from_crs), id(to_crs), always_xy)\n\n\n@cachetools.cached({}, key=_make_crs_transform_key)\ndef _make_crs_transform(from_crs, to_crs, always_xy):\n    return Transformer.from_crs(from_crs, to_crs, always_xy=always_xy).transform\n\n\ndef _guess_crs_str(crs_spec: Any) -> Optional[str]:\n    """"""\n    Returns a string representation of the crs spec.\n    Returns `None` if it does not understand the spec.\n    """"""\n    if isinstance(crs_spec, str):\n        return crs_spec\n    if isinstance(crs_spec, dict):\n        crs_spec = _CRS.from_dict(crs_spec)\n\n    if hasattr(crs_spec, \'to_epsg\'):\n        epsg = crs_spec.to_epsg()\n        if epsg is not None:\n            return \'EPSG:{}\'.format(crs_spec.to_epsg())\n    if hasattr(crs_spec, \'to_wkt\'):\n        return crs_spec.to_wkt()\n    return None\n\n\nclass CRS:\n    """"""\n    Wrapper around `pyproj.CRS` for backwards compatibility.\n    """"""\n    DEFAULT_WKT_VERSION = (WktVersion.WKT1_GDAL if LooseVersion(rasterio.__gdal_version__) < LooseVersion(""3.0.0"")\n                           else WktVersion.WKT2_2019)\n\n    __slots__ = (\'_crs\', \'_epsg\', \'_str\')\n\n    def __init__(self, crs_str: Any):\n        """"""\n        :param crs_str: string representation of a CRS, often an EPSG code like \'EPSG:4326\'\n        :raises: `pyproj.exceptions.CRSError`\n        """"""\n        crs_str = _guess_crs_str(crs_str)\n        if crs_str is None:\n            raise CRSError(""Expect string or any object with `.to_epsg()` or `.to_wkt()` method"")\n\n        _crs, _epsg = _make_crs(crs_str)\n\n        self._crs = _crs\n        self._epsg = _epsg\n        self._str = crs_str\n\n    def __getstate__(self):\n        return {\'crs_str\': self._str}\n\n    def __setstate__(self, state):\n        self.__init__(state[\'crs_str\'])\n\n    def to_wkt(self, pretty: bool = False, version: Optional[WktVersion] = None) -> str:\n        """"""\n        WKT representation of the CRS\n        """"""\n        if version is None:\n            version = self.DEFAULT_WKT_VERSION\n\n        return self._crs.to_wkt(pretty=pretty, version=version)\n\n    @property\n    def wkt(self) -> str:\n        return self.to_wkt(version=""WKT1_GDAL"")\n\n    def to_epsg(self) -> Optional[int]:\n        """"""\n        EPSG Code of the CRS or None\n        """"""\n        return self._epsg\n\n    @property\n    def epsg(self) -> Optional[int]:\n        return self._epsg\n\n    @property\n    def semi_major_axis(self):\n        return self._crs.ellipsoid.semi_major_metre\n\n    @property\n    def semi_minor_axis(self):\n        return self._crs.ellipsoid.semi_minor_metre\n\n    @property\n    def inverse_flattening(self):\n        return self._crs.ellipsoid.inverse_flattening\n\n    @property\n    def geographic(self) -> bool:\n        return self._crs.is_geographic\n\n    @property\n    def projected(self) -> bool:\n        return self._crs.is_projected\n\n    @property\n    def dimensions(self) -> Tuple[str, str]:\n        """"""\n        List of dimension names of the CRS.\n        The ordering of the names is intended to reflect the `numpy` array axis order of the loaded raster.\n        """"""\n        if self.geographic:\n            return \'latitude\', \'longitude\'\n\n        if self.projected:\n            return \'y\', \'x\'\n\n        raise ValueError(\'Neither projected nor geographic\')  # pragma: no cover\n\n    @property\n    def units(self) -> Tuple[str, str]:\n        """"""\n        List of dimension units of the CRS.\n        The ordering of the units is intended to reflect the `numpy` array axis order of the loaded raster.\n        """"""\n        if self.geographic:\n            return \'degrees_north\', \'degrees_east\'\n\n        if self.projected:\n            x, y = self._crs.axis_info\n            return x.unit_name, y.unit_name\n\n        raise ValueError(\'Neither projected nor geographic\')  # pragma: no cover\n\n    def __str__(self) -> str:\n        return self._str\n\n    def __hash__(self) -> int:\n        return hash(self.to_wkt())\n\n    def __repr__(self) -> str:\n        return ""CRS(\'%s\')"" % self._str\n\n    def __eq__(self, other: SomeCRS) -> bool:\n        if not isinstance(other, CRS):\n            try:\n                other = CRS(other)\n            except Exception:\n                return False\n\n        if self._crs is other._crs:\n            return True\n\n        if self.epsg is not None and other.epsg is not None:\n            return self.epsg == other.epsg\n\n        return self._crs == other._crs\n\n    def __ne__(self, other) -> bool:\n        return not (self == other)\n\n    @property\n    def proj(self) -> _CRS:\n        """""" Access proj.CRS object that this wraps\n        """"""\n        return self._crs\n\n    @property\n    def valid_region(self) -> Optional[\'Geometry\']:\n        """""" Return valid region of this CRS.\n\n            Bounding box in Lon/Lat as a 4 point Polygon in EPSG:4326.\n            None if not defined\n        """"""\n        region = self._crs.area_of_use\n        if region is None:\n            return None\n        x1, y1, x2, y2 = region.bounds\n        return box(x1, y1, x2, y2, \'EPSG:4326\')\n\n    @property\n    def crs_str(self) -> str:\n        """""" DEPRECATED\n        """"""\n        warnings.warn(""Please use `str(crs)` instead of `crs.crs_str`"", category=DeprecationWarning)\n        return self._str\n\n    def transformer_to_crs(self, other: \'CRS\', always_xy=True) -> Callable[[Any, Any], Tuple[Any, Any]]:\n        """"""\n        Returns a function that maps x, y -> x\', y\' where x, y are coordinates in\n        this stored either as scalars or ndarray objects and x\', y\' are the same\n        points in the `other` CRS.\n        """"""\n        transform = _make_crs_transform(self._crs, other._crs, always_xy=always_xy)\n\n        def result(x, y):\n            rx, ry = transform(x, y)\n\n            if not isinstance(rx, numpy.ndarray) or not isinstance(ry, numpy.ndarray):\n                return (rx, ry)\n\n            missing = numpy.isnan(rx) | numpy.isnan(ry)\n            rx[missing] = numpy.nan\n            ry[missing] = numpy.nan\n            return (rx, ry)\n\n        return result\n\n\nclass CRSMismatchError(ValueError):\n    pass\n\n\ndef _norm_crs(crs: MaybeCRS) -> Optional[CRS]:\n    if isinstance(crs, CRS):\n        return crs\n    if crs is None:\n        return None\n    return CRS(crs)\n\n\ndef _norm_crs_or_error(crs: MaybeCRS) -> CRS:\n    if isinstance(crs, CRS):\n        return crs\n    if crs is None:\n        raise ValueError(""Expect valid CRS"")\n    return CRS(crs)\n\n\ndef wrap_shapely(method):\n    """"""\n    Takes a method that expects shapely geometry arguments\n    and converts it to a method that operates on `Geometry`\n    objects that carry their CRSs.\n    """"""\n    @functools.wraps(method, assigned=(\'__doc__\', ))\n    def wrapped(*args):\n        first = args[0]\n        for arg in args[1:]:\n            if first.crs != arg.crs:\n                raise CRSMismatchError((first.crs, arg.crs))\n\n        result = method(*[arg.geom for arg in args])\n        if isinstance(result, base.BaseGeometry):\n            return Geometry(result, first.crs)\n        return result\n    return wrapped\n\n\ndef force_2d(geojson: Dict[str, Any]) -> Dict[str, Any]:\n    assert \'type\' in geojson\n    assert \'coordinates\' in geojson\n\n    def is_scalar(x):\n        return isinstance(x, (int, float))\n\n    def go(x):\n        if is_scalar(x):\n            return x\n\n        if isinstance(x, Sequence):\n            if all(is_scalar(y) for y in x):\n                return x[:2]\n            return [go(y) for y in x]\n\n        raise ValueError(\'invalid coordinate {}\'.format(x))\n\n    return {\'type\': geojson[\'type\'],\n            \'coordinates\': go(geojson[\'coordinates\'])}\n\n\ndef densify(coords: CoordList, resolution: float) -> CoordList:\n    """"""\n    Adds points so they are at most `resolution` units apart.\n    """"""\n    d2 = resolution**2\n\n    def short_enough(p1, p2):\n        return (p1[0]**2 + p2[0]**2) < d2\n\n    new_coords = [coords[0]]\n    for p1, p2 in zip(coords[:-1], coords[1:]):\n        if not short_enough(p1, p2):\n            segment = geometry.LineString([p1, p2])\n            segment_length = segment.length\n            d = resolution\n            while d < segment_length:\n                pt, = segment.interpolate(d).coords\n                new_coords.append(pt)\n                d += resolution\n\n        new_coords.append(p2)\n\n    return new_coords\n\n\ndef _clone_shapely_geom(geom: base.BaseGeometry) -> base.BaseGeometry:\n    return type(geom)(geom)\n\n\nclass Geometry:\n    """"""\n    2D Geometry with CRS\n\n    Instantiate with a GeoJSON structure\n\n    If 3D coordinates are supplied, they are converted to 2D by dropping the Z points.\n    """"""\n\n    def __init__(self,\n                 geom: Union[base.BaseGeometry, Dict[str, Any], \'Geometry\'],\n                 crs: MaybeCRS = None):\n        if isinstance(geom, Geometry):\n            assert crs is None\n            self.crs: Optional[CRS] = geom.crs\n            self.geom: base.BaseGeometry = _clone_shapely_geom(geom.geom)\n            return\n\n        crs = _norm_crs(crs)\n        self.crs = crs\n        if isinstance(geom, base.BaseGeometry):\n            self.geom = geom\n        elif isinstance(geom, dict):\n            self.geom = geometry.shape(force_2d(geom))\n        else:\n            raise ValueError(f\'Unexpected type {type(geom)}\')\n\n    def clone(self) -> \'Geometry\':\n        return Geometry(self)\n\n    @wrap_shapely\n    def contains(self, other: \'Geometry\') -> bool:\n        return self.contains(other)\n\n    @wrap_shapely\n    def crosses(self, other: \'Geometry\') -> bool:\n        return self.crosses(other)\n\n    @wrap_shapely\n    def disjoint(self, other: \'Geometry\') -> bool:\n        return self.disjoint(other)\n\n    @wrap_shapely\n    def intersects(self, other: \'Geometry\') -> bool:\n        return self.intersects(other)\n\n    @wrap_shapely\n    def touches(self, other: \'Geometry\') -> bool:\n        return self.touches(other)\n\n    @wrap_shapely\n    def within(self, other: \'Geometry\') -> bool:\n        return self.within(other)\n\n    @wrap_shapely\n    def overlaps(self, other: \'Geometry\') -> bool:\n        return self.overlaps(other)\n\n    @wrap_shapely\n    def difference(self, other: \'Geometry\') -> \'Geometry\':\n        return self.difference(other)\n\n    @wrap_shapely\n    def intersection(self, other: \'Geometry\') -> \'Geometry\':\n        return self.intersection(other)\n\n    @wrap_shapely\n    def symmetric_difference(self, other: \'Geometry\') -> \'Geometry\':\n        return self.symmetric_difference(other)\n\n    @wrap_shapely\n    def union(self, other: \'Geometry\') -> \'Geometry\':\n        return self.union(other)\n\n    @wrap_shapely\n    def __and__(self, other: \'Geometry\') -> \'Geometry\':\n        return self.__and__(other)\n\n    @wrap_shapely\n    def __or__(self, other: \'Geometry\') -> \'Geometry\':\n        return self.__or__(other)\n\n    @wrap_shapely\n    def __xor__(self, other: \'Geometry\') -> \'Geometry\':\n        return self.__xor__(other)\n\n    @wrap_shapely\n    def __sub__(self, other: \'Geometry\') -> \'Geometry\':\n        return self.__sub__(other)\n\n    def svg(self) -> str:\n        return self.geom.svg()\n\n    def _repr_svg_(self) -> str:\n        return self.geom._repr_svg_()\n\n    @property\n    def type(self) -> str:\n        return self.geom.type\n\n    @property\n    def is_empty(self) -> bool:\n        return self.geom.is_empty\n\n    @property\n    def is_valid(self) -> bool:\n        return self.geom.is_valid\n\n    @property\n    def boundary(self) -> \'Geometry\':\n        return Geometry(self.geom.boundary, self.crs)\n\n    @property\n    def exterior(self) -> \'Geometry\':\n        return Geometry(self.geom.exterior, self.crs)\n\n    @property\n    def interiors(self) -> List[\'Geometry\']:\n        return [Geometry(g, self.crs) for g in self.geom.interiors]\n\n    @property\n    def centroid(self) -> \'Geometry\':\n        return Geometry(self.geom.centroid, self.crs)\n\n    @property\n    def coords(self) -> CoordList:\n        return list(self.geom.coords)\n\n    @property\n    def points(self) -> CoordList:\n        return self.coords\n\n    @property\n    def length(self) -> float:\n        return self.geom.length\n\n    @property\n    def area(self) -> float:\n        return self.geom.area\n\n    @property\n    def xy(self) -> Tuple[array.array, array.array]:\n        return self.geom.xy\n\n    @property\n    def convex_hull(self) -> \'Geometry\':\n        return Geometry(self.geom.convex_hull, self.crs)\n\n    @property\n    def envelope(self) -> \'Geometry\':\n        return Geometry(self.geom.envelope, self.crs)\n\n    @property\n    def boundingbox(self) -> BoundingBox:\n        minx, miny, maxx, maxy = self.geom.bounds\n        return BoundingBox(left=minx, right=maxx, bottom=miny, top=maxy)\n\n    @property\n    def wkt(self) -> str:\n        return self.geom.wkt\n\n    @property\n    def __array_interface__(self):\n        return self.geom.__array_interface__\n\n    @property\n    def __geo_interface__(self):\n        return self.geom.__geo_interface__\n\n    @property\n    def json(self):\n        return self.__geo_interface__\n\n    def segmented(self, resolution: float) -> \'Geometry\':\n        """"""\n        Possibly add more points to the geometry so that no edge is longer than `resolution`.\n        """"""\n\n        def segmentize_shapely(geom: base.BaseGeometry) -> base.BaseGeometry:\n            if geom.type in [\'Point\', \'MultiPoint\']:\n                return type(geom)(geom)  # clone without changes\n\n            if geom.type in [\'GeometryCollection\', \'MultiPolygon\', \'MultiLineString\']:\n                return type(geom)([segmentize_shapely(g) for g in geom])\n\n            if geom.type in [\'LineString\', \'LinearRing\']:\n                return type(geom)(densify(list(geom.coords), resolution))\n\n            if geom.type == \'Polygon\':\n                return geometry.Polygon(densify(list(geom.exterior.coords), resolution),\n                                        [densify(list(i.coords), resolution) for i in geom.interiors])\n\n            raise ValueError(\'unknown geometry type {}\'.format(geom.type))  # pragma: no cover\n\n        return Geometry(segmentize_shapely(self.geom), self.crs)\n\n    def interpolate(self, distance: float) -> \'Geometry\':\n        """"""\n        Returns a point distance units along the line.\n        Raises TypeError if geometry doesn\'t support this operation.\n        """"""\n        return Geometry(self.geom.interpolate(distance), self.crs)\n\n    def buffer(self, distance: float, resolution: float = 30) -> \'Geometry\':\n        return Geometry(self.geom.buffer(distance, resolution=resolution), self.crs)\n\n    def simplify(self, tolerance: float, preserve_topology: bool = True) -> \'Geometry\':\n        return Geometry(self.geom.simplify(tolerance, preserve_topology=preserve_topology), self.crs)\n\n    def transform(self, func) -> \'Geometry\':\n        """"""Applies func to all coordinates of Geometry and returns a new Geometry\n           of the same type and in the same projection from the transformed coordinates.\n\n           func maps x, y, and optionally z to output xp, yp, zp. The input\n           parameters may be iterable types like lists or arrays or single values.\n           The output shall be of the same type: scalars in, scalars out; lists\n           in, lists out.\n        """"""\n        return Geometry(ops.transform(func, self.geom), self.crs)\n\n    def _to_crs(self, crs: CRS) -> \'Geometry\':\n        assert self.crs is not None\n        return Geometry(ops.transform(self.crs.transformer_to_crs(crs),\n                                      self.geom), crs)\n\n    def to_crs(self, crs: SomeCRS,\n               resolution: Optional[float] = None,\n               wrapdateline: bool = False) -> \'Geometry\':\n        """"""\n        Convert geometry to a different Coordinate Reference System\n\n        :param crs: CRS to convert to\n\n        :param resolution: Subdivide the geometry such it has no segment longer then the given distance.\n                           Defaults to 1 degree for geographic and 100km for projected. To disable\n                           completely use Infinity float(\'+inf\')\n\n        :param wrapdateline: Attempt to gracefully handle geometry that intersects the dateline\n                                  when converting to geographic projections.\n                                  Currently only works in few specific cases (source CRS is smooth over the dateline).\n        """"""\n        crs = _norm_crs_or_error(crs)\n        if self.crs == crs:\n            return self\n\n        if self.crs is None:\n            raise ValueError(""Cannot project geometries without CRS"")\n\n        if resolution is None:\n            resolution = 1 if self.crs.geographic else 100000\n\n        geom = self.segmented(resolution) if math.isfinite(resolution) else self\n\n        eps = 1e-4\n        if wrapdateline and crs.geographic:\n            # TODO: derive precision from resolution by converting to degrees\n            precision = 0.1\n            chopped = chop_along_antimeridian(geom, precision)\n            chopped_lonlat = chopped._to_crs(crs)\n            return clip_lon180(chopped_lonlat, eps)\n\n        return geom._to_crs(crs)\n\n    def split(self, splitter: \'Geometry\') -> Iterable[\'Geometry\']:\n        """""" shapely.ops.split\n        """"""\n        if splitter.crs != self.crs:\n            raise CRSMismatchError(self.crs, splitter.crs)\n\n        for g in ops.split(self.geom, splitter.geom):\n            yield Geometry(g, self.crs)\n\n    def __iter__(self) -> Iterator[\'Geometry\']:\n        for geom in self.geom:\n            yield Geometry(geom, self.crs)\n\n    def __nonzero__(self) -> bool:\n        return not self.is_empty\n\n    def __bool__(self) -> bool:\n        return not self.is_empty\n\n    def __eq__(self, other: Any) -> bool:\n        return (hasattr(other, \'crs\') and self.crs == other.crs and\n                hasattr(other, \'geom\') and self.geom == other.geom)\n\n    def __str__(self):\n        return \'Geometry(%s, %r)\' % (self.__geo_interface__, self.crs)\n\n    def __repr__(self):\n        return \'Geometry(%s, %s)\' % (self.geom, self.crs)\n\n    # Implement pickle/unpickle\n    # It does work without these two methods, but gdal/ogr prints \'ERROR 1: Empty geometries cannot be constructed\'\n    # when unpickling, which is quite unpleasant.\n    def __getstate__(self):\n        return {\'geom\': self.json, \'crs\': self.crs}\n\n    def __setstate__(self, state):\n        self.__init__(**state)\n\n\ndef common_crs(geoms: Iterable[Geometry]) -> Optional[CRS]:\n    """""" Return CRS common across geometries, or raise CRSMismatchError\n    """"""\n    all_crs = [g.crs for g in geoms]\n    if len(all_crs) == 0:\n        return None\n    ref = all_crs[0]\n    for crs in all_crs[1:]:\n        if crs != ref:\n            raise CRSMismatchError()\n    return ref\n\n\ndef projected_lon(crs: MaybeCRS,\n                  lon: float,\n                  lat: Tuple[float, float] = (-90.0, 90.0),\n                  step: float = 1.0) -> Geometry:\n    """""" Project vertical line along some longitude into given CRS.\n    """"""\n    crs = _norm_crs_or_error(crs)\n    yy = numpy.arange(lat[0], lat[1], step, dtype=\'float32\')\n    xx = numpy.full_like(yy, lon)\n    tr = CRS(\'EPSG:4326\').transformer_to_crs(crs)\n    xx_, yy_ = tr(xx, yy)\n    pts = [(float(x), float(y))\n           for x, y in zip(xx_, yy_)\n           if math.isfinite(x) and math.isfinite(y)]\n    return line(pts, crs)\n\n\ndef clip_lon180(geom: Geometry, tol=1e-6) -> Geometry:\n    """"""For every point in the ``lon=180|-180`` band clip to either 180 or -180\n        180|-180 is decided based on where the majority of other points lie.\n\n        NOTE: this will only do ""right thing"" for chopped geometries,\n              expectation is that all the points are to one side of lon=180\n              line, or in the the capture zone of lon=(+/-)180\n    """"""\n    thresh = 180 - tol\n\n    def _clip_180(xx, clip):\n        return [x if abs(x) < thresh else clip for x in xx]\n\n    def _pick_clip(xx: List[float]):\n        cc = 0\n        for x in xx:\n            if abs(x) < thresh:\n                cc += (1 if x > 0 else -1)\n        return 180 if cc >= 0 else -180\n\n    def transformer(xx, yy):\n        clip = _pick_clip(xx)\n        return _clip_180(xx, clip), yy\n\n    if geom.type.startswith(\'Multi\'):\n        return multigeom(g.transform(transformer) for g in geom)\n\n    return geom.transform(transformer)\n\n\ndef chop_along_antimeridian(geom: Geometry,\n                            precision: float = 0.1) -> Geometry:\n    """"""\n    Chop a geometry along the antimeridian\n\n    :param geom: Geometry to maybe partition\n    :param precision: in degrees\n    :returns: either the same geometry if it doesn\'t intersect the antimeridian,\n              or multi-geometry that has been split.\n    """"""\n    if geom.crs is None:\n        raise ValueError(""Expect geometry with CRS defined"")\n\n    l180 = projected_lon(geom.crs, 180, step=precision)\n    if geom.intersects(l180):\n        return multigeom(geom.split(l180))\n\n    return geom\n\n\n###########################################\n# Helper constructor functions a la shapely\n###########################################\n\n\ndef point(x: float, y: float, crs: MaybeCRS) -> Geometry:\n    """"""\n    Create a 2D Point\n\n    >>> point(10, 10, crs=None)\n    Geometry(POINT (10 10), None)\n    """"""\n    return Geometry({\'type\': \'Point\', \'coordinates\': [float(x), float(y)]}, crs=crs)\n\n\ndef multipoint(coords: CoordList, crs: MaybeCRS) -> Geometry:\n    """"""\n    Create a 2D MultiPoint Geometry\n\n    >>> multipoint([(10, 10), (20, 20)], None)\n    Geometry(MULTIPOINT (10 10, 20 20), None)\n\n    :param coords: list of x,y coordinate tuples\n    """"""\n    return Geometry({\'type\': \'MultiPoint\', \'coordinates\': coords}, crs=crs)\n\n\ndef line(coords: CoordList, crs: MaybeCRS) -> Geometry:\n    """"""\n    Create a 2D LineString (Connected set of lines)\n\n    >>> line([(10, 10), (20, 20), (30, 40)], None)\n    Geometry(LINESTRING (10 10, 20 20, 30 40), None)\n\n    :param coords: list of x,y coordinate tuples\n    """"""\n    return Geometry({\'type\': \'LineString\', \'coordinates\': coords}, crs=crs)\n\n\ndef multiline(coords: List[CoordList], crs: MaybeCRS) -> Geometry:\n    """"""\n    Create a 2D MultiLineString (Multiple disconnected sets of lines)\n\n    >>> multiline([[(10, 10), (20, 20), (30, 40)], [(50, 60), (70, 80), (90, 99)]], None)\n    Geometry(MULTILINESTRING ((10 10, 20 20, 30 40), (50 60, 70 80, 90 99)), None)\n\n    :param coords: list of lists of x,y coordinate tuples\n    """"""\n    return Geometry({\'type\': \'MultiLineString\', \'coordinates\': coords}, crs=crs)\n\n\ndef polygon(outer, crs: MaybeCRS, *inners) -> Geometry:\n    """"""\n    Create a 2D Polygon\n\n    >>> polygon([(10, 10), (20, 20), (20, 10), (10, 10)], None)\n    Geometry(POLYGON ((10 10, 20 20, 20 10, 10 10)), None)\n\n    :param coords: list of 2d x,y coordinate tuples\n    """"""\n    return Geometry({\'type\': \'Polygon\', \'coordinates\': (outer, )+inners}, crs=crs)\n\n\ndef multipolygon(coords: List[List[CoordList]], crs: MaybeCRS) -> Geometry:\n    """"""\n    Create a 2D MultiPolygon\n\n    >>> multipolygon([[[(10, 10), (20, 20), (20, 10), (10, 10)]], [[(40, 10), (50, 20), (50, 10), (40, 10)]]], None)\n    Geometry(MULTIPOLYGON (((10 10, 20 20, 20 10, 10 10)), ((40 10, 50 20, 50 10, 40 10))), None)\n\n    :param coords: list of lists of x,y coordinate tuples\n    """"""\n    return Geometry({\'type\': \'MultiPolygon\', \'coordinates\': coords}, crs=crs)\n\n\ndef box(left: float, bottom: float, right: float, top: float, crs: MaybeCRS) -> Geometry:\n    """"""\n    Create a 2D Box (Polygon)\n\n    >>> box(10, 10, 20, 20, None)\n    Geometry(POLYGON ((10 10, 10 20, 20 20, 20 10, 10 10)), None)\n    """"""\n    points = [(left, bottom), (left, top), (right, top), (right, bottom), (left, bottom)]\n    return polygon(points, crs=crs)\n\n\ndef polygon_from_transform(width: float, height: float, transform: Affine, crs: MaybeCRS) -> Geometry:\n    """"""\n    Create a 2D Polygon from an affine transform\n\n    :param width:\n    :param height:\n    :param transform:\n    :param crs: CRS\n    """"""\n    points = [(0, 0), (0, height), (width, height), (width, 0), (0, 0)]\n    transform.itransform(points)\n    return polygon(points, crs=crs)\n\n\ndef sides(poly: Geometry) -> Iterable[Geometry]:\n    """""" Returns a sequence of Geometry[Line] objects.\n\n        One for each side of the exterior ring of the input polygon.\n    """"""\n    XY = poly.exterior.points\n    crs = poly.crs\n    for p1, p2 in zip(XY[:-1], XY[1:]):\n        yield line([p1, p2], crs)\n\n\ndef multigeom(geoms: Iterable[Geometry]) -> Geometry:\n    """""" Construct Multi{Polygon|LineString|Point}\n    """"""\n    geoms = [g for g in geoms]  # force into list\n    src_type = {g.type for g in geoms}\n    if len(src_type) > 1:\n        raise ValueError(""All Geometries must be of the same type"")\n\n    crs = common_crs(geoms)  # will raise if some differ\n    raw_geoms = [g.geom for g in geoms]\n    src_type = src_type.pop()\n    if src_type == \'Polygon\':\n        return Geometry(geometry.MultiPolygon(raw_geoms), crs)\n    elif src_type == \'Point\':\n        return Geometry(geometry.MultiPoint(raw_geoms), crs)\n    elif src_type == \'LineString\':\n        return Geometry(geometry.MultiLineString(raw_geoms), crs)\n\n    raise ValueError(""Only understand Polygon|LineString|Point"")\n\n\n###########################################\n# Multi-geometry operations\n###########################################\n\n\ndef unary_union(geoms: Iterable[Geometry]) -> Optional[Geometry]:\n    """"""\n    compute union of multiple (multi)polygons efficiently\n    """"""\n    geoms = list(geoms)\n    if len(geoms) == 0:\n        return None\n\n    first = geoms[0]\n    crs = first.crs\n    for g in geoms[1:]:\n        if crs != g.crs:\n            raise CRSMismatchError((crs, g.crs))\n\n    return Geometry(ops.unary_union([g.geom for g in geoms]), crs)\n\n\ndef unary_intersection(geoms: Iterable[Geometry]) -> Geometry:\n    """"""\n    compute intersection of multiple (multi)polygons\n    """"""\n    return functools.reduce(Geometry.intersection, geoms)\n\n\ndef _align_pix(left: float, right: float, res: float, off: float) -> Tuple[float, int]:\n    if res < 0:\n        res = -res\n        val = math.ceil((right - off) / res) * res + off\n        width = max(1, int(math.ceil((val - left - 0.1 * res) / res)))\n    else:\n        val = math.floor((left - off) / res) * res + off\n        width = max(1, int(math.ceil((right - val - 0.1 * res) / res)))\n    return val, width\n\n\nclass GeoBox:\n    """"""\n    Defines the location and resolution of a rectangular grid of data,\n    including it\'s :py:class:`CRS`.\n\n    :param crs: Coordinate Reference System\n    :param affine: Affine transformation defining the location of the geobox\n    """"""\n\n    def __init__(self, width: int, height: int, affine: Affine, crs: MaybeCRS):\n        assert is_affine_st(affine), ""Only axis-aligned geoboxes are currently supported""\n        self.width = width\n        self.height = height\n        self.affine = affine\n        self.extent = polygon_from_transform(width, height, affine, crs=crs)\n\n    @classmethod\n    def from_geopolygon(cls,\n                        geopolygon: Geometry,\n                        resolution: Tuple[float, float],\n                        crs: MaybeCRS = None,\n                        align: Optional[Tuple[float, float]] = None) -> \'GeoBox\':\n        """"""\n        :param resolution: (y_resolution, x_resolution)\n        :param crs: CRS to use, if different from the geopolygon\n        :param align: Align geobox such that point \'align\' lies on the pixel boundary.\n        """"""\n        align = align or (0.0, 0.0)\n        assert 0.0 <= align[1] <= abs(resolution[1]), ""X align must be in [0, abs(x_resolution)] range""\n        assert 0.0 <= align[0] <= abs(resolution[0]), ""Y align must be in [0, abs(y_resolution)] range""\n\n        if crs is None:\n            crs = geopolygon.crs\n        else:\n            geopolygon = geopolygon.to_crs(crs)\n\n        bounding_box = geopolygon.boundingbox\n        offx, width = _align_pix(bounding_box.left, bounding_box.right, resolution[1], align[1])\n        offy, height = _align_pix(bounding_box.bottom, bounding_box.top, resolution[0], align[0])\n        affine = (Affine.translation(offx, offy) * Affine.scale(resolution[1], resolution[0]))\n        return GeoBox(crs=crs, affine=affine, width=width, height=height)\n\n    def buffered(self, ybuff, xbuff) -> \'GeoBox\':\n        """"""\n        Produce a tile buffered by ybuff, xbuff (in CRS units)\n        """"""\n        by, bx = (_round_to_res(buf, res) for buf, res in zip((ybuff, xbuff), self.resolution))\n        affine = self.affine * Affine.translation(-bx, -by)\n\n        return GeoBox(width=self.width + 2*bx,\n                      height=self.height + 2*by,\n                      affine=affine,\n                      crs=self.crs)\n\n    def __getitem__(self, roi) -> \'GeoBox\':\n        if isinstance(roi, int):\n            roi = (slice(roi, roi+1), slice(None, None))\n\n        if isinstance(roi, slice):\n            roi = (roi, slice(None, None))\n\n        if len(roi) > 2:\n            raise ValueError(\'Expect 2d slice\')\n\n        if not all(s.step is None or s.step == 1 for s in roi):\n            raise NotImplementedError(\'scaling not implemented, yet\')\n\n        roi = roi_normalise(roi, self.shape)\n        ty, tx = [s.start for s in roi]\n        h, w = roi_shape(roi)\n\n        affine = self.affine * Affine.translation(tx, ty)\n\n        return GeoBox(width=w, height=h, affine=affine, crs=self.crs)\n\n    def __or__(self, other) -> \'GeoBox\':\n        """""" A geobox that encompasses both self and other. """"""\n        return geobox_union_conservative([self, other])\n\n    def __and__(self, other) -> \'GeoBox\':\n        """""" A geobox that is contained in both self and other. """"""\n        return geobox_intersection_conservative([self, other])\n\n    def is_empty(self) -> bool:\n        return self.width == 0 or self.height == 0\n\n    def __bool__(self) -> bool:\n        return not self.is_empty()\n\n    @property\n    def transform(self) -> Affine:\n        return self.affine\n\n    @property\n    def shape(self) -> Tuple[int, int]:\n        return self.height, self.width\n\n    @property\n    def crs(self) -> Optional[CRS]:\n        return self.extent.crs\n\n    @property\n    def dimensions(self) -> Tuple[str, str]:\n        """"""\n        List of dimension names of the GeoBox\n        """"""\n        crs = self.crs\n        if crs is None:\n            return (\'y\', \'x\')\n        return crs.dimensions\n\n    @property\n    def resolution(self) -> Tuple[float, float]:\n        """"""\n        Resolution in Y,X dimensions\n        """"""\n        return self.affine.e, self.affine.a\n\n    @property\n    def alignment(self) -> Tuple[float, float]:\n        """"""\n        Alignment of pixel boundaries in Y,X dimensions\n        """"""\n        return self.affine.yoff % abs(self.affine.e), self.affine.xoff % abs(self.affine.a)\n\n    @property\n    def coordinates(self) -> Dict[str, Coordinate]:\n        """"""\n        dict of coordinate labels\n        """"""\n        yres, xres = self.resolution\n        yoff, xoff = self.affine.yoff, self.affine.xoff\n\n        xs = numpy.arange(self.width) * xres + (xoff + xres / 2)\n        ys = numpy.arange(self.height) * yres + (yoff + yres / 2)\n\n        units = self.crs.units if self.crs is not None else (\'1\', \'1\')\n\n        return OrderedDict((dim, Coordinate(labels, units, res))\n                           for dim, labels, units, res in zip(self.dimensions, (ys, xs), units, (yres, xres)))\n\n    def xr_coords(self, with_crs: Union[bool, str] = False) -> Dict[Hashable, xr.DataArray]:\n        """""" Dictionary of Coordinates in xarray format\n\n            :param with_crs: If True include netcdf/cf style CRS Coordinate\n            with default name \'spatial_ref\', if with_crs is a string then treat\n            the string as a name of the coordinate.\n\n            Returns\n            =======\n\n            OrderedDict name:str -> xr.DataArray\n\n            where names are either `y,x` for projected or `latitude, longitude` for geographic.\n\n        """"""\n        spatial_ref = ""spatial_ref""\n        if isinstance(with_crs, str):\n            spatial_ref = with_crs\n            with_crs = True\n\n        attrs = {}\n        coords = self.coordinates\n        crs = self.crs\n        if crs is not None:\n            attrs[\'crs\'] = str(crs)\n\n        coords = dict((n, _coord_to_xr(n, c, **attrs))\n                      for n, c in coords.items())  # type: Dict[Hashable, xr.DataArray]\n\n        if with_crs and crs is not None:\n            coords[spatial_ref] = _mk_crs_coord(crs, spatial_ref)\n\n        return coords\n\n    @property\n    def geographic_extent(self) -> Geometry:\n        """""" GeoBox extent in EPSG:4326\n        """"""\n        if self.crs is None or self.crs.geographic:\n            return self.extent\n        return self.extent.to_crs(CRS(\'EPSG:4326\'))\n\n    coords = coordinates\n    dims = dimensions\n\n    def __str__(self):\n        return ""GeoBox({})"".format(self.geographic_extent)\n\n    def __repr__(self):\n        return ""GeoBox({width}, {height}, {affine!r}, {crs})"".format(\n            width=self.width,\n            height=self.height,\n            affine=self.affine,\n            crs=self.extent.crs\n        )\n\n    def __eq__(self, other):\n        if not isinstance(other, GeoBox):\n            return False\n\n        return (self.shape == other.shape\n                and self.transform == other.transform\n                and self.crs == other.crs)\n\n\ndef bounding_box_in_pixel_domain(geobox: GeoBox, reference: GeoBox) -> BoundingBox:\n    """"""\n    Returns the bounding box of `geobox` with respect to the pixel grid\n    defined by `reference` when their coordinate grids are compatible,\n    that is, have the same CRS, same pixel size and orientation, and\n    are related by whole pixel translation,\n    otherwise raises `ValueError`.\n    """"""\n    tol = 1.e-8\n\n    if reference.crs != geobox.crs:\n        raise ValueError(""Cannot combine geoboxes in different CRSs"")\n\n    a, b, c, d, e, f, *_ = ~reference.affine * geobox.affine\n\n    if not (numpy.isclose(a, 1) and numpy.isclose(b, 0) and is_almost_int(c, tol)\n            and numpy.isclose(d, 0) and numpy.isclose(e, 1) and is_almost_int(f, tol)):\n        raise ValueError(""Incompatible grids"")\n\n    tx, ty = round(c), round(f)\n    return BoundingBox(tx, ty, tx + geobox.width, ty + geobox.height)\n\n\ndef geobox_union_conservative(geoboxes: List[GeoBox]) -> GeoBox:\n    """""" Union of geoboxes. Fails whenever incompatible grids are encountered. """"""\n    if len(geoboxes) == 0:\n        raise ValueError(""No geoboxes supplied"")\n\n    reference, *_ = geoboxes\n\n    bbox = bbox_union(bounding_box_in_pixel_domain(geobox, reference=reference)\n                      for geobox in geoboxes)\n\n    affine = reference.affine * Affine.translation(*bbox[:2])\n\n    return GeoBox(width=bbox.width, height=bbox.height, affine=affine, crs=reference.crs)\n\n\ndef geobox_intersection_conservative(geoboxes: List[GeoBox]) -> GeoBox:\n    """"""\n    Intersection of geoboxes. Fails whenever incompatible grids are encountered.\n    """"""\n    if len(geoboxes) == 0:\n        raise ValueError(""No geoboxes supplied"")\n\n    reference, *_ = geoboxes\n\n    bbox = bbox_intersection(bounding_box_in_pixel_domain(geobox, reference=reference)\n                             for geobox in geoboxes)\n\n    # standardise empty geobox representation\n    if bbox.left > bbox.right:\n        bbox = BoundingBox(left=bbox.left, bottom=bbox.bottom, right=bbox.left, top=bbox.top)\n    if bbox.bottom > bbox.top:\n        bbox = BoundingBox(left=bbox.left, bottom=bbox.bottom, right=bbox.right, top=bbox.bottom)\n\n    affine = reference.affine * Affine.translation(*bbox[:2])\n\n    return GeoBox(width=bbox.width, height=bbox.height, affine=affine, crs=reference.crs)\n\n\ndef scaled_down_geobox(src_geobox: GeoBox, scaler: int) -> GeoBox:\n    """"""Given a source geobox and integer scaler compute geobox of a scaled down image.\n\n        Output geobox will be padded when shape is not a multiple of scaler.\n        Example: 5x4, scaler=2 -> 3x2\n\n        NOTE: here we assume that pixel coordinates are 0,0 at the top-left\n              corner of a top-left pixel.\n\n    """"""\n    assert scaler > 1\n\n    H, W = [X//scaler + (1 if X % scaler else 0)\n            for X in src_geobox.shape]\n\n    # Since 0,0 is at the corner of a pixel, not center, there is no\n    # translation between pixel plane coords due to scaling\n    A = src_geobox.transform * Affine.scale(scaler, scaler)\n\n    return GeoBox(W, H, A, src_geobox.crs)\n\n\ndef _round_to_res(value: float, res: float, acc: float = 0.1) -> int:\n    res = abs(res)\n    return int(math.ceil((value - 0.1 * res) / res))\n\n\ndef intersects(a: Geometry, b: Geometry) -> bool:\n    return a.intersects(b) and not a.touches(b)\n\n\ndef bbox_union(bbs: Iterable[BoundingBox]) -> BoundingBox:\n    """""" Given a stream of bounding boxes compute enclosing BoundingBox\n    """"""\n    # pylint: disable=invalid-name\n\n    L = B = float(\'+inf\')\n    R = T = float(\'-inf\')\n\n    for bb in bbs:\n        l, b, r, t = bb\n        L = min(l, L)\n        B = min(b, B)\n        R = max(r, R)\n        T = max(t, T)\n\n    return BoundingBox(L, B, R, T)\n\n\ndef bbox_intersection(bbs: Iterable[BoundingBox]) -> BoundingBox:\n    """""" Given a stream of bounding boxes compute the overlap BoundingBox\n    """"""\n    # pylint: disable=invalid-name\n\n    L = B = float(\'-inf\')\n    R = T = float(\'+inf\')\n\n    for bb in bbs:\n        l, b, r, t = bb\n        L = max(l, L)\n        B = max(b, B)\n        R = min(r, R)\n        T = min(t, T)\n\n    return BoundingBox(L, B, R, T)\n\n\ndef _mk_crs_coord(crs: CRS, name: str = \'spatial_ref\') -> xr.DataArray:\n    if crs.projected:\n        grid_mapping_name = crs._crs.to_cf().get(\'grid_mapping_name\')\n        if grid_mapping_name is None:\n            grid_mapping_name = ""??""\n        grid_mapping_name = grid_mapping_name.lower()\n    else:\n        grid_mapping_name = ""latitude_longitude""\n\n    epsg = 0 if crs.epsg is None else crs.epsg\n\n    return xr.DataArray(numpy.asarray(epsg, \'int32\'),\n                        name=name,\n                        dims=(),\n                        attrs={\'spatial_ref\': crs.wkt,\n                               \'grid_mapping_name\': grid_mapping_name})\n\n\ndef _coord_to_xr(name: str, c: Coordinate, **attrs) -> xr.DataArray:\n    """""" Construct xr.DataArray from named Coordinate object, this can then be used\n        to define coordinates for xr.Dataset|xr.DataArray\n    """"""\n    attrs = dict(units=c.units,\n                 resolution=c.resolution,\n                 **attrs)\n    return xr.DataArray(c.values,\n                        coords={name: c.values},\n                        dims=(name,),\n                        attrs=attrs)\n\n\ndef crs_units_per_degree(crs: SomeCRS,\n                         lon: Union[float, Tuple[float, float]],\n                         lat: float = 0,\n                         step: float = 0.1) -> float:\n    """""" Compute number of CRS units per degree for a projected CRS at a given location\n        in lon/lat.\n\n        Location can be supplied as a tuple or as two arguments.\n\n        Returns\n        -------\n        A floating number S such that `S*degrees -> meters`\n    """"""\n    if isinstance(lon, tuple):\n        lon, lat = lon\n\n    lon2 = lon + step\n    if lon2 > 180:\n        lon2 = lon - step\n\n    ll = line([(lon, lat),\n               (lon2, lat)],\n              \'EPSG:4326\')\n    xy = ll.to_crs(crs, resolution=math.inf)\n\n    return xy.length / step\n\n\ndef lonlat_bounds(geom: Geometry,\n                  mode: str = ""safe"",\n                  resolution: Optional[float] = None) -> BoundingBox:\n    """"""\n    Return the bounding box of a geometry\n\n    :param geom: Geometry in any projection\n    :param mode: safe|quick\n    :param resolution: If supplied will first segmentize input geometry to have no segment longer than ``resolution``,\n                       this increases accuracy at the cost of computation\n    """"""\n    assert mode in (""safe"", ""quick"")\n    if geom.crs is None:\n        raise ValueError(""lonlat_bounds can only operate on Geometry with CRS defined"")\n\n    if geom.crs.geographic:\n        return geom.boundingbox\n\n    if geom.type in (\'Polygon\', \'MultiPolygon\'):\n        geom = geom.exterior\n\n    if resolution is not None and math.isfinite(resolution):\n        geom = geom.segmented(resolution)\n\n    xx, yy = geom.to_crs(\'EPSG:4326\', resolution=math.inf).xy\n    xx_range = min(xx), max(xx)\n    yy_range = min(yy), max(yy)\n\n    if mode == ""safe"":\n        # If range in Longitude is more than 180 then it\'s probably wrapped\n        # around 180 (X-360 for X > 180), so we add back 360 but only for X<0\n        # values. This only works if input geometry doesn\'t span more than half\n        # a globe, so we need to check for that too, but this is not yet\n        # implemented...\n\n        span_x = xx_range[1] - xx_range[0]\n        if span_x > 180:\n            # TODO: check the case when input geometry spans >180 region.\n            #       For now we assume ""smaller"" geometries not too close\n            #       to poles.\n            xx_ = [x + 360 if x < 0 else x for x in xx]\n            xx_range_ = min(xx_), max(xx_)\n            span_x_ = xx_range_[1] - xx_range_[0]\n            if span_x_ < span_x:\n                xx_range = xx_range_\n\n    return BoundingBox.from_xy(xx_range, yy_range)\n'"
datacube/utils/geometry/_warp.py,11,"b'from typing import Union, Optional\nimport rasterio.warp\nimport rasterio.crs\nimport numpy as np\nfrom affine import Affine\nfrom . import GeoBox\n\nResampling = Union[str, int, rasterio.warp.Resampling]  # pylint: disable=invalid-name\nNodata = Optional[Union[int, float]]  # pylint: disable=invalid-name\n_WRP_CRS = rasterio.crs.CRS.from_epsg(3857)\n\n\ndef resampling_s2rio(name: str) -> rasterio.warp.Resampling:\n    """"""\n    Convert from string to rasterio.warp.Resampling enum, raises ValueError on bad input.\n    """"""\n    try:\n        return getattr(rasterio.warp.Resampling, name.lower())\n    except AttributeError:\n        raise ValueError(\'Bad resampling parameter: {}\'.format(name))\n\n\ndef is_resampling_nn(resampling: Resampling) -> bool:\n    """"""\n    :returns: True if resampling mode is nearest neighbour\n    :returns: False otherwise\n    """"""\n    if isinstance(resampling, str):\n        return resampling.lower() == \'nearest\'\n    return resampling == rasterio.warp.Resampling.nearest\n\n\ndef warp_affine_rio(src: np.ndarray,\n                    dst: np.ndarray,\n                    A: Affine,\n                    resampling: Resampling,\n                    src_nodata: Nodata = None,\n                    dst_nodata: Nodata = None,\n                    **kwargs) -> np.ndarray:\n    """"""\n    Perform Affine warp using rasterio as backend library.\n\n    :param        src: image as ndarray\n    :param        dst: image as ndarray\n    :param          A: Affine transformm, maps from dst_coords to src_coords\n    :param resampling: str|rasterio.warp.Resampling resampling strategy\n    :param src_nodata: Value representing ""no data"" in the source image\n    :param dst_nodata: Value to represent ""no data"" in the destination image\n\n    :param     kwargs: any other args to pass to ``rasterio.warp.reproject``\n\n    :returns: dst\n    """"""\n    crs = _WRP_CRS\n    src_transform = Affine.identity()\n    dst_transform = A\n\n    if isinstance(resampling, str):\n        resampling = resampling_s2rio(resampling)\n\n    # GDAL support for int8 is patchy, warp doesn\'t support it, so we need to convert to int16\n    if src.dtype.name == \'int8\':\n        src = src.astype(\'int16\')\n\n    if dst.dtype.name == \'int8\':\n        _dst = dst.astype(\'int16\')\n    else:\n        _dst = dst\n\n    rasterio.warp.reproject(src,\n                            _dst,\n                            src_transform=src_transform,\n                            dst_transform=dst_transform,\n                            src_crs=crs,\n                            dst_crs=crs,\n                            resampling=resampling,\n                            src_nodata=src_nodata,\n                            dst_nodata=dst_nodata,\n                            **kwargs)\n\n    if dst is not _dst:\n        # int8 workaround copy pixels back to int8\n        np.copyto(dst, _dst, casting=\'unsafe\')\n\n    return dst\n\n\ndef warp_affine(src: np.ndarray,\n                dst: np.ndarray,\n                A: Affine,\n                resampling: Resampling,\n                src_nodata: Nodata = None,\n                dst_nodata: Nodata = None,\n                **kwargs) -> np.ndarray:\n    """"""\n    Perform Affine warp using best available backend (GDAL via rasterio is the only one so far).\n\n    :param        src: image as ndarray\n    :param        dst: image as ndarray\n    :param          A: Affine transformm, maps from dst_coords to src_coords\n    :param resampling: str resampling strategy\n    :param src_nodata: Value representing ""no data"" in the source image\n    :param dst_nodata: Value to represent ""no data"" in the destination image\n\n    :param     kwargs: any other args to pass to implementation\n\n    :returns: dst\n    """"""\n    return warp_affine_rio(src, dst, A, resampling,\n                           src_nodata=src_nodata,\n                           dst_nodata=dst_nodata,\n                           **kwargs)\n\n\ndef rio_reproject(src: np.ndarray,\n                  dst: np.ndarray,\n                  s_gbox: GeoBox,\n                  d_gbox: GeoBox,\n                  resampling: Resampling,\n                  src_nodata: Nodata = None,\n                  dst_nodata: Nodata = None,\n                  **kwargs) -> np.ndarray:\n    """"""\n    Perform reproject from ndarray->ndarray using rasterio as backend library.\n\n    :param        src: image as ndarray\n    :param        dst: image as ndarray\n    :param     s_gbox: GeoBox of source image\n    :param     d_gbox: GeoBox of destination image\n    :param resampling: str|rasterio.warp.Resampling resampling strategy\n    :param src_nodata: Value representing ""no data"" in the source image\n    :param dst_nodata: Value to represent ""no data"" in the destination image\n\n    :param     kwargs: any other args to pass to ``rasterio.warp.reproject``\n\n    :returns: dst\n    """"""\n    if isinstance(resampling, str):\n        resampling = resampling_s2rio(resampling)\n\n    # GDAL support for int8 is patchy, warp doesn\'t support it, so we need to convert to int16\n    if src.dtype.name == \'int8\':\n        src = src.astype(\'int16\')\n\n    if dst.dtype.name == \'int8\':\n        _dst = dst.astype(\'int16\')\n    else:\n        _dst = dst\n\n    rasterio.warp.reproject(src,\n                            _dst,\n                            src_transform=s_gbox.transform,\n                            dst_transform=d_gbox.transform,\n                            src_crs=str(s_gbox.crs),\n                            dst_crs=str(d_gbox.crs),\n                            resampling=resampling,\n                            src_nodata=src_nodata,\n                            dst_nodata=dst_nodata,\n                            **kwargs)\n\n    if dst is not _dst:\n        # int8 workaround copy pixels back to int8\n        np.copyto(dst, _dst, casting=\'unsafe\')\n\n    return dst\n'"
datacube/utils/geometry/gbox.py,0,"b'"""""" Geometric operations on GeoBox class\n""""""\n\nfrom typing import Optional, Tuple, Dict, Iterable\nimport itertools\nimport math\nfrom affine import Affine\n\nfrom . import Geometry, GeoBox, BoundingBox\nfrom .tools import align_up\nfrom datacube.utils.math import clamp\n\n# pylint: disable=invalid-name\nMaybeInt = Optional[int]\nMaybeFloat = Optional[float]\n\n\ndef flipy(gbox: GeoBox) -> GeoBox:\n    """"""\n    :returns: GeoBox covering the same region but with Y-axis flipped\n    """"""\n    H, W = gbox.shape\n    A = Affine.translation(0, H)*Affine.scale(1, -1)\n    A = gbox.affine*A\n    return GeoBox(W, H, A, gbox.crs)\n\n\ndef flipx(gbox: GeoBox) -> GeoBox:\n    """"""\n    :returns: GeoBox covering the same region but with X-axis flipped\n    """"""\n    H, W = gbox.shape\n    A = Affine.translation(W, 0)*Affine.scale(-1, 1)\n    A = gbox.affine*A\n    return GeoBox(W, H, A, gbox.crs)\n\n\ndef translate_pix(gbox: GeoBox, tx: float, ty: float) -> GeoBox:\n    """"""\n    Shift GeoBox in pixel plane. (0,0) of the new GeoBox will be at the same\n    location as pixel (tx, ty) in the original GeoBox.\n    """"""\n    H, W = gbox.shape\n    A = gbox.affine*Affine.translation(tx, ty)\n    return GeoBox(W, H, A, gbox.crs)\n\n\ndef pad(gbox: GeoBox, padx: int, pady: MaybeInt = None) -> GeoBox:\n    """"""\n    Expand GeoBox by fixed number of pixels on each side\n    """"""\n    pady = padx if pady is None else pady\n\n    H, W = gbox.shape\n    A = gbox.affine*Affine.translation(-padx, -pady)\n    return GeoBox(W + padx*2, H + pady*2, A, gbox.crs)\n\n\ndef pad_wh(gbox: GeoBox,\n           alignx: int = 16,\n           aligny: MaybeInt = None) -> GeoBox:\n    """"""\n    Expand GeoBox such that width and height are multiples of supplied number.\n    """"""\n    aligny = alignx if aligny is None else aligny\n    H, W = gbox.shape\n\n    return GeoBox(align_up(W, alignx),\n                  align_up(H, aligny),\n                  gbox.affine, gbox.crs)\n\n\ndef zoom_out(gbox: GeoBox, factor: float) -> GeoBox:\n    """"""\n    factor > 1 --> smaller width/height, fewer but bigger pixels\n    factor < 1 --> bigger width/height, more but smaller pixels\n\n    :returns: GeoBox covering the same region but with bigger pixels (i.e. lower resolution)\n    """"""\n    from math import ceil\n\n    H, W = (max(1, ceil(s/factor)) for s in gbox.shape)\n    A = gbox.affine*Affine.scale(factor, factor)\n    return GeoBox(W, H, A, gbox.crs)\n\n\ndef zoom_to(gbox: GeoBox, shape: Tuple[int, int]) -> GeoBox:\n    """"""\n    :returns: GeoBox covering the same region but with different number of pixels\n              and therefore resolution.\n    """"""\n    H, W = gbox.shape\n    h, w = shape\n\n    sx, sy = W/float(w), H/float(h)\n    A = gbox.affine*Affine.scale(sx, sy)\n    return GeoBox(w, h, A, gbox.crs)\n\n\ndef rotate(gbox: GeoBox, deg: float) -> GeoBox:\n    """"""\n    Rotate GeoBox around the center.\n\n    It\'s as if you stick a needle through the center of the GeoBox footprint\n    and rotate it counter clock wise by supplied number of degrees.\n\n    Note that from pixel point of view image rotates the other way. If you have\n    source image with an arrow pointing right, and you rotate GeoBox 90 degree,\n    in that view arrow should point down (this is assuming usual case of inverted\n    y-axis)\n    """"""\n    h, w = gbox.shape\n    c0 = gbox.transform*(w*0.5, h*0.5)\n    A = Affine.rotation(deg, c0)*gbox.transform\n    return GeoBox(w, h, A, gbox.crs)\n\n\ndef affine_transform_pix(gbox: GeoBox, transform: Affine) -> GeoBox:\n    """"""\n    Apply affine transform on pixel side.\n\n    :param transform: Affine matrix mapping from new pixel coordinate space to\n    pixel coordinate space of input gbox\n\n    :returns: GeoBox of the same pixel shape but covering different region,\n    pixels in the output gbox relate to input geobox via `transform`\n\n    X_old_pix = transform * X_new_pix\n\n    """"""\n    H, W = gbox.shape\n    A = gbox.affine*transform\n    return GeoBox(W, H, A, gbox.crs)\n\n\nclass GeoboxTiles():\n    """""" Partition GeoBox into sub geoboxes\n    """"""\n\n    def __init__(self, box: GeoBox, tile_shape: Tuple[int, int]):\n        """""" Construct from a ``GeoBox``\n\n        :param box: source :class:`datacube.utils.geometry.GeoBox`\n        :param tile_shape: Shape of sub-tiles in pixels (rows, cols)\n        """"""\n        self._gbox = box\n        self._tile_shape = tile_shape\n        self._shape = tuple(math.ceil(float(N)/n)\n                            for N, n in zip(box.shape, tile_shape))\n        self._cache = {}  # type: Dict[Tuple[int, int], GeoBox]\n\n    @property\n    def base(self) -> GeoBox:\n        return self._gbox\n\n    @property\n    def shape(self):\n        """""" Number of tiles along each dimension\n        """"""\n        return self._shape\n\n    def _idx_to_slice(self, idx: Tuple[int, int]) -> Tuple[slice, slice]:\n        def _slice(i, N, n) -> slice:\n            _in = i*n\n            if 0 <= _in < N:\n                return slice(_in, min(_in + n, N))\n            else:\n                raise IndexError(""Index ({},{})is out of range"".format(*idx))\n\n        ir, ic = (_slice(i, N, n)\n                  for i, N, n in zip(idx, self._gbox.shape, self._tile_shape))\n        return (ir, ic)\n\n    def chunk_shape(self, idx: Tuple[int, int]) -> Tuple[int, int]:\n        """""" Chunk shape for a given chunk index.\n\n            :param idx: (row, col) index\n            :returns: (nrow, ncols) shape of a tile (edge tiles might be smaller)\n            :raises: IndexError when index is outside of [(0,0) -> .shape)\n        """"""\n        def _sz(i: int, n: int, tile_sz: int, total_sz: int) -> int:\n            if 0 <= i < n - 1:  # not edge tile\n                return tile_sz\n            elif i == n - 1:    # edge tile\n                return total_sz - (i*tile_sz)\n            else:               # out of index case\n                raise IndexError(""Index ({},{}) is out of range"".format(*idx))\n\n        n1, n2 = map(_sz, idx, self._shape, self._tile_shape, self._gbox.shape)\n        return (n1, n2)\n\n    def __getitem__(self, idx: Tuple[int, int]) -> GeoBox:\n        """""" Lookup tile by index, index is in matrix access order: (row, col)\n\n            :param idx: (row, col) index\n            :returns: GeoBox of a tile\n            :raises: IndexError when index is outside of [(0,0) -> .shape)\n        """"""\n        sub_gbox = self._cache.get(idx, None)\n        if sub_gbox is not None:\n            return sub_gbox\n\n        roi = self._idx_to_slice(idx)\n        return self._cache.setdefault(idx, self._gbox[roi])\n\n    def range_from_bbox(self, bbox: BoundingBox) -> Tuple[range, range]:\n        """""" Compute rows and columns overlapping with a given ``BoundingBox``\n        """"""\n        def clamped_range(v1: float, v2: float, N: int) -> range:\n            _in = clamp(math.floor(v1), 0, N)\n            _out = clamp(math.ceil(v2), 0, N)\n            return range(_in, _out)\n\n        sy, sx = self._tile_shape\n        A = Affine.scale(1.0/sx, 1.0/sy)*(~self._gbox.transform)\n        # A maps from X,Y in meters to chunk index\n        bbox = bbox.transform(A)\n\n        NY, NX = self.shape\n        xx = clamped_range(bbox.left, bbox.right, NX)\n        yy = clamped_range(bbox.bottom, bbox.top, NY)\n        return (yy, xx)\n\n    def tiles(self, polygon: Geometry) -> Iterable[Tuple[int, int]]:\n        """""" Return tile indexes overlapping with a given geometry.\n        """"""\n        poly = polygon.to_crs(self._gbox.crs)\n        yy, xx = self.range_from_bbox(poly.boundingbox)\n        for idx in itertools.product(yy, xx):\n            gbox = self[idx]\n            if gbox.extent.intersects(poly):\n                yield idx\n'"
datacube/utils/geometry/tools.py,29,"b'import numpy as np\nimport collections\nfrom types import SimpleNamespace\nfrom typing import Tuple\nfrom affine import Affine\n\n# This is numeric code, short names make sense in this context, so disabling\n# ""invalid name"" checks for the whole file\n# pylint: disable=invalid-name\n\n\nclass WindowFromSlice:\n    """""" Translate numpy slices to rasterio window tuples.\n    """"""\n    def __getitem__(self, roi):\n        if roi is None:\n            return None\n\n        if not isinstance(roi, collections.abc.Sequence) or len(roi) != 2:\n            raise ValueError(""Need 2d roi"")\n\n        row, col = roi\n        return ((0 if row.start is None else row.start, row.stop),\n                (0 if col.start is None else col.start, col.stop))\n\n\nw_ = WindowFromSlice()\n\n\ndef polygon_path(x, y=None):\n    """"""A little bit like numpy.meshgrid, except returns only boundary values and\n    limited to 2d case only.\n\n    Examples:\n      [0,1], [3,4] =>\n      array([[0, 1, 1, 0, 0],\n             [3, 3, 4, 4, 3]])\n\n      [0,1] =>\n      array([[0, 1, 1, 0, 0],\n             [0, 0, 1, 1, 0]])\n    """"""\n\n    if y is None:\n        y = x\n\n    return np.vstack([\n        np.vstack([x, np.full_like(x, y[0])]).T,\n        np.vstack([np.full_like(y, x[-1]), y]).T[1:],\n        np.vstack([x, np.full_like(x, y[-1])]).T[::-1][1:],\n        np.vstack([np.full_like(y, x[0]), y]).T[::-1][1:]]).T\n\n\ndef gbox_boundary(gbox, pts_per_side=16):\n    """"""Return points in pixel space along the perimeter of a GeoBox, or a 2d array.\n\n    """"""\n    H, W = gbox.shape[:2]\n    xx = np.linspace(0, W, pts_per_side, dtype=\'float32\')\n    yy = np.linspace(0, H, pts_per_side, dtype=\'float32\')\n\n    return polygon_path(xx, yy).T[:-1]\n\n\ndef roi_boundary(roi, pts_per_side=2):\n    """"""\n    Get boundary points from a 2d roi.\n\n    roi needs to be in the normalised form, i.e. no open-ended start/stop, see roi_normalise\n\n    :returns: Nx2 float32 array of X,Y points on the perimeter of the envelope defined by `roi`\n    """"""\n    yy, xx = roi\n    xx = np.linspace(xx.start, xx.stop, pts_per_side, dtype=\'float32\')\n    yy = np.linspace(yy.start, yy.stop, pts_per_side, dtype=\'float32\')\n\n    return polygon_path(xx, yy).T[:-1]\n\n\ndef align_down(x, align):\n    return x - (x % align)\n\n\ndef align_up(x, align):\n    return align_down(x+(align-1), align)\n\n\ndef scaled_down_roi(roi, scale: int):\n    return tuple(slice(s.start//scale,\n                       align_up(s.stop, scale)//scale) for s in roi)\n\n\ndef scaled_up_roi(roi, scale: int, shape=None):\n    roi = tuple(slice(s.start*scale,\n                      s.stop*scale) for s in roi)\n    if shape is not None:\n        roi = tuple(slice(min(dim, s.start),\n                          min(dim, s.stop))\n                    for s, dim in zip(roi, shape))\n    return roi\n\n\ndef scaled_down_shape(shape, scale: int):\n    return tuple(align_up(s, scale)//scale for s in shape)\n\n\ndef roi_shape(roi):\n    def slice_dim(s):\n        return s.stop if s.start is None else s.stop - s.start\n\n    if isinstance(roi, slice):\n        roi = (roi,)\n\n    return tuple(slice_dim(s) for s in roi)\n\n\ndef roi_is_empty(roi):\n    return any(d <= 0 for d in roi_shape(roi))\n\n\ndef roi_is_full(roi, shape):\n    """"""\n    :returns: True if roi covers region from (0,..) -> shape\n    """"""\n    def slice_full(s, n):\n        return s.start in (0, None) and s.stop in (n, None)\n\n    if isinstance(roi, slice):\n        roi = (roi,)\n        shape = (shape,)\n\n    return all(slice_full(s, n) for s, n in zip(roi, shape))\n\n\ndef roi_normalise(roi, shape):\n    """"""\n    Fill in missing .start/.stop, also deal with negative values, which are\n    treated as offsets from the end.\n\n    .step parameter is left unchanged.\n\n    Example:\n          np.s_[:3, 4:  ], (10, 20) -> np._s[0:3, 4:20]\n          np.s_[:3,  :-3], (10, 20) -> np._s[0:3, 0:17]\n\n    """"""\n\n    def fill_if_none(x, val_if_none):\n        return val_if_none if x is None else x\n\n    def norm_slice(s, n):\n        start = fill_if_none(s.start, 0)\n        stop = fill_if_none(s.stop, n)\n        start, stop = [x if x >= 0 else n+x for x in (start, stop)]\n        return slice(start, stop, s.step)\n\n    if not isinstance(shape, collections.Sequence):\n        shape = (shape,)\n\n    if isinstance(roi, slice):\n        return norm_slice(roi, shape[0])\n\n    return tuple([norm_slice(s, n) for s, n in zip(roi, shape)])\n\n\ndef roi_pad(roi, pad, shape):\n    """"""\n    Pad ROI on each side, with clamping (0,..) -> shape\n    """"""\n    def pad_slice(s, n):\n        return slice(max(0, s.start - pad), min(n, s.stop + pad))\n\n    if isinstance(roi, slice):\n        return pad_slice(roi, shape)\n\n    return tuple(pad_slice(s, n) for s, n in zip(roi, shape))\n\n\ndef apply_affine(A: Affine, x: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    """"""\n    broadcast A*(x_i, y_i) across all elements of x/y arrays in any shape (usually 2d image)\n    """"""\n\n    shape = x.shape\n\n    A = np.asarray(A).reshape(3, 3)\n    t = A[:2, -1].reshape((2, 1))\n    A = A[:2, :2]\n\n    x, y = A @ np.vstack([x.ravel(), y.ravel()]) + t\n    x, y = (a.reshape(shape) for a in (x, y))\n    return (x, y)\n\n\ndef split_translation(t):\n    """"""\n    Split translation into pixel aligned and sub-pixel components.\n\n    Subpixel translation is guaranteed to be in [-0.5, +0.5] range.\n\n    >  x + t = x + t_whole + t_subpix\n\n    :param t: (float, float)\n\n    :returns: (t_whole: (float, float), t_subpix: (float, float))\n    """"""\n    from math import fmod\n\n    def _split1(x):\n        x_part = fmod(x, 1.0)\n        x_whole = x - x_part\n        if x_part > 0.5:\n            x_part -= 1\n            x_whole += 1\n        elif x_part < -0.5:\n            x_part += 1\n            x_whole -= 1\n\n        return (x_whole, x_part)\n\n    _tt = [_split1(x) for x in t]\n\n    return tuple(t[0] for t in _tt), tuple(t[1] for t in _tt)\n\n\ndef is_affine_st(A, tol=1e-10):\n    """"""\n    True if Affine transform has scale and translation components only.\n    """"""\n    (_, wx, _,\n     wy, _, _,\n     *_) = A\n\n    return abs(wx) < tol and abs(wy) < tol\n\n\ndef decompose_rws(A):\n    """"""\n    Compute decomposition Affine matrix sans translation into Rotation, Shear and Scale.\n\n    Note: that there are ambiguities for negative scales.\n\n    Example: R(90)*S(1,1) == R(-90)*S(-1,-1),\n    (R*(-I))*((-I)*S) == R*S\n\n    A = R W S\n\n    Where:\n\n    R [ca -sa]  W [1, w]  S [sx,  0]\n      [sa  ca]    [0, 1]    [ 0, sy]\n\n    :return: Rotation, Sheer, Scale\n    """"""\n    # pylint: disable=too-many-locals\n\n    from numpy.linalg import cholesky, det, inv\n\n    if isinstance(A, Affine):\n        def to_affine(m, t=(0, 0)):\n            a, b, d, e = m.ravel()\n            c, f = t\n            return Affine(a, b, c,\n                          d, e, f)\n\n        (a, b, c,\n         d, e, f,\n         *_) = A\n        R, W, S = decompose_rws(np.asarray([[a, b],\n                                            [d, e]], dtype=\'float64\'))\n\n        return to_affine(R, (c, f)), to_affine(W), to_affine(S)\n\n    assert A.shape == (2, 2)\n\n    WS = cholesky(A.T @ A).T\n    R = A @ inv(WS)\n\n    if det(R) < 0:\n        R[:, -1] *= -1\n        WS[-1, :] *= -1\n\n    ss = np.diag(WS)\n    S = np.diag(ss)\n    W = WS @ np.diag(1.0/ss)\n\n    return R, W, S\n\n\ndef affine_from_pts(X, Y):\n    """"""\n    Given points X,Y compute A, such that: Y = A*X.\n\n    Needs at least 3 points.\n\n    :rtype: Affine\n    """"""\n    from numpy.linalg import lstsq\n\n    assert len(X) == len(Y)\n    assert len(X) >= 3\n\n    n = len(X)\n\n    XX = np.ones((n, 3), dtype=\'float64\')\n    YY = np.vstack(Y)\n    for i, x in enumerate(X):\n        XX[i, :2] = x\n\n    mm, *_ = lstsq(XX, YY, rcond=-1)\n    a, d, b, e, c, f = mm.ravel()\n\n    return Affine(a, b, c,\n                  d, e, f)\n\n\ndef get_scale_from_linear_transform(A):\n    """"""\n    Given a linear transform compute scale change.\n\n    1. Y = A*X + t\n    2. Extract scale components of A\n\n    Returns (sx, sy), where sx > 0, sy > 0\n    """"""\n    _, _, S = decompose_rws(A)\n    return abs(S.a), abs(S.e)\n\n\ndef get_scale_at_point(pt, tr, r=None):\n    """"""\n    Given an arbitrary locally linear transform estimate scale change around a point.\n\n    1. Approximate Y = tr(X) as Y = A*X+t in the neighbourhood of pt, for X,Y in R2\n    2. Extract scale components of A\n\n\n    pt - estimate transform around this point\n    r  - radius around the point (default 1)\n\n    tr - List((x,y)) -> List((x,y))\n         takes list of 2-d points on input and outputs same length list of 2d on output\n\n    Returns (sx, sy), where sx > 0, sy > 0\n    """"""\n    pts0 = [(0, 0), (-1, 0), (0, -1), (1, 0), (0, 1)]\n    x0, y0 = pt\n    if r is None:\n        XX = [(float(x+x0), float(y+y0)) for x, y in pts0]\n    else:\n        XX = [(float(x*r+x0), float(y*r+y0)) for x, y in pts0]\n    YY = tr(XX)\n    A = affine_from_pts(XX, YY)\n    return get_scale_from_linear_transform(A)\n\n\ndef _same_crs_pix_transform(src, dst):\n    assert src.crs == dst.crs\n\n    def transform(pts, A):\n        return [A*pt[:2] for pt in pts]\n\n    _fwd = (~dst.transform) * src.transform  # src -> dst\n    _bwd = ~_fwd                             # dst -> src\n\n    def pt_tr(pts):\n        return transform(pts, _fwd)\n    pt_tr.back = lambda pts: transform(pts, _bwd)\n    pt_tr.back.back = pt_tr\n    pt_tr.linear = _fwd\n    pt_tr.back.linear = _bwd\n\n    return pt_tr\n\n\ndef compute_axis_overlap(Ns: int, Nd: int, s: float, t: float) -> Tuple[slice, slice]:\n    """"""\n    s, t define linear transform from destination coordinate space to source\n    >>  x_s = s * x_d + t\n\n    Ns -- number of pixels along some dimension of source image: (0, Ns)\n    Nd -- same as Ns but for destination image\n\n    :returns: (slice in the source image,\n               slice in the destination image)\n    """"""\n    from math import floor, ceil\n\n    needs_flip = s < 0\n\n    if needs_flip:\n        # change s, t to map into flipped src, i.e. src[::-1]\n        s, t = -s, Ns - t\n\n    assert s > 0\n\n    # x_d = (x_s - t)/s => 1/s * x_s + t*(-1/s)\n    #\n    # x_d = s_ * x_s + t_\n    s_ = 1.0/s\n    t_ = -t*s_\n\n    if t < 0:\n        #  |<------- ... D\n        #      |<--- ... S\n        _in = (0, min(floor(t_), Nd))\n    else:\n        #        |<--... D\n        # |<---------... S\n        _in = (min(floor(t), Ns), 0)\n\n    a = ceil(Nd*s + t)\n    if a <= Ns:\n        # ...----->|    D\n        # ...-------->| S\n        _out = (max(a, 0), Nd)\n    else:\n        # ...-------->|  D\n        # ...----->|     S\n        _out = (Ns, max(0, ceil(Ns*s_ + t_)))\n\n    src, dst = (slice(_in[i], _out[i]) for i in range(2))\n\n    if needs_flip:\n        # remap src from flipped space to normal\n        src = slice(Ns - src.stop, Ns - src.start)  # type: ignore\n\n    return (src, dst)\n\n\ndef box_overlap(src_shape, dst_shape, ST):\n    """"""\n    Given two image planes whose coordinate systems are related via scale and\n    translation only, find overlapping regions within both.\n\n    :param src_shape: Shape of source image plane\n    :param dst_shape: Shape of destination image plane\n    :param        ST: Affine transform with only scale/translation,\n                      direction is: Xsrc = ST*Xdst\n    """"""\n    (sx, _, tx,\n     _, sy, ty,\n     *_) = ST\n\n    s0, d0 = compute_axis_overlap(src_shape[0], dst_shape[0], sy, ty)\n    s1, d1 = compute_axis_overlap(src_shape[1], dst_shape[1], sx, tx)\n    return (s0, s1), (d0, d1)\n\n\ndef native_pix_transform(src, dst):\n    """"""\n\n    direction: from src to dst\n    .back: goes the other way\n    .linear: None|Affine linear transform src->dst if transform is linear (i.e. same CRS)\n    """"""\n    # Special case CRS_in == CRS_out\n    if src.crs == dst.crs:\n        return _same_crs_pix_transform(src, dst)\n\n    _in = SimpleNamespace(crs=src.crs, A=src.transform)\n    _out = SimpleNamespace(crs=dst.crs, A=dst.transform)\n\n    _fwd = _in.crs.transformer_to_crs(_out.crs)\n    _bwd = _out.crs.transformer_to_crs(_in.crs)\n\n    _fwd = (_in.A, _fwd, ~_out.A)\n    _bwd = (_out.A, _bwd, ~_in.A)\n\n    def transform(pts, params):\n        A, f, B = params\n        return [B*pt[:2] for pt in [f(*(A*pt[:2])) for pt in pts]]\n\n    def tr(pts):\n        return transform(pts, _fwd)\n    tr.back = lambda pts: transform(pts, _bwd)\n    tr.back.back = tr\n    tr.linear = None\n    tr.back.linear = None\n\n    return tr\n\n\ndef roi_intersect(a, b):\n    """"""\n    Compute intersection of two ROIs\n    """"""\n    def slice_intersect(a, b):\n        if a.stop < b.start:\n            return slice(a.stop, a.stop)\n        elif a.start > b.stop:\n            return slice(a.start, a.start)\n\n        _in = max(a.start, b.start)\n        _out = min(a.stop, b.stop)\n\n        return slice(_in, _out)\n\n    if isinstance(a, slice):\n        if not isinstance(b, slice):\n            b = b[0]\n        return slice_intersect(a, b)\n\n    b = (b,) if isinstance(b, slice) else b\n\n    return tuple(slice_intersect(sa, sb) for sa, sb in zip(a, b))\n\n\ndef roi_center(roi):\n    """""" Return center point of roi\n    """"""\n    def slice_center(s):\n        return (s.start + s.stop)*0.5\n\n    if isinstance(roi, slice):\n        return slice_center(roi)\n\n    return tuple(slice_center(s) for s in roi)\n\n\ndef roi_from_points(xy, shape, padding=0, align=None):\n    """"""\n    Compute envelope around a bunch of points and return it as roi (tuple of\n    row/col slices)\n\n    Returned roi is clipped (0,0) --> shape, so it won\'t stick outside of the\n    valid region.\n    """"""\n    def to_roi(*args):\n        return tuple(slice(v[0], v[1]) for v in args)\n\n    assert len(shape) == 2\n    assert xy.ndim == 2 and xy.shape[1] == 2\n\n    ny, nx = shape\n\n    _in = np.floor(xy.min(axis=0)).astype(\'int32\') - padding\n    _out = np.ceil(xy.max(axis=0)).astype(\'int32\') + padding\n\n    if align is not None:\n        _in = align_down(_in, align)\n        _out = align_up(_out, align)\n\n    xx = np.asarray([_in[0], _out[0]])\n    yy = np.asarray([_in[1], _out[1]])\n\n    xx = np.clip(xx, 0, nx, out=xx)\n    yy = np.clip(yy, 0, ny, out=yy)\n\n    return to_roi(yy, xx)\n\n\ndef compute_reproject_roi(src, dst, padding=None, align=None):\n    """"""Given two GeoBoxes find the region within the source GeoBox that overlaps\n    with the destination GeoBox, and also compute the scale factor (>1 means\n    shrink). Scale is chosen such that if you apply it to the source image\n    before reprojecting, then reproject will have roughly no scale component.\n\n    So we breaking up reprojection into two stages:\n\n    1. Scale in the native pixel CRS\n    2. Reprojection (possibly non-linear with CRS change)\n\n    - src[roi] -> scale      -> reproject -> dst  (using native pixels)\n    - src(scale)[roi(scale)] -> reproject -> dst  (using overview image)\n\n    Here roi is ""minimal"", padding is configurable though, so you only read what you need.\n    Also scale can be used to pick the right kind of overview level to read.\n\n    Applying reprojection in two steps allows us to use pre-computed overviews,\n    particularly useful when shrink factor is large. But even for data sources\n    without overviews there are advantages for shrinking source image before\n    applying reprojection: mainly quality of the output (reduces aliasing for\n    large shrink factors), improved efficiency of the computation is likely as\n    well.\n\n    Also compute and return ROI of the dst geobox that is affected by src.\n\n    If padding is None ""appropriate"" padding will be used depending on the\n    transform between src<>dst:\n\n    - No padding beyond sub-pixel alignment if Scale+Translation\n    - 1 pixel source padding in all other cases\n\n    :returns: SimpleNamespace with following fields:\n     .roi_src    : (slice, slice)\n     .roi_dst    : (slice, slice)\n     .scale      : float\n     .scale2     : (sx: float, sy: float)\n     .is_st      : True|False\n     .transform  : src coord -> dst coord\n\n    For scale direction is: ""scale > 1 --> shrink src to fit dst""\n\n    """"""\n    pts_per_side = 5\n\n    def compute_roi(src, dst, tr, pts_per_side, padding, align):\n        XY = np.vstack(tr.back(gbox_boundary(dst, pts_per_side)))\n        roi_src = roi_from_points(XY, src.shape, padding, align=align)\n\n        if roi_is_empty(roi_src):\n            return (roi_src, np.s_[0:0, 0:0])\n\n        # project src roi back into dst and compute roi from that\n        xy = np.vstack(tr(roi_boundary(roi_src, pts_per_side)))\n        roi_dst = roi_from_points(xy, dst.shape, padding=0)  # no need to add padding twice\n        return (roi_src, roi_dst)\n\n    tr = native_pix_transform(src, dst)\n\n    if tr.linear is not None:\n        tight_ok = align in (None, 0) and padding in (0, None)\n        is_st = is_affine_st(tr.linear)\n\n        if tight_ok and is_st:\n            roi_src, roi_dst = box_overlap(src.shape, dst.shape, tr.back.linear)\n        else:\n            padding = 1 if padding is None else padding\n            roi_src, roi_dst = compute_roi(src, dst, tr, 2, padding, align)\n\n        scale2 = get_scale_from_linear_transform(tr.linear)\n    else:\n        is_st = False\n        padding = 1 if padding is None else padding\n\n        roi_src, roi_dst = compute_roi(src, dst, tr, pts_per_side, padding, align)\n        center_pt = roi_center(roi_src)[::-1]\n        scale2 = get_scale_at_point(center_pt, tr)\n\n    # change scale direction to be a shrink by factor\n    scale2 = tuple(1/s for s in scale2)\n    scale = min(scale2)\n\n    return SimpleNamespace(roi_src=roi_src,\n                           roi_dst=roi_dst,\n                           scale=scale,\n                           scale2=scale2,\n                           is_st=is_st,\n                           transform=tr)\n'"
datacube/utils/rio/__init__.py,0,"b'""""""\nThis will move into IO driver eventually.\n\nFor now this provides tools to configure GDAL environment for performant reads from S3.\n""""""\nfrom ._rio import (\n    activate_rio_env,\n    deactivate_rio_env,\n    get_rio_env,\n    set_default_rio_config,\n    activate_from_config,\n    configure_s3_access,\n)\n\n__all__ = (\n    \'activate_rio_env\',\n    \'deactivate_rio_env\',\n    \'get_rio_env\',\n    \'set_default_rio_config\',\n    \'activate_from_config\',\n    \'configure_s3_access\',\n)\n'"
datacube/utils/rio/_rio.py,0,"b'"""""" rasterio environment management tools\n""""""\nimport threading\nfrom types import SimpleNamespace\nimport functools\nimport rasterio\nfrom rasterio.session import AWSSession, DummySession\nimport rasterio.env\nfrom datacube.utils.generic import thread_local_cache\n\n_CFG_LOCK = threading.Lock()\n_CFG = SimpleNamespace(aws=None,\n                       cloud_defaults=False,\n                       kwargs={},\n                       epoch=0)\n\n\nSECRET_KEYS = (\'AWS_ACCESS_KEY_ID\',\n               \'AWS_SECRET_ACCESS_KEY\',\n               \'AWS_SESSION_TOKEN\')\n\n\ndef _sanitize(opts, keys):\n    return {k: (v if k not in keys\n                else \'xx..xx\')\n            for k, v in opts.items()}\n\n\ndef _state(purge=False):\n    """"""\n    .env   None| rasterio.Env\n    .epoch -1  | +Int\n    """"""\n    return thread_local_cache(""__rio_state__"",\n                              SimpleNamespace(env=None, epoch=-1),\n                              purge=purge)\n\n\ndef get_rio_env(sanitize=True):\n    """""" Get GDAL params configured by rasterio for the current thread.\n\n    :param sanitize: If True replace sensitive Values with \'x\'\n    """"""\n\n    env = rasterio.env.local._env  # pylint: disable=protected-access\n    if env is None:\n        return {}\n    opts = env.get_config_options()\n    if sanitize:\n        opts = _sanitize(opts, SECRET_KEYS)\n\n    return opts\n\n\ndef deactivate_rio_env():\n    """""" Exit previously configured environment, or do nothing if one wasn\'t configured.\n    """"""\n    state = _state(purge=True)\n\n    if state.env is not None:\n        state.env.__exit__(None, None, None)\n\n\ndef activate_rio_env(aws=None, cloud_defaults=False, **kwargs):\n    """""" Inject activated rasterio.Env into current thread.\n\n    This de-activates previously setup environment.\n\n    :param aws: Dictionary of options for rasterio.session.AWSSession\n                OR \'auto\' -- session = rasterio.session.AWSSession()\n\n    :param cloud_defaults: When True inject settings for reading COGs\n    :param **kwargs: Passed on to rasterio.Env(..) constructor\n    """"""\n    session = DummySession()\n\n    if aws is not None:\n        if not (aws == \'auto\' or\n                isinstance(aws, dict)):\n            raise ValueError(\'Only support: None|""auto""|{..} for `aws` parameter\')\n\n        aws = {} if aws == \'auto\' else dict(**aws)\n        region_name = aws.get(\'region_name\', \'auto\')\n\n        if region_name == \'auto\':\n            from datacube.utils.aws import auto_find_region\n            try:\n                aws[\'region_name\'] = auto_find_region()\n            except ValueError as e:\n                # only treat it as error if it was requested by user\n                if \'region_name\' in aws:\n                    raise e\n\n        session = AWSSession(**aws)\n\n    opts = dict(\n        GDAL_DISABLE_READDIR_ON_OPEN=\'EMPTY_DIR\',\n        GDAL_HTTP_MAX_RETRY=\'10\',\n        GDAL_HTTP_RETRY_DELAY=\'0.5\',\n    ) if cloud_defaults else {}\n\n    opts.update(**kwargs)\n\n    state = _state()\n\n    if state.env is not None:\n        state.env.__exit__(None, None, None)\n\n    env = rasterio.Env(session=session, **opts)\n    env.__enter__()\n    state.env = env\n    state.epoch = -1\n\n    return get_rio_env()\n\n\ndef activate_from_config():\n    """""" Check if this threads needs to reconfigure, then does reconfigure.\n\n    - Does nothing if this thread is already configured and configuration hasn\'t changed.\n    - Configures current thread with default rio settings\n    """"""\n    cfg = _CFG\n    state = _state()\n\n    if cfg.epoch != state.epoch:\n        ee = activate_rio_env(aws=cfg.aws,\n                              cloud_defaults=cfg.cloud_defaults,\n                              **cfg.kwargs)\n        state.epoch = cfg.epoch\n        return ee\n\n    return None\n\n\ndef set_default_rio_config(aws=None, cloud_defaults=False, **kwargs):\n    """""" Setup default configuration for rasterio/GDAL.\n\n    Doesn\'t actually activate one, just stores configuration for future\n    use from IO threads.\n\n    :param aws: Dictionary of options for rasterio.session.AWSSession\n                OR \'auto\' -- session = rasterio.session.AWSSession()\n\n    :param cloud_defaults: When True inject settings for reading COGs\n    :param **kwargs: Passed on to rasterio.Env(..) constructor\n    """"""\n    global _CFG  # pylint: disable=global-statement\n\n    with _CFG_LOCK:\n        _CFG = SimpleNamespace(aws=aws,\n                               cloud_defaults=cloud_defaults,\n                               kwargs=kwargs,\n                               epoch=_CFG.epoch + 1)\n\n\ndef configure_s3_access(profile=None,\n                        region_name=""auto"",\n                        aws_unsigned=False,\n                        requester_pays=False,\n                        cloud_defaults=True,\n                        client=None,\n                        **gdal_opts):\n    """""" Credentialize for S3 bucket access.\n\n    This function obtains credentials for S3 access and passes them on to\n    processing threads, either local or on dask cluster.\n\n    @returns credentials object or None if `aws_unsigned=True`\n\n    NOTE: if credentials are STS based they will eventually expire, currently\n    this case is not handled very well, reads will just start failing\n    eventually and will never recover.\n\n    :param profile:        AWS profile name to use\n    :param region_name:    Default region_name to use if not configured for a given/default AWS profile\n    :param aws_unsigned:   if True don\'t bother with credentials when reading from S3\n    :param requester_pays: needed when accessing requester pays buckets\n\n    :param cloud_defaults: Assume files are in the cloud native format, i.e. no side-car files, disables\n                           looking for side-car files, makes things faster but won\'t work for files\n                           that do have side-car files with extra metadata.\n\n    :param client:         Dask distributed `Client` instance, if supplied apply settings on the dask cluster\n                           rather than locally\n    """"""\n    from datacube.utils.aws import get_aws_settings\n\n    aws, creds = get_aws_settings(profile=profile,\n                                  region_name=region_name,\n                                  aws_unsigned=aws_unsigned,\n                                  requester_pays=requester_pays)\n\n    if client is None:\n        set_default_rio_config(aws=aws, cloud_defaults=cloud_defaults, **gdal_opts)\n    else:\n        client.register_worker_callbacks(\n            functools.partial(set_default_rio_config,\n                              aws=aws,\n                              cloud_defaults=cloud_defaults,\n                              **gdal_opts))\n\n    return creds\n'"
docs/user/recipes/line_transect.py,0,"b'import fiona\nimport numpy\nimport xarray\n\nimport datacube\nfrom datacube.utils import geometry\n\n\ndef transect(data, line, resolution, method=\'nearest\', tolerance=None):\n    """"""\n    Extract line transect from data along geom\n\n    :param xarray.Dataset data: data loaded via `Datacube.load`\n    :param datacube.utils.Geometry line: line along which to extract the transect\n    :param float resolution: interval used to extract points along the line (in data CRS units)\n    :param str method: see xarray.Dataset.sel_points\n    :param float tolerance: see xarray.Dataset.sel_points\n    """"""\n    assert line.type == \'LineString\'\n    line = line.to_crs(data.crs)\n    dist = numpy.arange(0, line.length, resolution)\n    points = [line.interpolate(d).coords[0] for d in dist]\n    indexers = {\n        data.crs.dimensions[0]: [p[1] for p in points],\n        data.crs.dimensions[1]: [p[0] for p in points]\n    }\n    return data.sel_points(xarray.DataArray(dist, name=\'distance\', dims=[\'distance\']),\n                           method=method,\n                           tolerance=tolerance,\n                           **indexers)\n\n\ndef main():\n    with fiona.open(\'line.shp\') as shapes:\n        crs = geometry.CRS(shapes.crs_wkt)\n        first_geometry = next(shapes)[\'geometry\']\n        line = geometry.Geometry(first_geometry, crs=crs)\n\n    query = {\n        \'time\': (\'1990-01-01\', \'1991-01-01\'),\n        \'geopolygon\': line\n    }\n\n    dc = datacube.Datacube(app=\'line-trans-recipe\')\n    data = dc.load(product=\'ls5_nbar_albers\', measurements=[\'red\'], **query)\n\n    trans = transect(data, line, abs(data.affine.a))\n    trans.red.plot(x=\'distance\', y=\'time\')\n'"
docs/user/recipes/multi_prod_series.py,0,"b""import numpy\nimport xarray\nimport datacube\n\nquery = {\n    'time': ('2013-01-01', '2014-01-01'),\n    'lat': (-35.2, -35.4),\n    'lon': (149.0, 149.2),\n}\n\nproducts = ['ls7_nbar_albers', 'ls8_nbar_albers']\n\ndc = datacube.Datacube(app='multi-prod-recipe')\n\n# find similarly named measurements\nmeasurements = set(dc.index.products.get_by_name(products[0]).measurements.keys())\nfor prod in products[1:]:\n    measurements.intersection(dc.index.products.get_by_name(products[0]).measurements.keys())\n\ndatasets = []\nfor prod in products:\n    ds = dc.load(product=prod, measurements=measurements, **query)\n    ds['product'] = ('time', numpy.repeat(prod, ds.time.size))\n    datasets.append(ds)\n\ncombined = xarray.concat(datasets, dim='time')\ncombined = combined.sortby('time')  # sort along time dim\n"""
docs/user/recipes/plot_rgb.py,0,"b""import datacube\nfrom datacube.utils.masking import mask_invalid_data\n\nquery = {\n    'time': ('1990-01-01', '1991-01-01'),\n    'lat': (-35.2, -35.4),\n    'lon': (149.0, 149.2),\n}\n\ndc = datacube.Datacube(app='plot-rgb-recipe')\ndata = dc.load(product='ls5_nbar_albers', measurements=['red', 'green', 'blue'], **query)\ndata = mask_invalid_data(data)\n\nfake_saturation = 4000\nrgb = data.to_array(dim='color')\nrgb = rgb.transpose(*(rgb.dims[1:]+rgb.dims[:1]))  # make 'color' the last dimension\nrgb = rgb.where((rgb <= fake_saturation).all(dim='color'))  # mask out pixels where any band is 'saturated'\nrgb /= fake_saturation  # scale to [0, 1] range for imshow\n\nrgb.plot.imshow(x=data.crs.dimensions[1], y=data.crs.dimensions[0],\n                col='time', col_wrap=5, add_colorbar=False)\n"""
docs/user/recipes/poly_drill.py,0,"b'import fiona\nimport rasterio.features\n\nimport datacube\nfrom datacube.utils import geometry\n\n\ndef geometry_mask(geoms, geobox, all_touched=False, invert=False):\n    """"""\n    Create a mask from shapes.\n\n    By default, mask is intended for use as a\n    numpy mask, where pixels that overlap shapes are False.\n    :param list[Geometry] geoms: geometries to be rasterized\n    :param datacube.utils.GeoBox geobox:\n    :param bool all_touched: If True, all pixels touched by geometries will be burned in. If\n                             false, only pixels whose center is within the polygon or that\n                             are selected by Bresenham\'s line algorithm will be burned in.\n    :param bool invert: If True, mask will be True for pixels that overlap shapes.\n    """"""\n    return rasterio.features.geometry_mask([geom.to_crs(geobox.crs) for geom in geoms],\n                                           out_shape=geobox.shape,\n                                           transform=geobox.affine,\n                                           all_touched=all_touched,\n                                           invert=invert)\n\n\ndef main():\n    shape_file = \'my_shape_file.shp\'\n    with fiona.open(shape_file) as shapes:\n        crs = geometry.CRS(shapes.crs_wkt)\n        first_geometry = next(iter(shapes))[\'geometry\']\n        geom = geometry.Geometry(first_geometry, crs=crs)\n\n    query = {\n        \'time\': (\'1990-01-01\', \'1991-01-01\'),\n        \'geopolygon\': geom\n    }\n\n    dc = datacube.Datacube(app=\'poly-drill-recipe\')\n    data = dc.load(product=\'ls5_nbar_albers\', measurements=[\'red\'], **query)\n\n    mask = geometry_mask([geom], data.geobox, invert=True)\n    data = data.where(mask)\n\n    data.red.plot.imshow(col=\'time\', col_wrap=5)\n'"
examples/io_plugin/dcio_example/__init__.py,0,b''
examples/io_plugin/dcio_example/pickles.py,0,"b'"""""" Example reader plugin\n""""""\nfrom contextlib import contextmanager\nimport pickle\n\n\nPROTOCOL = \'file\'\nFORMAT = \'pickle\'\n\n\ndef uri_split(uri):\n    loc = uri.find(\'://\')\n    if loc < 0:\n        return uri, PROTOCOL\n    return uri[loc+3:], uri[:loc]\n\n\nclass PickleDataSource(object):\n    class BandDataSource(object):\n        def __init__(self, da):\n            self._da = da\n            self.nodata = da.nodata\n\n        @property\n        def crs(self):\n            return self._da.crs\n\n        @property\n        def transform(self):\n            return self._da.affine\n\n        @property\n        def dtype(self):\n            return self._da.dtype\n\n        @property\n        def shape(self):\n            return self._da.shape\n\n        def read(self, window=None, out_shape=None):\n            if window is None:\n                data = self._da.values\n            else:\n                rows, cols = [slice(*w) for w in window]\n                data = self._da.values[rows, cols]\n\n            if out_shape is None or out_shape == data.shape:\n                return data\n\n            raise NotImplementedError(\'Native reading not supported for this data source\')\n\n    def __init__(self, band):\n        self._band = band\n        uri = band.uri\n        self._filename, protocol = uri_split(uri)\n\n        if protocol not in [PROTOCOL, \'pickle\']:\n            raise ValueError(\'Expected file:// or pickle:// url\')\n\n    @contextmanager\n    def open(self):\n        with open(self._filename, \'rb\') as f:\n            ds = pickle.load(f)\n\n        yield PickleDataSource.BandDataSource(ds[self._band.name].isel(time=0))\n\n\nclass PickleReaderDriver(object):\n    def __init__(self):\n        self.name = \'PickleReader\'\n        self.protocols = [PROTOCOL, \'pickle\']\n        self.formats = [FORMAT]\n\n    def supports(self, protocol, fmt):\n        return (protocol in self.protocols and\n                fmt in self.formats)\n\n    def new_datasource(self, band):\n        return PickleDataSource(band)\n\n\ndef rdr_driver_init():\n    return PickleReaderDriver()\n\n\nclass PickleWriterDriver(object):\n    def __init__(self):\n        pass\n\n    @property\n    def aliases(self):\n        return [\'pickles\']\n\n    @property\n    def format(self):\n        return FORMAT\n\n    @property\n    def uri_scheme(self):\n        return PROTOCOL\n\n    def write_dataset_to_storage(self, dataset, filename,\n                                 global_attributes=None,\n                                 variable_params=None,\n                                 storage_config=None,\n                                 **kwargs):\n        with open(filename, \'wb\') as f:\n            pickle.dump(dataset, f)\n        return {}\n\n\ndef writer_driver_init():\n    return PickleWriterDriver()\n'"
examples/io_plugin/dcio_example/zeros.py,0,"b'"""""" Sample plugin ""reads"" zeros each time every time\n\nTODO: not implemented yet\n""""""\n\n\nclass ZerosReaderDriver(object):\n    def __init__(self):\n        self.name = \'ZerosReader\'\n        self.protocols = [\'zero\']\n        self.formats = [\'0\']\n\n    def supports(self, protocol, fmt):\n        return protocol == \'zero\'\n\n    def new_datasource(self, band):\n        return None  # TODO\n\n\ndef init_driver():\n    return ZerosReaderDriver()\n'"
tests/drivers/fail_drivers/setup.py,0,"b'from setuptools import setup, find_packages\n\nsetup(\n    name=\'dc_tests_io\',\n    version=""1.0"",\n    description=\'Example ""bad drivers"" for testing driver loading protections\',\n    author=\'AGDC Collaboration\',\n    packages=find_packages(),\n\n    entry_points={\n        \'datacube.plugins.io.read\': [\n            \'bad_end_point=dc_tests_io.nosuch.module:init\',\n            \'failing_end_point_throw=dc_tests_io.dummy:fail_to_init\',\n            \'failing_end_point_none=dc_tests_io.dummy:init_to_none\',\n        ]\n    }\n)\n'"
tests/drivers/fail_drivers/dc_tests_io/__init__.py,0,b''
tests/drivers/fail_drivers/dc_tests_io/dummy.py,0,"b""def fail_to_init():\n    raise ValueError('This will always fail')\n\n\ndef init_to_none():\n    return None\n"""
