file_path,api_count,code
setup.py,0,"b'# -*- coding: UTF-8 -*-\n\nfrom setuptools import setup\n\nimport lava as lv\n\n\nsetup(\n    name=""lava"",\n    version=lv.__version__,\n    description=""Highlevel Wrapper for Vulkan\'s Compute API"",\n    author=""Jonas Schuepfer"",\n    author_email=""jonasschuepfer@gmail.com"",\n    packages=[""lava"", ""lava.api"", ""lava.api.constants""],\n    include_package_data=True,\n    install_requires=[""vulkan"", ""numpy"", ""future""],\n    url=""https://github.com/osanj/lava"",\n    keywords=[""Vulkan"", ""Parallel Computing"", ""Numpy""]\n)\n'"
lava/__init__.py,0,"b'# -*- coding: UTF-8 -*-\n\nimport atexit\nimport logging\nimport os\nimport platform\nimport warnings\n\n\n__version__ = ""0.3.2""\n\nENV_VAR_SDK = ""VULKAN_SDK""\nENV_VAR_LAYER_PATH = ""VK_LAYER_PATH""\n\nVALIDATION_LEVEL_DEBUG = logging.DEBUG\nVALIDATION_LEVEL_INFO = logging.INFO\nVALIDATION_LEVEL_WARNING = logging.WARNING\nVALIDATION_LEVEL_ERROR = logging.ERROR\n\nVALIDATION_LEVEL = None\n\n\n__instance = None\n__instance_usages = 0\n__devices = []\n\n\ntry:\n    import vulkan as vk\n    __error = None\nexcept OSError as e:\n    __error = e\n\nif ENV_VAR_SDK not in os.environ:\n    __error = ImportError(""{} environment variable not found"".format(ENV_VAR_SDK))\n\n\ndef __initialize():\n    global __error, __instance, __instance_usages, __devices, VALIDATION_LEVEL\n\n    if __error:\n        return\n\n    from .api.constants.vk import QueueType\n    from .api.device import PhysicalDevice\n    from .api.instance import Instance\n\n    if VALIDATION_LEVEL is not None and platform.system() == ""Linux"":\n        if ENV_VAR_LAYER_PATH not in os.environ:\n            __error = ImportError(""{} environment variable not found (required for validations)""\n                                  .format(ENV_VAR_LAYER_PATH))\n            return\n\n    try:\n        __instance = Instance(validation_lvl=VALIDATION_LEVEL)\n        __instance_usages = 0\n        __devices = []\n\n        for candidate in PhysicalDevice.all(__instance):\n            if candidate.supports_queue_type(QueueType.COMPUTE):\n                __devices.append(candidate)\n\n        if len(__devices) == 0:\n            warnings.warn(""Did not find any suitable device"", RuntimeWarning)\n\n    except:\n        __error = ImportError(""Could not initialize {}"".format(__name__))\n\n\ndef initialized():\n    global __error\n    return __error is None\n\n\ndef instance():\n    global __error, __instance, __instance_usages, __devices, VALIDATION_LEVEL\n\n    if __error:\n        raise __error\n\n    if __instance.validation_lvl != VALIDATION_LEVEL:\n        if __instance_usages > 0:\n            __instance_usages = 0\n            warnings.warn(""Recreating Vulkan instance with new validation level, any previously created sessions, ""\n                          ""devices, etc. will be no longer usable"", UserWarning)\n\n        __cleanup()\n        __initialize()\n\n    __instance_usages += 1\n    return __instance\n\n\ndef devices():\n    instance()  # validation level might has been changed\n    return __devices\n\n\n@atexit.register\ndef __cleanup():\n    global __instance, __devices\n\n    if __instance is not None:\n        from .session import sessions\n        for sess in sessions:\n            sess.destroy()\n\n        del __devices\n        __instance.destroy()\n\n\n__initialize()\n\nfrom .buffer import *\nfrom .session import *\nfrom .shader import *\nfrom .util import *\n'"
lava/buffer.py,0,"b'# -*- coding: UTF-8 -*-\n\nimport warnings\n\nfrom lava.api.bytes import ByteCache, Struct\nfrom lava.api.constants.spirv import Access\nfrom lava.api.constants.vk import BufferUsage, MemoryType\nfrom lava.api.memory import Buffer as _Buffer\nfrom lava.api.pipeline import CopyOperation\nfrom lava.api.util import Destroyable, LavaError, LavaUnsupportedError\n\n__all__ = [""BufferCPU"", ""BufferGPU"", ""StagedBuffer""]\n\n\nclass BufferInterface(Destroyable):\n\n    USAGE_STORAGE = BufferUsage.STORAGE_BUFFER\n    USAGE_UNIFORM = BufferUsage.UNIFORM_BUFFER\n\n    SYNC_EAGER = ""eager""\n    SYNC_LAZY = ""lazy""\n    SYNC_MANUAL = ""manual""\n    SYNC_DEFAULT = SYNC_LAZY\n\n    def __init__(self, session, block_definition, block_usage):\n        super(BufferInterface, self).__init__()\n        if not isinstance(block_definition, Struct):\n            raise LavaError(""Block definitions must be structs"")\n        self.session = session\n        self.block_definition = block_definition\n        self.block_usage = block_usage\n\n    def _destroy(self):\n        raise NotImplementedError()\n\n    def size(self):\n        return self.block_definition.size()\n\n    def get_block_definition(self):\n        return self.block_definition\n\n    def get_block_usage(self):\n        return self.block_usage\n\n    def get_vulkan_buffer(self):\n        raise NotImplementedError()\n\n    def before_stage(self, stage, binding, access_modifier):\n        pass\n\n    def after_stage(self, stage, binding, access_modifier):\n        pass\n\n    def flush(self):\n        pass\n\n    def fetch(self):\n        pass\n\n    def __getitem__(self, key):\n        raise NotImplementedError()\n\n    def __setitem__(self, key, value):\n        raise NotImplementedError()\n\n\nclass Buffer(BufferInterface):\n\n    LOCATION_CPU = ""CPU""\n    LOCATION_GPU = ""GPU""\n\n    def __init__(self, session, block_definition, block_usage, location):\n        super(Buffer, self).__init__(session, block_definition, block_usage)\n        self.location = location\n        self.vulkan_buffer = None\n        self.vulkan_memory = None\n        self.session.register_buffer(self)\n        self.allocate()\n        self.copy_operation = CopyOperation(self.session.device, self.session.queue_index)\n\n    def _destroy(self):\n        self.vulkan_memory.destroy()\n        self.vulkan_buffer.destroy()\n        self.copy_operation.destroy()\n\n    def allocate(self):\n        if self.vulkan_buffer is not None:\n            raise LavaError(""Buffer is already allocated"")\n\n        self.vulkan_buffer = _Buffer(self.session.device, self.size(), self.block_usage, self.session.queue_index)\n\n        minimum_size, _, supported_memory_indices = self.vulkan_buffer.get_memory_requirements()\n        memory_types = {self.LOCATION_CPU: MemoryType.CPU, self.LOCATION_GPU: MemoryType.GPU}[self.location]\n\n        self.vulkan_memory = self.session.device.allocate_memory(memory_types, minimum_size, supported_memory_indices)\n        self.vulkan_buffer.bind_memory(self.vulkan_memory)\n\n    def get_vulkan_buffer(self):\n        return self.vulkan_buffer\n\n    def get_location(self):\n        return self.location\n\n    def copy_to(self, other):\n        self.copy_operation.record(self.vulkan_buffer, other.vulkan_buffer)\n        self.copy_operation.run_and_wait()\n\n\nclass BufferCPU(Buffer):\n\n    def __init__(self, session, block_definition, block_usage, sync_mode=BufferInterface.SYNC_DEFAULT):\n        super(BufferCPU, self).__init__(session, block_definition, block_usage, Buffer.LOCATION_CPU)\n        self.cache = ByteCache(self.block_definition)\n        self.sync_mode = sync_mode\n        self.fresh_bytez = False\n\n    @classmethod\n    def from_shader(cls, session, shader, binding, sync_mode=BufferInterface.SYNC_DEFAULT):\n        block_definition = shader.get_block_definition(binding)\n        block_usage = shader.get_block_usage(binding)\n        return cls(session, block_definition, block_usage, sync_mode)\n\n    def before_stage(self, stage, binding, access_modifier):\n        if access_modifier in [Access.READ_ONLY, Access.READ_WRITE]:\n            if self.sync_mode is self.SYNC_LAZY and self.cache.is_dirty():\n                self.flush()\n\n    def after_stage(self, stage, binding, access_modifier):\n        if access_modifier in [Access.WRITE_ONLY, Access.READ_WRITE]:\n            self.fresh_bytez = True\n            if self.sync_mode is self.SYNC_EAGER:\n                self.fetch()\n\n    def __getitem__(self, key):\n        if self.fresh_bytez and self.sync_mode is self.SYNC_LAZY:\n            self.fetch()\n        return self.cache[key]\n\n    def __setitem__(self, key, value):\n        self.cache[key] = value\n\n        if self.sync_mode is self.SYNC_EAGER:\n            self.flush()\n\n    def is_synced(self):\n        return not self.fresh_bytez and not self.cache.is_dirty()\n\n    def flush(self):\n        if self.fresh_bytez:\n            warnings.warn(""Buffer contains (probably) data which is not in the cache, it will be overwritten"",\n                          RuntimeWarning)\n\n        data = self.cache.get_as_dict()\n        bytez = self.block_definition.to_bytes(data)\n        self.vulkan_buffer.map(bytez)\n        self.cache.set_dirty(False)\n        self.fresh_bytez = False\n\n    def fetch(self):\n        if self.cache.is_dirty():\n            warnings.warn(""Cache contains data which is not in the buffer, it will by overwritten"", RuntimeWarning)\n\n        with self.vulkan_buffer.mapped() as bytebuffer:\n            bytez = bytebuffer[:]\n        data = self.block_definition.from_bytes(bytez)\n        self.cache.set_from_dict(data)\n        self.cache.set_dirty(False)\n        self.fresh_bytez = False\n\n\nclass BufferGPU(Buffer):\n\n    def __init__(self, session, block_definition, block_usage):\n        super(BufferGPU, self).__init__(session, block_definition, block_usage, Buffer.LOCATION_GPU)\n        self.buffer_cpu = None\n        self.copy_operation = CopyOperation(self.session.device, self.session.queue_index)\n\n    @classmethod\n    def from_shader(cls, session, shader, binding):\n        block_definition = shader.get_block_definition(binding)\n        block_usage = shader.get_block_usage(binding)\n        return cls(session, block_definition, block_usage)\n\n    def __getitem__(self, key):\n        raise LavaUnsupportedError(""Unsupported, the only way to read gpu buffers, is to copy them to a cpu buffer"")\n\n    def __setitem__(self, key, value):\n        raise LavaUnsupportedError(""Unsupported, the only way to write gpu buffers, is to copy from a cpu buffer"")\n\n\nclass StagedBuffer(BufferInterface):\n\n    def __init__(self, session, block_definition, block_usage, sync_mode=BufferInterface.SYNC_DEFAULT):\n        super(StagedBuffer, self).__init__(session, block_definition, block_usage)\n        self.buffer_cpu = BufferCPU(session, block_definition, block_usage, sync_mode)\n        self.buffer_gpu = BufferGPU(session, block_definition, block_usage)\n        self.sync_mode = sync_mode\n        self.fresh_bytez = False\n\n    def _destroy(self):\n        self.buffer_cpu.destroy()\n        self.buffer_gpu.destroy()\n\n    @classmethod\n    def from_shader(cls, session, shader, binding, sync_mode=BufferInterface.SYNC_DEFAULT):\n        block_definition = shader.get_block_definition(binding)\n        block_usage = shader.get_block_usage(binding)\n        return cls(session, block_definition, block_usage, sync_mode)\n\n    def get_vulkan_buffer(self):\n        return self.buffer_gpu.get_vulkan_buffer()\n\n    def before_stage(self, stage, binding, access_modifier):\n        if access_modifier in [Access.READ_ONLY, Access.READ_WRITE]:\n            if self.sync_mode is self.SYNC_LAZY and self.buffer_cpu.cache.is_dirty():\n                self.flush()\n\n    def after_stage(self, stage, binding, access_modifier):\n        if access_modifier in [Access.WRITE_ONLY, Access.READ_WRITE]:\n            self.fresh_bytez = True\n            if self.sync_mode is self.SYNC_EAGER:\n                self.fetch()\n\n    def __getitem__(self, key):\n        if self.fresh_bytez:\n            self.fetch()\n        return self.buffer_cpu[key]\n\n    def __setitem__(self, key, value):\n        self.buffer_cpu[key] = value\n\n        if self.sync_mode is self.SYNC_EAGER:\n            self.flush()\n\n    def is_synced(self):\n        return not self.fresh_bytez and self.buffer_cpu.is_synced()\n\n    def flush(self):\n        if self.buffer_cpu.cache.is_dirty():\n            self.buffer_cpu.flush()\n        self.buffer_cpu.copy_to(self.buffer_gpu)\n        self.fresh_bytez = False\n\n    def fetch(self):\n        self.buffer_gpu.copy_to(self.buffer_cpu)\n        self.buffer_cpu.fetch()\n        self.fresh_bytez = False\n'"
lava/session.py,0,"b'# -*- coding: UTF-8 -*-\n\nimport lava\nfrom lava.api.constants.vk import QueueType\nfrom lava.api.device import Device\nfrom lava.api.util import Destroyable\n\n__all__ = [""Session""]\n\nsessions = set()\n\n\nclass Session(Destroyable):\n\n    def __init__(self, physical_device, queue_index=None):\n        super(Session, self).__init__()\n\n        self.instance = lava.instance()  # validation level might has been changed\n        if physical_device not in lava.devices():\n            raise RuntimeError(""Provided invalid / outdated device object"")\n\n        self.queue_index = queue_index or physical_device.get_queue_indices(QueueType.COMPUTE)[0]\n        self.device = Device(physical_device, [(QueueType.COMPUTE, self.queue_index)],\n                             validation_lvl=lava.VALIDATION_LEVEL)\n\n        self.buffers = set()\n        self.shaders = set()\n        self.stages = set()\n\n        sessions.add(self)\n\n    def _destroy(self):\n        for stage in self.stages:\n            stage.destroy()\n        for shader in self.shaders:\n            shader.destroy()\n        for buffer in self.buffers:\n            buffer.destroy()\n        self.device.destroy()\n\n    def register_buffer(self, buffer):\n        self.buffers.add(buffer)\n\n    def register_shader(self, shader):\n        self.shaders.add(shader)\n\n    def register_stage(self, stage):\n        self.stages.add(stage)\n'"
lava/shader.py,0,"b'# -*- coding: UTF-8 -*-\n\nimport warnings\n\nfrom future.utils import raise_with_traceback\n\nfrom lava.api.constants.vk import BufferUsage\nfrom lava.api.pipeline import ShaderOperation, Pipeline\nfrom lava.api.shader import Shader as _Shader\nfrom lava.api.util import Destroyable, LavaError\n\n__all__ = [""Shader"", ""Stage""]\n\n\nclass Shader(Destroyable):\n\n    def __init__(self, session, path, entry_point=None):\n        super(Shader, self).__init__()\n        self.session = session\n        self.session.register_shader(self)\n        self.vulkan_shader = _Shader.from_file(self.session.device, path, entry_point)\n        self.vulkan_shader.inspect()\n\n    def _destroy(self):\n        self.vulkan_shader.destroy()\n\n    def get_bindings(self):\n        return self.vulkan_shader.code.get_bindings()\n\n    def get_block_definition(self, binding):\n        return self.vulkan_shader.code.get_block_definition(binding)\n\n    def get_block_usage(self, binding):\n        return self.vulkan_shader.code.get_block_usage(binding)\n\n    def get_local_size(self):\n        return self.vulkan_shader.get_local_size()\n\n    def get_block_access(self, binding):\n        return self.vulkan_shader.code.get_block_access(binding)\n\n\nclass Stage(Destroyable):\n\n    def __init__(self, shader, bindings):\n        super(Stage, self).__init__()\n        self.session = shader.session\n        self.session.register_stage(self)\n        self.shader = shader\n        self.bindings = bindings\n\n        self.checked = False\n        self.check_workgroups()\n        memory_object_binding = self.check_block_definitions()\n        self.checked = True\n\n        self.pipeline = Pipeline(self.session.device, shader.vulkan_shader, memory_object_binding)\n        self.operation = ShaderOperation(self.session.device, self.pipeline, self.session.queue_index)\n\n    def _destroy(self):\n        if self.checked:\n            self.operation.destroy()\n            self.pipeline.destroy()\n\n    def check_workgroups(self):\n        physical_device = self.session.device.physical_device\n\n        x, y, z = self.shader.get_local_size()\n        x_max, y_max, z_max = physical_device.get_maximum_work_group_sizes()\n        group_invocations_max = physical_device.get_maximum_work_group_invocations()\n        if x > x_max or y > y_max or z > z_max:\n            msg = ""Device supports work group sizes up to x={} y={} z={}, but shader defines x={} y={} z={}"".format(\n                x_max, y_max, z_max, x, y, z)\n            raise LavaError(msg)\n\n        if x * y * z > group_invocations_max:\n            msg = ""Device supports work group invocations up to {}, but shader defines {}*{}*{}={}"".format(\n                group_invocations_max, x, y, z, x*y*z)\n            raise LavaError(msg)\n\n    def check_block_definitions(self):\n        memory_object_binding = {}\n        bindings_shader = self.shader.get_bindings()\n        max_uniform_size = self.session.device.physical_device.get_maximum_uniform_size()\n\n        for binding, buffer in self.bindings.items():\n            if binding not in bindings_shader:\n                raise LavaError(""Shader does not define binding {}"".format(binding))\n\n            usage_shader = self.shader.get_block_usage(binding)\n            usage_buffer = buffer.get_block_usage()\n\n            if usage_buffer != usage_shader:\n                raise LavaError(""Shader defines binding {} as {}, but got {}"".format(\n                    binding, usage_shader, usage_buffer))\n\n            definition_shader = self.shader.get_block_definition(binding)\n            definition_buffer = buffer.get_block_definition()\n\n            try:\n                definition_shader.compare(definition_buffer, quiet=False)\n            except TypeError as e:\n                msg = ""Block definition mismatch of buffer and shader at binding {}"".format(binding)\n                msg += ""\\n"" + e.args[0]\n                raise_with_traceback(LavaError(msg))\n\n            if usage_buffer == BufferUsage.UNIFORM_BUFFER:\n                if definition_buffer.size() > max_uniform_size:\n                    msg = ""Uniform at binding {} will be larger than maximum of {}, weird things might happen"".format(\n                        binding, max_uniform_size)\n                    warnings.warn(msg, UserWarning)\n\n            memory_object_binding[binding] = buffer.get_vulkan_buffer()\n        return memory_object_binding\n\n    def record(self, x, y, z, after_stages=(),):\n        x_max, y_max, z_max = self.session.device.physical_device.get_maximum_work_group_counts()\n        if x > x_max or y > y_max or z > z_max:\n            msg = ""Device supports work group counts up to x={} y={} z={}, but requested are x={} y={} z={}"".format(\n                x_max, y_max, z_max, x, y, z)\n            raise LavaError(msg)\n\n        wait_events = [stage.executor.event for stage in after_stages]\n        self.operation.record(x, y, z, wait_events)\n\n    def run(self):\n        for binding, buffer in self.bindings.items():\n            access_modifier = self.shader.get_block_access(binding)\n            buffer.before_stage(self, binding, access_modifier)\n\n        self.operation.run()\n\n    def wait(self):\n        self.operation.wait()\n\n        for binding, buffer in self.bindings.items():\n            access_modifier = self.shader.get_block_access(binding)\n            buffer.after_stage(self, binding, access_modifier)\n\n    def run_and_wait(self):\n        self.run()\n        self.wait()\n\n\n# class Flow(object):\n#\n#     def __init__(self, session):\n#         pass\n\n# buffers either have\n#   unlimited reads\n# or\n#   one write and unlimited reads afterwards\n'"
lava/util.py,0,"b'# -*- coding: UTF-8 -*-\n\nimport os\nimport platform\nimport subprocess\n\nfrom lava import ENV_VAR_SDK\n\n__all__ = [""compile_glsl""]\n\nEXT_SPIR_V = "".spv""\n\n\ndef compile_glsl(path, verbose=True):\n    path_output = path + EXT_SPIR_V\n\n    if platform.system() not in (""Windows"", ""Linux""):\n        raise NotImplementedError()\n\n    if ENV_VAR_SDK not in os.environ:\n        raise RuntimeError(""Could not find environment variable {}"".format(ENV_VAR_SDK))\n\n    if platform.system() == ""Linux"":\n        path_compiler = os.path.join(os.environ[ENV_VAR_SDK], ""bin"", ""glslangValidator"")\n    else:\n        path_compiler = os.path.join(os.environ[ENV_VAR_SDK], ""Bin"", ""glslangValidator.exe"")\n\n    cmd = [path_compiler, ""-V"", path, ""-o"", path_output]\n\n    if verbose:\n        result = subprocess.call(cmd, stderr=subprocess.STDOUT)\n    else:\n        dev_null = open(os.devnull, ""w"")\n        result = subprocess.call(cmd, stdout=dev_null, stderr=subprocess.STDOUT)\n        dev_null.close()\n\n    if result != 0:\n        raise RuntimeError(""Could not compile shader {}, try yourself with\\n{}"".format(path, "" "".join(cmd)))\n\n    return path_output\n'"
test/__init__.py,0,"b'# -*- coding: UTF-8 -*-\n\nimport os\nimport sys\n\n# load project\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "".."")))\nimport lava\n'"
test/buffer.py,4,"b'# -*- coding: UTF-8 -*-\n\nimport itertools\nimport os\nimport unittest\n\nimport numpy as np\n\nimport lava as lv\nfrom lava.api.bytes import Array, ScalarFloat, Struct\nfrom lava.api.constants.spirv import Layout\n\nfrom test.util import write_to_temp_file\n\n\nclass BufferTest(unittest.TestCase):\n\n    def setUp(self):\n        super(BufferTest, self).setUp()\n        self.session = lv.Session(lv.devices()[0])\n\n    def tearDown(self):\n        super(BufferTest, self).tearDown()\n        self.session.destroy()\n\n    def shader_from_txt(self, txt, verbose=True, clean_up=True):\n        path_shader = write_to_temp_file(txt, suffix="".comp"")\n        shader_path_spirv = lv.compile_glsl(path_shader, verbose)\n        shader = lv.Shader(self.session, shader_path_spirv)\n        if clean_up:\n            os.remove(path_shader)\n            os.remove(shader_path_spirv)\n        return shader\n\n    def test_copy(self):\n        data = np.arange(128, dtype=np.float32)\n        definition = Struct([Array(ScalarFloat(), len(data), Layout.STD140)], Layout.STD140)\n\n        buffer_a = lv.BufferCPU(self.session, definition, lv.BufferCPU.USAGE_STORAGE)\n        buffer_b = lv.BufferCPU(self.session, definition, lv.BufferCPU.USAGE_STORAGE)\n\n        buffer_a[0] = data\n        buffer_a.flush()\n        buffer_a.copy_to(buffer_b)\n        buffer_b.fetch()\n\n        self.assertTrue((buffer_a[0] == buffer_b[0]).all())\n\n    def test_copy_over_gpu(self):\n        data = np.arange(128, dtype=np.float32)\n        definition = Struct([Array(ScalarFloat(), len(data), Layout.STD140)], Layout.STD140)\n\n        buffer_a = lv.BufferCPU(self.session, definition, lv.BufferCPU.USAGE_STORAGE)\n        buffer_gpu = lv.BufferGPU(self.session, definition, lv.BufferCPU.USAGE_STORAGE)\n        buffer_b = lv.BufferCPU(self.session, definition, lv.BufferCPU.USAGE_STORAGE)\n\n        buffer_a[0] = data\n        buffer_a.flush()\n        buffer_a.copy_to(buffer_gpu)\n        buffer_gpu.copy_to(buffer_b)\n        buffer_b.fetch()\n\n        self.assertTrue((buffer_a[0] == buffer_b[0]).all())\n\n    def test_sync_modes(self):\n        glsl = """"""\n            #version 450\n            #extension GL_ARB_separate_shader_objects : enable\n\n            layout(local_size_x=1, local_size_y=1, local_size_z=1) in;\n\n            layout(std140, binding = 0) buffer readonly BufferA {\n                float[72][128][3] imageIn;\n            };\n\n            layout(std140, binding = 1) buffer writeonly BufferB {\n                float[72][128][3] imageOut;\n            };\n\n            void main() {\n                vec3 pixel = gl_GlobalInvocationID;\n                int h = int(pixel.x);\n                int w = int(pixel.y);\n                int c = int(pixel.z);\n\n                imageOut[h][w][c] = imageIn[h][w][c];\n            }\n            """"""\n\n        shader = self.shader_from_txt(glsl, verbose=False)\n        classes = [lv.BufferCPU, lv.StagedBuffer]\n        sync_modes = [lv.BufferCPU.SYNC_LAZY, lv.BufferCPU.SYNC_EAGER]\n\n        data = np.ones((72, 128, 3), dtype=np.float32)\n\n        for cls_in, cls_out, sync_mode in itertools.product(classes, classes, sync_modes):\n            buffer_in = cls_in.from_shader(self.session, shader, binding=0, sync_mode=sync_mode)\n            buffer_out = cls_in.from_shader(self.session, shader, binding=1, sync_mode=sync_mode)\n\n            stage = lv.Stage(shader, {0: buffer_in, 1: buffer_out})\n            stage.record(*data.shape)\n\n            buffer_in[0] = data\n\n            if sync_mode is lv.BufferCPU.SYNC_LAZY:\n                self.assertTrue(not buffer_in.is_synced())\n            if sync_mode is lv.BufferCPU.SYNC_EAGER:\n                self.assertTrue(buffer_in.is_synced())\n\n            stage.run_and_wait()\n\n            if sync_mode is lv.BufferCPU.SYNC_LAZY:\n                self.assertTrue(buffer_in.is_synced())\n                self.assertTrue(not buffer_out.is_synced())\n\n            if sync_mode is lv.BufferCPU.SYNC_EAGER:\n                self.assertTrue(buffer_out.is_synced())\n\n            self.assertTrue((buffer_in[0] == buffer_out[0]).all())\n\n            if sync_mode is lv.BufferCPU.SYNC_LAZY:\n                self.assertTrue(buffer_out.is_synced())\n\n    def test_partially_modified_buffer(self):\n        glsl = """"""\n            #version 450\n            #extension GL_ARB_separate_shader_objects : enable\n\n            layout(local_size_x=1, local_size_y=1, local_size_z=1) in;\n            \n            struct Struct1 {\n                uint var1;\n                uint var2;\n            };\n            \n            struct Struct2 {\n                double var1;\n                Struct1[2] var2;\n            };\n\n            layout(std140, binding = 0) buffer readonly BufferA {\n                float var1;\n                Struct2 var2;\n                vec2[3] var3;\n            } inputData;\n            \n            layout(std140, binding = 1) buffer writeonly BufferB {\n                float var1;\n                Struct2 var2;\n                vec2[3] var3;\n            } outputData;\n\n            void main() {\n                outputData.var1 = inputData.var1;\n                outputData.var2 = inputData.var2;\n                outputData.var3 = inputData.var3;\n            }\n            """"""\n\n        shader = self.shader_from_txt(glsl, verbose=False)\n        classes = [lv.BufferCPU, lv.StagedBuffer]\n        sync_mode = lv.BufferCPU.SYNC_LAZY\n\n        for cls_in, cls_out in itertools.product(classes, classes):\n            buffer_in = cls_in.from_shader(self.session, shader, binding=0, sync_mode=sync_mode)\n            buffer_out = cls_in.from_shader(self.session, shader, binding=1, sync_mode=sync_mode)\n\n            stage = lv.Stage(shader, {0: buffer_in, 1: buffer_out})\n            stage.record(1, 1, 1)\n\n            buffer_in[""var1""] = 0.0\n            buffer_in[""var2""][""var1""] = 1.0\n            buffer_in[""var2""][""var2""][0][""var1""] = 11\n            buffer_in[""var2""][""var2""][0][""var2""] = 12\n            buffer_in[""var2""][""var2""][1][""var1""] = 21\n            buffer_in[""var2""][""var2""][1][""var2""] = 22\n            buffer_in[""var3""] = np.array(((1.0, 1.0), (2.0, 2.0), (3.0, 3.0)), dtype=np.float32)\n\n            self.assertFalse(buffer_in.is_synced())\n\n            stage.run_and_wait()\n\n            # change in nested struct\n            new_value = 9.0\n            buffer_in[""var2""][""var1""] = new_value\n            self.assertFalse(buffer_in.is_synced())\n\n            stage.run_and_wait()\n\n            self.assertEqual(buffer_in[""var2""][""var1""], new_value)\n            self.assertEqual(buffer_out[""var2""][""var1""], new_value)\n\n            # change in array of structs\n            new_value = 99\n            buffer_in[""var2""][""var2""][0][""var2""] = new_value\n            self.assertFalse(buffer_in.is_synced())\n\n            stage.run_and_wait()\n\n            self.assertEqual(buffer_in[""var2""][""var2""][0][""var2""], new_value)\n            self.assertEqual(buffer_out[""var2""][""var2""][0][""var2""], new_value)\n\n            # change on highest level\n            new_value = 999.0\n            buffer_in[""var1""] = new_value\n            self.assertFalse(buffer_in.is_synced())\n\n            stage.run_and_wait()\n\n            self.assertEqual(buffer_in[""var1""], new_value)\n            self.assertEqual(buffer_out[""var1""], new_value)\n\n# more tests:\n# missing readonly, writeonly decorations\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/init.py,0,"b'# -*- coding: UTF-8 -*-\n\nimport builtins\nimport contextlib\nimport os\nimport sys\nimport unittest\n\nfrom test.util import write_to_temp_file\n\n\nclass InitializationTest(unittest.TestCase):\n\n    PKG_NAME = ""lava""\n\n    @staticmethod\n    @contextlib.contextmanager\n    def env_backup():\n        environ_backup = dict(os.environ)\n        try:\n            yield\n        finally:\n            os.environ.clear()\n            os.environ.update(environ_backup)\n\n    @classmethod\n    def clear_lava(cls):\n        if cls.PKG_NAME in sys.modules:\n            del sys.modules[cls.PKG_NAME]\n\n    def setUp(self):\n        self.clear_lava()\n\n    def tearDown(self):\n        self.clear_lava()\n\n    def test_validation_level(self):\n        import lava as lv\n        lv.VALIDATION_LEVEL = lv.VALIDATION_LEVEL_INFO\n\n        debugger = lv.instance().debugger\n        self.assertTrue(len(debugger.history) > 0)\n        self.assertTrue(""CREATE PhysicalDevice object"" in "" "".join(debugger.history))\n\n        lv.VALIDATION_LEVEL = None\n\n    def test_import_missing_variable(self):\n        error_on_import = None\n        error_on_instance = None\n        error_on_devices = None\n\n        with self.env_backup():\n            del os.environ[""VULKAN_SDK""]\n\n            try:\n                import lava as lv\n            except Exception as e:\n                error_on_import = e\n\n            try:\n                lv.instance()\n            except Exception as e:\n                error_on_instance = e\n\n            try:\n                lv.devices()\n            except Exception as e:\n                error_on_devices = e\n\n        self.assertIsNone(error_on_import)\n        self.assertTrue(isinstance(error_on_instance, ImportError))\n        self.assertTrue(isinstance(error_on_devices, ImportError))\n        self.assertFalse(lv.initialized())\n\n    def test_import_vulkan_unavailable(self):\n        errors_on_module_import = []\n        error_on_instance = None\n        error_on_devices = None\n\n        # mock vulkan initialization error\n        # https://stackoverflow.com/a/2481588\n        import_original = builtins.__import__\n\n        def import_mock(name, *args, **kwargs):\n            if name == ""vulkan"":\n                # https://github.com/realitix/vulkan/blob/1.1.99.0/generator/vulkan.template.py#L105\n                raise OSError(""Cannot find Vulkan SDK version. Please ensure..."")\n            return import_original(name, *args, **kwargs)\n\n        builtins.__import__ = import_mock\n\n        # import should not fail\n        import lava as lv\n\n        # find and try all possible module imports, e.g. import lava.api.shader, ...\n        pkg_name = self.PKG_NAME\n        pkg_path = lv.__path__[0]\n        ext = "".py""\n        paths = []\n\n        for root, dirs, files in os.walk(pkg_path):\n            for name in files:\n                if name.endswith(ext):\n                    paths.append([pkg_name] + root[len(pkg_path):].split(os.sep)[1:] + [name[:-len(ext)]])\n\n        for path in paths:\n            try:\n                __import__(""."".join(path))\n            except Exception as e:\n                errors_on_module_import.append(e)\n\n        try:\n            lv.instance()\n        except Exception as e:\n            error_on_instance = e\n\n        try:\n            lv.devices()\n        except Exception as e:\n            error_on_devices = e\n\n        self.assertTrue(len(errors_on_module_import) == 0, ""\\n"".join([str(e) for e in errors_on_module_import]))\n        self.assertTrue(isinstance(error_on_instance, OSError))\n        self.assertTrue(isinstance(error_on_devices, OSError))\n        self.assertFalse(lv.initialized())\n\n        builtins.__import__ = import_original\n\n    def test_bytecode_parsing_without_gpu(self):\n        with self.env_backup():\n            del os.environ[""VULKAN_SDK""]\n\n            import lava as lv\n            from lava.api.bytecode.logical import ByteCode\n            from lava.api.bytecode.physical import ByteCodeData\n            from lava.api.bytes import Vector, Scalar, Struct\n            from lava.api.constants.spirv import Layout\n\n            self.assertTrue(not lv.initialized())\n\n        glsl = """"""\n            #version 450\n            #extension GL_ARB_separate_shader_objects : enable\n\n            layout(local_size_x=1, local_size_y=1, local_size_z=1) in;\n\n            layout(std430, binding = 0) readonly buffer bufIn {\n                vec3 var1;\n            };\n\n            layout(std430, binding = 1) writeonly buffer bufOut {\n                float var2;\n            };\n\n            void main() {\n                var2 = var1.x + var1.y + var1.z;\n            }\n            """"""\n\n        path_shader = write_to_temp_file(glsl, suffix="".comp"")\n        path_shader_spirv = lv.compile_glsl(path_shader, verbose=True)\n\n        with self.env_backup():\n            del os.environ[""VULKAN_SDK""]\n\n            self.assertTrue(not lv.initialized())\n\n            byte_code_data = ByteCodeData.from_file(path_shader_spirv)\n            byte_code = ByteCode(byte_code_data, None)\n\n            quiet = True\n            container0 = Struct([Vector.vec3()], Layout.STD430)\n            container1 = Struct([Scalar.float()], Layout.STD430)\n            self.assertTrue(container0.compare(byte_code.get_block_definition(0), quiet=quiet))\n            self.assertTrue(container1.compare(byte_code.get_block_definition(1), quiet=quiet))\n\n        os.remove(path_shader)\n        os.remove(path_shader_spirv)\n'"
test/shader.py,6,"b'# -*- coding: UTF-8 -*-\n\nimport os\nimport unittest\n\nimport numpy as np\n\nimport lava as lv\nfrom lava.api.bytes import Array, ScalarFloat, Struct\nfrom lava.api.constants.spirv import Layout\nfrom lava.api.util import LavaError\n\nfrom test.util import write_to_temp_file\n\n\nclass ShaderTest(unittest.TestCase):\n\n    def setUp(self):\n        super(ShaderTest, self).setUp()\n        self.session = lv.Session(lv.devices()[0])\n\n    def tearDown(self):\n        super(ShaderTest, self).tearDown()\n        self.session.destroy()\n\n    def shader_from_txt(self, txt, verbose=True, clean_up=True):\n        path_shader = write_to_temp_file(txt, suffix="".comp"")\n        shader_path_spirv = lv.compile_glsl(path_shader, verbose)\n        shader = lv.Shader(self.session, shader_path_spirv)\n        if clean_up:\n            os.remove(path_shader)\n            os.remove(shader_path_spirv)\n        return shader\n\n    def test_definition_check(self):\n        glsl = """"""\n            #version 450\n            #extension GL_ARB_separate_shader_objects : enable\n\n            layout(std140, binding = 0) buffer BufferA {\n                float[720][1280][3] image;\n            };\n\n            void main() {}\n            """"""\n        shader = self.shader_from_txt(glsl, verbose=False)\n        incompatible_definition = Struct([Array(ScalarFloat(), (721, 1281, 4), Layout.STD140)], Layout.STD140)\n        incompatible_buffer = lv.BufferCPU(self.session, incompatible_definition, lv.BufferCPU.USAGE_STORAGE)\n        self.assertRaises(LavaError, lv.Stage, shader=shader, bindings={0: incompatible_buffer})\n\n    def test_convolution(self):\n        glsl = """"""\n            #version 450\n            #extension GL_ARB_separate_shader_objects : enable\n\n            layout(local_size_x=1, local_size_y=1, local_size_z=1) in;\n\n            layout(std430, binding = 0) buffer readonly BufferA {\n                float[72][128][3] imageIn;\n            };\n\n            layout(std430, binding = 1) buffer readonly BufferB {\n                float[5][5] convWeights;\n            };\n\n            layout(std430, binding = 2) buffer writeonly BufferC {\n                float[72][128][3] imageOut;\n            };\n\n            int height = imageIn.length();\n            int width = imageIn[0].length();\n            int channels = imageIn[0][0].length();\n\n            bool inputHasPixel(int y, int x, int c) {\n                return !(y < 0 || y >= height || x < 0 || x >= width || c < 0 || c >= channels);\n            }\n\n            float inputPixel(int y, int x, int c, float defaultValue) {\n                if (inputHasPixel(y, x, c)) {\n                    return imageIn[y][x][c];\n                }\n\n                return defaultValue;\n            }\n\n            void main() {\n                int filterHeight = convWeights.length();\n                int filterWidth = convWeights[0].length();\n                int fdh = (filterWidth - 1) / 2;\n                int fdw = (filterHeight - 1) / 2;\n\n                vec3 pixel = gl_GlobalInvocationID;\n                int h = int(pixel.x);\n                int w = int(pixel.y);\n                int c = int(pixel.z);\n\n                if (!inputHasPixel(h, w, c)) {\n                    return;\n                }\n\n                float filteredPixel = 0;\n\n                for (int i = 0; i < filterHeight; i++) {\n                    for (int j = 0; j < filterWidth; j++) {\n                        float a = inputPixel(h - fdh + i, w - fdw + j, c, 0.0);\n                        float b = convWeights[i][j];\n                        filteredPixel += a * b;\n                    }\n                }\n\n                imageOut[h][w][c] = filteredPixel;\n            }\n            """"""\n\n        shader = self.shader_from_txt(glsl, verbose=False)\n\n        buf_in = lv.StagedBuffer.from_shader(self.session, shader, binding=0)\n        buf_weights = lv.StagedBuffer.from_shader(self.session, shader, binding=1)\n        buf_out = lv.StagedBuffer.from_shader(self.session, shader, binding=2)\n\n        im = np.random.randint(0, 255, size=(72, 128, 3)).astype(np.float32)\n        weights = np.ones((5, 5), dtype=np.float32) / (5 * 5)\n\n        buf_in[""imageIn""] = im\n        buf_weights[""convWeights""] = weights\n\n        stage = lv.Stage(shader, {0: buf_in, 1: buf_weights, 2: buf_out})\n        stage.record(*im.shape)\n        stage.run_and_wait()\n\n        im_filtered = buf_out[""imageOut""]\n\n        # convolution on cpu\n        padding = 2\n        h, w, c = im.shape\n        im_padded = np.zeros((h + 2 * padding, w + 2 * padding, c), dtype=np.float32)\n        im_padded[padding:-padding, padding:-padding, :] = im\n        im_filtered_expected = np.zeros(im.shape, dtype=np.float32)\n\n        for i in range(h):\n            for j in range(w):\n                for k in range(c):\n                    im_filtered_expected[i, j, k] = np.sum(\n                        im_padded[i:i+2*padding+1, j:j+2*padding+1, k] * weights)\n\n        self.assertTrue(np.mean(np.abs(im_filtered - im_filtered_expected)) < 1e-3)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/util.py,0,"b'# -*- coding: UTF-8 -*-\n\nimport tempfile\n\n\ndef write_to_temp_file(txt, mode=""w"", prefix=""lavatest-"", suffix=""""):\n    with tempfile.NamedTemporaryFile(mode=mode, prefix=prefix, suffix=suffix, delete=False) as f:\n        f.write(txt)\n        return f.name\n'"
lava/api/__init__.py,0,b''
lava/api/bytes.py,43,"b'# -*- coding: UTF-8 -*-\n\nimport numpy as np\n\nfrom lava.api.constants.spirv import DataType, Layout, Order\nfrom lava.api.util import NdArray, LavaError\n\n\nclass BytesError(LavaError):\n\n    def __init__(self, message):\n        super(BytesError, self).__init__(message)\n\n    @classmethod\n    def out_of_bounds(cls, value, glsl_dtype):\n        raise cls(""Value {} is out of memory bounds for type {}"".format(value, glsl_dtype))\n\n\nclass ByteRepresentation(object):\n\n    """"""Data structure implementation of Vulkan specification 14.5.4.""""""\n\n    # References\n    # https://www.khronos.org/registry/vulkan/specs/1.1/html/chap14.html#interfaces-resources-layout\n    # https://www.khronos.org/registry/OpenGL/extensions/ARB/ARB_uniform_buffer_object.txt\n    #   search document for ""Sub-section 2.15.3.1.2""\n    #   search document for ""The following example illustrates the rules specified by the ""std140"" layout""\n\n    def __init__(self):\n        pass\n\n    def copy(self):\n        raise NotImplementedError()\n\n    def size(self):\n        raise NotImplementedError()\n\n    def alignment(self):\n        # called \'base alignment\' in the specs\n        raise NotImplementedError()\n\n    def __str__(self, indent=2):\n        raise NotImplementedError()\n\n    # def glsl(self, var_name):\n    #     return ""{} {};"".format(self.glsl_dtype(), var_name)\n\n    def glsl_dtype(self):\n        raise NotImplementedError()\n\n    @classmethod\n    def path_to_str(cls, path):\n        return "" > "".join(path)\n\n    @classmethod\n    def compare_type(cls, type_expected, type_other, path, quiet=True):\n        if type_expected != type_other:\n            if not quiet:\n                raise TypeError(\n                    ""Expected type {}, but got type {} at {}"".format(type_expected, type_other, cls.path_to_str(path)))\n            return False\n\n        return True\n\n    @classmethod\n    def compare_layout(cls, layout_expected, layout_other, path, quiet=True):\n        if layout_expected == Layout.STDXXX:\n            return True\n\n        if layout_expected != layout_other and layout_other != Layout.STDXXX:\n            if not quiet:\n                raise TypeError(\n                    ""Expected layout {}, but got layout {} at {}"".format(layout_expected, layout_other,\n                                                                         cls.path_to_str(path)))\n            return False\n\n        return True\n\n    @classmethod\n    def compare_order(cls, order_expected, order_other, path, quiet=True):\n        if order_expected != order_other:\n            if not quiet:\n                raise TypeError(\n                    ""Expected layout {}, but got layout {} at {}"".format(order_expected, order_other,\n                                                                         cls.path_to_str(path)))\n            return False\n        return True\n\n    @classmethod\n    def compare_shape(cls, shape_expected, shape_other, path, quiet):\n        if shape_expected != shape_other:\n            if not quiet:\n                raise TypeError(""Expected shape {}, but got shape {} at {}"".format(shape_expected, shape_other,\n                                                                                   cls.path_to_str(path)))\n            return False\n        return True\n\n    def compare(self, other, path, quiet=True):\n        raise NotImplementedError()\n\n    def to_bytes(self, values, path=()):\n        raise NotImplementedError()\n\n    def from_bytes(self, bytez):\n        raise NotImplementedError()\n\n\nclass Scalar(ByteRepresentation):\n\n    def __init__(self, input_dtypes):\n        super(Scalar, self).__init__()\n        self.input_dtypes = input_dtypes\n\n    @classmethod\n    def int(cls):\n        return cls.of(DataType.INT)\n\n    @classmethod\n    def uint(cls):\n        return cls.of(DataType.UINT)\n\n    @classmethod\n    def float(cls):\n        return cls.of(DataType.FLOAT)\n\n    @classmethod\n    def double(cls):\n        return cls.of(DataType.DOUBLE)\n\n    @classmethod\n    def of(cls, dtype):\n        if dtype == DataType.INT:\n            return ScalarInt()\n        if dtype == DataType.UINT:\n            return ScalarUnsignedInt()\n        if dtype == DataType.FLOAT:\n            return ScalarFloat()\n        if dtype == DataType.DOUBLE:\n            return ScalarDouble()\n        raise ValueError(""Unknown scalar type \'{}\'"".format(dtype))\n\n    def copy(self):\n        return self.__class__()\n\n    def alignment(self):\n        # ""A scalar of size N has a base alignment of N.""\n        return self.size()\n\n    def __str__(self, name=None, indent=2):\n        return ""{} [{}]"".format(self.glsl_dtype(), name or ""?"")\n\n    def glsl_dtype(self):\n        raise NotImplementedError()\n\n    def compare(self, other, path=(), quiet=True):\n        return self.compare_type(type(self), type(other), path, quiet)\n\n    def to_bytes(self, value, path=()):\n        if not isinstance(value, self.input_dtypes):\n            raise TypeError(""{} got dtype {}, expected one of the following {} at {}"".format(\n                self.__class__.__name__, type(value), self.input_dtypes, self.path_to_str(path)\n            ))\n\n    def from_bytes(self, bytez):\n        return np.frombuffer(bytez, self.numpy_dtype())[0]\n\n    def numpy_dtype(self):\n        raise NotImplementedError()\n\n\nclass ScalarInt(Scalar):\n\n    def __init__(self):\n        super(ScalarInt, self).__init__((int, np.int32))\n\n    def numpy_dtype(self):\n        return np.int32\n\n    def size(self):\n        return 4\n\n    def glsl_dtype(self):\n        return ""int""\n\n    def to_bytes(self, value, path=()):\n        super(ScalarInt, self).to_bytes(value, path)\n        if type(value) is int:\n            if not (-(0x7FFFFFFF + 1) <= value <= 0x7FFFFFFF):\n                raise BytesError.out_of_bounds(value, self.glsl_dtype())\n            value = np.int32(value)\n        return bytearray(value.tobytes())\n\n\nclass ScalarUnsignedInt(Scalar):\n\n    def __init__(self):\n        super(ScalarUnsignedInt, self).__init__((int, np.uint32, bool, np.bool, np.bool_))\n\n    def numpy_dtype(self):\n        return np.uint32\n\n    def size(self):\n        return 4\n\n    def glsl_dtype(self):\n        return ""uint""\n\n    def to_bytes(self, value, path=()):\n        super(ScalarUnsignedInt, self).to_bytes(value, path)\n        if not (0 <= value <= 0xFFFFFFFF):\n            raise BytesError.out_of_bounds(value, self.glsl_dtype())\n        if type(value) != np.uint32:\n            value = np.uint32(value)\n        return bytearray(value.tobytes())\n\n\nclass ScalarFloat(Scalar):\n\n    def __init__(self):\n        super(ScalarFloat, self).__init__((float, np.float32))\n\n    def numpy_dtype(self):\n        return np.float32\n\n    def size(self):\n        return 4\n\n    def glsl_dtype(self):\n        return ""float""\n\n    def to_bytes(self, value, path=()):\n        super(ScalarFloat, self).to_bytes(value, path)\n        if type(value) is float:\n            # TODO: add range check\n            value = np.float32(value)\n        return bytearray(value.tobytes())\n\n\nclass ScalarDouble(Scalar):\n\n    def __init__(self):\n        super(ScalarDouble, self).__init__((float, np.float64))\n\n    def numpy_dtype(self):\n        return np.float64\n\n    def size(self):\n        return 8\n\n    def glsl_dtype(self):\n        return ""double""\n\n    def to_bytes(self, value, path=()):\n        super(ScalarDouble, self).to_bytes(value, path)\n        if type(value) is float:\n            # TODO: add range check\n            value = np.float64(value)\n        return bytearray(value.tobytes())\n\n\nclass Vector(ByteRepresentation):\n\n    def __init__(self, n, dtype):\n        super(Vector, self).__init__()\n        self.dtype = dtype\n        self.n = n\n        self.scalar = Scalar.of(dtype)\n\n    @classmethod\n    def ivec2(cls):\n        return Vector(2, DataType.INT)\n\n    @classmethod\n    def ivec3(cls):\n        return Vector(3, DataType.INT)\n\n    @classmethod\n    def ivec4(cls):\n        return Vector(4, DataType.INT)\n\n    @classmethod\n    def uvec2(cls):\n        return Vector(2, DataType.UINT)\n\n    @classmethod\n    def uvec3(cls):\n        return Vector(3, DataType.UINT)\n\n    @classmethod\n    def uvec4(cls):\n        return Vector(4, DataType.UINT)\n\n    @classmethod\n    def vec2(cls):\n        return Vector(2, DataType.FLOAT)\n\n    @classmethod\n    def vec3(cls):\n        return Vector(3, DataType.FLOAT)\n\n    @classmethod\n    def vec4(cls):\n        return Vector(4, DataType.FLOAT)\n\n    @classmethod\n    def dvec2(cls):\n        return Vector(2, DataType.DOUBLE)\n\n    @classmethod\n    def dvec3(cls):\n        return Vector(3, DataType.DOUBLE)\n\n    @classmethod\n    def dvec4(cls):\n        return Vector(4, DataType.DOUBLE)\n\n    def copy(self):\n        return Vector(self.n, self.dtype)\n\n    def size(self):\n        return self.scalar.size() * self.n\n\n    def length(self):\n        return self.n\n\n    def alignment(self):\n        if self.n == 2:\n            # ""A two-component vector, with components of size N, has a base alignment of 2 N.""\n            return self.scalar.size() * 2\n        if self.n in (3, 4):\n            # ""A three- or four-component vector, with components of size N, has a base alignment of 4 N.""\n            return self.scalar.size() * 4\n        return -1\n\n    def __str__(self, name=None, indent=2):\n        return ""{} [{}]"".format(self.glsl_dtype(), name or ""?"")\n\n    def glsl_dtype(self):\n        return ""{}vec{}"".format(self.dtype.lower()[0] if self.dtype is not DataType.FLOAT else """", self.n)\n\n    def compare(self, other, path=(), quiet=True):\n        if not self.compare_type(type(self), type(other), path, quiet):\n            return False\n\n        count_expected = self.n\n        count_other = other.n\n\n        if count_expected != count_other:\n            if not quiet:\n                raise TypeError(""Expected vector length {}, but got length {} at {}"".format(\n                    count_expected, count_other, self.path_to_str(path)))\n            return False\n\n        return True\n\n    def to_bytes(self, array, path=()):\n        expected_types = (np.ndarray, list, tuple)\n        if not isinstance(array, expected_types):\n            raise TypeError(""Got datatype {} for {} variable, expected {} at {}""\n                            .format(type(array), self.glsl_dtype(), expected_types, self.path_to_str(path)))\n        if len(array) != self.n:\n            raise TypeError(""Got length {} for {} variable, expected {} at {}""\n                            .format(len(array), self.glsl_dtype(), self.n, self.path_to_str(path)))\n\n        bytez = bytearray()\n\n        for value in array:\n            bytez += self.scalar.to_bytes(value, path)\n\n        return bytez\n\n    def from_bytes(self, bytez):\n        return np.frombuffer(bytez, self.scalar.numpy_dtype())[:self.n]\n\n\nclass Matrix(ByteRepresentation):\n\n    DEFAULT_ORDER = Order.COLUMN_MAJOR\n\n    def __init__(self, cols, rows, dtype, layout, order=DEFAULT_ORDER):\n        super(Matrix, self).__init__()\n        if dtype not in (DataType.FLOAT, DataType.DOUBLE):\n            raise TypeError(""Matrices of type {} are not supported"".format(dtype))\n        self.dtype = dtype\n        self.cols = cols\n        self.rows = rows\n        self.order = order\n        self.layout = layout\n\n    @property\n    def order(self):\n        return self._order\n\n    @order.setter\n    def order(self, order):\n        self._order = order\n        self.vector = Vector(self.rows if order == Order.COLUMN_MAJOR else self.cols, self.dtype)\n        self.vector_count = self.cols if order == Order.COLUMN_MAJOR else self.rows\n\n    @property\n    def layout(self):\n        return self._layout\n\n    @layout.setter\n    def layout(self, layout):\n        self._layout = layout\n        self.a = None\n        self.precompute_alignment()\n\n    def precompute_alignment(self):\n        if self.layout in [Layout.STD140, Layout.STD430, Layout.STDXXX]:\n            self.a = self.vector.alignment()\n\n            if self.layout == Layout.STD140:\n                self.a += (16 - self.a % 16) % 16\n\n    def copy(self):\n        return Matrix(self.cols, self.rows, self.dtype, self.layout, self.order)\n\n    def size(self):\n        return self.step_size() * self.vector_count\n\n    def stride(self):\n        return self.step_size()\n\n    def step_size(self):\n        s = self.vector.size()\n        s += (self.a - s % self.a) % self.a  # pad to array stride\n        return s\n\n    def shape(self):\n        return self.rows, self.cols\n\n    def alignment(self):\n        return self.a\n\n    def __str__(self, name=None, indent=2):\n        return ""{} [{}]"".format(self.glsl_dtype(), name or ""?"")\n\n    def glsl_dtype(self):\n        return ""{}mat{}x{}"".format(self.dtype.lower()[0] if self.dtype is not DataType.FLOAT else """", self.cols, self.rows)\n\n    def compare(self, other, path=(), quiet=True):\n        if not self.compare_type(type(self), type(other), path, quiet):\n            return False\n        if not self.compare_layout(self.layout, other.layout, path, quiet):\n            return False\n        if not self.compare_shape(self.shape(), other.shape(), path, quiet):\n            return False\n        return True\n\n    def to_bytes(self, array, path=()):\n        expected_types = (np.ndarray, list, tuple)\n        if not (isinstance(array, expected_types) or isinstance(array[0], expected_types)):\n            raise TypeError(""Got datatype {} for {} variable, expected {} at {}""\n                            .format(type(array), self.glsl_dtype(), expected_types, self.path_to_str(path)))\n        shape = (len(array), len(array[0]))\n        expected_shape = self.shape()\n        if shape != expected_shape:\n            raise TypeError(""Got shape {} for {} variable, expected {} at {}""\n                            .format(len(array), self.glsl_dtype(), expected_shape, self.path_to_str(path)))\n\n        bytez = bytearray()\n\n        if self.order == Order.COLUMN_MAJOR:\n            for col in range(self.cols):\n                bytez += self.vector.to_bytes([array[row][col] for row in range(self.rows)], path)\n                padding = (self.a - len(bytez) % self.a) % self.a\n                bytez += bytearray(padding)\n\n        if self.order == Order.ROW_MAJOR:\n            for row in range(self.rows):\n                bytez += self.vector.to_bytes([array[row][col] for col in range(self.cols)], path)\n                padding = (self.a - len(bytez) % self.a) % self.a\n                bytez += bytearray(padding)\n\n        return bytez\n\n    def from_bytes(self, bytez):\n        array = np.zeros((self.rows, self.cols), dtype=self.vector.scalar.numpy_dtype())\n        offset = 0\n        size = self.vector.size()\n\n        if self.order == Order.COLUMN_MAJOR:\n            for col in range(self.cols):\n                array[:, col] = self.vector.from_bytes(bytez[offset:offset + size])\n                offset += size\n                offset += (self.a - offset % self.a) % self.a  # bytes\n\n            return array\n\n        if self.order == Order.ROW_MAJOR:\n            for row in range(self.rows):\n                array[row, :] = self.vector.from_bytes(bytez[offset:offset + size])\n                offset += size\n                offset += (self.a - offset % self.a) % self.a  # bytes\n\n            return array\n\n        return None\n\n\nclass Array(ByteRepresentation):\n\n    def __init__(self, definition, dims, layout):\n        super(Array, self).__init__()\n        self.definition = definition\n        self.dims = tuple(dims) if isinstance(dims, (list, tuple)) else (dims,)\n        self.layout = layout  # precomputes alignment\n\n    @property\n    def layout(self):\n        return self._layout\n\n    @layout.setter\n    def layout(self, layout):\n        self._layout = layout\n\n        # set children first\n        if isinstance(self.definition, (Matrix, Struct)):\n            self.definition.layout = layout\n\n        self.a = None\n        self.precompute_alignment()\n\n    def precompute_alignment(self):\n        if self.layout in [Layout.STD140, Layout.STD430, Layout.STDXXX]:\n            self.a = self.definition.alignment()\n\n            if self.layout == Layout.STD140:\n                self.a += (16 - self.a % 16) % 16\n\n    def shape(self):\n        return self.dims\n\n    def copy(self):\n        return Array(self.definition.copy(), self.dims, self.layout)\n\n    def size(self):\n        return self.step_size() * np.product(self.shape())\n\n    def strides(self):\n        strides = [self.step_size()]\n        shape = self.shape()\n\n        for i in reversed(range(1, len(shape))):\n            strides.insert(0, shape[i] * strides[0])\n\n        return strides\n\n    def step_size(self):\n        s = self.definition.size()\n        s += (self.a - s % self.a) % self.a  # pad to array stride\n        return s\n\n    def alignment(self):\n        return self.a\n\n    @classmethod\n    def is_array_of_structs(cls, definition):\n        if isinstance(definition, Array):\n            if isinstance(definition.definition, Struct):\n                return True\n        return False\n\n    def __str__(self, name=None, indent=2):\n        s = self.definition.glsl_dtype()  # ""array""\n        s += (""[{}]"" * len(self.shape())).format(*self.shape())\n        s += "" [{}] {{ {} }}"".format(name or ""?"", self.definition.__str__(indent=indent + 2))\n        return s\n\n    def glsl_dtype(self):\n        return (""{}"" + ""[{}]"" * len(self.shape())).format(self.definition.glsl_dtype(), *self.shape())\n\n    def compare(self, other, path=(), quiet=True):\n        if not self.compare_type(type(self), type(other), path, quiet):\n            return False\n        if not self.compare_layout(self.layout, other.layout, path, quiet):\n            return False\n        if not self.compare_shape(self.shape(), other.shape(), path, quiet):\n            return False\n\n        return self.definition.compare(other.definition,\n                                       list(path) + [""array {}"".format(""x"".join(map(str, self.shape())))], quiet)\n\n    def to_bytes(self, values, path=()):\n        if isinstance(self.definition, Scalar):\n            return self.to_bytes_for_scalars(values, path)\n\n        elif isinstance(self.definition, Vector):\n            return self.to_bytes_for_vectors(values, path)\n\n        elif isinstance(self.definition, Matrix):\n            return self.to_bytes_for_matrices(values, path)\n\n        else:\n            bytez = bytearray()\n\n            for indices in NdArray.iterate(self.shape()):\n                bytez += self.definition.to_bytes(NdArray.get(values, indices),\n                                                  list(path) + [""array"" + """".join([""[{}]"".format(i) for i in indices])])\n                padding = (self.a - len(bytez) % self.a) % self.a\n                bytez += bytearray(padding)\n\n            return bytez\n\n    def to_bytes_for_scalars(self, array, path):\n        transfer_dtype = self.definition.numpy_dtype()\n\n        if not isinstance(array, np.ndarray):\n            raise TypeError(""Got datatype {} for {} variable, expected {} at {}""\n                            .format(type(array), self.glsl_dtype(), np.ndarray, self.path_to_str(path)))\n        if array.dtype.type not in self.definition.input_dtypes:\n            raise TypeError(""Got datatype {} for {} variable, expected {} at {}""\n                            .format(array.dtype, self.glsl_dtype(), self.definition.input_dtypes, self.path_to_str(path)))\n        if tuple(array.shape) != self.shape():\n            raise TypeError(""Got shape {} for {} variable, expected {} at {}""\n                            .format(array.shape, self.glsl_dtype(), self.shape(), self.path_to_str(path)))\n\n        if self.layout == Layout.STD430:\n            return array.astype(transfer_dtype).flatten().tobytes()\n\n        else:\n            p = (self.a - self.definition.alignment()) // self.definition.size()\n            a = self.a // self.definition.size()\n\n            array_padded = np.zeros(a * np.product(array.shape), dtype=transfer_dtype)\n            mask = (np.arange(len(array_padded)) % a) < (a - p)\n            array_padded[mask] = array.flatten()\n\n            return array_padded.tobytes()\n\n    def to_bytes_for_vectors(self, array, path):\n        numpy_dtypes = self.definition.scalar.input_dtypes\n        transfer_dtype = self.definition.scalar.numpy_dtype()\n        shape = tuple(list(self.shape()) + [self.definition.length()])\n\n        if not isinstance(array, np.ndarray):\n            raise TypeError(""Got datatype {} for {} variable, expected {} at {}""\n                            .format(type(array), self.glsl_dtype(), np.ndarray, self.path_to_str(path)))\n        if array.dtype.type not in numpy_dtypes:\n            raise TypeError(""Got datatype {} for {} variable, expected {} at {}""\n                            .format(array.dtype, self.glsl_dtype(), numpy_dtypes, self.path_to_str(path)))\n        if tuple(array.shape) != shape:\n            raise TypeError(""Got shape {} for {} variable, expected {} at {}""\n                            .format(array.shape, self.glsl_dtype(), shape, self.path_to_str(path)))\n\n        p = (self.a - self.definition.size()) // self.definition.scalar.size()\n        a = self.a // self.definition.scalar.size()\n\n        array_padded = np.zeros(a * np.product(shape[:-1]), dtype=transfer_dtype)\n        mask = (np.arange(len(array_padded)) % a) < (a - p)\n        array_padded[mask] = array.flatten()\n\n        return array_padded.tobytes()\n\n    def to_bytes_for_matrices(self, array, path):\n        numpy_dtype = self.definition.vector.scalar.numpy_dtype()\n        shape = tuple(list(self.shape()) + list(self.definition.shape()))\n\n        if not isinstance(array, np.ndarray):\n            raise RuntimeError(""Got datatype {} for {} variable, expected {} at {}""\n                               .format(type(array), self.glsl_dtype(), np.ndarray, self.path_to_str(path)))\n        if array.dtype != numpy_dtype:\n            raise RuntimeError(""Got datatype {} for {} variable, expected {} at {}""\n                               .format(array.dtype, self.glsl_dtype(), numpy_dtype, self.path_to_str(path)))\n        if tuple(array.shape) != shape:\n            raise RuntimeError(""Got shape {} for {} variable, expected {} at {}""\n                               .format(array.shape, self.glsl_dtype(), shape, self.path_to_str(path)))\n\n        # swap the last two dimensions if necessary\n        if self.definition.order == Order.COLUMN_MAJOR:\n            array = np.swapaxes(array, -2, -1)\n            shape = array.shape\n\n        p = (self.a - self.definition.vector.size()) // self.definition.vector.scalar.size()\n        a = self.a // self.definition.vector.scalar.size()\n\n        array_padded = np.zeros(a * np.product(shape[:-1]), dtype=array.dtype)\n        mask = (np.arange(len(array_padded)) % a) < (a - p)\n        array_padded[mask] = array.flatten()\n\n        return array_padded.tobytes()\n\n    def from_bytes(self, bytez):\n        if isinstance(self.definition, Scalar):\n            return self.from_bytes_for_scalars(bytez)\n\n        elif isinstance(self.definition, Vector):\n            return self.from_bytes_for_vectors(bytez)\n\n        elif isinstance(self.definition, Matrix):\n            return self.from_bytes_for_matrices(bytez)\n\n        else:\n            values = np.zeros(self.shape()).tolist()\n            offset = 0\n            size = self.definition.size()\n\n            for indices in NdArray.iterate(self.shape()):\n                NdArray.assign(values, indices, self.definition.from_bytes(bytez[offset:offset + size]))\n                offset += size\n                offset += (self.a - offset % self.a) % self.a  # bytes\n\n            return values\n\n    def from_bytes_for_scalars(self, bytez):\n        if self.layout == Layout.STD430:\n            array_flat = np.frombuffer(bytez, dtype=self.definition.numpy_dtype())\n\n        else:\n            p = (self.a - self.definition.alignment()) // self.definition.size()\n            a = self.a // self.definition.size()\n\n            array_padded = np.frombuffer(bytez, dtype=self.definition.numpy_dtype())\n            mask = (np.arange(a * np.product(self.shape())) % a) < (a - p)\n            array_flat = array_padded[mask]\n\n        return array_flat.reshape(self.shape())\n\n    def from_bytes_for_vectors(self, bytez):\n        shape = tuple(list(self.shape()) + [self.definition.length()])\n        p = (self.a - self.definition.size()) // self.definition.scalar.size()\n        a = self.a // self.definition.scalar.size()\n\n        array_padded = np.frombuffer(bytez, dtype=self.definition.scalar.numpy_dtype())\n        mask = (np.arange(a * np.product(shape[:-1])) % a) < (a - p)\n        return array_padded[mask].reshape(shape)\n\n    def from_bytes_for_matrices(self, bytez):\n        shape = tuple(list(self.shape()) + list(self.definition.shape()))\n        p = (self.a - self.definition.vector.size()) // self.definition.vector.scalar.size()\n        a = self.a // self.definition.vector.scalar.size()\n\n        if self.definition.order == Order.COLUMN_MAJOR:\n            shape = tuple(list(shape[:-2]) + [shape[-1], shape[-2]])\n\n        array_padded = np.frombuffer(bytez, dtype=self.definition.vector.scalar.numpy_dtype())\n        mask = (np.arange(a * np.product(shape[:-1])) % a) < (a - p)\n        array = array_padded[mask].reshape(shape)\n\n        if self.definition.order == Order.COLUMN_MAJOR:\n            array = np.swapaxes(array, -2, -1)\n\n        return array\n\n\nclass Struct(ByteRepresentation):\n\n    def __init__(self, definitions, layout, member_names=None, type_name=None):\n        super(Struct, self).__init__()\n        self.definitions = definitions\n        self.member_names = member_names or ([None] * len(definitions))\n        self.type_name = type_name\n        self.layout = layout  # precomputes alignment\n\n        if len(set(definitions)) != len(definitions):\n            raise BytesError(""For struct members a definition object can not be used more than once"")\n\n    @property\n    def layout(self):\n        return self._layout\n\n    @layout.setter\n    def layout(self, layout):\n        self._layout = layout\n\n        # set children first\n        for definition in self.definitions:\n            if isinstance(definition, (Array, Matrix, Struct)):\n                definition.layout = layout\n\n        self.a = None\n        self.precompute_alignment()\n\n    def precompute_alignment(self):\n        if self.layout in [Layout.STD140, Layout.STD430, Layout.STDXXX]:\n            self.a = max([d.alignment() for d in self.definitions])\n\n            if self.layout == Layout.STD140:\n                self.a += (16 - self.a % 16) % 16\n\n    def copy(self):\n        return Struct([d.copy() for d in self.definitions], self.layout, self.member_names, self.type_name)\n\n    def size(self):\n        return self.steps()[-1]\n\n    def offsets(self):\n        return self.steps()[:-1]\n\n    def steps(self):\n        steps = [0]\n\n        for d in self.definitions:\n            a = d.alignment()\n            padding_before = (a - steps[-1] % a) % a\n            steps[-1] = steps[-1] + padding_before  # update last step + size to next step\n            steps.append(steps[-1] + d.size())\n\n        a = self.alignment()\n        padding_after = (a - steps[-1] % a) % a\n        steps[-1] = steps[-1] + padding_after\n\n        return steps\n\n    def alignment(self):\n        return self.a\n\n    def __str__(self, name=None, indent=2):\n        # s = ""struct [{}] {{\\n"".format(name or ""?"")\n        s = ""{} [{}] {{\\n"".format(self.type_name or ""<struct_type>"", name or ""?"")\n        for i, (definition, member_name) in enumerate(zip(self.definitions, self.member_names)):\n            s += ""{}({}) {}\\n"".format("" "" * indent, i, definition.__str__(name=member_name, indent=indent + 2))\n        s += ""{}}}"".format("" "" * (indent - 2))\n        return s\n\n    def glsl_dtype(self):\n        if self.type_name is None:\n            raise BytesError(""Type name was not defined for structure"")\n        return self.type_name\n\n    def __extend_path(self, path, member_index):\n        if self.member_names[member_index]:\n            step = ""\'{}\'"".format(self.member_names[member_index])\n        else:\n            step = str(member_index)\n        return list(path) + [""member "" + step]\n\n    def compare(self, other, path=(), quiet=True):\n        if not self.compare_type(type(self), type(other), path, quiet):\n            return False\n        if not self.compare_layout(self.layout, other.layout, path, quiet):\n            return False\n\n        definitions_expected = self.definitions\n        definitions_other = other.definitions\n\n        if len(definitions_expected) != len(definitions_other):\n            if not quiet:\n                raise TypeError(""Expected {} members, but got {} members at {}"".format(\n                    len(definitions_expected), len(definitions_other), self.path_to_str(path)\n                ))\n            return False\n\n        for i, (definition_expected, definition_other) in enumerate(zip(definitions_expected, definitions_other)):\n            if not definition_expected.compare(definition_other, self.__extend_path(path, i), quiet):\n                return False\n\n        return True\n\n    def to_bytes(self, values, path=()):\n        bytez = bytearray()\n\n        for i, d in enumerate(self.definitions):\n            a = d.alignment()\n            padding_before = (a - len(bytez) % a) % a\n            bytez += bytearray(padding_before)\n            bytez += d.to_bytes(values[d], self.__extend_path(path, i))\n\n        # padding at the end\n        a = self.alignment()\n        padding_after = (a - len(bytez) % a) % a\n        bytez += bytearray(padding_after)\n\n        return bytez\n\n    def from_bytes(self, bytez):\n        values = {}\n        offset = 0\n\n        for i, d in enumerate(self.definitions):\n            a = d.alignment()\n            offset += (a - offset % a) % a  # padding before\n            size = d.size()\n            values[d] = d.from_bytes(bytez[offset:offset + size])\n            offset += size\n\n        # nothing needs to be done about the padding at the end\n        return values\n\n\nclass ByteCache(object):\n\n    def __init__(self, definition):\n        if type(definition) != Struct:\n            raise BytesError(""ByteCaches can only be initialized with struct definitions"")\n        self.definition = definition\n        self.values = {}\n        self.dirty = False\n\n        for i, d in enumerate(self.definition.definitions):\n            value = None\n\n            if isinstance(d, Struct):\n                value = ByteCache(d)\n\n            if Array.is_array_of_structs(d):\n                value = np.zeros(d.shape()).tolist()\n\n                for indices in NdArray.iterate(d.shape()):\n                    NdArray.assign(value, indices, ByteCache(d.definition))\n\n            self.values[d] = value\n\n    def get_as_dict(self):\n        data = {}\n\n        for d in self.definition.definitions:\n            value = self.values[d]\n\n            if isinstance(d, Struct):\n                value = self.values[d].get_as_dict()\n\n            if Array.is_array_of_structs(d):\n                value = np.zeros(d.shape()).tolist()\n\n                for indices in NdArray.iterate(d.shape()):\n                    cache = NdArray.get(self.values[d], indices)\n                    NdArray.assign(value, indices, cache.get_as_dict())\n\n            data[d] = value\n\n        return data\n\n    def set_from_dict(self, values):\n        for d in self.definition.definitions:\n            if isinstance(d, Struct):\n                self.values[d].set_from_dict(values[d])\n\n            elif isinstance(d, Array) and isinstance(d.definition, Struct):\n                for indices in NdArray.iterate(d.shape()):\n                    value = NdArray.get(values[d], indices)\n                    cache = NdArray.get(self.values[d], indices)\n                    cache.set_from_dict(value)\n\n            else:\n                self.values[d] = values[d]\n\n    def set_dirty(self, dirty, include_children=True):\n        self.dirty = dirty\n\n        if include_children:\n            for d in self.definition.definitions:\n                value = self.values[d]\n\n                if isinstance(d, Struct):\n                    value.set_dirty(dirty, include_children)\n\n                if Array.is_array_of_structs(d):\n                    for indices in NdArray.iterate(d.shape()):\n                        NdArray.get(value, indices).set_dirty(dirty, include_children)\n\n    def is_dirty(self, include_children=True):\n        if not include_children:\n            return self.dirty\n\n        dirty = self.dirty\n\n        for d in self.definition.definitions:\n            if dirty:\n                return True\n\n            value = self.values[d]\n\n            if isinstance(d, Struct):\n                dirty = dirty or value.is_dirty(include_children)\n\n            if Array.is_array_of_structs(d):\n                for indices in NdArray.iterate(d.shape()):\n                    dirty = dirty or NdArray.get(value, indices).is_dirty(include_children)\n\n        return dirty\n\n    def __str__(self):\n        s = self.__class__.__name__ + "" around\\n""\n        s += str(self.definition)\n        return s\n\n    def __definition_from_key(self, key):\n        if isinstance(key, int):\n            return self.definition.definitions[key]\n\n        if isinstance(key, str):\n            if key not in self.definition.member_names:\n                possible_reasons = [\n                    ""the definition was defined manually and the member names were not specified"",\n                    ""the definition was taken from shader bytecode, but it did not contain the member names ""\n                    ""(they are optional, the compiler must not put them in)""\n                ]\n                raise ValueError(\n                    ""Did not find {} in member names. In case this is not a typo please reference by index, member ""\n                    ""names can be empty in following scenarios:\\n{}"".format(\n                        key, """".join([""* {}\\n"".format(reason) for reason in possible_reasons])\n                    )\n                )\n            index = self.definition.member_names.index(key)\n            return self.definition.definitions[index]\n\n        if isinstance(key, ByteRepresentation):\n            return key\n\n        raise ValueError(""Invalid key"")\n\n    def __getitem__(self, key):\n        return self.values[self.__definition_from_key(key)]\n\n    def __setitem__(self, key, value):\n        self.values[self.__definition_from_key(key)] = value\n        self.dirty = True\n\n\n# specs\n# https://www.khronos.org/registry/vulkan/specs/1.1/html/chap14.html#interfaces-resources-layout\n# https://github.com/KhronosGroup/glslang/issues/201#issuecomment-204785552 (example)\n#\n# https://www.khronos.org/registry/OpenGL/extensions/ARB/ARB_uniform_buffer_object.txt\n#   CTRL+F ""Sub-section 2.15.3.1.2""\n#   CTRL+F ""The following example illustrates the rules specified by the ""std140"" layout""\n# https://www.khronos.org/registry/OpenGL/extensions/ARB/ARB_shader_storage_buffer_object.txt\n# https://www.khronos.org/registry/OpenGL/extensions/ARB/ARB_arrays_of_arrays.txt\n#\n# Wiki layout bindings etc\n# https://www.khronos.org/opengl/wiki/Interface_Block_(GLSL)\n# https://www.khronos.org/opengl/wiki/Data_Type_(GLSL)\n#\n# more stuff to come\n# http://www.paranormal-entertainment.com/idr/blog/posts/2014-01-29T17:08:42Z-GLSL_multidimensional_array_madness/\n'"
lava/api/device.py,0,"b'# -*- coding: UTF-8 -*-\n\nimport itertools\n\nimport lava.api.vulkan as vk\nfrom lava.api.constants.vk import DeviceType, MemoryType, QueueType, VALIDATION_LAYERS\nfrom lava.api.memory import Memory\nfrom lava.api.util import Destroyable\n\n\nclass PhysicalDevice(object):\n\n    def __init__(self, handle):\n        self.handle = handle\n\n        # meta data\n        properties = vk.vkGetPhysicalDeviceProperties(self.handle)\n        self.device_name = properties.deviceName\n        self.device_type = DeviceType.from_vulkan(properties.deviceType)\n        self.uniform_max_bytes = properties.limits.maxUniformBufferRange\n        self.work_group_max_invocations = properties.limits.maxComputeWorkGroupInvocations\n        self.work_group_max_sizes = [properties.limits.maxComputeWorkGroupSize[i] for i in range(3)]\n        self.work_group_max_counts = [properties.limits.maxComputeWorkGroupCount[i] for i in range(3)]\n\n        # queue information\n        queue_families = vk.vkGetPhysicalDeviceQueueFamilyProperties(self.handle)\n        self.queue_support = {}\n        self.queue_idx = {t: [] for t in QueueType.keys()}\n\n        for idx, queue_family in enumerate(queue_families):\n            for queue_type in QueueType.keys():\n                support = bool(queue_family.queueFlags & QueueType.to_vulkan(queue_type))\n                self.queue_support[queue_type] = self.queue_support.get(queue_type, False) or support\n                if support:\n                    self.queue_idx[queue_type].append(idx)\n\n        # memory properties\n        memory_properties = vk.vkGetPhysicalDeviceMemoryProperties(self.handle)\n        self.memory_type_idx = {t: [] for t in MemoryType.keys()}\n        self.memory_type_idx_to_heap = {}\n        self.heap_sizes = {}\n\n        for idx, memory_type in itertools.product(range(memory_properties.memoryTypeCount), MemoryType.keys()):\n            support = memory_properties.memoryTypes[idx].propertyFlags & MemoryType.to_vulkan(memory_type)\n            if support:\n                self.memory_type_idx[memory_type].append(idx)\n\n            heap_idx = memory_properties.memoryTypes[idx].heapIndex\n            self.memory_type_idx_to_heap[idx] = heap_idx\n\n            if heap_idx not in self.heap_sizes:\n                self.heap_sizes[heap_idx] = memory_properties.memoryHeaps[heap_idx].size\n\n    @classmethod\n    def all(cls, instance):\n        return [cls(handle) for handle in vk.vkEnumeratePhysicalDevices(instance.handle)]\n\n    def __str__(self):\n        return ""\\n"".join([""{}: {}"".format(key, self.__dict__[key]) for key in sorted(self.__dict__)])\n\n    def get_name(self):\n        return self.device_name\n\n    def get_type(self):\n        return self.device_type\n\n    def get_queue_indices(self, queue_type):\n        return self.queue_idx[queue_type]\n\n    def supports_queue_type(self, queue_type):\n        return self.queue_support[queue_type]\n\n    def get_memory_index(self, memory_types, supported_memory_indices):\n        sets_of_indices = []\n\n        for memory_type in memory_types:\n            sets_of_indices.append(set(self.memory_type_idx[memory_type]))\n\n        indices = sets_of_indices[0]\n        for other_indices in sets_of_indices[1:]:\n            indices.intersection_update(other_indices)\n\n        indices.intersection_update(set(supported_memory_indices))\n\n        if len(indices) == 0:\n            return None\n        else:\n            return min(list(indices))\n\n    def get_maximum_uniform_size(self):\n        return self.uniform_max_bytes\n\n    def get_maximum_work_group_sizes(self):\n        return self.work_group_max_sizes\n\n    def get_maximum_work_group_counts(self):\n        return self.work_group_max_counts\n\n    def get_maximum_work_group_invocations(self):\n        return self.work_group_max_invocations\n\n\nclass Device(Destroyable):\n\n    def __init__(self, physical_device, queue_definitions, validation_lvl=None, extensions=()):\n        super(Device, self).__init__()\n        self.validation_lvl = validation_lvl\n        self.physical_device = physical_device\n        self.memories = []\n        queue_create_infos = []\n\n        for queue_idx in set(map(lambda type_and_idx: type_and_idx[1], queue_definitions)):\n            queue_create_infos.append(vk.VkDeviceQueueCreateInfo(\n                queueFamilyIndex=queue_idx,\n                queueCount=1,\n                pQueuePriorities=[1.0]\n            ))\n\n        features = vk.VkPhysicalDeviceFeatures()\n\n        if validation_lvl:\n            create_info = vk.VkDeviceCreateInfo(\n                pQueueCreateInfos=queue_create_infos,\n                ppEnabledLayerNames=VALIDATION_LAYERS,\n                ppEnabledExtensionNames=extensions,\n                pEnabledFeatures=features\n            )\n        else:\n            create_info = vk.VkDeviceCreateInfo(\n                pQueueCreateInfos=queue_create_infos,\n                enabledLayerCount=0,\n                ppEnabledExtensionNames=extensions,\n                pEnabledFeatures=features,\n            )\n\n        self.handle = vk.vkCreateDevice(physical_device.handle, create_info, None)\n        self.queue_handles = {}\n        self.queue_indices = {}\n\n        for queue_type, queue_idx in queue_definitions:\n            self.queue_handles[queue_type] = vk.vkGetDeviceQueue(self.handle, queue_idx, 0)\n            self.queue_indices[queue_type] = queue_idx\n\n    def _destroy(self):\n        for memory in self.memories:\n            memory.destroy()\n        vk.vkDestroyDevice(self.handle, None)  # does also destroy the queues\n\n    def get_physical_device(self):\n        return self.physical_device\n\n    def get_queue_handle(self, queue_type):\n        return self.queue_handles[queue_type]\n\n    def get_queue_index(self, queue_type):\n        return self.queue_indices[queue_type]\n\n    def allocate_memory(self, memory_types, size, supported_memory_indices):\n        memory_index = self.physical_device.get_memory_index(memory_types, supported_memory_indices)\n\n        if memory_index is None:\n            raise RuntimeError(""Could not find a device memory which supports {}"".format("" and "".join(memory_types)))\n\n        memory = Memory(self, memory_index, size)\n        self.memories.append(memory)\n        return memory\n\n    def free_memory(self, memory):\n        pass\n'"
lava/api/instance.py,0,"b'# -*- coding: UTF-8 -*-\n\nimport lava.api.vulkan as vk\nfrom lava.api.constants.vk import VALIDATION_LAYERS\nfrom lava.api.util import Debugger, Destroyable\n\n\nclass Instance(Destroyable):\n\n    def __init__(self, extensions=(), validation_lvl=None):\n        super(Instance, self).__init__()\n        self.validation_lvl = validation_lvl\n        extensions = list(extensions)\n\n        app_info = vk.VkApplicationInfo(\n            pApplicationName=self.__class__.__name__,\n            apiVersion=vk.VK_API_VERSION_1_0\n        )\n\n        if validation_lvl:\n            create_info = vk.VkInstanceCreateInfo(\n                pApplicationInfo=app_info,\n                ppEnabledExtensionNames=extensions + [vk.VK_EXT_DEBUG_REPORT_EXTENSION_NAME],\n                ppEnabledLayerNames=VALIDATION_LAYERS,\n            )\n\n        else:\n            create_info = vk.VkInstanceCreateInfo(\n                pApplicationInfo=app_info,\n                ppEnabledExtensionNames=extensions,\n                enabledLayerCount=0\n            )\n\n        self.handle = vk.vkCreateInstance(create_info, None)\n        self.debugger = Debugger(self, lvl=validation_lvl) if validation_lvl else None\n\n    def _destroy(self):\n        if self.debugger:\n            self.debugger.destroy()\n        vk.vkDestroyInstance(self.handle, None)\n\n    def __getattr__(self, item):\n        # syntactic sugar to call vulkan instance functions as ""pseudo methods"" on this object, i.e.\n        if item in vk._vulkan._instance_ext_funcs:\n            def wrapper(*args, **kwargs):\n                return vk.vkGetInstanceProcAddr(self.handle, item)(*args, **kwargs)\n            return wrapper\n        else:\n            super(Instance, self).__getattribute__(item)\n'"
lava/api/memory.py,0,"b'# -*- coding: UTF-8 -*-\n\nimport contextlib\n\nimport lava.api.vulkan as vk\nfrom lava.api.constants.vk import BufferUsage, DescriptorType\nfrom lava.api.util import Destroyable\n\n\nclass Memory(Destroyable):\n\n    def __init__(self, device, memory_type_index, size):\n        super(Memory, self).__init__()\n        self.device = device\n        self.memory_type_index = memory_type_index\n        self.size = size\n        self.memory_obj = None\n\n        allocate_info = vk.VkMemoryAllocateInfo(allocationSize=size, memoryTypeIndex=memory_type_index)\n        self.handle = vk.vkAllocateMemory(self.device.handle, allocate_info, None)\n\n        # try:\n        #     self.handle = vk.vkAllocateMemory(self.device.handle, allocate_info, None)\n        # except vk.VkErrorOutOfDeviceMemory:\n        #     raise RuntimeError(""Device {} is out of memory"".format(self.device.get_physical_device().get_name()))\n        # except vk.VkErrorOutOfHostMemory:\n        #     raise RuntimeError(""Host is out of memory"")\n        # except vk.VkErrorTooManyObjects:\n        #     raise RuntimeError(""Too many allocations"")\n\n        # VK_ERROR_INVALID_EXTERNAL_HANDLE\n\n    def _destroy(self):\n        vk.vkFreeMemory(self.device.handle, self.handle, None)\n\n    def get_size(self):\n        return self.size\n\n    def get_device(self):\n        return self.device\n\n    def get_memory_obj(self):\n        return self.memory_obj\n\n    @contextlib.contextmanager\n    def mapped(self, size=-1, offset=0):\n        if size == -1:\n            size = self.size\n        # https://cffi.readthedocs.io/en/latest/ref.html#ffi-buffer-ffi-from-buffer\n        bytebuffer = vk.vkMapMemory(self.device.handle, self.handle, offset, size, flags=0)\n        yield bytebuffer\n        vk.vkUnmapMemory(self.device.handle, self.handle)\n\n\nclass MemoryObject(Destroyable):\n\n    def __init__(self, device, size):\n        super(MemoryObject, self).__init__()\n        self.device = device\n        self.size = size\n        self.memory = None\n        self.memory_offset = None\n\n    def _destroy(self):\n        raise NotImplementedError()\n\n    def get_memory(self):\n        return self.memory\n\n    def get_size(self):\n        return self.size\n\n    def bind_memory(self, memory, memory_offset):\n        if memory.get_memory_obj() is not None:\n            raise RuntimeError(""Memory is already bound"")\n        self.memory = memory\n        self.memory_offset = memory_offset\n\n    def descriptor_type(self):\n        raise NotImplementedError()\n\n    def descriptor_set_layout(self, binding):\n        raise NotImplementedError()\n\n    def write_descriptor_set(self, descriptor_set_handle, binding):\n        raise NotImplementedError()\n\n\nclass Buffer(MemoryObject):\n\n    def __init__(self, device, size, usage, queue_index):\n        super(Buffer, self).__init__(device, size)\n        self.usage = usage\n        self.queue_index = queue_index\n\n        usage_flags = BufferUsage.to_vulkan(usage)\n        usage_flags |= vk.VK_BUFFER_USAGE_TRANSFER_SRC_BIT\n        usage_flags |= vk.VK_BUFFER_USAGE_TRANSFER_DST_BIT\n\n        create_info = vk.VkBufferCreateInfo(\n            size=self.size, usage=usage_flags, sharingMode=vk.VK_SHARING_MODE_EXCLUSIVE,\n            pQueueFamilyIndices=[queue_index]\n        )\n\n        self.handle = vk.vkCreateBuffer(self.device.handle, create_info, None)\n\n    def _destroy(self):\n        vk.vkDestroyBuffer(self.device.handle, self.handle, None)\n\n    def get_memory_requirements(self):\n        requirements = vk.vkGetBufferMemoryRequirements(self.device.handle, self.handle)\n\n        supported_memory_indices = []\n        for i in range(32):\n            if requirements.memoryTypeBits & (1 << i) > 0:\n                supported_memory_indices.append(i)\n\n        return requirements.size, requirements.alignment, supported_memory_indices\n\n    def bind_memory(self, memory, offset=0):\n        super(Buffer, self).bind_memory(memory, offset)\n        vk.vkBindBufferMemory(device=self.device.handle, buffer=self.handle, memory=self.memory.handle,\n                              memoryOffset=offset)\n\n    @contextlib.contextmanager\n    def mapped(self):\n        with self.memory.mapped(self.size, self.memory_offset) as bytebuffer:\n            yield bytebuffer\n\n    def map(self, bytez):\n        with self.mapped() as bytebuffer:\n            bytebuffer[:] = bytez\n\n    def descriptor_type(self):\n        if self.usage == BufferUsage.STORAGE_BUFFER:\n            descriptor_type = DescriptorType.STORAGE_BUFFER\n        elif self.usage == BufferUsage.UNIFORM_BUFFER:\n            descriptor_type = DescriptorType.UNIFORM_BUFFER\n        else:\n            raise NotImplementedError(""Buffer usage {} is not supported"".format(self.usage))\n\n        return descriptor_type\n\n    def descriptor_set_layout(self, binding):\n        descriptor_type = self.descriptor_type()\n        return vk.VkDescriptorSetLayoutBinding(binding=binding,\n                                               descriptorType=DescriptorType.to_vulkan(descriptor_type),\n                                               descriptorCount=1, stageFlags=vk.VK_SHADER_STAGE_COMPUTE_BIT)\n\n    def write_descriptor_set(self, descriptor_set_handle, binding):\n        descriptor_type = self.descriptor_type()\n        buffer_info = vk.VkDescriptorBufferInfo(self.handle, 0, self.size)\n        return vk.VkWriteDescriptorSet(dstSet=descriptor_set_handle, dstBinding=binding,\n                                       descriptorType=DescriptorType.to_vulkan(descriptor_type),\n                                       pBufferInfo=[buffer_info])\n\n# PushConstants ?\n# SampledImage ?\n# StorageImage ?\n'"
lava/api/pipeline.py,0,"b'# -*- coding: UTF-8 -*-\n\nimport ctypes\n\nimport lava.api.vulkan as vk\nfrom lava.api.constants.vk import DescriptorType, QueueType, TIMEOUT_FOREVER\nfrom lava.api.util import Destroyable, Event, Fence\n\n\nclass Pipeline(Destroyable):\n\n    def __init__(self, device, shader, memory_objects, push_constants=()):\n        super(Pipeline, self).__init__()\n        self.device = device\n        self.shader = shader\n        self.memory_objects = memory_objects\n        self.descriptor_set_handle = None\n        self.descriptor_pool_handle = None\n        bindings = []\n\n        for i, memory_obj in memory_objects.items():\n            bindings.append(memory_obj.descriptor_set_layout(i))\n\n        descriptor_layout_create_info = vk.VkDescriptorSetLayoutCreateInfo(flags=None, pBindings=bindings)\n        self.descriptor_set_layout_handle = vk.vkCreateDescriptorSetLayout(self.device.handle,\n                                                                           descriptor_layout_create_info, None)\n\n        entrypoint = shader.get_entry_point().encode(""ascii"")\n        shader_stage_create_info = vk.VkPipelineShaderStageCreateInfo(stage=vk.VK_SHADER_STAGE_COMPUTE_BIT,\n                                                                      module=shader.handle,\n                                                                      pName=entrypoint)\n\n        if len(push_constants) > 0:\n            if len(push_constants) > 1:\n                raise NotImplementedError(""Using more than one push constant is not implemented"")\n\n            push_constant_range = vk.VkPushConstantRange(stageFlags=vk.VK_SHADER_STAGE_FRAGMENT_BIT, offset=0,\n                                                         size=ctypes.sizeof(push_constants[0]))\n\n            pipeline_layout_create_info = vk.VkPipelineLayoutCreateInfo(pSetLayouts=[self.descriptor_set_layout_handle],\n                                                                        pPushConstantRanges=[push_constant_range])\n\n        else:\n            pipeline_layout_create_info = vk.VkPipelineLayoutCreateInfo(pSetLayouts=[self.descriptor_set_layout_handle])\n\n        self.pipeline_layout_handle = vk.vkCreatePipelineLayout(self.device.handle, pipeline_layout_create_info, None)\n        pipeline_create_info = vk.VkComputePipelineCreateInfo(stage=shader_stage_create_info,\n                                                              layout=self.pipeline_layout_handle)\n\n        # https://github.com/realitix/vulkan/compare/1.1.99.0..1.1.99.1#diff-e2b1cc8bd860dd77d19209db868c1a58R5920\n        handles = vk.vkCreateComputePipelines(self.device.handle, None, 1, [pipeline_create_info], None)\n        try:\n            # vulkan 1.1.99.1 and onwards\n            self.handle = handles[0]\n        except TypeError:\n            # vulkan 1.1.99.0 and before\n            self.handle = handles\n\n    def _destroy(self):\n        if self.descriptor_pool_handle is not None:\n            vk.vkDestroyDescriptorPool(self.device.handle, self.descriptor_pool_handle, None)\n        vk.vkDestroyDescriptorSetLayout(self.device.handle, self.descriptor_set_layout_handle, None)\n        vk.vkDestroyPipelineLayout(self.device.handle, self.pipeline_layout_handle, None)\n        vk.vkDestroyPipeline(self.device.handle, self.handle, None)\n\n    def allocate_descriptor_sets(self):\n        pool_sizes = []\n        descriptor_types = {DescriptorType.UNIFORM_BUFFER: 0, DescriptorType.STORAGE_BUFFER: 0}\n\n        for memory_obj in self.memory_objects.values():\n            descriptor_type = memory_obj.descriptor_type()\n            descriptor_types[descriptor_type] += 1\n\n        for descriptor_type, count in descriptor_types.items():\n            if count > 0:\n                pool_sizes.append(vk.VkDescriptorPoolSize(DescriptorType.to_vulkan(descriptor_type), count))\n\n        pool_create_info = vk.VkDescriptorPoolCreateInfo(maxSets=1, pPoolSizes=pool_sizes)\n        self.descriptor_pool_handle = vk.vkCreateDescriptorPool(self.device.handle, pool_create_info, None)\n\n        allocate_info = vk.VkDescriptorSetAllocateInfo(descriptorPool=self.descriptor_pool_handle,\n                                                       pSetLayouts=[self.descriptor_set_layout_handle])\n\n        self.descriptor_set_handle = vk.vkAllocateDescriptorSets(self.device.handle, allocate_info)[0]\n\n    def update_descriptor_sets(self):\n        write_data = []\n\n        for i, memory_obj in self.memory_objects.items():\n            write_data.append(memory_obj.write_descriptor_set(self.descriptor_set_handle, i))\n\n        vk.vkUpdateDescriptorSets(self.device.handle, len(write_data), write_data, 0, None)\n\n    def get_memory_objects(self):\n        return self.memory_objects\n\n    def get_descriptor_set_handle(self):\n        if self.descriptor_set_handle is None:\n            raise RuntimeError(""Descriptor set was not allocated yet"")\n        return self.descriptor_set_handle\n\n    def get_pipeline_layout_handle(self):\n        return self.pipeline_layout_handle\n\n\nclass ShaderOperation(Destroyable):\n\n    def __init__(self, device, pipeline, queue_index):\n        super(ShaderOperation, self).__init__()\n        self.device = device\n        self.pipeline = pipeline\n        self.queue_index = queue_index\n        self.event = Event(self.device)\n        self.fence = Fence(self.device, signalled=False)\n\n        self.pipeline.allocate_descriptor_sets()\n        self.pipeline.update_descriptor_sets()\n\n        # VK_COMMAND_POOL_CREATE_TRANSIENT_BIT\n        command_pool_create_info = vk.VkCommandPoolCreateInfo(flags=vk.VK_COMMAND_POOL_CREATE_RESET_COMMAND_BUFFER_BIT,\n                                                              queueFamilyIndex=queue_index)\n        self.command_pool_handle = vk.vkCreateCommandPool(self.device.handle, command_pool_create_info, None)\n\n        allocate_info = vk.VkCommandBufferAllocateInfo(commandPool=self.command_pool_handle,\n                                                       level=vk.VK_COMMAND_BUFFER_LEVEL_PRIMARY, commandBufferCount=1)\n        self.command_buffer_handle = vk.vkAllocateCommandBuffers(self.device.handle, allocate_info)[0]\n\n    def _destroy(self):\n        vk.vkDestroyCommandPool(self.device.handle, self.command_pool_handle, None)\n        self.event.destroy()\n        self.fence.destroy()\n\n    def record(self, count_x, count_y, count_z, wait_events=()):\n        vk.vkBeginCommandBuffer(self.command_buffer_handle,\n                                vk.VkCommandBufferBeginInfo(flags=vk.VK_COMMAND_BUFFER_USAGE_SIMULTANEOUS_USE_BIT))\n\n        vk.vkCmdBindPipeline(self.command_buffer_handle, vk.VK_PIPELINE_BIND_POINT_COMPUTE, self.pipeline.handle)\n\n        vk.vkCmdBindDescriptorSets(self.command_buffer_handle, vk.VK_PIPELINE_BIND_POINT_COMPUTE,\n                                   self.pipeline.get_pipeline_layout_handle(), firstSet=0, descriptorSetCount=1,\n                                   pDescriptorSets=[self.pipeline.get_descriptor_set_handle()], dynamicOffsetCount=0,\n                                   pDynamicOffsets=None)\n\n        vk.vkCmdResetEvent(self.command_buffer_handle, self.event.handle, vk.VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT)\n\n        if len(wait_events) > 0:\n            # https://stackoverflow.com/questions/45680135/pipeline-barriers-across-multiple-shaders\n            vk.vkCmdWaitEvents(self.command_buffer_handle, len(wait_events), [event.handle for event in wait_events],\n                               srcStageMask=vk.VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,\n                               dstStageMask=vk.VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,\n                               memoryBarrierCount=0, pMemoryBarriers=None, bufferMemoryBarrierCount=0,\n                               pBufferMemoryBarriers=None, imageMemoryBarrierCount=0, pImageMemoryBarriers=None)\n\n        # vk.vkCmdPushConstants()\n\n        vk.vkCmdDispatch(self.command_buffer_handle, count_x, count_y, count_z)\n\n        vk.vkCmdSetEvent(self.command_buffer_handle, self.event.handle, vk.VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT)\n\n        vk.vkEndCommandBuffer(self.command_buffer_handle)\n\n    def run(self):\n        queue_handle = self.device.get_queue_handle(QueueType.COMPUTE)\n\n        submit_info = vk.VkSubmitInfo(pCommandBuffers=[self.command_buffer_handle])\n\n        vk.vkQueueSubmit(queue_handle, 1, [submit_info], self.fence.handle)\n\n    def wait(self):\n        vk.vkWaitForFences(self.device.handle, 1, [self.fence.handle], True, TIMEOUT_FOREVER)\n        vk.vkResetFences(self.device.handle, 1, [self.fence.handle])\n\n    def run_and_wait(self):\n        self.run()\n        self.wait()\n\n\nclass CopyOperation(Destroyable):\n\n    def __init__(self, device, queue_index):\n        super(CopyOperation, self).__init__()\n        self.device = device\n        self.queue_index = queue_index\n        self.fence = Fence(self.device, signalled=False)\n\n        command_pool_create_info = vk.VkCommandPoolCreateInfo(flags=vk.VK_COMMAND_POOL_CREATE_RESET_COMMAND_BUFFER_BIT,\n                                                              queueFamilyIndex=queue_index)\n        self.command_pool_handle = vk.vkCreateCommandPool(self.device.handle, command_pool_create_info, None)\n\n        allocation_info = vk.VkCommandBufferAllocateInfo(commandPool=self.command_pool_handle, commandBufferCount=1,\n                                                         level=vk.VK_COMMAND_BUFFER_LEVEL_PRIMARY)\n        self.command_buffer_handle = vk.vkAllocateCommandBuffers(self.device.handle, allocation_info)[0]\n\n    def _destroy(self):\n        vk.vkDestroyCommandPool(self.device.handle, self.command_pool_handle, None)\n        self.fence.destroy()\n\n    def record(self, src_buffer, dst_buffer):\n        vk.vkBeginCommandBuffer(self.command_buffer_handle,\n                                vk.VkCommandBufferBeginInfo(flags=vk.VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT))\n\n        region = vk.VkBufferCopy(0, 0, src_buffer.get_size())\n        vk.vkCmdCopyBuffer(self.command_buffer_handle, src_buffer.handle, dst_buffer.handle, 1, [region])\n\n        vk.vkEndCommandBuffer(self.command_buffer_handle)\n\n    def run(self):\n        queue_handle = self.device.get_queue_handle(QueueType.COMPUTE)\n\n        submit_info = vk.VkSubmitInfo(pCommandBuffers=[self.command_buffer_handle])\n\n        vk.vkQueueSubmit(queue_handle, 1, [submit_info], self.fence.handle)\n\n    def wait(self):\n        vk.vkWaitForFences(self.device.handle, 1, [self.fence.handle], True, TIMEOUT_FOREVER)\n        vk.vkResetFences(self.device.handle, 1, [self.fence.handle])\n\n    def run_and_wait(self):\n        self.run()\n        self.wait()\n'"
lava/api/shader.py,0,"b'# -*- coding: UTF-8 -*-\n\nimport lava.api.vulkan as vk\nfrom lava.api.bytecode.logical import ByteCode\nfrom lava.api.bytecode.physical import ByteCodeData\nfrom lava.api.util import Destroyable\n\n\nclass Shader(Destroyable):\n\n    def __init__(self, device, bytez, entry_point=None):\n        super(Shader, self).__init__()\n        self.device = device\n        self.handle = vk.vkCreateShaderModule(self.device.handle, vk.VkShaderModuleCreateInfo(codeSize=len(bytez),\n                                                                                              pCode=bytez), None)\n        self.byte_code_data = ByteCodeData(bytez)\n        self.byte_code = None\n\n        self.entry_point, self.entry_point_index = ByteCode.check_entry_point(self.byte_code_data, entry_point)\n        self.local_size = ByteCode.check_local_size(self.byte_code_data, self.entry_point_index)\n\n    @classmethod\n    def from_file(cls, device, path, entry_point=None):\n        with open(path, ""rb"") as f:\n            bytez = f.read()\n        return cls(device, bytez, entry_point)\n\n    def inspect(self):\n        self.byte_code = ByteCode(self.byte_code_data, self.entry_point)\n\n    @property\n    def code(self):\n        if self.byte_code is None:\n            raise RuntimeError(""Shader bytecode was not inspected yet"")\n        return self.byte_code\n\n    def get_entry_point(self):\n        return self.entry_point\n\n    def get_local_size(self):\n        return self.local_size\n\n    def _destroy(self):\n        vk.vkDestroyShaderModule(self.device.handle, self.handle, None)\n'"
lava/api/util.py,0,"b'# -*- coding: UTF-8 -*-\n\nfrom functools import reduce\nimport itertools\nimport logging\nimport operator\n\nimport lava.api.vulkan as vk\n\n\nclass Destroyable(object):\n\n    def __init__(self):\n        self.__destroyed = False\n\n    def __del__(self):\n        if not self.__destroyed:\n            self.destroy()\n\n    def destroy(self):\n        if not self.__destroyed:\n            self._destroy()\n            self.__destroyed = True\n\n    def _destroy(self):\n        raise NotImplementedError()\n\n\nclass LavaError(Exception):\n\n    def __init__(self, message):\n        super(LavaError, self).__init__(message)\n\n\nclass LavaUnsupportedError(LavaError):\n\n    def __init__(self, message):\n        super(LavaUnsupportedError, self).__init__(message)\n\n\nclass Debugger(Destroyable):\n\n    def __init__(self, instance, lvl=logging.INFO):\n        super(Debugger, self).__init__()\n        self.instance = instance\n        self.lvl = lvl\n        self.handle = None\n        self.history = []\n        self.history_size = 100\n        self.attach()\n\n    def _destroy(self):\n        self.detach()\n\n    def log(self, flags, object_type, object, location, message_code, layer, message, user_data):\n        lvl = logging.DEBUG\n\n        if flags & vk.VK_DEBUG_REPORT_INFORMATION_BIT_EXT:\n            lvl = logging.INFO\n        if flags & vk.VK_DEBUG_REPORT_WARNING_BIT_EXT:\n            lvl = logging.WARNING\n        if flags & vk.VK_DEBUG_REPORT_ERROR_BIT_EXT:\n            lvl = logging.ERROR\n\n        message_str = message[::]\n        self.history.insert(0, message_str)\n        if len(self.history) > self.history_size:\n            self.history = self.history[:self.history_size]\n\n        print(""[VULKAN] "" + message_str)\n        return 0\n\n    def attach(self):\n        all_bits = [vk.VK_DEBUG_REPORT_DEBUG_BIT_EXT, vk.VK_DEBUG_REPORT_INFORMATION_BIT_EXT,\n                    vk.VK_DEBUG_REPORT_WARNING_BIT_EXT, vk.VK_DEBUG_REPORT_ERROR_BIT_EXT]\n        all_lvls = [logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR]\n\n        create_info = vk.VkDebugReportCallbackCreateInfoEXT(\n            flags=reduce(operator.or_, all_bits[all_lvls.index(self.lvl):]),\n            pfnCallback=self.log\n        )\n\n        self.handle = self.instance.vkCreateDebugReportCallbackEXT(self.instance.handle, create_info, None)\n\n    def detach(self):\n        self.instance.vkDestroyDebugReportCallbackEXT(self.instance.handle, self.handle, None)\n\n\nclass Event(Destroyable):\n\n    def __init__(self, device):\n        super(Event, self).__init__()\n        self.device = device\n        self.handle = vk.vkCreateEvent(self.device.handle, vk.VkEventCreateInfo(), None)\n\n    def _destroy(self):\n        vk.vkDestroyEvent(self.device.handle, self.handle, None)\n\n\nclass Fence(Destroyable):\n\n    def __init__(self, device, signalled):\n        super(Fence, self).__init__()\n        self.device = device\n\n        if signalled:\n            create_info = vk.VkFenceCreateInfo(flags=vk.VK_FENCE_CREATE_SIGNALED_BIT)\n        else:\n            create_info = vk.VkFenceCreateInfo()\n\n        self.handle = vk.vkCreateFence(self.device.handle, create_info, None)\n\n    def _destroy(self):\n        vk.vkDestroyFence(self.device.handle, self.handle, None)\n\n\nclass NdArray(object):\n\n    @classmethod\n    def iterate(cls, dims):\n        return itertools.product(*[range(d) for d in dims])\n\n    @classmethod\n    def assign(cls, nd_array, indices, value):\n        data = nd_array\n\n        for index in indices[:-1]:\n            data = data[index]\n\n        data[indices[-1]] = value\n\n    @classmethod\n    def get(cls, nd_array, indices):\n        data = nd_array\n\n        for index in indices:\n            data = data[index]\n\n        return data\n'"
lava/api/vulkan.py,0,b'# -*- coding: UTF-8 -*-\n\n__initialized = False\n\ntry:\n    from vulkan import *\n    from vulkan import __version__\n    import vulkan._vulkan as _vulkan\n    __initialized = True\n\nexcept OSError:\n    # patch constants from vulkan package which are needed for the modules to be importable\n    VK_PHYSICAL_DEVICE_TYPE_INTEGRATED_GPU = 1\n    VK_PHYSICAL_DEVICE_TYPE_DISCRETE_GPU = 2\n    VK_PHYSICAL_DEVICE_TYPE_VIRTUAL_GPU = 3\n    VK_PHYSICAL_DEVICE_TYPE_CPU = 4\n\n    VK_QUEUE_GRAPHICS_BIT = 0x00000001\n    VK_QUEUE_COMPUTE_BIT = 0x00000002\n    VK_QUEUE_TRANSFER_BIT = 0x00000004\n    VK_QUEUE_SPARSE_BINDING_BIT = 0x00000008\n\n    VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT = 0x00000001\n    VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT = 0x00000002\n    VK_MEMORY_PROPERTY_HOST_COHERENT_BIT = 0x00000004\n    VK_MEMORY_PROPERTY_HOST_CACHED_BIT = 0x00000008\n    VK_MEMORY_PROPERTY_LAZILY_ALLOCATED_BIT = 0x00000010\n\n    VK_BUFFER_USAGE_TRANSFER_SRC_BIT = 0x00000001\n    VK_BUFFER_USAGE_TRANSFER_DST_BIT = 0x00000002\n    VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT = 0x00000010\n    VK_BUFFER_USAGE_STORAGE_BUFFER_BIT = 0x00000020\n\n    VK_DESCRIPTOR_TYPE_SAMPLER = 0\n    VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER = 6\n    VK_DESCRIPTOR_TYPE_STORAGE_BUFFER = 7\n\n    VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT = 0x00000001\n    VK_COMMAND_BUFFER_USAGE_SIMULTANEOUS_USE_BIT = 0x00000004\n\n\ndef initialized():\n    return __initialized\n'
test/api/__init__.py,0,b''
test/api/base.py,8,"b'# -*- coding: UTF-8 -*-\n\nimport itertools\nimport os\nimport pprint\nimport unittest\n\nimport numpy as np\n\nimport lava as lv\nfrom lava.api.bytes import Array, Matrix, Vector, Scalar, Struct\nfrom lava.api.constants.spirv import Layout\nfrom lava.api.constants.vk import BufferUsage, MemoryType\nfrom lava.api.memory import Buffer\nfrom lava.api.pipeline import ShaderOperation, Pipeline\nfrom lava.api.shader import Shader\n\nfrom test.util import write_to_temp_file\n\n\nclass GlslBasedTest(unittest.TestCase):\n\n    SESSION = None\n    MEMORY = None\n\n    LAYOUTS = [Layout.STD140, Layout.STD430]\n    LAYOUT_MAP = {Layout.STD140: ""std140"", Layout.STD430: ""std430""}\n\n    @classmethod\n    def setUpClass(cls):\n        # lv.VALIDATION_LEVEL = lv.VALIDATION_LEVEL_DEBUG\n        cls.SESSION = lv.Session(lv.devices()[0])\n        cls.MEMORY = {}\n\n    @classmethod\n    def tearDownClass(cls):\n        cls.SESSION.destroy()\n\n    # Util\n\n    @classmethod\n    def shader_from_txt(cls, txt, verbose=True, clean_up=True):\n        path_shader = write_to_temp_file(txt, suffix="".comp"")\n        shader_path_spirv = lv.compile_glsl(path_shader, verbose)\n        shader = Shader.from_file(cls.SESSION.device, shader_path_spirv)\n        if clean_up:\n            os.remove(path_shader)\n            os.remove(shader_path_spirv)\n        return shader\n\n    @classmethod\n    def allocate_buffer(cls, size, usage, types):\n        buf = Buffer(cls.SESSION.device, size, usage, cls.SESSION.queue_index)\n        min_size, _, supported_indices = buf.get_memory_requirements()\n        mem = cls.SESSION.device.allocate_memory(types, min_size, supported_indices)\n        buf.bind_memory(mem)\n        cls.MEMORY[buf] = (buf, mem)\n        return buf\n\n    @classmethod\n    def destroy_buffer(cls, buf):\n        buf, mem = cls.MEMORY[buf]\n        buf.destroy()\n        mem.destroy()\n\n    @classmethod\n    def run_program(cls, glsl, bytez_input, bytez_output_size, usage_input=BufferUsage.STORAGE_BUFFER,\n                    usage_output=BufferUsage.STORAGE_BUFFER, groups=(1, 1, 1), verbose=True):\n        shader = cls.shader_from_txt(glsl, verbose)\n        return cls.run_compiled_program(shader, bytez_input, bytez_output_size, usage_input, usage_output, groups)\n\n    @classmethod\n    def run_compiled_program(cls, shader, bytez_input, bytez_output_size, usage_input=BufferUsage.STORAGE_BUFFER,\n                             usage_output=BufferUsage.STORAGE_BUFFER, groups=(1, 1, 1)):\n        session = cls.SESSION\n\n        buffer_in = cls.allocate_buffer(len(bytez_input), usage_input, MemoryType.CPU)\n        buffer_out = cls.allocate_buffer(bytez_output_size, usage_output, MemoryType.CPU)\n\n        buffer_in.map(bytez_input)\n\n        pipeline = Pipeline(session.device, shader, {0: buffer_in, 1: buffer_out})\n        operation = ShaderOperation(session.device, pipeline, session.queue_index)\n\n        operation.record(*groups)\n        operation.run_and_wait()\n\n        with buffer_out.mapped() as bytebuffer:\n            bytez_output = bytebuffer[:]\n\n        operation.destroy()\n        pipeline.destroy()\n        cls.destroy_buffer(buffer_in)\n        cls.destroy_buffer(buffer_out)\n\n        return bytez_output\n\n    @classmethod\n    def generate_var_name(cls, definition, index, prefix=""""):\n        return ""{}{}{}"".format(prefix, definition.__class__.__name__.lower(), index)\n\n    @classmethod\n    def build_glsl_block_definition(cls, container, binding=0, usage=BufferUsage.STORAGE_BUFFER):\n        # TODO: extend for matrix order?\n        glsl = ""layout({}, binding = {}) {} dataIn {{"".format(\n            ""std140"" if container.layout == Layout.STD140 else ""std430"", binding,\n            ""buffer"" if usage == BufferUsage.STORAGE_BUFFER else ""uniform"", )\n        glsl += ""\\n""\n\n        for i, d in enumerate(container.definitions):\n            glsl += ""  {} {};"".format(d.glsl_dtype(), cls.generate_var_name(d, i))\n            glsl += ""\\n""\n\n        return glsl + ""};""\n\n    @classmethod\n    def build_glsl_struct_definition(cls, struct):\n        glsl = ""struct {} {{"".format(struct.glsl_dtype())\n        glsl += ""\\n""\n\n        for i, d in enumerate(struct.definitions):\n            glsl += ""  {} {};"".format(d.glsl_dtype(), cls.generate_var_name(d, i))\n            glsl += ""\\n""\n\n        return glsl + ""};""\n\n    @classmethod\n    def build_glsl_assignments(cls, definitions, var_name=None, array_name=""array"", array_index=0, parent=None, var_name_prefix="""", to_array=True):\n        glsl = """"\n        j = array_index\n\n        for i, d in enumerate(definitions):\n            if isinstance(d, Scalar):\n                var_name_complete = var_name or cls.generate_var_name(d, i, var_name_prefix)\n                glsl_code, step = cls.build_glsl_assignments_scalar(d, j, var_name_complete, array_name, to_array=to_array)\n\n            elif isinstance(d, Vector):\n                var_name_complete = var_name or cls.generate_var_name(d, i, var_name_prefix)\n                glsl_code, step = cls.build_glsl_assignments_vector(d, j, var_name_complete, array_name, to_array=to_array)\n\n            elif isinstance(d, Matrix):\n                var_name_complete = var_name or cls.generate_var_name(d, i, var_name_prefix)\n                glsl_code, step = cls.build_glsl_assignments_matrix(d, j, var_name_complete, array_name, to_array=to_array)\n\n            elif isinstance(d, Array):\n                var_name_complete = var_name or cls.generate_var_name(d, i, var_name_prefix)\n                if isinstance(d.definition, Scalar):\n                    glsl_code, step = cls.build_glsl_assignments_array_scalar(d, j, var_name_complete, array_name, to_array=to_array)\n                else:\n                    glsl_code, step = cls.build_glsl_assignments_array_complex(d, j, var_name_complete, array_name, to_array=to_array)\n\n            elif isinstance(d, Struct):\n                if isinstance(parent, Array):\n                    var_name_complete = var_name + "".""\n                    glsl_code, step_overall = cls.build_glsl_assignments(d.definitions, var_name=None, var_name_prefix=var_name_complete,\n                                                                         array_name=array_name, array_index=j, parent=d, to_array=to_array)\n                    step = step_overall - j\n                else:\n                    var_name_complete = var_name or cls.generate_var_name(d, i, var_name_prefix) + "".""\n                    glsl_code, step_overall = cls.build_glsl_assignments(d.definitions, var_name=None, var_name_prefix=var_name_complete,\n                                                                         array_name=array_name, array_index=j, parent=d, to_array=to_array)\n                    step = step_overall - j\n\n            else:\n                raise RuntimeError()\n\n            glsl += glsl_code\n            j += step\n\n        return glsl, j\n\n    @classmethod\n    def build_glsl_assignments_scalar(cls, dfn, i, var_name_complete, array_name=""array"", to_array=True):\n        if to_array:\n            glsl = ""{}[{}] = float({});"".format(array_name, i, var_name_complete)\n        else:\n            glsl = ""{} = {}({}[{}]);"".format(var_name_complete, dfn.glsl_dtype(), array_name, i)\n        glsl += ""\\n""\n        return glsl, 1\n\n    @classmethod\n    def build_glsl_assignments_vector(cls, dfn, i, var_name_complete, array_name=""array"", to_array=True):\n        glsl = """"\n        n = dfn.length()\n        for j in range(n):\n            if to_array:\n                glsl += ""{}[{}] = float({}.{});"".format(array_name, i + j, var_name_complete, ""xyzw""[j])\n            else:\n                glsl += ""{}.{} = {}({}[{}]);"".format(var_name_complete, ""xyzw""[j], dfn.scalar.glsl_dtype(), array_name, i + j)\n            glsl += ""\\n""\n        return glsl, n\n\n    @classmethod\n    def build_glsl_assignments_matrix(cls, dfn, i, var_name_complete, array_name=""array"", to_array=True):\n        glsl = """"\n        cols, rows = dfn.cols, dfn.rows\n        for k, (r, c) in enumerate(itertools.product(range(rows), range(cols))):\n            if to_array:\n                glsl += ""{}[{}] = float({}[{}][{}]);"".format(array_name, i + k, var_name_complete, c, r)\n            else:\n                glsl += ""{}[{}][{}] = {}({}[{}]);"".format(var_name_complete, c, r, dfn.vector.scalar.glsl_dtype(), array_name, i + k)\n            glsl += ""\\n""\n        return glsl, cols * rows\n\n    @classmethod\n    def build_glsl_assignments_array_scalar(cls, dfn, i, var_name_complete, array_name=""array"", to_array=True):\n        glsl = """"\n        dims = dfn.shape()\n        glsl_dtype = dfn.definition.glsl_dtype()\n\n        for k, indices in enumerate(itertools.product(*[range(d) for d in dims])):\n            var_name_complete_with_indices = (""{}"" + ""[{}]"" * len(dims)).format(var_name_complete, *indices)\n            if to_array:\n                glsl += ""{}[{}] = float({});"".format(array_name, i + k, var_name_complete_with_indices)\n            else:\n                glsl += ""{} = {}({}[{}]);"".format(var_name_complete_with_indices, glsl_dtype, array_name, i + k)\n            glsl += ""\\n""\n        return glsl, np.product(dims)\n\n    @classmethod\n    def build_glsl_assignments_array_complex(cls, array, i, var_name_complete, array_name=""array"", to_array=True):\n        glsl = """"\n        old_i = i\n        for indices in itertools.product(*[range(d) for d in array.dims]):\n            new_var_name_complete = (""{}"" + ""[{}]"" * len(array.dims)).format(var_name_complete, *indices)\n            new_glsl, new_i = cls.build_glsl_assignments([array.definition], new_var_name_complete, array_name=array_name, array_index=i, parent=array, to_array=to_array)\n            glsl += new_glsl\n            i = new_i\n        return glsl, i - old_i\n\n    @classmethod\n    def build_values(cls, definitions, offset=0):\n        count = offset\n        values_raw = []\n        values_mapped = {}\n\n        for i, d in enumerate(definitions):\n            if isinstance(d, Scalar):\n                values_mapped[d] = d.numpy_dtype()(count)\n                values_raw.append(values_mapped[d])\n                count += 1\n\n            elif isinstance(d, Vector):\n                values_mapped[d] = np.arange(count, count + d.length(), dtype=d.scalar.numpy_dtype())\n                values_raw.extend(values_mapped[d])\n                count += d.length()\n\n            elif isinstance(d, Matrix):\n                rows, cols = d.shape()\n                values_flat = np.arange(count, count + rows * cols, dtype=d.vector.scalar.numpy_dtype())\n                values_mapped[d] = values_flat.reshape(rows, cols)\n                values_raw.extend(values_flat)\n                count += rows * cols\n\n            elif isinstance(d, Array):\n                if isinstance(d.definition, Scalar):\n                    data = np.zeros(d.shape(), dtype=d.definition.numpy_dtype())\n                    for indices in itertools.product(*[range(s) for s in d.shape()]):\n                        data[indices] = count\n                        values_raw.append(count)\n                        count += 1\n                    values_mapped[d] = data\n\n                elif isinstance(d.definition, Vector):\n                    shape = list(d.shape()) + [d.definition.length()]\n                    data = np.zeros(shape, dtype=d.definition.scalar.numpy_dtype())\n                    for indices in itertools.product(*[range(s) for s in shape]):\n                        data[indices] = count\n                        values_raw.append(count)\n                        count += 1\n                    values_mapped[d] = data\n\n                elif isinstance(d.definition, Matrix):\n                    shape = list(d.shape()) + list(d.definition.shape())\n                    data = np.zeros(shape, dtype=d.definition.vector.scalar.numpy_dtype())\n                    for indices in itertools.product(*[range(s) for s in shape]):\n                        data[indices] = count\n                        values_raw.append(count)\n                        count += 1\n                    values_mapped[d] = data\n\n                else:\n                    data = np.zeros(d.shape()).tolist()\n                    for indices in itertools.product(*[range(s) for s in d.shape()]):\n                        tmp1, tmp2 = cls.build_values([d.definition], offset=count)\n                        _data = data\n                        for index in indices[:-1]:\n                            _data = _data[index]\n                        _data[indices[-1]] = tmp1[d.definition]\n                        values_raw.extend(tmp2)\n                        count += len(tmp2)\n                    values_mapped[d] = data\n\n            elif isinstance(d, Struct):\n                tmp1, tmp2 = cls.build_values(d.definitions, offset=count)\n                values_mapped[d] = tmp1\n                values_raw.extend(tmp2)\n                count += len(tmp2)\n            else:\n                raise RuntimeError()\n\n        return values_mapped, values_raw\n\n    @classmethod\n    def build_register(cls, register, definition, steps):\n        """"""Readable keys for values dict""""""\n        simple_types = (Scalar, Vector, Matrix)\n\n        if isinstance(definition, Struct):\n            if definition not in register:\n                register[definition] = ""struct{}"".format(steps[Struct])\n                steps[Struct] += 1\n\n            for d in definition.definitions:\n                if isinstance(d, simple_types):\n                    cls.build_register_simple(register, d, steps)\n                else:\n                    cls.build_register(register, d, steps)\n\n        elif isinstance(definition, Array):\n            if definition not in register:\n                register[definition] = ""array{}"".format(steps[Array])\n                steps[Array] += 1\n\n            d = definition.definition\n\n            if isinstance(d, simple_types):\n                cls.build_register_simple(register, d, steps)\n            else:\n                cls.build_register(register, d, steps)\n\n        else:\n            RuntimeError(""Unexpected type {}"".format(type(definition)))\n\n    @classmethod\n    def build_register_simple(cls, register, definition, steps):\n        if isinstance(definition, Scalar) and definition not in register:\n            register[definition] = ""{}{}"".format(definition.glsl_dtype(), steps[Scalar])\n            steps[Scalar] += 1\n\n        elif isinstance(definition, Vector) and definition not in register:\n            register[definition] = ""vector{}"".format(steps[Vector])\n            steps[Vector] += 1\n\n        elif isinstance(definition, Matrix) and definition not in register:\n            register[definition] = ""matrix{}"".format(steps[Matrix])\n            steps[Matrix] += 1\n\n    @classmethod\n    def format_values(cls, definition, values, register):\n        """"""Values with readable keys and no numpy arrays (allows comparison with \'==\' operator)""""""\n        simple_types = (Scalar, Vector, Matrix)\n\n        if isinstance(definition, Struct):\n            data = []\n\n            for d in definition.definitions:\n                if isinstance(d, simple_types):\n                    # data[register[d]] = cls.format_values_simple(d, values[d])\n                    data.append((register[d], cls.format_values_simple(d, values[d])))\n                else:\n                    # data[register[d]] = cls.format_values(d, values[d], register)\n                    data.append((register[d], cls.format_values(d, values[d], register)))\n\n            return data\n\n        elif isinstance(definition, Array):\n            d = definition.definition\n            data = np.zeros(definition.shape()).tolist()\n\n            for indices in itertools.product(*[range(s) for s in definition.shape()]):\n                _tmp1 = values\n                _tmp2 = data\n\n                for index in indices[:-1]:\n                    _tmp1 = _tmp1[index]\n                    _tmp2 = _tmp2[index]\n\n                if isinstance(d, simple_types):\n                    _tmp2[indices[-1]] = cls.format_values_simple(d, _tmp1[indices[-1]])\n                else:\n                    _tmp2[indices[-1]] = cls.format_values(d, _tmp1[indices[-1]], register)\n\n            return data\n\n        else:\n            RuntimeError(""Unexpected type {}"".format(type(definition)))\n\n    @classmethod\n    def format_values_simple(cls, definition, value):\n        if isinstance(definition, Scalar):\n            return value\n        if isinstance(definition, Vector):\n            return value.tolist()\n        if isinstance(definition, Matrix):\n            return value.tolist()\n\n    @classmethod\n    def print_formatted_values(cls, values_ftd):\n        pprint.pprint(values_ftd, width=200)\n\n\nclass Random(object):\n\n    @classmethod\n    def shape(cls, rng, max_dims, max_dim, min_dims=1, min_dim=1):\n        dims = rng.randint(min_dims, max_dims + 1)\n        return tuple(rng.randint(min_dim, max_dim, dims))\n\n'"
test/api/combined.py,9,"b'# -*- coding: UTF-8 -*-\n\nimport itertools\nimport unittest\n\nimport numpy as np\n\nfrom lava.api.bytes import ByteCache, Matrix, Scalar, Struct, Vector\nfrom lava.api.constants.spirv import DataType, Layout, Order\n\nfrom test.api.base import GlslBasedTest\n\n\nclass CombinedTest(GlslBasedTest):\n\n    LAYOUTS = [Layout.STD140, Layout.STD430]\n    LAYOUT_MAP = {Layout.STD140: ""std140"", Layout.STD430: ""std430""}\n    ORDERS = [Order.COLUMN_MAJOR, Order.ROW_MAJOR]\n    ORDERS_MAP = {Order.COLUMN_MAJOR: ""column_major"", Order.ROW_MAJOR: ""row_major""}\n\n    def test_pass_through_array_of_scalars(self):\n        glsl_template = """"""\n            #version 450\n            #extension GL_ARB_separate_shader_objects : enable\n\n            layout(local_size_x=1, local_size_y=1, local_size_z=1) in;\n\n            layout({layout_in}, binding = 0) buffer BufferA {{\n                {dtype}[720][1280][3] imageIn;\n            }};\n\n            layout({layout_out}, binding = 1) buffer BufferB {{\n                {dtype}[720][1280][3] imageOut;\n            }};\n\n            void main() {{\n                vec3 pixel = gl_GlobalInvocationID;\n                int h = int(pixel.x);\n                int w = int(pixel.y);\n                int c = int(pixel.z);\n\n                imageOut[h][w][c] = imageIn[h][w][c];\n            }}\n            """"""\n\n        rng = np.random.RandomState(123)\n        w = 1280\n        h = 720\n\n        for layout_in, layout_out, dtype in itertools.product(self.LAYOUTS, self.LAYOUTS, DataType.ALL):\n            scalar = Scalar.of(dtype)\n            im = rng.randint(0, 255, size=(h, w, 3)).astype(scalar.numpy_dtype())\n\n            glsl = glsl_template.format(**{\n                ""layout_in"": self.LAYOUT_MAP[layout_in],\n                ""layout_out"": self.LAYOUT_MAP[layout_out],\n                ""dtype"": scalar.glsl_dtype()\n            })\n\n            shader = self.shader_from_txt(glsl, verbose=False)\n            shader.inspect()\n\n            cache_in = ByteCache(shader.code.get_block_definition(0))\n            cache_in[""imageIn""] = im\n            bytez_in = cache_in.definition.to_bytes(cache_in.get_as_dict())\n\n            cache_out = ByteCache(shader.code.get_block_definition(1))\n            bytez_out_count = cache_out.definition.size()\n            bytez_out = self.run_compiled_program(shader, bytez_in, bytez_out_count, groups=im.shape)\n            cache_out.set_from_dict(cache_out.definition.from_bytes(bytez_out))\n\n            self.assertTrue((cache_out[""imageOut""] == im).all())\n\n    def test_pass_through_array_of_vectors(self):\n        glsl_template = """"""\n            #version 450\n            #extension GL_ARB_separate_shader_objects : enable\n\n            layout(local_size_x=1, local_size_y=1, local_size_z=1) in;\n\n            layout({layout_in}, binding = 0) buffer BufferA {{\n                {dtype}[720][1280] imageIn;\n            }};\n\n            layout({layout_out}, binding = 1) buffer BufferB {{\n                {dtype}[720][1280] imageOut;\n            }};\n\n            void main() {{\n                vec3 pixel = gl_GlobalInvocationID;\n                int h = int(pixel.x);\n                int w = int(pixel.y);\n\n                imageOut[h][w] = imageIn[h][w];\n            }}\n            """"""\n\n        rng = np.random.RandomState(123)\n        w = 1280\n        h = 720\n\n        for layout_in, layout_out, n, dtype in itertools.product(self.LAYOUTS, self.LAYOUTS, range(2, 5), DataType.ALL):\n            vector = Vector(n, dtype)\n            im = rng.randint(0, 255, size=(h, w, n)).astype(vector.scalar.numpy_dtype())\n\n            glsl = glsl_template.format(**{\n                ""layout_in"": self.LAYOUT_MAP[layout_in],\n                ""layout_out"": self.LAYOUT_MAP[layout_out],\n                ""dtype"": vector.glsl_dtype()\n            })\n\n            shader = self.shader_from_txt(glsl, verbose=False)\n            shader.inspect()\n\n            cache_in = ByteCache(shader.code.get_block_definition(0))\n            cache_in[""imageIn""] = im\n            bytez_in = cache_in.definition.to_bytes(cache_in.get_as_dict())\n\n            cache_out = ByteCache(shader.code.get_block_definition(1))\n            bytez_out_count = cache_out.definition.size()\n            bytez_out = self.run_compiled_program(shader, bytez_in, bytez_out_count, groups=(h, w, 1))\n            cache_out.set_from_dict(cache_out.definition.from_bytes(bytez_out))\n\n            self.assertTrue((cache_out[""imageOut""] == im).all())\n\n    def test_pass_through_array_of_matrices(self):\n        glsl_template = """"""\n            #version 450\n            #extension GL_ARB_separate_shader_objects : enable\n\n            layout(local_size_x=1, local_size_y=1, local_size_z=1) in;\n\n            layout({layout_in}, {order_in}, binding = 0) buffer BufferA {{\n                {dtype}[32][48] dataIn;\n            }};\n\n            layout({layout_out}, {order_out}, binding = 1) buffer BufferB {{\n                {dtype}[32][48] dataOut;\n            }};\n\n            void main() {{\n                vec3 pixel = gl_GlobalInvocationID;\n                int h = int(pixel.x);\n                int w = int(pixel.y);\n\n                dataOut[h][w] = dataIn[h][w];\n            }}\n            """"""\n\n        rng = np.random.RandomState(123)\n        w = 48\n        h = 32\n\n        matrix_combinations = itertools.product(range(2, 5), range(2, 5), [DataType.FLOAT, DataType.DOUBLE])\n        layout_order_combinations = itertools.product(self.LAYOUTS, self.LAYOUTS, self.ORDERS, self.ORDERS)\n\n        for combos1, combos2 in itertools.product(layout_order_combinations, matrix_combinations):\n            layout_in, layout_out, order_in, order_out = combos1\n            matrix_in = Matrix(*combos2, layout=layout_in, order=order_in)\n\n            shape = [h, w] + list(matrix_in.shape())\n            mat = rng.randint(0, 255, size=shape).astype(matrix_in.vector.scalar.numpy_dtype())\n\n            glsl = glsl_template.format(**{\n                ""layout_in"": self.LAYOUT_MAP[layout_in],\n                ""layout_out"": self.LAYOUT_MAP[layout_out],\n                ""order_in"": self.ORDERS_MAP[order_in],\n                ""order_out"": self.ORDERS_MAP[order_out],\n                ""dtype"": matrix_in.glsl_dtype()\n            })\n\n            shader = self.shader_from_txt(glsl, verbose=False)\n            shader.inspect()\n\n            cache_in = ByteCache(shader.code.get_block_definition(0))\n            cache_in[""dataIn""] = mat\n            bytez_in = cache_in.definition.to_bytes(cache_in.get_as_dict())\n\n            cache_out = ByteCache(shader.code.get_block_definition(1))\n            bytez_out_count = cache_out.definition.size()\n            bytez_out = self.run_compiled_program(shader, bytez_in, bytez_out_count, groups=(h, w, 1))\n            cache_out.set_from_dict(cache_out.definition.from_bytes(bytez_out))\n\n            self.assertTrue((cache_out[""dataOut""] == mat).all())\n\n    def test_pass_through_matrix(self):\n        glsl_template = """"""\n            #version 450\n            #extension GL_ARB_separate_shader_objects : enable\n    \n            layout(local_size_x=1, local_size_y=1, local_size_z=1) in;\n    \n            layout({layout_in}, {order_in}, binding = 0) buffer BufferA {{\n                {dtype} matrixIn;\n            }};\n    \n            layout({layout_out}, {order_out}, binding = 1) buffer BufferB {{\n                {dtype} matrixOut;\n            }};\n    \n            void main() {{\n                matrixOut = matrixIn;\n            }}\n            """"""\n\n        rng = np.random.RandomState(123)\n        matrix_combinations = itertools.product(range(2, 5), range(2, 5), [DataType.FLOAT, DataType.DOUBLE])\n        layout_order_combinations = itertools.product(self.LAYOUTS, self.LAYOUTS, self.ORDERS, self.ORDERS)\n\n        for combos1, combos2 in itertools.product(layout_order_combinations, matrix_combinations):\n            layout_in, layout_out, order_in, order_out = combos1\n            matrix_in = Matrix(*combos2, layout=layout_in, order=order_in)\n            mat = rng.randint(0, 255, size=matrix_in.shape()).astype(matrix_in.vector.scalar.numpy_dtype())\n\n            glsl = glsl_template.format(**{\n                ""layout_in"": self.LAYOUT_MAP[layout_in],\n                ""layout_out"": self.LAYOUT_MAP[layout_out],\n                ""order_in"": self.ORDERS_MAP[order_in],\n                ""order_out"": self.ORDERS_MAP[order_out],\n                ""dtype"": matrix_in.glsl_dtype()\n            })\n\n            shader = self.shader_from_txt(glsl, verbose=False)\n            shader.inspect()\n\n            cache_in = ByteCache(shader.code.get_block_definition(0))\n            cache_in[""matrixIn""] = mat\n            bytez_in = cache_in.definition.to_bytes(cache_in.get_as_dict())\n\n            cache_out = ByteCache(shader.code.get_block_definition(1))\n            bytez_out_count = cache_out.definition.size()\n            bytez_out = self.run_compiled_program(shader, bytez_in, bytez_out_count)\n            cache_out.set_from_dict(cache_out.definition.from_bytes(bytez_out))\n\n            self.assertTrue((cache_out[""matrixOut""] == mat).all())\n\n    def test_pass_through_struct(self):\n        glsl_template = """"""\n            #version 450\n            #extension GL_ARB_separate_shader_objects : enable\n\n            layout(local_size_x=1, local_size_y=1, local_size_z=1) in;\n\n            {struct_glsl}\n\n            layout({layout_in}, binding = 0) buffer BufferA {{\n                {dtype} structIn;\n            }};\n\n            layout({layout_out}, binding = 1) buffer BufferB {{\n                {dtype} structOut;\n            }};\n\n            void main() {{\n                structOut = structIn;\n            }}\n            """"""\n\n        rng = np.random.RandomState(123)\n\n        scalars = [Scalar.of(dtype) for dtype in DataType.ALL]\n        vectors = [Vector(n, dtype) for n, dtype in itertools.product(range(2, 5), DataType.ALL)]\n\n        type_name = ""SomeStruct""\n\n        for layout_in, layout_out in itertools.product(self.LAYOUTS, self.LAYOUTS):\n            members = rng.permutation(scalars + scalars + vectors + vectors)\n\n            glsl_struct = [""struct {} {{"".format(type_name)]\n            for i, member in enumerate(members):\n                glsl_struct.append(""{} member{};"".format(member.glsl_dtype(), i))\n            glsl_struct.append(""};"")\n\n            glsl = glsl_template.format(**{\n                ""layout_in"": self.LAYOUT_MAP[layout_in],\n                ""layout_out"": self.LAYOUT_MAP[layout_out],\n                ""struct_glsl"": ""\\n"".join(glsl_struct),\n                ""dtype"": type_name\n            })\n\n            shader = self.shader_from_txt(glsl, verbose=False)\n            shader.inspect()\n\n            cache_in = ByteCache(shader.code.get_block_definition(0))\n\n            for i, member in enumerate(members):\n                if isinstance(member, Scalar):\n                    value = member.numpy_dtype()(1.)\n                elif isinstance(member, Vector):\n                    value = np.ones(member.length(), member.scalar.numpy_dtype())\n                else:\n                    value = None\n\n                cache_in[""structIn""][i] = value\n\n            bytez_in = cache_in.definition.to_bytes(cache_in.get_as_dict())\n\n            cache_out = ByteCache(shader.code.get_block_definition(1))\n            bytez_out_count = cache_out.definition.size()\n            bytez_out = self.run_compiled_program(shader, bytez_in, bytez_out_count)\n            cache_out.set_from_dict(cache_out.definition.from_bytes(bytez_out))\n\n            for i, member in enumerate(members):\n                a = cache_in[""structIn""][i]\n                b = cache_out[""structOut""][i]\n                if isinstance(member, Vector):\n                    a = a.tolist()\n                    b = b.tolist()\n                self.assertEqual(a, b)\n\n    def test_pass_through_bools(self):\n        glsl_template = """"""\n            #version 450\n            #extension GL_ARB_separate_shader_objects : enable\n\n            layout(local_size_x=1, local_size_y=1, local_size_z=1) in;\n\n            layout({layout_in}, binding = 0) buffer BufferA {{\n                {dtype} dataIn;\n                {dtype}[{array_size}] dataArrayIn;\n            }};\n\n            layout({layout_out}, binding = 1) buffer BufferB {{\n                {dtype} dataOut;\n                {dtype}[{array_size}] dataArrayOut;\n            }};\n\n            void main() {{\n                dataOut = dataIn;\n                dataArrayOut = dataArrayIn;\n            }}\n            """"""\n\n        dtypes = [""bool"", ""bvec2"", ""bvec3"", ""bvec4""]\n        array_size = 5\n\n        for dtype, layout_in, layout_out in itertools.product(dtypes, self.LAYOUTS, self.LAYOUTS):\n            glsl = glsl_template.format(**{\n                ""layout_in"": self.LAYOUT_MAP[layout_in],\n                ""layout_out"": self.LAYOUT_MAP[layout_out],\n                ""dtype"": dtype,\n                ""array_size"": array_size\n            })\n\n            shader = self.shader_from_txt(glsl, verbose=False)\n            shader.inspect()\n\n            cache_in = ByteCache(shader.code.get_block_definition(0))\n            m = 1 if dtype == ""bool"" else cache_in.definition.definitions[0].length()\n\n            if m == 1:\n                data_in = True\n                data_array_in = (np.arange(array_size) % 2).astype(bool)\n            else:\n                data_in = np.array([True] * m)\n                data_array_in = (np.arange(array_size * m).reshape((array_size, m)) % 2).astype(bool)\n\n            cache_in[""dataIn""] = data_in\n            cache_in[""dataArrayIn""] = data_array_in\n            bytez_in = cache_in.definition.to_bytes(cache_in.get_as_dict())\n\n            cache_out = ByteCache(shader.code.get_block_definition(1))\n            bytez_out_count = cache_out.definition.size()\n            bytez_out = self.run_compiled_program(shader, bytez_in, bytez_out_count)\n            cache_out.set_from_dict(cache_out.definition.from_bytes(bytez_out))\n\n            if m == 1:\n                self.assertEqual(cache_out[""dataOut""], data_in)\n            else:\n                self.assertTrue((cache_out[""dataOut""] == data_in).all())\n            self.assertTrue((cache_out[""dataArrayOut""] == data_array_in).all())\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/api/shader.py,3,"b'# -*- coding: UTF-8 -*-\n\nimport itertools\nimport unittest\n\nimport numpy as np\n\nfrom lava.api.bytes import Array, Matrix, Vector, Scalar, Struct\nfrom lava.api.constants.spirv import DataType, Layout\nfrom lava.api.constants.vk import BufferUsage\nfrom lava.api.util import LavaError\n\nfrom test.api.base import GlslBasedTest, Random\n\n\nclass TestByteCodeInspection(GlslBasedTest):\n\n    @classmethod\n    def build_glsl_program(cls, container_data, structs=()):\n        template = """"""\n            #version 450\n            #extension GL_ARB_separate_shader_objects : enable\n        \n            layout(local_size_x=1, local_size_y=1, local_size_z=1) in;\n        \n            {}\n        \n            {}\n                \n            void main() {{\n            }}""""""\n\n        glsl_structs = ""\\n\\n"".join([cls.build_glsl_struct_definition(struct) for struct in structs])\n\n        glsl_blocks = []\n        for container, binding, usage in container_data:\n            glsl_blocks.append(cls.build_glsl_block_definition(container, binding, usage))\n\n        return template.format(glsl_structs, ""\\n\\n"".join(glsl_blocks))\n\n    @unittest.skip(""test for development purposes"")\n    def test_manual(self):\n        glsl = """"""\n            #version 450\n            #extension GL_ARB_separate_shader_objects : enable\n\n            layout(local_size_x=1, local_size_y=1, local_size_z=16) in;\n            //layout(local_size_x=100, local_size_y=1, local_size_z=1) in;\n\n            struct Data1 {\n                vec3 var1;\n                ivec2 var2;\n            };\n\n            struct Data2 {\n                double var1;\n                double var2;\n                Data1 var3;\n                dmat4 var4;\n            };\n\n            struct Data3 {\n                mat3x3[3] var1;\n            };\n\n            //layout(std140, binding = 0, row_major) uniform uniIn\n            layout(std430, binding = 0, row_major) readonly buffer bufIn\n            {\n                bool flag2;\n                vec2[5][2][3] abc;\n                float bufferIn[5];\n                //bool flag;\n                mat3x4 model;\n                dmat3x2[99] modelz;\n                Data1[2] datas1;\n                Data2 datas2;\n            };\n\n            layout(std140, binding = 1) writeonly buffer bufOut\n            //layout(std430, binding = 1) writeonly buffer bufOut\n            {\n                uint width;\n                uint height;\n                layout(row_major) Data2 datas3;\n                float bufferOut[4];\n                uint asd;\n                dmat3x2[99] modelz2;\n            };\n\n            void main() {\n                uint index = gl_GlobalInvocationID.x;\n                //bufferOut[index] = bufferIn[index] + 1;\n\n                Data3 something;\n                something.var1[0][0].x = 1;\n            }\n            """"""\n\n        shader = self.shader_from_txt(glsl)\n\n        shader.inspect()\n\n        print(shader.code.data)\n\n        print("""")\n        print(""scalars"")\n        print(shader.code.data.types_scalar)\n        print("""")\n        print(""vectors"")\n        print(shader.code.data.types_vector)\n        print("""")\n        print(""matrices"")\n        print(shader.code.data.types_matrix)\n        print("""")\n        print(""array"")\n        print(shader.code.data.types_array)\n        print("""")\n        print(""struct"")\n        print(shader.code.data.types_struct)\n        print("""")\n        print(""names"")\n        names = []\n        for idx in shader.code.data.types_struct:\n            struct_name, member_names = shader.code.data.find_names(idx)\n            offsets = shader.code.data.find_offsets(idx)\n            names.append(""  {}) {} {{ {} }}"".format(idx, struct_name, "", "".join(\n                [""{}({})"".format(mname, offsets.get(i)) for i, mname in enumerate(member_names)])))\n        print(""\\n"".join(names))\n        print("""")\n        print(""blocks"")\n        print(shader.code.data.find_blocks())\n        print("""")\n\n    def test_detection_type_nested_with_structs(self):\n        rng = np.random.RandomState(321)\n\n        simple = [Scalar.uint(), Scalar.int(), Scalar.float(), Scalar.double()]\n        simple += [Vector(n, dtype) for n, dtype in itertools.product(range(2, 5), DataType.ALL)]\n\n        for layout, _ in itertools.product([Layout.STD140, Layout.STD430], range(5)):\n            matrices = [Matrix(n, m, dtype, layout) for n, m, dtype in\n                        itertools.product(range(2, 5), range(2, 5), [DataType.FLOAT, DataType.DOUBLE])]\n            simple_and_matrices = simple + matrices\n\n            struct = Struct(rng.choice(simple_and_matrices, size=3, replace=False), layout, type_name=""SomeStruct"")\n            structs = [struct]\n\n            for _ in range(4):\n                members = [structs[-1]] + rng.choice(simple_and_matrices, size=2, replace=False).tolist()\n                structs.append(Struct(rng.permutation(members), layout, type_name=""SomeStruct{}"".format(len(structs))))\n\n            container = structs[-1]\n            structs = structs[:-1]\n\n            glsl = self.build_glsl_program(((container, 0, BufferUsage.STORAGE_BUFFER),), structs)\n            shader = self.shader_from_txt(glsl, verbose=False)\n            shader.inspect()\n\n            definition, _ = shader.code.get_block(0)\n            self.assertTrue(container.compare(definition, quiet=True))\n\n    def test_detection_type_arrays(self):\n        rng = np.random.RandomState(321)\n        variables = [Scalar.uint(), Scalar.int(), Scalar.float(), Scalar.double()]\n        variables += [Vector(n, dtype) for n, dtype in itertools.product(range(2, 5), DataType.ALL)]\n\n        for definition, layout, _ in itertools.product(variables, [Layout.STD140, Layout.STD430], range(3)):\n            container = Struct([Array(definition, Random.shape(rng, 3, 5), layout)], layout)\n\n            glsl = self.build_glsl_program(((container, 0, BufferUsage.STORAGE_BUFFER),))\n            shader = self.shader_from_txt(glsl, verbose=False)\n            shader.inspect()\n\n            detected_definition, _ = shader.code.get_block(0)\n            self.assertTrue(container.compare(detected_definition, quiet=True))\n\n            if isinstance(definition, Vector):\n                if definition.length() < 3 and definition.dtype != DataType.DOUBLE:\n                    self.assertEqual(detected_definition.layout, layout)\n\n    def test_detection_type_arrays_of_matrices(self):\n        rng = np.random.RandomState(321)\n        matrix_attributes = itertools.product(range(2, 5), range(2, 5), [DataType.FLOAT, DataType.DOUBLE])\n\n        for (n, m, dtype), layout, _ in itertools.product(matrix_attributes, [Layout.STD140, Layout.STD430], range(3)):\n            matrix = Matrix(n, m, dtype, layout)\n            container = Struct([Array(matrix, Random.shape(rng, 3, 5), layout)], layout)\n\n            glsl = self.build_glsl_program(((container, 0, BufferUsage.STORAGE_BUFFER),))\n            shader = self.shader_from_txt(glsl, verbose=False)\n            shader.inspect()\n\n            detected_definition, _ = shader.code.get_block(0)\n            self.assertTrue(container.compare(detected_definition, quiet=True))\n\n    def test_detection_type_bools(self):\n        glsl = """"""\n            #version 450\n            #extension GL_ARB_separate_shader_objects : enable\n\n            layout({layout}, binding = 0) buffer Buffer {{\n                bool var1;\n                bool[720][1280] var2;\n                bvec2 var3;\n                bvec3 var4;\n                bvec4 var5;\n                bvec3[5] var6;\n            }};\n\n            void main() {{}}\n            """"""\n\n        for layout in (Layout.STD140, Layout.STD430):\n            # the vulkan spir-v compiler turns bools into uints\n            expected_definition = Struct([\n                Scalar.uint(),\n                Array(Scalar.uint(), (720, 1280), layout),\n                Vector.uvec2(),\n                Vector.uvec3(),\n                Vector.uvec4(),\n                Array(Vector.uvec3(), 5, layout)\n            ], layout)\n\n            shader = self.shader_from_txt(glsl.format(layout=layout), verbose=False)\n            shader.inspect()\n\n            detected_definition, _ = shader.code.get_block(0)\n            equal = expected_definition.compare(detected_definition, quiet=True)\n            self.assertTrue(equal)\n\n    def test_detection_layout_stdxxx_ssbo(self):\n        variables = [Scalar.uint(), Scalar.int(), Scalar.float(), Scalar.double()]\n        variables += [Vector(n, dtype) for n, dtype in itertools.product(range(2, 5), DataType.ALL)]\n\n        binding = 0\n        usage = BufferUsage.STORAGE_BUFFER\n\n        glsl_std140 = self.build_glsl_program(((Struct(variables, Layout.STD140), binding, usage),))\n        glsl_std430 = self.build_glsl_program(((Struct(variables, Layout.STD430), binding, usage),))\n\n        glsls = [glsl_std140, glsl_std430]\n\n        for glsl in glsls:\n            shader = self.shader_from_txt(glsl, verbose=False)\n            shader.inspect()\n\n            definition, detected_usage = shader.code.get_block(binding)\n\n            self.assertEqual(detected_usage, usage)\n            self.assertEqual(definition.layout, Layout.STDXXX)\n\n    def test_detection_layout_stdxxx_ubo(self):\n        variables = [Scalar.uint(), Scalar.int(), Scalar.float(), Scalar.double()]\n        variables += [Vector(n, dtype) for n, dtype in itertools.product(range(2, 5), DataType.ALL)]\n\n        binding = 0\n        usage = BufferUsage.UNIFORM_BUFFER\n\n        glsl = self.build_glsl_program(((Struct(variables, Layout.STD140), binding, usage),))\n        shader = self.shader_from_txt(glsl, verbose=False)\n        shader.inspect()\n\n        definition, detected_usage = shader.code.get_block(binding)\n\n        self.assertEqual(detected_usage, usage)\n        self.assertEqual(definition.layout, Layout.STD140)  # uniform buffer objects can not use std430\n\n    def test_detection_name(self):\n        glsl = """"""\n            #version 450\n            #extension GL_ARB_separate_shader_objects : enable\n\n            layout(std140, binding = 0) buffer BufferA {\n                float var1;\n                double var2;\n                int var3;\n                uint var4;\n                vec3 var5;\n                ivec4 var6;\n                dvec2[5][5] var7;\n            };\n\n            void main() {}\n            """"""\n        shader = self.shader_from_txt(glsl, verbose=False)\n        shader.inspect()\n\n        definition, _ = shader.code.get_block(0)\n        self.assertListEqual(definition.member_names, [""var{}"".format(i) for i in range(1, 8)])\n\n    def test_detection_binding(self):\n        container = Struct([Scalar.int(), Vector.vec3()], Layout.STD140)\n\n        for binding, usage in itertools.product([0, 1, 2, 3, 4, 99, 512], [BufferUsage.UNIFORM_BUFFER, BufferUsage.STORAGE_BUFFER]):\n            glsl = self.build_glsl_program(((container, binding, usage),))\n            shader = self.shader_from_txt(glsl, verbose=False)\n            shader.inspect()\n\n            detected_definition, detected_usage = shader.code.get_block(binding)\n\n            self.assertEqual(detected_usage, usage)\n            equal = container.compare(detected_definition, quiet=True)\n            self.assertTrue(equal)\n\n    def test_struct_shared_between_different_layouts(self):\n        glsl = """"""\n            #version 450\n            #extension GL_ARB_separate_shader_objects : enable\n\n            struct Shared {\n                int var1;\n                double var2;\n            };\n\n            layout(std140, binding = 0) buffer BufferA {\n                uvec2 varA1;\n                Shared varA2; // expected offset 16\n            };\n            \n            layout(std430, binding = 1) buffer BufferB {\n                uvec2 varB1;\n                Shared varB2; // expected offset 8\n            };\n\n            void main() {}\n            """"""\n        shared_std140 = Struct([Scalar.int(), Scalar.double()], Layout.STD140)\n        shared_std430 = Struct([Scalar.int(), Scalar.double()], Layout.STD430)\n\n        container_std140 = Struct([Vector.uvec2(), shared_std140], Layout.STD140)\n        container_std430 = Struct([Vector.uvec2(), shared_std430], Layout.STD430)\n\n        shader = self.shader_from_txt(glsl, verbose=False)\n        shader.inspect()\n\n        definition0, _ = shader.code.get_block(0)\n        definition1, _ = shader.code.get_block(1)\n        self.assertTrue(container_std140.compare(definition0, quiet=True))\n        self.assertFalse(container_std140.compare(definition1, quiet=True))\n        self.assertFalse(container_std430.compare(definition0, quiet=True))\n        self.assertTrue(container_std430.compare(definition1, quiet=True))\n\n    def test_struct_unused(self):\n        glsl = """"""\n            #version 450\n            #extension GL_ARB_separate_shader_objects : enable\n\n            struct NonUsed {\n                uint var1;\n                dmat4x3 var2;\n            };\n\n            struct Shared {\n                int var1;\n                double var2;\n            };\n\n            layout(std140, binding = 0) buffer BufferA {\n                uvec2 varA1;\n                Shared varA2; // expected offset 16\n            };\n\n            layout(std430, binding = 1) buffer BufferB {\n                uvec2 varB1;\n                Shared varB2; // expected offset 8\n            };\n\n            void main() {\n                NonUsed nonUsed;\n                nonUsed.var2[0].x = 1;\n            }\n            """"""\n        shader = self.shader_from_txt(glsl, verbose=False)\n        shader.inspect()  # just test whether this blows up\n\n    def test_binding_multi_usage(self):\n        glsl = """"""\n            #version 450\n            #extension GL_ARB_separate_shader_objects : enable\n\n            layout(std140, binding = 0) buffer BufferA {\n                float varA;\n            };\n\n            layout(std140, binding = 0) buffer BufferB {\n                double varB;\n            };\n\n            void main() {}\n            """"""\n        shader = self.shader_from_txt(glsl, verbose=False)\n        self.assertRaises(LavaError, shader.inspect)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
lava/api/bytecode/__init__.py,0,"b'# -*- coding: UTF-8 -*-\n\nfrom lava.api.util import LavaError\n\n\nclass ByteCodeError(LavaError):\n\n    UNEXPECTED = ""Something unexpected happened""\n\n    def __init__(self, message):\n        super(ByteCodeError, self).__init__(message)\n\n    @classmethod\n    def unexpected(cls):\n        return cls(cls.UNEXPECTED)\n'"
lava/api/bytecode/logical.py,0,"b'# -*- coding: UTF-8 -*-\n\nfrom lava.api.bytecode import ByteCodeError\nfrom lava.api.bytes import Array, Matrix, Scalar, Struct, Vector\nfrom lava.api.constants.spirv import Access, Decoration, ExecutionMode, ExecutionModel, Layout, Order, StorageClass\nfrom lava.api.constants.vk import BufferUsage\nfrom lava.api.util import LavaError, LavaUnsupportedError\n\n\nclass ByteCode(object):\n\n    def __init__(self, byte_code_data, entry_point=None):\n        self.data = byte_code_data\n\n        # placeholder for inspection variables\n        self.definitions_block = None\n        self.block_data = None\n\n        # check / set entry point\n        self.entry_point, self.entry_point_index = self.check_entry_point(self.data, entry_point)\n        self.local_size = self.check_local_size(self.data, self.entry_point_index)\n\n        # off we go\n        self.inspect()\n\n    @classmethod\n    def check_entry_point(cls, byte_code_data, entry_point):\n        entry_points_detected = byte_code_data.find_entry_points(execution_model=ExecutionModel.GL_COMPUTE)\n        index = None\n\n        if len(entry_points_detected) == 0:\n            raise RuntimeError(""Could not find entry points for execution model {}"".format(ExecutionModel.GL_COMPUTE))\n\n        if entry_point is not None and entry_point not in entry_points_detected.values():\n            raise RuntimeError(""Could not find entry point {} in detected entry points {}"".format(\n                entry_point, "", "".join(entry_points_detected)))\n\n        if entry_point is None:\n            if len(entry_points_detected) > 1:\n                raise RuntimeError(""Multiple entry points found {}"".format("", "".join(entry_points_detected.values())))\n            entry_point = list(entry_points_detected.values())[0]\n\n        for index_candidate, entry_point_candidate in entry_points_detected.items():\n            if entry_point_candidate == entry_point:\n                index = index_candidate\n\n        return entry_point, index\n\n    @classmethod\n    def check_local_size(cls, byte_code_data, entry_point_index):\n        execution_mode, literals = byte_code_data.find_entry_point_details(entry_point_index)\n\n        if execution_mode != ExecutionMode.LOCAL_SIZE:\n            raise LavaUnsupportedError(""Unsupported execution mode {}"".format(execution_mode))\n\n        return literals\n\n    def inspect(self):\n        self.block_data = self.data.find_blocks()\n        self.definitions_block = {}\n\n        defs_scalar = {index: Scalar.of(dtype) for index, dtype in self.data.types_scalar.items()}\n        defs_vector = {index: Vector(n, dtype) for index, (dtype, n) in self.data.types_vector.items()}\n        defs_array = {}\n        defs_struct = {}\n\n        for binding in self.get_bindings():\n            index, _ = self.get_block_index(binding)\n            self.deduce_definition(index, defs_scalar, defs_vector, defs_array, defs_struct, index)\n            self.definitions_block[index] = defs_struct[index]\n            self.deduce_layout(binding)\n\n    def deduce_definition(self, index, definitions_scalar, definitions_vector, definitions_array, definitions_struct,\n                          last_struct=None):\n        default_layout = Layout.STD140\n\n        if index in self.data.types_array:\n            type_index, dims = self.data.types_array[index]\n\n            # build missing definition\n            if type_index in self.data.types_struct and type_index not in definitions_struct:\n                self.deduce_definition(type_index, definitions_scalar, definitions_vector, definitions_array,\n                                       definitions_struct, last_struct)\n\n            definition = None\n\n            # matrix types are shared, but still affected by the layout, create a instance for every occurrence\n            if type_index in self.data.types_matrix:\n                definition = self.build_matrix_definition(type_index, default_layout, last_struct)\n\n            definition = definition or definitions_scalar.get(type_index)\n            definition = definition or definitions_vector.get(type_index)\n            definition = definition or definitions_struct.get(type_index)\n\n            definitions_array[index] = Array(definition.copy(), dims, default_layout)\n\n        elif index in self.data.types_struct:\n            member_indices = self.data.types_struct[index]\n\n            # build missing definitions\n            for member_index in member_indices:\n                is_struct = member_index in self.data.types_struct\n                is_array = member_index in self.data.types_array\n\n                defs = {\n                    ""definitions_scalar"": definitions_scalar,\n                    ""definitions_vector"": definitions_vector,\n                    ""definitions_array"": definitions_array,\n                    ""definitions_struct"": definitions_struct\n                }\n\n                if is_struct and member_index not in definitions_struct:\n                    self.deduce_definition(member_index, last_struct=member_index, **defs)\n\n                if is_array and member_index not in definitions_array:\n                    self.deduce_definition(member_index, last_struct=index, **defs)\n\n            definitions = []\n            for member_index in member_indices:\n                definition = None\n\n                # matrix types are shared, but still affected by the layout, create a instance for every occurrence\n                if member_index in self.data.types_matrix:\n                    definition = self.build_matrix_definition(member_index, default_layout, last_struct)\n\n                definition = definition or definitions_scalar.get(member_index)\n                definition = definition or definitions_vector.get(member_index)\n                definition = definition or definitions_array.get(member_index)\n                definition = definition or definitions_struct.get(member_index)\n                definitions.append(definition.copy())\n\n            struct_name, member_names = self.data.find_names(index)\n            definitions_struct[index] = Struct(definitions, default_layout, member_names=member_names,\n                                               type_name=struct_name)\n\n        else:\n            raise ByteCodeError.unexpected()\n\n    def build_matrix_definition(self, matrix_index, layout, last_struct_index):\n        # we assume that matrices always have the order decoration in bytecode (if they are part of a interface block)\n        if last_struct_index is None:\n            raise ByteCodeError.unexpected()\n\n        member_indices = self.data.find_member_ids(last_struct_index)\n        member_orders = self.data.find_orders(last_struct_index)\n\n        # matrix is direct member of the last struct\n        if matrix_index in member_indices:\n            member = member_indices.index(matrix_index)\n\n        # if the matrix is a member of an array which is a struct member, the array is also decorated with the\n        # matrix order\n        else:\n            member = None\n\n            for index in member_indices:\n                if index in self.data.types_array:\n                    type_index, _ = self.data.types_array[index]\n                    if type_index == matrix_index:\n                        member = member_indices.index(index)\n                        break\n\n            if member is None:\n                raise ByteCodeError.unexpected()\n\n        order_decoration = member_orders[member]\n        order = {Decoration.ROW_MAJOR: Order.ROW_MAJOR, Decoration.COL_MAJOR: Order.COLUMN_MAJOR}[order_decoration]\n\n        dtype, rows, cols = self.data.types_matrix[matrix_index]\n        return Matrix(cols, rows, dtype, layout, order)\n\n    def deduce_layout(self, binding):\n        index, usage = self.get_block_index(binding)\n        definition = self.get_block_definition(binding)\n\n        definition.layout = Layout.STD140\n        match_std140 = self.check_layout(index)\n\n        definition.layout = Layout.STD430\n        match_std430 = self.check_layout(index)\n\n        # deduce layout\n        if match_std140 and not match_std430:\n            definition.layout = Layout.STD140\n\n        elif not match_std140 and match_std430:\n            definition.layout = Layout.STD430\n\n        elif match_std140 and match_std430:\n            # std430 is not allowed for uniform buffer objects\n            if usage == BufferUsage.UNIFORM_BUFFER:\n                definition.layout = Layout.STD140\n            else:\n                definition.layout = Layout.STDXXX\n\n        else:\n            possible_reasons = [\n                ""a memory layout other than std140 or std430 was used"",\n                ""an offset was defined manually, e.g. for a struct member: layout(offset=128) int member;"",\n                ""a matrix memory order was defined manually, e.g. for a struct member: layout(row_major) ""\n                ""StructXYZ structWithMatrices;""\n            ]\n            raise RuntimeError(""Found unexpected memory offsets, this might occur because of\\n"" +\n                               """".join([""* {}\\n"".format(reason) for reason in possible_reasons]))\n\n    def check_layout(self, index):\n        definition = self.definitions_block[index]\n\n        member_indices = self.data.find_member_ids(index)\n        offsets_bytecode = self.data.find_offsets(index)\n        offsets_bytecode = [offsets_bytecode.get(i) for i in range(len(definition.definitions))]\n\n        if None in offsets_bytecode:\n            raise ByteCodeError.unexpected()\n\n        if offsets_bytecode != definition.offsets():\n            return False\n\n        for i, (member_index, d) in enumerate(zip(member_indices, definition.definitions)):\n            if isinstance(d, Array):\n                if self.data.find_strides(member_index) != d.strides():\n                    return False\n\n            if isinstance(d, Matrix):\n                if self.data.find_matrix_stride(index, i) != d.stride():\n                    return False\n\n        return True\n\n    def get_bindings(self):\n        bindings = []\n\n        for index in self.block_data:\n            bindings.append(self.block_data[index][2])\n\n        for binding in bindings:\n            count = bindings.count(binding)\n            if count > 1:\n                raise LavaError(""Binding {} is used {} times"".format(binding, count))\n\n        return list(sorted(bindings))\n\n    def get_block_index(self, binding):\n        for index in self.block_data:\n            block_type, storage_class, binding_id = self.block_data[index]\n\n            if binding_id == binding:\n                usage = None\n\n                if block_type == Decoration.BLOCK and storage_class == StorageClass.UNIFORM:\n                    usage = BufferUsage.UNIFORM_BUFFER\n                elif block_type == Decoration.BUFFER_BLOCK and storage_class == StorageClass.UNIFORM:\n                    usage = BufferUsage.STORAGE_BUFFER\n\n                return index, usage\n\n        raise ValueError(""Binding {} not found"".format(binding))\n\n    def get_block(self, binding):\n        index, usage = self.get_block_index(binding)\n        return self.definitions_block[index], usage\n\n    def get_block_definition(self, binding):\n        return self.get_block(binding)[0]\n\n    def get_block_usage(self, binding):\n        return self.get_block(binding)[1]\n\n    def get_block_access(self, binding):\n        index, usage = self.get_block_index(binding)\n        block_definition, _ = self.get_block(binding)\n\n        if usage == BufferUsage.UNIFORM_BUFFER:\n            return Access.READ_ONLY\n\n        decorations = self.data.find_accesses(index)\n\n        if len(decorations) not in (0, len(block_definition.definitions)):  # 0 for no access decorations at all\n            raise ByteCodeError.unexpected()\n\n        accesses = set()\n\n        for member, d in decorations.items():\n            if Decoration.NON_WRITABLE in d and Decoration.NON_READABLE not in d:\n                accesses.add(Access.READ_ONLY)\n            elif Decoration.NON_WRITABLE not in d and Decoration.NON_READABLE in d:\n                accesses.add(Access.WRITE_ONLY)\n            elif Decoration.NON_WRITABLE in d and Decoration.NON_READABLE in d:\n                accesses.add(Access.NEITHER)\n            else:\n                accesses.add(Access.READ_WRITE)\n\n        if len(accesses) == 0:\n            accesses.add(Access.READ_WRITE)\n        elif len(accesses) > 1:\n            raise ByteCodeError.unexpected()\n\n        return accesses.pop()\n'"
lava/api/bytecode/physical.py,0,"b'# -*- coding: UTF-8 -*-\n\nimport itertools\nimport struct\n\nimport lava.api.constants.spirv as spirv\nfrom lava.api.bytecode import ByteCodeError\n\n\nclass ByteCodeData(object):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf\n\n    def __init__(self, bytez):\n        self.bytez = bytearray(bytez)\n\n        self.header = ByteCodeHeader(self.bytez)\n        self.instructions = []\n\n        # parse instruction by instruction\n        step = self.header.words() * spirv.WORD_BYTE_SIZE\n        while step < len(bytez):\n            instruction = ByteCodeInstruction(self.bytez[step:])\n            self.instructions.append(instruction)\n            step += instruction.words() * spirv.WORD_BYTE_SIZE\n\n        # recover types\n        self.types_scalar, self.types_vector, self.types_matrix = self.find_basic_types()\n        self.types_array = self.find_array_types()\n        self.types_struct = self.find_struct_types()\n\n    @classmethod\n    def from_file(cls, path):\n        with open(path, ""rb"") as f:\n            bytez = f.read()\n        return cls(bytez)\n\n    @classmethod\n    def read_word(cls, bytez, offset=0):\n        # word to unsigned integer\n        return cls.read_words(bytez, n=1, offset=offset)[0]\n\n    @classmethod\n    def read_words(cls, bytez, n=-1, offset=0):\n        # words to unsigned integers\n        if n == -1:\n            n = len(bytez) // spirv.WORD_BYTE_SIZE - offset\n        a = offset * spirv.WORD_BYTE_SIZE\n        b = a + n * spirv.WORD_BYTE_SIZE\n        return struct.unpack(""I"" * n, bytez[a:b])\n\n    @classmethod\n    def read_words_as_string(cls, bytez, n=-1, offset=0):\n        a = offset * spirv.WORD_BYTE_SIZE\n        b = len(bytez) if n == -1 else a + n * spirv.WORD_BYTE_SIZE\n        return bytez[a:b].rstrip(b""\\0"").decode(""utf-8"")\n\n    def __str__(self):\n        strings = []\n        for instruction in self.instructions:\n            if instruction.op is None:\n                strings.append(""Op id={} words={}"".format(instruction.op_id, instruction.words()))\n            else:\n                strings.append(str(instruction.op))\n        return ""\\n"".join(strings)\n\n    def abort(self):\n        raise ByteCodeError.unexpected()\n\n    def find_instructions(self, operation):\n        results = []\n        for instruction in self.instructions:\n            if instruction.op_id == operation.ID:\n                results.append(instruction)\n        return results\n\n    def find_instructions_with_attributes(self, operation, **attributes):\n        results = self.find_instructions(operation)\n        results_filtered = []\n\n        for instruction in results:\n            matches = 0\n\n            for attr_key, attr_value in attributes.items():\n                if attr_key not in instruction.op.__dict__:\n                    break\n                if instruction.op.__dict__[attr_key] != attr_value:\n                    break\n                matches += 1\n\n            if matches == len(attributes):\n                results_filtered.append(instruction)\n\n        return results_filtered\n\n    def find_basic_types(self):\n        types_scalar = {}\n        types_vector = {}\n        types_matrix = {}\n\n        search_scalar = [\n            (spirv.DataType.FLOAT, {""operation"": OpTypeFloat, ""width"": 32}),\n            (spirv.DataType.DOUBLE, {""operation"": OpTypeFloat, ""width"": 64}),\n            (spirv.DataType.INT, {""operation"": OpTypeInt, ""width"": 32, ""signedness"": 1}),\n            (spirv.DataType.UINT, {""operation"": OpTypeInt, ""width"": 32, ""signedness"": 0}),\n        ]\n\n        # scalar types are ""standalone""\n        for type_scalar, search_data in search_scalar:\n            instructions = self.find_instructions_with_attributes(**search_data)\n            if len(instructions) == 1:\n                types_scalar[instructions[0].op.result_id] = type_scalar\n            elif len(instructions) > 1:\n                self.abort()\n\n        # vector types reference the scalar types\n        for result_id, n in itertools.product(types_scalar.keys(), range(2, 5)):\n            instructions = self.find_instructions_with_attributes(operation=OpTypeVector, component_type=result_id,\n                                                                  component_count=n)\n            if len(instructions) == 1:\n                types_vector[instructions[0].op.result_id] = (types_scalar[result_id], n)\n            elif len(instructions) > 1:\n                self.abort()\n\n        # matrix types reference the vector types\n        for result_id, cols in itertools.product(types_vector.keys(), range(2, 5)):\n            instructions = self.find_instructions_with_attributes(operation=OpTypeMatrix, column_type=result_id,\n                                                                  column_count=cols)\n            if len(instructions) == 1:\n                scalar_type, rows = types_vector[result_id]\n                types_matrix[instructions[0].op.result_id] = (scalar_type, rows, cols)\n            elif len(instructions) > 1:\n                self.abort()\n\n        return types_scalar, types_vector, types_matrix\n\n    def find_array_types(self):\n        types_array = {}\n        tmp = {}\n\n        # find all array types first\n        for instruction in self.find_instructions(OpTypeArray):\n            constants = self.find_instructions_with_attributes(OpConstant, result_id=instruction.op.length)\n            if len(constants) == 1:\n                n = constants[0].op.literals[0]\n                tmp[instruction.op.result_id] = (instruction.op.element_type, [n])\n\n        # collapse them into nd-arrays\n        for idx in sorted(tmp.keys(), reverse=True):\n            if idx in tmp:\n                ref, dims = tmp[idx]\n\n                while ref in tmp:\n                    other_ref, other_dims = tmp[ref]\n                    del tmp[ref]\n                    ref = other_ref\n                    dims += other_dims\n\n                types_array[idx] = (ref, tuple(dims))\n                del tmp[idx]\n\n        return types_array\n\n    def find_struct_types(self):\n        types_struct = {}\n\n        for instruction in self.find_instructions(OpTypeStruct):\n            types_struct[instruction.op.result_id] = instruction.op.member_types\n\n        return types_struct\n\n    def find_names(self, struct_id):\n        struct_name = None\n\n        instructions = self.find_instructions_with_attributes(OpName, target_id=struct_id)\n        if len(instructions) == 1:\n            struct_name = instructions[0].op.name\n\n        instructions = self.find_instructions_with_attributes(OpMemberName, type_id=struct_id)\n        member_names = {instruction.op.member: instruction.op.name for instruction in instructions}\n        return struct_name, [member_names.get(i) for i in range(max(member_names.keys()) + 1)]\n\n    def find_member_ids(self, struct_id):\n        instructions = self.find_instructions_with_attributes(OpTypeStruct, result_id=struct_id)\n        if len(instructions) == 1:\n            return instructions[0].op.member_types\n        else:\n            return None\n\n    def find_offsets(self, struct_id):\n        offsets = {}\n\n        for instruction in self.find_instructions_with_attributes(OpMemberDecorate, decoration=spirv.Decoration.OFFSET,\n                                                                  type_id=struct_id):\n            member = instruction.op.member\n            offset = instruction.op.literals[0]\n            offsets[member] = offset\n\n        return offsets\n\n    def find_accesses(self, struct_id):\n        accesses = {}\n\n        instructions1 = self.find_instructions_with_attributes(OpMemberDecorate, type_id=struct_id,\n                                                               decoration=spirv.Decoration.NON_READABLE)\n        instructions2 = self.find_instructions_with_attributes(OpMemberDecorate, type_id=struct_id,\n                                                               decoration=spirv.Decoration.NON_WRITABLE)\n\n        for instruction in instructions1 + instructions2:\n            member = instruction.op.member\n\n            tmp = accesses.get(member, set())\n            tmp.add(instruction.op.decoration)\n            accesses[member] = tmp\n\n        return accesses\n\n    def find_strides(self, array_id):\n        array_ids = [array_id]\n\n        instructions = self.find_instructions_with_attributes(OpTypeArray, result_id=array_ids[-1])\n        while len(instructions) == 1:\n            array_ids.append(instructions[0].op.element_type)\n            instructions = self.find_instructions_with_attributes(OpTypeArray, result_id=array_ids[-1])\n        array_ids = array_ids[:-1]  # last type is no array\n\n        strides = []\n\n        for array_id in array_ids:\n            instructions = self.find_instructions_with_attributes(OpDecorate, decoration=spirv.Decoration.ARRAY_STRIDE,\n                                                                  target_id=array_id)\n            if len(instructions) == 1:\n                strides.append(instructions[0].op.literals[0])\n            else:\n                self.abort()\n\n        return strides\n\n    def find_matrix_stride(self, struct_id, member):\n        instructions = self.find_instructions_with_attributes(OpMemberDecorate, type_id=struct_id, member=member,\n                                                              decoration=spirv.Decoration.MATRIX_STRIDE)\n        if len(instructions) == 1:\n            return instructions[0].op.literals[0]\n        else:\n            return None\n\n    def find_orders(self, struct_id):\n        orders = {}\n\n        instructions_row_major = self.find_instructions_with_attributes(OpMemberDecorate, type_id=struct_id,\n                                                                        decoration=spirv.Decoration.ROW_MAJOR)\n\n        instructions_col_major = self.find_instructions_with_attributes(OpMemberDecorate, type_id=struct_id,\n                                                                        decoration=spirv.Decoration.COL_MAJOR)\n\n        for instruction in instructions_row_major + instructions_col_major:\n            member = instruction.op.member\n            orders[member] = instruction.op.decoration\n\n        return orders\n\n    def find_blocks(self):\n        blocks = {}\n        blocks1 = self.find_instructions_with_attributes(OpDecorate, decoration=spirv.Decoration.BLOCK)  # ubo\n        blocks2 = self.find_instructions_with_attributes(OpDecorate, decoration=spirv.Decoration.BUFFER_BLOCK)  # ssbo\n\n        for candidate in blocks1 + blocks2:\n            # find associated pointer\n            instructions = self.find_instructions_with_attributes(OpTypePointer, type_id=candidate.op.target_id)\n            if len(instructions) != 1:\n                self.abort()\n\n            # find associated variable\n            instructions = self.find_instructions_with_attributes(OpVariable, result_type=instructions[0].op.result_id)\n            if len(instructions) != 1:\n                self.abort()\n            storage_class = instructions[0].op.storage_class\n\n            # find associated binding\n            instructions = self.find_instructions_with_attributes(OpDecorate, target_id=instructions[0].op.result_id,\n                                                                  decoration=spirv.Decoration.BINDING)\n\n            block_type = candidate.op.decoration\n\n            if len(instructions) == 1:\n                binding_id = instructions[0].op.literals[0]\n                blocks[candidate.op.target_id] = (block_type, storage_class, binding_id)\n\n            else:\n                self.abort()\n\n        return blocks\n\n    def find_entry_points(self, execution_model):\n        entry_points = {}\n        instructions = self.find_instructions_with_attributes(OpEntryPoint, execution_model=execution_model)\n\n        for instruction in instructions:\n            entry_points[instruction.op.entry_point] = instruction.op.name\n\n        return entry_points\n\n    def find_entry_point_details(self, entry_point_index):\n        execution_mode = None\n        literals = None\n        instructions = self.find_instructions_with_attributes(OpExecutionMode, entry_point=entry_point_index)\n\n        if len(instructions) == 1:\n            execution_mode = instructions[0].op.execution_mode\n            literals = instructions[0].op.literals\n        else:\n            self.abort()\n\n        return execution_mode, literals\n\n\nclass ByteCodeHeader(object):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#subsection.2.3\n\n    def __init__(self, bytez):\n        self.bytez = bytez\n        magic_number, version, generator_magic_number, bound, _ = ByteCodeData.read_words(bytez, spirv.WORD_COUNT_HEADER)\n\n        if magic_number != spirv.MAGIC_NUMBER:\n            raise ByteCodeError(""MagicNumber does not match SPIR-V specs"")\n\n        self.version_major = (version & 0x00FF0000) >> 16\n        self.version_minor = (version & 0x0000FF00) >> 8\n        self.generator_magic_number = generator_magic_number\n        self.bound = bound\n\n    def words(self):\n        return spirv.WORD_COUNT_HEADER\n\n\nclass ByteCodeInstruction(object):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#subsection.2.3\n\n    def __init__(self, bytez):\n        first_word = ByteCodeData.read_word(bytez)\n        self.word_count = first_word >> 16\n        self.op_id = first_word & 0x0000FFFF\n        self.bytez = bytez[:self.word_count * spirv.WORD_BYTE_SIZE]\n        self.op = None\n\n        if self.op_id in OPS_REGISTER:\n            self.op = OPS_REGISTER[self.op_id](self.bytez[spirv.WORD_BYTE_SIZE:])\n\n    def words(self):\n        return self.word_count\n\n\nclass Op(object):\n    ID = -1\n\n    def __init__(self, bytez):\n        self.bytez = bytez\n\n    def describe(self):\n        raise NotImplementedError()\n\n    def __str__(self):\n        return ""{:<20}{}"".format(self.__class__.__name__, self.describe() or """")\n\n\nclass OpName(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpName\n    ID = 5\n\n    def __init__(self, bytez):\n        super(OpName, self).__init__(bytez)\n        self.target_id = ByteCodeData.read_word(bytez)\n        self.name = ByteCodeData.read_words_as_string(bytez, offset=1)\n\n    def describe(self):\n        return ""target_id={} name={}"".format(self.target_id, self.name)\n\n\nclass OpMemberName(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpMemberName\n    ID = 6\n\n    def __init__(self, bytez):\n        super(OpMemberName, self).__init__(bytez)\n        self.type_id = ByteCodeData.read_word(bytez)\n        self.member = ByteCodeData.read_word(bytez, offset=1)\n        self.name = ByteCodeData.read_words_as_string(bytez, offset=2)\n\n    def describe(self):\n        return ""type_id={} member_id={} name={}"".format(self.type_id, self.member, self.name)\n\n\nclass OpEntryPoint(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpEntryPoint\n    ID = 15\n\n    def __init__(self, bytez):\n        super(OpEntryPoint, self).__init__(bytez)\n        self.execution_model = spirv.ExecutionModel.from_spirv(ByteCodeData.read_word(bytez))\n        self.entry_point = ByteCodeData.read_word(bytez, offset=1)\n        self.name = ByteCodeData.read_words_as_string(bytez, n=1, offset=2)\n        self.ids = ByteCodeData.read_words(bytez, offset=3)\n\n    def describe(self):\n        return ""execution_model={} entry_point={} name={} ids={}"".format(self.execution_model, self.entry_point,\n                                                                         self.name, self.ids)\n\n\nclass OpExecutionMode(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpExecutionMode\n    ID = 16\n\n    def __init__(self, bytez):\n        super(OpExecutionMode, self).__init__(bytez)\n        self.entry_point = ByteCodeData.read_word(bytez)\n        self.execution_mode = spirv.ExecutionMode.from_spirv(ByteCodeData.read_word(bytez, offset=1))\n        self.literals = ByteCodeData.read_words(bytez, offset=2)\n\n    def describe(self):\n        return ""entry_point={} execution_mode={} literals={}"".format(self.entry_point, self.execution_mode,\n                                                                     self.literals)\n\n\nclass OpDecorate(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpDecorate\n    ID = 71\n\n    def __init__(self, bytez):\n        super(OpDecorate, self).__init__(bytez)\n        self.target_id = ByteCodeData.read_word(bytez)\n        self.decoration = spirv.Decoration.from_spirv(ByteCodeData.read_word(bytez, offset=1))\n        self.literals = ByteCodeData.read_words(bytez, offset=2)\n\n    def describe(self):\n        return ""target_id={} decoration={} literals={}"".format(self.target_id, self.decoration, self.literals)\n\n\nclass OpMemberDecorate(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpMemberDecorate\n    ID = 72\n\n    def __init__(self, bytez):\n        super(OpMemberDecorate, self).__init__(bytez)\n        self.type_id = ByteCodeData.read_word(bytez)\n        self.member = ByteCodeData.read_word(bytez, offset=1)\n        self.decoration = spirv.Decoration.from_spirv(ByteCodeData.read_word(bytez, offset=2))\n        self.literals = ByteCodeData.read_words(bytez, n=(len(bytez) // spirv.WORD_BYTE_SIZE - 3), offset=3)\n\n    def describe(self):\n        return ""type_id={} member={} decoration={} literals={}"".format(self.type_id, self.member, self.decoration,\n                                                                       self.literals)\n\n\nclass OpTypeVoid(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpTypeVoid\n    ID = 19\n\n    def __init__(self, bytez):\n        super(OpTypeVoid, self).__init__(bytez)\n        self.result_id = ByteCodeData.read_word(bytez)\n\n    def describe(self):\n        return ""result_id={}"".format(self.result_id)\n\n\nclass OpTypeBool(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpTypeBool\n    ID = 20\n\n    def __init__(self, bytez):\n        super(OpTypeBool, self).__init__(bytez)\n        self.result_id = ByteCodeData.read_word(bytez)\n\n    def describe(self):\n        return ""result_id={}"".format(self.result_id)\n\n\nclass OpTypeInt(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpTypeInt\n    ID = 21\n\n    def __init__(self, bytez):\n        super(OpTypeInt, self).__init__(bytez)\n        self.result_id = ByteCodeData.read_word(bytez)\n        self.width = ByteCodeData.read_word(bytez, offset=1)\n        self.signedness = ByteCodeData.read_word(bytez, offset=2)\n\n    def describe(self):\n        return ""result_id={} width={} signedness={}"".format(self.result_id, self.width, self.signedness)\n\n\nclass OpTypeFloat(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpTypeFloat\n    ID = 22\n\n    def __init__(self, bytez):\n        super(OpTypeFloat, self).__init__(bytez)\n        self.result_id = ByteCodeData.read_word(bytez)\n        self.width = ByteCodeData.read_word(bytez, offset=1)\n\n    def describe(self):\n        return ""result_id={} width={}"".format(self.result_id, self.width)\n\n\nclass OpTypeVector(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpTypeVector\n    ID = 23\n\n    def __init__(self, bytez):\n        super(OpTypeVector, self).__init__(bytez)\n        self.result_id = ByteCodeData.read_word(bytez)\n        self.component_type = ByteCodeData.read_word(bytez, offset=1)\n        self.component_count = ByteCodeData.read_word(bytez, offset=2)\n\n    def describe(self):\n        return ""result_id={} component_type={} component_count={}"".format(self.result_id, self.component_type,\n                                                                          self.component_count)\n\n\nclass OpTypeMatrix(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpTypeMatrix\n    ID = 24\n\n    def __init__(self, bytez):\n        super(OpTypeMatrix, self).__init__(bytez)\n        self.result_id = ByteCodeData.read_word(bytez)\n        self.column_type = ByteCodeData.read_word(bytez, offset=1)\n        self.column_count = ByteCodeData.read_word(bytez, offset=2)\n\n    def describe(self):\n        return ""result_id={} column_type={} column_count={}"".format(self.result_id, self.column_type, self.column_count)\n\n\nclass OpTypeImage(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpTypeImage\n    ID = 25\n\n    def __init__(self, bytez):\n        super(OpTypeImage, self).__init__(bytez)\n        self.result_id = ByteCodeData.read_word(bytez)\n        self.sampled_type = ByteCodeData.read_word(bytez, offset=1)\n        # TODO: read out remaining 5 tons of information\n\n    def describe(self):\n        return ""result_id={} sampled_type={}"".format(self.result_id, self.sampled_type)\n\n\nclass OpTypeSampler(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpTypeSampler\n    ID = 26\n\n    def __init__(self, bytez):\n        super(OpTypeSampler, self).__init__(bytez)\n        self.result_id = ByteCodeData.read_word(bytez)\n\n    def describe(self):\n        return ""result_id={}"".format(self.result_id)\n\n\nclass OpTypeArray(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpTypeArray\n    ID = 28\n\n    def __init__(self, bytez):\n        super(OpTypeArray, self).__init__(bytez)\n        self.result_id = ByteCodeData.read_word(bytez)\n        self.element_type = ByteCodeData.read_word(bytez, offset=1)\n        self.length = ByteCodeData.read_word(bytez, offset=2)\n\n    def describe(self):\n        return ""result_id={} element_type={} length={}"".format(self.result_id, self.element_type, self.length)\n\n\nclass OpTypeRuntimeArray(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpTypeRuntimeArray\n    ID = 29\n\n    def __init__(self, bytez):\n        super(OpTypeRuntimeArray, self).__init__(bytez)\n        self.result_id = ByteCodeData.read_word(bytez)\n        self.element_type = ByteCodeData.read_word(bytez, offset=1)\n\n    def describe(self):\n        return ""result_id={} element_type={}"".format(self.result_id, self.element_type)\n\n\nclass OpTypeStruct(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpTypeStruct\n    ID = 30\n\n    def __init__(self, bytez):\n        super(OpTypeStruct, self).__init__(bytez)\n        self.result_id = ByteCodeData.read_word(bytez)\n        self.member_types = ByteCodeData.read_words(bytez, offset=1)\n\n    def describe(self):\n        return ""result_id={} member_types={}"".format(self.result_id, self.member_types)\n\n\nclass OpSource(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpSource\n    ID = 3\n\n    def __init__(self, bytez):\n        super(OpSource, self).__init__(bytez)\n        self.source_language = spirv.SourceLanguage.from_spirv(ByteCodeData.read_word(bytez))\n        self.version = ByteCodeData.read_word(bytez, offset=1)\n        # ignore other attributes\n\n    def describe(self):\n        return ""source_language={} version={}"".format(self.source_language, self.version)\n\n\nclass OpSourceExtension(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpSourceExtension\n    ID = 4\n\n    def __init__(self, bytez):\n        super(OpSourceExtension, self).__init__(bytez)\n        self.extension = ByteCodeData.read_words_as_string(bytez)\n\n    def describe(self):\n        return ""extension={}"".format(self.extension)\n\n\nclass OpTypePointer(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpTypePointer\n    ID = 32\n\n    def __init__(self, bytez):\n        super(OpTypePointer, self).__init__(bytez)\n        self.result_id = ByteCodeData.read_word(bytez)\n        self.storage_class = spirv.StorageClass.from_spirv(ByteCodeData.read_word(bytez, offset=1))\n        self.type_id = ByteCodeData.read_word(bytez, offset=2)\n\n    def describe(self):\n        return ""result_id={} storage_class={} type_id={}"".format(self.result_id, self.storage_class, self.type_id)\n\n\nclass OpVariable(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpVariable\n    ID = 59\n\n    def __init__(self, bytez):\n        super(OpVariable, self).__init__(bytez)\n        self.result_type = ByteCodeData.read_word(bytez)\n        self.result_id = ByteCodeData.read_word(bytez, offset=1)\n        self.storage_class = spirv.StorageClass.from_spirv(ByteCodeData.read_word(bytez, offset=2))\n        # self.initializer = ... (optional)\n\n    def describe(self):\n        return ""result_type={} result_id={} storage_class={}"".format(self.result_type, self.result_id,\n                                                                     self.storage_class)\n\n\nclass OpConstant(Op):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#OpConstant\n    ID = 43\n\n    def __init__(self, bytez):\n        super(OpConstant, self).__init__(bytez)\n        self.result_type = ByteCodeData.read_word(bytez)\n        self.result_id = ByteCodeData.read_word(bytez, offset=1)\n        self.literals = ByteCodeData.read_words(bytez, offset=2)\n        # self.initializer = ... (optional)\n\n    def describe(self):\n        return ""result_type={} result_id={} literals={}"".format(self.result_type, self.result_id, self.literals)\n\n\nOPS_REGISTER = {op.ID: op for op in [\n    OpName, OpMemberName, OpEntryPoint, OpExecutionMode, OpDecorate, OpMemberDecorate, OpTypeBool, OpTypeInt,\n    OpTypeFloat, OpTypeVector, OpTypeMatrix, OpTypeArray, OpTypeRuntimeArray, OpTypeStruct, OpTypeVoid, OpTypeImage,\n    OpTypeSampler, OpSource, OpSourceExtension, OpTypePointer, OpVariable, OpConstant\n]}\n'"
lava/api/constants/__init__.py,0,b''
lava/api/constants/spirv.py,0,"b'# -*- coding: UTF-8 -*-\n\nWORD_BYTE_SIZE = 4\nMAGIC_NUMBER = 0x07230203\nWORD_COUNT_HEADER = 5\n\n\nclass DataType(object):\n    # BOOL = ""bool""  # machine unit 1 byte? (probably not 1 bit)\n    INT = ""int""\n    UINT = ""uint""\n    FLOAT = ""float""\n    DOUBLE = ""double""\n\n    ALL = [INT, UINT, FLOAT, DOUBLE]\n\n\nclass Layout(object):\n    STD140 = ""std140""\n    STD430 = ""std430""\n    STDXXX = ""stdxxx""  # can not be inferred / does not matter\n\n\nclass Order(object):\n    COLUMN_MAJOR = ""column_major""\n    ROW_MAJOR = ""row_major""\n\n\nclass Access(object):\n    READ_ONLY = ""read_only""\n    WRITE_ONLY = ""write_only""\n    READ_WRITE = ""read_write""\n    NEITHER = ""neither""  # only ""metadata"" can be accessed, e.g. length of an array\n\n\nclass SpirvEnum(object):\n\n    _MAP = ()\n    _UNKNOWN = ""UNKNOWN""\n\n    @classmethod\n    def map(cls, key, keys, values, default=None):\n        if key in keys:\n            return values[keys.index(key)]\n        if default:\n            return default\n        raise RuntimeError(""Could not map {}"".format(key))\n\n    @classmethod\n    def to_spirv(cls, key):\n        keys, values = zip(*cls._MAP)\n        return cls.map(key, keys, values)\n\n    @classmethod\n    def from_spirv(cls, key):\n        values, keys = zip(*cls._MAP)\n        return cls.map(key, keys, values, default=cls._UNKNOWN)\n\n    @classmethod\n    def keys(cls):\n        return list(zip(*cls._MAP))[0]\n\n\nclass SourceLanguage(SpirvEnum):\n\n    UNKNOWN = ""UNKNOWN""\n    ESSL = ""ESSL""\n    GLSL = ""GLSL""\n    OPEN_CL_C = ""OPEN_CL_C""\n    OPEN_CL_CPP = ""OPEN_CL_CPP""\n    HLSL = ""HLSL""\n\n    _MAP = (\n        (UNKNOWN, 0), (ESSL, 1), (GLSL, 2), (OPEN_CL_C, 3), (OPEN_CL_CPP, 4), (HLSL, 5),\n    )\n\n\nclass ExecutionModel(SpirvEnum):\n\n    VERTEX = ""VERTEX""\n    TESSELATION_CONTROL = ""TESSELATION_CONTROL""\n    TESSELATION_EVALUATION = ""TESSELATION_EVALUATION""\n    GEOMETRY = ""GEOMETRY""\n    FRAGMENT = ""FRAGMENT""\n    GL_COMPUTE = ""GL_COMPUTE""\n    KERNEL = ""KERNEL""\n\n    _MAP = (\n        (VERTEX, 0), (TESSELATION_CONTROL, 1), (TESSELATION_EVALUATION, 2), (GEOMETRY, 3), (FRAGMENT, 4),\n        (GL_COMPUTE, 5), (KERNEL, 6)\n    )\n\n\nclass ExecutionMode(SpirvEnum):\n\n    INVOCATIONS = ""INVOCATIONS""\n    SPACING_EQUAL = ""SPACING_EQUAL""\n    SPACING_FRACTIONAL_EVEN = ""SPACING_FRACTIONAL_EVEN""\n    SPACING_FRACTIONAL_ODD = ""SPACING_FRACTIONAL_ODD""\n    VERTEX_ORDER_CW = ""VERTEX_ORDER_CW""\n    VERTEX_ORDER_CCW = ""VERTEX_ORDER_CCW""\n    PIXEL_CENTER_INTEGER = ""PIXEL_CENTER_INTEGER""\n    ORIGIN_UPPER_LEFT = ""ORIGIN_UPPER_LEFT""\n    ORIGIN_LOWER_LEFT = ""ORIGIN_LOWER_LEFT""\n    EARLY_FRAGMENT_TESTS = ""EARLY_FRAGMENT_TESTS""\n    POINT_MODE = ""POINT_MODE""\n    XFB = ""XFB""\n    DEPTH_REPLACING = ""DEPTH_REPLACING""\n    DEPTH_GREATER = ""DEPTH_GREATER""\n    DEPTH_LESS = ""DEPTH_LESS""\n    DEPTH_UNCHANGED = ""DEPTH_UNCHANGED""\n    LOCAL_SIZE = ""LOCAL_SIZE""\n    LOCAL_SIZE_HINT = ""LOCAL_SIZE_HINT""\n    INPUT_POINTS = ""INPUT_POINTS""\n    INPUT_LINES = ""INPUT_LINES""\n    INPUT_LINES_ADJACENCY = ""INPUT_LINES_ADJACENCY""\n    TRIANGLES = ""TRIANGLES""\n    INPUT_TRIANGLES_ADJACENCY = ""INPUT_TRIANGLES_ADJACENCY""\n    QUADS = ""QUADS""\n    ISOLINES = ""ISOLINES""\n    OUTPUT_VERTICES = ""OUTPUT_VERTICES""\n    OUTPUT_POINTS = ""OUTPUT_POINTS""\n    OUTPUT_LINE_STRIP = ""OUTPUT_LINE_STRIP""\n    OUTPUT_TRIANGLE_STRIP = ""OUTPUT_TRIANGLE_STRIP""\n    VEC_TYPE_HINT = ""VEC_TYPE_HINT""\n    CONTRACTION_OFF = ""CONTRACTION_OFF""\n    INITIALIZER = ""INITIALIZER""\n    FINALIZER = ""FINALIZER""\n    SUBGROUP_SIZE = ""SUBGROUP_SIZE""\n    SUBGROUPS_PER_WORKGROUP = ""SUBGROUPS_PER_WORKGROUP""\n    SUBGROUPS_PER_WORKGROUP_ID = ""SUBGROUPS_PER_WORKGROUP_ID""\n    LOCAL_SIZE_ID = ""LOCAL_SIZE_ID""\n    LOCAL_SIZE_HINT_ID = ""LOCAL_SIZE_HINT_ID""\n    POST_DEPTH_COVERAGE = ""POST_DEPTH_COVERAGE""\n    STENCIL_REF_REPLACING_EXT = ""STENCIL_REF_REPLACING_EXT""\n\n    _MAP = (\n        (INVOCATIONS, 0), (SPACING_EQUAL, 1), (SPACING_FRACTIONAL_EVEN, 2), (SPACING_FRACTIONAL_ODD, 3),\n        (VERTEX_ORDER_CW, 4), (VERTEX_ORDER_CCW, 5), (PIXEL_CENTER_INTEGER, 6), (ORIGIN_UPPER_LEFT, 7),\n        (ORIGIN_LOWER_LEFT, 8), (EARLY_FRAGMENT_TESTS, 9), (POINT_MODE, 10), (XFB, 11), (DEPTH_REPLACING, 12),\n        (DEPTH_GREATER, 14), (DEPTH_LESS, 15), (DEPTH_UNCHANGED, 16), (LOCAL_SIZE, 17), (LOCAL_SIZE_HINT, 18),\n        (INPUT_POINTS, 19), (INPUT_LINES, 20), (INPUT_LINES_ADJACENCY, 21), (TRIANGLES, 22),\n        (INPUT_TRIANGLES_ADJACENCY, 23), (QUADS, 24), (ISOLINES, 25), (OUTPUT_VERTICES, 26), (OUTPUT_POINTS, 27),\n        (OUTPUT_LINE_STRIP, 28), (OUTPUT_TRIANGLE_STRIP, 29), (VEC_TYPE_HINT, 30), (CONTRACTION_OFF, 31),\n        (INITIALIZER, 33), (FINALIZER, 34), (SUBGROUP_SIZE, 35), (SUBGROUPS_PER_WORKGROUP, 36),\n        (SUBGROUPS_PER_WORKGROUP_ID, 37), (LOCAL_SIZE_ID, 38), (LOCAL_SIZE_HINT_ID, 39),\n        (POST_DEPTH_COVERAGE, 4446), (STENCIL_REF_REPLACING_EXT, 5027),\n    )\n\n\nclass Decoration(SpirvEnum):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#Decoration\n\n    RELAXED_PRECISION = ""RELAXED_PRECISION""\n    SPEC_ID = ""SPEC_ID""\n    BLOCK = ""BLOCK""\n    BUFFER_BLOCK = ""BUFFER_BLOCK""\n    ROW_MAJOR = ""ROW_MAJOR""\n    COL_MAJOR = ""COL_MAJOR""\n    ARRAY_STRIDE = ""ARRAY_STRIDE""\n    MATRIX_STRIDE = ""MATRIX_STRIDE""\n    GLSL_SHARED = ""GLSL_SHARED""\n    GLSL_PACKED = ""GLSL_PACKED""\n    C_PACKED = ""C_PACKED""\n    BUILT_IN = ""BUILT_IN""\n    NO_PERSPECTIVE = ""NO_PERSPECTIVE""\n    FLAT = ""FLAT""\n    PATCH = ""PATCH""\n    CENTROID = ""CENTROID""\n    SAMPLE = ""SAMPLE""\n    INVARIANT = ""INVARIANT""\n    RESTRICT = ""RESTRICT""\n    ALIASED = ""ALIASED""\n    VOLATILE = ""VOLATILE""\n    CONSTANT = ""CONSTANT""\n    COHERENT = ""COHERENT""\n    NON_WRITABLE = ""NON_WRITABLE""\n    NON_READABLE = ""NON_READABLE""\n    UNIFORM = ""UNIFORM""\n    SATURATED_CONVERSION = ""SATURATED_CONVERSION""\n    STREAM = ""STREAM""\n    LOCATION = ""LOCATION""\n    COMPONENT = ""COMPONENT""\n    INDEX = ""INDEX""\n    BINDING = ""BINDING""\n    DESCRIPTOR_SET = ""DESCRIPTOR_SET""\n    OFFSET = ""OFFSET""\n    XFB_BUFFER = ""XFB_BUFFER""\n    XFB_STRIDE = ""XFB_STRIDE""\n    FUNC_PARAM_ATTR = ""FUNC_PARAM_ATTR""\n    FP_ROUNDING_MODE = ""FP_ROUNDING_MODE""\n    FP_FAST_MATH_MODE = ""FP_FAST_MATH_MODE""\n    LINKAGE_ATTRIBUTES = ""LINKAGE_ATTRIBUTES""\n    NO_CONTRACTION = ""NO_CONTRACTION""\n    INPUT_ATTACHMENT_INDEX = ""INPUT_ATTACHMENT_INDEX""\n    ALIGNMENT = ""ALIGNMENT""\n    MAX_BYTE_OFFSET = ""MAX_BYTE_OFFSET""\n    ALIGNMENT_ID = ""ALIGNMENT_ID""\n    MAX_BYTE_OFFSET_ID = ""MAX_BYTE_OFFSET_ID""\n    EXPLICIT_INTERP_AMD = ""EXPLICIT_INTERP_AMD""\n    OVERRIDE_COVERAGE_NV = ""OVERRIDE_COVERAGE_NV""\n    PASSTHROUGH_NV = ""PASSTHROUGH_NV""\n    VIEWPORT_RELATIVE_NV = ""VIEWPORT_RELATIVE_NV""\n    SECONDARY_VIEWPORT_RELATIVE_NV = ""SECONDARY_VIEWPORT_RELATIVE_NV""\n    HLSL_COUNTER_BUFFER_GOOGLE = ""HLSL_COUNTER_BUFFER_GOOGLE""\n    HLSL_SEMANTIC_GOOGLE = ""HLSL_SEMANTIC_GOOGLE""\n\n    _MAP = (\n        (RELAXED_PRECISION, 0), (SPEC_ID, 1), (BLOCK, 2), (BUFFER_BLOCK, 3), (ROW_MAJOR, 4), (COL_MAJOR, 5),\n        (ARRAY_STRIDE, 6), (MATRIX_STRIDE, 7), (GLSL_SHARED, 8), (GLSL_PACKED, 9), (C_PACKED, 10), (BUILT_IN, 11),\n        (NO_PERSPECTIVE, 13), (FLAT, 14), (PATCH, 15), (CENTROID, 16), (SAMPLE, 17), (INVARIANT, 18),\n        (RESTRICT, 19), (ALIASED, 20), (VOLATILE, 21), (CONSTANT, 22), (COHERENT, 23), (NON_WRITABLE, 24),\n        (NON_READABLE, 25), (UNIFORM, 26), (SATURATED_CONVERSION, 28), (STREAM, 29), (LOCATION, 30),\n        (COMPONENT, 31), (INDEX, 32), (BINDING, 33), (DESCRIPTOR_SET, 34), (OFFSET, 35), (XFB_BUFFER, 36),\n        (XFB_STRIDE, 37), (FUNC_PARAM_ATTR, 38), (FP_ROUNDING_MODE, 39), (FP_FAST_MATH_MODE, 40),\n        (LINKAGE_ATTRIBUTES, 41), (NO_CONTRACTION, 42), (INPUT_ATTACHMENT_INDEX, 43), (ALIGNMENT, 44),\n        (MAX_BYTE_OFFSET, 45), (ALIGNMENT_ID, 46), (MAX_BYTE_OFFSET_ID, 47), (EXPLICIT_INTERP_AMD, 4999),\n        (OVERRIDE_COVERAGE_NV, 5248), (PASSTHROUGH_NV, 5250), (VIEWPORT_RELATIVE_NV, 5252),\n        (SECONDARY_VIEWPORT_RELATIVE_NV, 5256), (HLSL_COUNTER_BUFFER_GOOGLE, 5634), (HLSL_SEMANTIC_GOOGLE, 5635),\n    )\n\n\nclass StorageClass(SpirvEnum):\n    # https://www.khronos.org/registry/spir-v/specs/1.2/SPIRV.pdf#subsection.3.7\n\n    UNIFORM_CONSTANT = ""UNIFORM_CONSTANT""\n    INPUT = ""INPUT""\n    UNIFORM = ""UNIFORM""\n    OUTPUT = ""OUTPUT""\n    WORKGROUP = ""WORKGROUP""\n    CROSS_WORKGROUP = ""CROSS_WORKGROUP""\n    PRIVATE = ""PRIVATE""\n    FUNCTION = ""FUNCTION""\n    GENERIC = ""GENERIC""\n    PUSH_CONSTANT = ""PUSH_CONSTANT""\n    ATOMIC_COUNTER = ""ATOMIC_COUNTER""\n    IMAGE = ""IMAGE""\n    STORAGE_BUFFER = ""STORAGE_BUFFER""\n\n    _MAP = (\n        (UNIFORM_CONSTANT, 0), (INPUT, 1), (UNIFORM, 2), (OUTPUT, 3), (WORKGROUP, 4), (CROSS_WORKGROUP, 5),\n        (PRIVATE, 6), (FUNCTION, 7), (GENERIC, 8), (PUSH_CONSTANT, 9), (ATOMIC_COUNTER, 10), (IMAGE, 11),\n        (STORAGE_BUFFER, 12),\n    )\n'"
lava/api/constants/vk.py,0,"b'# -*- coding: UTF-8 -*-\n\nimport lava.api.vulkan as vk\n\n\nVALIDATION_LAYERS = [""VK_LAYER_LUNARG_standard_validation""]\nSHADER_ENTRY = ""main""\nTIMEOUT_FOREVER = 0xFFFFFFFFFFFFFFFF\n\n\nclass VulkanEnum(object):\n\n    _MAP = ()\n\n    @classmethod\n    def map(cls, key, keys, values):\n        return values[keys.index(key)]\n\n    @classmethod\n    def to_vulkan(cls, key):\n        keys, values = zip(*cls._MAP)\n        return cls.map(key, keys, values)\n\n    @classmethod\n    def from_vulkan(cls, key):\n        values, keys = zip(*cls._MAP)\n        return cls.map(key, keys, values)\n\n    @classmethod\n    def keys(cls):\n        return list(zip(*cls._MAP))[0]\n\n\nclass DeviceType(VulkanEnum):\n\n    CPU = ""CPU""\n    DISCRETE_GPU = ""DISCRETE_CPU""\n    INTEGRATED_GPU = ""INTEGRATED_GPU""\n    VIRTUAL_GPU = ""VIRTUAL_GPU""\n    OTHER = ""OTHER""\n\n    _MAP = (\n        (CPU, vk.VK_PHYSICAL_DEVICE_TYPE_CPU),\n        (DISCRETE_GPU, vk.VK_PHYSICAL_DEVICE_TYPE_DISCRETE_GPU),\n        (INTEGRATED_GPU, vk.VK_PHYSICAL_DEVICE_TYPE_INTEGRATED_GPU),\n        (VIRTUAL_GPU, vk.VK_PHYSICAL_DEVICE_TYPE_VIRTUAL_GPU)\n    )\n\n\nclass QueueType(VulkanEnum):\n\n    GRAPHICS = ""GRAPHICS""\n    COMPUTE = ""COMPUTE""\n    TRANSFER = ""TRANSFER""\n    SPARSE_BINDING = ""SPARSE_BINDING""\n\n    _MAP = (\n        (GRAPHICS, vk.VK_QUEUE_GRAPHICS_BIT),\n        (COMPUTE, vk.VK_QUEUE_COMPUTE_BIT),\n        (TRANSFER, vk.VK_QUEUE_TRANSFER_BIT),\n        (SPARSE_BINDING, vk.VK_QUEUE_SPARSE_BINDING_BIT),\n    )\n\n\nclass MemoryType(VulkanEnum):\n    # https://www.khronos.org/registry/vulkan/specs/1.1-extensions/man/html/VkMemoryPropertyFlagBits.html\n\n    DEVICE_LOCAL = ""DEVICE_LOCAL""\n    HOST_VISIBLE = ""HOST_VISIBLE""\n    HOST_COHERENT = ""HOST_COHERENT""\n    HOST_CACHED = ""HOST_CACHED""\n    LAZILY_ALLOCATED = ""LAZILY_ALLOCATED""\n    # PROTECTED = ""PROTECTED""\n\n    CPU = [HOST_COHERENT, HOST_VISIBLE]\n    GPU = [DEVICE_LOCAL]\n\n    _MAP = (\n        (DEVICE_LOCAL, vk.VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT),\n        (HOST_VISIBLE, vk.VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT),\n        (HOST_COHERENT, vk.VK_MEMORY_PROPERTY_HOST_COHERENT_BIT),\n        (HOST_CACHED, vk.VK_MEMORY_PROPERTY_HOST_CACHED_BIT),\n        (LAZILY_ALLOCATED, vk.VK_MEMORY_PROPERTY_LAZILY_ALLOCATED_BIT),\n    )\n\n\nclass BufferUsage(VulkanEnum):\n    # https://www.khronos.org/registry/vulkan/specs/1.1-extensions/man/html/VkBufferUsageFlagBits.html\n\n    UNIFORM_BUFFER = ""UNIFORM_BUFFER""\n    STORAGE_BUFFER = ""STORAGE_BUFFER""\n    TRANSFER_SRC = ""TRANSFER_SRC""\n    TRANSFER_DST = ""TRANSFER_DST""\n\n    _MAP = (\n        (UNIFORM_BUFFER, vk.VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT),\n        (STORAGE_BUFFER, vk.VK_BUFFER_USAGE_STORAGE_BUFFER_BIT),\n        (TRANSFER_SRC, vk.VK_BUFFER_USAGE_TRANSFER_SRC_BIT),\n        (TRANSFER_DST, vk.VK_BUFFER_USAGE_TRANSFER_DST_BIT)\n    )\n\n\nclass DescriptorType(VulkanEnum):\n    # https://www.khronos.org/registry/vulkan/specs/1.1-extensions/man/html/VkDescriptorType.html\n\n    SAMPLER = ""SAMPLER""\n    UNIFORM_BUFFER = ""UNIFORM_BUFFER""\n    STORAGE_BUFFER = ""STORAGE_BUFFER""\n\n    _MAP = (\n        (SAMPLER, vk.VK_DESCRIPTOR_TYPE_SAMPLER),\n        (UNIFORM_BUFFER, vk.VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER),\n        (STORAGE_BUFFER, vk.VK_DESCRIPTOR_TYPE_STORAGE_BUFFER),\n    )\n\n\nclass CommandBufferUsage(VulkanEnum):\n    # https://www.khronos.org/registry/vulkan/specs/1.1-extensions/man/html/VkCommandBufferUsageFlagBits.html\n\n    ONE_TIME_SUBMIT = ""ONE_TIME_SUBMIT""\n    SIMULTANEOUS_USE = ""SIMULTANEOUS""\n\n    _MAP = (\n        (ONE_TIME_SUBMIT, vk.VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT),\n        (SIMULTANEOUS_USE, vk.VK_COMMAND_BUFFER_USAGE_SIMULTANEOUS_USE_BIT)\n    )\n\n\n\n\n'"
test/api/bytes/__init__.py,0,b''
test/api/bytes/cache.py,0,"b'# -*- coding: UTF-8 -*-\n\nimport pprint\nimport unittest\n\nimport numpy as np\n\nfrom lava.api.bytes import Array, ByteCache, Vector, Scalar, Struct\nfrom lava.api.constants.spirv import Layout, Order\nfrom lava.api.constants.vk import BufferUsage\n\nfrom test.api.base import GlslBasedTest\n\n\nclass TestByteCache(GlslBasedTest):\n\n    @unittest.skip(""test for development purposes"")\n    def test_manually(self):\n        # byte cache test\n        buffer_usage = BufferUsage.STORAGE_BUFFER\n        buffer_layout = Layout.STD430\n        buffer_order = Order.ROW_MAJOR\n\n        struct1 = Struct([Vector.vec3(), Vector.ivec2()], buffer_layout, member_names=[""a"", ""b""], type_name=""structB"")\n        struct2 = Struct([Scalar.double(), Scalar.double(), struct1], buffer_layout, type_name=""structC"")\n\n        structs = [struct1, struct2]\n\n        variables = [\n            Scalar.uint(),\n            Array(Vector.vec2(), (5, 2, 3), buffer_layout),\n            Array(Scalar.float(), 5, buffer_layout),\n            struct2,  # this struct needs padding at the end\n            Scalar.uint(),\n            Array(struct1, (2, 3), buffer_layout)\n        ]\n\n        container = Struct(variables, buffer_layout, type_name=""block"")\n\n        cache = ByteCache(container)\n\n        print("""")\n        print("""")\n        pprint.pprint(cache.values)\n        print(cache[-1][0][0][""a""])\n        print("""")\n        print("""")\n        pprint.pprint(cache)\n        print(cache[-1][0][0])\n        print("""")\n        print("""")\n        pprint.pprint(cache.get_as_dict())\n\n'"
test/api/bytes/transfer_in.py,13,"b'# -*- coding: UTF-8 -*-\n\nimport itertools\nimport unittest\n\nimport numpy as np\n\nfrom lava.api.bytes import Array, Matrix, Vector, Scalar, Struct\nfrom lava.api.constants.spirv import DataType, Layout, Order\nfrom lava.api.constants.vk import BufferUsage\n\nfrom test.api.base import GlslBasedTest, Random\n\n\nclass TestCpuToShader(GlslBasedTest):\n    """"""Transferring data in arbitrary order into the shader""""""\n\n    @classmethod\n    def build_glsl_program(cls, container, structs, buffer_usage):\n        template = """"""\n            #version 450\n            #extension GL_ARB_separate_shader_objects : enable\n            \n            layout(local_size_x=1, local_size_y=1, local_size_z=1) in;\n            \n            {}\n            \n            {}\n            \n            layout(std430, binding = 1) buffer dataOut {{\n                float array[];\n            }}; // stdout430 so we have no array alignment fiddling\n            \n            void main() {{\n            {}\n            }}""""""\n\n        glsl_structs = ""\\n\\n"".join([cls.build_glsl_struct_definition(struct) for struct in structs])\n        glsl_block = cls.build_glsl_block_definition(container, binding=0, usage=buffer_usage)\n        glsl_assignments, _ = cls.build_glsl_assignments(container.definitions)\n\n        return template.format(glsl_structs, glsl_block, glsl_assignments)\n\n    def run_test(self, container, structs, buffer_usage):\n        glsl = self.build_glsl_program(container, structs, buffer_usage)\n\n        values, array_expected = self.build_values(container.definitions)\n        array_expected = np.array(array_expected, dtype=np.float32)\n\n        bytez_input = container.to_bytes(values)\n        bytez_output = self.run_program(glsl, bytez_input, array_expected.nbytes, usage_input=buffer_usage, verbose=False)\n        array = np.frombuffer(bytez_output, dtype=array_expected.dtype)\n\n        diff = array_expected - array\n        equal = (diff == 0).all()\n        if not equal:\n            np.set_printoptions(precision=3, suppress=True)\n            print(""{}\\n"".format(glsl))\n            print(""{}\\nexpected\\n{}\\nactual\\n{}\\ndiff\\n{}\\n"".format(container, array_expected, array, diff))\n        self.assertTrue(equal)\n\n    @unittest.skip(""test for development purposes"")\n    def test_manual(self):\n        buffer_usage = BufferUsage.STORAGE_BUFFER\n        buffer_layout = Layout.STD140\n        buffer_order = Order.ROW_MAJOR\n\n        structA = Struct([Vector.ivec2(), Scalar.double()], buffer_layout, member_names=[""a"", ""b""], type_name=""structA"")\n        structB = Struct([Scalar.uint(), Scalar.double()], buffer_layout, type_name=""structB"")\n        structC = Struct([structB, Vector.ivec2()], buffer_layout, type_name=""structC"")\n\n        structs = [structA, structB, structC]\n\n        variables = [\n            Vector.vec3(),\n            Vector.ivec4(),\n            Array(structC, 2, buffer_layout),\n            Vector.ivec4(),\n            Scalar.uint(),\n            Array(Scalar.double(), (5, 2), buffer_layout),\n            Scalar.int(),\n            Array(Vector.vec4(), (2, 3, 4), buffer_layout),\n            Vector.dvec2(),\n            structA\n        ]\n\n        container = Struct(variables, buffer_layout, type_name=""block"")\n        print(container)\n\n        glsl = self.build_glsl_program(container, structs, buffer_usage)\n        # print(glsl)\n\n        values, array_expected = self.build_values(container.definitions)\n        array_expected = np.array(array_expected, dtype=np.float32)\n\n        bytez_input = container.to_bytes(values)\n        bytez_output = self.run_program(glsl, bytez_input, array_expected.nbytes, usage_input=buffer_usage)\n        array = np.frombuffer(bytez_output, dtype=array_expected.dtype)\n\n        print(array_expected)\n        print(array)\n        print(""equal"", ((array_expected - array) == 0).all())\n\n    def test_scalars_and_vectors(self):\n        rng = np.random.RandomState(123)\n\n        variables = [Scalar.uint(), Scalar.int(), Scalar.float(), Scalar.double()]\n        variables += [Vector(n, dtype) for n, dtype in itertools.product(range(2, 5), DataType.ALL)]\n\n        containers_std140 = [Struct(variables, Layout.STD140)]\n        containers_std430 = [Struct(variables, Layout.STD430)]\n\n        for _ in range(5):\n            containers_std140.append(Struct(rng.permutation(variables), Layout.STD140))\n            containers_std430.append(Struct(rng.permutation(variables), Layout.STD430))\n\n        for container in containers_std140:\n            self.run_test(container, [], BufferUsage.STORAGE_BUFFER)\n            self.run_test(container, [], BufferUsage.UNIFORM_BUFFER)\n        for container in containers_std430:\n            self.run_test(container, [], BufferUsage.STORAGE_BUFFER)\n\n    def test_scalars_and_vectors_and_matrices(self):\n        rng = np.random.RandomState(123)\n\n        variables = [Scalar.uint(), Scalar.int(), Scalar.float(), Scalar.double()]\n        variables += [Vector(n, dtype) for n, dtype in itertools.product(range(2, 5), DataType.ALL)]\n\n        matrix_combinations = itertools.product(range(2, 5), range(2, 5), [DataType.FLOAT, DataType.DOUBLE])\n        variables_std140 = variables + [Matrix(n, m, dtype, Layout.STD140) for n, m, dtype in matrix_combinations]\n        variables_std430 = variables + [Matrix(n, m, dtype, Layout.STD430) for n, m, dtype in matrix_combinations]\n\n        containers_std140 = [Struct(variables_std140, Layout.STD140)]\n        containers_std430 = [Struct(variables_std430, Layout.STD430)]\n\n        for _ in range(5):\n            containers_std140.append(Struct(rng.permutation(variables_std140), Layout.STD140))\n            containers_std430.append(Struct(rng.permutation(variables_std430), Layout.STD430))\n\n        for container in containers_std140:\n            self.run_test(container, [], BufferUsage.STORAGE_BUFFER)\n            self.run_test(container, [], BufferUsage.UNIFORM_BUFFER)\n        for container in containers_std430:\n            self.run_test(container, [], BufferUsage.STORAGE_BUFFER)\n\n    def test_matrices(self):\n        # skipping ROW_MAJOR order for now since the glsl generation does not support it\n        for n, m, dtype, order, layout in itertools.product(range(2, 5), range(2, 5), [DataType.FLOAT, DataType.DOUBLE],\n                                                            [Order.COLUMN_MAJOR], [Layout.STD140, Layout.STD430]):\n            matrix = Matrix(n, m, dtype, layout, order)\n            container = Struct([matrix], layout)\n\n            self.run_test(container, [], BufferUsage.STORAGE_BUFFER)\n            if layout == Layout.STD140:\n                self.run_test(container, [], BufferUsage.UNIFORM_BUFFER)\n\n    def test_array_of_scalars(self):\n        rng = np.random.RandomState(123)\n        scalar_types = [Scalar.uint(), Scalar.int(), Scalar.float(), Scalar.double()]\n\n        for definition, layout, _ in itertools.product(scalar_types, [Layout.STD140, Layout.STD430], range(5)):\n            container = Struct([Array(definition, Random.shape(rng, 5, 7), layout)], layout)\n\n            self.run_test(container, [], BufferUsage.STORAGE_BUFFER)\n            if layout == Layout.STD140:\n                self.run_test(container, [], BufferUsage.UNIFORM_BUFFER)\n\n    def test_array_of_vectors(self):\n        rng = np.random.RandomState(123)\n        vector_types = [Vector(n, dtype) for n, dtype in itertools.product(range(2, 5), DataType.ALL)]\n\n        for definition, layout, _ in itertools.product(vector_types, [Layout.STD140, Layout.STD430], range(5)):\n            container = Struct([Array(definition, Random.shape(rng, 3, 5), layout)], layout)\n\n            self.run_test(container, [], BufferUsage.STORAGE_BUFFER)\n            if layout == Layout.STD140:\n                self.run_test(container, [], BufferUsage.UNIFORM_BUFFER)\n\n    def test_array_of_matrices(self):\n        # skipping ROW_MAJOR order for now since the glsl generation does not support it\n        rng = np.random.RandomState(123)\n        matrix_combinations = itertools.product(range(2, 5), range(2, 5), [DataType.FLOAT, DataType.DOUBLE])\n\n        for (n, m, dtype), layout, _ in itertools.product(matrix_combinations, [Layout.STD140, Layout.STD430], range(3)):\n            matrix = Matrix(n, m, dtype, layout)\n            container = Struct([Array(matrix, Random.shape(rng, 3, 5), layout)], layout)\n\n            self.run_test(container, [], BufferUsage.STORAGE_BUFFER)\n            if layout == Layout.STD140:\n                self.run_test(container, [], BufferUsage.UNIFORM_BUFFER)\n\n    def test_array_of_structs(self):\n        rng = np.random.RandomState(123)\n        simple = [Scalar.uint(), Scalar.int(), Scalar.float(), Scalar.double()]\n        simple += [Vector(n, dtype) for n, dtype in itertools.product(range(2, 5), DataType.ALL)]\n\n        for layout, _ in itertools.product([Layout.STD140, Layout.STD430], range(5)):\n            struct = Struct(rng.choice(simple, size=3, replace=False), layout, type_name=""SomeStruct"")\n            array = Array(struct, Random.shape(rng, 3, 5), layout)\n            container = Struct([array], layout)\n\n            self.run_test(container, [struct], BufferUsage.STORAGE_BUFFER)\n            if layout == Layout.STD140:\n                self.run_test(container, [struct], BufferUsage.UNIFORM_BUFFER)\n\n    def test_nested_with_structs(self):\n        rng = np.random.RandomState(123)\n\n        simple = [Scalar.uint(), Scalar.int(), Scalar.float(), Scalar.double()]\n        simple += [Vector(n, dtype) for n, dtype in itertools.product(range(2, 5), DataType.ALL)]\n\n        for layout, _ in itertools.product([Layout.STD140, Layout.STD430], range(10)):\n            matrices = [Matrix(n, m, dtype, layout) for n, m, dtype in\n                        itertools.product(range(2, 5), range(2, 5), [DataType.FLOAT, DataType.DOUBLE])]\n            simple_and_matrices = simple + matrices\n\n            struct = Struct(rng.choice(simple_and_matrices, size=4, replace=False), layout, type_name=""SomeStruct"")\n            structs = [struct]\n\n            for _ in range(4):\n                members = [structs[-1]] + rng.choice(simple_and_matrices, size=3, replace=False).tolist()\n                structs.append(Struct(rng.permutation(members), layout, type_name=""SomeStruct{}"".format(len(structs))))\n\n            container = structs[-1]\n            structs = structs[:-1]\n\n            self.run_test(container, structs, BufferUsage.STORAGE_BUFFER)\n            if layout == Layout.STD140:\n                self.run_test(container, structs, BufferUsage.UNIFORM_BUFFER)\n\n    def test_nested_with_arrays_of_structs(self):\n        rng = np.random.RandomState(123)\n\n        simple = [Scalar.uint(), Scalar.int(), Scalar.float(), Scalar.double()]\n        simple += [Vector(n, dtype) for n, dtype in itertools.product(range(2, 5), DataType.ALL)]\n\n        for layout, _ in itertools.product([Layout.STD140, Layout.STD430], range(10)):\n            matrices = [Matrix(n, m, dtype, layout) for n, m, dtype in\n                        itertools.product(range(2, 5), range(2, 5), [DataType.FLOAT, DataType.DOUBLE])]\n            simple_and_matrices = simple + matrices\n\n            struct = Struct(rng.choice(simple_and_matrices, size=4, replace=False), layout, type_name=""SomeStruct"")\n            structs = [struct]\n            arrays = [Array(struct, Random.shape(rng, 2, 3), layout)]\n\n            for _ in range(2):\n                members = [arrays[-1]] + rng.choice(simple_and_matrices, size=3, replace=False).tolist()\n                structs.append(Struct(rng.permutation(members), layout, type_name=""SomeStruct{}"".format(len(structs))))\n                arrays.append(Array(structs[-1], Random.shape(rng, 2, 3), layout))\n\n            container = structs[-1]\n            structs = structs[:-1]\n\n            self.run_test(container, structs, BufferUsage.STORAGE_BUFFER)\n            if layout == Layout.STD140:\n                self.run_test(container, structs, BufferUsage.UNIFORM_BUFFER)\n\n    def test_arb_example_std140(self):\n        layout = Layout.STD430\n\n        struct_a = Struct([\n            Scalar.int(),\n            Vector.uvec2()  # actually bvec2\n        ], layout, type_name=""structA"")\n\n        struct_b = Struct([\n            Vector.uvec3(),\n            Vector.vec2(),\n            Array(Scalar.float(), 2, layout),\n            Vector.vec2(),\n            Array(Matrix(3, 3, DataType.FLOAT, layout), 2, layout)\n        ], layout, type_name=""structB"")\n\n        container = Struct([\n            Scalar.float(),\n            Vector.vec2(),\n            Vector.vec3(),\n            struct_a,\n            Scalar.float(),\n            Array(Scalar.float(), 2, layout),\n            Matrix(2, 3, DataType.FLOAT, layout),\n            Array(struct_b, 2, layout)\n        ], layout)\n\n        self.run_test(container, [struct_a, struct_b], BufferUsage.STORAGE_BUFFER)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/api/bytes/transfer_out.py,11,"b'# -*- coding: UTF-8 -*-\n\nimport itertools\nimport unittest\n\nimport numpy as np\n\nfrom lava.api.bytes import Array, Matrix, Vector, Scalar, Struct\nfrom lava.api.constants.spirv import DataType, Layout, Order\nfrom lava.api.constants.vk import BufferUsage\n\nfrom test.api.base import GlslBasedTest, Random\n\n\nclass TestShaderToCpu(GlslBasedTest):\n    """"""Transferring data in arbitrary order from the shader""""""\n\n    @classmethod\n    def build_glsl_program(cls, container, structs, buffer_usage):\n        template = """"""\n            #version 450\n            #extension GL_ARB_separate_shader_objects : enable\n            \n            layout(local_size_x=1, local_size_y=1, local_size_z=1) in;\n            \n            layout(std430, binding = 0) buffer dataOut {{\n                float array[];\n            }}; // stdout430 so we have no array alignment fiddling\n            \n            {}\n            \n            {}\n            \n            \n            void main() {{\n            {}\n            }}""""""\n\n        glsl_structs = ""\\n\\n"".join([cls.build_glsl_struct_definition(struct) for struct in structs])\n        glsl_block = cls.build_glsl_block_definition(container, binding=1, usage=buffer_usage)\n        glsl_assignments, _ = cls.build_glsl_assignments(container.definitions, to_array=False)\n\n        return template.format(glsl_structs, glsl_block, glsl_assignments)\n\n    def run_test(self, container, structs, buffer_usage):\n        glsl = self.build_glsl_program(container, structs, buffer_usage)\n\n        values_expected, array = self.build_values(container.definitions)\n        array = np.array(array, dtype=np.float32)\n\n        bytez_in = array.tobytes()  # std430\n        bytez_out = self.run_program(glsl, bytez_in, container.size(), verbose=False)\n        values = container.from_bytes(bytez_out)\n\n        register = {}\n        steps = {Scalar: 0, Vector: 0, Matrix: 0, Array: 0, Struct: 0}\n\n        for struct in structs + [container]:\n            self.build_register(register, struct, steps)\n\n        values_ftd = self.format_values(container, values, register)\n        values_expected_ftd = self.format_values(container, values_expected, register)\n\n        equal = values_ftd == values_expected_ftd\n        if not equal:\n            np.set_printoptions(precision=3, suppress=True)\n            print(""{}"".format(glsl))\n            print(""\\nexepected"")\n            self.print_formatted_values(values_expected_ftd)\n            print(""\\nactual"")\n            self.print_formatted_values(values_ftd)\n\n        self.assertTrue(equal)\n\n    @unittest.skip(""test for development purposes"")\n    def test_manually(self):\n        buffer_usage = BufferUsage.STORAGE_BUFFER\n        buffer_layout = Layout.STD140\n        buffer_order = Order.ROW_MAJOR\n\n        structA = Struct([Vector.ivec2(), Scalar.double()], buffer_layout, member_names=[""a"", ""b""], type_name=""structA"")\n        structB = Struct([Scalar.uint(), Scalar.double()], buffer_layout, type_name=""structB"")\n        structC = Struct([structB, Vector.ivec2()], buffer_layout, type_name=""structC"")\n\n        structs = [structA, structB, structC]\n\n        variables = [\n            Vector.vec3(),\n            Vector.ivec4(),\n            Array(structC, 2, buffer_layout),\n            Vector.ivec4(),\n            Scalar.uint(),\n            Array(Scalar.double(), (5, 2), buffer_layout),\n            Scalar.int(),\n            Array(Vector.vec4(), (2, 3, 4), buffer_layout),\n            Vector.dvec2(),\n            structA\n        ]\n\n        container = Struct(variables, buffer_layout, type_name=""block"")\n        print(container)\n\n        glsl = self.build_glsl_program(container, structs, buffer_usage)\n        # print(glsl)\n\n        values_expected, array = self.build_values(container.definitions)\n        array = np.array(array, dtype=np.float32)\n\n        bytez_in = array.tobytes()  # std430\n        bytez_out = self.run_program(glsl, bytez_in, container.size())\n        values = container.from_bytes(bytez_out)\n\n        register = {}\n        steps = {Scalar: 0, Vector: 0, Array: 0, Struct: 0}\n\n        for struct in structs + [container]:\n            self.build_register(register, struct, steps)\n\n        values_ftd = self.format_values(container, values, register)\n        values_expected_ftd = self.format_values(container, values_expected, register)\n\n        print("""")\n        self.print_formatted_values(values_ftd)\n        print("""")\n        self.print_formatted_values(values_expected_ftd)\n        print("""")\n\n        print("""")\n\n        print(values_ftd == values_expected_ftd)\n\n    def test_scalars_and_vectors(self):\n        rng = np.random.RandomState(123)\n\n        variables = [Scalar.uint(), Scalar.int(), Scalar.float(), Scalar.double()]\n        variables += [Vector(n, dtype) for n, dtype in itertools.product(range(2, 5), DataType.ALL)]\n\n        matrix_combinations = itertools.product(range(2, 5), range(2, 5), [DataType.FLOAT, DataType.DOUBLE])\n        variables_std140 = variables + [Matrix(n, m, dtype, Layout.STD140) for n, m, dtype in matrix_combinations]\n        variables_std430 = variables + [Matrix(n, m, dtype, Layout.STD430) for n, m, dtype in matrix_combinations]\n\n        containers_std140 = [Struct(variables_std140, Layout.STD140)]\n        containers_std430 = [Struct(variables_std430, Layout.STD430)]\n\n        for _ in range(5):\n            containers_std140.append(Struct(rng.permutation(variables_std140), Layout.STD140))\n            containers_std430.append(Struct(rng.permutation(variables_std430), Layout.STD430))\n\n        for container in containers_std140 + containers_std430:\n            self.run_test(container, [], BufferUsage.STORAGE_BUFFER)\n\n    def test_scalars_and_vectors_and_matrices(self):\n        rng = np.random.RandomState(123)\n\n        variables = [Scalar.uint(), Scalar.int(), Scalar.float(), Scalar.double()]\n        variables += [Vector(n, dtype) for n, dtype in itertools.product(range(2, 5), DataType.ALL)]\n\n        matrix_combinations = itertools.product(range(2, 5), range(2, 5), [DataType.FLOAT, DataType.DOUBLE])\n        variables_std140 = variables + [Matrix(n, m, dtype, Layout.STD140) for n, m, dtype in matrix_combinations]\n        variables_std430 = variables + [Matrix(n, m, dtype, Layout.STD430) for n, m, dtype in matrix_combinations]\n\n        containers_std140 = [Struct(variables_std140, Layout.STD140)]\n        containers_std430 = [Struct(variables_std430, Layout.STD430)]\n\n        for _ in range(5):\n            containers_std140.append(Struct(rng.permutation(variables_std140), Layout.STD140))\n            containers_std430.append(Struct(rng.permutation(variables_std430), Layout.STD430))\n\n        for container in containers_std140 + containers_std430:\n            self.run_test(container, [], BufferUsage.STORAGE_BUFFER)\n\n    def test_matrices(self):\n        # skipping ROW_MAJOR order for now since the glsl generation does not support it\n        for n, m, dtype, order, layout in itertools.product(range(2, 5), range(2, 5), [DataType.FLOAT, DataType.DOUBLE],\n                                                            [Order.COLUMN_MAJOR], [Layout.STD140, Layout.STD430]):\n            matrix = Matrix(n, m, dtype, layout, order)\n            container = Struct([matrix], layout)\n\n            self.run_test(container, [], BufferUsage.STORAGE_BUFFER)\n\n    def test_array_of_scalars(self):\n        scalar_types = [Scalar.uint(), Scalar.int(), Scalar.float(), Scalar.double()]\n        rng = np.random.RandomState(123)\n        containers = []\n\n        for definition, layout, _ in itertools.product(scalar_types, [Layout.STD140, Layout.STD430], range(5)):\n            containers.append(Struct([Array(definition, Random.shape(rng, 5, 7), layout)], layout))\n\n        for container in containers:\n            self.run_test(container, [], BufferUsage.STORAGE_BUFFER)\n\n    def test_array_of_vectors(self):\n        vector_types = [Vector(n, dtype) for n, dtype in itertools.product(range(2, 5), DataType.ALL)]\n        rng = np.random.RandomState(123)\n        containers = []\n\n        for definition, layout, _ in itertools.product(vector_types, [Layout.STD140, Layout.STD430], range(5)):\n            containers.append(Struct([Array(definition, Random.shape(rng, 3, 5), layout)], layout))\n\n        for container in containers:\n            self.run_test(container, [], BufferUsage.STORAGE_BUFFER)\n\n    def test_array_of_matrices(self):\n        # skipping ROW_MAJOR order for now since the glsl generation does not support it\n        rng = np.random.RandomState(123)\n        matrix_combinations = itertools.product(range(2, 5), range(2, 5), [DataType.FLOAT, DataType.DOUBLE])\n\n        for (n, m, dtype), layout, _ in itertools.product(matrix_combinations, [Layout.STD140, Layout.STD430], range(3)):\n            matrix = Matrix(n, m, dtype, layout)\n            container = Struct([Array(matrix, Random.shape(rng, 3, 5), layout)], layout)\n\n            self.run_test(container, [], BufferUsage.STORAGE_BUFFER)\n\n    def test_array_of_structs(self):\n        rng = np.random.RandomState(123)\n        data = []\n\n        simple = [Scalar.uint(), Scalar.int(), Scalar.float(), Scalar.double()]\n        simple += [Vector(n, dtype) for n, dtype in itertools.product(range(2, 5), DataType.ALL)]\n\n        for layout, _ in itertools.product([Layout.STD140, Layout.STD430], range(5)):\n            struct = Struct(rng.choice(simple, size=3, replace=False), layout, type_name=""SomeStruct"")\n            array = Array(struct, Random.shape(rng, 3, 5), layout)\n            container = Struct([array], layout)\n            data.append((container, [struct]))\n\n        for container, structs in data:\n            self.run_test(container, structs, BufferUsage.STORAGE_BUFFER)\n\n    def test_nested_with_structs(self):\n        rng = np.random.RandomState(123)\n        data = []\n\n        simple = [Scalar.uint(), Scalar.int(), Scalar.float(), Scalar.double()]\n        simple += [Vector(n, dtype) for n, dtype in itertools.product(range(2, 5), DataType.ALL)]\n\n        for layout, _ in itertools.product([Layout.STD140, Layout.STD430], range(10)):\n            matrices = [Matrix(n, m, dtype, layout) for n, m, dtype in\n                        itertools.product(range(2, 5), range(2, 5), [DataType.FLOAT, DataType.DOUBLE])]\n            simple_and_matrices = simple + matrices\n\n            struct = Struct(rng.choice(simple_and_matrices, size=4, replace=False), layout, type_name=""SomeStruct"")\n            structs = [struct]\n\n            for _ in range(4):\n                members = [structs[-1]] + rng.choice(simple_and_matrices, size=3, replace=False).tolist()\n                structs.append(Struct(rng.permutation(members), layout, type_name=""SomeStruct{}"".format(len(structs))))\n\n            data.append((structs[-1], structs[:-1]))\n\n        for container, structs in data:\n            self.run_test(container, structs, BufferUsage.STORAGE_BUFFER)\n\n    def test_nested_with_arrays_of_structs(self):\n        rng = np.random.RandomState(123)\n        data = []\n\n        simple = [Scalar.uint(), Scalar.int(), Scalar.float(), Scalar.double()]\n        simple += [Vector(n, dtype) for n, dtype in itertools.product(range(2, 5), DataType.ALL)]\n\n        for layout, _ in itertools.product([Layout.STD140, Layout.STD430], range(10)):\n            matrices = [Matrix(n, m, dtype, layout) for n, m, dtype in\n                        itertools.product(range(2, 5), range(2, 5), [DataType.FLOAT, DataType.DOUBLE])]\n            simple_and_matrices = simple + matrices\n\n            struct = Struct(rng.choice(simple_and_matrices, size=4, replace=False), layout, type_name=""SomeStruct"")\n            structs = [struct]\n            arrays = [Array(struct, Random.shape(rng, 2, 3), layout)]\n\n            for _ in range(2):\n                members = [arrays[-1]] + rng.choice(simple_and_matrices, size=3, replace=False).tolist()\n                structs.append(Struct(rng.permutation(members), layout, type_name=""SomeStruct{}"".format(len(structs))))\n                arrays.append(Array(structs[-1], Random.shape(rng, 2, 3), layout))\n\n            data.append((structs[-1], structs[:-1]))\n\n        for container, structs in data:\n            self.run_test(container, structs, BufferUsage.STORAGE_BUFFER)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
