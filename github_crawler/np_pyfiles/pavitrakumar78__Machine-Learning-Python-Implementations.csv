file_path,api_count,code
K-NearestNeighbors.py,6,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sun Jun 21 14:06:04 2015\n\n@author: Pavitrakumar\nCredits: Jason Brownlee[Machinelearningmastery.com]\n""""""\n\nfrom __future__ import division\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn import cross_validation\nfrom sklearn.metrics import mean_squared_error\nimport math\nimport operator\n\n""""""\nEuclidean distance measure: This is defined as the square root of the sum of the \nsquared differences between the two arrays of numbers\n""""""\n\ndef euclideanDistance(instance1, instance2, no_of_features):\n    distance = 0\n    for x in range(no_of_features):\n        distance += pow((instance1[x] - instance2[x]), 2)\n    return math.sqrt(distance)\n\n""""""\ngetNeighbors function returns k most similar neighbors from the training set \nfor a given test instance (using the already defined euclideanDistance function)\n""""""\ndef getNeighbors(X_train,y_train, test_instance, k):\n    # getting the k-nearest neighbors of the data point testInsatance\n    distances = []\n    no_of_features = len(test_instance)\n    for x,y in zip(X_train,y_train): \n        # we are finding distance from each training example to out testInstance data point\n        # and storing it as a list of pairs i.e (ith training example\'s response,distance to our instance data point)\n        dist = euclideanDistance(test_instance, x, no_of_features)\n        distances.append((y, dist)) \n    distances.sort(key=operator.itemgetter(1))\n    #sorting the list by the 2nd element in each pair - sorting by distance\n    #extracting the top k elements from the sorted list\n    #we only need the response\n    neighbors = [response for (response,distance) in distances]\n    neighbors = neighbors[0:k]\n    return neighbors\n""""""\ngetReponse just returns the most commonly occuring class in the given set of neighbors\n""""""\ndef getResponse(neighbors):\n    # neighbors is a vector of length k \n    # now, all we need to do is to find the most occuring class\n    counts = np.bincount(neighbors)\n    max_count = np.argmax(counts)\n    return max_count\n\ndef predict(X_test,X_train,y_train,k = 5):\n    predicted = []\n    for each_test_instance in X_test:\n        neighbors = getNeighbors(X_train,y_train,each_test_instance,k)\n        predicted.append(getResponse(neighbors))\n    return predicted\n    \n\ndef normalize(X):\n    X_norm = X\n    mu = np.zeros((1,X.shape[1]))\n    sigma = np.zeros((1,X.shape[1]))\n    """"""\n    First, for each feature dimension, compute the mean\n    of the feature and subtract it from the dataset,\n    storing the mean value in mu. Next, compute the \n    standard deviation of each feature and divide\n    each feature by it\'s standard deviation, storing\n    the standard deviation in sigma. \n    \n    Note that X is a matrix where each column is a \n    feature and each row is an example. You need \n    to perform the normalization separately for \n    each feature. - taken from Andrew Ng\'s comments\n    """"""\n    mu = np.mean(X,axis = 0)\n    #Taking column-wise mean\n    X_norm = X_norm - mu\n    sigma = np.std(X,axis = 0)\n    X_norm = X_norm/sigma\n    \n    return X_norm\n\n\n""""""\ntesting using IRIS data set\n""""""\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n#X = normalize(X) #if needed\n\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.8)\n\n\npred = predict(X_test,X_train,y_train)\n\nfrom sklearn.metrics import accuracy_score\n\nprint accuracy_score(y_test,pred)\n'"
LinearRegression.py,22,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Fri Jun 19 13:08:41 2015\n\n@author: Pavitrakumar\n""""""\nfrom __future__ import division\nimport numpy as np\nfrom sklearn import datasets\nfrom numpy.linalg import inv\nfrom sklearn import cross_validation\nfrom sklearn.metrics import mean_squared_error\n\n\n\n#We use gradient descent to learn the theta values for linear regression\n#Theta is the main parameter of our model\n\n#The dimensions of theta matrix is [no. of features] x 1\n#No. of features is the no. of columns in the X matrix\n\n#One way to solve for theta\n\ndef normal_eqn_theta(X,y):\n    #This is the closed-form solution to lienar regression\n    \n    #Insert a column(axis = 1) of 1s at 0th pos.\n    X = np.insert(X,0,1,axis=1)\n    \n    a = inv(np.dot(X.T , X))\n    b = X.T\n    c = y\n    \n    theta = np.dot(np.dot(a,b),c)\n    \n    #theta = y * X.T * inv(X*X.T)\n    #return np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(y)\n    return theta\n\n\n\n#Another way to solve for theta is to use gradient descent\n#For this method, we need to normalize our input matrix (X)\n\n\ndef gradient_descent_theta(X,y,alpha = 1e-7,num_iters = 500):\n    #X - we need to normalize and add bias terms\n    #print X[1:5,:],""\\n\\n""\n    X = normalize(X)\n    X = np.insert(X,0,1,axis=1)\n    theta = np.zeros((X.shape[1],1))\n    #print X[1:5,:]\n    #Alpha is the learning rate\n    #num_iters is the total number of gradient steps\n    print ""X shape is "",X.shape\n    print ""Theta shape is(b) "",theta.shape\n    \n    no_of_rows = len(y) # number of training examples\n    J_history = np.zeros((num_iters,1))\n    #We can make use of the J-History vector to visualize how the cost is minimized\n    y = np.asmatrix(y)\n    y = y.T\n    for i in range(num_iters):\n        h = np.dot(X,theta)\n        error = h - y\n        gradient = np.dot(error.T,X).T\n        theta_change = (alpha/no_of_rows)*gradient\n        theta = theta - theta_change\n        J_history[i] = compute_cost(X,y,theta)\n    print ""Theta shape is(a) "",theta.shape\n    return theta,J_history\n    \ndef compute_cost(X,y,theta):\n    #computes the cost of theta as parameter for linear regression to fit the \n    #data point in X and y\n    no_of_rows = len(y) # number of training examples\n    J = 0 # J is the cost\n    h = np.dot(X,theta)\n    square_error = np.power((h-y),2)\n    J = (1.0/(2.0*no_of_rows))*np.sum(square_error)\n    \n    return J\n        \ndef normalize(X):\n    X_norm = X\n    mu = np.zeros((1,X.shape[1]))\n    sigma = np.zeros((1,X.shape[1]))\n    """"""\n    First, for each feature dimension, compute the mean\n    of the feature and subtract it from the dataset,\n    storing the mean value in mu. Next, compute the \n    standard deviation of each feature and divide\n    each feature by it\'s standard deviation, storing\n    the standard deviation in sigma. \n    \n    Note that X is a matrix where each column is a \n    feature and each row is an example. You need \n    to perform the normalization separately for \n    each feature. - taken from Andrew Ng\'s comments\n    """"""\n    mu = np.mean(X,axis = 0)\n    #Taking column-wise mean\n    X_norm = X_norm - mu\n    sigma = np.std(X,axis = 0)\n    X_norm = X_norm/sigma\n    \n    return X_norm\n    \n\n\ndef lin_reg(X,theta): \n    X = np.insert(X,0,1,axis=1)\n    theta = np.asmatrix(theta)\n    print ""LR-theta shape"",theta.shape\n    print ""LR-X shape"",X.shape\n    pred = np.dot(X,theta)\n    return pred\n\n\n\n\n""""""\nLoading and training on toy dataset (boston land prices)\n""""""\n\nboston = datasets.load_boston()\n\n""""""\nlinear regression with multiple variables\n""""""\nX = boston.data\ny = boston.target\n\n""""""\n#linear regression with single variable\nX = np.asmatrix(boston.data[:,0]).T #taking only the 1st column\ny = boston.target\n""""""\n\n\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4)\n\n\n\nnormal_eqn_theta = np.asmatrix(normal_eqn_theta(X_train,y_train)).T\n\n\ngradient_descent_theta,_ = gradient_descent_theta(X_train,y_train)\n\n\n\n\npred1 = lin_reg(X_test,normal_eqn_theta)\npred2 = lin_reg(X_test,gradient_descent_theta)\n\n\n\nprint ""MSE for prediction using normal_eqn Theta is: "", mean_squared_error(y_test, pred1)  \nprint ""MSE for prediction using gradient_desc Theta is: "", mean_squared_error(y_test, pred2)  \n'"
LogisticRegression.py,13,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sat Jun 20 12:27:16 2015\n\n@author: Pavitrakumar\n""""""\n\nfrom __future__ import division\nimport numpy as np\nfrom sklearn import datasets\nfrom numpy.linalg import inv\nfrom sklearn import cross_validation\nfrom sklearn.metrics import mean_squared_error\nfrom scipy import optimize\n\n\ndef sigmoid(z):\n    g = 1.0 / (1.0 + np.exp(-z))\n    return g\n\n\n\n""""""\nThis function computes the cost (J) of using theta as parameter for \nregularized logistic regression and the gradient (grad) of the cost\nw.r.t to the parameters\n""""""\n\n#if returnJ is True, function returns J \n#if returnJ is False, function returns grad\ndef cost_function(theta,X,y,lam,returnJ = True):\n    #calculating cost\n    \n    no_of_rows = len(y)\n    J = 0\n\n    h = sigmoid(np.dot(X,theta))\n\n    #J = (1.0/no_of_rows)*np.sum(np.multiply(-y,np.log(h))-np.multiply((1-y),np.log(1-h)),axis = 0)\n    # both are same! (above and below)\n    J = (1.0/no_of_rows)*(np.dot(-y.T,np.log(h))-np.dot((1-y).T,np.log(1-h)))\n    \n    reg = (lam/(2.0*no_of_rows))*np.sum(np.power(theta[1:],2))\n    J = J + reg\n\n    if returnJ is True:\n        return J\n\n    #calculating gradient\n    grad = np.zeros(theta.shape)\n    error = h - y\n    grad[0] = (1.0/no_of_rows)*(np.dot(error.T,X))[0] #for theta0\n    grad[1:] = (((1.0/no_of_rows)*(np.dot(error.T,X)).T) + (lam/no_of_rows)*theta)[1:] # for rest of the theta terms except theta0\n        \n    return grad\n\n\ndef returnJ(theta,X,y,lam):\n    return cost_function(theta,X,y,lam)\n\ndef returnThetaGrad(theta,X,y,lam):\n    return cost_function(theta,X,y,lam,False)\n\n\n\n\ndef fit(X,y,maxiter = 50,method = \'TNC\',lam = 0.1):\n    no_of_rows = X.shape[0]\n    no_of_features = X.shape[1]\n    no_of_labels = len(set(y))\n    fit_theta = np.zeros((no_of_labels,no_of_features+1))\n    #adding a vector of ones to the X matrix(as the first column) - bias terms for each training exmaples\n    X = np.insert(X,0,1,axis=1)\n    \n    initial_theta = np.zeros((no_of_features+1,1))\n\n    for i in range(no_of_labels):\n        temp_y = (y == (i)) + 0 # here labels are 0,1,2,3.. if they are 1,2,3,4... use: temp_y = (y == (i+1))+0\n        #temp_y is a vector of size no_of_training_examples\n        #since each iteration corresponds to finding theta for a single class (one-vs-all)\n        #each time, we only take the predection of class \'i\'on all training example\n        \n        _res = optimize.fmin_cg(returnJ, fprime=returnThetaGrad,x0 = initial_theta,args=(X, temp_y,lam), maxiter=50, disp=False, full_output=True)\n        fit_theta[i,:] = _res[0]  \n        """"""\n        different minimization functions (above and below)\n        """"""\n        #options = {\'maxiter\': maxiter}\n        #_res = optimize.minimize(returnJ, initial_theta, jac=returnThetaGrad, method=method,args=(X, temp_y,lam), options=options)        \n        #fit_theta[i,:] = _res.x\n\n    return fit_theta\n\n\ndef predict(theta,X):\n    X = np.insert(X,0,1,axis=1)\n    h = np.dot(X,theta.T)\n    print h\n    return h.argmax(1)\n\n\n""""""\ntesting using IRIS data set\n""""""\nimport sklearn.datasets as datasets\nfrom sklearn import cross_validation\n\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.9)\n\n\n#The optimized theta values after training using the training data\ntheta = fit(X_train,y_train)\n\n    \npred = predict(theta,X_test)\n\nfrom sklearn.metrics import accuracy_score\n\nprint accuracy_score(y_test,pred)\n'"
NeuralNetwork.py,21,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Jun 18 12:14:35 2015\n\n@author: Pavitrakumar\n""""""\n\nfrom __future__ import division\nimport numpy as np\nfrom scipy import optimize\n\n""""""\nA simple neural netowork with 3 layers.\nNo. of units in Input layer  -  taken from dataset (X)\nNo. of units in Hidden layer -  given as user input\nNo. of units in Output layer -  taken from dataset(y)\n\nSo, there will be 2 theta matrices\nTheta1 for input->hidden\nTheta2 for hidden->output\n\nX is the features matrix - the features\ny is the response vector -the output\n""""""\n\n""""""\nSize of Theta1 is no.of hiddenlayers(in hidden layer1) x no. of input layers\ni.e simply the [A]x[B] where A on the left side corresponds to the no.of units\nin the layer on the left, similarly B on the right side(of the dim) corrensponds to\nthe no.of units in the layer on the right side \nSo, our Theta1\'s dim is hidden_layer_size x input_layer_size + 1 \nSimilarly the Theta2 it is output_layer_size x hidden_layer_size + 1\n\nNOTE: +1 is for the bias term\n""""""\n\n""""""\nNow, we implement the feedforward function to get the cost\ncost is h(x) the hypothesis calculated by feeding the input through the\nnetwork using the respective weights.\nAfter feed-forward we back propogate to find the delta values,\nusing this, we find the theta gradients which will be the input to an optimization algorithm\n[optimization algorithm used is from scikit]\n""""""\n\ndef sigmoid(z):\n    g = 1.0 / (1.0 + np.exp(-z))\n    return g\n    \n\ndef gradient_of_sigmoid(z):\n    sig = sigmoid(z)\n    g = sig * (1 - sig)\n    return g\n    \n    \ndef pack_thetas(Theta1, Theta2):\n    #reshaping a matrix here return a vector of size (no. of rows x no. of columns) \n    #i.e each row is appended one after the other to make a huge vector from a matirx\n    return np.concatenate((Theta1.reshape(-1), Theta2.reshape(-1)))\n        \ndef unpack_thetas(Theta_combi, input_layer_size, hidden_layer_size, output_layer_size):\n    Theta1_start = 0\n    Theta1_end = hidden_layer_size * (input_layer_size + 1)\n    #Using the total size (which we already know because of the dimensions of the matirx)\n    #We can restore the vector into a matrix by simply reshaping it as per its dimensions.\n    Theta1 = Theta_combi[Theta1_start:Theta1_end].reshape((hidden_layer_size, input_layer_size + 1))\n    Theta2 = Theta_combi[Theta1_end:].reshape((output_layer_size, hidden_layer_size + 1))\n    return Theta1, Theta2\n        \ndef rand_init(l_in, l_out,e):\n    #randomly initialize the weights for theta matrices in the range [-e,e]\n    return np.random.rand(l_out, l_in) * 2 * e - e\n        \n\ndef feed_forward_back_prop(Theta1,Theta2,input_layer_size,hidden_layer_size,output_layer_size,X,y,lam,returnJ = False):\n    J = 0\n    print ""Shape of y is: "" , np.shape(y),""\\n""\n    print ""Shape of X is: "" , np.shape(X),""\\n""\n    print ""output layer size is: "" , output_layer_size\n    \n    Theta1_gradient = np.zeros(np.shape(Theta1))\n    Theta2_gradient = np.zeros(np.shape(Theta2))\n    #for a multi-layer implementation, take the theta values in a single vector and unroll them   \n    \n    no_training_samples = np.shape(X)[0]\n    \n    #since its a classification problem, we build a new Y matrix(binary) from vector of Y\n    #i.e it now has output_layer_size columns - each column represents the probability of that class being the result(or output)\n    \n    new_Y = np.eye(output_layer_size)[y]\n\n    #adding a vector of ones to the X matrix(as the first column) - bias terms for each training example\n    X = np.insert(X,0,1,axis=1)\n    print ""head of X is:\\n""\n    print X[1:5,:]\n    \n    #Start the feed-forward steps\n    a1 = X\n    #a1 = [no. of training examples] x [input_layer_size + 1]\n    #theta1 = [hidden_layer_size] x [input_layer_size + 1]\n    #theta1 transpose = [input_layer_size + 1] x [hidden_layer_size]\n    z2 = np.dot(a1,Theta1.T)\n    #z2 = [no. of training examples]  x [hidden_layer_size]\n    a2 = sigmoid(z2)\n    #For each hidden layer(if it exists), this process continues\n    \n    #Now, a2 is the new treated as a new X and a similar process to above is carried out\n    a2 = np.insert(a2,0,1,axis=1)\n    z3 = np.dot(a2,Theta2.T)\n    a3 = sigmoid(z3)\n    \n    #Now we compute the cost function J(theta)\n    #f = positive cost - negative cost\n    f = np.sum(np.multiply(-new_Y,np.log(a3))-np.multiply((1-new_Y),np.log(1-a3)),axis = 0) # columnwise sum    \n    f = (1.0/no_training_samples)*f\n    f = sum(f)\n    J = f\n    print ""J is: "",J\n    \n    reg_term = (lam/(2.0*no_training_samples))*(np.sum(np.sum(np.power(Theta1[:,1:],2),axis=0))+np.sum(np.sum(np.power(Theta2[:,1:],2),axis=0)))\n    print ""Regularization term is: "",reg_term    \n    J = J + reg_term\n    print ""J after regularization: "",J,""\\n""\n    \n    if returnJ is True:\n        return J\n\n\n    #Now we need to calculate the gradients by backpropagating\n    \n    #for layer 3 (output layer)\n    s_delta_3 = a3-new_Y\n    #we are splicing theta2 to get rid of the bias terms\n    #for layer 2 (hidden layer)\n    s_delta_2 = np.multiply(np.dot(s_delta_3,Theta2[:,1:]),gradient_of_sigmoid(z2))\n    #nothing for input layer because we dont aassociate error terms with input layer.\n    #now,for accumulating the s_deltas, we use delta\n    \n    delta_2 = np.dot(a2.T,s_delta_3)\n    delta_1 = np.dot(a1.T,s_delta_2)\n    \n    Theta1_gradient = (1/no_training_samples)*delta_1.T\n    Theta2_gradient = (1/no_training_samples)*delta_2.T\n    \n    #Regularizing the theta terms\n\n    reg_term_T1 = (lam/no_training_samples)*Theta1[:,1:]\n    reg_term_T2 = (lam/no_training_samples)*Theta2[:,1:]\n    \n    Theta1_gradient[:,1:] += reg_term_T1\n    Theta2_gradient[:,1:] += reg_term_T2\n    \n    return Theta1_gradient,Theta2_gradient\n\n\ndef returnsJ(Theta_combi,input_layer_size,hidden_layer_size,output_layer_size,X,y,lam):\n    Theta1, Theta2 = unpack_thetas(Theta_combi, input_layer_size, hidden_layer_size, output_layer_size)\n    J = feed_forward_back_prop(Theta1,Theta2,input_layer_size,hidden_layer_size,output_layer_size,X,y,lam,True)\n    return J\n    \ndef returnThetaGrad(Theta_combi,input_layer_size,hidden_layer_size,output_layer_size,X,y,lam):\n    Theta1, Theta2 = unpack_thetas(Theta_combi, input_layer_size, hidden_layer_size, output_layer_size)\n    Theta1_grad, Theta2_grad = feed_forward_back_prop(Theta1,Theta2,input_layer_size,hidden_layer_size,output_layer_size,X,y,lam)\n    return pack_thetas(Theta1_grad,Theta2_grad)\n    \n\ndef fit(X,y,maxiter = 750,method = \'TNC\',hidden_layer_size = 25,lam = 0):\n    input_layer_size = np.shape(X)[1]   #dependant on dataset\n    #hidden_layer_size = 50   # user input\n    output_layer_size = len(set(y)) \n    \n    Theta1 = rand_init(hidden_layer_size, input_layer_size + 1,0.12)\n    Theta2 = rand_init(output_layer_size, hidden_layer_size + 1,0.12)\n    print ""dim of theta1: "",Theta1.shape\n    print ""dim of theta2: "",Theta2.shape\n    Theta_combi = pack_thetas(Theta1,Theta2)\n    #Optimization algorithms which returs the optimized values of theta given the gradients and cost function result\n    options = {\'maxiter\': maxiter}\n    _res = optimize.minimize(returnsJ, Theta_combi, jac=returnThetaGrad, method=method,args=(input_layer_size, hidden_layer_size, output_layer_size, X, y,lam), options=options)\n    t1, t2 = unpack_thetas(_res.x, input_layer_size, hidden_layer_size, output_layer_size)\n    return t1,t2\n    \ndef predict(t1,t2,X):\n    #here, we just perform a feed-forward step\n    h1 = sigmoid(np.dot(np.insert(X,0,1,axis=1),t1.T))\n    h2 = sigmoid(np.dot(np.insert(h1,0,1,axis=1),t2.T))\n    return h2\n\n\n\n""""""\ntesting using IRIS data set\n""""""\n\nimport sklearn.datasets as datasets\nfrom sklearn import cross_validation\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.3)\n\n\n#The optimized theta values after training using the training data\nt1,t2 = fit(X_train,y_train)\n\n\n#Predicting using the optimized theta values\npred = predict(t1,t2,X_test)\n\n#predict function returns the matrix of probability of each class for each test data point\n\npred = pred.argmax(1)\n\n#pred matrix is convertex to a vector where each element represents the index of maximum probability in the row\n#i.e it now holds the vector of classes where each element represents the classification result of a test data point\n\n\nfrom sklearn.metrics import accuracy_score\n\nprint ""Prediction accuracy: "",accuracy_score(y_test,pred)\n'"
SVM.py,27,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sat Jan 02 12:40:14 2016\n\n@author: Pavitrakumar\n""""""\n\nfrom __future__ import division\nimport numpy as np\nfrom sklearn import datasets\nimport cvxopt\nimport cvxopt.solvers\nfrom sklearn.cross_validation import train_test_split\n\n#generating linearly seprable data set.\n#Target values = -1 or 1.\n\nmean1 = np.array([0, 2])\nmean2 = np.array([2, 0])\ncov = np.array([[0.8, 0.6], [0.6, 0.8]])\nX1 = np.random.multivariate_normal(mean1, cov, 100)\ny1 = np.ones(len(X1))\nX2 = np.random.multivariate_normal(mean2, cov, 100)\ny2 = np.ones(len(X2)) * -1\nX = np.vstack((X1, X2))\ny = np.hstack((y1, y2))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n\n#X is 150x 4 matrix (2 featues, 150 data points)\n#y is 150x 1 matrix \n\nn_samples = X_train.shape[0]\nn_features = X_train.shape[1]\n\ndef kernel(x,y):\n    return np.dot(x,y)\n\nK = np.zeros((n_samples, n_samples))\nfor i in range(n_samples):\n    for j in range(n_samples):\n        K[i,j] = kernel(X_train[i], X_train[j])\n        \n#K is (150,150) matrix\n        \n#look at lecture notes, each term of P matrix is like y1y1(X1^T*X1)...\n#we have already calculated the (Xi^T*Xj) part in matix K.\n#now we need to calculate the yiyj part - since yi is just scalar, we just compute the outer product.\n\nY = np.outer(y_train,y_train)\n\n#now to combine K and Y into P\n\nP = cvxopt.matrix(Y * K)\n\n\n#the quadratic equation we are trying to minimize is :\n# 1/2 x^T*P*x + Q^T*X\n#we have computed P already, now for Q:\n\n#according to the final minimization equation equation, the co-efficient for sigma xn is (-1)\n#so, we pass a col-matrix of -1s as Q\n\nQ = cvxopt.matrix(-1 * np.ones(n_samples))\n\n\n#now, we have the solver which looks like this:\n#cvxopt.solvers.qp(P, Q, G, h, A, b) \n\n#We also need to pass in the constraints h,A,b,G\n\n\n\nA = cvxopt.matrix(y_train, (1,n_samples),tc=\'d\') #reshaping (1x150 matrix)\nb = cvxopt.matrix(0.0)\n\n#The above A and b correspnd to Ax=b general form and y^T*x=0 as a constraint in our equation.\n\n#for general Qp using cvxopt refer: #https://courses.csail.mit.edu/6.867/wiki/images/a/a7/Qp-cvxopt.pdf\n\n#G and h general form: Gx <= h and 0<=xn<=C as a constraint in our equation if we need margin. For non-margins, we have 0 <= xn.\n\n#for non-margin support vectors, 0 <= xn but, it is not in the form Gx <= h. To convert it to that form, we have to convert it as:\n# (-1)* xn <= 0\n# so, G will be a matrix of diaognals -1 and h will be a 150x1 matrix of 0s\n# RHS = 0 so h is a 1d matrix of 0 as coefficient is easy to understand.\n# Why does the G matrix have a diaognal of -1s?\n# Since our goal is to find the ""support vectors"" and these support vectors are a subset of points from the exisitng data point.\n# (support vectors define the margin of the hyperplane we are trying to find out)\n# So, each of the data point is to be considered as a unknown variable and the resultant value decides whether it is a support vector or not.\n#If we have 5 data points, equations are: 1*x1+0*x2+0*x3+0*x4+0*x5 = 0 so here h = [0] and G = [1,0,0,0,0] .... 0*x1+0*x2+0*x3+0*x4+1*x5 = 0 so here h = [0] and G = [0,0,0,0,1]\nG = cvxopt.matrix(np.diag(np.ones(n_samples) * -1)) # 150x150\nh = cvxopt.matrix(np.zeros(n_samples)) # 150x1\n   \n#for margin support vectors, we have a range in which xn lies i.e 0<=xn<=C\n#similar to previous where we had only one equality constraint to take care of, here we have 2.\n#that is: 0<=xn<=C can be split in 2. (we need to split because constraints to QP algo has to be in the form Gx<=h)\n# 0 < xn ==> (-1)*xn < = 0   (is in the form Gx<=h) .....(1)\n# (1)*xn <= C ==> (1)*xn <= C   (is in the form Gx<=h, no need to change signs) ...........(2)\n   \n   \n\n#So, the matrix for G is the previous matrix stacked(vertically) with another diaognal matrix of 1s\n\ntmp1 = np.diag(np.ones(n_samples) * -1) # = G if non-margin SVs (only using (1))\ntmp2 = np.identity(n_samples) # using (2) (1 x xn each row)\nG = cvxopt.matrix(np.vstack((tmp1, tmp2))) # vertically stack the 2 matrices above\n#G is now a 300x150 matrix\n\n#Similarly, for h here xn <= C so h matrix also has to take care of that constraint \n#It is a stacked(horizontally) with a (1x150 stacked horizontally with 1x150) 1x150 matrix of C value.\n\nC = 2\ntmp1 = np.zeros(n_samples) # = h if non-margin SVs (only using (1))\ntmp2 = np.ones(n_samples) * C # using (2)\nh = cvxopt.matrix(np.hstack((tmp1, tmp2))) # 300x1 matrix\n#h is 1x300 matrix [0,0,0,....0,0,C,C,....,C,C,C]\n\n\n\n\n#Now we have all the required variables needed to pass to the QP solver and get the xns (the support vectors)\n\nsolution = cvxopt.solvers.qp(P, Q, G, h, A, b)\n\n\na = np.ravel(solution[\'x\'])\n#in the above vector, the points which have non-zero(or a very low cut off value - we have chosen 1e-5) values are the support vectors.\n\nsv = a > 1e-5 # [true if condition satisfies, else false]\nind = np.arange(len(a))[sv] #getting the indices of the support vectors in the training set\n#ex: ind = [1,4,12,59,30] <- 1st,4th,12th etc.. are all support vectors.\na = a[sv] # all support vectors.\n\nsv_x = X[sv]\nsv_y = y[sv]\n\n#now we need to compute intercept (b)\n#b = ym - sigma xnynK(xn,xm) wehre xn>cutoff and K(xn,xm) is kernalized input\n#basically, it is b = acual - predicted (error)\n#since we are only using linear kernel, it is Xi^T.Xj (K matrix)\n\nb = 0\n\nfor i in range(len(a)):\n    ym = sv_y[i]\n    xm = sv_x[i]\n    b+=ym\n    b-=np.sum(a*sv_y*K[sv,ind[i]])\nb/= len(a)      \n\n#computing weight vector:\nw = np.zeros(n_features)\n#w = sigma xn * Xn * yn where [Xn,yn] are the data points of the support vectors and xn is the alpha of the support vector.\n#since we are using linear kernel, we need weights.\nfor i in range(len(a)):\n    w += a[i]*sv_x[i]*sv_y[i]\n\n\n\n\n\ny_predict = np.zeros(len(X_test))\nfor i in range(len(X_test)):\n    #similar to the formula for b just re-arrange it and calculate kernel for test set this time.\n    s = 0\n    for a_val,xm,ym in zip(a,sv_x,sv_y):\n        print a_val \n        print xm\n        print kernel(X_test[i],xm)\n        s += a_val * ym * kernel(X_test[i],xm)\n    y_predict[i] = s\n\ny_predict = np.sign(y_predict + b)\n\nprint ""accuracy is "",(sum(y_predict==y_test)/len(y_predict))\n\n\n#right now, it only works on binary classification tasks but one-vs-all technique can be used to implement a multi-class classification algo.\n\n'"
