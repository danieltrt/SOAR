file_path,api_count,code
setup.py,0,"b'#! /usr/bin/env python\n\nimport os\nimport setuptools  # noqa; we are using a setuptools namespace\nfrom numpy.distutils.core import setup\n\ndescr = """"""Optimization with autodiff""""""\n\nversion = None\nwith open(os.path.join(\'autoptim\', \'__init__.py\'), \'r\') as fid:\n    for line in (line.strip() for line in fid):\n        if line.startswith(\'__version__\'):\n            version = line.split(\'=\')[1].strip().strip(\'\\\'\')\n            break\nif version is None:\n    raise RuntimeError(\'Could not determine version\')\n\n\nDISTNAME = \'autoptim\'\nDESCRIPTION = descr\nMAINTAINER = \'Pierre Ablin\'\nMAINTAINER_EMAIL = \'pierre.ablin@inria.fr\'\nLICENSE = \'MIT\'\nDOWNLOAD_URL = \'https://github.com/pierreablin/autoptim.git\'\nVERSION = version\nURL = \'\'\n\n\ndef package_tree(pkgroot):\n    """"""Get the submodule list.""""""\n    # Adapted from VisPy\n    path = os.path.dirname(__file__)\n    subdirs = [os.path.relpath(i[0], path).replace(os.path.sep, \'.\')\n               for i in os.walk(os.path.join(path, pkgroot))\n               if \'__init__.py\' in i[2]]\n    return sorted(subdirs)\n\n\nif __name__ == ""__main__"":\n    setup(name=DISTNAME,\n          maintainer=MAINTAINER,\n          maintainer_email=MAINTAINER_EMAIL,\n          description=DESCRIPTION,\n          license=LICENSE,\n          version=VERSION,\n          url=URL,\n          download_url=DOWNLOAD_URL,\n          long_description=open(""README.md"").read(),\n          install_requires=[\n          ],\n          classifiers=[\n              \'Intended Audience :: Science/Research\',\n              \'Intended Audience :: Developers\',\n              \'License :: OSI Approved :: MIT License\',\n              \'Programming Language :: Python\',\n              \'Topic :: Software Development\',\n              \'Topic :: Scientific/Engineering\',\n              \'Operating System :: Microsoft :: Windows\',\n              \'Operating System :: POSIX\',\n              \'Operating System :: Unix\',\n              \'Operating System :: MacOS\',\n          ],\n          platforms=\'any\',\n          packages=package_tree(\'autoptim\'),\n          )\n'"
autoptim/__init__.py,0,"b""# Author: Pierre Ablin <pierreablin@gmail.com>\n# License: MIT\n\n\n__version__ = '0.4dev'\nfrom .autoptim import minimize  # noqa\n"""
autoptim/autoptim.py,5,"b'# Author: Pierre Ablin <pierreablin@gmail.com>\n# License: MIT\n\nimport numpy as np_\n\nimport autograd.numpy as np\n\nfrom autograd import grad\n\nfrom scipy.optimize import minimize as minimize_\n\n\ndef _scipy_func(objective_function, gradient, x, shapes, args=()):\n    optim_vars = _split(x, shapes)\n    obj = objective_function(optim_vars, *args)\n    gradients = gradient(optim_vars, *args)\n    g_vectorized, _ = _vectorize(gradients)\n    return obj, g_vectorized\n\n\ndef _convert_to_tuple(optim_vars):\n    if type(optim_vars) not in (list, tuple):\n        return (optim_vars,)\n    return optim_vars\n\n\ndef minimize(objective_function, optim_vars, args=(), precon_fwd=None,\n             precon_bwd=None, **kwargs):\n    """"""A wrapper to call scipy.optimize.minimize while computing the gradients\n       using autograd\'s auto-differentiation.\n        Parameters\n        ----------\n        objective_function : callable\n            The objective function to be minimized.\n                ``fun(optim_vars, *args) -> float``\n            or\n                ``fun(*optim_vars, *args) -> float``\n            where optim_vars is either a numpy array or a list of numpy\n            arrays and `args` is a tuple of fixed parameters needed to\n            completely specify the function.\n        optim_vars : ndarray or list of ndarrays\n            Initial guess.\n        args : tuple, optional\n            Extra arguments passed to the objective function.\n        precon_fwd : callable, optional\n            The forward preconditioning.\n                ``fun(optim_vars, *args) -> precon_optim_vars``\n            or\n                ``fun(*optim_vars, *args) -> precon_optim_vars``\n            where optim_vars is either a numpy array or a list of numpy\n            arrays and `args` is a tuple of fixed parameters needed to\n            completely specify the function.\n            The optimized function will be the composition:\n            `objective_function(precon_fwd(optim_vars))`.\n        precon_bwd : callable, optional\n            The backward preconditioning.\n                ``fun(precon_optim_vars, *args) -> optim_vars``\n            or\n                ``fun(*precon_optim_vars, *args) -> optim_vars``\n            where optim_vars is either a numpy array or a list of numpy\n            arrays and `args` is a tuple of fixed parameters needed to\n            completely specify the function.\n            This should be the reciprocal function of precon_fwd.\n        kwargs : dict, optional\n            Extra arguments passed to scipy.optimize.minimize. See\n            https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n            for the full list of available keywords.\n        Returns\n        -------\n        output : ndarray or list of ndarrays\n            The solution, of same shape as the input `optim_vars`.\n        res : OptimizeResult\n            The optimization result represented as a ``OptimizeResult`` object.\n            Important attributes are: ``x`` the solution array, ``success`` a\n            Boolean flag indicating if the optimizer exited successfully and\n            ``message`` which describes the cause of the termination. See\n            `OptimizeResult` for a description of other attributes.\n        """"""\n    # Check if there is preconditioning:\n    precondition = precon_fwd is not None\n    if precondition != (precon_bwd is not None):\n        error_string = {True: \'precon_fwd\', False: \'precon_bwd\'}[precondition]\n        raise ValueError(\'You should specify both precon_fwd and precon_bwd,\'\n                         \' you only specified %s\' % error_string)\n    if precondition:  # Run `minimize` in the preconditioned space:\n\n        optim_vars = _convert_to_tuple(optim_vars)\n        precon_optim_vars = precon_fwd(*optim_vars, *args)\n        n_args = len(args)\n\n        def precon_objective(*precon_optim_vars_and_args):\n            args = precon_optim_vars_and_args[-n_args:]\n            optim_vars = precon_bwd(*precon_optim_vars_and_args)\n            optim_vars = _convert_to_tuple(optim_vars)\n            return objective_function(*optim_vars, *args)\n\n        precon_result, res = minimize(precon_objective, precon_optim_vars,\n                                      args=args, precon_fwd=None,\n                                      precon_bwd=None, **kwargs)\n        precon_result = _convert_to_tuple(precon_result)\n        return precon_bwd(*precon_result, *args), res\n    # Check if there are bounds:\n    bounds = kwargs.get(\'bounds\')\n    bounds_in_kwargs = bounds is not None\n\n    # Convert input to a list if it is a single array\n    if type(optim_vars) is np.ndarray:\n        input_is_array = True\n        optim_vars = (optim_vars,)\n        if bounds_in_kwargs:\n            bounds = (bounds,)\n    else:\n        input_is_array = False\n\n    # Convert loss to readable autograd format\n\n    def objective_converted(optim_vars, *args):\n        return objective_function(*optim_vars, *args)\n\n    # Compute the gradient\n    gradient = grad(objective_converted)\n    # Vectorize optimization variables\n    x0, shapes = _vectorize(optim_vars)\n\n    # Convert bounds to the correct format\n    if bounds_in_kwargs:\n        bounds = _convert_bounds(bounds, shapes)\n        kwargs[\'bounds\'] = bounds\n\n    # Define the scipy optimized function and run scipy.minimize\n    def func(x):\n        return _scipy_func(objective_converted, gradient, x, shapes, args)\n    res = minimize_(func, x0, jac=True, **kwargs)\n\n    # Convert output to the input format\n    output = _split(res[\'x\'], shapes)\n    if input_is_array:\n        output = output[0]\n    return output, res\n\n\ndef _convert_bounds(bounds, shapes):\n    output_bounds = []\n    for shape, bound in zip(shapes, bounds):\n        # Check is the bound is already parsable by scipy.optimize\n        b = bound[0]\n        if isinstance(b, (list, tuple, np.ndarray)):\n            output_bounds += bound\n        else:\n            output_bounds += [bound, ] * np.prod(shape)\n    return output_bounds\n\n\ndef _vectorize(optim_vars):\n    shapes = [var.shape for var in optim_vars]\n    x = np.concatenate([var.ravel() for var in optim_vars])\n    return x, shapes\n\n\ndef _split(x, shapes):\n    x_split = np.split(x, np.cumsum([np.prod(shape) for shape in shapes[:-1]]))\n    optim_vars = [var.reshape(*shape) for (var, shape) in zip(x_split, shapes)]\n    return optim_vars\n'"
tutorials/gaussian_mixture.py,8,"b'# Author: Pierre Ablin <pierreablin@gmail.com>\n# License: MIT\n\n# Example with several variables\n\nimport autograd.numpy as np\n\nfrom autoptim import minimize\n\nn = 1000\nn_components = 3\n\nx = np.concatenate((np.random.randn(n) - 1,\n                    3 * np.random.randn(n),\n                    np.random.randn(n) + 2))\n\n\n# Here, the model should fit both the means and the variances. Using\n# scipy.optimize.minimize, one would have to vectorize by hand these variables.\n\ndef loss(means, variances, x):\n    tmp = np.zeros(n_components * n)\n    for m, v in zip(means, variances):\n        tmp += np.exp(-(x - m) ** 2 / (2 * v ** 2)) / v\n    return -np.sum(np.log(tmp))\n\n\n# autoptim can handle lists of unknown variables\n\nmeans0 = np.random.randn(n_components)\nvariances0 = np.random.rand(n_components)\noptim_vars = [means0, variances0]\n# The variances should be constrained to positivity. To do so, we can pass\n# a `bounds` list to `minimize`. Bounds are automatically broadcasted to\n# match the input size.\n\nbounds = [(None, None),  # corresponds to means: no constraint\n          (0, None)]  # corresponds to variances: positivity constraint.\n(means, variances), _ = minimize(loss, optim_vars, args=(x,),\n                                 bounds=bounds)\n\nprint(means, variances)  # Notice that they have the correct shape.\n'"
tutorials/preconditioning.py,7,"b""# Author: Pierre Ablin <pierreablin@gmail.com>\n# License: MIT\n\n# An example about preconditioning: the problem is to minimize\n# || y - X.dot(beta)|| ** 2 + lambda * || beta\xc2\xa0|| ** 2.\n# The Hessian of the problem is 2 * (X^TX + lambda * Id).\n# For approximately decorellated X, it is well approximated by its diagonal,\n# whose square root gives a natural preconditioner. It is simple to\n# implement using autoptim.\nfrom time import time\n\nimport autograd.numpy as np\nfrom autoptim import minimize\n\n\nn = int(1e6)\np = 20\n\n# Define the parameters : the rows of X have different powers.\nX = np.random.randn(n, p) * np.arange(1, p+1)\ny = np.random.randn(n)\nlbda = 0.1\n\n# Define the loss :\n\n\ndef loss(beta, X, y, lbda):\n    return np.sum((np.dot(X, beta) - y) ** 2) + lbda * np.sum(beta ** 2)\n\n# Define the preconditioner. Note that it is much faster to compute than the\n# whole Hessian.\n\n\nhessian_approx = 2 * lbda * np.ones(p)\nfor i in range(p):\n    hessian_approx[i] += 2 * np.dot(X[:, i], X[:, i])\n\ndiag_precon = np.sqrt(hessian_approx)\n\n# Define the forward and backward preconditioning functions. They should have\n# the same signature as the objective function. Here, we dot not use the extra\n# arguments.\n\n\ndef precon_fwd(beta, X, y, lbda):\n    return beta * diag_precon\n\n\ndef precon_bwd(beta_precon, X, y, lbda):\n    return beta_precon / diag_precon\n\n\nbeta0 = np.random.randn(p)\n# Run the minimization with the preconditioning\nt0 = time()\nbeta_min, _ = minimize(loss, beta0, args=(X, y, lbda), precon_fwd=precon_fwd,\n                       precon_bwd=precon_bwd)\nprint('Minimization with preconditioning took %.2f sec.' % (time() - t0))\nprint(beta_min)\n\n# It gives the same output without preconditioning:\nt0 = time()\nbeta_min, _ = minimize(loss, beta0, args=(X, y, lbda))\nprint('Minimization without preconditioning took %.2f sec.' % (time() - t0))\nprint(beta_min)\n\n# But it is faster with preconditioning (about twice in this example, but it\n# can give more impressing speedups)!\n"""
tutorials/ridge_regression.py,4,"b'# Author: Pierre Ablin <pierreablin@gmail.com>\n# License: MIT\n\n# An example with additional variables\n\n\nimport autograd.numpy as np\nfrom autoptim import minimize\n\n\nn = 10\np = 5\n\nX = np.random.randn(n, p)\ny = np.random.randn(n)\nlbda = 0.1\n\n# The loss shoulb be optimized over beta, with the other parameters fixed.\n\n\ndef loss(beta, X, y, lbda):\n    return np.sum((np.dot(X, beta) - y) ** 2) + lbda * np.sum(beta ** 2)\n\n\nbeta0 = np.random.randn(p)\n\nbeta_min, _ = minimize(loss, beta0, args=(X, y, lbda))\nprint(beta_min)\n'"
tutorials/rosenbrock.py,1,"b'# Author: Pierre Ablin <pierreablin@gmail.com>\n# License: MIT\n\n# This is the simplest example of autoptim use.\n\n\nimport autograd.numpy as np\nfrom autoptim import minimize\n\n\n# Specify the loss function :\ndef rosenbrock(x):\n    return (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2\n\n# Choose the starting point:\n\n\nx0 = np.zeros(2)\n\nx_min, _ = minimize(rosenbrock, x0)\nprint(x_min)\n'"
tutorials/whitening.py,4,"b'# Author: Pierre Ablin <pierreablin@gmail.com>\n# License: MIT\n\n\n# Example of multi-dimensional arrays\nimport autograd.numpy as np\nfrom autoptim import minimize\n\n\nn = 100\np = 2\n\nX = np.random.randn(n, p)\n\n# The loss is minimized when X.dot(W) is decorrelated.\n\n\ndef loss(W, X):\n    Y = np.dot(X, W)\n    return -np.linalg.slogdet(W)[1] + 0.5 * np.sum(Y ** 2) / n\n\n\n# The input is a square matrix\nW0 = np.eye(p)\n\nW, _ = minimize(loss, W0, args=(X, ))\nprint(W)\nY = X.dot(W)\nprint(Y.T.dot(Y) / n)  # Equal to identity\n'"
autoptim/tests/test_minimize.py,5,"b""# Author: Pierre Ablin <pierreablin@gmail.com>\n# License: MIT\n\nimport autograd.numpy as np\nfrom numpy.testing import assert_allclose\n\nfrom autoptim import minimize\n\n\ndef test_rosenbrock():\n    def rosenbrock(x):\n        return (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2\n\n    x0 = np.zeros(2)\n\n    x_min, res = minimize(rosenbrock, x0)\n    assert res['success']\n\n\ndef test_multiple_shapes():\n    def f(x, y, z, a):\n        return np.sum(x ** 2) + np.sum((y - 3) ** 2) + np.sum((z + a) ** 4)\n\n    a = 2\n    shapes = [(2, 3), (2, 2), (3,)]\n    optim_vars_init = [np.ones(shape) for shape in shapes]\n    optim_vars, res = minimize(f, optim_vars_init, args=(a,))\n    assert res['success']\n    assert [var.shape for var in optim_vars] == shapes\n    for var, target in zip(optim_vars, [0, 3, -a]):\n        assert_allclose(var, target, atol=1e-1)\n\n\ndef test_preconditioning():\n    def f(x, y, z, a, b):\n        return np.sum(x ** 2) + np.sum((y - 3) ** 2) + np.sum((z + a) ** 4)\n\n    a = 2\n    b = 5\n    shapes = [(2, 3), (2, 2), (3,)]\n    optim_vars_init = [np.ones(shape) for shape in shapes]\n\n    def precon_fwd(x, y, z, a, b):\n        return 3 * x, y / 2, z * 4\n\n    def precon_bwd(x, y, z, a, b):\n        return x / 3, 2 * y, z / 4\n\n    optim_vars, res = minimize(f, optim_vars_init, args=(a, b),\n                               precon_fwd=precon_fwd, precon_bwd=precon_bwd)\n    assert res['success']\n    assert [var.shape for var in optim_vars] == shapes\n    for var, target in zip(optim_vars, [0, 3, -a]):\n        assert_allclose(var, target, atol=1e-1)\n"""
