file_path,api_count,code
run_benchmark.py,4,"b'import os\nimport time\nimport PIL\nimport PIL.Image\nimport zipfile\nimport numpy as np\nimport dareblopy as db\nfrom test_utils import benchmark\n\n\n# unzip zipfile with images\nwith zipfile.ZipFile(""test_utils/test_image_archive.zip"", \'r\') as zip_ref:\n    zip_ref.extractall(""test_utils/test_images"")\n\n\ndef run_reading_to_bytes_benchmark():\n    bm = benchmark.Benchmark()\n\n    ##################################################################\n    # Reading a file to bytes object\n    ##################################################################\n    def read_to_bytes_native():\n        for i in range(2000):\n            f = open(\'test_utils/test_images/%d.jpg\' % (i % 200), \'rb\')\n            b = f.read()\n\n    def read_to_bytes_db():\n        for i in range(2000):\n            b = db.open_as_bytes(\'test_utils/test_images/%d.jpg\' % (i % 200))\n\n    bm.add(\'reading files to `bytes` from filesystem\',\n           baseline=read_to_bytes_native,\n           dareblopy=read_to_bytes_db)\n\n    ##################################################################\n    # Reading files to bytes object from zip archive\n    ##################################################################\n    def read_jpg_bytes_from_zip_native():\n        archive = zipfile.ZipFile(""test_utils/test_image_archive.zip"", \'r\')\n\n        for i in range(2000):\n            s = archive.open(\'%d.jpg\' % (i % 200))\n            b = s.read()\n            # picture_stream = io.BytesIO(b)\n            # picture = PIL.Image.open(picture_stream)\n            # picture.show()\n\n    def read_jpg_bytes_from_zip_db():\n        archive = db.open_zip_archive(""test_utils/test_image_archive.zip"")\n\n        for i in range(2000):\n            b = archive.open_as_bytes(\'%d.jpg\' % (i % 200))\n            # picture_stream = io.BytesIO(b)\n            # picture = PIL.Image.open(picture_stream)\n            # picture.show()\n\n    bm.add(\'reading files to `bytes` from a zip archive\',\n           baseline=read_jpg_bytes_from_zip_native,\n           dareblopy=read_jpg_bytes_from_zip_db,\n           preheat=lambda: (db.open_zip_archive(""test_utils/test_image_archive.zip""),\n                            zipfile.ZipFile(""test_utils/test_image_archive.zip"", \'r\')))\n    # Run everything and save plot\n    bm.run(title=\'Running time of reading files to `bytes`\\nfor DareBlopy and equivalent python code\',\n           label_baseline=\'Python Standard Library + zipfile\',\n           output_file=\'test_utils/benchmark_reading_files.png\', loc=\'ul\', figsize=(8, 6),\n           caption=""Reading 200 jpeg files, each file ~30kb. Files are read to \'bytes object (no decoding). ""\n                   ""Reading is performed from filesystem and from a zip archive with no compression (storage type). ""\n                   ""All files are read 10 times and then measured time is averaged over 10 trials."")\n\n\ndef run_reading_jpeg_to_numpy_benchmark():\n    bm = benchmark.Benchmark()\n\n    ##################################################################\n    # Reading a jpeg image to numpy array\n    ##################################################################\n    def read_jpg_to_numpy_pil():\n        for i in range(2000):\n            image = PIL.Image.open(\'test_utils/test_images/%d.jpg\' % (i % 200))\n            ndarray = np.array(image)\n\n    def read_jpg_to_numpy_db():\n        for i in range(2000):\n            ndarray = db.read_jpg_as_numpy(\'test_utils/test_images/%d.jpg\' % (i % 200))\n\n    def read_jpg_to_numpy_db_turbo():\n        for i in range(2000):\n            ndarray = db.read_jpg_as_numpy(\'test_utils/test_images/%d.jpg\' % (i % 200), True)\n\n    bm.add(\'reading jpeg image to numpy\',\n           baseline=read_jpg_to_numpy_pil,\n           dareblopy=read_jpg_to_numpy_db,\n           dareblopy_turbo=read_jpg_to_numpy_db_turbo)\n\n    ##################################################################\n    # Reading jpeg images to numpy array from zip archive\n    ##################################################################\n    def read_jpg_to_numpy_from_zip_native():\n        archive = zipfile.ZipFile(""test_utils/test_image_archive.zip"", \'r\')\n\n        for i in range(2000):\n            s = archive.open(\'%d.jpg\' % (i % 200))\n            image = PIL.Image.open(s)\n            ndarray = np.array(image)\n\n    def read_jpg_to_numpy_from_zip_db():\n        archive = db.open_zip_archive(""test_utils/test_image_archive.zip"")\n        for i in range(2000):\n            ndarray = archive.read_jpg_as_numpy(\'%d.jpg\' % (i % 200))\n\n    def read_jpg_to_numpy_from_zip_db_turbo():\n        archive = db.open_zip_archive(""test_utils/test_image_archive.zip"")\n        for i in range(2000):\n            ndarray = archive.read_jpg_as_numpy(\'%d.jpg\' % (i % 200), True)\n\n    bm.add(\'reading jpeg to numpy from zip\',\n           baseline=read_jpg_to_numpy_from_zip_native,\n           dareblopy=read_jpg_to_numpy_from_zip_db,\n           dareblopy_turbo=read_jpg_to_numpy_from_zip_db_turbo,\n           preheat=lambda: (db.open_zip_archive(""test_utils/test_image_archive.zip""),\n                            zipfile.ZipFile(""test_utils/test_image_archive.zip"", \'r\')))\n\n    # Run everything and save plot\n    bm.run(title=\'Running time of reading jpeg files to numpy `ndarray`\\nfor DareBlopy and equivalent python code\',\n           label_baseline=\'Python Standard Library + zipfile\\n + PIL + numpy\',\n           output_file=\'test_utils/benchmark_reading_jpeg.png\', loc=\'lr\', figsize=(8, 6),\n           caption=""Reading 200 jpeg files, each file is ~30kb and  has 256x256 resolution. ""\n                   ""Files are read to numpy `ndarray` (jpeg\'s are decoded). ""\n                   ""Reading is performed from filesystem and from a zip archive with no compression (storage type). ""\n                   ""All files are read 10 times and then measured time is averaged over 10 trials."")\n\n\ndef run_reading_jpeg_to_numpy_benchmark_nat_storage():\n    bm = benchmark.Benchmark()\n\n    ##################################################################\n    # Reading a jpeg image to numpy array\n    ##################################################################\n    def read_jpg_to_numpy_pil():\n        for i in range(200):\n            image = PIL.Image.open(\'/data/for_benchmark/test_utils/test_images/%d.jpg\' % (i % 200))\n            ndarray = np.array(image)\n\n    def read_jpg_to_numpy_db():\n        for i in range(200):\n            ndarray = db.read_jpg_as_numpy(\'/data/for_benchmark/test_utils/test_images/%d.jpg\' % (i % 200))\n\n    def read_jpg_to_numpy_db_turbo():\n        for i in range(200):\n            ndarray = db.read_jpg_as_numpy(\'/data/for_benchmark/test_utils/test_images/%d.jpg\' % (i % 200), True)\n\n    bm.add(\'reading jpeg image to numpy\',\n           baseline=read_jpg_to_numpy_pil,\n           dareblopy=read_jpg_to_numpy_db,\n           dareblopy_turbo=read_jpg_to_numpy_db_turbo)\n\n    ##################################################################\n    # Reading jpeg images to numpy array from zip archive\n    ##################################################################\n    def read_jpg_to_numpy_from_zip_native():\n        archive = zipfile.ZipFile(""/data/for_benchmark/test_utils/test_image_archive.zip"", \'r\')\n\n        for i in range(200):\n            s = archive.open(\'%d.jpg\' % (i % 200))\n            image = PIL.Image.open(s)\n            ndarray = np.array(image)\n\n    def read_jpg_to_numpy_from_zip_db():\n        archive = db.open_zip_archive(""/data/for_benchmark/test_utils/test_image_archive.zip"")\n        for i in range(200):\n            ndarray = archive.read_jpg_as_numpy(\'%d.jpg\' % (i % 200))\n\n    def read_jpg_to_numpy_from_zip_db_turbo():\n        archive = db.open_zip_archive(""/data/for_benchmark/test_utils/test_image_archive.zip"")\n        for i in range(200):\n            ndarray = archive.read_jpg_as_numpy(\'%d.jpg\' % (i % 200), True)\n\n    bm.add(\'reading jpeg to numpy from zip\',\n           baseline=read_jpg_to_numpy_from_zip_native,\n           dareblopy=read_jpg_to_numpy_from_zip_db,\n           dareblopy_turbo=read_jpg_to_numpy_from_zip_db_turbo,\n           preheat=lambda: (db.open_zip_archive(""test_utils/test_image_archive.zip""),\n                            zipfile.ZipFile(""test_utils/test_image_archive.zip"", \'r\')))\n\n    # Run everything and save plot\n    bm.run(title=\'Running time of reading jpeg files to numpy `ndarray`for DareBlopy\\n and equivalent python code. \'\n                 \' Reading from NAT storage\',\n           label_baseline=\'Python Standard Library + zipfile\\n + PIL + numpy\',\n           output_file=\'test_utils/benchmark_reading_jpeg_nat.png\', loc=\'ur\', figsize=(8, 6),\n           caption=""Reading 200 jpeg files, each file is ~30kb and  has 256x256 resolution. ""\n                   ""Files are read to numpy `ndarray` (jpeg\'s are decoded). ""\n                   ""Reading is performed from filesystem and from a zip archive with no compression (storage type). ""\n                   ""In both cases, data is on a NAT storage. ""\n                   ""All files are read once and then measured time is averaged over 10 trials."")\n\n\ndef run_reading_tfrecords_ablation_benchmark():\n    ##################################################################\n    # Benchmarking different record reading strategies\n    ##################################################################\n    filenames = [\'test_utils/test-large-r00.tfrecords\',\n                 \'test_utils/test-large-r01.tfrecords\',\n                 \'test_utils/test-large-r02.tfrecords\',\n                 \'test_utils/test-large-r03.tfrecords\',\n                 \'test_utils/test-large-r00.tfrecords\',\n                 \'test_utils/test-large-r01.tfrecords\',\n                 \'test_utils/test-large-r02.tfrecords\',\n                 \'test_utils/test-large-r03.tfrecords\']\n\n    if not all(os.path.exists(x) for x in filenames):\n        raise RuntimeError(\'Could not find tfrecords. You need to run test_utils/make_tfrecords.py\')\n\n    results = []\n\n    @benchmark.timeit\n    def simple_reading_of_records():\n        records = []\n        for filename in filenames:\n            rr = db.RecordReader(filename)\n            records += list(rr)\n\n    results.append((simple_reading_of_records(), ""Reading records with\\nRecordReader\\nNo parsing""))\n\n    @benchmark.timeit\n    def test_yielder_basic():\n        record_yielder = db.RecordYielderBasic(filenames)\n        records = []\n        while True:\n            try:\n                records += record_yielder.next_n(32)\n            except StopIteration:\n                break\n\n    results.append((test_yielder_basic(), ""Reading records with\\nRecordYielderBasic\\nNo parsing""))\n\n    @benchmark.timeit\n    def test_yielder_randomized():\n        features = {\n            #\'shape\': db.FixedLenFeature([3], db.int64),\n            \'data\': db.FixedLenFeature([3, 256, 256], db.uint8)\n        }\n        parser = db.RecordParser(features, False)\n        record_yielder = db.ParsedRecordYielderRandomized(parser, filenames, 64, 1, 0)\n        records = []\n        while True:\n            try:\n                records += record_yielder.next_n(32)\n            except StopIteration:\n                break\n\n    results.append((test_yielder_randomized(), ""Reading records with\\nParsedRecordYielderRandomized\\nHas parsing""))\n\n    @benchmark.timeit\n    def test_yielder_randomized_parallel():\n        features = {\n            #\'shape\': db.FixedLenFeature([3], db.int64),\n            \'data\': db.FixedLenFeature([3, 256, 256], db.uint8)\n        }\n        parser = db.RecordParser(features, True)\n        record_yielder = db.ParsedRecordYielderRandomized(parser, filenames, 64, 1, 0)\n        records = []\n        while True:\n            try:\n                records += record_yielder.next_n(32)\n            except StopIteration:\n                break\n\n    results.append((test_yielder_randomized_parallel(), ""Reading records with\\nParsedRecordYielderRandomized\\n +parallel parsing\\nHas parsing""))\n\n    @benchmark.timeit\n    def test_ParsedTFRecordsDatasetIterator():\n        features = {\n            #\'shape\': db.FixedLenFeature([3], db.int64),\n            \'data\': db.FixedLenFeature([3, 256, 256], db.uint8)\n        }\n        iterator = db.ParsedTFRecordsDatasetIterator(filenames, features, 32, 64)\n        records = []\n        for batch in iterator:\n            records += batch\n\n    results.append((test_ParsedTFRecordsDatasetIterator(), ""Reading records with\\nParsedTFRecordsDatasetIterator\\n +parallel parsing\\nHas parsing""))\n\n    @benchmark.timeit\n    def test_ParsedTFRecordsDatasetIterator_and_dataloader():\n        features = {\n            #\'shape\': db.FixedLenFeature([3], db.int64),\n            \'data\': db.FixedLenFeature([3, 256, 256], db.uint8)\n        }\n        iterator = db.data_loader(db.ParsedTFRecordsDatasetIterator(filenames, features, 32, 64), worker_count=6)\n        records = []\n        for batch in iterator:\n            records += batch\n\n    results.append((test_ParsedTFRecordsDatasetIterator_and_dataloader(), ""Reading records with\\nParsedTFRecordsDatasetIterator\\n+multy worker dataloader\\nHas parsing""))\n\n    benchmark.do_simple_plot(results,\n                             figsize=(16, 6),\n                             title=\'Reading tfrecords\',\n                             output_file=\'test_utils/benchmark_reading_tfrecords_ablation.png\')\n\n\ndef run_reading_tfrecords_comparison_to_tensorflow_benchmark():\n    ##################################################################\n    # Benchmarking reading tfrecords\n    ##################################################################\n    import tensorflow as tf\n    time.sleep(1.0)\n\n    filenames = [\'test_utils/test-large-r00.tfrecords\',\n                 \'test_utils/test-large-r01.tfrecords\',\n                 \'test_utils/test-large-r02.tfrecords\',\n                 \'test_utils/test-large-r03.tfrecords\']\n\n    if not all(os.path.exists(x) for x in filenames):\n        raise RuntimeError(\'TFRecords were not found. Please run make_tfrecords.py\')\n\n    bm = benchmark.Benchmark()\n\n    batch_size = 32\n\n    ##################################################################\n    # Reading a file to bytes object\n    ##################################################################\n    def reading_tf_records_from_dareblopy_withoutdecoding():\n        features = {\n            \'data\': db.FixedLenFeature([], db.string)\n        }\n        iterator = db.data_loader(db.ParsedTFRecordsDatasetIterator(filenames, features, batch_size, 128), worker_count=6)\n        records = []\n        for batch in iterator:\n            records += batch\n\n    def reading_tf_records_from_tensorflow_withoutdecoding():\n        raw_dataset = tf.data.TFRecordDataset(filenames)\n\n        feature_description = {\n            \'data\': tf.io.FixedLenFeature([], tf.string)\n        }\n\n        records = []\n        for batch in raw_dataset.batch(batch_size, drop_remainder=True):\n            s = tf.io.parse_example(batch, feature_description)[\'data\']\n            records.append(s)\n\n    bm.add(\'reading tfrecords\\nwithout decoding tf.string to numpy\',\n           baseline=reading_tf_records_from_tensorflow_withoutdecoding,\n           dareblopy=reading_tf_records_from_dareblopy_withoutdecoding)\n\n    ##################################################################\n    # Reading files to bytes object from zip archive\n    ##################################################################\n    def reading_tf_records_from_dareblopy():\n        features = {\n            \'data\': db.FixedLenFeature([3, 256, 256], db.uint8)\n        }\n        iterator = db.data_loader(db.ParsedTFRecordsDatasetIterator(filenames, features, batch_size, 64), worker_count=6)\n        records = []\n        for batch in iterator:\n            records += batch\n\n    def reading_tf_records_from_tensorflow():\n        raw_dataset = tf.data.TFRecordDataset(filenames)\n\n        feature_description = {\n            \'data\': tf.io.FixedLenFeature([], tf.string)\n        }\n\n        records = []\n        for batch in raw_dataset.batch(batch_size, drop_remainder=True):\n            s = tf.io.parse_example(batch, feature_description)[\'data\']\n            data = tf.reshape(tf.io.decode_raw(s, tf.uint8), [-1, 3, 256, 256])\n            records.append(data)\n\n    bm.add(\'reading tfrecords\\nwith decoding tf.string to numpy\',\n           baseline=reading_tf_records_from_tensorflow,\n           dareblopy=reading_tf_records_from_dareblopy)\n    # Run everything and save plot\n    bm.run(title=\'Running time of reading tfrecords\\nfor DareBlopy and TensorFlow\',\n           label_baseline=\'TensorFlow\',\n           output_file=\'test_utils/benchmark_reading_tfrecords_comparion_to_tf.png\', loc=\'lr\', figsize=(8, 6),\n           caption=""Reading 200 raw uint8 images from four tfrecords, each of which is 59MB. Formatting of tfrecords is""\n                   "" similar to one used for training StyleGAN by NVidia. ""\n                   ""Reading is done two times, without decoding and with decoding tf.string to uint8 ndarray.""\n                   ""Time is averaged over 10 trials."")\n\n\nrun_reading_to_bytes_benchmark()\ntime.sleep(1.0)\nrun_reading_jpeg_to_numpy_benchmark()\ntime.sleep(1.0)\n# run_reading_jpeg_to_numpy_benchmark_nat_storage()\n# time.sleep(1.0)\nrun_reading_tfrecords_ablation_benchmark()\ntime.sleep(1.0)\nrun_reading_tfrecords_comparison_to_tensorflow_benchmark()\n'"
setup.py,0,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\nfrom setuptools import setup, Extension\nfrom distutils.errors import *\nfrom distutils.dep_util import newer_group\nfrom distutils import log\nfrom distutils.command.build_ext import build_ext\n\nfrom codecs import open\nimport os\nimport sys\nimport platform\nimport re\nimport glob\n\ntarget_os = \'none\'\n\nif sys.platform == \'darwin\':\n    target_os = \'darwin\'\nelif os.name == \'posix\':\n    target_os = \'posix\'\nelif platform.system() == \'Windows\':\n    target_os = \'win32\'\n\nhere = os.path.abspath(os.path.dirname(__file__))\n\nwith open(os.path.join(here, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\n\ndef filter_sources(sources):\n    """"""Filters sources into c, cpp, objc and asm""""""\n    cpp_ext_match = re.compile(r\'.*[.](cpp|cxx|cc)\\Z\', re.I).match\n    c_ext_match = re.compile(r\'.*[.](c|C)\\Z\', re.I).match\n    objc_ext_match = re.compile(r\'.*[.]m\\Z\', re.I).match\n    asm_ext_match = re.compile(r\'.*[.](asm|s|S)\\Z\', re.I).match\n\n    c_sources = []\n    cpp_sources = []\n    objc_sources = []\n    asm_sources = []\n    other_sources = []\n    for source in sources:\n        if c_ext_match(source):\n            c_sources.append(source)\n        elif cpp_ext_match(source):\n            cpp_sources.append(source)\n        elif objc_ext_match(source):\n            objc_sources.append(source)\n        elif asm_ext_match(source):\n            asm_sources.append(source)\n        else:\n            other_sources.append(source)\n    return c_sources, cpp_sources, objc_sources, asm_sources, other_sources\n\n\ndef build_extension(self, ext):\n    """"""Modified version of build_extension method from distutils.\n       Can handle compiler args for different files""""""\n\n    sources = ext.sources\n    if sources is None or not isinstance(sources, (list, tuple)):\n        raise DistutilsSetupError(\n              ""in \'ext_modules\' option (extension \'%s\'), ""\n              ""\'sources\' must be present and must be ""\n              ""a list of source filenames"" % ext.name)\n\n    sources = list(sources)\n    ext_path = self.get_ext_fullpath(ext.name)\n    depends = sources + ext.depends\n    if not (self.force or newer_group(depends, ext_path, \'newer\')):\n        log.debug(""skipping \'%s\' extension (up-to-date)"", ext.name)\n        return\n    else:\n        log.info(""building \'%s\' extension"", ext.name)\n\n    sources = self.swig_sources(sources, ext)\n\n    extra_args = ext.extra_compile_args or []\n    extra_c_args = getattr(ext, ""extra_compile_c_args"", [])\n    extra_cpp_args = getattr(ext, ""extra_compile_cpp_args"", [])\n    extra_objc_args = getattr(ext, ""extra_compile_objc_args"", [])\n    extra_asm_args = getattr(ext, ""extra_compile_asm_args"", [])\n    file_specific_definitions = getattr(ext, ""file_specific_definitions"", {})\n    asm_include = getattr(ext, ""asm_include"", [])\n\n    macros = ext.define_macros[:]\n    for undef in ext.undef_macros:\n        macros.append((undef,))\n\n    c_sources, cpp_sources, objc_sources, asm_sources, other_sources = filter_sources(sources)\n\n    self.compiler.src_extensions += [\'.asm\']\n\n    self.compiler.set_executable(\'assembler\', [\'nasm\'])\n\n    def _compile(src, args):\n        obj = []\n        for s in src:\n            additional_macros = []\n            if s in file_specific_definitions.keys():\n                additional_macros += file_specific_definitions[s]\n            obj += self.compiler.compile([s],\n                                         output_dir=self.build_temp,\n                                         macros=macros + additional_macros,\n                                         include_dirs=ext.include_dirs,\n                                         debug=self.debug,\n                                         extra_postargs=extra_args + args,\n                                         depends=ext.depends)\n        return obj\n\n    def _compile_asm(src):\n        obj = []\n        for s in src:\n            additional_macros = []\n            if s in file_specific_definitions.keys():\n                additional_macros += file_specific_definitions[s]\n            macros_, objects, extra_postargs, asm_args, build = \\\n                self.compiler._setup_compile(self.build_temp, macros + additional_macros, asm_include, [s],\n                                             depends, extra_asm_args)\n\n            for o in objects:\n                try:\n                    src, ext = build[o]\n                except KeyError:\n                    continue\n                try:\n                    self.spawn(self.compiler.assembler + extra_postargs + asm_args + [\'-o\', o, src])\n                except DistutilsExecError as msg:\n                    raise CompileError(msg)\n            obj += objects\n\n        return obj\n\n    objects = []\n    objects += _compile_asm(asm_sources)\n    objects += _compile(c_sources, extra_c_args)\n    objects += _compile(cpp_sources, extra_cpp_args)\n    objects += _compile(objc_sources, extra_objc_args)\n    objects += _compile(other_sources, [])\n\n    self._built_objects = objects[:]\n    if ext.extra_objects:\n        objects.extend(ext.extra_objects)\n\n    extra_args = ext.extra_link_args or []\n\n    language = ext.language or self.compiler.detect_language(sources)\n    self.compiler.link_shared_object(\n        objects, ext_path,\n        libraries=self.get_libraries(ext),\n        library_dirs=ext.library_dirs,\n        runtime_library_dirs=ext.runtime_library_dirs,\n        extra_postargs=extra_args,\n        export_symbols=self.get_export_symbols(ext),\n        debug=self.debug,\n        build_temp=self.build_temp,\n        target_lang=language)\n\n\n# patching\nbuild_ext.build_extension = build_extension\n\n\nfsal = list(glob.glob(\'libs/fsal/sources/*.cpp\'))\nzlib = list(glob.glob(\'libs/zlib/*.c\'))\nlz4 = list(glob.glob(\'libs/lz4/lib/*.c\'))\ndareblopy = list(glob.glob(\'sources/*.c*\')) + list(glob.glob(\'sources/protobuf/*.c*\'))\n\ncrc32c = """"""crc32c.cc crc32c_arm64.cc crc32c_portable.cc crc32c_sse42.cc""""""\n\ncrc32c = [\'libs/crc32c/src/\' + x for x in crc32c.split()]\n\nprotobuf = """"""any_lite.cc arena.cc extension_set.cc generated_enum_util.cc\n        generated_message_table_driven_lite.cc generated_message_util.cc implicit_weak_message.cc\n        io/coded_stream.cc io/io_win32.cc io/strtod.cc io/zero_copy_stream.cc io/zero_copy_stream_impl.cc\n        io/zero_copy_stream_impl_lite.cc message_lite.cc parse_context.cc repeated_field.cc stubs/bytestream.cc\n        stubs/common.cc stubs/int128.cc stubs/status.cc stubs/statusor.cc stubs/stringpiece.cc stubs/stringprintf.cc\n        stubs/structurally_valid.cc stubs/strutil.cc stubs/time.cc wire_format_lite.cc\n        any.cc  any.pb.cc  api.pb.cc  compiler/importer.cc  compiler/parser.cc  descriptor.cc  descriptor.pb.cc\n        descriptor_database.cc  duration.pb.cc  dynamic_message.cc  empty.pb.cc  extension_set_heavy.cc\n        field_mask.pb.cc  generated_message_reflection.cc  generated_message_table_driven.cc  io/gzip_stream.cc\n        io/printer.cc  io/tokenizer.cc  map_field.cc  message.cc  reflection_ops.cc  service.cc  source_context.pb.cc\n        struct.pb.cc  stubs/mathlimits.cc  stubs/substitute.cc  text_format.cc  timestamp.pb.cc  type.pb.cc\n        unknown_field_set.cc  util/delimited_message_util.cc  util/field_comparator.cc  util/field_mask_util.cc\n        util/internal/datapiece.cc  util/internal/default_value_objectwriter.cc  util/internal/error_listener.cc\n        util/internal/field_mask_utility.cc  util/internal/json_escaping.cc  util/internal/json_objectwriter.cc\n        util/internal/json_stream_parser.cc  util/internal/object_writer.cc  util/internal/proto_writer.cc\n        util/internal/protostream_objectsource.cc  util/internal/protostream_objectwriter.cc\n        util/internal/type_info.cc  util/internal/type_info_test_helper.cc  util/internal/utility.cc\n        util/json_util.cc  util/message_differencer.cc  util/time_util.cc  util/type_resolver_util.cc\n        wire_format.cc  wrappers.pb.cc""""""\n\nprotobuf = [\'libs/protobuf/src/google/protobuf/\' + x for x in protobuf.split()]\n\n\njpeg_turbo = """"""jcapimin.c jcapistd.c jccoefct.c jccolor.c jcdctmgr.c jchuff.c\n        jcicc.c jcinit.c jcmainct.c jcmarker.c jcmaster.c jcomapi.c jcparam.c\n        jcphuff.c jcprepct.c jcsample.c jctrans.c jdapimin.c jdapistd.c jdatadst.c\n        jdatasrc.c jdcoefct.c jdcolor.c jddctmgr.c jdhuff.c jdicc.c jdinput.c\n        jdmainct.c jdmarker.c jdmaster.c jdmerge.c jdphuff.c jdpostct.c jdsample.c\n        jdtrans.c jerror.c jfdctflt.c jfdctfst.c jfdctint.c jidctflt.c jidctfst.c\n        jidctint.c jidctred.c jquant1.c jquant2.c jutils.c jmemmgr.c jmemnobs.c\n        jaricom.c jcarith.c jdarith.c""""""\n\njpeg_turbo = [\'libs/libjpeg-turbo/\' + x for x in jpeg_turbo.split()]\n\np64 = sys.maxsize > 2**32\n\njpeg_turbo_simd_64 = """"""x86_64/jsimdcpu.asm x86_64/jfdctflt-sse.asm\n        x86_64/jccolor-sse2.asm x86_64/jcgray-sse2.asm x86_64/jchuff-sse2.asm\n        x86_64/jcphuff-sse2.asm x86_64/jcsample-sse2.asm x86_64/jdcolor-sse2.asm\n        x86_64/jdmerge-sse2.asm x86_64/jdsample-sse2.asm x86_64/jfdctfst-sse2.asm\n        x86_64/jfdctint-sse2.asm x86_64/jidctflt-sse2.asm x86_64/jidctfst-sse2.asm\n        x86_64/jidctint-sse2.asm x86_64/jidctred-sse2.asm x86_64/jquantf-sse2.asm\n        x86_64/jquanti-sse2.asm\n        x86_64/jccolor-avx2.asm x86_64/jcgray-avx2.asm x86_64/jcsample-avx2.asm\n        x86_64/jdcolor-avx2.asm x86_64/jdmerge-avx2.asm x86_64/jdsample-avx2.asm\n        x86_64/jfdctint-avx2.asm x86_64/jidctint-avx2.asm x86_64/jquanti-avx2.asm x86_64/jsimd.c""""""\n\njpeg_turbo_simd_86 = """"""i386/jccolor-avx2.asm  i386/jccolor-mmx.asm  i386/jccolor-sse2.asm  \n        i386/jcgray-avx2.asm  i386/jcgray-mmx.asm  i386/jcgray-sse2.asm  i386/jchuff-sse2.asm  \n        i386/jcphuff-sse2.asm  i386/jcsample-avx2.asm  i386/jcsample-mmx.asm  i386/jcsample-sse2.asm  \n        i386/jdcolor-avx2.asm  i386/jdcolor-mmx.asm  i386/jdcolor-sse2.asm  i386/jdmerge-avx2.asm  \n        i386/jdmerge-mmx.asm  i386/jdmerge-sse2.asm  i386/jdsample-avx2.asm  i386/jdsample-mmx.asm  \n        i386/jdsample-sse2.asm  i386/jfdctflt-3dn.asm  i386/jfdctflt-sse.asm  i386/jfdctfst-mmx.asm  \n        i386/jfdctfst-sse2.asm  i386/jfdctint-avx2.asm  i386/jfdctint-mmx.asm  i386/jfdctint-sse2.asm  \n        i386/jidctflt-3dn.asm  i386/jidctflt-sse.asm  i386/jidctflt-sse2.asm  i386/jidctfst-mmx.asm  \n        i386/jidctfst-sse2.asm  i386/jidctint-avx2.asm  i386/jidctint-mmx.asm  i386/jidctint-sse2.asm  \n        i386/jidctred-mmx.asm  i386/jidctred-sse2.asm  i386/jquant-3dn.asm  i386/jquant-mmx.asm  \n        i386/jquant-sse.asm  i386/jquantf-sse2.asm  i386/jquanti-avx2.asm  i386/jquanti-sse2.asm  \n        i386/jsimd.c  i386/jsimdcpu.asm""""""\n\n\njpeg_turbo_simd = [\'libs/libjpeg-turbo/simd/\' + x for x in (jpeg_turbo_simd_64 if p64 else jpeg_turbo_simd_86).split()]\n\njpeg_vanila = """"""jmemnobs.c jaricom.c jcapimin.c jcapistd.c jcarith.c jccoefct.c jccolor.c\n        jcdctmgr.c jchuff.c jcinit.c jcmainct.c jcmarker.c jcmaster.c jcomapi.c jcparam.c\n        jcprepct.c jcsample.c jctrans.c jdapimin.c jdapistd.c jdarith.c jdatadst.c jdatasrc.c\n        jdcoefct.c jdcolor.c jddctmgr.c jdhuff.c jdinput.c jdmainct.c jdmarker.c jdmaster.c\n        jdmerge.c jdpostct.c jdsample.c jdtrans.c jerror.c jfdctflt.c jfdctfst.c jfdctint.c\n        jidctflt.c jidctfst.c jidctint.c jquant1.c jquant2.c jutils.c jmemmgr.c""""""\n\njpeg_vanila = [\'libs/libjpeg/\' + x for x in jpeg_vanila.split()]\n\n\ndefinitions = {\n    \'darwin\': [(\'HAVE_SSE42\', 0), (\'HAVE_PTHREAD\', 0)],\n    \'posix\': [(\'HAVE_SSE42\', 0), (\'HAVE_PTHREAD\', 0)],\n    \'win32\': [(\'HAVE_SSE42\', 0)],\n}\n\nfile_specific_definitions = {}\nfor file in jpeg_turbo:\n    file_specific_definitions[file] = [(\'TURBO\', 0)]\nfor file in jpeg_turbo_simd:\n    file_specific_definitions[file] = [(\'TURBO\', 0)]\nfor file in jpeg_vanila:\n    file_specific_definitions[file] = [(\'VANILA\', 0)]\n\nlibs = {\n    \'darwin\': [],\n    \'posix\': [""rt"", ""m"", ""stdc++fs"", ""gomp""],\n    \'win32\': [""ole32"", ""shell32""],\n}\n\nextra_link = {\n    \'darwin\': [],\n    \'posix\': [\'-static-libstdc++\', \'-static-libgcc\', \'-flto\'],\n    \'win32\': [],\n}\n\nextra_compile_args = {\n    \'darwin\': [\'-fPIC\', \'-msse2\', \'-msse3\', \'-msse4\', \'-funsafe-math-optimizations\'],\n    \'posix\': [\'-fPIC\', \'-msse2\', \'-msse3\', \'-msse4\', \'-funsafe-math-optimizations\'],\n    \'win32\': [\'/MT\', \'/fp:fast\', \'/GL\', \'/GR-\'],\n}\n\nextra_compile_cpp_args = {\n    \'darwin\': [\'-std=c++14\', \'-lstdc++fs\', \'-Ofast\', \'-flto\', \'-fopenmp\'],\n    \'posix\': [\'-std=c++14\', \'-lstdc++fs\', \'-Ofast\', \'-flto\', \'-fopenmp\'],\n    \'win32\': [],\n}\n\nextra_compile_c_args = {\n    \'darwin\': [\'-std=c99\', \'-Ofast\', \'-flto\'],\n    \'posix\': [\'-std=c99\', \'-Ofast\', \'-flto\'],\n    \'win32\': [],\n}\n\nextra_compile_asm_args = {\n    \'darwin\': [\'-DMACHO\', \'-D__x86_64__\' if p64 else \'\', \'-DPIC\', \'-DTURBO\', \'-f macho\', \'-Ox\'],\n    \'posix\': [\'-DELF\', \'-D__x86_64__\'  if p64 else \'\', \'-DPIC\', \'-DTURBO\', \'-f elf64\' if p64 else \'-f elf\', \'-Ox\'],\n    \'win32\': [\'-DWIN64\' if p64 else \'-DWIN32\', \'-D__x86_64__\'  if p64 else \'\', \'-DPIC\', \'-DTURBO\', \'-f win64\' if p64 else \'-f win32\', \'-Ox\'],\n}\n\nextension = Extension(""_dareblopy"",\n                      jpeg_turbo + jpeg_vanila + jpeg_turbo_simd + dareblopy + fsal + crc32c + zlib + protobuf + lz4,\n                             define_macros = definitions[target_os],\n                             include_dirs=[\n                                 ""libs/zlib"",\n                                 ""libs/fsal/sources"",\n                                 ""libs/lz4/lib"",\n                                 ""libs/pybind11/include"",\n                                 ""libs/crc32c/include"",\n                                 ""libs/protobuf/src"",\n                                 ""sources"",\n                                 ""configs""\n                             ],\n                             extra_compile_args=extra_compile_args[target_os],\n                             extra_link_args=extra_link[target_os],\n                             libraries = libs[target_os])\n\nextension.extra_compile_cpp_args = extra_compile_cpp_args[target_os]\nextension.extra_compile_c_args = extra_compile_c_args[target_os]\nextension.file_specific_definitions = file_specific_definitions\nextension.extra_compile_asm_args = extra_compile_asm_args[target_os]\nextension.asm = \'nasm\'\nextension.asm_include = [\'libs/libjpeg-turbo/simd/nasm/\', \'libs/libjpeg-turbo/simd/x86_64/\' if p64 else \'libs/libjpeg-turbo/simd/i386/\']\n\nsetup(\n    name=\'dareblopy\',\n\n    version=\'0.0.3\',\n\n    description=\'dareblopy\',\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n\n    url=\'https://github.com/podgorskiy/dareblopy\',\n\n    author=\'Stanislav Pidhorskyi\',\n    author_email=\'stpidhorskyi@mix.wvu.edu\',\n\n    license=\'Apache 2.0 License\',\n\n    classifiers=[\n        \'Development Status :: 3 - Alpha\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Programming Language :: Python :: 3.8\',\n    ],\n\n    keywords=\'dareblopy\',\n\n    packages=[\'dareblopy\'],\n\n    ext_modules=[extension],\n\n    install_requires=[\'numpy\']\n)\n'"
test_dareblopy.py,20,"b'import unittest\nimport PIL\nimport PIL.Image\nimport zipfile\nimport numpy as np\nimport pickle\nimport dareblopy as db\n\n\nclass BasicFileOps(unittest.TestCase):\n    def test_file_exist(self):\n        fs = db.FileSystem()\n        self.assertTrue(fs.exists(\'README.md\'))\n\n    def test_rename(self):\n        fs = db.FileSystem()\n        self.assertFalse(fs.exists(\'README2.md\'))\n        fs.rename(\'README.md\', \'README2.md\')\n        self.assertTrue(fs.exists(\'README2.md\'))\n        fs.rename(\'README2.md\', \'README.md\')\n\n    def test_seek_tell(self):\n        fs = db.FileSystem()\n        file = fs.open(""test_utils/test_archive.zip"")\n        self.assertTrue(file)\n\n        self.assertEqual(file.tell(), 0)\n        file.seek(0, 2)\n        size = file.tell()\n        self.assertEqual(file.size(), size)\n\n        file.seek(-1, 2)\n        self.assertEqual(file.tell(), size - 1)\n        file.seek(-100, 1)\n        self.assertEqual(file.tell(), size - 101)\n        file.seek(2, 1)\n        self.assertEqual(file.tell(), size - 101 + 2)\n        file.seek(0)\n        self.assertEqual(file.tell(), 0)\n\n    def test_zip_mounting(self):\n        fs = db.FileSystem()\n        zip = fs.open(""test_utils/test_archive.zip"", lockable=True)\n        self.assertTrue(zip)\n\n        fs.mount_archive(db.open_zip_archive(zip))\n\n        s = fs.open(\'test.txt\')\n\n        self.assertEqual(s.read().decode(\'utf-8\'), ""asdasdasd"")\n\n\nclass FileAndImageReadingOps(unittest.TestCase):\n    def test_reading_to_bytes(self):\n        f = open(""test_utils/test_image.jpg"", \'rb\')\n        b1 = f.read()\n        f.close()\n\n        b2 = db.open_as_bytes(""test_utils/test_image.jpg"")\n        self.assertEqual(b1, b2)\n\n    def test_reading_to_numpy(self):\n        image = PIL.Image.open(""test_utils/test_image.jpg"")\n        ndarray1 = np.array(image)\n\n        ndarray2 = db.read_jpg_as_numpy(""test_utils/test_image.jpg"")\n\n        ndarray3 = db.read_jpg_as_numpy(""test_utils/test_image.jpg"", True)\n\n        PIL.Image.fromarray(ndarray3).save(""test_image2.png"")\n\n        self.assertTrue(np.all(ndarray1 == ndarray2))\n\n        mean_error = np.abs(ndarray1.astype(int) - ndarray3.astype(int)).mean()\n\n        PIL.Image.fromarray(np.abs(ndarray1.astype(int) - ndarray3.astype(int)).astype(np.uint8)).save(""test_image_diff.png"")\n\n        self.assertTrue(mean_error < 0.5)\n\n    def test_reading_to_bytes_from_zip(self):\n        archive = zipfile.ZipFile(""test_utils/test_image_archive.zip"", \'r\')\n        s = archive.open(\'0.jpg\')\n        b1 = s.read()\n\n        archive = db.open_zip_archive(""test_utils/test_image_archive.zip"")\n        b2 = archive.open_as_bytes(\'0.jpg\')\n\n        b3 = archive.open(\'0.jpg\').read()\n\n        self.assertEqual(b1, b2)\n        self.assertEqual(b1, b3)\n\n    def test_reading_to_numpy_from_zip(self):\n        archive = zipfile.ZipFile(""test_utils/test_image_archive.zip"", \'r\')\n        s = archive.open(\'0.jpg\')\n        image = PIL.Image.open(s)\n        ndarray1 = np.array(image)\n\n        archive = db.open_zip_archive(""test_utils/test_image_archive.zip"")\n        ndarray2 = archive.read_jpg_as_numpy(\'0.jpg\')\n\n        self.assertTrue(np.all(ndarray1 == ndarray2))\n\n\nclass TFRecordsReading(unittest.TestCase):\n    def test_reading_record(self):\n        rr = db.RecordReader(\'test_utils/test-small-r00.tfrecords\')\n        self.assertIsNotNone(rr)\n\n        file_size, data_size, entries = rr.get_metadata()\n        self.assertEqual(entries, 50)\n\n        records = list(rr)\n\n        self.assertEqual(len(records), 50)\n\n        # reading ground truth records to confirm reading container was correct\n        with open(\'test_utils/test-small-records-r00.pth\', \'rb\') as f:\n            records_gt = pickle.load(f)\n\n        self.assertEqual(records_gt, records)\n\n    def test_record_yielder(self):\n        record_yielder = db.RecordYielderBasic([\'test_utils/test-small-r00.tfrecords\',\n                                                \'test_utils/test-small-r01.tfrecords\',\n                                                \'test_utils/test-small-r02.tfrecords\',\n                                                \'test_utils/test-small-r03.tfrecords\'])\n\n        self.assertIsNotNone(record_yielder)\n        records = []\n\n        while True:\n            try:\n                batch = record_yielder.next_n(32)\n                records += batch\n            except StopIteration:\n                break\n\n        # reading ground truth records to confirm reading the container was correct\n        records_gt = []\n        for file in [\'test_utils/test-small-records-r00.pth\',\n                     \'test_utils/test-small-records-r01.pth\',\n                     \'test_utils/test-small-records-r02.pth\',\n                     \'test_utils/test-small-records-r03.pth\']:\n            with open(file, \'rb\') as f:\n                records_gt += pickle.load(f)\n\n        self.assertEqual(records_gt, records)\n\n    def test_record_yielder_randomized(self):\n        record_yielder = db.RecordYielderRandomized([\'test_utils/test-small-r00.tfrecords\',\n                                                     \'test_utils/test-small-r01.tfrecords\',\n                                                     \'test_utils/test-small-r02.tfrecords\',\n                                                     \'test_utils/test-small-r03.tfrecords\'],\n                                                    buffer_size=16,\n                                                    seed=0,\n                                                    epoch=0)\n\n        self.assertIsNotNone(record_yielder)\n        records = []\n\n        while True:\n            try:\n                batch = record_yielder.next_n(32)\n                records += batch\n            except StopIteration:\n                break\n\n        # reading ground truth records to confirm reading the container was correct\n        records_gt = []\n        for file in [\'test_utils/test-small-records-r00.pth\',\n                     \'test_utils/test-small-records-r01.pth\',\n                     \'test_utils/test-small-records-r02.pth\',\n                     \'test_utils/test-small-records-r03.pth\']:\n            with open(file, \'rb\') as f:\n                records_gt += pickle.load(f)\n\n        self.assertNotEqual(records_gt, records)\n\n        # Check that all records present, and they are unique.\n        for record in records:\n            self.assertIn(record, records_gt)\n        for record_gt in records_gt:\n            self.assertIn(record_gt, records)\n\n        index = []\n\n        for record in records:\n            index.append(records_gt.index(record))\n\n        # TODO: Check if sequence is random? For small `buffer_size` it\'s going to be random only at local scale.\n        print(index)\n\n\nclass TFRecordsParsing(unittest.TestCase):\n    def setUp(self):\n        # reading records\n        self.records = []\n        for file in [\'test_utils/test-small-records-r00.pth\',\n                     \'test_utils/test-small-records-r01.pth\',\n                     \'test_utils/test-small-records-r02.pth\',\n                     \'test_utils/test-small-records-r03.pth\']:\n            with open(file, \'rb\') as f:\n                self.records += pickle.load(f)\n\n        # reading ground-truth data\n        self.images_gt = []\n        for file in [\'test_utils/test-small-images-r00.pth\',\n                     \'test_utils/test-small-images-r01.pth\',\n                     \'test_utils/test-small-images-r02.pth\',\n                     \'test_utils/test-small-images-r03.pth\']:\n            with open(file, \'rb\') as f:\n                self.images_gt += pickle.load(f)\n\n    def test_parsing_single_record(self):\n        features = {\n            \'shape\': db.FixedLenFeature([3], db.int64),\n            \'data\': db.FixedLenFeature([], db.string)\n        }\n\n        parser = db.RecordParser(features)\n        self.assertIsNotNone(parser)\n\n        for record, image_gt in zip(self.records, self.images_gt):\n            shape, data = parser.parse_single_example(record)\n\n            self.assertTrue(np.all(shape == [3, 32, 32]))\n            self.assertTrue(np.all(\n                np.frombuffer(data[0], dtype=np.uint8).reshape(3, 32, 32) == image_gt\n            ))\n\n    def test_parsing_single_record_with_uint8_alternative_to_string(self):\n        features_alternative = {\n            \'shape\': db.FixedLenFeature([3], db.int64),\n            \'data\': db.FixedLenFeature([3, 32, 32], db.uint8)\n        }\n\n        parser = db.RecordParser(features_alternative)\n        self.assertIsNotNone(parser)\n\n        for record, image_gt in zip(self.records, self.images_gt):\n            shape, data = parser.parse_single_example(record)\n\n            self.assertTrue(np.all(shape == [3, 32, 32]))\n            self.assertTrue(np.all(data == image_gt))\n\n    def test_parsing_records_in_batch(self):\n        features_alternative = {\n            \'data\': db.FixedLenFeature([3, 32, 32], db.uint8)\n        }\n\n        parser = db.RecordParser(features_alternative, False)\n        self.assertIsNotNone(parser)\n\n        data = parser.parse_example(self.records)[0]\n        self.assertTrue(np.all(data == self.images_gt))\n\n    def test_parsing_single_record_inplace(self):\n        features = {\n            \'shape\': db.FixedLenFeature([3], db.int64),\n            \'data\': db.FixedLenFeature([], db.string)}\n\n        parser = db.RecordParser(features)\n        self.assertIsNotNone(parser)\n\n        shape = np.zeros(3, dtype=np.int64)\n        data = np.asarray([bytes()], dtype=object)\n\n        for record, image_gt in zip(self.records, self.images_gt):\n            parser.parse_single_example_inplace(record, [shape, data], 0)\n            image = np.frombuffer(data[0], dtype=np.uint8).reshape(shape)\n            self.assertTrue(np.all(image == image_gt))\n\n    def test_parsing_single_record_inplace_with_uint8(self):\n        features_alternative = {\n            \'data\': db.FixedLenFeature([3, 32, 32], db.uint8)\n        }\n\n        parser = db.RecordParser(features_alternative, False)\n        self.assertIsNotNone(parser)\n\n        data = np.zeros([3, 32, 32], dtype=np.uint8)\n\n        for record, image_gt in zip(self.records, self.images_gt):\n            parser.parse_single_example_inplace(record, [data], 0)\n            self.assertTrue(np.all(data == image_gt))\n\n\nclass DatasetIterator(unittest.TestCase):\n    def setUp(self):\n        # reading ground-truth data\n        self.images_gt = []\n        for file in [\'test_utils/test-small-images-r00.pth\']:\n            with open(file, \'rb\') as f:\n                self.images_gt += pickle.load(f)\n\n    def test_dataset_iterator(self):\n        features = {\n            \'data\': db.FixedLenFeature([3, 32, 32], db.uint8)\n        }\n        iterator = db.ParsedTFRecordsDatasetIterator([\'test_utils/test-small-r00.tfrecords\'],\n                                                     features, 32, buffer_size=1)\n\n        images = np.concatenate([x[0] for x in iterator], axis=0)\n        self.assertTrue(np.all(images == self.images_gt))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
dareblopy/TFRecordsDatasetIterator.py,2,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\nimport dareblopy as db\nimport time\nimport numpy as np\n\n\nclass TFRecordsDatasetIterator:\n    def __init__(self, filenames, batch_size, buffer_size=1000, seed=None, epoch=0):\n        if seed is None:\n            seed = np.uint64(time.time() * 1000)\n        self.record_yielder = db.RecordYielderRandomized(filenames, buffer_size, seed, epoch)\n        self.batch_size = batch_size\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return self.record_yielder.next_n(self.batch_size)\n\n\nclass ParsedTFRecordsDatasetIterator:\n    def __init__(self, filenames, features, batch_size, buffer_size=1000, seed=None, epoch=0):\n        if seed is None:\n            seed = np.uint64(time.time() * 1000)\n        self.parser = db.RecordParser(features, True)\n        self.record_yielder = db.ParsedRecordYielderRandomized(self.parser, filenames, buffer_size, seed, epoch)\n        self.batch_size = batch_size\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return self.record_yielder.next_n(self.batch_size)\n'"
dareblopy/__init__.py,0,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport sys\nimport os\n\n\ndef _handle_debugging():\n    # if running debug session\n    if os.path.exists(""cmake-build-debug/""):\n        print(\'Running Debugging session!\')\n        # sys.path.insert(0, ""cmake-build-debug/"")\n        sys.path.insert(0, ""cmake-build-release/"")\n\n\n_handle_debugging()\n\n\nfrom _dareblopy import *\n\n# A hack to force sphinx to do the right thing\nif \'sphinx\' in sys.modules:\n    print(\'Sphinx detected!!!\')\n    del os\n    del sys\n    __all__ = dir()\nelse:\n    del os\n    del sys\n\nimport dareblopy.utils\nfrom dareblopy.data_loader import data_loader\nfrom dareblopy.TFRecordsDatasetIterator import TFRecordsDatasetIterator, ParsedTFRecordsDatasetIterator\n'"
dareblopy/data_loader.py,0,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\ntry:\n    from Queue import Queue, Empty\nexcept ImportError:\n    from queue import Queue, Empty\nfrom threading import Thread, Lock, Event\n\n\ndef data_loader(yielder, collator=None, iteration_count=None, worker_count=1, queue_size=16):\n    """""" Return an iterator that retrieves objects from yielder and passes them through collator.\n\n    Maintains a queue of given size and can run several worker threads. Intended to be used for asynchronous, buffered\n    data loading. Uses threads instead of multiprocessing, so tensors can be uploaded to GPU in collator.\n\n    There are many purposes that function :attr:`collator` can be used for, depending on your use case.\n\n    - Reading data from disk or db\n    - Data decoding, e.g. from JPEG.\n    - Augmenting data, flipping, rotating adding nose, etc.\n    - Concatenation of data, stacking to single ndarray, conversion to a tensor, uploading to GPU.\n    - Data generation.\n\n    Note:\n        Sequential order of batches is guaranteed only if number of workers is 1 (Default), otherwise batches might\n        be supplied out of order.\n\n    Args:\n        yielder (iterator): Input data, returns batches.\n        collator (Callable, optional): Function for processing batches. Receives batch from yielder.\n        Can return object of any type. Defaults to None.\n        worker_count (int, optional): Number of workers, should be greater or equal to one. To process data in parallel\n            and fully load CPU :attr:`worker_count` should be close to the number of CPU cores. Defaults to one.\n        queue_size (int, optional): Maximum size of the queue, which is number of batches to buffer. Should be larger\n            than :attr:`worker_count`. Typically, one would want this to be as large as possible to amortize all disk\n            IO and computational costs. Downside of large value is increased RAM consumption. Defaults to 16.\n\n    Returns:\n        Iterator: An object that produces a sequence of batches. :meth:`next()` method of the iterator will return\n        object that was produced by :attr:`collator` function\n\n\n    Raises:\n        StopIteration: When all data was iterated through. Stops the for loop.\n\n    """"""\n\n    class State:\n        def __init__(self):\n            self.lock = Lock()\n            self.iteration_count = iteration_count\n            self.quit_event = Event()\n            self.queue = Queue(queue_size)\n            self.active_workers = 0\n            self.collator = collator\n\n    def _worker(state):\n        while not state.quit_event.is_set():\n            try:\n                b = next(yielder)\n                if state.collator:\n                    b = state.collator(b)\n                state.queue.put(b)\n            except StopIteration:\n                break\n\n        with state.lock:\n            state.active_workers -= 1\n            if state.active_workers == 0:\n                state.queue.put(None)\n\n    class Iterator:\n        def __init__(self):\n            self.state = State()\n\n            self.workers = []\n            self.state.active_workers = worker_count\n            for i in range(worker_count):\n                worker = Thread(target=_worker, args=(self.state, ))\n                worker.daemon = True\n                worker.start()\n                self.workers.append(worker)\n\n        def __len__(self):\n            return self.state.iteration_count\n\n        def __iter__(self):\n            return self\n\n        def __next__(self):\n            item = self.state.queue.get()\n            self.state.queue.task_done()\n            if item is None:\n                raise StopIteration\n            return item\n\n        def __del__(self):\n            self.state.quit_event.set()\n            while not self.state.queue.empty():\n                self.state.queue.get(False)\n                self.state.queue.task_done()\n            for worker in self.workers:\n                worker.join()\n\n    return Iterator()\n'"
dareblopy/utils.py,2,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\nimport math\nimport numpy as np\n\n\ndef make_grid(tensor, nrow=8, padding=2):\n    nmaps = tensor.shape[0]\n    xmaps = min(nrow, nmaps)\n    ymaps = int(math.ceil(float(nmaps) / xmaps))\n    height, width = int(tensor.shape[2] + padding), int(tensor.shape[3] + padding)\n\n    grid = np.zeros((3, height * ymaps + padding, width * xmaps + padding), dtype=np.uint8)\n    k = 0\n    for y in range(ymaps):\n        for x in range(xmaps):\n            if k >= nmaps:\n                break\n            grid[:, y * height + padding : y * height + padding + height - padding,\n                    x * width + padding : x * width + padding + width - padding] = tensor[k]\n            k = k + 1\n    return np.transpose(grid, (1, 2, 0))\n\n\ndef display_grid(tensor, nrow=8, padding=2):\n    import IPython.display as display\n    import PIL.Image\n    display.display(PIL.Image.fromarray(make_grid(tensor, nrow, padding)))\n'"
scripts/generate_defines.py,0,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport re\n\n\ndef parse_symbols(logfile_path):\n    regexp_iter = r\'(\\S*) (\\S) (\\S*)\'\n\n    re_iter = re.compile(regexp_iter)\n\n    symbols = []\n\n    with open(logfile_path, \'r\') as f:\n        for line_terminated in f:\n            line = line_terminated.rstrip(\'\\n\')\n            r = re_iter.findall(line)\n            if len(r) == 0:\n                continue\n            if r[0][1] != \'U\':\n                symbols.append(str(r[0][2]))\n\n    return symbols\n\n\ndef main():\n    # The files that are parsed here contain all symbols from libjpeg and libjpeg-turbo\n    vanila_symbols = set(parse_symbols(\'libjpeg-vanila-symbols.txt\'))\n    turbo_symbols = set(parse_symbols(\'libjpeg-turbo-symbols.txt\'))\n\n    collision_symbols = []\n\n    print(\'#ifdef TURBO\')\n\n    for s in turbo_symbols:\n        if s in vanila_symbols:\n            collision_symbols.append(s)\n\n    for s in collision_symbols:\n        print(""#define %s %s_turbo"" % (s, s))\n\n    print(\'#include ""jconfig-turbo.h""\\n#elif defined(VANILA)\')\n\n    for s in collision_symbols:\n        print(""#define %s %s_vanila"" % (s, s))\n\n    print(\'#include ""jconfig-vanila.h""\\n#else\\n#error Error! JPEG library not defined! Use TURBO or VANILA\\n#endif\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
test_utils/__init__.py,0,b''
test_utils/benchmark.py,8,"b'import time\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport gc\n\n\nclass Benchmark:\n    def __init__(self):\n        self.schedule = []\n\n    def add(self, name, baseline, dareblopy, dareblopy_turbo=None, preheat=()):\n        self.schedule.append((name, baseline, dareblopy, dareblopy_turbo, preheat))\n\n    def run(self, title, label_baseline, output_file, loc, figsize=(12, 6), caption=None):\n        locmap = {\'ul\': 2, \'ur\': 1, \'ll\': 3, \'lr\': 4}\n        if loc in locmap:\n            loc = locmap[loc]\n\n        results = []\n        for name, baseline, dareblopy, dareblopy_turbo, preheat in self.schedule:\n            print(\'\\n\' + \'#\' * 80)\n            print(\'Benchmarking: %s\' % name)\n            print(\'#\' * 80)\n            if preheat:\n                preheat()\n            print(\'With native python:\')\n            baseline = timeit(baseline)\n            gc.collect()\n            time.sleep(0.1)\n            average_time_for_baseline = baseline()\n\n            print(\'With DareBlopy: \')\n            dareblopy = timeit(dareblopy)\n            gc.collect()\n            time.sleep(0.1)\n            average_time_for_dareblopy = dareblopy()\n            average_time_for_dareblopy_turbo = average_time_for_dareblopy\n            if dareblopy_turbo:\n                print(\'With DareBlopy (turbo): \')\n                dareblopy_turbo = timeit(dareblopy_turbo)\n                gc.collect()\n                time.sleep(0.1)\n                average_time_for_dareblopy_turbo = dareblopy_turbo()\n\n            results.append((name,\n                            average_time_for_baseline,\n                            average_time_for_dareblopy,\n                            average_time_for_dareblopy_turbo))\n\n        x = np.arange(len(results))\n        width = 0.2\n\n        has_turbo = np.asarray([(x[2] != x[3]) for x in results])\n\n        fig, ax = plt.subplots(figsize=figsize, dpi=120, facecolor=\'w\', edgecolor=\'k\')\n\n        rects1 = ax.bar(x - width * 1.0 + 0.5 * width * np.logical_not(has_turbo), [x[1] for x in results], width, label=label_baseline)\n        rects2 = ax.bar(x - width * 0.0 + 0.5 * width * np.logical_not(has_turbo), [x[2] for x in results], width, label=\'DareBlopy\')\n        if np.any(has_turbo):\n            rects3 = ax.bar(x + width * has_turbo, [x[3] for x in results] * has_turbo, width, label=\'DareBlopy +libjpeg-turbo\')\n\n        # Add some text for labels, title and custom x-axis tick labels, etc.\n        ax.set_ylabel(\'Running time, [ms]. Lower is better\')\n        ax.set_title(title)\n        ax.set_xticks(x)\n        ax.set_xticklabels([x[0] for x in results])\n        ax.legend(loc=loc)\n\n        if caption:\n            fig.text(0.5, 0.03, caption, wrap=True, horizontalalignment=\'center\', fontsize=12)\n            plt.subplots_adjust(left=.1, right=0.9, top=0.92, bottom=0.22)\n\n        autolabel(ax, rects1)\n        autolabel(ax, rects2)\n        if np.any(has_turbo):\n            autolabel(ax, rects3, has_turbo)\n        fig.savefig(output_file)\n\n\ndef timeit(method):\n    def timed(*args, **kw):\n        ds = 0.0\n        trials = 10\n        # preheat\n        method(*args, **kw)\n\n        for i in range(trials):\n            ts = time.time()\n            method(*args, **kw)\n            te = time.time()\n            delta = te - ts\n            ds += delta\n            print(\'%r  %2.2f ms\' % (method.__name__, delta * 1000))\n        ds /= trials\n        print(method.__name__)\n        print(\'Total: %r  %2.2f ms\' % (method.__name__, ds * 1000))\n        return ds * 1000\n    return timed\n\n\ndef autolabel(ax, rects, show=None):\n    if show is None:\n        show = np.ones(len(rects))\n    for rect, s in zip(rects, show):\n        if s:\n            height = rect.get_height()\n            ax.annotate(\'{:.2f}\'.format(height),\n                        xy=(rect.get_x() + rect.get_width() / 2, height),\n                        xytext=(0, 3),\n                        textcoords=""offset points"",\n                        ha=\'center\', va=\'bottom\')\n\n\ndef do_simple_plot(results, figsize, title, output_file):\n    fig, ax = plt.subplots(figsize=figsize, dpi=120, facecolor=\'w\', edgecolor=\'k\')\n    x = np.arange(len(results))\n    width = 0.2\n    rects1 = ax.bar(x, [x[0] for x in results], width)\n    # Add some text for labels, title and custom x-axis tick labels, etc.\n    ax.set_ylabel(\'Running time, [ms]. Lower is better\')\n    ax.set_title(title)\n    ax.set_xticks(x)\n    ax.set_xticklabels([x[1] for x in results])\n\n    autolabel(ax, rects1)\n    fig.tight_layout()\n\n    fig.savefig(output_file)\n'"
test_utils/make_tfrecords.py,2,"b'import zipfile\nimport tqdm\nimport imageio\nimport numpy as np\nimport random\nimport pickle\nimport os\nimport tensorflow as tf\nimport PIL.Image\n\n\ndef prepare_tfrecords():\n    directory = os.path.dirname(os.path.abspath(__file__))\n\n    os.makedirs(directory, exist_ok=True)\n\n    archive = zipfile.ZipFile(os.path.join(directory, \'test_image_archive.zip\'), \'r\')\n\n    names = archive.namelist()\n\n    names = [x for x in names if x[-4:] == \'.jpg\']\n\n    count = len(names)\n    print(""Count: %d"" % count)\n\n    random.seed(1)\n    random.shuffle(names)\n\n    folds_count = 4\n    folds = [[] for _ in range(folds_count)]\n\n    count_per_fold = count // len(folds)\n    for i in range(len(folds)):\n            folds[i] += names[i * count_per_fold: (i + 1) * count_per_fold]\n\n    for i in range(len(folds)):\n        images_small = []\n        images_large = []\n        for x in tqdm.tqdm(folds[i]):\n            imgfile = archive.open(x)\n            image = imageio.imread(imgfile.read())\n            if image.ndim == 2:\n                image = np.stack([image] * 3, axis=-1)\n            image_small = np.asarray(PIL.Image.fromarray(image).resize((32, 32)))\n            images_small.append(image_small.transpose((2, 0, 1)))\n            images_large.append(image.transpose((2, 0, 1)))\n\n        tfr_opt = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.NONE)\n\n        part_path_large = ""test-large-r%02d.tfrecords"" % i\n        part_path_small = ""test-small-r%02d.tfrecords"" % i\n        part_path_images_pickle = ""test-small-images-r%02d.pth"" % i\n        part_path_records_pickle = ""test-small-records-r%02d.pth"" % i\n\n        tfr_writer = tf.python_io.TFRecordWriter(part_path_large, tfr_opt)\n        for image in images_large * 6:\n            ex = tf.train.Example(features=tf.train.Features(feature={\n                \'shape\': tf.train.Feature(int64_list=tf.train.Int64List(value=image.shape)),\n                \'data\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image.tostring()]))}))\n            record = ex.SerializeToString()\n            tfr_writer.write(record)\n        tfr_writer.close()\n\n        tfr_writer = tf.python_io.TFRecordWriter(part_path_small, tfr_opt)\n        records = []\n        for image in images_small:\n            ex = tf.train.Example(features=tf.train.Features(feature={\n                \'shape\': tf.train.Feature(int64_list=tf.train.Int64List(value=image.shape)),\n                \'data\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image.tostring()]))}))\n            record = ex.SerializeToString()\n            records.append(record)\n            tfr_writer.write(record)\n        tfr_writer.close()\n\n        # Dump source images as target for comparison for testing\n        with open(part_path_images_pickle, \'wb\') as f:\n            pickle.dump(images_small, f)\n\n        # Dump records as target for comparison for testing\n        with open(part_path_records_pickle, \'wb\') as f:\n            pickle.dump(records, f)\n\n\nif __name__ == \'__main__\':\n    prepare_tfrecords()\n'"
doc_sources/source/conf.py,0,"b'\n# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'../../\'))\nsys.path.insert(0, os.path.abspath(\'../../cmake-build-debug\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'DareBlopy\'\ncopyright = \'2020, Stanislav Pidhorskyi\'\nauthor = \'Stanislav Pidhorskyi\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.ifconfig\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\',\n    \'sphinx.ext.napoleon\',\n]\n\nnapoleon_use_ivar = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = []\n\nmaster_doc = \'index\'\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n## The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n\ndef skip(app, what, name, obj, would_skip, options):\n    if name == ""__init__"":\n        return False\n    if name == ""__iter__"":\n        return False\n    if name == ""__next__"":\n        return False\n    return would_skip\n\n\ndef setup(app):\n    app.connect(""autodoc-skip-member"", skip)\n'"
