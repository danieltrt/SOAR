file_path,api_count,code
A3C-Torcs/A3C-Torcs.py,22,"b'import threading\nimport scipy.signal\nimport sys\nfrom helper import *\nfrom time import sleep\nfrom gym_torcs import TorcsEnv\nimport numpy as np\nimport tensorflow as tf\nimport scipy.misc\nimport os\nimport tensorflow.contrib.slim as slim\n\n# Copies one set of variables to another.\n# Used to set worker network parameters to those of global network.\ndef update_target_graph(from_scope, to_scope):\n    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n\n    op_holder = []\n    for from_var, to_var in zip(from_vars, to_vars):\n        op_holder.append(to_var.assign(from_var))\n    return op_holder\n\n\n# Processes Doom screen image to produce cropped and resized image.\ndef process_frame(frame):\n    # s = frame[10:-10, 30:-30] #Get the frame quadratic 120 - 20 and 160 - 60\n    # s = scipy.misc.imresize(s, [84, 84]) #resize 100x100 to 84x84\n    # s = np.reshape(s, [np.prod(s.shape)]) / 255.0 #make it one tuple size 7056 and makes values between 0-1 by dividing 255\n    r, g, b = frame[:, 0], frame[:, 1], frame[:, 2]  # find r,g,b values\n    img_gray = 0.2989 * r + 0.5870 * g + 0.1140 * b  # make rgb to grayscale\n    s = np.reshape(img_gray, [np.prod(img_gray.shape)]) / 255.0\n    return s\n\n\n# Discounting function used to calculate discounted returns.\ndef discount(x, gamma):\n    return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n\n\n# Used to initialize weights for policy and value output layers\ndef normalized_columns_initializer(std=1.0):\n    def _initializer(shape, dtype=None, partition_info=None):\n        out = np.random.randn(*shape).astype(np.float32)\n        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n        return tf.constant(out)\n\n    return _initializer\n\n\nclass AC_Network:\n    def __init__(self, s_size, a_size, scope, trainer, continuous=False):\n        with tf.variable_scope(scope):\n            # Input and visual encoding layers\n            self.inputs = tf.placeholder(shape=[None, s_size], dtype=tf.float32)\n            self.imageIn = tf.reshape(self.inputs, shape=[-1, 64, 64, 1])\n            self.conv1 = slim.conv2d(activation_fn=tf.nn.elu,\n                                     inputs=self.imageIn, num_outputs=16,\n                                     kernel_size=[8, 8], stride=[4, 4], padding=\'VALID\')\n            self.conv2 = slim.conv2d(activation_fn=tf.nn.elu,\n                                     inputs=self.conv1, num_outputs=32,\n                                     kernel_size=[4, 4], stride=[2, 2], padding=\'VALID\')\n            hidden = slim.fully_connected(slim.flatten(self.conv2), 256, activation_fn=tf.nn.elu)\n\n            # Recurrent network for temporal dependencies\n            lstm_cell = tf.contrib.rnn.BasicLSTMCell(256, state_is_tuple=True)\n            c_init = np.zeros((1, lstm_cell.state_size.c), np.float32)\n            h_init = np.zeros((1, lstm_cell.state_size.h), np.float32)\n            self.state_init = [c_init, h_init]  # initial state of the rnn\n            c_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.c])\n            h_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.h])\n            self.state_in = (c_in, h_in)\n            rnn_in = tf.expand_dims(hidden, [0])\n            step_size = tf.shape(self.imageIn)[:1]\n            state_in = tf.contrib.rnn.LSTMStateTuple(c_in, h_in)\n            lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n                lstm_cell, rnn_in, initial_state=state_in, sequence_length=step_size,\n                time_major=False)\n            lstm_c, lstm_h = lstm_state\n            self.state_out = (lstm_c[:1, :], lstm_h[:1, :])\n            rnn_out = tf.reshape(lstm_outputs, [-1, 256])\n\n            # Output layers for policy and value estimations\n            if not continuous:\n                self.discrete_policy = slim.fully_connected(rnn_out, a_size,\n                                                            activation_fn=tf.nn.softmax,\n                                                            weights_initializer=normalized_columns_initializer(0.01),\n                                                            biases_initializer=None)\n            else:\n                self.variance = slim.fully_connected(rnn_out, 1,  # 1 action instead of 3 actions\n                                                     activation_fn=None,\n                                                     weights_initializer=normalized_columns_initializer(0.01),\n                                                     biases_initializer=None)\n                self.mean = slim.fully_connected(rnn_out, 1,  # 1 action instead of 3 actions\n                                                 activation_fn=None,\n                                                 weights_initializer=normalized_columns_initializer(0.01),\n                                                 biases_initializer=None)\n                self.variance = tf.squeeze(self.variance)\n                self.variance = tf.nn.softplus(self.variance) + 1e-5\n                self.normal_dist = tf.contrib.distributions.Normal(self.mean, self.variance)\n                self.actions = self.normal_dist._sample_n(1)\n                # self.actions = [tf.clip_by_value(self.actions[0][0][0], -1, 1),\n                #                 tf.clip_by_value(self.actions[0][0][1], 0, 1),\n                #                 tf.clip_by_value(self.actions[0][0][2], 0, 1)]\n                self.actions = [tf.clip_by_value(self.actions[0][0][0], -1, 1)]  # 1 action instead of 3 actions\n\n            self.value = slim.fully_connected(rnn_out, 1,\n                                              activation_fn=None,\n                                              weights_initializer=normalized_columns_initializer(1.0),\n                                              biases_initializer=None)\n\n            # Only the worker network need ops for loss functions and gradient updating.\n            if scope != \'global\':\n                self.target_v = tf.placeholder(shape=[None], dtype=tf.float32)\n                self.advantages = tf.placeholder(shape=[None], dtype=tf.float32)\n\n                if not continuous:\n                    self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n                    self.actions_onehot = tf.one_hot(self.actions, a_size, dtype=tf.float32)\n\n                    self.responsible_outputs = tf.reduce_sum(self.discrete_policy * self.actions_onehot, [1])\n\n                    self.entropy_loss = - tf.reduce_sum(self.discrete_policy * tf.log(self.discrete_policy))\n                    self.policy_loss = -tf.reduce_sum(tf.log(self.responsible_outputs) * self.advantages)\n                    self.value_loss = 0.5 * tf.reduce_sum(tf.square(self.target_v - tf.reshape(self.value, [-1])))\n\n                    self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy_loss * 0.01\n                else:\n                    self.steer = tf.placeholder(shape=[None], dtype=tf.float32)\n                    # self.accelerate = tf.placeholder(shape=[None], dtype=tf.float32)\n                    # self.brake = tf.placeholder(shape=[None], dtype=tf.float32)\n\n                    epsilon = 1e-10\n                    # actions = tf.stack([self.steer, self.accelerate, self.brake], axis=1)  # 3 actions\n                    actions = tf.stack([self.steer], axis=1)  # steering only\n                    entropy = tf.reduce_sum(tf.log(self.variance + epsilon))\n                    self.entropy_loss = -tf.reduce_sum(entropy)\n\n                    squared_difference = tf.squared_difference(actions, self.mean)\n                    squared_distance = tf.reduce_sum(squared_difference / (self.variance + epsilon), axis=1)\n\n                    self.policy_loss = -tf.reduce_sum((entropy + squared_distance) * self.advantages)\n\n                    self.value_loss = tf.reduce_sum(tf.squared_difference(self.target_v, self.value))\n\n                    self.loss = (self.policy_loss + 0.5 * self.value_loss + 1e-4 * self.entropy_loss)\n\n                # Get gradients from local network using local losses\n                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n                self.gradients = tf.gradients(self.loss, local_vars)\n                self.var_norms = tf.global_norm(local_vars)\n                grads, self.grad_norms = tf.clip_by_global_norm(self.gradients, 40.0)\n\n                # Apply local gradients to global network\n                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \'global\')\n                self.apply_grads = trainer.apply_gradients(zip(grads, global_vars))\n\n\nclass Worker:\n    def __init__(self, game, name, s_size, a_size, trainer, model_path, global_episodes, continuous=False):\n        self.continuous = continuous\n        self.name = ""worker_"" + str(name)\n        self.number = name\n        self.model_path = model_path\n        self.trainer = trainer\n        self.global_episodes = global_episodes\n        self.increment = self.global_episodes.assign_add(1)\n        self.episode_rewards = []\n        self.episode_lengths = []\n        self.episode_mean_values = []\n        self.summary_writer_train = tf.summary.FileWriter(""train_"" + str(self.number))\n        self.summary_writer_play = tf.summary.FileWriter(""play_"" + str(self.number))\n\n        # Create the local copy of the network and the tensorflow op to copy global params to local network\n        self.local_AC = AC_Network(s_size, a_size, self.name, trainer, continuous)\n        self.update_local_ops = update_target_graph(\'global\', self.name)\n        self.env = game\n        if not continuous:\n            self.actions = np.identity(a_size, dtype=bool).tolist()    #To have same format as doom\n\n    def train(self, rollout, sess, gamma, bootstrap_value):\n        rollout = np.array(rollout)\n        observations = rollout[:, 0]\n        actions = np.asarray(rollout[:, 1].tolist())\n        rewards = rollout[:, 2]\n        next_observations = rollout[:, 3]\n        values = rollout[:, 5]\n\n        # We take the rewards and values from rollout, and use them to generate the advantage and discounted returns.\n        # The advantage function uses ""Generalized Advantage Estimation""\n        self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n        discounted_rewards = discount(self.rewards_plus, gamma)[:-1]\n        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n        advantages = discount(advantages, gamma)\n\n        # Update the global network using gradients from loss\n        # Generate network statistics to periodically save\n        rnn_state = self.local_AC.state_init\n        if not self.continuous:\n            feed_dict = {self.local_AC.target_v: discounted_rewards,\n                         self.local_AC.inputs: np.vstack(observations),\n                         self.local_AC.actions: actions,\n                         self.local_AC.advantages: advantages,\n                         self.local_AC.state_in[0]: rnn_state[0],\n                         self.local_AC.state_in[1]: rnn_state[1]}\n        else:\n            feed_dict = {self.local_AC.target_v: discounted_rewards,\n                         self.local_AC.inputs: np.vstack(observations),\n                         self.local_AC.steer: actions[:, 0],\n                         # self.local_AC.accelerate: actions[:, 1],\n                         # self.local_AC.brake: actions[:, 2],\n                         self.local_AC.advantages: advantages,\n                         self.local_AC.state_in[0]: rnn_state[0],\n                         self.local_AC.state_in[1]: rnn_state[1]}\n        v_l, p_l, e_l, loss_f, g_n, v_n, _ = sess.run([self.local_AC.value_loss,\n                                                       self.local_AC.policy_loss,\n                                                       self.local_AC.entropy_loss,\n                                                       self.local_AC.loss,\n                                                       self.local_AC.grad_norms,\n                                                       self.local_AC.var_norms,\n                                                       self.local_AC.apply_grads],\n                                                      feed_dict=feed_dict)\n        return v_l / len(rollout), p_l / len(rollout), e_l / len(rollout), loss_f / len(rollout), g_n, v_n\n\n    def work(self, max_episode_length, gamma, sess, coord, saver, training):\n        episode_count = sess.run(self.global_episodes)\n        total_steps = 0\n        print(""Starting worker "" + str(self.number))\n        with sess.as_default(), sess.graph.as_default():\n            while not coord.should_stop():\n                sess.run(self.update_local_ops)\n                episode_buffer = []\n                episode_values = []\n                episode_frames = []\n                episode_reward = 0\n                episode_step_count = 0\n                d = False\n\n                ob = self.env.reset(relaunch=False)\n                s = ob.img\n\n                # For state\n                s = process_frame(s)\n\n                # For creating gifs\n                to_gif = np.reshape(s, (64, 64)) * 255\n                episode_frames.append(to_gif)\n                rnn_state = self.local_AC.state_init\n\n                while not d:\n                    # Take an action using probabilities from policy network output.\n                    if not self.continuous:\n                        a_dist, v, rnn_state = sess.run(\n                            [self.local_AC.discrete_policy, self.local_AC.value, self.local_AC.state_out],\n                            feed_dict={self.local_AC.inputs: [s],\n                                       self.local_AC.state_in[0]: rnn_state[0],\n                                       self.local_AC.state_in[1]: rnn_state[1]})\n                        a_t = np.random.choice(a_dist[0], p=a_dist[0])  # a random sample is generated given probabs\n                        a_t = np.argmax(a_dist == a_t)\n                        ob, reward, d, info = self.env.step_discrete(self.actions[a_t])\n                        r = reward/1000\n                    else:\n                        a_t, v, rnn_state = sess.run(\n                            [self.local_AC.actions,\n                             self.local_AC.value,\n                             self.local_AC.state_out],\n                            feed_dict={self.local_AC.inputs: [s],\n                                       self.local_AC.state_in[0]: rnn_state[0],\n                                       self.local_AC.state_in[1]: rnn_state[1]})\n\n                        a_t.append(0)  # add accel\n                        a_t.append(0)  # add brake\n                        ob, r, d, info = self.env.step(a_t)\n                        del a_t[-1]  # delete accel\n                        del a_t[-1]  # delete brake\n                        r = r/1000\n\n                    if not d:\n                        s1 = ob.img\n\n                        # for state\n                        s1 = process_frame(s1)\n                        # For creating gifs\n                        to_gif1 = np.reshape(s1, (64, 64)) * 255\n                        episode_frames.append(to_gif1)\n                    else:\n                        s1 = s\n\n                    episode_buffer.append([s, a_t, r, s1, d, v[0, 0]])\n                    episode_values.append(v[0, 0])\n\n                    episode_reward += r\n                    s = s1\n                    total_steps += 1\n                    episode_step_count += 1\n\n                    # If the episode hasn\'t ended, but the experience buffer is full, then we\n                    # make an update step using that experience rollout.\n                    if training and len(episode_buffer) == 100 and d is not True:  #batch to 100 30 before \n                        # Since we don\'t know what the true final return is, we ""bootstrap"" from our current\n                        # value estimation\n                        v1 = sess.run(self.local_AC.value,\n                                      feed_dict={self.local_AC.inputs: [s],\n                                                 self.local_AC.state_in[0]: rnn_state[0],\n                                                 self.local_AC.state_in[1]: rnn_state[1]})[0, 0]\n                        v_l, p_l, e_l, loss_f, g_n, v_n = self.train(episode_buffer, sess, gamma, v1)\n                        episode_buffer = []\n                        sess.run(self.update_local_ops)\n                    if d:\n                        break\n\n                self.episode_rewards.append(episode_reward)\n                self.episode_lengths.append(episode_step_count)\n                self.episode_mean_values.append(np.mean(episode_values))\n\n                # Update the network using the experience buffer at the end of the episode.\n                if training and len(episode_buffer) != 0:\n                    v_l, p_l, e_l, loss_f, g_n, v_n = self.train(episode_buffer, sess, gamma, 0.0)\n\n                # Periodically save gifs of episodes, model parameters, and summary statistics.\n                if episode_count % 5 == 0 and episode_count != 0:\n                    if training and self.name == \'worker_0\' and episode_count % 50 == 0:\n                        time_per_step = 0.05\n                        images = np.array(episode_frames)\n                        make_gif(images, \'./frames/image\' + str(episode_count) +\'_reward_\' + str(episode_reward) + \'.gif\',\n                                 duration=len(images) * time_per_step, true_image=True, salience=False)\n                    if training and episode_count % 5 == 0 and self.name == \'worker_0\':\n                        saver.save(sess, self.model_path + \'/model-\' + str(episode_count) + \'.cptk\')\n                        print(""Saved Model"")\n\n                    mean_reward = np.mean(self.episode_rewards[-5:])  # mean over the last 5 elements of episode Rs\n                    mean_length = np.mean(self.episode_lengths[-5:])\n                    mean_value = np.mean(self.episode_mean_values[-5:])\n\n                    summary = tf.Summary()\n                    summary.value.add(tag=\'Performance/Reward\', simple_value=float(mean_reward))\n                    summary.value.add(tag=\'Performance/Length\', simple_value=float(mean_length))\n                    summary.value.add(tag=\'Performance/Value\', simple_value=float(mean_value))\n                    if training:\n                        summary.value.add(tag=\'Losses/Value Loss\', simple_value=float(v_l))\n                        summary.value.add(tag=\'Losses/Policy Loss\', simple_value=float(p_l))\n                        summary.value.add(tag=\'Losses/Entropy\', simple_value=float(e_l))\n                        summary.value.add(tag=\'Losses/Grad Norm\', simple_value=float(g_n))\n                        summary.value.add(tag=\'Losses/Var Norm\', simple_value=float(v_n))\n                        summary.value.add(tag=\'Losses/Loss\', simple_value=float(loss_f))\n                        self.summary_writer_train.add_summary(summary, episode_count)\n                        self.summary_writer_train.flush()\n                    else:\n                        self.summary_writer_play.add_summary(summary, episode_count)\n                        self.summary_writer_play.flush()\n\n                if self.name == \'worker_0\':\n                    sess.run(self.increment)\n                episode_count += 1\n\n\ndef initialize_variables(saver, sess, load_model):\n    if load_model:\n        print(\'Loading Model...\')\n        ckpt = tf.train.get_checkpoint_state(model_path)\n        saver.restore(sess, ckpt.model_checkpoint_path)\n    else:\n        sess.run(tf.global_variables_initializer())\n\n\ndef play_training(training=True, load_model=True):\n    with tf.device(""/cpu:0""):\n        global_episodes = tf.Variable(0, dtype=tf.int32, name=\'global_episodes\', trainable=False)\n        # trainer = tf.train.RMSPropOptimizer(learning_rate=1e-4, decay=0.99, epsilon=1)\n        trainer = tf.train.AdamOptimizer(learning_rate=1e-4)\n        master_network = AC_Network(s_size, a_size, \'global\', None, False)\n\n        if training:\n            #num_workers = multiprocessing.cpu_count()  # Set workers at number of available CPU threads\n            num_workers = 4\n        else:\n            num_workers = 1\n\n        workers = []\n        for i in range(num_workers):\n            workers.append(\n                Worker(TorcsEnv(vision=True, throttle=False, gear_change=False, port=3101 + i), i, s_size, a_size,\n                       trainer, model_path, global_episodes, False))\n        saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        coord = tf.train.Coordinator()\n        initialize_variables(saver, sess, load_model)\n        # Asynchronous magic happens: start the ""work"" process for each worker in a separate thread.\n        worker_threads = []\n        for worker in workers:\n            worker_work = lambda: worker.work(max_episode_length, gamma, sess, coord, saver, training)\n            t = threading.Thread(target=worker_work)\n            t.start()\n            sleep(0.5)\n            worker_threads.append(t)\n        coord.join(worker_threads)  # waits until the specified threads have stopped.\n\n\nif __name__ == ""__main__"":\n    max_episode_length = 300\n    gamma = .99  # discount rate for advantage estimation and reward discounting\n    s_size = 4096  # Observations are greyscale frames of 84 * 84 * 1\n    a_size = 3  # Left, Right, Forward, Brake\n    model_path = \'./model\'\n\n    tf.reset_default_graph()\n\n    if not os.path.exists(model_path):\n        os.makedirs(model_path)\n\n    # Create a directory to save episode playback gifs to\n    if not os.path.exists(\'./frames\'):\n        os.makedirs(\'./frames\')\n\n    if len(sys.argv) == 1:  # run from PyCharm\n        play_training(training=True, load_model=False)\n    elif sys.argv[1] == ""1"":  # lunch from Terminal and specify 0 or 1 as arguments\n        play_training(training=True, load_model=False)\n    elif sys.argv[1] == ""0"":\n        play_training(training=False, load_model=True)\n'"
A3C-Torcs/gym_torcs.py,34,"b'import gym\nfrom gym import spaces\nimport numpy as np\n# from os import path\nimport snakeoil3_gym as snakeoil3\nimport numpy as np\nimport copy\nimport collections as col\nimport os\nimport subprocess\nimport time\nimport signal\n\n\nclass TorcsEnv:\n    terminal_judge_start = 100  # Speed limit is applied after this step\n    termination_limit_progress = 5  # [km/h], episode terminates if car is running slower than this limit\n    default_speed = 50\n    speed_ok = False\n\n    initial_reset = True\n\n    def start_torcs_process(self):\n        if self.torcs_proc is not None:\n            os.killpg(os.getpgid(self.torcs_proc.pid), signal.SIGKILL)\n            time.sleep(0.5)\n            self.torcs_proc = None\n        window_title = str(self.port)\n        print(""Window title:"", window_title)\n        command = \'torcs -nofuel -nodamage -nolaptime -title {} -p {}\'.format(window_title, self.port)\n        if self.vision is True:\n            command += \' -vision\'\n        self.torcs_proc = subprocess.Popen([command], shell=True, preexec_fn=os.setsid)\n        time.sleep(0.5)\n        os.system(\'sh autostart.sh {}\'.format(window_title))\n        time.sleep(0.5)\n\n    def __init__(self, vision=False, throttle=False, gear_change=False, port=3101):\n       #print(""Init"")\n        self.vision = vision\n        self.throttle = throttle\n        self.gear_change = gear_change\n        self.port = port\n        self.torcs_proc = None\n\n        self.initial_run = True\n\n        print(""launch torcs"")\n        time.sleep(0.5)\n        self.start_torcs_process()\n        time.sleep(0.5)\n\n        """"""\n        # Modify here if you use multiple tracks in the environment\n        self.client = snakeoil3.Client(p=3101)  # Open new UDP in vtorcs\n        self.client.MAX_STEPS = np.inf\n        client = self.client\n        client.get_servers_input()  # Get the initial input from torcs\n        obs = client.S.d  # Get the current full-observation from torcs\n        """"""\n        if throttle is False:\n            self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(1,))\n        else:\n            self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(2,))\n\n        if vision is False:\n            high = np.array([1., np.inf, np.inf, np.inf, 1., np.inf, 1., np.inf])\n            low = np.array([0., -np.inf, -np.inf, -np.inf, 0., -np.inf, 0., -np.inf])\n            self.observation_space = spaces.Box(low=low, high=high)\n        else:\n            high = np.array([1., np.inf, np.inf, np.inf, 1., np.inf, 1., np.inf, 255])\n            low = np.array([0., -np.inf, -np.inf, -np.inf, 0., -np.inf, 0., -np.inf, 0])\n            self.observation_space = spaces.Box(low=low, high=high)\n\n    def step_discrete(self, u):\n        # convert thisAction to the actual torcs actionstr\n        client = self.client\n\n        this_action = self.agent_to_torcs_discrete(u)\n\n        # Apply Action\n        action_torcs = client.R.d\n\n        # Steering\n        if this_action[\'steerleft\'] == True:\n            action_torcs[\'steer\'] += 1\n            action_torcs[\'accel\'] = 0\n            action_torcs[\'brake\'] = 0\n        if this_action[\'steerright\'] == True:\n            action_torcs[\'steer\'] += -1\n            action_torcs[\'accel\'] = 0\n            action_torcs[\'brake\'] = 0\n        #if this_action[\'accel\'] == True:\n        #    action_torcs[\'accel\'] += 1\n        #    action_torcs[\'steer\'] = 0\n        #    action_torcs[\'brake\'] = 0\n        if this_action[\'brake\'] == True:\n            action_torcs[\'brake\'] = 0\n            action_torcs[\'accel\'] = 0\n            action_torcs[\'steer\'] = 0\n\n        #print(""action_torcs: "",self.port, action_torcs)\n        #action_torcs[\'steer\'] = this_action[\'steer\']  # in [-1, 1]\n\n        #for throttle\n        if self.throttle is False:\n            target_speed = self.default_speed\n            speed_okay = self.speed_ok\n            if client.S.d[\'speedX\'] < target_speed - (client.R.d[\'steer\']*50):   \n                client.R.d[\'accel\'] += 0.2                                  #0.2 to around 30 km/h\n                \n            else:\n                client.R.d[\'accel\'] -= 0.01\n                \n\n            if client.R.d[\'accel\'] > 0.2:\n                client.R.d[\'accel\'] = 0.2\n\n            if client.S.d[\'speedX\'] < target_speed and speed_okay == False:\t\t\n                client.R.d[\'accel\'] += 1#/(client.S.d[\'speedX\']+.1)\n                \n            #To go fast to target speed and then stop\n            if client.S.d[\'speedX\'] > target_speed:\n                self.speed_ok = True    \n\n\n            #Traction Control System\n            if ((client.S.d[\'wheelSpinVel\'][2]+client.S.d[\'wheelSpinVel\'][3]) -\n               (client.S.d[\'wheelSpinVel\'][0]+client.S.d[\'wheelSpinVel\'][1]) > 5):\n                action_torcs[\'accel\'] -= 0.2\n                \n\n        #  Automatic Gear Change by Snakeoil\n        if self.gear_change is True:\n            action_torcs[\'gear\'] = this_action[\'gear\']\n        else:\n            #  Automatic Gear Change by Snakeoil is possible\n            action_torcs[\'gear\'] = 1\n            if self.throttle:\n                if client.S.d[\'speedX\'] > 50:\n                    action_torcs[\'gear\'] = 2\n                if client.S.d[\'speedX\'] > 80:\n                    action_torcs[\'gear\'] = 3\n                if client.S.d[\'speedX\'] > 110:\n                    action_torcs[\'gear\'] = 4\n                if client.S.d[\'speedX\'] > 140:\n                    action_torcs[\'gear\'] = 5\n                if client.S.d[\'speedX\'] > 170:\n                    action_torcs[\'gear\'] = 6\n\n        # Save the privious full-obs from torcs for the reward calculation\n        obs_pre = copy.deepcopy(client.S.d)\n\n        # One-Step Dynamics Update #################################\n        # Apply the Agent\'s action into torcs\n        client.respond_to_server()\n        # Get the response of TORCS\n        client.get_servers_input()\n\n        # Get the current full-observation from torcs\n        obs = client.S.d\n\n        # Make an obsevation from a raw observation vector from TORCS\n        self.observation = self.make_observaton(obs)\n\n        # Reward setting Here #######################################\n        # direction-dependent positive reward\n        track = np.array(obs[\'track\'])\n        trackPos = np.array(obs[\'trackPos\'])\n        sp = np.array(obs[\'speedX\'])\n        damage = np.array(obs[\'damage\'])\n        rpm = np.array(obs[\'rpm\'])\n        distance = np.array(obs[\'distRaced\'])\n        #print (""Speed: "", sp)\n\n        progress = sp * np.cos(obs[\'angle\']) - np.abs(sp * np.sin(obs[\'angle\'])) - sp * np.abs(obs[\'trackPos\'])\n        reward = progress\n\n        # collision detection\n        if obs[\'damage\'] - obs_pre[\'damage\'] > 0:\n            reward = -1\n\n        # Termination judgement #########################\n        episode_terminate = False\n        if abs(track.any()) > 1 or abs(trackPos) > 1:  # Episode is terminated if the car is out of track\n            reward = -200\n            episode_terminate = True\n            client.R.d[\'meta\'] = True\n\n        #if self.terminal_judge_start < self.time_step:  # Episode terminates if the progress of agent is small\n        #    if progress < self.termination_limit_progress:\n        #        print(""No progress"")\n        #        episode_terminate = True\n        #        client.R.d[\'meta\'] = True\n\n        if obs[\'lastLapTime\'] > 0:\n            reward = 200\n            episode_terminate = True\n            client.R.d[\'meta\'] = True\n\n        if np.cos(obs[\'angle\']) < 0:  # Episode is terminated if the agent runs backward\n            reward = -200\n            episode_terminate = True\n            client.R.d[\'meta\'] = True\n\n        if client.R.d[\'meta\'] is True:  # Send a reset signal\n            self.initial_run = False\n            self.speed_ok = False\n            client.respond_to_server()\n\n        self.time_step += 1\n\n        return self.get_obs(), reward, client.R.d[\'meta\'], {}\n\n    def step(self, u):\n        #print(""Step"")\n        # convert thisAction to the actual torcs actionstr\n        client = self.client\n\n        this_action = self.agent_to_torcs(u)\n\n        # Apply Action\n        action_torcs = client.R.d\n\n        # Steering\n        action_torcs[\'steer\'] = this_action[\'steer\']  # in [-1, 1]\n\n        #  Simple Autnmatic Throttle Control by Snakeoil\n        if self.throttle is False:\n            target_speed = self.default_speed\n            if client.S.d[\'speedX\'] < target_speed - (client.R.d[\'steer\']*50):\n                client.R.d[\'accel\'] += .01\n            else:\n                client.R.d[\'accel\'] -= .01\n\n            if client.R.d[\'accel\'] > 0.2:\n                client.R.d[\'accel\'] = 0.2\n\n            if client.S.d[\'speedX\'] < 10:\n                client.R.d[\'accel\'] += 1/(client.S.d[\'speedX\']+.1)\n\n            # Traction Control System\n            if ((client.S.d[\'wheelSpinVel\'][2]+client.S.d[\'wheelSpinVel\'][3]) -\n               (client.S.d[\'wheelSpinVel\'][0]+client.S.d[\'wheelSpinVel\'][1]) > 5):\n                action_torcs[\'accel\'] -= .2\n        else:\n            action_torcs[\'accel\'] = this_action[\'accel\']\n            action_torcs[\'brake\'] = this_action[\'brake\']\n\n        #  Automatic Gear Change by Snakeoil\n        if self.gear_change is True:\n            action_torcs[\'gear\'] = this_action[\'gear\']\n        else:\n            #  Automatic Gear Change by Snakeoil is possible\n            action_torcs[\'gear\'] = 1\n            if self.throttle:\n                if client.S.d[\'speedX\'] > 50:\n                    action_torcs[\'gear\'] = 2\n                if client.S.d[\'speedX\'] > 80:\n                    action_torcs[\'gear\'] = 3\n                if client.S.d[\'speedX\'] > 110:\n                    action_torcs[\'gear\'] = 4\n                if client.S.d[\'speedX\'] > 140:\n                    action_torcs[\'gear\'] = 5\n                if client.S.d[\'speedX\'] > 170:\n                    action_torcs[\'gear\'] = 6\n        # Save the privious full-obs from torcs for the reward calculation\n        obs_pre = copy.deepcopy(client.S.d)\n\n        # One-Step Dynamics Update #################################\n        # Apply the Agent\'s action into torcs\n        client.respond_to_server()\n        # Get the response of TORCS\n        client.get_servers_input()\n\n        # Get the current full-observation from torcs\n        obs = client.S.d\n\n        # Make an obsevation from a raw observation vector from TORCS\n        self.observation = self.make_observaton(obs)\n\n        # Reward setting Here #######################################\n        # direction-dependent positive reward\n        track = np.array(obs[\'track\'])\n        trackPos = np.array(obs[\'trackPos\'])\n        sp = np.array(obs[\'speedX\'])\n        damage = np.array(obs[\'damage\'])\n        rpm = np.array(obs[\'rpm\'])\n        distance = np.array(obs[\'distRaced\'])\n\n        progress = sp * np.cos(obs[\'angle\']) - np.abs(sp * np.sin(obs[\'angle\'])) - sp * np.abs(obs[\'trackPos\'])\n        reward = progress\n\n        # collision detection\n        if obs[\'damage\'] - obs_pre[\'damage\'] > 0:\n            reward = -1\n\n        # Termination judgement #########################\n        episode_terminate = False\n        if abs(track.any()) > 1 or abs(trackPos) > 1:  # Episode is terminated if the car is out of track\n            reward = -200\n            episode_terminate = True\n            client.R.d[\'meta\'] = True\n\n        if obs[\'lastLapTime\'] > 0:\n            reward = 200\n            episode_terminate = True\n            client.R.d[\'meta\'] = True\n\n        if np.cos(obs[\'angle\']) < 0:  # Episode is terminated if the agent runs backward\n            reward = -200\n            episode_terminate = True\n            client.R.d[\'meta\'] = True\n\n        if client.R.d[\'meta\'] is True:  # Send a reset signal\n            self.initial_run = False\n            client.respond_to_server()\n\n        self.time_step += 1\n\n        return self.get_obs(), reward, client.R.d[\'meta\'], {}\n\n    def reset(self, relaunch=False):\n        #print(""Reset"")\n\n        self.time_step = 0\n\n        if self.initial_reset is not True:\n            self.client.R.d[\'meta\'] = True\n            self.client.respond_to_server()\n\n            ## TENTATIVE. Restarting TORCS every episode suffers the memory leak bug!\n            if relaunch is True:\n                self.reset_torcs()\n                print(""### TORCS is RELAUNCHED ###"")\n\n        # Modify here if you use multiple tracks in the environment\n        self.client = snakeoil3.Client(self.start_torcs_process, p=self.port)  # Open new UDP in vtorcs\n        self.client.MAX_STEPS = np.inf\n\n        client = self.client\n        client.get_servers_input()  # Get the initial input from torcs\n\n        obs = client.S.d  # Get the current full-observation from torcs\n        self.observation = self.make_observaton(obs)\n\n        self.last_u = None\n\n        self.initial_reset = False\n        return self.get_obs()\n\n    def end(self):\n        os.killpg(os.getpgid(self.torcs_proc.pid), signal.SIGKILL)\n\n    def get_obs(self):\n        return self.observation\n\n    def reset_torcs(self):\n       #print(""relaunch torcs"")\n        self.torcs_proc.terminate()\n        time.sleep(0.5)\n        self.start_torcs_process()\n        time.sleep(0.5)\n\n    def agent_to_torcs(self, u):\n        torcs_action = {\'steer\': u[0]}\n\n        if self.throttle is True:  # throttle action is enabled\n            torcs_action.update({\'accel\': u[1]})\n            torcs_action.update({\'brake\': u[2]})\n\n        if self.gear_change is True: # gear change action is enabled\n            torcs_action.update({\'gear\': int(u[3])})\n\n        return torcs_action\n\n    def agent_to_torcs_discrete(self, u):\n        torcs_action = {\'steerleft\': u[0]}\n        torcs_action.update({\'steerright\': u[1]})\n\n        if self.throttle is True:  # throttle action is enabled\n            torcs_action.update({\'accel\': u[2]})\n            torcs_action.update({\'brake\': u[3]})\n        else:\n            torcs_action.update({\'brake\': u[2]})\n\n        if self.gear_change is True: # gear change action is enabled\n            torcs_action.update({\'gear\': u[2]})\n\n        return torcs_action\n\n\n    def obs_vision_to_image_rgb(self, obs_image_vec):\n        image_vec = obs_image_vec\n        rgb = []\n        temp = []\n        # convert size 64x64x3 = 12288 to 64x64=4096 2-D list\n        # with rgb values grouped together.\n        # Format similar to the observation in openai gym\n        for i in range(0,12286,3):\n            temp.append(image_vec[i])\n            temp.append(image_vec[i+1])\n            temp.append(image_vec[i+2])\n            rgb.append(temp)\n            temp = []\n        return np.array(rgb, dtype=np.uint8)\n\n    def make_observaton(self, raw_obs):\n        names = [\'focus\', \'speedX\', \'speedY\', \'speedZ\', \'angle\', \'damage\', \'opponents\', \'rpm\', \'track\', \'trackPos\',\n                 \'wheelSpinVel\', \'img\']\n        if self.vision is True:\n            image_rgb = self.obs_vision_to_image_rgb(raw_obs[names[11]])\n        else:\n            image_rgb = None\n        Observation = col.namedtuple(\'Observaion\', names)\n        return Observation(focus=np.array(raw_obs[\'focus\'], dtype=np.float32) / 200.,\n                           speedX=np.array(raw_obs[\'speedX\'], dtype=np.float32) / 300.0,\n                           speedY=np.array(raw_obs[\'speedY\'], dtype=np.float32) / 300.0,\n                           speedZ=np.array(raw_obs[\'speedZ\'], dtype=np.float32) / 300.0,\n                           angle=np.array(raw_obs[\'angle\'], dtype=np.float32) / 3.1416,\n                           damage=np.array(raw_obs[\'damage\'], dtype=np.float32),\n                           opponents=np.array(raw_obs[\'opponents\'], dtype=np.float32) / 200.,\n                           rpm=np.array(raw_obs[\'rpm\'], dtype=np.float32) / 10000,\n                           track=np.array(raw_obs[\'track\'], dtype=np.float32) / 200.,\n                           trackPos=np.array(raw_obs[\'trackPos\'], dtype=np.float32) / 1.,\n                           wheelSpinVel=np.array(raw_obs[\'wheelSpinVel\'], dtype=np.float32),\n                           img=image_rgb)\n'"
A3C-Torcs/helper.py,17,"b'import numpy as np\nimport tensorflow as tf\nimport csv\n\n\n# This is a simple function to reshape our game frames.\ndef processState(state1):\n    return np.reshape(state1, [21168])\n\n\n# These functions allows us to update the parameters of our target network with those of the primary network.\ndef updateTargetGraph(tfVars, tau):\n    total_vars = len(tfVars)\n    op_holder = []\n    for idx, var in enumerate(tfVars[0:total_vars / 2]):\n        op_holder.append(tfVars[idx + total_vars // 2].assign(\n            (var.value() * tau) + ((1 - tau) * tfVars[idx + total_vars // 2].value())))\n    return op_holder\n\n\ndef updateTarget(op_holder, sess):\n    for op in op_holder:\n        sess.run(op)\n    total_vars = len(tf.trainable_variables())\n    a = tf.trainable_variables()[0].eval(session=sess)\n    b = tf.trainable_variables()[total_vars / 2].eval(session=sess)\n    if a.all() == b.all():\n        print(""Target Set Success"")\n    else:\n        print(""Target Set Failed"")\n\n\n# Record performance metrics and episode logs for the Control Center.\ndef saveToCenter(i, rList, jList, bufferArray, summaryLength, h_size, sess, mainQN, time_per_step):\n    with open(\'./Center/log.csv\', \'a\') as myfile:\n        state_display = (np.zeros([1, h_size]), np.zeros([1, h_size]))\n        imagesS = []\n        for idx, z in enumerate(np.vstack(bufferArray[:, 0])):\n            img, state_display = sess.run([mainQN.salience, mainQN.rnn_state], \\\n                                          feed_dict={\n                                              mainQN.scalarInput: np.reshape(bufferArray[idx, 0], [1, 21168]) / 255.0, \\\n                                              mainQN.trainLength: 1, mainQN.state_in: state_display,\n                                              mainQN.batch_size: 1})\n            imagesS.append(img)\n        imagesS = (imagesS - np.min(imagesS)) / (np.max(imagesS) - np.min(imagesS))\n        imagesS = np.vstack(imagesS)\n        imagesS = np.resize(imagesS, [len(imagesS), 84, 84, 3])\n        luminance = np.max(imagesS, 3)\n        imagesS = np.multiply(np.ones([len(imagesS), 84, 84, 3]), np.reshape(luminance, [len(imagesS), 84, 84, 1]))\n        make_gif(np.ones([len(imagesS), 84, 84, 3]), \'./Center/frames/sal\' + str(i) + \'.gif\',\n                 duration=len(imagesS) * time_per_step, true_image=False, salience=True, salIMGS=luminance)\n\n        images = zip(bufferArray[:, 0])\n        images.append(bufferArray[-1, 3])\n        images = np.vstack(images)\n        images = np.resize(images, [len(images), 84, 84, 3])\n        make_gif(images, \'./Center/frames/image\' + str(i) + \'.gif\', duration=len(images) * time_per_step,\n                 true_image=True, salience=False)\n\n        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n        wr.writerow([i, np.mean(jList[-100:]), np.mean(rList[-summaryLength:]), \'./frames/image\' + str(i) + \'.gif\',\n                     \'./frames/log\' + str(i) + \'.csv\', \'./frames/sal\' + str(i) + \'.gif\'])\n        myfile.close()\n    with open(\'./Center/frames/log\' + str(i) + \'.csv\', \'w\') as myfile:\n        state_train = (np.zeros([1, h_size]), np.zeros([1, h_size]))\n        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n        wr.writerow([""ACTION"", ""REWARD"", ""A0"", ""A1"", \'A2\', \'A3\', \'V\'])\n        a, v = sess.run([mainQN.Advantage, mainQN.Value], \\\n                        feed_dict={mainQN.scalarInput: np.vstack(bufferArray[:, 0]) / 255.0,\n                                   mainQN.trainLength: len(bufferArray), mainQN.state_in: state_train,\n                                   mainQN.batch_size: 1})\n        wr.writerows(zip(bufferArray[:, 1], bufferArray[:, 2], a[:, 0], a[:, 1], a[:, 2], a[:, 3], v[:, 0]))\n\n\n# This code allows gifs to be saved of the training episode for use in the Control Center.\ndef make_gif(images, fname, duration=2, true_image=False, salience=False, salIMGS=None):\n    import moviepy.editor as mpy\n\n    def make_frame(t):\n        try:\n            x = images[int(len(images) / duration * t)]\n        except:\n            x = images[-1]\n\n        if true_image:\n            return x.astype(np.uint8)\n        else:\n            return ((x + 1) / 2 * 255).astype(np.uint8)\n\n    def make_mask(t):\n        try:\n            x = salIMGS[int(len(salIMGS) / duration * t)]\n        except:\n            x = salIMGS[-1]\n        return x\n\n    clip = mpy.VideoClip(make_frame, duration=duration)\n    if salience == True:\n        mask = mpy.VideoClip(make_mask, ismask=True, duration=duration)\n        clipB = clip.set_mask(mask)\n        clipB = clip.set_opacity(0)\n        mask = mask.set_opacity(0.1)\n        mask.write_gif(fname, fps=len(images) / duration, verbose=False, opt=None)\n        # clipB.write_gif(fname, fps = len(images) / duration,verbose=False)\n    else:\n        clip.write_gif(fname, fps=len(images) / duration, verbose=False, opt=None)\n'"
A3C-Torcs/snakeoil3_gym.py,0,"b'#!/usr/bin/python\n# snakeoil.py\n# Chris X Edwards <snakeoil@xed.ch>\n# Snake Oil is a Python library for interfacing with a TORCS\n# race car simulator which has been patched with the server\n# extentions used in the Simulated Car Racing competitions.\n# http://scr.geccocompetitions.com/\n#\n# To use it, you must import it and create a ""drive()"" function.\n# This will take care of option handling and server connecting, etc.\n# To see how to write your own client do something like this which is\n# a complete working client:\n# /-----------------------------------------------\\\n# |#!/usr/bin/python                              |\n# |import snakeoil                                |\n# |if __name__ == ""__main__"":                     |\n# |    C= snakeoil.Client()                       |\n# |    for step in xrange(C.maxSteps,0,-1):       |\n# |        C.get_servers_input()                  |\n# |        snakeoil.drive_example(C)              |\n# |        C.respond_to_server()                  |\n# |    C.shutdown()                               |\n# \\-----------------------------------------------/\n# This should then be a full featured client. The next step is to\n# replace \'snakeoil.drive_example()\' with your own. There is a\n# dictionary which holds various option values (see `default_options`\n# variable for all the details) but you probably only need a few\n# things from it. Mainly the `trackname` and `stage` are important\n# when developing a strategic bot.\n#\n# This dictionary also contains a ServerState object\n# (key=S) and a DriverAction object (key=R for response). This allows\n# you to get at all the information sent by the server and to easily\n# formulate your reply. These objects contain a member dictionary ""d""\n# (for data dictionary) which contain key value pairs based on the\n# server\'s syntax. Therefore, you can read the following:\n#    angle, curLapTime, damage, distFromStart, distRaced, focus,\n#    fuel, gear, lastLapTime, opponents, racePos, rpm,\n#    speedX, speedY, speedZ, track, trackPos, wheelSpinVel, z\n# The syntax specifically would be something like:\n#    X= o[S.d[\'tracPos\']]\n# And you can set the following:\n#    accel, brake, clutch, gear, steer, focus, meta\n# The syntax is:\n#     o[R.d[\'steer\']]= X\n# Note that it is \'steer\' and not \'steering\' as described in the manual!\n# All values should be sensible for their type, including lists being lists.\n# See the SCR manual or http://xed.ch/help/torcs.html for details.\n#\n# If you just run the snakeoil.py base library itself it will implement a\n# serviceable client with a demonstration drive function that is\n# sufficient for getting around most tracks.\n# Try `snakeoil.py --help` to get started.\n\n# for Python3-based torcs python robot client\nimport socket\nimport sys\nimport getopt\nimport os\nimport time\nPI= 3.14159265359\n\ndata_size = 2**17\n\n# Initialize help messages\nophelp=  \'Options:\\n\'\nophelp+= \' --host, -H <host>    TORCS server host. [localhost]\\n\'\nophelp+= \' --port, -p <port>    TORCS port. [3001]\\n\'\nophelp+= \' --id, -i <id>        ID for server. [SCR]\\n\'\nophelp+= \' --steps, -m <#>      Maximum simulation steps. 1 sec ~ 50 steps. [100000]\\n\'\nophelp+= \' --episodes, -e <#>   Maximum learning episodes. [1]\\n\'\nophelp+= \' --track, -t <track>  Your name for this track. Used for learning. [unknown]\\n\'\nophelp+= \' --stage, -s <#>      0=warm up, 1=qualifying, 2=race, 3=unknown. [3]\\n\'\nophelp+= \' --debug, -d          Output full telemetry.\\n\'\nophelp+= \' --help, -h           Show this help.\\n\'\nophelp+= \' --version, -v        Show current version.\'\nusage= \'Usage: %s [ophelp [optargs]] \\n\' % sys.argv[0]\nusage= usage + ophelp\nversion= ""20130505-2""\n\ndef clip(v,lo,hi):\n    if v<lo: return lo\n    elif v>hi: return hi\n    else: return v\n\ndef bargraph(x,mn,mx,w,c=\'X\'):\n    \'\'\'Draws a simple asciiart bar graph. Very handy for\n    visualizing what\'s going on with the data.\n    x= Value from sensor, mn= minimum plottable value,\n    mx= maximum plottable value, w= width of plot in chars,\n    c= the character to plot with.\'\'\'\n    if not w: return \'\' # No width!\n    if x<mn: x= mn      # Clip to bounds.\n    if x>mx: x= mx      # Clip to bounds.\n    tx= mx-mn # Total real units possible to show on graph.\n    if tx<=0: return \'backwards\' # Stupid bounds.\n    upw= tx/float(w) # X Units per output char width.\n    if upw<=0: return \'what?\' # Don\'t let this happen.\n    negpu, pospu, negnonpu, posnonpu= 0,0,0,0\n    if mn < 0: # Then there is a negative part to graph.\n        if x < 0: # And the plot is on the negative side.\n            negpu= -x + min(0,mx)\n            negnonpu= -mn + x\n        else: # Plot is on pos. Neg side is empty.\n            negnonpu= -mn + min(0,mx) # But still show some empty neg.\n    if mx > 0: # There is a positive part to the graph\n        if x > 0: # And the plot is on the positive side.\n            pospu= x - max(0,mn)\n            posnonpu= mx - x\n        else: # Plot is on neg. Pos side is empty.\n            posnonpu= mx - max(0,mn) # But still show some empty pos.\n    nnc= int(negnonpu/upw)*\'-\'\n    npc= int(negpu/upw)*c\n    ppc= int(pospu/upw)*c\n    pnc= int(posnonpu/upw)*\'_\'\n    return \'[%s]\' % (nnc+npc+ppc+pnc)\n\nclass Client():\n    def __init__(self,relaunch_torcs,H=None,p=None,i=None,e=None,t=None,s=None,d=None,vision=False):\n        # If you don\'t like the option defaults,  change them here.\n        self.vision = vision\n\n        self.host= \'localhost\'\n        self.port= 3001\n        self.sid= \'SCR\'\n        self.maxEpisodes=1 # ""Maximum number of learning episodes to perform""\n        self.trackname= \'unknown\'\n        self.stage= 3 # 0=Warm-up, 1=Qualifying 2=Race, 3=unknown <Default=3>\n        self.debug= False\n        self.maxSteps= 100000  # 50steps/second\n        self.relaunch_torcs = relaunch_torcs\n        if H: self.host= H\n        if p: self.port= p\n        if i: self.sid= i\n        if e: self.maxEpisodes= e\n        if t: self.trackname= t\n        if s: self.stage= s\n        if d: self.debug= d\n        self.S= ServerState()\n        self.R= DriverAction()\n        self.setup_connection()\n\n    def setup_connection(self):\n        # == Set Up UDP Socket ==\n        try:\n            self.so= socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        except socket.error as emsg:\n            print(\'Error: Could not create socket...\')\n            sys.exit(-1)\n        # == Initialize Connection To Server ==\n        self.so.settimeout(1)\n\n        n_fail = 5\n        while True:\n            # This string establishes track sensor angles! You can customize them.\n            #a= ""-90 -75 -60 -45 -30 -20 -15 -10 -5 0 5 10 15 20 30 45 60 75 90""\n            # xed- Going to try something a bit more aggressive...\n            a= ""-45 -19 -12 -7 -4 -2.5 -1.7 -1 -.5 0 .5 1 1.7 2.5 4 7 12 19 45""\n\n            initmsg=\'%s(init %s)\' % (self.sid,a)\n\n            try:\n                self.so.sendto(initmsg.encode(), (self.host, self.port))\n            except socket.error as emsg:\n                sys.exit(-1)\n            sockdata= str()\n            try:\n                sockdata,addr= self.so.recvfrom(data_size)\n                sockdata = sockdata.decode(\'utf-8\')\n            except socket.error as emsg:\n                print(""Waiting for server on %d............"" % self.port)\n                print(""Count Down : "" + str(n_fail))\n                if n_fail < 0:\n                    self.relaunch_torcs()\n                    n_fail = 5\n                n_fail -= 1\n\n            identify = \'***identified***\'\n            if identify in sockdata:\n                print(""Client connected on %d.............."" % self.port)\n                break\n\n    def parse_the_command_line(self):\n        try:\n            (opts, args) = getopt.getopt(sys.argv[1:], \'H:p:i:m:e:t:s:dhv\',\n                       [\'host=\',\'port=\',\'id=\',\'steps=\',\n                        \'episodes=\',\'track=\',\'stage=\',\n                        \'debug\',\'help\',\'version\'])\n        except getopt.error as why:\n            print(\'getopt error: %s\\n%s\' % (why, usage))\n            sys.exit(-1)\n        try:\n            for opt in opts:\n                if opt[0] == \'-h\' or opt[0] == \'--help\':\n                    print(usage)\n                    sys.exit(0)\n                if opt[0] == \'-d\' or opt[0] == \'--debug\':\n                    self.debug= True\n                if opt[0] == \'-H\' or opt[0] == \'--host\':\n                    self.host= opt[1]\n                if opt[0] == \'-i\' or opt[0] == \'--id\':\n                    self.sid= opt[1]\n                if opt[0] == \'-t\' or opt[0] == \'--track\':\n                    self.trackname= opt[1]\n                if opt[0] == \'-s\' or opt[0] == \'--stage\':\n                    self.stage= int(opt[1])\n                if opt[0] == \'-p\' or opt[0] == \'--port\':\n                    self.port= int(opt[1])\n                if opt[0] == \'-e\' or opt[0] == \'--episodes\':\n                    self.maxEpisodes= int(opt[1])\n                if opt[0] == \'-m\' or opt[0] == \'--steps\':\n                    self.maxSteps= int(opt[1])\n                if opt[0] == \'-v\' or opt[0] == \'--version\':\n                    print(\'%s %s\' % (sys.argv[0], version))\n                    sys.exit(0)\n        except ValueError as why:\n            print(\'Bad parameter \\\'%s\\\' for option %s: %s\\n%s\' % (\n                                       opt[1], opt[0], why, usage))\n            sys.exit(-1)\n        if len(args) > 0:\n            print(\'Superflous input? %s\\n%s\' % (\', \'.join(args), usage))\n            sys.exit(-1)\n\n    def get_servers_input(self):\n        \'\'\'Server\'s input is stored in a ServerState object\'\'\'\n        if not self.so: return\n        sockdata= str()\n\n        while True:\n            try:\n                # Receive server data\n                sockdata,addr= self.so.recvfrom(data_size)\n                sockdata = sockdata.decode(\'utf-8\')\n            except socket.error as emsg:\n                print(\'.\', end=\' \')\n                #print ""Waiting for data on %d.............."" % self.port\n            if \'***identified***\' in sockdata:\n                print(""Client connected on %d.............."" % self.port)\n                continue\n            elif \'***shutdown***\' in sockdata:\n                print(((""Server has stopped the race on %d. ""+\n                        ""You were in %d place."") %\n                        (self.port,self.S.d[\'racePos\'])))\n                self.shutdown()\n                return\n            elif \'***restart***\' in sockdata:\n                # What do I do here?\n                print(""Server has restarted the race on %d."" % self.port)\n                # I haven\'t actually caught the server doing this.\n                self.shutdown()\n                return\n            elif not sockdata: # Empty?\n                continue       # Try again.\n            else:\n                self.S.parse_server_str(sockdata)\n                if self.debug:\n                    sys.stderr.write(""\\x1b[2J\\x1b[H"") # Clear for steady output.\n                    print(self.S)\n                break # Can now return from this function.\n\n    def respond_to_server(self):\n        if not self.so: return\n        try:\n            message = repr(self.R)\n            self.so.sendto(message.encode(), (self.host, self.port))\n        except socket.error as emsg:\n            print(""Error sending to server: %s Message %s"" % (emsg[1],str(emsg[0])))\n            sys.exit(-1)\n        if self.debug: print(self.R.fancyout())\n        # Or use this for plain output:\n        #if self.debug: print self.R\n\n    def shutdown(self):\n        if not self.so: return\n        print((""Race terminated or %d steps elapsed. Shutting down %d.""\n               % (self.maxSteps,self.port)))\n        self.so.close()\n        self.so = None\n        #sys.exit() # No need for this really.\n\nclass ServerState():\n    \'\'\'What the server is reporting right now.\'\'\'\n    def __init__(self):\n        self.servstr= str()\n        self.d= dict()\n\n    def parse_server_str(self, server_string):\n        \'\'\'Parse the server string.\'\'\'\n        self.servstr= server_string.strip()[:-1]\n        sslisted= self.servstr.strip().lstrip(\'(\').rstrip(\')\').split(\')(\')\n        for i in sslisted:\n            w= i.split(\' \')\n            self.d[w[0]]= destringify(w[1:])\n\n    def __repr__(self):\n        # Comment the next line for raw output:\n        return self.fancyout()\n        # -------------------------------------\n        out= str()\n        for k in sorted(self.d):\n            strout= str(self.d[k])\n            if type(self.d[k]) is list:\n                strlist= [str(i) for i in self.d[k]]\n                strout= \', \'.join(strlist)\n            out+= ""%s: %s\\n"" % (k,strout)\n        return out\n\n    def fancyout(self):\n        \'\'\'Specialty output for useful ServerState monitoring.\'\'\'\n        out= str()\n        sensors= [ # Select the ones you want in the order you want them.\n        #\'curLapTime\',\n        #\'lastLapTime\',\n        \'stucktimer\',\n        #\'damage\',\n        #\'focus\',\n        \'fuel\',\n        #\'gear\',\n        \'distRaced\',\n        \'distFromStart\',\n        #\'racePos\',\n        \'opponents\',\n        \'wheelSpinVel\',\n        \'z\',\n        \'speedZ\',\n        \'speedY\',\n        \'speedX\',\n        \'targetSpeed\',\n        \'rpm\',\n        \'skid\',\n        \'slip\',\n        \'track\',\n        \'trackPos\',\n        \'angle\',\n        ]\n\n        #for k in sorted(self.d): # Use this to get all sensors.\n        for k in sensors:\n            if type(self.d.get(k)) is list: # Handle list type data.\n                if k == \'track\': # Nice display for track sensors.\n                    strout= str()\n                 #  for tsensor in self.d[\'track\']:\n                 #      if   tsensor >180: oc= \'|\'\n                 #      elif tsensor > 80: oc= \';\'\n                 #      elif tsensor > 60: oc= \',\'\n                 #      elif tsensor > 39: oc= \'.\'\n                 #      #elif tsensor > 13: oc= chr(int(tsensor)+65-13)\n                 #      elif tsensor > 13: oc= chr(int(tsensor)+97-13)\n                 #      elif tsensor >  3: oc= chr(int(tsensor)+48-3)\n                 #      else: oc= \'_\'\n                 #      strout+= oc\n                 #  strout= \' -> \'+strout[:9] +\' \' + strout[9] + \' \' + strout[10:]+\' <-\'\n                    raw_tsens= [\'%.1f\'%x for x in self.d[\'track\']]\n                    strout+= \' \'.join(raw_tsens[:9])+\'_\'+raw_tsens[9]+\'_\'+\' \'.join(raw_tsens[10:])\n                elif k == \'opponents\': # Nice display for opponent sensors.\n                    strout= str()\n                    for osensor in self.d[\'opponents\']:\n                        if   osensor >190: oc= \'_\'\n                        elif osensor > 90: oc= \'.\'\n                        elif osensor > 39: oc= chr(int(osensor/2)+97-19)\n                        elif osensor > 13: oc= chr(int(osensor)+65-13)\n                        elif osensor >  3: oc= chr(int(osensor)+48-3)\n                        else: oc= \'?\'\n                        strout+= oc\n                    strout= \' -> \'+strout[:18] + \' \' + strout[18:]+\' <-\'\n                else:\n                    strlist= [str(i) for i in self.d[k]]\n                    strout= \', \'.join(strlist)\n            else: # Not a list type of value.\n                if k == \'gear\': # This is redundant now since it\'s part of RPM.\n                    gs= \'_._._._._._._._._\'\n                    p= int(self.d[\'gear\']) * 2 + 2  # Position\n                    l= \'%d\'%self.d[\'gear\'] # Label\n                    if l==\'-1\': l= \'R\'\n                    if l==\'0\':  l= \'N\'\n                    strout= gs[:p]+ \'(%s)\'%l + gs[p+3:]\n                elif k == \'damage\':\n                    strout= \'%6.0f %s\' % (self.d[k], bargraph(self.d[k],0,10000,50,\'~\'))\n                elif k == \'fuel\':\n                    strout= \'%6.0f %s\' % (self.d[k], bargraph(self.d[k],0,100,50,\'f\'))\n                elif k == \'speedX\':\n                    cx= \'X\'\n                    if self.d[k]<0: cx= \'R\'\n                    strout= \'%6.1f %s\' % (self.d[k], bargraph(self.d[k],-30,300,50,cx))\n                elif k == \'speedY\': # This gets reversed for display to make sense.\n                    strout= \'%6.1f %s\' % (self.d[k], bargraph(self.d[k]*-1,-25,25,50,\'Y\'))\n                elif k == \'speedZ\':\n                    strout= \'%6.1f %s\' % (self.d[k], bargraph(self.d[k],-13,13,50,\'Z\'))\n                elif k == \'z\':\n                    strout= \'%6.3f %s\' % (self.d[k], bargraph(self.d[k],.3,.5,50,\'z\'))\n                elif k == \'trackPos\': # This gets reversed for display to make sense.\n                    cx=\'<\'\n                    if self.d[k]<0: cx= \'>\'\n                    strout= \'%6.3f %s\' % (self.d[k], bargraph(self.d[k]*-1,-1,1,50,cx))\n                elif k == \'stucktimer\':\n                    if self.d[k]:\n                        strout= \'%3d %s\' % (self.d[k], bargraph(self.d[k],0,300,50,""\'""))\n                    else: strout= \'Not stuck!\'\n                elif k == \'rpm\':\n                    g= self.d[\'gear\']\n                    if g < 0:\n                        g= \'R\'\n                    else:\n                        g= \'%1d\'% g\n                    strout= bargraph(self.d[k],0,10000,50,g)\n                elif k == \'angle\':\n                    asyms= [\n                          ""  !  "", "".|\'  "", ""./\'  "", ""_.-  "", "".--  "", ""..-  "",\n                          ""---  "", "".__  "", ""-._  "", ""\'-.  "", ""\'\\.  "", ""\'|.  "",\n                          ""  |  "", ""  .|\'"", ""  ./\'"", ""  .-\'"", ""  _.-"", ""  __."",\n                          ""  ---"", ""  --."", ""  -._"", ""  -.."", ""  \'\\."", ""  \'|.""  ]\n                    rad= self.d[k]\n                    deg= int(rad*180/PI)\n                    symno= int(.5+ (rad+PI) / (PI/12) )\n                    symno= symno % (len(asyms)-1)\n                    strout= \'%5.2f %3d (%s)\' % (rad,deg,asyms[symno])\n                elif k == \'skid\': # A sensible interpretation of wheel spin.\n                    frontwheelradpersec= self.d[\'wheelSpinVel\'][0]\n                    skid= 0\n                    if frontwheelradpersec:\n                        skid= .5555555555*self.d[\'speedX\']/frontwheelradpersec - .66124\n                    strout= bargraph(skid,-.05,.4,50,\'*\')\n                elif k == \'slip\': # A sensible interpretation of wheel spin.\n                    frontwheelradpersec= self.d[\'wheelSpinVel\'][0]\n                    slip= 0\n                    if frontwheelradpersec:\n                        slip= ((self.d[\'wheelSpinVel\'][2]+self.d[\'wheelSpinVel\'][3]) -\n                              (self.d[\'wheelSpinVel\'][0]+self.d[\'wheelSpinVel\'][1]))\n                    strout= bargraph(slip,-5,150,50,\'@\')\n                else:\n                    strout= str(self.d[k])\n            out+= ""%s: %s\\n"" % (k,strout)\n        return out\n\nclass DriverAction():\n    \'\'\'What the driver is intending to do (i.e. send to the server).\n    Composes something like this for the server:\n    (accel 1)(brake 0)(gear 1)(steer 0)(clutch 0)(focus 0)(meta 0) or\n    (accel 1)(brake 0)(gear 1)(steer 0)(clutch 0)(focus -90 -45 0 45 90)(meta 0)\'\'\'\n    def __init__(self):\n       self.actionstr= str()\n       # ""d"" is for data dictionary.\n       self.d= { \'accel\':0.2,\n                   \'brake\':0,\n                  \'clutch\':0,\n                    \'gear\':1,\n                   \'steer\':0,\n                   \'focus\':[-90,-45,0,45,90],\n                    \'meta\':0\n                    }\n\n    def clip_to_limits(self):\n        """"""There pretty much is never a reason to send the server\n        something like (steer 9483.323). This comes up all the time\n        and it\'s probably just more sensible to always clip it than to\n        worry about when to. The ""clip"" command is still a snakeoil\n        utility function, but it should be used only for non standard\n        things or non obvious limits (limit the steering to the left,\n        for example). For normal limits, simply don\'t worry about it.""""""\n        self.d[\'steer\']= clip(self.d[\'steer\'], -1, 1)\n        self.d[\'brake\']= clip(self.d[\'brake\'], 0, 1)\n        self.d[\'accel\']= clip(self.d[\'accel\'], 0, 1)\n        self.d[\'clutch\']= clip(self.d[\'clutch\'], 0, 1)\n        if self.d[\'gear\'] not in [-1, 0, 1, 2, 3, 4, 5, 6]:\n            self.d[\'gear\']= 0\n        if self.d[\'meta\'] not in [0,1]:\n            self.d[\'meta\']= 0\n        if type(self.d[\'focus\']) is not list or min(self.d[\'focus\'])<-180 or max(self.d[\'focus\'])>180:\n            self.d[\'focus\']= 0\n\n    def __repr__(self):\n        self.clip_to_limits()\n        out= str()\n        for k in self.d:\n            out+= \'(\'+k+\' \'\n            v= self.d[k]\n            if not type(v) is list:\n                out+= \'%.3f\' % v\n            else:\n                out+= \' \'.join([str(x) for x in v])\n            out+= \')\'\n        return out\n        return out+\'\\n\'\n\n    def fancyout(self):\n        \'\'\'Specialty output for useful monitoring of bot\'s effectors.\'\'\'\n        out= str()\n        od= self.d.copy()\n        od.pop(\'gear\',\'\') # Not interesting.\n        od.pop(\'meta\',\'\') # Not interesting.\n        od.pop(\'focus\',\'\') # Not interesting. Yet.\n        for k in sorted(od):\n            if k == \'clutch\' or k == \'brake\' or k == \'accel\':\n                strout=\'\'\n                strout= \'%6.3f %s\' % (od[k], bargraph(od[k],0,1,50,k[0].upper()))\n            elif k == \'steer\': # Reverse the graph to make sense.\n                strout= \'%6.3f %s\' % (od[k], bargraph(od[k]*-1,-1,1,50,\'S\'))\n            else:\n                strout= str(od[k])\n            out+= ""%s: %s\\n"" % (k,strout)\n        return out\n\n# == Misc Utility Functions\ndef destringify(s):\n    \'\'\'makes a string into a value or a list of strings into a list of\n    values (if possible)\'\'\'\n    if not s: return s\n    if type(s) is str:\n        try:\n            return float(s)\n        except ValueError:\n            print(""Could not find a value in %s"" % s)\n            return s\n    elif type(s) is list:\n        if len(s) < 2:\n            return destringify(s[0])\n        else:\n            return [destringify(i) for i in s]\n\ndef drive_example(c):\n    \'\'\'This is only an example. It will get around the track but the\n    correct thing to do is write your own `drive()` function.\'\'\'\n    S,R= c.S.d,c.R.d\n    target_speed=100\n\n    # Steer To Corner\n    R[\'steer\']= S[\'angle\']*10 / PI\n    # Steer To Center\n    R[\'steer\']-= S[\'trackPos\']*.10\n\n    # Throttle Control\n    if S[\'speedX\'] < target_speed - (R[\'steer\']*50):\n        R[\'accel\']+= .01\n    else:\n        R[\'accel\']-= .01\n    if S[\'speedX\']<10:\n       R[\'accel\']+= 1/(S[\'speedX\']+.1)\n\n    # Traction Control System\n    if ((S[\'wheelSpinVel\'][2]+S[\'wheelSpinVel\'][3]) -\n       (S[\'wheelSpinVel\'][0]+S[\'wheelSpinVel\'][1]) > 5):\n       R[\'accel\']-= .2\n\n    # Automatic Transmission\n    R[\'gear\']=1\n    if S[\'speedX\']>50:\n        R[\'gear\']=2\n    if S[\'speedX\']>80:\n        R[\'gear\']=3\n    if S[\'speedX\']>110:\n        R[\'gear\']=4\n    if S[\'speedX\']>140:\n        R[\'gear\']=5\n    if S[\'speedX\']>170:\n        R[\'gear\']=6\n    return\n\n# ================ MAIN ================\nif __name__ == ""__main__"":\n    C = Client(relaunch_torcs=False, p=3101)\n    for step in range(C.maxSteps,0,-1):\n        C.get_servers_input()\n        drive_example(C)\n        C.respond_to_server()\n    C.shutdown()\n'"
