file_path,api_count,code
setup.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, absolute_import, print_function\n\nimport os\nimport sys\nfrom setuptools import find_packages\nfrom numpy.distutils.core import setup\nfrom numpy.distutils.misc_util import Configuration\n\n# Only way to get --noopt defaulted into the build environment\n# since f2py_options is not working below\nsys.argv[:] = sys.argv[:1] + [\'config_fc\', \'--noopt\'] + sys.argv[1:]\n\nrootpath = os.path.abspath(os.path.dirname(__file__))\n\n\ndef read(*parts):\n    return open(os.path.join(rootpath, *parts), \'r\').read()\n\n\nwith open(""README.md"", ""r"") as f:\n    long_description = f.read()\n\n# Dependencies.\nwith open(\'requirements.txt\') as f:\n    tests_require = f.readlines()\ninstall_requires = [t.strip() for t in tests_require]\n\npackage_data = {\n    \'\': [\'constituents.npz\',\n         \'hawaii_coast/*\',\n         \'roms/cdl/*.cdl\',\n         \'roms/cobalt/*.cdl\']\n}\n\nconfig = Configuration(\'\')\nflags = [] if os.name == \'nt\' else [\'-fPIC\']\n# ifort generated libraries produce invalid results in interpolation (NOT\n# OBVIOUS)\nconfig.add_extension(\'oalib\', sources=\'src/oalib.f\',\n                     # f2py_options=[""noopt""],\n                     extra_f77_compile_args=flags)\nconfig.add_extension(\'hindices\', sources=\'src/hindices.f\',\n                     # f2py_options=[""noopt""],\n                     extra_f77_compile_args=flags)\n\nconfig = dict(\n    name=os.getenv(\'PACKAGE_NAME\', \'seapy\'),\n    version=\'0.5\',\n    license=\'MIT\',\n    description=\'State Estimation and Analysis in PYthon\',\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    author=\'Brian Powell\',\n    author_email=\'powellb@hawaii.edu\',\n    url=\'https://github.com/powellb/seapy\',\n    classifiers=[\n        \'Programming Language :: Python :: 3.6\',\n        \'License :: OSI Approved :: MIT License\',\n    ],\n    packages=find_packages(),\n    package_data=package_data,\n    ext_package=\'seapy.external\',\n    scripts=[\'bin/convert_clim.py\', \'bin/convert_frc.py\'],\n    install_requires=install_requires,\n    zip_safe=False,\n    **config.todict()\n)\n\n\nsetup(**config)\n'"
bin/convert_clim.py,1,"b'#!/usr/bin/env python\n""""""\nSimple script to convert older-style climatology files with\nmultiple time dimensions to a new, single, unlimited time dimension\nfor all fields.\n""""""\nimport sys\nimport seapy\nimport numpy as np\n\ntry:\n    infile = sys.argv[1]\n    outfile = sys.argv[2]\nexcept:\n    print(""Usage: {:s} input_file output_file"".format(sys.argv[0]))\n    sys.exit()\n\nprint(""Convert {:s} to {:s}"".format(infile, outfile))\nmaxrecs = 30\n\n# Get the parameters\ninc = seapy.netcdf(infile)\neta_rho = len(inc.dimensions[\'eta_rho\'])\nxi_rho = len(inc.dimensions[\'xi_rho\'])\ns_rho = len(inc.dimensions[\'s_rho\'])\nepoch, tvar = seapy.roms.get_reftime(inc)\n\n# Create the new file\nonc = seapy.roms.ncgen.create_clim(\n    outfile, eta_rho=eta_rho, xi_rho=xi_rho, s_rho=s_rho, reftime=epoch, clobber=True)\n\n# Save the times\nonc.variables[\'clim_time\'][:] = inc.variables[tvar][:]\nntimes = len(onc.dimensions[\'clim_time\'])\n\n# Copy the variables\nfor v in seapy.roms.fields:\n    print(""{:s}, "".format(v), end=\'\', flush=True)\n    for l in seapy.chunker(np.arange(ntimes), maxrecs):\n        onc.variables[v][l, :] = inc.variables[v][l, :]\nprint(\'done.\')\nonc.close()\ninc.close()\n'"
bin/convert_frc.py,1,"b'#!/usr/bin/env python\n""""""\nSimple script to convert older-style bulk forcing files with\nmultiple time dimensions to a new, single, unlimited time dimension\nfor all fields.\n""""""\nimport sys\nimport seapy\nimport numpy as np\n\ntry:\n    infile = sys.argv[1]\n    outfile = sys.argv[2]\nexcept:\n    print(""Usage: {:s} input_file output_file"".format(sys.argv[0]))\n    sys.exit()\n\nprint(""Convert {:s} to {:s}"".format(infile, outfile))\nmaxrecs = 30\n\n# Get the parameters\ninc = seapy.netcdf(infile)\nlat = len(inc.dimensions[\'lat\'])\nlon = len(inc.dimensions[\'lon\'])\nepoch, tvar = seapy.roms.get_reftime(inc)\n\n# Create the new file\nonc = seapy.roms.ncgen.create_frc_bulk(\n    outfile, lat=lat, lon=lon, reftime=epoch, clobber=True)\n\n# Save the times\nonc.variables[\'time\'][:] = inc.variables[tvar][:]\nntimes = len(onc.dimensions[\'time\'])\nonc.variables[\'lat\'][:] = inc.variables[\'lat\'][:]\nonc.variables[\'lon\'][:] = inc.variables[\'lon\'][:]\n\n# Copy the variables\nfor v in seapy.roms.forcing.fields:\n    print(""{:s}, "".format(v), end=\'\', flush=True)\n    for l in seapy.chunker(np.arange(ntimes), maxrecs):\n        onc.variables[v][l, :] = inc.variables[v][l, :]\nprint(\'done.\')\nonc.close()\ninc.close()\n'"
docs/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# SeaPY documentation build configuration file, created by\n# sphinx-quickstart on Fri Feb  6 11:03:18 2015.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#sys.path.insert(0, os.path.abspath(\'.\'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.autosummary\',\n    \'numpydoc\',\n]\n\nnumpydoc_show_class_members = False\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix of source filenames.\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'seapy\'\ncopyright = \'2020, University of Hawaii, MIT-License\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'1.0a1\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'1.0a1\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [\'_build\']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n#keep_warnings = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \'default\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#html_extra_path = []\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'SeaPYdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    # \'preamble\': \'\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (\'index\', \'SeaPY.tex\', \'SeaPY Documentation\',\n     \'Brian\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (\'index\', \'seapy\', \'seapy Documentation\',\n     [\'Powell Lab\'], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\'index\', \'SeaPY\', \'seapy Documentation\',\n     \'Powell Lab\', \'SeaPY\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#texinfo_no_detailmenu = False\n\n\n# -- Options for Epub output ----------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = \'seapy\'\nepub_author = \'Powell Lab\'\nepub_publisher = \'Powell Lab\'\nepub_copyright = \'2020, Powell Lab\'\n\n# The basename for the epub file. It defaults to the project name.\n#epub_basename = \'seapy\'\n\n# The HTML theme for the epub output. Since the default themes are not optimized\n# for small screen space, using the same theme for HTML and epub output is\n# usually not wise. This defaults to \'epub\', a theme designed to save visual\n# space.\n#epub_theme = \'epub\'\n\n# The language of the text. It defaults to the language option\n# or en if the language is not set.\n#epub_language = \'\'\n\n# The scheme of the identifier. Typical schemes are ISBN or URL.\n#epub_scheme = \'\'\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#epub_identifier = \'\'\n\n# A unique identification for the text.\n#epub_uid = \'\'\n\n# A tuple containing the cover image and cover page html template filenames.\n#epub_cover = ()\n\n# A sequence of (type, uri, title) tuples for the guide element of content.opf.\n#epub_guide = ()\n\n# HTML files that should be inserted before the pages created by sphinx.\n# The format is a list of tuples containing the path and title.\n#epub_pre_files = []\n\n# HTML files shat should be inserted after the pages created by sphinx.\n# The format is a list of tuples containing the path and title.\n#epub_post_files = []\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n# The depth of the table of contents in toc.ncx.\n#epub_tocdepth = 3\n\n# Allow duplicate toc entries.\n#epub_tocdup = True\n\n# Choose between \'default\' and \'includehidden\'.\n#epub_tocscope = \'default\'\n\n# Fix unsupported image types using the PIL.\n#epub_fix_images = False\n\n# Scale large images.\n#epub_max_image_width = 0\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#epub_show_urls = \'inline\'\n\n# If false, no index is generated.\n#epub_use_index = True\n'"
seapy/__init__.py,0,"b'""""""\n  __init__.py\n\n  State Estimation and Analysis for PYthon\n\n    Module for working with oceanographic data and models\n\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n\n  Requires the following packages: joblib\n\n  Import classes include:\n\n  - :class:`~seapy.environ.opt`\n  - :class:`~seapy.progressbar.ProgressBar`\n  - :class:`~seapy.tidal_energy.energetics`\n\n  Imported functions include:\n\n  - :func:`~seapy.lib.adddim`\n  - :func:`~seapy.lib.chunker`\n  - :func:`~seapy.lib.convolve_mask`\n  - :func:`~seapy.lib.day2date`\n  - :func:`~seapy.lib.date2day`\n  - :func:`~seapy.lib.earth_angle`\n  - :func:`~seapy.lib.earth_distance`\n  - :func:`~seapy.lib.flatten`\n  - :func:`~seapy.lib.list_files`\n  - :func:`~seapy.lib.netcdf`\n  - :func:`~seapy.lib.rotate`\n  - :func:`~seapy.lib.today2day`\n  - :func:`~seapy.lib.unique_rows`\n  - :func:`~seapy.lib.vecfind`\n  - :func:`~seapy.oa.oasurf`\n  - :func:`~seapy.oa.oavol`\n  - :func:`~seapy.tidal_energy.tidal_energy`\n  - :func:`~seapy.progressbar.progress`\n\n""""""\n\nfrom .lib import *\nfrom . import roms\nfrom . import model\nfrom . import qserver\nfrom . import mapping\nfrom . import filt\nfrom . import plot\nfrom . import progressbar\nfrom . import seawater\nfrom . import tide\nfrom .tidal_energy import tidal_energy\nfrom .environ import opt\nfrom .hawaii import hawaii\nfrom .oa import *\n'"
seapy/cdl_parser.py,0,"b'# !/usr/bin/env python\n""""""\n  Module to load and parse Common Data Language (CDL) files and\n  tokenize the dimensions and variables\n\n  Written by Brian Powell on 04/30/13\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\n\n\nimport re\n\n\ndef cdl_parser(filename):\n    """"""\n    Given a netcdf-compliant CDL file, parse it to determine the structure:\n    dimensions, variables, attributes, and global attributes\n\n    Parameters\n    ----------\n    filename : string\n        name and path of CDL file to parse\n\n    Returns\n    -------\n    dims, vars, attr: dict\n        dictionaries description dimensions, variables, and attributes\n\n    """"""\n    dim_pat = re.compile(r""\\s*(\\w+)\\s*=\\s*(\\w*)\\s*;"")\n    var_pat = re.compile(r""\\s*(\\w+)\\s*(\\w+)\\({0,1}([\\w\\s,]*)\\){0,1}\\s*;"")\n    attr_pat = re.compile(r""\\s*(\\w+):(\\w+)\\s*=\\s*\\""*([^\\""]*)\\""*\\s*;"")\n    global_attr_pat = re.compile(r""\\s*:(\\w+)\\s*=\\s*\\""*([^\\""]*)\\""*\\s*;"")\n    dims = dict()\n    attr = dict()\n    vars = list()\n    vcount = dict()\n    types = {""float"": ""f4"", ""double"": ""f8"",\n             ""short"": ""i2"", ""int"": ""i4"", ""char"": ""S1""}\n\n    for line in open(filename, \'r\'):\n        # Check if this is a dimension definition line. If it is, add\n        # the dimension to the definition\n        parser = dim_pat.match(line)\n        if parser is not None:\n            tokens = parser.groups()\n            if tokens[1].upper() == ""UNLIMITED"":\n                dims[tokens[0]] = 0\n            else:\n                dims[tokens[0]] = int(tokens[1])\n            continue\n\n        # Check if this is a variable definition line. If it is, add\n        # the variable to the definition\n        parser = var_pat.match(line)\n        if parser is not None:\n            tokens = parser.groups()\n            nvar = {""name"": tokens[1],\n                    ""type"": types[tokens[0]],\n                    ""dims"": tokens[2].strip().split("", "")}\n            vars.append(nvar)\n            vcount[tokens[1]] = len(vars) - 1\n            continue\n\n        # If this is an attribute, add the info to the appropriate variable\n        parser = attr_pat.match(line)\n        if parser is not None:\n            tokens = parser.groups()\n            if ""attr"" not in vars[vcount[tokens[0]]]:\n                vars[vcount[tokens[0]]][""attr""] = dict()\n            vars[vcount[tokens[0]]][""attr""][tokens[1]] = tokens[2]\n            continue\n\n        # If this is a global attribute, add the info\n        parser = global_attr_pat.match(line)\n        if parser is not None:\n            tokens = parser.groups()\n            attr[tokens[0]] = tokens[1]\n            continue\n\n    return dims, vars, attr\n\n\nif __name__ == ""__main__"":\n    cdl_parser(""out.cdl"")\n'"
seapy/debug.py,0,"b'#!/usr/bin/env python\n\nimport seapy\nimport ipdb\n\n# Call the code we want to debug here... Make sure to insert:\n# ipdb.set_trace()\n# where we wish to begin our debugging trace\nipdb.set_trace()\nt = seapy.roms.obsgen.modis_sst_map(""grid.nc"", 5)\n'"
seapy/environ.py,0,"b'#!/usr/bin/env python\n""""""\n  Module to store all environment variables in an easy dictionary\n\n  Written by Brian Powell on 04/26/13\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\n\n\nimport os\n\n\nclass options(object):\n    pass\n\n\nopt = options()\nfor key in os.environ.keys():\n    setattr(opt, key, os.environ[key])\n'"
seapy/filt.py,17,"b'#!/usr/bin/env python\n""""""\n  filt.py\n\n  Functions for quickly filtering time-series of data.\n\n\n  Written by Brian Powell on 02/09/16\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\nimport numpy as np\nimport seapy\nimport scipy.signal\n\n\ndef average_err(x, window=5):\n    """"""\n    Generate a moving (boxcar) average and variance of the time-series, x, using\n    the specified window size.\n\n    Parameters\n    ----------\n    x: ndarray,\n        The time-series of data to bandpass filter.\n    cutoff: float,\n        The period at which the bandpass filter will apply the cutoff.\n        Units are same as the time-step of the signal provided (e.g., if the\n        data are provided every hour, then a cutoff=12 would be a 12 hour\n        cutoff.)\n\n    Returns\n    -------\n    x, variance: ndarray, ndarray\n        Returns the moving average of x with the moving variance of the\n        average window\n\n    Examples\n    --------\n    Create data every 30 minutes for 3 days with time in days:\n\n      >>>  t = np.linspace(0, 3.0, 2 * 24 * 3.0, endpoint=False)\n      >>>  x = 0.1 * np.sin(2 * np.pi / .008 * t)\n      >>>  x += 0.2 * np.cos(2 * np.pi / 0.6 * t + 0.1)\n      >>>  x += 0.2 * np.cos(2 * np.pi / 1.6 * t + .11)\n      >>>  x += 1 * np.cos(2 * np.pi / 10 * t + 11)\n\n    Average the data over 6 hour period\n\n      >>>  nx, err = average_err(x, window=12)\n      >>>  plt.plot(t, x, \'k\', t, nx, \'r\', label=[\'Raw\', \'Average\'])\n      >>>  plt.figure()\n      >>>  plt.plot(t, err, \'g\', label=\'Variance\')\n\n    """"""\n    x = np.atleast_1d(x).flatten()\n    nx = np.ma.masked_all(x.shape)\n    err = np.ma.masked_all(x.shape)\n    filt = np.ones(window) / window\n    padlen = window * 4\n\n    # Go over all contiguous regions\n    regions = seapy.contiguous(x)\n    for r in regions:\n        if ((r.stop - r.start) >= padlen):\n            nx[r] = scipy.signal.filtfilt(\n                filt, [1.0], x[r], padlen=padlen, axis=0)\n            err[r] = scipy.signal.filtfilt(\n                filt, [1.0], (nx[r] - x[r])**2, padlen=padlen, axis=0)\n\n    return nx, err\n\n\ndef bandpass(x, dt, low_cutoff=None, hi_cutoff=None, order=7):\n    """"""\n    Perform a bandpass filter at the cutoff period (same units as the\n    time-series step).\n\n    Parameters\n    ----------\n    x  : ndarray,\n        The time-series of data to bandpass filter.\n    dt : float,\n        The time-step between the values in x. Units must be consistent\n        with the cutoff period.\n    low_cutoff: float,\n        The period at which the bandpass filter will apply the lowpass filter.\n        Units are same as the time-step of the signal provided (e.g., if the\n        data are provided every hour, then a cutoff=12 would be a 12 hour\n        cutoff.) Everything that has a longer period will remain. If you only\n        want a hi-pass filter, this value should be None.\n    hi_cutoff: float,\n        The period at which the bandpass filter will apply the high-pass filter.\n        Units are same as the time-step of the signal provided (e.g., if the\n        data are provided every hour, then a cutoff=12 would be a 12 hour\n        cutoff.) Everything that has a shorter period will remain. If you only\n        want a low-pass filter, this value should be None.\n    order: int optional,\n        The order of the filter to apply\n\n    Returns\n    -------\n    x : ndarray\n        The bandpass filtered time-series\n\n    Examples\n    --------\n    Create data every 30 minutes for 3 days with time in days:\n\n      >>>  t = np.linspace(0, 3.0, 2 * 24 * 3.0, endpoint=False)\n      >>>  x = 0.1 * np.sin(2 * np.pi / .008 * t)\n      >>>  x += 0.2 * np.cos(2 * np.pi / 0.6 * t + 0.1)\n      >>>  x += 0.2 * np.cos(2 * np.pi / 1.6 * t + .11)\n      >>>  x += 1 * np.cos(2 * np.pi / 10 * t + 11)\n\n    Filter the data to low-pass everything longer than the 1 day period\n\n      >>>  nx = bandpass(x, dt=0.5, low_cutoff=24 )\n      >>>  plt.plot(t, x, \'k\', t, nx, \'r\', label=[\'Raw\', \'Filter\'])\n\n    Filter the data to low-pass everything longer the 2 day period\n\n      >>>  nx = bandpass(x, dt=0.5, low_cutoff=48 )\n      >>>  plt.plot(t, x, \'k\', t, nx, \'r\', label=[\'Raw\', \'Filter\'])\n\n    Filter the data to band-pass everything shorter the 2 day period\n    and longer the 1 hour period\n\n      >>>  nx = bandpass(x, dt=0.5, low_cutoff=48, hi_cutoff=1 )\n      >>>  plt.plot(t, x, \'k\', t, nx, \'r\', label=[\'Raw\', \'Filter\'])\n    """"""\n    x = np.ma.array(np.atleast_1d(x).flatten(), copy=False)\n    nx = np.ma.masked_all(x.shape)\n\n    if low_cutoff and hi_cutoff:\n        freq = 2.0 * dt / np.array([hi_cutoff, low_cutoff])\n        btype = \'bandpass\'\n    elif low_cutoff:\n        freq = 2.0 * dt / low_cutoff\n        btype = \'lowpass\'\n    elif hi_cutoff:\n        freq = 2.0 * dt / hi_cutoff\n        btype = \'highpass\'\n    else:\n        raise AttributeError(""You must specify either low or hi cutoff."")\n    b, a = scipy.signal.butter(order, freq, btype=btype)\n    padlen = max(len(a), len(b))\n\n    # Go over all contiguous regions\n    regions = seapy.contiguous(x)\n    for r in regions:\n        if ((r.stop - r.start) >= padlen):\n            nx[r] = scipy.signal.filtfilt(\n                b, a, x[r], padlen=5 * padlen, axis=0)\n    return nx\n'"
seapy/hawaii.py,0,"b'#!/usr/bin/env python\n""""""\n  hawaii.py\n\n  State Estimation and Analysis for PYthon\n\n  Utilities for dealing with data around Hawaii\n\n    Examples\n    --------\n\n    Assume you have longitude, latitude, and sst values:\n\n    >>> m=seapy.hawaii()\n    >>> m.pcolormesh(lon,lat,sst,vmin=22,vmax=26,cmap=plt.cm.bwr)\n    >>> m.land()\n    >>> m.colorbar(label=""Sea Surface Temp [$^\\circ$C]"",cticks=[22,23,24,25,26])\n    >>> m.ax.patch.set_facecolor(""aqua"")\n    >>> m.ax.patch.set_alpha(1)\n    >>> m.fig.patch.set_alpha(0.0)\n    >>> m.fig.savefig(""sst.png"",dpi=100)\n\n  Written by Brian Powell on 9/4/14\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\n\n\nfrom .mapping import map\nfrom matplotlib.patches import Polygon\nfrom matplotlib.collections import PolyCollection\nimport os\n\n_shape_file = os.path.dirname(__file__) + ""/hawaii_coast/hawaii""\n\n\nclass hawaii(map):\n\n    def __init__(self, grid=None, llcrnrlon=-163, llcrnrlat=17, urcrnrlon=-153,\n                 urcrnrlat=24, figsize=(8., 6.), dlat=1, dlon=2, fig=None, ax=None,\n                 fill_color=""aqua""):\n        super().__init__(grid=grid, llcrnrlon=llcrnrlon, llcrnrlat=llcrnrlat,\n                         urcrnrlon=urcrnrlon, urcrnrlat=urcrnrlat,\n                         figsize=figsize, dlat=dlat, dlon=dlon, fig=fig, ax=ax,\n                         fill_color=fill_color)\n\n    def land(self, color=""black""):\n        """"""\n        Draw the GIS coastline data from the state of Hawaii to draw the\n        land boundaries. This does not include rivers, etc., only the\n        coastline.\n\n        Parameters\n        ----------\n        color: string, optional\n            Color to draw the land mask with\n\n        Returns\n        -------\n        None\n\n        """"""\n\n        if hasattr(self.basemap, ""coast"") == False or hasattr(self, ""landpoly""):\n            self.basemap.readshapefile(_shape_file, ""coast"")\n            vert = []\n            for shape in self.basemap.coast:\n                vert.append(shape)\n\n            self.landpoly = PolyCollection(\n                vert, facecolors=color, edgecolors=color)\n        # Draw the loaded shapes\n        self.ax.add_collection(self.landpoly)\n'"
seapy/lib.py,115,"b'#!/usr/bin/env python\n""""""\n  lib.py\n\n  State Estimation and Analysis for PYthon\n\n  Library of utilities for general seapy module, imported into the namespace\n  when importing the seapy module\n\n  Written by Brian Powell on 10/18/13\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\n\n\nimport numpy as np\nfrom scipy import ndimage\nimport os\nimport re\nimport datetime\nimport itertools\n\n\nsecs2day = 1.0 / 86400.0\ndefault_epoch = datetime.datetime(2000, 1, 1)\n_default_timeref = ""days since "" + default_epoch.strftime(""%Y-%m-%d %H:%M:%S"")\n\n\ndef adddim(fld, size=1):\n    """"""\n    replicate a field and add a new first dimension with given size\n\n    Parameters\n    ----------\n    fld : array_like\n        Input field.\n    size : int, optional\n        Size of additional first dimension\n\n    Returns\n    -------\n    fld : array\n\n    Examples\n    --------\n    >>> a=np.array([4, 5, 6, 7])\n    >>> a.shape\n    (4,)\n    >>> b = seapy.adddim(a, 2)\n    >>> b.shape\n    (2, 4)\n    >>> b\n    array([[4, 5, 6, 7],\n           [4, 5, 6, 7]])\n\n    """"""\n    fld = np.atleast_1d(fld)\n    s = np.ones(fld.ndim + 1).astype(int)\n    s[0] = int(size)\n    return np.tile(fld, s)\n\n\ndef fill(x, max_gap=None, kind=\'linear\'):\n    """"""\n    Fill missing data from a 1-D vector. When data are missing from a\n    vector, this method will interpolate to fill gaps that are less than\n    the specified max (or ignored).\n\n    Parameters\n    ----------\n    x : array\n      The array to be filled. It will be cast as a masked array for\n      invalid values. If already a masked array, then that mask will\n      persist.\n    max_gap : int, optional\n      The maximum number of continuous values to interpolate (e.g.,\n      if this value is 10 and there are 12 continuous missing values,\n      they will be left unfilled). Default is to fill everything.\n    kind : str, optional\n      The kind of interpolant to use (see scipy.interpolate.interp1d).\n      Default is \'linear\'\n\n    Returns\n    -------\n    x : array\n      The filled array\n    """"""\n    from scipy.interpolate import interp1d\n    x = np.ma.masked_invalid(np.atleast_1d(x).flatten(), copy=False)\n    # If no gaps or empty data, do nothing\n    if not np.any(x.mask) or len(x.compressed()) < 3:\n        return x\n    f = interp1d(x.nonzero()[0], x.compressed())\n    nx = x.copy()\n    if max_gap is not None:\n        regions = contiguous(x)\n        for r in regions:\n            if ((r.stop - r.start) <= max_gap) and \\\n                    (r.stop < f.x.max()) and (r.start > f.x.min()):\n                nx[r] = f(np.arange(r.start, r.stop))\n    else:\n        bad = np.nonzero(x.mask)[0]\n        bad = np.delete(bad, np.nonzero(\n            np.logical_or(bad <= f.x.min(), bad >= f.x.max())))\n        nx[bad] = f(bad)\n    return nx\n\n\ndef contiguous(x):\n    """"""\n    Find the indices that provide contiguous regions of a numpy.masked_array.\n    This will find all regions of valid data. NOTE: this casts as 1-D.\n\n    Parameters\n    ----------\n    x : np.array or np.ma.array\n      The data to find the contiguous regions\n\n    Returns\n    -------\n    idx : array of slices\n      Array of slices for each contiguous region\n\n    Examples\n    --------\n    >>> a = np.array([4, 3, 2, np.nan, 6, 7, 2])\n    >>> r = contiguous(a)\n    [slice(0, 2, None), slice(4, 6, None)]\n\n    If no contiguous regions are available, an empty array is returned.\n\n    """"""\n    x = np.ma.masked_invalid(np.atleast_1d(x).flatten(), copy=False)\n    idx = x.nonzero()[0]\n    try:\n        d = idx[np.nonzero(np.diff(idx) - 1)[0] + 1]\n        return np.array([np.s_[r[0]:r[1]] for r in\n                         zip(np.hstack((idx.min(), d)),\n                             np.hstack((d - 1, idx.max() + 1)))])\n    except:\n        return []\n\n\ndef chunker(seq, size):\n    """"""\n    Iterate over an iterable in \'chunks\' of a given size\n\n    Parameters\n    ----------\n    seq : iterable,\n        The sequence to iterate over\n    size : int,\n        The number of items to be returned in each \'chunk\'\n\n    Returns\n    -------\n    chunk : seq,\n        The items of the chunk to be iterated\n\n    Examples\n    --------\n    >>> x = [0,3,4,7,9,10,12,14]\n    >>> for i in chunker(x, 3):\n    >>>     print(i)\n    [0, 3, 4]\n    [7, 9, 10]\n    [12, 14]\n\n    """"""\n    return (seq[pos:pos + size] for pos in range(0, len(seq), max(1, size)))\n\n\ndef smooth(data, ksize=3, kernel=None, copy=True):\n    """"""\n    Smooth the data field using a specified convolution kernel\n    or a default averaging kernel.\n\n    Parameters\n    ----------\n    data : masked array_like\n        Input field.\n    ksize : int, optional\n        Size of square kernel\n    kernel : ndarray, optional\n        Define a convolution kernel. Default is averaging\n    copy : bool, optional\n        If true, a copy of input array is made\n\n    Returns\n    -------\n    fld : masked array\n    """"""\n    fld = np.ma.array(data, copy=copy)\n    mask = np.ma.getmaskarray(fld).copy()\n\n    # Make sure ksize is odd\n    ksize = int(ksize + 1) if int(ksize) % 2 == 0 else int(ksize)\n    if fld.ndim > 3 or fld.ndim < 2:\n        raise AttributeError(""Can only convolve 2- or 3-D fields"")\n    if ksize < 3:\n        raise ValueError(""ksize must be greater than or equal to 3"")\n\n    if kernel is None:\n        kernel = np.ones((ksize, ksize)) / (ksize * ksize)\n    else:\n        ksize = kernel.shape[0]\n\n    # First, convole over any masked values\n    fld = convolve_mask(fld, ksize=ksize, copy=False)\n\n    # Next, perform the convolution\n    if fld.ndim == 2:\n        fld = ndimage.convolve(fld.data, kernel,\n                               mode=""reflect"", cval=0.0)\n    else:\n        kernel = np.expand_dims(kernel, axis=3)\n        fld = np.transpose(ndimage.convolve(\n            fld.filled(0).transpose(1, 2, 0), kernel,\n            mode=""reflect"", cval=0.0), (2, 0, 1))\n\n    # Apply the initial mask\n    return np.ma.array(fld, mask=mask)\n\n\ndef convolve(data, ksize=3, kernel=None, copy=True, only_mask=False):\n    """"""\n    Convolve the kernel across the data to smooth or highlight\n    the field across the masked region.\n\n    Parameters\n    ----------\n    data : masked array_like\n        Input field.\n    ksize : int, optional\n        Size of square kernel\n    kernel : ndarray, optional\n        Define a convolution kernel. Default is averaging\n    copy : bool, optional\n        If true, a copy of input array is made\n    only_mask : bool, optional\n        If true, only consider the smoothing over the masked\n        region\n\n    Returns\n    -------\n    fld : masked array\n    """"""\n    fld = np.ma.array(data, copy=copy)\n    if not copy:\n        fld._sharedmask = False\n\n    # Make sure ksize is odd\n    ksize = int(ksize + 1) if int(ksize) % 2 == 0 else int(ksize)\n    if fld.ndim > 3 or fld.ndim < 2:\n        raise AttributeError(""Can only convolve 2- or 3-D fields"")\n    if ksize < 3:\n        raise ValueError(""ksize must be greater than or equal to 3"")\n\n    if kernel is None:\n        center = np.round(ksize / 2).astype(int)\n        kernel = np.ones([ksize, ksize])\n        kernel[center, center] = 0.0\n    else:\n        ksize = kernel.shape[0]\n\n    # Convolve the mask\n    msk = np.ma.getmaskarray(fld)\n    if fld.ndim == 2:\n        count = ndimage.convolve((~msk).view(np.int8), kernel,\n                                 mode=""constant"", cval=0.0)\n        nfld = ndimage.convolve(fld.data * (~msk).view(np.int8), kernel,\n                                mode=""constant"", cval=0.0)\n    else:\n        kernel = np.expand_dims(kernel, axis=3)\n        count = np.transpose(ndimage.convolve(\n            (~msk).view(np.int8).transpose(1, 2, 0), kernel,\n            mode=""constant"", cval=0.0), (2, 0, 1))\n        nfld = np.transpose(ndimage.convolve(\n            (fld.data * (~msk).view(np.int8)).transpose(1, 2, 0), kernel,\n            mode=""constant"", cval=0.0), (2, 0, 1))\n\n    if only_mask:\n        lst = np.nonzero(np.logical_and(msk, count > 0))\n        fld[lst] = np.ma.nomask\n        fld[lst] = nfld[lst] / count[lst]\n    else:\n        lst = np.nonzero(~msk)\n        fld[lst] = nfld[lst] / count[lst]\n    return fld\n\n\ndef convolve_mask(data, ksize=3, kernel=None, copy=True):\n    """"""\n    Convolve data over the missing regions of a mask\n\n    Parameters\n    ----------\n    data : masked array_like\n        Input field.\n    ksize : int, optional\n        Size of square kernel\n    kernel : ndarray, optional\n        Define a convolution kernel. Default is averaging\n    copy : bool, optional\n        If true, a copy of input array is made\n\n    Returns\n    -------\n    fld : masked array\n    """"""\n    return convolve(data, ksize, kernel, copy, True)\n\n\ndef matlab2date(daynum):\n    """"""\n    Given a day number from matlab, convert into a datetime\n\n    Parameters\n    ----------\n    daynum: float\n      Scalar or array of matlab day numbers\n\n    Returns\n    -------\n    datetime : list\n    """"""\n    daynum = np.atleast_1d(daynum)\n    return [datetime.datetime.fromordinal(d.astype(np.int)) +\n            datetime.timedelta(days=(d % 1 - 366)) for d in daynum]\n\n\ndef date2day(date=default_epoch, epoch=default_epoch):\n    """"""\n    Compute the fractional number of days elapsed since the epoch to the date\n    given.\n\n    Parameters\n    ----------\n    date : datetime\n        Input date\n    epoch : datetime\n        Date of epoch\n\n    Returns\n    -------\n    numdays : list\n    """"""\n    date = np.atleast_1d(date)\n    return [(t - epoch).total_seconds() * secs2day for t in date]\n\n\ndef day2date(day=0, epoch=default_epoch):\n    """"""\n    Return a datetime object from the number of days since the epoch\n\n    Parameters\n    ----------\n    day : scalar\n        Input day number\n    epoch : datetime\n        Date of epoch\n\n    Returns\n    -------\n    date : list of datetime(s)\n    """"""\n    day = np.atleast_1d(day)\n    return [epoch + datetime.timedelta(days=float(t)) for t in day]\n\n\ndef matlab2date(daynum=0):\n    """"""\n    Return a datetime object from a Matlab datenum value\n\n    Parameters\n    ----------\n    daynum : scalar\n        Input Matlab day number\n\n    Returns\n    -------\n    date : list of datetime(s)\n    """"""\n    daynum = np.atleast_1d(daynum)\n\n    return np.array([datetime.datetime.fromordinal(int(d)) +\n                     datetime.timedelta(days=d % 1) -\n                     datetime.timedelta(days=366) for d in daynum])\n\n\ndef _distq(lon1, lat1, lon2, lat2):\n    """"""\n    Compute the geodesic distance between lat/lon points. This code is\n    taken from the dist.f routine and the Matlab version distg.m passed\n    around WHOI and APL. This was stripped down to use the WGS84 ellipsoid.\n\n    Parameters\n    ----------\n    lon1 : array_like or scalar\n        Input array of source longitude(s)\n    lat1 : array_like or scalar\n        Input array of source latitude(s)\n    lon2 : array_like or scalar\n        Input array of destination longitude(s)\n    lat2 : array_like or scalar\n        Input array of destination latitude(s)\n\n    Returns\n    -------\n    distance : array or scalar of distance in meters\n    angle: array or scalar of angle in radians\n\n    """"""\n    lon1 = np.asanyarray(np.radians(lon1))\n    lat1 = np.asanyarray(np.radians(lat1))\n    lon2 = np.asanyarray(np.radians(lon2))\n    lat2 = np.asanyarray(np.radians(lat2))\n\n    # # If one of the points is a singleton and the other is an\n    # array, make them the same size\n    if lon1.size == 1 and lon2.size > 1:\n        lon1 = lon1.repeat(lon2.size)\n        lat1 = lat1.repeat(lat2.size)\n    if lon2.size == 1 and lon1.size > 1:\n        lon2 = lon2.repeat(lon1.size)\n        lat2 = lat2.repeat(lat1.size)\n\n    # Set the WGS84 parameters\n    A = 6378137.\n    E = 0.081819191\n    B = np.sqrt(A * A - (A * E)**2)\n    EPS = E * E / (1.0 - E * E)\n\n    # Move any latitudes off of the equator\n    lat1[lat1 == 0] = np.finfo(float).eps\n    lat2[lat2 == 0] = -np.finfo(float).eps\n\n    # COMPUTE THE RADIUS OF CURVATURE IN THE PRIME VERTICAL FOR EACH POINT\n    xnu1 = A / np.sqrt(1.0 - (E * np.sin(lat1))**2)\n    xnu2 = A / np.sqrt(1.0 - (E * np.sin(lat2))**2)\n\n    TPSI2 = (1.0 - E * E) * np.tan(lat2) + E * E * xnu1 * np.sin(lat1) / \\\n        (xnu2 * np.cos(lat2))\n    PSI2 = np.arctan(TPSI2)\n\n    DPHI2 = lat2 - PSI2\n    DLAM = (lon2 - lon1) + np.finfo(float).eps\n    CTA12 = np.sin(DLAM) / (np.cos(lat1) * TPSI2 - np.sin(lat1) * np.cos(DLAM))\n    A12 = np.arctan(CTA12)\n    CTA21P = np.sin(DLAM) / (np.sin(PSI2) * np.cos(DLAM) -\n                             np.cos(PSI2) * np.tan(lat1))\n    A21P = np.arctan(CTA21P)\n\n    # C    GET THE QUADRANT RIGHT\n    DLAM2 = (np.abs(DLAM) < np.pi).astype(int) * DLAM + \\\n        (DLAM >= np.pi).astype(int) * (-2 * np.pi + DLAM) + \\\n        (DLAM <= -np.pi).astype(int) * (2 * np.pi + DLAM)\n    A12 = A12 + (A12 < -np.pi).astype(int) * 2 * np.pi - \\\n        (A12 >= np.pi).astype(int) * 2 * np.pi\n    A12 = A12 + np.pi * np.sign(-A12) * \\\n        (np.sign(A12).astype(int) != np.sign(DLAM2))\n    A21P = A21P + (A21P < -np.pi).astype(int) * 2 * np.pi - \\\n        (A21P >= np.pi).astype(int) * 2 * np.pi\n    A21P = A21P + np.pi * np.sign(-A21P) * \\\n        (np.sign(A21P).astype(int) != np.sign(-DLAM2))\n\n    SSIG = np.sin(DLAM) * np.cos(PSI2) / np.sin(A12)\n\n    dd1 = np.array([np.cos(lon1) * np.cos(lat1),\n                    np.sin(lon1) * np.cos(lat1), np.sin(lat1)])\n    dd2 = np.array([np.cos(lon2) * np.cos(lat2),\n                    np.sin(lon2) * np.cos(lat2), np.sin(lat2)])\n    dd2 = np.sum((dd2 - dd1)**2, axis=0)\n    bigbrnch = (dd2 > 2).astype(int)\n\n    SIG = np.arcsin(SSIG) * (bigbrnch == 0).astype(int) + \\\n        (np.pi - np.arcsin(SSIG)) * bigbrnch\n\n    SSIGC = -np.sin(DLAM) * np.cos(lat1) / np.sin(A21P)\n    SIGC = np.arcsin(SSIGC)\n    A21 = A21P - DPHI2 * np.sin(A21P) * np.tan(SIG / 2.0)\n\n    # C   COMPUTE RANGE\n    G2 = EPS * (np.sin(lat1))**2\n    G = np.sqrt(G2)\n    H2 = EPS * (np.cos(lat1) * np.cos(A12))**2\n    H = np.sqrt(H2)\n    SIG2 = SIG * SIG\n    TERM1 = -H2 * (1.0 - H2) / 6.0\n    TERM2 = G * H * (1.0 - 2.0 * H2) / 8.0\n    TERM3 = (H2 * (4.0 - 7.0 * H2) - 3.0 * G2 * (1.0 - 7.0 * H2)) / 120.0\n    TERM4 = -G * H / 48.0\n    rng = xnu1 * SIG * (1.0 + SIG2 * (TERM1 + SIG * TERM2 + SIG2 * TERM3 +\n                                      SIG2 * SIG * TERM4))\n\n    return rng, A12\n\n\ndef earth_distance(lon1, lat1, lon2, lat2):\n    """"""\n    Compute the geodesic distance between lat/lon points.\n\n    Parameters\n    ----------\n    lon1 : array_like or scalar\n        Input array of source longitude(s)\n    lat1 : array_like or scalar\n        Input array of source latitude(s)\n    lon2 : array_like or scalar\n        Input array of destination longitude(s)\n    lat2 : array_like or scalar\n        Input array of destination latitude(s)\n\n    Returns\n    -------\n    distance : array or scalar of distance in meters\n\n    """"""\n    rng, _ = _distq(lon1, lat1, lon2, lat2)\n    return rng\n\n\ndef earth_angle(lon1, lat1, lon2, lat2):\n    """"""\n    Compute the angle between lat/lon points. NOTE: The bearing angle\n    is computed, but then converted to geometric (counter-clockwise)\n    angle to be returned.\n\n    Parameters\n    ----------\n    lon1 : array_like or scalar\n        Input array of source longitude(s)\n    lat1 : array_like or scalar\n        Input array of source latitude(s)\n    lon2 : array_like or scalar\n        Input array of destination longitude(s)\n    lat2 : array_like or scalar\n        Input array of destination latitude(s)\n\n    Returns\n    -------\n    angle : array or scalar of bearing in radians\n\n    """"""\n    _, angle = _distq(lon1, lat1, lon2, lat2)\n    return (np.pi / 2.0 - angle)\n\n\ndef flatten(l, ltypes=(list, tuple, set)):\n    """"""\n    Flatten a list or tuple that contains additional lists or tuples. Like\n    the numpy flatten, but for python types.\n\n    Parameters\n    ----------\n    l: tuple or list,\n        The data that is to be flattened\n    ltypes: tuple,\n        Data types to attempt to flatten\n\n    Returns\n    -------\n    list\n\n    See Also\n    --------\n    numpy.flatten()\n\n    Notes\n    -----\n    This code was taken from:\n    <http://rightfootin.blogspot.com.au/2006/09/more-on-python-flatten.html>\n\n    Examples\n    --------\n    >>> a=[[1,3,4,1], (\'test\', \'this\'), [5,2]]\n    >>> flatten(a)\n    [1, 3, 4, 1, \'test\', \'this\', 5, 2]\n    """"""\n    ltype = type(l)\n    l = list(l)\n    i = 0\n    while i < len(l):\n        while isinstance(l[i], ltypes):\n            if not l[i]:\n                l.pop(i)\n                i -= 1\n                break\n            else:\n                l[i:i + 1] = l[i]\n        i += 1\n    return ltype(l)\n\n\ndef list_files(path=""."", regex=None, full_path=True):\n    """"""\n    list all sorted file names in the given path that conform to the regular\n    expression pattern. This is not a generator function because it sorts\n    the files in alphabetic/numeric order.\n\n    Parameters\n    ----------\n    path : string\n        Search for the given matches\n    regex : string, optional\n        Input regular expression string to filter filenames\n    full_path : bool, optional\n        If True, return the full path for each found object. If false,\n        return only the filename\n\n    Returns\n    -------\n    files : array\n\n    Examples\n    --------\n    >>> files = seapy.list_files(\'/path/to/dir/test_.*txt\')\n    >>> print(files)\n    [\'/path/to/dir/test_001.txt\', \'/path/to/dir/test_002.txt\']\n\n    NOTE: this is equivalent for separating:\n    >>> files = seapy.list_files(\'/path/to/dir\', \'test_.*txt\')\n    """"""\n    # If only one parameter is given, parse into its components\n    if regex is None:\n        regex = os.path.basename(path)\n        path = os.path.dirname(path)\n    if not path:\n        path = \'./\'\n    elif path[-1] != \'/\':\n        path += \'/\'\n    files = []\n    prog = re.compile(regex)\n    for file in os.listdir(path):\n        if prog.search(file) is not None:\n            if full_path:\n                files.append(path + file)\n            else:\n                files.append(file)\n    files.sort()\n    return files\n\n\ndef netcdf(file, aggdim=None):\n    """"""\n    Wrapper around netCDF4 to open a file as either a Dataset or an\n    MFDataset.\n\n    Parameters\n    ----------\n    file : string or list,\n        Filename(s) to open. If the string has wildcards or is a list,\n        this attempts to open an MFDataset\n    aggdim : string,\n        Name of dimension to concatenate along if loading a set of files.\n        A value of None (default) uses the unlimited dimension.\n\n    Returns\n    -------\n    netCDF4 Dataset or MFDataset\n    """"""\n    import netCDF4\n    try:\n        nc = netCDF4.Dataset(file)\n    except (OSError, RuntimeError):\n        try:\n            nc = netCDF4.MFDataset(file, aggdim=aggdim)\n        except IndexError:\n            raise FileNotFoundError(""{:s} cannot be found."".format(file))\n    return nc\n\n\ndef primes(number):\n    """"""\n    Return a list of primes less than or equal to a given value.\n\n    Parameters\n    ----------\n    number : int\n        Find prime values up to this value\n\n    Returns\n    -------\n    primes : ndarray\n\n    Notes\n    -----\n    This code was taken from ""Cooking with Python, Part 2"" by Martelli, et al.\n\n    <http://archive.oreilly.com/pub/a/python/excerpt/pythonckbk_chap1/index1.html?page=last>\n\n    """"""\n    def __erat2():\n        D = {}\n        yield 2\n        for q in itertools.islice(itertools.count(3), 0, None, 2):\n            p = D.pop(q, None)\n            if p is None:\n                D[q * q] = q\n                yield q\n            else:\n                x = p + q\n                while x in D or not (x & 1):\n                    x += p\n                D[x] = p\n\n    return np.array(list(itertools.takewhile(lambda p: p < number, __erat2())))\n\n\ndef rotate(u, v, angle):\n    """"""\n    Rotate a vector field by the given angle\n\n    Parameters\n    ----------\n    u : array like\n        Input u component\n    v : array like\n        Input v component\n    angle : array like\n        Input angle of rotation in radians\n\n    Returns\n    -------\n    rotated_u, rotated_v : array\n    """"""\n    u = np.asanyarray(u)\n    v = np.asanyarray(v)\n    angle = np.asanyarray(angle)\n    sa = np.sin(angle)\n    ca = np.cos(angle)\n\n    return u * ca - v * sa, u * sa + v * ca\n\n\ndef today2day(epoch=default_epoch):\n    """"""\n    Return the day number of today (UTC time) since the epoch.\n\n    Parameters\n    ----------\n    epoch : datetime\n        Date of epoch\n\n    Returns\n    -------\n    numdays : scalar\n    """"""\n    return date2day(datetime.datetime.utcnow(), epoch)\n\n\ndef unique_rows(x):\n    """"""\n    Convert rows into godelnumbers and find the rows that are unique using\n    np.unique\n\n    Parameters\n    ----------\n    x : ndarray or tuple,\n        array of elements to find unique value. If columns are greater\n        than 1, then the columns are combined into a single Godel number.\n        If a tuple of arrays are passed, they are combined.\n\n    Returns\n    -------\n    idx : ndarray,\n        Indices of the unique values\n\n    Examples\n    --------\n    >>> a = np.array([3, 3, 5, 5, 6])\n    >>> b = np.array([2, 3, 3, 3, 3])\n    >>> idx = unique_rows((a, b))\n    >>> idx\n    array([0, 1, 2, 4])\n    """"""\n    if isinstance(x, tuple):\n        x = np.vstack(x).T\n    else:\n        x = np.atleast_1d(x)\n    vals, idx = np.unique(godelnumber(x), return_index=True)\n\n    return idx\n\n\ndef vecfind(a, b, tolerance=None):\n    """"""\n    Find all occurences of b in a within the given tolerance and return\n    the sorted indices of a and b that yield the corresponding values.\n    The indices are of equal length, such that\n\n    Written by Eric Firing, University of Hawaii.\n\n    Parameters\n    ----------\n    a : array\n        Input vector\n    b : array\n        Input vector\n    tolerance : same type as stored values of a and b, optional\n        Input tolerance for how close a is to b. If not specified,\n        then elements of a and b must be equal.\n\n    Returns\n    -------\n    index_a, index_b : arrays of indices for each vector where values are equal,\n            such that a[index_a] == b[index_b]\n\n    Examples\n    --------\n    >>> a = np.array([3,4,1,8,9])\n    >>> b = np.array([4,7,1])\n    >>> ia, ib = vecfind(a, b)\n\n    By definition,\n\n    >>> len(ia) == len(ib)\n    True\n    >>> a[ia] == b[ib]\n    True\n\n    """"""\n    a = np.asanyarray(a).flatten()\n    b = np.asanyarray(b).flatten()\n\n    # if no tolerance, compute a zero distance  the proper type\n    if tolerance is None:\n        tolerance = a[0] - a[0]\n\n    _, uniq_a = np.unique(a, return_index=True)\n    _, uniq_b = np.unique(b, return_index=True)\n    na = len(uniq_a)\n    t = np.hstack((a[uniq_a], b[uniq_b]))\n    is_a = np.zeros(t.shape, dtype=np.int8)\n    is_a[:na] = 1\n    isorted = np.argsort(t)\n    tsorted = t[isorted]\n    is_a_sorted = is_a[isorted]\n\n    dt = np.diff(tsorted)\n    mixed = np.abs(np.diff(is_a_sorted)) == 1\n    ipair = np.nonzero((np.abs(dt) <= tolerance) & mixed)[0]\n\n    # Now ipair should be the indices of the first elements\n    # of consecutive pairs in tsorted for which the two items\n    # are from different arrays, and differ by less than tolerance.\n    # The problem is that they could be in either order.\n\n    iswap = np.nonzero(is_a_sorted[ipair] == 0)[0]  # b is first, so swap\n\n    temp = isorted[ipair[iswap] + 1]\n    isorted[ipair[iswap] + 1] = isorted[ipair[iswap]]\n    isorted[ipair[iswap]] = temp\n\n    isorted_a = isorted[ipair]\n    isorted_b = isorted[ipair + 1] - na\n\n    return uniq_a[isorted_a], uniq_b[isorted_b]\n\n\ndef godelnumber(x):\n    """"""\n    Convert the columns of x into godel numbers. If x is MxN, return an Mx1\n    vector. The Godel number is prime**x\n\n    Parameters\n    ----------\n    x : ndarray,\n        Values to convert into Godel number(s)\n\n    Returns\n    -------\n    godel : ndarray\n    """"""\n    x = np.atleast_2d(x.astype(int))\n    if x.ndim > 1:\n        primevals = primes(x.shape[1] * 10)[:x.shape[1]].astype(float)\n        return(np.prod(primevals**x, axis=1))\n    else:\n        return 2.0**x\n\n\npass\n'"
seapy/mapping.py,15,"b'#!/usr/bin/env python\n""""""\n  map.py\n\n  State Estimation and Analysis for PYthon\n\n  Utilities for dealing with basemap plotting. These routnes are simply\n  abstractions over the existing basemap to make it quicker for generating\n  basemap plots and figures.\n\n    Examples\n    -------\n\n    Assume you have longitude, latitude, and sst values:\n\n    >>> m=seapy.mapping.map(llcrnrlon=lon[0,0],llcrnrlat=lat[0,0],\n    >>>     urcrnrlon=lon[-1,-1],urcrnrlat=lat[-1,-1],dlat=2,dlon=2)\n    >>> m.pcolormesh(lon,lat,sst,vmin=22,vmax=26,cmap=plt.cm.bwr)\n    >>> m.land()\n    >>> m.colorbar(label=""Sea Surface Temp [$^\\circ$C]"",cticks=[22,23,24,25,26])\n    >>> m.ax.patch.set_facecolor(""aqua"")\n    >>> m.ax.patch.set_alpha(1)\n    >>> m.fig.patch.set_alpha(0.0)\n    >>> m.fig.savefig(""sst.png"",dpi=100)\n\n\n  Written by Brian Powell on 9/4/14\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.basemap import Basemap\nfrom seapy.model import asgrid\n\n\ndef gen_coastline(lon, lat, bathy, depth=0):\n    """"""\n    Given lon, lat, and bathymetry, generate vectors of line segments\n    of the coastline. This can be exported to matlab (via savemat) to be\n    used with the \'editmask\' routine for creating grid masks.\n\n    Input\n    -----\n    lon : array,\n        longitudes of bathymetry locations\n    lat : array,\n        latitudes of bathymetry locations\n    bathy : array,\n        bathymetry (negative for ocean, positive for land) values\n    depth : float,\n        depth to use as the definition of the coast\n\n    Returns\n    -------\n    lon : ndarray,\n        vector of coastlines, separated by nan (matlab-style)\n    lat : ndarray,\n        vector of coastlines, separated by nan (matlab-style)\n    """"""\n    CS = plt.contour(lon, lat, bathy, [depth - 0.25, depth + 0.25])\n    lon = list()\n    lat = list()\n    for col in CS.collections:\n        for path in col.get_paths():\n            lon.append(path.vertices[:, 0])\n            lon.append(np.nan)\n            lat.append(path.vertices[:, 1])\n            lat.append(np.nan)\n    return (np.hstack(lon), np.hstack(lat))\n\n\nclass map(object):\n\n    def __init__(self, grid=None, llcrnrlon=-180, llcrnrlat=-40, urcrnrlon=180,\n                 urcrnrlat=40, proj=\'lcc\', resolution=\'c\', figsize=(8., 6.),\n                 dlat=1, dlon=2, fig=None, ax=None, fontsize=12, fill_color=""aqua""):\n        """"""\n        map class for abstracting the basemap methods for quick and easy creation\n        of geographically referenced data figures\n\n\n        Parameters\n        ----------\n        grid: seapy.model.grid or string, optional:\n            grid to use to define boundaries\n        llcrnrlon: float, optional\n            longitude of lower, left corner\n        llcrnrlat: float, optional\n            latitude of lower, left corner\n        urcrnrlon: float, optional\n            longitude of upper, right corner\n        urcrnrlat: float, optional\n            latitude of upper, right corner\n        proj: string, optional\n            projection to use for map\n        resolution: character\n            resolution to use for coastline, etc. From Basemap:\n            \'c\' (crude), \'l\' (low), \'i\' (intermediate),\n            \'h\' (high), \'f\' (full), or None\n        figsize: list, optional\n            dimensions to use for creation of figure\n        dlat: float, optional\n            interval to mark latitude lines (e.g., if dlat=0.5 every 0.5deg mark)\n        dlon: float, optional\n            interval to mark longitude lines (e.g., if dlon=0.5 every 0.5deg mark)\n        fig: matplotlib.pyplot.figure object, optional\n            If you want to plot on a pre-configured figure, pass the figure object\n            along with the axis object.\n        ax: matplotlib.pyplot.axis object, optional\n            If you want to plot on a pre-configured figure, pass the axis object\n            along with the figure object.\n        fill_color: string, optional\n            The color to use for the axis background\n\n        Returns\n        -------\n        None\n\n        """"""\n        if grid is not None:\n            grid = asgrid(grid)\n            llcrnrlat = np.min(grid.lat_rho)\n            urcrnrlat = np.max(grid.lat_rho)\n            llcrnrlon = np.min(grid.lon_rho)\n            urcrnrlon = np.max(grid.lon_rho)\n\n        self.basemap = Basemap(llcrnrlon=llcrnrlon, llcrnrlat=llcrnrlat,\n                               urcrnrlon=urcrnrlon, urcrnrlat=urcrnrlat,\n                               projection=proj,\n                               lat_0=urcrnrlat - (urcrnrlat - llcrnrlat) / 2.,\n                               lon_0=urcrnrlon - (urcrnrlon - llcrnrlon) / 2.,\n                               resolution=resolution, area_thresh=0.0, ax=ax)\n\n        self.figsize = figsize\n        delta = (np.abs(urcrnrlon - llcrnrlon) // .04) / 100\n        self.dlon = np.minimum(delta, dlon)\n        delta = (np.abs(urcrnrlat - llcrnrlat) // .04) / 100\n        self.dlat = np.minimum(delta, dlat)\n        self.fig = fig\n        self.ax = ax\n        self.fill_color = fill_color\n        self.fontsize = fontsize\n        reset = True if fig is None else False\n        self.new_figure(reset=reset)\n\n    def new_figure(self, fill_color=None, reset=False, dpi=150):\n        """"""\n        Create or update a figure for plotting\n\n        Parameters\n        ----------\n        fill_color: string, optional\n           Color to fill the background of the axes with\n        reset: bool, optional\n           Reset the figure\n        """"""\n        if reset:\n            if self.ax:\n                self.ax.set_axis_off()\n                self.ax = None\n            if self.fig:\n                self.fig.clf()\n                self.fig = None\n\n        if self.fig is None or self.ax is None:\n            self.fig = plt.figure(figsize=self.figsize, dpi=dpi)\n            self.ax = self.fig.add_axes([-0.01, 0.25, 1.01, 0.7])\n\n        if fill_color is None:\n            fill_color = self.fill_color\n\n        self.basemap.drawmapboundary(fill_color=fill_color)\n        # Create the longitude lines\n        nticks = int((self.basemap.urcrnrlon - self.basemap.llcrnrlon) /\n                     self.dlon)\n        md = np.mod(self.basemap.llcrnrlon, self.dlon)\n        if md:\n            slon = self.basemap.llcrnrlon + self.dlon - md\n        else:\n            slon = self.basemap.llcrnrlon\n            nticks += 1\n        lon_lines = np.arange(nticks) * self.dlon + slon\n        self.basemap.drawmeridians(lon_lines, color=""0.5"",\n                                   linewidth=0.25, dashes=[1, 1, 0.1, 1],\n                                   labels=[0, 0, 0, 1], fontsize=self.fontsize)\n\n        # Create the latitude lines\n        nticks = int((self.basemap.urcrnrlat - self.basemap.llcrnrlat) /\n                     self.dlat)\n        md = np.mod(self.basemap.llcrnrlat, self.dlat)\n        if md:\n            slat = self.basemap.llcrnrlat + self.dlat - md\n        else:\n            slat = self.basemap.llcrnrlat\n            nticks += 1\n        lat_lines = np.arange(nticks) * self.dlat + slat\n        self.basemap.drawparallels(lat_lines, color=""0.5"",\n                                   linewidth=0.25, dashes=[1, 1, 0.1, 1],\n                                   labels=[1, 0, 0, 0], fontsize=self.fontsize)\n\n    def land(self, color=""black""):\n        """"""\n        Draw the land mask\n\n        Parameters\n        ----------\n        color: string, optional\n            color to draw the mask with\n        """"""\n        self.basemap.drawcoastlines()\n        self.basemap.drawcountries()\n        self.basemap.fillcontinents(color=color)\n\n    def zoom(self, xrange, yrange):\n        """"""\n        zoom the figure to a specified lat, lon range\n\n        Parameters\n        ----------\n        xrange: array\n            minimum and maximum longitudes to display\n        yrange: array\n            minimum and maximum latitudes to display\n        """"""\n        x, y = self.basemap(xrange, yrange)\n        self.ax.set_xlim(x)\n        self.ax.set_ylim(y)\n        self.fig.canvas.draw()\n\n    def pcolormesh(self, lon, lat, data, **kwargs):\n        """"""\n        pcolormesh field data onto our geographic plot\n\n        Parameters\n        ----------\n        lon: array\n            Longitude field for data\n        lat: array\n            Latitude field for data\n        data: array\n            data to pcolor\n        **kwargs: arguments, optional\n            additional arguments to pass to pcolor\n        """"""\n        # Pcolor requires a modification to the locations to line up with\n        # the geography\n        dlon = lon * 0\n        dlat = lat * 0\n        dlon[:, 0:-1] = lon[:, 1:] - lon[:, 0:-1]\n        dlat[0:-1, :] = lat[1:, :] - lat[0:-1, :]\n        x, y = self.basemap(lon - dlon * 0.5, lat - dlat * 0.5)\n        self.pc = self.ax.pcolormesh(x, y, data, **kwargs)\n\n    def contourf(self, lon, lat, data, **kwargs):\n        """"""\n        contourf field data onto our geographic plot\n\n        Parameters\n        ----------\n        lon: array\n            Longitude field for data\n        lat: array\n            Latitude field for data\n        data: array\n            data to contourf\n        **kwargs: arguments, optional\n            additional arguments to pass to pcolor\n        """"""\n        # Pcolor requires a modification to the locations to line up with\n        # the geography\n        dlon = lon * 0\n        dlat = lat * 0\n        dlon[:, 0:-1] = lon[:, 1:] - lon[:, 0:-1]\n        dlat[0:-1, :] = lat[1:, :] - lat[0:-1, :]\n        x, y = self.basemap(lon - dlon * 0.5, lat - dlat * 0.5)\n        self.pc = self.ax.contourf(x, y, data, **kwargs)\n\n    def scatter(self, lon, lat, data, **kwargs):\n        """"""\n        scatter plot data onto our geographic plot\n\n        Parameters\n        ----------\n        lon: array\n            Longitude field for data\n        lat: array\n            Latitude field for data\n        data: array\n            data to pcolor\n        **kwargs: arguments, optional\n            additional arguments to pass to pcolor\n        """"""\n        x, y = self.basemap(lon, lat)\n        self.pc = self.ax.scatter(x, y, c=data, **kwargs)\n\n    def colorbar(self, label=None, cticks=None, **kwargs):\n        """"""\n        Display a colorbar on the figure\n\n        Parameters\n        ----------\n        label: string, optional\n            Colorbar label title\n        cticks: array, optional\n            Where to place the tick marks and values for the colorbar\n        **kwargs: arguments, optional\n            additional arguments to pass to colorbar\n        """"""\n        self.cax = self.fig.add_axes([0.25, 0.16, 0.5, 0.03])\n        self.cb = plt.colorbar(self.pc, cax=self.cax, orientation=""horizontal"",\n                               ticks=cticks, **kwargs)\n        self.basemap.set_axes_limits(ax=self.ax)\n        if label is not None:\n            self.cb.set_label(label)\n'"
seapy/oa.py,7,"b'#!/usr/bin/env python\n""""""\n  oa\n\n  Objective analysis.  This function will interpolate data using the\n  fortran routines written by Emanuelle Di Lorenzo and Bruce Cornuelle\n\n  Written by Brian Powell on 10/08/13\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\n\nimport numpy as np\nfrom seapy.external import oalib\n\n__bad_val = -999999.0\n\n\ndef oasurf(x, y, d, xx, yy, pmap=None, weight=10, nx=2, ny=2, verbose=False):\n    """"""\n    Objective analysis interpolation for 2D fields\n\n    Parameters\n    ----------\n    x: array [2-D]\n        x-values of source data\n    y: array [2-D]\n        y-values of source data\n    d: array [2-D]\n        data values of source\n    xx: array [2-D]\n        x-values of destination\n    yy: array [2-D]\n        y-values of destination\n    pmap: array, optional\n        weighting array to map between source and destination.\n        NOTE: best to save this after using to prevent recomputing\n        weights for every interpolate\n    weight: int, optional\n        number of neighbor points to consider for every destination point\n    nx: int, optional\n        decorrelation lengthscale in x [same units as x]\n    ny: int, optional\n        decorrelation lengthscale in y [same units as y]\n    verbose : bool, optional\n        display information within the OA routine\n\n    Returns\n    -------\n    new_data: ndarray\n        data interpolated onto the new grid\n    pmap: ndarray\n        weighting map used in the interpolation\n\n    """"""\n    # Do some error checking\n    nx = ny if nx == 0 else nx\n    ny = nx if ny == 0 else ny\n    d = np.ma.masked_invalid(d, copy=False)\n\n    # Generate a mapping weight matrix if not passed\n    if pmap is None:\n        pmap = np.zeros([xx.size, weight], order=\'F\')\n\n    # Call FORTRAN library to objectively map\n    vv, err = oalib.oa2d(x.ravel(), y.ravel(),\n                         d.filled(__bad_val).ravel(),\n                         xx.ravel(), yy.ravel(), nx, ny,\n                         pmap, verbose)\n\n    # Reshape the results and return\n    return np.ma.masked_equal(vv.reshape(xx.shape), __bad_val, copy=False), \\\n        pmap\n\n\ndef oavol(x, y, z, v, xx, yy, zz, pmap=None, weight=10, nx=2, ny=2,\n          verbose=False):\n    """"""\n    Objective analysis interpolation for 3D fields\n\n    Parameters\n    ----------\n    x: array [2-D]\n        x-values of source data\n    y: array [2-D]\n        y-values of source data\n    z: array [3-D]\n        z-values of source data\n    v: array [3-D]\n        data values of source\n    xx: array [2-D]\n        x-values of destination\n    yy: array [2-D]\n        y-values of destination\n    zz: array [3-D]\n        z-values of destination\n    pmap: array, optional\n        weighting array to map between source and destination.\n        NOTE: best to save this after using to prevent recomputing\n        weights for every interpolate\n    weight: int, optional\n        number of neighbor points to consider for every destination point\n    nx: int, optional\n        decorrelation lengthscale in x [same units as x]\n    ny: int, optional\n        decorrelation lengthscale in y [same units as y]\n    verbose : bool, optional\n        display information within the OA routine\n\n    Returns\n    -------\n    new_data: ndarray\n        data interpolated onto the new grid\n    pmap: ndarray\n        weighting map used in the interpolation\n\n    """"""\n    # Do some error checking\n    nx = ny if nx == 0 else nx\n    ny = nx if ny == 0 else ny\n    v = np.ma.masked_invalid(v, copy=False)\n\n    # Generate a mapping weight matrix if not passed\n    if pmap is None:\n        # Build the map\n        tmp = np.ones(x.ravel().shape, order=\'F\')\n        pmap = np.zeros([xx.size, weight], order=\'F\')\n        oalib.oa2d(x.ravel(), y.ravel(), tmp,\n                   xx.ravel(), yy.ravel(), nx, ny, pmap, verbose)\n\n    # Call FORTRAN library to objectively map\n    vv, err = oalib.oa3d(x.ravel(), y.ravel(),\n                         z.reshape(z.shape[0], -1).transpose(),\n                         v.filled(__bad_val).reshape(\n                             v.shape[0], -1).transpose(),\n                         xx.ravel(), yy.ravel(),\n                         zz.reshape(zz.shape[0], -1).transpose(),\n                         nx, ny, pmap, verbose)\n\n    # Reshape the results and return\n    return np.ma.masked_equal(vv.transpose().reshape(zz.shape), __bad_val,\n                              copy=False), pmap\n'"
seapy/plot.py,5,"b'#!/usr/bin/env python\n""""""\n  plot.py\n\n  State Estimation and Analysis for PYthon\n\n  Module with plotting utilities\n\n  Written by Brian Powell on 10/18/13\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\n\n\nimport numpy as np\nfrom scipy import ndimage\nimport os\nimport re\nfrom matplotlib import pyplot as plt\n\n\ndef stackbar(x, y, colors=None, **kwargs):\n    """"""\n    Given an array of vectors in y, draw a bar chart for each one stacked on\n    the prior.\n    """"""\n    x = np.asarray(x)\n    if colors is None:\n        colors = ["""" for i in range(0, y.shape[0])]\n    # Stack positive and negative separately\n    for op in (""__ge__"", ""__lt__""):\n        d = getattr(y, op)(0)\n        s = y[0, :] * 0\n        l = np.where(d[0, :])[0]\n        if np.any(l):\n            plt.bar(x[l], y[0, l], color=colors[0], **kwargs)\n        s[l] = y[0, l]\n        for i in range(1, y.shape[0]):\n            l = np.where(d[i, :])[0]\n            if np.any(l):\n                plt.bar(x[l], y[i, l], color=colors[i], bottom=s[l], **kwargs)\n                s[l] += y[i, l]\n'"
seapy/progressbar.py,0,"b'#!/usr/bin/env python\n""""""\nASCII Progress Bar that work in IPython notebooks.\n\nCode take from examples shown at:\n\n<http://nbviewer.ipython.org/github/ipython/ipython/blob/3607712653c66d63e0d7f13f073bde8c0f209ba8/docs/examples/notebooks/Animations_and_Progress.ipynb>\n\nModified to include a timer and added an iterator that displays a\nprogressbar as it iterates.\n\nExamples\n--------\n>>> for i in progressbar.progress(range(10)):\n>>>     frobnicate(i)\n\n""""""\n\nimport sys\nimport time\ntry:\n    from IPython.display import clear_output\n    have_ipython = True\nexcept ImportError:\n    have_ipython = False\n\n\nclass ProgressBar:\n\n    def __init__(self, iterations):\n        self.iterations = iterations\n        self.prog_bar = \'[]\'\n        self.fill_char = \'=\'\n        self.width = 40\n        self.__update_amount(0)\n        self.start = time.process_time()\n        if have_ipython:\n            self.animate = self.animate_ipython\n        else:\n            self.animate = self.animate_noipython\n\n    def animate_ipython(self, iter):\n        print(\'\\r\', self, end=\'\', file=sys.stderr)\n        sys.stderr.flush()\n        self.update_iteration(iter + 1)\n\n    def update_iteration(self, elapsed_iter):\n        self.__update_amount((elapsed_iter / float(self.iterations)) * 100.0)\n        t = time.process_time()\n        delta = t - self.start\n        togo = (delta / elapsed_iter) * (self.iterations - elapsed_iter)\n        self.prog_bar += \'  [%d of %d, %.1f secs elapsed/%.1f secs ETA]\' % \\\n            (elapsed_iter, self.iterations, delta, togo)\n        if elapsed_iter > self.iterations:\n            print(""\\r [ COMPLETED %d ITERATIONS IN %.1f SECS ] %s"" %\n                  (self.iterations, delta, "" "" * 60))\n            print(""\\r [ COMPLETED %d ITERATIONS IN %.1f SECS ] %s"" %\n                  (self.iterations, delta, "" "" * 60), file=sys.stderr)\n\n    def __update_amount(self, new_amount):\n        percent_done = int(round((new_amount / 100.0) * 100.0))\n        all_full = self.width - 2\n        num_hashes = int(round((percent_done / 100.0) * all_full))\n        self.prog_bar = \'[\' + self.fill_char * num_hashes + \' \' * \\\n            (all_full - num_hashes) + \']\'\n        pct_place = (len(self.prog_bar) // 2) - len(str(percent_done))\n        pct_string = \'%d%%\' % percent_done\n        self.prog_bar = self.prog_bar[0:pct_place] + \\\n            (pct_string + self.prog_bar[pct_place + len(pct_string):])\n\n    def __str__(self):\n        return str(self.prog_bar)\n\n\nclass progress:\n    """"""\n    Draw a progess bar while going through the given iterator.\n\n    Notes\n    -----\n    If the iterator does not support the len() method, you should\n    supply the maxlen parameter.\n\n    Examples\n    --------\n    >>> for i in progress(range(10)):\n    >>>    frobnicate(i)\n\n    or\n\n    >>> for n,i in progressbar(enumerate(range(10)),10)\n    >>>    frobnicate(n, i)\n    """"""\n\n    def __init__(self, iterable, maxlen=100):\n        try:\n            self.size = len(iterable)\n        except TypeError:\n            self.size = maxlen\n        self.pbar = ProgressBar(self.size + 1)\n        self.iterator = iter(iterable)\n        self.count = 0\n\n    def __getitem__(self, index):\n        self.count += 1\n        self.pbar.animate(self.count)\n        return next(self.iterator)\n'"
seapy/qserver.py,0,"b'#!/usr/bin/env python\n""""""\n  This module will execute any number of tasks using a queue of the\n  specified number of threads.\n\n  Define a list of qserver.task objects and then tell the server to\n  execute the list with a given number of threads.\n\n  **Examples**\n\n  >>> tasks = (qserver.os_task(""list files"",""ls -1""),\n  >>>          qserver.task(""my job"",my_func,arg1,arg2,arg3))\n  >>> qserver.execute(tasks, nthreads=2)\n\n""""""\n\n\nimport sys\nif sys.version_info < (3, 0):\n    from Queue import Queue\nelse:\n    from queue import Queue\nimport threading\nimport subprocess\nimport sys\nimport datetime\nfrom seapy.timeout import timeout, TimeoutError\n\n\nclass task:\n    """"""\n    task class simply defines a task for the queue server to process.\n    It requires a descriptive name of the task for logging and the command\n    to execute.\n\n    Parameters\n    ----------\n\n    name : string\n        title of the task\n    cmd : string\n        shell command with arguments to ru\n\n    Returns\n    -------\n        none\n\n    """"""\n\n    def __init__(self, name, cmd, *args):\n        self.cmd = cmd\n        self.name = name\n        self.args = args\n        pass\n\n    def run(self):\n        if callable(self.cmd):\n            self.cmd(*self.args)\n        pass\n\n    pass\n\n\nclass os_task(task):\n    """"""\n    subclass of task to simply call shell commands\n\n    It requires a descriptive name of the task for logging, the method\n    to call, and a list of arguments for the method.\n\n    Parameters\n    ----------\n\n    name : string\n        title of the task\n    cmd : method name, function pointer\n        method to call\n    args : vary [optional]\n        arguments to pass to cmd\n\n    Returns\n    -------\n        none\n\n    """"""\n\n    def run(self):\n        subprocess.call(self.cmd, shell=True)\n\n\nclass process_thread(threading.Thread):\n    def __init__(self, queue):\n        threading.Thread.__init__(self)\n        self.queue = queue\n\n    def run(self):\n        while True:\n            item = self.queue.get()\n            print(self.getName() + "" running "" +\n                  item.name + "" at "" + str(datetime.datetime.now()) + ""\\n"")\n            sys.stdout.flush()\n            item.run()\n            print(self.getName() + "" completed "" + item.name +\n                  "" at "" + str(datetime.datetime.now()) + ""\\n"")\n            sys.stdout.flush()\n            self.queue.task_done()\n\n\ndef execute(tasks, nthreads=2):\n    """"""\n    Run the list of tasks in a queued server with the specified number of\n    threads.\n\n    Parameters\n    ----------\n    tasks : list\n        list of task classes to execute in the queue server\n    nthreads: int\n        number of threads to use to process tasks in the queue\n\n    Returns\n    -------\n    None\n    """"""\n    q = Queue()\n    for i in range(nthreads):\n        t = process_thread(q)\n        t.daemon = True\n        t.start()\n\n    for item in tasks:\n        q.put(item)\n\n    q.join()\n\n\n'"
seapy/tidal_energy.py,26,"b'#!/usr/bin/env python\n""""""\ntidal_energy.py\n\nState Estimation and Analysis for PYthon\n\nModule to compute tidal energy from a column of data.\n\nWritten by Brian Powell on 03/30/16\nCopyright (c)2020 University of Hawaii under the MIT-License.\n\nNotes\n-----\n\nBarotropic to Baroclinic conversion is given by:\n\n.. math::\n\n    C=1 / T_t \\int_0^T_t P\'_t * wbar_t * dt,                    (1)\n\nwhere, T_t is the tidal period for consituent, t, P\' is the pressure perturbation,\nwbar is the vertical velocity. Hence, conversion is the time average of the\nvertical motion of the bottom pressure perturbation. We can do it spectrally if\nwe represent P\'_t and wbar_t as waves:\n\n.. math::\n\n    P\'_t = Amp_P\'_t * sin( 2 * pi * t / T_t + Pha_P\'_t )                   (2) \\\\\n    wbar_t = Amp_wbar_t * sin( 2 * pi * t / T_t + Pha_wbar_t )             (3)\n\nIf we substitute (2) and (3) into (1) using trig. identity and integrate over\nthe period (recall that integrating a wave over one period is zero):\n\n.. math::\n\n    Conversion = 0.5 * Amp_P\'_t * Amp_wbar_t * cos( Pha_P\'_t - Pha_wbar_t )(4)\n\nEnergy Flux is given by:\n\n.. math::\n\n    Flux_u = 1 / T_t * \\int_0^T_t u\'_t * P\'_t * dt,                        (5) \\\\\n    Flux_v = 1 / T_t * \\int_0^T_t v\'_t * P\'_t * dt,                        (6)\n\nwhere u\' and v\' are the velocity anomalies for the constituent, t. As per\nabove, we can express as waves to yield:\n\n.. math::\n\n    Flux_u = 0.5 * Amp_u\'_t * Amp_P\'_t * cos( Pha_u\'_t - Pha_P\'_t )        (7) \\\\\n    Flux_v = 0.5 * Amp_v\'_t * Amp_P\'_t * cos( Pha_v\'_t - Pha_P\'_t )        (8)\n\nDisplacement is given by:\n\n.. math::\n\n    Displace = \\int_0^T_t/2 g * rho\'_t / ( rho0 * N_t**2 ) * dt,           (9)\n\nwhere rho\' is the density anomaly and N**2 is the Brunt-Vaisala. NOTE:\nthis is integrated over one-half period becuase (by definition), it would\nintegrate to zero. However, if we know the tidal vertical velocity, then\nwe can integrate it for one-half period for the todal displacement:\n\n.. math::\n\n    Displace = \\int_0^T_t/2 w_t * dt \\\\\n        = \\int_0^T_t/2 Amp_w_t * sin( 2 * pi * t / T_t )               (10) \\\\\n        = Amp_w_t * T_t / pi\n\nHorizontal Kinetic Energy is given by:\n\n.. math::\n\n    HKE = 0.5 * rho0 * 1 / T_t * \\int_0^T_t (u\'_t**2 + v\'_t**2) * dt        (11)\n\nsubstitute u\' and v\' as waveforms and integrate over a period,\n\n.. math::\n\n    HKE = 0.5 * rho0 * 0.5 * ( Amp_u\'_t**2 _ Amp_v\'_t**2 )                  (12)\n\nAvailable Potential Energy is given by:\n\n.. math::\n\n    APE = 0.5 * rho0 * 1 / T_t * \\int_0^T_t N_t**2 * Displace_t**2 * dt     (13)\n\nFor this, we will use the time-average N**2 (not at the specific tidal\nfrequency) and use (10); hence, it becomes:\n\n.. math::\n\n    APE = 0.5 * rho0 * (Amp_w_t * T_t / pi)**2 * 1/T_t \\int_0^T_t N**2 * dt (14)\n""""""\n\nimport numpy as np\nimport seapy\n\n_rho0 = 1000\n\n\nclass energetics():\n    """"""\n      This class is a container for the energetics produced by the tidal_energy\n      calculation to simplify access to the resulting data.\n    """"""\n\n    def __init__(self, tides, energy, integrals, ellipse):\n        try:\n            self.tides = tides.tolist()\n        except AttributeError:\n            self.tides = tides\n        self.energy = energy\n        if len(tides) != energy.shape[0]:\n            raise ValueError(\n                ""The number of tides and energy values are inconsistent"")\n        self.integrals = integrals\n        self.ellipse = ellipse\n        pass\n\n    def __getitem__(self, key):\n        """"""\n        Return the energetics for a tidal constituent\n        """"""\n        t = self.tides.index(key.upper())\n        return {""conversion"": self.integrals[t, 2],\n                ""flux_u"": self.energy[t, :, 0],\n                ""flux_v"": self.energy[t, :, 1],\n                ""disp"": self.energy[t, :, 2],\n                ""hke"": self.energy[t, :, 3],\n                ""ape"": self.energy[t, :, 4],\n                ""total_flux_u"": self.integrals[t, 0],\n                ""total_flux_v"": self.integrals[t, 1],\n                ""total_hke"": self.integrals[t, 3],\n                ""total_ape"": self.integrals[t, 4],\n                ""ellipse"": self.ellipse[key.upper()]}\n        pass\n\n\ndef tidal_energy(time, hz, u, v, w, pressure, bvf=None, tides=None,\n                 ubar=None, vbar=None, wbar=None):\n    """"""\n    Calculate aspects of tidal energy from the given data: baroclinic energy flux,\n    HKE, APE, displacement, and conversion.\n\n    This only works for a single depth profile, and the arrays are to be 2D with\n    dimensions of [time, depth] with depth index 0 as the bottom and inded -1 as\n    the surface. Likewise, the hz field is oriented the same.\n\n    Parameters\n    ----------\n    time : list of datetime,\n      times of data\n    hz : ndarray,\n      Thickness of the water column represented by 3D quantities [m]\n    u : ndarray,\n      u-component of 3D velocity [m s**-1]\n    v : ndarray,\n      v-component of 3D velocity  [m s**-1]\n    w : ndarray,\n      w-component of 3D velocity  [m s**-1]\n    pressure : ndarray,\n      pressure of the 3D field [dbar]\n    bvf : ndarray, optional\n      Brunt-Vaisala Frequency of the 3D field [s**-1]. If not specified\n      the APE will not be computed\n    tides: list of strings, optional\n      The names of the tides to use for analysis. If none\n      provided, use the defaults from seapy.tide\n    ubar : ndarray, optional\n      u-component of barotropic velocity [m s**-1]. If none\n      provided, compute from u\n    vbar : ndarray, optional\n      v-component of barotropic velocity [m s**-1]. If none\n      provided, compute from v\n    wbar : ndarray, optional\n      w-component of barotropic velocity [m s**-1]. If none\n      provided, compute from w\n\n    Returns\n    -------\n    energetics : class,\n      The energetics for each tidal consituent as well as the\n      vertically integrated properties. The energetics class\n      provides various methods for accessing the data\n    """"""\n\n    # Ensure arrays as needed\n    u = np.ma.array(u)\n    v = np.ma.array(v)\n    w = np.ma.array(w)\n    pressure = np.ma.array(pressure)\n\n    # Setup the thicknesses in time\n    hz = np.ma.array(hz)\n    if hz.ndims == 1:\n        hz = np.tile(hz, (u.shape[0]))\n        total_h = np.sum(hz, axis=1)\n        ndep = hz.shape[1]\n\n    # If BVF not given, set to zero\n    if bvf:\n        bvf = np.ma.array(bvf).mean(axis=0)\n    else:\n        bvf = np.zeros(ndep)\n\n    # Setup the tides\n    tides = seapy.tide._set_tides(tides)\n    ntides = len(tides)\n    period = 3600 / seapy.tide.frequency(tides)\n\n    # Setup the barotropic velocities\n    if ubar and vbar:\n        ubar = np.ma.array(ubar)\n        vbar = np.ma.array(vbar)\n        wbar = np.ma.array(wbar)\n    else:\n        ubar = np.sum(hz * u, axis=1) / total_h\n        vbar = np.sum(hz * v, axis=1) / total_h\n        wbar = np.sum(hz * w, axis=1) / total_h\n\n    # Calculate Pressure Anomalies\n    p_prime = pressure - pressure.mean(axis=0)\n    # Apply baroclinicity\n    p_prime -= np.sum(p_prime * hz) / np.sum(hz)\n\n    # Calculate tides\n    tidal_vel = seapy.tide.fit(time, ubar + 1j * vbar, tides)\n    wbar = seapy.tide.fit(time, wbar, tides)\n\n    # Store the tidal ellipse\n    ellipse = {}\n    for t in tides:\n        ellipse[t] = seapy.tide.tellipse(tidal_vel[\'major\'][t].amp,\n                                         tidal_vel[\'minor\'][t].amp,\n                                         tidal_vel[\'minor\'][t].phase,\n                                         tidal_vel[\'major\'][t].phase)\n\n    # Velocity anomalies\n    u_prime = u - u.mean(axis=0) - np.real(tidal_vel[\'fit\'])\n    v_prime = v - v.mean(axis=0) - np.imag(tidal_vel[\'fit\'])\n    w_prime = w - v.mean(axis=0) - wbar[\'fit\']\n    wbar = wbar[\'major\']\n\n    # Set the results structure: for each tide, and for each\n    # depth, we will store five values (flux_u, flux_v,\n    # displacement, HKE, APE)\n    energy = np.zeros((ntides, ndep, 5))\n\n    # For vertically integrated, we will also have five values:\n    # (flux_u, flux_v, conversion, HKE, APE)\n    integrals = np.zeros((ntides, 5))\n\n    # Compute over all depths\n    for d in seapy.progressbar.progress(np.arange(ndep)):\n        # Generate the tidal components\n        t_pres = seapy.tide.fit(time, p_prime[:, d], tides)[\'major\']\n\n        # velocity\n        t_u = seapy.tide.fit(time, u_prime[:, d], tides)[\'major\']\n        t_v = seapy.tide.fit(time, v_prime[:, d], tides)[\'major\']\n        t_w = seapy.tide.fit(time, w_prime[:, d], tides)[\'major\']\n\n        # Compute each term for each tide\n        for n, t in enumerate(tides):\n            # If this is the bottom, generate the conversion\n            if d == 0:\n                integrals[n, 2] = 0.5 * t_pres[t].amp * wbar[t].amp * \\\n                    np.cos(t_pres[t].phase - wbar[t].phase)\n\n            # Calculate Energy Flux\n            energy[n, d, 0] = 0.5 * t_u[t].amp * \\\n                t_pres[t].amp * np.cos(t_u[t].phase - t_pres[t].phase)\n            energy[n, d, 1] = 0.5 * t_v[t].amp * \\\n                t_pres[t].amp * np.cos(t_v[t].phase - t_pres[t].phase)\n\n            # Calculate Displacement\n            energy[n, d, 2] = t_w[t].amp * 3600 * period[t] / np.pi\n\n            # Calculate HKE and APE\n            energy[n, d, 3] = 0.5 * _rho0 * bvf[d] * displace\n            energy[n, d, 4] = 0.25 * _rho0 * (t_u[t].amp + t_v[t].amp)\n\n    # Vertically Integrate\n    for n, t in enumerate(tides):\n        for i in [0, 1, 3, 4]:\n            integrals[n, i] = np.sum(energy[n, :, i] * hz, axis=1) / total_h\n\n    # Put it all together to return\n    return energetics(tides, energy, integrals, ellipse)\n'"
seapy/tide.py,78,"b'#!/usr/bin/env python\n""""""\n  tide.py\n\n  Functions for working with tidal time-series\n\n  Written by Glenn Carter and Dale Partridge\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\nimport numpy as np\nimport datetime\nfrom collections import namedtuple\nimport os\nfrom warnings import warn\n\namp_phase = namedtuple(\'amp_phase\', \'amp phase\')\ntellipse = namedtuple(\'tellipse\', \'major minor angle phase\')\nvuf_vals = namedtuple(\'vuf_vals\', \'v u f\')\n__cformat = namedtuple(\'__cformat\', \'freq doodson semi sat shallow\')\n__satinfo = namedtuple(\'__satinfo\', \'deldood phcorr amprat ilatfac\')\n__shallowinfo = namedtuple(\'__shallowinfo\', \'isshallow iname coef\')\n# Load the constituent data when the module is imported\n__reftime = datetime.datetime(1899, 12, 31, 12, 0, 0)\n\nwith np.load(os.path.dirname(__file__) + ""/constituents.npz"", allow_pickle=True) as data:\n    __const_file = data[\'__const\'][()]\n\n__const = {}\nfor f in list(__const_file.keys()):\n    __shallow = __shallowinfo(__const_file[f][\'shallow\'][\'isshallow\'],\n                              __const_file[f][\'shallow\'][\'iname\'], __const_file[f][\'shallow\'][\'coef\'])\n    __sat = __satinfo(__const_file[f][\'sat\'][\'deldood\'],\n                      __const_file[f][\'sat\'][\'phcorr\'],\n                      __const_file[f][\'sat\'][\'amprat\'],\n                      __const_file[f][\'sat\'][\'ilatfac\'])\n    __const[f] = __cformat(np.float(__const_file[f][\'freq\']),\n                           __const_file[f][\'doodson\'],\n                           np.float(__const_file[f][\'semi\']),\n                           __sat, __shallow)\n\ndefault_tides = [\'M4\', \'K2\', \'S2\', \'M2\', \'N2\',\n                 \'K1\', \'P1\', \'O1\', \'Q1\', \'MF\', \'MM\']\n\n\ndef _set_tides(tides=None):\n    """"""\n    Private method: make sure that tides passed in are a proper array\n    and in uppercase\n    """"""\n    return default_tides if tides is None else np.array([t.upper() for t in np.atleast_1d(tides)])\n\n\ndef frequency(tides=None):\n    """"""\n    Returns array of the frequency in cycles per hour for the requested\n    tidal constituents.\n\n    Parameters\n    ----------\n    tides: string or list of strings,\n        List of consituents to obtain the frequency for\n\n    Returns\n    -------\n    ndarray:\n        List of frequencies matching the tidal names\n\n    Examples\n    --------\n    Display the period (in hours) of M2 and S2\n\n    >>> 1 / frequency([\'M2\', \'S2\'])\n    array([ 12.4206012,  12.       ])\n\n    """"""\n    return np.array([__const[x].freq for x in _set_tides(tides)])\n\n\ndef __astron(ctime):\n    """"""\n    PRIVATE METHOD\n    --------------\n        This subroutine calculates the following five ephermides of the sun\n        and moon following t_tide\'s t_astron.\n                h = mean longitude of the sum\n                pp = mean longitude of the solar perigee\n                s = mean longitude of the moon\n                p = mean longitude of the lunar perigee\n                np = negative of the longitude of the mean ascending node\n        and their rates of change. Units for the ephermides are cycles and\n        for their derivatives are cycles/365 days\n        The formulae for calculating this ephermides were taken from pages 98\n        and 107 of the Explanatory Supplement to the Astronomical\n        Ephermeris and the American Ephermis and Nautical Almanac (1961).\n    """"""\n    # Compute number of days from epoch of 12:00 UT Dec 31, 1899.\n    dt = ctime - __reftime\n    d = dt.days + dt.seconds / 86400\n\n    # Coefficients used in t_tide\n    sc = np.array([270.434164, 13.1763965268, -0.0000850, 0.000000039])\n    hc = np.array([279.696678, 0.9856473354, 0.00002267, 0.000000000])\n    pc = np.array([334.329556, 0.1114040803, -0.0007739, -0.00000026])\n    npc = np.array([-259.183275, 0.0529539222, -0.0001557, -0.000000050])\n    ppc = np.array([281.220844, 0.0000470684, 0.0000339, 0.000000070])\n    coefs = np.vstack((sc, hc, pc, npc, ppc))\n\n    # output variables\n    astro = np.empty(6,)\n    ader = np.empty(6,)\n\n    # Compute astronomical constants at ctime; we only need the fractional\n    # part of the cycle.\n    D = d * 1e-4\n    args = np.vstack((1, d, D**2, D**3))\n    astro[1:] = (np.remainder(np.dot(coefs, args) / 360, 1)).ravel()\n\n    # Compute lunar time tau, based on fractional part of solar day.\n    # Add the hour angle to the longitude of the sun and subtract the\n    # longitude of the moon.\n    astro[0] = (ctime - datetime.datetime(ctime.year, ctime.month, ctime.day)) / \\\n        datetime.timedelta(1) + astro[2] - astro[1]\n\n    # Compute rates of change.\n    dargs = np.vstack((0, 1, 2e-4 * D, 3e-4 * D**2))\n\n    ader[1:] = (np.dot(coefs, dargs) / 360).ravel()\n    ader[0] = 1.0 + ader[2] - ader[1]\n\n    return astro, ader\n\n\ndef vuf(time, tides=None, lat=55):\n    """"""\n    Returns the astronomical phase V, the nodal phase modulation U, and\n    the nodal amplitude correction F at ctime for the requested\n    constituents and latitude.\n\n    Note, there are differences compared to t_tide:\n        - That V and U are returned as radians, not cycles.\n        - Shallow water constituents are not supported.\n\n    Parameters\n    ----------\n    time : datetime,\n        Time for the nodal correction\n    tides : string or list of strings, optional,\n        Names of tidal constituents\n    lat : float\n        Latitude for nodal correction\n\n    Returns\n    -------\n    dict :\n        dictionary of vuf corrections with the tide names as keys\n    """"""\n    # If the user tries to pass a list, we only want the mid-point;\n    # otherwise, just use what was sent\n    try:\n        time = time[np.floor(len(time) / 2)]\n    except:\n        pass\n\n    tides = _set_tides(tides)\n\n    # Calculate astronomical arguments at the requested time (mid-point of\n    # timeseries).\n    astro, ader = __astron(time)\n\n    # According to t_tide, the second order terms in the tidal potential\n    # go to zero at the equator, but the third order terms do not. Hence,\n    # when trying to infer the third-order terms from the second-order\n    # terms the nodal correction factors blow up. In order to prevent\n    # this it is assumed that the equatorial forcing at the equator is\n    # due to second-order forcing off the equator from about the 5 degree\n    # location. Latitudes are hence (somewhat arbitrarily) forced to be\n    # no closer than 5 degrees to the equator.\n    if(np.abs(lat) < 5.0):\n        lat = 5 * np.sign(lat + np.finfo(\'float\').eps)\n    slat = np.sin(lat * np.pi / 180.)\n\n    # Setup output dictionaries\n    vufs = {}\n\n    for c in tides:\n        shtides = __const[c].shallow.iname if __const[\n            c].shallow.isshallow else [c]\n        v, u, f = 0, 0, 1\n        for i, s in enumerate(shtides):\n            vtemp = np.fmod(np.dot(__const[s].doodson, astro) +\n                            __const[s].semi, 1) * (2 * np.pi)\n\n            fsum = 1\n            for k in range(len(__const[s].sat.ilatfac)):\n                uu = np.fmod(np.dot(__const[s].sat.deldood[k, :], astro[3:])\n                             + __const[s].sat.phcorr[k], 1)\n                rr = {\n                    0: __const[s].sat.amprat[k],\n                    1: __const[s].sat.amprat[k] * 0.36309 *\n                    (1.0 - 5.0 * slat**2) / slat,\n                    2: __const[s].sat.amprat[k] * 2.59808 * slat\n                }.get(__const[s].sat.ilatfac[k], 0)\n                fsum += rr * np.exp(1j * 2 * np.pi * uu)\n            ftemp = np.absolute(fsum)\n            utemp = np.angle(fsum)\n\n            if __const[c].shallow.isshallow:\n                v += vtemp * __const[c].shallow.coef[i]\n                u += utemp * __const[c].shallow.coef[i]\n                f *= ftemp**np.abs(__const[c].shallow.coef[i])\n            else:\n                v, u, f = vtemp, utemp, ftemp\n        vufs[c] = vuf_vals(v, u, f)\n\n    return vufs\n\n\ndef vel_ellipse(u, v):\n    """"""\n    Generate ellipse parameters from the U and V amplitude and phases\n\n    Parameters\n    ----------\n    u : dict,\n       Dictionary of the u-component of velocity tides with the\n       constituent name as a key, and the value is an amp_phase namedtuple.\n    v : dict,\n       Dictionary of the v-component of velocity tides with the\n       constituent name as a key, and the value is an amp_phase namedtuple.\n\n    Returns\n    -------\n    ellipse: dict,\n       Dictionary of the tidal ellipses with the constituent name as the\n       keys and the values are the parameters of the tellipse namedtuple.\n\n    Examples\n    --------\n    With a set of u and v amplitude and phases, generate the ellipse\n    parameters:\n    >>> u = {""M2"":amp_phase(4.3, np.radians(231.4))}\n    >>> v = {""M2"":amp_phase(0.7, np.radians(10.1))}\n    >>> ell = vel_ellipse(u, v)\n    >>> print(ell)\n    {\'M2\': ellipse(major=4.3324053381635519, minor=0.45854551121121889,\n     angle=6.1601050372480319, phase=4.0255995338808006)}\n    """"""\n    ell = {}\n    for c in u:\n        # Compute the parameters of the tide\n        au = u[c].amp * np.exp(-1j * u[c].phase)\n        av = v[c].amp * np.exp(-1j * v[c].phase)\n        rccw = (au + 1j * av) / 2.0\n        rcw = ((au - 1j * av) / 2.0).conjugate()\n        theta_ccw = np.angle(rccw)\n        theta_cw = np.angle(rcw)\n        rccw = np.abs(rccw)\n        rcw = np.abs(rcw)\n\n        # Set the ellipse parameters\n        major = rccw + rcw\n        minor = rccw - rcw\n        phase = (theta_cw - theta_ccw) / 2.0\n        angle = (theta_cw + theta_ccw) / 2.0\n        phase = np.mod(phase, 2 * np.pi) if phase > 0 else phase + 2 * np.pi\n        angle = np.mod(angle, 2 * np.pi) if angle > 0 else angle + 2 * np.pi\n\n        # Store the result\n        ell[c.upper()] = tellipse(major, minor, angle, phase)\n\n    return ell\n\n\ndef predict(times, tide, tide_minor=None, lat=55, tide_start=None):\n    """"""\n    Generate a tidal time-series for the given tides. Nodal correction\n    is applied for the time as well as the given latitude (if specified).\n\n    Parameters\n    ----------\n    times : datetime array,\n        The times of the predicted tide(s)\n    tide : dict,\n        Dictionary of the tides to predict with the constituent name as\n        the key, and the value is an amp_phase namedtuple.\n    tide_minor : dict optional,\n        Dictionary of the minor axis amplitude and angle to predict with\n        the constituent name as the key, and the value is an amp_phase namedtuple.\n    lat : float optional,\n        latitude of the nodal correction\n    tide_start : datetime optional,\n        If specified, the nodal correction are applied to adjust from\n        the tide_start to the center of the times record\n\n    Returns\n    -------\n    ndarray :\n        Values of the total tidal prediction of all constituents provided\n\n    Examples\n    --------\n    Generate a time series every hour for the month of April, 2014 for\n    a specific phase and amplitude of M2 and K1.\n\n    >>> times = [ datetime(2014,4,1) + timedelta(t/24) for t in range(31*24) ]\n    >>> tide = {\'M2\': amp_phase(2.3, np.radians(22)),\n                \'K1\': amp_phase(0.4, np.radians(227))}\n    >>> z = predict(times, tide, lat=23)\n\n\n    """"""\n    times = np.atleast_1d(times)\n    clist = list(tide.keys())\n    freq = frequency(clist)\n\n    # If given a tide_start, then the phase is relative to that datetime,\n    # and no corrections need to be applied; furthermore, the times to predict\n    # are relative to tide_start.\n    #\n    # If no tide_start is given, then everything is done as per standard.\n    if tide_start:\n        vufs = dict((ap.upper(), vuf_vals(0, 0, 1)) for ap in clist)\n        hours = np.array(\n            [(t - tide_start).total_seconds() / 3600.0 for t in times])\n    else:\n        ctime = times[0] + (times[-1] - times[0]) / 2\n        vufs = vuf(ctime, clist, lat)\n        hours = np.array([(t - ctime).total_seconds() / 3600.0 for t in times])\n\n    # Calculate time series\n    ts = np.zeros(len(times))\n    if tide_minor:\n        ts = np.zeros(len(times), dtype=np.complex)\n    for i, ap in enumerate(tide):\n        c = tide[ap]\n        ap = ap.upper()\n        if tide_minor:\n            m = tide_minor[ap]\n\n            ts += np.exp(1j * m.phase) * (\n                c.amp * vufs[ap].f * np.cos(2.0 * np.pi * np.dot(freq[i], hours)\n                                            + (vufs[ap].v + vufs[ap].u) - c.phase)\n                + m.amp * vufs[ap].f * np.sin(2.0 * np.pi * np.dot(freq[i], hours)\n                                            + (vufs[ap].v + vufs[ap].u) - c.phase))\n        else:\n            ts += c.amp * vufs[ap].f * np.cos(2.0 * np.pi * np.dot(freq[i], hours)\n                                              + (vufs[ap].v + vufs[ap].u) - c.phase)\n\n    return ts\n\n\ndef fit(times, xin, tides=None, lat=55, tide_start=None, trend=True):\n    """"""\n    Perform a harmonic fit of tidal constituents to a time-series of data. The\n    series can be unevenly spaced in time, but every time must be specified.\n    Note that returned amplitudes and phases are zero if time series is not long\n    enough to resolve specific tides\n\n    Parameters\n    ----------\n    times : datetime array,\n        List of times matching each value of the time-series input\n    xin : ndarray,\n        Data values to perform the harmonic fit upon\n    tides : string or list of strings, optional,\n        list of tidal constituents to fit to the data provided. If not\n        specified, the dominant 11 consituents are used:\n        [\'M4\', \'K2\', \'S2\', \'M2\', \'N2\', \'K1\', \'P1\', \'O1\', \'Q1\', \'MF\', \'MM\']\n    lat : float, optional,\n        latitude of the nodal correction\n    tide_start : datetime, optional,\n        If specified, the phases of the fit will be relative to the\n        given tide_start day.\n    trend : boolean, optional,\n        If True (default), adjust the fit for a linear time trend\n\n    Returns\n    -------\n    dict :\n        A dictionary of the results are returned with the following keys:\n        \'tide_start\': Reference date used as phase reference in fit\n        \'fit\': the time-series of the tidal fit\n        \'percent\': percentage of the signal explained by the tides\n        \'major\': dictionary of the major axis fit comprised of:\n            tidename: amp_phase namedtuple\n            Providing the amplitude and phase of each fit constituent\n        \'minor\': dictionary of the minor axis fit comprised of:\n            tidename: amp_phase namedtuple\n            Providing the minimum axis amplitude and angle of each fit constituent\n\n    Examples\n    --------\n    Fit the \'M2\' and \'K1\' to the given time-series, x.\n\n    >>> data = np.load(\'water_level.npz\')\n    >>> out = fit(data.times, data.level, [\'M2\',\'K1\'], lat=28)\n    >>> plt.plot(data.times, out[\'fit\'])\n    """"""\n\n    xin = np.atleast_1d(xin).flatten()\n    if len(xin) != len(times):\n        raise ValueError(""The times and input data must be of same size."")\n    tides = _set_tides(tides)\n\n    # Exclude long period tides if time series not long enough\n    freq = frequency(tides)\n    total_tides = len(tides)\n    total_hours = (times[-1] - times[0]).total_seconds() / 3600\n    invalid_tides = [t for t, f in zip(tides, 1 / freq) if 2 * f > total_hours]\n    tides = [t for t in tides if t not in invalid_tides]\n    freq = frequency(tides)\n\n    # time series as hours from ctime\n    ctime = times[0] + (times[-1] - times[0]) / 2\n    hours = np.array([(t - ctime).total_seconds() / 3600.0 for t in times])\n\n    # Generate cosines and sines for all the requested constitutents.\n    if trend:\n        A = np.hstack([np.cos(2 * np.pi * np.outer(hours, freq)),\n                       np.sin(2 * np.pi * np.outer(hours, freq)),\n                       np.atleast_2d(hours).T,\n                       np.ones((len(hours), 1))])\n    else:\n        A = np.hstack([np.cos(2 * np.pi * np.outer(hours, freq)),\n                       np.sin(2 * np.pi * np.outer(hours, freq)),\n                       np.ones((len(hours), 1))])\n\n    # Calculate coefficients\n    ntides = len(tides)\n    coef = np.linalg.lstsq(A, xin, rcond=None)[0]\n    xout = np.dot(A[:, :2 * ntides], coef[:2 * ntides])\n\n    # Explained variance\n    var_exp = 100 * (np.cov(np.real(xout)) + np.cov(np.imag(xout))) / \\\n        (np.cov(np.real(xin)) + np.cov(np.imag(xin)))\n\n    # Calculate amplitude & phase\n    ap = (coef[:ntides] - 1j * coef[ntides:2 * ntides]) / 2.0\n    am = (coef[:ntides] + 1j * coef[ntides:2 * ntides]) / 2.0\n\n    # Nodal Corrections values\n    vufs = vuf(ctime, tides, lat)\n    if tide_start:\n        vufs_ref = vuf(tide_start, tides, lat)\n        for v in vufs:\n            vufs[v] = vuf_vals(np.mod(vufs[v].v - vufs_ref[v].v, 2.0 * np.pi),\n                               np.mod(vufs[v].u - vufs_ref[v].u, 2.0 * np.pi),\n                               vufs[v].f / vufs_ref[v].f)\n\n    # Compute major/minor axis amplitude and phase\n    maj_amp = np.zeros((total_tides,))\n    maj_pha = maj_amp.copy()\n    min_amp = maj_amp.copy()\n    min_pha = maj_amp.copy()\n    for i, c in enumerate(tides):\n        maj_amp[i] = (np.abs(ap[i]) + np.abs(am[i])) / vufs[c].f\n        min_amp[i] = (np.abs(ap[i]) - np.abs(am[i])) / vufs[c].f\n        min_pha[i] = np.mod(\n            ((np.angle(ap[i]) + np.angle(am[i])) / 2), np.pi)\n        maj_pha[i] = np.mod(vufs[c].v + vufs[c].u - np.angle(ap[i]) + min_pha[i],\n                            2.0 * np.pi)\n\n    return {\n        \'tide_start\': tide_start,\n        \'fit\': xout,\n        \'percent\': var_exp,\n        \'major\': pack_amp_phase(tides + invalid_tides, maj_amp, maj_pha),\n        \'minor\': pack_amp_phase(tides + invalid_tides, min_amp, min_pha)\n    }\n\n\ndef pack_amp_phase(tides, amp, phase):\n    """"""\n    Pack together the tides, amplitudes, and phases into a dictionary of\n    tide names as keys and amp_phase namedtuples as values\n\n    Parameters\n    ----------\n    tides : ndarray of strings,\n       List of constituent strings\n    amp : ndarray,\n       Amplitudes for each constituent\n    phase : ndarray,\n       phases (rad) for each constituent\n\n    Returns\n    -------\n       dict:  amplitudes and phases\n        constituent name as key and amp_phase namedtuple as value\n    """"""\n    tides = _set_tides(tides)\n    amp = np.atleast_1d(amp)\n    phase = np.atleast_1d(phase)\n    if np.any(phase > 2 * np.pi):\n        warn(""Phases appear to be degrees. Beware of results."")\n\n    amp_ph = {}\n    for i, c in enumerate(tides):\n        amp_ph[c] = amp_phase(amp[i], phase[i])\n\n    return amp_ph\n\n\ndef unpack_amp_phase(amp_ph, tides=None):\n    """"""\n    Given a dictionary of amplitude and phase parameters, unpack the dictionary\n    and return an amp_phase namedtuple with arrays for each element that\n    are in the order of the tides array.\n\n    Parameters\n    ----------\n    amp_ph : dict,\n      amp_phase dictionary returned by fit or pack_amp_phase\n    tides: list of strings,\n      List of strings to provide the order of the arrays\n\n    Returns\n    -------\n      amp_phase : namedtuple\n\n    """"""\n    tides = _set_tides(tides)\n    am = np.zeros((len(tides),))\n    ph = am.copy()\n\n    for n, t in enumerate(tides):\n        try:\n            am[n] = amp_ph[t].amp\n            ph[n] = amp_ph[t].phase\n        except KeyError:\n            continue\n    return amp_phase(am, ph)\n\n\ndef unpack_vuf(vuf, tides=None):\n    """"""\n    Given a dictionary of vuf parameters, unpack the dictionary\n    and return a vuf_vals namedtuple with arrays for each element that\n    are in the order of the tides array.\n\n    Parameters\n    ----------\n    vuf : dict,\n      vuf_vals dictionary returned by vuf\n    tides: list of strings,\n      List of strings to provide the order of the arrays\n\n    Returns\n    -------\n      vuf_vals : namedtuple\n\n    """"""\n    tides = _set_tides(tides)\n    v = np.zeros((len(tides),))\n    u = v.copy()\n    f = v.copy()\n\n    for n, t in enumerate(tides):\n        try:\n            v[n] = vuf[t].v\n            u[n] = vuf[t].u\n            f[n] = vuf[t].f\n        except KeyError:\n            continue\n    return vuf_vals(v, u, f)\n\n\ndef unpack_ellipse(ellipse, tides=None):\n    """"""\n    Given a dictionary of tidal ellipse parameters, unpack the dictionary\n    and return an ellipse namedtuple with arrays for each element that\n    are in the order of the tides array.\n\n    Parameters\n    ----------\n    ellipse : dict,\n      Ellipse dictionary returned by tidal_ellipse\n    tides: list of strings,\n      List of strings to provide the order of the arrays\n\n    Returns\n    -------\n      tellipse : namedtuple\n\n    """"""\n    tides = _set_tides(tides)\n    mj = np.zeros((len(tides),))\n    mn = mj.copy()\n    ph = mj.copy()\n    ag = mj.copy()\n\n    for n, t in enumerate(tides):\n        try:\n            mj[n] = ellipse[t].major\n            mn[n] = ellipse[t].minor\n            ph[n] = ellipse[t].phase\n            ag[n] = ellipse[t].angle\n        except KeyError:\n            continue\n    return tellipse(mj, mn, ag, ph)\n'"
seapy/timeout.py,0,"b'#!/usr/bin/env python\n""""""\n  Wrap your code with a time limit to prevent something from taking too long\n  (getting into an infinite loop, etc.)\n\n  **Examples**\n\n  >>> from timeout import timeout\n  >>> with timeout(seconds=3):\n  >>>     do something\n\n  Taken and slightly modified from Thomas Ahle at:\n  <http://stackoverflow.com/questions/2281850/timeout-function-if-it-takes-too-long-to-finish>\n\n""""""\n\n\nimport errno\nimport os\nimport signal\n\nclass TimeoutError(Exception):\n    pass\n\nclass timeout:\n    def __init__(self, seconds=1, minutes=None, error_message=\'Timeout\'):\n        self.seconds = seconds\n        if minutes is not None:\n            self.seconds = minutes*60\n        self.error_message = error_message\n    def handle_timeout(self, signum, frame):\n        raise TimeoutError(self.error_message)\n    def __enter__(self):\n        signal.signal(signal.SIGALRM, self.handle_timeout)\n        signal.alarm(self.seconds)\n    def __exit__(self, type, value, traceback):\n        signal.alarm(0)\n\n\n'"
seapy/model/__init__.py,0,"b'""""""\n  State Estimation and Analysis for PYthon\n\n    Module for working with oceanographic data and models\n\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n\n  Import classes include:\n\n  - :class:`~seapy.model.grid`\n\n  Imported functions include:\n\n  - :func:`~seapy.model.lib.bvf`\n  - :func:`~seapy.model.lib.density`\n  - :func:`~seapy.model.hycom.load_history`\n  - :func:`~seapy.model.soda.load_history`\n  - :func:`~seapy.model.lib.pressure`\n  - :func:`~seapy.model.lib.rho2u`\n  - :func:`~seapy.model.lib.rho2v`\n  - :func:`~seapy.model.lib.sound`\n  - :func:`~seapy.model.lib.u2rho`\n  - :func:`~seapy.model.lib.v2rho`\n  - :func:`~seapy.model.lib.v2rho`\n  - :func:`~seapy.model.lib.w`\n""""""\nfrom .grid import grid, asgrid\nfrom .lib import *\nfrom .hycom import *\nfrom .soda import *\n'"
seapy/model/grid.py,66,"b'#!/usr/bin/env python\n""""""\n  grid\n\n  This module handles general model grid information, whether from ROMS or\n  other models; however, it is mostly geared towards ROMS\n\n  Written by Brian Powell on 10/09/13\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n\n  **Examples**\n\n  >>> grid = seapy.model.asgrid(""grid_file.nc"")\n\n""""""\n\n\nimport os\nimport re\nimport seapy\nimport numpy as np\nimport scipy.spatial\nimport matplotlib.path\nimport netCDF4\nfrom warnings import warn\n\n\ndef asgrid(grid):\n    """"""\n    Return either an existing or new grid object. This decorator will ensure that\n    the variable being used is a seapy.model.grid. If it is not, it will attempt\n    to construct a new grid with the variable passed.\n\n    Parameters\n    ----------\n    grid: string, list, netCDF4 Dataset, or model.seapy.grid\n        Input variable to cast. If it is already a grid, it will return it;\n        otherwise, it attempts to construct a new grid.\n\n    Returns\n    -------\n    seapy.model.grid\n\n    """"""\n    if grid is None:\n        raise AttributeError(""No grid was specified"")\n    if isinstance(grid, seapy.model.grid):\n        return grid\n    if isinstance(grid, netCDF4._netCDF4.Dataset):\n        return seapy.model.grid(nc=grid)\n    else:\n        return seapy.model.grid(filename=grid)\n\n\nclass grid:\n\n    def __init__(self, filename=None, nc=None, lat=None, lon=None, z=None,\n                 depths=True, cgrid=False):\n        """"""\n            Class to wrap around a numerical model grid for oceanography.\n            It attempts to track latitude, longitude, z, and other\n            parameters. A grid can be constructed by specifying a filename or\n            by specifying lat, lon, and z.\n\n            Parameters\n            ----------\n            filename: filename or list, optional\n              name to load to build data structure\n                or\n            nc: netCDF4 Dataset, optional\n              If a file is already open, pass the reference.\n            lat: ndarray,\n                latitude values of grid\n            lon: ndarray,\n                longitude values of grid\n            z : ndarray,\n                z-level depths of grid\n\n            Options\n            -------\n            depths: ndarray,\n                Set the depths of the grid [True]\n            cgrid: bool,\n                Whether the grid is an Arakawa C-Grid [False]\n        """"""\n        self.filename = filename\n        self.cgrid = cgrid\n        self._nc = nc\n\n        if (self.filename or self._nc) is not None:\n            self._initfile()\n            self._isroms = True if \\\n                (len(list(set((""s_rho"", ""pm"", ""pn"", ""theta_s"", ""theta_b"",\n                               ""vtransform"", ""vstretching"")).intersection(\n                    set(self.__dict__)))) > 0) else False\n            self.cgrid = True if self._isroms else self.cgrid\n        else:\n            self._nc = None\n            self._isroms = False\n            self.lat_rho = lat\n            self.lon_rho = lon\n            self.z = z\n            self.cgrid = False\n        self._verify_shape()\n        if depths:\n            self.set_dims()\n            self.set_depth()\n            self.set_thickness()\n            self.set_mask_h()\n        self.ijinterp = None\n        self.llinterp = None\n\n    def _initfile(self):\n        """"""\n        Using an input file, try to load as much information\n        as can be found in the given file.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None : sets attributes in grid\n\n        """"""\n        # Define a dictionary to go through and convert netcdf variables\n        # to internal class attributes\n        gvars = {""lat_rho"": [""lat_rho"", ""lat"", ""latitude"", ""y_rho"", ""geolat_t""],\n                 ""lon_rho"": [""lon_rho"", ""lon"", ""longitude"", ""x_rho"", ""geolon_t""],\n                 ""lat_u"": [""lat_u"", ""y_u"", ""geolat_u""],\n                 ""lon_u"": [""lon_u"", ""x_u"", ""geolon_u""],\n                 ""lat_v"": [""lat_v"", ""y_v"", ""geolat_u""],\n                 ""lon_v"": [""lon_v"", ""x_v"", ""geolon_u""],\n                 ""mask_rho"": [""mask_rho"", ""mask""],\n                 ""mask_u"": [""mask_u""],\n                 ""mask_v"": [""mask_v""],\n                 ""angle"": [""angle""],\n                 ""h"": [""h""],\n                 ""n"": [""n""],\n                 ""theta_s"": [""theta_s""],\n                 ""theta_b"": [""theta_b""],\n                 ""tcline"": [""tcline""],\n                 ""hc"": [""hc""],\n                 ""vtransform"": [""vtransform""],\n                 ""vstretching"": [""vstretching""],\n                 ""s_rho"": [""s_rho""],\n                 ""cs_r"": [""cs_r""],\n                 ""f"": [""f""],\n                 ""pm"": [""pm""],\n                 ""pn"": [""pn""],\n                 ""z"": [""z"", ""depth"", ""lev"", ""st_ocean""],\n                 ""wtype_grid"": [""mask_rho""],\n                 ""rdrag"": [""rdrag""],\n                 ""rdrag2"": [""rdrag2""],\n                 ""diff_factor"": [""diff_factor""],\n                 ""visc_factor"": [""visc_factor""]\n                 }\n\n        # Open the file\n        close = False\n        if self._nc is None:\n            close = True\n            self._nc = seapy.netcdf(self.filename)\n        try:\n            self.name = re.search(""[^\\.]*"",\n                                  os.path.basename(self.filename)).group()\n        except:\n            self.name = ""untitled""\n        self.key = {}\n        ncvars = {v.lower(): v for v in self._nc.variables.keys()}\n        for var in gvars:\n            for inp in gvars[var]:\n                if inp in ncvars:\n                    self.key[var] = inp\n                    self.__dict__[var] = self._nc.variables[ncvars[inp]][:]\n                    break\n\n        if close:\n            # Close the file\n            self._nc.close()\n            self._nc = None\n\n    def _verify_shape(self):\n        """"""\n        Verify the dimensionality of the system, create variables that\n        can be generated from the others if they aren\'t already loaded\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None : sets attributes in grid\n        """"""\n        # Check that we have the minimum required data\n        if (""lat_rho"" or ""lon_rho"") not in self.__dict__:\n            raise AttributeError(\n                ""grid does not have attribute lat_rho or lon_rho"")\n\n        # Check that it is formatted into 2-D\n        self.spatial_dims = self.lat_rho.ndim\n        if self.lat_rho.ndim == 1 and self.lon_rho.ndim == 1:\n            [self.lon_rho, self.lat_rho] = np.meshgrid(self.lon_rho,\n                                                       self.lat_rho)\n\n        # Compute the dimensions\n        self.ln = int(self.lat_rho.shape[0])\n        self.lm = int(self.lat_rho.shape[1])\n        self.shape = (self.ln, self.lm)\n        if self.cgrid:\n            self.shape_u = (self.ln, self.lm - 1)\n            self.shape_v = (self.ln - 1, self.lm)\n        else:\n            self.shape_u = self.shape_v = self.shape\n\n    def __repr__(self):\n        return ""{:s}: {:d}x{:d}x{:d}"".format(""C-Grid"" if self.cgrid\n                                             else ""A-Grid"", self.n, self.ln, self.lm)\n\n    def __str__(self):\n        return ""\\n"".join((self.filename if self.filename else ""Constructed"",\n                          ""{:d}x{:d}x{:d}: {:s} with {:s}"".format(\n                              self.n, self.ln, self.lm,\n                              ""C-Grid"" if self.cgrid else ""A-Grid"",\n                              ""S-level"" if self._isroms else ""Z-Level""),\n                          ""Available: "" + "","".join(sorted(\n                              list(self.__dict__.keys())))))\n\n    def east(self):\n        """"""\n        Test the longitude convention of the grid. If there are negative\n        values, then east is False. If there are only positive values then\n        assume that east is True.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        east : bool,\n            True - The convention is all positive values to the east\n            False - The convention is positive values east and negative west\n        """"""\n        return np.min(self.lon_rho > 0)\n\n    def set_east(self, east=False):\n        """"""\n        When working with various other grids, we may want the longitudes\n        to be consistent. This can be changed by setting the east to be\n        either True or False. If False, then longitudes will be positive east\n        and negative west. If True, only positive east.\n\n        Parameters\n        ----------\n        east : bool,\n            True - all longitudes are positive\n            False - longitudes are positive east and negative west\n\n        Returns\n        -------\n        None : sets attributes in grid\n        """"""\n        try:\n            if east:\n                self.lon_rho[self.lon_rho < 0] += 360.0\n                self.lon_u[self.lon_u < 0] += 360.0\n                self.lon_v[self.lon_v < 0] += 360.0\n            else:\n                self.lon_rho[self.lon_rho > 180] -= 360.0\n                self.lon_u[self.lon_u > 180] -= 360.0\n                self.lon_v[self.lon_v > 180] -= 360.0\n        except:\n            pass\n\n    def set_dims(self):\n        """"""\n        Compute the dimension attributes of the grid based upon the information provided.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None : sets attributes in grid\n        """"""\n        # If C-Grid, set the dimensions for consistency\n        if self.cgrid:\n            self.eta_rho = self.ln\n            self.eta_u = self.ln\n            self.eta_v = self.ln - 1\n            self.xi_rho = self.lm\n            self.xi_u = self.lm - 1\n            self.xi_v = self.lm\n\n        # Set the number of layers\n        if ""n"" not in self.__dict__:\n            if ""s_rho"" in self.__dict__:\n                self.n = int(self.s_rho.size)\n            elif ""z"" in self.__dict__:\n                self.n = int(self.z.size)\n            else:\n                self.n = 1\n                self.z = np.zeros(self.lat_rho.shape)\n        else:\n            self.n = int(self.n)\n\n        # Generate the u- and v-grids\n        if (""lat_u"" or ""lon_u"") not in self.__dict__:\n            if self.cgrid:\n                self.lat_u = 0.5 * \\\n                    (self.lat_rho[:, 1:] - self.lat_rho[:, 0:-1])\n                self.lon_u = 0.5 * \\\n                    (self.lon_rho[:, 1:] - self.lon_rho[:, 0:-1])\n            else:\n                self.lat_u = self.lat_rho\n                self.lon_u = self.lon_rho\n        if (""lat_v"" or ""lon_v"") not in self.__dict__:\n            if self.cgrid:\n                self.lat_v = 0.5 * \\\n                    (self.lat_rho[1:, :] - self.lat_rho[0:-1, :])\n                self.lon_v = 0.5 * \\\n                    (self.lon_rho[1:, :] - self.lon_rho[0:-1, :])\n            else:\n                self.lat_v = self.lat_rho\n                self.lon_v = self.lon_rho\n        if ""mask_rho"" in self.__dict__:\n            if ""mask_u"" not in self.__dict__:\n                if self.cgrid:\n                    self.mask_u = self.mask_rho[:, 1:] * self.mask_rho[:, 0:-1]\n                else:\n                    self.mask_u = self.mask_rho\n            if ""mask_v"" not in self.__dict__:\n                if self.cgrid:\n                    self.mask_v = self.mask_rho[1:, :] * self.mask_rho[0:-1, :]\n                else:\n                    self.mask_v = self.mask_rho\n\n        # Compute the resolution\n        if ""pm"" in self.__dict__:\n            self.dm = 1.0 / self.pm\n        else:\n            self.dm = np.ones(self.lon_rho.shape, dtype=np.float32)\n            self.dm[:, 0:-1] = seapy.earth_distance(self.lon_rho[:, 1:],\n                                                    self.lat_rho[:, 1:],\n                                                    self.lon_rho[:, 0:-1],\n                                                    self.lat_rho[:, 0:-1]).astype(np.float32)\n            self.dm[:, -1] = self.dm[:, -2]\n            self.pm = 1.0 / self.dm\n        if ""pn"" in self.__dict__:\n            self.dn = 1.0 / self.pn\n        else:\n            self.dn = np.ones(self.lat_rho.shape, dtype=np.float32)\n            self.dn[0:-1, :] = seapy.earth_distance(self.lon_rho[1:, :],\n                                                    self.lat_rho[1:, :],\n                                                    self.lon_rho[0:-1, :],\n                                                    self.lat_rho[0:-1, :]).astype(np.float32)\n            self.dn[-1, :] = self.dn[-2, :]\n            self.pn = 1.0 / self.dn\n\n        # Compute the Coriolis\n        if ""f"" not in self.__dict__:\n            omega = 2 * np.pi * seapy.secs2day\n            self.f = 2 * omega * np.sin(np.radians(self.lat_rho))\n\n        # Set the grid index coordinates\n        self.I, self.J = np.meshgrid(\n            np.arange(0, self.lm), np.arange(0, self.ln))\n\n    def set_mask_h(self, fld=None):\n        """"""\n        Compute the mask and h array from a z-level model\n\n        Parameters\n        ----------\n        fld : np.array\n            3D array of values (such as temperature) to analyze to determine\n            where the bottom and land lie\n\n        Returns\n        -------\n        None : sets mask and h attributes in grid\n\n        """"""\n        if hasattr(self, ""mask_rho"") or self.cgrid:\n            return\n        if fld is None and self.filename is not None:\n            if self._nc is None:\n                self._nc = seapy.netcdf(self.filename)\n\n            # Try to load a field from the file\n            for f in [""temp"", ""temperature"", ""water_temp"", ""fed""]:\n                if f in self._nc.variables:\n                    fld = self._nc.variables[f][0, :, :, :]\n                    fld = np.ma.array(fld, mask=np.isnan(fld))\n                    break\n\n            # Close the file\n            self._nc.close()\n\n        # If we don\'t have a field to examine, then we cannot compute the\n        # mask and bathymetry\n        if fld is None:\n            warn(""Missing 3D field to evaluate."")\n            return\n\n        # Next, we go over the field to examine the depths and mask\n        self.h = np.zeros(self.lat_rho.shape)\n        self.mask_rho = np.zeros(self.lat_rho.shape)\n        for k in range(self.z.size):\n            water = np.nonzero(np.logical_not(fld.mask[k, :, :]))\n            self.h[water] = self.z[k]\n            if k == 0:\n                self.mask_rho[water] = 1.0\n        self.mask_u = self.mask_v = self.mask_rho\n\n    def set_depth(self, force=False):\n        """"""\n        Compute the depth of each cell for the model grid.\n\n        Parameters\n        ----------\n        force : boolean, default False\n                If True, force the update of the depths\n\n        Returns\n        -------\n        None : sets depth attributes in grid\n        """"""\n        try:\n            if self._isroms:\n                if ""s_rho"" not in self.__dict__ or \\\n                   ""cs_r"" not in self.__dict__ or force:\n                    self.s_rho, self.cs_r = seapy.roms.stretching(\n                        self.vstretching, self.theta_s, self.theta_b,\n                        self.hc, self.n)\n                self.depth_rho = seapy.roms.depth(\n                    self.vtransform, self.h, self.hc, self.s_rho, self.cs_r)\n                self.depth_u = seapy.model.rho2u(self.depth_rho).filled(0)\n                self.depth_v = seapy.model.rho2v(self.depth_rho).filled(0)\n            else:\n                d = self.z.copy()\n                l = np.nonzero(d > 0)\n                d[l] = -d[l]\n                if self.n > 1:\n                    self.depth_rho = np.kron(np.kron(\n                        d, np.ones(self.lon_rho.shape[1])),\n                        np.ones(self.lon_rho.shape[0])).reshape(\n                        [self.z.size, self.lon_rho.shape[0],\n                         self.lon_rho.shape[1]])\n                else:\n                    self.depth_rho = self.z\n                if self.cgrid:\n                    self.depth_u = seapy.model.rho2u(self.depth_rho).filled(0)\n                    self.depth_v = seapy.model.rho2v(self.depth_rho).filled(0)\n                else:\n                    self.depth_u = self.depth_rho\n                    self.depth_v = self.depth_rho\n        except (AttributeError, ValueError):\n            warn(""could not compute grid depths."")\n            pass\n\n    def set_thickness(self):\n        """"""\n        Compute the thickness of each cell for the model grid.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None : sets thick attributes in grid\n        """"""\n        if ""n"" not in self.__dict__:\n            self.set_dims()\n        if self.n == 1:\n            return\n        try:\n            if self._isroms:\n                s_w, cs_w = seapy.roms.stretching(\n                    self.vstretching, self.theta_s, self.theta_b, self.hc,\n                    self.n, w_grid=True)\n                self.thick_rho = seapy.roms.thickness(\n                    self.vtransform, self.h, self.hc, s_w, cs_w)\n                self.thick_u = seapy.model.rho2u(self.thick_rho)\n                self.thick_v = seapy.model.rho2v(self.thick_rho)\n            else:\n                d = np.abs(self.z.copy())\n                w = d * 0\n                # Check which way the depths are going\n                if d[0] < d[-1]:\n                    w[0] = d[0]\n                    w[1:] = d[1:] - d[0:-1]\n                else:\n                    w[-1] = d[-1]\n                    w[0:-1] = d[0:-1] - d[1:]\n\n                self.thick_rho = np.kron(np.kron(w,\n                                                 np.ones(self.lon_rho.shape[1])),\n                                         np.ones(self.lon_rho.shape[0])).reshape(\n                    [self.z.size, self.lon_rho.shape[0],\n                     self.lon_rho.shape[1]])\n                if self.cgrid:\n                    self.thick_u = seapy.model.rho2u(self.thick_rho)\n                    self.thick_v = seapy.model.rho2v(self.thick_rho)\n                else:\n                    self.thick_u = self.thick_rho\n                    self.thick_v = self.thick_rho\n        except AttributeError:\n            warn(""could not compute grid thicknesses."")\n            pass\n\n    def plot_trace(self, basemap=None, **kwargs):\n        """"""\n        Trace the boundary of the grid onto a map projection\n\n        Parameters\n        ----------\n        basemap: basemap instance\n            The basemap instance to use for drawing\n        **kwargs: optional\n            Arguments to pass to the plot routine\n\n        Returns\n        -------\n        None\n        """"""\n        lon = np.concatenate([self.lon_rho[0, :], self.lon_rho[:, -1],\n                              self.lon_rho[-1, ::-1], self.lon_rho[::-1, 0]])\n        lat = np.concatenate([self.lat_rho[0, :], self.lat_rho[:, -1],\n                              self.lat_rho[-1, ::-1], self.lat_rho[::-1, 0]])\n        if basemap:\n            x, y = basemap(lon, lat)\n            basemap.plot(x, y, **kwargs)\n        else:\n            from matplotlib import pyplot\n            pyplot.plot(lon, lat, **kwargs)\n\n    def plot_depths(self, row=None, col=None, ax=None):\n        """"""\n        Plot the depths of a model grid along a row or column transect.\n        If the bathymetry is known, it is plotted also.\n\n        Parameters\n        ----------\n        row : int, optional\n          The row number to plot\n        col : int, optional\n          The column number to plot\n        ax : matplotlib.axes, optional\n          The axes to use for the figure\n\n        Returns\n        -------\n        ax : matplotlib.axes\n          The axes containing the plot\n        """"""\n        import matplotlib.pyplot as plt\n\n        # Create the axes if we don\'t have any\n        if not ax:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n            # ax.set_bg_color(\'darkseagreen\')\n\n        # Get the data\n        if row:\n            sz = np.s_[:, row, :]\n            s = np.s_[row, :]\n            x = self.lon_rho[s]\n            label = ""Longitude""\n        elif col:\n            sz = np.s_[:, :, col]\n            s = np.s_[:, col]\n            x = self.lat_rho[s]\n            label = ""Latitude""\n        else:\n            warn(""You must specify a row or column"")\n            return\n\n        # If it is ROMS, we should plot the top and bottom of the cells\n        if self._isroms:\n            sr, csr = seapy.roms.stretching(\n                self.vstretching, self.theta_s, self.theta_b,\n                self.hc, self.n, w_grid=True)\n            dep = np.ma.masked_where(seapy.adddim(self.mask_rho[s],\n                                                  self.n + 1) == 0,\n                                     seapy.roms.depth(self.vtransform,\n                                                      self.h[s], self.hc,\n                                                      sr, csr,\n                                                      w_grid=True))\n        else:\n            dep = np.ma.masked_where(seapy.adddim(self.mask_rho[s],\n                                                  self.n) == 0,\n                                     self.depth_rho[sz])\n        h = -self.h[s]\n\n        # Begin with the bathymetric data\n        ax.fill_between(x, h, np.min(h), facecolor=""darkseagreen"",\n                        interpolate=True)\n\n        # Plot the layers\n        ax.plot(x, dep.T, color=""grey"")\n\n        # Labels\n        ax.set_xlabel(label + "" [deg]"")\n        ax.set_ylabel(""Depth [m]"")\n\n        # Make it tight\n        plt.autoscale(ax, tight=True)\n\n        return ax\n\n    def to_netcdf(self, nc):\n        """"""\n        Write all available grid information into the records present in the\n        netcdf file.  This is used to pre-fill boundary, initial, etc. files\n        that require some of the grid information.\n\n        Parameters\n        ----------\n        nc : netCDF4\n            File to fill all known records from the grid information\n\n        Returns\n        -------\n        None\n\n        """"""\n        for var in nc.variables:\n            if hasattr(self, var.lower()):\n                nc.variables[var][:] = getattr(self, var.lower())\n\n    def nearest(self, lon, lat, grid=""rho""):\n        """"""\n        Find the indices nearest to each point in the given list of\n        longitudes and latitudes.\n\n        Parameters\n        ----------\n        lon : ndarray,\n            longitude of points to find\n        lat : ndarray\n            latitude of points to find\n        grid : string, optional,\n            ""rho"", ""u"", or ""v"" grid to search\n\n        Returns\n        -------\n        indices : tuple of ndarray\n            The indices for each dimension of the grid that are closest\n            to the lon/lat points specified\n        """"""\n\n        glat = getattr(self, ""lat_"" + grid)\n        glon = getattr(self, ""lon_"" + grid)\n        xy = np.dstack([glat.ravel(), glon.ravel()])[0]\n        pts = np.dstack([np.atleast_1d(lat), np.atleast_1d(lon)])[0]\n        grid_tree = scipy.spatial.cKDTree(xy)\n        dist, idx = grid_tree.query(pts)\n        return np.unravel_index(idx, glat.shape)\n\n    def ij(self, points):\n        """"""\n        Compute the fractional i,j indices of the grid from a\n        set of lon, lat points.\n\n        Parameters\n        ----------\n        points : list of tuples\n            longitude, latitude points to compute i, j indicies\n\n        Returns\n        -------\n        out : tuple of numpy masked array (with netcdf-type indexing),\n            list of j,i indices for the given lon, lat points. NOTE: values\n            that lie on the mask_rho are masked; however, if you wish to\n            ignore masking, you can use the data field (i.data) directly.\n            Values that do not lie within the grid are masked and stored as\n            np.nan.\n\n        Examples\n        --------\n        >>> a = ([-158, -160.5, -155.5], [20, 22.443, 19.5])\n        >>> idx = g.ij(a)\n        """"""\n\n        from seapy.external.hindices import hindices\n\n        # Interpolate the lat/lons onto the I, J\n        xgrid, ygrid = np.ma.masked_equal(hindices(self.angle.T,\n                                                   self.lon_rho.T, self.lat_rho.T,\n                                                   points[0], points[1]), -999.0)\n        mask = self.mask_rho[(ygrid.filled(0).astype(int),\n                              xgrid.filled(0).astype(int))]\n        xgrid[mask == 0] = np.ma.masked\n        ygrid[mask == 0] = np.ma.masked\n        return (ygrid, xgrid)\n\n    def ijk(self, points, depth_adjust=False):\n        """"""\n        Compute the fractional i, j, k indices of the grid from a\n        set of lon, lat, depth points.\n\n        Parameters\n        ----------\n        points : list of tuples,\n            longitude, latitude, depth points to compute i, j, k indicies.\n            NOTE: depth is in meters (defaults to negative)\n        depth_adjust : bool,\n            If True, depths that are deeper (shallower) than the grid are set\n            to the bottom (top) layer, 0 (N). If False, a nan value is used for\n            values beyond the grid depth. Default is False.\n\n        Returns\n        -------\n        out : tuple of numpy.maskedarray (with netcdf-type indexing),\n            list of k, j, i indices for the given lon, lat, depth points\n\n        Examples\n        --------\n        >>> a = ([-158, -160.5, -155.5], [20, 22.443, 19.5], [-10 -200 0])\n        >>> idx = g.ijk(a)\n\n        """"""\n        # NOTE: Attempted to use a 3D griddata, but it took over 2 minutes\n        # for each call, resulting in a 6minute runtime for this method\n        # Reverted to 2D i,j indices, then looping a 1-D interpolation\n        # to get depths for increased-speed (though this method is still slow)\n\n        from scipy.interpolate import interp1d\n\n        # Get the i,j points\n        (j, i) = self.ij((points[0], points[1]))\n        k = j * np.ma.masked\n        grid_k = np.arange(0, self.n)\n        depth = np.asanyarray(points[2])\n        depth[depth > 0] *= -1\n\n        # Determine the unique points\n        good = np.where(~np.logical_or(i.mask, j.mask))[0]\n        ii = np.floor(i[good]).astype(int)\n        jj = np.floor(j[good]).astype(int)\n        idx = seapy.unique_rows((jj, ii))\n        fill_value = 0 if depth_adjust else np.nan\n        for n in idx:\n            pts = np.where(np.logical_and(jj == jj[n], ii == ii[n]))\n            griddep = self.depth_rho[:, jj[n], ii[n]]\n            if griddep[0] < griddep[-1]:\n                griddep[-1] = 0.0\n            else:\n                griddep[0] = 0.0\n\n            fi = interp1d(griddep, grid_k, bounds_error=False,\n                          fill_value=fill_value)\n            k[good[pts]] = fi(depth[good][pts])\n\n        # Mask bad points\n        l = np.isnan(k.data)\n        i[l] = np.ma.masked\n        j[l] = np.ma.masked\n        k[l] = np.ma.masked\n\n        return (k, j, i)\n\n    def latlon(self, indices):\n        """"""\n        Compute the latitude and longitude from the given (i,j) indices\n        of the grid\n\n        Parameters\n        ----------\n        indices : list of tuples\n            i, j points to compute latitude and longitude\n\n        Returns\n        -------\n        out : tuple of ndarray\n            list of lat,lon points from the given i,j indices\n\n        Examples\n        --------\n        >>> a = [(23.4, 16.5), (3.66, 22.43)]\n        >>> idx = g.latlon(a)\n        """"""\n        from scipy.interpolate import RegularGridInterpolator\n\n        lati = RegularGridInterpolator((self.I[0, :], self.J[:, 0]),\n                                       self.lat_rho.T)\n        loni = RegularGridInterpolator((self.I[0, :], self.J[:, 0]),\n                                       self.lon_rho.T)\n\n        return (lati(indices), loni(indices))\n\n    def rfactor(self):\n        """"""\n        Return the 2D field of the r-factor for the given grid.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        ndarray:\n          array of r-factors size of the grid\n\n        """"""\n        hx = np.zeros(self.shape)\n        hy = hx.copy()\n        r = hx.copy()\n\n        hx[:, :-1] = np.abs(np.diff(self.h, axis=1) /\n                            (self.h[:, 1:] + self.h[:, :-1]))\n        hy[:-1, :] = np.abs(np.diff(self.h, axis=0) /\n                            (self.h[1:, :] + self.h[:-1, :]))\n        hx[:, :-1] *= self.mask_u\n        hy[:-1, :] *= self.mask_v\n\n        r[:-1, :-1] = np.maximum(np.maximum(hx[:-1, :-1], hx[:-1, 1:]),\n                                 np.maximum(hy[:-1, :-1], hy[1:, :-1]))\n        r[:, -1] = r[:, -2]\n        r[-1, :] = r[-2, :]\n        hx = hy = 0\n        return r * self.mask_rho\n\n    def dHdxy(self):\n        """"""\n        Calculate the spatial derivative of water depth in each direction\n        (xi and eta).\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        dHdxi : ndarray,\n          Slope in x-direction\n        dHdeta : ndarray,\n          Slope in eta-direction\n        """"""\n        dHdxi = np.zeros(self.h.shape)\n        dHdeta = np.zeros(self.h.shape)\n        dHdxi[:, :-1] = -np.diff(self.h, axis=1) * self.pm[:, 1:]\n        dHdxi[:, -1] = dHdxi[:, -2]\n        dHdeta[:-1, :] = -np.diff(self.h, axis=0) * self.pn[1:, :]\n        dHdeta[-1, :] = dHdeta[-2, :]\n\n        return dHdxi, dHdeta\n\n    def mask_poly(self, vertices, lat_lon=False, radius=0.0):\n        """"""\n        Create an np.masked_array of the same shape as the grid with values\n        masked if they are not within the given polygon vertices\n\n        Parameters\n        ----------\n        vertices: list of tuples,\n            points that define the vertices of the polygon\n        lat_lon : bool, optional,\n            If True, the vertices are a list of lon, lat points rather\n            than indexes\n\n        Returns\n        -------\n        mask : np.masked_array\n            mask of values that are located within the polygon\n\n        Examples\n        --------\n        >>> vertices = [ (1,2), (4,5), (1,3) ]\n        >>> mask = grid.mask_poly(vertices)\n        """"""\n        # If lat/lon vertices are given, we need to put these onto\n        # the grid coordinates\n        if lat_lon:\n            points = self.ij(vertices, asint=True)\n            vertices = list(zip(points[0], points[1]))\n\n        # Now, with grid coordinates, test the grid against the vertices\n        poly = matplotlib.path.Path(vertices)\n        inside = poly.contains_points(np.vstack((self.J.flatten(),\n                                                 self.I.flatten())).T,\n                                      radius=radius)\n        return np.ma.masked_where(inside.reshape(self.lat_rho.shape),\n                                  np.ones(self.lat_rho.shape))\n'"
seapy/model/hycom.py,10,"b'#!/usr/bin/env python\n""""""\n  hycom.py\n\n  Functions for dealing with the HYCOM model for importation into ROMS\n\n  Written by Brian Powell on 07/24/15\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\n\n\nimport numpy as np\nfrom datetime import datetime\nimport netCDF4\nfrom seapy.lib import default_epoch, chunker\nfrom seapy.model.grid import asgrid\nfrom seapy.roms import ncgen, num2date\n\n# _url = ""http://tds.hycom.org/thredds/dodsC/GLBu0.08/expt_19.1/2010""\n_url = ""http://tds.hycom.org/thredds/dodsC/GLBu0.08/expt_91.1""\n_maxrecs = 5\n\n\ndef load_history(filename,\n                 start_time=datetime(1, 1, 1),\n                 end_time=datetime(1, 1, 1),\n                 grid=None,\n                 epoch=default_epoch, url=_url, load_data=False):\n    """"""\n    Download HYCOM data and save into local file\n\n    Parameters\n    ----------\n    filename: string\n        name of output file\n    start_time: datetime\n        starting date to load HYCOM data\n    end_time: datetime\n        ending date for loading HYCOM data\n    grid: seapy.model.grid, optional\n        if specified, only load SODA data that covers the grid\n    epoch: datetime, optional\n        reference time for new file\n    url: string, optional\n        URL to load SODA data from\n    load_data: bool, optional\n        If true actually load the data. If false (default), it\n        displays the information needed to load the data using ncks\n\n    Returns\n    -------\n    None\n    """"""\n    # Load the grid\n    grid = asgrid(grid)\n\n    # Open the HYCOM data\n    hycom = netCDF4.Dataset(url)\n\n    # Figure out the time records that are required\n    hycom_time = num2date(hycom, ""time"")\n\n    time_list = np.where(np.logical_and(hycom_time >= start_time,\n                                        hycom_time <= end_time))\n    if not np.any(time_list):\n        raise Exception(""Cannot find valid times"")\n\n    # Get the latitude and longitude ranges\n    minlat = np.min(grid.lat_rho) - 0.5\n    maxlat = np.max(grid.lat_rho) + 0.5\n    minlon = np.min(grid.lon_rho) - 0.5\n    maxlon = np.max(grid.lon_rho) + 0.5\n    hycom_lon = hycom.variables[""lon""][:]\n    hycom_lat = hycom.variables[""lat""][:]\n\n    # Ensure same convention\n    if not grid.east():\n        hycom_lon[hycom_lon > 180] -= 360\n\n    latlist = np.where(np.logical_and(hycom_lat >= minlat,\n                                      hycom_lat <= maxlat))\n    lonlist = np.where(np.logical_and(hycom_lon >= minlon,\n                                      hycom_lon <= maxlon))\n    if not np.any(latlist) or not np.any(lonlist):\n        raise Exception(""Bounds not found"")\n\n    # Build the history file\n    if load_data:\n        his = ncgen.create_zlevel(filename, len(latlist[0]),\n                                  len(lonlist[0]),\n                                  len(hycom.variables[""depth""][:]), epoch,\n                                  ""HYCOM history from "" + url, dims=1)\n\n        # Write out the data\n        his.variables[""lat""][:] = hycom_lat[latlist]\n        his.variables[""lon""][:] = hycom_lon[lonlist]\n        his.variables[""depth""][:] = hycom.variables[""depth""]\n        his.variables[""time""][:] = seapy.roms.date2num(\n            hycom_time[time_list], his, \'time\')\n\n    # Loop over the variables\n    hycomvars = {""surf_el"": 3, ""water_u"": 4, ""water_v"": 4, ""water_temp"": 4,\n                 ""salinity"": 4}\n    hisvars = {""surf_el"": ""zeta"", ""water_u"": ""u"", ""water_v"": ""v"",\n               ""water_temp"": ""temp"", ""salinity"": ""salt""}\n\n    if not load_data:\n        print(""ncks -v {:s} -d time,{:d},{:d} -d lat,{:d},{:d} -d lon,{:d},{:d} {:s} {:s}"".format(\n            "","".join(hycomvars.keys()),\n            time_list[0][0], time_list[0][-1], latlist[0][0],\n            latlist[0][-1], lonlist[0][0], lonlist[0][-1], url, filename))\n    else:\n        for rn, recs in enumerate(chunker(time_list[0], _maxrecs)):\n            print(""{:s}-{:s}: "".format(hycom_time[recs[0]].strftime(""%m/%d/%Y""),\n                                       hycom_time[recs[-1]].strftime(""%m/%d/%Y"")),\n                  end=\'\', flush=True)\n            for var in hycomvars:\n                print(""{:s} "".format(var), end=\'\', flush=True)\n                hisrange = np.arange(\n                    rn * _maxrecs, (rn * _maxrecs) + len(recs))\n                if hycomvars[var] == 3:\n                    his.variables[hisvars[var]][hisrange, :, :] = \\\n                        hycom.variables[var][recs, latlist[0], lonlist[0]].filled(\n                        fill_value=9.99E10)\n                else:\n                    his.variables[hisvars[var]][hisrange, :, :, :] = \\\n                        hycom.variables[var][recs, :, latlist[0],\n                                             lonlist[0]].filled(fill_value=9.99E10)\n            his.sync()\n            print("""", flush=True)\n    pass\n'"
seapy/model/lib.py,42,"b'#!/usr/bin/env python\n""""""\n  lib.py\n\n  Library of utilities for ocean models, imported into the namespace\n  when importing the model module\n\n  Written by Brian Powell on 10/18/13\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\nimport numpy as np\nimport seapy\nimport scipy.constants\n\n# Define reference constants\n_R0 = 999.83\n_R0a = 5.053e-3\n_R0b = 0.048e-6\n\n\ndef _cgrid_rho_vel(rho, dim, fill):\n    """"""\n    Private Method: Compute the u- or v-grid velocity from a rho-field for a c-grid\n    """"""\n    rho = np.ma.array(rho, copy=False)\n    if fill:\n        rho = seapy.convolve_mask(rho, copy=True)\n    shp = np.array(rho.shape)\n    fore = np.product([shp[i] for i in np.arange(0, dim)]).astype(int)\n    aft = np.product([shp[i]\n                      for i in np.arange(dim + 1, rho.ndim)]).astype(int)\n    nfld = 0.5 * (rho.reshape([fore, shp[dim], aft])[:, 0:-1, :].filled(np.nan) +\n                  rho.reshape([fore, shp[dim], aft])[:, 1:, :].filled(np.nan))\n    shp[dim] = shp[dim] - 1\n    return np.ma.fix_invalid(nfld.reshape(shp), copy=False, fill_value=1e+37)\n\n\ndef rho2u(rho, fill=False):\n    """"""\n    Put the rho field onto the u field for the c-grid\n\n    Parameters\n    ----------\n    rho : masked array like\n        Input rho field\n    fill : bool, optional\n        Fill the masked data before moving grids\n\n    Returns\n    -------\n    u : masked array\n    """"""\n    return _cgrid_rho_vel(rho, rho.ndim - 1, fill)\n\n\ndef rho2v(rho, fill=False):\n    """"""\n    Put the rho field onto the v field for the c-grid\n\n    Parameters\n    ----------\n    rho : masked array like\n        Input rho field\n    fill : bool, optional\n        Fill the masked data before moving grids\n\n    Returns\n    -------\n    v : masked array\n    """"""\n    return _cgrid_rho_vel(rho, rho.ndim - 2, fill)\n\n\ndef u2rho(u, fill=False):\n    """"""\n    Put the u field onto the rho field for the c-grid\n\n    Parameters\n    ----------\n    u : masked array like\n        Input u field\n    fill : bool, optional\n        Fill the masked data before moving grids\n\n    Returns\n    -------\n    rho : masked array\n    """"""\n    u = np.ma.array(u, copy=False)\n    if fill:\n        u = seapy.convolve_mask(u, copy=True)\n    shp = np.array(u.shape)\n    nshp = shp.copy()\n    nshp[-1] = nshp[-1] + 1\n    fore = np.product([shp[i] for i in np.arange(0, u.ndim - 1)]).astype(int)\n    nfld = np.ones([fore, nshp[-1]])\n    nfld[:, 1:-1] = 0.5 * \\\n        (u.reshape([fore, shp[-1]])[:, 0:-1].filled(np.nan) +\n         u.reshape([fore, shp[-1]])[:, 1:].filled(np.nan))\n    nfld[:, 0] = nfld[:, 1] + (nfld[:, 2] - nfld[:, 3])\n    nfld[:, -1] = nfld[:, -2] + (nfld[:, -2] - nfld[:, -3])\n    return np.ma.fix_invalid(nfld.reshape(nshp), copy=False, fill_value=1e+37)\n\n\ndef v2rho(v, fill=False):\n    """"""\n    Put the v field onto the rho field for the c-grid\n\n    Parameters\n    ----------\n    v : masked array like\n        Input v field\n    fill : bool, optional\n        Fill the masked data before moving grids\n\n    Returns\n    -------\n    rho : masked array\n    """"""\n    v = np.ma.array(v, copy=False)\n    if fill:\n        v = seapy.convolve_mask(v, copy=True)\n    shp = np.array(v.shape)\n    nshp = shp.copy()\n    nshp[-2] = nshp[-2] + 1\n    fore = np.product([shp[i] for i in np.arange(0, v.ndim - 2)]).astype(int)\n    nfld = np.ones([fore, nshp[-2], nshp[-1]])\n    nfld[:, 1:-1, :] = 0.5 * \\\n        (v.reshape([fore, shp[-2], shp[-1]])[:, 0:-1, :].filled(np.nan) +\n         v.reshape([fore, shp[-2], shp[-1]])[:, 1:, :].filled(np.nan))\n    nfld[:, 0, :] = nfld[:, 1, :] + (nfld[:, 2, :] - nfld[:, 3, :])\n    nfld[:, -1, :] = nfld[:, -2, :] + (nfld[:, -2, :] - nfld[:, -3, :])\n    return np.ma.fix_invalid(nfld.reshape(nshp), copy=False, fill_value=1e+37)\n\n\ndef density(depth, temp, salt):\n    """"""\n    Calcuate density and adjoint-forcing terms of density using the quadratic\n    equation of state.\n\n    Parameters\n    ----------\n    depth: ndarray\n      Depth of temperature and salinity values\n    temp: ndarray,\n      Model temperature\n    salt: ndarray,\n      Model salt\n\n    Returns\n    -------\n    rho: ndarray,\n      Density of field\n    drhodt : ndarray,\n      Variation of density with respect to temperature\n    drhods : ndarray,\n      Variation of density with respect to salt\n    """"""\n    Ba = 0.808\n    Bb = 0.0085e-3\n    Aa = 0.0708\n    Ab = 0.351e-3\n    Ac = 0.068\n    Ad = 0.0683e-3\n    Ga = 0.003\n    Gb = 0.059e-3\n    Gc = 0.012\n    Gd = 0.064e-3\n\n    Z = np.abs(np.ma.asarray(depth))\n    T = np.ma.asarray(temp)\n    S = np.ma.asarray(salt)\n\n    # Calculate the density\n    rho0 = _R0 + _R0a * Z - _R0b * Z * Z\n    alpha = Aa * (1 + Ab * Z + Ac * (1 - Ad * Z) * T)\n    beta = Ba - Bb * Z\n    gamma = Ga * (1 - Gb * Z - Gc * (1 - Gd * Z) * T)\n    rho = rho0 + beta * S - alpha * T - gamma * (35 - S) * T\n\n    # Calculate drhodt\n    rho0 = 0\n    alpha = Aa * Ab * Z - 2 * Aa * Ac * Ad * T * Z + 2 * Aa * Ac * T + Aa\n    beta = 0\n    gamma = Ga * Gb * S * Z - 35 * Ga * Gb * Z - 2 * Ga * Gc * Gd * S * T * Z + \\\n        2 * 35 * Ga * Gc * Gd * T * Z + 2 * Ga * Gc * S * T - 2 * 35 * Ga * Gc * T - \\\n        Ga * S + 35 * Ga\n    drhodt = rho0 + beta - alpha - gamma\n\n    # Calculate drhods\n    rho0 = 0\n    alpha = 0\n    beta = Ba - Bb * Z\n    gamma = Ga * Gb * T * Z - Ga * Gc * Gd * T * T * Z + Ga * Gc * T * T - Ga * T\n    drhods = rho0 + beta - alpha - gamma\n\n    return rho, drhodt, drhods\n\n\ndef w(grid, u, v):\n    """"""\n    Compute the vertical velocity for the grid from the u and v velocity.\n\n    For a standard, z-level model the formulation is:\n\n    w_ij = u_ij * delta Hz / delta x + v_ij * delta Hz / delta y\n\n    Parameters\n    ----------\n    grid: seapy.model.grid or string\n      The grid to use to compute the vertical velocity\n    u   : ndarray\n      u-component of velocity\n    v   : ndarray\n      v-component of velocity\n\n    Returns\n    -------\n    w: ndarray,\n      vertical velocity as [m s**-1]\n    """"""\n    grid = seapy.model.asgrid(grid)\n    u = np.ma.array(u)\n    v = np.ma.array(v)\n\n    if grid.cgrid:\n        w = u2rho(0.5 * u * (grid.thick_rho[:, :, 1:] -\n                             grid.thick_rho[:, :, :-1]) *\n                  (grid.pm[:, 1:] + grid.pm[:, :-1])) + \\\n            v2rho(0.5 * v * (grid.thick_rho[:, 1:, :] -\n                             grid.thick_rho[:, :-1, :]) *\n                  (grid.pn[1:, :] + grid.pn[:-1, :]))\n    else:\n        w = np.ma.array(np.zeros(u.shape))\n        w = u * grid.thick_rho * grid.pm + \\\n            v * grid.thick_rho * grid.pn\n\n    return w\n\n\ndef pressure(hz, rho, axis=None, drhodt=None, drhods=None):\n    """"""\n    Calculate the water pressure and the adjoint forcing of pressure\n    (if adjoint forcing of density is provided).\n\n    Parameters\n    ----------\n    hz : ndarray\n      Thickness of rho values\n    rho : ndarray\n      Density of water\n    axis : int, optional\n      Axis to integrate for pressure. If not specified, try to\n      estimate the best dimension to integrate\n    drhodt: ndarray, optional\n      Variation of rho with temperature. Only required to compute\n      the variation of pressure\n    drhods: ndarray, optional\n      Variation of rho with salt. Only required to compute\n      the variation of pressure\n\n    Returns\n    -------\n    pres: ndarray\n      Pressure\n    dpresdt : ndarray\n      Variation of pressure with respect to temperature\n    dpresds : ndarray\n      Variation of pressure with respect to salt\n    """"""\n    hz = np.ma.array(hz)\n    rho = np.ma.array(rho)\n    dpresdt = None\n    dpresds = None\n\n    if not axis:\n        # Figure out the dimensionss. If rho has fewer dimension than depth,\n        # then time is the mismatched. If they are the same, we will integrate\n        # the first dimension\n        if hz.ndim != rho.ndim:\n            nd = rho.ndim - hz.ndim\n            if hz.shape == rho.shape[nd:]:\n                axis = nd\n            elif hz.shape == rho.shape[:-nd]:\n                axis = -nd\n        else:\n            axis = 1\n\n    flip = [slice(None, None, None) for i in range(rho.ndim)]\n    flip[axis] = slice(None, None, -1)\n\n    pres = np.cumsum(scipy.constants.g *\n                     (rho * hz)[flip], axis=axis)[flip] * 1e-4\n\n    # Calculate adjoint forcing\n    if drhodt is not None:\n        dpresdt = np.cumsum(scipy.constants.g * (drhodt * hz)[flip],\n                            axis=axis)[flip] * 1e-4\n    if drhods is not None:\n        dpresds = np.cumsum(scipy.constants.g * (drhods * hz)[flip],\n                            axis=axis)[flip] * 1e-4\n\n    return pres, dpresdt, dpresds\n\n\ndef sound(depth, temp, salt):\n    """"""\n    Calcuate speed of sound and adjoint-forcing terms of sound speed using the\n    quadratic equation of state.\n\n    Parameters\n    ----------\n    depth: ndarray\n      Depth of temperature and salinity values\n    temp: ndarray,\n      Model temperature\n    salt: ndarray,\n      Model salt\n\n    Returns\n    -------\n    c: ndarray,\n      Speed of Sound field\n    dcdt : ndarray,\n      Variation of sound speed with respect to temperature\n    dcds : ndarray,\n      Variation of sound speed with respect to salt\n    """"""\n    Ca = 1448.96\n    Cb = 4.591\n    Cc = 0.05304\n    Cd = 2.374e-4\n    Ce = 1.34\n    Cf = 0.01630\n    Cg = 1.675e-7\n    Ch = 1.025e-2\n    Ci = 7.139e-13\n\n    Z = np.abs(np.ma.asarray(depth))\n    T = np.ma.asarray(temp)\n    S = np.ma.asarray(salt)\n\n    c = Ca + Cb * T - Cc * T * T + Cd * T**3 + Ce * S - Ce * 35 + Cf * Z + \\\n        Cg * Z * Z - Ch * T * S + Ch * 35 * T - Ci * T * Z**3\n\n    dcdt = Cb - 2 * Cc * T + 3 * Cd * T * T - Ch * S + Ch * 35 - Ci * Z**3\n    dcds = Ce - Ch * T\n\n    return c, dcdt, dcds\n\n\ndef bvf(depth, rho, axis=None, drhodt=None, drhods=None):\n    """"""\n    Calculate the Brunt-Vaisala Frequency of the density\n\n    Parameters\n    ----------\n    depth: ndarray\n      Depth of temperature and salinity values\n    rho : ndarray\n      Density of water\n    axis : int, optional\n      Axis to integrate for pressure. If not specified, try to\n      estimate the best dimension to integrate\n    drhodt: ndarray, optional\n      Variation of rho with temperature. Only required to compute\n      the variation of pressure\n    drhods: ndarray, optional\n      Variation of rho with salt. Only required to compute\n      the variation of pressure\n\n    Returns\n    -------\n    bvf : ndarray\n      Brunt-Vaisala Frequency\n    dbvfdt : ndarray\n      Variation of BVF with respect to temperature\n    dbvfds : ndarray\n      Variation of BVF with respect to salt\n    """"""\n    Z = np.abs(np.ma.asarray(depth))\n    rho = np.ma.asarray(rho)\n    dbvfdt = None\n    dbvfds = None\n\n    if not axis:\n        # Figure out the dimensionss. If rho has fewer dimension than depth,\n        # then time is the mismatched. If they are the same, we will integrate\n        # the first dimension\n        if depth.ndim != rho.ndim:\n            nd = rho.ndim - depth.ndim\n            if depth.shape == rho.shape[nd:]:\n                axis = nd\n            elif depth.shape == rho.shape[:-nd]:\n                axis = -nd\n        else:\n            axis = 1\n\n    fill = [slice(None, None, None) for i in range(rho.ndim)]\n    fill[axis] = slice(None, -1, None)\n\n    rho0 = _R0 + _R0a * Z - _R0b * Z * Z\n    bvf = np.zeros(rho.shape)\n    bvf[fill] = - scipy.constants.g / rho0[::-1, ...][:-1, ...] * \\\n        np.diff(rho, axis=axis) / -np.diff(Z, axis=0)\n    if drhodt:\n        dbvfdt[fill] = - scipy.constants.g / rho0[::-1, ...][:-1, ...] * \\\n            np.diff(drhodt, axis=axis) / -np.diff(Z, axis=0)\n    if drhods:\n        dbvfds[fill] = - scipy.constants.g / rho0[::-1, ...][:-1, ...] * \\\n            np.diff(drhods, axis=axis) / -np.diff(Z, axis=0)\n\n    return bvf, dbvfdt, dbvfds\n'"
seapy/model/soda.py,10,"b'#!/usr/bin/env python\n""""""\n  soda.py\n\n  Functions for dealing with the soda model for importation into ROMS\n\n  Written by Brian Powell on 07/24/15\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\n\n\nimport numpy as np\nfrom datetime import datetime\nimport netCDF4\nfrom seapy.lib import default_epoch, chunker\nfrom seapy.model.grid import asgrid\nfrom seapy.roms import ncgen, num2date, date2num\n\n_url = ""http://apdrc.soest.hawaii.edu:80/dods/public_data/SODA/soda_pop2.2.4""\n_maxrecs = 5\n\n\ndef load_history(filename,\n                 start_time=datetime(1, 1, 1),\n                 end_time=datetime(1, 1, 1),\n                 grid=None,\n                 epoch=default_epoch, url=_url, load_data=False):\n    """"""\n    Download soda data and save into local file\n\n    Parameters\n    ----------\n    filename: string\n        name of output file\n    start_time: datetime\n        starting date to load soda data\n    end_time: datetime\n        ending date for loading soda data\n    grid: seapy.model.grid, optional\n        if specified, only load SODA data that covers the grid\n    epoch: datetime, optional\n        reference time for new file\n    url: string, optional\n        URL to load SODA data from\n    load_data: bool, optional\n        If true (default) actually load the data. If false, it\n        displays the information needed to load the data using ncks\n\n    Returns\n    -------\n    None\n    """"""\n    # Load the grid\n    grid = asgrid(grid)\n\n    # Open the soda data\n    soda = netCDF4.Dataset(url)\n\n    # Figure out the time records that are required\n    soda_time = num2date(soda, ""time"")\n\n    time_list = np.where(np.logical_and(soda_time >= start_time,\n                                        soda_time <= end_time))\n    if not any(time_list):\n        raise Exception(""Cannot find valid times"")\n\n    # Get the latitude and longitude ranges\n    minlat = np.min(grid.lat_rho) - 0.5\n    maxlat = np.max(grid.lat_rho) + 0.5\n    minlon = np.min(grid.lon_rho) - 0.5\n    maxlon = np.max(grid.lon_rho) + 0.5\n    soda_lon = soda.variables[""lon""][:]\n    soda_lat = soda.variables[""lat""][:]\n\n    # Ensure same convention\n    if not grid.east():\n        soda_lon[soda_lon > 180] -= 360\n\n    latlist = np.where(np.logical_and(soda_lat >= minlat,\n                                      soda_lat <= maxlat))\n    lonlist = np.where(np.logical_and(soda_lon >= minlon,\n                                      soda_lon <= maxlon))\n    if not np.any(latlist) or not np.any(lonlist):\n        raise Exception(""Bounds not found"")\n\n    # Build the history file\n    if load_data:\n        his = ncgen.create_zlevel(filename, len(latlist[0]),\n                                  len(lonlist[0]),\n                                  len(soda.variables[""lev""][:]), epoch,\n                                  ""soda history from "" + url, dims=1)\n\n        # Write out the data\n        his.variables[""lat""][:] = soda_lat[latlist]\n        his.variables[""lon""][:] = soda_lon[lonlist]\n        his.variables[""depth""][:] = soda.variables[""lev""]\n        his.variables[""time""][:] = date2num(\n            soda_time[time_list], his, \'time\')\n    # Loop over the variables\n    sodavars = {""ssh"": 3, ""u"": 4, ""v"": 4, ""temp"": 4, ""salt"": 4}\n    hisvars = {""ssh"": ""zeta"", ""u"": ""u"", ""v"": ""v"", ""temp"": ""temp"",\n               ""salt"": ""salt""}\n\n    if not load_data:\n        print(""ncks -v {:s} -d time,{:d},{:d} -d lat,{:d},{:d} -d lon,{:d},{:d} {:s} {:s}"".format(\n            "","".join(sodavars.keys()),\n            time_list[0][0], time_list[0][-1], latlist[0][0],\n            latlist[0][-1], lonlist[0][0], lonlist[0][-1], _url, filename))\n    else:\n        for rn, recs in enumerate(chunker(time_list[0], _maxrecs)):\n            print(""{:s}-{:s}: "".format(soda_time[recs[0]].strftime(""%m/%d/%Y""),\n                                       soda_time[recs[-1]].strftime(""%m/%d/%Y"")),\n                  end=\'\', flush=True)\n            for var in sodavars:\n                print(""{:s} "".format(var), end=\'\', flush=True)\n                hisrange = np.arange(\n                    rn * _maxrecs, (rn * _maxrecs) + len(recs))\n                if sodavars[var] == 3:\n                    his.variables[hisvars[var]][hisrange, :, :] = \\\n                        np.ma.array(\n                        soda.variables[var][recs, latlist[0], lonlist[0]]). \\\n                        filled(fill_value=9.99E10)\n                else:\n                    his.variables[hisvars[var]][hisrange, :, :, :] = \\\n                        soda.variables[var][recs, :, latlist[0],\n                                            lonlist[0]].filled(fill_value=9.99E10)\n            his.sync()\n            print("""", flush=True)\n    pass\n'"
seapy/roms/__init__.py,0,"b'""""""\n  State Estimation and Analysis for PYthon\n\n    Module for working with ROMS data\n\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n\n  Imported functions include:\n\n  - :func:`~seapy.roms.lib.depth`\n  - :func:`~seapy.roms.lib.get_reftime`\n  - :func:`~seapy.roms.lib.date2num`\n  - :func:`~seapy.roms.lib.num2date`\n  - :func:`~seapy.roms.lib.get_timevar`\n  - :func:`~seapy.roms.lib.stretching`\n  - :func:`~seapy.roms.lib.thickness`\n""""""\nfrom . import analysis\nfrom . import boundary\nfrom . import clim\nfrom . import ezgrid\nfrom . import forcing\nfrom . import initial\nfrom . import interp\nfrom . import ncgen\nfrom . import obs\nfrom . import obsgen\nfrom . import tide\nfrom . import psource\nfrom .lib import *\n'"
seapy/roms/analysis.py,94,"b'#!/usr/bin/env python\n""""""\n  analysis.py\n\n  Methods to assist in the analysis of ROMS fields\n\n  Written by Brian Powell on 05/24/15\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\n\nimport numpy as np\nfrom joblib import Parallel, delayed\nimport seapy\nimport netCDF4\n\n\ndef __find_surface_thread(grid, field, value, zeta, const_depth=False,\n                          k_values=False, u_grid=False, v_grid=False):\n    """"""\n    Internal function to find a value in field_a and return the\n    values from field_b at the same positions.\n    """"""\n    depth = seapy.roms.depth(grid.vtransform, grid.h, grid.hc,\n                             grid.s_rho, grid.cs_r, zeta)\n    if u_grid:\n        depth = seapy.model.rho2u(depth)\n    elif v_grid:\n        depth = seapy.model.rho2v(depth)\n\n    # Set the variables based on what we are finding\n    if const_depth:\n        field_a, field_b = depth, field\n    else:\n        field_a, field_b = field, depth\n\n    # Determine the upper and lower bounds of the value in the field\n    tmp = np.ma.masked_equal(\n        np.diff(((field_a - value) < 0).astype(np.short), axis=0), 0)\n    factor = -np.sign(np.mean(np.diff(field, axis=0))).astype(np.short)\n\n    # Determine the points of the upper bound and the lower bound\n    bad = np.sum(tmp, axis=0).astype(bool)\n    k_ones = np.arange(grid.n, dtype=np.short)\n    upper = (k_ones[:, np.newaxis, np.newaxis] ==\n             np.argmax(np.abs(tmp), axis=0)) * bad\n    k_ones = np.arange(grid.n, dtype=np.short) - factor\n    lower = (k_ones[:, np.newaxis, np.newaxis] ==\n             np.argmax(np.abs(tmp), axis=0)) * bad\n\n    # Now that we have the bounds, we can linearly interpolate to\n    # find where the value lies\n    u_a = np.sum(field_a * upper, axis=0)\n    d_a = u_a - np.sum(field_a * lower, axis=0)\n    d_z = (u_a - value) / d_a\n    if k_values:\n        return np.argmax(upper, axis=0) + factor * d_z\n\n    # Calculate the values from field_b\n    u_b = np.sum(field_b * upper, axis=0)\n    d_b = u_b - np.sum(field_b * lower, axis=0)\n    return u_b - d_b * d_z\n\n\ndef constant_depth(field, grid, depth, zeta=None, threads=2):\n    """"""\n    Find the values of a 3-D field at a constant depth for all times given.\n\n    Parameters\n    ----------\n    field : ndarray,\n        ROMS 3-D field to interpolate onto a constant depth level. If 4-D, it\n        will calculate through time.\n    grid : seapy.model.grid or string or list,\n        Grid that defines the depths and stretching for the field given\n    depth : float,\n        Depth (in meters) to find all values\n    zeta : ndarray, optional,\n        ROMS zeta field corresponding to field if you wish to apply the SSH\n        correction to the depth calculations.\n    threads : int, optional,\n        Number of threads to use for processing\n\n    Returns\n    -------\n    nfield : ndarray,\n        Values from ROMS field on the given constant depth\n    """"""\n    grid = seapy.model.asgrid(grid)\n    field = np.ma.masked_invalid(field, copy=False)\n    depth = depth if depth < 0 else -depth\n    if depth is None or grid.depth_rho.min() > depth > grid.depth_rho.max():\n        warn(""Error: {:f} is out of range for the depth."".format(value))\n        return\n    if np.ndim(field) == 3:\n        field = seapy.adddim(field)\n    nt = field.shape[0]\n    threads = np.minimum(nt, threads)\n    if zeta is None:\n        zeta = np.zeros((nt, 1, 1))\n    if np.ndim(zeta) == 2:\n        zeta = seapy.adddim(zeta, nt)\n\n    v_grid = u_grid = False\n    if field.shape[-2:] == grid.mask_u:\n        u_grid = True\n    elif field.shape[-2:] == grid.mask_v:\n        v_grid = True\n\n    return np.ma.array(Parallel(n_jobs=threads, verbose=2)\n                       (delayed(__find_surface_thread)\n                        (grid, field[i, :], depth, zeta[i, :],\n                         const_depth=True, u_grid=u_grid, v_grid=v_grid)\n                        for i in range(nt)), copy=False)\n\n\ndef constant_value(field, grid, value, zeta=None, threads=2):\n    """"""\n    Find the depth of the value across the field. For example, find the depth\n    of a given isopycnal if the field is density.\n\n    Parameters\n    ----------\n    field : ndarray,\n        ROMS 3-D field to interpolate onto a constant depth level. If 4-D, it\n        will calculate through time.\n    grid : seapy.model.grid or string or list,\n        Grid that defines the depths and stretching for the field given\n    value : float,\n        Value to find the depths for in same units as the \'field\'\n    zeta : ndarray, optional,\n        ROMS zeta field corresponding to field if you wish to apply the SSH\n        correction to the depth calculations.\n    threads : int, optional,\n        Number of threads to use for processing\n\n    Returns\n    -------\n    nfield : ndarray,\n        Depths from ROMS field on the given value\n    """"""\n    grid = seapy.model.asgrid(grid)\n    field = np.ma.masked_invalid(field, copy=False)\n    if value is None or field.min() > value > field.max():\n        warn(""Error: {:f} is out of range for the field."".format(value))\n        return\n    if np.ndim(field) == 3:\n        field = seapy.adddim(field)\n    nt = field.shape[0]\n    threads = np.minimum(nt, threads)\n    if zeta is None:\n        zeta = np.zeros((nt, 1, 1))\n    if np.ndim(zeta) == 2:\n        zeta = seapy.adddim(zeta, nt)\n\n    v_grid = u_grid = False\n    if field.shape[-2:] == grid.mask_u:\n        u_grid = True\n    elif field.shape[-2:] == grid.mask_v:\n        v_grid = True\n\n    return np.ma.array(Parallel(n_jobs=threads, verbose=2)\n                       (delayed(__find_surface_thread)\n                        (grid, field[i, :], value, zeta[i, :],\n                         u_grid=u_grid, v_grid=v_grid)\n                        for i in range(nt)), copy=False)\n\n\ndef constant_value_k(field, grid, value, zeta=None, threads=2):\n    """"""\n    Find the layer number of the value across the field. For example, find the k\n    of a given isopycnal if the field is density.\n\n    Parameters\n    ----------\n    field : ndarray,\n        ROMS 3-D field to interpolate onto a constant depth level. If 4-D, it\n        will calculate through time.\n    grid : seapy.model.grid or string or list,\n        Grid that defines the depths and stretching for the field given\n    value : float,\n        Value to find the depths for in same units as the \'field\'\n    threads : int, optional,\n        Number of threads to use for processing\n\n    Returns\n    -------\n    nfield : ndarray,\n        Depths from ROMS field on the given value\n    """"""\n    grid = seapy.model.asgrid(grid)\n    field = np.ma.masked_invalid(field, copy=False)\n    if value is None or field.min() > value > field.max():\n        warn(""Error: {:f} is out of range for the field."".format(value))\n        return\n    if np.ndim(field) == 3:\n        field = seapy.adddim(field)\n    nt = field.shape[0]\n    threads = np.minimum(nt, threads)\n    if zeta is None:\n        zeta = np.zeros((nt, 1, 1))\n    if np.ndim(zeta) == 2:\n        zeta = seapy.adddim(zeta, nt)\n\n    v_grid = u_grid = False\n    if field.shape[-2:] == grid.mask_u:\n        u_grid = True\n    elif field.shape[-2:] == grid.mask_v:\n        v_grid = True\n\n    return np.ma.array(Parallel(n_jobs=threads, verbose=2)\n                       (delayed(__find_surface_thread)\n                        (grid, field[i, :], value, zeta[i, :],\n                         k_values=True, u_grid=u_grid, v_grid=v_grid)\n                        for i in range(nt)), copy=False)\n\n\ndef depth_average(field, grid, bottom, top, zeta=None):\n    """"""\n    Compute the depth-averaged field down to the depth specified. NOTE:\n    This just finds the nearest layer, so at every grid cell, it may not be\n    exactly the specified depth.\n\n    Parameters\n    ----------\n    field : ndarray,\n        ROMS 3-D field to integrate from a depth level. Must be\n        three-dimensional array (single time).\n    grid : seapy.model.grid or string or list,\n        Grid that defines the depths and stretching for the field given\n    bottom : float,\n        Depth (in meters) to integrate from\n    top : float,\n        Depth (in meters) to integrate to\n    zeta : ndarray, optional,\n        ROMS zeta field corresponding to field if you wish to apply the SSH\n        correction to the depth calculations.\n\n    Returns\n    -------\n    ndarray,\n        Values from depth integrated ROMS field\n    """"""\n    grid = seapy.model.asgrid(grid)\n    bottom = bottom if bottom < 0 else -bottom\n    top = top if top < 0 else -top\n    if bottom > top:\n        bottom, top = top, bottom\n    drange = top - bottom\n\n    # If we have zeta, we need to compute thickness\n    if zeta is not None:\n        s_w, cs_w = seapy.roms.stretching(grid.vstretching, grid.theta_s,\n                                          grid.theta_b, grid.hc,\n                                          grid.n, w_grid=True)\n        depths = np.ma.masked_equal(seapy.roms.depth(\n            grid.vtransform, grid.h, grid.hc, grid.s_rho, grid.cs_r) *\n            grid.mask_rho, 0)\n\n        thickness = np.ma.masked_array(seapy.roms.thickness(\n            grid.vtransform, grid.h, grid.hc, s_w, cs_w, zeta) *\n            grid.mask_rho, 0)\n\n    else:\n        depths = np.ma.masked_equal(grid.depth_rho * grid.mask_rho, 0)\n        thickness = np.ma.masked_equal(grid.thick_rho * grid.mask_rho, 0)\n\n    # If we are on u- or v-grid, transform\n    if field.shape == grid.thick_u.shape:\n        depths = seapy.model.rho2u(depths)\n        thickness = seapy.model.rho2u(thickness)\n    elif field.shape == grid.thick_v.shape:\n        depths = seapy.model.rho2v(depths)\n        thickness = seapy.model.rho2v(thickness)\n\n    # 1. pick all of the points that are deeper and shallower than the limits\n    k_ones = np.arange(grid.n, dtype=int)\n    top_depth = depths[-1, :, :] if top_depth == 0 else top_depth\n    upper = depths - top_depth\n    upper[np.where(upper < 0)] = np.float(\'inf\')\n    lower = depths - depth\n    lower[np.where(lower > 0)] = -np.float(\'inf\')\n    thickness *= np.ma.masked_equal(np.logical_and(\n        k_ones[:, np.newaxis, np.newaxis] <= np.argmin(upper, axis=0),\n        k_ones[:, np.newaxis, np.newaxis] >=\n        np.argmax(lower, axis=0)).astype(int), 0)\n\n    # Do the integration\n    return np.sum(field * thickness, axis=0) / \\\n        np.sum(thickness, axis=0)\n\n\ndef transect(lon, lat, depth, data, nx=200, nz=40, z=None):\n    """"""\n    Generate an equidistant transect from data at varying depths. Can be\n    used to plot a slice of model or observational data.\n\n    Parameters\n    ----------\n    lat: array\n        n-dimensional array with latitude of points\n    lon: array\n        n-dimensional array with longitude of points\n    depth: array\n        [k,n] dimensional array of the depths for all k-layers at each n point\n    data: array\n        [k,n] dimensional array of values\n    nx: int, optional\n        number of horizontal points desired in the transect\n    nz: int, optional\n        number of vertical points desired in the transect\n    z: array, optional\n        list of depths to use if you do not want equidistant depths\n\n    Returns\n    -------\n    x: array\n        x-location values in [m] along transect\n    z: array\n        depth values in [m] of the new transect\n    vals: np.ma.array\n        data values of the new transect with masked values\n\n    Examples\n    --------\n    Generate series of transects from ROMS output\n\n    >>> nc = seapy.netcdf(\'roms_his.nc\')\n    >>> grid = seapy.model.asgrid(nc)\n    >>> data = nc.variables[\'salt\'][:,:,150,:]\n    >>> shp = (data.shape[0], 50, 400)\n    >>> transect = np.zeros(shp)\n    >>> for i in range(shp[0]):\n    >>>     x, z, d = \\\n    >>>          seapy.roms.analysis.transect(grid.lon_rho[150,:],\n    >>>                                       grid.lat_rho[150,:],\n    >>>                                       grid.depth_rho[:,150,:],\n    >>>                                       data[i,:,:], nx=shp[2],\n    >>>                                       nz=shp[1])\n    >>>     transect[i,:,:] = d.filled(np.nan)\n    >>> nc.close()\n    >>> plt.pcolormesh(x/1000, z, transect[0, :, :])\n    """"""\n    from scipy.interpolate import griddata\n    depth = np.atleast_2d(depth)\n    data = np.ma.atleast_2d(data).filled(np.mean(data))\n    lon = np.atleast_1d(lon)\n    lat = np.atleast_1d(lat)\n\n    # Generate the depths\n    depth[depth > 0] *= -1\n    if z is None:\n        z = np.linspace(depth.min() - 2, depth.max(), nz)\n    else:\n        z[z > 0] *= -1\n        nz = len(z)\n    dz = np.abs(np.diff(z).mean())\n\n    # Determine the distance between points and the weighting to apply\n    dist = np.hstack(([0], seapy.earth_distance(\n        lon[0], lat[0], lon[1:], lat[1:])))\n    dx = np.diff(dist).mean()\n    zscale = np.maximum(1, 10**int(np.log10(dx / dz)))\n    dx /= zscale\n    x = np.linspace(0, dist.max(), nx)\n\n    # All arrays have to be the same size\n    xx, zz = np.meshgrid(x / zscale, z)\n\n    # For the source data, we don\'t want to extrpolate,\n    # so make the data go from the surface to twice its\n    # depth.\n    zl = np.argsort(depth[:, 0])\n    dep = np.vstack((np.ones((1, depth.shape[1])) * 2 * depth.min(),\n                     depth[zl, :],\n                     np.zeros((1, depth.shape[1]))))\n\n    # repeat the same data at the top and bottom\n    dat = np.vstack((data[zl[0], :], data[zl],\n                     data[zl[-1], :]))\n    dist = np.tile(dist.T, [dep.shape[0], 1]) / zscale\n\n    # Find the bottom indices to create a mask for nodata/land\n    idx = np.interp(xx[0, :], dist[0, :],\n                    np.interp(depth.min(axis=0), z,\n                              np.arange(nz))).astype(int)\n    mask = np.arange(nz)[:, np.newaxis] <= idx\n\n    # Interpolate\n    ndat = np.ma.array(griddata(\n        (dist.ravel(), dep.ravel()), dat.ravel(), (xx.ravel(), zz.ravel()),\n        method=\'cubic\').reshape(xx.shape), mask=mask)\n\n    # Return everything\n    return x, z, ndat\n\n\ndef gen_std_i(roms_file, std_file, std_window=5, pad=1, skip=30, fields=None):\n    """"""\n    Create a std file for the given ocean fields. This std file can be used\n    for initial conditions constraint in 4D-Var. This requires a long-term\n    model spinup file from which to compute the standard deviation.\n\n    Parameters\n    ----------\n    roms_file: string or list of strings,\n        The ROMS (history or average) file from which to compute the std. If\n        it is a list of strings, a netCDF4.MFDataset is opened instead.\n    std_file: string,\n        The name of the file to store the standard deviations fields\n    std_window: int,\n        The size of the window (in number of records) to compute the std over\n    pad: int,\n        How much to pad each side of the window for overlap. For example,\n        std_window=10 and pad=2 would give a total window of 14 with 2 records\n        used in the prior window and 2 in the post window as well.\n    skip: int,\n        How many records to skip at the beginning of the file\n    fields: list of str,\n        The fields to compute std for. Default is to use the ROMS prognostic\n        variables.\n\n    Returns\n    -------\n        None\n    """"""\n    # Create the fields to process\n    if fields is None:\n        fields = set(seapy.roms.fields)\n\n    # Open the ROMS info\n    grid = seapy.model.asgrid(roms_file)\n    nc = seapy.netcdf(roms_file)\n\n    # Filter the fields for the ones in the ROMS file\n    fields = set(nc.variables).intersection(fields)\n\n    # Build the output file\n    epoch, time_var = seapy.roms.get_reftime(nc)\n    time = nc.variables[time_var][:]\n    ncout = seapy.roms.ncgen.create_da_ini_std(std_file,\n                                               eta_rho=grid.ln, xi_rho=grid.lm, s_rho=grid.n,\n                                               reftime=epoch, title=""std from "" + str(roms_file))\n    grid.to_netcdf(ncout)\n\n    # If there are any fields that are not in the standard output file,\n    # add them to the output file\n    for f in fields.difference(ncout.variables):\n        ncout.createVariable(f, np.float32,\n                             (\'ocean_time\', ""s_rho"", ""eta_rho"", ""xi_rho""))\n\n    # Loop over the time with the variance window:\n    for n, t in enumerate(seapy.progressbar.progress(np.arange(skip + pad,\n                                                               len(time) - std_window - pad, std_window))):\n        idx = np.arange(t - pad, t + std_window + pad)\n        ncout.variables[time_var][n] = np.mean(time[idx])\n        for v in fields:\n            dat = nc.variables[v][idx, :].std(axis=0)\n            dat[dat > 10] = 0.0\n            ncout.variables[v][n, :] = dat\n        ncout.sync()\n    ncout.close()\n    nc.close()\n\n\ndef gen_std_f(roms_file, std_file, records=None, fields=None):\n    """"""\n    Create a std file for the given atmospheric forcing fields. This std\n    file can be used for the forcing constraint in 4D-Var. This requires a\n    long-term model spinup file from which to compute the standard deviation.\n\n    Parameters\n    ----------\n    roms_file: string or list of strings,\n        The ROMS (history or average) file from which to compute the std. If\n        it is a list of strings, a netCDF4.MFDataset is opened instead.\n    std_file: string,\n        The name of the file to store the standard deviations fields\n    records: ndarray,\n        List of records to perform the std over. These records are used to\n        avoid the solar diurnal cycles in the fields.\n    fields: list of str,\n        The fields to compute std for. Default is to use the ROMS atmospheric\n        variables (sustr, svstr, shflux, ssflux).\n\n    Returns\n    -------\n        None\n    """"""\n    # Create the fields to process\n    if fields is None:\n        fields = set([""sustr"", ""svstr"", ""shflux"", ""ssflux""])\n\n    # Open the ROMS info\n    grid = seapy.model.asgrid(roms_file)\n    nc = seapy.netcdf(roms_file)\n\n    # Filter the fields for the ones in the ROMS file\n    fields = set(nc.variables).intersection(fields)\n\n    # Build the output file\n    epoch, time_var = seapy.roms.get_reftime(nc)\n    time = nc.variables[time_var][:]\n    ncout = seapy.roms.ncgen.create_da_frc_std(std_file,\n                                               eta_rho=grid.ln, xi_rho=grid.lm, s_rho=grid.n,\n                                               reftime=epoch, title=""std from "" + str(roms_file))\n    grid.to_netcdf(ncout)\n\n    # Set the records\n    if records is None:\n        records = np.arange(len(time))\n    else:\n        records = np.atleast_1d(records)\n        records = records[records <= len(time)]\n\n    # If there are any fields that are not part of the standard, add them\n    # to the output file\n    for f in fields.difference(ncout.variables):\n        ncout.createVariable(f, np.float32,\n                             (\'ocean_time\', ""eta_rho"", ""xi_rho""))\n\n    # Loop over the time with the variance window:\n    ncout.variables[time_var][:] = np.mean(time[records])\n    for v in fields:\n        dat = nc.variables[v][records, :].std(axis=0)\n        ncout.variables[v][0, :] = dat\n        ncout.sync()\n    ncout.close()\n    nc.close()\n\n\ndef plot_obs_spatial(obs, type=\'zeta\', prov=None, time=None, depth=0,\n                     gridcoord=False, error=False, **kwargs):\n    """"""\n    Create a surface plot of the observations.\n\n    Parameters\n    ----------\n    obs: filename, list, or observation class\n        The observations to use for plotting\n    type: string or int,\n        The type of observation to plot (\'zeta\', \'temp\', \'salt\', etc.)\n    prov: string or int,\n        The provenance of the observations to plot\n    time: ndarray,\n        The times of the observations to plot\n    depth: float,\n        The depth of the obs to plot over the spatial region\n    gridcoord: bool,\n        If True, plot on grid coordinates. If False [default] plot on lat/lon\n    error: bool,\n        If True plot the errors rather than values. Default is False.\n    **kwargs: keywords\n        Passed to matplotlib.pyplot.scatter\n\n    Returns\n    -------\n    None\n    """"""\n    import matplotlib.pyplot as plt\n\n    obs = seapy.roms.obs.asobs(obs)\n    otype = seapy.roms.obs.astype(type)\n    if prov is not None:\n        prov = seapy.roms.obs.asprovenance(prov)\n    if time is not None:\n        time = np.atleast_1d(time)\n\n    # Search the obs for the user\n    if prov is not None:\n        idx = np.where(np.logical_and.reduce((\n            obs.type == otype,\n            obs.provenance == prov,\n            np.logical_or(obs.z == 0, obs.depth == depth))))[0]\n\n    else:\n        idx = np.where(np.logical_and(\n            obs.type == otype,\n            np.logical_or(obs.z == 0, obs.depth == depth)))[0]\n\n    # If there is a time specific condition, find the sets\n    if time is not None:\n        idx = idx[np.in1d(obs.time[idx], time)]\n\n    # If we don\'t have anything to plot, return\n    if not idx.any():\n        return\n\n    # Plot it up\n    if not kwargs:\n        kwargs = {\'s\': 30, \'alpha\': 0.8, \'linewidths\': (0, 0)}\n    if gridcoord:\n        x = obs.x\n        y = obs.y\n    else:\n        x = obs.lon\n        y = obs.lat\n    val = obs.value if not error else np.sqrt(obs.error)\n    plt.scatter(x[idx], y[idx], c=val[idx], **kwargs)\n    plt.colorbar()\n\n\ndef plot_obs_profile(obs, type=\'temp\', prov=None, time=None,\n                     gridcoord=False, error=False, **kwargs):\n    """"""\n    Create a sub-surface profile plot of the observations.\n\n    Parameters\n    ----------\n    obs: filename, list, or observation class\n        The observations to use for plotting\n    type: string or int,\n        The type of observation to plot (\'zeta\', \'temp\', \'salt\', etc.)\n    prov: string or int,\n        The provenance of the observations to plot\n    time: ndarray,\n        The times of the observations to plot\n    gridcoord: bool,\n        If True, plot on grid coordinates. If False [default] plot on lat/lon\n    error: bool,\n        If True plot the errors rather than values. Default is False.\n    **kwargs: keywords\n        Passed to matplotlib.pyplot.scatter\n\n    Returns\n    -------\n    None\n    """"""\n    import matplotlib.pyplot as plt\n\n    obs = seapy.roms.obs.asobs(obs)\n    otype = seapy.roms.obs.astype(type)\n    if prov is not None:\n        prov = seapy.roms.obs.asprovenance(prov)\n    if time is not None:\n        time = np.atleast_1d(time)\n\n    # Search the obs for the user\n    if prov is not None:\n        idx = np.where(np.logical_and.reduce((\n            obs.type == otype,\n            obs.provenance == prov,\n            np.logical_or(obs.z < 0, obs.depth < 0))))[0]\n\n    else:\n        idx = np.where(np.logical_and(\n            obs.type == otype,\n            np.logical_or(obs.z < 0, obs.depth < 0)))[0]\n\n    # If there is a time specific condition, find the sets\n    if time is not None:\n        idx = idx[np.in1d(obs.time[idx], time)]\n\n    # If we don\'t have anything to plot, return\n    if not idx.any():\n        return\n\n    # Plot it up\n    if gridcoord:\n        dep = obs.z if np.mean(obs.z[idx] > 0) else obs.depth\n    else:\n        dep = obs.z if np.mean(obs.z[idx] < 0) else obs.depth\n    val = obs.value if not error else np.sqrt(obs.error)\n    plt.plot(val[idx], dep[idx], \'k+\', **kwargs)\n'"
seapy/roms/boundary.py,78,"b'#!/usr/bin/env python\n""""""\n  boundary.py\n\n  ROMS boundary utilities\n\n  Written by Brian Powell on 01/15/14\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\n\n\nimport seapy\nimport numpy as np\nimport netCDF4\nimport textwrap\nfrom collections import namedtuple\n\n# Define the sides of ROMS boundaries along with the DA ordering\n__side_info = namedtuple(""__side_info"", ""indices order xi"")\nsides = {""west"": __side_info((np.s_[:], 0), 1, False),\n         ""south"": __side_info((0, np.s_[:]), 2, True),\n         ""east"": __side_info((np.s_[:], -1), 3, False),\n         ""north"": __side_info((-1, np.s_[:]), 4, True)}\n\n\ndef from_roms(roms_file, bry_file, grid=None, records=None,\n              clobber=False, cdl=None):\n    """"""\n    Given a ROMS history, average, or climatology file, generate\n    boundary conditions on the same grid.\n\n    Parameters\n    ----------\n    roms_file : string or list,\n        ROMS source (history, average, climatology file)\n    bry_file : string,\n        output boundary file,\n    grid : seapy.model.grid or string, optional,\n        ROMS grid for boundaries\n    records : array, optional,\n        record indices to put into the boundary\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n\n    Returns\n    -------\n    None\n\n    """"""\n    if grid is None:\n        grid = seapy.model.asgrid(roms_file)\n    else:\n        grid = seapy.model.asgrid(grid)\n    ncroms = seapy.netcdf(roms_file)\n    src_ref, time = seapy.roms.get_reftime(ncroms)\n    records = np.arange(0, len(ncroms.variables[time][:])) \\\n        if records is None else records\n\n    # Create the boundary file and fill up the descriptive data\n    ncbry = seapy.roms.ncgen.create_bry(bry_file,\n                                        eta_rho=grid.eta_rho, xi_rho=grid.xi_rho,\n                                        s_rho=grid.n, reftime=src_ref,\n                                        cdl=cdl, clobber=clobber,\n                                        title=""generated from "" + roms_file)\n    brytime = seapy.roms.get_timevar(ncbry)\n    grid.to_netcdf(ncbry)\n    ncbry.variables[brytime][:] = seapy.roms.date2num(\n        seapy.roms.num2date(ncroms, time, records), ncbry, brytime)\n\n    for var in seapy.roms.fields:\n        if var in ncroms.variables:\n            for bry in sides:\n                ndim = seapy.roms.fields[var][""dims""]\n                if ndim == 3:\n                    ncbry.variables[""_"".join((var, bry))][:] = \\\n                        ncroms.variables[var][records, :,\n                                              sides[bry].indices[0],\n                                              sides[bry].indices[1]]\n                elif ndim == 2:\n                    ncbry.variables[""_"".join((var, bry))][:] = \\\n                        ncroms.variables[var][records,\n                                              sides[bry].indices[0],\n                                              sides[bry].indices[1]]\n    ncbry.close()\n    pass\n\n\ndef gen_ncks(parent_file, grid, sponge=0, pad=3):\n    """"""\n    Create the ncks commands for extracting a section of a global model\n    that encompasses each of the boundaries of the given grid. The reason\n    for this is that often we end up with massive global files, and we do\n    not want to interpolate the entirety onto our regional grids (if our\n    regional grid is large also), but only the boundary and nudge/sponge\n    region.\n\n    This script simply does the computation and outputs the necessary `ncks`\n    commands that the user needs to produce global boundary region and\n    boundary ""grid"" for the regional grid to then use in the interpolation\n    (e.g., seapy.interp.to_clim). This is purely to save disk and cpu\n    expense, and it is non-trivial to use.\n\n    Parameters\n    ----------\n    parent_file : seapy.model.grid or string,\n        Parent file (HYCOM, etc.)\n    grid : seapy.model.grid or string, optional,\n        ROMS grid for boundaries\n    sponge : int,\n        Width to extract along each boundary. If 0, only the boundary itself\n        will be extracted.\n    pad : int,\n        Additional rows/columns to extract from parent around the region\n\n    Returns\n    -------\n    None\n    """"""\n    import re\n\n    parent_grid = seapy.model.asgrid(parent_file)\n    child_grid = seapy.model.asgrid(grid)\n    fre = re.compile(\'.nc\')\n    # Make sure we are on the same coordinates\n    if parent_grid.east() != child_grid.east():\n        if child_grid.east():\n            parent_grid.lon_rho[parent_grid.lon_rho < 0] += 360\n        else:\n            parent_grid.lon_rho[parent_grid.lon_rho > 180] -= 360\n\n    # Loop over each side of the grid and determine the indices from the\n    # parent and child\n    for side in sides:\n        # Figure out which dimension this side is on and determine all\n        # of the indices needed\n        idx = sides[side].indices\n        if isinstance(idx[0], int):\n            pdim = parent_grid.key[""lat_rho""]\n            cdim = ""eta""\n            if idx[0] == -1:\n                idx = np.s_[-(sponge + 2):, :]\n                # pdidx = ""{:d},"".format(parent_grid.lat_rho.shape[0]-sponge-2)\n                cdidx = ""{:d},"".format(child_grid.eta_rho - sponge - 2)\n                pass\n            else:\n                idx = np.s_[:sponge + 1, :]\n                cdidx = "",{:d}"".format(sponge + 1)\n            l = np.where(np.logical_and(\n                parent_grid.lat_rho >= np.min(child_grid.lat_rho[idx]),\n                parent_grid.lat_rho <= np.max(child_grid.lat_rho[idx])))\n            i = np.maximum(0, np.min(l[0]) - pad)\n            j = np.minimum(parent_grid.lat_rho.shape[0],\n                           np.max(l[0]) + pad + 1)\n        else:\n            pdim = parent_grid.key[""lon_rho""]\n            cdim = ""xi""\n            if idx[1] == -1:\n                idx = np.s_[:, -(sponge + 2):]\n                cdidx = ""{:d},"".format(child_grid.xi_rho - sponge - 2)\n            else:\n                idx = np.s_[:, :sponge + 1]\n                cdidx = "",{:d}"".format(sponge + 1)\n            l = np.where(np.logical_and(\n                parent_grid.lon_rho >= np.min(child_grid.lon_rho[idx]),\n                parent_grid.lon_rho <= np.max(child_grid.lon_rho[idx])))\n            i = np.maximum(0, np.min(l[1]) - pad)\n            j = np.minimum(parent_grid.lon_rho.shape[1],\n                           np.max(l[1]) + pad + 1)\n\n        # Put the indices together into strings for extracting out new files\n        pdidx = ""{:d},{:d}"".format(i, j)\n\n        # Display the commands:\n        cmd = ""ncks""\n        pfiles = ""{:s} {:s}"".format(parent_grid.filename,\n                                    fre.sub(""_{:s}.nc"".format(side),\n                                            parent_grid.filename))\n        cfiles = ""{:s} {:s}"".format(child_grid.filename,\n                                    fre.sub(""_{:s}.nc"".format(side),\n                                            child_grid.filename))\n\n        grids = (\'rho\', \'u\', \'v\', \'psi\')\n        if parent_grid.cgrid:\n            pdim = \' -d\'.join([""{:s}_{:s},{:s}"".format(pdim, k, pdidx)\n                               for k in grids])\n        else:\n            pdim = ""{:s},{:s}"".format(pdim, pdidx)\n\n        if child_grid.cgrid:\n            cdim = \' -d\'.join([""{:s}_{:s},{:s}"".format(cdim, k, cdidx)\n                               for k in grids])\n        else:\n            cdim = ""{:s},{:s}"".format(cdim, cdidx)\n\n        print(""-"" * 40 + ""\\n"" + side + ""\\n"" + ""-"" * 40)\n        print(""{:s} -O -d{:s} {:s}"".format(cmd, pdim, pfiles))\n        print(""{:s} -O -d{:s} {:s}"".format(cmd, cdim, cfiles))\n\n    pass\n\n\ndef from_std(std_filename, bry_std_file, fields=None, clobber=False, cdl=None):\n    """"""\n    Generate the boundary standard deviations file for 4DVAR from the\n    standard deviation of a boundary file. Best to use nco:\n\n    $ ncwa -a bry_time roms_bry_file tmp.nc\n\n    $ ncbo -O -y sub roms_bry_file tmp.nc tmp.nc\n\n    $ ncra -y rmssdn tmp.nc roms_bry_std.nc\n\n    to generate the standard deviations. This method simply takes the\n    standard deviations of the boundaries and puts them into the\n    proper format for ROMS 4DVAR.\n\n    Parameters\n    ----------\n    std_filename : string or list,\n        Filename of the boundary standard deviation file\n    bry_std_file : string,\n        Filename of the boundary standard deviations file to create\n    fields : list, optional,\n        ROMS fields to generate boundaries for. The default are the\n        standard fields as defined in seapy.roms.fields\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n\n    Returns\n    -------\n    None\n    """"""\n    ncstd = seapy.netcdf(std_filename)\n    eta_rho = len(ncstd.dimensions[""eta_rho""])\n    xi_rho = len(ncstd.dimensions[""xi_rho""])\n    s_rho = len(ncstd.dimensions[""s_rho""])\n    ncout = seapy.roms.ncgen.create_da_bry_std(bry_std_file,\n                                               eta_rho=eta_rho, xi_rho=xi_rho,\n                                               s_rho=s_rho, clobber=clobber, cdl=cdl,\n                                               title=\'STD from \' + std_filename)\n    ncout.variables[""ocean_time""][:] = ncstd.variables[""bry_time""][0]\n\n    if fields is None:\n        fields = seapy.roms.fields\n\n    # Go over every side for every field and put it together\n    for var in fields:\n        vname = var + ""_obc""\n        if vname not in ncout.variables:\n            ncout.createVariable(vname, np.float32,\n                                 (\'ocean_time\', ""boundary"", ""s_rho"", ""IorJ""))\n        ndat = np.zeros(ncout.variables[vname].shape)\n        for bry in sides:\n            order = sides[bry].order - 1\n            dat = ncstd.variables[var + ""_"" + bry][0, :]\n            if dat.ndim == 1:\n                ndat[0, order, :len(dat)] = dat\n            else:\n                ndat[0, order, :, :dat.shape[1]] = dat\n        ncout.variables[vname][:] = ndat\n        ncout.sync()\n    pass\n\n\ndef gen_stations(filename, grid):\n    """"""\n    Generate a station file with stations at every boundary location for use in\n    nesting one grid within another.\n\n    Parameters\n    ----------\n    filename: string\n        Input name of station file to create\n    grid: string or seapy.model.grid\n        Input grid to generate station file from. If string, it will open\n        the grid file. If grid, it will use the grid information\n\n    Returns\n    -------\n    None\n\n    """"""\n    grid = seapy.model.asgrid(grid)\n\n    # Put together the boundaries\n    lon = np.concatenate([grid.lon_rho[0, :], grid.lon_rho[-1, :],\n                          grid.lon_rho[:, 0], grid.lon_rho[:, -1]])\n    lat = np.concatenate([grid.lat_rho[0, :], grid.lat_rho[-1, :],\n                          grid.lat_rho[:, 0], grid.lat_rho[:, -1]])\n    Npts = len(lon)\n\n    header = """"""\\\n\n    ! Switch to control the writing of stations data within nested and/or multiple\n    ! connected grids, [1:Ngrids].\n\n       Lstations == T\n\n    ! Logical switches (TRUE/FALSE) to activate writing of fields in STATION\n    ! output file, [Sout(:,ng), ng=1, Ngrids].\n\n    Sout(idUvel) == T       ! u                  3D U-velocity\n    Sout(idVvel) == T       ! v                  3D V-velocity\n    Sout(idWvel) == F       ! w                  3D W-velocity\n    Sout(idOvel) == F       ! omega              3D omega vertical velocity\n    Sout(idUbar) == T       ! ubar               2D U-velocity\n    Sout(idVbar) == T       ! vbar               2D V-velocity\n    Sout(idFsur) == T       ! zeta               free-surface\n    Sout(idBath) == F       ! bath               time-dependent bathymetry\n\n    Sout(idTvar) == T T     ! temp, salt, ...    all (NT) tracers\n\n    Sout(idUsms) == F       ! sustr              surface U-stress\n    Sout(idVsms) == F       ! svstr              surface V-stress\n    Sout(idUbms) == F       ! bustr              bottom U-stress\n    Sout(idVbms) == F       ! bvstr              bottom V-stress\n\n    Sout(idUbrs) == F       ! bustrc             bottom U-current stress\n    Sout(idVbrs) == F       ! bvstrc             bottom V-current stress\n    Sout(idUbws) == F       ! bustrw             bottom U-wave stress\n    Sout(idVbws) == F       ! bvstrw             bottom V-wave stress\n    Sout(idUbcs) == F       ! bustrcwmax         bottom max wave-current U-stress\n    Sout(idVbcs) == F       ! bvstrcwmax         bottom max wave-current V-stress\n\n    Sout(idUbot) == F       ! Ubot               bed wave orbital U-velocity\n    Sout(idVbot) == F       ! Vbot               bed wave orbital V-velocity\n    Sout(idUbur) == F       ! Ur                 bottom U-velocity above bed\n    Sout(idVbvr) == F       ! Vr                 bottom V-velocity above bed\n\n    Sout(idW2xx) == F       ! Sxx_bar            2D radiation stress, Sxx component\n    Sout(idW2xy) == F       ! Sxy_bar            2D radiation stress, Sxy component\n    Sout(idW2yy) == F       ! Syy_bar            2D radiation stress, Syy component\n    Sout(idU2rs) == F       ! Ubar_Rstress       2D radiation U-stress\n    Sout(idV2rs) == F       ! Vbar_Rstress       2D radiation V-stress\n    Sout(idU2Sd) == F       ! ubar_stokes        2D U-Stokes velocity\n    Sout(idV2Sd) == F       ! vbar_stokes        2D V-Stokes velocity\n\n    Sout(idW3xx) == F       ! Sxx                3D radiation stress, Sxx component\n    Sout(idW3xy) == F       ! Sxy                3D radiation stress, Sxy component\n    Sout(idW3yy) == F       ! Syy                3D radiation stress, Syy component\n    Sout(idW3zx) == F       ! Szx                3D radiation stress, Szx component\n    Sout(idW3zy) == F       ! Szy                3D radiation stress, Szy component\n    Sout(idU3rs) == F       ! u_Rstress          3D U-radiation stress\n    Sout(idV3rs) == F       ! v_Rstress          3D V-radiation stress\n    Sout(idU3Sd) == F       ! u_stokes           3D U-Stokes velocity\n    Sout(idV3Sd) == F       ! v_stokes           3D V-Stokes velocity\n\n    Sout(idWamp) == F       ! Hwave              wave height\n    Sout(idWlen) == F       ! Lwave              wave length\n    Sout(idWdir) == F       ! Dwave              wave direction\n    Sout(idWptp) == F       ! Pwave_top          wave surface period\n    Sout(idWpbt) == F       ! Pwave_bot          wave bottom period\n    Sout(idWorb) == F       ! Ub_swan            wave bottom orbital velocity\n    Sout(idWdis) == F       ! Wave_dissip        wave dissipation\n\n    Sout(idPair) == F       ! Pair               surface air pressure\n    Sout(idUair) == F       ! Uair               surface U-wind component\n    Sout(idVair) == F       ! Vair               surface V-wind component\n\n    Sout(idTsur) == F F     ! shflux, ssflux     surface net heat and salt flux\n    Sout(idLhea) == F       ! latent             latent heat flux\n    Sout(idShea) == F       ! sensible           sensible heat flux\n    Sout(idLrad) == F       ! lwrad              longwave radiation flux\n    Sout(idSrad) == F       ! swrad              shortwave radiation flux\n    Sout(idEmPf) == F       ! EminusP            E-P flux\n    Sout(idevap) == F       ! evaporation        evaporation rate\n    Sout(idrain) == F       ! rain               precipitation rate\n\n    Sout(idDano) == F       ! rho                density anomaly\n    Sout(idVvis) == F       ! AKv                vertical viscosity\n    Sout(idTdif) == F       ! AKt                vertical T-diffusion\n    Sout(idSdif) == F       ! AKs                vertical Salinity diffusion\n    Sout(idHsbl) == F       ! Hsbl               depth of surface boundary layer\n    Sout(idHbbl) == F       ! Hbbl               depth of bottom boundary layer\n    Sout(idMtke) == F       ! tke                turbulent kinetic energy\n    Sout(idMtls) == F       ! gls                turbulent length scale\n\n    ! Logical switches (TRUE/FALSE) to activate writing of exposed sediment\n    ! layer properties into STATIONS output file.  Currently, MBOTP properties\n    ! are expected for the bottom boundary layer and/or sediment models:\n    !\n    ! idBott( 1=isd50)   grain_diameter          mean grain diameter\n    ! idBott( 2=idens)   grain_density           mean grain density\n    ! idBott( 3=iwsed)   settling_vel            mean settling velocity\n    ! idBott( 4=itauc)   erosion_stres           critical erosion stress\n    ! idBott( 5=irlen)   ripple_length           ripple length\n    ! idBott( 6=irhgt)   ripple_height           ripple height\n    ! idBott( 7=ibwav)   bed_wave_amp            wave excursion amplitude\n    ! idBott( 8=izdef)   Zo_def                  default bottom roughness\n    ! idBott( 9=izapp)   Zo_app                  apparent bottom roughness\n    ! idBott(10=izNik)   Zo_Nik                  Nikuradse bottom roughness\n    ! idBott(11=izbio)   Zo_bio                  biological bottom roughness\n    ! idBott(12=izbfm)   Zo_bedform              bed form bottom roughness\n    ! idBott(13=izbld)   Zo_bedload              bed load bottom roughness\n    ! idBott(14=izwbl)   Zo_wbl                  wave bottom roughness\n    ! idBott(15=iactv)   active_layer_thickness  active layer thickness\n    ! idBott(16=ishgt)   saltation               saltation height\n    !\n    !                                 1 1 1 1 1 1 1\n    !               1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6\n\n    Sout(idBott) == F F F F F F F F F F F F F F F F\n\n    ! Number of stations to process in each nested grid.  These values are\n    ! essential because the station arrays are dynamically allocated using\n    ! these values, [1:Ngrids].\n\n    """"""\n    stations = """"""\n    ! Station locations for all grids in any desired order.  The horizontal\n    ! location for a particular station may be specified in terms of fractional\n    ! (I,J) grid pairs (FLAG=0) or (longitude,latitude) grid pairs (FLAG=1).\n    ! Here, FLAG is a special switch and may be used for multiple purposes.\n    ! The GRID column indicates nested grid number to process. This value must\n    ! be one in non-nested applications.  The COMMENT section is ignored during\n    ! reading and may be used to help documentation.\n\n    POS =  GRID  FLAG      X-POS       Y-POS     COMMENT\n    """"""\n    with open(filename, ""w"") as text_file:\n        print(""! BOUNDARY STATIONS FOR GRID: {}"".format(grid.filename),\n              file=text_file)\n        print(textwrap.dedent(header), file=text_file)\n        print(""        NSTATION ==  {}"".format(Npts), file=text_file)\n        print(textwrap.dedent(stations), file=text_file)\n        for i in range(Npts):\n            print(""        1     1    {0:10.6f}   {1:10.6f}   BRY"".format(\n                lon[i], lat[i]), file=text_file)\n\n    pass\n\n\ndef from_stations(station_file, bry_file, grid=None):\n    """"""\n    Construct a boundary forcing file from a stations file generated by a parent-grid.\n    The stations.in file must have been generated by the seapy.roms.gen_stations method;\n    otherwise, the order will be incorrect.\n\n    Parameters\n    ==========\n    station_file : string\n        Filename of the stations file that is the source for the boundary data\n    bry_file : string\n        Filename of the boundary conditions file to generate\n    grid : string or seapy.model.grid\n        Grid that the boundary conditions are created for\n\n    Returns\n    -------\n    None\n\n    """"""\n    grid = seapy.model.asgrid(grid)\n    ncstation = netCDF4.Dataset(station_file)\n    src_ref, time = seapy.roms.get_reftime(ncstation)\n\n    # Create the boundary file and fill up the descriptive data\n    ncbry = seapy.roms.ncgen.create_bry(bry_file,\n                                        eta_rho=grid.eta_rho, xi_rho=grid.xi_rho,\n                                        s_rho=grid.n, reftime=src_ref, clobber=False,\n                                        title=""generated from "" + station_file)\n    grid.to_netcdf(ncbry)\n\n    # Load the times: we need to see if the times are duplicated\n    # because if using assimilation, they may be duplicated for every\n    # outer-loop. Currently, it overwrites the first one each time (this\n    # will need to be fixed if ROMS is fixed).\n    statime = ncstation.variables[time][:]\n    dup = np.where(statime[1:] == statime[0])[0]\n    rng = np.s_[:]\n    if dup.size > 0:\n        rng = np.s_[0:np.min(dup)]\n        statime = statime[rng]\n    brytime = seapy.roms.get_timevar(ncbry)\n    ncbry.variables[brytime][:] = seapy.roms.date2num(\n        seapy.roms.num2date(ncstation, time, rng), ncbry, brytime)\n\n    # Set up the indices\n    bry = {\n        ""south"": range(0, grid.lm),\n        ""north"": range(grid.lm, 2 * grid.lm),\n        ""west"": range(2 * grid.lm, 2 * grid.lm + grid.ln),\n        ""east"": range(2 * grid.lm + grid.ln, 2 * (grid.lm + grid.ln))\n    }\n\n    # Get the information to construct the depths of the station data\n    sta_vt = ncstation.variables[""Vtransform""][:]\n    sta_hc = ncstation.variables[""hc""][:]\n    sta_s_rho = ncstation.variables[""s_rho""][:]\n    sta_cs_r = ncstation.variables[""Cs_r""][:]\n    sta_h = ncstation.variables[""h""][:]\n    sta_angle = ncstation.variables[""angle""][:]\n    sta_lon = ncstation.variables[""lon_rho""][:]\n    sta_lat = ncstation.variables[""lat_rho""][:]\n    sta_mask = np.ones(sta_lat.shape)\n    sta_mask[sta_lon * sta_lat > 1e10] = 0\n\n    # Load the station data as we need to manipulate it\n    sta_zeta = np.ma.masked_greater(ncstation.variables[""zeta""][rng], 100)\n    sta_ubar = np.ma.masked_greater(ncstation.variables[""ubar""][rng], 100)\n    sta_vbar = np.ma.masked_greater(ncstation.variables[""vbar""][rng], 100)\n    sta_temp = np.ma.masked_greater(ncstation.variables[""temp""][rng], 100)\n    sta_salt = np.ma.masked_greater(ncstation.variables[""salt""][rng], 100)\n    sta_u = np.ma.masked_greater(ncstation.variables[""u""][rng], 100)\n    sta_v = np.ma.masked_greater(ncstation.variables[""v""][rng], 100)\n    ncstation.close()\n\n    # Create the true positions and mask\n    grid_h = np.concatenate([grid.h[0, :], grid.h[-1, :],\n                             grid.h[:, 0], grid.h[:, -1]])\n    grid_lon = np.concatenate([grid.lon_rho[0, :], grid.lon_rho[-1, :],\n                               grid.lon_rho[:, 0], grid.lon_rho[:, -1]])\n    grid_lat = np.concatenate([grid.lat_rho[0, :], grid.lat_rho[-1, :],\n                               grid.lat_rho[:, 0], grid.lat_rho[:, -1]])\n    grid_mask = np.concatenate([grid.mask_rho[0, :], grid.mask_rho[-1, :],\n                                grid.mask_rho[:, 0], grid.mask_rho[:, -1]])\n    grid_angle = np.concatenate([grid.angle[0, :], grid.angle[-1, :],\n                                 grid.angle[:, 0], grid.angle[:, -1]])\n\n    # Search for bad stations due to child grid overlaying parent mask.\n    # Unfortunately, ROMS will give points that are not at the locations\n    # you specify if those points conflict with the mask. So, these points\n    # are simply replaced with the nearest.\n    dist = np.sqrt((sta_lon - grid_lon)**2 + (sta_lat - grid_lat)**2)\n    bad_pts = np.where(np.logical_and(dist > 0.001, grid_mask == 1))[0]\n    good_pts = np.where(np.logical_and(dist < 0.001, grid_mask == 1))[0]\n    for i in bad_pts:\n        didx = np.sqrt((sta_lon[i] - sta_lon[good_pts])**2 +\n                       (sta_lat[i] - sta_lat[good_pts])**2).argmin()\n        index = good_pts[didx]\n        sta_h[i] = sta_h[index]\n        sta_angle[i] = sta_angle[index]\n        sta_lon[i] = sta_lon[index]\n        sta_lat[i] = sta_lat[index]\n        sta_zeta[:, i] = sta_zeta[:, index]\n        sta_ubar[:, i] = sta_ubar[:, index]\n        sta_vbar[:, i] = sta_vbar[:, index]\n        sta_temp[:, i, :] = sta_temp[:, index, :]\n        sta_salt[:, i, :] = sta_salt[:, index, :]\n        sta_u[:, i, :] = sta_u[:, index, :]\n        sta_v[:, i, :] = sta_v[:, index, :]\n\n    # Construct the boundaries: a dictionary of boundary side and two element\n    # array whether the u[0] or v[1] dimensions need to be averaged\n    sides = {""north"": [True, False], ""south"": [True, False],\n             ""east"": [False, True], ""west"": [False, True]}\n    delta_angle = sta_angle - grid_angle\n    sta_ubar, sta_vbar = seapy.rotate(sta_ubar, sta_vbar, delta_angle)\n    sta_u, sta_v = seapy.rotate(sta_u, sta_v, np.tile(delta_angle,\n                                                      (sta_u.shape[-1], 1)).T)\n\n    # Set up the parameters for depth-interpolated\n    wght = 5\n    nx = 3\n    ny = 9\n\n    # Build a non-extrapolating field to interpolate. Generate the\n    # position and depth\n    def __expand_field(x):\n        shp = x.shape\n        y = np.zeros((shp[0] + 2, shp[1] + 2))\n        y[1:-1, 1:-1] = x\n        y[1:-1, 0] = x[:, 0]\n        y[1:-1, -1] = x[:, -1]\n        y[0, :] = y[1, :]\n        y[-1, :] = y[-2, :]\n        return y\n\n    for side in sides:\n        print(side)\n\n        # Masks\n        sta_ocean = np.where(sta_mask[bry[side]] == 1)[0]\n        ocean = np.where(grid_mask[bry[side]] == 1)[0]\n\n        # If we have a masked boundary, skip it\n        if not np.any(ocean):\n            continue\n\n        # 1) Zeta\n        ncbry.variables[""zeta_"" + side][:,\n                                        ocean] = sta_zeta[:, bry[side]][:, ocean]\n\n        # 2) Ubar\n        if sides[side][0]:\n            ncbry.variables[""ubar_"" + side][:] = 0.5 * (\n                sta_ubar[:, bry[side][0:-1]] + sta_ubar[:, bry[side][1:]])\n        else:\n            ncbry.variables[""ubar_"" + side][:] = sta_ubar[:, bry[side]]\n\n        # 3) Vbar\n        if sides[side][1]:\n            ncbry.variables[""vbar_"" + side][:] = 0.5 * (\n                sta_vbar[:, bry[side][0:-1]] + sta_vbar[:, bry[side][1:]])\n        else:\n            ncbry.variables[""vbar_"" + side][:] = sta_vbar[:, bry[side]]\n\n        # For 3D variables, we need to loop through time and interpolate\n        # onto the child grid. Construct the distances\n        x = np.zeros(len(bry[side]))\n        x[1:] = np.cumsum(seapy.earth_distance(grid_lon[bry[side][0:-1]],\n                                               grid_lat[bry[side][0:-1]],\n                                               grid_lon[bry[side][1:]],\n                                               grid_lat[bry[side][1:]]))\n        sta_x = seapy.adddim(x, len(sta_s_rho))\n        x = seapy.adddim(x, len(grid.s_rho))\n\n        for n, t in seapy.progressbar.progress(enumerate(statime), statime.size):\n            sta_depth = seapy.roms.depth(sta_vt, sta_h[bry[side]], sta_hc,\n                                         sta_s_rho, sta_cs_r, sta_zeta[n, bry[side]])\n            depth = seapy.roms.depth(grid.vtransform, grid_h[bry[side]],\n                                     grid.hc, grid.s_rho, grid.cs_r, sta_zeta[n, bry[side]])\n\n            in_x = __expand_field(sta_x[:, sta_ocean])\n            in_x[:, 0] = in_x[:, 0] - 3600\n            in_x[:, -1] = in_x[:, -1] + 3600\n            in_depth = __expand_field(sta_depth[:, sta_ocean])\n            in_depth[0, :] = in_depth[0, :] - 1000\n            in_depth[-1, :] = in_depth[-1, :] + 10\n\n            # 4) Temp\n            in_data = __expand_field(np.transpose(\n                sta_temp[n, bry[side], :][sta_ocean, :]))\n            ncbry.variables[""temp_"" + side][n, :] = 0.0\n            ncbry.variables[""temp_"" + side][n, :, ocean], pmap = seapy.oa.oasurf(\n                in_x, in_depth, in_data,\n                x[:, ocean], depth[:, ocean], nx=nx, ny=ny, weight=wght)\n\n            # 5) Salt\n            in_data = __expand_field(np.transpose(\n                sta_salt[n, bry[side], :][sta_ocean, :]))\n            ncbry.variables[""salt_"" + side][n, :] = 0.0\n            ncbry.variables[""salt_"" + side][n, :, ocean], pmap = seapy.oa.oasurf(\n                in_x, in_depth, in_data,\n                x[:, ocean], depth[:, ocean], pmap=pmap, nx=nx, ny=ny, weight=wght)\n\n            # 6) U\n            in_data = __expand_field(np.transpose(\n                sta_u[n, bry[side], :][sta_ocean, :]))\n            data = np.zeros(x.shape)\n            data[:, ocean], pmap = seapy.oa.oasurf(in_x, in_depth, in_data,\n                                                   x[:, ocean],\n                                                   depth[:, ocean],\n                                                   pmap=pmap, nx=nx, ny=ny, weight=wght)\n            if sides[side][0]:\n                ncbry.variables[""u_"" + side][n, :] = 0.5 * (\n                    data[:, 0:-1] + data[:, 1:])\n            else:\n                ncbry.variables[""u_"" + side][n, :] = data\n\n            # 7) V\n            in_data = __expand_field(np.transpose(\n                sta_v[n, bry[side], :][sta_ocean, :]))\n            data = data * 0\n            data[:, ocean], pmap = seapy.oa.oasurf(in_x, in_depth, in_data,\n                                                   x[:, ocean],\n                                                   depth[:, ocean],\n                                                   pmap=pmap, nx=nx, ny=ny,\n                                                   weight=wght)\n            if sides[side][1]:\n                ncbry.variables[""v_"" + side][n, :] = 0.5 * (\n                    data[:, 0:-1] + data[:, 1:])\n            else:\n                ncbry.variables[""v_"" + side][n, :] = data\n            ncbry.sync()\n    ncbry.close()\n    pass\n\n\ndef detide(grid, bryfile, tidefile, tides=None, tide_start=None):\n    """"""\n    Given a boundary file, detide the barotropic components and create tidal\n    forcing file for the grid. This method will update the given boundary file.\n\n    Parameters\n    ----------\n    grid : seapy.model.grid or string,\n       The grid that defines the boundaries shape and mask\n    bryfile : string,\n       The boundary file to detide\n    tidefile : string,\n       The output tidal forcing file with the tide spectral forcing\n    tides : string array, optional\n       Array of strings defining which tides to extract. Defaults to the\n       standard 11 constituents.\n    tide_start : datetime, optional\n       The reference date to use for the tide forcing. If None, the\n       center of the time period is used.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Make a long time-series boundary conditions from a group of boundary files,\n    skipping the last record of each file to prevent overlap (if there are 100 records\n    in each file). Afterwards, detide the resulting file.\n\n    >>> !ncrcat -dbry_time,0,,100,99 bry_out_*nc bry_detide.nc\n    >>> seapy.roms.boundary.detide(""mygrid.nc"", ""bry_detide.nc"", ""tide_out.nc"")\n\n    """"""\n    import datetime\n\n    if not tides:\n        tides = seapy.tide.default_tides\n    else:\n        tides = np.atleast_1d(tides)\n\n    # Load Files\n    grid = seapy.model.asgrid(grid)\n    bry = netCDF4.Dataset(bryfile, ""a"")\n\n    # Get the definitions of the boundary file\n    epoch, timevar = seapy.roms.get_reftime(bry)\n    time = seapy.roms.num2date(bry, timevar)\n\n    # Pick the time for the tide file reference\n    if not tide_start:\n        tide_start = time[0] + (time[-1] - time[0]) / 2\n        tide_start = datetime.datetime(\n            tide_start.year, tide_start.month, tide_start.day)\n\n    try:\n        s_rho = len(bry.dimensions[\'s_rho\'])\n    except:\n        s_rho = grid.n\n\n    # Set variables to detide\n    detide_vars = [\'zeta\', \'ubar\', \'vbar\']\n\n    # Create the tide forcing file\n    bry.detide = ""Detided to generate tide forcing: {:s}"".format(tidefile)\n\n    # Detide the free-surface\n    eamp = np.zeros((len(tides), grid.eta_rho, grid.xi_rho))\n    epha = np.zeros((len(tides), grid.eta_rho, grid.xi_rho))\n    cmin = np.zeros((len(tides), grid.eta_rho, grid.xi_rho))\n    cmax = np.zeros((len(tides), grid.eta_rho, grid.xi_rho))\n    cang = np.zeros((len(tides), grid.eta_rho, grid.xi_rho))\n    cpha = np.zeros((len(tides), grid.eta_rho, grid.xi_rho))\n\n    for side in sides:\n        lvar = ""zeta_"" + side\n        idx = sides[side].indices\n        lat = grid.lat_rho[idx[0], idx[1]]\n        size = grid.xi_rho if sides[side].xi else grid.eta_rho\n        if lvar in bry.variables:\n            print(lvar)\n            zeta = np.ma.array(bry.variables[lvar][:])\n            mask = np.ma.getmaskarray(zeta)\n            # Detide\n            for i in seapy.progressbar.progress(range(size)):\n                if np.any(mask[:, i]):\n                    continue\n                out = seapy.tide.fit(time, zeta[:, i], tides=tides, lat=lat[i],\n                                     tide_start=tide_start)\n                zeta[:, i] -= out[\'fit\'].data\n\n                # Save the amp/phase in the tide file\n                for n, t in enumerate(tides):\n                    if sides[side].xi:\n                        eamp[n, idx[0], i] = out[\'major\'][t].amp\n                        epha[n, idx[0], i] = np.mod(\n                            out[\'major\'][t].phase, 2 * np.pi)\n                    else:\n                        eamp[n, i, idx[1]] = out[\'major\'][t].amp\n                        epha[n, i, idx[1]] = np.mod(\n                            out[\'major\'][t].phase, 2 * np.pi)\n\n            # Save out the detided information\n            bry.variables[lvar][:] = zeta\n            zeta = [0]\n            bry.sync()\n\n        # Detide the barotropic velocity\n        uvar = ""ubar_"" + side\n        vvar = ""vbar_"" + side\n        if uvar in bry.variables and vvar in bry.variables:\n            print(uvar, vvar)\n            ubar = np.zeros((len(time), size))\n            vbar = np.zeros((len(time), size))\n\n            # Load data, put onto rho-grid, and rotate\n            bubar = np.ma.array(bry.variables[uvar][:]).filled(0)\n            bvbar = np.ma.array(bry.variables[vvar][:]).filled(0)\n            if sides[side].xi:\n                ubar[:, 1:-1] = 0.5 * (bubar[:, 1:] + bubar[:, :-1])\n                ubar[:, 0] = bubar[:, 1]\n                ubar[:, -1] = bubar[:, -2]\n                vbar = bvbar.copy()\n            else:\n                vbar[:, 1:-1] = 0.5 * (bvbar[:, 1:] + bvbar[:, :-1])\n                vbar[:, 0] = bvbar[:, 1]\n                vbar[:, -1] = bvbar[:, -2]\n                ubar = bubar.copy()\n            ubar, vbar = seapy.rotate(ubar, vbar, grid.angle[idx[0], idx[1]])\n            bubar = bvbar = []\n\n            # Detide\n            for i in seapy.progressbar.progress(range(size)):\n                if np.any(mask[:, i]):\n                    continue\n                out = seapy.tide.fit(\n                    time, ubar[:, i] + 1j * vbar[:, i], tides=tides, lat=lat[i],\n                    tide_start=tide_start)\n                ubar[:, i] -= np.real(out[\'fit\'])\n                vbar[:, i] -= np.imag(out[\'fit\'])\n\n                # Save the amp/phase in the tide file\n                for n, t in enumerate(tides):\n                    if sides[side].xi:\n                        cmax[n, idx[0], i] = out[\'major\'][t].amp\n                        cmin[n, idx[0], i] = out[\'minor\'][t].amp\n                        cpha[n, idx[0], i] = out[\'major\'][t].phase\n                        cang[n, idx[0], i] = out[\'minor\'][t].phase\n                    else:\n                        cmax[n, i, idx[1]] = out[\'major\'][t].amp\n                        cmin[n, i, idx[1]] = out[\'minor\'][t].amp\n                        cpha[n, i, idx[1]] = out[\'major\'][t].phase\n                        cang[n, i, idx[1]] = out[\'minor\'][t].phase\n\n            ubar, vbar = seapy.rotate(ubar, vbar, -grid.angle[idx[0], idx[1]])\n            if sides[side].xi:\n                bry.variables[uvar][:] = 0.5 * (ubar[:, 1:] + ubar[:, :-1])\n                bry.variables[vvar][:] = vbar\n            else:\n                bry.variables[vvar][:] = 0.5 * (vbar[:, 1:] + vbar[:, :-1])\n                bry.variables[uvar][:] = ubar\n            bry.sync()\n            ubar = vbar = []\n\n    # Have to duplicate the boundary tide info into the inner row/column\n    eamp[:, 1:-1, 1] = eamp[:, 1:-1, 0]\n    eamp[:, 1:-1, -2] = eamp[:, 1:-1, -1]\n    eamp[:, 1, 1:-1] = eamp[:, 0, 1:-1]\n    eamp[:, -2, 1:-1] = eamp[:, -1, 1:-1]\n    epha[:, 1:-1, 1] = epha[:, 1:-1, 0]\n    epha[:, 1:-1, -2] = epha[:, 1:-1, -1]\n    epha[:, 1, 1:-1] = epha[:, 0, 1:-1]\n    epha[:, -2, 1:-1] = epha[:, -1, 1:-1]\n    cmax[:, 1:-1, 1] = cmax[:, 1:-1, 0]\n    cmax[:, 1:-1, -2] = cmax[:, 1:-1, -1]\n    cmax[:, 1, 1:-1] = cmax[:, 0, 1:-1]\n    cmax[:, -2, 1:-1] = cmax[:, -1, 1:-1]\n    cmin[:, 1:-1, 1] = cmin[:, 1:-1, 0]\n    cmin[:, 1:-1, -2] = cmin[:, 1:-1, -1]\n    cmin[:, 1, 1:-1] = cmin[:, 0, 1:-1]\n    cmin[:, -2, 1:-1] = cmin[:, -1, 1:-1]\n    cpha[:, 1:-1, 1] = cpha[:, 1:-1, 0]\n    cpha[:, 1:-1, -2] = cpha[:, 1:-1, -1]\n    cpha[:, 1, 1:-1] = cpha[:, 0, 1:-1]\n    cpha[:, -2, 1:-1] = cpha[:, -1, 1:-1]\n    cang[:, 1:-1, 1] = cang[:, 1:-1, 0]\n    cang[:, 1:-1, -2] = cang[:, 1:-1, -1]\n    cang[:, 1, 1:-1] = cang[:, 0, 1:-1]\n    cang[:, -2, 1:-1] = cang[:, -1, 1:-1]\n\n    # Set the tide reference\n    tideout = {}\n    tideout[\'tides\'] = tides\n    tideout[\'tide_start\'] = tide_start\n    tideout[\'Eamp\'] = eamp\n    tideout[\'Ephase\'] = epha\n    tideout[\'Cmajor\'] = cmax\n    tideout[\'Cminor\'] = cmin\n    tideout[\'Cphase\'] = cpha\n    tideout[\'Cangle\'] = cang\n\n    seapy.roms.tide.create_forcing(tidefile, tideout,\n                                   title=""Tides from "" + bryfile, epoch=epoch)\n    bry.close()\n\n    pass\n'"
seapy/roms/clim.py,5,"b'#!/usr/bin/env python\n""""""\n  clim.py\n\n  ROMS climatology utilities\n\n  Written by Brian Powell on 08/15/15\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\n\n\nimport seapy\nimport numpy as np\nimport netCDF4\n\n\ndef gen_bry_clim(clim_file, grid, bry, clobber=False, cdl=None):\n    """"""\n    Taking the results of gen_ncks and interpolation, stitch together\n    climatology files that were interpolated using only the boundary regions\n    into a single climatology (with no data where interpolation wasn\'t\n    performed).\n\n    Parameters\n    ----------\n    clim_file: str,\n        The name of the output climate file\n    grid: seapy.model.grid or str,\n        The output ROMS grid\n    bry: dict,\n        A dictionary prescribing the climatology file interpolated for each\n        boundary side.\n        {""west"":filename, ""south"":filename}, ...}\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n\n    Returns\n    -------\n        None\n    """"""\n    grid = seapy.model.asgrid(grid)\n\n    # Grab the first dictionary record and use it to determine the number\n    # of times in the new climatology file\n    nc = netCDF4.Dataset(bry[list(bry.keys())[0]])\n    reftime, time = seapy.roms.get_reftime(nc)\n    times = nc.variables[time][:]\n    nc.close()\n\n    # Create the new climatology file\n    ncout = seapy.roms.ncgen.create_clim(clim_file,\n                                         eta_rho=grid.ln, xi_rho=grid.lm, s_rho=grid.n,\n                                         reftime=reftime, clobber=clobber, cdl=cdl,\n                                         title=""stitched from boundary interpolation"")\n    ncout.variables[""clim_time""][:] = times\n\n    for side in bry:\n        if bry[side] is None:\n            continue\n        ncin = netCDF4.Dataset(bry[side])\n        for fld in seapy.roms.fields:\n            idx = [np.s_[:] for i in range(seapy.roms.fields[fld][""dims""] + 1)]\n            dat = ncin.variables[fld][:]\n            shp = dat.shape\n            if side == ""west"":\n                idx[-1] = np.s_[:shp[-1]]\n                pass\n            elif side == ""east"":\n                idx[-1] = np.s_[-shp[-1]:]\n                pass\n            elif side == ""north"":\n                idx[-2] = np.s_[-shp[-2]:]\n                pass\n            elif side == ""south"":\n                idx[-2] = np.s_[:shp[-2]]\n                pass\n            ncout.variables[fld][idx] = dat\n            ncout.sync()\n        ncin.close()\n    ncout.close()\n'"
seapy/roms/ezgrid.py,49,"b'#!/usr/bin/env python\n""""""\n  ezgrid.py\n\n  Functions for generating ROMS grid files\n\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\nimport numpy as np\nfrom datetime import datetime\nimport netCDF4\nimport seapy\nfrom collections import namedtuple\n\n\ndef create_grid(grid_file, lat, lon):\n    """"""\n    Create a new, basic grid. This will construct the ROMS grid netcdf file\n    given a set of latitude and longitude coordinates for the rho-grid.\n    The coordinates can be spaced however specified, and the grid will be\n    created; however, this does not guarantee the grid will be valid.\n    After the grid is created, the bathymetry and mask will need to be\n    generated independently.\n\n    Parameters\n    ----------\n    grid_file : string,\n      Name of the grid file to create. NOTE: this file will be\n      overwritten.\n    lat : ndarray,\n      latitude of the grid cells. The array must be the size\n      of the desired grid.\n    lon : ndarray,\n      longitude of the grid cells. The array must be the size\n      of the desired grid.\n\n    Returns\n    -------\n    netCDF4 :\n      The netcdf object of the new grid\n\n    Examples\n    --------\n    To create a basic, evenly spaced grid:\n\n    >>> lat = np.linspace(10,20,0.25)\n    >>> lon = np.linspace(-100,-80,0.25)\n    >>> lon, lat = np.meshgrid(lon, lat)\n    >>> create_grid(\'mygrid.nc\', lat, lon)\n\n    To create more advanced grids, simply generate the\n    2D arrays of lat and lon in the manner you want your cells\n    and call create_grid:\n\n    >>> create_grid(\'mygrid.nc\', lat, lon)\n\n    """"""\n\n    # Put lat/lon into proper arrays\n    lat = np.atleast_2d(lat)\n    lon = np.atleast_2d(lon)\n\n    if lat.shape != lon.shape:\n        raise AttributeError(""lat and lon shapes are not equal"")\n\n    # Calculate the angle between the points\n    angle = np.zeros(lat.shape)\n    angle[:, :-1] = seapy.earth_angle(lon[:, :-1],\n                                      lat[:, :-1], lon[:, 1:], lat[:, 1:])\n    angle[:, -1] = angle[:, -2]\n\n    # Calculate distances/parameters\n    f = 2.0 * 7.2921150e-5 * np.sin(lat * np.pi / 180.0)\n    dx = np.zeros(f.shape)\n    dy = np.zeros(f.shape)\n    dx[:, 1:] = seapy.earth_distance(\n        lon[:, 1:], lat[:, 1:], lon[:, :-1], lat[:, :-1])\n    dy[1:, :] = seapy.earth_distance(\n        lon[1:, :], lat[1:, :], lon[:-1, :], lat[:-1, :])\n    dx[:, 0] = dx[:, 1]\n    dy[0, :] = dy[1, :]\n    pm = 1.0 / dx\n    pn = 1.0 / dy\n    dndx = np.zeros(dx.shape)\n    dmde = np.zeros(dx.shape)\n    dndx[:, 1:-1] = 0.5 * (dy[:, 2:] - dy[:, :-2])\n    dmde[1:-1, :] = 0.5 * (dx[2:, :] - dx[:-2, :])\n    xl = seapy.earth_distance(\n        np.min(lon), np.mean(lat), np.max(lon), np.mean(lat))\n    el = seapy.earth_distance(\n        np.mean(lon), np.min(lat), np.mean(lon), np.max(lat))\n\n    # Generate rho-grid coordinates\n    x_rho = np.zeros(lat.shape)\n    y_rho = np.zeros(lat.shape)\n    x_rho[:, 1:] = seapy.earth_distance(\n        lon[:, :-1], lat[:, :-1], lon[:, 1:], lat[:, 1:])\n    x_rho = np.cumsum(x_rho, axis=1)\n    y_rho[1:, :] = seapy.earth_distance(\n        lon[:-1, :], lat[:-1, :], lon[1:, :], lat[1:, :])\n    y_rho = np.cumsum(y_rho, axis=0)\n\n    # Create u-grid\n    lat_u = 0.5 * (lat[:, 1:] + lat[:, :-1])\n    lon_u = 0.5 * (lon[:, 1:] + lon[:, 0:-1])\n    x_u = np.zeros(lat_u.shape)\n    y_u = np.zeros(lat_u.shape)\n    x_u[:, 1:] = seapy.earth_distance(\n        lon_u[:, :-1], lat_u[:, :-1], lon_u[:, 1:], lat_u[:, 1:])\n    x_u = np.cumsum(x_u, axis=1)\n    y_u[1:, :] = seapy.earth_distance(\n        lon_u[:-1, :], lat_u[:-1, :], lon_u[1:, :], lat_u[1:, :])\n    y_u = np.cumsum(y_u, axis=0)\n\n    # Create v-grid\n    lat_v = 0.5 * (lat[1:, :] + lat[0:-1, :])\n    lon_v = 0.5 * (lon[1:, :] + lon[0:-1, :])\n    x_v = np.zeros(lat_v.shape)\n    y_v = np.zeros(lat_v.shape)\n    x_v[:, 1:] = seapy.earth_distance(\n        lon_v[:, :-1], lat_v[:, :-1], lon_v[:, 1:], lat_v[:, 1:])\n    x_v = np.cumsum(x_v, axis=1)\n    y_v[1:, :] = seapy.earth_distance(\n        lon_v[:-1, :], lat_v[:-1, :], lon_v[1:, :], lat_v[1:, :])\n    y_v = np.cumsum(y_v, axis=0)\n\n    # Create psi-grid\n    lat_psi = lat_v[:, :-1]\n    lon_psi = lon_u[:-1, :]\n    x_psi = np.zeros(lat_psi.shape)\n    y_psi = np.zeros(lat_psi.shape)\n    x_psi[:, 1:] = seapy.earth_distance(\n        lon_psi[:, :-1], lat_psi[:, :-1], lon_psi[:, 1:], lat_psi[:, 1:])\n    x_psi = np.cumsum(x_psi, axis=1)\n    y_psi[1:, :] = seapy.earth_distance(\n        lon_psi[:-1, :], lat_psi[:-1, :], lon_psi[1:, :], lat_psi[1:, :])\n    y_psi = np.cumsum(y_psi, axis=0)\n\n    # Create the new grid\n    nc = seapy.roms.ncgen.create_grid(\n        grid_file, lat.shape[0], lat.shape[1], clobber=True)\n\n    nc.variables[""xl""][:] = xl\n    nc.variables[""el""][:] = el\n    nc.variables[""spherical""][:] = 1\n    nc.variables[""f""][:] = f\n    nc.variables[""pm""][:] = pm\n    nc.variables[""pn""][:] = pn\n    nc.variables[""dndx""][:] = dndx\n    nc.variables[""dmde""][:] = dmde\n    nc.variables[""x_rho""][:] = x_rho\n    nc.variables[""y_rho""][:] = y_rho\n    nc.variables[""x_psi""][:] = x_psi\n    nc.variables[""y_psi""][:] = y_psi\n    nc.variables[""x_u""][:] = x_u\n    nc.variables[""y_u""][:] = y_u\n    nc.variables[""x_v""][:] = x_v\n    nc.variables[""y_v""][:] = y_v\n    nc.variables[""lat_rho""][:] = lat\n    nc.variables[""lon_rho""][:] = lon\n    nc.variables[""lat_psi""][:] = lat_psi\n    nc.variables[""lon_psi""][:] = lon_psi\n    nc.variables[""lat_u""][:] = lat_u\n    nc.variables[""lon_u""][:] = lon_u\n    nc.variables[""lat_v""][:] = lat_v\n    nc.variables[""lon_v""][:] = lon_v\n    nc.variables[""N""][:] = 1\n    nc.variables[""mask_rho""][:] = np.ones(lat.shape)\n    nc.variables[""mask_u""][:] = np.ones(lat_u.shape)\n    nc.variables[""mask_v""][:] = np.ones(lat_v.shape)\n    nc.variables[""mask_psi""][:] = np.ones(lat_psi.shape)\n    nc.variables[""angle""][:] = angle\n    nc.variables[""rdrag""][:] = np.ones(lon.shape) * 0.0003\n    nc.variables[""rdrag2""][:] = np.ones(lon.shape) * 0.003\n    nc.variables[""visc_factor""][:] = np.ones(lon.shape)\n    nc.variables[""diff_factor""][:] = np.ones(lon.shape)\n    nc.variables[""ZoBot""][:] = np.ones(lon.shape) * 0.02\n    nc.sync()\n\n    return nc\n\n\ndef calc_latlon(llcrnrlat, llcrnrlon, reseta, resxi=None, rotate=0):\n    """"""\n    Generate arrays for latitude and longitude for use in\n    creating simple grids.\n\n    NOTE: You can specify variational resolutions and rotations;\n    however, this uses a simple algorithm to step along from the\n    origin, and discrepencies are averaged together. THEREFORE,\n    if you are not specifying inconsistent resolutions or\n    rotations within single row or columns, you may get results\n    slightly different than specified.\n\n    Parameters\n    ----------\n    llcrnrlat : float,\n      Latitude for the lower, left of the grid\n    llcrnrlon : float,\n      Longitude for the lower, left of the grid\n    reseta : ndarray,\n      Resolution in meters in the eta-direction of each grid cell.\n      A 2D array that is the horizontal size of the grid (e.g.,\n      resolution.shape = (100,70)) and the values stored are\n      the desired resolution of the grid cell in m. Hence,\n      a 100 x 70 array of ones would create a grid that is\n      100 in the eta-direction, 70 in the xi-direction, and\n      a constant resolution of 1km. If the lat and lon is\n      specified as arrays, this is optional.\n    resxi : ndarray, optional,\n      Resolution in meters in the xi-direction of each grid cell.\n      This is the same as reseta; however, it is optional, and\n      is set to the same as reseta if not specified.\n    rotate : float or ndarray,\n      Amount to rotate the grid in degrees. If given as a scalar,\n      the entire grid is rotated at the same angle; otherwise, the\n      grid my have curvilinear shape. The angle is geometric\n      (counter-clockwise).\n\n    Returns\n    -------\n    lat, lon : ndarray\n       Two arrays of size resolution containing the computed lat\n       and lons\n\n    Examples\n    --------\n    Create a grid of 1km resolution in both eta and xi,\n    rotated toward the SE by 33 degrees, with the lower left\n    point at 20N, 150E:\n\n    >>> res = np.ones((100,70)) * 1000\n    >>> lat, lon = calc_latlon(20, 150, res, rotate=-33)\n    """"""\n\n    # Set up the resolutions\n    if resxi is None:\n        resxi = reseta\n    else:\n        if resxi.shape != reseta.shape:\n            raise AttributeError(\n                ""xi and eta resolutions must be same size array"")\n    # Since we are taking steps, average the values together as we\n    # can\'t take the final step\n    reseta[:-1, :] = 0.5 * (reseta[:-1, :] + reseta[1:, :])\n    resxi[:, :-1] = 0.5 * (resxi[:, :-1] + resxi[:, 1:])\n\n    # Set up the rotations\n    if np.isscalar(rotate):\n        rotate = np.ones(reseta.shape) * rotate\n    rotate = np.radians(rotate)\n\n    # Set up the lat and lon arrays\n    lat = np.ones(reseta.shape) * np.nan\n    lon = np.ones(reseta.shape) * np.nan\n    lat[0, 0] = llcrnrlat\n    lon[0, 0] = llcrnrlon\n    pi2 = np.pi / 2.0\n\n    # Loop over each row, and within the row, step along in the\n    # xi-direction before moving to the next row\n    for j in seapy.progressbar.progress(range(lat.shape[0] - 1)):\n        for i in range(lat.shape[1] - 1):\n            # Compute the local deltas\n            dlat = seapy.earth_distance(\n                lon[j, i], lat[j, i] - 0.5, lon[j, i], lat[j, i] + 0.5)\n            dlon = seapy.earth_distance(\n                lon[j, i] - 0.5, lat[j, i], lon[j, i] + 0.5, lat[j, i])\n\n            # Compute how far to step in xi\n            xdx = resxi[j, i + 1] * np.cos(rotate[j, i]) / dlon\n            xdy = resxi[j, i + 1] * np.sin(rotate[j, i]) / dlat\n\n            # Take the step\n            lat[j, i + 1] = lat[j, i] + xdy\n            lon[j, i + 1] = lon[j, i] + xdx\n\n            # Compute how far to step in eta\n            edx = reseta[j + 1, i] * np.cos(rotate[j, i] + pi2) / dlon\n            edy = reseta[j + 1, i] * np.sin(rotate[j, i] + pi2) / dlat\n\n            # Take the step\n            lat[j + 1, i] = lat[j, i] + edy\n            lon[j + 1, i] = lon[j, i] + edx\n\n    lon[-1, -1] = lon[-1, -2] + xdx\n    lat[-1, -1] = lat[-2, -1] + edy\n\n    return lat, lon\n'"
seapy/roms/forcing.py,13,"b'#!/usr/bin/env python\n""""""\n  forcing.py\n\n  Functions for generating ROMS forcing files (atmosphere, tides,\n  rivers, etc.)\n\n  Written by Brian Powell on 02/09/16\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\nimport numpy as np\nfrom datetime import datetime\nimport netCDF4\nimport seapy\nfrom collections import namedtuple\n\n# Define a named tuple to store raw data for the gridder\nforcing_data = namedtuple(\'forcing_data\', \'field ratio offset\')\nfields = (""Tair"", ""Qair"", ""Pair"", ""rain"",\n          ""Uwind"", ""Vwind"", ""lwrad_down"", ""swrad"")\n\ngfs_url = ""http://oos.soest.hawaii.edu/thredds/dodsC/hioos/model/atm/ncep_global/NCEP_Global_Atmospheric_Model_best.ncd""\n\ngfs_map = {\n    ""pad"": 1.0,\n    ""frc_lat"": ""latitude"",\n    ""frc_lon"": ""longitude"",\n    ""frc_time"": ""time"",\n    ""Tair"": forcing_data(""tmp2m"", 1, -273.15),\n    ""Pair"": forcing_data(""prmslmsl"", 0.01, 0),\n    ""Qair"": forcing_data(""rh2m"", 1, 0),\n    ""rain"": forcing_data(""pratesfc"", 1, 0),\n    ""Uwind"": forcing_data(""ugrd10m"", 1, 0),\n    ""Vwind"": forcing_data(""vgrd10m"", 1, 0),\n    ""lwrad_down"": forcing_data(""dlwrfsfc"", 1, 0),\n    ""swrad"": forcing_data(""dswrfsfc"", 1, 0)\n}\n\nncep_map = {\n    ""pad"": 2.0,\n    ""frc_lat"": ""lat"",\n    ""frc_lon"": ""lon"",\n    ""frc_time"": ""time"",\n    ""Tair"": forcing_data(""air"", 1, -273.15),\n    ""Pair"": forcing_data(""slp"", 0.01, 0),\n    ""Qair"": forcing_data(""rhum"", 1, 0),\n    ""rain"": forcing_data(""prate"", 1, 0),\n    ""Uwind"": forcing_data(""uwnd"", 1, 0),\n    ""Vwind"": forcing_data(""vwnd"", 1, 0),\n    ""lwrad_down"": forcing_data(""dlwrf"", 1, 0),\n    ""swrad"": forcing_data(""dswrf"", 1, 0)\n}\n\n\ndef gen_bulk_forcing(infile, fields, outfile, grid, start_time, end_time,\n                     epoch=seapy.default_epoch, clobber=False, cdl=None):\n    """"""\n    Given a source file (or URL), a dictionary that defines the\n    source fields mapped to the ROMS fields, then it will generate\n    a new bulk forcing file for ROMS.\n\n    Parameters\n    ----------\n    infile: string,\n      The source file (or URL) to load the data from\n    fields: dict,\n        A dictionary of the fields to load and process. The dictionary\n        is composed of:\n         ""frc_lat"":STRING name of latitude field in forcing file\n         ""frc_lon"":STRING name of longitude field in forcing file\n         ""frc_time"":STRING name of time field in forcing file\n         ""frc_time_units"":STRING optional, supply units of frc time\n                                field in forcing file\n         keys of ROMS bulk forcing field names (Tair, Pair, Qair,\n         rain, Uwind, Vwind, lwrad_down, swrad) each with an\n         array of values of a named tuple (forcing_data) with the\n         following fields:\n            field: STRING value of the forcing field to use\n            ratio: FLOAT value to multiply with the source data\n            offset: FLOAT value to add to the source data\n    outfile: string,\n        Name of the output file to create\n    grid: seapy.model.grid or string,\n        Grid to use for selecting spatial region\n    start_time: datetime,\n        Starting time of data to process\n    end_time: datetime,\n        Ending time of data to process\n    epoch: datetime,\n        Epoch to use for ROMS times\n    clobber: bool optional,\n        Delete existing file or not, default False\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n\n    Returns\n    -------\n    None: Generates an output file of bulk forcings\n\n    Examples\n    --------\n    To generate GFS forcing for the grid ""mygrid.nc"" for the year\n    2014, then use the standard GFS map definitions (and even\n    the built-in GFS archive url):\n\n    >>> seapy.roms.forcing.gen_bulk_forcing(seapy.roms.forcing.gfs_url,\n            seapy.roms.forcing.gfs_map, \'my_forcing.nc\',\n            \'mygrid.nc\', datetime.datetime(2014,1,1),\n            datetime.datetime(2014,1,1))\n\n    NCEP reanalysis is trickier as the files are broken up by\n    variable type; hence, a separate file will be created for\n    each variable. We can use the wildcards though to put together\n    multiple time period (e.g., 2014 through 2015).\n\n    >>> seapy.roms.forcing.gen_bulk_forcing(""uwnd.10m.*nc"",\n            seapy.roms.forcing.ncep_map, \'ncep_frc.nc\',\n            \'mygrid.nc\', datetime.datetime(2014,1,1),\n            datetime.datetime(2015,12,31)), clobber=True)\n    >>> seapy.roms.forcing.gen_bulk_forcing(""vwnd.10m.*nc"",\n            seapy.roms.forcing.ncep_map, \'ncep_frc.nc\',\n            \'mygrid.nc\', datetime.datetime(2014,1,1),\n            datetime.datetime(2015,12,31)), clobber=False)\n    >>> seapy.roms.forcing.gen_bulk_forcing(""air.2m.*nc"",\n            seapy.roms.forcing.ncep_map, \'ncep_frc.nc\',\n            \'mygrid.nc\', datetime.datetime(2014,1,1),\n            datetime.datetime(2015,12,31)), clobber=False)\n    >>> seapy.roms.forcing.gen_bulk_forcing(""dlwrf.sfc.*nc"",\n            seapy.roms.forcing.ncep_map, \'ncep_frc.nc\',\n            \'mygrid.nc\', datetime.datetime(2014,1,1),\n            datetime.datetime(2015,12,31)), clobber=False)\n    >>> seapy.roms.forcing.gen_bulk_forcing(""dswrf.sfc.*nc"",\n            seapy.roms.forcing.ncep_map, \'ncep_frc.nc\',\n            \'mygrid.nc\', datetime.datetime(2014,1,1),\n            datetime.datetime(2015,12,31)), clobber=False)\n    >>> seapy.roms.forcing.gen_bulk_forcing(""prate.sfc.*nc"",\n            seapy.roms.forcing.ncep_map, \'ncep_frc.nc\',\n            \'mygrid.nc\', datetime.datetime(2014,1,1),\n            datetime.datetime(2015,12,31)), clobber=False)\n    >>> seapy.roms.forcing.gen_bulk_forcing(""rhum.sig995.*nc"",\n            seapy.roms.forcing.ncep_map, \'ncep_frc_rhum_slp.nc\',\n            \'mygrid.nc\', datetime.datetime(2014,1,1),\n            datetime.datetime(2015,12,31)), clobber=True)\n    >>> seapy.roms.forcing.gen_bulk_forcing(""slp.*nc"",\n            seapy.roms.forcing.ncep_map, \'ncep_frc_rhum_slp.nc\',\n            \'mygrid.nc\', datetime.datetime(2014,1,1),\n            datetime.datetime(2015,12,31)), clobber=False)\n\n    Two forcing files, \'ncep_frc.nc\' and \'ncep_frc_rhum_slp.nc\', are\n    generated for use with ROMS. NOTE: You will have to use \'ncks\'\n    to eliminate the empty forcing fields between the two files\n    to prevent ROMS from loading them.\n    """"""\n    # Load the grid\n    grid = seapy.model.asgrid(grid)\n\n    # Open the Forcing data\n    forcing = seapy.netcdf(infile)\n\n    # Gather the information about the forcing\n    if \'frc_time_units\' in fields:\n        frc_time = netCDF4.num2date(forcing.variables[fields[\'frc_time\']][:],\n                                    fields[\'frc_time_units\'])\n    else:\n        frc_time = seapy.roms.num2date(forcing, fields[\'frc_time\'])\n\n    # Figure out the time records that are required\n    time_list = np.where(np.logical_and(frc_time >= start_time,\n                                        frc_time <= end_time))[0]\n    if not np.any(time_list):\n        raise Exception(""Cannot find valid times"")\n\n    # Get the latitude and longitude ranges\n    minlat = np.floor(np.min(grid.lat_rho)) - fields[\'pad\']\n    maxlat = np.ceil(np.max(grid.lat_rho)) + fields[\'pad\']\n    minlon = np.floor(np.min(grid.lon_rho)) - fields[\'pad\']\n    maxlon = np.ceil(np.max(grid.lon_rho)) + fields[\'pad\']\n    frc_lon = forcing.variables[fields[\'frc_lon\']][:]\n    frc_lat = forcing.variables[fields[\'frc_lat\']][:]\n    # Make the forcing lat/lon on 2D grid\n    if frc_lon.ndim == 3:\n        frc_lon = np.squeeze(frc_lon[0, :, :])\n        frc_lat = np.squeeze(frc_lat[0, :, :])\n    elif frc_lon.ndim == 1:\n        frc_lon, frc_lat = np.meshgrid(frc_lon, frc_lat)\n\n    # Find the values in our region\n    if not grid.east():\n        frc_lon[frc_lon > 180] -= 360\n    region_list = np.where(np.logical_and.reduce((\n        frc_lon <= maxlon,\n        frc_lon >= minlon,\n        frc_lat <= maxlat,\n        frc_lat >= minlat)))\n    if not np.any(region_list):\n        raise Exception(""Cannot find valid region"")\n\n    eta_list = np.s_[np.min(region_list[0]):np.max(region_list[0]) + 1]\n    xi_list = np.s_[np.min(region_list[1]):np.max(region_list[1]) + 1]\n    frc_lon = frc_lon[eta_list, xi_list]\n    frc_lat = frc_lat[eta_list, xi_list]\n\n    # Create the output file\n    out = seapy.roms.ncgen.create_frc_bulk(outfile, lat=frc_lat.shape[0],\n                                           lon=frc_lon.shape[1], reftime=epoch,\n                                           clobber=clobber, cdl=cdl)\n    out.variables[\'frc_time\'][:] = seapy.roms.date2num(\n        frc_time[time_list], out, \'frc_time\')\n    out.variables[\'lat\'][:] = frc_lat\n    out.variables[\'lon\'][:] = frc_lon\n\n    # Loop over the fields and fill out the output file\n    for f in seapy.progressbar.progress(list(set(fields.keys()) & (out.variables.keys()))):\n        if hasattr(fields[f], \'field\') and fields[f].field in forcing.variables:\n            out.variables[f][:] = \\\n                forcing.variables[fields[f].field][time_list, eta_list, xi_list] * \\\n                fields[f].ratio + fields[f].offset\n            out.sync()\n\n    out.close()\n\n\ndef gen_direct_forcing(his_file, frc_file, cdl=None):\n    """"""\n    Generate a direct forcing file from a history (or other ROMS output) file. It requires\n    that sustr, svstr, shflux, and ssflux (or swflux) with salt be available. This will\n    generate a forcing file that contains: sustr, svstr, swflux, and ssflux.\n\n    Parameters\n    ----------\n    his_file: string,\n      The ROMS history (or other) file(s) (can use wildcards) that contains the fields to\n      make forcing from\n    frc_file: string,\n      The output forcing file\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n\n    Returns\n    -------\n    None: Generates an output file of bulk forcings\n    """"""\n    import os\n\n    infile = seapy.netcdf(his_file)\n    ref, _ = seapy.roms.get_reftime(infile)\n\n    # Create the output file\n    nc = seapy.roms.ncgen.create_frc_direct(frc_file,\n                                            eta_rho=infile.dimensions[\n                                                \'eta_rho\'].size,\n                                            xi_rho=infile.dimensions[\n                                                \'xi_rho\'].size,\n                                            reftime=ref,\n                                            clobber=True,\n                                            title=""Forcing from "" +\n                                            os.path.basename(his_file),\n                                            cdl=cdl)\n\n    # Copy the data over\n    time = seapy.roms.num2date(infile, \'ocean_time\')\n    nc.variables[\'frc_time\'][:] = seapy.roms.date2num(time, nc, \'frc_time\')\n    for x in seapy.progressbar.progress(seapy.chunker(range(len(time)), 1000)):\n        nc.variables[\'SSS\'][x, :, :] = seapy.convolve_mask(\n            infile.variables[\'salt\'][x, -1, :, :], copy=False)\n        if \'EminusP\' in infile.variables:\n            nc.variables[\'swflux\'][x, :, :] = seapy.convolve_mask(\n                infile.variables[\'EminusP\'][x, :, :], copy=False) * 86400\n        elif \'swflux\' in infile.variables:\n            nc.variables[\'swflux\'][x, :, :] = seapy.convolve_mask(\n                infile.variables[\'swflux\'][x, :, :], copy=False)\n        else:\n            nc.variables[\'swflux\'][x, :, :] = seapy.convolve_mask(\n                infile.variables[\'ssflux\'][x, :, :]\n                / nc.variables[\'SSS\'][x, :, :], copy=False)\n\n        nc.sync()\n        for f in (""sustr"", ""svstr"", ""shflux"", ""swrad""):\n            if f in infile.variables:\n                nc.variables[f][x, :, :] = seapy.convolve_mask(\n                    infile.variables[f][x, :, :], copy=False)\n                nc.sync()\n\n    for f in (""lat_rho"", ""lat_u"", ""lat_v"", ""lon_rho"", ""lon_u"", ""lon_v""):\n        if f in infile.variables:\n            nc.variables[f][:] = infile.variables[f][:]\n    nc.close()\n'"
seapy/roms/initial.py,0,"b'#!/usr/bin/env python\n""""""\n  initial.py\n\n  ROMS initial conditions utilities\n\n  Written by Brian Powell on 01/15/14\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\n\n\nimport seapy\nimport numpy as np\nimport netCDF4\n\n\ndef from_roms(roms_file, ini_file, record=0, time=None, grid=None,\n              clobber=False, cdl=None):\n    """"""\n    Given a ROMS history, average, or climatology file, generate\n    initial conditions on the same grid.\n\n    Parameters\n    ----------\n    roms_file: string or list\n        Input ROMS source (history, average, climatology file)\n    ini_file: string\n        Input name for output initial condition file\n    record: int\n        Input index to use as initial condition\n    time: datetime optional\n        Input datetime to use for the initial condition (default to record time)\n    grid: seapy.model.grid or string, optional\n        Input ROMS grid: specify the grid if loaded or filename to load\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n\n    Returns\n    -------\n    None\n\n    """"""\n    # Load the grid\n    if grid is None:\n        grid = seapy.model.asgrid(roms_file)\n    else:\n        grid = seapy.model.asgrid(grid)\n    ncroms = seapy.netcdf(roms_file)\n    src_ref, romstime = seapy.roms.get_reftime(ncroms)\n\n    # Create the initial file and fill up the descriptive data\n    ncini = seapy.roms.ncgen.create_ini(ini_file,\n                                        eta_rho=grid.eta_rho, xi_rho=grid.xi_rho, s_rho=grid.n,\n                                        reftime=src_ref, clobber=clobber, cdl=cdl,\n                                        title=""generated from "" + roms_file)\n    grid.to_netcdf(ncini)\n    if time is None:\n        time = seapy.roms.num2date(ncroms, romstime, record)\n    ncini.variables[""ocean_time""][:] = seapy.roms.date2num(\n        time, ncini, ""ocean_time"")\n\n    # Fill up the initial state with the roms file data\n    for var in seapy.roms.fields:\n        if var in ncini.variables and var in ncroms.variables:\n            ncini.variables[var][0, :] = ncroms.variables[var][record, :]\n\n    # Close up\n    ncini.close()\n    ncroms.close()\n    pass\n'"
seapy/roms/interp.py,68,"b'#!/usr/bin/env python\n""""""\n  roms.interp\n\n  Methods to interpolate ROMS fields onto other grids\n\n  Written by Brian Powell on 11/02/13\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\n\n\nimport numpy as np\nimport netCDF4\nimport os\nimport seapy\nfrom seapy.timeout import timeout, TimeoutError\nfrom joblib import Parallel, delayed\nfrom warnings import warn\n\n_up_scaling = {""zeta"": 1.0, ""u"": 1.0, ""v"": 1.0, ""temp"": 1.0, ""salt"": 1.0}\n_down_scaling = {""zeta"": 1.0, ""u"": 0.999,\n                 ""v"": 0.999, ""temp"": 0.999, ""salt"": 1.001}\n_ksize_range = (7, 15)\n# Limit amount of memory in bytes to process in a single read. This determines how to\n# divide up the time-records in interpolation\n_max_memory = 768 * 1024 * 1024   # 768 MBytes\n\n\ndef __mask_z_grid(z_data, src_depth, z_depth):\n    """"""\n    When interpolating to z-grid, we need to apply depth dependent masking\n    based on the original ROMS depths\n    """"""\n    for k in np.arange(0, z_depth.shape[0]):\n        idx = np.nonzero(z_depth[k, :, :] < src_depth)\n        if z_data.ndim == 4:\n            z_data.mask[:, k, idx[0], idx[1]] = True\n        elif z_data.ndim == 3:\n            z_data.mask[k, idx[0], idx[1]] = True\n\n\ndef __interp2_thread(rx, ry, data, zx, zy, pmap, weight, nx, ny, mask):\n    """"""\n    internal routine: 2D interpolation thread for parallel interpolation\n    """"""\n    data = np.ma.fix_invalid(data, copy=False)\n\n    # Convolve the water over the land\n    ksize = 2 * np.round(np.sqrt((nx / np.median(np.diff(rx)))**2 +\n                                 (ny / np.median(np.diff(ry.T)))**2)) + 1\n    if ksize < _ksize_range[0]:\n        warn(""nx or ny values are too small for stable OA, {:f}"".format(ksize))\n        ksize = _ksize_range[0]\n    elif ksize > _ksize_range[1]:\n        warn(""nx or ny values are too large for stable OA, {:f}"".format(ksize))\n        ksize = _ksize_range[1]\n    data = seapy.convolve_mask(data, ksize=ksize, copy=False)\n\n    # Interpolate the field and return the result\n    with timeout(minutes=30):\n        res, pm = seapy.oasurf(rx, ry, data, zx, zy, pmap, weight, nx, ny)\n\n    return np.ma.masked_where(np.logical_or(mask == 0, np.abs(res) > 9e4), res,\n                              copy=False)\n\n\ndef __interp3_thread(rx, ry, rz, data, zx, zy, zz, pmap,\n                     weight, nx, ny, mask, up_factor=1.0, down_factor=1.0):\n    """"""\n    internal routine: 3D interpolation thread for parallel interpolation\n    """"""\n    # Make the mask 3D\n    mask = seapy.adddim(mask, zz.shape[0])\n    data = np.ma.fix_invalid(data, copy=False)\n\n    # To avoid extrapolation, we are going to convolve ocean over the land\n    # and add a new top and bottom layer that replicates the data of the\n    # existing current and top. 1) iteratively convolve until we have\n    # filled most of the points, 2) Determine which way the\n    # depth goes and add/subtract new layers, and 3) fill in masked values\n    # from the layer above/below.\n    gradsrc = (rz[0, 1, 1] - rz[-1, 1, 1]) > 0\n\n    # Convolve the water over the land\n    ksize = 2 * np.round(np.sqrt((nx / np.median(np.diff(rx)))**2 +\n                                 (ny / np.median(np.diff(ry.T)))**2)) + 1\n    if ksize < _ksize_range[0]:\n        warn(""nx or ny values are too small for stable OA, {:f}"".format(ksize))\n        ksize = _ksize_range[0]\n    elif ksize > _ksize_range[1]:\n        warn(""nx or ny values are too large for stable OA, {:f}"".format(ksize))\n        ksize = _ksize_range[1]\n\n    # Iterate at most 5 times, but we will hopefully break out before that by\n    # checking if we have filled at least 40% of the bottom to be like\n    # the surface\n    bot = -1 if gradsrc else 0\n    top = 0 if gradsrc else -1\n    topmask = np.maximum(1, np.ma.count_masked(data[top, :, :]))\n    if np.ma.count_masked(data[bot, :, :]) > 0:\n        for iter in range(5):\n            # Check if we have most everything by checking the bottom\n            data = seapy.convolve_mask(data, ksize=ksize + iter, copy=False)\n            if topmask / np.maximum(1, np.ma.count_masked(data[bot, :, :])) > 0.4:\n                break\n\n    # Now fill vertically\n    nrz = np.zeros((data.shape[0] + 2, data.shape[1], data.shape[2]))\n    nrz[1:-1, :, :] = rz\n    nrz[bot, :, :] = rz[bot, :, :] - 5000\n    nrz[top, :, :] = 1\n\n    if not gradsrc:\n        # The first level is the bottom\n        # factor = down_factor\n        levs = np.arange(data.shape[0], 0, -1) - 1\n    else:\n        # The first level is the top\n        # factor = up_factor\n        levs = np.arange(0, data.shape[0])\n\n    # Fill in missing values where we have them from the shallower layer\n    for k in levs[1:]:\n        if np.ma.count_masked(data[k, :, :]) == 0:\n            continue\n        idx = np.nonzero(np.logical_xor(data.mask[k, :, :],\n                                        data.mask[k - 1, :, :]))\n        data.mask[k, idx[0], idx[1]] = data.mask[k - 1, idx[0], idx[1]]\n        data[k, idx[0], idx[1]] = data[k - 1, idx[0], idx[1]] * down_factor\n\n    # Add upper and lower boundaries\n    ndat = np.zeros((data.shape[0] + 2, data.shape[1], data.shape[2]))\n    ndat[bot, :, :] = data[bot, :, :].filled(np.nan) * down_factor\n    ndat[1:-1, :, :] = data.filled(np.nan)\n    ndat[top, :, :] = data[top, :, :].filled(np.nan) * up_factor\n\n    # Interpolate the field and return the result\n    with timeout(minutes=30):\n        if gradsrc:\n            res, pm = seapy.oavol(rx, ry, nrz[::-1, :, :], ndat[::-1, :, :],\n                                  zx, zy, zz, pmap, weight, nx, ny)\n        else:\n            res, pm = seapy.oavol(rx, ry, nrz, ndat, zx, zy, zz,\n                                  pmap, weight, nx, ny)\n\n    return np.ma.masked_where(np.logical_or(mask == 0, np.abs(res) > 9e4), res,\n                              copy=False)\n\n\ndef __interp3_vel_thread(rx, ry, rz, ra, u, v, zx, zy, zz, za, pmap,\n                         weight, nx, ny, mask):\n    """"""\n    internal routine: 3D velocity interpolation thread for parallel interpolation\n    """"""\n    # Put on the same grid\n    if u.shape != v.shape:\n        u = seapy.model.u2rho(u, fill=True)\n        v = seapy.model.v2rho(v, fill=True)\n\n    # Rotate the fields (NOTE: ROMS angle is negative relative to ""true"")\n    if ra is not None:\n        u, v = seapy.rotate(u, v, ra)\n\n    # Interpolate\n    u = __interp3_thread(rx, ry, rz, u, zx, zy, zz, pmap,\n                         weight, nx, ny, mask, _up_scaling[""u""],\n                         _down_scaling[""u""])\n    v = __interp3_thread(rx, ry, rz, v, zx, zy, zz, pmap,\n                         weight, nx, ny, mask, _up_scaling[""v""],\n                         _down_scaling[""v""])\n\n    # Rotate to destination (NOTE: ROMS angle is negative relative to ""true"")\n    if za is not None:\n        u, v = seapy.rotate(u, v, -za)\n\n    # Return the masked data\n    return u, v\n\n\ndef __interp_grids(src_grid, child_grid, ncsrc, ncout, records=None,\n                   threads=2, nx=0, ny=0, weight=10, vmap=None, z_mask=False,\n                   pmap=None):\n    """"""\n    internal method:  Given a model file (average, history, etc.),\n    interpolate the fields onto another gridded file.\n\n    Parameters\n    ----------\n    src_grid : seapy.model.grid data of source\n    child_grid : seapy.model.grid output data grid\n    ncsrc : netcdf input file  (History, Average, etc. file)\n    ncout : netcdf output file\n    [records] : array of the record indices to interpolate\n    [threads] : number of processing threads\n    [nx] : decorrelation length in grid-cells for x\n    [ny] : decorrelation length in grid-cells for y\n    [vmap] : variable name mapping\n    [z_mask] : mask out depths in z-grids\n    [pmap] : use the specified pmap rather than compute it\n\n    Returns\n    -------\n    None\n\n    """"""\n    # If we don\'t have a variable map, then do a one-to-one mapping\n    if vmap is None:\n        vmap = dict()\n        for k in seapy.roms.fields:\n            vmap[k] = k\n\n    # Generate a file to store the pmap information\n    sname = getattr(src_grid, \'name\', None)\n    cname = getattr(child_grid, \'name\', None)\n    pmap_file = None if any(v is None for v in (sname, cname)) else \\\n        sname + ""_"" + cname + ""_pmap.npz""\n\n    # Create or load the pmaps depending on if they exist\n    if nx == 0:\n        if hasattr(src_grid, ""dm"") and hasattr(child_grid, ""dm""):\n            nx = np.ceil(np.mean(src_grid.dm) / np.mean(child_grid.dm))\n        else:\n            nx = 5\n    if ny == 0:\n        if hasattr(src_grid, ""dn"") and hasattr(child_grid, ""dn""):\n            ny = np.ceil(np.mean(src_grid.dn) / np.mean(child_grid.dn))\n        else:\n            ny = 5\n\n    if pmap is None:\n        if pmap_file is not None and os.path.isfile(pmap_file):\n            pmap = np.load(pmap_file)\n        else:\n            tmp = np.ma.masked_equal(src_grid.mask_rho, 0)\n            tmp, pmaprho = seapy.oasurf(src_grid.lon_rho, src_grid.lat_rho,\n                                        tmp, child_grid.lon_rho, child_grid.lat_rho,\n                                        weight=weight, nx=nx, ny=ny)\n            tmp = np.ma.masked_equal(src_grid.mask_u, 0)\n            tmp, pmapu = seapy.oasurf(src_grid.lon_u, src_grid.lat_u,\n                                      tmp, child_grid.lon_rho, child_grid.lat_rho,\n                                      weight=weight, nx=nx, ny=ny)\n            tmp = np.ma.masked_equal(src_grid.mask_v, 0)\n            tmp, pmapv = seapy.oasurf(src_grid.lon_v, src_grid.lat_v,\n                                      tmp, child_grid.lon_rho, child_grid.lat_rho,\n                                      weight=weight, nx=nx, ny=ny)\n            if pmap_file is not None:\n                np.savez(pmap_file, pmaprho=pmaprho, pmapu=pmapu, pmapv=pmapv)\n            pmap = {""pmaprho"": pmaprho, ""pmapu"": pmapu, ""pmapv"": pmapv}\n\n    # Get the time field\n    time = seapy.roms.get_timevar(ncsrc)\n\n    # Interpolate the depths from the source to final grid\n    src_depth = np.min(src_grid.depth_rho, 0)\n    dst_depth = __interp2_thread(src_grid.lon_rho, src_grid.lat_rho, src_depth,\n                                 child_grid.lon_rho, child_grid.lat_rho, pmap[\n                                     ""pmaprho""],\n                                 weight, nx, ny, child_grid.mask_rho)\n    # Interpolate the scalar fields\n    records = np.arange(0, ncsrc.variables[time].shape[0]) \\\n        if records is None else np.atleast_1d(records)\n    for src in vmap:\n        dest = vmap[src]\n\n        # Extra fields will probably be user tracers (biogeochemical)\n        fld = seapy.roms.fields.get(dest, {""dims"": 3})\n\n        # Only interpolate the fields we want in the destination\n        if (dest not in ncout.variables) or \\\n           (src not in ncsrc.variables) or \\\n           (""rotate"" in fld):\n            continue\n\n        if fld[""dims""] == 2:\n            # Compute the max number of hold in memory\n            maxrecs = np.maximum(1, np.minimum(len(records),\n                                               np.int(_max_memory / (child_grid.lon_rho.nbytes +\n                                                                     src_grid.lon_rho.nbytes))))\n            for rn, recs in enumerate(seapy.chunker(records, maxrecs)):\n                outr = np.s_[\n                    rn * maxrecs:np.minimum((rn + 1) * maxrecs, len(records))]\n                ndata = np.ma.array(Parallel(n_jobs=threads, verbose=2, max_nbytes=_max_memory)\n                                    (delayed(__interp2_thread)(\n                                     src_grid.lon_rho, src_grid.lat_rho,\n                                     ncsrc.variables[src][i, :, :],\n                                     child_grid.lon_rho, child_grid.lat_rho,\n                                     pmap[""pmaprho""], weight,\n                                     nx, ny, child_grid.mask_rho)\n                                     for i in recs), copy=False)\n                ncout.variables[dest][outr, :, :] = ndata\n                ncout.sync()\n        else:\n            maxrecs = np.maximum(1, np.minimum(\n                len(records), np.int(_max_memory /\n                                     (child_grid.lon_rho.nbytes *\n                                      child_grid.n +\n                                      src_grid.lon_rho.nbytes *\n                                      src_grid.n))))\n            for rn, recs in enumerate(seapy.chunker(records, maxrecs)):\n                outr = np.s_[\n                    rn * maxrecs:np.minimum((rn + 1) * maxrecs, len(records))]\n                ndata = np.ma.array(Parallel(n_jobs=threads, verbose=2, max_nbytes=_max_memory)\n                                    (delayed(__interp3_thread)(\n                                        src_grid.lon_rho, src_grid.lat_rho,\n                                        src_grid.depth_rho,\n                                        ncsrc.variables[src][i, :, :, :],\n                                        child_grid.lon_rho, child_grid.lat_rho,\n                                        child_grid.depth_rho,\n                                        pmap[""pmaprho""], weight,\n                                        nx, ny, child_grid.mask_rho,\n                                        up_factor=_up_scaling.get(dest, 1.0),\n                                        down_factor=_down_scaling.get(dest, 1.0))\n                                     for i in recs), copy=False)\n                if z_mask:\n                    __mask_z_grid(ndata, dst_depth, child_grid.depth_rho)\n                ncout.variables[dest][outr, :, :, :] = ndata\n                ncout.sync()\n\n    # Rotate and Interpolate the vector fields. First, determine which\n    # are the ""u"" and the ""v"" vmap fields\n    try:\n        velmap = {\n            ""u"": list(vmap.keys())[list(vmap.values()).index(""u"")],\n            ""v"": list(vmap.keys())[list(vmap.values()).index(""v"")]}\n    except:\n        warn(""velocity not present in source file"")\n        return\n\n    srcangle = getattr(src_grid, \'angle\', None)\n    dstangle = getattr(child_grid, \'angle\', None)\n    maxrecs = np.minimum(len(records),\n                         np.int(_max_memory /\n                                (2 * (child_grid.lon_rho.nbytes *\n                                      child_grid.n +\n                                      src_grid.lon_rho.nbytes *\n                                      src_grid.n))))\n    for nr, recs in enumerate(seapy.chunker(records, maxrecs)):\n        vel = Parallel(n_jobs=threads, verbose=2, max_nbytes=_max_memory)(delayed(__interp3_vel_thread)(\n            src_grid.lon_rho, src_grid.lat_rho,\n            src_grid.depth_rho, srcangle,\n            ncsrc.variables[velmap[""u""]][i, :, :, :],\n            ncsrc.variables[velmap[""v""]][i, :, :, :],\n            child_grid.lon_rho, child_grid.lat_rho,\n            child_grid.depth_rho, dstangle,\n            pmap[""pmaprho""], weight, nx, ny,\n            child_grid.mask_rho) for i in recs)\n\n        for j in range(len(vel)):\n            vel_u = np.ma.array(vel[j][0], copy=False)\n            vel_v = np.ma.array(vel[j][1], copy=False)\n            if z_mask:\n                __mask_z_grid(vel_u, dst_depth, child_grid.depth_rho)\n                __mask_z_grid(vel_v, dst_depth, child_grid.depth_rho)\n\n            if child_grid.cgrid:\n                vel_u = seapy.model.rho2u(vel_u)\n                vel_v = seapy.model.rho2v(vel_v)\n\n            ncout.variables[""u""][nr * maxrecs + j, :] = vel_u\n            ncout.variables[""v""][nr * maxrecs + j, :] = vel_v\n\n            if ""ubar"" in ncout.variables:\n                # Create ubar and vbar\n                # depth = seapy.adddim(child_grid.depth_u, vel_u.shape[0])\n                ncout.variables[""ubar""][nr * maxrecs + j, :] = \\\n                    np.sum(vel_u * child_grid.depth_u, axis=0) /  \\\n                    np.sum(child_grid.depth_u, axis=0)\n\n            if ""vbar"" in ncout.variables:\n                # depth = seapy.adddim(child_grid.depth_v, vel_v.shape[0])\n                ncout.variables[""vbar""][nr * maxrecs + j, :] = \\\n                    np.sum(vel_v * child_grid.depth_v, axis=0) /  \\\n                    np.sum(child_grid.depth_v, axis=0)\n\n            ncout.sync()\n\n    # Return the pmap that was used\n    return pmap\n\n\ndef field2d(src_lon, src_lat, src_field, dest_lon, dest_lat, dest_mask=None,\n            nx=0, ny=0, weight=10, threads=2, pmap=None):\n    """"""\n    Given a 2D field with time (dimensions [time, lat, lon]), interpolate\n    onto a new grid and return the new field. This is a helper function\n    when needing to interpolate data within files, etc.\n\n    Parameters\n    ----------\n    src_lon: numpy.ndarray\n        longitude that field is on\n    src_lat: numpy.ndarray\n        latitude that field is on\n    src_field: numpy.ndarray\n        field to interpolate\n    dest_lon: numpy.ndarray\n        output longitudes to interpolate to\n    dest_lat: numpy.ndarray\n        output latitudes to interpolate to\n    dest_mask: numpy.ndarray, optional\n        mask to apply to interpolated data\n    reftime: datetime, optional:\n        Reference time as the epoch for z-grid file\n    nx : float, optional:\n        decorrelation length-scale for OA (same units as source data)\n    ny : float, optional:\n        decorrelation length-scale for OA (same units as source data)\n    weight : int, optional:\n        number of points to use in weighting matrix\n    threads : int, optional:\n        number of processing threads\n    pmap : numpy.ndarray, optional:\n        use the specified pmap rather than compute it\n\n    Output\n    ------\n    ndarray:\n        interpolated field on the destination grid\n    pmap:\n        the pmap used in the inerpolation\n    """"""\n    if pmap is None:\n        tmp, pmap = seapy.oasurf(src_lon, src_lat, src_lat,\n                                 dest_lon, dest_lat, weight=weight, nx=nx, ny=ny)\n    if dest_mask is None:\n        dest_mask = np.ones(dest_lat.shape)\n    records = np.arange(0, src_field.shape[0])\n    maxrecs = np.maximum(1,\n                         np.minimum(records.size,\n                                    np.int(_max_memory /\n                                           (dest_lon.nbytes + src_lon.nbytes))))\n    for rn, recs in enumerate(seapy.chunker(records, maxrecs)):\n        nfield = np.ma.array(Parallel(n_jobs=threads, verbose=2)\n                             (delayed(__interp2_thread)(\n                                 src_lon, src_lat, src_field[i, :, :],\n                                 dest_lon, dest_lat,\n                                 pmap, weight,\n                                 nx, ny, dest_mask)\n                              for i in recs), copy=False)\n    return nfield, pmap\n\n\ndef field3d(src_lon, src_lat, src_depth, src_field, dest_lon, dest_lat,\n            dest_depth, dest_mask=None, nx=0, ny=0, weight=10,\n            threads=2, pmap=None):\n    """"""\n    Given a 3D field with time (dimensions [time, z, lat, lon]), interpolate\n    onto a new grid and return the new field. This is a helper function\n    when needing to interpolate data within files, etc.\n\n    Parameters\n    ----------\n    src_lon: numpy.ndarray\n        longitude that field is on\n    src_lat: numpy.ndarray\n        latitude that field is on\n    srf_depth: numpy.ndarray\n        depths of the field\n    src_field: numpy.ndarray\n        field to interpolate\n    dest_lon: numpy.ndarray\n        output longitudes to interpolate to\n    dest_lat: numpy.ndarray\n        output latitudes to interpolate to\n    dest_depth: numpy.ndarray\n        output depths to interpolate to\n    dest_mask: numpy.ndarray, optional\n        mask to apply to interpolated data\n    reftime: datetime, optional:\n        Reference time as the epoch for z-grid file\n    nx : float, optional:\n        decorrelation length-scale for OA (same units as source data)\n    ny : float, optional:\n        decorrelation length-scale for OA (same units as source data)\n    weight : int, optional:\n        number of points to use in weighting matrix\n    threads : int, optional:\n        number of processing threads\n    pmap : numpy.ndarray, optional:\n        use the specified pmap rather than compute it\n\n    Output\n    ------\n    ndarray:\n        interpolated field on the destination grid\n    pmap:\n        the pmap used in the interpolation\n    """"""\n    if pmap is None:\n        tmp, pmap = seapy.oasurf(src_lon, src_lat, src_lat,\n                                 dest_lon, dest_lat, weight=weight, nx=nx, ny=ny)\n    if dest_mask is None:\n        dest_mask = np.ones(dest_lat.shape)\n    records = np.arange(0, src_field.shape[0])\n    maxrecs = np.maximum(1,\n                         np.minimum(records.size,\n                                    np.int(_max_memory /\n                                           (dest_lon.nbytes *\n                                               dest_depth.shape[0] +\n                                               src_lon.nbytes *\n                                               src_depth.shape[0]))))\n    for rn, recs in enumerate(seapy.chunker(records, maxrecs)):\n        nfield = np.ma.array(Parallel(n_jobs=threads, verbose=2)\n                             (delayed(__interp3_thread)(\n                                 src_lon, src_lat, src_depth,\n                                 src_field[i, :, :],\n                                 dest_lon, dest_lat, dest_depth,\n                                 pmap, weight, nx, ny, dest_mask,\n                                 up_factor=1, down_factor=1)\n                              for i in recs), copy=False)\n\n    return nfield, pmap\n\n\ndef to_zgrid(roms_file, z_file, src_grid=None, z_grid=None, depth=None,\n             records=None, threads=2, reftime=None, nx=0, ny=0, weight=10,\n             vmap=None, cdl=None, dims=2, pmap=None):\n    """"""\n    Given an existing ROMS history or average file, create (if does not exit)\n    a new z-grid file. Use the given z_grid or otherwise build one with the\n    same horizontal extent and the specified depths and interpolate the\n    ROMS fields onto the z-grid.\n\n    Parameters\n    ----------\n    roms_file  : string,\n        File name of src file to interpolate from\n    z_file : string,\n        Name of desination file to write to\n    src_grid : (string or seapy.model.grid), optional:\n        Name or instance of source grid. If nothing is specified,\n        derives grid from the roms_file\n    z_grid: (string or seapy.model.grid), optional:\n        Name or instance of output definition\n    depth: numpy.ndarray, optional:\n        array of depths to use for z-level\n    records : numpy.ndarray, optional:\n        Record indices to interpolate\n    threads : int, optional:\n        number of processing threads\n    reftime: datetime, optional:\n        Reference time as the epoch for z-grid file\n    nx : float, optional:\n        decorrelation length-scale for OA (same units as source data)\n    ny : float, optional:\n        decorrelation length-scale for OA (same units as source data)\n    weight : int, optional:\n        number of points to use in weighting matrix\n    vmap : dictionary, optional\n        mapping source and destination variables\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    dims : int, optional\n        number of dimensions to use for lat/lon arrays (default 2)\n    pmap : numpy.ndarray, optional:\n        use the specified pmap rather than compute it\n\n    Returns\n    -------\n    pmap : ndarray\n        the weighting matrix computed during the interpolation\n\n    """"""\n    if src_grid is None:\n        src_grid = seapy.model.asgrid(roms_file)\n    else:\n        src_grid = seapy.model.asgrid(src_grid)\n    ncsrc = seapy.netcdf(roms_file)\n    src_ref, time = seapy.roms.get_reftime(ncsrc)\n    if reftime is not None:\n        src_ref = reftime\n    records = np.arange(0, ncsrc.variables[time].shape[0]) \\\n        if records is None else np.atleast_1d(records)\n\n    # Load the grid\n    if z_grid is not None:\n        z_grid = seapy.model.asgrid(z_grid)\n    elif os.path.isfile(z_file):\n        z_grid = seapy.model.asgrid(z_file)\n\n    if not os.path.isfile(z_file):\n        if z_grid is None:\n            lat = src_grid.lat_rho.shape[0]\n            lon = src_grid.lat_rho.shape[1]\n            if depth is None:\n                raise ValueError(""depth must be specified"")\n            ncout = seapy.roms.ncgen.create_zlevel(z_file, lat, lon,\n                                                   len(depth), src_ref, ""ROMS z-level"",\n                                                   cdl=cdl, dims=dims)\n            if dims == 1:\n                ncout.variables[""lat""][:] = src_grid.lat_rho[:, 0]\n                ncout.variables[""lon""][:] = src_grid.lon_rho[0, :]\n            else:\n                ncout.variables[""lat""][:] = src_grid.lat_rho\n                ncout.variables[""lon""][:] = src_grid.lon_rho\n            ncout.variables[""depth""][:] = depth\n            ncout.variables[""mask""][:] = src_grid.mask_rho\n            ncout.sync()\n            z_grid = seapy.model.grid(z_file)\n        else:\n            lat = z_grid.lat_rho.shape[0]\n            lon = z_grid.lat_rho.shape[1]\n            dims = z_grid.spatial_dims\n            ncout = seapy.roms.ncgen.create_zlevel(z_file, lat, lon,\n                                                   len(z_grid.z), src_ref, ""ROMS z-level"",\n                                                   cdl=cdl, dims=dims)\n            if dims == 1:\n                ncout.variables[""lat""][:] = z_grid.lat_rho[:, 0]\n                ncout.variables[""lon""][:] = z_grid.lon_rho[0, :]\n            else:\n                ncout.variables[""lat""][:] = z_grid.lat_rho\n                ncout.variables[""lon""][:] = z_grid.lon_rho\n            ncout.variables[""depth""][:] = z_grid.z\n            ncout.variables[""mask""][:] = z_grid.mask_rho\n    else:\n        ncout = netCDF4.Dataset(z_file, ""a"")\n\n    ncout.variables[""time""][:] = seapy.roms.date2num(\n        seapy.roms.num2date(ncsrc, time, records), ncout, ""time"")\n\n    # Call the interpolation\n    try:\n        src_grid.set_east(z_grid.east())\n        pmap = __interp_grids(src_grid, z_grid, ncsrc, ncout, records=records,\n                              threads=threads, nx=nx, ny=ny, vmap=vmap, weight=weight,\n                              z_mask=True, pmap=pmap)\n    except TimeoutError:\n        print(""Timeout: process is hung, deleting output."")\n        # Delete the output file\n        os.remove(z_file)\n    finally:\n        # Clean up\n        ncsrc.close()\n        ncout.close()\n\n    return pmap\n\n\ndef to_grid(src_file, dest_file, src_grid=None, dest_grid=None, records=None,\n            clobber=False, cdl=None, threads=2, reftime=None, nx=0, ny=0,\n            weight=10, vmap=None, pmap=None):\n    """"""\n    Given an existing model file, create (if does not exit) a\n    new ROMS history file using the given ROMS destination grid and\n    interpolate the ROMS fields onto the new grid. If an existing destination\n    file is given, it is interpolated onto the specified.\n\n    Parameters\n    ----------\n    src_file  : string,\n        Filename of src file to interpolate from\n    dest_file : string,\n        Name of desination file to write to\n    src_grid : (string or seapy.model.grid), optional:\n        Name or instance of source grid. If nothing is specified,\n        derives grid from the roms_file\n    dest_grid: (string or seapy.model.grid), optional:\n        Name or instance of output definition\n    records : numpy.ndarray, optional:\n        Record indices to interpolate\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    threads : int, optional:\n        number of processing threads\n    reftime: datetime, optional:\n        Reference time as the epoch for ROMS file\n    nx : float, optional:\n        decorrelation length-scale for OA (same units as source data)\n    ny : float, optional:\n        decorrelation length-scale for OA (same units as source data)\n    weight : int, optional:\n        number of points to use in weighting matrix\n    vmap : dictionary, optional\n        mapping source and destination variables\n    pmap : numpy.ndarray, optional:\n        use the specified pmap rather than compute it\n\n    Returns\n    -------\n    pmap : ndarray\n        the weighting matrix computed during the interpolation\n    """"""\n    if src_grid is None:\n        src_grid = seapy.model.asgrid(src_file)\n    else:\n        src_grid = seapy.model.asgrid(src_grid)\n    ncsrc = seapy.netcdf(src_file)\n    if dest_grid is not None:\n        destg = seapy.model.asgrid(dest_grid)\n\n        if not os.path.isfile(dest_file):\n            src_ref, time = seapy.roms.get_reftime(ncsrc)\n            if reftime is not None:\n                src_ref = reftime\n            records = np.arange(0, ncsrc.variables[time].shape[0]) \\\n                if records is None else np.atleast_1d(records)\n            ncout = seapy.roms.ncgen.create_ini(dest_file,\n                                                eta_rho=destg.eta_rho,\n                                                xi_rho=destg.xi_rho,\n                                                s_rho=destg.n,\n                                                reftime=src_ref,\n                                                clobber=clobber,\n                                                cdl=cdl,\n                                                title=""interpolated from "" + src_file)\n            destg.to_netcdf(ncout)\n            ncout.variables[""ocean_time""][:] = seapy.roms.date2num(\n                seapy.roms.num2date(ncsrc, time, records),\n                ncout, ""ocean_time"")\n\n    if os.path.isfile(dest_file):\n        ncout = netCDF4.Dataset(dest_file, ""a"")\n        if dest_grid is None:\n            destg = seapy.model.asgrid(dest_file)\n    else:\n        raise AttributeError(""Missing destination grid or file"")\n\n    # Call the interpolation\n    try:\n        src_grid.set_east(destg.east())\n        pmap = __interp_grids(src_grid, destg, ncsrc, ncout, records=records,\n                              threads=threads, nx=nx, ny=ny, weight=weight,\n                              vmap=vmap, pmap=pmap)\n    except TimeoutError:\n        print(""Timeout: process is hung, deleting output."")\n        # Delete the output file\n        os.remove(dest_file)\n    finally:\n        # Clean up\n        ncsrc.close()\n        ncout.close()\n\n    return pmap\n\n\ndef to_clim(src_file, dest_file, src_grid=None, dest_grid=None,\n            records=None, clobber=False, cdl=None, threads=2, reftime=None,\n            nx=0, ny=0, weight=10, vmap=None, pmap=None):\n    """"""\n    Given an model output file, create (if does not exit) a\n    new ROMS climatology file using the given ROMS destination grid and\n    interpolate the ROMS fields onto the new grid. If an existing destination\n    file is given, it is interpolated onto the specified.\n\n    Parameters\n    ----------\n    src_file  : string,\n        Filename of src file to interpolate from\n    dest_file : string,\n        Name of desination file to write to\n    src_grid : (string or seapy.model.grid), optional:\n        Name or instance of source grid. If nothing is specified,\n        derives grid from the roms_file\n    dest_grid: (string or seapy.model.grid), optional:\n        Name or instance of output definition\n    records : numpy.ndarray, optional:\n        Record indices to interpolate\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    threads : int, optional:\n        number of processing threads\n    reftime: datetime, optional:\n        Reference time as the epoch for climatology file\n    nx : float, optional:\n        decorrelation length-scale for OA (same units as source data)\n    ny : float, optional:\n        decorrelation length-scale for OA (same units as source data)\n    weight : int, optional:\n        number of points to use in weighting matrix\n    vmap : dictionary, optional\n        mapping source and destination variables\n    pmap : numpy.ndarray, optional:\n        use the specified pmap rather than compute it\n\n    Returns\n    -------\n    pmap : ndarray\n        the weighting matrix computed during the interpolation\n    """"""\n    if dest_grid is not None:\n        destg = seapy.model.asgrid(dest_grid)\n        if src_grid is None:\n            src_grid = seapy.model.asgrid(src_file)\n        else:\n            src_grid = seapy.model.asgrid(src_grid)\n        ncsrc = seapy.netcdf(src_file)\n        src_ref, time = seapy.roms.get_reftime(ncsrc)\n        if reftime is not None:\n            src_ref = reftime\n        records = np.arange(0, ncsrc.variables[time].shape[0]) \\\n            if records is None else np.atleast_1d(records)\n        ncout = seapy.roms.ncgen.create_clim(dest_file,\n                                             eta_rho=destg.ln,\n                                             xi_rho=destg.lm,\n                                             s_rho=destg.n,\n                                             reftime=src_ref,\n                                             clobber=clobber,\n                                             cdl=cdl,\n                                             title=""interpolated from "" + src_file)\n        src_time = seapy.roms.num2date(ncsrc, time, records)\n        ncout.variables[""clim_time""][:] = seapy.roms.date2num(\n            src_time, ncout, ""clim_time"")\n    else:\n        raise AttributeError(\n            ""you must supply a destination file or a grid to make the file"")\n\n    # Call the interpolation\n    try:\n        src_grid.set_east(destg.east())\n        pmap = __interp_grids(src_grid, destg, ncsrc, ncout, records=records, threads=threads,\n                              nx=nx, ny=ny, vmap=vmap, weight=weight, pmap=pmap)\n    except TimeoutError:\n        print(""Timeout: process is hung, deleting output."")\n        # Delete the output file\n        os.remove(dest_file)\n    finally:\n        # Clean up\n        ncsrc.close()\n        ncout.close()\n    return pmap\n\n\npass\n'"
seapy/roms/lib.py,52,"b'#!/usr/bin/env python\n""""""\n  lib.py\n\n  General ROMS utils\n\n  Written by Brian Powell on 05/24/13\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\n\nimport numpy as np\nimport seapy\nfrom seapy.lib import default_epoch, secs2day\nimport netCDF4\n\nfields = {""zeta"": {""grid"": ""rho"", ""dims"": 2},\n          ""ubar"": {""grid"": ""u"", ""dims"": 2, ""rotate"": ""vbar""},\n          ""vbar"": {""grid"": ""v"", ""dims"": 2, ""rotate"": ""ubar""},\n          ""u"": {""grid"": ""u"", ""dims"": 3, ""rotate"": ""v""},\n          ""v"": {""grid"": ""v"", ""dims"": 3, ""rotate"": ""u""},\n          ""temp"": {""grid"": ""rho"", ""dims"": 3},\n          ""salt"": {""grid"": ""rho"", ""dims"": 3}}\nids = {1: ""zeta"", 2: ""ubar"", 3: ""vbar"", 4: ""u"", 5: ""v"", 6: ""temp"", 7: ""salt""}\n\n\ndef stretching(vstretching=2, theta_s=2, theta_b=0.1, hc=100, s_rho=10,\n               w_grid=False):\n    """"""\n    Compute the stretching function for ROMS\n\n    Parameters\n    ----------\n    vstretching : int, optional\n        stretching algorithm type\n    theta_s: float, optional\n        value of surface theta\n    theta_b: float, optional\n        value of bottom theta\n    hc: int, optional\n        critical depth\n    s_rho: int, optional\n        number of s-levels\n    w_grid: bool, optional\n        solve stretching on the w-grid\n\n    Returns\n    -------\n    s, cs: array\n\n    """"""\n    ds = 1.0 / s_rho\n    if w_grid:\n        lev = np.arange(1, s_rho + 1)\n    else:\n        lev = np.arange(1, s_rho + 1) - 0.5\n    s = (lev - s_rho) * ds\n\n    if vstretching == 1:\n        if theta_s > 0:\n            ptheta = np.sinh(theta_s * s) / np.sinh(theta_s)\n            rtheta = np.tanh(theta_s * (s + 0.5)) / \\\n                (2.0 * np.tanh(0.5 * theta_s)) - 0.5\n            cs = (1.0 - theta_b) * ptheta + theta_b * rtheta\n        else:\n            cs = s\n        pass\n    elif vstretching == 2:\n        if theta_s > 0:\n            csur = (1.0 - np.cosh(theta_s * s)) / (np.cosh(theta_s) - 1.0)\n            if theta_b > 0:\n                cbot = -1.0 + np.sinh(theta_b * (s + 1.0)) / np.sinh(theta_b)\n                weight = (s + 1.0) * (1.0 - s)\n                cs = weight * csur + (1.0 - weight) * cbot\n            else:\n                cs = csur\n        else:\n            cs = s\n        pass\n    elif vstretching == 3:\n        if theta_s > 0:\n            exp_s = theta_s\n            exp_b = theta_b\n            alpha = 3\n            cbot = np.log(np.cosh(alpha * (s + 1.0)**exp_b)) / \\\n                np.log(np.cosh(alpha)) - 1.0\n            csur = -np.log(np.cosh(alpha * np.abs(s)**exp_s)) / \\\n                np.log(np.cosh(alpha))\n            weight = (1.0 - np.tanh(alpha * (s + .5))) / 2.0\n            cs = weight * cbot + (1.0 - weight) * csur\n        else:\n            cs = s\n        pass\n    elif vstretching == 4:\n        if theta_s > 0:\n            csur = (1.0 - np.cosh(theta_s * s)) / (np.cosh(theta_s) - 1.0)\n        else:\n            csur = -(s * s)\n        pass\n        if theta_b > 0:\n            cs = (np.exp(theta_b * csur) - 1.0) / (1.0 - np.exp(-theta_b))\n        else:\n            cs = s\n        pass\n    elif vstretching == 5:\n        s = -(lev * lev - 2 * lev * s_rho + lev + s_rho * s_rho - s_rho) / \\\n            (1.0 * s_rho * s_rho - s_rho) - \\\n            0.01 * (lev * lev - lev * s_rho) / (1.0 - s_rho)\n        if theta_s > 0:\n            csur = (1.0 - np.cosh(theta_s * s)) / (np.cosh(theta_s) - 1)\n        else:\n            csur = -(s * s)\n        if theta_b > 0:\n            cs = (np.exp(theta_b * (csur + 1.0)) - 1.0) / \\\n                (np.exp(theta_b) - 1.0) - 1.0\n        else:\n            cs = csur\n        pass\n    else:\n        raise ValueError(""stretching value must be between 1 and 5"")\n\n    return s, cs\n\n\ndef depth(vtransform=1, h=None, hc=100, scoord=None,\n          stretching=None, zeta=0, w_grid=False):\n    """"""\n    Solve the depth of the given bathymetry in s-levels.\n\n    Parameters\n    ----------\n    vtransform : int, optional\n        transform algorithm type\n    h: array, optional\n        value of bottom depths\n    hc: int, optional\n        critical depth\n    scoord: array\n        s coordinates from stretching method\n    stretching: array\n        stretching values from stretching method\n    zeta: array\n        sea surface height to add to bottom\n    w_grid: bool, optional\n        solve stretching on the w-grid\n\n    Returns\n    -------\n    z: ndarray,\n      depth of grid cells\n\n    """"""\n    if h is None or scoord is None or stretching is None:\n        raise AttributeError(""you must supply h, scoord, and stretching"")\n    if scoord.size != stretching.size:\n        raise ValueError(\n            ""the stretching and scoord arrays must be the same size"")\n    N = scoord.size\n    hinv = 1 / h\n    h = np.asanyarray(h)\n    wk = 0\n    r = range(N)\n    if w_grid:\n        N = N + 1\n        wk = 1\n    z = np.zeros(np.hstack((N, h.shape)))\n\n    if vtransform == 1:\n        cff = hc * (scoord - stretching)\n        for k in r:\n            z0 = cff[k] + stretching[k] * h\n            z[k + wk, :] = z0 + zeta * (1.0 + z0 * hinv)\n    elif vtransform == 2:\n        cff = 1 / (hc + h)\n        for k in r:\n            cff1 = hc * scoord[k] + h * stretching[k]\n            z[k + wk, :] = zeta + (zeta + h) * cff * cff1\n    else:\n        raise ValueError(""transform value must be between 1 and 2"")\n    if w_grid:\n        z[0, :] = -h\n\n    return z\n\n\ndef thickness(vtransform=1, h=None, hc=100, scoord=None,\n              stretching=None, zeta=0):\n    """"""\n    Get the thickness of the grid cells for the given sigma-parameters.\n\n    Parameters\n    ----------\n    vtransform : int, optional\n        transform algorithm type\n    h: array, optional\n        value of bottom depths\n    hc: int, optional\n        critical depth\n    scoord: array\n        s coordinates from stretching method\n    stretching: array\n        stretching values from stretching method\n    zeta: array\n        sea surface height to add to bottom\n    w_grid: bool, optional\n        solve stretching on the w-grid\n\n    Returns\n    -------\n    hz : array,\n      thickness\n    """"""\n    # Get the w-coordinate depths and return the differenc\n    z_w = depth(vtransform, h, hc, scoord, stretching, zeta, True)\n    return z_w[1:, :, :] - z_w[0:-1, :, :]\n\n\ndef gen_boundary_region(shp, north=None, east=None, west=None, south=None,\n                        kind=\'linear\'):\n    """"""\n    Generate a masked field varying from 1 at the boundary to 0 in the\n    middle along each of the specified boundaries. This is used to create\n    nudging and sponge fields to save into the respective ROMS files.\n\n    Parameters\n    ----------\n    shp : tuple,\n      The shape of the grid to use\n    north : int, optional,\n      The size of the region in the north boundary\n    south : int, optional,\n      The size of the region in the south boundary\n    east : int, optional,\n      The size of the region in the east boundary\n    west : int, optional,\n      The size of the region in the west boundary\n    kind : string, optional,\n      The type of transition:\n         \'linear\' (default)\n         \'cosine\'\n\n    Returns\n    -------\n    fld : np.ma.array,\n      array containing boundary values ranging from 0 to 1. masked values\n      were not set by the routine, but the fill_value is set to 0.\n    """"""\n    fld = np.ma.zeros(shp, fill_value=0)\n    fld[:] = np.ma.masked\n\n    # Set up a dictionary to define how to deal with each boundary.\n    # The tuple is (dimension, array_end, rotate)\n    dirs = {""north"": (shp[1], True, True),\n            ""south"": (shp[1], False, True),\n            ""east"": (shp[0], True, False),\n            ""west"": (shp[0], False, False)}\n    ref = locals()\n    for d in dirs:\n        # Set the factor to generate the values\n        nx = ref[d]\n        if nx is None or nx == 0:\n            continue\n        x = np.arange(nx)\n        if kind == ""cosine"":\n            x = np.cos(np.pi / (2.0 * nx) * x)[::-1]\n        else:\n            x = 1.0 / nx * x\n        x = np.tile(x[::-1], [dirs[d][0], 1])\n        # If the boundary is the end, flip it\n        sl = np.array([slice(None, None, None), slice(None, nx, None)])\n        if dirs[d][1]:\n            x = np.fliplr(x)\n            sl[1] = slice(-nx, None, None)\n        # If the dimensions are rotated, transpose\n        if dirs[d][2]:\n            x = np.transpose(x)\n            sl = sl[::-1]\n        sl = (sl[0], sl[1])\n        fld[sl] = np.maximum(fld.filled()[sl], x)\n\n    return fld\n\n\ndef _get_calendar(var):\n    """"""\n    Get the proper calendar string from a netcdf file\n\n    Parameters\n    ----------\n    var : netCDF4.variable\n\n    Returns\n    -------\n    calendar type: string,\n      The type of calendar system used\n    convert : bool\n      True if the calendar needs to be converted to datetime\n    """"""\n    # Set up the mapping for calendars\n    default = 1\n    calendar_conv = [False, False, False,\n                     True, True, True, False, False, False]\n    calendar_types = [\'standard\', \'gregorian\', \'proleptic_gregorian\', \'noleap\',\n                      \'julian\', \'all_leap\', \'365_day\', \'366_day\', \'360_day\']\n    cals = {v: v for v in calendar_types}\n    cals[\'gregorian_proleptic\'] = \'proleptic_gregorian\'\n\n    # Load the calendar type. If it is incorrectly specified (*cough* ROMS), change it\n    for cal in (\'calendar\', \'calendar_type\'):\n        if hasattr(var, cal):\n            cal = cals.get(str(getattr(var, cal)).lower(),\n                           calendar_types[default])\n            return cal, calendar_conv[calendar_types == cal]\n    return calendar_types[default], calendar_conv[default]\n\n\ndef date2num(dates, nc, tvar=None):\n    """"""\n    Convert the datetime vector to number for the given netcdf files considering\n    the units and the calendar type used. This is a wrapper to the netCDF4.date2num\n    function to account for calendar strangeness in ROMS\n\n    Parameters\n    ----------\n    dates : array of datetime.datetime\n      Values to convert\n    nc : netCDF4.Dataset,\n      netcdf input file\n    tvar : string, optional\n      time variable to load. If not specified, it will find the\n      time variable from predefined\n\n    Returns\n    -------\n    ndarray,\n       Array of values in the correct units/calendar of the netCDF file\n    """"""\n    tvar = tvar if tvar else get_timevar(nc)\n\n    calendar, _ = _get_calendar(nc.variables[tvar])\n    # Convert the times\n    return netCDF4.date2num(dates,\n                            nc.variables[tvar].units,\n                            calendar=calendar)\n\n\ndef num2date(nc, tvar=None, records=None, epoch=None):\n    """"""\n    Load the time vector from a netCDF file as a datetime array, accounting\n    for units and the calendar type used. This is a wrapper to the netCDF4.num2date\n    function to account for calendar strangeness in ROMS\n\n    Parameters\n    ----------\n    nc : netCDF4.Dataset,\n      netcdf input file\n    tvar : string, optional\n      time variable to load. If not specified, it will find the\n      time variable from predefined\n    records : array or slice, optional\n      the indices of records to load\n    epoch : datetime.datetime, optional\n      if you would like the values relative to an epoch, then\n      specify the epoch to remove.\n\n    Returns\n    -------\n    ndarray,\n       Array of datetimes if no epoch is supplied. If epoch, array\n       is in days since epoch\n    """"""\n    import datetime\n    records = records if records is not None else np.s_[:]\n    tvar = tvar if tvar else get_timevar(nc)\n    calendar, convert = _get_calendar(nc.variables[tvar])\n\n    # Load the times\n    times = netCDF4.num2date(nc.variables[tvar][records],\n                             nc.variables[tvar].units,\n                             calendar=calendar)\n    if convert:\n        times = [datetime.datetime.strptime(t.strftime(), \'%Y-%m-%d-%H:%M:%S\')\n                 for t in times]\n\n    if not epoch:\n        return times\n    else:\n        return np.asarray([(t - epoch).total_seconds() * secs2day for t in times])\n\n\ndef get_timevar(nc):\n    """"""\n    Find the appropriate time variable (bry_time, ocean_time, etc.) from a\n    given netcdf file\n\n    Parameters\n    ----------\n    nc : netCDF4.Dataset netcdf input file\n\n    Returns\n    -------\n    time: string\n\n    """"""\n    for time in (""ocean_time"", ""time"", ""bry_time"", ""wind_time"",\n                 ""clim_time"", ""frc_time"", ""zeta_time""):\n        if time in nc.variables:\n            return time\n    return None\n\n\ndef get_reftime(nc, epoch=default_epoch):\n    """"""\n    Given a ROMS netCDF4 file, return the reference time for the file. This\n    is the timebase of the record dimension in the format:\n    ""<units> since <reftime>""\n\n    Parameters\n    ----------\n    nc : netCDF4 dataset\n        Input ROMS file\n    epoch_str : string, optional\n        If lacking units, use this string as the units\n\n    Returns\n    -------\n    timebase : datetime\n        datetime of the origin for the file\n    time : string\n        name of variable used to generate the base (None if default)\n    """"""\n    try:\n        tvar = get_timevar(nc)\n        calendar, _ = _get_calendar(nc.variables[tvar])\n\n        return netCDF4.num2date(0, nc.variables[tvar].units,\n                                calendar=calendar), tvar\n    except AttributeError:\n        return epoch, None\n\n\ndef omega(grid, u, v, zeta=0, scale=True, work=False):\n    """"""\n    Compute the vertical velocity on s-grid.\n\n    Parameters\n    ----------\n    grid : seapy.model.grid,\n      The grid to use for the calculations\n    u : ndarray,\n      The u-field in time\n    v : ndarray,\n      The v-field in time\n    zeta : ndarray, optional,\n      The zeta-field in time\n    scale : bool, optional,\n      If [True], return omega in [m s**-1];\n      If False, return omega in [m**3 s**-1]\n    work : bool, optional,\n      If True, return the work arrays:\n        z_r : ndarray,\n          Depth on rho-grid (time-varying if zeta != 0)\n        z_w : ndarray,\n          Depth on w-grid (time-varying if zeta != 0)\n        thick_u : ndarray\n          Thickness of the u-grid\n        thick_v : ndarray\n          Thickness of the v-grid\n      If False, return only omega\n\n    Returns\n    -------\n    omega : ndarray,\n      Vertical Velocity on s-grid\n    """"""\n    grid = seapy.model.asgrid(grid)\n    u = np.ma.array(u)\n    v = np.ma.array(v)\n    zeta = np.ma.array(zeta)\n\n    # Check the sizes\n    while u.ndim < 4:\n        u = u[np.newaxis, ...]\n    while v.ndim < 4:\n        v = v[np.newaxis, ...]\n    while zeta.ndim < 3:\n        zeta = zeta[np.newaxis, ...]\n\n    # Get the model grid parameters for the given thickness\n    thick_u = u * 0\n    thick_v = v * 0\n    z_r = np.ma.zeros((u.shape[0], u.shape[1], zeta.shape[1], zeta.shape[2]))\n    z_w = np.ma.zeros((u.shape[0], u.shape[1] + 1,\n                       zeta.shape[1], zeta.shape[2]))\n    for i in range(zeta.shape[0]):\n        s_w, cs_w = seapy.roms.stretching(\n            grid.vstretching, grid.theta_s, grid.theta_b, grid.hc,\n            grid.n, w_grid=True)\n        z_r[i, ...] = seapy.roms.depth(grid.vtransform, grid.h, grid.hc,\n                                       s_w, cs_w, zeta=zeta[i, ...], w_grid=False)\n        z_w[i, ...] = seapy.roms.depth(grid.vtransform, grid.h, grid.hc,\n                                       s_w, cs_w, zeta=zeta[i, ...], w_grid=True)\n        thick_rho = np.squeeze(z_w[i, 1:, :, :] - z_w[i, :-1, :, :])\n        thick_u[i, ...] = seapy.model.rho2u(thick_rho)\n        thick_v[i, ...] = seapy.model.rho2v(thick_rho)\n    z_r[z_r > 50000] = np.ma.masked\n    z_w[z_w > 50000] = np.ma.masked\n\n    # Compute W (omega)\n    Huon = u * thick_u * seapy.model.rho2u(grid.dn)\n    Hvom = v * thick_v * seapy.model.rho2v(grid.dm)\n    W = z_w * 0\n    for k in range(grid.n):\n        W[:, k + 1, :-2, :-2] = W[:, k, :-2, :-2] - \\\n            (Huon[:, k, 1:-1, 1:] - Huon[:, k, 1:-1, :-1]\n             + Hvom[:, k, 1:, 1:-1] - Hvom[:, k, :-1, 1:-1])\n    wrk = W[:, -1:, :, :] / (z_w[:, -1:, :, :] - z_w[:, 0:1, :, :])\n    W[:, :-1, :, :] = W[:, :-1, :, :] - wrk * \\\n        (z_w[:, :-1, :, :] - z_w[:, 0:1, :, :])\n    W[:, -1, :, :] = 0\n\n    if scale:\n        W *= grid.pn * grid.pm\n    if work:\n        return W, z_r, z_w, thick_u, thick_v\n    else:\n        return W\n\n\ndef wvelocity(grid, u, v, zeta=0):\n    """"""\n    Compute ""true"" vertical velocity\n\n    Parameters\n    ----------\n    grid : seapy.model.grid,\n      The grid to use for the calculations\n    u : ndarray,\n      The u-field in time\n    v : ndarray,\n      The v-field in time\n    zeta : ndarray, optional,\n      The zeta-field in time\n\n    Returns\n    -------\n    w : ndarray,\n      Vertical Velocity\n    """"""\n    grid = seapy.model.asgrid(grid)\n    u = np.ma.array(u)\n    v = np.ma.array(v)\n    zeta = np.ma.array(zeta)\n\n    # Check the sizes\n    while u.ndim < 4:\n        u = u[np.newaxis, ...]\n    while v.ndim < 4:\n        v = v[np.newaxis, ...]\n    while zeta.ndim < 3:\n        zeta = zeta[np.newaxis, ...]\n\n    # Get omega\n    W, z_r, z_w, thick_u, thick_v = omega(grid, u, v, zeta, scale=True,\n                                          work=True)\n\n    # Compute quasi-horizontal motions (Ui + Vj)*GRAD s(z)\n    vert = z_r * 0\n    # U-contribution\n    wrk = u * (z_r[:, :, :, 1:] - z_r[:, :, :, :-1]) * \\\n        (grid.pm[:, 1:] - grid.pm[:, :-1])\n    vert[:, :, :, 1:-1] = 0.25 * (wrk[:, :, :, :-1] + wrk[:, :, :, 1:])\n    # V-contribution\n    wrk = v * (z_r[:, :, 1:, :] - z_r[:, :, :-1, :]) * \\\n        (grid.pn[1:, :] - grid.pn[:-1, :])\n    vert[:, :, 1:-1, :] += 0.25 * (wrk[:, :, :-1, :] + wrk[:, :, 1:, :])\n\n    # Compute barotropic velocity [ERROR IN FORMULATION RIGHT NOW]\n    wrk = np.zeros((vert.shape[0], vert.shape[2], vert.shape[3]))\n    ubar = np.sum(u * thick_u, axis=1) / np.sum(thick_u, axis=1)\n    vbar = np.sum(v * thick_v, axis=1) / np.sum(thick_v, axis=1)\n    # wrk[:, 1:-1, 1:-1] = (ubar[:, 1:-1, :-1] - ubar[:, 1:-1, 1:] +\n    #                       vbar[:, :-1, 1:-1] - vbar[:, 1:, 1:-1])\n\n    # Shift vert from rho to w\n    wvel = z_w * 0\n    # First two layers\n    slope = (z_r[:, 0, :, :] - z_w[:, 0, :, :]) / \\\n        (z_r[:, 1, :, :] - z_r[:, 0, :, :])\n    wvel[:, 0, :, :] = 0.375 * (vert[:, 0, :, :] - slope *\n                                (vert[:, 1, :, :] - vert[:, 0, :, :])) + \\\n        0.75 * vert[:, 0, :, :] - \\\n        0.125 * vert[:, 1, :, :]\n    wvel[:, 1, :, :] = W[:, 1, :, :] + wrk + \\\n        0.375 * vert[:, 0, :, :] + \\\n        0.75 * vert[:, 1, :, :] - 0.125 * vert[:, 2, :, :]\n\n    # Middle of the grid\n    wvel[:, 2:-2, :, :] = W[:, 2:-2, :, :] + \\\n        wrk[:, np.newaxis, :, :] + \\\n        0.5625 * (vert[:, 1:-2, :, :] + vert[:, 2:-1, :, :]) - \\\n        0.0625 * (vert[:, :-3, :, :] + vert[:, 3:, :, :])\n\n    # Upper two layers\n    slope = (z_w[:, -1, :, :] - z_r[:, -1, :, :]) / \\\n        (z_r[:, -1, :, :] - z_r[:, -2, :, :])\n    wvel[:, -1, :, :] = wrk + 0.375 * (vert[:, -1, :, :] + slope *\n                                       (vert[:, -1, :, :] - vert[:, -2, :, :])) + \\\n        0.75 * vert[:, -1, :, :] - \\\n        0.0625 * vert[:, -2, :, :]\n    wvel[:, -2, :, :] = W[:, -2, :, :] + 0.375 * vert[:, -1, :, :] + \\\n        wrk + 0.75 * vert[:, -2, :, :] - \\\n        0.125 * vert[:, -3, :, :]\n\n    # No gradient at the boundaries\n    wvel[:, :, 0, :] = wvel[:, :, 1, :]\n    wvel[:, :, -2:, :] = wvel[:, :, -3:-2, :]\n    wvel[:, :, :, 0] = wvel[:, :, :, 1]\n    wvel[:, :, :, -2:] = wvel[:, :, :, -3:-2]\n\n    return wvel\n\n\npass\n'"
seapy/roms/ncgen.py,1,"b'#!/usr/bin/env python\n""""""\n  Functions to generate ROMS netcdf files\n\n  Written by Brian Powell on 04/26/13\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\n\n\nimport os\nimport re\nimport netCDF4\nimport numpy as np\nfrom datetime import datetime\nfrom seapy.lib import default_epoch\nfrom seapy.cdl_parser import cdl_parser\nfrom seapy.roms import lib\nfrom warnings import warn\n\n""""""\n    Module variables\n""""""\n_cdl_dir = os.path.dirname(lib.__file__)\n_cdl_dir = ""/"".join(((\'.\' if not _cdl_dir else _cdl_dir), ""cdl/""))\n_format = ""NETCDF4_CLASSIC""\n\n\ndef __number_or_string(val):\n    """"""\n    convert a string to a number if the string represents a number;\n    otherwise, return the string.\n    """"""\n    try:\n        val = float(val.strip())\n    except ValueError:\n        pass\n    return val\n\n\ndef ncgen(filename, dims=None, vars=None, attr=None, title=None,\n          clobber=False, format=_format):\n    """"""\n    Create a new netcdf file with the given definitions. Need to define\n    the dimensions, the variables, and the attributes.\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    dims : dict\n        dictionary of dimensions with dimension name as keys, and the value\n        as the length of the dimension. NOTE: 0 value means UNLIMITED.\n    vars: list of dictionaries\n        each variable to define is a dictionary that contains three keys:\n            name: string name of variable\n            type: string type (float, double, etc.)\n            dims: comma separated string of dimensions (""ocean_time, eta_rho"")\n            attr: dictionary of variable attributes where the key is\n                  the attribute name and the value is the attribute string\n    attr: dict, optional\n        optional dictionary of global attributes for the netcdf file:\n        key is the attribute name and the value is the attribute string\n    title: string, optional\n        netcdf attribute title\n    clobber: bool, optional\n        If True, destroy existing file\n    format: string, optional\n        NetCDF format to use. Default is NETCDF4_CLASSIC\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    Examples\n    --------\n    >>> dims = {""ocean_time"":0, ""eta_rho"":120, ""xi_rho"":100}\n    >>> vars = [  {""name"":""eta_slice"", ""type"":""double"",\n                   ""dims"":""ocean_time, eta_rho"",\n                   ""attr"":{""units"":""degrees Celcius""}},\n                  {""name"":""xi_slice"", ""type"":""double"",\n                   ""dims"":""ocean_time, xi_rho"",\n                   ""attr"":{""units"":""degrees Celcius""}} ]\n    >>> seapy.roms.ncgen(""test.nc"", dims=dims, vars=vars, title=""Test"")\n    """"""\n    vars = np.atleast_1d(vars)\n    if dims is None:\n        dims = {}\n    if attr is None:\n        attr = {}\n    # Create the file\n    if not os.path.isfile(filename) or clobber:\n        _nc = netCDF4.Dataset(filename, ""w"", format=format)\n        # Loop over the dimensions and add them\n        for dim in dims:\n            _nc.createDimension(dim, dims[dim])\n        # Loop over the variables and add them\n        for var in vars:\n            add_variable(_nc, var)\n\n        # Add global attributes\n        for a in attr:\n            _nc.setncattr(a, attr[a])\n\n        try:\n            _nc.author = os.getenv(\'USER\') or \\\n                os.getenv(\'LOGNAME\') or \\\n                os.getenv(\'USERNAME\') or \\\n                os.getlogin() or \\\n                \'nobody\'\n        except (AttributeError, IOError, OSError, FileNotFoundError) as e:\n            _nc.author = \'nobody\'\n\n        _nc.history = datetime.now().strftime(\n            ""Created on %a, %B %d, %Y at %H:%M"")\n        if title is not None:\n            _nc.title = title\n        _nc.close()\n    else:\n        warn(filename + "" already exists. Using existing definition"")\n    return netCDF4.Dataset(filename, ""a"")\n    pass\n\n\ndef _set_grid_dimensions(dims, eta_rho, xi_rho, s_rho):\n    """"""\n        internal method: Set grid dimensions\n    """"""\n    if ""xi_rho"" in dims.keys():\n        dims[""xi_rho""] = xi_rho\n    if ""xi_u"" in dims.keys():\n        dims[""xi_u""] = xi_rho - 1\n    if ""xi_v"" in dims.keys():\n        dims[""xi_v""] = xi_rho\n    if ""xi_psi"" in dims.keys():\n        dims[""xi_psi""] = xi_rho - 1\n    if ""eta_rho"" in dims.keys():\n        dims[""eta_rho""] = eta_rho\n    if ""eta_u"" in dims.keys():\n        dims[""eta_u""] = eta_rho\n    if ""eta_v"" in dims.keys():\n        dims[""eta_v""] = eta_rho - 1\n    if ""eta_psi"" in dims.keys():\n        dims[""eta_psi""] = eta_rho - 1\n    if ""s_rho"" in dims.keys():\n        dims[""s_rho""] = s_rho\n    if ""s_w"" in dims.keys():\n        dims[""s_w""] = s_rho + 1\n\n    return dims\n\n\ndef _set_time_ref(vars, timevar, reftime, cycle=None):\n    """"""\n        internal method: Set time reference\n    """"""\n    if isinstance(timevar, str):\n        timevar = [timevar]\n    for tvar in timevar:\n        for nvar in vars:\n            if nvar[""name""] == tvar:\n                if ""units"" in nvar[""attr""]:\n                    t = re.findall(\'(\\w+) since .*\', nvar[""attr""][""units""])\n                    nvar[""attr""][""units""] = \\\n                        ""{:s} since {:s}"".format(t[0], str(reftime))\n                else:\n                    nvar[""attr""][""units""] = \\\n                        ""days since {:s}"".format(str(reftime))\n                if cycle is not None:\n                    nvar[""attr""][""cycle_length""] = cycle\n    return vars\n\n\ndef add_variable(nc, var):\n    """"""\n    Add a new variable with meta data to an existing netcdf file\n\n    Parameters\n    ----------\n    filename : string or netCDF4 object\n        name or file to add the variable to\n    vars: dictionary\n            name: string name of variable\n            type: string type (float, double, etc.)\n            dims: comma separated string of dimensions (""ocean_time, eta_rho"")\n            attr: dictionary of variable attributes where the key is\n                  the attribute name and the value is the attribute string\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    Examples\n    --------\n    >>> var = {""name"":""eta_slice"", ""type"":""double"",\n               ""dims"":""ocean_time, eta_rho"",\n               ""attr"":{""units"":""degrees Celcius""}}\n    >>> nc = seapy.roms.ncgen.add_variable(""test.nc"", var)\n    """"""\n    if nc is None:\n        raise AttributeError(""No file was specified"")\n    if isinstance(nc, netCDF4._netCDF4.Dataset):\n        pass\n    else:\n        nc = netCDF4.Dataset(nc, ""a"")\n\n    # Handle the dimensions by enforcing a tuple list rather\n    # than a list of strings, then add whatever we have\n    try:\n        dims = var[\'dims\'].replace("" "", """").split(\',\')\n    except:\n        dims = var[\'dims\']\n    try:\n        nvar = nc.createVariable(var[""name""], var[""type""], dims)\n    except:\n        nvar = nc.createVariable(var[""name""], var[""type""])\n\n    try:\n        for key in var[""attr""]:\n            # Check if it is a number and convert\n            astr = __number_or_string(var[""attr""][key])\n            setattr(nvar, key, astr)\n    except KeyError:\n        pass\n\n    return nc\n\n\ndef _create_generic_file(filename, cdl, eta_rho, xi_rho, s_rho,\n                         reftime=None, clobber=False, title=""ROMS""):\n    """"""\n        internal method: Generic file creator that uses ocean_time\n    """"""\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(cdl)\n\n    # Fill in the appropriate dimension values\n    dims = _set_grid_dimensions(dims, eta_rho, xi_rho, s_rho)\n    if reftime is not None:\n        vars = _set_time_ref(vars, ""ocean_time"", reftime)\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_psource(filename, nriver=1, s_rho=5,\n                   reftime=default_epoch, clobber=False, cdl=None, title=""My River""):\n    """"""\n    Create a new, blank point source file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    nriver : int, optional\n        number of rivers to put in file\n    s_rho: int, optional\n        number of s-levels\n    reftime: datetime, optional\n        date of epoch for time origin in netcdf\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(\n        _cdl_dir + ""frc_rivers.cdl"" if cdl is None else cdl)\n\n    # Fill in the appropriate river values\n    dims[""river""] = nriver\n    dims[""s_rho""] = s_rho\n    vars = _set_time_ref(vars, ""river_time"", reftime)\n\n    # Create the river file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_grid(filename, eta_rho=10, xi_rho=10, s_rho=1, clobber=False,\n                cdl=None, title=""My Grid""):\n    """"""\n    Create a new, blank grid file\n\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    eta_rho: int, optional\n        number of rows in the eta direction\n    xi_rho: int, optional\n        number of columns in the xi direction\n    s_rho: int, optional\n        number of s-levels\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(\n        _cdl_dir + ""roms_grid.cdl"" if cdl is None else cdl)\n\n    # Fill in the appropriate dimension values\n    dims = _set_grid_dimensions(dims, eta_rho, xi_rho, s_rho)\n\n    print(dims)\n\n    # Create the grid file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_adsen(filename, eta_rho=10, xi_rho=10, s_rho=1,\n                 reftime=default_epoch, clobber=False, cdl=None, title=""My Adsen""):\n    """"""\n    Create a new adjoint sensitivity file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    eta_rho: int, optional\n        number of rows in the eta direction\n    xi_rho: int, optional\n        number of columns in the xi direction\n    s_rho: int, optional\n        number of s-levels\n    reftime: datetime, optional\n        date of epoch for time origin in netcdf\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    # Create the general file\n    return _create_generic_file(filename, _cdl_dir + ""adsen.cdl"" if cdl is None else cdl,\n                                eta_rho, xi_rho, s_rho, reftime, clobber, title)\n\n\ndef create_bry(filename, eta_rho=10, xi_rho=10, s_rho=1,\n               reftime=default_epoch, clobber=False, cdl=None, title=""My BRY""):\n    """"""\n    Create a bry forcing file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    eta_rho: int, optional\n        number of rows in the eta direction\n    xi_rho: int, optional\n        number of columns in the xi direction\n    s_rho: int, optional\n        number of s-levels\n    reftime: datetime, optional\n        date of epoch for time origin in netcdf\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(\n        _cdl_dir + ""bry_unlimit.cdl"" if cdl is None else cdl)\n\n    # Fill in the appropriate dimension values\n    dims = _set_grid_dimensions(dims, eta_rho, xi_rho, s_rho)\n    vars = _set_time_ref(vars, ""bry_time"", reftime)\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_clim(filename, eta_rho=10, xi_rho=10, s_rho=1,\n                reftime=default_epoch, clobber=False, cdl=None, title=""My CLIM""):\n    """"""\n    Create a climatology forcing file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    eta_rho: int, optional\n        number of rows in the eta direction\n    xi_rho: int, optional\n        number of columns in the xi direction\n    s_rho: int, optional\n        number of s-levels\n    reftime: datetime, optional\n        date of epoch for time origin in netcdf\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(\n        _cdl_dir + ""clm_ts.cdl"" if cdl is None else cdl)\n\n    # Fill in the appropriate dimension values\n    dims = _set_grid_dimensions(dims, eta_rho, xi_rho, s_rho)\n    vars = _set_time_ref(vars, ""clim_time"", reftime)\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_frc_bulk(filename, lat=10, lon=10,\n                    reftime=default_epoch, clobber=False, cdl=None,\n                    title=""My Forcing""):\n    """"""\n    Create a bulk flux forcing file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    eta_rho: int, optional\n        number of rows in the eta direction\n    xi_rho: int, optional\n        number of columns in the xi direction\n    reftime: datetime, optional\n        date of epoch for time origin in netcdf\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(\n        _cdl_dir + ""frc_bulk.cdl"" if cdl is None else cdl)\n\n    # Fill in the appropriate dimension values\n    dims[""lat""] = lat\n    dims[""lon""] = lon\n    vars = _set_time_ref(vars, ""frc_time"", reftime)\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_frc_direct(filename, eta_rho=10, xi_rho=10,\n                      reftime=default_epoch, clobber=False, cdl=None,\n                      title=""My Forcing""):\n    """"""\n    Create a direct surface forcing file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    eta_rho: int, optional\n        number of rows in the eta direction\n    xi_rho: int, optional\n        number of columns in the xi direction\n    reftime: datetime, optional\n        date of epoch for time origin in netcdf\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(\n        _cdl_dir + ""frc_direct.cdl"" if cdl is None else cdl)\n\n    # Fill in the appropriate dimension values\n    dims = {\'y_rho\': eta_rho,\n            \'y_u\': eta_rho,\n            \'y_v\': eta_rho - 1,\n            \'x_rho\': xi_rho,\n            \'x_u\': xi_rho - 1,\n            \'x_v\': xi_rho,\n            \'frc_time\': 0}\n    vars = _set_time_ref(vars, \'frc_time\', reftime)\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_frc_flux(filename, eta_rho=10, xi_rho=10, ntimes=1,\n                    cycle=None, reftime=default_epoch, clobber=False,\n                    cdl=None, title=""My Flux""):\n    """"""\n    Create a surface flux forcing file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    eta_rho: int, optional\n        number of rows in the eta direction\n    xi_rho: int, optional\n        number of columns in the xi direction\n    s_rho: int, optional\n        number of s-levels\n    ntimes: int, optional\n        number of time records (climatology files do not have unlimited\n        dimension)\n    cycle: int or None, optional\n        The number of days before cycling the forcing records\n    reftime: datetime, optional\n        date of epoch for time origin in netcdf\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(\n        _cdl_dir + ""frc_fluxclm.cdl"" if cdl is None else cdl)\n\n    # Fill in the appropriate dimension values\n    dims = _set_grid_dimensions(dims, eta_rho, xi_rho, 1)\n    times = (""srf_time"", ""shf_time"", ""swf_time"", ""sss_time"")\n    for n in times:\n        dims[n] = ntimes\n    vars = _set_time_ref(vars, times, reftime)\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_frc_srelax(filename, eta_rho=10, xi_rho=10, s_rho=1, cycle=None,\n                      reftime=default_epoch, clobber=False, cdl=None,\n                      title=""My Srelaxation""):\n    """"""\n    Create a Salt Relaxation forcing file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    eta_rho: int, optional\n        number of rows in the eta direction\n    xi_rho: int, optional\n        number of columns in the xi direction\n    s_rho: int, optional\n        number of s-levels\n    cycle: int or None, optional\n        The number of days before cycling the forcing records\n    reftime: datetime, optional\n        date of epoch for time origin in netcdf\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(\n        _cdl_dir + ""frc_srelax.cdl"" if cdl is None else cdl)\n\n    # Fill in the appropriate dimension values\n    dims = _set_grid_dimensions(dims, eta_rho, xi_rho, s_rho)\n    vars = _set_time_ref(vars, ""sss_time"", reftime, cycle)\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_frc_qcorr(filename, eta_rho=10, xi_rho=10, s_rho=1, cycle=None,\n                     reftime=default_epoch, clobber=False, cdl=None,\n                     title=""My Qcorrection""):\n    """"""\n    Create a Q Correction forcing file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    eta_rho: int, optional\n        number of rows in the eta direction\n    xi_rho: int, optional\n        number of columns in the xi direction\n    s_rho: int, optional\n        number of s-levels\n    cycle: int or None, optional\n        The number of days before cycling the forcing records\n    reftime: datetime, optional\n        date of epoch for time origin in netcdf\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(\n        _cdl_dir + ""frc_qcorr.cdl"" if cdl is None else cdl)\n\n    # Fill in the appropriate dimension values\n    dims = _set_grid_dimensions(dims, eta_rho, xi_rho, s_rho)\n    vars = _set_time_ref(vars, ""sst_time"", reftime, cycle)\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_frc_wind(filename, eta_rho=10, xi_rho=10, s_rho=1, cycle=None,\n                    reftime=default_epoch, clobber=False, cdl=None,\n                    title=""My Winds""):\n    """"""\n    Create a surface wind stress forcing file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    eta_rho: int, optional\n        number of rows in the eta direction\n    xi_rho: int, optional\n        number of columns in the xi direction\n    s_rho: int, optional\n        number of s-levels\n    cycle: int or None, optional\n        The number of days before cycling the forcing records\n    reftime: datetime, optional\n        date of epoch for time origin in netcdf\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(\n        _cdl_dir + ""frc_windstress.cdl"" if cdl is None else cdl)\n\n    # Fill in the appropriate dimension values\n    dims = _set_grid_dimensions(dims, eta_rho, xi_rho, s_rho)\n    vars = _set_time_ref(vars, ""sms_time"", reftime, cycle)\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_frc_wave(filename, eta_rho=10, xi_rho=10, reftime=default_epoch,\n                    clobber=False, cdl=None, title=""My Waves""):\n    """"""\n    Create a surface wave forcing file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    eta_rho: int, optional\n        number of rows in the eta direction\n    xi_rho: int, optional\n        number of columns in the xi direction\n    reftime: datetime, optional\n        date of epoch for time origin in netcdf\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(\n        _cdl_dir + ""frc_wave.cdl"" if cdl is None else cdl)\n\n    # Fill in the appropriate dimension values\n    dims = _set_grid_dimensions(dims, eta_rho, xi_rho, s_rho=1)\n    vars = _set_time_ref(vars, ""wave_time"", reftime)\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_tide(filename, eta_rho=10, xi_rho=10, s_rho=1, ntides=1,\n                reftime=default_epoch, clobber=False,\n                title=""My Tides""):\n    """"""\n    Create a barotropic tide forcing file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    eta_rho: int, optional\n        number of rows in the eta direction\n    xi_rho: int, optional\n        number of columns in the xi direction\n    s_rho: int, optional\n        number of s-levels\n    ntides: int, optional\n        number of tidal frequencies to force with\n    reftime: datetime, optional\n        date of epoch for time origin in netcdf\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(_cdl_dir + ""frc_tides.cdl"")\n\n    # Fill in the appropriate dimension values\n    dims = _set_grid_dimensions(dims, eta_rho, xi_rho, s_rho)\n    dims[""tide_period""] = ntides\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_ini(filename, eta_rho=10, xi_rho=10, s_rho=1,\n               reftime=default_epoch, clobber=False, cdl=None, title=""My Ini""):\n    """"""\n    Create an initial condition file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    eta_rho: int, optional\n        number of rows in the eta direction\n    xi_rho: int, optional\n        number of columns in the xi direction\n    s_rho: int, optional\n        number of s-levels\n    reftime: datetime, optional\n        date of epoch for time origin in netcdf\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(\n        _cdl_dir + ""ini_hydro.cdl"" if cdl is None else cdl)\n\n    # Fill in the appropriate dimension values\n    dims = _set_grid_dimensions(dims, eta_rho, xi_rho, s_rho)\n    vars = _set_time_ref(vars, ""ocean_time"", reftime)\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_nudge_coef(filename, eta_rho=10, xi_rho=10, s_rho=1, clobber=False,\n                      cdl=None, title=""My Nudging""):\n    """"""\n    Create a nudging coefficients file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    eta_rho: int, optional\n        number of rows in the eta direction\n    xi_rho: int, optional\n        number of columns in the xi direction\n    s_rho: int, optional\n        number of s-levels\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(\n        _cdl_dir + ""nudge_coef.cdl"" if cdl is None else cdl)\n\n    # Fill in the appropriate dimension values\n    dims = _set_grid_dimensions(dims, eta_rho, xi_rho, s_rho)\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_da_obs(filename, state_variable=20, survey=1, provenance=None,\n                  clobber=False, cdl=None, title=""My Observations""):\n    """"""\n    Create an assimilation observations file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    survey: int, optional\n        number of surveys in the file\n    state_variable: int, optional\n        number of state variables in the observations\n    provenance: string, optional\n        Description of the provenance values\n    reftime: datetime, optional\n        date of epoch for time origin in netcdf\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n\n    """"""\n\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(\n        _cdl_dir + ""s4dvar_obs.cdl"" if cdl is None else cdl)\n\n    # Fill in the appropriate dimension values\n    dims[""survey""] = survey\n    dims[""state_variable""] = state_variable\n\n    # Set the provenance values in the global attributes\n    if provenance is not None:\n        attr[""obs_provenance""] = str(provenance)\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title, format=""NETCDF3_64BIT"")\n\n    # Return the new file\n    return _nc\n\n\ndef create_da_ray_obs(filename, ray_datum=1, provenance=""None"",\n                      reftime=default_epoch, clobber=False,\n                      cdl=None, title=""My Observations""):\n    """"""\n    Create an acoustic ray assimilation observations file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    ray_datum: int, optional\n        Number of rays to assimilate\n    provenance: string, optional\n        Description of the provenance values\n    reftime: datetime, optional\n        date of epoch for time origin in netcdf\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(\n        _cdl_dir + ""s4dvar_obs_ray.cdl"" if cdl is None else cdl)\n\n    # Fill in the appropriate dimension values\n    dims[""ray_datum""] = ray_datum\n    vars = _set_time_ref(vars, ""obs_time"", reftime)\n\n    # Set the provenance values in the global attributes\n    attr[""obs_provenance""] = provenance\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_da_bry_std(filename, eta_rho=10, xi_rho=10, s_rho=1, bry=4,\n                      reftime=default_epoch, clobber=False, cdl=None,\n                      title=""My BRY STD""):\n    """"""\n    Create a boundaries standard deviation file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    eta_rho: int, optional\n        number of rows in the eta direction\n    xi_rho: int, optional\n        number of columns in the xi direction\n    s_rho: int, optional\n        number of s-levels\n    bry: int, optional\n        number of open boundaries to specify\n    reftime: datetime, optional\n        date of epoch for time origin in netcdf\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(\n        _cdl_dir + ""s4dvar_std_b.cdl"" if cdl is None else cdl)\n\n    # Fill in the appropriate dimension values\n    dims = _set_grid_dimensions(dims, eta_rho, xi_rho, s_rho)\n    dims[""IorJ""] = max(eta_rho, xi_rho)\n    dims[""boundary""] = bry\n    vars = _set_time_ref(vars, ""ocean_time"", reftime)\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_da_frc_std(filename, eta_rho=10, xi_rho=10, s_rho=1,\n                      reftime=default_epoch, clobber=False,\n                      cdl=None, title=""My FRC STD""):\n    """"""\n    Create a forcing standard deviation file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    eta_rho: int, optional\n        number of rows in the eta direction\n    xi_rho: int, optional\n        number of columns in the xi direction\n    s_rho: int, optional\n        number of s-levels\n    reftime: datetime, optional\n        date of epoch for time origin in netcdf\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(\n        _cdl_dir + ""s4dvar_std_f.cdl"" if cdl is None else cdl)\n\n    # Fill in the appropriate dimension values\n    dims = _set_grid_dimensions(dims, eta_rho, xi_rho, s_rho)\n    vars = _set_time_ref(vars, ""ocean_time"", reftime)\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_da_ini_std(filename, eta_rho=10, xi_rho=10, s_rho=1,\n                      reftime=default_epoch, clobber=False,\n                      cdl=None, title=""My INI STD""):\n    """"""\n    Create an initialization standard deviation file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    eta_rho: int, optional\n        number of rows in the eta direction\n    xi_rho: int, optional\n        number of columns in the xi direction\n    s_rho: int, optional\n        number of s-levels\n    reftime: datetime, optional\n        date of epoch for time origin in netcdf\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(\n        _cdl_dir + ""s4dvar_std_i.cdl"" if cdl is None else cdl)\n\n    # Fill in the appropriate dimension values\n    dims = _set_grid_dimensions(dims, eta_rho, xi_rho, s_rho)\n    vars = _set_time_ref(vars, ""ocean_time"", reftime)\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_da_model_std(filename, eta_rho=10, xi_rho=10, s_rho=1,\n                        reftime=default_epoch, clobber=False,\n                        cdl=None, title=""My Model STD""):\n    """"""\n    Create an time varying model standard deviation file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    eta_rho: int, optional\n        number of rows in the eta direction\n    xi_rho: int, optional\n        number of columns in the xi direction\n    s_rho: int, optional\n        number of s-levels\n    reftime: datetime, optional\n        date of epoch for time origin in netcdf\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(\n        _cdl_dir + ""s4dvar_std_m.cdl"" if cdl is None else cdl)\n\n    # Fill in the appropriate dimension values\n    dims = _set_grid_dimensions(dims, eta_rho, xi_rho, s_rho)\n    vars = _set_time_ref(vars, ""ocean_time"", reftime)\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_zlevel_grid(filename, lat=10, lon=10, depth=1,\n                       clobber=False, cdl=None,\n                       title=""Zlevel Grid"", dims=2):\n    """"""\n    Create z-level grid file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    lat: int, optional\n        number of latitudinal rows\n    lon: int, optional\n        number of longitudinal columns\n    depth: int, optional\n        number of z-levels\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n    dims: int, optional\n        number of dimensions to use for lat/lon\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    if cdl == None:\n        if dims == 1:\n            cdlfile = _cdl_dir + ""zlevel_1d_grid.cdl""\n        else:\n            cdlfile = _cdl_dir + ""zlevel_2d_grid.cdl""\n    else:\n        cdlfile = cdl\n\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(cdlfile)\n\n    # Fill in the appropriate dimension values\n    dims[""lat""] = lat\n    dims[""lon""] = lon\n    dims[""depth""] = depth\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\ndef create_zlevel(filename, lat=10, lon=10, depth=1,\n                  reftime=default_epoch,\n                  clobber=False, cdl=None,\n                  title=""Zlevel Model Data"", dims=2):\n    """"""\n    Create an time varying model standard deviation file\n\n    Parameters\n    ----------\n    filename : string\n        name and path of file to create\n    lat: int, optional\n        number of latitudinal rows\n    lon: int, optional\n        number of longitudinal columns\n    depth: int, optional\n        number of z-levels\n    reftime: datetime, optional\n        date of epoch for time origin in netcdf\n    clobber: bool, optional\n        If True, clobber any existing files and recreate. If False, use\n        the existing file definition\n    cdl: string, optional,\n        Use the specified CDL file as the definition for the new\n        netCDF file.\n    title: string, optional\n        netcdf attribute title\n    dims: int, optional\n        number of dimensions to use for lat/lon\n\n    Returns\n    -------\n    nc, netCDF4 object\n\n    """"""\n    if cdl == None:\n        if dims == 1:\n            cdlfile = _cdl_dir + ""zlevel_1d.cdl""\n        else:\n            cdlfile = _cdl_dir + ""zlevel_2d.cdl""\n    else:\n        cdlfile = cdl\n\n    # Generate the Structure\n    dims, vars, attr = cdl_parser(cdlfile)\n\n    # Fill in the appropriate dimension values\n    dims[""lat""] = lat\n    dims[""lon""] = lon\n    dims[""depth""] = depth\n    vars = _set_time_ref(vars, ""time"", reftime)\n\n    # Create the file\n    _nc = ncgen(filename, dims=dims, vars=vars, attr=attr, clobber=clobber,\n                title=title)\n\n    # Return the new file\n    return _nc\n\n\nif __name__ == ""__main__"":\n    grid = create_zlevel(""test.nc"")\n'"
seapy/roms/obs.py,109,"b'#!/usr/bin/env python\n""""""\n  obs.py\n\n  State Estimation and Analysis for PYthon\n\n  Module to handle the observation structure within ROMS. The ROMS structure\n  defines the obs_provenance, which is a numeric ID for tracking the source\n  of the observations you use. This module defines a dictionary to translate\n  between the numeric and string representations so that either can be used.\n  Standard instruments are predefined; however, for your own applications,\n  you will want to define additional provenances for the observations you use.\n  This is accomplished via:\n\n  >>> import seapy\n  >>> seapy.roms.obs.obs_provenance.update({353:\'MY_OBS1\', 488:\'MY_OBS2\'})\n\n  You can make your own module for importing seapy and adding your definitions\n  easily.\n\n  Written by Brian Powell on 08/05/14\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\n\n\nimport numpy as np\nimport netCDF4\nimport seapy\nfrom collections import namedtuple\nfrom warnings import warn\n\n# Define a named tuple to store raw data for the gridder\nraw_data = namedtuple(\'raw_data\', \'type provenance values error min_error\')\n\n# Define the observation type\nobs_types = {\n    1: ""ZETA"",\n    2: ""UBAR"",\n    3: ""VBAR"",\n    4: ""U"",\n    5: ""V"",\n    6: ""TEMP"",\n    7: ""SALT"",\n    20: ""RADIAL""\n}\n# Define the observation provenances used within my applications\nobs_provenance = {\n    0: ""UNKNOWN"",\n    100: ""GLIDER"",\n    102: ""GLIDER_SG022"",\n    103: ""GLIDER_SG023"",\n    114: ""GLIDER_SG114"",\n    139: ""GLIDER_SG139"",\n    146: ""GLIDER_SG146"",\n    147: ""GLIDER_SG147"",\n    148: ""GLIDER_SG148"",\n    150: ""GLIDER_SG500"",\n    151: ""GLIDER_SG511"",\n    152: ""GLIDER_SG512"",\n    153: ""GLIDER_SG513"",\n    154: ""GLIDER_SG523"",\n    155: ""GLIDER_SG626"",\n    200: ""CTD"",\n    210: ""CTD_HOT"",\n    220: ""CTD_ARGO"",\n    300: ""SST"",\n    301: ""SST_OSTIA"",\n    315: ""SST_NAVO_MAP"",\n    317: ""SST_AVHRR_17"",\n    318: ""SST_AVHRR_18"",\n    330: ""SST_MODIS_AQUA"",\n    331: ""SST_MODIS_TERRA"",\n    332: ""SST_VIIRS"",\n    340: ""SST_REMSS"",\n    341: ""SST_AMSRE"",\n    342: ""SST_TMI"",\n    400: ""SSH"",\n    410: ""SSH_AVISO_MAP"",\n    411: ""SSH_AVISO_TOPEX_POSEIDON"",\n    412: ""SSH_AVISO_JASON1"",\n    413: ""SSH_AVISO_JASON2"",\n    414: ""SSH_AVISO_JASON3"",\n    420: ""SSH_AVISO_GFO"",\n    421: ""SSH_AVISO_ENVISAT"",\n    422: ""SSH_AVISO_ERS1"",\n    423: ""SSH_AVISO_ERS2"",\n    430: ""SSH_AVISO_ALTIKA"",\n    431: ""SSH_AVISO_CRYOSAT2"",\n    432: ""SSH_AVISO_HAIYANG"",\n    433: ""SSH_AVISO_SENTINEL3A"",\n    450: ""SSH_HYCOM"",\n    460: ""SSS_AQUARIUS"",\n    500: ""DRIFTERS"",\n    600: ""RADAR"",\n    610: ""RADAR_KOK"",\n    620: ""RADAR_KAK"",\n    630: ""RADAR_KAL"",\n    640: ""RADAR_KAP"",\n    650: ""RADAR_KNA"",\n    660: ""RADAR_KKH"",\n    670: ""RADAR_PPK"",\n    700: ""ADCP"",\n    800: ""MOORING"",\n    810: ""TAO_ARRAY""\n}\n\n\ndef _type_from_string(s):\n    """"""\n    PRIVATE method: Search the type dictionary for the key of the\n    given the value. If the key isn\'t a string or properly resolves, try to\n    return the value as such\n    """"""\n    try:\n        return list(obs_types.keys())[\n            list(obs_types.values()).index(s.upper())]\n    except AttributeError:\n        return int(s)\n\n\ndef _provenance_from_string(s):\n    """"""\n    PRIVATE method: Search the provenance dictionary for the key of the\n    given the value. If the key isn\'t a string or properly resolves, try to\n    return the value as such\n    """"""\n    try:\n        return list(obs_provenance.keys())[\n            list(obs_provenance.values()).index(s.upper())]\n    except AttributeError:\n        return int(s)\n\n\ndef asobs(obs):\n    """"""\n    Return the input as an observation array if possible. If the parameter\n    is already an observation, just return; otherwise, create a new class.\n\n    Parameters\n    ----------\n    obs: obs class, string, or list\n        what to cast as observation\n\n    Output\n    ------\n    obs: seapy.roms.obs.obs\n    """"""\n    if obs is None:\n        raise AttributeError(""No obs were specified"")\n    if isinstance(obs, seapy.roms.obs.obs):\n        return obs\n    else:\n        return seapy.roms.obs.obs(filename=obs)\n\n\ndef astype(otype):\n    """"""\n    Return the integer type of the given observation array.\n\n    Input\n    -----\n    type: ndarray,\n        List of types to put into integer form\n\n    Output\n    ------\n    types: array,\n        List of integer types\n    """"""\n    otype = np.atleast_1d(otype)\n    if otype.dtype.type == np.str_:\n        return np.array([_type_from_string(s) for s in otype])\n    else:\n        return otype\n\n\ndef asprovenance(prov):\n    """"""\n    Return the integer provenance of the given provenance array.\n\n    Input\n    -----\n    prov: array,\n        List of provenances to put into integer form\n\n    Output\n    ------\n    provs: ndarray,\n        List of integer provenances\n    """"""\n    prov = np.atleast_1d(prov)\n    if prov.dtype.type == np.str_:\n        return np.array([_provenance_from_string(s) for s in prov])\n    else:\n        return prov\n\n\nclass obs:\n\n    def __init__(self, filename=None, time=None, x=None, y=None, z=None,\n                 lat=None, lon=None, depth=None, value=None, error=None,\n                 type=None, provenance=None, meta=None,\n                 title=""ROMS Observations""):\n        """"""\n        Class to deal with ROMS observations for data assimilation\n\n        Parameters\n        ----------\n        filename : string or list, optional,\n            if filename is given, the data are loaded from a netcdf file\n        time : ndarray, optional,\n          time of observation in days\n        x : ndarray, optional,\n          obs location on grid in x (eta)\n        y : ndarray, optional,\n          obs location on grid in y (xi)\n        z : ndarray, optional,\n          obs location on grid in z (positive layers or negative depth [m])\n        lat : ndarray, optional,\n          obs true latitude [deg]\n        lon : ndarray, optional,\n          obs true longitude [deg]\n        depth : ndarray, optional,\n          obs true depth [m]\n        value : ndarray, optional,\n          obs value [units]\n        error : ndarray, optional,\n          obs error [units**2]\n        type : ndarray, optional,\n          obs type [1-zeta, 2-ubar, 3-vbar, 4-u, 5-v, 6-temp, 7-salt]\n        provenance : ndarray, optional,\n          obs provenance\n        meta : ndarray, optional,\n          obs additional information\n        """"""\n        self.title = title\n        if filename is not None:\n            nc = seapy.netcdf(filename)\n            # Construct an array from the data in the file. If obs_meta\n            # exists in the file, then load it; otherwise, fill with zeros\n            self.filename = filename\n            self.time = nc.variables[""obs_time""][:]\n            self.x = nc.variables[""obs_Xgrid""][:]\n            self.y = nc.variables[""obs_Ygrid""][:]\n            self.z = nc.variables[""obs_Zgrid""][:]\n            self.lat = nc.variables[""obs_lat""][:]\n            self.lon = nc.variables[""obs_lon""][:]\n            self.depth = nc.variables[""obs_depth""][:]\n            self.value = nc.variables[""obs_value""][:]\n            self.error = nc.variables[""obs_error""][:]\n            self.type = nc.variables[""obs_type""][:]\n            self.provenance = nc.variables[""obs_provenance""][:]\n            # Update the provenance definitions\n            try:\n                obs_provenance.update(dict((int(k.strip()), v.strip())\n                                           for v, k in\n                                           (it.split(\':\') for it in\n                                            nc.obs_provenance.split(\',\'))))\n            except (AttributeError, ValueError):\n                pass\n            try:\n                self.meta = nc.variables[""obs_meta""][:]\n            except KeyError:\n                self.meta = np.zeros(self.value.size)\n            finally:\n                nc.close()\n        else:\n            self.filename = None\n            if time is not None:\n                self.time = np.atleast_1d(time)\n            if x is not None:\n                self.x = np.atleast_1d(x)\n            if y is not None:\n                self.y = np.atleast_1d(y)\n            if z is not None:\n                self.z = np.atleast_1d(z)\n            if lat is not None:\n                self.lat = np.atleast_1d(lat)\n            if lon is not None:\n                self.lon = np.atleast_1d(lon)\n            if depth is not None:\n                self.depth = np.atleast_1d(depth)\n            if value is not None:\n                self.value = np.atleast_1d(value)\n            if error is not None:\n                self.error = np.atleast_1d(error)\n            if type is not None:\n                self.type = astype(type)\n            if provenance is not None:\n                self.provenance = asprovenance(provenance)\n            else:\n                self.provenance = 0\n            if meta is not None:\n                self.meta = np.atleast_1d(meta)\n            self._consistent()\n\n    def _consistent(self):\n        """"""\n        PRIVATE method: try to make the structure self-consistent. Throw\n        an exception if not possible.\n        """"""\n        # Make sure required arrays are a 1d array\n        self.time = self.time.ravel()\n        self.x = self.x.ravel()\n        self.y = self.y.ravel()\n        self.value = self.value.ravel()\n        self.error = self.error.ravel()\n        self.type = astype(self.type.ravel())\n\n        lt = self.time.size\n        if not lt == self.x.size == self.y.size == \\\n                self.value.size == self.error.size == self.type.size:\n            # If these lengths are not equal, then there is a serious issue\n            raise ValueError(\n                ""Lengths of observation attributes are not equal."")\n        else:\n            # For the others, we can pad the information to ensure\n            # consistency\n            def _resizearr(key, n):\n                arr = getattr(self, key, np.zeros(n))\n                if arr.size == n:\n                    return arr\n                return np.resize(arr, n)\n\n            self.z = _resizearr(\'z\', lt)\n            self.lat = _resizearr(\'lat\', lt)\n            self.lon = _resizearr(\'lon\', lt)\n            self.depth = _resizearr(\'depth\', lt)\n            self.provenance = asprovenance(_resizearr(\'provenance\', lt))\n            self.meta = _resizearr(\'meta\', lt)\n\n        # Eliminate bad values\n        good_vals = np.logical_and.reduce((\n            np.isfinite(self.value),\n            np.isfinite(self.x),\n            np.isfinite(self.y),\n            np.isfinite(self.error),\n            np.isfinite(self.time)))\n        if np.any(~good_vals):\n            self.delete(np.where(good_vals == False))\n\n        # Set the shape parameter\n        self.shape = self.value.shape\n\n        # Ensure consistency in depth and z\n        self.z[self.depth > 0] = self.depth[self.depth > 0]\n\n    def __len__(self):\n        self.shape = self.value.shape\n        return self.value.size\n\n    def __getitem__(self, l):\n        return obs(time=self.time[l], x=self.x[l], y=self.y[l],\n                   z=self.z[l], lon=self.lon[l], lat=self.lat[l],\n                   depth=self.depth[l], value=self.value[l],\n                   error=self.error[l], type=self.type[l],\n                   provenance=self.provenance[l], meta=self.meta[l])\n\n    def __setitem__(self, l, new_obs):\n        if not isinstance(new_obs, seapy.roms.obs.obs):\n            raise TypeError(""Trying to assign obs to a non-obs type."")\n\n        self.time[l] = new_obs.time\n        self.x[l] = new_obs.x\n        self.y[l] = new_obs.y\n        self.z[l] = new_obs.z\n        self.lon[l] = new_obs.lon\n        self.lat[l] = new_obs.lat\n        self.depth[l] = new_obs.depth\n        self.value[l] = new_obs.value\n        self.error[l] = new_obs.error\n        self.type[l] = new_obs.type\n        self.provenance[l] = new_obs.provenance\n        self.meta[l] = new_obs.meta\n        self._consistent()\n\n    def __repr__(self):\n        return ""< {:d} obs: {:.1f} to {:.1f} >"".format(self.value.size,\n                                                       np.min(self.time), np.max(self.time))\n\n    def __str__(self):\n        return ""\\n"".join([repr(self), ""\\n"".join(\n            ""{:.3f}, [{:s}:{:s}] ({:.2f},{:.2f},{:.2f}) = {:.4f} +/- {:.4f}"".format(\n                t, obs_types[self.type[n]],\n                obs_provenance.get(self.provenance[n], ""UNKNOWN""),\n                self.lon[n], self.lat[n], self.depth[n],\n                self.value[n], self.error[n])\n            for n, t in enumerate(self.time))])\n\n    def add(self, new_obs):\n        """"""\n        Add another class of obs into this one\n\n        Parameters\n        ----------\n        new_obs : obs,\n            Class of obs to append to the existing\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Load a list from netcdf, then append a new set of values\n\n        >>> a=obs(""test.nc"")\n        >>> b=obs(time=4,x=3.2,y=2.8,z=0,value=23.44,error=0.5,type=""temp"",\n        >>>       provenance=""glider"")\n        >>> a.add(b)\n\n        """"""\n        self._consistent()\n        new_obs._consistent()\n        self.time = np.append(self.time, new_obs.time)\n        self.x = np.append(self.x, new_obs.x)\n        self.y = np.append(self.y, new_obs.y)\n        self.z = np.append(self.z, new_obs.z)\n        self.lat = np.append(self.lat, new_obs.lat)\n        self.lon = np.append(self.lon, new_obs.lon)\n        self.depth = np.append(self.depth, new_obs.depth)\n        self.value = np.append(self.value, new_obs.value)\n        self.error = np.append(self.error, new_obs.error)\n        self.type = np.append(self.type, new_obs.type)\n        self.provenance = np.append(self.provenance, new_obs.provenance)\n        self.meta = np.append(self.meta, new_obs.meta)\n\n    def copy(self):\n        """"""\n        deep copy this class and return the new copy.\n\n        Returns\n        -------\n        obs : obs,\n            deep copy of the class\n        """"""\n        import copy\n        return copy.deepcopy(self)\n\n    def delete(self, obj):\n        """"""\n        delete observations from the record.\n\n        Parameters\n        ----------\n        obj : slice, int or array of ints\n            Indicate which sub-arrays to remove.\n\n        Returns\n        -------\n        Nothing: updates the class arrays\n\n        Examples\n        --------\n        Delete every other observation\n        >>> myobs.delete(np.s_[::2])\n        """"""\n        self.time = np.delete(self.time, obj)\n        self.x = np.delete(self.x, obj)\n        self.y = np.delete(self.y, obj)\n        self.z = np.delete(self.z, obj)\n        self.lat = np.delete(self.lat, obj)\n        self.lon = np.delete(self.lon, obj)\n        self.depth = np.delete(self.depth, obj)\n        self.value = np.delete(self.value, obj)\n        self.error = np.delete(self.error, obj)\n        self.type = np.delete(self.type, obj)\n        self.provenance = np.delete(self.provenance, obj)\n        self.meta = np.delete(self.meta, obj)\n\n    def create_survey(self, dt=0):\n        """"""\n        Build the survey structure from the observations\n        """"""\n        # Generate the sort list\n        self.sort = np.argsort(self.time, kind=\'mergesort\')\n\n        # Build the survey structure\n        times, counts = np.unique(self.time[self.sort], return_counts=True)\n\n        # Make sure everything is within dt\n        if dt:\n            delta = np.diff(times)\n            while np.any(delta < dt):\n                idx = np.argmin(delta)\n                self.time[self.time == times[idx + 1]] = times[idx]\n                times[idx + 1] = times[idx]\n                times = np.unique(times)\n                delta = np.diff(times)\n\n            # Re-sort the surveys\n            times, counts = np.unique(self.time[self.sort], return_counts=True)\n\n        self.survey_time = times\n        self.nobs = counts\n\n    def to_netcdf(self, filename=None, dt=0, clobber=True):\n        """"""\n        Write out the observations into the specified netcdf file\n\n        Parameters\n        ----------\n        filename : string, optional\n            name of file to save. If obs were loaded from a file and filename\n            is not specified, then write to the same.\n        dt : float,\n            ensure data are at least separated in time by dt; otherwise,\n            make as part of same survey\n        clobber : bool, optional\n            if True, any existing file is overwritten\n        """"""\n        import os\n\n        # Check filename\n        if filename is None and self.filename is not None:\n            filename = self.filename\n        if filename is None:\n            error(""No filename given"")\n\n        # Save out the observations by survey\n        self._consistent()\n        self.create_survey(dt)\n        if not self.value.size:\n            warn(\n                ""No observations are available to be written to {:s}"".format(filename))\n            return None\n\n        if not clobber and os.path.exists(filename):\n            warn(""{:s} exists with no clobber."".format(filename))\n            return None\n\n        state_vars = np.maximum(7, np.max(self.type))\n        nc = seapy.roms.ncgen.create_da_obs(filename,\n                                            survey=self.survey_time.size,\n                                            state_variable=state_vars,\n                                            provenance=\',\'.join((\':\'.join(\n                                                (obs_provenance.get(v, ""UNKNOWN""), str(v)))\n                                                for v in np.unique(self.provenance))),\n                                            clobber=True, title=self.title)\n        nc.variables[""spherical""][:] = 1\n        nc.variables[""Nobs""][:] = self.nobs\n        nc.variables[""survey_time""][:] = self.survey_time\n        nc.variables[""obs_variance""][:] = np.ones(state_vars) * 0.1\n        nc.variables[""obs_time""][:] = self.time[self.sort]\n        nc.variables[""obs_Xgrid""][:] = self.x[self.sort]\n        nc.variables[""obs_Ygrid""][:] = self.y[self.sort]\n        nc.variables[""obs_Zgrid""][:] = self.z[self.sort]\n        nc.variables[""obs_lat""][:] = self.lat[self.sort]\n        nc.variables[""obs_lon""][:] = self.lon[self.sort]\n        nc.variables[""obs_depth""][:] = self.depth[self.sort]\n        nc.variables[""obs_value""][:] = self.value[self.sort]\n        nc.variables[""obs_error""][:] = self.error[self.sort]\n        nc.variables[""obs_type""][:] = self.type[self.sort]\n        nc.variables[""obs_provenance""][:] = self.provenance[self.sort]\n        nc.variables[""obs_meta""][:] = self.meta[self.sort]\n        nc.close()\n\n\ndef gridder(grid, time, lon, lat, depth, data, dt, depth_adjust=False,\n            title=\'ROMS Observations\'):\n    """"""\n    Construct an observations set from raw observations by placing them\n    onto a grid.\n\n    Parameters\n    ----------\n    grid : seapy.model.grid or filename string,\n        Grid to place the raw observations onto\n    time : ndarray,\n        Time of the observations. This can be a scalar and all values\n        will be assigned to the single time; otherwise, there must be a\n        corresponding time to each value in the data.\n    lon : ndarray,\n        longitude of the observations. This can be a scalar and all values\n        will be assigned to the single location; otherwise, there must be a\n        corresponding longitude to each value in the data.\n    lat : ndarray,\n        latitude of the observations. This can be a scalar and all values\n        will be assigned to the single location; otherwise, there must be a\n        corresponding latitude to each value in the data.\n    depth : ndarray or None,\n        depth of the observations. If None, then all values are placed on\n        the surface; otherwise, must be a corresponding depth for each\n        value in the data.\n    data : list of named tuples of seapy.roms.obs.raw_data,\n        This list is comprised of each set of observation data types that\n        are to be gridded together. If there is only one type (e.g.,\n        SSH observations, there is only one item). An Argo float would have\n        two items in the list (temperature and salinity observations).\n        The list is comprised of named tuples of the raw observations\n        with the following fields:\n            ""type"" : string (or integer) of the type from\n                     seapy.roms.obs.obs_types\n             ""provenance""  : string (or integer) of the type from\n                             seapy.roms.obs.obs_provenance\n            ""values"" : ndarray of actual observed values in units\n                       for type\n            ""error"" : ndarray (or None) of individual observational\n                      uncertainty (same units of values). If not known,\n                      use None\n            ""min_error"" : float of the minimum error that should be\n                          prescribed to the observations (typically,\n                          the instrument error) in the same units of\n                          values.\n    dt : float\n        The bin size of time for observations to be considered at the\n        same time. The units must be the same as the provided time.\n    title : string, optional,\n        Title to assign the observations structure for output\n\n    Returns\n    -------\n    obs : seapy.obs class\n        Resulting observations from the raw data as placed onto grid.\n\n    Examples\n    --------\n    A profile of temp and salt observations at a given lat/lon:\n\n    >>> obs = seapy.obs.gridder(grid, times, lon, lat,\n            [ seapy.roms.obs.raw_data(""TEMP"", ""CTD_ARGO"", temp, None, 0.1),\n              seapy.roms.obs.raw_data(""SALT"", ""CTD_ARGO"", salt, None, 0.05)],\n            dt = 1/24, title=""Argo"")\n\n    Satellite Data from a number of lat/lons at a single time\n\n    >>> obs = seapy.obs.gridder(grid, time, lon, lat,\n            seapy.roms.obs.raw_data(""ZETA"", ""SSH_AVISO"", sla, sla_err, 0.05),\n            dt = 2/24, title=""SSH"")\n\n    These will generate new observation structures from the raw data.\n    """"""\n    from numpy_groupies import aggregate\n\n    # Make sure the input is of the proper form\n    grid = seapy.model.asgrid(grid)\n    time = np.atleast_1d(time)\n    lon = np.atleast_1d(lon)\n    lat = np.atleast_1d(lat)\n\n    # First, before relying on gridding, extract only the data that are\n    # encompassed by the grid\n    region_list = np.where(np.logical_and.reduce((\n        lat >= np.min(grid.lat_rho), lat <= np.max(grid.lat_rho),\n        lon >= np.min(grid.lon_rho), lon <= np.max(grid.lon_rho))))\n    if not np.any(region_list):\n        warn(""No observations were located within grid region_list"")\n        return None\n    lat = lat[region_list]\n    lon = lon[region_list]\n\n    # Get the appropriate k-dimension depending on whether the data\n    # are 2-D or 3-D\n    if depth is None:\n        # Get the grid locations from the data locations\n        subsurface_values = False\n        (j, i) = grid.ij((lon, lat))\n        depth = grid.n * np.ones(i.size)\n        k = np.ma.array(np.resize(grid.n, i.size))\n    else:\n        # Get the grid locations from the data locations\n        subsurface_values = True\n        depth = np.atleast_1d(depth)[region_list]\n        (k, j, i) = grid.ijk((lon, lat, depth), depth_adjust)\n\n    # Sub-select only the points that lie on our grid\n    valid_list = np.where((~i.mask * ~j.mask * ~k.mask) == True)\n    i = i[valid_list].compressed()\n    j = j[valid_list].compressed()\n    k = k[valid_list].compressed()\n    depth = depth[valid_list]\n\n    # Make sure the times are consistent and in dt-space\n    if time.size == 1:\n        time = np.resize(time, valid_list[0].size)\n    else:\n        time = time[region_list][valid_list]\n    dtime = np.floor(time / dt)\n\n    # Loop over all time intervals putting everything together. NOTE: The\n    # preference is to use aggregate over the time-dimension just as we do\n    # in the spatial-dimension; however, this led to crashing.\n    ot = list()\n    ox = list()\n    oy = list()\n    oz = list()\n    odep = list()\n    olon = list()\n    olat = list()\n    oval = list()\n    oerr = list()\n    oprov = list()\n    otype = list()\n    for t in seapy.progressbar.progress(np.unique(dtime)):\n        time_list = np.where(dtime == t)\n        mtime = np.nanmean(time[time_list])\n\n        for v in data:\n            valid_data = np.s_[:]\n            if isinstance(v.values, np.ma.core.MaskedArray):\n                valid_data = \\\n                    (v.values[region_list][valid_list][time_list].nonzero())[0]\n                if not valid_data.size:\n                    continue\n\n            # Put together the indices based on the type of data we have\n            if subsurface_values:\n                idx = (k[time_list][valid_data],\n                       j[time_list][valid_data],\n                       i[time_list][valid_data])\n            else:\n                idx = (j[time_list][valid_data],\n                       i[time_list][valid_data])\n            indices = np.floor(idx).astype(int)\n\n            # Grid the data onto our grid and compute the mean and variance\n            ii = aggregate(indices, i[time_list][valid_data], func=\'mean\')\n            jj = aggregate(indices, j[time_list][valid_data], func=\'mean\')\n            binned = np.where(ii * jj > 0)\n            ii = ii[binned].ravel()\n            jj = jj[binned].ravel()\n            (latl, lonl) = grid.latlon((ii, jj))\n            Nd = ii.size\n\n            # Put the co-located values together\n            nvalues = aggregate(indices,\n                                v.values[region_list][valid_list][\n                                    time_list][valid_data],\n                                func=\'mean\')\n\n            # Get their variance\n            vari = aggregate(indices,\n                             v.values[region_list][valid_list][\n                                 time_list][valid_data],\n                             func=\'var\')\n\n            # Put together the known observation values\n            if v.error is not None:\n                errs = aggregate(indices,\n                                 v.error[region_list][valid_list][\n                                     time_list][valid_data]**2,\n                                 func=\'mean\')\n                errs = errs[binned].flatten()\n            else:\n                errs = 0.0\n\n            # Build the depth vectors\n            if subsurface_values:\n                dd = aggregate(indices, depth[time_list][valid_data],\n                               func=\'mean\')\n                kk = aggregate(indices, k[time_list][valid_data],\n                               func=\'mean\')\n                dd = dd[binned].ravel()\n                # ROMS counts from 1 for depth layers\n                kk = kk[binned].ravel() + 1\n            else:\n                kk = np.resize(grid.n, Nd)\n                dd = kk\n\n            # Put all of the data from this time into our lists\n            ot.append(np.resize(mtime, Nd))\n            ox.append(ii)\n            oy.append(jj)\n            oz.append(kk)\n            odep.append(dd)\n            olon.append(lonl)\n            olat.append(latl)\n            oval.append(nvalues[binned].flatten())\n            otype.append(np.resize(seapy.roms.obs.astype(v.type), Nd))\n            oprov.append(np.resize(\n                seapy.roms.obs.asprovenance(v.provenance), Nd))\n            oerr.append(np.maximum(v.min_error**2,\n                                   np.maximum(vari[binned].flatten(),\n                                              errs)))\n\n    # Make sure that we have something relevant\n    if not oval:\n        return None\n\n    # Put everything together and create an observation class\n    return seapy.roms.obs.obs(time=np.hstack(ot).ravel(),\n                              x=np.hstack(ox).ravel(),\n                              y=np.hstack(oy).ravel(),\n                              z=np.hstack(oz).ravel(),\n                              lat=np.hstack(olat).ravel(),\n                              lon=np.hstack(olon).ravel(),\n                              depth=np.hstack(odep).ravel(),\n                              value=np.hstack(oval).ravel(),\n                              error=np.hstack(oerr).ravel(),\n                              type=np.hstack(otype).ravel(),\n                              provenance=np.hstack(oprov).ravel(),\n                              title=title)\n\n\ndef merge_files(obs_files, out_files, days, dt, limits=None, clobber=True):\n    """"""\n    merge together a group of observation files into combined new files\n    with observations that lie only within the corresponding dates\n\n    Parameters\n    ----------\n    obs_files : list,\n        List of files to merge together (a single file will work, it will\n        just be filtered by the dates)\n    out_files : list or string,\n        list of the filenames to create for each of the output periods.\n        If a single string is given, the character \'#\' will be replaced\n        by the starting time of the observation (e.g. out_files=""out_#.nc""\n        will become out_03234.nc)\n    days : list of tuples,\n        List of starting and ending day numbers for each cycle to process.\n        The first value is the start day, the second is the end day. The\n        number of tuples is the number of files to output.\n    dt : float,\n        Time separation of observations. Observations that are less than\n        dt apart in time will be set to the same time.\n    limits : dict, optional\n        Set the limits of the grid points that observations are allowed\n        within, {\'north\':i, \'south\':i, \'east\':i, \'west\':i }. As obs near\n        the boundaries are not advisable, this allows you to specify the\n        valid grid range to accept obs within.\n    clobber: bool, optional\n        If True, output files are overwritten. If False, they are skipped.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n\n    Put together three files into 5 separate files in two day intervals from\n    day 10 through day 20:\n\n    >>> merge_files([""obs_1.nc"", ""obs_2.nc"", ""obs_3.nc""], ""new_#.nc"",\n                   [(i, i+2) for i in range(10, 20, 2)])\n\n    Put together same three files into 3 overlapping separate files in five\n    day intervals with one overlapping day:\n\n    >>> merge_files([""obs_1.nc"", ""obs_2.nc"", ""obs_3.nc""], ""new_#.nc"",\n                   [(i, i+5) for i in range(10, 20, 4)])\n\n    """"""\n    import re\n    import os\n\n    # Only unique files\n    obs_files = set().union(seapy.flatten(obs_files))\n    outtime = False\n    if isinstance(out_files, str):\n        outtime = True\n        time = re.compile(\'\\#\')\n\n    # Go through the files to determine which periods they cover\n    myobs = list()\n    sdays = list()\n    edays = list()\n    for file in obs_files:\n        nc = seapy.netcdf(file)\n        fdays = nc.variables[\'survey_time\'][:]\n        nc.close()\n        l = np.where(np.logical_and(fdays >= np.min(days),\n                                    fdays <= np.max(days)))[0]\n        if not l.size:\n            continue\n        myobs.append(file)\n        sdays.append(fdays[0])\n        edays.append(fdays[-1])\n    sdays = np.asarray(sdays)\n    edays = np.asarray(edays)\n\n    # Loop over the dates in pairs\n    for n, t in enumerate(seapy.progressbar.progress(days)):\n        # Set output file name\n        if outtime:\n            outfile = time.sub(""{:05d}"".format(t[0]), out_files)\n        else:\n            outfile = out_files[n]\n\n        if os.path.exists(outfile) and not clobber:\n            continue\n\n        # Find the files that cover the current period\n        fidx = np.where(np.logical_and(sdays <= t[1], edays >= t[0]))[0]\n        if not fidx.size:\n            continue\n\n        # Create new observations for this time period\n        nobs = obs(myobs[fidx[0]])\n        l = np.where(np.logical_or(nobs.time < t[0], nobs.time > t[1]))\n        nobs.delete(l)\n        for idx in fidx[1:]:\n            o = obs(myobs[idx])\n            l = np.where(np.logical_and(o.time >= t[0], o.time <= t[1]))\n            nobs.add(o[l])\n        # Remove any limits\n        if limits is not None:\n            l = np.where(np.logical_or.reduce((\n                nobs.x < limits[\'west\'],\n                nobs.x > limits[\'east\'],\n                nobs.y < limits[\'south\'],\n                nobs.y > limits[\'north\'])))\n            nobs.delete(l)\n\n        # Save out the new observations\n        nobs.to_netcdf(outfile, dt=dt)\n\n        pass\n'"
seapy/roms/obsgen.py,123,"b'#!/usr/bin/env python\n""""""\n  obsgen.py\n\n  State Estimation and Analysis for PYthon\n\n  Module to process observations:\n    obsgen : class to convert from raw to ROMS observations using\n             specific subclasses\n\n  Written by Brian Powell on 08/15/15\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\n\n\nimport numpy as np\nimport netCDF4\nimport h5py\nimport seapy\nimport datetime\nfrom warnings import warn\n\n\ndef error_profile(obs, depth, error, provenance=None):\n    """"""\n    Apply a vertical error profile to a given observation structure.\n    This allows for error minimums to vary by depth and observation\n    type.\n\n    Parameters\n    ----------\n    obs : seapy.roms.obs.obs or string,\n      The observations to enforce the error profile upon.\n    depth : ndarray,\n      Array of depths for the errors provided\n    error : dict,\n      Dictionary of the errors, where the key is the type of observation\n      (as defined by seapy.roms.obs.obs_types) and the value is\n      an ndarray of same length as depth with the error [in squared units]\n      of the observation profile.\n    provenance : list of int or string, optional,\n      The provenance to apply the errors to (ignore other observations\n      of the same type, but different instrument)\n\n    Returns\n    -------\n    None:\n      The obs structure is mutable is changed in place\n\n    Examples\n    --------\n    >>> obs = obs(\'observation_file.nc\')\n    >>> depth = [10, 30, 50, 1000, 2000]\n    >>> error[\'temp\'] = [0.5, 0.2, 0.4, 0.1, 0.01]\n    >>> error_profile(obs, depth, error)\n\n    The resulting \'obs\' class will have had its error profiles\n    modified.\n    """"""\n    from scipy.interpolate import interp1d\n    obs = seapy.roms.obs.asobs(obs)\n    depth = np.atleast_1d(depth).flatten()\n    depth = np.abs(depth)\n    pro = seapy.roms.obs.asprovenance(provenance) if provenance else None\n\n    # Loop over all of the profiles in the error dictionary and\n    # apply them to the observations\n    for var in error:\n        typ = seapy.roms.obs.astype(var)\n        try:\n            fint = interp1d(depth, error[var].flatten(), copy=False)\n            if pro.any():\n                l = np.where(np.logical_and(obs.type == typ,\n                                            np.in1d(obs.provenance, pro)))\n            else:\n                l = np.where(np.logical_and(obs.type == typ, obs.depth < 0))\n            nerr = fint(np.abs(obs.depth[l]))\n            obs.error[l] = np.maximum(obs.error[l], nerr)\n        except ValueError:\n            warn(""Error for {:s} is the wrong size"".format(var))\n            continue\n    pass\n\n\ndef add_ssh_tides(obs, tide_file, tide_error, tide_start=None, provenance=None,\n                  reftime=seapy.default_epoch):\n    """"""\n    Apply predicted barotropic tides to the SSH values of given observations\n    using the tide_file given.\n\n    Parameters\n    ----------\n    obs : seapy.roms.obs.obs or string,\n      The observations to enforce the error profile upon.\n    tide_file : string,\n      The name of the ROMS tidal forcing file to use for predicting the\n      barotropic tides.\n    tide_error : np.masked_array\n      A two dimensional array of the tidal fit errors to apply to\n      the ssh errors when adding the tides. This should be the same size\n      as the rho-grid. The units of the error must be in meters. If it is\n      masked, the mask will be honored and obs that are in the mask will\n      be removed. This allows you to filter on regions of high error.\n    tide_start : bool, optional,\n      If given, the tide_start of the tide file. If not specified,\n      will read the attribute of the tidal forcing file\n    provenance : list of int or string, optional,\n      The provenance to apply the tides to (ignore other observations\n      of the same type, but different instrument)\n    reftime: datetime,\n      Reference time for the observation times\n\n    Returns\n    -------\n    None:\n      The obs structure is mutable is changed in place\n\n    Examples\n    --------\n    >>> obs = obs(\'observation_file.nc\')\n    >>> add_ssh_tides(obs, \'tide_frc.nc\', errmap)\n\n    The resulting \'obs\' variable will have modified data. To save it:\n\n    >>> obs.to_netcdf()\n    """"""\n\n    # Load tidal file data\n    frc = seapy.roms.tide.load_forcing(tide_file)\n    if not tide_start:\n        tide_start = frc[\'tide_start\']\n\n    # Make sure that the sizes are the same\n    if frc[\'Eamp\'].shape[1:] != tide_error.shape:\n        raise ValueError(\n            ""The error array is not the same size as the tidal grid"")\n\n    # Gather the observations that need tidal information\n    obs = seapy.roms.obs.asobs(obs)\n    pro = seapy.roms.obs.asprovenance(provenance) if provenance else None\n    if pro:\n        l = np.where(np.logical_and(obs.type == 1,\n                                    np.in1d(obs.provenance, pro)))\n    else:\n        l = np.where(obs.type == 1)\n\n    # If we have any, then do tidal predictions and add the signal\n    # and error to the observations\n    bad = []\n    if l[0].any():\n        ox = np.rint(obs.x[l]).astype(int)\n        oy = np.rint(obs.y[l]).astype(int)\n        idx = seapy.unique_rows((ox, oy))\n        for cur in seapy.progressbar.progress(idx):\n            pts = np.where(np.logical_and(ox == ox[cur], oy == oy[cur]))\n            # If this point is masked, remove from the observations\n            if not tide_error[oy[cur], ox[cur]]:\n                bad.append(l[0][pts].tolist())\n            else:\n                time = [reftime + datetime.timedelta(t) for t in\n                        obs.time[l][pts]]\n                amppha = seapy.tide.pack_amp_phase(\n                    frc[\'tides\'], frc[\'Eamp\'][:, oy[cur], ox[cur]],\n                    frc[\'Ephase\'][:, oy[cur], ox[cur]])\n                zpred = seapy.tide.predict(time, amppha,\n                                           lat=obs.lat[l][cur],\n                                           tide_start=tide_start)\n                # Add the information to the observations\n                obs.value[l[0][pts]] += zpred\n                obs.error[l[0][pts]] = np.maximum(\n                    obs.error[l[0][pts]], tide_error[oy[cur], ox[cur]]**2)\n\n    # If any were bad, then remove them\n    if bad:\n        obs.delete(seapy.flatten(bad))\n    pass\n\n\nclass obsgen(object):\n\n    def __init__(self, grid, dt, reftime=seapy.default_epoch):\n        """"""\n        class for abstracting the processing of raw observation files\n        (satellite, in situ, etc.) into ROMS observations files. All\n        processing has commonalities which this class encapsulates, while\n        leaving the loading and translation of individual data formats\n        to subclasses.\n\n        Parameters\n        ----------\n        grid: seapy.model.grid or string,\n            grid to use for generating observations\n        dt: float,\n            Model time-step or greater in units of days\n        epoch: datetime, optional,\n            Time to reference all observations from\n\n        Returns\n        -------\n        None\n\n        """"""\n        self.grid = seapy.model.asgrid(grid)\n        self.dt = dt\n        self.epoch = reftime\n\n    def convert_file(self, file, title=None):\n        """"""\n        convert a raw observation file into a ROMS observations structure.\n        The subclasses are responsible for the conversion, and this method\n        is obsgen is only a stub.\n\n        Parameters\n        ----------\n        file : string,\n            filename of the file to process\n        title : string,\n            Title to give the new observation structure global attribute\n\n        Returns\n        -------\n        seapy.roms.obs.obs,\n            observation structure from raw obs\n        """"""\n        pass\n\n    def datespan_file(self, file):\n        """"""\n        check the given file and return the date span of data that are\n        covered by the file.\n\n        Parameters\n        ----------\n        file : string,\n            filename of the file to process\n\n        Returns\n        -------\n        start : datetime\n            starting date and time of the data\n        end : datetime\n            ending date and time of the data\n        """"""\n\n        return None, None\n\n    def batch_files(self, in_files, out_files, start_time=None,\n                    end_time=None, clobber=True):\n        """"""\n        Given a list of input files, process each one and save each result\n        into the given output file.\n\n        Parameters\n        ----------\n        in_files : list of strings,\n            filenames of the files to process\n        out_files : list of strings,\n            filenames of the files to create for each of the input filenames.\n            If a single string is given, the character \'#\' will be replaced\n            by the starting time of the observation (e.g. out_files=""out_#.nc""\n            will become out_03234.nc)\n        start_time : datetime, optional\n            starting date and time for data to process (ignore files that are\n            outside of the time period)\n        end_time : datetime, optional\n            ending date and time for data to process (ignore files that are\n            outside of the time period). If start_time is provided, and\n            end_time is not, then a period of one day is assumed.\n        clobber : bool, optional\n            If TRUE, overwrite any existing output files. If False, the\n            file is given a letter suffix.\n\n        Returns\n        -------\n        None\n        """"""\n        import re\n        import os\n\n        datecheck = False\n        if start_time is not None:\n            datecheck = True\n            if end_time is None:\n                end_time = start_time + datetime.timedelta(1)\n\n        outtime = False\n        if isinstance(out_files, str):\n            outtime = True\n            time = re.compile(\'\\#\')\n\n        for n, file in enumerate(in_files):\n            try:\n                # Check the times if user requested\n                print(file, end="""")\n                if datecheck:\n                    st, en = self.datespan_file(file)\n                    if (en is not None and en < start_time) or \\\n                            (st is not None and st > end_time):\n                        print("": SKIPPED"")\n                        continue\n\n                # Convert the file\n                obs = self.convert_file(file)\n                if obs is None:\n                    print("": NO OBS"")\n                    continue\n\n                # Output the obs to the correct file\n                if outtime:\n                    ofile = time.sub(""{:05d}"".format(int(obs.time[0])),\n                                     out_files)\n                else:\n                    ofile = out_files[n]\n\n                if clobber:\n                    obs.to_netcdf(ofile, True)\n                else:\n                    for i in ""abcdefgh"":\n                        if os.path.isfile(ofile):\n                            ofile = re.sub(""[a-h]{0,1}\\.nc"", i + "".nc"", ofile)\n                        else:\n                            break\n                    obs.to_netcdf(ofile, False)\n                print("": SAVED"")\n\n            except (BaseException, UserWarning) as e:\n                warn(""WARNING: {:s} cannot be processed.\\nError: {:}"".format(\n                    file, e.args))\n        pass\n\n\n##############################################################################\n#\n# REMOTE-SENSING DATA\n#\n##############################################################################\n\nclass aquarius_sss(obsgen):\n    """"""\n    class to process Aquarius SSS HDF5 files into ROMS observation\n    files. This is a subclass of seapy.roms.genobs.genobs, and handles\n    the loading of the data.\n    """"""\n\n    def __init__(self, grid, dt, reftime=seapy.default_epoch, salt_limits=None,\n                 salt_error=0.2):\n        if salt_limits is None:\n            self.salt_limits = (10, 36)\n        else:\n            self.salt_limits = salt_limits\n        self.salt_error = salt_error\n        super().__init__(grid, dt, reftime)\n\n    def datespan_file(self, file):\n        f = h5py.File(file, \'r\')\n        try:\n            year = f.attrs[\'Period End Year\']\n            day = f.attrs[\'Period End Day\']\n            st = datetime.datetime(year, 1, 1) + datetime.timedelta(int(day))\n            en = st + datetime.timedelta(1)\n        except:\n            st = en = None\n            pass\n        finally:\n            f.close()\n            return st, en\n\n    def convert_file(self, file, title=""AQUARIUS Obs""):\n        """"""\n        Load an Aquarius file and convert into an obs structure\n        """"""\n        f = h5py.File(file, \'r\')\n        salt = np.ma.masked_equal(np.flipud(f[\'l3m_data\'][:]),\n                                  f[\'l3m_data\'].attrs[\'_FillValue\'])\n        year = f.attrs[\'Period End Year\']\n        day = f.attrs[\'Period End Day\']\n        nlat = f.attrs[\'Northernmost Latitude\'] - 0.5\n        slat = f.attrs[\'Southernmost Latitude\'] + 0.5\n        wlon = f.attrs[\'Westernmost Longitude\'] + 0.5\n        elon = f.attrs[\'Easternmost Longitude\'] - 0.5\n        dlat = f.attrs[\'Latitude Step\']\n        dlon = f.attrs[\'Longitude Step\']\n        f.close()\n\n        [lon, lat] = np.meshgrid(np.arange(wlon, elon + dlon, dlon),\n                                 np.arange(slat, nlat + dlat, dlat))\n        time = (datetime.datetime(year, 1, 1) + datetime.timedelta(int(day)) -\n                self.epoch).days\n        lat = lat.flatten()\n        lon = lon.flatten()\n        if self.grid.east():\n            lon[lon < 0] += 360\n\n        salt = np.ma.masked_outside(salt.flatten(), self.salt_limits[0],\n                                    self.salt_limits[1])\n        data = [seapy.roms.obs.raw_data(""SALT"", ""SSS_AQUARIUS"",\n                                        salt, None, self.salt_error)]\n        # Grid it\n        return seapy.roms.obs.gridder(self.grid, time, lon, lat, None,\n                                      data, self.dt, title)\n        pass\n\n\nclass aviso_sla_map(obsgen):\n    """"""\n    class to process AVISO SLA map netcdf files into ROMS observation\n    files. This is a subclass of seapy.roms.genobs.genobs, and handles\n    the loading of the data.\n    """"""\n\n    def __init__(self, grid, dt, reftime=seapy.default_epoch, ssh_mean=None,\n                 ssh_error=0.05):\n        if ssh_mean is not None:\n            self.ssh_mean = seapy.convolve_mask(ssh_mean, ksize=5, copy=True)\n        else:\n            self.ssh_mean = None\n        self.ssh_error = ssh_error\n        super().__init__(grid, dt, reftime)\n\n    def datespan_file(self, file):\n        nc = seapy.netcdf(file)\n        try:\n            st = datetime.datetime.strptime(nc.getncattr(""time_coverage_start""),\n                                            ""%Y-%m-%dT%H:%M:%SZ"")\n            en = datetime.datetime.strptime(nc.getncattr(""time_coverage_end""),\n                                            ""%Y-%m-%dT%H:%M:%SZ"")\n        except:\n            st = en = None\n            pass\n        finally:\n            nc.close()\n            return st, en\n\n    def convert_file(self, file, title=""AVISO Obs""):\n        """"""\n        Load an AVISO file and convert into an obs structure\n        """"""\n        # Load AVISO Data\n        nc = seapy.netcdf(file)\n        lonname = \'lon\' if \'lon\' in nc.variables.keys() else \'longitude\'\n        lon = nc.variables[lonname][:]\n        latname = \'lat\' if \'lat\' in nc.variables.keys() else \'latitude\'\n        lat = nc.variables[latname][:]\n        dat = np.squeeze(nc.variables[""sla""][:])\n        err = np.squeeze(nc.variables[""err""][:])\n        time = seapy.roms.get_time(\n            nc, ""time"", records=[0], epoch=self.epoch)[0]\n        nc.close()\n        lon, lat = np.meshgrid(lon, lat)\n        lat = lat.flatten()\n        lon = lon.flatten()\n        if not self.grid.east():\n            lon[lon > 180] -= 360\n        data = [seapy.roms.obs.raw_data(""ZETA"", ""SSH_AVISO_MAP"",\n                                        dat.flatten(), err.flatten(), self.ssh_error)]\n        # Grid it\n        obs = seapy.roms.obs.gridder(self.grid, time, lon, lat, None,\n                                     data, self.dt, title)\n\n        # Apply the model mean ssh to the sla data\n        if self.ssh_mean is not None:\n            m, p = seapy.oasurf(self.grid.I, self.grid.J, self.ssh_mean,\n                                obs.x, obs.y, nx=1, ny=1, weight=7)\n            obs.value += m\n        return obs\n\n\n_aviso_sla_errors = {\n    ""SSH_AVISO_ENVISAT"": 0.06,\n    ""SSH_AVISO_JASON1"": 0.05,\n    ""SSH_AVISO_JASON2"": 0.05,\n    ""SSH_AVISO_JASON3"": 0.05,\n    ""SSH_AVISO_GFO"": 0.05,\n    ""SSH_AVISO_ALTIKA"": 0.07,\n    ""SSH_AVISO_CRYOSAT2"": 0.07,\n    ""SSH_AVISO_HAIYANG"": 0.07,\n    ""SSH_AVISO_ERS1"": 0.06,\n    ""SSH_AVISO_ERS2"": 0.06,\n    ""SSH_AVISO_TOPEX_POSEIDON"": 0.05,\n    ""SSH_AVISO_SENTINEL3A"": 0.05\n}\n\n\nclass aviso_sla_track(obsgen):\n    """"""\n    class to process AVISO SLA track netcdf files into ROMS observation\n    files. This is a subclass of seapy.roms.genobs.genobs, and handles\n    the loading of the data. THIS COVERS ALL SATELLITES/INSTRUMENTS FROM AVISO TRACK:\n    al, c2, e1, e2, en, enn, g2, h2, j1, j1g, j1n, j2, tp and tpn.\n\n    Parameters\n    ----------\n    ssh_mean : ndarray,\n      Spatial map of rho-grid shape that contains the model mean SSH\n    ssh_error: dict, optional\n      Dictionary of the minimum errors for each satellite. The default\n      uses the errors defined in _aviso_sla_errors\n    repeat: int\n      Number of hours to repeat the track before and after its initial\n      pass\n    """"""\n\n    def __init__(self, grid, dt, reftime=seapy.default_epoch, ssh_mean=None,\n                 ssh_error=None, repeat=3, provenance=""SSH""):\n        self.provenance = provenance.upper()\n        self.repeat = repeat\n        self.ssh_error = ssh_error if ssh_error else _aviso_sla_errors\n        if ssh_mean is not None:\n            self.ssh_mean = seapy.convolve_mask(ssh_mean, ksize=5, copy=True)\n        else:\n            self.ssh_mean = None\n        super().__init__(grid, dt, reftime)\n\n    def convert_file(self, file, title=""AVISO SLA Track Obs""):\n        """"""\n        Load an AVISO file and convert into an obs structure\n        """"""\n        # Load AVISO Data\n        nc = seapy.netcdf(file)\n        lon = nc.variables[""longitude""][:]\n        lat = nc.variables[""latitude""][:]\n        slaname = \'SLA\' if \'SLA\' in nc.variables.keys() else \'sla_filtered\'\n        dat = nc.variables[slaname][:]\n        time = seapy.roms.num2date(nc, ""time"", epoch=self.epoch)\n        nc.close()\n\n        # make them into vectors\n        lat = lat.ravel()\n        lon = lon.ravel()\n        dat = dat.ravel()\n        err = np.ones(dat.shape) * _aviso_sla_errors.get(self.provenance, 0.1)\n\n        if not self.grid.east():\n            lon[lon > 180] -= 360\n\n        good = dat.nonzero()\n        data = [seapy.roms.obs.raw_data(""ZETA"", self.provenance,\n                                        dat[good], err[good], err[0])]\n        # Grid it\n        obs = seapy.roms.obs.gridder(self.grid, time, lon[good], lat[good], None,\n                                     data, self.dt, title)\n\n        # Apply the model mean ssh to the sla data\n        if self.ssh_mean is not None and obs is not None:\n            m, p = seapy.oasurf(self.grid.I, self.grid.J, self.ssh_mean,\n                                obs.x, obs.y, nx=1, ny=1, weight=7)\n            obs.value += m\n\n        # Duplicate the observations before and after as per the repeat\n        # time unless it is zero\n        if self.repeat and obs:\n            prior = obs.copy()\n            after = obs.copy()\n            prior.time -= self.repeat / 24\n            after.time += self.repeat / 24\n            obs.add(prior)\n            obs.add(after)\n\n        return obs\n\n\nclass ostia_sst_map(obsgen):\n    """"""\n    class to process OSTIA SST map netcdf files into ROMS observation\n    files. This is a subclass of seapy.roms.genobs.genobs, and handles\n    the loading of the data.\n    """"""\n\n    def __init__(self, grid, dt, reftime=seapy.default_epoch, temp_error=0.4,\n                 temp_limits=None):\n        self.temp_error = temp_error\n        if temp_limits is None:\n            self.temp_limits = (2, 35)\n        else:\n            self.temp_limits = temp_limits\n        super().__init__(grid, dt, reftime)\n\n    def convert_file(self, file, title=""OSTIA SST Obs""):\n        """"""\n        Load an OSTIA file and convert into an obs structure\n        """"""\n        # Load OSTIA Data\n        nc = seapy.netcdf(file)\n        lon = nc.variables[""lon""][:]\n        lat = nc.variables[""lat""][:]\n        dat = np.ma.masked_outside(np.squeeze(\n            nc.variables[""analysed_sst""][:]) - 273.15,\n            self.temp_limits[0], self.temp_limits[1])\n        err = np.ma.masked_outside(np.squeeze(\n            nc.variables[""analysis_error""][:]), 0.01, 2.0)\n        dat[err.mask] = np.ma.masked\n        time = seapy.roms.num2date(\n            nc, ""time"", records=[0], epoch=self.epoch)[0]\n        nc.close()\n        if self.grid.east():\n            lon[lon < 0] += 360\n        lon, lat = np.meshgrid(lon, lat)\n        good = dat.nonzero()\n        lat = lat[good]\n        lon = lon[good]\n        data = [seapy.roms.obs.raw_data(""TEMP"", ""SST_OSTIA"", dat.compressed(),\n                                        err[good], self.temp_error)]\n        # Grid it\n        return seapy.roms.obs.gridder(self.grid, time, lon, lat, None,\n                                      data, self.dt, title)\n\n\nclass navo_sst_map(obsgen):\n    """"""\n    class to process NAVO SST map netcdf files into ROMS observation\n    files. This is a subclass of seapy.roms.genobs.genobs, and handles\n    the loading of the data.\n    """"""\n\n    def __init__(self, grid, dt, depth=None, reftime=seapy.default_epoch,\n                 temp_error=0.25, temp_limits=None, provenance=""SST_NAVO_MAP""):\n\n        self.temp_error = temp_error\n        self.provenance = provenance.upper()\n        self.temp_limits = (2, 35) if temp_limits is None else temp_limits\n        self.depth = 4 if depth is None else np.abs(depth)\n        super().__init__(grid, dt, reftime)\n\n    def datespan_file(self, file):\n        nc = seapy.netcdf(file)\n        try:\n            st = datetime.datetime.strptime(nc.getncattr(""start_date""),\n                                            ""%Y-%m-%d UTC"")\n            en = datetime.datetime.strptime(nc.getncattr(""stop_date""),\n                                            ""%Y-%m-%d UTC"")\n        except:\n            st = en = None\n            pass\n        finally:\n            nc.close()\n            return st, en\n\n    def convert_file(self, file, title=""NAVO SST Obs""):\n        """"""\n        Load a NAVO map file and convert into an obs structure\n        """"""\n        import re\n        import sys\n\n        nc = seapy.netcdf(file)\n        lon = nc.variables[""lon""][:]\n        lat = nc.variables[""lat""][:]\n        dat = np.ma.masked_outside(np.squeeze(nc.variables[""analysed_sst""][:]) - 273.15,\n                                   self.temp_limits[0], self.temp_limits[1])\n        err = np.ma.array(np.squeeze(\n            nc.variables[""analysis_error""][:]), mask=dat.mask)\n\n        # this is an analyzed product and provides errors as a function\n        # of space and time directly the temperature is the bulk\n        # temperature (ie at around 4m depth, below the e-folding depths of\n        # sunlight in the ocean so the product does not have a diuranl cycle\n        # (ie you don;t have to worry about hourly variations)\n        time = seapy.roms.num2date(\n            nc, ""time"", records=[0], epoch=self.epoch)[0]\n        nc.close()\n\n        # here we set the depth to be 4 m below the surface\n        if self.grid.east():\n            lon[lon < 0] += 360\n        lon, lat = np.meshgrid(lon, lat)\n        good = dat.nonzero()\n        lat = lat[good]\n        lon = lon[good]\n        data = [seapy.roms.obs.raw_data(""TEMP"", self.provenance, dat.compressed(),\n                                        err[good], self.temp_error)]\n        # Grid it\n        obs = seapy.roms.obs.gridder(self.grid, time, lon, lat, None,\n                                     data, self.dt, depth_adjust=True, title=title)\n        obs.z *= 0\n        obs.depth = -self.depth * np.ones(len(obs.depth))\n        return obs\n\n\nclass modis_sst_map(obsgen):\n    """"""\n    class to process MODIS SST map netcdf files into ROMS observation\n    files. This is a subclass of seapy.roms.genobs.genobs, and handles\n    the loading of the data.\n    """"""\n\n    def __init__(self, grid, dt, reftime=seapy.default_epoch, temp_error=0.5,\n                 temp_limits=None, provenance=""SST_MODIS_AQUA""):\n\n        self.temp_error = temp_error\n        self.provenance = provenance.upper()\n        if temp_limits is None:\n            self.temp_limits = (2, 35)\n        else:\n            self.temp_limits = temp_limits\n        super().__init__(grid, dt, reftime)\n\n    def convert_file(self, file, title=""MODIS SST Obs""):\n        """"""\n        Load an MODIS file and convert into an obs structure\n        """"""\n        # Load MODIS Data\n        import re\n\n        nc = seapy.netcdf(file)\n        lon = nc.variables[""lon""][:]\n        lat = nc.variables[""lat""][:]\n        dat = np.ma.masked_outside(nc.variables[""sst""][:],\n                                   self.temp_limits[0], self.temp_limits[1])\n        err = np.ones(dat.shape) * self.temp_error\n\n        time = seapy.date2day(datetime.datetime.strptime(\n            re.sub(\'\\.[0-9]+Z$\', \'\', nc.time_coverage_end),\n            ""%Y-%m-%dT%H:%M:%S""), self.epoch)\n\n        # Check the data flags\n        flags = np.ma.masked_not_equal(nc.variables[""qual_sst""][:], 0)\n        dat[flags.mask] = np.ma.masked\n\n        nc.close()\n\n        if self.grid.east():\n            lon[lon < 0] += 360\n        lon, lat = np.meshgrid(lon, lat)\n        good = dat.nonzero()\n        lat = lat[good]\n        lon = lon[good]\n        data = [seapy.roms.obs.raw_data(""TEMP"", self.provenance, dat.compressed(),\n                                        err[good], self.temp_error)]\n        # Grid it\n        return seapy.roms.obs.gridder(self.grid, time, lon, lat, None,\n                                      data, self.dt, title)\n\n\nclass remss_swath(obsgen):\n    """"""\n    class to process REMSS SST swath netcdf files into ROMS observation\n    files. The files may be AMSRE, TMI, etc. This is a subclass of\n    seapy.roms.genobs.genobs, and handles the loading of the data.\n    """"""\n\n    def __init__(self, grid, dt, check_qc_flags=True, reftime=seapy.default_epoch, temp_error=0.4,\n                 temp_limits=None, provenance=""SST_REMSS""):\n        self.temp_error = temp_error\n        self.provenance = provenance.upper()\n        self.check_qc_flags = check_qc_flags\n        if temp_limits is None:\n            self.temp_limits = (2, 35)\n        else:\n            self.temp_limits = temp_limits\n        super().__init__(grid, dt, reftime)\n\n    def convert_file(self, file, title=""REMSS SST Obs""):\n        """"""\n        Load an REMSS file and convert into an obs structure\n        """"""\n        # Load REMSS Data\n        nc = seapy.netcdf(file)\n        lon = nc.variables[""lon""][:]\n        lat = nc.variables[""lat""][:]\n        dat = np.ma.masked_outside(np.squeeze(\n            nc.variables[""sea_surface_temperature""][:]) - 273.15,\n            self.temp_limits[0], self.temp_limits[1])\n        err = np.ma.masked_outside(np.squeeze(\n            nc.variables[""sses_standard_deviation""][:]), 0.01, 2.0)\n        dat[err.mask] = np.ma.masked\n\n        # Check the data flags\n        if self.check_qc_flags:\n            flags = np.ma.masked_not_equal(\n                np.squeeze(nc.variables[""quality_level""][:]), 5)\n            dat[flags.mask] = np.ma.masked\n        else:\n            dat = np.ma.masked_where(\n                np.squeeze(nc.variables[""quality_level""][:]).data == 1, dat)\n\n        # Grab the observation time\n        time = seapy.roms.num2date(nc, ""time"", records=[0])[0] - self.epoch\n        dtime = nc.variables[""sst_dtime""][:]\n        time = np.squeeze((time.total_seconds() + dtime) * seapy.secs2day)\n        nc.close()\n        if self.grid.east():\n            lon[lon < 0] += 360\n        good = dat.nonzero()\n        data = [seapy.roms.obs.raw_data(""TEMP"", self.provenance,\n                                        dat.compressed(),\n                                        err[good], self.temp_error)]\n        # Grid it\n        return seapy.roms.obs.gridder(self.grid, time[good], lon[good], lat[good],\n                                      None, data, self.dt, title)\n\n\nclass remss_map(obsgen):\n    """"""\n    class to process REMSS SST map netcdf files into ROMS observation\n    files. The files may be AMSRE, TMI, etc. This is a subclass of\n    seapy.roms.genobs.genobs, and handles the loading of the data.\n    """"""\n\n    def __init__(self, grid, dt, reftime=seapy.default_epoch, temp_error=0.4,\n                 temp_limits=None, provenance=""SST_REMSS""):\n        self.temp_error = temp_error\n        self.provenance = provenance.upper()\n        if temp_limits is None:\n            self.temp_limits = (2, 35)\n        else:\n            self.temp_limits = temp_limits\n        super().__init__(grid, dt, reftime)\n\n    def convert_file(self, file, title=""REMSS SST Obs""):\n        """"""\n        Load an REMSS file and convert into an obs structure\n        """"""\n        # Load REMSS Data\n        nc = seapy.netcdf(file)\n        lon = nc.variables[""lon""][:]\n        lat = nc.variables[""lat""][:]\n        dat = np.ma.masked_outside(np.squeeze(\n            nc.variables[""sea_surface_temperature""][:]) - 273.15,\n            self.temp_limits[0], self.temp_limits[1])\n        err = np.ma.masked_outside(np.squeeze(\n            nc.variables[""SSES_standard_deviation_error""][:]), 0.01, 2.0)\n        dat[err.mask] = np.ma.masked\n\n        # Check the data flags\n        flags = np.ma.masked_not_equal(\n            np.squeeze(nc.variables[""rejection_flag""][:]), 0)\n        dat[flags.mask] = np.ma.masked\n        err[flags.mask] = np.ma.masked\n\n        # Grab the observation time\n        time = seapy.roms.num2date(nc, ""time"", epoch=self.epoch)\n        sst_time = nc.variables[""sst_dtime""][:] * seapy.secs2day\n        for n, i in enumerate(time):\n            sst_time[n, :, :] += i\n        sst_time[dat.mask] = np.ma.masked\n\n        # Set up the coordinate\n        lon, lat = np.meshgrid(lon, lat)\n        lon = np.ma.masked_where(dat.mask, seapy.adddim(lon, len(time)))\n        lat = np.ma.masked_where(dat.mask, seapy.adddim(lat, len(time)))\n\n        nc.close()\n\n        if self.grid.east():\n            lon[lon < 0] += 360\n        data = [seapy.roms.obs.raw_data(""TEMP"", self.provenance,\n                                        dat.compressed(),\n                                        err.compressed(), self.temp_error)]\n        # Grid it\n        return seapy.roms.obs.gridder(self.grid, sst_time.compressed(),\n                                      lon.compressed(), lat.compressed, None,\n                                      data, self.dt, title)\n\n\nclass viirs_swath(obsgen):\n    """"""\n    class to process VIIRS SST swath netcdf files into ROMS observation\n    files.  This is a subclass of\n    seapy.roms.obsgen.obsgen, and handles the loading of the data.\n    """"""\n\n    def __init__(self, grid, dt, check_qc_flags=True, reftime=seapy.default_epoch,\n                 temp_error=0.4, temp_limits=None, provenance=""SST_VIIRS""):\n        self.temp_error = temp_error\n        self.provenance = provenance.upper()\n        self.check_qc_flags = check_qc_flags\n        if temp_limits is None:\n            self.temp_limits = (2, 35)\n        else:\n            self.temp_limits = temp_limits\n        super().__init__(grid, dt, reftime)\n\n    def convert_file(self, file, title=""VIIRS SST Obs""):\n        """"""\n        Load a VIIRS file and convert into an obs structure\n        """"""\n        # Load VIIRS Data\n        nc = seapy.netcdf(file, aggdim=""time"")\n        lon = nc.variables[""lon""][:]\n        lat = nc.variables[""lat""][:]\n        dat = np.ma.masked_outside(\n            nc.variables[""sea_surface_temperature""][:] - 273.15,\n            self.temp_limits[0], self.temp_limits[1])\n        err = np.ma.masked_outside(\n            nc.variables[""sses_standard_deviation""][:], 0.01, 2.0)\n        dat[err.mask] = np.ma.masked\n\n        # Check the data flags\n        if self.check_qc_flags:\n            flags = np.ma.masked_not_equal(\n                nc.variables[""quality_level""][:], 5)\n            dat[flags.mask] = np.ma.masked\n        else:\n            dat = np.ma.masked_where(\n                nc.variables[""quality_level""][:].data == 1, dat)\n\n        # Grab the observation time\n        time = netCDF4.num2date(nc.variables[""time""][:],\n                                nc.variables[""time""].units) - self.epoch\n        time = np.asarray([x.total_seconds() for x in time])[\n            :, np.newaxis, np.newaxis]\n        dtime = nc.variables[""sst_dtime""][:]\n        time = (time + dtime) * seapy.secs2day\n        nc.close()\n\n        # Set up the coordinate\n        lon = np.ma.masked_where(dat.mask, seapy.adddim(lon, len(time)))\n        lat = np.ma.masked_where(dat.mask, seapy.adddim(lat, len(time)))\n        if self.grid.east():\n            lon[lon < 0] += 360\n        good = dat.nonzero()\n        data = [seapy.roms.obs.raw_data(""TEMP"", self.provenance,\n                                        dat.compressed(),\n                                        err[good], self.temp_error)]\n        # Grid it\n        return seapy.roms.obs.gridder(self.grid, time[good], lon[good], lat[good],\n                                      None, data, self.dt, title)\n\n\n##############################################################################\n#\n# IN SITU DATA\n#\n##############################################################################\n\n\nclass seaglider_profile(obsgen):\n    """"""\n    class to process SeaGlider .pro files into ROMS observation\n    files. This is a subclass of seapy.roms.genobs.genobs, and handles\n    the loading of the data.\n    """"""\n\n    def __init__(self, grid, dt, reftime=seapy.default_epoch, dtype=None, temp_limits=None,\n                 salt_limits=None, depth_limit=-15, temp_error=0.2,\n                 salt_error=0.05):\n        if temp_limits is None:\n            self.temp_limits = (5, 30)\n        else:\n            self.temp_limits = temp_limits\n        if salt_limits is None:\n            self.salt_limits = (31, 35.5)\n        else:\n            self.salt_limits = salt_limits\n        if dtype is None:\n            self.dtype = {\'names\': (\'time\', \'pres\', \'depth\', \'temp\', \'cond\',\n                                    \'salt\', \'sigma\', \'lat\', \'lon\'),\n                          \'formats\': [\'f4\'] * 9}\n        else:\n            self.dtype = dtype\n        self.depth_limit = depth_limit\n        self.temp_error = temp_error\n        self.salt_error = salt_error\n        super().__init__(grid, dt, reftime)\n\n    def convert_file(self, file, title=""SeaGlider Obs""):\n        """"""\n        Load a SeaGlider .pro file and convert into an obs structure\n        """"""\n        import re\n\n        # Load the text file. All data goes into the pro dictionary\n        # as defined by dtype. The header information needs to be parsed\n        with open(file) as myfile:\n            header = [myfile.readline() for i in range(19)]\n            pro = np.loadtxt(myfile, self.dtype, delimiter=\',\', comments=\'%\')\n\n        # Parse the header information\n        parser = re.compile(\'^%(\\w+): (.*)$\')\n        params = {}\n        for line in header:\n            try:\n                opt = parser.findall(line)\n                params[opt[0][0]] = opt[0][1]\n            except:\n                pass\n\n        # Determine the needed information from the headers\n        glider_name = ""GLIDER"" if params.get(""glider"", None) is None else \\\n                      ""GLIDER_SG"" + params[""glider""]\n        provenance = seapy.roms.obs.asprovenance(glider_name)\n        try:\n            date = [int(s) for s in re.findall(\'([\\d]{2})\\s\', params[""start""])]\n            start_time = datetime.datetime.strptime(params[""start""].strip(),\n                                                    ""%m %d 1%y %H %M %S"")\n            dtime = (start_time - self.epoch).total_seconds() / 86400\n        except:\n            raise ValueError(""date format incorrect in file: "" + file)\n\n        # Make sure that the GPS fix isn\'t screwy\n        if self.grid.east():\n            pro[""lon""][pro[""lon""] < 0] += 360\n        dist = seapy.earth_distance(pro[""lon""][0], pro[""lat""][0],\n                                    pro[""lon""][-1], pro[""lat""][-1])\n        velocity = dist / pro[""time""][-1]\n        if velocity > 2:\n            warn(""WARNING: GPS fix is incorrect for "" + file)\n            return None\n\n        # Build the data with masked entries\n        temp = np.ma.masked_outside(pro[""temp""], self.temp_limits[0],\n                                    self.temp_limits[1])\n        salt = np.ma.masked_outside(pro[""salt""], self.salt_limits[0],\n                                    self.salt_limits[1])\n        depth = np.ma.masked_greater(-pro[""depth""], self.depth_limit)\n        good = ~np.ma.getmaskarray(depth)\n\n        # Grid it\n        data = [seapy.roms.obs.raw_data(""TEMP"", provenance, temp[good],\n                                        None, self.temp_error),\n                seapy.roms.obs.raw_data(""SALT"", provenance, salt[good],\n                                        None, self.salt_error)]\n        return seapy.roms.obs.gridder(self.grid, pro[""time""][good] / 86400 + dtime,\n                                      pro[""lon""][good],\n                                      pro[""lat""][good],\n                                      depth.compressed(),\n                                      data, self.dt, title)\n\n\nclass mooring(obsgen):\n    """"""\n    Class to process generic moorings into ROMS observation files. This\n    handles temp, salt, u, and v.\n    """"""\n\n    def __init__(self, grid, dt, reftime=seapy.default_epoch, temp_limits=None,\n                 salt_limits=None, u_limits=None, v_limits=None,\n                 depth_limit=0, temp_error=0.25, salt_error=0.08,\n                 u_error=0.08, v_error=0.08, lat=None, lon=None,\n                 provenance=None):\n        if temp_limits is None:\n            self.temp_limits = (5, 35)\n        else:\n            self.temp_limits = temp_limits\n        if salt_limits is None:\n            self.salt_limits = (31, 35.5)\n        else:\n            self.salt_limits = salt_limits\n        if u_limits is None:\n            self.u_limits = (-3, 3)\n        else:\n            self.u_limits = u_limits\n        if v_limits is None:\n            self.v_limits = (-3, 3)\n        else:\n            self.v_limits = v_limits\n        if provenance is None:\n            self.provenance = seapy.roms.obs.asprovenance(""MOORING"")\n        else:\n            self.provenance = provenance.upper()\n        self.depth_limit = depth_limit\n        self.temp_error = temp_error\n        self.salt_error = salt_error\n        self.u_error = u_error\n        self.v_error = v_error\n        self.lat = np.atleast_1d(lat)\n        self.lon = np.atleast_1d(lon)\n        super().__init__(grid, dt, reftime)\n\n    def convert_data(self, time, depth, data, error=None, title=""Mooring Obs""):\n        """"""\n        Given a set of data, process into an observation structure\n\n        Parameters\n        ----------\n        time : ndarray\n          time of observations\n        depth : ndarray\n          depth of observations. depth is in rows, time in columns.\n          If depth does not change with time, it will be replicated in time.\n        data : dict\n          data to put into observations. A dictionary using seapy.roms.fields\n          as keys.\n        error : dict, optional\n          error of the observations (same keys and sizes as data)\n        title : string, optional\n          title for obs\n\n        Returns\n        -------\n        obs: seapy.roms.obs.obs\n        """"""\n        # Check that the lat/lon is in the grid\n        if self.grid.east():\n            self.lon[self.lon <= 0] += 360\n        else:\n            self.lon[self.lon >= 180] -= 360\n        if not np.logical_and.reduce((\n                self.lon >= np.min(self.grid.lon_rho),\n                self.lon <= np.max(self.grid.lon_rho),\n                self.lat >= np.min(self.grid.lat_rho),\n                self.lat <= np.max(self.grid.lat_rho))):\n            warn(""Mooring location is not in grid"")\n            return\n        depth = np.atleast_1d(depth)\n\n        if not error:\n            error = {}\n\n        if not data:\n            warn(""No data is provided"")\n            return\n\n        # Process the data\n        obsdata = []\n        for field in data:\n            limit = getattr(self, field + \'_limits\')\n            vals = np.ma.masked_outside(data[field], limit[0], limit[1],\n                                        copy=False)\n            obsdata.append(seapy.roms.obs.raw_data(field, self.provenance,\n                                                   vals, getattr(\n                                                       error, field, None),\n                                                   getattr(self, field + \'_error\')))\n\n        ndep = depth.size\n        nt = len(time)\n        lat = np.resize(self.lat, (nt, ndep))\n        lon = np.resize(self.lon, (nt, ndep))\n        depth = np.resize(depth, (nt, ndep))\n        time = np.resize(time, (nt, ndep))\n        return seapy.roms.obs.gridder(self.grid, time, lon, lat, depth,\n                                      obsdata, self.dt, title)\n\n\nclass tao_mooring(mooring):\n    """"""\n    class to process TAO files into ROMS observation\n    files. This is a subclass of seapy.roms.genobs.genobs, and handles\n    the loading of the data.\n    """"""\n\n    def __init__(self, grid, dt, reftime=seapy.default_epoch, temp_limits=None,\n                 salt_limits=None, u_limits=None, v_limits=None,\n                 depth_limit=0, temp_error=0.25, salt_error=0.08,\n                 u_error=0.08, v_error=0.08):\n        super().__init__(grid, dt, reftime)\n\n    def convert_file(self, file, title=""TAO Obs""):\n        """"""\n        Load a TAO netcdf file and convert into an obs structure\n        """"""\n        vals = {""temp"": [""T_20"", ""QT_5020""],\n                ""salt"": [""S_41"", ""QS_5041""],\n                ""u"": [""U_320"", ""QS_5300""],\n                ""v"": [""V_321"", ""QS_5300""]}\n        nc = seapy.netcdf(file)\n        lat = nc.variables[""lat""][:]\n        lon = nc.variables[""lon""][:]\n        if not self.grid.east():\n            lon[lon > 180] -= 360\n        lat, lon = np.meshgrid(lat, lon)\n        time = seapy.roms.num2date(nc, ""time"", epoch=self.epoch)\n        depth = -nc.variables[""depth""][:]\n        profile_list = np.where(np.logical_and.reduce((\n            lon >= np.min(self.grid.lon_rho),\n            lon <= np.max(self.grid.lon_rho),\n            lat >= np.min(self.grid.lat_rho),\n            lat <= np.max(self.grid.lat_rho))))\n\n        # If nothing is in the area, return nothing\n        if not profile_list[0].size:\n            return None\n\n        # Process each of the variables that are present\n        obsdata = []\n        for field in vals:\n            limit = getattr(self, field + \'_limits\')\n            if vals[field][0] in nc.variables:\n                data = nc.variables[vals[field][0]][:]\n                data = np.ma.masked_outside(\n                    data[profile_list[0], profile_list[1], :, :],\n                    limit[0], limit[1], copy=False)\n                qc = nc.variables[vals[field][1]][:]\n                qc = qc[profile_list[0], profile_list[1], :, :]\n                bad = np.where(np.logical_and(qc != 1, qc != 2))\n                data[bad] = np.ma.masked\n                obsdata.append(seapy.roms.obs.raw_data(field, ""TAO_ARRAY"",\n                                                       data.compressed(), None,\n                                                       getattr(self, field + \'_error\')))\n        nc.close()\n\n        # Build the time, lon, lat, and depth arrays of appropriate size\n        npts = profile_list[0].size\n        ndep = depth.size\n        nt = len(time)\n        lat = np.resize(lat[profile_list], (nt, ndep, npts))\n        lat = np.squeeze(np.transpose(lat, (2, 1, 0)))[~data.mask]\n        lon = np.resize(lon[profile_list], (nt, ndep, npts))\n        lon = np.squeeze(np.transpose(lon, (2, 1, 0)))[~data.mask]\n        depth = np.resize(depth, (npts, nt, ndep))\n        depth = np.squeeze(np.transpose(depth, (0, 2, 1)))[~data.mask]\n        time = np.squeeze(np.resize(time, (npts, ndep, nt)))[~data.mask]\n        return seapy.roms.obs.gridder(self.grid, time, lon, lat, depth,\n                                      obsdata, self.dt, title)\n\n\nclass argo_ctd(obsgen):\n    """"""\n    class to process ARGO CTD netcdf files into ROMS observation\n    files. This is a subclass of seapy.roms.genobs.genobs, and handles\n    the loading of the data.\n    """"""\n\n    def __init__(self, grid, dt, reftime=seapy.default_epoch, temp_limits=None,\n                 salt_limits=None, temp_error=0.25,\n                 salt_error=0.1):\n        if temp_limits is None:\n            self.temp_limits = (2, 35)\n        else:\n            self.temp_limits = temp_limits\n        if salt_limits is None:\n\n            self.salt_limits = (10, 35.5)\n        else:\n            self.salt_limits = salt_limits\n        self.temp_error = temp_error\n        self.salt_error = salt_error\n        super().__init__(grid, dt, reftime)\n\n    def datespan_file(self, file):\n        """"""\n        return the just the day that this argo file covers\n        """"""\n        nc = seapy.netcdf(file)\n        try:\n            d = netCDF4.num2date(nc.variables[\'JULD\'][0],\n                                 nc.variables[\'JULD\'].units)\n            st = datetime.datetime(*d.timetuple()[:3])\n            en = datetime.datetime(*d.timetuple()[:3] + (23, 59, 59))\n        except:\n            st = en = None\n            pass\n        finally:\n            nc.close()\n            return st, en\n\n    def convert_file(self, file, title=""Argo Obs""):\n        """"""\n        Load an Argo file and convert into an obs structure\n        """"""\n        nc = seapy.netcdf(file, aggdim=""N_PROF"")\n\n        # Load the position of all profiles in the file\n        lon = nc.variables[""LONGITUDE""][:]\n        lat = nc.variables[""LATITUDE""][:]\n        pro_q = nc.variables[""POSITION_QC""][:].astype(int)\n        # Find the profiles that are in our area with known locations quality\n        if self.grid.east():\n            lon[lon < 0] += 360\n        profile_list = np.where(np.logical_and.reduce((\n            lat >= np.min(self.grid.lat_rho),\n            lat <= np.max(self.grid.lat_rho),\n            lon >= np.min(self.grid.lon_rho),\n            lon <= np.max(self.grid.lon_rho),\n            pro_q == 1)))[0]\n\n        # Check which are good profiles\n        profile_qc = nc.variables[""PROFILE_PRES_QC""][\n            profile_list].astype(\'<U1\')\n        profile_list = profile_list[profile_qc == \'A\']\n        if not profile_list.size:\n            return None\n\n        # Load only the data from those in our area\n        julian_day = nc.variables[""JULD_LOCATION""][profile_list]\n        argo_epoch = datetime.datetime.strptime(\'\'.join(\n            nc.variables[""REFERENCE_DATE_TIME""][:].astype(\'<U1\')), \'%Y%m%d%H%M%S\')\n        time_delta = (self.epoch - argo_epoch).days\n        file_stamp = datetime.datetime.strptime(\'\'.join(\n            nc.variables[""DATE_CREATION""][:].astype(\'<U1\')), \'%Y%m%d%H%M%S\')\n\n        # Grab data over the previous day\n        file_time = np.minimum((file_stamp - argo_epoch).days,\n                               int(np.max(julian_day)))\n        time_list = np.where(julian_day >= file_time - 1)[0]\n        julian_day = julian_day[time_list]\n        lon = lon[profile_list[time_list]]\n        lat = lat[profile_list[time_list]]\n        profile_list = profile_list[time_list]\n\n        # Load the data in our region and time\n        temp = nc.variables[""TEMP""][profile_list, :]\n        temp_qc = nc.variables[""TEMP_QC""][profile_list, :]\n        salt = nc.variables[""PSAL""][profile_list, :]\n        salt_qc = nc.variables[""PSAL_QC""][profile_list, :]\n        pres = nc.variables[""PRES""][profile_list, :]\n        pres_qc = nc.variables[""PRES_QC""][profile_list, :]\n        nc.close()\n\n        # Ensure consistency\n        full_mask = np.logical_or.reduce((temp.mask, salt.mask, pres.mask))\n        temp[full_mask] = np.ma.masked\n        temp_qc[full_mask] = np.ma.masked\n        salt[full_mask] = np.ma.masked\n        salt_qc[full_mask] = np.ma.masked\n        pres[full_mask] = np.ma.masked\n        pres_qc[full_mask] = np.ma.masked\n\n        # Combine the QC codes\n        qc = np.mean(np.vstack((temp_qc.compressed(), salt_qc.compressed(),\n                                pres_qc.compressed())).astype(int), axis=0)\n        good_data = np.where(qc == 1)\n\n        # Put everything together into individual observations\n        time = np.resize(julian_day - time_delta,\n                         pres.shape[::-1]).T[~temp.mask][good_data]\n        lat = np.resize(lat, pres.shape[::-1]).T[~temp.mask][good_data]\n        lon = np.resize(lon, pres.shape[::-1]).T[~temp.mask][good_data]\n        depth = -seapy.seawater.depth(pres.compressed()[good_data], lat)\n\n        # Apply the limits\n        temp = np.ma.masked_outside(temp.compressed()[good_data],\n                                    self.temp_limits[0], self.temp_limits[1])\n        salt = np.ma.masked_outside(salt.compressed()[good_data],\n                                    self.salt_limits[0], self.salt_limits[1])\n\n        data = [seapy.roms.obs.raw_data(""TEMP"", ""CTD_ARGO"", temp,\n                                        None, self.temp_error),\n                seapy.roms.obs.raw_data(""SALT"", ""CTD_ARGO"", salt,\n                                        None, self.salt_error)]\n\n        return seapy.roms.obs.gridder(self.grid, time, lon, lat, depth,\n                                      data, self.dt, title)\n'"
seapy/roms/psource.py,12,"b'#!/usr/bin/env python\n""""""\n   psource.py\n\n   Routines for dealing with point source files in ROMS\n\n   Author: Brian Powell <powellb@hawaii.edu>\n   Copyright (c)2020 University of Hawaii under the MIT-License.\n   Created: 10 January 2020\n\n""""""\n\nimport seapy.roms.ncgen\nimport numpy as np\nimport urllib.request as urllib\nimport json\nimport datetime\n\ndischarge_url = \'https://waterdata.usgs.gov/nwisweb/get_ratings?file_type=exsa&site_no=\'\nriver_url = \'https://waterservices.usgs.gov/nwis/iv/?format=json\'\nft3m3 = 1 / (3.28**3)\n\n\ndef create(filename, river, s_rho=None, cdl=None):\n    """"""\n    Construct a point source file with all of the point sources configured.\n\n    Input\n    -----\n    filename : string\n            Output filename\n    river : array,\n            array of dictionaries of rivers with the following information that\n            defines a point source:\n               The x and y values on the grid for where the point source is,\n               the direction of the point source (0 for xi or 1 for eta),\n               an identification number (any choice),\n               optional flag (1=temp, 2=salt, 3=temp+salt, 4=temp+salt+sed,\n                              5=temp+salt+sed+bio),\n               and an array of values for the vertical shape (a value for each s-level)\n               that sum to 1.\n            { ""x"":grid_x,\n              ""y"":grid_y,\n              ""direction"":0 or 1,\n              ""id"":value,\n              ""flag"":[1,2,3,4,or 5], [optional]\n              ""vshape"":[vals] } [optional]\n\n    s_rho : int, optional\n            Number of s-levels in the point source file (should match the grid).\n            If not specified, it will derive it from the vshape parameter\n    cdl : string, optional,\n            Name of CDL file to use\n\n    Output\n    ------\n    nc : netCDF4 id\n       If successful, returns the netcdf id for writing\n    """"""\n    river = np.asarray(river)\n    s_rho = len(river[0][\'vshape\']) if s_rho is None else s_rho\n\n    # Create an empty, new file\n    nc = seapy.roms.ncgen.create_psource(\n        filename, nriver=len(river), s_rho=s_rho, clobber=True, cdl=cdl)\n\n    # Go through each river and set up the basics\n    for i, r in enumerate(river):\n        try:\n            nc.variables[\'river\'][i] = int(r[\'id\'])\n        except:\n            nc.variables[\'river\'][i] = 999\n        nc.variables[\'river_Xposition\'][i] = int(r[\'x\'])\n        nc.variables[\'river_Eposition\'][i] = int(r[\'y\'])\n        nc.variables[\'river_direction\'][i] = int(r[\'direction\'])\n        try:\n            nc.variables[\'river_flag\'][i] = int(r[\'flag\'])\n        except:\n            nc.variables[\'river_flag\'][i] = 0\n        try:\n            vshape = np.asarray(r[\'vshape\'])[:, np.newaxis]\n            nc.variables[\'river_Vshape\'][:, i] = vshape\n        except Exception as err:\n            print(""Using default shape"")\n            vshape = np.ones((s_rho, 1)) / s_rho\n            nc.variables[\'river_Vshape\'][:, i] = vshape\n\n    return nc\n\n\ndef stage2discharge(gage, usgs_id):\n    \'\'\'\n    Function to convert stage data to discharge using usgs reference table\n\n    Input\n    -------\n    gage : masked array,\n                array of stage data to be converted\n    usgs_id : int,\n                8 digit identifier for river on usgs\n\n    Output\n    ---------\n    flux : masked array,\n                discharge data in cubic feet\n    \'\'\'\n    import urllib.request as urllib\n\n    # Load lookup table\n    url = discharge_url + usgs_id\n    stage = []\n    discharge = []\n    header = 0\n    for line in urllib.urlopen(url):\n        if not line.startswith(b\'#\'):\n            if header < 2:\n                header += 1\n            else:\n                a = line.split(b\'\\t\')\n                stage.append(float(line.split(b\'\\t\')[0]))\n                discharge.append(float(line.split(b\'\\t\')[2]))\n    stage = np.asarray(stage).astype(float)\n    discharge = np.asarray(discharge).astype(float)\n\n    # Convert data\n    flux = np.ma.masked_array(np.zeros(len(gage)), mask=gage.mask)\n    for i, f in enumerate(gage):\n        if f:\n            flux[i] = discharge[(np.abs(stage - f)).argmin()]\n    return flux\n\n\ndef get_usgs_transport(usgs_id, times=1, source=\'discharge\'):\n    \'\'\'\n    Function to get flux data from usgs and convert to cubic meters\n\n    Input\n    -------\n    usgs_id : int,\n                8 digit identifier for river on usgs\n    times : datetime array,\n                list of values to get usgs data for. Values from USGS\n                will be linearly interpolated onto these times.\n    source : string,\n                  set to \'discharge\' or \'stage\' to access corresponding\n                  parameter. Stage data is then converted to discharge\n                  using stage2discharge function.\n    Output\n    ---------\n    dates : list,\n                list of datetimes associated with the flux data\n    flux : array,\n                flux data in cubic meters per second\n    \'\'\'\n    # Build url\n    siteurl = \'&sites=\' + usgs_id\n    if source == \'discharge\':\n        sourceurl = \'&parameterCd=00060\'\n    elif source == \'stage\':\n        sourceurl = \'&parameterCd=00065\'\n    else:\n        print(\'Incorrect source type specified\')\n        return None\n    timeurl = \'&startDT=%s&endDT=%s\' % (times[0].strftime(\n        \'%Y-%m-%d\'), times[-1].strftime(\'%Y-%m-%d\'))\n    url = river_url + siteurl + sourceurl + timeurl\n\n    # Access url\n    print(\'Accessing \' + url)\n    x = json.loads(urllib.urlopen(url).read().decode(\'utf-8\'))\n    dates = []\n    flux = []\n    if x[\'value\'][\'timeSeries\']:\n        for l in x[\'value\'][\'timeSeries\'][0][\'values\'][0][\'value\']:\n            dates.append(datetime.datetime.strptime(\n                l[\'dateTime\'][:16], \'%Y-%m-%dT%H:%M\'))\n            flux.append(l[\'value\'])\n        flux = np.ma.masked_values(\n            np.ma.masked_array(flux).astype(np.float), -999999)\n        try:\n            if source == \'stage\':\n                flux = stage2discharge(flux, usgs_id)\n            # Interpolate the data for consistency\n            flux *= ft3m3\n            return np.interp(seapy.date2day(times),\n                             seapy.date2day(np.array(dates)[\n                                            ~np.ma.getmaskarray(flux)]),\n                             flux.compressed(), left=flux.min(), right=flux.min())\n        except ValueError:\n            print(\'No valid data found\')\n            return None\n    else:\n        print(\'Cannot process file from url\')\n        return None\n'"
seapy/roms/tide.py,10,"b'#!/usr/bin/env python\n""""""\n  tide.py\n\n  Methods for working tidal forcing files in ROMS\n\n  Written by Brian Powell on 04/05/16\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\n\n\nimport numpy as np\nimport netCDF4\nimport seapy\nimport datetime\nfrom warnings import warn\n\n\ndef create_forcing(filename, tide, title=""Tidal Forcing"", epoch=seapy.default_epoch):\n    """"""\n    Create a tidal forcing file from the given tidal values.\n\n    Parameters\n    ----------\n    filename: string,\n      File name of the tidal forcing to create\n    tide: dict,\n      Dictionary of the tidal forcing containing the following keys:\n       Eamp : SSH amplitdue\n       Ephase : SSH phase (radians)\n       Cmajor : velocity major ellipse\n       Cminor : velocity minor ellipse\n       Cphase : velocity ellipse phase (radians)\n       Cangle : velocity ellipse angle (radians)\n       tide_start : datetime of the tide reference\n       tides  : list of the tides\n    title: string, optional,\n      NetCDF title string to use\n    epoch: datetime, optional,\n      Epoch date for time reference\n\n    Returns\n    -------\n    None\n    """"""\n    # Create the tide forcing file\n    ntides, eta_rho, xi_rho = tide[\'Eamp\'].shape\n    if ntides != len(tide[\'tides\']):\n        raise ValueError(\n            ""The number of tidal data are different than the tides."")\n\n    tideout = seapy.roms.ncgen.create_tide(filename, eta_rho=eta_rho,\n                                           xi_rho=xi_rho,\n                                           reftime=epoch,\n                                           ntides=ntides,\n                                           clobber=True, title=title)\n    # Set the tide periods and attributes\n    tideout.variables[\'tide_period\'][:] = 1.0 / \\\n        seapy.tide.frequency(tide[\'tides\'])\n    tideout.setncattr(""tidal_constituents"", "", "".join(tide[\'tides\']))\n    tideout.setncattr(""tide_start"", ""Day {:5.1f} ({:s})"".format((tide[\'tide_start\']\n                                                                 - epoch).total_seconds() / 86400,\n                                                                str(tide[\'tide_start\'])))\n    tideout.setncattr(""base_date"", ""days since {:s}"".format(\n        str(tide[\'tide_start\'])))\n    tideout.variables[\'tide_Eamp\'][:] = tide[\'Eamp\']\n    tideout.variables[\'tide_Ephase\'][:] = np.degrees(tide[\'Ephase\'])\n    tideout.variables[\'tide_Cmax\'][:] = tide[\'Cmajor\']\n    tideout.variables[\'tide_Cmin\'][:] = tide[\'Cminor\']\n    tideout.variables[\'tide_Cphase\'][:] = np.degrees(tide[\'Cphase\'])\n    tideout.variables[\'tide_Cangle\'][:] = np.degrees(tide[\'Cangle\'])\n    tideout.close()\n\n\ndef load_forcing(filename):\n    """"""\n    Load a tidal forcing file into a dictionary\n\n    Parameters\n    ----------\n    filename: string\n      File name of the tidal forcing file to load\n\n    Returns\n    -------\n    dict:\n      Dictionary of the tidal forcing information with keys:\n       Eamp : SSH amplitdue\n       Ephase : SSH phase (radians)\n       Cmajor : velocity major ellipse\n       Cminor : velocity minor ellipse\n       Cphase : velocity ellipse phase (radians)\n       Cangle : velocity ellipse angle (radians)\n       tide_start : datetime of the tide reference\n       tides  : list of the tides\n    """"""\n    import re\n\n    nc = seapy.netcdf(filename)\n    frc = {}\n    frc[\'Eamp\'] = nc.variables[\'tide_Eamp\'][:]\n    frc[\'Ephase\'] = np.radians(nc.variables[\'tide_Ephase\'][:])\n    frc[\'Cmajor\'] = nc.variables[\'tide_Cmax\'][:]\n    frc[\'Cminor\'] = nc.variables[\'tide_Cmin\'][:]\n    frc[\'Cphase\'] = np.radians(nc.variables[\'tide_Cphase\'][:])\n    frc[\'Cangle\'] = np.radians(nc.variables[\'tide_Cangle\'][:])\n    start_str = getattr(nc, \'tide_start\', None) or \\\n        getattr(nc, \'base_date\', None)\n    tides = getattr(nc, \'tidal_constituents\', None) or \\\n        getattr(nc, \'tides\', None)\n    frc[\'tides\'] = tides.upper().split("", "")\n    frc[\'tide_start\'] = None\n    nc.close()\n    if start_str:\n        try:\n            frc[\'tide_start\'] = datetime.datetime.strptime(\n                re.sub(\'^.*since\\s*\', \'\', start_str),\n                ""%Y-%m-%d %H:%M:%S"")\n        except ValueError:\n            pass\n\n    return frc\n\n\ndef tide_error(his_file, tide_file, grid=None):\n    """"""\n    Calculates the tidal error for each point given a model history and the\n    tidal file used\n\n    Parameters\n    ----------\n    his_file : string,\n      String of history file location. Can be multiple files using wildcard\n    tide_file: string,\n      String of tidal file location\n    grid : string or grid, optional,\n      If specified, use this grid. Default is to build a grid from the history\n      file.\n\n    Returns\n    -------\n      tide_error : masked_array,\n        Array containing the tidal error at each point, with land points masked\n\n    """"""\n    if grid:\n        grid = seapy.model.asgrid(grid)\n    else:\n        grid = seapy.model.asgrid(his_file)\n\n    # Load tidal file data\n    frc = load_forcing(tide_file)\n\n    # Calculate tidal error for each point\n    nc = seapy.netcdf(his_file)\n    times = seapy.roms.num2date(nc)\n    tide_error = np.ma.masked_where(\n        grid.mask_rho == 0, np.zeros((grid.mask_rho.shape)))\n    zeta = nc.variables[\'zeta\'][:]\n    nc.close()\n    for i in seapy.progressbar.progress(range(grid.ln)):\n        for j in range(grid.lm):\n            if not tide_error.mask[i, j]:\n                z = zeta[:, i, j]\n                t_ap = seapy.tide.pack_amp_phase(frc[\'tides\'],\n                                                 frc[\'Eamp\'][:, i, j], frc[\'Ephase\'][:, i, j])\n                mout = seapy.tide.fit(times, z, tides=frc[\'tides\'],\n                                      lat=grid.lat_rho[i, j], tide_start=frc[\'tide_start\'])\n                for c in t_ap:\n                    m = mout[\'major\'][c]\n                    t = t_ap[c]\n                    tide_error[i, j] += 0.5 * (m.amp**2 + t.amp**2) - \\\n                        m.amp * t.amp * np.cos(m.phase - t.phase)\n                tide_error[i, j] = np.sqrt(tide_error[i, j])\n    return tide_error\n'"
seapy/seawater/__init__.py,0,"b'# --- encoding: iso-8859-1 ---\n\n""""""\n Seawater -- Python functions for properties of sea water\n    Bj\xc3\xb8rn \xc3\x85dlandsvik <bjorn@imr.no>,\n    Institute of Marine Research,\n    Version 1.1, 13 November 2002\n\n Public functions:\n Density related\n   dens(S,T,P)           Density of sea water              kg/m**3\n   delta(S,T,P)          Specific volume anomaly           m**3/kg\n   sigma(S,T,P)          Density anomaly                   kg/m**3\n   drhodt(S,T,P)         Temperature derivative of density kg/(K*m**3)\n   alpha(S,T,P)          Thermal expansion coefficient     1/K\n   drhods(S,T,P)         Salinity derivative of density    kg/m**3\n   beta(S,T,P)           Salinity expansion coefficient\n\n Salinity related\n   salt(R,T,P)           Salinity\n   cond(S,T,P)           Conductivity ratio\n\n Heat related\n   heatcap(S,T,P)        Heat capacity                     J/(kg*K)\n   adtgrad(S,T,P)        Adiabatic lapse rate              K/dbar\n   temppot(S,T,P,Pref)   Potential temperature             \xc2\xb0C\n   temppot0(S,T,P)       Potential temperature             \xc2\xb0C\n\n Miscellaneous\n   freezept(S,P)         Freezing point                    \xc2\xb0C\n   soundvel(S,T,P)       Sound velocity                    m/s\n   depth(P,lat)          Depth                             m\n\n Arguments:\n   S     = Salinity\n   T     = Temperature               \xc2\xb0C\n   P     = Pressure                  dbar\n   R     = Conductivity ratio\n   Pref  = Reference pressure        dbar\n   lat   = Latitude                  deg\n\n References:\n   [Bryden 1973], New polynomials for thermal expansion, adiabatic\n   temperature gradient and potential temperature gradient of sea water\n   Deep-Sea Res. 20, 401-408\n\n   [UNESCO 1981], Tenth report of the joint panel on oceanographic\n   tables and standards, Unesco technical papers in marine science 36.\n\n   [UNESCO 1983], N.P. Fofonoff and R.C. Millard Jr., Algorithms for\n   computation of fundamental properties of seawater, Unesco technical\n   papers in marine science 44.\n\n""""""\n\n# --- Exceptions ---\nclass OutOfRangeError(Exception): pass\n\nfrom .density import dens, svan, sigma, drhodt, alpha, drhods, beta\nfrom .salinity import salt, cond\nfrom .heat import heatcap, adtgrad, temppot, temppot0\nfrom .misc import freezept, soundvel, depth\n\n\n'"
seapy/seawater/density.py,14,"b'# --- encoding: iso-8859-1 ---\n\n""""""Seawater density module\n\ndens(S, T[, P])   -- Density\nsvan(S, T[, P])   -- Specific volume anomaly\nsigma(S, T[, P])  -- Density anomaly\ndrhodt(S, T[, P]) -- Temperature derivative of density\nalpha(S, T[, P])  -- Thermal expansion coefficient\ndrhods(S, T[, P]) -- Salinity derivative of density\nbeta(S, T[, P])   -- Saline expansion coefficient\n\nBj\xc3\xb8rn \xc3\x85dlandsvik <bjorn@imr.no>, 07 November 2004\n\n""""""\nimport numpy as np\n\n# -----------------------------------------------\n\n\ndef _dens0(S, T):\n    """"""Density of seawater at zero pressure""""""\n\n    S = np.asarray(S)\n    T = np.asarray(T)\n\n    # --- Define constants ---\n    a0 = 999.842594\n    a1 = 6.793952e-2\n    a2 = -9.095290e-3\n    a3 = 1.001685e-4\n    a4 = -1.120083e-6\n    a5 = 6.536332e-9\n\n    b0 = 8.24493e-1\n    b1 = -4.0899e-3\n    b2 = 7.6438e-5\n    b3 = -8.2467e-7\n    b4 = 5.3875e-9\n\n    c0 = -5.72466e-3\n    c1 = 1.0227e-4\n    c2 = -1.6546e-6\n\n    d0 = 4.8314e-4\n\n    # --- Computations ---\n    # Density of pure water\n    SMOW = a0 + (a1 + (a2 + (a3 + (a4 + a5 * T) * T) * T) * T) * T\n\n    # More temperature polynomials\n    RB = b0 + (b1 + (b2 + (b3 + b4 * T) * T) * T) * T\n    RC = c0 + (c1 + c2 * T) * T\n\n    return SMOW + RB * S + RC * (S**1.5) + d0 * S * S\n\n# -----------------------------------------------------------------\n\n\ndef _seck(S, T, P=0):\n    """"""Secant bulk modulus""""""\n\n    S = np.asarray(S)\n    T = np.asarray(T)\n    P = np.asarray(P)\n\n    # --- Pure water terms ---\n\n    h0 = 3.239908\n    h1 = 1.43713E-3\n    h2 = 1.16092E-4\n    h3 = -5.77905E-7\n    AW = h0 + (h1 + (h2 + h3 * T) * T) * T\n\n    k0 = 8.50935E-5\n    k1 = -6.12293E-6\n    k2 = 5.2787E-8\n    BW = k0 + (k1 + k2 * T) * T\n\n    e0 = 19652.21\n    e1 = 148.4206\n    e2 = -2.327105\n    e3 = 1.360477E-2\n    e4 = -5.155288E-5\n    KW = e0 + (e1 + (e2 + (e3 + e4 * T) * T) * T) * T\n\n    # --- seawater, P = 0 ---\n\n    SR = S**0.5\n\n    i0 = 2.2838E-3\n    i1 = -1.0981E-5\n    i2 = -1.6078E-6\n    j0 = 1.91075E-4\n    A = AW + (i0 + (i1 + i2 * T) * T + j0 * SR) * S\n\n    f0 = 54.6746\n    f1 = -0.603459\n    f2 = 1.09987E-2\n    f3 = -6.1670E-5\n    g0 = 7.944E-2\n    g1 = 1.6483E-2\n    g2 = -5.3009E-4\n    K0 = KW + (f0 + (f1 + (f2 + f3 * T) * T) * T\n               + (g0 + (g1 + g2 * T) * T) * SR) * S\n\n    # --- General expression ---\n\n    m0 = -9.9348E-7\n    m1 = 2.0816E-8\n    m2 = 9.1697E-10\n    B = BW + (m0 + (m1 + m2 * T) * T) * S\n\n    K = K0 + (A + B * P) * P\n\n    return K\n\n# ----------------------------------------------\n\n\ndef dens(S, T, P=0):\n    """"""Compute density of seawater from salinity, temperature, and pressure\n\n    Usage: dens(S, T, [P])\n\n    Input:\n        S = Salinity,     [PSS-78]\n        T = Temperature,  [\xc2\xb0C]\n        P = Pressure,     [dbar = 10**4 Pa]\n    P is optional, with default value zero\n\n    Output:\n        Density,          [kg/m**3]\n\n    Algorithm: UNESCO 1983\n\n    """"""\n\n    S = np.asarray(S)\n    T = np.asarray(T)\n    P = np.asarray(P)\n\n    P = 0.1 * P  # Convert to bar\n    return _dens0(S, T) / (1 - P / _seck(S, T, P))\n\n# -------------------------------------------\n\n\ndef svan(S, T, P=0):\n    """"""Compute specific volume anomaly\n\n    Usage: svan(S, T, [P])\n\n    Input:\n        S = Salinity,     [PSS-78]\n        T = Temperature,  [\xc2\xb0C]\n        P = Pressure,     [dbar]\n    P is optional, with a default value = zero\n\n    Output:\n        Specific volume anomaly  [m**3/kg]\n\n    """"""\n    return 1.0 / dens(S, T, P) - 1.0 / dens(35, 0, P)\n\n# -----------------------------------------------\n\n\ndef sigma(S, T, P=0):\n    """"""Compute density anomaly, sigma-T\n\n    Usage: sigma(S, T, [P])\n\n    Input:\n        S = Salinity,     [PSS-78]\n        T = Temperature,  [\xc2\xb0C]\n        P = Pressure,     [dbar]\n    P is optional, with a default value = zero\n\n    Output:\n       Density anomaly,  [kg/m**3]\n\n    """"""\n    return dens(S, T, P) - 1000.0\n\n# ----------------------------------------------\n\n\ndef drhodt(S, T, P=0):\n    """"""Compute temperature derivative of density\n\n    Usage: drhodt(S, T, [P])\n\n    Input:\n        S = Salinity,     [PSS-78]\n        T = Temperature,  [\xc2\xb0C]\n        P = Pressure,     [dbar]\n    P is optional, with a default value = zero\n\n    Output:\n        Temperature derivative of density  [kg /(K m**3)]\n\n    """"""\n    S = np.asarray(S)\n    T = np.asarray(T)\n    P = np.asarray(P)\n\n    a1 = 6.793952e-2\n    a2 = -1.819058e-2\n    a3 = 3.005055e-4\n    a4 = -4.480332e-6\n    a5 = 3.268166e-8\n\n    b1 = -4.0899e-3\n    b2 = 1.52876e-4\n    b3 = -2.47401e-6\n    b4 = 2.155e-8\n\n    c1 = 1.0227e-4\n    c2 = -3.3092e-6\n\n    e1 = 148.4206\n    e2 = -4.65421\n    e3 = 4.081431e-2\n    e4 = -2.0621152e-4\n\n    f1 = -0.603459\n    f2 = 2.19974e-2\n    f3 = -1.8501e-4\n\n    g1 = 1.6483e-2\n    g2 = -1.06018e-3\n\n    h1 = 1.43713e-3\n    h2 = 2.32184e-4\n    h3 = -1.733715e-6\n\n    i1 = -1.0981e-5\n    i2 = -3.2156e-6\n\n    k1 = -6.12293e-6\n    k2 = 1.05574e-7\n\n    m1 = 2.0816e-8\n    m2 = 1.83394e-9\n\n    P = P / 10.0\n\n    DSMOV = a1 + (a2 + (a3 + (a4 + a5 * T) * T) * T) * T\n    DRHO0 = DSMOV + (b1 + (b2 + (b3 + b4 * T) * T) * T) * \\\n        S + (c1 + c2 * T) * S**1.5\n\n    DAW = h1 + (h2 + h3 * T)\n    DA = DAW + (i1 + i2 * T) * S\n\n    DBW = k1 + k2 * T\n    DB = DBW + (m1 + m2 * T) * S\n\n    DKW = e1 + (e2 + (e3 + e4 * T) * T) * T\n    DK0 = DKW + (f1 + (f2 + f3 * T) * T) * S + (g1 + g2 * T) * S**1.5\n    DK = DK0 + (DA + DB * P) * P\n\n    K = _seck(S, T, P)\n    RHO0 = _dens0(S, T)\n    denom = 1. - P / K\n    return (DRHO0 * denom - RHO0 * P * DK / (K * K)) / (denom * denom)\n\n# -----------------------------------------------\n\n\ndef alpha(S, T, P=0):\n    """"""Compute thermal expansion coefficient\n\n    Usage: alpha(S, T, [P])\n\n    Input:\n        S = Salinity,     [PSS-78]\n        T = Temperature,  [\xc2\xb0C]\n        P = Pressure,     [dbar]\n    P is optional, with a default value = zero\n\n    Output:\n        Thermal expansion coefficient,  [1/K]\n\n    """"""\n\n    ALPHA = - drhodt(S, T, P) / dens(S, T, P)\n    return ALPHA\n\n# ------------------------------------------------\n\n\ndef drhods(S, T, P=0):\n    """"""Compute salinity derivative of density\n\n    Usage: drhodt(S, T, [P])\n\n    Input:\n        S = Salinity,     [PSS-78]\n        T = Temperature,  [\xc2\xb0C]\n        P = Pressure,     [dbar]\n    P is optional, with a default value = zero\n\n    Output:\n        Salinity derivative of density [kg/m**3]\n\n    """"""\n    S = np.asarray(S)\n    T = np.asarray(T)\n    P = np.asarray(P)\n\n    b0 = 8.24493e-1\n    b1 = -4.0899e-3\n    b2 = 7.6438e-5\n    b3 = -8.2467e-7\n    b4 = 5.3875e-9\n\n    c0 = -5.72466e-3\n    c1 = 1.0227e-4\n    c2 = -1.6546e-6\n\n    d0 = 9.6628e-4\n\n    f0 = 54.6746\n    f1 = -0.603459\n    f2 = 1.09987e-2\n    f3 = -6.1670e-5\n\n    g0 = 7.944e-2\n    g1 = 1.6483e-2\n    g2 = -5.3009e-4\n\n    i0 = 2.2838e-3\n    i1 = -1.0981e-5\n    i2 = -1.6078e-6\n\n    j0 = 2.866125e-4\n\n    m0 = -9.9348e-7\n    m1 = 2.0816e-8\n    m2 = 9.1697e-10\n\n    P = 0.1 * P  # Convert to bar\n\n    DRHO0 = b0 + T * (b1 + T * (b2 + T * (b3 + T * b4))) +   \\\n        1.5 * S**0.5 * (c0 + T * (c1 + T * c2)) + S * d0\n    DK0 = f0 + T * (f1 + T * (f2 + T * f3)) +              \\\n        1.5 * S**0.5 * (g0 + T * (g1 + T * g2))\n    DA = i0 + T * (i1 + T * i2) + j0 * S**0.5\n    DB = m0 + T * (m1 + T * m2)\n    DK = DK0 + P * (DA + P * DB)\n    RHO0 = _dens0(S, T)\n    K = _seck(S, T, P)\n    denom = 1. - P / K\n    DRHO = (DRHO0 * denom - RHO0 * P * DK / (K * K)) / (denom * denom)\n    return DRHO\n\n# ------------------------------------------------------\n\n\ndef beta(S, T, P=0):\n    """"""Compute saline expansion coefficient\n\n    Usage: alpha(S, T, [P])\n\n    Input:\n        S = Salinity,     [PSS-78]\n        T = Temperature,  [\xc2\xb0C]\n        P = Pressure,     [dbar]\n    P is optional, with a default value = zero\n\n    Output:\n        Saline expansion coefficient\n\n    """"""\n\n    BETA = drhods(S, T, P) / dens(S, T, P)\n    return BETA\n\n\n### SALINITY FUNCTIONS #################################\n\n# def _sal(XR,XT):\n\n##     a0 =  0.0080\n##     a1 = -0.1692\n##     a2 = 25.3851\n##     a3 = 14.0941\n##     a4 = -7.0261\n##     a5 =  2.7081\n\n##     b0 =  0.0005\n##     b1 = -0.0056\n##     b2 = -0.0066\n##     b3 = -0.0375\n##     b4 =  0.0636\n##     b5 = -0.0144\n\n##     k  =  0.0162\n\n# DS = (XT / (1+k*XT) ) *        \\\n##          (b0 + (b1 + (b2 + (b3 + (b4 + b5*XR)*XR)*XR)*XR)*XR)\n\n# return a0 + (a1 + (a2 + (a3 + (a4 + a5*XR)*XR)*XR)*XR)*XR + DS\n\n# ---------------------------------------------------\n\n# def _dsal(XR,XT):\n\n##     a1 = -0.1692\n##     a2 = 25.3851\n##     a3 = 14.0941\n##     a4 = -7.0261\n##     a5 =  2.7081\n\n##     b1 = -0.0056\n##     b2 = -0.0066\n##     b3 = -0.0375\n##     b4 =  0.0636\n##     b5 = -0.0144\n\n##     k  =  0.0162\n\n# dDS = (XT / (1+k*XT) ) *      \\\n##           (b1 + (b2*2 + (b3*3 + (b4*4 + b5*5*XR)*XR)*XR)*XR)\n\n# return a1 + (a2*2 + (a3*3 + (a4*4 + a5*5*XR)*XR)*XR)*XR + dDS\n\n# ---------------------------------------------\n\n# def _rt(T):\n\n##     c0 =  0.6766097\n##     c1 =  2.00564e-2\n##     c2 =  1.104259e-4\n##     c3 = -6.9698e-7\n##     c4 =  1.0031e-9\n\n# return c0 + (c1 + (c2 + (c3 + c4*T)*T)*T)*T\n\n# ---------------------------------------------------\n\n# def _c(P):\n\n##     e1 =  2.070e-5\n##     e2 = -6.370e-10\n##     e3 =  3.989e-15\n\n# return (e1 + (e2 + e3*P)*P)*P\n\n# ---------------------------------------------------\n\n# def _b(T):\n\n##     d1 =  3.426e-2\n##     d2 =  4.464e-4\n\n# return 1.0 + (d1 + d2*T)*T\n\n# ---------------------------------------------------\n\n# def _a(T):\n\n##     d3 =  4.215e-1\n##     d4 = -3.107e-3\n# return d3 + d4*T\n\n'"
seapy/seawater/heat.py,12,"b'# --- encoding: iso-8859-1 ---\n\n""""""Seawater heat module\n\nheatcap(S, T[, P])       -- Heat capacity\nadtgrad(S, T[, P])       -- Adiabatic temperature gradiente\ntemppot(S, T, P[, Pref]) -- Potential temperature\ntemppot0(S, T, P)        -- Potential temperature, relative to surface\n\nBj\xc3\xb8rn \xc3\x85dlandsvik, <bjorn@imr.no>, 07 November 2004\n\n""""""\nimport numpy as np\n\n# -------------------------------------------------\n\ndef heatcap(S, T, P=0):\n    """"""Compute heat capacity\n\n    Usage: heatcap(S, T, [P])\n\n    Input:\n        S = Salinity,     [PSS-78]\n        T = Temperature,  [\xc2\xb0C]\n        P = Pressure,     [dbar]\n    P is optional, with a default value = zero\n\n    Output:\n        Heat capacity  [J/(kg*K)]\n\n    Algorithm: UNESCO 1983\n\n    """"""\n    S = np.asarray(S)\n    T = np.asarray(T)\n    P = np.asarray(P)\n\n    P = 0.1*P  # Conversion to bar\n\n    # - Temperatur dependence\n    c0 =  4217.4\n    c1 = -3.720283\n    c2 =  0.1412855\n    c3 = -2.654387e-3\n    c4 =  2.093236e-5\n\n    a0 = -7.64357\n    a1 =  0.1072763\n    a2 = -1.38385e-3\n\n    b0 =  0.1770383\n    b1 = -4.07718e-3\n    b2 =  5.148e-5\n\n    CP0 =  c0 + c1*T + c2*T**2 + c3*T**3 + c4*T**4  \\\n          + (a0 + a1*T + a2*T**2)*S \\\n          + (b0 + b1*T + b2*T**2)*S**1.5\n\n    # - Pressure dependence\n    a0 = -4.9592e-1\n    a1 =  1.45747e-2\n    a2 = -3.13885e-4\n    a3 =  2.0357e-6\n    a4 =  1.7168e-8\n\n    b0 =  2.4931e-4\n    b1 = -1.08645e-5\n    b2 =  2.87533e-7\n    b3 = -4.0027e-9\n    b4 =  2.2956e-11\n\n    c0 = -5.422e-8\n    c1 =  2.6380e-9\n    c2 = -6.5637e-11\n    c3 =  6.136e-13\n\n    CP1 = (a0 + a1*T + a2*T**2 + a3*T**3 + a4*T**4)*P  \\\n              + (b0 + b1*T + b2*T**2 + b3*T**3 + b4*T**4)*P**2 \\\n              + (c0 + c1*T + c2*T**2 + c3*T**3)*P**3\n\n    # - Salinity dependence\n    d0 =  4.9247e-3\n    d1 = -1.28315e-4\n    d2 =  9.802e-7\n    d3 =  2.5941e-8\n    d4 = -2.9179e-10\n\n    e0 = -1.2331e-4\n    e1 = -1.517e-6\n    e2 =  3.122e-8\n\n    f0 = -2.9558e-6\n    f1 =  1.17054e-7\n    f2 = -2.3905e-9\n    f3 =  1.8448e-11\n\n    g0 =  9.971e-8\n\n    h0 =  5.540e-10\n    h1 = -1.7682e-11\n    h2 =  3.513e-13\n\n    j1 = -1.4300e-12\n    S3_2  = S**1.5\n\n    CP2 = ((d0 + d1*T + d2*T**2 + d3*T**3 + d4*T**4)*S \\\n           + (e0 + e1*T + e2*T**2)*S3_2)*P  \\\n\t   + ((f0 + f1*T + f2*T**2 + f3*T**3)*S  \\\n\t   +   g0*S3_2)*P**2 \\\n\t   + ((h0 + h1*T + h2*T**2)*S + j1*T*S3_2)*P**3\n\n\n    return CP0 + CP1 + CP2\n\n# --------------------------------------------------------------\n\ndef adtgrad(S, T, P=0):\n    """"""Compute adiabatic temperature gradient\n\n    Usage: adtgrad(S, T, [P])\n\n    Input:\n        S = Salinity,     [PSS-78]\n        T = Temperature,  [\xc2\xb0C]\n        P = Pressure,     [dbar]\n    P is optional, with a default value = zero\n\n    Output:\n        Adiabatic temperature gradient,  [K/dbar]\n\n    Algorithm: UNESCO 1983\n\n    """"""\n    S = np.asarray(S)\n    T = np.asarray(T)\n    P = np.asarray(P)\n\n    a0 =  3.5803e-5\n    a1 = +8.5258e-6\n    a2 = -6.836e-8\n    a3 =  6.6228e-10\n\n    b0 = +1.8932e-6\n    b1 = -4.2393e-8\n\n    c0 = +1.8741e-8\n    c1 = -6.7795e-10\n    c2 = +8.733e-12\n    c3 = -5.4481e-14\n\n    d0 = -1.1351e-10\n    d1 =  2.7759e-12\n\n    e0 = -4.6206e-13\n    e1 = +1.8676e-14\n    e2 = -2.1687e-16\n\n    return  a0 + (a1 + (a2 + a3*T)*T)*T  \\\n         + (b0 + b1*T)*(S-35)  \\\n\t + ( (c0 + (c1 + (c2 + c3*T)*T)*T) \\\n         +   (d0 + d1*T)*(S-35) )*P \\\n         + (e0 + (e1 + e2*T)*T )*P*P\n\n# ---------------------------------------------------------------\n\ndef temppot(S, T, P, Pref=0):\n    """"""Compute potential temperature\n\n    Usage: temppot(S, T, P, [Pref])\n\n    Input:\n        S = Salinity,                [PSS-78]\n        T = Temperature,             [\xc2\xb0C]\n        P = Pressure,                [dbar]\n        Pref = Reference pressure,   [dbar]\n    Pref is optional, with a default value = zero\n\n    Output:\n        Potential temperature,  [\xc2\xb0C]\n\n    Algorithm: UNESCO 1983\n\n    """"""\n    S = np.asarray(S)\n    T = np.asarray(T)\n    P = np.asarray(P)\n\n    H = Pref-P\n    XK = H*adtgrad(S,T,P)\n\n    T = T + 0.5*XK\n    Q = XK\n    P = P + 0.5*H\n    XK = H*adtgrad(S,T,P)\n\n    T = T + 0.29289322*(XK-Q)\n    Q = 0.58578644*XK + 0.121320344*Q\n    XK = H*adtgrad(S,T,P)\n\n    T = T + 1.707106781*(XK-Q)\n    Q = 3.414213562*XK - 4.121320344*Q\n    P = P + 0.5*H\n    XK = H*adtgrad(S,T,P)\n\n    return T + (XK-2.0*Q)/6.0\n\n\n# ------------------------------------------------------\n\ndef temppot0(S,T,P):\n    """"""Compute potential temperature relative to surface\n\n    Usage: temppot0(S, T, P)\n\n    Input:\n        S = Salinity,                [PSS-78]\n        T = Temperature,             [\xc2\xb0C]\n        P = Pressure,                [dbar]\n\n    Output:\n        Potential temperature,       [\xc2\xb0C]\n\n    Algorithm: Bryden 1973\n\n    Note: Due to different algorithms,\n        temppot0(S, T, P) != tempot(S, T, P, Pref=0)\n\n    """"""\n    S = np.asarray(S)\n    T = np.asarray(T)\n    P = np.asarray(P)\n\n    P = P/10  # Conversion from dbar\n\n    a0 =  3.6504e-4\n    a1 =  8.3198e-5\n    a2 = -5.4065e-7\n    a3 =  4.0274e-9\n\n    b0 =  1.7439e-5\n    b1 = -2.9778e-7\n\n    c0 =  8.9309e-7\n    c1 = -3.1628e-8\n    c2 =  2.1987e-10\n\n    d0 =  4.1057e-9\n\n    e0 = -1.6056e-10\n    e1 =  5.0484e-12\n\n    S0 = S - 35.0\n\n    return  T - (a0 + (a1 + (a2 + a3*T)*T)*T)*P  \\\n              - (b0 + b1*T)*P*S0                 \\\n              - (c0 + (c1 + c2*T)*T)*P*P         \\\n              + d0*S0*P*P                        \\\n              - (e0 + e1*T)*P*P*P\n\n\n'"
seapy/seawater/misc.py,9,"b'# --- encoding: iso-8859-1 ---\n\n""""""Miscellaneous sea water functions\n\nfreezept(S[, P])    -- Freezing point\nsoundvel(S, T[, P]) -- Sound velocity\ndepth(P, lat)       -- Depth from pressure\n\nBj\xc3\xb8rn \xc3\x85dlandsvik, <bjorn@imr.no>  07 November 2004\n\n""""""\n\nimport numpy as np\n\n# ----------------------------------------------------------\n\ndef freezept(S, P=0):\n\n    """"""Compute freezing temperature of sea water\n\n    Usage: freezept(S, [P])\n\n    Input:\n        S = Salinity,      [psu]\n        P = Pressure,      [dbar]\n    P is optional, with a default value = 0\n\n    Output:\n        T = Freezing point,   [\xc2\xb0C]\n\n    Algorithm: UNESCO 1983\n\n    """"""\n    S = np.asarray(S)\n    P = np.asarray(P)\n\n    a0 = -0.0575\n    a1 =  1.710523e-3\n    a2 = -2.154996e-4\n    b  = -7.53e-4\n\n    Tf = a0*S + a1*S**1.5 + a2*S**2 + b*P\n    return Tf\n\n# ----------------------------------------------------------------\n\ndef soundvel(S, T, P=0):\n    """"""Compute velocity of sound\n\n    Usage: soundvel(S, T, [P])\n\n    Input:\n        S = Salinity,     [PSS-78]\n        T = Temperature,  [\xc2\xb0C]\n        P = Pressure,     [dbar]\n    P is optional, with a default value = zero\n\n    Output:\n        Sound velocity,  [m/s]\n\n    Algorithm: UNESCO 1983\n\n    """"""\n\n    S = np.asarray(S)\n    T = np.asarray(T)\n    P = np.asarray(P)\n\n    P = 0.1*P  # Conversion to bar\n\n    c00 = 1402.388\n    c01 =  5.03711\n    c02 = -5.80852e-2\n    c03 =  3.3420e-4\n    c04 = -1.47800e-6\n    c05 =  3.1464e-9\n\n    c10 =  0.153563\n    c11 =  6.8982e-4\n    c12 = -8.1788e-6\n    c13 =  1.3621e-7\n    c14 = -6.1185e-10\n\n    c20 =  3.1260e-5\n    c21 = -1.7107e-6\n    c22 =  2.5974e-8\n    c23 = -2.5335e-10\n    c24 =  1.0405e-12\n\n    c30 = -9.7729e-9\n    c31 =  3.8504e-10\n    c32 = -2.3643e-12\n\n    P2 = P*P\n    P3 = P2*P\n    Cw =  c00 + (c01 + (c02 + (c03 + (c04 + c05*T)*T)*T)*T)*T   \\\n       + (c10 + (c11 + (c12 + (c13 + c14*T)*T)*T)*T)*P          \\\n       + (c20 + (c21 + (c22 + (c23 + c24*T)*T)*T)*T)*P2         \\\n       + (c30 + (c31 + c32*T)*T)*P3\n\n    a00 =  1.389\n    a01 = -1.262e-2\n    a02 =  7.164e-5\n    a03 =  2.006e-6\n    a04 = -3.21e-8\n\n    a10 =  9.4742e-5\n    a11 = -1.2580e-5\n    a12 = -6.4885e-8\n    a13 =  1.0507e-8\n    a14 = -2.0122e-10\n\n    a20 = -3.9064e-7\n    a21 =  9.1041e-9\n    a22 = -1.6002e-10\n    a23 =  7.988e-12\n\n    a30 =  1.100e-10\n    a31 =  6.649e-12\n    a32 = -3.389e-13\n\n    A =  a00 + (a01 + (a02 + (a03 + a04*T)*T)*T)*T      \\\n      + (a10 + (a11 + (a12 + (a13 + a14*T)*T)*T)*T)*P   \\\n      + (a20 + (a21 + (a22 + a23*T)*T)*T)*P2            \\\n      + (a30 + (a31 + a32*T)*T)*P3\n\n    b00 = -1.922e-2\n    b01 = -4.42e-5\n    b10 =  7.3637e-5\n    b11 =  1.7945e-7\n\n    B = b00 + b01*T + (b10 + b11*T)*P\n\n    d00 =  1.727e-3\n    d10 = -7.9836e-6\n\n    D = d00 + d10*P\n\n    return Cw + A*S + B*S**1.5 + D*S**2\n\n# ----------------------------------------------------------------\n\ndef depth(P, lat):\n    """"""Compute depth from pressure and latitude\n\n    Usage: depth(P, lat)\n\n    Input:\n        P = Pressure,     [dbar]\n        lat = Latitude    [deg]\n\n    Output:\n        Depth             [m]\n\n    Algorithm: UNESCO 1983\n\n    """"""\n\n    P = np.asarray(P)\n    lat = np.asarray(lat)\n\n    a1 =  9.72659\n    a2 = -2.2512e-5\n    a3 =  2.279e-10\n    a4 = -1.82e-15\n\n    b  =  1.092e-6\n\n    g0 =  9.780318\n    g1 =  5.2788e-3\n    g2 =  2.36e-5\n\n    rad = np.pi / 180.\n\n    X = np.sin(lat*rad)\n    X = X*X\n    grav = g0 * (1.0 + (g1 + g2*X)*X) + b*P\n    nom = (a1 + (a2 + (a3 + a4*P)*P)*P)*P\n\n    return nom / grav\n\n\n\n\n'"
seapy/seawater/salinity.py,6,"b'# --- encoding: iso-8859-1 ---\n\n""""""Seawater salinity module, providing salt and cond functions\n\nS = salt(R, T[, P]) -- Salinity\nR = cond(S, T[, P]) -- Conductivity ratio\n\nArguments:\n    R = Conductivity ratio\n    S = Salinity\n    T = Temperature         [\xc2\xb0C]\n    P = Pressure,           [dbar = 10**4 Pa]\n\nBj\xc3\xb8rn \xc3\x85dlandsvik <bjorn@imr.no>, 07 November 2004\n\n""""""\n\n# -------------------------------------------------------\n\ndef _sal(XR,XT):\n\n    a0 =  0.0080\n    a1 = -0.1692\n    a2 = 25.3851\n    a3 = 14.0941\n    a4 = -7.0261\n    a5 =  2.7081\n\n    b0 =  0.0005\n    b1 = -0.0056\n    b2 = -0.0066\n    b3 = -0.0375\n    b4 =  0.0636\n    b5 = -0.0144\n\n    k  =  0.0162\n\n    DS = (XT / (1+k*XT) ) *        \\\n         (b0 + (b1 + (b2 + (b3 + (b4 + b5*XR)*XR)*XR)*XR)*XR)\n\n    return a0 + (a1 + (a2 + (a3 + (a4 + a5*XR)*XR)*XR)*XR)*XR + DS\n\n# ---------------------------------------------------\n\ndef _dsal(XR,XT):\n\n    a1 = -0.1692\n    a2 = 25.3851\n    a3 = 14.0941\n    a4 = -7.0261\n    a5 =  2.7081\n\n    b1 = -0.0056\n    b2 = -0.0066\n    b3 = -0.0375\n    b4 =  0.0636\n    b5 = -0.0144\n\n    k  =  0.0162\n\n    dDS = (XT / (1+k*XT) ) *      \\\n          (b1 + (b2*2 + (b3*3 + (b4*4 + b5*5*XR)*XR)*XR)*XR)\n\n    return a1 + (a2*2 + (a3*3 + (a4*4 + a5*5*XR)*XR)*XR)*XR + dDS\n\n# ---------------------------------------------\n\ndef _rt(T):\n\n    c0 =  0.6766097\n    c1 =  2.00564e-2\n    c2 =  1.104259e-4\n    c3 = -6.9698e-7\n    c4 =  1.0031e-9\n\n    return c0 + (c1 + (c2 + (c3 + c4*T)*T)*T)*T\n\n# ---------------------------------------------------\n\ndef _c(P):\n\n    e1 =  2.070e-5\n    e2 = -6.370e-10\n    e3 =  3.989e-15\n\n    return (e1 + (e2 + e3*P)*P)*P\n\n# ---------------------------------------------------\n\ndef _b(T):\n\n    d1 =  3.426e-2\n    d2 =  4.464e-4\n\n    return 1.0 + (d1 + d2*T)*T\n\n# ---------------------------------------------------\n\ndef _a(T):\n\n    d3 =  4.215e-1\n    d4 = -3.107e-3\n\n    return d3 + d4*T\n\n# --------------------------------------------------\n\ndef salt(R, T, P):\n    """"""Compute salinity from conductivity, temperature, and pressure\n\n    Usage: salt(R, T, [P])\n\n    Input:\n        R = Conductivity ratio\n        T = Temperature        [\xc2\xb0C]\n        P = Pressure,          [dbar = 10**4 Pa]\n    P is optional, with default value zero\n\n    Output:\n        S = Salinity           [PSS-78]\n\n    """"""\n    R = np.asarray(R)\n    T = np.asarray(T)\n    P = np.asarray(P)\n\n    DT = T - 15.0\n    RT = R/(_rt(T)*(1.0 + _c(P)/(_b(T) + _a(T)*R)))\n    RT = abs(RT)**0.5\n\n    return _sal(RT,DT)\n\n# -------------------------------------------------\n\ndef cond(S, T, P):\n    """"""Compute conductivity ratio from salinity, temperature, and pressure\n\n    Usage: cond(S, T, [P])\n\n    Input:\n        S = Salinity      [PSS-78]\n        T = Temperature   [\xc2\xb0C]\n        P = Pressure,     [dbar = 10**4 Pa]\n    P is optional, with default value zero\n\n    Output:\n        R = Conductivity ratio\n\n""""""\n    S = np.asarray(S)\n    T = np.asarray(T)\n    P = np.asarray(P)\n\n    DT = T-15.0\n    RT = (S/35.0)**0.5\n    SI = _sal(RT,DT)\n    # Iteration\n    for n in xrange(100):\n        RT = RT + (S-SI)/_dsal(RT,DT)\n        SI = _sal(RT,DT)\n        try:\n            DELS = max(abs(SI-S))\n        except TypeError: # Not sequence, i.e. scalar S\n            DELS = abs(SI-S)\n        if (DELS < 1.0E-4):\n            break\n\n    RTT = _rt(T)*RT*RT\n    AT = _a(T)\n    BT = _b(T)\n    CP = _c(P)\n    CP = RTT*(CP + BT)\n    BT = BT - RTT*AT\n\n    R = abs(BT*BT + 4.0*AT*CP)**0.5 - BT\n\n    return 0.5*R/AT\n\n\n'"
seapy/roms/cobalt/__init__.py,0,"b'""""""\n   __init__.py\n\n\n   Load the routines for cobalt\n\n   Import functions include:\n   - :func:`cobalt.enable`\n   - :func:`cobalt.disable`\n""""""\n\nfrom .cobalt import *\n'"
seapy/roms/cobalt/cobalt.py,0,"b'#!/usr/bin/env python\n""""""\n   cobalt.py\n\n   Define fields and utility functions for working with GFDL COBALT\n   and ROMS\n\n   Author: Brian Powell <powellb@hawaii.edu>\n  Copyright (c)2020 University of Hawaii under the MIT-License.\n""""""\nimport seapy\nimport os\n\n# Define the COBALT fields that are used\nfields = {""alk"": {""grid"": ""rho"", ""dims"": 3},\n          ""cadet_arag"": {""grid"": ""rho"", ""dims"": 3},\n          ""cadet_calc"": {""grid"": ""rho"", ""dims"": 3},\n          ""dic"": {""grid"": ""rho"", ""dims"": 3},\n          ""fed"": {""grid"": ""rho"", ""dims"": 3},\n          ""fedet"": {""grid"": ""rho"", ""dims"": 3},\n          ""fedi"": {""grid"": ""rho"", ""dims"": 3},\n          ""felg"": {""grid"": ""rho"", ""dims"": 3},\n          ""fesm"": {""grid"": ""rho"", ""dims"": 3},\n          ""ldon"": {""grid"": ""rho"", ""dims"": 3},\n          ""ldop"": {""grid"": ""rho"", ""dims"": 3},\n          ""lith"": {""grid"": ""rho"", ""dims"": 3},\n          ""lithdet"": {""grid"": ""rho"", ""dims"": 3},\n          ""nbact"": {""grid"": ""rho"", ""dims"": 3},\n          ""ndet"": {""grid"": ""rho"", ""dims"": 3},\n          ""ndi"": {""grid"": ""rho"", ""dims"": 3},\n          ""nlg"": {""grid"": ""rho"", ""dims"": 3},\n          ""nsm"": {""grid"": ""rho"", ""dims"": 3},\n          ""nh4"": {""grid"": ""rho"", ""dims"": 3},\n          ""no3"": {""grid"": ""rho"", ""dims"": 3},\n          ""o2"": {""grid"": ""rho"", ""dims"": 3},\n          ""pdet"": {""grid"": ""rho"", ""dims"": 3},\n          ""po4"": {""grid"": ""rho"", ""dims"": 3},\n          ""srdon"": {""grid"": ""rho"", ""dims"": 3},\n          ""srdop"": {""grid"": ""rho"", ""dims"": 3},\n          ""sldon"": {""grid"": ""rho"", ""dims"": 3},\n          ""sldop"": {""grid"": ""rho"", ""dims"": 3},\n          ""sidet"": {""grid"": ""rho"", ""dims"": 3},\n          ""silg"": {""grid"": ""rho"", ""dims"": 3},\n          ""sio4"": {""grid"": ""rho"", ""dims"": 3},\n          ""nsmz"": {""grid"": ""rho"", ""dims"": 3},\n          ""nmdz"": {""grid"": ""rho"", ""dims"": 3},\n          ""nlgz"": {""grid"": ""rho"", ""dims"": 3}}\n\n# Extra aggregate fields that are required to initialize, but are not\n# provided by the COBALT output\nini_fields = {""chl"": {""grid"": ""rho"", ""dims"": 3},\n              ""irr_mem"": {""grid"": ""rho"", ""dims"": 3},\n              ""htotal"": {""grid"": ""rho"", ""dims"": 3},\n              ""co3_ion"": {""grid"": ""rho"", ""dims"": 3},\n              ""mu_mem_di"": {""grid"": ""rho"", ""dims"": 3},\n              ""mu_mem_sm"": {""grid"": ""rho"", ""dims"": 3},\n              ""mu_mem_lg"": {""grid"": ""rho"", ""dims"": 3}}\n\n# Diagnostic fields that are saved in diagnostic files that are useful\ndia_fields = {""chl"": {""grid"": ""rho"", ""dims"": 3},\n              ""irr_mem"": {""grid"": ""rho"", ""dims"": 3},\n              ""htotal"": {""grid"": ""rho"", ""dims"": 3},\n              ""co3_ion"": {""grid"": ""rho"", ""dims"": 3},\n              ""fe_bulk_flx"": {""grid"": ""rho"", ""dims"": 3},\n              ""omega_cadet_calc"": {""grid"": ""rho"", ""dims"": 3},\n              ""omega_cadet_arag"": {""grid"": ""rho"", ""dims"": 3}}\n\n# Extra fields that are required in the atmospheric forcing\nfrc_fields = {""atmCO2"": {""grid"": ""rho"", ""dims"": 2},\n              ""ironsed"": {""grid"": ""rho"", ""dims"": 2},\n              ""fecoast"": {""grid"": ""rho"", ""dims"": 2},\n              ""solublefe"": {""grid"": ""rho"", ""dims"": 2},\n              ""mineralfe"": {""grid"": ""rho"", ""dims"": 2}}\n\n# Define the vmap for nesting. This simply forms a one-to-one correspondence\n# between the fields.\nvmap = {k: k for k in fields}\n\n# Create a dictionary of CDL files\n_cdl_dir = os.path.dirname(__file__)\ncdl = {""ini"": _cdl_dir + ""/ini.cdl"",\n       ""his"": _cdl_dir + ""/his.cdl"",\n       ""nudge"": _cdl_dir + ""/nudge.cdl"",\n       ""clim"": _cdl_dir + ""/clim.cdl"",\n       ""bry"": _cdl_dir + ""/bry.cdl"",\n       ""frc"": _cdl_dir + ""/frc_bulk.cdl"",\n       ""zlevel"": _cdl_dir + ""/zlevel.cdl""}\n\n\n# Keep track of original ROMS fields\nroms_fields = dict(seapy.roms.fields)\n\n# Helper functions to enable/disable COBALT fields\n\n\ndef add_psource_var(nc, name):\n    """"""\n    Add a list of COBALT variables to an existing netCDF file that\n    has been opened for writing.\n\n    Input\n    -----\n      nc: netCDF4,\n          The netCDF4 river file object to write to.\n          NOTE: it must be opened as writeable.\n      name: string or list of strings,\n          The COBALT variable names to add as point sources to the\n          river file.\n\n    Output\n    ------\n       None\n    """"""\n    from numpy import asarray\n    name = asarray(name)\n    for n in name:\n        var = {""name"": f""river_{n}"",\n               ""type"": ""float"",\n               ""dims"": ""river_time, s_rho, river"",\n               ""attr"": {""units"": ""mol/kg"",\n                        ""long_name"": f""River runoff for {n}"",\n                        ""fields"": f""river {n}, scalar, series""}}\n        seapy.roms.ncgen.add_variable(nc, var)\n\n\ndef enable():\n    """"""\n    Switch seapy to use all fields from ROMS hydrodynamics and COBALT\n    """"""\n    seapy.roms.fields.update(fields)\n\n\ndef disable():\n    """"""\n    Switch seapy to use only fields from ROMS hydrodynamics\n    """"""\n    seapy.roms.fields = dict(roms_fields)\n\n\ndef enable_ini():\n    """"""\n    Switch seapy to use all fields from ROMS hydrodynamics and COBALT ini fields\n    """"""\n    enable()\n    seapy.roms.fields.update(ini_fields)\n\n\ndef disable_ini():\n    """"""\n    Switch seapy to use ROMS and COBALT fields without the ini fields\n    """"""\n    disable()\n    enable()\n\n\nenable()\n'"
