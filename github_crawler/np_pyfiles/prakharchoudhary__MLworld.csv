file_path,api_count,code
classifying_iris/figure1.py,0,"b""\r\nfrom matplotlib import pyplot as plt\r\n\r\nfrom sklearn.datasets import load_iris\r\n\r\ndata = load_iris()\r\nfeatures = data.data\r\nfeature_names = data.feature_names\r\ntarget = data.target\r\ntarget_names = data.target_names\r\n\r\n# subplots shown in 2 rows and 3 columns\r\nfig, axes = plt.subplots(2,3)\r\npairs = [(0,1), (0,2), (0,3), (1, 2), (1, 3), (2, 3)]\r\n\r\n# Set up 3 different pairs of (color, marker)\r\ncolor_markers = [\r\n        ('r', '>'),\r\n        ('g', 'o'),\r\n        ('b', 'x'),\r\n        ]\r\nfor i, (p0,p1) in enumerate(pairs):\r\n\tax = axes.flat[i]\r\n\r\n\tfor t in range(3):\r\n        # Use a different color/marker for each class `t`\r\n\t\tc,marker = color_markers[t]\r\n\t\tax.scatter(features[target == t, p0], features[\r\n\t\t\ttarget == t,p1], marker=marker, c=c)\r\n\r\n\tax.set_xlabel(feature_names[p0])\r\n\tax.set_ylabel(feature_names[p1])\r\n\tax.set_xticks([])\r\n\tax.set_yticks([])\r\nfig.tight_layout()\r\nfig.savefig('./charts/figure1.png')\r\n"""
classifying_iris/load.py,2,"b""\r\nimport numpy as np\r\n\r\ndef load_dataset(dataset_name):\r\n\r\n\tdata = []\r\n\tlabels = []\r\n\twith open('./data/{0}.tsv'.format(dataset_name)) as ifile:\r\n\t\tfor line in ifile:\r\n\t\t\ttokens = line.strip().split('\\t')\r\n\t\t\tdata.append([float(tk) for tk in  tokens[:-1]])\r\n\t\t\tlabels.append(tokens[-1])\r\n\r\n\tdata = np.array(data)\r\n\tlabels = np.array(labels)\r\n\treturn data, labels"""
classifying_iris/main.py,8,"b'\r\nfrom matplotlib import pyplot as plt\r\nimport numpy as np\r\n\r\nfrom sklearn.datasets import load_iris\r\n\r\ndata = load_iris()\r\n\r\n# load_iris returns an object with iris dataset\r\nfeatures = data.data\r\nfeature_names = data.feature_names\r\ntarget = data.target\r\ntarget_names = data.target_names\r\n\r\nfor t in range(3):\r\n\tif t==0:\r\n\t\tc = \'r\'\r\n\t\tmarker = \'>\'\r\n\r\n\telif t==1:\r\n\t\tc = \'g\'\r\n\t\tmarker = \'o\'\r\n\r\n\telif t==2:\r\n\t\tc = \'b\'\r\n\t\tmarker = \'x\'\r\n\r\n\tplt.scatter(features[target == t,0],\r\n\t\t\t\tfeatures[target == t,1],\r\n\t\t\t\tmarker=marker,\r\n\t\t\t\tc=c\r\n\t\t\t\t)\r\n\r\n#we use numpy fancy indexing to get an array of strings:\r\nlabels = target_names[target]\r\n\r\n#The petal length is the feature at position 2\r\nplength = features[:, 2]\r\n\r\n#Build an array of booleans\r\nis_setosa = (labels == \'setosa\')\r\n\r\n#This is the important step:\r\nmax_setosa = plength[is_setosa].max()\r\nmin_non_setosa = plength[~is_setosa].min()\r\nprint(\'Maximum of setosa: {0}.\'.format(max_setosa))\r\nprint (\'Minimum of others: {0}.\'.format(min_non_setosa))\r\n\r\n# ~ is the boolean negation operator\r\nfeatures = features[~is_setosa]\r\nlabels = labels[~is_setosa]\r\n# Build a new target variable, is_virginica\r\nis_virginica = (labels == \'virginica\') \r\n\r\n#initialise the best accuracy to impossibly low value\r\nbest_acc = -1.0\r\nfor fi in range(features.shape[1]):\r\n\t#we are going to test all possible thresholds\r\n\tthresh = features[:, fi]\r\n\tfor t in thresh:\r\n\t\t#get the vectors for feature \'fi\'\r\n\t\tfeature_i = features[:, fi]\r\n\t\t#apply thresholf \'t\'\r\n\t\tpred = (feature_i > t)\r\n\t\tacc = (pred == is_virginica).mean()\r\n\t\trev_acc = (pred == ~is_virginica).mean()\r\n\t\tif rev_acc > acc:\r\n\t\t\treverse = True\r\n\t\t\tacc = rev_acc\r\n\t\telse:\r\n\t\t\treverse = False\r\n\r\n\t\tif acc > best_acc:\r\n\t\t\tbest_acc = acc\r\n\t\t\tbest_fi = fi\r\n\t\t\tbest_t = t\r\n\t\t\tbest_reverse = reverse\r\n\r\ndef is_virginica_test(fi, i, reverse, example):\r\n\t\'Apply threshold model to a new example\'\r\n\ttest = example[fi]>t\r\n\tif reverse:\r\n\t\ttest = not test\r\n\r\n\treturn test\r\n\r\n# Training accuracy was 96.0%.\r\n# Testing accuracy was 90.0% (N = 50).\r\n\r\nfrom threshold import fit_model, accuracy, predict\r\ncorrect = 0.0 \r\nfor ei in range(len(features)):\r\n\t# select all but the one at position \'ei\'\r\n\ttraining = np.ones(len(features), bool)\r\n\ttraining[ei] = False\r\n\ttesting = ~training\r\n\tmodel = fit_model(features[training], is_virginica[training])\r\n\tpredictions = predict(model, features[testing])\r\n\tcorrect += np.sum(predictions == is_virginica[testing])\r\n\r\nacc = correct/float(len(features))\r\nprint(\'Accuracy: {0:.1%}\'.format(acc))\r\n\r\n##############SEEDS DATASET################################\r\nfrom load import load_dataset\r\n\r\nfeature_names = [\r\n    \'area\',\r\n    \'perimeter\',\r\n    \'compactness\',\r\n    \'length of kernel\',\r\n    \'width of kernel\',\r\n    \'asymmetry coefficien\',\r\n    \'length of kernel groove\',\r\n]\r\n\r\nfeatures, labels = load_dataset(\'seeds\')\r\n\r\n\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nclassifier = KNeighborsClassifier(n_neighbors=1)\r\n\r\nfrom sklearn.cross_validation import KFold\r\n\r\nkf = KFold(len(features), n_folds=5, shuffle=True)\r\n# \'means\' will be a lidt of mean accuracies.\r\nmeans = []\r\nfor training, testing in kf:\r\n\t# We fit a model to this fold and then apply it to the\r\n\t# testing data with \'predict\':\r\n\r\n\tclassifier.fit(features[training], labels[training])\r\n\tprediction = classifier.predict(features[testing])\r\n\r\n\t# np.mean on an arrays of booleans returns a fraction of\r\n\t# correct decisions for this fold:\r\n\tcurmean = np.mean(prediction == labels[testing])\r\n\tmeans.append(curmean)\r\n\r\nprint(""Mean accuracy: {:.1%}"".format(np.mean(means)))\r\n\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\nclassifier = KNeighborsClassifier(n_neighbors=1)\r\nclassifier = Pipeline([(\'norm\', StandardScaler()), (\'knn\', classifier)])\r\n\r\nmeans = []\r\nfor training, testing in kf:\r\n\t# We fit a model to this fold and then apply it to the\r\n\t# testing data with \'predict\':\r\n\r\n\tclassifier.fit(features[training], labels[training])\r\n\tprediction = classifier.predict(features[testing])\r\n\r\n\t# np.mean on an arrays of booleans returns a fraction of\r\n\t# correct decisions for this fold:\r\n\tcurmean = np.mean(prediction == labels[testing])\r\n\tmeans.append(curmean)\r\n\r\nprint(""Mean accuracy: {:.1%}"".format(np.mean(means)))\r\n'"
classifying_iris/threshold.py,1,"b""import numpy as np\r\n\r\n\r\n# This function was called ``learn_model`` in the first edition\r\ndef fit_model(features, labels):\r\n    '''Learn a simple threshold model'''\r\n    best_acc = -1.0\r\n    # Loop over all the features:\r\n    for fi in range(features.shape[1]):\r\n        thresh = features[:, fi].copy()\r\n        # test all feature values in order:\r\n        thresh.sort()\r\n        for t in thresh:\r\n            pred = (features[:, fi] > t)\r\n\r\n            # Measure the accuracy of this \r\n            acc = (pred == labels).mean()\r\n\r\n            rev_acc = (pred == ~labels).mean()\r\n            if rev_acc > acc:\r\n                acc = rev_acc\r\n                reverse = True\r\n            else:\r\n                reverse = False\r\n            if acc > best_acc:\r\n                best_acc = acc\r\n                best_fi = fi\r\n                best_t = t\r\n                best_reverse = reverse\r\n\r\n    # A model is a threshold and an index\r\n    return best_t, best_fi, best_reverse\r\n\r\n# This function was called ``apply_model`` in the first edition\r\ndef predict(model, features):\r\n    '''Apply a learned model'''\r\n    # A model is a pair as returned by fit_model\r\n    t, fi, reverse = model\r\n    if reverse:\r\n        return features[:, fi] <= t\r\n    else:\r\n        return features[:, fi] > t\r\n\r\ndef accuracy(features, labels, model):\r\n    '''Compute the accuracy of the model'''\r\n    preds = predict(model, features)\r\n    return np.mean(preds == labels)\r\n"""
finding_related_posts/noise_analysis.py,0,"b'import sklearn.datasets\r\n\r\ngroups = [\r\n    \'comp.graphics\', \'comp.os.ms-windows.misc\', \'comp.sys.ibm.pc.hardware\',\r\n    \'comp.sys.mac.hardware\', \'comp.windows.x\', \'sci.space\']\r\ntrain_data = sklearn.datasets.fetch_20newsgroups(subset=""train"",\r\n                                                 categories=groups)\r\n\r\nlabels = train_data.target\r\nnum_clusters = 50 \r\n\r\nimport nltk.stem\r\nenglish_stemmer = nltk.stem.SnowballStemmer(\'english\')\r\n\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\n\r\n\r\nclass StemmedTfidfVectorizer(TfidfVectorizer):\r\n\r\n    def build_analyzer(self):\r\n        analyzer = super(TfidfVectorizer, self).build_analyzer()\r\n        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\r\n\r\nvectorizer = StemmedTfidfVectorizer(min_df=10, max_df=0.5,\r\n                                    stop_words=\'english\', decode_error=\'ignore\'\r\n                                    )\r\nvectorized = vectorizer.fit_transform(train_data.data)\r\n\r\npost_group = zip(train_data.data, train_data.target)\r\n\r\nall = [(len(post[0]), post[0], train_data.target_names[post[1]]) for post in post_group]\r\n\r\ngraphics = sorted([post for post in all if post[2] == \'comp.graphics\'])\r\nprint(graphics[5])\r\n# (245, \'From: SITUNAYA@IBM3090.BHAM.AC.UK\\nSubject: test....(sorry)\\nOrganization:\r\n# The University of Birmingham, United Kingdom\\nLines: 1\\nNNTP-Posting-Host: ibm3090.bham.ac.uk\r\n# \\n\\n==============================================================================\\n\',\r\n# \'comp.graphics\')\r\n\r\nnoise_post = graphics[5][1]\r\n\r\nanalyzer = vectorizer.build_analyzer()\r\nprint(list(analyzer(noise_post)))\r\n\r\nuseful = set(analyzer(noise_post)).intersection(vectorizer.get_feature_names())\r\nprint(sorted(useful))\r\n\r\nfor term in sorted(useful):\r\n\tprint(\'IDF(%s)=%.2f\' % (term, vectorizer._tfidf.idf_[vectorizer.vocabulary_[term]]))'"
finding_related_posts/rel_posts_20newsgroup.py,0,"b'\r\nimport sklearn.datasets\r\nimport scipy as sp\r\n\r\nall_data = sklearn.datasets.fetch_20newsgroups(subset=""all"")\r\nprint(""Number of total posts: %i"" % len(all_data.filenames))\r\n\r\ngroups = [\r\n    \'comp.graphics\', \'comp.os.ms-windows.misc\', \'comp.sys.ibm.pc.hardware\',\r\n    \'comp.sys.mac.hardware\', \'comp.windows.x\', \'sci.space\']\r\n\r\ntrain_data = sklearn.datasets.fetch_20newsgroups(subset=\'train\', categories=groups)\r\nprint len(train_data.filenames)\r\n\r\ntest_data = sklearn.datasets.fetch_20newsgroups(subset=\'test\', categories=groups)\r\nprint len(test_data.filenames)\r\n\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer \r\nimport nltk.stem\r\nenglish_stemmer = nltk.stem.SnowballStemmer(\'english\')\r\n\r\nclass StemmedTfidfVectorizer(TfidfVectorizer):\r\n\r\n\tdef build_analyzer(self):\r\n\t\tanalyzer = super(TfidfVectorizer, self).build_analyzer()\r\n\t\treturn lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\r\n\r\nvectorizer = StemmedTfidfVectorizer(min_df=10, max_df=0.5, stop_words=\'english\', decode_error=\'ignore\')\r\nvectorized = vectorizer.fit_transform(train_data.data)\r\nnum_samples, num_features = vectorized.shape\r\nprint(""#samples: %d, #features: %d"" % (num_samples, num_features))\r\n\r\nnum_clusters = 50\r\nfrom sklearn.cluster import KMeans\r\nkm = KMeans(n_clusters=num_clusters, n_init=1, verbose=3, random_state=3)\r\nkm.fit(vectorized)\r\n\r\nprint(""km.labels_=%s"" % km.labels_)\r\n\r\nprint(""km.labels_.shape=%s"" % km.labels_.shape)\r\n\r\nnew_post = \\\r\n    """"""Disk drive problems. Hi, I have a problem with my hard disk.\r\nAfter 1 year it is working only sporadically now.\r\nI tried to format it, but now it doesn\'t boot any more.\r\nAny ideas? Thanks.\r\n""""""\r\n\r\nnew_post_vec = vectorizer.transform([new_post])\r\nnew_post_label = km.predict(new_post_vec)[0]\r\n\r\nsimilar_indices = (km.labels_ == new_post_label).nonzero()[0]\r\n\r\nsimilar = []\r\nfor i in similar_indices:\r\n    dist = sp.linalg.norm((new_post_vec - vectorized[i]).toarray())\r\n    similar.append((dist, train_data.data[i]))\r\n\r\nsimilar = sorted(similar)\r\nprint(""Count similar: %i"" % len(similar))\r\n\r\nshow_at_1 = similar[0]\r\nshow_at_2 = similar[int(len(similar) / 10)]\r\nshow_at_3 = similar[int(len(similar) / 2)]\r\n\r\nprint(""=== #1 ==="")\r\nprint(show_at_1)\r\nprint()\r\n\r\nprint(""=== #2 ==="")\r\nprint(show_at_2)\r\nprint()\r\n\r\nprint(""=== #3 ==="")\r\nprint(show_at_3)\r\nprint()\r\n\r\n\r\n'"
finding_related_posts/related_posts.py,0,"b'\r\nimport os\r\nimport numpy as np\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\nfrom utils import DATA_DIR\r\n\r\nfrom nltk.stem import SnowballStemmer\r\nenglish_stemmer = SnowballStemmer(\'english\')\r\n\r\nclass StemmedCountVectorizer(CountVectorizer):\r\n\tdef build_analyzer(self):\r\n\t\tanalyzer = super(StemmedCountVectorizer, self).build_analyzer()\r\n\t\treturn lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\r\n\r\nclass StemmedTfidfVectorizer(TfidfVectorizer):\r\n\tdef build_analyzer(self):\r\n\t\tanalyzer = super(TfidfVectorizer, self).build_analyzer()\r\n\t\treturn lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\r\n\r\nvectorizer = StemmedTfidfVectorizer(min_df=1, stop_words=""english"", decode_error=""ignore"")  #stop_words is used to not consider those words which occur quite frequently that too in varying contexts.\r\n# print(vectorizer)\r\n\r\n# content = [""How to format my hard disk"", ""Hard disk format problems""]\r\n# X = vectorizer.fit_transform(content)\r\n# print vectorizer.get_feature_names()\r\n# print(X.toarray().transpose())\r\n\r\nposts = [open(os.path.join(DATA_DIR, f)).read() for f in os.listdir(DATA_DIR)]\t\r\n\r\nX_train = vectorizer.fit_transform(posts)\r\nnum_samples, num_features = X_train.shape\r\nprint(""#samples: %d, #features: %d"" % (num_samples, num_features))\r\n\r\nprint(vectorizer.get_feature_names())\r\n\r\nnew_post = ""imaging databases""\r\nnew_post_vec = vectorizer.transform([new_post])\r\n\r\nprint new_post_vec \r\n#(a,b) is the location of the feature or each word in new post and it is followed by the number of times it occurs\r\nprint new_post_vec.toarray() \r\n#returns a one-d matrix which has the number of times each feature occurs in the new post\r\n\r\n#===================== Calculating the Euclidean Distance ========================================#\r\nimport scipy as sp\r\n\r\ndef dist_raw(v1, v2):\r\n\tdelta = v1-v2\r\n\treturn sp.linalg.norm(delta.toarray())\r\n\t# norm() function calculates the euclidean function.\r\n\r\n\r\n# print(X_train.getrow(3).toarray())\r\n# print(X_train.getrow(4).toarray())\r\n\r\n# Both the files 04 and 05 have the same text. Only file 04 has every word repeated thrice.\r\n# So they should have the same distance whch is not the case, hence simply finding the euclidean distance\r\n# work. We must normalize the data.\r\n\r\n#================================= Normalizing the word count vectors =================================#\r\n\r\n\r\ndef dist_norm(v1, v2):\r\n\tv1_normalised = v1/sp.linalg.norm(v1.toarray())\r\n\tv2_normalised = v2/sp.linalg.norm(v2.toarray())\r\n\tdelta = v1_normalised - v2_normalised\r\n\treturn sp.linalg.norm(delta.toarray())\r\n\r\n\r\nimport sys\r\nbest_doc = None\r\nbest_dist = sys.maxint\r\nbest_i = None\r\nfor i, post in enumerate(posts):\r\n\tif post == new_post:\r\n\t\tcontinue\r\n\tpost_vec = X_train.getrow(i)\r\n\td = dist_norm(post_vec, new_post_vec)\r\n\tprint(""=== Post %i with dist = %.2f: %s"" % (i, d, post))\r\n\r\n\tif d<best_dist:\r\n\t\tbest_dist = d\r\n\t\tbest_i = i\r\nprint(""Best post is %i with dist=%.2f""%(best_i, best_dist))'"
finding_related_posts/tfidf.py,0,"b'\r\nimport scipy as sp\r\n\r\n\r\ndef tfidf(t, d, D):\r\n    tf = float(d.count(t)) / sum(d.count(w) for w in set(d))\r\n    idf = sp.log(float(len(D)) / (len([doc for doc in D if t in doc])))\r\n    return tf * idf\r\n\r\n\r\na, abb, abc = [""a""], [""a"", ""b"", ""b""], [""a"", ""b"", ""c""]\r\nD = [a, abb, abc]\r\n\r\nprint(tfidf(""a"", a, D))\r\nprint(tfidf(""b"", abb, D))\r\nprint(tfidf(""a"", abc, D))\r\nprint(tfidf(""b"", abc, D))\r\nprint(tfidf(""c"", abc, D))\r\n\r\n'"
finding_related_posts/utils.py,0,"b'\r\nimport os\r\nimport sys\r\n\r\nDATA_DIR = os.path.join(\r\n    os.path.dirname(os.path.realpath(__file__)), ""data"")\r\n\r\nif not os.path.exists(DATA_DIR):\r\n    print(""Uh, we were expecting a data directory, which contains the toy data"")\r\n    sys.exit(1)\r\n\r\nCHART_DIR = os.path.join(\r\n    os.path.dirname(os.path.realpath(__file__)), ""charts"")\r\nif not os.path.exists(CHART_DIR):\r\n    os.mkdir(CHART_DIR)\r\n'"
movie_reviews/NNmodel.py,0,"b'from keras import models\nfrom keras import layers\nfrom keras import optimizers\nfrom matplotlib import pyplot as plt\nimport pickle\n\n\nclass NeuralNet:\n\n    def __init__(self, x_train, y_train, lr=0.001):\n        self.model = models.Sequential()\n        self.x_train = x_train\n        self.y_train = y_train\n        self.partial_x_train = None\n        self.partial_y_train = None\n        self.x_val = None\n        self.y_val = None\n        self.lr = lr\n        self.history = None\n\n    def divide_data(self):\n        self.x_val = self.x_train[:10000]\n        self.y_val = self.y_train[:10000]\n        self.partial_x_train = self.x_train[10000:]\n        self.partial_y_train = self.y_train[10000:]\n\n    def network(self):\n        self.model.add(layers.Dense(\n            16, activation=\'relu\', input_shape=(10000,)))\n        self.model.add(layers.Dense(16, activation=\'relu\'))\n        self.model.add(layers.Dense(1, activation=\'sigmoid\'))\n        self.model.compile(optimizer=optimizers.RMSprop(self.lr),\n                           loss=\'binary_crossentropy\',\n                           metrics=[\'accuracy\'])\n\n    def train_model(self):\n        self.history = self.model.fit(self.partial_x_train,\n                                      self.partial_y_train,\n                                      epochs=20,\n                                      batch_size=512,\n                                      validation_data=(self.x_val, self.y_val))\n        self.save_model()\n        with open(""history.pkl"", ""wb"") as file:\n            pickle.dump(self.history.history, file)\n\n    def save_model(self):\n        model_json = self.model.to_json()\n        with open(""model.json"", ""w"") as json_file:\n            json_file.write(model_json)\n        # serialize weights to HDF5\n        self.model.save_weights(""model.h5"")\n        print(""Saved model to disk"")\n\n    def load_model(self):\n        json_file = open(\'model.json\', \'r\')\n        loaded_model_json = json_file.read()\n        json_file.close()\n        loaded_model = model_from_json(loaded_model_json)\n        # load weights into new model\n        loaded_model.load_weights(""model.h5"")\n        print(""Loaded model from disk"")\n'"
movie_reviews/__init__.py,0,b''
movie_reviews/plot_graphs.py,0,"b'import pickle\nfrom matplotlib import pyplot as plt\n\nfile = open(""history.pkl"", \'rb\')\nhistory_dict = pickle.load(file)\nloss_values = history_dict[\'loss\']\nval_loss_values = history_dict[\'val_loss\']\nepochs = range(1, 21)\nplt.plot(epochs, loss_values, \'bo\', label=\'Training loss\')\nplt.plot(epochs, val_loss_values,\n         \'b\', label=\'Validation loss\')\nplt.title(\'Training and validation loss\')\nplt.xlabel(\'Epochs\')\nplt.ylabel(\'Loss\')\nplt.legend()\nplt.savefig(""loss_graph.png"", dpi=150)\n\n# plot the training and validation accuracy\nplt.clf()\nloss_values = history_dict[\'acc\']\nval_loss_values = history_dict[\'val_acc\']\nplt.plot(epochs, loss_values, \'bo\',\n         label=\'Training Accuracy\')\nplt.plot(epochs, val_loss_values, \'b\',\n         label=\'Validation Accuracy\')\nplt.title(\'Training and validation Accuracy\')\nplt.xlabel(\'Epochs\')\nplt.ylabel(\'Accuracy\')\nplt.legend()\nplt.savefig(""acc_graph.png"", dpi=150)\n'"
movie_reviews/train.py,3,"b'from keras.datasets import imdb\nimport matplotlib.pyplot as plt\nimport NNmodel\nimport numpy as np\n\n(train_data, train_labels), (test_data,\n                             test_labels) = imdb.load_data(num_words=10000)\n\n\ndef vectorize_data(sequences, dimensions=10000):\n    """""" Vectorize the array of integers per document """"""\n    results = np.zeros((len(sequences), dimensions))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.\n    return results\n\n# Preparing the train and test feature set\nX_train = vectorize_data(train_data)\nX_test = vectorize_data(test_data)\n\n# Preparing the train and test labels\ny_train = np.asarray(train_labels).astype(\'float32\')\ny_test = np.asarray(test_labels).astype(\'float32\')\n\nnetwork = NNmodel.NeuralNet(X_train, y_train)\nnetwork.divide_data()\nnetwork.network()\nnetwork.train_model()\n'"
multi_class_news_classification/__init__.py,0,b'from matplotlib import pyplot as plt\nimport pickle\n'
multi_class_news_classification/nnet.py,0,"b'from keras import models\nfrom keras import layers\nfrom keras import optimizers\nfrom keras.models import model_from_json\nimport pickle\n\n\nclass Model:\n    """"""A simple neural network to classify the newswires, as per multiple labels""""""\n\n    def __init__(self, data, labels, lr=0.001):\n        """"""\n        - Initialize all the variable to be used by the Neural Network.\n        - Split the data and labels into training and validation set.\n        """"""\n        self.train_data = data[1000:]\n        self.train_labels = labels[1000:]\n        self.val_data = data[:1000]\n        self.val_labels = labels[:1000]\n        self.model = models.Sequential()\n        self.lr = lr\n        self.history = None\n\n    def network(self):\n        """"""\n        A three layer Neural Network.\n        - Input layer : 64 nodes, ReLU activation\n        - Hidden layer : 64 nodes, ReLU activation\n        - Output layer : 64 nodes, \'softmax\' activation\n        - Optimizer : RMSprop\n        - Loss function : Categorical Cross-entropy\n        - Metrics : Accuracy\n        """"""\n        self.model.add(layers.Dense(\n            64, activation=\'relu\', input_shape=(10000,)))\n        self.model.add(layers.Dense(64, activation=\'relu\'))\n        self.model.add(layers.Dense(46, activation=\'softmax\'))\n        self.model.compile(optimizer=optimizers.RMSprop(self.lr),\n                           loss=\'categorical_crossentropy\',\n                           metrics=[\'accuracy\'])\n\n    def train_model(self):\n        """"""Train the model and store the details.""""""\n        self.history = self.model.fit(self.train_data,\n                                      self.train_labels,\n                                      epochs=20,\n                                      batch_size=512,\n                                      validation_data=(self.val_data, self.val_labels))\n        self.save_model()\n        with open(""history.pkl"", ""wb"") as file:\n            pickle.dump(self.history.history, file)\n\n    def save_model(self):\n        """"""Save the model architecture in a json file and store weights in h5 format.""""""\n        model_json = self.model.to_json()\n        with open(""model.json"", ""w"") as json_file:\n            json_file.write(model_json)\n        # serialize weights to HDF5\n        self.model.save_weights(""model.h5"")\n        print(""Saved model to disk"")\n\n    def load_model(self):\n        """"""Load the model to use for prediction on new data.""""""\n        json_file = open(\'model.json\', \'r\')\n        loaded_model_json = json_file.read()\n        json_file.close()\n        loaded_model = model_from_json(loaded_model_json)\n        # load weights into new model\n        loaded_model.load_weights(""model.h5"")\n        self.model = loaded_model\n        print(""Loaded model from disk"")\n'"
multi_class_news_classification/plot_graph.py,0,"b'import pickle\nfrom matplotlib import pyplot as plt\n\nfile = open(""history.pkl"", \'rb\')\nhistory_dict = pickle.load(file)\nloss_values = history_dict[\'loss\']\nval_loss_values = history_dict[\'val_loss\']\nepochs = range(1, 21)\nplt.plot(epochs, loss_values, \'bo\', label=\'Training loss\')\nplt.plot(epochs, val_loss_values,\n         \'b\', label=\'Validation loss\')\nplt.title(\'Training and validation loss\')\nplt.xlabel(\'Epochs\')\nplt.ylabel(\'Loss\')\nplt.legend()\nplt.savefig(""loss_graph.png"", dpi=150)\n\n# plot the training and validation accuracy\nplt.clf()\nloss_values = history_dict[\'acc\']\nval_loss_values = history_dict[\'val_acc\']\nplt.plot(epochs, loss_values, \'bo\',\n         label=\'Training Accuracy\')\nplt.plot(epochs, val_loss_values, \'b\',\n         label=\'Validation Accuracy\')\nplt.title(\'Training and validation Accuracy\')\nplt.xlabel(\'Epochs\')\nplt.ylabel(\'Accuracy\')\nplt.legend()\nplt.savefig(""acc_graph.png"", dpi=150)\n'"
multi_class_news_classification/test.py,0,b''
multi_class_news_classification/train.py,2,"b'import numpy as np\nfrom keras.datasets import reuters\nimport nnet\nimport pickle\n\n# load the dataset and prepare train and test data\n(train_data, train_labels), (test_data, test_labels) = \\\n    reuters.load_data(num_words=10000)\n\n# decoding newswires back to text\nword_index = reuters.get_word_index()\nreverse_word_index = dict([(value, word)\n                           for word, value in word_index.items()])\ndecoded_newswire = \' \'.join(\n    [reverse_word_index.get(i - 3, \'?\') for i in train_data[0]])\n\n\n# vectorizing data\ndef vectorize_sequences(sequences, dimensions=10000):\n    results = np.zeros((len(sequences), dimensions))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.\n    return results\n\nX_train = vectorize_sequences(train_data)\nX_test = vectorize_sequences(test_data)\n\n# test to check encoding of data\nassert len(X_train[0]) == 10000\n\n\n# One-hot encoding the labels\ndef one_hot_encoding(labels, dims=46):\n    results = np.zeros((len(labels), dims))\n    for i, label in enumerate(labels):\n        results[i, label] = 1.\n    return results\n\none_hot_train_labels = one_hot_encoding(train_labels)\none_hot_test_labels = one_hot_encoding(test_labels)\n\n# test to check encoding of labels\nassert len(one_hot_train_labels[0]) == 46\n\n\n# Train the model\nnetwork = nnet.Model(X_train, one_hot_train_labels)\nnetwork.network()\nnetwork.train_model()\n\n# evaluate and print results\nresults = network.model.evaluate(X_test, one_hot_test_labels)\nprint(""The results are: "", str(results))\n'"
np_pd/analyze_webstat.py,0,"b'\r\nimport os\r\nfrom utils import DATA_DIR, CHART_DIR\r\nimport scipy as sp\r\nimport matplotlib.pyplot as plt\r\n\r\nsp.random.seed(3)\t# to reproduce the data later on\r\n\r\ndata = sp.genfromtxt(os.path.join(DATA_DIR,""web_traffic.tsv""), delimiter=""\\t"")\r\n\r\ncolors = [\'g\', \'k\', \'b\', \'m\', \'r\']\r\nlinestyles = [\'-\', \'-.\', \'--\', \':\', \'-\']\r\n\r\nx = data[:,0]\r\ny = data[:,1]\r\nx = x[~sp.isnan(y)]\r\ny = y[~sp.isnan(y)]\r\n\r\ndef plot_models(x, y, models, fname, mx=None, ymax=None, xmin=None):\r\n\t\'\'\'plot input data\'\'\'\r\n\r\n\tplt.figure(num=None, figsize=(8, 6))\r\n\tplt.clf()\r\n\tplt.scatter(x, y, s=10)\r\n\tplt.title(""Web traffic over the last month"")\r\n\tplt.xlabel(""Time"")\r\n\tplt.ylabel(""Hits/hour"")\r\n\tplt.xticks(\r\n\t[w * 7 * 24 for w in range(10)], [\'week %i\' % w for w in range(10)])\r\n\r\n\tif models:\r\n\t\tif mx is None:\r\n\t\t\tmx = sp.linspace(0, x[-1], 1000)\r\n\t\tfor model, style, color in zip(models, linestyles, colors):\r\n\t\t\t# print ""Model:"",model\r\n\t\t\t# print ""Coeffs:"",model.coeffs\r\n\t\t\tplt.plot(mx, model(mx), linestyle=style, linewidth=2, c=color)\r\n\r\n\t\tplt.legend([""d=%i"" % m.order for m in models], loc=""upper left"")\r\n\r\n\tplt.autoscale(tight=True)\r\n\tplt.ylim(ymin=0)\r\n\tif ymax:\r\n\t\tplt.ylim(ymax=ymax)\r\n\tif xmin:\r\n\t\tplt.xlim(xmin=xmin)\r\n\tplt.grid(True, linestyle=\'-\', color=\'0.75\')\r\n\tplt.savefig(fname)\r\n\r\n# first look at the model\r\nplot_models(x, y, None, os.path.join(CHART_DIR, ""1400_01_01.png""))\r\n\r\n#creating and plotting models\r\nfp1, res1, rank1, sv1, rcond1 = sp.polyfit(x, y, 1, full=True)\r\nprint(""Model parameters of fp1: %s"" % fp1)\r\nprint(""Error of the model of fp1:"", res1)\r\nf1 = sp.poly1d(fp1)\r\n\r\nfp2, res2, rank2, sv2, rcond2 = sp.polyfit(x, y, 2, full=True)\r\nprint(""Model parameters of fp2: %s"" % fp2)\r\nprint(""Error of the model of fp2:"", res2)\r\nf2 = sp.poly1d(fp2)\r\nf3 = sp.poly1d(sp.polyfit(x, y, 3))\r\nf10 = sp.poly1d(sp.polyfit(x, y, 10))\r\nf100 = sp.poly1d(sp.polyfit(x, y, 100))\r\n\r\nplot_models(x, y, [f1], os.path.join(CHART_DIR, ""1400_01_02.png""))\r\nplot_models(x, y, [f1, f2], os.path.join(CHART_DIR, ""1400_01_03.png""))\r\nplot_models(\r\n    x, y, [f1, f2, f3, f10, f100], os.path.join(CHART_DIR, ""1400_01_04.png""))\r\n\r\n# fit and plot a model using the knowledge about inflection point\r\ninflection = 3.5 * 7 * 24\r\nxa = x[:inflection]\r\nya = y[:inflection]\r\nxb = x[inflection:]\r\nyb = y[inflection:]\r\n\r\nfa = sp.poly1d(sp.polyfit(xa, ya, 1))\r\nfb = sp.poly1d(sp.polyfit(xb, yb, 1))\r\n\r\nplot_models(x, y, [fa, fb], os.path.join(CHART_DIR, ""1400_01_05.png""))\r\n\r\ndef error(f, x, y):\r\n\r\n\treturn sp.sum((f(x)-y)**2)\r\n\r\nprint(""Errors for the complete data set:"")\r\nfor f in [f1, f2, f3, f10, f100]:\r\n    print(""Error d=%i: %f"" % (f.order, error(f, x, y)))\r\n\r\nprint(""Errors for only the time after inflection point"")\r\nfor f in [f1, f2, f3, f10, f100]:\r\n    print(""Error d=%i: %f"" % (f.order, error(f, xb, yb)))\r\n\r\nprint(""Error inflection=%f"" % (error(fa, xa, ya) + error(fb, xb, yb)))\r\n\r\n# extrapolating into the future\r\nplot_models(\r\n    x, y, [f1, f2, f3, f10, f100],\r\n    os.path.join(CHART_DIR, ""1400_01_06.png""),\r\n    mx=sp.linspace(0 * 7 * 24, 6 * 7 * 24, 100),\r\n    ymax=10000, xmin=0 * 7 * 24)\r\n\r\nprint(""Trained only on data after inflection point"")\r\nfb1 = fb\r\nfb2 = sp.poly1d(sp.polyfit(xb, yb, 2))\r\nfb3 = sp.poly1d(sp.polyfit(xb, yb, 3))\r\nfb10 = sp.poly1d(sp.polyfit(xb, yb, 10))\r\nfb100 = sp.poly1d(sp.polyfit(xb, yb, 100))\r\n\r\nprint(""Errors for only the time after inflection point"")\r\nfor f in [fb1, fb2, fb3, fb10, fb100]:\r\n    print(""Error d=%i: %f"" % (f.order, error(f, xb, yb)))\r\n\r\nplot_models(\r\n    x, y, [fb1, fb2, fb3, fb10, fb100],\r\n    os.path.join(CHART_DIR, ""1400_01_07.png""),\r\n    mx=sp.linspace(0 * 7 * 24, 6 * 7 * 24, 100),\r\n    ymax=10000, xmin=0 * 7 * 24)\r\n\r\n# separating training from testing data\r\nfrac = 0.3\r\nsplit_idx = int(frac * len(xb))\r\nshuffled = sp.random.permutation(list(range(len(xb))))\r\ntest = sorted(shuffled[:split_idx])\r\ntrain = sorted(shuffled[split_idx:])\r\nfbt1 = sp.poly1d(sp.polyfit(xb[train], yb[train], 1))\r\nfbt2 = sp.poly1d(sp.polyfit(xb[train], yb[train], 2))\r\nprint(""fbt2(x)= \\n%s"" % fbt2)\r\nprint(""fbt2(x)-100,000= \\n%s"" % (fbt2-100000))\r\nfbt3 = sp.poly1d(sp.polyfit(xb[train], yb[train], 3))\r\nfbt10 = sp.poly1d(sp.polyfit(xb[train], yb[train], 10))\r\nfbt100 = sp.poly1d(sp.polyfit(xb[train], yb[train], 100))\r\n\r\nprint(""Test errors for only the time after inflection point"")\r\nfor f in [fbt1, fbt2, fbt3, fbt10, fbt100]:\r\n    print(""Error d=%i: %f"" % (f.order, error(f, xb[test], yb[test])))\r\n\r\nplot_models(\r\n    x, y, [fbt1, fbt2, fbt3, fbt10, fbt100],\r\n    os.path.join(CHART_DIR, ""1400_01_08.png""),\r\n    mx=sp.linspace(0 * 7 * 24, 6 * 7 * 24, 100),\r\n    ymax=10000, xmin=0 * 7 * 24)\r\n\r\nfrom scipy.optimize import fsolve\r\nprint(fbt2)\r\nprint(fbt2 - 100000)\r\nreached_max = fsolve(fbt2 - 100000, x0=800) / (7 * 24)\r\nprint(""100,000 hits/hour expected at week %f"" % reached_max[0])\r\n'"
np_pd/plotData.py,0,"b'import scipy as sp\r\ndata = sp.genfromtxt(""../web_traffic.tsv"", delimiter=""\\t"")\r\nx = data[:,0]\r\ny = data[:,1]\r\nx = x[~sp.isnan(y)]\r\ny = y[~sp.isnan(y)]\r\nimport matplotlib.pyplot as plt\r\nplt.scatter(x,y,s=10)\r\nplt.title(""Web Traffic over the last month"")\r\nplt.xlabel(""Time"")\r\nplt.ylabel(""Hits/hour"")\r\nplt.xticks([w*7*24 for w in range(10)], [\'week %i\' % w for w in range(10)])\r\nplt.autoscale(tight=True)\r\nplt.grid(True, linestyle=\'-\', color=\'0.75\')\r\nplt.show()'"
np_pd/random_data.py,0,"b'from random import randint\r\nf = open(""web_traffic.tsv"", ""w+"")\r\n\r\nfor i in range(1, 1000):\r\n\tif i<500:\r\n\t\tx = ""%d\\t%d\\n""%(i, randint(i, 200+i))\r\n\r\n\tf.write(x)\r\n\r\nf.close()'"
np_pd/utils.py,0,"b'# This code is supporting material for the book\r\n# Building Machine Learning Systems with Python\r\n# by Willi Richert and Luis Pedro Coelho\r\n# published by PACKT Publishing\r\n#\r\n# It is made available under the MIT License\r\n\r\nimport os\r\n\r\nDATA_DIR = os.path.join(\r\n    os.path.dirname(os.path.realpath(__file__)), ""data"")\r\n\r\nCHART_DIR = os.path.join(\r\n    os.path.dirname(os.path.realpath(__file__)), ""charts"")\r\n\r\nfor d in [DATA_DIR, CHART_DIR]:\r\n    if not os.path.exists(d):\r\n        os.mkdir(d)'"
poor_answers/classify.py,2,"b'\ndef fetch_posts():\n\tfor line in open(""data.tsv"", ""r""):\n\t\tpost_id, text = line.split(""\\t"")\n\t\tyield int(post_id), text.strip()\n\nimport re\nimport numpy as np\nfrom sklearn import neighbors\n\n# knn = neighbors.KNeighborsClassfier(n_neighbors=2)\n# print knn\n\ncode_match = re.compile(\'<pre>(.*?)</pre>\',re.MULTILINE | re.DOTALL)\nlink_match = re.compile(\'<a href=""http://.*?"".*?>(.*?)</a>\',re.MULTILINE | re.DOTALL)\ntag_match = re.compile(\'<[^>]*>\',re.MULTILINE | re.DOTALL)\n\ndef extract_features_from_body(s):\n\n\tlink_count_in_code = 0\n\t#count links in code to later subtract them\n\tfor match_str in code_match.findall(s):\n\t\tlink_count_in_code += len(link_match.findall(match_str))\n\n\treturn len(link_match.findall(s)) - link_count_in_code\n\n\n## Training the classifier\nX = np.asarray([extract_features_from_body(text) for post_id, text in fetch_posts() if post_id in all_answers])\nknn = neighbors.KNeighborsClassifier()\nknn.fit(X, Y)\n\n###### Measuring the classifier\'s performance\nfrom sklearn.cross_validation import KFold\nscores=[]\n\ncv = KFold(n=len(X), k=10, indices=True)\n\nfor train, test in cv:\n\tX_train, y_train = X[train], Y[train]\n\tX_test, y_test = X[test], Y[test]\n\tclf = neighbors.KNeighborsClassifier()\n\tclf.fit(X,Y)\n\tscores.append(clf.score(X_test, y_test))\n\nprint(""Mean(scores)=%.5f\\tStddev(scores)=%.5ff""%(np.mean(scores), np.std(scores)))'"
