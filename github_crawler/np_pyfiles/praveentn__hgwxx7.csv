file_path,api_count,code
datascience/ds_python_outliers_1.py,0,"b'# removing outliers\n# if value is greater than (mean - 2* standard deviation) OR\n# less than (mean + 2* standard deviation)\n\n>>> import numpy\n\n>>> arr = [10, 386, 479, 627, 20, 523, 482, 483, 542, 699, 535, 617, 577, \n           471, 615, 583, 441, 562, 563, 527, 453, 530, 433, 541, 585, 704, \n           443, 569, 430, 637, 331, 511, 552, 496, 484, 566, 554, 472, 335, \n           440, 579, 341, 545, 615, 548, 604, 439, 556, 442, 461, 624, 611, \n           444, 578, 405, 487, 490, 496, 398, 512, 422, 455, 449, 432, 607, \n           679, 434, 597, 639, 565, 415, 486, 668, 414, 665, 763, 557, 304, \n           404, 454, 689, 610, 483, 441, 657, 590, 492, 476, 437, 483, 529, \n           363, 711, 543]\n\n>>> elements = numpy.array(arr)\n[ 10 386 479 627  20 523 482 483 542 699 535 617 577 471 615 583 441 562\n 563 527 453 530 433 541 585 704 443 569 430 637 331 511 552 496 484 566\n 554 472 335 440 579 341 545 615 548 604 439 556 442 461 624 611 444 578\n 405 487 490 496 398 512 422 455 449 432 607 679 434 597 639 565 415 486\n 668 414 665 763 557 304 404 454 689 610 483 441 657 590 492 476 437 483\n 529 363 711 543]\n \n >>> elements.shape\n (94,)\n \n>>> mean = numpy.mean(elements, axis=0)\n509.531914893617\n\n>>> sd = numpy.std(elements, axis=0)\n118.51857760182034\n\n>>> final_list = [x for x in arr if (x > mean - 2 * sd)]\n>>> final_list = [x for x in final_list if (x < mean + 2 * sd)]\n[386, 479, 627, 523, 482, 483, 542, 699, 535, 617, 577, 471, 615, 583, 441, 562, 563, 527,\n 453, 530, 433, 541, 585, 704, 443, 569, 430, 637, 331, 511, 552, 496, 484, 566, 554, 472,\n 335, 440, 579, 341, 545, 615, 548, 604, 439, 556, 442, 461, 624, 611, 444, 578, 405, 487,\n 490, 496, 398, 512, 422, 455, 449, 432, 607, 679, 434, 597, 639, 565, 415, 486, 668, 414,\n 665, 557, 304, 404, 454, 689, 610, 483, 441, 657, 590, 492, 476, 437, 483, 529, 363, 711,\n 543]\n\n>>> elements = numpy.array(final_list)\n>>> elements.shape\n (91,)\n\n>>> list(set(arr) - set(final_list))\n[10, 763, 20]\n\n# 3 outliers has been removed (10, 20 and 763)\n# As an astute commenter on CrossValidated put it: \xe2\x80\x9cSometimes outliers are bad data, and should be excluded, such as typos. \n# Sometimes they are Wayne Gretzky or Michael Jordan, and should be kept.\xe2\x80\x9d \n# Domain knowledge and practical wisdom are the only ways to tell the difference.\n'"
helpers/correlation_mask_1.py,2,"b'# load libraries\n\nimport numpy as np\nimport seaborn as sns\n\n# asusming you have a dataframe df\ncorrelations = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(correlations, dtype=np.bool)\n# array([[False, False],\n#       [False, False]])\n\nmask[np.triu_indices_from(mask)] = True\n# array([[ True,  True],\n#       [False,  True]])\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(17, 13))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(correlations, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={""shrink"": .5})\n'"
helpers/ds_pandas_max_unique_count_.py,0,"b"">>> import pandas as pd\n\n>>> data = {'shoe': ['a', 'b'], 'fury': ['c','d','e','f'], 'chaos': ['g','h', 'i']}\n>>> data\n{'shoe': ['a', 'b'], 'fury': ['c', 'd', 'e', 'f'], 'chaos': ['g', 'h', 'i']}\n\n>>> df = pd.DataFrame({k:pd.Series(v) for k, v in data.items()})\n>>> df\n  chaos fury shoe\n0     g    c    a\n1     h    d    b\n2     i    e  NaN\n3   NaN    f  NaN\n\n>>> print(df.count().max())\n4\n\n>>> df.notnull().sum(0).max()\n4\n>>>"""
helpers/generic_.py,0,b'# consolidated helpers\n\n# display all columns\npd.options.display.max_columns = len(<dataframe>)\n\n# use all cores\nn_jobs = -1\n\n'
helpers/min_max_scaling.py,0,"b""# ways to normalize numerical (min-max scaling) data\n# data is scaled to a fixed range - usually 0 to 1\n\n'''\nStandardizing the features so that they are centered around 0 with \na standard deviation of 1 is not only important if we are comparing\nmeasurements that have different units, but it is also a general \nrequirement for many machine learning algorithms.\n'''\n\n# input list (il)\nil = [1,1,1,2,3,2,2,2,1,1,1,2,1,12,111]\n\n# find the min, max and range\nrng = max(il) - min(il)\n\n# following code will give a generator\n# which will have the normalized output\n((x - min(il))/rng for x in il)\n\n# output\n[0.0, 0.0, 0.0, 0.00909090909090909, 0.01818181818181818, 0.00909090909090909, \n 0.00909090909090909, 0.00909090909090909, 0.0, 0.0, 0.0, 0.00909090909090909, 0.0, 0.1, 1.0]\n"""
helpers/missingno_1.py,0,"b""# missing values\n\n# load libraries\nimport missingno as msno\nimport matplotlib.pyplot as plt\n\ntrain_df.isnull().sum()[train_df.isnull().sum() > 0]\n# output of these - columns, are fed to msno matrix\n\nmsno.matrix(train_df[['v2a1', 'v18q1', 'rez_esc', 'meaneduc', 'SQBmeaned']], color = (0.211, 0.215, 0.274))\n\nplt.show()\n"""
helpers/n_jobs.py,0,"b'# set n_jobs = cpu count \n# to take advantage of parallel computing\n\n# n_jobs: Number of cores used for the training process\n# If set to -1, all cores will be used\nn_jobs = -1\n\n# alternately,\n\nimport multiprocessing\n\nn_jobs = multiprocessing.cpu_count()\n\n>>> n_jobs\n4\n'"
helpers/numpy_repeat_1.py,1,"b""# numpy repeat - use case 1\n# creating a new pandas column from a list\n# assuming you've a dataframe df\n\nimport numpy as np\n\nmy_list = ['A','B','C','D']\n\ndf['my_list'] = np.repeat(my_list, 5)\n# repeats each item in my_list 5 times\n"""
helpers/pandas_1.py,4,"b""# dict to dataframe\nd = {'Sex': ['F', 'F', 'M', 'M'], \n      'Size': ['S', 'L', 'S', 'L'], \n      'Age': [10, 48, 24, 35]}\n      \ndf = pd.DataFrame(d)\ndf\n  Sex Size  Age\n0   F    S   10\n1   F    L   48\n2   M    S   24\n3   M    L   35\n\n# get data types\ndf.dtypes\nSex     object\nSize    object\nAge      int64\ndtype: object\n\n# get categories\npd.Categorical(d['Sex'])\n[F, F, M, M]\nCategories (2, object): [F, M]\n\n# get categorical indices\npd.CategoricalIndex(df)\nCategoricalIndex(['Sex', 'Size', 'Age'], categories=['Age', 'Sex', 'Size'], \n                  ordered=False, dtype='category')\n\npd.CategoricalIndex(df['Sex'])\nCategoricalIndex(['F', 'F', 'M', 'M'], categories=['F', 'M'], ordered=False, \n                  name='Sex', dtype='category')\n\npd.CategoricalIndex(df['Sex'] == 'M')\nCategoricalIndex([False, False, True, True], categories=[False, True], \n                  ordered=False, name='Sex', dtype='category')\n\ntype(pd.CategoricalIndex(df['Sex'] == 'M'))\n<class 'pandas.core.indexes.category.CategoricalIndex'>\n\nx = pd.CategoricalIndex(df['Sex'] == 'M')\nx\nCategoricalIndex([False, False, True, True], categories=[False, True], \n                  ordered=False, name='Sex', dtype='category')\n\nx[0]\nFalse\n\nx[2]\nTrue\n\nx.categories\nIndex([False, True], dtype='object')\n\nlen(x.categories)\n2\n\n\n# categorical to continuous\npd.get_dummies(df)\n   Age  Sex_F  Sex_M  Size_L  Size_S\n0   10      1      0       0       1\n1   48      1      0       1       0\n2   24      0      1       0       1\n3   35      0      1       1       0\n\n# pandas factorize\n# categorical to continuous\npandas.factorize(values, sort=False, na_sentinel=-1, size_hint=None)\n\npd.factorize(df.Sex)\n(array([0, 0, 1, 1], dtype=int32), Index(['F', 'M'], dtype='object'))\n\npd.factorize(df.Size)\n(array([0, 1, 0, 1], dtype=int32), Index(['S', 'L'], dtype='object'))\n\npd.factorize(['b', None, 'a', 'c', 'b', np.nan])\n(array([ 0, -1,  1,  2,  0, -1], dtype=int32), array(['b', 'a', 'c'], dtype=object))\n\npd.factorize(['b', None, 'a', 'c', 'b', np.nan, 0, 1])\n(array([ 0, -1,  1,  2,  0, -1,  3,  4], dtype=int32), \n        array(['b', 'a', 'c', 0, 1], dtype=object))\n        \npd.factorize(['b', None, 'a', 'c', 'b', np.nan, 0, 1])[0]\narray([ 0, -1,  1,  2,  0, -1,  3,  4], dtype=int32)\n\npd.factorize(['b', None, 'a', 'c', 'b', np.nan, 0, 1])[1]\narray(['b', 'a', 'c', 0, 1], dtype=object)\n\n\ndf\n           model    type  cost\n0  Peter England    coir    20\n1         Turtle   linen    30\n2         Levi's  cotton    40\n3     Van Heusen    coir    50\n\ndf[df['model'].str.contains('an')]\n           model   type  cost\n0  Peter England   coir    20\n3     Van Heusen   coir    50\n\ndf[df['model'].str.match('an')]\nEmpty DataFrame\nColumns: [model, type, cost]\nIndex: []\n\n\n\n\n\n\n\n"""
helpers/pandas_2.py,0,"b""# pandas merge\n\ndf1\n  skuid    brand\n0  ax12     sony\n1  ak22     nike\n2  ak22     levi\n3  zm23  addidas\n4  zm23  ferrari\n5  zm24      NaN\n\ndf2\n    sid    brand\n0  ax11     sony\n1  ax12     nike\n2  ax12     levi\n3  zm23  addidas\n4  zm23  ferrari\n5  zm23      NaN\n\ndf1.merge(df2, how='left')\n  skuid    brand   sid\n0  ax12     sony  ax11\n1  ak22     nike  ax12\n2  ak22     levi  ax12\n3  zm23  addidas  zm23\n4  zm23  ferrari  zm23\n5  zm24      NaN  zm23\n\ndf1.merge(df2, how='right')\n  skuid    brand   sid\n0  ax12     sony  ax11\n1  ak22     nike  ax12\n2  ak22     levi  ax12\n3  zm23  addidas  zm23\n4  zm23  ferrari  zm23\n5  zm24      NaN  zm23\n\n\ndf1.merge(df2, left_on=['brand'], right_on=['brand'])\n\n\ndf1.merge(df2, left_on=['skuid'], right_on=['sid'])\n\n\ndf1.merge(df2, left_on=['skuid', 'brand'], right_on=['sid', 'brand'])\n\n\n"""
helpers/pandas_column_reordering_1.py,0,"b""# pandas change order of columns\n# last column as first column\n\n# assuming you've a pandas dataframe df\n\ncols = df.columns.tolist()\ncols = cols[-1:] + cols[:-1]\n\ndf = df[cols]\n\n# alternate way is by explicitly setting the order in cols\n\ncols = [1,2,3,4]\ndf = df[cols]\n"""
helpers/pandas_max_cols.py,0,"b""# how to display all columns by default\n# when you're using Jupyter QtConsole or Notebook or any other IDE's\n\nimport pandas as pd\n\ndf = pd.read_csc('file')\n\n# set the max_columns equal to the number of columns\npd.options.display.max_columns = len(df.columns)\n\n# you'll be able to see all columns now\ndf.head()\n"""
helpers/pandas_sample.py,0,"b""# viewing a random sample of the dataframe\n\n# import libraries\nimport pandas as pd\n\n# load dataframe\ndf = pd.read_csv('file')\n\n# to display all columns in IDE's\npd.options.display.max_columns = len(df.columns)\n\n# random sample of 15 rows\ndf.sample(15)\n"""
helpers/pandas_squeeze_1.py,4,"b'# vstack all the arrays of a column into a larger array\n\nimport numpy as np\nimport pandas as pd\n\ns = pd.Series([np.array([[1,0,0,0,1]]), \n               np.array([[0,1,0,1,0]]), \n               np.array([[0,1,0,1,0]])])\n\nres = np.array(s.values.tolist()).squeeze()\n\n# res\n# array([[1, 0, 0, 0, 1],\n#        [0, 1, 0, 1, 0],\n#        [0, 1, 0, 1, 0]])\n'"
helpers/plot_17.py,1,"b""# sample plottings\n# training dataframe; costarica dataset\n\n# getting object dtypes\ntrain.select_dtypes('object')\n\n# sample 1\ntrain = pd.read_csv('train.csv')\n\n# plot the count of Unique Values in integer Columns\ntrain.select_dtypes(np.int64).nunique().value_counts().sort_index()\n            .plot.bar(color = 'red', figsize = (9, 6), edgecolor = 'k', linewidth = 2);\nplt.xlabel('Number of Unique Values'); plt.ylabel('Count');\nplt.title('Count of Unique Values in Integer Columns');\n\n\n# sample 2\n# plotting different categories\nfrom collections import OrderedDict\n\nplt.figure(figsize = (20, 16))\nplt.style.use('fivethirtyeight')\n\n# Color mapping\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\npoverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n\n# Iterate through the float columns\nfor i, col in enumerate(train.select_dtypes('float')):\n    ax = plt.subplot(4, 2, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # Plot each poverty level as a separate line\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)\n\n\n# sample 3\n# distribution by category\n\nplt.figure(figsize = (16, 12))\n\n# Iterate through the float columns\nfor i, col in enumerate(['dependency', 'edjefa', 'edjefe']):\n    ax = plt.subplot(3, 1, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # Plot each poverty level as a separate line\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)\n\n\n\n"""
helpers/plot_categoricals_1.py,4,"b'# load libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# plot two categorical variables\ndef plot_categoricals(x, y, data, annotate=True):\n    """"""Plot counts of two categoricals.\n    Size is raw count for each grouping.\n    Percentages are for a given value of y.""""""\n    \n    # Raw counts \n    raw_counts = pd.DataFrame(data.groupby(y)[x].value_counts(normalize = False))\n    raw_counts = raw_counts.rename(columns = {x: \'raw_count\'})\n    \n    # Calculate counts for each group of x and y\n    counts = pd.DataFrame(data.groupby(y)[x].value_counts(normalize = True))\n    \n    # Rename the column and reset the index\n    counts = counts.rename(columns = {x: \'normalized_count\'}).reset_index()\n    counts[\'percent\'] = 100 * counts[\'normalized_count\']\n    \n    # Add the raw count\n    counts[\'raw_count\'] = list(raw_counts[\'raw_count\'])\n    \n    plt.figure(figsize = (14, 10))\n    # Scatter plot sized by percent\n    plt.scatter(counts[x], counts[y], edgecolor = \'k\', color = \'lightgreen\',\n                s = 100 * np.sqrt(counts[\'raw_count\']), marker = \'o\',\n                alpha = 0.6, linewidth = 1.5)\n    \n    if annotate:\n        # Annotate the plot with text\n        for i, row in counts.iterrows():\n            # Put text with appropriate offsets\n            plt.annotate(xy = (row[x] - (1 / counts[x].nunique()), \n                               row[y] - (0.15 / counts[y].nunique())),\n                         color = \'navy\',\n                         s = f""{round(row[\'percent\'], 1)}%"")\n        \n    # Set tick marks\n    plt.yticks(counts[y].unique())\n    plt.xticks(counts[x].unique())\n    \n    # Transform min and max to evenly space in square root domain\n    sqr_min = int(np.sqrt(raw_counts[\'raw_count\'].min()))\n    sqr_max = int(np.sqrt(raw_counts[\'raw_count\'].max()))\n    \n    # 5 sizes for legend\n    msizes = list(range(sqr_min, sqr_max,\n                        int(( sqr_max - sqr_min) / 5)))\n    markers = []\n    \n    # Markers for legend\n    for size in msizes:\n        markers.append(plt.scatter([], [], s = 100 * size, \n                                   label = f\'{int(round(np.square(size) / 100) * 100)}\', \n                                   color = \'lightgreen\',\n                                   alpha = 0.6, edgecolor = \'k\', linewidth = 1.5))\n        \n    # Legend and formatting\n    plt.legend(handles = markers, title = \'Counts\',\n               labelspacing = 3, handletextpad = 2,\n               fontsize = 16,\n               loc = (1.10, 0.19))\n    \n    plt.annotate(f\'* Size represents raw count while % is for a given y value.\',\n                 xy = (0, 1), xycoords = \'figure points\', size = 10)\n    \n    # Adjust axes limits\n    plt.xlim((counts[x].min() - (6 / counts[x].nunique()), \n              counts[x].max() + (6 / counts[x].nunique())))\n    plt.ylim((counts[y].min() - (4 / counts[y].nunique()), \n              counts[y].max() + (4 / counts[y].nunique())))\n    plt.grid(None)\n    plt.xlabel(f""{x}""); plt.ylabel(f""{y}""); plt.title(f""{y} vs {x}"")\n\n\n# plots two categorical columns cat_1 and cat_2 in the dataframe df\nplot_categoricals(\'cat_1\', \'cat_2\', df);\n\nplot_categoricals(\'cat_1\', \'cat_2\', df, annotate = False)\n'"
helpers/plot_confusion_matrix_1.py,2,"b'# plot confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title=\'Confusion matrix\',\n                          cmap=plt.cm.Oranges):\n    """"""\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n    """"""\n    if normalize:\n        cm = cm.astype(\'float\') / cm.sum(axis=1)[:, np.newaxis]\n        print(""Normalized confusion matrix"")\n    else:\n        print(\'Confusion matrix, without normalization\')\n\n    print(cm)\n\n    plt.figure(figsize = (10, 10))\n    plt.imshow(cm, interpolation=\'nearest\', cmap=cmap)\n    plt.title(title, size = 24)\n    plt.colorbar(aspect=4)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45, size = 14)\n    plt.yticks(tick_marks, classes, size = 14)\n\n    fmt = \'.2f\' if normalize else \'d\'\n    thresh = cm.max() / 2.\n    \n    # Labeling the plot\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), fontsize = 20,\n                 horizontalalignment=""center"",\n                 color=""white"" if cm[i, j] > thresh else ""black"")\n        \n    plt.grid(None)\n    plt.tight_layout()\n    plt.ylabel(\'True label\', size = 18)\n    plt.xlabel(\'Predicted label\', size = 18)\n\n\ncm = confusion_matrix(y_true, y_pred)\n\nplot_confusion_matrix(cm, classes = [\'class 1\', \'class 2\', \'class 3\', \'class 4\'],\n                      title = \'Confusion Matrix\')\n\n# normalized\nplot_confusion_matrix(cm, classes = [\'class 1\', \'class 2\', \'class 3\', \'class 4\'],\n                      title = \'Confusion Matrix\', normalize=True)\n'"
helpers/plot_value_counts_1.py,0,"b'# plot value counts of a column\ndef plot_value_counts(df, col, condition=False):\n    """"""Plot value counts of a column, optionally with only the heads of a household""""""\n    # apply condition here if required\n    if condition:\n        # define condition below <>\n        df = df.loc[df[col] == condition].copy()\n        \n    plt.figure(figsize = (8, 6))\n    df[col].value_counts().sort_index().plot.bar(color = \'red\',\n                                                 edgecolor = \'g\',\n                                                 linewidth = 2)\n    plt.xlabel(f\'{col}\')\n    plt.title(f\'{col} Value Counts\')\n    plt.ylabel(\'Count\')\n    plt.show()\n\n# plot\nplot_value_counts(df, col)\nplot_value_counts(df, col, condition=True)\n'"
helpers/slr_1.py,0,"b'# simple linear regression f(x)\n\n# x is the input and y is the output\ndef slope_intercept_simple_linear_regression(x, y):\n    \n    # compute the sum of input and output\n    sum = x + y\n    \n    # compute the product of the output and the input and its sum\n    product = x * y\n    sum_of_product = product.sum()\n    \n    # compute the squared value of the input and its sum\n    x_squared = x * x\n    sum_x_squared = x_squared.sum()\n    \n    # use the formula for the slope\n    numerator = sum_of_product - ((x.sum() * y.sum()) / x.size())\n    denominator = sum_x_squared - ((x.sum() * x.sum()) / x.size())\n    slope = numerator / denominator\n    \n    # use the formula for the intercept\n    intercept = y.mean() - (slope * x.mean())\n    \n    # returns intercept and slope\n    return (intercept, slope)\n\n'"
reinforcement_learning/con_bandit_1.py,0,"b'Python 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 14:00:49) [MSC v.1915 64 bit (AMD64)] on win32\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\n>>>\n>>>\n>>> import vowpalwabbit\n>>>\n>>> import pandas as pd\n>>> import sklearn as sk\n>>> import numpy as np\n>>>\n>>> # generate sample data that could originate from previous random trial, e.g. AB test, for the CB to explore\n... ## data here are equivalent to example in https://github.com/JohnLangford/vowpal_wabbit/wiki/Contextual-Bandit-Example\n... train_data = [{\'action\': 1, \'cost\': 2, \'probability\': 0.4, \'feature1\': \'a\', \'feature2\': \'c\', \'feature3\': \'\'},\n...               {\'action\': 3, \'cost\': 0, \'probability\': 0.2, \'feature1\': \'b\', \'feature2\': \'d\', \'feature3\': \'\'},\n...               {\'action\': 4, \'cost\': 1, \'probability\': 0.5, \'feature1\': \'a\', \'feature2\': \'b\', \'feature3\': \'\'},\n...               {\'action\': 2, \'cost\': 1, \'probability\': 0.3, \'feature1\': \'a\', \'feature2\': \'b\', \'feature3\': \'c\'},\n...               {\'action\': 3, \'cost\': 1, \'probability\': 0.7, \'feature1\': \'a\', \'feature2\': \'d\', \'feature3\': \'\'}]\n>>>\n>>>\n>>> train_df = pd.DataFrame(train_data)\n>>>\n>>> ## add index to df\n... train_df[\'index\'] = range(1, len(train_df) + 1)\n>>> train_df = train_df.set_index(""index"")\n>>>\n>>> # generate some test data that you want the CB to make decisions for, e.g. features describing new users, for the CB to exploit\n... test_data = [{\'feature1\': \'b\', \'feature2\': \'c\', \'feature3\': \'\'},\n...             {\'feature1\': \'a\', \'feature2\': \'\', \'feature3\': \'b\'},\n...             {\'feature1\': \'b\', \'feature2\': \'b\', \'feature3\': \'\'},\n...             {\'feature1\': \'a\', \'feature2\': \'\', \'feature3\': \'b\'}]\n>>>\n>>>\n>>> test_df = pd.DataFrame(test_data)\n>>>\n>>> ## add index to df\n... test_df[\'index\'] = range(1, len(test_df) + 1)\n>>> test_df = test_df.set_index(""index"")\n>>>\n>>>\n>>> # take a look at dataframes\n... print(train_df)\n       action  cost  probability feature1 feature2 feature3\nindex\n1           1     2          0.4        a        c\n2           3     0          0.2        b        d\n3           4     1          0.5        a        b\n4           2     1          0.3        a        b        c\n5           3     1          0.7        a        d\n>>> print(test_df)\n      feature1 feature2 feature3\nindex\n1            b        c\n2            a                 b\n3            b        b\n4            a                 b\n>>>\n>>> # import vowpal wabbit\'s python wrapper\n... import vowpalwabbit\n>>> from vowpalwabbit import pyvw\n>>>\n>>> # create python model - this stores the model parameters in the python vw object; here a contextual bandit with four possible actions\n... vw = pyvw.vw(""--cb 4"")\nNum weight bits = 18\nlearning rate = 0.5\ninitial_t = 0\npower_t = 0.5\nusing no cache\nReading datafile =\nnum sources = 1\n>>>\n>>> vw\n<vowpalwabbit.pyvw.vw object at 0x0000021411AC0458>\n>>>\n>>> # use the learn method to train the vw model, train model row by row using a loop\n... i==1\nTraceback (most recent call last):\n  File ""<stdin>"", line 2, in <module>\nNameError: name \'i\' is not defined\n>>> for i in train_df.index:\n...     ## provide data to cb in requested format\n...     action = train_df.loc[i, ""action""]\n...     cost = train_df.loc[i, ""cost""]\n...     probability = train_df.loc[i, ""probability""]\n...     feature1 = train_df.loc[i, ""feature1""]\n...     feature2 = train_df.loc[i, ""feature2""]\n...     feature3 = train_df.loc[i, ""feature3""]\n...     ## do the actual learning\n...     vw.learn(str(action)+"":""+str(cost)+"":""+str(probability)+"" | ""+str(feature1)+"" ""+str(feature2)+"" ""+str(feature3))\n...\n5.000000 5.000000            1            1.0    known        1        3\n2.500000 0.000000            2            2.0    known        2        3\n2.083333 1.666667            4            4.0    known        2        4\n>>>\n>>> # predict row by row and output results\n... j==1\nTraceback (most recent call last):\n  File ""<stdin>"", line 2, in <module>\nNameError: name \'j\' is not defined\n>>> for j in test_df.index:\n...     feature1 = test_df.loc[j, ""feature1""]\n...     feature2 = test_df.loc[j, ""feature2""]\n...     feature3 = test_df.loc[j, ""feature3""]\n...     choice = vw.predict(""| ""+str(feature1)+"" ""+str(feature2)+"" ""+str(feature3))\n...     print(j, choice)\n...\n1 3\n2 3\n1.952381 1.428571            8            8.0  unknown        3        3\n3 3\n4 3\n>>>\n>>>\n>>> # the CB assigns every instance to action 3 as it should per the cost structure of the train data; you can play with the cost structure to see that the CB updates its predictions accordingly\n...\n>>>\n>>> # BONUS: save and load the CB model\n... # save model\n... vw.save(\'cb.model\')\n>>> del vw\n>>> # load from saved file\n... vw = pyvw.vw(""--cb 4 -i cb.model"")\nNum weight bits = 18\nlearning rate = 0.5\ninitial_t = 0\npower_t = 0.5\nusing no cache\nReading datafile =\nnum sources = 1\n>>> print(vw.predict(\'| a b\'))\n    n.a.     n.a.            1            1.0  unknown        3        3\n3\n>>>\n>>>\n>>>\n>>> import os\n>>>\n>>> os.getcwd()\n\nfinished run\nnumber of examples = 9\nweighted example sum = 9.000000\nweighted label sum = 0.000000\naverage loss = 1.952381\ntotal feature number = 28\n\'C:\\\\Continuum\\\\anaconda3\\\\envs\'\n>>>\n>>>\n>>> exit()\n\nfinished run\nnumber of examples = 1\nweighted example sum = 1.000000\nweighted label sum = 0.000000\naverage loss = n.a.\ntotal feature number = 3\n\n'"
reinforcement_learning/personalizer.py,0,"b'\r\n# !pip install azure-cognitiveservices-personalizer\r\n\r\n\r\n# Add the dependencies\r\nfrom azure.cognitiveservices.personalizer import PersonalizerClient\r\nfrom azure.cognitiveservices.personalizer.models import RankableAction, RewardRequest, RankRequest\r\nfrom msrest.authentication import CognitiveServicesCredentials\r\n\r\nimport datetime, json, os, time, uuid\r\n\r\n# Add Personalizer resource information\r\nkey_var_name = \'PERSONALIZER_KEY\'\r\nif not key_var_name in os.environ:\r\n    raise Exception(\'Please set/export the environment variable: {}\'.format(key_var_name))\r\npersonalizer_key = os.environ[key_var_name]\r\n\r\n# For example: https://westus2.api.cognitive.microsoft.com/\r\nendpoint_var_name = \'PERSONALIZER_ENDPOINT\'\r\nif not endpoint_var_name in os.environ:\r\n    raise Exception(\'Please set/export the environment variable: {}\'.format(endpoint_var_name))\r\npersonalizer_endpoint = os.environ[endpoint_var_name]\r\n\r\n\r\n# Create a Personalizer client\r\n# PERSONALIZER_RESOURCE_ENDPOINT\r\n# PERSONALIZER_RESOURCE_KEY\r\n# Instantiate a LUIS client\r\nclient = PersonalizerClient(personalizer_endpoint, CognitiveServicesCredentials(personalizer_key))\r\n\r\n\r\n# Get content choices represented as actions\r\ndef get_user_timeofday():\r\n    res={}\r\n    time_features = [""morning"", ""afternoon"", ""evening"", ""night""]\r\n    time = input(""What time of day is it (enter number)? 1. morning 2. afternoon 3. evening 4. night\\n"")\r\n    try:\r\n        ptime = int(time)\r\n        if(ptime<=0 or ptime>len(time_features)):\r\n            raise IndexError\r\n        res[\'time_of_day\'] = time_features[ptime-1]\r\n    except (ValueError, IndexError):\r\n        print(""Entered value is invalid. Setting feature value to"", time_features[0] + ""."")\r\n        res[\'time_of_day\'] = time_features[0]\r\n    return res\r\n\r\n\r\ndef get_user_preference():\r\n    res = {}\r\n    taste_features = [\'salty\',\'sweet\']\r\n    pref = input(""What type of food would you prefer? Enter number 1.salty 2.sweet\\n"")\r\n    \r\n    try:\r\n        ppref = int(pref)\r\n        if(ppref<=0 or ppref>len(taste_features)):\r\n            raise IndexError\r\n        res[\'taste_preference\'] = taste_features[ppref-1]\r\n    except (ValueError, IndexError):\r\n        print(""Entered value is invalid. Setting feature value to"", taste_features[0]+ ""."")\r\n        res[\'taste_preference\'] = taste_features[0]\r\n    return res\r\n\r\n\r\n\r\n## Create the learning loop\r\n\r\nkeep_going = True\r\nwhile keep_going:\r\n\r\n    eventid = str(uuid.uuid4())\r\n\r\n    context = [get_user_preference(), get_user_timeofday()]\r\n    actions = get_actions()\r\n\r\n    rank_request = RankRequest( actions=actions, context_features=context, excluded_actions=[\'juice\'], event_id=eventid)\r\n    response = client.rank(rank_request=rank_request)\r\n    \r\n    print(""Personalizer service ranked the actions with the probabilities listed below:"")\r\n    \r\n    rankedList = response.ranking\r\n    for ranked in rankedList:\r\n        print(ranked.id, \':\',ranked.probability)\r\n\r\n    print(""Personalizer thinks you would like to have"", response.reward_action_id+""."")\r\n    answer = input(""Is this correct?(y/n)\\n"")[0]\r\n\r\n    reward_val = ""0.0""\r\n    if(answer.lower()==\'y\'):\r\n        reward_val = ""1.0""\r\n    elif(answer.lower()==\'n\'):\r\n        reward_val = ""0.0""\r\n    else:\r\n        print(""Entered choice is invalid. Service assumes that you didn\'t like the recommended food choice."")\r\n\r\n    client.events.reward(event_id=eventid, value=reward_val)\r\n\r\n    br = input(""Press Q to exit, any other key to continue: "")[0]\r\n    if(br.lower()==\'q\'):\r\n        keep_going = False\r\n\r\n\r\n\r\n## Request a rank\r\nrank_request = RankRequest( actions=actions, context_features=context, excluded_actions=[\'juice\'], event_id=eventid)\r\nresponse = client.rank(rank_request=rank_request)\r\n\r\n\r\n## Send a reward\r\nreward_val = ""0.0""\r\nif(answer.lower()==\'y\'):\r\n    reward_val = ""1.0""\r\nelif(answer.lower()==\'n\'):\r\n    reward_val = ""0.0""\r\nelse:\r\n    print(""Entered choice is invalid. Service assumes that you didn\'t like the recommended food choice."")\r\n\r\nclient.events.reward(event_id=eventid, value=reward_val)\r\n\r\n\r\n# sample run\r\n# python <filename>\r\n'"
datascience/samples/df_plot.py,0,"b""import tkinter \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame([\n['abcd',         'sw-off ',        '07:53:15 +05:00'],\\\n['abcd',         'sw-off ',        '07:53:15 +05:00'],\\\n['abcde',         'sw-off ',        '07:53:15 +05:00'],\\\n['abcd',         'sw-on',          '07:53:15 +05:00'],\\\n['abcdf',         'sw-off ',        '07:53:15 +05:00'],\\\n['abcd',         'sw-on',          '07:53:15 +05:00'],\\\n['abcd',         'sw-on',          '00:53:15 +05:00'],\\\n['abcd',         'sw-on',          '01:53:15 +05:00'],\\\n['abcd',         'sw-off',          '02:53:15 +05:00'],\\\n['abcde',         'sw-off',          '02:53:15 +05:00'],\\\n['abcde',         'sw-off',          '02:53:15 +05:00'],\\\n['abcde',         'sw-off',          '02:53:15 +05:00'],\\\n['abcd',         'sw-off ',        '07:53:15 +05:00'],\\\n['abcd',         'sw-on',          '03:53:15 +05:00'],\\\n['abcd',         'sw-on',          '05:53:15 +05:00'],\\\n['abcd',         'sw-off',          '04:53:15 +05:00'],\\\n['abcd',         'sw-on',          '08:53:15 +05:00'],\\\n['abcd',         'sw-off',          '06:53:15 +05:00'],\\\n['abcde',         'sw-off',          '09:53:15 +05:00'],\\\n])\n\ndf.columns = ['username',    'switch_state',    'time',]\n\nprint(df)\n\ndf = df.loc[df['switch_state']=='sw-off']\ndf['count'] = df.groupby(['username','time'])['username'].transform('count')\ndf = df.drop_duplicates(subset=['username', 'time'], keep='first')\n\nplt.scatter(df['time'], df['username'])\nplt.show()\n"""
datascience/samples/minmaxscaler.py,0,"b""from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# scale col1 and col2\nvals = df[['col1', 'col2']].values #returns a numpy array\n\nmin_max_scaler = MinMaxScaler()\nvals_scaled = min_max_scaler.fit_transform(vals)\n\n# scaled\ndf_scaled = pd.DataFrame(vals_scaled, columns=['scaled_col1', 'scaled_col2'])\n"""
datascience/samples/ml_cross_val_predict_plot_1.py,0,"b""import tkinter\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets, linear_model\nfrom sklearn.model_selection import cross_val_predict\n\nimport matplotlib.pyplot as plt\n\ndiabetes = datasets.load_diabetes()\n\nX = diabetes.data[:150]\ny = diabetes.target[:150]\nprint(X, y)\nprint(X.shape, y.shape)\n\nlasso = linear_model.Lasso()\nprint(lasso)\n\ny_pred = cross_val_predict(lasso, X, y)\nprint(y_pred)\n\nfig, ax = plt.subplots()\nax.scatter(y, y_pred, edgecolors=(0, 0, 0))\nax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nplt.show()\n"""
datascience/samples/mpl_axes3d_1.py,3,"b""import tkinter as tk\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport random\n\nfig = plt.figure()\nax = Axes3D(fig)\n\nx_vals = np.random.rand(1000)\ny_vals = np.random.rand(1000)\nz_vals = np.random.rand(1000)\n\nax.scatter(x_vals, y_vals, z_vals, color='red')\nax.scatter(x_vals+0.3, y_vals-0.7, z_vals, color='blue')\nax.scatter(x_vals-0.3, y_vals+0.7, z_vals, color='green')\n\nplt.show()\n"""
datascience/samples/pylab_probability_plot_1.py,0,"b'import tkinter as tk\nimport numpy as np\nimport pylab\nimport scipy.stats as stats\nfrom urllib.request import urlopen\nimport sys\n\ntarget_url = (""https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data"")\ndata = urlopen(target_url)\nprint(type(data))\nprint(data)\n\ncontent =  data.read().decode(data.headers.get_content_charset())\nprint(type(content))\n\nf = open(""ml_sonar_1_.csv"", ""w"")\nf.write(content)\nf.close()\n\n#arrange data into list for labels and list of lists for attributes\nxList = []\nlabels = []\nwith open(""ml_sonar_1_.csv"", ""r"") as r:\n    for line in r.readlines():\n        #split on comma\n        row = line.strip().split("","")\n        xList.append(row)\n\nnrow = len(xList)\nncol = len(xList[1])\n\ntype = [0]*3\ncolCounts = []\n\n#generate summary statistics for column 3 (e.g.)\ncol = 3\ncolData = []\nfor row in xList:\n    #print(row)\n    colData.append(float(row[col]))\nstats.probplot(colData, dist=""norm"", plot=pylab)\npylab.show()\n'"
datascience/samples/sns_concentration_plot_1.py,0,"b""import tkinter as tk\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn import tree\nfrom sklearn.datasets import load_iris\nimport numpy as np\nimport seaborn as sns\n\ndata = load_iris()\ndf = pd.DataFrame(data['data'])\ndf.head(5)\nfig, axes = plt.subplots(figsize=(13, 3))\naxes = sns.countplot(x=2, data=df, orient='h')\nplt.show()\n"""
datascience/samples/sns_regplot.py,1,"b'# Seaborn Regression Plot\n\nimport tkinter\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ns = np.matrix([[1,1],[2,3],[2,4],[3,6],[3,9],[4,12],[4,15],[5,19],\\\n[5,25],[5,36],[6,41],[6,49],[7,54]])\ns\nmatrix([[ 1,  1],\n        [ 2,  3],\n        [ 2,  4],\n        [ 3,  6],\n        [ 3,  9],\n        [ 4, 12],\n        [ 4, 15],\n        [ 5, 19],\n        [ 5, 25],\n        [ 5, 36],\n        [ 6, 41],\n        [ 6, 49],\n        [ 7, 54]])\n\ndf = pd.DataFrame(s)\nsns.regplot(df[0], df[1])\n#<matplotlib.axes._subplots.AxesSubplot object at 0x0D87E1D0>\nplt.show()\n\nsns.regplot(df[0], df[1], order=2)\n#<matplotlib.axes._subplots.AxesSubplot object at 0x0EB5E270>\nplt.show()\n'"
datascience/samples/sns_subplots_1.py,1,"b""import numpy as np\nimport pandas as pd\nimport tkinter as tk\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nNcols = 9\ncols = ['col_{:d}'.format(i) for i in range(Ncols)]\ndf = pd.DataFrame(np.random.random(size=(1000,Ncols)),columns=cols)\n\nfig, axs = plt.subplots(3,3) # adjust the geometry based on your number of columns to plot\nfor ax,col in zip(axs.flatten(), cols):\n    sns.kdeplot(df[col], ax=ax)\n\nplt.show()\n"""
datascience/samples/xgboost_plots_1.py,0,"b""# xgboost\n\nimport xgboost\nfrom xgboost import plot_tree\nfrom xgboost import XGBClassifier\nimport matplotlib.pyplot as plt\n\n# fit model no training data\n# assuming you've X and y\nmodel = XGBClassifier()\nmodel.fit(X, y)\n\n# plot single tree\nplot_tree(model)\n\n# plot_importance\nh,w = 13\nfig, ax = plt.subplots(figsize=(h, w))\nxgboost.plot_importance(model, ax=ax)\nplt.show()\n"""
deeplearning/rnn/utils.py,17,"b'from keras.models import load_model\nfrom keras.models import Sequential, load_model\nfrom keras.layers import LSTM, Dense, Dropout, Embedding, Masking\nfrom keras.optimizers import Adam\nfrom keras.utils import Sequence\nfrom keras.preprocessing.text import Tokenizer\n\nfrom sklearn.utils import shuffle\n\nfrom IPython.display import HTML\n\nfrom itertools import chain\nfrom keras.utils import plot_model\nimport numpy as np\nimport pandas as pd\nimport random\nimport json\nimport re\n\nRANDOM_STATE = 50\nTRAIN_FRACTION = 0.7\n\n\ndef get_model(model_name):\n    """"""Retrieve a Keras model and embeddings""""""\n    model = load_model(f\'../models/{model_name}.h5\')\n    embeddings = model.get_layer(index = 0)\n    embeddings = embeddings.get_weights()[0]\n    embeddings = embeddings / np.linalg.norm(embeddings, axis = 1).reshape((-1, 1))\n    embeddings = np.nan_to_num(embeddings)\n    word_idx = []\n    with open(f\'../data/training-rnn.json\', \'rb\') as f:\n        for l in f:\n            word_idx.append(json.loads(l))\n        \n    word_idx = word_idx[0]\n    word_idx[\'UNK\'] = 0\n    idx_word = {index: word for word, index in word_idx.items()}\n    return model, embeddings, word_idx, idx_word\n\ndef get_embeddings(model):\n    """"""Retrieve the embeddings in a model""""""\n    embeddings = model.get_layer(index = 0)\n    embeddings = embeddings.get_weights()[0]\n    embeddings = embeddings / np.linalg.norm(embeddings, axis = 1).reshape((-1, 1))\n    embeddings = np.nan_to_num(embeddings)\n    return embeddings\n    \ndef find_closest(query, embedding_matrix, word_idx, idx_word, n = 10):\n    """"""Find closest words to a query word in embeddings""""""\n    \n    idx = word_idx.get(query, None)\n    # Handle case where query is not in vocab\n    if idx is None:\n        print(f\'{query} not found in vocab.\')\n        return\n    else:\n        vec = embedding_matrix[idx]\n        # Handle case where word doesn\'t have an embedding\n        if np.all(vec == 0):\n            print(f\'{query} has no pre-trained embedding.\')\n            return\n        else:\n            # Calculate distance between vector and all others\n            dists = np.dot(embedding_matrix, vec)\n            \n            # Sort indexes in reverse order\n            idxs = np.argsort(dists)[::-1][:n]\n            sorted_dists = dists[idxs]\n            closest = [idx_word[i] for i in idxs]\n            \n    print(f\'Query: {query}\\n\')\n    # Print out the word and cosine distances\n    for word, dist in zip(closest, sorted_dists):\n        print(f\'Word: {word:15} Cosine Similarity: {round(dist, 4)}\')\n        \ndef format_sequence(s):\n    """"""Add spaces around punctuation and remove references to images/citations.""""""\n    \n    # Add spaces around punctuation\n    s =  re.sub(r\'(?<=[^\\s0-9])(?=[.,;?])\', r\' \', s)\n    \n    # Remove references to figures\n    s = re.sub(r\'\\((\\d+)\\)\', r\'\', s)\n    \n    # Remove double spaces\n    s = re.sub(r\'\\s\\s\', \' \', s)\n    return s\n\ndef remove_spaces(s):\n    """"""Remove spaces around punctuation""""""\n    s = re.sub(r\'\\s+([.,;?])\', r\'\\1\', s)\n    \n    return s\n\n\ndef get_data(file, filters=\'!""%;[\\\\]^_`{|}~\\t\\n\', training_len=50,\n             lower=False):\n    """"""Retrieve formatted training and validation data from a file""""""\n    \n    data = pd.read_csv(file, parse_dates=[\'patent_date\']).dropna(subset = [\'patent_abstract\'])\n    abstracts = [format_sequence(a) for a in list(data[\'patent_abstract\'])]\n    word_idx, idx_word, num_words, word_counts, texts, sequences, features, labels = make_sequences(\n        abstracts, training_len, lower, filters)\n    X_train, X_valid, y_train, y_valid = create_train_valid(features, labels, num_words)\n    training_dict = {\'X_train\': X_train, \'X_valid\': X_valid, \n                     \'y_train\': y_train, \'y_valid\': y_valid}\n    return training_dict, word_idx, idx_word, sequences\n\ndef create_train_valid(features,\n                       labels,\n                       num_words,\n                       train_fraction=0.7):\n    """"""Create training and validation features and labels.""""""\n    \n    # Randomly shuffle features and labels\n    features, labels = shuffle(features, labels, random_state=RANDOM_STATE)\n\n    # Decide on number of samples for training\n    train_end = int(train_fraction * len(labels))\n\n    train_features = np.array(features[:train_end])\n    valid_features = np.array(features[train_end:])\n\n    train_labels = labels[:train_end]\n    valid_labels = labels[train_end:]\n\n    # Convert to arrays\n    X_train, X_valid = np.array(train_features), np.array(valid_features)\n\n    # Using int8 for memory savings\n    y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n    y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n\n    # One hot encoding of labels\n    for example_index, word_index in enumerate(train_labels):\n        y_train[example_index, word_index] = 1\n\n    for example_index, word_index in enumerate(valid_labels):\n        y_valid[example_index, word_index] = 1\n\n    # Memory management\n    import gc\n    gc.enable()\n    del features, labels, train_features, valid_features, train_labels, valid_labels\n    gc.collect()\n\n    return X_train, X_valid, y_train, y_valid\n\ndef make_sequences(texts, training_length = 50,\n                   lower = True, filters=\'!""#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\'):\n    """"""Turn a set of texts into sequences of integers""""""\n    \n    # Create the tokenizer object and train on texts\n    tokenizer = Tokenizer(lower=lower, filters=filters)\n    tokenizer.fit_on_texts(texts)\n    \n    # Create look-up dictionaries and reverse look-ups\n    word_idx = tokenizer.word_index\n    idx_word = tokenizer.index_word\n    num_words = len(word_idx) + 1\n    word_counts = tokenizer.word_counts\n    \n    print(f\'There are {num_words} unique words.\')\n    \n    # Convert text to sequences of integers\n    sequences = tokenizer.texts_to_sequences(texts)\n    \n    # Limit to sequences with more than training length tokens\n    seq_lengths = [len(x) for x in sequences]\n    over_idx = [i for i, l in enumerate(seq_lengths) if l > (training_length + 20)]\n    \n    new_texts = []\n    new_sequences = []\n    \n    # Only keep sequences with more than training length tokens\n    for i in over_idx:\n        new_texts.append(texts[i])\n        new_sequences.append(sequences[i])\n        \n    features = []\n    labels = []\n    \n    # Iterate through the sequences of tokens\n    for seq in new_sequences:\n        \n        # Create multiple training examples from each sequence\n        for i in range(training_length, len(seq)):\n            # Extract the features and label\n            extract = seq[i - training_length: i + 1]\n            \n            # Set the features and label\n            features.append(extract[:-1])\n            labels.append(extract[-1])\n    \n    print(f\'There are {len(features)} sequences.\')\n    \n    # Return everything needed for setting up the model\n    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, features, labels\n\n\n\ndef header(text, color = \'black\', gen_text = None):\n    if gen_text:\n        raw_html = f\'<h1 style=""color: {color};""><p><center>\' + str(\n        text) + \'<span style=""color: red"">\' + str(gen_text) + \'</center></p></h1>\'\n    else:\n        raw_html = f\'<h1 style=""color: {color};""><center>\' + str(\n            text) + \'</center></h1>\'\n    return raw_html\n\n\ndef box(text, gen_text=None):\n    if gen_text:\n        raw_html = \'<div style=""border:1px inset black;padding:1em;font-size: 20px;""> <p>\' + str(\n            text) +\'<span style=""color: red"">\' + str(gen_text) + \'</p></div>\'\n\n    else:\n        raw_html = \'<div style=""border:1px inset black;padding:1em;font-size: 20px;"">\' + str(\n            text) + \'</div>\'\n    return raw_html\n\n\ndef addContent(old_html, raw_html):\n    old_html += raw_html\n    return old_html\n\ndef seed_sequence(model, s, word_idx, idx_word, \n                  diversity = 0.75, num_words = 50):\n    """"""Generate output starting from a seed sequence.""""""\n    # Original formated text\n    start = format_sequence(s).split()\n    gen = []\n    s = start[:]\n    # Generate output\n    for _ in range(num_words):\n        # Conver to arry\n        x = np.array([word_idx.get(word, 0) for word in s]).reshape((1, -1))\n\n        # Make predictions\n        preds = model.predict(x)[0].astype(float)\n\n        # Diversify\n        preds = np.log(preds) / diversity\n        exp_preds = np.exp(preds)\n        # Softmax\n        preds = exp_preds / np.sum(exp_preds)\n        # Pick next index\n        next_idx = np.argmax(np.random.multinomial(1, preds, size = 1))\n        s.append(idx_word[next_idx])\n        gen.append(idx_word[next_idx])\n    \n    # Formatting in html\n    start = remove_spaces(\' \'.join(start)) + \' \'\n    gen = remove_spaces(\' \'.join(gen)) \n    html = \'\'\n    html = addContent(html, header(\'Input Seed \', color = \'black\', gen_text = \'Network Output\'))\n    html = addContent(html, box(start, gen))\n    return html\n\n   \ndef make_sequences_new(texts,\n                   training_length=50,\n                   lower=True,\n                   filters=\'!""#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\'):\n    """"""Turn a set of texts into sequences of integers""""""\n\n    # Create the tokenizer object and train on texts\n    tokenizer = Tokenizer(lower=lower, filters=filters)\n    tokenizer.fit_on_texts(texts)\n\n    # Convert text to sequences of integers\n    sequences = tokenizer.texts_to_sequences(texts)\n\n    # Limit to sequences with more than (training length + 20) tokens\n    seq_lengths = [len(x) for x in sequences]\n    over_idx = [\n        i for i, l in enumerate(seq_lengths) if l > (training_length + 20)\n    ]\n\n    new_texts = []\n\n    # Only keep sequences with more than training length tokens\n    for i in over_idx:\n        new_texts.append(texts[i])\n    \n    tokenizer = Tokenizer(lower=lower, filters=filters)\n    # Refit on long texts\n    tokenizer.fit_on_texts(new_texts)\n    new_sequences = tokenizer.texts_to_sequences(new_texts)\n    \n    # Create look-up dictionaries and reverse look-ups\n    word_idx = tokenizer.word_index\n    idx_word = tokenizer.index_word\n    num_words = len(word_idx) + 1\n    word_counts = tokenizer.word_counts\n\n    print(f\'There are {num_words} unique words.\')\n\n    features = []\n    labels = []\n\n    # Iterate through the sequences of tokens\n    for seq in new_sequences:\n\n        # Create multiple training examples from each sequence\n        for i in range(training_length, len(seq)):\n            # Extract the features and label\n            extract = seq[i - training_length:i + 1]\n\n            # Set the features and label\n            features.append(extract[:-1])\n            labels.append(extract[-1])\n\n    print(f\'There are {len(features)} training sequences.\')\n\n    # Return everything needed for setting up the model\n    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, features, labels'"
deeplearning/rnn/utils_tpb_1.py,24,"b'from keras.models import load_model\nfrom keras.models import Sequential, load_model\nfrom keras.layers import LSTM, Dense, Dropout, Embedding, Masking\nfrom keras.optimizers import Adam\nfrom keras.utils import Sequence\nfrom keras.preprocessing.text import Tokenizer\n\nfrom sklearn.utils import shuffle\n\nfrom IPython.display import HTML\n\nfrom itertools import chain\nfrom keras.utils import plot_model\nimport numpy as np\nimport pandas as pd\nimport random\nimport json\nimport re\n\nRANDOM_STATE = 50\nTRAIN_FRACTION = 0.7\n\n\ndef get_embeddings(model):\n    """"""Retrieve the embeddings in a model""""""\n    embeddings = model.get_layer(index = 0)\n    embeddings = embeddings.get_weights()[0]\n    embeddings = embeddings / np.linalg.norm(embeddings, axis = 1).reshape((-1, 1))\n    embeddings = np.nan_to_num(embeddings)\n    return embeddings\n    \ndef find_closest(query, embedding_matrix, word_idx, idx_word, n = 10):\n    """"""Find closest words to a query word in embeddings""""""\n    \n    idx = word_idx.get(query, None)\n    # Handle case where query is not in vocab\n    if idx is None:\n        print(f\'{query} not found in vocab.\')\n        return\n    else:\n        vec = embedding_matrix[idx]\n        # Handle case where word doesn\'t have an embedding\n        if np.all(vec == 0):\n            print(f\'{query} has no pre-trained embedding.\')\n            return\n        else:\n            # Calculate distance between vector and all others\n            dists = np.dot(embedding_matrix, vec)\n            \n            # Sort indexes in reverse order\n            idxs = np.argsort(dists)[::-1][:n]\n            sorted_dists = dists[idxs]\n            closest = [idx_word[i] for i in idxs]\n            \n    print(f\'Query: {query}\\n\')\n    # Print out the word and cosine distances\n    for word, dist in zip(closest, sorted_dists):\n        print(f\'Word: {word:15} Cosine Similarity: {round(dist, 4)}\')\n        \ndef format_sequence(s):\n    """"""Add spaces around punctuation and remove references to images/citations.""""""\n    \n    # Add spaces around punctuation\n    s =  re.sub(r\'(?<=[^\\s0-9])(?=[.,;?])\', r\' \', s)\n    \n    # Remove references to figures\n    s = re.sub(r\'\\((\\d+)\\)\', r\'\', s)\n    \n    # Remove double spaces\n    s = re.sub(r\'\\s\\s\', \' \', s)\n    return s\n\ndef remove_spaces(s):\n    """"""Remove spaces around punctuation""""""\n    s = re.sub(r\'\\s+([.,;?])\', r\'\\1\', s)\n    \n    return s\n\n\ndef get_data(file, filters=\'!""%;[\\\\]^_`{|}~\\t\\n\', training_len=50,\n             lower=False):\n    """"""Retrieve formatted training and validation data from a file""""""\n    \n    data = pd.read_csv(file, encoding=\'latin\', parse_dates=[\'IndustryCode\']).dropna(subset = [\'TEText\'])\n    abstracts = [format_sequence(a) for a in list(data[\'TEText\'])]\n    word_idx, idx_word, num_words, word_counts, texts, sequences, features, labels = make_sequences(\n        abstracts, training_len, lower, filters)\n    X_train, X_valid, y_train, y_valid = create_train_valid(features, labels, num_words)\n    training_dict = {\'X_train\': X_train, \'X_valid\': X_valid, \n                     \'y_train\': y_train, \'y_valid\': y_valid}\n    return training_dict, word_idx, idx_word, sequences\n\ndef create_train_valid(features,\n                       labels,\n                       num_words,\n                       train_fraction=0.7):\n    """"""Create training and validation features and labels.""""""\n    \n    # Randomly shuffle features and labels\n    features, labels = shuffle(features, labels, random_state=RANDOM_STATE)\n\n    # Decide on number of samples for training\n    train_end = int(train_fraction * len(labels))\n\n    train_features = np.array(features[:train_end])\n    valid_features = np.array(features[train_end:])\n\n    train_labels = labels[:train_end]\n    valid_labels = labels[train_end:]\n\n    # Convert to arrays\n    X_train, X_valid = np.array(train_features), np.array(valid_features)\n\n    # Using int8 for memory savings\n    y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n    y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n\n    # One hot encoding of labels\n    for example_index, word_index in enumerate(train_labels):\n        y_train[example_index, word_index] = 1\n\n    for example_index, word_index in enumerate(valid_labels):\n        y_valid[example_index, word_index] = 1\n\n    # Memory management\n    import gc\n    gc.enable()\n    del features, labels, train_features, valid_features, train_labels, valid_labels\n    gc.collect()\n\n    return X_train, X_valid, y_train, y_valid\n\ndef make_sequences(texts, training_length = 50,\n                   lower = True, filters=\'!""#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\'):\n    """"""Turn a set of texts into sequences of integers""""""\n    \n    # Create the tokenizer object and train on texts\n    tokenizer = Tokenizer(lower=lower, filters=filters)\n    tokenizer.fit_on_texts(texts)\n    \n    # Create look-up dictionaries and reverse look-ups\n    word_idx = tokenizer.word_index\n    idx_word = tokenizer.index_word\n    num_words = len(word_idx) + 1\n    word_counts = tokenizer.word_counts\n    \n    print(f\'There are {num_words} unique words.\')\n    \n    # Convert text to sequences of integers\n    sequences = tokenizer.texts_to_sequences(texts)\n    \n    # Limit to sequences with more than training length tokens\n    seq_lengths = [len(x) for x in sequences]\n    over_idx = [i for i, l in enumerate(seq_lengths) if l > (training_length + 20)]\n    \n    new_texts = []\n    new_sequences = []\n    \n    # Only keep sequences with more than training length tokens\n    for i in over_idx:\n        new_texts.append(texts[i])\n        new_sequences.append(sequences[i])\n        \n    features = []\n    labels = []\n    \n    # Iterate through the sequences of tokens\n    for seq in new_sequences:\n        \n        # Create multiple training examples from each sequence\n        for i in range(training_length, len(seq)):\n            # Extract the features and label\n            extract = seq[i - training_length: i + 1]\n            \n            # Set the features and label\n            features.append(extract[:-1])\n            labels.append(extract[-1])\n    \n    print(f\'There are {len(features)} sequences.\')\n    \n    # Return everything needed for setting up the model\n    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, features, labels\n\ndef generate_output(model,\n                    sequences,\n                    idx_word,\n                    seed_length=50,\n                    new_words=50,\n                    diversity=1,\n                    return_output=False,\n                    n_gen=1):\n    """"""Generate `new_words` words of output from a trained model and format into HTML.""""""\n\n    # Choose a random sequence\n    seq = random.choice(sequences)\n\n    # Choose a random starting point\n    seed_idx = random.randint(0, len(seq) - seed_length - 10)\n    # Ending index for seed\n    end_idx = seed_idx + seed_length\n\n    gen_list = []\n\n    for n in range(n_gen):\n        # Extract the seed sequence\n        seed = seq[seed_idx:end_idx]\n        original_sequence = [idx_word[i] for i in seed]\n        generated = seed[:] + [\'#\']\n\n        # Find the actual entire sequence\n        actual = generated[:] + seq[end_idx:end_idx + new_words]\n\n        # Keep adding new words\n        for i in range(new_words):\n\n            # Make a prediction from the seed\n            preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(\n                np.float64)\n\n            # Diversify\n            preds = np.log(preds) / diversity\n            exp_preds = np.exp(preds)\n\n            # Softmax\n            preds = exp_preds / sum(exp_preds)\n\n            # Choose the next word\n            probas = np.random.multinomial(1, preds, 1)[0]\n\n            next_idx = np.argmax(probas)\n\n            # New seed adds on old word\n            #             seed = seed[1:] + [next_idx]\n            seed += [next_idx]\n            generated.append(next_idx)\n\n        # Showing generated and actual abstract\n        n = []\n\n        for i in generated:\n            n.append(idx_word.get(i, \'< --- >\'))\n\n        gen_list.append(n)\n\n    a = []\n\n    for i in actual:\n        a.append(idx_word.get(i, \'< --- >\'))\n\n    a = a[seed_length:]\n\n    gen_list = [gen[seed_length:seed_length + len(a)] for gen in gen_list]\n\n    if return_output:\n        return original_sequence, gen_list, a\n\n    # HTML formatting\n    seed_html = \'\'\n    seed_html = addContent(seed_html, header(\n        \'Seed Sequence\', color=\'darkblue\'))\n    seed_html = addContent(seed_html,\n                           box(remove_spaces(\' \'.join(original_sequence))))\n\n    gen_html = \'\'\n    gen_html = addContent(gen_html, header(\'RNN Generated\', color=\'darkred\'))\n    gen_html = addContent(gen_html, box(remove_spaces(\' \'.join(gen_list[0]))))\n\n    a_html = \'\'\n    a_html = addContent(a_html, header(\'Actual\', color=\'darkgreen\'))\n    a_html = addContent(a_html, box(remove_spaces(\' \'.join(a))))\n\n    return seed_html, gen_html, a_html\n\n\n\ndef header(text, color = \'black\', gen_text = None):\n    if gen_text:\n        raw_html = f\'<h1 style=""color: {color};""><p><center>\' + str(\n        text) + \'<span style=""color: red"">\' + str(gen_text) + \'</center></p></h1>\'\n    else:\n        raw_html = f\'<h1 style=""color: {color};""><center>\' + str(\n            text) + \'</center></h1>\'\n    return raw_html\n\n\ndef box(text, gen_text=None):\n    if gen_text:\n        raw_html = \'<div style=""border:1px inset black;padding:1em;font-size: 20px;""> <p>\' + str(\n            text) +\'<span style=""color: red"">\' + str(gen_text) + \'</p></div>\'\n\n    else:\n        raw_html = \'<div style=""border:1px inset black;padding:1em;font-size: 20px;"">\' + str(\n            text) + \'</div>\'\n    return raw_html\n\n\ndef addContent(old_html, raw_html):\n    old_html += raw_html\n    return old_html\n\ndef seed_sequence(model, s, word_idx, idx_word, \n                  diversity = 0.75, num_words = 50):\n    """"""Generate output starting from a seed sequence.""""""\n    # Original formated text\n    start = format_sequence(s).split()\n    gen = []\n    s = start[:]\n    # Generate output\n    for _ in range(num_words):\n        # Conver to arry\n        x = np.array([word_idx.get(word, 0) for word in s]).reshape((1, -1))\n\n        # Make predictions\n        preds = model.predict(x)[0].astype(float)\n\n        # Diversify\n        preds = np.log(preds) / diversity\n        exp_preds = np.exp(preds)\n        # Softmax\n        preds = exp_preds / np.sum(exp_preds)\n        # Pick next index\n        next_idx = np.argmax(np.random.multinomial(1, preds, size = 1))\n        s.append(idx_word[next_idx])\n        gen.append(idx_word[next_idx])\n    \n    # Formatting in html\n    start = remove_spaces(\' \'.join(start)) + \' \'\n    gen = remove_spaces(\' \'.join(gen)) \n    html = \'\'\n    html = addContent(html, header(\'Input Seed \', color = \'black\', gen_text = \'Network Output\'))\n    html = addContent(html, box(start, gen))\n    return html\n\ndef guess_human(model, sequences, idx_word, seed_length=50):\n    """"""Produce 2 RNN sequences and play game to compare to actaul.\n       Diversity is randomly set between 0.5 and 1.25""""""\n    \n    new_words = np.random.randint(10, 50)\n    diversity = np.random.uniform(0.5, 1.25)\n    sequence, gen_list, actual = generate_output(model, sequences, idx_word, seed_length, new_words,\n                                                 diversity=diversity, return_output=True, n_gen = 2)\n    gen_0, gen_1 = gen_list\n    \n    output = {\'sequence\': remove_spaces(\' \'.join(sequence)),\n              \'computer0\': remove_spaces(\' \'.join(gen_0)),\n              \'computer1\': remove_spaces(\' \'.join(gen_1)),\n              \'human\': remove_spaces(\' \'.join(actual))}\n    \n    print(f""Seed Sequence: {output[\'sequence\']}\\n"")\n    \n    choices = [\'human\', \'computer0\', \'computer1\']\n          \n    selected = []\n    i = 0\n    while len(selected) < 3:\n        choice = random.choice(choices)\n        selected.append(choice)\n        print(f\'\\nOption {i + 1} {output[choice]}\')\n        choices.remove(selected[-1])\n        i += 1\n    \n    print(\'\\n\')\n    guess = int(input(\'Enter option you think is human (1-3): \')) - 1\n    print(\'\\n\')\n    \n    if guess == np.where(np.array(selected) == \'human\')[0][0]:\n        print(\'*\' * 3 + \'Correct\' + \'*\' * 3 + \'\\n\')\n        print(\'-\' * 60)\n        print(\'Ordering: \', selected)\n    else:\n        print(\'*\' * 3 + \'Incorrect\' + \'*\' * 3 + \'\\n\')\n        print(\'-\' * 60)\n        print(\'Correct Ordering: \', selected)\n          \n    print(\'Diversity\', round(diversity, 2))\n    \ndef make_sequences_new(texts,\n                   training_length=50,\n                   lower=True,\n                   filters=\'!""#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\'):\n    """"""Turn a set of texts into sequences of integers""""""\n\n    # Create the tokenizer object and train on texts\n    tokenizer = Tokenizer(lower=lower, filters=filters)\n    tokenizer.fit_on_texts(texts)\n\n    # Convert text to sequences of integers\n    sequences = tokenizer.texts_to_sequences(texts)\n\n    # Limit to sequences with more than (training length + 20) tokens\n    seq_lengths = [len(x) for x in sequences]\n    over_idx = [\n        i for i, l in enumerate(seq_lengths) if l > (training_length + 20)\n    ]\n\n    new_texts = []\n\n    # Only keep sequences with more than training length tokens\n    for i in over_idx:\n        new_texts.append(texts[i])\n    \n    tokenizer = Tokenizer(lower=lower, filters=filters)\n    # Refit on long texts\n    tokenizer.fit_on_texts(new_texts)\n    new_sequences = tokenizer.texts_to_sequences(new_texts)\n    \n    # Create look-up dictionaries and reverse look-ups\n    word_idx = tokenizer.word_index\n    idx_word = tokenizer.index_word\n    num_words = len(word_idx) + 1\n    word_counts = tokenizer.word_counts\n\n    print(f\'There are {num_words} unique words.\')\n\n    features = []\n    labels = []\n\n    # Iterate through the sequences of tokens\n    for seq in new_sequences:\n\n        # Create multiple training examples from each sequence\n        for i in range(training_length, len(seq)):\n            # Extract the features and label\n            extract = seq[i - training_length:i + 1]\n\n            # Set the features and label\n            features.append(extract[:-1])\n            labels.append(extract[-1])\n\n    print(f\'There are {len(features)} training sequences.\')\n\n    # Return everything needed for setting up the model\n    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, features, labels'"
deeplearning/tensorflow/tf_intro.py,1,"b'E:\\Python>python.exe\nPython 3.6.3 (v3.6.3:2c5fed8, Oct  3 2017, 18:11:49) [MSC v.1900 64 bit (AMD64)] on win32\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\n>>> import tensorflow as tf\n>>> tf.session()\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nAttributeError: module \'tensorflow\' has no attribute \'session\'\n>>> tf.Session()\n2018-10-03 10:21:40.779195: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n<tensorflow.python.client.session.Session object at 0x0000020816B24DA0>\n\n>>> s = tf.Session()\n>>> s\n<tensorflow.python.client.session.Session object at 0x0000020816B24DA0>\n\n# placeholder\n>>> x = tf.placeholder(tf.float32, shape=[2,2])\n>>> x\n<tf.Tensor \'Placeholder:0\' shape=(2, 2) dtype=float32>\n>>>\n\n# identity\n>>> y = tf.identity(x)\n>>> y\n<tf.Tensor \'Identity:0\' shape=(2, 2) dtype=float32>\n>>>\n\n# load numpy\n>>> import numpy as np\n\n>>> x_vals = np.random.rand(2,2)\n>>> x_vals\narray([[0.97882811, 0.60712575],\n       [0.18916714, 0.35782924]])\n>>>\n\n# tf run\n>>> s.run(y, feed_dict={x: x_vals})\narray([[0.97882813, 0.60712576],\n       [0.18916714, 0.35782924]], dtype=float32)\n>>>\n\n\n'"
helpers/pandas/ds_pandas_datetime_1.py,0,"b""# loaf libraries\n\nimport pandas as pd\n\n# read csv file\ndf = pd.read_csv('input.csv')\n\nprint(df.sample(5))\n'''\n     time        date\n4  198898  2017-12-06\n3  198897  2017-12-06\n2  198896  2017-12-05\n1  198895  2017-12-05\n5  198899  2017-12-07\n'''\n\n# using pandas datetime\nprint(pd.to_datetime(df['date']).dt.year.value_counts())\n# 2017    6\n# Name: date, dtype: int64\n"""
nlp/pmi/pmi.py,7,"b'\n# coding: utf-8\n\n# # Comparables \n\n# In[1]:\n\n\n\'\'\'\n\n\'\'\'\n\n# load libraries\nimport numpy as np\nimport pandas as pd\n\nfrom scipy import sparse\nfrom scipy.sparse import linalg \nfrom collections import Counter\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom nltk.corpus import stopwords\n\n\n# ### Subject Company Data\n\n# In[2]:\n\n\n# Subject Company Description\nsub_com_keywords = [""railway services"", ""rail parts"", ""rail supplies"", ""rail maintenance"", ""rail traffic"", ""railway service"", \n                    ""rail infrastructure"", ""electricity"", ""high speed rail"", ""mono rail"", ""electric train"", ""railways"",\n                    ""tunnel"", ""maglev"", ""station"", ""rail industry"", ""train"", ""metro"", ""platform"", ""service""]\nprint(sub_com_keywords)\n\n\n# ### Comparable Company Data\n\n# In[3]:\n\n\n# loading data\n\n#fname = \'Sample_Comparables.csv\'\n#df = pd.read_csv(fname)\n\nfname1 = \'classes.csv\'\nfname2 = \'keys.xlsx\'\n\ndf1 = pd.read_csv(fname1, index_col=\'ID\').reset_index()\ndf2 = pd.read_excel(fname2)\n\ndf2.head()\n\n\n# In[4]:\n\n\n#keywords = df2[\'Keywords-manual\'].tolist()\n\ndf3 = df2.groupby(\'ID\', as_index=False).agg(lambda x: list(x))\ndf3.head()\n\n\n# In[5]:\n\n\nkeywords = df3[\'Keywords-manual\'].tolist()\nkeywords[3]\n\n\n# In[6]:\n\n\n# using keywords as words for PMI\n\ndescriptions = keywords\n\nprint(""total: "", len(descriptions))\n\n# remove single word headlines\ndescriptions = [d for d in descriptions if len(d) > 1]\n# show results\ndescriptions[0:1]\n\n\n# In[7]:\n\n\n# treating whole set of keywords as input\ndescriptions.append(sub_com_keywords)\n\n\n# ### Unigrams\n\n# In[8]:\n\n\n# Comparable Company Unigrams\ntok2indx = dict()\nunigram_counts = Counter()\nfor ii, description in enumerate(descriptions):\n    if ii % 5 == 0:\n        print(f\'finished {ii/len(descriptions):.2%} of descriptions\')\n    for token in description:\n        unigram_counts[token] += 1\n        if token not in tok2indx:\n            tok2indx[token] = len(tok2indx)\nindx2tok = {indx:tok for tok,indx in tok2indx.items()}\nprint(\'done\')\nprint(\'vocabulary size: {}\'.format(len(unigram_counts)))\nprint(\'most common: {}\'.format(unigram_counts.most_common(10)))\n\n\n# ### Skipgrams\n\n# In[9]:\n\n\n# Comparable Company Skipgrams\n# note add dynammic window hyperparameter\nback_window = 2\nfront_window = 2\nskipgram_counts = Counter()\nfor idescription, description in enumerate(descriptions):\n    for ifw, fw in enumerate(description):\n        icw_min = max(0, ifw - back_window)\n        icw_max = min(len(description) - 1, ifw + front_window)\n        icws = [ii for ii in range(icw_min, icw_max + 1) if ii != ifw]\n        for icw in icws:\n            skipgram = (description[ifw], description[icw])\n            skipgram_counts[skipgram] += 1    \n    if idescription % 500 == 0:\n        print(f\'finished {idescription/len(descriptions):.2%} of descriptions\')\n        \nprint(\'done\')\nprint(\'number of skipgrams: {}\'.format(len(skipgram_counts)))\nprint(\'most common: {}\'.format(skipgram_counts.most_common(30)))\n\n\n# In[10]:\n\n\n# 10 most common skip-grams\nprint(skipgram_counts.most_common(10))\n\n\n# In[11]:\n\n\nprint(""Number of Skipgrams: "", len(skipgram_counts))\n\n\n# ### Word-Count Matrix\n\n# In[12]:\n\n\nrow_indxs = []\ncol_indxs = []\ndat_values = []\nii = 0\nfor (tok1, tok2), sg_count in skipgram_counts.items():\n    ii += 1\n    if ii % 500 == 0:\n        print(f\'finished {ii/len(skipgram_counts):.2%} of skipgrams\')\n    tok1_indx = tok2indx[tok1]\n    tok2_indx = tok2indx[tok2]\n        \n    row_indxs.append(tok1_indx)\n    col_indxs.append(tok2_indx)\n    dat_values.append(sg_count)\n    \nwwcnt_mat = sparse.csr_matrix((dat_values, (row_indxs, col_indxs)))\nprint(\'done\')\n\n\n# In[13]:\n\n\n# normalize word-count matrix\nwwcnt_norm_mat = normalize(wwcnt_mat, norm=\'l2\', axis=1)\n#wwcnt_norm_mat\n\n\n# ### Cosine Similarity\n\n# In[14]:\n\n\ndef ww_sim(word, mat, topn=10):\n    """"""find topn most similar words to word""""""\n    indx = tok2indx[word]\n    \n    # check type of \'mat\'\n    if isinstance(mat, sparse.csr_matrix):\n        v1 = mat.getrow(indx)\n    else:\n        print(""not sparse"")\n        v1 = mat[indx:indx+1, :]\n    \n    # using sklearn cosine similarity\n    sims = cosine_similarity(mat, v1).flatten()\n    sindxs = np.argsort(-sims)\n    sim_word_scores = [(indx2tok[sindx], sims[sindx]) for sindx in sindxs[0:topn]]\n    \n    return sim_word_scores\n\n\n# ### Pointwise Mutual Information Matrices\n# \n# The pointwise mutual information (PMI) for a (word, context) pair in our corpus is defined as the probability of their co-occurrence divided by the probabilities of them appearing individually, \n# $$\n# {\\rm pmi}(w, c) = \\log \\frac{p(w, c)}{p(w) p(c)}\n# $$\n# \n# $$\n# p(w, c) = \\frac{\n# f_{i,j}\n# }{\n# \\sum_{i=1}^N \\sum_{j=1}^N f_{i,j}\n# }, \\quad \n# p(w) = \\frac{\n# \\sum_{j=1}^N f_{i,j}\n# }{\n# \\sum_{i=1}^N \\sum_{j=1}^N f_{i,j}\n# }, \\quad\n# p(c) = \\frac{\n# \\sum_{i=1}^N f_{i,j}\n# }{\n# \\sum_{i=1}^N \\sum_{j=1}^N f_{i,j}\n# }\n# $$\n# where $f_{i,j}$ is the word-word count matrix we defined above.\n# In addition we can define the positive pointwise mutual information as, \n# $$\n# {\\rm ppmi}(w, c) = {\\rm max}\\left[{\\rm pmi(w,c)}, 0 \\right]\n# $$\n# \n# Note that the definition of PMI above implies that ${\\rm pmi}(w, c) = {\\rm pmi}(w, c)$ and so this matrix will be symmetric.  \n# \n# \n# <b>In the Datafox approach, they find (p(w,v)) by counting co-occurrences of words (w) and (v) in a <u>company\xe2\x80\x99s keywords set</u>, while (p(w)) and (p(v)) are found from <u>total occurrences across all companies\xe2\x80\x99 keywords.</u></b>\n# \n\n# In[15]:\n\n\n# using skip-grams of 2 window\n\nnum_skipgrams = wwcnt_mat.sum()\nassert(sum(skipgram_counts.values())==num_skipgrams)\n\nsum_over_words = np.array(wwcnt_mat.sum(axis=0)).flatten()\nsum_over_contexts = np.array(wwcnt_mat.sum(axis=1)).flatten()\n\nprint(wwcnt_mat.sum(axis=0).shape)\n\n\n# for creating sparce matrices\nrow_indxs = []\ncol_indxs = []\n\npmi_values = []\nppmi_values = []\nspmi_values = []\nsppmi_values = []\n\n\n# In[16]:\n\n\nii = 0\n# looping through all skip-grams of Comparables + Subject Company\nfor (w, v), sg_count in skipgram_counts.items():\n    \n    ii += 1\n    if ii % 1000 == 0:\n        print(f\'finished {ii/len(skipgram_counts):.2%} of skipgrams\')\n    w_indx = tok2indx[w]\n    v_indx = tok2indx[v]\n    \n    nwv = sg_count\n    pwv = nwv / num_skipgrams\n    \n    nw = sum_over_contexts[w_indx]\n    pw = nw / num_skipgrams\n    \n    nv = sum_over_words[v_indx]\n    pv = nv / num_skipgrams\n\n    pmi = np.log2(pwv/(pw*pv))\n    ppmi = max(pmi, 0)\n    \n    row_indxs.append(w_indx)\n    col_indxs.append(v_indx)\n\n    pmi_values.append(pmi)\n    ppmi_values.append(ppmi)\n    \npmi_mat = sparse.csr_matrix((pmi_values, (row_indxs, col_indxs)))\nppmi_mat = sparse.csr_matrix((ppmi_values, (row_indxs, col_indxs)))\n\n\nprint(\'done\')\n\n\n# In[17]:\n\n\n# dicts mapping of words and indices\nw = \'France\'\nw_indx = tok2indx[w]\nsum_over_contexts[w_indx], w_indx, indx2tok[w_indx]\n\n\n# In[18]:\n\n\n#print(pmi_mat)\n\n\n# In[19]:\n\n\nww_sim(\'natural gas\', pmi_mat)\n\n\n# In[20]:\n\n\nww_sim(\'railway services\', pmi_mat)\n\n\n# ### Singular Value Decomposition (SVD)\n# \n# With the PMI and PPMI matrices in hand, we can apply a singular value decomposition to create dense word vectors from the sparse ones we\'ve been using. \n\n# In[21]:\n\n\n# svd    \nembedding_size = 25\nuu, ss, vv = linalg.svds(pmi_mat, embedding_size) \n\nprint(\'vocab size: {}\'.format(len(unigram_counts)))\nprint(\'embedding size: {}\'.format(embedding_size))\nprint(\'uu.shape: {}\'.format(uu.shape))\nprint(\'ss.shape: {}\'.format(ss.shape))\nprint(\'vv.shape: {}\'.format(vv.shape))\n\nunorm = uu / np.sqrt(np.sum(uu*uu, axis=1, keepdims=True))\nvnorm = vv / np.sqrt(np.sum(vv*vv, axis=0, keepdims=True))\n\nword_vecs = uu + vv.T\nword_vecs_norm = word_vecs / np.sqrt(np.sum(word_vecs*word_vecs, axis=1, keepdims=True))\n\n\n# In[22]:\n\n\nww_sim(\'natural gas\', word_vecs_norm)\n\n\n# In[23]:\n\n\nww_sim(\'railway services\', word_vecs_norm, 50)\n\n\n# ---\n'"
nlp/pytextrank/pytextrank.py,2,"b'\n#!/usr/bin/env python\n# encoding: utf-8\n\nfrom collections import namedtuple\nfrom datasketch import MinHash\nfrom graphviz import Digraph\nimport hashlib\nimport json\nimport math\nimport networkx as nx\nimport os\nimport os.path\nimport re\nimport spacy\nimport statistics\nimport string\nimport regex\nimport ast\nfrom gensim.models import Word2Vec\n\nDEBUG = False # True\n\nParsedGraf = namedtuple(\'ParsedGraf\', \'id, sha1, graf\')\nWordNode = namedtuple(\'WordNode\', \'word_id, raw, root, pos, keep, idx\')\nRankedLexeme = namedtuple(\'RankedLexeme\', \'text, rank, ids, pos, count\')\nSummarySent = namedtuple(\'SummarySent\', \'dist, idx, text\')\n\n\n######################################################################\n## filter the novel text versus quoted text in an email message\n\nPAT_FORWARD = regex.compile(""\\n\\-+ Forwarded message \\-+\\n"")\nPAT_REPLIED = regex.compile(""\\nOn.*\\d+.*\\n?wrote\\:\\n+\\>"")\nPAT_UNSUBSC = regex.compile(""\\n\\-+\\nTo unsubscribe,.*\\nFor additional commands,.*"")\n\n# spacy models - considering small model \'en_core_web_sm\'\nspacy_models = [\'en_core_web_sm\', \'en_core_web_md\']\nspacy_model = spacy_models[0]\n\n# stopwords file\nstopfile = ""stop.txt""\n\n######################################################################\n## get weights and similarity from gensim word2vec model\n\ndef w2v_similarity(word2vec, word1, word2):\n    """"""\n    input: word pair from pytextrank\n    output: w2v distance between the words (float)\n    """"""\n    global STOPWORDS\n    \n    w2v = word2vec\n    wl = [word1, word2]\n\n    # default similarity\n    sim = 1.0\n    \n    if not STOPWORDS:\n        STOPWORDS = load_stopwords(stopfile)\n\n    if not any(w in wl for w in STOPWORDS):\n        if word1 in w2v.wv.vocab and word2 in w2v.wv.vocab:\n            similarity = w2v.wv.similarity(word1, word2)\n            return sim + similarity if similarity > 0 else sim\n        \n    return sim\n\n\ndef w2v_weighted_edges(word2vec, word):\n    """"""\n    input: word pair from pytextrank\n    output: weighted edges for most similar 2 words (list of tuples)\n    """"""\n    global STOPWORDS\n    \n    w2v = word2vec\n    \n    # default similarity\n    sim = 1.0\n    l = []\n\n    if not STOPWORDS:\n        STOPWORDS = load_stopwords(stopfile)\n\n    if word in w2v.wv.vocab:\n        similar = w2v.wv.most_similar(word, topn=2)\n        for s in similar:\n            similar_word = s[0]\n            if similar_word not in STOPWORDS:\n                similar_word_weight = sim + s[1]\n                l.append((similar_word, word, similar_word_weight))\n\n        return l\n    return None\n\ndef split_grafs (lines):\n    """"""\n    segment the raw text into paragraphs\n    """"""\n    graf = []\n\n    for line in lines:\n        line = line.strip()\n\n        if len(line) < 1:\n            if len(graf) > 0:\n                yield ""\\n"".join(graf)\n                graf = []\n        else:\n            graf.append(line)\n\n    if len(graf) > 0:\n        yield ""\\n"".join(graf)\n\n\ndef filter_quotes (text, is_email=True):\n    """"""\n    filter the quoted text out of a message\n    """"""\n    global DEBUG\n    global PAT_FORWARD, PAT_REPLIED, PAT_UNSUBSC\n\n    if is_email:\n        text = filter(lambda x: x in string.printable, text)\n\n        if DEBUG:\n            print(""text:"", text)\n\n        # strip off quoted text in a forward\n        m = PAT_FORWARD.split(text, regex.M(concurrent=True))\n\n        if m and len(m) > 1:\n            text = m[0]\n\n        # strip off quoted text in a reply\n        m = PAT_REPLIED.split(text, regex.M(concurrent=True))\n\n        if m and len(m) > 1:\n            text = m[0]\n\n        # strip off any trailing unsubscription notice\n        m = PAT_UNSUBSC.split(text, regex.M(concurrent=True))\n\n        if m:\n            text = m[0]\n\n    # replace any remaining quoted text with blank lines\n    lines = []\n\n    for line in text.split(""\\n""):\n        if line.startswith("">""):\n            lines.append("""")\n        else:\n            lines.append(line)\n\n    return list(split_grafs(lines))\n\n\n######################################################################\n## parse and markup text paragraphs for semantic analysis\n\nPAT_PUNCT = regex.compile(r\'^\\W+$\')\nPAT_SPACE = regex.compile(r\'\\_+$\')\n\nPOS_KEEPS = [\'v\', \'n\', \'j\']\nPOS_LEMMA = [\'v\', \'n\']\nUNIQ_WORDS = { ""."": 0 }\n\n\ndef is_not_word (word):\n    return PAT_PUNCT.match(word) or PAT_SPACE.match(word)\n\n\ndef get_word_id (root):\n    """"""\n    lookup/assign a unique identify for each word root\n    """"""\n    global UNIQ_WORDS\n\n    # in practice, this should use a microservice via some robust\n    # distributed cache, e.g., Redis, Cassandra, etc.\n    if root not in UNIQ_WORDS:\n        UNIQ_WORDS[root] = len(UNIQ_WORDS)\n\n    return UNIQ_WORDS[root]\n\n\ndef fix_microsoft (foo):\n    """"""\n    fix special case for `c#`, `f#`, etc.; thanks Microsoft\n    """"""\n    i = 0\n    bar = []\n\n    while i < len(foo):\n        text, lemma, pos, tag = foo[i]\n\n        if (text == ""#"") and (i > 0):\n            prev_tok = bar[-1]\n\n            prev_tok[0] += ""#""\n            prev_tok[1] += ""#""\n\n            bar[-1] = prev_tok\n        else:\n            bar.append(foo[i])\n\n        i += 1\n\n    return bar\n\n\ndef fix_hypenation (foo):\n    """"""\n    fix hyphenation in the word list for a parsed sentence\n    """"""\n    i = 0\n    bar = []\n\n    while i < len(foo):\n        text, lemma, pos, tag = foo[i]\n\n        if (tag == ""HYPH"") and (i > 0) and (i < len(foo) - 1):\n            prev_tok = bar[-1]\n            next_tok = foo[i + 1]\n\n            prev_tok[0] += ""-"" + next_tok[0]\n            prev_tok[1] += ""-"" + next_tok[1]\n\n            bar[-1] = prev_tok\n            i += 2\n        else:\n            bar.append(foo[i])\n            i += 1\n\n    return bar\n\n\ndef parse_graf (doc_id, graf_text, base_idx, spacy_nlp=None):\n    """"""\n    CORE ALGORITHM: parse and markup sentences in the given paragraph\n    """"""\n    global DEBUG\n    global POS_KEEPS, POS_LEMMA, SPACY_NLP\n    \n    # set up the spaCy NLP parser\n    if not spacy_nlp:\n        if not SPACY_NLP:\n            SPACY_NLP = spacy.load(spacy_model)\n\n        spacy_nlp = SPACY_NLP\n    \n    markup = []\n    new_base_idx = base_idx\n    #doc = spacy_nlp(graf_text, parse=True)\n    doc = spacy_nlp(graf_text)\n    \n    for span in doc.sents:\n        \n        graf = []\n        digest = hashlib.sha1()\n\n        if DEBUG:\n            print(span)\n\n        # build a word list, on which to apply corrections\n        word_list = []\n        append = word_list.append\n        for tag_idx in range(span.start, span.end):\n            token = doc[tag_idx]\n\n            if DEBUG:\n                print(""IDX"", tag_idx, token.text, token.tag_, token.pos_)\n                print(""reg"", is_not_word(token.text))\n\n            append([token.text, token.lemma_, token.pos_, token.tag_])\n\n        # scan the parsed sentence, annotating as a list of `WordNode`\n        corrected_words = fix_microsoft(fix_hypenation(word_list))\n\n        update = digest.update\n        append = graf.append\n\n        for tok_text, tok_lemma, tok_pos, tok_tag in corrected_words:\n            word = WordNode(word_id=0, raw=tok_text, root=tok_text.lower(), pos=tok_tag, keep=0, idx=new_base_idx)\n\n            if is_not_word(tok_text) or (tok_tag == ""SYM""):\n                # a punctuation, or other symbol\n                pos_family = \'.\'\n                word = word._replace(pos=pos_family)\n            else:\n                pos_family = tok_tag.lower()[0]\n\n            if pos_family in POS_LEMMA:\n                # can lemmatize this word?\n                word = word._replace(root=tok_lemma)\n\n            if pos_family in POS_KEEPS:\n                word = word._replace(word_id=get_word_id(word.root), keep=1)\n\n            update(word.root.encode(\'utf-8\'))\n\n            # schema: word_id, raw, root, pos, keep, idx\n            if DEBUG:\n                print(word)\n\n            append(list(word))\n            new_base_idx += 1\n\n        markup.append(ParsedGraf(id=doc_id, sha1=digest.hexdigest(), graf=graf))\n\n    return markup, new_base_idx\n\ndef parse_doc_process (input_queue, output_queue, output_queue2):\n    """"""\n    parse one document to prep for TextRank\n    """"""\n    global DEBUG\n\n    while True:\n       try:\n           jsontext = input_queue.get()\n           json_iter = json_iters(jsontext)\n           doc_graph = list()\n           for meta in json_iter:\n               base_idx = 0\n               \n               for graf_text in filter_quotes(meta[""text""], is_email=False):\n                   if DEBUG:\n                       print(""graf_text:"", graf_text)\n\n                   # build word2vec model\n                   try:\n                       sentences = [sentence.split() for sentence in list(graf_text.lower().split(\'. \'))]\n                       word2vec = Word2Vec(sentences, seed=9, size=300, window=1, iter=900, min_count=1)\n                   except:\n                       print(""error building word2vec model"")\n\n                   grafs, new_base_idx = parse_graf(meta[""id""], graf_text, base_idx)\n                   base_idx = new_base_idx\n                   for graf in grafs:\n                        doc_graph.append(graf._asdict())\n\n              #print(""PARSE_DOC_PROCESS: pushing parse_doc_process to output queues"")\n\n               output_queue.put((doc_graph, word2vec))\n               output_queue2.put(doc_graph)\n       except:\n            print(""Error in parse_doc_process"")\n\ndef parse_doc (json_iter):\n    """"""\n    parse one document to prep for TextRank\n    """"""\n    global DEBUG\n\n    for meta in json_iter:\n        base_idx = 0\n\n        for graf_text in filter_quotes(meta[""text""], is_email=False):\n            if DEBUG:\n                print(""graf_text:"", graf_text)\n\n            grafs, new_base_idx = parse_graf(meta[""id""], graf_text, base_idx)\n            base_idx = new_base_idx\n\n            for graf in grafs:\n                yield graf\n\n\ndef parse_text (text):\n    """"""\n    parse one document to prep for TextRank\n    """"""\n    global DEBUG\n\n    #for meta in json_iter:\n    #for meta in json_iter_m:\n    base_idx = 0\n\n    for graf_text in filter_quotes(text, is_email=False):\n        if DEBUG:\n            print(""graf_text:"", graf_text)\n\n        grafs, new_base_idx = parse_graf(123, graf_text, base_idx)\n        base_idx = new_base_idx\n\n        for graf in grafs:\n            yield graf\n\n\n######################################################################\n## graph analytics\n\ndef get_tiles (graf, size=3):\n    """"""\n    generate word pairs for the TextRank graph\n    """"""\n    keeps = list(filter(lambda w: w.word_id > 0, graf))\n    keeps_len = len(keeps)\n\n    for i in iter(range(0, keeps_len - 1)):\n        w0 = keeps[i]\n\n        for j in iter(range(i + 1, min(keeps_len, i + 1 + size))):\n            w1 = keeps[j]\n\n            if (w1.idx - w0.idx) <= size:\n                yield (w0.root, w1.root,)\n\n\ndef build_graph (json_iter, word2vec):\n    """"""\n    construct the TextRank graph from parsed paragraphs\n    """"""\n    global DEBUG, WordNode\n    graph = nx.DiGraph()\n    \n    for meta in json_iter:\n        if DEBUG:\n            print(meta[""graf""])\n\n        for pair in get_tiles(map(WordNode._make, meta[""graf""])):\n            if DEBUG:\n                print(pair)\n\n            for word_id in pair:\n                if not graph.has_node(word_id):\n                    graph.add_node(word_id)  \n\n                try:\n                    # gensim weighted edges (most_similar)\n                    wedges = w2v_weighted_edges(word2vec, word_id)\n                    if wedges:\n                        graph.add_weighted_edges_from(wedges)\n                except:\n                    print(""Error adding weighted edges"")\n\n                try:\n                    #graph.edge[pair[0]][pair[1]][""weight""] += 1.0\n                    # graph.adj[pair[0]][pair[1]][""weight""] += 1.0\n                    # gensim word2vec similarity\n                    similarity = w2v_similarity(word2vec, pair[0], pair[1])\n                    graph.adj[pair[0]][pair[1]][""weight""] += similarity\n                    graph.adj[pair[1]][pair[0]][""weight""] += similarity\n\n                except KeyError:\n                    graph.add_edge(pair[0], pair[1], weight=1.0)\n\n                    \n    return graph\n\n\ndef write_dot (graph, ranks, path=""graph.dot""):\n    """"""\n    output the graph in Dot file format\n    """"""\n    dot = Digraph()\n\n    for node in graph.nodes():\n        dot.node(node, ""%s %0.3f"" % (node, ranks[node]))\n\n    for edge in graph.edges():\n        dot.edge(edge[0], edge[1], constraint=""false"")\n        \n    with open(path, \'w\') as f:\n        f.write(dot.source)\n\n\ndef render_ranks (graph, ranks, dot_file=""graph.dot""):\n    """"""\n    render the TextRank graph for visual formats\n    """"""\n    if dot_file:\n        write_dot(graph, ranks, path=dot_file)\n\n    ## omitted since matplotlib isn\'t reliable enough\n    #import matplotlib.pyplot as plt\n    #nx.draw_networkx(graph)\n    #plt.savefig(img_file)\n    #plt.show()\n\n\ndef text_rank (path):\n    """"""\n    run the TextRank algorithm\n    """"""\n    graph = build_graph(json_iter(path))\n    #graph = build_graph(json_iter_m(path))\n    ranks = nx.pagerank(graph)\n\n    return graph, ranks\n\ndef text_rank_process (input_queue, output_queue):\n    """"""\n    Run the TextRank algorithm\n    """"""\n    while True:\n       var1, word2vec = input_queue.get()\n       graph = build_graph(json_iterm(var1), word2vec)\n       ranks = nx.pagerank(graph)\n       output_queue.put(ranks)\n\n######################################################################\n## collect key phrases\n\nSPACY_NLP = None\nSTOPWORDS = None\n\n\ndef load_stopwords (stop_file):\n    stopwords = set([])\n\n    # provide a default if needed\n    if not stop_file:\n        stop_file = ""stop.txt""\n\n    # check whether the path is fully qualified\n    if os.path.isfile(stop_file):\n        stop_path = stop_file\n\n    # check for the file in the current working directory\n    else:\n        cwd = os.getcwd()\n        stop_path = os.path.join(cwd, stop_file)\n\n        # check for the file in the same directory as this code module\n        if not os.path.isfile(stop_path):\n            loc = os.path.realpath( os.path.join(cwd, os.path.dirname(__file__)) )\n            stop_path = os.path.join(loc, stop_file)\n\n    try:\n        with open(stop_path, ""r"") as f:\n            for line in f.readlines():\n                stopwords.add(line.strip().lower())\n    except FileNotFoundError:\n        pass\n\n    return stopwords\n\n\ndef find_chunk_sub (phrase, np, i):\n    for j in iter(range(0, len(np))):\n        p = phrase[i + j]\n\n        if p.text != np[j]:\n            return None\n\n    return phrase[i:i + len(np)]\n\n\ndef find_chunk (phrase, np):\n    """"""\n    leverage noun phrase chunking\n    """"""\n    for i in iter(range(0, len(phrase))):\n        parsed_np = find_chunk_sub(phrase, np, i)\n\n        if parsed_np:\n            return parsed_np\n\n\ndef enumerate_chunks (phrase, spacy_nlp):\n    """"""\n    iterate through the noun phrases\n    """"""\n    if (len(phrase) > 1):\n        found = False\n        text = "" "".join([rl.text for rl in phrase])\n        #doc = spacy_nlp(text.strip(), parse=True)\n        doc = spacy_nlp(text.strip())\n\n        for np in doc.noun_chunks:\n            if np.text != text:\n                found = True\n                yield np.text, find_chunk(phrase, np.text.split("" ""))\n\n        if not found and all([rl.pos[0] != ""v"" for rl in phrase]):\n            yield text, phrase\n\n\ndef collect_keyword (sent, ranks, stopwords):\n    """"""\n    iterator for collecting the single-word keyphrases\n    """"""\n    for w in sent:\n        if (w.word_id > 0) and (w.root in ranks) and (w.pos[0] in ""NV"") and (w.root not in stopwords):\n            rl = RankedLexeme(text=w.raw.lower(), rank=ranks[w.root]/2.0, ids=[w.word_id], pos=w.pos.lower(), count=1)\n\n            if DEBUG:\n                print(rl)\n\n            yield rl\n\n\ndef find_entity (sent, ranks, ent, i):\n    if i >= len(sent):\n        return None, None\n    else:\n        for j in iter(range(0, len(ent))):\n            w = sent[i + j]\n\n            if w.raw != ent[j]:\n                return find_entity(sent, ranks, ent, i + 1)\n\n        w_ranks = []\n        w_ids = []\n\n        for w in sent[i:i + len(ent)]:\n            w_ids.append(w.word_id)\n\n            if w.root in ranks:\n                w_ranks.append(ranks[w.root])\n            else:\n                w_ranks.append(0.0)\n\n        return w_ranks, w_ids\n\n\ndef collect_entities (sent, ranks, stopwords, spacy_nlp):\n    """"""\n    iterator for collecting the named-entities\n    """"""\n    global DEBUG\n    sent_text = "" "".join([w.raw for w in sent])\n\n    if DEBUG:\n        print(""sent:"", sent_text)\n\n    for ent in spacy_nlp(sent_text).ents:\n        if DEBUG:\n            print(""NER:"", ent.label_, ent.text)\n\n        if (ent.label_ not in [""CARDINAL""]) and (ent.text.lower() not in stopwords):\n            w_ranks, w_ids = find_entity(sent, ranks, ent.text.split("" ""), 0)\n\n            if w_ranks and w_ids:\n                rl = RankedLexeme(text=ent.text.lower(), rank=w_ranks, ids=w_ids, pos=ent.label_, count=1)\n\n                if DEBUG:\n                    print(rl)\n\n                yield rl\n\n\ndef collect_phrases (sent, ranks, spacy_nlp):\n    """"""\n    iterator for collecting the noun phrases\n    """"""\n    tail = 0\n    last_idx = sent[0].idx - 1\n    phrase = []\n\n    while tail < len(sent):\n        w = sent[tail]\n\n        if (w.word_id > 0) and (w.root in ranks) and ((w.idx - last_idx) == 1):\n            # keep collecting...\n            rl = RankedLexeme(text=w.raw.lower(), rank=ranks[w.root], ids=w.word_id, pos=w.pos.lower(), count=1)\n            phrase.append(rl)\n        else:\n            # just hit a phrase boundary\n            for text, p in enumerate_chunks(phrase, spacy_nlp):\n                if p:\n                    id_list = [rl.ids for rl in p]\n                    rank_list = [rl.rank for rl in p]\n                    np_rl = RankedLexeme(text=text, rank=rank_list, ids=id_list, pos=""np"", count=1)\n\n                    if DEBUG:\n                        print(np_rl)\n\n                    yield np_rl\n\n            phrase = []\n\n        last_idx = w.idx\n        tail += 1\n\n\ndef calc_rms (values):\n    """"""\n    calculate a root-mean-squared metric for a list of float values\n    """"""\n    #return math.sqrt(sum([x**2.0 for x in values])) / float(len(values))\n    # take the max() which works fine\n    return max(values)\n\ndef normalize_key_phrases_process (input_queue, input_queue2, output_queue ):\n    stopwords =None\n    spacy_nlp=None\n\n    while True:\n        #print(""NORMALIZE_KEYPHRASE_PROCESS: waiting at normalize_key_phrases_process()"")\n        path = input_queue.get()\n        ranks = input_queue2.get()\n        #print(""NORMALIZE_KEYPHRASE_PROCESS: received data in normalize_key_phrases_process()"")\n        """"""\n        collect keyphrases, named entities, etc., while removing stop words\n        """"""\n        global STOPWORDS, SPACY_NLP\n\n        # set up the stop words\n        if (type(stopwords) is list) or (type(stopwords) is set):\n            # explicit conversion to a set, for better performance\n            stopwords = set(stopwords)\n        else:\n            if not STOPWORDS:\n                STOPWORDS = load_stopwords(stopwords)\n\n            stopwords = STOPWORDS\n\n        # set up the spaCy NLP parser\n        if not spacy_nlp:\n            if not SPACY_NLP:\n                SPACY_NLP = spacy.load(spacy_model)\n            spacy_nlp = SPACY_NLP\n\n        # collect keyphrases\n        single_lex = {}\n        phrase_lex = {}\n\n        #data = json.loads(json.dumps(path))\n        if isinstance(path, str):\n            data = json_iter(path)\n        else:\n            data = json_iterm (path)\n   \n  \n        for meta in data:\n            sent = [w for w in map(WordNode._make, meta[""graf""])]\n            for rl in collect_entities(sent, ranks, stopwords, spacy_nlp):\n                id = str(rl.ids)\n\n                if id not in phrase_lex:\n                    phrase_lex[id] = rl\n                else:\n                    prev_lex = phrase_lex[id]\n                    phrase_lex[id] = rl._replace(count = prev_lex.count + 1)\n\n\n            for rl in collect_keyword(sent, ranks, stopwords):\n                id = str(rl.ids)\n        \n                if id not in single_lex:\n                    single_lex[id] = rl\n                else:\n                    prev_lex = single_lex[id]\n                    single_lex[id] = rl._replace(count = prev_lex.count + 1)\n      \n\n            for rl in collect_phrases(sent, ranks, spacy_nlp):\n                id = str(rl.ids)\n        \n                if id not in phrase_lex:\n                    phrase_lex[id] = rl\n                else:\n                    prev_lex = phrase_lex[id]\n                    phrase_lex[id] = rl._replace(count = prev_lex.count + 1)\n   \n     \n        \n        # normalize ranks across single keywords and longer phrases:\n        #    * boost the noun phrases based on their length\n        #    * penalize the noun phrases for repeated words\n        \n        rank_list = [rl.rank for rl in single_lex.values()]\n        \n        if len(rank_list) < 1:\n            max_single_rank = 0\n        else:\n            max_single_rank = max(rank_list)\n\n        repeated_roots = {}\n        \n        for rl in sorted(phrase_lex.values(), key=lambda rl: len(rl), reverse=True):\n            rank_list = []\n\n            for i in iter(range(0, len(rl.ids))):\n                id = rl.ids[i]\n\n                if not id in repeated_roots:\n                    repeated_roots[id] = 1.0\n                    rank_list.append(rl.rank[i])\n                else:\n                    repeated_roots[id] += 1.0\n                    rank_list.append(rl.rank[i] / repeated_roots[id])\n\n            phrase_rank = calc_rms(rank_list)\n            single_lex[str(rl.ids)] = rl._replace(rank = phrase_rank)\n        \n        # scale all the ranks together, so they sum to 1.0\n        sum_ranks = sum([rl.rank for rl in single_lex.values()])\n    \n        output = list()\n        for rl in sorted(single_lex.values(), key=lambda rl: rl.rank, reverse=True):\n            if sum_ranks > 0.0:\n                rl = rl._replace(rank=rl.rank / sum_ranks)\n            elif rl.rank == 0.0:\n                rl = rl._replace(rank=0.1)\n\n            rl = rl._replace(text=regex.sub(r""\\s([\\.\\,\\-\\+\\:\\@])\\s"", r""\\1"", rl.text))\n            output.append(rl._asdict())\n\n        #print(""NORMALIZE_KEYPHRASE_PROCESS: pushing normalize_key_phrases_process to output queues"")\n        output_queue.put(output)  \n        \n\ndef normalize_key_phrases (path, ranks, stopwords=None, spacy_nlp=None):\n    """"""\n    collect keyphrases, named entities, etc., while removing stop words\n    """"""\n    global STOPWORDS, SPACY_NLP\n\n    # set up the stop words\n    if (type(stopwords) is list) or (type(stopwords) is set):\n        # explicit conversion to a set, for better performance\n        stopwords = set(stopwords)\n    else:\n        if not STOPWORDS:\n            STOPWORDS = load_stopwords(stopwords)\n\n        stopwords = STOPWORDS\n\n    # set up the spaCy NLP parser\n    if not spacy_nlp:\n        if not SPACY_NLP:\n            SPACY_NLP = spacy.load(spacy_model)\n\n        spacy_nlp = SPACY_NLP\n\n    # collect keyphrases\n    single_lex = {}\n    phrase_lex = {}\n\n    #if isinstance(path, str):\n    #    path = json_iter(path)\n    if isinstance(path, str):\n        data = json_iter(path)\n    else:\n        data = json_iter_m (path)\n\n\n    #for meta in path:\n    for meta in data:\n        sent = [w for w in map(WordNode._make, meta[""graf""])]\n\n        for rl in collect_entities(sent, ranks, stopwords, spacy_nlp):\n            id = str(rl.ids)\n\n            if id not in phrase_lex:\n                phrase_lex[id] = rl\n            else:\n                prev_lex = phrase_lex[id]\n                phrase_lex[id] = rl._replace(count = prev_lex.count + 1)\n\n        for rl in collect_keyword(sent, ranks, stopwords):\n            id = str(rl.ids)\n\n            if id not in single_lex:\n                single_lex[id] = rl\n            else:\n                prev_lex = single_lex[id]\n                single_lex[id] = rl._replace(count = prev_lex.count + 1)\n\n        for rl in collect_phrases(sent, ranks, spacy_nlp):\n            id = str(rl.ids)\n\n            if id not in phrase_lex:\n                phrase_lex[id] = rl\n            else:\n                prev_lex = phrase_lex[id]\n                phrase_lex[id] = rl._replace(count = prev_lex.count + 1)\n\n    # normalize ranks across single keywords and longer phrases:\n    #    * boost the noun phrases based on their length\n    #    * penalize the noun phrases for repeated words\n    rank_list = [rl.rank for rl in single_lex.values()]\n\n    if len(rank_list) < 1:\n        max_single_rank = 0\n    else:\n        max_single_rank = max(rank_list)\n\n    repeated_roots = {}\n\n    for rl in sorted(phrase_lex.values(), key=lambda rl: len(rl), reverse=True):\n        rank_list = []\n\n        for i in iter(range(0, len(rl.ids))):\n            id = rl.ids[i]\n\n            if not id in repeated_roots:\n                repeated_roots[id] = 1.0\n                rank_list.append(rl.rank[i])\n            else:\n                repeated_roots[id] += 1.0\n                rank_list.append(rl.rank[i] / repeated_roots[id])\n\n        phrase_rank = calc_rms(rank_list)\n        single_lex[str(rl.ids)] = rl._replace(rank = phrase_rank)\n\n    # scale all the ranks together, so they sum to 1.0\n    sum_ranks = sum([rl.rank for rl in single_lex.values()])\n\n    for rl in sorted(single_lex.values(), key=lambda rl: rl.rank, reverse=True):\n        if sum_ranks > 0.0:\n            rl = rl._replace(rank=rl.rank / sum_ranks)\n        elif rl.rank == 0.0:\n            rl = rl._replace(rank=0.1)\n\n        rl = rl._replace(text=regex.sub(r""\\s([\\.\\,\\-\\+\\:\\@])\\s"", r""\\1"", rl.text))\n        yield rl\n\n\n\n######################################################################\n## sentence significance\n\ndef mh_digest (data):\n    """"""\n    create a MinHash digest\n    """"""\n    num_perm = 512\n    m = MinHash(num_perm)\n\n    for d in data:\n        m.update(d.encode(\'utf8\'))\n\n    return m\n\n\ndef rank_kernel (path):\n    """"""\n    return a list (matrix-ish) of the key phrases and their ranks\n    """"""\n    kernel = []\n\n    if isinstance(path, str):\n        path = json_iter(path)\n\n    for meta in path:\n        if not isinstance(meta, RankedLexeme):\n            rl = RankedLexeme(**meta)\n        else:\n            rl = meta\n\n        m = mh_digest(map(lambda x: str(x), rl.ids))\n        kernel.append((rl, m,))\n\n    return kernel\n\n\ndef top_sentences (kernel, path):\n    """"""\n    determine distance for each sentence\n    """"""\n    key_sent = {}\n    i = 0\n\n    if isinstance(path, str):\n        path = json_iter(path)\n\n    for meta in path:\n        graf = meta[""graf""]\n        tagged_sent = [WordNode._make(x) for x in graf]\n        text = "" "".join([w.raw for w in tagged_sent])\n\n        m_sent = mh_digest([str(w.word_id) for w in tagged_sent])\n        dist = sum([m_sent.jaccard(m) * rl.rank for rl, m in kernel])\n        key_sent[text] = (dist, i)\n        i += 1\n\n    for text, (dist, i) in sorted(key_sent.items(), key=lambda x: x[1][0], reverse=True):\n        yield SummarySent(dist=dist, idx=i, text=text)\n\n\n######################################################################\n## document summarization\n\ndef limit_keyphrases (path, phrase_limit=20):\n    """"""\n    iterator for the most significant key phrases\n    """"""\n    rank_thresh = None\n\n    if isinstance(path, str):\n        lex = []\n\n        for meta in json_iter(path):\n            rl = RankedLexeme(**meta)\n            lex.append(rl)\n    else:\n        lex = path\n\n    if len(lex) > 0:\n        rank_thresh = statistics.mean([rl.rank for rl in lex])\n    else:\n            rank_thresh = 0\n\n    used = 0\n\n    for rl in lex:\n        if rl.pos[0] != ""v"":\n            if (used > phrase_limit) or (rl.rank < rank_thresh):\n                return\n\n            used += 1\n            yield rl.text.replace("" - "", ""-"")\n\n\ndef limit_sentences (path, word_limit=100):\n    """"""\n    iterator for the most significant sentences, up to a specified limit\n    """"""\n    word_count = 0\n\n    if isinstance(path, str):\n        path = json_iter(path)\n\n    for meta in path:\n        if not isinstance(meta, SummarySent):\n            p = SummarySent(**meta)\n        else:\n            p = meta\n\n        sent_text = p.text.strip().split("" "")\n        sent_len = len(sent_text)\n\n        if (word_count + sent_len) > word_limit:\n            break\n        else:\n            word_count += sent_len\n            yield sent_text, p.idx\n\n\ndef make_sentence (sent_text):\n    """"""\n    construct a sentence text, with proper spacing\n    """"""\n    lex = []\n    idx = 0\n\n    for word in sent_text:\n        if len(word) > 0:\n            if (idx > 0) and not (word[0] in "",.:;!?-\\""\'""):\n                lex.append("" "")\n\n            lex.append(word)\n\n        idx += 1\n\n    return """".join(lex)\n\n\n######################################################################\n## common utilities\n\ndef json_iter (path):\n    """"""\n    iterator for JSON-per-line in a file pattern\n    """"""\n    with open(path, \'r\') as f:\n        for line in f.readlines():\n            yield json.loads(line)\n\ndef json_iter_m (text):\n    """"""\n    convert the text to json\n    """"""\n    for line in text:\n        yield json.loads(json.dumps(line,sort_keys=True))\n\ndef json_iterm (var2):\n    for line in var2:\n        yield line\n\ndef json_iters (var):\n    yield json.loads(json.dumps(ast.literal_eval(var)))\n\ndef json_iter_t (text):\n    yield json.loads(var)\n\ndef pretty_print (obj, indent=False):\n    """"""\n    pretty print a JSON object\n    """"""\n\n    if indent:\n        return json.dumps(obj, sort_keys=True, indent=2, separators=(\',\', \': \'))\n    else:\n        return json.dumps(obj, sort_keys=True)\n\n'"
projects/classify/ml_python_xgbclassifier_income_category_1.py,0,"b'import tkinter\r\nimport numpy as np\r\nimport pandas as pd\r\nimport category_encoders as ce\r\n\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import accuracy_score\r\nfrom xgboost import XGBClassifier\r\n\r\n\r\ncol_names = [\'age\', \'work-class\', \'fnlwgt\', \'education\', \'education-num\', \'marital-status\', \r\n            \'occupation\', \'relationship\', \'race\', \'sex\', \'capital-gain\', \'capital-loss\', \r\n            \'hrs-per-week\', \'native-country\', \'income\']\r\n\r\ndata = pd.read_csv(\'income.csv\', header = None, names = col_names, na_values = "" ?"")\r\n\r\nprint(data.shape)\r\ndata.head(n = 10)\r\n\r\n## Search for NAN values:\r\nprint(data[data.isnull().any(axis=1)].count())\r\nprint(data.isnull().values.sum())\r\ndata = data.dropna(axis = 0)\r\nprint(data.shape)\r\n\r\ndata.dtypes\r\n\r\n\r\n## first the simplest ones\r\n# Income: map >50K: 1, <=50K: 0\r\ndata[\'income\'] = data[\'income\'].map({\' >50K\': 1, \' <=50K\': 0})\r\n# data[\'income\'].astype(\'int32\')\r\n# Sex: map Male: 1, Female: 0\r\ndata[\'sex\'] = data[\'sex\'].map({\' Male\': 1, \' Female\': 0})\r\n# data[\'sex\'].astype(\'int32\')\r\ndata.head(n = 10)\r\n\r\n##  Merge Never-worked & Without pay\r\ndata[\'work-class\'] = data[\'work-class\'].replace([\' Without-pay\', \' Never-worked\'], \'Unpayed\')\r\nprint(data[\'work-class\'].value_counts().count())\r\ndata[\'work-class\'].unique()\r\n\r\n## Not many different categories so will use Label Encoding\r\nlabels = data[\'work-class\'].astype(\'category\').cat.categories.tolist()\r\nmapping = {\'work-class\': {k: v for k, v in zip(labels, list(range(1, len(labels)+1)))}}\r\n\r\ndata.replace(mapping, inplace = True)\r\n\r\ndata.head(n = 10)\r\n\r\ndata = data.drop(columns = [\'education-num\'], axis = 1)\r\nprint(data[\'education\'].value_counts())\r\n\r\ndata.head(n = 2)\r\n\r\ndata[\'education\'] = data[\'education\'].replace([\' 10th\', \' 11th\', \' 12th\'], \'HS-Student\')\r\ndata[\'education\'] = data[\'education\'].replace([\' 7th-8th\', \' 9th\'], \'Mid-Student\')\r\ndata[\'education\'] = data[\'education\'].replace([\' 5th-6th\', \' 1st-4th\'], \'Elem-Student\')\r\n\r\nprint(data[\'education\'].value_counts())\r\n\r\nlabels = data[\'education\'].astype(\'category\').cat.categories.tolist()\r\nmapping = {\'education\': {k: v for k, v in zip(labels, list(range(1, len(labels)+1)))}}\r\n\r\ndata.replace(mapping, inplace = True)\r\n\r\ndata.head(n = 10)\r\n\r\nlabels = data[\'marital-status\'].astype(\'category\').cat.categories.tolist()\r\nmapping = {\'marital-status\': {k: v for k, v in zip(labels, list(range(1, len(labels)+1)))}}\r\ndata.replace(mapping, inplace = True)\r\n\r\nlabels = data[\'relationship\'].astype(\'category\').cat.categories.tolist()\r\nmapping = {\'relationship\': {k: v for k, v in zip(labels, list(range(1, len(labels)+1)))}}\r\ndata.replace(mapping, inplace = True)\r\n\r\nlabels = data[\'race\'].astype(\'category\').cat.categories.tolist()\r\nmapping = {\'race\': {k: v for k, v in zip(labels, list(range(1, len(labels)+1)))}}\r\ndata.replace(mapping, inplace = True)\r\n\r\ndata.head(n = 10)\r\n\r\n## Occupation & Nativity have many categories > Binary Encode\r\nencoder = ce.BinaryEncoder(cols = [\'occupation\', \'native-country\'])\r\ndata = encoder.fit_transform(data)\r\n\r\ndata.head(n = 10)\r\n\r\nlabels = data[\'income\']\r\nfeatures = data.drop(columns = [\'income\'], axis = 1)\r\n\r\nlabels.head(n = 5)\r\nfeatures.head(n = 5)\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.25, random_state = 2)\r\nprint(X_train.shape)\r\nprint(X_test.shape)\r\n\r\nmodel = XGBClassifier()\r\nmodel.fit(X_train, y_train)\r\nprint(model)\r\n\r\ny_hat_train = model.predict(X_train)\r\ntrain_pred = [round(value) for value in y_hat_train]\r\n\r\ntrain_accuracy = accuracy_score(y_train, train_pred)\r\nprint(\'Train Accuracy: \', train_accuracy)\r\n\r\ny_hat_test = model.predict(X_test)\r\n\r\ntest_accuracy = accuracy_score(y_test, y_hat_test)\r\nprint(\'Test Accuracy: \', test_accuracy)\r\n'"
projects/clusters/ml_python_k_means_clustering_1.py,1,"b'import numpy as np\r\nfrom sklearn.cluster import KMeans\r\nfrom sklearn.datasets import make_blobs\r\n\r\nimport tkinter\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\n\r\n\'\'\'\r\n# define number of samples\r\nn_samples = 100\r\n\r\n# define random state value to initialize the center\r\nrandom_state = 20\r\n\r\n# define number of feature as 5\r\nX,y = make_blobs(n_samples=n_samples, n_features=5, random_state=None)\r\n# print(X,y)\r\n\r\n# The goal of K-means algorithm is to find groups in the data, with the number of groups represented by the variable K. \r\n# mean distance to the centroid as a function of K is plotted and the ""elbow point,"" where the rate of decrease sharply shifts, can be used to roughly determine K.\r\n\r\n# define number of cluster to be formed as 3 and \r\n# in random state and fit features into the model\r\npredict_y = KMeans(n_clusters=3, random_state=random_state)\r\n\r\nprint(predict_y)\r\n\r\npredict_y_fit = predict_y.fit_predict(X)\r\n# estimator function\r\n# print(predict_y_fit)\r\nprint(type(predict_y_fit))\r\n# print(predict_y_fit.labels_)\r\nprint(predict_y_fit.shape)\r\n\r\ndf = pd.DataFrame(predict_y_fit)\r\n# print(df)\r\nprint(df.describe())\r\n\r\nprint(df[0].value_counts())\r\n\'\'\'\r\n# rule\r\n\'\'\'\r\ndf[0].value_counts().plot.bar()\r\nplt.show()\r\n\r\ndf[0].value_counts().plot.pie()\r\nplt.gca().set_aspect(""equal"")\r\nplt.show()\r\n\r\n\r\nlabels = []\r\nfor i, dfi in enumerate(df.groupby(df[0])):\r\n    labels.append(dfi[0])\r\n    plt.bar(i, dfi[1].count(), label=dfi[0])\r\nplt.xticks(range(len(labels)), labels)\r\nplt.legend()\r\nplt.show()\r\n\'\'\'\r\n\r\ndata = pd.read_csv(\'data_1024.csv\', sep=\'\\t\')\r\nprint(data.describe())\r\nprint(data.head())\r\n\r\n\r\n# plt.scatter(data[\'Distance_Feature\'], data[\'Speeding_Feature\'])\r\n# plt.show()\r\n\r\nX = np.matrix(list(zip(data[\'Distance_Feature\'].values,data[\'Speeding_Feature\'].values)))\r\nkmeans = KMeans(n_clusters=2)\r\nprint(kmeans)\r\n\r\nkmeans_fit = kmeans.fit(X)\r\nprint(kmeans_fit)\r\n\r\nkmeans_predict = kmeans_fit.predict(X)\r\nprint(kmeans_predict)\r\nprint(kmeans_predict.shape)\r\n\r\ndfx = pd.DataFrame(X)\r\n\r\nplt.scatter(dfx[0], dfx[1], c=kmeans_predict, s=50, cmap=\'viridis\')\r\n\r\ncenters = kmeans.cluster_centers_\r\nprint(centers)\r\nplt.scatter(centers[:, 0], centers[:, 1], c=\'black\', s=200, alpha=0.5)\r\n\r\nplt.show()\r\n\r\n\r\nkmeans = KMeans(n_clusters=4)\r\nkmeans_fit = kmeans.fit(X)\r\nkmeans_predict = kmeans_fit.predict(X)\r\nplt.scatter(dfx[0], dfx[1], c=kmeans_predict, s=50, cmap=\'viridis\')\r\ncenters = kmeans.cluster_centers_\r\nplt.scatter(centers[:, 0], centers[:, 1], c=\'black\', s=200, alpha=0.5)\r\nplt.show()\r\n\r\n\r\n\r\n\r\n'"
projects/finance/datareader_1.py,0,"b'import pandas_datareader.data as pdr\nimport datetime\nimport tkinter \n\nstart = datetime.datetime(2018, 1, 1)\nend = datetime.datetime(2018, 7, 7)\n\n# f = web.DataReader(\'F\', \'google\', start, end)\n\nf = pdr.get_data_yahoo(\'BABA\',start,end)\n\nprint(f.ix[\'2018-01-04\'])\n\n\n\n# from pandas_datareader import data as pdr\nimport plotly.offline as py_offline\nimport plotly.graph_objs as go\nimport fix_yahoo_finance as yf\n\npy_offline.init_notebook_mode(connected=True)\n\nyf.pdr_override()\nmcd = pdr.get_data_yahoo(""MCD"", start=""2005-07-01"", end=""2005-07-31"")\nmcd_candle = go.Candlestick(x=mcd.index,\n                            open=mcd.Open,\n                            high=mcd.High,\n                            low=mcd.Low,\n                            close=mcd.Close,\n                            increasing=dict(line=dict(color= \'#00FF00\')),\n                            decreasing=dict(line=dict(color= \'#FF0000\'))\n                           )\ndata = [mcd_candle]\n\nlayout = go.Layout(\n    plot_bgcolor=\'rgb(59,68,75)\'\n)\n\nfig = go.Figure(data=data, layout=layout)\n'"
projects/midcurvenn/__init__.py,0,b''
projects/nltk/classicGen.py,0,"b""import nltk\nimport sklearn\nfrom nltk import word_tokenize as wt\n\n# --------- optional start ------------ #\nf = open('NEWS.txt', encoding='utf-8')\nraw = f.read()\n\n# WORD TOKENIZER\ntokens = wt(raw)\n\n# CREATE NLTK TEXT \ntxt = nltk.Text(tokens)\n# ----------optional end -------------- #\n\n# CLASSIFIER\nfrom nltk.corpus import names\n\n# SINGLE FEATURE GENERATOR & CLASSIFICATION #\n# ------------------- start --------------- #\ndef feature_generator(word):\n    return {'last_letter': word[-1]}\n\nprint(feature_generator('praveen'))\n\nimport random\n\nnames = ([(name, 'male') for name in names.words('male.txt')] + \\\n\t\t\t[(name, 'female') for name in names.words('female.txt')])\n\nrandom.shuffle(names)\n\nfeaturesets = [(feature_generator(n), g) for (n,g) in names]\nprint(featuresets[:10])\nprint(len(featuresets))\n\nfrom sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(featuresets, test_size=0.25, random_state=42)\n\nprint(len(train_set), type(train_set))\nprint(len(test_set), type(test_set))\n\n# CLASSIFIER\nclassifier = nltk.NaiveBayesClassifier.train(train_set)\nprint(classifier)\n\ndef classify_gender(s):\n    return classifier.classify(feature_generator(s))\n\nprint(classify_gender('Praveen'))\nprint(classify_gender('Ra'))\n\n# ACCURACY SCORE\nprint(nltk.classify.accuracy(classifier, test_set))\n\n# INFORMATIVE FEATURES\nprint(classifier.show_most_informative_features(5))\n\n# ----------------- end ------------------ #\n\n\n# MULTIPLE FEATURES + CLASSIFIER #\n# ------------------- start -------------- #\n\ndef feature_generator2(word):\n    return {'name_length': len(word), 'last_letter': word[-1]}\n\nfeaturesets2 = [(feature_generator2(n), g) for (n,g) in names]\n\ntrain_set2, test_set2 = train_test_split(featuresets2, test_size=0.25, random_state=42)\n\nprint(len(train_set2), type(train_set2))\nprint(len(test_set2), type(test_set2))\n\nclassifier2 = nltk.NaiveBayesClassifier.train(train_set2)\nprint(classifier2)\n\ndef classify_gender2(s):\n    return classifier2.classify(feature_generator2(s))\n\nprint(classify_gender2('Praveen'))\nprint(classify_gender2('Fa'))\n\n\n# ACCURACY SCORE\nprint(nltk.classify.accuracy(classifier2, test_set2))\n\n# INFORMATIVE FEATURES\nprint(classifier2.show_most_informative_features(5))\n# ------------------ end ---------------- #\n"""
projects/nltk/nltk_classification.py,0,"b'import tkinter\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nimport nltk\r\nfrom nltk.corpus import stopwords\r\nfrom nltk.classify import SklearnClassifier\r\n\r\nfrom wordcloud import WordCloud, STOPWORDS\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\ndata = pd.read_csv(\'sentiment.csv\')\r\n# Keeping only the neccessary columns\r\ndata = data[[\'text\',\'sentiment\']]\r\n\r\ndata.head()\r\n\r\ndata.describe()\r\n\r\ndata.sentiment.unique()\r\n\r\ndata[\'sentiment\'].value_counts()\r\n\r\n# Splitting the dataset into train and test set\r\ntrain, test = train_test_split(data,test_size = 0.2)\r\n# Removing neutral sentiments\r\ntrain = train[train.sentiment != ""Neutral""]\r\n\r\ntrain.describe()\r\n\r\ntrain.sentiment.value_counts()\r\n\r\ntest.describe()\r\n\r\ntest.sentiment.value_counts()\r\n\r\ntrain.sentiment.unique()\r\n\r\ntest.sentiment.unique()\r\n\r\ntrain_pos = train[ train[\'sentiment\'] == \'Positive\']\r\ntrain_pos = train_pos[\'text\']\r\ntrain_neg = train[ train[\'sentiment\'] == \'Negative\']\r\ntrain_neg = train_neg[\'text\']\r\n\r\n\r\ndef wordcloud_draw(data, color = \'black\'):\r\n    words = \' \'.join(data)\r\n    cleaned_word = "" "".join([word for word in words.split()\r\n                            if \'http\' not in word\r\n                                and not word.startswith(\'@\')\r\n                                and not word.startswith(\'#\')\r\n                                and word != \'RT\'\r\n                            ])\r\n    wordcloud = WordCloud(stopwords=STOPWORDS,\r\n                      background_color=color,\r\n                      width=2500,\r\n                      height=2000\r\n                     ).generate(cleaned_word)\r\n    plt.figure(1,figsize=(13, 13))\r\n    plt.imshow(wordcloud)\r\n    plt.axis(\'off\')\r\n    plt.show()\r\n    \r\nprint(""Positive words"")\r\nwordcloud_draw(train_pos,\'white\')\r\nprint(""Negative words"")\r\nwordcloud_draw(train_neg)\r\n\r\n\r\ntweets = []\r\nstopwords_set = set(stopwords.words(""english""))\r\n\r\nfor index, row in train.iterrows():\r\n    words_filtered = [e.lower() for e in row.text.split() if len(e) >= 3]\r\n    words_cleaned = [word for word in words_filtered\r\n        if \'http\' not in word\r\n        and not word.startswith(\'@\')\r\n        and not word.startswith(\'#\')\r\n        and word != \'RT\']\r\n    words_without_stopwords = [word for word in words_cleaned if not word in stopwords_set]\r\n    tweets.append((words_without_stopwords, row.sentiment))\r\n\r\ntest_pos = test[ test[\'sentiment\'] == \'Positive\']\r\ntest_pos = test_pos[\'text\']\r\ntest_neg = test[ test[\'sentiment\'] == \'Negative\']\r\ntest_neg = test_neg[\'text\']\r\n\r\n\r\n# Extracting word features\r\ndef get_words_in_tweets(tweets):\r\n    all = []\r\n    for (words, sentiment) in tweets:\r\n        all.extend(words)\r\n    return all\r\n\r\ndef get_word_features(wordlist):\r\n    wordlist = nltk.FreqDist(wordlist)\r\n    features = wordlist.keys()\r\n    return features\r\nw_features = get_word_features(get_words_in_tweets(tweets))\r\n\r\ndef extract_features(document):\r\n    document_words = set(document)\r\n    features = {}\r\n    for word in w_features:\r\n        features[\'contains(%s)\' % word] = (word in document_words)\r\n    return features\r\n\r\nwordcloud_draw(w_features)\r\n\r\n# Training the Naive Bayes classifier\r\ntraining_set = nltk.classify.apply_features(extract_features,tweets)\r\nclassifier = nltk.NaiveBayesClassifier.train(training_set)\r\n\r\n\r\n\r\nneg_cnt = 0\r\npos_cnt = 0\r\nfor obj in test_neg: \r\n    res =  classifier.classify(extract_features(obj.split()))\r\n    if(res == \'Negative\'): \r\n        neg_cnt = neg_cnt + 1\r\nfor obj in test_pos: \r\n    res =  classifier.classify(extract_features(obj.split()))\r\n    if(res == \'Positive\'): \r\n        pos_cnt = pos_cnt + 1\r\n        \r\nprint(\'[Negative]: %s/%s \'  % (len(test_neg),neg_cnt))        \r\nprint(\'[Positive]: %s/%s \'  % (len(test_pos),pos_cnt)) \r\n\r\n# [Negative]: 1714/1617\r\n# [Positive]: 431/155\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n'"
projects/nltk/nltk_senti_analysis.py,0,"b'# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here\'s several helpful packages to load in \nimport nltk\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom nltk.classify import SklearnClassifier\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.tokenize import word_tokenize\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer \nimport matplotlib.pyplot as plt\n\n# Input data files are available in the ""../input/"" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(""../input""))\n\n# Any results you write to the current directory are saved as output.\n\n%matplotlib inline\n\n\ndir = os.getcwd()\ndir\n\ntrain_data = pd.read_csv(""../input/train.csv"")\n\ntrain_data.head()\n\nlen(train_data)\n\ntrain_data = train_data[train_data.Tweet != \'Not Available\']\n\n# stopwords, URL\'s, RT cleanup coming soon!\n\nlen(train_data)\n\ntrain_data.head()\n\nfeedback = train_data[\'Tweet\']\ntrain = []\nsia = SentimentIntensityAnalyzer()\nfor sentence in feedback:\n     # print(sentence)\n     s = sia.polarity_scores(sentence)\n     for v in s:\n         # print(\'{0}: {1}, \'.format(v, s[v]), end=\'\\n\')\n         if v == \'compound\' and s[v] > 0:\n             train.append((sentence, ""pos""))\n         elif v == \'compound\' and s[v] <= 0:\n             train.append((sentence, ""neg""))\n\ntrain\n\ndictionary = set(word.lower() for sentence in train for word in word_tokenize(sentence[0]))\ndictionary\n\nt = [({word: (word in word_tokenize(x[0])) for word in dictionary}, x[1]) for x in train]\n\nclassifier = nltk.NaiveBayesClassifier.train(t)\n\ntest_data = ""Why should I tweet against someone?""\n\ntest_data_features = {word.lower(): (word in word_tokenize(test_data.lower())) for word in dictionary}\n\ntest_data_features\n\nclassifier.classify(test_data_features)\n\n'"
projects/novelty/anomaly_1.py,22,"b'import tkinter\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n## HELPER FUNCTIONS ##\n\ndef plot_decision_function(model, ax, sv=True):\n    ## Create a grid to evaluate the model\n    xx, yy = np.meshgrid(np.linspace(-4, 4, 100),\n                         np.linspace(-4, 4, 100))\n    ## Evaluate the model\n    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    ## Plot the margin\n    ax = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors=\'darkred\')\n    ax = plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors=\'palevioletred\')\n    \n    ax = plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 6), cmap=plt.cm.PuBu)\n    \n    if sv:\n        ax = plt.scatter(model.support_vectors_[:, 0],\n                       model.support_vectors_[:, 1],\n                       s=300, linewidth=1, facecolors=\'none\', edgecolors=\'black\')\n\n\ndef plot_new_observations(X_train, X_new, anomaly, ax):\n    _ = plt.scatter(X_train[:,0], X_train[:,1],\n                    axes=ax, color=\'w\', s=40, edgecolors=\'k\',\n                    label=\'Training data\')\n\n    _ = plt.scatter(X_new[:,0], X_new[:,1], axes=ax,\n                    color=\'violet\', s=40, edgecolors=\'k\',\n                    label=\'New regular observations\')\n\n    _ = plt.scatter(anomaly[:,0], anomaly[:,1], axes=ax,\n                    color=\'gold\', s=40, edgecolors=\'k\',\n                    label=\'New abnormal observations\')\n\n    _ = ax.legend()\n    _ = ax.set_xlim([-4, 4])\n    _ = ax.set_ylim([-4, 4])\n\n\nn_points = 200\n\n## generate a cluster of points for the training \nnp.random.seed(42)\nX_train = 0.5 * np.random.randn(n_points, 2)\n\n## Plot\nfig, ax = plt.subplots(figsize=(12,8))\n_ = plt.scatter(X_train[:,0], X_train[:,1], axes=ax, color=\'w\', s=40, edgecolors=\'k\')\n_ = ax.set_xlim([-4, 4])\n_ = ax.set_ylim([-4, 4])\n\n\nmodel = svm.OneClassSVM(nu=0.05, kernel=""rbf"", gamma=0.7)\nmodel.fit(X_train)\n\nfig, ax = plt.subplots(figsize=(12,8))\n\n## Plot the decision function\nplot_decision_function(model, ax, sv=True)\n\n## Add the training points\n_ = plt.scatter(X_train[:,0], X_train[:,1], axes=ax,\n                color=\'w\', s=40, edgecolors=\'k\',label=\'Training data\')\n_ = ax.legend()\n\n\n## Compute the empirical error \ny_train = model.predict(X_train)\nerr_emp_1 = y_train[y_train == -1].size\nprint(""Training error = {}/{}"".format(err_emp_1, n_points))\n\n\n## Reduce nu, i.e. weight more the slack variables\nmodel = svm.OneClassSVM(nu=0.005, kernel=""rbf"", gamma=0.7)\nmodel.fit(X_train)\n\nfig, ax = plt.subplots(figsize=(12,8))\n\n## Plot the decision function\nplot_decision_function(model, ax, sv=True)\n\n## Add the training points\n_ = plt.scatter(X_train[:,0], X_train[:,1], axes=ax,\n                color=\'w\', s=40, edgecolors=\'k\',label=\'Training data\')\n_ = ax.legend()\n\n## Compute the empirical error \ny_train = model.predict(X_train)\nerr_emp_2 = y_train[y_train == -1].size\nprint(""Training error = {}/{}"".format(err_emp_2, n_points))\n# Training error = 5/200\n\n""""""\nAs we can see from the picture above, by reducing $\\nu$ we are increasing the weight of the slack variables (because $C \\sim 1/\\nu$). This leads to a reduction of the error but increase the risk of overfitting.\n""""""\n\nnew_observation = 25\nnew_anomaly = 10\n\n## Generate new observation from the same distribution\nnp.random.seed(42)\nX_new = 0.5 * np.random.randn(new_observation, 2)\n\n## Generate outliers\nanomaly = np.random.uniform(low=-3, high=3, size=(new_anomaly, 2))\n\n## Plot\nfig, ax = plt.subplots(figsize=(12,8))\nplot_new_observations(X_train, X_new, anomaly, ax)\n\ny_new = model.predict(X_new)\ny_anomaly = model.predict(anomaly)\n\nerr_new = y_new[y_new == -1].size\nerr_anomaly = y_anomaly[y_anomaly == -1].size\n\nprint(""Fraction of new regular observations misclassified = {}/{}"".format(err_new, new_observation))\nprint(""Fraction of new abnormal observations correctly classified = {}/{}"".format(err_anomaly, new_anomaly))\n\nfig, ax = plt.subplots(figsize=(12,8))\n\n## Plot the decision function\nplot_decision_function(model, ax, sv=False)\nplot_new_observations(X_train, X_new, anomaly, ax)\n# Fraction of new regular observations misclassified = 0/25\n# Fraction of new abnormal observations correctly classified = 10/10\n\n# Let\'s try to make the problem a little bit more complex, i.e. use two clusters as positive examples\n\n## generate two cluster for the training \nnp.random.seed(42)\nX_train1 = 0.5 * np.random.randn(n_points//2, 2)+1.5\nX_train2 = 0.5 * np.random.randn(n_points//2, 2)-1.5\n\nX_train = np.r_[X_train1, X_train2]\n\n## Plot\nfig, ax = plt.subplots(figsize=(12,8))\n_ = plt.scatter(X_train[:,0], X_train[:,1], axes=ax, color=\'w\', s=40, edgecolors=\'k\')\n_ = ax.set_xlim([-4, 4])\n_ = ax.set_ylim([-4, 4])\n\nmodel = svm.OneClassSVM(nu=0.06, kernel=""rbf"", gamma=0.5)\nmodel.fit(X_train)\n\nfig, ax = plt.subplots(figsize=(12,8))\n\n## Plot the decision function\nplot_decision_function(model, ax, sv=True)\n\n## Add the training points\n_ = plt.scatter(X_train[:,0], X_train[:,1], axes=ax,\n                color=\'w\', s=40, edgecolors=\'k\',label=\'Training data\')\n_ = ax.legend()\n\n## Compute the empirical error \ny_train = model.predict(X_train)\nerr_emp_1 = y_train[y_train == -1].size\nprint(""Training error = {}/{}"".format(err_emp_1, n_points))\n\nnew_observation = 50\nnew_anomaly = 20\n\n## Generate new observation from the same distribution\nnp.random.seed(42)\nX_new_1 = 0.5 * np.random.randn(new_observation//2, 2)+1.5\nX_new_2 = 0.5 * np.random.randn(new_observation//2, 2)-1.5\nX_new = np.r_[X_new_1, X_new_2]\n\n## Generate outliers\nanomaly = np.random.uniform(low=-4, high=4, size=(new_anomaly, 2))\n\n## Plot\nfig, ax = plt.subplots(figsize=(12,8))\nplot_new_observations(X_train, X_new, anomaly, ax)\n\n\ny_new = model.predict(X_new)\ny_anomaly = model.predict(anomaly)\n\nerr_new = y_new[y_new == -1].size\nerr_anomaly = y_anomaly[y_anomaly == -1].size\n\nprint(""Fraction of new regular observations misclassified = {}/{}"".format(err_new, new_observation))\nprint(""Fraction of new abnormal observations correctly classified = {}/{}"".format(err_anomaly, new_anomaly))\n\nfig, ax = plt.subplots(figsize=(12,8))\n\n## Plot the decision function\nplot_decision_function(model, ax, sv=False)\nplot_new_observations(X_train, X_new, anomaly, ax)\n# Fraction of new regular observations misclassified = 2/50\n# Fraction of new abnormal observations correctly classified = 18/20\n\n# We can try to move the cluster closer\n\nnp.random.seed(42)\nX_train1 = 0.5 * np.random.randn(n_points//2, 2)+0.9\nX_train2 = 0.5 * np.random.randn(n_points//2, 2)-0.9\n\nX_train = np.r_[X_train1, X_train2]\n\n## Plot\nfig, ax = plt.subplots(figsize=(12,8))\n_ = plt.scatter(X_train[:,0], X_train[:,1], axes=ax, color=\'w\', s=40, edgecolors=\'k\')\n_ = ax.set_xlim([-4, 4])\n_ = ax.set_ylim([-4, 4])\n\n\nmodel = svm.OneClassSVM(nu=0.05, kernel=""rbf"", gamma=0.1)\nmodel.fit(X_train)\n\nfig, ax = plt.subplots(figsize=(12,8))\n\n## Plot the decision function\nplot_decision_function(model, ax, sv=True)\n\n## Add the training points\n_ = plt.scatter(X_train[:,0], X_train[:,1], axes=ax,\n                color=\'w\', s=40, edgecolors=\'k\',label=\'Training data\')\n_ = ax.legend()\n\n## Compute the empirical error \ny_train = model.predict(X_train)\nerr_emp_1 = y_train[y_train == -1].size\nprint(""Training error = {}/{}"".format(err_emp_1, n_points))\n\n# If we want to try to separate the two cluster we need to increase the value of gamma and $\\nu$ but this increase the errors a lot.\n\nmodel = svm.OneClassSVM(nu=0.3, kernel=""rbf"", gamma=1)\nmodel.fit(X_train)\n\nfig, ax = plt.subplots(figsize=(12,8))\n\n## Plot the decision function\nplot_decision_function(model, ax, sv=True)\n\n## Add the training points\n_ = plt.scatter(X_train[:,0], X_train[:,1], axes=ax,\n                color=\'w\', s=40, edgecolors=\'k\',label=\'Training data\')\n_ = ax.legend()\n\n## Compute the empirical error \ny_train = model.predict(X_train)\nerr_emp_1 = y_train[y_train == -1].size\nprint(""Training error = {}/{}"".format(err_emp_1, n_points))\n\n'"
projects/novelty/anomaly_2.py,1,"b'# load the libraries\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, recall_score, precision_score\n\nimport seaborn as sns\nsns.set(style=""darkgrid"")\n\n# load the data \ndf = pd.read_csv(""creditCard.csv"")\ndf.head()\n\n# Classify as 1 or -1 \ndf[\'Class\'] = df[\'Class\'].map(lambda x: -1 if x==""\'1\'"" else 1)\n\nfig, ax = plt.subplots(figsize=(8,6))\n_ = sns.countplot(x=\'Class\', data=df, ax=ax)\n_ = plt.yscale(\'log\')\n\ndf = df.drop([\'Time\'], axis=1)\n\n# split into training an test\nX_train, X_test = train_test_split(df, test_size=0.25, random_state=23)\n\n# X,y train\ny_train = X_train[\'Class\']\n\n# train using the normal transactions\nX_train = X_train[X_train.Class == 1].drop([\'Class\'], axis=1)\n\n# X,y test\ny_test = X_test[\'Class\']\nX_test = X_test.drop([\'Class\'], axis=1).values\n\nprint(""Train dataset: \\n{}"".format(pd.value_counts(y_train)))\nprint(""Test dataset: \\n{}"".format(pd.value_counts(y_test)))\n\noc_svm = svm.OneClassSVM(nu=0.0007, kernel=""rbf"", gamma=0.04)\n\n%%time\noc_svm.fit(X_train)\n\n%%time\ny_pred = oc_svm.predict(X_test)\n\n\nLABELS = [1, -1]\nconf_matrix = confusion_matrix(y_test, y_pred, labels=LABELS)\n\nfig, ax = plt.subplots(figsize=(10,8))\n_ = sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, \n               annot=True, ax=ax)\n_ = ax.set_xlabel(\'Predicted Class\')\n_ = ax.set_ylabel(\'True Class\')\n\n# Normalizer confusion matrix\ncm = conf_matrix.astype(\'float\') / conf_matrix.sum(axis=1)[:, np.newaxis]\nfig, ax = plt.subplots(figsize=(10,8))\n_ = sns.heatmap(cm, xticklabels=LABELS, yticklabels=LABELS, \n               annot=True, ax=ax)\n_ = ax.set_xlabel(\'Predicted Class\')\n_ = ax.set_ylabel(\'True Class\')\n\nprint(""Precision = {}"".format(precision_score(y_test, y_pred, pos_label=-1)))\nprint(""Recall = {}"".format(recall_score(y_test, y_pred, pos_label=-1)))\n'"
projects/novelty/novelty_0.py,11,"b'import tkinter as tk \r\nimport numpy as np  \r\nimport pandas as pd  \r\nfrom sklearn import utils\r\nfrom sklearn import svm\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib.font_manager\r\n\r\n# random data\r\nc = np.random.randint(low=-2.5, high=3, size=(100))\r\nd = np.random.randint(low=-2.5, high=3, size=(77))\r\ne = np.random.randint(low=-5, high=1, size=(22))\r\n\r\n# train\r\nX_train = np.array([np.arange(0,100), c]).T\r\nprint(X_train)\r\nprint(X_train.shape)\r\n\r\n# normal\r\nX_test = np.array([np.arange(0,77), d]).T\r\nprint(X_test)\r\nprint(X_test.shape)\r\n\r\n# outlier\r\nX_outliers = np.array([np.arange(78,100), e]).T\r\nprint(X_outliers)\r\nprint(X_outliers.shape)\r\n\r\n# fit the model\r\nclf = svm.OneClassSVM(nu=0.1, kernel=""rbf"", gamma=0.1)\r\nclf.fit(X_train)\r\nprint(clf)\r\n\r\ny_pred_train = clf.predict(X_train)\r\ny_pred_test = clf.predict(X_test)\r\ny_pred_outliers = clf.predict(X_outliers)\r\n\r\nprint(y_pred_train, y_pred_test, y_pred_outliers)\r\n\r\nn_error_train = y_pred_train[y_pred_train == 1].size\r\nn_error_test = y_pred_test[y_pred_test == -1].size\r\nn_error_outliers = y_pred_outliers[y_pred_outliers == -1].size\r\n\r\nxx, yy = np.meshgrid(np.linspace(0, 100, 100), np.linspace(-5, 5, 100))\r\nprint(xx.shape)\r\n\r\n# plot the line, the points, and the nearest vectors to the plane\r\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\r\nZ = Z.reshape(xx.shape)\r\n\r\nprint(Z.shape)\r\nprint(Z.min())\r\n\r\n# print(np.c_[xx.ravel()])\r\n# print(np.c_[yy.ravel()])\r\n\r\nplt.figure(figsize=(11,7))\r\nplt.title(""Novelty Detection"")\r\nplt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\r\na = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors=\'darkred\')\r\nplt.contourf(xx, yy, Z, levels=[0, Z.max()], colors=\'palevioletred\')\r\n\r\ns = 15\r\nb1 = plt.scatter(X_train[:, 0], X_train[:, 1], c=\'white\', s=s, edgecolors=\'k\')\r\nb2 = plt.scatter(X_test[:, 0], X_test[:, 1], c=\'blueviolet\', s=s*2, edgecolors=\'k\')\r\nc = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c=\'gold\', s=s*3, edgecolors=\'k\')\r\nplt.axis(\'tight\')\r\n#plt.xlim((-5, 5))\r\n#plt.ylim((-5, 5))\r\nplt.legend([a.collections[0], b1, b2, c],\r\n           [""learned frontier"", ""training observations"",\r\n            ""new regular observations"", ""new abnormal observations""],\r\n           loc=""upper left"",\r\n           prop=matplotlib.font_manager.FontProperties(size=11))\r\nplt.xlabel(\r\n    ""error train: %d/100 ; errors novel regular: %d/77 ; ""\r\n    ""errors novel abnormal: %d/22""\r\n    % (n_error_train, n_error_test, n_error_outliers))\r\nplt.show()\r\n\r\n'"
projects/novelty/novelty_4.py,0,"b'\'\'\'\r\nOne Class SVM\r\n\'\'\'\r\nimport tkinter\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n# %matplotlib inline\r\n\r\nimport pandas as pd\r\nfrom sklearn import svm\r\n\r\ntr_data = pd.read_csv(\'novelty_4.csv\') \r\n#print(tr_data.head(1))\r\n\r\nX = tr_data.drop([\'Class\',\'R\',\'S\'], axis=1, inplace=True)\r\n\r\nnu = [.001, .01, .05, .1, .5,]\r\ngamma = [.001, .01, .05, .1, 1, 10, 100]\r\n\r\n#for n in nu:\r\n#    for g in gamma:\r\nclf = svm.OneClassSVM(nu=0.02, kernel=""rbf"", gamma=.001)\r\nclf.fit(tr_data)\r\n\r\npred = clf.predict(tr_data)\r\n#print(type(pred))\r\n\r\ntr_data[\'pred\'] = pred\r\n#print(tr_data.head(1))\r\n\r\n# inliers are labeled 1, outliers are labeled -1\r\nnormal = tr_data[[\'P\',\'Q\']][pred == 1].values\r\nabnormal = tr_data[[\'P\',\'Q\']][pred == -1].values\r\n\r\n#print(normal)\r\n#print(abnormal)\r\n\r\nplt.figure(\'One Class SVM\')\r\nplt.plot(normal[:,0],normal[:,1],\'bx\')\r\nplt.plot(abnormal[:,0],abnormal[:,1],\'ro\')\r\nplt.xlabel(\'Latency (ms)\')\r\nplt.ylabel(\'Throughput (mb/s)\')\r\nplt.show()\r\n\r\n'"
projects/pattern_recognition_forex/pr.py,1,"b'# pattern recognition\r\n\r\nimport tkinter as tk\r\nimport matplotlib\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib.ticker as mticker\r\nimport matplotlib.dates as mdates\r\nimport numpy as np\r\nfrom matplotlib import style\r\nstyle.use(""ggplot"")\r\n\r\nfn = lambda astr: mdates.strpdate2num(\'%Y%m%d%H%M%S\')(astr.decode())\r\n\r\ndef graphRawFX():\r\n    date,bid,ask = np.loadtxt(\'GBPUSD1d.txt\', unpack=True, delimiter=\',\', converters={0:fn})\r\n\r\n    fig = plt.figure(figsize=(10,7))\r\n\r\n    ax1 = plt.subplot2grid((40,40), (0,0), rowspan=40, colspan=40)\r\n    ax1.plot(date,bid)\r\n    ax1.plot(date,ask)\r\n\r\n    ax1.xaxis.set_major_formatter(mdates.DateFormatter(\'%Y-%m-%d %H:%M:%S\'))\r\n\r\n    for label in ax1.xaxis.get_ticklabels():\r\n        label.set_rotation(45)\r\n    plt.subplots_adjust(bottom=.23)\r\n    plt.gca().get_yaxis().get_major_formatter().set_useOffset(False)\r\n    plt.grid(True)\r\n    plt.show()\r\n\r\ngraphRawFX()'"
projects/predictions/price_predictions.py,6,"b'\n# coding: utf-8\n\n# In[16]:\n\n\n# import libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\n\n\n# In[2]:\n\n\ndf_train = pd.read_csv(""train.csv"", index_col=""Id"", na_values=""NA"")\ndf_train.head()\n\n\n# In[3]:\n\n\ndf_train.shape\n\n\n# In[5]:\n\n\ndf_train.describe(include=\'all\')\n\n\n# In[6]:\n\n\ndf_train.corr()\n\n\n# In[7]:\n\n\ndf_test = pd.read_csv(""test.csv"", index_col=""Id"", na_values=""NA"")\n\n\n# In[8]:\n\n\ndf_test.describe(include=\'all\')\n\n\n# In[9]:\n\n\ndf = df_train.append(df_test)\n\n\n# In[10]:\n\n\ndf.drop(""SalePrice"", axis=1, inplace=True)\n\n\n# #### Feature Engineering\n\n# In[11]:\n\n\n# helper method to obtain all numerical/categorical features of a data frame\ndef get_features(df, feature_type):\n    if feature_type == ""num"":\n        return list(df.select_dtypes(include=[""float"", ""int""]).columns)\n    elif feature_type == ""cat"":\n        return list(df.select_dtypes(include=[""object""]).columns)\n    else:\n        raise ValueError(""feature_type must be \'num\' (numerical) or \'cat\' (categorical)."")\n\n\n# In[12]:\n\n\nnum_features = get_features(df, ""num"")\nnum_features\n\n\n# In[13]:\n\n\ncolumns_with_na_values = df.columns[df.isnull().any()]\ncolumns_with_na_values\n\n\n# In[14]:\n\n\nnum_features_without_na = [x for x in num_features if x not in columns_with_na_values]\n\nprint(""Numerical features without missing values in train.csv and test.csv:"")\nprint(num_features_without_na)\n\n\n# #### Visualization\n\n# In[15]:\n\n\nsns.regplot(x=""GrLivArea"", y=""SalePrice"", data=df_train)\n\n\n# In[17]:\n\n\n# helper method to compute the root mean squared error\ndef root_mean_squared_error(model, X, y_true):\n    y_predict = model.predict(X)    \n    return np.sqrt(mean_squared_error(y_predict, y_true))\n\n\n# In[18]:\n\n\n# Helper method to check performance of a model on 4 different sized training data sets.\ndef model_eval_helper(features, model):\n    X = df_train[features].values\n    y = np.ravel(df_train[[""SalePrice""]].apply(np.log).values)\n    n = len(df_train)\n\n    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n\n    for train_size in np.linspace(0.1, 1, num=4):\n        last = int(n * train_size)\n        model.fit(X_train[:last], y_train[:last])\n        score_out = root_mean_squared_error(model, X_val, y_val)\n        score_in = root_mean_squared_error(model, X_train[:last], y_train[:last])\n        print("""")\n        print(""Using {} samples ({}%) of train.csv for training."".format(last, train_size * 100))\n        print(""Root mean squared logs error on validation data: {}"".format(score_out))\n        print(""Root mean squared logs error on training data: {}"".format(score_in))\n\n\n# In[19]:\n\n\nprint(""Training linear regression model with feature \'GrLivArea\'."")\nmodel_eval_helper([""GrLivArea""], LinearRegression())\n\n\n# In[21]:\n\n\n# helper function to display correlation matrices\ndef corr_helper(features=None, min_corr=0, max_corr=1, draw=True):\n    if not features:\n        features = list(df_train.columns.values)\n    \n    if ""SalePrice"" in features:\n        features.remove(""SalePrice"")\n    \n    corr = pd.DataFrame(df_train[[""SalePrice""] + features].corr())\n    corr = corr[(corr[""SalePrice""].abs() >= min_corr) & (corr[""SalePrice""].abs() <= max_corr)]\n    selected_features = corr.index.values\n    corr = corr[selected_features]\n\n    if draw:\n        f, ax = plt.subplots(figsize=(15, 10))\n        sns.heatmap(corr, annot=True, fmt="".2f"")\n        \n    selected_features = list(selected_features)\n    \n    if ""SalePrice"" in selected_features:\n        selected_features.remove(""SalePrice"")\n        \n    return selected_features\n\n\n# In[22]:\n\n\nselected_features = corr_helper(features=num_features_without_na, min_corr=0.1)\n\n\n# Only feature \'OverallQual\' has a higher correlation to \'SalePrice\' than \'GrLivArea\'. According to the description, \'OverallQual\' rates the overall material and finish of the house. There are other features like \'1stFlrSF\', \'2ndFlrSF\' and \'TotRmsAbvGrd\' that strongly correlate to the sale price, but they also strongly correlate to \'GrLivArea\' - which makes sense.\n\n# In[23]:\n\n\nsns.regplot(x=""OverallQual"", y=""SalePrice"", data=df_train)\n\n\n# ### Imputation\n\n# In[25]:\n\n\ncolumns_with_na_values = df.columns[df.isnull().any()]\ncolumns_with_na_values\n\n\n# In[26]:\n\n\nprint(""{} of {} features have missing values."".format(len(columns_with_na_values), len(df.columns)))\nprint(df[columns_with_na_values].isnull().sum())\n\n\n# In[27]:\n\n\ncat_features = get_features(df, ""cat"")\ncat_features\n\n\n# In[28]:\n\n\nnone_values = [""Alley"", ""BsmtQual"", ""BsmtCond"", ""FireplaceQu"", ""Utilities"", ""BsmtExposure"", ""BsmtFinType1"", ""BsmtFinType2"", ""GarageType"", ""GarageFinish"", \n               ""Fence"", ""MiscFeature"", ""PoolQC"", ""GarageCond"", ""GarageQual"", ""KitchenQual"", ""MasVnrType""]\nnone_values\n\n\n# In[29]:\n\n\ndf[none_values] = df[none_values].fillna(""None"")\n\n\n# In[30]:\n\n\ncat_features_with_na_values = df[cat_features].columns[df[cat_features].isnull().any()]\ncat_features_with_na_values\n\n\n# In[31]:\n\n\nprint(""Remaining categorical features with missing values:"")\nprint(df[cat_features_with_na_values].isnull().sum())\n\n\n# In[32]:\n\n\naverage_value_map = {""Electrical"": ""Mix"", ""Exterior1st"": ""Other"", ""Exterior2nd"": ""Other"", ""Functional"": ""Mod"", \n                     ""MSZoning"": ""Oth"", ""SaleType"": ""Oth""}\naverage_value_map\n\n\n# In[33]:\n\n\nfor col, value in average_value_map.items():\n    df_train[col] = df_train[col].fillna(value)\n    df_test[col] = df_test[col].fillna(value)\n    df[col] = df[col].fillna(value)\n\n\n# In[34]:\n\n\ncat_features_with_na_values = df[cat_features].columns[df[cat_features].isnull().any()]\nprint(""Number of categorical features with missing values: {}"".format(len(cat_features_with_na_values)))\n\n\n# In[35]:\n\n\nnum_features = get_features(df, ""num"")\nnum_features\n\n\n# In[36]:\n\n\nnum_features_with_na_values = df[num_features].columns[df[num_features].isnull().any()].values\nnum_features_with_na_values\n\n\n# In[37]:\n\n\nprint(""{} of {} numerical features have missing values:"".format(len(num_features_with_na_values), len(num_features)))\nprint(df[num_features_with_na_values].isnull().sum())\n\n\n# In[38]:\n\n\nnone_values = [""BsmtFinSF1"", ""BsmtFinSF2"", ""BsmtFullBath"", ""BsmtHalfBath"", ""BsmtUnfSF"", ""GarageArea"", ""GarageCars"", \n               ""LotFrontage"", ""MasVnrArea"", ""TotalBsmtSF""]\nnone_values\n\n\n# In[39]:\n\n\ndf[none_values] = df[none_values].fillna(0)\n\n\n# In[41]:\n\n\ndf[none_values].head()\n\n\n# In[42]:\n\n\nnum_features_with_na_values = df.columns[df.isnull().any()].values\nnum_features_with_na_values\n\n\n# In[43]:\n\n\nprint(""Columns with missing values:"")\nprint(num_features_with_na_values)\n\n\n# In[44]:\n\n\ndf.drop(""GarageYrBlt"", axis=1, inplace=True)\nprint(""Removed feature \'GarageYrBlt\'."")\n\n\n# In[45]:\n\n\nfeatures_with_na_values = df.columns[df.isnull().any()]\nprint(""Number of features with missing values: {}"".format(len(features_with_na_values)))\n\n\n# In[48]:\n\n\ntrain_size = len(df_train)\ntrain_size\n\n\n# In[49]:\n\n\n# helper method to update df_train and df_test according to df.\ndef update_dfs(df, features=None):\n    global df_train, df_test\n    if not features:\n        features = df.columns\n    \n    targets = df_train[""SalePrice""]\n    df_train = df[:train_size][features]\n    df_train[""SalePrice""] = targets\n    df_test = df[train_size:][features]\n\n\n# In[50]:\n\n\nupdate_dfs(df)\n\nprint(""There are {} rows in df_train."".format(len(df_train)))\nprint(""There are {} rows in df_test."".format(len(df_test)))\n\n\n# ### III. Converting Categorical Features to Numerical Features\n\n# #### Categorical Features with a Natural Order\n# \n\n# In[51]:\n\n\n# helper function to convert a categorical feature into a numerical one.\ndef make_numerical(features, transform):\n    if not isinstance(features, list):\n        features = [features]\n        \n    features = list(df[features].select_dtypes(include=[""object""]).columns)\n    df[features] = df[features].replace(transform)\n\n\n# In[54]:\n\n\ncondition_rated_features = [""ExterQual"", ""ExterCond"", ""BsmtQual"", ""BsmtCond"", ""HeatingQC"", ""KitchenQual"", \n                            ""FireplaceQu"", ""GarageQual"", ""GarageCond"", ""PoolQC""]\n\nprint(""Example row before transformation:"")\nprint(df[condition_rated_features].head())\n\n\n# In[55]:\n\n\nmake_numerical(condition_rated_features, {""None"": 0, ""Po"": 1, ""Fa"": 2, ""TA"": 3, ""Gd"": 4, ""Ex"": 5})\n\nprint("""")\nprint(""After feature transformation:"")\nprint(df[condition_rated_features].head())\n\n\n# In[56]:\n\n\nremaining_cat_features = get_features(df, ""cat"")\nprint(""Remaining categorical features:"")\nprint(remaining_cat_features)\n\n\n# In[57]:\n\n\nmake_numerical(""Utilities"", {""None"": 0, ""ELO"": 1, ""NoSeWa"": 2, ""NoSewr"": 3, ""AllPub"": 4})\nmake_numerical(""LandSlope"", {""Gtl"": 1, ""Mod"": 2, ""Sev"": 3})\nmake_numerical(""LotShape"", {""Reg"": 0, ""IR1"": 1, ""IR2"": 2, ""IR3"": 3})\nmake_numerical(""BsmtExposure"", {""None"": 0, ""No"": 1, ""Mn"": 2, ""Av"": 3, ""Gd"": 4})\nmake_numerical(""BsmtFinType1"", {""None"": 0, ""Unf"": 1, ""LwQ"": 2, ""Rec"": 3, ""BLQ"": 4, ""ALQ"": 5, ""GLQ"": 6})\nmake_numerical(""BsmtFinType2"", {""None"": 0, ""Unf"": 1, ""LwQ"": 2, ""Rec"": 3, ""BLQ"": 4, ""ALQ"": 5, ""GLQ"": 6})\nmake_numerical(""CentralAir"", {""N"": 0, ""Y"": 1})\nmake_numerical(""Functional"", {""Typ"": 7, ""Min1"": 6, ""Min2"": 5, ""Mod"": 4, ""Maj1"": 3, ""Maj2"": 2, ""Sev"": 1, ""Sal"": 0})\nmake_numerical(""GarageFinish"", {""None"": 0, ""Unf"": 1, ""RFn"": 2, ""Fin"": 3})\nmake_numerical(""PavedDrive"", {""N"": 0, ""P"": 1, ""Y"": 2})\n\n\n# In[60]:\n\n\nupdate_dfs(df)\ndf.describe(include=\'all\')\n\n\n# In[61]:\n\n\nnew_num_features = list(set(cat_features) - set(remaining_cat_features))\nnew_num_features\n\n\n# In[62]:\n\n\nselected_features = corr_helper(features=new_num_features)\n\n\n# #### Categorical Features without a Natural Order\n\n# In[64]:\n\n\nremaining_cat_features = get_features(df, ""cat"")\nprint(""Remaining categorical features:"")\n\nremaining_cat_features\n\n\n# ##### sale price distribution for a few neighborhoods\n\n# In[65]:\n\n\ndef plot_helper(df, feature, limit=None, y_max=0.00002, height=4):\n    labels = df[feature].unique()\n    f, ax = plt.subplots(figsize=(15, height))\n    \n    if not limit:\n        limit = len(labels)\n        \n    for label in labels[:limit]:\n        sns.kdeplot(df[df_train[feature] == label][""SalePrice""], label=str(label))\n\n    plt.yticks([0, y_max])\n\n\n# In[66]:\n\n\nplot_helper(df_train, ""Neighborhood"", limit=5)\n\n\n# Prices in North Ridge are significantly higher on average than in Mitchell for example.\n\n# list of all neighborhoods and the average sale price for a house in this neighborhood.\n\n# In[67]:\n\n\n# helper method to display the average price for the different values of a feature\ndef avg_price(feature):\n    df_avg_price = df_train[[feature, ""SalePrice""]].groupby(feature).mean().sort_values(""SalePrice"")\n    df_avg_price[""NumberOfHouses""] = df_train[[feature]].groupby(feature).size()\n    return df_avg_price\n\n\n# In[68]:\n\n\navg_price_neighborhood = avg_price(""Neighborhood"")\nprint(avg_price_neighborhood)\n\n\n# Northridge actually has the highest prices on average compared to all other neighborhoods.\n# \n# If we want to use feature ""Neighborhood"" for a future model, it would make sense to group some of the neighborhoods before generating one-hot-encodings for this feature in order to reduce the dimensionality. Another way would be to assume an order on the neighborhoods based on the average sale price. This way, we can convert this feature the same way we did in the previous section.\n\n# In[69]:\n\n\nneighborhood_dict = {}\nfor num, name in enumerate(avg_price_neighborhood.index):\n    neighborhood_dict[name] = num\n    \nneighborhood_dict\n\n\n# In[70]:\n\n\nmake_numerical(""Neighborhood"", neighborhood_dict)\n\n\n# In[71]:\n\n\ndf[""FencePrivacy""] = df[""Fence""]\ndf[""FenceWood""] = df[""Fence""]\n\ndf.drop(""Fence"", axis=1, inplace=True)\n\nmake_numerical(""FencePrivacy"", {""GdPrv"": 2, ""MnPrv"": 1, ""GdWo"": 0, ""MnWw"": 0, ""None"": 0})\nmake_numerical(""FenceWood"", {""GdPrv"": 0, ""MnPrv"": 0, ""GdWo"": 2, ""MnWw"": 1, ""None"": 0})\nprint(""Created features \'FencePrivacy\' and \'FenceWood\' and removed feature \'Fence\'."")\n\n\n# In[72]:\n\n\nprint(""We currently have {} numerical features."".format(len(get_features(df, ""num""))))\n\n\n# In[73]:\n\n\nremaining_cat_features = get_features(df, ""cat"")\nprint(""We have {} unused features that can be one-hot-encoded:"".format(len(remaining_cat_features)))\n\nremaining_cat_features\n\n\n# In[75]:\n\n\n# We will keep track of all one-hot-encoded features in a list so that we can remove them at the end.\n\nencoded_features = set()\n\n\n# In[76]:\n\n\ncount = 0\n\nfor f in remaining_cat_features:\n    len_uniques = len(pd.get_dummies(df[f]).columns)\n    count += len_uniques\n    print(""One-hot-encoding of feature \'{}\' would generate {} new features"".format(f, len_uniques))\n    \nprint("""")\nprint(""Total number of potentially generated features: {}"".format(count))\n\n\n# We can actually bring this number down a bit. In my opinion, it makes sense to create combined one-hot-encodings for the feature pairs (\'Condition1\', \'Condition2\') and (\'Exterior1st\', \'Exterior2nd\').\n# \n# There are some misspellings in \'Exterior1st\' and \'Exterior2nd\'. We should correct those before encoding the features.\n\n# In[77]:\n\n\nprint(df[""Exterior1st""].unique())\nprint(df[""Exterior2nd""].unique())\n\ndf[[""Exterior1st"", ""Exterior2nd""]] = df[[""Exterior1st"", ""Exterior2nd""]].replace(""Wd Shng"", ""WdShing"")\ndf[[""Exterior1st"", ""Exterior2nd""]] = df[[""Exterior1st"", ""Exterior2nd""]].replace(""Brk Cmn"", ""BrkComm"")\ndf[[""Exterior1st"", ""Exterior2nd""]] = df[[""Exterior1st"", ""Exterior2nd""]].replace(""CemntBd"", ""CmentBd"")\n\nprint("""")\nprint(""Corrected misspellings."")\nsame_values = set(df[""Exterior1st""].unique()) == set(df[""Exterior2nd""].unique())\nprint(""\'Exterior1st\' and \'Exterior2nd\' have the same values: {}"".format(same_values))\n\n\n# In[78]:\n\n\n# helper method to create one-hot-encodings of features.\ndef one_hot_encode(df, features, prefix=None):   \n    global encoded_features\n    if not isinstance(features, list):\n        features = [features]\n    \n    if not set(features) <= set(df.columns.values):\n        print(""Not all features are columns in data frame."")\n        return None\n    \n    dummies = pd.get_dummies(df[features[0]], prefix=prefix).columns.values\n    df_dummies = pd.DataFrame(0, index=df.index, columns=dummies)\n    df[dummies] = df_dummies\n    \n    for f in features:\n        df[dummies] = df[dummies] + pd.get_dummies(df[f], prefix=prefix)\n        \n    df[dummies] = df[dummies].apply(lambda x: x > 0).astype(int)\n    encoded_features = encoded_features | set(features)\n    return list(dummies)\n\n\n# In[79]:\n\n\nprint(""Created combined one-hot-encoding for \'Condition1\' and \'Condition2\'. New features:"")\nprint(one_hot_encode(df, [""Condition1"", ""Condition2""], ""Condition""))\n\n\n# In[80]:\n\n\nprint(""Created combined one-hot-encoding for \'Exterior1st\' and \'Exterior2nd\'. New features:"")\nprint(one_hot_encode(df, [""Exterior1st"", ""Exterior2nd""], ""Exterior""))\n\n\n# In[81]:\n\n\nremaining_cat_features = [x for x in remaining_cat_features if x not in encoded_features]\nremaining_cat_features\n\n\n# In[82]:\n\n\nnew_features = []\n\nfor feature in remaining_cat_features:\n    features = one_hot_encode(df, feature, prefix=feature)        \n    new_features = new_features + features\n    \nprint(""Generated {} new features."".format(len(new_features)))\n\n\n# Here is another correlation matrix of the newly generated features that have an absolute correlation of at least 0.2 to \'SalePrice\'.\n\n# In[83]:\n\n\nupdate_dfs(df)\n\n\n# In[84]:\n\n\none_hot_enc_20_percent_corr = corr_helper(new_features, min_corr=0.2)\n\n\n# ##### Adding new features\n# \n# Let\'s try to come up with completely new features by combining some of the available features. \n# \n# I can imagine that the average room size of a house might have an impact on its sale price.\n\n# In[85]:\n\n\ndf[""AvgRoomSize""] = df[""GrLivArea""] / df[""TotRmsAbvGrd""]\nprint(""Created feature \'AvgRoomSize\'."")\n\n\n# In[86]:\n\n\n# Another feature that might be interesting are the sums of all condition and quality ratings.\n\nratings_qual = [""OverallQual"", ""ExterQual"", ""BsmtQual"", ""HeatingQC"", ""KitchenQual"", ""FireplaceQu"", ""GarageQual"",\n               ""PoolQC""]\nratings_cond = [""OverallCond"", ""ExterCond"", ""BsmtCond"", ""HeatingQC"", ""GarageCond""]\n\ndf[""SumQualCond""] = 0\n\nfor item in ratings_qual + ratings_cond:\n    df[""SumQualCond""] = df[""SumQualCond""] + df[item]\n\n\n# In[87]:\n\n\nfinal_features = get_features(df, ""num"")\nprint(""There are {} numerical features."".format(len(final_features)))\n\n# we only want the final features in the data for our final model\nupdate_dfs(df, final_features)\n\nprint(""There are {} rows in df_train."".format(len(df_train)))\nprint(""There are {} rows in df_test."".format(len(df_test)))\n\n\n# In[88]:\n\n\nfeatures_40_percent_sale_price_corr = corr_helper(min_corr=0.4)\n\n\n# ## V. Developing a Sophisticated Regression Model\n# \n# Just for comparison, let\'s see how a simple linear regression model performs with the features that we created in the previous sections.\n\n# In[89]:\n\n\nmodel_eval_helper(final_features, model=LinearRegression())\n\n\n# In[90]:\n\n\nprint(""Training linear regression model with features:"")\nprint(features_40_percent_sale_price_corr)\nmodel_eval_helper(features_40_percent_sale_price_corr, LinearRegression())\n\n\n# Again, we should have scaled our data before we trained our linear regression model. But since we won\'t use linear regression models from now on, we will skip scaling the data.\n\n# ### GradientBoostingRegressor\n# \n# Now we\'ll use a more GradientBoostingRegressor as a more sophisticated model. Ensemble methods like GradientBoostingRegressor usually perform extremely good in Kaggle competitions. Further, we don\'t have to worry about feature scaling and having too many features.\n\n# In[92]:\n\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# In[93]:\n\n\nreg = GradientBoostingRegressor(n_estimators=200, max_depth=2)\nreg\n\n\n# In[94]:\n\n\nmodel_eval_helper(final_features, model=reg)\n\n\n# In[95]:\n\n\ndf_importances = pd.DataFrame(reg.feature_importances_, index=final_features, columns=[""Importance""])\ndf_importances.sort_values(""Importance"", ascending=False, inplace=True)\n\nprint(df_importances)\n\n\n# Both in and out of sample error went down dramatically. And interestingly, \'GrLivArea\' actually seems to be the most important feature for predicting the sale price. \n# \n# There are a few features that are completely useless for our model.\n\n# In[96]:\n\n\nuseful_features = df_importances[df_importances[""Importance""] > 0].index.values\nprint(""{} of {} features had an importance greater than 0.0 for GradientBoostingRegressor:"".format(\n    len(useful_features), len(df_train.columns)))\nprint(useful_features)\n\n\n# In[97]:\n\n\nprint("""")\nuseless_features = df_importances[df_importances[""Importance""] == 0].index.values\nprint(""The following features had importance 0.0 for GradientBoostingRegressor:"")\nprint(useless_features)\n\n\n# ### Parameter Tuning\n\n# In[98]:\n\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import ShuffleSplit\n\n\n# In[99]:\n\n\nparam_grid = [{""max_depth"": [2, 3],\n               ""n_estimators"": [300, 500, 750],\n               ""max_features"": [""sqrt"", ""auto""]}]\nparam_grid\n\n\n# In[100]:\n\n\ncv = ShuffleSplit(n_splits=3, test_size=0.1, random_state=0)\ncv\n\n\n# In[101]:\n\n\nmodel = GradientBoostingRegressor(random_state=0)\nmodel\n\n\n# In[102]:\n\n\ngrid_search = GridSearchCV(model, param_grid, cv=cv, scoring=""neg_mean_squared_error"", n_jobs=-1)\ngrid_search\n\n\n# In[103]:\n\n\nX_train = df_train.drop(""SalePrice"", axis=1).values\ny_train = np.ravel(df_train[[""SalePrice""]].apply(np.log).values)\n\nX_train, y_train\n\n\n# In[104]:\n\n\nprint(""Training models..."")\ngrid_search.fit(X_train, y_train)\nprint(""Finished training models."")\n\n\n# In[105]:\n\n\nprint(""Best estimator:"")\nprint(grid_search.best_estimator_)\n\nrmse = np.sqrt(np.abs(grid_search.best_score_))\nprint(""Best estimator score: {}"".format(rmse))\n\n\n# ### Predictions\n\n# In[106]:\n\n\nbest_model = grid_search.best_estimator_\nbest_model\n\n\n# In[109]:\n\n\nX_predict = df_test.values\nX_predict\n\n\n# In[110]:\n\n\ny_predict = best_model.predict(X_predict)\ny_predict\n\n\n# In[111]:\n\n\n# rescale back from logarithmic price range.\ny_predict = np.exp(y_predict)\ny_predict\n\n\n# In[112]:\n\n\ndf_pred = pd.DataFrame(y_predict, index=df_test.index, columns=[""SalePrice""])\ndf_pred.to_csv(""predictions.csv"")\n\nprint(""Created predictions. Showing five sample rows of \'predictions.csv\':"")\nprint(df_pred.head())\n\n'"
projects/starlite/ml_model_deploy_pickle_1.py,3,"b'\r\n# coding: utf-8\r\n\r\n# # XOR Model | Pickle\r\n\r\nimport numpy as np\r\nimport sklearn\r\nimport pickle\r\nimport os\r\nfrom sklearn.neural_network import MLPClassifier as mlpc\r\n\r\nos.getcwd()\r\nos.listdir()\r\n\r\nxs = np.array([0,0,0,1,1,0,1,1]).reshape(4,2)\r\nys = np.array([0,1,1,0]).reshape(4,)\r\n\r\nmdl = mlpc(activation=\'relu\', max_iter=10000, hidden_layer_sizes=(4,2))\r\nmdl.fit(xs, ys)\r\n\r\nfilename = \'XOR_model.pkl\'\r\n\r\npickle.dump(mdl, open(filename, \'wb\'))\r\n\r\nos.listdir()\r\n\r\n\r\n# #### \'XOR_model.pkl\' \r\n# \r\n# Model has been saved into a \'pkl\' file.\r\n\r\n# help(\'modules\')\r\n\r\n\r\nfilename = \'XOR_model.pkl\'\r\n\r\n\r\nfrom flask import Flask, jsonify, request\r\n\r\ndef xor_predictor(a, b):\r\n    output = {\'XOR\': 0}\r\n    x_input = np.array([a,b]).reshape(1,2)\r\n    m1 = pickle.load(open(filename, \'rb\'))\r\n    output[\'XOR\'] = int(m1.predict(x_input)[0])\r\n    return output\r\n\r\napp = Flask(__name__)\r\n\r\n\r\n#### builtins.TypeError TypeError: Object of type \'int32\' is not JSON serializable\r\n\r\n#### To avoid this error - use int(m1.predict(x_input)[0])\r\n\r\n\r\n@app.route(""/"")\r\ndef index():\r\n    return ""XOR Predictor!!""\r\n\r\n\r\n# In[31]:\r\n\r\n\r\n@app.route(""/XOR"", methods=[\'GET\'])\r\ndef cal_xor_predictor():\r\n    body = request.get_data()\r\n    header = request.headers\r\n    \r\n    try:\r\n        num1 = int(request.args[\'x1\'])\r\n        num2 = int(request.args[\'x2\'])\r\n        if (num1 != None) and (num2 != None) and ((num1 == 0) or (num1 == 1)) and ((num2 == 0) or (num2 == 1)):\r\n            res = xor_predictor(num1, num2)\r\n        else:\r\n            res = {\r\n                \'success\': False,\r\n                \'message\': \'invalid inputs; 0 or 1 expected\'\r\n            }\r\n    except:\r\n        res = {\r\n            \'success\': False,\r\n            \'message\': \'Unknown error\'\r\n        }\r\n        \r\n    return jsonify(res)\r\n\r\n\r\n# In[33]:\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    app.run(debug=True, port=8808)\r\n\r\n'"
projects/midcurvenn/misc/build_cnn_encoderdecoder_model.py,4,"b'from keras import regularizers\nfrom keras.layers import Input, Dense\nfrom keras.models import Model, load_model\nfrom prepare_data import get_training_data\nfrom prepare_plots import plot_results\nimport numpy as np\nimport os\nimport random\nimport sys\nnp.set_printoptions(threshold=sys.maxsize)\n\nfrom keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose, ZeroPadding2D\nfrom keras import backend as K\n\nclass cnn_encoderdecoder:\n    def __init__(self):\n        self.encoding_dim = 100\n        self.input_dim = 100\n        self.epochs = 50\n        self.cnn_autoencoder_model_pkl = ""models/cnn_autoencoder_model.pkl""\n\n                \n    def process_images(self,grayobjs):\n        flat_objs = [x.reshape(self.input_dim,self.input_dim,1) for x in grayobjs]\n        pngs_objs = np.array(flat_objs)\n        return pngs_objs\n\n    def train(self,\n            profile_pngs_gray_objs, \n            midcurve_pngs_gray_objs):\n        \n        if not os.path.exists(self.cnn_autoencoder_model_pkl):       \n            input_img = Input(shape=(self.input_dim, self.input_dim, 3))  # adapt this if using `channels_first` image data format\n            \n            #x = ZeroPadding2D((5,5))(input_img)\n            x = Conv2D(32, (3, 3), strides=(2, 2), activation=\'relu\', padding=\'same\')(input_img)\n            #x = MaxPooling2D((2, 2), padding=\'same\')(x)\n            x = Conv2D(16, (3, 3), strides=(2, 2), activation=\'relu\', padding=\'same\')(x)\n            #x = MaxPooling2D((2, 2), padding=\'same\')(x)\n            x = Conv2D(16, (3, 3), strides=(2, 2), activation=\'relu\', padding=\'same\')(x)\n            x = Conv2D(8, (3, 3), strides=(2, 2), activation=\'relu\', padding=\'same\')(x)\n            encoded = Conv2D(8, (3, 3), strides=(2,2), activation=\'relu\', padding=\'same\')(x)\n            #\n            #encoded = MaxPooling2D((2, 2), padding=\'same\')(x)\n        \n            # at this point the representation is (4, 4, 8) i.e. 128-dimensional\n            \n            \'\'\'x = Conv2D(8, (3, 3), activation=\'relu\', padding=\'same\')(encoded)\n            x = UpSampling2D((2, 2))(x)\n            x = Conv2D(8, (3, 3), activation=\'relu\', padding=\'same\')(x)\n            x = UpSampling2D((2, 2))(x)\n            x = Conv2D(16, (3, 3), activation=\'relu\')(x)\n            x = UpSampling2D((2, 2))(x)\n            decoded = Conv2D(1, (3, 3), activation=\'sigmoid\', padding=\'same\')(x)\'\'\'\n            #y = ZeroPadding2D()(encoded)\n            y = Conv2DTranspose(8, (3, 3), strides = (2, 2), padding=\'same\', activation=\'relu\')(encoded)\n            y = Conv2DTranspose(4, (3, 3), strides = (2, 2), padding=\'same\', activation=\'relu\')(y)\n            y = Conv2DTranspose(4, (3, 3), strides = (2, 2), padding=\'same\', activation=\'relu\')(y)\n            y = Conv2DTranspose(2, (3, 3), strides = (2, 2), padding=\'same\', activation=\'relu\')(y)\n            decoded = Conv2DTranspose(1, (3, 3), strides = (2, 2), activation=\'tanh\', padding=\'same\')(y)\n            \n            \n            # Model 1: Full AutoEncoder, includes both encoder single dense layer and decoder single dense layer. \n            # This model maps an input to its reconstruction\n            self.cnn_autoencoder = Model(input_img, decoded)\n            self.cnn_autoencoder.summary()\n            \n            # Compilation of Autoencoder (only)\n            self.cnn_autoencoder.compile(optimizer=\'adam\', loss=\'binary_crossentropy\')\n            \n            \n            # Training\n            #profile_pngs_flat_objs = [x.reshape(input_dim,input_dim,1) for x in profile_pngs_gray_objs]\n            #midcurve_pngs_flat_objs = [x.reshape(input_dim,input_dim,1) for x in midcurve_pngs_gray_objs]\n            \n            #profile_pngs_objs = np.array(profile_pngs_flat_objs)\n            #midcurve_pngs_objs= np.array(midcurve_pngs_flat_objs)\n        \n#             profile_pngs_objs = profile_pngs_gray_objs\n#             midcurve_pngs_objs = midcurve_pngs_gray_objs\n#             \n#             train_size = int(len(profile_pngs_objs)*0.7)\n#             x_train = profile_pngs_objs[:train_size]\n#             y_train = midcurve_pngs_objs[:train_size]\n#             x_test = profile_pngs_objs[train_size:]\n#             y_test = midcurve_pngs_objs[train_size:]\n#             self.cnn_autoencoder.fit(x_train, y_train,\n#                         epochs=50,\n#                         batch_size=5,\n#                         shuffle=True,\n#                         validation_data=(x_test, y_test))\n                \n            profile_pngs_objs = self.process_images(profile_pngs_gray_objs)\n            midcurve_pngs_objs = self.process_images(midcurve_pngs_gray_objs)                \n            self.x = profile_pngs_objs\n            self.y = midcurve_pngs_objs\n            \n            self.cnn_autoencoder.fit(self.x, self.y,\n                        epochs=5,\n                        batch_size=5,\n                        shuffle=True)\n                            \n            # Save models\n            self.cnn_autoencoder.save(self.autoencoder_model_pkl)\n        else:\n            # Save models\n            self.cnn_autoencoder = load_model(self.autoencoder_model_pkl)\n\n    \n    def predict(self, test_profile_images):\n        encoded_imgs = self.cnn_autoencoder.predict(test_profile_images)\n        decoded_imgs = self.cnn_autoencoder.predict(encoded_imgs)       \n        return test_profile_images,decoded_imgs  \n    \n\nif __name__ == ""__main__"":\n    profile_gray_objs, midcurve_gray_objs = get_training_data()\n    endec = cnn_encoderdecoder()\n    endec.train(profile_gray_objs, midcurve_gray_objs)\n    \n    test_gray_images = random.sample(profile_gray_objs,5)\n    original_profile_imgs,predicted_midcurve_imgs = endec.predict(test_gray_images)\n    plot_results(original_profile_imgs,predicted_midcurve_imgs)'"
projects/midcurvenn/misc/build_denoiser_encoderdecoder_model.py,4,"b'from keras.layers import Input, Dense\nfrom keras.models import Model, load_model\nfrom keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose, ZeroPadding2D\nfrom keras import backend as K\nfrom keras import regularizers\nfrom build_simple_encoderdecoder_model import simple_encoderdecoder\nfrom prepare_data import get_training_data\nfrom prepare_plots import plot_results\nimport os\nimport numpy as np\nimport random\nimport sys\nnp.set_printoptions(threshold=sys.maxsize)\n\nclass denoiser_encoderdecoder:\n\tdef __init__(self):\n\t\tself.encoding_dim = 100\n\t\tself.input_dim = 100\n\t\tself.epochs = 50\n\t\tself.denoiser_autoencoder_model_pkl = ""models/denoiser_autoencoder_model.pkl""\n\n\t\t\t\t\n\tdef process_images(self,grayobjs):\n\t\tflat_objs = [x.reshape(self.input_dim,self.input_dim,1) for x in grayobjs]\n\t\tpngs_objs = np.array(flat_objs)\n\t\treturn pngs_objs\n\n\tdef train(self,noisy_images_objs, clean_images_objs):\n\t\t\n\t\tif not os.path.exists(self.denoiser_autoencoder_model_pkl):\t   \n\t\t\t# this is our input placeholder\n\t\t\tinput_img = Input(shape=(self.input_dim,self.input_dim,1))\n\t\t\t\n\t\t\tx = Conv2D(32, (3, 3), activation=\'relu\', padding=\'same\')(input_img)\n\t\t\tx = MaxPooling2D((2, 2), padding=\'same\')(x)\n\t\t\tx = Conv2D(32, (3, 3), activation=\'relu\', padding=\'same\')(x)\n\t\t\tencoded = MaxPooling2D((2, 2), padding=\'same\')(x)\n\t\t\t\n\t\t\t# at this point the representation is (7, 7, 32)\n\t\t\t\n\t\t\tx = Conv2D(32, (3, 3), activation=\'relu\', padding=\'same\')(encoded)\n\t\t\tx = UpSampling2D((2, 2))(x)\n\t\t\tx = Conv2D(32, (3, 3), activation=\'relu\', padding=\'same\')(x)\n\t\t\tx = UpSampling2D((2, 2))(x)\n\t\t\tdecoded = Conv2D(1, (3, 3), activation=\'sigmoid\', padding=\'same\')(x)\n\t\t\t\n\t\t\t# create the self.denoiser_autoencoder model\n\t\t\tself.denoiser_autoencoder = Model(input_img, decoded)\n\t\t\t\n\t\t\t# Compilation of Autoencoder (only)\n\t\t\tself.denoiser_autoencoder.compile(optimizer=\'adadelta\', loss=\'binary_crossentropy\')\n\t\t\t\n# \t\t\t# Training\n# \t\t\tprofile_pngs_flat_objs = [x.reshape(input_dim) for x in profile_pngs_gray_objs]\n# \t\t\tmidcurve_pngs_flat_objs = [x.reshape(input_dim) for x in midcurve_pngs_gray_objs]\n# \t\t\t\n# \t\t\tprofile_pngs_objs = np.array(profile_pngs_flat_objs)\n# \t\t\tmidcurve_pngs_objs= np.array(midcurve_pngs_flat_objs)\n# \t\t\t\n# \t\t\ttrain_size = int(len(profile_pngs_objs)*0.7)\n# \t\t\tx_train = profile_pngs_objs[:train_size]\n# \t\t\ty_train = midcurve_pngs_objs[:train_size]\n# \t\t\tx_test = profile_pngs_objs[train_size:]\n# \t\t\ty_test = midcurve_pngs_objs[train_size:]\n# \t\t\tautoencoder.fit(x_train, y_train,\n# \t\t\t\t\t\tepochs=200,\n# \t\t\t\t\t\tbatch_size=5,\n# \t\t\t\t\t\tshuffle=True,\n# \t\t\t\t\t\tvalidation_data=(x_test, y_test))\n\t\t\t\t\n\t\t\tself.x = noisy_images_objs\n\t\t\tself.y = clean_images_objs\n\t\t\tself.denoiser_autoencoder.fit(self.x, self.y,\n\t\t\t\t\t\tepochs=self.epochs,\n\t\t\t\t\t\tbatch_size=5,\n\t\t\t\t\t\tshuffle=True)\t\t\t\t\t \n\t\t\t# Save models\n\t\t\tself.denoiser_autoencoder.save(self.denoiser_autoencoder_model_pkl)\n\n\t\telse:\n\t\t\t# Save models\n\t\t\tself.denoiser_autoencoder = load_model(self.denoiser_autoencoder_model_pkl)\n\n\t\n\tdef predict(self, test_noisy_images):\n\t\tpng_profile_images = self.process_images(test_noisy_images)\n\t\tencoded_imgs = self.denoiser_autoencoder.predict(png_profile_images)\n\t\tdecoded_imgs = self.denoiser_autoencoder.predict(encoded_imgs)\t\n\t\treturn test_noisy_images,decoded_imgs  \n\t\n\nif __name__ == ""__main__"":\n\tprofile_gray_objs, midcurve_gray_objs = get_training_data()\n\tendec = simple_encoderdecoder()\n\tendec.train(profile_gray_objs, midcurve_gray_objs)\n\toriginal_profile_images,noisy_predicted_midcurve_images = endec.predict(profile_gray_objs)\n\tplot_results(original_profile_images[:5],noisy_predicted_midcurve_images[:5])\n\t\n\tdenoiser = denoiser_encoderdecoder()\n\tdenoiser.train(noisy_predicted_midcurve_images, midcurve_gray_objs)\n\tsample_noisy_midcurve_images = random.sample(noisy_predicted_midcurve_images,5)\n\toriginal_noisy_midcurve_images,clean_predicted_midcurve_images = denoiser.predict(sample_noisy_midcurve_images)\n\tplot_results(original_noisy_midcurve_images,clean_predicted_midcurve_images)\t'"
projects/midcurvenn/misc/build_dense_encoderdecoder_model.py,4,"b'from keras import regularizers\nfrom keras.layers import Input, Dense\nfrom keras.models import Model, load_model\nfrom prepare_data import get_training_data\nfrom prepare_plots import plot_results\nimport numpy as np\nimport os\nimport sys\nimport random\n\nnp.set_printoptions(threshold=sys.maxsize)\n\nclass dense_encoderdecoder:\n    def __init__(self):\n        self.encoding_dim = 100\n        self.input_dim = 10000\n        self.epochs = 50\n        self.autoencoder_model_pkl = ""models/dense_autoencoder_model.pkl""\n        self.encoder_model_pkl = ""models/dense_encoder_model.pkl""\n        self.decoder_model_pkl = ""models/dense_decoder_model.pkl""\n                \n    def process_images(self,grayobjs):\n        flat_objs = [x.reshape(self.input_dim) for x in grayobjs]\n        pngs_objs = np.array(flat_objs)\n        return pngs_objs\n\n    def train(self,\n            profile_pngs_gray_objs, \n            midcurve_pngs_gray_objs):\n        if not os.path.exists(self.autoencoder_model_pkl):        \n            # this is our input placeholder\n            input_img = Input(shape=(self.input_dim,))\n            \n            # ""encoded"" is the encoded representation of the input\n            encoded = Dense(self.input_dim, activation=\'relu\')(input_img)\n            encoded = Dense(self.input_dim, activation=\'relu\')(encoded)\n            encoded = Dense(self.encoding_dim, activation=\'relu\')(encoded)\n            \n            # ""decoded"" is the lossy reconstruction of the input\n            decoded = Dense(self.encoding_dim, activation=\'relu\')(encoded)\n            decoded = Dense(self.input_dim, activation=\'relu\')(decoded)\n            decoded = Dense(self.input_dim, activation=\'sigmoid\')(decoded) \n            \n            # Model 1: Full AutoEncoder, includes both encoder single dense layer and decoder single dense layer. \n            # This model maps an input to its reconstruction\n            self.autoencoder = Model(input_img, decoded)\n                    \n            # Model 2: a separate encoder model: -------------------\n            # this model maps an input to its encoded representation\n            self.encoder = Model(input_img, encoded)\n            \n            # Model 3: a separate encoder model: -------------------\n            # create a placeholder for an encoded (32-dimensional) input\n            encoded_input = Input(shape=(self.encoding_dim,))\n            # retrieve the last to last layer of the autoencoder model\n            decoder_layer = self.autoencoder.layers[-2]\n            # create the decoder model\n            self.decoder = Model(encoded_input, decoder_layer(encoded_input))            \n                    \n            # Compilation of Autoencoder (only)\n            self.autoencoder.compile(optimizer=\'adadelta\', loss=\'binary_crossentropy\')\n        \n                \n#             # Training\n#             profile_pngs_flat_objs = [x.reshape(input_dim) for x in profile_pngs_gray_objs]\n#             midcurve_pngs_flat_objs = [x.reshape(input_dim) for x in midcurve_pngs_gray_objs]\n#              \n#             profile_pngs_objs = np.array(profile_pngs_flat_objs)\n#             midcurve_pngs_objs= np.array(midcurve_pngs_flat_objs)\n        \n            profile_pngs_objs = self.process_images(profile_pngs_gray_objs)\n            midcurve_pngs_objs = self.process_images(midcurve_pngs_gray_objs)\n                    \n#             train_size = int(len(profile_pngs_objs)*0.7)\n#             self.x_train = profile_pngs_objs[:train_size]\n#             self.y_train = midcurve_pngs_objs[:train_size]\n#             self.x_test = profile_pngs_objs[train_size:]\n#             self.y_test = midcurve_pngs_objs[train_size:]\n#             self.autoencoder.fit(self.x_train, self.y_train,\n#                         epochs=self.epochs,\n#                         batch_size=32,\n#                         shuffle=True,\n#                         validation_data=(self.x_test, self.y_test))\n                \n            self.x = profile_pngs_objs\n            self.y = midcurve_pngs_objs\n            self.autoencoder.fit(self.x, self.y,\n                        epochs=self.epochs,\n                        batch_size=5,\n                        shuffle=True)                     \n            # Save models\n            self.autoencoder.save(self.autoencoder_model_pkl)\n            self.encoder.save(self.encoder_model_pkl)\n            self.decoder.save(self.decoder_model_pkl)  \n        else:\n            # Save models\n            self.autoencoder = load_model(self.autoencoder_model_pkl)\n            self.encoder= load_model(self.encoder_model_pkl)\n            self.decoder = load_model(self.decoder_model_pkl)\n    \n    def predict(self, test_profile_images):\n        png_profile_images = self.process_images(test_profile_images)\n        encoded_imgs = self.encoder.predict(png_profile_images)\n        decoded_imgs = self.decoder.predict(encoded_imgs)    \n        return test_profile_images,decoded_imgs  \n    \n\nif __name__ == ""__main__"":\n    profile_gray_objs, midcurve_gray_objs = get_training_data()\n    endec = dense_encoderdecoder()\n    endec.train(profile_gray_objs, midcurve_gray_objs)\n    \n    test_gray_images = random.sample(profile_gray_objs,5)\n    original_profile_imgs,predicted_midcurve_imgs = endec.predict(test_gray_images)\n    plot_results(original_profile_imgs,predicted_midcurve_imgs)'"
projects/midcurvenn/misc/build_simple_encoderdecoder_model.py,4,"b'from keras import regularizers\nfrom keras.layers import Input, Dense\nfrom keras.models import Model, load_model\nfrom prepare_data import get_training_data\nfrom prepare_plots import plot_results\nimport os\nimport numpy as np\nimport sys\nimport random\n\nnp.set_printoptions(threshold=sys.maxsize)\n\nclass simple_encoderdecoder:\n    def __init__(self):\n        self.encoding_dim = 100\n        self.input_dim = 10000\n        self.epochs = 200\n        self.autoencoder_model_pkl = ""models/autoencoder_model.pkl""\n        self.encoder_model_pkl = ""models/encoder_model.pkl""\n        self.decoder_model_pkl = ""models/decoder_model.pkl""\n                \n    def process_images(self,grayobjs):\n        flat_objs = [x.reshape(self.input_dim) for x in grayobjs]\n        pngs_objs = np.array(flat_objs)\n        return pngs_objs\n\n    def train(self,\n            profile_pngs_gray_objs, \n            midcurve_pngs_gray_objs):\n        \n        if not os.path.exists(self.autoencoder_model_pkl):\n            # this is our input placeholder\n            input_img = Input(shape=(self.input_dim,))\n            \n            # ""encoded"" is the encoded representation of the input\n            encoded = Dense(self.encoding_dim, activation=\'relu\',activity_regularizer=regularizers.l1(10e-5))(input_img)\n            # ""decoded"" is the lossy reconstruction of the input\n            decoded = Dense(self.input_dim, activation=\'sigmoid\')(encoded) \n            \n            # Model 1: Full AutoEncoder, includes both encoder single dense layer and decoder single dense layer. \n            # This model maps an input to its reconstruction\n            self.autoencoder = Model(input_img, decoded)\n                    \n            # Model 2: a separate encoder model: -------------------\n            # this model maps an input to its encoded representation\n            self.encoder = Model(input_img, encoded)\n            \n            # Model 3: a separate encoder model: -------------------\n            # create a placeholder for an encoded (32-dimensional) input\n            encoded_input = Input(shape=(self.encoding_dim,))\n            # retrieve the last layer of the autoencoder model\n            decoder_layer = self.autoencoder.layers[-1]\n            # create the decoder model\n            self.decoder = Model(encoded_input, decoder_layer(encoded_input))\n            \n            # Compilation of Autoencoder (only)\n            self.autoencoder.compile(optimizer=\'adadelta\', loss=\'binary_crossentropy\')\n            \n#             # Training\n#             profile_pngs_flat_objs = [x.reshape(self.input_dim) for x in profile_pngs_gray_objs]\n#             midcurve_pngs_flat_objs = [x.reshape(self.input_dim) for x in midcurve_pngs_gray_objs]\n#             \n#             profile_pngs_objs = np.array(profile_pngs_flat_objs)\n#             midcurve_pngs_objs= np.array(midcurve_pngs_flat_objs)\n#             \n            profile_pngs_objs = self.process_images(profile_pngs_gray_objs)\n            midcurve_pngs_objs = self.process_images(midcurve_pngs_gray_objs)\n            \n#             train_size = int(len(profile_pngs_objs)*0.7)\n#             self.x_train = profile_pngs_objs[:train_size]\n#             self.y_train = midcurve_pngs_objs[:train_size]\n#             self.x_test = profile_pngs_objs[train_size:]\n#             self.y_test = midcurve_pngs_objs[train_size:]\n#             self.autoencoder.fit(self.x_train, self.y_train,\n#                         epochs=self.epochs,\n#                         batch_size=5,\n#                         shuffle=True,\n#                         validation_data=(self.x_test, self.y_test))\n                \n            self.x = profile_pngs_objs\n            self.y = midcurve_pngs_objs\n            self.autoencoder.fit(self.x, self.y,\n                        epochs=self.epochs,\n                        batch_size=5,\n                        shuffle=True)                \n            # Save models\n            self.autoencoder.save(self.autoencoder_model_pkl)\n            self.encoder.save(self.encoder_model_pkl)\n            self.decoder.save(self.decoder_model_pkl)  \n        else:\n            # Save models\n            self.autoencoder = load_model(self.autoencoder_model_pkl)\n            self.encoder= load_model(self.encoder_model_pkl)\n            self.decoder = load_model(self.decoder_model_pkl)\n    \n    def predict(self, test_profile_images):\n        png_profile_images = self.process_images(test_profile_images)\n        encoded_imgs = self.encoder.predict(png_profile_images)\n        decoded_imgs = self.decoder.predict(encoded_imgs)    \n        return test_profile_images,decoded_imgs  \n           \nif __name__ == ""__main__"":\n    profile_gray_objs, midcurve_gray_objs = get_training_data()\n    endec = simple_encoderdecoder()\n    endec.train(profile_gray_objs, midcurve_gray_objs)\n    \n    test_gray_images = random.sample(profile_gray_objs,5)\n    original_profile_imgs,predicted_midcurve_imgs = endec.predict(test_gray_images)\n    plot_results(original_profile_imgs,predicted_midcurve_imgs)'"
projects/midcurvenn/misc/keras_gan_data_loader.py,22,"b'# Original : https://github.com/eriklindernoren/Keras-GAN/tree/master/pix2pix\nfrom glob import glob\nimport numpy as np\nimport imageio\n\n#datasetpath = ""D:/Yogesh/Projects/Learning/DataScience/Datasets/pix2pix/""\n\nclass DataLoader():\n    def __init__(self, dataset_name, img_res=(256, 256)):\n        self.dataset_name = dataset_name\n        self.img_res = img_res\n        \n    def binarize(self, image):\n        h, w = image.shape\n        for i in range(h):\n          for j in range(w):\n              if image[i][j] <= 192:\n                image[i][j] = 0\n        return image\n\n    def load_data(self, batch_size=1, is_testing=False):\n        data_type = ""train"" if not is_testing else ""test""\n        path = glob(\'data/%s/datasets/%s/%s/*\' % (self.dataset_name, self.dataset_name, data_type))\n        #path = glob(PATH + \'%s/*\' % (data_type))\n        batch_images = np.random.choice(path, size=batch_size)\n\n        imgs_A = []\n        imgs_B = []\n        for img_path in batch_images:\n            img = self.imread(img_path)\n            img = self.binarize(img)\n            img = np.expand_dims(img, axis=-1)\n            h, w, _ = img.shape\n            _w = int(w/2)\n            img_A, img_B = img[:, :_w, :], img[:, _w:, :]\n\n            #  img_A = scipy.misc.imresize(img_A, self.img_res)\n            #  img_A = np.array(Img.fromarray(img_A).resize(self.img_res))\n            #img_A = np.array(skimage.transform.resize(img_A,self.img_res))\n            #  img_B = scipy.misc.imresize(img_B, self.img_res)\n            #  img_B = np.array(Img.fromarray(img_B).resize(self.img_res))\n            #img_B = np.array(skimage.transform.resize(img_B,self.img_res))\n\n            # If training => do random flip\n            if not is_testing and np.random.random() < 0.5:\n                img_A = np.fliplr(img_A)\n                img_B = np.fliplr(img_B)\n\n            imgs_A.append(img_A)\n            imgs_B.append(img_B)\n\n        imgs_A = np.array(imgs_A)/127.5 - 1.\n        imgs_B = np.array(imgs_B)/127.5 - 1.\n\n        return imgs_A, imgs_B\n\n    def load_batch(self, batch_size=1, is_testing=False):\n        data_type = ""train"" if not is_testing else ""val""\n        path = glob(\'data/%s/datasets/%s/%s/*\' % (self.dataset_name, self.dataset_name, data_type))\n        #path = glob(PATH + \'%s/*\' % (data_type))\n        self.n_batches = int(len(path) / batch_size)\n\n        for i in range(self.n_batches-1):\n            batch = path[i*batch_size:(i+1)*batch_size]\n            imgs_A, imgs_B = [], []\n            for img in batch:\n                img = self.imread(img)\n                img = self.binarize(img)\n                img = np.expand_dims(img, axis=-1)\n                h, w, _ = img.shape\n                half_w = int(w/2)\n                img_A = img[:, :half_w, :]\n                img_B = img[:, half_w:, :]\n\n                #  img_A = scipy.misc.imresize(img_A, self.img_res)\n                #  img_A = np.array(Img.fromarray(img_A).resize(self.img_res))\n                #img_A = np.array(skimage.transform.resize(img_A,self.img_res))\n                #  img_B = scipy.misc.imresize(img_B, self.img_res)\n                #  img_B = np.array(Img.fromarray(img_B).resize(self.img_res))\n                #img_B = np.array(skimage.transform.resize(img_B,self.img_res))\n\n                if not is_testing and np.random.random() > 0.5:\n                        img_A = np.fliplr(img_A)\n                        img_B = np.fliplr(img_B)\n\n                imgs_A.append(img_A)\n                imgs_B.append(img_B)\n\n            imgs_A = np.array(imgs_A)/127.5 - 1.\n            imgs_B = np.array(imgs_B)/127.5 - 1.\n\n            yield imgs_A, imgs_B\n\n\n    def imread(self, path):\n        return imageio.imread(path).astype(np.float)\n'"
projects/midcurvenn/misc/keras_gan_pix2pix.py,4,"b'# Original : https://github.com/eriklindernoren/Keras-GAN/tree/master/pix2pix\nfrom __future__ import print_function, division\n\nimport datetime\nimport matplotlib.pyplot as plt\nfrom keras_gan_data_loader import DataLoader\nimport numpy as np\nimport os\nimport tensorflow as tf\n\n# from keras import backend as K\n# import tensorflow as tf\n# \n# NUM_PARALLEL_EXEC_UNITS = 6\n# \n# config = tf.ConfigProto(intra_op_parallelism_threads = NUM_PARALLEL_EXEC_UNITS, \n#          inter_op_parallelism_threads = 0, \n#          allow_soft_placement = True, \n#          device_count = {\'CPU\': NUM_PARALLEL_EXEC_UNITS })\n# \n# session = tf.Session(config=config)\n# \n# K.set_session(session)\n\nclass Pix2Pix():\n    def __init__(self):\n        # Input shape\n        self.img_rows = 256\n        self.img_cols = 256\n        self.channels = 1\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n\n        # Configure data loader\n        self.dataset_name = \'pix2pix\'\n        self.data_loader = DataLoader(dataset_name=self.dataset_name,\n                                      img_res=(self.img_rows, self.img_cols))\n\n\n        # Calculate output shape of D (PatchGAN)\n        patch = int(self.img_rows / 2**4)\n        self.disc_patch = (patch, patch, 1)\n\n        # Number of filters in the first layer of G and D\n        self.gf = int(self.img_rows/4) # 64\n        self.df = int(self.img_rows/4) # 64\n\n        optimizer = tf.keras.optimizers.Adam(0.0002, 0.5)\n\n        # Build and compile the discriminator\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss=\'mse\',\n            optimizer=optimizer,\n            metrics=[\'accuracy\'])\n\n        #-------------------------\n        # Construct Computational\n        #   Graph of Generator\n        #-------------------------\n\n        # Build the generator\n        self.generator = self.build_generator()\n\n        # Input images and their conditioning images\n        img_A = tf.keras.layers.Input(shape=self.img_shape)\n        img_B = tf.keras.layers.Input(shape=self.img_shape)\n\n        #By conditioning on A generate a fake version of B\n        fake_B = self.generator(img_A)\n        \n        # For the combined model we will only train the generator\n        self.discriminator.trainable = False\n\n        # Discriminators determines validity of translated images / condition pairs\n        valid = self.discriminator([img_A, fake_B])\n\n        self.combined = tf.keras.models.Model(inputs=[img_A, img_B], outputs=[valid, fake_B])\n        self.combined.compile(loss=[\'mse\', \'mae\'],\n                              loss_weights=[1, 100],\n                              optimizer=optimizer)\n\n    def build_generator(self):\n        """"""U-Net Generator""""""\n\n        def conv2d(layer_input, filters, f_size=4, bn=True):\n            """"""Layers used during downsampling""""""\n            d = tf.keras.layers.Conv2D(filters, kernel_size=f_size, strides=2, padding=\'same\')(layer_input)\n            d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n            if bn:\n                d = tf.keras.layers.BatchNormalization(momentum=0.8)(d)\n            return d\n\n        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n            """"""Layers used during upsampling""""""\n            u = tf.keras.layers.UpSampling2D(size=2)(layer_input)\n            u = tf.keras.layers.Conv2D(filters, kernel_size=f_size, strides=1, padding=\'same\', activation=\'relu\')(u)\n            if dropout_rate:\n                u = tf.keras.layers.Dropout(dropout_rate)(u)\n            u = tf.keras.layers.BatchNormalization(momentum=0.8)(u)\n            u = tf.keras.layers.Concatenate()([u, skip_input])\n            return u\n\n        # Image input\n        d0 = tf.keras.layers.Input(shape=self.img_shape)\n\n        # Downsampling\n        d1 = conv2d(d0, self.gf, bn=False)\n        d2 = conv2d(d1, self.gf*2)\n        d3 = conv2d(d2, self.gf*4)\n        d4 = conv2d(d3, self.gf*8)\n        d5 = conv2d(d4, self.gf*8)\n        d6 = conv2d(d5, self.gf*8)\n        d7 = conv2d(d6, self.gf*8)\n\n        # Upsampling\n        u1 = deconv2d(d7, d6, self.gf*8)\n        u2 = deconv2d(u1, d5, self.gf*8)\n        u3 = deconv2d(u2, d4, self.gf*8)\n        u4 = deconv2d(u3, d3, self.gf*4)\n        u5 = deconv2d(u4, d2, self.gf*2)\n        u6 = deconv2d(u5, d1, self.gf)\n\n        u7 = tf.keras.layers.UpSampling2D(size=2)(u6)\n        output_img = tf.keras.layers.Conv2D(self.channels, kernel_size=4, strides=1, padding=\'same\', activation=\'tanh\')(u7)\n\n        return tf.keras.models.Model(d0, output_img)\n\n    def build_discriminator(self):\n\n        def d_layer(layer_input, filters, f_size=4, bn=True):\n            """"""Discriminator layer""""""\n            d = tf.keras.layers.Conv2D(filters, kernel_size=f_size, strides=2, padding=\'same\')(layer_input)\n            d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n            if bn:\n                d = tf.keras.layers.BatchNormalization(momentum=0.8)(d)\n            return d\n\n        img_A = tf.keras.layers.Input(shape=self.img_shape)\n        img_B = tf.keras.layers.Input(shape=self.img_shape)\n\n        # Concatenate image and conditioning image by channels to produce input\n        combined_imgs = tf.keras.layers.Concatenate(axis=-1)([img_A, img_B])\n\n        d1 = d_layer(combined_imgs, self.df, bn=False)\n        d2 = d_layer(d1, self.df*2)\n        d3 = d_layer(d2, self.df*4)\n        d4 = d_layer(d3, self.df*8)\n\n        validity = tf.keras.layers.Conv2D(1, kernel_size=4, strides=1, padding=\'same\')(d4)\n\n        return tf.keras.models.Model([img_A, img_B], validity)\n\n    def train(self, epochs, batch_size=1, sample_interval=50):\n        start_time = datetime.datetime.now()\n\n        # Adversarial loss ground truths\n        valid = np.ones((batch_size,) + self.disc_patch)\n        fake = np.zeros((batch_size,) + self.disc_patch)\n\n        for epoch in range(epochs):\n            for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n                # ---------------------\n                #  Train Discriminator\n                # ---------------------\n\n                #Condition on A and generate a translated version\n                fake_B = self.generator.predict(imgs_A)\n\n                # Train the discriminators (original images = real / generated = Fake)\n                d_loss_real = self.discriminator.train_on_batch([imgs_A, imgs_B], valid)\n                d_loss_fake = self.discriminator.train_on_batch([imgs_A, fake_B], fake)\n                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n                # -----------------\n                #  Train Generator\n                # -----------------\n\n                # Train the generators\n                g_loss = self.combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_B])\n\n                elapsed_time = datetime.datetime.now() - start_time\n                # Plot the progress\n                print (""[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %f] time: %s"" % (epoch, epochs,\n                                                                        batch_i, self.data_loader.n_batches,\n                                                                        d_loss[0], 100*d_loss[1],\n                                                                        g_loss[0],\n                                                                        elapsed_time))\n\n                # If at save interval => save generated image samples\n                if batch_i % sample_interval == 0:\n                    self.sample_images(epoch, batch_i)\n\n    def sample_images(self, epoch, batch_i):\n        os.makedirs(\'images/%s\' % self.dataset_name, exist_ok=True)\n        r, c = 3, 3\n\n        imgs_A, imgs_B = self.data_loader.load_data(batch_size=3, is_testing=True)\n        fake_B = self.generator.predict(imgs_A)\n\n        gen_imgs = np.concatenate([imgs_A, fake_B, imgs_B])\n\n        # Rescale images 0 - 1\n        gen_imgs = 0.5 * gen_imgs + 0.5\n\n        titles = [\'Condition\', \'Generated\', \'Original\']\n        fig, axs = plt.subplots(r, c, figsize=(15,15))\n        cnt = 0\n        for i in range(r):\n            for j in range(c):\n                axs[i,j].imshow(gen_imgs[cnt][:,:,0], cmap=\'gray\')\n                axs[i, j].set_title(titles[i])\n                axs[i,j].axis(\'off\')\n                cnt += 1\n        fig.savefig(""images/%s/%d_%d.png"" % (self.dataset_name, epoch, batch_i))\n        plt.close()\n\nif __name__ == \'__main__\':\n    gan = Pix2Pix()\n    gan.train(epochs=5, batch_size=1, sample_interval=200)\n'"
projects/midcurvenn/misc/main_cnn_encoderdecoder.py,10,"b'from prepare_data import get_training_data\nfrom prepare_plots import plot_results\nfrom build_cnn_encoderdecoder_model import build_cnn_encoderdecoder_model\n\nimport numpy as np\n\nif __name__ == ""__main__"":\n    profile_pngs_objs, midcurve_pngs_objs = get_training_data(size = (128, 128))\n\n    profile_pngs_objs = np.asarray(profile_pngs_objs)    \n    midcurve_pngs_objs = np.asarray(midcurve_pngs_objs)    \n    \n    profile_pngs_objs = np.expand_dims(profile_pngs_objs, axis=-1)\n    midcurve_pngs_objs = np.expand_dims(midcurve_pngs_objs, axis=-1)\n\n    profile_pngs_objs = (profile_pngs_objs - 127.5)/127.5 #Normalize [-1, 1]\n    midcurve_pngs_objs = (midcurve_pngs_objs - 127.5)/127.5 #Normalize [-1, 1]\n    \n    x_coord = np.zeros(shape=(128, 128, 1))\n    y_coord = np.zeros(shape=(128, 128, 1))\n    for i in range(0, 128):\n        x_coord[:, i, 0] = i\n        y_coord[i, :, 0] = i\n    coords = np.append(x_coord, y_coord, axis=-1)\n    coords = (coords - 63.5)/63.5 #Normalize [-1, 1]\n    #coords = np.expand_dims(coords, axis=0)\n    \n    profile_pngs = []\n    \n    for i in range(len(profile_pngs_objs)):\n        profile_pngs.append(np.append(profile_pngs_objs[i], coords, axis=-1))\n    \n    profile_pngs = np.asarray(profile_pngs)\n    \n    original_imgs,decoded_imgs = build_cnn_encoderdecoder_model(profile_pngs, midcurve_pngs_objs)\n    plot_results(original_imgs,decoded_imgs)'"
projects/midcurvenn/misc/main_denoiser_encoderdecoder.py,0,"b'from prepare_data import get_training_data\nfrom prepare_plots import plot_results\nfrom build_simple_encoderdecoder_w_denoiser_model import simple_encoderdecoder_w_denoiser\n\nif __name__ == ""__main__"":\n    profile_pngs_objs, midcurve_pngs_objs = get_training_data()\n\tendec = simple_encoderdecoder_w_denoiser()\n    original_imgs,decoded_imgs = endec.simple_encoderdecoder_w_denoiser(profile_pngs_objs, midcurve_pngs_objs)\n    plot_results(original_imgs,decoded_imgs)\n'"
projects/midcurvenn/misc/main_dense_encoderdecoder.py,0,"b'from prepare_data import get_training_data\nfrom prepare_plots import plot_results\nfrom build_dense_encoderdecoder_model import dense_encoderdecoder\nimport random\n\nif __name__ == ""__main__"":\n    profile_gray_objs, midcurve_gray_objs = get_training_data()\n    endec = dense_encoderdecoder()\n    endec.train(profile_gray_objs, midcurve_gray_objs)\n    \n    test_gray_images = random.sample(profile_gray_objs,5)\n    original_profile_imgs,predicted_midcurve_imgs = endec.predict(test_gray_images)\n    plot_results(original_profile_imgs,predicted_midcurve_imgs)\n'"
projects/midcurvenn/misc/main_pix2pix.py,0,"b'from keras_gan_pix2pix import Pix2Pix\n\nif __name__ == ""__main__"":\n    datasetpath = ""data/pix2pix/""\n    datasetname = ""pix2pix""\n    rowpixels = 256\n    colpixles = 256\n    colorchannels = 1\n    gan = Pix2Pix()\n    gan.train(epochs=5, batch_size=1, sample_interval=200)\n'"
projects/midcurvenn/misc/main_simple_encoderdecoder.py,0,"b'from prepare_data import get_training_data\nfrom prepare_plots import plot_results\nfrom build_simple_encoderdecoder_model import simple_encoderdecoder\nimport random\n\nif __name__ == ""__main__"":\n    profile_gray_objs, midcurve_gray_objs = get_training_data()\n    endec = simple_encoderdecoder()\n    endec.train(profile_gray_objs, midcurve_gray_objs)\n    \n    test_gray_images = random.sample(profile_gray_objs,5)\n    original_profile_imgs,predicted_midcurve_imgs = endec.predict(test_gray_images)\n    plot_results(original_profile_imgs,predicted_midcurve_imgs)\n'"
projects/midcurvenn/misc/prepare_data.py,9,"b'""""""\n    Prepare Data: populating input images from raw profile data\n    Takes raw data from ""data/raw/*"" files for both, profile shape (shape.dat) as well as midcurve shape (shape.mid)\n    Generates raster image files from svg (simple vector graphics)\n    Multiple variations are populated using image transformations.\n    These images become input for further modeling (stored in ""data/input/*"")\n""""""\nfrom keras.preprocessing.image import img_to_array, load_img, array_to_img\nfrom random import shuffle\nimport PIL\nimport PIL.ImageOps\nimport json\nimport numpy as np\nimport os\nimport shutil\nimport sys\nnp.set_printoptions(threshold=sys.maxsize)\n\n\nraw_data_folder = ""data/raw""\ninput_data_folder = ""data/input""\npix2pix_data_folder = ""data/pix2pix/datasets/pix2pix""\n\ndef combine_images(imga, imgb):\n    """"""\n    Combines two color image ndarrays side-by-side.\n    Ref: https://stackoverflow.com/questions/30227466/combine-several-images-horizontally-with-python\n    """"""\n    ha,wa = imga.shape[:2]\n    hb,wb = imgb.shape[:2]\n    max_height = np.max([ha, hb])\n    total_width = wa+wb\n    new_img = np.zeros(shape=(max_height, total_width))\n    new_img[:ha,:wa]=imga\n    new_img[:hb,wa:wa+wb]=imgb\n    return new_img    \n#     images = map(Image.open, [imga,imgb])\n#     widths, heights = zip(*(i.size for i in images))\n#     total_width = sum(widths)\n#     max_height = max(heights)\n# \n#     new_im = Image.new(\'RGB\', (total_width, max_height))\n# \n#     x_offset = 0\n#     for im in images:\n#         new_im.paste(im, (x_offset,0))\n#         x_offset += im.size[0]    \n#     return new_im\n    \ndef generate_pix2pix_dataset(inputdatafolder = input_data_folder, pix2pixdatafolder=pix2pix_data_folder):\n\n    profile_pngs,midcurve_pngs = read_input_image_pairs(inputdatafolder)\n\n    profile_pngs_objs = [img_to_array(load_img(f, color_mode=\'rgba\', target_size=(256, 256))) for f in profile_pngs ]\n    midcurve_pngs_objs = [img_to_array(load_img(f, color_mode=\'rgba\', target_size=(256, 256))) for f in midcurve_pngs ]\n    \n#     combo_pngs_objs = np.array([x.reshape((1,) + x.shape) for x in combo_pngs_objs])\n    profile_pngs_gray_objs = [x[:,:,3] for x in profile_pngs_objs]\n    midcurve_pngs_gray_objs = [x[:,:,3] for x in midcurve_pngs_objs]\n    \n#     combo_pngs_gray_objs = [np.where(x>128, 0, 1) for x in combo_pngs_gray_objs]\n        \n    combo_pngs = [combine_images(p,m) for p,m in zip(profile_pngs_gray_objs,midcurve_pngs_gray_objs)]\n\n    # shufle them\n    shuffle(combo_pngs)\n    train_size = int(len(combo_pngs)*0.6)\n    val_size = int(len(combo_pngs)*0.2)\n    \n    train_combo_files = combo_pngs[:train_size]\n    val_combo_files = combo_pngs[train_size:train_size+val_size]\n    test_combo_files = combo_pngs[train_size+val_size:]\n\n    if os.path.exists(pix2pixdatafolder):\n        shutil.rmtree(pix2pixdatafolder,ignore_errors=True)\n        \n    os.makedirs(pix2pixdatafolder)\n    os.makedirs(pix2pixdatafolder+""/train"")\n    os.makedirs(pix2pixdatafolder+""/val"")\n    os.makedirs(pix2pixdatafolder+""/test"")\n\n    # SAVE into 3 dirs\n    for i, arr in enumerate(train_combo_files):\n        img = PIL.Image.fromarray(arr.astype(\'uint8\'))\n        img = PIL.ImageOps.invert(img)\n        filename = pix2pixdatafolder+""/train/"" + str(i) + "".jpg""\n        img.save(filename)\n        \n    for i, arr in enumerate(val_combo_files):\n        img = PIL.Image.fromarray(arr.astype(\'uint8\'))\n        img = PIL.ImageOps.invert(img)\n        filename = pix2pixdatafolder+""/val/"" + str(i) + "".jpg""\n        img.save(filename)\n\n    for i, arr in enumerate(test_combo_files):\n        img = PIL.Image.fromarray(arr.astype(\'uint8\'))\n        img = PIL.ImageOps.invert(img)\n        filename = pix2pixdatafolder+""/test/"" + str(i) + "".jpg""\n        img.save(filename)\n            \n    return train_combo_files, val_combo_files,test_combo_files \n    \ndef get_training_data(datafolder = input_data_folder, size=(100, 100)):\n    profile_pngs,midcurve_pngs = read_input_image_pairs(datafolder)\n    \n    profile_pngs_objs = [img_to_array(load_img(f, color_mode=\'rgba\', target_size=size)) for f in profile_pngs ]\n    midcurve_pngs_objs = [img_to_array(load_img(f, color_mode=\'rgba\', target_size=size)) for f in midcurve_pngs]\n\n#     profile_pngs_objs = np.array([x.reshape((1,) + x.shape) for x in profile_pngs_objs])\n#     midcurve_pngs_objs = np.array([x.reshape((1,) + x.shape) for x in midcurve_pngs_objs])\n\n    profile_pngs_gray_objs = [x[:,:,3] for x in profile_pngs_objs]\n    midcurve_pngs_gray_objs =[x[:,:,3] for x in midcurve_pngs_objs]\n    \n#     profile_pngs_gray_objs = [np.where(x>128, 0, 1) for x in profile_pngs_gray_objs]\n#     midcurve_pngs_gray_objs =[np.where(x>128, 0, 1) for x in midcurve_pngs_gray_objs]\n        \n    # shufle them\n    zipped_profiles_midcurves = [(p,m) for p,m in zip(profile_pngs_gray_objs,midcurve_pngs_gray_objs)]\n    shuffle(zipped_profiles_midcurves)\n    profile_pngs_gray_objs, midcurve_pngs_gray_objs = zip(*zipped_profiles_midcurves)\n    \n    return profile_pngs_gray_objs, midcurve_pngs_gray_objs\n\ndef get_profile_dict(shapename,profiles_dict_list):\n    for i in profiles_dict_list:\n        if i[\'ShapeName\'] == shapename:\n            return i\n    profile_dict = {}\n    profile_dict[\'ShapeName\'] = shapename\n    return profile_dict\n\ndef read_dat_files(datafolder=raw_data_folder):\n    profiles_dict_list = []\n    for file in os.listdir(datafolder):\n        if os.path.isdir(os.path.join(datafolder, file)):\n            continue\n        filename = file.split(""."")[0]\n        profile_dict = get_profile_dict(filename,profiles_dict_list)        \n        if file.endswith("".dat""):\n            with open(os.path.join(datafolder, file)) as f:\n                profile_dict[\'Profile\'] = [tuple(map(float, i.split(\'\\t\'))) for i in f]  \n        if file.endswith("".mid""):\n            with open(os.path.join(datafolder, file)) as f:\n                profile_dict[\'Midcurve\'] = [tuple(map(float, i.split(\'\\t\'))) for i in f]\n                                \n        profiles_dict_list.append(profile_dict)\n    return profiles_dict_list\n\nimport drawSvg as draw\ndef create_image_file(fieldname,profile_dict,datafolder=input_data_folder,imgsize=100, isOpenClose=True):\n    d = draw.Drawing(imgsize, imgsize, origin=\'center\')\n    profilepoints = []\n    for tpl in profile_dict[fieldname]:\n        profilepoints.append(tpl[0])\n        profilepoints.append(tpl[1])\n    d.append(draw.Lines(profilepoints[0],profilepoints[1],*profilepoints,close=isOpenClose,fill=\'none\',stroke=\'black\'))\n    \n    shape = profile_dict[\'ShapeName\']\n#     d.saveSvg(datafolder+""/""+shape+\'.svg\')\n    d.savePng(datafolder+""/""+shape+\'_\'+fieldname+\'.png\')\n\ndef get_original_png_files(datafolder=input_data_folder):\n    pngfilenames = []\n    for file in os.listdir(datafolder):\n        fullpath = os.path.join(datafolder, file)\n        if os.path.isdir(fullpath):\n            continue\n        if file.endswith("".png"") and file.find(""_rotated_"") == -1 and file.find(""_translated_"")==-1 and file.find(""_mirrored_"")==-1:\n            pngfilenames.append(fullpath)\n    return pngfilenames\n\nfrom PIL import Image\ndef rotate_images(pngfilenames, angle=90):\n    for fullpath in pngfilenames:\n        picture= Image.open(fullpath)\n        newfilename = fullpath.replace("".png"", ""_rotated_""+str(angle)+"".png"")\n        picture.rotate(angle).save(newfilename)\n\ndef mirror_images(pngfilenames, mode=PIL.Image.TRANSPOSE):\n    mirrored_filenames = []\n    for fullpath in pngfilenames:\n        picture= Image.open(fullpath)\n        newfilename = fullpath.replace("".png"", ""_mirrored_""+str(mode)+"".png"")\n        picture.transpose(mode).save(newfilename)\n        mirrored_filenames.append(newfilename)\n    return mirrored_filenames\n        \ndef translate_images(pngfilenames, dx=1,dy=1):\n    for fullpath in pngfilenames:\n        picture= Image.open(fullpath)\n        x_shift = dx\n        y_shift = dy\n        a = 1\n        b = 0\n        c = x_shift #left/right (i.e. 5/-5)\n        d = 0\n        e = 1\n        f = y_shift #up/down (i.e. 5/-5)\n        translate = picture.transform(picture.size, Image.AFFINE, (a, b, c, d, e, f))\n#         # Calculate the size after cropping\n#         size = (translate.size[0] - x_shift, translate.size[1] - y_shift)\n#         # Crop to the desired size\n#         translate = translate.transform(size, Image.EXTENT, (0, 0, size[0], size[1]))\n        newfilename = fullpath.replace("".png"", ""_translated_""+str(dx)+""_""+str(dy)+"".png"")\n        translate.save(newfilename)\n        \ndef read_input_image_pairs(datafolder=input_data_folder):\n    profile_pngs = []\n    midcurve_pngs = []\n    for file in os.listdir(datafolder):\n        fullpath = os.path.join(datafolder, file)\n        if os.path.isdir(fullpath):\n            continue\n        if file.endswith("".png""):\n            if file.find(""Profile"") != -1:\n                profile_pngs.append(fullpath)\n            if file.find(""Midcurve"") != -1:\n                midcurve_pngs.append(fullpath)\n    profile_pngs = sorted(profile_pngs)\n    midcurve_pngs = sorted(midcurve_pngs)\n    return profile_pngs,midcurve_pngs\n    \ndef generate_images(datafolder = input_data_folder):\n    \n    if not os.path.exists(datafolder):\n        os.makedirs(datafolder)    \n    else:    \n        for file in os.listdir(datafolder):\n            if file.endswith("".png"") and (file.find(""_rotated_"") != -1 or file.find(""_translated_"") !=-1):\n                print(""files already present, not generating..."")\n                return\n                \n    print(""transformed files not present, generating..."")\n    profiles_dict_list = read_dat_files()\n        \n    for profile_dict in profiles_dict_list:\n        create_image_file(\'Profile\',profile_dict,datafolder,100,True)\n        create_image_file(\'Midcurve\',profile_dict,datafolder,100,False)\n        \n    pngfilenames = get_original_png_files(datafolder)\n    mirrored_filenames_left_right = mirror_images(pngfilenames, PIL.Image.FLIP_LEFT_RIGHT)\n    mirrored_filenames_top_bottom = mirror_images(pngfilenames, PIL.Image.FLIP_TOP_BOTTOM)\n    mirrored_filenames_transpose = mirror_images(pngfilenames, PIL.Image.TRANSPOSE)\n    \n    files_list_list = [pngfilenames,mirrored_filenames_left_right,mirrored_filenames_top_bottom,mirrored_filenames_transpose]\n    for filelist in files_list_list:\n        for angle in range(30,360,30):\n            rotate_images(filelist,angle)\n            \n        for dx in range(5,21,5):\n            for dy in range(5,21,5):\n                translate_images(filelist,dx,-dy)\n            \n        \nif __name__ == ""__main__"":\n    # generate_images()\n    # profile_pngs,midcurve_pngs = read_input_image_pairs()\n    generate_pix2pix_dataset()\n\n    \n'"
projects/midcurvenn/misc/prepare_plots.py,0,"b'import matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.use(\'TKAgg\')\n\ndef plot_results(original_imgs,computed_imgs):\n    n = len(original_imgs)#10  # how many digits we will display\n    plt.figure(figsize=(20, 4))\n    for i in range(n):\n        # display original\n        ax = plt.subplot(2, n, i + 1)\n        plt.imshow(original_imgs[i].reshape(100, 100),cmap=\'gray_r\')\n#         plt.gray()\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n    \n        # display reconstruction\n        ax = plt.subplot(2, n, i + 1 + n)\n        plt.imshow(computed_imgs[i].reshape(100, 100),cmap=\'gray_r\')\n#         plt.gray()\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n    plt.show()\n\nif __name__ == ""__main__"":\n    pass'"
projects/midcurvenn/misc/reference_drawSvg_test.py,0,"b""# Ref: http://www.petercollingridge.co.uk/tools/drawsvgpy/\n# \nimport drawSvg as draw\n\nd = draw.Drawing(200, 100, origin='center')\n\nd.append(draw.Lines(-80, -45,\n                    70, -49,\n                    95, 49,\n                    -90, 40,\n                    close=False,\n            fill='#eeee00',\n            stroke='black'))\n\nd.append(draw.Rectangle(0,0,40,50, fill='#1248ff'))\nd.append(draw.Circle(-40, -10, 30,\n            fill='red', stroke_width=2, stroke='black'))\n\np = draw.Path(stroke_width=2, stroke='green',\n              fill='black', fill_opacity=0.5)\np.M(-30,5)  # Start path at point (-30, 5)\np.l(60,30)  # Draw line to (60, 30)\np.h(-70)    # Draw horizontal line to x=-70\np.Z()       # Draw line to start\nd.append(p)\n\nd.append(draw.ArcLine(60,-20,20,60,270,\n            stroke='red', stroke_width=5, fill='red', fill_opacity=0.2))\nd.append(draw.Arc(60,-20,20,60,270,cw=False,\n            stroke='green', stroke_width=3, fill='none'))\nd.append(draw.Arc(60,-20,20,270,60,cw=True,\n            stroke='blue', stroke_width=1, fill='black', fill_opacity=0.3))\n\nd.setPixelScale(2)  # Set number of pixels per geometry unit\n#d.setRenderSize(400,200)  # Alternative to setPixelScale\nd.saveSvg('example.svg')\nd.savePng('example.png')\n\n# Display in iPython notebook\nd.rasterize()  # Display as PNG\nprint(d)  # Display as SVG\n"""
projects/midcurvenn/misc/reference_keras_test.py,3,"b'from keras.preprocessing.image import load_img,img_to_array\nimport numpy as np\nimport sys\nnp.set_printoptions(threshold=sys.maxsize)\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.use(\'TKAgg\')\n\nimg_path = ""output/L_Midcurve.png""\nimg = load_img(img_path, color_mode=\'rgba\', target_size=(100, 100))\nprint(type(img)) # <class \'PIL.Image.Image\'>\nimg = img.rotate(45)\nx = img_to_array(img)\nprint(type(x)) # <class \'numpy.ndarray\'>\nprint(x.shape) # (100, 100, 4)\nx = x[:,:,3]\nprint(x) \nx = np.where(x>128, 0, 1)\nprint(x) \n\nplt.imshow(x) # x/255.\nplt.show()\n\n# img = img.convert(\'1\')\n# x = img_to_array(img)\n# print(type(x)) # <class \'numpy.ndarray\'>\n# print(x.shape) # (100, 100, 1)\n# plt.imshow(x/255.) # doesnt work either\n# plt.show() # TypeError: Invalid dimensions for image data\n\n# \n# # import numpy as np\n# # img = np.random.rand(224,224,3)\n# # plt.imshow(img)\n# # plt.show()\n# \n\n# # load and display an image with Matplotlib\n# from PIL import Image\n#  \n# def black_and_white(input_image_path,output_image_path):\n#     color_image = Image.open(input_image_path)\n#     bw = color_image.convert(\'1\')\n#     bw.save(output_image_path)\n#  \n# if __name__ == \'__main__\':  \n#     black_and_white(\'output/L_Profile.png\',\n#         \'output/L_bw_Profile.png\')'"
projects/midcurvenn/misc/reference_mnist_autoencoder.py,2,"b'# Reference: Building Autoencoders in Keras\n# https://blog.keras.io/building-autoencoders-in-keras.html\n\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\n\n# this is the size of our encoded representations\nencoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n\n# this is our input placeholder\ninput_img = Input(shape=(784,))\n\n# ""encoded"" is the encoded representation of the input\nencoded = Dense(encoding_dim, activation=\'relu\')(input_img)\n# Let\'s also create a separate encoder model:\n# this model maps an input to its encoded representation\nencoder = Model(input_img, encoded)\n# create a placeholder for an encoded (32-dimensional) input\nencoded_input = Input(shape=(encoding_dim,))\n\n# ""decoded"" is the lossy reconstruction of the input\ndecoded = Dense(784, activation=\'sigmoid\')(encoded)\n# this model maps an input to its encoded representation\n# this model maps an input to its reconstruction\nautoencoder = Model(input_img, decoded)\n# retrieve the last layer of the autoencoder model\ndecoder_layer = autoencoder.layers[-1]\n# create the decoder model\ndecoder = Model(encoded_input, decoder_layer(encoded_input))\n\nautoencoder.compile(optimizer=\'adadelta\', loss=\'binary_crossentropy\')\n\nfrom keras.datasets import mnist\nimport numpy as np\n(x_train, _), (x_test, _) = mnist.load_data()\n\nx_train = x_train.astype(\'float32\') / 255.\nx_test = x_test.astype(\'float32\') / 255.\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\nprint(x_train.shape)\nprint(x_test.shape)\n\nautoencoder.fit(x_train, x_train,\n                epochs=5,\n                batch_size=256,\n                shuffle=True,\n                validation_data=(x_test, x_test))\n\n# encode and decode some digits\n# note that we take them from the *test* set\nencoded_imgs = encoder.predict(x_test)\ndecoded_imgs = decoder.predict(encoded_imgs)\n\n# use Matplotlib (don\'t ask)\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.use(\'TKAgg\')\n\nn = 10  # how many digits we will display\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # display original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # display reconstruction\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()'"
projects/nltk/seinfeld/seinfeld.py,0,"b'# This Python 3 environment comes with many helpful analytics libraries installed\r\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\r\n# For example, here\'s several helpful packages to load in \r\n\r\nimport os\r\nimport numpy as np # linear algebra\r\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n\r\nimport nltk\r\nfrom nltk.corpus import PlaintextCorpusReader\r\n\r\n# Input data files are available in the ""../input/"" directory.\r\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\r\n\r\n\r\nraw_file = \'scripts.csv\'\r\nraw_file_na = \'scripts_nonascii.csv\'\r\ncorpus_root = \'C://Python36//\'\r\nfname = \'dialogues_corpus.txt\'\r\n\r\n\'\'\'\r\n# CHECK ENCODING\r\nimport codecs\r\nimport unicodedata\r\nimport chardet\r\nprint(chardet.detect(open(raw_file_na, ""rb"").read()))\r\n# {\'confidence\': 0.9690625, \'encoding\': \'utf-8\'}\r\n# {\'encoding\': None, \'confidence\': 0.0, \'language\': None}\r\n\r\n# lines = codecs.open(raw_file_na, \'r\', encoding=\'utf-8\').readlines()\r\nlines = codecs.open(raw_file_na, \'r\', encoding=\'utf-8\').readlines()\r\nprint(len(lines))\r\n\'\'\'\r\n\r\n# DATA ANALYSIS\r\n# raw_data = pd.read_csv(raw_file, sep=\',\')\r\nraw_data = pd.read_csv(raw_file_na, sep=\',\', encoding=\'utf-8\')\r\n\r\nprint(raw_data.head())\r\n\'\'\'\r\n   Unnamed: 0 Character   ...      SEID  Season\r\n0           0     JERRY   ...    S01E01     1.0\r\n1           1     JERRY   ...    S01E01     1.0\r\n2           2    GEORGE   ...    S01E01     1.0\r\n3           3     JERRY   ...    S01E01     1.0\r\n4           4    GEORGE   ...    S01E01     1.0\r\n\r\n[5 rows x 6 columns]\r\n\'\'\'\r\n\r\nprint(raw_data.tail(1))\r\n\'\'\'\r\n       Unnamed: 0 Character Dialogue  EpisodeNo    SEID  Season\r\n54616       54616   PRAVEEN     \'??\'       24.0  S09E23     9.0\r\n\'\'\'\r\n\r\n\r\nprint(raw_data.describe())\r\n\'\'\'\r\n         Unnamed: 0     EpisodeNo        Season\r\ncount  54616.000000  54616.000000  54616.000000\r\nmean   27307.500000     11.360737      5.677732\r\nstd    15766.425488      6.757064      2.243955\r\nmin        0.000000      1.000000      1.000000\r\n25%    13653.750000      5.000000      4.000000\r\n50%    27307.500000     11.000000      6.000000\r\n75%    40961.250000     17.000000      8.000000\r\nmax    54615.000000     24.000000      9.000000\r\n\'\'\'\r\n\r\n# sample count of George\r\ncount = 0\r\nfor x in raw_data[\'Dialogue\']:\r\n    if \'George\' in str(x).split():\r\n        count+=1\r\nprint(count)\r\n# 551\r\n\r\ndialogues = raw_data[\'Dialogue\']\r\nprint(type(dialogues))\r\n# <class \'pandas.core.series.Series\'>\r\n\r\n\r\n# Create Corpus!\r\nwith open(fname, \'w\', encoding=\'utf-8\') as f:\r\n    for d in dialogues:\r\n        f.write(str(d)+\'\\n\')\r\n\r\nfilelist = PlaintextCorpusReader(corpus_root, fname)\r\n\r\nprint(filelist.fileids())\r\n# [\'dialogues_corpus.txt\']\r\n\r\nd_words = filelist.words(fname)\r\n\r\nprint(d_words)\r\n# [\'0\', \'Do\', \'you\', \'know\', \'what\', \'this\', \'is\', \'all\', ...]\r\n\r\nprint(type(d_words))\r\n# <class \'nltk.corpus.reader.util.StreamBackedCorpusView\'>\r\n\r\nfrom nltk import word_tokenize as wt\r\n\r\ndef freq_words(file, min=1, num=10):\r\n    text = open(file).read()\r\n    tokens = wt(text)\r\n    print(len(tokens))\r\n    # 734989\r\n    print(len(set(tokens)))\r\n    freqdist = nltk.FreqDist(t for t in tokens if len(t) <= min)\r\n    print(len(freqdist))\r\n    # 4540\r\n    # freqdist.plot()\r\n    return freqdist.keys()\r\n\r\n# fw = freq_words(fname, 4, 10)\r\n\r\n# print(fw)\r\n\r\nd_words = open(fname).read()\r\ntokens = wt(d_words)\r\n\r\nprint(tokens[100:113])\r\n# [\'on\', \'an\', \'imaginary\', \'phone\', \')\', \'Did\', \'you\', \'ring\', \'?\', \',\', \'I\', \'cant\', \'find-\']\r\n\r\ntext = nltk.Text(tokens)\r\nprint(text)\r\n# <Text: Do you know what this is all about...>\r\n\r\nprint(type(text))\r\n# <class \'nltk.text.Text\'>\r\n\r\nprint(text.collocations())\r\n\'\'\'\r\nn\'t know; n\'t want; \'ve got; New York; n\'t even; last night; n\'t\r\nbelieve; would n\'t; \'ll tell; \'re gon; n\'t think; \'ll see; Jerry\r\nSeinfeld; could n\'t; whole thing; \'re going; Uncle Leo; years ago; \'ve\r\nnever; Mr. Pitt\r\nNone\r\n\'\'\'\r\n\r\n# FREQUENCY DISTRIBUTION \r\nfrom nltk import FreqDist\r\nfd1 = FreqDist(text)\r\n# fd1.plot(10, cumulative=True)\r\n\r\n# TAGGING\r\ndefault_tagger = nltk.DefaultTagger(\'NN\')\r\ndtags = default_tagger.tag(tokens)\r\n\r\nfor item in dtags:\r\n    if item[1] != \'NN\':\r\n        print(item)\r\n\r\n\'\'\'\r\n# REGULAR EXPRESSION PATTERNS\r\n\r\npatterns = [\r\n... (r\'.*ing$\', \'VBG\'), # gerunds\r\n... (r\'.*ed$\', \'VBD\'), # simple past\r\n... (r\'.*es$\', \'VBZ\'), # 3rd singular present\r\n... (r\'.*ould$\', \'MD\'), # modals\r\n... (r\'.*\\\'s$\', \'NN$\'), # possessive nouns\r\n... (r\'.*s$\', \'NNS\'), # plural nouns\r\n... (r\'^-?[0-9]+(.[0-9]+)?$\', \'CD\'), # cardinal numbers\r\n... (r\'.*\', \'NN\') # nouns (default)\r\n... ]\r\n\'\'\'\r\n\r\n# patterns = [(r\'.*ing$\', \'VBG\'), (r\'.*ed$\', \'VBD\'), (r\'.*es$\', \'VBZ\'), (r\'.*ould$\', \'MD\'), \\\r\n# (r\'.*\\\'s$\', \'NN$\'), (r\'.*s$\', \'NNS\'), (r\'^-?[0-9]+(.[0-9]+)?$\', \'CD\'), (r\'.*\', \'NN\')]\r\n\r\npatterns = [\r\n    (r\'.*ing$\', \'VBG\'),               # gerunds\r\n    (r\'.*ed$\', \'VBD\'),                # simple past\r\n    (r\'.*es$\', \'VBZ\'),                # 3rd singular present\r\n    (r\'.*ould$\', \'MD\'),               # modals\r\n    (r\'.*\\\'s$\', \'NN$\'),               # possessive nouns\r\n    (r\'.*s$\', \'NNS\'),                 # plural nouns\r\n    (r\'^-?[0-9]+(.[0-9]+)?$\', \'CD\'),  # cardinal numbers\r\n    (r\'(The|the|A|a|An|an)$\', \'AT\'),   # articles \r\n    (r\'.*able$\', \'JJ\'),                # adjectives \r\n    (r\'.*ness$\', \'NN\'),                # nouns formed from adjectives\r\n    (r\'.*ly$\', \'RB\'),                  # adverbs\r\n    (r\'(He|he|She|she|It|it|I|me|Me|You|you)$\', \'PRP\'), # pronouns\r\n    (r\'(His|his|Her|her|Its|its)$\', \'PRP$\'),    # possesive\r\n    (r\'(my|Your|your|Yours|yours)$\', \'PRP$\'),   # possesive\r\n    (r\'(on|On|in|In|at|At|since|Since)$\', \'IN\'),# time prepopsitions\r\n    (r\'(for|For|ago|Ago|before|Before)$\', \'IN\'),# time prepopsitions\r\n    (r\'(till|Till|until|Until)$\', \'IN\'),        # time prepopsitions\r\n    (r\'(by|By|beside|Beside)$\', \'IN\'),          # space prepopsitions\r\n    (r\'(under|Under|below|Below)$\', \'IN\'),      # space prepopsitions\r\n    (r\'(over|Over|above|Above)$\', \'IN\'),        # space prepopsitions\r\n    (r\'(across|Across|through|Through)$\', \'IN\'),# space prepopsitions\r\n    (r\'(into|Into|towards|Towards)$\', \'IN\'),    # space prepopsitions\r\n    (r\'(onto|Onto|from|From)$\', \'IN\'),          # space prepopsitions    \r\n    (r\'\\.$\',\'.\'), (r\'\\,$\',\',\'), (r\'\\?$\',\'?\'),    # fullstop, comma, Qmark\r\n    (r\'\\($\',\'(\'), (r\'\\)$\',\')\'),             # round brackets\r\n    (r\'\\[$\',\'[\'), (r\'\\]$\',\']\'),             # square brackets\r\n    (r\'(Sam)$\', \'NAM\'),\r\n    # WARNING : Put the default value in the end\r\n    (r\'.*\', \'NN\')                      # nouns (default)\r\n    ]\r\n\r\nregexp_tagger = nltk.RegexpTagger(patterns)\r\n\r\nrtags = regexp_tagger.tag(tokens[:1000])\r\nprint(rtags)\r\n\r\n\'\'\'\r\n# tokens[:100]\r\n[(\'Do\', \'NN\'), (\'you\', \'NN\'), (\'know\', \'NN\'), (\'what\', \'NN\'), (\'this\', \'NNS\'), (\'is\', \'NNS\r\n\'), (\'all\', \'NN\'), (\'about\', \'NN\'), (\'?\', \'NN\'), (\'Do\', \'NN\'), (\'you\', \'NN\'), (\'know\', \'NN\r\n\'), (\',\', \'NN\'), (\'why\', \'NN\'), (\'were\', \'NN\'), (\'here\', \'NN\'), (\'?\', \'NN\'), (\'To\', \'NN\'),\r\n (\'be\', \'NN\'), (\'out\', \'NN\'), (\',\', \'NN\'), (\'this\', \'NNS\'), (\'is\', \'NNS\'), (\'out\', \'NN\'),\r\n(\'...\', \'NN\'), (\'and\', \'NN\'), (\'out\', \'NN\'), (\'is\', \'NNS\'), (\'one\', \'NN\'), (\'of\', \'NN\'), (\r\n\'the\', \'NN\'), (\'single\', \'NN\'), (\'most\', \'NN\'), (\'enjoyable\', \'NN\'), (\'experiences\', \'VBZ\'\r\n), (\'of\', \'NN\'), (\'life\', \'NN\'), (\'.\', \'NN\'), (\'People\', \'NN\'), (\'...\', \'NN\'), (\'did\', \'NN\r\n\'), (\'you\', \'NN\'), (\'ever\', \'NN\'), (\'hear\', \'NN\'), (\'people\', \'NN\'), (\'talking\', \'VBG\'), (\r\n\'about\', \'NN\'), (\'We\', \'NN\'), (\'should\', \'MD\'), (\'go\', \'NN\'), (\'out\', \'NN\'), (\'?\', \'NN\'),\r\n(\'This\', \'NNS\'), (\'is\', \'NNS\'), (\'what\', \'NN\'), (\'theyre\', \'NN\'), (\'talking\', \'VBG\'), (\'ab\r\nout\', \'NN\'), (\'...\', \'NN\'), (\'this\', \'NNS\'), (\'whole\', \'NN\'), (\'thing\', \'VBG\'), (\',\', \'NN\'\r\n), (\'were\', \'NN\'), (\'all\', \'NN\'), (\'out\', \'NN\'), (\'now\', \'NN\'), (\',\', \'NN\'), (\'no\', \'NN\'),\r\n (\'one\', \'NN\'), (\'is\', \'NNS\'), (\'home\', \'NN\'), (\'.\', \'NN\'), (\'Not\', \'NN\'), (\'one\', \'NN\'),\r\n(\'person\', \'NN\'), (\'here\', \'NN\'), (\'is\', \'NNS\'), (\'home\', \'NN\'), (\',\', \'NN\'), (\'were\', \'NN\r\n\'), (\'all\', \'NN\'), (\'out\', \'NN\'), (\'!\', \'NN\'), (\'There\', \'NN\'), (\'are\', \'NN\'), (\'people\',\r\n\'NN\'), (\'tryin\', \'NN\'), (\'to\', \'NN\'), (\'find\', \'NN\'), (\'us\', \'NNS\'), (\',\', \'NN\'), (\'they\',\r\n \'NN\'), (\'dont\', \'NN\'), (\'know\', \'NN\'), (\'where\', \'NN\'), (\'we\', \'NN\'), (\'are\', \'NN\'), (\'.\'\r\n, \'NN\'), (\'(\', \'NN\')]\r\n\'\'\'\r\n\r\n\'\'\'\r\n# tokens[:1000] \r\n[(\'Do\', \'NN\'), (\'you\', \'PRP\'), (\'know\', \'NN\'), (\'what\', \'NN\'), (\'this\', \'NNS\'), (\'is\', \'NN\r\nS\'), (\'all\', \'NN\'), (\'about\', \'NN\'), (\'?\', \'?\'), (\'Do\', \'NN\'), (\'you\', \'PRP\'), (\'know\', \'N\r\nN\'), (\',\', \',\'), (\'why\', \'NN\'), (\'were\', \'NN\'), (\'here\', \'NN\'), (\'?\', \'?\'), (\'To\', \'NN\'),\r\n(\'be\', \'NN\'), (\'out\', \'NN\'), (\',\', \',\'), (\'this\', \'NNS\'), (\'is\', \'NNS\'), (\'out\', \'NN\'), (\'\r\n...\', \'NN\'), (\'and\', \'NN\'), (\'out\', \'NN\'), (\'is\', \'NNS\'), (\'one\', \'NN\'), (\'of\', \'NN\'), (\'t\r\nhe\', \'AT\'), (\'single\', \'NN\'), (\'most\', \'NN\'), (\'enjoyable\', \'JJ\'), (\'experiences\', \'VBZ\'),\r\n (\'of\', \'NN\'), (\'life\', \'NN\'), (\'.\', \'.\'), (\'People\', \'NN\'), (\'...\', \'NN\'), (\'did\', \'NN\'),\r\n (\'you\', \'PRP\'), (\'ever\', \'NN\'), (\'hear\', \'NN\'), (\'people\', \'NN\'), (\'talking\', \'VBG\'), (\'a\r\nbout\', \'NN\'), (\'We\', \'NN\'), (\'should\', \'MD\'), (\'go\', \'NN\'), (\'out\', \'NN\'), (\'?\', \'?\'), (\'T\r\nhis\', \'NNS\'), (\'is\', \'NNS\'), (\'what\', \'NN\'), (\'theyre\', \'NN\'), (\'talking\', \'VBG\'), (\'about\r\n....\r\n(\'you\', \'PRP\'), (\'to\', \'NN\'), (\'analyze\', \'NN\'), (\'a\', \'AT\'), (\'hypothetical\', \'NN\'), (\'ph\r\none\', \'NN\'), (\'call\', \'NN\'), (\',\', \',\'), (\'you\', \'PRP\'), (\'know\', \'NN\'), (\',\', \',\'), (\'fro\r\nm\', \'IN\'), (\'a\', \'AT\'), (\'female\', \'NN\'), (\'point\', \'NN\'), (\'of\', \'NN\'), (\'view\', \'NN\'), (\r\n\'.\', \'.\'), (\'(\', \'(\'), (\'to\', \'NN\'), (\'Claire\', \'NN\'), (\')\', \')\'), (\'Now\', \'NN\'), (\',\', \',\r\n\'), (\'a\', \'AT\'), (\'woman\', \'NN\'), (\'calls\', \'NNS\'), (\'me\', \'PRP\'), (\',\', \',\'), (\'all\', \'NN\r\n\'), (\'right\', \'NN\'), (\'?\', \'?\'), (\'Uh\', \'NN\'), (\'huh\', \'NN\'), (\'.\', \'.\'), (\'She\', \'PRP\'),\r\n(\'says\', \'NNS\'), (\'she\', \'PRP\'), (\'has\', \'NNS\'), (\'to\', \'NN\'), (\'come\', \'NN\'), (\'to\', \'NN\'\r\n), (\'New\', \'NN\'), (\'York\', \'NN\'), (\'on\', \'IN\'), (\'business\', \'NNS\'), (\'...\', \'NN\'), (\'Oh\',\r\n \'NN\'), (\'you\', \'PRP\'), (\'are\', \'NN\'), (\'beautiful\', \'NN\'), (\'!\', \'NN\'), (\'...\', \'NN\'), (\'\r\nand\', \'NN\'), (\',\', \',\'), (\'and\', \'NN\'), (\'maybe\', \'NN\'), (\'shell\', \'NN\'), (\'see\', \'NN\'), (\r\n\'me\', \'PRP\'), (\'when\', \'NN\'), (\'she\', \'PRP\'), (\'gets\', \'NNS\'), (\'there\', \'NN\'), (\',\', \',\')\r\n, (\'does\', \'VBZ\'), (\'this\', \'NNS\'), (\'woman\', \'NN\'), (\'intend\', \'NN\'), (\'to\', \'NN\'), (\'spe\r\nnd\', \'NN\'), (\'time\', \'NN\'), (\'with\', \'NN\'), (\'me\', \'PRP\'), (\'?\', \'?\'), (\'Id\', \'NN\'), (\'hav\r\ne\', \'NN\'), (\'to\', \'NN\'), (\'say\', \'NN\'), (\',\', \',\'), (\'uuhh\', \'NN\'), (\',\', \',\'), (\'no\', \'NN\r\n\'), (\'.\', \'.\'), (\'NO\', \'NN\'), (\'.\', \'.\'), (\')\', \')\'), (\'To\', \'NN\')]\r\n\'\'\'\r\n\r\n\r\nrtags_eval = regexp_tagger.evaluate([rtags])\r\nprint(rtags_eval)\r\n# 1.0\r\n\r\n\r\nfrom nltk.corpus import brown\r\n\r\nbrown_tagged_sents = brown.tagged_sents(categories=\'news\')\r\nprint(type(brown_tagged_sents))\r\n# <class \'nltk.corpus.reader.util.ConcatenatedCorpusView\'>\r\n\r\nprint(brown_tagged_sents)\r\n\'\'\'\r\n[[(\'The\', \'AT\'), (\'Fulton\', \'NP-TL\'), (\'County\', \'NN-TL\'), (\'Grand\', \'JJ-TL\'), (\'Jury\', \'N\r\nN-TL\'), (\'said\', \'VBD\'), (\'Friday\', \'NR\'), (\'an\', \'AT\'), (\'investigation\', \'NN\'), (\'of\', \'\r\nIN\'), (""Atlanta\'s"", \'NP$\'), (\'recent\', \'JJ\'), (\'primary\', \'NN\'), (\'election\', \'NN\'), (\'pro\r\nduced\', \'VBD\'), (\'``\', \'``\'), (\'no\', \'AT\'), (\'evidence\', \'NN\'), (""\'\'"", ""\'\'""), (\'that\', \'CS\r\n\'), (\'any\', \'DTI\'), (\'irregularities\', \'NNS\'), (\'took\', \'VBD\'), (\'place\', \'NN\'), (\'.\', \'.\'\r\n)], [(\'The\', \'AT\'), (\'jury\', \'NN\'), (\'further\', \'RBR\'), (\'said\', \'VBD\'), (\'in\', \'IN\'), (\'t\r\nerm-end\', \'NN\'), (\'presentments\', \'NNS\'), (\'that\', \'CS\'), (\'the\', \'AT\'), (\'City\', \'NN-TL\')\r\n, (\'Executive\', \'JJ-TL\'), (\'Committee\', \'NN-TL\'), (\',\', \',\'), (\'which\', \'WDT\'), (\'had\', \'H\r\nVD\'), (\'over-all\', \'JJ\'), (\'charge\', \'NN\'), (\'of\', \'IN\'), (\'the\', \'AT\'), (\'election\', \'NN\'\r\n), (\',\', \',\'), (\'``\', \'``\'), (\'deserves\', \'VBZ\'), (\'the\', \'AT\'), (\'praise\', \'NN\'), (\'and\',\r\n \'CC\'), (\'thanks\', \'NNS\'), (\'of\', \'IN\'), (\'the\', \'AT\'), (\'City\', \'NN-TL\'), (\'of\', \'IN-TL\')\r\n, (\'Atlanta\', \'NP-TL\'), (""\'\'"", ""\'\'""), (\'for\', \'IN\'), (\'the\', \'AT\'), (\'manner\', \'NN\'), (\'in\r\n\', \'IN\'), (\'which\', \'WDT\'), (\'the\', \'AT\'), (\'election\', \'NN\'), (\'was\', \'BEDZ\'), (\'conducte\r\nd\', \'VBN\'), (\'.\', \'.\')], ...]\r\n\'\'\'\r\n\r\nbrown_sents = brown.sents(categories=\'news\')\r\nprint(type(brown_sents))\r\n# <class \'nltk.corpus.reader.util.ConcatenatedCorpusView\'>\r\n\r\nrtags = regexp_tagger.tag(brown_sents[3])\r\nprint(rtags)\r\n\'\'\'\r\n[(\'``\', \'NN\'), (\'Only\', \'NN\'), (\'a\', \'NN\'), (\'relative\', \'NN\'), (\'handful\', \'NN\'), (\'of\',\r\n\'NN\'), (\'such\', \'NN\'), (\'reports\', \'NNS\'), (\'was\', \'NNS\'), (\'received\', \'VBD\'), (""\'\'"", \'NN\r\n\'), (\',\', \'NN\'), (\'the\', \'NN\'), (\'jury\', \'NN\'), (\'said\', \'NN\'), (\',\', \'NN\'), (\'``\', \'NN\'),\r\n (\'considering\', \'VBG\'), (\'the\', \'NN\'), (\'widespread\', \'NN\'), (\'interest\', \'NN\'), (\'in\', \'\r\nNN\'), (\'the\', \'NN\'), (\'election\', \'NN\'), (\',\', \'NN\'), (\'the\', \'NN\'), (\'number\', \'NN\'), (\'o\r\nf\', \'NN\'), (\'voters\', \'NNS\'), (\'and\', \'NN\'), (\'the\', \'NN\'), (\'size\', \'NN\'), (\'of\', \'NN\'),\r\n(\'this\', \'NNS\'), (\'city\', \'NN\'), (""\'\'"", \'NN\'), (\'.\', \'NN\')]\r\n\'\'\'\r\n\r\n# AFTER UPDATING PATTERNS\r\n\'\'\'\r\n[(\'``\', \'NN\'), (\'Only\', \'RB\'), (\'a\', \'AT\'), (\'relative\', \'NN\'), (\'handful\', \'NN\'), (\'of\',\r\n\'NN\'), (\'such\', \'NN\'), (\'reports\', \'NNS\'), (\'was\', \'NNS\'), (\'received\', \'VBD\'), (""\'\'"", \'NN\r\n\'), (\',\', \',\'), (\'the\', \'AT\'), (\'jury\', \'NN\'), (\'said\', \'NN\'), (\',\', \',\'), (\'``\', \'NN\'), (\r\n\'considering\', \'VBG\'), (\'the\', \'AT\'), (\'widespread\', \'NN\'), (\'interest\', \'NN\'), (\'in\', \'IN\r\n\'), (\'the\', \'AT\'), (\'election\', \'NN\'), (\',\', \',\'), (\'the\', \'AT\'), (\'number\', \'NN\'), (\'of\',\r\n \'NN\'), (\'voters\', \'NNS\'), (\'and\', \'NN\'), (\'the\', \'AT\'), (\'size\', \'NN\'), (\'of\', \'NN\'), (\'t\r\nhis\', \'NNS\'), (\'city\', \'NN\'), (""\'\'"", \'NN\'), (\'.\', \'.\')]\r\n\'\'\'\r\n\r\nrtags_eval = regexp_tagger.evaluate(brown_tagged_sents)\r\nprint(rtags_eval)\r\n# 0.20326391789486245\r\n# 0.4461085585854367\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n'"
python/language/args/args_1.py,0,"b'# *args\n\ndef fun_args(arg1, *argv):\n    print(""usual argument:"", arg1)\n    for arg in argv:\n        print(""args:"", *argv)\n\nfun_args(\'praveen\', \'python\', \'args\', \'kwargs\')\n\n# output\nusual argument: praveen\nargs: python args kwargs\nargs: python args kwargs\nargs: python args kwargs\n'"
python/language/args/kwargs_1.py,0,"b""# **kwargs\n\ndef fun_(x, y, z):\n    print(x, y, z)\n\ntuple_vec = (1, 0, 1)\ndict_vec = {'x': 1, 'y': 0, 'z': 1}\n\nfun_(*tuple_vec)\n1 0 1\nfun_(**dict_vec)\n1 0 1\n"""
python/language/collections/counter_1.py,0,"b""# sample use case for counter\n\nfrom collections import Counter\n\n# check if a word is an anagram of another\ndef is_anagram(str1, str2):\n    return Counter(str1) == Counter(str2)\n\n\nprint(is_anagram('praveen', 'vanpeer'))\n# True\n\nprint(is_anagram('vapeven', 'neevarp'))\n# False\n\n>>> Counter('praveen')\nCounter({'e': 2, 'p': 1, 'r': 1, 'a': 1, 'v': 1, 'n': 1})\n"""
python/language/contextmanager/context_manager_1.py,0,"b'# context manager\n# A context manager, in Python, is a resource acquisition and release mechanism \n# that prevents resource leak and ensures startup and cleanup (exit) actions are always done.\n\nimport gc\nimport time\nimport pandas as pd\nfrom contextlib import contextmanager\n\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(""{} in {:.0f}s"".format(title, time.time() - t0))\n\n# Preprocess credit_card_balance.csv\ndef process_file(num_rows = None, nan_as_category = True):\n    df = pd.read_csv(\'file_name.csv\', nrows = num_rows)\n    # perform operations\n    # df.transform, df.agg, df.groupby, df.drop\n    gc.collect()\n    return df\n\ndef main(debug = False):\n    with timer(""Process file""):\n        df = process_file(num_rows)\n        print(""Pandas DataFrame shape:"", df.shape)\n        # sample operations\n        df = df.join(df, how=\'left\', on=\'ID\')\n        del df\n        gc.collect()\n\nif __name__ == ""__main__"":\n    with timer(""timer""):\n        main()\n'"
python/language/datetime/date_diff_1.py,0,"b""# load library\nimport datetime\n\n# find number of days between two dates\ntoday = datetime.date.today()\n\nsomeday = datetime.date(2016, 11, 11)\n\ndiff = someday - today\n\nprint ('days: ' + str(diff.days))\n# -655\n"""
python/language/deepcopy/copy_1.py,0,"b'# load library\n\nimport copy\n\nl1 = [1,2,3]\n\n# deep copy\nl2 = copy.deepcopy(l1)\n\n\nl1 = [1,2,3]\n\n# shallow copy\nl2 = l1\n'"
python/language/deque/deque_1.py,0,"b'\n\'\'\'\n    Deque -  doubly-linked list of fixed length blocks\n    \n    Deques have O(1) speed for appendleft() and popleft() while lists have O(n) performance for insert(0, value) and pop(0).\n\n    List append performance is hit and miss because it uses realloc() under the hood. As a result, \n    it tends to have over-optimistic timings in simple code (because the realloc doesn\'t have to move data) \n    and really slow timings in real code (because fragmentation forces realloc to move all the data). \n    \n    In contrast, deque append performance is consistent because it never reallocs and never moves data.\n\n\'\'\'\n\n# load libraries\nimport timeit\nfrom collections import deque\n\n# append - append and pop \ndef appends():\n    s_append, s_pop = s.append, s.pop\n\n# deque - append and pop \ndef deques():\n    d_append, d_pop = d.append, d.pop\n\n# main\nif __name__ == \'__main__\':\n    \'\'\' main  \'\'\'\n    s = list(range(1000))\n    d = deque(s)\n\n    print(timeit.timeit(\'appends()\', setup=""from __main__ import appends""))\n    print(timeit.timeit(\'deques()\', setup=""from __main__ import deques""))\n\n# 0.2518334704937214\n# 0.19763738898872668\n\n# Append takes more time than deque as we can see from the output\n'"
python/language/deque/deque_2.py,0,"b'# print last n lines based on error code\n# collections.deque is implemented in C and lock-free\n\n# import libraries\nfrom collections import deque\n\n# deque of max size = required lines count \nline_history = deque(maxlen=25)\n\n# read file\nwith open(file, \'r\') as input:\n    for line in input:\n        if ""error code"" in line: \n            print(*line_history, line, sep=\'\')\n            # Clear history so if two errors seen in close proximity, we don\'t\n            # echo some lines twice\n            line_history.clear()\n        else:\n            # When deque reaches 25 lines, will automatically evict oldest\n            line_history.append(line)\n            \n'"
python/language/dict/sorting_1.py,0,"b""xs = {'a': 4, 'b': 3, 'c': 2, 'd': 1}\n\nsorted(xs.items(), key=lambda x: x[1])\n# [('d', 1), ('c', 2), ('b', 3), ('a', 4)]\n\n# Recommended one \n\nimport operator\n\nsorted(xs.items(), key=operator.itemgetter(1))\n# [('d', 1), ('c', 2), ('b', 3), ('a', 4)]\n"""
python/language/dir/dir_1.py,0,"b""# information about live objects\n# dir, type, id, inspect\n\n# dir - returns a list of attributes and methods belonging to an object\n\nmy_list = [1, 2, 3]\n\ndir(my_list)\n\n# ['__add__', '__class__', '__contains__', '__delattr__', '__delitem__',\n# '__delslice__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__',\n# '__getitem__', '__getslice__', '__gt__', '__hash__', '__iadd__', '__imul__',\n# '__init__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__',\n# '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__',\n# '__setattr__', '__setitem__', '__setslice__', '__sizeof__', '__str__',\n# '__subclasshook__', 'append', 'count', 'extend', 'index', 'insert', 'pop',\n# 'remove', 'reverse', 'sort']\n"""
python/language/dis/disassembler.py,0,"b'import sys\n\nl1 = [None] * 10\nl2 = [None for _ in range(10)]\n\nprint(\'Size of l1 =\', sys.getsizeof(l1))\n# 144\nprint(\'Size of l2 =\', sys.getsizeof(l2))\n# 192\n\n\'\'\'\nWhen you write [None] * 10, Python knows that it will need a list of exactly 10 objects, \nso it allocates exactly that.\n\nWhen you use a list comprehension, Python doesn\'t know how much it will need. \nSo it gradually grows the list as elements are added. \n\nFor each reallocation it allocates more room than is immediately needed, \nso that it doesn\'t have to reallocate for each element. \n\nThe resulting list is likely to be somewhat bigger than needed.\n\nlist-comprehension uses list.append under the hood, \nso it will call the list-resize method, which overallocates.\n\'\'\'\n\ncode = compile(\'[x for x in iterable]\', \'\', \'eval\')\nimport dis\ndis.dis(code)\n\n\'\'\'\n  1           0 LOAD_CONST               0 (<code object <listcomp> at 0x10560b810, file """", line 1>)\n              2 LOAD_CONST               1 (\'<listcomp>\')\n              4 MAKE_FUNCTION            0\n              6 LOAD_NAME                0 (iterable)\n              8 GET_ITER\n             10 CALL_FUNCTION            1\n             12 RETURN_VALUE\n\nDisassembly of <code object <listcomp> at 0x10560b810, file """", line 1>:\n  1           0 BUILD_LIST               0\n              2 LOAD_FAST                0 (.0)\n        >>    4 FOR_ITER                 8 (to 14)\n              6 STORE_FAST               1 (x)\n              8 LOAD_FAST                1 (x)\n             10 LIST_APPEND              2\n             12 JUMP_ABSOLUTE            4\n        >>   14 RETURN_VALUE\n\'\'\'\n'"
python/language/enum/enum_1.py,0,"b'# enum\n\nclass EnumExample:\n    att_1, att_2 = 1, 2\n\nprint(EnumExample.att_1)\nprint(EnumExample.att_2)\n'"
python/language/enumerate/find_all_occurrences.py,0,"b""# enumerate use case - find all occurrences of an element in a list\n\n# my_list\nmy_list = [1,2,1,2,1,2,3,4,5,4,3,2,2,3,4,5,6,2,2,2,1,1,1,1,0,0]\n\n# elem whose indices needs to be found\nreq_elem = 2\n\n# indices will have the indices/positions of the required element, here 2\nindices = [i for i, x in enumerate(my_list) if x == req_elem]\nindices\n# [1, 3, 5, 11, 12, 17, 18, 19]\n\n'''\nhow enumerate works?\n    i has the index \n    x has the element\n'''\n"""
python/language/functools/reduce_1.py,0,"b'# using reduce \n\n# load library\nimport functools\n\n# define a list\nmy_list = [10,15,20,25,35]\n\n# using reduce to perform an operation\nsum_numbers = functools.reduce(lambda x,y: x+y, my_list)\n\nprint(sum_numbers)\n# 105\n\n# check the output - use sum\nprint(sum(my_list))\n# 105\n'"
python/language/gc/gc_1.py,0,"b""# explicitly free memory\n# garbage collection\n\nimport gc\n\ngc.collect()\n\n>>> gc.collect()\n347 # integer denoting the freed memory in bytes\n\n# The del statement might be of use, \n# but it isn't guaranteed to free the memory.\n"""
python/language/gc/gc_2.py,0,"b""# gc stats\n\nimport gc\n\n>>> gc.isenabled()\nTrue\n\n>>> gc.get_stats()\n[{'collections': 158, 'collected': 755, 'uncollectable': 0}, \n{'collections': 14, 'collected': 952, 'uncollectable': 0}, \n{'collections': 4, 'collected': 616, 'uncollectable': 0}]\n\n>>> gc.get_objects()\n...\n...\n\n\n>>> gc.garbage\n[]\n\n"""
python/language/get/dict_get.py,0,"b""# get()\n\n# string values\nmy_str_dict = {\n      'red': '113a',\n      'green': '347b',\n      'cyan': '553c'\n}\n\ndef valueof(colour):\n    return 'value is %s' % my_str_dict.get(colour, 'unknown')\n\nvalueof('red')\n# 'value is 113a'\n\nvalueof('purple')\n# 'value is unknown'\n\n####################\n\n# numerical values\nmy_num_dict = {\n      'red': 113,\n      'green': 347,\n      'cyan': 553\n}\n\ndef valueof(colour):\n    return 'value is %d' % my_num_dict.get(colour, 0)\n\nvalueof('red')\n# 'value is 113'\n\nvalueof('purple')\n# 'value is 0'\n"""
python/language/hashlib/hashing_1.py,0,"b'# hashing using hashlib\n\n# load library\nimport hashlib\n\n# hashing (SHA-256) in Python\nhashlib.sha256(b""hello world"").hexdigest()\n\'b94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9\'\n\n# hashing (SHA-512) in Python\nhashlib.sha512(b""hello world"").hexdigest()\n# \'309ecc489c12d6eb4cc40f50c902f2b4d0ed77ee511a7c7a9bcd3ca86d4cd86f9\n# 89dd35bc5ff499670da34255b45b0cfd830e81f605dcf7dc5542e93ae9cd76f\'\n'"
python/language/inspect/get_classes.py,0,"b""'''\nextract classes from a module\nfor eg. consider numpy\n'''\n\n# load libraries\nimport inspect\nimport numpy as np\n\n# prints class name and object definition\nfor name, obj in inspect.getmembers(np):\n    if inspect.isclass(obj):\n        print(name, obj)\n\n'''\nAxisError <class 'numpy.core._internal.AxisError'>\nComplexWarning <class 'numpy.core.numeric.ComplexWarning'>\nDataSource <class 'numpy.lib._datasource.DataSource'>\nMachAr <class 'numpy.core.machar.MachAr'>\nModuleDeprecationWarning <class 'numpy._globals.ModuleDeprecationWarning'>\nPackageLoader <class 'numpy._import_tools.PackageLoader'>\nRankWarning <class 'numpy.lib.polynomial.RankWarning'>\nTester <class 'numpy.testing.nose_tools.nosetester.NoseTester'>\nTooHardError <class 'numpy.core._internal.TooHardError'>\nVisibleDeprecationWarning <class 'numpy._globals.VisibleDeprecationWarning'>\n_NoValue <class 'numpy._globals._NoValue'>\nbool <class 'bool'>\nbool8 <class 'numpy.bool_'>\nbool_ <class 'numpy.bool_'>\nbroadcast <class 'numpy.broadcast'>\nbusdaycalendar <class 'numpy.busdaycalendar'>\nbyte <class 'numpy.int8'>\nbytes0 <class 'numpy.bytes_'>\nbytes_ <class 'numpy.bytes_'>\ncdouble <class 'numpy.complex128'>\ncfloat <class 'numpy.complex128'>\ncharacter <class 'numpy.character'>\nchararray <class 'numpy.core.defchararray.chararray'>\nclongdouble <class 'numpy.complex128'>\nclongfloat <class 'numpy.complex128'>\ncomplex <class 'complex'>\ncomplex128 <class 'numpy.complex128'>\ncomplex64 <class 'numpy.complex64'>\ncomplex_ <class 'numpy.complex128'>\ncomplexfloating <class 'numpy.complexfloating'>\ncsingle <class 'numpy.complex64'>\ndatetime64 <class 'numpy.datetime64'>\ndouble <class 'numpy.float64'>\ndtype <class 'numpy.dtype'>\nerrstate <class 'numpy.core.numeric.errstate'>\nfinfo <class 'numpy.core.getlimits.finfo'>\nflatiter <class 'numpy.flatiter'>\nflexible <class 'numpy.flexible'>\nfloat <class 'float'>\nfloat16 <class 'numpy.float16'>\nfloat32 <class 'numpy.float32'>\nfloat64 <class 'numpy.float64'>\nfloat_ <class 'numpy.float64'>\nfloating <class 'numpy.floating'>\nformat_parser <class 'numpy.core.records.format_parser'>\ngeneric <class 'numpy.generic'>\nhalf <class 'numpy.float16'>\niinfo <class 'numpy.core.getlimits.iinfo'>\ninexact <class 'numpy.inexact'>\nint <class 'int'>\nint0 <class 'numpy.int64'>\nint16 <class 'numpy.int16'>\nint32 <class 'numpy.int32'>\nint64 <class 'numpy.int64'>\nint8 <class 'numpy.int8'>\nint_ <class 'numpy.int32'>\nintc <class 'numpy.int32'>\ninteger <class 'numpy.integer'>\nintp <class 'numpy.int64'>\nlong <class 'int'>\nlongcomplex <class 'numpy.complex128'>\nlongdouble <class 'numpy.float64'>\nlongfloat <class 'numpy.float64'>\nlonglong <class 'numpy.int64'>\nmatrix <class 'numpy.matrixlib.defmatrix.matrix'>\nmemmap <class 'numpy.core.memmap.memmap'>\nndarray <class 'numpy.ndarray'>\nndenumerate <class 'numpy.lib.index_tricks.ndenumerate'>\nndindex <class 'numpy.lib.index_tricks.ndindex'>\nnditer <class 'numpy.nditer'>\nnumber <class 'numpy.number'>\nobject <class 'object'>\nobject0 <class 'numpy.object_'>\nobject_ <class 'numpy.object_'>\npoly1d <class 'numpy.lib.polynomial.poly1d'>\nrecarray <class 'numpy.recarray'>\nrecord <class 'numpy.record'>\nshort <class 'numpy.int16'>\nsignedinteger <class 'numpy.signedinteger'>\nsingle <class 'numpy.float32'>\nsinglecomplex <class 'numpy.complex64'>\nstr <class 'str'>\nstr0 <class 'numpy.str_'>\nstr_ <class 'numpy.str_'>\nstring_ <class 'numpy.bytes_'>\ntimedelta64 <class 'numpy.timedelta64'>\nubyte <class 'numpy.uint8'>\nufunc <class 'numpy.ufunc'>\nuint <class 'numpy.uint32'>\nuint0 <class 'numpy.uint64'>\nuint16 <class 'numpy.uint16'>\nuint32 <class 'numpy.uint32'>\nuint64 <class 'numpy.uint64'>\nuint8 <class 'numpy.uint8'>\nuintc <class 'numpy.uint32'>\nuintp <class 'numpy.uint64'>\nulonglong <class 'numpy.uint64'>\nunicode <class 'str'>\nunicode_ <class 'numpy.str_'>\nunsignedinteger <class 'numpy.unsignedinteger'>\nushort <class 'numpy.uint16'>\nvectorize <class 'numpy.lib.function_base.vectorize'>\nvoid <class 'numpy.void'>\nvoid0 <class 'numpy.void'>\n'''\n\n\n# get all classes of current module\nimport sys\n\n# set current module to sys modules\ncurrent_module = sys.modules[__name__]\n\n# get all classes\ndef get_classes(mod):\n    '''\n\tpass required module as param\n\treturns/prints the classes\n\t'''\n    for name, obj in inspect.getmembers(mod):\n        if inspect.isclass(obj):\n            print(name, obj)\n\n\n# call the function\nget_classes(current_module)\n# __loader__ <class '_frozen_importlib.BuiltinImporter'>\n\n# set current module to pandas DataFrame\nimport pandas as pd\ncurrent_module = pd.DataFrame\n\n# call the function\nget_classes(current_module)\n\n'''\n>>> current_module\n<class 'pandas.core.frame.DataFrame'>\n>>> get_classes(current_module)\n__class__ <class 'type'>\n_constructor_sliced <class 'pandas.core.series.Series'>\nplot <class 'pandas.plotting._core.FramePlotMethods'>\n>>>\n'''\n"""
python/language/iter/iter_1.py,0,"b'# using iter basics\n\n>>> iter(\'vanautu\')\n<str_iterator object at 0x02E27030>\n>>> _ = iter(\'vanautu\')\n>>> next(_)\n\'v\'\n>>> next(_)\n\'a\'\n>>> next(_)\n\'n\'\n>>> next(_)\n\'a\'\n>>> next(_)\n\'u\'\n>>> next(_)\n\'t\'\n>>> next(_)\n\'u\'\n>>> next(_)\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nStopIteration\n>>>\n'"
python/language/iter/iter_read_file_till_char.py,0,"b""# read file till condition/char is met\n'''\n# mydata.txt\n1\n2\n3\n\n4\n5\n'''\n\nl = []\n\n# read file, use iter with condition as '\\n'\n# this will read till '\\n' is reached\nwith open('mydata.txt') as f:\n    for line in iter(f.readline, '\\n'):\n        l.append(line)\n\nl\n# ['1\\n', '2\\n', '3\\n']\n"""
python/language/json/json_1.py,0,"b'# using the ""json"" module\nimport json\n\n# The standard string repr for dicts is hard to read:\njson_map = {\'a\': 23, \'b\': 42, \'c\': 0xc0ffee}\n>>> json_map\n{\'a\': 23, \'b\': 42, \'c\': 12648430} # normal printing\n\nprint(json.dumps(json_map, indent=4, sort_keys=True))\n{\n    ""a"": 23,\n    ""b"": 42,\n    ""c"": 12648430\n}\n# pretty print!\n'"
python/language/json/json_2.py,0,"b""# load json\n\njson_map = {'a': 23, 'b': 42, 'c': 0xc0ffee, 'd': True}\n>>> json_map\n{'a': 23, 'b': 42, 'c': 12648430, 'd': True}\n\n# writing to file\nwith open('data.json', 'w') as outfile:\n    json.dump(json_map, outfile)\n\nwith open('data.json', 'r') as infile:\n    f = json.load(infile)\n\n>>> f\n{'a': 23, 'b': 42, 'c': 12648430, 'd': True}\n\n\n# enable_ascii\n# \n"""
python/language/keyword/keyword_kwlist.py,0,"b'\'\'\'\nE:\\>Python\nPython 3.6.3 (v3.6.3:2c5fed8, Oct  3 2017, 18:11:49) [MSC v.1900 64 bit (AMD64)] on win32\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\n\'\'\'\n\n# load library\nimport keyword\n\n# keyword\nkeyword\n<module \'keyword\' from \'E:\\\\Python\\\\lib\\\\keyword.py\'>\n\n# list of keywords!\nkeyword.kwlist\n[\'False\', \'None\', \'True\', \'and\', \'as\', \'assert\', \'break\', \'class\', \'continue\', \'def\', \'del\', \'elif\', \'else\', \'except\', \'finally\', \'for\', \'from\', \'global\', \'if\', \'import\', \'in\', \'is\', \'lambda\', \'nonlocal\', \'not\', \'or\', \'pass\', \'raise\', \'return\', \'try\', \'while\', \'with\', \'yield\']\n\n# length of keyword list\nlen(keyword.kwlist)\n# 33\n'"
python/language/list/lists_matching_index.py,0,"b""# find matching indices of elements in two lists\n\ndef find_matching_index(list1, list2):\n    ''' input -> lists\n        output -> set of indices\n    '''\n    \n    inverse_index = { element: index for index, element in enumerate(list1) }\n\n    return [ (index, inverse_index[element]) for index, element in enumerate(list2) \n                                                    if element in inverse_index ]\n\nfind_matching_index([1,2,3], [3,2,1]) \n# [(0, 2), (1, 1), (2, 0)]\n\nfind_matching_index([1,2,4], [3,2,1])\n# [(1, 1), (2, 0)]\n"""
python/language/multiprocessing/mp_1.py,0,"b""# multiprocessing example\n\nimport os\nfrom multiprocessing import Process\n\ndef f(name):\n    print('Hello,', name)\n\nname = input('Name:')\n\nif __name__ == '__main__':\n    print('Parent process', name)\n    p = Process(target=f, args=[os.environ.get('USER', 'Unknown user')])\n    p.start()\n    p.join()\n\n# Parent process <name>\n# Hello <os user id>\n\n# unlike multi=threading, here we cannot share variables between processes\n"""
python/language/multiprocessing/mp_2.py,0,"b'# getting cpu count\n\nimport multiprocessing\n\nmultiprocessing.cpu_count()\n\n# getting cpu\'s used by current process\n\nimport os\nimport re\nimport subprocess\n\n\ndef available_cpu_count():\n    """""" Number of available virtual or physical CPUs on this system, i.e.\n    user/real as output by time(1) when called with an optimally scaling\n    userspace-only program""""""\n\n    # cpuset\n    # cpuset may restrict the number of *available* processors\n    try:\n        m = re.search(r\'(?m)^Cpus_allowed:\\s*(.*)$\',\n                      open(\'/proc/self/status\').read())\n        if m:\n            res = bin(int(m.group(1).replace(\',\', \'\'), 16)).count(\'1\')\n            if res > 0:\n                return res\n    except IOError:\n        pass\n\n    # Python 2.6+\n    try:\n        import multiprocessing\n        return multiprocessing.cpu_count()\n    except (ImportError, NotImplementedError):\n        pass\n\n    # https://github.com/giampaolo/psutil\n    try:\n        import psutil\n        return psutil.cpu_count()   # psutil.NUM_CPUS on old versions\n    except (ImportError, AttributeError):\n        pass\n\n    # POSIX\n    try:\n        res = int(os.sysconf(\'SC_NPROCESSORS_ONLN\'))\n\n        if res > 0:\n            return res\n    except (AttributeError, ValueError):\n        pass\n\n    # Windows\n    try:\n        res = int(os.environ[\'NUMBER_OF_PROCESSORS\'])\n\n        if res > 0:\n            return res\n    except (KeyError, ValueError):\n        pass\n\n    # jython\n    try:\n        from java.lang import Runtime\n        runtime = Runtime.getRuntime()\n        res = runtime.availableProcessors()\n        if res > 0:\n            return res\n    except ImportError:\n        pass\n\n    # BSD\n    try:\n        sysctl = subprocess.Popen([\'sysctl\', \'-n\', \'hw.ncpu\'],\n                                  stdout=subprocess.PIPE)\n        scStdout = sysctl.communicate()[0]\n        res = int(scStdout)\n\n        if res > 0:\n            return res\n    except (OSError, ValueError):\n        pass\n\n    # Linux\n    try:\n        res = open(\'/proc/cpuinfo\').read().count(\'processor\\t:\')\n\n        if res > 0:\n            return res\n    except IOError:\n        pass\n\n    # Solaris\n    try:\n        pseudoDevices = os.listdir(\'/devices/pseudo/\')\n        res = 0\n        for pd in pseudoDevices:\n            if re.match(r\'^cpuid@[0-9]+$\', pd):\n                res += 1\n\n        if res > 0:\n            return res\n    except OSError:\n        pass\n\n    # Other UNIXes (heuristic)\n    try:\n        try:\n            dmesg = open(\'/var/run/dmesg.boot\').read()\n        except IOError:\n            dmesgProcess = subprocess.Popen([\'dmesg\'], stdout=subprocess.PIPE)\n            dmesg = dmesgProcess.communicate()[0]\n\n        res = 0\n        while \'\\ncpu\' + str(res) + \':\' in dmesg:\n            res += 1\n\n        if res > 0:\n            return res\n    except OSError:\n        pass\n\n    raise Exception(\'Cannot determine number of CPUs on this system\')\n'"
python/language/namedtuple/named_tuple_1.py,0,"b'from collections import namedtuple\n\nCar = namedtuple(\'Car\', \'color mileage\')\nCar\n# <class \'__main__.Car\'>\n\n# Our new ""Car"" class works as expected:\nmy_car = Car(\'red\', 3812.4)\nmy_car.color\n# \'red\'\n\nmy_car.mileage\n# 3812.4\n\n# We get a nice string repr for free:\nmy_car\n# Car(color=\'red\' , mileage=3812.4)\n\n# Like tuples, namedtuples are immutable:\nmy_car.color = \'blue\'\n#AttributeError: ""can\'t set attribute""\n'"
python/language/ordereddict/OrderedDict_1.py,0,"b'# OrderedDict\n# Python 3.6+ preserves insertion order\n\n# `dict` is not explicitly meant to be an ordered collection, \n# so if you want to stay consistent and \n# not rely on a side effect of the new implementation \n# you should stick with `OrderedDict`\n\n\nfrom collections import OrderedDict\nprint(""This is a Dict:\\n"")\n# This is a Dict:\n\nd = {}\nd[\'a\'] = 1\nd[\'b\'] = 2\nd[\'c\'] = 3\nd[\'d\'] = 4\n\nfor key, value in d.items():\n    print(key, value)\n\na 1\nb 2\nc 3\nd 4\n\nprint(""\\nThis is an Ordered Dict:\\n"")\n# This is an Ordered Dict:\n\nod = OrderedDict()\nod[\'a\'] = 1\nod[\'b\'] = 2\nod[\'c\'] = 3\nod[\'d\'] = 4\nfor key, value in od.items():\n    print(key, value)\n\na 1\nb 2\nc 3\nd 4\n\n# Changing the value of a key preserves the order in OrderedDict\n# Deleting a key and adding it again sends the key to the last\n'"
python/language/os/eval_1.py,0,"b""from os import cpu_count\neval('[1,cores]', {'__builtins__': None}, {'cores': cpu_count()})\n# [1, 4]\n"""
python/language/pandas/remove_stopwords_1.py,0,"b'\'\'\'\nDesired output:\n   ID               name\n0   1              Kitty\n1   2              Puppy\n2   3              is example\n3   4              stackoverflow\n4   5              World\n\'\'\'\n\nimport pandas as pd\n\ndf = pd.DataFrame(\n    {\n        ""ID"": [1, 2, 3, 4, 5],\n        ""name"": [\n            ""Hello Kitty"",\n            ""Hello Puppy"",\n            ""It is an Helloexample"",\n            ""for stackoverflow"",\n            ""Hello World"",\n        ],\n    }\n)\n\n\'\'\'\nInput:\n   ID               name\n0   1        Hello Kitty\n1   2        Hello Puppy\n2   3   It is an Helloexample\n3   4  for stackoverflow\n4   5        Hello World\n\'\'\'\n\nstopwords = [""Hello"", ""for"", ""an"", ""It""]\n\n# Straight-forward way >>>\ndf = df[\'name\'].replace(stopwords, value=\'\', regex=True)\n\n# Alternately, using \'re\'\nimport re\np = re.compile(\'|\'.join(map(re.escape, stopwords)))\ndf[\'name\'] = [p.sub(\'\', text) for text in df[\'name\']]\n\nprint(df)\n\n# List comprehensions are implemented in C and operate in C speed. \n# I highly recommend list comprehensions when working with string \n# and regex data over pandas str functions for the time-being because the API is a bit slow.\n\n# The use of map(re.escape, stopwords) is to escape any possible \n# regex metacharacters which are meant to be treated literally during replacement.\n\n# The pattern is precompiled before calling regex.sub to reduce the overhead of compilation at each iteration.\n\n# Timings\n\ndf = pd.concat([df] * 10000)\n%timeit df[\'name\'].str.replace(\'|\'.join(To_remove_lst), \'\')\n%timeit [p.sub(\'\', text) for text in df[\'name\']] \n\n# 100 ms \xc2\xb1 5.88 ms per loop (mean \xc2\xb1 std. dev. of 7 runs, 10 loops each)\n# 60 ms \xc2\xb1 3.27 ms per loop (mean \xc2\xb1 std. dev. of 7 runs, 10 loops each)\n'"
python/language/pdb/debugging_1.py,0,"b'# run a script from the commandline using the Python debugger\npython -m pdb debug_script.py\n\n# load library\nimport pdb\n\n# sample function\ndef make_bread():\n    \'\'\' pdb set_trace() \'\'\'\n    pdb.set_trace()\n    return ""I don\'t have time""\n\nprint(make_bread())\n\n# Command Line Interpreter output\n>>> make_bread()\n> <stdin>(3)make_bread()\n(Pdb) 2\n2\n\n(Pdb) 3\n3\n\n(Pdb) 0\n0\n\n(Pdb) pr\n*** NameError: name \'pr\' is not defined\n\n(Pdb) \'van peer\'\n\'van peer\'\n\n# use of c, s, w, a, n (pdb-cswan)\n>>> make_bread()\n> <stdin>(3)make_bread()\n(Pdb) \'pears\'\n\'pears\'\n\n(Pdb) a                         # prints the arguments if any\n\n(Pdb) s                         # execute the current line and \n--Return--                      # stop at the first possible occasion\n> <stdin>(3)make_bread()->""I don\'t have time""\n\n(Pdb) w                         # shows the executing line context \n  <stdin>(1)<module>()\n> <stdin>(3)make_bread()->""I don\'t have time""\n\n(Pdb) n                         # Continue execution until the next line \n""I don\'t have time""             # in the current function is reached or \n--Return--                      # it returns\n> <stdin>(1)<module>()->None\n\n(Pdb) c                         # continue execution\n>>>\n'"
python/language/performance/pandas_groupby_value_count_1.py,0,"b'# import libraries\nimport tkinter\nimport pandas as pd\nfrom timeit import timeit\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame([\n [\'A\',    4.3],\n [\'B\',    3.1],\n [\'C\',    8.9],\n [\'D\',    2.1],\n [\'E\',    3.9],\n [\'F\',    4.5],\n [\'B\',    3.1],\n [\'C\',    8.9],\n [\'A\',    2.1],\n [\'A\',    3.9],\n [\'B\',    4.5],\n [\'C\',    3.27],\n [\'D\',    8.7],\n [\'E\',    2.37],\n [\'F\',    3.7],\n [\'B\',    4.7],\n [\'B\',    3.37],\n [\'C\',    8.7],\n [\'A\',    2.7],\n [\'A\',    3.7],\n [\'A\',    4.3],\n ])\n\ndf.columns = [\'Parameter\',  \'Value\']\n\n# filter\ndef fun1(df):\n    return df.groupby(\'Parameter\').filter(lambda x : x[\'Parameter\'].shape[0]>=4)\n\n# value_counts + isin\ndef fun2(df):\n    v = df.Parameter.value_counts()\n    return df[df.Parameter.isin(v.index[v.gt(4)])]\n\n# groupby + transform \ndef fun3(df):\n    return df[df.groupby(\'Parameter\')[\'Parameter\'].transform(\'size\') > 4]\n\n\n# create a pandas dataframe \n# index has the list of functions\n# columns has the multiplication factor - \n# to increase input list size (thereby complexity)\nres = pd.DataFrame(\n       index=[\'fun1\', \'fun2\', \'fun3\'],\n       columns=[1, 10, 25, 50, 100, 500, 1000, 5000,] # 10000, 50000, 100000,] # 1000000],\n)\n\n# each function to be looped over the mul.factors\n# timeit is used and output to dataframe\nfor f in res.index: \n    for c in res.columns:\n        l = pd.concat([df] * c)\n        stmt = \'{}(l)\'.format(f)       # f(l)\n        setp = \'from __main__ import l, {}\'.format(f)\n        res.at[f, c] = timeit(stmt, setp, number=50)\n\n\n# using matplotlib to plot \nax = res.div(res.min()).T.plot(loglog=True) \nax.set_xlabel(""N"")\nax.set_ylabel(""time (relative)"")\n\nplt.show()\n'"
python/language/performance/pandas_obj_to_float.py,0,"b'# import libraries\nimport operator\nfrom itertools import islice\nfrom timeit import timeit\nfrom itertools import chain\nimport tkinter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# function 1 \ndef fun1(l):\n    df = pd.DataFrame(l)\n    print(len(df))\n    return df[0].astype(float)\n\n# function 2\ndef fun2(l):\n    df = pd.DataFrame(l)\n    print(len(df))\n    return df[0].transform(float)\n\n# function 2\ndef fun3(l):\n    df = pd.DataFrame(l)\n    print(len(df))\n    return df[0].transform(lambda x: float(x))\n\n# create a pandas dataframe \n# index has the list of functions\n# columns has the multiplication factor - \n# to increase input list size (thereby complexity)\nres = pd.DataFrame(\n       index=[\'fun1\', \'fun2\', \'fun3\',],\n       columns=[1, 10, 25, 50, 100, 500, 1000, 5000, 10000, 50000], #100000],\n       dtype=float\n)\n\n# each function to be looped over the mul.factors\n# timeit is used and output to dataframe\nfor f in res.index: \n    for c in res.columns:\n        l = [1,2,3,4,5,6,7,8,9,10] * c\n        stmt = \'{}(l)\'.format(f)       # f(l)\n        setp = \'from __main__ import l, {}\'.format(f)\n        res.at[f, c] = timeit(stmt, setp, number=50)\n\n\n# using matplotlib to plot \nax = res.div(res.min()).T.plot(loglog=True) \nax.set_xlabel(""N"")\nax.set_ylabel(""time (relative)"")\n\nplt.show()\n\n\n'"
python/language/performance/performance_join_list_of_tuples.py,0,"b'# import libraries\nimport tkinter\nimport pandas as pd\nfrom timeit import timeit\nimport matplotlib.pyplot as plt\n\n# function 1 \ndef fun1(l):\n    return \' \'.join([\'\'.join(w) for w in l])\n\n# function 2\ndef fun2(l):\n    return \' \'.join([x+y for x, y in l])\n\nlist_of_tuples = [(\'e\xcb\x90\', \'n\'), (\'a\', \'k\'), (\'a\', \'s\')]\n\n# create a pandas dataframe \n# index has the list of functions\n# columns has the multiplication factor - \n# to increase input list size (thereby complexity)\nres = pd.DataFrame(\n       index=[\'fun1\', \'fun2\',],\n       columns=[1, 10, 25, 50, 100, 500, 1000, 5000, 10000, 50000, 100000, 1000000],\n)\n\n# each function to be looped over the mul.factors\n# timeit is used and output to dataframe\nfor f in res.index: \n    for c in res.columns:\n        l = list_of_tuples * c\n        stmt = \'{}(l)\'.format(f)       # f(l)\n        setp = \'from __main__ import l, {}\'.format(f)\n        res.at[f, c] = timeit(stmt, setp, number=50)\n\n\n# using matplotlib to plot \nax = res.div(res.min()).T.plot(loglog=True) \nax.set_xlabel(""N"")\nax.set_ylabel(""time (relative)"")\n\nplt.show()'"
python/language/performance/performance_list_search_1.py,0,"b'# import libraries\nimport tkinter\nimport pandas as pd\nfrom timeit import timeit\nimport matplotlib.pyplot as plt\n\nk = 775\nl = list(range(1111))\n\n# function 1 \ndef fun1(l):\n    for i in l:\n        if i==k:\n            return True\n\n# function 2\ndef fun2(l):\n    if k in l:\n        return True\n\n\n# create a pandas dataframe \n# index has the list of functions\n# columns has the multiplication factor - \n# to increase input list size (thereby complexity)\nres = pd.DataFrame(\n       index=[\'fun1\', \'fun2\',],\n       columns=[1, 10, 25, 50, 100, 500, 1000, 5000, 10000, 50000, 100000,] # 1000000],\n)\n\n# each function to be looped over the mul.factors\n# timeit is used and output to dataframe\nfor f in res.index: \n    for c in res.columns:\n        l_ = l * c\n        stmt = \'{}(l_)\'.format(f)       # f(l_)\n        setp = \'from __main__ import l_, {}\'.format(f)\n        res.at[f, c] = timeit(stmt, setp, number=50)\n\n\n# using matplotlib to plot \nax = res.div(res.min()).T.plot(loglog=True) \nax.set_xlabel(""N"")\nax.set_ylabel(""time (relative)"")\n\nplt.show()'"
python/language/performance/performance_pandas_groupby_text_concatenate_1.py,1,"b'# import libraries\nimport tkinter\nimport itertools\nimport collections\nimport numpy as np\nimport pandas as pd\nfrom timeit import timeit\nfrom itertools import chain\nimport matplotlib.pyplot as plt\n\ndocuments = [[\'Human\', \'machine\', \'interface\'],[\'A\', \'survey\', \'of\', \'user\'],[\'The\', \'EPS\', \'user\'],\n             [\'System\', \'and\', \'human\'],[\'Relation\', \'of\', \'user\'],[\'The\', \'generation\'],\n             [\'The\', \'intersection\'],[\'Graph\', \'minors\'],[\'Graph\', \'minors\', \'a\']]\n\ndf = pd.DataFrame({\'date\': np.array([\'2014-05-01\', \'2014-05-02\', \'2014-05-10\', \'2014-05-10\', \'2014-05-15\', \'2014-05-15\', \'2014-05-20\', \'2014-05-20\', \'2014-05-20\'], dtype=np.datetime64), \'text\': documents})\n\n\'\'\'\nDesired Output: (based on unique dates + concatenate string)\n        date                                               text\n0 2014-05-01                        [Human, machine, interface]\n1 2014-05-02                              [A, survey, of, user]\n2 2014-05-10               [The, EPS, user, System, and, human]\n3 2014-05-15              [Relation, of, user, The, generation]\n4 2014-05-20  [The, intersection, Graph, minors, Graph, \'minors\', \'a\']\n\'\'\'\n\n# function 1 \ndef fun1(df):\n    return df.groupby(\'date\').text.sum()\n\n# function 2\ndef fun2(df):\n    return df.groupby(\'date\').text.agg(\'sum\')\n\n# function 1 \ndef fun3(df):\n    return df.groupby(\'date\').text.agg(lambda x: [item for z in x for item in z])\n\n# function 2\ndef fun4(df):\n    return df.groupby(\'date\').text.agg(lambda x: list(itertools.chain.from_iterable(x)))\n\n\n# create a pandas dataframe \n# index has the list of functions\n# columns has the multiplication factor - \n# to increase input list size (thereby complexity)\nres = pd.DataFrame(\n       index=[\'fun1\', \'fun2\',\'fun3\', \'fun4\',],\n       columns=[1, 10, 25, 50, 100, 500, 1000, 5000, 10000,] # 50000, 100000, 1000000],\n)\n\n# each function to be looped over the mul.factors\n# timeit is used and output to dataframe\nfor f in res.index: \n    for c in res.columns:\n        l = pd.concat([df] * c)\n        stmt = \'{}(l)\'.format(f)       # f(l)\n        setp = \'from __main__ import l, {}\'.format(f)\n        res.at[f, c] = timeit(stmt, setp, number=50)\n\n\n# using matplotlib to plot \nax = res.div(res.min()).T.plot(loglog=True) \nax.set_xlabel(""N"")\nax.set_ylabel(""time (relative)"")\n\nplt.show()\n\n\n\n'"
python/language/performance/performance_pandas_stopwords_1.py,0,"b'# import libraries\nimport re\nimport tkinter\nimport pandas as pd\nfrom timeit import timeit\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame(\n    {\n        ""ID"": [1, 2, 3, 4, 5],\n        ""name"": [\n            ""Hello Kitty"",\n            ""Hello Puppy"",\n            ""It is an Helloexample"",\n            ""for stackoverflow"",\n            ""Hello World"",\n        ],\n    }\n)\n\nstopwords = [""Hello"", ""for"", ""an"", ""It""]\n\n# function 1 \ndef fun1(df):\n    return df[\'name\'].str.replace(\'|\'.join(stopwords), \'\')\n\n# function 2\ndef fun2(df):\n    p = re.compile(\'|\'.join(map(re.escape, stopwords)))\n    return [p.sub(\'\', text) for text in df[\'name\']]\n\n# function 3\ndef fun3(df):\n    return df[\'name\'].replace(stopwords, value=\'\', regex=True)\n\n\n# create a pandas dataframe \n# index has the list of functions\n# columns has the multiplication factor - \n# to increase input list size (thereby complexity)\nres = pd.DataFrame(\n       index=[\'fun1\', \'fun2\', \'fun3\',],\n       columns=[1, 10, 25, 50, 100, 500, 1000, 5000,] # 10000, 50000, 100000,] # 1000000],\n)\n\n# each function to be looped over the mul.factors\n# timeit is used and output to dataframe\nfor f in res.index: \n    for c in res.columns:\n        l = pd.concat([df] * c)\n        stmt = \'{}(l)\'.format(f)       # f(l)\n        setp = \'from __main__ import l, {}\'.format(f)\n        res.at[f, c] = timeit(stmt, setp, number=50)\n\n\n# using matplotlib to plot \nax = res.div(res.min()).T.plot(loglog=True) \nax.set_xlabel(""N"")\nax.set_ylabel(""time (relative)"")\n\nplt.show()'"
python/language/print/print_format_1.py,0,"b""name = 'John'\n\n#1 basic method\nprint('Hello, his name is ' + name)\n\n#2 .format method\nprint('Hello, his name is {}'.format(name))\n\n#3 f-string literal method\nprint(f'Hello, his name is {name}')\n\n#4 Template\nfrom string import Template\ns = Template('$who likes $what')\ns.substitute(who='tim', what='kung pao')\n# 'tim likes kung pao'\n\n\n#1: won't work if name isn't a string.\n\n#2: is fine on any version but is a little unwieldy to type. \n# Generally the best for compatibility across python versions.\n\n#3: If the best option in terms of readability (and performance). \n# But it only works on python3.6+, so not a good idea if you want your code to be backwards compatible.\n\n#4: Template strings support $-based substitutions\n\n#5: old style formatting ala %s, %d, etc, which are now discouraged in favour for str.format.\n"""
python/language/psutil/psu_1.py,0,b'# cpu count using psutil\n\nimport psutil\n\npsutil.cpu_count()\n'
python/language/queue/queue_1.py,0,"b'# Queues (FIFO) are not built-in Python methods\n\nimport queue\n\nq = queue.Queue()\n\nq.empty()\nTrue\n\nq = queue.Queue(2)\n\nq.empty()\nTrue\n\nq.put(1)\nq.full()\nFalse\n\nq.put(2)\nq.full()\nTrue\n\nq.put_nowait()\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nTypeError: put_nowait() missing 1 required positional argument: \'item\'\n\n\nq.get()\n1\n\nq.get()\n2\n\nq.get_nowait()\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""E:\\Python\\lib\\queue.py"", line 192, in get_nowait\n    return self.get(block=False)\n  File ""E:\\Python\\lib\\queue.py"", line 161, in get\n    raise Empty\nqueue.Empty\n\nq.empty()\nTrue\n'"
python/language/reduce/reduce_1.py,0,"b'# load library\nfrom functools import reduce\n\n# normal way\nproduct = 1\nlist = [1, 2, 3, 4]\n\nfor num in list:\n    product = product * num\n\n\n# using reduce!\nproduct = reduce((lambda x, y: x * y), [1, 2, 3, 4])\nproduct\n24\n'"
python/language/samples/df_shift.py,1,"b""import pandas as pd\nimport numpy as np\n\ndf2 = pd.DataFrame(np.random.randint(low=0, high=10, size=(5, 2)), columns=['a', 'b'])\nprint(df2)\n\ndf2['c'] = df2['a'].reindex(df2.index - df2['b']).values\nprint(df2)\n"""
python/language/samples/dict_to_df_row_key_value.py,0,"b'\'\'\'\ndict / list to rows\n\'\'\'\n\nd={\n  \'information\': [\'row1\', \'row2\', \'row3\', \'row4\'],\n  \'record\': [\n      \'val1\', {\'val2\': {\'a\': 300, \'b\': [ {""b1"": 10.5}, {""b2"": 2} ]  }, \'val3\': { \'a\': 10, \'b\': 15 } },\n      \'val4\', \n\t\t[{\t\'val5\': [ {\t\'a\': { \'c\': [ {\t\'d\': { \'e\': [ {\t\'f\': 1 },{\'g\': 3 }]\t}} ]}}]},\n\t\t{\'b\': \'bar\'}\n\t\t]\n\t]\n}\n\nimport itertools\nimport collections\nimport pandas as pd\n\nch = lambda ite: list(itertools.chain.from_iterable(ite))\n\ndef isseq(obj):\n    if isinstance(obj, str): return False\n    return isinstance(obj, collections.abc.Sequence)\n\ndef unnest(k, v):\n    if isseq(v): return ch([unnest(k, v_) for v_ in v])\n    if isinstance(v, dict): return ch([unnest(""_"".join([k, k_]), v_) for k_, v_ in v.items()])\n    return k,v\n\ndef pairwise(i):\n    _a = iter(i)\n    return list(zip(_a, _a))\n\na = ch([(unnest(k, v)) for k, v in zip(d[\'information\'], d[\'record\'])])\nprint(pd.DataFrame(pairwise(a)))\n\n\'\'\'\n                     0     1\n0                 row1  val1\n1          row2_val2_a   300\n2       row2_val2_b_b1  10.5\n3       row2_val2_b_b2     2\n4          row2_val3_a    10\n5          row2_val3_b    15\n6                 row3  val4\n7  row4_val5_a_c_d_e_f     1\n8  row4_val5_a_c_d_e_g     3\n9               row4_b   bar\n\'\'\'\n'"
python/language/set/membership.py,0,"b""# use case for set\n\n# input list\nmy_list = ['random','range','python','freedom','open','dreams','live','unknown','a',\n            'the','is','you','are','hope','world','life','a','real','the','you','yours','reality','magic']\n\n# finding whether an element is present in list or not\n'python' in set(my_list)\nTrue\n\n# Set membership testing is O(1) constant time.\n"""
python/language/switch-case/switch-case.py,0,"b""# switch-case\n\ndef fun1(x):\n    return x**x\n\ndef fun2(x):\n    return x**(x**x)\n\nswitch_case = {\n\t'xpx': fun1,\n\t'xpxpx': fun2,\n\t'default': None\n}\n\nprint(switch_case.get('xpx')(2))\n# 4\nprint(switch_case.get('xpxpx')(2))\n# 16\n"""
python/language/sys/sys_recursion.py,0,"b'>>> sys.getrecursionlimit()\n1000\n\n>>> sys.setrecursionlimit(0)\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nValueError: recursion limit must be greater or equal than 1\n\n>>> sys.setrecursionlimit(1)\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nRecursionError: cannot set the recursion limit to 1 at the recursion depth 1: the limit is too low\n\n>>> sys.setrecursionlimit(2)\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nRecursionError: cannot set the recursion limit to 2 at the recursion depth 1: the limit is too low\n\n>>> sys.setrecursionlimit(3)\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nRecursionError: cannot set the recursion limit to 3 at the recursion depth 1: the limit is too low\n\n>>> sys.setrecursionlimit(4)\n\n>>> sys.getrecursionlimit()\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nRecursionError: maximum recursion depth exceeded while calling a Python object\n4\n\n>>>'"
python/language/sys/sys_size.py,0,"b'# get size of objects\n\nimport sys\n\nmy_list = [1,2,3,4]\n\nsys.getsizeof(my_list)\n# 52\n'"
python/language/sys/sys_version.py,0,"b""# loading modules specific to a Python version\n\nimport sys\n\nsys.version_info\n# sys.version_info(major=3, minor=6, micro=4, releaselevel='final', serial=0)\n\nif sys.version_info[0]>=3:\n    from html.parser import HTMLParser\nelse:\n    from HTMLParser import HTMLParser\n\n"""
python/language/threading/threading_1.py,0,"b'# Copying two files of 1GB each\n# with and without threading\n# threads can be used for parallel filesystem IO\n\n# load libraries\nimport time\nimport shutil\nimport threading\nfrom queue import Queue\n\n# without threading\nstart = time.time()\nshutil.copy(\'v1.7z\', \'wot_v1.7z\')\nshutil.copy(\'v2.7z\', \'wot_v2.7z\')\nprint(""Execution time without threading = {0:.5f}"".format(time.time() - start))\n\n# threading lock\nprint_lock = threading.Lock()\n\n# function to copy using shutil\ndef copy_op(file_data):\n    with print_lock:\n        print(""Starting thread : {}"".format(threading.current_thread().name))\n\n    mydata = threading.local()\n    mydata.ip, mydata.op = next(iter(file_data.items()))\n\n    shutil.copy(mydata.ip, mydata.op)\n\n    with print_lock:\n        print(""Finished thread : {}"".format(threading.current_thread().name))\n\n# process queuing\ndef process_queue():\n    while True:\n        file_data = compress_queue.get()\n        copy_op(file_data)\n        compress_queue.task_done()\n\n# A queue is used to store the files that need to be processed.\ncompress_queue = Queue()\n\n# file names mapping\noutput_names = [{\'v1.7z\': \'wt_v1.7z\'}, {\'v2.7z\': \'wt_v2.7z\'}]\n\n# threading\nfor i in range(2):\n    t = threading.Thread(target=process_queue)\n    t.daemon = True\n    t.start()\n\n# noting time\nstart = time.time()\n\nfor file_data in output_names:\n    compress_queue.put(file_data)\n\ncompress_queue.join()\n\nprint(""Execution time = {0:.5f}"".format(time.time() - start))\n\n# Output\n\n# Execution time without threading = 17.47281\n\n# Starting thread : Thread-1\n# Starting thread : Thread-2\n# Finished thread : Thread-2\n# Finished thread : Thread-1\n# Execution time = 5.71958\n'"
python/language/type/type.py,0,"b""# Python type\n# Metaclass\n\n>>> type(object)\n<class 'type'>\n\n>>> type(type)\n<class 'type'>\n\n>>> type.__class__\n<class 'type'>\n\n>>> type.__new__\n<built-in method __new__ of type object at 0x5074BDC0>\n\n>>> isinstance(object, object)\nTrue\n\n>>> isinstance(type, object)\nTrue\n\n>>> type(1)\n<class 'int'>\n\n>>> type(type(1))\n<class 'type'>\n\n>>> type(list())\n<class 'list'>\n\n>>> type(list)\n<class 'type'>\n"""
python/language/urllib/urllib_0.py,0,"b'import os\nimport urllib\n#import urllib2\nfrom bs4 import BeautifulSoup\n\nurl = ""https://github.com/praveentn""\nhtml = urllib.request.urlopen(url)\nsoup = BeautifulSoup(html)\n\nimgs = soup.findAll(""div"", {""class"":""h-card col-3 float-left pr-3""})\nfor img in imgs:\n    imgUrl = img.a[\'href\'].split(""imgurl="")[1]\n    urllib.request.urlretrieve(imgUrl, os.path.basename(imgUrl))\n\n'"
python/language/utilities/first_class_objects.py,0,"b""# functions are first class in Python\n\n>>> def my_fun(a,b):\n...     '''return bth power of a'''\n...     return a**b\n...\n>>>\n\n>>> my_fun\n<function my_fun at 0x007DC660>\n\n>>> funs = [my_fun,my_fun]\n\n>>> funs[0]\n<function my_fun at 0x007DC660>\n\n>>> funs[1]\n<function my_fun at 0x007DC660>\n\n>>> funs_v = [x(3,4) for x in funs]\n\n>>> funs_v\n[81, 81]\n\n>>>\n\n"""
python/language/virtualenv/setup_1.py,0,"b""'''\nsetting up virtualenv in Windows\nPython 3.6\npip3.6\n\nE:\\Python\\Scripts>pip3.6.exe install virtualenv\nCollecting virtualenv\n  Downloading https://files.pythonhosted.org/packages/b6/30/96a02b2287098b23b875bc8c2f58071c35d2efe84f747b64d523721dc2b5/virtualenv-16.0.0-py2.py3-none-any.whl (1.9MB)\n    100% |\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88| 1.9MB 99kB/s\nInstalling collected packages: virtualenv\nSuccessfully installed virtualenv-16.0.0\n\nE:\\Python\\Scripts>\n\n\n'''\n"""
python/language/wraps/decorator_1.py,0,"b'# using decorators in python\n\n# load wraps from functools\nfrom functools import wraps\n\n# decorator function\ndef decor(fun):\n    @wraps(fun)\n    def with_logging(*args, **kwargs):\n        print(fun.__name__ + "" is being called!"")\n        return fun(*args, **kwargs)\n    return with_logging\n\n\n# calling a decorator\n@decor\ndef adder(x):\n    """"""return sum()""""""\n    return x + x\n\n\n# call adder()\nadder(8)\nadder is being called!\n16\n'"
python/language/yield/sample_1.py,0,b'# using yield /generator\n\n>>> def generator_function(x):\n...     for i in range(x):\n...         yield i\n...\n>>>\n\n>>> for item in generator_function(7):\n...     print(item)\n...\n0\n1\n2\n3\n4\n5\n6\n>>>\n'
python/language/zfill/zfill_0.py,0,"b""# zfill pad\n\nn = '7'\nn.zfill(7)\n# '0000007'\n"""
python/language/utilities/extendedslice/step.py,0,"b'# extended slice or step or stride or jump\n# seq[i:j:k] is ""slice of s from i to j with step k""\n# seq[::k] is a sequence of each k-th item in the entire sequence\n\ns = \'hello\'\ns[-1] \n# o\n\ns[1:2]\n# e\n\ns[1:-2]\n# el\n\ns[:-1]\n# hell\n\ns[::]\n# hello\n\ns[::-1]\n# olleh\n\nfor x in range(100)[::10]:\n    print(x)\n\n#  0\n# 10\n# 20\n# 30\n# 40\n# 50\n# 60\n# 70\n# 80\n# 90\n\n'"
python/language/utilities/unpacking/intro_1.py,0,"b""# arguments unpacking simplified!!\n# *\n\na, b = range(2)\na\n0\n\nb\n1\n\na, b, *rest = range(5)\na\n0\n\nb\n1\n\nrest\n[2,3,4]\n\n# first and last lines\nwith open('file.name') as f:\n    first, *_, last = f.readlines()\n    \n\n# Refactor functions\ndef f(a, b, *args):\n    print(a, b, *args)\n\n# instead -> *args => a, b, *args\ndef f(*args):\n    a, b, *args = args\n    print(a, b, *args)\n \n"""
