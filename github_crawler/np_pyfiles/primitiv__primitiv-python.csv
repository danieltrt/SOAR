file_path,api_count,code
setup.py,2,"b'#!/usr/bin/env python3\n\nimport os\nimport sys\n\nfrom distutils.dir_util import copy_tree\nfrom setuptools.extension import Extension\n\nimport numpy as np\n\nfrom Cython.Build import build_ext\n\nVERSION = ""0.4.0""\n\nSUBMODULE_DIR = ""primitiv-core""\nEIGEN_DIR = ""eigen-headers""\n\nSUBMODULE_CMAKELIST = os.path.join(SUBMODULE_DIR, ""CMakeLists.txt"")\nEIGEN_HEADER_DIR = os.path.join(EIGEN_DIR, ""Eigen"")\n\nbuild_number = os.getenv(""PRIMITIV_PYTHON_BUILD_NUMBER"")\nif build_number is not None:\n    version_full = VERSION + ""."" + build_number\nelse:\n    version_full = VERSION\n\nbundle_eigen_headers = False\nif ""--bundle-eigen-headers"" in sys.argv:\n    i = sys.argv.index(""--bundle-eigen-headers"")\n    sys.argv.pop(i)\n    eigen_path = sys.argv.pop(i)\n    copy_tree(eigen_path, EIGEN_DIR)\n    bundle_eigen_headers = True\n\ndirname = os.path.dirname(os.path.abspath(__file__))\n\nif ""--no-build-core-library"" in sys.argv:\n    build_core = False\n    sys.argv.remove(""--no-build-core-library"")\nelse:\n    build_core = os.path.exists(os.path.join(dirname, SUBMODULE_CMAKELIST))\n\neigen_bundled_exists = os.path.exists(os.path.join(dirname, EIGEN_HEADER_DIR))\nif ""--disable-eigen"" in sys.argv:\n    enable_eigen = False\n    sys.argv.remove(""--disable-eigen"")\nelse:\n    enable_eigen = eigen_bundled_exists\n\nif build_core:\n    import skbuild\n    from skbuild import setup\nelse:\n    from setuptools import setup\n\nbundle_core_library = False\nif ""--bundle-core-library"" in sys.argv:\n    if not build_core:\n        print(""%s is not found"" % SUBMODULE_CMAKELIST, file=sys.stderr)\n        print("""", file=sys.stderr)\n        print(""Run the following command to download primitiv core library:"",\n              file=sys.stderr)\n        print(""  git submodule update --init"", file=sys.stderr)\n        print("""", file=sys.stderr)\n        sys.exit(1)\n    bundle_core_library = True\n    sys.argv.remove(""--bundle-core-library"")\n\nenable_cuda = False\nif ""--enable-cuda"" in sys.argv:\n    enable_cuda = True\n    sys.argv.remove(""--enable-cuda"")\n\nif ""--enable-eigen"" in sys.argv:\n    enable_eigen = True\n    sys.argv.remove(""--enable-eigen"")\n\nenable_opencl = False\nif ""--enable-opencl"" in sys.argv:\n    enable_opencl = True\n    sys.argv.remove(""--enable-opencl"")\n\n\ndef ext_common_args(*args, libraries=[], **kwargs):\n    if build_core:\n        libs = [""primitiv""]\n        libs.extend(libraries)\n        return Extension(\n            *args, **kwargs,\n            language=""c++"",\n            libraries=libs,\n            library_dirs=[os.path.join(skbuild.constants.CMAKE_INSTALL_DIR, ""lib"")],\n            include_dirs=[\n                np.get_include(),\n                os.path.join(skbuild.constants.CMAKE_INSTALL_DIR, ""include""),\n                os.path.join(dirname, ""primitiv""),\n            ],\n            extra_compile_args=[""-std=c++11""],\n        )\n    else:\n        return Extension(\n            *args, **kwargs,\n            language=""c++"",\n            libraries=[""primitiv""],\n            include_dirs=[\n                np.get_include(),\n                os.path.join(dirname, ""primitiv""),\n            ],\n            extra_compile_args=[""-std=c++11""],\n        )\n\n\next_modules = [\n    ext_common_args(""primitiv._shape"",\n                    sources=[""primitiv/_shape.pyx""]),\n    ext_common_args(""primitiv._tensor"",\n                    sources=[""primitiv/_tensor.pyx""]),\n    ext_common_args(""primitiv._device"",\n                    sources=[""primitiv/_device.pyx""]),\n    ext_common_args(""primitiv.devices._naive_device"",\n                    sources=[""primitiv/devices/_naive_device.pyx""]),\n    ext_common_args(""primitiv._parameter"",\n                    sources=[""primitiv/_parameter.pyx""]),\n    ext_common_args(""primitiv._initializer"",\n                    sources=[""primitiv/_initializer.pyx""]),\n    ext_common_args(""primitiv.initializers._initializer_impl"",\n                    sources=[""primitiv/initializers/_initializer_impl.pyx""]),\n    ext_common_args(""primitiv._graph"",\n                    sources=[""primitiv/_graph.pyx""]),\n    ext_common_args(""primitiv._optimizer"",\n                    sources=[""primitiv/_optimizer.pyx""]),\n    ext_common_args(""primitiv.optimizers._optimizer_impl"",\n                    sources=[""primitiv/optimizers/_optimizer_impl.pyx""]),\n    ext_common_args(""primitiv._function"",\n                    sources=[""primitiv/_function.pyx""]),\n    ext_common_args(""primitiv._model"",\n                    sources=[""primitiv/_model.pyx""]),\n    ext_common_args(""primitiv.config"",\n                    sources=[""primitiv/config.pyx""]),\n]\n\nif enable_cuda:\n    ext_modules.append(\n        ext_common_args(\n            ""primitiv.devices._cuda_device"",\n            libraries=[\n                ""cublas"",\n                ""cudart"",\n                ""curand"",\n                ""pthread"",\n                ""rt"",\n            ],\n            sources=[""primitiv/devices/_cuda_device.pyx""],\n        )\n    )\n\nif enable_eigen:\n    ext_modules.append(\n        ext_common_args(\n            ""primitiv.devices._eigen_device"",\n            sources=[""primitiv/devices/_eigen_device.pyx""],\n        )\n    )\n\nif enable_opencl:\n    ext_modules.append(\n        ext_common_args(\n            ""primitiv.devices._opencl_device"",\n            libraries=[\n                ""clblast"",\n                ""OpenCL"",\n            ],\n            sources=[""primitiv/devices/_opencl_device.pyx""],\n        )\n    )\n\nsetup_kwargs = {}\nif build_core:\n    setup_kwargs[""cmake_source_dir""] = SUBMODULE_DIR\n    setup_kwargs[""cmake_install_dir""] = ""./""\n    setup_kwargs[""setup_requires""] = [""scikit-build""]\n    setup_kwargs[""cmake_args""] = [""-DPRIMITIV_BUILD_STATIC_LIBRARY=ON""]\n    if sys.platform == ""darwin"":\n        # NOTE(vbkaisetsu):\n        # scikit-build adds -DCMAKE_OSX_DEPLOYMENT_TARGET with the default target if it does not\n        # set manually. However scikit-build does not check cmake_args argument of setup()\n        # for the target.\n        try:\n            cmake_args_pos = sys.argv.index(""--"")\n        except ValueError:\n            cmake_args_pos = len(sys.argv)\n            sys.argv.append(""--"")\n        sys.argv.insert(cmake_args_pos + 1, ""-DCMAKE_OSX_DEPLOYMENT_TARGET:STRING=10.12"")\n    if enable_cuda:\n        setup_kwargs[""cmake_args""].append(""-DPRIMITIV_USE_CUDA=ON"")\n    if enable_eigen:\n        setup_kwargs[""cmake_args""].append(""-DPRIMITIV_USE_EIGEN=ON"")\n        if eigen_bundled_exists:\n            setup_kwargs[""cmake_args""].append(""-DEIGEN3_INCLUDE_DIR=%s"" % os.path.join(dirname, EIGEN_DIR))\n    if enable_opencl:\n        setup_kwargs[""cmake_args""].append(""-DPRIMITIV_USE_OPENCL=ON"")\n\nwith open(os.path.join(dirname, ""MANIFEST.in""), ""w"") as fp:\n    print(""include README.md package_description.rst primitiv/py_optimizer.h"", file=fp)\n    print(""recursive-include primitiv *.pyx *.pxd"", file=fp)\n    if bundle_core_library:\n        print(""recursive-include %s *"" % SUBMODULE_DIR, file=fp)\n    if bundle_eigen_headers:\n        print(""include %s/COPYING.* %s/README.md"" % (EIGEN_DIR, EIGEN_DIR), file=fp)\n        print(""recursive-include %s *"" % EIGEN_HEADER_DIR, file=fp)\n\nsetup(\n    name=""primitiv"",\n    version=version_full,\n    description=""primitiv: A Neural Network Toolkit. (Python frontend)"",\n    long_description=open(os.path.join(dirname,\n                                       ""package_description.rst"")).read(),\n    url=""https://github.com/primitiv/primitiv-python"",\n    author=""primitiv developer group"",\n    author_email=""primitiv-developer-group@googlegroups.com"",\n    classifiers=[\n        ""Development Status :: 3 - Alpha"",\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Science/Research"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Operating System :: POSIX"",\n        ""Programming Language :: C++"",\n        ""Programming Language :: Python :: 3"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n    ],\n    ext_modules=ext_modules,\n    cmdclass={\'build_ext\': build_ext},\n    packages=[\n        ""primitiv"",\n        ""primitiv.devices"",\n        ""primitiv.initializers"",\n        ""primitiv.optimizers"",\n    ],\n    install_requires=[\n        ""cython"",\n        ""numpy"",\n    ],\n    **setup_kwargs,\n)\n'"
primitiv/__init__.py,0,"b'from primitiv._device import Device\nfrom primitiv._graph import Graph\nfrom primitiv._initializer import Initializer\nfrom primitiv._model import Model\nfrom primitiv._graph import Node\nfrom primitiv._parameter import Parameter\nfrom primitiv._shape import Shape\nfrom primitiv._tensor import Tensor\nfrom primitiv._optimizer import Optimizer\n\nfrom primitiv import devices\nfrom primitiv import initializers\nfrom primitiv._function import functions\nfrom primitiv._function import tensor_functions\nfrom primitiv import optimizers\nfrom primitiv import config\n\n# NOTE(vbkaisetsu):\n# Python uses unicode for string management, but C++ only uses raw byte arrays.\n# This code sets the current locale information for the default encoding to convert\n# strings between Python and C++.\nconfig.set_encoding()\n\n\n__all__ = [\n    ""Device"",\n    ""Graph"",\n    ""Initializer"",\n    ""Model"",\n    ""Node"",\n    ""Parameter"",\n    ""Shape"",\n    ""Tensor"",\n    ""Optimizer"",\n\n    ""devices"",\n    ""initializers"",\n    ""functions"",\n    ""tensor_functions"",\n    ""optimizers"",\n    ""config"",\n]\n'"
tests/__init__.py,0,b''
tests/check_arguments.py,4,"b'from primitiv import Device\nfrom primitiv import Graph\nfrom primitiv import Parameter\nfrom primitiv import Shape\nfrom primitiv import initializers as I\nfrom primitiv import functions as F\nfrom primitiv.devices import Naive\n\nimport numpy as np\nimport unittest\n\n\nclass ArgumentTest(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        pass\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def setUp(self):\n        self.device = Naive()\n        self.graph = Graph()\n        Device.set_default(self.device)\n        Graph.set_default(self.graph)\n        self.ndarray_data = [\n            np.array([\n                [ 1, 2, 3],\n                [ 4, 5, 6],\n                [ 7, 8, 9],\n                [10,11,12],\n            ], np.float32),\n            np.array([\n                [13,14,15],\n                [16,17,18],\n                [19,20,21],\n                [22,23,24],\n            ], np.float32),\n        ]\n        self.list_data = [\n             1.0,  4.0,  7.0, 10.0,  2.0,  5.0,  8.0, 11.0,  3.0,  6.0,  9.0, 12.0,\n            13.0, 16.0, 19.0, 22.0, 14.0, 17.0, 20.0, 23.0, 15.0, 18.0, 21.0, 24.0,\n        ]\n\n    def tearDown(self):\n        pass\n\n    def test_functions_input_argument(self):\n        # list[ndarray] w/o shape\n        x = F.input(self.ndarray_data)\n        self.assertEqual(x.to_list(), self.list_data)\n        self.assertEqual(x.shape(), Shape([4, 3], 2))\n\n        # ndarray w/o shape\n        x = F.input(self.ndarray_data[0])\n        self.assertEqual(x.to_list(), self.list_data[:12])\n        self.assertEqual(x.shape(), Shape([4, 3], 1))\n\n        # list[float] w/o shape\n        self.assertRaises(TypeError, lambda: F.input(self.list_data))\n\n        # list[float] w/ shape\n        x = F.raw_input(Shape([4, 3], 2), self.list_data)\n        self.assertEqual(x.to_list(), self.list_data)\n        self.assertEqual(x.shape(), Shape([4, 3], 2))\n\n    def test_Parameter_argument(self):\n        # no argument\n        p = Parameter()\n        self.assertFalse(p.valid())\n\n        # shape w/ Initializer\n        p = Parameter(Shape([4, 3]), I.Constant(1))\n        self.assertEqual(p.shape(), Shape([4, 3]))\n        self.assertEqual(p.value.to_list(), [1] * 12)\n'"
tests/graph_test.py,6,"b'import unittest\n\nfrom primitiv import devices as D\nfrom primitiv import functions as F\nfrom primitiv import initializers as I\nfrom primitiv import tensor_functions as tF\nfrom primitiv import Device\nfrom primitiv import Graph\nfrom primitiv import Node\nfrom primitiv import Parameter\nfrom primitiv import Shape\n\nimport numpy as np\n\n\nclass GraphTest(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.dev = D.Naive()\n        cls.dev2 = D.Naive()\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def test_GraphTest_CheckInvalidNode(self):\n        node = Node()\n        self.assertFalse(node.valid())\n        with self.assertRaises(RuntimeError):\n            node.graph()\n        with self.assertRaises(RuntimeError):\n            node.operator_id()\n        with self.assertRaises(RuntimeError):\n            node.value_id()\n        with self.assertRaises(RuntimeError):\n            node.shape()\n        with self.assertRaises(RuntimeError):\n            node.device()\n        with self.assertRaises(RuntimeError):\n            node.to_float()\n        with self.assertRaises(RuntimeError):\n            node.to_list()\n        with self.assertRaises(RuntimeError):\n            node.to_ndarrays()\n        with self.assertRaises(RuntimeError):\n            node.backward()\n\n    def test_GraphTest_CheckMultipleDevices(self):\n        Device.set_default(GraphTest.dev)\n\n        g = Graph()\n        Graph.set_default(g)\n\n        data1 = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]\n        data2 = [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]\n        data3 = [1, 2, 3, 4, 2, 3, 4, 5, 3, 4, 5, 6]\n        grad = [12, 1]\n        x1 = F.raw_input(Shape([2, 2], 3), data1)\n        x2 = F.raw_input(Shape([2, 2], 3), data2, GraphTest.dev2)\n        x3 = F.copy(x1, GraphTest.dev2) + x2\n        self.assertEqual(Shape([2, 2], 3), x3.shape())\n        self.assertIs(GraphTest.dev, x1.device())\n        self.assertIs(GraphTest.dev2, x2.device())\n        self.assertIs(GraphTest.dev2, x3.device())\n        g.forward(x3)\n        self.assertEqual(data1, g.forward(x1).to_list())\n        self.assertEqual(data1, x1.to_list())\n        self.assertEqual(data2, g.forward(x2).to_list())\n        self.assertEqual(data2, x2.to_list())\n        self.assertEqual(data3, g.forward(x3).to_list())\n        self.assertEqual(data3, x3.to_list())\n\n    def test_GraphTest_CheckInvalidMultipleDevices(self):\n        Device.set_default(GraphTest.dev)\n\n        g = Graph()\n        Graph.set_default(g)\n\n        dummy = [0] * 12\n        x1 = F.raw_input(Shape([2, 2], 3), dummy)\n        x2 = F.raw_input(Shape([2, 2], 3), dummy, GraphTest.dev2)\n        x3 = x1 + x2\n        with self.assertRaises(RuntimeError):\n            g.forward(x3)\n\n    def test_GraphTest_CheckClear(self):\n        Device.set_default(GraphTest.dev)\n\n        g = Graph()\n        Graph.set_default(g)\n\n        self.assertEqual(0, g.num_operators())\n\n        F.raw_input([], [1])\n        F.raw_input([], [1])\n        self.assertEqual(2, g.num_operators())\n\n        g.clear()\n        self.assertEqual(0, g.num_operators())\n\n        F.raw_input([], [1])\n        F.raw_input([], [1])\n        F.raw_input([], [1])\n        self.assertEqual(3, g.num_operators())\n\n        g.clear()\n        self.assertEqual(0, g.num_operators())\n\n        g.clear()\n        self.assertEqual(0, g.num_operators())\n\n    def test_GraphTest_CheckForwardBackward(self):\n        Device.set_default(GraphTest.dev)\n\n        g = Graph()\n        Graph.set_default(g)\n\n        data1 = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]\n        data3 = [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]\n\n        nodes = []\n        nodes.append(F.raw_input(Shape([2, 2], 3), data1))\n        nodes.append(F.ones([2, 2]))\n        nodes.append(F.raw_input(Shape([2, 2], 3), data3))\n        nodes.append(nodes[0] + nodes[1])\n        nodes.append(nodes[1] - nodes[2])\n        nodes.append(nodes[3] * nodes[4])\n        nodes.append(nodes[5] + 1)\n        nodes.append(F.sum(nodes[6], 0))\n        nodes.append(F.sum(nodes[7], 1))\n        nodes.append(F.batch.sum(nodes[8]))\n\n        self.assertEqual(10, len(nodes))\n        self.assertEqual(10, g.num_operators())\n\n        print(g.dump(""dot""))\n\n        expected_shapes = [\n            Shape([2, 2], 3), Shape([2, 2]), Shape([2, 2], 3),\n            Shape([2, 2], 3), Shape([2, 2], 3), Shape([2, 2], 3),\n            Shape([2, 2], 3),\n            Shape([1, 2], 3), Shape([], 3), Shape([]),\n        ]\n        for i, node in enumerate(nodes):\n            self.assertEqual(expected_shapes[i], node.shape())\n            self.assertIs(GraphTest.dev, node.device())\n\n        g.forward(nodes[-1])\n\n        expected_values = [\n            [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4],\n            [1, 1, 1, 1],\n            [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2],\n            [2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5],\n            [1, 1, 1, 1, 0, 0, 0, 0, -1, -1, -1, -1],\n            [2, 3, 4, 5, 0, 0, 0, 0, -2, -3, -4, -5],\n            [3, 4, 5, 6, 1, 1, 1, 1, -1, -2, -3, -4],\n            [7, 11, 2, 2, -3, -7],\n            [18, 4, -10],\n            [12],\n        ]\n        for i, node in enumerate(nodes):\n            val = g.forward(node)\n            self.assertTrue(val.valid())\n            self.assertEqual(expected_values[i], val.to_list())\n            self.assertEqual(expected_values[i], node.to_list())\n\n    def test_GraphTest_CheckXor(self):\n        Device.set_default(GraphTest.dev)\n\n        w1 = Parameter([2, 2], I.Constant(0))\n        w1.value += tF.raw_input([2, 2], [1, -1, 1, -1])\n        b1 = Parameter([2], I.Constant(0))\n        b1.value += tF.raw_input([2], [-1, -1])\n        w2 = Parameter([1, 2], I.Constant(0))\n        w2.value += tF.raw_input([1, 2], [1, 1])\n        b2 = Parameter([], I.Constant(0))\n        b2.value += tF.raw_input([], [1])\n\n        inputs = [1, 1, 1, -1, -1, 1, -1, -1]\n        outputs = [1, -1, -1, 1]\n\n        g = Graph()\n        Graph.set_default(g)\n\n        nodes = []\n\n        # sources\n        nodes.append(F.raw_input(Shape([2], 4), inputs))\n        nodes.append(F.parameter(w1))\n        nodes.append(F.parameter(b1))\n        nodes.append(F.parameter(w2))\n        nodes.append(F.parameter(b2))\n        # calculation\n        nodes.append(F.matmul(nodes[1], nodes[0]));\n        nodes.append(nodes[5] + nodes[2]);\n        nodes.append(F.tanh(nodes[6]));\n        nodes.append(F.matmul(nodes[3], nodes[7]));\n        nodes.append(nodes[8] + nodes[4]);\n        # losses\n        nodes.append(F.raw_input(Shape([], 4), outputs));\n        nodes.append(nodes[9] - nodes[10]);\n        nodes.append(nodes[11] * nodes[11]);\n        nodes.append(F.batch.sum(nodes[12]));\n\n        self.assertEqual(len(nodes), g.num_operators())\n        print(g.dump(""dot""))\n\n        g.forward(nodes[-1])\n\n        # Check all node values.\n        h1 = .76159416  # tanh(1)\n        h2 = .99505475  # tanh(3)\n        h3 = -.23346060 # tanh(1) - tanh(3)\n        h4 = -1.5231883 # -2 * tanh(1)\n        h5 = .76653940  # 1 + tanh(1) - tanh(3)\n        h6 = -.52318831 # 1 - 2 * tanh(1)\n        h7 = .47681169  # 2 - 2 * tanh(1)\n        expected_values = [\n            [1, 1, 1, -1, -1, 1, -1, -1],\n            [1, -1, 1, -1],\n            [-1, -1],\n            [1, 1],\n            [1],\n            [2, -2, 0, 0, 0, 0, -2, 2],\n            [1, -3, -1, -1, -1, -1, -3, 1],\n            [h1, -h2, -h1, -h1, -h1, -h1, -h2, h1],\n            [h3, h4, h4, h3],\n            [h5, h6, h6, h5],\n            [1, -1, -1, 1],\n            [h3, h7, h7, h3],\n            [h3 * h3, h7 * h7, h7 * h7, h3 * h3],\n            [2 * (h3 * h3 + h7 * h7)],\n        ]\n        for i, node in enumerate(nodes):\n            val = g.forward(nodes[i])\n            self.assertTrue(val.valid())\n            self.assertTrue(np.isclose(expected_values[i], val.to_list()).all())\n            self.assertTrue(np.isclose(expected_values[i], node.to_list()).all())\n\n    def test_GraphTest_CheckLSTM(self):\n        Device.set_default(GraphTest.dev)\n\n        pWix = Parameter([2, 2], I.Constant(0))\n        pWix.value += tF.raw_input([2, 2], [.3, .1, .5, .3])\n        pWfx = Parameter([2, 2], I.Constant(0))\n        pWfx.value += tF.raw_input([2, 2], [.4, .1, .5, .8])\n        pWox = Parameter([2, 2], I.Constant(0))\n        pWox.value += tF.raw_input([2, 2], [.5, .9, .9, .7])\n        pWjx = Parameter([2, 2], I.Constant(0))\n        pWjx.value += tF.raw_input([2, 2], [.2, .6, .9, .3])\n        pWih = Parameter([2, 2], I.Constant(0))\n        pWih.value += tF.raw_input([2, 2], [.2, .3, .3, .3])\n        pWfh = Parameter([2, 2], I.Constant(0))\n        pWfh.value += tF.raw_input([2, 2], [.8, .4, .8, .3])\n        pWoh = Parameter([2, 2], I.Constant(0))\n        pWoh.value += tF.raw_input([2, 2], [.6, .2, .2, .7])\n        pWjh = Parameter([2, 2], I.Constant(0))\n        pWjh.value += tF.raw_input([2, 2], [.6, .4, .9, .5])\n        pbi = Parameter([2], I.Constant(0))\n        pbf = Parameter([2], I.Constant(0))\n        pbo = Parameter([2], I.Constant(0))\n        pbj = Parameter([2], I.Constant(0))\n\n        g = Graph()\n        Graph.set_default(g)\n\n        x = F.raw_input(Shape([2], 2), [2, -2, 0.5, -0.5])\n        h = F.raw_input(Shape([2], 2), [-1, 1, -0.5, 0.5])\n        c = F.zeros([2])\n        Wfx = F.parameter(pWfx)\n        Wix = F.parameter(pWix)\n        Wox = F.parameter(pWox)\n        Wjx = F.parameter(pWjx)\n        Wih = F.parameter(pWih)\n        Wfh = F.parameter(pWfh)\n        Woh = F.parameter(pWoh)\n        Wjh = F.parameter(pWjh)\n        bi = F.parameter(pbi)\n        bf = F.parameter(pbf)\n        bo = F.parameter(pbo)\n        bj = F.parameter(pbj)\n\n        i = F.sigmoid(F.matmul(Wix, x) + F.matmul(Wih, h) + bi)\n        f = F.sigmoid(F.matmul(Wfx, x) + F.matmul(Wfh, h) + bf)\n        o = F.sigmoid(F.matmul(Wox, x) + F.matmul(Woh, h) + bo)\n        j = F.tanh(F.matmul(Wjx, x) + F.matmul(Wjh, h) + bj)\n        cc = f * c + i * j\n        hh = o * F.tanh(cc)\n\n        t = F.zeros([2])\n        diff = hh - t\n        loss = diff * diff\n        sum_loss = F.batch.sum(F.sum(loss, 0))\n\n        self.assertEqual(45, g.num_operators());\n\n        loss_tensor = g.forward(loss)\n        sum_loss_tensor = g.forward(sum_loss)\n        sum_loss.backward()\n\n        expected_losses = [\n            5.7667205e-03, 2.8605087e-02, 1.4819370e-03, 3.0073307e-03\n        ]\n        expected_sum_loss = sum(expected_losses)\n\n        self.assertTrue(np.isclose(expected_losses, loss_tensor.to_list()).all())\n        self.assertTrue(np.isclose(expected_losses, loss.to_list()).all())\n        self.assertAlmostEqual(expected_sum_loss, sum_loss_tensor.to_float())\n        self.assertAlmostEqual(expected_sum_loss, sum_loss.to_float())\n\n        def print_node_val(name, value):\n            print(""%s: value=%s"" % (name, value.to_ndarrays()))\n\n        print(""VALUES:"")\n        print_node_val(""x"", x)\n        print_node_val(""h"", h)\n        print_node_val(""c"", c)\n        print_node_val(""Wix"", Wix)\n        print_node_val(""Wfx"", Wfx)\n        print_node_val(""Wox"", Wox)\n        print_node_val(""Wjx"", Wjx)\n        print_node_val(""Wih"", Wih)\n        print_node_val(""Wfh"", Wfh)\n        print_node_val(""Woh"", Woh)\n        print_node_val(""Wjh"", Wjh)\n        print_node_val(""bi"", bi)\n        print_node_val(""bf"", bf)\n        print_node_val(""bo"", bo)\n        print_node_val(""bj"", bj)\n        print_node_val(""i"", i)\n        print_node_val(""f"", f)\n        print_node_val(""o"", o)\n        print_node_val(""j"", j)\n        print_node_val(""cc"", cc)\n        print_node_val(""hh"", hh)\n        print_node_val(""t"", t)\n        print_node_val(""diff"", diff)\n        print_node_val(""loss"", loss)\n\n    def test_GraphTest_CheckConcatLSTM(self):\n        Device.set_default(GraphTest.dev)\n\n        pWx = Parameter([8, 2], I.Constant(0))\n        pWx.value += tF.raw_input([8, 2], [\n            .3, .1, .4, .1, .5, .9, .2, .6,\n            .5, .3, .5, .8, .9, .7, .9, .3,\n        ])\n        pWh = Parameter([8, 2], I.Constant(0))\n        pWh.value += tF.raw_input([8, 2], [\n            .2, .3, .8, .4, .6, .2, .6, .4,\n            .3, .3, .8, .3, .2, .7, .9, .5,\n        ])\n        pb = Parameter([8], I.Constant(0))\n\n        g = Graph()\n        Graph.set_default(g)\n\n        x = F.raw_input(Shape([2], 2), [2, -2, 0.5, -0.5])\n        h = F.raw_input(Shape([2], 2), [-1, 1, -0.5, 0.5])\n        c = F.zeros([2])\n        Wx = F.parameter(pWx)\n        Wh = F.parameter(pWh)\n        b = F.parameter(pb)\n\n        u = F.matmul(Wx, x) + F.matmul(Wh, h) + b\n        i = F.sigmoid(F.slice(u, 0, 0, 2))\n        f = F.sigmoid(F.slice(u, 0, 2, 4))\n        o = F.sigmoid(F.slice(u, 0, 4, 6))\n        j = F.tanh(F.slice(u, 0, 6, 8))\n        cc = f * c + i * j\n        hh = o * F.tanh(cc)\n\n        t = F.zeros([2])\n        diff = hh - t\n        loss = diff * diff\n        sum_loss = F.batch.sum(F.sum(loss, 0))\n\n        self.assertEqual(28, g.num_operators())\n\n        loss_tensor = g.forward(loss)\n        sum_loss_tensor = g.forward(sum_loss)\n        sum_loss.backward()\n\n        expected_losses = [\n            5.7667205e-03, 2.8605087e-02, 1.4819370e-03, 3.0073307e-03\n        ]\n        expected_sum_loss = sum(expected_losses)\n\n        self.assertTrue(np.isclose(expected_losses, loss_tensor.to_list()).all())\n        self.assertTrue(np.isclose(expected_losses, loss.to_list()).all());\n        self.assertAlmostEqual(expected_sum_loss, sum_loss_tensor.to_float());\n        self.assertAlmostEqual(expected_sum_loss, sum_loss.to_float());\n\n        def print_node_val(name, value):\n            print(""%s: value=%s"" % (name, value.to_ndarrays()))\n\n        print(""VALUES:"")\n        print_node_val(""x"", x)\n        print_node_val(""h"", h)\n        print_node_val(""c"", c)\n        print_node_val(""Wx"", Wx)\n        print_node_val(""Wh"", Wh)\n        print_node_val(""b"", b)\n        print_node_val(""i"", i)\n        print_node_val(""f"", f)\n        print_node_val(""o"", o)\n        print_node_val(""j"", j)\n        print_node_val(""cc"", cc)\n        print_node_val(""hh"", hh)\n        print_node_val(""t"", t)\n        print_node_val(""diff"", diff)\n        print_node_val(""loss"", loss)\n'"
tests/instance_match.py,0,"b'from primitiv import Device\nfrom primitiv import Graph\nfrom primitiv import Parameter\nfrom primitiv import Shape\nfrom primitiv import Tensor\nfrom primitiv import initializers as I\nfrom primitiv import functions as F\nfrom primitiv import tensor_functions as tF\nfrom primitiv.devices import Naive\n\nimport numpy as np\nimport unittest\n\n\nclass ArgumentTest(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        pass\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def setUp(self):\n        self.device = Naive()\n        Device.set_default(self.device)\n        self.graph = Graph()\n        Graph.set_default(self.graph)\n\n    def tearDown(self):\n        pass\n\n    def test_device_instance(self):\n        tensor = tF.raw_input([], [0])\n        dev = tensor.device()\n        self.assertIs(dev, self.device)\n\n        node = F.raw_input([], [0])\n        dev = node.device()\n        self.assertIs(dev, self.device)\n\n        my_device = Naive()\n        self.assertIsNot(my_device, self.device)\n\n        node = F.raw_input([], [0], device=my_device)\n        dev = node.device()\n        self.assertIs(dev, my_device)\n\n        param = Parameter([], I.Constant(1))\n        dev = param.device()\n        self.assertIs(dev, self.device)\n\n    def test_graph_instance(self):\n        node = F.raw_input([], [0])\n        g = node.graph()\n        self.assertIs(g, self.graph)\n\n    def test_tensor_instance(self):\n        param = Parameter([], I.Constant(1))\n        t_origin = param.gradient\n        t = param.gradient\n        self.assertIs(t, t_origin)\n\n        t = Tensor(t_origin)\n        self.assertEqual(t.to_list(), t.to_list())\n        self.assertIsNot(t, t_origin)\n\n        t = t_origin\n        t *= 2\n        self.assertIs(t, t_origin)\n\n        t = t * 2\n        self.assertIsNot(t, t_origin)\n'"
tests/model.py,8,"b'from primitiv import Parameter, Device, Model, Shape\nfrom primitiv import devices as D\nfrom primitiv import initializers as I\nfrom primitiv import tensor_functions as tF\n\nimport numpy as np\n\nimport unittest\nimport tempfile\n\n\nclass TestModel(Model):\n    # NOTE(vbkaisetsu):\n    # Custom models can be created without calling super().__init__()\n    # function.\n    # This override suppresses calling __init__() function of\n    # the parent Model class to simulate the actual model implementation.\n    def __init__(self):\n        pass\n\n\nclass ModelTest(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        pass\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def setUp(self):\n        self.dev = D.Naive()\n        Device.set_default(self.dev)\n\n    def tearDown(self):\n         pass\n\n    def test_model_load_save(self):\n        submodel = TestModel()\n        sp1 = Parameter([2, 4], I.Constant(0))\n        sp1.value = tF.input(np.array([[0,1,2,3],[4,5,6,7]]))\n        sp2 = Parameter([2, 4], I.Constant(0))\n        sp2.value = tF.input(np.array([[9,8,7,6],[5,4,3,2]]))\n        submodel.add(""sp1"", sp1)\n        submodel.add(""sp2"", sp2)\n        parentmodel = TestModel()\n        p1 = Parameter([4, 2], I.Constant(0))\n        p1.value = tF.input(np.array([[0,1],[2,3],[4,5],[6,7]]))\n        p2 = Parameter([4, 2], I.Constant(0))\n        p2.value = tF.input(np.array([[9,8],[7,6],[5,4],[3,2]]))\n        parentmodel.add(""p1"", p1)\n        parentmodel.add(""p2"", p2)\n        parentmodel.add(""sub"", submodel)\n        submodel_load = TestModel()\n        sp1 = Parameter()\n        sp2 = Parameter()\n        submodel_load.add(""sp1"", sp1)\n        submodel_load.add(""sp2"", sp2)\n        parentmodel_load = TestModel()\n        p1 = Parameter()\n        p2 = Parameter()\n        parentmodel_load.add(""p1"", p1)\n        parentmodel_load.add(""p2"", p2)\n        parentmodel_load.add(""sub"", submodel_load)\n        with tempfile.NamedTemporaryFile() as fp:\n            parentmodel.save(fp.name)\n            parentmodel_load.load(fp.name)\n        self.assertTrue((parentmodel_load[""p1""].value.to_ndarrays()[0] == np.array([[0,1],[2,3],[4,5],[6,7]])).all())\n        self.assertTrue((parentmodel_load[""p2""].value.to_ndarrays()[0] == np.array([[9,8],[7,6],[5,4],[3,2]])).all())\n        self.assertTrue((parentmodel_load[""sub"", ""sp1""].value.to_ndarrays()[0] == np.array([[0,1,2,3],[4,5,6,7]])).all())\n        self.assertTrue((parentmodel_load[""sub"", ""sp2""].value.to_ndarrays()[0] == np.array([[9,8,7,6],[5,4,3,2]])).all())\n\n    def test_model_parameter(self):\n        model = Model()\n        param = Parameter()\n        model.add(""p"", param)\n        self.assertIs(model[""p""], param)\n        self.assertIs(model[(""p"",)], param)\n\n    def test_model_parameter_deep(self):\n        model1 = Model()\n        model2 = Model()\n        model1.add(""m2"", model2)\n        model3 = Model()\n        model2.add(""m3"", model3)\n        param = Parameter()\n        model3.add(""p"", param)\n        self.assertIs(model1[""m2"", ""m3"", ""p""], param)\n        self.assertIs(model1[""m2""][""m3""][""p""], param)\n\n    def test_model_submodel(self):\n        model1 = Model()\n        model2 = Model()\n        model1.add(""m"", model2)\n        self.assertIs(model1[""m""], model2)\n\n    def test_model_submodel_deep(self):\n        model1 = Model()\n        model2 = Model()\n        model1.add(""m2"", model2)\n        model3 = Model()\n        model2.add(""m3"", model3)\n        model4 = Model()\n        model3.add(""m4"", model4)\n        self.assertIs(model1[""m2"", ""m3"", ""m4""], model4)\n        self.assertIs(model1[""m2""][""m3""][""m4""], model4)\n\n    def test_model_invalid_operation(self):\n        model1 = Model()\n        model2 = Model()\n        model1.add(""m"", model2)\n        param = Parameter()\n        model1.add(""p"", param)\n        with self.assertRaises(TypeError) as e:\n            model1[""notfound""]\n        self.assertEqual(str(e.exception), ""\'name\' is not a name of neither parameter nor submodel"")\n        with self.assertRaises(TypeError):\n            del model1[""p""]\n        with self.assertRaises(TypeError):\n            del model1[""m""]\n        with self.assertRaises(TypeError):\n            del model1[0]\n        with self.assertRaises(TypeError):\n            model1[(0, 1)]\n        with self.assertRaises(TypeError):\n            model1[[0, 1]]\n        model3 = TestModel()\n        model3.p = Parameter()\n        model3.m = TestModel()\n        model3.a = ""test""\n        del model3.a\n        self.assertNotIn(""a"", model3.__dict__)\n        with self.assertRaises(TypeError):\n            del model3.p\n        self.assertIn(""p"", model3.__dict__)\n        with self.assertRaises(TypeError):\n            del model3.m\n        self.assertIn(""m"", model3.__dict__)\n\n    def test_model_get_all_parameters(self):\n        submodel = TestModel()\n        sp1 = Parameter()\n        sp2 = Parameter()\n        submodel.add(""sp1"", sp1)\n        submodel.add(""sp2"", sp2)\n        parentmodel = TestModel()\n        p1 = Parameter()\n        p2 = Parameter()\n        sub = submodel\n        parentmodel.add(""p1"", p1)\n        parentmodel.add(""p2"", p2)\n        parentmodel.add(""sub"", sub)\n        params = parentmodel.get_all_parameters()\n        self.assertIs(params[(""p1"",)], p1)\n        self.assertIs(params[(""p2"",)], p2)\n        self.assertIs(params[(""sub"", ""sp1"")], sp1)\n        self.assertIs(params[(""sub"", ""sp2"")], sp2)\n\n    def test_model_setattr(self):\n        model = TestModel()\n        model.p1 = Parameter()\n        model.p2 = Parameter()\n        model.p3 = Parameter()\n        model.m1 = TestModel()\n        model.m2 = TestModel()\n        model.m3 = TestModel()\n        self.assertIs(model[""p1""], model.p1)\n        self.assertIs(model[""p2""], model.p2)\n        self.assertIs(model[""p3""], model.p3)\n        model.p4 = Parameter()\n        self.assertIs(model[""m1""], model.m1)\n        self.assertIs(model[""m2""], model.m2)\n        self.assertIs(model[""m3""], model.m3)\n        model.m4 = TestModel()\n        self.assertIs(model[""p4""], model.p4)\n        self.assertIs(model[""m4""], model.m4)\n'"
tests/model_test.py,0,"b'import tempfile\nimport unittest\n\nfrom primitiv import Device\nfrom primitiv import Model\nfrom primitiv import Parameter\nfrom primitiv import Shape\nfrom primitiv.devices import Naive\nfrom primitiv import initializers as I\nfrom primitiv import tensor_functions as tF\n\n\nclass ModelTest(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.device = Naive()\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def setUp(self):\n        Device.set_default(ModelTest.device)\n\n    def tearDown(self):\n        pass\n\n    def test_ModelTest_CheckAddParameter(self):\n        m = Model()\n        p1 = Parameter()\n        p2 = Parameter()\n        p3 = Parameter()\n\n        m.add(""p1"", p1)\n        m.add(""p1"", p1)\n        with self.assertRaises(RuntimeError):\n            m.add(""x"", p1)\n\n        with self.assertRaises(RuntimeError):\n            m.add(""p1"", p2)\n        m.add(""p2"", p2)\n        m.add(""p2"", p2)\n        with self.assertRaises(RuntimeError):\n            m.add(""x"", p2)\n\n        with self.assertRaises(RuntimeError):\n            m.add(""p1"", p3)\n        with self.assertRaises(RuntimeError):\n            m.add(""p2"", p3)\n        m.add(""p3"", p3)\n        m.add(""p3"", p3)\n        with self.assertRaises(RuntimeError):\n            m.add(""x"", p3)\n\n    def test_ModelTest_CheckAddSubmodel(self):\n        m = Model()\n        sm1 = Model()\n        sm2 = Model()\n        p1 = Parameter()\n        p2 = Parameter()\n\n        m.add(""p1"", p1)\n        m.add(""sm1"", sm1)\n        m.add(""sm1"", sm1)\n        with self.assertRaises(RuntimeError):\n            m.add(""x"", sm1)\n\n        with self.assertRaises(RuntimeError):\n            m.add(""p1"", p2)\n        with self.assertRaises(RuntimeError):\n            m.add(""sm1"", p2)\n        with self.assertRaises(RuntimeError):\n            m.add(""p1"", sm2)\n        with self.assertRaises(RuntimeError):\n            m.add(""sm1"", sm2)\n\n        m.add(""p2"", p2)\n        m.add(""sm2"", sm2)\n        m.add(""sm2"", sm2)\n        with self.assertRaises(RuntimeError):\n            m.add(""x"", sm2)\n\n    def test_ModelTest_CheckAddSubmodelCycle(self):\n        m1 = Model()\n        m2 = Model()\n        m3 = Model()\n        m4 = Model()\n\n        with self.assertRaises(RuntimeError):\n            m1.add(""self"", m1)\n\n        m1.add(""m2"", m2)\n        with self.assertRaises(RuntimeError):\n            m2.add(""m1"", m1)\n\n        m2.add(""m3"", m3)\n        with self.assertRaises(RuntimeError):\n            m3.add(""m1"", m1)\n        with self.assertRaises(RuntimeError):\n            m3.add(""m2"", m2)\n\n        m2.add(""m4"", m4)\n        with self.assertRaises(RuntimeError):\n            m4.add(""m1"", m1)\n        with self.assertRaises(RuntimeError):\n            m4.add(""m2"", m2)\n\n        m4.add(""m3"", m3)\n\n    def test_ModelTest_CheckGetParameteer(self):\n        m = Model()\n        sm = Model()\n        p1 = Parameter()\n        p2 = Parameter()\n        p3 = Parameter()\n        m.add(""p1"", p1)\n        m.add(""p2"", p2)\n        sm.add(""p3"", p3)\n        m.add(""sm"", sm)\n\n        self.assertIs(p1, m[""p1""])\n        self.assertIs(p2, m[""p2""])\n        with self.assertRaises(TypeError):\n            m[""p3""]\n        self.assertIs(sm, m[""sm""])\n        with self.assertRaises(TypeError):\n            m[""x""]\n\n    def test_ModelTest_CheckGetParameterRecursiveByTuple(self):\n        m = Model()\n        sm = Model()\n        p1 = Parameter()\n        p2 = Parameter()\n        p3 = Parameter()\n        m.add(""p1"", p1)\n        sm.add(""p2"", p2)\n        sm.add(""p3"", p3)\n        m.add(""sm"", sm)\n\n        self.assertIs(p1, m[""p1""])\n        self.assertIs(p2, m[""sm"", ""p2""])\n        self.assertIs(p3, m[""sm"", ""p3""])\n        self.assertIs(p2, sm[""p2""])\n        self.assertIs(p3, sm[""p3""])\n        with self.assertRaises(TypeError):\n            m[""p2""]\n        with self.assertRaises(TypeError):\n            m[""p3""]\n        m[""sm""]\n        with self.assertRaises(TypeError):\n            m[""sm"", ""p1""]\n        with self.assertRaises(TypeError):\n            sm[""p1""]\n        with self.assertRaises(TypeError):\n            m[""x""]\n\n    def test_ModelTest_CheckGetSubmodel(self):\n        m = Model()\n        sm1 = Model()\n        sm2 = Model()\n        ssm = Model()\n        p = Parameter()\n        m.add(""p"", p)\n        m.add(""sm1"", sm1)\n        m.add(""sm2"", sm2)\n        sm1.add(""ssm"", ssm)\n\n        self.assertIs(sm1, m[""sm1""]);\n        self.assertIs(sm2, m[""sm2""]);\n        with self.assertRaises(TypeError):\n            m[""ssm""]\n        m[""p""]\n\n    def test_ModelTest_CheckGetSubmodelRecursiveByTuple(self):\n        m = Model()\n        sm1 = Model()\n        sm2 = Model()\n        ssm = Model()\n        p = Parameter()\n        m.add(""p"", p)\n        m.add(""sm1"", sm1)\n        m.add(""sm2"", sm2)\n        sm1.add(""ssm"", ssm)\n\n        self.assertIs(sm1, m[""sm1""]);\n        self.assertIs(sm2, m[""sm2""]);\n        self.assertIs(ssm, m[""sm1"", ""ssm""]);\n        self.assertIs(ssm, sm1[""ssm""]);\n        m[""p""]\n        with self.assertRaises(TypeError):\n            m[""ssm""]\n        with self.assertRaises(TypeError):\n            m[""sm2"", ""ssm""]\n        with self.assertRaises(TypeError):\n            m[""x""]\n\n    def test_ModelTest_CheckGetAllParameters(self):\n        m = Model()\n        p1 = Parameter()\n        p2 = Parameter()\n        p3 = Parameter()\n        m.add(""p1"", p1)\n        m.add(""p2"", p2)\n        m.add(""p3"", p3)\n        params = m.get_all_parameters()\n        self.assertEqual(3, len(params))\n        self.assertIsInstance(params, dict)\n        self.assertIs(p1, params[(""p1"",)])\n        self.assertIs(p2, params[(""p2"",)])\n        self.assertIs(p3, params[(""p3"",)])\n\n    def test_ModelTest_CheckGetAllParametersWithSubmodels(self):\n        m1 = Model()\n        m2 = Model()\n        m3 = Model()\n        p1 = Parameter()\n        p2 = Parameter()\n        p3 = Parameter()\n        m1.add(""p"", p1)\n        m2.add(""p"", p2)\n        m3.add(""p"", p3)\n        m1.add(""sm"", m2)\n        m2.add(""sm"", m3)\n\n        params1 = m1.get_all_parameters()\n        self.assertEqual(3, len(params1))\n        self.assertIsInstance(params1, dict)\n        self.assertIs(p1, params1[(""p"",)])\n        self.assertIs(p2, params1[(""sm"", ""p"",)])\n        self.assertIs(p3, params1[(""sm"", ""sm"", ""p"",)])\n\n        params2 = m2.get_all_parameters()\n        self.assertEqual(2, len(params2))\n        self.assertIsInstance(params2, dict)\n        self.assertIs(p2, params2[(""p"",)])\n        self.assertIs(p3, params2[(""sm"", ""p"",)])\n\n        params3 = m3.get_all_parameters()\n        self.assertEqual(1, len(params3))\n        self.assertIsInstance(params3, dict)\n        self.assertIs(p3, params3[(""p"",)])\n\n    def test_ModelTest_CheckGetTrainableParameters(self):\n        m = Model()\n        p1 = Parameter()\n        p2 = Parameter()\n        p3 = Parameter()\n        m.add(""p1"", p1)\n        m.add(""p2"", p2)\n        m.add(""p3"", p3)\n        params = m.get_trainable_parameters()\n        self.assertEqual(3, len(params))\n        self.assertIsInstance(params, dict)\n        self.assertIs(p1, params[(""p1"",)]);\n        self.assertIs(p2, params[(""p2"",)]);\n        self.assertIs(p3, params[(""p3"",)]);\n\n    def test_ModelTest_CheckGetTrainableParametersWithSubmodels(self):\n        m1 = Model()\n        m2 = Model()\n        m3 = Model()\n        p1 = Parameter()\n        p2 = Parameter()\n        p3 = Parameter()\n        m1.add(""p"", p1)\n        m2.add(""p"", p2)\n        m3.add(""p"", p3)\n        m1.add(""sm"", m2)\n        m2.add(""sm"", m3)\n\n        params1 = m1.get_trainable_parameters()\n        self.assertEqual(3, len(params1))\n        self.assertIsInstance(params1, dict)\n        self.assertIs(p1, params1[(""p"",)])\n        self.assertIs(p2, params1[(""sm"", ""p"",)])\n        self.assertIs(p3, params1[(""sm"", ""sm"", ""p"",)])\n\n        params2 = m2.get_trainable_parameters()\n        self.assertEqual(2, len(params2))\n        self.assertIsInstance(params2, dict)\n        self.assertIs(p2, params2[(""p"",)])\n        self.assertIs(p3, params2[(""sm"", ""p"",)])\n\n        params3 = m3.get_trainable_parameters()\n        self.assertEqual(1, len(params3))\n        self.assertIsInstance(params3, dict)\n        self.assertIs(p3, params3[(""p"",)])\n\n\n    def test_ModelTest_CheckSaveLoad_Same(self):\n        shape = Shape([2, 2])\n        values1 = [1, 2, 3, 4]\n        values2 = [5, 6, 7, 8]\n        tmp = tempfile.NamedTemporaryFile()\n\n        m1 = Model()\n        m2 = Model()\n        p1 = Parameter(shape, I.Constant(0))\n        p1.value += tF.raw_input(shape, values1)\n        p2 = Parameter(shape, I.Constant(0))\n        p2.value += tF.raw_input(shape, values2)\n        m1.add(""p"", p1)\n        m2.add(""p"", p2)\n        m1.add(""sm"", m2)\n\n        m1.save(tmp.name)\n\n        m1 = Model()\n        m2 = Model()\n        p1 = Parameter()\n        p2 = Parameter()\n        m1.add(""p"", p1)\n        m2.add(""p"", p2)\n        m1.add(""sm"", m2)\n\n        m1.load(tmp.name)\n\n        self.assertTrue(p1.valid())\n        self.assertTrue(p2.valid())\n        self.assertEqual(shape, p1.shape())\n        self.assertEqual(shape, p2.shape())\n        self.assertEqual(values1, p1.value.to_list())\n        self.assertEqual(values2, p2.value.to_list())\n\n    def test_ModelTest_CheckSaveLoad_Insufficient(self):\n        shape = Shape([2, 2])\n        values1 = [1, 2, 3, 4]\n        values2 = [5, 6, 7, 8]\n        tmp = tempfile.NamedTemporaryFile()\n\n        m1 = Model()\n        m2 = Model()\n        p1 = Parameter(shape, I.Constant(0))\n        p1.value += tF.raw_input(shape, values1)\n        p2 = Parameter(shape, I.Constant(0))\n        p2.value += tF.raw_input(shape, values2)\n        m1.add(""p"", p1)\n        m2.add(""p"", p2)\n        m1.add(""sm"", m2)\n\n        m1.save(tmp.name)\n\n        m1 = Model()\n        m2 = Model()\n        p1 = Parameter()\n        m1.add(""p"", p1)\n        m1.add(""sm"", m2)\n\n        with self.assertRaises(RuntimeError):\n            m1.load(tmp.name)\n\n    def test_ModelTest_CheckSaveLoad_Excessive(self):\n        shape = Shape([2, 2])\n        values1 = [1, 2, 3, 4]\n        values2 = [5, 6, 7, 8]\n        tmp = tempfile.NamedTemporaryFile()\n\n        m1 = Model()\n        m2 = Model()\n        p1 = Parameter(shape, I.Constant(0))\n        p1.value += tF.raw_input(shape, values1)\n        p2 = Parameter(shape, I.Constant(0))\n        p2.value += tF.raw_input(shape, values2)\n        m1.add(""p"", p1)\n        m2.add(""p"", p2)\n        m1.add(""sm"", m2)\n\n        m1.save(tmp.name)\n\n        m1 = Model()\n        m2 = Model()\n        p1 = Parameter()\n        p2 = Parameter()\n        p3 = Parameter()\n        m1.add(""p"", p1)\n        m2.add(""p"", p2)\n        m2.add(""pp"", p3)\n        m1.add(""sm"", m2)\n\n        m1.load(tmp.name)\n\n        self.assertTrue(p1.valid())\n        self.assertTrue(p2.valid())\n        self.assertFalse(p3.valid())\n        self.assertEqual(shape, p1.shape())\n        self.assertEqual(shape, p2.shape())\n        self.assertEqual(values1, p1.value.to_list())\n        self.assertEqual(values2, p2.value.to_list())\n\n    def test_ModelTest_CheckSaveLoadWithStats(self):\n        shape = Shape([2, 2])\n        values1 = [1, 2, 3, 4]\n        values2 = [5, 6, 7, 8]\n        stats1 = [10, 20, 30, 40]\n        stats2 = [50, 60, 70, 80]\n        tmp = tempfile.NamedTemporaryFile()\n\n        m1 = Model()\n        m2 = Model()\n        p1 = Parameter(shape, I.Constant(0))\n        p1.value += tF.raw_input(shape, values1)\n        p2 = Parameter(shape, I.Constant(0))\n        p2.value += tF.raw_input(shape, values2)\n        p1.add_stats(""a"", shape)\n        p2.add_stats(""b"", shape)\n        p1.stats[""a""].reset_by_vector(stats1);\n        p2.stats[""b""].reset_by_vector(stats2);\n        m1.add(""p"", p1)\n        m2.add(""p"", p2)\n        m1.add(""sm"", m2)\n\n        m1.save(tmp.name)\n\n        m1 = Model()\n        m2 = Model()\n        p1 = Parameter()\n        p2 = Parameter()\n        m1.add(""p"", p1)\n        m2.add(""p"", p2)\n        m1.add(""sm"", m2)\n\n        m1.load(tmp.name)\n\n        self.assertTrue(p1.valid())\n        self.assertTrue(p2.valid())\n        self.assertEqual(shape, p1.shape())\n        self.assertEqual(shape, p2.shape())\n        self.assertEqual(values1, p1.value.to_list())\n        self.assertEqual(values2, p2.value.to_list())\n        self.assertTrue(""a"" in p1.stats)\n        self.assertTrue(""b"" in p2.stats)\n        self.assertEqual(stats1, p1.stats[""a""].to_list())\n        self.assertEqual(stats2, p2.stats[""b""].to_list())\n\n    def test_ModelTest_CheckSaveWithoutStats(self):\n        shape = Shape([2, 2])\n        values1 = [1, 2, 3, 4]\n        values2 = [5, 6, 7, 8]\n        stats1 = [10, 20, 30, 40]\n        stats2 = [50, 60, 70, 80]\n        tmp = tempfile.NamedTemporaryFile()\n\n        m1 = Model()\n        m2 = Model()\n        p1 = Parameter(shape, I.Constant(0))\n        p1.value += tF.raw_input(shape, values1)\n        p2 = Parameter(shape, I.Constant(0))\n        p2.value += tF.raw_input(shape, values2)\n        p1.add_stats(""a"", shape)\n        p2.add_stats(""b"", shape)\n        p1.stats[""a""].reset_by_vector(stats1)\n        p2.stats[""b""].reset_by_vector(stats2)\n        m1.add(""p"", p1)\n        m2.add(""p"", p2)\n        m1.add(""sm"", m2)\n\n        m1.save(tmp.name, False)\n\n        m1 = Model()\n        m2 = Model()\n        p1 = Parameter()\n        p2 = Parameter()\n        m1.add(""p"", p1)\n        m2.add(""p"", p2)\n        m1.add(""sm"", m2)\n\n        m1.load(tmp.name)\n\n        self.assertTrue(p1.valid())\n        self.assertTrue(p2.valid())\n        self.assertEqual(shape, p1.shape())\n        self.assertEqual(shape, p2.shape())\n        self.assertEqual(values1, p1.value.to_list())\n        self.assertEqual(values2, p2.value.to_list())\n        self.assertFalse(""a"" in p1.stats)\n        self.assertFalse(""b"" in p2.stats)\n\n    def test_ModelTest_CheckLoadWithoutStats(self):\n        shape = Shape([2, 2])\n        values1 = [1, 2, 3, 4]\n        values2 = [5, 6, 7, 8]\n        stats1 = [10, 20, 30, 40]\n        stats2 = [50, 60, 70, 80]\n        tmp = tempfile.NamedTemporaryFile()\n\n        m1 = Model()\n        m2 = Model()\n        p1 = Parameter(shape, I.Constant(0))\n        p1.value += tF.raw_input(shape, values1)\n        p2 = Parameter(shape, I.Constant(0))\n        p2.value += tF.raw_input(shape, values2)\n        p1.add_stats(""a"", shape)\n        p2.add_stats(""b"", shape)\n        p1.stats[""a""].reset_by_vector(stats1)\n        p2.stats[""b""].reset_by_vector(stats2)\n        m1.add(""p"", p1)\n        m2.add(""p"", p2)\n        m1.add(""sm"", m2)\n\n        m1.save(tmp.name)\n\n        m1 = Model()\n        m2 = Model()\n        p1 = Parameter()\n        p2 = Parameter()\n        m1.add(""p"", p1)\n        m2.add(""p"", p2)\n        m1.add(""sm"", m2)\n\n        m1.load(tmp.name, False)\n\n        self.assertTrue(p1.valid())\n        self.assertTrue(p2.valid())\n        self.assertEqual(shape, p1.shape())\n        self.assertEqual(shape, p2.shape())\n        self.assertEqual(values1, p1.value.to_list())\n        self.assertEqual(values2, p2.value.to_list())\n        self.assertFalse(""a"" in p1.stats)\n        self.assertFalse(""b"" in p2.stats)\n'"
tests/ndarray_order.py,4,"b'from primitiv import Device\nfrom primitiv import Graph\nfrom primitiv import functions as F\nfrom primitiv.devices import Naive\n\nimport numpy as np\nimport unittest\n\n\nclass ArrayOrderingTest(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        pass\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def setUp(self):\n        self.device = Naive()\n        self.graph = Graph()\n        Device.set_default(self.device)\n        Graph.set_default(self.graph)\n        self.input_data = [\n            np.array([\n                [ 1, 2, 3],\n                [ 4, 5, 6],\n                [ 7, 8, 9],\n            ], np.float32),\n            np.array([\n                [11,12,13],\n                [14,15,16],\n                [17,18,19],\n            ], np.float32),\n        ]\n        self.list_expected = [\n             1.0,  4.0,  7.0,  2.0,  5.0,  8.0,  3.0,  6.0,  9.0,\n            11.0, 14.0, 17.0, 12.0, 15.0, 18.0, 13.0, 16.0, 19.0,\n        ]\n\n    def tearDown(self):\n        pass\n\n    def test_input_ndarrays(self):\n        x = F.input(self.input_data)\n        self.assertEqual(x.to_list(), self.list_expected)\n        self.assertTrue((x.to_ndarrays()[0] == self.input_data[0]).all())\n        self.assertTrue((x.to_ndarrays()[1] == self.input_data[1]).all())\n'"
tests/node_functions.py,25,"b'from primitiv import Device\nfrom primitiv import Graph\nfrom primitiv import functions as F\nfrom primitiv.devices import Naive\n\nimport numpy as np\nimport unittest\n\n\nclass NodeFunctionsTest(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        pass\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def setUp(self):\n        self.device = Naive()\n        self.graph = Graph()\n        Device.set_default(self.device)\n        Graph.set_default(self.graph)\n        self.a = np.array([[1, 2], [3, 4]], np.float32)\n        self.b = np.array([[1, 1], [4, 8]], np.float32)\n\n    def tearDown(self):\n        pass\n\n    def test_node_pos(self):\n        x = F.input(self.a)\n        y = F.input(self.b)\n        self.assertTrue(((+x).to_ndarrays()[0] == self.a).all())\n\n    def test_node_neg(self):\n        x = F.input(self.a)\n        y = F.input(self.b)\n        self.assertTrue(((-x).to_ndarrays()[0] == -self.a).all())\n\n    def test_node_add(self):\n        x = F.input(self.a)\n        y = F.input(self.b)\n        self.assertTrue(((x + y).to_ndarrays()[0] == np.array([[2, 3], [7, 12]])).all())\n        self.assertTrue(((x + 2).to_ndarrays()[0] == np.array([[3, 4], [5, 6]])).all())\n        self.assertTrue(((2 + x).to_ndarrays()[0] == np.array([[3, 4], [5, 6]])).all())\n\n    def test_node_sub(self):\n        x = F.input(self.a)\n        y = F.input(self.b)\n        self.assertTrue(((x - y).to_ndarrays()[0] == np.array([[0, 1], [-1, -4]])).all())\n        self.assertTrue(((x - 2).to_ndarrays()[0] == np.array([[-1, 0], [1, 2]])).all())\n        self.assertTrue(((2 - x).to_ndarrays()[0] == np.array([[1, 0], [-1, -2]])).all())\n\n    def test_node_mul(self):\n        x = F.input(self.a)\n        y = F.input(self.b)\n        self.assertTrue(((x * y).to_ndarrays()[0] == np.array([[1, 2], [12, 32]])).all())\n        self.assertTrue(((x * 2).to_ndarrays()[0] == np.array([[2, 4], [6, 8]])).all())\n        self.assertTrue(((2 * x).to_ndarrays()[0] == np.array([[2, 4], [6, 8]])).all())\n\n    def test_node_matmul(self):\n        x = F.input(self.a)\n        y = F.input(self.b)\n        self.assertTrue(((x @ y).to_ndarrays()[0] == np.array([[9, 17], [19, 35]])).all())\n        self.assertRaises(TypeError, lambda: x @ 2)\n        self.assertRaises(TypeError, lambda: 2 @ x)\n\n    def test_node_truediv(self):\n        x = F.input(self.a)\n        y = F.input(self.b)\n        self.assertTrue(((x / y).to_ndarrays()[0] == np.array([[1, 2], [0.75, 0.5]])).all())\n        self.assertTrue(((x / 2).to_ndarrays()[0] == np.array([[0.5, 1], [1.5, 2]])).all())\n        self.assertTrue(((2 / y).to_ndarrays()[0] == np.array([[2, 2], [0.5, 0.25]])).all())\n\n    def test_node_pow(self):\n        x = F.input(self.a)\n        y = F.input(self.b)\n        self.assertTrue(np.isclose((x ** y).to_ndarrays()[0], np.array([[1, 2], [81, 65536]])).all())\n        self.assertTrue(np.isclose((x ** 2).to_ndarrays()[0], np.array([[1, 4], [9, 16]])).all())\n        self.assertTrue(np.isclose((2 ** x).to_ndarrays()[0], np.array([[2, 4], [8, 16]])).all())\n        self.assertTrue(np.isclose((x ** -2).to_ndarrays()[0], np.array([[1, 1/4], [1/9, 1/16]])).all())\n        input_arr = np.array([1, -1, 3, -3, 5, -5])\n        x = F.input(input_arr)\n        self.assertTrue(((x ** 6).to_ndarrays()[0] == np.array([1, 1, 729, 729, 15625, 15625])).all())\n        self.assertTrue(((x ** 9).to_ndarrays()[0] == np.array([1, -1, 19683, -19683, 1953125, -1953125])).all())\n        input_arr = np.array([1, -1])\n        x = F.input(input_arr)\n        self.assertTrue(((x ** 0x7fffffff).to_ndarrays()[0] == np.array([1, -1])).all())\n        self.assertTrue(((x ** -0x80000000).to_ndarrays()[0] == np.array([1, 1])).all())\n\n        self.assertRaises(TypeError, lambda: pow(x, y, 2))\n'"
tests/optimizer.py,0,"b'from primitiv import Device\nfrom primitiv import devices as D\nfrom primitiv import initializers as I\nfrom primitiv import Model\nfrom primitiv import optimizers as O\nfrom primitiv import Parameter\nfrom primitiv import tensor_functions as tF\n\nimport unittest\n\n\nclass TestModel(Model):\n    def __init__(self):\n        self.param = Parameter([5], I.Constant(0))\n        self.param.gradient = tF.raw_input([5], [1, 2, 3, 4, 5])\n\n\nclass Optimizer(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        pass\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def setUp(self):\n        self.device = D.Naive()\n        Device.set_default(self.device)\n\n    def tearDown(self):\n        pass\n\n    def test_optimizer_add(self):\n        model = TestModel()\n        p = Parameter([5], I.Constant(0))\n        p.gradient = tF.raw_input([5], [1, 2, 3, 4, 5])\n        optimizer = O.Adam()\n        optimizer.set_weight_decay(1e-6)\n        optimizer.set_gradient_clipping(5)\n        optimizer.add(model)\n        optimizer.add(p)\n        self.assertEqual(p.gradient.to_list(), [1, 2, 3, 4, 5])\n        self.assertEqual(model.param.gradient.to_list(), [1, 2, 3, 4, 5])\n        optimizer.reset_gradients()\n        self.assertEqual(p.gradient.to_list(), [0, 0, 0, 0, 0])\n        self.assertEqual(model.param.gradient.to_list(), [0, 0, 0, 0, 0])\n'"
tests/optimizers.py,0,"b""from primitiv import optimizers as O\nfrom primitiv import Optimizer\n\nimport unittest\nimport tempfile\n\n\nclass Optimizers(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        pass\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def test_sgd_virtual(self):\n        t = O.SGD()\n        uint_configs = {'Optimizer.epoch': 1}\n        float_configs = {'SGD.eta': 0.0,\n                         'Optimizer.clip_threshold': 0.0,\n                         'Optimizer.lr_scale': 1.0,\n                         'Optimizer.l2_strength': 0.0,\n        }\n        t.set_configs(uint_configs, float_configs)\n        uint_configs, float_configs = t.get_configs()\n        self.assertEqual(uint_configs['Optimizer.epoch'], 1)\n\n    def test_momentum_sgd_virtual(self):\n        t = O.MomentumSGD()\n        uint_configs = {'Optimizer.epoch': 1}\n        float_configs = {'MomentumSGD.momentum': 1.0,\n                         'MomentumSGD.eta': 0.0,\n                         'Optimizer.clip_threshold': 0.0,\n                         'Optimizer.lr_scale': 1.0,\n                         'Optimizer.l2_strength': 0.0,\n        }\n        t.set_configs(uint_configs, float_configs)\n        uint_configs, float_configs = t.get_configs()\n        self.assertEqual(uint_configs['Optimizer.epoch'], 1)\n\n    def test_adagrad_virtual(self):\n        t = O.AdaGrad()\n        uint_configs = {'Optimizer.epoch': 1}\n        float_configs = {'AdaGrad.eps': 0.0,\n                         'AdaGrad.eta': 0.0,\n                         'Optimizer.clip_threshold': 0.0,\n                         'Optimizer.lr_scale': 1.0,\n                         'Optimizer.l2_strength': 0.0,\n        }\n        t.set_configs(uint_configs, float_configs)\n        uint_configs, float_configs = t.get_configs()\n        self.assertEqual(uint_configs['Optimizer.epoch'], 1)\n\n    def test_rmsprop_virtual(self):\n        t = O.RMSProp()\n        uint_configs = {'Optimizer.epoch': 1}\n        float_configs = {'RMSProp.eta': 2.0,\n                         'RMSProp.alpha': 3.0,\n                         'RMSProp.eps': 4.0,\n                         'Optimizer.clip_threshold': 0.0,\n                         'Optimizer.lr_scale': 1.0,\n                         'Optimizer.l2_strength': 0.0,\n        }\n        t.set_configs(uint_configs, float_configs)\n        uint_configs, float_configs = t.get_configs()\n        self.assertEqual(uint_configs['Optimizer.epoch'], 1)\n\n    def test_adadelta_virtual(self):\n        t = O.AdaDelta()\n        uint_configs = {'Optimizer.epoch': 1}\n        float_configs = {'AdaDelta.rho': 2.0,\n                         'AdaDelta.eps': 3.0,\n                         'Optimizer.clip_threshold': 0.0,\n                         'Optimizer.lr_scale': 1.0,\n                         'Optimizer.l2_strength': 0.0,\n        }\n        t.set_configs(uint_configs, float_configs)\n        uint_configs, float_configs = t.get_configs()\n        self.assertEqual(uint_configs['Optimizer.epoch'], 1)\n\n    def test_adam_virtual(self):\n        t = O.Adam()\n        uint_configs = {'Optimizer.epoch': 1}\n        float_configs = {'Optimizer.lr_scale': 1.0,\n                         'Adam.beta2': 1.0,\n                         'Adam.eps': 0.0,\n                         'Optimizer.clip_threshold': 0.0,\n                         'Adam.alpha': 0.0,\n                         'Optimizer.l2_strength': 0.0,\n                         'Adam.beta1': 1.0,\n        }\n        t.set_configs(uint_configs, float_configs)\n        uint_configs, float_configs = t.get_configs()\n        self.assertEqual(uint_configs['Optimizer.epoch'], 1)\n"""
tests/parameter.py,9,"b'from primitiv import Optimizer, Parameter, Device, Graph, Shape\nfrom primitiv import initializers as I\nfrom primitiv import devices as D\nfrom primitiv import functions as F\nfrom primitiv import tensor_functions as tF\nfrom primitiv._parameter import ParameterStatistics\n\nimport unittest\n\nimport numpy as np\n\n\nclass ParameterTest(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        pass\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def setUp(self):\n        self.dev = D.Naive()\n        Device.set_default(self.dev)\n        self.p = Parameter([8], I.Constant(0))\n        self.p.value.reset_by_vector([1, 2, 3, 4, 5, 6, 7, 8])\n\n    def tearDown(self):\n         pass\n\n    def test_parameter_stats(self):\n        self.p.add_stats(""stat1"", Shape([2, 3]))\n        self.p.add_stats(""stat2"", Shape([2, 4]))\n        st1 = self.p.stats[""stat1""]\n        st1.reset(0)\n        self.assertTrue((st1.to_ndarrays()[0] == np.zeros([2, 3])).all())\n        self.p.stats[""stat1""] = tF.input(np.ones([2, 3]))\n        self.assertTrue((st1.to_ndarrays()[0] == np.ones([2, 3])).all())\n        self.assertIn(""stat1"", self.p.stats)\n        self.assertIn(""stat2"", self.p.stats)\n        self.assertNotIn(""stat3"", self.p.stats)\n        with self.assertRaises(NotImplementedError):\n            del self.p.stats[""stat1""]\n        with self.assertRaises(AttributeError):\n            self.p.stats = ParameterStatistics(self.p)\n\n    def test_parameter_value(self):\n        self.assertTrue((self.p.value.to_ndarrays() == np.array([1, 2, 3, 4, 5, 6, 7, 8])).all())\n        val = self.p.value\n        self.p.value += tF.input(np.ones([8]))\n        self.assertTrue((val.to_ndarrays()[0] == np.array([2, 3, 4, 5, 6, 7, 8, 9])).all())\n        with self.assertRaises(NotImplementedError):\n            del self.p.value\n\n    def test_parameter_gradient(self):\n        self.p.reset_gradient()\n        self.assertTrue((self.p.gradient.to_ndarrays() == np.zeros([8])).all())\n        grad = self.p.gradient\n        self.p.gradient += tF.input(np.ones([8]))\n        self.assertTrue((grad.to_ndarrays()[0] == np.ones([8])).all())\n        with self.assertRaises(NotImplementedError):\n            del self.p.gradient\n'"
tests/python_optimizer.py,8,"b'from primitiv import optimizers as O\nfrom primitiv import Optimizer, Parameter, Device, Graph, Shape\nfrom primitiv import initializers as I\nfrom primitiv import devices as D\nfrom primitiv import functions as F\nfrom primitiv import tensor_functions as tF\n\nimport unittest\nimport tempfile\n\nimport numpy as np\n\n\nclass TestAdam(Optimizer):\n    def __init__(self, alpha, beta1, beta2, eps):\n        super().__init__()\n        self.alpha_ = np.float32(alpha)\n        self.beta1_ = np.float32(beta1)\n        self.beta2_ = np.float32(beta2)\n        self.eps_ = np.float32(eps)\n\n    def configure_parameter(self, param):\n        for name in (""testadam-m1"", ""testadam-m2""):\n            if name not in param.stats:\n                param.add_stats(name, param.shape())\n                param.stats[name].reset(0)\n\n    def update_parameter(self, scale, param):\n        epoch = self.get_epoch() + 1\n        g = param.gradient\n        param.stats[""testadam-m1""] = self.beta1_ * param.stats[""testadam-m1""] + (1 - self.beta1_) * g\n        param.stats[""testadam-m2""] = self.beta2_ * param.stats[""testadam-m2""] + (1 - self.beta2_) * g * g\n        mm1 = param.stats[""testadam-m1""] / (1 - self.beta1_ ** epoch)\n        mm2 = param.stats[""testadam-m2""] / (1 - self.beta2_ ** epoch)\n        param.value -= (scale * self.alpha_) * mm1 / (tF.sqrt(mm2) + self.eps_)\n\n    def get_configs(self):\n        uint_configs = {}\n        float_configs = {\n            ""TestAdam.alpha"": self.alpha_,\n            ""TestAdam.beta1"": self.beta1_,\n            ""TestAdam.beta2"": self.beta2_,\n            ""TestAdam.eps"": self.eps_,\n        }\n        return uint_configs, float_configs\n\n    def set_configs(self, uint_configs, float_configs):\n        self.alpha_ = float_configs[""TestAdam.alpha""]\n        self.beta1_ = float_configs[""TestAdam.beta1""]\n        self.beta2_ = float_configs[""TestAdam.beta2""]\n        self.eps_ = float_configs[""TestAdam.eps""]\n\n\nclass TestException(Exception):\n    pass\n\n\nclass ExceptionOptimizer(Optimizer):\n\n    def configure_parameter(self, param):\n        raise TestException(""configure_parameter"")\n\n    def update_parameter(self, scale, param):\n        raise TestException(""update_parameter"")\n\n    def get_configs(self):\n        raise TestException(""get_configs"")\n\n    def set_configs(self, uint_configs, float_configs):\n        raise TestException(""set_configs"")\n\n\nclass IncompleteOptimizer(Optimizer):\n    pass\n\n\ndef train_func(optimizer):\n    dev = D.Naive(12345)\n    Device.set_default(dev)\n    g = Graph()\n    Graph.set_default(g)\n\n    pw1 = Parameter([8, 2], I.XavierUniform())\n    pb1 = Parameter([8], I.Constant(0))\n    pw2 = Parameter([1, 8], I.XavierUniform())\n    pb2 = Parameter([1], I.Constant(0))\n\n    optimizer.add(pw1, pb1, pw2, pb2)\n\n    input_data = [1, 1, 1, -1, -1, 1, -1, -1]\n    output_data = [1, -1, -1, 1]\n\n    for i in range(10):\n        g.clear()\n        x = F.raw_input(Shape([2], 4), input_data)\n        w1 = F.parameter(pw1)\n        b1 = F.parameter(pb1)\n        w2 = F.parameter(pw2)\n        b2 = F.parameter(pb2)\n        h = F.tanh(w1 @ x + b1)\n        y = w2 @ h + b2\n\n        t = F.raw_input(Shape([], 4), output_data)\n        diff = t - y\n        loss = F.batch.mean(diff * diff)\n\n        optimizer.reset_gradients()\n        loss.backward()\n        optimizer.update()\n\n    return [pw1.value.to_list(),\n            pb1.value.to_list(),\n            pw2.value.to_list(),\n            pb2.value.to_list()]\n\n\nclass PythonOptimizerTest(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        pass\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def setUp(self):\n        self.t = TestAdam(alpha = 0.001, beta1 = 0.9, beta2 = 0.999, eps = 1e-8)\n\n    def tearDown(self):\n        pass\n\n    def test_pyoptimizer_get_set_config(self):\n        uint_configs, float_configs = Optimizer.get_configs(self.t)\n        self.assertAlmostEqual(uint_configs[\'Optimizer.epoch\'], 0)\n        self.assertAlmostEqual(float_configs[\'TestAdam.alpha\'], 0.001)\n        self.assertAlmostEqual(float_configs[\'TestAdam.beta1\'], 0.9)\n        self.assertAlmostEqual(float_configs[\'TestAdam.beta2\'], 0.999)\n        self.assertAlmostEqual(float_configs[\'TestAdam.eps\'], 1e-8, places=10)\n        float_configs[\'TestAdam.beta1\'] = 200\n        Optimizer.set_configs(self.t, uint_configs, float_configs)\n        self.assertEqual(self.t.beta1_, 200)\n\n    def test_pyoptimizer_parameter(self):\n        dev = D.Naive()\n        Device.set_default(dev)\n        pw1 = Parameter([8, 2], I.XavierUniform())\n        self.t.add(pw1)\n        self.assertIn(""testadam-m1"", pw1.stats)\n        self.assertIn(""testadam-m2"", pw1.stats)\n\n    def test_pyoptimizer_compare_with_cpp(self):\n        c_optimizer = O.Adam(alpha = 0.001, beta1 = 0.9, beta2 = 0.999, eps = 1e-8)\n        py_params = train_func(self.t)\n        c_params = train_func(c_optimizer)\n        py_uint_configs, py_float_configs = Optimizer.get_configs(self.t)\n        c_uint_configs, c_float_configs = c_optimizer.get_configs()\n        self.assertEqual(py_uint_configs[""Optimizer.epoch""], c_uint_configs[""Optimizer.epoch""])\n        self.assertEqual(py_float_configs[""TestAdam.alpha""], c_float_configs[""Adam.alpha""])\n        self.assertEqual(py_float_configs[""TestAdam.beta1""], c_float_configs[""Adam.beta1""])\n        self.assertEqual(py_float_configs[""TestAdam.beta2""], c_float_configs[""Adam.beta2""])\n        self.assertEqual(py_float_configs[""TestAdam.eps""], c_float_configs[""Adam.eps""])\n        self.assertEqual(py_float_configs[""Optimizer.clip_threshold""], c_float_configs[""Optimizer.clip_threshold""])\n        self.assertEqual(py_float_configs[""Optimizer.l2_strength""], c_float_configs[""Optimizer.l2_strength""])\n        self.assertEqual(py_float_configs[""Optimizer.lr_scale""], c_float_configs[""Optimizer.lr_scale""])\n        self.assertTrue(np.isclose(py_params[0], c_params[0]).all())\n        self.assertTrue(np.isclose(py_params[1], c_params[1]).all())\n        self.assertTrue(np.isclose(py_params[2], c_params[2]).all())\n        self.assertTrue(np.isclose(py_params[3], c_params[3]).all())\n\n    def test_pyoptimizer_loadsave(self):\n        t_loaded = TestAdam(alpha = 0, beta1 = 0, beta2 = 0, eps = 0)\n        self.assertEqual(t_loaded.alpha_, 0)\n        self.assertEqual(t_loaded.beta1_, 0)\n        self.assertEqual(t_loaded.beta2_, 0)\n        self.assertEqual(t_loaded.eps_, 0)\n        with tempfile.NamedTemporaryFile() as fp:\n            self.t.save(fp.name)\n            t_loaded.load(fp.name)\n        self.assertAlmostEqual(t_loaded.alpha_, 0.001)\n        self.assertAlmostEqual(t_loaded.beta1_, 0.9)\n        self.assertAlmostEqual(t_loaded.beta2_, 0.999)\n        self.assertAlmostEqual(t_loaded.eps_, 1e-8, places=10)\n\n    def test_pyoptimizer_propagate_exception(self):\n        dev = D.Naive()\n        Device.set_default(dev)\n        optimizer = ExceptionOptimizer()\n        p = Parameter()\n        with self.assertRaises(TestException) as ctx:\n            optimizer.add(p)\n        self.assertEqual(str(ctx.exception), ""configure_parameter"")\n        with self.assertRaises(TestException) as ctx:\n            optimizer.update()\n        self.assertEqual(str(ctx.exception), ""update_parameter"")\n        with self.assertRaises(TestException) as ctx:\n            Optimizer.get_configs(optimizer)\n        self.assertEqual(str(ctx.exception), ""get_configs"")\n        with self.assertRaises(TestException) as ctx:\n            Optimizer.set_configs(optimizer, {\'Optimizer.epoch\': 1},\n                                         {\'Optimizer.clip_threshold\': 0.0,\n                                          \'Optimizer.lr_scale\': 1.0,\n                                          \'Optimizer.l2_strength\': 0.0})\n        self.assertEqual(str(ctx.exception), ""set_configs"")\n\n\n    def test_pyoptimizer_not_implemented(self):\n        dev = D.Naive()\n        Device.set_default(dev)\n        optimizer = IncompleteOptimizer()\n        p = Parameter()\n        with self.assertRaises(NotImplementedError):\n            optimizer.add(p)\n        with self.assertRaises(NotImplementedError):\n            optimizer.update()\n        with self.assertRaises(NotImplementedError):\n            Optimizer.get_configs(optimizer)\n        with self.assertRaises(NotImplementedError):\n            Optimizer.set_configs(optimizer, {\'Optimizer.epoch\': 1},\n                                         {\'Optimizer.clip_threshold\': 0.0,\n                                          \'Optimizer.lr_scale\': 1.0,\n                                          \'Optimizer.l2_strength\': 0.0})\n'"
tests/tensor_forward_test.py,77,"b'import math\nimport random\nimport sys\nimport unittest\n\nfrom primitiv import Device\nfrom primitiv import Shape\nfrom primitiv import Parameter\nfrom primitiv import Tensor\nfrom primitiv import initializers as I\nfrom primitiv import tensor_functions as tF\n\nimport numpy as np\nfrom . import test_utils\n\n\nclass TensorForwardTest(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.devices = test_utils.available_devices()\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def test_TensorForwardTest_CheckInputByVector(self):\n        data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n        for dev in TensorForwardTest.devices:\n            y = tF.raw_input(Shape([2, 2], 3), data, dev)\n            self.assertEqual(Shape([2, 2], 3), y.shape())\n            self.assertIs(dev, y.device())\n            self.assertEqual(data, y.to_list())\n\n    def test_TensorForwardTest_CheckInputByParameter(self):\n        data = [1, 2, 3, 4]\n        for dev in TensorForwardTest.devices:\n            param = Parameter([2, 2], I.Constant(0), dev)\n            param.value += tF.raw_input([2, 2], data, dev)\n            y = tF.parameter(param)\n            self.assertEqual(Shape([2, 2]), y.shape())\n            self.assertIs(dev, y.device())\n            self.assertEqual(data, y.to_list())\n\n    def test_TensorForwardTest_CheckInputByNdArray(self):\n        y_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n        x_data = [\n            np.array([[1, 3], [2, 4]]),\n            np.array([[5, 7], [6, 8]]),\n            np.array([[9, 11], [10, 12]]),\n        ]\n        for dev in TensorForwardTest.devices:\n            y = tF.input(x_data, dev)\n            self.assertEqual(Shape([2, 2], 3), y.shape())\n            self.assertIs(dev, y.device())\n            self.assertEqual(y_data, y.to_list())\n\n    def test_TensorForwardTest_CheckCopy(self):\n        i = 0\n        for dev in TensorForwardTest.devices:\n            for dev2 in TensorForwardTest.devices:\n                data = list(range(i, i + 12))\n                print(data)\n                i += 12\n                x = tF.raw_input(Shape([2, 2], 3), data, dev)\n                y = tF.copy(x, dev2)\n                self.assertEqual(Shape([2, 2], 3), y.shape())\n                self.assertIs(dev, x.device())\n                self.assertIs(dev2, y.device())\n                self.assertEqual(x.to_list(), y.to_list())\n                y *= 2\n                self.assertNotEqual(x.to_list(), y.to_list())\n\n    def test_TensorForwardTest_CheckInvalidCopy(self):\n        for dev in TensorForwardTest.devices:\n            with self.assertRaises(RuntimeError):\n                tF.copy(Tensor(), dev)\n\n    def test_TensorForwardTest_CheckIdentity(self):\n        test_cases = [\n            (1, Shape(), [1]),\n            (2, Shape([2, 2]), [1, 0, 0, 1]),\n            (3, Shape([3, 3]), [1, 0, 0, 0, 1, 0, 0, 0, 1]),\n            (4, Shape([4, 4]), [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n        ]\n        for dev in TensorForwardTest.devices:\n            Device.set_default(dev)\n            for tc in test_cases:\n                y = tF.identity(tc[0])\n                self.assertEqual(tc[1], y.shape())\n                self.assertEqual(tc[2], y.to_list())\n\n    def test_TensorForwardTest_CheckInvalidIdentity(self):\n        for dev in TensorForwardTest.devices:\n            Device.set_default(dev)\n            with self.assertRaises(RuntimeError):\n                tF.identity(0)\n\n    def test_TensorForwardTest_CheckPickNN(self):\n        test_cases = [\n            (Shape([2, 2, 2], 3), 0, [0, 0, 0],\n                Shape([1, 2, 2], 3),\n                [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]),\n            (Shape([2, 2, 2], 3), 0, [1, 0, 1],\n                Shape([1, 2, 2], 3),\n                [1, 3, 5, 7, 8, 10, 12, 14, 17, 19, 21, 23]),\n            (Shape([2, 2, 2], 3), 0, [0],\n                Shape([1, 2, 2], 3),\n                [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]),\n            (Shape([2, 2, 2]), 0, [0, 1, 0],\n                Shape([1, 2, 2], 3),\n                [0, 2, 4, 6, 1, 3, 5, 7, 0, 2, 4, 6]),\n            (Shape([2, 2, 2], 3), 1, [0, 0, 0],\n                Shape([2, 1, 2], 3),\n                [0, 1, 4, 5, 8, 9, 12, 13, 16, 17, 20, 21]),\n            (Shape([2, 2, 2], 3), 2, [0, 0, 0],\n                Shape([2, 2, 1], 3),\n                [0, 1, 2, 3, 8, 9, 10, 11, 16, 17, 18, 19]),\n        ]\n        for dev in TensorForwardTest.devices:\n            for tc in test_cases:\n                print(""x_shape ="", tc[0],\n                      "", dim ="", tc[1], "", ids = ["", file=sys.stderr)\n                print(tc[2], file=sys.stderr)\n                print(""]"", file=sys.stderr)\n                x_data = list(range(tc[0].size()))\n                x = tF.raw_input(tc[0], x_data, dev)\n                y = tF.pick(x, tc[2], tc[1])\n                self.assertEqual(tc[3], y.shape())\n                self.assertEqual(tc[4], y.to_list())\n\n    def test_TensorForwardTest_CheckInvalidPick(self):\n        test_cases = [\n            (0, []),\n            (0, [2]),\n            (0, [0, 1]),\n            (0, [0, 1, 2]),\n            (1, [2]),\n            (2, [2]),\n            (3, [1]),\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2, 2], 3), [0] * 24, dev)\n            for tc in test_cases:\n                with self.assertRaises(RuntimeError):\n                    tF.pick(x, tc[1], tc[0])\n\n    def test_TensorForwardTest_CheckSlice(self):\n        x_data = list(range(3 * 3 * 2 * 4))\n        test_cases = [\n            (0, 0, 1, Shape([1, 3, 2], 4),\n                [0, 3, 6, 9, 12, 15,\n                 18, 21, 24, 27, 30, 33,\n                 36, 39, 42, 45, 48, 51,\n                 54, 57, 60, 63, 66, 69]),\n            (1, 0, 1, Shape([3, 1, 2], 4),\n                [0, 1, 2, 9, 10, 11,\n                 18, 19, 20, 27, 28, 29,\n                 36, 37, 38, 45, 46, 47,\n                 54, 55, 56, 63, 64, 65]),\n            (2, 0, 1, Shape([3, 3, 1], 4),\n                [0, 1, 2, 3, 4, 5, 6, 7, 8,\n                 18, 19, 20, 21, 22, 23, 24, 25, 26,\n                 36, 37, 38, 39, 40, 41, 42, 43, 44,\n                 54, 55, 56, 57, 58, 59, 60, 61, 62]),\n            (0, 1, 2, Shape([1, 3, 2], 4),\n                [1, 4, 7, 10, 13, 16,\n                 19, 22, 25, 28, 31, 34,\n                 37, 40, 43, 46, 49, 52,\n                 55, 58, 61, 64, 67, 70]),\n            (1, 1, 2, Shape([3, 1, 2], 4),\n                [3, 4, 5, 12, 13, 14,\n                 21, 22, 23, 30, 31, 32,\n                 39, 40, 41, 48, 49, 50,\n                 57, 58, 59, 66, 67, 68]),\n            (2, 1, 2, Shape([3, 3, 1], 4),\n                [9, 10, 11, 12, 13, 14, 15, 16, 17,\n                 27, 28, 29, 30, 31, 32, 33, 34, 35,\n                 45, 46, 47, 48, 49, 50, 51, 52, 53,\n                 63, 64, 65, 66, 67, 68, 69, 70, 71]),\n            (0, 2, 3, Shape([1, 3, 2], 4),\n                [2, 5, 8, 11, 14, 17,\n                 20, 23, 26, 29, 32, 35,\n                 38, 41, 44, 47, 50, 53,\n                 56, 59, 62, 65, 68, 71]),\n            (1, 2, 3, Shape([3, 1, 2], 4),\n                [6, 7, 8, 15, 16, 17,\n                 24, 25, 26, 33, 34, 35,\n                 42, 43, 44, 51, 52, 53,\n                 60, 61, 62, 69, 70, 71]),\n            (3, 0, 1, Shape([3, 3, 2], 4), x_data),\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([3, 3, 2], 4), x_data, dev)\n            for tc in test_cases:\n                print(""dim ="", tc[0], "", lower ="", tc[1],\n                      "", upper ="", tc[2], file=sys.stderr)\n                y = tF.slice(x, tc[0], tc[1], tc[2])\n                self.assertEqual(tc[3], y.shape())\n                self.assertEqual(tc[4], y.to_list())\n\n    def test_TensorForwardTest_CheckInvalidSlice(self):\n        test_cases = [\n            (0, 0, 0), (0, 1, 0), (0, 0, 4), (0, 3, 4),\n            (1, 0, 0), (1, 1, 0), (1, 0, 4), (1, 3, 4),\n            (2, 0, 0), (2, 1, 0), (2, 0, 2), (2, 1, 2),\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input([3, 3], [3] * 9)\n            for tc in test_cases:\n                with self.assertRaises(RuntimeError):\n                    tF.slice(x, tc[0], tc[1], tc[2])\n\n    def test_TensorForwardTest_CheckConcatN_3x3(self):\n        y_data = [\n            1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6,\n        ]\n        for dev in TensorForwardTest.devices:\n            a = tF.raw_input([1, 3], [1, 1, 1], dev)\n            b = tF.raw_input([2, 3], [2, 3, 2, 3, 2, 3], dev)\n            c = tF.raw_input([3, 3], [4, 5, 6, 4, 5, 6, 4, 5, 6], dev)\n            y = tF.concat([a, b, c], 0)\n            self.assertEqual(Shape([6, 3]), y.shape())\n            self.assertEqual(y_data, y.to_list())\n\n    def test_TensorForwardTest_CheckConcat5x4(self):\n        shapes = [\n            Shape([20]),\n            Shape([5, 4]),\n            Shape([5, 1, 4]),\n        ]\n        y_data = [\n            1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4,\n        ]\n        for dev in TensorForwardTest.devices:\n            a = tF.raw_input([5], [1, 1, 1, 1, 1], dev)\n            b = tF.raw_input([5], [2, 2, 2, 2, 2], dev)\n            c = tF.raw_input([5], [3, 3, 3, 3, 3], dev)\n            d = tF.raw_input([5], [4, 4, 4, 4, 4], dev)\n            for i in range(3):\n                y = tF.concat([a, b, c, d], i)\n                self.assertEqual(shapes[i], y.shape())\n                self.assertEqual(y_data, y.to_list())\n\n    def test_TensorForwardTest_CheckConcat2_2_2x2(self):\n        a_data = [\n            1, 2, 3, 4, 5, 6, 7, 8,\n            11, 22, 33, 44, 55, 66, 77, 88,\n        ]\n        b_data = [\n            -1, -2, -3, -4, -5, -6, -7, -8,\n            -11, -22, -33, -44, -55, -66, -77, -88,\n        ]\n        shapes = [\n            Shape([4, 2, 2], 2),\n            Shape([2, 4, 2], 2),\n            Shape([2, 2, 4], 2),\n            Shape([2, 2, 2, 2], 2),\n            Shape([2, 2, 2, 1, 2], 2),\n        ]\n        y_data = [\n            [1, 2, -1, -2, 3, 4, -3, -4,\n             5, 6, -5, -6, 7, 8, -7, -8,\n             11, 22, -11, -22, 33, 44, -33, -44,\n             55, 66, -55, -66, 77, 88, -77, -88],\n            [1, 2, 3, 4, -1, -2, -3, -4,\n             5, 6, 7, 8, -5, -6, -7, -8,\n             11, 22, 33, 44, -11, -22, -33, -44,\n             55, 66, 77, 88, -55, -66, -77, -88],\n            [1, 2, 3, 4, 5, 6, 7, 8,\n             -1, -2, -3, -4, -5, -6, -7, -8,\n             11, 22, 33, 44, 55, 66, 77, 88,\n             -11, -22, -33, -44, -55, -66, -77, -88],\n        ]\n        for dev in TensorForwardTest.devices:\n            a = tF.raw_input(Shape([2, 2, 2], 2), a_data, dev)\n            b = tF.raw_input(Shape([2, 2, 2], 2), b_data, dev)\n            for i in range(5):\n                y = tF.concat([a, b], i)\n                self.assertEqual(shapes[i], y.shape())\n                self.assertEqual(y_data[i if i < 2 else 2], y.to_list())\n\n    def test_TensorForwardTest_CheckConcatBatchBroadcast(self):\n        for dev in TensorForwardTest.devices:\n            y_data = [\n                1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n                11, 11, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n            ]\n            a = tF.raw_input(Shape([2, 1], 2), [1, 1, 11, 11], dev)\n            b = tF.raw_input([2, 2], [2, 2, 2, 2], dev)\n            c = tF.raw_input([2, 3], [3, 3, 3, 3, 3, 3], dev)\n            y = tF.concat([a, b, c], 1)\n            self.assertEqual(Shape([2, 6], 2), y.shape())\n            self.assertEqual(y_data, y.to_list())\n\n            y_data = [\n                1, 1, 1, 2, 2, 3, 1, 1, 1, 2, 2, 3,\n                1, 1, 1, 22, 22, 33, 1, 1, 1, 22, 22, 33,\n            ]\n            a = tF.raw_input([3, 2], [1, 1, 1, 1, 1, 1], dev)\n            b = tF.raw_input(Shape([2, 2], 2),\n                             [2, 2, 2, 2, 22, 22, 22, 22], dev)\n            c = tF.raw_input(Shape([1, 2], 2), [3, 3, 33, 33], dev)\n            y = tF.concat([a, b, c], 0)\n            self.assertEqual(Shape([6, 2], 2), y.shape())\n            self.assertEqual(y_data, y.to_list())\n\n            y_data = [1, 2, 3, 1, 2, 33, 1, 2, 333]\n            a = tF.raw_input([], [1], dev)\n            b = tF.raw_input([], [2], dev)\n            c = tF.raw_input(Shape([], 3), [3, 33, 333], dev)\n            y = tF.concat([a, b, c], 0)\n            self.assertEqual(Shape([3], 3), y.shape())\n            self.assertEqual(y_data, y.to_list())\n\n    def test_TensorForwardTest_CheckInvalidConcat(self):\n        for dev in TensorForwardTest.devices:\n            a = tF.zeros(Shape([1, 42], 2), dev)\n            b = tF.zeros(Shape([2, 42], 2), dev)\n            c = tF.zeros(Shape([1, 42], 3), dev)\n            d = tF.zeros([2, 42], dev)\n\n            tF.concat([a, b], 0)\n            with self.assertRaises(RuntimeError):\n                tF.concat([a, b], 1)\n            with self.assertRaises(RuntimeError):\n                tF.concat([a, b], 2)\n            with self.assertRaises(RuntimeError):\n                tF.concat([a, c], 0)\n            with self.assertRaises(RuntimeError):\n                tF.concat([a, c], 1)\n            with self.assertRaises(RuntimeError):\n                tF.concat([a, c], 2)\n            with self.assertRaises(RuntimeError):\n                tF.concat([b, c], 0)\n            with self.assertRaises(RuntimeError):\n                tF.concat([b, c], 1)\n            with self.assertRaises(RuntimeError):\n                tF.concat([b, c], 2)\n            tF.concat([a, d], 0)\n            with self.assertRaises(RuntimeError):\n                tF.concat([a, d], 1)\n            with self.assertRaises(RuntimeError):\n                tF.concat([a, d], 2)\n\n    def test_TensorForwardTest_CheckReshape(self):\n        shapes = [\n            Shape([6]), Shape([1, 6]), Shape([1, 1, 6]), Shape([1, 1, 1, 6]),\n            Shape([2, 3]), Shape([2, 1, 3]), Shape([1, 2, 3]),\n            Shape([2, 1, 1, 3]), Shape([1, 2, 1, 3]), Shape([1, 1, 2, 3]),\n            Shape([3, 2]), Shape([3, 1, 2]), Shape([1, 3, 2]),\n            Shape([3, 1, 1, 2]), Shape([1, 3, 1, 2]), Shape([1, 1, 3, 2]),\n        ]\n        for dev in TensorForwardTest.devices:\n            data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n            a = tF.raw_input(Shape([6], 2), data, dev)\n            for shape in shapes:\n                y1 = tF.reshape(a, shape)\n                self.assertEqual(shape.resize_batch(2), y1.shape())\n                self.assertEqual(data, y1.to_list())\n                y2 = tF.reshape(a, shape.resize_batch(2))\n                self.assertEqual(shape.resize_batch(2), y2.shape())\n                self.assertEqual(data, y2.to_list())\n\n    def test_TensorForwardTest_CheckInvalidReshape(self):\n        for dev in TensorForwardTest.devices:\n            a = tF.zeros(Shape([6], 2), dev)\n            with self.assertRaises(RuntimeError):\n                tF.reshape(a, [7])\n            with self.assertRaises(RuntimeError):\n                tF.reshape(a, Shape([6], 3))\n            with self.assertRaises(RuntimeError):\n                tF.reshape(a, Shape([7], 3))\n\n    def test_TensorForwardTest_CheckFlatten(self):\n        shapes = [\n            Shape([6]), Shape([1, 6]), Shape([1, 1, 6]), Shape([1, 1, 1, 6]),\n            Shape([2, 3]), Shape([2, 1, 3]), Shape([1, 2, 3]),\n            Shape([2, 1, 1, 3]), Shape([1, 2, 1, 3]), Shape([1, 1, 2, 3]),\n            Shape([3, 2]), Shape([3, 1, 2]), Shape([1, 3, 2]),\n            Shape([3, 1, 1, 2]), Shape([1, 3, 1, 2]), Shape([1, 1, 3, 2]),\n        ]\n        for dev in TensorForwardTest.devices:\n            data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n            for shape in shapes:\n                a = tF.raw_input(shape.resize_batch(2), data, dev)\n                y = tF.flatten(a)\n                self.assertEqual(Shape([6], 2), y.shape())\n                self.assertEqual(data, y.to_list())\n\n    def test_TensorForwardTest_CheckDuplicate(self):\n        x_data = [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2], 2), x_data, dev)\n            y = +x\n            self.assertEqual(Shape([2, 2], 2), y.shape())\n            self.assertTrue(np.isclose(x_data, y.to_list()).all())\n\n    def test_TensorForwardTest_CheckNegate(self):\n        x_data = [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001]\n        y_data = [-1000, -100, -10, -1, -0.1, -0.01, -0.001, -0.0001]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2], 2), x_data, dev)\n            y = -x\n            self.assertEqual(Shape([2, 2], 2), y.shape())\n            self.assertTrue(np.isclose(y_data, y.to_list()).all())\n\n    def test_TensorForwardTest_CheckAddConst(self):\n        x_data = [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001]\n        k = 1\n        y_data = [1001, 101, 11, 2, 1.1, 1.01, 1.001, 1.0001]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2], 2), x_data, dev)\n            y1 = k + x\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertTrue(np.isclose(y_data, y1.to_list()).all())\n            y2 = x + k\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertTrue(np.isclose(y_data, y2.to_list()).all())\n\n    def test_TensorForwardTest_CheckAddScalar(self):\n        x_data = [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001]\n        k_data = [10, 1]\n        y_data = [1010, 110, 20, 11, 1.1, 1.01, 1.001, 1.0001]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2], 2), x_data, dev)\n            k = tF.raw_input(Shape([], 2), k_data, dev)\n            y1 = k + x\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertTrue(np.isclose(y_data, y1.to_list()).all())\n            y2 = x + k\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertTrue(np.isclose(y_data, y2.to_list()).all())\n\n    def test_TensorForwardTest_CheckAddScalarBatchBroadcast(self):\n        x_data = [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001]\n        k_data = [1]\n        y_data = [1001, 101, 11, 2, 1.1, 1.01, 1.001, 1.0001]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2], 2), x_data, dev)\n            k = tF.raw_input([], k_data, dev)\n            y1 = k + x\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertTrue(np.isclose(y_data, y1.to_list()).all())\n            y2 = x + k\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertTrue(np.isclose(y_data, y2.to_list()).all())\n        x_data = [1000, 100, 10, 1]\n        k_data = [10, 1]\n        y_data = [1010, 110, 20, 11, 1001, 101, 11, 2]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input([2, 2], x_data, dev)\n            k = tF.raw_input(Shape([], 2), k_data, dev)\n            y1 = k + x\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertEqual(y_data, y1.to_list())\n            y2 = x + k\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertEqual(y_data, y2.to_list())\n\n    def test_TensorForwardTest_CheckAdd(self):\n        a_data = [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001]\n        b_data = [   0, 100, 20, 3, 0.4, 0.05, 0.006, 0.0007]\n        y_data = [1000, 200, 30, 4, 0.5, 0.06, 0.007, 0.0008]\n        for dev in TensorForwardTest.devices:\n            a = tF.raw_input(Shape([2, 2], 2), a_data, dev)\n            b = tF.raw_input(Shape([2, 2], 2), b_data, dev)\n            y1 = a + b\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertTrue(np.isclose(y_data, y1.to_list()).all())\n            y2 = b + a\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertTrue(np.isclose(y_data, y2.to_list()).all())\n\n    def test_TensorForwardTest_CheckAddBatchBroadcast(self):\n        a_data = [0, 1, 2, 3]\n        b_data = [0, 0, 0, 0, 4, 4, 4, 4]\n        y_data = [0, 1, 2, 3, 4, 5, 6, 7]\n        for dev in TensorForwardTest.devices:\n            a = tF.raw_input([2, 2], a_data, dev)\n            b = tF.raw_input(Shape([2, 2], 2), b_data, dev)\n            y1 = a + b\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertEqual(y_data, y1.to_list())\n            y2 = b + a\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertEqual(y_data, y2.to_list())\n\n    def test_TensorForwardTest_CheckSubtractConst(self):\n        x_data = [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001]\n        k = 1\n        y1_data = [-999, -99, -9, 0, 0.9, 0.99, 0.999, 0.9999]\n        y2_data = [999, 99, 9, 0, -0.9, -0.99, -0.999, -0.9999]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2], 2), x_data, dev)\n            y1 = k - x\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertTrue(np.isclose(y1_data, y1.to_list()).all())\n            y2 = x - k\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertTrue(np.isclose(y2_data, y2.to_list()).all())\n\n    def test_TensorForwardTest_CheckSubtractScalar(self):\n        x_data = [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001]\n        k_data = [10, 1]\n        y1_data = [-990, -90, 0, 9, 0.9, 0.99, 0.999, 0.9999]\n        y2_data = [990, 90, 0, -9, -0.9, -0.99, -0.999, -0.9999]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2], 2), x_data, dev)\n            k = tF.raw_input(Shape([], 2), k_data, dev)\n            y1 = k - x\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertTrue(np.isclose(y1_data, y1.to_list()).all())\n            y2 = x - k\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertTrue(np.isclose(y2_data, y2.to_list()).all())\n\n    def test_TensorForwardTest_CheckSubtractScalarBatchBroadcast(self):\n        x_data = [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001]\n        k_data = [1]\n        y1_data = [-999, -99, -9, 0, 0.9, 0.99, 0.999, 0.9999]\n        y2_data = [999, 99, 9, 0, -0.9, -0.99, -0.999, -0.9999]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2], 2), x_data, dev)\n            k = tF.raw_input([], k_data, dev)\n            y1 = k - x\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertTrue(np.isclose(y1_data, y1.to_list()).all())\n            y2 = x - k\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertTrue(np.isclose(y2_data, y2.to_list()).all())\n        x_data = [1000, 100, 10, 1]\n        k_data = [10, 1]\n        y1_data = [-990, -90, 0, 9, -999, -99, -9, 0]\n        y2_data = [990, 90, 0, -9, 999, 99, 9, 0]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input([2, 2], x_data, dev)\n            k = tF.raw_input(Shape([], 2), k_data, dev)\n            y1 = k - x\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertEqual(y1_data, y1.to_list())\n            y2 = x - k\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertEqual(y2_data, y2.to_list())\n\n    def test_TensorForwardTest_CheckSubtract(self):\n        a_data = [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001]\n        b_data = [   0, 100, 20, 3, 0.4, 0.05, 0.006, 0.0007]\n        y1_data = [1000, 0, -10, -2, -0.3, -0.04, -0.005, -0.0006]\n        y2_data = [-1000, 0, 10, 2, 0.3, 0.04, 0.005, 0.0006]\n        for dev in TensorForwardTest.devices:\n            a = tF.raw_input(Shape([2, 2], 2), a_data, dev)\n            b = tF.raw_input(Shape([2, 2], 2), b_data, dev)\n            y1 = a - b\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertTrue(np.isclose(y1_data, y1.to_list()).all())\n            y2 = b - a\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertTrue(np.isclose(y2_data, y2.to_list()).all())\n\n    def test_TensorForwardTest_CheckSubtractBatchBroadcast(self):\n        a_data = [0, 1, 2, 3]\n        b_data = [0, 0, 0, 0, 4, 4, 4, 4]\n        y1_data = [0, 1, 2, 3, -4, -3, -2, -1]\n        y2_data = [0, -1, -2, -3, 4, 3, 2, 1]\n        for dev in TensorForwardTest.devices:\n            a = tF.raw_input([2, 2], a_data, dev)\n            b = tF.raw_input(Shape([2, 2], 2), b_data, dev)\n            y1 = a - b\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertTrue(y1_data, y1.to_list())\n            y2 = b - a\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertTrue(y2_data, y2.to_list())\n\n    def test_TensorForwardTest_CheckMultiplyConst(self):\n        x_data = [1000, -100, 10, -1, 0.1, -0.01, 0.001, -0.0001]\n        k = 10\n        y_data = [10000, -1000, 100, -10, 1, -0.1, 0.01, -0.001]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2], 2), x_data, dev)\n            y1 = k * x\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertTrue(np.isclose(y_data, y1.to_list()).all())\n            y2 = x * k\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertTrue(np.isclose(y_data, y2.to_list()).all())\n\n    def test_TensorForwardTest_CheckMultiplyScalar(self):\n        x_data = [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001]\n        k_data = [0.1, 10]\n        y_data = [100, 10, 1, 0.1, 1, 0.1, 0.01, 0.001]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2], 2), x_data, dev)\n            k = tF.raw_input(Shape([], 2), k_data, dev)\n            y1 = k * x\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertTrue(np.isclose(y_data, y1.to_list()).all())\n            y2 = x * k\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertTrue(np.isclose(y_data, y2.to_list()).all())\n\n    def test_TensorForwardTest_CheckMultiplyScalarBatchBroadcast(self):\n        x_data = [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001]\n        k_data = [10]\n        y_data = [10000, 1000, 100, 10, 1, 0.1, 0.01, 0.001]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2], 2), x_data, dev)\n            k = tF.raw_input([], k_data, dev)\n            y1 = k * x\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertTrue(np.isclose(y_data, y1.to_list()).all())\n            y2 = x * k\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertTrue(np.isclose(y_data, y2.to_list()).all())\n        x_data = [1000, 100, 10, 1]\n        k_data = [0.1, 10]\n        y_data = [100, 10, 1, 0.1, 10000, 1000, 100, 10]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input([2, 2], x_data, dev)\n            k = tF.raw_input(Shape([], 2), k_data, dev)\n            y1 = k * x\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertTrue(np.isclose(y_data, y1.to_list()).all())\n            y2 = x * k\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertTrue(np.isclose(y_data, y2.to_list()).all())\n\n    def test_TensorForwardTest_CheckMultiply(self):\n        a_data = [1000, -100, 10, -1, 0.1, -0.01, 0.001, -0.0001]\n        b_data = [0, 1, 2, 3, -4, -5, -6, -7]\n        y_data = [0, -100, 20, -3, -0.4, 0.05, -0.006, 0.0007]\n        for dev in TensorForwardTest.devices:\n            a = tF.raw_input(Shape([2, 2], 2), a_data, dev)\n            b = tF.raw_input(Shape([2, 2], 2), b_data, dev)\n            y1 = a * b\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertTrue(np.isclose(y_data, y1.to_list()).all())\n            y2 = b * a\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertTrue(np.isclose(y_data, y2.to_list()).all())\n\n    def test_TensorForwardTest_CheckMultiplyBatchBroadcast(self):\n        a_data = [0, 1, 2, 3]\n        b_data = [1, 1, 1, 1, 0, 1, 2, 3]\n        y_data = [0, 1, 2, 3, 0, 1, 4, 9]\n        for dev in TensorForwardTest.devices:\n            a = tF.raw_input([2, 2], a_data, dev)\n            b = tF.raw_input(Shape([2, 2], 2), b_data, dev)\n            y1 = a * b\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertEqual(y_data, y1.to_list())\n            y2 = b * a\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertEqual(y_data, y2.to_list())\n\n    def test_TensorForwardTest_CheckDivideConst(self):\n        x_data = [1000, -100, 10, -1, 0.1, -0.01, 0.001, -0.0001]\n        k = 10\n        y1_data = [0.01, -0.1, 1, -10, 100, -1000, 10000, -100000]\n        y2_data = [\n            100, -10, 1, -0.1, 0.01, -0.001, 0.0001, -0.00001,\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2], 2), x_data, dev)\n            y1 = k / x\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertTrue(np.isclose(y1_data, y1.to_list()).all())\n            y2 = x / k\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertTrue(np.isclose(y2_data, y2.to_list()).all())\n\n    def test_TensorForwardTest_CheckDivideScalar(self):\n      x_data = [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001]\n      k_data = [10, 0.1]\n      y1_data = [0.01, 0.1, 1, 10, 1, 10, 100, 1000]\n      y2_data = [100, 10, 1, 0.1, 1, 0.1, 0.01, 0.001]\n      for dev in TensorForwardTest.devices:\n          x = tF.raw_input(Shape([2, 2], 2), x_data, dev)\n          k = tF.raw_input(Shape([], 2), k_data, dev)\n          y1 = k / x\n          self.assertEqual(Shape([2, 2], 2), y1.shape())\n          self.assertTrue(np.isclose(y1_data, y1.to_list()).all())\n          y2 = x / k\n          self.assertEqual(Shape([2, 2], 2), y2.shape())\n          self.assertTrue(np.isclose(y2_data, y2.to_list()).all())\n\n    def test_TensorForwardTest_CheckDivideScalarBatchBroadcast(self):\n        x_data = [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001]\n        k_data = [10]\n        y1_data = [0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]\n        y2_data = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2], 2), x_data, dev)\n            k = tF.raw_input([], k_data, dev)\n            y1 = k / x\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertTrue(np.isclose(y1_data, y1.to_list()).all())\n            y2 = x / k\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertTrue(np.isclose(y2_data, y2.to_list()).all())\n        x_data = [1000, 100, 10, 1]\n        k_data = [10, 0.1]\n        y1_data = [0.01, 0.1, 1, 10, 0.0001, 0.001, 0.01, 0.1]\n        y2_data = [100, 10, 1, 0.1, 10000, 1000, 100, 10]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input([2, 2], x_data, dev)\n            k = tF.raw_input(Shape([], 2), k_data, dev)\n            y1 = k / x\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertTrue(np.isclose(y1_data, y1.to_list()).all())\n            y2 = x / k\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertTrue(np.isclose(y2_data, y2.to_list()).all())\n\n    def test_TensorForwardTest_CheckDivide(self):\n        a_data = [1000, -100, 10, -1, 0.1, -0.01, 0.001, -0.0001]\n        b_data = [1, 2, 3, 4, -5, -6, -7, -8]\n        y1_data = [\n            1000, -50, 10.0/3, -0.25, -0.02, 0.01/6, -0.001/7, 1.25e-5,\n        ]\n        y2_data = [0.001, -0.02, 0.3, -4, -50, 600, -7000, 80000]\n        for dev in TensorForwardTest.devices:\n            a = tF.raw_input(Shape([2, 2], 2), a_data, dev)\n            b = tF.raw_input(Shape([2, 2], 2), b_data, dev)\n            y1 = a / b\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertTrue(np.isclose(y1_data, y1.to_list()).all())\n            y2 = b / a\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertTrue(np.isclose(y2_data, y2.to_list()).all())\n\n    def test_TensorForwardTest_CheckDivideBatchBroadcast(self):\n        a_data = [1, 2, 3, 4]\n        b_data = [1, 1, 1, 1, 1, 2, 3, 4]\n        y1_data = [1, 2, 3, 4, 1, 1, 1, 1]\n        y2_data = [1, 0.5, 1.0/3, 0.25, 1, 1, 1, 1]\n        for dev in TensorForwardTest.devices:\n            a = tF.raw_input([2, 2], a_data, dev)\n            b = tF.raw_input(Shape([2, 2], 2), b_data, dev)\n            y1 = a / b\n            self.assertEqual(Shape([2, 2], 2), y1.shape())\n            self.assertTrue(np.isclose(y1_data, y1.to_list()).all())\n            y2 = b / a\n            self.assertEqual(Shape([2, 2], 2), y2.shape())\n            self.assertTrue(np.isclose(y2_data, y2.to_list()).all())\n\n    def test_TensorForwardTest_CheckInvalidArithmeticOps(self):\n        sa = [\n            Shape([2, 2], 2), Shape([2, 2], 2), Shape([2, 2], 2),\n        ]\n        sb = [\n            Shape([2, 2], 3), Shape([3, 3], 2), Shape([3, 3], 3),\n        ]\n        for dev in TensorForwardTest.devices:\n            for ssa, ssb in zip(sa, sb):\n                a = tF.zeros(ssa, dev)\n                b = tF.zeros(ssb, dev)\n                with self.assertRaises(RuntimeError):\n                    a + b\n                with self.assertRaises(RuntimeError):\n                    a - b\n                with self.assertRaises(RuntimeError):\n                    a * b\n                with self.assertRaises(RuntimeError):\n                    a / b\n\n    def test_TensorForwardTest_CheckTranspose11(self):\n        for dev in TensorForwardTest.devices:\n            x_data = [42]\n            y_data = [42]\n            x = tF.raw_input([], x_data, dev)\n            y = tF.transpose(x)\n            self.assertEqual(Shape(), y.shape())\n            self.assertEqual(y_data, y.to_list())\n\n    def test_TensorForwardTest_CheckTransposeN1(self):\n        x_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n        y_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input([12], x_data, dev)\n            y = tF.transpose(x)\n            self.assertEqual(Shape([1, 12]), y.shape())\n            self.assertEqual(y_data, y.to_list())\n\n    def test_TensorForwardTest_CheckTranspose1N(self):\n        x_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n        y_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([1, 3], 4), x_data, dev)\n            y = tF.transpose(x)\n            self.assertEqual(Shape([3], 4), y.shape())\n            self.assertEqual(y_data, y.to_list())\n\n    def test_TensorForwardTest_CheckTransposeNN(self):\n        x_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n        y_data = [1, 3, 2, 4, 5, 7, 6, 8, 9, 11, 10, 12]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2], 3), x_data, dev)\n            y = tF.transpose(x)\n            self.assertEqual(Shape([2, 2], 3), y.shape())\n            self.assertEqual(y_data, y.to_list())\n\n    def test_TensorForwardTest_CheckTransposeMN(self):\n        x_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n        y_data = [1, 3, 5, 2, 4, 6, 7, 9, 11, 8, 10, 12]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n            y = tF.transpose(x)\n            self.assertEqual(Shape([3, 2], 2), y.shape())\n            self.assertEqual(y_data, y.to_list())\n\n    def test_TensorForwardTest_CheckInvalidTranspose(self):\n        for dev in TensorForwardTest.devices:\n            x = tF.zeros([2, 3, 4], dev)\n            with self.assertRaises(RuntimeError):\n                tF.transpose(x)\n\n    def test_TensorForwardTest_CheckMatMulAA(self):\n        x_data = [1, 2, 3, 4, 1, 0, 0, 1, 0, 2, 3, 0]\n        y_data = [7, 10, 15, 22, 1, 0, 0, 1, 6, 0, 0, 6]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2], 3), x_data, dev)\n            y1 = tF.matmul(x, x)\n            y2 = x @ x\n            self.assertEqual(Shape([2, 2], 3), y1.shape())\n            self.assertEqual(y_data, y1.to_list())\n            self.assertEqual(Shape([2, 2], 3), y2.shape())\n            self.assertEqual(y_data, y2.to_list())\n\n    def test_TensorForwardTest_CheckMatMulAB(self):\n        a_data = [\n            1, 1000, 1,\n            10, 100, 100,\n            100, 10, 10000,\n            1000, 1, 1000000,\n        ]\n        b_data = [\n            0, 2, 4, 6,\n            1, 3, 5, 7,\n            8, 6, 4, 2,\n            9, 7, 5, 3,\n            2, 3, 5, 7,\n            9, 4, 1, 0,\n        ]\n        y_data = [\n            6420, 246, 6040200,\n            7531, 1357, 7050301,\n            2468, 8642, 2040608,\n            3579, 9753, 3050709,\n            7532, 2357, 7050302,\n            149, 9410, 10409,\n        ]\n        for dev in TensorForwardTest.devices:\n            a = tF.raw_input([3, 4], a_data, dev)\n            b = tF.raw_input([4, 6], b_data, dev)\n            y1 = tF.matmul(a, b)\n            y2 = a @ b\n            self.assertEqual(Shape([3, 6]), y1.shape())\n            self.assertEqual(y_data, y1.to_list())\n            self.assertEqual(Shape([3, 6]), y2.shape())\n            self.assertEqual(y_data, y2.to_list())\n\n    def test_TensorForwardTest_CheckMatMulBatchBroadcast1N(self):\n        a_data = [10, 1000, 1, 100]\n        b_data = [1, 2, 3, 4, 5, 6, 7, 8]\n        y_data = [12, 1200, 34, 3400, 56, 5600, 78, 7800]\n        for dev in TensorForwardTest.devices:\n            a = tF.raw_input([2, 2], a_data, dev)\n            b = tF.raw_input(Shape([2, 2], 2), b_data, dev)\n            y = tF.matmul(a, b)\n            self.assertEqual(Shape([2, 2], 2), y.shape())\n            self.assertEqual(y_data, y.to_list())\n\n    def test_TensorForwardTest_CheckMatMulBatchBroadcastN1(self):\n        a_data = [1, 2, 3, 4, 5, 6, 7, 8]\n        b_data = [10, 1, 1000, 100]\n        y_data = [13, 24, 1300, 2400, 57, 68, 5700, 6800]\n        for dev in TensorForwardTest.devices:\n            a = tF.raw_input(Shape([2, 2], 2), a_data, dev)\n            b = tF.raw_input([2, 2], b_data, dev)\n            y = tF.matmul(a, b)\n            self.assertEqual(Shape([2, 2], 2), y.shape())\n            self.assertEqual(y_data, y.to_list())\n\n    def test_TensorForwardTest_CheckMatMulLarge(self):\n        N = 123\n        a_data = [0] * (N * N)\n        b_data = [0] * (N * N)\n        y1_data = [0] * (N * N)\n        y2_data = [0] * (N * N)\n        k = 0\n        for i in range(N):\n            k += i * i\n        for i in range(N):\n            for j in range(N):\n                a_data[i + j * N] = i\n                b_data[i + j * N] = j\n                y1_data[i + j * N] = N * i * j\n                y2_data[i + j * N] = k\n        for dev in TensorForwardTest.devices:\n            a = tF.raw_input(Shape([N, N]), a_data, dev)\n            b = tF.raw_input([N, N], b_data, dev)\n            y1 = tF.matmul(a, b)\n            y2 = tF.matmul(b, a)\n            self.assertEqual(Shape([N, N]), y1.shape())\n            self.assertEqual(Shape([N, N]), y2.shape())\n            self.assertEqual(y1_data, y1.to_list())\n            self.assertEqual(y2_data, y2.to_list())\n\n    def test_TensorForwardTest_CheckInvalidMatMul(self):\n        for dev in TensorForwardTest.devices:\n            a = tF.zeros([2, 3], dev)\n            b = tF.zeros([], dev)\n            with self.assertRaises(RuntimeError):\n                tF.matmul(a, b)\n            a = tF.zeros([], dev)\n            b = tF.zeros([2, 3], dev)\n            with self.assertRaises(RuntimeError):\n                tF.matmul(a, b)\n            a = tF.zeros([2, 3, 4], dev)\n            b = tF.zeros([4], dev)\n            with self.assertRaises(RuntimeError):\n                tF.matmul(a, b)\n            a = tF.zeros([1, 2], dev)\n            b = tF.zeros([2, 3, 4], dev)\n            with self.assertRaises(RuntimeError):\n                tF.matmul(a, b)\n            a = tF.zeros([2, 3], dev)\n            b = tF.zeros([2, 3], dev)\n            with self.assertRaises(RuntimeError):\n                tF.matmul(a, b)\n\n    def test_TensorForwardTest_CheckSqrt(self):\n        x_data = [\n            0, 1, 2, 3, 4, 5,\n            0, 1, 4, 9, 16, 25,\n        ]\n        y_data = [\n            0, 1, 1.41421356, 1.73205041, 2, 2.23606798,\n            0, 1, 2, 3, 4, 5,\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n            y = tF.sqrt(x)\n            self.assertEqual(Shape([2, 3], 2), y.shape())\n            self.assertTrue(np.isclose(y_data, y.to_list()).all())\n\n    def test_TensorForwardTest_CheckExp(self):\n        x_data = [\n            0, .5, 1, 2, 4, 8,\n            0, -.5, -1, -2, -4, -8,\n        ]\n        y_data = [\n            1, 1.6487213, 2.7182818, 7.3890561, 54.598150, 2980.9580,\n            1, .60653066, .36787944, .13533528, .018315639, .00033546263,\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n            y = tF.exp(x)\n            self.assertEqual(Shape([2, 3], 2), y.shape())\n            self.assertTrue(np.isclose(y_data, y.to_list()).all())\n\n    def test_TensorForwardTest_CheckLog(self):\n        x_data = [\n            0.01, .5, 1, 2, 4, 8,\n            0.01, .5, 1, 2, 4, 8,\n        ]\n        y_data = [\n            -4.60517019, -0.69314718, 0, 0.69314718, 1.38629436, 2.07944154,\n            -4.60517019, -0.69314718, 0, 0.69314718, 1.38629436, 2.07944154,\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n            y = tF.log(x)\n            self.assertEqual(Shape([2, 3], 2), y.shape())\n            self.assertTrue(np.isclose(y_data, y.to_list()).all())\n\n    def test_TensorForwardTest_CheckPow(self):\n        x_data = [\n            0.01, .5, 1, 2, 4, 8,\n            0.01, .5, 1, 2, 4, 8,\n        ]\n        y_data = [\n            0.00001, 0.17677670, 1, 5.65685425, 32, 181.01933598,\n            0.00001, 0.17677670, 1, 5.65685425, 32, 181.01933598,\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n            y1 = tF.pow(x, 2.5)\n            y2 = x ** 2.5\n            self.assertEqual(Shape([2, 3], 2), y1.shape())\n            self.assertTrue(np.isclose(y_data, y1.to_list()).all())\n            self.assertEqual(Shape([2, 3], 2), y2.shape())\n            self.assertTrue(np.isclose(y_data, y2.to_list()).all())\n\n    def test_TensorForwardTest_CheckIPowPositive(self):\n        x_data = [\n            0.01, .5, 1, 2, 4, 8,\n            -0.01, -.5, -1, -2, -4, -8,\n        ]\n        y_data = [\n            0.000001, 0.125, 1, 8, 64, 512,\n            -0.000001, -0.125, -1, -8, -64, -512,\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n            y1 = tF.pow(x, 3)\n            y2 = x ** 3\n            self.assertEqual(Shape([2, 3], 2), y1.shape())\n            self.assertTrue(np.isclose(y_data, y1.to_list()).all())\n            self.assertEqual(Shape([2, 3], 2), y2.shape())\n            self.assertTrue(np.isclose(y_data, y2.to_list()).all())\n\n    def test_TensorForwardTest_CheckIPowNegative(self):\n        x_data = [\n            0.01, .5, 1, 2, 4, 8,\n            -0.01, -.5, -1, -2, -4, -8,\n        ]\n        y_data = [\n            1000000, 8, 1, 0.125, 0.015625, 0.001953125,\n            -1000000, -8, -1, -0.125, -0.015625, -0.001953125,\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n            y1 = tF.pow(x, -3)\n            y2 = x ** -3\n            self.assertEqual(Shape([2, 3], 2), y1.shape())\n            self.assertTrue(np.isclose(y_data, y1.to_list()).all())\n            self.assertEqual(Shape([2, 3], 2), y2.shape())\n            self.assertTrue(np.isclose(y_data, y2.to_list()).all())\n\n    def test_TensorForwardTest_CheckIPowUpperBound(self):\n        x_data = [\n            1, -1, 1, -1, 1, -1,\n            1, -1, 1, -1, 1, -1,\n        ]\n        y_data = [\n            1, -1, 1, -1, 1, -1,\n            1, -1, 1, -1, 1, -1,\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n            y1 = tF.pow(x, 0x7fffffff)\n            y2 = x ** 0x7fffffff\n            self.assertEqual(Shape([2, 3], 2), y1.shape())\n            self.assertTrue(np.isclose(y_data, y1.to_list()).all())\n            self.assertEqual(Shape([2, 3], 2), y2.shape())\n            self.assertTrue(np.isclose(y_data, y2.to_list()).all())\n\n    def test_TensorForwardTest_CheckIPowLowerBound(self):\n        x_data = [\n            1, -1, 1, -1, 1, -1,\n            1, -1, 1, -1, 1, -1,\n        ]\n        y_data  = [\n            1, 1, 1, 1, 1, 1,\n            1, 1, 1, 1, 1, 1,\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n            y1 = tF.pow(x, -2147483648) # 0x80000000\n            y2 = x ** -2147483648\n            self.assertEqual(Shape([2, 3], 2), y1.shape())\n            self.assertTrue(np.isclose(y_data, y1.to_list()).all())\n            self.assertEqual(Shape([2, 3], 2), y2.shape())\n            self.assertTrue(np.isclose(y_data, y2.to_list()).all())\n\n    def test_TensorForwardTest_CheckIPowPositiveConvergence(self):\n        x_data = [\n            0.9999999, -0.9999999, 0.9999999, -0.9999999, 0.9999999, -0.9999999,\n            0.9999999, -0.9999999, 0.9999999, -0.9999999, 0.9999999, -0.9999999,\n        ]\n        y_data = [\n            0, 0, 0, 0, 0, 0,\n            0, 0, 0, 0, 0, 0,\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n            y1 = tF.pow(x, 0x7fffffff)\n            y2 = x ** 0x7fffffff\n            self.assertEqual(Shape([2, 3], 2), y1.shape())\n            self.assertEqual(y_data, y1.to_list())\n            self.assertEqual(Shape([2, 3], 2), y2.shape())\n            self.assertEqual(y_data, y2.to_list())\n\n    def test_TensorForwardTest_CheckIPowNegativeConvergence(self):\n        x_data = [\n            1.000001, -1.000001, 1.000001, -1.000001, 1.000001, -1.000001,\n            1.000001, -1.000001, 1.000001, -1.000001, 1.000001, -1.000001,\n        ]\n        y_data = [\n            0, 0, 0, 0, 0, 0,\n            0, 0, 0, 0, 0, 0,\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n            y1 = tF.pow(x, -2147483648) # 0x80000000\n            y2 = x ** -2147483648\n            self.assertEqual(Shape([2, 3], 2), y1.shape())\n            self.assertEqual(y_data, y1.to_list())\n            self.assertEqual(Shape([2, 3], 2), y2.shape())\n            self.assertEqual(y_data, y2.to_list())\n\n    def test_TensorForwardTest_CheckTanh(self):\n        x_data = [\n            0, .5, 1, 2, 4, 8,\n            0, -.5, -1, -2, -4, -8,\n        ]\n        y_data = [\n            0, .46211716, .76159416, .96402758, .99932930, .99999977,\n            0, -.46211716, -.76159416, -.96402758, -.99932930, -.99999977,\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n            y = tF.tanh(x)\n            self.assertEqual(Shape([2, 3], 2), y.shape())\n            self.assertTrue(np.isclose(y_data, y.to_list()).all())\n\n    def test_TensorForwardTest_CheckSigmoid(self):\n        x_data = [\n            0, .5, 1, 2, 3, 4,\n            0, -.5, -1, -2, -3, -4,\n        ]\n        y_data = [\n            .5, .62245933, .73105858, .88079708, .95257413, .98201379,\n            .5, .37754067, .26894142, .11920292, .047425873, .017986210,\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n            y = tF.sigmoid(x)\n            self.assertEqual(Shape([2, 3], 2), y.shape())\n            self.assertTrue(np.isclose(y_data, y.to_list()).all())\n\n    def test_TensorForwardTest_CheckSoftplus(self):\n        x_data = [\n            0, .5, 1, 2, 3, 4,\n            0, -.5, -1, -2, -3, -4,\n        ]\n        y_data = [\n            .69314718, .97407698, 1.3132617, 2.1269280, 3.0485874, 4.0181499,\n            .69314718, .47407698, .31326169, .12692801, .048587352, .018149928,\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n            y = tF.softplus(x)\n            self.assertEqual(Shape([2, 3], 2), y.shape())\n            self.assertTrue(np.isclose(y_data, y.to_list(), 0, 1e-6).all())\n\n    def test_TensorForwardTest_CheckSin(self):\n        x_data = [\n            0, .5, 1, 2, 3, 4,\n            0, -.5, -1, -2, -3, -4,\n        ]\n        y_data = [\n            0, .47942554, .84147098, .90929743, .14112001, -.75680250,\n            0, -.47942554, -.84147098, -.90929743, -.14112001, .75680250,\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n            y = tF.sin(x)\n            self.assertEqual(Shape([2, 3], 2), y.shape())\n            self.assertTrue(np.isclose(y_data, y.to_list()).all())\n\n    def test_TensorForwardTest_CheckCos(self):\n        x_data = [\n            0, .5, 1, 2, 3, 4,\n            0, -.5, -1, -2, -3, -4,\n        ]\n        y_data = [\n            1, .87758256, .54030231, -.41614684, -.98999250, -.65364362,\n            1, .87758256, .54030231, -.41614684, -.98999250, -.65364362,\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n            y = tF.cos(x)\n            self.assertEqual(Shape([2, 3], 2), y.shape())\n            self.assertTrue(np.isclose(y_data, y.to_list()).all())\n\n    def test_TensorForwardTest_CheckTan(self):\n        x_data = [\n            0, .5, 1, 2, 3, 4,\n            0, -.5, -1, -2, -3, -4,\n        ]\n        y_data = [\n            0, .54630249, 1.5574077, -2.1850399, -.14254654, 1.1578213,\n            0, -.54630249, -1.5574077, 2.1850399, .14254654, -1.1578213,\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n            y = tF.tan(x)\n            self.assertEqual(Shape([2, 3], 2), y.shape())\n            self.assertTrue(np.isclose(y_data, y.to_list()).all())\n\n    def test_TensorForwardTest_CheckReLU(self):\n        x_data = [\n            0, .5, 1, 2, 4, 8,\n            0, -.5, -1, -2, -4, -8,\n        ]\n        y_data = [\n            0, .5, 1, 2, 4, 8,\n            0, 0, 0, 0, 0, 0,\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n            y = tF.relu(x)\n            self.assertEqual(Shape([2, 3], 2), y.shape())\n            self.assertEqual(y_data, y.to_list())\n\n    def test_TensorForwardTest_CheckLReLU(self):\n        x_data = [\n            0, .5, 1, 2, 4, 8,\n            0, -.5, -1, -2, -4, -8,\n        ]\n        y_data = [\n            0, .5, 1, 2, 4, 8,\n            0, -.005, -.01, -.02, -.04, -.08,\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n            y = tF.lrelu(x)\n            self.assertEqual(Shape([2, 3], 2), y.shape())\n            self.assertTrue(np.isclose(y_data, y.to_list()).all())\n\n    def test_TensorForwardTest_CheckPReLU(self):\n        ks = [.01, .1, 1., 10., 100., -.01, -.1, -1., -10., -100.]\n        for dev in TensorForwardTest.devices:\n            for k in ks:\n                x_data = [\n                    0, .5, 1, 2, 4, 8,\n                    0, -.5, -1, -2, -4, -8,\n                ]\n                y_data = [\n                    0, .5, 1, 2, 4, 8,\n                    0, -.5 * k, -k, -2 * k, -4 * k, -8 * k,\n                ]\n                x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n                y = tF.prelu(x, k)\n                self.assertEqual(Shape([2, 3], 2), y.shape())\n                self.assertTrue(np.isclose(y_data, y.to_list()).all())\n\n    def test_TensorForwardTest_CheckELU(self):\n        ks = [.01, .1, 1., 10., 100., -.01, -.1, -1., -10., -100.]\n        for dev in TensorForwardTest.devices:\n            for k in ks:\n                x_data = [\n                    0, .5, 1, 2, 4, 8,\n                    0, -.5, -1, -2, -4, -8,\n                ]\n                y_data = [\n                    0, .5, 1, 2, 4, 8,\n                    0, -.39346934 * k, -.63212056 * k,\n                    -.86466472 * k, -.98168436 * k, -.99966454 * k,\n                ]\n                x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n                y = tF.elu(x, k)\n                self.assertEqual(Shape([2, 3], 2), y.shape())\n                self.assertTrue(np.isclose(y_data, y.to_list()).all())\n\n    def test_TensorForwardTest_CheckSum(self):\n        x_data = [\n            1, 2, 3, 4, 5, 6, 7, 8, -1, -2, -3, -4, -5, -6, -7, -8,\n        ]\n        shape = [\n            Shape([1, 2, 2], 2),\n            Shape([2, 1, 2], 2),\n            Shape([2, 2], 2),\n            Shape([2, 2, 2], 2),\n        ]\n        y_data = [\n            [3, 7, 11, 15, -3, -7, -11, -15],\n            [4, 6, 12, 14, -4, -6, -12, -14],\n            [6, 8, 10, 12, -6, -8, -10, -12],\n            [1, 2, 3, 4, 5, 6, 7, 8, -1, -2, -3, -4, -5, -6, -7, -8],\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2, 2], 2), x_data, dev)\n            for i in range(4):\n                y = tF.sum(x, i)\n                self.assertEqual(shape[i], y.shape())\n                self.assertEqual(y_data[i], y.to_list())\n\n    def test_TensorForwardTest_CheckSum2(self):\n        ns = [\n            1, 2, 3, 15, 16, 17, 255, 256, 257, 1023, 1024, 1025, 65535, 65536, 65537,\n        ]\n        for dev in TensorForwardTest.devices:\n            for n in ns:\n                x = tF.constant([n], 1)\n                y = tF.sum(x, 0)\n                self.assertEqual(Shape(), y.shape())\n                self.assertEqual(n, y.to_float())\n\n    def test_TensorForwardTest_CheckLogSumExp(self):\n        x_data = [\n            1, 2, 3, 4, 5, 6, 7, 8, -1, -2, -3, -4, -5, -6, -7, -8,\n        ]\n        shape = [\n            Shape([1, 2, 2], 2),\n            Shape([2, 1, 2], 2),\n            Shape([2, 2], 2),\n            Shape([2, 2, 2], 2),\n        ]\n        y_data = [\n            [2.31326169, 4.31326169, 6.31326169, 8.31326169,\n             -0.68673831, -2.68673831, -4.68673831, -6.68673831],\n            [3.12692801, 4.12692801, 7.12692801, 8.12692801,\n             -0.87307199, -1.87307199, -4.87307199, -5.87307199],\n            [5.01814993, 6.01814993, 7.01814993, 8.01814993,\n             -0.98185007, -1.98185007, -2.98185007, -3.98185007],\n            [1, 2, 3, 4, 5, 6, 7, 8, -1, -2, -3, -4, -5, -6, -7, -8],\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2, 2], 2), x_data, dev)\n            for i in range(4):\n                y = tF.logsumexp(x, i)\n                self.assertEqual(shape[i], y.shape())\n                self.assertTrue(np.isclose(y_data[i], y.to_list()).all())\n\n    def test_TensorForwardTest_CheckLogSumExp2(self):\n      ns = [\n          1, 2, 3, 15, 16, 17, 255, 256, 257, 1023,\n          1024, 1025, 65535, 65536, 65537,\n      ]\n      for dev in TensorForwardTest.devices:\n          for n in ns:\n              for k in [-5, -1, 0, 1, 5]:\n                  x = tF.constant([n], k, dev)\n                  y = tF.logsumexp(x, 0)\n                  self.assertEqual(Shape(), y.shape())\n                  self.assertTrue(np.isclose(\n                        [k + math.log(n)], y.to_list(), 0, 1e-3))\n\n    def test_TensorForwardTest_CheckLogSoftmax(self):\n        x_data = [\n            1, 2, 3, 4, 5, 6, 7, 8, -1, -2, -3, -4, -5, -6, -7, -8,\n        ]\n        y_data = [\n            [-1.31326169, -0.31326169, -1.31326169, -0.31326169,\n             -1.31326169, -0.31326169, -1.31326169, -0.31326169,\n             -0.31326169, -1.31326169, -0.31326169, -1.31326169,\n             -0.31326169, -1.31326169, -0.31326169, -1.31326169],\n            [-2.12692801, -2.12692801, -0.12692801, -0.12692801,\n             -2.12692801, -2.12692801, -0.12692801, -0.12692801,\n             -0.12692801, -0.12692801, -2.12692801, -2.12692801,\n             -0.12692801, -0.12692801, -2.12692801, -2.12692801],\n            [-4.01814993, -4.01814993, -4.01814993, -4.01814993,\n             -0.01814993, -0.01814993, -0.01814993, -0.01814993,\n             -0.01814993, -0.01814993, -0.01814993, -0.01814993,\n             -4.01814993, -4.01814993, -4.01814993, -4.01814993],\n            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2, 2], 2), x_data, dev)\n            for i in range(4):\n                y = tF.log_softmax(x, i)\n                self.assertEqual(Shape([2, 2, 2], 2), y.shape())\n                self.assertTrue(np.isclose(y_data[i], y.to_list(), 0, 1e-6).all())\n\n    def test_TensorForwardTest_CheckLogSoftmax2(self):\n        ns = [\n            1, 2, 3, 15, 16, 17, 255, 256, 257, 1023,\n            1024, 1025, 65535, 65536, 65537,\n        ]\n        for dev in TensorForwardTest.devices:\n            for n in ns:\n                for k in [-5, -1, 0, 1, 5]:\n                    x = tF.constant([n], k, dev)\n                    y = tF.log_softmax(x, 0)\n                    self.assertEqual(Shape([n]), y.shape())\n                    self.assertTrue(\n                      np.isclose([-math.log(n)] * n, y.to_list(), 0, 1e-3).all())\n\n    def test_TensorForwardTest_CheckSoftmax(self):\n        x_data = [\n          1, 2, 3, 4, 5, 6, 7, 8, -1, -2, -3, -4, -5, -6, -7, -8,\n        ]\n        y_data = [\n            [0.26894142, 0.73105858, 0.26894142, 0.73105858,\n             0.26894142, 0.73105858, 0.26894142, 0.73105858,\n             0.73105858, 0.26894142, 0.73105858, 0.26894142,\n             0.73105858, 0.26894142, 0.73105858, 0.26894142],\n            [0.11920292, 0.11920292, 0.88079708, 0.88079708,\n             0.11920292, 0.11920292, 0.88079708, 0.88079708,\n             0.88079708, 0.88079708, 0.11920292, 0.11920292,\n             0.88079708, 0.88079708, 0.11920292, 0.11920292],\n            [0.01798621, 0.01798621, 0.01798621, 0.01798621,\n             0.98201379, 0.98201379, 0.98201379, 0.98201379,\n             0.98201379, 0.98201379, 0.98201379, 0.98201379,\n             0.01798621, 0.01798621, 0.01798621, 0.01798621],\n            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2, 2], 2), x_data, dev)\n            for i in range(4):\n                y = tF.softmax(x, i)\n                self.assertEqual(Shape([2, 2, 2], 2), y.shape())\n                self.assertTrue(np.isclose(y_data[i], y.to_list(), 0, 1e-6).all())\n\n    def test_TensorForwardTest_CheckSoftmax2(self):\n        ns = [\n            1, 2, 3, 15, 16, 17, 255, 256, 257, 1023,\n            1024, 1025, 65535, 65536, 65537,\n        ]\n        for dev in TensorForwardTest.devices:\n            for n in ns:\n                for k in [-5, -1, 0, 1, 5]:\n                    x = tF.constant([n], k, dev)\n                    y = tF.softmax(x, 0)\n                    self.assertEqual(Shape([n]), y.shape())\n                    self.assertTrue(\n                      np.isclose([1./n] * n, y.to_list(), 0, 1e-6).all())\n\n    def test_TensorForwardTest_CheckBroadcast(self):\n        test_cases = [\n            (0, 1, Shape([]), [1]),\n            (0, 20, Shape([20]), [1] * 20),\n            (1, 50, Shape([1, 50]), [1] * 50),\n            (2, 100, Shape([1, 1, 100]), [1] * 100),\n        ]\n        for dev in TensorForwardTest.devices:\n            for tc in test_cases:\n                x = tF.constant([], 1, dev)\n                y = tF.broadcast(x, tc[0], tc[1])\n                self.assertEqual(tc[2], y.shape())\n                self.assertEqual(tc[3], y.to_list())\n\n    def test_TensorForwardTest_CheckBroadcast2(self):\n        test_cases = [\n            (1, 1, Shape([2], 3), [1, 2, 3, 4, 5, 6]),\n            (2, 1, Shape([2], 3), [1, 2, 3, 4, 5, 6]),\n            (1, 2, Shape([2, 2], 3), [1, 2, 1, 2, 3, 4, 3, 4, 5, 6, 5, 6]),\n            (2, 2, Shape([2, 1, 2], 3), [1, 2, 1, 2, 3, 4, 3, 4, 5, 6, 5, 6]),\n        ]\n        for dev in TensorForwardTest.devices:\n            for tc in test_cases:\n                x = tF.raw_input(Shape([2], 3), [1, 2, 3, 4, 5, 6], dev)\n                y = tF.broadcast(x, tc[0], tc[1])\n                self.assertEqual(tc[2], y.shape())\n                self.assertEqual(tc[3], y.to_list())\n\n    def test_TensorForwardTest_CheckBroadcast3(self):\n        test_cases = [\n            (0, 1, Shape([1, 2, 1, 2], 2),\n              [1, 2, 3, 4, 5, 6, 7, 8]),\n            (2, 1, Shape([1, 2, 1, 2], 2),\n              [1, 2, 3, 4, 5, 6, 7, 8]),\n            (4, 1, Shape([1, 2, 1, 2], 2),\n              [1, 2, 3, 4, 5, 6, 7, 8]),\n            (0, 2, Shape([2, 2, 1, 2], 2),\n              [1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8]),\n            (2, 2, Shape([1, 2, 2 ,2], 2),\n              [1, 2, 1, 2, 3, 4, 3, 4, 5, 6, 5, 6, 7, 8, 7, 8]),\n            (4, 2, Shape([1, 2, 1, 2, 2], 2),\n              [1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 5, 6, 7, 8]),\n        ]\n        for dev in TensorForwardTest.devices:\n            for tc in test_cases:\n                x = tF.raw_input(\n                    Shape([1, 2, 1, 2], 2), [1, 2, 3, 4, 5, 6, 7, 8], dev)\n                y = tF.broadcast(x, tc[0], tc[1])\n                self.assertEqual(tc[2], y.shape())\n                self.assertEqual(tc[3], y.to_list())\n\n    def test_TensorForwardTest_CheckInvalidBroadcast(self):\n        for dev in TensorForwardTest.devices:\n            x = tF.zeros([1, 2], dev)\n            with self.assertRaises(RuntimeError):\n                tF.broadcast(x, 0, 0)\n            with self.assertRaises(RuntimeError):\n                tF.broadcast(x, 1, 0)\n            with self.assertRaises(RuntimeError):\n                tF.broadcast(x, 1, 1)\n            with self.assertRaises(RuntimeError):\n                tF.broadcast(x, 1, 3)\n            with self.assertRaises(RuntimeError):\n                tF.broadcast(x, 2, 0)\n\n    def test_TensorForwardTest_CheckBatchSum(self):\n        x_data = [\n            1, 2, 3, 4, 5, 6, 7, 8,\n            -2, -4, -6, -8, -10, -12, -14, -16,\n        ]\n        y_data = [\n            -1, -2, -3, -4, -5, -6, -7, -8,\n        ]\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 2, 2], 2), x_data, dev)\n            y = tF.batch.sum(x)\n            self.assertEqual(Shape([2, 2, 2]), y.shape())\n            self.assertEqual(y_data, y.to_list())\n\n    def test_TensorForwardTest_CheckSoftmaxCrossEntropy(self):\n        x_data = [\n            [-1, 0, 1, 1, 0, 0, 0, 0, 1],\n            [-1, 1, 0, 0, 0, 0, 1, 0, 1],\n        ]\n        t_data = [\n            [1./3, 1./3, 1./3, .5, .25, .25, 0, 0, 1],\n            [1./3, .5, 0, 1./3, .25, 0, 1./3, .25, 1],\n        ]\n        y_data = [\n            [1.40760596, 1.05144471, 0.55144471],\n            [1.40760596, 1.05144471, 0.55144471],\n        ]\n        shape = [Shape([1, 3]), Shape([3])]\n        for dev in TensorForwardTest.devices:\n            for dim in [0, 1]:\n                x = tF.raw_input([3, 3], x_data[dim], dev)\n                t = tF.raw_input([3, 3], t_data[dim], dev)\n                y = tF.softmax_cross_entropy(x, t, dim)\n                self.assertEqual(shape[dim], y.shape())\n                self.assertTrue(np.isclose(y_data[dim], y.to_list()).all())\n\n    def test_TensorForwardTest_CheckSoftmaxCrossEntropyBatchBroadcast(self):\n        test_cases = [\n            ([-1, 0, 1],\n             [1, 0, 0, 0, 1, 0, 0, 0, 1],\n             [2.40760596, 1.40760596, 0.40760596],\n             Shape([3]), Shape([3], 3), Shape([], 3)),\n            ([-1, 0, 1, 1, -1, 0, 0, 1, -1],\n             [1, 0, 0],\n             [2.40760596, 0.40760596, 1.40760596],\n             Shape([3], 3), Shape([3]), Shape([], 3)),\n        ]\n        for dev in TensorForwardTest.devices:\n            for tc in test_cases:\n                x = tF.raw_input(tc[3], tc[0], dev)\n                t = tF.raw_input(tc[4], tc[1], dev)\n                y = tF.softmax_cross_entropy(x, t, 0)\n                self.assertEqual(tc[5], y.shape())\n                self.assertTrue(np.isclose(tc[2], y.to_list()).all())\n\n    def test_TensorForwardTest_CheckInvalidSoftmaxCrossEntropy(self):\n        for dev in TensorForwardTest.devices:\n            x = tF.constant([2, 2], .5, dev)\n            t = tF.constant([2, 3], .5, dev)\n            with self.assertRaises(RuntimeError):\n                tF.softmax_cross_entropy(x, t, 0)\n            with self.assertRaises(RuntimeError):\n                tF.softmax_cross_entropy(x, t, 1)\n            with self.assertRaises(RuntimeError):\n                tF.softmax_cross_entropy(x, t, 2)\n            x = tF.constant(Shape([2, 2], 2), .5, dev)\n            t = tF.constant(Shape([2, 3], 3), .5, dev)\n            with self.assertRaises(RuntimeError):\n                tF.softmax_cross_entropy(x, t, 0)\n            with self.assertRaises(RuntimeError):\n                tF.softmax_cross_entropy(x, t, 1)\n            with self.assertRaises(RuntimeError):\n                tF.softmax_cross_entropy(x, t, 2)\n\n    def test_TensorForwardTest_CheckSparseSoftmaxCrossEntropy(self):\n        test_cases = [\n            ([-1, 0, 1, 1, -1, 0, 0, 1, -1],\n             0, [0], Shape([3, 3]), Shape([1, 3]),\n             [2.40760596, 0.40760596, 1.40760596]),\n            ([-1, 0, 1, 1, -1, 0, 0, 1, -1],\n             1, [1], Shape([3, 3]), Shape([3]),\n             [0.40760596, 2.40760596, 1.40760596]),\n            ([-1, 0, 1, 1, -1, 0, 0, 1, -1],\n             2, [0], Shape([3, 3]), Shape([3, 3]),\n             [0, 0, 0, 0, 0, 0, 0, 0, 0]),\n            ([-1, 0, 1, 1, -1, 0, 0, 1, -1, -2, 0, 2, 2, -2, 0, 0, 2, -2],\n             0, [0, 1], Shape([3, 3], 2), Shape([1, 3], 2),\n             [2.40760596, 0.40760596, 1.40760596,\n              2.14293163, 4.14293163, 0.14293163]),\n            ([-1, 0, 1, 1, -1, 0, 0, 1, -1, -2, 0, 2, 2, -2, 0, 0, 2, -2],\n             0, [0], Shape([3, 3], 2), Shape([1, 3], 2),\n             [2.40760596, 0.40760596, 1.40760596,\n              4.14293163, 0.14293163, 2.14293163]),\n            ([-1, 0, 1, 1, -1, 0, 0, 1, -1],\n             0, [0, 1], Shape([3, 3]), Shape([1, 3], 2),\n             [2.40760596, 0.40760596, 1.40760596,\n              1.40760596, 2.40760596, 0.40760596]),\n        ]\n        for dev in TensorForwardTest.devices:\n            for tc in test_cases:\n                x = tF.raw_input(tc[3], tc[0], dev)\n                y = tF.softmax_cross_entropy(x, tc[2], tc[1])\n                self.assertEqual(tc[4], y.shape())\n                self.assertTrue(np.isclose(tc[5], y.to_list(), 0, 1e-6).all())\n\n    def test_TensorForwardTest_CheckInvalidSparseSoftmaxCrossEntropy(self):\n        for dev in TensorForwardTest.devices:\n            x = tF.constant([2, 2], .5, dev)\n            t = [2]\n            with self.assertRaises(RuntimeError):\n                tF.softmax_cross_entropy(x, t, 0)\n            with self.assertRaises(RuntimeError):\n                tF.softmax_cross_entropy(x, t, 1)\n            with self.assertRaises(RuntimeError):\n                tF.softmax_cross_entropy(x, t, 2)\n            x = tF.constant(Shape([2, 2], 2), .5, dev)\n            t = [0, 0, 0]\n            with self.assertRaises(RuntimeError):\n                tF.softmax_cross_entropy(x, t, 0)\n            with self.assertRaises(RuntimeError):\n                tF.softmax_cross_entropy(x, t, 1)\n            with self.assertRaises(RuntimeError):\n                tF.softmax_cross_entropy(x, t, 2)\n\n    def test_TensorForwardTest_CheckStopGradient(self):\n        x_data = [\n            0, .5, 1, 2, 3, 4,\n            0, -.5, -1, -2, -3, -4,\n        ]\n        y_data = x_data\n        for dev in TensorForwardTest.devices:\n            x = tF.raw_input(Shape([2, 3], 2), x_data, dev)\n            y = tF.stop_gradient(x)\n            self.assertEqual(Shape([2, 3], 2), y.shape())\n            self.assertTrue(np.isclose(y_data, y.to_list()).all())\n\n    def run_test_conv2d(self, x_shape, x_data, w_shape, w_data, y_shape, y_data, pad0, pad1, str0, str1, dil0, dil1):\n        for dev in TensorForwardTest.devices:\n            try:\n                x = tF.raw_input(x_shape, x_data, dev)\n                w = tF.raw_input(w_shape, w_data, dev)\n                y = tF.conv2d(x, w, pad0, pad1, str0, str1, dil0, dil1)\n                self.assertEqual(y_shape, y.shape())\n                self.assertTrue(np.isclose(y_data, y.to_list()).all)\n            except RuntimeError as e:\n                # TODO(vbkaisetsu):\n                # We have to implement a better method to detect\n                # NotImplementedError in Python\n                if ""Not implemented"" not in str(e):\n                    raise\n\n    def test_TensorForwardTest_CheckConv2D_1x1x1_1x1x1x1(self):\n        x_data = [123]\n        w_data = [42]\n        y_data = [123 * 42]\n        x_shape = Shape([])\n        w_shape = Shape([])\n        y_shape = Shape([])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x1x1_1x1x1x1(self):\n        x_data = list(range(1, 5 + 1))\n        w_data = [42]\n        y_data = [42, 84, 126, 168, 210]\n        x_shape = Shape([5])\n        w_shape = Shape([])\n        y_shape = Shape([5])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x1x1_2x1x1x1(self):\n        x_data = list(range(1, 5 + 1))\n        w_data = list(range(1, 2 + 1))\n        y_data = [4, 7, 10, 13]\n        x_shape = Shape([5])\n        w_shape = Shape([2])\n        y_shape = Shape([4])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x1x1_5x1x1x1(self):\n        x_data = list(range(1, 5 + 1))\n        w_data = list(range(1, 5 + 1))\n        y_data = [35]\n        x_shape = Shape([5])\n        w_shape = Shape([5])\n        y_shape = Shape([])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_1x5x1_1x1x1x1(self):\n        x_data = list(range(1, 5 + 1))\n        w_data = [42]\n        y_data = [42, 84, 126, 168, 210]\n        x_shape = Shape([1, 5])\n        w_shape = Shape([])\n        y_shape = Shape([1, 5])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_1x5x1_1x2x1x1(self):\n        x_data = list(range(1, 5 + 1))\n        w_data = list(range(1, 2 + 1))\n        y_data = [4, 7, 10, 13]\n        x_shape = Shape([1, 5])\n        w_shape = Shape([1, 2])\n        y_shape = Shape([1, 4])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_1x5x1_1x5x1x1(self):\n        x_data = list(range(1, 5 + 1))\n        w_data = list(range(1, 5 + 1))\n        y_data = [35]\n        x_shape = Shape([1, 5])\n        w_shape = Shape([1, 5])\n        y_shape = Shape([])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_1x1x1x1(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        w_data = [42]\n        y_data = [\n             42,  84, 126,  168,  210,\n            252, 294, 336,  378,  420,\n            462, 504, 546,  588,  630,\n            672, 714, 756,  798,  840,\n            882, 924, 966, 1008, 1050,\n        ]\n        x_shape = Shape([5, 5])\n        w_shape = Shape([])\n        y_shape = Shape([5, 5])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_2x1x1x1(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        w_data = list(range(1, 2 + 1))\n        y_data = [\n             4,  7, 10, 13,\n            19, 22, 25, 28,\n            34, 37, 40, 43,\n            49, 52, 55, 58,\n            64, 67, 70, 73,\n        ]\n        x_shape = Shape([5, 5])\n        w_shape = Shape([2])\n        y_shape = Shape([4, 5])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_5x1x1x1(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        w_data = list(range(1, 5 + 1))\n        y_data = [\n             35,\n            110,\n            185,\n            260,\n            335,\n        ]\n        x_shape = Shape([5, 5])\n        w_shape = Shape([5])\n        y_shape = Shape([1, 5])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_1x2x1x1(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        w_data = list(range(1, 2 + 1))\n        y_data = [\n             8, 11, 14, 17, 20,\n            23, 26, 29, 32, 35,\n            38, 41, 44, 47, 50,\n            53, 56, 59, 62, 65,\n        ]\n        x_shape = Shape([5, 5])\n        w_shape = Shape([1, 2])\n        y_shape = Shape([5, 4])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_2x2x1x1(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        w_data = list(range(1, 2 * 2 + 1))\n        y_data = [\n             29,  39,  49,  59,\n             79,  89,  99, 109,\n            129, 139, 149, 159,\n            179, 189, 199, 209,\n        ]\n        x_shape = Shape([5, 5])\n        w_shape = Shape([2, 2])\n        y_shape = Shape([4, 4])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_5x2x1x1(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        w_data = list(range(1, 5 * 2 + 1))\n        y_data = [\n             220,\n             495,\n             770,\n            1045,\n        ]\n        x_shape = Shape([5, 5])\n        w_shape = Shape([5, 2])\n        y_shape = Shape([1, 4])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_1x5x1x1(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        w_data = list(range(1, 1 * 5 + 1))\n        y_data = [\n            115, 130, 145, 160, 175,\n        ]\n        x_shape = Shape([5, 5])\n        w_shape = Shape([1, 5])\n        y_shape = Shape([5])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_2x5x1x1(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        w_data = list(range(1, 2 * 5 + 1))\n        y_data = [\n            430, 485, 540, 595,\n        ]\n        x_shape = Shape([5, 5])\n        w_shape = Shape([2, 5])\n        y_shape = Shape([4])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_5x5x1x1(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        w_data = list(range(1, 5 * 5 + 1))\n        y_data = [2925]\n        x_shape = Shape([5, 5])\n        w_shape = Shape([5, 5])\n        y_shape = Shape([])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x3_2x2x3x1(self):\n        x_data = list(range(1, 5 * 5 * 3 + 1))\n        w_data = list(range(1, 2 * 2 * 3 + 1))\n        y_data = [\n            3029, 3107, 3185, 3263,\n            3419, 3497, 3575, 3653,\n            3809, 3887, 3965, 4043,\n            4199, 4277, 4355, 4433,\n        ]\n        x_shape = Shape([5, 5, 3])\n        w_shape = Shape([2, 2, 3])\n        y_shape = Shape([4, 4])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_2x2x1x3(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        w_data = list(range(1, 2 * 2 * 3 + 1))\n        y_data = [\n            # channel 1\n             29,  39,  49,  59,\n             79,  89,  99, 109,\n            129, 139, 149, 159,\n            179, 189, 199, 209,\n            # channel 2\n             93, 119, 145, 171,\n            223, 249, 275, 301,\n            353, 379, 405, 431,\n            483, 509, 535, 561,\n            # channel 3\n            157, 199, 241, 283,\n            367, 409, 451, 493,\n            577, 619, 661, 703,\n            787, 829, 871, 913,\n        ]\n        x_shape = Shape([5, 5])\n        w_shape = Shape([2, 2, 1, 3])\n        y_shape = Shape([4, 4, 3])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x3_2x2x3x3(self):\n        x_data = list(range(1, 5 * 5 * 3 + 1))\n        w_data = list(range(1, 2 * 2 * 3 * 3 + 1))\n        y_data = [\n            # channel 1\n            3029, 3107, 3185, 3263,\n            3419, 3497, 3575, 3653,\n            3809, 3887, 3965, 4043,\n            4199, 4277, 4355, 4433,\n            # channel 2\n             7205,  7427,  7649,  7871,\n             8315,  8537,  8759,  8981,\n             9425,  9647,  9869, 10091,\n            10535, 10757, 10979, 11201,\n            # channel 3\n            11381, 11747, 12113, 12479,\n            13211, 13577, 13943, 14309,\n            15041, 15407, 15773, 16139,\n            16871, 17237, 17603, 17969,\n        ]\n        x_shape = Shape([5, 5, 3])\n        w_shape = Shape([2, 2, 3, 3])\n        y_shape = Shape([4, 4, 3])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_2x2x1x1_Padding10(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        w_data = list(range(1, 2 * 2 + 1))\n        y_data = [\n             9,  29,  39,  49,  59,  40,\n            29,  79,  89,  99, 109,  70,\n            49, 129, 139, 149, 159, 100,\n            69, 179, 189, 199, 209, 130,\n        ]\n        x_shape = Shape([5, 5])\n        w_shape = Shape([2, 2])\n        y_shape = Shape([6, 4])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 1, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_2x2x1x1_Padding01(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        w_data = list(range(1, 2 * 2 + 1))\n        y_data = [\n              4,   7,  10,  13,\n             29,  39,  49,  59,\n             79,  89,  99, 109,\n            129, 139, 149, 159,\n            179, 189, 199, 209,\n            150, 157, 164, 171,\n        ]\n        x_shape = Shape([5, 5])\n        w_shape = Shape([2, 2])\n        y_shape = Shape([4, 6])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 1, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_2x2x1x1_Padding11(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        w_data = list(range(1, 2 * 2 + 1))\n        y_data = [\n             1,   4,   7,  10,  13,  10,\n             9,  29,  39,  49,  59,  40,\n            29,  79,  89,  99, 109,  70,\n            49, 129, 139, 149, 159, 100,\n            69, 179, 189, 199, 209, 130,\n            63, 150, 157, 164, 171, 100,\n        ]\n        x_shape = Shape([5, 5])\n        w_shape = Shape([2, 2])\n        y_shape = Shape([6, 6])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 1, 1, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_2x2x1x1_Stride21(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        w_data = list(range(1, 2 * 2 + 1))\n        y_data = [\n             29,  49,\n             79,  99,\n            129, 149,\n            179, 199,\n        ]\n        x_shape = Shape([5, 5])\n        w_shape = Shape([2, 2])\n        y_shape = Shape([2, 4])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 2, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_2x2x1x1_Stride12(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        w_data = list(range(1, 2 * 2 + 1))\n        y_data = [\n             29,  39,  49,  59,\n            129, 139, 149, 159,\n        ]\n        x_shape = Shape([5, 5])\n        w_shape = Shape([2, 2])\n        y_shape = Shape([4, 2])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 2, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_2x2x1x1_Stride22(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        w_data = list(range(1, 2 * 2 + 1))\n        y_data = [\n             29,  49,\n            129, 149,\n        ]\n        x_shape = Shape([5, 5])\n        w_shape = Shape([2, 2])\n        y_shape = Shape([2, 2])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 2, 2, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_2x2x1x1_Dilation21(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        w_data = list(range(1, 2 * 2 + 1))\n        y_data = [\n             33,  43,  53,\n             83,  93, 103,\n            133, 143, 153,\n            183, 193, 203,\n        ]\n        x_shape = Shape([5, 5])\n        w_shape = Shape([2, 2])\n        y_shape = Shape([3, 4])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 2, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_2x2x1x1_Dilation12(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        w_data = list(range(1, 2 * 2 + 1))\n        y_data = [\n             44,  54,  64,  74,\n             94, 104, 114, 124,\n            144, 154, 164, 174,\n        ]\n        x_shape = Shape([5, 5])\n        w_shape = Shape([2, 2])\n        y_shape = Shape([4, 3])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 2)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_2x2x1x1_Dilation22(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        w_data = list(range(1, 2 * 2 + 1))\n        y_data = [\n             48,  58,  68,\n             98, 108, 118,\n            148, 158, 168,\n        ]\n        x_shape = Shape([5, 5])\n        w_shape = Shape([2, 2])\n        y_shape = Shape([3, 3])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 2, 2)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_2x2x1x1_N1(self):\n        x_data = list(range(1, 5 * 5 * 3 + 1))\n        w_data = list(range(1, 2 * 2 + 1))\n        y_data = [\n            # minibatch 1\n             29,  39,  49,  59,\n             79,  89,  99, 109,\n            129, 139, 149, 159,\n            179, 189, 199, 209,\n            # minibatch 2\n            279, 289, 299, 309,\n            329, 339, 349, 359,\n            379, 389, 399, 409,\n            429, 439, 449, 459,\n            # minibatch 3\n            529, 539, 549, 559,\n            579, 589, 599, 609,\n            629, 639, 649, 659,\n            679, 689, 699, 709,\n        ]\n        x_shape = Shape([5, 5], 3)\n        w_shape = Shape([2, 2])\n        y_shape = Shape([4, 4], 3)\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_2x2x1x1_1N(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        w_data = list(range(1, 2 * 2 * 3 + 1))\n        y_data = [\n            # minibatch 1\n             29,  39,  49,  59,\n             79,  89,  99, 109,\n            129, 139, 149, 159,\n            179, 189, 199, 209,\n            # minibatch 2\n             93, 119, 145, 171,\n            223, 249, 275, 301,\n            353, 379, 405, 431,\n            483, 509, 535, 561,\n            # minibatch 3\n            157, 199, 241, 283,\n            367, 409, 451, 493,\n            577, 619, 661, 703,\n            787, 829, 871, 913,\n        ]\n        x_shape = Shape([5, 5])\n        w_shape = Shape([2, 2], 3)\n        y_shape = Shape([4, 4], 3)\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_5x5x1_2x2x1x1_NN(self):\n        x_data = list(range(1, 5 * 5 * 3 + 1))\n        w_data = list(range(1, 2 * 2 * 3 + 1))\n        y_data = [\n            # minibatch 1\n             29,  39,  49,  59,\n             79,  89,  99, 109,\n            129, 139, 149, 159,\n            179, 189, 199, 209,\n            # minibatch 2\n             743,  769,  795,  821,\n             873,  899,  925,  951,\n            1003, 1029, 1055, 1081,\n            1133, 1159, 1185, 1211,\n            # minibatch 3\n            2257, 2299, 2341, 2383,\n            2467, 2509, 2551, 2593,\n            2677, 2719, 2761, 2803,\n            2887, 2929, 2971, 3013,\n        ]\n        x_shape = Shape([5, 5], 3)\n        w_shape = Shape([2, 2], 3)\n        y_shape = Shape([4, 4], 3)\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 0, 0, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckConv2D_VGG16FirstLayer(self):\n        x_data = [1] * (224 * 224 * 3)\n        w_data = [1] * (3 * 3 * 3 * 64)\n        y_data = [27] * (224 * 224 * 64)\n        for b in range(64):\n            y_data[0 + b * 224 * 224] += 3;\n            y_data[223 + b * 224 * 224] += 3;\n            y_data[223 * 224 + b * 224 * 224] += 3;\n            y_data[223 * 224 + 223 + b * 224 * 224] += 3;\n            for i in range(224):\n                y_data[i + b * 224 * 224] -= 3 * 3\n                y_data[223 * 224 + i + b * 224 * 224] -= 3 * 3\n                y_data[i * 224 + b * 224 * 224] -= 3 * 3\n                y_data[i * 224 + 223 + b * 224 * 224] -= 3 * 3\n\n        x_shape = Shape([224, 224, 3])\n        w_shape = Shape([3, 3, 3, 64])\n        y_shape = Shape([224, 224, 64])\n        self.run_test_conv2d(x_shape, x_data, w_shape, w_data, y_shape, y_data, 1, 1, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckInvalidConv2D(self):\n        test_cases = [\n            # invalid #dimensions\n            (Shape([1, 1, 1, 2]), Shape([]), 0, 0, 1, 1, 1, 1, False),\n            (Shape([]), Shape([1, 1, 1, 1, 2]), 0, 0, 1, 1, 1, 1, False),\n            # zero-stride/dilation\n            (Shape([]), Shape([]), 0, 0, 1, 1, 1, 1, True),\n            (Shape([]), Shape([]), 0, 0, 0, 1, 1, 1, False),\n            (Shape([]), Shape([]), 0, 0, 1, 0, 1, 1, False),\n            (Shape([]), Shape([]), 0, 0, 1, 1, 0, 1, False),\n            (Shape([]), Shape([]), 0, 0, 1, 1, 1, 0, False),\n            # minibatches mismatching\n            (Shape([], 2), Shape([], 2), 0, 0, 1, 1, 1, 1, True),\n            (Shape([], 3), Shape([], 3), 0, 0, 1, 1, 1, 1, True),\n            (Shape([], 2), Shape([], 3), 0, 0, 1, 1, 1, 1, False),\n            # channels mismatching\n            (Shape([3, 3, 42]), Shape([3, 3, 42]), 0, 0, 1, 1, 1, 1, True),\n            (Shape([3, 3, 42]), Shape([3, 3, 43]), 0, 0, 1, 1, 1, 1, False),\n            # sizes mismatching\n            (Shape([3, 3]), Shape([3, 3]), 0, 0, 1, 1, 1, 1, True),\n            (Shape([3, 3]), Shape([4, 3]), 0, 0, 1, 1, 1, 1, False),\n            (Shape([3, 3]), Shape([3, 4]), 0, 0, 1, 1, 1, 1, False),\n            (Shape([3, 3]), Shape([4, 4]), 0, 0, 1, 1, 1, 1, False),\n            # sizes mismatching with padding\n            (Shape([3, 3]), Shape([5, 5]), 1, 1, 1, 1, 1, 1, True),\n            (Shape([3, 3]), Shape([6, 5]), 1, 1, 1, 1, 1, 1, False),\n            (Shape([3, 3]), Shape([5, 6]), 1, 1, 1, 1, 1, 1, False),\n            (Shape([3, 3]), Shape([6, 6]), 1, 1, 1, 1, 1, 1, False),\n            # sizes mismatching with stride\n            (Shape([3, 3]), Shape([3, 3]), 0, 0, 2, 2, 1, 1, True),\n            (Shape([3, 3]), Shape([4, 3]), 0, 0, 2, 2, 1, 1, False),\n            (Shape([3, 3]), Shape([3, 4]), 0, 0, 2, 2, 1, 1, False),\n            (Shape([3, 3]), Shape([4, 4]), 0, 0, 2, 2, 1, 1, False),\n            # sizes mismatching with dilation\n            (Shape([3, 3]), Shape([2, 2]), 0, 0, 1, 1, 2, 2, True),\n            (Shape([2, 3]), Shape([2, 2]), 0, 0, 1, 1, 2, 2, False),\n            (Shape([3, 2]), Shape([2, 2]), 0, 0, 1, 1, 2, 2, False),\n            (Shape([2, 2]), Shape([2, 2]), 0, 0, 1, 1, 2, 2, False),\n            (Shape([3, 3]), Shape([2, 2]), 0, 0, 1, 1, 3, 2, False),\n            (Shape([3, 3]), Shape([2, 2]), 0, 0, 1, 1, 2, 3, False),\n            (Shape([3, 3]), Shape([2, 2]), 0, 0, 1, 1, 3, 3, False),\n        ]\n\n        for dev in TensorForwardTest.devices:\n            for tc in test_cases:\n                x = tF.constant(tc[0], 0, dev)\n                w = tF.constant(tc[1], 0, dev)\n                if tc[8]:\n                    try:\n                        tF.conv2d(x, w, tc[2], tc[3], tc[4], tc[5], tc[6], tc[7])\n                    except RuntimeError as e:\n                        # TODO(vbkaisetsu):\n                        # We have to implement a better method to detect\n                        # NotImplementedError in Python\n                        if ""Not implemented"" not in str(e):\n                            raise\n                else:\n                    with self.assertRaises(RuntimeError) as e:\n                        tF.conv2d(x, w, tc[2], tc[3], tc[4], tc[5], tc[6], tc[7])\n\n    def run_test_max_pool2d(self, x_shape, x_data, y_shape, y_data, win0, win1, pad0, pad1, str0, str1):\n        for dev in TensorForwardTest.devices:\n            try:\n                print(dev)\n                x = tF.raw_input(x_shape, x_data, dev)\n                y = tF.max_pool2d(x, win0, win1, pad0, pad1, str0, str1)\n                self.assertEqual(y_shape, y.shape())\n                self.assertTrue(np.isclose(y_data, y.to_list()).all)\n            except RuntimeError as e:\n                # TODO(vbkaisetsu):\n                # We have to implement a better method to detect\n                # NotImplementedError in Python\n                if ""Not implemented"" not in str(e):\n                    raise\n\n    def test_TensorForwardTest_CheckMaxPool2D_1x1x1_1x1(self):\n        x_data = [123]\n        y_data = [123]\n        x_shape = Shape([])\n        y_shape = Shape([])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 1, 1, 0, 0, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_5x1x1_1x1(self):\n        x_data = list(range(1, 5 + 1))\n        y_data = [1, 2, 3, 4, 5]\n        x_shape = Shape([5])\n        y_shape = Shape([5])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 1, 1, 0, 0, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_5x1x1_2x1(self):\n        x_data = list(range(1, 5 + 1))\n        y_data = [2, 3, 4, 5]\n        x_shape = Shape([5])\n        y_shape = Shape([4])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 2, 1, 0, 0, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_5x1x1_5x1(self):\n        x_data = list(range(1, 5 + 1))\n        y_data = [5]\n        x_shape = Shape([5])\n        y_shape = Shape([])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 5, 1, 0, 0, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_1x5x1_1x1(self):\n        x_data = list(range(1, 5 + 1))\n        y_data = [1, 2, 3, 4, 5]\n        x_shape = Shape([1, 5])\n        y_shape = Shape([1, 5])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 1, 1, 0, 0, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_1x5x1_1x2(self):\n        x_data = list(range(1, 5 + 1))\n        y_data = [2, 3, 4, 5]\n        x_shape = Shape([1, 5])\n        y_shape = Shape([1, 4])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 1, 2, 0, 0, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_1x5x1_1x5(self):\n        x_data = list(range(1, 5 + 1))\n        y_data = [5]\n        x_shape = Shape([1, 5])\n        y_shape = Shape([])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 1, 5, 0, 0, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_5x5x1_1x1(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        y_data = [\n             1,  2,  3,  4,  5,\n             6,  7,  8,  9, 10,\n            11, 12, 13, 14, 15,\n            16, 17, 18, 19, 20,\n            21, 22, 23, 24, 25,\n        ]\n        x_shape = Shape([5, 5])\n        y_shape = Shape([5, 5])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 1, 1, 0, 0, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_5x5x1_2x1(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        y_data = [\n             2,  3,  4,  5,\n             7,  8,  9, 10,\n            12, 13, 14, 15,\n            17, 18, 19, 20,\n            22, 23, 24, 25,\n        ]\n        x_shape = Shape([5, 5])\n        y_shape = Shape([4, 5])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 2, 1, 0, 0, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_5x5x1_5x1(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        y_data = [\n             5,\n            10,\n            15,\n            20,\n            25,\n        ]\n        x_shape = Shape([5, 5])\n        y_shape = Shape([1, 5])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 5, 1, 0, 0, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_5x5x1_1x2(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        y_data = [\n             6,  7,  8,  9, 10,\n            11, 12, 13, 14, 15,\n            16, 17, 18, 19, 20,\n            21, 22, 23, 24, 25,\n        ]\n        x_shape = Shape([5, 5])\n        y_shape = Shape([5, 4])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 1, 2, 0, 0, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_5x5x1_2x2(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        y_data = [\n             7,  8,  9, 10,\n            12, 13, 14, 15,\n            17, 18, 19, 20,\n            22, 23, 24, 25,\n        ]\n        x_shape = Shape([5, 5])\n        y_shape = Shape([4, 4])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 2, 2, 0, 0, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_5x5x1_5x2(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        y_data = [\n            10,\n            15,\n            20,\n            25,\n        ]\n        x_shape = Shape([5, 5])\n        y_shape = Shape([1, 4])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 5, 2, 0, 0, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_5x5x1_1x5(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        y_data = [\n            21, 22, 23, 24, 25,\n        ]\n        x_shape = Shape([5, 5])\n        y_shape = Shape([5])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 1, 5, 0, 0, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_5x5x1_2x5(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        y_data = [\n            22, 23, 24, 25,\n        ]\n        x_shape = Shape([5, 5])\n        y_shape = Shape([4])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 2, 5, 0, 0, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_5x5x1_5x5(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        y_data = [25]\n        x_shape = Shape([5, 5])\n        y_shape = Shape([])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 5, 5, 0, 0, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_5x5x3_2x2(self):\n        x_data = list(range(1, 5 * 5 * 3 + 1))\n        y_data = [\n            # channel 1\n             7,  8,  9, 10,\n            12, 13, 14, 15,\n            17, 18, 19, 20,\n            22, 23, 24, 25,\n            # channel 2\n            32, 33, 34, 35,\n            37, 38, 39, 40,\n            42, 43, 44, 45,\n            47, 48, 49, 50,\n            # channel 3\n            57, 58, 59, 60,\n            62, 63, 64, 65,\n            67, 68, 69, 70,\n            72, 73, 74, 75,\n        ]\n        x_shape = Shape([5, 5, 3])\n        y_shape = Shape([4, 4, 3])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 2, 2, 0, 0, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_5x5x1_2x2_Padding10(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        y_data = [\n             6,  7,  8,  9, 10, 10,\n            11, 12, 13, 14, 15, 15,\n            16, 17, 18, 19, 20, 20,\n            21, 22, 23, 24, 25, 25,\n        ]\n        x_shape = Shape([5, 5])\n        y_shape = Shape([6, 4])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 2, 2, 1, 0, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_5x5x1_2x2_Padding01(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        y_data = [\n             2,  3,  4,  5,\n             7,  8,  9, 10,\n            12, 13, 14, 15,\n            17, 18, 19, 20,\n            22, 23, 24, 25,\n            22, 23, 24, 25,\n        ]\n        x_shape = Shape([5, 5])\n        y_shape = Shape([4, 6])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 2, 2, 0, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_5x5x1_2x2_Padding11(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        y_data = [\n             1,  2,  3,  4,  5,  5,\n             6,  7,  8,  9, 10, 10,\n            11, 12, 13, 14, 15, 15,\n            16, 17, 18, 19, 20, 20,\n            21, 22, 23, 24, 25, 25,\n            21, 22, 23, 24, 25, 25,\n        ]\n        x_shape = Shape([5, 5])\n        y_shape = Shape([6, 6])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 2, 2, 1, 1, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_5x5x1_2x2_Stride21(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        y_data = [\n             7,  9,\n            12, 14,\n            17, 19,\n            22, 24,\n        ]\n        x_shape = Shape([5, 5])\n        y_shape = Shape([2, 4])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 2, 2, 0, 0, 2, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_5x5x1_2x2_Stride12(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        y_data = [\n             7,  8,  9, 10,\n            17, 18, 19, 20,\n        ]\n        x_shape = Shape([5, 5])\n        y_shape = Shape([4, 2])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 2, 2, 0, 0, 1, 2)\n\n    def test_TensorForwardTest_CheckMaxPool2D_5x5x1_2x2_Stride22(self):\n        x_data = list(range(1, 5 * 5 + 1))\n        y_data = [\n             7,  9,\n            17, 19,\n        ]\n        x_shape = Shape([5, 5])\n        y_shape = Shape([2, 2])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 2, 2, 0, 0, 2, 2)\n\n    def test_TensorForwardTest_CheckMaxPool2D_5x5x1_2x2_N(self):\n        x_data = list(range(1, 5 * 5 * 3 + 1))\n        y_data = [\n            # minibatch 1\n             7,  8,  9, 10,\n            12, 13, 14, 15,\n            17, 18, 19, 20,\n            22, 23, 24, 25,\n            # minibatch 2\n            32, 33, 34, 35,\n            37, 38, 39, 40,\n            42, 43, 44, 45,\n            47, 48, 49, 50,\n            # minibatch 3\n            57, 58, 59, 60,\n            62, 63, 64, 65,\n            67, 68, 69, 70,\n            72, 73, 74, 75,\n        ]\n        x_shape = Shape([5, 5], 3)\n        y_shape = Shape([4, 4], 3)\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 2, 2, 0, 0, 1, 1)\n\n    def test_TensorForwardTest_CheckMaxPool2D_VGG16ThirdLayer(self):\n        # NOTE(odashi): 224*224*64 < 2^23 (float precision)\n        x_data = list(range(1, 224 * 224 * 64 + 1))\n        y_data = [0] * (112 * 112 * 64)\n        for b in range(64):\n            b_ofs = b * 224 * 224\n            for x in range(112):\n                x_ofs = b_ofs + (2 * x + 1) * 224\n                for y in range(112):\n                    y_data[y + b * 112 * 112 + x * 112] = x_ofs + 2 * y + 2\n        x_shape = Shape([224, 224, 64])\n        y_shape = Shape([112, 112, 64])\n        self.run_test_max_pool2d(x_shape, x_data, y_shape, y_data, 2, 2, 0, 0, 2, 2)\n\n    def test_TensorForwardTest_CheckInvalidPool2D(self):\n        test_cases = [\n            # invalid #dimensions\n            (Shape([1, 1, 1, 2]), 1, 1, 0, 0, 1, 1, False),\n            # zero-window/stride\n            (Shape([]), 1, 1, 0, 0, 1, 1, True),\n            (Shape([]), 0, 1, 0, 0, 1, 1, False),\n            (Shape([]), 1, 0, 0, 0, 1, 1, False),\n            (Shape([]), 1, 1, 0, 0, 0, 1, False),\n            (Shape([]), 1, 1, 0, 0, 1, 0, False),\n            # sizes mismatching\n            (Shape([3, 3]), 3, 3, 0, 0, 1, 1, True),\n            (Shape([3, 3]), 4, 3, 0, 0, 1, 1, False),\n            (Shape([3, 3]), 3, 4, 0, 0, 1, 1, False),\n            (Shape([3, 3]), 4, 4, 0, 0, 1, 1, False),\n            # sizes mismatching with padding\n            (Shape([3, 3]), 5, 5, 1, 1, 1, 1, True),\n            (Shape([3, 3]), 6, 5, 1, 1, 1, 1, False),\n            (Shape([3, 3]), 5, 6, 1, 1, 1, 1, False),\n            (Shape([3, 3]), 6, 6, 1, 1, 1, 1, False),\n            # sizes mismatching with stride\n            (Shape([3, 3]), 3, 3, 0, 0, 2, 2, True),\n            (Shape([3, 3]), 4, 3, 0, 0, 2, 2, False),\n            (Shape([3, 3]), 3, 4, 0, 0, 2, 2, False),\n            (Shape([3, 3]), 4, 4, 0, 0, 2, 2, False),\n        ]\n        for dev in TensorForwardTest.devices:\n            for tc in test_cases:\n                x = tF.constant(tc[0], 0, dev)\n                if tc[7]:\n                    try:\n                        tF.max_pool2d(x, tc[1], tc[2], tc[3], tc[4], tc[5], tc[6])\n                    except RuntimeError as e:\n                        # TODO(vbkaisetsu):\n                        # We have to implement a better method to detect\n                        # NotImplementedError in Python\n                        if ""Not implemented"" not in str(e):\n                            raise\n                else:\n                    with self.assertRaises(RuntimeError) as e:\n                        tF.max_pool2d(x, tc[1], tc[2], tc[3], tc[4], tc[5], tc[6])\n'"
tests/tensor_functions.py,28,"b'from primitiv import Device\nfrom primitiv import tensor_functions as tF\nfrom primitiv.devices import Naive\n\nimport numpy as np\nimport unittest\n\n\nclass TensorFunctionsTest(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        pass\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def setUp(self):\n        self.device = Naive()\n        Device.set_default(self.device)\n        self.a = np.array([[1, 2], [3, 4]], np.float32)\n        self.b = np.array([[1, 1], [4, 8]], np.float32)\n\n    def tearDown(self):\n        pass\n\n    def test_tensor_pos(self):\n        x = tF.input(self.a)\n        y = tF.input(self.b)\n        self.assertTrue(((+x).to_ndarrays()[0] == self.a).all())\n\n    def test_tensor_neg(self):\n        x = tF.input(self.a)\n        y = tF.input(self.b)\n        self.assertTrue(((-x).to_ndarrays()[0] == -self.a).all())\n\n    def test_tensor_add(self):\n        x = tF.input(self.a)\n        y = tF.input(self.b)\n        self.assertTrue(((x + y).to_ndarrays()[0] == np.array([[2, 3], [7, 12]])).all())\n        self.assertTrue(((x + 2).to_ndarrays()[0] == np.array([[3, 4], [5, 6]])).all())\n        self.assertTrue(((2 + x).to_ndarrays()[0] == np.array([[3, 4], [5, 6]])).all())\n\n    def test_tensor_sub(self):\n        x = tF.input(self.a)\n        y = tF.input(self.b)\n        self.assertTrue(((x - y).to_ndarrays()[0] == np.array([[0, 1], [-1, -4]])).all())\n        self.assertTrue(((x - 2).to_ndarrays()[0] == np.array([[-1, 0], [1, 2]])).all())\n        self.assertTrue(((2 - x).to_ndarrays()[0] == np.array([[1, 0], [-1, -2]])).all())\n\n    def test_tensor_mul(self):\n        x = tF.input(self.a)\n        y = tF.input(self.b)\n        self.assertTrue(((x * y).to_ndarrays()[0] == np.array([[1, 2], [12, 32]])).all())\n        self.assertTrue(((x * 2).to_ndarrays()[0] == np.array([[2, 4], [6, 8]])).all())\n        self.assertTrue(((2 * x).to_ndarrays()[0] == np.array([[2, 4], [6, 8]])).all())\n\n    def test_tensor_matmul(self):\n        x = tF.input(self.a)\n        y = tF.input(self.b)\n        self.assertTrue(((x @ y).to_ndarrays()[0] == np.array([[9, 17], [19, 35]])).all())\n        self.assertRaises(TypeError, lambda: x @ 2)\n        self.assertRaises(TypeError, lambda: 2 @ x)\n\n    def test_tensor_truediv(self):\n        x = tF.input(self.a)\n        y = tF.input(self.b)\n        self.assertTrue(((x / y).to_ndarrays()[0] == np.array([[1, 2], [0.75, 0.5]])).all())\n        self.assertTrue(((x / 2).to_ndarrays()[0] == np.array([[0.5, 1], [1.5, 2]])).all())\n        self.assertTrue(((2 / y).to_ndarrays()[0] == np.array([[2, 2], [0.5, 0.25]])).all())\n\n    def test_tensor_pow(self):\n        x = tF.input(self.a)\n        y = tF.input(self.b)\n        self.assertTrue(np.isclose((x ** y).to_ndarrays()[0], np.array([[1, 2], [81, 65536]])).all())\n        self.assertTrue(np.isclose((x ** 2).to_ndarrays()[0], np.array([[1, 4], [9, 16]])).all())\n        self.assertTrue(np.isclose((2 ** x).to_ndarrays()[0], np.array([[2, 4], [8, 16]])).all())\n        self.assertTrue(np.isclose((x ** -2).to_ndarrays()[0], np.array([[1, 1/4], [1/9, 1/16]])).all())\n        input_arr = np.array([1, -1, 3, -3, 5, -5])\n        x = tF.input(input_arr)\n        self.assertTrue(((x ** 6).to_ndarrays()[0] == np.array([1, 1, 729, 729, 15625, 15625])).all())\n        self.assertTrue(((x ** 9).to_ndarrays()[0] == np.array([1, -1, 19683, -19683, 1953125, -1953125])).all())\n        input_arr = np.array([1, -1])\n        x = tF.input(input_arr)\n        self.assertTrue(((x ** 0x7fffffff).to_ndarrays()[0] == np.array([1, -1])).all())\n        self.assertTrue(((x ** -0x80000000).to_ndarrays()[0] == np.array([1, 1])).all())\n\n        self.assertRaises(TypeError, lambda: pow(x, y, 2))\n\n    def test_tensor_iadd(self):\n        x = tF.input(self.a)\n        y = tF.input(self.b)\n        x_tmp = x\n        x += y\n        self.assertIs(x, x_tmp)\n        self.assertTrue((x.to_ndarrays()[0] == np.array([[2, 3], [7, 12]])).all())\n\n    def test_tensor_isub(self):\n        x = tF.input(self.a)\n        y = tF.input(self.b)\n        x_tmp = x\n        x -= y\n        self.assertIs(x, x_tmp)\n        self.assertTrue((x.to_ndarrays()[0] == np.array([[0, 1], [-1, -4]])).all())\n\n    def test_tensor_imul(self):\n        x = tF.input(self.a)\n        x_tmp = x\n        x *= 2\n        self.assertIs(x, x_tmp)\n        self.assertTrue((x.to_ndarrays()[0] == np.array([[2, 4], [6, 8]])).all())\n'"
tests/tensor_test.py,8,"b'import random\nimport unittest\n\nfrom primitiv import Shape\nfrom primitiv import Tensor\nfrom primitiv import tensor_functions as tF\n\nimport numpy as np\nfrom . import test_utils\n\n\nclass TensorTest(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.devices = test_utils.available_devices()\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def test_TensorTest_CheckInvalid(self):\n        x = Tensor()\n        self.assertFalse(x.valid())\n        with self.assertRaises(RuntimeError):\n            x.shape()\n        with self.assertRaises(RuntimeError):\n            x.device()\n        with self.assertRaises(RuntimeError):\n            x.to_float()\n        with self.assertRaises(RuntimeError):\n            x.to_list()\n        with self.assertRaises(RuntimeError):\n            x.to_ndarrays()\n\n    def test_TensorTest_CheckNewScalarWithData(self):\n        for dev in TensorTest.devices:\n            x = tF.raw_input([], [1], dev)\n            x_ndarray = [\n                np.array([1]),\n            ]\n            self.assertTrue(x.valid())\n            self.assertIs(dev, x.device())\n            self.assertEqual(Shape(), x.shape())\n            self.assertEqual([1], x.to_list())\n            self.assertEqual(1.0, x.to_float())\n            self.assertEqual(x_ndarray, x.to_ndarrays())\n\n    def test_TensorTest_CheckNewMatrixWithData(self):\n        for dev in TensorTest.devices:\n            data = [1, 2, 3, 4, 5, 6]\n            data_ndarray = [\n                np.array([[1, 3, 5], [2, 4, 6]]),\n            ]\n            x = tF.raw_input([2, 3], data, dev)\n            self.assertTrue(x.valid())\n            self.assertIs(dev, x.device())\n            self.assertEqual(Shape([2, 3]), x.shape())\n            self.assertEqual(data, x.to_list())\n            with self.assertRaises(RuntimeError):\n                x.to_float()\n            self.assertTrue(np.array_equal(data_ndarray, x.to_ndarrays()))\n\n    def test_TensorTest_CheckNewMatrixMinibatchWithData(self):\n        for dev in TensorTest.devices:\n            data = [\n                3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8,\n                9, 7, 9, 3, 2, 3, 8, 4, 6, 2, 6, 4,\n            ]\n            data_ndarray = [\n                np.array([[3, 4, 5], [1, 1, 9]]),\n                np.array([[2, 5, 5], [6, 3, 8]]),\n                np.array([[9, 9, 2], [7, 3, 3]]),\n                np.array([[8, 6, 6], [4, 2, 4]]),\n            ]\n            x = tF.raw_input(Shape([2, 3], 4), data, dev)\n            self.assertTrue(x.valid())\n            self.assertIs(dev, x.device())\n            self.assertEqual(Shape([2, 3], 4), x.shape())\n            self.assertEqual(data, x.to_list())\n            with self.assertRaises(RuntimeError):\n                x.to_float()\n            self.assertTrue(np.array_equal(data_ndarray, x.to_ndarrays()))\n\n    def test_TensorTest_CheckCopyValidToNew(self):\n        for dev in TensorTest.devices:\n            print(dev)\n            tmp = tF.raw_input(Shape([2], 3), [1, 2, 3, 4, 5, 6], dev)\n            x = Tensor(tmp)\n            self.assertTrue(x.valid())\n            self.assertTrue(tmp.valid())\n            self.assertEqual(Shape([2], 3), x.shape())\n            self.assertEqual(Shape([2], 3), tmp.shape())\n            self.assertEqual([1, 2, 3, 4, 5, 6], x.to_list())\n            self.assertEqual([1, 2, 3, 4, 5, 6], tmp.to_list())\n\n    def test_TensorTest_CheckCopyInvalidToNew(self):\n        for dev in TensorTest.devices:\n            tmp = Tensor()\n            x = Tensor(tmp)\n            self.assertFalse(x.valid())\n            self.assertFalse(tmp.valid())\n\n    def test_TesnorTest_CheckResetValuesByConstant(self):\n        for dev in TensorTest.devices:\n            x = tF.raw_input(Shape([2, 2], 2), [42] * 8, dev)\n            self.assertEqual([42] * 8, x.to_list())\n\n            x = tF.raw_input(Shape([2, 2], 2), [0] * 8, dev)\n            x.reset(42)\n            self.assertEqual([42] * 8, x.to_list())\n\n            x = tF.raw_input(Shape([2, 2], 2), [123] * 8, dev)\n            copied = Tensor(x)\n\n            x.reset(42)\n            self.assertEqual([42] * 8, x.to_list())\n            self.assertEqual([123] * 8, copied.to_list())\n\n    def test_TensorTest_CheckResetValuesByVector(self):\n        for dev in TensorTest.devices:\n            data = [1, 2, 3, 4, 5, 6, 7, 8]\n            x = tF.raw_input(Shape([2, 2], 2), data, dev)\n            self.assertEqual(data, x.to_list())\n\n            data = [1, 2, 3, 4, 5, 6, 7, 8]\n            x = tF.raw_input(Shape([2, 2], 2), [0] * 8, dev)\n            x.reset_by_vector(data)\n            self.assertEqual(data, x.to_list())\n\n            data = [1, 2, 3, 4, 5, 6, 7, 8]\n            x = tF.raw_input(Shape([2, 2], 2), [123] * 8, dev)\n            copied = Tensor(x)\n\n            x.reset_by_vector(data)\n            self.assertEqual(data, x.to_list())\n            self.assertEqual([123] * 8, copied.to_list())\n\n    def test_TensorTest_InplaceMultiplyConst(self):\n        for dev in TensorTest.devices:\n            x_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n            y_data = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24]\n            x = tF.raw_input(Shape([2, 2], 3), x_data, dev)\n            x *= 2\n            self.assertEqual(y_data, x.to_list())\n        for dev in TensorTest.devices:\n            x_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n            y_data = [.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6]\n            x = tF.raw_input(Shape([2, 2], 3), x_data, dev)\n            x *= .5\n            self.assertEqual(y_data, x.to_list())\n\n    def test_TensorTest_CheckInplaceAddNN(self):\n        for dev in TensorTest.devices:\n            a_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n            b_data = [0, -1, -2, -3, -3, -4, -5, -6, -6, -7, -8, -9]\n            y_data = [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]\n            a = tF.raw_input(Shape([2, 2], 3), a_data, dev)\n            b = tF.raw_input(Shape([2, 2], 3), b_data, dev)\n            a += b\n            self.assertEqual(y_data, a.to_list())\n\n    def test_TensorTest_CheckInplaceAdd1N(self):\n        for dev in TensorTest.devices:\n            a_data = [1, 2, 3, 4]\n            b_data = [0, -1, -2, -3, -3, -4, -5, -6, -6, -7, -8, -9]\n            y_data = [-8, -10, -12, -14]\n            a = tF.raw_input([2, 2], a_data, dev)\n            b = tF.raw_input(Shape([2, 2], 3), b_data, dev)\n            a += b\n            self.assertEqual(y_data, a.to_list())\n\n    def test_TensorTest_CheckInplaceAddN1(self):\n        for dev in TensorTest.devices:\n            a_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n            b_data = [0, -1, -2, -3]\n            y_data = [1, 1, 1, 1, 5, 5, 5, 5, 9, 9, 9, 9]\n            a = tF.raw_input(Shape([2, 2], 3), a_data, dev)\n            b = tF.raw_input([2, 2], b_data, dev)\n            a += b\n            self.assertEqual(y_data, a.to_list())\n\n    def test_TensorTest_CheckInplaceAdd(self):\n        for dev in TensorTest.devices:\n            a_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n            b_data = [0, -1, -2, -3, -3, -4, -5, -6, -6, -7, -8, -9]\n            y_data = [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]\n            a = tF.raw_input(Shape([2, 2], 3), a_data, dev)\n            b = tF.raw_input(Shape([2, 2], 3), b_data, dev)\n\n            copied = Tensor(a)\n            ref_a = a\n\n            a += b\n            self.assertEqual(y_data, a.to_list())\n\n            self.assertEqual(y_data, a.to_list());\n            self.assertEqual(a_data, copied.to_list());\n            self.assertIs(ref_a, a)\n\n    def test_TensorTest_CheckInplaceSubtractNN(self):\n        for dev in TensorTest.devices:\n            a_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n            b_data = [0, 1, 2, 3, 3, 4, 5, 6, 6, 7, 8, 9]\n            y_data = [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]\n            a = tF.raw_input(Shape([2, 2], 3), a_data, dev)\n            b = tF.raw_input(Shape([2, 2], 3), b_data, dev)\n            a -= b\n            self.assertEqual(y_data, a.to_list())\n\n    def test_TensorTest_CheckInplaceSubtract1N(self):\n        for dev in TensorTest.devices:\n            a_data = [1, 2, 3, 4]\n            b_data = [0, 1, 2, 3, 3, 4, 5, 6, 6, 7, 8, 9]\n            y_data = [-8, -10, -12, -14]\n            a = tF.raw_input([2, 2], a_data, dev)\n            b = tF.raw_input(Shape([2, 2], 3), b_data, dev)\n            a -= b\n            self.assertEqual(y_data, a.to_list())\n\n    def test_TensorTest_CheckInplaceSubtractN1(self):\n        for dev in TensorTest.devices:\n            a_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n            b_data = [0, 1, 2, 3]\n            y_data = [1, 1, 1, 1, 5, 5, 5, 5, 9, 9, 9, 9]\n            a = tF.raw_input(Shape([2, 2], 3), a_data, dev)\n            b = tF.raw_input([2, 2], b_data, dev)\n            a -= b\n            self.assertEqual(y_data, a.to_list())\n\n    def test_TensorTest_CheckInplaceSubtract(self):\n        for dev in TensorTest.devices:\n            a_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n            b_data = [0, 1, 2, 3, 3, 4, 5, 6, 6, 7, 8, 9]\n            y_data = [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]\n            a = tF.raw_input(Shape([2, 2], 3), a_data, dev)\n            b = tF.raw_input(Shape([2, 2], 3), b_data, dev)\n\n            copied = Tensor(a)\n            ref_a = a\n\n            a -= b\n            self.assertEqual(y_data, a.to_list())\n\n            self.assertEqual(y_data, a.to_list());\n            self.assertEqual(a_data, copied.to_list());\n            self.assertIs(ref_a, a)\n\n    def test_TensorTest_CheckInvalidInplaceOps(self):\n        for dev in TensorTest.devices:\n            shapes = [\n                Shape(),\n                Shape([], 3),\n                Shape([2, 2], 2),\n            ]\n            a = tF.raw_input(Shape([2, 2], 3), [0] * 12, dev)\n\n            for shape in shapes:\n                b = tF.raw_input(shape, [0] * shape.size(), dev)\n                with self.assertRaises(RuntimeError):\n                    a += b\n                with self.assertRaises(RuntimeError):\n                    a -= b\n\n    def test_TensorTest_CheckArgMaxDims(self):\n        data = [\n            0, 1, 2, 6, 7, 8, 3, 4, 5, -3, -4, -5, 0, -1, -2, -6, -7, -8,\n        ]\n        expected = [\n            [2, 2, 2, 0, 0, 0],\n            [1, 1, 1, 1, 1, 1],\n            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        ]\n        for dev in TensorTest.devices:\n            a = tF.raw_input(Shape([3, 3], 2), data, dev)\n            for i, exp in enumerate(expected):\n                self.assertEqual(exp, a.argmax(i))\n\n    def test_TensorTest_CheckArgMaxLarge(self):\n        ns = [\n            1, 2, 3, 15, 16, 17, 255, 256, 257, 1023, 1024,\n            1025, 65535, 65536, 65537,\n        ]\n        for dev in TensorTest.devices:\n            for n in ns:\n                data = list(range(n))\n                random.shuffle(data)\n                pos = data.index(n - 1)\n                a = tF.raw_input([n], data, dev)\n                self.assertEqual(pos, a.argmax(0)[0])\n\n    def test_TensorTest_CheckArgMinDims(self):\n        data = [\n            3, 4, 5, 0, 1, 2, 6, 7, 8, 0, -1, -2, -6, -7, -8, -3, -4, -5,\n        ]\n        expected = [\n            [0, 0, 0, 2, 2, 2],\n            [1, 1, 1, 1, 1, 1],\n            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        ]\n        for dev in TensorTest.devices:\n            a = tF.raw_input(Shape([3, 3], 2), data, dev)\n            for i, exp in enumerate(expected):\n                self.assertEqual(exp, a.argmin(i))\n\n    def test_TensorTest_CheckArgMinLarge(self):\n        ns = [\n            1, 2, 3, 15, 16, 17, 255, 256, 257, 1023, 1024,\n            1025, 65535, 65536, 65537,\n        ]\n        for dev in TensorTest.devices:\n            for n in ns:\n                data = list(range(n))\n                random.shuffle(data)\n                pos = data.index(0)\n                a = tF.raw_input([n], data, dev)\n                self.assertEqual(pos, a.argmin(0)[0])\n'"
tests/test_utils.py,0,"b'import sys\n\n\ndef available_devices():\n    devices = []\n    devices.extend(available_naive_devices())\n    devices.extend(available_cuda_devices())\n    devices.extend(available_eigen_devices())\n    devices.extend(available_opencl_devices())\n    for dev in devices:\n        dev.dump_description()\n    return devices\n\n\ndef available_naive_devices():\n    from primitiv.devices import Naive\n    return [\n        Naive(),\n        Naive(),\n    ]\n\n\ndef available_cuda_devices():\n    try:\n        from primitiv.devices import CUDA\n    except ImportError:\n        return []\n    devices = []\n    num_devs = CUDA.num_devices()\n    num_avail_devs = 0\n    for dev_id in range(num_devs):\n        if CUDA.check_support(dev_id):\n            devices.append(CUDA(dev_id))\n            num_avail_devs += 1\n            if len(devices) == 1:\n                devices.append(CUDA(dev_id))\n    if num_devs - num_avail_devs:\n        print(""%d CUDA device(s) are not supported.""\n              % (num_devs - num_avail_devs), file=sys.stderr)\n    return devices\n\n\ndef available_eigen_devices():\n    try:\n        from primitiv.devices import Eigen\n    except ImportError:\n        return []\n    return [\n        Eigen(),\n        Eigen(),\n    ]\n\n\ndef available_opencl_devices():\n    try:\n        from primitiv.devices import OpenCL\n    except ImportError:\n        return []\n    devices = []\n    num_pfs = OpenCL.num_platforms();\n    num_avail_devs = 0\n    num_devs = 0\n    for pf_id in range(num_pfs):\n        num_devs += OpenCL.num_devices(pf_id)\n        for dev_id in range(num_devs):\n            if OpenCL.check_support(pf_id, dev_id):\n                devices.append(OpenCL(pf_id, dev_id))\n                num_avail_devs += 1\n                if len(devices) == 1:\n                    devices.append(OpenCL(pf_id, dev_id))\n    if num_devs != num_avail_devs:\n        print(""%d OpenCL device(s) are not supported.""\n              % (num_devs - num_avail_devs), file=sys.stderr)\n    return devices\n'"
examples/encdec/bleu.py,0,"b""# This code is cloned by: https://gist.github.com/odashi/fb4ffa936817551a7209\n\nimport math\nfrom collections import defaultdict\n\n\ndef get_bleu_stats(ref, hyp, N=4):\n    stats = defaultdict(int, {'rl': len(ref), 'hl': len(hyp)})\n    N = len(hyp) if len(hyp) < N else N\n    for n in range(N):\n        matched = 0\n        possible = defaultdict(int)\n        for k in range(len(ref) - n):\n            possible[tuple(ref[k : k + n + 1])] += 1\n\n        for k in range(len(hyp) - n):\n            ngram = tuple(hyp[k : k + n + 1])\n            if possible[ngram] > 0:\n                possible[ngram] -= 1\n                matched += 1\n\n        stats['d' + str(n + 1)] = len(hyp) - n\n        stats['n' + str(n + 1)] = matched\n\n    return stats\n\n\ndef calculate_bleu(stats, N=4):\n    np = 0.0\n    for n in range(N):\n        nn = stats['n' + str(n + 1)]\n        if nn == 0:\n            return 0.0\n\n        dd = stats['d' + str(n + 1)]\n        np += math.log(nn) - math.log(dd)\n\n    bp = 1.0 - stats['rl'] / stats['hl']\n    if bp > 0.0:\n        bp = 0.0\n\n    return math.exp(np / N + bp)\n"""
examples/encdec/encdec.py,4,"b'#!/usr/bin/env python3\n\nimport sys\nimport random\nimport math\n\nimport numpy as np\n\nfrom primitiv import Device, Graph, Model, Parameter, Optimizer\n\nfrom primitiv import devices as D\nfrom primitiv import functions as F\nfrom primitiv import initializers as I\nfrom primitiv import optimizers as O\n\nfrom lstm import LSTM\nfrom utils import (\n    make_vocab, load_corpus, load_corpus_ref, count_labels, make_batch,\n    save_ppl, make_inv_vocab, line_to_sent, load_ppl\n)\n\nfrom argparse import ArgumentParser\nfrom bleu import get_bleu_stats, calculate_bleu\nfrom collections import defaultdict\n\n\nSRC_VOCAB_SIZE = 4000\nTRG_VOCAB_SIZE = 5000\nNUM_EMBED_UNITS = 512\nNUM_HIDDEN_UNITS = 512\nBATCH_SIZE = 64\nMAX_EPOCH = 100\nDROPOUT_RATE = 0.5\nGENERATION_LIMIT = 32\n\nSRC_TRAIN_FILE = ""data/train.en""\nTRG_TRAIN_FILE = ""data/train.ja""\nSRC_VALID_FILE = ""data/dev.en""\nTRG_VALID_FILE = ""data/dev.ja""\nSRC_TEST_FILE = ""data/test.en""\nREF_TEST_FILE = ""data/test.ja""\n\n\nclass EncoderDecoder(Model):\n    """"""Standard encoder-decoder model.""""""\n\n    def __init__(self):\n        self.dropout_rate = DROPOUT_RATE\n        self.psrc_lookup = Parameter()\n        self.ptrg_lookup = Parameter()\n        self.pwhy = Parameter()\n        self.pby = Parameter()\n        self.src_lstm = LSTM()\n        self.trg_lstm = LSTM()\n        self.scan_attributes()\n\n    def init(self, src_vocab_size, trg_vocab_size, embed_size, hidden_size):\n        """"""Creates a new EncoderDecoder object.""""""\n        self.psrc_lookup.init([embed_size, src_vocab_size], I.XavierUniform())\n        self.ptrg_lookup.init([embed_size, trg_vocab_size], I.XavierUniform())\n        self.pwhy.init([trg_vocab_size, hidden_size], I.XavierUniform())\n        self.pby.init([trg_vocab_size], I.Constant(0))\n        self.src_lstm.init(embed_size, hidden_size)\n        self.trg_lstm.init(embed_size, hidden_size)\n\n    def encode(self, src_batch, train):\n        """"""Encodes source sentences and prepares internal states.""""""\n        # Reversed encoding.\n        src_lookup = F.parameter(self.psrc_lookup)\n        self.src_lstm.restart()\n        for it in src_batch:\n            x = F.pick(src_lookup, it, 1)\n            x = F.dropout(x, self.dropout_rate, train)\n            self.src_lstm.forward(x)\n\n        # Initializes decoder states.\n        self.trg_lookup = F.parameter(self.ptrg_lookup)\n        self.why = F.parameter(self.pwhy)\n        self.by = F.parameter(self.pby)\n        self.trg_lstm.restart(self.src_lstm.get_c(), self.src_lstm.get_h())\n\n    def decode_step(self, trg_words, train):\n        """"""One step decoding.""""""\n        x = F.pick(self.trg_lookup, trg_words, 1)\n        x = F.dropout(x, self.dropout_rate, train)\n        h = self.trg_lstm.forward(x)\n        h = F.dropout(h, self.dropout_rate, train)\n        return self.why @ h + self.by\n\n    def loss(self, trg_batch, train):\n        """"""Calculates loss values.""""""\n        losses = []\n        for i in range(len(trg_batch) - 1):\n            y = self.decode_step(trg_batch[i], train)\n            losses.append(F.softmax_cross_entropy(y, trg_batch[i + 1], 0))\n        return F.batch.mean(F.sum(losses))\n\n\ndef train(encdec, optimizer, prefix, best_valid_ppl):\n    # Registers all parameters to the optimizer.\n    optimizer.add(encdec)\n\n    # Loads vocab.\n    src_vocab = make_vocab(SRC_TRAIN_FILE, SRC_VOCAB_SIZE)\n    trg_vocab = make_vocab(TRG_TRAIN_FILE, TRG_VOCAB_SIZE)\n    inv_trg_vocab = make_inv_vocab(trg_vocab)\n    print(""#src_vocab:"", len(src_vocab))  # == SRC_VOCAB_SIZE\n    print(""#trg_vocab:"", len(trg_vocab))  # == TRG_VOCAB_SIZE\n\n    # Loads all corpus.\n    train_src_corpus = load_corpus(SRC_TRAIN_FILE, src_vocab)\n    train_trg_corpus = load_corpus(TRG_TRAIN_FILE, trg_vocab)\n    valid_src_corpus = load_corpus(SRC_VALID_FILE, src_vocab)\n    valid_trg_corpus = load_corpus(TRG_VALID_FILE, trg_vocab)\n    test_src_corpus = load_corpus(SRC_TEST_FILE, src_vocab)\n    test_ref_corpus = load_corpus_ref(REF_TEST_FILE, trg_vocab)\n    num_train_sents = len(train_trg_corpus)\n    num_valid_sents = len(valid_trg_corpus)\n    num_test_sents = len(test_ref_corpus)\n    num_train_labels = count_labels(train_trg_corpus)\n    num_valid_labels = count_labels(valid_trg_corpus)\n    print(""train:"", num_train_sents, ""sentences,"", num_train_labels, ""labels"")\n    print(""valid:"", num_valid_sents, ""sentences,"", num_valid_labels, ""labels"")\n\n    # Sentence IDs.\n    train_ids = list(range(num_train_sents))\n    valid_ids = list(range(num_valid_sents))\n\n    # Train/valid loop.\n    for epoch in range(MAX_EPOCH):\n        # Computation graph.\n        g = Graph()\n        Graph.set_default(g)\n\n        print(""epoch %d/%d:"" % (epoch + 1, MAX_EPOCH))\n        # Shuffles train sentence IDs.\n        random.shuffle(train_ids)\n\n        # Training.\n        train_loss = 0\n        for ofs in range(0, num_train_sents, BATCH_SIZE):\n            print(""%d"" % ofs, end=""\\r"")\n            sys.stdout.flush()\n\n            batch_ids = train_ids[ofs : min(ofs + BATCH_SIZE, num_train_sents)]\n            src_batch = make_batch(train_src_corpus, batch_ids, src_vocab)\n            trg_batch = make_batch(train_trg_corpus, batch_ids, trg_vocab)\n\n            g.clear()\n\n            encdec.encode(src_batch, True)\n            loss = encdec.loss(trg_batch, True)\n            train_loss += loss.to_float() * len(batch_ids)\n\n            optimizer.reset_gradients()\n            loss.backward()\n            optimizer.update()\n\n        train_ppl = math.exp(train_loss / num_train_labels)\n        print(""  train PPL = %.4f"" % train_ppl)\n\n        # Validation.\n        valid_loss = 0\n        for ofs in range(0, num_valid_sents, BATCH_SIZE):\n            print(""%d"" % ofs, end=""\\r"")\n            sys.stdout.flush()\n\n            batch_ids = valid_ids[ofs : min(ofs + BATCH_SIZE, num_valid_sents)]\n            src_batch = make_batch(valid_src_corpus, batch_ids, src_vocab)\n            trg_batch = make_batch(valid_trg_corpus, batch_ids, trg_vocab)\n\n            g.clear()\n\n            encdec.encode(src_batch, False)\n            loss = encdec.loss(trg_batch, False)\n            valid_loss += loss.to_float() * len(batch_ids)\n\n        valid_ppl = math.exp(valid_loss / num_valid_labels)\n        print(""  valid PPL = %.4f"" % valid_ppl)\n\n        # Calculates test BLEU.\n        stats = defaultdict(int)\n        for ofs in range(0, num_test_sents, BATCH_SIZE):\n            print(""%d"" % ofs, end=""\\r"")\n            sys.stdout.flush()\n\n            src_batch = test_src_corpus[ofs : min(ofs + BATCH_SIZE, num_test_sents)]\n            ref_batch = test_ref_corpus[ofs : min(ofs + BATCH_SIZE, num_test_sents)]\n\n            hyp_ids = test_batch(encdec, src_vocab, trg_vocab, src_batch)\n            for hyp_line, ref_line in zip(hyp_ids, ref_batch):\n                for k, v in get_bleu_stats(ref_line[1:-1], hyp_line).items():\n                    stats[k] += v\n\n        bleu = calculate_bleu(stats)\n        print(""  test BLEU = %.2f"" % (100 * bleu))\n\n        # Saves best model/optimizer.\n        if valid_ppl < best_valid_ppl:\n            best_valid_ppl = valid_ppl\n            print(""  saving model/optimizer ... "", end="""")\n            sys.stdout.flush()\n            encdec.save(prefix + "".model"")\n            optimizer.save(prefix + "".optimizer"")\n            save_ppl(prefix + "".valid_ppl"", best_valid_ppl)\n            print(""done."")\n\n\ndef test_batch(encdec, src_vocab, trg_vocab, lines):\n    g = Graph()\n    Graph.set_default(g)\n\n    src_batch = make_batch(lines, list(range(len(lines))), src_vocab)\n\n    encdec.encode(src_batch, False)\n\n    # Generates target words one-by-one.\n    trg_ids = [np.array([trg_vocab[""<bos>""]] * len(lines))]\n    eos_id = trg_vocab[""<eos>""]\n    eos_ids = np.array([eos_id] * len(lines))\n    while (trg_ids[-1] != eos_ids).any():\n        if len(trg_ids) > GENERATION_LIMIT + 1:\n            print(""Warning: Sentence generation did not finish in"", GENERATION_LIMIT, ""iterations."", file=sys.stderr)\n            trg_ids.append(eos_ids)\n            break\n        y = encdec.decode_step(trg_ids[-1], False)\n        trg_ids.append(np.array(y.argmax(0)).T)\n\n    return [hyp[:np.where(hyp == eos_id)[0][0]] for hyp in np.array(trg_ids[1:]).T]\n\n\ndef test(encdec):\n    # Loads vocab.\n    src_vocab = make_vocab(SRC_TRAIN_FILE, SRC_VOCAB_SIZE)\n    trg_vocab = make_vocab(TRG_TRAIN_FILE, TRG_VOCAB_SIZE)\n    inv_trg_vocab = make_inv_vocab(trg_vocab)\n\n    for line in sys.stdin:\n        trg_ids = test_batch(encdec, src_vocab, trg_vocab, [line_to_sent(line.strip(), src_vocab)])[0]\n        # Prints the result.\n        print("" "".join(inv_trg_vocab[wid] for wid in trg_ids))\n\n\ndef main():\n    parser = ArgumentParser()\n    parser.add_argument(""mode"", help=""(train|resume|test)"")\n    parser.add_argument(""model_prefix"", help=""prefix of the model files."")\n    args = parser.parse_args()\n\n    mode = args.mode\n    prefix = args.model_prefix\n    print(""mode:"", mode, file=sys.stderr)\n    print(""prefix:"", prefix, file=sys.stderr)\n\n    if mode not in (""train"", ""resume"", ""test""):\n        print(""unknown mode:"", mode, file=sys.stderr)\n        return\n\n    print(""initializing device ... "", end="""", file=sys.stderr)\n    sys.stderr.flush()\n    dev = D.CUDA(0)\n    Device.set_default(dev)\n    print(""done."", file=sys.stderr)\n\n    if mode == ""train"":\n        encdec = EncoderDecoder()\n        encdec.init(SRC_VOCAB_SIZE, TRG_VOCAB_SIZE, NUM_EMBED_UNITS, NUM_HIDDEN_UNITS)\n        optimizer = O.Adam()\n        optimizer.set_weight_decay(1e-6)\n        optimizer.set_gradient_clipping(5)\n        train(encdec, optimizer, prefix, 1e10)\n    elif mode == ""resume"":\n        print(""loading model/optimizer ... "", end="""", file=sys.stderr)\n        sys.stderr.flush()\n        encdec = EncoderDecoder()\n        encdec.load(prefix + "".model"")\n        optimizer = O.Adam()\n        optimizer.load(prefix + "".optimizer"")\n        valid_ppl = load_ppl(prefix + "".valid_ppl"")\n        print(""done."", file=sys.stderr)\n        train(encdec, optimizer, prefix, valid_ppl)\n    else:  # mode == ""test""\n        print(""loading model ... "", end="""", file=sys.stderr)\n        sys.stderr.flush()\n        encdec = EncoderDecoder()\n        encdec.load(prefix + "".model"")\n        print(""done."", file=sys.stderr)\n        test(encdec)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/encdec/encdec_attention.py,4,"b'#!/usr/bin/env python3\n# coding: utf-8\n\nimport sys\nimport random\nimport math\n\nimport numpy as np\n\nfrom primitiv import Device, Graph, Model, Parameter, Optimizer\n\nfrom primitiv import devices as D\nfrom primitiv import functions as F\nfrom primitiv import initializers as I\nfrom primitiv import optimizers as O\n\nfrom lstm import LSTM\nfrom utils import (\n    make_vocab, load_corpus, load_corpus_ref, count_labels, make_batch,\n    save_ppl, make_inv_vocab, line_to_sent, load_ppl\n)\n\nfrom argparse import ArgumentParser\nfrom bleu import get_bleu_stats, calculate_bleu\nfrom collections import defaultdict\n\nSRC_VOCAB_SIZE = 4000\nTRG_VOCAB_SIZE = 5000\nNUM_EMBED_UNITS = 512\nNUM_HIDDEN_UNITS = 512\nBATCH_SIZE = 64\nMAX_EPOCH = 100\nGENERATION_LIMIT = 32\nDROPOUT_RATE = 0.5\n\nSRC_TRAIN_FILE = ""data/train.en""\nTRG_TRAIN_FILE = ""data/train.ja""\nSRC_VALID_FILE = ""data/dev.en""\nTRG_VALID_FILE = ""data/dev.ja""\nSRC_TEST_FILE = ""data/test.en""\nREF_TEST_FILE = ""data/test.ja""\n\nclass AttentionalEncoderDecoder(Model):\n    """"""Encoder-decoder translation model with dot-attention.""""""\n\n    def __init__(self):\n        self.dropout_rate = DROPOUT_RATE\n        self.psrc_lookup = Parameter()\n        self.ptrg_lookup = Parameter()\n        self.pwhj = Parameter()\n        self.pbj = Parameter()\n        self.pwjy = Parameter()\n        self.pby = Parameter()\n        self.src_fw_lstm = LSTM()\n        self.src_bw_lstm = LSTM()\n        self.trg_lstm = LSTM()\n        self.scan_attributes()\n\n    def init(self, src_vocab_size, trg_vocab_size, embed_size, hidden_size):\n        """"""Creates a new AttentionalEncoderDecoder object.""""""\n        self.psrc_lookup.init([embed_size, src_vocab_size], I.XavierUniform())\n        self.ptrg_lookup.init([embed_size, trg_vocab_size], I.XavierUniform())\n        self.pwhj.init([embed_size, 2 * hidden_size], I.XavierUniform())\n        self.pbj.init([embed_size], I.Constant(0))\n        self.pwjy.init([trg_vocab_size, embed_size], I.XavierUniform())\n        self.pby.init([trg_vocab_size], I.Constant(0))\n        self.src_fw_lstm.init(embed_size, hidden_size)\n        self.src_bw_lstm.init(embed_size, hidden_size)\n        self.trg_lstm.init(2 * embed_size, hidden_size)\n\n    def encode(self, src_batch, train):\n        """"""Encodes source sentences and prepares internal states.""""""\n        # Embedding lookup.\n        src_lookup = F.parameter(self.psrc_lookup)\n        e_list = []\n        for x in src_batch:\n            e = F.pick(src_lookup, x, 1)\n            e = F.dropout(e, self.dropout_rate, train)\n            e_list.append(e)\n\n        # Forward encoding\n        self.src_fw_lstm.restart()\n        f_list = []\n        for e in e_list:\n            f = self.src_fw_lstm.forward(e)\n            f = F.dropout(f, self.dropout_rate, train)\n            f_list.append(f)\n\n        # Backward encoding\n        self.src_bw_lstm.restart()\n        b_list = []\n        for e in reversed(e_list):\n            b = self.src_bw_lstm.forward(e)\n            b = F.dropout(b, self.dropout_rate, train)\n            b_list.append(b)\n\n        b_list.reverse()\n\n        # Concatenates RNN states.\n        fb_list = [f_list[i] + b_list[i] for i in range(len(src_batch))]\n        self.concat_fb = F.concat(fb_list, 1)\n        self.t_concat_fb = F.transpose(self.concat_fb)\n\n        # Initializes decode states.\n        embed_size = self.psrc_lookup.shape()[0]\n        self.trg_lookup = F.parameter(self.ptrg_lookup)\n        self.whj = F.parameter(self.pwhj)\n        self.bj = F.parameter(self.pbj)\n        self.wjy = F.parameter(self.pwjy)\n        self.by = F.parameter(self.pby)\n        self.feed = F.zeros([embed_size])\n        self.trg_lstm.restart(\n            self.src_fw_lstm.get_c() + self.src_bw_lstm.get_c(),\n            self.src_fw_lstm.get_h() + self.src_bw_lstm.get_h())\n\n    def decode_step(self, trg_words, train):\n        """"""One step decoding.""""""\n        e = F.pick(self.trg_lookup, trg_words, 1)\n        e = F.dropout(e, self.dropout_rate, train)\n        h = self.trg_lstm.forward(F.concat([e, self.feed], 0))\n        h = F.dropout(h, self.dropout_rate, train)\n        atten_probs = F.softmax(self.t_concat_fb @ h, 0)\n        c = self.concat_fb @ atten_probs\n        self.feed = F.tanh(self.whj @ F.concat([h, c], 0) + self.bj)\n        return self.wjy @ self.feed + self.by\n\n    def loss(self, trg_batch, train):\n        """"""Calculates loss values.""""""\n        losses = []\n        for i in range(len(trg_batch)-1):\n            y = self.decode_step(trg_batch[i], train)\n            loss = F.softmax_cross_entropy(y, trg_batch[i + 1], 0)\n            losses.append(loss)\n        return F.batch.mean(F.sum(losses))\n\n\ndef train(encdec, optimizer, prefix, best_valid_ppl):\n    # Registers all parameters to the optimizer.\n    optimizer.add(encdec)\n\n    # Loads vocab.\n    src_vocab = make_vocab(SRC_TRAIN_FILE, SRC_VOCAB_SIZE)\n    trg_vocab = make_vocab(TRG_TRAIN_FILE, TRG_VOCAB_SIZE)\n    inv_trg_vocab = make_inv_vocab(trg_vocab)\n    print(""#src_vocab:"", len(src_vocab))\n    print(""#trg_vocab:"", len(trg_vocab))\n\n    # Loads all corpus\n    train_src_corpus = load_corpus(SRC_TRAIN_FILE, src_vocab)\n    train_trg_corpus = load_corpus(TRG_TRAIN_FILE, trg_vocab)\n    valid_src_corpus = load_corpus(SRC_VALID_FILE, src_vocab)\n    valid_trg_corpus = load_corpus(TRG_VALID_FILE, trg_vocab)\n    test_src_corpus = load_corpus(SRC_TEST_FILE, src_vocab)\n    test_ref_corpus = load_corpus_ref(REF_TEST_FILE, trg_vocab)\n    num_train_sents = len(train_trg_corpus)\n    num_valid_sents = len(valid_trg_corpus)\n    num_test_sents = len(test_ref_corpus)\n    num_train_labels = count_labels(train_trg_corpus)\n    num_valid_labels = count_labels(valid_trg_corpus)\n    print(""train:"", num_train_sents, ""sentences,"", num_train_labels, ""labels"")\n    print(""valid:"", num_valid_sents, ""sentences,"", num_valid_labels, ""labels"")\n\n    # Sentence IDs\n    train_ids = list(range(num_train_sents))\n    valid_ids = list(range(num_valid_sents))\n\n    # Train/valid loop.\n    for epoch in range(MAX_EPOCH):\n        # Computation graph.\n        g = Graph()\n        Graph.set_default(g)\n\n        print(""epoch %d/%d:"" % (epoch + 1, MAX_EPOCH))\n        print(""  learning rate scale = %.4e"" % optimizer.get_learning_rate_scaling())\n\n        # Shuffles train sentence IDs.\n        random.shuffle(train_ids)\n\n        # Training.\n        train_loss = 0.\n        for ofs in range(0, num_train_sents, BATCH_SIZE):\n            print(""%d"" % ofs, end=""\\r"")\n            sys.stdout.flush()\n\n            batch_ids = train_ids[ofs:min(ofs + BATCH_SIZE, num_train_sents)]\n            src_batch = make_batch(train_src_corpus, batch_ids, src_vocab)\n            trg_batch = make_batch(train_trg_corpus, batch_ids, trg_vocab)\n\n            g.clear()\n            encdec.encode(src_batch, True)\n            loss = encdec.loss(trg_batch, True)\n            train_loss += loss.to_float() * len(batch_ids)\n\n            optimizer.reset_gradients()\n            loss.backward()\n            optimizer.update()\n\n        train_ppl = math.exp(train_loss / num_train_labels)\n        print(""  train PPL = %.4f"" % train_ppl)\n\n        # Validation.\n        valid_loss = 0.\n        for ofs in range(0, num_valid_sents, BATCH_SIZE):\n            print(""%d"" % ofs, end=""\\r"")\n            sys.stdout.flush()\n\n            batch_ids = valid_ids[ofs:min(ofs + BATCH_SIZE, num_valid_sents)]\n            src_batch = make_batch(valid_src_corpus, batch_ids, src_vocab)\n            trg_batch = make_batch(valid_trg_corpus, batch_ids, trg_vocab)\n\n            g.clear()\n            encdec.encode(src_batch, False)\n            loss = encdec.loss(trg_batch, False)\n            valid_loss += loss.to_float() * len(batch_ids)\n\n        valid_ppl = math.exp(valid_loss/num_valid_labels)\n        print(""  valid PPL = %.4f"" % valid_ppl)\n\n        # Calculates test BLEU.\n        stats = defaultdict(int)\n        for ofs in range(0, num_test_sents, BATCH_SIZE):\n            print(""%d"" % ofs, end=""\\r"")\n            sys.stdout.flush()\n\n            src_batch = test_src_corpus[ofs : min(ofs + BATCH_SIZE, num_test_sents)]\n            ref_batch = test_ref_corpus[ofs : min(ofs + BATCH_SIZE, num_test_sents)]\n\n            hyp_ids = test_batch(encdec, src_vocab, trg_vocab, src_batch)\n            for hyp_line, ref_line in zip(hyp_ids, ref_batch):\n                for k, v in get_bleu_stats(ref_line[1:-1], hyp_line).items():\n                    stats[k] += v\n\n        bleu = calculate_bleu(stats)\n        print(""  test BLEU = %.2f"" % (100 * bleu))\n\n        # Saves best model/optimizer.\n        if valid_ppl < best_valid_ppl:\n            best_valid_ppl = valid_ppl\n            print(""  saving model/optimizer ... "", end="""")\n            sys.stdout.flush()\n            encdec.save(prefix + "".model"")\n            optimizer.save(prefix + "".optimizer"")\n            save_ppl(prefix + "".valid_ppl"", best_valid_ppl)\n            print(""done."")\n        else:\n            # Learning rate decay by 1/sqrt(2)\n            new_scale = .7071 * optimizer.get_learning_rate_scaling()\n            optimizer.set_learning_rate_scaling(new_scale)\n\n\ndef test_batch(encdec, src_vocab, trg_vocab, lines):\n    g = Graph()\n    Graph.set_default(g)\n\n    src_batch = make_batch(lines, list(range(len(lines))), src_vocab)\n\n    encdec.encode(src_batch, False)\n\n    # Generates target words one-by-one.\n    trg_ids = [np.array([trg_vocab[""<bos>""]] * len(lines))]\n    eos_id = trg_vocab[""<eos>""]\n    eos_ids = np.array([eos_id] * len(lines))\n    while (trg_ids[-1] != eos_ids).any():\n        if len(trg_ids) > GENERATION_LIMIT + 1:\n            print(""Warning: Sentence generation did not finish in"",\n                    GENERATION_LIMIT, ""iterations."", file=sys.stderr)\n            trg_ids.append(eos_ids)\n            break\n        y = encdec.decode_step(trg_ids[-1], False)\n        trg_ids.append(np.array(y.argmax(0)).T)\n\n    return [hyp[:np.where(hyp == eos_id)[0][0]] for hyp in np.array(trg_ids[1:]).T]\n\n\ndef test(encdec):\n    # Loads vocab.\n    src_vocab = make_vocab(SRC_TRAIN_FILE, SRC_VOCAB_SIZE)\n    trg_vocab = make_vocab(TRG_TRAIN_FILE, TRG_VOCAB_SIZE)\n    inv_trg_vocab = make_inv_vocab(trg_vocab)\n\n    for line in sys.stdin:\n        trg_ids = test_batch(encdec, src_vocab, trg_vocab, [line_to_sent(line.strip(), src_vocab)])[0]\n        # Prints the result.\n        print("" "".join(inv_trg_vocab[wid] for wid in trg_ids))\n\n\ndef main():\n    parser = ArgumentParser()\n    parser.add_argument(""mode"", help=""(train|resume|test)"")\n    parser.add_argument(""model_prefix"", help=""prefix of the model files."")\n    args = parser.parse_args()\n\n    mode = args.mode\n    prefix = args.model_prefix\n    print(""mode:"", mode, file=sys.stderr)\n    print(""prefix:"", prefix, file=sys.stderr)\n\n    if mode not in (""train"", ""resume"", ""test""):\n        print(""unknown mode:"", mode, file=sys.stderr)\n        return\n\n    print(""initializing device ... "", end="""", file=sys.stderr)\n    sys.stderr.flush()\n    dev = D.CUDA(0)\n    Device.set_default(dev)\n    print(""done."", file=sys.stderr)\n\n    if mode == ""train"":\n        encdec = AttentionalEncoderDecoder()\n        encdec.init(SRC_VOCAB_SIZE, TRG_VOCAB_SIZE, NUM_EMBED_UNITS, NUM_HIDDEN_UNITS)\n        optimizer = O.Adam()\n        optimizer.set_weight_decay(1e-6)\n        optimizer.set_gradient_clipping(5)\n        train(encdec, optimizer, prefix, 1e10)\n    elif mode == ""resume"":\n        print(""loading model/optimizer ... "", end="""", file=sys.stderr)\n        sys.stderr.flush()\n        encdec = AttentionalEncoderDecoder()\n        encdec.load(prefix + "".model"")\n        optimizer = O.Adam()\n        optimizer.load(prefix + "".optimizer"")\n        valid_ppl = load_ppl(prefix + "".valid_ppl"")\n        print(""done."", file=sys.stderr)\n        train(encdec, optimizer, prefix, valid_ppl)\n    else:\n        print(""loading model ... "", end="""", file=sys.stderr)\n        sys.stderr.flush()\n        encdec = AttentionalEncoderDecoder()\n        encdec.load(prefix + "".model"")\n        print(""done."", file=sys.stderr)\n        test(encdec)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/encdec/lstm.py,0,"b'# Hand-written LSTM with input/forget/output gates and no peepholes.\n# Formulation:\n#   i = sigmoid(W_xi . x[t] + W_hi . h[t-1] + b_i)\n#   f = sigmoid(W_xf . x[t] + W_hf . h[t-1] + b_f)\n#   o = sigmoid(W_xo . x[t] + W_ho . h[t-1] + b_o)\n#   j = tanh   (W_xj . x[t] + W_hj . h[t-1] + b_j)\n#   c[t] = i * j + f * c[t-1]\n#   h[t] = o * tanh(c[t])\n\nfrom primitiv import Model\nfrom primitiv import Node\nfrom primitiv import Parameter\nfrom primitiv import functions as F\nfrom primitiv import initializers as I\n\n\nclass LSTM(Model):\n    """"""LSTM cell.""""""\n\n    def __init__(self):\n        self.pwxh = Parameter()\n        self.pwhh = Parameter()\n        self.pbh = Parameter()\n        self.scan_attributes()\n\n    def init(self, in_size, out_size):\n        """"""Creates a new LSTM.""""""\n        self.pwxh.init([4 * out_size, in_size], I.XavierUniform())\n        self.pwhh.init([4 * out_size, out_size], I.XavierUniform())\n        self.pbh.init([4 * out_size], I.Constant(0))\n\n    def restart(self, init_c = Node(), init_h = Node()):\n        """"""Initializes internal states.""""""\n        out_size = self.pwhh.shape()[1]\n        self.wxh = F.parameter(self.pwxh)\n        self.whh = F.parameter(self.pwhh)\n        self.bh = F.parameter(self.pbh)\n        self.c = init_c if init_c.valid() else F.zeros([out_size])\n        self.h = init_h if init_h.valid() else F.zeros([out_size])\n\n    def forward(self, x):\n        """"""One step forwarding.""""""\n        out_size = self.pwhh.shape()[1]\n        u = self.wxh @ x + self.whh @ self.h + self.bh\n        i = F.sigmoid(F.slice(u, 0, 0, out_size))\n        f = F.sigmoid(F.slice(u, 0, out_size, 2 * out_size))\n        o = F.sigmoid(F.slice(u, 0, 2 * out_size, 3 * out_size))\n        j = F.tanh(F.slice(u, 0, 3 * out_size, 4 * out_size))\n        self.c = i * j + f * self.c\n        self.h = o * F.tanh(self.c)\n        return self.h\n\n    def get_c(self):\n        """"""Retrieves current internal cell state.""""""\n        return self.c\n\n    def get_h(self):\n        """"""Retrieves current hidden value.""""""\n        return self.h\n'"
examples/encdec/utils.py,0,"b'from collections import defaultdict\n\n\n# Gathers the set of words from space-separated corpus and makes a vocabulary.\ndef make_vocab(path, size):\n    if (size < 3):\n        print(""Vocab size should be >= 3."", file=sys.stderr)\n        sys.exit(1)\n    ifs = open(path, ""r"")\n\n    # Counts all word existences.\n    freq = defaultdict(lambda : 0)\n    for line in ifs:\n        for word in line.split():\n            freq[word] += 1\n\n    # Sorting.\n    # Chooses top size-3 frequent words to make the vocabulary.\n    vocab = {}\n    vocab[""<unk>""] = 0\n    vocab[""<bos>""] = 1\n    vocab[""<eos>""] = 2\n    for i, (k, v) in zip(range(3, size), sorted(freq.items(), key=lambda x: -x[1])):\n        vocab[k] = i\n    return vocab\n\n# Generates ID-to-word dictionary.\ndef make_inv_vocab(vocab):\n    ret = [k for k, v in sorted(vocab.items(), key=lambda x: x[1])]\n    return ret\n\n# Generates word ID list from a sentence.\ndef line_to_sent(line, vocab):\n    unk_id = vocab[""<unk>""]\n    converted = ""<bos> "" + line + "" <eos>""\n    return [vocab.get(word, unk_id) for word in converted.split()]\n\n# Generates word ID list from a corpus.\n# All out-of-vocab words are replaced to <unk>.\ndef load_corpus(path, vocab):\n    return [line_to_sent(line, vocab) for line in open(path)]\n\n# Generates word ID list from a reference sentence.\ndef line_to_sent_ref(line, vocab):\n    # NOTE(odashi):\n    # -1 never becomes a word ID of any specific words and this is useful to\n    # prevent BLEU contamination.\n    unk_id = -1\n    converted = ""<bos> "" + line + "" <eos>""\n    return [vocab.get(word, unk_id) for word in converted.split()]\n\n# Generates word ID list from a reference corpus.\n# All out-of-vocab words are replaced to -1.\ndef load_corpus_ref(path, vocab):\n    return [line_to_sent_ref(line, vocab) for line in open(path)]\n\n# Counts output labels in the corpus.\ndef count_labels(corpus):\n    ret = 0\n    for sent in corpus:\n        ret += len(sent) - 1  # w/o <bos>\n    return ret\n\n# Extracts a minibatch from loaded corpus\n# NOTE(odashi):\n# Lengths of all sentences are adjusted to the maximum one in the minibatch.\n# All additional subsequences are filled by <eos>. E.g.,\n#   input: {\n#     {<bos>, w1, <eos>},\n#     {<bos>, w1, w2, w3, w4, <eos>},\n#     {<bos>, w1, w2, <eos>},\n#     {<bos>, w1, w2, w3, <eos>},\n#   }\n#   output: {\n#     {<bos>, <bos>, <bos>, <bos>},\n#     {   w1,    w1,    w1,    w1},\n#     {<eos>,    w2,    w2,    w2},\n#     {<eos>,    w3, <eos>,    w3},\n#     {<eos>,    w4, <eos>, <eos>},\n#     {<eos>, <eos>, <eos>, <eos>},\n#   }\ndef make_batch(corpus, sent_ids, vocab):\n    batch_size = len(sent_ids)\n    eos_id = vocab[""<eos>""]\n    max_len = 0\n    for sid in sent_ids:\n        max_len = max(max_len, len(corpus[sid]))\n    batch = [[eos_id] * batch_size for i in range(max_len)]\n    for i in range(batch_size):\n        sent = corpus[sent_ids[i]]\n        for j in range(len(sent)):\n            batch[j][i] = sent[j]\n    return batch\n\n\n# Helper to save current ppl.\ndef save_ppl(path, ppl):\n    with open(path, ""w"") as ofs:\n        print(ppl, file=ofs)\n\n\n# Helper to load last ppl.\ndef load_ppl(path):\n    with open(path, ""r"") as ifs:\n        return float(ifs.readline());\n'"
examples/mnist/mnist.py,4,"b'#!/usr/bin/env python3\n\n# Sample code to train/test the MNIST dataset:\n#   http://yann.lecun.com/exdb/mnist/\n#\n# The model consists of a full-connected 2-layer (input/hidden/output)\n# perceptron with the softmax cross entropy loss.\n#\n# Usage:\n#   $ ./download_data.sh\n#   $ python3 ./mnist.py\n\n\nfrom primitiv import Device\nfrom primitiv import Graph\nfrom primitiv import Parameter\n\nfrom primitiv import devices as D\nfrom primitiv import initializers as I\nfrom primitiv import functions as F\nfrom primitiv import optimizers as O\n\nimport random\nimport sys\nimport numpy as np\n\n\nNUM_TRAIN_SAMPLES = 60000\nNUM_TEST_SAMPLES = 10000\nNUM_INPUT_UNITS = 28 * 28\nNUM_HIDDEN_UNITS = 800\nNUM_OUTPUT_UNITS = 10\nBATCH_SIZE = 200\nNUM_TRAIN_BATCHES = NUM_TRAIN_SAMPLES // BATCH_SIZE\nNUM_TEST_BATCHES = NUM_TEST_SAMPLES // BATCH_SIZE\nMAX_EPOCH = 100\n\n\ndef load_images(filename, n):\n    with open(filename, ""rb"") as ifs:\n        ifs.seek(16)  # header\n        return (np.fromfile(ifs, dtype=np.uint8, count=n*NUM_INPUT_UNITS) / 255) \\\n            .astype(np.float32) \\\n            .reshape((n, NUM_INPUT_UNITS))\n\n\ndef load_labels(filename, n):\n    with open(filename, ""rb"") as ifs:\n        ifs.seek(8)  # header\n        return np.fromfile(ifs, dtype=np.uint8, count=n) \\\n            .astype(np.uint32)\n\n\ndef main():\n    # Loads data\n    train_inputs = load_images(""data/train-images-idx3-ubyte"", NUM_TRAIN_SAMPLES)\n    train_labels = load_labels(""data/train-labels-idx1-ubyte"", NUM_TRAIN_SAMPLES)\n    test_inputs = load_images(""data/t10k-images-idx3-ubyte"", NUM_TEST_SAMPLES)\n    test_labels = load_labels(""data/t10k-labels-idx1-ubyte"", NUM_TEST_SAMPLES)\n\n    dev = D.Naive()  # or D.CUDA(gpuid)\n    Device.set_default(dev)\n\n    pw1 = Parameter([NUM_HIDDEN_UNITS, NUM_INPUT_UNITS], I.XavierUniform())\n    pb1 = Parameter([NUM_HIDDEN_UNITS], I.Constant(0))\n    pw2 = Parameter([NUM_OUTPUT_UNITS, NUM_HIDDEN_UNITS], I.XavierUniform())\n    pb2 = Parameter([NUM_OUTPUT_UNITS], I.Constant(0))\n\n    optimizer = O.SGD(.5)\n    optimizer.add(pw1, pb1, pw2, pb2)\n\n    def make_graph(inputs, train):\n        x = F.input(inputs)\n\n        w1 = F.parameter(pw1)\n        b1 = F.parameter(pb1)\n        h = F.relu(w1 @ x + b1)\n\n        h = F.dropout(h, .5, train)\n\n        w2 = F.parameter(pw2)\n        b2 = F.parameter(pb2)\n        return w2 @ h + b2\n\n    ids = list(range(NUM_TRAIN_SAMPLES))\n\n    g = Graph()\n    Graph.set_default(g)\n\n    for epoch in range(MAX_EPOCH):\n        random.shuffle(ids)\n\n        # Training loop\n        for batch in range(NUM_TRAIN_BATCHES):\n            print(""\\rTraining... %d / %d"" % (batch + 1, NUM_TRAIN_BATCHES), end="""")\n            inputs = [train_inputs[ids[batch * BATCH_SIZE + i]] for i in range(BATCH_SIZE)]\n            labels = [train_labels[ids[batch * BATCH_SIZE + i]] for i in range(BATCH_SIZE)]\n\n            g.clear()\n\n            y = make_graph(inputs, True)\n            loss = F.softmax_cross_entropy(y, labels, 0)\n            avg_loss = F.batch.mean(loss)\n\n            optimizer.reset_gradients()\n            avg_loss.backward()\n            optimizer.update()\n\n        print()\n\n        match = 0\n\n        # Test loop\n        for batch in range(NUM_TEST_BATCHES):\n            print(""\\rTesting... %d / %d"" % (batch + 1, NUM_TEST_BATCHES), end="""")\n            inputs = [test_inputs[batch * BATCH_SIZE + i] for i in range(BATCH_SIZE)]\n\n            g.clear()\n\n            y = make_graph(inputs, False)\n            y_val = y.to_list()\n            for i in range(BATCH_SIZE):\n                maxval = -1e10\n                argmax = -1\n                for j in range(NUM_OUTPUT_UNITS):\n                    v = y_val[j + i * NUM_OUTPUT_UNITS]\n                    if (v > maxval):\n                        maxval = v\n                        argmax = j\n                if argmax == test_labels[i + batch * BATCH_SIZE]:\n                    match += 1\n\n        accuracy = 100.0 * match / NUM_TEST_SAMPLES\n        print(""\\nepoch %d: accuracy: %.2f%%\\n"" % (epoch, accuracy))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/mnist/mnist_cnn.py,4,"b'#!/usr/bin/env python3\n\n#  Python example of Convolutional Neural Network.\n#  Please refer primitiv repository for more details.\n#\n# Usage:\n#   $ ./download_data.sh\n#   $ python3 ./mnist_cnn.py\n\nimport random\n\nimport numpy as np\n\nfrom primitiv import functions as F\nfrom primitiv import initializers as I\nfrom primitiv import optimizers as O\nfrom primitiv import devices as D\nfrom primitiv import Device, Graph, Parameter, Shape\n\nNUM_TRAIN_SAMPLES = 60000\nNUM_TEST_SAMPLES = 10000\nBATCH_SIZE = 200\nNUM_TRAIN_BATCHES = NUM_TRAIN_SAMPLES // BATCH_SIZE\nNUM_TEST_BATCHES = NUM_TEST_SAMPLES // BATCH_SIZE\nMAX_EPOCH = 100\n\nIMAGE_HEIGHT = 28\nIMAGE_WIDTH = 28\n\nKERNEL_SIZE1 = 5  # should be an odd number\nKERNEL_SIZE2 = 5  # ditto\nNUM_CHANNELS1 = 8\nNUM_CHANNELS2 = 16\nPADDING1 = KERNEL_SIZE1 // 2\nPADDING2 = KERNEL_SIZE2 // 2\n\nNUM_INPUT_UNITS = (IMAGE_HEIGHT // 4) * (IMAGE_WIDTH // 4) * NUM_CHANNELS2\nNUM_HIDDEN_UNITS = 256\nNUM_OUTPUT_UNITS = 10\n\n\ndef load_images(filename, n):\n    with open(filename, ""rb"") as ifs:\n        ifs.seek(16)  # header\n        return (np.fromfile(ifs, dtype=np.uint8, count=n*NUM_INPUT_UNITS) / 255) \\\n            .astype(np.float32) \\\n            .reshape((n, IMAGE_HEIGHT, IMAGE_WIDTH))\n\n\ndef load_labels(filename, n):\n    with open(filename, ""rb"") as ifs:\n        ifs.seek(8)  # header\n        return np.fromfile(ifs, dtype=np.uint8, count=n) \\\n            .astype(np.uint32)\n\ndef main():\n    # Loads data\n    train_inputs = load_images(""data/train-images-idx3-ubyte"", NUM_TRAIN_SAMPLES)\n    train_labels = load_labels(""data/train-labels-idx1-ubyte"", NUM_TRAIN_SAMPLES)\n    test_inputs = load_images(""data/t10k-images-idx3-ubyte"", NUM_TEST_SAMPLES)\n    test_labels = load_labels(""data/t10k-labels-idx1-ubyte"", NUM_TEST_SAMPLES)\n\n    dev = D.CUDA(0);\n    Device.set_default(dev)\n    g = Graph()\n    Graph.set_default(g)\n\n    # Parameters of CNNs\n    # Shape: {kernel_height, kernel_width, in_channels, out_channels}\n    pw_cnn1 = Parameter(\n        Shape([KERNEL_SIZE1, KERNEL_SIZE1, 1, NUM_CHANNELS1]),\n        I.XavierUniformConv2D())\n    pw_cnn2 = Parameter(\n        Shape([KERNEL_SIZE2, KERNEL_SIZE2, NUM_CHANNELS1, NUM_CHANNELS2]),\n        I.XavierUniformConv2D())\n\n    # Parameters of FC layers\n    pw_fc1 = Parameter(Shape([NUM_HIDDEN_UNITS, NUM_INPUT_UNITS]), I.XavierUniform())\n    pw_fc2 = Parameter(Shape([NUM_OUTPUT_UNITS, NUM_HIDDEN_UNITS]), I.XavierUniform())\n    pb_fc1 = Parameter(Shape([NUM_HIDDEN_UNITS]), I.Constant(0))\n    pb_fc2 = Parameter(Shape([NUM_OUTPUT_UNITS]), I.Constant(0))\n\n    # Optimizer\n    optimizer = O.SGD(.1)\n    optimizer.add(pw_cnn1, pw_cnn2, pw_fc1, pw_fc2, pb_fc1, pb_fc2)\n\n    # Helper lambda to construct the predictor network.\n    def make_graph(inputs, train):\n        # Input and parameters.\n        #x = F.input(Shape([IMAGE_HEIGHT, IMAGE_WIDTH], BATCH_SIZE), inputs)\n        x = F.input(inputs)\n        w_cnn1 = F.parameter(pw_cnn1)\n        w_cnn2 = F.parameter(pw_cnn2)\n        w_fc1 = F.parameter(pw_fc1)\n        w_fc2 = F.parameter(pw_fc2)\n        b_fc1 = F.parameter(pb_fc1)\n        b_fc2 = F.parameter(pb_fc2)\n        # CNNs\n        h_cnn1 = F.relu(F.conv2d(x, w_cnn1, PADDING1, PADDING1, 1, 1, 1, 1))\n        h_pool1 = F.max_pool2d(h_cnn1, 2, 2, 0, 0, 2, 2)\n        h_cnn2 = F.relu(F.conv2d(h_pool1, w_cnn2, PADDING2, PADDING2, 1, 1, 1, 1))\n        h_pool2 = F.max_pool2d(h_cnn2, 2, 2, 0, 0, 2, 2)\n        # FC layers\n        x_fc = F.dropout(F.flatten(h_pool2), .5, train)\n        h_fc = F.dropout(\n            F.relu(F.matmul(w_fc1, x_fc) + b_fc1), .5, train)\n        return F.matmul(w_fc2, h_fc) + b_fc2\n\n    # Batch randomizer\n    ids = list(range(NUM_TRAIN_SAMPLES))\n\n    for epoch in range(MAX_EPOCH):\n        # Shuffles sample IDs.\n        random.shuffle(ids)\n\n        # Training loop\n        for batch in range(NUM_TRAIN_BATCHES):\n            print(""\\rTraining... %d / %d"" % (batch + 1, NUM_TRAIN_BATCHES), end="""")\n            # Makes a minibatch for training.\n            inputs = [train_inputs[ids[batch * BATCH_SIZE + i]] for i in range(BATCH_SIZE)]\n            labels = [train_labels[ids[batch * BATCH_SIZE + i]] for i in range(BATCH_SIZE)]\n\n            # Constructs the graph.\n            g.clear();\n            y = make_graph(inputs, True);\n            loss = F.softmax_cross_entropy(y, labels, 0)\n            avg_loss = F.batch.mean(loss)\n\n            # Dump computation graph at the first time.\n            # if epoch == 0 and batch == 0:\n            #     print(g.dump(""dot""))\n\n            # Implicit forward, backward, and updates parameters.\n            optimizer.reset_gradients()\n            avg_loss.backward()\n            optimizer.update()\n\n        print()\n\n        match = 0\n\n        # Test loop\n        for batch in range(NUM_TEST_BATCHES):\n            print(""\\rTesting... %d / %d"" % (batch + 1, NUM_TEST_BATCHES), end="""")\n            # Makes a test minibatch.\n            inputs = [test_inputs[batch * BATCH_SIZE + i] for i in range(BATCH_SIZE)]\n\n            # Constructs the graph.\n            g.clear()\n            y = make_graph(inputs, False)\n\n            # Gets outputs, argmax, and compares them with the label.\n            y_val = y.to_list()\n            for i in range(BATCH_SIZE):\n                maxval = -1e10\n                argmax = -1\n                for j in range(NUM_OUTPUT_UNITS):\n                    v = y_val[j + i * NUM_OUTPUT_UNITS]\n                    if v > maxval:\n                        maxval = v\n                        argmax = j\n\n                if argmax == test_labels[i + batch * BATCH_SIZE]:\n                    match += 1\n\n        accuracy = 100.0 * match / NUM_TEST_SAMPLES;\n        print(""epoch %d: accuracy: %.2f%%"" % (epoch, accuracy))\n\n    return 0\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/mnist/mnist_multi_gpu.py,4,"b'#!/usr/bin/env python3\n\nfrom primitiv import Graph\nfrom primitiv import Parameter\n\nfrom primitiv import devices as D\nfrom primitiv import initializers as I\nfrom primitiv import functions as F\nfrom primitiv import optimizers as O\n\nimport random\nimport sys\nimport numpy as np\n\n\nNUM_TRAIN_SAMPLES = 60000\nNUM_TEST_SAMPLES = 10000\nNUM_INPUT_UNITS = 28 * 28\nNUM_HIDDEN_UNITS = 800\nNUM_OUTPUT_UNITS = 10\nBATCH_SIZE = 50\nNUM_TRAIN_BATCHES = NUM_TRAIN_SAMPLES // BATCH_SIZE\nNUM_TEST_BATCHES = NUM_TEST_SAMPLES // BATCH_SIZE\nMAX_EPOCH = 100\n\n\ndef load_images(filename, n):\n    with open(filename, ""rb"") as ifs:\n        ifs.seek(16)  # header\n        return (np.fromfile(ifs, dtype=np.uint8, count=n*NUM_INPUT_UNITS) / 255) \\\n            .astype(np.float32) \\\n            .reshape((n, NUM_INPUT_UNITS))\n\n\ndef load_labels(filename, n):\n    with open(filename, ""rb"") as ifs:\n        ifs.seek(8)  # header\n        return np.fromfile(ifs, dtype=np.uint8, count=n) \\\n            .astype(np.uint32)\n\n\ndef main():\n    # Loads data\n    train_inputs = load_images(""data/train-images-idx3-ubyte"", NUM_TRAIN_SAMPLES)\n    train_labels = load_labels(""data/train-labels-idx1-ubyte"", NUM_TRAIN_SAMPLES)\n    test_inputs = load_images(""data/t10k-images-idx3-ubyte"", NUM_TEST_SAMPLES)\n    test_labels = load_labels(""data/t10k-labels-idx1-ubyte"", NUM_TEST_SAMPLES)\n\n    # Initializes 2 device objects which manage different GPUs.\n    dev0 = D.CUDA(0)\n    dev1 = D.CUDA(1)\n\n    # Parameters on GPU 0.\n    pw1 = Parameter([NUM_HIDDEN_UNITS, NUM_INPUT_UNITS], I.XavierUniform(), dev0)\n    pb1 = Parameter([NUM_HIDDEN_UNITS], I.Constant(0), dev0)\n\n    # Parameters on GPU 1.\n    pw2 = Parameter([NUM_OUTPUT_UNITS, NUM_HIDDEN_UNITS], I.XavierUniform(), dev1)\n    pb2 = Parameter([NUM_OUTPUT_UNITS], I.Constant(0), dev1)\n\n    optimizer = O.SGD(.1)\n    optimizer.add(pw1, pb1, pw2, pb2)\n\n    def make_graph(inputs):\n        # We first store input values explicitly on GPU 0.\n        x = F.input(inputs, device=dev0)\n        w1 = F.parameter(pw1)\n        b1 = F.parameter(pb1)\n        w2 = F.parameter(pw2)\n        b2 = F.parameter(pb2)\n        # The hidden layer is calculated and implicitly stored on GPU 0.\n        h_on_gpu0 = F.relu(w1 @ x + b1)\n        # `copy()` transfers the hiddne layer to GPU 1.\n        h_on_gpu1 = F.copy(h_on_gpu0, dev1)\n        # The output layer is calculated and implicitly stored on GPU 1.\n        return w2 @ h_on_gpu1 + b2\n\n    ids = list(range(NUM_TRAIN_SAMPLES))\n\n    g = Graph()\n    Graph.set_default(g)\n\n    for epoch in range(MAX_EPOCH):\n        random.shuffle(ids)\n\n        # Training loop\n        for batch in range(NUM_TRAIN_BATCHES):\n            print(""\\rTraining... %d / %d"" % (batch + 1, NUM_TRAIN_BATCHES), end="""")\n            inputs = [train_inputs[ids[batch * BATCH_SIZE + i]] for i in range(BATCH_SIZE)]\n            labels = [train_labels[ids[batch * BATCH_SIZE + i]] for i in range(BATCH_SIZE)]\n\n            g.clear()\n\n            y = make_graph(inputs)\n            loss = F.softmax_cross_entropy(y, labels, 0)\n            avg_loss = F.batch.mean(loss)\n\n            optimizer.reset_gradients()\n            avg_loss.backward()\n            optimizer.update()\n\n        print()\n\n        match = 0\n\n        # Test loop\n        for batch in range(NUM_TEST_BATCHES):\n            print(""\\rTesting... %d / %d"" % (batch + 1, NUM_TEST_BATCHES), end="""")\n            inputs = [test_inputs[batch * BATCH_SIZE + i] for i in range(BATCH_SIZE)]\n\n            g.clear()\n\n            y = make_graph(inputs)\n            y_val = y.to_list()\n            for i in range(BATCH_SIZE):\n                maxval = -1e10\n                argmax = -1\n                for j in range(NUM_OUTPUT_UNITS):\n                    v = y_val[j + i * NUM_OUTPUT_UNITS]\n                    if (v > maxval):\n                        maxval = v\n                        argmax = j\n                if argmax == test_labels[i + batch * BATCH_SIZE]:\n                    match += 1\n\n        accuracy = 100.0 * match / NUM_TEST_SAMPLES\n        print(""\\nepoch %d: accuracy: %.2f%%\\n"" % (epoch, accuracy))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/ptb/ptb_rnnlm.py,0,"b'#!/usr/bin/env python3\n\nimport random\nimport sys\nimport math\n\nfrom primitiv import initializers as I\nfrom primitiv import functions as F\nfrom primitiv import devices as D\nfrom primitiv import optimizers as O\nfrom primitiv import Device, Graph, Model, Parameter, Shape\n\nfrom utils import make_vocab, load_corpus, count_labels, make_batch\n\nNUM_HIDDEN_UNITS = 256\nBATCH_SIZE = 64\nMAX_EPOCH = 100\n\n\nclass RNNLM(Model):\n\n    def __init__(self, vocab_size, eos_id):\n        self.eos_id = eos_id\n        self.pwlookup = Parameter([NUM_HIDDEN_UNITS, vocab_size], I.XavierUniform())\n        self.pwxs = Parameter([NUM_HIDDEN_UNITS, NUM_HIDDEN_UNITS], I.XavierUniform())\n        self.pwsy = Parameter([vocab_size, NUM_HIDDEN_UNITS], I.XavierUniform())\n        self.scan_attributes()\n\n    # Forward function of RNNLM. Input data should be arranged below:\n    # inputs = {\n    #   {sent1_word1, sent2_word1, ..., sentN_word1},  // 1st input (<s>)\n    #   {sent1_word2, sent2_word2, ..., sentN_word2},  // 2nd input/1st output\n    #\n    #   {sent1_wordM, sent2_wordM, ..., sentN_wordM},  // last output (<s>)\n    # };\n    def forward(self, inputs):\n        batch_size = len(inputs[0])\n        wlookup = F.parameter(self.pwlookup)\n        wxs = F.parameter(self.pwxs)\n        wsy = F.parameter(self.pwsy)\n        s = F.zeros(Shape([NUM_HIDDEN_UNITS], batch_size))\n        outputs = []\n        for i in range(len(inputs) - 1):\n            w = F.pick(wlookup, inputs[i], 1)\n            x = w + s\n            s = F.sigmoid(wxs @ x)\n            outputs.append(wsy @ s)\n        return outputs\n\n    # Loss function.\n    def forward_loss(self, outputs, inputs):\n        losses = [F.softmax_cross_entropy(outputs[i], inputs[i + 1], 0)\n                                            for i in range(len(outputs))]\n        return F.batch.mean(F.sum(losses))\n\n\ndef main():\n    # Loads vocab.\n    vocab = make_vocab(""data/ptb.train.txt"")\n    print(""#vocab:"", len(vocab))  # maybe 10000\n    eos_id = vocab[""<s>""]\n\n    # Loads all corpus.\n    train_corpus = load_corpus(""data/ptb.train.txt"", vocab)\n    valid_corpus = load_corpus(""data/ptb.valid.txt"", vocab)\n    num_train_sents = len(train_corpus)\n    num_valid_sents = len(valid_corpus)\n    num_train_labels = count_labels(train_corpus)\n    num_valid_labels = count_labels(valid_corpus)\n    print(""train:"", num_train_sents, ""sentences,"", num_train_labels, ""labels"")\n    print(""valid:"", num_valid_sents, ""sentences,"", num_valid_labels, ""labels"")\n\n    # Device and computation graph.\n    dev = D.CUDA(0);\n    Device.set_default(dev)\n    g = Graph();\n    Graph.set_default(g)\n\n    # Our LM.\n    lm = RNNLM(len(vocab), eos_id)\n\n    # Optimizer.\n    optimizer = O.Adam()\n    optimizer.set_weight_decay(1e-6)\n    optimizer.set_gradient_clipping(5)\n    optimizer.add(lm)\n\n    # Sentence IDs.\n    train_ids = list(range(num_train_sents))\n    valid_ids = list(range(num_valid_sents))\n\n    # Train/valid loop.\n    for epoch in range(MAX_EPOCH):\n        print(""epoch"", (epoch + 1), ""/"", MAX_EPOCH, "":"")\n        # Shuffles train sentence IDs.\n        random.shuffle(train_ids)\n\n        # Training.\n        train_loss = 0\n        for ofs in range(0, num_train_sents, BATCH_SIZE):\n            batch_ids = train_ids[ofs : min(ofs + BATCH_SIZE, num_train_sents)]\n            batch = make_batch(train_corpus, batch_ids, eos_id)\n\n            g.clear()\n\n            outputs = lm.forward(batch)\n            loss = lm.forward_loss(outputs, batch)\n            train_loss += loss.to_float() * len(batch_ids)\n\n            optimizer.reset_gradients()\n            loss.backward()\n            optimizer.update()\n\n            print(""%d"" % ofs, end=""\\r"")\n            sys.stdout.flush()\n\n        train_ppl = math.exp(train_loss / num_train_labels)\n        print(""  train ppl ="", train_ppl)\n\n        # Validation.\n        valid_loss = 0\n        for ofs in range(0, num_valid_sents, BATCH_SIZE):\n            batch_ids = valid_ids[ofs : min(ofs + BATCH_SIZE, num_valid_sents)]\n            batch = make_batch(valid_corpus, batch_ids, eos_id)\n\n            g.clear()\n\n            outputs = lm.forward(batch)\n            loss = lm.forward_loss(outputs, batch)\n            valid_loss += loss.to_float() * len(batch_ids)\n            print(""%d"" % ofs, end=""\\r"")\n            sys.stdout.flush()\n\n        valid_ppl = math.exp(valid_loss / num_valid_labels)\n        print(""  valid ppl ="", valid_ppl)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/ptb/ptb_rnnlm_lstm.py,0,"b'#!/usr/bin/env python3\n\nimport random\nimport sys\nimport math\n\nfrom primitiv import initializers as I\nfrom primitiv import functions as F\nfrom primitiv import devices as D\nfrom primitiv import optimizers as O\nfrom primitiv import Device, Graph, Model, Parameter, Shape\n\nfrom utils import make_vocab, load_corpus, count_labels, make_batch\n\nNUM_HIDDEN_UNITS = 650\nBATCH_SIZE = 20\nMAX_EPOCH = 50\nDROPOUT_RATE = 0.5\n\n\n# Affine transform:\n#   y = W . x + b\nclass Affine(Model):\n\n    def __init__(self, in_size, out_size):\n        self.pw = Parameter([out_size, in_size], I.Uniform(-0.1, 0.1))\n        self.pb = Parameter([out_size], I.Constant(0))\n        self.scan_attributes()\n\n    # Initializes internal values.\n    def reset(self):\n        self.w = F.parameter(self.pw)\n        self.b = F.parameter(self.pb)\n\n    # Applies transform.\n    def forward(self, x):\n        return self.w @ x + self.b\n\n\n# LSTM with input/forget/output gates and no peepholes.\n# Formulation:\n#   i = sigmoid(W_xi . x[t] + W_hi . h[t-1] + b_i)\n#   f = sigmoid(W_xf . x[t] + W_hf . h[t-1] + b_f)\n#   o = sigmoid(W_xo . x[t] + W_ho . h[t-1] + b_o)\n#   j = tanh   (W_xj . x[t] + W_hj . h[t-1] + b_j)\n#   c[t] = i * j + f * c[t-1]\n#   h[t] = o * tanh(c[t])\nclass LSTM(Model):\n\n    def __init__(self, in_size, out_size):\n        self.out_size = out_size\n        self.pwxh = Parameter([4 * out_size, in_size], I.Uniform(-0.1, 0.1))\n        self.pwhh = Parameter([4 * out_size, out_size], I.Uniform(-0.1, 0.1))\n        self.pbh = Parameter([4 * out_size], I.Constant(0))\n        self.scan_attributes()\n\n    # Initializes internal values.\n    def restart(self):\n        self.wxh = F.parameter(self.pwxh)\n        self.whh = F.parameter(self.pwhh)\n        self.bh = F.parameter(self.pbh)\n        self.h = self.c = F.zeros([self.out_size])\n\n    # Forward one step.\n    def forward(self, x):\n        u = self.wxh @ x + self.whh @ self.h + self.bh\n        i = F.sigmoid(F.slice(u, 0, 0, self.out_size))\n        f = F.sigmoid(F.slice(u, 0, self.out_size, 2 * self.out_size))\n        o = F.sigmoid(F.slice(u, 0, 2 * self.out_size, 3 * self.out_size))\n        j = F.tanh(F.slice(u, 0, 3 * self.out_size, 4 * self.out_size))\n        self.c = i * j + f * self.c\n        self.h = o * F.tanh(self.c)\n        return self.h\n\n\n# Language model using above LSTM.\nclass RNNLM(Model):\n\n    def __init__(self, vocab_size, eos_id):\n        self.eos_id = eos_id\n        self.plookup = Parameter([NUM_HIDDEN_UNITS, vocab_size], I.Uniform(-0.1, 0.1))\n        self.rnn1 = LSTM(NUM_HIDDEN_UNITS, NUM_HIDDEN_UNITS)\n        self.rnn2 = LSTM(NUM_HIDDEN_UNITS, NUM_HIDDEN_UNITS)\n        self.hy = Affine(NUM_HIDDEN_UNITS, vocab_size)\n        self.scan_attributes()\n\n\n    # Forward function of RNNLM. Input data should be arranged below:\n    # inputs = {\n    #   {sent1_word1, sent2_word1, ..., sentN_word1},  // 1st input (<bos>)\n    #   {sent1_word2, sent2_word2, ..., sentN_word2},  // 2nd input/1st output\n    #   ...,\n    #   {sent1_wordM, sent2_wordM, ..., sentN_wordM},  // last output (<eos>)\n    # };\n    def forward(self, inputs, train):\n        batch_size = len(inputs[0])\n        lookup = F.parameter(self.plookup)\n        self.rnn1.restart()\n        self.rnn2.restart()\n        self.hy.reset()\n\n        outputs = []\n        for i in range(len(inputs) - 1):\n            x = F.pick(lookup, inputs[i], 1)\n            x = F.dropout(x, DROPOUT_RATE, train)\n            h1 = self.rnn1.forward(x)\n            h1 = F.dropout(h1, DROPOUT_RATE, train)\n            h2 = self.rnn2.forward(h1)\n            h2 = F.dropout(h2, DROPOUT_RATE, train)\n            outputs.append(self.hy.forward(h2))\n\n        return outputs\n\n\n    # Loss function.\n    def loss(self, outputs, inputs):\n        losses = [F.softmax_cross_entropy(outputs[i], inputs[i + 1], 0) for i in range(len(outputs))]\n        return F.batch.mean(F.sum(losses))\n\n\ndef main():\n    # Loads vocab.\n    vocab = make_vocab(""data/ptb.train.txt"")\n    print(""#vocab:"", len(vocab))  # maybe 10000\n    eos_id = vocab[""<s>""]\n\n    # Loads all corpus.\n    train_corpus = load_corpus(""data/ptb.train.txt"", vocab)\n    valid_corpus = load_corpus(""data/ptb.valid.txt"", vocab)\n    num_train_sents = len(train_corpus)\n    num_valid_sents = len(valid_corpus)\n    num_train_labels = count_labels(train_corpus)\n    num_valid_labels = count_labels(valid_corpus)\n    print(""train:"", num_train_sents, ""sentences,"", num_train_labels, ""labels"")\n    print(""valid:"", num_valid_sents, ""sentences,"", num_valid_labels, ""labels"")\n\n    # Device and computation graph.\n    dev = D.CUDA(0)\n    Device.set_default(dev)\n    g = Graph()\n    Graph.set_default(g)\n\n    # Our LM.\n    lm = RNNLM(len(vocab), eos_id)\n\n    # Optimizer.\n    optimizer = O.SGD(1)\n    #optimizer.set_weight_decay(1e-6)\n    optimizer.set_gradient_clipping(5)\n    optimizer.add(lm)\n\n    # Sentence IDs.\n    train_ids = list(range(num_train_sents))\n    valid_ids = list(range(num_valid_sents))\n\n    best_valid_ppl = 1e10\n\n    # Train/valid loop.\n    for epoch in range(MAX_EPOCH):\n        print(""epoch"", epoch + 1, ""/"", MAX_EPOCH, "":"")\n        # Shuffles train sentence IDs.\n        random.shuffle(train_ids)\n\n        # Training.\n        train_loss = 0\n        for ofs in range(0, num_train_sents, BATCH_SIZE):\n            batch_ids = train_ids[ofs : min(ofs + BATCH_SIZE, num_train_sents)]\n            batch = make_batch(train_corpus, batch_ids, eos_id)\n\n            g.clear()\n\n            outputs = lm.forward(batch, True)\n            loss = lm.loss(outputs, batch)\n            train_loss += loss.to_float() * len(batch_ids)\n\n            optimizer.reset_gradients()\n            loss.backward()\n            optimizer.update()\n\n            print(""%d"" % ofs, end=""\\r"")\n            sys.stdout.flush()\n\n        train_ppl = math.exp(train_loss / num_train_labels)\n        print(""  train ppl ="", train_ppl)\n\n        # Validation.\n        valid_loss = 0\n        for ofs in range(0, num_valid_sents, BATCH_SIZE):\n            batch_ids = valid_ids[ofs : min(ofs + BATCH_SIZE, num_valid_sents)]\n            batch = make_batch(valid_corpus, batch_ids, eos_id)\n\n            g.clear()\n\n            outputs = lm.forward(batch, False)\n            loss = lm.loss(outputs, batch)\n            valid_loss += loss.to_float() * len(batch_ids)\n            print(""%d"" % ofs, end=""\\r"")\n            sys.stdout.flush()\n\n        valid_ppl = math.exp(valid_loss / num_valid_labels)\n        print(""  valid ppl ="", valid_ppl)\n\n        if valid_ppl < best_valid_ppl:\n            best_valid_ppl = valid_ppl\n            print(""  BEST"")\n        else:\n            old_lr = optimizer.get_learning_rate_scaling()\n            new_lr = 0.5 * old_lr\n            optimizer.set_learning_rate_scaling(new_lr)\n            print(""  learning rate scaled:"", old_lr, ""->"", new_lr)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/ptb/ptb_rnnlm_sru.py,0,"b'#!/usr/bin/env python3\n\nimport random\nimport sys\nimport math\n\nfrom primitiv import initializers as I\nfrom primitiv import functions as F\nfrom primitiv import devices as D\nfrom primitiv import optimizers as O\nfrom primitiv import Device, Graph, Model, Parameter, Shape\n\nfrom utils import make_vocab, load_corpus, count_labels, make_batch\n\nNUM_HIDDEN_UNITS = 650\nBATCH_SIZE = 20\nMAX_EPOCH = 50\nDROPOUT_RATE = 0.5\n\n\n# Affine transform:\n#   y = W . x + b\nclass Affine(Model):\n\n    def __init__(self, in_size, out_size):\n        self.pw = Parameter([out_size, in_size], I.Uniform(-0.1, 0.1))\n        self.pb = Parameter([out_size], I.Constant(0))\n        self.scan_attributes()\n\n    # Initializes internal values.\n    def reset(self):\n        self.w = F.parameter(self.pw)\n        self.b = F.parameter(self.pb)\n\n    # Applies transform.\n    def forward(self, x):\n        return self.w @ x + self.b\n\n\n# SRU cell.\n# Formulation:\n#   j[t] = W_xj . x[t]\n#   f[t] = sigmoid(W_xf . x[t] + b_f)\n#   r[t] = sigmoid(W_xr . x[t] + b_r)\n#   c[t] = f[t] * c[t-1] + (1 - f[t]) * j[t]\n#   h[t] = r[t] * tanh(c[t]) + (1 - r[t]) * x[t]\nclass SRU(Model):\n\n    def __init__(self, in_size, out_size):\n        self.out_size = out_size\n        self.pw = Parameter([3 * out_size, in_size], I.Uniform(-0.1, 0.1))\n        self.pbf = Parameter([out_size], I.Constant(0))\n        self.pbr = Parameter([out_size], I.Constant(0))\n        self.scan_attributes()\n\n    # Initializes internal values.\n    def restart(self):\n        self.w = F.parameter(self.pw)\n        self.bf = F.parameter(self.pbf)\n        self.br = F.parameter(self.pbr)\n\n    # Forward.\n    def forward(self, xs):\n        x = F.concat(xs, 1)\n        u = self.w @ x\n        j = F.slice(u, 0, 0, self.out_size)\n        f = F.sigmoid(\n                F.slice(u, 0, self.out_size, 2 * self.out_size)\n                + F.broadcast(self.bf, 1, len(xs)))\n        r = F.sigmoid(\n                F.slice(u, 0, 2 * self.out_size, 3 * self.out_size)\n                + F.broadcast(self.bf, 1, len(xs)))\n        c = F.zeros([self.out_size])\n        hs = []\n        for i in range(len(xs)):\n            ji = F.slice(j, 1, i, i + 1)\n            fi = F.slice(f, 1, i, i + 1)\n            ri = F.slice(r, 1, i, i + 1)\n            c = fi * c + (1 - fi) * ji\n            hs.append(ri * F.tanh(c) + (1 - ri) * xs[i])\n\n        return hs\n\n\n# Language model using above SRU.\nclass RNNLM(Model):\n\n    def __init__(self, vocab_size, eos_id):\n        self.eos_id = eos_id\n        self.plookup = Parameter([NUM_HIDDEN_UNITS, vocab_size], I.Uniform(-0.1, 0.1))\n        self.rnn1 = SRU(NUM_HIDDEN_UNITS, NUM_HIDDEN_UNITS)\n        self.rnn2 = SRU(NUM_HIDDEN_UNITS, NUM_HIDDEN_UNITS)\n        self.hy = Affine(NUM_HIDDEN_UNITS, vocab_size)\n        self.scan_attributes()\n\n    # Forward function of RNNLM. Input data should be arranged below:\n    # inputs = {\n    #   {sent1_word1, sent2_word1, ..., sentN_word1},  // 1st input (<bos>)\n    #   {sent1_word2, sent2_word2, ..., sentN_word2},  // 2nd input/1st output\n    #   ...,\n    #   {sent1_wordM, sent2_wordM, ..., sentN_wordM},  // last output (<eos>)\n    # };\n    def forward(self, inputs, train):\n        batch_size = len(inputs[0])\n        lookup = F.parameter(self.plookup)\n        self.rnn1.restart()\n        self.rnn2.restart()\n        self.hy.reset()\n\n        xs = [F.dropout(F.pick(lookup, inputs[i], 1), DROPOUT_RATE, train)\n            for i in range(len(inputs) - 1)]\n        hs1 = self.rnn1.forward(xs)\n        for i in range(len(inputs) - 1):\n            hs1[i] = F.dropout(hs1[i], DROPOUT_RATE, train)\n        hs2 = self.rnn2.forward(hs1)\n        outputs = [self.hy.forward(F.dropout(hs2[i], DROPOUT_RATE, train))\n            for i in range(len(inputs) - 1)]\n\n        return outputs\n\n    # Loss function.\n    def loss(self, outputs, inputs):\n        losses = [F.softmax_cross_entropy(outputs[i], inputs[i + 1], 0)\n            for i in range(len(outputs))]\n        return F.batch.mean(F.sum(losses))\n\n\ndef main():\n    # Loads vocab.\n    vocab = make_vocab(""data/ptb.train.txt"")\n    print(""#vocab:"", len(vocab))  # maybe 10000\n    eos_id = vocab[""<s>""]\n\n    # Loads all corpus.\n    train_corpus = load_corpus(""data/ptb.train.txt"", vocab)\n    valid_corpus = load_corpus(""data/ptb.valid.txt"", vocab)\n    num_train_sents = len(train_corpus)\n    num_valid_sents = len(valid_corpus)\n    num_train_labels = count_labels(train_corpus)\n    num_valid_labels = count_labels(valid_corpus)\n    print(""train:"", num_train_sents, ""sentences,"", num_train_labels, ""labels"")\n    print(""valid:"", num_valid_sents, ""sentences,"", num_valid_labels, ""labels"")\n\n    # Device and computation graph.\n    dev = D.CUDA(0)\n    Device.set_default(dev)\n    g = Graph()\n    Graph.set_default(g)\n\n    # Our LM.\n    lm = RNNLM(len(vocab), eos_id)\n\n    # Optimizer.\n    optimizer = O.SGD(1)\n    #optimizer.set_weight_decay(1e-6)\n    optimizer.set_gradient_clipping(5)\n    optimizer.add(lm)\n\n    # Sentence IDs.\n    train_ids = list(range(num_train_sents))\n    valid_ids = list(range(num_valid_sents))\n\n    best_valid_ppl = 1e10\n\n    # Train/valid loop.\n    for epoch in range(MAX_EPOCH):\n        print(""epoch"", epoch + 1, ""/"", MAX_EPOCH, "":"")\n        # Shuffles train sentence IDs.\n        random.shuffle(train_ids)\n\n        # Training.\n        train_loss = 0\n        for ofs in range(0, num_train_sents, BATCH_SIZE):\n            batch_ids = train_ids[ofs : min(ofs + BATCH_SIZE, num_train_sents)]\n            batch = make_batch(train_corpus, batch_ids, eos_id)\n\n            g.clear()\n\n            outputs = lm.forward(batch, True)\n            loss = lm.loss(outputs, batch)\n            train_loss += loss.to_float() * len(batch_ids)\n\n            optimizer.reset_gradients()\n            loss.backward()\n            optimizer.update()\n\n            print(""%d"" % ofs, end=""\\r"")\n            sys.stdout.flush()\n\n        train_ppl = math.exp(train_loss / num_train_labels)\n        print(""  train ppl ="", train_ppl)\n\n        # Validation.\n        valid_loss = 0\n        for ofs in range(0, num_valid_sents, BATCH_SIZE):\n            batch_ids = valid_ids[ofs : min(ofs + BATCH_SIZE, num_valid_sents)]\n            batch = make_batch(valid_corpus, batch_ids, eos_id)\n\n            g.clear()\n\n            outputs = lm.forward(batch, False)\n            loss = lm.loss(outputs, batch)\n            valid_loss += loss.to_float() * len(batch_ids)\n            print(""%d"" % ofs, end=""\\r"")\n            sys.stdout.flush()\n\n        valid_ppl = math.exp(valid_loss / num_valid_labels)\n        print(""  valid ppl ="", valid_ppl)\n\n        if valid_ppl < best_valid_ppl:\n            best_valid_ppl = valid_ppl\n            print(""  BEST"")\n        else:\n            old_lr = optimizer.get_learning_rate_scaling()\n            new_lr = 0.5 * old_lr\n            optimizer.set_learning_rate_scaling(new_lr)\n            print(""  learning rate scaled:"", old_lr, ""->"", new_lr)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/ptb/utils.py,0,"b'# Common utility functions for PTB examples.\n\n# Gathers the set of words from space-separated corpus.\ndef make_vocab(filename):\n    vocab = {}\n    with open(filename, ""r"") as ifs:\n        for line in ifs:\n            line = ""<s> "" + line.strip() + "" <s>""\n            for word in line.split():\n                if word not in vocab:\n                    vocab[word] = len(vocab)\n    return vocab\n\n\n# Generates word ID list using corpus and vocab.\ndef load_corpus(filename, vocab):\n    corpus = []\n    with open(filename, ""r"") as ifs:\n        for line in ifs:\n            line = ""<s> "" + line.strip() + "" <s>""\n            sentence = [vocab[word] for word in line.split()]\n            corpus.append(sentence)\n    return corpus\n\n\n# Counts output labels in the corpus.\ndef count_labels(corpus):\n    ret = 0\n    for sent in corpus:\n        ret += len(sent) - 1\n    return ret\n\n\n# Extracts a minibatch from loaded corpus\ndef make_batch(corpus, sent_ids, eos_id):\n    batch_size = len(sent_ids)\n    max_len = 0\n    for sid in sent_ids:\n        max_len = max(max_len, len(corpus[sid]))\n    batch = [[eos_id] * batch_size for i in range(max_len)]\n    for i in range(batch_size):\n        sent = corpus[sent_ids[i]]\n        for j in range(len(sent)):\n            batch[j][i] = sent[j]\n    return batch\n'"
examples/xor/xor.py,8,"b'#!/usr/bin/env python3\n\nfrom primitiv import Device\nfrom primitiv import Graph\nfrom primitiv import Parameter\n\nfrom primitiv import devices as D\nfrom primitiv import initializers as I\nfrom primitiv import functions as F\nfrom primitiv import optimizers as O\n\nimport numpy as np\n\n\ndef main():\n    dev = D.Naive()  # or D.CUDA(gpuid)\n    Device.set_default(dev)\n\n    # Parameters\n    pw1 = Parameter([8, 2], I.XavierUniform())\n    pb1 = Parameter([8], I.Constant(0))\n    pw2 = Parameter([1, 8], I.XavierUniform())\n    pb2 = Parameter([], I.Constant(0))\n\n    # Optimizer\n    optimizer = O.SGD(0.1)\n\n    # Registers parameters.\n    optimizer.add(pw1, pb1, pw2, pb2)\n\n    # Training data\n    input_data = [\n        np.array([ 1,  1], dtype=np.float32),  # Sample 1\n        np.array([ 1, -1], dtype=np.float32),  # Sample 2\n        np.array([-1,  1], dtype=np.float32),  # Sample 3\n        np.array([-1, -1], dtype=np.float32),  # Sample 4\n    ]\n    output_data = [\n        np.array([ 1], dtype=np.float32),  # Label 1\n        np.array([-1], dtype=np.float32),  # Label 2\n        np.array([-1], dtype=np.float32),  # Label 3\n        np.array([ 1], dtype=np.float32),  # Label 4\n    ]\n\n    g = Graph()\n    Graph.set_default(g)\n\n    for i in range(10):\n        g.clear()\n\n        # Builds a computation graph.\n        x = F.input(input_data)\n        w1 = F.parameter(pw1)\n        b1 = F.parameter(pb1)\n        w2 = F.parameter(pw2)\n        b2 = F.parameter(pb2)\n        h = F.tanh(w1 @ x + b1)\n        y = w2 @ h + b2\n\n        # Obtains values.\n        y_val = y.to_list()\n        print(""epoch "", i, "":"")\n        for j in range(4):\n            print(""  ["", j, ""]: "", y_val[j])\n\n        # Extends the computation graph to calculate loss values.\n        t = F.input(output_data)\n        diff = t - y\n        loss = F.batch.mean(diff * diff)\n\n        # Obtains the loss.\n        loss_val = loss.to_float()\n        print(""  loss: "", loss_val)\n\n        # Updates parameters.\n        optimizer.reset_gradients()\n        loss.backward()\n        optimizer.update()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
primitiv/devices/__init__.py,0,"b'from primitiv.devices._naive_device import Naive\n__all__ = [""Naive""]\n\ntry:\n    from primitiv.devices._cuda_device import CUDA\n    __all__.append(""CUDA"")\n#except ModuleNotFoundError:\nexcept ImportError:\n    pass\n\ntry:\n    from primitiv.devices._eigen_device import Eigen\n    __all__.append(""Eigen"")\n#except ModuleNotFoundError:\nexcept ImportError:\n    pass\n\ntry:\n    from primitiv.devices._opencl_device import OpenCL\n    __all__.append(""OpenCL"")\n#except ModuleNotFoundError:\nexcept ImportError:\n    pass\n'"
primitiv/initializers/__init__.py,0,"b'from primitiv.initializers._initializer_impl import Constant\nfrom primitiv.initializers._initializer_impl import Uniform\nfrom primitiv.initializers._initializer_impl import Normal\nfrom primitiv.initializers._initializer_impl import Identity\nfrom primitiv.initializers._initializer_impl import XavierUniform\nfrom primitiv.initializers._initializer_impl import XavierNormal\nfrom primitiv.initializers._initializer_impl import XavierUniformConv2D\nfrom primitiv.initializers._initializer_impl import XavierNormalConv2D\n\n\n__all__ = [\n    ""Constant"",\n    ""Uniform"",\n    ""Normal"",\n    ""Identity"",\n    ""XavierUniform"",\n    ""XavierNormal"",\n    ""XavierUniformConv2D"",\n    ""XavierNormalConv2D"",\n]\n'"
primitiv/optimizers/__init__.py,0,"b'from primitiv.optimizers._optimizer_impl import SGD\nfrom primitiv.optimizers._optimizer_impl import MomentumSGD\nfrom primitiv.optimizers._optimizer_impl import AdaGrad\nfrom primitiv.optimizers._optimizer_impl import RMSProp\nfrom primitiv.optimizers._optimizer_impl import AdaDelta\nfrom primitiv.optimizers._optimizer_impl import Adam\n\n__all__ = [\n    ""SGD"",\n    ""MomentumSGD"",\n    ""AdaGrad"",\n    ""RMSProp"",\n    ""AdaDelta"",\n    ""Adam"",\n]\n'"
