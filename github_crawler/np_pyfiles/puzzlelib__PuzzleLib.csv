file_path,api_count,code
Blueprint.py,7,"b'import json, os, importlib.util\n\nimport numpy as np\n\nfrom PuzzleLib.Config import libname\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Modules.Module import Module\nfrom PuzzleLib.Containers.Node import Node\n\n\nclass BlueprintError(Exception):\n\tpass\n\n\nclass BlueprintFactory:\n\tdef __init__(self):\n\t\tself.containers = {}\n\t\tself.modules = {}\n\n\t\tpaths = [""Containers"", ""Modules""]\n\n\t\tignores = [\n\t\t\t{""Node"", ""Container"", ""__init__""},\n\t\t\t{""Module"", ""__init__""}\n\t\t]\n\n\t\tfactories = [self.containers, self.modules]\n\n\t\tfor path, ignore, factory in zip(paths, ignores, factories):\n\t\t\tfactoryPath = os.path.join(os.path.dirname(__file__), path)\n\t\t\tfor file in os.listdir(factoryPath):\n\t\t\t\tif file.endswith("".py"") and os.path.splitext(file)[0] not in ignore:\n\t\t\t\t\tfilepath = os.path.abspath(os.path.join(factoryPath, file))\n\n\t\t\t\t\tspec = importlib.util.spec_from_file_location(os.path.basename(filepath)[:-3], filepath)\n\t\t\t\t\tmod = importlib.util.module_from_spec(spec)\n\n\t\t\t\t\tspec.loader.exec_module(mod)\n\n\t\t\t\t\tname = os.path.splitext(file)[0]\n\t\t\t\t\tfactory[name] = getattr(mod, name)\n\n\n\tdef build(self, blueprint, log=False, logwidth=20):\n\t\tclassname = blueprint[""classname""]\n\t\tscheme = blueprint[""scheme""]\n\n\t\tif classname in self.containers:\n\t\t\tgraph = blueprint[""graph""]\n\t\t\telements = blueprint[""modules""]\n\n\t\t\tif classname in {""Sequential"", ""Parallel""}:\n\t\t\t\tmod = self.containers[classname](name=scheme[""name""])\n\n\t\t\t\tfor name in graph:\n\t\t\t\t\tcl = self.build(elements[name], log=log)\n\t\t\t\t\tmod.append(cl)\n\n\t\t\telif classname == ""Graph"":\n\t\t\t\tnodes = {name: Node(self.build(bprint, log=log)) for name, bprint in elements.items()}\n\t\t\t\tfor node in nodes.values():\n\t\t\t\t\tnode.addBackwards([(nodes[name], slots) for name, slots in graph[node.name]])\n\n\t\t\t\tinputs = [nodes[name] for name in blueprint[""inputs""]]\n\t\t\t\toutputs = [nodes[name] for name in blueprint[""outputs""]]\n\n\t\t\t\tmod = self.containers[classname](inputs, outputs, name=scheme[""name""])\n\n\t\t\telse:\n\t\t\t\traise NotImplementedError(classname)\n\n\t\telif classname in self.modules:\n\t\t\tcl = self.modules[classname]\n\n\t\t\tif log:\n\t\t\t\tfmt = ""[%s] Loading module named %-"" + str(logwidth) + ""s type %-"" + str(logwidth) + ""s ...""\n\t\t\t\tprint(fmt % (libname, ""\'%s\'"" % scheme[""name""], classname), end="""")\n\n\t\t\tif ""initscheme"" in scheme:\n\t\t\t\tscheme[""initscheme""] = ""none""\n\n\t\t\tmod = cl(**scheme)\n\n\t\t\tif log:\n\t\t\t\tprint("" Done"")\n\n\t\telse:\n\t\t\traise BlueprintError(""Cannot build module with class name \'%s\'"" % classname)\n\n\t\treturn mod\n\n\ndef load(hdf, name=None, assumeUniqueNames=False, log=False, logwidth=20):\n\twith Module.ensureHdf(hdf, ""r"") as hdf:\n\t\tblueprint = json.loads(str(np.array(hdf[""blueprint""])))\n\n\t\tif log:\n\t\t\tprint(""[%s] Building model from blueprint ..."" % libname)\n\n\t\tmod = BlueprintFactory().build(blueprint, log=log, logwidth=logwidth)\n\n\t\tif log:\n\t\t\tprint(""[%s] Loading model data ..."" % libname)\n\n\t\tmod.load(hdf, name=name, assumeUniqueNames=assumeUniqueNames)\n\n\treturn mod\n\n\ndef unittest():\n\tfileTest()\n\tmemoryTest()\n\tgraphTest()\n\n\ndef buildNet():\n\tfrom PuzzleLib.Containers import Sequential, Parallel\n\tfrom PuzzleLib.Modules import Linear, Activation, relu, Replicate, Concat\n\n\tseq = Sequential()\n\n\tseq.append(Linear(20, 10, name=""linear-1""))\n\tseq.append(Activation(relu, name=""relu-1""))\n\n\tseq.append(Linear(10, 5, name=""linear-2""))\n\tseq.append(Activation(relu, name=""relu-2""))\n\n\tseq.append(Replicate(times=2, name=""repl""))\n\tseq.append(Parallel().append(Linear(5, 2, name=""linear-3-1"")).append(Linear(5, 3, name=""linear-3-2"")))\n\tseq.append(Concat(axis=1, name=""concat""))\n\n\treturn seq\n\n\ndef buildGraph():\n\tfrom PuzzleLib.Containers import Graph\n\tfrom PuzzleLib.Modules import Linear, Activation, relu, Concat\n\n\tinp = Linear(20, 10, name=""linear-1"").node()\n\th = Activation(relu, name=""relu-1"").node(inp)\n\n\th1 = Linear(10, 5, name=""linear-2"").node(h)\n\th2 = Linear(10, 5, name=""linear-3"").node(h)\n\n\toutput = Concat(axis=1, name=""concat"").node(h1, h2)\n\tgraph = Graph(inputs=inp, outputs=output)\n\n\treturn graph\n\n\ndef fileTest():\n\tseq = buildNet()\n\n\tdata = gpuarray.to_gpu(np.random.randn(32, 20).astype(np.float32))\n\torigOutData = seq(data)\n\n\ttry:\n\t\tseq.save(""./TestData/seq.hdf"", withBlueprint=True)\n\t\tnewSeq = load(""./TestData/seq.hdf"", log=True)\n\n\tfinally:\n\t\tif os.path.exists(""./TestData/seq.hdf""):\n\t\t\tos.remove(""./TestData/seq.hdf"")\n\n\tnewOutData = newSeq(data)\n\tassert np.allclose(origOutData.get(), newOutData.get())\n\n\ndef memoryTest():\n\tseq = buildNet()\n\n\tdata = gpuarray.to_gpu(np.random.randn(32, 20).astype(np.float32))\n\torigOutData = seq(data)\n\n\tmmap = seq.save(withBlueprint=True)\n\tnewSeq = load(mmap, log=True)\n\n\tnewOutData = newSeq(data)\n\tassert np.allclose(origOutData.get(), newOutData.get())\n\n\ndef graphTest():\n\tgraph = buildGraph()\n\n\tdata = gpuarray.to_gpu(np.random.randn(32, 20).astype(np.float32))\n\torigOutData = graph(data)\n\n\ttry:\n\t\tgraph.save(""./TestData/graph.hdf"", withBlueprint=True)\n\t\tnewGraph = load(""./TestData/graph.hdf"", log=True)\n\n\tfinally:\n\t\tif os.path.exists(""./TestData/graph.hdf""):\n\t\t\tos.remove(""./TestData/graph.hdf"")\n\n\tnewOutData = newGraph(data)\n\tassert np.allclose(origOutData.get(), newOutData.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Board.py,0,"b'from graphviz import Digraph\n\n\ndef drawBoard(net, filename, view=True, fmt=""svg"", modulesOnly=False, name=None, fontname=""Consolas"", fullnames=True):\n\tif name is None:\n\t\tname = net.name\n\n\tg = Digraph(name, filename=filename)\n\tg.format = fmt\n\n\tg.attr(label=name, labelloc=""top"", labeljust=""center"", fontcolor=""#31343F"", fontname=fontname)\n\tg.edge_attr.update(color=""#31343F"")\n\tg.node_attr.update(style=""filled"", color=""#CA5237"", shape=""Mrecord"", fontname=fontname, fontcolor=""white"",\n\t\t\t\t\t   fontsize=""8"")\n\n\tblueprint = net.getBlueprint()\n\tdrawGraph(g, blueprint, childName=name, modulesOnly=modulesOnly, fullnames=fullnames)\n\n\tg.view(filename) if view else g.render(filename)\n\n\ndef buildContainerLabel(classname, params, name, showFullname):\n\tlabel = """"""<\n\t<table border=""0"" cellspacing=""5"" bgcolor=""#FFB84D"" style=""rounded"">\n\t\t<tr><td align=""center"" colspan=""2""><font point-size=""10"">%s</font></td></tr>\n\t"""""" % classname\n\n\tparams = params.copy()\n\n\tif showFullname:\n\t\tparams[""fullname""] = name\n\n\tfor paramName in sorted(params.keys()):\n\t\tlabel += ""<tr><td align=\\""left\\"">%s</td><td align=\\""right\\"">%s</td></tr>"" % (paramName, params[paramName])\n\n\tlabel += ""</table>>""\n\treturn label\n\n\ndef buildModuleLabel(classname, params, name, showFullname):\n\tlabel = """"""<\n\t<table cellspacing=""0"">\n\t\t<tr><td align=""center"" colspan=""2""><font point-size=""10"">%s</font></td></tr>\n\t"""""" % classname\n\n\tparams = params.copy()\n\n\tif showFullname:\n\t\tparams[""fullname""] = name\n\n\tfor paramName in sorted(params.keys()):\n\t\tcolor = ""white""\n\t\tif paramName == ""name"":\n\t\t\tcolor = ""#31343F""\n\n\t\tlabel += ""<tr><td align=\\""left\\""><font color=\\""%s\\"">%s</font></td>"" \\\n\t\t\t\t ""<td align=\\""right\\""><font color=\\""%s\\"">%s</font></td></tr>"" % \\\n\t\t\t\t (color, paramName, color, params[paramName])\n\n\tlabel += ""</table>>""\n\treturn label\n\n\ndef drawGraph(g, blueprint, parentName=None, childName=None, clusterIdx=0, modulesOnly=False, fullnames=True):\n\tclassname = blueprint[""classname""]\n\tscheme = blueprint[""scheme""]\n\n\tname = ""%s.%s"" % (parentName, childName) if parentName is not None else str(childName)\n\n\tif classname in {""Sequential"", ""Parallel"", ""Graph""}:\n\t\tgraph = blueprint[""graph""]\n\t\telements = blueprint[""modules""]\n\n\t\twith g.subgraph(name=""cluster_%s"" % clusterIdx) as c:\n\t\t\tclusterIdx += 1\n\n\t\t\tif not modulesOnly:\n\t\t\t\tc.attr(label=buildContainerLabel(classname, {""name"": scheme[""name""]}, name, fullnames),\n\t\t\t\t\t   labeljust=""right"", shape=""Mrecord"", color=""#31343F"",\n\t\t\t\t\t   fontcolor=""#554037"", fontsize=""8"", rankdir=""TB"")\n\t\t\telse:\n\t\t\t\tc.attr(color=""#FFFFFF"", fontcolor=""#FFFFFF"")\n\n\t\t\tinNodes, outNodes = [], []\n\t\t\tif classname == ""Sequential"":\n\t\t\t\tif len(graph) > 0:\n\t\t\t\t\tclusterIdx, inNodes, outNodes = drawGraph(c, elements[graph[0]], parentName=name,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  childName=graph[0], clusterIdx=clusterIdx,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  modulesOnly=modulesOnly, fullnames=fullnames)\n\n\t\t\t\tcurOutNodes = outNodes\n\t\t\t\tfor i, nm in enumerate(graph[1:]):\n\t\t\t\t\tclusterIdx, newInNodes, outNodes = drawGraph(c, elements[nm], parentName=name,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t childName=nm, clusterIdx=clusterIdx,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t modulesOnly=modulesOnly, fullnames=fullnames)\n\n\t\t\t\t\tconnectNodes(c, curOutNodes, newInNodes)\n\t\t\t\t\tcurOutNodes = outNodes\n\n\t\t\t\treturn clusterIdx, [inNode + "":w"" for inNode in inNodes if isinstance(inNode, str)], outNodes\n\n\t\t\telif classname == ""Parallel"":\n\t\t\t\tfor nm in graph:\n\t\t\t\t\tclusterIdx, newInNodes, newOutNodes = drawGraph(c, elements[nm], parentName=name,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tchildName=nm, clusterIdx=clusterIdx,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmodulesOnly=modulesOnly, fullnames=fullnames)\n\n\t\t\t\t\tinNodes.append(newInNodes)\n\t\t\t\t\toutNodes.append(newOutNodes)\n\n\t\t\t\treturn clusterIdx, inNodes, outNodes\n\n\t\t\telif classname == ""Graph"":\n\t\t\t\tinputs, outputs = set(blueprint[""inputs""]), set(blueprint[""outputs""])\n\t\t\t\tnodes = {}\n\n\t\t\t\tfor nm, mod in elements.items():\n\t\t\t\t\t_, newInNodes, newOutNodes = drawGraph(c, mod, parentName=name, childName=nm, clusterIdx=clusterIdx,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t   modulesOnly=modulesOnly, fullnames=fullnames)\n\n\t\t\t\t\tnodes[nm] = (newInNodes, newOutNodes)\n\n\t\t\t\t\tif nm in inputs:\n\t\t\t\t\t\tinNodes.extend(newInNodes)\n\n\t\t\t\t\tif nm in outputs:\n\t\t\t\t\t\toutNodes.extend(newOutNodes)\n\n\t\t\t\tfor nm, node in nodes.items():\n\t\t\t\t\tconnectNodes(c, [nodes[name][0] for name, _ in graph[nm]], node[1])\n\n\t\t\t\treturn clusterIdx, inNodes, outNodes\n\n\t\t\telse:\n\t\t\t\traise NotImplementedError(classname)\n\n\telse:\n\t\tg.node(name, label=buildModuleLabel(classname, scheme, name, fullnames))\n\t\treturn clusterIdx, [name], [name]\n\n\ndef connectNodes(g, inNodes, outNodes):\n\tif isinstance(inNodes, str):\n\t\tif isinstance(outNodes, str):\n\t\t\tg.edges([(inNodes, outNodes)])\n\t\telse:\n\t\t\tfor outNode in outNodes:\n\t\t\t\tconnectNodes(g, inNodes, outNode)\n\n\telif isinstance(outNodes, str):\n\t\tfor inNode in inNodes:\n\t\t\tconnectNodes(g, inNode, outNodes)\n\n\telif len(inNodes) == len(outNodes):\n\t\tfor j, node in enumerate(outNodes):\n\t\t\tconnectNodes(g, inNodes[j], node)\n\n\telif len(inNodes) == 1:\n\t\tfor node in outNodes:\n\t\t\tconnectNodes(g, inNodes[0], node)\n\n\telif len(outNodes) == 1:\n\t\tfor node in inNodes:\n\t\t\tconnectNodes(g, node, outNodes[0])\n\n\telse:\n\t\tassert False\n\n\ndef netTest():\n\tfrom PuzzleLib.Models.Nets.Inception import loadInceptionV3\n\tnet = loadInceptionV3(None)\n\n\tdrawBoard(net, filename=""./TestData/net.gv"", view=False, modulesOnly=False)\n\n\ndef graphTest():\n\tfrom PuzzleLib.Models.Nets.ResNet import loadResNet\n\tnet = loadResNet(None, layers=""50"")\n\n\tfrom PuzzleLib.Passes.ConvertToGraph import toGraph\n\tgraph = toGraph(net, nodesOnly=True)\n\n\tdrawBoard(graph, filename=""./TestData/graph.gv"", view=False, modulesOnly=False)\n\n\ndef mixedTest():\n\tfrom PuzzleLib.Containers import Graph, Sequential\n\tfrom PuzzleLib.Modules import Linear, Split, Concat, Activation, relu\n\n\tv1 = Linear(100, 50, name=""v1"").node()\n\th1 = Split(axis=1, sections=(20, 20, 10), name=""h1"").node(v1)\n\n\tv2 = Linear(100, 50, name=""v2"").node()\n\th2 = Concat(axis=1, name=""h2"").node((h1, [1, 2]), v2)\n\th3 = Activation(relu, name=""h3"").node(h2)\n\n\th4 = Concat(axis=1, name=""h4"").node((h1, 0), h3)\n\tmlp = Graph(inputs=[v1, v2], outputs=h4)\n\n\tseq = Sequential()\n\n\tseq.append(Linear(10, 200))\n\tseq.append(Split(axis=1, sections=(100, 100)))\n\n\tseq.append(mlp)\n\tseq.append(Activation(relu))\n\n\tdrawBoard(seq, filename=""./TestData/mixed.gv"", view=False, modulesOnly=False)\n\n\ndef unittest():\n\tnetTest()\n\tgraphTest()\n\tmixedTest()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Config.py,0,"b'import multiprocessing\nfrom enum import Enum\n\n\nclass ConfigError(Exception):\n\tpass\n\n\nclass Backend(Enum):\n\tcuda = 0\n\thip = 1\n\tcpu = 2\n\tintel = 3\n\n\nbackend = Backend.cuda\ndeviceIdx = 0\n\n\nallowMultiContext = False\nsystemLog = False\n\n\nlibname = ""PuzzleLib""\ncachepath = None\n\n\nglobalEvalMode = False\ndisableDtypeShapeChecks = False\ndisableModuleCompatChecks = False\nverifyData = False\nshowWarnings = True\n\n\ndef isCPUBased(bnd):\n\treturn bnd in {Backend.cpu, Backend.intel}\n\n\ndef shouldInit():\n\treturn multiprocessing.current_process().name == ""MainProcess"" or allowMultiContext\n'"
Grid.py,0,"b'from multiprocessing import Process, SimpleQueue\n\n\ndef runGrid(target, size, *args, devices=None, **kwargs):\n\tgridinfo = generateGridInfo(size, devices)\n\tnodes = [Process(target=nodeRunner, args=(target, nodeinfo) + args, kwargs=kwargs) for nodeinfo in gridinfo]\n\n\tfor node in nodes:\n\t\tnode.start()\n\n\tfor node in nodes:\n\t\tnode.join()\n\n\ndef generateGridInfo(size, devices):\n\tdevices = range(size) if devices is None else devices\n\n\tqueues = [(SimpleQueue(), SimpleQueue()) for _ in range(size - 1)]\n\tparent = ParentNode(0, size, devices[0], queues)\n\n\tnodes = [ChildNode(index + 1, size, devices[index + 1], queues[index]) for index in range(size - 1)]\n\treturn [parent] + nodes\n\n\ndef nodeRunner(target, nodeinfo, *args, **kwargs):\n\tfrom PuzzleLib import Config\n\n\tConfig.allowMultiContext = True\n\tConfig.deviceIdx = nodeinfo.device\n\n\ttry:\n\t\ttarget(nodeinfo, *args, **kwargs)\n\n\tfinally:\n\t\tnodeinfo.close()\n\n\nclass NodeInfo:\n\tdef __init__(self, index, gridsize, device, queues):\n\t\tself.index = index\n\t\tself.gridsize = gridsize\n\n\t\tself.device = device\n\t\tself.queues = queues\n\n\t\tself.outTensors, self.inTensors = {}, {}\n\n\n\tdef close(self):\n\t\tfor mapped, _ in self.inTensors.values():\n\t\t\tmapped.free()\n\n\n\tdef meanValue(self, value):\n\t\traise NotImplementedError()\n\n\n\tdef broadcastBuffer(self, name, buffer):\n\t\traise NotImplementedError()\n\n\n\tdef sumTensor(self, name, tensor):\n\t\traise NotImplementedError()\n\n\n\tdef recvBuffer(self, name, queue, buffer=None):\n\t\tfrom PuzzleLib.Backend.Utils import backend\n\n\t\tparentname, bufipc, bufsize, args = queue.get()\n\t\tassert name == parentname\n\n\t\tcache = self.inTensors.get(name, None)\n\n\t\tif cache is None:\n\t\t\tcache = (backend.Driver.allocateFromIPCHandle(bufipc, bufsize), None)\n\t\t\tself.inTensors[name] = cache\n\n\t\tmapped, _ = cache\n\n\t\tif buffer is not None:\n\t\t\tmapped.copy(dst=buffer)\n\t\t\tbackend.Driver.Device.synchronize()\n\n\t\t\tmapped = buffer\n\n\t\treturn mapped, args\n\n\n\tdef sendBuffer(self, name, buffer, queue, *args):\n\t\tfrom PuzzleLib.Backend.Utils import backend\n\n\t\tif name not in self.outTensors:\n\t\t\tself.outTensors[name] = buffer\n\t\t\tbufipc = buffer.getIPCHandle()\n\t\telse:\n\t\t\tassert self.outTensors[name] is buffer\n\t\t\tbufipc = None\n\n\t\tbackend.Driver.Device.synchronize()\n\t\tqueue.put((name, bufipc, buffer.size, args))\n\n\nclass ParentNode(NodeInfo):\n\tdef meanValue(self, value):\n\t\tvalue += sum(ctopQueue.get() for _, ctopQueue in self.queues)\n\t\tvalue /= self.gridsize\n\n\t\tfor ptocQueue, _ in self.queues:\n\t\t\tptocQueue.put(value)\n\n\t\treturn value\n\n\n\tdef broadcastBuffer(self, name, buffer):\n\t\tfor ptocQueue, _ in self.queues:\n\t\t\tself.sendBuffer(name, buffer, ptocQueue)\n\n\t\tfor _, ctopQueue in self.queues:\n\t\t\tchildname = ctopQueue.get()\n\t\t\tassert name == childname\n\n\n\tdef sumTensor(self, name, tensor):\n\t\tfrom PuzzleLib.Backend.Blas import addVectorToVector\n\t\tfrom PuzzleLib.Backend.gpuarray import GPUArray\n\n\t\tbeta = 1.0 / self.gridsize\n\n\t\tfor index, (_, ctopQueue) in enumerate(self.queues):\n\t\t\tbuffer, (shape, dtype) = self.recvBuffer(name, ctopQueue)\n\t\t\tassert shape == tensor.shape and dtype == tensor.dtype\n\n\t\t\tchildTensor = GPUArray(shape, dtype, gpudata=buffer)\n\t\t\taddVectorToVector(tensor, childTensor, out=tensor, alpha=beta if index == 0 else 1.0, beta=beta)\n\n\t\tself.broadcastBuffer(name, tensor.gpudata)\n\n\nclass ChildNode(NodeInfo):\n\tdef meanValue(self, value):\n\t\tptocQueue, ctopQueue = self.queues\n\n\t\tctopQueue.put(value)\n\t\treturn ptocQueue.get()\n\n\n\tdef broadcastBuffer(self, name, buffer):\n\t\tptocQueue, ctopQueue = self.queues\n\n\t\tself.recvBuffer(name, ptocQueue, buffer=buffer)\n\t\tctopQueue.put(name)\n\n\n\tdef sumTensor(self, name, tensor):\n\t\t_, ctopQueue = self.queues\n\n\t\tself.sendBuffer(name, tensor.gpudata, ctopQueue, tensor.shape, tensor.dtype)\n\t\tself.broadcastBuffer(name, tensor.gpudata)\n'"
Statistics.py,11,"b'import numpy as np\n\n\ndef confusion(labels, predictions, dim=0, log=True):\n\tif dim <= 0:\n\t\tfor label in labels:\n\t\t\tif int(label) >= dim:\n\t\t\t\tdim = int(label)+1\n\n\t\tfor label in predictions:\n\t\t\tif int(label) >= dim:\n\t\t\t\tdim = int(label) + 1\n\n\tcm = [[0] * dim for _ in range(dim)]\n\n\tfor i in range(min(len(labels), len(predictions))):\n\t\tcm[int(labels[i])][int(predictions[i])] += 1\n\n\tif log:\n\t\tprint(""Confusion Matrix:"")\n\n\t\tfor mst in cm:\n\t\t\tprint(str(mst))\n\n\treturn cm\n\n\ndef precision(cm, log=True, verbose=True):\n\tdim = len(cm)\n\tprs, pr = [], 0\n\n\tfor i in range(dim):\n\t\tsummary = 0\n\n\t\tfor j in range(dim):\n\t\t\tsummary += cm[j][i]\n\n\t\tif summary == 0:\n\t\t\ttpr = 1.0\n\t\telse:\n\t\t\ttpr = cm[i][i] / summary\n\n\t\tpr += tpr\n\t\tprs.append(tpr)\n\n\t\tif log and verbose:\n\t\t\tprint(""Precision on class %s: %s"" % (i, tpr))\n\n\tpr /= dim\n\n\tif log:\n\t\tprint(""Precision mean: %s"" % pr)\n\n\treturn pr, prs\n\n\ndef recall(cm, log=True, verbose=True):\n\tdim = len(cm)\n\trcs, rc = [], 0\n\n\tfor i in range(dim):\n\t\tsummary = 0\n\n\t\tfor j in range(dim):\n\t\t\tsummary += cm[i][j]\n\n\t\tif summary == 0:\n\t\t\ttrc = 1.0\n\t\telse:\n\t\t\ttrc = cm[i][i] / summary\n\n\t\trc += trc\n\t\trcs.append(trc)\n\n\t\tif log and verbose:\n\t\t\tprint(""Recall on class %d: %f"" % (i, trc))\n\n\trc /= dim\n\n\tif log:\n\t\tprint(""Recall mean: %s"" % rc)\n\n\treturn rc, rcs\n\n\ndef accuracy(cm, log=True):\n\tdim = len(cm)\n\tacc, summary = 0, 0\n\n\tfor i in range(dim):\n\t\tfor j in range(dim):\n\t\t\tsummary += cm[i][j]\n\n\t\tacc += cm[i][i]\n\n\tacc /= summary\n\n\tif log:\n\t\tprint(""Accuracy: %s"" % acc)\n\n\treturn acc\n\n\ndef fullstats(labels, predictions, dim=0, printing=True, verbose=True):\n\tcm = confusion(labels, predictions, dim, printing)\n\tpr, prs = precision(cm, printing, verbose)\n\trc, rcs = recall(cm, printing, verbose)\n\n\treturn cm, pr, rc, prs, rcs\n\n\ndef unittest():\n\tcm = np.ones(shape=(1000, 1000), dtype=np.float32)\n\n\tassert np.isclose(precision(cm, verbose=False)[0], 0.001)\n\tassert np.isclose(recall(cm, verbose=False)[0], 0.001)\n\tassert np.isclose(accuracy(cm), 0.001)\n\n\timport random\n\tlabels = [random.randint(0, 5) for _ in range(10000)]\n\tresults = [random.randint(0, 5) for _ in range(10000)]\n\n\tcm, pr, rc, prs, rcs = fullstats(labels, results, verbose=False)\n\n\tlabels = np.array(labels)\n\tresults = np.array(results)\n\n\tnpcm, nppr, nprc, npprs, nprcs = fullstats(labels, results, verbose=False)\n\n\tassert np.allclose(np.array(cm), npcm)\n\tassert np.allclose(np.array(pr), nppr)\n\tassert np.allclose(np.array(rc), nprc)\n\tassert np.allclose(np.array(prs), npprs)\n\tassert np.allclose(np.array(rcs), nprcs)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Unittester.py,0,"b'import sys, os, traceback, gc, random, importlib\nfrom colorama import Fore, Style\n\nfrom PuzzleLib import Config\nConfig.systemLog = True\n\n\nif ""PYCHARM_HOSTED"" not in os.environ:\n\timport colorama\n\tcolorama.init()\n\n\ndef runModuleTest(mod, path, threshold=20):\n\titern = 0\n\tprevpath = os.getcwd()\n\n\ttry:\n\t\tos.chdir(path)\n\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\tmod.unittest()\n\n\t\t\texcept Exception as e:\n\t\t\t\tif isinstance(e, AssertionError):\n\t\t\t\t\ttraceinfo = traceback.extract_tb(sys.exc_info()[-1])\n\t\t\t\t\t_, line, _, text = traceinfo[-1]\n\n\t\t\t\t\te = ""An assert error occurred on line %s in statement:\\n\'%s\'"" % (line, text)\n\n\t\t\t\tprint(\n\t\t\t\t\tFore.YELLOW + ""%s unittest failed on try #%s with reason: %s"" % (mod, itern + 1, e) +\n\t\t\t\t\tStyle.RESET_ALL\n\t\t\t\t)\n\n\t\t\t\tif itern < threshold - 1:\n\t\t\t\t\titern += 1\n\t\t\t\t\tcontinue\n\n\t\t\t\telse:\n\t\t\t\t\tprint(Fore.RED + ""... threshold limit exceeded, skipping ..."" + Style.RESET_ALL)\n\t\t\t\t\treturn False\n\n\t\t\tprint(Fore.LIGHTGREEN_EX + ""%s unittest finished successfully"" % mod + Style.RESET_ALL)\n\t\t\treturn True\n\n\tfinally:\n\t\tos.chdir(prevpath)\n\n\ndef runUnittests(filenames, basepath, threshold=20):\n\tfrom PuzzleLib.Backend.Utils import setupDebugAllocator\n\n\tprint(""Setting debug allocator ..."")\n\tsetupDebugAllocator()\n\n\tgc.set_debug(gc.DEBUG_UNCOLLECTABLE)\n\n\tfailure, success, ignored = [], [], []\n\tfor filename in filenames:\n\t\timportname = os.path.splitext(os.path.relpath(filename, basepath))[0].replace(os.sep, ""."")\n\n\t\ttry:\n\t\t\tmod = importlib.import_module(""%s.%s"" % (Config.libname, importname))\n\n\t\t\tif hasattr(mod, ""unittest"") and not hasattr(mod, ""main""):\n\t\t\t\tprint(""%s has unittest. Starting ..."" % filename)\n\n\t\t\t\tdstContainer = success if runModuleTest(mod, os.path.dirname(filename), threshold) else failure\n\t\t\t\tdstContainer.append(filename)\n\n\t\t\t\tgc.collect()\n\n\t\t\telse:\n\t\t\t\tprint(""Skipping %s ..."" % filename)\n\t\t\t\tignored.append(filename)\n\n\t\texcept Exception as e:\n\t\t\ttraceinfo = traceback.extract_tb(sys.exc_info()[-1])\n\t\t\t_, line, _, _ = traceinfo[-1]\n\n\t\t\te = ""(line: %s, type: %s): %s"" % (line, type(e), e)\n\t\t\tprint(Fore.RED + ""%s testing failed with reason: %s"" % (importname, e) + Style.RESET_ALL)\n\n\t\t\tfailure.append(filename)\n\n\treturn failure, success, ignored\n\n\ndef prepareBackend(exclude):\n\tif Config.backend == Config.Backend.cuda:\n\t\texclude = prepareCudaBackend(exclude)\n\n\telif Config.backend == Config.Backend.hip:\n\t\texclude = prepareHipBackend(exclude)\n\n\telif Config.backend == Config.Backend.cpu:\n\t\texclude = prepareCPUBackend(exclude)\n\n\telif Config.backend == Config.Backend.intel:\n\t\texclude = prepareIntelBackend(exclude)\n\n\telse:\n\t\traise NotImplementedError(Config.backend)\n\n\treturn exclude\n\n\ndef prepareCudaBackend(exclude):\n\texclude.discard(""Cuda"")\n\treturn exclude\n\n\ndef prepareHipBackend(exclude):\n\texclude.discard(""Hip"")\n\texclude.update([\n\t\t""Modules/Pad2D.py"", ""Modules/Split.py"", ""Modules/Slice.py"",\n\t\t""Modules/MaxPool3D.py"", ""Modules/AvgPool3D.py"", ""Modules/SpatialTf.py"",\n\t\t""Modules/CrossMapLRN.py"", ""Modules/LCN.py""\n\t])\n\n\treturn exclude\n\n\ndef prepareCPUBackend(exclude):\n\texclude.discard(""CPU"")\n\n\texclude.update([\n\t\t""Modules/Conv1D.py"", ""Modules/Conv2D.py"", ""Modules/Conv3D.py"",\n\t\t""Modules/Deconv1D.py"", ""Modules/Deconv2D.py"", ""Modules/Deconv3D.py"",\n\t\t""Modules/BatchNorm1D.py"", ""Modules/BatchNorm2D.py"", ""Modules/BatchNorm3D.py"", ""Modules/BatchNorm.py"",\n\t\t""Modules/MaxPool1D.py"", ""Modules/MaxPool2D.py"", ""Modules/MaxPool3D.py"",\n\t\t""Modules/AvgPool1D.py"", ""Modules/AvgPool2D.py"", ""Modules/AvgPool3D.py"",\n\t\t""Modules/Pad1D.py"", ""Modules/Pad2D.py"", ""Modules/Upsample2D.py"", ""Modules/Upsample3D.py"",\n\t\t""Modules/InstanceNorm2D.py"", ""Modules/MaxUnpool2D.py"", ""Modules/Cast.py"", ""Modules/Sum.py"",\n\t\t""Modules/DepthConcat.py"", ""Modules/LCN.py"",""Modules/PRelu.py"", ""Modules/RNN.py"", ""Modules/Embedder.py"",\n\t\t""Modules/SpatialTf.py"", ""Modules/Gelu.py"", ""Modules/CrossMapLRN.py"", ""Modules/SoftMax.py"", ""Modules/MapLRN.py"",\n\t\t""Modules/GroupLinear.py"", ""Modules/SubtractMean.py"",\n\n\t\t""Models/Nets/NiN.py"", ""Models/Nets/VGG.py"", ""Models/Nets/ResNet.py"", ""Models/Nets/Inception.py"",\n\t\t""Models/Nets/WaveToLetter.py"", ""Models/Nets/MiniYolo.py"", ""Models/Nets/UNet.py"",\n\t\t""Models/Nets/SentiNet.py"", ""Models/Nets/Presets/SentiNet.py"", ""Models/Misc/RBM.py"",\n\n\t\t""Cost/CrossEntropy.py"", ""Cost/BCE.py"", ""Cost/L1Hinge.py"", ""Cost/SmoothL1.py"", ""Cost/KLDivergence.py"",\n\t\t""Cost/Hinge.py"", ""Cost/SVM.py"", ""Cost/CTC.py"",\n\n\t\t""Containers/Sequential.py"", ""Containers/Parallel.py"", ""Containers/Graph.py"",\n\t\t""Handlers/Trainer.py"", ""Handlers/Validator.py"", ""Passes/ConvertToGraph.py"",\n\n\t\t""Optimizers/AdaDelta.py"", ""Optimizers/AdaGrad.py"", ""Optimizers/Adam.py"",\n\t\t""Optimizers/SGD.py"", ""Optimizers/MomentumSGD.py"", ""Optimizers/NesterovSGD.py"",\n\t\t""Optimizers/RMSProp.py"", ""Optimizers/RMSPropGraves.py"", ""Optimizers/SMORMS3.py""\n\t])\n\n\treturn exclude\n\n\ndef prepareIntelBackend(exclude):\n\tos.environ[""OMP_NUM_THREADS""] = str(2)\n\texclude.difference_update([""CPU"", ""Intel""])\n\n\texclude.update([\n\t\t""Modules/Pad1D.py"", ""Modules/Pad2D.py"", ""Modules/Embedder.py"", ""Modules/PRelu.py"", ""Modules/Cast.py"",\n\t\t""Modules/Upsample2D.py"", ""Modules/Upsample3D.py"", ""Modules/MapLRN.py"", ""Modules/Gelu.py"",\n\t\t""Modules/SubtractMean.py"", ""Modules/LCN.py"", ""Modules/MaxUnpool2D.py"", ""Modules/DepthConcat.py"",\n\t\t""Modules/BatchNorm.py"", ""Modules/BatchNorm1D.py"", ""Modules/BatchNorm2D.py"", ""Modules/BatchNorm3D.py"",\n\t\t""Modules/Sum.py"", ""Modules/GroupLinear.py"", ""Modules/RNN.py"", ""Modules/SpatialTf.py"",\n\t\t""Cost/CTC.py"",\n\t\t""Models/Nets/SentiNet.py"", ""Models/Nets/Presets/SentiNet.py""\n\t])\n\n\treturn exclude\n\n\ndef gatherTestableFiles(exclude):\n\tfilenames = []\n\texclude = set(os.path.abspath(file) for file in exclude)\n\n\tfor dirname, subdirs, names in os.walk(os.getcwd()):\n\t\tfiles = (os.path.join(dirname, file) for file in names)\n\t\tfilenames.extend(file for file in files if isTestableFile(file, exclude))\n\n\trandom.shuffle(filenames)\n\treturn filenames\n\n\ndef isTestableFile(filename, exclude):\n\tif not filename.endswith("".py"") or any(file for file in exclude if os.path.commonprefix((filename, file)) == file):\n\t\treturn False\n\n\treturn True\n\n\ndef main():\n\tbasepath = os.path.dirname(os.path.abspath(__file__))\n\tos.chdir(basepath)\n\n\texclude = {\n\t\t""Converter"",\n\t\t""Cuda"", ""Hip"", ""CPU"", ""Intel""\n\t}\n\n\tfilenames = gatherTestableFiles(prepareBackend(exclude))\n\tfailure, success, ignored = runUnittests(filenames, basepath, threshold=20)\n\n\tprint(""Checked %s source files: %s"" % (len(filenames), filenames))\n\tprint(""Ignored %s files without unittests: %s"" % (len(ignored), ignored))\n\n\tprint(""Success on %s files: %s"" % (len(success), success))\n\tprint(""Failure on %s files: %s"" % (len(failure), failure))\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Variable.py,0,"b'from PuzzleLib import Config\nfrom PuzzleLib.Backend import gpuarray\n\n\nclass Variable:\n\tindex = 0\n\n\n\tdef __init__(self, data, name=None, withgrad=True, grad=None, updater=None, postUpdater=None):\n\t\tif name is None:\n\t\t\tself.name = str(type(self).index)\n\t\t\ttype(self).index += 1\n\t\telse:\n\t\t\tself.name = name\n\n\t\tself.data = data\n\t\tself.updater = updater\n\n\t\tif updater is not None:\n\t\t\treturn\n\n\t\tself.postUpdater = postUpdater\n\t\tself.grad = None\n\n\t\tif grad is not None:\n\t\t\tself.grad = grad\n\n\t\telif withgrad and not Config.globalEvalMode:\n\t\t\tself.grad = gpuarray.zeros(shape=self.data.shape, dtype=self.data.dtype)\n\n\t\tself.learnRate, self.momRate = 1.0, 1.0\n\t\tself.wc = 0.0\n\n\n\t@property\n\tdef hasUpdater(self):\n\t\treturn self.updater is not None\n\n\n\t@property\n\tdef hasPostUpdater(self):\n\t\treturn self.postUpdater is not None\n\n\n\tdef update(self, learnRate):\n\t\tself.updater(self, learnRate)\n\n\n\tdef postUpdate(self):\n\t\tself.postUpdater(self)\n\n\n\tdef set(self, variable):\n\t\tself.data.set(variable.data)\n\n\t\tif self.grad is not None:\n\t\t\tself.grad.set(variable.grad)\n'"
Visual.py,27,"b'import math, os, io\n\nimport numpy as np\nfrom PIL import Image\n\n\nclass VisualError(Exception):\n\tpass\n\n\ndef loadImage(filename, shape=None, normalize=True, mapsToFront=True):\n\timg = Image.open(filename)\n\treturn imageToArray(img, shape, normalize, mapsToFront)\n\n\ndef loadImageFromBytes(bytebuffer, shape=None, normalize=True, mapsToFront=True):\n\timg = Image.open(io.BytesIO(bytebuffer))\n\treturn imageToArray(img, shape, normalize, mapsToFront)\n\n\ndef imageToArray(img, shape=None, normalize=True, mapsToFront=True):\n\timg = img.resize(shape, Image.ANTIALIAS) if shape is not None else img\n\timg = np.array(img, dtype=np.float32 if normalize else np.uint8)\n\n\tif img.ndim == 3 and img.shape[-1] == 4:\n\t\timg = img[:, :, :3]\n\n\tif normalize:\n\t\tif img.max() > 0.0:\n\t\t\timg *= 2.0 / img.max()\n\n\t\timg -= 1.0\n\n\tif mapsToFront:\n\t\tif img.ndim == 2:\n\t\t\timg = img.reshape((1, 1, *img.shape))\n\n\t\telse:\n\t\t\timg = np.rollaxis(img, 2)\n\t\t\timg = np.ascontiguousarray(img, dtype=img.dtype).reshape(1, *img.shape)\n\n\telif img.ndim == 2:\n\t\timg = img.reshape(*img.shape, 1)\n\n\treturn img\n\n\ndef showImage(img, filename, rollMaps=True):\n\tif img.ndim == 4:\n\t\tif img.shape[0] != 1:\n\t\t\traise VisualError(""Image tensor must be exactly one image"")\n\t\telse:\n\t\t\timg = img[0]\n\n\tnormImg = img\n\n\tif img.dtype == np.float32:\n\t\tnormImg = np.copy(img)\n\t\tnormalizeImageInplace(normImg)\n\n\t\tif normImg.ndim == 3:\n\t\t\tif normImg.shape[0] == 1:\n\t\t\t\tnormImg = normImg.reshape(*normImg.shape[1:])\n\n\t\t\telif rollMaps:\n\t\t\t\tnormImg = np.rollaxis(normImg, 0, 3)\n\n\t\tnormImg = imageToInt(normImg)\n\n\tImage.fromarray(normImg).save(filename)\n\n\ndef showImageBatch(batch, filebase, ext=""png"", rollMaps=True):\n\tif batch.ndim != 4:\n\t\traise VisualError(""Imagebatch tensor must be 4d"")\n\n\tfor i in range(batch.shape[0]):\n\t\timg = batch[i]\n\t\timg = img[0] if img.shape[0] == 1 else img\n\n\t\tshowImage(img, ""%s-%d.%s"" % (filebase, i + 1, ext.replace(""."", """")), rollMaps)\n\n\ndef showImageBatchInFolder(batch, foldername, basename, ext=""png"", rollMaps=True):\n\tif not os.path.isdir(foldername):\n\t\tos.mkdir(foldername)\n\n\tshowImageBatch(batch, os.path.join(foldername, basename), ext, rollMaps)\n\n\ndef showImageBasedFilters(filters, filename, offset=4, normalize=True, cols=16):\n\toutmaps, inmaps, fh, fw = filters.shape\n\tif inmaps != 3:\n\t\traise VisualError(""Filter tensor must have 3 inmaps"")\n\n\trows = int(math.ceil(outmaps / cols))\n\n\twidth = cols * fw + (cols + 1) * offset\n\theight = rows * fh + (rows + 1) * offset\n\n\timage = np.zeros((height, width, 3), dtype=np.uint8)\n\n\thstep = offset + fh\n\twstep = offset + fw\n\n\tfor r in range(rows):\n\t\tfor c in range(cols):\n\t\t\tif r * cols + c >= outmaps:\n\t\t\t\tbreak\n\n\t\t\tf = np.copy(filters[r * cols + c])\n\n\t\t\tif normalize:\n\t\t\t\tnormalizeImageInplace(f)\n\n\t\t\tf = np.rollaxis(imageToInt(f), 0, 3)\n\t\t\timage[offset + r * hstep:offset + r * hstep + fh, offset + c * wstep:offset + c * wstep + fw] = f\n\n\tImage.fromarray(image).save(filename)\n\n\ndef showFilters(filters, filename, offset=4, normalize=True):\n\toutmaps, inmaps, fh, fw = filters.shape\n\n\tif fh == fw == 1:\n\t\tprint(""Aborting showing 1x1 filters in file %s ..."" % filename)\n\t\treturn\n\n\twidth = inmaps * fw + (inmaps + 1) * offset\n\theight = outmaps * fh + (outmaps + 1) * offset\n\n\timage = np.zeros((height, width), dtype=np.uint8)\n\n\thstep = offset + fh\n\twstep = offset + fw\n\n\tfor i in range(outmaps):\n\t\tfor j in range(inmaps):\n\t\t\tf = np.copy(filters[i, j])\n\n\t\t\tif normalize:\n\t\t\t\tnormalizeImageInplace(f)\n\n\t\t\tf = imageToInt(f)\n\t\t\timage[offset + i * hstep:offset + i * hstep + fh, offset + j * wstep:offset + j * wstep + fw] = f\n\n\tImage.fromarray(image).save(filename)\n\n\ndef showChanneledFilters(filters, filename, offset=4, normalize=True):\n\toutmaps, inmaps, ch, fh, fw = filters.shape\n\n\twidth = inmaps * fw + (inmaps + 1) * offset\n\theight = outmaps * fh + (outmaps + 1) * offset\n\n\timage = np.zeros((height, width, ch), dtype=np.uint8)\n\n\thstep = offset + fh\n\twstep = offset + fw\n\n\tfor i in range(outmaps):\n\t\tfor j in range(inmaps):\n\t\t\tf = np.copy(filters[i, j])\n\n\t\t\tif normalize:\n\t\t\t\tnormalizeImageInplace(f)\n\n\t\t\tf = np.moveaxis(imageToInt(f), 0, 2)\n\t\t\timage[offset + i * hstep:offset + i * hstep + fh, offset + j * wstep:offset + j * wstep + fw, :] = f\n\n\tImage.fromarray(image).save(filename)\n\n\ndef normalizeImageInplace(img):\n\timg -= img.min()\n\tif img.max() > 0.0:\n\t\timg /= img.max()\n\n\ndef imageToInt(img):\n\treturn (img * 255.0).astype(np.uint8)\n\n\ndef whiten(batch, epsilon=1e-2, PCA=False):\n\tshape = batch.shape\n\tbatch = batch.reshape(batch.shape[0], -1)\n\n\tmean = np.mean(batch, axis=0)\n\tbatch -= mean[np.newaxis, :]\n\n\tsigma = np.dot(batch.T, batch) / batch.shape[0]\n\tU, S, V = np.linalg.svd(sigma.astype(np.float32))\n\n\tzca = np.dot(U, np.diag(1.0 / np.sqrt(S + epsilon)))\n\tzca = np.dot(zca, V) if not PCA else zca\n\n\treturn np.dot(batch, zca).reshape(shape)\n\n\ndef unittest():\n\tfilters = np.random.randn(16, 16, 16, 16).astype(np.float32)\n\tshowFilters(filters, ""./TestData/testFilters.png"")\n\n\tfilters = np.random.normal(size=(32, 3, 32, 32)).astype(np.float32)\n\tshowImageBasedFilters(filters, ""./TestData/testColorFilters.png"")\n\n\tfilters = np.random.normal(size=(16, 24, 3, 16, 16)).astype(np.float32)\n\tshowChanneledFilters(filters, ""./TestData/testChanneledFilters.png"")\n\n\timg = np.random.normal(size=(3, 32, 32)).astype(np.float32)\n\tshowImage(img, ""./TestData/testImage.png"")\n\n\tbatch = np.random.normal(size=(4, 1, 16, 16)).astype(np.float32)\n\tshowImageBatch(batch, ""./TestData/testBatch"", "".png"")\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
setup.py,0,"b'import os, stat, shutil\nfrom enum import Enum\n\nfrom setuptools import setup, find_packages\nfrom setuptools.command.install import install\nfrom setuptools.command.sdist import sdist\n\n\nlibname = ""PuzzleLib""\nversion = ""1.0.1""\n\n\nclass Options(str, Enum):\n\tcuda = ""cuda""\n\thip = ""hip""\n\tintel = ""intel""\n\ttensorrt = ""tensorrt""\n\topenvino = ""openvino""\n\n\ndef removeReadOnly(_, name, __):\n\tos.chmod(name, stat.S_IWRITE)\n\tos.remove(name)\n\n\ndef markPackages(path):\n\tfor dirpath, dirnames, filenames in os.walk(path):\n\t\tif ""__init__.py"" not in filenames:\n\t\t\tinit(dirpath)\n\n\ndef init(dirpath):\n\tinitfile = os.path.join(dirpath, ""__init__.py"")\n\n\twith open(initfile, ""w"", encoding=""utf-8""):\n\t\tpass\n\n\treturn initfile\n\n\ndef pathToPackageName(path, withLibname=True):\n\ti = 1 if withLibname else 0\n\n\tpackage = path.split(os.path.sep)\n\tidx = list(reversed(package)).index(libname)\n\n\treturn ""."".join(package[-idx - i:])\n\n\nclass InstallCommand(install):\n\tdescription = ""command for installation with the ability to select the desired backends and converters""\n\n\tuser_options = install.user_options + [\n\t\t(\n\t\t\t""backend="", None,\n\t\t\t""desired backend. Possible entries: cuda, hip, intel; through comma if several""\n\t\t),\n\t\t(\n\t\t\t""converter="", None,\n\t\t\t""desired converter which will be included. Possible entries: tensorrt, openvino; through comma if both""\n\t\t)\n\t]\n\n\n\tbackend, converter = """", """"\n\n\tprojectPath = os.path.dirname(os.path.abspath(__file__))\n\tcachePath = os.path.join(projectPath, libname)\n\n\n\tdef run(self):\n\t\tbackends = self.backend.split("","") if len(self.backend) > 0 else []\n\t\tconverters = self.converter.split("","") if len(self.converter) > 0 else []\n\n\t\tprint(""backends chosen: %s"" % backends)\n\t\tprint(""converters chosen: %s"" % converters)\n\n\t\toptions = set()\n\t\tfor option in backends + converters:\n\t\t\ttry:\n\t\t\t\toption = Options[option]\n\t\t\texcept KeyError:\n\t\t\t\traise ValueError(""Invalid option: %s"" % option)\n\n\t\t\toptions.add(option)\n\n\t\thandlers = [\n\t\t\t(""Compiler"", self.installCompilerPackage),\n\n\t\t\t(""CPU"", self.installPythonPackage),\n\t\t\t(""Intel"", self.installIntelPackage),\n\t\t\t((""Cuda"", ""Hip""), self.installGpuPackages),\n\t\t\t(""Converter"", self.installConverterPackage),\n\n\t\t\t(""Backend"", self.installPythonPackage),\n\t\t\t(""Modules"", self.installPythonPackage),\n\t\t\t(""Containers"", self.installPythonPackage),\n\t\t\t(""Cost"", self.installPythonPackage),\n\t\t\t(""Optimizers"", self.installPythonPackage),\n\t\t\t(""Handlers"", self.installPythonPackage),\n\t\t\t(""Passes"", self.installPythonPackage),\n\t\t\t(""Models"", self.installPythonPackage),\n\n\t\t\t(""Datasets"", self.installPythonPackage),\n\t\t\t(""Transformers"", self.installPythonPackage),\n\n\t\t\t(""TestData"", self.installDataPackage),\n\t\t\t(""TestLib"", self.installPythonPackage)\n\t\t]\n\n\t\tos.mkdir(self.cachePath)\n\n\t\ttry:\n\t\t\tself.distribution.package_data = self.installPackages(self.projectPath, self.cachePath, handlers, options)\n\t\t\tmarkPackages(self.cachePath)\n\n\t\t\tself.distribution.packages = [libname] + [\n\t\t\t\t""%s."" % libname + pkg for pkg in find_packages(where=self.cachePath)\n\t\t\t]\n\t\t\tsuper().run()\n\n\t\tfinally:\n\t\t\tshutil.rmtree(self.cachePath, onerror=removeReadOnly)\n\n\n\t@staticmethod\n\tdef installPackages(src, dst, handlers, options):\n\t\tpackageData = {}\n\n\t\tif not os.path.exists(dst):\n\t\t\tos.mkdir(dst)\n\n\t\tfor file in os.listdir(src):\n\t\t\tif file.endswith("".py"") and os.path.abspath(os.path.join(src, file)) != __file__:\n\t\t\t\tshutil.copy(os.path.join(src, file), os.path.join(dst, file))\n\n\t\tfor name, handler in handlers:\n\t\t\tif isinstance(name, str):\n\t\t\t\tpkgSrc, pkgDst = os.path.join(src, name), os.path.join(dst, name)\n\t\t\telse:\n\t\t\t\tpkgSrc, pkgDst = [os.path.join(src, nm) for nm in name], [os.path.join(dst, nm) for nm in name]\n\n\t\t\tpackageData.update(handler(pkgSrc, pkgDst, options))\n\n\t\treturn packageData\n\n\n\t@staticmethod\n\tdef installPythonPackage(src, dst, _):\n\t\tdef ignore(s, names):\n\t\t\tfiles = {name for name in names if not os.path.isdir(os.path.join(s, name)) and not name.endswith("".py"")}\n\t\t\tfiles.add(""__pycache__"")\n\n\t\t\treturn files\n\n\t\tshutil.copytree(src, dst, ignore=ignore)\n\t\treturn {}\n\n\n\t@staticmethod\n\tdef installCompilerPackage(src, dst, _):\n\t\tshutil.copytree(src, dst, ignore=lambda s, names: {""__pycache__"", ""TestData""})\n\t\tos.mkdir(os.path.join(dst, ""TestData""))\n\n\t\tdata = {}\n\n\t\tfor dirpath, dirnames, filenames in os.walk(dst):\n\t\t\tbuildFiles = [file for file in filenames if any(file.endswith(ext) for ext in ["".c"", "".h""])]\n\t\t\tif len(buildFiles) > 0:\n\t\t\t\tdata[pathToPackageName(dirpath)] = buildFiles\n\n\t\treturn data\n\n\n\tdef installGpuPackages(self, src, dst, options):\n\t\tdata = {}\n\n\t\tcudaSrc, hipSrc = src\n\t\tcudaDst, hipDst = dst\n\n\t\tcuda, hip = Options.cuda in options, Options.hip in options\n\n\t\tif cuda or hip:\n\t\t\tshutil.copytree(cudaSrc, cudaDst, ignore=lambda s, names: {""__pycache__"", "".gitignore""})\n\n\t\tif cuda:\n\t\t\tfrom PuzzleLib.Cuda.CheckInstall import checkCudaInstall\n\t\t\tfrom PuzzleLib.Cuda.Source.Build import main as buildDriver\n\n\t\t\tdata.update(self.installGpuPackage(""Cuda"", checkCudaInstall, buildDriver, cudaDst))\n\n\t\tif hip:\n\t\t\tshutil.copytree(hipSrc, hipDst, ignore=lambda s, names: {""__pycache__"", "".gitignore""})\n\n\t\t\tfrom PuzzleLib.Hip.CheckInstall import main as checkHipInstall\n\t\t\tfrom PuzzleLib.Hip.Source.Build import main as buildDriver\n\n\t\t\tdata.update(self.installGpuPackage(""Hip"", checkHipInstall, buildDriver, hipDst))\n\n\t\tif cuda or hip:\n\t\t\tshutil.rmtree(os.path.join(cudaDst, ""Source""), onerror=removeReadOnly)\n\n\t\tif hip:\n\t\t\tshutil.rmtree(os.path.join(hipDst, ""Source""), onerror=removeReadOnly)\n\n\t\treturn data\n\n\n\t@staticmethod\n\tdef installGpuPackage(name, checkInstall, buildDriver, dst):\n\t\tprint(""\\nChecking if all dependencies for %s are satisfied ..."" % name)\n\t\tcheckInstall(withPip=False)\n\n\t\tcwd = os.getcwd()\n\t\ttry:\n\t\t\tprint(""\\nBuilding %s driver ..."" % name)\n\n\t\t\tos.chdir(os.path.join(dst, ""Source""))\n\t\t\tdriver = os.path.abspath(buildDriver())\n\n\t\tfinally:\n\t\t\tos.chdir(cwd)\n\n\t\treturn {pathToPackageName(dst): [os.path.basename(driver)]}\n\n\n\t@staticmethod\n\tdef installIntelPackage(src, dst, options):\n\t\tif Options.intel not in options:\n\t\t\treturn {}\n\n\t\tshutil.copytree(src, dst, ignore=lambda s, names: {""__pycache__"", "".gitignore""})\n\t\tdata = {}\n\n\t\tfrom PuzzleLib.Intel.ThirdParty.finddnnl import findDNNL\n\n\t\tprint(""\\nChecking dnnl installation ..."")\n\t\tlib = findDNNL()\n\n\t\tif os.path.commonpath([dst, lib]) == dst:\n\t\t\tdata = {pathToPackageName(dst): [lib]}\n\n\t\treturn data\n\n\n\tdef installConverterPackage(self, src, dst, options):\n\t\thandlers = [\n\t\t\t(""Caffe"", self.installPythonPackage),\n\t\t\t(""MXNet"", self.installPythonPackage),\n\n\t\t\t(""Examples"", self.installPythonPackage),\n\t\t\t(""ONNX"", self.installPythonPackage),\n\n\t\t\t(""TensorRT"", self.installTensorRTPackage),\n\t\t\t(""OpenVINO"", self.installOpenVINOPackage)\n\t\t]\n\n\t\tdata = self.installPackages(src, dst, handlers, options)\n\t\tos.mkdir(os.path.join(dst, ""TestData""))\n\n\t\treturn data\n\n\n\tdef installTensorRTPackage(self, src, dst, options):\n\t\tif Options.tensorrt not in options:\n\t\t\treturn {}\n\n\t\tos.mkdir(dst)\n\t\tshutil.copytree(os.path.join(src, ""Source""), os.path.join(dst, ""Source""))\n\n\t\tfrom PuzzleLib.Converter.TensorRT.Source.Build import main as buildDriver\n\t\treturn self.installInferenceEnginePackage(""TensorRT"", buildDriver, src, dst)\n\n\n\tdef installOpenVINOPackage(self, src, dst, options):\n\t\tif Options.openvino not in options:\n\t\t\treturn {}\n\n\t\tos.mkdir(dst)\n\t\tshutil.copytree(os.path.join(src, ""Source""), os.path.join(dst, ""Source""))\n\n\t\tfrom PuzzleLib.Converter.OpenVINO.Source.Build import main as buildDriver\n\t\treturn self.installInferenceEnginePackage(""OpenVINO"", buildDriver, src, dst)\n\n\n\t@staticmethod\n\tdef installInferenceEnginePackage(name, buildDriver, src, dst):\n\t\tfor file in os.listdir(src):\n\t\t\tif file.endswith("".py""):\n\t\t\t\tshutil.copy(os.path.join(src, file), os.path.join(dst, file))\n\n\t\tshutil.copytree(os.path.join(src, ""Tests""), os.path.join(dst, ""Tests""))\n\t\tos.mkdir(os.path.join(dst, ""TestData""))\n\n\t\tcwd = os.getcwd()\n\t\ttry:\n\t\t\tprint(""\\nBuilding %s driver ..."" % name)\n\n\t\t\tsourcePath = os.path.join(dst, ""Source"")\n\t\t\tos.chdir(sourcePath)\n\n\t\t\tdriver = buildDriver()\n\n\t\tfinally:\n\t\t\tos.chdir(cwd)\n\n\t\tshutil.rmtree(sourcePath, onerror=removeReadOnly)\n\t\treturn {pathToPackageName(dst): [os.path.basename(driver)]}\n\n\n\t@staticmethod\n\tdef installDataPackage(src, dst, _):\n\t\tos.mkdir(dst)\n\n\t\tdata = [""test.tar"", ""test.zip""]\n\t\tfor file in data:\n\t\t\tshutil.copy(os.path.join(src, file), os.path.join(dst, file))\n\n\t\treturn {pathToPackageName(dst): data}\n\n\nclass SdistCommand(sdist):\n\tprojectPath = os.path.dirname(os.path.abspath(__file__))\n\n\n\tdef run(self):\n\t\tinitfiles = []\n\n\t\thandlers = [\n\t\t\t(""Compiler"", self.distributeCompilerPackage),\n\n\t\t\t(""CPU"", self.distributePythonPackage),\n\t\t\t(""Intel"", self.distributePythonPackage),\n\t\t\t(""Hip"", self.distributeGpuPackage),\n\t\t\t(""Cuda"", self.distributeGpuPackage),\n\t\t\t(""Converter"", self.distributeConverterPackage),\n\n\t\t\t(""Backend"", self.distributePythonPackage),\n\t\t\t(""Modules"", self.distributePythonPackage),\n\t\t\t(""Containers"", self.distributePythonPackage),\n\t\t\t(""Cost"", self.distributePythonPackage),\n\t\t\t(""Optimizers"", self.distributePythonPackage),\n\t\t\t(""Handlers"", self.distributePythonPackage),\n\t\t\t(""Passes"", self.distributePythonPackage),\n\t\t\t(""Models"", self.distributePythonPackage),\n\n\t\t\t(""Datasets"", self.distributePythonPackage),\n\t\t\t(""Transformers"", self.distributePythonPackage),\n\n\t\t\t(""TestData"", self.distributeDataPackage),\n\t\t\t(""TestLib"", self.distributePythonPackage)\n\t\t]\n\n\t\ttry:\n\t\t\tinitfiles, data = self.distributePackages(self.projectPath, handlers)\n\n\t\t\tself.distribution.package_data = data\n\t\t\tself.distribution.packages = find_packages(where=self.projectPath)\n\n\t\t\tsuper().run()\n\n\t\tfinally:\n\t\t\tfor initfile in initfiles:\n\t\t\t\tos.unlink(initfile)\n\n\n\t@staticmethod\n\tdef distributePackage(path, exclude, includeExts):\n\t\tinitfiles = []\n\t\tdata = {}\n\n\t\tfor dirpath, dirnames, filenames in os.walk(path):\n\t\t\tfor exdir in exclude:\n\t\t\t\tif exdir in dirnames:\n\t\t\t\t\tdirnames.remove(exdir)\n\n\t\t\tincludeFiles = [file for file in filenames if any(file.endswith(ext) for ext in includeExts)]\n\t\t\tif len(includeFiles) > 0:\n\t\t\t\tdata[pathToPackageName(dirpath, withLibname=False)] = includeFiles\n\n\t\t\tif ""__init__.py"" not in filenames:\n\t\t\t\tinitfiles.append(init(dirpath))\n\n\t\treturn initfiles, data\n\n\n\t@staticmethod\n\tdef distributePackages(path, handlers):\n\t\tinitfiles = []\n\t\tdata = {}\n\n\t\tfor name, handler in handlers:\n\t\t\tpkgInitfiles, pkgData = handler(os.path.join(path, name))\n\n\t\t\tdata.update(pkgData)\n\t\t\tinitfiles.extend(pkgInitfiles)\n\n\t\treturn initfiles, data\n\n\n\tdef distributePythonPackage(self, path):\n\t\treturn self.distributePackage(path=path, exclude=[""__pycache__""], includeExts=[])\n\n\n\tdef distributeCompilerPackage(self, path):\n\t\treturn self.distributePackage(path=path, exclude=[""__pycache__""], includeExts=["".c"", "".h""])\n\n\n\tdef distributeGpuPackage(self, path):\n\t\treturn self.distributeCompilerPackage(path)\n\n\n\tdef distributeConverterPackage(self, path):\n\t\thandlers = [\n\t\t\t(""Caffe"", self.distributePythonPackage),\n\t\t\t(""MXNet"", self.distributePythonPackage),\n\n\t\t\t(""Examples"", self.distributePythonPackage),\n\t\t\t(""ONNX"", self.distributePythonPackage),\n\n\t\t\t(""TensorRT"", self.distributeTensorRTPackage),\n\t\t\t(""OpenVINO"", self.distributeOpenVINOPackage)\n\t\t]\n\n\t\tinitfiles, data = self.distributePackages(path, handlers)\n\t\tinitfiles.append(init(path))\n\n\t\treturn initfiles, data\n\n\n\tdef distributeTensorRTPackage(self, path):\n\t\treturn self.distributePackage(path=path, exclude=[""__pycache__""], includeExts=["".cpp"", "".h"", "".cu""])\n\n\n\tdef distributeOpenVINOPackage(self, path):\n\t\treturn self.distributePackage(path=path, exclude=[""__pycache__""], includeExts=["".cpp""])\n\n\n\tdef distributeDataPackage(self, path):\n\t\tdata = [""test.tar"", ""test.zip""]\n\n\t\tinits, pkgData = self.distributePythonPackage(path)\n\t\tpkgData.update({pathToPackageName(path, withLibname=False): data})\n\n\t\treturn inits, pkgData\n\n\ndef main():\n\tsetup(\n\t\tname=libname,\n\t\tversion=version,\n\t\tcmdclass={\n\t\t\t""install"": InstallCommand,\n\t\t\t""sdist"": SdistCommand\n\t\t},\n\t\turl=""https://puzzlelib.org"",\n\t\tdownload_url=""https://github.com/puzzlelib/PuzzleLib/tags"",\n\t\tauthor=""Ashmanov Neural Networks"",\n\t\tpython_requires="">=3.5"",\n\t\tlicense=""Apache-2.0"",\n\t\tkeywords=[""puzzlelib"", ""deep learning"", ""neural nets""]\n\t)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Backend/Benchmarks.py,0,"b'from PuzzleLib import Config\n\n\ntimeKernel = None\n\n\ndef autoinit():\n\tif not Config.shouldInit():\n\t\treturn\n\n\tif Config.backend == Config.Backend.cuda:\n\t\tinitCuda()\n\telif Config.backend == Config.Backend.hip:\n\t\tinitHip()\n\telif Config.isCPUBased(Config.backend):\n\t\tinitCPU()\n\telse:\n\t\traise Config.ConfigError(Config.backend)\n\n\ndef initCuda():\n\tfrom PuzzleLib.Cuda import Backend\n\tinitGPU(Backend)\n\n\ndef initHip():\n\tfrom PuzzleLib.Hip import Backend\n\tinitGPU(Backend)\n\n\ndef initGPU(Backend):\n\tbackend = Backend.getBackend(Config.deviceIdx, initmode=0)\n\n\tglobal timeKernel\n\ttimeKernel = backend.timeKernel\n\n\ndef initCPU():\n\tfrom PuzzleLib.CPU.Benchmarks import Utils\n\n\tglobal timeKernel\n\ttimeKernel = Utils.timeKernel\n\n\nautoinit()\n'"
Backend/Blas.py,0,"b'from PuzzleLib import Config\n\n\ntoVectorAddVector = None\naddVectorToVector = None\ndot = None\nvectorL1Norm = None\n\nmulMatrixOnMatrix = None\nsumOnMatrix = None\n\n\ndef autoinit():\n\tif not Config.shouldInit():\n\t\treturn\n\n\tif Config.backend == Config.Backend.cuda:\n\t\tinitCuda()\n\telif Config.backend == Config.Backend.hip:\n\t\tinitHip()\n\telif Config.backend == Config.Backend.cpu:\n\t\tinitCPU()\n\telif Config.backend == Config.Backend.intel:\n\t\tinitIntel()\n\telse:\n\t\traise Config.ConfigError(Config.backend)\n\n\ndef initCuda():\n\tfrom PuzzleLib.Cuda import Backend\n\tinitGPU(Backend)\n\n\ndef initHip():\n\tfrom PuzzleLib.Hip import Backend\n\tinitGPU(Backend)\n\n\ndef initGPU(Backend):\n\tbackend = Backend.getBackend(Config.deviceIdx, initmode=2)\n\tGPUArray, memoryPool, blas, matmod = backend.GPUArray, backend.memoryPool, backend.blas, backend.matmod\n\n\tdef wrapToVectorAddVector(y, x, alpha=1.0):\n\t\tbackend.toVectorAddVectorKer(y.dtype)(y, x, alpha)\n\t\treturn y\n\n\tdef wrapAddVectorToVector(x, y, out=None, alpha=1.0, beta=1.0):\n\t\tif out is None:\n\t\t\tout = GPUArray.empty(x.shape, dtype=x.dtype, allocator=memoryPool)\n\t\telse:\n\t\t\tassert out.shape == x.shape\n\n\t\tbackend.addKer(out.dtype)(out, x, alpha, y, beta)\n\t\treturn out\n\n\tdef wrapGemm(A, B, out=None, transpA=False, transpB=False, alpha=1.0, beta=0.0):\n\t\treturn blas.gemm(A, B, out, transpA, transpB, alpha, beta, memoryPool)\n\n\tdef wrapSumOnMatrix(A, out=None, cols=True, alpha=1.0, beta=0.0):\n\t\tassert A.ndim == 2\n\t\treturn matmod.matsum(A, 0 if cols else 1, out, alpha, beta, memoryPool)\n\n\tglobal toVectorAddVector, addVectorToVector, dot, vectorL1Norm\n\ttoVectorAddVector = wrapToVectorAddVector\n\taddVectorToVector = wrapAddVectorToVector\n\tdot = blas.dot\n\tvectorL1Norm = blas.l1norm\n\n\tglobal mulMatrixOnMatrix, sumOnMatrix\n\tmulMatrixOnMatrix = wrapGemm\n\tsumOnMatrix = wrapSumOnMatrix\n\n\ndef initCPU():\n\tfrom PuzzleLib.CPU.Wrappers import NumpyBlas\n\n\tglobal toVectorAddVector, addVectorToVector, dot, vectorL1Norm\n\ttoVectorAddVector = NumpyBlas.toVectorAddVector\n\taddVectorToVector = NumpyBlas.addVectorToVector\n\tdot = NumpyBlas.dot\n\tvectorL1Norm = NumpyBlas.vectorL1Norm\n\n\tglobal mulMatrixOnMatrix, sumOnMatrix\n\tmulMatrixOnMatrix = NumpyBlas.mulMatrixOnMatrix\n\tsumOnMatrix = NumpyBlas.sumOnMatrix\n\n\ndef initIntel():\n\tinitCPU()\n\n\tfrom PuzzleLib.Intel.Wrappers import DNNLBlas\n\n\tglobal mulMatrixOnMatrix\n\tmulMatrixOnMatrix = DNNLBlas.mulMatrixOnMatrix\n\n\nautoinit()\n'"
Backend/BlasGroup.py,0,"b'from PuzzleLib import Config\n\n\nmulTensorOnVecGroup = None\nsumOnTensorGroup = None\nmulTensorBatch = None\n\n\ndef autoinit():\n\tif not Config.shouldInit():\n\t\treturn\n\n\tif Config.backend == Config.Backend.cuda:\n\t\tinitCuda()\n\telif Config.backend == Config.Backend.hip:\n\t\tinitHip()\n\telif Config.isCPUBased(Config.backend):\n\t\tinitCPU()\n\telse:\n\t\traise Config.ConfigError(Config.backend)\n\n\ndef initCuda():\n\tfrom PuzzleLib.Cuda import Backend\n\tinitGPU(Backend)\n\n\ndef initHip():\n\tfrom PuzzleLib.Hip import Backend\n\tinitGPU(Backend)\n\n\ndef initGPU(Backend):\n\tbackend = Backend.getBackend(Config.deviceIdx, initmode=2)\n\tmemoryPool, blas, matmod = backend.memoryPool, backend.blas, backend.matmod\n\n\tformats = {\n\t\t""gbp"": backend.GroupFormat.gbp.value,\n\t\t""bgp"": backend.GroupFormat.bgp.value\n\t}\n\n\tdef wrapMulTensorOnVecGroup(tensor, vecs, out=None, formatT=""bgp"", transpT=False, alpha=1.0, beta=0.0):\n\t\tassert tensor.ndim == 3 and formatT == ""gbp""\n\t\taxis = 0 if transpT else 1\n\n\t\treturn matmod.matvec(tensor, vecs, axis, out, alpha, beta, memoryPool)\n\n\tdef wrapSumOnTensorGroup(tensor, out=None, formatT=""bgp"", cols=True, alpha=1.0, beta=0.0):\n\t\tassert tensor.ndim == 3\n\t\taxis = (1 if formatT == ""gbp"" else 0) if cols else 2\n\n\t\treturn matmod.matsum(tensor, axis, out, alpha, beta, memoryPool)\n\n\tdef wrapMulTensorBatch(A, B, formatA=""bgp"", formatB=""bgp"", out=None, formatOut=""bgp"", transpA=False, transpB=False,\n\t\t\t\t\t\t   alpha=1.0, beta=0.0):\n\t\tformatA, formatB, formatOut = formats[formatA], formats[formatB], formats[formatOut]\n\t\treturn blas.gemmBatched(A, B, formatA, formatB, formatOut, transpA, transpB, alpha, beta, out, memoryPool)\n\n\tglobal mulTensorOnVecGroup, sumOnTensorGroup, mulTensorBatch\n\tmulTensorOnVecGroup = wrapMulTensorOnVecGroup\n\tsumOnTensorGroup = wrapSumOnTensorGroup\n\tmulTensorBatch = wrapMulTensorBatch\n\n\ndef initCPU():\n\tpass\n\n\nautoinit()\n'"
Backend/Memory.py,3,"b'from PuzzleLib import Config\n\n\ndepthConcat = None\ndepthSplit = None\n\nmoveaxis = None\nswapaxes = None\ntranspose = None\n\n\ndef autoinit():\n\tif not Config.shouldInit():\n\t\treturn\n\n\tif Config.backend == Config.Backend.cuda:\n\t\tinitCuda()\n\telif Config.backend == Config.Backend.hip:\n\t\tinitHip()\n\telif Config.isCPUBased(Config.backend):\n\t\tinitCPU()\n\telse:\n\t\traise Config.ConfigError(Config.backend)\n\n\ndef initCuda():\n\tfrom PuzzleLib.Cuda.Backend import getBackend\n\n\tbackend = getBackend(Config.deviceIdx, initmode=1)\n\tmemoryPool, dnn = backend.memoryPool, backend.dnn\n\n\tinitGPU(memoryPool, dnn)\n\n\ndef initHip():\n\tfrom PuzzleLib.Hip.Backend import getBackend\n\n\tbackend = getBackend(Config.deviceIdx, initmode=2)\n\tmemoryPool, memmod = backend.memoryPool, backend.memmod\n\n\tinitGPU(memoryPool, memmod)\n\n\ndef initGPU(memoryPool, module):\n\tdef wrapDepthConcat(data):\n\t\treturn module.depthConcat(data, allocator=memoryPool)\n\n\tdef wrapDepthSplit(grad, indata):\n\t\treturn module.depthSplit(grad, indata, allocator=memoryPool)\n\n\tglobal depthConcat, depthSplit\n\tdepthConcat = wrapDepthConcat\n\tdepthSplit = wrapDepthSplit\n\n\tdef wrapMoveaxis(data, src, dst):\n\t\treturn module.moveaxis(data, src, dst, allocator=memoryPool)\n\n\tdef wrapSwapaxes(data, axis1, axis2):\n\t\treturn module.swapaxes(data, axis1, axis2, allocator=memoryPool)\n\n\tdef wrapTranspose(data, axes):\n\t\treturn module.transpose(data, tuple(axes), allocator=memoryPool)\n\n\tglobal moveaxis, swapaxes, transpose\n\tmoveaxis = wrapMoveaxis\n\tswapaxes = wrapSwapaxes\n\ttranspose = wrapTranspose\n\n\ndef initCPU():\n\timport numpy as np\n\tfrom PuzzleLib.CPU.CPUArray import CPUArray\n\n\tdef wrapMoveAxis(a, src, dst):\n\t\tout = np.copy(np.moveaxis(a.get(copy=False), src, dst), order=""C"")\n\t\treturn CPUArray(out.shape, out.dtype, data=out, acquire=True)\n\n\tdef wrapSwapAxes(a, axis1, axis2):\n\t\tout = np.copy(np.swapaxes(a.get(copy=False), axis1, axis2), order=""C"")\n\t\treturn CPUArray(out.shape, out.dtype, data=out, acquire=True)\n\n\tdef wrapTranspose(a, axes):\n\t\tout = np.copy(np.transpose(a.get(copy=False), axes), order=""C"")\n\t\treturn CPUArray(out.shape, out.dtype, data=out, acquire=True)\n\n\tglobal moveaxis, swapaxes, transpose\n\tmoveaxis = wrapMoveAxis\n\tswapaxes = wrapSwapAxes\n\ttranspose = wrapTranspose\n\n\nautoinit()\n'"
Backend/Utils.py,7,"b'from PuzzleLib import Config\n\n\nbackend = None\n\nSharedArray = None\nmemoryPool = None\n\nstreamManager = None\nglobalRng = None\n\ncopy = None\nconcatenate = None\nsplit = None\ntile = None\n\nfillUniform = None\nfillNormal = None\n\nsetupDebugAllocator = None\ndtypesSupported = None\n\n\ndef autoinit():\n\tif not Config.shouldInit():\n\t\treturn\n\n\tif Config.backend == Config.Backend.cuda:\n\t\tinitCuda()\n\telif Config.backend == Config.Backend.hip:\n\t\tinitHip()\n\telif Config.isCPUBased(Config.backend):\n\t\tinitCPU()\n\telse:\n\t\traise Config.ConfigError(Config.backend)\n\n\ndef initCuda():\n\tfrom PuzzleLib.Cuda import Backend, Utils\n\tinitGPU(Backend, Utils)\n\n\ndef initHip():\n\tfrom PuzzleLib.Cuda import Utils\n\tfrom PuzzleLib.Hip import Backend\n\tinitGPU(Backend, Utils)\n\n\ndef initGPU(Backend, Utils):\n\tglobal backend\n\tbackend = Backend.getBackend(Config.deviceIdx, initmode=0)\n\n\tglobal SharedArray, memoryPool, streamManager, globalRng\n\tSharedArray = backend.SharedArray\n\tmemoryPool = backend.memoryPool\n\n\tstreamManager = backend.streamManager\n\tglobalRng = backend.globalRng\n\n\tdef wrapCopy(dest, source):\n\t\treturn backend.copy(dest, source, allocator=memoryPool)\n\n\tdef wrapConcatenate(tup, axis, out=None):\n\t\treturn backend.concatenate(tup, axis, out, allocator=memoryPool)\n\n\tdef wrapSplit(ary, sections, axis):\n\t\treturn backend.split(ary, sections, axis, allocator=memoryPool)\n\n\tdef wrapTile(ary, times, axis):\n\t\treturn backend.tile(ary, times, axis, allocator=memoryPool)\n\n\tglobal copy, concatenate, split, tile\n\tcopy = wrapCopy\n\tconcatenate = wrapConcatenate\n\tsplit = wrapSplit\n\ttile = wrapTile\n\n\tdef wrapFillUniform(data, minval, maxval, rng):\n\t\tbackend.fillUniform(data, minval, maxval, rng)\n\n\tdef wrapFillNormal(data, mean, stddev, rng):\n\t\tbackend.fillNormal(data, mean, stddev, rng)\n\n\tglobal fillUniform, fillNormal\n\tfillUniform = wrapFillUniform\n\tfillNormal = wrapFillNormal\n\n\tglobal setupDebugAllocator, dtypesSupported\n\tsetupDebugAllocator = lambda: Utils.setupDebugAllocator(backend.GPUArray)\n\tdtypesSupported = backend.dtypesSupported\n\n\ndef initCPU():\n\timport numpy as np\n\n\tfrom PuzzleLib.CPU.CPUArray import CPUArray\n\tfrom PuzzleLib.CPU import Utils\n\n\tclass ProxyMemoryPool:\n\t\tdef freeHeld(self):\n\t\t\tpass\n\n\tclass ProxyRNG:\n\t\t@staticmethod\n\t\tdef fillUniform(data, minval=0.0, maxval=1.0):\n\t\t\tdata.set(np.random.uniform(minval, maxval, size=data.shape))\n\n\t\t@staticmethod\n\t\tdef fillNormal(data, mean=0.0, sigma=1.0):\n\t\t\tdata.set(np.random.normal(mean, sigma, size=data.shape))\n\n\t\t@staticmethod\n\t\tdef fillInteger(data):\n\t\t\tdata.set(np.random.randint(np.iinfo(data.dtype).min, np.iinfo(data.dtype).max, dtype=data.dtype))\n\n\tglobal SharedArray, memoryPool, globalRng\n\tSharedArray = Utils.SharedArray\n\tmemoryPool = ProxyMemoryPool()\n\tglobalRng = ProxyRNG()\n\n\tdef wrapCopy(dest, source):\n\t\tif dest is None:\n\t\t\tdest = CPUArray.empty(source.shape, source.dtype)\n\n\t\tnp.copyto(dest.data, source.data)\n\t\treturn dest\n\n\tdef wrapConcatenate(tup, axis, out=None):\n\t\tout = np.concatenate(tuple(ary.data for ary in tup), axis=axis, out=None if out is None else out.data)\n\t\treturn CPUArray(out.shape, out.dtype, data=out, acquire=True)\n\n\tdef wrapSplit(ary, sections, axis):\n\t\touts = (out.copy() for out in np.split(ary.data, np.cumsum(sections)[:-1], axis))\n\t\treturn [CPUArray(out.shape, out.dtype, data=out, acquire=True) for out in outs]\n\n\tdef wrapTile(ary, times, axis):\n\t\tshape = (times, )\n\t\tif axis > 0:\n\t\t\tshape = (1, ) * axis + shape\n\t\tif axis < ary.ndim - 1:\n\t\t\tshape = shape + (1, ) * (ary.ndim - 1 - axis)\n\n\t\tout = np.tile(ary.data, shape)\n\t\treturn CPUArray(out.shape, out.dtype, data=out, acquire=True)\n\n\tglobal copy, concatenate, split, tile\n\tcopy = wrapCopy\n\tconcatenate = wrapConcatenate\n\tsplit = wrapSplit\n\ttile = wrapTile\n\n\tdef wrapFillUniform(data, minval, maxval, rng):\n\t\trng.fillUniform(data, minval, maxval)\n\n\tdef wrapFillNormal(data, mean, sigma, rng):\n\t\trng.fillNormal(data, mean, sigma)\n\n\tglobal fillUniform, fillNormal\n\tfillUniform = wrapFillUniform\n\tfillNormal = wrapFillNormal\n\n\tglobal setupDebugAllocator, dtypesSupported\n\tsetupDebugAllocator = lambda: None\n\tdtypesSupported = Utils.dtypesSupported\n\n\nautoinit()\n'"
Backend/gpuarray.py,0,"b'from PuzzleLib import Config\n\n\nGPUArray = None\n\nto_gpu = None\nempty = None\nzeros = None\n\nminimum = None\nmaximum = None\n\n\ndef autoinit():\n\tif not Config.shouldInit():\n\t\treturn\n\n\tif Config.backend == Config.Backend.cuda:\n\t\tinitCuda()\n\telif Config.backend == Config.Backend.hip:\n\t\tinitHip()\n\telif Config.isCPUBased(Config.backend):\n\t\tinitCPU()\n\telse:\n\t\traise Config.ConfigError(Config.backend)\n\n\ndef initCuda():\n\tfrom PuzzleLib.Cuda import Backend\n\tinitGPU(Backend)\n\n\ndef initHip():\n\tfrom PuzzleLib.Hip import Backend\n\tinitGPU(Backend)\n\n\ndef initGPU(Backend):\n\tbackend = Backend.getBackend(Config.deviceIdx, initmode=0)\n\n\tglobal GPUArray, to_gpu, empty, zeros\n\tGPUArray = backend.GPUArray\n\tto_gpu = backend.GPUArray.toGpu\n\tempty = backend.GPUArray.empty\n\tzeros = backend.GPUArray.zeros\n\n\tglobal minimum, maximum\n\tminimum = backend.GPUArray.min\n\tmaximum = backend.GPUArray.max\n\n\ndef initCPU():\n\tfrom PuzzleLib.CPU.CPUArray import CPUArray\n\n\tglobal GPUArray, to_gpu, empty, zeros\n\tGPUArray = CPUArray\n\tto_gpu = CPUArray.toDevice\n\tempty = CPUArray.empty\n\tzeros = CPUArray.zeros\n\n\tglobal minimum, maximum\n\tminimum = CPUArray.minimum\n\tmaximum = CPUArray.maximum\n\n\nautoinit()\n'"
CPU/CPUArray.py,13,"b'import numpy as np\n\n\nclass CPUArray:\n\tdef __init__(self, shape, dtype, data=None, acquire=False):\n\t\tif isinstance(shape, int):\n\t\t\tshape = (shape, )\n\n\t\tself.shape = tuple(shape)\n\t\tself.dtype = dtype if isinstance(dtype, np.dtype) else np.dtype(dtype)\n\n\t\tif data is not None:\n\t\t\tassert shape == data.shape and dtype == data.dtype\n\n\t\t\tif not isinstance(data, np.ndarray):\n\t\t\t\tdata = np.array(data)\n\n\t\t\tself.data = data if acquire else np.ascontiguousarray(np.copy(data))\n\n\t\telse:\n\t\t\tself.data = np.empty(shape, dtype=dtype)\n\n\n\t@property\n\tdef flags(self):\n\t\treturn self.data.flags\n\n\n\t@property\n\tdef strides(self):\n\t\treturn self.data.strides\n\n\n\t@property\n\tdef size(self):\n\t\treturn self.data.size\n\n\n\t@property\n\tdef ndim(self):\n\t\treturn self.data.ndim\n\n\n\t@property\n\tdef nbytes(self):\n\t\treturn self.data.nbytes\n\n\n\t@property\n\tdef ptr(self):\n\t\treturn self.data.__array_interface__[""data""][0]\n\n\n\tdef get(self, copy=True):\n\t\treturn np.copy(self.data) if copy else self.data\n\n\n\tdef set(self, ary):\n\t\tif isinstance(ary, CPUArray):\n\t\t\tary = ary.data\n\n\t\tself.data[...] = ary\n\n\n\tdef fill(self, value):\n\t\tself.data[...] = value\n\n\n\tdef reshape(self, *args):\n\t\tdata = self.data.reshape(*args)\n\t\treturn CPUArray(data.shape, data.dtype, data=data, acquire=True)\n\n\n\tdef ravel(self):\n\t\tdata = self.data.reshape((self.data.size, ))\n\t\treturn CPUArray(data.shape, data.dtype, data=data, acquire=True)\n\n\n\tdef view(self, dtype):\n\t\tdata = self.data.view(dtype)\n\t\treturn CPUArray(data.shape, data.dtype, data=data, acquire=True)\n\n\n\tdef copy(self):\n\t\tdata = self.data.copy()\n\t\treturn CPUArray(data.shape, data.dtype, data=data, acquire=True)\n\n\n\t@staticmethod\n\tdef unpackArg(arg):\n\t\treturn arg if isinstance(arg, (int, float)) else arg.data\n\n\n\tdef __setitem__(self, item, other):\n\t\tself.data[item] = other.data\n\n\n\tdef __getitem__(self, item):\n\t\tdata = self.data.__getitem__(item)\n\t\treturn CPUArray(data.shape, data.dtype, data=data, acquire=True)\n\n\n\tdef __add__(self, other):\n\t\tresult = self.data.__add__(self.unpackArg(other))\n\t\treturn CPUArray(result.shape, result.dtype, data=result, acquire=True)\n\n\n\tdef __radd__(self, other):\n\t\treturn self.__add__(other)\n\n\n\tdef __iadd__(self, other):\n\t\tself.data.__iadd__(self.unpackArg(other))\n\t\treturn self\n\n\n\tdef __sub__(self, other):\n\t\tresult = self.data.__sub__(self.unpackArg(other))\n\t\treturn CPUArray(result.shape, result.dtype, data=result, acquire=True)\n\n\n\tdef __isub__(self, other):\n\t\tself.data.__isub__(self.unpackArg(other))\n\t\treturn self\n\n\n\tdef __mul__(self, other):\n\t\tresult = self.data.__mul__(self.unpackArg(other))\n\t\treturn CPUArray(result.shape, result.dtype, data=result, acquire=True)\n\n\n\tdef __rmul__(self, other):\n\t\treturn self.__mul__(other)\n\n\n\tdef __imul__(self, other):\n\t\tself.data.__imul__(self.unpackArg(other))\n\t\treturn self\n\n\n\tdef __truediv__(self, other):\n\t\tresult = self.data.__truediv__(self.unpackArg(other))\n\t\treturn CPUArray(result.shape, result.dtype, data=result, acquire=True)\n\n\n\tdef __itruediv__(self, other):\n\t\tself.data.__itruediv__(self.unpackArg(other))\n\t\treturn self\n\n\n\tdef __str__(self):\n\t\treturn str(self.data)\n\n\n\tdef __repr__(self):\n\t\treturn repr(self.data)\n\n\n\t@staticmethod\n\tdef toDevice(ary, **_):\n\t\treturn CPUArray(ary.shape, ary.dtype, data=ary)\n\n\n\t@staticmethod\n\tdef empty(shape, dtype, **_):\n\t\treturn CPUArray(shape, dtype)\n\n\n\t@staticmethod\n\tdef zeros(shape, dtype, **_):\n\t\tary = np.zeros(shape, dtype=dtype)\n\t\treturn CPUArray(shape, ary.dtype, data=ary, acquire=True)\n\n\n\t@staticmethod\n\tdef minimum(ary):\n\t\tmn = np.min(ary.data)\n\t\treturn CPUArray(mn.shape, mn.dtype, data=mn, acquire=True)\n\n\n\t@staticmethod\n\tdef maximum(ary):\n\t\tmx = np.max(ary.data)\n\t\treturn CPUArray(mx.shape, mx.dtype, data=mx, acquire=True)\n\n\n\t@staticmethod\n\tdef arange(start=None, stop=None, step=None, dtype=None):\n\t\tary = np.arange(start, stop, step, dtype)\n\t\treturn CPUArray(ary.shape, ary.dtype, data=ary, acquire=True)\n\n\n\t@staticmethod\n\tdef full(shape, fillvalue, dtype):\n\t\tary = np.full(shape, fillvalue, dtype)\n\t\treturn CPUArray(ary.shape, ary.dtype, data=ary, acquire=True)\n\n\n\t@staticmethod\n\tdef moveaxis(ary, src, dest):\n\t\tout = np.ascontiguousarray(np.moveaxis(ary.data, src, dest))\n\t\treturn CPUArray(out.shape, out.dtype, data=out, acquire=True)\n\n\n\t@staticmethod\n\tdef swapaxes(ary, axis1, axis2):\n\t\tout = np.copy(np.swapaxes(ary.data, axis1, axis2))\n\t\treturn CPUArray(out.shape, out.dtype, data=out, acquire=True)\n'"
CPU/SourceModule.py,17,"b'import sys, os\nfrom string import Template\n\nimport numpy as np\n\nfrom PuzzleLib.Compiler.Codegen.Types import PointerType, void_t, ptrdiff_t, float_t, double_t\nfrom PuzzleLib.Compiler.Codegen.Types import int8_t, int16_t, int32_t, int64_t\nfrom PuzzleLib.Compiler.Codegen.Types import uint8_t, uint16_t, uint32_t, uint64_t\n\nfrom PuzzleLib.Compiler.Codegen.Python import generatePythonBinding, defaultConverter\nfrom PuzzleLib.Compiler.Toolchain import guessToolchain\nfrom PuzzleLib.Compiler.JIT import extensionFromString\n\nfrom PuzzleLib.CPU.CPUArray import CPUArray\n\n\nclass SourceModule:\n\tcToNumpy = {\n\t\tint8_t: ""NPY_INT8"",\n\t\tint16_t: ""NPY_INT16"",\n\t\tint32_t: ""NPY_INT32"",\n\t\tint64_t: ""NPY_INT64"",\n\n\t\tuint8_t: ""NPY_UINT8"",\n\t\tuint16_t: ""NPY_UINT16"",\n\t\tuint32_t: ""NPY_UINT32"",\n\t\tuint64_t: ""NPY_UINT64"",\n\n\t\tfloat_t: ""NPY_FLOAT"",\n\t\tdouble_t: ""NPY_DOUBLE""\n\t}\n\n\tlayoutChecker = """"""if (PyArray_TYPE(%s) != %s || !PyArray_IS_C_CONTIGUOUS(%s))\n\t{\n\t\tPyErr_SetString(PyExc_ValueError, ""tensor #%s has wrong data layout"");\n\t\treturn NULL;\n\t}""""""\n\n\n\tdef __init__(self, source, functions, converter=None, finalizer=None, debug=False):\n\t\tself.source, self.functions = source, functions\n\t\tself.mod, self.debug = None, debug\n\n\t\tself.aryIndex = 0\n\n\t\tself.converter = converter if converter is not None else self.paramConverter\n\t\tself.finalizer = finalizer\n\n\n\tdef paramConverter(self, T, name, parser):\n\t\tif isinstance(T.aliasBase, PointerType):\n\t\t\tself.aryIndex += 1\n\t\t\tdecl, objname = T.typegen(asDecl=True) % name, ""%sObj"" % name\n\n\t\t\tparser.header.append(""PyArrayObject *%s;"" % objname)\n\n\t\t\tparser.parsestr.append(""O!"")\n\t\t\tparser.parseparams.extend([""&PyArray_Type"", ""&%s"" % objname])\n\n\t\t\tparser.footer.extend([\n\t\t\t\tself.layoutChecker % (objname, self.cToNumpy[T.aliasBase.basetype], objname, self.aryIndex),\n\t\t\t\t""%s = PyArray_DATA(%s);"" % (decl, objname), """"\n\t\t\t])\n\n\t\t\tparser.callparams.append(name)\n\n\t\telse:\n\t\t\tdefaultConverter(T, name, parser)\n\n\t\tif parser.argindex + 1 == parser.totalargs:\n\t\t\tself.aryIndex = 0\n\n\n\tdef generateSource(self, modname):\n\t\theader = """"""\n#include <stddef.h>\n\n#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\n#include <numpy/arrayobject.h>\n\n""""""\n\n\t\tmodinit = ""import_array();""\n\n\t\tbinding = generatePythonBinding(\n\t\t\tmodname, self.functions, converter=self.converter, finalizer=self.finalizer, modinit=modinit\n\t\t)\n\n\t\tsource = ""%s%s%s"" % (header, self.source, binding)\n\t\treturn source\n\n\n\tdef build(self):\n\t\tif self.mod is None:\n\t\t\tmodname = ""module""\n\t\t\tsource = self.generateSource(modname)\n\n\t\t\ttoolchain = guessToolchain(verbose=1).withOptimizationLevel(\n\t\t\t\tlevel=0 if self.debug else 4,\n\t\t\t\tdebuglevel=3 if self.debug else 0\n\t\t\t).addLibrary(""numpy"", [np.get_include()], [], [])\n\n\t\t\tif sys.platform != ""win32"":\n\t\t\t\ttoolchain = toolchain.addLibrary(""math"", [], [], [""m""])\n\n\t\t\tfrom PuzzleLib.Config import libname, Backend\n\t\t\tcachepath = os.path.join(libname, Backend.cpu.name)\n\n\t\t\tself.mod = extensionFromString(toolchain, modname, source, cachepath=cachepath, cleanup=False)\n\n\n\tdef getFunction(self, name):\n\t\tself.build()\n\t\treturn getattr(self.mod, name)\n\n\n\tdef __getattr__(self, name):\n\t\treturn self.getFunction(name)\n\n\nclass Kernel:\n\tdef __init__(self, debug=False):\n\t\tself.module, self.debug = None, debug\n\n\n\tdef generateSource(self):\n\t\traise NotImplementedError()\n\n\nclass ElementwiseKernel(Kernel):\n\teltwiseTmpl = Template(""""""\n\nstatic void $name($arguments, ptrdiff_t size)\n{\n\tfor (ptrdiff_t i = 0; i < size; i++)\n\t{\n\t\t$operation;\n\t}\n}\n\n\nstatic void ${name}_strided($arguments, ptrdiff_t start, ptrdiff_t stop, ptrdiff_t step)\n{\n\tfor (ptrdiff_t i = start; i < stop; i += step)\n\t{\n\t\t$operation;\n\t}\n}\n\n"""""")\n\n\n\tdef __init__(self, arguments, operation, name, debug=False):\n\t\tsuper().__init__(debug)\n\n\t\tself.arguments, self.operation, self.name = arguments, operation, name\n\t\tself.foundArray = False\n\n\n\tdef generateSource(self):\n\t\targuments = [(T.restrict if isinstance(T, PointerType) else T, name) for T, name in self.arguments]\n\n\t\tsource = self.eltwiseTmpl.substitute(\n\t\t\targuments="", "".join(T.typegen(asDecl=True) % name for T, name in arguments),\n\t\t\toperation=self.operation, name=self.name\n\t\t)\n\n\t\tfunctions = [\n\t\t\t(self.name, void_t, arguments, True),\n\t\t\t(""%s_strided"" % self.name, void_t, arguments, True)\n\t\t]\n\n\t\treturn source, functions\n\n\n\tdef paramConverter(self, T, name, parser):\n\t\tif not self.foundArray and isinstance(T.aliasBase, PointerType):\n\t\t\tparser.footer.append(""npy_intp size = PyArray_SIZE(%sObj);"" % name)\n\t\t\tself.foundArray = True\n\n\t\tself.module.paramConverter(T, name, parser)\n\n\t\tif parser.argindex + 1 == parser.totalargs:\n\t\t\tself.foundArray = False\n\n\n\t@staticmethod\n\tdef funcFinalizer(name, parser):\n\t\tif name.endswith(""strided""):\n\t\t\tparser.header.append(""Py_ssize_t start, stop, step;"")\n\n\t\t\tparser.parsestr.append(""(nnn)"")\n\t\t\tparser.parseparams.extend([""&start"", ""&stop"", ""&step""])\n\n\t\t\tparser.callparams.extend([""start"", ""size > stop ? stop : size"", ""step""])\n\n\t\telse:\n\t\t\tparser.callparams.append(""size"")\n\n\n\tdef __call__(self, *args, **kwargs):\n\t\tif self.module is None:\n\t\t\tsource, functions = self.generateSource()\n\t\t\tself.module = SourceModule(\n\t\t\t\tsource, functions, converter=self.paramConverter,finalizer=self.funcFinalizer, debug=self.debug\n\t\t\t)\n\n\t\tfunc = getattr(self.module, self.name)\n\t\tfunc(*(arg.data if isinstance(arg, CPUArray) else arg for arg in args))\n\n\nclass ReductionKernel(Kernel):\n\treduceTmpl = Template(""""""\n\n#define READ_AND_MAP(i) ($mapExpr)\n#define REDUCE(a, b) ($reduceExpr)\n\n\nstatic $outtype reduction($arguments, ptrdiff_t size)\n{\n\t$outtype acc = $neutral;\n\n\tfor (ptrdiff_t i = 0; i < size; i++)\n\t\tacc = REDUCE(acc, READ_AND_MAP(i));\n\n\treturn acc;\n}\n\n"""""")\n\n\n\tnp2c = {\n\t\tnp.int8: int8_t,\n\t\tnp.int16: int16_t,\n\t\tnp.int32: int32_t,\n\t\tnp.int64: int64_t,\n\t\tnp.float32: float_t,\n\t\tnp.float64: double_t\n\t}\n\n\n\tdef __init__(self, outtype, neutral, reduceExpr, mapExpr, arguments, debug=False):\n\t\tsuper().__init__(debug)\n\n\t\tself.outtype, self.neutral = outtype, neutral\n\t\tself.reduceExpr, self.mapExpr, self.arguments = reduceExpr, mapExpr, arguments\n\n\t\tself.foundArray = False\n\n\n\tdef generateSource(self):\n\t\targuments = [(T.restrict if isinstance(T, PointerType) else T, name) for T, name in self.arguments]\n\n\t\tsource = self.reduceTmpl.substitute(\n\t\t\touttype = self.np2c[self.outtype].typegen(asDecl=False), neutral=self.neutral,\n\t\t\targuments="", "".join(T.typegen(asDecl=True) % name for T, name in arguments),\n\t\t\treduceExpr=self.reduceExpr, mapExpr=self.mapExpr\n\t\t)\n\n\t\tfunctions = [(""reduction"", self.np2c[self.outtype], arguments, True)]\n\t\treturn source, functions\n\n\n\tdef paramConverter(self, T, name, parser):\n\t\tif not self.foundArray and isinstance(T.aliasBase, PointerType):\n\t\t\tparser.footer.append(""ptrdiff_t size = PyArray_SIZE(%sObj);"" % name)\n\t\t\tself.foundArray = True\n\n\t\tself.module.paramConverter(T, name, parser)\n\n\t\tif parser.argindex + 1 == parser.totalargs:\n\t\t\tself.foundArray = False\n\n\n\t@staticmethod\n\tdef funcFinalizer(_, parser):\n\t\tparser.callparams.append(""size"")\n\n\n\tdef __call__(self, *args, **kwargs):\n\t\tif self.module is None:\n\t\t\tsource, functions = self.generateSource()\n\t\t\tself.module = SourceModule(\n\t\t\t\tsource, functions, converter=self.paramConverter, finalizer=self.funcFinalizer, debug=self.debug\n\t\t\t)\n\n\t\tacc = self.module.reduction(*(arg.data if isinstance(arg, CPUArray) else arg for arg in args))\n\n\t\tresult = CPUArray.empty((), self.outtype)\n\t\tresult.fill(acc)\n\n\t\treturn result\n\n\ndef unittest():\n\tmoduleTest()\n\teltwiseTest()\n\treductionTest()\n\n\ndef moduleTest():\n\toutdata = np.empty((10, ), dtype=np.float32)\n\tindata = np.random.randn(10).astype(np.float32)\n\n\tmodule = SourceModule(""""""\n\nstatic void square(float * __restrict outdata, const float * __restrict indata, ptrdiff_t size)\n{\n\tfor (ptrdiff_t i = 0; i < size; i++)\n\t\toutdata[i] = indata[i] * indata[i];\n}\n\n"""""", functions=[\n\t\t(""square"", void_t, [\n\t\t\t(float_t.ptr.restrict, ""outdata""), (float_t.const.ptr.restrict, ""indata""), (ptrdiff_t, ""size"")\n\t\t], True)\n\t])\n\n\tmodule.square(outdata, indata, outdata.size)\n\tassert np.allclose(indata * indata, outdata)\n\n\ndef eltwiseTest():\n\toutdata = CPUArray.empty((10, ), dtype=np.float32)\n\tindata = CPUArray.toDevice(np.random.randn(10).astype(np.float32))\n\n\tsquare = ElementwiseKernel(\n\t\t[(float_t.ptr, ""outdata""), (float_t.const.ptr, ""indata"")],\n\t\t""outdata[i] = indata[i] * indata[i]"",\n\t\t""square""\n\t)\n\n\tsquare(outdata, indata)\n\n\thostInData = indata.get()\n\thostOutData = hostInData * hostInData\n\n\tassert np.allclose(hostOutData, outdata.get())\n\n\ndef reductionTest():\n\tdata = CPUArray.toDevice(np.random.randn(10).astype(np.float32))\n\n\taccumulate = ReductionKernel(\n\t\tnp.float32, neutral=""0.0f"", reduceExpr=""a + b"", mapExpr=""data[i]"",\n\t\targuments=[(float_t.const.ptr, ""data"")]\n\t)\n\n\tacc = accumulate(data)\n\n\thostSum = np.sum(data.get())\n\tassert np.allclose(hostSum, acc.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
CPU/Utils.py,9,"b'import platform, multiprocessing\nimport numpy as np\n\nfrom PuzzleLib import Config\nfrom PuzzleLib.CPU.CPUArray import CPUArray\n\n\ndef autoinit():\n\tprint(""[%s]: Using device #%s (%s)"" % (Config.libname, Config.deviceIdx, platform.processor()))\n\n\nif multiprocessing.current_process().name == ""MainProcess"" or Config.allowMultiContext:\n\tautoinit()\n\n\ndef memoize(fn):\n\tcache = {}\n\n\tdef memoizer(*args):\n\t\tobj = cache.get(args, None)\n\t\tif obj is not None:\n\t\t\treturn obj\n\n\t\tobj = fn(*args)\n\t\tcache[args] = obj\n\n\t\treturn obj\n\n\treturn memoizer\n\n\ndef dtypesSupported():\n\treturn [(np.float32, 1e-5)]\n\n\nclass SharedArray:\n\tdef __init__(self, dtype=np.float32):\n\t\tself.regs = []\n\n\t\tself.mem = None\n\t\tself.dtype = dtype\n\n\t\tself.blocks = {}\n\t\tself.ary = None\n\n\n\tdef register(self, shape, dtype, name):\n\t\tself.regs.append((shape, dtype, name))\n\n\n\tdef build(self):\n\t\tnbytes = 0\n\t\tfor reg in self.regs:\n\t\t\tshape, dtype, _ = reg\n\t\t\tassert dtype == self.dtype\n\t\t\tnbytes += int(np.prod(shape) * dtype(0).itemsize)\n\n\t\tself.mem = CPUArray.empty((nbytes, ), np.uint8)\n\t\toffset = 0\n\n\t\tfor shape, dtype, name in self.regs:\n\t\t\tregbytes = int(np.prod(shape) * dtype(0).itemsize)\n\t\t\tassert offset + regbytes <= self.mem.size\n\n\t\t\tself.blocks[name] = self.mem[offset:offset + regbytes].view(dtype).reshape(shape)\n\t\t\toffset += regbytes\n\n\t\tself.regs.clear()\n\t\tself.ary = self.mem.view(dtype=self.dtype)\n\n\n\tdef __getitem__(self, item):\n\t\treturn self.blocks[item]\n\n\ndef unittest():\n\tshareMemTest()\n\n\ndef shareMemTest():\n\tshMem = SharedArray()\n\n\tshMem.register((10, 10, 10), np.float32, ""a"")\n\tshMem.register((50, 1, 5), np.float32, ""b"")\n\tshMem.build()\n\n\tassert shMem[""a""].shape == (10, 10, 10) and shMem[""a""].dtype == np.float32\n\tassert shMem[""b""].shape == (50, 1, 5) and shMem[""b""].dtype == np.float32\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Compiler/BuildSystem.py,0,"b'import os, json, itertools\n\nfrom PuzzleLib.Compiler.Compilers.Compiler import CompilerError\nfrom PuzzleLib.Compiler.Toolchain import guessToolchain, loadDynamicModule\n\n\nclass BuildError(Exception):\n\tpass\n\n\nclass Rule:\n\tdef __init__(self, deps, toolchain=None, target=None, ext=None):\n\t\tself.deps, self.toolchain = deps, toolchain\n\t\tself.target = target if target is not None else self.inferTarget(deps, toolchain.oext if ext is None else ext)\n\n\n\t@staticmethod\n\tdef inferTarget(deps, ext):\n\t\treturn ""%s%s"" % (os.path.splitext(deps[0])[0], ext)\n\n\n\tdef toString(self):\n\t\treturn ""%s <- (%s)"" % (self.target, "", "".join(self.deps))\n\n\n\tdef __str__(self):\n\t\treturn self.toString()\n\n\n\tdef __repr__(self):\n\t\treturn self.toString()\n\n\nclass LinkRule(Rule):\n\tdef __init__(self, deps, toolchain=None, target=None, forPython=True):\n\t\tsuper().__init__(\n\t\t\t[dep.target for dep in deps], toolchain, target, toolchain.pydext if forPython else toolchain.soext\n\t\t)\n\n\ndef extractCompilable(filenames):\n\tcext = {"".c"", "".cpp"", "".cu""}\n\treturn [filename for filename in filenames if any(filename.endswith(ext) for ext in cext)]\n\n\ndef validate(rules):\n\terrors = []\n\tcwd = os.path.commonprefix(list(itertools.chain(*(rule.deps for rule in rules))))\n\n\tfor i, rule in enumerate(rules):\n\t\tprint(""### Validating rule \'%s\' (%s out of %s) ..."" % (rule.target, i + 1, len(rules)), flush=True)\n\n\t\ttry:\n\t\t\tdeps = rule.toolchain.getDependencies(extractCompilable(rule.deps), cwd)\n\n\t\t\tif set(os.path.normcase(os.path.realpath(file)) for file in rule.deps) != deps:\n\t\t\t\terrors.append(""Rule: %s\\nValidated dependencies: (%s)\\n"" % (rule, "", "".join(deps)))\n\n\t\texcept CompilerError as e:\n\t\t\terrors.append(str(e))\n\n\tif len(errors) > 0:\n\t\traise BuildError(""Dependencies validation failed with following error(s):\\n\\n%s"" % ""\\n"".join(errors))\n\n\tprint(""### Dependencies validation finished successfully"", flush=True)\n\n\ndef clean(rules, linkrule):\n\tpaths = set()\n\n\tfor rule in rules + [linkrule]:\n\t\tremoved = False\n\n\t\ttry:\n\t\t\tpath = os.path.dirname(rule.target)\n\n\t\t\tif path not in paths:\n\t\t\t\tpaths.add(path)\n\t\t\t\trule.toolchain.clearPath(path)\n\n\t\t\tos.remove(rule.target)\n\t\t\tremoved = True\n\n\t\texcept OSError:\n\t\t\tpass\n\n\t\tprint(""### %s \'%s\' ..."" % (""Removed"" if removed else ""Skipped"", rule.target), flush=True)\n\n\nclass Config:\n\tdef __init__(self, rulesMap, toolchains):\n\t\tself.rulesMap, self.toolchains = rulesMap, toolchains\n\n\n\tdef getSignature(self, rule):\n\t\tindex = self.rulesMap.get(rule.target, None)\n\n\t\tif index is None:\n\t\t\treturn None\n\n\t\treturn self.toolchains[index]\n\n\n\tdef save(self, filename):\n\t\twith open(filename, mode=""w"", encoding=""utf-8"") as f:\n\t\t\tconfig = {\n\t\t\t\t""rulesMap"": self.rulesMap,\n\t\t\t\t""toolchains"": self.toolchains\n\t\t\t}\n\t\t\tjson.dump(config, f, indent=4)\n\n\n\t@classmethod\n\tdef loadFromFile(cls, filename):\n\t\trulesMap, toolchains = {}, []\n\n\t\tif os.path.exists(filename):\n\t\t\twith open(filename, mode=""r"", encoding=""utf-8"") as f:\n\t\t\t\tconfig = json.load(f)\n\t\t\t\trulesMap, toolchains = config[""rulesMap""], config[""toolchains""]\n\n\t\treturn cls(rulesMap, toolchains)\n\n\n\t@classmethod\n\tdef loadFromRules(cls, rules):\n\t\trulesMap, toolchains = {}, []\n\t\tids = {}\n\n\t\tfor rule in rules:\n\t\t\ttoolchain = rule.toolchain\n\t\t\tindex = ids.get(toolchain, None)\n\n\t\t\tif index is None:\n\t\t\t\tindex = len(toolchains)\n\n\t\t\t\ttoolchains.append(toolchain.signature())\n\t\t\t\tids[toolchain] = index\n\n\t\t\trulesMap[rule.target] = index\n\n\t\treturn cls(rulesMap, toolchains)\n\n\ndef build(rules, linkrule, recompile=False, prevalidate=False):\n\tif prevalidate:\n\t\tvalidate(rules)\n\n\tconfigname = ""%s.json"" % linkrule.target\n\n\tprevcfg = Config.loadFromFile(configname)\n\tconfig = Config.loadFromRules(rules + [linkrule])\n\n\terrors = []\n\tneedLinking = False\n\n\ttry:\n\t\tfor i, rule in enumerate(rules):\n\t\t\tneedLinking = compileObj(rule, i, len(rules), recompile, errors, config, prevcfg) or needLinking\n\n\t\tif len(errors) > 0:\n\t\t\traise BuildError(""Build failed with following error(s):\\n\\n%s"" % ""\\n"".join(errors))\n\n\t\tlink(linkrule, needLinking, config, prevcfg)\n\t\tprint(""### Build finished successfully"", flush=True)\n\n\tfinally:\n\t\tconfig.save(configname)\n\n\ndef compileObj(rule, i, nrules, recompile, errors, config, prevcfg):\n\tif recompile:\n\t\tinfo = ""forcing recompilation""\n\n\telif os.path.exists(rule.target):\n\t\tobjMTime = os.path.getmtime(rule.target)\n\t\tchanged = [dep for dep in rule.deps if os.path.getmtime(dep) > objMTime]\n\n\t\tif len(changed) > 0:\n\t\t\tinfo = ""dependencies %s changed"" % changed\n\t\telif config.getSignature(rule) != prevcfg.getSignature(rule):\n\t\t\tinfo = ""compiler settings changed""\n\t\telse:\n\t\t\tprint(""### Skipping building \'%s\' (%s out of %s) ..."" % (rule.target, i + 1, nrules), flush=True)\n\t\t\treturn False\n\n\telse:\n\t\tinfo = ""output file not found""\n\n\tprint(""### Building \'%s\' - %s (%s out of %s) ..."" % (rule.target, info, i + 1, nrules), flush=True)\n\n\ttry:\n\t\trule.toolchain.buildObject(rule.target, extractCompilable(rule.deps))\n\n\texcept CompilerError as e:\n\t\terrors.append(str(e))\n\n\treturn True\n\n\ndef link(rule, force, config, prevcfg):\n\tforce = config.getSignature(rule) != prevcfg.getSignature(rule) or force\n\n\tif not os.path.exists(rule.target) or force:\n\t\tprint(""### Linking \'%s\' ..."" % rule.target, flush=True)\n\t\trule.toolchain.link(rule.target, rule.deps)\n\n\telse:\n\t\tprint(""### Skipping linking \'%s\' ..."" % rule.target, flush=True)\n\n\ndef unittest():\n\theader = """"""\n#pragma once\nPyObject *hello(PyObject *self, PyObject *args);\n""""""\n\n\tbody1 = """"""\n#include <Python.h>\n#include ""header.h""\n\n\nstatic PyMethodDef methods[] = {\n\t{""hello"", hello, METH_NOARGS, NULL},\n\t{NULL, NULL, 0, NULL}\n};\n\n\nstatic PyModuleDef mod = {\n\tPyModuleDef_HEAD_INIT,\n\t.m_name = ""test"",\n\t.m_methods = methods\n};\n\n\nPyMODINIT_FUNC PyInit_test(void)\n{\n\treturn PyModule_Create(&mod);\n}\n""""""\n\n\tbody2 = """"""\n#include <Python.h>\n#include ""header.h""\n\n\nPyObject *hello(PyObject *self, PyObject *args)\n{\n\t(void)self, (void)args;\n\n\tputs(""Hello, Build!"");\n\tfflush(stdout);\n\n\tPy_RETURN_NONE;\n}\n""""""\n\n\twith open(""./TestData/header.h"", mode=""w"", encoding=""utf-8"") as f:\n\t\tf.write(header)\n\n\tsrcnames = [""./TestData/test.c"", ""./TestData/test2.c""]\n\n\tfor srcname, src in zip(srcnames, [body1, body2]):\n\t\twith open(srcname, mode=""w"", encoding=""utf-8"") as f:\n\t\t\tf.write(src)\n\n\ttoolchain = guessToolchain(verbose=2).withOptimizationLevel(level=4)\n\n\trules = [Rule([srcname, ""./TestData/header.h""], toolchain=toolchain) for srcname in srcnames]\n\tlinkrule = LinkRule(rules, toolchain=toolchain)\n\n\tvalidate(rules)\n\tbuild(rules, linkrule)\n\n\tmodule = loadDynamicModule(os.path.join(os.path.dirname(__file__), linkrule.target))\n\n\ttry:\n\t\tmodule.hello()\n\n\tfinally:\n\t\tclean(rules, linkrule)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Compiler/JIT.py,0,"b'import sys, os, time, hashlib, tempfile\n\nfrom PuzzleLib.Compiler.Toolchain import guessToolchain, loadDynamicModule\n\n\nstdCachePath = ""PuzzleLib""\n\n\nclass JITError(Exception):\n\tpass\n\n\ndef extensionFromString(toolchain, name, string, cachepath, cleanup=False, recompile=False, srcext="".c""):\n\tmodulename, extfile = compileFromString(toolchain, name, string, cachepath, cleanup, recompile, srcext)\n\treturn loadDynamicModule(extfile, modulename)\n\n\ndef compileFromString(toolchain, name, string, cachepath, cleanup, recompile, srcext):\n\tcachedir = getCacheDir(cachepath)\n\tsourcename = ""%s%s"" % (name, srcext)\n\n\tcmdline = toolchain.cmdline(toolchain.buildLine(name, [sourcename]))\n\thashsum = computeHash(string, *cmdline)\n\n\tmodulepath = os.path.join(cachedir, hashsum)\n\tmodulename = ""%s.%s"" % (hashsum, name)\n\n\textfile = os.path.join(modulepath, name + toolchain.pydext)\n\tsourcename = os.path.join(modulepath, sourcename)\n\n\twith FileLock(cachedir):\n\t\tif not os.path.exists(extfile) or recompile:\n\t\t\tif toolchain.verbose > 1:\n\t\t\t\tmsg = (\n\t\t\t\t\t""### Forcing extension \'%s\' recompilation ..."" if recompile else\n\t\t\t\t\t""### No cache found for extension \'%s\', performing compilation ...""\n\t\t\t\t) % name\n\t\t\t\tprint(msg, flush=True)\n\n\t\t\tos.makedirs(modulepath, exist_ok=True)\n\n\t\t\tif cleanup:\n\t\t\t\tf = tempfile.NamedTemporaryFile(mode=""w"", encoding=""utf-8"", suffix=srcext, delete=False)\n\t\t\t\ttry:\n\t\t\t\t\twith f:\n\t\t\t\t\t\tf.write(string)\n\n\t\t\t\t\ttoolchain.build(extfile, f.name)\n\n\t\t\t\tfinally:\n\t\t\t\t\tos.remove(f.name)\n\t\t\t\t\ttoolchain.clearPath(modulepath)\n\n\t\t\telse:\n\t\t\t\twith open(sourcename, mode=""w"", encoding=""utf-8"") as f:\n\t\t\t\t\tf.write(string)\n\n\t\t\t\ttoolchain.build(extfile, sourcename)\n\n\t\telif toolchain.verbose > 1:\n\t\t\tprint(""### Found cached compilation for extension \'%s\', skipping compilation ..."" % name, flush=True)\n\n\treturn modulename, extfile\n\n\ndef getCacheDir(dirname):\n\tif sys.platform == ""win32"":\n\t\tpath = os.path.normpath(os.environ[""LOCALAPPDATA""])\n\n\telif sys.platform == ""linux"":\n\t\tpath = os.path.expanduser(""~/.cache"")\n\n\telif sys.platform == ""darwin"":\n\t\tpath = os.path.expanduser(""~/Library/Caches"")\n\n\telse:\n\t\traise NotImplementedError(sys.platform)\n\n\tcachedir = os.path.join(path, dirname)\n\tos.makedirs(cachedir, exist_ok=True)\n\n\treturn cachedir\n\n\ndef computeHash(*lines):\n\thasher = hashlib.sha256()\n\n\tfor line in lines:\n\t\thasher.update(line.encode())\n\n\treturn hasher.hexdigest()\n\n\nclass FileLock:\n\tdef __init__(self, dirpath, timeout=10.0):\n\t\tself.lockfile = os.path.join(dirpath, ""lock"")\n\n\t\tself.dirpath = dirpath\n\t\tself.fd = None\n\n\t\tself.timeout = timeout\n\n\n\tdef __enter__(self):\n\t\tdt, checkpoint = 0.0, time.time()\n\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\tfd = os.open(self.lockfile, os.O_WRONLY | os.O_CREAT | os.O_EXCL | os.O_TRUNC)\n\t\t\t\tbreak\n\n\t\t\texcept (IOError, OSError):\n\t\t\t\ttm = time.time()\n\t\t\t\tdt += tm - checkpoint\n\t\t\t\tcheckpoint = tm\n\n\t\t\t\tif dt >= self.timeout:\n\t\t\t\t\traise JITError(""Could not lock directory \'%s\' (timeout is %s secs)"" % (self.dirpath, self.timeout))\n\n\t\tself.fd = fd\n\n\n\tdef __exit__(self, exc_type, exc_val, exc_tb):\n\t\tos.close(self.fd)\n\t\tself.fd = None\n\n\t\ttry:\n\t\t\tos.remove(self.lockfile)\n\n\t\texcept OSError:\n\t\t\tpass\n\n\ndef unittest():\n\ttoolchain = guessToolchain(verbose=2).withOptimizationLevel(level=4)\n\n\tsrc = """"""\n#include <Python.h>\n\n\nstatic PyObject *hello(PyObject *self, PyObject *args)\n{\n\t(void)self, (void)args;\n\n\tputs(""Hello, JIT!"");\n\tfflush(stdout);\n\n\tPy_RETURN_NONE;\n}\n\n\nstatic PyMethodDef methods[] = {\n\t{""hello"", hello, METH_NOARGS, NULL},\n\t{NULL, NULL, 0, NULL}\n};\n\n\nstatic PyModuleDef mod = {\n\tPyModuleDef_HEAD_INIT,\n\t.m_name = ""test"",\n\t.m_methods = methods\n};\n\n\nPyMODINIT_FUNC PyInit_test(void)\n{\n\treturn PyModule_Create(&mod);\n}\n""""""\n\n\ttest = extensionFromString(\n\t\ttoolchain, name=""test"", string=src, cachepath=os.path.join(stdCachePath, ""tests""), cleanup=True, recompile=True\n\t)\n\ttest.hello()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Compiler/Toolchain.py,0,"b'import sys, os, stat, shutil, importlib.util\nfrom enum import Enum\n\nfrom PuzzleLib.Compiler.Compilers.GCC import GCC, Clang\nfrom PuzzleLib.Compiler.Compilers.MSVC import MSVC\nfrom PuzzleLib.Compiler.Compilers.NVCC import NVCC\n\n\nclass Toolchain(Enum):\n\tmsvc = ""msvc""\n\tgcc = ""gcc""\n\tclang = ""clang""\n\n\ndef guessToolchain(verbose=0, tc=None):\n\tif tc is None:\n\t\ttc = {\n\t\t\t""win32"": Toolchain.msvc,\n\t\t\t""linux"": Toolchain.gcc,\n\t\t\t""darwin"": Toolchain.clang\n\t\t}[sys.platform]\n\n\tcls = {\n\t\tToolchain.msvc: MSVC,\n\t\tToolchain.gcc: GCC,\n\t\tToolchain.clang: Clang\n\t}[Toolchain(tc)]\n\n\treturn cls(verbose=verbose)\n\n\ndef guessNVCCToolchain(verbose=0, forPython=False):\n\treturn NVCC(verbose, forPython)\n\n\ndef loadDynamicModule(extfile, modulename=None):\n\tmodulename = os.path.basename(extfile).split(sep=""."")[0] if modulename is None else modulename\n\tspec = importlib.util.spec_from_file_location(modulename, os.path.abspath(extfile))\n\n\tmodule = importlib.util.module_from_spec(spec)\n\tspec.loader.exec_module(module)\n\n\treturn module\n\n\ndef createTemplateNames(name):\n\treturn [""%s.gen%s"" % (name, ext) for name, ext in [(name, "".h""), (name, "".c"")]]\n\n\ndef writeTemplates(sources):\n\tfor source, filename in sources:\n\t\tif os.path.exists(filename):\n\t\t\tos.chmod(filename, stat.S_IWUSR | stat.S_IREAD)\n\n\t\twith open(filename, mode=""w"", encoding=""utf-8"") as f:\n\t\t\tf.write(source)\n\n\t\tos.chmod(filename, stat.S_IREAD | stat.S_IRGRP | stat.S_IROTH)\n\n\ndef copySource(src, dst):\n\tif os.path.exists(dst):\n\t\tos.chmod(dst, stat.S_IWUSR | stat.S_IREAD)\n\n\tshutil.copy(src, dst)\n\tos.chmod(dst, stat.S_IREAD | stat.S_IRGRP | stat.S_IROTH)\n\n\ndef buildTemplateTest(name, path, bindingName, generator, verbose=2, debuglevel=0, level=4, defines=None, **kwargs):\n\tsources = generator(name=name, filename=os.path.join(path, name), **kwargs)\n\tcc = guessToolchain(verbose=verbose).withOptimizationLevel(debuglevel=debuglevel, level=level)\n\n\tdefines = [] if defines is None else defines\n\tcc.addDefine(*defines)\n\n\tbinding = os.path.join(path, bindingName)\n\tif os.path.exists(binding):\n\t\tos.chmod(binding, stat.S_IWUSR | stat.S_IREAD)\n\n\tshutil.copy(bindingName, binding)\n\tos.chmod(binding, stat.S_IREAD | stat.S_IRGRP | stat.S_IROTH)\n\n\tsources = sources + [binding] if isinstance(sources, list) else [sources, binding]\n\tmodname = os.path.join(path, name + cc.pydext)\n\n\tcc.build(modname, sources).clearPath(path)\n\treturn loadDynamicModule(modname)\n'"
Containers/Container.py,1,"b'import json\n\nimport numpy as np\nimport h5py\n\nfrom PuzzleLib.Modules.Module import Module, ModuleError\n\n\nclass ContainerError(ModuleError):\n\tpass\n\n\nclass Container(Module):\n\t__slots__ = [""modules""]\n\n\n\tdef __init__(self, name=None):\n\t\tsuper().__init__(name)\n\t\tself.modules = {}\n\n\n\tdef getBlueprint(self):\n\t\tblueprint = super().getBlueprint()\n\t\tblueprint[""modules""] = {name: mod.getBlueprint() for name, mod in self.modules.items()}\n\n\t\treturn blueprint\n\n\n\tdef append(self, mod, acquire=True):\n\t\tmod.name = str(len(self.modules)) if mod.name is None else mod.name\n\n\t\tif mod.name in self.modules:\n\t\t\tif acquire:\n\t\t\t\tmod.name = str(len(self.modules))\n\t\t\telse:\n\t\t\t\traise ContainerError(""Module with name \'%s\' is already in container"" % mod.name)\n\n\t\tself.modules[mod.name] = mod\n\t\treturn self\n\n\n\tdef removeModule(self, mod):\n\t\tself.modules.pop(mod.name)\n\t\treturn mod\n\n\n\tdef getByName(self, name):\n\t\tmod = None\n\n\t\tif name in self.modules:\n\t\t\tmod = self.modules[name]\n\t\telse:\n\t\t\tfor m in self.modules.values():\n\t\t\t\tif isinstance(m, Container):\n\t\t\t\t\tmod = m.getByName(name)\n\n\t\t\t\t\tif mod is not None:\n\t\t\t\t\t\tbreak\n\n\t\treturn mod\n\n\n\tdef getAllByType(self, typ):\n\t\tlst = []\n\n\t\tfor mod in self.modules.values():\n\t\t\tif isinstance(mod, typ):\n\t\t\t\tlst.append(mod)\n\n\t\t\telif isinstance(mod, Container):\n\t\t\t\tlst.extend(mod.getAllByType(typ))\n\n\t\treturn lst\n\n\n\tdef __getitem__(self, item):\n\t\tif isinstance(item, str):\n\t\t\treturn self.modules[item]\n\t\telse:\n\t\t\traise NotImplementedError(type(item).__name__)\n\n\n\tdef setVar(self, name, var):\n\t\tsep = name.index(""."")\n\t\tif sep == -1:\n\t\t\traise ContainerError(""Cannot find dot-delimiter in variable name: %s"" % name)\n\n\t\tself.modules[name[:sep]].setVar(name[sep+1:], var)\n\n\n\tdef getVar(self, name):\n\t\tsep = name.index(""."")\n\t\tif sep == -1:\n\t\t\traise ContainerError(""Cannot find dot-delimiter in variable name: %s"" % name)\n\n\t\treturn self.modules[name[:sep]].getVar(name[sep+1:])\n\n\n\tdef getVarTable(self, vartable=None, name=None, root=True):\n\t\tname = """" if root else name\n\t\tvartable = {} if vartable is None else vartable\n\n\t\tfor mod in self.modules.values():\n\t\t\tmod.getVarTable(vartable, ""%s%s."" % (name, mod.name), root=False)\n\n\t\treturn vartable\n\n\n\tdef setAttr(self, name, attr):\n\t\tctrName = self.name if self.name else """"\n\t\tself.attrs[""%s.%s"" % (ctrName, name)] = attr\n\n\n\tdef getAttr(self, name):\n\t\tctrName = self.name if self.name else """"\n\t\treturn self.attrs[""%s.%s"" % (ctrName, name)]\n\n\n\tdef hasAttr(self, name):\n\t\tctrName = self.name if self.name else """"\n\t\treturn (""%s.%s"" % (ctrName, name)) in self.attrs\n\n\n\tdef zeroGradParams(self):\n\t\tfor mod in self.modules.values():\n\t\t\tmod.zeroGradParams()\n\n\n\tdef updateParams(self, learnRate):\n\t\tfor mod in self.modules.values():\n\t\t\tmod.updateParams(learnRate)\n\n\n\tdef genericCheckDataType(self, dtype):\n\t\tpass\n\n\n\tdef save(self, hdf=None, varlinks=None, name=None, compress=""gzip"", assumeUniqueNames=False, withBlueprint=False,\n\t\t\t isRoot=True):\n\t\tserialize = True if hdf is None else False\n\n\t\thdf = self.ensureHdf(hdf, ""w"")\n\t\tvarlinks = {} if varlinks is None else varlinks\n\n\t\tif name is None:\n\t\t\tname = self.name if self.name is not None else """"\n\n\t\ttry:\n\t\t\tfor mod in self.modules.values():\n\t\t\t\tmod.save(\n\t\t\t\t\thdf, varlinks, ""%s.%s"" % (name, mod.name), compress=compress,\n\t\t\t\t\tassumeUniqueNames=assumeUniqueNames, isRoot=False\n\t\t\t\t)\n\n\t\t\tattrGrp = hdf.require_group(""attrs.%s"" % name)\n\t\t\tfor attrName, attr in self.attrs.items():\n\t\t\t\tattrGrp.create_dataset(attrName, data=attr)\n\n\t\t\tif withBlueprint:\n\t\t\t\thdf.create_dataset(\n\t\t\t\t\t""blueprint"", (), dtype=h5py.special_dtype(vlen=str),\n\t\t\t\t\tdata=json.dumps(self.getBlueprint(), indent=4, sort_keys=True)\n\t\t\t\t)\n\n\t\t\tbuffer = None\n\t\t\tif isRoot and serialize:\n\t\t\t\thdf.flush()\n\t\t\t\tbuffer = hdf.id.get_file_image()\n\n\t\texcept Exception as e:\n\t\t\traise ContainerError(""Container %s save error: %s"" % (name, e))\n\n\t\tfinally:\n\t\t\tif isRoot:\n\t\t\t\thdf.close()\n\n\t\treturn buffer\n\n\n\tdef load(self, hdf, initvars=None, name=None, assumeUniqueNames=False, isRoot=True):\n\t\thdf = self.ensureHdf(hdf, ""r"")\n\t\tinitvars = {} if initvars is None else initvars\n\n\t\tif name is None:\n\t\t\tname = self.name if self.name is not None else """"\n\n\t\ttry:\n\t\t\tfor mod in self.modules.values():\n\t\t\t\tmod.load(hdf, initvars, ""%s.%s"" % (name, mod.name), assumeUniqueNames=assumeUniqueNames, isRoot=False)\n\n\t\t\tgrpName = ""attrs.%s"" % name\n\n\t\t\tif grpName in hdf:\n\t\t\t\tattrGrp = hdf[grpName]\n\t\t\t\tself.attrs.update((attrName, np.array(attr)) for attrName, attr in attrGrp.items())\n\n\t\texcept Exception as e:\n\t\t\traise ContainerError(""Container %s load error: %s"" % (name, e))\n\n\t\tfinally:\n\t\t\tif isRoot:\n\t\t\t\thdf.close()\n\n\n\tdef trainMode(self):\n\t\tsuper().trainMode()\n\t\tfor mod in self.modules.values():\n\t\t\tmod.trainMode()\n\n\n\tdef evalMode(self):\n\t\tsuper().evalMode()\n\t\tfor mod in self.modules.values():\n\t\t\tmod.evalMode()\n\n\n\tdef calcMode(self, T):\n\t\tfor mod in self.modules.values():\n\t\t\ttry:\n\t\t\t\tmod.calcMode(T)\n\n\t\t\texcept Exception as e:\n\t\t\t\tself.handleError(mod, e)\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\t\tfor mod in self.modules.values():\n\t\t\tmod.reset()\n\n\n\tdef __str__(self):\n\t\treturn ""Container %s (name: %s)"" % (self.__class__.__name__, self.name)\n\n\n\tdef handleError(self, mod, e):\n\t\tmsg = str(e)\n\t\tmsg = "": %s"" % msg if len(msg) > 0 else """"\n\n\t\traise ContainerError(""%s:\\nModule (%s) error:\\n%s%s"" % (self, mod, type(e), msg))\n\n\n\tdef numOfParams(self):\n\t\treturn sum(mod.numOfParams() for mod in self.modules.values())\n\n\n\tdef paramSize(self, unit=None):\n\t\tsize = sum(mod.paramSize(unit=None) for mod in self.modules.values())\n\t\treturn self.convertUnit(size, unit=unit) if unit is not None else size\n\n\n\tdef updateData(self, data):\n\t\traise NotImplementedError()\n\n\n\tdef updateGrad(self, grad):\n\t\traise NotImplementedError()\n\n\n\tdef dataShapeFrom(self, shape):\n\t\traise NotImplementedError()\n\n\n\tdef gradShapeFrom(self, shape):\n\t\traise NotImplementedError()\n'"
Containers/Graph.py,15,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Containers.Container import ContainerError, Container\nfrom PuzzleLib.Containers.Node import Node\n\n\nclass Graph(Container):\n\tdef __init__(self, inputs, outputs, unsafe=False, nodesOnly=False, name=None):\n\t\tsuper().__init__(name)\n\t\tself.unsafe = unsafe\n\n\t\tself.inputs = [inputs] if not isinstance(inputs, list) else inputs\n\n\t\timpureInputs = [inp.name for inp in self.inputs if len(inp.bwds) > 0]\n\t\tif len(impureInputs) > 0:\n\t\t\traise ContainerError(""Found input nodes with parents: %s"" % "", "".join(impureInputs))\n\n\t\tself.outputs = [outputs] if not isinstance(outputs, list) else outputs\n\n\t\timpureOutputs = [output.name for output in self.outputs if len(output.fwds) > 0]\n\t\tif len(impureOutputs) > 0:\n\t\t\traise ContainerError(""Found output nodes with ancestors: %s"" % "", "".join(impureOutputs))\n\n\t\tself.nodes = {}\n\t\tfor inp in self.inputs:\n\t\t\tinp.traverseForward(inp, lambda node: self.gatherTopology(node, nodesOnly))\n\n\t\tunvisited = [output.name for output in self.outputs if not output.fwdVisited]\n\t\tif len(unvisited) > 0:\n\t\t\traise ContainerError(""Could not visit output nodes: %s"" % "", "".join(unvisited))\n\n\t\tself.reset()\n\n\n\tdef gatherTopology(self, node, nodesOnly):\n\t\tif not nodesOnly:\n\t\t\tself.append(node.module)\n\n\t\tassert node.name not in self.nodes\n\t\tself.nodes[node.name] = node\n\n\t\tif getattr(node.module, ""inplace"", False) and not self.unsafe:\n\t\t\tfor fwd in node.fwds:\n\t\t\t\tif len(fwd[0].bwds) > 1:\n\t\t\t\t\traise ContainerError(""Invalid inplace mode - module %s has non-trivial ancestor %s"" %\n\t\t\t\t\t\t\t\t\t\t (node.module, fwd[0]))\n\n\t\t\tfor bwd in node.bwds:\n\t\t\t\tif len(bwd[0].fwds) > 1:\n\t\t\t\t\traise ContainerError(""Invalid inplace mode - module %s has non-trivial parent %s"" %\n\t\t\t\t\t\t\t\t\t\t (node.module, bwd[0]))\n\n\n\tdef getBlueprint(self):\n\t\tblueprint = super().getBlueprint()\n\n\t\tblueprint[""graph""] = {node.name: [(n.name, slots) for n, slots in node.bwds] for node in self.nodes.values()}\n\t\tblueprint[""inputs""] = [inp.name for inp in self.inputs]\n\t\tblueprint[""outputs""] = [output.name for output in self.outputs]\n\n\t\treturn blueprint\n\n\n\tdef getNodeByName(self, name):\n\t\treturn self.nodes[name]\n\n\n\tdef optimizeForShape(self, shape, memlimit=None):\n\t\tself.graphDataShape(shape, lambda module, sh: module.optimizeForShape(sh, memlimit))\n\n\n\tdef updateData(self, data):\n\t\tdata = data if isinstance(data, list) else [data]\n\n\t\tfor i, inp in enumerate(self.inputs):\n\t\t\tinp.forward(data[i])\n\n\t\tself.data = self.outputs[0].data if len(self.outputs) == 1 else [output.data for output in self.outputs]\n\t\tself.clearTraverse()\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn self.graphDataShape(shape, None)\n\n\n\tdef graphDataShape(self, shape, onmodule):\n\t\tshape = shape if isinstance(shape, list) else [shape]\n\n\t\tinshapes = {inp.name: shape[i] for i, inp in enumerate(self.inputs)}\n\t\tshapes = {}\n\n\t\tfor i, inp in enumerate(self.inputs):\n\t\t\tinp.traverseForward(inp, Node.dataShapeFrom, inshapes, shapes, onmodule)\n\n\t\toutshapes = [shapes[output.name] for output in self.outputs]\n\t\tif len(self.outputs) == 1:\n\t\t\toutshapes = outshapes[0]\n\n\t\tself.clearTraverse()\n\t\treturn outshapes\n\n\n\tdef backward(self, grad, updParamGrads=True, updGrad=True, scale=1.0, momentum=1.0):\n\t\tgrad = grad if isinstance(grad, list) else [grad]\n\n\t\tfor i, output in enumerate(self.outputs):\n\t\t\toutput.backward(grad[i], updParamGrads=updParamGrads, updGrad=updGrad, scale=scale, momentum=momentum)\n\n\t\tself.grad = self.inputs[0].grad if len(self.inputs) == 1 else [inp.grad for inp in self.inputs]\n\t\tself.clearTraverse()\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tshape = shape if isinstance(shape, list) else [shape]\n\n\t\toutshapes = {output.name: shape[i] for i, output in enumerate(self.outputs)}\n\t\tshapes = {}\n\n\t\tfor i, output in enumerate(self.outputs):\n\t\t\toutput.traverseBackward(output, Node.gradShapeFrom, outshapes, shapes)\n\n\t\tinshape = [shapes[inp.name] for inp in self.inputs]\n\t\tif len(self.inputs) == 1:\n\t\t\tinshape = inshape[0]\n\n\t\tself.clearTraverse()\n\t\treturn inshape\n\n\n\tdef updateGrad(self, grad):\n\t\tassert False\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\n\t\tfor node in self.nodes.values():\n\t\t\tnode.reset()\n\n\n\tdef clearTraverse(self):\n\t\tfor node in self.nodes.values():\n\t\t\tnode.clearTraverse()\n\n\ndef unittest():\n\tcalcTest()\n\tmatchTest()\n\n\ndef calcTest():\n\tfrom PuzzleLib.Modules import Linear, Split, Concat, Activation, relu\n\n\tv1 = Linear(100, 50, name=""v1"").node()\n\th1 = Split(axis=1, sections=(20, 20, 10), name=""h1"").node(v1)\n\n\tv2 = Linear(100, 50, name=""v2"").node()\n\th2 = Concat(axis=1, name=""h2"").node((h1, [1, 2]), v2)\n\th3 = Activation(relu, name=""h3"").node(h2)\n\n\th4 = Concat(axis=1, name=""h4"").node((h1, 0), h3)\n\n\tmlp = Graph(inputs=[v1, v2], outputs=h4)\n\n\tv1data = gpuarray.to_gpu(np.random.randn(5, 100).astype(np.float32))\n\tv2data = gpuarray.to_gpu(np.random.randn(5, 100).astype(np.float32))\n\n\tmlp.optimizeForShape([v1data.shape, v2data.shape])\n\tmlp([v1data, v2data])\n\n\tassert mlp.data.shape == (5, 100)\n\tassert mlp.dataShapeFrom([v1data.shape, v2data.shape]) == mlp.data.shape\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*mlp.data.shape).astype(np.float32))\n\tmlp.backward(grad)\n\n\tassert len(mlp.grad) == 2 and mlp.grad[0].shape == mlp.grad[1].shape == (5, 100)\n\tassert mlp.gradShapeFrom(grad.shape) == [gr.shape for gr in mlp.grad]\n\n\ndef matchTest():\n\tfrom PuzzleLib.Containers import Sequential, Parallel\n\tfrom PuzzleLib.Modules import Linear, Activation, sigmoid, Replicate, Concat\n\n\tseq = Sequential()\n\tseq.append(Linear(128, 64, name=""linear-1""))\n\tseq.append(Activation(sigmoid))\n\tseq.append(Replicate(times=2))\n\n\tparallel = Parallel()\n\tparallel.append(Linear(64, 10, name=""linear-2""))\n\tparallel.append(Linear(64, 5, name=""linear-3""))\n\tseq.append(parallel)\n\n\tseq.append(Concat(axis=1))\n\n\tv1 = Linear(128, 64, name=""linear-1"").node()\n\th1 = Activation(sigmoid).node(v1)\n\n\th2 = Linear(64, 10, name=""linear-2"").node(h1)\n\th3 = Linear(64, 5, name=""linear-3"").node(h1)\n\n\th4 = Concat(axis=1).node(h2, h3)\n\n\tmlp = Graph(inputs=v1, outputs=h4)\n\n\tmlp.getByName(""linear-1"").W.set(seq.getByName(""linear-1"").W)\n\tmlp.getByName(""linear-1"").b.set(seq.getByName(""linear-1"").b)\n\n\tmlp.getByName(""linear-2"").W.set(seq.getByName(""linear-2"").W)\n\tmlp.getByName(""linear-2"").b.set(seq.getByName(""linear-2"").b)\n\n\tmlp.getByName(""linear-3"").W.set(seq.getByName(""linear-3"").W)\n\tmlp.getByName(""linear-3"").b.set(seq.getByName(""linear-3"").b)\n\n\tdata = gpuarray.to_gpu(np.random.randn(32, 128).astype(np.float32))\n\tseq(data)\n\tmlp(data)\n\n\tassert np.allclose(seq.data.get(), mlp.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(32, 15).astype(np.float32))\n\tseq.backward(grad)\n\tmlp.backward(grad)\n\n\tassert np.allclose(seq.grad.get(), mlp.grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Containers/Node.py,0,"b'from PuzzleLib.Backend import Blas\nfrom PuzzleLib.Backend.Utils import copy\n\n\nclass NodeError(Exception):\n\tpass\n\n\nclass Node:\n\tdef __init__(self, mod, parents=None, name=None):\n\t\tself.module = mod\n\t\tself.rename = name\n\n\t\tself.fwdVisited = False\n\t\tself.bwdVisited = False\n\n\t\tself.data = None\n\t\tself.grad = None\n\n\t\tself.fwds = []\n\t\tself.bwds = []\n\n\t\tself.addBackwards(parents)\n\n\n\tdef addBackwards(self, nodes):\n\t\tif nodes is None:\n\t\t\treturn\n\n\t\tif isinstance(nodes, Node):\n\t\t\tnodes.addForward((self, None))\n\t\t\tself.bwds.append((nodes, None))\n\n\t\telif isinstance(nodes, tuple):\n\t\t\tnode, slots = nodes\n\t\t\tif not isinstance(slots, (list, type(None))):\n\t\t\t\tslots = [slots]\n\n\t\t\tnode.addForward((self, slots))\n\t\t\tself.bwds.append((node, slots))\n\n\t\telif isinstance(nodes, list):\n\t\t\tfor node in nodes:\n\t\t\t\tself.addBackwards(node)\n\n\t\telse:\n\t\t\traise NodeError(""Unrecognized parent object type %s"" % type(nodes).__name__)\n\n\n\tdef addForward(self, node):\n\t\tself.fwds.append(node)\n\n\n\t@property\n\tdef name(self):\n\t\treturn self.module.name if self.rename is None else self.rename\n\n\n\tdef forward(self, data):\n\t\tself.traverseForward(self, Node.updateData, data)\n\n\n\tdef updateData(self, data):\n\t\tif len(self.bwds) > 0:\n\t\t\tif len(self.bwds) == 1 and self.bwds[0][1] is None:\n\t\t\t\tdata = self.bwds[0][0].data\n\t\t\telse:\n\t\t\t\tdata = []\n\t\t\t\tfor node, slots in self.bwds:\n\t\t\t\t\tdata.extend([node.data] if slots is None else (node.data[slot] for slot in slots))\n\n\t\tself.data = self.module(data)\n\n\n\tdef dataShapeFrom(self, inshapes, shapes, onmodule):\n\t\tif len(self.bwds) == 0:\n\t\t\tshape = inshapes[self.name]\n\t\telse:\n\t\t\tshape = []\n\t\t\tfor node, slots in self.bwds:\n\t\t\t\tshape.extend([shapes[node.name]] if slots is None else (shapes[node.name][slot] for slot in slots))\n\n\t\t\tif len(self.bwds) == 1:\n\t\t\t\tshape = shape[0]\n\n\t\toutshape = self.module.dataShapeFrom(shape)\n\t\tif onmodule is not None:\n\t\t\tonmodule(self.module, shape)\n\n\t\tshapes[self.name] = outshape\n\n\n\tdef backward(self, grad=None, updParamGrads=True, updGrad=True, scale=1.0, momentum=0.0):\n\t\tself.traverseBackward(self, Node.updateGrad, grad, updParamGrads, updGrad, scale, momentum)\n\n\n\tdef updateGrad(self, grad, updParamGrads, updGrad, scale, momentum):\n\t\tgrad = self.buildOutGrad(grad)\n\n\t\tupdGrad = updGrad if len(self.bwds) == 0 else True\n\t\tself.module.backward(grad, updParamGrads=updParamGrads, updGrad=updGrad, scale=scale, momentum=momentum)\n\n\t\tself.grad = self.routeInGrad(self.module.grad)\n\n\n\tdef buildOutGrad(self, grad):\n\t\tif len(self.fwds) == 0:\n\t\t\treturn grad\n\n\t\tgrad = [[] for _ in range(len(self.data) if isinstance(self.data, list) else 1)]\n\n\t\tfor node, slots in self.fwds:\n\t\t\tif slots is not None:\n\t\t\t\tfor slot in slots:\n\t\t\t\t\tgrad[slot].append(node.grad[self.name][slot])\n\n\t\t\telse:\n\t\t\t\tfor i, gr in enumerate(node.grad[self.name]):\n\t\t\t\t\tgrad[i].append(gr)\n\n\t\tfor i, grads in enumerate(grad):\n\t\t\tif len(grads) > 1:\n\t\t\t\tgr = copy(None, grads[0])\n\n\t\t\t\tfor j in range(1, len(grads)):\n\t\t\t\t\tBlas.toVectorAddVector(gr.ravel(), grads[j].ravel())\n\n\t\t\telse:\n\t\t\t\tgr = grads[0]\n\n\t\t\tgrad[i] = gr\n\n\t\tif len(grad) == 1:\n\t\t\tgrad = grad[0]\n\n\t\treturn grad\n\n\n\tdef routeInGrad(self, grad):\n\t\tif len(self.bwds) == 0:\n\t\t\treturn grad\n\n\t\tgrad = grad if isinstance(grad, list) else [grad]\n\t\troutedgrad = {}\n\n\t\ti = 0\n\t\tfor node, slots in self.bwds:\n\t\t\tif slots is None:\n\t\t\t\tln = len(node.data) if isinstance(node.data, list) else 1\n\n\t\t\t\troutedgrad[node.name] = grad[i:i + ln]\n\t\t\t\ti += ln\n\n\t\t\telse:\n\t\t\t\td = {slot: grad[i + j] for j, slot in enumerate(slots)}\n\t\t\t\ti += len(slots)\n\n\t\t\t\troutedgrad[node.name] = d\n\n\t\treturn routedgrad\n\n\n\tdef gradShapeFrom(self, outshapes, shapes):\n\t\tshape = self.buildOutGradShape(outshapes, shapes)\n\n\t\tinshape = self.routeInGrad(self.module.gradShapeFrom(shape))\n\t\tshapes[self.name] = inshape\n\n\n\tdef buildOutGradShape(self, outshapes, shapes):\n\t\tif len(self.fwds) == 0:\n\t\t\treturn outshapes[self.name]\n\n\t\tshape = [None for _ in range(len(self.data) if isinstance(self.data, list) else 1)]\n\n\t\tfor node, slots in self.fwds:\n\t\t\tif slots is not None:\n\t\t\t\tfor slot in slots:\n\t\t\t\t\tshape[slot] = shapes[node.name][self.name][slot]\n\n\t\t\telse:\n\t\t\t\tfor i, sh in enumerate(shapes[node.name][self.name]):\n\t\t\t\t\tshape[i] = sh\n\n\t\tif len(shape) == 1:\n\t\t\tshape = shape[0]\n\n\t\treturn shape\n\n\n\tdef reset(self):\n\t\tself.clearTraverse()\n\n\t\tself.data = None\n\t\tself.grad = None\n\n\t\tself.module.reset()\n\n\n\tdef clearTraverse(self):\n\t\tself.fwdVisited = False\n\t\tself.bwdVisited = False\n\n\n\tdef __str__(self):\n\t\treturn ""Node %s (name: %s)"" % (type(self.module), self.name)\n\n\n\t@staticmethod\n\tdef traverseForward(node, func, *args):\n\t\twhile True:\n\t\t\tif node.fwdVisited:\n\t\t\t\treturn\n\n\t\t\tif not all(bwd[0].fwdVisited for bwd in node.bwds):\n\t\t\t\treturn\n\n\t\t\tfunc(node, *args)\n\t\t\tnode.fwdVisited = True\n\n\t\t\tif len(node.fwds) == 1:\n\t\t\t\tnode, _ = node.fwds[0]\n\t\t\t\tcontinue\n\n\t\t\telse:\n\t\t\t\tfor n, _ in node.fwds:\n\t\t\t\t\tn.traverseForward(n, func, *args)\n\n\t\t\t\tbreak\n\n\n\t@staticmethod\n\tdef traverseBackward(node, func, *args):\n\t\twhile True:\n\t\t\tif node.bwdVisited:\n\t\t\t\treturn\n\n\t\t\tif not all(fwd[0].bwdVisited for fwd in node.fwds):\n\t\t\t\treturn\n\n\t\t\tfunc(node, *args)\n\t\t\tnode.bwdVisited = True\n\n\t\t\tif len(node.bwds) == 1:\n\t\t\t\tnode, _ = node.bwds[0]\n\t\t\t\tcontinue\n\n\t\t\telse:\n\t\t\t\tfor n, _ in node.bwds:\n\t\t\t\t\tn.traverseBackward(n, func, *args)\n\n\t\t\t\tbreak\n'"
Containers/Parallel.py,6,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Modules.Module import ModuleError\nfrom PuzzleLib.Containers.Container import Container\n\n\nclass Parallel(Container):\n\tdef __init__(self, name=None):\n\t\tsuper().__init__(name)\n\t\tself.graph = []\n\n\n\t@property\n\tdef gradUsesOutData(self):\n\t\tif len(self.graph) == 0:\n\t\t\treturn False\n\n\t\tfor mod in self.graph:\n\t\t\tif mod.gradUsesOutData:\n\t\t\t\treturn True\n\n\t\treturn False\n\n\n\t@gradUsesOutData.setter\n\tdef gradUsesOutData(self, val):\n\t\tpass\n\n\n\t@property\n\tdef inplace(self):\n\t\tfor mod in self.graph[:-1]:\n\t\t\tif getattr(mod, ""inplace"", False):\n\t\t\t\treturn True\n\n\t\treturn False\n\n\n\tdef getBlueprint(self):\n\t\tblueprint = super().getBlueprint()\n\t\tblueprint[""graph""] = [mod.name for mod in self.graph]\n\n\t\treturn blueprint\n\n\n\tdef append(self, mod, acquire=True):\n\t\tsuper().append(mod, acquire)\n\t\tself.graph.append(mod)\n\n\t\treturn self\n\n\n\tdef extend(self, container, acquire=True):\n\t\tif isinstance(container, Parallel):\n\t\t\tcontainer = container.graph\n\n\t\tfor mod in container:\n\t\t\tself.append(mod, acquire)\n\n\n\tdef pop(self):\n\t\tmod = self.graph.pop()\n\t\tsuper().removeModule(mod)\n\n\t\treturn mod\n\n\n\tdef __getitem__(self, item):\n\t\tif isinstance(item, str):\n\t\t\treturn super().__getitem__(item)\n\n\t\telif isinstance(item, int):\n\t\t\treturn self.graph[item]\n\n\t\telif isinstance(item, slice):\n\t\t\tparallel = Parallel()\n\t\t\tparallel.extend(self.graph[item.start:item.stop:item.step])\n\n\t\t\treturn parallel\n\n\t\telse:\n\t\t\traise NotImplementedError(type(item).__name__)\n\n\n\tdef getByIndex(self, index):\n\t\treturn self.graph[index]\n\n\n\tdef optimizeForShape(self, shapes, memlimit=None):\n\t\tfor i, mod in enumerate(self.graph):\n\t\t\tmod.optimizeForShape(shapes[i], memlimit)\n\n\n\tdef updateData(self, data):\n\t\tassert len(data) == len(self.graph)\n\n\t\tself.data = []\n\t\tfor i, mod in enumerate(self.graph):\n\t\t\ttry:\n\t\t\t\tmod(data[i])\n\n\t\t\texcept ModuleError as e:\n\t\t\t\traise ModuleError(""%s:\\nData error in module %d (%s):\\n%s"" % (self, i, mod, e))\n\n\t\t\texcept Exception as e:\n\t\t\t\tself.handleError(mod, e)\n\n\t\t\tself.data.append(mod.data)\n\n\n\tdef dataShapeFrom(self, shapes):\n\t\toutshapes = []\n\n\t\tfor i, mod in enumerate(self.graph):\n\t\t\toutshapes.append(mod.dataShapeFrom(shapes[i]))\n\n\t\treturn outshapes\n\n\n\tdef backward(self, grad, updParamGrads=True, updGrad=True, scale=1.0, momentum=1.0):\n\t\tassert len(grad) == len(self.graph)\n\n\t\tself.grad = []\n\t\tfor i, mod in enumerate(self.graph):\n\t\t\ttry:\n\t\t\t\tmod.backward(grad[i], updParamGrads=updParamGrads, updGrad=updGrad, scale=scale, momentum=momentum)\n\n\t\t\texcept ModuleError as e:\n\t\t\t\traise ModuleError(""%s:\\nGrad error in module %d (%s):\\n%s"" % (self, i, mod, e))\n\n\t\t\texcept Exception as e:\n\t\t\t\tself.handleError(mod, e)\n\n\t\t\tself.grad.append(mod.grad)\n\n\n\tdef gradShapeFrom(self, shapes):\n\t\tinshapes = []\n\n\t\tfor i, mod in enumerate(self.graph):\n\t\t\tinshapes.append(mod.gradShapeFrom(shapes[i]))\n\n\t\treturn inshapes\n\n\n\tdef updateGrad(self, grad):\n\t\tassert False\n\n\ndef unittest():\n\tfrom PuzzleLib.Containers.Sequential import Sequential\n\tfrom PuzzleLib.Modules import Linear, Activation, sigmoid, Identity, Concat\n\n\tdata1 = gpuarray.to_gpu(np.random.randn(128, 128).astype(np.float32))\n\tdata2 = gpuarray.to_gpu(np.random.randn(128, 16).astype(np.float32))\n\tdata3 = gpuarray.to_gpu(np.random.randn(128, 32).astype(np.float32))\n\n\tseq = Sequential()\n\tseq.append(Linear(128, 64))\n\tseq.append(Activation(sigmoid))\n\n\tparallel = Parallel()\n\tparallel.append(seq)\n\tparallel.append(Identity())\n\tparallel.append(Identity())\n\n\tconcat = Concat(axis=1)\n\n\tparallel([data1, data2, data3])\n\tconcat(parallel.data)\n\n\tassert np.allclose(data2.get(), concat.data.get()[:, 64:64 + 16])\n\n\tgrad = gpuarray.to_gpu(np.random.randn(128, 112).astype(np.float32))\n\tconcat.backward(grad)\n\tparallel.backward(concat.grad)\n\n\tassert np.allclose(grad.get()[:, 64:64 + 16], parallel.grad[1].get())\n\n\tparallel = parallel[::2]\n\tparallel([data1, data3])\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Containers/Sequential.py,5,"b'import numpy as np\n\nfrom PuzzleLib import Config\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Modules.Module import ModuleError\nfrom PuzzleLib.Containers.Container import ContainerError, Container\n\n\nclass Sequential(Container):\n\tdef __init__(self, name=None):\n\t\tsuper().__init__(name)\n\t\tself.graph = []\n\n\n\t@property\n\tdef gradUsesOutData(self):\n\t\tif len(self.graph) == 0:\n\t\t\treturn False\n\n\t\tindex = -1\n\t\tmod = self.graph[index]\n\n\t\twhile mod.movesData:\n\t\t\tindex -= 1\n\t\t\tmod = self.graph[index]\n\n\t\treturn mod.gradUsesOutData\n\n\n\t@gradUsesOutData.setter\n\tdef gradUsesOutData(self, val):\n\t\tpass\n\n\n\t@property\n\tdef inplace(self):\n\t\tfwdinp = True\n\n\t\tfor mod in self.graph:\n\t\t\tif mod.movesData:\n\t\t\t\tcontinue\n\t\t\telif getattr(mod, ""inplace"", False):\n\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\tfwdinp = False\n\t\t\t\tbreak\n\n\t\tbwdinp = True\n\n\t\tfor mod in reversed(self.graph):\n\t\t\tif mod.movesGrad:\n\t\t\t\tcontinue\n\t\t\telif getattr(mod, ""inplace"", False):\n\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\tbwdinp = False\n\t\t\t\tbreak\n\n\t\treturn fwdinp or bwdinp\n\n\n\tdef getBlueprint(self):\n\t\tblueprint = super().getBlueprint()\n\t\tblueprint[""graph""] = [mod.name for mod in self.graph]\n\n\t\treturn blueprint\n\n\n\tdef append(self, mod, acquire=True):\n\t\tif len(self.graph) > 0:\n\t\t\tself.checkModulesCompatibility(self.graph[-1], mod)\n\n\t\tsuper().append(mod, acquire)\n\t\tself.graph.append(mod)\n\n\t\treturn self\n\n\n\tdef extend(self, container, acquire=True):\n\t\tif isinstance(container, Sequential):\n\t\t\tcontainer = container.graph\n\n\t\tfor mod in container:\n\t\t\tself.append(mod, acquire)\n\n\n\tdef pop(self):\n\t\tmod = self.graph.pop()\n\t\tsuper().removeModule(mod)\n\n\t\treturn mod\n\n\n\tdef insert(self, mod, index):\n\t\tif index > 0:\n\t\t\tself.checkModulesCompatibility(self.graph[index - 1], mod)\n\n\t\tsuper().append(mod)\n\t\tself.graph.insert(index, mod)\n\n\n\tdef insertAfter(self, mod, name):\n\t\tindex = self.getModuleIndex(name)\n\t\tself.checkModulesCompatibility(self.graph[index], mod)\n\n\t\tsuper().append(mod)\n\t\tself.graph.insert(index + 1, mod)\n\n\n\tdef checkModulesCompatibility(self, mod1, mod2):\n\t\tif Config.disableModuleCompatChecks:\n\t\t\treturn\n\n\t\tif not getattr(mod2, ""inplace"", False):\n\t\t\treturn\n\n\t\tif not mod1.gradUsesOutData:\n\t\t\tif not mod1.movesData:\n\t\t\t\treturn\n\t\t\telse:\n\t\t\t\tindex = self.getModuleIndex(mod1.name) - 1\n\n\t\t\t\twhile index >= 0:\n\t\t\t\t\tmod1 = self.getByIndex(index)\n\t\t\t\t\tindex -= 1\n\n\t\t\t\t\tif mod1.movesData:\n\t\t\t\t\t\tcontinue\n\n\t\t\t\t\tif not mod1.gradUsesOutData:\n\t\t\t\t\t\treturn\n\t\t\t\t\telse:\n\t\t\t\t\t\tbreak\n\n\t\t\t\tif index < 0:\n\t\t\t\t\treturn\n\n\t\traise ContainerError(\n\t\t\t""%s: Can\'t insert inplace module %s after module %s (gradient uses outdata)"" % (self, mod2, mod1)\n\t\t)\n\n\n\tdef __getitem__(self, item):\n\t\tif isinstance(item, str):\n\t\t\treturn super().__getitem__(item)\n\n\t\telif isinstance(item, int):\n\t\t\treturn self.graph[item]\n\n\t\telif isinstance(item, slice):\n\t\t\tassert item.step == 1 or item.step is None\n\n\t\t\tseq = Sequential()\n\t\t\tseq.extend(self.graph[item.start:item.stop:item.step])\n\n\t\t\treturn seq\n\n\t\telse:\n\t\t\traise NotImplementedError(type(item).__name__)\n\n\n\tdef getByIndex(self, index):\n\t\treturn self.graph[index]\n\n\n\tdef getModuleIndex(self, name):\n\t\tindex = None\n\t\tfor i, mod in enumerate(self.graph):\n\t\t\tif mod.name == name:\n\t\t\t\tindex = i\n\t\t\t\tbreak\n\n\t\tif index is None:\n\t\t\traise ContainerError(""%s: Module %s not found"" % (self, name))\n\n\t\treturn index\n\n\n\tdef optimizeForShape(self, shape, memlimit=None):\n\t\tfor mod in self.graph:\n\t\t\tmod.optimizeForShape(shape, memlimit)\n\t\t\tshape = mod.dataShapeFrom(shape)\n\n\n\tdef updateData(self, data):\n\t\tfor i, mod in enumerate(self.graph):\n\t\t\ttry:\n\t\t\t\tmod(data)\n\n\t\t\texcept ModuleError as e:\n\t\t\t\traise ModuleError(""%s:\\nData error in module %d (%s):\\n%s"" % (self, i, mod, e))\n\n\t\t\texcept Exception as e:\n\t\t\t\tself.handleError(mod, e)\n\n\t\t\tdata = mod.data\n\n\t\tif len(self.graph) == 0:\n\t\t\tself.data = data\n\t\telse:\n\t\t\tself.data = self.graph[-1].data\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tfor mod in self.graph:\n\t\t\tshape = mod.dataShapeFrom(shape)\n\n\t\treturn shape\n\n\n\tdef backward(self, grad, updParamGrads=True, updGrad=True, scale=1.0, momentum=1.0):\n\t\tfor i, mod in enumerate(reversed(self.graph)):\n\t\t\ttry:\n\t\t\t\tif i < len(self.graph):\n\t\t\t\t\tmod.backward(grad, updParamGrads=updParamGrads, scale=scale, momentum=momentum)\n\t\t\t\telse:\n\t\t\t\t\tmod.backward(grad, updParamGrads=updParamGrads, updGrad=updGrad, scale=scale, momentum=momentum)\n\n\t\t\texcept ModuleError as e:\n\t\t\t\traise ModuleError(""%s:\\nGrad error in module %d (%s):\\n%s"" % (self, len(self.graph)-1 - i, mod, e))\n\n\t\t\texcept Exception as e:\n\t\t\t\tself.handleError(mod, e)\n\n\t\t\tgrad = mod.grad\n\n\t\tif len(self.graph) == 0:\n\t\t\tself.grad = grad\n\t\telse:\n\t\t\tself.grad = self.graph[0].grad\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tfor mod in reversed(self.graph):\n\t\t\tshape = mod.gradShapeFrom(shape)\n\n\t\treturn shape\n\n\n\tdef updateGrad(self, grad):\n\t\tassert False\n\n\ndef unittest():\n\tsimpleNetTest()\n\tcomplexNetTest()\n\n\ndef simpleNetTest():\n\tfrom PuzzleLib.Modules import Linear, Activation, sigmoid\n\n\tdata = gpuarray.to_gpu(np.random.randn(128, 128).astype(np.float32))\n\n\tseq = Sequential()\n\n\tseq.append(Linear(128, 64))\n\tseq.append(Activation(sigmoid))\n\n\tseq.append(Linear(64, 32))\n\tseq.append(Activation(sigmoid))\n\n\tseq(data)\n\tassert seq.data.shape == (128, 32)\n\n\tgrad = gpuarray.to_gpu(np.random.randn(128, 32).astype(np.float32))\n\tseq.backward(grad)\n\tseq.updateParams(1e-4)\n\tassert seq.grad.shape == data.shape\n\n\tdata = gpuarray.to_gpu(np.random.randn(64, 128).astype(np.float32))\n\tseq = seq[:2]\n\tseq(data)\n\tassert seq.data.shape == (64, 64)\n\n\ndef complexNetTest():\n\tfrom PuzzleLib.Modules import Conv2D, MaxPool2D, Activation, relu, Flatten\n\n\tdata = gpuarray.to_gpu(np.random.randn(128, 3, 150, 150).astype(np.float32))\n\n\tseq = Sequential()\n\n\tseq.append(Conv2D(3, 16, 11))\n\tseq.append(MaxPool2D())\n\tseq.append(Activation(relu))\n\n\tseq.append(Conv2D(16, 16, 5))\n\tseq.append(MaxPool2D())\n\tseq.append(Activation(relu))\n\n\tseq.append(Flatten())\n\n\tseq(data)\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*seq.data.shape).astype(np.float32))\n\tseq.backward(grad)\n\tseq.updateParams(1e-4)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Containers/__init__.py,0,b'from PuzzleLib.Containers.Container import Container\nfrom PuzzleLib.Containers.Graph import Graph\nfrom PuzzleLib.Containers.Parallel import Parallel\nfrom PuzzleLib.Containers.Sequential import Sequential\n'
Cost/Abs.py,10,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\nfrom PuzzleLib.Backend import Blas\nfrom PuzzleLib.Backend.Kernels.ElementWise import l1gradKer\n\nfrom PuzzleLib.Cost.Cost import Cost\n\n\nclass Abs(Cost):\n\tdef calcGrad(self, pred, target):\n\t\tgrad = gpuarray.empty(pred.shape, dtype=np.float32, allocator=memPool)\n\t\tnorm = 1.0 / np.prod(target.shape)\n\n\t\tl1gradKer(grad, pred, target, norm)\n\n\t\treturn grad\n\n\n\tdef calcError(self, pred, target):\n\t\tdiff = Blas.addVectorToVector(pred.ravel(), target.ravel(), alpha=1.0, beta=-1.0)\n\n\t\tself.devErr.fill(Blas.vectorL1Norm(diff) / np.prod(pred.shape[1:]))\n\t\tself.accumErr += self.devErr\n\n\n\tdef calcVal(self, pred, target):\n\t\tdiff = Blas.addVectorToVector(pred.ravel(), target.ravel(), alpha=1.0, beta=-1.0)\n\t\terror = Blas.vectorL1Norm(diff) / np.prod(target.shape)\n\n\t\treturn error\n\n\n\tdef checkDataShape(self, pred, target):\n\t\tassert pred.shape[1:] == target.shape[1:]\n\n\n\tdef checkValDataShape(self, pred, target):\n\t\tassert pred.shape[1:] == target.shape[1:]\n\n\ndef unittest():\n\terrorTest()\n\tvalTest()\n\n\ndef errorTest():\n\tpred = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\ttarget = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\n\tabscost = Abs()\n\tabscost(pred, target)\n\n\tassert np.isclose(abscost.error, np.linalg.norm((target.get() - pred.get()).ravel(), ord=1) / np.prod(target.shape))\n\n\ndef valTest():\n\tpred = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\ttarget = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\n\tabscost = Abs()\n\terror = abscost.validate(pred, target)\n\n\tassert np.isclose(error, np.linalg.norm((target.get() - pred.get()).ravel(), ord=1) / np.prod(target.shape))\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cost/BCE.py,18,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\nfrom PuzzleLib.Backend.Kernels.Costs import getAccuracyKernel, bceKer\n\nfrom PuzzleLib.Cost.Cost import CostError, Cost\n\n\nclass BCE(Cost):\n\tdef calcGrad(self, scores, labels):\n\t\tif Config.verifyData:\n\t\t\tself.verifyLabels(labels)\n\n\t\tgrad = gpuarray.empty(scores.shape, dtype=np.float32, allocator=memPool)\n\t\tself.devErr.fill(0.0)\n\n\t\tbceKer(scores, labels, self.devErr, grad, scores.shape[0], np.prod(scores.shape[1:]))\n\t\treturn grad\n\n\n\tdef calcError(self, scores, labels):\n\t\tself.accumErr += self.devErr\n\n\n\tdef calcVal(self, scores, labels):\n\t\tif Config.verifyData:\n\t\t\tself.verifyLabels(labels)\n\n\t\tcalcBCEAccuracy = getAccuracyKernel(""calcBCEAccuracy"")\n\t\terror = calcBCEAccuracy(scores, labels, allocator=memPool).get() / np.prod(scores.shape)\n\t\treturn error\n\n\n\tdef checkDataShape(self, scores, labels):\n\t\tself.checkShapeCompatibility(scores, labels)\n\n\n\tdef checkValDataShape(self, scores, labels):\n\t\tself.checkShapeCompatibility(scores, labels)\n\n\n\t@staticmethod\n\tdef checkShapeCompatibility(scores, labels):\n\t\tassert labels.dtype == np.int32\n\n\t\tif scores.ndim == 2 and scores.shape[1] == 1:\n\t\t\tassert labels.ndim == 1\n\t\telse:\n\t\t\tassert np.prod(scores.shape[1:]) == np.prod(labels.shape[1:])\n\n\n\t@staticmethod\n\tdef verifyLabels(labels):\n\t\tmn, mx = gpuarray.minimum(labels).get(), gpuarray.maximum(labels).get()\n\t\tif mn < 0:\n\t\t\traise CostError(""BCE labels verification failed, found index %s (< 0)"" % mn)\n\n\t\tif mx > 1:\n\t\t\traise CostError(""BCE labels verification failed, found index %s (> 1)"" % mx)\n\n\ndef unittest():\n\terrorTest()\n\tvalTest()\n\tverifyLabelsTest()\n\n\ndef errorTest():\n\tscores = gpuarray.to_gpu(np.random.randn(20, 1, 4, 4).astype(np.float32))\n\tlabels = gpuarray.to_gpu(np.random.randint(low=0, high=2, size=(20, 4, 4)).astype(np.int32))\n\n\tbce = BCE()\n\terror, grad = bce(scores, labels)\n\n\thostSigm = 1.0 / (1.0 + np.exp(-scores.get()))\n\n\thostGrad = (labels.get().flatten() - hostSigm.flatten()).reshape(*scores.shape) / np.prod(scores.shape)\n\tassert np.allclose(hostGrad, grad.get())\n\n\thostError = -np.sum(labels.get().flatten() * np.log(hostSigm).flatten() +\n\t\t\t\t\t\t(1.0 - labels.get().flatten()) * np.log(1.0 - hostSigm).flatten()) / np.prod(hostSigm.shape)\n\tassert np.isclose(hostError, error)\n\n\ndef valTest():\n\tscores = gpuarray.to_gpu(np.array([[-1.0]] * 500 + [[1.0]] * 500, dtype=np.float32))\n\tlabels = gpuarray.to_gpu(np.array([0] * 490 + [1] * 510, dtype=np.int32))\n\n\tbce = BCE()\n\terror = bce.validate(scores, labels)\n\n\tprint(""Validation error: %s"" % error)\n\tassert np.isclose(error, np.sum(np.equal((scores.get()<=0.0), labels.get()[:,np.newaxis]), axis=0)/scores.shape[0])\n\n\ndef verifyLabelsTest():\n\tscores = gpuarray.to_gpu(np.random.randn(20, 1).astype(np.float32))\n\tlabels = gpuarray.to_gpu(np.random.randint(low=0, high=3, size=(20, )).astype(np.int32))\n\n\tbce = BCE()\n\n\tConfig.verifyData = True\n\n\ttry:\n\t\tbce(scores, labels)\n\n\texcept CostError as e:\n\t\tprint(""Caught labels verification error: %s"" % e)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cost/CTC.py,25,"b'import math, random\n\nimport numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Kernels.Costs import ctcLoss, ctcLossTest\n\nfrom PuzzleLib.Cost.Cost import Cost\n\n\nclass CTC(Cost):\n\tdef __init__(self, blank, vocabsize=None, normalized=False):\n\t\tsuper().__init__()\n\t\tself.normalized = normalized\n\n\t\tif vocabsize is not None:\n\t\t\tassert 0 <= blank <= vocabsize\n\n\t\tself.vocabsize = vocabsize\n\t\tself.blank = blank\n\n\n\tdef calcGrad(self, pred, target):\n\t\tdata, datalen = pred\n\t\tlabels, lengths = target\n\n\t\tself.devErr.fill(0.0)\n\n\t\t_, grad = ctcLoss(data, datalen, labels, lengths, self.blank, error=self.devErr, normalized=self.normalized)\n\t\treturn grad\n\n\n\tdef calcError(self, scores, labels):\n\t\tself.accumErr += self.devErr\n\n\n\tdef calcVal(self, pred, target):\n\t\traise NotImplementedError()\n\n\n\tdef checkDataShape(self, pred, target):\n\t\tdata, datalen = pred\n\t\tlabels, lengths = target\n\n\t\tassert datalen.dtype == labels.dtype and labels.dtype == lengths.dtype and lengths.dtype == np.int32\n\t\tassert datalen.shape[0] == lengths.shape[0] and lengths.shape[0] == data.shape[1]\n\n\t\tif self.vocabsize is not None:\n\t\t\tassert data.shape[2] == self.vocabsize\n\n\n\tdef checkValDataShape(self, pred, target):\n\t\tpass\n\n\n\tdef getBatchsize(self, pred):\n\t\treturn pred[0].shape[1]\n\n\ndef unittest():\n\tsmallTest()\n\tmediumTest()\n\trandomTest()\n\n\ndef smallTest():\n\thostData = np.array([[[0.1, 0.6, 0.1, 0.1, 0.1]], [[0.1, 0.1, 0.6, 0.1, 0.1]]], dtype=np.float32)\n\n\tdata = gpuarray.to_gpu(hostData)\n\tdatalen = gpuarray.to_gpu(np.array([2], dtype=np.int32))\n\n\tlabels = gpuarray.to_gpu(np.array([1, 2], dtype=np.int32))\n\tlengths = np.array([2], dtype=np.int32)\n\n\tctc = CTC(blank=4, vocabsize=5, normalized=True)\n\terror, grad = ctc([data, datalen], [labels, lengths])\n\n\thostScore = hostData[0, 0, 1] * hostData[1, 0, 2]\n\tassert np.isclose(math.exp(-error), hostScore)\n\n\ndef mediumTest():\n\thostData = np.array([\n\t\t[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553],\n\t\t [0.30176, 0.28562, 0.0831517, 0.0862751, 0.0816851, 0.161508]],\n\n\t\t[[0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436],\n\t\t [0.24082, 0.397533, 0.0557226, 0.0546814, 0.0557528, 0.19549]],\n\n\t\t[[0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688],\n\t\t [0.230246, 0.450868, 0.0389607, 0.038309, 0.0391602, 0.202456]],\n\n\t\t[[0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533],\n\t\t [0.280884, 0.429522, 0.0326593, 0.0339046, 0.0326856, 0.190345]],\n\n\t\t[[0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107],\n\t\t [0.423286, 0.315517, 0.0338439, 0.0393744, 0.0339315, 0.154046]]\n\t], dtype=np.float32)\n\n\tdata = gpuarray.to_gpu(hostData)\n\tdatalen = gpuarray.to_gpu(np.array([5, 5], dtype=np.int32))\n\n\tlabels = gpuarray.to_gpu(np.array([\n\t\t0, 1, 2, 1, 0,\n\t\t0, 1, 1, 0\n\t], dtype=np.int32))\n\n\tlengths = np.array([5, 4], dtype=np.int32)\n\n\thostGrad = -np.array([\n\t\t[[-0.366234, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553],\n\t\t [-0.69824, 0.28562, 0.0831517, 0.0862751, 0.0816851, 0.161508]],\n\n\t\t[[0.111121, -0.411608, 0.278779, 0.0055756, 0.00569609, 0.010436],\n\t\t [0.24082, -0.602467, 0.0557226, 0.0546814, 0.0557528, 0.19549]],\n\n\t\t[[0.0357786, 0.633813, -0.678582, 0.00249248, 0.00272882, 0.0037688],\n\t\t [0.230246, 0.450868, 0.0389607, 0.038309, 0.0391602, -0.797544]],\n\n\t\t[[0.0663296, -0.356151, 0.280111, 0.00283995, 0.0035545, 0.00331533],\n\t\t [0.280884, -0.570478, 0.0326593, 0.0339046, 0.0326856, 0.190345]],\n\n\t\t[[-0.541765, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107],\n\t\t [-0.576714, 0.315517, 0.0338439, 0.0393744, 0.0339315, 0.154046]]\n\t], dtype=np.float32)\n\n\tctc = CTC(vocabsize=6, blank=5, normalized=True)\n\terror, grad = ctc([data, datalen], [labels, lengths])\n\n\thostScore = np.empty((2, ), dtype=np.float32)\n\n\thostScore[0] = -math.log(\n\t\thostData[0, 0, 0] * hostData[1, 0, 1] * hostData[2, 0, 2] * hostData[3, 0, 1] * hostData[4, 0, 0]\n\t)\n\thostScore[1] = 5.42262\n\n\thostError = np.mean(hostScore)\n\n\tassert np.isclose(hostError, error)\n\tassert np.allclose(hostGrad, grad.get())\n\n\ndef randomTest():\n\ttimes, batchsize, vocabsize = 20, 3, 6\n\thostData, hostDataLen, hostLabels, lengths = createData(times, batchsize, vocabsize)\n\n\tdata, datalen, labels = gpuarray.to_gpu(hostData), gpuarray.to_gpu(hostDataLen), gpuarray.to_gpu(hostLabels)\n\tblank = 0\n\n\tctc = CTC(blank=0, vocabsize=vocabsize)\n\n\terror, grad = ctc([data, datalen], [labels, lengths])\n\thostError, hostGrad, _ = ctcLossTest(hostData, hostDataLen, hostLabels, lengths, blank)\n\n\tassert np.isclose(hostError / batchsize, error)\n\tassert np.allclose(hostGrad, grad.get(), atol=1e-5)\n\n\ndef createData(times, batchsize, vocabsize):\n\tdata = np.random.randn(times, batchsize, vocabsize).astype(np.float32)\n\tdatalen = np.array([times] * batchsize, dtype=np.int32)\n\n\tlengths = np.array([random.randint(a=times // 4, b=times // 2 - 1) for _ in range(batchsize)], dtype=np.int32)\n\tlabels = np.concatenate([\n\t\tnp.array([random.randint(a=1, b=vocabsize - 1) for _ in range(lengths[b])], dtype=np.int32)\n\t\tfor b in range(batchsize)\n\t])\n\n\treturn data, datalen, labels, lengths\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cost/Cost.py,2,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\n\n\nclass CostError(Exception):\n\tpass\n\n\nclass Cost:\n\tdef __init__(self):\n\t\tself.accumErr = gpuarray.empty((), dtype=np.float32)\n\t\tself.devErr = gpuarray.empty((), dtype=np.float32)\n\n\t\tself.error = None\n\t\tself.valError = None\n\t\tself.grad = None\n\n\t\tself.batchsize = None\n\t\tself.numOfSamples = None\n\n\t\tself.dirty = True\n\t\tself.resetAccumulator()\n\n\n\tdef resetAccumulator(self):\n\t\tself.resetDeviceAccumulator()\n\n\t\tself.batchsize = 0\n\t\tself.numOfSamples = 0\n\n\n\tdef updateState(self, samples):\n\t\tself.batchsize = samples\n\t\tself.numOfSamples += samples\n\n\n\tdef resetDeviceAccumulator(self):\n\t\tself.accumErr.fill(0.0)\n\n\n\tdef getError(self):\n\t\tif self.dirty:\n\t\t\tself.error = self.devErr.get() / self.batchsize\n\t\t\tself.dirty = False\n\n\t\treturn self.error\n\n\n\tdef getMeanError(self):\n\t\treturn self.accumErr.get() / self.numOfSamples\n\n\n\tdef getValError(self):\n\t\treturn self.valError\n\n\n\tdef __call__(self, pred, target, queryError=True):\n\t\tif isinstance(target, gpuarray.GPUArray) and isinstance(pred, gpuarray.GPUArray):\n\t\t\tassert pred.shape[0] == target.shape[0]\n\n\t\tself.checkDataShape(pred, target)\n\t\tself.reset()\n\n\t\tself.grad = self.calcGrad(pred, target)\n\t\tself.calcError(pred, target)\n\t\tself.dirty = True\n\n\t\tself.updateState(self.getBatchsize(pred))\n\n\t\tif queryError:\n\t\t\tself.error = self.getError()\n\n\t\tif queryError:\n\t\t\treturn self.error, self.grad\n\t\telse:\n\t\t\treturn self.grad\n\n\n\tdef calcError(self, pred, target):\n\t\traise NotImplementedError()\n\n\n\tdef calcGrad(self, pred, target):\n\t\traise NotImplementedError()\n\n\n\tdef validate(self, pred, target):\n\t\tif isinstance(target, gpuarray.GPUArray) and isinstance(pred, gpuarray.GPUArray):\n\t\t\tassert pred.shape[0] == target.shape[0]\n\n\t\tself.checkValDataShape(pred, target)\n\t\tself.valError = self.calcVal(pred, target)\n\n\t\treturn self.valError\n\n\n\tdef calcVal(self, pred, target):\n\t\traise NotImplementedError()\n\n\n\tdef reset(self):\n\t\tself.error = None\n\t\tself.valError = None\n\n\t\tself.grad = None\n\n\n\tdef checkDataShape(self, pred, target):\n\t\tpass\n\n\n\tdef checkValDataShape(self, pred, target):\n\t\tpass\n\n\n\tdef getBatchsize(self, pred):\n\t\treturn pred.shape[0]\n'"
Cost/CrossEntropy.py,42,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\nfrom PuzzleLib.Backend.Kernels.Costs import getAccuracyKernel, crossEntropyKernel\nfrom PuzzleLib.Backend.Kernels.MatVec import argmax\nfrom PuzzleLib.Backend.Kernels.MatVecBatch import argmaxBatch\n\nfrom PuzzleLib.Cost.Cost import CostError, Cost\n\n\nclass CrossEntropy(Cost):\n\tdef __init__(self, maxlabels=None, weights=None):\n\t\tsuper().__init__()\n\n\t\tself.maxlabels = maxlabels\n\t\tself.mostProb = None\n\n\t\tif isinstance(weights, np.ndarray):\n\t\t\tweights = gpuarray.to_gpu(weights)\n\n\t\tself.weights = weights\n\n\n\tdef calcGrad(self, scores, labels):\n\t\tif Config.verifyData:\n\t\t\tself.verifyLabels(scores, labels)\n\n\t\tself.devErr, grad = crossEntropyKernel(scores, labels, weights=self.weights, error=self.devErr)\n\t\treturn grad\n\n\n\tdef calcError(self, scores, labels):\n\t\tself.accumErr += self.devErr\n\n\n\tdef calcVal(self, scores, labels):\n\t\tif Config.verifyData:\n\t\t\tself.verifyLabels(scores, labels)\n\n\t\tif scores.ndim == 2:\n\t\t\tself.mostProb = argmax(scores, axis=1)\n\n\t\telse:\n\t\t\tscores = scores.reshape(*scores.shape[:2], int(np.prod(scores.shape[2:])))\n\t\t\tself.mostProb = argmaxBatch(scores, axis=1).reshape(labels.shape)\n\n\t\tcalcAccuracy = getAccuracyKernel(""calcAccuracy"")\n\t\terror = calcAccuracy(self.mostProb, labels, allocator=memPool).get() / np.prod(labels.shape)\n\n\t\treturn error\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\t\tself.mostProb = None\n\n\n\tdef checkDataShape(self, scores, labels):\n\t\tassert scores.ndim > 1 and labels.ndim == scores.ndim - 1\n\t\tassert labels.dtype == np.int32\n\n\t\tif scores.ndim > 2:\n\t\t\tassert scores.shape[2:] == labels.shape[1:]\n\n\t\tif self.maxlabels:\n\t\t\tassert scores.shape[1] == self.maxlabels\n\n\t\tif self.weights is not None:\n\t\t\tassert self.weights.shape[0] == scores.shape[1]\n\n\n\tdef checkValDataShape(self, scores, labels):\n\t\tassert scores.ndim > 1 and labels.ndim == scores.ndim - 1\n\t\tassert labels.dtype == np.int32\n\n\t\tif scores.ndim > 2:\n\t\t\tassert scores.shape[2:] == labels.shape[1:]\n\n\t\tif self.maxlabels:\n\t\t\tassert scores.shape[1] == self.maxlabels\n\n\n\t@staticmethod\n\tdef verifyLabels(scores, labels):\n\t\tmn, mx = gpuarray.minimum(labels).get(), gpuarray.maximum(labels).get()\n\t\tif mn < 0:\n\t\t\traise CostError(""Cross entropy labels verification failed, found index %s (< 0)"" % mn)\n\n\t\tif mx >= scores.shape[1]:\n\t\t\traise CostError(""Cross entropy labels verification failed, found index %s (> %s)"" %\n\t\t\t\t\t\t\t(mx, scores.shape[1] - 1))\n\n\ndef unittest():\n\terrorTest()\n\tvalTest()\n\n\twceErrorTest()\n\twceValTest()\n\n\tverifyLabelsTest()\n\n\ndef errorTest():\n\tscores = gpuarray.to_gpu(np.random.randn(20, 10, 3).astype(np.float32))\n\tlabels = gpuarray.to_gpu(np.random.randint(low=0, high=10, size=(20, 3)).astype(np.int32))\n\n\tentr = CrossEntropy()\n\terror, grad = entr(scores, labels)\n\n\tdef softmax(w):\n\t\te = np.exp(w - np.amax(w))\n\t\tdist = e / np.sum(e)\n\t\treturn dist\n\n\tdef crossEntropy(smax, target):\n\t\tsmax = np.moveaxis(smax, 1, -1).reshape(-1, smax.shape[1])\n\t\ttarget = target.flatten()\n\t\terr = np.sum(np.log(np.array([smax[i, target[i]] for i in range(smax.shape[0])])))\n\n\t\treturn -err / target.size\n\n\tdef crossEntropyGrad(target, smax):\n\t\treturn np.array([(target == i) - smax[i] for i in range(smax.shape[0])])\n\n\thostSoftmax = np.apply_along_axis(softmax, 1, scores.get())\n\n\thostGrad = np.vstack([crossEntropyGrad(labels.get()[i], hostSoftmax[i]) / scores.shape[0]\n\t\t\t\t\t\t  for i in range(scores.shape[0])]).reshape(*hostSoftmax.shape)\n\n\tassert np.allclose(hostGrad, grad.get())\n\n\thostError = crossEntropy(hostSoftmax, labels.get())\n\tassert np.isclose(hostError, error)\n\n\ndef valTest():\n\tscores = gpuarray.to_gpu(np.array([[0.1, 0.0, 0.0, -1.0]] * 150 + [[-0.2, 1.0, 0.0, 0.5]] * 150 +\n\t\t\t\t\t\t\t\t\t  [[0.0, -1.0, 2.0, 1.5]] * 300 + [[0.0, 0.0, -6.0, 1.0]] * 400, dtype=np.float32))\n\tscores = scores.reshape(*scores.shape, 1)\n\n\tlabels = gpuarray.to_gpu(np.array([0] * 100 + [1] * 200 + [2] * 300 + [3] * 400, dtype=np.int32))\n\tlabels = labels.reshape(*labels.shape, 1)\n\n\tentr = CrossEntropy()\n\terror = entr.validate(scores, labels)\n\tprint(""Validation error: %s"" % error)\n\tassert np.allclose(np.argmax(scores.get(), axis=1), entr.mostProb.get())\n\n\ndef wceErrorTest():\n\tscores = gpuarray.to_gpu(np.random.randn(20, 10, 3).astype(np.float32))\n\tlabels = gpuarray.to_gpu(np.random.randint(low=0, high=10, size=(20, 3)).astype(np.int32))\n\n\tweights = np.random.random_sample(10).astype(np.float32)\n\tweights /= np.sum(weights)\n\n\tentr = CrossEntropy(weights=weights)\n\terror, grad = entr(scores, labels)\n\n\tdef softmax(w):\n\t\te = np.exp(w - np.amax(w))\n\t\tdist = e / np.sum(e)\n\t\treturn dist\n\n\tdef weightedCrossEntropy(smax, w, target):\n\t\tsmax = np.moveaxis(smax, 1, -1).reshape(-1, smax.shape[1])\n\t\ttarget = target.flatten()\n\t\terr = np.sum(np.array([w[target[i]] * np.log(smax[i, target[i]]) for i in range(smax.shape[0])]))\n\t\treturn -err / smax.shape[0]\n\n\tdef weightedCrossEntropyGrad(target, w, smax):\n\t\treturn np.array([w[i] * ((target == i) - smax[i]) for i in range(smax.shape[0])])\n\n\thostSoftmax = np.apply_along_axis(softmax, 1, scores.get())\n\n\thostGrad = np.vstack([weightedCrossEntropyGrad(labels.get()[i], weights, hostSoftmax[i]) / scores.shape[0]\n\t\t\t\t\t\t  for i in range(scores.shape[0])]).reshape(*hostSoftmax.shape)\n\n\tassert np.allclose(hostGrad, grad.get())\n\n\thostError = np.array(weightedCrossEntropy(hostSoftmax, weights, labels.get()))\n\tassert np.isclose(hostError, error)\n\n\ndef wceValTest():\n\tscores = gpuarray.to_gpu(np.array([[0.1, 0.0, 0.0, -1.0]] * 150 + [[-0.2, 1.0, 0.0, 0.5]] * 150 +\n\t\t\t\t\t\t\t\t\t  [[0.0, -1.0, 2.0, 1.5]] * 300 + [[0.0, 0.0, -6.0, 1.0]] * 400, dtype=np.float32))\n\tscores = scores.reshape(*scores.shape, 1)\n\n\tlabels = gpuarray.to_gpu(np.array([0] * 100 + [1] * 200 + [2] * 300 + [3] * 400, dtype=np.int32))\n\tlabels = labels.reshape(*labels.shape, 1)\n\n\tweights = np.random.random_sample(10).astype(np.float32)\n\tweights /= np.sum(weights)\n\n\tentr = CrossEntropy(weights=weights)\n\terror = entr.validate(scores, labels)\n\n\tprint(""Validation error: %s"" % error)\n\tassert np.allclose(np.argmax(scores.get(), axis=1), entr.mostProb.get())\n\n\ndef verifyLabelsTest():\n\tscores = gpuarray.to_gpu(np.random.randn(20, 10).astype(np.float32))\n\tlabels = gpuarray.to_gpu(np.random.randint(low=0, high=11, size=(20, )).astype(np.int32))\n\n\tentr = CrossEntropy()\n\n\tConfig.verifyData = True\n\n\ttry:\n\t\tentr(scores, labels)\n\n\texcept CostError as e:\n\t\tprint(""Caught labels verification error: %s"" % e)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cost/Hinge.py,13,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\nfrom PuzzleLib.Backend.Kernels.Costs import hingeKer\n\nfrom PuzzleLib.Cost.Cost import CostError, Cost\n\n\nclass Hinge(Cost):\n\tdef calcGrad(self, scores, labels):\n\t\tif Config.verifyData:\n\t\t\tself.verifyLabels(labels)\n\n\t\tgrad = gpuarray.empty(scores.shape, dtype=np.float32, allocator=memPool)\n\t\tself.devErr.fill(0.0)\n\n\t\thingeKer(scores, labels, self.devErr, grad, scores.shape[0], scores.shape[1])\n\t\treturn grad\n\n\n\tdef calcError(self, scores, labels):\n\t\tself.accumErr += self.devErr\n\n\n\tdef calcVal(self, scores, labels):\n\t\tif Config.verifyData:\n\t\t\tself.verifyLabels(labels)\n\n\t\tdiff = gpuarray.empty(scores.shape, dtype=np.float32, allocator=memPool)\n\t\tdevErr = gpuarray.zeros((), dtype=np.float32, allocator=memPool)\n\n\t\thingeKer(scores, labels, devErr, diff, scores.shape[0], scores.shape[1])\n\t\treturn devErr.get() / scores.shape[0]\n\n\n\tdef checkDataShape(self, scores, labels):\n\t\tassert scores.ndim == 2 and scores.shape == labels.shape\n\t\tassert labels.dtype == np.int32\n\n\n\tdef checkValDataShape(self, scores, labels):\n\t\tassert scores.ndim == 2 and scores.shape == labels.shape\n\t\tassert labels.dtype == np.int32\n\n\n\t@staticmethod\n\tdef verifyLabels(labels):\n\t\tmn, mx = gpuarray.minimum(labels).get(), gpuarray.maximum(labels).get()\n\t\tif mn < -1:\n\t\t\traise CostError(""Hinge labels verification failed, found index %s (< -1)"" % mn)\n\n\t\tif mx > 1:\n\t\t\traise CostError(""Hinge labels verification failed, found index %s (> 1)"" % mx)\n\n\ndef unittest():\n\terrorValTest()\n\tverifyLabelsTest()\n\n\ndef errorValTest():\n\tbatchsize, size = 20, 4\n\n\tscores = gpuarray.to_gpu(np.random.randn(batchsize, size).astype(np.float32))\n\tlabels = gpuarray.to_gpu((np.random.randint(low=0, high=2, size=(batchsize, size)) * 2 - 1).astype(np.int32))\n\n\thinge = Hinge()\n\terror, grad = hinge(scores, labels)\n\n\thostScores, hostLabels = scores.get(), labels.get()\n\n\thostGrad = np.empty(grad.shape, dtype=np.float32)\n\thostError = 0.0\n\n\tfor b in range(batchsize):\n\t\tfor n in range(size):\n\t\t\tval = hostLabels[b, n] * hostScores[b, n]\n\n\t\t\thostGrad[b, n] = hostLabels[b, n] / batchsize / size if val < 1.0 else 0.0\n\t\t\thostError += max(0.0, 1.0 - val) / batchsize / size\n\n\tassert np.allclose(hostGrad, grad.get())\n\tassert np.isclose(hostError, error)\n\n\terror = hinge.validate(scores, labels)\n\tassert np.isclose(hostError, error)\n\n\ndef verifyLabelsTest():\n\tbatchsize, size = 20, 4\n\n\tscores = gpuarray.to_gpu(np.random.randn(batchsize, size).astype(np.float32))\n\tlabels = gpuarray.to_gpu(np.random.randint(low=-2, high=3, size=(batchsize, size)).astype(np.int32))\n\n\thinge = Hinge()\n\n\tConfig.verifyData = True\n\n\ttry:\n\t\thinge(scores, labels)\n\n\texcept CostError as e:\n\t\tprint(""Caught labels verification error: %s"" % e)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cost/KLDivergence.py,23,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\nfrom PuzzleLib.Backend.Kernels.Costs import getAccuracyKernel\nfrom PuzzleLib.Backend.Dnn.Basic import softmaxNd\n\nfrom PuzzleLib.Cost.Cost import Cost\n\n\nclass KLDivergence(Cost):\n\tdef __init__(self, maxlabels=None, normTarget=False):\n\t\tsuper().__init__()\n\n\t\tself.maxlabels = maxlabels\n\t\tself.normTarget = normTarget\n\n\n\tdef calcGrad(self, pred, target):\n\t\tshape = pred.shape\n\t\tsoftmax = softmaxNd(pred.reshape(shape[0], int(np.prod(shape[1:])), 1, 1))\n\n\t\tif self.normTarget:\n\t\t\tshape = target.shape\n\t\t\ttarget = softmaxNd(target.reshape(shape[0], int(np.prod(shape[1:])), 1, 1))\n\n\t\tgrad = gpuarray.empty(pred.shape, dtype=np.float32, allocator=memPool)\n\n\t\tself.devErr = None\n\n\t\tgradnorm = 1.0 / softmax.shape[0]\n\n\t\tklDivergence = getAccuracyKernel(""klDivergence"")\n\t\tself.devErr = klDivergence(softmax, target, grad, gradnorm, allocator=memPool)\n\n\t\treturn grad\n\n\n\tdef calcError(self, pred, target):\n\t\tself.accumErr += self.devErr\n\n\n\tdef calcVal(self, pred, target):\n\t\tshape = pred.shape\n\t\tsoftmax = softmaxNd(pred.reshape(shape[0], int(np.prod(shape[1:])), 1, 1))\n\n\t\tif self.normTarget:\n\t\t\tshape = target.shape\n\t\t\ttarget = softmaxNd(target.reshape(shape[0], int(np.prod(shape[1:])), 1, 1))\n\n\t\tgrad = gpuarray.empty(pred.shape, dtype=np.float32, allocator=memPool)\n\n\t\tgradnorm = 1.0 / softmax.shape[0]\n\n\t\tklDivergence = getAccuracyKernel(""klDivergence"")\n\t\terror = klDivergence(softmax, target, grad, gradnorm, allocator=memPool)\n\n\t\treturn error.get() / shape[0]\n\n\n\tdef checkDataShape(self, pred, target):\n\t\tassert pred.shape[1:] == target.shape[1:]\n\n\t\tif self.maxlabels:\n\t\t\tassert pred.shape[1] == self.maxlabels\n\n\n\tdef checkValDataShape(self, pred, target):\n\t\tassert pred.shape[1:] == target.shape[1:]\n\n\t\tif self.maxlabels:\n\t\t\tassert pred.shape[1] == self.maxlabels\n\n\ndef unittest():\n\tdef softmax(w):\n\t\te = np.exp(w - np.amax(w))\n\t\tdist = e / np.sum(e)\n\t\treturn dist\n\n\tdef klDivergence(smax, target):\n\t\terror = np.sum(target * (np.log(target) - np.log(smax)))\n\t\treturn error / smax.shape[0]\n\n\tdef klDivergenceGrad(target, smax):\n\t\treturn target - smax\n\n\terrorTest(softmax, klDivergence, klDivergenceGrad)\n\tvalTest(softmax, klDivergence)\n\n\ndef errorTest(softmax, klDivergence, klDivergenceGrad):\n\tpred = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\ttarget = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\n\tdiv = KLDivergence(normTarget=True)\n\terror, grad = div(pred, target)\n\n\thostSoftmax = np.vstack([softmax(pred.get()[i]) for i in range(pred.shape[0])])\n\thostTarget = np.vstack([softmax(target.get()[i]) for i in range(target.shape[0])])\n\n\thostGrad = np.vstack([klDivergenceGrad(hostTarget[i], hostSoftmax[i]) / pred.shape[0]\n\t\t\t\t\t\t  for i in range(pred.shape[0])])\n\tassert np.allclose(hostGrad, grad.get())\n\n\thostError = np.array(klDivergence(hostSoftmax, hostTarget))\n\tassert np.isclose(hostError, error)\n\n\ndef valTest(softmax, klDivergence):\n\tpred = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\ttarget = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\n\tdiv = KLDivergence(normTarget=True)\n\terror = div.validate(pred, target)\n\n\thostSoftmax = np.vstack([softmax(pred.get()[i]) for i in range(pred.shape[0])])\n\thostTarget = np.vstack([softmax(target.get()[i]) for i in range(target.shape[0])])\n\n\thostError = np.array(klDivergence(hostSoftmax, hostTarget))\n\tassert np.isclose(hostError, error)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cost/L1Hinge.py,26,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\nfrom PuzzleLib.Backend.Kernels.ElementWise import absKer\nfrom PuzzleLib.Backend.Kernels.Costs import l1HingeKer, getAccuracyKernel\nfrom PuzzleLib.Backend import Blas\n\nfrom PuzzleLib.Cost.Cost import CostError, Cost\n\n\nclass L1Hinge(Cost):\n\tdef calcGrad(self, pair, labels):\n\t\tif Config.verifyData:\n\t\t\tself.verifyLabels(labels)\n\n\t\tg1 = gpuarray.empty(pair[0].shape, dtype=np.float32, allocator=memPool)\n\t\tg2 = gpuarray.empty(pair[1].shape, dtype=np.float32, allocator=memPool)\n\n\t\tself.devErr.fill(0.0)\n\n\t\tl1HingeKer(pair[0], pair[1], labels, self.devErr, g1, g2, pair[0].shape[0], pair[0].shape[1])\n\t\treturn [g1, g2]\n\n\n\tdef calcError(self, pair, labels):\n\t\tself.accumErr += self.devErr\n\n\n\tdef calcVal(self, pair, labels):\n\t\tif Config.verifyData:\n\t\t\tself.verifyLabels(labels)\n\n\t\tdiff = Blas.addVectorToVector(pair[0].ravel(), pair[1].ravel(), alpha=1.0, beta=-1.0).reshape(pair[0].shape)\n\t\tabsKer(diff, diff)\n\n\t\tdist = Blas.sumOnMatrix(diff, cols=False, alpha=1.0 / pair[0].shape[1])\n\n\t\tl1HingeAccuracy = getAccuracyKernel(""l1HingeAccuracy"")\n\t\terror = l1HingeAccuracy(dist, labels, allocator=memPool).get() / pair[0].shape[0]\n\n\t\treturn error\n\n\n\tdef checkDataShape(self, pair, labels):\n\t\tassert len(pair) == 2 and pair[0].shape == pair[1].shape and pair[0].dtype == pair[1].dtype\n\t\tassert pair[0].dtype == np.float32\n\t\tassert pair[0].ndim == 2\n\n\t\tassert labels.dtype == np.int32\n\n\n\tdef checkValDataShape(self, pair, labels):\n\t\tassert len(pair) == 2 and pair[0].shape == pair[1].shape and pair[0].dtype == pair[1].dtype\n\t\tassert pair[0].dtype == np.float32\n\t\tassert pair[0].ndim == 2\n\n\t\tassert labels.dtype == np.int32\n\n\n\tdef getBatchsize(self, pair):\n\t\treturn pair[0].shape[0]\n\n\n\t@staticmethod\n\tdef verifyLabels(labels):\n\t\tmn, mx = gpuarray.minimum(labels).get(), gpuarray.maximum(labels).get()\n\t\tif mn < 0:\n\t\t\traise CostError(""L1 Hinge labels verification failed, found index %s (< 0)"" % mn)\n\n\t\tif mx > 1:\n\t\t\traise CostError(""L1 Hinge labels verification failed, found index %s (> 1)"" % mx)\n\n\ndef unittest():\n\terrorTest()\n\tvalTest()\n\tverifyLabelsTest()\n\n\ndef errorTest():\n\tbatchsize, size = 20, 4\n\n\tx1 = gpuarray.to_gpu(np.random.randn(batchsize, size).astype(np.float32))\n\tx2 = gpuarray.to_gpu(np.random.randn(batchsize, size).astype(np.float32))\n\n\tlabels = gpuarray.to_gpu(np.random.randint(low=0, high=2, size=(batchsize, )).astype(np.int32))\n\n\tl1Hinge = L1Hinge()\n\terror, (g1, g2) = l1Hinge([x1, x2], labels)\n\n\thostX1, hostX2, hostLabels = x1.get(), x2.get(), labels.get()\n\n\thostG1, hostG2 = np.empty(g1.shape, dtype=np.float32), np.empty(g2.shape, dtype=np.float32)\n\thostError = 0.0\n\n\tfor b in range(batchsize):\n\t\tfor n in range(size):\n\t\t\tdiff = hostX1[b, n] - hostX2[b, n]\n\t\t\tsign = 1.0 if diff > 0.0 else -1.0\n\n\t\t\tif hostLabels[b] == 1:\n\t\t\t\thostG1[b, n] = sign / batchsize / size\n\t\t\t\thostG2[b, n] = -sign / batchsize / size\n\t\t\t\thostError += np.abs(diff) / batchsize / size\n\n\t\t\telse:\n\t\t\t\thostG1[b, n] = -sign / batchsize / size if np.abs(diff) < 1.0 else 0.0\n\t\t\t\thostG2[b, n] = sign / batchsize / size if np.abs(diff) < 1.0 else 0.0\n\t\t\t\thostError += max(0.0, 1.0 - np.abs(diff)) / batchsize / size\n\n\tassert np.allclose(hostG1, g1.get())\n\tassert np.allclose(hostG2, g2.get())\n\tassert np.isclose(hostError, error)\n\n\ndef valTest():\n\tbatchsize, size = 20, 4\n\n\tx1 = gpuarray.to_gpu(np.random.randn(batchsize, size).astype(np.float32))\n\tx2 = gpuarray.to_gpu(np.random.randn(batchsize, size).astype(np.float32))\n\n\tlabels = gpuarray.to_gpu(np.random.randint(low=0, high=2, size=(batchsize, )).astype(np.int32))\n\n\tl1Hinge = L1Hinge()\n\terror = l1Hinge.validate([x1, x2], labels)\n\n\thostX1, hostX2, hostLabels = x1.get(), x2.get(), labels.get()\n\n\tdist = np.linalg.norm(hostX1 - hostX2, axis=1, ord=1) / size\n\thostError = np.sum((dist <= 1.0) != labels.get()) / batchsize\n\n\tassert np.isclose(hostError, error)\n\n\ndef verifyLabelsTest():\n\tbatchsize, size = 20, 4\n\n\tx1 = gpuarray.to_gpu(np.random.randn(batchsize, size).astype(np.float32))\n\tx2 = gpuarray.to_gpu(np.random.randn(batchsize, size).astype(np.float32))\n\tlabels = gpuarray.to_gpu(np.random.randint(low=-1, high=3, size=(batchsize, size)).astype(np.int32))\n\n\tl1Hinge = L1Hinge()\n\n\tConfig.verifyData = True\n\n\ttry:\n\t\tl1Hinge([x1, x2], labels)\n\n\texcept CostError as e:\n\t\tprint(""Caught labels verification error: %s"" % e)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cost/MSE.py,9,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend import Blas\n\nfrom PuzzleLib.Cost.Cost import Cost\n\n\nclass MSE(Cost):\n\tdef calcGrad(self, pred, target):\n\t\tc = 1.0 / np.prod(target.shape)\n\t\tgrad = Blas.addVectorToVector(target.ravel(), pred.ravel(), alpha=c, beta=-c)\n\t\tgrad = grad.reshape(pred.shape)\n\n\t\treturn grad\n\n\n\tdef calcError(self, pred, target):\n\t\tself.devErr.fill(Blas.dot(self.grad.ravel(), self.grad.ravel()) * np.prod(self.grad.shape)\n\t\t\t\t\t\t * self.grad.shape[0] / 2.0)\n\t\tself.accumErr += self.devErr\n\n\n\tdef calcVal(self, pred, target):\n\t\tdiff = Blas.addVectorToVector(target.ravel(), pred.ravel(), alpha=1.0, beta=-1.0)\n\t\terror = Blas.dot(diff, diff) / (2.0 * np.prod(target.shape))\n\n\t\treturn error\n\n\n\tdef checkDataShape(self, pred, target):\n\t\tassert pred.shape[1:] == target.shape[1:]\n\n\n\tdef checkValDataShape(self, pred, target):\n\t\tassert pred.shape[1:] == target.shape[1:]\n\n\ndef unittest():\n\terrorTest()\n\tvalTest()\n\n\ndef errorTest():\n\tpred = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\ttarget = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\n\tmse = MSE()\n\tmse(pred, target)\n\n\tassert np.isclose(mse.error, np.linalg.norm(target.get() - pred.get())**2 / (2.0 * np.prod(target.shape)))\n\n\ndef valTest():\n\tpred = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\ttarget = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\n\tmse = MSE()\n\terror = mse.validate(pred, target)\n\n\tassert np.isclose(error, np.linalg.norm(target.get() - pred.get())**2 / (2.0 * np.prod(target.shape)))\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cost/Multi.py,12,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Cost.Cost import Cost\n\n\nclass Multi(Cost):\n\tdef __init__(self):\n\t\tself.costs = []\n\t\tsuper().__init__()\n\n\t\tself.devErr = None\n\t\tself.accumErr = None\n\n\n\tdef append(self, cost):\n\t\tself.costs.append(cost)\n\t\treturn self\n\n\n\tdef resetAccumulator(self):\n\t\tfor cost in self.costs:\n\t\t\tcost.resetAccumulator()\n\n\n\tdef updateState(self, samples):\n\t\tfor cost in self.costs:\n\t\t\tcost.updateState(samples)\n\n\n\tdef resetDeviceAccumulator(self):\n\t\tfor cost in self.costs:\n\t\t\tcost.resetDeviceAccumulator()\n\n\n\tdef getError(self):\n\t\tif self.dirty:\n\t\t\tself.error = []\n\t\t\tfor cost in self.costs:\n\t\t\t\tself.error.append(cost.getError())\n\n\t\t\tself.dirty = False\n\n\t\treturn self.error\n\n\n\tdef getMeanError(self):\n\t\taccumErr = []\n\t\tfor cost in self.costs:\n\t\t\taccumErr.append(cost.getMeanError())\n\n\t\treturn accumErr\n\n\n\tdef calcGrad(self, preds, targets):\n\t\tgrads = []\n\n\t\tfor i, cost in enumerate(self.costs):\n\t\t\tcost.grad = cost.calcGrad(preds[i], targets[i])\n\t\t\tgrads.append(cost.grad)\n\n\t\treturn grads\n\n\n\tdef calcError(self, preds, targets):\n\t\tfor i, cost in enumerate(self.costs):\n\t\t\tcost.calcError(preds[i], targets[i])\n\n\n\tdef calcVal(self, preds, targets):\n\t\terror = []\n\n\t\tfor i, cost in enumerate(self.costs):\n\t\t\terror.append(cost.calcVal(preds[i], targets[i]))\n\n\t\treturn error\n\n\n\tdef checkDataShape(self, preds, targets):\n\t\tassert len(preds) == len(targets)\n\t\tassert [preds[i].dtype == preds[0].dtype for i in range(len(preds))]\n\n\t\tfor i, cost in enumerate(self.costs):\n\t\t\tcost.checkDataShape(preds[i], targets[i])\n\n\n\tdef checkValDataShape(self, preds, targets):\n\t\tassert len(preds) == len(targets)\n\t\tassert [preds[i].dtype == preds[0].dtype for i in range(len(preds))]\n\n\t\tfor i, cost in enumerate(self.costs):\n\t\t\tcost.checkValDataShape(preds[i], targets[i])\n\n\n\tdef getBatchsize(self, preds):\n\t\treturn preds[0].shape[0]\n\n\ndef unittest():\n\terrorTest()\n\tvalTest()\n\n\ndef errorTest():\n\tpred = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\ttarget = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\n\tfrom PuzzleLib.Cost.Abs import Abs\n\tfrom PuzzleLib.Cost.MSE import MSE\n\n\tmulti = Multi().append(MSE()).append(Abs())\n\tmulti([pred, pred], [target, target])\n\n\thostError = [np.linalg.norm(target.get() - pred.get())**2 / (2.0 * np.prod(target.shape)),\n\t\t\t\t np.linalg.norm((target.get() - pred.get()).ravel(), ord=1) / np.prod(target.shape)]\n\n\tassert np.isclose(multi.error[0], hostError[0])\n\tassert np.isclose(multi.error[1], hostError[1])\n\n\ndef valTest():\n\tpred = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\ttarget = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\n\tfrom PuzzleLib.Cost.Abs import Abs\n\tfrom PuzzleLib.Cost.MSE import MSE\n\n\tmulti = Multi().append(Abs()).append(MSE())\n\terror = multi.validate([pred, pred], [target, target])\n\n\thostError = [np.linalg.norm((target.get() - pred.get()).ravel(), ord=1) / np.prod(target.shape),\n\t\t\t\t np.linalg.norm(target.get() - pred.get())**2 / (2.0 * np.prod(target.shape))]\n\n\tassert np.isclose(error[0], hostError[0])\n\tassert np.isclose(error[1], hostError[1])\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cost/SVM.py,17,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\nfrom PuzzleLib.Backend.Kernels.Costs import getAccuracyKernel, svmKernel\nfrom PuzzleLib.Backend.Kernels.MatVec import argmax\nfrom PuzzleLib.Backend.Kernels.MatVecBatch import argmaxBatch\n\nfrom PuzzleLib.Cost.Cost import CostError, Cost\n\n\nclass SVM(Cost):\n\tdef __init__(self, mode=""l1""):\n\t\tsuper().__init__()\n\n\t\tself.mode = mode\n\t\tself.mostProb = None\n\n\n\tdef calcGrad(self, scores, labels):\n\t\tif Config.verifyData:\n\t\t\tself.verifyLabels(scores, labels)\n\n\t\tself.devErr, grad = svmKernel(scores, labels, mode=self.mode, error=self.devErr)\n\t\treturn grad\n\n\n\tdef calcError(self, scores, labels):\n\t\tself.accumErr += self.devErr\n\n\n\tdef calcVal(self, scores, labels):\n\t\tif Config.verifyData:\n\t\t\tself.verifyLabels(scores, labels)\n\n\t\tif scores.ndim == 2:\n\t\t\tshape = scores.shape\n\t\t\tself.mostProb = argmax(scores, axis=1)\n\n\t\telse:\n\t\t\tshape = scores.shape[:1] + scores.shape[2:]\n\t\t\tscores = scores.reshape(*scores.shape[:2], np.prod(scores.shape[2:]))\n\n\t\t\tself.mostProb = argmaxBatch(scores, axis=1).reshape(shape)\n\n\t\tcalcAccuracy = getAccuracyKernel(""calcAccuracy"")\n\t\terror = calcAccuracy(self.mostProb, labels, allocator=memPool).get() / shape[0]\n\n\t\treturn error\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\t\tself.mostProb = None\n\n\n\tdef checkDataShape(self, scores, labels):\n\t\tassert scores.ndim > 1 and labels.ndim == scores.ndim - 1\n\t\tassert labels.dtype == np.int32\n\n\t\tif scores.ndim > 2:\n\t\t\tassert scores.shape[2:] == labels.shape[1:]\n\n\n\tdef checkValDataShape(self, scores, labels):\n\t\tassert scores.ndim > 1 and labels.ndim == scores.ndim - 1\n\t\tassert labels.dtype == np.int32\n\n\t\tif scores.ndim > 2:\n\t\t\tassert scores.shape[2:] == labels.shape[1:]\n\n\n\t@staticmethod\n\tdef verifyLabels(scores, labels):\n\t\tmn, mx = gpuarray.minimum(labels).get(), gpuarray.maximum(labels).get()\n\t\tif mn < 0:\n\t\t\traise CostError(""SVM labels verification failed, found index %s (< 0)"" % mn)\n\n\t\tif mx >= scores.shape[1]:\n\t\t\traise CostError(""SVM labels verification failed, found index %s (> %s)"" % (mx, scores.shape[1] - 1))\n\n\ndef unittest():\n\tl1SVMTest()\n\tl2SVMTest()\n\tverifyLabelsTest()\n\n\ndef l1SVMTest():\n\tbatchsize, size = 20, 4\n\n\tscores = gpuarray.to_gpu(np.random.randn(batchsize, size).astype(np.float32))\n\tlabels = gpuarray.to_gpu(np.random.randint(low=0, high=size, size=(batchsize, ), dtype=np.int32))\n\n\tsvm = SVM(mode=""l1"")\n\terror, grad = svm(scores, labels)\n\n\thostScores, hostLabels = scores.get(), labels.get()\n\n\thostGrad = np.empty(grad.shape, dtype=np.float32)\n\thostError = 0.0\n\n\tfor b in range(batchsize):\n\t\tfor n in range(size):\n\t\t\tcls = 2 * (hostLabels[b] == n) - 1\n\t\t\tval = hostScores[b, n] * cls\n\n\t\t\thostGrad[b, n] = cls / batchsize / size if val < 1 else 0.0\n\t\t\thostError += max(0.0, 1.0 - val) / batchsize / size\n\n\tassert np.allclose(hostGrad, grad.get())\n\tassert np.isclose(hostError, error)\n\n\terror = svm.validate(scores, labels)\n\tprint(""Validation error: %s"" % error)\n\tassert np.allclose(np.argmax(scores.get(), axis=1), svm.mostProb.get())\n\n\ndef l2SVMTest():\n\tbatchsize, size = 20, 4\n\n\tscores = gpuarray.to_gpu(np.random.randn(batchsize, size).astype(np.float32))\n\tlabels = gpuarray.to_gpu(np.random.randint(low=0, high=size, size=(batchsize, ), dtype=np.int32))\n\n\tsvm = SVM(mode=""l2"")\n\terror, grad = svm(scores, labels)\n\n\thostScores, hostLabels = scores.get(), labels.get()\n\n\thostGrad = np.empty(grad.shape, dtype=np.float32)\n\thostError = 0.0\n\n\tfor b in range(batchsize):\n\t\tfor n in range(size):\n\t\t\tcls = 2 * (hostLabels[b] == n) - 1\n\t\t\terr = max(0.0, 1.0 - hostScores[b, n] * cls)\n\n\t\t\thostGrad[b, n] = 2.0 * cls * err / batchsize / size\n\t\t\thostError += err**2 / batchsize / size\n\n\tassert np.allclose(hostGrad, grad.get())\n\tassert np.isclose(hostError, error)\n\n\terror = svm.validate(scores, labels)\n\n\tprint(""Validation error: %s"" % error)\n\tassert np.allclose(np.argmax(scores.get(), axis=1), svm.mostProb.get())\n\n\ndef verifyLabelsTest():\n\tbatchsize, size = 20, 4\n\n\tscores = gpuarray.to_gpu(np.random.randn(batchsize, size).astype(np.float32))\n\tlabels = gpuarray.to_gpu(np.random.randint(low=0, high=size + 1, size=(batchsize, ), dtype=np.int32))\n\n\tsvm = SVM(mode=""l1"")\n\n\tConfig.verifyData = True\n\n\ttry:\n\t\tsvm(scores, labels)\n\n\texcept CostError as e:\n\t\tprint(""Caught labels verification error: %s"" % e)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cost/SmoothL1.py,19,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\nfrom PuzzleLib.Backend.Kernels.Costs import smoothL1Ker\n\nfrom PuzzleLib.Cost.Cost import Cost\n\n\nclass SmoothL1(Cost):\n\tdef calcGrad(self, pred, target):\n\t\tgrad = gpuarray.empty(pred.shape, dtype=np.float32, allocator=memPool)\n\n\t\tfullnorm = 1.0 / np.prod(target.shape)\n\t\tnorm = 1.0 / np.prod(target.shape[1:])\n\n\t\tself.devErr.fill(0.0)\n\n\t\tsmoothL1Ker(pred, target, self.devErr, grad, norm, fullnorm)\n\t\treturn grad\n\n\n\tdef calcError(self, pred, target):\n\t\tself.accumErr += self.devErr\n\n\n\tdef calcVal(self, pred, target):\n\t\tdiff = gpuarray.empty(pred.shape, dtype=np.float32, allocator=memPool)\n\n\t\tfullnorm = 1.0 / np.prod(target.shape)\n\n\t\tdevErr = gpuarray.zeros((), dtype=np.float32, allocator=memPool)\n\t\tsmoothL1Ker(pred, target, devErr, diff, fullnorm, fullnorm)\n\n\t\treturn devErr.get()\n\n\n\tdef checkDataShape(self, pred, target):\n\t\tassert pred.shape[1:] == target.shape[1:]\n\n\n\tdef checkValDataShape(self, pred, target):\n\t\tassert pred.shape[1:] == target.shape[1:]\n\n\ndef unittest():\n\terrorTest()\n\tvalTest()\n\n\ndef errorTest():\n\tpred = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\ttarget = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\n\tsmoothL1 = SmoothL1()\n\tsmoothL1(pred, target)\n\n\thostPred, hostTarget = pred.get(), target.get()\n\thostGrad = ((np.abs(hostPred - hostTarget) >= 1.0) * np.sign(hostPred - hostTarget) +\n\t\t\t   (np.abs(hostPred - hostTarget) < 1.0) * (hostPred - hostTarget)) / np.prod(pred.shape)\n\n\tassert np.allclose(hostGrad, smoothL1.grad.get())\n\n\thostError = np.mean((np.abs(hostPred - hostTarget) >= 1.0) * (np.abs(hostPred - hostTarget) - 0.5) +\n\t\t\t\t\t\t(np.abs(hostPred - hostTarget) < 1.0) * (hostPred - hostTarget)**2 / 2.0)\n\n\tassert np.isclose(smoothL1.error, hostError)\n\n\ndef valTest():\n\tpred = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\ttarget = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\n\tsmoothL1 = SmoothL1()\n\terror = smoothL1.validate(pred, target)\n\n\thostPred, hostTarget = pred.get(), target.get()\n\n\thostError = np.mean((np.abs(hostPred - hostTarget) >= 1.0) * (np.abs(hostPred - hostTarget) - 0.5) +\n\t\t\t\t\t\t(np.abs(hostPred - hostTarget) < 1.0) * (hostPred - hostTarget)**2 / 2.0)\n\n\tassert np.isclose(error, hostError)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cost/__init__.py,0,b'from PuzzleLib.Cost.Abs import Abs\nfrom PuzzleLib.Cost.BCE import BCE\nfrom PuzzleLib.Cost.CrossEntropy import CrossEntropy\nfrom PuzzleLib.Cost.CTC import CTC\nfrom PuzzleLib.Cost.Hinge import Hinge\nfrom PuzzleLib.Cost.KLDivergence import KLDivergence\nfrom PuzzleLib.Cost.L1Hinge import L1Hinge\nfrom PuzzleLib.Cost.MSE import MSE\nfrom PuzzleLib.Cost.Multi import Multi\nfrom PuzzleLib.Cost.SmoothL1 import SmoothL1\nfrom PuzzleLib.Cost.SVM import SVM\n'
Cuda/Backend.py,1,"b'from enum import Enum\nimport numpy as np\n\nfrom PuzzleLib.Cuda.GPUBackend import GPUBackend\n\nfrom PuzzleLib.Cuda import Driver as CudaDriver\nfrom PuzzleLib.Cuda.Driver import CudaError, CuRand, CuBlas, CuDnn\n\nfrom PuzzleLib.Cuda.GPUArray import extendGPUArray\nfrom PuzzleLib.Cuda.SourceModule import SourceModule, ElementwiseKernel, ElementHalf2Kernel, ReductionKernel\nfrom PuzzleLib.Cuda.Utils import SharedArray, prod\n\n\ncudaWarpBit, cudaBlockBit = 5, 9\ncudaWarpSize, cudaBlockSize = 1 << cudaWarpBit, 1 << cudaBlockBit\n\n\nclass CudaSourceModule(SourceModule):\n\tDriver = CudaDriver\n\n\nclass CudaEltwiseKernel(ElementwiseKernel):\n\tDriver = CudaDriver\n\tSourceModule = CudaSourceModule\n\n\twarpBit, warpSize = cudaWarpBit, cudaWarpSize\n\tblockBit, blockSize = cudaBlockBit, cudaBlockSize\n\n\nclass CudaEltHalf2Kernel(ElementHalf2Kernel):\n\tDriver = CudaDriver\n\tSourceModule = CudaSourceModule\n\n\twarpBit, warpSize = cudaWarpBit, cudaWarpSize\n\tblockBit, blockSize = cudaBlockBit, cudaBlockSize\n\n\nclass CudaReductionKernel(ReductionKernel):\n\tDriver = CudaDriver\n\tSourceModule = CudaSourceModule\n\n\twarpBit, warpSize = cudaWarpBit, cudaWarpSize\n\tblockBit, blockSize = cudaBlockBit, cudaBlockSize\n\n\nclass CudaSharedArray(SharedArray):\n\tGPUArray = CudaDriver.GPUArray\n\n\nclass CudaBackend(GPUBackend):\n\tBackendName = ""Cuda""\n\n\twarpSize = cudaWarpSize\n\tnthreads = 1024\n\n\tDriver = CudaDriver\n\tGPUArray = extendGPUArray(CudaDriver, CudaEltwiseKernel, CudaEltHalf2Kernel, CudaReductionKernel)\n\tError = CudaError\n\n\tSourceModule = CudaSourceModule\n\tElementwiseKernel = CudaEltwiseKernel\n\tElementHalf2Kernel = CudaEltHalf2Kernel\n\tReductionKernel = CudaReductionKernel\n\n\tSharedArray = CudaSharedArray\n\tRand, Blas, Dnn = CuRand, CuBlas, CuDnn\n\n\n\tclass GroupFormat(Enum):\n\t\tgbp = CuBlas.GROUPFORMAT_GBP\n\t\tbgp = CuBlas.GROUPFORMAT_BGP\n\n\n\tclass ConvFwdAlgo(Enum):\n\t\timplicitGemm = CuDnn.CONV_FWD_IMPLICIT_GEMM\n\t\timplicitPrecompGemm = CuDnn.CONV_FWD_IMPLICIT_PRECOMP_GEMM\n\t\tgemm = CuDnn.CONV_FWD_GEMM\n\t\tdirect = CuDnn.CONV_FWD_DIRECT\n\t\tfft = CuDnn.CONV_FWD_FFT\n\t\tfftTiling = CuDnn.CONV_FWD_FFT_TILING\n\t\twinograd = CuDnn.CONV_FWD_WINOGRAD\n\t\twinogradNonfused = CuDnn.CONV_FWD_WINOGRAD_NONFUSED\n\n\n\tclass ConvBwdDataAlgo(Enum):\n\t\talgo0 = CuDnn.CONV_BWD_DATA_ALGO_0\n\t\talgo1 = CuDnn.CONV_BWD_DATA_ALGO_1\n\t\tfft = CuDnn.CONV_BWD_DATA_FFT\n\t\tfftTiling = CuDnn.CONV_BWD_DATA_FFT_TILING\n\t\twinograd = CuDnn.CONV_BWD_DATA_WINOGRAD\n\t\twinogradNonfused = CuDnn.CONV_BWD_DATA_WINOGRAD_NONFUSED\n\n\n\tclass ConvBwdFilterAlgo(Enum):\n\t\talgo0 = CuDnn.CONV_BWD_PARAM_ALGO_0\n\t\talgo1 = CuDnn.CONV_BWD_PARAM_ALGO_1\n\t\tfft = CuDnn.CONV_BWD_PARAM_FFT\n\t\talgo3 = CuDnn.CONV_BWD_PARAM_ALGO_3\n\t\twinograd = CuDnn.CONV_BWD_PARAM_WINOGRAD\n\t\twinogradNonfused = CuDnn.CONV_BWD_PARAM_WINOGRAD_NONFUSED\n\t\tfftTiling = CuDnn.CONV_BWD_PARAM_FFT_TILING\n\n\n\tclass PoolMode(Enum):\n\t\tmax = CuDnn.POOL_MODE_MAX\n\t\tavgWithPad = CuDnn.POOL_MODE_AVG_WITH_PAD\n\t\tavgNoPad = CuDnn.POOL_MODE_AVG_NO_PAD\n\t\tmaxDeterminism = CuDnn.POOL_MODE_MAX_DETERMINISM\n\n\n\tclass SoftMaxMode(Enum):\n\t\tperActivation = CuDnn.SOFTMAX_MODE_PER_ACTIVATION\n\t\tspatial = CuDnn.SOFTMAX_MODE_SPATIAL\n\n\n\tclass MathType(Enum):\n\t\tdefault = CuDnn.MATH_DEFAULT\n\t\ttensorOp = CuDnn.MATH_TENSOR_OP\n\t\ttensorOpAllowConv = CuDnn.MATH_TENSOR_OP_ALLOW_CONVERSION\n\n\n\tclass BatchNormMode(Enum):\n\t\tperActivation = CuDnn.BATCHNORM_MODE_PER_ACTIVATION\n\t\tspatial = CuDnn.BATCHNORM_MODE_SPATIAL\n\t\tspatialPersistent = CuDnn.BATCHNORM_MODE_SPATIAL_PERSISTENT\n\n\n\tclass RNNAlgo(Enum):\n\t\tstandard = CuDnn.RNN_ALGO_STANDARD\n\t\tpersistStatic = CuDnn.RNN_ALGO_PERSIST_STATIC\n\t\tpersistDynamic = CuDnn.RNN_ALGO_PERSIST_DYNAMIC\n\n\n\tclass RNNMode(Enum):\n\t\trelu = CuDnn.RNN_MODE_RELU\n\t\ttanh = CuDnn.RNN_MODE_TANH\n\t\tlstm = CuDnn.RNN_MODE_LSTM\n\t\tgru = CuDnn.RNN_MODE_GRU\n\n\n\tclass DirectionMode(Enum):\n\t\tuni = CuDnn.RNN_DIRECTION_UNIDIRECTIONAL\n\t\tbi = CuDnn.RNN_DIRECTION_BIDIRECTIONAL\n\n\n\tclass ConvPerf:\n\t\tdef __init__(self, algo, tm, memory, determinism, mathType):\n\t\t\tself.algo = algo\n\t\t\tself.time = tm\n\t\t\tself.memory = memory\n\t\t\tself.determinism = determinism == 1\n\t\t\tself.mathType = CudaBackend.MathType(mathType)\n\n\n\t\tdef toString(self):\n\t\t\treturn ""%-40s %-25s %-28s %-20s %s"" % (\n\t\t\t\t""Algo %s"" % self.algo, ""time %.6f secs"" % self.time,\n\t\t\t\t""memory %.6f mbytes"" % (self.memory / 1024 ** 2), ""determinism=%s"" % self.determinism,\n\t\t\t\t""mathType=%s"" % self.mathType\n\t\t\t)\n\n\n\t\tdef __str__(self):\n\t\t\treturn self.toString()\n\n\n\t\tdef __repr__(self):\n\t\t\treturn self.toString()\n\n\n\tdef createRnn(self, insize, hsize, dtype, layers=1, algo=None, mode=None, direction=None, dropout=0.0,\n\t\t\t\t  seed=0, batchsize=0):\n\t\talgo = self.RNNAlgo.standard if algo is None else algo\n\t\tmode = self.RNNMode.lstm if mode is None else mode\n\t\tdirection = self.DirectionMode.uni if direction is None else direction\n\n\t\trnn = self.Dnn.Rnn(\n\t\t\tself.dnn, insize, hsize, np.dtype(dtype), layers, algo.value, mode.value, direction.value,\n\t\t\tdropout, seed, batchsize\n\t\t)\n\n\t\tW = self.GPUArray.empty((rnn.wsize, ), dtype=dtype)\n\t\tparams = self.acquireRnnParams(rnn, W)\n\n\t\treturn rnn, W, params\n\n\n\tdef deviceSupportsBatchHint(self):\n\t\treturn self.device.computeCapability() >= (6, 1)\n\n\n\tdef acquireRnnParams(self, rnn, W):\n\t\tmode = self.RNNMode(rnn.mode)\n\n\t\tif mode == self.RNNMode.relu or mode == self.RNNMode.tanh:\n\t\t\treturn self.acquireNativeRnnParams(rnn, W)\n\t\telif mode == self.RNNMode.lstm:\n\t\t\treturn self.acquireLSTMParams(rnn, W)\n\t\telif mode == self.RNNMode.gru:\n\t\t\treturn self.acquireGRUParams(rnn, W)\n\t\telse:\n\t\t\traise NotImplementedError(mode.value)\n\n\n\tdef getRnnParam(self, rnn, W, layer, linLayer, Wshape):\n\t\tWtuple, biasTuple = rnn.getParam(W, layer, linLayer)\n\n\t\tWoffset, wsize = Wtuple\n\t\tbiasOffset, biasSize = biasTuple\n\n\t\tdtype, gpudata = W.dtype, W.gpudata\n\t\tWbytes, biasBytes = wsize * dtype.itemsize, biasSize * dtype.itemsize\n\n\t\tassert prod(Wshape) == wsize\n\t\tw = self.GPUArray(Wshape, dtype=W.dtype, gpudata=W.gpudata[Woffset:Woffset + Wbytes])\n\n\t\tbias = self.GPUArray((biasSize, ), dtype=W.dtype, gpudata=W.gpudata[biasOffset:biasOffset + biasBytes])\n\t\treturn w, bias\n\n\n\tdef acquireNativeRnnParams(self, rnn, W):\n\t\tdirection = self.DirectionMode(rnn.direction)\n\n\t\tlinLayers = 2\n\t\tlayers = rnn.layers if direction == self.DirectionMode.uni else rnn.layers * 2\n\n\t\tlayerTypes = {0: ""w"", 1: ""r""}\n\n\t\tparams = []\n\t\tfor layer in range(layers):\n\t\t\tlayerparams = {}\n\t\t\tfor linLayer in range(linLayers):\n\t\t\t\tif linLayer == 0:\n\t\t\t\t\tif layer == 0 or layer == 1 and direction == self.DirectionMode.bi:\n\t\t\t\t\t\tsize = rnn.insize\n\t\t\t\t\telse:\n\t\t\t\t\t\tsize = 2 * rnn.hsize if direction == self.DirectionMode.bi else rnn.hsize\n\n\t\t\t\t\tshape = (rnn.hsize, size)\n\n\t\t\t\telif linLayer == 1:\n\t\t\t\t\tshape = (rnn.hsize, rnn.hsize)\n\n\t\t\t\telse:\n\t\t\t\t\tassert False\n\n\t\t\t\tw, bias = self.getRnnParam(rnn, W, layer, linLayer, shape)\n\t\t\t\tT = layerTypes[linLayer]\n\n\t\t\t\tWname = ""%si"" % T\n\t\t\t\tassert Wname not in layerparams\n\n\t\t\t\tbiasname = ""b%si"" % T\n\t\t\t\tassert biasname not in layerparams\n\n\t\t\t\tlayerparams[Wname] = w\n\t\t\t\tlayerparams[biasname] = bias\n\n\t\t\tparams.append(layerparams)\n\n\t\treturn params\n\n\n\tdef acquireLSTMParams(self, rnn, W):\n\t\tdirection = self.DirectionMode(rnn.direction)\n\n\t\tlinLayers = 8\n\t\tlayers = rnn.layers if direction == self.DirectionMode.uni else rnn.layers * 2\n\n\t\tlayerTypes = {\n\t\t\t0: ""i"", 4: ""i"",\n\t\t\t1: ""f"", 5: ""f"",\n\t\t\t2: ""c"", 6: ""c"",\n\t\t\t3: ""o"", 7: ""o""\n\t\t}\n\n\t\tparams = []\n\t\tfor layer in range(layers):\n\t\t\tlayerparams = {}\n\t\t\tfor linLayer in range(linLayers):\n\t\t\t\tif linLayer < 4:\n\t\t\t\t\tif layer == 0 or layer == 1 and direction == self.DirectionMode.bi:\n\t\t\t\t\t\tsize = rnn.insize\n\t\t\t\t\telse:\n\t\t\t\t\t\tsize = 2 * rnn.hsize if direction == self.DirectionMode.bi else rnn.hsize\n\n\t\t\t\t\tshape, wtype = (rnn.hsize, size), ""w""\n\n\t\t\t\telse:\n\t\t\t\t\tshape, wtype = (rnn.hsize, rnn.hsize), ""r""\n\n\t\t\t\tw, bias = self.getRnnParam(rnn, W, layer, linLayer, shape)\n\t\t\t\tT = layerTypes[linLayer]\n\n\t\t\t\tWname = ""%s%s"" % (wtype, T)\n\t\t\t\tassert Wname not in layerparams\n\n\t\t\t\tbiasname = ""b%s%s"" % (wtype, T)\n\t\t\t\tassert biasname not in layerparams\n\n\t\t\t\tlayerparams[Wname] = w\n\t\t\t\tlayerparams[biasname] = bias\n\n\t\t\tparams.append(layerparams)\n\n\t\treturn params\n\n\n\tdef acquireGRUParams(self, rnn, W):\n\t\tdirection = self.DirectionMode(rnn.direction)\n\n\t\tlinLayers = 6\n\t\tlayers = rnn.layers if direction == self.DirectionMode.uni else rnn.layers * 2\n\n\t\tlayerTypes = {\n\t\t\t0: ""r"", 3: ""r"",\n\t\t\t1: ""i"", 4: ""i"",\n\t\t\t2: ""h"", 5: ""h""\n\t\t}\n\n\t\tparams = []\n\t\tfor layer in range(layers):\n\t\t\tlayerparams = {}\n\t\t\tfor linLayer in range(linLayers):\n\t\t\t\tif linLayer < 3:\n\t\t\t\t\tif layer == 0 or layer == 1 and direction == self.DirectionMode.bi:\n\t\t\t\t\t\tsize = rnn.insize\n\t\t\t\t\telse:\n\t\t\t\t\t\tsize = 2 * rnn.hsize if direction == self.DirectionMode.bi else rnn.hsize\n\n\t\t\t\t\tshape, wtype = (rnn.hsize, size), ""w""\n\n\t\t\t\telse:\n\t\t\t\t\tshape, wtype = (rnn.hsize, rnn.hsize), ""r""\n\n\t\t\t\tw, bias = self.getRnnParam(rnn, W, layer, linLayer, shape)\n\t\t\t\tT = layerTypes[linLayer]\n\n\t\t\t\tWname = ""%s%s"" % (wtype, T)\n\t\t\t\tassert Wname not in layerparams\n\n\t\t\t\tbiasname = ""b%s%s"" % (wtype, T)\n\t\t\t\tassert biasname not in layerparams\n\n\t\t\t\tlayerparams[Wname] = w\n\t\t\t\tlayerparams[biasname] = bias\n\n\t\t\tparams.append(layerparams)\n\n\t\treturn params\n\n\nbackendCache = {}\n\n\ndef getDeviceCount():\n\treturn CudaBackend.Driver.Device.count()\n\n\ndef getBackend(deviceIdx, initmode=0):\n\tbnd = backendCache.get(deviceIdx, None)\n\n\tif bnd is None:\n\t\tbnd = CudaBackend(deviceIdx, initmode)\n\t\tbackendCache[deviceIdx] = bnd\n\n\telse:\n\t\tbnd.updateBackend(initmode)\n\n\treturn bnd\n'"
Cuda/CheckInstall.py,0,"b'import sys, os, subprocess, tempfile\nfrom colorama import Fore, Style\n\n\nif ""PYCHARM_HOSTED"" not in os.environ:\n\timport colorama\n\tcolorama.init()\n\n\ncudaTestKernel = """"""\n\n#include <stdio.h>\n\n\n__global__ void iaxpy(int *y, const int *x, int a, int size)\n{\n\tint i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < size) y[i] += a * x[i];\n}\n\n\n#define CUDA_ASSERT(status) do { if (!cudaAssertStatus((status), __LINE__)) exit(1); } while (0)\ninline bool cudaAssertStatus(cudaError_t code, int line)\n{\n\tif (code != cudaSuccess) \n\t{\n\t\tfprintf(stderr, ""%s (line:%d)\\\\n"", cudaGetErrorString(code), line);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n\nint main()\n{\n\tint exitcode = 0;\n\n\tconst int SIZE = 1 << 20;\n\tconst int NBYTES = SIZE * sizeof(int);\n\n\tint *hostx = (int *)malloc(NBYTES);\n\tint *hosty = (int *)malloc(NBYTES);\n\n\tint *devx = NULL, *devy = NULL;\n\tCUDA_ASSERT(cudaMalloc(&devx, NBYTES));\n\tCUDA_ASSERT(cudaMalloc(&devy, NBYTES));\n\n\tfor (int i = 0; i < SIZE; i++)\n\t{\n\t\thostx[i] = i;\n\t\thosty[i] = -i * 2;\n\t}\n\n\tCUDA_ASSERT(cudaMemcpy(devx, hostx, NBYTES, cudaMemcpyHostToDevice));\n\tCUDA_ASSERT(cudaMemcpy(devy, hosty, NBYTES, cudaMemcpyHostToDevice));\n\n\tconst int NT = 256;\n\tiaxpy<<<(SIZE + NT - 1) / NT, NT>>>(devy, devx, 2, SIZE);\n\n\tCUDA_ASSERT(cudaMemcpy(hosty, devy, NBYTES, cudaMemcpyDeviceToHost));\n\n\tCUDA_ASSERT(cudaFree(devx));\n\tCUDA_ASSERT(cudaFree(devy));\n\n\tfor (int i = 0; i < SIZE; i++)\n\t\tif (hosty[i] != 0)\n\t\t{\n\t\t\tfprintf(stderr, ""kernel invocation failed!"");\n\n\t\t\texitcode = 1;\n\t\t\tgoto exit;\n\t\t}\n\n\tprintf(""finished successfully!"");\n\tfflush(stdout);\n\nexit:\n\tfree(hostx);\n\tfree(hosty);\n\n\treturn exitcode;\n}\n\n""""""\n\n\ndef checkRuntime(name, compiler, download, envpath):\n\tprint(""%sChecking %s installation ...%s"" % (Fore.LIGHTBLUE_EX, name, Style.RESET_ALL))\n\n\ttry:\n\t\tversion = subprocess.getoutput(""%s --version"" % compiler).split()[-1]\n\n\texcept Exception as e:\n\t\terror = ""%s%s is not found with error(s):%s\\n%s"" % (Fore.RED, name, Style.RESET_ALL, e)\n\t\tnote = ""Download and install appropriate version from %s"" % download\n\t\traise RuntimeError(""%s\\n%s"" % (error, note))\n\n\tprint(""%s%s %s and SDK libraries are found!%s"" % (Fore.LIGHTGREEN_EX, name, version, Style.RESET_ALL))\n\tprint(""Continuing ..."", end=""\\n\\n"")\n\n\tif sys.platform != ""win32"":\n\t\treturn\n\n\tprint(""%sChecking %s environment on Windows platform ...%s"" % (Fore.LIGHTBLUE_EX, name, Style.RESET_ALL))\n\tRUNTIME_PATH = os.environ.get(envpath, None)\n\n\tif RUNTIME_PATH is None:\n\t\traise RuntimeError(\n\t\t\t""%s%s is not set - set it to CUDA installation path!%s"" % (Fore.RED, envpath, Style.RESET_ALL)\n\t\t)\n\n\tprint(""%s%s is set!%s"" % (Fore.LIGHTGREEN_EX, envpath, Style.RESET_ALL))\n\tprint(""Continuing ..."", end=""\\n\\n"")\n\n\ndef checkCompiler(name, compiler, kernel, ext):\n\tprint(""%sChecking %s compiler ...%s"" % (Fore.LIGHTBLUE_EX, compiler.upper(), Style.RESET_ALL))\n\n\ttemp = tempfile.NamedTemporaryFile(mode=""w"", encoding=""utf-8"", suffix=ext, delete=False)\n\texefile = os.path.join(os.path.dirname(temp.name), ""a.out"")\n\n\ttry:\n\t\twith temp:\n\t\t\ttemp.write(kernel)\n\n\t\ttry:\n\t\t\tres = subprocess.check_output([compiler, ""-o"", exefile, temp.name])\n\t\t\tprint(""%s%s compiled test kernel with output:%s %s"" % (\n\t\t\t\tFore.LIGHTGREEN_EX, compiler, Style.RESET_ALL, res.decode(""utf-8"")\n\t\t\t))\n\n\t\t\tprint(""Continuing ..."", end=""\\n\\n"")\n\n\t\texcept subprocess.CalledProcessError as e:\n\t\t\traise RuntimeError(""%s%s failed compiling test kernel with error(s):%s\\n%s"" % (\n\t\t\t\tFore.RED, compiler, Style.RESET_ALL, e.output.decode(""utf-8"")\n\t\t\t))\n\n\tfinally:\n\t\tos.remove(temp.name)\n\n\tprint(""%sChecking compiled %s kernel ...%s"" % (Fore.LIGHTBLUE_EX, name, Style.RESET_ALL))\n\n\ttry:\n\t\tresult = subprocess.check_output(exefile, stderr=subprocess.PIPE).decode(""utf-8"")\n\t\tprint(\n\t\t\t""%sTest kernel answered:%s %s\\nContinuing ..."" % (Fore.LIGHTGREEN_EX, Style.RESET_ALL, result), end=""\\n\\n""\n\t\t)\n\n\texcept subprocess.CalledProcessError as e:\n\t\traise RuntimeError(\n\t\t\t""%sTest kernel failed with error:%s %s"" % (Fore.RED, Style.RESET_ALL, e.stderr.decode(""utf-8""))\n\t\t)\n\n\tfinally:\n\t\tos.remove(exefile)\n\n\ndef checkPipPackages():\n\tprint(""%sChecking python packages ...%s\\n"" % (Fore.LIGHTBLUE_EX, Style.RESET_ALL))\n\tpackages = [""numpy"", ""h5py"", ""Pillow"", ""graphviz"", ""colorama""]\n\n\ttry:\n\t\tpip = ""pip3""\n\t\tsubprocess.check_output([pip])\n\n\texcept subprocess.CalledProcessError:\n\t\tpip = ""pip""\n\n\tinstalled = subprocess.check_output([pip, ""list"", ""--format=freeze""]).decode(""utf-8"")\n\tinstalled = {k: v for k, v in map(lambda s: s.split(sep=""==""), installed.splitlines())}\n\n\tfor package in packages:\n\t\tprint(""%sChecking %s installation ...%s"" % (Fore.LIGHTBLUE_EX, package, Style.RESET_ALL))\n\t\tversion = installed.get(package, None)\n\n\t\tif version is None:\n\t\t\tprint(""%s%s is not installed%s\\n"" % (Fore.YELLOW, package, Style.RESET_ALL))\n\n\t\t\ttry:\n\t\t\t\tprint(""%sInstalling %s ...%s"" % (Fore.LIGHTBLUE_EX, package, Style.RESET_ALL))\n\t\t\t\tcmd = [pip, ""install""]\n\n\t\t\t\tif sys.platform != ""win32"":\n\t\t\t\t\tcmd.append(""--user"")\n\n\t\t\t\tresult = subprocess.check_output(cmd + [package])\n\t\t\t\tprint(result.decode(""utf-8""))\n\n\t\t\texcept subprocess.CalledProcessError as e:\n\t\t\t\traise RuntimeError(\n\t\t\t\t\t""%s%s installation error:%s\\n%s"" % (Fore.RED, package, Style.RESET_ALL, e.output.decode(""utf-8""))\n\t\t\t\t)\n\n\t\telse:\n\t\t\tprint(""%sFound package %s==%s%s"" % (Fore.LIGHTGREEN_EX, package, version, Style.RESET_ALL))\n\n\t\tprint(""Continuing ..."", end=""\\n\\n"")\n\n\ndef checkCudaInstall(withPip):\n\tcheckRuntime(\n\t\tname=""CUDA"", compiler=""nvcc"", download=""https://developer.nvidia.com/cuda-downloads"", envpath=""CUDA_PATH""\n\t)\n\tcheckCompiler(\n\t\tname=""CUDA"", compiler=""nvcc"", kernel=cudaTestKernel, ext="".cu""\n\t)\n\n\tif withPip:\n\t\tcheckPipPackages()\n\n\tprint(""%sAll done, exiting ...%s"" % (Fore.LIGHTGREEN_EX, Style.RESET_ALL))\n\n\ndef main():\n\ttry:\n\t\tcheckCudaInstall(withPip=True)\n\n\texcept RuntimeError as e:\n\t\tprint(e)\n\n\t\tprint(""Exiting ..."")\n\t\tsys.exit(1)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Cuda/GPUArray.py,36,"b'import numpy as np\n\n\ndef memoize(fn):\n\tcache = {}\n\n\tdef memoizer(*args):\n\t\tobj = cache.get(args, None)\n\t\tif obj is not None:\n\t\t\treturn obj\n\n\t\tobj = fn(*args)\n\t\tcache[args] = obj\n\n\t\treturn obj\n\n\treturn memoizer\n\n\ndef extendGPUArray(Driver, ElementwiseKernel, ElementHalf2Kernel, ReductionKernel):\n\tfrom PuzzleLib.Cuda.SourceModule import dtypeToCtype\n\tGPUArray = Driver.GPUArray\n\n\n\t@memoize\n\tdef getFillKernel(dtype):\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\targuments = [(ctype.ptr, ""data""), (ctype, ""value"")]\n\t\tname = ""fillKer""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""data2[i] = half2(value, value);"",\n\t\t\t\t""data[i] = value"",\n\t\t\t\tname\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(\n\t\t\t\targuments,\n\t\t\t\t""data[i] = value"",\n\t\t\t\tname\n\t\t\t)\n\n\n\t@memoize\n\tdef getConvertTypeKernel(intype, outtype):\n\t\tassert intype != outtype\n\t\tinctype, outctype = dtypeToCtype[intype.type], dtypeToCtype[outtype.type]\n\n\t\targuments = [(outctype.ptr, ""outdata""), (inctype.const.ptr, ""indata"")]\n\t\tname = ""convertTypeKer""\n\n\t\tif intype == np.float16 or outtype == np.float16:\n\t\t\tif intype == np.float16:\n\t\t\t\tassert outtype == np.float32\n\t\t\t\toperation2 = ""((float2 *)outdata)[i] = __half22float2(indata2[i])""\n\t\t\t\toperation = ""outdata[i] = indata[i]""\n\n\t\t\telse:\n\t\t\t\tassert intype == np.float32\n\t\t\t\toperation2 = ""outdata2[i] = __float22half2_rn(((float2 *)indata)[i])""\n\t\t\t\toperation = ""outdata[i] = indata[i]""\n\n\t\t\treturn ElementHalf2Kernel(arguments, operation2, operation, name)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(\n\t\t\t\targuments,\n\t\t\t\t""outdata[i] = (%s)indata[i]"" % outctype,\n\t\t\t\tname\n\t\t\t)\n\n\n\t@memoize\n\tdef getMinMaxKernel(dtype):\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tif issubclass(dtype.type, np.floating):\n\t\t\ttypeinfo = np.finfo(dtype)\n\t\telif issubclass(dtype.type, np.integer):\n\t\t\ttypeinfo = np.iinfo(dtype)\n\t\telse:\n\t\t\traise NotImplementedError(dtype)\n\n\t\tminval, maxval = typeinfo.min, typeinfo.max\n\t\targuments = [(ctype.const.ptr, ""data"")]\n\n\t\tminKer = ReductionKernel(\n\t\t\touttype=dtype.type, neutral=maxval, reduceExpr=""min(a, b)"", mapExpr=""data[i]"",\n\t\t\targuments=arguments, name=""min""\n\t\t)\n\n\t\tmaxKer = ReductionKernel(\n\t\t\touttype=dtype.type, neutral=minval, reduceExpr=""max(a, b)"", mapExpr=""data[i]"",\n\t\t\targuments=arguments, name=""max""\n\t\t)\n\n\t\treturn minKer, maxKer\n\n\n\t@memoize\n\tdef getArithmKernel(op, dtype):\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\targuments = [(ctype.ptr, ""outdata""), (ctype.const.ptr, ""lhs""), (ctype.const.ptr, ""rhs"")]\n\t\tname = ""arithmKer""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 lhsvec = __half22float2(lhs2[i]), rhsvec = __half22float2(rhs2[i]);\n\t\t\t\toutdata2[i] = __float22half2_rn(make_float2(lhsvec.x %s rhsvec.x, lhsvec.y %s rhsvec.y));\n\t\t\t\t"""""" % (op, op),\n\t\t\t\t""outdata[i] = (float)lhs[i] %s (float)rhs[i]"" % op,\n\t\t\t\tname\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(\n\t\t\t\targuments,\n\t\t\t\t""outdata[i] = lhs[i] %s rhs[i]"" % op,\n\t\t\t\tname\n\t\t\t)\n\n\n\t@memoize\n\tdef getInplaceArithmKernel(op, dtype):\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\treturn ElementwiseKernel(\n\t\t\t[(ctype.ptr, ""outdata""), (ctype.const.ptr, ""indata"")],\n\t\t\t""outdata[i] %s indata[i]"" % op,\n\t\t\t""inplaceArithmKer""\n\t\t)\n\n\n\tdef enforceContiguous(self):\n\t\tif not self.contiguous:\n\t\t\traise ValueError(""gpuarray is not contiguous"")\n\n\n\tdef findParentAllocator(*args):\n\t\tfor arg in args:\n\t\t\tparent = arg.parent\n\n\t\t\tif isinstance(parent, Driver.MemoryPool):\n\t\t\t\treturn parent\n\n\t\treturn None\n\n\n\tdef enforceEqualShapesAndDtypes(self, other):\n\t\tenforceContiguous(self)\n\t\tenforceContiguous(other)\n\n\t\tif self.shape != other.shape:\n\t\t\traise ValueError(""gpuarray shapes are not equal"")\n\n\t\tif self.dtype != other.dtype:\n\t\t\traise ValueError(""gpuarray datatypes are not equal"")\n\n\n\tdef fill(self, val):\n\t\tenforceContiguous(self)\n\t\titem = self.dtype.type(val)\n\n\t\tif self.dtype.itemsize == 4:\n\t\t\tself.gpudata[:self.nbytes].fillD32(item.view(np.uint32))\n\t\telif self.dtype.itemsize == 2:\n\t\t\tself.gpudata[:self.nbytes].fillD16(item.view(np.uint16))\n\t\telif self.dtype.itemsize == 1:\n\t\t\tself.gpudata[:self.nbytes].fillD8(item.view(np.uint8))\n\t\telse:\n\t\t\tgetFillKernel(self.dtype)(self, self.dtype.type(val))\n\n\t\treturn self\n\n\n\tdef astype(self, dtype):\n\t\tenforceContiguous(self)\n\n\t\tallocator = findParentAllocator(self.gpudata)\n\t\toutdata = GPUArray.empty(self.shape, dtype, allocator=allocator)\n\n\t\tif self.dtype == dtype:\n\t\t\toutdata.set(self)\n\t\telse:\n\t\t\tgetConvertTypeKernel(self.dtype, np.dtype(dtype))(outdata, self)\n\n\t\treturn outdata\n\n\n\tdef setitem(self, key, value):\n\t\tself[key].set(value)\n\n\n\tdef aryMin(self):\n\t\tenforceContiguous(self)\n\n\t\tminKer, _ = getMinMaxKernel(self.dtype)\n\t\treturn minKer(self)\n\n\n\tdef aryMax(self):\n\t\tenforceContiguous(self)\n\n\t\t_, maxKer = getMinMaxKernel(self.dtype)\n\t\treturn maxKer(self)\n\n\n\tdef add(self, other):\n\t\tenforceEqualShapesAndDtypes(self, other)\n\n\t\tallocator = findParentAllocator(self.gpudata, other.gpudata)\n\t\toutdata = GPUArray.empty(self.shape, self.dtype, allocator=allocator)\n\n\t\tgetArithmKernel(""+"", self.dtype)(outdata, self, other)\n\t\treturn outdata\n\n\n\tdef mul(self, other):\n\t\tenforceEqualShapesAndDtypes(self, other)\n\n\t\tallocator = findParentAllocator(self.gpudata, other.gpudata)\n\t\toutdata = GPUArray.empty(self.shape, self.dtype, allocator=allocator)\n\n\t\tgetArithmKernel(""*"", self.dtype)(outdata, self, other)\n\t\treturn outdata\n\n\n\tdef iadd(self, other):\n\t\tenforceEqualShapesAndDtypes(self, other)\n\n\t\tgetInplaceArithmKernel(""+="", self.dtype)(self, other)\n\t\treturn self\n\n\n\tdef imul(self, other):\n\t\tenforceEqualShapesAndDtypes(self, other)\n\n\t\tgetInplaceArithmKernel(""*="", self.dtype)(self, other)\n\t\treturn self\n\n\n\tGPUArray.__setitem__ = setitem\n\n\tGPUArray.fill = fill\n\tGPUArray.astype = astype\n\tGPUArray.min = aryMin\n\tGPUArray.max = aryMax\n\n\tGPUArray.__add__ = add\n\tGPUArray.__mul__ = mul\n\tGPUArray.__iadd__ = iadd\n\tGPUArray.__imul__ = imul\n\n\n\treturn GPUArray\n\n\ndef unittest():\n\tfrom PuzzleLib.Cuda import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tbnd = Backend.getBackend(deviceIdx)\n\n\t\tfor dtype, _ in bnd.dtypesSupported():\n\t\t\tarithmTest(bnd, dtype)\n\t\t\tmemoryTest(bnd, dtype)\n\n\ndef arithmTest(bnd, dtype):\n\thostA = np.random.randn(13, 15).astype(dtype)\n\thostB = np.random.randn(13, 15).astype(dtype)\n\n\ta, b = bnd.GPUArray.toGpu(hostA), bnd.GPUArray.toGpu(hostB)\n\n\tc = a + b\n\tassert np.allclose(hostA + hostB, c.get())\n\n\td = a * b\n\tassert np.allclose(hostA * hostB, d.get())\n\n\ta.fill(3.0)\n\tassert np.allclose(np.full_like(hostA, fill_value=3.0), a.get())\n\n\tc = b.astype(np.float32 if dtype == np.float16 else np.float16)\n\tassert np.allclose(hostB.astype(c.dtype), c.get())\n\n\ndef memoryTest(bnd, dtype):\n\thostA = np.random.randn(10, 10).astype(dtype)\n\ta = bnd.GPUArray.toGpu(hostA)\n\n\tb = a[:, :6]\n\thostB = hostA[:, :6]\n\n\tassert np.allclose(hostB.reshape((2, 5, 6)), b.reshape(2, 5, 6).get())\n\tassert np.allclose(hostB.reshape((5, 2, 3, 2)), b.reshape(5, 2, 3, 2).get())\n\tassert np.allclose(hostB.reshape((10, 1, 6)), b.reshape(10, 1, 6).get())\n\n\thostA = np.random.randn(10, 10, 10).astype(dtype)\n\ta = bnd.GPUArray.toGpu(hostA)\n\n\tb = a[:, :, :6]\n\tassert np.allclose(hostA[:, :, :6], b.get())\n\n\thostB = np.random.randn(*b.shape).astype(dtype)\n\tb.set(hostB)\n\tassert np.allclose(hostB, b.get())\n\n\thostB = b.get()\n\tb = a[:, :6, :6]\n\tassert np.allclose(hostB[:, :6, :6], b.get())\n\n\thostB = np.random.randn(*b.shape).astype(dtype)\n\tb.set(hostB)\n\tassert np.allclose(hostB, b.get())\n\n\thostB = np.random.randn(10, 6, 10).astype(dtype)[:, :, :6]\n\tb.set(hostB)\n\tassert np.allclose(hostB, b.get())\n\n\thostB = np.random.randn(10, 10, 6).astype(dtype)[:, :6, :]\n\tb.set(hostB)\n\tassert np.allclose(hostB, b.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cuda/GPUBackend.py,5,"b'import sys, time\nimport numpy as np\n\nfrom PuzzleLib import Config\nfrom PuzzleLib.Cuda.Utils import prod, QueueManager\n\nfrom PuzzleLib.Cuda.Kernels import ElementWise\nfrom PuzzleLib.Cuda.Kernels.Costs import bce, hinge, smoothL1, l1Hinge, CostModule\nfrom PuzzleLib.Cuda.Kernels.CTC import CTCModule\nfrom PuzzleLib.Cuda.Kernels.Embedder import EmbedModule\nfrom PuzzleLib.Cuda.Kernels.MatVec import MatModule\nfrom PuzzleLib.Cuda.Kernels.Pad import PadModule\nfrom PuzzleLib.Cuda.Kernels.Pool import PoolModule\nfrom PuzzleLib.Cuda.Kernels.PRelu import PReluModule\nfrom PuzzleLib.Cuda.Kernels.Upsample import UpsampleModule\n\n\nclass GPUBackend:\n\tBackendName = None\n\n\twarpSize = None\n\tnthreads = None\n\n\tDriver = None\n\tGPUArray = None\n\tError = None\n\n\tSourceModule = None\n\tElementwiseKernel = None\n\tElementHalf2Kernel = None\n\tReductionKernel = None\n\n\tSharedArray = None\n\tRand, Blas, Dnn = None, None, None\n\n\tConvPerf = None\n\tConvFwdAlgo, ConvBwdDataAlgo, ConvBwdFilterAlgo = None, None, None\n\n\tRNNAlgo, RNNMode, DirectionMode = None, None, None\n\n\n\tdef __init__(self, deviceIdx, initmode=0):\n\t\tself.deviceIdx = deviceIdx\n\n\t\tndevices = self.Driver.Device.count()\n\t\tif ndevices == 0:\n\t\t\traise self.Error(""No %s enabled device found"" % self.BackendName)\n\n\t\tif deviceIdx >= ndevices:\n\t\t\traise self.Error(""Invalid %s config device index"" % self.BackendName)\n\n\t\tself.device = self.Driver.Device(deviceIdx).set()\n\t\tprint(""[%s] Using device #%s (%s)"" % (Config.libname, deviceIdx, self.device.name()), flush=True)\n\n\t\tif Config.systemLog:\n\t\t\tprint(\n\t\t\t\t""[%s] Created %s context (Using driver version: %s)"" %\n\t\t\t\t(Config.libname, self.BackendName, self.Driver.getDriverVersion()), flush=True\n\t\t\t)\n\n\t\tself.memoryPool = self.Driver.MemoryPool()\n\n\t\trngtype, seed = self.Rand.RAND_RNG_PSEUDO_XORWOW, int(np.random.randint(sys.maxsize, dtype=np.intp))\n\t\tself.globalRng = self.Rand.RandomNumberGenerator(type=rngtype, seed=seed)\n\n\t\tif Config.systemLog:\n\t\t\tprint(\n\t\t\t\t""[%s] Created %s global rng (type=%s, seed=%s)"" %\n\t\t\t\t(Config.libname, self.Rand.__name__, self.globalRng.type, hex(self.globalRng.seed)), flush=True\n\t\t\t)\n\n\t\tself.streamManager = QueueManager(objtype=self.Driver.Stream)\n\t\tself.eventManager = QueueManager(objtype=self.Driver.Event)\n\n\t\tself.blas, self.dnn = None, None\n\n\t\tself.costmod = None\n\t\tself.ctcmod = None\n\t\tself.embedmod = None\n\t\tself.matmod = None\n\t\tself.padmod = None\n\t\tself.poolmod = None\n\t\tself.prelumod = None\n\t\tself.upsamplemod = None\n\n\t\tself.bceKer = None\n\t\tself.hingeKer = None\n\t\tself.smoothL1Ker = None\n\t\tself.l1HingeKer = None\n\t\tself.getAccuracyKernel = None\n\n\t\tself.sigmoidKer = None\n\t\tself.sigmoidDerKer = None\n\t\tself.tanhKer = None\n\t\tself.tanhDerKer = None\n\t\tself.reluKer = None\n\t\tself.reluDerKer = None\n\t\tself.leakyReluKer = None\n\t\tself.leakyReluDerKer = None\n\t\tself.eluKer = None\n\t\tself.eluDerKer = None\n\t\tself.softPlusKer = None\n\t\tself.softPlusDerKer = None\n\t\tself.clipKer = None\n\t\tself.clipDerKer = None\n\t\tself.geluKer = None\n\t\tself.geluDerKer = None\n\n\t\tself.dropoutKer = None\n\t\tself.dropout2dKer = None\n\t\tself.rbmKer = None\n\t\tself.absKer = None\n\t\tself.toVectorAddVectorKer = None\n\n\t\tself.classicMomSGDKer = None\n\t\tself.nesterovMomSGDKer = None\n\t\tself.rmspropKer = None\n\t\tself.adamKer = None\n\t\tself.rmspropGravesKer = None\n\t\tself.adagradKer = None\n\t\tself.adadeltaKer = None\n\t\tself.smorms3Ker = None\n\n\t\tself.weightDecayKer = None\n\t\tself.linearKer = None\n\t\tself.addKer = None\n\t\tself.mulKer = None\n\t\tself.l1penaltyKer = None\n\t\tself.l1gradKer = None\n\n\t\tself.castFP16toFP32 = None\n\t\tself.castFP32toFP16 = None\n\n\t\tself.initmode = 0\n\t\tself.updateBackend(initmode)\n\n\n\tdef updateBackend(self, initmode):\n\t\tif initmode > 0 >= self.initmode:\n\t\t\tself.initLibs()\n\n\t\tif initmode > 1 >= self.initmode:\n\t\t\tself.initKernels()\n\n\t\tself.initmode = max(initmode, self.initmode)\n\n\n\tdef initLibs(self):\n\t\tself.blas = self.Blas.BlasContext().enableTensorOps(True)\n\t\tif Config.systemLog:\n\t\t\tprint(\n\t\t\t\t""[%s] Created %s context (Using version: %s)"" %\n\t\t\t\t(Config.libname, self.Blas.__name__, self.blas.getVersion())\n\t\t\t)\n\n\t\tself.dnn = self.Dnn.DnnContext().enableTensorOps(True)\n\t\tif Config.systemLog:\n\t\t\tprint(\n\t\t\t\t""[%s] Created %s context (Using version: %s)"" %\n\t\t\t\t(Config.libname, self.Dnn.__name__, self.dnn.getVersion())\n\t\t\t)\n\n\n\tdef initKernels(self):\n\t\tself.costmod = CostModule(self)\n\t\tself.ctcmod = CTCModule(self)\n\t\tself.embedmod = EmbedModule(self)\n\t\tself.matmod = MatModule(self)\n\t\tself.padmod = PadModule(self)\n\t\tself.poolmod = PoolModule(self)\n\t\tself.prelumod = PReluModule(self.matmod)\n\t\tself.upsamplemod = UpsampleModule(self)\n\n\t\tself.bceKer = bce(self.ElementwiseKernel)\n\t\tself.hingeKer = hinge(self.ElementwiseKernel)\n\t\tself.smoothL1Ker = smoothL1(self.ElementwiseKernel)\n\t\tself.l1HingeKer = l1Hinge(self.ElementwiseKernel)\n\t\tself.getAccuracyKernel = self.costmod.getAccuracyKernel\n\n\t\tself.sigmoidKer = ElementWise.sigmoid(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.sigmoidDerKer = ElementWise.sigmoidDer(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.tanhKer = ElementWise.tanh(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.tanhDerKer = ElementWise.tanhDer(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.reluKer = ElementWise.relu(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.reluDerKer = ElementWise.reluDer(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.leakyReluKer = ElementWise.leakyRelu(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.leakyReluDerKer = ElementWise.leakyReluDer(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.eluKer = ElementWise.elu(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.eluDerKer = ElementWise.eluDer(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.softPlusKer = ElementWise.softPlus(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.softPlusDerKer = ElementWise.softPlusDer(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.clipKer = ElementWise.clip(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.clipDerKer = ElementWise.clipDer(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.geluKer = ElementWise.gelu(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.geluDerKer = ElementWise.geluDer(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\n\t\tself.dropoutKer = ElementWise.dropout(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.dropout2dKer = ElementWise.dropout2d(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.rbmKer = ElementWise.rbmKer(self.ElementwiseKernel)\n\t\tself.absKer = ElementWise.absKer(self.ElementwiseKernel)\n\t\tself.toVectorAddVectorKer = ElementWise.toVectorAddVector(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\n\t\tself.classicMomSGDKer = ElementWise.classicMomSGD(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.nesterovMomSGDKer = ElementWise.nesterovMomSGD(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.rmspropKer = ElementWise.rmsprop(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.adamKer = ElementWise.adam(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.rmspropGravesKer = ElementWise.rmspropGraves(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.adagradKer = ElementWise.adagrad(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.adadeltaKer = ElementWise.adadelta(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.smorms3Ker = ElementWise.smorms3(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\n\t\tself.weightDecayKer = ElementWise.weightDecayKer(self.ElementwiseKernel)\n\t\tself.linearKer = ElementWise.linear(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.addKer = ElementWise.add(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.mulKer = ElementWise.mul(self.ElementwiseKernel, self.ElementHalf2Kernel)\n\t\tself.l1penaltyKer = ElementWise.l1penaltyKer(self.ElementwiseKernel)\n\t\tself.l1gradKer = ElementWise.l1gradKer(self.ElementwiseKernel)\n\n\t\tself.castFP16toFP32 = ElementWise.castFP16toFP32(self.ElementwiseKernel)\n\t\tself.castFP32toFP16 = ElementWise.castFP32toFP16(self.ElementwiseKernel)\n\n\n\t@staticmethod\n\tdef dtypesSupported():\n\t\treturn [(np.float32, 1e-5), (np.float16, 1e-2)]\n\n\n\t@staticmethod\n\tdef copy(dest, source, allocator=None):\n\t\tif dest is None:\n\t\t\treturn source.copy(allocator=allocator)\n\t\telse:\n\t\t\tdest.set(source)\n\t\t\treturn dest\n\n\n\tdef getDeviceComputeCap(self, index):\n\t\treturn self.Driver.Device(index).computeCapability()\n\n\n\tdef fillUniform(self, data, minval=0.0, maxval=1.0, rng=None):\n\t\tassert data.dtype == np.float32\n\n\t\trng = self.globalRng if rng is None else rng\n\t\trng.fillUniform(data)\n\n\t\tdtype = data.dtype\n\t\tself.linearKer(dtype)(data, data, dtype.type(maxval - minval), dtype.type(minval))\n\n\n\tdef fillNormal(self, data, mean=0.0, stddev=1.0, rng=None):\n\t\trng = self.globalRng if rng is None else rng\n\t\trng.fillNormal(data, mean=mean, stddev=stddev)\n\n\n\tdef dstack(self, tup, allocator=None):\n\t\treturn self.concatenate(tup, axis=2, allocator=allocator)\n\n\n\tdef hstack(self, tup, allocator=None):\n\t\treturn self.concatenate(tup, axis=1, allocator=allocator)\n\n\n\tdef vstack(self, tup, allocator=None):\n\t\treturn self.concatenate(tup, axis=0, allocator=allocator)\n\n\n\tdef dsplit(self, ary, sections, allocator=None):\n\t\treturn self.split(ary, sections, axis=2, allocator=allocator)\n\n\n\tdef hsplit(self, ary, sections, allocator=None):\n\t\treturn self.split(ary, sections, axis=1, allocator=allocator)\n\n\n\tdef vsplit(self, ary, sections, allocator=None):\n\t\treturn self.split(ary, sections, axis=0, allocator=allocator)\n\n\n\tdef concatenate(self, tup, axis, out=None, allocator=None):\n\t\tary = tup[0]\n\n\t\tdtype, reducedShape = ary.dtype, ary.shape\n\t\treducedShape = reducedShape[:axis] + reducedShape[axis + 1:]\n\n\t\tassert all(a.dtype == dtype and a.shape[:axis] + a.shape[axis + 1:] == reducedShape for a in tup[1:])\n\n\t\tconcatDim = sum(a.dimAt(axis) for a in tup)\n\t\tshape = reducedShape[:axis] + (concatDim, ) + reducedShape[axis:]\n\n\t\tif out is None:\n\t\t\tout = self.GPUArray.empty(shape, dtype=dtype, allocator=allocator)\n\t\telse:\n\t\t\tassert out.shape == shape and out.dtype == dtype\n\n\t\tdstPitch = out.strideAt(axis - 1) if axis > 0 else out.nbytes\n\t\theight = prod(shape[:axis])\n\n\t\tstride = 0\n\n\t\tfor a in tup:\n\t\t\tsrcPitch = width = a.strideAt(axis - 1) if axis > 0 else a.nbytes\n\n\t\t\tself.Driver.memcpy2D(width, height, a.gpudata, srcPitch, out.gpudata, dstPitch, dstX=stride)\n\t\t\tstride += width\n\n\t\treturn out\n\n\n\tdef split(self, ary, sections, axis, allocator=None):\n\t\tshape = ary.shape\n\t\tassert sum(sections) == shape[axis]\n\n\t\touts = [\n\t\t\tself.GPUArray.empty(shape[:axis] + (sec, ) + shape[axis + 1:], dtype=ary.dtype, allocator=allocator)\n\t\t\tfor sec in sections\n\t\t]\n\n\t\tsrcPitch = ary.strideAt(axis - 1) if axis > 0 else ary.nbytes\n\t\theight = prod(shape[:axis])\n\n\t\tstride = 0\n\n\t\tfor out in outs:\n\t\t\tdstPitch = width = out.strideAt(axis - 1) if axis > 0 else out.nbytes\n\n\t\t\tself.Driver.memcpy2D(width, height, ary.gpudata, srcPitch, out.gpudata, dstPitch, srcX=stride)\n\t\t\tstride += width\n\n\t\treturn outs\n\n\n\tdef tile(self, ary, repeats, axis, allocator=None):\n\t\treturn self.concatenate([ary] * repeats, axis=axis, allocator=allocator)\n\n\n\tdef timeKernel(self, func, args, kwargs=None, looplength=1000, log=True, logname=None, normalize=False,\n\t\t\t\t   hotpass=True):\n\t\tkwargs = {} if kwargs is None else kwargs\n\n\t\tif hotpass:\n\t\t\tfunc(*args, **kwargs)\n\n\t\tstart, end = self.Driver.Event(), self.Driver.Event()\n\n\t\thostStart = time.time()\n\t\tstart.record()\n\n\t\tfor _ in range(looplength):\n\t\t\tfunc(*args, **kwargs)\n\n\t\tend.record()\n\t\thostEnd = time.time()\n\n\t\tend.synchronize()\n\t\tmillisInSec = 1e-3\n\n\t\tdevsecs = start.timeTill(end) * millisInSec\n\t\thostsecs = hostEnd - hostStart\n\n\t\tif logname is None:\n\t\t\tfuncname = func.__name__ if hasattr(func, ""__name__"") else func.__class__.__name__\n\t\t\tlogname = ""%s.%s"" % (func.__module__, funcname)\n\n\t\tif normalize:\n\t\t\tdevsecs /= looplength\n\t\t\thostsecs /= looplength\n\n\t\tif log:\n\t\t\tprint(""%s device time: %s secs"" % (logname, devsecs))\n\t\t\tprint(""%s host time: %s secs"" % (logname, hostsecs))\n\n\t\treturn devsecs, hostsecs\n\n\n\tdef convNdbenchmark(self, datashape, Wshape, dtype, stride=1, pad=0, dilation=1, groups=1, algoCount=10):\n\t\tresults = self.dnn.convNdbenchmark(datashape, Wshape, dtype, stride, pad, dilation, groups, algoCount)\n\t\tresults = tuple(\n\t\t\t[self.ConvPerf(algotype(values[0]), *values[1:]) for values in subresults] for algotype, subresults in\n\t\t\tzip((self.ConvFwdAlgo, self.ConvBwdDataAlgo, self.ConvBwdFilterAlgo), results)\n\t\t)\n\n\t\treturn results\n\n\n\tdef instanceNorm2d(self, data, scale, bias, epsilon=1e-5, out=None, allocator=None):\n\t\tbatchsize, maps, height, width = data.shape\n\t\textmaps = batchsize * maps\n\n\t\tindata = data.reshape(1, extmaps, height, width)\n\n\t\tmean = self.GPUArray.empty((extmaps, ), dtype=np.float32, allocator=allocator)\n\t\tvar = self.GPUArray.empty((extmaps, ), dtype=np.float32, allocator=allocator)\n\n\t\tif batchsize > 1:\n\t\t\tscale = self.tile(scale, batchsize, axis=0, allocator=allocator)\n\t\t\tbias = self.tile(bias, batchsize, axis=0, allocator=allocator)\n\n\t\toutdata, savemean, saveinvvar = self.dnn.batchNormNd(\n\t\t\tindata, mean, var, scale, bias, epsilon, test=False, out=out, allocator=allocator\n\t\t)\n\t\treturn outdata.reshape(data.shape), savemean, saveinvvar, scale\n\n\n\tdef instanceNorm2dBackward(self, grad, data, extscale, savemean, saveinvvar, epsilon, affine=True,\n\t\t\t\t\t\t\t   out=None, allocator=None):\n\t\tbatchsize, maps, height, width = grad.shape\n\t\textmaps = batchsize * maps\n\n\t\toutgrad = grad.reshape(1, extmaps, height, width)\n\t\tindata = data.reshape(1, extmaps, height, width)\n\n\t\tingrad, scalegrad, bgrad = self.dnn.batchNormNdBackward(\n\t\t\toutgrad, indata, extscale, savemean, saveinvvar, epsilon, out=out, allocator=allocator\n\t\t)\n\n\t\tif affine and batchsize > 1:\n\t\t\tscalegrad = self.matmod.matsum(scalegrad.reshape(batchsize, -1), axis=0, allocator=allocator)\n\t\t\tbgrad = self.matmod.matsum(bgrad.reshape(batchsize, -1), axis=0, allocator=allocator)\n\n\t\treturn (ingrad.reshape(grad.shape), scalegrad, bgrad) if affine else ingrad.reshape(grad.shape)\n\n\n\tdef createRnn(self, insize, hsize, dtype, layers=1, algo=None, mode=None, direction=None, dropout=0.0,\n\t\t\t\t  seed=0, batchsize=0):\n\t\traise NotImplementedError()\n\n\n\tdef acquireRnnParams(self, rnn, W):\n\t\traise NotImplementedError()\n\n\n\tdef updateRnnParams(self, rnn, W, params):\n\t\tpass\n\n\n\tdef deviceSupportsBatchHint(self):\n\t\treturn False\n'"
Cuda/SourceModule.py,23,"b'import sys, os, stat\nfrom string import Template\n\nimport numpy as np\n\nfrom PuzzleLib.Compiler.Codegen.Types import PointerType\nfrom PuzzleLib.Compiler.Codegen.Types import half_t, half2_t, float_t, double_t\nfrom PuzzleLib.Compiler.Codegen.Types import schar_t, short_t, short2_t, int_t, llong_t\nfrom PuzzleLib.Compiler.Codegen.Types import uchar_t, ushort_t, ushort2_t, uint_t, ullong_t\n\n\ndtypeToCtype = {\n\tnp.float16: half_t,\n\tnp.float32: float_t,\n\tnp.float64: double_t,\n\n\tnp.int8: schar_t,\n\tnp.int16: short_t,\n\tnp.int32: int_t,\n\tnp.int64: llong_t,\n\n\tnp.uint8: uchar_t,\n\tnp.uint16: ushort_t,\n\tnp.uint32: uint_t,\n\tnp.uint64: ullong_t\n}\n\n\nctypeToDtype = {ctype: dtype for (dtype, ctype) in dtypeToCtype.items()}\n\n\nclass SourceModule:\n\tDriver = None\n\n\n\tdef __init__(self, source, options=None, includes=None, externC=False, verbose=True, debug=False, name=None):\n\t\tself.source = source\n\t\tself.externC = externC\n\n\t\tself.options = options if options is not None else self.getDefaultOptions()\n\t\tself.includes = includes\n\n\t\tif debug and name is None:\n\t\t\traise self.Driver.RtcError(""invalid source module name for debug mode"")\n\n\t\tself.debug = debug\n\t\tself.name = name\n\n\t\tself.verbose = verbose\n\t\tself.cumod = None\n\n\t\tself.functions = {}\n\n\n\tdef build(self):\n\t\tsource = ""extern \\""C\\""\\n{\\n%s\\n}\\n"" % self.source if self.externC else self.source\n\n\t\toptions = self.options\n\t\tname = ""%s.debug.cu"" % self.name if self.debug else (None if self.name is None else ""%s.cu"" % self.name)\n\n\t\tif self.debug:\n\t\t\twith open(name, mode=""w"") as f:\n\t\t\t\tf.write(source)\n\n\t\t\tos.chmod(name, stat.S_IREAD | stat.S_IRGRP | stat.S_IROTH)\n\t\t\toptions = options + [""-G""]\n\n\t\tptx, log = self.Driver.compile(source, options=options, includes=self.includes, name=name)\n\n\t\tif ptx is None:\n\t\t\ttext = log if self.debug else ""%s\\nSource:\\n%s"" % (\n\t\t\t\tlog,\n\t\t\t\t""\\n"".join(""%-4s    %s"" % (i + 1, line) for i, line in enumerate(source.splitlines(keepends=False)))\n\t\t\t)\n\n\t\t\traise self.Driver.RtcError(text)\n\n\t\telif log is not None and self.verbose:\n\t\t\tprint(log, flush=True)\n\n\t\tself.cumod = self.Driver.Module(ptx)\n\n\n\tdef getFunction(self, name):\n\t\tfunc = self.functions.get(name, None)\n\n\t\tif func is None:\n\t\t\tif self.cumod is None:\n\t\t\t\tself.build()\n\n\t\t\tfunc = self.cumod.getFunction(name)\n\t\t\tself.functions[name] = func\n\n\t\treturn func\n\n\n\tdef __len__(self):\n\t\tassert False\n\n\n\tdef __getattr__(self, name):\n\t\treturn self.getFunction(name)\n\n\n\t@classmethod\n\tdef getDefaultOptions(cls):\n\t\tdeviceIdx = cls.Driver.Device.getCurrent()\n\n\t\treturn [\n\t\t\t""-arch=compute_%s%s"" % cls.Driver.Device(deviceIdx).computeCapability(), ""-use_fast_math"",\n\t\t\t""-I%s%sinclude"" % (os.environ[""CUDA_PATH""] if sys.platform == ""win32"" else ""/usr/local/cuda"", os.sep)\n\t\t]\n\n\nclass Kernel:\n\tDriver = None\n\tSourceModule = None\n\n\twarpBit, warpSize = None, None\n\tblockBit, blockSize = None, None\n\n\n\tdef __init__(self, arguments, name):\n\t\tself.arguments, self.name = arguments, name\n\t\tself.module = None\n\n\n\tdef prepareArguments(self, args):\n\t\tGPUArray = self.Driver.GPUArray\n\t\tsize = next(arg.size for arg in args if isinstance(arg, GPUArray))\n\n\t\targs = tuple(\n\t\t\tctypeToDtype[T](arg) if not isinstance(T, PointerType) else arg\n\t\t\tfor arg, (T, name) in zip(args, self.arguments)\n\t\t)\n\n\t\treturn size, args\n\n\n\tdef generateSource(self):\n\t\traise NotImplementedError()\n\n\nclass ElementwiseKernel(Kernel):\n\tdef __init__(self, arguments, operation, name, preambule=""""):\n\t\tsuper().__init__(arguments, name)\n\n\t\tself.operation = operation\n\t\tself.preambule = preambule\n\n\n\tdef generateSource(self):\n\t\targuments = "", "".join(\n\t\t\t(T.restrict if isinstance(T, PointerType) else T).typegen(asDecl=True) % name\n\t\t\tfor T, name in self.arguments\n\t\t)\n\n\t\treturn self.eltwiseTmpl.substitute(\n\t\t\targuments=arguments, operation=self.operation, name=self.name, preambule=self.preambule\n\t\t)\n\n\n\tdef prepareForSlice(self, slc, size, args):\n\t\tfuncname = ""%s_strided"" % self.name\n\n\t\tstart = 0 if slc.start is None else slc.start\n\t\tstop = size if slc.stop is None else slc.stop\n\t\tstep = 1 if slc.step is None else slc.step\n\n\t\targs += (np.int32(start), np.int32(stop), np.int32(step))\n\t\tsize = (stop - start + step - 1) // step\n\n\t\treturn funcname, size, args\n\n\n\t@classmethod\n\tdef prepareGrid(cls, size):\n\t\tif size < cls.blockSize:\n\t\t\tblock, grid = ((size + cls.warpSize - 1) >> cls.warpBit << cls.warpBit, 1, 1), (1, 1, 1)\n\t\telse:\n\t\t\tblock, grid = (cls.blockSize, 1, 1), ((size + cls.blockSize - 1) >> cls.blockBit, 1, 1)\n\n\t\treturn block, grid\n\n\n\tdef __call__(self, *args, **kwargs):\n\t\tif self.module is None:\n\t\t\tself.module = self.SourceModule(self.generateSource(), name=self.name)\n\n\t\tfuncname = self.name\n\t\tsize, args = self.prepareArguments(args)\n\n\t\tslc = kwargs.get(""slice"", None)\n\t\tif slc is not None:\n\t\t\tfuncname, size, args = self.prepareForSlice(slc, size, args)\n\n\t\tfunc = self.module.getFunction(funcname)\n\n\t\tblock, grid = self.prepareGrid(size)\n\t\tfunc(*args, np.intp(size), block=block, grid=grid)\n\n\n\teltwiseTmpl = Template(""""""\n\n$preambule\n\nextern ""C"" __global__ void $name($arguments, int size)\n{\n\tint i = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (i < size)\n\t{\n\t\t$operation;\n\t}\n}\n\n\nextern ""C"" __global__ void ${name}_strided($arguments, int start, int stop, int step)\n{\n\tint i = start + (threadIdx.x + blockIdx.x * blockDim.x) * step;\n\tif (i < stop)\n\t{\n\t\t$operation;\n\t}\n}\n\n"""""")\n\n\nclass ElementHalf2Kernel(ElementwiseKernel):\n\tdef __init__(self, arguments, operation2, operation, name, preambule=""""):\n\t\tsuper().__init__(arguments, operation, name, preambule)\n\t\tself.operation2 = operation2\n\n\n\t@classmethod\n\tdef prepareGrid(cls, size):\n\t\treturn super().prepareGrid(size >> 1 if size > 1 else 1)\n\n\n\tdef generateSource(self):\n\t\targs = tuple((T.restrict if isinstance(T, PointerType) else T, name) for T, name in self.arguments)\n\t\targuments = "", "".join(T.typegen(asDecl=True) % name for T, name in args)\n\n\t\tcasts = []\n\n\t\tfor T, name in args:\n\t\t\tU = T.unqual\n\n\t\t\tif isinstance(U, PointerType):\n\t\t\t\tB = U.basetype.unqual\n\n\t\t\t\textB = {\n\t\t\t\t\thalf_t: half2_t,\n\t\t\t\t\tshort_t: short2_t,\n\t\t\t\t\tushort_t: ushort2_t\n\t\t\t\t}.get(B, None)\n\n\t\t\t\tif extB is not None:\n\t\t\t\t\tcasts.append(\n\t\t\t\t\t\t""%s %s2 __attribute__((unused)) = (%s)%s;"" % (T.basedWith(extB), name, U.basedWith(extB), name)\n\t\t\t\t\t)\n\n\t\treturn self.eltwiseTmpl.substitute(\n\t\t\tname=self.name, arguments=arguments, casts=""\\n"".join(casts),\n\t\t\toperation2=self.operation2, operation=self.operation, preambule=self.preambule\n\t\t)\n\n\n\tdef prepareForSlice(self, slc, size, args):\n\t\tassert False\n\n\n\teltwiseTmpl = Template(""""""\n\n$preambule\n#include <cuda_fp16.h>\n\n\nextern ""C"" __global__ void $name($arguments, int size)\n{\n\tint i = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (i < size / 2)\n\t{\n\t\t$casts\n\t\t$operation2;\n\t}\n\n\tif (size % 2 == 1 && i == size / 2 - 1 || size == 1 && i == 0)\n\t{\n\t\ti = size - 1;\n\t\t$operation;\n\t}\n}\n\n"""""")\n\n\nclass ReductionKernel(Kernel):\n\tdef __init__(self, outtype, neutral, reduceExpr, mapExpr, arguments, name):\n\t\tsuper().__init__(arguments, name)\n\n\t\tself.outtype, self.neutral = outtype, neutral\n\t\tself.reduceExpr, self.mapExpr = reduceExpr, mapExpr\n\n\n\tdef generateSource(self):\n\t\tT = dtypeToCtype[self.outtype]\n\n\t\targuments = [(T.restrict if isinstance(T, PointerType) else T, name) for T, name in self.arguments]\n\t\targuments = "", "".join(T.typegen(asDecl=True) % name for T, name in arguments)\n\n\t\tstage1 = self.reduceTmpl.substitute(\n\t\t\tT=T, neutral=self.neutral, reduceExpr=self.reduceExpr, mapExpr=self.mapExpr,\n\t\t\targuments=arguments, warpSize=self.warpSize, NT=self.blockSize, name=""%s_stage1"" % self.name\n\t\t)\n\n\t\tstage2 = self.reduceTmpl.substitute(\n\t\t\tT=T, neutral=self.neutral, reduceExpr=self.reduceExpr, mapExpr=""indata[i]"",\n\t\t\targuments=""const %s* indata"" % T, warpSize=self.warpSize, NT=self.blockSize, name=""%s_stage2"" % self.name\n\t\t)\n\n\t\treturn stage1 + stage2\n\n\n\tdef reduce(self, stage, allocator, *args):\n\t\tsize, args = self.prepareArguments(args)\n\n\t\tblocks = min((size + self.blockSize - 1) >> self.blockBit, self.blockSize)\n\t\tpartials = self.Driver.GPUArray.empty((blocks, ) if blocks > 1 else (), dtype=self.outtype, allocator=allocator)\n\n\t\tkernel = self.module.getFunction(""%s_stage%s"" % (self.name, stage))\n\t\tkernel(*args, partials, np.int32(size), block=(self.blockSize, 1, 1), grid=(blocks, 1, 1))\n\n\t\treturn self.reduce(2, allocator, partials) if blocks > 1 else partials\n\n\n\tdef __call__(self, *args, **kwargs):\n\t\tif self.module is None:\n\t\t\tself.module = self.SourceModule(self.generateSource(), name=self.name)\n\n\t\tallocator = kwargs.get(""allocator"", None)\n\t\treturn self.reduce(1, allocator, *args)\n\n\n\treduceTmpl = Template(""""""\n\n#undef READ_AND_MAP\n#undef REDUCE\n\n#define READ_AND_MAP(i) ($mapExpr)\n#define REDUCE(a, b) ($reduceExpr)\n\n\nextern ""C"" __global__ void $name($arguments, $T *partials, int size)\n{\n\t__shared__ $T sdata[$warpSize];\n\n\tint tid = threadIdx.x;\n\tint gid = tid + blockIdx.x * $NT;\n\n\t$T acc = $neutral;\n\n\tfor (int i = gid; i < size; i += $NT * gridDim.x)\n\t\tacc = REDUCE(acc, READ_AND_MAP(i));\n\n\tfor (int mask = $warpSize / 2; mask > 0; mask /= 2)\n\t{\n\t\t$T upval = __shfl_xor_sync((unsigned)-1, acc, mask, $warpSize);\n\t\tacc = REDUCE(acc, upval);\n\t}\n\n\tif (tid % $warpSize == 0)\n\t\tsdata[tid / $warpSize] = acc;\n\n\t__syncthreads();\n\tint nwarps = $NT / $warpSize;\n\n\tif (tid < $warpSize)\n\t{\n\t\tacc = (tid < nwarps) ? sdata[tid] : $neutral;\n\n\t\tfor (int mask = $warpSize / 2; mask > 0; mask /= 2)\n\t\t{\n\t\t\t$T upval = __shfl_xor_sync((unsigned)-1, acc, mask, $warpSize);\n\t\t\tacc = REDUCE(acc, upval);\n\t\t}\n\t}\n\n\tif (tid == 0)\n\t\tpartials[blockIdx.x] = acc;\n}\n\n"""""")\n\n\ndef unittest():\n\tfrom PuzzleLib.Cuda import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tbnd = Backend.getBackend(deviceIdx)\n\n\t\trtcTest(bnd)\n\t\teltwiseTest(bnd)\n\t\treductionTest(bnd)\n\n\ndef rtcTest(bnd):\n\tsource = """"""\n\nextern ""C"" __global__ void linearKer(float *outdata, const float *indata, float a, float b, int size)\n{\n\tint tid = threadIdx.x;\n\tint gridsize = gridDim.x * blockDim.x;\n\tint start = blockDim.x * blockIdx.x;\n\n\tfor (int i = start + tid; i < size; i += gridsize)\n\t\toutdata[i] = a * indata[i] + b;\n}\n\n""""""\n\n\toptions = bnd.SourceModule.getDefaultOptions()\n\tptx, errors = bnd.Driver.compile(source, options=[""-lineinfo""] + options, name=""linearKer.c"")\n\n\tassert ptx is not None and errors is None\n\tprint(ptx.decode())\n\n\ndef eltwiseTest(bnd):\n\thostInData = np.random.randint(0, 1000, size=(1 << 18, ), dtype=np.int32)\n\n\tindata = bnd.GPUArray.toGpu(hostInData)\n\toutdata = bnd.GPUArray.empty((1 << 18, ), dtype=np.int32)\n\n\tsquare = bnd.ElementwiseKernel(\n\t\t[(int_t.ptr, ""outdata""), (int_t.const.ptr, ""indata"")],\n\t\t""outdata[i] = indata[i] * indata[i]"",\n\t\t""square""\n\t)\n\n\tsquare(outdata, indata)\n\n\thostOutData = hostInData**2\n\tassert np.allclose(hostOutData, outdata.get())\n\n\tsquare(outdata, outdata, slice=slice(None, None, 10))\n\n\thostOutData[::10] = hostOutData[::10]**2\n\tassert np.allclose(hostOutData, outdata.get())\n\n\ndef reductionTest(bnd):\n\tsumkernel = bnd.ReductionKernel(\n\t\tnp.float32, neutral=""0.0f"", reduceExpr=""a + b"", mapExpr=""data[i]"", arguments=[(float_t.const.ptr, ""data"")],\n\t\tname=""sum""\n\t)\n\n\thostData1 = np.random.randn((1 << 18) + 1).astype(np.float32)\n\thostData2 = np.ones(shape=((1 << 20) + 1, ), dtype=np.float32)\n\n\tfor hostData in (hostData1, hostData2):\n\t\tdata = bnd.GPUArray.toGpu(hostData)\n\n\t\tacc = sumkernel(data)\n\t\thostAcc = np.sum(hostData)\n\n\t\tassert np.isclose(hostAcc, acc.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cuda/Utils.py,24,"b'import functools, operator\nfrom collections import OrderedDict\n\nimport numpy as np\n\n\ndef roundUpDiv(a, b):\n\treturn (a + b - 1) // b\n\n\ndef roundUp(a, b):\n\treturn roundUpDiv(a, b) * b\n\n\ndef prod(seq, start=1):\n\treturn functools.reduce(operator.mul, seq, start)\n\n\nclass SharedArray:\n\tGPUArray = None\n\talignment = 16\n\n\n\tdef __init__(self, dtype=np.float32, allocator=None):\n\t\tself.ary = None\n\t\tself.blocks = OrderedDict()\n\n\t\tself.dtype = np.dtype(dtype)\n\t\tself.allocator = allocator\n\n\n\tdef register(self, shape, dtype, name):\n\t\tassert name not in self.blocks\n\t\tassert dtype == self.dtype\n\n\t\tself.blocks[name] = (shape, prod(shape) * self.dtype.itemsize)\n\n\n\tdef build(self):\n\t\ttotalbytes = sum(self.align(nbytes) for _, nbytes in self.blocks.values())\n\n\t\tself.ary = self.GPUArray.empty(\n\t\t\tshape=(totalbytes // self.dtype.itemsize, ), dtype=self.dtype, allocator=self.allocator\n\t\t)\n\n\t\tblocks = OrderedDict()\n\t\toffset = 0\n\n\t\tfor name, (shape, nbytes) in self.blocks.items():\n\t\t\tblocks[name] = self.GPUArray(\n\t\t\t\tshape=shape, dtype=self.dtype, gpudata=self.ary.gpudata[offset:offset + nbytes]\n\t\t\t)\n\t\t\toffset += self.align(nbytes)\n\n\t\tself.blocks = blocks\n\n\n\tdef __getitem__(self, item):\n\t\treturn self.blocks[item]\n\n\n\t@classmethod\n\tdef align(cls, nbytes):\n\t\treturn (nbytes + cls.alignment - 1) // cls.alignment * cls.alignment\n\n\nclass QueueManager:\n\tdef __init__(self, objtype):\n\t\tself.objtype = objtype\n\t\tself.items = []\n\n\n\tdef reserve(self, nitems):\n\t\tself.items.extend(self.objtype() for _ in range(nitems))\n\n\n\tdef borrow(self, nitems):\n\t\tif len(self.items) < nitems:\n\t\t\tself.reserve(nitems - len(self.items))\n\n\t\tnewEnd = len(self.items) - nitems\n\n\t\tborrowed = self.items[newEnd:]\n\t\tself.items = self.items[:newEnd]\n\n\t\treturn borrowed\n\n\n\tdef give(self, items):\n\t\tself.items.extend(items)\n\n\n\tdef clear(self):\n\t\tself.items.clear()\n\n\ndef setupDebugAllocator(GPUArray):\n\tempty = GPUArray.empty\n\n\tdef emptyDebug(shape, dtype, allocator=None):\n\t\tary = empty(shape, dtype, allocator)\n\t\tdtype = np.dtype(dtype).type\n\n\t\tif issubclass(dtype, np.floating):\n\t\t\tvalue = dtype(np.nan)\n\t\telif issubclass(dtype, np.integer):\n\t\t\tvalue = np.iinfo(dtype).max\n\t\telse:\n\t\t\traise NotImplementedError(dtype)\n\n\t\tary.fill(value)\n\t\treturn ary\n\n\tGPUArray.empty = emptyDebug\n\n\ndef unittest():\n\tfrom PuzzleLib.Cuda import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tbnd = Backend.getBackend(deviceIdx, initmode=2)\n\n\t\tfor dtype, _ in bnd.dtypesSupported():\n\t\t\tshareMemTest(bnd, dtype)\n\t\t\tmemCopyTest(bnd, dtype)\n\n\t\trandomTest(bnd)\n\n\ndef shareMemTest(bnd, dtype):\n\tshMem = bnd.SharedArray(dtype=dtype)\n\n\tshMem.register((10, 10, 10), dtype, ""a"")\n\tshMem.register((50, 1, 5), dtype, ""b"")\n\tshMem.build()\n\n\ta, b = shMem[""a""], shMem[""b""]\n\tassert a.shape == (10, 10, 10) and a.dtype == dtype\n\tassert b.shape == (50, 1, 5) and b.dtype == dtype\n\n\ndef memCopyTest(bnd, dtype):\n\thostSrc = np.random.randn(4, 4, 4, 4).astype(dtype)\n\n\tsrc = bnd.GPUArray.toGpu(hostSrc)\n\tassert np.allclose(hostSrc, src.copy().get())\n\n\thostA = np.random.randn(7, 4, 4, 4).astype(dtype)\n\ta = bnd.GPUArray.toGpu(hostA)\n\n\tout = bnd.concatenate((src, a), axis=0)\n\tassert np.allclose(np.concatenate((hostSrc, hostA), axis=0), out.get())\n\n\thostA = np.random.randn(4, 2, 4, 4).astype(dtype)\n\thostB = np.random.randn(4, 1, 4, 4).astype(dtype)\n\n\ta, b = bnd.GPUArray.toGpu(hostA), bnd.GPUArray.toGpu(hostB)\n\n\tout = bnd.concatenate((src, a, b), axis=1)\n\tassert np.allclose(np.concatenate((hostSrc, hostA, hostB), axis=1), out.get())\n\n\thostA = np.random.randn(4, 4, 5, 4).astype(dtype)\n\n\tout = bnd.concatenate((bnd.GPUArray.toGpu(hostA), src), axis=2)\n\tassert np.allclose(np.concatenate((hostA, hostSrc), axis=2), out.get())\n\n\thostA = np.random.randn(4, 4, 4, 5).astype(dtype)\n\n\tout = bnd.concatenate((bnd.GPUArray.toGpu(hostA), src), axis=3)\n\tassert np.allclose(np.concatenate((hostA, hostSrc), axis=3), out.get())\n\n\touts = bnd.split(src, (2, 2), axis=0)\n\tassert all(np.allclose(hostSrc[2 * i:2 * (i + 1)], out.get()) for i, out in enumerate(outs))\n\n\touts = bnd.split(src, (2, 2), axis=1)\n\tassert all(np.allclose(hostSrc[:, 2 * i:2 * (i + 1), :, :], out.get()) for i, out in enumerate(outs))\n\n\touts = bnd.split(src, (2, 2), axis=2)\n\tassert all(np.allclose(hostSrc[:, :, 2 * i:2 * (i + 1), :], out.get()) for i, out in enumerate(outs))\n\n\touts = bnd.split(src, (2, 2), axis=3)\n\tassert all(np.allclose(hostSrc[:, :, :, 2 * i:2 * (i + 1)], out.get()) for i, out in enumerate(outs))\n\n\tassert np.allclose(np.tile(hostB, (1, 3, 1, 1)), bnd.tile(b, 3, axis=1).get())\n\n\ndef randomTest(bnd):\n\tdata = bnd.GPUArray.empty((100, ), dtype=np.float32)\n\n\tbnd.fillUniform(data, minval=-1.0, maxval=1.0)\n\tbnd.fillNormal(data, mean=1.0, stddev=0.1)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Datasets/Cifar10Loader.py,4,"b'import os, tarfile, pickle\n\nimport numpy as np\nimport h5py\n\nfrom PuzzleLib.Datasets.DataLoader import DataLoader\n\n\nclass Cifar10Loader(DataLoader):\n\tdef __init__(self, onSample=None, onSampleBatch=None, cachename=""cifar10.hdf""):\n\t\tsuper().__init__((""data"", ""labels""), cachename)\n\n\t\tif onSample:\n\t\t\tself.onSample = onSample\n\t\telse:\n\t\t\tself.onSample = lambda smp: smp.reshape(3, 32, 32).astype(np.float32) * 2.0 / 255.0 - 1.0\n\n\t\tif onSampleBatch:\n\t\t\tself.onSampleBatch = onSampleBatch\n\t\telse:\n\t\t\tself.onSampleBatch = lambda smp, b: smp.reshape(b, 3, 32, 32).astype(np.float32) * 2.0 / 255.0 - 1.0\n\n\t\tself.datafiles = [""cifar-10-python.tar.gz"", ""cifar-10-python.tar""]\n\n\n\tdef load(self, path, compress=""gzip"", log=True):\n\t\tself.cachename = os.path.join(path, self.cachename)\n\n\t\tfilename = None\n\t\tfor datafile in self.datafiles:\n\t\t\tif tarfile.is_tarfile(os.path.join(path, datafile)):\n\t\t\t\tfilename = os.path.join(path, datafile)\n\t\t\t\tbreak\n\n\t\tif filename is None:\n\t\t\traise ValueError(""No proper datafile found in path %s (searched for %s)"" % (path, self.datafiles))\n\n\t\tif not os.path.exists(self.cachename):\n\t\t\tdicts = []\n\n\t\t\twith tarfile.open(filename) as tar:\n\t\t\t\tfor name in tar.getnames():\n\t\t\t\t\tif ""data_batch"" in name or ""test_batch"" in name:\n\t\t\t\t\t\tf = tar.extractfile(name)\n\t\t\t\t\t\tdicts.append(pickle.load(f, encoding=""latin1""))\n\n\t\t\t\t\t\tif log:\n\t\t\t\t\t\t\tprint(""[%s] Unpacked %s"" % (self.__class__.__name__, name))\n\n\t\t\ttotallen = 0\n\t\t\tfor d in dicts:\n\t\t\t\ttotallen += len(d[""labels""])\n\n\t\t\timages = np.empty((totallen, 3, 32, 32), dtype=np.float32)\n\t\t\tlabels = np.empty((totallen, ), dtype=np.int32)\n\n\t\t\tif log:\n\t\t\t\tprint(""[%s] Started merging ..."" % self.__class__.__name__)\n\n\t\t\tidx = 0\n\t\t\tfor i, d in enumerate(dicts):\n\t\t\t\tdata = d[""data""]\n\t\t\t\tlbls = d[""labels""]\n\n\t\t\t\timages[idx:idx + data.shape[0]] = self.onSampleBatch(data, data.shape[0])\n\t\t\t\tlabels[idx:idx + len(lbls)] = lbls\n\t\t\t\tidx += data.shape[0]\n\n\t\t\t\tif log:\n\t\t\t\t\tprint(""[%s] Merged #%d batch out of %d"" % (self.__class__.__name__, i + 1, len(dicts)))\n\n\t\t\tif log:\n\t\t\t\tprint(""[%s] Writing in cache ..."" % self.__class__.__name__)\n\n\t\t\twith h5py.File(self.cachename, ""w"") as hdf:\n\t\t\t\tdsetname, lblsetname = self.datanames\n\t\t\t\thdf.create_dataset(dsetname, data=images, compression=compress)\n\t\t\t\thdf.create_dataset(lblsetname, data=labels, compression=compress)\n\n\t\thdf = h5py.File(self.cachename, ""r"")\n\t\tdsetname, lblsetname = self.datanames\n\t\treturn hdf[dsetname], hdf[lblsetname]\n\n\ndef unittest():\n\tcifar10 = Cifar10Loader()\n\tcifar10.load(path=""../TestData/"")\n\tcifar10.clear()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Datasets/DataLoader.py,0,"b'import os\n\n\nclass DataLoader:\n\tdef __init__(self, datanames=None, cachename=None):\n\t\tself.cachename = cachename\n\n\t\tif datanames is None:\n\t\t\tself.datanames = [""data""]\n\t\telse:\n\t\t\tif isinstance(datanames, list) or isinstance(datanames, tuple):\n\t\t\t\tself.datanames = datanames\n\t\t\telse:\n\t\t\t\tself.datanames = [datanames]\n\n\n\tdef clear(self):\n\t\tif os.path.exists(self.cachename):\n\t\t\tos.remove(self.cachename)\n'"
Datasets/IMDBLoader.py,8,"b'import os, json\n\nimport numpy as np\nimport h5py\n\nfrom PuzzleLib.Datasets.DataLoader import DataLoader\n\n\nclass IMDBLoader(DataLoader):\n\tdef __init__(self, numwords=None, skiptop=0, maxlen=None, padchar=0, startchar=1, oovchar=2, indexFrom=3):\n\t\tsuper().__init__((""data"", ""labels"", ""vocabulary""), ""imdb.hdf"")\n\n\t\tself.numwords = numwords\n\t\tself.skiptop = skiptop\n\n\t\tself.maxlen = maxlen\n\n\t\tself.padchar = padchar\n\t\tself.startchar = startchar\n\t\tself.oovchar = oovchar\n\t\tself.indexFrom = indexFrom\n\n\t\tself.datafile = ""imdb.npz""\n\t\tself.indexfile = ""imdb_word_index.json""\n\n\n\tdef checkCacheParams(self, log=True):\n\t\tif os.path.exists(self.cachename):\n\t\t\twith h5py.File(self.cachename, ""r"") as hdf:\n\t\t\t\tparams = json.loads(str(np.array(hdf[""params""])))\n\n\t\t\t\tfor paramName in [""numwords"", ""skiptop"", ""maxlen"", ""padchar"", ""startchar"", ""oovchar"", ""indexFrom""]:\n\t\t\t\t\tif params[paramName] != getattr(self, paramName):\n\t\t\t\t\t\tif log:\n\t\t\t\t\t\t\tprint(""[%s] Existing cache has different param \'%s\', clearing ..."" %\n\t\t\t\t\t\t\t\t  (self.__class__.__name__, paramName))\n\n\t\t\t\t\t\treturn False\n\n\t\treturn True\n\n\n\tdef loadVocabulary(self, path):\n\t\twith open(os.path.join(path, self.indexfile)) as f:\n\t\t\td = json.load(f)\n\n\t\tdt = h5py.special_dtype(vlen=str)\n\t\tvocab = np.empty(shape=(self.numwords, ), dtype=dt)\n\n\t\tfor word, idx in d.items():\n\t\t\tif idx < self.numwords:\n\t\t\t\tvocab[int(idx)] = word\n\n\t\treturn vocab\n\n\n\tdef load(self, path, compress=""gzip"", log=True):\n\t\tself.cachename = os.path.join(path, self.cachename)\n\n\t\tif not self.checkCacheParams():\n\t\t\tself.clear()\n\n\t\tif not os.path.exists(self.cachename):\n\t\t\tif log:\n\t\t\t\tprint(""[%s] Started unpacking ..."" % self.__class__.__name__)\n\n\t\t\twith np.load(os.path.join(path, self.datafile), allow_pickle=True) as f:\n\t\t\t\ttraindata, testdata = f[""x_train""], f[""x_test""]\n\t\t\t\ttrainlabels, testlabels = f[""y_train""], f[""y_test""]\n\n\t\t\ttrainperm = np.random.permutation(traindata.shape[0])\n\t\t\ttestperm = np.random.permutation(testdata.shape[0])\n\n\t\t\ttraindata, trainlabels = traindata[trainperm], trainlabels[trainperm]\n\t\t\ttestdata, testlabels = testdata[testperm], testlabels[testperm]\n\n\t\t\tdata, labels = np.concatenate([traindata, testdata]), np.concatenate([trainlabels, testlabels])\n\n\t\t\tif self.startchar is not None:\n\t\t\t\tdata = [[self.startchar] + [w + self.indexFrom for w in sample] for sample in data]\n\t\t\telif self.indexFrom:\n\t\t\t\tdata = [[w + self.indexFrom for w in sample] for sample in data]\n\n\t\t\tif self.numwords is None:\n\t\t\t\tself.numwords = max([max(sample) for sample in data])\n\n\t\t\tif log:\n\t\t\t\tprint(""[%s] Started truncating vocabulary (%s max) ..."" % (self.__class__.__name__, self.numwords))\n\n\t\t\tif self.oovchar is not None:\n\t\t\t\tdata = [[self.oovchar if (w >= self.numwords or w < self.skiptop) else w for w in sample]\n\t\t\t\t\t\tfor sample in data]\n\t\t\telse:\n\t\t\t\ttruncdata = []\n\t\t\t\tfor sample in data:\n\t\t\t\t\ttruncsample = []\n\t\t\t\t\tfor w in sample:\n\t\t\t\t\t\tif self.skiptop <= w < self.numwords:\n\t\t\t\t\t\t\ttruncsample.append(w)\n\n\t\t\t\t\ttruncsample = [self.padchar] * (len(sample) - len(truncsample)) + truncsample\n\t\t\t\t\ttruncdata.append(np.array(truncsample, dtype=np.int32))\n\n\t\t\t\tdata = truncdata\n\n\t\t\tif log:\n\t\t\t\tprint(""[%s] Started adjusting samples length (%s max) ..."" % (self.__class__.__name__, self.maxlen))\n\n\t\t\tif self.maxlen is None:\n\t\t\t\tself.maxlen = max([len(sample) for sample in data])\n\n\t\t\tadjdata = []\n\t\t\tfor sample, label in zip(data, labels):\n\t\t\t\tif len(sample) < self.maxlen:\n\t\t\t\t\tadjdata.append([self.padchar] * (self.maxlen - len(sample)) + sample)\n\t\t\t\telse:\n\t\t\t\t\tadjdata.append(sample[-self.maxlen:])\n\n\t\t\tdata = adjdata\n\n\t\t\tprint(""[%s] Building cache ..."" % self.__class__.__name__)\n\n\t\t\tvocab = self.loadVocabulary(path)\n\t\t\tdata, labels = np.array(data, dtype=np.int32), np.array(labels, dtype=np.int32)\n\n\t\t\twith h5py.File(self.cachename, ""w"") as hdf:\n\t\t\t\tdsetname, lblsetname, vocsetname = self.datanames\n\t\t\t\thdf.create_dataset(dsetname, data=data, compression=compress)\n\t\t\t\thdf.create_dataset(lblsetname, data=labels, compression=compress)\n\t\t\t\thdf.create_dataset(vocsetname, data=vocab, compression=compress)\n\n\t\t\t\tparams = json.dumps({""numwords"": self.numwords, ""skiptop"": self.skiptop, ""maxlen"": self.maxlen,\n\t\t\t\t\t\t\t\t\t ""padchar"": self.padchar, ""startchar"": self.startchar, ""oovchar"": self.oovchar,\n\t\t\t\t\t\t\t\t\t ""indexFrom"": self.indexFrom})\n\n\t\t\t\tdt = h5py.special_dtype(vlen=str)\n\t\t\t\thdf.create_dataset(""params"", (), dtype=dt, data=params)\n\n\t\thdf = h5py.File(self.cachename, ""r"")\n\t\tdsetname, lblsetname, vocsetname = self.datanames\n\t\treturn hdf[dsetname], hdf[lblsetname], hdf[vocsetname]\n\n\ndef unittest():\n\timdb = IMDBLoader(numwords=20000, maxlen=80)\n\timdb.load(path=""../TestData/"")\n\timdb.clear()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Datasets/InputLoader.py,4,"b'import os\n\nimport h5py\nimport numpy as np\nfrom PIL import Image\n\nfrom PuzzleLib.Datasets.DataLoader import DataLoader\n\n\nclass InputLoader(DataLoader):\n\tdef __init__(self, onFile=None, exts=None, dataname=None, cachename=None, onFileList=None):\n\t\tsuper().__init__(dataname, cachename)\n\n\t\tif onFile is None:\n\t\t\tdef onFile(f):\n\t\t\t\timg = np.array(Image.open(f), dtype=np.float32) * 2.0 / 255.0 - 1.0\n\t\t\t\timg = np.rollaxis(img, 2)\n\n\t\t\t\treturn img.reshape(1, *img.shape)\n\n\t\tself.onFile = onFile\n\t\tself.onFileList = onFileList\n\n\t\tif exts is None:\n\t\t\tself.exts = ["".png"", "".jpg"", "".jpeg""]\n\t\telse:\n\t\t\tself.exts = [""."" + ext if not ext.startswith(""."") else ext for ext in exts]\n\n\t\tself.resizeFactor = 1.5\n\n\t\tself.log = True\n\n\t\tself.hdf = None\n\t\tself.compress = None\n\t\tself.dataset = None\n\n\t\tself.maxsamples = 0\n\t\tself.samples = 0\n\n\n\tdef checkNeedToLoad(self, log=True):\n\t\tif os.path.exists(self.cachename):\n\t\t\twith h5py.File(self.cachename, ""r"") as hdf:\n\t\t\t\tmtimes = hdf[""timestamps""]\n\n\t\t\t\tfor inputname, mtime in mtimes.items():\n\t\t\t\t\tif mtime[()] < os.path.getmtime(inputname.replace(""\\\\"", ""/"")):\n\t\t\t\t\t\tif log:\n\t\t\t\t\t\t\tprint(""[%s] Archive %s has newer time stamp"" % (self.__class__.__name__, inputname))\n\n\t\t\t\t\t\treturn True\n\t\telse:\n\t\t\treturn True\n\n\t\treturn False\n\n\n\tdef createDataset(self, unpacked):\n\t\tdataset = self.hdf.create_dataset(self.datanames[0], shape=unpacked.shape,\n\t\t\t\t\t\t\t\t\t\t  maxshape=(None, ) + unpacked.shape[1:], dtype=unpacked.dtype,\n\t\t\t\t\t\t\t\t\t\t  compression=self.compress)\n\n\t\tdataset[:] = unpacked\n\t\treturn dataset\n\n\n\tdef load(self, inputnames, maxsamples=None, filepacksize=5000, compress=""gzip"", log=True):\n\t\tself.log = log\n\n\t\tif isinstance(inputnames, str):\n\t\t\tinputnames = [inputnames]\n\n\t\tif self.cachename is None:\n\t\t\tself.cachename = os.path.splitext(inputnames[0])[0] + "".hdf""\n\n\t\tneedsToLoad = self.checkNeedToLoad(log)\n\t\tif needsToLoad:\n\t\t\tif log:\n\t\t\t\tprint(""[%s] Creating cache file %s ..."" % (self.__class__.__name__, self.cachename))\n\n\t\t\ttimestamps = {}\n\t\t\tfor inputname in inputnames:\n\t\t\t\ttimestamps[inputname] = os.path.getmtime(inputname)\n\n\t\t\twith h5py.File(self.cachename, ""w"") as hdf:\n\t\t\t\ttimeGrp = hdf.create_group(""timestamps"")\n\t\t\t\tfor name, attr in timestamps.items():\n\t\t\t\t\ttimeGrp.create_dataset(os.path.normpath(name).replace(""/"", ""\\\\""), data=attr)\n\n\t\t\t\tself.hdf = hdf\n\t\t\t\tself.compress = compress\n\t\t\t\tself.dataset = None\n\n\t\t\t\tself.maxsamples = maxsamples\n\t\t\t\tself.samples = 0\n\n\t\t\t\tfor i, inputname in enumerate(inputnames):\n\t\t\t\t\tif log:\n\t\t\t\t\t\tprint(""[%s] Unpacking archive %s (%d out of %d) ..."" %\n\t\t\t\t\t\t\t  (self.__class__.__name__, inputname, i + 1, len(inputnames)))\n\n\t\t\t\t\tself.unpack(inputname, filepacksize)\n\t\t\t\t\tif self.maxsamples is not None and self.samples == self.maxsamples:\n\t\t\t\t\t\tprint(""[%s] Reached max limit of samples (%d)"" % (self.__class__.__name__, self.maxsamples))\n\n\t\telse:\n\t\t\tif log:\n\t\t\t\tprint(""[%s] Using cache %s ..."" % (self.__class__.__name__, self.cachename))\n\n\t\treturn h5py.File(self.cachename, ""r"")[self.datanames[0]]\n\n\n\tdef unpack(self, inputname, filepacksize):\n\t\tself.checkInput(inputname)\n\n\t\twith self.openInput(inputname) as inp:\n\t\t\tfiles = self.getFilelist(inp)\n\n\t\t\tnumofpacks = len(files) // filepacksize\n\t\t\tresid = (len(files) % filepacksize != 0)\n\n\t\t\tpacks = []\n\t\t\tfor idx in range(numofpacks):\n\t\t\t\tpacks.append(files[idx * filepacksize:(idx + 1) * filepacksize])\n\n\t\t\tif resid:\n\t\t\t\tpacks.append(files[numofpacks * filepacksize:])\n\n\t\t\tfor idx, pack in enumerate(packs):\n\t\t\t\tif self.log:\n\t\t\t\t\tprint(""[%s] Started unpacking pack %d out of %d ..."" %\n\t\t\t\t\t\t  (self.__class__.__name__, idx + 1, len(packs)))\n\n\t\t\t\tself.cacheFilepack(inp, pack)\n\t\t\t\tif self.maxsamples is not None and self.samples == self.maxsamples:\n\t\t\t\t\tbreak\n\n\n\tdef cacheFilepack(self, inp, pack):\n\t\tdata = None\n\t\tnsamples = 0\n\n\t\tfor i, file in enumerate(pack):\n\t\t\ttry:\n\t\t\t\tif self.log:\n\t\t\t\t\tprint(""[%s] Unpacking file %s (%d out of %d)"" % (self.__class__.__name__, file, i + 1, len(pack)))\n\n\t\t\t\tbatch = self.onFile(self.openFile(inp, file))\n\t\t\texcept Exception as e:\n\t\t\t\traise RuntimeError(""Unpacking failure: %s"" % e)\n\n\t\t\tif data is None:\n\t\t\t\tdata = np.empty((len(pack)-1 + batch.shape[0], ) + batch.shape[1:], dtype=batch.dtype)\n\n\t\t\tif nsamples + batch.shape[0] > data.shape[0]:\n\t\t\t\tnewShape = (int(self.resizeFactor * (data.shape[0] + batch.shape[0])), ) + data.shape[1:]\n\t\t\t\tnewData = np.empty(newShape, dtype=batch.dtype)\n\n\t\t\t\tnewData[:data.shape[0]] = data\n\t\t\t\tdata = newData\n\n\t\t\tdata[nsamples:nsamples + batch.shape[0]] = batch\n\t\t\tnsamples += batch.shape[0]\n\n\t\t\tif self.maxsamples is not None and self.samples + nsamples >= self.maxsamples - 1:\n\t\t\t\tdata = data[:self.maxsamples - self.samples]\n\t\t\t\tnsamples = self.maxsamples - self.samples\n\t\t\t\tbreak\n\n\t\tdata = data[:nsamples]\n\n\t\tif self.log:\n\t\t\tsize = data.nbytes / 1024**2\n\t\t\tprint(""[%s] Saving unpacked data ... (shape=%s, size=%s mbytes)"" %\n\t\t\t\t  (self.__class__.__name__, data.shape, size))\n\n\t\tif self.dataset is None:\n\t\t\tself.dataset = self.createDataset(data)\n\n\t\telse:\n\t\t\tif self.samples + nsamples > self.dataset.shape[0]:\n\t\t\t\tself.dataset.resize((self.samples + nsamples, ) + self.dataset.shape[1:])\n\t\t\t\tself.dataset[self.samples:] = data\n\n\t\t\telse:\n\t\t\t\tself.dataset[self.samples:self.samples + nsamples] = data\n\n\t\tself.samples += nsamples\n\t\tprint(""[%s] Samples ready: %d (max: %s)"" % (self.__class__.__name__, self.samples, self.maxsamples))\n\n\n\tdef checkInput(self, inputname):\n\t\traise NotImplementedError()\n\n\n\tdef openInput(self, inputname):\n\t\traise NotImplementedError()\n\n\n\tdef getFilelist(self, inp):\n\t\tlst = self.loadFilelist(inp)\n\t\tif self.onFileList is not None:\n\t\t\tlst = self.onFileList(lst)\n\n\t\treturn lst\n\n\n\tdef loadFilelist(self, inp):\n\t\traise NotImplementedError()\n\n\n\tdef openFile(self, inp, file):\n\t\traise NotImplementedError()\n'"
Datasets/MnistLoader.py,3,"b'import os, struct, array\n\nimport numpy as np\nimport h5py\n\nfrom PuzzleLib.Datasets.DataLoader import DataLoader\n\n\nclass MnistLoader(DataLoader):\n\tdef __init__(self, onSample=None, cachename=""mnist.hdf""):\n\t\tsuper().__init__((""data"", ""labels""), cachename)\n\n\t\tif onSample:\n\t\t\tself.onSample = onSample\n\t\telse:\n\t\t\tself.onSample = lambda smp: np.array(smp, dtype=np.float32).reshape((1, 28, 28)) / 255.0\n\n\t\tself.testdata = ""t10k-images.idx3-ubyte""\n\t\tself.testlabels = ""t10k-labels.idx1-ubyte""\n\n\t\tself.traindata = ""train-images.idx3-ubyte""\n\t\tself.trainlabels = ""train-labels.idx1-ubyte""\n\n\n\tdef load(self, path, compress=""gzip"", log=True):\n\t\tself.cachename = os.path.join(path, self.cachename)\n\n\t\tif not os.path.exists(self.cachename):\n\t\t\timgs, lbls = [], []\n\n\t\t\tif log:\n\t\t\t\tprint(""[%s] Started unpacking ..."" % self.__class__.__name__)\n\n\t\t\tfor filename in [self.testlabels, self.trainlabels]:\n\t\t\t\twith open(os.path.join(path, filename), ""rb"") as file:\n\t\t\t\t\tmagic, size = struct.unpack("">II"", file.read(8))\n\n\t\t\t\t\ttrueMagic = 2049\n\t\t\t\t\tif magic != trueMagic:\n\t\t\t\t\t\traise ValueError(""Bad magic number (got %s, expected %s)"" % (magic, trueMagic))\n\n\t\t\t\t\tlbls += array.array(""B"", file.read())\n\n\t\t\tfor filename in [self.testdata, self.traindata]:\n\t\t\t\twith open(os.path.join(path, filename), ""rb"") as file:\n\t\t\t\t\tmagic, size, rows, cols = struct.unpack("">IIII"", file.read(16))\n\n\t\t\t\t\ttrueMagic = 2051\n\t\t\t\t\tif magic != trueMagic:\n\t\t\t\t\t\traise ValueError(""Bad magic number (got %s, expected %s)"" % (magic, trueMagic))\n\n\t\t\t\t\tdata = array.array(""B"", file.read())\n\t\t\t\t\tdatsize = rows * cols\n\n\t\t\t\t\tfor i in range(size):\n\t\t\t\t\t\tdat = data[i * datsize:(i+1) * datsize]\n\t\t\t\t\t\timgs.append(dat)\n\n\t\t\timages = np.empty((len(imgs), 1, rows, cols), dtype=np.float32)\n\t\t\tlabels = np.empty((len(imgs), ), dtype=np.int32)\n\n\t\t\tprint(""[%s] Building cache ..."" % self.__class__.__name__)\n\n\t\t\tfor i in range(len(lbls)):\n\t\t\t\timages[i] = self.onSample(imgs[i])\n\t\t\t\tlabels[i] = lbls[i]\n\n\t\t\twith h5py.File(self.cachename, ""w"") as hdf:\n\t\t\t\tdsetname, lblsetname = self.datanames\n\t\t\t\thdf.create_dataset(dsetname, data=images, compression=compress)\n\t\t\t\thdf.create_dataset(lblsetname, data=labels, compression=compress)\n\n\t\thdf = h5py.File(self.cachename, ""r"")\n\t\tdsetname, lblsetname = self.datanames\n\t\treturn hdf[dsetname], hdf[lblsetname]\n\n\ndef unittest():\n\tmnist = MnistLoader()\n\tmnist.load(path=""../TestData/"")\n\tmnist.clear()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Datasets/PathLoader.py,0,"b'import os, shutil, zipfile\n\nfrom PuzzleLib.Datasets.InputLoader import InputLoader\n\n\nclass PathLoader(InputLoader):\n\tdef __init__(self, onFile=None, exts=None, dataname=None, cachename=None, onFileList=None, doOpen=True):\n\t\tsuper().__init__(onFile, exts, dataname, cachename, onFileList)\n\t\tself.doOpen = doOpen\n\n\n\tclass Path:\n\t\tdef __init__(self, path):\n\t\t\tself.path = path\n\n\n\t\tdef __enter__(self):\n\t\t\treturn self\n\n\n\t\tdef __exit__(self, exc_type, exc_val, exc_tb):\n\t\t\tpass\n\n\n\tdef checkInput(self, path):\n\t\tif not os.path.exists(path):\n\t\t\traise RuntimeError(""Path \'%s\' does not exist"" % path)\n\n\n\tdef openInput(self, path):\n\t\treturn self.Path(path)\n\n\n\tdef loadFilelist(self, path):\n\t\tlst = []\n\n\t\tfor dirpath, dirnames, filenames in os.walk(path.path):\n\t\t\tlst.extend([file for file in filenames if any([file.lower().endswith(ext) for ext in self.exts])])\n\n\t\treturn lst\n\n\n\tdef openFile(self, path, file):\n\t\tfullname = os.path.join(path.path, file)\n\t\treturn open(fullname, mode=""rb"") if self.doOpen else fullname\n\n\ndef unittest():\n\tzipname = ""../TestData/test.zip""\n\tpath = os.path.splitext(zipname)[0]\n\n\tzipfile.ZipFile(zipname).extractall(path)\n\n\tloader = PathLoader()\n\tloader.load(path)\n\tloader.clear()\n\n\tshutil.rmtree(path)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Datasets/SmallNorbLoader.py,12,"b'import os, struct\n\nimport numpy as np\nimport h5py\nfrom PIL import Image\n\nfrom PuzzleLib.Datasets.DataLoader import DataLoader\n\n\nclass SmallNorbLoader(DataLoader):\n\tdef __init__(self, onSample=None, sampleInfo=None, cachename=None):\n\t\tsuper().__init__((""data"", ""labels"", ""info""), ""smallnorb.hdf"" if cachename is None else cachename)\n\n\t\tself.sampleInfo = lambda: (np.float32, (28, 28)) if sampleInfo is None else sampleInfo\n\t\tself.onSample = lambda sample: np.array(Image.fromarray(sample).resize((28, 28))) \\\n\t\t\tif onSample is None else onSample\n\n\t\tself.testdata = ""smallnorb-5x01235x9x18x6x2x96x96-testing-dat.mat""\n\t\tself.testlabels = ""smallnorb-5x01235x9x18x6x2x96x96-testing-cat.mat""\n\t\tself.testinfo = ""smallnorb-5x01235x9x18x6x2x96x96-testing-info.mat""\n\n\t\tself.traindata = ""smallnorb-5x46789x9x18x6x2x96x96-training-dat.mat""\n\t\tself.trainlabels = ""smallnorb-5x46789x9x18x6x2x96x96-training-cat.mat""\n\t\tself.traininfo = ""smallnorb-5x46789x9x18x6x2x96x96-training-info.mat""\n\n\t\tself.nlabels = 5\n\t\tself.ninstances = 10\n\t\tself.nelevs = 9\n\t\tself.nazimuths = 18\n\t\tself.nlights = 6\n\n\n\tdef load(self, path, sort=False, compress=""gzip"", log=True, onlyTest=False):\n\t\tself.cachename = os.path.join(path, self.cachename)\n\n\t\tif not os.path.exists(self.cachename):\n\t\t\tif log:\n\t\t\t\tprint(""[%s] Started unpacking ..."" % self.__class__.__name__)\n\n\t\t\tdata, labels, info = None, None, None\n\t\t\tfiles = [self.testdata] if onlyTest else [self.traindata, self.testdata]\n\n\t\t\tfor filename in files:\n\t\t\t\twith open(os.path.join(path, filename), ""rb"") as file:\n\t\t\t\t\tmagic, ndim = struct.unpack(""<ii"", file.read(8))\n\t\t\t\t\tdims = struct.unpack(""<"" + ""i"" * max(ndim, 3), file.read(max(ndim, 3) * 4))\n\n\t\t\t\t\ttrueMagic = 0x1E3D4C55\n\t\t\t\t\tif magic != trueMagic:\n\t\t\t\t\t\traise ValueError(""Bad magic number (got 0x%x, expected 0x%x)"" % (magic, trueMagic))\n\n\t\t\t\t\tindata = np.fromfile(file, dtype=np.uint8).reshape(*dims)\n\n\t\t\t\t\tdtype, reqdims = self.sampleInfo()\n\t\t\t\t\toutdata = np.empty(dims[:2] + reqdims, dtype=dtype)\n\n\t\t\t\t\tfor i in range(dims[0]):\n\t\t\t\t\t\tfor j in range(dims[1]):\n\t\t\t\t\t\t\toutdata[i, j] = self.onSample(indata[i, j])\n\n\t\t\t\t\t\tif (i + 1) % 100 == 0 and log:\n\t\t\t\t\t\t\tprint(""[%s] Unpacked %s pairs out of %s"" % (self.__class__.__name__, i + 1, dims[0]))\n\n\t\t\t\t\tdata = outdata if data is None else np.vstack((data, outdata))\n\n\t\t\tfor filename in [self.trainlabels, self.testlabels]:\n\t\t\t\twith open(os.path.join(path, filename), ""rb"") as file:\n\t\t\t\t\tmagic, ndim = struct.unpack(""<ii"", file.read(8))\n\t\t\t\t\tstruct.unpack(""<"" + ""i"" * max(ndim, 3), file.read(max(ndim, 3) * 4))\n\n\t\t\t\t\ttrueMagic = 0x1E3D4C54\n\t\t\t\t\tif magic != trueMagic:\n\t\t\t\t\t\traise ValueError(""Bad magic number (got 0x%x, expected 0x%x)"" % (magic, trueMagic))\n\n\t\t\t\t\tinlabels = np.fromfile(file, dtype=np.uint32)\n\t\t\t\t\tlabels = inlabels if labels is None else np.concatenate((labels, inlabels))\n\n\t\t\tfor filename in [self.traininfo, self.testinfo]:\n\t\t\t\twith open(os.path.join(path, filename), ""rb"") as file:\n\t\t\t\t\tmagic, ndim = struct.unpack(""<ii"", file.read(8))\n\t\t\t\t\tdims = struct.unpack(""<"" + ""i"" * max(ndim, 3), file.read(max(ndim, 3) * 4))\n\n\t\t\t\t\ttrueMagic = 0x1E3D4C54\n\t\t\t\t\tif magic != trueMagic:\n\t\t\t\t\t\traise ValueError(""Bad magic number (got 0x%x, expected 0x%x)"" % (magic, trueMagic))\n\n\t\t\t\t\tininfo = np.fromfile(file, dtype=np.uint32).reshape(dims[:2])\n\t\t\t\t\tinfo = ininfo if info is None else np.vstack((info, ininfo))\n\n\t\t\tif sort:\n\t\t\t\tdata, labels, info = self.sortDataset(data, labels, info, log=log)\n\n\t\t\tprint(""[%s] Building cache ..."" % self.__class__.__name__)\n\n\t\t\twith h5py.File(self.cachename, ""w"") as hdf:\n\t\t\t\tdsetname, lblsetname, infosetname = self.datanames\n\t\t\t\thdf.create_dataset(dsetname, data=data, compression=compress)\n\t\t\t\thdf.create_dataset(lblsetname, data=labels, compression=compress)\n\t\t\t\thdf.create_dataset(infosetname, data=info, compression=compress)\n\n\t\thdf = h5py.File(self.cachename, ""r"")\n\t\tdsetname, lblsetname, infosetname = self.datanames\n\t\treturn hdf[dsetname], hdf[lblsetname], hdf[infosetname]\n\n\n\tdef sortDataset(self, data, labels, info, log=True):\n\t\tshape = (self.nlabels, self.ninstances, self.nlights, self.nelevs, self.nazimuths)\n\n\t\tsortdata = np.empty(shape + data.shape[1:], dtype=np.float32)\n\t\tsortlabels = np.empty(shape, dtype=np.uint32)\n\t\tsortinfo = np.empty(shape + info.shape[1:], dtype=np.uint32)\n\n\t\tif log:\n\t\t\tprint(""[%s] Started sorting dataset ..."" % self.__class__.__name__)\n\n\t\tfor i in range(data.shape[0]):\n\t\t\tinstance, elev, azimuth, light = info[i]\n\t\t\tlabel = labels[i]\n\n\t\t\tsortdata[label, instance, light, elev, azimuth // 2] = data[i]\n\t\t\tsortlabels[labels, instance, light, elev, azimuth // 2] = label\n\t\t\tsortinfo[labels, instance, light, elev, azimuth // 2] = info[i]\n\n\t\t\tif log and (i + 1) % 100 == 0:\n\t\t\t\tprint(""[%s] Sorted %s pairs out of %s"" % (self.__class__.__name__, i + 1, data.shape[0]))\n\n\t\treturn sortdata, sortlabels, sortinfo\n\n\ndef unittest():\n\tsmallnorb = SmallNorbLoader()\n\tsmallnorb.load(path=""../TestData/"", sort=True, onlyTest=True)\n\tsmallnorb.clear()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Datasets/TarLoader.py,0,"b'import tarfile\n\nfrom PuzzleLib.Datasets.InputLoader import InputLoader\n\n\nclass TarLoader(InputLoader):\n\tdef checkInput(self, archivename):\n\t\tif not tarfile.is_tarfile(archivename):\n\t\t\traise RuntimeError(""\'%s\' is not tar file"" % archivename)\n\n\n\tdef openInput(self, archivename):\n\t\treturn tarfile.open(archivename)\n\n\n\tdef loadFilelist(self, archive):\n\t\treturn [file for file in archive.getnames() if any([file.lower().endswith(ext) for ext in self.exts])]\n\n\n\tdef openFile(self, archive, file):\n\t\treturn archive.extractfile(file)\n\n\ndef unittest():\n\tloader = TarLoader()\n\tloader.load(""../TestData/test.tar"", maxsamples=5, filepacksize=3)\n\tloader.clear()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Datasets/Utils.py,16,"b'import random\n\nimport numpy as np\n\nfrom PuzzleLib.Handlers.Calculator import Calculator\nfrom PuzzleLib import Statistics\n\n\ndef validate(net, valData, valLabels, dim=0, batchsize=128, log=False):\n\tif dim == 0:\n\t\tdim = getDim(valLabels)\n\n\tconfMat = np.zeros(shape=(dim, dim))\n\tpredictions = Calculator(net, batchsize=batchsize).calcFromHost(valData)\n\n\tfor i in range(predictions.shape[0]):\n\t\tconfMat[valLabels[i], np.argmax(predictions[i])] += 1\n\n\tif log:\n\t\tprint(""Confusion matrix:\\n"" + str(confMat))\n\n\tprecision, _ = Statistics.precision(confMat, log=log)\n\trecall, _ = Statistics.recall(confMat, log=log)\n\taccuracy = Statistics.accuracy(confMat, log=log)\n\n\treturn precision, recall, accuracy\n\n\ndef splitData(data, labels=None, dim=0, validation=0.1, permutation=True, uniformVal=True):\n\tif len(data) == 0:\n\t\treturn None\n\n\tif permutation:\n\t\tdata, labels = permutateData(data, labels)\n\n\tif labels is None:\n\t\tsplitter = int(validation * len(data))\n\t\treturn data[splitter:], data[:splitter]\n\n\tif dim < 1:\n\t\tdim = getDim(labels)\n\n\tcounter = [0] * dim\n\tcoe = [0] * dim\n\tfor label in labels:\n\t\tcoe[label] += 1\n\n\tif uniformVal:\n\t\tsize = int(validation * min(coe))\n\t\tfor i in range(len(coe)):\n\t\t\tcoe[i] = size\n\t\tvalSize = dim * size\n\telse:\n\t\tfor i in range(len(coe)):\n\t\t\tcoe[i] = int(coe[i] * validation)\n\t\tvalSize = sum(coe)\n\n\ttrainSize = len(data) - valSize\n\n\tvalLabels = np.empty((valSize, ), labels.dtype) if isinstance(labels, np.ndarray) else [labels[0]] * valSize\n\tvalData = np.empty((valSize, ) + data.shape[1:], data.dtype) if isinstance(data, np.ndarray) else \\\n\t\t[data[0]] * valSize\n\n\ttrainLabels = np.empty((trainSize, ), labels.dtype) if isinstance(labels, np.ndarray) else [labels[0]] * trainSize\n\ttrainData = np.empty((trainSize, ) + data.shape[1:], data.dtype) if isinstance(data, np.ndarray) else \\\n\t\t[data[0]] * trainSize\n\n\tvalIdx, trainIdx = 0, 0\n\tfor i in range(len(data)):\n\t\tif counter[labels[i]] < coe[labels[i]]:\n\t\t\tvalData[valIdx] = data[i]\n\t\t\tvalLabels[valIdx] = labels[i]\n\n\t\t\tvalIdx += 1\n\t\t\tcounter[labels[i]] += 1\n\t\telse:\n\t\t\ttrainData[trainIdx] = data[i]\n\t\t\ttrainLabels[trainIdx] = labels[i]\n\n\t\t\ttrainIdx += 1\n\n\treturn trainData, valData, trainLabels, valLabels\n\n\ndef replicateData(data, labels, dim=0, permutation=True):\n\tcheckShape(data, labels)\n\n\tif dim < 1:\n\t\tdim = getDim(labels)\n\n\tcoe = [0] * dim\n\tfor label in labels:\n\t\tcoe[label] += 1\n\n\ttop = max(coe)\n\tfor i in range(dim):\n\t\tif coe[i] > 0:\n\t\t\tcoe[i] = top / coe[i]\n\n\tcur = [0] * dim\n\tres = [0] * dim\n\n\tlength = dim * top\n\n\tnewData = np.empty((length, ) + data.shape[1:], data.dtype) if isinstance(data, np.ndarray) else [data[0]] * length\n\tnewLabels = np.empty((length, ), labels.dtype) if isinstance(labels, np.ndarray) else [labels[0]] * length\n\n\tidx = 0\n\tfor i in range(len(data)):\n\t\tcur[labels[i]] += coe[labels[i]]\n\n\t\twhile res[labels[i]] < cur[labels[i]] - 0.1:\n\t\t\tnewData[idx] = data[i]\n\t\t\tnewLabels[idx] = labels[i]\n\t\t\tidx += 1\n\t\t\tres[labels[i]] += 1\n\n\tif permutation:\n\t\tnewData, newLabels = permutateData(newData, newLabels)\n\n\treturn newData, newLabels\n\n\ndef permutateData(data, labels=None, constantMemory=False):\n\tperm = np.random.permutation(len(data))\n\n\tif not constantMemory:\n\t\tif labels is not None:\n\t\t\ttmp = labels.copy()\n\t\t\tfor i in range(checkShape(data, labels)):\n\t\t\t\tlabels[i] = tmp[perm[i]]\n\n\t\ttmp = data.copy()\n\t\tfor i in range(len(data)):\n\t\t\tdata[i] = tmp[perm[i]]\n\n\telse:\n\t\twhile True:\n\t\t\tidx = 0\n\t\t\tflag = False\n\n\t\t\tfor idx in range(len(perm)):\n\t\t\t\tif perm[idx] >= 0:\n\t\t\t\t\tflag = True\n\t\t\t\t\tbreak\n\n\t\t\tif not flag:\n\t\t\t\tbreak\n\n\t\t\tjdx = idx\n\t\t\twhile True:\n\t\t\t\tif perm[jdx] != idx:\n\t\t\t\t\tdata[jdx], data[perm[jdx]] = data[perm[jdx]], data[jdx]\n\n\t\t\t\t\tif labels is not None:\n\t\t\t\t\t\tlabels[jdx], labels[perm[jdx]] = labels[perm[jdx]], labels[jdx]\n\n\t\t\t\t\toldjdx = jdx\n\t\t\t\t\tjdx = perm[jdx]\n\t\t\t\t\tperm[oldjdx] = -1  # in place\n\n\t\t\t\telse:\n\t\t\t\t\tperm[jdx] = -1  # in place\n\t\t\t\t\tbreak\n\n\treturn data, labels\n\n\ndef checkShape(data, labels):\n\tassert len(data) == len(labels)\n\treturn len(data)\n\n\ndef getDim(labels, log=False):\n\tassert len(labels) > 0\n\tassert (isinstance(labels[0], np.int32) or isinstance(labels[0], int))\n\n\tdim = np.max(labels) + 1\n\n\tif log:\n\t\tcoe = [0] * dim\n\n\t\tfor label in labels:\n\t\t\tcoe[label] += 1\n\n\t\tprint(""Labels count:"")\n\t\tfor i in range(dim):\n\t\t\tprint(""%d: %d (~%d%%)"" % (i, coe[i], 100 * coe[i] // labels.shape[0]))\n\n\treturn int(dim)\n\n\ndef merge(data):\n\tres = []\n\n\tfor i, item in enumerate(data):\n\t\ttext = []\n\n\t\tfor j, sentence in enumerate(item):\n\t\t\ttext += data[i][j]\n\n\t\tres.append(text)\n\n\treturn res\n\n\ndef merge2D(data):\n\tmesh = [0] * len(data)\n\tres = []\n\tcnt = 0\n\n\tfor i, item in enumerate(data):\n\t\tres += item\n\t\tmesh[i] = {""x1"": cnt, ""x2"": cnt+len(item)}\n\t\tcnt += len(item)\n\n\treturn res, mesh\n\n\ndef split2D(data, mesh):\n\tres = []\n\n\tfor idx in mesh:\n\t\tres.append(data[(idx[""x1""]):(idx[""x2""])])\n\n\treturn res\n\n\ndef resizeDataToSize(data, dataSize):\n\tnewData = [\'\'] * (dataSize - len(data))\n\tnewData = data + newData\n\n\treturn newData\n\n\ndef unittest():\n\tmergeTest()\n\tnumpyInterfaceTest()\n\tpyInterfaceTest()\n\n\ndef mergeTest():\n\tdata = [[""sfas"", ""sdfasfasdf"", ""gdfgd""], [""dfg""], [""yry"", ""rtyher""]]\n\ta, mesh = merge2D(data)\n\tb = split2D(a, mesh)\n\n\tassert data == b\n\n\ndef numpyInterfaceTest():\n\tdata = np.random.randn(10000).astype(np.float32)\n\tlabels = np.random.randint(0, 10, size=(10000, ), dtype=np.int32)\n\n\tassert checkShape(data, labels) == len(data)\n\tassert getDim(labels) == 10\n\n\ttData, vData, tLabels, vLabels = splitData(data, labels, validation=0.1, permutation=True)\n\n\tassert isinstance(tData, np.ndarray)\n\tassert isinstance(tLabels, np.ndarray)\n\tassert checkShape(tData, tLabels) == len(tLabels)\n\tassert checkShape(vData, vLabels) == len(vLabels)\n\n\tinterfaceTest(tData, tLabels, np.ndarray)\n\n\ndef pyInterfaceTest():\n\tdata = [random.random() for _ in range(10000)]\n\tlabels = [random.randint(0, 9) for _ in range(10000)]\n\n\tassert checkShape(data, labels) == len(data)\n\tassert getDim(labels) == 10\n\n\ttData, vData, tLabels, vLabels = splitData(data, labels, validation=0.5, permutation=True, uniformVal=False)\n\n\tassert isinstance(tData, list)\n\tassert isinstance(tLabels, list)\n\tassert checkShape(tData, tLabels) == len(tLabels)\n\tassert checkShape(vData, vLabels) == len(vLabels)\n\n\tinterfaceTest(tData, tLabels, list)\n\n\ndef interfaceTest(data, labels, typ):\n\tdata, labels = replicateData(data, labels, permutation=True)\n\n\tassert isinstance(data, typ)\n\tassert isinstance(labels, typ)\n\tassert checkShape(data, labels) == len(labels)\n\n\ttData, tLabels = permutateData(data, labels)\n\n\tassert isinstance(tData, typ)\n\tassert isinstance(tLabels, typ)\n\tassert checkShape(tData, tLabels) == len(tLabels)\n\n\ttData, tLabels = permutateData(tData, tLabels, constantMemory=True)\n\n\tassert isinstance(tData, typ)\n\tassert isinstance(tLabels, typ)\n\tassert checkShape(tData, tLabels) == len(tLabels)\n\n\tres = [0] * 10\n\n\tfor i in range(len(tLabels)):\n\t\tres[tLabels[i]] += 1\n\n\tfor r in res:\n\t\tassert r == res[0]\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Datasets/ZipLoader.py,0,"b'import zipfile\n\nfrom PuzzleLib.Datasets.InputLoader import InputLoader\n\n\nclass ZipLoader(InputLoader):\n\tdef checkInput(self, archivename):\n\t\tif not zipfile.is_zipfile(archivename):\n\t\t\traise RuntimeError(""\'%s\' is not zip file"" % archivename)\n\n\n\tdef openInput(self, archivename):\n\t\treturn zipfile.ZipFile(archivename)\n\n\n\tdef loadFilelist(self, archive):\n\t\treturn [file for file in archive.namelist() if any([file.lower().endswith(ext) for ext in self.exts])]\n\n\n\tdef openFile(self, archive, file):\n\t\treturn archive.open(file)\n\n\ndef unittest():\n\tloader = ZipLoader()\n\tloader.load(""../TestData/test.zip"", maxsamples=5, filepacksize=3)\n\tloader.clear()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Datasets/__init__.py,0,b'from PuzzleLib.Datasets.Cifar10Loader import Cifar10Loader\nfrom PuzzleLib.Datasets.IMDBLoader import IMDBLoader\nfrom PuzzleLib.Datasets.MnistLoader import MnistLoader\nfrom PuzzleLib.Datasets.PathLoader import PathLoader\nfrom PuzzleLib.Datasets.SmallNorbLoader import SmallNorbLoader\nfrom PuzzleLib.Datasets.TarLoader import TarLoader\nfrom PuzzleLib.Datasets.ZipLoader import ZipLoader\n'
Handlers/Calculator.py,3,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import copy\n\nfrom PuzzleLib.Handlers.Handler import Handler\n\n\nclass Calculator(Handler):\n\tdef calcFromHost(self, data, macroBatchSize=10000, onMacroBatchFinish=None):\n\t\tstate = {""hostSize"": self.getDataSize(data)}\n\n\t\tself.module.evalMode()\n\t\tself.handleFromHost(data, state, macroBatchSize, onMacroBatchFinish, random=False)\n\t\treturn state[""hostData""]\n\n\n\tdef calc(self, data):\n\t\tstate = {""devSize"": self.getDataSize(data)}\n\n\t\tself.module.evalMode()\n\t\tself.handle(data, state, random=False)\n\t\treturn state[""devData""]\n\n\n\tdef onMacroBatchStart(self, idx, macroBatchSize, state):\n\t\tstate[""devSize""] = macroBatchSize\n\n\n\tdef onMacroBatchFinish(self, idx, macroBatchSize, state):\n\t\tif not ""hostData"" in state:\n\t\t\tdef reserveHostData(data):\n\t\t\t\treturn np.empty((state[""hostSize""], ) + data.shape[1:], dtype=data.dtype)\n\n\t\t\tstate[""hostData""] = self.parseShapeTree(state[""devData""], onData=reserveHostData)\n\n\t\tdef copyHostData(indata, outdata):\n\t\t\toutdata[idx * macroBatchSize:(idx + 1) * macroBatchSize] = indata.get()\n\n\t\tself.parseShapeTree(state[""devData""], copyHostData, state[""hostData""])\n\t\tdel state[""devData""]\n\n\n\tdef handleBatch(self, batch, idx, state):\n\t\toutBatch = self.module(batch)\n\n\t\tif not ""devData"" in state:\n\t\t\tdef reserveDevData(data):\n\t\t\t\treturn gpuarray.empty((state[""devSize""], ) + data.shape[1:], dtype=data.dtype)\n\n\t\t\tstate[""devData""] = self.parseShapeTree(outBatch, onData=reserveDevData)\n\n\t\tdef copyDevData(indata, outdata):\n\t\t\tcopy(outdata[idx * self.batchsize:(idx + 1) * self.batchsize], indata)\n\n\t\tself.parseShapeTree(outBatch, copyDevData, state[""devData""])\n\n\ndef unittest():\n\tonDeviceTest()\n\tonHostTest()\n\n\ndef onDeviceTest():\n\tfrom PuzzleLib.Containers import Sequential\n\tfrom PuzzleLib.Modules import Conv2D, MaxPool2D, Activation, relu, Flatten, Linear\n\n\tdata = gpuarray.to_gpu(np.random.randn(10000, 3, 28, 28).astype(np.float32))\n\n\tseq = Sequential()\n\tseq.append(Conv2D(3, 16, 9))\n\tseq.append(MaxPool2D())\n\tseq.append(Activation(relu))\n\n\tseq.append(Conv2D(16, 32, 5))\n\tseq.append(MaxPool2D())\n\tseq.append(Activation(relu))\n\n\tseq.append(Flatten())\n\tseq.append(Linear(3 * 3 * 32, 10))\n\n\tcalc = Calculator(seq)\n\tcalc.onBatchFinish = lambda calculator: print(""Finished batch #%d"" % calculator.currBatch)\n\tcalc.calc(data)\n\n\ndef onHostTest():\n\tfrom PuzzleLib.Containers import Sequential\n\tfrom PuzzleLib.Modules import Conv2D, MaxPool2D, Activation, relu, Flatten, Linear\n\n\tdata = np.random.randn(50000, 3, 28, 28).astype(np.float32)\n\n\tseq = Sequential()\n\tseq.append(Conv2D(3, 16, 9))\n\tseq.append(MaxPool2D())\n\tseq.append(Activation(relu))\n\n\tseq.append(Conv2D(16, 32, 5))\n\tseq.append(MaxPool2D())\n\tseq.append(Activation(relu))\n\n\tseq.append(Flatten())\n\tseq.append(Linear(3 * 3 * 32, 10))\n\n\tcalc = Calculator(seq)\n\tcalc.onBatchFinish = lambda calculator: print(""Finished batch #%d"" % calculator.currBatch)\n\tcalc.calcFromHost(data, onMacroBatchFinish=lambda calculator: print(""Finished mb #%d"" % calculator.currMacroBatch))\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Handlers/Handler.py,2,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\n\n\nclass Handler:\n\tdef __init__(self, mod, onBatchFinish=None, batchsize=128):\n\t\tself.module = mod\n\n\t\tself.batchsize = batchsize\n\t\tself.onBatchFinish = onBatchFinish\n\n\t\tself.currBatch = 0\n\t\tself.totalBatches = 0\n\n\t\tself.currMacroBatch = 0\n\t\tself.totalMacroBatches = 0\n\n\n\tdef handleFromHost(self, data, state=None, macroBatchSize=10000, onMacroBatchFinish=None, random=True):\n\t\tdatasize = self.getDataSize(data)\n\t\tself.totalMacroBatches = (datasize + macroBatchSize - 1) // macroBatchSize\n\n\t\torder = np.random.permutation(self.totalMacroBatches) if random else np.arange(self.totalMacroBatches)\n\n\t\tfor i, n in enumerate(order):\n\t\t\tmacrobatch = self.sliceData(data, n, macroBatchSize, postSlice=lambda dat: gpuarray.to_gpu(dat))\n\n\t\t\tself.currMacroBatch = i + 1\n\n\t\t\tself.onMacroBatchStart(n, macroBatchSize, state)\n\t\t\tself.handle(macrobatch, state, random=random)\n\t\t\tself.onMacroBatchFinish(n, macroBatchSize, state)\n\n\t\t\tif onMacroBatchFinish:\n\t\t\t\tonMacroBatchFinish(self)\n\n\n\tdef handle(self, data, state=None, random=True):\n\t\tdatasize = self.getDataSize(data)\n\t\tself.totalBatches = (datasize + self.batchsize - 1) // self.batchsize\n\n\t\torder = np.random.permutation(self.totalBatches) if random else np.arange(self.totalBatches)\n\n\t\tfor i, n in enumerate(order):\n\t\t\tbatch = self.sliceData(data, n, self.batchsize, postSlice=lambda dat: dat)\n\n\t\t\tself.currBatch = i + 1\n\n\t\t\tself.handleBatch(batch, n, state)\n\t\t\tself.module.reset()\n\n\t\t\tif self.onBatchFinish:\n\t\t\t\tself.onBatchFinish(self)\n\n\n\t@staticmethod\n\tdef getDataSize(data):\n\t\twhile isinstance(data, list):\n\t\t\tdata = data[0]\n\n\t\treturn data.shape[0]\n\n\n\t@classmethod\n\tdef parseShapeTree(cls, data, onData, auxdata=None):\n\t\tif isinstance(data, list):\n\t\t\toutdata = [\n\t\t\t\tcls.parseShapeTree(dat, onData, auxdata[i] if auxdata is not None else None)\n\t\t\t\tfor i, dat in enumerate(data)\n\t\t\t]\n\n\t\telse:\n\t\t\toutdata = onData(data, auxdata) if auxdata is not None else onData(data)\n\n\t\treturn outdata\n\n\n\t@classmethod\n\tdef sliceData(cls, data, idx, batchsize, postSlice):\n\t\tif isinstance(data, list):\n\t\t\tslicing = [cls.sliceData(dat, idx, batchsize, postSlice) for dat in data]\n\t\telse:\n\t\t\tslicing = postSlice(data[idx * batchsize:(idx + 1) * batchsize])\n\n\t\treturn slicing\n\n\n\tdef onMacroBatchStart(self, idx, macroBatchSize, state):\n\t\tpass\n\n\n\tdef onMacroBatchFinish(self, idx, macroBatchSize, state):\n\t\tpass\n\n\n\tdef handleBatch(self, batch, idx, state):\n\t\traise NotImplementedError()\n'"
Handlers/Trainer.py,4,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Handlers.Handler import Handler\n\n\nclass Trainer(Handler):\n\tdef __init__(self, mod, cost, optimizer, onBatchFinish=None, batchsize=128):\n\t\tsuper().__init__(mod, onBatchFinish, batchsize)\n\t\tself.cost = cost\n\t\tself.optimizer = optimizer\n\n\n\tdef trainFromHost(self, data, target, macroBatchSize=10000, onMacroBatchFinish=None, random=True):\n\t\tself.cost.resetAccumulator()\n\n\t\tself.module.trainMode()\n\t\tself.handleFromHost([data, target], None, macroBatchSize, onMacroBatchFinish, random=random)\n\n\n\tdef train(self, data, target, random=True):\n\t\tself.cost.resetAccumulator()\n\n\t\tself.module.trainMode()\n\t\tself.handle([data, target], None, random=random)\n\n\n\tdef handleBatch(self, batch, idx, state):\n\t\tdata, target = batch\n\n\t\tgrad = self.cost(self.module(data), target, queryError=False)\n\n\t\tself.optimizer.zeroGradParams()\n\t\tself.module.backward(grad, updGrad=False)\n\t\tself.optimizer.update()\n\n\ndef unittest():\n\tonDeviceTest()\n\tonHostTest()\n\n\ndef onDeviceTest():\n\tfrom PuzzleLib.Containers import Sequential\n\tfrom PuzzleLib.Modules import Conv2D, MaxPool2D, Activation, relu, Flatten, Linear\n\n\tfrom PuzzleLib.Cost.CrossEntropy import CrossEntropy\n\tfrom PuzzleLib.Optimizers.NesterovSGD import NesterovSGD\n\n\tdata = gpuarray.to_gpu(np.random.randn(10000, 3, 28, 28).astype(np.float32))\n\tdataTarget = gpuarray.to_gpu(np.random.randint(low=0, high=10, size=(10000, )).astype(np.int32))\n\n\tseq = Sequential()\n\tseq.append(Conv2D(3, 16, 9))\n\tseq.append(MaxPool2D())\n\tseq.append(Activation(relu))\n\n\tseq.append(Conv2D(16, 32, 5))\n\tseq.append(MaxPool2D())\n\tseq.append(Activation(relu))\n\n\tseq.append(Flatten())\n\tseq.append(Linear(3 * 3 * 32, 10))\n\n\tentr = CrossEntropy()\n\n\topt = NesterovSGD()\n\topt.setupOn(seq)\n\n\tdef onBatchFinish(train):\n\t\tprint(""Finished batch #%d, error=%s"" % (train.currBatch, train.cost.getError()))\n\n\ttrainer = Trainer(seq, entr, opt, onBatchFinish=onBatchFinish)\n\ttrainer.train(data, dataTarget)\n\n\ndef onHostTest():\n\tfrom PuzzleLib.Containers import Sequential\n\tfrom PuzzleLib.Modules import Conv2D, MaxPool2D, Activation, relu, Flatten, Linear\n\n\tfrom PuzzleLib.Cost.CrossEntropy import CrossEntropy\n\tfrom PuzzleLib.Optimizers.NesterovSGD import NesterovSGD\n\n\tdata = np.random.randn(50000, 3, 28, 28).astype(np.float32)\n\tdataTarget = np.random.randint(low=0, high=10, size=(50000, )).astype(np.int32)\n\n\tseq = Sequential()\n\tseq.append(Conv2D(3, 16, 9))\n\tseq.append(MaxPool2D())\n\tseq.append(Activation(relu))\n\n\tseq.append(Conv2D(16, 32, 5))\n\tseq.append(MaxPool2D())\n\tseq.append(Activation(relu))\n\n\tseq.append(Flatten())\n\tseq.append(Linear(3 * 3 * 32, 10))\n\n\tentr = CrossEntropy()\n\n\topt = NesterovSGD()\n\topt.setupOn(seq)\n\n\tdef onMacroBatchFinish(train):\n\t\tprint(""Finished mb #%d, error=%s"" % (train.currMacroBatch, train.cost.getMeanError()))\n\n\ttrainer = Trainer(seq, entr, opt)\n\ttrainer.trainFromHost(data, dataTarget, onMacroBatchFinish=onMacroBatchFinish)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Handlers/Validator.py,4,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Handlers.Handler import Handler\n\n\nclass Validator(Handler):\n\tdef __init__(self, mod, cost, onBatchFinish=None, batchsize=128):\n\t\tsuper().__init__(mod, onBatchFinish, batchsize)\n\t\tself.error = 0.0\n\t\tself.cost = cost\n\n\n\tdef validateFromHost(self, data, target, macroBatchSize=10000, onMacroBatchFinish=None):\n\t\tnstates = len(target) if isinstance(target, list) else 1\n\t\tstate = {""error"": [0.0] * nstates}\n\n\t\tself.module.evalMode()\n\t\tself.handleFromHost([data, target], state, macroBatchSize, onMacroBatchFinish, random=False)\n\n\t\terror = [error / self.getDataSize(target) for error in state[""error""]]\n\t\tself.error = error if isinstance(target, list) else error[0]\n\n\t\treturn self.error\n\n\n\tdef validate(self, data, target):\n\t\tnstates = len(target) if isinstance(target, list) else 1\n\t\tstate = {""error"": [0.0] * nstates}\n\n\t\tself.module.evalMode()\n\t\tself.handle([data, target], state, random=False)\n\n\t\terror = [error / self.getDataSize(target) for error in state[""error""]]\n\t\tself.error = error if isinstance(target, list) else error[0]\n\n\t\treturn self.error\n\n\n\tdef handleBatch(self, batch, idx, state):\n\t\tdata, target = batch\n\t\terror = state[""error""]\n\n\t\tbatchError = self.cost.validate(self.module(data), target)\n\t\tbatchError = batchError if isinstance(batchError, list) else [batchError]\n\n\t\tfor i in range(len(error)):\n\t\t\terror[i] += self.getDataSize(data) * batchError[i]\n\n\ndef unittest():\n\tonDeviceTest()\n\tonHostTest()\n\n\ndef onDeviceTest():\n\tfrom PuzzleLib.Containers import Sequential\n\tfrom PuzzleLib.Modules import Conv2D, MaxPool2D, Activation, relu, Flatten, Linear\n\n\tfrom PuzzleLib.Cost.CrossEntropy import CrossEntropy\n\n\tdata = gpuarray.to_gpu(np.random.randn(10000, 3, 28, 28).astype(np.float32))\n\tdataTarget = gpuarray.to_gpu(np.random.randint(low=0, high=10, size=(10000, )).astype(np.int32))\n\n\tseq = Sequential()\n\tseq.append(Conv2D(3, 16, 9))\n\tseq.append(MaxPool2D())\n\tseq.append(Activation(relu))\n\n\tseq.append(Conv2D(16, 32, 5))\n\tseq.append(MaxPool2D())\n\tseq.append(Activation(relu))\n\n\tseq.append(Flatten())\n\tseq.append(Linear(3 * 3 * 32, 10))\n\n\tentr = CrossEntropy()\n\n\tval = Validator(seq, entr)\n\tprint(""Validation error on small data: %s"" % val.validate(data, dataTarget))\n\n\ndef onHostTest():\n\tfrom PuzzleLib.Containers import Sequential\n\tfrom PuzzleLib.Modules import Conv2D, MaxPool2D, Activation, relu, Flatten, Linear\n\n\tfrom PuzzleLib.Cost.CrossEntropy import CrossEntropy\n\n\tdata = np.random.randn(50000, 3, 28, 28).astype(np.float32)\n\tdataTarget = np.random.randint(low=0, high=10, size=(50000, )).astype(np.int32)\n\n\tseq = Sequential()\n\tseq.append(Conv2D(3, 16, 9))\n\tseq.append(MaxPool2D())\n\tseq.append(Activation(relu))\n\n\tseq.append(Conv2D(16, 32, 5))\n\tseq.append(MaxPool2D())\n\tseq.append(Activation(relu))\n\n\tseq.append(Flatten())\n\tseq.append(Linear(3 * 3 * 32, 10))\n\n\tentr = CrossEntropy()\n\n\tval = Validator(seq, entr)\n\tval.validateFromHost(data, dataTarget)\n\tprint(""Validation error on big data: %s"" % val.error)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Handlers/__init__.py,0,b'from PuzzleLib.Handlers.Calculator import Calculator\nfrom PuzzleLib.Handlers.Trainer import Trainer\nfrom PuzzleLib.Handlers.Validator import Validator\n'
Hip/Backend.py,0,"b'from enum import Enum\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Cuda.GPUBackend import GPUBackend\nfrom PuzzleLib.Cuda.Kernels.Memory import MemoryModule\n\nfrom PuzzleLib.Hip import Driver as HipDriver\nfrom PuzzleLib.Hip.Driver import HipError, HipRand, RocBlas\n\nfrom PuzzleLib.Hip.GPUArray import HipGPUArray\nfrom PuzzleLib.Hip.SourceModule import hipWarpSize, hipBlockSize\nfrom PuzzleLib.Hip.SourceModule import HipSourceModule, HipEltwiseKernel, HipEltHalf2Kernel, HipReductionKernel\nfrom PuzzleLib.Hip.Utils import HipSharedArray\n\nfrom PuzzleLib.Hip.Kernels.CTC import HipCTCModule\nfrom PuzzleLib.Hip.Wrappers import MIOpen, MIOpenRnn\n\n\nclass HipBackend(GPUBackend):\n\tBackendName = ""Hip""\n\n\twarpSize = hipWarpSize\n\tnthreads = hipBlockSize\n\n\tDriver = HipDriver\n\tGPUArray = HipGPUArray\n\tError = HipError\n\n\tSharedArray = HipSharedArray\n\tRand, Blas, Dnn = HipRand, RocBlas, MIOpen\n\n\tSourceModule = HipSourceModule\n\tElementwiseKernel = HipEltwiseKernel\n\tElementHalf2Kernel = HipEltHalf2Kernel\n\tReductionKernel = HipReductionKernel\n\n\n\tclass GroupFormat(Enum):\n\t\tgbp = RocBlas.GROUPFORMAT_GBP\n\t\tbgp = RocBlas.GROUPFORMAT_BGP\n\n\n\tConvPerf = MIOpen.ConvPerf\n\n\tConvFwdAlgo = MIOpen.ConvFwdAlgo\n\tConvBwdDataAlgo = MIOpen.ConvBwdDataAlgo\n\tConvBwdFilterAlgo = MIOpen.ConvBwdFilterAlgo\n\n\tPoolMode = MIOpen.PoolMode\n\tSoftMaxMode = MIOpen.SoftMaxMode\n\n\tBatchNormMode = MIOpen.BatchNormMode\n\tLRNMode = MIOpen.LRNMode\n\n\tRNNAlgo, RNNMode, DirectionMode = MIOpenRnn.RNNAlgo, MIOpenRnn.RNNMode, MIOpenRnn.DirectionMode\n\n\n\tdef __init__(self, deviceIdx, initmode=0):\n\t\tself.memmod = None\n\t\tsuper().__init__(deviceIdx, initmode)\n\n\n\tdef initLibs(self):\n\t\tself.blas = self.Blas.BlasContext().enableTensorOps(True)\n\t\tif Config.systemLog:\n\t\t\tprint(\n\t\t\t\t""[%s] Created %s context (Using version: %s)"" %\n\t\t\t\t(Config.libname, self.Blas.__name__, self.blas.getVersion())\n\t\t\t)\n\n\t\tself.dnn = self.Dnn.DnnContext(self).enableTensorOps(True)\n\t\tif Config.systemLog:\n\t\t\tprint(\n\t\t\t\t""[%s] Created %s context (Using version: %s)"" %\n\t\t\t\t(Config.libname, self.Dnn.__name__, self.dnn.getVersion())\n\t\t\t)\n\n\n\tdef initKernels(self):\n\t\tsuper().initKernels()\n\n\t\tself.ctcmod = HipCTCModule(self)\n\t\tself.memmod = MemoryModule(self)\n\n\n\tdef createRnn(self, insize, hsize, dtype, layers=1, algo=None, mode=None, direction=None, dropout=0.0,\n\t\t\t\t  seed=0, batchsize=0):\n\t\tmode = self.RNNMode.lstm if mode is None else mode\n\t\tdirection = self.DirectionMode.uni if direction is None else direction\n\n\t\trnn = MIOpenRnn.Rnn(self.dnn, insize, hsize, dtype, layers, mode, direction)\n\n\t\tW = self.GPUArray.empty(rnn.descW.shape, dtype=rnn.dtype)\n\t\tparams = self.acquireRnnParams(rnn, W)\n\n\t\treturn rnn, W, params\n\n\n\tdef acquireRnnParams(self, rnn, W):\n\t\tif rnn.mode == self.RNNMode.relu or rnn.mode == self.RNNMode.tanh:\n\t\t\treturn self.acquireNativeRnnParams(rnn, W)\n\t\telif rnn.mode == self.RNNMode.lstm:\n\t\t\treturn self.acquireLSTMParams(rnn, W)\n\t\telif rnn.mode == self.RNNMode.gru:\n\t\t\treturn self.acquireGRUParams(rnn, W)\n\t\telse:\n\t\t\traise NotImplementedError(rnn.mode.value)\n\n\n\tdef getRnnParam(self, rnn, layer, W, linLayer, shape):\n\t\tlinLayerMatDesc = MIOpenRnn.libmiopen.miopenCreateTensorDescriptor()\n\n\t\tsize = MIOpenRnn.libmiopen.miopenGetRNNLayerParamSize(\n\t\t\tself.dnn.context, rnn.desc, layer, rnn.descData.desc, linLayer\n\t\t)\n\t\tw = self.GPUArray.empty((size // W.dtype.itemsize, ), dtype=W.dtype)\n\n\t\tMIOpenRnn.libmiopen.miopenGetRNNLayerParam(\n\t\t\tself.dnn.context, rnn.desc, layer, rnn.descData.desc, rnn.descW.desc, W.ptr, linLayer,\n\t\t\tlinLayerMatDesc, w.ptr\n\t\t)\n\n\t\tMIOpenRnn.libmiopen.miopenDestroyTensorDescriptor(linLayerMatDesc)\n\n\t\tlinLayerBiasDesc = MIOpenRnn.libmiopen.miopenCreateTensorDescriptor()\n\t\tsize = MIOpenRnn.libmiopen.miopenGetRNNLayerBiasSize(self.dnn.context, rnn.desc, layer, linLayer)\n\n\t\tbias = self.GPUArray.empty((size // W.dtype.itemsize, ), dtype=W.dtype)\n\n\t\tMIOpenRnn.libmiopen.miopenGetRNNLayerBias(\n\t\t\tself.dnn.context, rnn.desc, layer, rnn.descData.desc, rnn.descW.desc, W.ptr, linLayer,\n\t\t\tlinLayerBiasDesc, bias.ptr\n\t\t)\n\n\t\tMIOpenRnn.libmiopen.miopenDestroyTensorDescriptor(linLayerBiasDesc)\n\t\treturn w.reshape(shape), bias\n\n\n\tdef acquireNativeRnnParams(self, rnn, W):\n\t\tlinLayers = 2\n\t\tlayers = rnn.layers if rnn.direction == self.DirectionMode.uni else rnn.layers * 2\n\n\t\tlayerTypes = {0: ""w"", 1: ""r""}\n\n\t\tparams = []\n\t\tfor layer in range(layers):\n\t\t\tlayerparams = {}\n\t\t\tfor linLayer in range(linLayers):\n\t\t\t\tif linLayer == 0:\n\t\t\t\t\tif layer == 0 or layer == 1 and rnn.direction == self.DirectionMode.bi:\n\t\t\t\t\t\tsize = rnn.insize\n\t\t\t\t\telse:\n\t\t\t\t\t\tsize = 2 * rnn.hsize if rnn.direction == self.DirectionMode.bi else rnn.hsize\n\n\t\t\t\t\tshape = (rnn.hsize, size)\n\n\t\t\t\telif linLayer == 1:\n\t\t\t\t\tshape = (rnn.hsize, rnn.hsize)\n\n\t\t\t\telse:\n\t\t\t\t\tassert False\n\n\t\t\t\tw, bias = self.getRnnParam(rnn, layer, W, linLayer, shape)\n\t\t\t\tT = layerTypes[linLayer]\n\n\t\t\t\tWname = ""%si"" % T\n\t\t\t\tassert Wname not in layerparams\n\n\t\t\t\tbiasname = ""b%si"" % T\n\t\t\t\tassert biasname not in layerparams\n\n\t\t\t\tlayerparams[Wname] = w\n\t\t\t\tlayerparams[biasname] = bias\n\n\t\t\tparams.append(layerparams)\n\n\t\treturn params\n\n\tdef acquireLSTMParams(self, rnn, W):\n\t\tlinLayers = 8\n\t\tlayers = rnn.layers if rnn.direction == self.DirectionMode.uni else rnn.layers * 2\n\n\t\tlayerTypes = {\n\t\t\t0: ""i"", 4: ""i"",\n\t\t\t1: ""f"", 5: ""f"",\n\t\t\t2: ""o"", 6: ""o"",\n\t\t\t3: ""c"", 7: ""c""\n\t\t}\n\n\t\tparams = []\n\t\tfor layer in range(layers):\n\t\t\tlayerparams = {}\n\t\t\tfor linLayer in range(linLayers):\n\t\t\t\tif linLayer < 4:\n\t\t\t\t\tif layer == 0 or layer == 1 and rnn.direction == self.DirectionMode.bi:\n\t\t\t\t\t\tsize = rnn.insize\n\t\t\t\t\telse:\n\t\t\t\t\t\tsize = 2 * rnn.hsize if rnn.direction == self.DirectionMode.bi else rnn.hsize\n\n\t\t\t\t\tshape, wtype = (rnn.hsize, size), ""w""\n\n\t\t\t\telse:\n\t\t\t\t\tshape, wtype = (rnn.hsize, rnn.hsize), ""r""\n\n\t\t\t\tw, bias = self.getRnnParam(rnn, layer, W, linLayer, shape)\n\t\t\t\tT = layerTypes[linLayer]\n\n\t\t\t\tWname = ""%s%s"" % (wtype, T)\n\t\t\t\tassert Wname not in layerparams\n\n\t\t\t\tbiasname = ""b%s%s"" % (wtype, T)\n\t\t\t\tassert biasname not in layerparams\n\n\t\t\t\tlayerparams[Wname] = w\n\t\t\t\tlayerparams[biasname] = bias\n\n\t\t\tparams.append(layerparams)\n\n\t\treturn params\n\n\n\tdef acquireGRUParams(self, rnn, W):\n\t\tlinLayers = 6\n\t\tlayers = rnn.layers if rnn.direction == self.DirectionMode.uni else rnn.layers * 2\n\n\t\tlayerTypes = {\n\t\t\t0: ""i"", 3: ""i"",\n\t\t\t1: ""r"", 4: ""r"",\n\t\t\t2: ""h"", 5: ""h""\n\t\t}\n\n\t\tparams = []\n\t\tfor layer in range(layers):\n\t\t\tlayerparams = {}\n\t\t\tfor linLayer in range(linLayers):\n\t\t\t\tif linLayer < 3:\n\t\t\t\t\tif layer == 0 or layer == 1 and rnn.direction == self.DirectionMode.bi:\n\t\t\t\t\t\tsize = rnn.insize\n\t\t\t\t\telse:\n\t\t\t\t\t\tsize = 2 * rnn.hsize if rnn.direction == self.DirectionMode.bi else rnn.hsize\n\n\t\t\t\t\tshape, wtype = (rnn.hsize, size), ""w""\n\n\t\t\t\telse:\n\t\t\t\t\tshape, wtype = (rnn.hsize, rnn.hsize), ""r""\n\n\t\t\t\tw, bias = self.getRnnParam(rnn, layer, W, linLayer, shape)\n\t\t\t\tT = layerTypes[linLayer]\n\n\t\t\t\tWname = ""%s%s"" % (wtype, T)\n\t\t\t\tassert Wname not in layerparams\n\n\t\t\t\tbiasname = ""b%s%s"" % (wtype, T)\n\t\t\t\tassert biasname not in layerparams\n\n\t\t\t\tlayerparams[Wname] = w\n\t\t\t\tlayerparams[biasname] = bias\n\n\t\t\tparams.append(layerparams)\n\n\t\treturn params\n\n\n\tdef updateRnnParams(self, rnn, W, params):\n\t\tif rnn.mode == self.RNNMode.relu or rnn.mode == self.RNNMode.tanh:\n\t\t\tself.updateNativeRnnParams(rnn, W, params)\n\t\telif rnn.mode == self.RNNMode.lstm:\n\t\t\tself.updateLSTMParams(rnn, W, params)\n\t\telif rnn.mode == self.RNNMode.gru:\n\t\t\tself.updateGRUParams(rnn, W, params)\n\t\telse:\n\t\t\traise NotImplementedError(rnn.mode.value)\n\n\n\tdef setRnnParam(self, rnn, layer, W, linLayer, linLayerMat, linLayerBias):\n\t\tdescLinLayerMat = self.dnn.createDescribedNdTensor(linLayerMat)\n\t\tdescLinLayerBias = self.dnn.createDescribedNdTensor(linLayerBias)\n\n\t\tMIOpenRnn.libmiopen.miopenSetRNNLayerParam(\n\t\t\tself.dnn.context, rnn.desc, layer, rnn.descData.desc, rnn.descW.desc, W.ptr,\n\t\t\tlinLayer, descLinLayerMat.desc, descLinLayerMat.ptr\n\t\t)\n\n\t\tMIOpenRnn.libmiopen.miopenSetRNNLayerBias(\n\t\t\tself.dnn.context, rnn.desc, layer, rnn.descData.desc, rnn.descW.desc, W.ptr,\n\t\t\tlinLayer, descLinLayerBias.desc, descLinLayerBias.ptr\n\t\t)\n\n\t\tself.dnn.destroyDescribedTensors(descLinLayerMat, descLinLayerBias)\n\n\n\tdef updateNativeRnnParams(self, rnn, W, params):\n\t\tlinLayers = {""wi"": 0, ""ri"": 1}\n\n\t\tfor layer, subparams in enumerate(params):\n\t\t\tfor name, param in subparams.items():\n\t\t\t\tif name[0] != ""b"":\n\t\t\t\t\tself.setRnnParam(rnn, layer, W, linLayers[name], param, subparams[""b%s"" % name])\n\n\n\tdef updateLSTMParams(self, rnn, W, params):\n\t\tlayerBases = {""w"": 0, ""r"": 4}\n\t\tlayerTypes = {""i"": 0, ""f"": 1, ""o"": 2, ""c"": 3}\n\n\t\tfor layer, subparams in enumerate(params):\n\t\t\tfor name, param in subparams.items():\n\t\t\t\tif name[0] != ""b"":\n\t\t\t\t\tlinLayer = layerBases[name[0]] + layerTypes[name[1]]\n\t\t\t\t\tself.setRnnParam(rnn, layer, W, linLayer, param, subparams[""b%s"" % name])\n\n\n\tdef updateGRUParams(self, rnn, W, params):\n\t\tlayerBases = {""w"": 0, ""r"": 3}\n\t\tlayerTypes = {""i"": 0, ""r"": 1, ""h"": 2}\n\n\t\tfor layer, subparams in enumerate(params):\n\t\t\tfor name, param in subparams.items():\n\t\t\t\tif name[0] != ""b"":\n\t\t\t\t\tlinLayer = layerBases[name[0]] + layerTypes[name[1]]\n\t\t\t\t\tself.setRnnParam(rnn, layer, W, linLayer, param, subparams[""b%s"" % name])\n\n\nbackendCache = {}\n\n\ndef getDeviceCount():\n\treturn HipBackend.Driver.Device.count()\n\n\ndef getBackend(deviceIdx, initmode=0):\n\tbnd = backendCache.get(deviceIdx, None)\n\n\tif bnd is None:\n\t\tbnd = HipBackend(deviceIdx, initmode)\n\t\tbackendCache[deviceIdx] = bnd\n\n\telse:\n\t\tbnd.updateBackend(initmode)\n\n\treturn bnd\n'"
Hip/CheckInstall.py,0,"b'import sys\nfrom PuzzleLib.Cuda.CheckInstall import checkRuntime, checkCompiler, checkPipPackages\n\n\nhipTestKernel = """"""\n\n#include <stdio.h>\n#include <hip/hip_runtime.h>\n\n\n__global__ void iaxpy(int *y, const int *x, int a, int size)\n{\n\tint i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < size) y[i] += a * x[i];\n}\n\n\n#define HIP_ASSERT(status) do { if (!hipAssertStatus((status), __LINE__)) exit(1); } while (0)\ninline bool hipAssertStatus(hipError_t code, int line)\n{\n\tif (code != hipSuccess) \n\t{\n\t\tfprintf(stderr, ""%s (line:%d)\\\\n"", hipGetErrorString(code), line);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n\nint main()\n{\n\tint exitcode = 0;\n\n\tconst int SIZE = 1 << 20;\n\tconst int NBYTES = SIZE * sizeof(int);\n\n\tint *hostx = (int *)malloc(NBYTES);\n\tint *hosty = (int *)malloc(NBYTES);\n\n\tint *devx = NULL, *devy = NULL;\n\tHIP_ASSERT(hipMalloc(&devx, NBYTES));\n\tHIP_ASSERT(hipMalloc(&devy, NBYTES));\n\n\tfor (int i = 0; i < SIZE; i++)\n\t{\n\t\thostx[i] = i;\n\t\thosty[i] = -i * 2;\n\t}\n\n\tHIP_ASSERT(hipMemcpy(devx, hostx, NBYTES, hipMemcpyHostToDevice));\n\tHIP_ASSERT(hipMemcpy(devy, hosty, NBYTES, hipMemcpyHostToDevice));\n\n\tconst int NT = 256;\n\thipLaunchKernelGGL(iaxpy, dim3((SIZE + NT - 1) / NT), dim3(NT), 0, 0, devy, devx, 2, SIZE);\n\n\tHIP_ASSERT(hipMemcpy(hosty, devy, NBYTES, hipMemcpyDeviceToHost));\n\n\tHIP_ASSERT(hipFree(devx));\n\tHIP_ASSERT(hipFree(devy));\n\n\tfor (int i = 0; i < SIZE; i++)\n\t\tif (hosty[i] != 0)\n\t\t{\n\t\t\tfprintf(stderr, ""kernel invocation failed!"");\n\n\t\t\texitcode = 1;\n\t\t\tgoto exit;\n\t\t}\n\n\tprintf(""finished successfully!"");\n\tfflush(stdout);\n\nexit:\n\tfree(hostx);\n\tfree(hosty);\n\n\treturn exitcode;\n}\n\n""""""\n\n\ndef checkHipInstall(withPip):\n\tcheckRuntime(\n\t\tname=""HIP"", compiler=""hipcc"",\n\t\tdownload=""https://rocm.github.io/install.html#ubuntu-support---installing-from-a-debian-repository"",\n\t\tenvpath=""HIP_PATH""\n\t)\n\tcheckCompiler(name=""HIP"", compiler=""hipcc"", kernel=hipTestKernel, ext="".hip.cpp"")\n\n\tif withPip:\n\t\tcheckPipPackages()\n\n\ndef main():\n\ttry:\n\t\tcheckHipInstall(withPip=True)\n\n\texcept RuntimeError as e:\n\t\tprint(e)\n\n\t\tprint(""Exiting ..."")\n\t\tsys.exit(1)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Hip/GPUArray.py,9,"b'import numpy as np\n\nfrom PuzzleLib.Cuda.GPUArray import extendGPUArray, arithmTest\n\nfrom PuzzleLib.Hip import Driver as HipDriver\nfrom PuzzleLib.Hip.SourceModule import HipEltwiseKernel, HipEltHalf2Kernel, HipReductionKernel\n\n\nHipGPUArray = extendGPUArray(HipDriver, HipEltwiseKernel, HipEltHalf2Kernel, HipReductionKernel)\n\n\ndef unittest():\n\tfrom PuzzleLib.Hip import Backend\n\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tbnd = Backend.getBackend(deviceIdx)\n\n\t\tfor dtype, _ in bnd.dtypesSupported():\n\t\t\tarithmTest(bnd, dtype)\n\t\t\tmemoryTest(bnd, dtype)\n\n\ndef memoryTest(bnd, dtype):\n\thostA = np.random.randn(10, 10).astype(dtype)\n\ta = bnd.GPUArray.toGpu(hostA)\n\n\tb = a[:, :6]\n\thostB = hostA[:, :6]\n\n\tassert np.allclose(hostB.reshape((2, 5, 6)), b.reshape(2, 5, 6).get())\n\tassert np.allclose(hostB.reshape((5, 2, 3, 2)), b.reshape(5, 2, 3, 2).get())\n\tassert np.allclose(hostB.reshape((10, 1, 6)), b.reshape(10, 1, 6).get())\n\n\thostA = np.random.randn(10, 10, 10).astype(dtype)\n\ta = bnd.GPUArray.toGpu(hostA)\n\n\tb = a[:, :, :6]\n\tassert np.allclose(hostA[:, :, :6], b.get())\n\n\thostB = np.random.randn(*b.shape).astype(dtype)\n\tb.set(hostB)\n\tassert np.allclose(hostB, b.get())\n\n\thostB = b.get()\n\tb = a[:, :6, :6]\n\tassert np.allclose(hostB[:, :6, :6], b.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Hip/SourceModule.py,0,"b'import os, tempfile, subprocess\nfrom string import Template\n\nfrom PuzzleLib.Config import libname, Backend, systemLog\nfrom PuzzleLib.Compiler.JIT import getCacheDir, computeHash, FileLock\n\nfrom PuzzleLib.Cuda.SourceModule import SourceModule, ElementwiseKernel, ElementHalf2Kernel, ReductionKernel\nfrom PuzzleLib.Cuda.SourceModule import eltwiseTest, reductionTest\n\nfrom PuzzleLib.Hip import Driver as HipDriver\n\n\nhipWarpBit, hipBlockBit = 6, 8\nhipWarpSize, hipBlockSize = 1 << hipWarpBit, 1 << hipBlockBit\n\n\nclass HipSourceModule(SourceModule):\n\tDriver = HipDriver\n\n\truntimeHeader = """"""\n#include <hip/hip_runtime.h>\n\n#define __shfl_xor_sync(mask, value, laneMask, ...) __shfl_xor(value, laneMask, __VA_ARGS__)\n#define __shfl_up_sync(mask, value, delta, ...) __shfl_up(value, delta, __VA_ARGS__)\n""""""\n\n\n\tdef __init__(self, source, options=None, includes=None, externC=False, verbose=True, debug=False, recompile=False,\n\t\t\t\t name=None):\n\t\tsuper().__init__(source, options, includes, externC, verbose, debug, name)\n\n\t\tself.recompile = recompile\n\t\tself.includes = [] if self.includes is None else self.includes\n\n\n\tdef build(self):\n\t\tsource = self.source.replace(""cuda_fp16.h"", ""hip/hip_fp16.h"")\n\t\tsource = (""%sextern \\""C\\""\\n{\\n%s\\n}\\n"" if self.externC else ""%s%s"") % (self.runtimeHeader, source)\n\n\t\tcachedir = getCacheDir(os.path.join(libname, Backend.hip.name))\n\n\t\twith FileLock(cachedir):\n\t\t\ttry:\n\t\t\t\tcodename = self.tryBuild(source, cachedir)\n\n\t\t\texcept subprocess.CalledProcessError as e:\n\t\t\t\tlog = e.output.decode()\n\t\t\t\ttext = log if self.debug else ""%s\\nSource:\\n%s"" % (\n\t\t\t\t\tlog,\n\t\t\t\t\t""\\n"".join(""%-4s    %s"" % (i + 1, line) for i, line in enumerate(source.splitlines(keepends=False)))\n\t\t\t\t)\n\n\t\t\t\traise self.Driver.RtcError(text)\n\n\t\twith open(codename, mode=""rb"") as f:\n\t\t\thsaco = f.read()\n\n\t\tself.cumod = self.Driver.Module(hsaco)\n\n\n\tdef tryBuild(self, source, cachedir):\n\t\toptions, includes = self.options, self.includes\n\t\thashsum = computeHash(source, *options, *includes)\n\n\t\tcodepath = os.path.join(cachedir, hashsum)\n\t\tname, srcext = ""module"" if self.name is None else self.name, "".hip.cpp""\n\n\t\tcodename = os.path.join(codepath, ""%s.code"" % name)\n\t\tsourcename = os.path.join(codepath, ""%s%s"" % (name, srcext))\n\n\t\tif not os.path.exists(codename) or self.recompile:\n\t\t\tos.makedirs(codepath, exist_ok=True)\n\n\t\t\targs = [""hipcc"", ""--genco""] + options + [""-o"", codename]\n\t\t\tstderr = subprocess.STDOUT if self.verbose else subprocess.DEVNULL\n\n\t\t\tif systemLog:\n\t\t\t\tprint(\n\t\t\t\t\t""[%s] No cache found for HIP extension \'%s\', performing compilation ..."" %\n\t\t\t\t\t(libname, name), flush=True\n\t\t\t\t)\n\n\t\t\tif not self.debug:\n\t\t\t\tf = tempfile.NamedTemporaryFile(mode=""w"", encoding=""utf-8"", suffix=srcext, delete=False)\n\t\t\t\ttry:\n\t\t\t\t\twith f:\n\t\t\t\t\t\tf.write(source)\n\n\t\t\t\t\tsubprocess.check_output(args + [f.name], stderr=stderr)\n\n\t\t\t\tfinally:\n\t\t\t\t\tos.remove(f.name)\n\n\t\t\telse:\n\t\t\t\twith open(sourcename, mode=""w"", encoding=""utf-8"") as f:\n\t\t\t\t\tf.write(source)\n\n\t\t\t\tsubprocess.check_output(args + [sourcename], stderr=stderr)\n\n\t\telse:\n\t\t\tif systemLog:\n\t\t\t\tprint(\n\t\t\t\t\t""[%s] Found cached compilation for HIP extension \'%s\', skipping compilation ..."" %\n\t\t\t\t\t(libname, name), flush=True\n\t\t\t\t)\n\n\t\treturn codename\n\n\n\t@classmethod\n\tdef getDefaultOptions(cls):\n\t\tdeviceIdx = cls.Driver.Device.getCurrent()\n\t\treturn [""--targets gfx%s"" % cls.Driver.Device(deviceIdx).getArch()]\n\n\nclass HipEltwiseKernel(ElementwiseKernel):\n\tDriver = HipDriver\n\tSourceModule = HipSourceModule\n\n\twarpBit, warpSize = hipWarpBit, hipWarpSize\n\tblockBit, blockSize = hipBlockBit, hipBlockSize\n\n\nclass HipEltHalf2Kernel(ElementHalf2Kernel):\n\tDriver = HipDriver\n\tSourceModule = HipSourceModule\n\n\twarpBit, warpSize = hipWarpBit, hipWarpSize\n\tblockBit, blockSize = hipBlockBit, hipBlockSize\n\n\nclass HipReductionKernel(ReductionKernel):\n\tDriver = HipDriver\n\tSourceModule = HipSourceModule\n\n\twarpBit, warpSize = hipWarpBit, hipWarpSize\n\tblockBit, blockSize = hipBlockBit, hipBlockSize\n\n\treduceTmpl = Template(""""""\n\n#undef READ_AND_MAP\n#undef REDUCE\n\n#define READ_AND_MAP(i) ($mapExpr)\n#define REDUCE(a, b) ($reduceExpr)\n\n\nextern ""C"" __global__ void $name($arguments, $T *partials, int size)\n{\n\t__shared__ $T sdata[$warpSize];\n\n\tint tid = threadIdx.x;\n\tint gid = tid + blockIdx.x * $NT;\n\n\t$T acc = $neutral;\n\n\tfor (int i = gid; i < size; i += $NT * gridDim.x)\n\t\tacc = REDUCE(acc, READ_AND_MAP(i));\n\n\tfor (int mask = $warpSize / 2; mask > 0; mask /= 2)\n\t{\n\t\t$T upval = __shfl_xor(acc, mask, $warpSize);\n\t\tacc = REDUCE(acc, upval);\n\t}\n\n\tif (tid % $warpSize == 0)\n\t\tsdata[tid / $warpSize] = acc;\n\n\t__syncthreads();\n\tint nwarps = $NT / $warpSize;\n\n\tif (tid < $warpSize)\n\t{\n\t\tacc = (tid < nwarps) ? sdata[tid] : $neutral;\n\n\t\tfor (int mask = $warpSize / 2; mask > 0; mask /= 2)\n\t\t{\n\t\t\t$T upval = __shfl_xor(acc, mask, $warpSize);\n\t\t\tacc = REDUCE(acc, upval);\n\t\t}\n\t}\n\n\tif (tid == 0)\n\t\tpartials[blockIdx.x] = acc;\n}\n\n"""""")\n\n\ndef unittest():\n\tfrom PuzzleLib.Hip import Backend\n\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tbnd = Backend.getBackend(deviceIdx)\n\n\t\teltwiseTest(bnd)\n\t\treductionTest(bnd)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Hip/Utils.py,13,"b'import numpy as np\n\nfrom PuzzleLib.Cuda.Utils import SharedArray, shareMemTest, randomTest\nfrom PuzzleLib.Hip import Driver as HipDriver\n\n\nclass HipSharedArray(SharedArray):\n\tGPUArray = HipDriver.GPUArray\n\n\ndef unittest():\n\tfrom PuzzleLib.Hip import Backend\n\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tbnd = Backend.getBackend(deviceIdx, initmode=2)\n\n\t\tfor dtype, _ in bnd.dtypesSupported():\n\t\t\tshareMemTest(bnd, dtype)\n\t\t\tmemCopyTest(bnd, dtype)\n\n\t\trandomTest(bnd)\n\n\ndef memCopyTest(bnd, dtype):\n\thostSrc = np.random.randn(4, 4, 4, 4).astype(dtype)\n\n\tsrc = bnd.GPUArray.toGpu(hostSrc)\n\tassert np.allclose(hostSrc, src.copy().get())\n\n\thostA = np.random.randn(7, 4, 4, 4).astype(dtype)\n\ta = bnd.GPUArray.toGpu(hostA)\n\n\tout = bnd.concatenate((src, a), axis=0)\n\tassert np.allclose(np.concatenate((hostSrc, hostA), axis=0), out.get())\n\n\thostA = np.random.randn(4, 2, 4, 4).astype(dtype)\n\thostB = np.random.randn(4, 1, 4, 4).astype(dtype)\n\n\ta, b = bnd.GPUArray.toGpu(hostA), bnd.GPUArray.toGpu(hostB)\n\n\tout = bnd.concatenate((src, a, b), axis=1)\n\tassert np.allclose(np.concatenate((hostSrc, hostA, hostB), axis=1), out.get())\n\n\thostA = np.random.randn(4, 4, 5, 4).astype(dtype)\n\n\tout = bnd.concatenate((bnd.GPUArray.toGpu(hostA), src), axis=2)\n\tassert np.allclose(np.concatenate((hostA, hostSrc), axis=2), out.get())\n\n\touts = bnd.split(src, (2, 2), axis=0)\n\tassert all(np.allclose(hostSrc[2 * i:2 * (i + 1)], out.get()) for i, out in enumerate(outs))\n\n\touts = bnd.split(src, (2, 2), axis=1)\n\tassert all(np.allclose(hostSrc[:, 2 * i:2 * (i + 1), :, :], out.get()) for i, out in enumerate(outs))\n\n\touts = bnd.split(src, (2, 2), axis=2)\n\tassert all(np.allclose(hostSrc[:, :, 2 * i:2 * (i + 1), :], out.get()) for i, out in enumerate(outs))\n\n\tassert np.allclose(np.tile(hostB, (1, 3, 1, 1)), bnd.tile(b, 3, axis=1).get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Activation.py,10,"b'from enum import Enum\n\nimport numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import dtypesSupported, memoryPool as memPool\n\nfrom PuzzleLib.Backend.Kernels.ElementWise import sigmoidKer, sigmoidDerKer, tanhKer, tanhDerKer, reluKer, reluDerKer\nfrom PuzzleLib.Backend.Kernels.ElementWise import leakyReluKer, leakyReluDerKer, eluKer, eluDerKer\nfrom PuzzleLib.Backend.Kernels.ElementWise import softPlusKer, softPlusDerKer, clipKer, clipDerKer\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass ActivationType(str, Enum):\n\tsigmoid = ""sigmoid""\n\ttanh = ""tanh""\n\trelu = ""relu""\n\tleakyRelu = ""leakyRelu""\n\telu = ""elu""\n\tsoftPlus = ""softPlus""\n\tclip = ""clip""\n\n\nsigmoid = ActivationType.sigmoid\ntanh = ActivationType.tanh\nrelu = ActivationType.relu\nleakyRelu = ActivationType.leakyRelu\nelu = ActivationType.elu\nsoftPlus = ActivationType.softPlus\nclip = ActivationType.clip\n\n\nclass Activation(Module):\n\tdef __init__(self, activation, slc=None, inplace=False, name=None, args=()):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.gradUsesOutData = True\n\n\t\tself.inplace = inplace\n\t\tif inplace and Config.showWarnings:\n\t\t\tprint(""[%s] Warning: %s is using inplace flag"" % (Config.libname, self))\n\n\t\tactivation = ActivationType(activation)\n\n\t\tself.actFunc, self.actFuncDer = {\n\t\t\tActivationType.sigmoid: (sigmoidKer, sigmoidDerKer),\n\t\t\tActivationType.tanh: (tanhKer, tanhDerKer),\n\t\t\tActivationType.relu: (reluKer, reluDerKer),\n\t\t\tActivationType.leakyRelu: (leakyReluKer, leakyReluDerKer),\n\t\t\tActivationType.elu: (eluKer, eluDerKer),\n\t\t\tActivationType.softPlus: (softPlusKer, softPlusDerKer),\n\t\t\tActivationType.clip: (clipKer, clipDerKer)\n\t\t}[activation]\n\n\t\tself.activation = activation\n\t\tself.slc = slc\n\n\t\tself.actArgs = args if len(args) > 0 else {\n\t\t\tActivationType.leakyRelu: (0.01, ),\n\t\t\tActivationType.elu: (1.0, ),\n\t\t\tActivationType.clip: (0.0, 6.0)\n\t\t}.get(activation, ())\n\n\n\tdef updateData(self, data):\n\t\tself.data = data if self.inplace else gpuarray.empty(data.shape, dtype=data.dtype, allocator=memPool)\n\t\tself.actFunc(data.dtype)(self.data, data, *self.actArgs, slice=self.slc)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = grad if self.inplace else gpuarray.empty(grad.shape, dtype=grad.dtype, allocator=memPool)\n\t\tself.actFuncDer(grad.dtype)(self.grad, grad, self.data, *self.actArgs, slice=self.slc)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef calcMode(self, T):\n\t\tdtypes = {dtype for dtype, _ in dtypesSupported()}\n\n\t\tif T not in dtypes:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n\n\ndef unittest():\n\tactFuncs = {\n\t\tActivationType.sigmoid: (\n\t\t\tlambda data: 1.0 / (1.0 + np.exp(-data)),\n\t\t\tlambda grad, outdata: grad * outdata * (1.0 - outdata)\n\t\t),\n\n\t\tActivationType.tanh: (\n\t\t\tnp.tanh,\n\t\t\tlambda grad, outdata: grad * (1.0 - outdata ** 2)\n\t\t),\n\n\t\tActivationType.relu: (\n\t\t\tlambda data: (data > 0.0) * data,\n\t\t\tlambda grad, outdata: (outdata > 0.0) * grad\n\t\t),\n\n\t\tActivationType.leakyRelu: (\n\t\t\tlambda data: data * ((data > 0.0) + (data <= 0.0) * 0.01),\n\t\t\tlambda grad, outdata: grad * ((outdata > 0.0) + (outdata <= 0.0) * 0.01)\n\t\t),\n\n\t\tActivationType.elu: (\n\t\t\tlambda data: (data > 0.0) * data + (data <= 0.0) * (np.exp(data) - 1.0),\n\t\t\tlambda grad, outdata: grad * ((outdata > 0.0) + (outdata <= 0.0) * (outdata + 1.0))\n\t\t),\n\n\t\tActivationType.softPlus: (\n\t\t\tlambda data: np.log(1.0 + np.exp(data)),\n\t\t\tlambda grad, outdata: grad * (1.0 - np.exp(-outdata))\n\t\t),\n\n\t\tActivationType.clip: (\n\t\t\tlambda data: np.clip(data, 0.0, 6.0),\n\t\t\tlambda grad, outdata: grad * ((0.0 < outdata) & (outdata < 6.0))\n\t\t)\n\t}\n\n\tfor dtype, atol in dtypesSupported():\n\t\tfor acttype, hostAct in actFuncs.items():\n\t\t\tactTest(acttype, hostAct, dtype, atol)\n\n\ndef actTest(devAct, hostAct, dtype, atol):\n\tact = Activation(devAct)\n\tact.calcMode(dtype)\n\n\thostData = np.random.randn(11, 51).astype(dtype)\n\n\tdata = gpuarray.to_gpu(hostData)\n\tact(data)\n\n\thostGrad = np.random.randn(*act.data.shape).astype(dtype)\n\n\tgrad = gpuarray.to_gpu(hostGrad)\n\tact.backward(grad)\n\n\thostActFwd, hostActBwd = hostAct\n\n\thostOutData = hostActFwd(hostData)\n\thostInGrad = hostActBwd(hostGrad, hostOutData)\n\n\tassert np.allclose(hostOutData, act.data.get(), atol=atol)\n\tassert np.allclose(hostInGrad, act.grad.get(), atol=atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Add.py,3,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray, Blas\nfrom PuzzleLib.Backend.Utils import dtypesSupported, memoryPool as memPool\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass Add(Module):\n\tdef __init__(self, name=None):\n\t\tsuper().__init__(name)\n\t\tself.movesGrad = True\n\n\n\tdef updateData(self, data):\n\t\tfirstdata = data[0]\n\n\t\tself.data = gpuarray.empty(firstdata.shape, dtype=firstdata.dtype, allocator=memPool)\n\t\tself.data.fill(0)\n\n\t\tfor dat in data:\n\t\t\tBlas.toVectorAddVector(self.data.ravel(), dat.ravel())\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = [grad] * len(self.inData)\n\n\n\tdef checkDataShape(self, shapes):\n\t\tfor shape in shapes:\n\t\t\tif shape != shapes[0]:\n\t\t\t\traise ModuleError(""Shape %s is not equal to initial shape %s"" % (shape, shapes[0]))\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape[0]\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn [shape] * len(self.inData)\n\n\n\tdef calcMode(self, T):\n\t\tdtypes = {dtype for dtype, _ in dtypesSupported()}\n\n\t\tif T not in dtypes:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n\n\ndef unittest():\n\tfor dtype, _ in dtypesSupported():\n\t\taddTest(dtype)\n\n\ndef addTest(dtype):\n\thostData1 = np.random.randn(2, 5, 5).astype(dtype)\n\thostData2 = np.random.randn(*hostData1.shape).astype(dtype)\n\n\tdata1, data2 = gpuarray.to_gpu(hostData1), gpuarray.to_gpu(hostData2)\n\n\tadd = Add()\n\tadd.calcMode(dtype)\n\n\tadd([data1, data2])\n\tassert np.allclose(hostData1 + hostData2, add.data.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/AvgPool1D.py,8,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Dnn.Basic import PoolMode, poolNd, poolNdBackward\n\nfrom PuzzleLib.Modules.Pool1D import Pool1D\n\n\nclass AvgPool1D(Pool1D):\n\tdef __init__(self, size=2, stride=2, pad=0, includePad=True, name=None):\n\t\tsuper().__init__(size, stride, pad, name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.mode = PoolMode.avgWithPad if includePad else PoolMode.avgNoPad\n\n\n\tdef updateData(self, data):\n\t\tdata = data.reshape(*data.shape[:2], 1, *data.shape[2:])\n\t\tself.data, self.workspace = poolNd(data, size=self.size, stride=self.stride, pad=self.pad, mode=self.mode,\n\t\t\t\t\t\t\t\t\t\t   test=not self.train)\n\t\tself.data = self.data.reshape(*self.data.shape[:2], *self.data.shape[3:])\n\n\n\tdef updateGrad(self, grad):\n\t\tgrad = grad.reshape(*grad.shape[:2], 1, *grad.shape[2:])\n\n\t\tindata = self.inData.reshape(*self.inData.shape[:2], 1, *self.inData.shape[2:])\n\t\toutdata = self.data.reshape(*self.data.shape[:2], 1, *self.data.shape[2:])\n\n\t\tself.grad = poolNdBackward(indata, outdata, grad, self.workspace, size=self.size, stride=self.stride,\n\t\t\t\t\t\t\t\t   pad=self.pad, mode=self.mode)\n\t\tself.grad = self.grad.reshape(*self.grad.shape[:2], *self.grad.shape[3:])\n\n\ndef unittest():\n\tbatchsize, maps, insize = 2, 6, 5\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, insize).astype(np.float32))\n\n\tsize = 3\n\tstride, pad = 2, 1\n\n\tavgpool1d = AvgPool1D(size=size, stride=stride, pad=pad, includePad=True)\n\tavgpool1d(data)\n\n\thostData = np.zeros(shape=(batchsize, maps, insize + 2 * pad), dtype=np.float32)\n\thostData[:, :, pad:-pad] = data.get()\n\thostOutData = np.empty(avgpool1d.data.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor x in range(hostOutData.shape[2]):\n\t\t\t\thostOutData[b, c, x] = np.mean(hostData[b, c, x * stride:x * stride + size])\n\n\tassert np.allclose(hostOutData, avgpool1d.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*avgpool1d.data.shape).astype(np.float32))\n\tavgpool1d.backward(grad)\n\n\thostGrad = grad.get()\n\thostInGrad = np.zeros(hostData.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor x in range(hostOutData.shape[2]):\n\t\t\t\tfor dx in range(size):\n\t\t\t\t\thostInGrad[b, c, x * stride+dx] += hostGrad[b, c, x] / size\n\n\tassert np.allclose(hostInGrad[:, :, pad:-pad], avgpool1d.grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/AvgPool2D.py,8,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Dnn.Basic import PoolMode, poolNd, poolNdBackward\n\nfrom PuzzleLib.Modules.Pool2D import Pool2D\n\n\nclass AvgPool2D(Pool2D):\n\tdef __init__(self, size=2, stride=2, pad=0, includePad=True, name=None):\n\t\tsuper().__init__(size, stride, pad, name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.mode = PoolMode.avgWithPad if includePad else PoolMode.avgNoPad\n\n\n\tdef updateData(self, data):\n\t\tself.data, self.workspace = poolNd(data, size=self.size, stride=self.stride, pad=self.pad, mode=self.mode,\n\t\t\t\t\t\t\t\t\t\t   test=not self.train)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = poolNdBackward(self.inData, self.data, grad, self.workspace, size=self.size, stride=self.stride,\n\t\t\t\t\t\t\t\t   pad=self.pad, mode=self.mode)\n\n\ndef unittest():\n\tbatchsize, maps, h, w = 2, 3, 6, 6\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, h, w).astype(np.float32))\n\n\tsize = 3\n\tstride, pad = 1, 1\n\n\tavgpool2d = AvgPool2D(size=size, stride=stride, pad=pad, includePad=True)\n\tavgpool2d(data)\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*avgpool2d.data.shape).astype(np.float32))\n\tavgpool2d.backward(grad)\n\n\thostData = np.zeros(shape=(batchsize, maps, h + 2 * pad, w + 2 * pad), dtype=np.float32)\n\thostData[:, :, pad:-pad, pad:-pad] = data.get()\n\n\thostOutData = np.empty(avgpool2d.data.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor y in range(avgpool2d.data.shape[2]):\n\t\t\t\tfor x in range(avgpool2d.data.shape[3]):\n\t\t\t\t\thostOutData[b,c,y,x] = np.sum(hostData[b,c,y*stride:y*stride+size, x*stride:x*stride+size])/size**2\n\n\tassert np.allclose(hostOutData, avgpool2d.data.get())\n\n\thostGrad, hostInGrad = grad.get(), np.zeros(hostData.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor y in range(hostGrad.shape[2]):\n\t\t\t\tfor x in range(hostGrad.shape[3]):\n\t\t\t\t\tfor dy in range(size):\n\t\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\t\thostInGrad[b, c, y * stride + dy, x * stride + dx] += hostGrad[b, c, y, x] / size**2\n\n\tassert np.allclose(hostInGrad[:, :, pad:-pad, pad:-pad], avgpool2d.grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/AvgPool3D.py,8,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Dnn.Basic import PoolMode, poolNd, poolNdBackward\n\nfrom PuzzleLib.Modules.Pool3D import Pool3D\n\n\nclass AvgPool3D(Pool3D):\n\tdef __init__(self, size=2, stride=2, pad=0, includePad=True, name=None):\n\t\tsuper().__init__(size, stride, pad, name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.mode = PoolMode.avgWithPad if includePad else PoolMode.avgNoPad\n\n\n\tdef updateData(self, data):\n\t\tself.data, self.workspace = poolNd(data, size=self.size, stride=self.stride, pad=self.pad, mode=self.mode,\n\t\t\t\t\t\t\t\t\t\t   test=not self.train)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = poolNdBackward(self.inData, self.data, grad, self.workspace, size=self.size, stride=self.stride,\n\t\t\t\t\t\t\t\t   pad=self.pad, mode=self.mode)\n\n\ndef unittest():\n\tbatchsize, maps, d, h, w = 2, 6, 5, 7, 5\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, d, h, w).astype(np.float32))\n\n\tsize = 3\n\tstride, pad = 2, 1\n\n\tavgpool3d = AvgPool3D(size=size, stride=stride, pad=pad, includePad=True)\n\tavgpool3d(data)\n\n\thostData = np.zeros(shape=(batchsize, maps, d + 2 * pad, h + 2 * pad, w + 2 * pad), dtype=np.float32)\n\thostData[:, :, pad:-pad, pad:-pad, pad:-pad] = data.get()\n\thostOutData = np.empty(avgpool3d.data.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor z in range(hostOutData.shape[2]):\n\t\t\t\tfor y in range(hostOutData.shape[3]):\n\t\t\t\t\tfor x in range(hostOutData.shape[4]):\n\t\t\t\t\t\thostOutData[b, c, z, y, x] = np.mean(hostData[b, c, z * stride:z * stride + size,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t y * stride:y * stride + size,x * stride:x * stride + size])\n\n\tassert np.allclose(hostOutData, avgpool3d.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*avgpool3d.data.shape).astype(np.float32))\n\tavgpool3d.backward(grad)\n\n\thostGrad = grad.get()\n\thostInGrad = np.zeros(hostData.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor z in range(hostOutData.shape[2]):\n\t\t\t\tfor y in range(hostOutData.shape[3]):\n\t\t\t\t\tfor x in range(hostOutData.shape[4]):\n\t\t\t\t\t\tfor dz in range(size):\n\t\t\t\t\t\t\tfor dy in range(size):\n\t\t\t\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\t\t\t\thostInGrad[b,c,z*stride+dz,y*stride+dy,x*stride+dx] += hostGrad[b,c,z,y,x]/size**3\n\n\tassert np.allclose(hostInGrad[:, :, pad:-pad, pad:-pad, pad:-pad], avgpool3d.grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/BatchNorm.py,24,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray, Blas\nfrom PuzzleLib.Backend.Dnn.Basic import BatchNormMode, batchNormNd, batchNormNdBackward\n\nfrom PuzzleLib.Variable import Variable\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass BatchNorm(Module):\n\tdef __init__(self, size, epsilon=1e-5, initFactor=1.0, minFactor=0.1, sscale=0.01, affine=True, name=None,\n\t\t\t\t empty=False, inplace=False):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.inplace = inplace\n\t\tif inplace and Config.showWarnings:\n\t\t\tprint(""[%s] Warning: %s is using inplace flag"" % (Config.libname, self))\n\n\t\tself.size = size\n\t\tself.epsilon = epsilon\n\t\tself.initFactor = initFactor\n\t\tself.minFactor = minFactor\n\t\tself.numOfProps = 0\n\n\t\tself.affine = affine\n\n\t\tself.scale, self.bias, self.mean, self.var = None, None, None, None\n\t\tself.savemean, self.saveinvvar, self.scalegrad, self.biasgrad = None, None, None, None\n\n\t\tif empty:\n\t\t\treturn\n\n\t\tscale = np.random.normal(1.0, sscale if affine else 0.0, (1, size, 1, 1)).astype(np.float32)\n\t\tvar = np.ones((1, size, 1, 1), dtype=np.float32)\n\n\t\tself.setVar(""scale"", Variable(gpuarray.to_gpu(scale)))\n\t\tself.setVar(""bias"", Variable(gpuarray.zeros((1, size, 1, 1), dtype=np.float32)))\n\n\t\tself.setAttr(""mean"", gpuarray.zeros((1, size, 1, 1), dtype=np.float32))\n\t\tself.setAttr(""var"", gpuarray.to_gpu(var))\n\n\n\tdef updateData(self, data):\n\t\tindata = data.reshape(data.shape[0], self.size, 1, 1)\n\n\t\tif self.train:\n\t\t\tif self.inplace:\n\t\t\t\traise ModuleError(""%s: using inplace flag in train mode is prohibited"" % self)\n\n\t\t\tself.numOfProps += 1\n\t\t\tfactor = max(self.initFactor / self.numOfProps, self.minFactor)\n\n\t\t\tself.data, self.savemean, self.saveinvvar = batchNormNd(\n\t\t\t\tindata, self.scale, self.bias, self.mean, self.var, self.epsilon, factor, False,\n\t\t\t\tBatchNormMode.perActivation\n\t\t\t)\n\n\t\telse:\n\t\t\tself.data = batchNormNd(\n\t\t\t\tindata, self.scale, self.bias, self.mean, self.var, self.epsilon, 0, True,\n\t\t\t\tBatchNormMode.perActivation, out=indata if self.inplace else None\n\t\t\t)\n\n\t\tself.data = self.data.reshape(*data.shape)\n\n\n\tdef updateGrad(self, grad):\n\t\tdata = self.inData.reshape(self.inData.shape[0], self.size, 1, 1)\n\t\toutgrad = grad.reshape(grad.shape[0], self.size, 1, 1)\n\n\t\ttup = batchNormNdBackward(\n\t\t\tdata, outgrad, self.scale, self.savemean, self.saveinvvar, self.epsilon, mode=BatchNormMode.perActivation\n\t\t)\n\n\t\tif self.affine:\n\t\t\tself.grad, self.scalegrad, self.biasgrad = tup\n\t\telse:\n\t\t\tself.grad, _, _ = tup\n\n\t\tself.grad = self.grad.reshape(*grad.shape)\n\n\n\tdef accGradParams(self, grad, scale=1.0, momentum=0.0):\n\t\tif self.affine:\n\t\t\tBlas.addVectorToVector(\n\t\t\t\tself.scalegrad.ravel(), self.vars[""scale""].grad.ravel(),\n\t\t\t\tout=self.vars[""scale""].grad.ravel(), alpha=scale, beta=momentum\n\t\t\t)\n\t\t\tBlas.addVectorToVector(\n\t\t\t\tself.biasgrad.ravel(), self.vars[""bias""].grad.ravel(),\n\t\t\t\tout=self.vars[""bias""].grad.ravel(), alpha=scale, beta=momentum\n\t\t\t)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 2:\n\t\t\traise ModuleError(""Data must be 2d matrix"")\n\n\t\tif int(np.prod(shape[1])) != self.size:\n\t\t\traise ModuleError(""Expected %d data dimensions, %d were given"" % (self.size, shape[1]))\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 2:\n\t\t\traise ModuleError(""Grad must be 2d matrix"")\n\n\t\tif int(np.prod(shape[1])) != self.size:\n\t\t\traise ModuleError(""Expected %d grad dimensions, %d were given"" % (self.size, shape[1]))\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\t\tself.savemean, self.saveinvvar = None, None\n\n\t\tif self.affine:\n\t\t\tself.scalegrad, self.biasgrad = None, None\n\n\n\tdef calcMode(self, T):\n\t\tif Config.backend == Config.Backend.cuda:\n\t\t\tif T not in {np.float16, np.float32}:\n\t\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\telif T != np.float32:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n\n\ndef unittest():\n\tbatchsize, insize = 16, 10\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, insize).astype(np.float32))\n\n\tbn = BatchNorm(insize)\n\tbn(data)\n\n\thostData = data.get()\n\thostMean = np.mean(hostData, axis=0, keepdims=False)\n\thostInvVar = 1.0 / np.sqrt(np.sum((hostData - hostMean[np.newaxis, :])**2, axis=0) / batchsize + 1e-5)\n\n\thostScale = bn.scale.get().squeeze()\n\thostBias = bn.bias.get().squeeze()\n\n\thostNormData = (data.get() - hostMean) * hostInvVar\n\thostOutData = hostNormData * hostScale + hostBias\n\n\tassert np.allclose(hostMean, bn.savemean.get().squeeze())\n\tassert np.allclose(hostInvVar, bn.saveinvvar.get().squeeze())\n\tassert np.allclose(hostOutData, bn.data.get().squeeze())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(batchsize, insize).astype(np.float32))\n\tbn.backward(grad)\n\n\thostGrad = grad.get()\n\n\thostBiasGrad = np.sum(hostGrad, axis=0)\n\thostScaleGrad = np.sum(hostGrad * hostNormData, axis=0)\n\thostMeanGrad = np.sum(hostGrad, axis=0) * hostScale * -hostInvVar\n\thostVarGrad = np.sum(hostGrad * (hostData - hostMean[np.newaxis, :]), axis=0) * \\\n\t\t\t\t  hostScale[np.newaxis, :] * -0.5 * hostInvVar[np.newaxis, :]**3\n\n\thostInGrad = grad.get() * hostScale[np.newaxis, :] * hostInvVar[np.newaxis, :] + \\\n\t\t\t\t hostVarGrad * 2 / batchsize * (data.get() - hostMean) + hostMeanGrad / batchsize\n\n\tassert np.allclose(hostBiasGrad, bn.vars[""bias""].grad.get().squeeze())\n\tassert np.allclose(hostScaleGrad, bn.vars[""scale""].grad.get().squeeze())\n\tassert np.allclose(hostInGrad, bn.grad.get().squeeze())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/BatchNorm1D.py,20,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Modules.Module import ModuleError\nfrom PuzzleLib.Modules.BatchNormND import BatchNormND\n\n\nclass BatchNorm1D(BatchNormND):\n\tdef __init__(self, maps, epsilon=1e-5, initFactor=1.0, minFactor=0.1, sscale=0.01, affine=True, name=None,\n\t\t\t\t empty=False, inplace=False):\n\t\tsuper().__init__(2, maps, epsilon, initFactor, minFactor, sscale, affine, name, empty, inplace)\n\t\tself.registerBlueprint(locals())\n\n\n\tdef updateData(self, data):\n\t\tdata = data.reshape(*data.shape[:2], 1, *data.shape[2:])\n\t\tsuper().updateData(data)\n\t\tself.data = self.data.reshape(*self.data.shape[:2], *self.data.shape[3:])\n\n\n\tdef updateGrad(self, grad):\n\t\tgrad = grad.reshape(*grad.shape[:2], 1, *grad.shape[2:])\n\n\t\tdata = self.inData\n\t\tself.inData = data.reshape(*data.shape[:2], 1, *data.shape[2:])\n\t\tsuper().updateGrad(grad)\n\t\tself.inData = data\n\n\t\tself.grad = self.grad.reshape(*self.grad.shape[:2], *self.grad.shape[3:])\n\n\n\tdef accGradParams(self, grad, scale=1.0, momentum=0.0):\n\t\tgrad = grad.reshape(*grad.shape[:2], 1, *grad.shape[2:])\n\n\t\tdata = self.inData\n\t\tself.inData = data.reshape(*data.shape[:2], 1, *data.shape[2:])\n\t\tsuper().accGradParams(grad, scale, momentum)\n\t\tself.inData = data\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 3:\n\t\t\traise ModuleError(""Data must be 3d tensor"")\n\n\t\t_, maps, _ = shape\n\t\tif maps != self.maps:\n\t\t\traise ModuleError(""Data has %d maps (expected: %d)"" % (maps, self.maps))\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 3:\n\t\t\traise ModuleError(""Grad must be 3d tensor"")\n\n\t\t_, maps, _ = shape\n\t\tif maps != self.maps:\n\t\t\traise ModuleError(""Grad has %d maps (expected: %d)"" % (maps, self.maps))\n\n\ndef unittest():\n\tbatchsize, maps, size = 16, 5, 4\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, size).astype(np.float32))\n\n\tbn = BatchNorm1D(maps)\n\tbn(data)\n\n\thostData, hostScale, hostBias = data.get(), bn.scale.get(), bn.bias.get()\n\thostNormData, hostOutData = np.empty(hostData.shape, dtype=np.float32), np.empty(hostData.shape, dtype=np.float32)\n\thostMean, hostInvVar = np.zeros(hostScale.shape, dtype=np.float32), np.zeros(hostScale.shape, dtype=np.float32)\n\tfor c in range(maps):\n\t\tfor b in range(batchsize):\n\t\t\thostMean[0, c, 0] += np.sum(hostData[b, c])\n\t\thostMean[0, c, 0] /= (batchsize * size)\n\n\t\tfor b in range(batchsize):\n\t\t\thostInvVar[0, c, 0] += np.sum((hostData[b, c] - hostMean[0, c, 0])**2)\n\t\thostInvVar[0, c, 0] /= (batchsize * size)\n\n\t\thostInvVar[0, c, 0] = 1.0 / np.sqrt(hostInvVar[0, c, 0] + bn.epsilon)\n\t\thostNormData[:, c, :] = (hostData[:, c, :] - hostMean[0, c, 0]) * hostInvVar[0, c, 0]\n\t\thostOutData[:, c, :] = hostNormData[:, c, :] * hostScale[0, c, 0] + hostBias[0, c, 0]\n\n\tassert np.allclose(hostMean, bn.mean.get())\n\tassert np.allclose(hostInvVar, bn.saveinvvar.get())\n\tassert np.allclose(hostOutData, bn.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(batchsize, maps, size).astype(np.float32))\n\tbn.backward(grad)\n\n\thostGrad, hostInGrad = grad.get(), np.empty_like(hostData)\n\thostScaleGrad, hostBiasGrad = np.empty_like(hostScale), np.empty_like(hostBias)\n\thostMeanGrad, hostVarGrad = np.empty_like(hostMean), np.empty_like(hostInvVar)\n\tfor c in range(maps):\n\t\thostBiasGrad[0, c, 0] = np.sum(hostGrad[:, c, :])\n\t\thostScaleGrad[0, c, 0] = np.sum(hostGrad[:, c, :] * hostNormData[:, c, :])\n\n\t\thostMeanGrad[0, c, 0] = np.sum(hostGrad[:, c, :]) * hostScale[0, c, 0] * -hostInvVar[0, c, 0]\n\t\thostVarGrad[0, c, 0] = np.sum(hostGrad[:, c, :] * (hostData[:, c, :] - hostMean[0, c, 0])) * \\\n\t\t\t\t\t\t\t   hostScale[0, c, 0] * -0.5 * hostInvVar[0, c, 0]**3\n\n\t\thostInGrad[:, c, :] = hostGrad[:, c, :] * hostScale[0, c, 0] * hostInvVar[0, c, 0] + \\\n\t\t\t\t\t\t\t  hostVarGrad[0, c, 0] * 2.0 / (batchsize * size) * \\\n\t\t\t\t\t\t\t  (hostData[:, c, :] - hostMean[0, c, 0]) + \\\n\t\t\t\t\t\t\t  hostMeanGrad[0, c, 0] / (batchsize * size)\n\n\tassert np.allclose(hostInGrad, bn.grad.get())\n\tassert np.allclose(hostScaleGrad, bn.vars[""scale""].grad.get())\n\tassert np.allclose(hostBiasGrad, bn.vars[""bias""].grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/BatchNorm2D.py,20,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Modules.Module import ModuleError\nfrom PuzzleLib.Modules.BatchNormND import BatchNormND\n\n\nclass BatchNorm2D(BatchNormND):\n\tdef __init__(self, maps, epsilon=1e-5, initFactor=1.0, minFactor=0.1, sscale=0.01, affine=True, name=None,\n\t\t\t\t empty=False, inplace=False):\n\t\tsuper().__init__(2, maps, epsilon, initFactor, minFactor, sscale, affine, name, empty, inplace)\n\t\tself.registerBlueprint(locals())\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 4:\n\t\t\traise ModuleError(""Data must be 4d tensor"")\n\n\t\t_, maps, _, _ = shape\n\t\tif maps != self.maps:\n\t\t\traise ModuleError(""Data has %d maps (expected: %d)"" % (maps, self.maps))\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 4:\n\t\t\traise ModuleError(""Grad must be 4d tensor"")\n\n\t\t_, maps, _, _ = shape\n\t\tif maps != self.maps:\n\t\t\traise ModuleError(""Grad has %d maps (expected: %d)"" % (maps, self.maps))\n\n\ndef unittest():\n\tbatchsize, maps, h, w = 16, 5, 4, 2\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, h, w).astype(np.float32))\n\n\tbn = BatchNorm2D(maps)\n\tbn(data)\n\n\thostData, hostScale, hostBias = data.get(), bn.scale.get(), bn.bias.get()\n\thostNormData, hostOutData = np.empty(hostData.shape, dtype=np.float32), np.empty(hostData.shape, dtype=np.float32)\n\thostMean, hostInvVar = np.zeros(hostScale.shape, dtype=np.float32), np.zeros(hostScale.shape, dtype=np.float32)\n\tfor c in range(maps):\n\t\tfor b in range(batchsize):\n\t\t\thostMean[0, c, 0, 0] += np.sum(hostData[b, c])\n\t\thostMean[0, c, 0, 0] /= (batchsize * w * h)\n\n\t\tfor b in range(batchsize):\n\t\t\thostInvVar[0, c, 0, 0] += np.sum((hostData[b, c] - hostMean[0, c, 0, 0])**2)\n\t\thostInvVar[0, c, 0, 0] /= (batchsize * w * h)\n\n\t\thostInvVar[0, c, 0, 0] = 1.0 / np.sqrt(hostInvVar[0, c, 0, 0] + bn.epsilon)\n\t\thostNormData[:, c, :, :] = (hostData[:, c, :, :] - hostMean[0, c, 0, 0]) * hostInvVar[0, c, 0, 0]\n\t\thostOutData[:, c, :, :] = hostNormData[:, c, :, :] * hostScale[0, c, 0, 0] + hostBias[0, c, 0, 0]\n\n\tassert np.allclose(hostMean, bn.mean.get())\n\tassert np.allclose(hostInvVar, bn.saveinvvar.get())\n\tassert np.allclose(hostOutData, bn.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(batchsize, maps, h, w).astype(np.float32))\n\tbn.backward(grad)\n\n\thostGrad, hostInGrad = grad.get(), np.empty_like(hostData)\n\thostScaleGrad, hostBiasGrad = np.empty_like(hostScale), np.empty_like(hostBias)\n\thostMeanGrad, hostVarGrad = np.empty_like(hostMean), np.empty_like(hostInvVar)\n\tfor c in range(maps):\n\t\thostBiasGrad[0, c, 0, 0] = np.sum(hostGrad[:, c, :, :])\n\t\thostScaleGrad[0, c, 0, 0] = np.sum(hostGrad[:, c, :, :] * hostNormData[:, c, :, :])\n\n\t\thostMeanGrad[0, c, 0, 0] = np.sum(hostGrad[:, c, :, :]) * hostScale[0, c, 0, 0] * -hostInvVar[0, c, 0, 0]\n\t\thostVarGrad[0, c, 0, 0] = np.sum(hostGrad[:, c, :, :] * (hostData[:, c, :, :] - hostMean[0, c, 0, 0])) * \\\n\t\t\t\t\t\t\t\t  hostScale[0, c, 0, 0] * -0.5 * hostInvVar[0, c, 0, 0]**3\n\n\t\thostInGrad[:, c, :, :] = hostGrad[:, c, :, :] * hostScale[0, c, 0, 0] * hostInvVar[0, c, 0, 0] + \\\n\t\t\t\t\t\t\t\t hostVarGrad[0, c, 0, 0] * 2.0 / (batchsize * w * h) * (\n\t\t\t\t\t\t\t\t hostData[:, c, :, :] - hostMean[0, c, 0, 0]) + \\\n\t\t\t\t\t\t\t\t hostMeanGrad[0, c, 0, 0] / (batchsize * w * h)\n\n\tassert np.allclose(hostInGrad, bn.grad.get())\n\tassert np.allclose(hostScaleGrad, bn.vars[""scale""].grad.get())\n\tassert np.allclose(hostBiasGrad, bn.vars[""bias""].grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/BatchNorm3D.py,20,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Modules.Module import ModuleError\nfrom PuzzleLib.Modules.BatchNormND import BatchNormND\n\n\nclass BatchNorm3D(BatchNormND):\n\tdef __init__(self, maps, epsilon=1e-5, initFactor=1.0, minFactor=0.1, sscale=0.01, affine=True, name=None,\n\t\t\t\t empty=False, inplace=False):\n\t\tsuper().__init__(3, maps, epsilon, initFactor, minFactor, sscale, affine, name, empty, inplace)\n\t\tself.registerBlueprint(locals())\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 5:\n\t\t\traise ModuleError(""Data must be 5d tensor"")\n\n\t\t_, maps, _, _, _ = shape\n\t\tif maps != self.maps:\n\t\t\traise ModuleError(""Data has %d maps (expected: %d)"" % (maps, self.maps))\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 5:\n\t\t\traise ModuleError(""Grad must be 5d tensor"")\n\n\t\t_, maps, _, _, _ = shape\n\t\tif maps != self.maps:\n\t\t\traise ModuleError(""Grad has %d maps (expected: %d)"" % (maps, self.maps))\n\n\ndef unittest():\n\tbatchsize, maps, d, h, w = 8, 5, 3, 4, 2\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, d, h, w).astype(np.float32))\n\n\tbn = BatchNorm3D(maps)\n\tbn(data)\n\n\thostData, hostScale, hostBias = data.get(), bn.scale.get(), bn.bias.get()\n\thostNormData, hostOutData = np.empty(hostData.shape, dtype=np.float32), np.empty(hostData.shape, dtype=np.float32)\n\thostMean, hostInvVar = np.zeros(hostScale.shape, dtype=np.float32), np.zeros(hostScale.shape, dtype=np.float32)\n\tfor c in range(maps):\n\t\tfor b in range(batchsize):\n\t\t\thostMean[0, c, 0, 0, 0] += np.sum(hostData[b, c])\n\t\thostMean[0, c, 0, 0, 0] /= (batchsize * w * h * d)\n\n\t\tfor b in range(batchsize):\n\t\t\thostInvVar[0, c, 0, 0, 0] += np.sum((hostData[b, c] - hostMean[0, c, 0, 0, 0])**2)\n\t\thostInvVar[0, c, 0, 0, 0] /= (batchsize * w * h * d)\n\n\t\thostInvVar[0, c, 0, 0, 0] = 1.0 / np.sqrt(hostInvVar[0, c, 0, 0, 0] + bn.epsilon)\n\t\thostNormData[:, c, :, :, :] = (hostData[:, c, :, :, :] - hostMean[0, c, 0, 0, 0]) * hostInvVar[0, c, 0, 0, 0]\n\t\thostOutData[:, c, :, :, :] = hostNormData[:, c, :, :, :] * hostScale[0, c, 0, 0, 0] + hostBias[0, c, 0, 0, 0]\n\n\tassert np.allclose(hostMean, bn.mean.get())\n\tassert np.allclose(hostInvVar, bn.saveinvvar.get())\n\tassert np.allclose(hostOutData, bn.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(batchsize, maps, d, h, w).astype(np.float32))\n\tbn.backward(grad)\n\n\thostGrad, hostInGrad = grad.get(), np.empty_like(hostData)\n\thostScaleGrad, hostBiasGrad = np.empty_like(hostScale), np.empty_like(hostBias)\n\thostMeanGrad, hostVarGrad = np.empty_like(hostMean), np.empty_like(hostInvVar)\n\tfor c in range(maps):\n\t\thostBiasGrad[0, c, 0, 0, 0] = np.sum(hostGrad[:, c, :, :, :])\n\t\thostScaleGrad[0, c, 0, 0, 0] = np.sum(hostGrad[:, c, :, :, :] * hostNormData[:, c, :, :, :])\n\n\t\thostMeanGrad[0, c, 0, 0, 0] = np.sum(hostGrad[:,c,:,:,:]) * hostScale[0,c,0,0,0] * -hostInvVar[0,c,0,0,0]\n\t\thostVarGrad[0, c, 0, 0, 0] = np.sum(hostGrad[:,c,:,:,:] * (hostData[:,c,:,:,:] - hostMean[0,c,0,0,0])) * \\\n\t\t\t\t\t\t\t\t\t hostScale[0, c, 0, 0, 0] * -0.5 * hostInvVar[0, c, 0, 0, 0]**3\n\n\t\thostInGrad[:, c, :, :, :] = hostGrad[:,c,:,:,:] * hostScale[0,c,0,0,0] * hostInvVar[0,c,0,0,0] + \\\n\t\t\t\t\t\t\t\t\thostVarGrad[0, c, 0, 0, 0] * 2.0 / (batchsize * w * h * d) * \\\n\t\t\t\t\t\t\t\t\t(hostData[:, c, :, :, :] - hostMean[0, c, 0, 0, 0]) + \\\n\t\t\t\t\t\t\t\t\thostMeanGrad[0, c, 0, 0, 0] / (batchsize * w * h * d)\n\n\tassert np.allclose(hostInGrad, bn.grad.get())\n\tassert np.allclose(hostScaleGrad, bn.vars[""scale""].grad.get())\n\tassert np.allclose(hostBiasGrad, bn.vars[""bias""].grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/BatchNormND.py,4,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray, Blas\nfrom PuzzleLib.Backend.Dnn.Basic import batchNormNd, batchNormNdBackward\n\nfrom PuzzleLib.Variable import Variable\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass BatchNormND(Module):\n\tdef __init__(self, nd, maps, epsilon=1e-5, initFactor=1.0, minFactor=0.1, sscale=0.01, affine=True, name=None,\n\t\t\t\t empty=False, inplace=False):\n\t\tsuper().__init__(name)\n\n\t\tself.inplace = inplace\n\t\tif inplace and Config.showWarnings:\n\t\t\tprint(""[%s] Warning: %s is using inplace flag"" % (Config.libname, self))\n\n\t\tself.maps = maps\n\t\tself.epsilon = epsilon\n\t\tself.initFactor = initFactor\n\t\tself.minFactor = minFactor\n\t\tself.numOfProps = 0\n\n\t\tself.affine = affine\n\n\t\tself.scale, self.bias, self.mean, self.var = None, None, None, None\n\t\tself.savemean, self.saveinvvar, self.scalegrad, self.biasgrad = None, None, None, None\n\n\t\tif empty:\n\t\t\treturn\n\n\t\tshape = (1, maps) + self.repeat(1, nd)\n\n\t\tscale = np.random.normal(1.0, sscale if affine else 0.0, shape).astype(self.calctype)\n\t\tvar = np.ones(shape, dtype=self.calctype)\n\n\t\tself.setVar(""scale"", Variable(gpuarray.to_gpu(scale)))\n\t\tself.setVar(""bias"", Variable(gpuarray.zeros(shape, dtype=self.calctype)))\n\n\t\tself.setAttr(""mean"", gpuarray.zeros(shape, dtype=self.calctype))\n\t\tself.setAttr(""var"", gpuarray.to_gpu(var))\n\n\n\tdef updateData(self, data):\n\t\tif self.train:\n\t\t\tif self.inplace:\n\t\t\t\traise ModuleError(""%s: using inplace flag in train mode is prohibited"" % self)\n\n\t\t\tself.numOfProps += 1\n\t\t\tfactor = max(self.initFactor / self.numOfProps, self.minFactor)\n\n\t\t\tself.data, self.savemean, self.saveinvvar = batchNormNd(\n\t\t\t\tdata, self.scale, self.bias, self.mean, self.var, self.epsilon, factor, False\n\t\t\t)\n\t\telse:\n\t\t\tself.data = batchNormNd(\n\t\t\t\tdata, self.scale, self.bias, self.mean, self.var, self.epsilon, 0, True,\n\t\t\t\tout=data if self.inplace else None\n\t\t\t)\n\n\n\tdef updateGrad(self, grad):\n\t\ttup = batchNormNdBackward(self.inData, grad, self.scale, self.savemean, self.saveinvvar, self.epsilon)\n\n\t\tif self.affine:\n\t\t\tself.grad, self.scalegrad, self.biasgrad = tup\n\t\telse:\n\t\t\tself.grad, _, _ = tup\n\n\n\tdef accGradParams(self, grad, scale=1.0, momentum=0.0):\n\t\tif self.affine:\n\t\t\tBlas.addVectorToVector(\n\t\t\t\tself.scalegrad.ravel(), self.vars[""scale""].grad.ravel(), out=self.vars[""scale""].grad.ravel(),\n\t\t\t\talpha=scale, beta=momentum\n\t\t\t)\n\t\t\tBlas.addVectorToVector(\n\t\t\t\tself.biasgrad.ravel(), self.vars[""bias""].grad.ravel(), out=self.vars[""bias""].grad.ravel(),\n\t\t\t\talpha=scale, beta=momentum\n\t\t\t)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\t\tself.savemean, self.saveinvvar = None, None\n\n\t\tif self.affine:\n\t\t\tself.scalegrad, self.biasgrad = None, None\n\n\n\tdef calcMode(self, T):\n\t\tif Config.backend == Config.Backend.cuda:\n\t\t\tif T not in {np.float16, np.float32}:\n\t\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\telif T != np.float32:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n'"
Modules/Cast.py,7,"b'from enum import Enum\n\nimport numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\nfrom PuzzleLib.Backend.Kernels.ElementWise import castFP16toFP32, castFP32toFP16\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass DataType(str, Enum):\n\tfloat32 = ""float32""\n\tfloat16 = ""float16""\n\n\nclass Cast(Module):\n\tdef __init__(self, intype, outtype, name=None):\n\t\tsuper().__init__(name)\n\n\t\tintype, outtype = self.dataTypeToNumpy(intype), self.dataTypeToNumpy(outtype)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.intype, self.outtype = intype, outtype\n\n\t\tself.dataKer = self.getCastKernel(intype, outtype)\n\t\tself.gradKer = self.getCastKernel(outtype, intype)\n\n\n\tdef updateData(self, data):\n\t\tif self.intype != self.outtype:\n\t\t\tself.data = gpuarray.empty(data.shape, dtype=self.outtype, allocator=memPool)\n\t\t\tself.dataKer(self.data, data)\n\n\t\telse:\n\t\t\tself.data = data\n\n\n\tdef updateGrad(self, grad):\n\t\tif self.intype != self.outtype:\n\t\t\tself.grad = gpuarray.empty(grad.shape, dtype=self.intype, allocator=memPool)\n\t\t\tself.gradKer(self.grad, grad)\n\n\t\telse:\n\t\t\tself.grad = grad\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef checkDataType(self, dtype):\n\t\tif dtype != self.intype:\n\t\t\traise ModuleError(""Expected dtype %s, got %s"" % (self.intype, dtype))\n\n\n\tdef checkGradType(self, dtype):\n\t\tif dtype != self.outtype:\n\t\t\traise ModuleError(""Expected dtype %s, got %s"" % (self.outtype, dtype))\n\n\n\t@staticmethod\n\tdef getCastKernel(intype, outtype):\n\t\treturn {\n\t\t\t(DataType.float16, DataType.float32): castFP16toFP32,\n\t\t\t(DataType.float32, DataType.float16): castFP32toFP16\n\t\t}.get((intype, outtype), None)\n\n\n\t@staticmethod\n\tdef dataTypeToNumpy(T):\n\t\treturn T if isinstance(T, DataType) else {\n\t\t\tnp.float32: DataType.float32,\n\t\t\tnp.float16: DataType.float16\n\t\t}[np.dtype(T).type]\n\n\ndef unittest():\n\tbatchsize, size, maps = 5, 4, 8\n\n\thostData = np.random.randn(batchsize, size, maps).astype(np.float16)\n\thostGrad = np.random.randn(batchsize, size, maps).astype(np.float32)\n\n\tdata, grad = gpuarray.to_gpu(hostData), gpuarray.to_gpu(hostGrad)\n\n\tcast = Cast(data.dtype, grad.dtype)\n\n\tcast(data)\n\tassert np.allclose(hostData.astype(grad.dtype), cast.data.get())\n\n\tcast.backward(grad)\n\tassert np.allclose(hostGrad.astype(data.dtype), cast.grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Concat.py,10,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray, Utils\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass Concat(Module):\n\tdef __init__(self, axis, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.axis = axis\n\t\tself.sections = None\n\n\n\tdef updateData(self, data):\n\t\tself.sections = [d.shape[self.axis] for d in data]\n\t\tself.data = Utils.concatenate(data, axis=self.axis)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = Utils.split(grad, self.sections, axis=self.axis)\n\n\n\tdef checkDataShape(self, shapes):\n\t\tfor i, shape in enumerate(shapes[1:]):\n\t\t\tif not shape[:self.axis] + shape[self.axis + 1:] == shapes[0][:self.axis] + shapes[0][self.axis + 1:]:\n\t\t\t\traise ModuleError(\n\t\t\t\t\t""Shape %d is inconsistent with initial shape (checking %s, init is %s)"" % (i, shape, shapes[0])\n\t\t\t\t)\n\n\n\tdef dataShapeFrom(self, shapes):\n\t\tconcatDim = 0\n\t\tfor shape in shapes:\n\t\t\tconcatDim += shape[self.axis]\n\n\t\tshape = shapes[0][:self.axis] + (concatDim, ) + shapes[0][self.axis + 1:]\n\t\treturn shape\n\n\n\tdef checkGradShape(self, shape):\n\t\tconcatDim = 0\n\t\tfor sec in self.sections:\n\t\t\tconcatDim += sec\n\n\t\tgradShape = self.data.shape[:self.axis] + (concatDim, ) + self.data.shape[self.axis+1:]\n\t\tif gradShape != shape:\n\t\t\traise ModuleError(""Expected grad shape %s (given %s)"" % (gradShape, shape))\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tshapes = []\n\t\tfor sec in self.sections:\n\t\t\tshapes.append(shape[:self.axis] + (sec, ) + shape[self.axis + 1:])\n\n\t\treturn shapes\n\n\n\tdef calcMode(self, T):\n\t\tdtypes = {dtype for dtype, _ in Utils.dtypesSupported()}\n\n\t\tif T not in dtypes:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n\n\ndef unittest():\n\talongBatchAxis()\n\talongDataAxis()\n\n\ndef alongBatchAxis():\n\tdata = []\n\tfor _ in range(3):\n\t\tdata.append(gpuarray.to_gpu(np.random.randn(np.random.randint(low=5, high=10), 10, 5, 3).astype(np.float32)))\n\n\tconcat = Concat(axis=0)\n\tconcat(data)\n\n\thostOutData = np.concatenate([d.get() for d in data], axis=0)\n\tassert np.allclose(hostOutData, concat.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*hostOutData.shape).astype(np.float32))\n\tconcat.backward(grad)\n\n\tstride = 0\n\thostInGrad = []\n\tfor i in range(len(data)):\n\t\thostInGrad.append(grad.get()[stride:stride + data[i].shape[0], :])\n\t\tstride += data[i].shape[0]\n\n\tassert all([np.allclose(hostInGrad[i], concat.grad[i].get()) for i in range(len(data))])\n\n\ndef alongDataAxis():\n\tdata = []\n\tfor _ in range(3):\n\t\tdata.append(gpuarray.to_gpu(np.random.randn(10, np.random.randint(low=4, high=8), 4, 5).astype(np.float32)))\n\n\tconcat = Concat(axis=1)\n\tconcat(data)\n\n\thostOutData = np.concatenate([d.get() for d in data], axis=1)\n\tassert np.allclose(hostOutData, concat.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*hostOutData.shape).astype(np.float32))\n\tconcat.backward(grad)\n\n\tstride = 0\n\thostInGrad = []\n\tfor i in range(len(data)):\n\t\thostInGrad.append(grad.get()[:, stride:stride + data[i].shape[1]])\n\t\tstride += data[i].shape[1]\n\n\tassert all([np.allclose(hostInGrad[i], concat.grad[i].get()) for i in range(len(data))])\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Conv1D.py,14,"b'import numpy as np\n\nfrom PuzzleLib import Config\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Modules.Module import ModuleError\nfrom PuzzleLib.Modules.ConvND import ConvND\n\n\nclass Conv1D(ConvND):\n\tdef __init__(self, inmaps, outmaps, size, stride=1, pad=0, dilation=1, wscale=1.0, useBias=True, name=None,\n\t\t\t\t initscheme=None, empty=False, groups=1):\n\t\tsuper().__init__(\n\t\t\t2, inmaps, outmaps, (1, size), (1, stride), (0, pad), (1, dilation), wscale, useBias, name, initscheme,\n\t\t\tempty, groups\n\t\t)\n\t\tself.registerBlueprint(locals())\n\n\n\tdef optimizeForShape(self, shape, memlimit=None):\n\t\tshape = shape[:2] + (1, ) + shape[2:]\n\t\tsuper().optimizeForShape(shape, memlimit)\n\n\n\tdef updateData(self, data):\n\t\tdata = data.reshape(*data.shape[:2], 1, *data.shape[2:])\n\t\tsuper().updateData(data)\n\t\tself.data = self.data.reshape(*self.data.shape[:2], *self.data.shape[3:])\n\n\n\tdef updateGrad(self, grad):\n\t\tgrad = grad.reshape(*grad.shape[:2], 1, *grad.shape[2:])\n\n\t\tdata = self.inData\n\t\tself.inData = data.reshape(*data.shape[:2], 1, *data.shape[2:])\n\t\tsuper().updateGrad(grad)\n\t\tself.inData = data\n\n\t\tself.grad = self.grad.reshape(*self.grad.shape[:2], *self.grad.shape[3:])\n\n\n\tdef accGradParams(self, grad, scale=1.0, momentum=0.0):\n\t\tgrad = grad.reshape(*grad.shape[:2], 1, *grad.shape[2:])\n\n\t\tdata = self.inData\n\t\tself.inData = data.reshape(*data.shape[:2], 1, *data.shape[2:])\n\t\tsuper().accGradParams(grad, scale, momentum)\n\t\tself.inData = data\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 3:\n\t\t\traise ModuleError(""Data must be 3d tensor"")\n\n\t\t_, inmaps, size = shape\n\n\t\tif inmaps != self.W.shape[1] * self.groups:\n\t\t\traise ModuleError(""Data has %d maps (expected: %d)"" % (inmaps, self.W.shape[1] * self.groups))\n\n\t\tif size + 2 * self.pad[1] < self.dilation[1] * (self.W.shape[3] - 1) + 1:\n\t\t\traise ModuleError(\n\t\t\t\t""Data maps size is too small (got %d, expected at least %d)"" %\n\t\t\t\t(size + 2 * self.pad[1], self.dilation[1] * (self.W.shape[3] - 1) + 1)\n\t\t\t)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tbatchsize, inmaps, insize = shape\n\t\toutmaps, _, _, fsize = self.W.shape\n\n\t\t_, pad = self.pad\n\t\t_, dilation = self.dilation\n\t\t_, stride = self.stride\n\n\t\toutsize = (insize + 2 * pad - dilation * (fsize - 1) - 1) // stride + 1\n\n\t\treturn batchsize, outmaps, outsize\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 3:\n\t\t\traise ModuleError(""Grad must be 3d tensor"")\n\n\t\t_, outmaps, _ = shape\n\t\tif outmaps != self.W.shape[0]:\n\t\t\traise ModuleError(""Grad has %d maps (expected: %d)"" % (outmaps, self.W.shape[0]))\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tbatchsize, outmaps, outsize = shape\n\t\t_, inmaps, _, fsize = self.W.shape\n\n\t\t_, pad = self.pad\n\t\t_, dilation = self.dilation\n\t\t_, stride = self.stride\n\n\t\tinmaps *= self.groups\n\t\tinsize = (outsize - 1) * stride + dilation * (fsize - 1) - 2 * pad + 1\n\n\t\treturn batchsize, inmaps, insize\n\n\ndef unittest():\n\tif Config.backend in {Config.Backend.cuda, Config.Backend.hip}:\n\t\tmultiMapsWithPadsTest()\n\n\ttrainTest()\n\n\ndef multiMapsWithPadsTest():\n\tbatchsize, inmaps, size = 5, 4, 3\n\toutmaps, fsize, stride, pad, dilation = 4, 2, 2, 2, 2\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, inmaps, size).astype(np.float32))\n\n\tconv = Conv1D(inmaps, outmaps, size=fsize, stride=stride, pad=pad, dilation=dilation, initscheme=""gaussian"")\n\tconv(data)\n\n\thostW, hostBias = conv.W.get(), conv.b.get()\n\n\thostData = np.zeros(shape=(batchsize, inmaps, size + 2 * pad))\n\thostData[:, :, pad:-pad] = data.get()\n\n\thostOutData = np.empty(conv.data.shape, dtype=np.float32)\n\tfor c in range(outmaps):\n\t\thostOutData[:, c, :] = hostBias[0, c, 0, 0]\n\n\tfor b in range(batchsize):\n\t\tfor oc in range(outmaps):\n\t\t\tfor ic in range(inmaps):\n\t\t\t\tfor x in range(conv.data.shape[2]):\n\t\t\t\t\tfor dx in range(fsize):\n\t\t\t\t\t\thostOutData[b, oc, x] += hostData[b, ic, x * stride + dx * dilation] * hostW[oc, ic, 0, dx]\n\n\tassert np.allclose(hostOutData, conv.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*conv.data.shape).astype(np.float32))\n\tconv.backward(grad)\n\n\thostGrad, hostInGrad = grad.get(), np.zeros(hostData.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor ic in range(inmaps):\n\t\t\tfor oc in range(outmaps):\n\t\t\t\tfor x in range(hostGrad.shape[2]):\n\t\t\t\t\tfor dx in range(fsize):\n\t\t\t\t\t\thostInGrad[b, ic, x * stride + dx * dilation] += hostW[oc, ic, 0, dx] * hostGrad[b, oc, x]\n\n\tassert np.allclose(hostInGrad[:, :, pad:-pad], conv.grad.get())\n\n\thostWGrad = np.zeros(conv.getVar(""W"").grad.shape, dtype=np.float32)\n\tfor b in range(batchsize):\n\t\tfor oc in range(outmaps):\n\t\t\tfor ic in range(inmaps):\n\t\t\t\tfor dx in range(fsize):\n\t\t\t\t\tfor x in range(hostGrad.shape[2]):\n\t\t\t\t\t\thostWGrad[oc, ic, 0, dx] += hostData[b, ic, x * stride + dx * dilation] * hostGrad[b, oc, x]\n\n\tassert np.allclose(hostWGrad, conv.getVar(""W"").grad.get())\n\n\thostBGrad = np.empty(hostBias.shape, dtype=np.float32)\n\tfor oc in range(outmaps):\n\t\thostBGrad[0, oc, 0, 0] = np.sum(hostGrad[:, oc, :])\n\n\tassert np.allclose(hostBGrad, conv.getVar(""b"").grad.get())\n\n\ndef trainTest():\n\tbatchsize, inmaps, size = 5, 1, 3\n\toutmaps = 1\n\tfsize = 3\n\n\tdata = gpuarray.to_gpu(np.random.normal(0.0, 1.0, (batchsize, inmaps, size)).astype(np.float32))\n\tconv = Conv1D(inmaps, outmaps, fsize)\n\n\tfrom PuzzleLib.Cost.MSE import MSE\n\tmse = MSE()\n\n\ttarget = gpuarray.to_gpu(np.random.normal(0.0, 1.0, (batchsize, outmaps, 1)).astype(np.float32))\n\n\tfor i in range(100):\n\t\tlearnRate = 1e-1\n\n\t\tconv(data)\n\t\terror, grad = mse(conv.data, target)\n\n\t\tconv.backward(grad)\n\t\tconv.updateParams(learnRate)\n\n\t\tif (i + 1) % 5 == 0:\n\t\t\tprint(""Iteration #%d error: %s"" % (i + 1, error))\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Conv2D.py,37,"b'import numpy as np\n\nfrom PuzzleLib import Config\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Modules.Module import ModuleError\nfrom PuzzleLib.Modules.ConvND import ConvND\n\n\nclass Conv2D(ConvND):\n\tdef __init__(self, inmaps, outmaps, size, stride=1, pad=0, dilation=1, wscale=1.0, useBias=True, name=None,\n\t\t\t\t initscheme=None, empty=False, groups=1):\n\t\tsuper().__init__(\n\t\t\t2, inmaps, outmaps, size, stride, pad, dilation, wscale, useBias, name, initscheme, empty, groups\n\t\t)\n\t\tself.registerBlueprint(locals())\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 4:\n\t\t\traise ModuleError(""Data must be 4d tensor"")\n\n\t\t_, inmaps, inh, inw = shape\n\t\t_, _, fh, fw = self.W.shape\n\n\t\thpad, wpad = self.pad\n\t\thdilation, wdilation = self.dilation\n\n\t\tif inmaps != self.W.shape[1] * self.groups:\n\t\t\traise ModuleError(""Data has %d maps (expected: %d)"" % (inmaps, self.W.shape[1] * self.groups))\n\n\t\tif inh + 2 * hpad < hdilation * (fh - 1) + 1:\n\t\t\traise ModuleError(\n\t\t\t\t""Data maps height is too small (got %d, expected at least %d)"" %\n\t\t\t\t(inh + 2 * hpad, hdilation * (fh - 1) + 1)\n\t\t\t)\n\n\t\tif inw + 2 * wpad < wdilation * (fw - 1) + 1:\n\t\t\traise ModuleError(\n\t\t\t\t""Data maps width is too small (got %d, expected at least %d)"" %\n\t\t\t\t(inw + 2 * wpad, wdilation * (fw - 1) + 1)\n\t\t\t)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tbatchsize, inmaps, inh, inw = shape\n\t\toutmaps, _, fh, fw = self.W.shape\n\n\t\thpad, wpad = self.pad\n\t\thdilation, wdilation = self.dilation\n\t\thstride, wstride = self.stride\n\n\t\touth = (inh + 2 * hpad - hdilation * (fh - 1) - 1) // hstride + 1\n\t\toutw = (inw + 2 * wpad - wdilation * (fw - 1) - 1) // wstride + 1\n\n\t\treturn batchsize, outmaps, outh, outw\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 4:\n\t\t\traise ModuleError(""Grad must be 4d tensor"")\n\n\t\t_, outmaps, _, _ = shape\n\t\tif outmaps != self.W.shape[0]:\n\t\t\traise ModuleError(""Grad has %d maps (expected: %d)"" % (outmaps, self.W.shape[0]))\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tbatchsize, outmaps, outh, outw = shape\n\t\t_, inmaps, fh, fw = self.W.shape\n\n\t\thpad, wpad = self.pad\n\t\thdilation, wdilation = self.dilation\n\t\thstride, wstride = self.stride\n\n\t\tinmaps *= self.groups\n\t\tinh = (outh - 1) * hstride + hdilation * (fh - 1) - 2 * hpad + 1\n\t\tinw = (outw - 1) * wstride + wdilation * (fw - 1) - 2 * wpad + 1\n\n\t\treturn batchsize, inmaps, inh, inw\n\n\ndef unittest():\n\toneMapTest()\n\tmultiOutMapsTest()\n\tmultiInMapsTest()\n\n\tif Config.backend in {Config.Backend.cuda, Config.Backend.hip}:\n\t\tmultiMapsWithPadsTest()\n\t\tgroupTest()\n\n\ttrainTest()\n\n\ndef oneMapTest():\n\tbatchsize, inmaps, h, w = 1, 1, 5, 5\n\toutmaps, size = 1, 2\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, inmaps, h, w).astype(np.float32))\n\n\tconv = Conv2D(inmaps, outmaps, size)\n\tconv(data)\n\n\thostData, hostW, hostBias = data.get(), conv.W.get(), conv.b.get()\n\thostOutData = np.empty(conv.data.shape, dtype=np.float32)\n\n\thostOutData[:, 0, :, :] = hostBias[0, 0, 0, 0]\n\n\tfor y in range(conv.data.shape[2]):\n\t\tfor x in range(conv.data.shape[3]):\n\t\t\tfor dy in range(size):\n\t\t\t\tfor dx in range(size):\n\t\t\t\t\thostOutData[0, 0, y, x] += hostData[0, 0, y + dy, x + dx] * hostW[0, 0, dy, dx]\n\n\tassert np.allclose(hostOutData, conv.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*conv.data.shape).astype(np.float32))\n\tconv.backward(grad)\n\n\thostInGrad, hostGrad = np.zeros(data.shape).astype(np.float32), grad.get()\n\n\tfor y in range(hostGrad.shape[2]):\n\t\tfor x in range(hostGrad.shape[3]):\n\t\t\tfor dy in range(size):\n\t\t\t\tfor dx in range(size):\n\t\t\t\t\thostInGrad[0, 0, y + dy, x + dx] += hostW[0, 0, dy, dx] * hostGrad[0, 0, y, x]\n\n\tassert np.allclose(hostInGrad, conv.grad.get())\n\n\ndef multiOutMapsTest():\n\tbatchsize, inmaps, h, w = 1, 1, 8, 8\n\toutmaps, size = 2, 4\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, inmaps, h, w).astype(np.float32))\n\n\tconv = Conv2D(inmaps, outmaps, size)\n\tconv(data)\n\n\thostData, hostW, hostBias = data.get(), conv.W.get(), conv.b.get()\n\thostOutData = np.empty(conv.data.shape, dtype=np.float32)\n\n\tfor c in range(outmaps):\n\t\thostOutData[:, c, :, :] = hostBias[0, c, 0, 0]\n\n\tfor oc in range(outmaps):\n\t\tfor y in range(conv.data.shape[2]):\n\t\t\tfor x in range(conv.data.shape[3]):\n\t\t\t\tfor dy in range(size):\n\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\thostOutData[0, oc, y, x] += hostData[0, 0, y + dy, x + dx] * hostW[oc, 0, dy, dx]\n\n\tassert np.allclose(hostOutData, conv.data.get())\n\n\ndef multiInMapsTest():\n\tbatchsize, inmaps, h, w = 1, 2, 10, 10\n\toutmaps, size = 1, 4\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, inmaps, h, w).astype(np.float32))\n\n\tconv = Conv2D(inmaps, outmaps, size)\n\tconv(data)\n\n\thostData, hostW, hostBias = data.get(), conv.W.get(), conv.b.get()\n\thostOutData = np.empty(conv.data.shape, dtype=np.float32)\n\n\tfor c in range(outmaps):\n\t\thostOutData[:, c, :, :] = hostBias[0, c, 0, 0]\n\n\tfor ic in range(inmaps):\n\t\tfor y in range(conv.data.shape[2]):\n\t\t\tfor x in range(conv.data.shape[3]):\n\t\t\t\tfor dy in range(size):\n\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\thostOutData[0, 0, y, x] += hostData[0, ic, y + dy, x + dx] * hostW[0, ic, dy, dx]\n\n\tassert np.allclose(hostOutData, conv.data.get())\n\n\ndef multiMapsWithPadsTest():\n\tbatchsize, inmaps, h, w = 3, 4, 3, 3\n\toutmaps, size, stride, pad, dilation = 4, 3, 2, 2, 2\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, inmaps, h, w).astype(np.float32))\n\n\tconv = Conv2D(inmaps, outmaps, size=size, stride=stride, pad=pad, dilation=dilation, initscheme=""gaussian"")\n\tconv(data)\n\n\thostW, hostBias = conv.W.get(), conv.b.get()\n\tdl = dilation\n\n\thostData = np.zeros(shape=(batchsize, inmaps, h + 2 * pad, w + 2 * pad))\n\thostData[:, :, pad:-pad, pad:-pad] = data.get()\n\n\thostOutData = np.empty(conv.data.shape, dtype=np.float32)\n\tfor c in range(outmaps):\n\t\thostOutData[:, c, :, :] = hostBias[0, c, 0, 0]\n\n\tfor b in range(batchsize):\n\t\tfor oc in range(outmaps):\n\t\t\tfor ic in range(inmaps):\n\t\t\t\tfor y in range(conv.data.shape[2]):\n\t\t\t\t\tfor x in range(conv.data.shape[3]):\n\t\t\t\t\t\tfor dy in range(size):\n\t\t\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\t\t\thostOutData[b,oc,y,x] += hostData[b,ic,y*stride+dy*dl,x*stride+dx*dl]*hostW[oc,ic,dy,dx]\n\n\tassert np.allclose(hostOutData, conv.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*conv.data.shape).astype(np.float32))\n\tconv.backward(grad)\n\n\thostGrad, hostInGrad = grad.get(), np.zeros(hostData.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor ic in range(inmaps):\n\t\t\tfor oc in range(outmaps):\n\t\t\t\tfor y in range(hostGrad.shape[2]):\n\t\t\t\t\tfor x in range(hostGrad.shape[3]):\n\t\t\t\t\t\tfor dy in range(size):\n\t\t\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\t\t\thostInGrad[b,ic,y*stride+dy*dl,x*stride+dx*dl] += hostW[oc,ic,dy,dx]*hostGrad[b,oc,y,x]\n\n\tassert np.allclose(hostInGrad[:, :, pad:-pad, pad:-pad], conv.grad.get())\n\n\thostWGrad = np.zeros(conv.getVar(""W"").grad.shape, dtype=np.float32)\n\tfor b in range(batchsize):\n\t\tfor oc in range(outmaps):\n\t\t\tfor ic in range(inmaps):\n\t\t\t\tfor dy in range(size):\n\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\tfor y in range(hostGrad.shape[2]):\n\t\t\t\t\t\t\tfor x in range(hostGrad.shape[3]):\n\t\t\t\t\t\t\t\thostWGrad[oc,ic,dy,dx]+=hostData[b,ic,y*stride+dy*dl,x*stride+dx*dl]*hostGrad[b,oc,y,x]\n\n\tassert np.allclose(hostWGrad, conv.getVar(""W"").grad.get())\n\n\thostBGrad = np.empty(hostBias.shape, dtype=np.float32)\n\tfor oc in range(outmaps):\n\t\thostBGrad[0, oc, 0, 0] = np.sum(hostGrad[:, oc, :, :])\n\n\tassert np.allclose(hostBGrad, conv.getVar(""b"").grad.get())\n\n\ndef groupTest():\n\tbatchsize, inmaps, inh, inw = 3, 4, 4, 5\n\tsize, outmaps = 3, 4\n\tgroups = 2\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, inmaps, inh, inw).astype(np.float32))\n\n\tconv = Conv2D(inmaps, outmaps, size=size, initscheme=""gaussian"", groups=groups)\n\tconv(data)\n\n\thostData = data.get()\n\thostOutData = np.empty(conv.data.shape, dtype=np.float32)\n\n\thostW, hostBias = conv.W.get(), conv.b.get()\n\n\tfor c in range(outmaps):\n\t\thostOutData[:, c, :, :] = hostBias[0, c, 0, 0]\n\n\tingrpsize = inmaps // groups\n\toutgrpsize = outmaps // groups\n\n\tfor g in range(groups):\n\t\thostOutGroup = hostOutData[:, g * outgrpsize:(g + 1) * outgrpsize, :, :]\n\t\thostGroup = hostData[:, g * ingrpsize:(g + 1) * ingrpsize, :, :]\n\t\tfor b in range(batchsize):\n\t\t\tfor oc in range(outgrpsize):\n\t\t\t\tfor ic in range(ingrpsize):\n\t\t\t\t\tfor y in range(conv.data.shape[2]):\n\t\t\t\t\t\tfor x in range(conv.data.shape[3]):\n\t\t\t\t\t\t\tfor dy in range(size):\n\t\t\t\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\t\t\t\thostOutGroup[b, oc, y, x] += hostGroup[b, ic, y + dy, x + dx] * \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t hostW[g * outgrpsize + oc, ic, dy, dx]\n\n\tassert np.allclose(hostOutData, conv.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*conv.data.shape).astype(np.float32))\n\tconv.backward(grad)\n\n\thostGrad, hostInGrad = grad.get(), np.zeros(hostData.shape, dtype=np.float32)\n\n\tfor g in range(groups):\n\t\thostGroup = hostGrad[:, g * outgrpsize:(g + 1) * outgrpsize, :, :]\n\t\thostInGroup = hostInGrad[:, g * ingrpsize:(g + 1) * ingrpsize, :, :]\n\t\tfor b in range(batchsize):\n\t\t\tfor ic in range(ingrpsize):\n\t\t\t\tfor oc in range(outgrpsize):\n\t\t\t\t\tfor y in range(hostGrad.shape[2]):\n\t\t\t\t\t\tfor x in range(hostGrad.shape[3]):\n\t\t\t\t\t\t\tfor dy in range(size):\n\t\t\t\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\t\t\t\thostInGroup[b, ic, y + dy, x + dx] += hostW[g * outgrpsize + oc, ic, dy, dx] * \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  hostGroup[b, oc, y, x]\n\n\tassert np.allclose(hostInGrad, conv.grad.get())\n\n\thostWGrad = np.zeros(conv.getVar(""W"").grad.shape, dtype=np.float32)\n\tfor g in range(groups):\n\t\thostGradGroup = hostGrad[:, g * outgrpsize:(g + 1) * outgrpsize, :, :]\n\t\thostDataGroup = hostData[:, g * ingrpsize:(g + 1) * ingrpsize, :, :]\n\t\tfor b in range(batchsize):\n\t\t\tfor oc in range(outgrpsize):\n\t\t\t\tfor ic in range(ingrpsize):\n\t\t\t\t\tfor dy in range(size):\n\t\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\t\tfor y in range(hostGrad.shape[2]):\n\t\t\t\t\t\t\t\tfor x in range(hostGrad.shape[3]):\n\t\t\t\t\t\t\t\t\thostWGrad[g * outgrpsize + oc, ic, dy, dx] += hostDataGroup[b, ic, y+dy, x+dx] * \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  hostGradGroup[b, oc, y, x]\n\n\tassert np.allclose(hostWGrad, conv.getVar(""W"").grad.get())\n\n\thostBGrad = np.empty(hostBias.shape, dtype=np.float32)\n\tfor oc in range(outmaps):\n\t\thostBGrad[0, oc, 0, 0] = np.sum(hostGrad[:, oc, :, :])\n\n\tassert np.allclose(hostBGrad, conv.getVar(""b"").grad.get())\n\n\ndef trainTest():\n\tbatchsize, inmaps, h, w = 5, 1, 8, 8\n\toutmaps = 1\n\tsize = 8\n\n\tdata = gpuarray.to_gpu(np.random.normal(0.0, 1.0, (batchsize, inmaps, h, w)).astype(np.float32))\n\tconv = Conv2D(inmaps, outmaps, size)\n\n\tfrom PuzzleLib.Cost.MSE import MSE\n\tmse = MSE()\n\n\ttarget = gpuarray.to_gpu(np.random.normal(0.0, 1.0, (batchsize, outmaps, 1, 1)).astype(np.float32))\n\n\tfor i in range(100):\n\t\tlearnRate = 1e-2\n\n\t\tconv(data)\n\t\terror, grad = mse(conv.data, target)\n\n\t\tconv.backward(grad)\n\t\tconv.updateParams(learnRate)\n\n\t\tif (i + 1) % 5 == 0:\n\t\t\tprint(""Iteration #%d error: %s"" % (i + 1, error))\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Conv3D.py,14,"b'import numpy as np\n\nfrom PuzzleLib import Config\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Modules.Module import ModuleError\nfrom PuzzleLib.Modules.ConvND import ConvND\n\n\nclass Conv3D(ConvND):\n\tdef __init__(self, inmaps, outmaps, size, stride=1, pad=0, dilation=1, wscale=1.0, useBias=True, name=None,\n\t\t\t\t initscheme=None, empty=False, groups=1):\n\t\tsuper().__init__(\n\t\t\t3, inmaps, outmaps, size, stride, pad, dilation, wscale, useBias, name, initscheme, empty, groups\n\t\t)\n\t\tself.registerBlueprint(locals())\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 5:\n\t\t\traise ModuleError(""Data must be 5d tensor"")\n\n\t\t_, inmaps, ind, inh, inw = shape\n\t\t_, _, fd, fh, fw = self.W.shape\n\n\t\tdpad, hpad, wpad = self.pad\n\t\tddilation, hdilation, wdilation = self.dilation\n\n\t\tif inmaps != self.W.shape[1] * self.groups:\n\t\t\traise ModuleError(""Data has %d maps (expected: %d)"" % (inmaps, self.W.shape[1] * self.groups))\n\n\t\tif ind + 2 * dpad < ddilation * (fd - 1) + 1:\n\t\t\traise ModuleError(\n\t\t\t\t""Data maps depth is too small (got %d, expected >= %d)"" % (ind + 2 * dpad, ddilation * (fd - 1) + 1)\n\t\t\t)\n\n\t\tif inh + 2 * hpad < hdilation * (fh - 1) + 1:\n\t\t\traise ModuleError(\n\t\t\t\t""Data maps height is too small (got %d, expected >= %d)"" % (inh + 2 * hpad, hdilation * (fh - 1) + 1)\n\t\t\t)\n\n\t\tif inw + 2 * wpad < wdilation * (fw - 1) + 1:\n\t\t\traise ModuleError(\n\t\t\t\t""Data maps width is too small (got %d, expected >= %d)"" % (inw + 2 * wpad, wdilation * (fw - 1) + 1)\n\t\t\t)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tbatchsize, inmaps, ind, inh, inw = shape\n\t\toutmaps, _, fd, fh, fw = self.W.shape\n\n\t\tdpad, hpad, wpad = self.pad\n\t\tddilation, hdilation, wdilation = self.dilation\n\t\tdstride, hstride, wstride = self.stride\n\n\t\toutd = (ind + 2 * dpad - ddilation * (fd - 1) - 1) // dstride + 1\n\t\touth = (inh + 2 * hpad - hdilation * (fh - 1) - 1) // hstride + 1\n\t\toutw = (inw + 2 * wpad - wdilation * (fw - 1) - 1) // wstride + 1\n\n\t\treturn batchsize, outmaps, outd, outh, outw\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 5:\n\t\t\traise ModuleError(""Grad must be 5d tensor"")\n\n\t\t_, outmaps, _, _, _ = shape\n\t\tif outmaps != self.W.shape[0]:\n\t\t\traise ModuleError(""Grad has %d maps (expected: %d)"" % (outmaps, self.W.shape[0]))\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tbatchsize, outmaps, outd, outh, outw = shape\n\t\t_, inmaps, fd, fh, fw = self.W.shape\n\n\t\tdpad, hpad, wpad = self.pad\n\t\tddilation, hdilation, wdilation = self.dilation\n\t\tdstride, hstride, wstride = self.stride\n\n\t\tinmaps *= self.groups\n\t\tind = (outd - 1) * dstride + ddilation * (fd - 1) - 2 * dpad + 1\n\t\tinh = (outh - 1) * hstride + hdilation * (fh - 1) - 2 * hpad + 1\n\t\tinw = (outw - 1) * wstride + wdilation * (fw - 1) - 2 * wpad + 1\n\n\t\treturn batchsize, inmaps, ind, inh, inw\n\n\ndef unittest():\n\tif Config.backend in {Config.Backend.cuda, Config.Backend.hip}:\n\t\tmultiMapsWithPadsTest()\n\n\ttrainTest()\n\n\ndef multiMapsWithPadsTest():\n\tbatchsize, inmaps, d, h, w = 2, 4, 2, 3, 3\n\toutmaps, size, stride, pad, dilation = 4, 2, 2, 2, 2\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, inmaps, d, h, w).astype(np.float32))\n\n\tconv = Conv3D(inmaps, outmaps, size=size, stride=stride, pad=pad, dilation=dilation, initscheme=""gaussian"")\n\tconv(data)\n\n\thostW, hostBias = conv.W.get(), conv.b.get()\n\tdl = dilation\n\n\thostData = np.zeros(shape=(batchsize, inmaps, d + 2 * pad, h + 2 * pad, w + 2 * pad))\n\thostData[:, :, pad:-pad, pad:-pad, pad:-pad] = data.get()\n\n\thostOutData = np.empty(conv.data.shape, dtype=np.float32)\n\tfor c in range(outmaps):\n\t\thostOutData[:, c, :, :, :] = hostBias[0, c, 0, 0, 0]\n\n\tfor b in range(batchsize):\n\t\tfor oc in range(outmaps):\n\t\t\tfor ic in range(inmaps):\n\t\t\t\tfor z in range(conv.data.shape[2]):\n\t\t\t\t\tfor y in range(conv.data.shape[3]):\n\t\t\t\t\t\tfor x in range(conv.data.shape[4]):\n\t\t\t\t\t\t\tfor dz in range(size):\n\t\t\t\t\t\t\t\tfor dy in range(size):\n\t\t\t\t\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\t\t\t\t\thostOutData[b, oc, z, y, x] += \\\n\t\t\t\t\t\t\t\t\t\t\thostData[b, ic, z*stride + dz*dl, y*stride + dy*dl, x*stride + dx*dl] * \\\n\t\t\t\t\t\t\t\t\t\t\thostW[oc, ic, dz, dy, dx]\n\n\tassert np.allclose(hostOutData, conv.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*conv.data.shape).astype(np.float32))\n\tconv.backward(grad)\n\n\thostGrad, hostInGrad = grad.get(), np.zeros(hostData.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor ic in range(inmaps):\n\t\t\tfor oc in range(outmaps):\n\t\t\t\tfor z in range(hostGrad.shape[2]):\n\t\t\t\t\tfor y in range(hostGrad.shape[3]):\n\t\t\t\t\t\tfor x in range(hostGrad.shape[4]):\n\t\t\t\t\t\t\tfor dz in range(size):\n\t\t\t\t\t\t\t\tfor dy in range(size):\n\t\t\t\t\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\t\t\t\t\thostInGrad[b, ic, z*stride + dz*dl, y*stride + dy*dl, x*stride + dx*dl] += \\\n\t\t\t\t\t\t\t\t\t\t\thostW[oc, ic, dz, dy, dx] * hostGrad[b, oc, z, y, x]\n\n\tassert np.allclose(hostInGrad[:, :, pad:-pad, pad:-pad, pad:-pad], conv.grad.get())\n\n\thostWGrad = np.zeros(conv.getVar(""W"").grad.shape, dtype=np.float32)\n\tfor b in range(batchsize):\n\t\tfor oc in range(outmaps):\n\t\t\tfor ic in range(inmaps):\n\t\t\t\tfor dz in range(size):\n\t\t\t\t\tfor dy in range(size):\n\t\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\t\tfor z in range(hostGrad.shape[2]):\n\t\t\t\t\t\t\t\tfor y in range(hostGrad.shape[3]):\n\t\t\t\t\t\t\t\t\tfor x in range(hostGrad.shape[4]):\n\t\t\t\t\t\t\t\t\t\thostWGrad[oc, ic, dz, dy, dx] += \\\n\t\t\t\t\t\t\t\t\t\t\thostData[b, ic, z*stride + dz*dl, y*stride + dy*dl, x*stride + dx*dl] * \\\n\t\t\t\t\t\t\t\t\t\t\thostGrad[b, oc, z, y, x]\n\n\tassert np.allclose(hostWGrad, conv.getVar(""W"").grad.get())\n\n\thostBGrad = np.empty(hostBias.shape, dtype=np.float32)\n\tfor oc in range(outmaps):\n\t\thostBGrad[0, oc, 0, 0, 0] = np.sum(hostGrad[:, oc, :, :, :])\n\n\tassert np.allclose(hostBGrad, conv.getVar(""b"").grad.get())\n\n\ndef trainTest():\n\tbatchsize, inmaps, d, h, w = 5, 1, 3, 3, 3\n\toutmaps = 1\n\tsize = 3\n\n\tdata = gpuarray.to_gpu(np.random.normal(0.0, 1.0, (batchsize, inmaps, d, h, w)).astype(np.float32))\n\tconv = Conv3D(inmaps, outmaps, size)\n\n\tfrom PuzzleLib.Cost.MSE import MSE\n\tmse = MSE()\n\n\ttarget = gpuarray.to_gpu(np.random.normal(0.0, 1.0, (batchsize, outmaps, 1, 1, 1)).astype(np.float32))\n\n\tfor i in range(100):\n\t\tlearnRate = 1e-2\n\n\t\tconv(data)\n\t\terror, grad = mse(conv.data, target)\n\n\t\tconv.backward(grad)\n\t\tconv.updateParams(learnRate)\n\n\t\tif (i + 1) % 5 == 0:\n\t\t\tprint(""Iteration #%d error: %s"" % (i + 1, error))\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/ConvND.py,0,"b'import sys\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Dnn.Basic import ConvFwdAlgo, ConvBwdDataAlgo, ConvBwdFilterAlgo\nfrom PuzzleLib.Backend.Dnn.Basic import convNdbenchmark, convNd, convNdBackwardData, convNdBackwardParams\n\nfrom PuzzleLib.Variable import Variable\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass ConvND(Module):\n\tdef __init__(self, nd, inmaps, outmaps, size, stride=1, pad=0, dilation=1, wscale=1.0, useBias=True, name=None,\n\t\t\t\t initscheme=None, empty=False, groups=1):\n\t\tsuper().__init__(name)\n\n\t\tself.stride = self.repeat(stride, nd)\n\t\tself.pad = self.repeat(pad, nd)\n\t\tself.dilation = self.repeat(dilation, nd)\n\n\t\tself.useBias = useBias\n\t\tself.groups = groups\n\n\t\tself.fwdAlgo, self.bwdFilterAlgo, self.bwdDataAlgo = None, None, None\n\t\tself.installDefaultAlgos()\n\n\t\tif inmaps % groups != 0 or outmaps % groups != 0:\n\t\t\traise ModuleError(\n\t\t\t\t""Number of input and output maps must be divisible by number of groups ""\n\t\t\t\t""(%d inmaps, %d outmaps, %d groups)"" % (inmaps, outmaps, groups)\n\t\t\t)\n\n\t\tinmaps //= groups\n\n\t\tself.W = None\n\t\tself.b = None\n\n\t\tif empty:\n\t\t\treturn\n\n\t\tWshape = (outmaps, inmaps, *self.repeat(size, nd))\n\t\tW = self.createTensorWithScheme(initscheme, Wshape, wscale)\n\n\t\tself.setVar(""W"", Variable(gpuarray.empty(Wshape, dtype=self.calctype) if W is None else gpuarray.to_gpu(W)))\n\n\t\tif useBias:\n\t\t\tbshape = (1, outmaps) + self.repeat(1, nd)\n\t\t\tself.setVar(""b"", Variable(gpuarray.zeros(bshape, dtype=self.calctype)))\n\n\n\tdef optimizeForShape(self, shape, memlimit=None):\n\t\tfwdRes, bwdFilterRes, bwdDataRes = convNdbenchmark(\n\t\t\tshape, self.W.shape, self.stride, self.pad, self.dilation, self.groups, transpose=False\n\t\t)\n\n\t\tmemlimit = sys.maxsize if memlimit is None else memlimit\n\n\t\tself.fwdAlgo = next(ConvFwdAlgo(res.algo) for res in fwdRes if res.memory <= memlimit)\n\t\tself.bwdFilterAlgo = next(ConvBwdFilterAlgo(res.algo) for res in bwdFilterRes if res.memory <= memlimit)\n\t\tself.bwdDataAlgo = next(ConvBwdDataAlgo(res.algo) for res in bwdDataRes if res.memory <= memlimit)\n\n\n\tdef installDefaultAlgos(self):\n\t\tif Config.backend == Config.Backend.cuda:\n\t\t\tself.fwdAlgo = ConvFwdAlgo.implicitGemm\n\t\t\tself.bwdFilterAlgo = ConvBwdFilterAlgo.algo0\n\t\t\tself.bwdDataAlgo = ConvBwdDataAlgo.algo0\n\n\t\telif Config.backend in {Config.Backend.hip, Config.Backend.intel}:\n\t\t\tself.fwdAlgo = ConvFwdAlgo.auto\n\t\t\tself.bwdFilterAlgo = ConvBwdFilterAlgo.auto\n\t\t\tself.bwdDataAlgo = ConvBwdDataAlgo.auto\n\n\n\tdef updateData(self, data):\n\t\tself.data = convNd(\n\t\t\tdata, self.W, self.b, stride=self.stride, pad=self.pad, dilation=self.dilation,\n\t\t\tgroups=self.groups, algo=self.fwdAlgo\n\t\t)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = convNdBackwardData(\n\t\t\tgrad, self.W, data=self.inData, stride=self.stride, pad=self.pad, dilation=self.dilation,\n\t\t\tgroups=self.groups, algo=self.bwdDataAlgo\n\t\t)\n\n\n\tdef accGradParams(self, grad, scale=1.0, momentum=0.0):\n\t\tconvNdBackwardParams(\n\t\t\tself.inData, grad, self.W, self.b, stride=self.stride, pad=self.pad, dilation=self.dilation,\n\t\t\tgroups=self.groups, wgrad=self.vars[""W""].grad, bgrad=self.vars[""b""].grad if self.b is not None else None,\n\t\t\tscale=scale, momentum=momentum, algo=self.bwdFilterAlgo\n\t\t)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\traise NotImplementedError()\n\n\n\tdef gradShapeFrom(self, shape):\n\t\traise NotImplementedError()\n\n\n\tdef calcMode(self, T):\n\t\tif Config.backend in {Config.Backend.cuda, Config.Backend.hip}:\n\t\t\tif self.calctype == T:\n\t\t\t\treturn\n\n\t\t\tvariables = self.vars\n\t\t\tself.vars = {}\n\n\t\t\tfor varName, var in variables.items():\n\t\t\t\tself.setVar(varName, Variable(\n\t\t\t\t\tvar.data.astype(T), name=var.name, grad=var.grad.astype(T) if var.grad is not None else None\n\t\t\t\t))\n\n\t\t\tself.calctype = T\n\n\t\telse:\n\t\t\tsuper().calcMode(T)\n'"
Modules/CrossMapLRN.py,8,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Dnn.Basic import crossMapLRN, crossMapLRNBackward\n\nfrom PuzzleLib.Modules.LRN import LRN\n\n\nclass CrossMapLRN(LRN):\n\tdef __init__(self, N=5, alpha=1e-4, beta=0.75, K=2.0, name=None):\n\t\tsuper().__init__(N, alpha, beta, K, name)\n\t\tself.gradUsesOutData = True\n\n\n\tdef updateData(self, data):\n\t\tself.data, self.workspace = crossMapLRN(data, N=self.N, alpha=self.alpha, beta=self.beta, K=self.K,\n\t\t\t\t\t\t\t\t\t\t\t\ttest=not self.train)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = crossMapLRNBackward(self.inData, self.data, grad, self.workspace,\n\t\t\t\t\t\t\t\t\t\tN=self.N, alpha=self.alpha, beta=self.beta, K=self.K)\n\n\ndef unittest():\n\tmaps = 10\n\tdata = gpuarray.to_gpu(np.random.randn(1, maps, 1, 1).astype(np.float32))\n\n\tcrossMapLrn = CrossMapLRN()\n\tcrossMapLrn(data)\n\n\tlookBehind = int((crossMapLrn.N - 1) / 2)\n\tlookAhead = crossMapLrn.N - lookBehind\n\n\thostData = data.get().reshape(maps, ).astype(np.float32)\n\tnorms = np.empty((maps, ), dtype=np.float32)\n\tfor i in range(maps):\n\t\tnorm = 0.0\n\t\tfor j in range(max(0, i - lookBehind), min(maps, i + lookAhead)):\n\t\t\tnorm += hostData[j]**2\n\t\tnorms[i] = crossMapLrn.K + norm * crossMapLrn.alpha / crossMapLrn.N\n\n\thostOutData = hostData / norms**crossMapLrn.beta\n\tassert np.allclose(hostOutData, crossMapLrn.data.get().reshape(maps, ).astype(np.float32))\n\n\tgrad = gpuarray.to_gpu(np.random.randn(1, maps, 1, 1).astype(np.float32))\n\tcrossMapLrn.backward(grad)\n\n\thostGrad = grad.get().reshape(maps, ).astype(np.float32)\n\thostInGrad = np.zeros((maps, ), dtype=np.float32)\n\n\tk = 2.0 * crossMapLrn.alpha * crossMapLrn.beta / crossMapLrn.N\n\tfor i in range(maps):\n\t\thostInGrad[i] += hostGrad[i] / norms[i]**crossMapLrn.beta\n\n\t\tfor j in range(max(0, i - lookBehind), min(maps, i + lookAhead)):\n\t\t\thostInGrad[j] -= hostGrad[i] * k * hostData[i] * hostData[j] / norms[i]**(crossMapLrn.beta+1)\n\tassert np.allclose(hostInGrad, crossMapLrn.grad.get().reshape(maps, ).astype(np.float32))\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Deconv1D.py,14,"b'import numpy as np\n\nfrom PuzzleLib import Config\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Modules.Module import ModuleError\nfrom PuzzleLib.Modules.DeconvND import DeconvND\n\n\nclass Deconv1D(DeconvND):\n\tdef __init__(self, inmaps, outmaps, size, stride=1, pad=0, dilation=1, wscale=1.0, useBias=True, name=None,\n\t\t\t\t initscheme=None, empty=False, groups=1):\n\t\tsuper().__init__(\n\t\t\t2, inmaps, outmaps, (1, size), (1, stride), (0, pad), (1, dilation), wscale, useBias, name, initscheme,\n\t\t\tempty, groups\n\t\t)\n\t\tself.registerBlueprint(locals())\n\n\n\tdef optimizeForShape(self, shape, memlimit=None):\n\t\tshape = shape[:2] + (1, ) + shape[2:]\n\t\tsuper().optimizeForShape(shape, memlimit)\n\n\n\tdef updateData(self, data):\n\t\tdata = data.reshape(*data.shape[:2], 1, *data.shape[2:])\n\t\tsuper().updateData(data)\n\t\tself.data = self.data.reshape(*self.data.shape[:2], *self.data.shape[3:])\n\n\n\tdef updateGrad(self, grad):\n\t\tgrad = grad.reshape(*grad.shape[:2], 1, *grad.shape[2:])\n\n\t\tdata = self.inData\n\t\tself.inData = data.reshape(*data.shape[:2], 1, *data.shape[2:])\n\t\tsuper().updateGrad(grad)\n\t\tself.inData = data\n\n\t\tself.grad = self.grad.reshape(*self.grad.shape[:2], *self.grad.shape[3:])\n\n\n\tdef accGradParams(self, grad, scale=1.0, momentum=0.0):\n\t\tgrad = grad.reshape(*grad.shape[:2], 1, *grad.shape[2:])\n\n\t\tdata = self.inData\n\t\tself.inData = data.reshape(*data.shape[:2], 1, *data.shape[2:])\n\t\tsuper().accGradParams(grad, scale, momentum)\n\t\tself.inData = data\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 3:\n\t\t\traise ModuleError(""Data must be 3d tensor"")\n\n\t\t_, inmaps, _ = shape\n\t\tif inmaps != self.W.shape[0]:\n\t\t\traise ModuleError(""Data has %d maps (expected: %d)"" % (inmaps, self.W.shape[0]))\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tbatchsize, inmaps, insize = shape\n\t\t_, outmaps, _, fsize = self.W.shape\n\n\t\t_, pad = self.pad\n\t\t_, dilation = self.dilation\n\t\t_, stride = self.stride\n\n\t\toutmaps *= self.groups\n\t\toutsize = (insize - 1) * stride + dilation * (fsize - 1) - 2 * pad + 1\n\n\t\treturn batchsize, outmaps, outsize\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 3:\n\t\t\traise ModuleError(""Grad must be 3d tensor"")\n\n\t\t_, outmaps, size = shape\n\n\t\tif outmaps != self.W.shape[1] * self.groups:\n\t\t\traise ModuleError(""Grad has %d maps (expected: %d)"" % (outmaps, self.W.shape[1] * self.groups))\n\n\t\tif size + 2 * self.pad[1] < self.dilation[1] * (self.W.shape[3] - 1) + 1:\n\t\t\traise ModuleError(\n\t\t\t\t""Grad maps height is too small (got %d, expected at least %d)"" %\n\t\t\t\t(size + 2 * self.pad[1], self.dilation[1] * (self.W.shape[3] - 1) + 1)\n\t\t\t)\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tbatchsize, outmaps, outsize = shape\n\t\tinmaps, _, _, fsize = self.W.shape\n\n\t\t_, pad = self.pad\n\t\t_, dilation = self.dilation\n\t\t_, stride = self.stride\n\n\t\tinsize = (outsize + 2 * pad - dilation * (fsize - 1) - 1) // stride + 1\n\n\t\treturn batchsize, inmaps, insize\n\n\ndef unittest():\n\tif Config.backend in {Config.Backend.cuda, Config.Backend.hip}:\n\t\tmultiMapsWithPadsTest()\n\n\ttrainTest()\n\n\ndef multiMapsWithPadsTest():\n\tbatchsize, inmaps, size = 5, 4, 2\n\toutmaps, fsize, stride, pad, dilation = 4, 2, 2, 1, 2\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, inmaps, size).astype(np.float32))\n\n\tdeconv = Deconv1D(inmaps, outmaps, size=size, stride=stride, pad=pad, dilation=dilation, initscheme=""gaussian"")\n\tdeconv(data)\n\n\thostW, hostBias = deconv.W.get(), deconv.b.get()\n\n\thostData, hostOutData = data.get(), np.zeros(deconv.data.shape[:2]+(deconv.data.shape[2]+2*pad, ), dtype=np.float32)\n\n\tfor c in range(outmaps):\n\t\thostOutData[:, c, :] = hostBias[0, c, 0, 0]\n\n\tfor b in range(batchsize):\n\t\tfor oc in range(outmaps):\n\t\t\tfor ic in range(inmaps):\n\t\t\t\tfor x in range(size):\n\t\t\t\t\tfor dx in range(fsize):\n\t\t\t\t\t\thostOutData[b, oc, x * stride + dx * dilation] += hostW[ic, oc, 0, dx] * hostData[b, ic, x]\n\n\tassert np.allclose(hostOutData[:, :, pad:-pad], deconv.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*deconv.data.shape).astype(np.float32))\n\tdeconv.backward(grad)\n\n\thostGrad = np.zeros(grad.shape[:2] + (grad.shape[2] + 2 * pad, ), dtype=np.float32)\n\thostGrad[:, :, pad:-pad] = grad.get()\n\n\thostInGrad = np.zeros(hostData.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor ic in range(inmaps):\n\t\t\tfor oc in range(outmaps):\n\t\t\t\tfor x in range(size):\n\t\t\t\t\tfor dx in range(fsize):\n\t\t\t\t\t\thostInGrad[b, ic, x] += hostGrad[b, oc, x * stride + dx * dilation] * hostW[ic, oc, 0, dx]\n\n\tassert np.allclose(hostInGrad, deconv.grad.get())\n\n\thostWGrad = np.zeros(deconv.getVar(""W"").grad.shape, dtype=np.float32)\n\tfor b in range(batchsize):\n\t\tfor ic in range(inmaps):\n\t\t\tfor oc in range(outmaps):\n\t\t\t\tfor dx in range(fsize):\n\t\t\t\t\tfor x in range(size):\n\t\t\t\t\t\thostWGrad[ic, oc, 0, dx] += hostGrad[b, oc, x * stride + dx * dilation] * hostData[b, ic, x]\n\n\tassert np.allclose(hostWGrad, deconv.getVar(""W"").grad.get())\n\n\thostBGrad = np.empty(hostBias.shape, dtype=np.float32)\n\tfor oc in range(outmaps):\n\t\thostBGrad[0, oc, 0, 0] = np.sum(hostGrad[:, oc, :])\n\n\tassert np.allclose(hostBGrad, deconv.getVar(""b"").grad.get())\n\n\ndef trainTest():\n\tbatchsize, inmaps, size = 5, 5, 2\n\toutmaps = 1\n\tfsize = 3\n\n\tdata = gpuarray.to_gpu(np.random.normal(0.0, 1.0, (batchsize, inmaps, size)).astype(np.float32))\n\tdeconv = Deconv1D(inmaps, outmaps, fsize)\n\n\tfrom PuzzleLib.Cost.MSE import MSE\n\tmse = MSE()\n\n\ttarget = gpuarray.to_gpu(np.random.normal(0.0, 1.0, (batchsize, outmaps, 4)).astype(np.float32))\n\n\tfor i in range(100):\n\t\tlearnRate = 1e-2\n\n\t\tdeconv(data)\n\t\terror, grad = mse(deconv.data, target)\n\n\t\tdeconv.backward(grad)\n\t\tdeconv.updateParams(learnRate)\n\n\t\tif (i + 1) % 5 == 0:\n\t\t\tprint(""Iteration #%d error: %s"" % (i + 1, error))\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Deconv2D.py,21,"b'import numpy as np\n\nfrom PuzzleLib import Config\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Modules.Module import ModuleError\nfrom PuzzleLib.Modules.DeconvND import DeconvND\n\n\nclass Deconv2D(DeconvND):\n\tdef __init__(self, inmaps, outmaps, size, stride=1, pad=0, dilation=1, wscale=1.0, useBias=True, name=None,\n\t\t\t\t initscheme=None, empty=False, groups=1):\n\t\tsuper().__init__(\n\t\t\t2, inmaps, outmaps, size, stride, pad, dilation, wscale, useBias, name, initscheme, empty, groups\n\t\t)\n\t\tself.registerBlueprint(locals())\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 4:\n\t\t\traise ModuleError(""Data must be 4d tensor"")\n\n\t\t_, inmaps, _, _ = shape\n\t\tif inmaps != self.W.shape[0]:\n\t\t\traise ModuleError(""Data has %d maps (expected: %d)"" % (inmaps, self.W.shape[0]))\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tbatchsize, inmaps, inh, inw = shape\n\t\t_, outmaps, fh, fw = self.W.shape\n\n\t\thpad, wpad = self.pad\n\t\thdilation, wdilation = self.dilation\n\t\thstride, wstride = self.stride\n\n\t\toutmaps *= self.groups\n\t\touth = (inh - 1) * hstride + hdilation * (fh - 1) - 2 * hpad + 1\n\t\toutw = (inw - 1) * wstride + wdilation * (fw - 1) - 2 * wpad + 1\n\n\t\treturn batchsize, outmaps, outh, outw\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 4:\n\t\t\traise ModuleError(""Grad must be 4d tensor"")\n\n\t\t_, outmaps, outh, outw = shape\n\t\t_, _, fh, fw = self.W.shape\n\n\t\thpad, wpad = self.pad\n\t\thdilation, wdilation = self.dilation\n\n\t\tif outmaps != self.W.shape[1] * self.groups:\n\t\t\traise ModuleError(""Grad has %d maps (expected: %d)"" % (outmaps, self.W.shape[1] * self.groups))\n\n\t\tif outh + 2 * hpad < hdilation * (fh - 1) + 1:\n\t\t\traise ModuleError(\n\t\t\t\t""Grad maps height is too small (got %d, expected at least %d)"" %\n\t\t\t\t(outh + 2 * hpad, hdilation * (fh - 1) + 1)\n\t\t\t)\n\n\t\tif outw + 2 * wpad < wdilation * (fw - 1) + 1:\n\t\t\traise ModuleError(\n\t\t\t\t""Grad maps width is too small (got %d, expected at least %d)"" %\n\t\t\t\t(outw + 2 * wpad, wdilation * (fw - 1) + 1)\n\t\t\t)\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tbatchsize, outmaps, outh, outw = shape\n\t\tinmaps, _, fh, fw = self.W.shape\n\n\t\thpad, wpad = self.pad\n\t\thdilation, wdilation = self.dilation\n\t\thstride, wstride = self.stride\n\n\t\tinh = (outh + 2 * hpad - hdilation * (fh - 1) - 1) // hstride + 1\n\t\tinw = (outw + 2 * wpad - wdilation * (fw - 1) - 1) // wstride + 1\n\n\t\treturn batchsize, inmaps, inh, inw\n\n\ndef unittest():\n\tmultiInMapsTest()\n\tmultiOutMapsTest()\n\n\tif Config.backend in {Config.Backend.cuda, Config.Backend.hip}:\n\t\tmultiMapsWithPadsTest()\n\n\ttrainTest()\n\n\ndef multiInMapsTest():\n\tbatchsize, inmaps, h, w = 1, 2, 10, 10\n\toutmaps = 1\n\tstride = 2\n\tsize = 4\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, inmaps, h, w).astype(np.float32))\n\n\tdeconv = Deconv2D(inmaps, outmaps, size, stride=2)\n\tdeconv(data)\n\n\thostOutData = np.zeros(deconv.data.shape).astype(np.float32)\n\n\tfor k in range(inmaps):\n\t\tfor i in range(0, hostOutData.shape[2] - size + 1, stride):\n\t\t\tfor j in range(0, hostOutData.shape[3] - size + 1, stride):\n\t\t\t\thostOutData[0,0,i:size+i,j:size+j] += deconv.W.get()[k,0] * data.get()[0,k,int(i/stride),int(j/stride)]\n\t\thostOutData[0, 0] += deconv.b.get()[0, 0]\n\n\tassert np.allclose(hostOutData, deconv.data.get())\n\n\ndef multiOutMapsTest():\n\tbatchsize, inmaps, h, w = 1, 1, 2, 2\n\toutmaps = 2\n\tstride = 2\n\tsize = 4\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, inmaps, h, w).astype(np.float32))\n\n\tdeconv = Deconv2D(inmaps, outmaps, size, stride=stride)\n\tdeconv(data)\n\n\thostOutData = np.zeros(deconv.data.shape).astype(np.float32)\n\n\tfor k in range(outmaps):\n\t\tfor i in range(0, hostOutData.shape[2] - size + 1, stride):\n\t\t\tfor j in range(0, hostOutData.shape[3] - size + 1, stride):\n\t\t\t\thostOutData[0,k,i:size+i,j:size+j] += deconv.W.get()[0,k] * data.get()[0,0,int(i/stride),int(j/stride)]\n\t\thostOutData[0, k] += deconv.b.get()[0, k]\n\n\tassert np.allclose(hostOutData, deconv.data.get())\n\n\ndef multiMapsWithPadsTest():\n\tbatchsize, inmaps, h, w = 3, 4, 2, 2\n\toutmaps, size, stride, pad, dilation = 4, 3, 2, 1, 2\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, inmaps, h, w).astype(np.float32))\n\n\tdeconv = Deconv2D(inmaps, outmaps, size=size, stride=stride, pad=pad, dilation=dilation, initscheme=""gaussian"")\n\tdeconv(data)\n\n\thostW, hostBias = deconv.W.get(), deconv.b.get()\n\tdl = dilation\n\n\thostData, hostOutData = data.get(), np.zeros(deconv.data.shape[:2] + (deconv.data.shape[2] + 2 * pad,\n\t\t\t\t\t\t\t\t\t\t\t\tdeconv.data.shape[3] + 2 * pad), dtype=np.float32)\n\n\tfor c in range(outmaps):\n\t\thostOutData[:, c, :, :] = hostBias[0, c, 0, 0]\n\n\tfor b in range(batchsize):\n\t\tfor oc in range(outmaps):\n\t\t\tfor ic in range(inmaps):\n\t\t\t\tfor y in range(h):\n\t\t\t\t\tfor x in range(w):\n\t\t\t\t\t\tfor dy in range(size):\n\t\t\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\t\t\thostOutData[b,oc,y*stride+dy*dl,x*stride+dx*dl] += hostW[ic,oc,dy,dx]*hostData[b,ic,y,x]\n\n\tassert np.allclose(hostOutData[:, :, pad:-pad, pad:-pad], deconv.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*deconv.data.shape).astype(np.float32))\n\tdeconv.backward(grad)\n\n\thostGrad = np.zeros(grad.shape[:2] + (grad.shape[2] + 2 * pad, grad.shape[3] + 2 * pad), dtype=np.float32)\n\thostGrad[:, :, pad:-pad, pad:-pad] = grad.get()\n\n\thostInGrad = np.zeros(hostData.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor ic in range(inmaps):\n\t\t\tfor oc in range(outmaps):\n\t\t\t\tfor y in range(h):\n\t\t\t\t\tfor x in range(w):\n\t\t\t\t\t\tfor dy in range(size):\n\t\t\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\t\t\thostInGrad[b,ic,y,x] += hostGrad[b,oc,y*stride+dy*dl, x*stride+dx*dl]*hostW[ic,oc,dy,dx]\n\n\tassert np.allclose(hostInGrad, deconv.grad.get())\n\n\thostWGrad = np.zeros(deconv.getVar(""W"").grad.shape, dtype=np.float32)\n\tfor b in range(batchsize):\n\t\tfor ic in range(inmaps):\n\t\t\tfor oc in range(outmaps):\n\t\t\t\tfor dy in range(size):\n\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\tfor y in range(h):\n\t\t\t\t\t\t\tfor x in range(w):\n\t\t\t\t\t\t\t\thostWGrad[ic,oc,dy,dx]+=hostGrad[b,oc,y*stride+dy*dl,x*stride+dx*dl]*hostData[b,ic,y,x]\n\n\tassert np.allclose(hostWGrad, deconv.getVar(""W"").grad.get())\n\n\thostBiasGrad = np.empty(hostBias.shape, dtype=np.float32)\n\tfor oc in range(outmaps):\n\t\thostBiasGrad[0, oc, 0, 0] = np.sum(hostGrad[:, oc, :, :])\n\n\tassert np.allclose(hostBiasGrad, deconv.getVar(""b"").grad.get())\n\n\ndef trainTest():\n\tbatchsize, inmaps, h, w = 5, 5, 2, 2\n\toutmaps = 1\n\tsize = 8\n\n\tdata = gpuarray.to_gpu(np.random.normal(0.0, 1.0, (batchsize, inmaps, h, w)).astype(np.float32))\n\tdeconv = Deconv2D(inmaps, outmaps, size)\n\n\tfrom PuzzleLib.Cost.MSE import MSE\n\tmse = MSE()\n\n\ttarget = gpuarray.to_gpu(np.random.normal(0.0, 1.0, (batchsize, outmaps, 9, 9)).astype(np.float32))\n\n\tfor i in range(100):\n\t\tlearnRate = 1e-2\n\n\t\tdeconv(data)\n\t\terror, grad = mse(deconv.data, target)\n\n\t\tdeconv.backward(grad)\n\t\tdeconv.updateParams(learnRate)\n\n\t\tif (i + 1) % 5 == 0:\n\t\t\tprint(""Iteration #%d error: %s"" % (i + 1, error))\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Deconv3D.py,15,"b'import numpy as np\n\nfrom PuzzleLib import Config\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Modules.Module import ModuleError\nfrom PuzzleLib.Modules.DeconvND import DeconvND\n\n\nclass Deconv3D(DeconvND):\n\tdef __init__(self, inmaps, outmaps, size, stride=1, pad=0, dilation=1, wscale=1.0, useBias=True, name=None,\n\t\t\t\t initscheme=None, empty=False, groups=1):\n\t\tsuper().__init__(\n\t\t\t3, inmaps, outmaps, size, stride, pad, dilation, wscale, useBias, name, initscheme, empty, groups\n\t\t)\n\t\tself.registerBlueprint(locals())\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 5:\n\t\t\traise ModuleError(""Data must be 5d tensor"")\n\n\t\t_, inmaps, _, _, _ = shape\n\t\tif inmaps != self.W.shape[0]:\n\t\t\traise ModuleError(""Data has %d maps (expected: %d)"" % (inmaps, self.W.shape[0]))\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tbatchsize, inmaps, ind, inh, inw = shape\n\t\t_, outmaps, fd, fh, fw = self.W.shape\n\n\t\tdpad, hpad, wpad = self.pad\n\t\tddilation, hdilation, wdilation = self.dilation\n\t\tdstride, hstride, wstride = self.stride\n\n\t\toutmaps *= self.groups\n\t\toutd = (ind - 1) * dstride + ddilation * (fd - 1) - 2 * dpad + 1\n\t\touth = (inh - 1) * hstride + hdilation * (fh - 1) - 2 * hpad + 1\n\t\toutw = (inw - 1) * wstride + wdilation * (fw - 1) - 2 * wpad + 1\n\n\t\treturn batchsize, outmaps, outd, outh, outw\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 5:\n\t\t\traise ModuleError(""Grad must be 5d tensor"")\n\n\t\t_, outmaps, outd, outh, outw = shape\n\t\t_, _, fd, fh, fw = self.W.shape\n\n\t\tdpad, hpad, wpad = self.pad\n\t\tddilation, hdilation, wdilation = self.dilation\n\n\t\tif outmaps != self.W.shape[1] * self.groups:\n\t\t\traise ModuleError(""Grad has %d maps (expected: %d)"" % (outmaps, self.W.shape[1] * self.groups))\n\n\t\tif outd + 2 * dpad < ddilation * (fd - 1) + 1:\n\t\t\traise ModuleError(\n\t\t\t\t""Grad maps depth is too small (got %d, expected at least %d)"" %\n\t\t\t\t(outd + 2 * dpad, ddilation * (fd - 1) + 1)\n\t\t\t)\n\n\t\tif outh + 2 * hpad < hdilation * (fh - 1) + 1:\n\t\t\traise ModuleError(\n\t\t\t\t""Grad maps height is too small (got %d, expected at least %d)"" %\n\t\t\t\t(outh + 2 * hpad, hdilation * (fh - 1) + 1)\n\t\t\t)\n\n\t\tif outw + 2 * wpad < wdilation * (fw - 1) + 1:\n\t\t\traise ModuleError(\n\t\t\t\t""Grad maps width is too small (got %d, expected at least %d)"" %\n\t\t\t\t(outw + 2 * wpad, wdilation * (fw - 1) + 1)\n\t\t\t)\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tbatchsize, outmaps, outd, outh, outw = shape\n\t\tinmaps, _, fd, fh, fw = self.W.shape\n\n\t\tdpad, hpad, wpad = self.pad\n\t\tddilation, hdilation, wdilation = self.dilation\n\t\tdstride, hstride, wstride = self.stride\n\n\t\tind = (outd + 2 * dpad - ddilation * (fd - 1) - 1) // dstride + 1\n\t\tinh = (outh + 2 * hpad - hdilation * (fh - 1) - 1) // hstride + 1\n\t\tinw = (outw + 2 * wpad - wdilation * (fw - 1) - 1) // wstride + 1\n\n\t\treturn batchsize, inmaps, ind, inh, inw\n\n\ndef unittest():\n\tif Config.backend in {Config.Backend.cuda, Config.Backend.hip}:\n\t\tmultiMapsWithPadsTest()\n\n\ttrainTest()\n\n\ndef multiMapsWithPadsTest():\n\tbatchsize, inmaps, d, h, w = 5, 4, 2, 2, 2\n\toutmaps, size, stride, pad, dilation = 4, 2, 2, 1, 2\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, inmaps, d, h, w).astype(np.float32))\n\n\tdeconv = Deconv3D(inmaps, outmaps, size=size, stride=stride, pad=pad, dilation=dilation, initscheme=""gaussian"")\n\tdeconv(data)\n\n\thostW, hostBias = deconv.W.get(), deconv.b.get()\n\tdl = dilation\n\n\thostData, hostOutData = data.get(), np.zeros(deconv.data.shape[:2] + (deconv.data.shape[2] + 2 * pad,\n\t\t\t\t\t\t\t\t\t\tdeconv.data.shape[3] + 2 * pad, deconv.data.shape[4] + 2 * pad),\n\t\t\t\t\t\t\t\t\t\t\t\t dtype=np.float32)\n\n\tfor c in range(outmaps):\n\t\thostOutData[:, c, :, :, :] = hostBias[0, c, 0, 0, 0]\n\n\tfor b in range(batchsize):\n\t\tfor oc in range(outmaps):\n\t\t\tfor ic in range(inmaps):\n\t\t\t\tfor z in range(d):\n\t\t\t\t\tfor y in range(h):\n\t\t\t\t\t\tfor x in range(w):\n\t\t\t\t\t\t\tfor dz in range(size):\n\t\t\t\t\t\t\t\tfor dy in range(size):\n\t\t\t\t\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\t\t\t\t\thostOutData[b, oc, z*stride + dz*dl, y*stride + dy*dl, x*stride + dx*dl] += \\\n\t\t\t\t\t\t\t\t\t\t\thostW[ic, oc, dz, dy, dx] * hostData[b, ic, z, y, x]\n\n\tassert np.allclose(hostOutData[:, :, pad:-pad, pad:-pad, pad:-pad], deconv.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*deconv.data.shape).astype(np.float32))\n\tdeconv.backward(grad)\n\n\thostGrad = np.zeros(grad.shape[:2]+(grad.shape[2]+2*pad,grad.shape[3]+2*pad,grad.shape[4]+2*pad), dtype=np.float32)\n\thostGrad[:, :, pad:-pad, pad:-pad, pad:-pad] = grad.get()\n\n\thostInGrad = np.zeros(hostData.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor ic in range(inmaps):\n\t\t\tfor oc in range(outmaps):\n\t\t\t\tfor z in range(d):\n\t\t\t\t\tfor y in range(h):\n\t\t\t\t\t\tfor x in range(w):\n\t\t\t\t\t\t\tfor dz in range(size):\n\t\t\t\t\t\t\t\tfor dy in range(size):\n\t\t\t\t\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\t\t\t\t\thostInGrad[b, ic, z, y, x] += \\\n\t\t\t\t\t\t\t\t\t\t\thostGrad[b, oc, z*stride + dz*dl, y*stride + dy*dl, x*stride+dx*dl] * \\\n\t\t\t\t\t\t\t\t\t\t\thostW[ic, oc, dz, dy, dx]\n\n\tassert np.allclose(hostInGrad, deconv.grad.get())\n\n\thostWGrad = np.zeros(deconv.getVar(""W"").grad.shape, dtype=np.float32)\n\tfor b in range(batchsize):\n\t\tfor ic in range(inmaps):\n\t\t\tfor oc in range(outmaps):\n\t\t\t\tfor dz in range(size):\n\t\t\t\t\tfor dy in range(size):\n\t\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\t\tfor z in range(d):\n\t\t\t\t\t\t\t\tfor y in range(h):\n\t\t\t\t\t\t\t\t\tfor x in range(w):\n\t\t\t\t\t\t\t\t\t\thostWGrad[ic, oc, dz, dy, dx] += \\\n\t\t\t\t\t\t\t\t\t\t\thostGrad[b, oc, z*stride + dz*dl, y*stride + dy*dl, x*stride + dx*dl] * \\\n\t\t\t\t\t\t\t\t\t\t\thostData[b, ic, z, y, x]\n\n\tassert np.allclose(hostWGrad, deconv.getVar(""W"").grad.get())\n\n\thostBGrad = np.empty(hostBias.shape, dtype=np.float32)\n\tfor oc in range(outmaps):\n\t\thostBGrad[0, oc, 0, 0, 0] = np.sum(hostGrad[:, oc, :, :, :])\n\n\tassert np.allclose(hostBGrad, deconv.getVar(""b"").grad.get())\n\n\ndef trainTest():\n\tbatchsize, inmaps, d, h, w = 5, 5, 2, 2, 2\n\toutmaps = 1\n\tsize = 3\n\n\tdata = gpuarray.to_gpu(np.random.normal(0.0, 1.0, (batchsize, inmaps, d, h, w)).astype(np.float32))\n\tdeconv = Deconv3D(inmaps, outmaps, size)\n\n\tfrom PuzzleLib.Cost.MSE import MSE\n\tmse = MSE()\n\n\ttarget = gpuarray.to_gpu(np.random.normal(0.0, 1.0, (batchsize, outmaps, 4, 4, 4)).astype(np.float32))\n\n\tfor i in range(100):\n\t\tlearnRate = 1e-2\n\n\t\tdeconv(data)\n\t\terror, grad = mse(deconv.data, target)\n\n\t\tdeconv.backward(grad)\n\t\tdeconv.updateParams(learnRate)\n\n\t\tif (i + 1) % 5 == 0:\n\t\t\tprint(""Iteration #%d error: %s"" % (i + 1, error))\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/DeconvND.py,0,"b'import sys\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Dnn.Basic import ConvFwdAlgo, ConvBwdDataAlgo, ConvBwdFilterAlgo\nfrom PuzzleLib.Backend.Dnn.Basic import convNdbenchmark, deconvNd, deconvNdBackwardData, deconvNdBackwardParams\n\nfrom PuzzleLib.Variable import Variable\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass DeconvND(Module):\n\tdef __init__(self, nd, inmaps, outmaps, size, stride=1, pad=0, dilation=1, wscale=1.0, useBias=True, name=None,\n\t\t\t\t initscheme=None, empty=False, groups=1):\n\t\tsuper().__init__(name)\n\n\t\tself.stride = self.repeat(stride, nd)\n\t\tself.pad = self.repeat(pad, nd)\n\t\tself.dilation = self.repeat(dilation, nd)\n\n\t\tself.useBias = useBias\n\t\tself.groups = groups\n\n\t\tself.fwdAlgo, self.bwdFilterAlgo, self.bwdDataAlgo = None, None, None\n\t\tself.installDefaultAlgos()\n\n\t\tif inmaps % groups != 0 or outmaps % groups != 0:\n\t\t\traise ModuleError(\n\t\t\t\t""Number of input and output maps must be divisible by number of groups ""\n\t\t\t\t""(%d inmaps, %d outmaps, %d groups)"" % (inmaps, outmaps, groups)\n\t\t\t)\n\n\t\toutmaps //= groups\n\n\t\tself.W = None\n\t\tself.b = None\n\n\t\tif empty:\n\t\t\treturn\n\n\t\tWshape = (inmaps, outmaps, *self.repeat(size, nd))\n\t\tW = self.createTensorWithScheme(initscheme, Wshape, wscale, factorTranspose=True)\n\n\t\tself.setVar(""W"", Variable(gpuarray.empty(Wshape, dtype=self.calctype) if W is None else gpuarray.to_gpu(W)))\n\n\t\tif useBias:\n\t\t\tbshape = (1, outmaps) + self.repeat(1, nd)\n\t\t\tself.setVar(""b"", Variable(gpuarray.zeros(bshape, dtype=self.calctype)))\n\n\n\tdef optimizeForShape(self, shape, memlimit=None):\n\t\tif Config.backend in {Config.Backend.cuda, Config.Backend.hip}:\n\t\t\tshape = self.dataShapeFrom(shape)\n\n\t\tfwdRes, bwdFilterRes, bwdDataRes = convNdbenchmark(\n\t\t\tshape, self.W.shape, self.stride, self.pad, self.dilation, self.groups, transpose=True\n\t\t)\n\n\t\tmemlimit = sys.maxsize if memlimit is None else memlimit\n\n\t\tself.fwdAlgo = next(ConvFwdAlgo(res.algo) for res in fwdRes if res.memory <= memlimit)\n\t\tself.bwdFilterAlgo = next(ConvBwdFilterAlgo(res.algo) for res in bwdFilterRes if res.memory <= memlimit)\n\t\tself.bwdDataAlgo = next(ConvBwdDataAlgo(res.algo) for res in bwdDataRes if res.memory <= memlimit)\n\n\n\tdef installDefaultAlgos(self):\n\t\tif Config.backend == Config.Backend.cuda:\n\t\t\tself.fwdAlgo = ConvFwdAlgo.implicitGemm\n\t\t\tself.bwdFilterAlgo = ConvBwdFilterAlgo.algo0\n\t\t\tself.bwdDataAlgo = ConvBwdDataAlgo.algo0\n\n\t\telif Config.backend in {Config.Backend.hip, Config.Backend.intel}:\n\t\t\tself.fwdAlgo = ConvFwdAlgo.auto\n\t\t\tself.bwdFilterAlgo = ConvBwdFilterAlgo.auto\n\t\t\tself.bwdDataAlgo = ConvBwdDataAlgo.auto\n\n\n\tdef updateData(self, data):\n\t\tself.data = deconvNd(\n\t\t\tdata, self.W, self.b, stride=self.stride, pad=self.pad, dilation=self.dilation,\n\t\t\tgroups=self.groups, algo=self.bwdDataAlgo\n\t\t)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = deconvNdBackwardData(\n\t\t\tgrad, self.W, data=self.inData, stride=self.stride, pad=self.pad, dilation=self.dilation,\n\t\t\tgroups=self.groups, algo=self.fwdAlgo\n\t\t)\n\n\n\tdef accGradParams(self, grad, scale=1.0, momentum=0.0):\n\t\tdeconvNdBackwardParams(\n\t\t\tself.inData, grad, self.W, self.b, stride=self.stride, pad=self.pad, dilation=self.dilation,\n\t\t\tgroups=self.groups, wgrad=self.vars[""W""].grad, bgrad=self.vars[""b""].grad if self.b is not None else None,\n\t\t\tscale=scale, momentum=momentum, algo=self.bwdFilterAlgo\n\t\t)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\traise NotImplementedError()\n\n\n\tdef gradShapeFrom(self, shape):\n\t\traise NotImplementedError()\n\n\n\tdef calcMode(self, T):\n\t\tif Config.backend in {Config.Backend.cuda, Config.Backend.hip}:\n\t\t\tif self.calctype == T:\n\t\t\t\treturn\n\n\t\t\tvariables = self.vars\n\t\t\tself.vars = {}\n\n\t\t\tfor varName, var in variables.items():\n\t\t\t\tself.setVar(varName, Variable(\n\t\t\t\t\tvar.data.astype(T), name=var.name, grad=var.grad.astype(T) if var.grad is not None else None\n\t\t\t\t))\n\n\t\t\tself.calctype = T\n\n\t\telse:\n\t\t\tsuper().calcMode(T)\n'"
Modules/DepthConcat.py,9,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray, Memory\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass DepthConcat(Module):\n\tdef __init__(self, name=None):\n\t\tsuper().__init__(name)\n\t\tself.movesData = True\n\n\n\tdef updateData(self, data):\n\t\tself.data = Memory.depthConcat(data)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = Memory.depthSplit(grad, self.inData)\n\n\n\tdef checkDataShape(self, shapes):\n\t\tif not isinstance(shapes, list):\n\t\t\traise ModuleError(""Data must be list of tensors"")\n\n\t\tfor shape in shapes:\n\t\t\tif len(shape) != 4:\n\t\t\t\traise ModuleError(""Data must consist of 4d tensors"")\n\n\t\t\tif shape[0] != shapes[0][0]:\n\t\t\t\traise ModuleError(""Inconsistency in batch size"")\n\n\n\tdef dataShapeFrom(self, shapes):\n\t\tdepth, h, w = 0, 0, 0\n\t\tfor shape in shapes:\n\t\t\tdepth += shape[1]\n\t\t\th, w = max(h, shape[2]), max(w, shape[3])\n\n\t\treturn shapes[0][0], depth, h, w\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 4:\n\t\t\traise ModuleError(""Grad must be 4d tensor"")\n\n\t\tdepth, h, w = 0, 0, 0\n\t\tfor data in self.inData:\n\t\t\tsh = data.shape\n\n\t\t\tdepth += sh[1]\n\t\t\th, w = max(h, sh[2]), max(w, sh[3])\n\n\t\tgradshape = (self.inData[0].shape[0], depth, h, w)\n\t\tif shape != gradshape:\n\t\t\traise ModuleError(""Bad grad shape (%s given, %s expected)"" % (shape, gradshape))\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tshapes = [data.shape for data in self.inData]\n\t\treturn shapes\n\n\ndef unittest():\n\tdata1 = gpuarray.to_gpu(np.random.randn(3, 4, 2, 2).astype(np.float32))\n\tdata2 = gpuarray.to_gpu(np.random.randn(3, 2, 6, 6).astype(np.float32))\n\tdata3 = gpuarray.to_gpu(np.random.randn(3, 5, 4, 4).astype(np.float32))\n\tdata4 = gpuarray.to_gpu(np.random.randn(3, 3, 5, 5).astype(np.float32))\n\talldata = [data1, data2, data3, data4]\n\n\tconcat = DepthConcat()\n\tconcat(alldata)\n\n\tdepth, h, w = 0, 0, 0\n\tfor data in alldata:\n\t\tdepth += data.shape[1]\n\t\th, w = max(h, data.shape[2]), max(w, data.shape[3])\n\n\thostOutData = np.zeros(shape=(data1.shape[0], depth, h, w), dtype=np.float32)\n\n\thostOutData[:, :4, 2:4, 2:4] = data1.get()\n\thostOutData[:, 4:6, :, :] = data2.get()\n\thostOutData[:, 6:11, 1:5, 1:5] = data3.get()\n\thostOutData[:, 11:, :5, :5] = data4.get()\n\n\tassert np.allclose(hostOutData, concat.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*hostOutData.shape).astype(np.float32))\n\tconcat.backward(grad)\n\n\thostInGrads = [np.empty(data.shape, dtype=np.float32) for data in alldata]\n\n\thostInGrads[0] = grad.get()[:, :4, 2:4, 2:4]\n\thostInGrads[1] = grad.get()[:, 4:6, :, :]\n\thostInGrads[2] = grad.get()[:, 6:11, 1:5, 1:5]\n\thostInGrads[3] = grad.get()[:, 11:, :5, :5]\n\n\tassert all(np.allclose(hostInGrad, concat.grad[i].get()) for i, hostInGrad in enumerate(hostInGrads))\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Dropout.py,12,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import dtypesSupported, globalRng, copy, memoryPool as memPool\nfrom PuzzleLib.Backend.Kernels.ElementWise import dropoutKer\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass Dropout(Module):\n\tdef __init__(self, p=0.5, rng=globalRng, slicing=None, inplace=False, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals(), exclude=[""rng""])\n\n\t\tif rng is None:\n\t\t\trng = globalRng\n\n\t\tself.p = p\n\t\tself.partition = None\n\n\t\tself.rng = rng\n\t\tself.rands = None\n\n\t\tself.slice = slicing\n\n\t\tself.inplace = inplace\n\t\tif inplace and Config.showWarnings:\n\t\t\tprint(""[%s] Warning: %s is using inplace flag"" % (Config.libname, self))\n\n\n\tdef updateData(self, data):\n\t\tif self.train:\n\t\t\tif self.inplace:\n\t\t\t\tself.data = data\n\t\t\telse:\n\t\t\t\tif self.slice is not None:\n\t\t\t\t\tself.data = copy(None, data)\n\t\t\t\telse:\n\t\t\t\t\tself.data = gpuarray.empty(data.shape, dtype=data.dtype, allocator=memPool)\n\n\t\t\tparttype = {\n\t\t\t\tnp.float32: np.uint32,\n\t\t\t\tnp.float16: np.uint16\n\t\t\t}[data.dtype.type]\n\n\t\t\tintsize = np.dtype(np.uint32).itemsize\n\n\t\t\tnbytes = (data.nbytes + intsize - 1) // intsize * intsize\n\t\t\tself.rands = gpuarray.empty((nbytes // np.dtype(parttype).itemsize, ), dtype=parttype, allocator=memPool)\n\n\t\t\tself.rng.fillInteger(self.rands.view(np.uint32))\n\n\t\t\tp = 1.0 - self.p\n\t\t\tself.partition = int(p * np.iinfo(parttype).max)\n\n\t\t\tdropoutKer(data.dtype)(self.data, data, self.rands, self.partition, np.float32(p), slice=self.slice)\n\n\t\telse:\n\t\t\tself.data = data\n\n\n\tdef updateGrad(self, grad):\n\t\tif self.train:\n\t\t\tif self.inplace:\n\t\t\t\tself.grad = grad\n\t\t\telse:\n\t\t\t\tif self.slice is not None:\n\t\t\t\t\tself.grad = copy(None, grad)\n\t\t\t\telse:\n\t\t\t\t\tself.grad = gpuarray.empty(grad.shape, dtype=grad.dtype, allocator=memPool)\n\n\t\t\tdropoutKer(grad.dtype)(self.grad, grad, self.rands, self.partition, 1.0 - self.p, slice=self.slice)\n\n\t\telse:\n\t\t\tself.grad = grad\n\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\t\tself.rands = None\n\n\n\tdef calcMode(self, T):\n\t\tdtypes = {dtype for dtype, _ in dtypesSupported()}\n\n\t\tif T not in dtypes:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n\n\ndef unittest():\n\tfor dtype, _ in dtypesSupported():\n\t\tdropoutTest(dtype)\n\n\ndef dropoutTest(dtype):\n\thostData = np.random.randn(11, 13, 4, 3).astype(dtype)\n\tdata = gpuarray.to_gpu(hostData)\n\n\tdropout = Dropout()\n\tdropout.calcMode(dtype)\n\n\tdropout(data)\n\n\thostRands = dropout.rands.get()[:data.size].reshape(data.shape)\n\n\thostOutData = hostData * (hostRands < dropout.partition) / (1.0 - dropout.p)\n\tassert np.allclose(hostOutData, dropout.data.get())\n\n\thostGrad = np.random.randn(*dropout.data.shape).astype(dtype)\n\tgrad = gpuarray.to_gpu(hostGrad)\n\n\tdropout.backward(grad)\n\n\thostInGrad = hostGrad * (hostRands < dropout.partition) / (1.0 - dropout.p)\n\tassert np.allclose(hostInGrad, dropout.grad.get())\n\n\tdropout.evalMode()\n\tdropout(data)\n\n\thostOutData = hostData\n\tassert np.allclose(hostOutData, dropout.data.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Dropout2D.py,12,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import dtypesSupported, globalRng, copy, memoryPool as memPool\nfrom PuzzleLib.Backend.Kernels.ElementWise import dropout2dKer\n\nfrom PuzzleLib.Modules.Dropout import Dropout\n\n\nclass Dropout2D(Dropout):\n\tdef __init__(self, p=0.5, rng=globalRng, slicing=None, inplace=False, name=None):\n\t\tsuper().__init__(p, rng, slicing, inplace, name)\n\t\tself.mapsize = None\n\n\n\tdef updateData(self, data):\n\t\tif self.train:\n\t\t\tif self.inplace:\n\t\t\t\tself.data = data\n\t\t\telse:\n\t\t\t\tif self.slice is not None:\n\t\t\t\t\tself.data = copy(None, data)\n\t\t\t\telse:\n\t\t\t\t\tself.data = gpuarray.empty(data.shape, dtype=data.dtype, allocator=memPool)\n\n\t\t\tbatchsize, maps, height, width = data.shape\n\t\t\tself.mapsize = height * width\n\n\t\t\tparttype = {\n\t\t\t\tnp.float32: np.uint32,\n\t\t\t\tnp.float16: np.uint16\n\t\t\t}[data.dtype.type]\n\n\t\t\tintsize = np.dtype(np.uint32).itemsize\n\t\t\titemsize = np.dtype(parttype).itemsize\n\n\t\t\tnbytes = (batchsize * maps * itemsize + intsize - 1) // intsize * intsize\n\t\t\tself.rands = gpuarray.empty((nbytes // itemsize, ), dtype=parttype, allocator=memPool)\n\n\t\t\tself.rng.fillInteger(self.rands.view(np.uint32))\n\n\t\t\tp = 1.0 - self.p\n\t\t\tself.partition = int(p * np.iinfo(parttype).max)\n\n\t\t\tdropout2dKer(data.dtype)(self.data, data, self.rands, self.partition, p, self.mapsize, slice=self.slice)\n\n\t\telse:\n\t\t\tself.data = data\n\n\n\tdef updateGrad(self, grad):\n\t\tif self.train:\n\t\t\tif self.inplace:\n\t\t\t\tself.grad = grad\n\t\t\telse:\n\t\t\t\tif self.slice is not None:\n\t\t\t\t\tself.grad = copy(None, grad)\n\t\t\t\telse:\n\t\t\t\t\tself.grad = gpuarray.empty(grad.shape, dtype=grad.dtype, allocator=memPool)\n\n\t\t\tdropout2dKer(grad.dtype)(self.grad, grad, self.rands, self.partition, 1.0 - self.p, self.mapsize)\n\n\t\telse:\n\t\t\tself.grad = grad\n\n\ndef unittest():\n\tfor dtype, _ in dtypesSupported():\n\t\tdropout2dTest(dtype)\n\n\ndef dropout2dTest(dtype):\n\tbatchsize, maps, height, width = 11, 13, 4, 3\n\n\thostData = np.random.randn(batchsize, maps, height, width).astype(dtype)\n\tdata = gpuarray.to_gpu(hostData)\n\n\tdropout2d = Dropout2D()\n\tdropout2d.calcMode(dtype)\n\n\tdropout2d(data)\n\n\thostRands = dropout2d.rands.get()[:batchsize * maps].reshape(batchsize, maps)[:, :, np.newaxis, np.newaxis]\n\n\thostOutData = hostData * (hostRands < dropout2d.partition) / (1.0 - dropout2d.p)\n\tassert np.allclose(hostOutData, dropout2d.data.get())\n\n\thostGrad = np.random.randn(*dropout2d.data.shape).astype(dtype)\n\tgrad = gpuarray.to_gpu(hostGrad)\n\n\tdropout2d.backward(grad)\n\n\thostInGrad = hostGrad * (hostRands < dropout2d.partition) / (1.0 - dropout2d.p)\n\tassert np.allclose(hostInGrad, dropout2d.grad.get())\n\n\tdropout2d.evalMode()\n\tdropout2d(data)\n\n\thostOutData = hostData\n\tassert np.allclose(hostOutData, dropout2d.data.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Embedder.py,11,"b'import os\n\nimport h5py\nimport numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Kernels.Embedder import embed, embedBackwardParams\nfrom PuzzleLib.Backend.Utils import dtypesSupported\n\nfrom PuzzleLib.Variable import Variable\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass Embedder(Module):\n\tdef __init__(self, vocabulary, sentlength, embsize, onVocabulary=None, initscheme=""uniform"", wscale=1.0,\n\t\t\t\t learnable=True, name=None):\n\t\tsuper().__init__(name)\n\t\targs = dict(locals())\n\n\t\tself.embsize = embsize\n\t\tself.sentlength = sentlength\n\n\t\tself.wgrad = None\n\t\tself.learnable = learnable\n\t\tself.outgrad = None\n\n\t\tdt = h5py.special_dtype(vlen=str)\n\n\t\tif isinstance(vocabulary, dict):\n\t\t\tvocabsize = len(vocabulary)\n\t\t\tvocab = np.empty(shape=(vocabsize, ), dtype=dt)\n\n\t\t\tfor word, idx in vocabulary.items():\n\t\t\t\tvocab[int(idx)] = word\n\n\t\telif isinstance(vocabulary, int):\n\t\t\tvocabsize = vocabulary\n\t\t\tvocab = np.empty(shape=(0, ), dtype=dt)\n\n\t\telse:\n\t\t\traise ModuleError(""Unrecognized vocabulary parameter type"")\n\n\t\tself.vocab = None\n\t\tself.setAttr(""vocab"", vocab)\n\n\t\targs[""vocabulary""] = vocabsize\n\t\tself.registerBlueprint(args, exclude=[""onVocabulary""])\n\n\t\tWshape = (vocabsize, embsize)\n\t\tW = self.createTensorWithScheme(initscheme, Wshape, wscale, (embsize, vocabsize))\n\t\tif W is None:\n\t\t\tW = np.empty(Wshape, dtype=np.float32)\n\n\t\tif onVocabulary is not None:\n\t\t\tonVocabulary(W)\n\n\t\tself.W = None\n\t\tself.setVar(""W"", Variable(gpuarray.to_gpu(W)))\n\n\t\tself.loadVarHook = self.checkVarOnLoad\n\t\tself.loadAttrHook = self.checkAttrOnLoad\n\n\n\tdef checkVarOnLoad(self, paramName, dataset):\n\t\tif paramName == ""W"":\n\t\t\tif dataset.shape[1] != self.embsize:\n\t\t\t\traise ModuleError(""Expected embedding size %s, was given %s"" % (self.embsize, dataset.shape[1]))\n\n\t\t\tself.setVar(""W"", Variable(gpuarray.to_gpu(dataset)))\n\n\t\telse:\n\t\t\traise ModuleError(""Unknown parameter name \'%s\' for embedder"" % paramName)\n\n\n\tdef checkAttrOnLoad(self, attrName, dataset):\n\t\tif attrName == ""vocab"":\n\t\t\tself.setAttr(""vocab"", dataset)\n\n\t\telse:\n\t\t\traise ModuleError(""Unknown attribute name \'%s\' for embedder"" % attrName)\n\n\n\tdef getVocabulary(self):\n\t\tvoc = {}\n\n\t\tif self.hasAttr(""vocab""):\n\t\t\tfor i in range(self.vocab.shape[0]):\n\t\t\t\tvoc[self.vocab[i]] = i\n\n\t\treturn voc\n\n\n\tdef verifyData(self, data):\n\t\tmn, mx = gpuarray.minimum(data).get(), gpuarray.maximum(data).get()\n\t\tif mn < -1:\n\t\t\traise ModuleError(""Embedder data verification failed, found index %s (< -1)"" % mn)\n\n\t\tif mx >= self.W.shape[0]:\n\t\t\traise ModuleError(""Embedder data verification failed, found index %s (vocabulary size is %s)"" %\n\t\t\t\t\t\t\t  (mx, self.W.shape[0]))\n\n\n\tdef updateData(self, data):\n\t\tif Config.verifyData:\n\t\t\tself.verifyData(data)\n\n\t\tself.data = embed(data, self.W)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = None\n\n\n\tdef accGradParams(self, grad, scale=1.0, momentum=0.0):\n\t\tself.outgrad = grad\n\t\tself.vars[""W""].grad.fill(0.0)\n\n\t\tif self.learnable:\n\t\t\tembedBackwardParams(self.inData, grad, self.vars[""W""].grad, scale)\n\n\n\tdef updateParams(self, learnRate):\n\t\tif self.learnable:\n\t\t\tembedBackwardParams(self.inData, self.outgrad, self.W, learnRate)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tbatchsize, sentlen = shape\n\t\treturn batchsize, sentlen, self.embsize\n\n\n\tdef gradShapeFrom(self, shape):\n\t\traise ModuleError(""Gradient propagation is undefined"")\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 2:\n\t\t\traise ModuleError(""Data must be 2d matrix"")\n\n\t\tif shape[1] != self.sentlength:\n\t\t\traise ModuleError(""Expected %d data sentence length, %d was given"" % (self.sentlength, shape[1]))\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 3:\n\t\t\traise ModuleError(""Grad must be 3d tensor"")\n\n\t\tbatchsize, sentlen, embsize = shape\n\t\tif sentlen != self.sentlength:\n\t\t\traise ModuleError(""Expected %d grad sentence length, %d was given"" % (self.sentlength, shape[1]))\n\n\t\tif embsize != self.embsize:\n\t\t\traise ModuleError(""Expected %d grad embedding size, %d was given"" % (self.embsize, embsize))\n\n\t\tif batchsize != self.inData.shape[0]:\n\t\t\traise ModuleError(""Expected %d grad batch size, %d was given"" % (self.inData.shape[0], batchsize))\n\n\n\tdef checkDataType(self, dtype):\n\t\tif dtype != np.int32:\n\t\t\traise ModuleError(""Expected int32-tensor (got dtype %s)"" % dtype)\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\t\tself.outgrad = None\n\n\n\tdef calcMode(self, T):\n\t\tif Config.backend in {Config.Backend.cuda, Config.Backend.hip}:\n\t\t\tif self.calctype == T:\n\t\t\t\treturn\n\n\t\t\tvariables = self.vars\n\t\t\tself.vars = {}\n\n\t\t\tfor varName, var in variables.items():\n\t\t\t\tself.setVar(varName, Variable(\n\t\t\t\t\tvar.data.astype(T), name=var.name, grad=var.grad.astype(T) if var.grad is not None else None\n\t\t\t\t))\n\n\t\t\tself.calctype = T\n\n\t\telse:\n\t\t\tsuper().calcMode(T)\n\n\ndef unittest():\n\tfor dtype, atol in dtypesSupported():\n\t\tcalcTest(dtype, atol)\n\n\tverifyDataTest()\n\n\ndef calcTest(dtype, atol):\n\tbatchsize, sentlength, embsize = 10, 20, 40\n\tvocabsize = 1000\n\n\thostData = np.random.randint(low=-1, high=vocabsize, size=(batchsize, sentlength), dtype=np.int32)\n\tdata = gpuarray.to_gpu(hostData)\n\n\tembedder = Embedder(vocabsize, sentlength, embsize)\n\tembedder.calcMode(dtype)\n\n\tembedder(data)\n\n\thostW = embedder.W.get()\n\thostOutData = np.zeros(embedder.data.shape, dtype=dtype)\n\n\tfor b in range(batchsize):\n\t\tfor s in range(sentlength):\n\t\t\twordidx = int(hostData[b, s])\n\n\t\t\tif wordidx != -1:\n\t\t\t\thostOutData[b, s] = hostW[wordidx]\n\n\tassert embedder.getVocabulary() == {}\n\tassert np.allclose(hostOutData, embedder.data.get())\n\n\thostGrad = np.random.randn(*embedder.data.shape).astype(dtype)\n\tgrad = gpuarray.to_gpu(hostGrad)\n\n\tembedder.backward(grad)\n\n\tlearnRate = 1e-1\n\tembedder.updateParams(learnRate)\n\n\tfor b in range(batchsize):\n\t\tfor s in range(sentlength):\n\t\t\twordidx = int(hostData[b, s])\n\n\t\t\tif wordidx != -1:\n\t\t\t\thostW[wordidx] += learnRate * hostGrad[b, s]\n\n\tassert np.allclose(hostW, embedder.W.get(), atol=atol)\n\n\tembedder.save(""../TestData/embedder.hdf"")\n\tembedder = Embedder(vocabsize, sentlength, embsize)\n\tembedder.load(""../TestData/embedder.hdf"")\n\n\tassert np.allclose(hostW, embedder.W.get(), atol=atol)\n\tos.remove(""../TestData/embedder.hdf"")\n\n\ndef verifyDataTest():\n\tbatchsize, sentlength, embsize = 10, 20, 40\n\tvocabsize = 1000\n\n\thostData = np.random.randint(low=-1, high=vocabsize, size=(batchsize, sentlength), dtype=np.int32)\n\thostData[-1, -1] = vocabsize\n\n\tdata = gpuarray.to_gpu(hostData)\n\tembedder = Embedder(vocabsize, sentlength, embsize)\n\n\tConfig.verifyData = True\n\n\ttry:\n\t\tembedder(data)\n\texcept ModuleError as e:\n\t\tprint(""Caught data verification error: %s"" % e)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Flatten.py,4,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Modules.Module import Module\n\n\nclass Flatten(Module):\n\tdef __init__(self, name=None):\n\t\tsuper().__init__(name)\n\n\t\tself.movesData = True\n\t\tself.movesGrad = True\n\n\t\tself.inshape = None\n\n\n\tdef updateData(self, data):\n\t\tself.inshape = data.shape\n\t\tself.data = data.reshape(data.shape[0], int(np.prod(data.shape[1:])))\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = grad.reshape(self.inshape)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape[0], int(np.prod(shape[1:]))\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn (shape[0], ) + self.inshape[1:]\n\n\n\tdef calcMode(self, T):\n\t\tself.calctype = T\n\n\ndef unittest():\n\tdata = gpuarray.to_gpu(np.random.randn(10, 10, 10, 10).astype(np.float32))\n\n\tflatten = Flatten()\n\tflatten(data)\n\n\tshape = (10, 1000)\n\tassert flatten.data.shape == shape\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*flatten.data.shape).astype(np.float32))\n\tflatten.backward(grad)\n\n\tassert flatten.grad.shape == data.shape\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Gelu.py,6,"b'import math\n\nimport numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import dtypesSupported, memoryPool as memPool\nfrom PuzzleLib.Backend.Kernels.ElementWise import geluKer, geluDerKer\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass Gelu(Module):\n\tdef __init__(self, inplace=False, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.inplace = inplace\n\t\tif inplace and Config.showWarnings:\n\t\t\tprint(""[%s] Warning: %s is using inplace flag"" % (Config.libname, self))\n\n\n\tdef updateData(self, data):\n\t\tself.data = data if self.inplace else gpuarray.empty(data.shape, dtype=data.dtype, allocator=memPool)\n\t\tgeluKer(data.dtype)(self.data, data)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = grad if self.inplace else gpuarray.empty(grad.shape, dtype=grad.dtype, allocator=memPool)\n\t\tgeluDerKer(grad.dtype)(self.grad, grad, self.inData)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef calcMode(self, T):\n\t\tdtypes = {dtype for dtype, _ in dtypesSupported()}\n\n\t\tif T not in dtypes:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n\n\ndef unittest():\n\tfor dtype, atol in dtypesSupported():\n\t\tgeluTest(dtype, atol)\n\n\ndef geluTest(dtype, atol):\n\tgelu = Gelu()\n\tgelu.calcMode(dtype)\n\n\thostData = np.random.randn(11, 51).astype(dtype)\n\n\tdata = gpuarray.to_gpu(hostData)\n\tgelu(data)\n\n\terf = np.vectorize(math.erf)\n\thostOutData = 0.5 * hostData * (1.0 + erf(hostData / math.sqrt(2)))\n\n\tassert np.allclose(hostOutData, gelu.data.get(), atol=atol)\n\n\thostGrad = np.random.randn(*gelu.data.shape).astype(dtype)\n\n\tgrad = gpuarray.to_gpu(hostGrad)\n\tgelu.backward(grad)\n\n\thostInGrad = hostGrad * (0.5 * (1.0 + erf(hostData / math.sqrt(2))) +\n\t\t\t\t hostData / math.sqrt(math.pi) * np.exp(-0.5 * hostData**2))\n\tassert np.allclose(hostInGrad, gelu.grad.get(), atol=atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Glue.py,6,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass Glue(Module):\n\tdef __init__(self, modules=None, fwdGlue=None, bwdGlue=None, fwdShapeGlue=None, bwdShapeGlue=None, name=None):\n\t\tsuper().__init__(name)\n\n\t\tif modules is not None and not isinstance(modules, dict):\n\t\t\traise ModuleError(""Modules object must be non-empty dictionary"")\n\n\t\tself.modules = modules\n\n\t\tself.fwdGlue = fwdGlue\n\t\tself.bwdGlue = bwdGlue\n\n\t\tself.fwdShapeGlue = fwdShapeGlue\n\t\tself.bwdShapeGlue = bwdShapeGlue\n\n\n\tdef updateData(self, data):\n\t\tself.data = self.fwdGlue(data, self.modules)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = self.bwdGlue(grad, self.modules)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tif self.fwdShapeGlue is not None:\n\t\t\treturn self.fwdShapeGlue(shape)\n\t\telse:\n\t\t\traise ModuleError(""Forward shape glue hook is not installed"")\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tif self.bwdShapeGlue is not None:\n\t\t\treturn self.bwdShapeGlue(shape)\n\t\telse:\n\t\t\traise ModuleError(""Backward shape glue hook is not installed"")\n\n\ndef unittest():\n\tdata1 = gpuarray.to_gpu(np.random.randn(10, 2, 3, 3).astype(np.float32))\n\tdata2 = gpuarray.to_gpu(np.random.randn(10, 2, 3, 3).astype(np.float32))\n\tdata3 = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\n\tdef fwdGlue(data, modules):\n\t\tdat1, dat2, dat3 = data\n\t\tsplit = modules[""split""]\n\t\tout1, out2 = split(data3)\n\n\t\treturn [dat1 + dat2, out1, out2]\n\n\tdef bwdGlue(grad, modules):\n\t\tgr1, gr2, gr3 = grad\n\t\tsplit = modules[""split""]\n\t\tsplit.backward([gr2, gr3])\n\n\t\treturn [gr1, gr1, split.grad]\n\n\tfrom PuzzleLib.Modules.Split import Split\n\tglue = Glue(fwdGlue=fwdGlue, bwdGlue=bwdGlue, modules={""split"": Split(axis=1, sections=(5, 5))})\n\tglue([data1, data2, data3])\n\n\tgrad1 = gpuarray.to_gpu(np.random.randn(*glue.data[0].shape).astype(np.float32))\n\tgrad2 = gpuarray.to_gpu(np.random.randn(*glue.data[1].shape).astype(np.float32))\n\tgrad3 = gpuarray.to_gpu(np.random.randn(*glue.data[2].shape).astype(np.float32))\n\n\tglue.backward([grad1, grad2, grad3])\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/GroupLinear.py,63,"b'from enum import Enum\n\nimport numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import copy\nfrom PuzzleLib.Backend.Kernels.MatVec import addVecToMat\nfrom PuzzleLib.Backend.Kernels.MatVecBatch import addVecToMatBatch\nfrom PuzzleLib.Backend import Blas, BlasGroup\n\nfrom PuzzleLib.Variable import Variable\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass GroupMode(str, Enum):\n\tfull = ""full""\n\tone = ""one""\n\n\nclass GroupLinear(Module):\n\tdef __init__(self, groups, insize, outsize, wscale=1.0, useW=True, useBias=True, initscheme=None,\n\t\t\t\t inmode=""full"", wmode=""full"", batchDim=0, name=None, empty=False, transpW=False):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tif not(useW or useBias):\n\t\t\traise ModuleError(""Not using W and bias is not supported"")\n\n\t\tself.transpW = transpW\n\t\tself.useW = useW\n\t\tself.useBias = useBias\n\n\t\tself.inmode = GroupMode(inmode)\n\t\tself.wmode = GroupMode(wmode)\n\n\t\tif batchDim == 0:\n\t\t\tself.format = ""bgp""\n\t\telif batchDim == 1:\n\t\t\tself.format = ""gbp""\n\t\telse:\n\t\t\traise ModuleError(""Unsupported batch dimension"")\n\n\t\tself.groupDim = 1 if batchDim == 0 else 0\n\t\tself.groups = 1 if groups is None else groups\n\n\t\tself.W = None\n\t\tself.b = None\n\n\t\tif empty:\n\t\t\treturn\n\n\t\tself.setupW(insize, outsize, initscheme, wscale)\n\t\tself.setupBias(insize, outsize)\n\n\n\tdef setupW(self, insize, outsize, initscheme, wscale):\n\t\tif not self.useW:\n\t\t\treturn\n\n\t\tasize, bsize = (outsize, insize) if self.transpW else (insize, outsize)\n\t\tgroups = self.groups if self.wmode == GroupMode.full else 1\n\n\t\tWshape = (groups, asize, bsize)\n\n\t\tW = self.createTensorWithScheme(initscheme, Wshape, wscale, factorShape=(asize, bsize))\n\t\tW = gpuarray.empty(Wshape, dtype=np.float32) if W is None else gpuarray.to_gpu(W)\n\n\t\tself.setVar(""W"", Variable(W))\n\n\n\tdef setupBias(self, insize, outsize):\n\t\tif not self.useBias:\n\t\t\treturn\n\n\t\tsize = outsize if self.useW else insize\n\t\tbshape = (self.groups, size) if self.wmode == GroupMode.full else (1, size)\n\n\t\tself.setVar(""b"", Variable(gpuarray.zeros(bshape, dtype=np.float32)))\n\n\n\tdef updateData(self, data):\n\t\tif self.useW:\n\t\t\tself.data = BlasGroup.mulTensorBatch(\n\t\t\t\tdata, self.W, formatA=self.format, formatB=""gbp"", transpB=self.transpW, formatOut=self.format\n\t\t\t)\n\t\telse:\n\t\t\tself.data = copy(None, data)\n\n\t\tif self.useBias:\n\t\t\tif self.groupDim == 1:\n\t\t\t\tb = self.b.reshape(int(np.prod(self.b.shape)))\n\t\t\t\toutdata = self.data.reshape(self.data.shape[0], int(np.prod(self.data.shape[1:])))\n\n\t\t\t\taddVecToMat(b, outdata, axis=1, out=outdata)\n\n\t\t\telse:\n\t\t\t\taddVecToMatBatch(self.b, self.data, axis=1, out=self.data)\n\n\n\tdef updateGrad(self, grad):\n\t\tif self.useW:\n\t\t\tformatOut = self.format if self.inmode == GroupMode.full else ""gbp""\n\n\t\t\tself.grad = BlasGroup.mulTensorBatch(\n\t\t\t\tgrad, self.W, formatA=self.format, formatB=""gbp"", transpB=not self.transpW, formatOut=formatOut\n\t\t\t)\n\n\t\t\tif self.inmode != GroupMode.full:\n\t\t\t\tself.grad = Blas.sumOnMatrix(self.grad.reshape(self.groups, grad.shape[0] * self.W.shape[1]))\n\t\t\t\tself.grad = self.grad.reshape(grad.shape[0], 1, self.W.shape[1])\n\n\t\telse:\n\t\t\tself.grad = grad\n\n\n\tdef accGradParams(self, grad, scale=1.0, momentum=0.0):\n\t\tif self.wmode == GroupMode.full:\n\t\t\tif self.useW:\n\t\t\t\tA, B = (grad, self.inData) if self. transpW else (self.inData, grad)\n\n\t\t\t\tBlasGroup.mulTensorBatch(\n\t\t\t\t\tA, B, out=self.vars[""W""].grad, formatA=self.format, formatB=self.format,\n\t\t\t\t\tformatOut=""gbp"", transpA=True, alpha=scale, beta=momentum\n\t\t\t\t)\n\n\t\t\tif self.useBias:\n\t\t\t\tBlasGroup.sumOnTensorGroup(grad, out=self.vars[""b""].grad, formatT=self.format)\n\n\t\telse:\n\t\t\tif self.useW:\n\t\t\t\tA, B = (grad, self.inData) if self.transpW else (self.inData, grad)\n\n\t\t\t\twgrad = BlasGroup.mulTensorBatch(\n\t\t\t\t\tA, B, transpA=True, formatA=self.format, formatB=self.format, formatOut=""gbp"",\n\t\t\t\t\talpha=scale, beta=momentum\n\t\t\t\t)\n\n\t\t\t\tBlas.sumOnMatrix(wgrad.reshape(wgrad.shape[0], -1), out=self.vars[""W""].grad.ravel())\n\n\t\t\tif self.useBias:\n\t\t\t\tBlas.sumOnMatrix(grad.reshape(grad.shape[0] * grad.shape[1], grad.shape[2]), out=self.vars[""b""].grad[0])\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tgroups = shape[self.groupDim] if self.inmode == GroupMode.full else self.groups\n\t\tbeg = (shape[0], groups) if self.groupDim == 1 else (groups, shape[1])\n\n\t\tif self.useW:\n\t\t\treturn beg + (self.W.shape[1], ) if self.transpW else beg + (self.W.shape[2], )\n\t\telse:\n\t\t\treturn beg + (shape[2], )\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 3:\n\t\t\traise ModuleError(""Data must be 3d tensor"")\n\n\t\tif self.inmode == GroupMode.one and shape[1] != 1:\n\t\t\traise ModuleError(""Expected 1 group in data, %d were given"" % (shape[1]))\n\n\t\tif self.inmode != GroupMode.one and self.wmode != GroupMode.one and shape[self.groupDim] != self.groups:\n\t\t\traise ModuleError(""Expected %d groups in data, %d were given"" % (self.groups, shape[self.groupDim]))\n\n\t\tif self.useW:\n\t\t\tif self.transpW and shape[2] != self.W.shape[2]:\n\t\t\t\traise ModuleError(""Expected %d data dimensions, %d were given"" % (self.W.shape[2], shape[2]))\n\n\t\t\telif shape[2] != self.W.shape[1]:\n\t\t\t\traise ModuleError(""Expected %d data dimensions, %d were given"" % (self.W.shape[1], shape[2]))\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tbeg = (shape[0], self.groups) if self.groupDim == 1 else (self.groups, shape[1])\n\t\tonebeg = (shape[0], 1) if self.groupDim == 1 else (1, shape[1])\n\n\t\tif self.useW:\n\t\t\tsize = self.W.shape[2 if self.transpW else 1]\n\t\t\treturn beg + (size, ) if self.inmode == GroupMode.full else onebeg + (size, )\n\n\t\telse:\n\t\t\treturn beg + (shape[2], ) if self.inmode == GroupMode.full else onebeg + (shape[2], )\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 3:\n\t\t\traise ModuleError(""Grad must be 3d tensor"")\n\n\t\tif self.wmode == GroupMode.full and shape[self.groupDim] != self.groups:\n\t\t\traise ModuleError(""Expected %d groups in grad, %d were given"" % (self.groups, shape[self.groupDim]))\n\n\t\tif self.useW:\n\t\t\tif self.transpW and shape[2] != self.W.shape[1]:\n\t\t\t\traise ModuleError(""Expected %d grad dimensions, %d were given"" % (self.W.shape[1], shape[2]))\n\n\t\t\telif shape[2] != self.W.shape[2]:\n\t\t\t\traise ModuleError(""Expected %d grad dimensions, %d were given"" % (self.W.shape[2], shape[2]))\n\n\ndef unittest():\n\tstdCalcTest()\n\toneInCalcTest()\n\toneWCalcTest()\n\tbatchDimTest()\n\ttrainTest()\n\n\ndef stdCalcTest():\n\tgroups, insize, outsize = 2, 5, 4\n\tbatchsize = 3\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, groups, insize).astype(np.float32))\n\n\tgrpLinear = GroupLinear(groups, insize, outsize)\n\tgrpLinear.b.fill(0.5)\n\tgrpLinear(data)\n\n\thostOutData = np.empty(grpLinear.data.shape, dtype=np.float32)\n\tfor i in range(groups):\n\t\thostOutData[:, i, :] = np.dot(data.get()[:, i, :], grpLinear.W.get()[i])\n\thostOutData += grpLinear.b.get()\n\n\tassert np.allclose(hostOutData, grpLinear.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(batchsize, groups, outsize).astype(np.float32))\n\tgrpLinear.backward(grad)\n\n\thostInGrad = np.empty(grpLinear.grad.shape, dtype=np.float32)\n\tfor i in range(groups):\n\t\thostInGrad[:, i, :] = np.dot(grad.get()[:, i, :], grpLinear.W.get()[i].T)\n\n\tassert np.allclose(hostInGrad, grpLinear.grad.get())\n\n\thostWGrad = np.empty(grpLinear.W.shape, dtype=np.float32)\n\tfor i in range(groups):\n\t\thostWGrad[i] = np.dot(data.get()[:, i, :].T, grad.get()[:, i, :])\n\n\thostBGrad = np.empty(grpLinear.b.shape, dtype=np.float32)\n\tfor i in range(groups):\n\t\thostBGrad[i] = np.sum(grad.get()[:, i, :], axis=0)\n\n\tassert np.allclose(hostWGrad, grpLinear.vars[""W""].grad.get())\n\tassert np.allclose(hostBGrad, grpLinear.vars[""b""].grad.get())\n\n\ndef oneInCalcTest():\n\tbatchsize, insize, outsize = 4, 5, 3\n\tgroups = 4\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, 1, insize).astype(np.float32))\n\n\tgrpLinear = GroupLinear(groups, insize, outsize, inmode=""one"")\n\tgrpLinear(data)\n\n\thostOutData = np.empty(grpLinear.data.shape, dtype=np.float32)\n\tfor i in range(groups):\n\t\thostOutData[:, i, :] = np.dot(data.get()[:, 0, :], grpLinear.W.get()[i])\n\thostOutData += grpLinear.b.get()[np.newaxis, :, :]\n\n\tassert np.allclose(hostOutData, grpLinear.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(batchsize, groups, outsize).astype(np.float32))\n\tgrpLinear.backward(grad)\n\n\thostInGrad = np.zeros(data.shape, dtype=np.float32)\n\tfor i in range(groups):\n\t\thostInGrad[:, 0, :] += np.dot(grad.get()[:, i, :], grpLinear.W.get()[i].T)\n\n\tassert np.allclose(hostInGrad, grpLinear.grad.get())\n\n\thostWGrad = np.empty(grpLinear.W.shape, dtype=np.float32)\n\tfor i in range(groups):\n\t\thostWGrad[i] = np.dot(data.get()[:, 0, :].T, grad.get()[:, i, :])\n\n\thostBGrad = np.empty(grpLinear.b.shape, dtype=np.float32)\n\tfor i in range(groups):\n\t\thostBGrad[i] = np.sum(grad.get()[:, i, :], axis=0)\n\n\tassert np.allclose(hostWGrad, grpLinear.vars[""W""].grad.get())\n\tassert np.allclose(hostBGrad, grpLinear.vars[""b""].grad.get())\n\n\ndef oneWCalcTest():\n\tbatchsize, insize, outsize = 4, 3, 4\n\tgroups = 3\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, groups, insize).astype(np.float32))\n\n\tgrpLinear = GroupLinear(None, insize, outsize, wmode=""one"")\n\tgrpLinear(data)\n\n\thostOutData = np.empty(grpLinear.data.shape, dtype=np.float32)\n\tfor i in range(groups):\n\t\thostOutData[:, i, :] = np.dot(data.get()[:, i, :], grpLinear.W.get()[0])\n\thostOutData += grpLinear.b.get()[np.newaxis, :, :]\n\n\tassert np.allclose(hostOutData, grpLinear.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(batchsize, groups, outsize).astype(np.float32))\n\tgrpLinear.backward(grad)\n\n\thostInGrad = np.empty(grpLinear.grad.shape, dtype=np.float32)\n\tfor i in range(groups):\n\t\thostInGrad[:, i, :] = np.dot(grad.get()[:, i, :], grpLinear.W.get()[0].T)\n\n\tassert np.allclose(hostInGrad, grpLinear.grad.get())\n\n\thostWGrad = np.zeros(grpLinear.W.shape, dtype=np.float32)\n\tfor i in range(groups):\n\t\thostWGrad += np.dot(data.get()[:, i, :].T, grad.get()[:, i, :])\n\n\thostBGrad = np.sum(grad.get().reshape(batchsize * groups, outsize), axis=0)\n\n\tassert np.allclose(hostWGrad, grpLinear.vars[""W""].grad.get())\n\tassert np.allclose(hostBGrad, grpLinear.vars[""b""].grad.get())\n\n\ndef batchDimTest():\n\tgroups, insize, outsize = 2, 5, 4\n\tbatchsize = 3\n\n\tdata = gpuarray.to_gpu(np.random.randn(groups, batchsize, insize).astype(np.float32))\n\n\tgrpLinear = GroupLinear(groups, insize, outsize, batchDim=1)\n\tgrpLinear.b.fill(0.5)\n\tgrpLinear(data)\n\n\thostOutData = np.empty(grpLinear.data.shape, dtype=np.float32)\n\tfor i in range(groups):\n\t\thostOutData[i] = np.dot(data.get()[i], grpLinear.W.get()[i])\n\n\tfor i in range(batchsize):\n\t\thostOutData[:, i, :] += grpLinear.b.get()\n\n\tassert np.allclose(hostOutData, grpLinear.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(groups, batchsize, outsize).astype(np.float32))\n\tgrpLinear.backward(grad)\n\n\thostInGrad = np.empty(grpLinear.grad.shape, dtype=np.float32)\n\tfor i in range(groups):\n\t\thostInGrad[i] = np.dot(grad.get()[i], grpLinear.W.get()[i].T)\n\n\tassert np.allclose(hostInGrad, grpLinear.grad.get())\n\n\thostWGrad = np.empty(grpLinear.W.shape, dtype=np.float32)\n\tfor i in range(groups):\n\t\thostWGrad[i] = np.dot(data.get()[i].T, grad.get()[i])\n\n\thostBGrad = np.empty(grpLinear.b.shape, dtype=np.float32)\n\tfor i in range(groups):\n\t\thostBGrad[i] = np.sum(grad.get()[i], axis=0)\n\n\tassert np.allclose(hostWGrad, grpLinear.vars[""W""].grad.get())\n\tassert np.allclose(hostBGrad, grpLinear.vars[""b""].grad.get())\n\n\ndef trainTest():\n\tgroups, insize, outsize = 16, 128, 32\n\tbatchsize = 32\n\n\tdata = gpuarray.to_gpu(np.random.normal(0.0, 1.0, (batchsize, groups, insize)).astype(np.float32))\n\ttarget = gpuarray.to_gpu(np.random.normal(0.0, 1.0, (batchsize, groups, outsize)).astype(np.float32))\n\n\tgrpLinear = GroupLinear(groups, insize, outsize)\n\n\tfrom PuzzleLib.Cost.MSE import MSE\n\tmse = MSE()\n\n\tfor i in range(100):\n\t\tlearnRate = 1e-1\n\n\t\tgrpLinear(data)\n\t\terror, grad = mse(grpLinear.data, target)\n\n\t\tgrpLinear.backward(grad)\n\t\tgrpLinear.updateParams(learnRate)\n\n\t\tif (i + 1) % 5 == 0:\n\t\t\tprint(""Iteration #%d error: %s"" % (i + 1, error))\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Identity.py,2,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Modules.Module import Module\n\n\nclass Identity(Module):\n\tdef __init__(self, name=None):\n\t\tsuper().__init__(name)\n\n\t\tself.movesData = True\n\t\tself.movesGrad = True\n\n\n\tdef updateData(self, data):\n\t\tself.data = data\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = grad\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef calcMode(self, T):\n\t\tself.calctype = T\n\n\ndef unittest():\n\tdata = gpuarray.to_gpu(np.random.normal(0.0, 0.01, (10, 3, 40, 40)).astype(np.float32))\n\n\tidentity = Identity()\n\tidentity(data)\n\n\tassert np.allclose(data.get(), identity.data.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/InstanceNorm2D.py,17,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend import Blas\nfrom PuzzleLib.Backend.Dnn.InstanceNorm import instanceNorm2d, instanceNorm2dBackward\n\nfrom PuzzleLib.Variable import Variable\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass InstanceNorm2D(Module):\n\tdef __init__(self, numOfMaps, epsilon=1e-5, affine=True, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.numOfMaps = numOfMaps\n\t\tself.epsilon = epsilon\n\n\t\tself.affine = affine\n\n\t\tshape = (1, numOfMaps, 1, 1)\n\t\tscale = np.ones(shape, dtype=np.float32)\n\n\t\tself.scale = None\n\t\tself.bias = None\n\n\t\tself.setVar(""scale"", Variable(gpuarray.to_gpu(scale)))\n\t\tself.setVar(""bias"", Variable(gpuarray.zeros(shape, dtype=np.float32)))\n\n\t\tself.savemean, self.saveinvvar, self.extscale, self.scalegrad, self.biasgrad = None, None, None, None, None\n\n\n\tdef updateData(self, data):\n\t\tself.data, self.savemean, self.saveinvvar, self.extscale = instanceNorm2d(\n\t\t\tdata, self.scale, self.bias, self.epsilon\n\t\t)\n\n\n\tdef updateGrad(self, grad):\n\t\tif self.affine:\n\t\t\tself.grad, self.scalegrad, self.biasgrad = instanceNorm2dBackward(\n\t\t\t\tgrad, self.inData, self.extscale, self.savemean, self.saveinvvar, self.epsilon, True\n\t\t\t)\n\t\telse:\n\t\t\tself.grad = instanceNorm2dBackward(\n\t\t\t\tgrad, self.inData, self.extscale, self.savemean, self.saveinvvar, self.epsilon, False\n\t\t\t)\n\n\n\tdef accGradParams(self, grad, scale=1.0, momentum=0.0):\n\t\tif self.affine:\n\t\t\tBlas.addVectorToVector(\n\t\t\t\tself.scalegrad.ravel(), self.vars[""scale""].grad.ravel(),\n\t\t\t\tout=self.vars[""scale""].grad.ravel(), alpha=scale, beta=momentum\n\t\t\t)\n\t\t\tBlas.addVectorToVector(\n\t\t\t\tself.biasgrad.ravel(), self.vars[""bias""].grad.ravel(),\n\t\t\t\tout=self.vars[""bias""].grad.ravel(), alpha=scale, beta=momentum\n\t\t\t)\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 4:\n\t\t\traise ModuleError(""Data must be 4d tensor"")\n\n\n\tdef checkGradShape(self, shape):\n\t\tif shape != self.data.shape:\n\t\t\traise ModuleError(""Inconsistency in grad shape - expected %s (%s given)"" % (self.data.shape, shape))\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\t\tself.savemean, self.saveinvvar, self.extscale = None, None, None\n\n\t\tif self.affine:\n\t\t\tself.scalegrad, self.biasgrad = None, None\n\n\n\tdef calcMode(self, T):\n\t\tif Config.backend == Config.Backend.cuda:\n\t\t\tif T not in {np.float16, np.float32}:\n\t\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\telif T != np.float32:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n\n\ndef unittest():\n\tbatchsize, maps, h, w = 5, 3, 4, 4\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, h, w).astype(np.float32))\n\n\tinstNorm2d = InstanceNorm2D(maps, affine=True)\n\tinstNorm2d(data)\n\n\thostData = data.get().reshape(data.shape[0] * data.shape[1], -1)\n\thostVar = np.var(hostData, axis=1)\n\thostInvVar = np.ones(hostData.shape[0], dtype=np.float32) / np.sqrt(hostVar + instNorm2d.epsilon)\n\thostOutData = (hostData - np.mean(hostData, axis=1, keepdims=True)) * hostInvVar[:, np.newaxis]\n\n\tassert np.allclose(instNorm2d.data.get(), hostOutData.reshape(data.shape))\n\tassert np.allclose(\n\t\tinstNorm2d.saveinvvar.get().ravel(), hostVar if Config.backend == Config.Backend.intel else hostInvVar\n\t)\n\n\tgrad = gpuarray.to_gpu(np.random.randn(batchsize, maps, h, w).astype(np.float32))\n\tinstNorm2d.backward(grad)\n\n\thostGrad = grad.get().reshape(grad.shape[0] * grad.shape[1], -1)\n\thostCorrs = np.empty(shape=hostInvVar.shape, dtype=np.float32)\n\tfor i in range(hostCorrs.shape[0]):\n\t\thostCorrs[i] = np.dot(hostGrad[i], hostOutData[i]) / hostGrad.shape[1]\n\n\thostInGrad = hostGrad - np.mean(hostGrad, axis=1, keepdims=True) - \\\n\t\t\t\t hostCorrs[:, np.newaxis] * instNorm2d.data.get().reshape(hostOutData.shape)\n\thostInGrad *= hostInvVar[:, np.newaxis]\n\n\tassert np.allclose(hostInGrad.reshape(grad.shape), instNorm2d.grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/KMaxPool.py,16,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass KMaxPool(Module):\n\tdef __init__(self, topk, axis, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.topk = topk\n\t\tself.axis = axis\n\t\tself.indices = None\n\n\n\t@staticmethod\n\tdef sliceAlongAxis(ary, axis, start, end):\n\t\tary = np.rollaxis(ary, axis)[start:end]\n\t\treturn np.rollaxis(ary, 0, axis + 1)\n\n\n\t@staticmethod\n\tdef getIndicesSlice(ary, axis, indices):\n\t\ttup = ()\n\n\t\tfor i in range(ary.ndim):\n\t\t\tif i == axis:\n\t\t\t\ttup += (indices, )\n\t\t\telse:\n\t\t\t\tshape = tuple([None] * i + [slice(None)] + [None] * (ary.ndim - 1 - i))\n\t\t\t\tr = np.arange(ary.shape[i])[shape]\n\n\t\t\t\ttup += (r, )\n\n\t\treturn tup\n\n\n\tdef sliceWithIndicesAlongAxis(self, ary, axis, indices):\n\t\treturn ary[self.getIndicesSlice(ary, axis, indices)]\n\n\n\tdef fillSliceWithIndicesAlongAxis(self, ary, axis, indices, data):\n\t\tary[self.getIndicesSlice(ary, axis, indices)] = data\n\n\n\tdef updateData(self, data):\n\t\tdata = data.get()\n\n\t\tindices = np.argpartition(data, -self.topk, axis=self.axis)\n\t\tindices = self.sliceAlongAxis(indices, self.axis, -self.topk, None)\n\n\t\ttopkData = self.sliceWithIndicesAlongAxis(data, self.axis, indices)\n\n\t\ttopkIndices = np.argsort(topkData, axis=self.axis)\n\t\ttopkData = self.sliceWithIndicesAlongAxis(topkData, self.axis, topkIndices)\n\n\t\tself.indices = self.sliceWithIndicesAlongAxis(indices, self.axis, topkIndices)\n\t\tself.data = gpuarray.to_gpu(topkData, allocator=memPool)\n\n\n\tdef updateGrad(self, grad):\n\t\tgrad = grad.get()\n\n\t\tingrad = np.zeros(self.inData.shape, dtype=np.float32)\n\t\tself.fillSliceWithIndicesAlongAxis(ingrad, self.axis, self.indices, grad)\n\n\t\tself.grad = gpuarray.to_gpu(ingrad, allocator=memPool)\n\n\n\tdef checkDataShape(self, shape):\n\t\tif self.axis >= len(shape):\n\t\t\traise ModuleError(""Data dimension needs to be at least %d, (data has %d)"" % (self.axis + 1, len(shape)))\n\n\t\tif shape[self.axis] < self.topk:\n\t\t\traise ModuleError(""Data topk axis is too small (got %d, expected at least %d)"" %\n\t\t\t\t\t\t\t  (shape[self.axis], self.topk))\n\n\n\tdef checkGradShape(self, shape):\n\t\tif self.axis >= len(shape):\n\t\t\traise ModuleError(""Grad dimension needs to be at least %d, (grad has %d)"" % (self.axis + 1, len(shape)))\n\n\t\tif shape[self.axis] != self.topk:\n\t\t\traise ModuleError(""Grad topk axis is wrong (got %d, expected exactly %d)"" % (shape[self.axis], self.topk))\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape[:self.axis] + (self.topk, ) + shape[self.axis + 1:]\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn shape[:self.axis] + self.inData.shape[self.axis] + shape[self.axis + 1:]\n\n\ndef unittest():\n\ttopk = 5\n\taxis = 2\n\n\tdata = gpuarray.to_gpu(np.random.randn(32, 10, 16).astype(np.float32))\n\n\tkmaxpool = KMaxPool(topk=topk, axis=axis)\n\tkmaxpool(data)\n\n\thostData = data.get()\n\n\thostOutData = np.partition(hostData, -topk, axis=axis)[:, :, -topk:]\n\thostIndices = np.argpartition(hostData, -topk, axis=axis)[:, :, -topk:]\n\n\thostInIndices = np.argsort(hostOutData, axis=axis)\n\n\ttup = (np.arange(hostOutData.shape[0])[:,None,None], np.arange(hostOutData.shape[1])[None,:,None], hostInIndices)\n\thostIndices = hostIndices[tup]\n\thostOutData = hostOutData[tup]\n\n\tassert np.allclose(kmaxpool.data.get(), hostOutData)\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*data.shape[:axis], topk).astype(np.float32))\n\tkmaxpool.backward(grad)\n\n\thostGrad = grad.get()\n\thostInGrad = np.zeros(hostData.shape, dtype=np.float32)\n\n\ttup = (np.arange(hostInGrad.shape[0])[:, None, None], np.arange(hostInGrad.shape[1])[None, :, None], hostIndices)\n\thostInGrad[tup] = hostGrad\n\n\tassert np.allclose(hostInGrad, kmaxpool.grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/LCN.py,11,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray, Blas\nfrom PuzzleLib.Backend.Dnn.Basic import PoolMode, poolNd, poolNdBackward, mapLRN, mapLRNBackward\n\nfrom PuzzleLib.Modules.Module import ModuleError\nfrom PuzzleLib.Modules.LRN import LRN\n\n\nclass LCN(LRN):\n\tdef __init__(self, N=5, alpha=1e-4, beta=0.75, K=2.0, includePad=True, name=None):\n\t\tsuper().__init__(N, alpha, beta, K, name)\n\t\tself.registerBlueprint(locals())\n\n\t\tif N % 2 != 1 or N == 1:\n\t\t\traise ModuleError(""LCN size must be odd and > 1"")\n\n\t\tself.size = self.repeat(N, 2)\n\t\tself.pad = (self.size[0] // 2, self.size[1] // 2)\n\n\t\tself.gradUsesOutData = Config.backend != Config.Backend.cuda\n\n\t\tself.includePad = includePad\n\t\tself.mode = PoolMode.avgWithPad if includePad else PoolMode.avgNoPad\n\n\t\tself.means = None\n\t\tself.poolspace = None\n\t\tself.lrnspace = None\n\n\n\tdef updateData(self, data):\n\t\tself.means, self.poolspace = poolNd(data, size=self.size, stride=1, pad=self.pad, mode=self.mode,\n\t\t\t\t\t\t\t\t\t\t\ttest=not self.train)\n\t\tself.data, self.lrnspace = mapLRN(data, self.means, N=self.N, alpha=self.alpha, beta=self.beta, K=self.K,\n\t\t\t\t\t\t\t\t\t\t  test=not self.train)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad, meansGrad = mapLRNBackward(self.inData, self.data, grad, self.means, None,\n\t\t\t\t\t\t\t\t\t\t\t  N=self.N, alpha=self.alpha, beta=self.beta, K=self.K)\n\n\t\tif self.includePad:\n\t\t\tmeansGrad = poolNdBackward(self.inData, self.means, meansGrad, self.workspace, size=self.size, stride=1,\n\t\t\t\t\t\t\t\t\t   pad=self.pad, mode=self.mode)\n\t\t\tBlas.addVectorToVector(self.grad.ravel(), meansGrad.ravel(), out=self.grad.ravel(), beta=-1.0)\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\t\tself.means = None\n\t\tself.poolspace = None\n\t\tself.lrnspace = None\n\n\ndef unittest():\n\tbatchsize, maps, h, w = 1, 1, 5, 5\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, h, w).astype(np.float32))\n\n\tlcn = LCN(N=5)\n\tlcn(data)\n\n\tlookBehind = int((lcn.N - 1) / 2)\n\tlookAhead = lcn.N - lookBehind\n\n\thsize, wsize = lcn.size\n\thpad, wpad = lcn.pad\n\n\thostData = np.zeros(shape=(batchsize, maps, h + 2 * hpad, w + 2 * wpad), dtype=np.float32)\n\thostData[:, :, hpad:-hpad, wpad:-wpad] = data.get()\n\n\thostMeans = np.empty(lcn.data.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor y in range(lcn.data.shape[2]):\n\t\t\t\tfor x in range(lcn.data.shape[3]):\n\t\t\t\t\thostMeans[b, c, y, x] = np.sum(hostData[b, c, y:y + hsize, x:x + wsize]) / (hsize * wsize)\n\n\tassert np.allclose(hostMeans, lcn.means.get())\n\tnorms = np.empty(lcn.data.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor y in range(h):\n\t\t\t\tfor x in range(w):\n\t\t\t\t\tnorm = 0.0\n\n\t\t\t\t\tfor dy in range(max(0, y - lookBehind), min(h, y + lookAhead)):\n\t\t\t\t\t\tfor dx in range(max(0, x - lookBehind), min(w, x + lookAhead)):\n\t\t\t\t\t\t\tnorm += (hostData[b, c, dy, dx] - hostMeans[b, c, y, x])**2\n\n\t\t\t\t\tnorms[b, c, y, x] = lcn.K + norm * lcn.alpha / lcn.N**2\n\n\thostOutData = hostData[:, :, hpad:-hpad, wpad:-wpad] / (norms**lcn.beta)\n\tassert np.allclose(hostOutData, lcn.data.get(), atol=1e-5)\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*lcn.data.shape).astype(np.float32))\n\tlcn.backward(grad)\n\n\thostGrad = grad.get()\n\thostInGrad, hostMeansGrad = np.zeros(data.shape, dtype=np.float32), np.zeros(data.shape, dtype=np.float32)\n\n\tk = 2.0 * lcn.alpha * lcn.beta / lcn.N**2\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor y in range(h):\n\t\t\t\tfor x in range(w):\n\t\t\t\t\thostInGrad[b, c, y, x] += hostGrad[b, c, y, x] / norms[b, c, y, x]**lcn.beta\n\n\t\t\t\t\tfor dy in range(max(0, y - lookBehind), min(h, y + lookAhead)):\n\t\t\t\t\t\tfor dx in range(max(0, x - lookBehind), min(w, x + lookAhead)):\n\t\t\t\t\t\t\thostInGrad[b, c, y, x] -= k * hostGrad[b, c, dy, dx] * (\n\t\t\t\t\t\t\t\t\thostData[b, c, y, x] - hostMeans[b, c, dy, dx]\n\t\t\t\t\t\t\t) * hostData[b, c, dy, dx] / norms[b, c, dy, dx]**(lcn.beta + 1)\n\n\t\t\t\t\t\t\thostMeansGrad[b, c, y, x] += hostData[b, c, dy, dx] - hostMeans[b, c, y, x]\n\n\t\t\t\t\tK = 2.0 * lcn.alpha * lcn.beta * hostData[b, c, y, x] / lcn.N**2 / \\\n\t\t\t\t\t\t   norms[b, c, y, x]**(lcn.beta + 1)\n\n\t\t\t\t\thostMeansGrad[b, c, y, x] *= K * hostGrad[b, c, y, x]\n\n\textInGrad = np.zeros(hostData.shape, dtype=np.float32)\n\textInGrad[:, :, hpad:-hpad, wpad:-wpad] = hostInGrad\n\n\thostInGrad = extInGrad\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor y in range(hostGrad.shape[2]):\n\t\t\t\tfor x in range(hostGrad.shape[3]):\n\t\t\t\t\tfor dy in range(lcn.N):\n\t\t\t\t\t\tfor dx in range(lcn.N):\n\t\t\t\t\t\t\thostInGrad[b, c, y + dy, x + dx] -= hostMeansGrad[b, c, y, x] / lcn.N**2\n\n\tassert np.allclose(hostInGrad[:, :, hpad:-hpad, wpad:-wpad], lcn.grad.get(), atol=1e-4)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/LRN.py,0,"b'from PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass LRN(Module):\n\tdef __init__(self, N=5, alpha=1e-4, beta=0.75, K=2.0, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.N = N\n\t\tself.alpha = alpha\n\t\tself.beta = beta\n\t\tself.K = K\n\n\t\tself.workspace = None\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 4:\n\t\t\traise ModuleError(""Data must be 4d tensor"")\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 4:\n\t\t\traise ModuleError(""Grad must be 4d tensor"")\n\n\n\tdef updateData(self, data):\n\t\traise NotImplementedError()\n\n\n\tdef updateGrad(self, grad):\n\t\traise NotImplementedError()\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\t\tself.workspace = None\n'"
Modules/Linear.py,14,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray, Blas\nfrom PuzzleLib.Backend.Kernels import MatVec\nfrom PuzzleLib.Backend.Utils import dtypesSupported\n\nfrom PuzzleLib.Variable import Variable\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass Linear(Module):\n\tdef __init__(self, insize, outsize, wscale=1.0, useBias=True, initscheme=None, name=None,\n\t\t\t\t empty=False, transpose=False):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.transpose = transpose\n\t\tself.useBias = useBias\n\n\t\tself.W = None\n\t\tself.b = None\n\n\t\tif empty:\n\t\t\treturn\n\n\t\tWshape, bshape = ((outsize, insize), (insize, )) if transpose else ((insize, outsize), (outsize, ))\n\t\tW = self.createTensorWithScheme(initscheme, Wshape, wscale, factorShape=Wshape)\n\n\t\tself.setVar(""W"", Variable(gpuarray.empty(Wshape, dtype=self.calctype) if W is None else gpuarray.to_gpu(W)))\n\n\t\tif useBias:\n\t\t\tself.setVar(""b"", Variable(gpuarray.zeros(bshape, dtype=self.calctype)))\n\n\n\tdef updateData(self, data):\n\t\tself.data = Blas.mulMatrixOnMatrix(data, self.W, transpB=self.transpose)\n\n\t\tif self.useBias:\n\t\t\tMatVec.addVecToMat(self.b, self.data, axis=1, out=self.data)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = Blas.mulMatrixOnMatrix(grad, self.W, transpB=not self.transpose)\n\n\n\tdef accGradParams(self, grad, scale=1.0, momentum=0.0):\n\t\tif not self.transpose:\n\t\t\tBlas.mulMatrixOnMatrix(self.inData, grad, out=self.vars[""W""].grad, transpA=True, alpha=scale, beta=momentum)\n\t\telse:\n\t\t\tBlas.mulMatrixOnMatrix(grad, self.inData, out=self.vars[""W""].grad, transpA=True, alpha=scale, beta=momentum)\n\n\t\tif self.useBias:\n\t\t\tBlas.sumOnMatrix(grad, out=self.vars[""b""].grad, alpha=scale, beta=momentum)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn (shape[0], self.W.shape[1]) if not self.transpose else (shape[0], self.W.shape[0])\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 2:\n\t\t\traise ModuleError(""Data must be 2d matrix"")\n\n\t\tif not self.transpose:\n\t\t\tif shape[1] != self.W.shape[0]:\n\t\t\t\traise ModuleError(""Expected %d data dimensions, %d were given"" % (self.W.shape[0], shape[1]))\n\t\telse:\n\t\t\tif shape[1]!= self.W.shape[1]:\n\t\t\t\traise ModuleError(""Expected %d data dimensions, %d were given"" % (self.W.shape[1], shape[1]))\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn (shape[0], self.W.shape[0]) if not self.transpose else (shape[0], self.W.shape[1])\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 2:\n\t\t\traise ModuleError(""Grad must be 2d matrix"")\n\n\t\tif not self.transpose:\n\t\t\tif shape[1] != self.W.shape[1]:\n\t\t\t\traise ModuleError(""Expected %d grad dimensions, %d were given"" % (self.W.shape[1], shape[1]))\n\t\telse:\n\t\t\tif shape[1] != self.W.shape[0]:\n\t\t\t\traise ModuleError(""Expected %d grad dimensions, %d were given"" % (self.W.shape[0], shape[1]))\n\n\n\tdef calcMode(self, T):\n\t\tif Config.backend in {Config.Backend.cuda, Config.Backend.hip}:\n\t\t\tif self.calctype == T:\n\t\t\t\treturn\n\n\t\t\tvariables = self.vars\n\t\t\tself.vars = {}\n\n\t\t\tfor varName, var in variables.items():\n\t\t\t\tself.setVar(varName, Variable(\n\t\t\t\t\tvar.data.astype(T), name=var.name, grad=var.grad.astype(T) if var.grad is not None else None\n\t\t\t\t))\n\n\t\t\tself.calctype = T\n\n\t\telse:\n\t\t\tsuper().calcMode(T)\n\n\ndef unittest():\n\tfor dtype, atol in dtypesSupported():\n\t\tcalcTest(dtype, atol)\n\t\ttrainTest(dtype)\n\n\ndef calcTest(dtype, atol):\n\tinsize, outsize = 5, 1\n\n\thostData = np.random.randn(5, insize).astype(dtype)\n\tdata = gpuarray.to_gpu(hostData)\n\n\tlinear = Linear(insize, outsize, initscheme=(""xavier"", ""avg""))\n\tlinear.calcMode(dtype)\n\n\tlinear(data)\n\n\thostGrad = np.random.randn(*linear.data.shape).astype(dtype)\n\tgrad = gpuarray.to_gpu(hostGrad)\n\n\tlinear.backward(grad)\n\n\thostW, hostBias = linear.W.get(), linear.b.get()\n\n\thostOutData = np.dot(hostData, hostW) + hostBias[np.newaxis, :]\n\thostInGrad = np.dot(hostGrad, hostW.T)\n\n\thostWGrad = np.dot(hostData.T, hostGrad)\n\thostBiasGrad = np.sum(hostGrad, axis=0)\n\n\tassert np.allclose(hostOutData, linear.data.get(), atol=atol)\n\tassert np.allclose(hostInGrad, linear.grad.get(), atol=atol)\n\n\tassert np.allclose(hostWGrad, linear.vars[""W""].grad.get(), atol=atol)\n\tassert np.allclose(hostBiasGrad, linear.vars[""b""].grad.get(), atol=atol)\n\n\ndef trainTest(dtype):\n\tinsize, outsize = 500, 100\n\n\thostData = np.random.randn(32, insize).astype(dtype)\n\thostTarget = np.random.randn(32, outsize).astype(np.float32)\n\n\tdata, target = gpuarray.to_gpu(hostData), gpuarray.to_gpu(hostTarget)\n\n\tlinear = Linear(insize, outsize)\n\tlinear.calcMode(dtype)\n\n\tfrom PuzzleLib.Cost.MSE import MSE\n\tmse = MSE()\n\n\tlearnRate = 1e-1\n\n\tfor i in range(100):\n\t\tlinear(data)\n\n\t\toutdata = linear.data if dtype == np.float32 else linear.data.astype(np.float32)\n\t\terror, grad = mse(outdata, target)\n\n\t\tlinear.backward(grad if dtype == np.float32 else grad.astype(dtype))\n\t\tlinear.updateParams(learnRate)\n\n\t\tif (i + 1) % 5 == 0:\n\t\t\tprint(""Iteration #%d error: %s"" % (i + 1, error))\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/MapLRN.py,8,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Dnn.Basic import mapLRN, mapLRNBackward\n\nfrom PuzzleLib.Modules.LRN import LRN\n\n\nclass MapLRN(LRN):\n\tdef __init__(self, N=5, alpha=1e-4, beta=0.75, K=2.0, name=None):\n\t\tsuper().__init__(N, alpha, beta, K, name)\n\n\t\tif Config.backend != Config.Backend.cuda:\n\t\t\tself.gradUsesOutData = True\n\n\n\tdef updateData(self, data):\n\t\tself.data, self.workspace = mapLRN(data, None, N=self.N, alpha=self.alpha, beta=self.beta, K=self.K,\n\t\t\t\t\t\t\t\t\t\t   test=not self.train)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = mapLRNBackward(self.inData, self.data, grad, None, self.workspace,\n\t\t\t\t\t\t\t\t   N=self.N, alpha=self.alpha, beta=self.beta, K=self.K)\n\n\ndef unittest():\n\th, w = 10, 10\n\tdata = gpuarray.to_gpu(np.random.randn(1, 1, h, w).astype(np.float32))\n\n\tmapLrn = MapLRN()\n\tmapLrn(data)\n\n\tlookBehind = int((mapLrn.N - 1) / 2)\n\tlookAhead = mapLrn.N - lookBehind\n\n\thostData = data.get().reshape(h, w).astype(np.float32)\n\tnorms = np.empty((h, w), dtype=np.float32)\n\tfor i in range(h):\n\t\tfor j in range(w):\n\t\t\tnorm = 0.0\n\t\t\tfor m in range(max(0, i - lookBehind), min(h, i + lookAhead)):\n\t\t\t\tfor n in range(max(0, j - lookBehind), min(w, j + lookAhead)):\n\t\t\t\t\tnorm += hostData[m, n]**2\n\t\t\tnorms[i, j] = mapLrn.K + norm * mapLrn.alpha / mapLrn.N**2\n\n\thostOutData = hostData / norms**mapLrn.beta\n\tassert np.allclose(hostOutData, mapLrn.data.get()[0, 0])\n\n\tgrad = gpuarray.to_gpu(np.random.randn(1, 1, h, w).astype(np.float32))\n\tmapLrn.backward(grad)\n\n\thostGrad = grad.get().reshape(h, w).astype(np.float32)\n\thostInGrad = np.zeros((h, w), dtype=np.float32)\n\n\tk = 2.0 * mapLrn.alpha * mapLrn.beta / mapLrn.N**2\n\tfor i in range(h):\n\t\tfor j in range(w):\n\t\t\thostInGrad[i, j] += hostGrad[i, j] / norms[i, j]**mapLrn.beta\n\n\t\t\tfor m in range(max(0, i - lookBehind), min(h, i + lookAhead)):\n\t\t\t\tfor n in range(max(0, j - lookBehind), min(w, j + lookAhead)):\n\t\t\t\t\thostInGrad[i, j] -= k*hostGrad[m, n]*hostData[i, j]*hostData[m, n]/norms[m, n]**(mapLrn.beta+1)\n\n\tassert np.allclose(hostInGrad, mapLrn.grad.get()[0, 0])\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/MaxPool1D.py,8,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Dnn.Basic import PoolMode, poolNd, poolNdBackward\nfrom PuzzleLib.Backend.Kernels import Pool\n\nfrom PuzzleLib.Modules.Pool1D import Pool1D\n\n\nclass MaxPool1D(Pool1D):\n\tdef __init__(self, size=2, stride=2, pad=0, useMask=False, name=None):\n\t\tsuper().__init__(size, stride, pad, name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.useMask = useMask\n\t\tself.mask = None\n\n\t\tself.mode = PoolMode.max\n\n\n\t@property\n\tdef withMask(self):\n\t\treturn self.useMask\n\n\n\t@withMask.setter\n\tdef withMask(self, val):\n\t\tself.useMask = val\n\t\tself.gradUsesOutData = False if val else True\n\n\n\tdef updateData(self, data):\n\t\tdata = data.reshape(*data.shape[:2], 1, *data.shape[2:])\n\n\t\tif self.useMask:\n\t\t\tself.data, self.mask = Pool.maxpool2d(data, size=self.size, stride=self.stride, pad=self.pad)\n\t\telse:\n\t\t\tself.data, self.workspace = poolNd(data, size=self.size, stride=self.stride, pad=self.pad,\n\t\t\t\t\t\t\t\t\t\t\t   test=not self.train, mode = self.mode)\n\n\t\tself.data = self.data.reshape(*self.data.shape[:2], *self.data.shape[3:])\n\n\n\tdef updateGrad(self, grad):\n\t\tgrad = grad.reshape(*grad.shape[:2], 1, *grad.shape[2:])\n\n\t\tindata = self.inData.reshape(*self.inData.shape[:2], 1, *self.inData.shape[2:])\n\t\toutdata = self.data.reshape(*self.data.shape[:2], 1, *self.data.shape[2:])\n\n\t\tif self.useMask:\n\t\t\tself.grad = Pool.maxpool2dBackward(grad, indata.shape, self.mask,\n\t\t\t\t\t\t\t\t\t\t\t   size=self.size, stride=self.stride, pad=self.pad)\n\t\telse:\n\t\t\tself.grad = poolNdBackward(indata, outdata, grad, self.workspace,\n\t\t\t\t\t\t\t\t\t   size=self.size, stride=self.stride, pad=self.pad, mode=self.mode)\n\n\t\tself.grad = self.grad.reshape(*self.grad.shape[:2], *self.grad.shape[3:])\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\t\tself.mask = None\n\n\ndef unittest():\n\tbatchsize, maps, insize = 1, 1, 6\n\tsize, stride, pad = 3, 2, 1\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, insize).astype(np.float32))\n\n\tmaxpool1d = MaxPool1D(size=size, stride=stride, pad=pad)\n\tmaxpool1d(data)\n\n\thostData = np.full(shape=(batchsize, maps, insize + 2 * pad), fill_value=np.finfo(np.float32).min, dtype=np.float32)\n\thostData[:, :, pad:-pad] = data.get()\n\thostOutData = np.empty(maxpool1d.data.shape)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor x in range(hostOutData.shape[2]):\n\t\t\t\t\t\thostOutData[b, c, x] = np.max(hostData[b, c, x * stride:x * stride + size])\n\n\tassert np.allclose(hostOutData, maxpool1d.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*maxpool1d.data.shape).astype(np.float32))\n\tmaxpool1d.backward(grad)\n\n\thostGrad = grad.get()\n\thostInGrad = np.zeros(hostData.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor x in range(hostOutData.shape[2]):\n\t\t\t\tfor dx in range(size):\n\t\t\t\t\tif hostData[b, c, x * stride + dx] == hostOutData[b, c, x]:\n\t\t\t\t\t\thostInGrad[b, c, x * stride + dx] += hostGrad[b, c, x]\n\n\tassert np.allclose(hostInGrad[:, :, pad:-pad], maxpool1d.grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/MaxPool2D.py,4,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Kernels import Pool\nfrom PuzzleLib.Backend.Dnn.Basic import PoolMode, poolNd, poolNdBackward\n\nfrom PuzzleLib.Modules.Pool2D import Pool2D\n\n\nclass MaxPool2D(Pool2D):\n\tdef __init__(self, size=2, stride=2, pad=0, useMask=False, name=None):\n\t\tsuper().__init__(size, stride, pad, name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.useMask = useMask\n\t\tself.mask = None\n\n\t\tself.mode = PoolMode.max\n\n\n\t@property\n\tdef withMask(self):\n\t\treturn self.useMask\n\n\n\t@withMask.setter\n\tdef withMask(self, val):\n\t\tself.useMask = val\n\t\tself.gradUsesOutData = False if val else True\n\n\n\tdef updateData(self, data):\n\t\tif self.useMask:\n\t\t\tself.data, self.mask = Pool.maxpool2d(data, size=self.size, stride=self.stride, pad=self.pad)\n\t\telse:\n\t\t\ttest = not self.train\n\t\t\tself.data, self.workspace = poolNd(data, size=self.size, stride=self.stride, pad=self.pad,\n\t\t\t\t\t\t\t\t\t\t\t   mode=self.mode, test=test)\n\n\n\tdef updateGrad(self, grad):\n\t\tif self.useMask:\n\t\t\tself.grad = Pool.maxpool2dBackward(grad, self.inData.shape, self.mask,\n\t\t\t\t\t\t\t\t\t\t\t   size=self.size, stride=self.stride, pad=self.pad)\n\t\telse:\n\t\t\tself.grad = poolNdBackward(self.inData, self.data, grad, self.workspace,\n\t\t\t\t\t\t\t\t\t   size=self.size, stride=self.stride, pad=self.pad, mode=self.mode)\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\t\tself.mask = None\n\n\ndef unittest():\n\tbatchsize, maps, h, w = 1, 1, 6, 6\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, h, w).astype(np.float32))\n\n\tmaxpool2d = MaxPool2D()\n\tmaxpool2d(data)\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*maxpool2d.data.shape).astype(np.float32))\n\tmaxpool2d.backward(grad)\n\n\tdef maxDownSample2d(dat, factor):\n\t\ttrimrows = dat.shape[0] // factor * factor\n\t\ttrimcols = dat.shape[1] // factor * factor\n\n\t\tmaxSoFar = None\n\t\tfirst = True\n\n\t\tfor coff in range(factor):\n\t\t\tfor roff in range(factor):\n\t\t\t\thopped = dat[roff:trimrows:factor, coff:trimcols:factor]\n\t\t\t\tif first:\n\t\t\t\t\tmaxSoFar = hopped\n\t\t\t\t\tfirst = False\n\t\t\t\telse:\n\t\t\t\t\tmaxSoFar = np.maximum(maxSoFar, hopped)\n\n\t\treturn maxSoFar\n\n\thostOutData = maxDownSample2d(data.get()[0, 0], 2)\n\tassert np.allclose(hostOutData, maxpool2d.data.get()[0, 0])\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/MaxPool3D.py,9,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Dnn.Basic import PoolMode, poolNd, poolNdBackward\n\nfrom PuzzleLib.Modules.Pool3D import Pool3D\n\n\nclass MaxPool3D(Pool3D):\n\tdef __init__(self, size=2, stride=2, pad=0, name=None):\n\t\tsuper().__init__(size, stride, pad, name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.mode = PoolMode.max\n\n\n\tdef updateData(self, data):\n\t\tself.data, self.workspace = poolNd(data, size=self.size, stride=self.stride, pad=self.pad, mode=self.mode,\n\t\t\t\t\t\t\t\t\t\t   test=not self.train)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = poolNdBackward(self.inData, self.data, grad, self.workspace,\n\t\t\t\t\t\t\t\t   size=self.size, stride=self.stride, pad=self.pad, mode=self.mode)\n\n\ndef unittest():\n\tbatchsize, maps, t, h, w = 1, 1, 6, 6, 6\n\tsize, stride, pad = 3, 2, 1\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, t, h, w).astype(np.float32))\n\n\tmaxpool3d = MaxPool3D(size=size, stride=stride, pad=pad)\n\tmaxpool3d(data)\n\n\thostData = np.full(shape=(batchsize, maps, t + 2 * pad, h + 2 * pad, w + 2 * pad),\n\t\t\t\t\t   fill_value=np.finfo(np.float32).min, dtype=np.float32)\n\thostData[:, :, pad:-pad, pad:-pad, pad:-pad] = data.get()\n\thostOutData = np.empty(maxpool3d.data.shape)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor z in range(hostOutData.shape[2]):\n\t\t\t\tfor y in range(hostOutData.shape[3]):\n\t\t\t\t\tfor x in range(hostOutData.shape[4]):\n\t\t\t\t\t\thostOutData[b, c, z, y, x] = np.max(hostData[b, c, z * stride:z * stride + size,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ty * stride:y * stride + size, x * stride:x * stride + size])\n\n\tassert np.allclose(hostOutData, maxpool3d.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*maxpool3d.data.shape).astype(np.float32))\n\tmaxpool3d.backward(grad)\n\n\thostGrad = grad.get()\n\thostInGrad = np.zeros(hostData.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor z in range(hostOutData.shape[2]):\n\t\t\t\tfor y in range(hostOutData.shape[3]):\n\t\t\t\t\tfor x in range(hostOutData.shape[4]):\n\t\t\t\t\t\tfor dz in range(size):\n\t\t\t\t\t\t\tfor dy in range(size):\n\t\t\t\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\t\t\t\tif hostData[b,c,z*stride+dz,y*stride + dy,x*stride + dx] == hostOutData[b,c,z,y,x]:\n\t\t\t\t\t\t\t\t\t\thostInGrad[b,c,z*stride + dz,y*stride + dy,x*stride + dx] += hostGrad[b,c,z,y,x]\n\n\tassert np.allclose(hostInGrad[:, :, pad:-pad, pad:-pad, pad:-pad], maxpool3d.grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/MaxUnpool2D.py,7,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Kernels import Pool\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\nfrom PuzzleLib.Modules.MaxPool2D import MaxPool2D\n\n\nclass MaxUnpool2D(Module):\n\tdef __init__(self, maxpool2d, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals(), exclude=[""maxpool2d""])\n\n\t\tself.maxpool2d = maxpool2d\n\t\tself.maxpool2d.withMask = True\n\n\n\tdef updateData(self, data):\n\t\tself.data = Pool.maxunpool2d(data, self.maxpool2d.inData.shape, self.maxpool2d.mask)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = Pool.maxunpool2dBackward(grad, self.maxpool2d.data.shape, self.maxpool2d.mask)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tbatchsize, maps, inh, inw = shape\n\n\t\thsize, wsize = self.maxpool2d.size\n\t\tpadh, padw = self.maxpool2d.pad\n\t\thstride, wstride = self.maxpool2d.stride\n\n\t\touth = (inh - 1) * hstride - 2 * padh + hsize\n\t\toutw = (inw - 1) * wstride - 2 * padw + wsize\n\n\t\treturn batchsize, maps, outh, outw\n\n\n\tdef checkDataShape(self, shape):\n\t\tif shape != self.maxpool2d.mask.shape:\n\t\t\traise ModuleError(""Data shape (current %s) must be equal to connected MaxPool2D mask shape (%s)"" %\n\t\t\t\t\t\t\t  (shape, self.maxpool2d.mask.shape))\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tbatchsize, maps, outh, outw = shape\n\n\t\thsize, wsize = self.maxpool2d.size\n\t\tpadh, padw = self.maxpool2d.pad\n\t\thstride, wstride = self.maxpool2d.stride\n\n\t\tinh = (outh + 2 * padh - hsize) // hstride + 1\n\t\tinw = (outw + 2 * padw - wsize) // wstride + 1\n\n\t\treturn batchsize, maps, inh, inw\n\n\n\tdef checkGradShape(self, shape):\n\t\tif shape != self.maxpool2d.inData.shape:\n\t\t\traise ModuleError(""Grad shape (current %s) must be equal to connected MaxPool2D data shape (%s)"" %\n\t\t\t\t\t\t\t  (shape, self.maxpool2d.inData.shape))\n\n\ndef unittest():\n\tbatchsize, maps, h, w = 15, 3, 4, 5\n\tindata = gpuarray.to_gpu(np.random.randn(batchsize, maps, h, w).astype(np.float32))\n\n\tmaxpool2d = MaxPool2D()\n\tmaxunpool2d = MaxUnpool2D(maxpool2d)\n\n\tmaxpool2d(indata)\n\n\tdata = gpuarray.to_gpu(np.random.randn(*maxpool2d.data.shape).astype(np.float32))\n\tmaxunpool2d(data)\n\n\thostPoolData = data.get()\n\thostMask = maxpool2d.mask.get()\n\n\thostOutData = np.zeros(maxpool2d.inData.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor y in range(maxpool2d.data.shape[2]):\n\t\t\t\tfor x in range(maxpool2d.data.shape[3]):\n\t\t\t\t\tmaxidx = hostMask[b, c, y, x]\n\t\t\t\t\thostOutData[b, c].ravel()[maxidx] = hostPoolData[b, c, y, x]\n\n\tassert np.allclose(hostOutData, maxunpool2d.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*maxunpool2d.data.shape).astype(np.float32))\n\tmaxunpool2d.backward(grad)\n\n\thostGrad = grad.get()\n\thostInGrad = np.empty(maxunpool2d.grad.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor y in range(maxpool2d.data.shape[2]):\n\t\t\t\tfor x in range(maxpool2d.data.shape[3]):\n\t\t\t\t\tmaxidx = hostMask[b, c, y, x]\n\t\t\t\t\thostInGrad[b, c, y, x] = hostGrad[b, c].ravel()[maxidx]\n\n\tassert np.allclose(hostInGrad, maxunpool2d.grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Module.py,13,"b'import warnings, json, tempfile, math, os\nfrom enum import Enum\n\nimport numpy as np\nimport h5py\nfrom h5py import h5p, h5f\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import Blas, gpuarray\nfrom PuzzleLib.Variable import Variable\n\n\nclass ModuleError(Exception):\n\tpass\n\n\nclass InitScheme(str, Enum):\n\tnone = ""none""\n\txavier = ""xavier""\n\txavierUniform = ""xavier_uniform""\n\txavierNormal = ""xavier_normal""\n\the = ""he""\n\tgaussian = ""gaussian""\n\tuniform = ""uniform""\n\n\nclass FactorType(str, Enum):\n\tin_ = ""in""\n\tout = ""out""\n\tavg = ""avg""\n\n\nclass MemoryUnit(str, Enum):\n\tmb = ""mb""\n\tkb = ""kb""\n\n\nclass Module:\n\t__slots__ = [\n\t\t""name"", ""blueprint"",\n\t\t""vars"", ""attrs"",\n\t\t""gradUsesOutData"", ""movesData"", ""movesGrad"",\n\t\t""grad"", ""inData"", ""data"",\n\t\t""train"", ""calctype"",\n\t\t""varLoader"", ""attrLoader""\n\t]\n\n\n\tdef __init__(self, name=None):\n\t\tself.name = name\n\n\t\tself.blueprint = None\n\t\tself.registerBlueprint(locals())\n\n\t\tself.vars = {}\n\t\tself.attrs = {}\n\n\t\tself.gradUsesOutData = False\n\t\tself.movesData = False\n\t\tself.movesGrad = False\n\n\t\tself.grad = None\n\n\t\tself.inData = None\n\t\tself.data = None\n\n\t\tself.train = False if Config.globalEvalMode else True\n\t\tself.calctype = np.float32\n\n\t\tself.varLoader = None\n\t\tself.attrLoader = None\n\n\n\tdef registerBlueprint(self, args, exclude=None):\n\t\texclude = set() if exclude is None else exclude\n\t\tignore = {""self"", ""__class__""}\n\n\t\tself.blueprint = {key: None if key in exclude else arg for key, arg in args.items() if key not in ignore}\n\n\n\tdef getBlueprint(self):\n\t\treturn {""classname"": self.__class__.__name__, ""scheme"": self.blueprint}\n\n\n\tdef setVar(self, name, var):\n\t\tsetattr(self, name, var.data)\n\t\tself.vars[name] = var\n\n\n\tdef getVar(self, name):\n\t\treturn self.vars[name]\n\n\n\tdef getVarTable(self, vartable=None, name=None, root=True):\n\t\tif root and name is None:\n\t\t\tname = self.name if self.name is not None else """"\n\n\t\tvartable = {} if vartable is None else vartable\n\n\t\tfor paramName, var in self.vars.items():\n\t\t\tif var not in vartable:\n\t\t\t\tvartable[var] = []\n\n\t\t\tvartable[var].append(""%s%s"" % (name, paramName))\n\n\t\treturn vartable\n\n\n\tdef setAttr(self, name, attr):\n\t\tsetattr(self, name, attr)\n\t\tself.attrs[name] = attr\n\n\n\tdef hasAttr(self, name):\n\t\treturn name in self.attrs\n\n\n\tdef node(self, *nodes):\n\t\tfrom PuzzleLib.Containers.Node import Node\n\t\treturn Node(self, parents=None if len(nodes) == 0 else list(nodes))\n\n\n\tdef __call__(self, data):\n\t\tif not Config.disableDtypeShapeChecks:\n\t\t\tself.checkDataShape(self.acquireShapesFrom(data))\n\t\t\tself.checkDataType(self.acquireDtypesFrom(data))\n\n\t\tself.data = None\n\t\tself.inData = data\n\n\t\tself.updateData(data)\n\t\treturn self.data\n\n\n\tdef backward(self, grad, updParamGrads=True, updGrad=True, scale=1.0, momentum=0.0):\n\t\tif not Config.disableDtypeShapeChecks:\n\t\t\tshape = self.acquireShapesFrom(grad)\n\t\t\tself.checkGradShape(shape)\n\n\t\t\tdtype = self.acquireDtypesFrom(grad)\n\t\t\tself.checkGradType(dtype)\n\n\t\tself.grad = None\n\n\t\tif updGrad:\n\t\t\tself.updateGrad(grad)\n\n\t\tif updParamGrads and self.train:\n\t\t\tself.accGradParams(grad, scale=scale, momentum=momentum)\n\n\n\tdef updateData(self, data):\n\t\traise NotImplementedError()\n\n\n\tdef updateGrad(self, grad):\n\t\traise NotImplementedError()\n\n\n\tdef zeroGradParams(self):\n\t\tfor var in self.vars.values():\n\t\t\tif var.hasUpdater:\n\t\t\t\tcontinue\n\n\t\t\tvar.grad.fill(0)\n\n\n\tdef accGradParams(self, grad, scale=1.0, momentum=0.0):\n\t\tpass\n\n\n\tdef updateParams(self, learnRate):\n\t\tfor var in self.vars.values():\n\t\t\tBlas.toVectorAddVector(var.data.ravel(), var.grad.ravel(), alpha=learnRate)\n\n\n\tdef optimizeForShape(self, shape, memlimit=None):\n\t\tpass\n\n\n\tdef save(self, hdf=None, varlinks=None, name=None, compress=""gzip"", assumeUniqueNames=False,\n\t\t\t withBlueprint=False, isRoot=True):\n\t\tserialize = True if hdf is None else False\n\n\t\thdf = self.ensureHdf(hdf, ""w"")\n\t\tvarlinks = {} if varlinks is None else varlinks\n\n\t\tif name is None:\n\t\t\tname = self.name if self.name is not None else """"\n\n\t\tif assumeUniqueNames and len(name) > 0:\n\t\t\ttokens = name.split(sep=""."")\n\t\t\tname = ""%s.%s"" % (tokens[0], tokens[-1])\n\n\t\ttry:\n\t\t\tparamGrp, linkGrp = hdf.require_group(""params""), hdf.require_group(""links"")\n\n\t\t\tfor paramName, var in self.vars.items():\n\t\t\t\tif var in varlinks:\n\t\t\t\t\tidx = varlinks[var]\n\t\t\t\telse:\n\t\t\t\t\tidx = len(varlinks)\n\t\t\t\t\tparamGrp.create_dataset(str(idx), data=var.data.get(), compression=compress)\n\t\t\t\t\tvarlinks[var] = idx\n\n\t\t\t\tlinkGrp[""%s.%s"" % (name, paramName)] = idx\n\n\t\t\tif len(self.attrs) > 0:\n\t\t\t\tattrGrp = hdf.require_group(""attrs"")\n\n\t\t\t\tfor attrName, attr in self.attrs.items():\n\t\t\t\t\tattrGrp.create_dataset(\n\t\t\t\t\t\t""%s.%s"" % (name, attrName),\n\t\t\t\t\t\tdata=attr.get() if isinstance(attr, gpuarray.GPUArray) else attr, compression=compress\n\t\t\t\t\t)\n\n\t\t\tif withBlueprint:\n\t\t\t\thdf.create_dataset(\n\t\t\t\t\t""blueprint"", (), dtype=h5py.special_dtype(vlen=str),\n\t\t\t\t\tdata=json.dumps(self.getBlueprint(), indent=4, sort_keys=True)\n\t\t\t\t)\n\n\t\t\tbuffer = None\n\t\t\tif isRoot and serialize:\n\t\t\t\thdf.flush()\n\t\t\t\tbuffer = hdf.id.get_file_image()\n\n\t\texcept Exception as e:\n\t\t\traise ModuleError(""Module %s save error: %s"" % (name, e))\n\n\t\tfinally:\n\t\t\tif isRoot:\n\t\t\t\thdf.close()\n\n\t\treturn buffer\n\n\n\tdef load(self, hdf, initvars=None, name=None, assumeUniqueNames=False, isRoot=True):\n\t\thdf = self.ensureHdf(hdf, ""r"")\n\t\tinitvars = {} if initvars is None else initvars\n\n\t\tif name is None:\n\t\t\tname = self.name if self.name is not None else """"\n\n\t\tif assumeUniqueNames and len(name) > 0:\n\t\t\ttokens = name.split(sep=""."")\n\t\t\tname = ""%s.%s"" % (tokens[0], tokens[-1])\n\n\t\twith warnings.catch_warnings():\n\t\t\twarnings.filterwarnings(""error"")\n\n\t\t\ttry:\n\t\t\t\tparamGrp, linkGrp = hdf[""params""], hdf[""links""]\n\n\t\t\t\tfor paramName, var in self.vars.items():\n\t\t\t\t\tif var not in initvars:\n\t\t\t\t\t\tidx = str(linkGrp[""%s.%s"" % (name, paramName)][()])\n\t\t\t\t\t\tparam = np.array(paramGrp[idx])\n\n\t\t\t\t\t\tif self.varLoader is not None:\n\t\t\t\t\t\t\tself.varLoader(paramName, param)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tvar.data.set(param.astype(var.data.dtype, casting=""safe"", copy=False))\n\n\t\t\t\t\t\tinitvars[var] = True\n\n\t\t\t\tif len(self.attrs) > 0:\n\t\t\t\t\tattrGrp = hdf[""attrs""]\n\n\t\t\t\t\tfor attrName, attr in self.attrs.items():\n\t\t\t\t\t\tattrVal = np.array(attrGrp[""%s.%s"" % (name, attrName)])\n\n\t\t\t\t\t\tif self.attrLoader is not None:\n\t\t\t\t\t\t\tself.attrLoader(attrName, attrVal)\n\t\t\t\t\t\telif isinstance(attr, gpuarray.GPUArray):\n\t\t\t\t\t\t\tattr.set(attrVal.astype(attr.dtype, casting=""safe"", copy=False))\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tnp.copyto(attr, attrVal.astype(attr.dtype, casting=""safe"", copy=False))\n\n\t\t\texcept Exception as e:\n\t\t\t\traise ModuleError(""Module %s load error: %s"" % (name, e))\n\n\t\t\tfinally:\n\t\t\t\tif isRoot:\n\t\t\t\t\thdf.close()\n\n\n\tdef trainMode(self):\n\t\tself.train = True\n\t\tself.reset()\n\n\n\tdef evalMode(self):\n\t\tself.train = False\n\t\tself.reset()\n\n\n\tdef calcMode(self, T):\n\t\tif T != np.float32:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n\n\n\tdef reset(self):\n\t\tself.inData, self.data, self.grad = None, None, None\n\n\n\tdef checkDataShape(self, shape):\n\t\tpass\n\n\n\tdef dataShapeFrom(self, shape):\n\t\traise NotImplementedError()\n\n\n\tdef checkDataType(self, dtype):\n\t\tself.genericCheckDataType(dtype)\n\n\n\tdef checkGradShape(self, shape):\n\t\tpass\n\n\n\tdef gradShapeFrom(self, shape):\n\t\traise NotImplementedError()\n\n\n\tdef checkGradType(self, dtype):\n\t\tself.genericCheckDataType(dtype)\n\n\n\tdef genericCheckDataType(self, dtype):\n\t\tif isinstance(dtype, (tuple, list)):\n\t\t\tfor d in dtype:\n\t\t\t\tself.genericCheckDataType(d)\n\t\telse:\n\t\t\tif dtype != self.calctype:\n\t\t\t\traise ModuleError(""Expected dtype %s, got %s"" % (self.calctype, dtype))\n\n\n\tdef __str__(self):\n\t\treturn ""Module %s (name: %s)"" % (self.__class__.__name__, self.name)\n\n\n\tdef numOfParams(self):\n\t\tnParams = sum(var.data.size for var in self.vars.values())\n\t\treturn nParams\n\n\n\tdef paramSize(self, unit=None):\n\t\tsize = sum(var.data.nbytes for var in self.vars.values())\n\t\treturn self.convertUnit(size, unit=unit) if unit is not None else size\n\n\n\t@staticmethod\n\tdef convertUnit(val, unit):\n\t\tdivider = {\n\t\t\tMemoryUnit.kb: 1024,\n\t\t\tMemoryUnit.mb: 1024**2\n\t\t}[unit]\n\n\t\treturn val / divider\n\n\n\t@staticmethod\n\tdef repeat(val, ntimes):\n\t\treturn (val, ) * ntimes if isinstance(val, int) else tuple(val)\n\n\n\t@staticmethod\n\tdef ensureHdf(file, mode):\n\t\tif isinstance(file, str) or file is None:\n\t\t\tdriver, driverKwds = None, {}\n\n\t\t\tif file is None:\n\t\t\t\tfile = tempfile.mktemp(suffix="".hdf"")\n\t\t\t\tdriver, driverKwds = ""core"", {""backing_store"": False}\n\n\t\t\tdirname = os.path.dirname(os.path.abspath(file))\n\t\t\tif not os.path.exists(dirname):\n\t\t\t\tos.makedirs(dirname)\n\n\t\t\treturn h5py.File(file, mode, libver=""earliest"", driver=driver, **driverKwds)\n\n\t\telif isinstance(file, bytes):\n\t\t\tfapl = h5p.create(h5p.FILE_ACCESS)\n\t\t\tfapl.set_fapl_core()\n\t\t\tfapl.set_file_image(file)\n\n\t\t\tfid = h5f.open(tempfile.mktemp(suffix="".hdf"").encode(), h5f.ACC_RDONLY, fapl=fapl)\n\t\t\treturn h5py.File(fid)\n\n\t\telse:\n\t\t\treturn file\n\n\n\t@classmethod\n\tdef acquireShapesFrom(cls, data):\n\t\treturn [cls.acquireShapesFrom(d) for d in data] if isinstance(data, (tuple, list)) else data.shape\n\n\n\t@classmethod\n\tdef acquireDtypesFrom(cls, data):\n\t\treturn [cls.acquireDtypesFrom(d) for d in data] if isinstance(data, (tuple, list)) else data.dtype\n\n\n\t@staticmethod\n\tdef createTensorWithScheme(scheme, shape, wscale, factorShape=None, factorTranspose=False, dtype=np.float32):\n\t\tfactorType = FactorType.in_\n\n\t\tif isinstance(scheme, (tuple, list)):\n\t\t\tif len(scheme) != 2:\n\t\t\t\traise ValueError(""Scheme tuple has %s length, expected 2"" % len(scheme))\n\n\t\t\tscheme, factorType = scheme\n\n\t\tscheme = InitScheme(scheme) if scheme is not None else scheme\n\t\tfactorType = FactorType(factorType)\n\n\t\touts, ins = Module.inferNeuronsNumber(shape if factorShape is None else factorShape, factorTranspose)\n\n\t\tif factorType == FactorType.avg:\n\t\t\tfactor = (outs + ins) / 2\n\t\telif factorType == FactorType.in_:\n\t\t\tfactor = ins\n\t\telif factorType == FactorType.out:\n\t\t\tfactor = outs\n\t\telse:\n\t\t\traise NotImplementedError(factorType.value)\n\n\t\tif scheme == InitScheme.none:\n\t\t\treturn None\n\n\t\telif scheme == InitScheme.xavierUniform or scheme is None:\n\t\t\tnwscale = math.sqrt(3.0 / factor)\n\t\t\treturn np.random.uniform(-nwscale, nwscale, shape).astype(dtype)\n\n\t\telif scheme == InitScheme.xavierNormal or scheme == InitScheme.xavier:\n\t\t\tnwscale = math.sqrt(1.0 / factor)\n\t\t\treturn np.random.normal(0, nwscale, shape).astype(dtype)\n\n\t\telif scheme == InitScheme.he:\n\t\t\tnwscale = math.sqrt(2.0 / factor)\n\t\t\treturn np.random.normal(0.0, nwscale, shape).astype(dtype)\n\n\t\telif scheme == InitScheme.gaussian:\n\t\t\treturn np.random.normal(0.0, wscale, shape).astype(dtype)\n\n\t\telif scheme == InitScheme.uniform:\n\t\t\treturn np.random.uniform(-wscale, wscale, shape).astype(dtype)\n\n\t\telse:\n\t\t\traise NotImplementedError(scheme.value)\n\n\n\t@staticmethod\n\tdef inferNeuronsNumber(shape, transpose):\n\t\tndim = len(shape)\n\n\t\tif ndim == 1:\n\t\t\treturn shape[0], shape[0]\n\n\t\telif ndim == 2:\n\t\t\tneuronsIn, neuronsOut = shape\n\n\t\telse:\n\t\t\toutmaps, inmaps = shape[:2]\n\t\t\treceptiveFieldSize = int(np.prod(shape[2:]))\n\n\t\t\tneuronsOut, neuronsIn = outmaps * receptiveFieldSize, inmaps * receptiveFieldSize\n\n\t\treturn (neuronsIn, neuronsOut) if transpose else (neuronsOut, neuronsIn)\n\n\ndef unittest():\n\tclass TestModule(Module):\n\t\tdef __init__(self, name=None):\n\t\t\tsuper().__init__(name)\n\t\t\tself.setVar(""var"", Variable(gpuarray.to_gpu(np.random.randn(10).astype(self.calctype)), withgrad=False))\n\n\t\tdef updateData(self, data):\n\t\t\traise NotImplementedError()\n\n\t\tdef updateGrad(self, grad):\n\t\t\traise NotImplementedError()\n\n\t\tdef dataShapeFrom(self, shape):\n\t\t\traise NotImplementedError()\n\n\t\tdef gradShapeFrom(self, shape):\n\t\t\traise NotImplementedError()\n\n\tmodule = TestModule(name=""module"")\n\tassert module.paramSize() == module.getVar(""var"").data.nbytes\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/MoveAxis.py,6,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray, Memory\nfrom PuzzleLib.Backend.Utils import dtypesSupported\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass MoveAxis(Module):\n\tdef __init__(self, src, dst, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tif src == dst:\n\t\t\traise ModuleError(""Trivial axis move is treated as error"")\n\n\t\tself.src, self.dst = src, dst\n\n\n\tdef updateData(self, data):\n\t\tself.data = Memory.moveaxis(data, self.src, self.dst)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = Memory.moveaxis(grad, self.dst, self.src)\n\n\n\tdef checkDataShape(self, shape):\n\t\tln = max(self.src, self.dst)\n\n\t\tif len(shape) - 1 < ln:\n\t\t\traise ModuleError(""Data dimension needs to be at least %d, (data has %d)"" % (ln + 1, len(shape)))\n\n\n\tdef checkGradShape(self, shape):\n\t\tln = max(self.src, self.dst)\n\n\t\tif len(shape) - 1 < ln:\n\t\t\traise ModuleError(""Grad dimension needs to be at least %d, (grad has %d)"" % (ln + 1, len(shape)))\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tif self.src < self.dst:\n\t\t\treturn shape[:self.src] + shape[self.src + 1:self.dst + 1] + (shape[self.src], ) + shape[self.dst + 1:]\n\t\telse:\n\t\t\treturn shape[:self.dst] + (shape[self.src], ) + shape[self.dst:self.src] + shape[self.src + 1:]\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tif self.src < self.dst:\n\t\t\treturn shape[:self.src] + (shape[self.dst], ) + shape[self.src:self.dst] + shape[self.dst + 1:]\n\t\telse:\n\t\t\treturn shape[:self.dst] + shape[self.dst + 1:self.src + 1] + (shape[self.dst], ) + shape[self.src + 1:]\n\n\n\tdef calcMode(self, T):\n\t\tdtypes = {dtype for dtype, _ in dtypesSupported()}\n\n\t\tif T not in dtypes:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n\n\ndef unittest():\n\tfor dtype, _ in dtypesSupported():\n\t\tmoveAxisTest(dtype)\n\n\ndef moveAxisTest(dtype):\n\tshape = (10, 3, 5, 4, 2)\n\n\tfor src in range(len(shape)):\n\t\tfor dst in range(len(shape)):\n\t\t\tif src == dst:\n\t\t\t\tcontinue\n\n\t\t\thostData = np.random.randn(*shape).astype(dtype)\n\t\t\tdata = gpuarray.to_gpu(hostData)\n\n\t\t\tmoveaxis = MoveAxis(src, dst)\n\t\t\tmoveaxis.calcMode(dtype)\n\n\t\t\tmoveaxis(data)\n\n\t\t\thostOutData = np.moveaxis(hostData, source=src, destination=dst)\n\t\t\tassert np.allclose(hostOutData, moveaxis.data.get())\n\n\t\t\thostGrad = np.random.randn(*moveaxis.data.shape).astype(dtype)\n\t\t\tgrad = gpuarray.to_gpu(hostGrad)\n\n\t\t\tmoveaxis.backward(grad)\n\n\t\t\thostInGrad = np.moveaxis(hostGrad, source=dst, destination=src)\n\n\t\t\tassert moveaxis.grad.shape == data.shape\n\t\t\tassert np.allclose(hostInGrad, moveaxis.grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Mul.py,6,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import dtypesSupported, copy, memoryPool as memPool\nfrom PuzzleLib.Backend.Kernels.ElementWise import mulKer\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass Mul(Module):\n\tdef updateData(self, data):\n\t\tself.data = gpuarray.empty(data[0].shape, dtype=data[0].dtype, allocator=memPool)\n\t\tself.data.fill(1.0)\n\n\t\tfor dat in data:\n\t\t\tmulKer(dat.dtype)(self.data, dat, self.data)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = []\n\t\tfor i in range(len(self.inData)):\n\t\t\tingrad = copy(None, grad)\n\n\t\t\tfor k in range(len(self.inData)):\n\t\t\t\tif k != i:\n\t\t\t\t\tmulKer(ingrad.dtype)(ingrad, self.inData[k], ingrad)\n\n\t\t\tself.grad.append(ingrad)\n\n\n\tdef checkDataShape(self, shapes):\n\t\tfor shape in shapes:\n\t\t\tif shape != shapes[0]:\n\t\t\t\traise ModuleError(""Shape %s is not equal to initial shape %s"" % (shape, shapes[0]))\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn [shape] * len(self.inData)\n\n\n\tdef calcMode(self, T):\n\t\tdtypes = {dtype for dtype, _ in dtypesSupported()}\n\n\t\tif T not in dtypes:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n\n\ndef unittest():\n\tfor dtype, _ in dtypesSupported():\n\t\tmulTest(dtype)\n\n\ndef mulTest(dtype):\n\thostData1 = np.random.randn(2, 5, 5).astype(dtype)\n\thostData2 = np.random.randn(*hostData1.shape).astype(dtype)\n\n\tdata1, data2 = gpuarray.to_gpu(hostData1), gpuarray.to_gpu(hostData2)\n\n\tmul = Mul()\n\tmul.calcMode(dtype)\n\n\tmul([data1, data2])\n\tassert np.allclose(mul.data.get(), hostData1 * hostData2)\n\n\thostGrad = np.random.randn(*mul.data.shape).astype(dtype)\n\n\tgrad = gpuarray.to_gpu(hostGrad)\n\tmul.backward(grad)\n\n\tassert np.allclose(mul.grad[0].get(), hostGrad * hostData2)\n\tassert np.allclose(mul.grad[1].get(), hostGrad * hostData1)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/MulAddConst.py,5,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import dtypesSupported, memoryPool as memPool\nfrom PuzzleLib.Backend.Kernels.ElementWise import linearKer\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass MulAddConst(Module):\n\tdef __init__(self, a=1.0, b=0.0, inplace=False, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.a, self.b = a, b\n\n\t\tself.inplace = inplace\n\t\tif inplace and Config.showWarnings:\n\t\t\tprint(""[%s] Warning: %s is using inplace flag"" % (Config.libname, self))\n\n\n\tdef updateData(self, data):\n\t\tself.data = data if self.inplace else gpuarray.empty(data.shape, dtype=data.dtype, allocator=memPool)\n\t\tlinearKer(data.dtype)(self.data, data, self.a, self.b)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = grad if self.inplace else gpuarray.empty(grad.shape, dtype=grad.dtype, allocator=memPool)\n\t\tlinearKer(grad.dtype)(self.grad, grad, self.a, 0.0)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef calcMode(self, T):\n\t\tdtypes = {dtype for dtype, _ in dtypesSupported()}\n\n\t\tif T not in dtypes:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n\n\ndef unittest():\n\tfor dtype, atol in dtypesSupported():\n\t\tmulAddConstTest(dtype, atol)\n\n\ndef mulAddConstTest(dtype, atol):\n\thostData = np.random.randn(10, 10, 4, 3).astype(dtype)\n\tdata = gpuarray.to_gpu(hostData)\n\n\tmulAdd = MulAddConst(a=3.141592, b=42.0)\n\tmulAdd.calcMode(dtype)\n\n\tmulAdd(data)\n\n\thostOutData = (hostData.astype(np.float32) * mulAdd.a + mulAdd.b).astype(dtype)\n\tassert np.allclose(hostOutData, mulAdd.data.get(), atol=atol)\n\n\thostGrad = np.random.randn(*data.shape).astype(dtype)\n\tgrad = gpuarray.to_gpu(hostGrad)\n\n\tmulAdd.backward(grad)\n\n\thostInGrad = hostGrad * mulAdd.a\n\tassert np.allclose(hostInGrad, mulAdd.grad.get(), atol=atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/NoiseInjector.py,8,"b'from enum import Enum\n\nimport numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import dtypesSupported, fillUniform, fillNormal, copy, globalRng, memoryPool as memPool\nfrom PuzzleLib.Backend.Kernels.ElementWise import mulKer, addKer\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass InjectMode(str, Enum):\n\tadd = ""add""\n\tmul = ""mul""\n\n\nclass NoiseType(str, Enum):\n\tgaussian = ""gaussian""\n\tuniform = ""uniform""\n\n\nclass NoiseInjector(Module):\n\tdef __init__(self, mode=""add"", noisetype=""uniform"", params=(0.0, 1.0), rng=globalRng, inplace=False, slicing=None,\n\t\t\t\t name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals(), exclude=[""rng""])\n\n\t\tself.rng = globalRng if rng is None else rng\n\n\t\tself.mode = InjectMode(mode)\n\t\tself.type = NoiseType(noisetype)\n\n\t\tself.params = params\n\t\tself.slice = slicing\n\n\t\tself.rands = None\n\n\t\tself.inplace = inplace\n\t\tif inplace and Config.showWarnings:\n\t\t\tprint(""[%s] Warning: %s is using inplace flag"" % (Config.libname, self))\n\n\n\tdef updateData(self, data):\n\t\tif self.train:\n\t\t\tsize = data.size if data.size % 2 == 0 else data.size + 1\n\t\t\trands = gpuarray.empty((size, ), dtype=np.float32, allocator=memPool)\n\n\t\t\tif self.type == NoiseType.uniform:\n\t\t\t\ta, b = self.params\n\t\t\t\tfillUniform(rands, a, b, self.rng)\n\n\t\t\telif self.type == NoiseType.gaussian:\n\t\t\t\tmean, sigma = self.params\n\t\t\t\tfillNormal(rands, mean, sigma, self.rng)\n\n\t\t\telse:\n\t\t\t\traise NotImplementedError(self.type)\n\n\t\t\tself.rands = rands if data.dtype == np.float32 else rands.astype(data.dtype)\n\t\t\tself.rands = self.rands[:data.size].reshape(data.shape)\n\n\t\t\tif self.inplace:\n\t\t\t\tself.data = data\n\t\t\telse:\n\t\t\t\tif self.slice is not None:\n\t\t\t\t\tself.data = copy(None, data)\n\t\t\t\telse:\n\t\t\t\t\tself.data = gpuarray.empty(data.shape, dtype=data.dtype, allocator=memPool)\n\n\t\t\tif self.mode == InjectMode.add:\n\t\t\t\taddKer(data.dtype)(self.data, data, 1, self.rands, 1, slice=self.slice)\n\t\t\telif self.mode == InjectMode.mul:\n\t\t\t\tmulKer(data.dtype)(self.data, data, self.rands, slice=self.slice)\n\n\t\t\telse:\n\t\t\t\traise NotImplementedError(self.mode)\n\t\telse:\n\t\t\tself.data = data\n\n\n\tdef updateGrad(self, grad):\n\t\tif self.mode == InjectMode.mul:\n\t\t\tif self.inplace:\n\t\t\t\tself.grad = grad\n\t\t\telse:\n\t\t\t\tif self.slice is not None:\n\t\t\t\t\tself.grad = copy(None, grad)\n\t\t\t\telse:\n\t\t\t\t\tself.grad = gpuarray.empty(grad.shape, dtype=grad.dtype, allocator=memPool)\n\n\t\t\tmulKer(grad.dtype)(self.grad, grad, self.rands, slice=self.slice)\n\n\t\telif self.mode == InjectMode.add:\n\t\t\tif self.inplace:\n\t\t\t\tself.grad = grad\n\t\t\telse:\n\t\t\t\tself.grad = copy(None, grad)\n\n\t\telse:\n\t\t\traise NotImplementedError(self.mode)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\t\tself.rands = None\n\n\n\tdef calcMode(self, T):\n\t\tdtypes = {dtype for dtype, _ in dtypesSupported()}\n\n\t\tif T not in dtypes:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n\n\ndef unittest():\n\tfor dtype, _ in dtypesSupported():\n\t\tnoiseInjectorTest(dtype)\n\n\ndef noiseInjectorTest(dtype):\n\thostData = np.random.randn(10, 3, 16, 16).astype(dtype)\n\tdata = gpuarray.to_gpu(hostData)\n\n\tinjector = NoiseInjector(mode=""mul"", noisetype=""uniform"", params=(0.0, 10.0))\n\tinjector.calcMode(dtype)\n\n\tinjector(data)\n\tassert np.allclose(injector.data.get(), hostData * injector.rands.get())\n\n\thostGrad = np.random.randn(*data.shape).astype(dtype)\n\tgrad = gpuarray.to_gpu(hostGrad)\n\n\tinjector.backward(grad)\n\tassert np.allclose(injector.grad.get(), hostGrad * injector.rands.get())\n\n\tinjector = NoiseInjector(mode=""add"", noisetype=""gaussian"", params=(0.0, 1.0))\n\tinjector.calcMode(dtype)\n\n\tinjector(data)\n\tassert np.allclose(injector.data.get(), hostData + injector.rands.get())\n\n\tinjector.backward(grad)\n\tassert np.allclose(injector.grad.get(), hostGrad)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/PRelu.py,10,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray, Blas\nfrom PuzzleLib.Backend.Kernels.PRelu import prelu, preluBackwardData, preluBackwardParams\n\nfrom PuzzleLib.Variable import Variable\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass PRelu(Module):\n\tdef __init__(self, maps, inplace=False, sharedMaps=False, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.sharedMaps = sharedMaps\n\n\t\tself.inplace = inplace\n\t\tif inplace and Config.showWarnings:\n\t\t\tprint(""[%s] Warning: %s is using inplace flag"" % (Config.libname, self))\n\n\t\tshape = (1, ) if sharedMaps else (maps, )\n\t\tslopes = gpuarray.to_gpu(np.full(shape, 0.25, dtype=np.float32))\n\n\t\tself.slopes = None\n\t\tself.setVar(""slopes"", Variable(slopes))\n\n\n\tdef updateData(self, data):\n\t\tself.data = prelu(data, self.slopes, self.inplace, self.sharedMaps)\n\n\n\tdef updateGrad(self, grad):\n\t\tif self.inplace:\n\t\t\traise ModuleError(""%s: using inplace flag while calculating gradient is prohibited"" % self)\n\n\t\tself.grad = preluBackwardData(grad, self.slopes, self.inData, self.sharedMaps)\n\n\n\tdef accGradParams(self, grad, scale=1.0, momentum=0.0):\n\t\tif self.inplace:\n\t\t\traise ModuleError(""%s: using inplace flag while calculating gradient is prohibited"" % self)\n\n\t\tslopegrad = preluBackwardParams(self.inData, grad, self.sharedMaps)\n\t\tBlas.addVectorToVector(\n\t\t\tslopegrad, self.vars[""slopes""].grad, out=self.vars[""slopes""].grad, alpha=scale, beta=momentum\n\t\t)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) < 2:\n\t\t\traise ModuleError(""Data tensor dimension must be at least 2"")\n\n\t\tif not self.sharedMaps and shape[1] != self.slopes.shape[0]:\n\t\t\traise ModuleError(""Data tensor has %s maps (expected %s)"" % (shape[1], self.slopes.shape[0]))\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef checkGradShape(self, shape):\n\t\tif shape != self.inData.shape:\n\t\t\traise ModuleError(""Grad tensor has shape %s (expected %s)"" % (shape, self.inData.shape))\n\n\ndef unittest():\n\tpreluTest()\n\n\ndef preluTest():\n\tbatchsize, maps, h, w = 5, 4, 6, 6\n\n\thostData = np.random.randn(batchsize, maps, h, w).astype(np.float32)\n\tdata = gpuarray.to_gpu(hostData)\n\n\tpreluMod = PRelu(maps)\n\tpreluMod(data)\n\n\thostSlopes = preluMod.slopes.get()\n\thostOutData = np.empty(preluMod.data.shape, dtype=np.float32)\n\n\tfor c in range(maps):\n\t\thostOutData[:, c] = ((hostData[:, c] > 0.0) + (hostData[:, c] <= 0.0) * hostSlopes[c]) * hostData[:, c]\n\n\tassert np.allclose(hostOutData, preluMod.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*preluMod.data.shape).astype(np.float32))\n\tpreluMod.backward(grad)\n\n\thostGrad = grad.get()\n\thostInGrad = np.empty(preluMod.grad.shape, dtype=np.float32)\n\n\tfor c in range(maps):\n\t\thostInGrad[:, c] = hostGrad[:, c] * ((hostData[:, c] > 0.0) + (hostData[:, c] <= 0.0) * hostSlopes[c])\n\n\tassert np.allclose(hostInGrad, preluMod.grad.get())\n\n\thostSlopeGrad = np.empty(preluMod.vars[""slopes""].grad.shape, dtype=np.float32)\n\n\tfor c in range(maps):\n\t\thostSlopeGrad[c] = np.sum(hostGrad[:, c] * hostData[:, c] * (hostData[:, c] <= 0.0))\n\n\tassert np.allclose(hostSlopeGrad, preluMod.vars[""slopes""].grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Pad1D.py,14,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Kernels import Pad\nfrom PuzzleLib.Backend.Utils import dtypesSupported, memoryPool as memPool\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\nfrom PuzzleLib.Modules.Pad2D import PadMode\n\n\nclass Pad1D(Module):\n\tdef __init__(self, pad, mode=""constant"", fillValue=None, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.mode = PadMode(mode)\n\n\t\tself.pad = self.repeat(pad, 2)\n\t\tself.fillValue = 0 if fillValue is None else fillValue\n\n\n\tdef updateData(self, data):\n\t\tif self.mode == PadMode.constant:\n\t\t\tinsize = data.shape[2]\n\t\t\tlpad, rpad = self.pad\n\n\t\t\toutsize = insize + lpad + rpad\n\t\t\tself.data = gpuarray.empty(data.shape[:2] + (outsize, ), dtype=np.float32, allocator=memPool)\n\n\t\t\tself.data.fill(self.fillValue)\n\t\t\tself.data[:, :, lpad:self.data.shape[2] - rpad] = data\n\n\t\telif self.mode == PadMode.reflect:\n\t\t\tself.data = Pad.reflectpad1d(data, self.pad)\n\n\t\telse:\n\t\t\traise NotImplementedError(self.mode)\n\n\n\tdef updateGrad(self, grad):\n\t\tif self.mode == PadMode.constant:\n\t\t\tsize = grad.shape[2]\n\t\t\tlpad, rpad = self.pad\n\n\t\t\tself.grad = grad[:, :, lpad:size - rpad].copy()\n\n\t\telif self.mode == PadMode.reflect:\n\t\t\tself.grad = Pad.reflectpad1dBackward(grad, self.pad)\n\n\t\telse:\n\t\t\traise NotImplementedError(self.mode)\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 3:\n\t\t\traise ModuleError(""Data must be 3d tensor"")\n\n\t\tlpad, rpad = self.pad\n\t\tsize = shape[2]\n\n\t\tpad = max(lpad, rpad)\n\n\t\tif size < pad + 1:\n\t\t\traise ModuleError(""Data maps size is too small (got %d, expected >= %d)"" % (size, pad + 1))\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 3:\n\t\t\traise ModuleError(""Grad must be 3d tensor"")\n\n\t\tlpad, rpad = self.pad\n\t\tsize = shape[2]\n\n\t\tif size < lpad + rpad + 1:\n\t\t\traise ModuleError(""Grad maps size is too small (got %d, expected >= %d)"" % (size, lpad + rpad + 1))\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tbatchsize, maps, size = shape\n\t\tlpad, rpad = self.pad\n\n\t\treturn batchsize, maps, size + lpad + rpad\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tbatchsize, maps, size = shape\n\t\tlpad, rpad = self.pad\n\n\t\treturn batchsize, maps, size - lpad - rpad\n\n\n\tdef calcMode(self, T):\n\t\tdtypes = {dtype for dtype, _ in dtypesSupported()}\n\n\t\tif T not in dtypes:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n\n\ndef unittest():\n\tconstantTest()\n\treflectTest()\n\n\ndef constantTest():\n\tdata = gpuarray.to_gpu(np.random.randn(3, 4, 5).astype(np.float32))\n\n\tlpad, rpad = 0, 1\n\tfillValue = -0.1\n\n\tpadmod = Pad1D(pad=(lpad, rpad), mode=PadMode.constant, fillValue=fillValue)\n\tpadmod(data)\n\n\tassert padmod.dataShapeFrom(data.shape) == padmod.data.shape\n\n\thostData, hostOutData = data.get(), padmod.data.get()\n\tassert np.allclose(hostOutData[:, :, lpad:hostOutData.shape[2] - rpad], hostData)\n\n\tassert np.isclose(hostOutData[0, 0, hostOutData.shape[2] - 1], fillValue)\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*hostOutData.shape).astype(np.float32))\n\tpadmod.backward(grad)\n\n\tassert padmod.gradShapeFrom(grad.shape) == data.shape\n\tassert np.allclose(padmod.grad.get(), grad.get()[:, :, lpad:grad.shape[2] - rpad])\n\n\ndef reflectTest():\n\tbatchsize, maps, insize = 4, 8, 48\n\tlpad, rpad = 2, 3\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, insize).astype(np.float32))\n\n\treflectpad = Pad1D(pad=(lpad, rpad), mode=PadMode.reflect)\n\treflectpad(data)\n\n\thostData, hostOutData = data.get(), reflectpad.data.get()\n\toutsize = hostOutData.shape[2]\n\n\tassert np.allclose(hostOutData[:, :, lpad:insize + lpad], hostData)\n\tassert np.allclose(hostOutData[:, :, :lpad][:, :, ::-1], hostData[:, :, 1:lpad+1])\n\tassert np.allclose(hostOutData[:, :, insize + lpad:][:, :, ::-1], hostData[:, :, insize - 1 - rpad:insize - 1])\n\n\tgrad = gpuarray.to_gpu(np.random.randn(batchsize, maps, outsize).astype(np.float32))\n\treflectpad.backward(grad)\n\n\thostGrad, hostInGrad = grad.get(), reflectpad.grad.get()\n\n\tassert np.allclose(hostInGrad[:, :, lpad + 1:insize - rpad - 1],\n\t\t\t\t\t   hostGrad[:, :, 2 * lpad + 1:outsize - 2 * rpad - 1])\n\tassert np.allclose(hostInGrad[:, :, 1:lpad + 1], hostGrad[:, :, :lpad][:, :, ::-1] +\n\t\t\t\t\t   hostGrad[:, :, lpad + 1:2 * lpad + 1])\n\tassert np.allclose(hostInGrad[:, :, insize - rpad - 1:insize - 1], hostGrad[:, :, outsize - rpad:][:, :, ::-1] +\n\t\t\t\t\t   hostGrad[:, :, outsize - 2 * rpad - 1:outsize - rpad - 1])\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Pad2D.py,14,"b'from enum import Enum\n\nimport numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Kernels import Pad\nfrom PuzzleLib.Backend.Utils import dtypesSupported, memoryPool as memPool\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass PadMode(str, Enum):\n\tconstant = ""constant""\n\treflect = ""reflect""\n\n\nclass Pad2D(Module):\n\tdef __init__(self, pad, mode=""constant"", fillValue=None, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.mode = PadMode(mode)\n\n\t\tself.pad = self.repeat(pad, 4)\n\t\tself.fillValue = 0 if fillValue is None else fillValue\n\n\n\tdef updateData(self, data):\n\t\tif self.mode == PadMode.constant:\n\t\t\tinh, inw = data.shape[2:]\n\t\t\tupad, bpad, lpad, rpad = self.pad\n\n\t\t\touth, outw = inh + upad + bpad, inw + lpad + rpad\n\t\t\tself.data = gpuarray.empty(data.shape[:2] + (outh, outw), dtype=np.float32, allocator=memPool)\n\n\t\t\tself.data.fill(self.fillValue)\n\t\t\tself.data[:, :, upad:self.data.shape[2] - bpad, lpad:self.data.shape[3] - rpad] = data\n\n\t\telif self.mode == PadMode.reflect:\n\t\t\tself.data = Pad.reflectpad2d(data, self.pad)\n\n\t\telse:\n\t\t\traise NotImplementedError(self.mode)\n\n\n\tdef updateGrad(self, grad):\n\t\tif self.mode == PadMode.constant:\n\t\t\theight, width = grad.shape[2:]\n\t\t\tupad, bpad, lpad, rpad = self.pad\n\n\t\t\tself.grad = grad[:, :, upad:height - bpad, lpad:width - rpad].copy()\n\n\t\telif self.mode == PadMode.reflect:\n\t\t\tself.grad = Pad.reflectpad2dBackward(grad, self.pad)\n\n\t\telse:\n\t\t\traise NotImplementedError(self.mode)\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 4:\n\t\t\traise ModuleError(""Data must be 4d tensor"")\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 4:\n\t\t\traise ModuleError(""Grad must be 4d tensor"")\n\n\t\tupad, bpad, lpad, rpad = self.pad\n\t\theight, width = shape[2:]\n\n\t\tif height < upad + bpad + 1:\n\t\t\traise ModuleError(""Grad maps height is too small (got %d, expected >= %d)"" % (height, upad + bpad + 1))\n\n\t\tif width < lpad + rpad + 1:\n\t\t\traise ModuleError(""Grad maps width is too small (got %d, expected >= %d)"" % (width, lpad + rpad + 1))\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tbatchsize, maps, inh, inw = shape\n\t\tupad, bpad, lpad, rpad = self.pad\n\n\t\treturn batchsize, maps, inh + upad + bpad, inw + lpad + rpad\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tbatchsize, maps, outh, outw = shape\n\t\tupad, bpad, lpad, rpad = self.pad\n\n\t\treturn batchsize, maps, outh - upad - bpad, outw - lpad - rpad\n\n\n\tdef calcMode(self, T):\n\t\tdtypes = {dtype for dtype, _ in dtypesSupported()}\n\n\t\tif T not in dtypes:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n\n\ndef unittest():\n\tconstantTest()\n\treflectTest()\n\n\ndef constantTest():\n\tdata = gpuarray.to_gpu(np.random.randn(3, 4, 5, 6).astype(np.float32))\n\n\tupad, bpad, lpad, rpad = 0, 1, 0, 1\n\tfillValue = -0.1\n\n\tpadmod = Pad2D(pad=(upad, bpad, lpad, rpad), fillValue=fillValue)\n\tpadmod(data)\n\n\tassert padmod.dataShapeFrom(data.shape) == padmod.data.shape\n\n\thostData, hostOutData = data.get(), padmod.data.get()\n\tassert np.allclose(hostOutData[:, :, upad:hostOutData.shape[2] - bpad, lpad:hostOutData.shape[3] - rpad], hostData)\n\n\tassert np.isclose(hostOutData[0, 0, hostOutData.shape[2] - 1, hostOutData.shape[3] - 1], fillValue)\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*hostOutData.shape).astype(np.float32))\n\tpadmod.backward(grad)\n\n\tassert padmod.gradShapeFrom(grad.shape) == data.shape\n\tassert np.allclose(padmod.grad.get(), grad.get()[:, :, upad:grad.shape[2] - bpad, lpad:grad.shape[3] - rpad])\n\n\ndef reflectTest():\n\tbatchsize, maps, inh, inw = 4, 8, 12, 15\n\tupad, bpad, lpad, rpad = 2, 3, 2, 3\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, inh, inw).astype(np.float32))\n\n\treflectpad = Pad2D(pad=(upad, bpad, lpad, rpad), mode=PadMode.reflect)\n\treflectpad(data)\n\n\thostData, hostOutData = data.get(), reflectpad.data.get()\n\n\tassert np.allclose(hostOutData[:, :, upad:inh + upad, lpad:inw + lpad], hostData)\n\tassert np.allclose(hostOutData[:, :, :upad, :lpad][:, :, ::-1, ::-1], hostData[:, :, 1:upad + 1, 1:lpad + 1])\n\tassert np.allclose(\n\t\thostOutData[:, :, inh + upad:, inw + lpad:][:, :, ::-1, ::-1],\n\t\thostData[:, :, inh - 1 - bpad:inh - 1, inw - 1 - rpad:inw - 1]\n\t)\n\n\touth, outw = hostOutData.shape[2:]\n\n\tgrad = gpuarray.to_gpu(np.random.randn(batchsize, maps, outh, outw).astype(np.float32))\n\treflectpad.backward(grad)\n\n\thostGrad, hostInGrad = grad.get(), reflectpad.grad.get()\n\n\tassert np.allclose(\n\t\thostInGrad[:, :, upad + 1:inh - bpad - 1, lpad + 1:inw - rpad - 1],\n\t\thostGrad[:, :, 2 * upad + 1:outh - 2 * bpad - 1, 2 * lpad + 1:outw - 2 * rpad - 1]\n\t)\n\tassert np.allclose(\n\t\thostInGrad[:, :, 1:upad + 1, 1:lpad + 1],\n\t\thostGrad[:, :, :upad, :lpad][:, :, ::-1, ::-1] +\n\t\thostGrad[:, :, upad + 1:2 * upad + 1, lpad + 1:2 * lpad + 1] +\n\t\thostGrad[:, :, :upad, lpad + 1:2 * lpad + 1][:, :, ::-1, :] +\n\t\thostGrad[:, :, upad + 1:2 * upad + 1, :lpad][:, :, :, ::-1]\n\t)\n\tassert np.allclose(\n\t\thostInGrad[:, :, inh - bpad - 1:inh - 1, inw - rpad - 1:inw - 1],\n\t\thostGrad[:, :, outh - bpad:, outw - rpad:][:, :, ::-1, ::-1] +\n\t\thostGrad[:, :, outh - 2 * bpad - 1:outh - bpad - 1, outw - 2 * rpad - 1:outw - rpad - 1] +\n\t\thostGrad[:, :, outh - bpad:, outw - 2 * rpad - 1:outw - rpad - 1][:, :, ::-1, :] +\n\t\thostGrad[:, :, outh - 2 * bpad - 1:outh - bpad - 1, outw - rpad:][:, :, :, ::-1]\n\t)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Penalty.py,4,"b'from enum import Enum\n\nimport numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\nfrom PuzzleLib.Backend import Blas\nfrom PuzzleLib.Backend.Kernels.ElementWise import l1penaltyKer\n\nfrom PuzzleLib.Modules.Module import Module\n\n\nclass PenaltyMode(str, Enum):\n\tl1 = ""l1""\n\tl2 = ""l2""\n\n\nclass Penalty(Module):\n\tdef __init__(self, mode=""l1"", weight=1e-2, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.gradUsesOutData = True\n\t\tself.movesData = True\n\n\t\tself.mode = PenaltyMode(mode)\n\t\tself.weight = weight\n\n\n\tdef updateData(self, data):\n\t\tself.data = data\n\n\n\tdef updateGrad(self, grad):\n\t\tif self.mode == PenaltyMode.l1:\n\t\t\tself.grad = gpuarray.empty(grad.shape, dtype=grad.dtype, allocator=memPool)\n\t\t\tl1penaltyKer(self.grad, grad, self.data, self.weight / grad.shape[0])\n\n\t\telif self.mode == PenaltyMode.l2:\n\t\t\tself.grad = Blas.addVectorToVector(grad.ravel(), self.data.ravel(), alpha=1.0,\n\t\t\t\t\t\t\t\t\t\t\t   beta=-self.weight / grad.shape[0])\n\t\t\tself.grad = self.grad.reshape(grad.shape)\n\n\t\telse:\n\t\t\traise NotImplementedError(self.mode)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn shape\n\n\ndef unittest():\n\tdata = gpuarray.to_gpu(np.random.randn(10, 50).astype(np.float32))\n\n\tpenalty = Penalty()\n\tpenalty(data)\n\n\tgrad = gpuarray.to_gpu(np.random.randn(10, 50).astype(np.float32))\n\tpenalty.backward(grad)\n\n\thostGrad = grad.get() - penalty.weight * np.sign(data.get()) / data.shape[0]\n\tassert np.allclose(hostGrad, penalty.grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Pool1D.py,0,"b'from PuzzleLib.Backend.Utils import dtypesSupported\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass Pool1D(Module):\n\tdef __init__(self, size=2, stride=2, pad=0, name=None):\n\t\tsuper().__init__(name)\n\t\tself.gradUsesOutData = True\n\n\t\tself.size = (1, size)\n\t\tself.stride = (1, stride)\n\t\tself.pad = (0, pad)\n\n\t\tself.workspace = None\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tbatchsize, maps, insize = shape\n\n\t\t_, size = self.size\n\t\t_, pad = self.pad\n\t\t_, stride = self.stride\n\n\t\toutsize = (insize + 2 * pad - size) // stride + 1\n\n\t\treturn batchsize, maps, outsize\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 3:\n\t\t\traise ModuleError(""Data must be 3d tensor"")\n\n\t\t_, _, insize = shape\n\t\tif insize + 2 * self.pad[1] < self.size[1]:\n\t\t\traise ModuleError(""Data maps size is too small (got %d, expected at least %d)"" %\n\t\t\t\t\t\t\t  (insize + 2 * self.pad[1], self.size[1]))\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tbatchsize, maps, outsize = shape\n\n\t\t_, size = self.size\n\t\t_, pad = self.pad\n\t\t_, stride = self.stride\n\n\t\tinsize = (outsize - 1) * stride - 2 * pad + size\n\n\t\treturn batchsize, maps, insize\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 3:\n\t\t\traise ModuleError(""Grad must be 3d tensor"")\n\n\n\tdef updateData(self, data):\n\t\traise NotImplementedError()\n\n\n\tdef updateGrad(self, grad):\n\t\traise NotImplementedError()\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\t\tself.workspace = None\n\n\n\tdef calcMode(self, T):\n\t\tdtypes = {dtype for dtype, _ in dtypesSupported()}\n\n\t\tif T not in dtypes:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n'"
Modules/Pool2D.py,0,"b'from PuzzleLib.Backend.Utils import dtypesSupported\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass Pool2D(Module):\n\tdef __init__(self, size=2, stride=2, pad=0, name=None):\n\t\tsuper().__init__(name)\n\t\tself.gradUsesOutData = True\n\n\t\tself.size = self.repeat(size, 2)\n\t\tself.stride = self.repeat(stride, 2)\n\t\tself.pad = self.repeat(pad, 2)\n\n\t\tself.workspace = None\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tbatchsize, maps, inh, inw = shape\n\n\t\thsize, wsize = self.size\n\t\thpad, wpad = self.pad\n\t\thstride, wstride = self.stride\n\n\t\touth = (inh + 2 * hpad - hsize) // hstride + 1\n\t\toutw = (inw + 2 * wpad - wsize) // wstride + 1\n\n\t\treturn batchsize, maps, outh, outw\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 4:\n\t\t\traise ModuleError(""Data must be 4d tensor"")\n\n\t\t_, _, inh, inw = shape\n\t\tif inh + 2 * self.pad[0] < self.size[0]:\n\t\t\traise ModuleError(""Data maps height is too small (got %d, expected at least %d)"" %\n\t\t\t\t\t\t\t  (inh + 2 * self.pad[0], self.size[0]))\n\n\t\tif inw + 2 * self.pad[1] < self.size[1]:\n\t\t\traise ModuleError(""Data maps width is too small (got %d, expected at least %d)"" %\n\t\t\t\t\t\t\t  (inw + 2 * self.pad[1], self.size[1]))\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tbatchsize, maps, outh, outw = shape\n\n\t\thsize, wsize = self.size\n\t\thpad, wpad = self.pad\n\t\thstride, wstride = self.stride\n\n\t\tinh = (outh - 1) * hstride - 2 * hpad + hsize\n\t\tinw = (outw - 1) * wstride - 2 * wpad + wsize\n\n\t\treturn batchsize, maps, inh, inw\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 4:\n\t\t\traise ModuleError(""Grad must be 4d tensor"")\n\n\n\tdef updateData(self, data):\n\t\traise NotImplementedError()\n\n\n\tdef updateGrad(self, grad):\n\t\traise NotImplementedError()\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\t\tself.workspace = None\n\n\n\tdef calcMode(self, T):\n\t\tdtypes = {dtype for dtype, _ in dtypesSupported()}\n\n\t\tif T not in dtypes:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n'"
Modules/Pool3D.py,0,"b'from PuzzleLib.Backend.Utils import dtypesSupported\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass Pool3D(Module):\n\tdef __init__(self, size=2, stride=2, pad=0, name=None):\n\t\tsuper().__init__(name)\n\t\tself.gradUsesOutData = True\n\n\t\tself.size = self.repeat(size, 3)\n\t\tself.stride = self.repeat(stride, 3)\n\t\tself.pad = self.repeat(pad, 3)\n\n\t\tself.workspace = None\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tbatchsize, maps, ind, inh, inw = shape\n\n\t\tdsize, hsize, wsize = self.size\n\t\tdpad, hpad, wpad = self.pad\n\t\tdstride, hstride, wstride = self.stride\n\n\t\toutd = (ind + 2 * dpad - dsize) // dstride + 1\n\t\touth = (inh + 2 * hpad - hsize) // hstride + 1\n\t\toutw = (inw + 2 * wpad - wsize) // wstride + 1\n\n\t\treturn batchsize, maps, outd, outh, outw\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 5:\n\t\t\traise ModuleError(""Data must be 5d tensor"")\n\n\t\t_, _, ind, inh, inw = shape\n\t\tif ind + 2 * self.pad[0] < self.size[0]:\n\t\t\traise ModuleError(""Data cube time is too small (got %d, expected at least %d)"" %\n\t\t\t\t\t\t\t  (ind + 2 * self.pad[0], self.size[0]))\n\n\t\tif inh + 2 * self.pad[1] < self.size[1]:\n\t\t\traise ModuleError(""Data cube height is too small (got %d, expected at least %d)"" %\n\t\t\t\t\t\t\t  (inh + 2 * self.pad[1], self.size[1]))\n\n\t\tif inw + 2 * self.pad[2] < self.size[2]:\n\t\t\traise ModuleError(""Data cube width is too small (got %d, expected at least %d)"" %\n\t\t\t\t\t\t\t  (inw + 2 * self.pad[2], self.size[2]))\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tbatchsize, maps, outd, outh, outw = shape\n\n\t\tdsize, hsize, wsize = self.size\n\t\tdpad, hpad, wpad = self.pad\n\t\tdstride, hstride, wstride = self.stride\n\n\t\tind = (outd - 1) * dstride - 2 * dpad + dsize\n\t\tinh = (outh - 1) * hstride - 2 * hpad + hsize\n\t\tinw = (outw - 1) * wstride - 2 * wpad + wsize\n\n\t\treturn batchsize, maps, ind, inh, inw\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 5:\n\t\t\traise ModuleError(""Grad must be 5d tensor"")\n\n\n\tdef updateData(self, data):\n\t\traise NotImplementedError()\n\n\n\tdef updateGrad(self, grad):\n\t\traise NotImplementedError()\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\t\tself.workspace = None\n\n\n\tdef calcMode(self, T):\n\t\tdtypes = {dtype for dtype, _ in dtypesSupported()}\n\n\t\tif T not in dtypes:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n'"
Modules/RNN.py,85,"b'from enum import Enum\n\nimport numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend import Blas\nfrom PuzzleLib.Backend.Dnn.Rnn import RNNMode as BackendRNNMode, DirectionMode as BackendDirectionMode, createRnn\nfrom PuzzleLib.Backend.Dnn.Rnn import updateRnnParams, acquireRnnParams, forwardRnn, backwardDataRnn, backwardParamsRnn\nfrom PuzzleLib.Backend.Utils import split, memoryPool as memPool\nfrom PuzzleLib.Modules.Module import ModuleError, Module\nfrom PuzzleLib.Variable import Variable\n\n\nclass RNNMode(str, Enum):\n\trelu = ""relu""\n\ttanh = ""tanh""\n\tlstm = ""lstm""\n\tgru = ""gru""\n\n\nclass DirectionMode(str, Enum):\n\tuni = ""uni""\n\tbi = ""bi""\n\n\nclass WeightModifier(str, Enum):\n\torthogonal = ""orthogonal""\n\tidentity = ""identity""\n\n\nclass RNN(Module):\n\tdef __init__(self, insize, hsize, layers=1, mode=""relu"", direction=""uni"", dropout=0.0, getSequences=False,\n\t\t\t\t initscheme=None, modifier=""orthogonal"", wscale=1.0, hintBatchSize=None, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.gradUsesOutData = True\n\n\t\tself.insize = insize\n\t\tself.hsize = hsize\n\t\tself.layers = layers\n\n\t\tself.mode = RNNMode(mode)\n\t\tself.direction = DirectionMode(direction)\n\n\t\tself.dropout = dropout\n\t\tself.getSequences = getSequences\n\n\t\tself.hintBatchSize = hintBatchSize\n\n\t\tmode = {\n\t\t\tRNNMode.relu: BackendRNNMode.relu,\n\t\t\tRNNMode.tanh: BackendRNNMode.tanh,\n\t\t\tRNNMode.lstm: BackendRNNMode.lstm,\n\t\t\tRNNMode.gru: BackendRNNMode.gru\n\t\t}[self.mode]\n\n\t\tdirection = {\n\t\t\tDirectionMode.uni: BackendDirectionMode.uni,\n\t\t\tDirectionMode.bi: BackendDirectionMode.bi\n\t\t}[self.direction]\n\n\t\tself.descRnn, W, params = createRnn(\n\t\t\tinsize, hsize, layers, mode, direction, dropout, seed=np.random.randint(1 << 31), batchsize=hintBatchSize\n\t\t)\n\n\t\tself.W = None\n\t\tself.setVar(""W"", Variable(W))\n\n\t\tself.params = params\n\t\tself.initParams(initscheme, wscale, modifier)\n\n\t\tself.reserve, self.fulldata, self.dw = None, None, None\n\n\n\tdef initParams(self, initscheme, wscale, modifier):\n\t\tmodifier = WeightModifier(modifier)\n\t\tlayers = (self.params[key] for key in sorted(self.params.keys()))\n\n\t\tfor layer in layers:\n\t\t\tfor paramName, param in sorted(layer.items()):\n\t\t\t\tif paramName.startswith(""b""):\n\t\t\t\t\tparam.fill(0.0)\n\n\t\t\t\telse:\n\t\t\t\t\tif paramName.startswith(""r""):\n\t\t\t\t\t\tif modifier == WeightModifier.orthogonal:\n\t\t\t\t\t\t\ta = np.random.normal(0.0, 1.0, param.shape)\n\t\t\t\t\t\t\tu, _, v = np.linalg.svd(a, full_matrices=False)\n\n\t\t\t\t\t\t\tW = u if u.shape == param.shape else v\n\t\t\t\t\t\t\tW = W[:param.shape[0], :param.shape[1]].astype(np.float32)\n\n\t\t\t\t\t\telif modifier == WeightModifier.identity:\n\t\t\t\t\t\t\tW = np.identity(param.shape[0], dtype=np.float32)\n\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\traise NotImplementedError(modifier)\n\n\t\t\t\t\telse:\n\t\t\t\t\t\tW = self.createTensorWithScheme(initscheme, param.shape, wscale)\n\n\t\t\t\t\t\tif W is None:\n\t\t\t\t\t\t\tcontinue\n\n\t\t\t\t\tparam.set(W)\n\n\t\tself.updateDeviceMemory()\n\n\n\tdef updateDeviceMemory(self):\n\t\tupdateRnnParams(self.descRnn, self.W, self.params)\n\n\n\tdef setVar(self, name, var):\n\t\tif name == ""W"" and hasattr(self, ""params""):\n\t\t\t_, self.params = acquireRnnParams(self.descRnn, w=var.data)\n\n\t\tsuper().setVar(name, var)\n\n\n\tdef updateData(self, data):\n\t\tif self.train:\n\t\t\tself.fulldata, self.reserve = forwardRnn(data, self.W, self.descRnn)\n\t\telse:\n\t\t\tself.fulldata = forwardRnn(data, self.W, self.descRnn, test=True)\n\n\t\tif self.direction == DirectionMode.uni:\n\t\t\tself.data = self.fulldata if self.getSequences else self.fulldata[-1]\n\n\t\telse:\n\t\t\tif self.getSequences:\n\t\t\t\tself.data = self.fulldata\n\t\t\telse:\n\t\t\t\tfwddata, bwddata = self.fulldata[-1], self.fulldata[0]\n\t\t\t\tsections = (self.hsize, self.hsize)\n\n\t\t\t\tself.data = [split(fwddata, sections, axis=1)[0], split(bwddata, sections, axis=1)[1]]\n\n\n\tdef updateGrad(self, grad):\n\t\tif self.getSequences:\n\t\t\tfullgrad = grad\n\t\telse:\n\t\t\tseqlen = self.fulldata.shape[0]\n\n\t\t\tif self.direction == DirectionMode.uni:\n\t\t\t\tfullgrad = gpuarray.empty((seqlen, ) + grad.shape, dtype=grad.dtype, allocator=memPool)\n\t\t\t\tfullgrad[:seqlen - 1].fill(0.0)\n\t\t\t\tfullgrad[seqlen - 1].set(grad)\n\n\t\t\telse:\n\t\t\t\tfwdgrad, bwdgrad = grad\n\t\t\t\tbatchsize, hsize = fwdgrad.shape[0], 2 * self.hsize\n\n\t\t\t\tfullgrad = gpuarray.zeros((seqlen, batchsize, hsize), dtype=fwdgrad.dtype, allocator=memPool)\n\n\t\t\t\tfullgrad[0, :, bwdgrad.shape[1]:].set(bwdgrad)\n\t\t\t\tfullgrad[-1, :, :fwdgrad.shape[1]].set(fwdgrad)\n\n\t\tself.grad, self.reserve = backwardDataRnn(fullgrad, self.fulldata, self.W, self.reserve, self.descRnn)\n\n\n\tdef accGradParams(self, grad, scale=1.0, momentum=0.0):\n\t\tself.dw = backwardParamsRnn(self.inData, self.fulldata, self.W, self.reserve, self.descRnn)\n\t\tBlas.addVectorToVector(self.dw, self.getVar(""W"").grad, out=self.getVar(""W"").grad, alpha=scale, beta=momentum)\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 3:\n\t\t\traise ModuleError(""Data must be 3d tensor"")\n\n\t\tif self.hintBatchSize is not None and shape[1] != self.hintBatchSize:\n\t\t\traise ModuleError(""Data batch size must be = %s (was given %s)"" % (self.hintBatchSize, shape[1]))\n\n\t\tif shape[2] != self.insize:\n\t\t\traise ModuleError(""Data must have data size = %s (was given %s)"" % (self.insize, shape[2]))\n\n\n\tdef checkGradShape(self, shape):\n\t\tif self.getSequences:\n\t\t\tif len(shape) != 3:\n\t\t\t\traise ModuleError(""Grad must be 3d tensor"")\n\n\t\t\tif self.hintBatchSize is not None and shape[1] != self.hintBatchSize:\n\t\t\t\traise ModuleError(""Grad batch size must be = %s (was given %s)"" % (self.hintBatchSize, shape[1]))\n\n\t\telse:\n\t\t\tif self.direction == DirectionMode.uni:\n\t\t\t\tif len(shape) != 2:\n\t\t\t\t\traise ModuleError(""Grad must be 2d matrix"")\n\n\t\t\t\tif self.hintBatchSize is not None and shape[0] != self.hintBatchSize:\n\t\t\t\t\traise ModuleError(""Grad batch size must be = %s (was given %s)"" % (self.hintBatchSize, shape[0]))\n\n\t\t\t\tif shape[-1] != self.hsize:\n\t\t\t\t\traise ModuleError(""Grad must have data size = %s (was given %s)"" % (self.hsize, shape[2]))\n\n\t\t\telse:\n\t\t\t\tfwdshape, bwdshape = shape\n\n\t\t\t\tif len(fwdshape) != 2 or len(bwdshape) != 2:\n\t\t\t\t\traise ModuleError(""Grads must be 2d matrices"")\n\n\t\t\t\tif self.hintBatchSize is not None and \\\n\t\t\t\t\t\t(fwdshape[0] != self.hintBatchSize or bwdshape[0] != self.hintBatchSize):\n\t\t\t\t\traise ModuleError(""Grads batch size must be = %s (was given %s and %s)"" %\n\t\t\t\t\t\t\t\t\t  (self.hintBatchSize, fwdshape[0], bwdshape[0]))\n\n\t\t\t\tif fwdshape[-1] != self.hsize or bwdshape[-1] != self.hsize:\n\t\t\t\t\traise ModuleError(""Grads must have data size = %s (was given %s and %s)"" %\n\t\t\t\t\t\t\t\t\t  (self.hsize, fwdshape[1], bwdshape[1]))\n\n\n\tdef dataShapeFrom(self, shape):\n\t\thsize = self.hsize if self.direction == DirectionMode.uni else 2 * self.hsize\n\n\t\tif self.getSequences:\n\t\t\treturn shape[:2] + (hsize, )\n\t\telse:\n\t\t\treturn (shape[1], hsize) if self.direction == DirectionMode.uni else [(shape[1], hsize), (shape[1], hsize)]\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tseqlen = self.inData.shape[0]\n\n\t\tif self.getSequences:\n\t\t\tbatchsize = shape[1]\n\t\telse:\n\t\t\tbatchsize = shape[0] if self.direction == DirectionMode.uni else shape[0][0]\n\n\t\treturn seqlen, batchsize, self.insize\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\t\tself.reserve = None\n\t\tself.fulldata = None\n\t\tself.dw = None\n\n\ndef unittest():\n\tsequencesTest()\n\tlastStateTest()\n\tbidirectionalTest()\n\ttrainTest()\n\n\ndef sequencesTest():\n\tseqlen, batchsize, insize, hsize = 4, 3, 4, 5\n\n\thostData = np.random.randn(seqlen, batchsize, insize).astype(np.float32)\n\tdata = gpuarray.to_gpu(hostData)\n\n\trnn = RNN(insize, hsize, mode=""relu"", getSequences=True)\n\trnn(data)\n\n\thostOutData = np.zeros((seqlen + 1, batchsize, hsize), dtype=np.float32)\n\tparams = {name: param.get() for name, param in rnn.params[0].items()}\n\n\tfor d in range(seqlen):\n\t\tres = np.dot(hostData[d], params[""wi""].T) + np.dot(hostOutData[d], params[""ri""].T) + \\\n\t\t\t  params[""bwi""] + params[""bri""]\n\t\thostOutData[d + 1] = (res > 0.0) * res\n\n\tassert np.allclose(hostOutData[1:], rnn.data.get())\n\n\thostGrad = np.random.randn(*rnn.data.shape).astype(np.float32)\n\tgrad = gpuarray.to_gpu(hostGrad)\n\n\trnn.backward(grad)\n\n\thostAccGrad = np.zeros((seqlen + 1, batchsize, hsize), dtype=np.float32)\n\thostInGrad = np.zeros((seqlen, batchsize, insize), dtype=np.float32)\n\n\tfor d in range(seqlen):\n\t\tacc = (hostGrad[seqlen - d - 1] + np.dot(hostAccGrad[seqlen - d], params[""ri""])) * (hostOutData[seqlen - d] > 0)\n\n\t\thostAccGrad[seqlen - d - 1] = acc\n\t\thostInGrad[seqlen - d - 1] = np.dot(acc, params[""wi""])\n\n\tassert np.allclose(hostInGrad, rnn.grad.get())\n\n\thostRiGrad = np.zeros(params[""ri""].shape, dtype=np.float32)\n\thostWiGrad = np.zeros(params[""wi""].shape, dtype=np.float32)\n\thostBiGrad = np.zeros(params[""bwi""].shape, dtype=np.float32)\n\n\tfor d in range(seqlen):\n\t\thostRiGrad += np.dot(hostAccGrad[seqlen - d - 1].T, hostOutData[seqlen - d - 1])\n\t\thostWiGrad += np.dot(hostAccGrad[seqlen - d - 1].T, hostData[seqlen - d - 1])\n\t\thostBiGrad += np.sum(hostAccGrad[seqlen - d - 1], axis=0)\n\n\t_, dwparams = acquireRnnParams(rnn.descRnn, w=rnn.dw)\n\tdwparams = dwparams[0]\n\n\tassert np.allclose(hostRiGrad, dwparams[""ri""].get())\n\tassert np.allclose(hostWiGrad, dwparams[""wi""].get())\n\tassert np.allclose(hostBiGrad, dwparams[""bwi""].get())\n\tassert np.allclose(hostBiGrad, dwparams[""bri""].get())\n\n\ndef lastStateTest():\n\tseqlen, batchsize, insize, hsize = 5, 3, 6, 5\n\n\thostData = np.random.randn(seqlen, batchsize, insize).astype(np.float32)\n\tdata = gpuarray.to_gpu(hostData)\n\n\trnn = RNN(insize, hsize, mode=""relu"", getSequences=False)\n\trnn(data)\n\n\thostOutData = np.zeros((seqlen + 1, batchsize, hsize), dtype=np.float32)\n\tparams = {name: param.get() for name, param in rnn.params[0].items()}\n\n\tfor d in range(seqlen):\n\t\tres = np.dot(hostData[d], params[""wi""].T) + np.dot(hostOutData[d], params[""ri""].T) + \\\n\t\t\t  params[""bwi""] + params[""bri""]\n\t\thostOutData[d + 1] = (res > 0.0) * res\n\n\tassert np.allclose(hostOutData[-1], rnn.data.get())\n\n\thostGrad = np.random.randn(*rnn.data.shape).astype(np.float32)\n\tgrad = gpuarray.to_gpu(hostGrad)\n\n\trnn.backward(grad)\n\n\thostGrad = np.zeros((seqlen, batchsize, hsize), dtype=np.float32)\n\thostGrad[-1] = grad.get()\n\n\thostAccGrad = np.zeros((seqlen + 1, batchsize, hsize), dtype=np.float32)\n\thostInGrad = np.zeros((seqlen, batchsize, insize), dtype=np.float32)\n\n\tfor d in range(seqlen):\n\t\tacc = (hostGrad[seqlen - d - 1] + np.dot(hostAccGrad[seqlen - d], params[""ri""])) * \\\n\t\t\t  (hostOutData[seqlen - d] > 0)\n\n\t\thostAccGrad[seqlen - d - 1] = acc\n\t\thostInGrad[seqlen - d - 1] = np.dot(acc, params[""wi""])\n\n\tassert np.allclose(hostInGrad, rnn.grad.get())\n\n\thostRiGrad = np.zeros(params[""ri""].shape, dtype=np.float32)\n\thostWiGrad = np.zeros(params[""wi""].shape, dtype=np.float32)\n\thostBiGrad = np.zeros(params[""bwi""].shape, dtype=np.float32)\n\n\tfor d in range(seqlen):\n\t\thostRiGrad += np.dot(hostAccGrad[seqlen - d - 1].T, hostOutData[seqlen - d - 1])\n\t\thostWiGrad += np.dot(hostAccGrad[seqlen - d - 1].T, hostData[seqlen - d - 1])\n\t\thostBiGrad += np.sum(hostAccGrad[seqlen - d - 1], axis=0)\n\n\t_, dwparams = acquireRnnParams(rnn.descRnn, w=rnn.dw)\n\tdwparams = dwparams[0]\n\n\tassert np.allclose(hostRiGrad, dwparams[""ri""].get())\n\tassert np.allclose(hostWiGrad, dwparams[""wi""].get())\n\tassert np.allclose(hostBiGrad, dwparams[""bwi""].get())\n\tassert np.allclose(hostBiGrad, dwparams[""bri""].get())\n\n\ndef bidirectionalTest():\n\tseqlen, batchsize, insize, hsize = 4, 3, 4, 5\n\n\thostData = np.random.randn(seqlen, batchsize, insize).astype(np.float32)\n\tdata = gpuarray.to_gpu(hostData)\n\n\trnn = RNN(insize, hsize, mode=""relu"", direction=""bi"", getSequences=False)\n\trnn(data)\n\n\thostOutData = np.zeros((seqlen + 2, batchsize, 2 * hsize), dtype=np.float32)\n\tparams = {layernm: {name: param.get() for name, param in layer.items()} for layernm, layer in rnn.params.items()}\n\n\tfor d in range(seqlen):\n\t\tres = np.dot(hostData[d], params[0][""wi""].T) + np.dot(hostOutData[d, :, :hsize], params[0][""ri""].T) + \\\n\t\t\t  params[0][""bwi""] + params[0][""bri""]\n\t\thostOutData[d + 1, :, :hsize] = (res > 0.0) * res\n\n\t\tres = np.dot(hostData[seqlen - d - 1], params[1][""wi""].T) + \\\n\t\t\t  np.dot(hostOutData[seqlen + 1 - d, :, hsize:], params[1][""ri""].T) + params[1][""bwi""] + params[1][""bri""]\n\t\thostOutData[seqlen - d, :, hsize:] = (res > 0.0) * res\n\n\thostFwdOutData, hostBwdOutData = hostOutData[-2, :, :hsize], hostOutData[1, :, hsize:]\n\n\tassert np.allclose(hostFwdOutData, rnn.data[0].get())\n\tassert np.allclose(hostBwdOutData, rnn.data[1].get())\n\n\thostEndGrad = [\n\t\tnp.random.randn(*rnn.data[0].shape).astype(np.float32),\n\t\tnp.random.randn(*rnn.data[1].shape).astype(np.float32)\n\t]\n\n\tgrad = [gpuarray.to_gpu(gr) for gr in hostEndGrad]\n\trnn.backward(grad)\n\n\thostGrad = np.zeros((seqlen, batchsize, 2 * hsize), dtype=np.float32)\n\thostGrad[-1, :, :hsize], hostGrad[0, :, hsize:] = hostEndGrad[0], hostEndGrad[1]\n\n\thostAccGrad = np.zeros((seqlen + 2, batchsize, 2 * hsize), dtype=np.float32)\n\thostInGrad = np.zeros((seqlen, batchsize, insize), dtype=np.float32)\n\n\tfor d in range(seqlen):\n\t\tacc = (hostGrad[seqlen - d - 1, :, :hsize] +\n\t\t\t   np.dot(hostAccGrad[seqlen + 1 - d, :, :hsize], params[0][""ri""])) * \\\n\t\t\t  (hostOutData[seqlen - d, :, :hsize] > 0)\n\n\t\thostAccGrad[seqlen - d, :, :hsize] = acc\n\t\thostInGrad[seqlen - d - 1] += np.dot(acc, params[0][""wi""])\n\n\t\tacc = (hostGrad[d, :, hsize:] + np.dot(hostAccGrad[d, :, hsize:], params[1][""ri""])) * \\\n\t\t\t  (hostOutData[d + 1, :, hsize:] > 0)\n\n\t\thostAccGrad[d + 1, :, hsize:] = acc\n\t\thostInGrad[d] += np.dot(acc, params[1][""wi""])\n\n\tassert np.allclose(hostInGrad, rnn.grad.get())\n\n\thostRi0Grad = np.zeros(params[0][""ri""].shape, dtype=np.float32)\n\thostRi1Grad = np.zeros(params[1][""ri""].shape, dtype=np.float32)\n\thostWi0Grad = np.zeros(params[0][""wi""].shape, dtype=np.float32)\n\thostWi1Grad = np.zeros(params[1][""wi""].shape, dtype=np.float32)\n\n\thostBi0Grad = np.zeros(params[0][""bwi""].shape, dtype=np.float32)\n\thostBi1Grad = np.zeros(params[1][""bwi""].shape, dtype=np.float32)\n\n\tfor d in range(seqlen):\n\t\thostRi0Grad += np.dot(hostAccGrad[seqlen - d + 1, :, :hsize].T, hostOutData[seqlen - d, :, :hsize])\n\t\thostWi0Grad += np.dot(hostAccGrad[seqlen - d, :, :hsize].T, hostData[seqlen - d - 1])\n\t\thostRi1Grad += np.dot(hostAccGrad[d, :, hsize:].T, hostOutData[d + 1, :, hsize:])\n\t\thostWi1Grad += np.dot(hostAccGrad[d + 1, :, hsize:].T, hostData[d])\n\n\t\thostBi0Grad += np.sum(hostAccGrad[seqlen - d, :, :hsize], axis=0)\n\t\thostBi1Grad += np.sum(hostAccGrad[d + 1, :, hsize:], axis=0)\n\n\t_, dwparams = acquireRnnParams(rnn.descRnn, w=rnn.dw)\n\n\tassert np.allclose(hostRi0Grad, dwparams[0][""ri""].get())\n\tassert np.allclose(hostWi0Grad, dwparams[0][""wi""].get())\n\tassert np.allclose(hostRi1Grad, dwparams[1][""ri""].get())\n\tassert np.allclose(hostWi1Grad, dwparams[1][""wi""].get())\n\n\tassert np.allclose(hostBi0Grad, dwparams[0][""bwi""].get())\n\tassert np.allclose(hostBi0Grad, dwparams[0][""bri""].get())\n\n\tassert np.allclose(hostBi1Grad, dwparams[1][""bwi""].get())\n\tassert np.allclose(hostBi1Grad, dwparams[1][""bri""].get())\n\n\ndef trainTest():\n\tseqlen, batchsize, insize, hsize = 10, 32, 64, 32\n\n\tdata = gpuarray.to_gpu(np.random.randn(seqlen, batchsize, insize).astype(np.float32))\n\ttarget = gpuarray.to_gpu(np.random.normal(0.0, 1.0, (seqlen, batchsize, hsize)).astype(np.float32))\n\n\trnn = RNN(insize, hsize, mode=""relu"", getSequences=True)\n\trnn(data)\n\n\tfrom PuzzleLib.Cost.MSE import MSE\n\tmse = MSE()\n\n\tfor i in range(200):\n\t\tlearnRate = 1e-1\n\n\t\trnn(data)\n\t\terror, grad = mse(rnn.data, target)\n\n\t\trnn.backward(grad)\n\t\trnn.updateParams(learnRate)\n\n\t\tif (i + 1) % 5 == 0:\n\t\t\tprint(""Iteration #%d error: %s"" % (i + 1, error))\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Replicate.py,4,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray, Blas\nfrom PuzzleLib.Backend.Utils import dtypesSupported, memoryPool as memPool\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass Replicate(Module):\n\tdef __init__(self, times, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.movesData = True\n\t\tself.times = times\n\n\n\tdef updateData(self, data):\n\t\tself.data = [data] * self.times\n\n\n\tdef updateGrad(self, grad):\n\t\tfirstgrad = grad[0]\n\n\t\tself.grad = gpuarray.empty(firstgrad.shape, dtype=firstgrad.dtype, allocator=memPool)\n\t\tself.grad.fill(0)\n\n\t\tfor gr in grad:\n\t\t\tBlas.toVectorAddVector(self.grad.ravel(), gr.ravel())\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn [shape] * self.times\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn shape[0]\n\n\n\tdef calcMode(self, T):\n\t\tdtypes = {dtype for dtype, _ in dtypesSupported()}\n\n\t\tif T not in dtypes:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n\n\ndef unittest():\n\tfor dtype, _ in dtypesSupported():\n\t\treplicateTest(dtype)\n\n\ndef replicateTest(dtype):\n\thostData = np.random.randn(10, 10, 3, 3).astype(dtype)\n\tdata = gpuarray.to_gpu(hostData)\n\n\ttimes = 3\n\n\trepl = Replicate(times)\n\trepl.calcMode(dtype)\n\n\trepl(data)\n\n\tassert len(repl.data) == times\n\n\thostGrad = [np.random.randn(10, 10, 3, 3).astype(dtype) for _ in range(times)]\n\tgrad = [gpuarray.to_gpu(gr) for gr in hostGrad]\n\n\trepl.backward(grad)\n\n\thostInGrad = np.zeros(grad[0].shape, dtype=dtype)\n\tfor i in range(times):\n\t\thostInGrad += hostGrad[i]\n\n\tassert np.allclose(hostInGrad, repl.grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Reshape.py,6,"b'import numpy as np\n\nfrom PuzzleLib.Config import libname\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass Reshape(Module):\n\tdef __init__(self, shape, showWarnings=True, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.showWarnings = showWarnings\n\n\t\tself.movesData = True\n\t\tself.movesGrad = True\n\n\t\tself.shape = shape\n\t\tself.inshape = None\n\n\t\tself.copyIdx = tuple(idx for idx, value in enumerate(shape) if value == 0)\n\n\n\tdef updateData(self, data):\n\t\tself.inshape = data.shape\n\t\tmodShape = self.copyAxis(self.shape, self.inshape)\n\t\tself.data = data.reshape(modShape)\n\n\t\tif self.showWarnings:\n\t\t\tif self.data.shape[0] != self.inshape[0]:\n\t\t\t\tprint(""[%s] Warning: %s changed data batch axis size (was given %s, reshaped to %s)"" %\n\t\t\t\t\t  (libname, self, data.shape, self.data.shape))\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = grad.reshape(self.inshape)\n\n\t\tif self.showWarnings:\n\t\t\tif self.grad.shape[0] != self.inshape[0]:\n\t\t\t\tprint(""[%s] Warning: %s changed grad batch axis size (was given %s, reshaped to %s)"" %\n\t\t\t\t\t  (libname, self, grad.shape, self.grad.shape))\n\n\n\tdef copyAxis(self, shape, mask):\n\t\treturn tuple(mask[idx] if idx in self.copyIdx else value for idx, value in enumerate(shape))\n\n\n\tdef checkDataShape(self, shape):\n\t\tmodShape = self.copyAxis(self.shape, shape)\n\t\ttry:\n\t\t\tidx = modShape.index(-1)\n\n\t\texcept ValueError:\n\t\t\tif int(np.prod(shape)) != int(np.prod(modShape)):\n\t\t\t\traise ModuleError(""Data shape %s is inconsistent with reshape %s"" % (shape, modShape))\n\n\t\t\treturn\n\n\t\tif int(np.prod(shape)) % int(np.prod(modShape[:idx] + modShape[idx + 1:])) != 0:\n\t\t\traise ModuleError(""Data shape %s is inconsistent with reshape %s"" % (shape, modShape))\n\n\n\tdef checkGradShape(self, shape):\n\t\tif int(np.prod(shape)) != int(np.prod(self.inshape)):\n\t\t\traise ModuleError(""Grad shape %s is inconsistent with reshape %s"" % (shape, self.inshape))\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tmodShape = self.copyAxis(self.shape, shape)\n\n\t\ttry:\n\t\t\tidx = self.shape.index(-1)\n\t\t\tdim = int(np.prod(shape)) // int(np.prod(modShape[:idx]) * np.prod(modShape[idx + 1:]))\n\n\t\t\treturn modShape[:idx] + (dim, ) + modShape[idx + 1:]\n\n\t\texcept ValueError:\n\t\t\treturn modShape\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn self.inshape\n\n\n\tdef calcMode(self, T):\n\t\tself.calctype = T\n\n\ndef unittest():\n\tshapes = [\n\t\t[(10, 10, 10, 10), (10, -1, 100), (10, 10, 100)],\n\t\t[(1, 4, 7, 7), (0, 2, -1, 0), (1, 2, 14, 7)]\n\t]\n\n\tfor inshape, shape, targetShape in shapes:\n\t\tdata = gpuarray.to_gpu(np.random.randn(*inshape).astype(np.float32))\n\n\t\treshape = Reshape(shape)\n\n\t\treshape(data)\n\t\tassert reshape.data.shape == targetShape\n\n\t\tgrad = gpuarray.to_gpu(np.random.randn(*reshape.data.shape).astype(np.float32))\n\n\t\treshape.backward(grad)\n\t\tassert reshape.grad.shape == data.shape\n\n\t\tassert reshape.dataShapeFrom(data.shape) == targetShape and targetShape == reshape.data.shape\n\t\tassert reshape.gradShapeFrom(grad.shape) == data.shape\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Slice.py,5,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass Slice(Module):\n\tdef __init__(self, slc=None, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.slc = slc\n\t\tself.inshape = None\n\n\n\tdef __getitem__(self, slc):\n\t\tif not isinstance(slc, tuple):\n\t\t\tslc = (slc, )\n\n\t\tself.slc = slc\n\t\treturn self\n\n\n\tdef updateData(self, data):\n\t\tself.inshape = data.shape\n\t\tself.data = data[self.slc].copy()\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = gpuarray.zeros(self.inshape, dtype=np.float32, allocator=memPool)\n\t\tself.grad[self.slc] = grad\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tif self.slc is None:\n\t\t\traise ModuleError(""Slice parameter is not initialized"")\n\n\t\toutshape = [None] * len(shape)\n\t\tfor i, dim in enumerate(shape):\n\t\t\tslc = self.slc[i]\n\t\t\tstart, stop, step = slc.indices(dim)\n\n\t\t\toutshape[i] = (stop - start) // step\n\n\t\treturn tuple(outshape)\n\n\n\tdef checkDataShape(self, shape):\n\t\tif self.slc is None:\n\t\t\traise ModuleError(""Slice parameter is not initialized"")\n\n\t\tif len(shape) < len(self.slc):\n\t\t\traise ModuleError(""Expected at least %d data dimensions, %d were given"" % (len(self.slc), len(shape)))\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn self.inshape\n\n\n\tdef checkGradShape(self, shape):\n\t\tif shape != self.data.shape:\n\t\t\traise ModuleError(""Grad shape %s is inconsistent with output data shape %s"" % (shape, self.data.shape))\n\n\ndef unittest():\n\tdata = gpuarray.to_gpu(np.random.randn(3, 4, 5, 6).astype(np.float32))\n\n\tslc = Slice()[:, :, 1:-1, 1:-1]\n\tslc(data)\n\n\tassert slc.dataShapeFrom(data.shape) == slc.data.shape\n\tassert np.allclose(slc.data.get(), data.get()[slc.slc])\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*slc.data.shape).astype(np.float32))\n\tslc.backward(grad)\n\n\tassert slc.gradShapeFrom(grad.shape) == data.shape\n\tassert np.allclose(slc.grad.get()[slc.slc], grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/SoftMax.py,11,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import dtypesSupported\nfrom PuzzleLib.Backend.Dnn.Basic import softmaxNd, softmaxNdBackward\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass SoftMax(Module):\n\tdef __init__(self, name=None):\n\t\tsuper().__init__(name)\n\t\tself.gradUsesOutData = True\n\n\n\tdef updateData(self, data):\n\t\tshape = data.shape\n\t\tndim = max(0, 4 - len(shape))\n\n\t\tdata = data.reshape(shape + tuple(1 for _ in range(ndim)))\n\t\tself.data = softmaxNd(data).reshape(shape)\n\n\n\tdef updateGrad(self, grad):\n\t\tshape = grad.shape\n\t\tndim = max(0, 4 - len(shape))\n\n\t\tgrad = grad.reshape(shape + tuple(1 for _ in range(ndim)))\n\t\tdata = self.data.reshape(shape + tuple(1 for _ in range(ndim)))\n\n\t\tself.grad = softmaxNdBackward(data, grad).reshape(shape)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef calcMode(self, T):\n\t\tdtypes = {dtype for dtype, _ in dtypesSupported()}\n\n\t\tif T not in dtypes:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n\n\ndef unittest():\n\tbatchsize, maps = 2, 3\n\n\thostData = np.random.randn(batchsize, maps, 1).astype(np.float32)\n\tdata = gpuarray.to_gpu(hostData)\n\n\tsoftmax = SoftMax()\n\tsoftmax(data)\n\n\tdef softMaxForward(w):\n\t\te = np.exp(w - np.amax(w))\n\t\tp = e / np.sum(e)\n\t\treturn p\n\n\thostData = hostData.reshape(batchsize, maps).astype(np.float32)\n\n\thostOutData = np.vstack([softMaxForward(hostData[i]) for i in range(batchsize)])\n\tassert np.allclose(hostOutData, softmax.data.get().reshape(batchsize, maps).astype(np.float32))\n\n\thostGrad = np.random.randn(batchsize, maps, 1, 1).astype(np.float32)\n\tgrad = gpuarray.to_gpu(hostGrad)\n\n\tsoftmax.backward(grad)\n\thostGrad = hostGrad.reshape(batchsize, maps).astype(np.float32)\n\n\tdef softMaxBackward(outdata, gr):\n\t\tingrad = np.zeros(outdata.shape, dtype=np.float32)\n\t\tfor i in range(ingrad.shape[0]):\n\t\t\tingrad[i] += outdata[i] * gr[i]\n\n\t\t\tfor j in range(outdata.shape[0]):\n\t\t\t\tingrad[i] -= outdata[i] * outdata[j] * gr[j]\n\t\treturn ingrad\n\n\thostInGrad = np.vstack([softMaxBackward(hostOutData[i], hostGrad[i]) for i in range(batchsize)])\n\tassert np.allclose(hostInGrad, softmax.grad.get().reshape(batchsize, maps).astype(np.float32))\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/SpatialTf.py,3,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Dnn.SpatialTf import spatialTf, spatialTfBackward\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass SpatialTf(Module):\n\tdef __init__(self, shape=None, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.shape = shape\n\t\tself.grid = None\n\n\n\tdef updateData(self, data):\n\t\tdata, transform = data\n\n\t\tif self.train:\n\t\t\tself.data, self.grid = spatialTf(data, transform, outshape=self.shape, getGrid=True)\n\t\telse:\n\t\t\tself.data = spatialTf(data, transform, outshape=self.shape, getGrid=False)\n\n\n\tdef updateGrad(self, grad):\n\t\tdata, _ = self.inData\n\t\tself.grad = spatialTfBackward(grad, data, self.grid)\n\n\n\tdef checkDataShape(self, shapes):\n\t\tdshape, tshape = shapes\n\n\t\tif len(tshape) != 3 or tshape[1:] != (2, 3):\n\t\t\traise ModuleError(""Bad transform shape (%s was given)"" % tshape)\n\n\t\tif len(dshape) != 4:\n\t\t\traise ModuleError(""Data must be 4d tensor"")\n\n\t\tif tshape[0] != dshape[0]:\n\t\t\traise ModuleError(""Inconsistency in transform and data batch size (%d in transform vs %d in data)"" %\n\t\t\t\t\t\t\t  (tshape[0], dshape[0]))\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 4:\n\t\t\traise ModuleError(""Grad must be 4d tensor"")\n\n\t\tif self.shape is not None:\n\t\t\tif self.shape != shape[1:]:\n\t\t\t\traise ModuleError(""Bad grad shape (was given %s, expected %s)"" % (shape[1:], self.shape))\n\t\telse:\n\t\t\tif self.inData[0].shape != shape:\n\t\t\t\traise ModuleError(""Bad grad shape (was given %s, expected %s)"" % (shape, self.inData[0].shape))\n\n\n\tdef dataShapeFrom(self, shapes):\n\t\tdshape, tshape = shapes\n\t\treturn (dshape[0], ) + self.shape if self.shape is not None else dshape\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn (shape[0], ) + self.inData[0].shape[1:], (shape[0], 2, 3)\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\t\tself.grid = None\n\n\ndef unittest():\n\tbatchsize, maps, inh, inw = 1, 1, 4, 4\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, inh, inw).astype(np.float32))\n\n\ttransform = gpuarray.to_gpu(\n\t\tnp.tile(np.array([[1.0, 0.0, 0.001], [0, 1.0, 0.001]], dtype=np.float32), reps=(batchsize, 1, 1))\n\t)\n\n\tspatialtf = SpatialTf()\n\tspatialtf([data, transform])\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*spatialtf.data.shape).astype(np.float32))\n\tspatialtf.backward(grad)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Split.py,6,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray, Utils\nfrom PuzzleLib.Backend.Utils import dtypesSupported\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass Split(Module):\n\tdef __init__(self, axis, sections, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.sections = sections\n\t\tself.axis = axis\n\n\n\tdef updateData(self, data):\n\t\tself.data = Utils.split(data, self.sections, self.axis)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = Utils.concatenate(grad, self.axis)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tshapes = []\n\t\tfor sec in self.sections:\n\t\t\tshapes.append(shape[:self.axis] + (sec, ) + shape[self.axis + 1:])\n\n\t\treturn shapes\n\n\n\tdef gradShapeFrom(self, shapes):\n\t\tconcatDim = 0\n\t\tfor shape in shapes:\n\t\t\tconcatDim += shape[self.axis]\n\n\t\treturn shapes[0][:self.axis] + (concatDim, ) + shapes[0][self.axis + 1:]\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) < self.axis:\n\t\t\traise ModuleError(""Not enough dims in data (%d were given, need at least %d)"" % (len(shape), self.axis))\n\n\t\tconcatDim = 0\n\t\tfor sec in self.sections:\n\t\t\tconcatDim += sec\n\n\t\tif concatDim != shape[self.axis]:\n\t\t\traise ModuleError(\n\t\t\t\t""Data shape %s is inconsistent with given sections %s""\n\t\t\t\t""(expected size %d on axis %d, %d was given)"" %\n\t\t\t\t(shape, self.sections, concatDim, self.axis, shape[self.axis])\n\t\t\t)\n\n\n\tdef checkGradShape(self, shapes):\n\t\tfor i, shape in enumerate(shapes):\n\t\t\tif shape != self.data[i].shape:\n\t\t\t\traise ModuleError(\n\t\t\t\t\t""Expected grad shape %s on %d place (%s was given)"" % (self.data[i].shape, i + 1, shape)\n\t\t\t\t)\n\n\n\tdef calcMode(self, T):\n\t\tdtypes = {dtype for dtype, _ in dtypesSupported()}\n\n\t\tif T not in dtypes:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n\n\ndef unittest():\n\tfor dtype, _ in dtypesSupported():\n\t\tsplitTest(dtype)\n\n\ndef splitTest(dtype):\n\tbatchsize, groups, size = 5, 3, 4\n\n\thostData = np.random.randn(batchsize, groups, size).astype(dtype)\n\tdata = gpuarray.to_gpu(hostData)\n\n\tsplit = Split(axis=2, sections=(3, 1))\n\tsplit.calcMode(dtype)\n\n\tsplit(data)\n\n\thostOutData = np.split(hostData, [split.sections[0]], axis=split.axis)\n\tassert all(np.allclose(hostOutData[i], split.data[i].get()) for i in range(len(hostOutData)))\n\n\thostGrad = [np.random.randn(*split.data[i].shape).astype(dtype) for i in range(len(split.data))]\n\tgrad = [gpuarray.to_gpu(gr) for gr in hostGrad]\n\n\tsplit.backward(grad)\n\n\thostInGrad = np.concatenate(hostGrad, axis=split.axis)\n\tassert np.allclose(hostInGrad, split.grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/SubtractMean.py,8,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray, Blas\nfrom PuzzleLib.Backend.Dnn.Basic import PoolMode, poolNd, poolNdBackward\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass SubtractMean(Module):\n\tdef __init__(self, size=5, includePad=True, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tif size % 2 != 1 or size == 1:\n\t\t\traise ModuleError(""Subtractive norm size must be odd and > 1"")\n\n\t\tself.size = self.repeat(size, 2)\n\t\tself.pad = (self.size[0] // 2, self.size[1] // 2)\n\n\t\tself.mode = PoolMode.avgWithPad if includePad else PoolMode.avgNoPad\n\n\t\tself.means = None\n\t\tself.workspace = None\n\n\n\tdef updateData(self, data):\n\t\tself.means, self.workspace = poolNd(data, size=self.size, stride=1, pad=self.pad, mode=self.mode,\n\t\t\t\t\t\t\t\t\t\t\ttest=not self.train)\n\t\tself.data = Blas.addVectorToVector(data.ravel(), self.means.ravel(), beta=-1.0).reshape(*data.shape)\n\n\n\tdef updateGrad(self, grad):\n\t\tmeansGrad = poolNdBackward(self.inData, self.means, grad, self.workspace,\n\t\t\t\t\t\t\t\t   size=self.size, stride=1, pad=self.pad, mode=self.mode)\n\n\t\tBlas.addVectorToVector(grad.ravel(), meansGrad.ravel(), out=meansGrad.ravel(), beta=-1.0)\n\t\tself.grad = meansGrad\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 4:\n\t\t\traise ModuleError(""Data must be 4d tensor"")\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn shape\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 4:\n\t\t\traise ModuleError(""Grad must be 4d tensor"")\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\t\tself.means = None\n\t\tself.workspace = None\n\n\ndef unittest():\n\tbatchsize, maps, h, w = 1, 1, 6, 6\n\tsize = 3\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, h, w).astype(np.float32))\n\n\tsubtractMean = SubtractMean(size=size)\n\tsubtractMean(data)\n\n\thpad, wpad = subtractMean.pad\n\thostData = np.zeros(shape=(batchsize, maps, h + 2 * hpad, w + 2 * wpad), dtype=np.float32)\n\thostData[:, :, hpad:-hpad, wpad:-wpad] = data.get()\n\n\thostOutData = np.empty(subtractMean.data.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor y in range(data.shape[2]):\n\t\t\t\tfor x in range(data.shape[3]):\n\t\t\t\t\thostOutData[b, c, y, x] -= np.sum(hostData[b, c, y:y + size, x:x + size]) / size**2\n\n\tassert np.allclose(hostOutData, subtractMean.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*subtractMean.data.shape).astype(np.float32))\n\tsubtractMean.backward(grad)\n\n\thostGrad = grad.get()\n\thostInGrad = np.zeros(shape=hostData.shape, dtype=np.float32)\n\thostInGrad[:, :, hpad:-hpad, wpad:-wpad] = hostGrad\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor y in range(hostGrad.shape[2]):\n\t\t\t\tfor x in range(hostGrad.shape[3]):\n\t\t\t\t\tfor dy in range(size):\n\t\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\t\thostInGrad[b, c, y + dy, x + dx] -= hostGrad[b, c, y, x] / size**2\n\n\tassert np.allclose(hostInGrad[:, :, hpad:-hpad, wpad:-wpad], subtractMean.grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Sum.py,22,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend import BlasGroup\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass Sum(Module):\n\tdef __init__(self, axis, useWeights=True, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.useWeights = useWeights\n\t\tself.axis = axis\n\n\t\tself.v = None\n\t\tself.axisSize = None\n\n\n\tdef updateData(self, batch):\n\t\tdata, self.v = batch if self.useWeights else (batch, None)\n\n\t\tpreAxis, postAxis = int(np.prod(data.shape[:self.axis])), int(np.prod(data.shape[self.axis + 1:]))\n\t\tself.axisSize = data.shape[self.axis]\n\n\t\tindata = data.reshape(preAxis, self.axisSize, postAxis)\n\n\t\tif self.useWeights:\n\t\t\tself.data = BlasGroup.mulTensorOnVecGroup(indata, self.v, formatT=""gbp"", transpT=True)\n\t\telse:\n\t\t\tself.data = BlasGroup.sumOnTensorGroup(indata, formatT=""gbp"", cols=True)\n\n\t\tself.data = self.data.reshape(*data.shape[:self.axis], *data.shape[self.axis + 1:])\n\n\n\tdef updateGrad(self, grad):\n\t\tpreAxis, postAxis = int(np.prod(grad.shape[:self.axis])), int(np.prod(grad.shape[self.axis:]))\n\n\t\toutgrad = grad.reshape(preAxis, 1, postAxis)\n\t\twgrad = None\n\n\t\tif self.useWeights:\n\t\t\tv = self.v.reshape(preAxis, self.axisSize, 1)\n\n\t\t\tdatagrad = BlasGroup.mulTensorBatch(v, outgrad, formatA=""gbp"", formatB=""gbp"", formatOut=""gbp"")\n\t\t\twgrad = BlasGroup.mulTensorOnVecGroup(self.inData[0], grad, formatT=""gbp"")\n\n\t\telse:\n\t\t\tones = gpuarray.empty(shape=(1, self.axisSize, 1), dtype=np.float32).fill(1.0)\n\t\t\tdatagrad = BlasGroup.mulTensorBatch(ones, outgrad, formatA=""gbp"", formatB=""gbp"", formatOut=""gbp"")\n\n\t\tdatagrad = datagrad.reshape(*grad.shape[:self.axis], self.axisSize, *grad.shape[self.axis:])\n\t\tself.grad = [datagrad, wgrad] if self.useWeights else datagrad\n\n\n\tdef dataShapeFrom(self, shapes):\n\t\tshape = shapes[0] if self.useWeights else shapes\n\t\treturn shape[:self.axis] + shape[self.axis + 1:]\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tinshape = shape[:self.axis] + (self.axisSize, ) + shape[self.axis + 1:]\n\t\treturn [inshape, (self.axisSize, )] if self.useWeights else inshape\n\n\n\tdef checkDataShape(self, shapes):\n\t\tif self.useWeights:\n\t\t\tshape, wshape = shapes\n\n\t\t\tif len(wshape) != self.axis + 1:\n\t\t\t\traise ModuleError(""Not enough dims in weights (%d were given, need at least %d)"" %\n\t\t\t\t\t\t\t\t  (len(wshape), self.axis + 1))\n\n\t\t\tif shape[:self.axis + 1] != wshape:\n\t\t\t\traise ModuleError(""Inconsistency in data and weights shapes (%s with %s)"" % (shape, wshape))\n\n\t\telse:\n\t\t\tshape = shapes\n\n\t\tif self.axis > len(shape) - 1:\n\t\t\traise ModuleError(""Not enough dims in data (%d were given, need at least %d)"" % (len(shape), self.axis + 1))\n\n\n\tdef checkGradShape(self, shape):\n\t\tif self.axis >= len(shape):\n\t\t\traise ModuleError(""Not enough dims in grad (%d were given, need at least %d)"" % (len(shape), self.axis))\n\n\t\tif self.useWeights:\n\t\t\tif shape[:self.axis] != self.v.shape[:self.axis]:\n\t\t\t\traise ModuleError(""Inconsistency in grad and weights shapes (%s  with %s)"" % (shape, self.v.shape))\n\n\n\tdef reset(self):\n\t\tsuper().reset()\n\n\t\tself.v = None\n\t\tself.axisSize = None\n\n\ndef groupAxisTest():\n\tbatchsize, groups, size = 5, 3, 4\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, groups, size).astype(np.float32))\n\n\tsummod = Sum(axis=1, useWeights=False)\n\tsummod(data)\n\n\thostOutData = np.sum(data.get(), axis=1)\n\tassert np.allclose(hostOutData, summod.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(batchsize, size).astype(np.float32))\n\tsummod.backward(grad)\n\n\thostInGrad = np.repeat(grad.get().reshape(batchsize, 1, size), repeats=groups, axis=1)\n\tassert np.allclose(hostInGrad, summod.grad.get())\n\n\tweights = gpuarray.to_gpu(np.random.randn(batchsize, groups).astype(np.float32))\n\thostWeights = weights.get().reshape(*weights.shape, 1)\n\n\tsummod = Sum(axis=1, useWeights=True)\n\tsummod([data, weights])\n\n\thostOutData = np.sum(data.get() * hostWeights, axis=1)\n\tassert np.allclose(hostOutData, summod.data.get())\n\n\tsummod.backward(grad)\n\n\thostInGrad = np.repeat(grad.get().reshape(batchsize, 1, size), repeats=groups, axis=1) * hostWeights\n\tassert np.allclose(hostInGrad, summod.grad[0].get())\n\n\thostWGrad = np.sum(data.get() * grad.get().reshape(batchsize, 1, size), axis=2)\n\tassert np.allclose(hostWGrad, summod.grad[1].get())\n\n\ndef preLastAxisTest():\n\tbatchsize, seqlen, groups, size = 5, 3, 2, 6\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, seqlen, groups, size).astype(np.float32))\n\n\tsummod = Sum(axis=2, useWeights=False)\n\tsummod(data)\n\n\thostOutData = np.sum(data.get(), axis=2)\n\tassert np.allclose(hostOutData, summod.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(batchsize, seqlen, size).astype(np.float32))\n\tsummod.backward(grad)\n\n\thostInGrad = np.repeat(grad.get().reshape(batchsize, seqlen, 1, size), repeats=groups, axis=2)\n\tassert np.allclose(hostInGrad, summod.grad.get())\n\n\ndef unittest():\n\tgroupAxisTest()\n\tpreLastAxisTest()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/SwapAxes.py,6,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray, Memory\nfrom PuzzleLib.Backend.Utils import dtypesSupported\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass SwapAxes(Module):\n\tdef __init__(self, axis1, axis2, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.axis1, self.axis2 = (axis2, axis1) if axis1 > axis2 else (axis1, axis2)\n\n\n\tdef updateData(self, data):\n\t\tself.data = Memory.swapaxes(data, self.axis1, self.axis2)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = Memory.swapaxes(grad, self.axis1, self.axis2)\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) - 1 < self.axis2:\n\t\t\traise ModuleError(""Data dimension needs to be at least %d, (data has %d)"" % (self.axis2 + 1, len(shape)))\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) - 1 < self.axis2:\n\t\t\traise ModuleError(""Grad dimension needs to be at least %d, (grad has %d)"" % (self.axis2 + 1, len(shape)))\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape[:self.axis1] + (shape[self.axis2], ) + shape[self.axis1 + 1:self.axis2] + \\\n\t\t\t   (shape[self.axis1], ) + shape[self.axis2 + 1:]\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn shape[:self.axis1] + (shape[self.axis2], ) + shape[self.axis1 + 1:self.axis2] + \\\n\t\t\t   (shape[self.axis1], ) + shape[self.axis2 + 1:]\n\n\n\tdef calcMode(self, T):\n\t\tdtypes = {dtype for dtype, _ in dtypesSupported()}\n\n\t\tif T not in dtypes:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n\n\ndef unittest():\n\tfor dtype, _ in dtypesSupported():\n\t\tswapAxesTest(dtype)\n\n\ndef swapAxesTest(dtype):\n\tshape = (10, 3, 5, 4, 2)\n\n\tfor axis1 in range(len(shape)):\n\t\tfor axis2 in range(axis1 + 1, len(shape)):\n\t\t\thostData = np.random.randn(*shape).astype(dtype)\n\t\t\tdata = gpuarray.to_gpu(hostData)\n\n\t\t\tswapaxes = SwapAxes(axis1, axis2)\n\t\t\tswapaxes.calcMode(dtype)\n\n\t\t\tswapaxes(data)\n\n\t\t\thostOutData = np.swapaxes(hostData, axis1=axis1, axis2=axis2)\n\t\t\tassert np.allclose(hostOutData, swapaxes.data.get())\n\n\t\t\thostGrad = np.random.randn(*swapaxes.data.shape).astype(dtype)\n\t\t\tgrad = gpuarray.to_gpu(hostGrad)\n\n\t\t\tswapaxes.backward(grad)\n\n\t\t\thostInGrad = np.swapaxes(hostGrad, axis1=axis2, axis2=axis1)\n\n\t\t\tassert swapaxes.grad.shape == data.shape\n\t\t\tassert np.allclose(hostInGrad, swapaxes.grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Tile.py,12,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray, Blas, Utils\nfrom PuzzleLib.Backend.Utils import dtypesSupported\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass Tile(Module):\n\tdef __init__(self, axis, times, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.axis = axis\n\t\tself.times = times\n\n\n\tdef updateData(self, data):\n\t\tself.data = Utils.tile(data, self.times, axis=self.axis)\n\n\n\tdef updateGrad(self, grad):\n\t\tsections = [grad.shape[self.axis] // self.times] * self.times\n\t\tingrad = Utils.split(grad, sections, axis=self.axis)\n\n\t\tfor i in range(1, len(ingrad)):\n\t\t\tBlas.toVectorAddVector(ingrad[0].ravel(), ingrad[i].ravel())\n\n\t\tself.grad = ingrad[0]\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) < self.axis + 1:\n\t\t\traise ModuleError(""Not enough dimensions in data shape (%s given, %s required)"" % (len(shape), self.axis+1))\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn shape[:self.axis] + (shape[self.axis] * self.times, ) + shape[self.axis + 1:]\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) < self.axis + 1:\n\t\t\traise ModuleError(""Not enough dimensions in grad shape (%s given, %s required)"" % (len(shape), self.axis+1))\n\n\t\tif shape[self.axis] % self.times != 0:\n\t\t\traise ModuleError(""Dimension %s in grad shape must be divisible by %s"" % (shape[self.axis], self.times))\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn shape[:self.axis] + (shape[self.axis] // self.times, ) + shape[self.axis + 1:]\n\n\n\tdef calcMode(self, T):\n\t\tdtypes = {dtype for dtype, _ in dtypesSupported()}\n\n\t\tif T not in dtypes:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n\n\ndef unittest():\n\tfor dtype, _ in dtypesSupported():\n\t\talongBatchAxisTest(dtype)\n\t\talongDataAxisTest(dtype)\n\n\ndef alongBatchAxisTest(dtype):\n\thostData = np.random.randn(3, 4, 5).astype(dtype)\n\tdata = gpuarray.to_gpu(hostData)\n\n\taxis, times = 0, 3\n\n\ttile = Tile(axis=axis, times=times)\n\ttile.calcMode(dtype)\n\n\ttile(data)\n\n\thostOutData = np.concatenate([data.get()] * times, axis=axis)\n\tassert np.allclose(hostOutData, tile.data.get())\n\n\thostGrad = np.random.randn(*hostOutData.shape).astype(dtype)\n\tgrad = gpuarray.to_gpu(hostGrad)\n\n\ttile.backward(grad)\n\n\thostInGrad = np.sum(hostGrad.reshape((-1, 3, 4, 5)), axis=axis)\n\tassert np.allclose(hostInGrad, tile.grad.get())\n\n\ndef alongDataAxisTest(dtype):\n\thostData = np.random.randn(3, 4, 5).astype(dtype)\n\tdata = gpuarray.to_gpu(hostData)\n\n\taxis, times = 1, 4\n\n\ttile = Tile(axis=axis, times=times)\n\ttile.calcMode(dtype)\n\n\ttile(data)\n\n\thostOutData = np.concatenate([data.get()] * times, axis=axis)\n\tassert np.allclose(hostOutData, tile.data.get())\n\n\thostGrad = np.random.randn(*hostOutData.shape).astype(dtype)\n\tgrad = gpuarray.to_gpu(hostGrad)\n\n\ttile.backward(grad)\n\n\thostInGrad = np.sum(hostGrad.reshape((3, -1, 4, 5)), axis=axis)\n\tassert np.allclose(hostInGrad, tile.grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/ToList.py,10,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass ToList(Module):\n\tdef __init__(self, name=None):\n\t\tsuper().__init__(name)\n\n\t\tself.movesData = True\n\t\tself.movesGrad = True\n\n\n\tdef updateData(self, data):\n\t\tself.data = []\n\t\tself.extendDataList(self.data, data)\n\n\n\tdef extendDataList(self, lst, data):\n\t\tif isinstance(data, gpuarray.GPUArray):\n\t\t\tlst.append(data)\n\t\telse:\n\t\t\tfor dat in data:\n\t\t\t\tself.extendDataList(lst, dat)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad, _ = self.buildGradList(grad, self.inData, 0)\n\n\n\tdef buildGradList(self, grad, data, i):\n\t\tif isinstance(data, gpuarray.GPUArray):\n\t\t\ti += 1\n\t\t\treturn grad[i - 1], i\n\t\telse:\n\t\t\tlst = []\n\t\t\tfor dat in data:\n\t\t\t\tinlst, i = self.buildGradList(grad, dat, i)\n\t\t\t\tlst.append(inlst)\n\n\t\t\treturn lst, i\n\n\n\tdef dataShapeFrom(self, shapes):\n\t\tlst = []\n\t\tself.extendDataShapeList(lst, shapes)\n\t\treturn lst\n\n\n\tdef extendDataShapeList(self, lst, shapes):\n\t\tif isinstance(shapes, tuple):\n\t\t\tlst.append(shapes)\n\t\telse:\n\t\t\tfor shape in shapes:\n\t\t\t\tself.extendDataShapeList(lst, shape)\n\n\n\tdef gradShapeFrom(self, shapes):\n\t\tinshapes, _ = self.buildGradShapeList(shapes, self.inData, 0)\n\t\treturn inshapes\n\n\n\tdef buildGradShapeList(self, shapes, data, i):\n\t\tif isinstance(data, gpuarray.GPUArray):\n\t\t\ti += 1\n\t\t\treturn shapes[i - 1], i\n\n\t\telse:\n\t\t\tlst = []\n\t\t\tfor dat in data:\n\t\t\t\tinlst, i = self.buildGradShapeList(shapes, dat, i)\n\t\t\t\tlst.append(inlst)\n\n\t\t\treturn lst, i\n\n\n\tdef checkGradShape(self, shapes):\n\t\tself.checkGradList(shapes, self.inData, 0)\n\n\n\tdef checkGradList(self, shapes, data, i):\n\t\tif isinstance(data, gpuarray.GPUArray):\n\t\t\tif data.shape != shapes[i]:\n\t\t\t\traise ModuleError(""Inconsistency in data and corresponding grad shapes at index %s ""\n\t\t\t\t\t\t\t\t  ""(expected %s, given %s)"" % (i, data.shape, shapes[i]))\n\t\t\ti += 1\n\t\t\treturn i\n\n\t\telse:\n\t\t\tfor dat in data:\n\t\t\t\ti = self.checkGradList(shapes, dat, i)\n\n\t\t\treturn i\n\n\ndef unittest():\n\tdata1 = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\tdata2 = gpuarray.to_gpu(np.random.randn(5, 5).astype(np.float32))\n\tdata3 = gpuarray.to_gpu(np.random.randn(3, 6).astype(np.float32))\n\n\tdata = [[data1, data2], data3]\n\toutdata = [data1, data2, data3]\n\n\ttolist = ToList()\n\ttolist(data)\n\n\tassert all(np.allclose(d.get(), outdata[i].get()) for i, d in enumerate(tolist.data))\n\n\tshapes = tolist.dataShapeFrom([data1.shape, [data2.shape, data3.shape]])\n\tassert all(outdata[i].shape == shape for i, shape in enumerate(shapes))\n\n\tgrad1 = gpuarray.to_gpu(np.random.randn(10, 10).astype(np.float32))\n\tgrad2 = gpuarray.to_gpu(np.random.randn(5, 5).astype(np.float32))\n\tgrad3 = gpuarray.to_gpu(np.random.randn(3, 6).astype(np.float32))\n\n\tgrad = [grad1, grad2, grad3]\n\tingrad = [[grad1, grad2], grad3]\n\n\ttolist.backward(grad)\n\n\tassert np.allclose(ingrad[0][0].get(), grad1.get())\n\tassert np.allclose(ingrad[0][1].get(), grad2.get())\n\tassert np.allclose(ingrad[1].get(), grad3.get())\n\n\tinshapes = tolist.gradShapeFrom([gr.shape for gr in grad])\n\n\tassert inshapes[0][0] == grad1.shape\n\tassert inshapes[0][1] == grad2.shape\n\tassert inshapes[1] == grad3.shape\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Transpose.py,6,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray, Memory\nfrom PuzzleLib.Backend.Utils import dtypesSupported\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass Transpose(Module):\n\tdef __init__(self, axes=None, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.axes = axes\n\n\t\tif axes is None:\n\t\t\tself.invaxes = None\n\t\telse:\n\t\t\tself.invaxes = [0] * len(axes)\n\t\t\tfor i, axis in enumerate(axes):\n\t\t\t\tself.invaxes[axis] = i\n\n\n\tdef updateData(self, data):\n\t\tself.data = Memory.transpose(data, self.axes)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = Memory.transpose(grad, self.invaxes)\n\n\n\tdef checkDataShape(self, shape):\n\t\tif self.axes is not None and len(shape) != len(self.axes):\n\t\t\traise ModuleError(""Data dimension needs to be %d, (data has %d)"" % (len(self.axes), len(shape)))\n\n\n\tdef checkGradShape(self, shape):\n\t\tif self.axes is not None and len(shape) != len(self.axes):\n\t\t\traise ModuleError(""Grad dimension needs to be %d, (grad has %d)"" % (len(self.axes), len(shape)))\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn tuple(shape[axis] for axis in self.axes)\n\n\n\tdef gradShapeFrom(self, shape):\n\t\treturn tuple(shape[axis] for axis in self.invaxes)\n\n\n\tdef calcMode(self, T):\n\t\tdtypes = {dtype for dtype, _ in dtypesSupported()}\n\n\t\tif T not in dtypes:\n\t\t\traise ModuleError(""Unsupported dtype %s"" % T)\n\n\t\tself.calctype = T\n\n\ndef unittest():\n\tfor dtype, _ in dtypesSupported():\n\t\ttransposeTest(dtype)\n\n\ndef transposeTest(dtype):\n\tshape = (10, 3, 5, 4, 2)\n\taxes = (2, 4, 1, 3, 0)\n\n\thostData = np.random.randn(*shape).astype(dtype)\n\tdata = gpuarray.to_gpu(hostData)\n\n\ttranspose = Transpose(axes)\n\ttranspose.calcMode(dtype)\n\n\ttranspose(data)\n\n\thostOutData = np.transpose(hostData, axes=axes)\n\tassert np.allclose(hostOutData, transpose.data.get())\n\n\thostGrad = np.random.randn(*transpose.data.shape).astype(dtype)\n\tgrad = gpuarray.to_gpu(hostGrad)\n\n\ttranspose.backward(grad)\n\n\thostInGrad = np.transpose(hostGrad, axes=transpose.invaxes)\n\tassert np.allclose(hostInGrad, transpose.grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Upsample2D.py,6,"b'from enum import Enum\n\nimport numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Kernels import Upsample\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\n\n\nclass UpsampleMode(str, Enum):\n\tnearest = ""nearest""\n\tlinear = ""linear""\n\n\nclass Upsample2D(Module):\n\tdef __init__(self, scale=2, mode=""nearest"", name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.scale = scale\n\t\tself.mode = UpsampleMode(mode)\n\n\n\tdef updateData(self, data):\n\t\tself.data = Upsample.upsample2d(data, self.scale, mode=self.mode.value)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = Upsample.upsample2dBackward(grad, self.scale, mode=self.mode.value)\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 4:\n\t\t\traise ModuleError(""Data must be 4d tensor"")\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 4:\n\t\t\traise ModuleError(""Grad must be 4d tensor"")\n\n\t\t_, _, h, w = shape\n\t\tif h % self.scale != 0 or w % self.scale != 0:\n\t\t\traise ModuleError(""Grad map size is not divisible by scale %s"" % self.scale)\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tbatchsize, maps, h, w = shape\n\t\treturn batchsize, maps, self.scale * h, self.scale * w\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tbatchsize, maps, h, w = shape\n\t\treturn batchsize, maps, h // self.scale, w // self.scale\n\n\ndef unittest():\n\tbatchsize, maps, inh, inw = 3, 4, 5, 6\n\tscale = 2\n\n\tdata = gpuarray.to_gpu(np.random.uniform(low=-1.0, high=1.0, size=(batchsize, maps, inh, inw)).astype(np.float32))\n\n\tupsample2d = Upsample2D(scale=scale, mode=""nearest"")\n\tupsample2d(data)\n\n\thostData = data.get()\n\thostOutData = np.empty(upsample2d.data.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor y in range(inh):\n\t\t\t\tfor x in range(inw):\n\t\t\t\t\thostOutData[b, c, y * scale:(y + 1) * scale, x * scale:(x + 1) * scale] = hostData[b, c, y, x]\n\n\tassert np.allclose(hostOutData, upsample2d.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.uniform(low=-1.0, high=1.0, size=upsample2d.data.shape).astype(np.float32))\n\tupsample2d.backward(grad)\n\n\thostGrad = grad.get()\n\thostInGrad = np.zeros(data.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor y in range(inh):\n\t\t\t\tfor x in range(inw):\n\t\t\t\t\tfor dy in range(scale):\n\t\t\t\t\t\tfor dx in range(scale):\n\t\t\t\t\t\t\thostInGrad[b, c, y, x] += hostGrad[b, c, y * scale + dy, x * scale + dx]\n\n\tassert np.allclose(hostInGrad, upsample2d.grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/Upsample3D.py,7,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Kernels import Upsample\n\nfrom PuzzleLib.Modules.Module import ModuleError, Module\nfrom PuzzleLib.Modules.Upsample2D import UpsampleMode\n\n\nclass Upsample3D(Module):\n\tdef __init__(self, scale=2, mode=""nearest"", name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tself.scale = scale\n\t\tself.mode = UpsampleMode(mode)\n\n\n\tdef updateData(self, data):\n\t\tself.data = Upsample.upsample3d(data, self.scale, mode=self.mode.value)\n\n\n\tdef updateGrad(self, grad):\n\t\tself.grad = Upsample.upsample3dBackward(grad, self.scale, mode=self.mode.value)\n\n\n\tdef checkDataShape(self, shape):\n\t\tif len(shape) != 5:\n\t\t\traise ModuleError(""Data must be 5d tensor"")\n\n\n\tdef checkGradShape(self, shape):\n\t\tif len(shape) != 5:\n\t\t\traise ModuleError(""Grad must be 5d tensor"")\n\n\t\t_, _, d, h, w = shape\n\t\tif d % self.scale != 0 or h % self.scale != 0 or w % self.scale != 0:\n\t\t\traise ModuleError(""Grad map size is not divisible by scale %s (got %s, %s, %s)"" % (self.scale, d, h, w))\n\n\n\tdef dataShapeFrom(self, shape):\n\t\tbatchsize, maps, d, h, w = shape\n\t\treturn batchsize, maps, self.scale * d, self.scale * h, self.scale * w\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tbatchsize, maps, d, h, w = shape\n\t\treturn batchsize, maps, d // self.scale, h // self.scale, w // self.scale\n\n\ndef unittest():\n\tbatchsize, maps, ind, inh, inw = 2, 2, 2, 2, 2\n\tscale = 2\n\n\tdata = gpuarray.to_gpu(np.random.uniform(low=-1.0, high=1.0,\n\t\t\t\t\t\t\t\t\t\t\t size=(batchsize, maps, ind, inh, inw)).astype(np.float32))\n\n\tupsample3d = Upsample3D(scale=scale, mode=""nearest"")\n\tupsample3d(data)\n\n\thostData = data.get()\n\thostOutData = np.empty(upsample3d.data.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor z in range(ind):\n\t\t\t\tfor y in range(inh):\n\t\t\t\t\tfor x in range(inw):\n\t\t\t\t\t\thostOutData[b, c, z * scale:(z+1) * scale, y * scale:(y+1) * scale, x * scale:(x+1) * scale] = \\\n\t\t\t\t\t\t\thostData[b, c, z, y, x]\n\n\tassert np.allclose(hostOutData, upsample3d.data.get())\n\n\tgrad = gpuarray.to_gpu(np.random.randn(*upsample3d.data.shape).astype(np.float32))\n\tupsample3d.backward(grad)\n\n\thostGrad = grad.get()\n\thostInGrad = np.zeros(data.shape, dtype=np.float32)\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor z in range(ind):\n\t\t\t\tfor y in range(inh):\n\t\t\t\t\tfor x in range(inw):\n\t\t\t\t\t\tfor dz in range(scale):\n\t\t\t\t\t\t\tfor dy in range(scale):\n\t\t\t\t\t\t\t\tfor dx in range(scale):\n\t\t\t\t\t\t\t\t\thostInGrad[b, c, z, y, x] += \\\n\t\t\t\t\t\t\t\t\t\thostGrad[b, c, z * scale + dz, y * scale + dy, x * scale + dx]\n\n\tassert np.allclose(hostInGrad, upsample3d.grad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Modules/__init__.py,0,"b'from PuzzleLib.Modules.Activation import Activation, ActivationType, sigmoid, tanh, relu, leakyRelu, elu, softPlus, clip\nfrom PuzzleLib.Modules.Add import Add\nfrom PuzzleLib.Modules.AvgPool1D import AvgPool1D\nfrom PuzzleLib.Modules.AvgPool2D import AvgPool2D\nfrom PuzzleLib.Modules.AvgPool3D import AvgPool3D\nfrom PuzzleLib.Modules.BatchNorm import BatchNorm\nfrom PuzzleLib.Modules.BatchNorm1D import BatchNorm1D\nfrom PuzzleLib.Modules.BatchNorm2D import BatchNorm2D\nfrom PuzzleLib.Modules.BatchNorm3D import BatchNorm3D\nfrom PuzzleLib.Modules.Cast import Cast, DataType\nfrom PuzzleLib.Modules.Concat import Concat\nfrom PuzzleLib.Modules.Conv1D import Conv1D\nfrom PuzzleLib.Modules.Conv2D import Conv2D\nfrom PuzzleLib.Modules.Conv3D import Conv3D\nfrom PuzzleLib.Modules.CrossMapLRN import CrossMapLRN\nfrom PuzzleLib.Modules.Deconv1D import Deconv1D\nfrom PuzzleLib.Modules.Deconv2D import Deconv2D\nfrom PuzzleLib.Modules.Deconv3D import Deconv3D\nfrom PuzzleLib.Modules.DepthConcat import DepthConcat\nfrom PuzzleLib.Modules.Dropout import Dropout\nfrom PuzzleLib.Modules.Dropout2D import Dropout2D\nfrom PuzzleLib.Modules.Embedder import Embedder\nfrom PuzzleLib.Modules.Flatten import Flatten\nfrom PuzzleLib.Modules.Gelu import Gelu\nfrom PuzzleLib.Modules.Glue import Glue\nfrom PuzzleLib.Modules.GroupLinear import GroupLinear, GroupMode\nfrom PuzzleLib.Modules.Identity import Identity\nfrom PuzzleLib.Modules.InstanceNorm2D import InstanceNorm2D\nfrom PuzzleLib.Modules.KMaxPool import KMaxPool\nfrom PuzzleLib.Modules.LCN import LCN\nfrom PuzzleLib.Modules.Linear import Linear\nfrom PuzzleLib.Modules.MapLRN import MapLRN\nfrom PuzzleLib.Modules.MaxPool1D import MaxPool1D\nfrom PuzzleLib.Modules.MaxPool2D import MaxPool2D\nfrom PuzzleLib.Modules.MaxPool3D import MaxPool3D\nfrom PuzzleLib.Modules.MaxUnpool2D import MaxUnpool2D\nfrom PuzzleLib.Modules.Module import Module, ModuleError, InitScheme, MemoryUnit\nfrom PuzzleLib.Modules.MoveAxis import MoveAxis\nfrom PuzzleLib.Modules.Mul import Mul\nfrom PuzzleLib.Modules.MulAddConst import MulAddConst\nfrom PuzzleLib.Modules.NoiseInjector import NoiseInjector, InjectMode, NoiseType\nfrom PuzzleLib.Modules.Pad1D import Pad1D\nfrom PuzzleLib.Modules.Pad2D import Pad2D, PadMode\nfrom PuzzleLib.Modules.Penalty import Penalty, PenaltyMode\nfrom PuzzleLib.Modules.PRelu import PRelu\nfrom PuzzleLib.Modules.Replicate import Replicate\nfrom PuzzleLib.Modules.Reshape import Reshape\nfrom PuzzleLib.Modules.RNN import RNN, RNNMode, DirectionMode, WeightModifier\nfrom PuzzleLib.Modules.Slice import Slice\nfrom PuzzleLib.Modules.SoftMax import SoftMax\nfrom PuzzleLib.Modules.SpatialTf import SpatialTf\nfrom PuzzleLib.Modules.Split import Split\nfrom PuzzleLib.Modules.SubtractMean import SubtractMean\nfrom PuzzleLib.Modules.Sum import Sum\nfrom PuzzleLib.Modules.SwapAxes import SwapAxes\nfrom PuzzleLib.Modules.Tile import Tile\nfrom PuzzleLib.Modules.ToList import ToList\nfrom PuzzleLib.Modules.Transpose import Transpose\nfrom PuzzleLib.Modules.Upsample2D import Upsample2D, UpsampleMode\nfrom PuzzleLib.Modules.Upsample3D import Upsample3D\n'"
Optimizers/AdaDelta.py,9,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import dtypesSupported\nfrom PuzzleLib.Backend.Kernels.ElementWise import adadeltaKer\n\nfrom PuzzleLib.Optimizers.Optimizer import Optimizer, trainSimpleTest, trainHardTest\n\n\nclass AdaDelta(Optimizer):\n\tdef __init__(self, rho=0.95, epsilon=1e-6, nodeinfo=None):\n\t\tsuper().__init__(nodeinfo)\n\n\t\tself.rho = None\n\t\tself.epsilon = None\n\n\t\tself.setAttr(""rho"", rho)\n\t\tself.setAttr(""epsilon"", epsilon)\n\n\n\tdef setupState(self, var):\n\t\treturn {\n\t\t\t""msg"": gpuarray.zeros(var.data.shape, dtype=var.data.dtype),\n\t\t\t""msdx"": gpuarray.zeros(var.data.shape, dtype=var.data.dtype)\n\t\t}\n\n\n\tdef updateVar(self, var, state, stream=None):\n\t\tadadeltaKer(var.data.dtype)(\n\t\t\tvar.data, var.grad, state[""msg""], state[""msdx""], self.rho, self.epsilon, stream=stream\n\t\t)\n\n\ndef unittest():\n\tfor dtype, atol in dtypesSupported():\n\t\tcalcTest(dtype, atol)\n\t\ttrainSimpleTest(AdaDelta, dtype)\n\n\t\tif Config.backend == Config.Backend.cuda:\n\t\t\ttrainHardTest(AdaDelta, dtype)\n\n\ndef calcTest(dtype, atol):\n\trho, epsilon = 0.95, 1e-6\n\tshape = (11, 13)\n\n\thostW, hostDw = np.random.randn(*shape).astype(dtype), np.random.randn(*shape).astype(dtype)\n\thostMsg = (1.0 + np.random.randn(*shape)**2).astype(dtype)\n\thostMsdx = (1.0 + np.random.randn(*shape)**2).astype(dtype)\n\n\tw, dw = gpuarray.to_gpu(hostW), gpuarray.to_gpu(hostDw)\n\tmsg, msdx = gpuarray.to_gpu(hostMsg), gpuarray.to_gpu(hostMsdx)\n\n\tadadeltaKer(w.dtype)(w, dw, msg, msdx, rho, epsilon)\n\n\thostW, hostDw = hostW.astype(np.float32), hostDw.astype(np.float32)\n\thostMsg, hostMsdx = hostMsg.astype(np.float32), hostMsdx.astype(np.float32)\n\n\thostMsg += (1.0 - rho) * (hostDw * hostDw - hostMsg)\n\thostDx = np.sqrt((hostMsdx + epsilon) / (hostMsg + epsilon)) * hostDw\n\thostMsdx += (1.0 - rho) * (hostDx**2 - hostMsdx)\n\thostW += hostDx\n\n\thostW, hostDw = hostW.astype(dtype), hostDw.astype(dtype)\n\thostMsg, hostMsdx = hostMsg.astype(dtype), hostMsdx.astype(dtype)\n\n\tassert np.allclose(hostMsg, msg.get(), atol=atol)\n\tassert np.allclose(hostMsdx, msdx.get(), atol=atol)\n\tassert np.allclose(hostW, w.get(), atol=atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Optimizers/AdaGrad.py,6,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import dtypesSupported\nfrom PuzzleLib.Backend.Kernels.ElementWise import adagradKer\n\nfrom PuzzleLib.Optimizers.Optimizer import Optimizer, trainSimpleTest, trainHardTest\n\n\nclass AdaGrad(Optimizer):\n\tdef __init__(self, learnRate=1e-3, epsilon=1e-8, nodeinfo=None):\n\t\tsuper().__init__(nodeinfo)\n\n\t\tself.epsilon = None\n\n\t\tself.setAttr(""learnRate"", learnRate)\n\t\tself.setAttr(""epsilon"", epsilon)\n\n\n\tdef setupState(self, var):\n\t\treturn {""h"": gpuarray.zeros(var.data.shape, dtype=var.data.dtype)}\n\n\n\tdef updateVar(self, var, state, stream=None):\n\t\tadagradKer(var.data.dtype)(\n\t\t\tvar.data, var.grad, state[""h""], self.learnRate * var.learnRate, self.epsilon, stream=stream\n\t\t)\n\n\ndef unittest():\n\tfor dtype, atol in dtypesSupported():\n\t\tcalcTest(dtype, atol)\n\t\ttrainSimpleTest(AdaGrad, dtype, learnRate=1e-2)\n\n\t\tif Config.backend == Config.Backend.cuda:\n\t\t\ttrainHardTest(AdaGrad, dtype, learnRate=1e-2)\n\n\ndef calcTest(dtype, atol):\n\tlr, epsilon = 0.01, 1e-8\n\tshape = (11, 13)\n\n\thostW, hostDw = np.random.randn(*shape).astype(dtype), np.random.randn(*shape).astype(dtype)\n\thostH = (1.0 + np.random.randn(*shape)**2).astype(dtype)\n\n\tw, dw, h = gpuarray.to_gpu(hostW), gpuarray.to_gpu(hostDw), gpuarray.to_gpu(hostH)\n\tadagradKer(w.dtype)(w, dw, h, lr, epsilon)\n\n\thostW, hostDw, hostH = hostW.astype(np.float32), hostDw.astype(np.float32), hostH.astype(np.float32)\n\n\thostH += hostDw**2\n\thostW += lr * hostDw / (np.sqrt(hostH) + epsilon)\n\n\thostW, hostDw, hostH = hostW.astype(dtype), hostDw.astype(dtype), hostH.astype(dtype)\n\n\tassert np.allclose(hostH, h.get(), atol=atol)\n\tassert np.allclose(hostW, w.get(), atol=atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Optimizers/Adam.py,9,"b'import math\n\nimport numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import dtypesSupported\nfrom PuzzleLib.Backend.Kernels.ElementWise import adamKer\n\nfrom PuzzleLib.Optimizers.Optimizer import Optimizer, trainSimpleTest, trainHardTest\n\n\nclass Adam(Optimizer):\n\tdef __init__(self, alpha=1e-3, beta1=0.9, beta2=0.999, epsilon=1e-8, nodeinfo=None):\n\t\tsuper().__init__(nodeinfo)\n\n\t\tself.alpha = None\n\t\tself.beta1 = None\n\t\tself.beta2 = None\n\t\tself.epsilon = None\n\n\t\tself.setAttr(""alpha"", alpha)\n\t\tself.setAttr(""beta1"", beta1)\n\t\tself.setAttr(""beta2"", beta2)\n\t\tself.setAttr(""epsilon"", epsilon)\n\n\n\tdef setupState(self, var):\n\t\treturn {\n\t\t\t""mg"": gpuarray.zeros(var.data.shape, dtype=np.float32),\n\t\t\t""ms"": gpuarray.zeros(var.data.shape, dtype=np.float32)\n\t\t}\n\n\n\tdef updateVar(self, var, state, stream=None):\n\t\tfix1, fix2 = 1.0 - self.beta1**self.t, 1.0 - self.beta2**self.t\n\t\tself.learnRate = self.alpha * math.sqrt(fix2) / fix1\n\n\t\tfix1, fix2 = 1.0 - self.beta1, 1.0 - self.beta2\n\t\tadamKer(var.data.dtype)(\n\t\t\tvar.data, var.grad, state[""mg""], state[""ms""], self.learnRate * var.learnRate, fix1, fix2, self.epsilon,\n\t\t\tstream=stream\n\t\t)\n\n\ndef unittest():\n\tfor dtype, atol in dtypesSupported():\n\t\tcalcTest(dtype, atol)\n\t\ttrainSimpleTest(Adam, dtype, alpha=1e-2)\n\n\t\tif Config.backend == Config.Backend.cuda:\n\t\t\ttrainHardTest(Adam, dtype, alpha=1e-2)\n\n\ndef calcTest(dtype, atol):\n\talpha, beta1, beta2, epsilon = 0.01, 0.9, 0.999, 1e-8\n\tshape = (11, 13)\n\n\thostW, hostDw = np.random.randn(*shape).astype(dtype), np.random.randn(*shape).astype(dtype)\n\thostMs, hostMg = (1.0 + np.random.randn(*shape)**2).astype(np.float32), np.random.randn(*shape).astype(np.float32)\n\n\tw, dw = gpuarray.to_gpu(hostW), gpuarray.to_gpu(hostDw)\n\tms, mg = gpuarray.to_gpu(hostMs), gpuarray.to_gpu(hostMg)\n\n\tfix1, fix2 = 1.0 - beta1, 1.0 - beta2\n\tlr = alpha * math.sqrt(fix2) / fix1\n\n\tfix1, fix2 = 1.0 - beta1, 1.0 - beta2\n\tadamKer(w.dtype)(w, dw, mg, ms, lr, fix1, fix2, epsilon)\n\n\thostW, hostDw = hostW.astype(np.float32), hostDw.astype(np.float32)\n\n\thostMg = (1 - fix1) * hostMg + fix1 * hostDw\n\thostMs = (1 - fix2) * hostMs + fix2 * hostDw**2\n\thostW += lr * hostMg / (np.sqrt(hostMs) + epsilon)\n\n\thostW, hostDw = hostW.astype(dtype), hostDw.astype(dtype)\n\n\tassert np.allclose(hostMg, mg.get(), atol=atol)\n\tassert np.allclose(hostMs, ms.get(), atol=atol)\n\tassert np.allclose(hostW, w.get(), atol=atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Optimizers/Hooks.py,1,"b'import numpy as np\n\nfrom PuzzleLib.Backend.Kernels.ElementWise import weightDecayKer\n\n\nclass Hook:\n\tdef __call__(self, var, state, stream=None):\n\t\traise NotImplementedError()\n\n\nclass WeightDecay(Hook):\n\tdef __init__(self, rate):\n\t\tself.rate = rate\n\n\n\tdef __call__(self, var, state, stream=None):\n\t\tassert var.grad.dtype == np.float32\n\t\tif var.wc > 0.0:\n\t\t\tweightDecayKer(var.grad, var.data, self.rate * var.wc, stream=stream)\n'"
Optimizers/MomentumSGD.py,5,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import dtypesSupported\nfrom PuzzleLib.Backend.Kernels.ElementWise import classicMomSGDKer\n\nfrom PuzzleLib.Optimizers.Optimizer import trainSimpleTest, trainHardTest\nfrom PuzzleLib.Optimizers.SGD import SGD\n\n\nclass MomentumSGD(SGD):\n\tdef __init__(self, learnRate=1e-3, momRate=0.9, nodeinfo=None):\n\t\tsuper().__init__(learnRate, nodeinfo)\n\n\t\tself.momRate = None\n\t\tself.setAttr(""momRate"", momRate)\n\n\n\tdef setupState(self, var):\n\t\treturn {""mom"": gpuarray.zeros(var.data.shape, dtype=var.data.dtype)}\n\n\n\tdef updateVar(self, var, state, stream=None):\n\t\tclassicMomSGDKer(var.data.dtype)(\n\t\t\tvar.data, var.grad, state[""mom""], self.learnRate * var.learnRate, self.momRate * var.momRate, stream=stream\n\t\t)\n\n\ndef unittest():\n\tfor dtype, atol in dtypesSupported():\n\t\tcalcTest(dtype, atol)\n\t\ttrainSimpleTest(MomentumSGD, dtype, learnRate=1e-1, momRate=0.9)\n\n\t\tif Config.backend == Config.Backend.cuda:\n\t\t\ttrainHardTest(MomentumSGD, dtype, learnRate=1e-1, momRate=0.9)\n\n\ndef calcTest(dtype, atol):\n\tlr, mr = 0.01, 0.9\n\tshape = (11, 13)\n\n\thostW, hostDw = np.random.randn(*shape).astype(dtype), np.random.randn(*shape).astype(dtype)\n\thostMom = np.random.randn(*shape).astype(dtype)\n\n\tw, dw, mom = gpuarray.to_gpu(hostW), gpuarray.to_gpu(hostDw), gpuarray.to_gpu(hostMom)\n\tclassicMomSGDKer(w.dtype)(w, dw, mom, lr, mr)\n\n\thostW, hostDw, hostMom = hostW.astype(np.float32), hostDw.astype(np.float32), hostMom.astype(np.float32)\n\n\thostMom = mr * hostMom + lr * hostDw\n\thostW += hostMom\n\n\thostW, hostDw, hostMom = hostW.astype(dtype), hostDw.astype(dtype), hostMom.astype(dtype)\n\n\tassert np.allclose(hostMom, mom.get(), atol=atol)\n\tassert np.allclose(hostW, w.get(), atol=atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Optimizers/NesterovSGD.py,5,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import dtypesSupported\nfrom PuzzleLib.Backend.Kernels.ElementWise import nesterovMomSGDKer\n\nfrom PuzzleLib.Optimizers.Optimizer import trainSimpleTest, trainHardTest\nfrom PuzzleLib.Optimizers.SGD import SGD\n\n\nclass NesterovSGD(SGD):\n\tdef __init__(self, learnRate=1e-3, momRate=0.9, nodeinfo=None):\n\t\tsuper().__init__(learnRate, nodeinfo)\n\n\t\tself.momRate = None\n\t\tself.setAttr(""momRate"", momRate)\n\n\n\tdef setupState(self, var):\n\t\treturn {""mom"": gpuarray.zeros(var.data.shape, dtype=var.data.dtype)}\n\n\n\tdef updateVar(self, var, state, stream=None):\n\t\tnesterovMomSGDKer(var.data.dtype)(\n\t\t\tvar.data, var.grad, state[""mom""], self.learnRate * var.learnRate, self.momRate * var.momRate, stream=stream\n\t\t)\n\n\ndef unittest():\n\tfor dtype, atol in dtypesSupported():\n\t\tcalcTest(dtype, atol)\n\t\ttrainSimpleTest(NesterovSGD, dtype, learnRate=1e-1, momRate=0.9)\n\n\t\tif Config.backend == Config.Backend.cuda:\n\t\t\ttrainHardTest(NesterovSGD, dtype, learnRate=1e-1, momRate=0.9)\n\n\ndef calcTest(dtype, atol):\n\tlr, mr = 0.01, 0.9\n\tshape = (11, 13)\n\n\thostW, hostDw = np.random.randn(*shape).astype(dtype), np.random.randn(*shape).astype(dtype)\n\thostMom = np.random.randn(*shape).astype(dtype)\n\n\tw, dw, mom = gpuarray.to_gpu(hostW), gpuarray.to_gpu(hostDw), gpuarray.to_gpu(hostMom)\n\tnesterovMomSGDKer(w.dtype)(w, dw, mom, lr, mr)\n\n\thostW, hostDw, hostMom = hostW.astype(np.float32), hostDw.astype(np.float32), hostMom.astype(np.float32)\n\n\thostW += mr**2 * hostMom + (1 + mr) * lr * hostDw\n\thostMom = mr * hostMom + lr * hostDw\n\n\thostW, hostDw, hostMom = hostW.astype(dtype), hostDw.astype(dtype), hostMom.astype(dtype)\n\n\tassert np.allclose(hostMom, mom.get(), atol=atol)\n\tassert np.allclose(hostW, w.get(), atol=atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Optimizers/Optimizer.py,8,"b'from collections import OrderedDict\n\nimport numpy as np\nimport h5py\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import SharedArray, streamManager\n\nfrom PuzzleLib.Variable import Variable\n\n\nclass Optimizer:\n\tdef __init__(self, nodeinfo=None):\n\t\tself.t = 0\n\t\tself.learnRate = 0.0\n\n\t\tself.attrs = {""t"", ""learnRate""}\n\n\t\tself.module = None\n\t\tself.states = {}\n\t\tself.hooks = []\n\n\t\tself.shParams, self.shGrads = {}, {}\n\n\t\tself.globalState = False\n\t\tself.globalVar = OrderedDict()\n\n\t\tself.customVars = []\n\t\tself.nodeinfo = nodeinfo\n\n\n\tdef setAttr(self, name, attr):\n\t\tsetattr(self, name, attr)\n\t\tself.attrs.add(name)\n\n\n\tdef getAttrDict(self):\n\t\treturn {attrName: getattr(self, attrName) for attrName in self.attrs}\n\n\n\tdef addHook(self, hook):\n\t\tif self.globalState and Config.showWarnings:\n\t\t\tprint(""[%s] Warning: adding hook to optimizer in global state mode"" % Config.libname)\n\n\t\tself.hooks.append(hook)\n\n\n\tdef setupOn(self, mod, useGlobalState=False):\n\t\tif self.nodeinfo is not None:\n\t\t\tassert useGlobalState\n\n\t\tself.module = mod\n\t\tvartable = self.module.getVarTable()\n\n\t\tif useGlobalState:\n\t\t\tself.globalState = True\n\t\t\tself.setupGlobalState(vartable)\n\n\t\telse:\n\t\t\tself.setupLocalStates(vartable)\n\n\t\tif self.nodeinfo is not None:\n\t\t\tassert len(self.customVars) == 0\n\n\n\tdef setupGlobalState(self, vartable):\n\t\tvariables = [(names, var) for var, names in vartable.items()]\n\t\tvariables = sorted(variables, key=lambda elem: elem[0][0])\n\n\t\tfor names, var in variables:\n\t\t\tif var.hasUpdater:\n\t\t\t\tassert self.nodeinfo is None\n\n\t\t\t\tself.customVars.append(names[0])\n\t\t\t\tcontinue\n\n\t\t\tshape, dtype = var.data.shape, var.data.dtype.type\n\n\t\t\tshParams = self.shParams.get(dtype, SharedArray(dtype))\n\t\t\tshGrads = self.shGrads.get(dtype, SharedArray(dtype))\n\n\t\t\tshParams.register(var.data.shape, var.data.dtype.type, names[0])\n\t\t\tshGrads.register(var.grad.shape, var.grad.dtype.type, names[0])\n\n\t\t\tself.shParams[dtype] = shParams\n\t\t\tself.shGrads[dtype] = shGrads\n\n\t\tfor shParams, shGrads in zip(self.shParams.values(), self.shGrads.values()):\n\t\t\tshParams.build()\n\t\t\tshGrads.build()\n\n\t\t\tself.globalVar[shParams.dtype] = Variable(shParams.ary, grad=shGrads.ary)\n\n\t\tfor names, var in variables:\n\t\t\tif var.hasUpdater:\n\t\t\t\tcontinue\n\n\t\t\tdtype = var.data.dtype.type\n\t\t\tdata, grad = self.shParams[dtype][names[0]], self.shGrads[dtype][names[0]]\n\n\t\t\tdata.set(var.data)\n\t\t\tgrad.set(var.grad)\n\n\t\t\tfor name in names:\n\t\t\t\tself.module.setVar(name, Variable(data, grad=grad))\n\n\t\tfor dtype, globalVar in self.globalVar.items():\n\t\t\tif self.nodeinfo is not None:\n\t\t\t\tself.nodeinfo.broadcastBuffer(""data"", globalVar.data.gpudata)\n\n\t\t\tself.states[dtype] = self.setupState(globalVar)\n\n\n\tdef setupLocalStates(self, vartable):\n\t\tfor var, names in vartable.items():\n\t\t\tif var.hasUpdater:\n\t\t\t\tself.customVars.append(names[0])\n\t\t\t\tcontinue\n\n\t\t\tself.states[names[0]] = self.setupState(var)\n\n\n\tdef zeroGradParams(self):\n\t\tself.zeroGradGlobalParams() if self.globalState else self.zeroGradLocalParams()\n\n\n\tdef zeroGradGlobalParams(self):\n\t\tfor globalVar in self.globalVar.values():\n\t\t\tglobalVar.grad.fill(0)\n\n\n\tdef zeroGradLocalParams(self):\n\t\tfor i, (name, state) in enumerate(self.states.items()):\n\t\t\tvar = self.module.getVar(name)\n\n\t\t\tif var.hasUpdater:\n\t\t\t\tcontinue\n\n\t\t\tvar.grad.fill(0)\n\n\n\tdef setupState(self, var):\n\t\treturn {}\n\n\n\tdef update(self, useStreams=False, sync=True):\n\t\tself.t += 1\n\n\t\tif self.globalState:\n\t\t\tself.updateGlobalState()\n\t\telse:\n\t\t\tself.updateLocalStates(useStreams, sync)\n\n\t\tfor name in self.customVars:\n\t\t\tvar = self.module.getVar(name)\n\t\t\tvar.update(self.learnRate)\n\n\n\tdef updateGlobalState(self):\n\t\tfor dtype, globalVar in self.globalVar.items():\n\t\t\tstate = self.states[dtype]\n\n\t\t\tfor hook in self.hooks:\n\t\t\t\thook(globalVar, state)\n\n\t\t\tif self.nodeinfo is not None:\n\t\t\t\tself.nodeinfo.sumTensor(""grad"", globalVar.grad)\n\n\t\t\tif globalVar.learnRate > 0.0:\n\t\t\t\tself.updateVar(globalVar, state)\n\n\n\tdef updateLocalStates(self, useStreams, sync):\n\t\tstreams = streamManager.borrow(len(self.states)) if useStreams else None\n\n\t\tfor i, (name, state) in enumerate(self.states.items()):\n\t\t\tvar = self.module.getVar(name)\n\n\t\t\tassert var.grad is not None\n\t\t\tassert var.data.shape == var.grad.shape\n\n\t\t\tstream = streams[i] if useStreams else None\n\n\t\t\tfor hook in self.hooks:\n\t\t\t\thook(var, state, stream)\n\n\t\t\tif var.learnRate > 0.0:\n\t\t\t\tself.updateVar(var, state, stream)\n\n\t\tif useStreams:\n\t\t\tif sync:\n\t\t\t\tfor stream in streams:\n\t\t\t\t\tstream.synchronize()\n\n\t\t\tstreamManager.give(streams)\n\n\n\tdef updateVar(self, var, state, stream=None):\n\t\traise NotImplementedError()\n\n\n\tdef save(self, hdf, name=None):\n\t\thdf = self.ensureHdf(hdf, ""w"")\n\n\t\tif name is None:\n\t\t\tname = str()\n\n\t\tif len(self.attrs) > 0:\n\t\t\tattrGrp = hdf.create_group(name + "".attrs"")\n\n\t\t\tfor attrName, attr in self.getAttrDict().items():\n\t\t\t\tattrGrp.create_dataset(attrName, data=attr)\n\n\t\tif len(self.states) > 0:\n\t\t\tstateGrp = hdf.create_group(name + "".states"")\n\n\t\t\tfor stateName, state in self.states.items():\n\t\t\t\tfor entityName, entity in state.items():\n\t\t\t\t\tstateGrp.create_dataset(""%s.%s"" % (stateName, entityName), data=entity.get())\n\n\n\tdef load(self, hdf, name=None):\n\t\thdf = self.ensureHdf(hdf, ""r"")\n\n\t\tif name is None:\n\t\t\tname = str()\n\n\t\tattrGrpName = name + "".attrs""\n\n\t\tif attrGrpName in hdf:\n\t\t\tattrGrp = hdf[attrGrpName]\n\t\t\tfor attrName, attr in attrGrp.items():\n\t\t\t\tT = type(getattr(self, attrName))\n\t\t\t\tself.setAttr(attrName, T(np.array(attr)))\n\n\t\tif len(self.states) > 0:\n\t\t\tstateGrp = hdf[name + "".states""]\n\n\t\t\tfor stateName, state in self.states.items():\n\t\t\t\tfor entityName, entity in state.items():\n\t\t\t\t\tentity.set(np.array(stateGrp[""%s.%s"" % (stateName, entityName)]))\n\n\n\t@staticmethod\n\tdef ensureHdf(file, mode):\n\t\treturn h5py.File(file, mode) if isinstance(file, str) else file\n\n\ndef trainSimpleTest(optCls, dtype, *args, **kwargs):\n\tfrom PuzzleLib.Containers.Sequential import Sequential\n\n\tfrom PuzzleLib.Modules.Linear import Linear\n\tfrom PuzzleLib.Modules.Activation import Activation, relu\n\tfrom PuzzleLib.Modules.Cast import Cast\n\n\tfrom PuzzleLib.Cost.MSE import MSE\n\n\tseq = Sequential()\n\n\tseq.append(Linear(128, 64, useBias=False))\n\tseq.append(Activation(relu))\n\tseq.append(Linear(64, 32, useBias=False))\n\tseq.append(Activation(relu))\n\tseq.append(Linear(32, 16))\n\n\tseq.calcMode(dtype)\n\tseq.append(Cast(intype=dtype, outtype=np.float32))\n\n\toptimizer = optCls(*args, **kwargs)\n\toptimizer.setupOn(seq, useGlobalState=True)\n\n\tmse = MSE()\n\n\tdata = gpuarray.to_gpu(np.random.randn(16, 128).astype(dtype))\n\ttarget = gpuarray.to_gpu(np.random.randn(16, 16).astype(np.float32))\n\n\tfor i in range(200):\n\t\terror, grad = mse(seq(data), target)\n\n\t\toptimizer.zeroGradParams()\n\t\tseq.backward(grad)\n\t\toptimizer.update()\n\n\t\tif (i + 1) % 5 == 0:\n\t\t\tprint(""Iteration #%d error: %s"" % (i + 1, error))\n\n\ndef trainHardTest(optCls, dtype, *args, **kwargs):\n\tfrom PuzzleLib.Containers.Sequential import Sequential\n\n\tfrom PuzzleLib.Modules.Conv2D import Conv2D\n\tfrom PuzzleLib.Modules.BatchNorm2D import BatchNorm2D\n\tfrom PuzzleLib.Modules.Activation import Activation, relu\n\tfrom PuzzleLib.Modules.Cast import Cast\n\n\tfrom PuzzleLib.Cost.MSE import MSE\n\n\tseq = Sequential()\n\n\tseq.append(Conv2D(4, 8, 5, pad=1))\n\tseq.append(BatchNorm2D(8))\n\tseq.append(Activation(relu))\n\n\tseq.append(Conv2D(8, 16, 5, pad=1))\n\n\tseq.calcMode(dtype)\n\tseq.append(Cast(intype=dtype, outtype=np.float32))\n\n\toptimizer = optCls(*args, **kwargs)\n\toptimizer.setupOn(seq, useGlobalState=True)\n\n\tmse = MSE()\n\n\tdata = gpuarray.to_gpu(np.random.randn(4, 4, 5, 5).astype(dtype))\n\ttarget = gpuarray.to_gpu(np.random.randn(4, 16, 1, 1).astype(np.float32))\n\n\tfor i in range(200):\n\t\terror, grad = mse(seq(data), target)\n\n\t\toptimizer.zeroGradParams()\n\t\tseq.backward(grad)\n\t\toptimizer.update()\n\n\t\tif (i + 1) % 5 == 0:\n\t\t\tprint(""Iteration #%d error: %s"" % (i + 1, error))\n'"
Optimizers/RMSProp.py,6,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import dtypesSupported\nfrom PuzzleLib.Backend.Kernels.ElementWise import rmspropKer\n\nfrom PuzzleLib.Optimizers.Optimizer import Optimizer, trainSimpleTest, trainHardTest\n\n\nclass RMSProp(Optimizer):\n\tdef __init__(self, learnRate=1e-3, factor=0.9, epsilon=1e-5, nodeinfo=None):\n\t\tsuper().__init__(nodeinfo)\n\n\t\tself.factor = None\n\t\tself.epsilon = None\n\n\t\tself.setAttr(""learnRate"", learnRate)\n\t\tself.setAttr(""factor"", factor)\n\t\tself.setAttr(""epsilon"", epsilon)\n\n\n\tdef setupState(self, var):\n\t\treturn {""ms"": gpuarray.zeros(var.data.shape, dtype=var.data.dtype)}\n\n\n\tdef updateVar(self, var, state, stream=None):\n\t\trmspropKer(var.data.dtype)(\n\t\t\tvar.data, var.grad, state[""ms""], self.learnRate * var.learnRate, self.factor, self.epsilon, stream=stream\n\t\t)\n\n\ndef unittest():\n\tfor dtype, atol in dtypesSupported():\n\t\tcalcTest(dtype, atol)\n\t\ttrainSimpleTest(RMSProp, dtype, learnRate=1e-2)\n\n\t\tif Config.backend == Config.Backend.cuda:\n\t\t\ttrainHardTest(RMSProp, dtype, learnRate=1e-2)\n\n\ndef calcTest(dtype, atol):\n\tlr, factor, epsilon = 0.01, 0.9, 1e-5\n\tshape = (11, 13)\n\n\thostW, hostDw = np.random.randn(*shape).astype(dtype), np.random.randn(*shape).astype(dtype)\n\thostMs = (1.0 + np.random.randn(*shape)**2).astype(dtype)\n\n\tw, dw, ms = gpuarray.to_gpu(hostW), gpuarray.to_gpu(hostDw), gpuarray.to_gpu(hostMs)\n\trmspropKer(w.dtype)(w, dw, ms, lr, factor, epsilon)\n\n\thostW, hostDw, hostMs = hostW.astype(np.float32), hostDw.astype(np.float32), hostMs.astype(np.float32)\n\n\thostMs = factor * hostMs + (1 - factor) * hostDw**2\n\thostW += lr * hostDw / (np.sqrt(hostMs) + epsilon)\n\n\thostW, hostDw, hostMs = hostW.astype(dtype), hostDw.astype(dtype), hostMs.astype(dtype)\n\n\tassert np.allclose(hostMs, ms.get(), atol=atol)\n\tassert np.allclose(hostW, w.get(), atol=atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Optimizers/RMSPropGraves.py,10,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import dtypesSupported\nfrom PuzzleLib.Backend.Kernels.ElementWise import rmspropGravesKer\n\nfrom PuzzleLib.Optimizers.Optimizer import Optimizer, trainSimpleTest, trainHardTest\n\n\nclass RMSPropGraves(Optimizer):\n\tdef __init__(self, learnRate=1e-4, alpha=0.95, momRate=0.9, epsilon=1e-4, nodeinfo=None):\n\t\tsuper().__init__(nodeinfo)\n\n\t\tself.alpha = None\n\t\tself.momRate = None\n\t\tself.epsilon = None\n\n\t\tself.setAttr(""learnRate"", learnRate)\n\t\tself.setAttr(""alpha"", alpha)\n\t\tself.setAttr(""momRate"", momRate)\n\t\tself.setAttr(""epsilon"", epsilon)\n\n\n\tdef setupState(self, var):\n\t\treturn {\n\t\t\t""mg"": gpuarray.zeros(var.data.shape, dtype=var.data.dtype),\n\t\t\t""ms"": gpuarray.zeros(var.data.shape, dtype=var.data.dtype),\n\t\t\t""delta"": gpuarray.zeros(var.data.shape, dtype=var.data.dtype)\n\t\t}\n\n\n\tdef updateVar(self, var, state, stream=None):\n\t\trmspropGravesKer(var.data.dtype)(\n\t\t\tvar.data, var.grad, state[""mg""], state[""ms""], state[""delta""], self.learnRate * var.learnRate, self.alpha,\n\t\t\tself.momRate * var.momRate, self.epsilon, stream=stream\n\t\t)\n\n\ndef unittest():\n\tfor dtype, atol in dtypesSupported():\n\t\tcalcTest(dtype, atol)\n\t\ttrainSimpleTest(RMSPropGraves, dtype, learnRate=1e-2)\n\n\t\tif Config.backend == Config.Backend.cuda:\n\t\t\ttrainHardTest(RMSPropGraves, dtype, learnRate=1e-2)\n\n\ndef calcTest(dtype, atol):\n\tlr, alpha, mr, epsilon = 0.01, 0.95, 0.9, 10.0\n\tshape = (11, 13)\n\n\thostW, hostDw = np.random.randn(*shape).astype(dtype), np.random.randn(*shape).astype(dtype)\n\thostMs, hostMg = (5.0 + np.random.randn(*shape)**2).astype(dtype), np.random.randn(*shape).astype(dtype)\n\thostDelta = np.random.randn(*shape).astype(dtype)\n\n\tw, dw = gpuarray.to_gpu(hostW), gpuarray.to_gpu(hostDw)\n\tms, mg, delta = gpuarray.to_gpu(hostMs), gpuarray.to_gpu(hostMg), gpuarray.to_gpu(hostDelta)\n\n\trmspropGravesKer(w.dtype)(w, dw, mg, ms, delta, lr, alpha, mr, epsilon)\n\n\thostW, hostDw = hostW.astype(np.float32), hostDw.astype(np.float32)\n\thostMs, hostMg, hostDelta = hostMs.astype(np.float32), hostMg.astype(np.float32), hostDelta.astype(np.float32)\n\n\thostMg = alpha * hostMg + (1 - alpha) * hostDw\n\thostMs = alpha * hostMs + (1 - alpha) * hostDw**2\n\thostDelta = mr * hostDelta + lr * hostDw / np.sqrt(hostMs - hostMg**2 + epsilon)\n\thostW += hostDelta\n\n\thostW, hostDw = hostW.astype(dtype), hostDw.astype(dtype)\n\thostMs, hostMg, hostDelta = hostMs.astype(dtype), hostMg.astype(dtype), hostDelta.astype(dtype)\n\n\tassert np.allclose(hostMg, mg.get(), atol=atol)\n\tassert np.allclose(hostMs, ms.get(), atol=atol)\n\tassert np.allclose(hostDelta, delta.get(), atol=atol)\n\tassert np.allclose(hostW, w.get(), atol=atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Optimizers/SGD.py,3,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import dtypesSupported\nfrom PuzzleLib.Backend.Kernels.ElementWise import toVectorAddVectorKer\n\nfrom PuzzleLib.Optimizers.Optimizer import Optimizer, trainSimpleTest, trainHardTest\n\n\nclass SGD(Optimizer):\n\tdef __init__(self, learnRate=1e-3, nodeinfo=None):\n\t\tsuper().__init__(nodeinfo)\n\t\tself.setAttr(""learnRate"", learnRate)\n\n\n\tdef updateVar(self, var, state, stream=None):\n\t\ttoVectorAddVectorKer(var.data.dtype)(var.data, var.grad, self.learnRate * var.learnRate, stream=stream)\n\n\ndef unittest():\n\tfor dtype, atol in dtypesSupported():\n\t\tcalcTest(dtype, atol)\n\t\ttrainSimpleTest(SGD, dtype, learnRate=1e-1)\n\n\t\tif Config.backend == Config.Backend.cuda:\n\t\t\ttrainHardTest(SGD, dtype, learnRate=1e-1)\n\n\ndef calcTest(dtype, atol):\n\tlr = 0.01\n\tshape = (11, 13)\n\n\thostW, hostDw = np.random.randn(*shape).astype(dtype), np.random.randn(*shape).astype(dtype)\n\n\tw, dw = gpuarray.to_gpu(hostW), gpuarray.to_gpu(hostDw)\n\ttoVectorAddVectorKer(w.dtype)(w, dw, lr)\n\n\thostW, hostDw = hostW.astype(np.float32), hostDw.astype(np.float32)\n\n\thostW += lr * hostDw\n\thostW, hostDw = hostW.astype(dtype), hostDw.astype(dtype)\n\n\tassert np.allclose(hostW, w.get(), atol=atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Optimizers/SMORMS3.py,12,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import dtypesSupported\nfrom PuzzleLib.Backend.Kernels.ElementWise import smorms3Ker\n\nfrom PuzzleLib.Optimizers.Optimizer import Optimizer, trainSimpleTest, trainHardTest\n\n\nclass SMORMS3(Optimizer):\n\tdef __init__(self, learnRate=1e-3, epsilon=1e-16, nodeinfo=None):\n\t\tsuper().__init__(nodeinfo)\n\n\t\tself.epsilon = None\n\n\t\tself.setAttr(""learnRate"", learnRate)\n\t\tself.setAttr(""epsilon"", epsilon)\n\n\n\tdef setupState(self, var):\n\t\treturn {\n\t\t\t""mem"": gpuarray.to_gpu(np.ones(var.data.shape, dtype=np.float32)),\n\t\t\t""mg"": gpuarray.zeros(var.data.shape, dtype=np.float32),\n\t\t\t""ms"": gpuarray.zeros(var.data.shape, dtype=np.float32)\n\t\t}\n\n\n\tdef updateVar(self, var, state, stream=None):\n\t\tsmorms3Ker(var.data.dtype)(\n\t\t\tvar.data, var.grad, state[""mem""], state[""mg""], state[""ms""], self.learnRate * var.learnRate, self.epsilon,\n\t\t\tstream=stream\n\t\t)\n\n\ndef unittest():\n\tfor dtype, atol in dtypesSupported():\n\t\tcalcTest(dtype, atol)\n\t\ttrainSimpleTest(SMORMS3, dtype, learnRate=1e-2)\n\n\t\tif Config.backend == Config.Backend.cuda:\n\t\t\ttrainHardTest(SMORMS3, dtype, learnRate=1e-2)\n\n\ndef calcTest(dtype, atol):\n\tlr, epsilon = 1e-3, 1e-16\n\tshape = (11, 13)\n\n\thostW, hostDw = np.random.randn(*shape).astype(dtype), np.random.randn(*shape).astype(dtype)\n\thostMem = (1.0 + np.random.randn(*shape)**2).astype(np.float32)\n\thostMg, hostMs = np.random.randn(*shape).astype(np.float32), np.random.randn(*shape).astype(np.float32)**2\n\n\tw, dw = gpuarray.to_gpu(hostW), gpuarray.to_gpu(hostDw)\n\tmem, mg, ms = gpuarray.to_gpu(hostMem), gpuarray.to_gpu(hostMg), gpuarray.to_gpu(hostMs)\n\n\tsmorms3Ker(w.dtype)(w, dw, mem, mg, ms, lr, epsilon)\n\n\thostW, hostDw = hostW.astype(np.float32), hostDw.astype(np.float32)\n\n\tr = 1.0 / (1.0 + hostMem)\n\thostMg = (1.0 - r) * hostMg + r * hostDw\n\thostMs = (1.0 - r) * hostMs + r * hostDw**2\n\tx = hostMg**2 / (hostMs + epsilon)\n\n\thostMem = 1.0 + hostMem * (1.0 - x)\n\thostW += hostDw * np.minimum(lr, x) / (np.sqrt(hostMs) + epsilon)\n\n\thostW, hostDw = hostW.astype(dtype), hostDw.astype(dtype)\n\n\tassert np.allclose(hostMem, mem.get(), atol=atol)\n\tassert np.allclose(hostMg, mg.get(), atol=atol)\n\tassert np.allclose(hostMs, ms.get(), atol=atol)\n\tassert np.allclose(hostW, w.get(), atol=atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Optimizers/__init__.py,0,b'from PuzzleLib.Optimizers.AdaDelta import AdaDelta\nfrom PuzzleLib.Optimizers.AdaGrad import AdaGrad\nfrom PuzzleLib.Optimizers.Adam import Adam\nfrom PuzzleLib.Optimizers.MomentumSGD import MomentumSGD\nfrom PuzzleLib.Optimizers.NesterovSGD import NesterovSGD\nfrom PuzzleLib.Optimizers.RMSProp import RMSProp\nfrom PuzzleLib.Optimizers.RMSPropGraves import RMSPropGraves\nfrom PuzzleLib.Optimizers.SGD import SGD\nfrom PuzzleLib.Optimizers.SMORMS3 import SMORMS3\n'
Passes/ConvertToGraph.py,7,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Containers.Sequential import Sequential\nfrom PuzzleLib.Containers.Parallel import Parallel\nfrom PuzzleLib.Containers.Graph import Graph\nfrom PuzzleLib.Containers.Node import Node\n\nfrom PuzzleLib.Modules.Identity import Identity\nfrom PuzzleLib.Modules.Replicate import Replicate\nfrom PuzzleLib.Modules.ToList import ToList\nfrom PuzzleLib.Modules.Glue import Glue\n\n\nclass ConverterError(Exception):\n\tpass\n\n\ndef toGraph(module, unsafe=False, nodesOnly=False, assumeUniqueNames=False):\n\tinputs, outputs = convertToGraph(module, None, None, assumeUniqueNames)\n\n\tgraph = Graph(inputs=inputs, outputs=outputs, unsafe=unsafe, nodesOnly=nodesOnly, name=module.name)\n\treturn graph\n\n\ndef convertToGraph(module, inputs, name, assumeUniqueNames):\n\tif isinstance(module, Sequential):\n\t\treturn convertSequential(module, inputs, name, assumeUniqueNames)\n\telif isinstance(module, Parallel):\n\t\treturn convertParallel(module, inputs, name, assumeUniqueNames)\n\telif isinstance(module, Graph):\n\t\treturn convertGraph(module, inputs, name, assumeUniqueNames)\n\telse:\n\t\treturn convertModule(module, inputs, name, assumeUniqueNames)\n\n\ndef convertSequential(seq, inputs, name, assumeUniqueNames):\n\toutputs = inputs\n\n\tfor mod in seq.graph:\n\t\tif assumeUniqueNames:\n\t\t\tmodname = None\n\t\telse:\n\t\t\tmodname = ""%s_%s"" % (name, mod.name) if name is not None else mod.name\n\n\t\tnewInputs, outputs = convertToGraph(mod, outputs, name=modname, assumeUniqueNames=assumeUniqueNames)\n\t\tinputs = inputs if inputs is not None else newInputs\n\n\treturn inputs, outputs\n\n\ndef convertParallel(parallel, inputs, name, assumeUniqueNames):\n\toverwriteInputs = False\n\toutputs = []\n\n\tif inputs is None:\n\t\toverwriteInputs = True\n\t\tinputs = []\n\n\tfor mod in parallel.graph:\n\t\tif assumeUniqueNames:\n\t\t\tmodname = None\n\t\telse:\n\t\t\tmodname = ""%s_%s"" % (name, mod.name) if name is not None else mod.name\n\n\t\tnewInputs, newOutputs = convertToGraph(mod, inputs, name=modname, assumeUniqueNames=assumeUniqueNames)\n\n\t\tif overwriteInputs:\n\t\t\tinputs.extend(newInputs)\n\n\t\toutputs.extend(newOutputs)\n\n\treturn inputs, outputs\n\n\ndef convertGraph(graph, inputs, name, assumeUniqueNames):\n\tnodes = {}\n\n\tfor node in graph.nodes.values():\n\t\tif assumeUniqueNames:\n\t\t\tmodname = None\n\t\telse:\n\t\t\tmodname = node.name if name is None else ""%s_%s"" % (name, node.name)\n\n\t\tname = node.name\n\n\t\tnewInputs, newOutputs = convertToGraph(node.module, None, name=modname, assumeUniqueNames=assumeUniqueNames)\n\t\tnodes[node.name] = newInputs, newOutputs, name\n\n\tfor nodeInputs, nodeOutputs, name in nodes.values():\n\t\tif not isinstance(nodeInputs, list):\n\t\t\tnodeInputs = [nodeInputs]\n\n\t\tfor inp in nodeInputs:\n\t\t\tinp.addBackwards([(nodes[n.name][1][0], slots) for n, slots in graph.nodes[name].bwds])\n\n\tnewInputs = [nodes[inp.name][0] for inp in graph.inputs]\n\tnewOutputs = [nodes[output.name][1] for output in graph.outputs]\n\n\tfor i, inp in enumerate(newInputs):\n\t\tinp.addBackwards(inputs[i])\n\n\treturn inputs, newOutputs\n\n\ndef convertModule(module, inputs, name, _):\n\tif isinstance(module, (Identity, Replicate, ToList)):\n\t\treturn inputs, inputs\n\n\tif isinstance(module, Glue):\n\t\traise ConverterError(""Cannot convert Glue module - result may be unpredictable"")\n\n\tnode = Node(module, parents=inputs, name=name)\n\tinputs = inputs if inputs is not None else node\n\n\treturn inputs, [node]\n\n\ndef netTest():\n\tfrom PuzzleLib.Models.Nets.ResNet import loadResNet\n\tnet = loadResNet(None, layers=""50"", initscheme=""xavier"")\n\n\tdata = gpuarray.to_gpu(np.random.randn(1, 3, 224, 224).astype(np.float32))\n\toutdata = net(data)\n\n\tgraph = toGraph(net)\n\tgraphdata = graph(data)\n\n\tassert np.allclose(outdata.get(), graphdata.get())\n\n\ndef graphTest():\n\tfrom PuzzleLib.Modules import Linear, Activation, relu, Add\n\n\tnet = Sequential()\n\n\tnet.append(Linear(10, 10))\n\tnet.append(Activation(relu))\n\n\tinp = Linear(10, 10).node()\n\tnode = Activation(relu).node(inp)\n\n\tsubseq = Sequential()\n\tsubseq.append(Linear(10, 10))\n\tsubseq.append(Activation(relu))\n\n\tnode2 = subseq.node(node)\n\tnode3 = Linear(10, 10).node(node)\n\n\tnode = Add(name=""add"").node(node2, node3)\n\n\tgraph = Graph(inputs=inp, outputs=node)\n\tnet.append(graph)\n\n\tnet.append(Linear(10, 10))\n\tnet.append(Activation(relu))\n\n\tdata = gpuarray.to_gpu(np.random.randn(1, 10).astype(np.float32))\n\toutdata = net(data)\n\n\tnet.reset()\n\n\tgraph = toGraph(net)\n\tgraphdata = graph(data)\n\n\tassert np.allclose(outdata.get(), graphdata.get())\n\n\ndef unittest():\n\tnetTest()\n\tgraphTest()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
TestLib/BiRnnIMDBTrain.py,0,"b'from PuzzleLib.Backend.Dnn.Rnn import deviceSupportsBatchHint\nfrom PuzzleLib.Datasets import IMDBLoader\n\nfrom PuzzleLib.Containers import *\nfrom PuzzleLib.Modules import *\nfrom PuzzleLib.Handlers import *\nfrom PuzzleLib.Optimizers import Adam\nfrom PuzzleLib.Cost import BCE\n\n\ndef main():\n\thintBatchsize, batchsize = (40, 40) if deviceSupportsBatchHint() else (None, 32)\n\tnumwords, maxlen = 20000, 100\n\n\timdb = IMDBLoader(numwords=numwords, maxlen=maxlen)\n\tdata, labels, _ = imdb.load(path=""../TestData/"")\n\tdata, labels = data[:], labels[:]\n\tprint(""Loaded IMDB"")\n\n\tseq = Sequential()\n\tseq.append(Embedder(numwords, maxlen, 128, initscheme=""uniform"", wscale=0.05, learnable=True))\n\n\tseq.append(SwapAxes(0, 1))\n\tseq.append(RNN(128, 64, mode=""lstm"", direction=""bi"", hintBatchSize=hintBatchsize))\n\n\tseq.append(Concat(axis=1))\n\tseq.append(Dropout(p=0.5))\n\n\tseq.append(Linear(128, 1))\n\n\toptimizer = Adam(alpha=1e-3)\n\toptimizer.setupOn(seq, useGlobalState=True)\n\n\tcost = BCE()\n\ttrainer = Trainer(seq, cost, optimizer, batchsize=batchsize)\n\tvalidator = Validator(seq, cost, batchsize=batchsize)\n\n\tprint(""Started training ..."")\n\n\tfor i in range(15):\n\t\ttrainer.trainFromHost(data[:25000], labels[:25000], macroBatchSize=25000,\n\t\t\t\t\t\t\t  onMacroBatchFinish=lambda train: print(""Train error: %s"" % train.cost.getMeanError()))\n\t\tprint(""Accuracy: %s"" % (1.0 - validator.validateFromHost(data[25000:], labels[25000:], macroBatchSize=25000)))\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
TestLib/CnnCifar10NIN.py,4,"b'from PuzzleLib.Datasets import Cifar10Loader\n\nfrom PuzzleLib.Containers import *\nfrom PuzzleLib.Modules import *\nfrom PuzzleLib.Handlers import *\nfrom PuzzleLib.Optimizers import MomentumSGD\nfrom PuzzleLib.Cost import CrossEntropy\n\nfrom PuzzleLib.Visual import *\nfrom PuzzleLib.Optimizers import Hooks\n\n\ndef main():\n\tcifar10 = Cifar10Loader()\n\tdata, labels = cifar10.load(path=""../TestData/"")\n\tdata, labels = data[:], labels[:]\n\tprint(""Loaded cifar10"")\n\n\tdata = np.array(data).reshape(data.shape[0], np.prod(data.shape[1:]))\n\tdata -= np.mean(data, axis=0, keepdims=True) + 10**(-8)\n\tdata /= np.std(data, axis=0, keepdims=True) + 10**(-5)\n\n\tdata = data.reshape(data.shape[0], 3, 32, 32)\n\n\tnp.random.seed(1234)\n\n\tseq = Sequential(name=""cifar"")\n\tseq.append(Conv2D(3, 192, 5, pad=2, initscheme=""gaussian"", wscale=0.05, name=""conv1""))\n\tseq.append(Activation(relu, name=""relu1""))\n\n\tseq.append(Conv2D(192, 160, 1, initscheme=""gaussian"", wscale=0.05, name=""cccp1""))\n\tseq.append(Activation(relu, name=""relu_cccp1""))\n\tseq.append(Conv2D(160, 96, 1, initscheme=""gaussian"", wscale=0.05, name=""cccp2""))\n\tseq.append(Activation(relu, name=""relu_cccp2""))\n\n\tseq.append(MaxPool2D(3, 2, pad=1, name=""pool1""))\n\tseq.append(Dropout(name=""drop3""))\n\n\tseq.append(Conv2D(96, 192, 5, pad=2, initscheme=""gaussian"", wscale=0.05, name=""conv2""))\n\tseq.append(Activation(relu, name=""relu2""))\n\n\tseq.append(Conv2D(192, 192, 1, initscheme=""gaussian"", wscale=0.05, name=""cccp3""))\n\tseq.append(Activation(relu, name=""relu_cccp3""))\n\tseq.append(Conv2D(192, 192, 1, initscheme=""gaussian"", wscale=0.05, name=""cccp4""))\n\tseq.append(Activation(relu, name=""relu_cccp4""))\n\n\tseq.append(AvgPool2D(3, 2, pad=1, name=""pool2""))\n\tseq.append(Dropout(name=""drop6""))\n\n\tseq.append(Conv2D(192, 192, 3, pad=1, initscheme=""gaussian"", wscale=0.05, name=""conv3""))\n\tseq.append(Activation(relu, name=""relu3""))\n\n\tseq.append(Conv2D(192, 192, 1, initscheme=""gaussian"", wscale=0.05, name=""cccp5""))\n\tseq.append(Activation(relu, name=""relu_cccp5""))\n\tseq.append(Conv2D(192, 10, 1, initscheme=""gaussian"", wscale=0.05, name=""cccp6""))\n\tseq.append(Activation(relu, name=""relu_cccp6""))\n\n\tseq.append(AvgPool2D(8, 1, name=""pool3""))\n\tseq.append(Flatten())\n\n\toptimizer = MomentumSGD(learnRate=0.1, momRate=0.9)\n\toptimizer.addHook(Hooks.WeightDecay(0.0001))\n\toptimizer.setupOn(seq, useGlobalState=True)\n\n\tcost = CrossEntropy(maxlabels=10)\n\ttrainer = Trainer(seq, cost, optimizer,\n\t\t\t\t\t  onBatchFinish=lambda train: print(""Processed batch %d out of %d"" %\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t(train.currBatch, train.totalBatches)))\n\n\tvalidator = Validator(seq, cost)\n\n\tnumofepochs = 100\n\tfor i in range(numofepochs):\n\t\ttrainer.trainFromHost(data[:50000], labels[:50000], macroBatchSize=25000,\n\t\t\t\t\t\t\t  onMacroBatchFinish=lambda train: print(""Train error: %s"" % train.cost.getMeanError()))\n\t\tvalerror = validator.validateFromHost(data[50000:], labels[50000:], macroBatchSize=10000)\n\t\tprint(""Finished epoch %d out of %d. Val error: %s"" % (i+1, numofepochs, valerror))\n\n\t\tif i+1 == 60 or i+1 == 80:\n\t\t\toptimizer.learnRate *= 0.1\n\t\t\tprint(""Lowered learn rate: %s"" % optimizer.learnRate)\n\n\t\tshowImageBasedFilters(seq[""conv1""].W.get(), ""../TestData/ninconv1.png"")\n\t\tshowFilters(seq[""conv2""].W.get(), ""../TestData/ninconv2.png"")\n\t\tshowFilters(seq[""conv3""].W.get(), ""../TestData/ninconv3.png"")\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
TestLib/CnnCifar10Simple.py,1,"b'from PuzzleLib.Datasets import Cifar10Loader\n\nfrom PuzzleLib.Containers import *\nfrom PuzzleLib.Modules import *\nfrom PuzzleLib.Handlers import *\nfrom PuzzleLib.Optimizers import MomentumSGD\nfrom PuzzleLib.Cost import CrossEntropy\n\nfrom PuzzleLib.Visual import *\n\n\ndef main():\n\tcifar10 = Cifar10Loader()\n\tdata, labels = cifar10.load(path=""../TestData/"")\n\tdata, labels = data[:], labels[:]\n\tprint(""Loaded cifar10"")\n\n\tnp.random.seed(1234)\n\n\tseq = Sequential()\n\tseq.append(Conv2D(3, 32, 5, pad=2, wscale=0.0001, initscheme=""gaussian""))\n\tseq.append(MaxPool2D(3, 2))\n\tseq.append(Activation(relu))\n\n\tseq.append(Conv2D(32, 32, 5, pad=2, wscale=0.01, initscheme=""gaussian""))\n\tseq.append(MaxPool2D(3, 2))\n\tseq.append(Activation(relu))\n\n\tseq.append(Conv2D(32, 64, 5, pad=2, wscale=0.01, initscheme=""gaussian""))\n\tseq.append(MaxPool2D(3, 2))\n\tseq.append(Activation(relu))\n\n\tseq.append(Flatten())\n\tseq.append(Linear(seq.dataShapeFrom((1, 3, 32, 32))[1], 64, wscale=0.1, initscheme=""gaussian""))\n\tseq.append(Activation(relu))\n\n\tseq.append(Linear(64, 10, wscale=0.1, initscheme=""gaussian""))\n\n\toptimizer = MomentumSGD()\n\toptimizer.setupOn(seq, useGlobalState=True)\n\toptimizer.learnRate = 0.01\n\toptimizer.momRate = 0.9\n\n\tcost = CrossEntropy(maxlabels=10)\n\n\ttrainer = Trainer(seq, cost, optimizer)\n\n\tvalidator = Validator(seq, cost)\n\tcurrerror = math.inf\n\n\tfor i in range(25):\n\t\ttrainer.trainFromHost(data[:50000], labels[:50000], macroBatchSize=50000,\n\t\t\t\t\t\t\t  onMacroBatchFinish=lambda train: print(""Train error: %s"" % train.cost.getMeanError()))\n\t\tvalerror = validator.validateFromHost(data[50000:], labels[50000:], macroBatchSize=10000)\n\t\tprint(""Accuracy: %s"" % (1.0 - valerror))\n\n\t\tif valerror >= currerror:\n\t\t\toptimizer.learnRate *= 0.5\n\t\t\tprint(""Lowered learn rate: %s"" % optimizer.learnRate)\n\n\t\tcurrerror = valerror\n\n\t\tshowImageBasedFilters(seq[0].W.get(), ""../TestData/conv1.png"")\n\t\tshowFilters(seq[3].W.get(), ""../TestData/conv2.png"")\n\t\tshowFilters(seq[6].W.get(), ""../TestData/conv3.png"")\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
TestLib/CnnIMDBTrain.py,0,"b'from PuzzleLib.Datasets import IMDBLoader\n\nfrom PuzzleLib.Containers import *\nfrom PuzzleLib.Modules import *\nfrom PuzzleLib.Handlers import *\nfrom PuzzleLib.Optimizers import Adam\nfrom PuzzleLib.Cost import BCE\n\n\ndef main():\n\tnumwords, maxlen, embsize = 5000, 250, 50\n\n\timdb = IMDBLoader(numwords=numwords, maxlen=maxlen)\n\tdata, labels, _ = imdb.load(path=""../TestData/"")\n\tdata, labels = data[:], labels[:]\n\tprint(""Loaded IMDB"")\n\n\tseq = Sequential()\n\n\tseq.append(Embedder(numwords, maxlen, embsize, initscheme=""uniform"", wscale=0.05, learnable=True))\n\n\tseq.append(Dropout(p=0.2))\n\tseq.append(SwapAxes(1, 2))\n\n\tseq.append(Conv1D(embsize, embsize, 3))\n\tseq.append(Activation(relu))\n\n\tseq.append(MaxPool1D(maxlen - 2, 1))\n\tseq.append(Flatten())\n\n\tseq.append(Linear(embsize, 250))\n\tseq.append(Dropout(p=0.2))\n\tseq.append(Activation(relu))\n\n\tseq.append(Linear(250, 1))\n\n\toptimizer = Adam(alpha=1e-3)\n\toptimizer.setupOn(seq, useGlobalState=True)\n\n\tcost = BCE()\n\ttrainer = Trainer(seq, cost, optimizer, batchsize=32)\n\tvalidator = Validator(seq, cost, batchsize=32)\n\n\tfor i in range(15):\n\t\ttrainer.trainFromHost(data[:25000], labels[:25000], macroBatchSize=25000,\n\t\t\t\t\t\t\t  onMacroBatchFinish=lambda train: print(""Train error: %s"" % train.cost.getMeanError()))\n\t\tprint(""Accuracy: %s"" % (1.0 - validator.validateFromHost(data[25000:], labels[25000:], macroBatchSize=25000)))\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
TestLib/CnnMnistLenet.py,1,"b'from PuzzleLib.Datasets import MnistLoader\n\nfrom PuzzleLib.Containers import *\nfrom PuzzleLib.Modules import *\nfrom PuzzleLib.Handlers import *\nfrom PuzzleLib.Optimizers import MomentumSGD\nfrom PuzzleLib.Cost import CrossEntropy\n\nfrom PuzzleLib.Visual import *\n\n\ndef main():\n\tmnist = MnistLoader()\n\tdata, labels = mnist.load(path=""../TestData/"")\n\tdata, labels = data[:], labels[:]\n\tprint(""Loaded mnist"")\n\n\tnp.random.seed(1234)\n\n\tseq = Sequential(name=""lenet-5-like"")\n\tseq.append(Conv2D(1, 16, 3))\n\tseq.append(MaxPool2D())\n\tseq.append(Activation(relu))\n\n\tseq.append(Conv2D(16, 32, 4))\n\tseq.append(MaxPool2D())\n\tseq.append(Activation(relu))\n\n\tseq.append(Flatten())\n\tseq.append(Linear(32 * 5 * 5, 1024))\n\tseq.append(Activation(relu))\n\n\tseq.append(Linear(1024, 10))\n\n\toptimizer = MomentumSGD()\n\toptimizer.setupOn(seq, useGlobalState=True)\n\toptimizer.learnRate = 0.1\n\toptimizer.momRate = 0.9\n\n\tcost = CrossEntropy(maxlabels=10)\n\ttrainer = Trainer(seq, cost, optimizer)\n\tvalidator = Validator(seq, cost)\n\n\tfor i in range(15):\n\t\ttrainer.trainFromHost(data[:60000], labels[:60000], macroBatchSize=60000,\n\t\t\t\t\t\t\t  onMacroBatchFinish=lambda train: print(""Train error: %s"" % train.cost.getMeanError()))\n\t\tprint(""Accuracy: %s"" % (1.0 - validator.validateFromHost(data[60000:], labels[60000:], macroBatchSize=10000)))\n\n\t\toptimizer.learnRate *= 0.9\n\n\t\tshowFilters(seq[0].W.get(), ""../TestData/conv1.png"")\n\t\tshowFilters(seq[3].W.get(), ""../TestData/conv2.png"")\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
TestLib/EncoderTrain.py,3,"b'from PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\n\nfrom PuzzleLib.Datasets import MnistLoader\n\nfrom PuzzleLib.Containers import *\nfrom PuzzleLib.Modules import *\nfrom PuzzleLib.Optimizers import MomentumSGD\nfrom PuzzleLib.Cost import MSE\nfrom PuzzleLib.Variable import Variable\n\nfrom PuzzleLib.Visual import *\n\n\ndef main():\n\tmnist = MnistLoader()\n\tdata, _ = mnist.load(path=""../TestData"")\n\tdata = data[:].reshape(data.shape[0], np.prod(data.shape[1:]))\n\tprint(""Loaded mnist"")\n\n\tnp.random.seed(1234)\n\n\tseq = Sequential()\n\tseq.append(Linear(784, 256))\n\tseq.append(Activation(relu, inplace=True))\n\tseq.append(Dropout())\n\tseq.append(Linear(256, 784, empty=True, transpose=True))\n\n\tseq[-1].setVar(""W"", seq[0].vars[""W""])\n\tseq[-1].setVar(""b"", Variable(gpuarray.zeros((784, ), dtype=np.float32, allocator=memPool)))\n\n\toptimizer = MomentumSGD()\n\toptimizer.setupOn(seq, useGlobalState=True)\n\toptimizer.learnRate = 10.0\n\toptimizer.momRate = 0.5\n\n\tdata = gpuarray.to_gpu(data)\n\tbatchsize = 100\n\n\tmse = MSE()\n\n\tfor epoch in range(40):\n\t\tfor i in range(data.shape[0] // batchsize):\n\t\t\tbatch = data[i * batchsize:(i + 1) * batchsize]\n\n\t\t\tseq(batch)\n\t\t\t_, grad = mse(seq.data, batch)\n\n\t\t\tseq.zeroGradParams()\n\t\t\tseq.backward(grad)\n\t\t\toptimizer.update()\n\n\t\toptimizer.learnRate *= 0.8\n\t\tprint(""Finished epoch %d"" % (epoch + 1))\n\n\t\tprint(""Error: %s"" % (mse.getMeanError()))\n\t\tmse.resetAccumulator()\n\n\t\tif (epoch + 1) % 5 == 0:\n\t\t\tfilters = seq[0].W.get().T\n\t\t\tshowImageBatchInFolder(filters.reshape(256, 1, 28, 28), ""../TestData/encoder/"", ""filter"")\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
TestLib/GradientCheck.py,4,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Modules import *\nfrom PuzzleLib.Containers import *\nfrom PuzzleLib.Cost import BCE\n\n\ndef buildNet():\n\tnet = Sequential(name=""test-net"")\n\tnet.append(Conv2D(1, 2, 3, wscale=1.0, initscheme=""gaussian""))\n\tnet.append(AvgPool2D(2, 2))\n\tnet.append(BatchNorm2D(2))\n\tnet.append(Activation(relu))\n\tnet.append(Conv2D(2, 1, 2, wscale=1.0, initscheme=""gaussian""))\n\tnet.append(Flatten())\n\n\treturn net\n\n\ndef gradientCheck(mod, data, target, cost, h=1e-3):\n\tvartable = mod.getVarTable()\n\n\tmod(data)\n\terror, grad = cost(mod.data, target)\n\tmod.backward(grad, updGrad=False)\n\n\tfor var in vartable.keys():\n\t\tw = var.data.get()\n\t\tdw = -var.grad.get()\n\n\t\tfor i in range(w.ravel().shape[0]):\n\t\t\twph = np.copy(w)\n\t\t\twmh = np.copy(w)\n\n\t\t\twph.ravel()[i] = w.ravel()[i] + h\n\t\t\tvar.data.set(wph)\n\t\t\typh, _ = cost(mod(data), target)\n\n\t\t\twmh.ravel()[i] = w.ravel()[i] - h\n\t\t\tvar.data.set(wmh)\n\t\t\tymh, _ = cost(mod(data), target)\n\n\t\t\thost = (yph - ymh) / (2.0 * h)\n\t\t\tdev = dw.ravel()[i]\n\t\t\tvar.data.set(w)\n\n\t\t\tprint(abs((host - dev) / (dev + h)))\n\n\ndef main():\n\tnet = buildNet()\n\tcost = BCE()\n\n\tdata = gpuarray.to_gpu(np.random.randn(1, 1, 6, 6).astype(np.float32))\n\ttarget = gpuarray.to_gpu(np.random.randint(0, 2, size=(1, )))\n\n\tgradientCheck(net, data, target, cost)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
TestLib/MultiGPUCifar10.py,1,"b'from PuzzleLib.Grid import runGrid\n\n\ndef buildNet():\n\tfrom PuzzleLib.Containers import Sequential\n\tfrom PuzzleLib.Modules import Conv2D, MaxPool2D, Activation, relu, Flatten, Linear\n\n\tseq = Sequential()\n\tseq.append(Conv2D(3, 32, 5, pad=2, wscale=0.0001, initscheme=""gaussian""))\n\tseq.append(MaxPool2D(3, 2))\n\tseq.append(Activation(relu))\n\n\tseq.append(Conv2D(32, 32, 5, pad=2, wscale=0.01, initscheme=""gaussian""))\n\tseq.append(MaxPool2D(3, 2))\n\tseq.append(Activation(relu))\n\n\tseq.append(Conv2D(32, 64, 5, pad=2, wscale=0.01, initscheme=""gaussian""))\n\tseq.append(MaxPool2D(3, 2))\n\tseq.append(Activation(relu))\n\n\tseq.append(Flatten())\n\tseq.append(Linear(seq.dataShapeFrom((1, 3, 32, 32))[1], 64, wscale=0.1, initscheme=""gaussian""))\n\tseq.append(Activation(relu))\n\n\tseq.append(Linear(64, 10, wscale=0.1, initscheme=""gaussian""))\n\treturn seq\n\n\ndef train(nodeinfo, verbose):\n\tfrom PuzzleLib.Datasets import Cifar10Loader\n\tcifar10 = Cifar10Loader(cachename=""cifar10-%s.hdf"" % nodeinfo.index)\n\tdata, labels = cifar10.load(path=""../TestData/"")\n\n\tdata, labels = data[:], labels[:]\n\tprint(""[%s]: Loaded cifar10"" % nodeinfo.index)\n\n\timport numpy as np\n\tnp.random.seed(1234)\n\n\tseq = buildNet()\n\n\tfrom PuzzleLib.Optimizers import MomentumSGD\n\toptimizer = MomentumSGD(learnRate=0.01, momRate=0.9, nodeinfo=nodeinfo)\n\toptimizer.setupOn(seq, useGlobalState=True)\n\n\tfrom PuzzleLib.Cost import CrossEntropy\n\tcost = CrossEntropy(maxlabels=10)\n\n\tfrom PuzzleLib.Handlers import Trainer, Validator\n\ttrainer = Trainer(seq, cost, optimizer, batchsize=128 // nodeinfo.gridsize)\n\tvalidator = Validator(seq, cost)\n\n\timport math\n\tcurrerror = math.inf\n\n\tvalsize = 10000\n\ttrainsize = data.shape[0] - valsize\n\n\ttrainpart = trainsize // nodeinfo.gridsize\n\tvalpart = valsize // nodeinfo.gridsize\n\n\tfor i in range(25):\n\t\tstart, end = nodeinfo.index * trainpart, (nodeinfo.index + 1) * trainpart\n\n\t\ttrainer.trainFromHost(data[start:end], labels[start:end], macroBatchSize=trainpart)\n\t\ttrerr = cost.getMeanError()\n\n\t\tif verbose:\n\t\t\tprint(""[%s]: Epoch %s local train error: %s"" % (nodeinfo.index, i + 1, trerr))\n\n\t\ttrerr = nodeinfo.meanValue(trerr)\n\n\t\tif nodeinfo.index == 0:\n\t\t\tprint(""Epoch %s global train error: %s"" % (i + 1, trerr))\n\n\t\tstart, end = trainsize + nodeinfo.index * valpart, trainsize + (nodeinfo.index + 1) * valpart\n\t\tvalerr = validator.validateFromHost(data[start:end], labels[start:end], macroBatchSize=valpart)\n\n\t\tif verbose:\n\t\t\tprint(""[%s]: Epoch %s local accuracy: %s"" % (nodeinfo.index, i + 1, 1.0 - valerr))\n\n\t\tvalerr = nodeinfo.meanValue(valerr)\n\n\t\tif nodeinfo.index == 0:\n\t\t\tprint(""Epoch %s global accuracy: %s"" % (i + 1, 1.0 - valerr))\n\n\t\tif valerr >= currerror:\n\t\t\toptimizer.learnRate *= 0.5\n\t\t\tprint(""[%s]: Lowered learn rate: %s"" % (nodeinfo.index, optimizer.learnRate))\n\n\t\tcurrerror = valerr\n\n\ndef main():\n\trunGrid(target=train, size=2, verbose=True)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
TestLib/MultiGPUMnist.py,1,"b'from PuzzleLib.Grid import runGrid\n\n\ndef buildNet():\n\tfrom PuzzleLib.Containers import Sequential\n\tfrom PuzzleLib.Modules import Conv2D, MaxPool2D, Activation, relu, Flatten, Linear\n\n\tseq = Sequential(name=""lenet-5-like"")\n\tseq.append(Conv2D(1, 16, 3))\n\tseq.append(MaxPool2D())\n\tseq.append(Activation(relu))\n\n\tseq.append(Conv2D(16, 32, 4))\n\tseq.append(MaxPool2D())\n\tseq.append(Activation(relu))\n\n\tseq.append(Flatten())\n\tseq.append(Linear(32 * 5 * 5, 1024))\n\tseq.append(Activation(relu))\n\n\tseq.append(Linear(1024, 10))\n\treturn seq\n\n\ndef train(nodeinfo, verbose):\n\tfrom PuzzleLib.Datasets import MnistLoader\n\tmnist = MnistLoader(cachename=""mnist-%s.hdf"" % nodeinfo.index)\n\tdata, labels = mnist.load(path=""../TestData/"")\n\n\tdata, labels = data[:], labels[:]\n\tprint(""[%s]: Loaded mnist"" % nodeinfo.index)\n\n\timport numpy as np\n\tnp.random.seed(1234)\n\n\tseq = buildNet()\n\n\tfrom PuzzleLib.Optimizers import MomentumSGD\n\toptimizer = MomentumSGD(learnRate=0.1, momRate=0.9, nodeinfo=nodeinfo)\n\toptimizer.setupOn(seq, useGlobalState=True)\n\n\tfrom PuzzleLib.Cost import CrossEntropy\n\tcost = CrossEntropy(maxlabels=10)\n\n\tfrom PuzzleLib.Handlers import Trainer, Validator\n\ttrainer = Trainer(seq, cost, optimizer, batchsize=128 // nodeinfo.gridsize)\n\tvalidator = Validator(seq, cost)\n\n\tvalsize = 10000\n\ttrainsize = data.shape[0] - valsize\n\n\ttrainpart = trainsize // nodeinfo.gridsize\n\tvalpart = valsize // nodeinfo.gridsize\n\n\tfor i in range(15):\n\t\tstart, end = nodeinfo.index * trainpart, (nodeinfo.index + 1) * trainpart\n\n\t\ttrainer.trainFromHost(data[start:end], labels[start:end], macroBatchSize=trainpart)\n\t\ttrerr = cost.getMeanError()\n\n\t\tif verbose:\n\t\t\tprint(""[%s]: Epoch %s local train error: %s"" % (nodeinfo.index, i + 1, trerr))\n\n\t\ttrerr = nodeinfo.meanValue(trerr)\n\n\t\tif nodeinfo.index == 0:\n\t\t\tprint(""Epoch %s global train error: %s"" % (i + 1, trerr))\n\n\t\tstart, end = trainsize + nodeinfo.index * valpart, trainsize + (nodeinfo.index + 1) * valpart\n\t\tvalerr = validator.validateFromHost(data[start:end], labels[start:end], macroBatchSize=valpart)\n\n\t\tif verbose:\n\t\t\tprint(""[%s]: Epoch %s local accuracy: %s"" % (nodeinfo.index, i + 1, 1.0 - valerr))\n\n\t\tvalerr = nodeinfo.meanValue(valerr)\n\n\t\tif nodeinfo.index == 0:\n\t\t\tprint(""Epoch %s global accuracy: %s"" % (i + 1, 1.0 - valerr))\n\n\t\toptimizer.learnRate *= 0.9\n\n\ndef main():\n\trunGrid(target=train, size=2, verbose=True)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
TestLib/NormFilters.py,0,"b'from PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib import Visual\nfrom PuzzleLib.Modules import SubtractMean, LCN\n\n\ndef main():\n\tsubtractMean = SubtractMean(size=7)\n\tlcn = LCN(N=7)\n\n\timg = gpuarray.to_gpu(Visual.loadImage(""../TestData/Bench.png""))\n\n\tsubtractMean(img)\n\tVisual.showImage(subtractMean.data.get(), ""../TestData/ResultSubtractNorm.png"")\n\n\tlcn(img)\n\tVisual.showImage(lcn.data.get(), ""../TestData/ResultLCN.png"")\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
TestLib/OptimizeNet.py,2,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Benchmarks import timeKernel\n\nfrom PuzzleLib.Models.Nets.VGG import loadVGG\n\nfrom PuzzleLib.Optimizers import SGD\nfrom PuzzleLib.Cost import CrossEntropy\nfrom PuzzleLib.Handlers import Trainer\n\n\ndef main():\n\tnet = loadVGG(None, ""16"")\n\n\tbatchsize = 16\n\tsize = (batchsize, 3, 224, 224)\n\n\tbatch = np.random.normal(size=size).astype(dtype=np.float32)\n\tbatch = gpuarray.to_gpu(batch)\n\n\tlabels = np.random.randint(low=0, high=1000, size=(batchsize, ), dtype=np.int32)\n\tlabels = gpuarray.to_gpu(labels)\n\n\toptimizer = SGD()\n\toptimizer.setupOn(net)\n\n\tcost = CrossEntropy(maxlabels=1000)\n\ttrainer = Trainer(net, cost, optimizer)\n\n\tprint(""Started benchmarking %s ..."" % net.name)\n\ttimeKernel(\n\t\ttrainer.train, args=(batch, labels), looplength=100, logname=""Before optimizing %s"" % net.name, normalize=True\n\t)\n\n\tnet.optimizeForShape(size)\n\ttimeKernel(\n\t\ttrainer.train, args=(batch, labels), looplength=100, logname=""After optimizing %s"" % net.name, normalize=True\n\t)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
TestLib/ResumeTrain.py,1,"b'import os\n\nimport numpy as np\n\nfrom PuzzleLib.Datasets import MnistLoader\n\nfrom PuzzleLib.Containers import *\nfrom PuzzleLib.Modules import *\nfrom PuzzleLib.Handlers import *\nfrom PuzzleLib.Optimizers import MomentumSGD\nfrom PuzzleLib.Cost import CrossEntropy\n\n\ndef buildNet():\n\tnet = Sequential()\n\tnet.append(Conv2D(1, 16, 3))\n\tnet.append(MaxPool2D())\n\tnet.append(Activation(relu))\n\n\tnet.append(Conv2D(16, 32, 4))\n\tnet.append(MaxPool2D())\n\tnet.append(Activation(relu))\n\n\tnet.append(Flatten())\n\tnet.append(Linear(32 * 5 * 5, 1024))\n\tnet.append(Activation(relu))\n\n\tnet.append(Linear(1024, 10))\n\n\treturn net\n\n\ndef train(net, optimizer, data, labels, epochs):\n\tcost = CrossEntropy(maxlabels=10)\n\ttrainer = Trainer(net, cost, optimizer)\n\tvalidator = Validator(net, cost)\n\n\tfor i in range(epochs):\n\t\ttrainer.trainFromHost(data[:60000], labels[:60000], macroBatchSize=60000,\n\t\t\t\t\t\t\t  onMacroBatchFinish=lambda tr: print(""Train error: %s"" % tr.cost.getMeanError()))\n\t\tprint(""Accuracy: %s"" % (1.0 - validator.validateFromHost(data[60000:], labels[60000:], macroBatchSize=10000)))\n\n\t\toptimizer.learnRate *= 0.9\n\t\tprint(""Reduced optimizer learn rate to %s"" % optimizer.learnRate)\n\n\ndef main():\n\tmnist = MnistLoader()\n\tdata, labels = mnist.load(path=""../TestData/"")\n\tdata, labels = data[:], labels[:]\n\tprint(""Loaded mnist"")\n\n\tnp.random.seed(1234)\n\n\tnet = buildNet()\n\n\toptimizer = MomentumSGD()\n\toptimizer.setupOn(net, useGlobalState=True)\n\toptimizer.learnRate = 0.1\n\toptimizer.momRate = 0.9\n\n\tepochs = 10\n\tprint(""Training for %s epochs ..."" % epochs)\n\ttrain(net, optimizer, data, labels, epochs)\n\n\tprint(""Saving net and optimizer ..."")\n\tnet.save(""../TestData/net.hdf"")\n\toptimizer.save(""../TestData/optimizer.hdf"")\n\n\tprint(""Reloading net and optimizer ..."")\n\tnet.load(""../TestData/net.hdf"")\n\toptimizer.load(""../TestData/optimizer.hdf"")\n\n\tprint(""Continuing training for %s epochs ..."" % epochs)\n\ttrain(net, optimizer, data, labels, epochs)\n\n\tos.remove(""../TestData/net.hdf"")\n\tos.remove(""../TestData/optimizer.hdf"")\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
TestLib/RnnIMDBTrain.py,0,"b'from PuzzleLib.Backend.Dnn.Rnn import deviceSupportsBatchHint\nfrom PuzzleLib.Datasets import IMDBLoader\n\nfrom PuzzleLib.Containers import *\nfrom PuzzleLib.Modules import *\nfrom PuzzleLib.Handlers import *\nfrom PuzzleLib.Optimizers import Adam\nfrom PuzzleLib.Cost import BCE\n\n\ndef main():\n\thintBatchsize, batchsize = (40, 40) if deviceSupportsBatchHint() else (None, 32)\n\tnumwords, maxlen = 20000, 80\n\n\timdb = IMDBLoader(numwords=numwords, maxlen=maxlen)\n\tdata, labels, _ = imdb.load(path=""../TestData/"")\n\tdata, labels = data[:], labels[:]\n\tprint(""Loaded IMDB"")\n\n\tseq = Sequential()\n\tseq.append(Embedder(numwords, maxlen, 128, initscheme=""uniform"", wscale=0.05, learnable=True))\n\n\tseq.append(SwapAxes(0, 1))\n\tseq.append(RNN(128, 128, mode=""lstm"", dropout=0.2, hintBatchSize=hintBatchsize))\n\n\tseq.append(Linear(128, 1))\n\n\toptimizer = Adam(alpha=1e-3)\n\toptimizer.setupOn(seq, useGlobalState=True)\n\n\tcost = BCE()\n\ttrainer = Trainer(seq, cost, optimizer, batchsize=batchsize)\n\tvalidator = Validator(seq, cost, batchsize=batchsize)\n\n\tprint(""Started training ..."")\n\n\tfor i in range(15):\n\t\ttrainer.trainFromHost(data[:25000], labels[:25000], macroBatchSize=25000,\n\t\t\t\t\t\t\t  onMacroBatchFinish=lambda train: print(""Train error: %s"" % train.cost.getMeanError()))\n\t\tprint(""Accuracy: %s"" % (1.0 - validator.validateFromHost(data[25000:], labels[25000:], macroBatchSize=25000)))\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Transformers/Generator.py,1,"b'import numpy as np\n\nfrom PuzzleLib.Transformers.Provider import Provider\nfrom PuzzleLib.Transformers.Transformer import Transformer\n\n\nclass Generator(Provider):\n\tdef getNextChunk(self, chunksize, **kwargs):\n\t\treturn None\n\n\nclass TestGenTransformer(Transformer):\n\tdef __call__(self, batch, threadidx):\n\t\treturn np.random.randn(10, 3, 4, 4).astype(np.float32)\n\n\ndef unittest():\n\twith Generator(numofthreads=4) as generator:\n\t\tgenerator.addTransformer(TestGenTransformer())\n\n\t\tgenerator.prepareData()\n\t\tassert generator.getData().shape == (40, 3, 4, 4)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Transformers/Merger.py,7,"b'import random\nimport numpy as np\n\nfrom PuzzleLib.Transformers.Provider import Provider\n\n\nclass Merger(Provider):\n\tdef __init__(self, datasets, labelIds=None, numofthreads=4):\n\t\tsuper().__init__(numofthreads)\n\n\t\tself.datalens = []\n\t\tself.datasets = datasets\n\t\tself.indices = [0] * len(self.datasets)\n\n\t\tself.labelIds = labelIds\n\n\t\tfor dataset in datasets:\n\t\t\tself.datalens.append(dataset.shape[0])\n\n\t\t\tif dataset.shape[1:] != datasets[0].shape[1:]:\n\t\t\t\traise ValueError(""Datasets must have same shapes"")\n\n\n\tdef getNextChunk(self, chunksize, **kwargs):\n\t\tratios, randomize, permutate = kwargs[""ratios""], kwargs[""randomize""], kwargs[""permutate""]\n\n\t\tif not randomize and chunksize >= sum(self.datalens):\n\t\t\tchunksize = sum(self.datalens)\n\n\t\tself.deriveChunkRatios(ratios, chunksize)\n\n\t\tif randomize:\n\t\t\treturn self.getRandomChunk(chunksize, ratios, permutate)\n\n\t\telse:\n\t\t\treviseRatios = False\n\t\t\tfor i, dataset in enumerate(self.datasets):\n\t\t\t\tif self.datalens[i] < ratios[i]:\n\t\t\t\t\tratios[i] = self.datalens[i]\n\t\t\t\t\treviseRatios = True\n\n\t\t\tif reviseRatios:\n\t\t\t\tchunksize = sum(ratios)\n\n\t\t\treturn self.getRationedChunk(chunksize, ratios, permutate)\n\n\n\tdef getRandomChunk(self, chunksize, ratios, permutate):\n\t\tchunk = np.empty((chunksize, ) + self.datasets[0].shape[1:], dtype=self.datasets[0].dtype)\n\n\t\tlabels = None\n\t\tif self.labelIds is not None:\n\t\t\tlabels = np.empty((chunksize, ), dtype=np.int32)\n\n\t\tif permutate:\n\t\t\torder = np.random.permutation(chunksize)\n\t\telse:\n\t\t\torder = np.arange(chunksize)\n\n\t\tidx = 0\n\t\tfor i, dataset in enumerate(self.datasets):\n\t\t\tfor _ in range(ratios[i]):\n\t\t\t\tchunk[order[idx]] = dataset[random.randint(0, self.datalens[i]-1)]\n\n\t\t\t\tif self.labelIds is not None:\n\t\t\t\t\tlabels[order[idx]] = self.labelIds[i]\n\n\t\t\t\tidx += 1\n\n\t\tif self.labelIds is not None:\n\t\t\treturn chunk, labels\n\t\telse:\n\t\t\treturn chunk\n\n\n\tdef getRationedChunk(self, chunksize, ratios, permutate):\n\t\tchunk = np.empty((chunksize, ) + self.datasets[0].shape[1:], dtype=self.datasets[0].dtype)\n\t\torder, labels = None, None\n\n\t\tif self.labelIds is not None:\n\t\t\tlabels = np.empty((chunksize, ), dtype=np.int32)\n\n\t\tif permutate:\n\t\t\torder = np.random.permutation(chunksize)\n\n\t\tidx = 0\n\t\tfor i, dataset in enumerate(self.datasets):\n\t\t\tbegin = self.indices[i]\n\t\t\tend = self.indices[i] + ratios[i]\n\n\t\t\tif end > self.datalens[i]:\n\t\t\t\tself.indices[i] = end - self.datalens[i]\n\n\t\t\t\tif permutate:\n\t\t\t\t\tfor d in range(ratios[i]):\n\t\t\t\t\t\tif begin + d < self.datalens[i]:\n\t\t\t\t\t\t\tchunk[order[idx + d]] = dataset[begin + d]\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tchunk[order[idx + d]] = dataset[begin + d - self.datalens[i]]\n\n\t\t\t\t\t\tif self.labelIds is not None:\n\t\t\t\t\t\t\tlabels[order[idx + d]] = self.labelIds[i]\n\n\t\t\t\telse:\n\t\t\t\t\tchunk[idx:idx + self.datalens[i] - begin] = dataset[begin:self.datalens[i]]\n\t\t\t\t\tchunk[idx + self.datalens[i] - begin:idx + ratios[i]] = dataset[:self.indices[i]]\n\n\t\t\t\t\tif self.labelIds is not None:\n\t\t\t\t\t\tlabels[idx:idx + self.datalens[i] - begin] = self.labelIds[i]\n\t\t\t\t\t\tlabels[idx + self.datalens[i] - begin:idx + ratios[i]] = self.labelIds[i]\n\t\t\telse:\n\t\t\t\tself.indices[i] = end\n\n\t\t\t\tif permutate:\n\t\t\t\t\tfor d in range(ratios[i]):\n\t\t\t\t\t\tchunk[order[idx + d]] = dataset[begin + d]\n\n\t\t\t\t\t\tif self.labelIds is not None:\n\t\t\t\t\t\t\tlabels[order[idx + d]] = self.labelIds[i]\n\t\t\t\telse:\n\t\t\t\t\tchunk[idx:idx + ratios[i]] = dataset[begin:end]\n\n\t\t\t\t\tif self.labelIds is not None:\n\t\t\t\t\t\tlabels[idx:idx + ratios[i]] = self.labelIds[i]\n\n\t\t\tidx += ratios[i]\n\n\t\tif self.labelIds is not None:\n\t\t\treturn chunk, labels\n\t\telse:\n\t\t\treturn chunk\n\n\n\t@staticmethod\n\tdef deriveChunkRatios(ratios, chunksize):\n\t\tnorm = sum(ratios)\n\t\tfor i in range(len(ratios) - 1):\n\t\t\tratios[i] = int(ratios[i] / norm * chunksize)\n\n\t\tratios[-1] = chunksize - sum(ratios[:-1])\n\n\n\tdef prepareData(self, ratios=None, chunksize=20000, randomize=False, permutate=True):\n\t\tif ratios is None:\n\t\t\tratios = [1] * len(self.datasets)\n\t\telse:\n\t\t\tassert (len(ratios) == len(self.datasets))\n\n\t\tsuper().prepareData(chunksize, ratios=ratios, randomize=randomize, permutate=permutate)\n\n\ndef unittest():\n\tfrom PuzzleLib.Datasets.ZipLoader import ZipLoader\n\n\tzipfile = ZipLoader()\n\tdata1 = zipfile.load(""../TestData/test.zip"")\n\tdata2 = zipfile.load(""../TestData/test.zip"")\n\n\twith Merger([data1, data2], [0, 1]) as merger:\n\t\tfor _ in range(10):\n\t\t\tmerger.prepareData(chunksize=10, ratios=[6, 4], permutate=False)\n\t\t\tmerger.getData()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Transformers/Provider.py,3,"b'from multiprocessing import Pool\n\nimport numpy as np\n\n\nclass Provider:\n\tdef __init__(self, numofthreads=4):\n\t\tself.transformers = []\n\n\t\tself.numofthreads = numofthreads\n\t\tself.pool = Pool(numofthreads)\n\t\tself.pool.starmap(lambda: np.random.seed(), ())\n\t\tself.poolresults = None\n\n\t\tself.data = None\n\n\n\tdef __enter__(self):\n\t\treturn self\n\n\n\tdef __exit__(self, exc_type, exc_value, traceback):\n\t\tself.closePool()\n\n\n\tdef closePool(self):\n\t\tself.pool.close()\n\t\tself.pool.join()\n\n\n\tdef addTransformer(self, transformer):\n\t\tself.transformers.append(transformer)\n\n\n\tdef getNextChunk(self, chunksize, **kwargs):\n\t\traise NotImplementedError()\n\n\n\tdef prepareData(self, chunksize=20000, **kwargs):\n\t\tresult = self.getNextChunk(chunksize, **kwargs)\n\n\t\tif len(self.transformers) == 0:\n\t\t\tself.data = result\n\t\t\treturn\n\n\t\tif result is not None:\n\t\t\tif isinstance(result, tuple) or isinstance(result, list):\n\t\t\t\tbatchsize = result[0].shape[0] // self.numofthreads\n\t\t\telse:\n\t\t\t\tbatchsize = result.shape[0] // self.numofthreads\n\n\t\t\tbatches = []\n\t\t\tfor i in range(self.numofthreads - 1):\n\t\t\t\tif isinstance(result, tuple) or isinstance(result, list):\n\t\t\t\t\tbatches.append([res[i * batchsize:(i + 1) * batchsize] for res in result])\n\t\t\t\telse:\n\t\t\t\t\tbatches.append(result[i * batchsize:(i + 1) * batchsize])\n\n\t\t\tif isinstance(result, tuple) or isinstance(result, list):\n\t\t\t\tbatches.append([res[(self.numofthreads - 1) * batchsize:] for res in result])\n\t\t\telse:\n\t\t\t\tbatches.append(result[(self.numofthreads - 1) * batchsize:])\n\n\t\t\targs = []\n\t\t\tfor i, batch in enumerate(batches):\n\t\t\t\targ = (self.transformers, batch, i)\n\t\t\t\targs.append(arg)\n\t\telse:\n\t\t\targs = []\n\t\t\tfor i in range(self.numofthreads):\n\t\t\t\targs.append((self.transformers, None, i))\n\n\t\tself.poolresults = self.pool.starmap_async(self.worker, args)\n\n\n\tdef getData(self):\n\t\tif self.poolresults is not None:\n\t\t\tself.poolresults.wait()\n\n\t\t\tresults = [None] * self.numofthreads\n\t\t\tfor data in self.poolresults.get():\n\t\t\t\tresult, threadidx = data\n\t\t\t\tresults[threadidx] = result\n\n\t\t\tself.poolresults = None\n\n\t\t\tlength = 0\n\t\t\tif isinstance(results[0], tuple) or isinstance(results[0], list):\n\t\t\t\tdatshape = [res.shape[1:] for res in results[0]]\n\n\t\t\t\tfor res in results:\n\t\t\t\t\tlength += res[0].shape[0]\n\n\t\t\t\tself.data = tuple(np.empty((length, )+shape, dtype=results[0][i].dtype)\n\t\t\t\t\t\t\t\t  for i, shape in enumerate(datshape))\n\n\t\t\t\tidx = 0\n\t\t\t\tfor res in results:\n\t\t\t\t\tfor i, dat in enumerate(res):\n\t\t\t\t\t\tself.data[i][idx:idx + dat.shape[0]] = dat\n\n\t\t\t\t\tidx += res[0].shape[0]\n\n\t\t\telse:\n\t\t\t\tdatshape = results[0].shape[1:]\n\n\t\t\t\tfor res in results:\n\t\t\t\t\tlength += res.shape[0]\n\n\t\t\t\tself.data = np.empty((length, ) + datshape, dtype=np.float32)\n\n\t\t\t\tidx = 0\n\t\t\t\tfor res in results:\n\t\t\t\t\tself.data[idx:idx + res.shape[0]] = res\n\t\t\t\t\tidx += res.shape[0]\n\n\t\treturn self.data\n\n\n\t@staticmethod\n\tdef worker(transformers, batch, threadidx):\n\t\tfor transformer in transformers:\n\t\t\tbatch = transformer(batch, threadidx)\n\n\t\treturn batch, threadidx\n'"
Transformers/Serial.py,6,"b'import numpy as np\n\nfrom PuzzleLib.Transformers.Provider import Provider\n\n\nclass Serial(Provider):\n\tdef __init__(self, dataset, labels=None, numofthreads=4):\n\t\tsuper().__init__(numofthreads)\n\n\t\tself.datalen = dataset.shape[0]\n\n\t\tself.labels = labels\n\t\tself.dataset = dataset\n\n\t\tself.index = 0\n\n\n\tdef getNextChunk(self, chunksize, **kwargs):\n\t\tif chunksize >= self.datalen:\n\t\t\tself.index = 0\n\n\t\t\tif self.labels is not None:\n\t\t\t\treturn np.array(self.dataset), np.array(self.labels)\n\t\t\telse:\n\t\t\t\treturn np.array(self.dataset)\n\n\t\tbegin = self.index\n\t\tend = self.index + chunksize\n\n\t\tif end > self.datalen:\n\t\t\tchunk = np.empty((chunksize, ) + self.dataset.shape[1:], dtype=self.dataset.dtype)\n\t\t\ttup = chunk\n\n\t\t\tchunk[:self.datalen - begin] = self.dataset[begin:self.datalen]\n\n\t\t\tself.index = end - self.datalen\n\t\t\tchunk[self.datalen - begin:] = self.dataset[:self.index]\n\n\t\t\tif self.labels is not None:\n\t\t\t\tlabels = np.empty((chunksize, ) + self.dataset.shape[1:], dtype=self.labels.dtype)\n\t\t\t\ttup = (chunk, labels)\n\n\t\t\t\tlabels[:self.datalen - begin] = self.labels[begin:self.datalen]\n\t\t\t\tlabels[self.datalen - begin:] = self.labels[:self.index]\n\n\t\telse:\n\t\t\tself.index = end\n\t\t\tchunk = np.array(self.dataset[begin:end])\n\t\t\ttup = chunk\n\n\t\t\tif self.labels is not None:\n\t\t\t\tlabels = np.array(self.labels[begin:end])\n\t\t\t\ttup = (chunk, labels)\n\n\t\treturn tup\n\n\ndef unittest():\n\tfrom PuzzleLib.Datasets.ZipLoader import ZipLoader\n\n\tzipfile = ZipLoader()\n\tdata = zipfile.load(""../TestData/test.zip"")\n\n\twith Serial(data) as serial:\n\t\tfor _ in range(10):\n\t\t\tserial.prepareData(chunksize=4)\n\t\t\tserial.getData()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Transformers/Transformer.py,0,"b'class Transformer:\n\tdef __call__(self, batch, threadidx):\n\t\treturn batch\n\n\ndef unittest():\n\tfrom PuzzleLib.Transformers.Merger import Merger\n\tfrom PuzzleLib.Datasets.ZipLoader import ZipLoader\n\n\tzipfile = ZipLoader()\n\tdata1 = zipfile.load(""../TestData/test.zip"")\n\tdata2 = zipfile.load(""../TestData/test.zip"")\n\n\twith Merger([data1, data2]) as merger:\n\t\tmerger.addTransformer(Transformer())\n\n\t\tfor _ in range(10):\n\t\t\tmerger.prepareData(chunksize=4, permutate=False)\n\t\t\tmerger.getData()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Backend/Dnn/Basic.py,1,"b'from enum import Enum\nfrom PuzzleLib import Config\n\n\nConvFwdAlgo = None\nConvBwdFilterAlgo = None\nConvBwdDataAlgo = None\nconvNd = None\nconvNdBackwardData = None\nconvNdBackwardParams = None\n\nconvNdbenchmark = None\n\ndeconvNd = None\ndeconvNdBackwardData = None\ndeconvNdBackwardParams = None\n\nPoolMode = None\npoolNd = None\npoolNdBackward = None\n\nBatchNormMode = None\nbatchNormNd = None\nbatchNormNdBackward = None\n\nSoftMaxMode = None\nsoftmaxNd = None\nsoftmaxNdBackward = None\n\nmapLRN = None\nmapLRNBackward = None\n\ncrossMapLRN = None\ncrossMapLRNBackward = None\n\n\ndef autoinit():\n\tif not Config.shouldInit():\n\t\treturn\n\n\tif Config.backend == Config.Backend.cuda:\n\t\tinitCuda()\n\telif Config.backend == Config.Backend.hip:\n\t\tinitHip()\n\telif Config.backend == Config.Backend.cpu:\n\t\tinitCPU()\n\telif Config.backend == Config.Backend.intel:\n\t\tinitIntel()\n\telse:\n\t\traise Config.ConfigError(Config.backend)\n\n\ndef initCuda():\n\tfrom PuzzleLib.Cuda import Backend\n\n\tbackend = initGPU(Backend)\n\tmemoryPool, dnn = backend.memoryPool, backend.dnn\n\n\tdef wrapPoolNd(data, size, stride, pad, mode, test):\n\t\treturn dnn.poolNd(data, size, stride, pad, mode.value, None, memoryPool), None\n\n\tdef wrapPoolNdBackward(indata, outdata, grad, _, size, stride, pad, mode):\n\t\treturn dnn.poolNdBackward(grad, indata, outdata, size, stride, pad, mode.value, None, memoryPool)\n\n\tglobal PoolMode, poolNd, poolNdBackward\n\tPoolMode = backend.PoolMode\n\tpoolNd = wrapPoolNd\n\tpoolNdBackward = wrapPoolNdBackward\n\n\tdef wrapMapLRN(data, means, N, alpha, beta, K, test):\n\t\treturn dnn.mapLRN(data, means, N, alpha, beta, K, allocator=memoryPool), None\n\n\tdef wrapMapLRNBackward(data, _, grad, means, __, N, alpha, beta, K):\n\t\treturn dnn.mapLRNBackward(data, grad, means, N, alpha, beta, K, allocator=memoryPool)\n\n\tglobal mapLRN, mapLRNBackward\n\tmapLRN = wrapMapLRN\n\tmapLRNBackward = wrapMapLRNBackward\n\n\tdef wrapCrossMapLRN(data, N, alpha, beta, K, test):\n\t\treturn dnn.crossMapLRN(data, N, alpha, beta, K, allocator=memoryPool), None\n\n\tdef wrapCrossMapLRNBackward(data, outdata, grad, _, N, alpha, beta, K):\n\t\treturn dnn.crossMapLRNBackward(data, outdata, grad, N, alpha, beta, K, allocator=memoryPool)\n\n\tglobal crossMapLRN, crossMapLRNBackward\n\tcrossMapLRN = wrapCrossMapLRN\n\tcrossMapLRNBackward = wrapCrossMapLRNBackward\n\n\ndef initHip():\n\tfrom PuzzleLib.Hip import Backend\n\tinitGPU(Backend)\n\n\tbackend = initGPU(Backend)\n\tmemoryPool, dnn = backend.memoryPool, backend.dnn\n\n\tdef wrapPoolNd(data, size, stride, pad, mode, test):\n\t\tresult = dnn.poolNd(data, size, stride, pad, mode.value, test, None, memoryPool)\n\t\treturn result if not test else (result, None)\n\n\tdef wrapPoolNdBackward(indata, outdata, grad, workspace, size, stride, pad, mode):\n\t\treturn dnn.poolNdBackward(grad, indata, outdata, workspace, size, stride, pad, mode.value, None, memoryPool)\n\n\tglobal PoolMode, poolNd, poolNdBackward\n\tPoolMode = backend.PoolMode\n\tpoolNd = wrapPoolNd\n\tpoolNdBackward = wrapPoolNdBackward\n\n\tdef wrapMapLRN(data, means, N, alpha, beta, K, test):\n\t\tassert means is None\n\n\t\tresult = dnn.lrn(data, N, alpha, beta, K, backend.LRNMode.map.value, test, allocator=memoryPool)\n\t\treturn result if not test else (result, None)\n\n\tdef wrapMapLRNBackward(data, outdata, grad, means, workspace, N, alpha, beta, K):\n\t\tassert means is None\n\t\treturn dnn.lrnBackward(\n\t\t\tgrad, data, outdata, workspace, N, alpha, beta, K, backend.LRNMode.map.value, allocator=memoryPool\n\t\t)\n\n\tglobal mapLRN, mapLRNBackward\n\tmapLRN = wrapMapLRN\n\tmapLRNBackward = wrapMapLRNBackward\n\n\ndef initGPU(Backend):\n\timport numpy as np\n\n\tbackend = Backend.getBackend(Config.deviceIdx, initmode=1)\n\tmemoryPool, dnn = backend.memoryPool, backend.dnn\n\n\tglobal ConvFwdAlgo, ConvBwdDataAlgo, ConvBwdFilterAlgo\n\tConvFwdAlgo = backend.ConvFwdAlgo\n\tConvBwdDataAlgo = backend.ConvBwdDataAlgo\n\tConvBwdFilterAlgo = backend.ConvBwdFilterAlgo\n\n\tdef wrapConvNd(data, W, bias, stride, pad, dilation, groups, algo):\n\t\treturn dnn.convNd(\n\t\t\tdata, W, bias.ravel() if bias is not None else None, stride, pad, dilation, groups,\n\t\t\talgo.value, None, memoryPool\n\t\t)\n\n\tdef wrapConvNdBackwardData(grad, W, data, stride, pad, dilation, groups, algo):\n\t\treturn dnn.convNdBackwardData(\n\t\t\tgrad, W, None, data, stride, pad, dilation, groups, algo.value, None, memoryPool\n\t\t)\n\n\tdef wrapConvNdBackwardParams(data, grad, W, bias, stride, pad, dilation, groups,\n\t\t\t\t\t\t\t\t wgrad, bgrad, scale, momentum, algo):\n\t\treturn dnn.convNdBackwardParams(\n\t\t\tdata, grad, W, stride, pad, dilation, groups, bias is not None, False,\n\t\t\twgrad, bgrad.ravel() if bgrad is not None else None, scale, momentum, algo.value, memoryPool\n\t\t)\n\n\tglobal convNd, convNdBackwardData, convNdBackwardParams\n\tconvNd = wrapConvNd\n\tconvNdBackwardData = wrapConvNdBackwardData\n\tconvNdBackwardParams = wrapConvNdBackwardParams\n\n\tdef wrapConvNdbenchmark(datashape, Wshape, stride, pad, dilation, groups, transpose):\n\t\tfwdResults, bwdDataResults, bwdParamResults = backend.convNdbenchmark(\n\t\t\tdatashape, Wshape, np.float32, stride, pad, dilation, groups\n\t\t)\n\n\t\treturn fwdResults, bwdParamResults, bwdDataResults\n\n\tglobal convNdbenchmark\n\tconvNdbenchmark = wrapConvNdbenchmark\n\n\tdef wrapDeconvNd(data, W, bias, stride, pad, dilation, groups, algo):\n\t\treturn dnn.convNdBackwardData(\n\t\t\tdata, W, bias.ravel() if bias is not None else None, None, stride, pad, dilation, groups,\n\t\t\talgo.value, None, memoryPool\n\t\t)\n\n\tdef wrapDeconvNdBackwardData(grad, W, data, stride, pad, dilation, groups, algo):\n\t\tassert data is not None\n\t\treturn dnn.convNd(grad, W, None, stride, pad, dilation, groups, algo.value, None, memoryPool)\n\n\tdef wrapDeconvNdBackwardParams(data, grad, W, bias, stride, pad, dilation, groups,\n\t\t\t\t\t\t\t\t   wgrad, bgrad, scale, momentum, algo):\n\t\treturn dnn.convNdBackwardParams(\n\t\t\tgrad, data, W, stride, pad, dilation, groups, bias is not None, True,\n\t\t\twgrad, bgrad.ravel() if bgrad is not None else None, scale, momentum, algo.value, memoryPool\n\t\t)\n\n\tglobal deconvNd, deconvNdBackwardData, deconvNdBackwardParams\n\tdeconvNd = wrapDeconvNd\n\tdeconvNdBackwardData = wrapDeconvNdBackwardData\n\tdeconvNdBackwardParams = wrapDeconvNdBackwardParams\n\n\tglobal BatchNormMode\n\tBatchNormMode = backend.BatchNormMode\n\n\tdef wrapBatchNormNd(data, scale, bias, mean, var, epsilon, factor, test, mode=BatchNormMode.spatial, out=None):\n\t\tshape = scale.shape\n\t\tresult = dnn.batchNormNd(\n\t\t\tdata, mean.ravel(), var.ravel(), scale.ravel(), bias.ravel(), epsilon, factor, test, mode.value, out=out,\n\t\t\tallocator=memoryPool\n\t\t)\n\n\t\tif test:\n\t\t\treturn result\n\n\t\toutdata, savemean, saveinvvar = result\n\t\treturn outdata, savemean.reshape(shape), saveinvvar.reshape(shape)\n\n\tdef wrapBatchNormNdBackward(data, grad, scale, savemean, saveinvvar, epsilon, mode=BatchNormMode.spatial):\n\t\tshape = scale.shape\n\t\tingrad, scalegrad, bgrad = dnn.batchNormNdBackward(\n\t\t\tgrad, data, scale.ravel(), savemean.ravel(), saveinvvar.ravel(), epsilon, mode.value, allocator=memoryPool\n\t\t)\n\n\t\treturn ingrad, scalegrad.reshape(shape), bgrad.reshape(shape)\n\n\tglobal batchNormNd, batchNormNdBackward\n\tbatchNormNd = wrapBatchNormNd\n\tbatchNormNdBackward = wrapBatchNormNdBackward\n\n\tglobal SoftMaxMode\n\tSoftMaxMode = backend.SoftMaxMode\n\n\tdef wrapSoftmaxNd(data, mode=SoftMaxMode.spatial):\n\t\treturn dnn.softmaxNd(data, mode.value, allocator=memoryPool)\n\n\tdef wrapSoftmaxNdBackward(outdata, grad):\n\t\treturn dnn.softmaxNdBackward(grad, outdata, allocator=memoryPool)\n\n\tglobal softmaxNd, softmaxNdBackward\n\tsoftmaxNd = wrapSoftmaxNd\n\tsoftmaxNdBackward = wrapSoftmaxNdBackward\n\n\treturn backend\n\n\ndef initCPU():\n\tfrom PuzzleLib.CPU.Wrappers import NumpyDnn\n\n\tdef wrapConvNd(data, W, bias, stride, pad, dilation, groups, algo):\n\t\tassert dilation == (1, 1) and groups == 1\n\t\treturn NumpyDnn.conv2d(data, W, bias, stride, pad)\n\n\tglobal convNd, convNdBackwardData, convNdBackwardParams\n\tconvNd = wrapConvNd\n\n\tdef wrapPoolNd(data, size, stride, pad, mode, test):\n\t\treturn NumpyDnn.pool2d(data, size, stride, pad, mode), None\n\n\tglobal PoolMode, poolNd, poolNdBackward\n\tPoolMode = NumpyDnn.PoolMode\n\tpoolNd = wrapPoolNd\n\n\tclass ProxyBatchNormMode(Enum):\n\t\tperActivation = 0\n\t\tspatial = 1\n\n\tdef wrapBatchNormNd(data, scale, bias, mean, var, epsilon, factor, test, mode=None, out=None):\n\t\toutdata = NumpyDnn.batchNorm2d(data, scale, bias, mean, var, epsilon, test, out=out)\n\t\treturn outdata if test else (outdata, mean, var)\n\n\tglobal BatchNormMode, batchNormNd\n\tBatchNormMode = ProxyBatchNormMode\n\tbatchNormNd = wrapBatchNormNd\n\n\ndef initIntel():\n\tfrom PuzzleLib.Intel.Wrappers import DNNL\n\n\tglobal ConvFwdAlgo, ConvBwdDataAlgo, ConvBwdFilterAlgo\n\tConvFwdAlgo = DNNL.ConvAlgo\n\tConvBwdDataAlgo = DNNL.ConvAlgo\n\tConvBwdFilterAlgo = DNNL.ConvAlgo\n\n\tdef wrapConvNd(data, W, bias, stride, pad, dilation, groups, algo):\n\t\tassert groups == 1\n\t\treturn DNNL.convNd(data, W, bias, stride, pad, dilation, algo=algo)\n\n\tdef wrapConvNdBackwardData(grad, W, data, stride, pad, dilation, groups, algo):\n\t\tassert groups == 1\n\t\treturn DNNL.convNdBackwardData(grad, W, data, stride, pad, dilation, algo=algo)\n\n\tdef wrapConvNdBackwardParams(data, grad, W, bias, stride, pad, dilation, groups, wgrad, bgrad, scale, momentum,\n\t\t\t\t\t\t\t\t algo):\n\t\tassert groups == 1\n\t\treturn DNNL.convNdBackwardParams(\n\t\t\tdata, grad, W, bias, stride, pad, dilation, wgrad, bgrad, scale, momentum, algo=algo\n\t\t)\n\n\tglobal convNd, convNdBackwardData, convNdBackwardParams\n\tconvNd = wrapConvNd\n\tconvNdBackwardData = wrapConvNdBackwardData\n\tconvNdBackwardParams = wrapConvNdBackwardParams\n\n\tdef wrapConvNdbenchmark(datashape, Wshape, stride, pad, dilation, groups, transpose):\n\t\tassert groups == 1\n\t\treturn DNNL.convNdbenchmark(datashape, Wshape, stride, pad, dilation, transpose)\n\n\tglobal convNdbenchmark\n\tconvNdbenchmark = wrapConvNdbenchmark\n\n\tdef wrapDeconvNd(data, W, bias, stride, pad, dilation, groups, algo):\n\t\tassert groups == 1\n\t\treturn DNNL.convNd(data, W, bias, stride, pad, dilation, algo=algo, transpose=True)\n\n\tdef wrapDeconvNdBackwardData(grad, W, data, stride, pad, dilation, groups, algo):\n\t\tassert groups == 1\n\t\treturn DNNL.convNdBackwardData(grad, W, data, stride, pad, dilation, algo=algo, transpose=True)\n\n\tdef wrapDeconvNdBackwardParams(data, grad, W, bias, stride, pad, dilation, groups, wgrad, bgrad, scale, momentum,\n\t\t\t\t\t\t\t\t   algo):\n\t\tassert groups == 1\n\t\treturn DNNL.convNdBackwardParams(\n\t\t\tdata, grad, W, bias, stride, pad, dilation, wgrad, bgrad, scale, momentum, algo=algo, transpose=True\n\t\t)\n\n\tglobal deconvNd, deconvNdBackwardData, deconvNdBackwardParams\n\tdeconvNd = wrapDeconvNd\n\tdeconvNdBackwardData = wrapDeconvNdBackwardData\n\tdeconvNdBackwardParams = wrapDeconvNdBackwardParams\n\n\tdef wrapPoolNd(data, size, stride, pad, mode, test):\n\t\tresult = DNNL.poolNd(data, size, stride, pad, mode, test)\n\t\treturn (result, None) if test else (result[0], result[1:])\n\n\tdef wrapPoolNdBackward(indata, outdata, grad, workspace, size, stride, pad, mode):\n\t\tworkspace, desc = workspace\n\t\treturn DNNL.poolNdBackward(indata, grad, workspace, desc, size, stride, pad, mode)\n\n\tglobal PoolMode, poolNd, poolNdBackward\n\tPoolMode = DNNL.PoolMode\n\tpoolNd = wrapPoolNd\n\tpoolNdBackward = wrapPoolNdBackward\n\n\tclass ProxyBatchNormMode(Enum):\n\t\tperActivation = 0\n\t\tspatial = 1\n\n\tdef wrapBatchNormNd(data, scale, bias, mean, var, epsilon, factor, test, mode=None, out=None):\n\t\toutdata, mean, var, desc = DNNL.batchNormNd(data, scale, bias, mean, var, epsilon, test, out=out)\n\t\treturn outdata if test else (outdata, mean, var)\n\n\tglobal BatchNormMode, batchNormNd\n\tBatchNormMode = ProxyBatchNormMode\n\tbatchNormNd = wrapBatchNormNd\n\n\tglobal softmaxNd, softmaxNdBackward\n\tsoftmaxNd = DNNL.softmaxNd\n\tsoftmaxNdBackward = DNNL.softmaxNdBackward\n\n\tdef wrapCrossMapLRN(data, N, alpha, beta, K, test):\n\t\tresult = DNNL.lrn(data, DNNL.LRNMode.cross, N, alpha, beta, K, test)\n\t\treturn (result[0], result[1:]) if not test else (result, None)\n\n\tdef wrapCrossMapLRNBackward(data, outdata, grad, workspace, N, alpha, beta, K):\n\t\tworkspace, desc = workspace\n\t\treturn DNNL.lrnBackward(data, grad, workspace, desc, DNNL.LRNMode.cross, N, alpha, beta, K)\n\n\tglobal crossMapLRN, crossMapLRNBackward\n\tcrossMapLRN = wrapCrossMapLRN\n\tcrossMapLRNBackward = wrapCrossMapLRNBackward\n\n\nautoinit()\n'"
Backend/Dnn/InstanceNorm.py,0,"b'from PuzzleLib import Config\n\n\ninstanceNorm2d = None\ninstanceNorm2dBackward = None\n\n\ndef autoinit():\n\tif not Config.shouldInit():\n\t\treturn\n\n\tif Config.backend == Config.Backend.cuda:\n\t\tinitCuda()\n\telif Config.backend == Config.Backend.hip:\n\t\tinitHip()\n\telif Config.backend == Config.Backend.cpu:\n\t\tinitCPU()\n\telif Config.backend == Config.Backend.intel:\n\t\tinitIntel()\n\telse:\n\t\traise Config.ConfigError(Config.backend)\n\n\ndef initCuda():\n\tfrom PuzzleLib.Cuda import Backend\n\tinitGPU(Backend)\n\n\ndef initHip():\n\tfrom PuzzleLib.Hip import Backend\n\tinitGPU(Backend)\n\n\ndef initGPU(Backend):\n\tbackend = Backend.getBackend(Config.deviceIdx, initmode=2)\n\tmemoryPool = backend.memoryPool\n\n\tdef wrapInstanceNorm2d(data, scale, bias, epsilon=1e-5):\n\t\treturn backend.instanceNorm2d(data, scale.ravel(), bias.ravel(), epsilon, allocator=memoryPool)\n\n\tdef wrapInstanceNorm2dBackward(grad, data, extscale, savemean, saveinvvar, epsilon, affine):\n\t\treturn backend.instanceNorm2dBackward(\n\t\t\tgrad, data, extscale, savemean, saveinvvar, epsilon, affine, allocator=memoryPool\n\t\t)\n\n\tglobal instanceNorm2d, instanceNorm2dBackward\n\tinstanceNorm2d = wrapInstanceNorm2d\n\tinstanceNorm2dBackward = wrapInstanceNorm2dBackward\n\n\ndef initCPU():\n\tpass\n\n\ndef initIntel():\n\tfrom PuzzleLib.Intel.Wrappers import DNNLInstanceNorm\n\n\tdef wrapInstanceNorm2d(data, scale, bias, epsilon):\n\t\tresult = DNNLInstanceNorm.instanceNorm2d(data, scale, bias, epsilon)\n\n\t\toutdata, savemean, savevar, extscale, extbias, desc = result\n\t\treturn outdata, savemean, savevar, (extscale, extbias, desc)\n\n\tdef wrapInstanceNorm2dBackward(grad, data, exts, savemean, savevar, epsilon, affine):\n\t\textscale, extbias, desc = exts\n\t\treturn DNNLInstanceNorm.instanceNorm2dBackward(\n\t\t\tgrad, data, extscale, extbias, savemean, savevar, epsilon, desc, affine\n\t\t)\n\n\tglobal instanceNorm2d, instanceNorm2dBackward\n\tinstanceNorm2d = wrapInstanceNorm2d\n\tinstanceNorm2dBackward = wrapInstanceNorm2dBackward\n\n\nautoinit()\n'"
Backend/Dnn/Rnn.py,1,"b'from PuzzleLib import Config\n\n\nRNNMode = None\nDirectionMode = None\n\ncreateRnn = None\n\nacquireRnnParams = None\nupdateRnnParams = None\n\nforwardRnn = None\nbackwardDataRnn = None\nbackwardParamsRnn = None\n\ndeviceSupportsBatchHint = None\n\n\ndef autoinit():\n\tif not Config.shouldInit():\n\t\treturn\n\n\tif Config.backend == Config.Backend.cuda:\n\t\tinitCuda()\n\telif Config.backend == Config.Backend.hip:\n\t\tinitHip()\n\telif Config.isCPUBased(Config.backend):\n\t\tinitCPU()\n\telse:\n\t\traise Config.ConfigError(Config.backend)\n\n\ndef initCuda():\n\tfrom PuzzleLib.Cuda import Backend\n\tinitGPU(Backend)\n\n\ndef initHip():\n\tfrom PuzzleLib.Hip import Backend\n\tinitGPU(Backend)\n\n\ndef initGPU(Backend):\n\tbackend = Backend.getBackend(Config.deviceIdx, initmode=1)\n\tmemoryPool = backend.memoryPool\n\n\tglobal RNNMode, DirectionMode\n\tRNNMode = backend.RNNMode\n\tDirectionMode = backend.DirectionMode\n\n\tdef wrapCreateRnn(insize, hsize, layers, mode, direction, dropout, seed, batchsize):\n\t\timport numpy as np\n\n\t\trnn, W, params = backend.createRnn(\n\t\t\tinsize, hsize, np.float32, layers, mode=mode, direction=direction, dropout=dropout,\n\t\t\tseed=seed, batchsize=0 if batchsize is None else batchsize\n\t\t)\n\n\t\treturn rnn, W, {i: layer for i, layer in enumerate(params)}\n\n\tdef wrapAcquireRnnParams(descRnn, w):\n\t\tparams = backend.acquireRnnParams(descRnn, w)\n\t\treturn w, params\n\n\tdef wrapUpdateRnnParams(descRnn, w, params):\n\t\tparams = [params[layer]for layer in sorted(params.keys())]\n\t\tbackend.updateRnnParams(descRnn, w, params)\n\n\tglobal createRnn, acquireRnnParams, updateRnnParams\n\tcreateRnn = wrapCreateRnn\n\tacquireRnnParams = wrapAcquireRnnParams\n\tupdateRnnParams = wrapUpdateRnnParams\n\n\tdef wrapForwardRnn(data, W, descRnn, test=False):\n\t\treturn descRnn.forward(data, W, test=test, allocator=memoryPool)\n\n\tdef wrapBackwardDataRnn(grad, outdata, W, reserve, descRnn):\n\t\tingrad, _, _ = descRnn.backwardData(grad, outdata, W, reserve, allocator=memoryPool)\n\t\treturn ingrad, reserve\n\n\tdef wrapBackwardParamsRnn(data, outdata, _, reserve, descRnn):\n\t\treturn descRnn.backwardParams(data, outdata, reserve, allocator=memoryPool)\n\n\tglobal forwardRnn, backwardDataRnn, backwardParamsRnn\n\tforwardRnn = wrapForwardRnn\n\tbackwardDataRnn = wrapBackwardDataRnn\n\tbackwardParamsRnn = wrapBackwardParamsRnn\n\n\tglobal deviceSupportsBatchHint\n\tdeviceSupportsBatchHint = backend.deviceSupportsBatchHint\n\n\ndef initCPU():\n\tglobal deviceSupportsBatchHint\n\tdeviceSupportsBatchHint = lambda: False\n\n\nautoinit()\n'"
Backend/Dnn/SpatialTf.py,0,"b'from PuzzleLib import Config\n\n\nspatialTf = None\nspatialTfBackward = None\n\n\ndef autoinit():\n\tif not Config.shouldInit():\n\t\treturn\n\n\tif Config.backend == Config.Backend.cuda:\n\t\tinitCuda()\n\telif Config.backend == Config.Backend.hip:\n\t\tinitHip()\n\telif Config.isCPUBased(Config.backend):\n\t\tinitCPU()\n\telse:\n\t\traise Config.ConfigError(Config.backend)\n\n\ndef initCuda():\n\tfrom PuzzleLib.Cuda.Backend import getBackend\n\n\tbackend = getBackend(Config.deviceIdx, initmode=1)\n\tmemoryPool, dnn = backend.memoryPool, backend.dnn\n\n\tdef wrapSpatialTf(data, transform, outshape, getGrid):\n\t\treturn dnn.spatialTf(data, transform, outshape, getGrid, allocator=memoryPool)\n\n\tdef wrapSpatialTfBackward(grad, data, grid):\n\t\treturn dnn.spatialTfBackward(grad, data, grid, allocator=memoryPool)\n\n\tglobal spatialTf, spatialTfBackward\n\tspatialTf = wrapSpatialTf\n\tspatialTfBackward = wrapSpatialTfBackward\n\n\ndef initHip():\n\tpass\n\n\ndef initCPU():\n\tpass\n\n\nautoinit()\n'"
Backend/Kernels/Costs.py,0,"b'from PuzzleLib import Config\n\n\nbceKer = None\nhingeKer = None\nsmoothL1Ker = None\nl1HingeKer = None\n\ngetAccuracyKernel = None\ncrossEntropyKernel = None\nsvmKernel = None\n\nctcLoss = None\nctcLossTest = None\n\n\ndef autoinit():\n\tif not Config.shouldInit():\n\t\treturn\n\n\tif Config.backend == Config.Backend.cuda:\n\t\tinitCuda()\n\telif Config.backend == Config.Backend.hip:\n\t\tinitHip()\n\telif Config.backend == Config.Backend.cpu:\n\t\tinitCPU()\n\telif Config.backend == Config.Backend.intel:\n\t\tinitIntel()\n\telse:\n\t\traise Config.ConfigError(Config.backend)\n\n\ndef initCuda():\n\tfrom PuzzleLib.Cuda import Backend\n\tfrom PuzzleLib.Cuda.Kernels import CTC\n\n\tinitGPU(Backend, CTC)\n\n\ndef initHip():\n\tfrom PuzzleLib.Hip import Backend\n\tfrom PuzzleLib.Cuda.Kernels import CTC\n\n\tinitGPU(Backend, CTC)\n\n\ndef initGPU(Backend, CTC):\n\tbackend = Backend.getBackend(Config.deviceIdx, initmode=2)\n\tmemoryPool, costmod, ctcmod = backend.memoryPool, backend.costmod, backend.ctcmod\n\n\tglobal bceKer, hingeKer, smoothL1Ker, l1HingeKer, getAccuracyKernel\n\tbceKer = backend.bceKer\n\thingeKer = backend.hingeKer\n\tsmoothL1Ker = backend.smoothL1Ker\n\tl1HingeKer = backend.l1HingeKer\n\tgetAccuracyKernel = backend.getAccuracyKernel\n\n\tdef wrapCrossEntropy(scores, labels, weights, error):\n\t\treturn costmod.crossEntropy(scores, labels, weights, error, memoryPool)\n\n\tdef wrapSVM(scores, labels, mode, error):\n\t\treturn costmod.svm(scores, labels, mode, error, memoryPool)\n\n\tglobal crossEntropyKernel, svmKernel\n\tcrossEntropyKernel = wrapCrossEntropy\n\tsvmKernel = wrapSVM\n\n\tdef wrapCTC(data, datalen, labels, lengths, blank, error, normalized):\n\t\treturn ctcmod.ctcLoss(data, datalen, labels, lengths, blank, error, normalized, allocator=memoryPool)\n\n\tglobal ctcLoss, ctcLossTest\n\tctcLoss = wrapCTC\n\tctcLossTest = CTC.hostCTCLoss\n\n\ndef initCPU():\n\tpass\n\n\ndef initIntel():\n\tfrom PuzzleLib.Intel.Kernels import Costs\n\n\tglobal bceKer, hingeKer, smoothL1Ker, l1HingeKer, getAccuracyKernel, crossEntropyKernel, svmKernel\n\tbceKer = Costs.bceKer\n\thingeKer = Costs.hingeKer\n\tsmoothL1Ker = Costs.smoothL1Ker\n\tl1HingeKer = Costs.l1HingeKer\n\tgetAccuracyKernel = Costs.getAccuracyKernel\n\tcrossEntropyKernel = Costs.crossEntropy\n\tsvmKernel = Costs.svm\n\n\nautoinit()\n'"
Backend/Kernels/ElementWise.py,0,"b'from PuzzleLib import Config\n\n\nsigmoidKer = None\nsigmoidDerKer = None\ntanhKer = None\ntanhDerKer = None\nreluKer = None\nreluDerKer = None\nleakyReluKer = None\nleakyReluDerKer = None\neluKer = None\neluDerKer = None\nsoftPlusKer = None\nsoftPlusDerKer = None\nclipKer = None\nclipDerKer = None\ngeluKer = None\ngeluDerKer = None\n\ndropoutKer = None\ndropout2dKer = None\nrbmKer = None\nabsKer = None\ntoVectorAddVectorKer = None\n\nclassicMomSGDKer = None\nnesterovMomSGDKer = None\nrmspropKer = None\nadamKer = None\nrmspropGravesKer = None\nadagradKer = None\nadadeltaKer = None\nsmorms3Ker = None\n\nweightDecayKer = None\nlinearKer = None\naddKer = None\nmulKer = None\nl1penaltyKer = None\nl1gradKer = None\n\ncastFP16toFP32 = None\ncastFP32toFP16 = None\n\n\ndef autoinit():\n\tif not Config.shouldInit():\n\t\treturn\n\n\tif Config.backend == Config.Backend.cuda:\n\t\tinitCuda()\n\telif Config.backend == Config.Backend.hip:\n\t\tinitHip()\n\telif Config.isCPUBased(Config.backend):\n\t\tinitCPU()\n\telse:\n\t\traise Config.ConfigError(Config.backend)\n\n\ndef initCuda():\n\tfrom PuzzleLib.Cuda import Backend\n\tinitGPU(Backend)\n\n\ndef initHip():\n\tfrom PuzzleLib.Hip import Backend\n\tinitGPU(Backend)\n\n\ndef initGPU(Backend):\n\tbackend = Backend.getBackend(Config.deviceIdx, initmode=2)\n\n\tglobal sigmoidKer, sigmoidDerKer, tanhKer, tanhDerKer, reluKer, reluDerKer, leakyReluKer, leakyReluDerKer\n\tglobal eluKer, eluDerKer, softPlusKer, softPlusDerKer, clipKer, clipDerKer, geluKer, geluDerKer\n\tsigmoidKer = backend.sigmoidKer\n\tsigmoidDerKer = backend.sigmoidDerKer\n\ttanhKer = backend.tanhKer\n\ttanhDerKer = backend.tanhDerKer\n\treluKer = backend.reluKer\n\treluDerKer = backend.reluDerKer\n\tleakyReluKer = backend.leakyReluKer\n\tleakyReluDerKer = backend.leakyReluDerKer\n\teluKer = backend.eluKer\n\teluDerKer = backend.eluDerKer\n\tsoftPlusKer = backend.softPlusKer\n\tsoftPlusDerKer = backend.softPlusDerKer\n\tclipKer = backend.clipKer\n\tclipDerKer = backend.clipDerKer\n\tgeluKer = backend.geluKer\n\tgeluDerKer = backend.geluDerKer\n\n\tglobal dropoutKer, dropout2dKer, rbmKer, absKer, toVectorAddVectorKer\n\tdropoutKer = backend.dropoutKer\n\tdropout2dKer = backend.dropout2dKer\n\trbmKer = backend.rbmKer\n\tabsKer = backend.absKer\n\ttoVectorAddVectorKer = backend.toVectorAddVectorKer\n\n\tglobal classicMomSGDKer, nesterovMomSGDKer, rmspropKer, adamKer, rmspropGravesKer, adagradKer, adadeltaKer\n\tglobal smorms3Ker\n\tclassicMomSGDKer = backend.classicMomSGDKer\n\tnesterovMomSGDKer = backend.nesterovMomSGDKer\n\trmspropKer = backend.rmspropKer\n\tadamKer = backend.adamKer\n\trmspropGravesKer = backend.rmspropGravesKer\n\tadagradKer = backend.adagradKer\n\tadadeltaKer = backend.adadeltaKer\n\tsmorms3Ker = backend.smorms3Ker\n\n\tglobal weightDecayKer, linearKer, addKer, mulKer, l1penaltyKer, l1gradKer\n\tweightDecayKer = backend.weightDecayKer\n\tlinearKer = backend.linearKer\n\taddKer = backend.addKer\n\tmulKer = backend.mulKer\n\tl1penaltyKer = backend.l1penaltyKer\n\tl1gradKer = backend.l1gradKer\n\n\tglobal castFP16toFP32, castFP32toFP16\n\tcastFP16toFP32 = backend.castFP16toFP32\n\tcastFP32toFP16 = backend.castFP32toFP16\n\n\ndef initCPU():\n\tfrom PuzzleLib.CPU.Kernels import ElementWise\n\n\tglobal sigmoidKer, sigmoidDerKer, tanhKer, tanhDerKer, reluKer, reluDerKer, leakyReluKer, leakyReluDerKer\n\tglobal eluKer, eluDerKer, softPlusKer, softPlusDerKer, clipKer, clipDerKer\n\tsigmoidKer = ElementWise.sigmoidKer\n\tsigmoidDerKer = ElementWise.sigmoidDerKer\n\ttanhKer = ElementWise.tanhKer\n\ttanhDerKer = ElementWise.tanhDerKer\n\treluKer = ElementWise.reluKer\n\treluDerKer = ElementWise.reluDerKer\n\tleakyReluKer = ElementWise.leakyReluKer\n\tleakyReluDerKer = ElementWise.leakyReluDerKer\n\teluKer = ElementWise.eluKer\n\teluDerKer = ElementWise.eluDerKer\n\tsoftPlusKer = ElementWise.softPlusKer\n\tsoftPlusDerKer = ElementWise.softPlusDerKer\n\tclipKer = ElementWise.clipKer\n\tclipDerKer = ElementWise.clipDerKer\n\n\tglobal dropoutKer, dropout2dKer, rbmKer, absKer, toVectorAddVectorKer\n\tdropoutKer = ElementWise.dropoutKer\n\tdropout2dKer = ElementWise.dropout2dKer\n\trbmKer = ElementWise.rbmKer\n\tabsKer = ElementWise.absKer\n\ttoVectorAddVectorKer = ElementWise.toVectorAddVectorKer\n\n\tglobal classicMomSGDKer, nesterovMomSGDKer, rmspropKer, adamKer, rmspropGravesKer, adagradKer, adadeltaKer\n\tglobal smorms3Ker\n\tclassicMomSGDKer = ElementWise.classicMomSGDKer\n\tnesterovMomSGDKer = ElementWise.nesterovMomSGDKer\n\trmspropKer = ElementWise.rmspropKer\n\tadamKer = ElementWise.adamKer\n\trmspropGravesKer = ElementWise.rmspropGravesKer\n\tadagradKer = ElementWise.adagradKer\n\tadadeltaKer = ElementWise.adadeltaKer\n\tsmorms3Ker = ElementWise.smorms3Ker\n\n\tglobal weightDecayKer, linearKer, addKer, mulKer, l1penaltyKer, l1gradKer\n\tweightDecayKer = ElementWise.weightDecayKer\n\tlinearKer = ElementWise.linearKer\n\taddKer = ElementWise.addKer\n\tmulKer = ElementWise.mulKer\n\tl1penaltyKer = ElementWise.l1penaltyKer\n\tl1gradKer = ElementWise.l1gradKer\n\n\nautoinit()\n'"
Backend/Kernels/Embedder.py,0,"b'from PuzzleLib import Config\n\n\nembed = None\nembedBackwardParams = None\n\n\ndef autoinit():\n\tif not Config.shouldInit():\n\t\treturn\n\n\tif Config.backend == Config.Backend.cuda:\n\t\tinitCuda()\n\telif Config.backend == Config.Backend.hip:\n\t\tinitHip()\n\telif Config.isCPUBased(Config.backend):\n\t\tinitCPU()\n\telse:\n\t\traise Config.ConfigError(Config.backend)\n\n\ndef initCuda():\n\tfrom PuzzleLib.Cuda import Backend\n\tinitGPU(Backend)\n\n\ndef initHip():\n\tfrom PuzzleLib.Hip import Backend\n\tinitGPU(Backend)\n\n\ndef initGPU(Backend):\n\tbackend = Backend.getBackend(Config.deviceIdx, initmode=2)\n\tmemoryPool, embedmod = backend.memoryPool, backend.embedmod\n\n\tdef wrapEmbed(data, W):\n\t\treturn embedmod.embed(data, W, memoryPool)\n\n\tglobal embed, embedBackwardParams\n\tembed = wrapEmbed\n\tembedBackwardParams = embedmod.embedBackwardParams\n\n\ndef initCPU():\n\tpass\n\n\nautoinit()\n'"
Backend/Kernels/MatVec.py,5,"b'from PuzzleLib import Config\n\n\naddVecToMat = None\nargmax = None\n\n\ndef autoinit():\n\tif not Config.shouldInit():\n\t\treturn\n\n\tif Config.backend == Config.Backend.cuda:\n\t\tinitCuda()\n\telif Config.backend == Config.Backend.hip:\n\t\tinitHip()\n\telif Config.isCPUBased(Config.backend):\n\t\tinitCPU()\n\telse:\n\t\traise Config.ConfigError(Config.backend)\n\n\ndef initCuda():\n\tfrom PuzzleLib.Cuda import Backend\n\tinitGPU(Backend)\n\n\ndef initHip():\n\tfrom PuzzleLib.Hip import Backend\n\tinitGPU(Backend)\n\n\ndef initGPU(Backend):\n\tbackend = Backend.getBackend(Config.deviceIdx, initmode=2)\n\tmemoryPool, matmod = backend.memoryPool, backend.matmod\n\n\tdef wrapAddVecToMat(vec, mat, axis, out):\n\t\treturn matmod.addVecToMat(vec, mat, axis, out, memoryPool)\n\n\tdef wrapArgmax(tensor, axis):\n\t\treturn matmod.argmax(tensor, axis, memoryPool)\n\n\tglobal addVecToMat, argmax\n\taddVecToMat = wrapAddVecToMat\n\targmax = wrapArgmax\n\n\ndef initCPU():\n\timport numpy as np\n\tfrom PuzzleLib.CPU.CPUArray import CPUArray\n\n\tdef wrapAddVecToMat(v, m, axis, out):\n\t\tif axis == 0:\n\t\t\tv = v[:, np.newaxis]\n\t\telif axis == 1:\n\t\t\tv = v[np.newaxis, :]\n\n\t\tnp.add(m.get(copy=False), v.get(copy=False), out=out.get(copy=False))\n\n\tdef wrapArgmax(mats, axis):\n\t\tout = np.empty(mats.shape[:axis] + mats.shape[axis + 1:], dtype=np.int32)\n\t\tnp.argmax(mats.get(copy=False), axis, out=out)\n\n\t\treturn CPUArray(out.shape, out.dtype, data=out, acquire=True)\n\n\tglobal addVecToMat, argmax\n\taddVecToMat = wrapAddVecToMat\n\targmax = wrapArgmax\n\n\nautoinit()\n'"
Backend/Kernels/MatVecBatch.py,2,"b'from PuzzleLib import Config\n\n\naddVecToMatBatch = None\nargmaxBatch = None\n\n\ndef autoinit():\n\tif not Config.shouldInit():\n\t\treturn\n\n\tif Config.backend == Config.Backend.cuda:\n\t\tinitCuda()\n\telif Config.backend == Config.Backend.hip:\n\t\tinitHip()\n\telif Config.isCPUBased(Config.backend):\n\t\tinitCPU()\n\telse:\n\t\traise Config.ConfigError(Config.backend)\n\n\ndef initCuda():\n\tfrom PuzzleLib.Cuda import Backend\n\tinitGPU(Backend)\n\n\ndef initHip():\n\tfrom PuzzleLib.Hip import Backend\n\tinitGPU(Backend)\n\n\ndef initGPU(Backend):\n\tbackend = Backend.getBackend(Config.deviceIdx, initmode=2)\n\tmemoryPool, matmod = backend.memoryPool, backend.matmod\n\n\tdef wrapAddVecToMatBatch(vec, mat, axis, out):\n\t\treturn matmod.addVecToMat(vec, mat, axis, out, memoryPool)\n\n\tdef wrapArgmaxBatch(tensor, axis):\n\t\treturn matmod.argmax(tensor, axis, memoryPool)\n\n\tglobal addVecToMatBatch, argmaxBatch\n\taddVecToMatBatch = wrapAddVecToMatBatch\n\targmaxBatch = wrapArgmaxBatch\n\n\ndef initCPU():\n\timport numpy as np\n\tfrom PuzzleLib.CPU.CPUArray import CPUArray\n\n\tdef wrapArgmax(mats, axis):\n\t\tout = np.empty(mats.shape[:axis] + mats.shape[axis + 1:], dtype=np.int32)\n\t\tnp.argmax(mats.get(copy=False), axis, out=out)\n\n\t\treturn CPUArray(out.shape, out.dtype, data=out, acquire=True)\n\n\tglobal argmaxBatch\n\targmaxBatch = wrapArgmax\n\n\nautoinit()\n'"
Backend/Kernels/PRelu.py,0,"b'from PuzzleLib import Config\n\n\nprelu = None\npreluBackwardData = None\npreluBackwardParams = None\n\n\ndef autoinit():\n\tif not Config.shouldInit():\n\t\treturn\n\n\tif Config.backend == Config.Backend.cuda:\n\t\tinitCuda()\n\telif Config.backend == Config.Backend.hip:\n\t\tinitHip()\n\telif Config.isCPUBased(Config.backend):\n\t\tinitCPU()\n\telse:\n\t\traise Config.ConfigError(Config.backend)\n\n\ndef initCuda():\n\tfrom PuzzleLib.Cuda import Backend\n\tinitGPU(Backend)\n\n\ndef initHip():\n\tfrom PuzzleLib.Hip import Backend\n\tinitGPU(Backend)\n\n\ndef initGPU(Backend):\n\tbackend = Backend.getBackend(Config.deviceIdx, initmode=2)\n\tmemoryPool, prelumod = backend.memoryPool, backend.prelumod\n\n\tdef wrapPRelu(data, slopes, inplace, sharedMaps):\n\t\treturn prelumod.prelu(data, slopes, inplace, sharedMaps, memoryPool)\n\n\tdef wrapPReluBackwardData(grad, slopes, indata, sharedMaps):\n\t\treturn prelumod.preluBackwardData(grad, slopes, indata, sharedMaps, memoryPool)\n\n\tdef wrapPReluBackwardParams(indata, outgrad, sharedMaps):\n\t\treturn prelumod.preluBackwardParams(indata, outgrad, sharedMaps, memoryPool)\n\n\tglobal prelu, preluBackwardData, preluBackwardParams\n\tprelu = wrapPRelu\n\tpreluBackwardData = wrapPReluBackwardData\n\tpreluBackwardParams = wrapPReluBackwardParams\n\n\ndef initCPU():\n\tpass\n\n\nautoinit()\n'"
Backend/Kernels/Pad.py,0,"b'from PuzzleLib import Config\n\n\nreflectpad1d = None\nreflectpad1dBackward = None\n\nreflectpad2d = None\nreflectpad2dBackward = None\n\n\ndef autoinit():\n\tif not Config.shouldInit():\n\t\treturn\n\n\tif Config.backend == Config.Backend.cuda:\n\t\tinitCuda()\n\telif Config.backend == Config.Backend.hip:\n\t\tinitHip()\n\telif Config.isCPUBased(Config.backend):\n\t\tinitCPU()\n\telse:\n\t\traise Config.ConfigError(Config.backend)\n\n\ndef initCuda():\n\tfrom PuzzleLib.Cuda import Backend\n\tinitGPU(Backend)\n\n\ndef initHip():\n\tfrom PuzzleLib.Hip import Backend\n\tinitGPU(Backend)\n\n\ndef initGPU(Backend):\n\tbackend = Backend.getBackend(Config.deviceIdx, initmode=2)\n\tmemoryPool, padmod = backend.memoryPool, backend.padmod\n\n\tdef wrapReflectPad(data, pad):\n\t\treturn padmod.reflectpad(data, pad, memoryPool)\n\n\tdef wrapReflectPadBackward(grad, pad):\n\t\treturn padmod.reflectpadBackward(grad, pad, memoryPool)\n\n\tglobal reflectpad1d, reflectpad1dBackward, reflectpad2d, reflectpad2dBackward\n\treflectpad1d = reflectpad2d = wrapReflectPad\n\treflectpad1dBackward = reflectpad2dBackward = wrapReflectPadBackward\n\n\ndef initCPU():\n\tfrom PuzzleLib.CPU.Kernels import Pad\n\n\tglobal reflectpad1d\n\treflectpad1d = Pad.reflectpad1d\n\n\tglobal reflectpad2d\n\treflectpad2d = Pad.reflectpad2d\n\n\nautoinit()\n'"
Backend/Kernels/Pool.py,0,"b'from PuzzleLib import Config\n\n\nmaxpool2d = None\nmaxpool2dBackward = None\nmaxunpool2d = None\nmaxunpool2dBackward = None\n\n\ndef autoinit():\n\tif not Config.shouldInit():\n\t\treturn\n\n\tif Config.backend == Config.Backend.cuda:\n\t\tinitCuda()\n\telif Config.backend == Config.Backend.hip:\n\t\tinitHip()\n\telif Config.isCPUBased(Config.backend):\n\t\tinitCPU()\n\telse:\n\t\traise Config.ConfigError(Config.backend)\n\n\ndef initCuda():\n\tfrom PuzzleLib.Cuda import Backend\n\tinitGPU(Backend)\n\n\ndef initHip():\n\tfrom PuzzleLib.Hip import Backend\n\tinitGPU(Backend)\n\n\ndef initGPU(Backend):\n\tbackend = Backend.getBackend(Config.deviceIdx, initmode=2)\n\tmemoryPool, poolmod = backend.memoryPool, backend.poolmod\n\n\tdef wrapMaxPool2d(data, size, stride, pad):\n\t\treturn poolmod.maxpool2d(data, size, stride, pad, memoryPool)\n\n\tdef wrapMaxPool2dBackward(grad, origshape, mask, size, stride, pad):\n\t\treturn poolmod.maxpool2dBackward(grad, origshape, mask, size, stride, pad, memoryPool)\n\n\tglobal maxpool2d, maxpool2dBackward\n\tmaxpool2d = wrapMaxPool2d\n\tmaxpool2dBackward = wrapMaxPool2dBackward\n\n\tdef wrapMaxUnpool2d(data, origshape, mask):\n\t\treturn poolmod.maxunpool2d(data, origshape, mask, memoryPool)\n\n\tdef wrapMaxUnpool2dBackward(grad, poolshape, mask):\n\t\treturn poolmod.maxunpool2dBackward(grad, poolshape, mask, memoryPool)\n\n\tglobal maxunpool2d, maxunpool2dBackward\n\tmaxunpool2d = wrapMaxUnpool2d\n\tmaxunpool2dBackward = wrapMaxUnpool2dBackward\n\n\ndef initCPU():\n\tpass\n\n\nautoinit()\n'"
Backend/Kernels/Upsample.py,0,"b'from PuzzleLib import Config\n\n\nupsample2d = None\nupsample2dBackward = None\n\nupsample3d = None\nupsample3dBackward = None\n\n\ndef autoinit():\n\tif not Config.shouldInit():\n\t\treturn\n\n\tif Config.backend == Config.Backend.cuda:\n\t\tinitCuda()\n\telif Config.backend == Config.Backend.hip:\n\t\tinitHip()\n\telif Config.isCPUBased(Config.backend):\n\t\tinitCPU()\n\telse:\n\t\traise Config.ConfigError(Config.backend)\n\n\ndef initCuda():\n\tfrom PuzzleLib.Cuda import Backend\n\tinitGPU(Backend)\n\n\ndef initHip():\n\tfrom PuzzleLib.Hip import Backend\n\tinitGPU(Backend)\n\n\ndef initGPU(Backend):\n\tbackend = Backend.getBackend(Config.deviceIdx, initmode=2)\n\tmemoryPool, upsamplemod = backend.memoryPool, backend.upsamplemod\n\n\tdef wrapUpsample2d(data, scale, mode):\n\t\treturn upsamplemod.upsample2d(data, scale, mode, memoryPool)\n\n\tdef wrapUpsample2dBackward(grad, scale, mode):\n\t\treturn upsamplemod.upsample2dBackward(grad, scale, mode, memoryPool)\n\n\tglobal upsample2d, upsample2dBackward\n\tupsample2d = wrapUpsample2d\n\tupsample2dBackward = wrapUpsample2dBackward\n\n\tdef wrapUpsample3d(data, scale, mode):\n\t\treturn upsamplemod.upsample3d(data, scale, mode, memoryPool)\n\n\tdef wrapUpsample3dBackward(grad, scale, mode):\n\t\treturn upsamplemod.upsample3dBackward(grad, scale, mode, memoryPool)\n\n\tglobal upsample3d, upsample3dBackward\n\tupsample3d = wrapUpsample3d\n\tupsample3dBackward = wrapUpsample3dBackward\n\n\ndef initCPU():\n\tfrom PuzzleLib.CPU.Kernels import Upsample2D\n\n\tglobal upsample2d\n\tupsample2d = Upsample2D.upsample2d\n\n\nautoinit()\n'"
CPU/Benchmarks/Utils.py,0,"b'import time\n\n\ndef timeKernel(func, args, kwargs=None, looplength=1000, log=True, logname=None, normalize=False, hotpass=True):\n\tif kwargs is None:\n\t\tkwargs = {}\n\n\tif hotpass:\n\t\tfunc(*args, **kwargs)\n\n\thostStart = time.time()\n\n\tfor _ in range(looplength):\n\t\tfunc(*args, **kwargs)\n\n\thostEnd = time.time()\n\thostsecs = hostEnd - hostStart\n\n\tif logname is None:\n\t\tif hasattr(func, ""__name__""):\n\t\t\tlogname = ""%s.%s"" % (func.__module__, func.__name__)\n\t\telse:\n\t\t\tlogname = ""%s.%s"" % (func.__module__ , func.__class__.__name__)\n\n\tif normalize:\n\t\thostsecs /= looplength\n\n\tif log:\n\t\tprint(""%s host time: %s secs"" % (logname, hostsecs))\n\n\treturn hostsecs\n'"
CPU/Kernels/ElementWise.py,28,"b'import numpy as np\n\nfrom PuzzleLib.Compiler.Codegen.Types import int32_t, uint32_t, float_t\n\nfrom PuzzleLib.CPU.SourceModule import ElementwiseKernel\nfrom PuzzleLib.CPU.Utils import memoize\n\n\n@memoize\ndef sigmoidKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""outdata""), (float_t.const.ptr, ""indata"")],\n\t\t""outdata[i] = 1.0f / (1.0f + expf(-indata[i]))"",\n\t\t""sigmoidKer""\n\t)\n\n\n@memoize\ndef sigmoidDerKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[\n\t\t\t(float_t.ptr, ""ingrad""), (float_t.const.ptr, ""outgrad""),\n\t\t\t(float_t.const.ptr, ""outdata"")\n\t\t],\n\t\t""ingrad[i] = outgrad[i] * outdata[i] * (1.0f - outdata[i])"",\n\t\t""sigmoidDerKer""\n\t)\n\n\n@memoize\ndef tanhKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""outdata""), (float_t.const.ptr, ""indata"")],\n\t\t""outdata[i] = tanhf(indata[i])"",\n\t\t""tanhKer""\n\t)\n\n\n@memoize\ndef tanhDerKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""ingrad""), (float_t.const.ptr, ""outgrad""), (float_t.const.ptr, ""outdata"")],\n\t\t""ingrad[i] = outgrad[i] * (1.0f - outdata[i] * outdata[i])"",\n\t\t""tanhDerKer""\n\t)\n\n\n@memoize\ndef reluKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""outdata""), (float_t.const.ptr, ""indata"")],\n\t\t""outdata[i] = indata[i] * (indata[i] > 0.0f)"",\n\t\t""reluKer""\n\t)\n\n\n@memoize\ndef reluDerKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""ingrad""), (float_t.const.ptr, ""outgrad""), (float_t.const.ptr, ""outdata"")],\n\t\t""ingrad[i] = outgrad[i] * (outdata[i] > 0.0f)"",\n\t\t""reluDerKer""\n\t)\n\n\n@memoize\ndef leakyReluKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""outdata""), (float_t.const.ptr, ""indata""), (float_t, ""a"")],\n\t\t""outdata[i] = indata[i] * ((indata[i] > 0.0f) + a * (indata[i] <= 0.0f))"",\n\t\t""leakyReluKer""\n\t)\n\n\n@memoize\ndef leakyReluDerKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""ingrad""), (float_t.const.ptr, ""outgrad""), (float_t.const.ptr, ""outdata""), (float_t, ""a"")],\n\t\t""ingrad[i] = outgrad[i] * ((outdata[i] > 0.0f) + a * (outdata[i] <= 0.0f))"",\n\t\t""leakyReluDerKer""\n\t)\n\n\n@memoize\ndef eluKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""outdata""), (float_t.const.ptr, ""indata""), (float_t, ""a"")],\n\t\t""outdata[i] = indata[i] * (indata[i] > 0.0f) + a * (expf(indata[i]) - 1.0f) * (indata[i] <= 0.0f)"",\n\t\t""eluKer""\n\t)\n\n\n@memoize\ndef eluDerKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""ingrad""), (float_t.const.ptr, ""outgrad""), (float_t.const.ptr, ""outdata""), (float_t, ""a"")],\n\t\t""ingrad[i] = outgrad[i] * ((outdata[i] > 0.0f) + (outdata[i] + a) * (outdata[i] <= 0.0f))"",\n\t\t""eluDerKer""\n\t)\n\n\n@memoize\ndef softPlusKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""outdata""), (float_t.const.ptr, ""indata"")],\n\t\t""outdata[i] = logf(1.0f + expf(indata[i]))"",\n\t\t""softPlusKer""\n\t)\n\n\n@memoize\ndef softPlusDerKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""ingrad""), (float_t.const.ptr, ""outgrad""), (float_t.const.ptr, ""outdata"")],\n\t\t""ingrad[i] = outgrad[i] * (1.0f - expf(-outdata[i]))"",\n\t\t""softPlusDerKer""\n\t)\n\n\n@memoize\ndef clipKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""outdata""), (float_t.const.ptr, ""indata""), (float_t, ""a""), (float_t, ""b"")],\n\t\t""outdata[i] = indata[i] * (indata[i] > a && indata[i] < b) + a * (indata[i] <= a) + b * (indata[i] >= b)"",\n\t\t""clipKer""\n\t)\n\n\n@memoize\ndef clipDerKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[\n\t\t\t(float_t.ptr, ""ingrad""), (float_t.const.ptr, ""outgrad""), (float_t.const.ptr, ""outdata""),\n\t\t\t(float_t, ""a""), (float_t, ""b"")\n\t\t],\n\t\t""ingrad[i] = outgrad[i] * (outdata[i] > a && outdata[i] < b);"",\n\t\t""clipDerKer""\n\t)\n\n\n@memoize\ndef dropoutKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[\n\t\t\t(float_t.ptr, ""outdata""), (float_t.const.ptr, ""indata""), (uint32_t.const.ptr, ""b""),\n\t\t\t(uint32_t, ""v""), (float_t, ""p"")\n\t\t],\n\t\t""outdata[i] = indata[i] * (b[i] < v) / p"",\n\t\t""dropoutKer""\n\t)\n\n\n@memoize\ndef dropout2dKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[\n\t\t\t(float_t.ptr, ""outdata""), (float_t.const.ptr, ""indata""), (uint32_t.const.ptr, ""b""), (uint32_t, ""v""),\n\t\t\t(float_t, ""p""), (int32_t, ""mapsize"")\n\t\t],\n\t\t""outdata[i] = indata[i] * (b[i / mapsize] < v) / p"",\n\t\t""dropout2dKer""\n\t)\n\n\n@memoize\ndef toVectorAddVectorKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""outdata""), (float_t.const.ptr, ""indata""), (float_t, ""alpha"")],\n\t\t""outdata[i] += indata[i] * alpha"",\n\t\t""toVectorAddVectorKer""\n\t)\n\n\naddVectorToVectorKer = ElementwiseKernel(\n\t[\n\t\t(float_t.ptr, ""outdata""), (float_t.const.ptr, ""x""), (float_t.const.ptr, ""y""),\n\t\t(float_t, ""alpha""), (float_t, ""beta"")\n\t],\n\t""outdata[i] = x[i] * alpha + y[i] * beta"",\n\t""addVectorToVectorKer""\n)\n\n\n@memoize\ndef adadeltaKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[\n\t\t\t(float_t.ptr, ""param""), (float_t.const.ptr, ""grad""), (float_t.ptr, ""msg""), (float_t.ptr, ""msdx""),\n\t\t\t(float_t, ""rho""), (float_t, ""epsilon"")\n\t\t],\n\t\t""""""\n\t\tmsg[i] += (1.0f - rho) * (grad[i] * grad[i] - msg[i]);\n\t\tfloat dx = sqrtf((msdx[i] + epsilon) / (msg[i] + epsilon)) * grad[i];\n\t\tmsdx[i] += (1.0f - rho) * (dx * dx - msdx[i]);\n\t\tparam[i] += dx;\n\t\t"""""",\n\t\t""adadeltaKer""\n\t)\n\n\n@memoize\ndef adagradKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[\n\t\t\t(float_t.ptr, ""param""), (float_t.const.ptr, ""grad""), (float_t.ptr, ""h""),\n\t\t\t(float_t, ""learnRate""), (float_t, ""epsilon"")\n\t\t],\n\t\t""""""\n\t\th[i] += grad[i] * grad[i];\n\t\tparam[i] += learnRate * grad[i] / (sqrtf(h[i]) + epsilon);\n\t\t"""""",\n\t\t""adagradKer""\n\t)\n\n\n@memoize\ndef adamKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[\n\t\t\t(float_t.ptr, ""param""), (float_t.const.ptr, ""grad""), (float_t.ptr, ""mg""), (float_t.ptr, ""ms""),\n\t\t\t(float_t, ""learnRate""), (float_t, ""fix1""), (float_t, ""fix2""), (float_t, ""epsilon"")\n\t\t],\n\t\t""""""\n\t\tmg[i] += fix1 * (grad[i] - mg[i]);\n\t\tms[i] += fix2 * (grad[i] * grad[i] - ms[i]);\n\t\tparam[i] += learnRate * mg[i] / (sqrtf(ms[i]) + epsilon);\n\t\t"""""",\n\t\t""adamKer""\n\t)\n\n\n@memoize\ndef classicMomSGDKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[\n\t\t\t(float_t.ptr, ""param""), (float_t.const.ptr, ""grad""), (float_t.ptr, ""mom""),\n\t\t\t(float_t, ""learnRate""), (float_t, ""momRate"")\n\t\t],\n\t\t""""""\n\t\tmom[i] = momRate * mom[i] + learnRate * grad[i];\n\t\tparam[i] += mom[i];\n\t\t"""""",\n\t\t""classicMomSGDKer""\n\t)\n\n\n@memoize\ndef nesterovMomSGDKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[\n\t\t\t(float_t.ptr, ""param""), (float_t.const.ptr, ""grad""), (float_t.ptr, ""mom""),\n\t\t\t(float_t, ""learnRate""), (float_t, ""momRate"")\n\t\t],\n\t\t""""""\n\t\tfloat m = mom[i];\n\t\tmom[i] = momRate * m + learnRate * grad[i];\n\t\tparam[i] += momRate * momRate * m + (1.0f + momRate) * learnRate * grad[i];\n\t\t"""""",\n\t\t""nesterovMomSGDKer""\n\t)\n\n\n@memoize\ndef rmspropKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[\n\t\t\t(float_t.ptr, ""param""), (float_t.const.ptr, ""grad""), (float_t.ptr, ""ms""),\n\t\t\t(float_t, ""learnRate""), (float_t, ""factor""), (float_t, ""epsilon"")\n\t\t],\n\t\t""""""\n\t\tms[i] = factor * ms[i] + (1.0f - factor) * grad[i] * grad[i];\n\t\tparam[i] += learnRate * grad[i] / (sqrtf(ms[i]) + epsilon);\n\t\t"""""",\n\t\t""rmspropKer""\n\t)\n\n\n@memoize\ndef rmspropGravesKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[\n\t\t\t(float_t.ptr, ""param""), (float_t.const.ptr, ""grad""), (float_t.ptr, ""mg""), (float_t.ptr, ""ms""),\n\t\t\t(float_t.ptr, ""delta""), (float_t, ""learnRate""), (float_t, ""alpha""),\n\t\t\t(float_t, ""momRate""), (float_t, ""epsilon"")\n\t\t],\n\t\t""""""\n\t\tms[i] = alpha * ms[i] + (1.0f - alpha) * grad[i] * grad[i];\n\t\tmg[i] = alpha * mg[i] + (1.0f - alpha) * grad[i];\n\t\tdelta[i] = momRate * delta[i] + learnRate * grad[i] / sqrtf(ms[i] - mg[i] * mg[i] + epsilon);\n\t\tparam[i] += delta[i];\n\t\t"""""",\n\t\t""rmspropGravesKer""\n\t)\n\n\n@memoize\ndef smorms3Ker(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[\n\t\t\t(float_t.ptr, ""param""), (float_t.const.ptr, ""grad""), (float_t.ptr, ""mem""),\n\t\t\t(float_t.ptr, ""mg""), (float_t.ptr, ""ms""), (float_t, ""learnRate""), (float_t, ""epsilon"")\n\t\t],\n\t\t""""""\n\t\tfloat r = 1.0f / (mem[i] + 1.0f);\n\n\t\tfloat mgi = (1.0f - r) * mg[i] + r * grad[i];\n\t\tfloat msi = (1.0f - r) * ms[i] + r * grad[i] * grad[i];\n\t\tfloat x = mgi * mgi / (msi + epsilon);\n\t\n\t\tmem[i] = 1.0f + mem[i] * (1.0f - x), mg[i] = mgi, ms[i] = msi;\n\t\tparam[i] += grad[i] * fminf(learnRate, x) / (sqrtf(msi) + epsilon);\n\t\t"""""",\n\t\t""smorms3Ker""\n\t)\n\n\n@memoize\ndef addKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[\n\t\t\t(float_t.ptr, ""outdata""), (float_t.const.ptr, ""a""), (float_t, ""alpha""),\n\t\t\t(float_t.const.ptr, ""b""), (float_t, ""beta"")\n\t\t],\n\t\t""outdata[i] = alpha * a[i] + beta * b[i]"",\n\t\t""addKer""\n\t)\n\n\n@memoize\ndef mulKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""outdata""), (float_t.const.ptr, ""a""), (float_t.const.ptr, ""b"")],\n\t\t""outdata[i] = a[i] * b[i]"",\n\t\t""mulKer""\n\t)\n\n\n@memoize\ndef linearKer(dtype):\n\tassert dtype == np.float32\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""outdata""), (float_t.const.ptr, ""indata""), (float_t, ""a""), (float_t, ""b"")],\n\t\t""outdata[i] = a * indata[i] + b"",\n\t\t""linearKer""\n\t)\n\n\nrbmKer = ElementwiseKernel(\n\t[(float_t.ptr, ""outdata""), (float_t.const.ptr, ""indata""), (float_t.const.ptr, ""uni"")],\n\t""float act = 1.0f / (1.0f + expf(-indata[i]));""\n\t""outdata[i] = (float)(uni[i] < act)"",\n\t""rbmKer""\n)\n\nweightDecayKer = ElementwiseKernel(\n\t[(float_t.ptr, ""grad""), (float_t.const.ptr, ""param""), (float_t, ""rate"")],\n\t""grad[i] -= rate * param[i]"",\n\t""weightDecayKer""\n)\n\n\nabsKer = ElementwiseKernel(\n\t[(float_t.ptr, ""outdata""), (float_t.const.ptr, ""indata"")],\n\t""outdata[i] = fabsf(indata[i])"",\n\t""absKer""\n)\n\nl1penaltyKer = ElementwiseKernel(\n\t[(float_t.ptr, ""outgrad""), (float_t.const.ptr, ""ingrad""), (float_t.ptr, ""data""), (float_t, ""a"")],\n\t""outgrad[i] = ingrad[i] - a * ((0.0f <= data[i]) - (data[i] < 0.0f))"",\n\t""l1penaltyKer""\n)\n\nl1gradKer = ElementwiseKernel(\n\t[(float_t.ptr, ""grad""), (float_t.const.ptr, ""pred""), (float_t.const.ptr, ""target""), (float_t, ""norm"")],\n\t""grad[i] = (pred[i] > target[i] ? -norm : norm)"",\n\t""l1gradKer""\n)\n'"
CPU/Kernels/Pad.py,10,"b'import numpy as np\n\nfrom PuzzleLib.Compiler.Codegen.Types import void_t, int32_t, float_t\n\nfrom PuzzleLib.CPU.SourceModule import SourceModule\nfrom PuzzleLib.CPU.CPUArray import CPUArray\n\n\nsrc = """"""\n\ninline static void map1d(int32_t b, int32_t c, int32_t maps, int32_t insize, int32_t outsize,\n\t\t\t\t\t\t int32_t index, int32_t lpad, int32_t *inindex, int32_t *outindex)\n{\n\tint32_t inoffset = (c + b * maps) * insize;\n\tint32_t outoffset = (c + b * maps) * outsize;\n\n\tint32_t instart = (lpad < 0) ? -lpad : 0;\n\tint32_t outstart = (lpad > 0) ? lpad : 0;\n\n\tint32_t x = abs(index - lpad) - abs(index - (insize + lpad - 1)) - index + 2 * lpad +\n\t\t\t\tinsize - 1 - outstart + instart;\n\n\t*inindex = inoffset + x, *outindex = outoffset + index;\n}\n\nstatic void reflectpad1d(float * __restrict outdata, const float * __restrict indata,\n\t\t\t\t\t\t int32_t batchsize, int32_t maps, int32_t insize, int32_t lpad, int32_t rpad)\n{\n\tint outsize = insize + lpad + rpad;\n\n\tfor (int32_t b = 0; b < batchsize; b++)\n\t\tfor (int32_t c = 0; c < maps; c++)\n\t\t\tfor (int32_t index = 0; index < outsize; index++)\n\t\t\t{\n\t\t\t\tint32_t inindex = 0, outindex = 0;\n\t\t\t\tmap1d(b, c, maps, insize, outsize, index, lpad, &inindex, &outindex);\n\n\t\t\t\toutdata[outindex] = indata[inindex];\n\t\t\t}\n}\n\n\ninline static void map2d(int32_t b, int32_t c, int32_t maps, int32_t inh, int32_t inw, int32_t outh, int32_t outw,\n\t\t\t\t\t\t int32_t index, int32_t upad, int32_t lpad, int32_t *inindex, int32_t *outindex)\n{\n\tint32_t inoffset = (c + b * maps) * inh * inw;\n\tint32_t outoffset = (c + b * maps) * outh * outw;\n\n\tint32_t outx = index % outw, outy = index / outw;\n\n\tint32_t instartx = (lpad < 0) ? -lpad : 0, outstartx = (lpad > 0) ? lpad : 0;\n\tint32_t instarty = (upad < 0) ? -upad : 0, outstarty = (upad > 0) ? upad : 0;\n\n\tint32_t inx = abs(outx - lpad) - abs(outx - (inw + lpad - 1)) - outx + 2 * lpad + inw - 1 - outstartx + instartx;\n\tint32_t iny = abs(outy - upad) - abs(outy - (inh + upad - 1)) - outy + 2 * upad + inh - 1 - outstarty + instarty;\n\n\t*inindex = inoffset + iny * inw + inx;\n\t*outindex = outoffset + outy * outw + outx;\n}\n\nstatic void reflectpad2d(float * __restrict outdata, const float * __restrict indata,\n\t\t\t\t\t\t int32_t batchsize, int32_t maps, int32_t inh, int32_t inw, int32_t upad, int32_t bpad,\n\t\t\t\t\t\t int32_t lpad, int32_t rpad)\n{\n\tint outh = inh + upad + bpad, outw = inw + lpad + rpad;\n\n\tfor (int32_t b = 0; b < batchsize; b++)\n\t\tfor (int32_t c = 0; c < maps; c++)\n\t\t\tfor (int32_t index = 0; index < outh * outw; index++)\n\t\t\t{\n\t\t\t\tint32_t inindex = 0, outindex = 0;\n\t\t\t\tmap2d(b, c, maps, inh, inw, outh, outw, index, upad, lpad, &inindex, &outindex);\n\n\t\t\t\toutdata[outindex] = indata[inindex];\n\t\t\t}\n}\n\n""""""\n\n\nmod = SourceModule(src, functions=[\n\t(""reflectpad1d"", void_t, [\n\t\t(float_t.ptr.restrict, ""outdata""), (float_t.const.ptr.restrict, ""indata""),\n\t\t(int32_t, ""batchsize""), (int32_t, ""maps""), (int32_t, ""insize""), (int32_t, ""lpad""), (int32_t, ""rpad"")\n\t]),\n\t(""reflectpad2d"", void_t, [\n\t\t(float_t.ptr.restrict, ""outdata""), (float_t.const.ptr.restrict, ""indata""),\n\t\t(int32_t, ""batchsize""), (int32_t, ""maps""), (int32_t, ""inh""), (int32_t, ""inw""),\n\t\t(int32_t, ""upad""), (int32_t, ""bpad""), (int32_t, ""lpad""), (int32_t, ""rpad"")\n\t])\n])\n\n\ndef reflectpad1d(data, pad):\n\tassert data.dtype == np.float32 and data.ndim == 3\n\n\tbatchsize, maps, insize = data.shape\n\tlpad, rpad = pad\n\n\tassert insize >= max(lpad, rpad) + 1\n\toutdata = CPUArray.empty((batchsize, maps, insize + lpad + rpad), dtype=data.dtype)\n\n\tmod.reflectpad1d(outdata.data, data.data, batchsize, maps, insize, lpad, rpad)\n\treturn outdata\n\n\ndef reflectpad2d(data, pad):\n\tassert data.dtype == np.float32 and data.ndim == 4\n\n\tbatchsize, maps, inh, inw = data.shape\n\tupad, bpad, lpad, rpad = pad\n\n\tassert inh >= max(upad, bpad) + 1 and inw >= max(lpad, rpad) + 1\n\toutdata = CPUArray.empty((batchsize, maps, inh + upad + bpad, inw + lpad + rpad), dtype=data.dtype)\n\n\tmod.reflectpad2d(outdata.data, data.data, batchsize, maps, inh, inw, upad, bpad, lpad, rpad)\n\treturn outdata\n\n\ndef unittest():\n\treflectpad1dTest()\n\treflectpad2dTest()\n\n\ndef reflectpad1dTest():\n\tbatchsize, maps, insize = 4, 8, 48\n\tlpad, rpad = 2, 3\n\n\tdata = CPUArray.toDevice(np.random.randn(batchsize, maps, insize).astype(np.float32))\n\toutdata = reflectpad1d(data, pad=(lpad, rpad))\n\n\thostData, hostOutData = data.get(), outdata.get()\n\n\tassert np.allclose(hostOutData[:, :, lpad:insize + lpad], hostData)\n\tassert np.allclose(hostOutData[:, :, :lpad][:, :, ::-1], hostData[:, :, 1:lpad+1])\n\tassert np.allclose(hostOutData[:, :, insize + lpad:][:, :, ::-1], hostData[:, :, insize - 1 - rpad:insize - 1])\n\n\ndef reflectpad2dTest():\n\tbatchsize, maps, inh, inw = 4, 8, 12, 15\n\tupad, bpad, lpad, rpad = 2, 3, 2, 3\n\n\tdata = CPUArray.toDevice(np.random.randn(batchsize, maps, inh, inw).astype(np.float32))\n\toutdata = reflectpad2d(data, pad=(upad, bpad, lpad, rpad))\n\n\thostData, hostOutData = data.get(), outdata.get()\n\n\tassert np.allclose(hostOutData[:, :, upad:inh + upad, lpad:inw + lpad], hostData)\n\tassert np.allclose(hostOutData[:, :, :upad, :lpad][:, :, ::-1, ::-1], hostData[:, :, 1:upad + 1, 1:lpad + 1])\n\tassert np.allclose(\n\t\thostOutData[:, :, inh + upad:, inw + lpad:][:, :, ::-1, ::-1],\n\t\thostData[:, :, inh - 1 - bpad:inh - 1, inw - 1 - rpad:inw - 1]\n\t)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
CPU/Kernels/Upsample2D.py,3,"b'import numpy as np\n\nfrom PuzzleLib.Compiler.Codegen.Types import void_t, int32_t, float_t\n\nfrom PuzzleLib.CPU.SourceModule import SourceModule\nfrom PuzzleLib.CPU.CPUArray import CPUArray\n\n\nupsample2dNearestTmpl = """"""\n\nstatic void upsample2dNearest(float * __restrict outdata, const float * __restrict indata,\n\t\t\t\t\t\t\t  int32_t batchsize, int32_t maps, int32_t inh, int32_t inw, int32_t hscale, int32_t wscale)\n{\n\tint32_t outh = inh * hscale, outw = inw * wscale;\n\n\tfor (int32_t z = 0; z < batchsize * maps; z++)\n\t\tfor (int32_t y = 0; y < inh; y++)\n\t\t\tfor (int32_t x = 0; x < inw; x++)\n\t\t\t\tfor (int32_t i = 0; i < hscale; i++)\n\t\t\t\t\tfor (int32_t j = 0; j < wscale; j++)\n\t\t\t\t\t{\n\t\t\t\t\t\tint32_t outidx = z * outh * outw + (y * hscale + i) * outw + (x * wscale + j);\n\t\t\t\t\t\toutdata[outidx] = indata[z * inh * inw + y * inw + x];\n\t\t\t\t\t}\n}\n\n""""""\n\n\nnearestMod = SourceModule(upsample2dNearestTmpl, functions=[\n\t(""upsample2dNearest"", void_t, [\n\t\t(float_t.ptr.restrict, ""outdata""), (float_t.const.ptr.restrict, ""indata""), (int32_t, ""batchsize""),\n\t\t(int32_t, ""maps""), (int32_t, ""inh""), (int32_t, ""inw""), (int32_t, ""hscale""), (int32_t, ""wscale"")\n\t], True)\n])\n\n\ndef upsample2d(data, scale, mode=""nearest""):\n\tbatchsize, maps, inh, inw = data.shape\n\thscale, wscale = (scale, scale) if isinstance(scale, int) else scale\n\n\touth, outw = hscale * inh, wscale * inw\n\toutdata = CPUArray.empty((batchsize, maps, outh, outw), dtype=data.dtype)\n\n\tif mode == ""nearest"":\n\t\tnearestMod.upsample2dNearest(outdata.data, data.data, batchsize, maps, inh, inw, hscale, wscale)\n\n\telse:\n\t\traise ValueError(""Unsupported upsampling mode"")\n\n\treturn outdata\n\n\ndef unittest():\n\tbatchsize, maps, inh, inw = 3, 2, 16, 15\n\tscale = 2\n\n\tdata = CPUArray.toDevice(np.random.uniform(low=-1.0, high=1.0, size=(batchsize, maps, inh, inw)).astype(np.float32))\n\toutdata = upsample2d(data, scale, mode=""nearest"")\n\n\thostData = data.get()\n\thostOutData = np.empty(outdata.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor y in range(inh):\n\t\t\t\tfor x in range(inw):\n\t\t\t\t\thostOutData[b, c, y * scale:(y + 1) * scale, x * scale:(x + 1) * scale] = hostData[b, c, y, x]\n\n\tassert np.allclose(hostOutData, outdata.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
CPU/Wrappers/NumpyBlas.py,14,"b'import numpy as np\n\nfrom PuzzleLib.CPU.CPUArray import CPUArray\nfrom PuzzleLib.CPU.Kernels import ElementWise\n\n\ndef sumOnMatrix(A, out=None, cols=True, alpha=1.0, beta=0.0):\n\tassert A.ndim == 2\n\tassert A.flags.c_contiguous\n\tassert A.dtype == np.float32\n\n\tif out is None:\n\t\tout = CPUArray.empty((A.shape[1], ) if cols else (A.shape[0], ), dtype=np.float32)\n\n\tif alpha == 1.0 and beta == 0.0:\n\t\tnp.sum(A.data, axis=0 if cols else 1, out=out.data)\n\n\telse:\n\t\ts = np.sum(A.data, axis=0 if cols else 1)\n\t\tnp.add(beta * out.data, alpha * s, out=out.data)\n\n\treturn out\n\n\ndef toVectorAddVector(y, x, alpha=1.0):\n\tassert x.ndim == 1\n\tassert x.shape == y.shape\n\tassert y.flags.forc and x.flags.forc\n\n\tassert x.dtype == y.dtype\n\tassert x.dtype == np.float32\n\n\tElementWise.toVectorAddVectorKer(y.dtype)(y, x, alpha)\n\n\ndef addVectorToVector(x, y, out=None, alpha=1.0, beta=1.0):\n\tassert x.ndim == 1\n\tassert x.flags.forc and y.flags.forc\n\tassert x.shape == y.shape\n\tassert x.dtype == y.dtype and x.dtype == np.float32\n\n\tif out is None:\n\t\tout = CPUArray.empty(x.shape, dtype=np.float32)\n\n\tElementWise.addVectorToVectorKer(out, x, y, alpha, beta)\n\treturn out\n\n\ndef vectorL1Norm(x):\n\tassert x.ndim == 1\n\tassert x.flags.forc\n\tassert x.dtype == np.float32\n\n\treturn np.linalg.norm(x.data, ord=1)\n\n\ndef dot(x, y):\n\tassert x.ndim == 1\n\tassert x.shape == y.shape\n\tassert x.flags.forc and y.flags.forc\n\tassert x.dtype == y.dtype and y.dtype == np.float32\n\n\treturn np.vdot(x.data, y.data)\n\n\ndef mulMatrixOnMatrix(A, B, out=None, transpA=False, transpB=False, alpha=1.0, beta=0.0):\n\tassert not (transpA and transpB)\n\tassert A.ndim == 2 and B.ndim == 2\n\n\tassert alpha == 1.0 and beta == 0.0\n\n\tif transpA:\n\t\tassert A.shape[0] == B.shape[0]\n\t\tshape = (A.shape[1], B.shape[1])\n\n\telif transpB:\n\t\tassert A.shape[1] == B.shape[1]\n\t\tshape = (A.shape[0], B.shape[0])\n\n\telse:\n\t\tassert A.shape[1] == B.shape[0]\n\t\tshape = (A.shape[0], B.shape[1])\n\n\tA = A.data.T if transpA else A.data\n\tB = B.data.T if transpB else B.data\n\n\tif out is None:\n\t\tout = CPUArray.empty(shape, dtype=np.float32)\n\n\tnp.dot(A, B, out=out.data)\n\treturn out\n'"
CPU/Wrappers/NumpyDnn.py,24,"b'from enum import Enum\nimport numpy as np\n\nfrom PuzzleLib.CPU.CPUArray import CPUArray\n\n\nclass PoolMode(Enum):\n\tmax = 0\n\tavgWithPad = 1\n\n\ndef repeatValue(val, ntimes):\n\tif isinstance(val, int):\n\t\treturn (val, ) * ntimes\n\n\telif isinstance(val, (list, tuple)):\n\t\treturn tuple(val)\n\n\telse:\n\t\traise NotImplementedError(val.__class__.__name__)\n\n\ndef outshape(inshape, size, stride, pad):\n\tinh, inw = inshape\n\n\thsize, wsize = size\n\thstride, wstride = stride\n\thpad, wpad = pad\n\n\touth = (inh + 2 * hpad - hsize) // hstride + 1\n\toutw = (inw + 2 * wpad - wsize) // wstride + 1\n\n\treturn outh, outw\n\n\ndef im2col(data, size, stride, pad):\n\tassert data.ndim == 4\n\n\thsize, wsize = size\n\thstride, wstride = stride\n\thpad, wpad = pad\n\n\tbatchsize, maps, inh, inw = data.shape\n\touth, outw = outshape((inh, inw), size, stride, pad)\n\n\tdata = np.pad(data, ((0, 0), (0, 0), (hpad, hpad), (wpad, wpad)), mode=""constant"", constant_values=0)\n\n\tstrides = (\n\t\tdata.strides[0], hstride * data.strides[2], wstride * data.strides[3],\n\t\tdata.strides[1], data.strides[2], data.strides[3]\n\t)\n\n\tcoldata = np.lib.stride_tricks.as_strided(data, shape=(batchsize, outh, outw, maps, hsize, wsize), strides=strides)\n\tcoldata = coldata.reshape(batchsize * outh * outw, maps * hsize * wsize)\n\n\treturn coldata\n\n\ndef col2im(data, maps, shape):\n\tassert data.ndim == 2\n\th, w = shape\n\n\tdata = data.reshape(-1, h, w, maps)\n\tdata = np.moveaxis(data, 3, 1)\n\n\treturn np.ascontiguousarray(data)\n\n\ndef linear(data, W, bias):\n\toutdata = np.dot(data, W)\n\n\tif bias is not None:\n\t\toutdata += bias\n\n\treturn outdata\n\n\ndef conv2d(data, W, bias=None, stride=1, pad=0):\n\tassert data.ndim == 4 and W.ndim == 4\n\n\tbatchsize, _, inh, inw = data.shape\n\tstride, pad = repeatValue(stride, 2), repeatValue(pad, 2)\n\n\toutmaps, _, hsize, wsize = W.shape\n\touth, outw = outshape((inh, inw), (hsize, wsize), stride, pad)\n\n\tcoldata = im2col(data.data, W.shape[2:], stride, pad)\n\tW = W.data.reshape(W.shape[0], -1).T\n\n\tbias = bias.data.reshape(1, bias.shape[1]) if bias is not None else None\n\toutdata = linear(coldata, W, bias)\n\n\toutdata = col2im(outdata, outmaps, (outh, outw))\n\treturn CPUArray(outdata.shape, outdata.dtype, data=outdata, acquire=True)\n\n\ndef pool2d(data, size=2, stride=2, pad=0, mode=PoolMode.max):\n\tassert data.ndim == 4\n\tonRow = np.max if mode == PoolMode.max else np.mean\n\n\tbatchsize, maps, inh, inw = data.shape\n\tsize, stride, pad = repeatValue(size, 2), repeatValue(stride, 2), repeatValue(pad, 2)\n\n\touth, outw = outshape((inh, inw), size, stride, pad)\n\n\tcoldata = im2col(data.data.reshape(batchsize * maps, 1, inh, inw), size, stride, pad)\n\toutdata = onRow(coldata, axis=1, keepdims=True).reshape((batchsize, maps, outh, outw))\n\n\treturn CPUArray(outdata.shape, outdata.dtype, data=outdata, acquire=True)\n\n\ndef batchNorm2d(data, scale, bias, mean, var, epsilon=1e-5, test=False, out=None):\n\tassert data.ndim == scale.ndim and scale.ndim == bias.ndim and bias.ndim == mean.ndim and mean.ndim == var.ndim\n\tassert test\n\n\tscale = scale.data / np.sqrt(var.data + epsilon)\n\toutdata = scale * (data.data - mean.data) + bias.data\n\n\treturn CPUArray(outdata.shape, outdata.dtype, data=outdata, acquire=True)\n\n\ndef unittest():\n\tconv2dTest()\n\tmaxpool2dTest()\n\tbatchNorm2dTest()\n\n\ndef conv2dTest():\n\tbatchsize, inmaps, h, w = 1, 2, 6, 6\n\tfsize, outmaps = 2, 4\n\n\tdata = CPUArray.toDevice(np.random.randn(batchsize, inmaps, h, w).astype(np.float32))\n\n\tW = CPUArray.toDevice(np.random.randn(outmaps, inmaps, fsize, fsize).astype(np.float32))\n\tbias = CPUArray.toDevice(np.random.randn(1, outmaps, 1, 1).astype(np.float32))\n\n\toutdata = conv2d(data, W, bias)\n\n\thostData, hostW, hostBias = data.get(), W.get(), bias.get()\n\thostOutData = np.empty(outdata.shape, dtype=np.float32)\n\n\tfor c in range(outmaps):\n\t\thostOutData[:, c, :, :] = hostBias[0, c, 0, 0]\n\n\tfor b in range(batchsize):\n\t\tfor oc in range(outmaps):\n\t\t\tfor ic in range(inmaps):\n\t\t\t\tfor y in range(outdata.shape[2]):\n\t\t\t\t\tfor x in range(outdata.shape[3]):\n\t\t\t\t\t\tfor dy in range(fsize):\n\t\t\t\t\t\t\tfor dx in range(fsize):\n\t\t\t\t\t\t\t\thostOutData[b, oc, y, x] += hostData[b, ic, y + dy, x + dx] * hostW[oc, ic, dy, dx]\n\n\tassert np.allclose(hostOutData, outdata.get())\n\n\ndef maxpool2dTest():\n\tbatchsize, maps, h, w = 1, 1, 8, 8\n\tdata = CPUArray.toDevice(np.random.randn(batchsize, maps, h, w).astype(np.float32))\n\n\toutdata = pool2d(data)\n\n\tdef maxDownSample2d(dat, factor):\n\t\ttrimrows = dat.shape[0] // factor * factor\n\t\ttrimcols = dat.shape[1] // factor * factor\n\n\t\tmaxSoFar = None\n\t\tfirst = True\n\n\t\tfor coff in range(factor):\n\t\t\tfor roff in range(factor):\n\t\t\t\thopped = dat[roff:trimrows:factor, coff:trimcols:factor]\n\t\t\t\tif first:\n\t\t\t\t\tmaxSoFar = hopped\n\t\t\t\t\tfirst = False\n\t\t\t\telse:\n\t\t\t\t\tmaxSoFar = np.maximum(maxSoFar, hopped)\n\n\t\treturn maxSoFar\n\n\thostOutData = maxDownSample2d(data.get()[0, 0], 2)\n\tassert np.allclose(hostOutData, outdata.get())\n\n\ndef batchNorm2dTest():\n\tbatchsize, maps, h, w = 4, 5, 3, 2\n\n\tdata = CPUArray.toDevice(np.random.randn(batchsize, maps, h, w).astype(np.float32))\n\thostData = data.get()\n\n\tscale = CPUArray.toDevice(np.random.randn(1, maps, 1, 1).astype(np.float32))\n\tbias = CPUArray.toDevice(np.random.randn(1, maps, 1, 1).astype(np.float32))\n\tmean = CPUArray.toDevice(np.random.randn(1, maps, 1, 1).astype(np.float32))\n\tvar = CPUArray.toDevice(\n\t\t(np.ones((1, maps, 1, 1)).astype(np.float32) + np.random.randn(1, maps, 1, 1).astype(np.float32))**2\n\t)\n\n\toutdata = batchNorm2d(data, scale, bias, mean, var, test=True)\n\n\thostScale, hostBias, hostMean, hostVar = scale.get(), bias.get(), mean.get(), var.get()\n\thostNormData = np.empty(hostData.shape, dtype=np.float32)\n\thostOutData = np.empty(hostData.shape, dtype=np.float32)\n\n\tfor c in range(maps):\n\t\thostNormData[:, c, :, :] = (hostData[:, c, :, :] - hostMean[0, c, 0, 0]) / np.sqrt(hostVar[0, c, 0, 0] + 1e-5)\n\t\thostOutData[:, c, :, :] = hostNormData[:, c, :, :] * hostScale[0, c, 0, 0] + hostBias[0, c, 0, 0]\n\n\tassert np.allclose(hostOutData, outdata.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Compiler/Codegen/Python.py,0,"b'import os\nfrom string import Template\n\nfrom PuzzleLib.Compiler.Codegen.Types import void_t, float_t, double_t, ptrdiff_t, Py_ssize_t\nfrom PuzzleLib.Compiler.Codegen.Types import schar_t, short_t, int_t, llong_t, int8_t, int16_t, int32_t, int64_t\nfrom PuzzleLib.Compiler.Codegen.Types import uchar_t, ushort_t, uint_t, ullong_t, uint8_t, uint16_t, uint32_t, uint64_t\n\nfrom PuzzleLib.Compiler.Toolchain import guessToolchain\nfrom PuzzleLib.Compiler.JIT import extensionFromString, stdCachePath\n\n\nfuncTmpl = Template(""""""\nstatic PyObject *${modname}_$funcname(PyObject *self, PyObject *args)\n{\n\t(void)self;\n\t$parsercode\n\t$valinit\n\n$gilstart\n\t$value$funcname($callparams);\n$gilend\n\n\t$retval;\n}\n"""""")\n\n\nparserTmpl = Template(""""""\n\t$header\n\n\tif (!PyArg_ParseTuple(args, ""$parsestr"", $parseparams))\n\t\treturn NULL;\n\n\t$footer\n"""""")\n\n\nmodTmpl = Template(""""""\n$bindings\n\nstatic PyMethodDef methods[] = {\n\t$methods,\n\t{NULL, NULL, 0, NULL}\n};\n\n\nstatic PyModuleDef mod = {\n\tPyModuleDef_HEAD_INIT,\n\t.m_name = ""$modname"",\n\t.m_methods = methods\n};\n\n\nPyMODINIT_FUNC PyInit_$modname(void)\n{\n\t$modinit\n\treturn PyModule_Create(&mod);\n}\n"""""")\n\n\nc2py = {\n\tschar_t: ""b"",\n\tshort_t: ""h"",\n\tint_t: ""i"",\n\tllong_t: ""L"",\n\n\tuchar_t: ""B"",\n\tushort_t: ""H"",\n\tuint_t: ""I"",\n\tullong_t: ""K"",\n\n\tfloat_t: ""f"",\n\tdouble_t: ""d"",\n\tPy_ssize_t: ""n""\n}\n\n\nc2native = {\n\tint8_t: schar_t,\n\tint16_t: short_t,\n\tint32_t: int_t,\n\tint64_t: llong_t,\n\n\tuint8_t: uchar_t,\n\tuint16_t: ushort_t,\n\tuint32_t: uint_t,\n\tuint64_t: ullong_t,\n\n\tfloat_t: float_t,\n\tdouble_t: double_t,\n\tptrdiff_t: Py_ssize_t\n}\n\n\nclass ParamParser:\n\tdef __init__(self, totalargs):\n\t\tself.header, self.footer = [], []\n\t\tself.parsestr, self.parseparams = [], []\n\t\tself.callparams = []\n\n\t\tself.argindex = 0\n\t\tself.totalargs = totalargs\n\n\ndef defaultConverter(T, name, parser):\n\tobjname = ""%sObj"" % name\n\tnativetype = c2native[T.aliasBase]\n\n\tparser.header.append(""%s;"" % (nativetype.typegen(asDecl=True) % objname))\n\n\tparser.parsestr.append(c2py[nativetype])\n\tparser.parseparams.append(""&%s"" % objname)\n\n\tparser.footer.append(""%s = %s;"" % (T.typegen(asDecl=True) % name, objname))\n\tparser.callparams.append(name)\n\n\ndef generatePythonBinding(modname, functions, converter=defaultConverter, finalizer=None, modinit=""""):\n\tbindings = (generateFunctionParamParser(modname, func, converter, finalizer) for func in functions)\n\tmethods = (\n\t\t""{\\""%s\\"", %s_%s, %s, NULL}"" % (\n\t\t\tfuncname, modname, funcname, ""METH_VARARGS"" if len(arguments) > 0 else ""METH_NOARGS""\n\t\t) for funcname, _, arguments, *_ in functions\n\t)\n\n\tmodule = modTmpl.substitute(\n\t\tmodname=modname, bindings=""\\n"".join(bindings), methods="",\\n\\t"".join(methods), modinit=modinit\n\t)\n\treturn module\n\n\ndef generateFunctionParamParser(modname, func, converter=defaultConverter, finalizer=None):\n\tfuncname, returntype, arguments, *args = func\n\tparser = ParamParser(totalargs=len(arguments))\n\n\tgilstart, gilend = (""Py_BEGIN_ALLOW_THREADS"", ""Py_END_ALLOW_THREADS"") if len(args) > 0 and args[0] else ("""", """")\n\n\tif len(arguments) == 0:\n\t\tparsercode = ""(void)args;\\n""\n\n\telse:\n\t\tfor i, (T, name) in enumerate(arguments):\n\t\t\tparser.argindex = i\n\t\t\tconverter(T, name, parser)\n\n\t\tif finalizer is not None:\n\t\t\tfinalizer(funcname, parser)\n\n\t\tparsercode = parserTmpl.substitute(\n\t\t\theader=""\\n\\t"".join(parser.header), footer=""\\n\\t"".join(parser.footer),\n\t\t\tparsestr="""".join(parser.parsestr), parseparams="", "".join(parser.parseparams)\n\t\t)\n\n\tif returntype is void_t:\n\t\tvalinit, value, retval = """", """", ""Py_RETURN_NONE""\n\telse:\n\t\tnativetype = c2native[returntype.aliasBase]\n\n\t\tvalinit, value = ""%s retval;"" % nativetype.typegen(asDecl=False), ""retval = ""\n\t\tretval = ""return Py_BuildValue(\\""%s\\"", retval)"" % c2py[nativetype]\n\n\tfunc = funcTmpl.substitute(\n\t\tmodname=modname, funcname=funcname, parsercode=parsercode, callparams="", "".join(parser.callparams),\n\t\tvalinit=valinit, value=value, retval=retval, gilstart=gilstart, gilend=gilend\n\t)\n\n\treturn func\n\n\ndef unittest():\n\tfunctions = """"""\n\nstatic int32_t test(int32_t a, int32_t b)\n{\n\treturn a + b;\n}\n\n\nstatic void test2(void)\n{\n\n}\n\n""""""\n\n\tbinding = generatePythonBinding(""module"", [\n\t\t(""test"", int32_t, [(int32_t, ""a""), (int32_t, ""b"")], True),\n\t\t(""test2"", void_t, [])\n\t])\n\n\tsource = ""#include <Python.h>\\n%s%s"" % (functions, binding)\n\tmod = extensionFromString(\n\t\tguessToolchain(verbose=2), ""module"", source, cachepath=os.path.join(stdCachePath, ""tests""), recompile=True\n\t)\n\n\tassert mod.test(2, 2) == 4\n\tassert mod.test2() is None\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Compiler/Codegen/Types.py,0,"b'class Type:\n\tprecedence = None\n\tcache = None\n\n\n\t@property\n\tdef ptr(self):\n\t\treturn getCached(PointerType, self)\n\n\n\tdef __getitem__(self, item):\n\t\treturn getCached(ArrayType, self, item)\n\n\n\tdef func(self, *argtypes):\n\t\treturn getCached(FunctionType, self, argtypes)\n\n\n\t@property\n\tdef unqual(self):\n\t\tT = self\n\n\t\twhile isinstance(T, QualifierType):\n\t\t\tT = T.basetype\n\n\t\treturn T\n\n\n\t@property\n\tdef const(self):\n\t\treturn getCached(ConstType, self)\n\n\n\t@property\n\tdef restrict(self):\n\t\treturn getCached(RestrictType, self)\n\n\n\t@property\n\tdef volatile(self):\n\t\treturn getCached(VolatileType, self)\n\n\n\tdef typegen(self, asDecl):\n\t\traise NotImplementedError()\n\n\n\t@property\n\tdef initializer(self):\n\t\traise NotImplementedError()\n\n\n\t@property\n\tdef aliasBase(self):\n\t\treturn self\n\n\n\tdef basedWith(self, T):\n\t\treturn T\n\n\n\tdef __str__(self):\n\t\treturn self.typegen(asDecl=False)\n\n\n\tdef __repr__(self):\n\t\treturn self.typegen(asDecl=False)\n\n\nclass QualifierType(Type):\n\tprecedence = 0\n\n\n\tdef __init__(self, basetype):\n\t\tself.basetype = basetype\n\n\n\tdef typegen(self, asDecl):\n\t\tT = self.qualifier + "" %s"" if asDecl else self.qualifier\n\t\treturn self.basetype.typegen(asDecl=True) % T\n\n\n\t@property\n\tdef initializer(self):\n\t\treturn self.basetype.initializer\n\n\n\t@property\n\tdef aliasBase(self):\n\t\treturn self.basetype.aliasBase\n\n\n\t@property\n\tdef qualifier(self):\n\t\traise NotImplementedError()\n\n\nclass ConstType(QualifierType):\n\tcache = {}\n\n\n\t@property\n\tdef qualifier(self):\n\t\treturn ""const""\n\n\n\tdef basedWith(self, T):\n\t\treturn self.basetype.basedWith(T).const\n\n\nclass VolatileType(QualifierType):\n\tcache = {}\n\n\n\t@property\n\tdef qualifier(self):\n\t\treturn ""volatile""\n\n\n\tdef basedWith(self, T):\n\t\treturn self.basetype.basedWith(T).volatile\n\n\nclass RestrictType(QualifierType):\n\tcache = {}\n\n\n\t@property\n\tdef qualifier(self):\n\t\treturn ""__restrict""\n\n\n\tdef basedWith(self, T):\n\t\treturn self.basetype.basedWith(T).restrict\n\n\nclass PointerType(Type):\n\tprecedence = 1\n\tcache = {}\n\n\n\tdef __init__(self, basetype):\n\t\tself.basetype = basetype\n\n\n\tdef typegen(self, asDecl):\n\t\tT = ""*%s"" if asDecl else ""*""\n\n\t\tT = ""(%s)"" % T if self.precedence < self.basetype.precedence else T\n\t\treturn self.basetype.typegen(asDecl=True) % T\n\n\n\t@property\n\tdef initializer(self):\n\t\treturn ""NULL""\n\n\n\t@property\n\tdef aliasBase(self):\n\t\treturn self.basetype.aliasBase.ptr\n\n\n\tdef basedWith(self, T):\n\t\treturn self.basetype.basedWith(T).ptr\n\n\nclass ArrayType(Type):\n\tprecedence = 2\n\tcache = {}\n\n\n\tdef __init__(self, elemtype, size):\n\t\tself.elemtype = elemtype\n\t\tself.size = size\n\n\n\tdef typegen(self, asDecl):\n\t\tT = (""%s"" if asDecl else """") + ""[%s]"" % ("""" if self.size is None else self.size)\n\n\t\tT = ""(%s)"" % T if self.precedence < self.elemtype.precedence else T\n\t\treturn self.elemtype.typegen(asDecl=True) % T\n\n\n\t@property\n\tdef initializer(self):\n\t\treturn ""{%s}"" % self.elemtype.initializer\n\n\n\t@property\n\tdef aliasBase(self):\n\t\treturn self.elemtype.aliasBase[self.size]\n\n\n\tdef basedWith(self, T):\n\t\treturn self.elemtype.basedWith(T)[self.size]\n\n\nclass FunctionType(Type):\n\tprecedence = 1\n\tcache = {}\n\n\n\tdef __init__(self, returntype, argtypes):\n\t\tself.returntype = returntype\n\t\tself.argtypes = argtypes\n\n\n\tdef typegen(self, asDecl):\n\t\targtypes = ""(%s)"" % "", "".join(argtype.typegen(asDecl=False) for argtype in self.argtypes)\n\t\tT = (""(*%s)"" if asDecl else ""(*)"") + argtypes\n\n\t\tT = ""(%s)"" % T if self.precedence < self.returntype.precedence else T\n\t\treturn self.returntype.typegen(asDecl=True) % T\n\n\n\t@property\n\tdef initializer(self):\n\t\treturn ""NULL""\n\n\n\t@property\n\tdef aliasBase(self):\n\t\treturn self.returntype.aliasBase(argtype.aliasBase for argtype in self.argtypes)\n\n\n\tdef basedWith(self, T):\n\t\treturn self.returntype.basedWith(T).func(*self.argtypes)\n\n\nclass VoidType(Type):\n\tprecedence = 0\n\n\n\tdef typegen(self, asDecl):\n\t\treturn ""void"" + ("" %s"" if asDecl else """")\n\n\n\t@property\n\tdef initializer(self):\n\t\treturn None\n\n\nclass NumberType(Type):\n\tprecedence = 0\n\n\n\tdef __init__(self, name, initvalue):\n\t\tself.name = name\n\t\tself.initvalue = initvalue\n\n\n\tdef typegen(self, asDecl):\n\t\treturn self.name + ("" %s"" if asDecl else """")\n\n\n\t@property\n\tdef initializer(self):\n\t\treturn self.initvalue\n\n\ndef getCached(cls, *args):\n\tT = cls.cache.get(args, None)\n\n\tif T is None:\n\t\tT = cls(*args)\n\t\tcls.cache[args] = T\n\n\treturn T\n\n\nvoid_t = VoidType()\nbool_t = NumberType(""bool"", initvalue=""false"")\n\nchar_t = NumberType(""char"", initvalue=""\'\\\\0\'"")\nwchar_t = NumberType(""wchar_t"", initvalue=""L\'\\\\0\'"")\n\nschar_t = NumberType(""signed char"", initvalue=""0"")\nuchar_t = NumberType(""unsigned char"", initvalue=""0U"")\n\nshort_t = NumberType(""short"", initvalue=""0"")\nshort2_t = NumberType(""short2"", initvalue=""short2()"")\nushort_t = NumberType(""unsigned short"", initvalue=""0U"")\nushort2_t = NumberType(""ushort2"", initvalue=""ushort2()"")\n\nint_t = NumberType(""int"", initvalue=""0"")\nuint_t = NumberType(""unsigned int"", initvalue=""0U"")\n\nlong_t = NumberType(""long"", initvalue=""0L"")\nulong_t = NumberType(""unsigned long"", initvalue=""0UL"")\n\nllong_t = NumberType(""long long"", initvalue=""0LL"")\nullong_t = NumberType(""unsigned long long"", initvalue=""0ULL"")\n\nhalf_t = NumberType(""half"", initvalue=""0"")\nhalf2_t = NumberType(""half2"", initvalue=""half2()"")\n\nfloat_t = NumberType(""float"", initvalue=""0.0f"")\ndouble_t = NumberType(""double"", initvalue=""0.0"")\nldouble_t = NumberType(""long double"", initvalue=""0.0L"")\n\n\nint8_t = NumberType(""int8_t"", initvalue=""0"")\nuint8_t = NumberType(""uint8_t"", initvalue=""0U"")\n\nint16_t = NumberType(""int16_t"", initvalue=""0"")\nuint16_t = NumberType(""uint16_t"", initvalue=""0U"")\n\nint32_t = NumberType(""int32_t"", initvalue=""0"")\nuint32_t = NumberType(""uint32_t"", initvalue=""0U"")\n\nint64_t = NumberType(""int64_t"", initvalue=""0LL"")\nuint64_t = NumberType(""uint64_t"", initvalue=""0ULL"")\n\nptrdiff_t = NumberType(""ptrdiff_t"", initvalue=""0"")\nsize_t = NumberType(""size_t"", initvalue=""0"")\n\nPy_ssize_t = NumberType(""Py_ssize_t"", initvalue=""0"")\n'"
Compiler/Compilers/Compiler.py,0,"b'import sys, os, subprocess, sysconfig\n\n\nclass CompilerError(Exception):\n\tpass\n\n\nclass Compiler:\n\tcc = None\n\n\tcflags = None\n\tldflags = None\n\n\n\tdef __init__(self, verbose=0, env=None, forPython=True):\n\t\tself.verbose, self.env = verbose, env\n\n\t\tplatform = self.setupPlatform(forPython)\n\n\t\tself.pydext, self.oext, self.libext, self.soext, self.linkext, self.debugext = platform[:6]\n\t\tself.includeDirs, self.libraryDirs, self.libraries = platform[6:]\n\n\t\tself.features = set()\n\t\tself.defines = set()\n\n\t\tself.optlevel = 0\n\t\tself.debuglevel = 0\n\n\t\tself.cpp = False\n\n\t\tself.keys = (\n\t\t\t""cc"", ""cflags"", ""ldflags"", ""features"", ""includeDirs"", ""libraryDirs"", ""libraries"", ""defines"",\n\t\t\t""optlevel"", ""debuglevel"", ""cpp""\n\t\t)\n\n\n\t@staticmethod\n\tdef setupPlatform(forPython):\n\t\tconfig = sysconfig.get_config_vars()\n\t\tpydext = config[""EXT_SUFFIX""]\n\n\t\tincludeDirs = [config[""INCLUDEPY""]] if forPython else []\n\t\tlibraryDirs, libraries = [], []\n\n\t\tif sys.platform == ""win32"":\n\t\t\toext, libext, soext = "".obj"", "".lib"", "".dll""\n\t\t\tlinkext, debugext = ["".exp""], ["".pdb"", "".idb"", "".ilk""]\n\n\t\t\tif forPython:\n\t\t\t\tbindir = config[""BINDIR""]\n\n\t\t\t\tlibraryDirs = [os.path.join(bindir, ""libs""), bindir]\n\t\t\t\tlibraries = [""python%s%s"" % sys.version_info[:2]]\n\n\t\telif sys.platform == ""linux"":\n\t\t\toext, libext, soext = "".o"", "".a"", "".so""\n\t\t\tlinkext, debugext = [], []\n\n\t\t\tif forPython:\n\t\t\t\tlibraryDirs = [config[""LIBDIR""]]\n\t\t\t\tlibraries = [""python%s.%sm"" % sys.version_info[:2]]\n\n\t\telse:\n\t\t\traise NotImplementedError(sys.platform)\n\n\t\treturn pydext, oext, libext, soext, linkext, debugext, includeDirs, libraryDirs, libraries\n\n\n\tdef cppMode(self, enabled):\n\t\tself.cpp = enabled\n\t\treturn self\n\n\n\tdef addLibrary(self, name, includeDirs, libraryDirs, libraries):\n\t\tif name in self.features:\n\t\t\treturn\n\n\t\tself.features.add(name)\n\n\t\tself.includeDirs.extend(os.path.normpath(path) for path in includeDirs)\n\t\tself.libraryDirs.extend(os.path.normpath(path) for path in libraryDirs)\n\n\t\tself.libraries.extend(libraries)\n\t\treturn self\n\n\n\tdef addDefine(self, *defines):\n\t\tself.defines.update(defines)\n\t\treturn self\n\n\n\tdef clearPath(self, path):\n\t\texts = [self.oext, self.libext] + self.linkext\n\n\t\tif self.debuglevel == 0:\n\t\t\texts.extend(self.debugext)\n\n\t\tfor file in filter(lambda f: any(f.endswith(ext) for ext in exts), os.listdir(path)):\n\t\t\tos.remove(os.path.join(path, file))\n\n\t\treturn self\n\n\n\tdef objectLine(self, extfile, sourcefiles):\n\t\traise NotImplementedError()\n\n\n\tdef linkLine(self, extfile, objfiles):\n\t\traise NotImplementedError()\n\n\n\tdef buildLine(self, extfile, sourcefiles):\n\t\traise NotImplementedError()\n\n\n\tdef depLine(self, sourcefiles):\n\t\traise NotImplementedError()\n\n\n\tdef buildObject(self, extfile, sourcefiles):\n\t\tsourcefiles = [sourcefiles] if not isinstance(sourcefiles, list) else sourcefiles\n\t\tself.invoke(self.cmdline(self.objectLine(extfile, sourcefiles)))\n\n\t\treturn self\n\n\n\tdef link(self, extfile, objfiles):\n\t\tobjfiles = [objfiles] if not isinstance(objfiles, list) else objfiles\n\t\tself.invoke(self.cmdline(self.linkLine(extfile, objfiles)), asLinker=True)\n\n\t\treturn self\n\n\n\tdef build(self, extfile, sourcefiles):\n\t\tsourcefiles = [sourcefiles] if not isinstance(sourcefiles, list) else sourcefiles\n\t\tself.invoke(self.cmdline(self.buildLine(extfile, sourcefiles)))\n\n\t\treturn self\n\n\n\tdef getDependencies(self, sourcefiles, cwd):\n\t\tsourcefiles = [sourcefiles] if not isinstance(sourcefiles, list) else sourcefiles\n\t\tdeps = self.invoke(self.cmdline(self.depLine(sourcefiles)), verbose=self.verbose - 1).decode()\n\n\t\tcwd = os.path.normcase(os.path.realpath(cwd))\n\t\tfiles = (\n\t\t\tos.path.normcase(os.path.abspath(path)) for path in deps.split() if os.path.exists(path) and len(path) > 1\n\t\t)\n\n\t\tdeps = set(file for file in files if os.path.commonprefix([file, cwd]) == cwd)\n\t\tdeps.update([os.path.normcase(os.path.realpath(file)) for file in sourcefiles])\n\n\t\treturn deps\n\n\n\tdef withOptimizationLevel(self, level=0, debuglevel=0):\n\t\tself.optlevel, self.debuglevel = level, debuglevel\n\t\treturn self\n\n\n\tdef invoke(self, cmdline, asLinker=False, verbose=None):\n\t\tverbose = self.verbose if verbose is None else verbose\n\n\t\tif verbose > 1:\n\t\t\tprint(""%s invocation: %s"" % (self.getInvoked(asLinker), "" "".join(cmdline)), flush=True)\n\n\t\tresult = subprocess.run(cmdline, env=self.env, stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\n\t\tif result.returncode != 0 or verbose > 0:\n\t\t\tstreams = [\n\t\t\t\tstream.decode().strip() if stream is not None else """" for stream in [result.stderr, result.stdout]\n\t\t\t]\n\n\t\t\ttext = ""\\n\\n"".join(stream for stream in streams if len(stream) > 0)\n\n\t\t\tif result.returncode != 0:\n\t\t\t\traise CompilerError(\n\t\t\t\t\t""%s invocation failed: %s\\n\\n%s\\n"" % (self.getInvoked(asLinker), "" "".join(cmdline), text)\n\t\t\t\t)\n\n\t\t\tif len(text) > 0 or verbose > 1:\n\t\t\t\tprint(""\\n%s\\n"" % text if len(text) > 0 else """", flush=True)\n\n\t\treturn result.stdout\n\n\n\tdef cmdline(self, flags):\n\t\treturn [self.cc] + flags\n\n\n\t@staticmethod\n\tdef getInvoked(asLinker):\n\t\treturn ""Linker"" if asLinker else ""Compiler""\n\n\n\tdef signature(self):\n\t\tkv = (\n\t\t\t(key, "","".join(value) if isinstance(value, (list, tuple)) else value) for key, value in\n\t\t\t((key, getattr(self, key)) for key in sorted(self.keys))\n\t\t)\n\t\treturn "";"".join(""%s:%s"" % (key, value) for key, value in kv)\n'"
Compiler/Compilers/GCC.py,0,"b'import sys\n\nfrom PuzzleLib.Compiler.Compilers.Compiler import Compiler\n\n\nclass GCCLike(Compiler):\n\tcflags = [""-Wall"", ""-Wextra""]\n\tldflags = [""--shared""]\n\n\n\tdef __init__(self, verbose, forPython=True):\n\t\tsuper().__init__(verbose, forPython=forPython)\n\n\t\tif sys.platform == ""linux"":\n\t\t\tself.cflags = self.cflags + [""-fPIC""]\n\n\n\tdef objectLine(self, extfile, sourcefiles):\n\t\treturn self.fullCFlags(asObject=True) + self.outFlags(extfile) + sourcefiles\n\n\n\tdef linkLine(self, extfile, objfiles):\n\t\treturn self.fullLDFlags() + self.outFlags(extfile) + objfiles + self.linkFlags()\n\n\n\tdef buildLine(self, extfile, sourcefiles):\n\t\treturn self.fullCFlags(asObject=False) + self.fullLDFlags() + self.outFlags(extfile) + \\\n\t\t\t   sourcefiles + self.linkFlags()\n\n\n\tdef depLine(self, sourcefiles):\n\t\treturn [""-M""] + self.fullCFlags(asObject=False, debug=False, optimize=False) + sourcefiles\n\n\n\tdef fullCFlags(self, asObject, debug=True, optimize=True):\n\t\toflags = self.fullCppFlags()\n\n\t\tif debug and self.debuglevel > 0:\n\t\t\toflags.append(""-g3"" if self.debuglevel >= 3 else ""-g"")\n\n\t\tif optimize and self.optlevel > 0:\n\t\t\toflags.append(""-O3"" if self.optlevel >= 3 else ""-O%s"" % self.optlevel)\n\n\t\t\tif self.optlevel >= 3:\n\t\t\t\toflags.extend([""-march=native"", ""-mtune=native"", ""-ffast-math""])\n\n\t\t\tif debug and self.debuglevel >= 3:\n\t\t\t\toflags.append(""-fno-omit-frame-pointer"")\n\n\t\toflags.extend(""-D%s"" % define for define in self.defines)\n\t\treturn self.cflags + oflags + [""-I%s"" % idir for idir in self.includeDirs] + ([""-c""] if asObject else [])\n\n\n\tdef fullCppFlags(self):\n\t\treturn [""-std=c++14"" if self.cpp else ""-std=gnu99""]\n\n\n\tdef fullLDFlags(self):\n\t\treturn self.ldflags + [""-L%s"" % ldir for ldir in self.libraryDirs]\n\n\n\tdef outFlags(self, extfile):\n\t\toutFlags = [""-o"", extfile]\n\n\t\tif self.optlevel >= 4:\n\t\t\toutFlags.append(""-flto"")\n\n\t\treturn outFlags\n\n\n\tdef linkFlags(self):\n\t\treturn [""-l%s"" % lib for lib in self.libraries]\n\n\nclass GCC(GCCLike):\n\tcc = ""gcc""\n\n\nclass Clang(GCCLike):\n\tcc = ""clang""\n\n\n\tdef fullCppFlags(self):\n\t\treturn [""-std=c++14"" if self.cpp else ""-std=c99""]\n\n\n\tdef outFlags(self, extfile):\n\t\toutflags = super().outFlags(extfile)\n\n\t\tif sys.platform == ""win32"":\n\t\t\toutflags.append(""-fuse-ld=lld"")\n\n\t\treturn outflags\n'"
Compiler/Compilers/MSVC.py,0,"b'import os, subprocess, itertools\n\nfrom PuzzleLib.Compiler.Compilers.Compiler import Compiler, CompilerError\n\n\nclass MSVC(Compiler):\n\tcc = ""cl""\n\n\tcflags = [""/W4""]\n\tldflags = [""/LD""]\n\n\tvcenv = None\n\n\n\tdef objectLine(self, extfile, sourcefiles):\n\t\treturn self.fullCFlags(asObject=True) + self.outflags(extfile, asObject=True) + sourcefiles\n\n\n\tdef linkLine(self, extfile, objfiles):\n\t\treturn self.fullLDFlags() + self.outflags(extfile, asObject=False) + objfiles + self.linkFlags(extfile)\n\n\n\tdef buildLine(self, extfile, sourcefiles):\n\t\treturn self.fullCFlags(asObject=False) + self.fullLDFlags() + self.outflags(extfile, asObject=False) + \\\n\t\t\t   [""/MP""] + sourcefiles + self.linkFlags(extfile)\n\n\n\tdef depLine(self, sourcefiles):\n\t\treturn [""/showIncludes""] + self.fullCFlags(asObject=True, debug=False, optimize=False) + \\\n\t\t\t   self.outflags(None, asObject=True, debug=False, optimize=False) + sourcefiles\n\n\n\tdef fullCFlags(self, asObject, debug=True, optimize=True):\n\t\toflags = [""/EHsc""] if self.cpp else []\n\n\t\tif optimize and self.optlevel > 0:\n\t\t\toflags.append(""/Ox"" if self.optlevel >= 3 else ""/O%s"" % self.optlevel)\n\n\t\t\tif self.optlevel >= 3:\n\t\t\t\toflags.append(""/fp:fast"")\n\n\t\t\tif debug and self.debuglevel >= 3:\n\t\t\t\toflags.append(""/Oy-"")\n\n\t\toflags.extend(""/D%s"" % define for define in self.defines)\n\t\treturn self.cflags + oflags + [""/I%s"" % idir for idir in self.includeDirs] + ([""/c""] if asObject else [])\n\n\n\tdef fullLDFlags(self):\n\t\treturn self.ldflags + [""%s%s"" % (lib, self.libext) for lib in self.libraries]\n\n\n\tdef outflags(self, extfile, asObject, debug=True, optimize=True):\n\t\tif extfile is None:\n\t\t\treturn [""/nologo"", ""/FoNUL""]\n\n\t\telif asObject:\n\t\t\toutpath, dbgpath = extfile, os.path.dirname(extfile) + os.path.sep\n\t\telse:\n\t\t\toutpath = dbgpath = os.path.dirname(extfile) + os.path.sep\n\n\t\toutflags = [""/nologo"", ""/Fo%s"" % outpath]\n\n\t\tif debug:\n\t\t\tif self.debuglevel > 0:\n\t\t\t\toutflags.extend([""/Fd%s"" % dbgpath, ""/ZI"" if self.debuglevel >= 3 else ""/Zi""])\n\n\t\tif optimize and self.debuglevel == 0 and self.optlevel >= 4:\n\t\t\toutflags.append(""/GL"")\n\n\t\treturn outflags\n\n\n\tdef linkFlags(self, extfile):\n\t\treturn [""/link"", ""/IMPLIB:%s"" % (os.path.splitext(extfile)[0] + self.libext), ""/OUT:%s"" % extfile] + \\\n\t\t\t   [""/LIBPATH:%s"" % ldir for ldir in self.libraryDirs]\n\n\n\tdef invoke(self, cmdline, asLinker=False, verbose=None):\n\t\tverbose = self.verbose if verbose is None else verbose\n\n\t\tif self.env is None:\n\t\t\tself.env = self.createEnvironment(verbose)\n\n\t\treturn super().invoke(cmdline, asLinker, verbose)\n\n\n\t@classmethod\n\tdef createEnvironment(cls, verbose):\n\t\tif cls.vcenv is None:\n\t\t\tif verbose > 1:\n\t\t\t\tprint(""Creating msvc environment ..."", flush=True)\n\n\t\t\tcls.vcenv = getEnv(""amd64"")\n\n\t\treturn cls.vcenv\n\n\ndef getEnv(platspec):\n\tvcvarsall = findVCVarsAll()\n\n\tif vcvarsall is None:\n\t\traise CompilerError(""Unable to find vcvarsall.bat"")\n\n\ttry:\n\t\tout = subprocess.check_output(\n\t\t\t""cmd /u /c \\""%s\\"" %s && set"" % (vcvarsall, platspec), stderr=subprocess.STDOUT\n\t\t).decode(""utf-16le"", errors=""replace"")\n\n\texcept subprocess.CalledProcessError as exc:\n\t\traise CompilerError(""Error executing %s"" % exc.cmd)\n\n\treturn {\n\t\tkey.lower(): value for key, _, value in (line.partition(""="") for line in out.splitlines()) if key and value\n\t}\n\n\ndef findVCVarsAll():\n\tvcpath = findVC2017()\n\tvcpath = findVC2015() if vcpath is None else vcpath\n\n\tif vcpath is None:\n\t\treturn None\n\n\tvcvarsall = os.path.join(vcpath, ""vcvarsall.bat"")\n\treturn vcvarsall if os.path.isfile(vcvarsall) else None\n\n\ndef findVC2017():\n\troot = os.environ.get(""ProgramFiles(x86)"") or os.environ.get(""ProgramFiles"")\n\n\tif root is None:\n\t\treturn None\n\n\ttry:\n\t\tvcpath = subprocess.check_output([\n\t\t\tos.path.join(root, ""Microsoft Visual Studio"", ""Installer"", ""vswhere.exe""),\n\t\t\t""-latest"", ""-prerelease"",\n\t\t\t""-requires"", ""Microsoft.VisualStudio.Component.VC.Tools.x86.x64"",\n\t\t\t""-property"", ""installationPath""\n\t\t], encoding=""mbcs"", errors=""strict"").strip()\n\n\texcept (subprocess.CalledProcessError, OSError, UnicodeDecodeError):\n\t\treturn None\n\n\tvcpath = os.path.join(vcpath, ""VC"", ""Auxiliary"", ""Build"")\n\treturn vcpath if os.path.isdir(vcpath) else None\n\n\ndef findVC2015():\n\timport winreg\n\n\ttry:\n\t\tkey = winreg.OpenKeyEx(\n\t\t\twinreg.HKEY_LOCAL_MACHINE, r""Software\\Microsoft\\VisualStudio\\SxS\\VC7"",\n\t\t\taccess=winreg.KEY_READ | winreg.KEY_WOW64_32KEY\n\t\t)\n\n\texcept OSError:\n\t\treturn None\n\n\twith key:\n\t\tvcversion, vcpath = 0, None\n\n\t\tfor i in itertools.count():\n\t\t\ttry:\n\t\t\t\tv, path, vt = winreg.EnumValue(key, i)\n\n\t\t\texcept OSError:\n\t\t\t\tbreak\n\n\t\t\tif v and vt == winreg.REG_SZ and os.path.isdir(path):\n\t\t\t\ttry:\n\t\t\t\t\tversion = int(v)\n\n\t\t\t\texcept (ValueError, TypeError):\n\t\t\t\t\tcontinue\n\n\t\t\t\tif version >= 14 and version > vcversion:\n\t\t\t\t\tvcversion, vcpath = version, path\n\n\treturn vcpath\n'"
Compiler/Compilers/NVCC.py,0,"b'import sys\n\nfrom PuzzleLib.Compiler.Compilers.GCC import GCCLike\nfrom PuzzleLib.Compiler.Compilers.MSVC import MSVC\n\n\nclass NVCC(GCCLike):\n\tcc = ""nvcc""\n\n\n\tdef __init__(self, verbose=0, forPython=False):\n\t\tsuper().__init__(verbose)\n\t\tcflags = MSVC.cflags if sys.platform == ""win32"" else self.cflags\n\n\t\tself.cflags = [flag for cflag in cflags for flag in [""-Xcompiler"", cflag]]\n\t\tself.cpp = True\n\n\t\tif not forPython:\n\t\t\tself.ldflags = []\n\n\n\tdef cppMode(self, enabled):\n\t\tassert False\n\n\n\tdef\tfullCFlags(self, asObject, debug=True, optimize=True):\n\t\toflags = self.fullCppFlags()\n\n\t\tif debug and self.debuglevel > 0:\n\t\t\toflags.extend([""-g"", ""-G"" if self.debuglevel >= 3 else ""-lineinfo""])\n\n\t\tif optimize and self.optlevel > 0:\n\t\t\toflags.append(""-O3"" if self.optlevel >= 3 else ""-O%s"" % self.optlevel)\n\n\t\t\tif self.optlevel >= 3:\n\t\t\t\toflags.append(""-use_fast_math"")\n\n\t\treturn self.cflags + oflags + [""-I%s"" % idir for idir in self.includeDirs] + ([""-c""] if asObject else [])\n\n\n\tdef fullCppFlags(self):\n\t\treturn [] if sys.platform == ""win32"" else [""-std=c++14""]\n\n\n\tdef outFlags(self, extfile):\n\t\treturn [""-o"", extfile]\n'"
Converter/Caffe/ConvertBlob.py,1,"b'import subprocess\n\nimport numpy as np\nimport h5py\n\n\ndef saveAttr(data, name, filename):\n\thdf = h5py.File(filename, mode=""a"")\n\n\tmodelname = next(iter(hdf[""links""].keys())).split(sep=""."")[0]\n\n\tattrGrpName = ""attrs.%s"" % modelname\n\n\tattrGrp = hdf.require_group(attrGrpName)\n\tattrGrp.create_dataset(""%s.%s"" % (modelname, name), data=data)\n\n\ndef main():\n\tbinaryname = ""ResNet_mean.binaryproto""\n\tmodelname = ""ResNet-50-model.hdf""\n\tattrName = ""mean""\n\n\tsubprocess.check_call([""protoc"", ""--proto_path"", ""."", ""--python_out"", ""."", ""caffe.proto""])\n\tprint(""Compiled caffe.proto"")\n\n\tfrom PuzzleLib.Converter.Caffe import caffe_pb2\n\tblob = caffe_pb2.BlobProto()\n\n\tmsg = open(binaryname, ""rb"").read()\n\n\tprint(""Started parsing binaryproto %s ..."" % binaryname)\n\tblob.ParseFromString(msg)\n\n\tdata = np.array(blob.data, dtype=np.float32).reshape((1, blob.channels, blob.height, blob.width))\n\tsaveAttr(data, attrName, modelname)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/Caffe/ConvertModel.py,0,"b'import os, subprocess, pickle\nfrom google.protobuf.descriptor import FieldDescriptor as FD\n\nfrom PuzzleLib.Converter.Caffe import Parsers\n\n\ndef js2hdf(js, hdf, compress=""gzip"", netName=None, **kwargs):\n\tif not ""layer"" in js:\n\t\tParsers.parseOldCaffeFormat(js, hdf, compress, netName)\n\telse:\n\t\tParsers.parseNewCaffeFormat(js, hdf, compress, netName, **kwargs)\n\n\ndef pb2json(pb):\n\tftype2js = {\n\t\tFD.TYPE_DOUBLE: float,\n\t\tFD.TYPE_FLOAT: float,\n\t\tFD.TYPE_INT64: int,\n\t\tFD.TYPE_UINT64: int,\n\t\tFD.TYPE_INT32: int,\n\t\tFD.TYPE_FIXED64: float,\n\t\tFD.TYPE_FIXED32: float,\n\t\tFD.TYPE_BOOL: bool,\n\t\tFD.TYPE_STRING: str,\n\t\tFD.TYPE_BYTES: lambda x: x.encode(\'string_escape\'),\n\t\tFD.TYPE_UINT32: int,\n\t\tFD.TYPE_ENUM: int,\n\t\tFD.TYPE_SFIXED32: float,\n\t\tFD.TYPE_SFIXED64: float,\n\t\tFD.TYPE_SINT32: int,\n\t\tFD.TYPE_SINT64: int,\n\t}\n\n\tjs = {}\n\tfields = pb.ListFields()\n\n\tfor field,value in fields:\n\t\tftype = None\n\n\t\tif field.type == FD.TYPE_MESSAGE:\n\t\t\tftype = pb2json\n\t\telif field.type in ftype2js:\n\t\t\tftype = ftype2js[field.type]\n\t\telse:\n\t\t\tprint(\n\t\t\t\t""WARNING: Field %s.%s of type \'%d\' is not supported"" %\n\t\t\t\t(pb.__class__.__name__, field.name, field.type)\n\t\t\t)\n\n\t\tjs[field.name] = [ftype(v) for v in value] if field.label == FD.LABEL_REPEATED else ftype(value)\n\n\treturn js\n\n\ndef isPickled(modelname):\n\treturn os.path.isfile(os.path.splitext(modelname)[0] + "".pkl"")\n\n\ndef savePickle(js, modelname):\n\tpickle.dump(js, open(os.path.splitext(modelname)[0] + "".pkl"", ""wb""))\n\n\ndef loadPickle(modelname):\n\treturn pickle.load(open(os.path.splitext(modelname)[0] + "".pkl"", ""rb""))\n\n\ndef main():\n\tmodelname = ""VGG_ILSVRC_16_layers.caffemodel""\n\tnetname = None\n\tbatchNormVarInverse = False\n\teps = 1e-5\n\n\tif not isPickled(modelname):\n\t\tprint(""Model is not pickled ..."")\n\n\t\tsubprocess.check_call([""protoc"", ""--proto_path"", ""."", ""--python_out"", ""."", ""caffe.proto""])\n\t\tprint(""Compiled caffe.proto"")\n\n\t\tfrom PuzzleLib.Converter.Caffe import caffe_pb2\n\t\tnet = caffe_pb2.NetParameter()\n\n\t\tmsg = open(modelname, ""rb"").read()\n\n\t\tprint(""Started parsing caffemodel %s ... May take a lot of time ..."" % modelname)\n\t\tnet.ParseFromString(msg)\n\n\t\tprint(""Started jsoning caffemodel ..."")\n\t\tjs = pb2json(net)\n\t\tdel net\n\n\t\tprint(""Saving pickle ..."")\n\t\tsavePickle(js, modelname)\n\telse:\n\t\tprint(""Loading pickle ..."")\n\t\tjs = loadPickle(modelname)\n\n\tprint(""Saving as hdf ..."")\n\n\tjs2hdf(\n\t\tjs, os.path.splitext(modelname)[0] + "".hdf"", compress=""gzip"", netName=netname,\n\t\tbatchNormVarInverse=batchNormVarInverse, eps=eps\n\t)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/Caffe/Parsers.py,18,"b'import h5py\nimport numpy as np\n\n\ndef parseOldCaffeFormat(js, hdf, compress=""gzip"", netName=None):\n\tparamlayers = {4: ""convolution"", 39: ""deconvolution"", 14: ""inner_product""}\n\toldparamlayers = {""conv"": ""convolution"", ""innerproduct"": ""inner_product""}\n\n\tif isinstance(hdf, str):\n\t\thdf = h5py.File(hdf, ""w"")\n\n\tlinkGrp = hdf.create_group(""links"")\n\tparamGrp = hdf.create_group(""params"")\n\thdf.require_group(""attrs"")\n\n\tlayers = js[""layers""]\n\n\tif netName is None:\n\t\tnetName = js[""name""]\n\n\tparamIdx = 0\n\tfor layer in layers:\n\t\tif ""layer"" in layer:\n\t\t\tlayer = layer[""layer""]\n\n\t\tlayerName = ""%s.%s"" % (netName, layer[""name""])\n\n\t\tif layer[""type""] in paramlayers:\n\t\t\tlayertype = paramlayers[layer[""type""]]\n\n\t\telif layer[""type""] in oldparamlayers:\n\t\t\tlayertype = oldparamlayers[layer[""type""]]\n\n\t\telse:\n\t\t\tcontinue\n\n\t\tif layertype == ""convolution"":\n\t\t\tblobs = layer[""blobs""]\n\t\t\tfor blob in blobs:\n\t\t\t\tparam = np.array(blob[""data""], dtype=np.float32)\n\t\t\t\tif blob[""num""] == blob[""channels""] == blob[""height""] == 1:\n\t\t\t\t\tb = np.reshape(param, (1, param.shape[0], 1, 1))\n\t\t\t\t\tlinkGrp.create_dataset(""%s.b"" % layerName, data=paramIdx)\n\t\t\t\t\tparamGrp.create_dataset(str(paramIdx), data=b, compression=compress)\n\n\t\t\t\telse:\n\t\t\t\t\tW = np.reshape(param, (blob[""num""], blob[""channels""], blob[""height""], blob[""width""]))\n\t\t\t\t\tlinkGrp.create_dataset(""%s.W"" % layerName, data=paramIdx)\n\t\t\t\t\tparamGrp.create_dataset(str(paramIdx), data=W, compression=compress)\n\n\t\t\t\tparamIdx += 1\n\n\t\telif layertype == ""inner_product"":\n\t\t\tblobs = layer[""blobs""]\n\t\t\tfor blob in blobs:\n\t\t\t\tparam = np.array(blob[""data""], dtype=np.float32)\n\t\t\t\tif blob[""num""] == blob[""channels""] == blob[""height""] == 1:\n\t\t\t\t\tb = np.reshape(param, (param.shape[0], ))\n\t\t\t\t\tlinkGrp.create_dataset(""%s.b"" % layerName, data=paramIdx)\n\t\t\t\t\tparamGrp.create_dataset(str(paramIdx), data=b, compression=compress)\n\n\t\t\t\telse:\n\t\t\t\t\tW = np.reshape(param, (blob[""height""], blob[""width""])).T\n\t\t\t\t\tlinkGrp.create_dataset(""%s.W"" % layerName, data=paramIdx)\n\t\t\t\t\tparamGrp.create_dataset(str(paramIdx), data=W, compression=compress)\n\n\t\t\t\tparamIdx += 1\n\n\t\telse:\n\t\t\traise NotImplementedError()\n\n\ndef parseNewCaffeFormat(js, hdf, compress=""gzip"", netName=None, **kwargs):\n\tparamlayers = {""Convolution"", ""Deconvolution"", ""InnerProduct"", ""BatchNorm"", ""Scale"", ""PReLU""}\n\n\tif isinstance(hdf, str):\n\t\thdf = h5py.File(hdf, ""w"")\n\n\tlinkGrp = hdf.create_group(""links"")\n\tparamGrp = hdf.create_group(""params"")\n\tattrGrp = hdf.require_group(""attrs"")\n\n\tlayers = js[""layer""]\n\n\tif netName is None:\n\t\tnetName = js[""name""]\n\n\tparamIdx = 0\n\tfor i, layer in enumerate(layers):\n\t\tif layer[""type""] in paramlayers:\n\t\t\tlayertype = layer[""type""]\n\t\t\tlayerName = ""%s.%s"" % (netName, layer[""name""])\n\n\t\t\tif layertype == ""Convolution"":\n\t\t\t\tblobs = layer[""blobs""]\n\t\t\t\tfor blob in blobs:\n\t\t\t\t\tparam = np.array(blob[""data""], dtype=np.float32)\n\t\t\t\t\tdim = blob[""shape""][""dim""]\n\t\t\t\t\tif len(dim) == 1:\n\t\t\t\t\t\tb = np.reshape(param, (1, param.shape[0], 1, 1))\n\t\t\t\t\t\tlinkGrp.create_dataset(""%s.b"" % layerName, data=paramIdx)\n\t\t\t\t\t\tparamGrp.create_dataset(str(paramIdx), data=b, compression=compress)\n\n\t\t\t\t\telse:\n\t\t\t\t\t\tW = np.reshape(param, dim)\n\t\t\t\t\t\tlinkGrp.create_dataset(""%s.W"" % layerName, data=paramIdx)\n\t\t\t\t\t\tparamGrp.create_dataset(str(paramIdx), data=W, compression=compress)\n\n\t\t\t\t\tparamIdx += 1\n\n\t\t\telif layertype == ""InnerProduct"":\n\t\t\t\tblobs = layer[""blobs""]\n\t\t\t\tfor blob in blobs:\n\t\t\t\t\tparam = np.array(blob[""data""], dtype=np.float32)\n\t\t\t\t\tdim = blob[""shape""][""dim""]\n\t\t\t\t\tif len(dim) == 1:\n\t\t\t\t\t\tb = np.reshape(param, (param.shape[0], ))\n\t\t\t\t\t\tlinkGrp.create_dataset(""%s.b"" % layerName, data=paramIdx)\n\t\t\t\t\t\tparamGrp.create_dataset(str(paramIdx), data=b, compression=compress)\n\n\t\t\t\t\telse:\n\t\t\t\t\t\tW = np.reshape(param, dim).T\n\t\t\t\t\t\tlinkGrp.create_dataset(""%s.W"" % layerName, data=paramIdx)\n\t\t\t\t\t\tparamGrp.create_dataset(str(paramIdx), data=W, compression=compress)\n\n\t\t\t\t\tparamIdx += 1\n\n\t\t\telif layertype == ""BatchNorm"":\n\t\t\t\tblobs = layer[""blobs""]\n\t\t\t\tdim = blobs[0][""shape""][""dim""][0]\n\n\t\t\t\tmean = np.array(blobs[0][""data""], dtype=np.float32).reshape((1, dim, 1, 1))\n\t\t\t\tvar = np.array(blobs[1][""data""], dtype=np.float32).reshape((1, dim, 1, 1))\n\n\t\t\t\tif len(blobs) > 2:\n\t\t\t\t\tscale = blobs[2][""data""][0]\n\n\t\t\t\t\tif scale > 0.0:\n\t\t\t\t\t\tscale = 1.0 / scale\n\n\t\t\t\t\tmean *= scale\n\t\t\t\t\tvar *= scale\n\n\t\t\t\tif ""batchNormVarInverse"" in kwargs and kwargs[""batchNormVarInverse""]:\n\t\t\t\t\tvar = 1 / np.sqrt(var + kwargs[""eps""])\n\n\t\t\t\tattrGrp.create_dataset(""%s.mean"" % layerName, data=mean)\n\t\t\t\tattrGrp.create_dataset(""%s.var"" % layerName, data=var)\n\n\t\t\telif layertype == ""Scale"":\n\t\t\t\tif i > 0 and layers[i-1][""type""] == ""BatchNorm"":\n\t\t\t\t\tblobs = layer[""blobs""]\n\t\t\t\t\tdim = blobs[0][""shape""][""dim""][0]\n\n\t\t\t\t\tlastLayerName = ""%s.%s"" % (netName, layers[i - 1][""name""])\n\n\t\t\t\t\tscale = np.array(blobs[0][""data""], dtype=np.float32).reshape((1, dim, 1, 1))\n\t\t\t\t\tlinkGrp.create_dataset(""%s.scale"" % lastLayerName, data=paramIdx)\n\t\t\t\t\tparamGrp.create_dataset(str(paramIdx), data=scale, compression=compress)\n\t\t\t\t\tparamIdx += 1\n\n\t\t\t\t\tif len(blobs) > 1:\n\t\t\t\t\t\tbias = np.array(blobs[1][""data""], dtype=np.float32).reshape((1, dim, 1, 1))\n\t\t\t\t\t\tlinkGrp.create_dataset(""%s.bias"" % lastLayerName, data=paramIdx)\n\t\t\t\t\t\tparamGrp.create_dataset(str(paramIdx), data=bias, compression=compress)\n\t\t\t\t\t\tparamIdx += 1\n\n\t\t\telif layertype == ""PReLU"":\n\t\t\t\tblobs = layer[""blobs""]\n\t\t\t\tif len(blobs) > 0:\n\t\t\t\t\tblob = blobs[0]\n\t\t\t\t\tslopes = np.array(blob[""data""], dtype=np.float32)\n\t\t\t\t\tlinkGrp.create_dataset(""%s.slopes"" % layerName, data = paramIdx)\n\t\t\t\t\tparamGrp.create_dataset(str(paramIdx), data=slopes, compression=compress)\n\t\t\t\t\tparamIdx += 1\n'"
Converter/Examples/Common.py,2,"b'import numpy as np\n\nfrom PuzzleLib import Visual\n\n\ndef loadV3Labels(filename):\n\twith open(filename) as f:\n\t\tsynsets = f.readlines()\n\t\tsynsets = [line.strip() for line in synsets]\n\n\tlabels = {}\n\tfor i, synset in enumerate(synsets):\n\t\tlabels[i] = synset\n\n\treturn labels\n\n\ndef loadLabels(synpath, wordpath):\n\twith open(synpath) as f:\n\t\tsynsets = f.readlines()\n\t\tsynsets = [line.strip() for line in synsets]\n\n\twith open(wordpath) as f:\n\t\tlines = f.readlines()\n\t\tlines = [line.strip() for line in lines]\n\n\twords = {}\n\tfor line in lines:\n\t\ttags = line.split(sep="" "", maxsplit=1)\n\t\twords[tags[0]] = tags[1]\n\n\tlabels = {}\n\tfor i, synset in enumerate(synsets):\n\t\tlabels[i] = words[synset]\n\n\treturn labels\n\n\ndef showLabelResults(res, labels, limit=5, header=""""):\n\tidx = (-res).argsort()[:limit]\n\n\tprint(""%sTop-%s predictions:"" % (""%s "" % header if len(header) > 0 else """", limit))\n\tfor i in range(limit):\n\t\tprint(""#%s %s (prob=%s)"" % (i + 1, labels[idx[i]], res[idx[i]]))\n\n\ndef loadVGGSample(filename, shape=None, normalize=False):\n\tmeanPixel = np.array([103.939, 116.779, 123.68], dtype=np.float32).reshape((1, 3, 1, 1))\n\tsample = loadSample(filename, shape) - meanPixel\n\n\treturn sample * (2.0 / 255.0) - 1.0 if normalize else sample\n\n\ndef loadResNetSample(net, filename, shape=None):\n\tmean = net.getAttr(""mean"")\n\treturn loadSample(filename, shape) - mean\n\n\ndef loadSample(filename, shape=None):\n\treturn Visual.loadImage(filename, shape, normalize=False)[:, ::-1, :, :].astype(np.float32)\n'"
Converter/Examples/Inception.py,0,"b'from PuzzleLib import Config\nConfig.globalEvalMode = True\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Models.Nets.Inception import loadInceptionBN, loadInceptionV3\n\nfrom PuzzleLib.Converter.Examples.Common import loadVGGSample, loadLabels, loadV3Labels, showLabelResults\n\n\ndef main():\n\tinceptionBNTest()\n\tinceptionV3Test()\n\n\ndef inceptionBNTest():\n\tnet = loadInceptionBN(modelpath=""../TestData/Inception-BN-0126.hdf"")\n\n\tsample = loadVGGSample(""../TestData/tarantula.jpg"")\n\tlabels = loadLabels(synpath=""../TestData/synsets.txt"", wordpath=""../TestData/synset_words.txt"")\n\n\tres = net(gpuarray.to_gpu(sample)).get().reshape(-1)\n\tshowLabelResults(res, labels, header=net.name)\n\n\ndef inceptionV3Test():\n\tnet = loadInceptionV3(modelpath=""../TestData/Inception-7-0001.hdf"")\n\n\tsample = loadVGGSample(""../TestData/tarantula.jpg"", shape=(299, 299), normalize=True)\n\tlabels = loadV3Labels(filename=""../TestData/synset_inception_v3.txt"")\n\n\tres = net(gpuarray.to_gpu(sample)).get().reshape(-1)\n\tshowLabelResults(res, labels, header=net.name)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/Examples/NiN.py,0,"b'from PuzzleLib import Config\nConfig.globalEvalMode = True\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Models.Nets import loadNiNImageNet\n\nfrom PuzzleLib.Converter.Examples.Common import loadSample, loadLabels, showLabelResults\n\n\ndef main():\n\tnet = loadNiNImageNet(modelpath=""../TestData/nin_imagenet.hdf"")\n\n\tsample = loadSample(""../TestData/barometer.jpg"")\n\tlabels = loadLabels(synpath=""../TestData/synsets.txt"", wordpath=""../TestData/synset_words.txt"")\n\n\tres = net(gpuarray.to_gpu(sample)).get().reshape(-1)\n\tshowLabelResults(res, labels, header=""NiN"")\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/Examples/ResNet.py,0,"b'from PuzzleLib import Config\nConfig.globalEvalMode = True\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Models.Nets.ResNet import loadResNet\n\nfrom PuzzleLib.Converter.Examples.Common import loadResNetSample, loadLabels, showLabelResults\n\n\ndef main():\n\tresNet50Test()\n\tresNet101Test()\n\tresNet152Test()\n\n\ndef resNet50Test():\n\tnet = loadResNet(modelpath=""../TestData/ResNet-50-model.hdf"", layers=""50"")\n\n\tsample = loadResNetSample(net, ""../TestData/tarantula.jpg"")\n\tlabels = loadLabels(synpath=""../TestData/synsets.txt"", wordpath=""../TestData/synset_words.txt"")\n\n\tres = net(gpuarray.to_gpu(sample)).get().reshape(-1)\n\tshowLabelResults(res, labels, header=net.name)\n\n\ndef resNet101Test():\n\tnet = loadResNet(modelpath=""../TestData/ResNet-101-model.hdf"", layers=""101"")\n\n\tsample = loadResNetSample(net, ""../TestData/tarantula.jpg"")\n\tlabels = loadLabels(synpath=""../TestData/synsets.txt"", wordpath=""../TestData/synset_words.txt"")\n\n\tres = net(gpuarray.to_gpu(sample)).get().reshape(-1)\n\tshowLabelResults(res, labels, header=net.name)\n\n\ndef resNet152Test():\n\tnet = loadResNet(modelpath=""../TestData/ResNet-152-model.hdf"", layers=""152"")\n\n\tsample = loadResNetSample(net, ""../TestData/tarantula.jpg"")\n\tlabels = loadLabels(synpath=""../TestData/synsets.txt"", wordpath=""../TestData/synset_words.txt"")\n\n\tres = net(gpuarray.to_gpu(sample)).get().reshape(-1)\n\tshowLabelResults(res, labels, header=net.name)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/Examples/VGG.py,0,"b'from PuzzleLib import Config\nConfig.globalEvalMode = True\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Models.Nets.VGG import loadVGG\n\nfrom PuzzleLib.Converter.Examples.Common import loadVGGSample, loadLabels, showLabelResults\n\n\ndef main():\n\tvgg16Test()\n\tvgg19Test()\n\n\ndef vgg16Test():\n\tnet = loadVGG(modelpath=""../TestData/VGG_ILSVRC_16_layers.hdf"", layers=""16"")\n\n\tsample = loadVGGSample(""../TestData/tarantula.jpg"")\n\tlabels = loadLabels(synpath=""../TestData/synsets.txt"", wordpath=""../TestData/synset_words.txt"")\n\n\tres = net(gpuarray.to_gpu(sample)).get().reshape(-1)\n\tshowLabelResults(res, labels, header=net.name)\n\n\ndef vgg19Test():\n\tnet = loadVGG(modelpath=""../TestData/VGG_ILSVRC_19_layers.hdf"", layers=""19"")\n\n\tsample = loadVGGSample(""../TestData/tarantula.jpg"")\n\tlabels = loadLabels(synpath=""../TestData/synsets.txt"", wordpath=""../TestData/synset_words.txt"")\n\n\tres = net(gpuarray.to_gpu(sample)).get().reshape(-1)\n\tshowLabelResults(res, labels, header=net.name)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/MXNet/ConvertModel.py,2,"b'import struct, array, enum, os, json\n\nimport numpy as np\nimport h5py\n\n\nclass TypeFlag(enum.Enum):\n\tkFloat32 = 0\n\tkFloat64 = 1\n\tkFloat16 = 2\n\tkUint8 = 3\n\tkInt32 = 4\n\n\ndef readHeader(file):\n\tmagic, reserved = struct.unpack(""<QQ"", file.read(16))\n\n\tif magic != 0x112:\n\t\traise ValueError()\n\n\ndef readData(file):\n\ttensors = []\n\tntensors = struct.unpack(""<Q"", file.read(8))[0]\n\n\tfor i in range(ntensors):\n\t\tndim = struct.unpack(""<I"", file.read(4))[0]\n\t\tshape = struct.unpack(""<"" + ""I"" * ndim, file.read(4 * ndim))\n\n\t\tdevtype, devid, typeflag = struct.unpack(""<iii"", file.read(12))\n\t\ttypeflag = TypeFlag(typeflag)\n\n\t\tif typeflag == TypeFlag.kFloat32:\n\t\t\tltrl = ""f""\n\t\telif typeflag == TypeFlag.kFloat64:\n\t\t\tltrl = ""d""\n\t\telif typeflag == TypeFlag.kFloat16:\n\t\t\tltrl = ""h""\n\t\telif typeflag == TypeFlag.kUint8:\n\t\t\tltrl = ""B""\n\t\telif typeflag == TypeFlag.kInt32:\n\t\t\tltrl = ""i""\n\t\telse:\n\t\t\traise ValueError()\n\n\t\tdata = array.array(ltrl)\n\t\tdata.fromfile(file, int(np.prod(shape)))\n\n\t\ttensor = np.array(data).reshape(shape)\n\t\ttensors.append(tensor)\n\n\treturn tensors\n\n\ndef readKeys(file):\n\tkeys = []\n\tnkeys = struct.unpack(""<Q"", file.read(8))[0]\n\n\tfor i in range(nkeys):\n\t\tlength = struct.unpack(""<Q"", file.read(8))[0]\n\t\tdata = array.array(""B"")\n\t\tdata.fromfile(file, length)\n\n\t\tkey = data.tobytes().decode()\n\t\tkeys.append(key)\n\n\treturn keys\n\n\ndef loadSymbols(symbolsname):\n\twith open(symbolsname) as file:\n\t\tsymbols = json.loads(file.read())\n\t\treturn symbols\n\n\ndef buildHdf(keys, tensors, symbols, hdf, modelname, compress=""gzip""):\n\thdf = h5py.File(hdf, ""w"") if isinstance(hdf, str) else hdf\n\n\ttable = {}\n\tfor i in range(len(keys)):\n\t\ttable[keys[i]] = tensors[i]\n\n\tlinkGrp = hdf.create_group(""links"")\n\tparamGrp = hdf.create_group(""params"")\n\tattrGrp = hdf.create_group(""attrs"")\n\n\tparamIdx = 0\n\n\tfor i in range(len(symbols[""nodes""])):\n\t\tnode = symbols[""nodes""][i]\n\n\t\tname = node[""name""]\n\t\tlayerName = ""%s.%s"" % (modelname, name)\n\n\t\top = node[""op""]\n\n\t\tif op == ""Convolution"":\n\t\t\tif (""arg:%s_weight"" % name) in keys:\n\t\t\t\tlinkGrp.create_dataset(""%s.W"" % layerName, data=paramIdx)\n\t\t\t\tparamGrp.create_dataset(str(paramIdx), data=table[""arg:%s_weight"" % name], compression=compress)\n\t\t\t\tparamIdx += 1\n\n\t\t\tif (""arg:%s_bias"" % name) in keys:\n\t\t\t\tlinkGrp.create_dataset(""%s.b"" % layerName, data=paramIdx)\n\t\t\t\tbias = table[""arg:%s_bias"" % name]\n\t\t\t\tparamGrp.create_dataset(str(paramIdx), data=bias.reshape(1, bias.shape[0], 1, 1), compression=compress)\n\t\t\t\tparamIdx += 1\n\n\t\telif op == ""BatchNorm"":\n\t\t\tif (""arg:%s_gamma"" % name) in keys:\n\t\t\t\tlinkGrp.create_dataset(""%s.scale"" % layerName, data=paramIdx)\n\t\t\t\tscale = table[""arg:%s_gamma"" % name]\n\t\t\t\tparamGrp.create_dataset(str(paramIdx), data=scale.reshape(1, scale.shape[0], 1,1), compression=compress)\n\t\t\t\tparamIdx += 1\n\n\t\t\tif (""arg:%s_beta"" % name) in keys:\n\t\t\t\tlinkGrp.create_dataset(""%s.bias"" % layerName, data=paramIdx)\n\t\t\t\tbias = table[""arg:%s_beta"" % name]\n\t\t\t\tparamGrp.create_dataset(str(paramIdx), data=bias.reshape(1, bias.shape[0], 1, 1), compression=compress)\n\t\t\t\tparamIdx += 1\n\n\t\t\tif (""aux:%s_moving_mean"" % name) in keys:\n\t\t\t\tmean = table[""aux:%s_moving_mean"" % name]\n\t\t\t\tattrGrp.create_dataset(""%s.mean"" % layerName, data=mean.reshape(1, mean.shape[0], 1, 1))\n\n\t\t\tif (""aux:%s_moving_var"" % name) in keys:\n\t\t\t\tvar = table[""aux:%s_moving_var"" % name]\n\t\t\t\tattrGrp.create_dataset(""%s.var"" % layerName, data=var.reshape(1, var.shape[0], 1, 1))\n\n\t\telif op == ""FullyConnected"":\n\t\t\tif (""arg:%s_weight"" % name) in keys:\n\t\t\t\tlinkGrp.create_dataset(""%s.W"" % layerName, data=paramIdx)\n\t\t\t\tparamGrp.create_dataset(str(paramIdx), data=table[""arg:%s_weight"" % name].T, compression=compress)\n\t\t\t\tparamIdx += 1\n\n\t\t\tif (""arg:%s_bias"" % name) in keys:\n\t\t\t\tlinkGrp.create_dataset(""%s.b"" % layerName, data=paramIdx)\n\t\t\t\tparamGrp.create_dataset(str(paramIdx), data=table[""arg:%s_bias"" % name], compression=compress)\n\t\t\t\tparamIdx += 1\n\n\ndef main():\n\tmodelname = ""Inception-7-0001.params""\n\tsymbolsname = ""Inception-7-symbol.json""\n\n\tprint(""Deserializing mxnet model ..."")\n\twith open(modelname, mode=""rb"") as file:\n\t\treadHeader(file)\n\t\ttensors = readData(file)\n\t\tkeys = readKeys(file)\n\n\tsymbols = loadSymbols(symbolsname)\n\n\tprint(""Parsing tensor dictionary and saving hdf ..."")\n\tname = os.path.basename(os.path.splitext(modelname)[0])\n\tbuildHdf(keys, tensors, symbols, os.path.splitext(modelname)[0] + "".hdf"", name)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/ONNX/Exporter.py,4,"b'import os\n\nimport numpy as np\nimport onnx, onnx.shape_inference\n\nfrom PuzzleLib.Models.Nets.ResNet import loadResNet\n\nfrom PuzzleLib.Containers.Container import Container\nfrom PuzzleLib.Containers.Sequential import Sequential\nfrom PuzzleLib.Containers.Parallel import Parallel\nfrom PuzzleLib.Containers.Graph import Graph\n\nfrom PuzzleLib.Modules.Conv2D import Conv2D\nfrom PuzzleLib.Modules.BatchNorm2D import BatchNorm2D\nfrom PuzzleLib.Modules.Activation import Activation, relu, leakyRelu\nfrom PuzzleLib.Modules.MaxPool2D import MaxPool2D\nfrom PuzzleLib.Modules.AvgPool2D import AvgPool2D\nfrom PuzzleLib.Modules.Flatten import Flatten\nfrom PuzzleLib.Modules.Linear import Linear\nfrom PuzzleLib.Modules.Replicate import Replicate\nfrom PuzzleLib.Modules.Add import Add\nfrom PuzzleLib.Modules.Concat import Concat\nfrom PuzzleLib.Modules.Identity import Identity\nfrom PuzzleLib.Modules.Dropout import Dropout\nfrom PuzzleLib.Modules.MulAddConst import MulAddConst\nfrom PuzzleLib.Modules.BatchNorm import BatchNorm\nfrom PuzzleLib.Modules.SoftMax import SoftMax\nfrom PuzzleLib.Modules.Split import Split\nfrom PuzzleLib.Modules.Upsample2D import Upsample2D\n\n\nclass ONNXExporter:\n\tdef __init__(self, validate=True, exportWeights=True):\n\t\tself.validate = validate\n\t\tself.exportWeights = exportWeights\n\n\t\tself.nodes = []\n\t\tself.initializer = []\n\n\n\tdef export(self, net, inshape, savepath):\n\t\toutshape = net.dataShapeFrom(inshape)\n\n\t\tinshape = [inshape] if not isinstance(inshape, list) else inshape\n\t\toutshape = [outshape] if not isinstance(outshape, list) else outshape\n\n\t\tinputs = [""data_%s"" % i for i in range(len(inshape))]\n\t\toutputs = self.convertModule(net, net.name, inputs)\n\n\t\tinputs = [\n\t\t\tonnx.helper.make_tensor_value_info(name, onnx.TensorProto.FLOAT, inshape[i])\n\t\t\tfor i, name in enumerate(inputs)\n\t\t]\n\n\t\tinputs.extend(\n\t\t\tonnx.helper.make_tensor_value_info(init.name, init.data_type, init.dims) for init in self.initializer\n\t\t)\n\n\t\toutputs = [\n\t\t\tonnx.helper.make_tensor_value_info(name, onnx.TensorProto.FLOAT, outshape[i])\n\t\t\tfor i, name in enumerate(outputs)\n\t\t]\n\n\t\tgraph = onnx.helper.make_graph(self.nodes, net.name, inputs, outputs, initializer=self.initializer)\n\t\tmodel = onnx.helper.make_model(graph, producer_name=""puzzlelib"")\n\n\t\tif self.validate:\n\t\t\tonnx.checker.check_model(model)\n\n\t\tif not self.exportWeights:\n\t\t\tmodel.graph.ClearField(""initializer"")\n\n\t\tmodel = onnx.shape_inference.infer_shapes(model)\n\t\tonnx.save_model(model, os.path.join(savepath, ""%s.onnx"" % net.name))\n\n\t\treturn model\n\n\n\tdef convertModule(self, module, fullname, inputs):\n\t\tif isinstance(module, Container):\n\t\t\tif isinstance(module, Sequential):\n\t\t\t\treturn self.convertSequential(module, fullname, inputs)\n\n\t\t\telif isinstance(module, Parallel):\n\t\t\t\treturn self.convertParallel(module, fullname, inputs)\n\n\t\t\telif isinstance(module, Graph):\n\t\t\t\treturn self.convertGraph(module, fullname, inputs)\n\n\t\t\telse:\n\t\t\t\traise NotImplementedError(module.__class__.__name__)\n\n\t\telse:\n\t\t\tif isinstance(module, Add):\n\t\t\t\treturn self.convertAdd(fullname, inputs)\n\n\t\t\telif isinstance(module, Concat):\n\t\t\t\treturn self.convertConcat(module, fullname, inputs)\n\n\t\t\tassert len(inputs) == 1\n\t\t\tinp = inputs[0]\n\n\t\t\tif isinstance(module, Conv2D):\n\t\t\t\treturn self.convertConv(module, fullname, inp)\n\n\t\t\telif isinstance(module, (BatchNorm, BatchNorm2D)):\n\t\t\t\treturn self.convertBatchNorm(module, fullname, inp)\n\n\t\t\telif isinstance(module, Activation):\n\t\t\t\treturn self.convertActivation(module, fullname, inp)\n\n\t\t\telif isinstance(module, (Identity, Dropout)):\n\t\t\t\treturn self.convertIdentity(inp)\n\n\t\t\telif isinstance(module, (MaxPool2D, AvgPool2D)):\n\t\t\t\treturn self.convertPool(module, fullname, inp)\n\n\t\t\telif isinstance(module, Flatten):\n\t\t\t\treturn self.convertFlatten(fullname, inp)\n\n\t\t\telif isinstance(module, Linear):\n\t\t\t\treturn self.convertLinear(module, fullname, inp)\n\n\t\t\telif isinstance(module, SoftMax):\n\t\t\t\treturn self.convertSoftmax(fullname, inp)\n\n\t\t\telif isinstance(module, Replicate):\n\t\t\t\treturn self.convertReplicate(module, inp)\n\n\t\t\telif isinstance(module, MulAddConst):\n\t\t\t\treturn self.convertMulAddConst(module, fullname, inp)\n\n\t\t\telif isinstance(module, Split):\n\t\t\t\treturn self.convertSplit(module, fullname, inp)\n\n\t\t\telif isinstance(module, Upsample2D):\n\t\t\t\treturn self.convertUpsample2D(module, fullname, inp)\n\n\t\t\telse:\n\t\t\t\traise NotImplementedError(module.__class__.__name__)\n\n\n\tdef convertSequential(self, seq, fullname, inputs):\n\t\tfor child in seq.graph:\n\t\t\tname = ""%s.%s"" % (fullname, child.name)\n\t\t\tinputs = self.convertModule(child, name, inputs)\n\n\t\treturn inputs\n\n\n\tdef convertParallel(self, parallel, fullname, inputs):\n\t\tassert len(inputs) == len(parallel.graph)\n\n\t\toutputs = []\n\t\tfor i, child in enumerate(parallel.graph):\n\t\t\tname = ""%s.%s"" % (fullname, child.name)\n\t\t\toutputs.append(self.convertModule(child, name, [inputs[i]])[0])\n\n\t\treturn outputs\n\n\n\tdef convertNode(self, node, fullname, inputs, nodes):\n\t\tname = None if node.name is None else ""%s.%s"" % (fullname, node.name)\n\t\tinputs = [inputs[node.name]] if len(node.bwds) == 0 else [nodes[output.name] for output, _ in node.bwds]\n\n\t\toutputs = self.convertModule(node.module, name, inputs)\n\t\tassert len(outputs) == 1\n\n\t\tnodes[node.name] = outputs[0]\n\n\n\tdef convertGraph(self, graph, fullname, inputs):\n\t\tassert len(inputs) == len(graph.inputs)\n\n\t\tnodes = {}\n\t\tinputs = {node.name: inputs[i] for i, node in enumerate(graph.inputs)}\n\n\t\tfor i, inp in enumerate(graph.inputs):\n\t\t\tinp.traverseForward(inp, self.convertNode, fullname, inputs, nodes)\n\n\t\tgraph.reset()\n\t\toutputs = [nodes[output.name] for output in graph.outputs]\n\n\t\treturn outputs\n\n\n\tdef convertAdd(self, fullname, inputs):\n\t\tassert len(inputs) == 2\n\n\t\tself.nodes.append(onnx.helper.make_node(\n\t\t\t""Add"", inputs=inputs, outputs=[fullname]\n\t\t))\n\n\t\treturn [fullname]\n\n\n\tdef convertConcat(self, module, fullname, inp):\n\t\tself.nodes.append(onnx.helper.make_node(\n\t\t\t""Concat"", inputs=inp, outputs=[fullname],\n\t\t\taxis=module.axis\n\t\t))\n\n\t\treturn [fullname]\n\n\n\tdef convertConv(self, module, fullname, inp):\n\t\tassert module.dilation == (1, 1) and module.groups == 1\n\t\tstrides = module.stride\n\n\t\twpad, hpad = module.pad\n\t\tpads = [wpad, hpad, wpad, hpad]\n\n\t\tWname = ""%s.W"" % fullname\n\t\tW = module.W.get()\n\n\t\tself.initializer.append(onnx.helper.make_tensor(\n\t\t\tname=Wname, data_type=onnx.TensorProto.FLOAT, dims=W.shape, vals=W.flatten()\n\t\t))\n\n\t\tinputs = [inp, Wname]\n\n\t\tif module.useBias:\n\t\t\tbiasname = ""%s.b"" % fullname\n\t\t\tbias = module.b.get()\n\n\t\t\tself.initializer.append(onnx.helper.make_tensor(\n\t\t\t\tname=biasname, data_type=onnx.TensorProto.FLOAT, dims=(bias.shape[1], ), vals=bias.flatten()\n\t\t\t))\n\t\t\tinputs.append(biasname)\n\n\t\tself.nodes.append(onnx.helper.make_node(\n\t\t\t""Conv"", inputs=inputs, outputs=[fullname],\n\t\t\tpads=pads, strides=strides\n\t\t))\n\n\t\treturn [fullname]\n\n\n\tdef convertBatchNorm(self, module, fullname, inp):\n\t\tscalename, biasname = ""%s.scale"" % fullname, ""%s.bias"" % fullname\n\t\tmeanname, varname = ""%s.mean"" % fullname, ""%s.var"" % fullname\n\n\t\tscale, bias = module.scale.get().flatten(), module.bias.get().flatten()\n\t\tmean, var = module.mean.get().flatten(), module.var.get().flatten()\n\n\t\tfor name, tensor in [(scalename, scale), (biasname, bias), (meanname, mean), (varname, var)]:\n\t\t\tself.initializer.append(onnx.helper.make_tensor(\n\t\t\t\tname=name, data_type=onnx.TensorProto.FLOAT, dims=tensor.shape, vals=tensor\n\t\t\t))\n\n\t\tself.nodes.append(onnx.helper.make_node(\n\t\t\t""BatchNormalization"", inputs=[inp, scalename, biasname, meanname, varname], outputs=[fullname],\n\t\t\tepsilon=module.epsilon\n\t\t))\n\n\t\treturn [fullname]\n\n\n\tdef convertActivation(self, module, fullname, inp):\n\t\tactType = module.getBlueprint()[""scheme""][""activation""]\n\t\tassert actType in {relu, leakyRelu}\n\n\t\tif actType == relu:\n\t\t\ttyp = ""Relu""\n\t\t\tattrs = {}\n\n\t\telse:\n\t\t\ttyp = ""LeakyRelu""\n\t\t\tattrs = {""alpha"": module.args[0]}\n\n\t\tself.nodes.append(onnx.helper.make_node(typ, inputs=[inp], outputs=[fullname], **attrs))\n\t\treturn [fullname]\n\n\n\t@classmethod\n\tdef convertIdentity(cls, inp):\n\t\treturn [inp]\n\n\n\tdef convertPool(self, module, fullname, inp):\n\t\ttyp = {\n\t\t\tMaxPool2D: ""MaxPool"",\n\t\t\tAvgPool2D: ""AveragePool""\n\t\t}[type(module)]\n\n\t\tstrides = module.stride\n\n\t\twpad, hpad = module.pad\n\t\tpads = [wpad, hpad, wpad, hpad]\n\n\t\tself.nodes.append(onnx.helper.make_node(\n\t\t\ttyp, inputs=[inp], outputs=[fullname],\n\t\t\tkernel_shape=module.size, pads=pads, strides=strides\n\t\t))\n\n\t\treturn [fullname]\n\n\n\tdef convertFlatten(self, fullname, inp):\n\t\tself.nodes.append(onnx.helper.make_node(\n\t\t\t""Flatten"", inputs=[inp], outputs=[fullname],\n\t\t\taxis=1\n\t\t))\n\n\t\treturn [fullname]\n\n\n\tdef convertLinear(self, module, fullname, inp):\n\t\tWname = ""%s.W"" % fullname\n\t\tW = module.W.get()\n\n\t\tself.initializer.append(onnx.helper.make_tensor(\n\t\t\tname=Wname, data_type=onnx.TensorProto.FLOAT, dims=W.shape, vals=W.flatten()\n\t\t))\n\n\t\tmulname = ""%s.mul"" % fullname\n\n\t\tself.nodes.append(onnx.helper.make_node(\n\t\t\t""MatMul"", inputs=[inp, Wname], outputs=[mulname]\n\t\t))\n\n\t\tif module.useBias:\n\t\t\tbiasname = ""%s.b"" % fullname\n\t\t\tbias = module.b.get()\n\n\t\t\tself.initializer.append(onnx.helper.make_tensor(\n\t\t\t\tname=biasname, data_type=onnx.TensorProto.FLOAT, dims=bias.shape, vals=bias\n\t\t\t))\n\n\t\t\tself.nodes.append(onnx.helper.make_node(\n\t\t\t\t""Add"", inputs=[mulname, biasname], outputs=[fullname]\n\t\t\t))\n\n\t\telse:\n\t\t\tfullname = mulname\n\n\t\treturn [fullname]\n\n\n\tdef convertSoftmax(self, fullname, inp):\n\t\tself.nodes.append(onnx.helper.make_node(\n\t\t\t""Softmax"", inputs=[inp], outputs=[fullname],\n\t\t\taxis=1\n\t\t))\n\n\t\treturn [fullname]\n\n\n\t@classmethod\n\tdef convertReplicate(cls, module, inp):\n\t\treturn [inp] * module.times\n\n\n\tdef convertMulAddConst(self, module, fullname, inp):\n\t\taname, bname = ""%s.a"" % fullname, ""%s.b"" % fullname\n\t\ta, b = np.array([module.a], dtype=np.float32), np.array([module.b], dtype=np.float32)\n\n\t\tfor name, tensor in [(aname, a), (bname, b)]:\n\t\t\tself.initializer.append(onnx.helper.make_tensor(\n\t\t\t\tname=name, data_type=onnx.TensorProto.FLOAT, dims=tensor.shape, vals=tensor\n\t\t\t))\n\n\t\tmulname = ""%s.mul"" % fullname\n\n\t\tself.nodes.append(onnx.helper.make_node(\n\t\t\t""Mul"", inputs=[inp, aname], outputs=[mulname]\n\t\t))\n\n\t\tself.nodes.append(onnx.helper.make_node(\n\t\t\t""Add"", inputs=[mulname, bname], outputs=[fullname]\n\t\t))\n\n\t\treturn [fullname]\n\n\n\tdef convertSplit(self, module, fullname, inp):\n\t\toutputs = [""%s_%s"" % (fullname, i) for i in range(len(module.sections))]\n\n\t\tself.nodes.append(onnx.helper.make_node(\n\t\t\t""Split"", inputs=inp, outputs=outputs, axis=module.axis, split=module.sections\n\t\t))\n\n\t\treturn outputs\n\n\n\tdef convertUpsample2D(self, module, fullname, inp):\n\t\tassert module.mode == ""nearest""\n\n\t\troiname = ""%s.roi"" % fullname\n\t\troi = np.array([], dtype=np.float32)\n\n\t\tself.initializer.append(onnx.helper.make_tensor(\n\t\t\tname=roiname, data_type=onnx.TensorProto.FLOAT, dims=roi.shape, vals=roi\n\t\t))\n\n\t\tscalename = ""%s.scales"" % fullname\n\t\tscales = np.array([1.0, 1.0, module.scale, module.scale], dtype=np.float32)\n\n\t\tself.initializer.append(onnx.helper.make_tensor(\n\t\t\tname=scalename, data_type=onnx.TensorProto.FLOAT, dims=scales.shape, vals=scales\n\t\t))\n\n\t\tself.nodes.append(onnx.helper.make_node(\n\t\t\t""Resize"", inputs=[inp, roiname, scalename], outputs=[fullname], mode=""nearest""\n\t\t))\n\n\t\treturn [fullname]\n\n\ndef unittest():\n\tnet = loadResNet(modelpath=""../TestData/ResNet-50-model.hdf"", layers=""50"")\n\tONNXExporter().export(net, inshape=(1, 3, 224, 224), savepath=""../TestData/"")\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Converter/OpenVINO/BuildVINOEngine.py,7,"b'import os\n\nimport numpy as np\n\nfrom PuzzleLib.Containers.Container import Container\nfrom PuzzleLib.Containers.Sequential import Sequential\nfrom PuzzleLib.Containers.Parallel import Parallel\nfrom PuzzleLib.Containers.Graph import Graph\n\nfrom PuzzleLib.Modules.Activation import Activation, relu, leakyRelu, sigmoid\nfrom PuzzleLib.Modules.Add import Add\nfrom PuzzleLib.Modules.AvgPool2D import AvgPool2D\nfrom PuzzleLib.Modules.BatchNorm import BatchNorm\nfrom PuzzleLib.Modules.BatchNorm2D import BatchNorm2D\nfrom PuzzleLib.Modules.Concat import Concat\nfrom PuzzleLib.Modules.Conv2D import Conv2D\nfrom PuzzleLib.Modules.Dropout import Dropout\nfrom PuzzleLib.Modules.Flatten import Flatten\nfrom PuzzleLib.Modules.Identity import Identity\nfrom PuzzleLib.Modules.Linear import Linear\nfrom PuzzleLib.Modules.MaxPool2D import MaxPool2D\nfrom PuzzleLib.Modules.MulAddConst import MulAddConst\nfrom PuzzleLib.Modules.Replicate import Replicate\nfrom PuzzleLib.Modules.SoftMax import SoftMax\nfrom PuzzleLib.Modules.Split import Split\nfrom PuzzleLib.Modules.Upsample2D import Upsample2D\n\nfrom PuzzleLib.Converter.OpenVINO import Driver\nfrom PuzzleLib.Converter.OpenVINO.VINOEngine import VINOEngine, genEngineName\n\n\ndef buildVINOEngine(net, inshape, savepath, returnEngine=True):\n\toutshape = net.dataShapeFrom(inshape)\n\n\tinshape = inshape if isinstance(inshape, list) else [inshape]\n\toutshape = outshape if isinstance(outshape, list) else [outshape]\n\n\tbatchsize = inshape[0][0]\n\tinshape, outshape = [sh[1:] for sh in inshape], [sh[1:] for sh in outshape]\n\n\tengineName = genEngineName(net.name, inshape, outshape)\n\n\txmlpath, binpath = os.path.join(savepath, ""%s.xml"" % engineName), os.path.join(savepath, ""%s.bin"" % engineName)\n\tconvert(net, inshape, xmlpath, binpath)\n\n\tif returnEngine:\n\t\treturn VINOEngine(batchsize, xmlpath, binpath, inshape, outshape)\n\n\ndef convert(net, inshape, xmlpath, binpath):\n\tgraph = Driver.createNetwork(net.name)\n\n\tinshape = inshape if isinstance(inshape, list) else [inshape]\n\tinputs = [graph.addInput(""data_%s"" % i, shape) for i, shape in enumerate(inshape)]\n\n\toutput = convertModule(net, net.name, graph, inputs)\n\n\tfor i, out in enumerate(output):\n\t\tgraph.markOutput(out, ""outdata_%s"" % i)\n\n\tgraph.build(xmlpath, binpath)\n\n\ndef numpyPtr(ary):\n\treturn ary.__array_interface__[""data""][0]\n\n\ndef convertModule(module, fullname, graph, inputs):\n\tif isinstance(module, Container):\n\t\tif isinstance(module, Sequential):\n\t\t\treturn convertSequential(module, fullname, graph, inputs)\n\n\t\telif isinstance(module, Parallel):\n\t\t\treturn convertParallel(module, fullname, graph, inputs)\n\n\t\telif isinstance(module, Graph):\n\t\t\treturn convertGraph(module, fullname, graph, inputs)\n\n\t\telse:\n\t\t\traise NotImplementedError(module.__class__.__name__)\n\n\telse:\n\t\tinshape = [tuple(inp.shape) for inp in inputs]\n\t\tshape = module.dataShapeFrom(inshape[0] if len(inshape) == 1 else inshape)\n\n\t\tif isinstance(module, Add):\n\t\t\treturn convertAdd(fullname, graph, inputs)\n\n\t\telif isinstance(module, Concat):\n\t\t\treturn convertConcat(module, fullname, graph, inputs, shape)\n\n\t\tassert len(inputs) == 1\n\t\tinp = inputs[0]\n\n\t\tif isinstance(module, Conv2D):\n\t\t\treturn convertConv(module, fullname, graph, inp, shape)\n\n\t\telif isinstance(module, (BatchNorm2D, BatchNorm)):\n\t\t\treturn convertBatchNorm(module, fullname, graph, inp)\n\n\t\telif isinstance(module, Activation):\n\t\t\treturn convertActivation(module, fullname, graph, inp)\n\n\t\telif isinstance(module, (Identity, Dropout)):\n\t\t\treturn convertIdentity(inp)\n\n\t\telif isinstance(module, Replicate):\n\t\t\treturn convertReplicate(module, inp)\n\n\t\telif isinstance(module, (MaxPool2D, AvgPool2D)):\n\t\t\treturn convertPool2D(module, fullname, graph, inp, shape)\n\n\t\telif isinstance(module, Flatten):\n\t\t\treturn convertFlatten(inp, fullname, graph, shape)\n\n\t\telif isinstance(module, Linear):\n\t\t\treturn convertLinear(module, fullname, graph, inp, shape)\n\n\t\telif isinstance(module, SoftMax):\n\t\t\treturn convertSoftmax(fullname, graph, inp)\n\n\t\telif isinstance(module, MulAddConst):\n\t\t\treturn convertMulAddConst(module, fullname, graph, inp)\n\n\t\telif isinstance(module, Split):\n\t\t\treturn convertSplit(module, fullname, graph, inp, shape)\n\n\t\telif isinstance(module, Upsample2D):\n\t\t\treturn convertUpsample2D(module, fullname, graph, inp)\n\n\t\telse:\n\t\t\traise NotImplementedError(module.__class__.__name__)\n\n\ndef convertSequential(seq, fullname, graph, inputs):\n\tfor child in seq.graph:\n\t\tname = None if child.name is None else ""%s.%s"" % (fullname, child.name)\n\t\tinputs = convertModule(child, name, graph, inputs)\n\n\treturn inputs\n\n\ndef convertParallel(parallel, fullname, graph, inputs):\n\tassert len(inputs) == len(parallel.graph)\n\n\toutputs = []\n\tfor i, child in enumerate(parallel.graph):\n\t\tname = None if child.name is None else ""%s.%s"" % (fullname, child.name)\n\t\toutputs.append(convertModule(child, name, graph, [inputs[i]])[0])\n\n\treturn outputs\n\n\ndef convertNode(node, fullname, graph, inputs, nodes):\n\tname = None if node.name is None else ""%s.%s"" % (fullname, node.name)\n\tinputs = [inputs[node.name]] if len(node.bwds) == 0 else [nodes[output.name] for output, _ in node.bwds]\n\n\toutputs = convertModule(node.module, name, graph, inputs)\n\tassert len(outputs) == 1\n\n\tnodes[node.name] = outputs[0]\n\n\ndef convertGraph(hostgraph, fullname, devgraph, inputs):\n\tassert len(inputs) == len(hostgraph.inputs)\n\n\tnodes = {}\n\tinputs = {node.name: inputs[i] for i, node in enumerate(hostgraph.inputs)}\n\n\tfor i, inp in enumerate(hostgraph.inputs):\n\t\tinp.traverseForward(inp, convertNode, fullname, devgraph, inputs, nodes)\n\n\thostgraph.reset()\n\toutputs = [nodes[output.name] for output in hostgraph.outputs]\n\n\treturn outputs\n\n\ndef convertAdd(fullname, graph, inputs):\n\tassert len(inputs) == 2\n\toutput = graph.addAdd(inputs[0], inputs[1], fullname)\n\n\treturn [output]\n\n\ndef convertConcat(module, fullname, graph, inputs, shape):\n\tassert module.axis == 1\n\toutput = graph.addConcat(inputs, shape, fullname)\n\n\treturn [output]\n\n\ndef convertConv(module, fullname, graph, inp, shape):\n\tassert module.groups == 1 and module.dilation == (1, 1)\n\n\tW = module.W.get()\n\n\tb = module.b.get() if module.useBias else None\n\tbptr = 0 if b is None else numpyPtr(b)\n\n\toutput = graph.addConvolution(inp, shape, module.W.shape[2:], numpyPtr(W), bptr, module.stride, module.pad, fullname)\n\treturn [output]\n\n\ndef convertBatchNorm(module, fullname, graph, inp):\n\tmean, var = module.mean.get(), module.var.get()\n\tscale, bias = module.scale.get(), module.bias.get()\n\n\teps = module.epsilon\n\n\tshift = (bias - scale * mean / np.sqrt(var + eps)).ravel()\n\tscale = (scale / np.sqrt(var + eps)).ravel()\n\n\toutput = graph.addScale(inp, scale.shape[0], numpyPtr(scale), numpyPtr(shift), fullname)\n\treturn [output]\n\n\ndef convertActivation(module, fullname, graph, inp):\n\tactType = module.getBlueprint()[""scheme""][""activation""]\n\n\ttyp = {\n\t\trelu: Driver.ActivationType.relu,\n\t\tleakyRelu: Driver.ActivationType.relu,\n\t\tsigmoid: Driver.ActivationType.sigmoid\n\t}[actType]\n\n\talpha = 0.0\n\tif actType == leakyRelu:\n\t\talpha = module.actArgs[0]\n\n\toutput = graph.addActivation(inp, typ, alpha, fullname)\n\treturn [output]\n\n\ndef convertIdentity(inp):\n\treturn [inp]\n\n\ndef convertReplicate(module, inp):\n\treturn [inp] * module.times\n\n\ndef convertPool2D(module, fullname, graph, inp, shape):\n\toutput = graph.addPooling(\n\t\tinp, shape, True if isinstance(module, AvgPool2D) else False, module.size, module.stride, module.pad, fullname\n\t)\n\n\treturn [output]\n\n\ndef convertFlatten(inp, fullname, graph, shape):\n\toutput = graph.addFlatten(inp, shape, fullname)\n\treturn [output]\n\n\ndef convertLinear(module, fullname, graph, inp, shape):\n\tW = module.W.get().T.ravel()\n\n\tb = module.b.get().ravel() if module.useBias else None\n\tbptr = 0 if b is None else numpyPtr(b)\n\n\toutput = graph.addLinear(inp, shape, numpyPtr(W), bptr, fullname)\n\treturn [output]\n\n\ndef convertSoftmax(fullname, graph, inp):\n\toutput = graph.addSoftmax(inp, fullname)\n\treturn [output]\n\n\ndef convertMulAddConst(module, fullname, graph, inp):\n\tc = inp.shape[1]\n\n\tshift = np.array([module.b] * c, dtype=np.float32)\n\tscale = np.array([module.a] * c, dtype=np.float32)\n\n\toutput = graph.addScale(inp, c, numpyPtr(scale), numpyPtr(shift), fullname)\n\treturn [output]\n\n\ndef convertSplit(module, fullname, graph, inp, shape):\n\tassert module.axis == 1\n\n\toutput = graph.addSplit(inp, module.axis, shape, fullname)\n\treturn output\n\n\ndef convertUpsample2D(module, fullname, graph, inp):\n\tassert module.mode == ""nearest""\n\n\toutput = graph.addUpsample(inp, module.scale, fullname)\n\treturn [output]\n'"
Converter/OpenVINO/VINOEngine.py,2,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\n\nfrom PuzzleLib.Modules.Module import Module\nfrom PuzzleLib.Converter.OpenVINO import Driver\n\n\ndef genEngineName(name, inshape, outshape):\n\tinshape = "","".join(""-"".join(str(s) for s in sh) for sh in inshape)\n\toutshape = "","".join(""-"".join(str(s) for s in sh) for sh in outshape)\n\n\tfullname = ""%s.%s.%s"" % (name, inshape, outshape)\n\treturn fullname\n\n\ndef parseEngineShape(enginename):\n\tsubnames = enginename.split(sep=""."")\n\tinshape, outshape = subnames[1], subnames[2]\n\n\tinshape = [tuple(int(v) for v in sh.split(sep=""-"")) for sh in inshape.split(sep="","")]\n\tinshape = inshape[0] if len(inshape) == 1 else inshape\n\n\toutshape = [tuple(int(v) for v in sh.split(sep=""-"")) for sh in outshape.split(sep="","")]\n\toutshape = outshape[0] if len(outshape) == 1 else outshape\n\n\treturn inshape, outshape\n\n\nclass VINOEngine(Module):\n\tdef __init__(self, batchsize, xmlpath, binpath, inshape=None, outshape=None, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tshapes = [inshape, outshape]\n\n\t\tif inshape is None or outshape is None:\n\t\t\tparsedInshape, parsedOutshape = parseEngineShape(xmlpath)\n\t\t\tshapes = [parsedInshape if inshape is None else inshape, parsedOutshape if outshape is None else outshape]\n\n\t\tshapes = [sh if isinstance(sh, list) else [sh] for sh in shapes]\n\t\tshapes = [[(batchsize, ) + s for s in sh] for sh in shapes]\n\t\tshapes = [sh[0] if len(sh) == 1 else sh for sh in shapes]\n\n\t\tself.inshape, self.outshape = shapes\n\t\tself.engine = Driver.VINOEngine(batchsize, xmlpath, binpath, ""CPU"")\n\n\n\tdef updateData(self, data):\n\t\tif isinstance(self.outshape, list):\n\t\t\tself.data = [gpuarray.empty(outshape, dtype=np.float32, allocator=memPool) for outshape in self.outshape]\n\t\telse:\n\t\t\tself.data = gpuarray.empty(self.outshape, dtype=np.float32, allocator=memPool)\n\n\t\tdata = data if isinstance(data, list) else [data]\n\t\tinputs = {""data_%s"" % i: (dat.ptr, dat.nbytes) for i, dat in enumerate(data)}\n\n\t\toutdata = self.data if isinstance(self.data, list) else [self.data]\n\t\toutputs = {""outdata_%s"" % i: (data.ptr, data.nbytes) for i, data in enumerate(outdata)}\n\n\t\tself.engine.infer(outputs, inputs)\n\n\n\tdef updateGrad(self, grad):\n\t\tassert False\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn self.outshape\n\n\n\tdef checkDataShape(self, shape):\n\t\tif isinstance(shape, list):\n\t\t\tfor i, sh in enumerate(shape):\n\t\t\t\tif sh != self.inshape[i]:\n\t\t\t\t\traise ValueError(""Shape %s is not equal to shape %s on index %s"" % (sh, self.inshape[i], i))\n\n\t\telif shape != self.inshape:\n\t\t\traise ValueError(""Data shape must be equal %s (was given %s)"" % (self.inshape, shape))\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tassert False\n\n\n\tdef checkGradShape(self, shape):\n\t\tassert False\n'"
Converter/TensorRT/BuildRTEngine.py,14,"b'import os\n\nimport numpy as np\n\nfrom PuzzleLib.Containers.Container import Container\nfrom PuzzleLib.Containers.Sequential import Sequential\nfrom PuzzleLib.Containers.Parallel import Parallel\nfrom PuzzleLib.Containers.Graph import Graph\n\nfrom PuzzleLib.Modules.Activation import Activation, sigmoid, tanh, relu, leakyRelu, clip\nfrom PuzzleLib.Modules.Add import Add\nfrom PuzzleLib.Modules.AvgPool2D import AvgPool2D\nfrom PuzzleLib.Modules.BatchNorm import BatchNorm\nfrom PuzzleLib.Modules.BatchNorm1D import BatchNorm1D\nfrom PuzzleLib.Modules.BatchNorm2D import BatchNorm2D\nfrom PuzzleLib.Modules.Concat import Concat\nfrom PuzzleLib.Modules.Conv1D import Conv1D\nfrom PuzzleLib.Modules.Conv2D import Conv2D\nfrom PuzzleLib.Modules.CrossMapLRN import CrossMapLRN\nfrom PuzzleLib.Modules.Deconv2D import Deconv2D\nfrom PuzzleLib.Modules.Dropout import Dropout\nfrom PuzzleLib.Modules.Flatten import Flatten\nfrom PuzzleLib.Modules.GroupLinear import GroupLinear\nfrom PuzzleLib.Modules.Identity import Identity\nfrom PuzzleLib.Modules.Linear import Linear\nfrom PuzzleLib.Modules.MaxPool2D import MaxPool2D\nfrom PuzzleLib.Modules.MoveAxis import MoveAxis\nfrom PuzzleLib.Modules.MulAddConst import MulAddConst\nfrom PuzzleLib.Modules.Pad1D import Pad1D, PadMode\nfrom PuzzleLib.Modules.PRelu import PRelu\nfrom PuzzleLib.Modules.Replicate import Replicate\nfrom PuzzleLib.Modules.Reshape import Reshape\nfrom PuzzleLib.Modules.RNN import RNN\nfrom PuzzleLib.Modules.SoftMax import SoftMax\nfrom PuzzleLib.Modules.Split import Split\nfrom PuzzleLib.Modules.Sum import Sum\nfrom PuzzleLib.Modules.SwapAxes import SwapAxes\nfrom PuzzleLib.Modules.Upsample2D import Upsample2D\n\nfrom PuzzleLib.Converter.TensorRT import Driver\nfrom PuzzleLib.Converter.TensorRT.DataCalibrator import CalibratorError\nfrom PuzzleLib.Converter.TensorRT.RTEngine import RTEngine, DataType, RTEngineType, genEngineName\n\n\nclass ConverterError(Exception):\n\tpass\n\n\ndef buildRTEngine(net, inshape, savepath, dtype, calibrator=None, workspace=1 << 30, returnEngine=True, log=True):\n\toutshape = net.dataShapeFrom(inshape)\n\tbatchsize, inshape = inshape[0], inshape[1:]\n\n\tengineName = genEngineName(net.name, dtype, (batchsize, ) + inshape, outshape)\n\tsavepath = os.path.join(savepath, engineName)\n\n\tconvert(net, batchsize, inshape, dtype, calibrator, workspace, savepath, log)\n\n\tif returnEngine:\n\t\treturn RTEngine(savepath, RTEngineType.puzzle, (batchsize, ) + inshape, outshape, log)\n\n\ndef buildRTEngineFromCaffe(model, inshape, outshape, outlayers, savepath, dtype, calibrator=None, workspace=1 << 22,\n\t\t\t\t\t\t   returnEngine=True, log=True):\n\tprototxt, caffemodel = model\n\n\tengineName = genEngineName(os.path.splitext(os.path.basename(caffemodel))[0], dtype, inshape, outshape)\n\tsavepath = os.path.join(savepath, engineName)\n\n\tbatchsize, inshape = inshape[0], inshape[1:]\n\tDriver.buildRTEngineFromCaffe(\n\t\tprototxt, caffemodel, batchsize, outlayers, dtype, calibrator, workspace, savepath, log\n\t)\n\n\tif returnEngine:\n\t\treturn RTEngine(savepath, RTEngineType.caffe, (batchsize, ) + inshape, outshape, log)\n\n\ndef buildRTEngineFromOnnx(model, inshape, outshape, savepath, dtype, calibrator=None, workspace=1 << 22,\n\t\t\t\t\t\t  returnEngine=True, log=True):\n\tengineName = genEngineName(os.path.splitext(os.path.basename(model))[0], dtype, inshape, outshape)\n\tsavepath = os.path.join(savepath, engineName)\n\n\tbatchsize, inshape = inshape[0], inshape[1:]\n\tDriver.buildRTEngineFromOnnx(model, batchsize, dtype, calibrator, workspace, savepath, log)\n\n\tif returnEngine:\n\t\treturn RTEngine(savepath, RTEngineType.onnx, (batchsize, ) + inshape, outshape, log)\n\n\ndef convert(net, batchsize, inshape, dtype, calibrator, workspace, savepath, log):\n\tgraph = Driver.createNetwork(log)\n\n\tif dtype == DataType.float16:\n\t\tif not graph.platformHasFastFp16():\n\t\t\traise ConverterError(""Platform has no fast fp16 support"")\n\n\t\tgraph.setFp16Mode(True)\n\n\telif dtype == DataType.int8:\n\t\tif not graph.platformHasFastInt8():\n\t\t\traise ConverterError(""Platform has no fast int8 support"")\n\n\t\tgraph.setInt8Mode(True)\n\t\tgraph.setInt8Calibrator(calibrator)\n\n\tif not isinstance(inshape, list):\n\t\tinshape = [inshape]\n\n\tif calibrator is not None:\n\t\tcalshape = calibrator.getDataShape()\n\t\tassert len(inshape) == 1\n\n\t\tif calshape != inshape[0]:\n\t\t\traise CalibratorError(""Calibrator data has shape %s, network has input shape %s"" % (calshape, inshape[0]))\n\n\tinputs = []\n\tfor i, shape in enumerate(inshape):\n\t\tinputs.append(graph.addInput(""data_%s"" % i, DataType.float32, shape))\n\n\tholder = []\n\toutput = convertModule(net, net.name, graph, inputs, holder)\n\n\tfor i, out in enumerate(output):\n\t\tout.setName(""output_%s"" % i)\n\t\tgraph.markOutput(out)\n\n\tgraph.setMaxBatchSize(batchsize)\n\tgraph.setMaxWorkspaceSize(workspace)\n\n\tgraph.buildCudaEngine(savepath)\n\n\ndef numpyPtr(ary):\n\treturn ary.__array_interface__[""data""][0]\n\n\ndef convertModule(module, fullname, graph, inputs, holder):\n\tif isinstance(module, Container):\n\t\tif isinstance(module, Sequential):\n\t\t\treturn convertSequential(module, fullname, graph, inputs, holder)\n\n\t\telif isinstance(module, Parallel):\n\t\t\treturn convertParallel(module, fullname, graph, inputs, holder)\n\n\t\telif isinstance(module, Graph):\n\t\t\treturn convertGraph(module, fullname, graph, inputs, holder)\n\n\t\telse:\n\t\t\traise NotImplementedError(module.__class__.__name__)\n\n\telse:\n\t\tif isinstance(module, Add):\n\t\t\treturn convertAdd(fullname, graph, inputs)\n\t\telif isinstance(module, Concat):\n\t\t\treturn convertConcat(module, fullname, graph, inputs)\n\n\t\tassert len(inputs) == 1\n\t\tinp = inputs[0]\n\n\t\tif isinstance(module, (Conv2D, Deconv2D)):\n\t\t\treturn convertConv2D(module, fullname, graph, inp, holder)\n\n\t\telif isinstance(module, BatchNorm2D):\n\t\t\treturn convertBatchNorm2D(module, fullname, graph, inp, holder)\n\n\t\telif isinstance(module, Conv1D):\n\t\t\treturn convertConv1D(module, fullname, graph, inp, holder)\n\n\t\telif isinstance(module, BatchNorm1D):\n\t\t\treturn convertBatchNorm1D(module, fullname, graph, inp, holder)\n\n\t\telif isinstance(module, Activation):\n\t\t\treturn convertActivation(module, fullname, graph, inp)\n\n\t\telif isinstance(module, (Identity, Dropout)):\n\t\t\treturn convertIdentity(inp)\n\n\t\telif isinstance(module, Replicate):\n\t\t\treturn convertReplicate(module, inp)\n\n\t\telif isinstance(module, (MaxPool2D, AvgPool2D)):\n\t\t\treturn convertPool2D(module, fullname, graph, inp)\n\n\t\telif isinstance(module, CrossMapLRN):\n\t\t\treturn convertCrossMapLRN(module, fullname, graph, inp)\n\n\t\telif isinstance(module, Flatten):\n\t\t\treturn convertFlatten(inp, fullname, graph)\n\n\t\telif isinstance(module, Linear):\n\t\t\treturn convertLinear(module, fullname, graph, inp, holder)\n\n\t\telif isinstance(module, SoftMax):\n\t\t\treturn convertSoftmax(fullname, graph, inp)\n\n\t\telif isinstance(module, SwapAxes):\n\t\t\treturn convertSwapAxes(module, fullname, graph, inp)\n\n\t\telif isinstance(module, MoveAxis):\n\t\t\treturn convertMoveAxis(module, fullname, graph, inp)\n\n\t\telif isinstance(module, Split):\n\t\t\treturn convertSplit(module, fullname, graph, inp)\n\n\t\telif isinstance(module, Reshape):\n\t\t\treturn convertReshape(module, fullname, graph, inp)\n\n\t\telif isinstance(module, GroupLinear):\n\t\t\treturn convertGroupLinear(module, fullname, graph, inp, holder)\n\n\t\telif isinstance(module, MulAddConst):\n\t\t\treturn convertMulAddConst(module, fullname, graph, inp, holder)\n\n\t\telif isinstance(module, BatchNorm):\n\t\t\treturn convertBatchNorm(module, fullname, graph, inp, holder)\n\n\t\telif isinstance(module, Sum):\n\t\t\treturn convertSum(module, fullname, graph, inp)\n\n\t\telif isinstance(module, RNN):\n\t\t\treturn convertRNN(module, fullname, graph, inp, holder)\n\n\t\telif isinstance(module, Upsample2D):\n\t\t\treturn convertUpsample2D(module, fullname, graph, inp)\n\n\t\telif isinstance(module, PRelu):\n\t\t\treturn convertPRelu(module, fullname, graph, inp, holder)\n\n\t\telif isinstance(module, Pad1D):\n\t\t\treturn convertPad1D(module, fullname, graph, inp)\n\n\t\telse:\n\t\t\traise NotImplementedError(module.__class__.__name__)\n\n\ndef convertSequential(seq, fullname, graph, inputs, holder):\n\tfor child in seq.graph:\n\t\tname = None if child.name is None else ""%s.%s"" % (fullname, child.name)\n\t\tinputs = convertModule(child, name, graph, inputs, holder)\n\n\treturn inputs\n\n\ndef convertParallel(parallel, fullname, graph, inputs, holder):\n\tassert len(inputs) == len(parallel.graph)\n\n\toutputs = []\n\tfor i, child in enumerate(parallel.graph):\n\t\tname = None if child.name is None else ""%s.%s"" % (fullname, child.name)\n\t\toutputs.append(convertModule(child, name, graph, [inputs[i]], holder)[0])\n\n\treturn outputs\n\n\ndef convertNode(node, fullname, graph, inputs, nodes, holder):\n\tname = None if node.name is None else ""%s.%s"" % (fullname, node.name)\n\tinputs = [inputs[node.name]] if len(node.bwds) == 0 else [nodes[output.name] for output, _ in node.bwds]\n\n\toutputs = convertModule(node.module, name, graph, inputs, holder)\n\tassert len(outputs) == 1\n\n\tnodes[node.name] = outputs[0]\n\n\ndef convertGraph(hostgraph, fullname, devgraph, inputs, holder):\n\tassert len(inputs) == len(hostgraph.inputs)\n\n\tnodes = {}\n\tinputs = {node.name: inputs[i] for i, node in enumerate(hostgraph.inputs)}\n\n\tfor i, inp in enumerate(hostgraph.inputs):\n\t\tinp.traverseForward(inp, convertNode, fullname, devgraph, inputs, nodes, holder)\n\n\thostgraph.reset()\n\toutputs = [nodes[output.name] for output in hostgraph.outputs]\n\n\treturn outputs\n\n\ndef convertAdd(fullname, graph, inputs):\n\tassert len(inputs) == 2\n\toutput = graph.addAdd(inputs[0], inputs[1], fullname)\n\n\treturn [output]\n\n\ndef convertConcat(module, fullname, graph, inputs):\n\tassert module.axis == 1\n\toutput = graph.addConcatenation(inputs, fullname)\n\n\treturn [output]\n\n\ndef convertConv2D(module, fullname, graph, inp, holder):\n\tassert module.groups == 1\n\tassert not isinstance(module, Deconv2D) or module.dilation == (1, 1)\n\n\tW = module.W.get().flatten()\n\tholder.append(W)\n\n\tb = module.b.get().flatten() if module.useBias else None\n\tif b is None:\n\t\tbptr, bsize = 0, 0\n\telse:\n\t\tbptr, bsize = numpyPtr(b), b.size\n\t\tholder.append(b)\n\n\tisDeconvolution = True if isinstance(module, Deconv2D) else False\n\toutmaps = module.W.shape[1] if isDeconvolution else module.W.shape[0]\n\n\toutput = graph.addConvolution(\n\t\tinp, outmaps, module.W.shape[2:], numpyPtr(W), W.size, bptr, bsize, module.stride, module.pad, module.dilation,\n\t\tisDeconvolution, fullname\n\t)\n\treturn [output]\n\n\ndef convertBatchNorm2D(module, fullname, graph, inp, holder):\n\tmean = module.mean.get()\n\tvar = module.var.get()\n\n\tscale = module.scale.get()\n\tbias = module.bias.get()\n\n\teps = module.epsilon\n\n\tshift = (bias - scale * mean / np.sqrt(var + eps)).flatten()\n\tscale = (scale / np.sqrt(var + eps)).flatten()\n\tpower = np.ones(mean.shape, dtype=mean.dtype).flatten()\n\n\tholder.extend([shift, scale, power])\n\toutput = graph.addScale(inp, numpyPtr(shift), numpyPtr(scale), numpyPtr(power), shift.size, fullname)\n\n\treturn [output]\n\n\ndef convertConv1D(module, fullname, graph, inp, holder):\n\tshape = tuple(inp.shape)\n\tassert len(shape) == 2\n\n\toutput = graph.addReshape(inp, (shape[0], 1, shape[1]), ""%s_inshape"" % fullname)\n\t[output] = convertConv2D(module, fullname, graph, output, holder)\n\n\tshape = output.shape\n\toutput = graph.addReshape(output, (shape[0], shape[2]), ""%s_outshape"" % fullname)\n\n\treturn [output]\n\n\ndef convertBatchNorm1D(module, fullname, graph, inp, holder):\n\tshape = tuple(inp.shape)\n\tassert len(shape) == 2\n\n\toutput = graph.addReshape(inp, (shape[0], 1, shape[1]), ""%s_inshape"" % fullname)\n\t[output] = convertBatchNorm2D(module, fullname, graph, output, holder)\n\n\toutput = graph.addReshape(output, shape, ""%s_outshape"" % fullname)\n\treturn [output]\n\n\ndef convertActivation(module, fullname, graph, inp):\n\tactType = module.getBlueprint()[""scheme""][""activation""]\n\tassert actType in {relu, sigmoid, tanh, leakyRelu, clip}\n\n\ttyp = {\n\t\trelu: Driver.ActivationType.relu,\n\t\tleakyRelu: Driver.ActivationType.leakyRelu,\n\t\tclip: Driver.ActivationType.clip,\n\t\tsigmoid: Driver.ActivationType.sigmoid,\n\t\ttanh: Driver.ActivationType.tanh\n\t}[actType]\n\n\talpha, beta = 0, 0\n\tif actType == leakyRelu:\n\t\talpha = module.actArgs[0]\n\telif actType == clip:\n\t\talpha, beta = module.actArgs\n\n\toutput = graph.addActivation(inp, typ, alpha, beta, fullname)\n\treturn [output]\n\n\ndef convertIdentity(inp):\n\treturn [inp]\n\n\ndef convertReplicate(module, inp):\n\treturn [inp] * module.times\n\n\ndef convertPool2D(module, fullname, graph, inp):\n\toutput = graph.addPooling(\n\t\tinp, True if isinstance(module, AvgPool2D) else False, module.size, module.stride, module.pad, fullname\n\t)\n\n\treturn [output]\n\n\ndef convertCrossMapLRN(module, fullname, graph, inp):\n\toutput = graph.addCrossMapLRN(inp, module.N, module.alpha, module.beta, module.K, fullname)\n\treturn [output]\n\n\ndef convertFlatten(inp, fullname, graph):\n\toutput = graph.addFlatten(inp, fullname)\n\treturn [output]\n\n\ndef convertLinear(module, fullname, graph, inp, holder):\n\tW = module.W.get().T.flatten()\n\tholder.append(W)\n\n\tb = module.b.get().flatten() if module.useBias else None\n\tif b is None:\n\t\tbptr, bsize = 0, 0\n\telse:\n\t\tbptr, bsize = numpyPtr(b), b.size\n\t\tholder.append(b)\n\n\toutput = graph.addLinear(inp, module.W.shape[1], numpyPtr(W), W.size, bptr, bsize, fullname)\n\treturn [output]\n\n\ndef convertSoftmax(fullname, graph, inp):\n\toutput = graph.addSoftMax(inp, fullname)\n\treturn [output]\n\n\ndef convertSwapAxes(module, fullname, graph, inp):\n\tif module.axis1 > 0 and module.axis2 > 0:\n\t\toutput = graph.addSwapAxes(inp, module.axis1 - 1, module.axis2 - 1, fullname)\n\n\telse:\n\t\toutput = inp\n\t\tprint(\n\t\t\t""Warning on module %s: ignoring swap for axes %s and %s - "" % (fullname, module.axis1, module.axis2) + \\\n\t\t\t""TensorRT does not support shuffles on batch axis. Assuming correct order of model batch shuffles ..."",\n\t\t\tflush=True\n\t\t)\n\n\treturn [output]\n\n\ndef convertMoveAxis(module, fullname, graph, inp):\n\tassert module.src > 0 and module.dst > 0\n\n\toutput = graph.addMoveAxis(inp, module.src - 1, module.dst - 1, fullname)\n\treturn [output]\n\n\ndef convertSplit(module, fullname, graph, inp):\n\tassert module.axis == 1\n\n\toutput = graph.addSplit(inp, module.axis, module.sections, fullname)\n\treturn output\n\n\ndef convertReshape(module, fullname, graph, inp):\n\toutput = graph.addReshape(inp, module.shape[1:], fullname)\n\treturn [output]\n\n\ndef convertGroupLinear(module, fullname, graph, inp, holder):\n\tassert module.inmode == ""full"" and module.wmode == ""one""\n\tassert not module.transpW\n\n\tgroups = inp.shape[0]\n\n\tW = module.W.get()[0].flatten()\n\tholder.append(W)\n\n\tb = module.b.get().flatten() if module.useBias else None\n\tif b is None:\n\t\tbptr, bsize = 0, 0\n\telse:\n\t\tb = np.tile(b, reps=groups)\n\t\tbptr, bsize = numpyPtr(b), b.size\n\n\t\tholder.append(b)\n\n\toutput = graph.addGroupLinear(\n\t\tinp, groups, module.W.shape[1], module.W.shape[2], numpyPtr(W), W.size, bptr, b.size, fullname\n\t)\n\treturn [output]\n\n\ndef convertMulAddConst(module, fullname, graph, inp, holder):\n\tc = inp.shape[0]\n\n\tshift = np.array([module.b] * c, dtype=np.float32)\n\tscale = np.array([module.a] * c, dtype=np.float32)\n\tpower = np.ones((c, ), dtype=np.float32)\n\n\tholder.extend([shift, scale, power])\n\n\toutput = graph.addScale(inp, numpyPtr(shift), numpyPtr(scale), numpyPtr(power), c, fullname)\n\treturn [output]\n\n\ndef convertBatchNorm(module, fullname, graph, inp, holder):\n\tshape = tuple(inp.shape)\n\tassert len(shape) == 1\n\n\toutput = graph.addReshape(inp, shape + (1, 1), ""%s_inshape"" % fullname)\n\t[output] = convertBatchNorm2D(module, fullname, graph, output, holder)\n\n\toutput = graph.addReshape(output, shape, ""%s_outshape"" % fullname)\n\treturn [output]\n\n\ndef convertSum(module, fullname, graph, inp):\n\toutput = graph.addSum(inp, module.axis, fullname)\n\treturn [output]\n\n\ndef convertRNN(module, fullname, graph, inp, holder):\n\tassert module.getSequences\n\tseqlen = inp.shape[0]\n\n\tmode = {\n\t\t""relu"": Driver.RNNMode.relu,\n\t\t""tanh"": Driver.RNNMode.tanh,\n\t\t""lstm"": Driver.RNNMode.lstm,\n\t\t""gru"": Driver.RNNMode.gru\n\t}[module.mode]\n\n\tdirection = {\n\t\t""uni"": Driver.RNNDirection.uni,\n\t\t""bi"": Driver.RNNDirection.bi\n\t}[module.direction]\n\n\tinputMode = Driver.RNNInputMode.linear\n\n\tparams = [\n\t\t{name: param.get() for name, param in module.params[key].items()} for key in sorted(module.params.keys())\n\t]\n\tholder.append(params)\n\n\tkeys = {\n\t\t""relu"": [""wi"", ""ri""],\n\t\t""tanh"": [""wi"", ""ri""],\n\t\t""lstm"": [""wf"", ""wi"", ""wc"", ""wo"", ""rf"", ""ri"", ""rc"", ""ro""],\n\t\t""gru"": [""wi"", ""wr"", ""wh"", ""ri"", ""rr"", ""rh""]\n\t}[module.mode]\n\n\tWdata = [numpyPtr(par[key]) for par in params for key in keys]\n\tWlen = [par[key].size for par in params for key in keys]\n\n\tbiasdata = [numpyPtr(par[""b%s"" % key]) for par in params for key in keys]\n\tbiaslen = [par[""b%s"" % key].size for par in params for key in keys]\n\n\toutput = graph.addRNN(\n\t\tinp, module.layers, module.hsize, seqlen, mode, direction, inputMode, Wdata, Wlen, biasdata, biaslen, fullname\n\t)\n\n\treturn [output]\n\n\ndef convertUpsample2D(module, fullname, graph, inp):\n\tassert module.mode == ""nearest""\n\toutput = graph.addUpsample(inp, module.scale, fullname)\n\n\treturn [output]\n\n\ndef convertPRelu(module, fullname, graph, inp, holder):\n\tassert not module.sharedMaps\n\n\tslopes = module.slopes.get()\n\tholder.append(slopes)\n\n\toutput = graph.addPRelu(inp, numpyPtr(slopes), slopes.size, fullname)\n\treturn [output]\n\n\ndef convertPad1D(module, fullname, graph, inp):\n\tassert module.mode == PadMode.reflect\n\n\toutput = graph.addReflectPad(inp, module.pad, fullname)\n\treturn [output]\n'"
Converter/TensorRT/DataCalibrator.py,0,"b'import ctypes\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\n\nfrom PuzzleLib.Converter.TensorRT import Driver\n\n\nclass CalibratorError(Exception):\n\tpass\n\n\nclass DataCalibrator(Driver.ICalibrator):\n\tdef __init__(self, data, batchsize=100, cachename=None, dataname=None):\n\t\tsuper().__init__()\n\n\t\tself.data = data\n\n\t\tself.idx = 0\n\t\tself.offset = 0\n\n\t\tself.nbatches = (data.shape[0] + batchsize - 1) // batchsize\n\t\tself.batchsize = batchsize\n\n\t\tif cachename is not None:\n\t\t\traise NotImplementedError()\n\n\t\tself.cachename = cachename\n\t\tself.dataname = dataname\n\n\t\tself.cache = None\n\n\n\tdef getDataShape(self):\n\t\treturn self.data.shape[1:]\n\n\n\tdef getBatchSize(self):\n\t\tbatchsize = min(self.data.shape[0] - self.offset, self.batchsize)\n\t\treturn batchsize\n\n\n\tdef getBatch(self, bindings, names):\n\t\tassert len(bindings) == 1\n\n\t\tif self.dataname is not None:\n\t\t\tassert names[0] == self.dataname\n\n\t\tbatchsize = self.getBatchSize()\n\t\tif batchsize == 0:\n\t\t\treturn False\n\n\t\tbatch = self.data[self.offset:self.offset + batchsize]\n\t\tself.offset += batchsize\n\n\t\tbatch = gpuarray.to_gpu(batch, allocator=memPool)\n\n\t\tptr = ctypes.cast(int(bindings[0]), ctypes.POINTER(ctypes.c_void_p))\n\t\tptr.contents.value = batch.ptr\n\n\t\tprint(""Sending batch #%s (%s out of %s)"" % (self.idx, self.idx + 1, self.nbatches))\n\t\tself.idx += 1\n\n\t\treturn True\n'"
Converter/TensorRT/RTEngine.py,2,"b'from enum import Enum\n\nimport numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\n\nfrom PuzzleLib.Modules.Module import Module\nfrom PuzzleLib.Converter.TensorRT import Driver\n\n\nclass DataType:\n\tfloat32 = Driver.DataType.float\n\tint8 = Driver.DataType.int8\n\tfloat16 = Driver.DataType.half\n\n\nclass RTEngineType(Enum):\n\tpuzzle = Driver.RTEngineType.puzzle\n\tcaffe = Driver.RTEngineType.caffe\n\tonnx = Driver.RTEngineType.onnx\n\n\ndef genEngineName(name, dtype, inshape, outshape):\n\tfrom PuzzleLib.Backend.Utils import backend\n\tarch = backend.device.name().replace("" "", ""_"")\n\n\tdtypes = {\n\t\tDataType.float32: ""float32"",\n\t\tDataType.int8: ""int8"",\n\t\tDataType.float16: ""float16""\n\t}\n\n\tif not isinstance(inshape, list):\n\t\tinshape = [inshape]\n\n\tinshape = "","".join(""-"".join(str(s) for s in sh) for sh in inshape)\n\n\tif not isinstance(outshape, list):\n\t\toutshape = [outshape]\n\n\toutshape = "","".join(""-"".join(str(s) for s in sh) for sh in outshape)\n\tfullname = ""%s.%s.%s.%s.%s.engine"" % (name, dtypes[dtype], inshape, outshape, arch)\n\n\treturn fullname\n\n\ndef parseEngineShape(enginename):\n\tsubnames = enginename.split(sep=""."")\n\tinshape, outshape = subnames[-4], subnames[-3]\n\n\tinshape = [tuple(int(v) for v in sh.split(sep=""-"")) for sh in inshape.split(sep="","")]\n\tinshape = inshape[0] if len(inshape) == 1 else inshape\n\n\toutshape = [tuple(int(v) for v in sh.split(sep=""-"")) for sh in outshape.split(sep="","")]\n\toutshape = outshape[0] if len(outshape) == 1 else outshape\n\n\treturn inshape, outshape\n\n\nclass RTEngine(Module):\n\tdef __init__(self, enginepath, enginetype, inshape=None, outshape=None, log=True, name=None):\n\t\tsuper().__init__(name)\n\t\tself.registerBlueprint(locals())\n\n\t\tif inshape is None or outshape is None:\n\t\t\tparsedInshape, parsedOutshape = parseEngineShape(enginepath)\n\n\t\t\tinshape = parsedInshape if inshape is None else inshape\n\t\t\toutshape = parsedOutshape if outshape is None else outshape\n\n\t\tself.inshape, self.outshape = inshape, outshape\n\t\tself.engine = Driver.RTEngine(enginepath, enginetype.value, log)\n\n\n\tdef updateData(self, data):\n\t\tif isinstance(data, list):\n\t\t\tbatchsize = data[0].shape[0]\n\t\t\tbindings = [dat.ptr for dat in data]\n\n\t\telse:\n\t\t\tbatchsize = data.shape[0]\n\t\t\tbindings = [data.ptr]\n\n\t\tif isinstance(self.outshape, list):\n\t\t\tself.data = [\n\t\t\t\tgpuarray.empty((batchsize, ) + outshape[1:], dtype=np.float32, allocator=memPool)\n\t\t\t\tfor outshape in self.outshape\n\t\t\t]\n\n\t\telse:\n\t\t\tself.data = gpuarray.empty((batchsize, ) + self.outshape[1:], dtype=np.float32, allocator=memPool)\n\n\t\tif isinstance(self.data, list):\n\t\t\tbindings.extend(data.ptr for data in self.data)\n\t\telse:\n\t\t\tbindings.append(self.data.ptr)\n\n\t\tself.engine.enqueue(batchsize, bindings)\n\n\n\tdef updateGrad(self, grad):\n\t\tassert False\n\n\n\tdef dataShapeFrom(self, shape):\n\t\treturn self.outshape\n\n\n\tdef checkDataShape(self, shape):\n\t\tif isinstance(shape, list):\n\t\t\tfor i, sh in enumerate(shape):\n\t\t\t\tif sh[1:] != self.inshape[1:]:\n\t\t\t\t\traise ValueError(""Shape %s is not equal to shape %s on index %s"" % (sh[1:], self.inshape[i][1:], i))\n\n\t\t\t\tif sh[0] > self.inshape[0] or sh[0] != shape[0][0]:\n\t\t\t\t\traise ValueError(""Bad batch size %s on index %s (maximum=%s)"" % (sh[0], i, self.inshape[0]))\n\n\t\telse:\n\t\t\tif shape[1:] != self.inshape[1:]:\n\t\t\t\traise ValueError(""Data shape must be equal %s (was given %s)"" % (self.inshape[1:], shape[1:]))\n\n\t\t\tif shape[0] > self.inshape[0]:\n\t\t\t\traise ValueError(""Maximum batch size is %s (was given %s)"" % (self.inshape[0], shape[0]))\n\n\n\tdef gradShapeFrom(self, shape):\n\t\tassert False\n\n\n\tdef checkGradShape(self, shape):\n\t\tassert False\n'"
Cuda/Benchmarks/ConvSpeed.py,1,"b'import numpy as np\nfrom PuzzleLib import Config\n\n\ndef main():\n\tdatashape = (128, 32, 64, 64)\n\tWshape = (64, 32, 11, 11)\n\n\tstride, pad, dilation, groups = 1, 0, 1, datashape[1] // Wshape[1]\n\n\tfrom PuzzleLib.Cuda.Backend import getBackend\n\tbackend = getBackend(Config.deviceIdx, initmode=1)\n\n\ttimeConv(backend, datashape, Wshape, np.float32, stride, pad, dilation, groups)\n\n\ndef timeConv(backend, datashape, Wshape, dtype, stride, pad, dilation, groups):\n\tfwdResults, bwdDataResults, bwdFilterResults = backend.convNdbenchmark(\n\t\tdatashape, Wshape, dtype, stride, pad, dilation, groups\n\t)\n\n\tprint(""Forward results:"")\n\tfor res in fwdResults:\n\t\tprint(res)\n\n\tprint(""\\nBackward filter results:"")\n\tfor res in bwdFilterResults:\n\t\tprint(res)\n\n\tprint(""\\nBackward data results:"")\n\tfor res in bwdDataResults:\n\t\tprint(res)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Cuda/Kernels/CTC.py,28,"b'import math, random\nfrom string import Template\n\nimport numpy as np\nfrom PuzzleLib.Cuda.Kernels.RadixSort import scanSumTmpl, radixSortTmpl, segmentSeqTmpl\n\n\nctcTmpl = Template(""""""\n\n#ifdef __CUDACC__\n\t#define HUGE_VALF (1.0f / 0.0f)\n#endif\n\n\n$segmentSeq\n\n\n__forceinline__ __device__ float logPlus(float p1, float p2)\n{\n\tif (p1 <= -HUGE_VALF)\n\t\treturn p2;\n\n\tif (p2 <= -HUGE_VALF)\n\t\treturn p1;\n\n\treturn log1pf(expf(-fabsf(p1 - p2))) + max(p1, p2);\n}\n\n\nextern ""C"" __launch_bounds__($NT) __global__\nvoid calcAlphas(const float * __restrict__ indata, const int * __restrict__ datalen, int T, int vocabsize,\n\t\t\t\tconst int * __restrict__ labels, const int * __restrict__ offsets, float * __restrict__ alphas,\n\t\t\t\tint blank, float * __restrict__ nll, float * __restrict__ error)\n{\n\t__shared__ int shlabels[$NV];\n\n\tint offset = offsets[blockIdx.x];\n\tint S = 2 * (offsets[blockIdx.x + 1] - offset) + 1;\n\n\tindata += blockIdx.x * vocabsize;\n\tlabels += offset;\n\n\talphas += T * (2 * offset + blockIdx.x);\n\n\tfor (int i = threadIdx.x; i < S; i += $NT)\n\t{\n\t\tint label = (i % 2 == 0) ? blank : labels[i / 2];\n\n\t\tshlabels[i] = label;\n\t\talphas[i] = (i < 2) ? logf(indata[label]) : -HUGE_VALF;\n\t}\n\n\t__syncthreads();\n\tT = datalen[blockIdx.x];\n\n\tfor (int t = 1; t < T; t++)\n\t{\n\t\tfor (int i = threadIdx.x; i < S; i += $NT)\n\t\t{\n\t\t\tfloat prevSum = alphas[(t - 1) * S + i];\n\n\t\t\tif (i > 0)\n\t\t\t{\n\t\t\t\tprevSum = logPlus(prevSum, alphas[(t - 1) * S + (i - 1)]);\n\n\t\t\t\tif (i > 1 && shlabels[i] != blank && shlabels[i] != shlabels[i - 2])\n\t\t\t\t\tprevSum = logPlus(prevSum, alphas[(t - 1) * S + (i - 2)]);\n\t\t\t}\n\n\t\t\talphas[t * S + i] = prevSum + logf(indata[t * gridDim.x * vocabsize + shlabels[i]]);\n\t\t}\n\n\t\t__syncthreads();\n\t}\n\n\tif (threadIdx.x == 0)\n\t{\n\t\tfloat loglike = logPlus(alphas[(T - 1) * S + (S - 2)], alphas[(T - 1) * S + (S - 1)]);\n\n\t\tnll[blockIdx.x] = -loglike;\n\t\tatomicAdd(error, -loglike);\n\t}\n}\n\n\nextern ""C"" __launch_bounds__($NT) __global__\nvoid calcBetas(const float * __restrict__ indata, const int * __restrict__ datalen, int T, int vocabsize,\n\t\t\t   const int * __restrict__ labels, const int * __restrict__ offsets, const float * __restrict__ alphas,\n\t\t\t   int blank, const float * __restrict__ nll, float * __restrict__ grad)\n{\n\t__shared__ int indices[$NV], shlabels[$NV];\n\n\t__shared__ union\n\t{\n\t\tSegmentStorage segmentStorage;\n\t\tfloat betas[2][$NV];\n\t}\n\tcache;\n\n\tint offset = offsets[blockIdx.x];\n\tint S = 2 * (offsets[blockIdx.x + 1] - offset) + 1;\n\n\tindata += blockIdx.x * vocabsize;\n\tgrad += blockIdx.x * vocabsize;\n\n\tlabels += offset;\n\talphas += T * (2 * offset + blockIdx.x);\n\n\tfloat loglike = nll[blockIdx.x];\n\n\tif (loglike >= HUGE_VALF)\n\t\treturn;\n\n\tint keys[$VT];\n\tfor (int i = 0; i < $VT; i++)\n\t{\n\t\tint j = threadIdx.x + i * $NT;\n\t\tkeys[i] = (j < S) ? ((j % 2 == 0) ? blank : labels[j / 2]) : 0x7FFFFFFF;\n\n\t\tif (j < S)\n\t\t\tshlabels[j] = keys[i];\n\t}\n\n\tSegmentResult segments = blockSegmentSeq(keys, S, indices, &cache.segmentStorage);\n\tT = datalen[blockIdx.x];\n\n\tint src = 0, dst = 1;\n\n\tfor (int t = T - 1; t >= 0; t--)\n\t{\n\t\tif (t < T - 1)\n\t\t{\n\t\t\tfor (int i = threadIdx.x; i < S; i += $NT)\n\t\t\t{\n\t\t\t\tfloat nextSum = cache.betas[src][i];\n\n\t\t\t\tif (i < S - 1)\n\t\t\t\t{\n\t\t\t\t\tnextSum = logPlus(nextSum, cache.betas[src][i + 1]);\n\n\t\t\t\t\tif (i < S - 2 && shlabels[i] != blank && shlabels[i] != shlabels[i + 2])\n\t\t\t\t\t\tnextSum = logPlus(nextSum, cache.betas[src][i + 2]);\n\t\t\t\t}\n\n\t\t\t\tcache.betas[dst][i] = nextSum + logf(indata[t * gridDim.x * vocabsize + shlabels[i]]);\n\t\t\t}\n\n\t\t\tsrc = (src + 1) % 2, dst = (dst + 1) % 2;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tfor (int i = threadIdx.x; i < S; i += $NT)\n\t\t\t{\n\t\t\t\tint offset = (T - 1) * gridDim.x * vocabsize + shlabels[i];\n\t\t\t\tcache.betas[0][i] = (i >= S - 2) ? logf(indata[offset]) : -HUGE_VALF;\n\t\t\t}\n\t\t}\n\n\t\t__syncthreads();\n\n\t\tfloat gr[$VT];\n\t\tfor (int i = 0; i < $VT; i++)\n\t\t{\n\t\t\tif (i >= segments.length)\n\t\t\t\tbreak;\n\n\t\t\tgr[i] = -HUGE_VALF;\n\n\t\t\tfor (int j = segments.start[i]; j < segments.end[i]; j++)\n\t\t\t\tgr[i] = logPlus(gr[i], alphas[t * S + indices[j]] + cache.betas[src][indices[j]]);\n\t\t}\n\n\t\tfor (int i = threadIdx.x; i < vocabsize; i += $NT)\n\t\t\tgrad[t * gridDim.x * vocabsize + i] = -indata[t * gridDim.x * vocabsize + i];\n\n\t\t__syncthreads();\n\n\t\tfor (int i = 0; i < $VT; i++)\n\t\t{\n\t\t\tif (i >= segments.length)\n\t\t\t\tbreak;\n\n\t\t\tint offset = t * gridDim.x * vocabsize + segments.label[i];\n\t\t\tfloat data = indata[offset];\n\n\t\t\tif (data > 0.0f)\n\t\t\t\tgrad[offset] += expf(gr[i] - logf(data) + loglike);\n\t\t}\n\t}\n}\n\n"""""")\n\n\nclass CTCModule:\n\tdef __init__(self, backend):\n\t\tself.GPUArray, self.dnn = backend.GPUArray, backend.dnn\n\n\t\tself.configs = self.generateConfig(backend)\n\t\tself.modules = [self.generateModule(backend, NT, VT) for NT, VT in self.configs]\n\n\n\t@staticmethod\n\tdef generateConfig(backend):\n\t\treturn [\n\t\t\t(backend.warpSize, 1),\n\t\t\t(backend.warpSize * 2, 1),\n\t\t\t(backend.warpSize * 4, 1),\n\t\t\t(backend.warpSize * 2, 3),\n\t\t\t(backend.warpSize * 4, 2),\n\t\t\t(backend.warpSize, 9),\n\t\t\t(backend.warpSize * 2, 6),\n\t\t\t(backend.warpSize * 4, 4),\n\t\t\t(backend.warpSize * 2, 9),\n\t\t\t(backend.warpSize * 4, 6),\n\t\t\t(backend.warpSize * 4, 9),\n\t\t\t(backend.warpSize * 4, 10)\n\t\t]\n\n\n\t@staticmethod\n\tdef generateModule(backend, NT, VT):\n\t\tNV = NT * VT\n\n\t\tscanSum = scanSumTmpl.substitute(warpSize=backend.warpSize, NT=NT)\n\t\tradixSort = radixSortTmpl.substitute(scanSum=scanSum, warpSize=backend.warpSize, NT=NT, VT=VT, NV=NV)\n\t\tsegmentSeq = segmentSeqTmpl.substitute(radixSort=radixSort, NT=NT, VT=VT, NV=NV)\n\n\t\treturn backend.SourceModule(ctcTmpl.substitute(segmentSeq=segmentSeq, NT=NT, VT=VT, NV=NV))\n\n\n\tdef ctcLoss(self, data, datalen, labels, lengths, blank, error=None, normalized=False, returnAlphas=False,\n\t\t\t\tallocator=None):\n\t\tT, batchsize, vocabsize = data.shape\n\t\tmx = 2 * np.max(lengths) + 1\n\n\t\tconfig = min(i for i, (NT, VT) in enumerate(self.configs) if mx <= NT * VT)\n\t\tmod, NT = self.modules[config], self.configs[config][0]\n\n\t\tif not normalized:\n\t\t\tdata = self.dnn.softmaxNd(data.reshape(T * batchsize, vocabsize, 1, 1), allocator=allocator).reshape(\n\t\t\t\tT, batchsize, vocabsize\n\t\t\t)\n\n\t\toffsets = np.cumsum(lengths, dtype=np.int32)\n\t\textOffsets = np.empty(shape=(batchsize + 1, ), dtype=np.int32)\n\n\t\textOffsets[0] = 0\n\t\textOffsets[1:] = offsets\n\n\t\talphas = self.GPUArray.empty((T * (2 * int(offsets[-1]) + batchsize), ), dtype=np.float32, allocator=allocator)\n\t\toffsets = self.GPUArray.toGpu(extOffsets, allocator=allocator)\n\n\t\tnll = self.GPUArray.empty((batchsize, ), dtype=np.float32, allocator=allocator)\n\n\t\terror = self.GPUArray.zeros((), dtype=np.float32, allocator=allocator) if error is None else error\n\t\tgrad = self.GPUArray.zeros(data.shape, dtype=np.float32, allocator=allocator)\n\n\t\tmod.calcAlphas(\n\t\t\tdata, datalen, np.int32(T), np.int32(vocabsize), labels, offsets, alphas, np.int32(blank),\n\t\t\tnll, error, block=(NT, 1, 1), grid=(batchsize, 1, 1)\n\t\t)\n\n\t\tmod.calcBetas(\n\t\t\tdata, datalen, np.int32(T), np.int32(vocabsize), labels, offsets, alphas, np.int32(blank),\n\t\t\tnll, grad, block=(NT, 1, 1), grid=(batchsize, 1, 1)\n\t\t)\n\n\t\treturn (error, grad) if not returnAlphas else (error, grad, alphas)\n\n\ndef unittest():\n\tfrom PuzzleLib.Cuda import Backend\n\tbackendTest(Backend, CTCModule)\n\n\ndef backendTest(Backend, Module):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tmodule = Module(Backend.getBackend(deviceIdx, initmode=1))\n\t\tctcLossTest(module)\n\n\ndef ctcLossTest(module):\n\ttimes, batchsize, vocabsize = 20, 3, 6\n\thostData, hostDataLen, hostLabels, lengths = createData(times, batchsize, vocabsize)\n\n\tdata, datalen = module.GPUArray.toGpu(hostData), module.GPUArray.toGpu(hostDataLen)\n\tlabels = module.GPUArray.toGpu(hostLabels)\n\n\tblank = 0\n\n\terror, grad, alphas = module.ctcLoss(data, datalen, labels, lengths, blank, returnAlphas=True)\n\thostError, hostGrad, hostAlphas = hostCTCLoss(hostData, hostDataLen, hostLabels, lengths, blank)\n\n\tassert np.allclose(hostAlphas, alphas.get())\n\n\tassert np.isclose(hostError, error.get())\n\tassert np.allclose(hostGrad, grad.get(), atol=1e-5)\n\n\ndef createData(times, batchsize, vocabsize):\n\tdata = np.random.randn(times, batchsize, vocabsize).astype(np.float32)\n\tdatalen = np.array([times] * batchsize, dtype=np.int32)\n\n\tlengths = np.array([random.randint(a=times // 4, b=times // 2 - 1) for _ in range(batchsize)], dtype=np.int32)\n\tlabels = np.concatenate([\n\t\tnp.array([random.randint(a=1, b=vocabsize - 1) for _ in range(lengths[b])], dtype=np.int32)\n\t\tfor b in range(batchsize)\n\t])\n\n\treturn data, datalen, labels, lengths\n\n\ndef hostSoftmax(w):\n\te = np.exp(w - np.amax(w, axis=-1, keepdims=True))\n\treturn e / np.sum(e, axis=-1, keepdims=True)\n\n\ndef logPlus(a, b):\n\tif a <= -math.inf:\n\t\treturn b\n\tif b <= -math.inf:\n\t\treturn a\n\n\treturn math.log1p(math.exp(-math.fabs(a - b))) + max(a, b)\n\n\ndef hostCTCLoss(data, datalen, labels, lengths, blank):\n\tdata = hostSoftmax(data)\n\n\talphas, nll = calcAlphasTest(data, datalen, labels, lengths, blank)\n\tgrad = calcBetasTest(alphas, nll, data, datalen, labels, lengths, blank)\n\n\treturn np.sum(nll), grad, alphas\n\n\ndef calcAlphasTest(data, datalen, labels, lengths, blank):\n\tT, batchsize, _ = data.shape\n\toffsets = np.cumsum(lengths)\n\n\talphas = np.full((T * (2 * offsets[-1] + batchsize), ), fill_value=np.nan, dtype=np.float32)\n\tnll = np.empty((batchsize, ), dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tL, T = int(lengths[b]), int(datalen[b])\n\t\tS = 2 * L + 1\n\n\t\toffset = 0 if b == 0 else int(offsets[b - 1])\n\n\t\textLabels = np.full((S, ), fill_value=blank, dtype=np.int32)\n\t\textLabels[1::2] = labels[offset:offset + L]\n\n\t\textOffset = 2 * offset + b\n\t\talpha = alphas[extOffset * T:(extOffset + S) * T].reshape(T, S)\n\n\t\talpha[0, 0] = math.log(data[0, b, blank])\n\t\talpha[0, 1] = math.log(data[0, b, extLabels[1]])\n\n\t\talpha[0, 2:] = -math.inf\n\n\t\tfor t in range(1, T):\n\t\t\talpha[t, 0] = alpha[t - 1, 0] + math.log(data[t, b, blank])\n\n\t\t\tfor i in range(1, S):\n\t\t\t\tprevSum = logPlus(alpha[t - 1, i], alpha[t - 1, i - 1])\n\n\t\t\t\tif i > 1 and extLabels[i] != blank and extLabels[i] != extLabels[i - 2]:\n\t\t\t\t\tprevSum = logPlus(prevSum, alpha[t - 1, i - 2])\n\n\t\t\t\talpha[t, i] = prevSum + math.log(data[t, b, extLabels[i]])\n\n\t\tloglike = logPlus(logPlus(-math.inf, alpha[T - 1, S - 2]), alpha[T - 1, S - 1])\n\t\tnll[b] = -loglike\n\n\treturn alphas, nll\n\n\ndef calcBetasTest(alphas, nll, data, datalen, labels, lengths, blank):\n\toffsets = np.cumsum(lengths)\n\n\t_, batchsize, nlabels = data.shape\n\tgrad = np.full(data.shape, fill_value=-math.inf, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tL, T = int(lengths[b]), int(datalen[b])\n\t\tS = 2 * L + 1\n\n\t\toffset = 0 if b == 0 else int(offsets[b - 1])\n\n\t\textLabels = np.full((S, ), fill_value=blank, dtype=np.int32)\n\t\textLabels[1::2] = labels[offset:offset + L]\n\n\t\textOffset = 2 * offset + b\n\t\talpha = alphas[extOffset * T:(extOffset + S) * T].reshape(T, S)\n\n\t\tbeta = np.empty((T, S), dtype=np.float32)\n\t\tbeta[T - 1, :S - 2] = -math.inf\n\n\t\tbeta[T - 1, S - 2] = math.log(data[T - 1, b, extLabels[S - 2]])\n\t\tbeta[T - 1, S - 1] = math.log(data[T - 1, b, blank])\n\n\t\tfor i in range(S):\n\t\t\tgrad[T - 1, b, extLabels[i]] = logPlus(grad[T - 1, b, extLabels[i]], alpha[T - 1, i] + beta[T - 1, i])\n\n\t\tfor i in range(nlabels):\n\t\t\tgrad[T - 1, b, i] = data[T - 1, b, i] - math.exp(grad[T - 1, b, i] - math.log(data[T - 1, b, i]) + nll[b])\n\n\t\tfor t in reversed(range(T - 1)):\n\t\t\tfor i in range(S - 1):\n\t\t\t\tnextSum = logPlus(beta[t + 1, i], beta[t + 1, i + 1])\n\n\t\t\t\tif i < S - 2 and extLabels[i] != blank and extLabels[i] != extLabels[i + 2]:\n\t\t\t\t\tnextSum = logPlus(nextSum, beta[t + 1, i + 2])\n\n\t\t\t\tbeta[t, i] = nextSum + math.log(data[t, b, extLabels[i]])\n\n\t\t\tbeta[t, S - 1] = beta[t + 1, S - 1] + math.log(data[t, b, blank])\n\n\t\t\tfor i in range(S):\n\t\t\t\tgrad[t, b, extLabels[i]] = logPlus(grad[t, b, extLabels[i]], alpha[t, i] + beta[t, i])\n\n\t\t\tfor i in range(nlabels):\n\t\t\t\tgrad[t, b, i] = data[t, b, i] - math.exp(grad[t, b, i] - math.log(data[t, b, i]) + nll[b])\n\n\t\tgrad[T:, b] = 0\n\n\treturn -grad\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cuda/Kernels/Costs.py,32,"b'from string import Template\nimport numpy as np\n\nfrom PuzzleLib.Compiler.Codegen.Types import int_t, float_t\nfrom PuzzleLib.Cuda.Utils import prod, roundUpDiv\n\n\ndef bce(ElementwiseKernel):\n\treturn ElementwiseKernel(\n\t\t[\n\t\t\t(float_t.const.ptr, ""scores""), (int_t.const.ptr, ""labels""),\n\t\t\t(float_t.ptr, ""totalError""), (float_t.ptr, ""grad""), (int_t, ""numsamples""), (int_t, ""spatialDim"")\n\t\t],\n\t\t""""""\n\t\tfloat prob = 1.0f / (1.0f + exp(-scores[i]));\n\t\tfloat error = labels[i] == 1 ? -log(prob) : -log(1.0f - prob);\n\t\tatomicAdd(totalError, error / spatialDim);\n\t\tgrad[i] = ((labels[i] == 1) - prob) / numsamples / spatialDim;\n\t\t"""""",\n\t\t""bceKer""\n\t)\n\n\ndef hinge(ElementwiseKernel):\n\treturn ElementwiseKernel(\n\t\t[\n\t\t\t(float_t.const.ptr, ""scores""), (int_t.const.ptr, ""labels""),\n\t\t\t(float_t.ptr, ""totalError""), (float_t.ptr, ""grad""), (int_t, ""numsamples""), (int_t, ""numcases"")\n\t\t],\n\t\t""""""\n\t\tfloat score = scores[i];\n\t\tint label = labels[i];\n\t\tfloat error = max(0.0f, 1.0f - score * label) / numcases;\n\t\tatomicAdd(totalError, error);\n\t\tgrad[i] = score * label < 1.0f ? (float)label / numsamples / numcases : 0.0f;\n\t\t"""""",\n\t\t""hingeKer""\n\t)\n\n\ndef smoothL1(ElementwiseKernel):\n\treturn ElementwiseKernel(\n\t\t[\n\t\t\t(float_t.const.ptr, ""pred""), (float_t.const.ptr, ""target""),\n\t\t\t(float_t.ptr, ""totalError""), (float_t.ptr, ""grad""), (float_t, ""norm""), (float_t, ""fullnorm"")\n\t\t],\n\t\t""""""\n\t\tfloat diff = pred[i] - target[i];\n\t\tfloat sign = pred[i] - target[i] > 0.0f ? 1.0f : -1.0f;\n\t\tatomicAdd(totalError, diff * sign < 1.0f ? diff * diff / 2.0f * norm : (sign * diff - 0.5f) * norm);\n\t\tgrad[i] = diff * sign < 1.0f ? diff * fullnorm : sign * fullnorm;\n\t\t"""""",\n\t\t""smoothL1Ker""\n\t)\n\n\ndef l1Hinge(ElementwiseKernel):\n\treturn ElementwiseKernel(\n\t\t[\n\t\t\t(float_t.const.ptr, ""x1""), (float_t.const.ptr, ""x2""), (int_t.const.ptr, ""labels""),\n\t\t\t(float_t.ptr, ""totalError""), (float_t.ptr, ""g1""), (float_t.ptr, ""g2""),\n\t\t\t(int_t, ""numsamples""), (int_t, ""numcases"")\n\t\t],\n\t\t""""""\n\t\tfloat diff = x1[i] - x2[i];\n\t\tfloat sign = diff > 0.0f ? 1.0f : -1.0f;\n\t\tint label = labels[i / numcases];\n\t\tfloat error = (label == 0) ? max(0.0f, 1.0f - abs(diff)) / numcases : abs(diff) / numcases;\n\t\tatomicAdd(totalError, error);\n\t\tg1[i] = (label == 0 ? (abs(diff) < 1.0f) * -sign : sign) / numsamples / numcases;\n\t\tg2[i] = (label == 0 ? (abs(diff) < 1.0f) * sign : -sign) / numsamples / numcases;\n\t\t"""""",\n\t\t""l1HingeKer""\n\t)\n\n\ncostLblTmpl = Template(""""""\n\nextern ""C""\n__global__ void cost(const float *scores, const int *labels, int size, int mapStride, int spatialDim,\n\t\t\t\t\t int numCases, int numSamples, float *totalError, float *grad)\n{\n\tfor (int index = blockIdx.x * blockDim.x + threadIdx.x; index < size; index += blockDim.x * gridDim.x)\n\t{\n\t\tint b = index / mapStride;\n\t\tint m = index % spatialDim;\n\t\tint c = (index / spatialDim) % numCases;\n\n\t\t$logic\n\t}\n}\n\n"""""")\n\n\ncrossEntropyLogic = """"""\nfloat score = scores[index];\nint label = labels[b * spatialDim + m];\ngrad[index] = ((c == label) - score) / numSamples;\n\nif (c == label)\n{\n\tfloat error = -log(score) / spatialDim;\n\tatomicAdd(totalError, error);\n}\n""""""\n\n\nsvmL1Logic = """"""\nfloat score = scores[index];\nint label = labels[b * spatialDim + m];\nfloat cls = (2 * (label == c) - 1);\ngrad[index] = score * cls < 1.0f ? cls / numCases / numSamples : 0.0f;\n\nfloat error = max(0.0f, 1.0f - score * cls) / numCases / spatialDim;\natomicAdd(totalError, error);\n""""""\n\n\nsvmL2Logic = """"""\nfloat score = scores[index];\nint label = labels[b * spatialDim + m];\nfloat cls = (2 * (label == c) - 1);\nfloat error = max(0.0f, 1.0f - score * cls);\ngrad[index] = 2.0f * cls * error / numCases / numSamples;\n\nerror = error * error / numCases / spatialDim;\natomicAdd(totalError, error);\n""""""\n\n\nwceTmpl = Template(""""""\n\nextern ""C""\n__global__ void cost(const float *scores, const int *labels, const float *weights, int size, int mapStride,\n\t\t\t\t\t int spatialDim, int numCases, int numSamples, float *totalError, float *grad)\n{\n\tfor (int index = blockIdx.x * blockDim.x + threadIdx.x; index < size; index += blockDim.x * gridDim.x)\n\t{\n\t\tint b = index / mapStride;\n\t\tint m = index % spatialDim;\n\t\tint c = (index / spatialDim) % numCases;\n\n\t\tfloat score = scores[index];\n\t\tint label = labels[b * spatialDim + m];\n\t\tfloat weight = weights[c];\n\t\tgrad[index] = weight * ((c == label) - score) / numSamples;\n\n\t\tif (c == label)\n\t\t{\n\t\t\tfloat error = -weight * log(score) / spatialDim;\n\t\t\tatomicAdd(totalError, error);\n\t\t}\n\t}\n}\n\n"""""")\n\n\nclass CostModule:\n\tdef __init__(self, backend):\n\t\tself.backend, self.GPUArray, self.dnn = backend, backend.GPUArray, backend.dnn\n\t\tself.nthreads = backend.nthreads\n\n\t\tself.ceMod = backend.SourceModule(costLblTmpl.substitute(logic=crossEntropyLogic))\n\t\tself.wceMod = backend.SourceModule(wceTmpl.substitute())\n\n\t\tself.svmL1Mod = backend.SourceModule(costLblTmpl.substitute(logic=svmL1Logic))\n\t\tself.svmL2Mod = backend.SourceModule(costLblTmpl.substitute(logic=svmL2Logic))\n\n\t\tself.accKernelCache = {}\n\n\n\tdef getAccuracyKernel(self, name):\n\t\tkrl = self.accKernelCache.get(name, None)\n\n\t\tif krl is None:\n\t\t\tif name == ""calcAccuracy"":\n\t\t\t\tkrl = self.backend.ReductionKernel(\n\t\t\t\t\tnp.float32, neutral=""0.0f"", reduceExpr=""a + b"", mapExpr=""x[i] != y[i]"",\n\t\t\t\t\targuments=[(int_t.const.ptr, ""x""), (int_t.const.ptr, ""y"")], name=""calcAccuracy""\n\t\t\t\t)\n\n\t\t\telif name == ""calcBCEAccuracy"":\n\t\t\t\tkrl = self.backend.ReductionKernel(\n\t\t\t\t\tnp.float32, neutral=""0.0f"", reduceExpr=""a + b"", mapExpr=""y[i] == 1 ? x[i] <= 0.0f : x[i] > 0.0f"",\n\t\t\t\t\targuments=[(float_t.const.ptr, ""x""), (int_t.const.ptr, ""y"")], name=""calcBCEAccuracy""\n\t\t\t\t)\n\n\t\t\telif name == ""klDivergence"":\n\t\t\t\tkrl = self.backend.ReductionKernel(\n\t\t\t\t\tnp.float32, neutral=""0.0f"", reduceExpr=""a + b"", mapExpr=\n\t\t\t\t\t""grad[i] = (y[i] - x[i]) * gradnorm, y[i] > 0.0f ? y[i] * (log(y[i]) - log(x[i])) : 0.0f"",\n\t\t\t\t\targuments=[\n\t\t\t\t\t\t(float_t.const.ptr, ""x""), (float_t.const.ptr, ""y""), (float_t.ptr, ""grad""), (float_t, ""gradnorm"")\n\t\t\t\t\t], name=""klDivergence""\n\t\t\t\t)\n\n\t\t\telif name == ""l1HingeAccuracy"":\n\t\t\t\tkrl = self.backend.ReductionKernel(\n\t\t\t\t\tnp.float32, neutral=""0.0f"", reduceExpr=""a + b"", mapExpr=""(d[i] <= 1.0f) != labels[i]"",\n\t\t\t\t\targuments=[(float_t.const.ptr, ""d""), (int_t.const.ptr, ""labels"")], name=""l1HingeAccuracy""\n\t\t\t\t)\n\n\t\t\telse:\n\t\t\t\traise NotImplementedError(name)\n\n\t\t\tself.accKernelCache[name] = krl\n\n\t\treturn krl\n\n\n\tdef crossEntropy(self, scores, labels, weights=None, error=None, allocator=None):\n\t\tassert scores.dtype == np.float32 and labels.dtype == np.int32\n\n\t\tshape = scores.shape\n\t\tif scores.ndim < 4:\n\t\t\tscores = scores.reshape(*shape, *(1 for _ in range(4 - scores.ndim)))\n\n\t\tsoftmax = self.dnn.softmaxNd(scores, mode=self.backend.SoftMaxMode.spatial.value, allocator=allocator)\n\n\t\tgrad = self.GPUArray.empty(shape, dtype=np.float32, allocator=allocator)\n\t\tif error is None:\n\t\t\terror = self.GPUArray.empty((), dtype=np.float32, allocator=allocator)\n\n\t\terror.fill(0.0)\n\n\t\tsize = prod(scores.shape)\n\t\tspatialDim = prod(scores.shape[2:])\n\t\tmapStride = spatialDim * scores.shape[1]\n\n\t\tblock = (self.nthreads, 1, 1)\n\t\tgrid = (roundUpDiv(size, self.nthreads), 1, 1)\n\n\t\tif weights is None:\n\t\t\tself.ceMod.cost(\n\t\t\t\tsoftmax, labels, np.int32(size), np.int32(mapStride), np.int32(spatialDim),\n\t\t\t\tnp.int32(scores.shape[1]), np.int32(scores.shape[0]), error, grad, block=block, grid=grid\n\t\t\t)\n\n\t\telse:\n\t\t\tself.wceMod.cost(\n\t\t\t\tsoftmax, labels, weights, np.int32(size), np.int32(mapStride), np.int32(spatialDim),\n\t\t\t\tnp.int32(shape[1]), np.int32(shape[0]), error, grad, block=block, grid=grid\n\t\t\t)\n\n\t\treturn error, grad\n\n\n\tdef svm(self, scores, labels, mode, error=None, allocator=None):\n\t\tassert scores.dtype == np.float32 and labels.dtype == np.int32\n\t\tshape = scores.shape\n\n\t\tgrad = self.GPUArray.empty(shape, dtype=np.float32, allocator=allocator)\n\t\tif error is None:\n\t\t\terror = self.GPUArray.empty((), dtype=np.float32, allocator=allocator)\n\n\t\terror.fill(0.0)\n\n\t\tsize = prod(scores.shape)\n\t\tspatialDim = prod(scores.shape[2:])\n\t\tmapStride = spatialDim * scores.shape[1]\n\n\t\tblock = (self.nthreads, 1, 1)\n\t\tgrid = (roundUpDiv(size, self.nthreads), 1, 1)\n\n\t\tmod = {\n\t\t\t""l1"": self.svmL1Mod,\n\t\t\t""l2"": self.svmL2Mod\n\t\t}[mode]\n\n\t\tmod.cost(\n\t\t\tscores, labels, np.int32(size), np.int32(mapStride), np.int32(spatialDim),\n\t\t\tnp.int32(shape[1]), np.int32(shape[0]), error, grad, block=block, grid=grid\n\t\t)\n\n\t\treturn error, grad\n\n\ndef unittest():\n\tfrom PuzzleLib.Cuda import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tmodule = CostModule(Backend.getBackend(deviceIdx, initmode=1))\n\n\t\tcrossEntropyTest(module)\n\t\tsvmTest(module)\n\n\ndef crossEntropyTest(module):\n\thostScores = np.random.randn(20, 10, 3).astype(np.float32)\n\thostLabels = np.random.randint(low=0, high=10, size=(20, 3)).astype(np.int32)\n\n\tscores, labels = module.GPUArray.toGpu(hostScores), module.GPUArray.toGpu(hostLabels)\n\terror, grad = module.crossEntropy(scores, labels)\n\n\tdef softmax(w):\n\t\te = np.exp(w - np.amax(w))\n\t\tdist = e / np.sum(e)\n\t\treturn dist\n\n\tdef hostCrossEntropy(smax, target):\n\t\tsmax = np.moveaxis(smax, 1, -1).reshape(-1, smax.shape[1])\n\t\ttarget = target.ravel()\n\t\terr = np.sum(np.log(np.array([smax[i, target[i]] for i in range(smax.shape[0])])))\n\n\t\treturn -err / target.size\n\n\tdef hostCrossEntropyGrad(target, smax):\n\t\treturn np.array([(target == i) - smax[i] for i in range(smax.shape[0])])\n\n\thostSoftmax = np.apply_along_axis(softmax, 1, hostScores)\n\n\thostGrad = np.vstack([\n\t\thostCrossEntropyGrad(hostLabels[i], hostSoftmax[i]) / scores.shape[0] for i in range(scores.shape[0])\n\t]).reshape(*hostSoftmax.shape)\n\n\tassert np.allclose(hostGrad, grad.get())\n\n\thostError = hostCrossEntropy(hostSoftmax, hostLabels)\n\tassert np.isclose(hostError, error.get() / scores.shape[0])\n\n\ndef svmTest(module):\n\tbatchsize, size = 20, 4\n\n\thostScores = np.random.randn(batchsize, size).astype(np.float32)\n\thostLabels = np.random.randint(low=0, high=size, size=(batchsize, ), dtype=np.int32)\n\n\tscores, labels = module.GPUArray.toGpu(hostScores), module.GPUArray.toGpu(hostLabels)\n\terror, grad = module.svm(scores, labels, mode=""l1"")\n\n\thostGrad = np.empty(grad.shape, dtype=np.float32)\n\thostError = 0.0\n\n\tfor b in range(batchsize):\n\t\tfor n in range(size):\n\t\t\tcls = 2 * (hostLabels[b] == n) - 1\n\t\t\tval = hostScores[b, n] * cls\n\n\t\t\thostGrad[b, n] = cls / batchsize / size if val < 1 else 0.0\n\t\t\thostError += max(0.0, 1.0 - val) / batchsize / size\n\n\tassert np.allclose(hostGrad, grad.get())\n\tassert np.isclose(hostError, error.get() / scores.shape[0])\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cuda/Kernels/ElementWise.py,64,"b'import numpy as np\n\nfrom PuzzleLib.Compiler.Codegen.Types import ushort_t, int_t, uint_t, half_t, float_t\n\nfrom PuzzleLib.Cuda.GPUArray import memoize\nfrom PuzzleLib.Cuda.SourceModule import dtypeToCtype\n\n\ndef sigmoid(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef sigmoidKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tname = ""sigmoidKer""\n\t\targuments = [(ctype.ptr, ""outdata""), (ctype.const.ptr, ""indata"")]\n\n\t\tpreambule = ""__forceinline__ __device__ float sigmoid(float x) { return 1.0f / (1.0f + expf(-x)); }""\n\t\toperation = ""outdata[i] = sigmoid((float)indata[i])""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 v = __half22float2(indata2[i]);\n\t\t\t\toutdata2[i] = __float22half2_rn(make_float2(sigmoid(v.x), sigmoid(v.y)));\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn sigmoidKer\n\n\ndef sigmoidDer(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef sigmoidDerKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tname = ""sigmoidDerKer""\n\t\targuments = [(ctype.ptr, ""ingrad""), (ctype.const.ptr, ""outgrad""), (ctype.const.ptr, ""outdata"")]\n\n\t\tpreambule = ""__forceinline__ __device__ float sigmoidDer(float g, float d) { return g * d * (1.0f - d); }""\n\t\toperation = ""ingrad[i] = sigmoidDer((float)outgrad[i], (float)outdata[i])""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 gvec = __half22float2(outgrad2[i]), dvec = __half22float2(outdata2[i]);\n\t\t\t\tingrad2[i] = __float22half2_rn(make_float2(sigmoidDer(gvec.x, dvec.x), sigmoidDer(gvec.y, dvec.y)));\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn sigmoidDerKer\n\n\ndef tanh(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef tanhKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tname = ""tanhKer""\n\t\targuments = [(ctype.ptr, ""outdata""), (ctype.const.ptr, ""indata"")]\n\n\t\toperation = ""outdata[i] = tanhf((float)indata[i])""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 v = __half22float2(indata2[i]);\n\t\t\t\toutdata2[i] = __float22half2_rn(make_float2(tanhf(v.x), tanhf(v.y)));\n\t\t\t\t"""""",\n\t\t\t\toperation, name\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name)\n\n\treturn tanhKer\n\n\ndef tanhDer(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef tanhDerKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tname = ""tanhDerKer""\n\t\targuments = [(ctype.ptr, ""ingrad""), (ctype.const.ptr, ""outgrad""), (ctype.const.ptr, ""outdata"")]\n\n\t\tpreambule = ""__forceinline__ __device__ float tanhDer(float g, float d) { return g * (1.0f - d * d); }""\n\t\toperation = ""ingrad[i] = tanhDer((float)outgrad[i], (float)outdata[i])""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 gvec = __half22float2(outgrad2[i]), dvec = __half22float2(outdata2[i]);\n\t\t\t\tingrad2[i] = __float22half2_rn(make_float2(tanhDer(gvec.x, dvec.x), tanhDer(gvec.y, dvec.y)));\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn tanhDerKer\n\n\ndef relu(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef reluKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tname = ""reluKer""\n\t\targuments = [(ctype.ptr, ""outdata""), (ctype.const.ptr, ""indata"")]\n\n\t\tpreambule = ""__forceinline__ __device__ float relu(float x) { return x * (x > 0.0f); }""\n\t\toperation = ""outdata[i] = relu((float)indata[i])""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 v = __half22float2(indata2[i]);\n\t\t\t\toutdata2[i] = __float22half2_rn(make_float2(relu(v.x), relu(v.y)));\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn reluKer\n\n\ndef reluDer(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef reluDerKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tname = ""reluDerKer""\n\t\targuments = [(ctype.ptr, ""ingrad""), (ctype.const.ptr, ""outgrad""), (ctype.const.ptr, ""outdata"")]\n\n\t\tpreambule = ""__forceinline__ __device__ float reluDer(float g, float d) { return g * (d > 0.0f); }""\n\t\toperation = ""ingrad[i] = reluDer((float)outgrad[i], (float)outdata[i])""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 gvec = __half22float2(outgrad2[i]), dvec = __half22float2(outdata2[i]);\n\t\t\t\tingrad2[i] = __float22half2_rn(make_float2(reluDer(gvec.x, dvec.x), reluDer(gvec.y, dvec.y)));\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn reluDerKer\n\n\ndef leakyRelu(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef leakyReluKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tname = ""leakyReluKer""\n\t\targuments = [(ctype.ptr, ""outdata""), (ctype.const.ptr, ""indata""), (float_t, ""a"")]\n\n\t\tpreambule = """"""\n\t\t__forceinline__ __device__ float leakyRelu(float x, float a) { return x * ((x > 0.0f) + a * (x <= 0.0f)); }\n\t\t""""""\n\n\t\toperation = ""outdata[i] = leakyRelu((float)indata[i], a)""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 v = __half22float2(indata2[i]);\n\t\t\t\toutdata2[i] = __float22half2_rn(make_float2(leakyRelu(v.x, a), leakyRelu(v.y, a)));\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn leakyReluKer\n\n\ndef leakyReluDer(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef leakyReluDerKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tname = ""leakyReluDerKer""\n\t\targuments = [(ctype.ptr, ""ingrad""), (ctype.const.ptr, ""outgrad""), (ctype.const.ptr, ""outdata""), (float_t, ""a"")]\n\n\t\tpreambule = """"""\n\t\t__forceinline__ __device__\n\t\tfloat leakyReluDer(float g, float d, float a) { return g * ((d > 0.0f) + a * (d <= 0.0f)); }\n\t\t""""""\n\n\t\toperation = ""ingrad[i] = leakyReluDer((float)outgrad[i], (float)outdata[i], a)""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 gvec = __half22float2(outgrad2[i]), dvec = __half22float2(outdata2[i]);\n\t\t\t\tingrad2[i] = __float22half2_rn(make_float2(\n\t\t\t\t\tleakyReluDer(gvec.x, dvec.x, a), leakyReluDer(gvec.y, dvec.y, a)\n\t\t\t\t));\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn leakyReluDerKer\n\n\ndef elu(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef eluKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tname = ""eluKer""\n\t\targuments = [(ctype.ptr, ""outdata""), (ctype.const.ptr, ""indata""), (float_t, ""a"")]\n\n\t\tpreambule = """"""\n\t\t__forceinline__ __device__\n\t\tfloat elu(float x, float a) { return x * (x > 0.0f) + a * (expf(x) - 1.0f) * (x <= 0.0f); }\n\t\t""""""\n\n\t\toperation = ""outdata[i] = elu((float)indata[i], a)""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 v = __half22float2(indata2[i]);\n\t\t\t\toutdata2[i] = __float22half2_rn(make_float2(elu(v.x, a), elu(v.y, a)));\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn eluKer\n\n\ndef eluDer(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef eluDerKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tname = ""eluDerKer""\n\t\targuments = [(ctype.ptr, ""ingrad""), (ctype.const.ptr, ""outgrad""), (ctype.const.ptr, ""outdata""), (float_t, ""a"")]\n\n\t\tpreambule = """"""\n\t\t__forceinline__ __device__\n\t\tfloat eluDer(float g, float d, float a) { return g * ((d > 0.0f) + (d + a) * (d <= 0.0f)); }\n\t\t""""""\n\n\t\toperation = ""ingrad[i] = eluDer((float)outgrad[i], (float)outdata[i], a)""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 gvec = __half22float2(outgrad2[i]), dvec = __half22float2(outdata2[i]);\n\t\t\t\tingrad2[i] = __float22half2_rn(make_float2(eluDer(gvec.x, dvec.x, a), eluDer(gvec.y, dvec.y, a)));\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn eluDerKer\n\n\ndef softPlus(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef softPlusKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tname = ""softPlusKer""\n\t\targuments = [(ctype.ptr, ""outdata""), (ctype.const.ptr, ""indata"")]\n\n\t\tpreambule = ""__forceinline__ __device__ float softPlus(float x) { return logf(1.0f + expf(x)); }""\n\t\toperation = ""outdata[i] = softPlus((float)indata[i])""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 v = __half22float2(indata2[i]);\n\t\t\t\toutdata2[i] = __float22half2_rn(make_float2(softPlus(v.x), softPlus(v.y)));\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn softPlusKer\n\n\ndef softPlusDer(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef softPlusDerKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tname = ""softPlusDerKer""\n\t\targuments = [(ctype.ptr, ""ingrad""), (ctype.const.ptr, ""outgrad""), (ctype.const.ptr, ""outdata"")]\n\n\t\tpreambule = ""__forceinline__ __device__ float softPlusDer(float g, float d) { return g * (1.0f - expf(-d)); }""\n\t\toperation = ""ingrad[i] = softPlusDer((float)outgrad[i], (float)outdata[i])""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 gvec = __half22float2(outgrad2[i]), dvec = __half22float2(outdata2[i]);\n\t\t\t\tingrad2[i] = __float22half2_rn(make_float2(softPlusDer(gvec.x, dvec.x), softPlusDer(gvec.y, dvec.y)));\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn softPlusDerKer\n\n\ndef clip(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef clipKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tname = ""clipKer""\n\t\targuments = [(ctype.ptr, ""outdata""), (ctype.const.ptr, ""indata""), (float_t, ""a""), (float_t, ""b"")]\n\n\t\tpreambule = """"""\n\t\t__forceinline__ __device__ float clip(float x, float a, float b) { return min(b, max(a, x)); }\n\t\t""""""\n\n\t\toperation = ""outdata[i] = clip((float)indata[i], a, b)""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 v = __half22float2(indata2[i]);\n\t\t\t\toutdata2[i] = __float22half2_rn(make_float2(clip(v.x, a, b), clip(v.y, a, b)));\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn clipKer\n\n\ndef clipDer(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef clipDerKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tname = ""clipDerKer""\n\t\targuments = [\n\t\t\t(ctype.ptr, ""ingrad""), (ctype.const.ptr, ""outgrad""), (ctype.const.ptr, ""outdata""),\n\t\t\t(float_t, ""a""), (float_t, ""b"")\n\t\t]\n\n\t\tpreambule = """"""\n\t\t__forceinline__ __device__ float clipDer(float g, float d, float a, float b) { return g * (d > a && d < b); }\n\t\t""""""\n\n\t\toperation = ""ingrad[i] = clipDer((float)outgrad[i], (float)outdata[i], a, b)""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 gvec = __half22float2(outgrad2[i]), dvec = __half22float2(outdata2[i]);\n\t\t\t\tingrad2[i] = __float22half2_rn(make_float2(\n\t\t\t\t\tclipDer(gvec.x, dvec.x, a, b), clipDer(gvec.y, dvec.y, a, b)\n\t\t\t\t));\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn clipDerKer\n\n\ndef gelu(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef geluKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tname = ""geluKer""\n\t\targuments = [(ctype.ptr, ""outdata""), (ctype.const.ptr, ""indata"")]\n\n\t\tpreambule = """"""\n\t\t#define M_SQRT2 1.4142135623730951f\n\t\t__forceinline__ __device__ float gelu(float x) { return 0.5f * x * (1.0f + erff(x / M_SQRT2)); }\n\t\t""""""\n\n\t\toperation = ""outdata[i] = gelu((float)indata[i])""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 v = __half22float2(indata2[i]);\n\t\t\t\toutdata2[i] = __float22half2_rn(make_float2(gelu(v.x), gelu(v.y)));\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn geluKer\n\n\ndef geluDer(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef geluDerKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tname = ""geluDerKer""\n\t\targuments = [(ctype.ptr, ""ingrad""), (ctype.const.ptr, ""outgrad""), (ctype.const.ptr, ""indata"")]\n\n\t\tpreambule = """"""\n\t\t#define M_SQRT2 1.4142135623730951f\n\t\t#define M_SQRTPI 1.7724538509055159f\n\t\t__forceinline__ __device__ float geluDer(float g, float d)\n\t\t{\n\t\t\treturn g * (0.5f * (1.0f + erff(d / M_SQRT2)) + d / M_SQRTPI * expf(-0.5f * d * d));\n\t\t}\n\t\t""""""\n\n\t\toperation = ""ingrad[i] = geluDer((float)outgrad[i], (float)indata[i])""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 gvec = __half22float2(outgrad2[i]), dvec = __half22float2(indata2[i]);\n\t\t\t\tingrad2[i] = __float22half2_rn(make_float2(geluDer(gvec.x, dvec.x), geluDer(gvec.y, dvec.y)));\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn geluDerKer\n\n\ndef dropout(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef dropoutKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tparttype = {\n\t\t\tnp.float32: uint_t,\n\t\t\tnp.float16: ushort_t\n\t\t}[dtype.type]\n\n\t\tname = ""dropoutKer""\n\t\targuments = [\n\t\t\t(ctype.ptr, ""outdata""), (ctype.const.ptr, ""indata""), (parttype.const.ptr, ""b""),\n\t\t\t(parttype, ""v""), (float_t, ""p"")\n\t\t]\n\n\t\tpreambule = """"""\n\t\t__forceinline__ __device__\n\t\tfloat dropout(float x, %s b, %s v, float p) { return x * (b < v) / p; }\n\t\t"""""" % (parttype, parttype)\n\n\t\toperation = ""outdata[i] = dropout((float)indata[i], b[i], v, p)""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 dvec = __half22float2(indata2[i]);\n\t\t\t\tushort2 bvec = b2[i];\n\t\n\t\t\t\toutdata2[i] = __float22half2_rn(make_float2(\n\t\t\t\t\tdropout(dvec.x, bvec.x, v, p), dropout(dvec.y, bvec.y, v, p)\n\t\t\t\t));\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn dropoutKer\n\n\ndef dropout2d(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef dropout2dKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tparttype = {\n\t\t\tnp.float32: uint_t,\n\t\t\tnp.float16: ushort_t\n\t\t}[dtype.type]\n\n\t\tname = ""dropout2dKer""\n\t\targuments = [\n\t\t\t(ctype.ptr, ""outdata""), (ctype.const.ptr, ""indata""), (parttype.const.ptr, ""b""), (parttype, ""v""),\n\t\t\t(float_t, ""p""), (int_t, ""mapsize"")\n\t\t]\n\n\t\tpreambule = """"""\n\t\t__forceinline__ __device__\n\t\tfloat dropout(float x, %s b, %s v, float p) { return x * (b < v) / p; }\n\t\t"""""" % (parttype, parttype)\n\n\t\toperation = ""outdata[i] = dropout((float)indata[i], b[i / mapsize], v, p)""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 dvec = __half22float2(indata2[i]);\n\t\n\t\t\t\toutdata2[i] = __float22half2_rn(make_float2(\n\t\t\t\t\tdropout(dvec.x, b[2 * i / mapsize], v, p), dropout(dvec.y, b[(2 * i + 1) / mapsize], v, p)\n\t\t\t\t));\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn dropout2dKer\n\n\ndef toVectorAddVector(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef toVectorAddVectorKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tname = ""toVectorAddVectorKer""\n\t\targuments = [(ctype.ptr, ""outdata""), (ctype.const.ptr, ""indata""), (float_t, ""alpha"")]\n\n\t\toperation = ""outdata[i] = (float)outdata[i] + (float)indata[i] * alpha""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 outv = __half22float2(outdata2[i]), inv = __half22float2(indata2[i]);\n\t\t\t\toutdata2[i] = __float22half2_rn(make_float2(outv.x + inv.x * alpha, outv.y + inv.y * alpha));\n\t\t\t\t"""""",\n\t\t\t\toperation, name\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name)\n\n\treturn toVectorAddVectorKer\n\n\ndef adadelta(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef adadeltaKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\n\t\tctype = dtypeToCtype[dtype.type]\n\t\tname = ""adadeltaKer""\n\n\t\targuments = [\n\t\t\t(ctype.ptr, ""param""), (ctype.const.ptr, ""grad""), (ctype.ptr, ""msg""), (ctype.ptr, ""msdx""),\n\t\t\t(float_t, ""rho""), (float_t, ""epsilon"")\n\t\t]\n\n\t\tpreambule = """"""\n\t\t__forceinline__ __device__ void adadelta(float *param, float g, float *msg, float *msdx, float rho, float eps)\n\t\t{\n\t\t\t*msg += (1.0f - rho) * (g * g - *msg);\n\t\t\tfloat dx = sqrt((*msdx + eps) / (*msg + eps)) * g;\n\n\t\t\t*msdx += (1.0f - rho) * (dx * dx - *msdx);\n\t\t\t*param += dx;\n\t\t}\n\t\t""""""\n\n\t\toperation = """"""\n\t\tfloat paramval = (float)param[i], msgval = (float)msg[i], msdxval = (float)msdx[i];\n\t\tadadelta(&paramval, (float)grad[i], &msgval, &msdxval, rho, epsilon);\n\n\t\tparam[i] = paramval, msg[i] = msgval, msdx[i] = msdxval;\n\t\t""""""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 pvec = __half22float2(param2[i]), gvec = __half22float2(grad2[i]); \n\t\t\t\tfloat2 msgvec = __half22float2(msg2[i]), msdxvec = __half22float2(msdx2[i]);\n\n\t\t\t\tadadelta(&pvec.x, gvec.x, &msgvec.x, &msdxvec.x, rho, epsilon);\n\t\t\t\tadadelta(&pvec.y, gvec.y, &msgvec.y, &msdxvec.y, rho, epsilon);\n\n\t\t\t\tparam2[i] = __float22half2_rn(pvec);\n\t\t\t\tmsg2[i] = __float22half2_rn(msgvec), msdx2[i] = __float22half2_rn(msdxvec);\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn adadeltaKer\n\n\ndef adagrad(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef adagradKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\n\t\tctype = dtypeToCtype[dtype.type]\n\t\tname = ""adagradKer""\n\n\t\targuments = [\n\t\t\t(ctype.ptr, ""param""), (ctype.const.ptr, ""grad""), (ctype.ptr, ""h""),\n\t\t\t(float_t, ""learnRate""), (float_t, ""epsilon"")\n\t\t]\n\n\t\tpreambule = """"""\n\t\t__forceinline__ __device__ void adagrad(float *param, float g, float *h, float learnRate, float epsilon)\n\t\t{\n\t\t\t*h += g * g;\n\t\t\t*param += learnRate * g / (sqrtf(*h) + epsilon);\n\t\t}\n\t\t""""""\n\n\t\toperation = """"""\n\t\tfloat paramval = (float)param[i], hval = (float)h[i];\n\t\tadagrad(&paramval, (float)grad[i], &hval, learnRate, epsilon);\n\n\t\tparam[i] = paramval, h[i] = hval;\n\t\t""""""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 pvec = __half22float2(param2[i]), gvec = __half22float2(grad2[i]), hvec = __half22float2(h2[i]);\n\n\t\t\t\tadagrad(&pvec.x, gvec.x, &hvec.x, learnRate, epsilon);\n\t\t\t\tadagrad(&pvec.y, gvec.y, &hvec.y, learnRate, epsilon);\n\n\t\t\t\tparam2[i] = __float22half2_rn(pvec), h2[i] = __float22half2_rn(hvec);\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn adagradKer\n\n\ndef adam(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef adamKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tname = ""adamKer""\n\t\targuments = [\n\t\t\t(ctype.ptr, ""param""), (ctype.const.ptr, ""grad""), (float_t.ptr, ""mg""), (float_t.ptr, ""ms""),\n\t\t\t(float_t, ""learnRate""), (float_t, ""fix1""), (float_t, ""fix2""), (float_t, ""epsilon"")\n\t\t]\n\n\t\tpreambule = """"""\n\t\t__forceinline__ __device__ void adam(float *param, float g, float *mg, float *ms,\n\t\t\t\t\t\t\t\t\t\t\t float learnRate, float fix1, float fix2, float epsilon)\n\t\t{\n\t\t\t*mg += fix1 * (g - *mg);\n\t\t\t*ms += fix2 * (g * g - *ms);\n\n\t\t\t*param += learnRate * *mg / (sqrtf(*ms) + epsilon);\n\t\t}\n\t\t""""""\n\n\t\toperation = """"""\n\t\tfloat paramval = (float)param[i], mgval = (float)mg[i], msval = (float)ms[i];\n\t\tadam(&paramval, (float)grad[i], &mgval, &msval, learnRate, fix1, fix2, epsilon);\n\n\t\tparam[i] = paramval, mg[i] = mgval, ms[i] = msval;\n\t\t""""""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 pvec = __half22float2(param2[i]), gvec = __half22float2(grad2[i]);\n\n\t\t\t\tadam(&pvec.x, gvec.x, &mg[2 * i], &ms[2 * i], learnRate, fix1, fix2, epsilon);\n\t\t\t\tadam(&pvec.y, gvec.y, &mg[2 * i + 1], &ms[2 * i + 1], learnRate, fix1, fix2, epsilon);\n\n\t\t\t\tparam2[i] = __float22half2_rn(pvec);\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn adamKer\n\n\ndef classicMomSGD(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef classicMomSGDKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\n\t\tctype = dtypeToCtype[dtype.type]\n\t\tname = ""classicMomSGDKer""\n\n\t\targuments = [\n\t\t\t(ctype.ptr, ""param""), (ctype.const.ptr, ""grad""), (ctype.ptr, ""mom""),\n\t\t\t(float_t, ""learnRate""), (float_t, ""momRate"")\n\t\t]\n\n\t\tpreambule = """"""\n\t\t__forceinline__ __device__ void classicMomSGD(float *param, float g, float *mom, float learnRate, float momRate)\n\t\t{\n\t\t\t*mom = momRate * *mom + learnRate * g;\n\t\t\t*param += *mom;\n\t\t}\n\t\t""""""\n\n\t\toperation = """"""\n\t\tfloat paramval = (float)param[i], momval = (float)mom[i];\n\t\tclassicMomSGD(&paramval, (float)grad[i], &momval, learnRate, momRate);\n\n\t\tparam[i] = paramval, mom[i] = momval;\n\t\t""""""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 pvec = __half22float2(param2[i]), gvec = __half22float2(grad2[i]);\n\t\t\t\tfloat2 mvec = __half22float2(mom2[i]);\n\n\t\t\t\tclassicMomSGD(&pvec.x, gvec.x, &mvec.x, learnRate, momRate);\n\t\t\t\tclassicMomSGD(&pvec.y, gvec.y, &mvec.y, learnRate, momRate);\n\n\t\t\t\tparam2[i] = __float22half2_rn(pvec), mom2[i] = __float22half2_rn(mvec);\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn classicMomSGDKer\n\n\ndef nesterovMomSGD(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef nesterovMomSGDKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\n\t\tctype = dtypeToCtype[dtype.type]\n\t\tname = ""nesterovMomSGDKer""\n\n\t\targuments = [\n\t\t\t(ctype.ptr, ""param""), (ctype.const.ptr, ""grad""), (ctype.ptr, ""mom""),\n\t\t\t(float_t, ""learnRate""), (float_t, ""momRate"")\n\t\t]\n\n\t\tpreambule = """"""\n\t\t__forceinline__ __device__ void nesterovMomSGD(float *param, float g, float *m, float learnRate, float momRate)\n\t\t{\n\t\t\t*param += momRate * momRate * *m + (1.0f + momRate) * learnRate * g;\n\t\t\t*m = momRate * *m + learnRate * g;\n\t\t}\n\t\t""""""\n\n\t\toperation = """"""\n\t\tfloat paramval = (float)param[i], momval = (float)mom[i];\n\t\tnesterovMomSGD(&paramval, (float)grad[i], &momval, learnRate, momRate);\n\n\t\tparam[i] = paramval, mom[i] = momval;\n\t\t""""""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 pvec = __half22float2(param2[i]), gvec = __half22float2(grad2[i]);\n\t\t\t\tfloat2 mvec = __half22float2(mom2[i]);\n\n\t\t\t\tnesterovMomSGD(&pvec.x, gvec.x, &mvec.x, learnRate, momRate);\n\t\t\t\tnesterovMomSGD(&pvec.y, gvec.y, &mvec.y, learnRate, momRate);\n\n\t\t\t\tparam2[i] = __float22half2_rn(pvec), mom2[i] = __float22half2_rn(mvec);\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn nesterovMomSGDKer\n\n\ndef rmsprop(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef rmspropKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\n\t\tctype = dtypeToCtype[dtype.type]\n\t\tname = ""rmspropKer""\n\n\t\targuments = [\n\t\t\t(ctype.ptr, ""param""), (ctype.const.ptr, ""grad""), (ctype.ptr, ""ms""),\n\t\t\t(float_t, ""learnRate""), (float_t, ""factor""), (float_t, ""epsilon"")\n\t\t]\n\n\t\tpreambule = """"""\n\t\t__forceinline__ __device__ void rmsprop(float *param, float g, float *ms,\n\t\t\t\t\t\t\t\t\t\t\t\tfloat learnRate, float factor, float epsilon)\n\t\t{\n\t\t\t*ms = factor * *ms + (1.0f - factor) * g * g;\n\t\t\t*param += learnRate * g / (sqrtf(*ms) + epsilon);\n\t\t}\n\t\t""""""\n\n\t\toperation = """"""\n\t\tfloat paramval = (float)param[i], msval = (float)ms[i];\n\t\trmsprop(&paramval, (float)grad[i], &msval, learnRate, factor, epsilon);\n\n\t\tparam[i] = paramval, ms[i] = msval;\n\t\t""""""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 pvec = __half22float2(param2[i]), gvec = __half22float2(grad2[i]);\n\t\t\t\tfloat2 msvec = __half22float2(ms2[i]);\n\t\n\t\t\t\trmsprop(&pvec.x, gvec.x, &msvec.x, learnRate, factor, epsilon);\n\t\t\t\trmsprop(&pvec.y, gvec.y, &msvec.y, learnRate, factor, epsilon);\n\t\n\t\t\t\tparam2[i] = __float22half2_rn(pvec), ms2[i] = __float22half2_rn(msvec);\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn rmspropKer\n\n\ndef rmspropGraves(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef rmspropGravesKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\n\t\tctype = dtypeToCtype[dtype.type]\n\t\tname = ""rmspropGravesKer""\n\n\t\targuments = [\n\t\t\t(ctype.ptr, ""param""), (ctype.const.ptr, ""grad""), (ctype.ptr, ""mg""), (ctype.ptr, ""ms""), (ctype.ptr, ""delta""),\n\t\t\t(float_t, ""learnRate""), (float_t, ""alpha""), (float_t, ""momRate""), (float_t, ""epsilon"")\n\t\t]\n\n\t\tpreambule = """"""\n\t\t__forceinline__ __device__ void rmspropGraves(float *param, float g, float *mg, float *ms, float *delta,\n\t\t\t\t\t\t\t\t\t\t\t\t\t  float learnRate, float alpha, float momRate, float epsilon)\n\t\t{\n\t\t\t*mg = alpha * *mg + (1.0f - alpha) * g;\n\t\t\t*ms = alpha * *ms + (1.0f - alpha) * g * g;\n\t\t\t*delta = momRate * *delta + learnRate * g / sqrtf(*ms - *mg * *mg + epsilon);\n\n\t\t\t*param += *delta;\n\t\t}\n\t\t""""""\n\n\t\toperation = """"""\n\t\tfloat paramval = (float)param[i], mgval = (float)mg[i], msval = (float)ms[i], deltaval = (float)delta[i];\n\t\trmspropGraves(&paramval, (float)grad[i], &mgval, &msval, &deltaval, learnRate, alpha, momRate, epsilon);\n\n\t\tparam[i] = paramval, mg[i] = mgval, ms[i] = msval, delta[i] = deltaval;\n\t\t""""""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 pvec = __half22float2(param2[i]), gvec = __half22float2(grad2[i]); \n\t\t\t\tfloat2 mgvec = __half22float2(mg2[i]), msvec = __half22float2(ms2[i]), dvec = __half22float2(delta2[i]);\n\n\t\t\t\trmspropGraves(&pvec.x, gvec.x, &mgvec.x, &msvec.x, &dvec.x, learnRate, alpha, momRate, epsilon);\n\t\t\t\trmspropGraves(&pvec.y, gvec.y, &mgvec.y, &msvec.y, &dvec.y, learnRate, alpha, momRate, epsilon);\n\n\t\t\t\tparam2[i] = __float22half2_rn(pvec), delta2[i] = __float22half2_rn(dvec);\n\t\t\t\tmg2[i] = __float22half2_rn(mgvec), ms2[i] = __float22half2_rn(msvec);\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn rmspropGravesKer\n\n\ndef smorms3(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef smorms3Ker(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\n\t\tctype = dtypeToCtype[dtype.type]\n\t\tname = ""smorms3Ker""\n\n\t\targuments = [\n\t\t\t(ctype.ptr, ""param""), (ctype.const.ptr, ""grad""), (float_t.ptr, ""mem""),\n\t\t\t(float_t.ptr, ""mg""), (float_t.ptr, ""ms""), (float_t, ""learnRate""), (float_t, ""epsilon"")\n\t\t]\n\n\t\tpreambule = """"""\n\t\t__forceinline__ __device__ void smorms3(float *param, float g, float *mem, float *mg, float *ms,\n\t\t\t\t\t\t\t\t\t\t\t\tfloat learnRate, float epsilon)\n\t\t{\n\t\t\tfloat r = 1.0f / (*mem + 1.0f);\n\t\n\t\t\t*mg = (1.0f - r) * *mg + r * g;\n\t\t\t*ms = (1.0f - r) * *ms + r * g * g;\n\t\t\tfloat x = *mg * *mg / (*ms + epsilon);\n\n\t\t\t*mem = 1.0f + *mem * (1.0f - x);\n\t\t\t*param += g * min(learnRate, x) / (sqrtf(*ms) + epsilon);\n\t\t}\n\t\t""""""\n\n\t\toperation = """"""\n\t\tfloat paramval = (float)param[i], memval = (float)mem[i], mgval = (float)mg[i], msval = (float)ms[i];\n\t\tsmorms3(&paramval, (float)grad[i], &memval, &mgval, &msval, learnRate, epsilon);\n\n\t\tparam[i] = paramval, mem[i] = memval, mg[i] = mgval, ms[i] = msval;\n\t\t""""""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 pvec = __half22float2(param2[i]), gvec = __half22float2(grad2[i]);\n\n\t\t\t\tsmorms3(&pvec.x, gvec.x, &mem[2 * i], &mg[2 * i], &ms[2 * i], learnRate, epsilon);\n\t\t\t\tsmorms3(&pvec.y, gvec.y, &mem[2 * i + 1], &mg[2 * i + 1], &ms[2 * i + 1], learnRate, epsilon);\n\n\t\t\t\tparam2[i] = __float22half2_rn(pvec);\n\t\t\t\t"""""",\n\t\t\t\toperation, name, preambule=preambule\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name, preambule=preambule)\n\n\treturn smorms3Ker\n\n\ndef add(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef addKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\n\t\tctype = dtypeToCtype[dtype.type]\n\t\tname = ""addKer""\n\n\t\targuments = [\n\t\t\t(ctype.ptr, ""outdata""), (ctype.const.ptr, ""x""), (float_t, ""alpha""),\n\t\t\t(ctype.const.ptr, ""y""), (float_t, ""beta"")\n\t\t]\n\n\t\toperation = ""outdata[i] = (float)x[i] * alpha + (float)y[i] * beta""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 xv = __half22float2(x2[i]), yv = __half22float2(y2[i]);\n\t\t\t\toutdata2[i] = __float22half2_rn(make_float2(xv.x * alpha + yv.x * beta, xv.y * alpha + yv.y * beta));\n\t\t\t\t"""""",\n\t\t\t\toperation, name\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name)\n\n\treturn addKer\n\n\ndef mul(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef mulKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tname = ""mulKer""\n\t\targuments = [(ctype.ptr, ""outdata""), (ctype.const.ptr, ""a""), (ctype.const.ptr, ""b"")]\n\n\t\toperation = ""outdata[i] = (float)a[i] * (float)b[i]""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 av = __half22float2(a2[i]), bv = __half22float2(b2[i]);\n\t\t\t\toutdata2[i] = __float22half2_rn(make_float2(av.x * bv.x, av.y * bv.y));\n\t\t\t\t"""""",\n\t\t\t\toperation, name\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name)\n\n\treturn mulKer\n\n\ndef linear(ElementwiseKernel, ElementHalf2Kernel):\n\t@memoize\n\tdef linearKer(dtype):\n\t\tassert dtype.type in {np.float32, np.float16}\n\t\tctype = dtypeToCtype[dtype.type]\n\n\t\tname = ""linearKer""\n\t\targuments = [(ctype.ptr, ""outdata""), (ctype.const.ptr, ""indata""), (float_t, ""a""), (float_t, ""b"")]\n\n\t\toperation = ""outdata[i] = a * (float)indata[i] + b""\n\n\t\tif dtype == np.float16:\n\t\t\treturn ElementHalf2Kernel(\n\t\t\t\targuments,\n\t\t\t\t""""""\n\t\t\t\tfloat2 v = __half22float2(indata2[i]);\n\t\t\t\toutdata2[i] = __float22half2_rn(make_float2(a * v.x + b, a * v.y + b));\n\t\t\t\t"""""",\n\t\t\t\toperation, name\n\t\t\t)\n\n\t\telse:\n\t\t\treturn ElementwiseKernel(arguments, operation, name)\n\n\treturn linearKer\n\n\ndef rbmKer(ElementwiseKernel):\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""outdata""), (float_t.const.ptr, ""indata""), (float_t.const.ptr, ""uni"")],\n\t\t""float p = 1.0f / (1.0f + expf(-indata[i]));""\n\t\t""outdata[i] = (uni[i] < p)"",\n\t\t""rbmKer""\n\t)\n\n\ndef weightDecayKer(ElementwiseKernel):\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""grad""), (float_t.const.ptr, ""param""), (float_t, ""rate"")],\n\t\t""grad[i] -= rate * param[i]"",\n\t\t""weightDecayKer""\n\t)\n\n\ndef absKer(ElementwiseKernel):\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""outdata""), (float_t.const.ptr, ""indata"")],\n\t\t""outdata[i] = fabsf(indata[i])"",\n\t\t""absKer""\n\t)\n\n\ndef l1penaltyKer(ElementwiseKernel):\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""outgrad""), (float_t.const.ptr, ""ingrad""), (float_t.ptr, ""data""), (float_t, ""a"")],\n\t\t""outgrad[i] = ingrad[i] - a * ((0.0f <= data[i]) - (data[i] < 0.0f))"",\n\t\t""l1penaltyKer""\n\t)\n\n\ndef l1gradKer(ElementwiseKernel):\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""grad""), (float_t.const.ptr, ""pred""), (float_t.const.ptr, ""target""), (float_t, ""norm"")],\n\t\t""grad[i] = (pred[i] - target[i] > 0.0f ? -norm : norm)"",\n\t\t""l1gradKer""\n\t)\n\n\ndef castFP16toFP32(ElementwiseKernel):\n\treturn ElementwiseKernel(\n\t\t[(float_t.ptr, ""outdata""), (half_t.const.ptr, ""indata"")],\n\t\t""outdata[i] = indata[i]"",\n\t\t""castFP16toFP32"", preambule=""#include <cuda_fp16.h>""\n\t)\n\n\ndef castFP32toFP16(ElementwiseKernel):\n\treturn ElementwiseKernel(\n\t\t[(half_t.ptr, ""outdata""), (float_t.const.ptr, ""indata"")],\n\t\t""outdata[i] = indata[i]"",\n\t\t""castFP32toFP16"", preambule=""#include <cuda_fp16.h>""\n\t)\n'"
Cuda/Kernels/Embedder.py,12,"b'from string import Template\n\nimport numpy as np\n\nfrom PuzzleLib.Compiler.Codegen.Types import half_t, float_t\nfrom PuzzleLib.Cuda.Utils import roundUpDiv\nfrom PuzzleLib.Cuda.Kernels.Pad import atomicAddTmpl\n\n\nembedTmpl = Template(""""""\n\nextern ""C""\n__global__ void embed$ext($T *outdata, const int *indata, const $T *vocabulary, int size, int embsize)\n{\n\tint idy = blockIdx.y * blockDim.y + threadIdx.y;\n\tint idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n\tif (idy >= size || idx >= embsize) return;\n\n\tint wordidx = indata[idy];\n\tif (wordidx == -1) return;\n\n\toutdata[embsize * idy + idx] = vocabulary[embsize * wordidx + idx];\n}\n\nextern ""C""\n__global__ void embedBackwardParams$ext($T *vocabulary, const $T *outgrad, const int *indata,\n\t\t\t\t\t\t\t\t\t\tfloat scale, int size, int embsize)\n{\n\tint idy = blockIdx.y * blockDim.y + threadIdx.y;\n\tint idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n\tif (idy >= size || idx >= embsize) return;\n\n\tint wordidx = indata[idy];\n\tif (wordidx == -1) return;\n\n\t$T gr = scale * (float)outgrad[embsize * idy + idx];\n\tatomicAdd$ext(&vocabulary[embsize * wordidx + idx], gr);\n}\n\n"""""")\n\n\nclass EmbedModule:\n\tdef __init__(self, backend):\n\t\tself.backend = backend\n\t\tself.GPUArray, self.warpSize = backend.GPUArray, backend.warpSize\n\n\t\tself.mod = backend.SourceModule(""%s%s%s"" % (\n\t\t\tatomicAddTmpl, embedTmpl.substitute(T=half_t, ext=""FP16""), embedTmpl.substitute(T=float_t, ext="""")\n\t\t))\n\t\tself.block = (self.warpSize, backend.nthreads // self.warpSize, 1)\n\n\n\tdef embed(self, data, W, allocator=None):\n\t\tassert data.dtype == np.int32 and (W.dtype == np.float32 or W.dtype == np.float16)\n\n\t\tbatchsize, sentlen = data.shape\n\t\t_, embsize = W.shape\n\n\t\toutdata = self.GPUArray.zeros((batchsize, sentlen, embsize), dtype=W.dtype, allocator=allocator)\n\t\tsize = batchsize * sentlen\n\n\t\txblock, yblock, _ = self.block\n\t\tgrid = (roundUpDiv(embsize, xblock), roundUpDiv(size, yblock), 1)\n\n\t\tfn = self.mod.embed if W.dtype == np.float32 else self.mod.embedFP16\n\t\tfn(outdata, data, W, np.int32(size), np.int32(embsize), block=self.block, grid=grid)\n\n\t\treturn outdata\n\n\n\tdef embedBackwardParams(self, indata, grad, W, scale):\n\t\tassert indata.shape == grad.shape[:2] and W.shape[1] == grad.shape[2]\n\t\tassert indata.dtype == np.int32 and grad.dtype == W.dtype\n\n\t\tbatchsize, sentlen = indata.shape\n\t\t_, embsize = W.shape\n\n\t\tsize = batchsize * sentlen\n\n\t\txblock, yblock, _ = self.block\n\t\tgrid = (roundUpDiv(embsize, xblock), roundUpDiv(size, yblock), 1)\n\n\t\tfn = self.mod.embedBackwardParams if W.dtype == np.float32 else self.mod.embedBackwardParamsFP16\n\t\tfn(W, grad, indata, np.float32(scale), np.int32(size), np.int32(embsize), block=self.block, grid=grid)\n\n\ndef unittest():\n\tfrom PuzzleLib.Cuda import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tmodule = EmbedModule(Backend.getBackend(deviceIdx))\n\n\t\tfor dtype, atol in module.backend.dtypesSupported():\n\t\t\tembedTest(module, dtype, atol)\n\n\ndef embedTest(module, dtype, atol):\n\tbatchsize, sentlen, embsize = 10, 5, 20\n\tvocabsize = 1000\n\n\thostInData = np.random.randint(low=-1, high=vocabsize, size=(batchsize, sentlen), dtype=np.int32)\n\thostW = np.random.randn(vocabsize, embsize).astype(dtype)\n\n\tindata, W = module.GPUArray.toGpu(hostInData), module.GPUArray.toGpu(hostW)\n\toutdata = module.embed(indata, W)\n\n\thostOutData = np.zeros(outdata.shape, dtype=dtype)\n\n\tfor b in range(batchsize):\n\t\tfor s in range(sentlen):\n\t\t\twordidx = int(hostInData[b, s])\n\n\t\t\tif wordidx != -1:\n\t\t\t\thostOutData[b, s] = hostW[wordidx]\n\n\tassert np.allclose(hostOutData, outdata.get())\n\n\tlearnRate = 0.1\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad = module.GPUArray.toGpu(hostGrad)\n\tmodule.embedBackwardParams(indata, grad, W, learnRate)\n\n\thostGrad = grad.get()\n\tfor b in range(batchsize):\n\t\tfor s in range(sentlen):\n\t\t\twordidx = int(hostInData[b, s])\n\n\t\t\tif wordidx != -1:\n\t\t\t\thostW[wordidx] += learnRate * hostGrad[b, s]\n\n\tassert np.allclose(hostW, W.get(), atol=atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cuda/Kernels/MatVec.py,66,"b'from string import Template\nimport numpy as np\n\nfrom PuzzleLib.Compiler.Codegen.Types import half_t, float_t\nfrom PuzzleLib.Cuda.Utils import prod, roundUpDiv\n\n\nminMaxTmpl = Template(""""""\n\nextern ""C""\n__global__ void minMaxOnRow$ext(int *idx, const $T *mat, int w)\n{\n\tfloat curMax = $initVal;\n\tint curIdx = -1;\n\n\tfor (int i = threadIdx.x; i < w; i += $warpSize)\n\t{\n\t\tfloat val = mat[blockIdx.x * w + i];\n\n\t\tif (val $cmpOp curMax)\n\t\t\tcurMax = val, curIdx = i;\n\t}\n\n\tfor (int mask = $warpSize / 2; mask > 0; mask /= 2)\n\t{\n\t\tfloat mx = __shfl_xor_sync((unsigned)-1, curMax, mask, $warpSize);\n\t\tint idx = __shfl_xor_sync((unsigned)-1, curIdx, mask, $warpSize);\n\n\t\tif (mx $cmpOp curMax)\n\t\t\tcurMax = mx, curIdx = idx;\n\t}\n\n\tidx[blockIdx.x] = curIdx;\n}\n\n\nextern ""C""\n__global__ void minMaxOnCol$ext(int *idx, const $T *mat, int w, int h)\n{\n\tfloat curMax = $initVal;\n\tint curIdx = -1;\n\n\tint gid = threadIdx.x + blockIdx.x * $NT;\n\tif (gid >= w) return;\n\n\tfor (int i = 0; i < h; i += 1)\n\t{\n\t\tfloat val = mat[blockIdx.z * h * w + i * w + gid];\n\n\t\tif (val $cmpOp curMax)\n\t\t\tcurMax = val, curIdx = i;\n\t}\n\n\tidx[blockIdx.z * w + gid] = curIdx;\n}\n\n"""""")\n\n\nsumTmpl = Template(""""""\n\nextern ""C""\n__global__ void sumOnRow$ext($T *out, const $T *mat, int w, float alpha, float beta)\n{\n\tfloat acc = 0.0f;\n\n\tfor (int i = threadIdx.x; i < w; i += $warpSize)\n\t\tacc += (float)mat[blockIdx.x * w + i];\n\n\tfor (int mask = $warpSize / 2; mask > 0; mask /= 2)\n\t\tacc += __shfl_xor_sync((unsigned)-1, acc, mask, $warpSize);\n\n\tout[blockIdx.x] = ($T)(beta * (float)out[blockIdx.x] + alpha * acc);\n}\n\n\nextern ""C""\n__global__ void sumOnCol$ext($T *out, const $T *mat, int w, int h, float alpha, float beta)\n{\n\tfloat acc = 0.0f;\n\n\tint gid = threadIdx.x + blockIdx.x * $NT;\n\tif (gid >= w) return;\n\n\tfor (int i = 0; i < h; i += 1)\n\t\tacc += (float)mat[blockIdx.z * h * w + i * w + gid];\n\n\tout[blockIdx.z * w + gid] = ($T)(beta * (float)out[blockIdx.z * w + gid] + alpha * acc);\n}\n\n"""""")\n\n\nvecMulTmpl = Template(""""""\n\nextern ""C""\n__global__ void vecMulOnRow$ext($T *out, const $T *mat, const $T *vec, int w, int h, float alpha, float beta)\n{\n\tfloat acc = 0.0f;\n\n\tfor (int i = threadIdx.x; i < w; i += $warpSize)\n\t\tacc += (float)mat[blockIdx.z * h * w + blockIdx.x * w + i] * (float)vec[blockIdx.z * w + i];\n\n\tfor (int mask = $warpSize / 2; mask > 0; mask /= 2)\n\t\tacc += __shfl_xor_sync((unsigned)-1, acc, mask, $warpSize);\n\n\tout[blockIdx.z * h + blockIdx.x] = ($T)(beta * (float)out[blockIdx.z * h + blockIdx.x] + alpha * acc);\n}\n\n\nextern ""C""\n__global__ void vecMulOnCol$ext($T *out, const $T *mat, const $T *vec, int w, int h, float alpha, float beta)\n{\n\tfloat acc = 0.0f;\n\n\tint gid = threadIdx.x + blockIdx.x * $NT;\n\tif (gid >= w) return;\n\n\tfor (int i = 0; i < h; i += 1)\n\t\tacc += (float)mat[blockIdx.z * h * w + i * w + gid] * (float)vec[blockIdx.z * h + i];\n\n\tout[blockIdx.z * w + gid] = ($T)(beta * (float)out[blockIdx.z * w + gid] + alpha * acc);\n}\n\n"""""")\n\n\nvecMatTmpl = Template(""""""\n\nextern ""C""\n__global__ void opRowVecToMat$ext($T *out, const $T *vec, const $T *mat, int n, int m)\n{\n\tint tidx = blockIdx.x * blockDim.x + threadIdx.x;\n\tint tidy = blockIdx.y * blockDim.y + threadIdx.y;\n\tint tidz = blockIdx.z;\n\n\tint offset = tidz * m * n + tidy * m + tidx;\n\n\tif (tidy < n && tidx < m)\n\t\tout[offset] = (float)mat[offset] $op (float)vec[tidz * m + tidx];\n}\n\n\nextern ""C""\n__global__ void opColVecToMat$ext($T *out, const $T *vec, const $T *mat, int n, int m)\n{\n\tint tidx = blockIdx.x * blockDim.x + threadIdx.x;\n\tint tidy = blockIdx.y * blockDim.y + threadIdx.y;\n\tint tidz = blockIdx.z;\n\n\tint offset = tidz * m * n + tidy * m + tidx;\n\n\tif (tidy < n && tidx < m)\n\t\tout[offset] = (float)mat[offset] $op (float)vec[tidz * n + tidy];\n}\n\n\nextern ""C""\n__global__ void opRowOneVecToMat$ext($T *out, const $T *vec, const $T *mat, int n, int m, int p)\n{\n\tint tidx = blockIdx.x * blockDim.x + threadIdx.x;\n\tint tidy = blockIdx.y * blockDim.y + threadIdx.y;\n\tint tidz = blockIdx.z;\n\n\tint offset = tidz * m * n + tidy * m + tidx;\n\n\tif (tidy < n && tidx < m)\n\t\tout[offset] = (float)mat[offset] $op (float)vec[tidz * m + tidx % p];\n}\n\n"""""")\n\n\nclass MatModule:\n\tdef __init__(self, backend):\n\t\tself.backend = backend\n\n\t\tself.GPUArray, self.warpSize = backend.GPUArray, backend.warpSize\n\t\tself.NT = 256\n\n\t\tself.maxmod = backend.SourceModule(\n\t\t\t""#include <cuda_fp16.h>\\n\\n%s%s"" % (\n\t\t\t\tminMaxTmpl.substitute(\n\t\t\t\t\twarpSize=self.warpSize, NT=self.NT, initVal=np.finfo(np.float32).min, cmpOp="">"",\n\t\t\t\t\tT=half_t, ext=""FP16""\n\t\t\t\t),\n\t\t\t\tminMaxTmpl.substitute(\n\t\t\t\t\twarpSize=self.warpSize, NT=self.NT, initVal=np.finfo(np.float32).min, cmpOp="">"",\n\t\t\t\t\tT=float_t, ext=""""\n\t\t\t\t)\n\t\t\t)\n\t\t)\n\n\t\tself.minmod = backend.SourceModule(\n\t\t\t""#include <cuda_fp16.h>\\n\\n%s%s"" % (\n\t\t\t\tminMaxTmpl.substitute(\n\t\t\t\t\twarpSize=self.warpSize, NT=self.NT, initVal=np.finfo(np.float32).max, cmpOp=""<"",\n\t\t\t\t\tT=half_t, ext=""FP16""\n\t\t\t\t),\n\t\t\t\tminMaxTmpl.substitute(\n\t\t\t\t\twarpSize=self.warpSize, NT=self.NT, initVal=np.finfo(np.float32).max, cmpOp=""<"",\n\t\t\t\t\tT=float_t, ext=""""\n\t\t\t\t)\n\t\t\t)\n\t\t)\n\n\t\tself.summod = backend.SourceModule(\n\t\t\t""#include <cuda_fp16.h>\\n\\n%s%s"" % (\n\t\t\t\tsumTmpl.substitute(warpSize=self.warpSize, NT=self.NT, T=half_t, ext=""FP16""),\n\t\t\t\tsumTmpl.substitute(warpSize=self.warpSize, NT=self.NT, T=float_t, ext="""")\n\t\t\t)\n\t\t)\n\n\t\tself.mulmod = backend.SourceModule(\n\t\t\t""#include <cuda_fp16.h>\\n\\n%s%s"" % (\n\t\t\t\tvecMulTmpl.substitute(warpSize=self.warpSize, NT=self.NT, T=half_t, ext=""FP16""),\n\t\t\t\tvecMulTmpl.substitute(warpSize=self.warpSize, NT=self.NT, T=float_t, ext="""")\n\t\t\t)\n\t\t)\n\n\t\tself.addmod = backend.SourceModule(\n\t\t\t""#include <cuda_fp16.h>\\n\\n%s%s"" % (\n\t\t\t\tvecMatTmpl.substitute(op=""+"", T=half_t, ext=""FP16""),\n\t\t\t\tvecMatTmpl.substitute(op=""+"", T=float_t, ext="""")\n\t\t\t)\n\t\t)\n\n\t\tself.addblock = (self.warpSize, backend.nthreads // self.warpSize, 1)\n\n\n\tdef argminmax(self, tensor, axis, mode, allocator=None):\n\t\tassert tensor.dtype == np.float32 or tensor.dtype == np.float16\n\t\tassert 0 <= axis < tensor.ndim\n\n\t\tmod = {\n\t\t\t""max"": self.maxmod,\n\t\t\t""min"": self.minmod\n\t\t}[mode]\n\n\t\tif axis == tensor.ndim - 1:\n\t\t\tblock = (self.warpSize, 1, 1)\n\t\t\tgrid = (prod(tensor.shape[:-1]), 1, 1)\n\n\t\t\tidx = self.GPUArray.empty(tensor.shape[:-1], dtype=np.int32, allocator=allocator)\n\t\t\tfn = mod.minMaxOnRow if tensor.dtype == np.float32 else mod.minMaxOnRowFP16\n\n\t\t\tfn(idx, tensor, np.int32(tensor.dimAt(-1)), block=block, grid=grid)\n\n\t\telse:\n\t\t\tz, width = prod(tensor.shape[:axis]), prod(tensor.shape[axis + 1:])\n\n\t\t\tblock = (self.NT, 1, 1)\n\t\t\tgrid = (roundUpDiv(width, block[0]), 1, z)\n\n\t\t\tidx = self.GPUArray.empty(\n\t\t\t\ttensor.shape[:axis] + tensor.shape[axis + 1:], dtype=np.int32, allocator=allocator\n\t\t\t)\n\t\t\tfn = mod.minMaxOnCol if tensor.dtype == np.float32 else mod.minMaxOnColFP16\n\n\t\t\tfn(idx, tensor, np.int32(width), np.int32(tensor.dimAt(axis)), block=block, grid=grid)\n\n\t\treturn idx\n\n\n\tdef argmax(self, tensor, axis=0, allocator=None):\n\t\treturn self.argminmax(tensor, axis, ""max"", allocator)\n\n\n\tdef argmin(self, tensor, axis=0, allocator=None):\n\t\treturn self.argminmax(tensor, axis, ""min"", allocator)\n\n\n\tdef matsum(self, tensor, axis=0, out=None, alpha=1.0, beta=0.0, allocator=None):\n\t\tassert tensor.dtype == np.float32 or tensor.dtype == np.float16\n\t\tassert 0 <= axis < tensor.ndim\n\n\t\tif axis == tensor.ndim - 1:\n\t\t\tblock = (self.warpSize, 1, 1)\n\t\t\tgrid = (prod(tensor.shape[:-1]), 1, 1)\n\n\t\t\tif out is None:\n\t\t\t\tout = self.GPUArray.zeros(tensor.shape[:-1], dtype=tensor.dtype, allocator=allocator)\n\t\t\telse:\n\t\t\t\tassert out.shape == tensor.shape[:-1]\n\n\t\t\tfn = self.summod.sumOnRow if tensor.dtype == np.float32 else self.summod.sumOnRowFP16\n\t\t\tfn(out, tensor, np.int32(tensor.dimAt(-1)), np.float32(alpha), np.float32(beta), block=block, grid=grid)\n\n\t\telse:\n\t\t\tz, width = prod(tensor.shape[:axis]), prod(tensor.shape[axis + 1:])\n\n\t\t\tblock = (self.NT, 1, 1)\n\t\t\tgrid = (roundUpDiv(width, block[0]), 1, z)\n\n\t\t\tif out is None:\n\t\t\t\tout = self.GPUArray.zeros(\n\t\t\t\t\ttensor.shape[:axis] + tensor.shape[axis + 1:], dtype=tensor.dtype, allocator=allocator\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tassert out.shape == tensor.shape[:axis] + tensor.shape[axis + 1:]\n\n\t\t\tfn = self.summod.sumOnCol if tensor.dtype == np.float32 else self.summod.sumOnColFP16\n\t\t\tfn(\n\t\t\t\tout, tensor, np.int32(width), np.int32(tensor.dimAt(axis)), np.float32(alpha), np.float32(beta),\n\t\t\t\tblock=block, grid=grid\n\t\t\t)\n\n\t\treturn out\n\n\n\tdef matvec(self, mat, vec, axis=0, out=None, alpha=1.0, beta=0.0, allocator=None):\n\t\tassert vec.dtype == mat.dtype and (mat.dtype == np.float32 or mat.dtype == np.float16)\n\t\tassert vec.ndim == mat.ndim - 1 and 0 <= axis < 2\n\n\t\th, w = mat.shape[-2:]\n\n\t\tif axis == 1:\n\t\t\tassert mat.dimAt(-1) == vec.dimAt(-1)\n\n\t\t\tblock = (self.warpSize, 1, 1)\n\t\t\tgrid = (h, 1, prod(mat.shape[:-2]))\n\n\t\t\tif out is None:\n\t\t\t\tout = self.GPUArray.zeros(mat.shape[:-1], dtype=mat.dtype, allocator=allocator)\n\t\t\telse:\n\t\t\t\tassert out.shape == mat.shape[:-1]\n\n\t\t\tfn = self.mulmod.vecMulOnRow if mat.dtype == np.float32 else self.mulmod.vecMulOnRowFP16\n\t\t\tfn(out, mat, vec, np.int32(w), np.int32(h), np.float32(alpha), np.float32(beta), block=block, grid=grid)\n\n\t\telse:\n\t\t\tblock = (self.NT, 1, 1)\n\t\t\tgrid = (roundUpDiv(w, block[0]), 1, prod(mat.shape[:-2]))\n\n\t\t\tif out is None:\n\t\t\t\tout = self.GPUArray.zeros(mat.shape[:-2] + (w, ), dtype=mat.dtype, allocator=allocator)\n\t\t\telse:\n\t\t\t\tassert out.shape == mat.shape[:-2] + (w, )\n\n\t\t\tfn = self.mulmod.vecMulOnCol if mat.dtype == np.float32 else self.mulmod.vecMulOnColFP16\n\t\t\tfn(out, mat, vec, np.int32(w), np.int32(h), np.float32(alpha), np.float32(beta), block=block, grid=grid)\n\n\t\treturn out\n\n\n\tdef addVecToMat(self, vec, mat, axis=0, out=None, allocator=None):\n\t\tassert vec.dtype == mat.dtype and (mat.dtype == np.float32 or mat.dtype == np.float16)\n\t\tassert vec.ndim == mat.ndim - 1 and 0 <= axis < 2\n\n\t\tassert mat.shape[:-2] == vec.shape[:-1]\n\t\tout = self.GPUArray.empty(mat.shape, dtype=mat.dtype, allocator=allocator) if out is None else out\n\n\t\tz = prod(mat.shape[:-2])\n\t\tn, m = mat.shape[-2:]\n\n\t\txblock, yblock, _ = self.addblock\n\t\tgrid = (roundUpDiv(m, xblock), roundUpDiv(n, yblock), z)\n\n\t\tif axis == 1:\n\t\t\tif mat.dimAt(-1) == vec.dimAt(-1):\n\t\t\t\tfn = self.addmod.opRowVecToMat if mat.dtype == np.float32 else self.addmod.opRowVecToMatFP16\n\t\t\t\tfn(out, vec, mat, np.int32(n), np.int32(m), block=self.addblock, grid=grid)\n\n\t\t\telse:\n\t\t\t\tassert mat.dimAt(-1) % vec.dimAt(-1) == 0\n\n\t\t\t\tfn = self.addmod.opRowOneVecToMat if mat.dtype == np.float32 else self.addmod.opRowOneVecToMatFP16\n\t\t\t\tfn(out, vec, mat, np.int32(n), np.int32(m), np.int32(vec.dimAt(-1)), block=self.addblock, grid=grid)\n\n\t\telse:\n\t\t\tfn = self.addmod.opColVecToMat if mat.dtype == np.float32 else self.addmod.opColVecToMatFP16\n\t\t\tfn(out, vec, mat, np.int32(n), np.int32(m), block=self.addblock, grid=grid)\n\n\t\treturn out\n\n\ndef unittest():\n\tfrom PuzzleLib.Cuda import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tmodule = MatModule(Backend.getBackend(deviceIdx))\n\n\t\tfor dtype, atol in module.backend.dtypesSupported():\n\t\t\tcalcTest(module, dtype, atol)\n\t\t\tbatchCalcTest(module, dtype, atol)\n\n\t\t\tspeedTest(module, dtype)\n\t\t\tbatchSpeedTest(module, dtype)\n\n\ndef calcTest(module, dtype, atol):\n\thostA = np.random.randn(128, 500).astype(dtype)\n\thostU = np.random.randn(500).astype(dtype)\n\thostV = np.random.randn(128).astype(dtype)\n\thostW = np.random.randn(125).astype(dtype)\n\n\tA = module.GPUArray.toGpu(hostA)\n\tu, v, w = module.GPUArray.toGpu(hostU), module.GPUArray.toGpu(hostV), module.GPUArray.toGpu(hostW)\n\n\tassert np.allclose(module.addVecToMat(u, A, axis=1).get(), hostA + hostU[np.newaxis, :], atol=atol)\n\tassert np.allclose(module.addVecToMat(v, A, axis=0).get(), hostA + hostV[:, np.newaxis], atol=atol)\n\tassert np.allclose(module.addVecToMat(w, A, axis=1).get(), hostA + np.tile(hostW, 4)[np.newaxis, :], atol=atol)\n\n\tassert np.allclose(\n\t\tmodule.matsum(A, axis=1).get(), np.sum(hostA.astype(np.float32), axis=1).astype(dtype), atol=atol\n\t)\n\tassert np.allclose(\n\t\tmodule.matsum(A, axis=0).get(), np.sum(hostA.astype(np.float32), axis=0).astype(dtype), atol=atol\n\t)\n\n\tout = module.matvec(A, u, axis=1)\n\tassert np.allclose(out.get(), np.dot(hostA.astype(np.float32), hostU.astype(np.float32)).astype(dtype), atol=atol)\n\n\tout = module.matvec(A, v, axis=0)\n\tassert np.allclose(out.get(), np.dot(hostA.T.astype(np.float32), hostV.astype(np.float32)).astype(dtype), atol=atol)\n\n\thostA = 16.0 * np.random.randn(129, 501).astype(dtype)\n\tA = module.GPUArray.toGpu(hostA)\n\n\tassert np.allclose(module.argmax(A, axis=1).get(), np.argmax(hostA, axis=1))\n\tassert np.allclose(module.argmax(A, axis=0).get(), np.argmax(hostA, axis=0))\n\n\ndef batchCalcTest(module, dtype, atol):\n\thostA = np.random.randn(8, 32, 64).astype(dtype)\n\thostV = np.random.randn(8, 64).astype(dtype)\n\thostW = np.random.randn(8, 32).astype(dtype)\n\n\tA = module.GPUArray.toGpu(hostA)\n\tv, w = module.GPUArray.toGpu(hostV), module.GPUArray.toGpu(hostW)\n\n\tassert np.allclose(module.addVecToMat(w, A, axis=0).get(), hostA + hostW[:, :, np.newaxis])\n\tassert np.allclose(module.addVecToMat(v, A, axis=1).get(), hostA + hostV[:, np.newaxis, :])\n\n\tassert np.allclose(\n\t\tmodule.matsum(A, axis=1).get(), np.sum(hostA.astype(np.float32), axis=1).astype(dtype), atol=atol\n\t)\n\tassert np.allclose(\n\t\tmodule.matsum(A, axis=2).get(), np.sum(hostA.astype(np.float32), axis=2).astype(dtype), atol=atol\n\t)\n\n\tout = module.matvec(A, v, axis=1)\n\thostOut = np.empty(out.shape, dtype=np.float32)\n\n\tfor i in range(hostA.shape[0]):\n\t\tnp.dot(hostA[i].astype(np.float32), hostV[i].astype(np.float32), out=hostOut[i])\n\n\tassert np.allclose(out.get(), hostOut.astype(dtype), atol=atol)\n\n\tout = module.matvec(A, w, axis=0)\n\thostOut = np.empty(out.shape, dtype=np.float32)\n\n\tfor i in range(hostA.shape[0]):\n\t\tnp.dot(hostA[i].T.astype(np.float32), hostW[i].astype(np.float32), out=hostOut[i])\n\n\tassert np.allclose(out.get(), hostOut.astype(dtype), atol=atol)\n\n\thostA = np.random.normal(scale=16.0, size=(9, 33, 65)).astype(dtype)\n\tA = module.GPUArray.toGpu(hostA)\n\n\tassert np.allclose(module.argmax(A, axis=1).get(), np.argmax(hostA, axis=1))\n\tassert np.allclose(module.argmax(A, axis=2).get(), np.argmax(hostA, axis=2))\n\n\ndef speedTest(module, dtype):\n\tA = module.GPUArray.toGpu(np.random.randn(1024, 1024).astype(dtype))\n\tv = module.GPUArray.toGpu(np.random.randn(1024).astype(dtype))\n\n\tbnd = module.backend\n\tbnd.timeKernel(module.addVecToMat, (v, A, 1, A), logname=""%s addVecToMat on rows"" % dtype)\n\tbnd.timeKernel(module.addVecToMat, (v, A, 0, A), logname=""%s addVecToMat on cols"" % dtype)\n\n\tbnd.timeKernel(module.argmax, (A, 1, bnd.memoryPool), logname=""%s argmax on rows"" % dtype)\n\tbnd.timeKernel(module.argmax, (A, 0, bnd.memoryPool), logname=""%s argmax on cols"" % dtype)\n\n\tbnd.timeKernel(module.matsum, (A, 1), kwargs={""allocator"": bnd.memoryPool}, logname=""%s matsum on rows"" % dtype)\n\tbnd.timeKernel(module.matsum, (A, 0), kwargs={""allocator"": bnd.memoryPool}, logname=""%s matsum on cols"" % dtype)\n\n\ndef batchSpeedTest(module, dtype):\n\tA = module.GPUArray.toGpu(np.random.randn(32, 128, 128).astype(dtype))\n\tv = module.GPUArray.toGpu(np.random.randn(32, 128).astype(dtype))\n\n\tbnd = module.backend\n\tbnd.timeKernel(module.addVecToMat, (v, A, 1, A), logname=""%s batched addVecToMat on rows"" % dtype)\n\tbnd.timeKernel(module.addVecToMat, (v, A, 0, A), logname=""%s batched addVecToMat on cols"" % dtype)\n\n\tbnd.timeKernel(module.argmax, (A, 2, bnd.memoryPool), logname=""%s batched argmax on rows"" % dtype)\n\tbnd.timeKernel(module.argmax, (A, 1, bnd.memoryPool), logname=""%s batched argmax on cols"" % dtype)\n\n\tbnd.timeKernel(\n\t\tmodule.matsum, (A, 2), kwargs={""allocator"": bnd.memoryPool}, logname=""%s batched matsum on rows"" % dtype\n\t)\n\tbnd.timeKernel(\n\t\tmodule.matsum, (A, 1), kwargs={""allocator"": bnd.memoryPool}, logname=""%s batched matsum on cols"" % dtype\n\t)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cuda/Kernels/Memory.py,25,"b'import itertools\nfrom string import Template\n\nimport numpy as np\n\nfrom PuzzleLib.Compiler.Codegen.Types import half_t, float_t\nfrom PuzzleLib.Cuda.Utils import roundUpDiv\n\n\ntransformTmpl = Template(""""""\n\nextern ""C""\n__global__ void transform2d$ext($T * __restrict__ dst, const $T * __restrict__ src, int dstOffset, int srcOffset,\n\t\t\t\t\t\t\t\tint dstStride0, int dstStride1, int srcStride0, int len0, int len1)\n{\n\tfor (int i = threadIdx.x + $NT * blockIdx.x; i < len0 * len1; i += $NT * blockDim.x)\n\t{\n\t\tint i0 = i / len1, i1 = i % len1;\n\t\tdst[dstOffset + i0 * dstStride0 + i1 * dstStride1] = src[srcOffset + i0 * srcStride0 + i1];\n\t}\n}\n\n\nextern ""C""\n__global__ void transform3d$ext($T * __restrict__ dst, const $T * __restrict__ src, int dstOffset, int srcOffset,\n\t\t\t\t\t\t\t\tint dstStride0, int dstStride1, int dstStride2, int srcStride0, int srcStride1,\n\t\t\t\t\t\t\t\tint len0, int len1, int len2)\n{\n\tfor (int i = threadIdx.x + $NT * blockIdx.x; i < len0 * len1 * len2; i += $NT * blockDim.x)\n\t{\n\t\tint i0 = i / (len1 * len2), i1 = (i / len2) % len1, i2 = i % len2;\n\n\t\tint outoffset = dstOffset + i0 * dstStride0 + i1 * dstStride1 + i2 * dstStride2;\n\t\tint inoffset = srcOffset + i0 * srcStride0 + i1 * srcStride1 + i2;\n\n\t\tdst[outoffset] = src[inoffset];\n\t}\n}\n\n\nextern ""C""\n__global__ void transform4d$ext($T * __restrict__ dst, const $T * __restrict__ src, int dstOffset, int srcOffset,\n\t\t\t\t\t\t\t\tint dstStride0, int dstStride1, int dstStride2, int dstStride3,\n\t\t\t\t\t\t\t\tint srcStride0, int srcStride1, int srcStride2, int len0, int len1, int len2, int len3)\n{\n\tfor (int i = threadIdx.x + $NT * blockIdx.x; i < len0 * len1 * len2 * len3; i += $NT * blockDim.x)\n\t{\n\t\tint i0 = i / (len1 * len2 * len3), i1 = (i / (len2 * len3)) % len1;\n\t\tint i2 = (i / len3) % len2, i3 = i % len3;\n\n\t\tint outoffset = dstOffset + i0 * dstStride0 + i1 * dstStride1 + i2 * dstStride2 + i3 * dstStride3;\n\t\tint inoffset = srcOffset + i0 * srcStride0 + i1 * srcStride1 + i2 * srcStride2 + i3;\n\n\t\tdst[outoffset] = src[inoffset];\n\t}\n}\n\n\nextern ""C""\n__global__ void transform5d$ext($T * __restrict__ dst, const $T * __restrict__ src, int dstOffset, int srcOffset,\n\t\t\t\t\t\t\t\tint dstStride0, int dstStride1, int dstStride2, int dstStride3, int dstStride4,\n\t\t\t\t\t\t\t\tint srcStride0, int srcStride1, int srcStride2, int srcStride3,\n\t\t\t\t\t\t\t\tint len0, int len1, int len2, int len3, int len4)\n{\n\tfor (int i = threadIdx.x + $NT * blockIdx.x; i < len0 * len1 * len2 * len3 * len4; i += $NT * blockDim.x)\n\t{\n\t\tint i0 = i / (len1 * len2 * len3 * len4), i1 = (i / (len2 * len3 * len4)) % len1;\n\t\tint i2 = (i / (len3 * len4)) % len2, i3 = (i / len4) % len3, i4 = i % len4;\n\t\t\n\t\tint offs = dstOffset + i0 * dstStride0 + i1 * dstStride1 + i2 * dstStride2 + i3 * dstStride3 + i4 * dstStride4;\n\t\tint inoffset = srcOffset + i0 * srcStride0 + i1 * srcStride1 + i2 * srcStride2 + i3 * srcStride3 + i4;\n\n\t\tdst[offs] = src[inoffset];\n\t}\n}\n\n"""""")\n\n\nclass MemoryModule:\n\tdef __init__(self, backend):\n\t\tself.backend = backend\n\t\tself.GPUArray, self.NT = backend.GPUArray, backend.nthreads\n\n\t\tself.mod = backend.SourceModule(""#include <cuda_fp16.h>\\n\\n%s%s"" % (\n\t\t\ttransformTmpl.substitute(NT=self.NT, T=half_t, ext=""FP16""),\n\t\t\ttransformTmpl.substitute(NT=self.NT, T=float_t, ext="""")\n\t\t))\n\n\n\tdef transform(self, tensor, shape, strides, out, inoffset=0, outoffset=0):\n\t\tassert tensor.dtype == np.float32 or tensor.dtype == np.float16\n\t\tassert tensor.ndim <= 5 and tensor.ndim == len(strides) and tensor.ndim == len(shape)\n\n\t\tndim = tensor.ndim\n\n\t\tif ndim == 1:\n\t\t\tout.set(tensor)\n\t\t\treturn out\n\n\t\tif ndim == 2:\n\t\t\ttransform = self.mod.transform2d if tensor.dtype == np.float32 else self.mod.transform2dFP16\n\t\telif ndim == 3:\n\t\t\ttransform = self.mod.transform3d if tensor.dtype == np.float32 else self.mod.transform3dFP16\n\t\telif ndim == 4:\n\t\t\ttransform = self.mod.transform4d if tensor.dtype == np.float32 else self.mod.transform4dFP16\n\t\telif ndim == 5:\n\t\t\ttransform = self.mod.transform5d if tensor.dtype == np.float32 else self.mod.transform5dFP16\n\t\telse:\n\t\t\tassert False\n\n\t\ttransform(\n\t\t\tout, tensor, np.int32(outoffset), np.int32(inoffset),\n\t\t\t*(np.int32(s // tensor.dtype.itemsize) for s in strides),\n\t\t\t*(np.int32(s // tensor.dtype.itemsize) for s in tensor.strides[:-1]),\n\t\t\t*(np.int32(dim) for dim in shape),\n\t\t\tblock=(self.NT, 1, 1), grid=(roundUpDiv(tensor.size, self.NT), 1, 1)\n\t\t)\n\n\t\treturn out\n\n\n\tdef transpose(self, tensor, axes=None, out=None, allocator=None):\n\t\tassert axes is None or len(axes) == tensor.ndim\n\n\t\taxes = tuple(reversed(range(tensor.ndim))) if axes is None else axes\n\t\tshape = tuple(tensor.dimAt(axis) for axis in axes)\n\n\t\tif out is None:\n\t\t\tout = self.GPUArray.empty(shape, dtype=tensor.dtype, allocator=allocator)\n\t\telse:\n\t\t\tassert out.shape == shape\n\n\t\toutstrides = [0] * len(axes)\n\t\tfor i, axis in enumerate(axes):\n\t\t\toutstrides[axis] = out.strideAt(i)\n\n\t\treturn self.transform(tensor, tensor.shape, outstrides, out)\n\n\n\tdef moveaxis(self, data, src, dst, out=None, allocator=None):\n\t\tif src < dst:\n\t\t\taxes = tuple(range(src)) + tuple(range(src + 1, dst + 1)) + (src, ) + tuple(range(dst + 1, data.ndim))\n\t\telse:\n\t\t\taxes = tuple(range(dst)) + (src, ) + tuple(range(dst, src)) + tuple(range(src + 1, data.ndim))\n\n\t\treturn self.transpose(data, axes, out=out, allocator=allocator)\n\n\n\tdef swapaxes(self, data, axis1, axis2, out=None, allocator=None):\n\t\tif axis1 == axis2:\n\t\t\taxes = tuple(range(data.ndim))\n\n\t\telse:\n\t\t\taxis1, axis2 = (axis1, axis2) if axis1 < axis2 else (axis2, axis1)\n\t\t\taxes = tuple(range(axis1)) + (axis2, ) + tuple(range(axis1 + 1, axis2)) + \\\n\t\t\t\t   (axis1, ) + tuple(range(axis2 + 1, data.ndim))\n\n\t\treturn self.transpose(data, axes, out=out, allocator=allocator)\n\n\n\tdef depthConcat(self, tensors, out=None, allocator=None):\n\t\tassert all(tn.ndim == 4 and tn.dtype == tensors[0].dtype for tn in tensors)\n\t\tassert all(tn.dimAt(0) == tensors[0].dimAt(0) for tn in tensors)\n\n\t\th, w, depth = 0, 0, 0\n\t\tfor tn in tensors:\n\t\t\tdepth += tn.dimAt(1)\n\t\t\th, w = max(h, tn.dimAt(2)), max(w, tn.dimAt(3))\n\n\t\tif out is None:\n\t\t\tout = self.GPUArray.zeros(\n\t\t\t\tshape=(tensors[0].dimAt(0), depth, h, w), dtype=tensors[0].dtype, allocator=allocator\n\t\t\t)\n\t\telse:\n\t\t\tassert out.shape == (tensors[0].dimAt(0), depth, h, w)\n\n\t\tstride = 0\n\t\tfor i, tn in enumerate(tensors):\n\t\t\tcenter = (h - tn.dimAt(2)) // 2 * out.strideAt(2) + (w - tn.dimAt(3)) // 2 * out.strideAt(3)\n\n\t\t\tself.transform(tn, tn.shape, out.strides, out=out, outoffset=stride + center // tn.dtype.itemsize)\n\t\t\tstride += out.strideAt(1) * tn.dimAt(1) // tn.dtype.itemsize\n\n\t\treturn out\n\n\n\tdef depthSplit(self, grad, tensors, allocator=None):\n\t\tassert all(tn.ndim == 4 and tn.dtype == tensors[0].dtype for tn in tensors)\n\t\tassert all(tn.dimAt(0) == tensors[0].dimAt(0) for tn in tensors)\n\n\t\tingrads = [self.GPUArray.empty(shape=tn.shape, dtype=tn.dtype, allocator=allocator) for tn in tensors]\n\n\t\tstride = 0\n\t\tfor i, gr in enumerate(ingrads):\n\t\t\tcenter = (grad.dimAt(2) - gr.dimAt(2)) // 2 * grad.strideAt(2) + (grad.dimAt(3) - gr.dimAt(3)) // 2 * \\\n\t\t\t\t\t grad.strideAt(3)\n\n\t\t\tself.transform(grad, gr.shape, gr.strides, gr, inoffset=stride + center // gr.dtype.itemsize)\n\t\t\tstride += grad.strideAt(1) * gr.dimAt(1) // gr.dtype.itemsize\n\n\t\treturn ingrads\n\n\ndef unittest():\n\tfrom PuzzleLib.Cuda import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tmodule = MemoryModule(Backend.getBackend(deviceIdx, initmode=2))\n\n\t\tfor dtype, _ in module.backend.dtypesSupported():\n\t\t\ttransposeTest(module.backend, module, dtype)\n\t\t\tmoveAxisTest(module.backend, module, dtype)\n\t\t\tswapAxesTest(module.backend, module, dtype)\n\t\t\tdepthConcatTest(module.backend, module, dtype)\n\n\ndef transposeTest(bnd, module, dtype):\n\tshapes = [(10, ), (10, 3), (10, 3, 5, 4, 2)]\n\n\tfor shape in shapes:\n\t\tfor axes in itertools.permutations(range(len(shape))):\n\t\t\thostData = np.random.randn(*shape).astype(dtype)\n\n\t\t\tdata = bnd.GPUArray.toGpu(hostData)\n\t\t\toutdata = module.transpose(data, axes=axes)\n\n\t\t\thostOutData = np.transpose(hostData, axes=axes)\n\t\t\tassert np.allclose(hostOutData, outdata.get())\n\n\ndef moveAxisTest(bnd, module, dtype):\n\tshapes = [(10, ), (10, 3), (10, 3, 5, 4, 2)]\n\n\tfor shape in shapes:\n\t\tfor src, dst in itertools.product(range(len(shape)), range(len(shape))):\n\t\t\thostData = np.random.randn(*shape).astype(dtype)\n\n\t\t\tdata = bnd.GPUArray.toGpu(hostData)\n\t\t\toutdata = module.moveaxis(data, src=src, dst=dst)\n\n\t\t\thostOutData = np.moveaxis(hostData, source=src, destination=dst)\n\t\t\tassert np.allclose(hostOutData, outdata.get())\n\n\ndef swapAxesTest(bnd, module, dtype):\n\tshapes = [(10, ), (10, 3), (10, 3, 5, 4, 2)]\n\n\tfor shape in shapes:\n\t\tfor axis1, axis2 in itertools.product(range(len(shape)), range(len(shape))):\n\t\t\thostData = np.random.randn(*shape).astype(dtype)\n\n\t\t\tdata = bnd.GPUArray.toGpu(hostData)\n\t\t\toutdata = module.swapaxes(data, axis1=axis1, axis2=axis2)\n\n\t\t\thostOutData = np.swapaxes(hostData, axis1=axis1, axis2=axis2)\n\t\t\tassert np.allclose(hostOutData, outdata.get())\n\n\ndef depthConcatTest(bnd, module, dtype):\n\thostData1 = np.random.randn(3, 4, 3, 3).astype(dtype)\n\thostData2 = np.random.randn(3, 2, 6, 6).astype(dtype)\n\thostData3 = np.random.randn(3, 5, 4, 4).astype(dtype)\n\tallHostData = [hostData1, hostData2, hostData3]\n\n\tallData = [bnd.GPUArray.toGpu(data) for data in allHostData]\n\toutdata = module.depthConcat(allData)\n\n\tdepth, h, w = 0, 0, 0\n\tfor data in allHostData:\n\t\tdepth += data.shape[1]\n\t\th, w = max(h, data.shape[2]), max(w, data.shape[3])\n\n\thostOutData = np.zeros(shape=(allHostData[0].shape[0], depth, h, w), dtype=dtype)\n\n\thostOutData[:, :4, 1:4, 1:4] = hostData1\n\thostOutData[:, 4:6, :, :] = hostData2\n\thostOutData[:, 6:, 1:5, 1:5] = hostData3\n\n\tassert np.allclose(hostOutData, outdata.get())\n\n\thostGrad = np.random.randn(*hostOutData.shape).astype(dtype)\n\n\tgrad = bnd.GPUArray.toGpu(hostGrad)\n\tingrads = module.depthSplit(grad, allData)\n\n\thostInGrads = [\n\t\thostGrad[:, :4, 1:4, 1:4],\n\t\thostGrad[:, 4:6, :, :],\n\t\thostGrad[:, 6:, 1:5, 1:5]\n\t]\n\n\tassert all(np.allclose(hostInGrad, ingrads[i].get()) for i, hostInGrad in enumerate(hostInGrads))\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cuda/Kernels/PRelu.py,20,"b'from string import Template\n\nimport numpy as np\n\nfrom PuzzleLib.Compiler.Codegen.Types import float_t\n\nfrom PuzzleLib.Cuda.Utils import prod, roundUpDiv\nfrom PuzzleLib.Cuda.Kernels.MatVec import MatModule\n\n\npreluTmpl = Template(""""""\n\nextern ""C""\n__global__ void prelu($T *outdata, const $T *indata, const float *slopes, int divFactor,\n\t\t\t\t\t  int mapsize, int maps, int size)\n{\n\tfor (int index = blockIdx.x * blockDim.x + threadIdx.x; index < size; index += blockDim.x * gridDim.x)\n\t{\n\t\tint c = (index / mapsize) % maps / divFactor;\n\n\t\tfloat data = (float)indata[index];\n\t\toutdata[index] = data * (data > 0.0f ? 1.0f : slopes[c]);\n\t}\n}\n\nextern ""C""\n__global__ void preluBackwardData($T *ingrad, const $T *outgrad, const float *slopes, const $T *indata,\n\t\t\t\t\t\t\t\t  int divFactor, int mapsize, int maps, int size)\n{\n\tfor (int index = blockIdx.x * blockDim.x + threadIdx.x; index < size; index += blockDim.x * gridDim.x)\n\t{\n\t\tint c = (index / mapsize) % maps / divFactor;\n\n\t\tfloat data = (float)indata[index];\n\t\tingrad[index] = (float)outgrad[index] * ((data > 0.0f) + (data <= 0.0f) * slopes[c]);\n\t}\n}\n\nextern ""C""\n__global__ void preluBackwardParams(float *slopegrad, const float *outgrad, const float *indata,\n\t\t\t\t\t\t\t\t\tint batchsize, int stride, int size)\n{\n\tfor (int index = blockIdx.x * blockDim.x + threadIdx.x; index < size; index += blockDim.x * gridDim.x)\n\t{\n\t\tfloat sgrad = 0.0f;\n\n\t\tfor (int b = 0; b < batchsize; b++)\n\t\t{\n\t\t\tfloat data = (float)indata[index + b * stride];\n\t\t\tsgrad += outgrad[index + b * stride] * data * (data <= 0.0f);\n\t\t}\n\n\t\tslopegrad[index] = sgrad;\n\t}\n}\n\n"""""")\n\n\nclass PReluModule:\n\tdef __init__(self, matmod):\n\t\tbackend = matmod.backend\n\t\tself.GPUArray, self.nthreads = backend.GPUArray, backend.nthreads\n\n\t\tself.mod = backend.SourceModule(preluTmpl.substitute(T=float_t))\n\t\tself.matmod = matmod\n\n\n\tdef prelu(self, data, slopes, inplace=False, sharedMaps=False, allocator=None):\n\t\tassert data.dtype == slopes.dtype and slopes.dtype == np.float32\n\t\tassert slopes.shape == (1, ) if sharedMaps else data.shape[1] == slopes.shape[0]\n\n\t\toutdata = data if inplace else self.GPUArray.empty(data.shape, dtype=np.float32, allocator=allocator)\n\n\t\tmapsize = prod(data.shape[2:])\n\t\tsize = prod(data.shape)\n\n\t\tblock = (self.nthreads, 1, 1)\n\t\tgrid = (roundUpDiv(size, self.nthreads), 1, 1)\n\n\t\tdivFactor = data.shape[1] if sharedMaps else 1\n\n\t\tself.mod.prelu(\n\t\t\toutdata, data, slopes, np.int32(divFactor), np.int32(mapsize), np.int32(data.shape[1]), np.int32(size),\n\t\t\tblock=block, grid=grid\n\t\t)\n\n\t\treturn outdata\n\n\n\tdef preluBackwardData(self, grad, slopes, indata, sharedMaps=False, allocator=None):\n\t\tassert grad.dtype == slopes.dtype and slopes.dtype == indata.dtype and indata.dtype == np.float32\n\t\tassert grad.shape == indata.shape\n\t\tassert slopes.shape == (1, ) if sharedMaps else grad.shape[1] == slopes.shape[0]\n\n\t\tingrad = self.GPUArray.empty(grad.shape, dtype=np.float32, allocator=allocator)\n\n\t\tmapsize = prod(grad.shape[2:])\n\t\tsize = prod(grad.shape)\n\n\t\tblock = (self.nthreads, 1, 1)\n\t\tgrid = (roundUpDiv(size, self.nthreads), 1, 1)\n\n\t\tdivFactor = grad.shape[1] if sharedMaps else 1\n\n\t\tself.mod.preluBackwardData(\n\t\t\tingrad, grad, slopes, indata, np.int32(divFactor), np.int32(mapsize), np.int32(grad.shape[1]),\n\t\t\tnp.int32(size), block=block, grid=grid\n\t\t)\n\n\t\treturn ingrad\n\n\n\tdef preluBackwardParams(self, indata, outgrad, sharedMaps=False, allocator=None):\n\t\tassert indata.dtype == outgrad.dtype and outgrad.dtype == np.float32\n\t\tassert indata.shape == outgrad.shape\n\n\t\tsize = prod(outgrad.shape[1:])\n\t\tstride = prod(outgrad.shape[1:])\n\n\t\tblock = (self.nthreads, 1, 1)\n\t\tgrid = (roundUpDiv(size, self.nthreads), 1, 1)\n\n\t\tslopegrad = self.GPUArray.empty(outgrad.shape[1:], dtype=np.float32, allocator=allocator)\n\n\t\tself.mod.preluBackwardParams(\n\t\t\tslopegrad, outgrad, indata, np.int32(outgrad.shape[0]), np.int32(stride), np.int32(size),\n\t\t\tblock=block, grid=grid\n\t\t)\n\n\t\tshape = (1, prod(slopegrad.shape)) if sharedMaps else (slopegrad.shape[0], prod(slopegrad.shape[1:]))\n\t\treturn self.matmod.matsum(slopegrad.reshape(shape), axis=1)\n\n\ndef unittest():\n\tfrom PuzzleLib.Cuda import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tbackend = Backend.getBackend(deviceIdx)\n\t\tmodule = PReluModule(MatModule(backend))\n\n\t\tpreluTest(module)\n\n\ndef preluTest(module):\n\tbatchsize, maps, h, w = 5, 4, 6, 6\n\n\thostData = np.random.randn(batchsize, maps, h, w).astype(np.float32)\n\thostSlopes = np.random.randn(maps).astype(np.float32)\n\n\tdata, slopes = module.GPUArray.toGpu(hostData), module.GPUArray.toGpu(hostSlopes)\n\toutdata = module.prelu(data, slopes)\n\n\thostOutData = np.empty(outdata.shape, dtype=np.float32)\n\n\tfor c in range(maps):\n\t\thostOutData[:, c] = (hostData[:, c] > 0.0) * hostData[:, c] + \\\n\t\t\t\t\t\t\t(hostData[:, c] <= 0.0) * hostSlopes[c] * hostData[:, c]\n\n\tassert np.allclose(hostOutData, outdata.get())\n\n\thostGrad = np.random.randn(*outdata.shape).astype(np.float32)\n\n\tgrad = module.GPUArray.toGpu(hostGrad)\n\tingrad = module.preluBackwardData(grad, slopes, data)\n\n\thostInGrad = np.empty(ingrad.shape, dtype=np.float32)\n\n\tfor c in range(maps):\n\t\thostInGrad[:, c] = hostGrad[:, c] * ((hostData[:, c] > 0.0) + (hostData[:, c] <= 0.0) * hostSlopes[c])\n\n\tassert np.allclose(hostInGrad, ingrad.get())\n\n\tslopegrad = module.preluBackwardParams(data, grad)\n\thostSlopeGrad = np.empty(slopegrad.shape, dtype=np.float32)\n\n\tfor c in range(maps):\n\t\thostSlopeGrad[c] = np.sum(hostGrad[:, c] * hostData[:, c] * (hostData[:, c] <= 0.0))\n\n\tassert np.allclose(hostSlopeGrad, slopegrad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cuda/Kernels/Pad.py,26,"b'from string import Template\n\nimport numpy as np\n\nfrom PuzzleLib.Compiler.Codegen.Types import half_t, float_t\nfrom PuzzleLib.Cuda.Utils import roundUpDiv\n\n\natomicAddTmpl = """"""\n\n#include <cuda_fp16.h>\n\n\n__device__ __forceinline__ void atomicAddFP16(half *address, half val)\n{\n#if __CUDA_ARCH__ < 700\n\tconst size_t halfbits = 8 * sizeof(half);\n\tsize_t offset = (size_t)address & 2;\n\n\tunsigned *addrUI = (unsigned *)((size_t)address - offset);\n\tunsigned assumed, old = *addrUI;\n\n\tdo\n\t{\n\t\tassumed = old;\n\n\t\tunsigned short s = offset ? (old >> halfbits) : (old & 0xFFFF);\n\t\ts = __half_as_short((float)__short_as_half(s) + (float)val);\n\n\t\tunsigned packed = offset ? ((old & 0xFFFF) | (s << halfbits)) : ((old & 0xFFFF0000) | s);\n\t\told = atomicCAS(addrUI, assumed, packed);\n\t}\n\twhile (assumed != old);\n\n#else\n\tatomicAdd(address, val);\n\n#endif\n}\n\n""""""\n\n\nmapTmpl = """"""\n\n__device__ __forceinline__ void map1d(int insize, int outsize, int index, int lpad, int *inindex, int *outindex)\n{\n\tint inoffset = (blockIdx.y + blockIdx.z * gridDim.y) * insize;\n\tint outoffset = (blockIdx.y + blockIdx.z * gridDim.y) * outsize;\n\n\tint instart = max(0, -lpad), outstart = max(0, lpad);\n\n\tint x = abs(index - lpad) - abs(index - (insize + lpad - 1)) - index + 2 * lpad + insize - 1 - outstart + instart;\n\t*inindex = inoffset + x, *outindex = outoffset + index;\n}\n\n__device__ __forceinline__ void map2d(int inh, int inw, int outh, int outw, int index, int upad, int lpad,\n\t\t\t\t\t\t\t\t\t  int *inindex, int *outindex)\n{\n\tint inoffset = (blockIdx.y + blockIdx.z * gridDim.y) * inh * inw;\n\tint outoffset = (blockIdx.y + blockIdx.z * gridDim.y) * outh * outw;\n\n\tint outx = index % outw, outy = index / outw;\n\n\tint instartx = max(0, -lpad), outstartx = max(0, lpad);\n\tint instarty = max(0, -upad), outstarty = max(0, upad);\n\n\tint inx = abs(outx - lpad) - abs(outx - (inw + lpad - 1)) - outx + 2 * lpad + inw - 1 - outstartx + instartx;\n\tint iny = abs(outy - upad) - abs(outy - (inh + upad - 1)) - outy + 2 * upad + inh - 1 - outstarty + instarty;\n\n\t*inindex = inoffset + iny * inw + inx;\n\t*outindex = outoffset + outy * outw + outx;\n}\n\n""""""\n\n\npadTmpl = Template(""""""\n\nextern ""C""\n__global__ void reflectpad1d$ext($T *outdata, const $T *indata, int insize, int lpad, int rpad)\n{\n\tint index = threadIdx.x + blockIdx.x * blockDim.x;\n\tint outsize = insize + lpad + rpad;\n\n\tif (index < outsize)\n\t{\n\t\tint inindex = 0, outindex = 0;\n\t\tmap1d(insize, outsize, index, lpad, &inindex, &outindex);\n\n\t\toutdata[outindex] = indata[inindex];\n\t}\n}\n\nextern ""C""\n__global__ void reflectpad1dBackward$ext($T *ingrad, const $T *outgrad, int insize, int lpad, int rpad)\n{\n\tint index = threadIdx.x + blockIdx.x * blockDim.x;\n\tint outsize = insize + lpad + rpad;\n\n\tif (index < outsize)\n\t{\n\t\tint inindex = 0, outindex = 0;\n\t\tmap1d(insize, outsize, index, lpad, &inindex, &outindex);\n\n\t\tatomicAdd$ext(&ingrad[inindex], outgrad[outindex]);\n\t}\n}\n\nextern ""C""\n__global__ void reflectpad2d$ext($T *outdata, const $T *indata, int inh, int inw,\n\t\t\t\t\t\t\t\t int upad, int bpad, int lpad, int rpad)\n{\n\tint index = threadIdx.x + blockIdx.x * blockDim.x;\n\tint outh = inh + upad + bpad, outw = inw + lpad + rpad;\n\n\tif (index < outh * outw)\n\t{\n\t\tint inindex = 0, outindex = 0;\n\t\tmap2d(inh, inw, outh, outw, index, upad, lpad, &inindex, &outindex);\n\n\t\toutdata[outindex] = indata[inindex];\n\t}\n}\n\nextern ""C""\n__global__ void reflectpad2dBackward$ext($T *ingrad, const $T *outgrad, int inh, int inw,\n\t\t\t\t\t\t\t\t\t\t int upad, int bpad, int lpad, int rpad)\n{\n\tint index = threadIdx.x + blockIdx.x * blockDim.x;\n\tint outh = inh + upad + bpad, outw = inw + lpad + rpad;\n\n\tif (index < outh * outw)\n\t{\n\t\tint inindex = 0, outindex = 0;\n\t\tmap2d(inh, inw, outh, outw, index, upad, lpad, &inindex, &outindex);\n\n\t\tatomicAdd$ext(&ingrad[inindex], outgrad[outindex]);\n\t}\n}\n\n"""""")\n\n\nclass PadModule:\n\tdef __init__(self, backend):\n\t\tself.backend = backend\n\t\tself.GPUArray, self.warpSize = backend.GPUArray, backend.warpSize\n\n\t\tself.mod = backend.SourceModule(""%s%s%s%s"" % (\n\t\t\tatomicAddTmpl, mapTmpl, padTmpl.substitute(T=half_t, ext=""FP16""), padTmpl.substitute(T=float_t, ext="""")\n\t\t))\n\n\n\tdef reflectpad(self, data, pad, allocator=None):\n\t\tif data.ndim == 3:\n\t\t\tbatchsize, maps, insize = data.shape\n\t\t\tlpad, rpad = pad\n\n\t\t\tassert insize >= max(lpad, rpad) + 1\n\t\t\toutsize = insize + lpad + rpad\n\n\t\t\tblock = (self.warpSize, 1, 1)\n\t\t\tgrid = (roundUpDiv(outsize, self.warpSize), maps, batchsize)\n\n\t\t\toutdata = self.GPUArray.empty((batchsize, maps, outsize), dtype=data.dtype, allocator=allocator)\n\t\t\tfn = self.mod.reflectpad1d if data.dtype == np.float32 else self.mod.reflectpad1dFP16\n\n\t\t\tfn(outdata, data, np.int32(insize), np.int32(lpad), np.int32(rpad), block=block, grid=grid)\n\n\t\telif data.ndim == 4:\n\t\t\tbatchsize, maps, inh, inw = data.shape\n\t\t\tupad, bpad, lpad, rpad = pad\n\n\t\t\tassert inh >= max(upad, bpad) + 1 and inw >= max(lpad, rpad) + 1\n\t\t\touth, outw = inh + upad + bpad, inw + lpad + rpad\n\n\t\t\tblock = (self.warpSize, 1, 1)\n\t\t\tgrid = (roundUpDiv(outh * outw, self.warpSize), maps, batchsize)\n\n\t\t\toutdata = self.GPUArray.empty((batchsize, maps, outh, outw), dtype=data.dtype, allocator=allocator)\n\t\t\tfn = self.mod.reflectpad2d if data.dtype == np.float32 else self.mod.reflectpad2dFP16\n\n\t\t\tfn(\n\t\t\t\toutdata, data, np.int32(inh), np.int32(inw),\n\t\t\t\tnp.int32(upad), np.int32(bpad), np.int32(lpad), np.int32(rpad), block=block, grid=grid\n\t\t\t)\n\n\t\telse:\n\t\t\traise NotImplementedError(data.ndim)\n\n\t\treturn outdata\n\n\n\tdef reflectpadBackward(self, grad, pad, allocator=None):\n\t\tif grad.ndim == 3:\n\t\t\tbatchsize, maps, outsize = grad.shape\n\t\t\tlpad, rpad = pad\n\n\t\t\tblock = (self.warpSize, 1, 1)\n\t\t\tgrid = (roundUpDiv(outsize, self.warpSize), maps, batchsize)\n\n\t\t\tinsize = outsize - lpad - rpad\n\t\t\tingrad = self.GPUArray.zeros((batchsize, maps, insize), dtype=grad.dtype, allocator=allocator)\n\t\t\tfn = self.mod.reflectpad1dBackward if grad.dtype == np.float32 else self.mod.reflectpad1dBackwardFP16\n\n\t\t\tfn(ingrad, grad, np.int32(insize), np.int32(lpad), np.int32(rpad), block=block, grid=grid)\n\n\t\telif grad.ndim == 4:\n\t\t\tbatchsize, maps, outh, outw = grad.shape\n\t\t\tupad, bpad, lpad, rpad = pad\n\n\t\t\tinh, inw = outh - upad - bpad, outw - lpad - rpad\n\n\t\t\tblock = (self.warpSize, 1, 1)\n\t\t\tgrid = (roundUpDiv(outh * outw, self.warpSize), maps, batchsize)\n\n\t\t\tingrad = self.GPUArray.zeros((batchsize, maps, inh, inw), dtype=grad.dtype, allocator=allocator)\n\t\t\tfn = self.mod.reflectpad2dBackward if grad.dtype == np.float32 else self.mod.reflectpad2dBackwardFP16\n\n\t\t\tfn(\n\t\t\t\tingrad, grad, np.int32(inh), np.int32(inw),\n\t\t\t\tnp.int32(upad), np.int32(bpad), np.int32(lpad), np.int32(rpad), block=block, grid=grid\n\t\t\t)\n\n\t\telse:\n\t\t\traise NotImplementedError(grad.ndim)\n\n\t\treturn ingrad\n\n\ndef unittest():\n\tfrom PuzzleLib.Cuda import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tmodule = PadModule(Backend.getBackend(deviceIdx))\n\n\t\tfor dtype, atol in module.backend.dtypesSupported():\n\t\t\treflectpad1dTest(module, dtype)\n\t\t\treflectpad2dTest(module, dtype, atol)\n\n\ndef reflectpad1dTest(module, dtype):\n\tbatchsize, maps, insize = 4, 8, 48\n\tlpad, rpad = 2, 3\n\n\thostData = np.random.randn(batchsize, maps, insize).astype(dtype)\n\n\tdata = module.GPUArray.toGpu(hostData)\n\toutdata = module.reflectpad(data, pad=(lpad, rpad))\n\n\thostOutData = outdata.get()\n\toutsize = hostOutData.shape[2]\n\n\tassert np.allclose(hostOutData[:, :, lpad:insize + lpad], hostData)\n\tassert np.allclose(hostOutData[:, :, :lpad][:, :, ::-1], hostData[:, :, 1:lpad+1])\n\tassert np.allclose(hostOutData[:, :, insize + lpad:][:, :, ::-1], hostData[:, :, insize - 1 - rpad:insize - 1])\n\n\thostGrad = np.random.randn(batchsize, maps, outsize).astype(np.float32)\n\n\tgrad = module.GPUArray.toGpu(hostGrad)\n\tingrad = module.reflectpadBackward(grad, pad=(lpad, rpad))\n\n\thostInGrad = ingrad.get()\n\n\tassert np.allclose(\n\t\thostInGrad[:, :, lpad + 1:insize - rpad - 1], hostGrad[:, :, 2 * lpad + 1:outsize - 2 * rpad - 1]\n\t)\n\tassert np.allclose(\n\t\thostInGrad[:, :, 1:lpad + 1], hostGrad[:, :, :lpad][:, :, ::-1] + hostGrad[:, :, lpad + 1:2 * lpad + 1]\n\t)\n\tassert np.allclose(\n\t\thostInGrad[:, :, insize - rpad - 1:insize - 1],\n\t\thostGrad[:, :, outsize - rpad:][:, :, ::-1] + hostGrad[:, :, outsize - 2 * rpad - 1:outsize - rpad - 1]\n\t)\n\n\ndef reflectpad2dTest(module, dtype, atol):\n\tbatchsize, maps, inh, inw = 4, 8, 12, 15\n\tupad, bpad, lpad, rpad = 2, 3, 2, 3\n\n\thostData = np.random.randn(batchsize, maps, inh, inw).astype(dtype)\n\n\tdata = module.GPUArray.toGpu(hostData)\n\toutdata = module.reflectpad(data, pad=(upad, bpad, lpad, rpad))\n\n\thostOutData = outdata.get()\n\touth, outw = hostOutData.shape[2:]\n\n\tassert np.allclose(hostOutData[:, :, upad:inh + upad, lpad:inw + lpad], hostData)\n\tassert np.allclose(hostOutData[:, :, :upad, :lpad][:, :, ::-1, ::-1], hostData[:, :, 1:upad + 1, 1:lpad + 1])\n\tassert np.allclose(\n\t\thostOutData[:, :, inh + upad:, inw + lpad:][:, :, ::-1, ::-1],\n\t\thostData[:, :, inh - 1 - bpad:inh - 1, inw - 1 - rpad:inw - 1]\n\t)\n\n\thostGrad = np.random.randn(batchsize, maps, outh, outw).astype(dtype)\n\n\tgrad = module.GPUArray.toGpu(hostGrad)\n\tingrad = module.reflectpadBackward(grad, pad=(upad, bpad, lpad, rpad))\n\n\thostInGrad = ingrad.get()\n\n\tassert np.allclose(\n\t\thostInGrad[:, :, upad + 1:inh - bpad - 1, lpad + 1:inw - rpad - 1],\n\t\thostGrad[:, :, 2 * upad + 1:outh - 2 * bpad - 1, 2 * lpad + 1:outw - 2 * rpad - 1]\n\t)\n\tassert np.allclose(\n\t\thostInGrad[:, :, 1:upad + 1, 1:lpad + 1],\n\t\thostGrad[:, :, :upad, :lpad][:, :, ::-1, ::-1] +\n\t\thostGrad[:, :, upad + 1:2 * upad + 1, lpad + 1:2 * lpad + 1] +\n\t\thostGrad[:, :, :upad, lpad + 1:2 * lpad + 1][:, :, ::-1, :] +\n\t\thostGrad[:, :, upad + 1:2 * upad + 1, :lpad][:, :, :, ::-1], atol=atol\n\t)\n\tassert np.allclose(\n\t\thostInGrad[:, :, inh - bpad - 1:inh - 1, inw - rpad - 1:inw - 1],\n\t\thostGrad[:, :, outh - bpad:, outw - rpad:][:, :, ::-1, ::-1] +\n\t\thostGrad[:, :, outh - 2 * bpad - 1:outh - bpad - 1, outw - 2 * rpad - 1:outw - rpad - 1] +\n\t\thostGrad[:, :, outh - bpad:, outw - 2 * rpad - 1:outw - rpad - 1][:, :, ::-1, :] +\n\t\thostGrad[:, :, outh - 2 * bpad - 1:outh - bpad - 1, outw - rpad:][:, :, :, ::-1], atol=atol\n\t)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cuda/Kernels/Pool.py,35,"b'from string import Template\nimport numpy as np\n\nfrom PuzzleLib.Cuda.Utils import prod, roundUpDiv\n\n\npoolTmpl = Template(""""""\n\nextern ""C""\n__global__ void maxpool2d(float *outdata, const float *indata, int *mask, int inh, int inw, int outh, int outw,\n\t\t\t\t\t\t  int maps, int hstride, int wstride, int hpad, int wpad, int fh, int fw, int size)\n{\n\tfor (int index = blockIdx.x * blockDim.x + threadIdx.x; index < size; index += blockDim.x * gridDim.x)\n\t{\n\t\tint pw = index % outw;\n\t\tint ph = (index / outw) % outh;\n\t\tint c = (index / outw / outh) % maps;\n\t\tint n = index / outw / outh / maps;\n\n\t\tint hstart = ph * hstride - hpad;\n\t\tint wstart = pw * wstride - wpad;\n\n\t\tint hend = min(hstart + fh, inh);\n\t\tint wend = min(wstart + fw, inw);\n\n\t\thstart = max(hstart, 0);\n\t\twstart = max(wstart, 0);\n\n\t\tfloat maxval = $initVal;\n\t\tint maxidx = -1;\n\n\t\tconst float *slice = indata + (n * maps + c) * inh * inw;\n\n\t\tfor (int h = hstart; h < hend; ++h)\n\t\t\tfor (int w = wstart; w < wend; ++w)\n\t\t\t{\n\t\t\t\tif (slice[h * inw + w] > maxval)\n\t\t\t\t{\n\t\t\t\t\tmaxidx = h * inw + w;\n\t\t\t\t\tmaxval = slice[maxidx];\n\t\t\t\t}\n\t\t\t}\n\n\t\toutdata[index] = maxval;\n\t\tmask[index] = maxidx;\n\t}\n}\n\nextern ""C""\n__global__ void maxunpool2d(float *outdata, const float *indata, const int *mask, int inh, int inw, int outh, int outw,\n\t\t\t\t\t\t\tint maps, int size)\n{\n\tfor (int index = blockIdx.x * blockDim.x + threadIdx.x; index < size; index += blockDim.x * gridDim.x)\n\t{\n\t\tint c = (index / inw / inh) % maps;\n\t\tint n = index / inw / inh / maps;\n\n\t\tfloat *slice = outdata + (n * maps + c) * outh * outw;\n\t\tint maxind = mask[index];\n\n\t\tslice[maxind] = indata[index];\n\t}\n}\n\nextern ""C""\n__global__ void maxpool2dBackward(float *ingrad, const float *outgrad, const int *mask, int inh, int inw,\n\t\t\t\t\t\t\t\t  int outh, int outw, int maps, int hstride, int wstride, int hpad, int wpad,\n\t\t\t\t\t\t\t\t  int fh, int fw, int size)\n{\n\tfor (int index = blockIdx.x * blockDim.x + threadIdx.x; index < size; index += blockDim.x * gridDim.x)\n\t{\n\t\tint w = index % inw;\n\t\tint h = (index / inw) % inh;\n\t\tint c = (index / inw / inh) % maps;\n\t\tint n = index / inw / inh / maps;\n\n\t\tint phstart = (h + hpad < fh) ? 0 : (h + hpad - fh) / hstride + 1;\n\t\tint phend = min((h + hpad) / hstride + 1, outh);\n\n\t\tint pwstart = (w + wpad < fw) ? 0 : (w + wpad - fw) / wstride + 1;\n\t\tint pwend = min((w + wpad) / wstride + 1, outw);\n\n\t\tfloat grad = 0.0f;\n\t\tint offset = (n * maps + c) * outh * outw;\n\n\t\tconst float *slice = outgrad + offset;\n\t\tconst int *maskSlice = mask + offset;\n\n\t\tfor (int ph = phstart; ph < phend; ++ph)\n\t\t\tfor (int pw = pwstart; pw < pwend; ++pw)\n\t\t\t\tif (maskSlice[ph * outw + pw] == h * inw + w)\n\t\t\t\t\tgrad += slice[ph * outw + pw];\n\n\t\tingrad[index] = grad;\n\t}\n}\n\nextern ""C""\n__global__ void maxunpool2dBackward(float *ingrad, const float *outgrad, const int *mask, int inh, int inw,\n\t\t\t\t\t\t\t\t\tint outh, int outw, int maps, int size)\n{\n\tfor (int index = blockIdx.x * blockDim.x + threadIdx.x; index < size; index += blockDim.x * gridDim.x)\n\t{\n\t\tint c = (index / inw / inh) % maps;\n\t\tint n = index / inw / inh / maps;\n\n\t\tconst float *slice = outgrad + (n * maps + c) * outh * outw;\n\t\tint maxind = mask[index];\n\n\t\tingrad[index] = slice[maxind];\n\t}\n}\n\n"""""")\n\n\nclass PoolModule:\n\tdef __init__(self, backend):\n\t\tself.GPUArray, self.nthreads = backend.GPUArray, backend.nthreads\n\t\tself.mod = backend.SourceModule(poolTmpl.substitute(initVal=str(np.finfo(np.float32).min)))\n\n\n\tdef maxpool2d(self, data, size, stride, pad, allocator=None):\n\t\tassert data.dtype == np.float32\n\t\tbatchsize, maps, inh, inw = data.shape\n\n\t\tfh, fw = size\n\t\thstride, wstride = stride\n\t\thpad, wpad = pad\n\n\t\touth = (inh - fh + 2 * hpad) // hstride + 1\n\t\toutw = (inw - fw + 2 * wpad) // wstride + 1\n\n\t\toutdata = self.GPUArray.empty((batchsize, maps, outh, outw), dtype=np.float32, allocator=allocator)\n\t\tmask = self.GPUArray.empty((batchsize, maps, outh, outw), dtype=np.int32, allocator=allocator)\n\n\t\tsize = prod(outdata.shape)\n\n\t\tblock = (self.nthreads, 1, 1)\n\t\tgrid = (roundUpDiv(size, self.nthreads), 1, 1)\n\n\t\tself.mod.maxpool2d(\n\t\t\toutdata, data, mask, np.int32(inh), np.int32(inw), np.int32(outh), np.int32(outw), np.int32(maps),\n\t\t\tnp.int32(hstride), np.int32(wstride), np.int32(hpad), np.int32(wpad), np.int32(fh), np.int32(fw),\n\t\t\tnp.int32(size), block=block, grid=grid\n\t\t)\n\n\t\treturn outdata, mask\n\n\n\tdef maxpool2dBackward(self, grad, origshape, mask, size, stride, pad, allocator=None):\n\t\tassert grad.dtype == np.float32 and mask.dtype == np.int32\n\t\tbatchsize, maps, outh, outw = grad.shape\n\n\t\tfh, fw = size\n\t\thstride, wstride = stride\n\t\thpad, wpad = pad\n\n\t\tinh, inw = origshape[2], origshape[3]\n\t\tingrad = self.GPUArray.empty((batchsize, maps, inh, inw), dtype=np.float32, allocator=allocator)\n\n\t\tsize = prod(ingrad.shape)\n\n\t\tblock = (self.nthreads, 1, 1)\n\t\tgrid = (roundUpDiv(size, self.nthreads), 1, 1)\n\n\t\tself.mod.maxpool2dBackward(\n\t\t\tingrad, grad, mask, np.int32(inh), np.int32(inw), np.int32(outh), np.int32(outw), np.int32(maps),\n\t\t\tnp.int32(hstride), np.int32(wstride), np.int32(hpad), np.int32(wpad), np.int32(fh), np.int32(fw),\n\t\t\tnp.int32(size), block=block, grid=grid\n\t\t)\n\n\t\treturn ingrad\n\n\n\tdef maxunpool2d(self, data, origshape, mask, allocator=None):\n\t\tassert data.dtype == np.float32\n\t\tbatchsize, maps, inh, inw = data.shape\n\n\t\touth, outw = origshape[2], origshape[3]\n\t\toutdata = self.GPUArray.zeros((batchsize, maps, outh, outw), dtype=np.float32, allocator=allocator)\n\n\t\tsize = prod(data.shape)\n\n\t\tblock = (self.nthreads, 1, 1)\n\t\tgrid = (roundUpDiv(size, self.nthreads), 1, 1)\n\n\t\tself.mod.maxunpool2d(\n\t\t\toutdata, data, mask, np.int32(inh), np.int32(inw), np.int32(outh), np.int32(outw), np.int32(maps),\n\t\t\tnp.int32(size), block=block, grid=grid\n\t\t)\n\n\t\treturn outdata\n\n\n\tdef maxunpool2dBackward(self, grad, poolshape, mask, allocator=None):\n\t\tassert grad.dtype == np.float32 and mask.dtype == np.int32\n\t\tbatchsize, maps, outh, outw = grad.shape\n\n\t\tinh, inw = poolshape[2], poolshape[3]\n\t\tingrad = self.GPUArray.empty((batchsize, maps, inh, inw), dtype=np.float32, allocator=allocator)\n\n\t\tsize = prod(ingrad.shape)\n\n\t\tblock = (self.nthreads, 1, 1)\n\t\tgrid = (roundUpDiv(size, self.nthreads), 1, 1)\n\n\t\tself.mod.maxunpool2dBackward(\n\t\t\tingrad, grad, mask, np.int32(inh), np.int32(inw), np.int32(outh), np.int32(outw), np.int32(maps),\n\t\t\tnp.int32(size), block=block, grid=grid\n\t\t)\n\n\t\treturn ingrad\n\n\ndef unittest():\n\tfrom PuzzleLib.Cuda import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tmodule = PoolModule(Backend.getBackend(deviceIdx))\n\n\t\tpoolTest(module)\n\t\tunpoolTest(module)\n\n\ndef poolTest(module):\n\tbatchsize, maps, h, w = 10, 4, 6, 6\n\tsize, stride, pad = 2, 2, 1\n\n\tindata = module.GPUArray.toGpu(np.random.randn(batchsize, maps, h, w).astype(np.float32))\n\tpooldata, mask = module.maxpool2d(indata, [size, size], [stride, stride], [pad, pad])\n\n\thostInData = np.zeros((batchsize, maps, h + 2 * pad, w + 2 * pad), dtype=np.float32)\n\thostInData[:, :, pad:-pad, pad:-pad] = indata.get()\n\n\thostPoolData = np.empty(pooldata.shape, dtype=np.float32)\n\thostMask = np.empty(mask.shape, dtype=np.int32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor py in range(hostPoolData.shape[2]):\n\t\t\t\tfor px in range(hostPoolData.shape[3]):\n\t\t\t\t\tmaxval = -np.finfo(np.float32).max\n\t\t\t\t\tmaxidx = -1\n\n\t\t\t\t\tiny = max(py * stride - pad, 0)\n\t\t\t\t\tinx = max(px * stride - pad, 0)\n\n\t\t\t\t\tfor y in range(iny, min(h, py * stride - pad + size)):\n\t\t\t\t\t\tfor x in range(inx, min(w, px * stride - pad + size)):\n\t\t\t\t\t\t\tval = hostInData[b, c, y + pad, x + pad]\n\t\t\t\t\t\t\tif val > maxval:\n\t\t\t\t\t\t\t\tmaxval = val\n\t\t\t\t\t\t\t\tmaxidx = y * h + x\n\n\t\t\t\t\thostPoolData[b, c, py, px] = maxval\n\t\t\t\t\thostMask[b, c, py, px] = maxidx\n\n\tassert np.allclose(hostPoolData, pooldata.get())\n\tassert (hostMask == mask.get()).all()\n\n\thostGrad = np.random.randn(*pooldata.shape).astype(np.float32)\n\n\tgrad = module.GPUArray.toGpu(hostGrad)\n\tingrad = module.maxpool2dBackward(grad, indata.shape, mask, [size, size], [stride, stride], [pad, pad])\n\n\thostInGrad = np.empty(ingrad.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor y in range(h):\n\t\t\t\tfor x in range(w):\n\t\t\t\t\touty = 0 if y + pad < size else (y + pad - size) // stride + 1\n\t\t\t\t\toutx = 0 if x + pad < size else (x + pad - size) // stride + 1\n\n\t\t\t\t\tgr = 0.0\n\n\t\t\t\t\tfor py in range(outy, min((y + pad) // stride + 1, hostPoolData.shape[2])):\n\t\t\t\t\t\tfor px in range(outx, min((x + pad) // stride + 1, hostPoolData.shape[3])):\n\t\t\t\t\t\t\tif hostMask[b, c, py, px] == y * w + x:\n\t\t\t\t\t\t\t\tgr += hostGrad[b, c, py, px]\n\n\t\t\t\t\thostInGrad[b, c, y, x] = gr\n\n\tassert np.allclose(hostInGrad, ingrad.get())\n\n\ndef unpoolTest(module):\n\tbatchsize, maps, h, w = 10, 4, 6, 6\n\tsize, stride, pad = 2, 2, 1\n\n\tindata = module.GPUArray.toGpu(np.random.randn(batchsize, maps, h, w).astype(np.float32))\n\n\tpooldata, mask = module.maxpool2d(indata, [size, size], [stride, stride], [pad, pad])\n\tunpooldata = module.maxunpool2d(pooldata, indata.shape, mask)\n\n\thostPoolData = pooldata.get()\n\thostMask = mask.get()\n\n\thostUnpoolData = np.zeros(unpooldata.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor y in range(pooldata.shape[2]):\n\t\t\t\tfor x in range(pooldata.shape[3]):\n\t\t\t\t\tmaxidx = hostMask[b, c, y, x]\n\t\t\t\t\thostUnpoolData[b, c].ravel()[maxidx] = hostPoolData[b, c, y, x]\n\n\tassert np.allclose(hostUnpoolData, unpooldata.get())\n\n\thostGrad = np.random.randn(*unpooldata.shape).astype(np.float32)\n\n\tgrad = module.GPUArray.toGpu(hostGrad)\n\tingrad = module.maxunpool2dBackward(grad, pooldata.shape, mask)\n\n\thostInGrad = np.empty(ingrad.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor y in range(pooldata.shape[2]):\n\t\t\t\tfor x in range(pooldata.shape[3]):\n\t\t\t\t\tmaxidx = hostMask[b, c, y, x]\n\t\t\t\t\thostInGrad[b, c, y, x] = hostGrad[b, c].ravel()[maxidx]\n\n\tassert np.allclose(hostInGrad, ingrad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cuda/Kernels/RadixSort.py,20,"b'from string import Template\nimport numpy as np\n\n\nscanSumTmpl = Template(""""""\n\ntypedef struct ScanResult\n{\n\tunsigned scan, reduction;\n}\nScanResult;\n\n\ntypedef struct ScanStorage\n{\n\tunsigned sums[$warpSize];\n}\nScanStorage;\n\n\n__forceinline__ __device__ unsigned warpScanSum(unsigned value)\n{\n\tint laneId = threadIdx.x % $warpSize;\n\n\tfor (int i = 1; i <= $warpSize; i *= 2)\n\t{\n\t\tunsigned downval = __shfl_up_sync((unsigned)-1, value, i, $warpSize);\n\t\tif (laneId >= i) value += downval;\n\t}\n\n\treturn value;\n}\n\n\n__forceinline__ __device__ ScanResult blockScanSum(unsigned value, ScanStorage *storage)\n{\n\tunsigned initval = value;\n\tint warpId = threadIdx.x / $warpSize, laneId = threadIdx.x % $warpSize;\n\n\tvalue = warpScanSum(value);\n\n\tif (laneId == $warpSize - 1)\n\t\tstorage->sums[warpId] = value;\n\n\t__syncthreads();\n\n\tif (warpId == 0)\n\t\tstorage->sums[laneId] = warpScanSum((laneId < $NT / $warpSize) ? storage->sums[laneId] : 0);\n\n\t__syncthreads();\n\n\tunsigned blockSum = 0;\n\tif (warpId > 0)\n\t\tblockSum = storage->sums[warpId - 1];\n\n\tunsigned reduction = storage->sums[$NT / $warpSize - 1];\n\t__syncthreads();\n\n\tScanResult result = {blockSum + value - initval, reduction};\n\treturn result;\n}\n\n"""""")\n\n\nradixSortTmpl = Template(""""""\n\n$scanSum\n\n\ntypedef unsigned short DigitCounter;\ntypedef unsigned PackCounter;\n\n\nenum\n{\n\tRadixSort_Bits = 6,\n\tRadixSort_Lanes = 1 << (RadixSort_Bits - 1),\n\tRadixSort_PackRatio = sizeof(PackCounter) / sizeof(DigitCounter)\n};\n\n\ntypedef struct SortStorage\n{\n\tunion\n\t{\n\t\tDigitCounter bins[(RadixSort_Lanes + 1) * $NT * RadixSort_PackRatio];\n\t\tPackCounter grid[$NT * (RadixSort_Lanes + 1)];\n\t}\n\thist;\n\n\tScanStorage scanStorage;\n}\nSortStorage;\n\n\n__forceinline__ __device__\nvoid blockRadixSort(int keys[$VT], int values[$VT], int outkeys[$NV], int outvalues[$NV], SortStorage *storage)\n{\n\tint currBit = 0;\n\n\twhile (true)\n\t{\n\t\tfor (int i = threadIdx.x; i < $NT * (RadixSort_Lanes + 1); i += $NT)\n\t\t\tstorage->hist.grid[i] = 0;\n\n\t\t__syncthreads();\n\t\tDigitCounter binOffsets[$VT], *binPtrs[$VT];\n\n\t\tfor (int i = 0; i < $VT; i++)\n\t\t{\n\t\t\tint radix = (keys[i] >> currBit) & ((1 << RadixSort_Bits) - 1);\n\t\t\tint subcounter = radix >> (RadixSort_Bits - 1), digit = radix & (RadixSort_Lanes - 1);\n\n\t\t\tbinPtrs[i] = storage->hist.bins + (digit * $NT + threadIdx.x) * RadixSort_PackRatio + subcounter;\n\t\t\tbinOffsets[i] = *binPtrs[i];\n\n\t\t\t*binPtrs[i] += 1;\n\t\t}\n\t\t__syncthreads();\n\n\t\tPackCounter laneCache[RadixSort_Lanes + 1];\n\t\tPackCounter upsweep = 0;\n\n\t\tfor (int i = 0; i < RadixSort_Lanes + 1; i++)\n\t\t{\n\t\t\tlaneCache[i] = storage->hist.grid[threadIdx.x * (RadixSort_Lanes + 1) + i];\n\t\t\tupsweep += laneCache[i];\n\t\t}\n\n\t\tScanResult result = blockScanSum(upsweep, &storage->scanStorage);\n\t\tPackCounter downsweep = result.scan + (result.reduction << (sizeof(DigitCounter) * 8));\n\n\t\tfor (int i = 0; i < RadixSort_Lanes + 1; i++)\n\t\t{\n\t\t\tstorage->hist.grid[threadIdx.x * (RadixSort_Lanes + 1) + i] = downsweep;\n\t\t\tdownsweep += laneCache[i];\n\t\t}\n\t\t__syncthreads();\n\n\t\tfor (int i = 0; i < $VT; i++)\n\t\t{\n\t\t\tint rank = *binPtrs[i] + binOffsets[i];\n\t\t\toutkeys[rank] = keys[i], outvalues[rank] = values[i];\n\t\t}\n\t\t__syncthreads();\n\n\t\tcurrBit += RadixSort_Bits;\n\t\tif (currBit >= sizeof(keys[0]) * 8)\n\t\t\tbreak;\n\n\t\tfor (int i = 0; i < $VT; i++)\n\t\t{\n\t\t\tint index = threadIdx.x * $VT + i;\n\t\t\tkeys[i] = outkeys[index], values[i] = outvalues[index];\n\t\t}\n\t}\n}\n\n"""""")\n\n\nsegmentSeqTmpl = Template(""""""\n\n$radixSort\n\n\ntypedef struct SegmentResult\n{\n\tint start[$VT], end[$VT], label[$VT];\n\tint length;\n}\nSegmentResult;\n\n\ntypedef struct SegmentStorage\n{\n\tunion\n\t{\n\t\tSortStorage sortStorage;\n\t\tScanStorage scanStorage;\n\t\tint labels[$NV];\n\t};\n\n\tint keys[$NV], offsets[$NV];\n}\nSegmentStorage;\n\n\n__forceinline__ __device__\nSegmentResult blockSegmentSeq(int keys[$VT], int length, int indices[$NV], SegmentStorage *storage)\n{\n\tint values[$VT];\n\n\tfor (int i = 0; i < $VT; i++)\n\t\tvalues[i] = threadIdx.x + i * $NT;\n\n\tblockRadixSort(keys, values, storage->keys, indices, &storage->sortStorage);\n\n\tunsigned splitFlags = 0;\n\tint key = storage->keys[threadIdx.x * $VT];\n\n\tfor (int i = 0; i < $VT; i++)\n\t{\n\t\tint index = threadIdx.x * $VT + i + 1;\n\t\tif (index < length)\n\t\t{\n\t\t\tint next = storage->keys[index];\n\n\t\t\tif (key != next)\n\t\t\t\tsplitFlags |= 1 << i;\n\n\t\t\tkey = next;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tif (index == length)\n\t\t\t\tsplitFlags |= 1 << i;\n\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tScanResult scanResult = blockScanSum(__popc(splitFlags), &storage->scanStorage);\n\tunsigned offset = scanResult.scan, uniqueKeys = scanResult.reduction;\n\n\tfor (int i = 0; i < $VT; i++)\n\t{\n\t\tif (splitFlags & (1 << i))\n\t\t{\n\t\t\tint index = threadIdx.x * $VT + i;\n\n\t\t\tstorage->offsets[offset] = index + 1;\n\t\t\tstorage->labels[offset] = storage->keys[index];\n\n\t\t\toffset += 1;\n\t\t}\n\t}\n\t__syncthreads();\n\n\tSegmentResult result;\n\tresult.length = 0;\n\n\tfor (int i = 0; i < $VT; i++)\n\t{\n\t\tint index = threadIdx.x + i * $NT;\n\t\tif (index < uniqueKeys)\n\t\t{\n\t\t\tresult.start[i] = (index > 0) ? storage->offsets[index - 1] : 0;\n\t\t\tresult.end[i] = storage->offsets[index];\n\n\t\t\tresult.label[i] = storage->labels[index];\n\t\t\tresult.length += 1;\n\t\t}\n\t}\n\t __syncthreads();\n\n\treturn result;\n}\n\n"""""")\n\n\nscanSumTestTmpl = Template(""""""\n\n$scanSum\n\n\nextern ""C""\n__global__ void scanSum(unsigned *outdata, const unsigned *indata, int length)\n{\n\t__shared__ ScanStorage scanStorage;\n\n\tunsigned key = (threadIdx.x < length) ? indata[threadIdx.x] : (unsigned)-1;\n\tScanResult result = blockScanSum(key, &scanStorage);\n\n\tif (threadIdx.x < length)\n\t\toutdata[threadIdx.x] = result.scan;\n}\n\n"""""")\n\n\nradixSortTestTmpl = Template(""""""\n\n$radixSort\n\n\nextern ""C""\n__global__ void radixSort(int *outkeys, int *outvalues, const int *inkeys, const int *invalues, int length)\n{\n\t__shared__ int shkeys[$NV], shvalues[$NV];\n\t__shared__ SortStorage storage;\n\n\tint keys[$VT], values[$VT];\n\n\tfor (int i = 0; i < $VT; i++)\n\t{\n\t\tint j = threadIdx.x + i * $NT;\n\n\t\tkeys[i] = (j < length) ? inkeys[j] : 0x7FFFFFFF;\n\t\tvalues[i] = (j < length) ? invalues[j] : 0;\n\t}\n \n\tblockRadixSort(keys, values, shkeys, shvalues, &storage);\n\n\tfor (int i = threadIdx.x; i < length; i += $NT)\n\t\toutkeys[i] = shkeys[i], outvalues[i] = shvalues[i];\n}\n\n"""""")\n\n\nsegmentTestTmpl = Template(""""""\n\n$segmentSeq\n\n\nextern ""C""\n__global__ void segmentSeq(int *segments, int *indices, const int *data, int length)\n{\n\t__shared__ int shIndices[$NV];\n\t__shared__ SegmentStorage storage;\n\n\tint keys[$VT];\n\n\tfor (int i = 0; i < $VT; i++)\n\t{\n\t\tint j = threadIdx.x + i * $NT;\n\t\tkeys[i] = (j < length) ? data[j] : 0x7FFFFFFF;\n\t}\n\n\tSegmentResult result = blockSegmentSeq(keys, length, shIndices, &storage);\n\tint3 *segments3 = (int3 *)segments;\n\n\tfor (int i = 0; i < $VT; i++)\n\t{\n\t\tint j = threadIdx.x + i * $NT;\n\n\t\tif (j < length)\n\t\t{\n\t\t\tsegments3[j] = (i < result.length) ?\n\t\t\t\tmake_int3(result.start[i], result.end[i], result.label[i]) : make_int3(-1, -1, -1);\n\n\t\t\tindices[j] = shIndices[j];\n\t\t}\n\t}\n}\n\n"""""")\n\n\nclass RadixSortModule:\n\tdef __init__(self, backend):\n\t\tself.GPUArray = backend.GPUArray\n\n\t\tself.NT, self.VT = 128, 2\n\t\tself.NV = self.NT * self.VT\n\n\t\tscanSum = scanSumTmpl.substitute(warpSize=backend.warpSize, NT=self.NT)\n\t\tself.scanMod = backend.SourceModule(scanSumTestTmpl.substitute(scanSum=scanSum))\n\n\t\tradixSort = radixSortTmpl.substitute(\n\t\t\tscanSum=scanSum, warpSize=backend.warpSize, NT=self.NT, VT=self.VT, NV=self.NV\n\t\t)\n\t\tself.radixMod = backend.SourceModule(radixSortTestTmpl.substitute(\n\t\t\tradixSort=radixSort, NT=self.NT, VT=self.VT, NV=self.NV\n\t\t))\n\n\t\tsegmentSeq = segmentSeqTmpl.substitute(radixSort=radixSort, NT=self.NT, VT=self.VT, NV=self.NV)\n\t\tself.segmentMod = backend.SourceModule(segmentTestTmpl.substitute(\n\t\t\tsegmentSeq=segmentSeq, NT=self.NT, VT=self.VT, NV=self.NV\n\t\t))\n\n\n\tdef scanSum(self, data):\n\t\tassert data.dtype == np.uint32\n\n\t\tlength, = data.shape\n\t\tassert length <= self.NT\n\n\t\toutdata = self.GPUArray.empty(data.shape, dtype=data.dtype)\n\n\t\tself.scanMod.scanSum(outdata, data, np.int32(length), block=(self.NT, 1, 1), grid=(1, 1, 1))\n\t\treturn outdata\n\n\n\tdef radixSort(self, keys, values):\n\t\tassert keys.dtype == np.int32 and values.dtype == np.int32\n\t\tassert keys.shape == values.shape\n\n\t\tlength, = keys.shape\n\t\tassert length <= self.NV\n\n\t\toutkeys = self.GPUArray.empty(keys.shape, dtype=keys.dtype)\n\t\toutvalues = self.GPUArray.empty(values.shape, dtype=values.dtype)\n\n\t\tself.radixMod.radixSort(\n\t\t\toutkeys, outvalues, keys, values, np.int32(length), block=(self.NT, 1, 1), grid=(1, 1, 1)\n\t\t)\n\t\treturn outkeys, outvalues\n\n\n\tdef segmentSeq(self, data):\n\t\tassert data.dtype == np.int32\n\n\t\tlength, = data.shape\n\t\tassert length <= self.NV\n\n\t\tsegments = self.GPUArray.empty((length, 3), dtype=np.int32)\n\t\tindices = self.GPUArray.empty(data.shape, dtype=np.int32)\n\n\t\tself.segmentMod.segmentSeq(segments, indices, data, np.int32(length), block=(self.NT, 1, 1), grid=(1, 1, 1))\n\t\treturn segments, indices\n\n\ndef unittest():\n\tfrom PuzzleLib.Cuda import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tmodule = RadixSortModule(Backend.getBackend(deviceIdx))\n\n\t\tscanSumTest(module)\n\t\tradixSortTest(module)\n\t\tsegmentTest(module)\n\n\ndef scanSumTest(module):\n\thostData = np.random.randint(0, 1000, size=(120, ), dtype=np.uint32)\n\toutdata = module.scanSum(module.GPUArray.toGpu(hostData))\n\n\thostOutData = np.empty_like(hostData)\n\n\thostOutData[0] = 0\n\thostOutData[1:] = np.cumsum(hostData)[:-1]\n\n\tassert np.allclose(outdata.get(), hostOutData)\n\n\ndef radixSortTest(module):\n\thostKeys = np.random.randint(0, (1 << 31) - 1, size=(250, ), dtype=np.int32)\n\thostValues = np.arange(0, hostKeys.shape[0], dtype=np.int32)\n\n\toutkeys, outvalues = module.radixSort(module.GPUArray.toGpu(hostKeys), module.GPUArray.toGpu(hostValues))\n\n\tassert (outkeys.get() == np.sort(hostKeys)).all()\n\tassert (outvalues.get() == np.argsort(hostKeys)).all()\n\n\ndef segmentTest(module):\n\thostData = np.random.randint(10, 30, size=(250, ), dtype=np.int32)\n\tsegments, indices = module.segmentSeq(module.GPUArray.toGpu(hostData))\n\n\thostSortedData = np.sort(hostData)\n\thostSegments = np.empty(shape=segments.shape, dtype=segments.dtype)\n\n\tsegIndex = 0\n\thostSegments[segIndex, 0] = 0\n\n\tfor i in range(hostData.shape[0] - 1):\n\t\tif hostSortedData[i] != hostSortedData[i + 1]:\n\t\t\thostSegments[segIndex, 1] = hostSegments[segIndex + 1, 0] = i + 1\n\t\t\thostSegments[segIndex, 2] = hostSortedData[i]\n\n\t\t\tsegIndex += 1\n\n\thostSegments[segIndex, 1] = hostData.shape[0]\n\thostSegments[segIndex, 2] = hostSortedData[-1]\n\n\thostSegments[segIndex + 1:] = -1\n\n\tassert (segments.get() == hostSegments).all()\n\tassert (hostData[indices.get()] == np.sort(hostData)).all()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cuda/Kernels/Upsample.py,40,"b'import itertools\nfrom string import Template\n\nimport numpy as np\nfrom PuzzleLib.Cuda.Utils import roundUpDiv\n\n\nupsampleNearestTmpl = Template(""""""\n\nextern ""C""\n__global__ void upsample2dNearest(float *outdata, const float *indata, int inh, int inw, int outh, int outw,\n\t\t\t\t\t\t\t\t  int hscale, int wscale)\n{\n\t__shared__ float shdata[$hBlockSize][$wBlockSize];\n\n\tint x = blockIdx.x * blockDim.x + threadIdx.x;\n\tint y = blockIdx.y * blockDim.y + threadIdx.y;\n\tint z = blockIdx.z;\n\n\tif (y >= inh || x >= inw) return;\n\n\tshdata[threadIdx.y][threadIdx.x] = indata[z * inh * inw + y * inw + x];\n\t__syncthreads();\n\n\tfor (int i = 0; i < hscale; i++)\n\t\tfor (int j = 0; j < wscale; j++)\n\t\t{\n\t\t\tint outidx = z * outh * outw + (y * hscale + i) * outw + (x * wscale + j);\n\t\t\toutdata[outidx] = shdata[threadIdx.y][threadIdx.x];\n\t\t}\n}\n\n\nextern ""C""\n__global__ void upsample2dNearestBackward(float *ingrad, const float *outgrad, int inw, int outw,\n\t\t\t\t\t\t\t\t\t\t  int hscale, int wscale, int insize)\n{\n\tint idx = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (idx >= insize) return;\n\n\tint x = (idx % inw) * wscale;\n\tint y = (idx / inw) * hscale;\n\n\tfloat acc = 0.0f;\n\n\tfor (int i = 0; i < wscale; i++)\n\t\tfor (int j = 0; j < hscale; j++)\n\t\t\tacc += outgrad[(y + j) * outw + x + i];\n\n\tingrad[idx] = acc;\n}\n\n\nextern ""C""\n__global__ void upsample3dNearest(float *outdata, const float *indata, int ind, int inh, int inw,\n\t\t\t\t\t\t\t\t  int outd, int outh, int outw, int dscale, int hscale, int wscale)\n{\n\t__shared__ float shdata[$hBlockSize][$wBlockSize];\n\n\tint x = blockIdx.x * blockDim.x + threadIdx.x;\n\tint y = blockIdx.y * blockDim.y + threadIdx.y;\n\tint z = blockIdx.z;\n\n\tif (y >= inh || x >= inw) return;\n\n\tshdata[threadIdx.y][threadIdx.x] = indata[z * inh * inw + y * inw + x];\n\t__syncthreads();\n\n\tfor (int i = 0; i < dscale; i++)\n\t\tfor (int j = 0; j < hscale; j++)\n\t\t\tfor (int k = 0; k < wscale; k++)\n\t\t\t{\n\t\t\t\tint outidx = (z * dscale + i) * outh * outw + (y * hscale + j) * outw + (x * wscale + k);\n\t\t\t\toutdata[outidx] = shdata[threadIdx.y][threadIdx.x];\n\t\t\t}\n}\n\n\nextern ""C""\n__global__ void upsample3dNearestBackward(float *ingrad, const float *outgrad, int inh, int inw, int outh, int outw,\n\t\t\t\t\t\t\t\t\t\t  int dscale, int hscale, int wscale, int insize)\n{\n\tint idx = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (idx >= insize) return;\n\n\tint x = (idx % inw) * wscale;\n\tint y = ((idx % (inh * inw)) / inw) * hscale;\n\tint z = idx / (inh * inw) * dscale;\n\n\tfloat acc = 0.0f;\n\n\tfor (int i = 0; i < dscale; i++)\n\t\tfor (int j = 0; j < hscale; j++)\n\t\t\tfor (int k = 0; k < wscale; k++)\n\t\t\t\tacc += outgrad[(z + i) * outh * outw + (y + j) * outw + x + k];\n\n\tingrad[idx] = acc;\n}\n\n"""""")\n\n\nupsampleLinearTmpl = Template(""""""\n\nextern ""C""\n__global__ void upsample2dLinear(float *outdata, const float *indata, int batchsize, int maps, int inh, int inw,\n\t\t\t\t\t\t\t\t int outh, int outw, float rh, float rw)\n{\n\tint outx = blockIdx.x * blockDim.x + threadIdx.x;\n\tint outy = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (outx >= outw || outy >= outh) return;\n\n\tfloat h1r = rh * outy;\n\tint h1 = h1r;\n\tint h1p = (h1 < inh - 1) ? 1 : 0;\n\tfloat dh1 = h1r - h1;\n\tfloat dh0 = 1.0f - dh1;\n\n\tfloat w1r = rw * outx;\n\tint w1 = w1r;\n\tint w1p = (w1 < inw - 1) ? 1 : 0;\n\tfloat dw1 = w1r - w1;\n\tfloat dw0 = 1.0f - dw1;\n\n\tfor (int b = 0; b < batchsize ; b++)\n\t{\n\t\tint obstride = b * maps * outh * outw;\n\t\tint ibstride = b * maps * inh * inw;\n\n\t\tfor (int c = 0; c < maps; c++)\n\t\t{\n\t\t\tint ocstride = c * outh * outw;\n\t\t\tint icstride = c * inh * inw;\n\n\t\t\tfloat val = dh0 * (dw0 * indata[ibstride + icstride + h1 * inw + w1] +\n\t\t\t\t\t\t\t   dw1 * indata[ibstride + icstride + h1 * inw + w1 + w1p]) +\n\t\t\t\t\t\tdh1 * (dw0 * indata[ibstride + icstride + (h1 + h1p) * inw + w1] +\n\t\t\t\t\t\t\t   dw1 * indata[ibstride + icstride + (h1 + h1p) * inw + w1 + w1p]);\n\n\t\t\toutdata[obstride + ocstride + outy * outw + outx] = val;\n\t\t}\n\t}\n}\n\n\nextern ""C""\n__global__ void upsample2dLinearBackward(float *ingrad, const float *outgrad, int batchsize, int maps,\n\t\t\t\t\t\t\t\t\t\t int inh, int inw, int outh, int outw, float rh, float rw)\n{\n\tint outx = blockIdx.x * blockDim.x + threadIdx.x;\n\tint outy = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (outx >= outw || outy >= outh) return;\n\n\tfloat h1r = rh * outy;\n\tint h1 = h1r;\n\tint h1p = (h1 < inh - 1) ? 1 : 0;\n\tfloat dh1 = h1r - h1;\n\tfloat dh0 = 1.0f - dh1;\n\n\tfloat w1r = rw * outx;\n\tint w1 = w1r;\n\tint w1p = (w1 < inw - 1) ? 1 : 0;\n\tfloat dw1 = w1r - w1;\n\tfloat dw0 = 1.0f - dw1;\n\n\tfor (int b = 0; b < batchsize; b++)\n\t{\n\t\tint obstride = b * maps * outh * outw;\n\t\tint ibstride = b * maps * inh * inw;\n\n\t\tfor (int c = 0; c < maps; c++)\n\t\t{\n\t\t\tint ocstride = c * outh * outw;\n\t\t\tint icstride = c * inh * inw;\n\n\t\t\tfloat val = outgrad[obstride + ocstride + outy * outw + outx];\n\n\t\t\tatomicAdd(&ingrad[ibstride + icstride + h1 * inw + w1], dh0 * dw0 * val);\n\t\t\tatomicAdd(&ingrad[ibstride + icstride + h1 * inw + w1 + w1p], dh0 * dw1 * val);\n\t\t\tatomicAdd(&ingrad[ibstride + icstride + (h1 + h1p) * inw + w1], dh1 * dw0 * val);\n\t\t\tatomicAdd(&ingrad[ibstride + icstride + (h1 + h1p) * inw + w1 + w1p], dh1 * dw1 * val);\n\t\t}\n\t}\n}\n\n\nextern ""C""\n__global__ void upsample3dLinear(float *outdata, const float *indata, int batchsize, int maps,\n\t\t\t\t\t\t\t\t int ind, int inh, int inw, int outd, int outh, int outw, float rd, float rh, float rw)\n{\n\tint outx = blockIdx.x * blockDim.x + threadIdx.x;\n\tint outy = blockIdx.y * blockDim.y + threadIdx.y;\n\tint outz = blockIdx.z;\n\n\tif (outx >= outw || outy >= outh) return;\n\n\tfloat d1r = rd * outz;\n\tint d1 = d1r;\n\tint d1p = (d1 < ind - 1) ? 1 : 0;\n\tfloat dd1 = d1r - d1;\n\tfloat dd0 = 1.0f - dd1;\n\n\tfloat h1r = rh * outy;\n\tint h1 = h1r;\n\tint h1p = (h1 < inh - 1) ? 1 : 0;\n\tfloat dh1 = h1r - h1;\n\tfloat dh0 = 1.0f - dh1;\n\n\tfloat w1r = rw * outx;\n\tint w1 = w1r;\n\tint w1p = (w1 < inw - 1) ? 1 : 0;\n\tfloat dw1 = w1r - w1;\n\tfloat dw0 = 1.0f - dw1;\n\n\tfor (int b = 0; b < batchsize; b++)\n\t{\n\t\tint obstride = b * maps * outd * outh * outw;\n\t\tint ibstride = b * maps * ind * inh * inw;\n\n\t\tfor (int c = 0; c < maps; c++)\n\t\t{\n\t\t\tint ocstride = c * outd * outh * outw;\n\t\t\tint icstride = c * ind * inh * inw;\n\n\t\t\tfloat val =\n\t\t\tdd0 * (dh0 * (dw0 * indata[ibstride + icstride + d1 * inh *inw + h1 * inw + w1] +\n\t\t\t\t\t\t  dw1 * indata[ibstride + icstride + d1 * inw *inw + h1 * inw + w1 + w1p]) +\n\t\t\t\t   dh1 * (dw0 * indata[ibstride + icstride + d1 * inh *inw + (h1 + h1p) * inw + w1] +\n\t\t\t\t\t\t  dw1 * indata[ibstride + icstride + d1 * inh *inw + (h1 + h1p) * inw + w1 + w1p])) +\n\t\t\tdd1 * (dh0 * (dw0 * indata[ibstride + icstride + (d1 + d1p) * inh * inw + h1 * inw + w1] +\n\t\t\t\t\t\t  dw1 * indata[ibstride + icstride + (d1 + d1p) * inh * inw + h1 * inw + w1 + w1p]) +\n\t\t\t\t   dh1 * (dw0 * indata[ibstride + icstride + (d1 + d1p) * inh * inw + (h1 + h1p) * inw + w1] +\n\t\t\t\t\t\t  dw1 * indata[ibstride + icstride + (d1 + d1p) * inh * inw + (h1 + h1p) * inw + w1 + w1p]));\n\n\t\t\toutdata[obstride + ocstride + outz * outh * outw + outy * outw + outx] = val;\n\t\t}\n\t}\n}\n\n\nextern ""C""\n__global__ void upsample3dLinearBackward(float *ingrad, const float *outgrad, int batchsize, int maps,\n\t\t\t\t\t\t\t\t\t\t int ind, int inh, int inw, int outd, int outh, int outw,\n\t\t\t\t\t\t\t\t\t\t float rd, float rh, float rw)\n{\n\tint outx = blockIdx.x * blockDim.x + threadIdx.x;\n\tint outy = blockIdx.y * blockDim.y + threadIdx.y;\n\tint outz = blockIdx.z;\n\n\tif (outx >= outw || outy >= outh) return;\n\n\tfloat d1r = rd * outz;\n\tint d1 = d1r;\n\tint d1p = (d1 < ind - 1) ? 1 : 0;\n\tfloat dd1 = d1r - d1;\n\tfloat dd0 = 1.0f - dd1;\n\n\tfloat h1r = rh * outy;\n\tint h1 = h1r;\n\tint h1p = (h1 < inh - 1) ? 1 : 0;\n\tfloat dh1 = h1r - h1;\n\tfloat dh0 = 1.0f - dh1;\n\n\tfloat w1r = rw * outx;\n\tint w1 = w1r;\n\tint w1p = (w1 < inw - 1) ? 1 : 0;\n\tfloat dw1 = w1r - w1;\n\tfloat dw0 = 1.0f - dw1;\n\n\tfor (int b = 0; b < batchsize; b++)\n\t{\n\t\tint obstride = b * maps * outd * outh * outw;\n\t\tint ibstride = b * maps * ind * inh * inw;\n\n\t\tfor (int c = 0; c < maps; c++)\n\t\t{\n\t\t\tint ocstride = c * outd * outh * outw;\n\t\t\tint icstride = c * ind * inh * inw;\n\n\t\t\tfloat val = outgrad[obstride + ocstride + outz * outh * outw + outy * outw + outx];\n\n\t\t\tatomicAdd(&ingrad[ibstride+icstride + d1 * inh*inw + h1 * inw + w1], dd0 * dh0 * dw0 * val);\n\t\t\tatomicAdd(&ingrad[ibstride+icstride + d1 * inh*inw + h1 * inw + w1+w1p], dd0 * dh0 * dw1 * val);\n\t\t\tatomicAdd(&ingrad[ibstride+icstride + d1 * inh*inw + (h1+h1p) * inw + w1], dd0 * dh1 * dw0 * val);\n\t\t\tatomicAdd(&ingrad[ibstride+icstride + d1 * inh*inw + (h1+h1p) * inw + w1+w1p], dd0 * dh1 * dw1 * val);\n\n\t\t\tatomicAdd(&ingrad[ibstride+icstride + (d1+d1p) * inh*inw + h1 * inw + w1], dd1 * dh0 * dw0 * val);\n\t\t\tatomicAdd(&ingrad[ibstride+icstride + (d1+d1p) * inh*inw + h1 * inw + w1+w1p], dd1 * dh0 * dw1 * val);\n\t\t\tatomicAdd(&ingrad[ibstride+icstride + (d1+d1p) * inh*inw + (h1+h1p) * inw + w1], dd1 * dh1 * dw0 * val);\n\t\t\tatomicAdd(&ingrad[ibstride+icstride + (d1+d1p) * inh*inw + (h1+h1p) * inw + w1+w1p], dd1 * dh1 * dw1 * val);\n\t\t}\n\t}\n}\n\n"""""")\n\n\nclass UpsampleModule:\n\tdef __init__(self, backend):\n\t\tself.backend, self.GPUArray = backend, backend.GPUArray\n\t\tself.warpSize, self.nthreads = backend.warpSize, backend.nthreads\n\n\t\tself.hblocksize, self.wblocksize = 4, self.warpSize\n\n\t\tself.nearestMod = backend.SourceModule(upsampleNearestTmpl.substitute(\n\t\t\thBlockSize=self.hblocksize, wBlockSize=self.wblocksize\n\t\t))\n\t\tself.linearMod = backend.SourceModule(upsampleLinearTmpl.substitute())\n\n\n\tdef upsample2d(self, data, scale, mode=""nearest"", allocator=None):\n\t\tbatchsize, maps, inh, inw = data.shape\n\t\thscale, wscale = (scale, scale) if isinstance(scale, int) else scale\n\n\t\touth, outw = hscale * inh, wscale * inw\n\t\toutdata = self.GPUArray.empty((batchsize, maps, outh, outw), dtype=data.dtype, allocator=allocator)\n\n\t\tif mode == ""nearest"":\n\t\t\tblock = (self.wblocksize, self.hblocksize, 1)\n\t\t\tgrid = (roundUpDiv(inw, block[0]), roundUpDiv(inh, block[1]), batchsize * maps)\n\n\t\t\tself.nearestMod.upsample2dNearest(\n\t\t\t\toutdata, data, np.int32(inh), np.int32(inw), np.int32(outh), np.int32(outw),\n\t\t\t\tnp.int32(hscale), np.int32(wscale), block=block, grid=grid\n\t\t\t)\n\n\t\telif mode == ""linear"":\n\t\t\tblock = (self.warpSize, self.nthreads // self.warpSize, 1)\n\t\t\tgrid = (roundUpDiv(outw, block[0]), roundUpDiv(outh, block[1]), 1)\n\n\t\t\trh, rw = (inh - 1) / (outh - 1), (inw - 1) / (outw - 1)\n\n\t\t\tself.linearMod.upsample2dLinear(\n\t\t\t\toutdata, data, np.int32(batchsize), np.int32(maps), np.int32(inh), np.int32(inw),\n\t\t\t\tnp.int32(outh), np.int32(outw), np.float32(rh), np.float32(rw), block=block, grid=grid\n\t\t\t)\n\n\t\telse:\n\t\t\traise NotImplementedError(mode)\n\n\t\treturn outdata\n\n\n\tdef upsample2dBackward(self, grad, scale, mode=""nearest"", allocator=None):\n\t\tbatchsize, maps, outh, outw = grad.shape\n\t\thscale, wscale = (scale, scale) if isinstance(scale, int) else scale\n\n\t\tinh, inw = outh // hscale, outw // wscale\n\n\t\tif mode == ""nearest"":\n\t\t\tingrad = self.GPUArray.empty((batchsize, maps, inh, inw), dtype=grad.dtype, allocator=allocator)\n\n\t\t\tblk = self.warpSize * 8\n\t\t\tblock = (blk, 1, 1)\n\t\t\tgrid = (roundUpDiv(ingrad.size, blk), 1, 1)\n\n\t\t\tself.nearestMod.upsample2dNearestBackward(\n\t\t\t\tingrad, grad, np.int32(inw), np.int32(outw), np.int32(hscale), np.int32(wscale), np.int32(ingrad.size),\n\t\t\t\tblock=block, grid=grid\n\t\t\t)\n\n\t\telif mode == ""linear"":\n\t\t\tingrad = self.GPUArray.zeros((batchsize, maps, inh, inw), dtype=grad.dtype, allocator=allocator)\n\n\t\t\tblock = (self.warpSize, self.nthreads // self.warpSize, 1)\n\t\t\tgrid = (roundUpDiv(outw, block[0]), roundUpDiv(outh, block[1]), 1)\n\n\t\t\trh, rw = (inh - 1) / (outh - 1), (inw - 1) / (outw - 1)\n\n\t\t\tself.linearMod.upsample2dLinearBackward(\n\t\t\t\tingrad, grad, np.int32(batchsize), np.int32(maps), np.int32(inh), np.int32(inw),\n\t\t\t\tnp.int32(outh), np.int32(outw), np.float32(rh), np.float32(rw), block=block, grid=grid\n\t\t\t)\n\n\t\telse:\n\t\t\traise NotImplementedError(mode)\n\n\t\treturn ingrad\n\n\n\tdef upsample3d(self, data, scale, mode=""nearest"", allocator=None):\n\t\tbatchsize, maps, ind, inh, inw = data.shape\n\t\tdscale, hscale, wscale = (scale, scale, scale) if isinstance(scale, int) else scale\n\n\t\toutd, outh, outw = dscale * ind, hscale * inh, wscale * inw\n\t\toutdata = self.GPUArray.empty((batchsize, maps, outd, outh, outw), dtype=data.dtype, allocator=allocator)\n\n\t\tif mode == ""nearest"":\n\t\t\tblock = (self.wblocksize, self.hblocksize, 1)\n\t\t\tgrid = (roundUpDiv(inw, block[0]), roundUpDiv(inh, block[1]), batchsize * maps * ind)\n\n\t\t\tself.nearestMod.upsample3dNearest(\n\t\t\t\toutdata, data, np.int32(ind), np.int32(inh), np.int32(inw),\n\t\t\t\tnp.int32(outd), np.int32(outh), np.int32(outw), np.int32(dscale), np.int32(hscale), np.int32(wscale),\n\t\t\t\tblock=block, grid=grid\n\t\t\t)\n\n\t\telif mode == ""linear"":\n\t\t\tblock = (self.warpSize, self.nthreads // self.warpSize, 1)\n\t\t\tgrid = (roundUpDiv(outw, block[0]), roundUpDiv(outh, block[1]), outd)\n\n\t\t\trd, rh, rw = (ind - 1) / (outd - 1), (inh - 1) / (outh - 1), (inw - 1) / (outw - 1)\n\n\t\t\tself.linearMod.upsample3dLinear(\n\t\t\t\toutdata, data, np.int32(batchsize), np.int32(maps), np.int32(ind), np.int32(inh), np.int32(inw),\n\t\t\t\tnp.int32(outd), np.int32(outh), np.int32(outw), np.float32(rd), np.float32(rh), np.float32(rw),\n\t\t\t\tblock=block, grid=grid\n\t\t\t)\n\n\t\telse:\n\t\t\traise NotImplementedError(mode)\n\n\t\treturn outdata\n\n\n\tdef upsample3dBackward(self, grad, scale, mode=""nearest"", allocator=None):\n\t\tbatchsize, maps, outd, outh, outw = grad.shape\n\t\tdscale, hscale, wscale = (scale, scale, scale) if isinstance(scale, int) else scale\n\n\t\tind, inh, inw = outd // dscale, outh // hscale, outw // wscale\n\n\t\tif mode == ""nearest"":\n\t\t\tingrad = self.GPUArray.empty((batchsize, maps, ind, inh, inw), dtype=grad.dtype, allocator=allocator)\n\n\t\t\tblk = self.warpSize * 8\n\t\t\tblock = (blk, 1, 1)\n\n\t\t\tgrid = (roundUpDiv(ingrad.size, blk), 1, 1)\n\n\t\t\tself.nearestMod.upsample3dNearestBackward(\n\t\t\t\tingrad, grad, np.int32(inh), np.int32(inw), np.int32(outh), np.int32(outw),\n\t\t\t\tnp.int32(dscale), np.int32(hscale), np.int32(wscale), np.int32(ingrad.size), block=block, grid=grid\n\t\t\t)\n\n\t\telif mode == ""linear"":\n\t\t\tingrad = self.GPUArray.zeros((batchsize, maps, ind, inh, inw), dtype=grad.dtype, allocator=allocator)\n\n\t\t\tblock = (self.warpSize, self.nthreads // self.warpSize, 1)\n\t\t\tgrid = (roundUpDiv(outw, block[0]), roundUpDiv(outh, block[1]), outd)\n\n\t\t\trd, rh, rw = (ind - 1) / (outd - 1), (inh - 1) / (outh - 1), (inw - 1) / (outw - 1)\n\n\t\t\tself.linearMod.upsample3dLinearBackward(\n\t\t\t\tingrad, grad, np.int32(batchsize), np.int32(maps), np.int32(ind), np.int32(inh), np.int32(inw),\n\t\t\t\tnp.int32(outd), np.int32(outh), np.int32(outw), np.float32(rd), np.float32(rh), np.float32(rw),\n\t\t\t\tblock=block, grid=grid\n\t\t\t)\n\n\t\telse:\n\t\t\traise NotImplementedError(mode)\n\n\t\treturn ingrad\n\n\ndef unittest():\n\tfrom PuzzleLib.Cuda import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tmodule = UpsampleModule(Backend.getBackend(deviceIdx))\n\n\t\tupsample2dNearestTest(module)\n\t\tupsample2dLinearTest(module)\n\t\tupsample2dSpeedTest(module)\n\n\t\tupsample3dNearestTest(module)\n\t\tupsample3dLinearTest(module)\n\t\tupsample3dSpeedTest(module)\n\n\ndef upsample2dNearestTest(module):\n\tbatchsize, maps, inh, inw = 1, 2, 16, 15\n\tscale = 2\n\n\thostData = np.random.uniform(low=-1.0, high=1.0, size=(batchsize, maps, inh, inw)).astype(np.float32)\n\n\tdata = module.GPUArray.toGpu(hostData)\n\toutdata = module.upsample2d(data, scale, mode=""nearest"")\n\n\thostOutData = np.empty(outdata.shape, dtype=np.float32)\n\n\tfor b, c, y, x in itertools.product(range(batchsize), range(maps), range(inh), range(inw)):\n\t\thostOutData[b, c, y * scale:(y + 1) * scale, x * scale:(x + 1) * scale] = hostData[b, c, y, x]\n\n\tassert np.allclose(hostOutData, outdata.get())\n\n\thostGrad = np.random.randn(*outdata.shape).astype(np.float32)\n\n\tgrad = module.GPUArray.toGpu(hostGrad)\n\tingrad = module.upsample2dBackward(grad, scale)\n\n\thostInGrad = np.zeros(data.shape, dtype=np.float32)\n\n\tfor b, c, y, x, dy, dx in itertools.product(\n\t\trange(batchsize), range(maps), range(inh), range(inw), range(scale), range(scale)\n\t):\n\t\thostInGrad[b, c, y, x] += hostGrad[b, c, y * scale + dy, x * scale + dx]\n\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=1e-5)\n\n\ndef upsample2dLinearTest(module):\n\tbatchsize, maps, inh, inw = 3, 2, 4, 4\n\thscale, wscale = 2, 3\n\n\thostData = np.random.randn(batchsize, maps, inh, inw).astype(np.float32)\n\n\tdata = module.GPUArray.toGpu(hostData)\n\toutdata = module.upsample2d(data, (hscale, wscale), mode=""linear"")\n\n\thostOutData = np.zeros(outdata.shape, dtype=np.float32)\n\trh, rw = (inh - 1) / (inh * hscale - 1), (inw - 1) / (inw * wscale - 1)\n\n\tfor b, c, y, x, in itertools.product(range(batchsize), range(maps), range(inh * hscale), range(inw * wscale)):\n\t\tiny, inx = int(rh * y), int(rw * x)\n\t\tdy, dx = 1.0 - (rh * y - iny), 1.0 - (rw * x - inx)\n\n\t\tyi, xi = 1 if y < inh * hscale - 1 else 0, 1 if x < inw * wscale - 1 else 0\n\n\t\thostOutData[b, c, y, x] = dy * (dx * hostData[b, c, iny, inx] + (1 - dx) * hostData[b, c, iny, inx + xi]) + \\\n\t\t\t\t\t\t\t\t  (1 - dy) * (dx * hostData[b, c, iny + yi, inx] +\n\t\t\t\t\t\t\t\t  (1 - dx) * hostData[b, c, iny + yi, inx + xi])\n\n\thostGrad = np.random.randn(*outdata.shape).astype(np.float32)\n\n\tgrad = module.GPUArray.toGpu(hostGrad)\n\tingrad = module.upsample2dBackward(grad, (hscale, wscale), mode=""linear"")\n\n\thostInGrad = np.zeros(data.shape, dtype=np.float32)\n\n\tfor b, c, y, x in itertools.product(range(batchsize), range(maps), range(inh * hscale), range(inw * wscale)):\n\t\tiny, inx = int(rh * y), int(rw * x)\n\t\tdy, dx = 1.0 - (rh * y - iny), 1.0 - (rw * x - inx)\n\n\t\tyi, xi = 1 if y < inh * hscale - 1 else 0, 1 if x < inw * wscale - 1 else 0\n\t\tval = hostGrad[b, c, y, x]\n\n\t\thostInGrad[b, c, iny, inx] += dy * dx * val\n\t\thostInGrad[b, c, iny, inx + xi] += dy * (1 - dx) * val\n\t\thostInGrad[b, c, iny + yi, inx] += (1 - dy) * dx * val\n\t\thostInGrad[b, c, iny + yi, inx + xi] += (1 - dy) * (1 - dx) * val\n\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=1e-5)\n\n\ndef upsample3dNearestTest(module):\n\tbatchsize, maps, ind, inh, inw = 4, 2, 3, 5, 3\n\tscale = 2\n\n\thostData = np.random.randn(batchsize, maps, ind, inh, inw).astype(np.float32)\n\n\tdata = module.GPUArray.toGpu(hostData)\n\toutdata = module.upsample3d(data, scale, mode=""nearest"")\n\n\thostOutData = np.empty(outdata.shape, dtype=np.float32)\n\n\tfor b, c, z, y, x in itertools.product(range(batchsize), range(maps), range(ind), range(inh), range(inw)):\n\t\thostOutData[b, c, z * scale:(z + 1) * scale, y * scale:(y + 1) * scale, x * scale:(x + 1) * scale] = \\\n\t\t\thostData[b, c, z, y, x]\n\n\tassert np.allclose(hostOutData, outdata.get())\n\n\thostGrad = np.random.randn(*outdata.shape).astype(np.float32)\n\n\tgrad = module.GPUArray.toGpu(hostGrad)\n\tingrad = module.upsample3dBackward(grad, scale)\n\n\thostInGrad = np.zeros(data.shape, dtype=np.float32)\n\n\tfor b, c, z, y, x, dz, dy, dx in itertools.product(\n\t\trange(batchsize), range(maps), range(ind), range(inh), range(inw), range(scale), range(scale), range(scale)\n\t):\n\t\thostInGrad[b, c, z, y, x] += hostGrad[b, c, z * scale + dz, y * scale + dy, x * scale + dx]\n\n\tassert np.allclose(hostInGrad, ingrad.get())\n\n\ndef upsample3dLinearTest(module):\n\tbatchsize, maps, ind, inh, inw = 1, 2, 2, 2, 2\n\tdscale, hscale, wscale = 2, 2, 1\n\n\thostData = np.random.randn(batchsize, maps, ind, inh, inw).astype(np.float32)\n\n\tdata = module.GPUArray.toGpu(hostData)\n\toutdata = module.upsample3d(data, (dscale, hscale, wscale), mode=""linear"")\n\n\thostOutData = np.zeros(outdata.shape, dtype=np.float32)\n\trd, rh, rw = (ind - 1) / (ind * dscale - 1), (inh - 1) / (inh * hscale - 1), (inw - 1) / (inw * wscale - 1)\n\n\tfor b, c, z, y, x in itertools.product(\n\t\trange(batchsize), range(maps), range(ind * dscale), range(inh * hscale), range(inw * wscale)\n\t):\n\t\tinz, iny, inx = int(rd * z), int(rh * y), int(rw * x)\n\t\tdz, dy, dx = 1.0 - (rd * z - inz), 1.0 - (rh * y - iny), 1.0 - (rw * x - inx)\n\n\t\tzi = 1 if z < ind * dscale - 1 else 0\n\t\tyi = 1 if y < inh * hscale - 1 else 0\n\t\txi = 1 if x < inw * wscale - 1 else 0\n\n\t\thostOutData[b, c, z, y, x] = dz * (dy * (\n\t\t\tdx * hostData[b, c, inz, iny, inx] + (1 - dx) * hostData[b, c, inz, iny, inx + xi]\n\t\t) + (1 - dy) * (\n\t\t\tdx * hostData[b, c, inz, iny + yi, inx] + (1 - dx) * hostData[b, c, inz, iny + yi, inx + xi]\n\t\t)) + (1 - dz) * (dy * (\n\t\t\tdx * hostData[b, c, inz+zi, iny, inx] + (1 - dx) * hostData[b, c, inz + zi, iny, inx + xi]\n\t\t) + (1 - dy) * (\n\t\t\tdx * hostData[b, c, inz + zi, iny + yi, inx] + (1 - dx) * hostData[b, c, inz + zi, iny + yi, inx + xi]\n\t\t))\n\n\tassert np.allclose(hostOutData, outdata.get())\n\n\thostGrad = np.random.randn(*outdata.shape).astype(np.float32)\n\n\tgrad = module.GPUArray.toGpu(hostGrad)\n\tingrad = module.upsample3dBackward(grad, (dscale, hscale, wscale), mode=""linear"")\n\n\thostInGrad = np.zeros(data.shape, dtype=np.float32)\n\n\tfor b, c, z, y, x in itertools.product(\n\t\t\trange(batchsize), range(maps), range(ind * dscale), range(inh * hscale), range(inw * wscale)\n\t):\n\t\tinz, iny, inx = int(rd * z), int(rh * y), int(rw * x)\n\t\tdz, dy, dx = 1.0 - (rd * z - inz), 1.0 - (rh * y - iny), 1.0 - (rw * x - inx)\n\n\t\tzi = 1 if z < ind * dscale - 1 else 0\n\t\tyi = 1 if y < inh * hscale - 1 else 0\n\t\txi = 1 if x < inw * wscale - 1 else 0\n\n\t\tval = hostGrad[b, c, z, y, x]\n\n\t\thostInGrad[b, c, inz, iny, inx] += dz * dy * dx * val\n\t\thostInGrad[b, c, inz, iny, inx + xi] += dz * dy * (1 - dx) * val\n\t\thostInGrad[b, c, inz, iny + yi, inx] += dz * (1 - dy) * dx * val\n\t\thostInGrad[b, c, inz, iny + yi, inx + xi] += dz * (1 - dy) * (1 - dx) * val\n\n\t\thostInGrad[b, c, inz + zi, iny, inx] += (1 - dz) * dy * dx * val\n\t\thostInGrad[b, c, inz + zi, iny, inx + xi] += (1 - dz) * dy * (1 - dx) * val\n\t\thostInGrad[b, c, inz + zi, iny + yi, inx] += (1 - dz) * (1 - dy) * dx * val\n\t\thostInGrad[b, c, inz + zi, iny + yi, inx + xi] += (1 - dz) * (1 - dy) * (1 - dx) * val\n\n\tassert np.allclose(hostInGrad, ingrad.get())\n\n\ndef upsample2dSpeedTest(module):\n\tbatchsize, maps, inh, inw = 32, 16, 32, 32\n\tscale = 2\n\n\tdata = module.GPUArray.toGpu(np.random.randn(batchsize, maps, inh, inw).astype(np.float32))\n\n\tbnd = module.backend\n\tbnd.timeKernel(module.upsample2d, args=(data, scale, ""nearest"", bnd.memoryPool), logname=""nearest 2d mode"")\n\tbnd.timeKernel(module.upsample2d, args=(data, scale, ""linear"", bnd.memoryPool), logname=""linear 2d mode"")\n\n\ndef upsample3dSpeedTest(module):\n\tbatchsize, maps, ind, inh, inw = 32, 16, 4, 32, 32\n\tscale = 2\n\n\tdata = module.GPUArray.toGpu(np.random.randn(batchsize, maps, ind, inh, inw).astype(np.float32))\n\n\tbnd = module.backend\n\tbnd.timeKernel(module.upsample3d, args=(data, scale, ""nearest"", bnd.memoryPool), logname=""nearest 3d mode"")\n\tbnd.timeKernel(module.upsample3d, args=(data, scale, ""linear"", bnd.memoryPool), logname=""linear 3d mode"")\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cuda/Source/Build.py,1,"b'import sys, os\n\nimport numpy as np\n\nfrom PuzzleLib.Compiler.Toolchain import guessToolchain\nfrom PuzzleLib.Compiler.Codegen.PyDefines.Generate import generatePyDefines\nfrom PuzzleLib.Compiler.Codegen.Vector.Generate import generateVector\nfrom PuzzleLib.Compiler.Codegen.Malloc.Generate import generateMalloc\n\n\ndef buildDriver(debugmode, verbose):\n\tcc = prepareCompiler(debugmode, verbose)\n\tcc = prepareCuda(cc)\n\n\tgenerateTemplates(path=""."")\n\n\tdriver = ""../Driver"" + (""_d"" if debugmode >= 3 else """") + cc.pydext\n\tcc.build(driver, collectSources(path=""."")).clearPath("".."")\n\n\treturn driver\n\n\ndef prepareCompiler(debugmode, verbose):\n\tcc = guessToolchain(verbose=verbose)\n\tcc.addLibrary(""numpy"", [np.get_include()], [], [])\n\n\tif debugmode > 0:\n\t\toptlevel, debuglevel = 0, 3\n\t\tcc.addDefine(""ENABLE_TRACE_MALLOC"")\n\n\t\tif debugmode >= 2:\n\t\t\tcc.addDefine(""TRACE_CUDA_DRIVER"", ""TRACE_CUDA_CURAND"", ""TRACE_CUDA_CUBLAS"", ""TRACE_CUDA_CUDNN"")\n\n\t\t\tif debugmode >= 3:\n\t\t\t\tcc.libraries[0] += ""_d""\n\n\t\t\t\tif sys.platform == ""win32"":\n\t\t\t\t\tcc.addDefine(""_DEBUG"")\n\n\telse:\n\t\toptlevel, debuglevel = 4, 0\n\t\tcc.addDefine(""NDEBUG"")\n\n\treturn cc.withOptimizationLevel(level=optlevel, debuglevel=debuglevel)\n\n\ndef prepareCuda(cc):\n\tif sys.platform == ""win32"":\n\t\tCUDA_PATH = os.environ[""CUDA_PATH""]\n\t\tinclude, lib = os.path.join(CUDA_PATH, ""include""), os.path.join(CUDA_PATH, ""lib\\\\x64"")\n\n\telse:\n\t\tinclude, lib = ""/usr/local/cuda/include"", ""/usr/local/cuda/lib64""\n\n\treturn cc.addLibrary(""cuda"", [include], [lib], [""cuda"", ""cudart"", ""nvrtc"", ""curand"", ""cublas"", ""cudnn""])\n\n\ndef generateTemplates(path):\n\tgeneratePyDefines(os.path.join(path, ""Core""))\n\n\tgenerateVector(\n\t\tname=""Cuda_AllocVector"", T=""Cuda_Ptr"", minCapacity=128,\n\t\tdestruct=""CUDA_FREE"",\n\t\theaderPreambule=""""""\n#include ""Common.h""\n\n\ntypedef void *Cuda_Ptr;\n"""""",\n\t\tbodyPreambule=""""""\n#include ""../TraceMalloc/TraceMalloc.gen.h""\n#define CUDA_FREE(ptr) CUDA_ASSERT(cudaFree(ptr))\n"""""",\n\t\tmalloc=""TRACE_MALLOC"", free=""TRACE_FREE"", filename=os.path.join(path, ""Core/AllocVector"")\n\t)\n\n\tgenerateMalloc(name=""TraceMalloc"", filename=os.path.join(path, ""TraceMalloc/TraceMalloc""))\n\n\ndef collectSources(path):\n\treturn collectCoreSources(path) + collectLibSources(path) + collectDnnSources(path)\n\n\ndef collectCoreSources(path):\n\tsources = [\n\t\t""./Core/AllocVector.gen.c"",\n\t\t""./Core/Allocator.c"",\n\n\t\t""./Core/Array.c"",\n\t\t""./Core/Buffer.c"",\n\t\t""./Core/Device.c"",\n\t\t""./Core/Driver.c"",\n\t\t""./Core/Module.c"",\n\t\t""./Core/Stream.c"",\n\n\t\t""./TraceMalloc/AllocTree.gen.c"",\n\t\t""./TraceMalloc/TraceMalloc.gen.c""\n\t]\n\n\treturn [os.path.join(path, source) for source in sources]\n\n\ndef collectLibSources(path):\n\tsources = [\n\t\t""./Libs/CuRand.c"",\n\t\t""./Libs/CuBlas.c""\n\t]\n\n\treturn [os.path.join(path, source) for source in sources]\n\n\ndef collectDnnSources(path):\n\tsources = [\n\t\t""./Libs/CuDnn.c"",\n\t\t""./Libs/CuDnnPool.c"",\n\t\t""./Libs/CuDnnNorm.c"",\n\t\t""./Libs/CuDnnMemory.c"",\n\t\t""./Libs/CuDnnRnn.c"",\n\t\t""./Libs/CuDnnSpatialTf.c""\n\t]\n\n\treturn [os.path.join(path, source) for source in sources]\n\n\ndef main():\n\treturn buildDriver(debugmode=0, verbose=2)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Cuda/ThirdParty/libnpp.py,0,"b'import sys, ctypes, warnings, itertools\n\n\n_version_list = [""10.2.1.89"", ""10.2.0.243"", ""10.1.168"", 10.0, 9.2, 9.1, 9.0, 8.0]\nif sys.platform == ""linux"":\n\t_libnppc_libname_list = [\n\t\t""libnppc.so.%s"" % v for v in _version_list\n\t]\n\n\t_libnppi_libname_list = [\n\t\t""libnppi%s.so.%s"" % (""g"" if isinstance(v, str) or v >= 9.0 else """", v) for v in _version_list\n\t]\n\nelif sys.platform == ""darwin"":\n\t_libnppc_libname_list = [""libnppc.dylib""]\n\t_libnppi_libname_list = [""libnppi.dylib""]\n\nelif sys.platform == ""win32"":\n\t_libnppc_libname_list = [""nppc64_10.dll""] + [\n\t\t""nppc64_%s.dll"" % int(10 * v) for v in _version_list[-5:]\n\t]\n\n\t_libnppi_libname_list = [""nppig64_10.dll""] + [\n\t\t""nppi%s64_%s.dll"" % (""g"" if v >= 9.0 else """", int(10 * v)) for v in _version_list[-5:]\n\t]\n\nelse:\n\traise RuntimeError(""Unsupported platform for npp"")\n\n\n_libnppc, _libnppi = None, None\nfor _libnppc_libname, _libnppi_libname in itertools.zip_longest(_libnppc_libname_list, _libnppi_libname_list):\n\ttry:\n\t\tif sys.platform == ""win32"":\n\t\t\t_libnppc = ctypes.windll.LoadLibrary(_libnppc_libname)\n\t\t\t_libnppi = ctypes.windll.LoadLibrary(_libnppi_libname)\n\t\telse:\n\t\t\t_libnppc = ctypes.cdll.LoadLibrary(_libnppc_libname)\n\t\t\t_libnppi = ctypes.cdll.LoadLibrary(_libnppi_libname)\n\texcept OSError:\n\t\tpass\n\telse:\n\t\tbreak\nif _libnppc is None or _libnppi is None:\n\traise OSError(""npp library not found (searched for following version(s): %s)"" % _version_list)\n\n\nclass nppError(Exception):\n\tpass\n\nclass nppNotSupportedModeError(nppError):\n\tpass\n\nclass nppInvalidHostPointerError(nppError):\n\tpass\n\nclass nppInvalidDevicePointerError(nppError):\n\tpass\n\nclass nppLutPaletteBitsizeError(nppError):\n\tpass\n\nclass nppZcModeNotSupportedError(nppError):\n\tpass\n\nclass nppNotSufficientComputeCapability(nppError):\n\tpass\n\nclass nppTextureBindError(nppError):\n\tpass\n\nclass nppWrongIntersectionRoiError(nppError):\n\tpass\n\nclass nppHaarClassifierPixelMatchError(nppError):\n\tpass\n\nclass nppMemfreeError(nppError):\n\tpass\n\nclass nppMemsetError(nppError):\n\tpass\n\nclass nppMemcpyError(nppError):\n\tpass\n\nclass nppAlignmentError(nppError):\n\tpass\n\nclass nppCudaKernelExecutionError(nppError):\n\tpass\n\nclass nppRoundModeNotSupportedError(nppError):\n\tpass\n\nclass nppQualityIndexError(nppError):\n\tpass\n\nclass nppResizeNoOperationError(nppError):\n\tpass\n\nclass nppOverflowError(nppError):\n\tpass\n\nclass nppNotEvenStepError(nppError):\n\tpass\n\nclass nppHistogramNumberOfLevelsError(nppError):\n\tpass\n\nclass nppLutNumberOfLevelsError(nppError):\n\tpass\n\nclass nppCorruptedDataError(nppError):\n\tpass\n\nclass nppChannelOrderError(nppError):\n\tpass\n\nclass nppZeroMaskValueError(nppError):\n\tpass\n\nclass nppQuadrangleError(nppError):\n\tpass\n\nclass nppRectangleError(nppError):\n\tpass\n\nclass nppCoefficientError(nppError):\n\tpass\n\nclass nppNumberOfChannelsError(nppError):\n\tpass\n\nclass nppCoiError(nppError):\n\tpass\n\nclass nppDivisorError(nppError):\n\tpass\n\nclass nppChannelError(nppError):\n\tpass\n\nclass nppStrideError(nppError):\n\tpass\n\nclass nppAnchorError(nppError):\n\tpass\n\nclass nppMaskSizeError(nppError):\n\tpass\n\nclass nppResizeFactorError(nppError):\n\tpass\n\nclass nppInterpolationError(nppError):\n\tpass\n\nclass nppMirrorFlipError(nppError):\n\tpass\n\nclass nppMoment00ZeroError(nppError):\n\tpass\n\nclass nppThresholdNegativeLevelError(nppError):\n\tpass\n\nclass nppThresholdError(nppError):\n\tpass\n\nclass nppContextMatchError(nppError):\n\tpass\n\nclass nppFftFlagError(nppError):\n\tpass\n\nclass nppFftOrderError(nppError):\n\tpass\n\nclass nppStepError(nppError):\n\tpass\n\nclass nppScaleRangeError(nppError):\n\tpass\n\nclass nppDataTypeError(nppError):\n\tpass\n\nclass nppOutOfRangeError(nppError):\n\tpass\n\nclass nppDivideByZeroError(nppError):\n\tpass\n\nclass nppMemoryAllocationError(nppError):\n\tpass\n\nclass nppNullPointerError(nppError):\n\tpass\n\nclass nppRangeError(nppError):\n\tpass\n\nclass nppSizeError(nppError):\n\tpass\n\nclass nppBadArgumentError(nppError):\n\tpass\n\nclass nppNoMemoryError(nppError):\n\tpass\n\nclass nppNotImplementedError(nppError):\n\tpass\n\nclass nppErrorReserved(nppError):\n\tpass\n\n\nclass nppWarning(Warning):\n\tpass\n\nclass nppNoOperationWarning(nppWarning):\n\tpass\n\nclass nppDivideByZeroWarning(nppWarning):\n\tpass\n\nclass nppAffineQuadIncorrectWarning(nppWarning):\n\tpass\n\nclass nppWrongIntersectionRoiWarning(nppWarning):\n\tpass\n\nclass nppWrongIntersectionQuadWarning(nppWarning):\n\tpass\n\nclass nppDoubleSizeWarning(nppWarning):\n\tpass\n\nclass nppMisalignedDstRoiWarning(nppWarning):\n\tpass\n\n\nnppExceptions = {\n\t-9999: nppNotSupportedModeError,\n\t-1032: nppInvalidHostPointerError,\n\t-1031: nppInvalidDevicePointerError,\n\t-1030: nppLutPaletteBitsizeError,\n\t-1028: nppZcModeNotSupportedError,\n\t-1027: nppNotSufficientComputeCapability,\n\t-1024: nppTextureBindError,\n\t-1020: nppWrongIntersectionRoiError,\n\t-1006: nppHaarClassifierPixelMatchError,\n\t-1005: nppMemfreeError,\n\t-1004: nppMemsetError,\n\t-1003: nppMemcpyError,\n\t-1002: nppAlignmentError,\n\t-1000: nppCudaKernelExecutionError,\n\t-213: nppRoundModeNotSupportedError,\n\t-210: nppQualityIndexError,\n\t-201: nppResizeNoOperationError,\n\t-109: nppOverflowError,\n\t-108: nppNotEvenStepError,\n\t-107: nppHistogramNumberOfLevelsError,\n\t-106: nppLutNumberOfLevelsError,\n\t-61: nppCorruptedDataError,\n\t-60: nppChannelOrderError,\n\t-59: nppZeroMaskValueError,\n\t-58: nppQuadrangleError,\n\t-57: nppRectangleError,\n\t-56: nppCoefficientError,\n\t-53: nppNumberOfChannelsError,\n\t-52: nppCoiError,\n\t-51: nppDivisorError,\n\t-47: nppChannelError,\n\t-37: nppStrideError,\n\t-34: nppAnchorError,\n\t-33: nppMaskSizeError,\n\t-23: nppResizeFactorError,\n\t-22: nppInterpolationError,\n\t-21: nppMirrorFlipError,\n\t-20: nppMoment00ZeroError,\n\t-19: nppThresholdNegativeLevelError,\n\t-18: nppThresholdError,\n\t-17: nppContextMatchError,\n\t-16: nppFftFlagError,\n\t-15: nppFftOrderError,\n\t-14: nppStepError,\n\t-13: nppScaleRangeError,\n\t-12: nppDataTypeError,\n\t-11: nppOutOfRangeError,\n\t-10: nppDivideByZeroError,\n\t-9: nppMemoryAllocationError,\n\t-8: nppNullPointerError,\n\t-7: nppRangeError,\n\t-6: nppSizeError,\n\t-5: nppBadArgumentError,\n\t-4: nppNoMemoryError,\n\t-3: nppNotImplementedError,\n\t-2: nppError,\n\t-1: nppErrorReserved\n}\n\n\nnppWarnings = {\n\t1: nppNoOperationWarning,\n\t6: nppDivideByZeroWarning,\n\t28: nppAffineQuadIncorrectWarning,\n\t29: nppWrongIntersectionRoiWarning,\n\t30: nppWrongIntersectionQuadWarning,\n\t35: nppDoubleSizeWarning,\n\t10000: nppMisalignedDstRoiWarning\n}\n\n\nNppiInterpolationMode = {\n\t""NPPI_INTER_UNDEFINED"": 0,\n\t""NPPI_INTER_NN"": 1,\n\t""NPPI_INTER_LINEAR"": 2,\n\t""NPPI_INTER_CUBIC"": 4,\n\t""NPPI_INTER_CUBIC2P_BSPLINE"": 5,\n\t""NPPI_INTER_CUBIC2P_CATMULLROM"": 6,\n\t""NPPI_INTER_CUBIC2P_B05C03"": 7,\n\t""NPPI_INTER_SUPER"": 8,\n\t""NPPI_INTER_LANCZOS"": 16,\n\t""NPPI_INTER_LANCZOS3_ADVANCED"": 17,\n\t""NPPI_SMOOTH_EDGE"": 1 << 31\n}\n\nclass NppLibraryVersion(ctypes.Structure):\n\t_fields_ = [\n\t\t(""major"", ctypes.c_int),\n\t\t(""minor"", ctypes.c_int),\n\t\t(""build"", ctypes.c_int)\n\t]\n\nclass NppiSize(ctypes.Structure):\n\t_fields_ = [\n\t\t(""width"", ctypes.c_int),\n\t\t(""height"", ctypes.c_int)\n\t]\n\nclass NppiRect(ctypes.Structure):\n\t_fields_ = [\n\t\t(""x"", ctypes.c_int),\n\t\t(""y"", ctypes.c_int),\n\t\t(""width"", ctypes.c_int),\n\t\t(""height"", ctypes.c_int)\n\t]\n\n\ndef nppCheckStatus(status):\n\tif status < 0:\n\t\ttry:\n\t\t\traise nppExceptions[status]\n\t\texcept KeyError:\n\t\t\traise nppError\n\telif status > 0:\n\t\twarnings.warn(str(nppWarnings[status]))\n\n\n_libnppc.nppGetLibVersion.restype = ctypes.POINTER(NppLibraryVersion)\n_libnppc.nppGetLibVersion.argtypes = []\ndef nppGetLibVersion():\n\tlibversion = _libnppc.nppGetLibVersion().contents\n\treturn libversion.major, libversion.minor, libversion.build\n\n\n_libnppc.nppGetStream.restype = ctypes.c_void_p\n_libnppc.nppGetStream.argtypes = []\ndef nppGetStream():\n\tstreamId = _libnppc.nppGetStream()\n\treturn streamId.value\n\n\n_libnppc.nppSetStream.restype = None\n_libnppc.nppSetStream.argtypes = [ctypes.c_void_p]\ndef nppSetStream(streamId):\n\t_libnppc.nppSetStream(streamId)\n\n\n_libnppi.nppiGetResizeRect.restype = ctypes.c_int\n_libnppi.nppiGetResizeRect.argtypes = [NppiRect, ctypes.POINTER(NppiRect), ctypes.c_double, ctypes.c_double,\n\t\t\t\t\t\t\t\t\t   ctypes.c_double, ctypes.c_double, ctypes.c_int]\ndef nppiGetResizeRect(rect, nXFactor, nYFactor, nXShift, nYShift, interpolation):\n\tx, y, width, height = rect\n\n\tinrect = NppiRect(x, y, width, height)\n\toutrect = NppiRect()\n\n\tstatus = _libnppi.nppiGetResizeRect(inrect, ctypes.byref(outrect), nXFactor, nYFactor, nXShift, nYShift,\n\t\t\t\t\t\t\t\t\t\tinterpolation)\n\tnppCheckStatus(status)\n\n\treturn outrect.x, outrect.y, outrect.width, outrect.height\n\n\nnppiResizeSqrPixelResType = ctypes.c_int\nnppiResizeSqrPixelArgTypes = [ctypes.c_void_p, NppiSize, ctypes.c_int, NppiRect, ctypes.c_void_p, ctypes.c_int,\n\t\t\t\t\t\t\t  NppiRect, ctypes.c_double, ctypes.c_double, ctypes.c_double, ctypes.c_double,\n\t\t\t\t\t\t\t  ctypes.c_int]\ndef nppiResizeSqrPixel(dataType, memoryType, src, srcSize, nSrcStep, srcROI, dst, nDstStep, dstROI, nXFactor, nYFactor,\n\t\t\t\t\t   nXShift, nYShift, interpolation):\n\tinSize = NppiSize(*srcSize)\n\tinROI, outROI = NppiRect(*srcROI), NppiRect(*dstROI)\n\n\tf = _libnppi[""%s_%s_%s"" % (nppiResizeSqrPixel.__name__, dataType, memoryType)]\n\tf.restype = nppiResizeSqrPixelResType\n\tf.argtypes = nppiResizeSqrPixelArgTypes\n\n\tstatus = f(src, inSize, nSrcStep, inROI, dst, nDstStep, outROI, nXFactor, nYFactor, nXShift, nYShift, interpolation)\n\tnppCheckStatus(status)\n\n\n_libnppi.nppiGetRotateQuad.restype = ctypes.c_int\n_libnppi.nppiGetRotateQuad.argtypes = [NppiRect, ctypes.POINTER(ctypes.c_double), ctypes.c_double, ctypes.c_double,\n\t\t\t\t\t\t\t\t\t   ctypes.c_double]\ndef nppiGetRotateQuad(rect, nAngle, nShiftX, nShiftY):\n\tx, y, width, height = rect\n\tinrect = NppiRect(x, y, width, height)\n\n\taQuad = (ctypes.c_double * 8)()\n\n\tstatus = _libnppi.nppiGetRotateQuad(inrect, aQuad, nAngle, nShiftX, nShiftY)\n\tnppCheckStatus(status)\n\n\treturn list(aQuad)\n\n\n_libnppi.nppiGetRotateBound.restype = ctypes.c_int\n_libnppi.nppiGetRotateBound.argtypes = [NppiRect, ctypes.POINTER(ctypes.c_double), ctypes.c_double, ctypes.c_double,\n\t\t\t\t\t\t\t\t\t\tctypes.c_double]\ndef nppiGetRotateBound(rect, nAngle, nShiftX, nShiftY):\n\tx, y, width, height = rect\n\tinrect = NppiRect(x, y, width, height)\n\n\taBoundingBox = (ctypes.c_double * 4)()\n\n\tstatus = _libnppi.nppiGetRotateBound(inrect, aBoundingBox, nAngle, nShiftX, nShiftY)\n\tnppCheckStatus(status)\n\n\treturn list(aBoundingBox)\n\n\nnppiRotateResType = ctypes.c_int\nnppiRotateArgTypes = [ctypes.c_void_p, NppiSize, ctypes.c_int, NppiRect, ctypes.c_void_p, ctypes.c_int, NppiRect,\n\t\t\t\t\t  ctypes.c_double, ctypes.c_double, ctypes.c_double, ctypes.c_int]\ndef nppiRotate(dataType, memoryType, src, srcSize, nSrcStep, srcROI, dst, nDstStep, dstROI, nAngle, nShiftX, nShiftY,\n\t\t\t   interpolation):\n\tinSize = NppiSize(*srcSize)\n\tinROI, outROI = NppiRect(*srcROI), NppiRect(*dstROI)\n\n\tf = _libnppi[""%s_%s_%s"" % (nppiRotate.__name__, dataType, memoryType)]\n\tf.restype = nppiRotateResType\n\tf.argtypes = nppiRotateArgTypes\n\n\tstatus = f(src, inSize, nSrcStep, inROI, dst, nDstStep, outROI, nAngle, nShiftX, nShiftY, interpolation)\n\tnppCheckStatus(status)\n\n\nnppiWarpAffineResType = ctypes.c_int\nnppiWarpAffineArgTypes = [ctypes.c_void_p, NppiSize, ctypes.c_int, NppiRect, ctypes.c_void_p, ctypes.c_int, NppiRect,\n\t\t\t\t\t\t  ctypes.c_void_p, ctypes.c_int]\ndef nppiWarpAffine(dataType, memoryType, src, srcSize, nSrcStep, srcROI, dst, nDstStep, dstROI, coeffs, interpolation):\n\tinSize = NppiSize(*srcSize)\n\tinROI, outROI = NppiRect(*srcROI), NppiRect(*dstROI)\n\n\tf = _libnppi[""%s_%s_%s"" % (nppiWarpAffine.__name__, dataType, memoryType)]\n\tf.restype = nppiWarpAffineResType\n\tf.argtypes = nppiWarpAffineArgTypes\n\n\taCoeffs = (ctypes.c_double * 6)(*coeffs)\n\n\tstatus = f(src, inSize, nSrcStep, inROI, dst, nDstStep, outROI, aCoeffs, interpolation)\n\tnppCheckStatus(status)\n\n\nnppiWarpAffineBackResType = ctypes.c_int\nnppiWarpAffineBackArgTypes = [ctypes.c_void_p, NppiSize, ctypes.c_int, NppiRect, ctypes.c_void_p, ctypes.c_int,\n\t\t\t\t\t\t\t  NppiRect, ctypes.c_void_p, ctypes.c_int]\ndef nppiWarpAffineBack(dataType, memoryType, src, srcSize, nSrcStep, srcROI, dst, nDstStep, dstROI, coeffs,\n\t\t\t\t\t   interpolation):\n\tinSize = NppiSize(*srcSize)\n\tinROI, outROI = NppiRect(*srcROI), NppiRect(*dstROI)\n\n\tf = _libnppi[""%s_%s_%s"" % (nppiWarpAffineBack.__name__, dataType, memoryType)]\n\tf.restype = nppiWarpAffineBackResType\n\tf.argtypes = nppiWarpAffineBackArgTypes\n\n\taCoeffs = (ctypes.c_double * 6)(*coeffs)\n\n\tstatus = f(src, inSize, nSrcStep, inROI, dst, nDstStep, outROI, aCoeffs, interpolation)\n\tnppCheckStatus(status)\n\n\nnppiWarpAffineQuadResType = ctypes.c_int\nnppiWarpAffineQuadArgTypes = [ctypes.c_void_p, NppiSize, ctypes.c_int, NppiRect, ctypes.c_void_p, ctypes.c_void_p,\n\t\t\t\t\t\t\t  ctypes.c_int, NppiRect, ctypes.c_void_p, ctypes.c_int]\ndef nppiWarpAffineQuad(dataType, memoryType, src, srcSize, nSrcStep, srcROI, srcQuad, dst, nDstStep, dstROI, dstQuad,\n\t\t\t\t\t   interpolation):\n\tinSize = NppiSize(*srcSize)\n\tinROI, outROI = NppiRect(*srcROI), NppiRect(*dstROI)\n\n\tf = _libnppi[""%s_%s_%s"" % (nppiWarpAffineQuad.__name__, dataType, memoryType)]\n\tf.restype = nppiWarpAffineQuadResType\n\tf.argtypes = nppiWarpAffineQuadArgTypes\n\n\taSrcQuad = (ctypes.c_double * 8)(*srcQuad)\n\taDstQuad = (ctypes.c_double * 8)(*dstQuad)\n\n\tstatus = f(src, inSize, nSrcStep, inROI, aSrcQuad, dst, nDstStep, outROI, aDstQuad, interpolation)\n\tnppCheckStatus(status)\n\n\n_libnppi.nppiGetAffineTransform.restype = ctypes.c_int\n_libnppi.nppiGetAffineTransform.argtypes = [NppiRect, ctypes.c_void_p, ctypes.c_void_p]\ndef nppiGetAffineTransform(srcROI, quad):\n\tinROI = NppiRect(*srcROI)\n\taQuad = (ctypes.c_double * 8)(*quad)\n\tcoeffs = (ctypes.c_double * 6)()\n\n\tstatus = _libnppi.nppiGetAffineTransform(inROI, aQuad, coeffs)\n\tnppCheckStatus(status)\n\n\treturn list(coeffs)\n'"
Cuda/Wrappers/CuBlas.py,59,"b'import numpy as np\n\n\ndef unittest():\n\tfrom PuzzleLib.Cuda import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tbnd = Backend.getBackend(deviceIdx, initmode=1)\n\n\t\tvectorTest(bnd)\n\n\t\tfor dtype, atol in bnd.dtypesSupported():\n\t\t\tmatrixTest(bnd, dtype, atol)\n\t\t\tgbpGbpTest(bnd, dtype, atol)\n\t\t\tgbpBgpTest(bnd, dtype, atol)\n\t\t\tbgpGbpTest(bnd, dtype, atol)\n\t\t\tbgpBgpTest(bnd, dtype, atol)\n\n\ndef vectorTest(bnd):\n\thostX, hostY = np.random.randn(5).astype(np.float32), np.random.randn(5).astype(np.float32)\n\tx, y = bnd.GPUArray.toGpu(hostX), bnd.GPUArray.toGpu(hostY)\n\n\tassert np.isclose(bnd.blas.dot(x, y), np.dot(hostX, hostY))\n\tassert np.isclose(bnd.blas.l1norm(x), np.linalg.norm(hostX, ord=1))\n\tassert np.isclose(bnd.blas.l2norm(x), np.linalg.norm(hostX, ord=2))\n\n\ndef matrixTest(bnd, dtype, atol):\n\thostA, hostB = np.random.randn(5, 3).astype(dtype), np.random.randn(3, 4).astype(dtype)\n\tA, B = bnd.GPUArray.toGpu(hostA), bnd.GPUArray.toGpu(hostB)\n\n\tC = bnd.blas.gemm(A, B)\n\thostC = C.get()\n\n\tassert np.allclose(np.dot(hostA, hostB), hostC)\n\n\tD = bnd.blas.gemm(B, C, transpB=True)\n\thostD = D.get()\n\n\tassert np.allclose(np.dot(hostB, hostC.T), hostD)\n\n\tE = bnd.blas.gemm(D, B, transpA=True)\n\tassert np.allclose(np.dot(hostD.T, hostB), E.get(), atol=atol)\n\n\ndef gbpGbpTest(bnd, dtype, atol):\n\tformatA, formatB, formatOut = bnd.GroupFormat.gbp.value, bnd.GroupFormat.gbp.value, bnd.GroupFormat.gbp.value\n\tgroups = 3\n\n\thostA = np.random.randn(groups, 4, 3).astype(dtype)\n\thostB = np.random.randn(groups, hostA.shape[2], 5).astype(dtype)\n\thostC = np.random.randn(groups, hostA.shape[1], 6).astype(dtype)\n\thostD = np.random.randn(groups, 8, hostC.shape[2]).astype(dtype)\n\n\tA, B = bnd.GPUArray.toGpu(hostA), bnd.GPUArray.toGpu(hostB)\n\tC, D = bnd.GPUArray.toGpu(hostC), bnd.GPUArray.toGpu(hostD)\n\tout = bnd.blas.gemmBatched(A, B, formatA=formatA, formatB=formatB, formatOut=formatOut)\n\n\thostOut = np.empty(out.shape, dtype=dtype)\n\tfor i in range(groups):\n\t\tnp.dot(hostA[i], hostB[i], out=hostOut[i])\n\n\tassert np.allclose(hostOut, out.get(), atol=atol)\n\n\tout = bnd.blas.gemmBatched(C, A, formatA=formatA, formatB=formatB, formatOut=formatOut, transpA=True)\n\n\thostOut = np.empty(out.shape, dtype=dtype)\n\tfor i in range(groups):\n\t\tnp.dot(hostC[i].T, hostA[i], out=hostOut[i])\n\n\tassert np.allclose(hostOut, out.get(), atol=atol)\n\n\tout = bnd.blas.gemmBatched(C, D, formatA=formatA, formatB=formatB, formatOut=formatOut, transpB=True)\n\n\thostOut = np.empty(out.shape, dtype=dtype)\n\tfor i in range(groups):\n\t\tnp.dot(hostC[i], hostD[i].T, out=hostOut[i])\n\n\tassert np.allclose(hostOut, out.get(), atol=atol)\n\n\ndef gbpBgpTest(bnd, dtype, atol):\n\tformatA, formatB, formatOut = bnd.GroupFormat.gbp.value, bnd.GroupFormat.bgp.value, bnd.GroupFormat.bgp.value\n\tgroups = 3\n\n\thostA = np.random.randn(groups, 4, 7).astype(dtype)\n\thostB = np.random.randn(hostA.shape[2], groups, 5).astype(dtype)\n\thostC = np.random.randn(hostA.shape[1], groups, 8).astype(dtype)\n\thostD = np.random.randn(6, groups, hostA.shape[2]).astype(dtype)\n\n\tA, B = bnd.GPUArray.toGpu(hostA), bnd.GPUArray.toGpu(hostB)\n\tC, D = bnd.GPUArray.toGpu(hostC), bnd.GPUArray.toGpu(hostD)\n\tout = bnd.blas.gemmBatched(A, B, formatA=formatA, formatB=formatB, formatOut=formatOut)\n\n\thostOut = np.empty(out.shape, dtype=dtype)\n\tfor i in range(groups):\n\t\thostOut[:, i, :] = np.dot(hostA[i], hostB[:, i, :])\n\n\tassert np.allclose(hostOut, out.get(), atol=atol)\n\n\tout = bnd.blas.gemmBatched(A, C, formatA=formatA, formatB=formatB, formatOut=formatOut, transpA=True)\n\n\thostOut = np.empty(out.shape, dtype=dtype)\n\tfor i in range(groups):\n\t\thostOut[:, i, :] = np.dot(hostA[i].T, hostC[:, i, :])\n\n\tassert np.allclose(hostOut, out.get(), atol=atol)\n\n\tout = bnd.blas.gemmBatched(A, D, formatA=formatA, formatB=formatB, formatOut=formatOut, transpB=True)\n\n\thostOut = np.empty(out.shape, dtype=dtype)\n\tfor i in range(groups):\n\t\thostOut[:, i, :] = np.dot(hostA[i], hostD[:, i, :].T)\n\n\tassert np.allclose(hostOut, out.get(), atol=atol)\n\n\ndef bgpGbpTest(bnd, dtype, atol):\n\tformatA, formatB, formatOut = bnd.GroupFormat.bgp.value, bnd.GroupFormat.gbp.value, bnd.GroupFormat.bgp.value\n\tgroups = 3\n\n\thostA = np.random.randn(4, groups, 7).astype(dtype)\n\thostB = np.random.randn(groups, hostA.shape[2], 5).astype(dtype)\n\thostC = np.random.randn(groups, hostA.shape[0], 8).astype(dtype)\n\thostD = np.random.randn(groups, 6, hostA.shape[2]).astype(dtype)\n\n\tA, B = bnd.GPUArray.toGpu(hostA), bnd.GPUArray.toGpu(hostB)\n\tC, D = bnd.GPUArray.toGpu(hostC), bnd.GPUArray.toGpu(hostD)\n\tout = bnd.blas.gemmBatched(A, B, formatA=formatA, formatB=formatB, formatOut=formatOut)\n\n\thostOut = np.empty(out.shape, dtype=dtype)\n\tfor i in range(groups):\n\t\thostOut[:, i, :] = np.dot(hostA[:, i, :], hostB[i])\n\n\tassert np.allclose(hostOut, out.get(), atol=atol)\n\n\tout = bnd.blas.gemmBatched(A, C, formatA=formatA, formatB=formatB, formatOut=formatOut, transpA=True)\n\n\thostOut = np.empty(out.shape, dtype=dtype)\n\tfor i in range(groups):\n\t\thostOut[:, i, :] = np.dot(hostA[:, i, :].T, hostC[i])\n\n\tassert np.allclose(hostOut, out.get(), atol=atol)\n\n\tout = bnd.blas.gemmBatched(A, D, formatA=formatA, formatB=formatB, formatOut=formatOut, transpB=True)\n\n\thostOut = np.empty(out.shape, dtype=dtype)\n\tfor i in range(groups):\n\t\thostOut[:, i, :] = np.dot(hostA[:, i, :], hostD[i].T)\n\n\tassert np.allclose(hostOut, out.get(), atol=atol)\n\n\ndef bgpBgpTest(bnd, dtype, atol):\n\tformatA, formatB, formatOut = bnd.GroupFormat.bgp.value, bnd.GroupFormat.bgp.value, bnd.GroupFormat.gbp.value\n\tgroups = 3\n\n\thostA = np.random.randn(4, groups, 7).astype(dtype)\n\thostB = np.random.randn(hostA.shape[2], groups, 5).astype(dtype)\n\thostC = np.random.randn(hostA.shape[0], groups, hostB.shape[2]).astype(dtype)\n\n\tA, B, C = bnd.GPUArray.toGpu(hostA), bnd.GPUArray.toGpu(hostB), bnd.GPUArray.toGpu(hostC)\n\tout = bnd.blas.gemmBatched(A, B, formatA=formatA, formatB=formatB, formatOut=formatOut)\n\n\thostOut = np.empty(out.shape, dtype=dtype)\n\tfor i in range(groups):\n\t\tnp.dot(hostA[:, i, :], hostB[:, i, :], out=hostOut[i])\n\n\tassert np.allclose(hostOut, out.get(), atol=atol)\n\n\tout = bnd.blas.gemmBatched(A, C, formatA=formatA, formatB=formatB, formatOut=formatOut, transpA=True)\n\n\thostOut = np.empty(out.shape, dtype=dtype)\n\tfor i in range(groups):\n\t\tnp.dot(hostA[:, i, :].T, hostC[:, i, :], out=hostOut[i])\n\n\tassert np.allclose(hostOut, out.get(), atol=atol)\n\n\tout = bnd.blas.gemmBatched(B, C, formatA=formatA, formatB=formatB, formatOut=formatOut, transpB=True)\n\n\thostOut = np.empty(out.shape, dtype=dtype)\n\tfor i in range(groups):\n\t\tnp.dot(hostB[:, i, :], hostC[:, i, :].T, out=hostOut[i])\n\n\tassert np.allclose(hostOut, out.get(), atol=atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cuda/Wrappers/CuDnn.py,105,"b'import itertools\nimport numpy as np\n\n\ndef unittest():\n\tfrom PuzzleLib.Cuda import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tbnd = Backend.getBackend(deviceIdx, initmode=1)\n\n\t\tfor dtype, atol in bnd.dtypesSupported():\n\t\t\tconv2dTest(bnd, dtype, atol)\n\t\t\tconv3dTest(bnd, dtype, atol)\n\t\t\tconvGroupTest(bnd, dtype, atol)\n\n\t\t\tdeconv2dTest(bnd, dtype, atol)\n\t\t\tdeconv3dTest(bnd, dtype, atol)\n\t\t\tdeconvGroupTest(bnd, dtype, atol)\n\n\t\t\tmaxpool2dTest(bnd, dtype, atol)\n\t\t\tmaxpool3dTest(bnd, dtype, atol)\n\n\t\t\tsoftmax2dTest(bnd, dtype, atol)\n\n\ndef conv2dTest(bnd, dtype, atol):\n\tbatchsize, inmaps, h, w = 1, 2, 6, 6\n\toutmaps, fsize, stride = 4, 2, 2\n\n\thostData = np.random.randn(batchsize, inmaps, h, w).astype(dtype)\n\thostW = np.random.randn(outmaps, inmaps, fsize, fsize).astype(dtype)\n\thostBias = np.random.randn(outmaps).astype(dtype)\n\n\tdata, W, bias = bnd.GPUArray.toGpu(hostData), bnd.GPUArray.toGpu(hostW), bnd.GPUArray.toGpu(hostBias)\n\toutdata = bnd.dnn.convNd(data, W, bias, stride=stride)\n\n\thostOutData = np.zeros(outdata.shape, dtype=np.float32)\n\n\tfor b, oc, ic, y, x, dy, dx in itertools.product(\n\t\trange(batchsize), range(outmaps), range(inmaps), range(outdata.shape[2]), range(outdata.shape[3]),\n\t\trange(fsize), range(fsize)\n\t):\n\t\thostOutData[b, oc, y, x] += hostData[b, ic, y * stride + dy, x * stride + dx] * hostW[oc, ic, dy, dx]\n\n\thostOutData = (hostOutData + hostBias[np.newaxis, :, np.newaxis, np.newaxis]).astype(dtype)\n\tassert np.allclose(hostOutData, outdata.get(), atol=atol)\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad = bnd.GPUArray.toGpu(hostGrad)\n\tingrad = bnd.dnn.convNdBackwardData(grad, W, data=data, stride=stride)\n\n\thostInGrad = np.zeros(data.shape).astype(np.float32)\n\n\tfor b, ic, oc, y, x, dy, dx in itertools.product(\n\t\trange(batchsize), range(inmaps), range(outmaps), range(hostGrad.shape[2]), range(hostGrad.shape[3]),\n\t\trange(fsize), range(fsize)\n\t):\n\t\thostInGrad[b, ic, y * stride + dy, x * stride + dx] += hostW[oc, ic, dy, dx] * hostGrad[b, oc, y, x]\n\n\thostInGrad = hostInGrad.astype(dtype)\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\n\twgrad, bgrad = bnd.dnn.convNdBackwardParams(data, grad, W, stride=stride, withbias=True)\n\thostWGrad = np.zeros(wgrad.shape, dtype=np.float32)\n\n\tfor b, oc, ic, dy, dx, y, x in itertools.product(\n\t\trange(batchsize), range(outmaps), range(inmaps), range(fsize), range(fsize),\n\t\trange(hostGrad.shape[2]), range(hostGrad.shape[3])\n\t):\n\t\thostWGrad[oc, ic, dy, dx] += hostData[b, ic, y * stride + dy, x * stride + dx] * hostGrad[b, oc, y, x]\n\n\thostWGrad = hostWGrad.astype(dtype)\n\tassert np.allclose(hostWGrad, wgrad.get(), atol=atol)\n\n\thostBiasGrad = np.sum(hostGrad, axis=(0, 2, 3), dtype=np.float32).astype(dtype)\n\tassert np.allclose(hostBiasGrad, bgrad.get())\n\n\ndef conv3dTest(bnd, dtype, atol):\n\tbatchsize, inmaps, d, h, w = 1, 2, 4, 4, 4\n\toutmaps, fsize, s = 3, 2, 2\n\n\thostData = np.random.randn(batchsize, inmaps, d, h, w).astype(dtype)\n\thostW = np.random.randn(outmaps, inmaps, fsize, fsize, fsize).astype(dtype)\n\thostBias = np.random.randn(outmaps).astype(dtype)\n\n\tdata, W, bias = bnd.GPUArray.toGpu(hostData), bnd.GPUArray.toGpu(hostW), bnd.GPUArray.toGpu(hostBias)\n\toutdata = bnd.dnn.convNd(data, W, bias, stride=s)\n\n\thostOutData = np.zeros(outdata.shape, dtype=np.float32)\n\n\tfor b, oc, ic, z, y, x, dz, dy, dx in itertools.product(\n\t\trange(batchsize), range(outmaps), range(inmaps), range(outdata.shape[2]),\n\t\trange(outdata.shape[3]), range(outdata.shape[4]), range(fsize), range(fsize), range(fsize)\n\t):\n\t\thostOutData[b, oc, z, y, x] += hostData[b, ic, z * s + dz, y * s + dy, x * s + dx] * hostW[oc, ic, dz, dy, dx]\n\n\thostOutData = (hostOutData + hostBias[np.newaxis, :, np.newaxis, np.newaxis, np.newaxis]).astype(dtype)\n\tassert np.allclose(hostOutData, outdata.get(), atol=atol)\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad = bnd.GPUArray.toGpu(hostGrad)\n\tingrad = bnd.dnn.convNdBackwardData(grad, W, data=data, stride=s)\n\n\thostInGrad = np.zeros(data.shape).astype(np.float32)\n\n\tfor b, ic, oc, z, y, x, dz, dy, dx in itertools.product(\n\t\trange(batchsize), range(inmaps), range(outmaps), range(hostGrad.shape[2]),\n\t\trange(hostGrad.shape[3]), range(hostGrad.shape[4]), range(fsize), range(fsize), range(fsize)\n\t):\n\t\thostInGrad[b, ic, z * s + dz, y * s + dy, x * s + dx] += hostW[oc, ic, dz, dy, dx] * hostGrad[b, oc, z, y, x]\n\n\thostInGrad = hostInGrad.astype(dtype)\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\n\twgrad, bgrad = bnd.dnn.convNdBackwardParams(data, grad, W, stride=s, withbias=True)\n\thostWGrad = np.zeros(wgrad.shape, dtype=np.float32)\n\n\tfor b, oc, ic, dz, dy, dx, z, y, x in itertools.product(\n\t\trange(batchsize), range(outmaps), range(inmaps), range(fsize), range(fsize), range(fsize),\n\t\trange(hostGrad.shape[2]), range(hostGrad.shape[3]), range(hostGrad.shape[4])\n\t):\n\t\thostWGrad[oc, ic, dz, dy, dx] += hostData[b, ic, z * s + dz, y * s + dy, x * s + dx] * hostGrad[b, oc, z, y, x]\n\n\thostWGrad = hostWGrad.astype(dtype)\n\tassert np.allclose(hostWGrad, wgrad.get(), atol=atol)\n\n\thostBiasGrad = np.sum(hostGrad, axis=(0, 2, 3, 4), dtype=np.float32).astype(dtype)\n\tassert np.allclose(hostBiasGrad, bgrad.get())\n\n\ndef convGroupTest(bnd, dtype, atol):\n\tbatchsize, inmaps, h, w = 5, 6, 3, 4\n\toutmaps, groups, fsize = 4, 2, 2\n\n\thostData = np.random.randn(batchsize, inmaps, h, w).astype(dtype)\n\thostW = np.random.randn(outmaps, inmaps // groups, fsize, fsize).astype(dtype)\n\thostBias = np.random.randn(outmaps).astype(dtype)\n\n\tdata, W, bias = bnd.GPUArray.toGpu(hostData), bnd.GPUArray.toGpu(hostW), bnd.GPUArray.toGpu(hostBias)\n\toutdata = bnd.dnn.convNd(data, W, bias, groups=groups)\n\n\thostOutData = np.zeros(outdata.shape, dtype=np.float32)\n\tingroup, outgroup = inmaps // groups, outmaps // groups\n\n\tfor g in range(groups):\n\t\thostOutGroup = hostOutData[:, g * outgroup:(g + 1) * outgroup, :, :]\n\t\thostGroup = hostData[:, g * ingroup:(g + 1) * ingroup, :, :]\n\n\t\tfor b, oc, ic, y, x, dy, dx in itertools.product(\n\t\t\trange(batchsize), range(outgroup), range(ingroup), range(outdata.shape[2]), range(outdata.shape[3]),\n\t\t\trange(fsize), range(fsize)\n\t\t):\n\t\t\thostOutGroup[b, oc, y, x] += hostGroup[b, ic, y + dy, x + dx] * hostW[g * outgroup + oc, ic, dy, dx]\n\n\thostOutData = (hostOutData + hostBias[np.newaxis, :, np.newaxis, np.newaxis]).astype(dtype)\n\tassert np.allclose(hostOutData, outdata.get(), atol=atol)\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad = bnd.GPUArray.toGpu(hostGrad)\n\tingrad = bnd.dnn.convNdBackwardData(grad, W, groups=groups)\n\n\thostInGrad = np.zeros(hostData.shape, dtype=np.float32)\n\n\tfor g in range(groups):\n\t\thostGroup = hostGrad[:, g * outgroup:(g + 1) * outgroup, :, :]\n\t\thostInGroup = hostInGrad[:, g * ingroup:(g + 1) * ingroup, :, :]\n\n\t\tfor b, ic, oc, y, x, dy, dx in itertools.product(\n\t\t\trange(batchsize), range(ingroup), range(outgroup), range(hostGrad.shape[2]), range(hostGrad.shape[3]),\n\t\t\trange(fsize), range(fsize)\n\t\t):\n\t\t\thostInGroup[b, ic, y + dy, x + dx] += hostW[g * outgroup + oc, ic, dy, dx] * hostGroup[b, oc, y, x]\n\n\thostInGrad = hostInGrad.astype(dtype)\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\n\twgrad, bgrad = bnd.dnn.convNdBackwardParams(data, grad, W, groups=groups, withbias=True)\n\thostWGrad = np.zeros(wgrad.shape, dtype=np.float32)\n\n\tfor g in range(groups):\n\t\thostGrGroup = hostGrad[:, g * outgroup:(g + 1) * outgroup, :, :]\n\t\thostDtGroup = hostData[:, g * ingroup:(g + 1) * ingroup, :, :]\n\n\t\tfor b, oc, ic, dy, dx, y, x in itertools.product(\n\t\t\trange(batchsize), range(outgroup), range(ingroup), range(fsize), range(fsize),\n\t\t\trange(hostGrad.shape[2]), range(hostGrad.shape[3])\n\t\t):\n\t\t\thostWGrad[g * outgroup + oc, ic, dy, dx] += hostDtGroup[b, ic, y + dy, x + dx] * hostGrGroup[b, oc, y, x]\n\n\thostWGrad = hostWGrad.astype(dtype)\n\tassert np.allclose(hostWGrad, wgrad.get(), atol=atol)\n\n\thostBiasGrad = np.sum(hostGrad, axis=(0, 2, 3), dtype=np.float32).astype(dtype)\n\tassert np.allclose(hostBiasGrad, bgrad.get())\n\n\ndef deconv2dTest(bnd, dtype, atol):\n\tbatchsize, inmaps, h, w = 1, 1, 2, 2\n\toutmaps, fsize, stride = 1, 3, 2\n\n\thostData = np.random.randn(batchsize, inmaps, h, w).astype(dtype)\n\thostW = np.random.randn(inmaps, outmaps, fsize, fsize).astype(dtype)\n\thostBias = np.random.randn(outmaps).astype(dtype)\n\n\tdata, W, bias = bnd.GPUArray.toGpu(hostData), bnd.GPUArray.toGpu(hostW), bnd.GPUArray.toGpu(hostBias)\n\toutdata = bnd.dnn.convNdBackwardData(data, W, bias, stride=stride)\n\n\thostOutData = np.zeros(outdata.shape).astype(np.float32)\n\n\tfor b, oc, ic, y, x, dy, dx in itertools.product(\n\t\trange(batchsize), range(outmaps), range(inmaps), range(h), range(w), range(fsize), range(fsize)\n\t):\n\t\thostOutData[b, oc, y * stride + dy, x * stride + dx] += hostW[ic, oc, dy, dx] * hostData[b, ic, y, x]\n\n\thostOutData = (hostOutData + hostBias[np.newaxis, :, np.newaxis, np.newaxis]).astype(dtype)\n\tassert np.allclose(hostOutData, outdata.get(), atol=atol)\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad = bnd.GPUArray.toGpu(hostGrad)\n\tingrad = bnd.dnn.convNd(grad, W, stride=stride)\n\n\thostInGrad = np.zeros(data.shape, dtype=np.float32)\n\n\tfor b, ic, oc, y, x, dy, dx in itertools.product(\n\t\trange(batchsize), range(inmaps), range(outmaps), range(h), range(w), range(fsize), range(fsize)\n\t):\n\t\thostInGrad[b, ic, y, x] += hostGrad[b, oc, y * stride + dy, x * stride + dx] * hostW[ic, oc, dy, dx]\n\n\thostInGrad = hostInGrad.astype(dtype)\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\n\twgrad, bgrad = bnd.dnn.convNdBackwardParams(grad, data, W, stride=stride, withbias=True, deconv=True)\n\thostWGrad = np.zeros(wgrad.shape, dtype=np.float32)\n\n\tfor b, ic, oc, dy, dx, y, x in itertools.product(\n\t\trange(batchsize), range(inmaps), range(outmaps), range(fsize), range(fsize), range(h), range(w)\n\t):\n\t\thostWGrad[ic, oc, dy, dx] += hostGrad[b, oc, y * stride + dy, x * stride + dx] * hostData[b, ic, y, x]\n\n\thostWGrad = hostWGrad.astype(dtype)\n\tassert np.allclose(hostWGrad, wgrad.get(), atol=atol)\n\n\thostBiasGrad = np.sum(hostGrad, axis=(0, 2, 3), dtype=np.float32).astype(dtype)\n\tassert np.allclose(hostBiasGrad, bgrad.get())\n\n\ndef deconv3dTest(bnd, dtype, atol):\n\tbatchsize, inmaps, d, h, w = 1, 2, 2, 2, 2\n\toutmaps, fsize, s = 2, 2, 2\n\n\thostData = np.random.randn(batchsize, inmaps, d, h, w).astype(dtype)\n\thostW = np.random.randn(inmaps, outmaps, fsize, fsize, fsize).astype(dtype)\n\thostBias = np.random.randn(outmaps).astype(dtype)\n\n\tdata, W, bias = bnd.GPUArray.toGpu(hostData), bnd.GPUArray.toGpu(hostW), bnd.GPUArray.toGpu(hostBias)\n\toutdata = bnd.dnn.convNdBackwardData(data, W, bias, stride=s)\n\n\thostOutData = np.zeros(outdata.shape, dtype=np.float32)\n\n\tfor b, oc, ic, z, y, x, dz, dy, dx in itertools.product(\n\t\trange(batchsize), range(outmaps), range(inmaps), range(d), range(h), range(w),\n\t\trange(fsize), range(fsize), range(fsize)\n\t):\n\t\thostOutData[b, oc, z * s + dz, y * s + dy, x * s + dx] += hostW[ic, oc, dz, dy, dx] * hostData[b, ic, z, y, x]\n\n\thostOutData = (hostOutData + hostBias[np.newaxis, :, np.newaxis, np.newaxis, np.newaxis]).astype(dtype)\n\tassert np.allclose(hostOutData, outdata.get(), atol=atol)\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad = bnd.GPUArray.toGpu(hostGrad)\n\tingrad = bnd.dnn.convNd(grad, W, stride=s)\n\n\thostInGrad = np.zeros(data.shape, dtype=np.float32)\n\n\tfor b, ic, oc, z, y, x, dz, dy, dx in itertools.product(\n\t\trange(batchsize), range(inmaps), range(outmaps), range(d), range(h), range(w),\n\t\trange(fsize), range(fsize), range(fsize)\n\t):\n\t\thostInGrad[b, ic, z, y, x] += hostGrad[b, oc, z * s + dz, y * s + dy, x * s + dx] * hostW[ic, oc, dz, dy, dx]\n\n\thostInGrad = hostInGrad.astype(dtype)\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\n\twgrad, bgrad = bnd.dnn.convNdBackwardParams(grad, data, W, stride=s, withbias=True, deconv=True)\n\thostWGrad = np.zeros(wgrad.shape, dtype=np.float32)\n\n\tfor b, ic, oc, dz, dy, dx, z, y, x in itertools.product(\n\t\trange(batchsize), range(inmaps), range(outmaps), range(fsize), range(fsize), range(fsize),\n\t\trange(d), range(h), range(w)\n\t):\n\t\thostWGrad[ic, oc, dz, dy, dx] += hostGrad[b, oc, z * s + dz, y * s + dy, x * s + dx] * hostData[b, ic, z, y, x]\n\n\thostWGrad = hostWGrad.astype(dtype)\n\tassert np.allclose(hostWGrad, wgrad.get(), atol=atol)\n\n\thostBiasGrad = np.sum(hostGrad, axis=(0, 2, 3, 4), dtype=np.float32).astype(dtype)\n\tassert np.allclose(hostBiasGrad, bgrad.get())\n\n\ndef deconvGroupTest(bnd, dtype, atol):\n\tbatchsize, inmaps, h, w = 3, 4, 3, 4\n\toutmaps, groups, fsize = 4, 2, 2\n\n\thostData = np.random.randn(batchsize, inmaps, h, w).astype(dtype)\n\thostW = np.random.randn(inmaps, outmaps // groups, fsize, fsize).astype(dtype)\n\thostBias = np.random.randn(outmaps).astype(dtype)\n\n\tdata, W, bias = bnd.GPUArray.toGpu(hostData), bnd.GPUArray.toGpu(hostW), bnd.GPUArray.toGpu(hostBias)\n\toutdata = bnd.dnn.convNdBackwardData(data, W, bias, groups=groups)\n\n\thostOutData = np.zeros(outdata.shape, dtype=np.float32)\n\tingroup, outgroup = inmaps // groups, outmaps // groups\n\n\tfor g in range(groups):\n\t\thostOutGroup = hostOutData[:, g * outgroup:(g + 1) * outgroup, :, :]\n\t\thostGroup = hostData[:, g * ingroup:(g + 1) * ingroup, :, :]\n\n\t\tfor b, oc, ic, y, x, dy, dx in itertools.product(\n\t\t\trange(batchsize), range(outgroup), range(ingroup), range(data.shape[2]), range(data.shape[3]),\n\t\t\trange(fsize), range(fsize)\n\t\t):\n\t\t\thostOutGroup[b, oc, y + dy, x + dx] += hostW[g * ingroup + ic, oc, dy, dx] * hostGroup[b, ic, y, x]\n\n\thostOutData = (hostOutData + hostBias[np.newaxis, :, np.newaxis, np.newaxis]).astype(dtype)\n\tassert np.allclose(hostOutData, outdata.get(), atol=atol)\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad = bnd.GPUArray.toGpu(hostGrad)\n\tingrad = bnd.dnn.convNd(grad, W, groups=groups)\n\n\thostInGrad = np.zeros(hostData.shape, dtype=np.float32)\n\n\tfor g in range(groups):\n\t\thostGroup = hostGrad[:, g * outgroup:(g + 1) * outgroup, :, :]\n\t\thostInGroup = hostInGrad[:, g * ingroup:(g + 1) * ingroup, :, :]\n\n\t\tfor b, ic, oc, y, x, dy, dx in itertools.product(\n\t\t\trange(batchsize), range(ingroup), range(outgroup), range(hostInGrad.shape[2]), range(hostInGrad.shape[3]),\n\t\t\trange(fsize), range(fsize)\n\t\t):\n\t\t\thostInGroup[b, ic, y, x] += hostGroup[b, oc, y + dy, x + dx] * hostW[g * ingroup + ic, oc, dy, dx]\n\n\thostInGrad = hostInGrad.astype(dtype)\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\n\twgrad, bgrad = bnd.dnn.convNdBackwardParams(grad, data, W, groups=groups, withbias=True, deconv=True)\n\thostWGrad = np.zeros(wgrad.shape, dtype=np.float32)\n\n\tfor g in range(groups):\n\t\thostGrGroup = hostGrad[:, g * outgroup:(g + 1) * outgroup, :, :]\n\t\thostDtGroup = hostData[:, g * ingroup:(g + 1) * ingroup, :, :]\n\n\t\tfor b, oc, ic, dy, dx, y, x in itertools.product(\n\t\t\trange(batchsize), range(outgroup), range(ingroup), range(fsize), range(fsize),\n\t\t\trange(hostData.shape[2]), range(hostData.shape[3])\n\t\t):\n\t\t\thostWGrad[g * ingroup + ic, oc, dy, dx] += hostDtGroup[b, ic, y, x] * hostGrGroup[b, oc, y + dy, x + dx]\n\n\thostWGrad = hostWGrad.astype(dtype)\n\tassert np.allclose(hostWGrad, wgrad.get(), atol=atol)\n\n\thostBiasGrad = np.sum(hostGrad, axis=(0, 2, 3), dtype=np.float32).astype(dtype)\n\tassert np.allclose(hostBiasGrad, bgrad.get())\n\n\ndef maxpool2dTest(bnd, dtype, atol):\n\tbatchsize, maps, h, w = 3, 2, 6, 6\n\tsize, stride, pad = 3, 2, 1\n\n\thostData = np.full(shape=(batchsize, maps, h + 2 * pad, w + 2 * pad), fill_value=np.finfo(dtype).min, dtype=dtype)\n\thostData[:, :, pad:-pad, pad:-pad] = np.random.randn(batchsize, maps, h, w).astype(dtype)\n\n\tdata = bnd.GPUArray.toGpu(hostData[:, :, pad:-pad, pad:-pad])\n\toutdata = bnd.dnn.poolNd(data, size=size, stride=stride, pad=pad, mode=bnd.PoolMode.max.value)\n\n\thostOutData = np.empty(outdata.shape, dtype=dtype)\n\n\tfor b, c, y, x in itertools.product(\n\t\trange(batchsize), range(maps), range(hostOutData.shape[2]), range(hostOutData.shape[3])\n\t):\n\t\thostOutData[b, c, y, x] = np.max(hostData[b, c, y * stride:y * stride + size, x * stride:x * stride + size])\n\n\tassert np.allclose(hostOutData, outdata.get(), atol=atol)\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad = bnd.GPUArray.toGpu(hostGrad)\n\tingrad = bnd.dnn.poolNdBackward(grad, data, outdata, size=size, stride=stride, pad=pad, mode=bnd.PoolMode.max.value)\n\n\thostInGrad = np.zeros(hostData.shape, dtype=dtype)\n\n\tfor b, c, y, x, dy, dx in itertools.product(\n\t\trange(batchsize), range(maps), range(hostOutData.shape[2]), range(hostOutData.shape[3]),\n\t\trange(size), range(size)\n\t):\n\t\tif hostData[b, c, y * stride + dy, x * stride + dx] == hostOutData[b, c, y, x]:\n\t\t\thostInGrad[b, c, y * stride + dy, x * stride + dx] += hostGrad[b, c, y, x]\n\n\thostInGrad = hostInGrad[:, :, pad:-pad, pad:-pad].astype(dtype)\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\n\ndef maxpool3dTest(bnd, dtype, atol):\n\tbatchsize, maps, d, h, w = 1, 1, 6, 6, 6\n\tsize, s, pad = 3, 2, 1\n\n\thostData = np.full(\n\t\tshape=(batchsize, maps, d + 2 * pad, h + 2 * pad, w + 2 * pad), fill_value=np.finfo(dtype).min, dtype=dtype\n\t)\n\thostData[:, :, pad:-pad, pad:-pad, pad:-pad] = np.random.randn(batchsize, maps, d, h, w).astype(dtype)\n\n\tdata = bnd.GPUArray.toGpu(np.ascontiguousarray(hostData[:, :, pad:-pad, pad:-pad, pad:-pad]))\n\toutdata = bnd.dnn.poolNd(data, size=size, stride=s, pad=pad, mode=bnd.PoolMode.max.value)\n\n\thostOutData = np.empty(outdata.shape, dtype=dtype)\n\n\tfor b, c, z, y, x in itertools.product(\n\t\trange(batchsize), range(maps),\n\t\trange(hostOutData.shape[2]), range(hostOutData.shape[3]), range(hostOutData.shape[4])\n\t):\n\t\thostOutData[b, c, z, y, x] = np.max(hostData[b, c, z * s:z * s + size, y * s:y * s + size, x * s:x * s + size])\n\n\tassert np.allclose(hostOutData, outdata.get())\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad = bnd.GPUArray.toGpu(hostGrad)\n\tingrad = bnd.dnn.poolNdBackward(grad, data, outdata, size=size, stride=s, pad=pad, mode=bnd.PoolMode.max.value)\n\n\thostInGrad = np.zeros(hostData.shape, dtype=np.float32)\n\n\tfor b, c, z, y, x, dz, dy, dx in itertools.product(\n\t\trange(batchsize), range(maps),\n\t\trange(hostOutData.shape[2]), range(hostOutData.shape[3]), range(hostOutData.shape[4]),\n\t\trange(size), range(size), range(size)\n\t):\n\t\tif hostData[b, c, z * s + dz, y * s + dy, x * s + dx] == hostOutData[b, c, z, y, x]:\n\t\t\thostInGrad[b, c, z * s + dz, y * s + dy, x * s + dx] += hostGrad[b, c, z, y, x]\n\n\thostInGrad = hostInGrad[:, :, pad:-pad, pad:-pad, pad:-pad].astype(dtype)\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\n\ndef softmax2dTest(bnd, dtype, atol):\n\tbatchsize, maps, h, w = 5, 8, 2, 3\n\thostData = np.random.randn(batchsize, maps, h, w).astype(dtype)\n\n\tdata = bnd.GPUArray.toGpu(hostData)\n\toutdata = bnd.dnn.softmaxNd(data)\n\n\tdef hostSoftmax(tensor):\n\t\te = np.exp(tensor - np.amax(tensor))\n\t\treturn e / np.sum(e)\n\n\thostOutData = np.empty(outdata.shape, dtype=dtype)\n\n\tfor b, y, x in itertools.product(range(batchsize), range(h), range(w)):\n\t\thostOutData[b, :, y, x] = hostSoftmax(hostData[b, :, y, x])\n\n\tassert np.allclose(hostOutData, outdata.get(), atol=atol)\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad = bnd.GPUArray.toGpu(hostGrad)\n\tingrad = bnd.dnn.softmaxNdBackward(grad, outdata)\n\n\thostInGrad = np.empty(ingrad.shape, dtype=dtype)\n\n\tdef hostSoftmaxBackward(d, gr):\n\t\treturn d * (gr - np.dot(d, gr))\n\n\tfor b, y, x in itertools.product(range(batchsize), range(h), range(w)):\n\t\thostInGrad[b, :, y, x] = hostSoftmaxBackward(hostOutData[b, :, y, x], hostGrad[b, :, y, x])\n\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cuda/Wrappers/CuDnnMemory.py,0,"b'from PuzzleLib.Cuda.Kernels.Memory import transposeTest, moveAxisTest, swapAxesTest, depthConcatTest\n\n\ndef unittest():\n\tfrom PuzzleLib.Cuda import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tbnd = Backend.getBackend(deviceIdx, initmode=1)\n\n\t\tfor dtype, _ in bnd.dtypesSupported():\n\t\t\ttransposeTest(bnd, bnd.dnn, dtype)\n\t\t\tmoveAxisTest(bnd, bnd.dnn, dtype)\n\t\t\tswapAxesTest(bnd, bnd.dnn, dtype)\n\t\t\tdepthConcatTest(bnd, bnd.dnn, dtype)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cuda/Wrappers/CuDnnNorm.py,77,"b'import itertools\nimport numpy as np\n\n\ndef unittest():\n\tfrom PuzzleLib.Cuda import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tbnd = Backend.getBackend(deviceIdx, initmode=2)\n\n\t\tfor dtype, atol in bnd.dtypesSupported():\n\t\t\tbatchNorm2dTest(bnd, dtype, atol, np.float32)\n\t\t\tbatchNorm3dTest(bnd, dtype, atol, np.float32)\n\t\t\tinstanceNorm2dTest(bnd, dtype, atol, np.float32)\n\n\t\t\tmapLRN2dTest(bnd, dtype, atol)\n\t\t\tcrossMapLRN2dTest(bnd, dtype, atol)\n\n\ndef batchNorm2dTest(bnd, dtype, atol, calctype):\n\tbatchsize, maps, h, w = 4, 5, 2, 3\n\tepsilon, norm = 1e-5, batchsize * h * w\n\n\thostData = np.random.randn(batchsize, maps, h, w).astype(dtype)\n\thostScale = np.random.randn(1, maps, 1, 1).astype(calctype)\n\thostBias = np.random.randn(1, maps, 1, 1).astype(calctype)\n\n\tdata = bnd.GPUArray.toGpu(hostData)\n\tscale, bias = bnd.GPUArray.toGpu(hostScale.ravel()), bnd.GPUArray.toGpu(hostBias.ravel())\n\tmean = bnd.GPUArray.zeros(scale.shape, dtype=calctype)\n\tvar = bnd.GPUArray.toGpu(np.ones(scale.shape, dtype=calctype))\n\n\toutdata, savemean, saveinvvar = bnd.dnn.batchNormNd(data, mean, var, scale, bias, epsilon=epsilon, out=data)\n\n\thostMean = np.sum(hostData, axis=(0, 2, 3), dtype=calctype, keepdims=True) / norm\n\n\thostInvVar = np.sum((hostData - hostMean)**2, axis=(0, 2, 3), dtype=calctype, keepdims=True) / norm\n\thostInvVar = 1.0 / np.sqrt(hostInvVar + epsilon)\n\n\thostNormData = (hostData - hostMean) * hostInvVar\n\thostOutData = (hostNormData * hostScale + hostBias).astype(dtype)\n\n\tassert np.allclose(hostMean.ravel(), mean.get(), atol=atol)\n\tassert np.allclose(hostInvVar.ravel(), saveinvvar.get(), atol=atol)\n\tassert np.allclose(hostOutData, outdata.get(), atol=atol)\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad, data = bnd.GPUArray.toGpu(hostGrad), bnd.GPUArray.toGpu(hostData)\n\tingrad, scalegrad, bgrad = bnd.dnn.batchNormNdBackward(grad, data, scale, savemean, saveinvvar, epsilon=epsilon)\n\n\thostScaleGrad = np.sum(hostGrad * hostNormData, axis=(0, 2, 3), dtype=calctype, keepdims=True)\n\thostBiasGrad = np.sum(hostGrad, axis=(0, 2, 3), dtype=calctype, keepdims=True)\n\n\thostMeanGrad = -hostInvVar * hostBiasGrad * hostScale\n\n\thostVarGrad = np.sum(hostGrad * (hostData - hostMean), axis=(0, 2, 3), dtype=calctype, keepdims=True)\n\thostVarGrad = -0.5 * hostVarGrad * hostScale * hostInvVar**3\n\n\thostInGrad = hostGrad * hostScale * hostInvVar + (2 * hostVarGrad * (hostData - hostMean) + hostMeanGrad) / norm\n\thostInGrad = hostInGrad.astype(dtype)\n\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\tassert np.allclose(hostScaleGrad.ravel(), scalegrad.get(), atol=atol)\n\tassert np.allclose(hostBiasGrad.ravel(), bgrad.get(), atol=atol)\n\n\thostMean = np.random.randn(*hostMean.shape).astype(np.float32)\n\thostVar = 1.0 + np.random.randn(*hostInvVar.shape).astype(np.float32)**2\n\n\tmean, var = bnd.GPUArray.toGpu(hostMean.ravel()), bnd.GPUArray.toGpu(hostVar.ravel())\n\toutdata = bnd.dnn.batchNormNd(data, mean, var, scale, bias, test=True)\n\n\thostOutData = ((hostData - hostMean) / np.sqrt(hostVar + epsilon) * hostScale + hostBias).astype(dtype)\n\tassert np.allclose(hostOutData, outdata.get(), atol=atol)\n\n\ndef batchNorm3dTest(bnd, dtype, atol, calctype):\n\tbatchsize, maps, d, h, w = 2, 5, 2, 3, 2\n\tepsilon, norm = 1e-5, batchsize * d * h * w\n\n\thostData = np.random.randn(batchsize, maps, d, h, w).astype(dtype)\n\n\thostScale = np.random.randn(1, maps, 1, 1, 1).astype(calctype)\n\thostBias = np.random.randn(1, maps, 1, 1, 1).astype(calctype)\n\n\tdata = bnd.GPUArray.toGpu(hostData)\n\tscale, bias = bnd.GPUArray.toGpu(hostScale.ravel()), bnd.GPUArray.toGpu(hostBias.ravel())\n\tmean = bnd.GPUArray.zeros(scale.shape, dtype=calctype)\n\tvar = bnd.GPUArray.toGpu(np.ones(scale.shape, dtype=calctype))\n\n\toutdata, savemean, saveinvvar = bnd.dnn.batchNormNd(data, mean, var, scale, bias, epsilon=epsilon, out=data)\n\n\thostMean = np.sum(hostData, axis=(0, 2, 3, 4), dtype=calctype, keepdims=True) / norm\n\n\thostInvVar = np.sum((hostData - hostMean) ** 2, axis=(0, 2, 3, 4), dtype=calctype, keepdims=True) / norm\n\thostInvVar = 1.0 / np.sqrt(hostInvVar + epsilon)\n\n\thostNormData = (hostData - hostMean) * hostInvVar\n\thostOutData = (hostNormData * hostScale + hostBias).astype(dtype)\n\n\tassert np.allclose(hostMean.ravel(), mean.get(), atol=atol)\n\tassert np.allclose(hostInvVar.ravel(), saveinvvar.get(), atol=atol)\n\tassert np.allclose(hostOutData, outdata.get(), atol=atol)\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad, data = bnd.GPUArray.toGpu(hostGrad), bnd.GPUArray.toGpu(hostData)\n\tingrad, scalegrad, biasgrad = bnd.dnn.batchNormNdBackward(grad, data, scale, savemean, saveinvvar, epsilon=epsilon)\n\n\thostScaleGrad = np.sum(hostGrad * hostNormData, axis=(0, 2, 3, 4), dtype=calctype, keepdims=True)\n\thostBiasGrad = np.sum(hostGrad, axis=(0, 2, 3, 4), dtype=calctype, keepdims=True)\n\n\thostMeanGrad = -hostInvVar * hostBiasGrad * hostScale\n\n\thostVarGrad = np.sum(hostGrad * (hostData - hostMean), axis=(0, 2, 3, 4), dtype=calctype, keepdims=True)\n\thostVarGrad = -0.5 * hostVarGrad * hostScale * hostInvVar**3\n\n\thostInGrad = hostGrad * hostScale * hostInvVar + (2 * hostVarGrad * (hostData - hostMean) + hostMeanGrad) / norm\n\thostInGrad = hostInGrad.astype(dtype)\n\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\tassert np.allclose(hostScaleGrad.ravel(), scalegrad.get(), atol=atol)\n\tassert np.allclose(hostBiasGrad.ravel(), biasgrad.get(), atol=atol)\n\n\thostMean = np.random.randn(*hostMean.shape).astype(np.float32)\n\thostVar = 1.0 + np.random.randn(*hostInvVar.shape).astype(np.float32)**2\n\n\tmean, var = bnd.GPUArray.toGpu(hostMean.ravel()), bnd.GPUArray.toGpu(hostVar.ravel())\n\toutdata = bnd.dnn.batchNormNd(data, mean, var, scale, bias, test=True)\n\n\thostOutData = ((hostData - hostMean) / np.sqrt(hostVar + epsilon) * hostScale + hostBias).astype(dtype)\n\tassert np.allclose(hostOutData, outdata.get(), atol=atol)\n\n\ndef instanceNorm2dTest(bnd, dtype, atol, calctype):\n\tbatchsize, maps, h, w = 3, 4, 5, 5\n\tepsilon, norm = 1e-5, h * w\n\n\thostData = np.random.randn(batchsize, maps, h, w).astype(dtype)\n\n\thostScale = np.random.randn(1, maps, 1, 1).astype(calctype)\n\thostBias = np.random.randn(1, maps, 1, 1).astype(calctype)\n\n\tdata = bnd.GPUArray.toGpu(hostData)\n\tscale, bias = bnd.GPUArray.toGpu(hostScale.ravel()), bnd.GPUArray.toGpu(hostBias.ravel())\n\toutdata, savemean, saveinvvar, extscale = bnd.instanceNorm2d(data, scale, bias, epsilon=epsilon)\n\n\thostExtScale, hostExtBias = np.tile(hostScale, (batchsize, 1, 1, 1)), np.tile(hostBias, (batchsize, 1, 1, 1))\n\n\thostMean = np.mean(hostData, axis=(2, 3), keepdims=True)\n\thostInvVar = 1.0 / np.sqrt(np.var(hostData, axis=(2, 3), keepdims=True) + epsilon)\n\n\thostNormData = (hostData - hostMean) * hostInvVar\n\thostOutData = hostNormData * hostExtScale + hostExtBias\n\n\tassert np.allclose(hostMean.ravel(), savemean.get(), atol=atol)\n\tassert np.allclose(hostInvVar.ravel(), saveinvvar.get(), atol=atol)\n\tassert np.allclose(hostOutData, outdata.get(), atol=atol)\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad = bnd.GPUArray.toGpu(hostGrad)\n\tingrad, scalegrad, bgrad = bnd.instanceNorm2dBackward(grad, data, extscale, savemean, saveinvvar, epsilon=epsilon)\n\n\thostScaleGrad = np.sum(hostGrad * hostNormData, axis=(0, 2, 3), dtype=calctype, keepdims=True)\n\thostBiasGrad = np.sum(hostGrad, axis=(0, 2, 3), dtype=calctype, keepdims=True)\n\n\thostScGrad = hostGrad * hostExtScale\n\thostCorrs = np.empty(hostInvVar.shape, dtype=np.float32)\n\n\tfor b, c in itertools.product(range(batchsize), range(maps)):\n\t\thostCorrs[b, c] = np.dot(hostScGrad[b, c].ravel(), hostNormData[b, c].ravel()) / norm\n\n\thostInGrad = (hostScGrad - np.mean(hostScGrad, axis=(2, 3), keepdims=True) - hostCorrs * hostNormData) * hostInvVar\n\thostInGrad = hostInGrad.astype(dtype)\n\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\tassert np.allclose(hostScaleGrad.ravel(), scalegrad.get(), atol=atol)\n\tassert np.allclose(hostBiasGrad.ravel(), bgrad.get(), atol=atol)\n\n\ndef mapLRN2dTest(bnd, dtype, atol):\n\tbatchsize, maps, h, w = 2, 2, 9, 10\n\tN, alpha, beta, K = 5, 1.0, 0.5, 2.0\n\n\tlookBehind = int((N - 1) / 2)\n\tlookAhead = N - lookBehind\n\n\thostData = np.random.randn(batchsize, maps, h, w).astype(dtype)\n\n\tdata = bnd.GPUArray.toGpu(hostData)\n\toutdata = bnd.dnn.mapLRN(data, N=N, alpha=alpha, beta=beta, K=K)\n\n\tnorms = np.empty(hostData.shape, dtype=np.float32)\n\n\tfor b, c, y, x in itertools.product(range(batchsize), range(maps), range(h), range(w)):\n\t\tslcy = slice(max(0, y - lookBehind), min(h, y + lookAhead))\n\t\tslcx = slice(max(0, x - lookBehind), min(w, x + lookAhead))\n\n\t\tslc = hostData[b, c, slcy, slcx].ravel()\n\t\tnorms[b, c, y, x] = K + np.dot(slc, slc) * alpha / N**2\n\n\thostOutData = (hostData / norms**beta).astype(dtype)\n\tassert np.allclose(hostOutData, outdata.get(), atol=atol)\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad = bnd.GPUArray.toGpu(hostGrad)\n\tingrad = bnd.dnn.mapLRNBackward(data, grad, N=N, alpha=alpha, beta=beta, K=K)\n\n\thostInGrad = hostGrad / norms**beta\n\tk = 2.0 * alpha * beta / N**2\n\n\tfor b, c, y, x in itertools.product(range(batchsize), range(maps), range(h), range(w)):\n\t\tslcy = slice(max(0, y - lookBehind), min(h, y + lookAhead))\n\t\tslcx = slice(max(0, x - lookBehind), min(w, x + lookAhead))\n\n\t\tslcdata, slcgrad = hostData[b, c, slcy, slcx].ravel(), hostGrad[b, c, slcy, slcx].ravel()\n\t\tslcnorms = norms[b, c, slcy, slcx].ravel()\n\n\t\thostInGrad[b, c, y, x] -= k * hostData[b, c, y, x] * np.dot(slcgrad, slcdata / slcnorms**(beta + 1))\n\n\thostInGrad = hostInGrad.astype(dtype)\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\n\ndef crossMapLRN2dTest(bnd, dtype, atol):\n\tbatchsize, maps, h, w = 2, 10, 2, 3\n\tN, alpha, beta, K = 5, 1.0, 0.5, 2.0\n\n\tlookBehind = int((N - 1) / 2)\n\tlookAhead = N - lookBehind\n\n\thostData = np.random.randn(batchsize, maps, h, w).astype(dtype)\n\n\tdata = bnd.GPUArray.toGpu(hostData)\n\toutdata = bnd.dnn.crossMapLRN(data, N=N, alpha=alpha, beta=beta, K=K)\n\n\tnorms = np.empty((batchsize, maps, h, w), dtype=np.float32)\n\n\tfor b, c, y, x in itertools.product(range(batchsize), range(maps), range(h), range(w)):\n\t\tslc = hostData[b, max(0, c - lookBehind):min(maps, c + lookAhead), y, x].ravel()\n\t\tnorms[b, c, y, x] = K + np.dot(slc, slc) * alpha / N\n\n\thostOutData = (hostData / norms**beta).astype(dtype)\n\tassert np.allclose(hostOutData, outdata.get(), atol=atol)\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad = bnd.GPUArray.toGpu(hostGrad)\n\tingrad = bnd.dnn.crossMapLRNBackward(data, outdata, grad, N=N, alpha=alpha, beta=beta, K=K)\n\n\thostInGrad = hostGrad / norms**beta\n\tk = 2.0 * alpha * beta / N\n\n\tfor b, c, y, x in itertools.product(range(batchsize), range(maps), range(h), range(w)):\n\t\tslcmaps = slice(max(0, c - lookBehind), min(maps, c + lookAhead))\n\n\t\tslcdata, slcgrad = hostData[b, slcmaps, y, x].ravel(), hostGrad[b, slcmaps, y, x].ravel()\n\t\tslcnorms = norms[b, slcmaps, y, x]\n\n\t\thostInGrad[b, c, y, x] -= k * hostData[b, c, y, x] * np.dot(slcgrad, slcdata / slcnorms**(beta + 1))\n\n\thostInGrad = hostInGrad.astype(dtype)\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cuda/Wrappers/CuDnnRnn.py,140,"b'import numpy as np\n\n\ndef randomWInit(bnd, rnn, W, params):\n\thostParams = []\n\n\tfor layer in params:\n\t\tlayerParams = {}\n\n\t\tfor paramName, param in layer.items():\n\t\t\thostParam = np.random.randn(*param.shape).astype(param.dtype)\n\t\t\tlayerParams[paramName] = hostParam\n\n\t\t\tparam.set(hostParam)\n\n\t\thostParams.append(layerParams)\n\n\tbnd.updateRnnParams(rnn, W, params)\n\treturn hostParams\n\n\ndef unittest():\n\tfrom PuzzleLib.Cuda import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tbnd = Backend.getBackend(deviceIdx, initmode=2)\n\n\t\tfor dtype, atol in bnd.dtypesSupported():\n\t\t\treluTest(bnd, dtype, atol)\n\t\t\ttanhTest(bnd, dtype, atol)\n\t\t\tlstmTest(bnd, dtype, atol)\n\t\t\tgruTest(bnd, dtype, atol)\n\n\ndef reluTest(bnd, dtype, atol):\n\tseqlen, batchsize, insize, hsize = 2, 3, 4, 5\n\trnn, W, params = bnd.createRnn(insize, hsize, dtype, mode=bnd.RNNMode.relu)\n\n\thostParams = randomWInit(bnd, rnn, W, params)[0]\n\n\thostData = np.random.randn(seqlen, batchsize, insize).astype(dtype)\n\thostHidden = np.random.randn(1, batchsize, hsize).astype(dtype)\n\n\tdata, inithidden = bnd.GPUArray.toGpu(hostData), bnd.GPUArray.toGpu(hostHidden)\n\toutdata, trainReserve = rnn.forward(data, W, hidden=inithidden)\n\n\thostOutData = np.zeros((seqlen + 1, batchsize, hsize), dtype=dtype)\n\n\thostOutData[0] = hostHidden\n\tfor d in range(seqlen):\n\t\tres = np.dot(hostData[d], hostParams[""wi""].T) + np.dot(hostOutData[d], hostParams[""ri""].T) + \\\n\t\t\t  hostParams[""bwi""] + hostParams[""bri""]\n\t\thostOutData[d + 1] = (res > 0.0) * res\n\n\tassert np.allclose(hostOutData[1:], outdata.get(), atol=atol)\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad = bnd.GPUArray.toGpu(hostGrad)\n\tingrad, dhx, _ = rnn.backwardData(grad, outdata, W, trainReserve, hidden=inithidden)\n\n\thostAccGrad = np.zeros((seqlen + 1, batchsize, hsize), dtype=dtype)\n\thostInGrad = np.zeros((seqlen, batchsize, insize), dtype=dtype)\n\n\tfor d in range(seqlen):\n\t\tacc = (hostGrad[seqlen - d - 1] + np.dot(hostAccGrad[seqlen - d], hostParams[""ri""])) * \\\n\t\t\t  (hostOutData[seqlen - d] > 0)\n\n\t\thostAccGrad[seqlen - d - 1] = acc\n\t\thostInGrad[seqlen - d - 1] = np.dot(acc, hostParams[""wi""])\n\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\n\tdw = rnn.backwardParams(data, outdata, trainReserve, hidden=inithidden)\n\tdwparams = bnd.acquireRnnParams(rnn, W=dw)\n\n\thostRiGrad = np.zeros(hostParams[""ri""].shape, dtype=dtype)\n\thostWiGrad = np.zeros(hostParams[""wi""].shape, dtype=dtype)\n\thostBiGrad = np.zeros(hostParams[""bwi""].shape, dtype=dtype)\n\n\tfor d in range(seqlen):\n\t\thostRiGrad += np.dot(hostAccGrad[seqlen - d - 1].T, hostOutData[seqlen - d - 1])\n\t\thostWiGrad += np.dot(hostAccGrad[seqlen - d - 1].T, hostData[seqlen - d - 1])\n\t\thostBiGrad += np.sum(hostAccGrad[seqlen - d - 1], axis=0)\n\n\tassert np.allclose(hostRiGrad, dwparams[0][""ri""].get(), atol=atol)\n\tassert np.allclose(hostWiGrad, dwparams[0][""wi""].get(), atol=atol)\n\tassert np.allclose(hostBiGrad, dwparams[0][""bwi""].get(), atol=atol)\n\tassert np.allclose(hostBiGrad, dwparams[0][""bri""].get(), atol=atol)\n\n\thostDhx = np.dot(hostAccGrad[0], hostParams[""ri""])\n\tassert np.allclose(hostDhx, dhx.get(), atol=atol)\n\n\ndef tanhTest(bnd, dtype, atol):\n\tseqlen, batchsize, insize, hsize = 3, 3, 3, 2\n\trnn, W, params = bnd.createRnn(insize, hsize, dtype, mode=bnd.RNNMode.tanh, direction=bnd.DirectionMode.bi)\n\n\thostParams = randomWInit(bnd, rnn, W, params)\n\thostData = np.random.randn(seqlen, batchsize, insize).astype(dtype)\n\n\tdata = bnd.GPUArray.toGpu(hostData)\n\toutdata, trainReserve = rnn.forward(data, W)\n\n\thostOutData = np.zeros((seqlen + 2, batchsize, 2 * hsize), dtype=dtype)\n\n\tfor d in range(seqlen):\n\t\tres = np.dot(hostData[d], hostParams[0][""wi""].T) + \\\n\t\t\t  np.dot(hostOutData[d, :, :hsize], hostParams[0][""ri""].T) + hostParams[0][""bwi""] + hostParams[0][""bri""]\n\t\thostOutData[d + 1, :, :hsize] = np.tanh(res)\n\n\t\tres = np.dot(hostData[seqlen - d - 1], hostParams[1][""wi""].T) + \\\n\t\t\t  np.dot(hostOutData[seqlen + 1 - d, :, hsize:], hostParams[1][""ri""].T) + \\\n\t\t\t  hostParams[1][""bwi""] + hostParams[1][""bri""]\n\t\thostOutData[seqlen - d, :, hsize:] = np.tanh(res)\n\n\tassert np.allclose(hostOutData[1:seqlen + 1], outdata.get(), atol=atol)\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad = bnd.GPUArray.toGpu(hostGrad)\n\tingrad, _, _ = rnn.backwardData(grad, outdata, W, trainReserve)\n\n\thostAccGrad = np.zeros((seqlen + 2, batchsize, 2 * hsize), dtype=dtype)\n\thostInGrad = np.zeros((seqlen, batchsize, insize), dtype=dtype)\n\n\tfor d in range(seqlen):\n\t\tacc = (hostGrad[seqlen - d - 1, :, :hsize] +\n\t\t\t   np.dot(hostAccGrad[seqlen + 1 - d, :, :hsize], hostParams[0][""ri""])) * \\\n\t\t\t  (1.0 - hostOutData[seqlen - d, :, :hsize]**2)\n\n\t\thostAccGrad[seqlen - d, :, :hsize] = acc\n\t\thostInGrad[seqlen - d - 1] += np.dot(acc, hostParams[0][""wi""])\n\n\t\tacc = (hostGrad[d, :, hsize:] + np.dot(hostAccGrad[d, :, hsize:], hostParams[1][""ri""])) * \\\n\t\t\t  (1.0 - hostOutData[d + 1, :, hsize:]**2)\n\n\t\thostAccGrad[d + 1, :, hsize:] = acc\n\t\thostInGrad[d] += np.dot(acc, hostParams[1][""wi""])\n\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\n\tdw = rnn.backwardParams(data, outdata, trainReserve)\n\tdwparams = bnd.acquireRnnParams(rnn, W=dw)\n\n\thostRi0Grad = np.zeros(hostParams[0][""ri""].shape, dtype=dtype)\n\thostRi1Grad = np.zeros(hostParams[1][""ri""].shape, dtype=dtype)\n\thostWi0Grad = np.zeros(hostParams[0][""wi""].shape, dtype=dtype)\n\thostWi1Grad = np.zeros(hostParams[1][""wi""].shape, dtype=dtype)\n\n\thostBi0Grad = np.zeros(hostParams[0][""bwi""].shape, dtype=dtype)\n\thostBi1Grad = np.zeros(hostParams[1][""bwi""].shape, dtype=dtype)\n\n\tfor d in range(seqlen):\n\t\thostRi0Grad += np.dot(hostAccGrad[seqlen - d + 1, :, :hsize].T, hostOutData[seqlen - d, :, :hsize])\n\t\thostWi0Grad += np.dot(hostAccGrad[seqlen - d, :, :hsize].T, hostData[seqlen - d - 1])\n\t\thostRi1Grad += np.dot(hostAccGrad[d, :, hsize:].T, hostOutData[d + 1, :, hsize:])\n\t\thostWi1Grad += np.dot(hostAccGrad[d + 1, :, hsize:].T, hostData[d])\n\n\t\thostBi0Grad += np.sum(hostAccGrad[seqlen - d, :, :hsize], axis=0)\n\t\thostBi1Grad += np.sum(hostAccGrad[d + 1, :, hsize:], axis=0)\n\n\tassert np.allclose(hostRi0Grad, dwparams[0][""ri""].get(), atol=atol)\n\tassert np.allclose(hostWi0Grad, dwparams[0][""wi""].get(), atol=atol)\n\tassert np.allclose(hostRi1Grad, dwparams[1][""ri""].get(), atol=atol)\n\tassert np.allclose(hostWi1Grad, dwparams[1][""wi""].get(), atol=atol)\n\n\tassert np.allclose(hostBi0Grad, dwparams[0][""bwi""].get(), atol=atol)\n\tassert np.allclose(hostBi0Grad, dwparams[0][""bri""].get(), atol=atol)\n\n\tassert np.allclose(hostBi1Grad, dwparams[1][""bwi""].get(), atol=atol)\n\tassert np.allclose(hostBi1Grad, dwparams[1][""bri""].get(), atol=atol)\n\n\ndef lstmTest(bnd, dtype, atol):\n\tseqlen, batchsize, insize, hsize = 4, 2, 4, 2\n\trnn, W, params = bnd.createRnn(insize, hsize, dtype, mode=bnd.RNNMode.lstm)\n\n\thostParams = randomWInit(bnd, rnn, W, params)[0]\n\n\thostData = np.random.randn(seqlen, batchsize, insize).astype(dtype)\n\thostInitHidden = np.random.randn(1, batchsize, hsize).astype(dtype)\n\thostInitCells = np.ones((1, batchsize, hsize), dtype=dtype)\n\n\tdata = bnd.GPUArray.toGpu(hostData)\n\tinithidden, initcells = bnd.GPUArray.toGpu(hostInitHidden), bnd.GPUArray.toGpu(hostInitCells)\n\n\toutdata, trainReserve = rnn.forward(data, W, hidden=inithidden, cells=initcells)\n\n\thostOutData = np.zeros((seqlen + 1, batchsize, hsize), dtype=dtype)\n\thostCells = np.empty((seqlen + 1, batchsize, hsize), dtype=dtype)\n\n\thostOutData[0] = hostInitHidden\n\thostCells[0] = hostInitCells\n\n\thostStates = np.zeros((seqlen + 2, batchsize, hsize * 4), dtype=dtype)\n\thostW = np.empty((insize + hsize, 4 * hsize), dtype=dtype)\n\thostBias = np.empty((4 * hsize, ), dtype=dtype)\n\n\thostW[:insize, :hsize] = hostParams[""wc""].T\n\thostW[:insize, hsize:2 * hsize] = hostParams[""wi""].T\n\thostW[:insize, 2 * hsize:3 * hsize] = hostParams[""wf""].T\n\thostW[:insize, 3 * hsize:] = hostParams[""wo""].T\n\n\thostW[insize:, :hsize] = hostParams[""rc""].T\n\thostW[insize:, hsize:2 * hsize] = hostParams[""ri""].T\n\thostW[insize:, 2 * hsize:3 * hsize] = hostParams[""rf""].T\n\thostW[insize:, 3 * hsize:] = hostParams[""ro""].T\n\n\thostBias[:hsize] = hostParams[""bwc""] + hostParams[""brc""]\n\thostBias[hsize:2 * hsize] = hostParams[""bwi""] + hostParams[""bri""]\n\thostBias[2 * hsize: 3 * hsize] = hostParams[""bwf""] + hostParams[""brf""]\n\thostBias[3 * hsize:] = hostParams[""bwo""] + hostParams[""bro""]\n\n\tdef lstmAct(dat, hsz):\n\t\tdat[:, :hsz] = np.tanh(dat[:, :hsz])\n\t\tdat[:, hsz:] = 1.0 / (np.exp(-dat[:, hsz:]) + 1.0)\n\t\treturn dat\n\n\tfor d in range(seqlen):\n\t\tinp = np.hstack((hostData[d], hostOutData[d]))\n\t\toutp = lstmAct(np.dot(inp, hostW) + hostBias, hsize)\n\t\thostStates[d + 1] = outp\n\n\t\tct = outp[:, 2 * hsize:3 * hsize] * hostCells[d] + outp[:, hsize :2 * hsize] * outp[:, :hsize]\n\n\t\thostCells[d + 1] = ct\n\t\thostOutData[d + 1] = outp[:, 3 * hsize:] * np.tanh(ct)\n\n\tassert np.allclose(hostOutData[1:], outdata.get(), atol=atol)\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad = bnd.GPUArray.toGpu(hostGrad)\n\tingrad, dhx, dcx = rnn.backwardData(grad, outdata, W, trainReserve, hidden=inithidden, cells=initcells)\n\n\tdw = rnn.backwardParams(data, outdata, trainReserve, hidden=inithidden)\n\tdwparams = bnd.acquireRnnParams(rnn, W=dw)\n\n\tdwparams = dwparams[0]\n\thostDw = np.zeros(hostW.shape, dtype=dtype)\n\thostDb = np.zeros(hostBias.shape, dtype=dtype)\n\n\thostAccCellsGrad = np.zeros((seqlen + 1, batchsize, hsize), dtype=dtype)\n\thostAccHiddenGrad = np.zeros((seqlen + 1, batchsize, hsize), dtype=dtype)\n\thostInGrad = np.zeros((seqlen, batchsize, insize), dtype=dtype)\n\n\tdef lstmActBwd(gr, dat, hsz):\n\t\tgr[:, :hsz] = gr[:, :hsz] * (1.0 - dat[:, :hsz]**2)\n\t\tgr[:, hsz:] = gr[:, hsz:] * dat[:, hsz:] * (1.0 - dat[:, hsz:])\n\t\treturn gr\n\n\tfor d in range(seqlen):\n\t\tdh = hostGrad[seqlen-1 - d] + hostAccHiddenGrad[seqlen - d]\n\t\tdc = dh * hostStates[seqlen-d, :, 3 * hsize:] * (1 - np.tanh(hostCells[seqlen - d])**2) + \\\n\t\t\t hostAccCellsGrad[seqlen - d] * hostStates[seqlen + 1 - d, :, 2 * hsize:3 * hsize]\n\n\t\tlayergr = np.empty((batchsize, 4 * hsize), dtype=dtype)\n\t\tlayergr[:, :hsize] = dc * hostStates[seqlen-d, :, hsize:2 * hsize]\n\t\tlayergr[:, hsize:2 * hsize] = dc * hostStates[seqlen-d, :, :hsize]\n\t\tlayergr[:, 2 * hsize:3 * hsize] = dc * hostCells[seqlen - 1 - d]\n\t\tlayergr[:, 3 * hsize:] = dh * np.tanh(hostCells[seqlen - d])\n\n\t\tlayergr = lstmActBwd(layergr, hostStates[seqlen - d], hsize)\n\t\tingr = np.dot(layergr, hostW.T)\n\n\t\tindata = np.hstack((hostData[seqlen - 1 - d], hostOutData[seqlen - 1 - d]))\n\t\thostDw += np.dot(indata.T, layergr)\n\t\thostDb += np.sum(layergr, axis=0)\n\n\t\thostAccHiddenGrad[seqlen-1 - d] = ingr[:, insize:]\n\t\thostAccCellsGrad[seqlen-1 - d] = dc\n\t\thostInGrad[seqlen-1 - d] = ingr[:, :insize]\n\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\n\tassert np.allclose(hostDw[:insize, :hsize], dwparams[""wc""].get().T, atol=atol)\n\tassert np.allclose(hostDw[:insize, hsize:2 * hsize], dwparams[""wi""].get().T, atol=atol)\n\tassert np.allclose(hostDw[:insize, 2 * hsize:3 * hsize], dwparams[""wf""].get().T, atol=atol)\n\tassert np.allclose(hostDw[:insize, 3 * hsize:], dwparams[""wo""].get().T, atol=atol)\n\n\tassert np.allclose(hostDw[insize:, :hsize], dwparams[""rc""].get().T, atol=atol)\n\tassert np.allclose(hostDw[insize:, hsize:2 * hsize], dwparams[""ri""].get().T, atol=atol)\n\tassert np.allclose(hostDw[insize:, 2 * hsize:3 * hsize], dwparams[""rf""].get().T, atol=atol)\n\tassert np.allclose(hostDw[insize:, 3 * hsize:], dwparams[""ro""].get().T, atol=atol)\n\n\tassert np.allclose(hostDb[:hsize], dwparams[""bwc""].get(), atol=atol)\n\tassert np.allclose(hostDb[:hsize], dwparams[""brc""].get(), atol=atol)\n\n\tassert np.allclose(hostDb[hsize:2 * hsize], dwparams[""bwi""].get(), atol=atol)\n\tassert np.allclose(hostDb[hsize:2 * hsize], dwparams[""bri""].get(), atol=atol)\n\n\tassert np.allclose(hostDb[2 * hsize: 3 * hsize], dwparams[""bwf""].get(), atol=atol)\n\tassert np.allclose(hostDb[2 * hsize: 3 * hsize], dwparams[""brf""].get(), atol=atol)\n\n\tassert np.allclose(hostDb[3 * hsize:], dwparams[""bwo""].get(), atol=atol)\n\tassert np.allclose(hostDb[3 * hsize:], dwparams[""bro""].get(), atol=atol)\n\n\ndef gruTest(bnd, dtype, atol):\n\tseqlen, batchsize, insize, hsize = 3, 3, 4, 2\n\trnn, W, params = bnd.createRnn(insize, hsize, dtype, mode=bnd.RNNMode.gru)\n\n\thostParams = randomWInit(bnd, rnn, W, params)[0]\n\n\thostData = np.random.randn(seqlen, batchsize, insize).astype(dtype)\n\thostInitHidden = np.random.randn(1, batchsize, hsize).astype(dtype)\n\n\tdata, inithidden = bnd.GPUArray.toGpu(hostData), bnd.GPUArray.toGpu(hostInitHidden)\n\toutdata, trainReserve = rnn.forward(data, W, hidden=inithidden)\n\n\thostOutData = np.zeros((seqlen + 1, batchsize, hsize), dtype=dtype)\n\thostOutData[0] = hostInitHidden\n\n\thostStates = np.zeros((seqlen + 1, batchsize, hsize * 4), dtype=dtype)\n\thostHts = np.zeros((seqlen + 1, batchsize, hsize), dtype=dtype)\n\thostW = np.zeros((insize + hsize, 4 * hsize), dtype=dtype)\n\thostBias = np.empty((4 * hsize, ), dtype=dtype)\n\n\thostW[:insize, hsize:2 * hsize] = hostParams[""wh""].T\n\thostW[:insize, 2 * hsize:3 * hsize] = hostParams[""wr""].T\n\thostW[:insize, 3 * hsize:] = hostParams[""wi""].T\n\n\thostW[insize:, :hsize] = hostParams[""rh""].T\n\thostW[insize:, 2 * hsize:3 * hsize] = hostParams[""rr""].T\n\thostW[insize:, 3 * hsize:] = hostParams[""ri""].T\n\n\thostBias[:hsize] = hostParams[""brh""]\n\thostBias[hsize:2 * hsize] = hostParams[""bwh""]\n\thostBias[2 * hsize: 3 * hsize] = hostParams[""bwr""] + hostParams[""brr""]\n\thostBias[3 * hsize:] = hostParams[""bwi""] + hostParams[""bri""]\n\n\tdef gruAct(dat, hsz):\n\t\tdat[:, 2 * hsz:] = 1.0 / (np.exp(-dat[:, 2 * hsz:]) + 1.0)\n\t\treturn dat\n\n\tfor d in range(seqlen):\n\t\tinp = np.hstack((hostData[d], hostOutData[d]))\n\t\toutp = gruAct(np.dot(inp, hostW) + hostBias, hsize)\n\t\thostStates[d + 1] = outp\n\n\t\tht = np.tanh(outp[:, hsize:2 * hsize] + outp[:, 2 * hsize: 3 * hsize] * outp[:, :hsize])\n\t\tit = outp[:, 3 * hsize:]\n\t\thostOutData[d + 1] = (1.0 - it) * ht + it * hostOutData[d]\n\t\thostHts[d + 1] = ht\n\n\tassert np.allclose(hostOutData[1:], outdata.get(), atol=atol)\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad = bnd.GPUArray.toGpu(hostGrad)\n\tingrad, dhx, _ = rnn.backwardData(grad, outdata, W, trainReserve, hidden=inithidden)\n\n\tdw = rnn.backwardParams(data, outdata, trainReserve, hidden=inithidden)\n\tdwparams = bnd.acquireRnnParams(rnn, W=dw)\n\n\tdwparams = dwparams[0]\n\thostDw = np.zeros(hostW.shape, dtype=dtype)\n\thostDb = np.zeros(hostBias.shape, dtype=dtype)\n\n\thostAccGrad = np.zeros((seqlen + 1, batchsize, hsize), dtype=dtype)\n\thostInGrad = np.zeros((seqlen, batchsize, insize), dtype=dtype)\n\n\tdef gruActBwd(gr, dat, hsz):\n\t\tgr[:, 2 * hsz:] = gr[:, 2 * hsz:] * dat[:, 2 * hsz:] * (1.0 - dat[:, 2 * hsz:])\n\t\treturn gr\n\n\tfor d in range(seqlen):\n\t\tdh = hostGrad[seqlen-1 - d] + hostAccGrad[seqlen - d]\n\t\tdht = (1 - hostStates[seqlen - d, :, 3 * hsize:]) * dh\n\n\t\tlayergr = np.empty((batchsize, 4 * hsize), dtype=dtype)\n\t\tlayergr[:, :hsize] = dht * (1.0 - hostHts[seqlen - d]**2) * hostStates[seqlen - d, :, 2 * hsize:3 * hsize]\n\t\tlayergr[:, hsize:2 * hsize] = dht * (1.0 - hostHts[seqlen - d]**2)\n\t\tlayergr[:, 2 * hsize:3 * hsize] = dht * (1.0 - hostHts[seqlen - d]**2) * hostStates[seqlen - d, :, :hsize]\n\t\tlayergr[:, 3 * hsize:] = dh * (hostOutData[seqlen - 1 - d] - hostHts[seqlen - d])\n\n\t\tlayergr = gruActBwd(layergr, hostStates[seqlen - d], hsize)\n\t\tingr = np.dot(layergr, hostW.T)\n\n\t\tindata = np.hstack((hostData[seqlen - 1 - d], hostOutData[seqlen - 1 - d]))\n\t\thostDw += np.dot(indata.T, layergr)\n\t\thostDb += np.sum(layergr, axis=0)\n\n\t\thostAccGrad[seqlen - 1 - d] = dh * hostStates[seqlen - d, :, 3 * hsize:] + ingr[:, insize:]\n\t\thostInGrad[seqlen - 1 - d] = ingr[:, :insize]\n\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\n\tassert np.allclose(hostDw[:insize, hsize:2 * hsize], dwparams[""wh""].get().T, atol=atol)\n\tassert np.allclose(hostDw[:insize, 2 * hsize:3 * hsize], dwparams[""wr""].get().T, atol=atol)\n\tassert np.allclose(hostDw[:insize, 3 * hsize:], dwparams[""wi""].get().T, atol=atol)\n\n\tassert np.allclose(hostDw[insize:, :hsize], dwparams[""rh""].get().T, atol=atol)\n\tassert np.allclose(hostDw[insize:, 2 * hsize:3 * hsize], dwparams[""rr""].get().T, atol=atol)\n\tassert np.allclose(hostDw[insize:, 3 * hsize:], dwparams[""ri""].get().T, atol=atol)\n\n\tassert np.allclose(hostDb[:hsize], dwparams[""brh""].get(), atol=atol)\n\tassert np.allclose(hostDb[hsize:2 * hsize], dwparams[""bwh""].get(), atol=atol)\n\n\tassert np.allclose(hostDb[2 * hsize: 3 * hsize], dwparams[""bwr""].get(), atol=atol)\n\tassert np.allclose(hostDb[2 * hsize: 3 * hsize], dwparams[""brr""].get(), atol=atol)\n\n\tassert np.allclose(hostDb[3 * hsize:], dwparams[""bwi""].get(), atol=atol)\n\tassert np.allclose(hostDb[3 * hsize:], dwparams[""bri""].get(), atol=atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cuda/Wrappers/CuDnnSpatialTf.py,16,"b'import itertools, math\nimport numpy as np\n\n\ndef unittest():\n\tfrom PuzzleLib.Cuda import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tbnd = Backend.getBackend(deviceIdx, initmode=2)\n\n\t\tfor dtype, atol in bnd.dtypesSupported():\n\t\t\tspatialTfTest(bnd, dtype, atol)\n\n\ndef spatialTfTest(bnd, dtype, atol):\n\tbatchsize, maps, inh, inw = 1, 1, 4, 4\n\touth, outw = int(1.0 * inh), int(1.0 * inw)\n\n\thostData = np.random.randn(batchsize, maps, inh, inw).astype(dtype)\n\thostTf = np.tile(np.array([[1.0, 0.1, -0.001], [0.0, 0.9, -0.001]], dtype=dtype), reps=(batchsize, 1, 1))\n\n\tdata, transform = bnd.GPUArray.toGpu(hostData), bnd.GPUArray.toGpu(hostTf)\n\toutdata, grid = bnd.dnn.spatialTf(data, transform, outshape=(batchsize, maps, outh, outw), getGrid=True)\n\n\thostGrid = np.empty((batchsize, outh, outw, 2), dtype=dtype)\n\txstep, ystep = 2.0 / (outw - 1), 2.0 / (outh - 1)\n\n\tfor b, y, x in itertools.product(range(batchsize), range(outh), range(outw)):\n\t\thostGrid[b, y, x] = np.dot(hostTf[b], np.array([-1.0 + x * xstep, -1.0 + y * ystep, 1.0], dtype=np.float32))\n\n\tassert np.allclose(hostGrid, grid.get(), atol=atol)\n\n\thostOutData = np.zeros(outdata.shape, dtype=dtype)\n\txstep, ystep = 2.0 / (inw - 1), 2.0 / (inh - 1)\n\n\tfor b, c, y, x in itertools.product(range(batchsize), range(maps), range(outh), range(outw)):\n\t\tdstx, dsty = hostGrid[b, y, x]\n\t\tny, nx = (dsty + 1.0) / ystep, (dstx + 1.0) / xstep\n\n\t\tsrcy, srcx = int(math.floor(ny)), int(math.floor(nx))\n\t\tdy, dx = ny - srcy, nx - srcx\n\n\t\tul, ur, bl, br = 0.0, 0.0, 0.0, 0.0\n\t\tif 0 <= srcy < inh and 0 <= srcx < inw: ul = hostData[b, c, srcy, srcx] * (1 - dy) * (1 - dx)\n\t\tif 0 <= srcy + 1 < inh and 0 <= srcx < inw: bl = hostData[0, 0, srcy + 1, srcx] * dy * (1 - dx)\n\t\tif 0 <= srcy < inh and 0 <= srcx + 1 < inw: ur = hostData[0, 0, srcy, srcx + 1] * (1 - dy) * dx\n\t\tif 0 <= srcy + 1 < inh and 0 <= srcx + 1 < inw: br = hostData[0, 0, srcy + 1, srcx + 1] * dy * dx\n\n\t\thostOutData[b, c, y, x] = ul + ur + bl + br\n\n\tassert np.allclose(hostOutData, outdata.get(), atol=atol)\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad = bnd.GPUArray.toGpu(hostGrad)\n\tingrad, dtransform, dgrid = bnd.dnn.spatialTfBackward(grad, data, grid, getDGrid=True)\n\n\thostInGrad = np.zeros(data.shape, dtype=dtype)\n\n\tfor b, c, y, x in itertools.product(range(batchsize), range(maps), range(outh), range(outw)):\n\t\tdstx, dsty = hostGrid[b, y, x]\n\t\tny, nx = (dsty + 1.0) / ystep, (dstx + 1.0) / xstep\n\n\t\tsrcy, srcx = int(math.floor(ny)), int(math.floor(nx))\n\t\tdy, dx = ny - srcy, nx - srcx\n\n\t\tval = hostGrad[b, c, y, x]\n\t\tif 0 <= srcy < inh and 0 <= srcx < inw: hostInGrad[b, c, srcy, srcx] += val * (1 - dy) * (1 - dx)\n\t\tif 0 <= srcy + 1 < inh and 0 <= srcx < inw: hostInGrad[b, c, srcy + 1, srcx] += val * dy * (1 - dx)\n\t\tif 0 <= srcy < inh and 0 <= srcx + 1 < inw: hostInGrad[b, c, srcy, srcx + 1] += val * (1 - dy) * dx\n\t\tif 0 <= srcy + 1 < inh and 0 <= srcx + 1 < inw: hostInGrad[b, c, srcy + 1, srcx + 1] += val * dy * dx\n\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\n\thostDGrid = np.zeros(dgrid.shape, dtype=dtype)\n\n\tfor b, c, y, x in itertools.product(range(batchsize), range(maps), range(outh), range(outw)):\n\t\tdstx, dsty = hostGrid[b, y, x]\n\t\tny, nx = (dsty + 1.0) / ystep, (dstx + 1.0) / xstep\n\n\t\tsrcy, srcx = int(math.floor(ny)), int(math.floor(nx))\n\t\tdy, dx = ny - srcy, nx - srcx\n\n\t\tvalx, valy = 0, 0\n\n\t\tif 0 <= srcy < inh and 0 <= srcx < inw: valx -= hostData[b, c, srcy, srcx] / xstep * (1 - dy)\n\t\tif 0 <= srcy + 1 < inh and 0 <= srcx < inw: valx -= hostData[b, c, srcy + 1, srcx] / xstep * dy\n\t\tif 0 <= srcy < inh and 0 <= srcx + 1 < inw:  valx += hostData[b, c, srcy, srcx + 1] / xstep * (1-dy)\n\t\tif 0 <= srcy + 1 < inh and 0 <= srcx + 1 < inw: valx += hostData[b, c, srcy + 1, srcx + 1] / xstep * dy\n\n\t\tif 0 <= srcy < inh and 0 <= srcx < inw: valy -= hostData[b, c, srcy, srcx] / ystep * (1 - dx)\n\t\tif 0 <= srcy + 1 < inh and 0 <= srcx < inw: valy += hostData[b, c, srcy + 1, srcx] / ystep * (1-dx)\n\t\tif 0 <= srcy < inh and 0 <= srcx + 1 < inw: valy -= hostData[b, c, srcy, srcx + 1] / ystep * dx\n\t\tif 0 <= srcy + 1 < inh and 0 <= srcx + 1 < inw: valy += hostData[b, c, srcy + 1, srcx + 1] / ystep * dx\n\n\t\thostDGrid[b, y, x] = hostGrad[b, c, y, x] * valx, hostGrad[b, c, y, x] * valy\n\n\tassert np.allclose(hostDGrid, dgrid.get(), atol=atol)\n\n\thostDTransform = np.zeros(dtransform.shape, dtype=dtype)\n\txstep, ystep = 2.0 / (outw - 1), 2.0 / (outh - 1)\n\n\tfor b, y, x in itertools.product(range(batchsize), range(outh), range(outw)):\n\t\thostDTransform[b] += np.outer(\n\t\t\thostDGrid[b, y, x], np.array([-1.0 + x * xstep, -1.0 + y * ystep, 1], dtype=np.float32)\n\t\t)\n\n\tassert np.allclose(hostDTransform, dtransform.get(), atol=atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Cuda/Wrappers/NPP.py,38,"b'import ctypes\nfrom enum import Enum\n\nimport numpy as np\nfrom PuzzleLib.Cuda.ThirdParty import libnpp\n\n\nclass InterpolationMode(Enum):\n\tnn = libnpp.NppiInterpolationMode[""NPPI_INTER_NN""]\n\tlinear = libnpp.NppiInterpolationMode[""NPPI_INTER_LINEAR""]\n\tcubic = libnpp.NppiInterpolationMode[""NPPI_INTER_CUBIC""]\n\tcubic2pbSpline = libnpp.NppiInterpolationMode[""NPPI_INTER_CUBIC2P_BSPLINE""]\n\tcubic2pCatmullRom = libnpp.NppiInterpolationMode[""NPPI_INTER_CUBIC2P_CATMULLROM""]\n\tcubic2pb05c03 = libnpp.NppiInterpolationMode[""NPPI_INTER_CUBIC2P_B05C03""]\n\tsuper = libnpp.NppiInterpolationMode[""NPPI_INTER_SUPER""]\n\tlanczos = libnpp.NppiInterpolationMode[""NPPI_INTER_LANCZOS""]\n\tlanczos3Advanced = libnpp.NppiInterpolationMode[""NPPI_INTER_LANCZOS3_ADVANCED""]\n\tsmoothEdge = libnpp.NppiInterpolationMode[""NPPI_SMOOTH_EDGE""]\n\n\nclass DataType(Enum):\n\tu8 = ""8u""\n\tu16 = ""16u""\n\ts16 = ""16s""\n\tf32 = ""32f""\n\tf64 = ""64f""\n\n\nclass MemoryType(Enum):\n\tgrayscale = ""C1R""\n\trgb = ""C3R""\n\trgba = ""C4R""\n\trgbPlanar = ""P3R""\n\trgbaPlanar = ""P4R""\n\n\ndef isPlanarMemoryType(memoryType):\n\treturn memoryType == MemoryType.rgbPlanar or memoryType == MemoryType.rgbaPlanar\n\n\ndef getDataRect(data, memoryType):\n\tif memoryType == MemoryType.grayscale or memoryType == MemoryType.rgb or memoryType == MemoryType.rgba:\n\t\treturn 0, 0, data.shape[1], data.shape[0]\n\n\telif isPlanarMemoryType(memoryType):\n\t\treturn 0, 0, data.shape[2], data.shape[1]\n\n\telse:\n\t\traise NotImplementedError(memoryType)\n\n\ndef getDataType(data):\n\tif data.dtype == np.uint8:\n\t\treturn DataType.u8\n\telif data.dtype == np.uint16:\n\t\treturn DataType.u16\n\telif data.dtype == np.int16:\n\t\treturn DataType.s16\n\telif data.dtype == np.float32:\n\t\treturn DataType.f32\n\telif data.dtype == np.float64:\n\t\treturn DataType.f64\n\n\telse:\n\t\traise NotImplementedError(data.dtype)\n\n\ndef getMemoryTypeLineSize(line, dtype, memoryType):\n\tif isPlanarMemoryType(memoryType) or memoryType == MemoryType.grayscale:\n\t\treturn line * dtype.itemsize\n\telif memoryType == MemoryType.rgb:\n\t\treturn 3 * dtype.itemsize * line\n\telif memoryType == MemoryType.rgba:\n\t\treturn 4 * dtype.itemsize * line\n\n\telse:\n\t\traise NotImplementedError(memoryType)\n\n\ndef getOutDataShape(data, outrect, memoryType):\n\t_, _, outw, outh = outrect\n\n\tif data.ndim == 2 and memoryType == MemoryType.grayscale:\n\t\treturn outh, outw\n\telif isPlanarMemoryType(memoryType):\n\t\treturn data.shape[0], outh, outw\n\telif memoryType == MemoryType.rgb or memoryType == MemoryType.rgba:\n\t\treturn outh, outw, data.shape[2]\n\n\telse:\n\t\traise NotImplementedError(memoryType)\n\n\ndef getOutDataRect(data, outshape, memoryType):\n\tif data.ndim == 2 and memoryType == MemoryType.grayscale:\n\t\touth, outw = outshape\n\t\treturn 0, 0, outw, outh\n\telif isPlanarMemoryType(memoryType):\n\t\t_, outh, outw = outshape\n\t\treturn 0, 0, outw, outh\n\telif memoryType == MemoryType.rgb or memoryType == MemoryType.rgba:\n\t\touth, outw, _ = outshape\n\t\treturn 0, 0, outw, outh\n\n\telse:\n\t\traise NotImplementedError(memoryType)\n\n\ndef getDataPointers(data, outdata, memoryType):\n\tdataPtr, outdataPtr = data.ptr, outdata.ptr\n\n\tif isPlanarMemoryType(memoryType):\n\t\tdataPtr = [data.ptr + data.strides[0] * i for i in range(data.shape[0])]\n\t\tdataPtr = (ctypes.c_void_p * len(dataPtr))(*dataPtr)\n\n\t\toutdataPtr = [outdata.ptr + outdata.strides[0] * i for i in range(outdata.shape[0])]\n\t\toutdataPtr = (ctypes.c_void_p * len(outdataPtr))(*outdataPtr)\n\n\treturn dataPtr, outdataPtr\n\n\ndef rescale(backend, data, scale, memoryType, interpolation=InterpolationMode.nn, outdata=None, allocator=None):\n\tassert data.ndim == 2 and memoryType == MemoryType.grayscale or data.ndim == 3\n\thscale, wscale = (scale, scale) if isinstance(scale, (int, float)) else scale\n\n\tinrect = getDataRect(data, memoryType)\n\tinsize, inline = (inrect[2], inrect[3]), getMemoryTypeLineSize(inrect[2], data.dtype, memoryType)\n\n\toutrect = libnpp.nppiGetResizeRect(inrect, wscale, hscale, 0, 0, interpolation.value)\n\toutline = getMemoryTypeLineSize(outrect[2], data.dtype, memoryType)\n\n\toutshape = getOutDataShape(data, outrect, memoryType)\n\n\tif outdata is None:\n\t\toutdata = backend.GPUArray.empty(outshape, dtype=data.dtype, allocator=allocator)\n\telse:\n\t\tassert outdata.shape == outshape\n\n\tdataPtr, outdataPtr = getDataPointers(data, outdata, memoryType)\n\n\tlibnpp.nppiResizeSqrPixel(\n\t\tgetDataType(data).value, memoryType.value, dataPtr, insize, inline, inrect,\n\t\toutdataPtr, outline, outrect, wscale, hscale, 0, 0, interpolation.value\n\t)\n\n\treturn outdata\n\n\ndef resize(backend, data, outshape, memoryType, interpolation=InterpolationMode.nn, outdata=None, allocator=None):\n\tinrect = getDataRect(data, memoryType)\n\toutrect = getOutDataRect(data, outshape, memoryType)\n\n\thscale, wscale = outrect[3] / inrect[3], outrect[2] / inrect[2]\n\treturn rescale(backend, data, (hscale, wscale), memoryType, interpolation, outdata=outdata, allocator=allocator)\n\n\ndef warpAffine(backend, data, coeffs, memoryType, outshape=None, interpolation=InterpolationMode.nn, cval=0,\n\t\t\t   backward=False, allocator=None):\n\tassert data.ndim == 2 and memoryType == MemoryType.grayscale or data.ndim == 3\n\n\tinrect = getDataRect(data, memoryType)\n\tinsize, inline = (inrect[2], inrect[3]), getMemoryTypeLineSize(inrect[2], data.dtype, memoryType)\n\n\toutshape = data.shape if outshape is None else outshape\n\n\toutrect = getOutDataRect(data, outshape, memoryType)\n\toutline = getMemoryTypeLineSize(outrect[2], data.dtype, memoryType)\n\n\toutdata = backend.GPUArray.empty(outshape, dtype=data.dtype, allocator=allocator)\n\toutdata.fill(cval)\n\n\tdataPtr, outdataPtr = getDataPointers(data, outdata, memoryType)\n\twarpMethod = libnpp.nppiWarpAffineBack if backward else libnpp.nppiWarpAffine\n\n\twarpMethod(\n\t\tgetDataType(data).value, memoryType.value, dataPtr, insize, inline, inrect, outdataPtr,\n\t\toutline, outrect, coeffs, interpolation.value\n\t)\n\n\treturn outdata\n\n\ndef genAffineQuads(inpoints, outpoints, clip, inrect):\n\tinx0, iny0 = inpoints[0]\n\tinx1, iny1 = inpoints[1]\n\tinx2, iny2 = inpoints[2]\n\n\toutx0, outy0 = outpoints[0]\n\toutx1, outy1 = outpoints[1]\n\toutx2, outy2 = outpoints[2]\n\n\tsrcQuad = inpoints[0] + inpoints[1] + inpoints[2]\n\tsrcQuad.extend([inx2 + inx0 - inx1, iny2 + iny0 - iny1])\n\n\tdstQuad = outpoints[0] + outpoints[1] + outpoints[2]\n\tdstQuad.extend([outx2 + outx0 - outx1, outy2 + outy0 - outy1])\n\n\tif not clip:\n\t\tintransform = np.zeros((3, 3), dtype=np.float32)\n\t\tintransform[2, 2] = 1.0\n\n\t\tintransform[:2] = np.array(libnpp.nppiGetAffineTransform(inrect, srcQuad), dtype=np.float32).reshape(2, 3)\n\n\t\touttransform = np.zeros((3, 3), dtype=np.float32)\n\t\touttransform[2, 2] = 1.0\n\n\t\touttransform[:2] = np.array(libnpp.nppiGetAffineTransform(inrect, dstQuad), dtype=np.float32).reshape(2, 3)\n\n\t\ttransform = np.dot(outtransform, np.linalg.inv(intransform))[:2]\n\n\t\tinh, inw = inrect[2], inrect[3]\n\t\tsrcQuad = [0, inw, 0.0, 0.0, inh, 0.0, inh, inw]\n\n\t\tdstQuad = []\n\t\tfor i in range(len(srcQuad) >> 1):\n\t\t\tinpoint = srcQuad[2 * i: 2 * (i + 1)]\n\t\t\tdstQuad.extend(list(np.dot(transform, np.array(inpoint + [1.0]))))\n\n\treturn srcQuad, dstQuad\n\n\ndef warpAffinePoints(backend, data, inpoints, outpoints, memoryType, outshape=None, interpolation=InterpolationMode.nn,\n\t\t\t\t\t cval=0, clip=True, allocator=None):\n\tassert data.ndim == 2 and memoryType == MemoryType.grayscale or data.ndim == 3\n\n\tinrect = getDataRect(data, memoryType)\n\tinsize, inline = (inrect[2], inrect[3]), getMemoryTypeLineSize(inrect[2], data.dtype, memoryType)\n\n\tif outshape is None:\n\t\toutshape = data.shape\n\n\toutrect = getOutDataRect(data, outshape, memoryType)\n\toutline = getMemoryTypeLineSize(outrect[2], data.dtype, memoryType)\n\n\toutdata = backend.GPUArray.empty(outshape, dtype=data.dtype, allocator=allocator)\n\toutdata.fill(cval)\n\n\tdataPtr, outdataPtr = getDataPointers(data, outdata, memoryType)\n\tsrcQuad, dstQuad = genAffineQuads(inpoints, outpoints, clip, inrect)\n\n\tlibnpp.nppiWarpAffineQuad(\n\t\tgetDataType(data).value, memoryType.value, dataPtr, insize, inline, inrect, srcQuad,\n\t\toutdataPtr, outline, outrect, dstQuad, interpolation.value\n\t)\n\n\treturn outdata\n\n\ndef unittest():\n\tfrom PuzzleLib.Cuda import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tbnd = Backend.getBackend(deviceIdx)\n\n\t\tresizeWidePixelTest(bnd)\n\t\tresizePlanarTest(bnd)\n\t\twarpAffineTest(bnd)\n\t\twarpAffinePointsTest(bnd)\n\n\ndef resizeWidePixelTest(bnd):\n\tinh, inw = 2, 4\n\thscale, wscale = 2.5, 1.5\n\n\thostData = np.random.randn(inh, inw, 3).astype(np.float32)\n\tdata = bnd.GPUArray.toGpu(hostData)\n\n\toutdata = rescale(\n\t\tbnd, data, scale=(hscale, wscale), memoryType=MemoryType.rgb, interpolation=InterpolationMode.linear\n\t)\n\toutresdata = resize(bnd, data, outdata.shape, memoryType=MemoryType.rgb, interpolation=InterpolationMode.linear)\n\n\thostOutData = np.empty((int(inh * hscale), int(inw * wscale), 3), dtype=data.dtype)\n\n\tdef hostResizeWide(hostDat, hostOutDat):\n\t\tfor y in range(hostOutDat.shape[0]):\n\t\t\tny = max(0.0, (y + 0.5) / hscale - 0.5)\n\t\t\tiyp = int(ny)\n\t\t\tdy = ny - iyp\n\t\t\tiypp = min(iyp + 1, hostDat.shape[0] - 1)\n\n\t\t\tfor x in range(hostOutDat.shape[1]):\n\t\t\t\tnx = max(0.0, (x + 0.5) / wscale - 0.5)\n\t\t\t\tixp = int(nx)\n\t\t\t\tdx = nx - ixp\n\t\t\t\tixpp = min(ixp + 1, hostDat.shape[1] - 1)\n\n\t\t\t\thostOutDat[y, x, :] = (1.0 - dy) * (1.0 - dx) * hostDat[iyp, ixp, :] + \\\n\t\t\t\t\t\t\t\t\t  dy * (1.0 - dx) * hostDat[iypp, ixp, :] + \\\n\t\t\t\t\t\t\t\t\t  (1.0 - dy) * dx * hostDat[iyp, ixpp, :] + \\\n\t\t\t\t\t\t\t\t\t  dy * dx * hostDat[iypp, ixpp, :]\n\n\thostResizeWide(hostData, hostOutData)\n\tassert np.allclose(hostOutData, outdata.get())\n\n\thscale, wscale = hostOutData.shape[0] / hostData.shape[0], hostOutData.shape[1] / hostData.shape[1]\n\n\thostResizeWide(hostData, hostOutData)\n\tassert np.allclose(hostOutData, outresdata.get())\n\n\ndef resizePlanarTest(bnd):\n\tinh, inw = 2, 4\n\thscale, wscale = 2.5, 1.5\n\n\thostData = np.random.randn(3, inh, inw).astype(np.float32)\n\tdata = bnd.GPUArray.toGpu(hostData)\n\n\toutdata = rescale(\n\t\tbnd, data, scale=(hscale, wscale), memoryType=MemoryType.rgbPlanar, interpolation=InterpolationMode.linear\n\t)\n\toutresdata = resize(\n\t\tbnd, data, outdata.shape, memoryType=MemoryType.rgbPlanar, interpolation=InterpolationMode.linear\n\t)\n\n\thostOutData = np.empty((3, int(inh * hscale), int(inw * wscale)), dtype=data.dtype)\n\n\tdef hostResizePlanar(hostDat, hostOutDat):\n\t\tfor y in range(hostOutDat.shape[1]):\n\t\t\tny = max(0.0, (y + 0.5) / hscale - 0.5)\n\t\t\tiyp = int(ny)\n\t\t\tdy = ny - iyp\n\t\t\tiypp = min(iyp + 1, hostDat.shape[1] - 1)\n\n\t\t\tfor x in range(hostOutDat.shape[2]):\n\t\t\t\tnx = max(0.0, (x + 0.5) / wscale - 0.5)\n\t\t\t\tixp = int(nx)\n\t\t\t\tdx = nx - ixp\n\t\t\t\tixpp = min(ixp + 1, hostDat.shape[2] - 1)\n\n\t\t\t\thostOutDat[:, y, x] = (1.0 - dy) * (1.0 - dx) * hostDat[:, iyp, ixp] + \\\n\t\t\t\t\t\t\t\t\t  dy * (1.0 - dx) * hostDat[:, iypp, ixp] + \\\n\t\t\t\t\t\t\t\t\t  (1.0 - dy) * dx * hostDat[:, iyp, ixpp] + \\\n\t\t\t\t\t\t\t\t\t  dy * dx * hostDat[:, iypp, ixpp]\n\n\thostResizePlanar(hostData, hostOutData)\n\tassert np.allclose(hostOutData, outdata.get())\n\n\thscale, wscale = hostOutData.shape[1] / hostData.shape[1], hostOutData.shape[2] / hostData.shape[2]\n\n\thostResizePlanar(hostData, hostOutData)\n\tassert np.allclose(hostOutData, outresdata.get())\n\n\ndef warpAffineTest(bnd):\n\tinh, inw = 4, 4\n\touth, outw = 10, 10\n\n\tmat = np.array([\n\t\t[0.0, 2.0, 0.0],\n\t\t[5.0, 0.0, 0.0],\n\t\t[0.0, 0.0, 1.0]\n\t], dtype=np.float32)\n\n\tinvMat = np.linalg.inv(mat)\n\tmat = list(mat[:2].ravel())\n\n\thostData = np.random.randn(inh, inw, 3).astype(np.float32)\n\tdata = bnd.GPUArray.toGpu(hostData)\n\n\toutdata = warpAffine(\n\t\tbnd, data, mat, memoryType=MemoryType.rgb, outshape=(outh, outw, 3), interpolation=InterpolationMode.nn\n\t)\n\n\toutbackdata = warpAffine(\n\t\tbnd, data, list(invMat[:2].ravel()), memoryType=MemoryType.rgb, outshape=(outh, outw, 3),\n\t\tinterpolation=InterpolationMode.nn, backward=True\n\t)\n\n\thostOutData = np.zeros(outdata.shape, dtype=np.float32)\n\n\tfor y in range(hostOutData.shape[0]):\n\t\tfor x in range(hostOutData.shape[1]):\n\t\t\tinx = int(invMat[0, 0] * x + invMat[0, 1] * y + invMat[0, 2] + 0.5)\n\t\t\tiny = int(invMat[1, 0] * x + invMat[1, 1] * y + invMat[1, 2] + 0.5)\n\n\t\t\tif 0 <= inx < hostData.shape[1] and 0 <= iny < hostData.shape[0]:\n\t\t\t\thostOutData[y, x, :] = hostData[iny, inx, :]\n\n\tassert np.allclose(hostOutData, outdata.get())\n\tassert np.allclose(hostOutData, outbackdata.get())\n\n\ndef warpAffinePointsTest(bnd):\n\tinh, inw = 4, 4\n\touth, outw = 4, 4\n\n\tinpoints = [[0, inw], [0, 0], [inh, 0]]\n\toutpoints = [[outw, 0], [0, 0], [0, outh]]\n\n\thostData = np.random.randn(inh, inw, 3).astype(np.float32)\n\tdata = bnd.GPUArray.toGpu(hostData)\n\n\toutdata = warpAffinePoints(\n\t\tbnd, data, inpoints, outpoints, memoryType=MemoryType.rgb, outshape=(outh, outw, 3),\n\t\tinterpolation=InterpolationMode.nn\n\t)\n\n\thostOutData = np.zeros(outdata.shape, dtype=np.float32)\n\tdx, dy = outpoints[1][0] - inpoints[1][0], outpoints[1][1] - inpoints[1][1]\n\n\tA = np.array([\n\t\t[inpoints[0][0] - inpoints[1][0], inpoints[2][0] - inpoints[1][0]],\n\t\t[inpoints[0][1] - inpoints[1][1], inpoints[2][1] - inpoints[1][1]]\n\t], dtype=np.float32)\n\n\toa = np.array([outpoints[0][0] - outpoints[1][0], outpoints[0][1] - outpoints[1][1]])\n\tob = np.array([outpoints[2][0] - outpoints[1][0], outpoints[2][1] - outpoints[1][1]])\n\n\tx1, y1 = np.linalg.solve(A, oa)\n\tx2, y2 = np.linalg.solve(A, ob)\n\n\tmat = np.array([\n\t\t[x1, x2, dx],\n\t\t[y1, y2, dy],\n\t\t[0.0, 0.0, 1.0]\n\t], dtype=np.float32)\n\n\tinvMat = np.linalg.inv(mat)\n\n\tfor y in range(hostOutData.shape[0]):\n\t\tfor x in range(hostOutData.shape[1]):\n\t\t\tinx = int(invMat[0, 0] * x + invMat[0, 1] * y + invMat[0, 2] + 0.5)\n\t\t\tiny = int(invMat[1, 0] * x + invMat[1, 1] * y + invMat[1, 2] + 0.5)\n\n\t\t\tif 0 <= inx < hostData.shape[1] and 0 <= iny < hostData.shape[0]:\n\t\t\t\thostOutData[y, x, :] = hostData[iny, inx, :]\n\n\tassert np.allclose(hostOutData, outdata.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Hip/Benchmarks/ConvSpeed.py,1,"b'import numpy as np\n\nfrom PuzzleLib import Config\nfrom PuzzleLib.Cuda.Benchmarks.ConvSpeed import timeConv\n\n\ndef main():\n\tdatashape = (128, 32, 64, 64)\n\tWshape = (64, 32, 11, 11)\n\n\tstride, pad, dilation, groups = 1, 0, 1, datashape[1] // Wshape[1]\n\n\tfrom PuzzleLib.Hip.Backend import getBackend\n\tbackend = getBackend(Config.deviceIdx, initmode=1)\n\n\ttimeConv(backend, datashape, Wshape, np.float32, stride, pad, dilation, groups)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Hip/Kernels/CTC.py,0,"b'from PuzzleLib.Cuda.Kernels.CTC import CTCModule, backendTest\n\n\nclass HipCTCModule(CTCModule):\n\t@staticmethod\n\tdef generateConfig(backend):\n\t\treturn [\n\t\t\t(backend.warpSize, 1),\n\t\t\t(backend.warpSize * 2, 1),\n\t\t\t(backend.warpSize, 3),\n\t\t\t(backend.warpSize * 2, 2),\n\t\t\t(backend.warpSize, 6),\n\t\t\t(backend.warpSize * 2, 4),\n\t\t\t(backend.warpSize, 9),\n\t\t\t(backend.warpSize * 2, 6),\n\t\t\t(backend.warpSize * 2, 9),\n\t\t\t(backend.warpSize * 2, 10)\n\t\t]\n\n\ndef unittest():\n\tfrom PuzzleLib.Hip import Backend\n\tbackendTest(Backend, HipCTCModule)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Hip/Kernels/Costs.py,0,"b'from PuzzleLib.Cuda.Kernels.Costs import backendTest\n\n\ndef unittest():\n\tfrom PuzzleLib.Hip import Backend\n\tbackendTest(Backend)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Hip/Kernels/Embedder.py,0,"b'from PuzzleLib.Cuda.Kernels.Embedder import backendTest\n\n\ndef unittest():\n\tfrom PuzzleLib.Hip import Backend\n\tbackendTest(Backend)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Hip/Kernels/MatVec.py,0,"b'from PuzzleLib.Cuda.Kernels.MatVec import backendTest\n\n\ndef unittest():\n\tfrom PuzzleLib.Hip import Backend\n\tbackendTest(Backend)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Hip/Kernels/Memory.py,0,"b'from PuzzleLib.Cuda.Kernels.Memory import backendTest\n\n\ndef unittest():\n\tfrom PuzzleLib.Hip import Backend\n\tbackendTest(Backend)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Hip/Kernels/PRelu.py,0,"b'from PuzzleLib.Cuda.Kernels.PRelu import backendTest\n\n\ndef unittest():\n\tfrom PuzzleLib.Hip import Backend\n\tbackendTest(Backend)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Hip/Kernels/Pad.py,0,"b'from PuzzleLib.Cuda.Kernels.Pad import backendTest\n\n\ndef unittest():\n\tfrom PuzzleLib.Hip import Backend\n\tbackendTest(Backend)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Hip/Kernels/Pool.py,0,"b'from PuzzleLib.Cuda.Kernels.Pool import backendTest\n\n\ndef unittest():\n\tfrom PuzzleLib.Hip import Backend\n\tbackendTest(Backend)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Hip/Kernels/RadixSort.py,0,"b'from PuzzleLib.Cuda.Kernels.RadixSort import backendTest\n\n\ndef unittest():\n\tfrom PuzzleLib.Hip import Backend\n\tbackendTest(Backend)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Hip/Kernels/Upsample.py,0,"b'from PuzzleLib.Cuda.Kernels.Upsample import backendTest\n\n\ndef unittest():\n\tfrom PuzzleLib.Hip import Backend\n\tbackendTest(Backend)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Hip/Source/Build.py,0,"b'from PuzzleLib.Cuda.Source.Build import prepareCompiler, generateTemplates, collectCoreSources, collectLibSources\n\n\ndef buildDriver(debugmode, verbose):\n\tcc = prepareCompiler(debugmode, verbose)\n\tprepareHip(cc)\n\n\tgenerateTemplates(path=""../../Cuda/Source"")\n\n\tdriver = ""../Driver"" + cc.pydext\n\tcc.build(driver, collectSources(path=""../../Cuda/Source"")).clearPath("".."")\n\n\treturn driver\n\n\ndef prepareHip(cc):\n\tcc.cppMode(True).addDefine(""__HIP_PLATFORM_HCC__"")\n\tcc.cflags.extend([""-x"", ""c++""])\n\n\tcc.addLibrary(\n\t\t""hip"",\n\t\t[\n\t\t\t""."", ""/opt/rocm/hsa/include"", ""/opt/rocm/hip/include"",\n\t\t\t""/opt/rocm/hiprand/include"", ""/opt/rocm/rocrand/include"",\n\t\t\t""/opt/rocm/rocblas/include"", ""/opt/rocm/miopen/include""\n\t\t],\n\t\t[""/opt/rocm/hip/lib"", ""/opt/rocm/hiprand/lib"", ""/opt/rocm/rocblas/lib"", ""/opt/rocm/miopen/lib""],\n\t\t[""hip_hcc"", ""hiprtc"", ""hiprand"", ""rocblas""]\n\t)\n\n\ndef collectSources(path):\n\treturn collectCoreSources(path) + collectLibSources(path)\n\n\ndef main():\n\treturn buildDriver(debugmode=0, verbose=2)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Hip/ThirdParty/libmiopen.py,0,"b'import sys, ctypes\n\n\n_version_list = [1]\nif sys.platform == ""linux"":\n\t_libmiopen_libname_list = [(""libMIOpen.so.%s"" % v, v) for v in _version_list]\nelse:\n\traise RuntimeError(""Unsupported platform for MIOpen"")\n\n\n_libmiopen, version = None, None\nfor _libmiopen_libname, v in _libmiopen_libname_list:\n\ttry:\n\t\t_libmiopen = ctypes.cdll.LoadLibrary(_libmiopen_libname)\n\t\tversion = v\n\texcept OSError:\n\t\tpass\n\telse:\n\t\tbreak\nif _libmiopen is None:\n\traise OSError(""MIOpen library not found (searched for following version(s): %s)"" % _version_list)\n\n\nclass miopenError(Exception):\n\tpass\n\nclass miopenStatusNotInitialized(miopenError):\n\tpass\n\nclass miopenStatusInvalidValue(miopenError):\n\tpass\n\nclass miopenStatusBadParam(miopenError):\n\tpass\n\nclass miopenStatusAllocFailed(miopenError):\n\tpass\n\nclass miopenStatusInternalError(miopenError):\n\tpass\n\nclass miopenStatusNotImplemented(miopenError):\n\tpass\n\nclass miopenStatusUnknownError(miopenError):\n\tpass\n\nclass miopenStatusUnsupportedOp(miopenError):\n\tpass\n\n\nmiopenExceptions = {\n\t1: miopenStatusNotInitialized,\n\t2: miopenStatusInvalidValue,\n\t3: miopenStatusBadParam,\n\t4: miopenStatusAllocFailed,\n\t5: miopenStatusInternalError,\n\t6: miopenStatusNotImplemented,\n\t7: miopenStatusUnknownError,\n\t8: miopenStatusUnsupportedOp\n}\n\n\nmiopenDataType = {\n\t""miopenHalf"": 0,\n\t""miopenFloat"": 1\n}\n\nmiopenConvolutionMode = {\n\t""miopenConvolution"": 0,\n\t""miopenTranspose"": 1,\n\t""miopenGroupConv"": 2,\n\t""miopenDepthWise"": 3\n}\n\nmiopenConvFwdAlgorithm = {\n\t""miopenConvolutionFwdAlgoGEMM"": 0,\n\t""miopenConvolutionFwdAlgoDirect"": 1,\n\t""miopenConvolutionFwdAlgoFFT"": 2,\n\t""miopenConvolutionFwdAlgoWinograd"": 3,\n\t""miopenConvolutionFwdAlgoImplicitGEMM"": 5,\n\t""miopenConvolutionFwdAlgoStaticCompiledGEMM"": 6\n}\n\nmiopenConvBwdWeightsAlgorithm = {\n\t""miopenConvolutionBwdWeightsAlgoGEMM"": 0,\n\t""miopenConvolutionBwdWeightsAlgoDirect"": 1,\n\t""miopenConvolutionBwdWeightsAlgoWinograd"": 3,\n\t""miopenConvolutionBwdWeightsAlgoImplicitGEMM"": 5\n}\n\nmiopenConvBwdDataAlgorithm = {\n\t""miopenConvolutionBwdDataAlgoGEMM"": 0,\n\t""miopenConvolutionBwdDataAlgoDirect"": 1,\n\t""miopenConvolutionBwdDataAlgoFFT"": 2,\n\t""miopenConvolutionBwdDataAlgoWinograd"": 3,\n\t""miopenTransposeBwdDataAlgoGEMM"": 4,\n\t""miopenConvolutionBwdDataAlgoImplicitGEMM"": 5\n}\n\nclass miopenConvAlgoPerf(ctypes.Structure):\n\t_fields_ = [\n\t\t(""algo"", ctypes.c_int),\n\t\t(""time"", ctypes.c_float),\n\t\t(""memory"", ctypes.c_size_t)\n\t]\n\nmiopenPoolingMode = {\n\t""miopenPoolingMax"": 0,\n\t""miopenPoolingAverage"": 1,\n\t""miopenPoolingAverageInclusive"": 2\n}\n\nmiopenSoftmaxAlgorithm = {\n\t""MIOPEN_SOFTMAX_FAST"": 0,\n\t""MIOPEN_SOFTMAX_ACCURATE"": 1,\n\t""MIOPEN_SOFTMAX_LOG"": 2\n}\n\nmiopenSoftmaxMode = {\n\t""MIOPEN_SOFTMAX_MODE_INSTANCE"": 0,\n\t""MIOPEN_SOFTMAX_MODE_CHANNEL"": 1\n}\n\nmiopenBatchNormMode = {\n\t""miopenBNPerActivation"": 0,\n\t""miopenBNSpatial"": 1\n}\n\nmiopenLRNMode = {\n\t""miopenLRNWithinChannel"": 0,\n\t""miopenLRNCrossChannel"": 1\n}\n\nmiopenRNNMode = {\n\t""miopenRNNRELU"": 0,\n\t""miopenRNNTANH"": 1,\n\t""miopenLSTM"": 2,\n\t""miopenGRU"": 3\n}\n\nmiopenRNNInputMode = {\n\t""miopenRNNlinear"": 0,\n\t""miopenRNNskip"": 1\n}\n\nmiopenRNNAlgo = {\n\t""miopenRNNdefault"": 0,\n\t""miopenRNNfundamental"": 1\n}\n\nmiopenRNNDirectionMode = {\n\t""miopenRNNunidirection"": 0,\n\t""miopenRNNbidirection"": 1\n}\n\nmiopenRNNBiasMode = {\n\t""miopenRNNNoBias"": 0,\n\t""miopenRNNwithBias"": 1\n}\n\n\ndef miopenCheckStatus(status):\n\tif status != 0:\n\t\ttry:\n\t\t\traise miopenExceptions[status]\n\t\texcept KeyError:\n\t\t\traise miopenError\n\n\n_libmiopen.miopenCreate.restype = int\n_libmiopen.miopenCreate.argtypes = [ctypes.c_void_p]\ndef miopenCreate():\n\thandle = ctypes.c_void_p()\n\tstatus = _libmiopen.miopenCreate(ctypes.byref(handle))\n\tmiopenCheckStatus(status)\n\n\treturn handle.value\n\n\n_libmiopen.miopenDestroy.restype = int\n_libmiopen.miopenDestroy.argtypes = [ctypes.c_void_p]\ndef miopenDestroy(handle):\n\tstatus = _libmiopen.miopenDestroy(ctypes.c_void_p(handle))\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenCreateTensorDescriptor.restype = int\n_libmiopen.miopenCreateTensorDescriptor.argtypes = [ctypes.c_void_p]\ndef miopenCreateTensorDescriptor():\n\ttensorDesc = ctypes.c_void_p()\n\tstatus = _libmiopen.miopenCreateTensorDescriptor(ctypes.byref(tensorDesc))\n\tmiopenCheckStatus(status)\n\n\treturn tensorDesc.value\n\n\n_libmiopen.miopenSetTensorDescriptor.restype = int\n_libmiopen.miopenSetTensorDescriptor.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.c_int, ctypes.POINTER(ctypes.c_int), ctypes.POINTER(ctypes.c_int)\n]\ndef miopenSetTensorDescriptor(tensorDesc, dataType, dims, strides):\n\tnbDims = len(dims)\n\tdimA, strideA = (ctypes.c_int * nbDims)(*dims), (ctypes.c_int * nbDims)(*strides)\n\n\tstatus = _libmiopen.miopenSetTensorDescriptor(tensorDesc, dataType, nbDims, dimA, strideA)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenDestroyTensorDescriptor.restype = int\n_libmiopen.miopenDestroyTensorDescriptor.argtypes = [ctypes.c_void_p]\ndef miopenDestroyTensorDescriptor(tensorDesc):\n\tstatus = _libmiopen.miopenDestroyTensorDescriptor(tensorDesc)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenCreateConvolutionDescriptor.restype = int\n_libmiopen.miopenCreateConvolutionDescriptor.argtypes = [ctypes.c_void_p]\ndef miopenCreateConvolutionDescriptor():\n\tconvDesc = ctypes.c_void_p()\n\n\tstatus = _libmiopen.miopenCreateConvolutionDescriptor(ctypes.byref(convDesc))\n\tmiopenCheckStatus(status)\n\n\treturn convDesc\n\n\n_libmiopen.miopenInitConvolutionNdDescriptor.restype = int\n_libmiopen.miopenInitConvolutionNdDescriptor.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.POINTER(ctypes.c_int), ctypes.POINTER(ctypes.c_int),\n\tctypes.POINTER(ctypes.c_int), ctypes.c_int\n]\ndef miopenInitConvolutionNdDescriptor(convDesc, pad, stride, dilation, mode):\n\tnbDims = len(pad)\n\tpadA, strideA = (ctypes.c_int * nbDims)(*pad), (ctypes.c_int * nbDims)(*stride)\n\tdilationA = (ctypes.c_int * nbDims)(*dilation)\n\n\tstatus = _libmiopen.miopenInitConvolutionNdDescriptor(convDesc, nbDims, padA, strideA, dilationA, mode)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenSetConvolutionGroupCount.restype = int\n_libmiopen.miopenSetConvolutionGroupCount.argtypes = [ctypes.c_void_p, ctypes.c_int]\ndef miopenSetConvolutionGroupCount(convDesc, groupCount):\n\tstatus = _libmiopen.miopenSetConvolutionGroupCount(convDesc, groupCount)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenGetConvolutionNdForwardOutputDim.restype = int\n_libmiopen.miopenGetConvolutionNdForwardOutputDim.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p\n]\ndef miopenGetConvolutionNdForwardOutputDim(convDesc, inputTensorDesc, filterDesc, nbDims):\n\tnDim = ctypes.c_int()\n\ttensorOutputDimA = (ctypes.c_int * nbDims)()\n\n\tstatus = _libmiopen.miopenGetConvolutionNdForwardOutputDim(\n\t\tconvDesc, inputTensorDesc, filterDesc, ctypes.byref(nDim), tensorOutputDimA\n\t)\n\tmiopenCheckStatus(status)\n\n\treturn tuple(tensorOutputDimA)\n\n\n_libmiopen.miopenDestroyConvolutionDescriptor.restype = int\n_libmiopen.miopenDestroyConvolutionDescriptor.argtypes = [ctypes.c_void_p]\ndef miopenDestroyConvolutionDescriptor(convDesc):\n\tstatus = _libmiopen.miopenDestroyConvolutionDescriptor(convDesc)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenConvolutionForwardGetWorkSpaceSize.restype = int\n_libmiopen.miopenConvolutionForwardGetWorkSpaceSize.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p\n]\ndef miopenConvolutionForwardGetWorkSpaceSize(handle, wDesc, xDesc, convDesc, yDesc):\n\tworkspaceSize = ctypes.c_size_t()\n\n\tstatus = _libmiopen.miopenConvolutionForwardGetWorkSpaceSize(\n\t\thandle, wDesc, xDesc, convDesc, yDesc, ctypes.byref(workspaceSize)\n\t)\n\tmiopenCheckStatus(status)\n\n\treturn workspaceSize.value\n\n\n_libmiopen.miopenFindConvolutionForwardAlgorithm.restype = int\n_libmiopen.miopenFindConvolutionForwardAlgorithm.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.POINTER(miopenConvAlgoPerf),\n\tctypes.c_void_p, ctypes.c_size_t, ctypes.c_bool\n]\ndef miopenFindConvolutionForwardAlgorithm(handle, xDesc, x, wDesc, w, convDesc, yDesc, y, requestAlgoCount,\n\t\t\t\t\t\t\t\t\t\t  workSpace, workSpaceSize, exhaustiveSearch):\n\treturnedAlgoCount = ctypes.c_int()\n\tperfResults = (miopenConvAlgoPerf * requestAlgoCount)()\n\n\tstatus = _libmiopen.miopenFindConvolutionForwardAlgorithm(\n\t\thandle, xDesc, x, wDesc, w, convDesc, yDesc, y, requestAlgoCount, ctypes.byref(returnedAlgoCount),\n\t\tperfResults, workSpace, workSpaceSize, exhaustiveSearch\n\t)\n\tmiopenCheckStatus(status)\n\n\treturn perfResults[:returnedAlgoCount.value]\n\n\n_libmiopen.miopenConvolutionForward.restype = int\n_libmiopen.miopenConvolutionForward.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_size_t\n]\ndef miopenConvolutionForward(handle, alpha, xDesc, x, wDesc, w, convDesc, algo, beta, yDesc, y, workSpace,\n\t\t\t\t\t\t\t workSpaceSize):\n\talphaRef, betaRef = ctypes.byref(ctypes.c_float(alpha)), ctypes.byref(ctypes.c_float(beta))\n\n\tstatus = _libmiopen.miopenConvolutionForward(\n\t\thandle, alphaRef, xDesc, x, wDesc, w, convDesc, algo, betaRef, yDesc, y, workSpace, workSpaceSize\n\t)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenConvolutionForwardBias.restype = int\n_libmiopen.miopenConvolutionForwardBias.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p\n]\ndef miopenConvolutionForwardBias(handle, alpha, bDesc, b, beta, yDesc, y):\n\talphaRef, betaRef = ctypes.byref(ctypes.c_float(alpha)), ctypes.byref(ctypes.c_float(beta))\n\n\tstatus = _libmiopen.miopenConvolutionForwardBias(handle, alphaRef, bDesc, b, betaRef, yDesc, y)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenConvolutionBackwardDataGetWorkSpaceSize.restype = int\n_libmiopen.miopenConvolutionBackwardDataGetWorkSpaceSize.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p\n]\ndef miopenConvolutionBackwardDataGetWorkSpaceSize(handle, dyDesc, wDesc, convDesc, dxDesc):\n\tworkspaceSize = ctypes.c_size_t()\n\n\tstatus = _libmiopen.miopenConvolutionBackwardDataGetWorkSpaceSize(\n\t\thandle, dyDesc, wDesc, convDesc, dxDesc, ctypes.byref(workspaceSize)\n\t)\n\tmiopenCheckStatus(status)\n\n\treturn workspaceSize.value\n\n\n_libmiopen.miopenFindConvolutionBackwardDataAlgorithm.restype = int\n_libmiopen.miopenFindConvolutionBackwardDataAlgorithm.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.POINTER(miopenConvAlgoPerf),\n\tctypes.c_void_p, ctypes.c_size_t, ctypes.c_bool\n]\ndef miopenFindConvolutionBackwardDataAlgorithm(handle, dyDesc, dy, wDesc, w, convDesc, dxDesc, dx, requestAlgoCount,\n\t\t\t\t\t\t\t\t\t\t\t   workSpace, workSpaceSize, exhaustiveSearch):\n\treturnedAlgoCount = ctypes.c_int()\n\tperfResults = (miopenConvAlgoPerf * requestAlgoCount)()\n\n\tstatus = _libmiopen.miopenFindConvolutionBackwardDataAlgorithm(\n\t\thandle, dyDesc, dy, wDesc, w, convDesc, dxDesc, dx, requestAlgoCount, ctypes.byref(returnedAlgoCount),\n\t\tperfResults, workSpace, workSpaceSize, exhaustiveSearch\n\t)\n\tmiopenCheckStatus(status)\n\n\treturn perfResults[:returnedAlgoCount.value]\n\n\n_libmiopen.miopenConvolutionBackwardData.restype = int\n_libmiopen.miopenConvolutionBackwardData.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_size_t\n]\ndef miopenConvolutionBackwardData(handle, alpha, dyDesc, dy, wDesc, w, convDesc, algo, beta, dxDesc, dx, workSpace,\n\t\t\t\t\t\t\t\t  workSpaceSize):\n\talphaRef, betaRef = ctypes.byref(ctypes.c_float(alpha)), ctypes.byref(ctypes.c_float(beta))\n\n\tstatus = _libmiopen.miopenConvolutionBackwardData(\n\t\thandle, alphaRef, dyDesc, dy, wDesc, w, convDesc, algo, betaRef, dxDesc, dx, workSpace, workSpaceSize\n\t)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenConvolutionBackwardWeightsGetWorkSpaceSize.restype = int\n_libmiopen.miopenConvolutionBackwardWeightsGetWorkSpaceSize.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p\n]\ndef miopenConvolutionBackwardWeightsGetWorkSpaceSize(handle, dyDesc, xDesc, convDesc, dwDesc):\n\tworkspaceSize = ctypes.c_size_t()\n\n\tstatus = _libmiopen.miopenConvolutionBackwardWeightsGetWorkSpaceSize(\n\t\thandle, dyDesc, xDesc, convDesc, dwDesc, ctypes.byref(workspaceSize)\n\t)\n\tmiopenCheckStatus(status)\n\n\treturn workspaceSize.value\n\n\n_libmiopen.miopenFindConvolutionBackwardWeightsAlgorithm.restype = int\n_libmiopen.miopenFindConvolutionBackwardWeightsAlgorithm.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.POINTER(miopenConvAlgoPerf),\n\tctypes.c_void_p, ctypes.c_size_t, ctypes.c_bool\n]\ndef miopenFindConvolutionBackwardWeightsAlgorithm(handle, dyDesc, dy, xDesc, x, convDesc, dwDesc, dw, requestAlgoCount,\n\t\t\t\t\t\t\t\t\t\t\t\t  workSpace, workSpaceSize, exhaustiveSearch):\n\treturnedAlgoCount = ctypes.c_int()\n\tperfResults = (miopenConvAlgoPerf * requestAlgoCount)()\n\n\tstatus = _libmiopen.miopenFindConvolutionBackwardWeightsAlgorithm(\n\t\thandle, dyDesc, dy, xDesc, x, convDesc, dwDesc, dw, requestAlgoCount, ctypes.byref(returnedAlgoCount),\n\t\tperfResults, workSpace, workSpaceSize, exhaustiveSearch\n\t)\n\tmiopenCheckStatus(status)\n\n\treturn perfResults[:returnedAlgoCount.value]\n\n\n_libmiopen.miopenConvolutionBackwardWeights.restype = int\n_libmiopen.miopenConvolutionBackwardWeights.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_size_t\n]\ndef miopenConvolutionBackwardWeights(handle, alpha, dyDesc, dy, xDesc, x, convDesc, algo, beta, dwDesc, dw, workSpace,\n\t\t\t\t\t\t\t\t\t workSpaceSize):\n\talphaRef, betaRef = ctypes.byref(ctypes.c_float(alpha)), ctypes.byref(ctypes.c_float(beta))\n\n\tstatus = _libmiopen.miopenConvolutionBackwardWeights(\n\t\thandle, alphaRef, dyDesc, dy, xDesc, x, convDesc, algo, betaRef, dwDesc, dw, workSpace, workSpaceSize\n\t)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenConvolutionBackwardBias.restype = int\n_libmiopen.miopenConvolutionBackwardBias.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p\n]\ndef miopenConvolutionBackwardBias(handle, alpha, dyDesc, dy, beta, dbDesc, db):\n\talphaRef, betaRef = ctypes.byref(ctypes.c_float(alpha)), ctypes.byref(ctypes.c_float(beta))\n\n\tstatus = _libmiopen.miopenConvolutionBackwardBias(handle, alphaRef, dyDesc, dy, betaRef, dbDesc, db)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenCreatePoolingDescriptor.restype = int\n_libmiopen.miopenCreatePoolingDescriptor.argtypes = [ctypes.c_void_p]\ndef miopenCreatePoolingDescriptor():\n\tpoolDesc = ctypes.c_void_p()\n\n\tstatus = _libmiopen.miopenCreatePoolingDescriptor(ctypes.byref(poolDesc))\n\tmiopenCheckStatus(status)\n\n\treturn poolDesc\n\n\n_libmiopen.miopenSet2dPoolingDescriptor.restype = int\n_libmiopen.miopenSet2dPoolingDescriptor.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_int\n]\ndef miopenSet2dPoolingDescriptor(poolDesc, mode, windowHeight, windowWidth, pad_h, pad_w, u, v):\n\tstatus = _libmiopen.miopenSet2dPoolingDescriptor(poolDesc, mode, windowHeight, windowWidth, pad_h, pad_w, u, v)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenPoolingGetWorkSpaceSize.restype = int\n_libmiopen.miopenPoolingGetWorkSpaceSize.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\ndef miopenPoolingGetWorkSpaceSize(yDesc):\n\tworkSpaceSize = ctypes.c_size_t()\n\n\tstatus = _libmiopen.miopenPoolingGetWorkSpaceSize(yDesc, ctypes.byref(workSpaceSize))\n\tmiopenCheckStatus(status)\n\n\treturn workSpaceSize.value\n\n\n_libmiopen.miopenPoolingForward.restype = int\n_libmiopen.miopenPoolingForward.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_bool, ctypes.c_void_p, ctypes.c_size_t\n]\ndef miopenPoolingForward(handle, poolDesc, alpha, xDesc, x, beta, yDesc, y, do_backward, workSpace, workSpaceSize):\n\talphaRef, betaRef = ctypes.byref(ctypes.c_float(alpha)), ctypes.byref(ctypes.c_float(beta))\n\n\tstatus = _libmiopen.miopenPoolingForward(\n\t\thandle, poolDesc, alphaRef, xDesc, x, betaRef, yDesc, y, do_backward, workSpace, workSpaceSize\n\t)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenPoolingBackward.restype = int\n_libmiopen.miopenPoolingBackward.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p\n]\ndef miopenPoolingBackward(handle, poolDesc, alpha, yDesc, y, dyDesc, dy, xDesc, x, beta, dxDesc, dx, workSpace):\n\talphaRef, betaRef = ctypes.byref(ctypes.c_float(alpha)), ctypes.byref(ctypes.c_float(beta))\n\n\tstatus = _libmiopen.miopenPoolingBackward(\n\t\thandle, poolDesc, alphaRef, yDesc, y, dyDesc, dy, xDesc, x, betaRef, dxDesc, dx, workSpace\n\t)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenDestroyPoolingDescriptor.restype = int\n_libmiopen.miopenDestroyPoolingDescriptor.argtypes = [ctypes.c_void_p]\ndef miopenDestroyPoolingDescriptor(poolDesc):\n\tstatus = _libmiopen.miopenDestroyPoolingDescriptor(poolDesc)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenSoftmaxForward_V2.restype = int\n_libmiopen.miopenSoftmaxForward_V2.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_int, ctypes.c_int\n]\ndef miopenSoftmaxForward(handle, alpha, xDesc, x, beta, yDesc, y, algorithm, mode):\n\talphaRef, betaRef = ctypes.byref(ctypes.c_float(alpha)), ctypes.byref(ctypes.c_float(beta))\n\n\tstatus = _libmiopen.miopenSoftmaxForward_V2(handle, alphaRef, xDesc, x, betaRef, yDesc, y, algorithm, mode)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenSoftmaxBackward_V2.restype = int\n_libmiopen.miopenSoftmaxBackward_V2.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.c_int\n]\ndef miopenSoftmaxBackward(handle, alpha, yDesc, y, dyDesc, dy, beta, dxDesc, dx, algorithm, mode):\n\talphaRef, betaRef = ctypes.byref(ctypes.c_float(alpha)), ctypes.byref(ctypes.c_float(beta))\n\n\tstatus = _libmiopen.miopenSoftmaxBackward_V2(\n\t\thandle, alphaRef, yDesc, y, dyDesc, dy, betaRef, dxDesc, dx, algorithm, mode\n\t)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenBatchNormalizationForwardTraining.restype = int\n_libmiopen.miopenBatchNormalizationForwardTraining.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_double, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_double, ctypes.c_void_p, ctypes.c_void_p\n]\ndef miopenBatchNormalizationForwardTraining(handle, bn_mode, alpha, beta, xDesc, x, yDesc, y, bnScaleBiasMeanVarDesc,\n\t\t\t\t\t\t\t\t\t\t\tbnScale, bnBias, expAvgFactor, resultRunningMean, resultRunningVariance,\n\t\t\t\t\t\t\t\t\t\t\tepsilon, resultSaveMean, resultSaveInvVariance):\n\talphaRef, betaRef = ctypes.byref(ctypes.c_float(alpha)), ctypes.byref(ctypes.c_float(beta))\n\n\tstatus = _libmiopen.miopenBatchNormalizationForwardTraining(\n\t\thandle, bn_mode, alphaRef, betaRef, xDesc, x, yDesc, y, bnScaleBiasMeanVarDesc, bnScale, bnBias, expAvgFactor,\n\t\tresultRunningMean, resultRunningVariance, epsilon, resultSaveMean, resultSaveInvVariance\n\t)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenBatchNormalizationForwardInference.restype = int\n_libmiopen.miopenBatchNormalizationForwardInference.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_double\n]\ndef miopenBatchNormalizationForwardInference(handle, bn_mode, alpha, beta, xDesc, x, yDesc, y, bnScaleBiasMeanVarDesc,\n\t\t\t\t\t\t\t\t\t\t\t bnScale, bnBias, estimatedMean, estimatedVariance, epsilon):\n\talphaRef, betaRef = ctypes.byref(ctypes.c_float(alpha)), ctypes.byref(ctypes.c_float(beta))\n\n\tstatus = _libmiopen.miopenBatchNormalizationForwardInference(\n\t\thandle, bn_mode, alphaRef, betaRef, xDesc, x, yDesc, y, bnScaleBiasMeanVarDesc, bnScale, bnBias, estimatedMean,\n\t\testimatedVariance, epsilon\n\t)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenBatchNormalizationBackward.restype = int\n_libmiopen.miopenBatchNormalizationBackward.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_double, ctypes.c_void_p, ctypes.c_void_p\n]\ndef miopenBatchNormalizationBackward(handle, bn_mode, alphaDataDiff, betaDataDiff, alphaParmDiff, betaParmDiff,\n\t\t\t\t\t\t\t\t\t xDesc, x, dyDesc, dy, dxDesc, dx, bnScaleBiasDiffDesc, bnScale, resultBnScaleDiff,\n\t\t\t\t\t\t\t\t\t resultBnBiasDiff, epsilon, savedMean, savedInvVariance):\n\talphaDataRef, betaDataRef = ctypes.byref(ctypes.c_float(alphaDataDiff)), ctypes.byref(ctypes.c_float(betaDataDiff))\n\talphaParmRef, betaParmRef = ctypes.byref(ctypes.c_float(alphaParmDiff)), ctypes.byref(ctypes.c_float(betaParmDiff))\n\n\tstatus = _libmiopen.miopenBatchNormalizationBackward(\n\t\thandle, bn_mode, alphaDataRef, betaDataRef, alphaParmRef, betaParmRef, xDesc, x, dyDesc, dy, dxDesc, dx,\n\t\tbnScaleBiasDiffDesc, bnScale, resultBnScaleDiff, resultBnBiasDiff, epsilon, savedMean, savedInvVariance\n\t)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenCreateLRNDescriptor.restype = int\n_libmiopen.miopenCreateLRNDescriptor.argtypes = [ctypes.c_void_p]\ndef miopenCreateLRNDescriptor():\n\tlrnDesc = ctypes.c_void_p()\n\n\tstatus = _libmiopen.miopenCreateLRNDescriptor(ctypes.byref(lrnDesc))\n\tmiopenCheckStatus(status)\n\n\treturn lrnDesc\n\n\n_libmiopen.miopenSetLRNDescriptor.restype = int\n_libmiopen.miopenSetLRNDescriptor.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.c_uint, ctypes.c_double, ctypes.c_double, ctypes.c_double\n]\ndef miopenSetLRNDescriptor(lrnDesc, mode, lrnN, lrnAlpha, lrnBeta, lrnK):\n\tstatus = _libmiopen.miopenSetLRNDescriptor(lrnDesc, mode, lrnN, lrnAlpha, lrnBeta, lrnK)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenLRNGetWorkSpaceSize.restype = int\n_libmiopen.miopenLRNGetWorkSpaceSize.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\ndef miopenLRNGetWorkSpaceSize(yDesc):\n\tworkSpaceSize = ctypes.c_size_t()\n\n\tstatus = _libmiopen.miopenLRNGetWorkSpaceSize(yDesc, ctypes.byref(workSpaceSize))\n\tmiopenCheckStatus(status)\n\n\treturn workSpaceSize.value\n\n\n_libmiopen.miopenLRNForward.restype = int\n_libmiopen.miopenLRNForward.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_bool, ctypes.c_void_p\n]\ndef miopenLRNForward(handle, lrnDesc, alpha, xDesc, x, beta, yDesc, y, do_backward, workSpace):\n\talphaRef, betaRef = ctypes.byref(ctypes.c_float(alpha)), ctypes.byref(ctypes.c_float(beta))\n\n\tstatus = _libmiopen.miopenLRNForward(handle, lrnDesc, alphaRef, xDesc, x, betaRef, yDesc, y, do_backward, workSpace)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenLRNBackward.restype = int\n_libmiopen.miopenLRNBackward.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p\n]\ndef miopenLRNBackward(handle, lrnDesc, alpha, yDesc, y, dyDesc, dy, xDesc, x, beta, dxDesc, dx, workSpace):\n\talphaRef, betaRef = ctypes.byref(ctypes.c_float(alpha)), ctypes.byref(ctypes.c_float(beta))\n\n\tstatus = _libmiopen.miopenLRNBackward(\n\t\thandle, lrnDesc, alphaRef, yDesc, y, dyDesc, dy, xDesc, x, betaRef, dxDesc, dx, workSpace\n\t)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenDestroyLRNDescriptor.restype = int\n_libmiopen.miopenDestroyLRNDescriptor.argtypes = [ctypes.c_void_p]\ndef miopenDestroyLRNDescriptor(lrnDesc):\n\tstatus = _libmiopen.miopenDestroyLRNDescriptor(lrnDesc)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenCreateRNNDescriptor.restype = int\n_libmiopen.miopenCreateRNNDescriptor.argtypes = [ctypes.c_void_p]\ndef miopenCreateRNNDescriptor():\n\trnnDesc = ctypes.c_void_p()\n\n\tstatus = _libmiopen.miopenCreateRNNDescriptor(ctypes.byref(rnnDesc))\n\tmiopenCheckStatus(status)\n\n\treturn rnnDesc.value\n\n\n_libmiopen.miopenSetRNNDescriptor.restype = int\n_libmiopen.miopenSetRNNDescriptor.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_int,\n\tctypes.c_int\n]\ndef miopenSetRNNDescriptor(rnnDesc, hsize, nlayers, inMode, direction, rnnMode, biasMode, algo, dataType):\n\tstatus = _libmiopen.miopenSetRNNDescriptor(\n\t\trnnDesc, hsize, nlayers, inMode, direction, rnnMode, biasMode, algo, dataType\n\t)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenDestroyRNNDescriptor.restype = int\n_libmiopen.miopenDestroyRNNDescriptor.argtypes = [ctypes.c_void_p]\ndef miopenDestroyRNNDescriptor(rnnDesc):\n\tstatus = _libmiopen.miopenDestroyRNNDescriptor(rnnDesc)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenGetRNNParamsSize.restype = int\n_libmiopen.miopenGetRNNParamsSize.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int\n]\ndef miopenGetRNNParamsSize(handle, rnnDesc, xDesc, dtype):\n\tnumBytes = ctypes.c_size_t()\n\n\tstatus = _libmiopen.miopenGetRNNParamsSize(handle, rnnDesc, xDesc, ctypes.byref(numBytes), dtype)\n\tmiopenCheckStatus(status)\n\n\treturn numBytes.value\n\n\n_libmiopen.miopenGetRNNLayerParamSize.restype = int\n_libmiopen.miopenGetRNNLayerParamSize.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p\n]\ndef miopenGetRNNLayerParamSize(handle, rnnDesc, layer, xDesc, paramID):\n\tnumBytes = ctypes.c_size_t()\n\n\tstatus = _libmiopen.miopenGetRNNLayerParamSize(handle, rnnDesc, layer, xDesc, paramID, ctypes.byref(numBytes))\n\tmiopenCheckStatus(status)\n\n\treturn numBytes.value\n\n\n_libmiopen.miopenGetRNNLayerBiasSize.restype = int\n_libmiopen.miopenGetRNNLayerBiasSize.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.c_int, ctypes.c_void_p\n]\ndef miopenGetRNNLayerBiasSize(handle, rnnDesc, layer, biasID):\n\tnumBytes = ctypes.c_size_t()\n\n\tstatus = _libmiopen.miopenGetRNNLayerBiasSize(handle, rnnDesc, layer, biasID, ctypes.byref(numBytes))\n\tmiopenCheckStatus(status)\n\n\treturn numBytes.value\n\n\n_libmiopen.miopenGetRNNLayerParam.restype = int\n_libmiopen.miopenGetRNNLayerParam.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int,\n\tctypes.c_void_p, ctypes.c_void_p\n]\ndef miopenGetRNNLayerParam(handle, rnnDesc, layer, xDesc, wDesc, w, paramID, paramDesc, layerParam):\n\tstatus = _libmiopen.miopenGetRNNLayerParam(handle, rnnDesc, layer, xDesc, wDesc, w, paramID, paramDesc, layerParam)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenGetRNNLayerBias.restype = int\n_libmiopen.miopenGetRNNLayerBias.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int,\n\tctypes.c_void_p, ctypes.c_void_p\n]\ndef miopenGetRNNLayerBias(handle, rnnDesc, layer, xDesc, wDesc, w, biasID, biasDesc, layerBias):\n\tstatus = _libmiopen.miopenGetRNNLayerBias(handle, rnnDesc, layer, xDesc, wDesc, w, biasID, biasDesc, layerBias)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenSetRNNLayerParam.restype = int\n_libmiopen.miopenSetRNNLayerParam.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int,\n\tctypes.c_void_p, ctypes.c_void_p\n]\ndef miopenSetRNNLayerParam(handle, rnnDesc, layer, xDesc, wDesc, w, paramID, paramDesc, layerParam):\n\tstatus = _libmiopen.miopenSetRNNLayerParam(handle, rnnDesc, layer, xDesc, wDesc, w, paramID, paramDesc, layerParam)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenSetRNNLayerBias.restype = int\n_libmiopen.miopenSetRNNLayerBias.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int,\n\tctypes.c_void_p, ctypes.c_void_p\n]\ndef miopenSetRNNLayerBias(handle, rnnDesc, layer, xDesc, wDesc, w, biasID, biasDesc, layerBias):\n\tstatus = _libmiopen.miopenSetRNNLayerBias(handle, rnnDesc, layer, xDesc, wDesc, w, biasID, biasDesc, layerBias)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenGetRNNWorkspaceSize.restype = int\n_libmiopen.miopenGetRNNWorkspaceSize.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.POINTER(ctypes.c_void_p), ctypes.c_void_p\n]\ndef miopenGetRNNWorkspaceSize(handle, rnnDesc, sequenceLen, xDesc):\n\tnumBytes = ctypes.c_size_t()\n\txDesc = (ctypes.c_void_p * len(xDesc))(*xDesc)\n\n\tstatus = _libmiopen.miopenGetRNNWorkspaceSize(handle, rnnDesc, sequenceLen, xDesc, ctypes.byref(numBytes))\n\tmiopenCheckStatus(status)\n\n\treturn numBytes.value\n\n\n_libmiopen.miopenGetRNNTrainingReserveSize.restype = int\n_libmiopen.miopenGetRNNTrainingReserveSize.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.POINTER(ctypes.c_void_p), ctypes.c_void_p\n]\ndef miopenGetRNNTrainingReserveSize(handle, rnnDesc, sequenceLen, xDesc):\n\tnumBytes = ctypes.c_size_t()\n\txDesc = (ctypes.c_void_p * len(xDesc))(*xDesc)\n\n\tstatus = _libmiopen.miopenGetRNNTrainingReserveSize(handle, rnnDesc, sequenceLen, xDesc, ctypes.byref(numBytes))\n\tmiopenCheckStatus(status)\n\n\treturn numBytes.value\n\n\n_libmiopen.miopenRNNForwardTraining.restype = int\n_libmiopen.miopenRNNForwardTraining.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.POINTER(ctypes.c_void_p), ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.POINTER(ctypes.c_void_p), ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_size_t, ctypes.c_void_p, ctypes.c_size_t\n]\ndef miopenRNNForwardTraining(handle, rnnDesc, seqLen, xDesc, x, hxDesc, hx, cxDesc, cx, wDesc, w, yDesc, y, hyDesc, hy,\n\t\t\t\t\t\t\t cyDesc, cy, workSpace, workSpaceNumBytes, reserveSpace, reserveSpaceNumBytes):\n\txDesc, yDesc = (ctypes.c_void_p * len(xDesc))(*xDesc), (ctypes.c_void_p * len(yDesc))(*yDesc)\n\n\tstatus = _libmiopen.miopenRNNForwardTraining(\n\t\thandle, rnnDesc, seqLen, xDesc, x, hxDesc, hx, cxDesc, cx, wDesc, w, yDesc, y, hyDesc, hy, cyDesc, cy,\n\t\tworkSpace, workSpaceNumBytes, reserveSpace, reserveSpaceNumBytes\n\t)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenRNNBackwardData.restype = int\n_libmiopen.miopenRNNBackwardData.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.POINTER(ctypes.c_void_p), ctypes.c_void_p,\n\tctypes.POINTER(ctypes.c_void_p), ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.POINTER(ctypes.c_void_p), ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_size_t, ctypes.c_void_p, ctypes.c_size_t\n]\ndef miopenRNNBackwardData(handle, rnnDesc, seqLen, yDesc, y, dyDesc, dy, dhyDesc, dhy, dcyDesc, dcy, wDesc, w,\n\t\t\t\t\t\t  hxDesc, hx, cxDesc, cx, dxDesc, dx, dhxDesc, dhx, dcxDesc, dcx, workSpace, workSpaceNumBytes,\n\t\t\t\t\t\t  reserveSpace, reserveSpaceNumBytes):\n\tyDesc = (ctypes.c_void_p * len(yDesc))(*yDesc)\n\tdyDesc, dxDesc = (ctypes.c_void_p * len(dyDesc))(*dyDesc), (ctypes.c_void_p * len(dxDesc))(*dxDesc)\n\n\tstatus = _libmiopen.miopenRNNBackwardData(\n\t\thandle, rnnDesc, seqLen, yDesc, y, dyDesc, dy, dhyDesc, dhy, dcyDesc, dcy, wDesc, w, hxDesc, hx, cxDesc, cx,\n\t\tdxDesc, dx, dhxDesc, dhx, dcxDesc, dcx, workSpace, workSpaceNumBytes, reserveSpace, reserveSpaceNumBytes\n\t)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenRNNBackwardWeights.restype = int\n_libmiopen.miopenRNNBackwardWeights.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.POINTER(ctypes.c_void_p), ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.POINTER(ctypes.c_void_p), ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_size_t, ctypes.c_void_p, ctypes.c_size_t\n]\ndef miopenRNNBackwardWeights(handle, rnnDesc, seqLen, xDesc, x, hxDesc, hx, yDesc, y, dwDesc, dw, workSpace,\n\t\t\t\t\t\t\t workSpaceNumBytes, reserveSpace, reserveSpaceNumBytes):\n\txDesc, yDesc = (ctypes.c_void_p * len(xDesc))(*xDesc), (ctypes.c_void_p * len(yDesc))(*yDesc)\n\n\tstatus = _libmiopen.miopenRNNBackwardWeights(\n\t\thandle, rnnDesc, seqLen, xDesc, x, hxDesc, hx, yDesc, y, dwDesc, dw, workSpace, workSpaceNumBytes,\n\t\treserveSpace, reserveSpaceNumBytes\n\t)\n\tmiopenCheckStatus(status)\n\n\n_libmiopen.miopenRNNForwardInference.restype = int\n_libmiopen.miopenRNNForwardInference.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.POINTER(ctypes.c_void_p), ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.POINTER(ctypes.c_void_p), ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_size_t\n]\ndef miopenRNNForwardInference(handle, rnnDesc, seqLen, xDesc, x, hxDesc, hx, cxDesc, cx, wDesc, w, yDesc, y, hyDesc, hy,\n\t\t\t\t\t\t\t  cyDesc, cy, workSpace, workSpaceNumBytes):\n\txDesc, yDesc = (ctypes.c_void_p * len(xDesc))(*xDesc), (ctypes.c_void_p * len(yDesc))(*yDesc)\n\n\tstatus = _libmiopen.miopenRNNForwardInference(\n\t\thandle, rnnDesc, seqLen, xDesc, x, hxDesc, hx, cxDesc, cx, wDesc, w, yDesc, y, hyDesc, hy, cyDesc, cy,\n\t\tworkSpace, workSpaceNumBytes\n\t)\n\tmiopenCheckStatus(status)\n'"
Hip/Wrappers/MIOpen.py,18,"b'import itertools\nfrom enum import Enum\n\nimport numpy as np\n\nfrom PuzzleLib.Cuda.Wrappers.CuDnn import conv2dTest, conv3dTest, convGroupTest\nfrom PuzzleLib.Cuda.Wrappers.CuDnn import deconv2dTest, deconv3dTest, deconvGroupTest, softmax2dTest\n\nfrom PuzzleLib.Hip.Driver import GPUArray\nfrom PuzzleLib.Hip.ThirdParty import libmiopen\n\n\nclass DataType(Enum):\n\tfloat = libmiopen.miopenDataType[""miopenFloat""]\n\thalf = libmiopen.miopenDataType[""miopenHalf""]\n\n\nclass ConvMode(Enum):\n\tconv = libmiopen.miopenConvolutionMode[""miopenConvolution""]\n\ttranspose = libmiopen.miopenConvolutionMode[""miopenTranspose""]\n\n\nclass ConvFwdAlgo(Enum):\n\tauto = -1\n\tgemm = libmiopen.miopenConvFwdAlgorithm[""miopenConvolutionFwdAlgoGEMM""]\n\tdirect = libmiopen.miopenConvFwdAlgorithm[""miopenConvolutionFwdAlgoDirect""]\n\tfft = libmiopen.miopenConvFwdAlgorithm[""miopenConvolutionFwdAlgoFFT""]\n\twinograd = libmiopen.miopenConvFwdAlgorithm[""miopenConvolutionFwdAlgoWinograd""]\n\timplicitGemm = libmiopen.miopenConvFwdAlgorithm[""miopenConvolutionFwdAlgoImplicitGEMM""]\n\tstaticGemm = libmiopen.miopenConvFwdAlgorithm[""miopenConvolutionFwdAlgoStaticCompiledGEMM""]\n\n\nclass ConvBwdFilterAlgo(Enum):\n\tauto = -1\n\tgemm = libmiopen.miopenConvBwdWeightsAlgorithm[""miopenConvolutionBwdWeightsAlgoGEMM""]\n\tdirect = libmiopen.miopenConvBwdWeightsAlgorithm[""miopenConvolutionBwdWeightsAlgoDirect""]\n\twinograd = libmiopen.miopenConvBwdWeightsAlgorithm[""miopenConvolutionBwdWeightsAlgoWinograd""]\n\timplicitGemm = libmiopen.miopenConvBwdWeightsAlgorithm[""miopenConvolutionBwdWeightsAlgoImplicitGEMM""]\n\n\nclass ConvBwdDataAlgo(Enum):\n\tauto = -1\n\tgemm = libmiopen.miopenConvBwdDataAlgorithm[""miopenConvolutionBwdDataAlgoGEMM""]\n\tdirect = libmiopen.miopenConvBwdDataAlgorithm[""miopenConvolutionBwdDataAlgoDirect""]\n\tfft = libmiopen.miopenConvBwdDataAlgorithm[""miopenConvolutionBwdDataAlgoFFT""]\n\twinograd = libmiopen.miopenConvBwdDataAlgorithm[""miopenConvolutionBwdDataAlgoWinograd""]\n\ttransposeGemm = libmiopen.miopenConvBwdDataAlgorithm[""miopenTransposeBwdDataAlgoGEMM""]\n\timplicitGemm = libmiopen.miopenConvBwdDataAlgorithm[""miopenConvolutionBwdDataAlgoImplicitGEMM""]\n\n\nclass PoolMode(Enum):\n\tmax = libmiopen.miopenPoolingMode[""miopenPoolingMax""]\n\tavgWithPad = libmiopen.miopenPoolingMode[""miopenPoolingAverageInclusive""]\n\tavgNoPad = libmiopen.miopenPoolingMode[""miopenPoolingAverage""]\n\n\nclass SoftMaxAlgo(Enum):\n\tfast = libmiopen.miopenSoftmaxAlgorithm[""MIOPEN_SOFTMAX_FAST""]\n\taccurate = libmiopen.miopenSoftmaxAlgorithm[""MIOPEN_SOFTMAX_ACCURATE""]\n\tlog = libmiopen.miopenSoftmaxAlgorithm[""MIOPEN_SOFTMAX_LOG""]\n\n\nclass SoftMaxMode(Enum):\n\tperActivation = libmiopen.miopenSoftmaxMode[""MIOPEN_SOFTMAX_MODE_INSTANCE""]\n\tspatial = libmiopen.miopenSoftmaxMode[""MIOPEN_SOFTMAX_MODE_CHANNEL""]\n\n\nclass BatchNormMode(Enum):\n\tperActivation = libmiopen.miopenBatchNormMode[""miopenBNPerActivation""]\n\tspatial = libmiopen.miopenBatchNormMode[""miopenBNSpatial""]\n\n\nclass LRNMode(Enum):\n\tmap = libmiopen.miopenLRNMode[""miopenLRNWithinChannel""]\n\tcross = libmiopen.miopenLRNMode[""miopenLRNCrossChannel""]\n\n\ntoDataType = {\n\tnp.float32: DataType.float,\n\tnp.float16: DataType.half\n}\n\n\nclass ConvPerf:\n\tdef __init__(self, algo, time, memory):\n\t\tself.algo = algo\n\t\tself.time = time\n\t\tself.memory = memory\n\n\n\tdef toString(self):\n\t\treturn ""%-40s %-25s %-28s"" % (\n\t\t\t""Algo %s"" % self.algo, ""time %.6f secs"" % self.time, ""memory %.6f mbytes"" % (self.memory / 1024**2)\n\t\t)\n\n\n\tdef __str__(self):\n\t\treturn self.toString()\n\n\n\tdef __repr__(self):\n\t\treturn self.toString()\n\n\nclass DescTensor:\n\t__slots__ = [""desc"", ""shape"", ""tensor"", ""ptr""]\n\n\tdef __init__(self, desc, shape, tensor):\n\t\tself.desc, self.shape, self.tensor = desc, shape, tensor\n\t\tself.ptr = None if tensor is None else tensor.ptr\n\n\nclass DescConvNd:\n\t__slots__ = [""desc"", ""mode"", ""stride"", ""pad"", ""dilation"", ""groups""]\n\n\tdef __init__(self, desc, mode, stride, pad, dilation, groups):\n\t\tself.desc, self.mode = desc, mode\n\t\tself.stride, self.pad, self.dilation, self.groups = stride, pad, dilation, groups\n\n\nclass DescPool2d:\n\t__slots__ = [""desc"", ""size"", ""stride"", ""pad""]\n\n\tdef __init__(self, desc, size, stride, pad):\n\t\tself.desc, self.size, self.stride, self.pad = desc, size, stride, pad\n\n\nclass DescLRN:\n\t__slots__ = [""desc"", ""N"", ""alpha"", ""beta"", ""K""]\n\n\tdef __init__(self, desc, N, alpha, beta, K):\n\t\tself.desc, self.N, self.alpha, self.beta, self.K = desc, N, alpha, beta, K\n\n\nclass DnnContext:\n\tdef __init__(self, backend):\n\t\tself.backend = backend\n\t\tself.context = libmiopen.miopenCreate()\n\n\t\tself.tensorDescCache = []\n\n\t\tself.convCache = {}\n\t\tself.convBackwardDataCache = {}\n\t\tself.convBackwardParamsCache = {}\n\n\n\tdef __del__(self):\n\t\tlibmiopen.miopenDestroy(self.context)\n\n\n\t@staticmethod\n\tdef getVersion():\n\t\treturn libmiopen.version\n\n\n\tdef enableTensorOps(self, _):\n\t\treturn self\n\n\n\tdef createDescribedNdTensor(self, tensor, dims=None, strides=None, dtype=DataType.float):\n\t\tdataType = dtype if tensor is None else toDataType[tensor.dtype.type]\n\n\t\tdims = tensor.shape if dims is None else dims\n\t\tstrides = tuple(stride // tensor.dtype.itemsize for stride in tensor.strides) if strides is None else strides\n\n\t\tdesc = self.tensorDescCache.pop() if len(self.tensorDescCache) > 0 else libmiopen.miopenCreateTensorDescriptor()\n\t\tlibmiopen.miopenSetTensorDescriptor(desc, dataType.value, dims, strides)\n\n\t\treturn DescTensor(desc, dims, tensor)\n\n\n\tdef createDescribed1dTensor(self, tensor, ndim):\n\t\tdataType = toDataType[tensor.dtype.type]\n\t\tassert tensor.ndim == 1\n\n\t\tdims = (1, tensor.dimAt(0)) + (1, ) * (ndim - 2)\n\t\tstrides = (dims[1], ) + (1, ) * (ndim - 1)\n\n\t\tdesc = self.tensorDescCache.pop() if len(self.tensorDescCache) > 0 else libmiopen.miopenCreateTensorDescriptor()\n\t\tlibmiopen.miopenSetTensorDescriptor(desc, dataType.value, dims, strides)\n\n\t\treturn DescTensor(desc, dims, tensor)\n\n\n\tdef destroyDescribedTensors(self, *descTensors):\n\t\tself.tensorDescCache.extend(descTensor.desc for descTensor in descTensors)\n\n\n\tdef toTensorAddTensor(self, descTensor, descBias):\n\t\tlibmiopen.miopenConvolutionForwardBias(\n\t\t\tself.context, 1.0, descBias.desc, descBias.ptr, 0.0, descTensor.desc, descTensor.ptr\n\t\t)\n\n\n\t@staticmethod\n\tdef createDescribedConvNd(ndim, stride=1, pad=0, dilation=1, groups=1):\n\t\tstride = tuple(stride for _ in range(ndim - 2)) if isinstance(stride, int) else stride\n\t\tpad = tuple(pad for _ in range(ndim - 2)) if isinstance(pad, int) else pad\n\t\tdilation = tuple(dilation for _ in range(ndim - 2)) if isinstance(dilation, int) else dilation\n\n\t\tmode = ConvMode.conv\n\n\t\tdesc = libmiopen.miopenCreateConvolutionDescriptor()\n\t\tlibmiopen.miopenInitConvolutionNdDescriptor(desc, pad, stride, dilation, mode.value)\n\n\t\tlibmiopen.miopenSetConvolutionGroupCount(desc, groups)\n\t\treturn DescConvNd(desc, mode, stride, pad, dilation, groups)\n\n\n\t@staticmethod\n\tdef destroyDescribedConv(descConv):\n\t\tlibmiopen.miopenDestroyConvolutionDescriptor(descConv.desc)\n\n\n\t@staticmethod\n\tdef getConvNdOutShape(descConv, descTensor, descW):\n\t\tshape = libmiopen.miopenGetConvolutionNdForwardOutputDim(\n\t\t\tdescConv.desc, descTensor.desc, descW.desc, len(descTensor.shape)\n\t\t)\n\t\treturn shape\n\n\n\t@staticmethod\n\tdef getConvNdInShape(descConv, descTensor, descW):\n\t\tfsize = descW.shape[2:]\n\n\t\tstride, pad, dilation = descConv.stride, descConv.pad, descConv.dilation\n\t\tgroups = descConv.groups\n\n\t\tshape = tuple(\n\t\t\tstride[d] * (descTensor.shape[d + 2] - 1) + dilation[d] * (fsize[d] - 1) - 2 * pad[d] + 1\n\t\t\tfor d in range(len(descConv.pad))\n\t\t)\n\n\t\treturn (descTensor.shape[0], descW.shape[1] * groups) + shape\n\n\n\t@staticmethod\n\tdef cacheKey(descData, descW, descConv):\n\t\tkey = (\n\t\t\tdescData.shape, descW.shape, descConv.mode,\n\t\t\tdescConv.stride, descConv.pad, descConv.dilation, descConv.groups, descData.tensor.dtype\n\t\t)\n\t\treturn key\n\n\n\tdef convAlgoGetWorkspace(self, descData, descW, descOutData, descConv):\n\t\tsize = libmiopen.miopenConvolutionForwardGetWorkSpaceSize(\n\t\t\tself.context, descW.desc, descData.desc, descConv.desc, descOutData.desc\n\t\t)\n\n\t\treturn GPUArray.empty((size, ), dtype=np.uint8) if size > 0 else None\n\n\n\tdef cacheConvAlgo(self, descData, descW, descConv, descOutData, algo):\n\t\tkey = self.cacheKey(descData, descW, descConv)\n\t\tperfResults = self.convCache.get(key, None)\n\n\t\tif perfResults is None:\n\t\t\tworkspace = self.convAlgoGetWorkspace(descData, descW, descOutData, descConv)\n\t\t\tptr, size = (workspace.ptr, workspace.size) if workspace is not None else (None, 0)\n\n\t\t\tperfResults = libmiopen.miopenFindConvolutionForwardAlgorithm(\n\t\t\t\tself.context, descData.desc, descData.ptr, descW.desc, descW.ptr, descConv.desc, descOutData.desc,\n\t\t\t\tdescOutData.ptr, len(ConvFwdAlgo), ptr, size, 0\n\t\t\t)\n\n\t\t\tself.convCache[key] = perfResults\n\n\t\tperf = next(p for p in perfResults if p.algo == algo) if algo != -1 else perfResults[0]\n\t\treturn perf.algo, perf.memory\n\n\n\tdef convBackwardDataAlgoGetWorkspace(self, descGrad, descW, descInGrad, descConv):\n\t\tsize = libmiopen.miopenConvolutionBackwardDataGetWorkSpaceSize(\n\t\t\tself.context, descGrad.desc, descW.desc, descConv.desc, descInGrad.desc\n\t\t)\n\n\t\treturn GPUArray.empty((size, ), dtype=np.uint8) if size > 0 else None\n\n\n\tdef cacheConvBackwardDataAlgo(self, descGrad, descW, descConv, descInGrad, algo):\n\t\tkey = self.cacheKey(descGrad, descW, descConv)\n\t\tperfResults = self.convBackwardDataCache.get(key, None)\n\n\t\tif perfResults is None:\n\t\t\tworkspace = self.convBackwardDataAlgoGetWorkspace(descGrad, descW, descInGrad, descConv)\n\t\t\tptr, size = (workspace.ptr, workspace.size) if workspace is not None else (None, 0)\n\n\t\t\tperfResults = libmiopen.miopenFindConvolutionBackwardDataAlgorithm(\n\t\t\t\tself.context, descGrad.desc, descGrad.ptr, descW.desc, descW.ptr, descConv.desc, descInGrad.desc,\n\t\t\t\tdescInGrad.ptr, len(ConvBwdDataAlgo), ptr, size, 0\n\t\t\t)\n\n\t\t\tself.convBackwardDataCache[key] = perfResults\n\n\t\tperf = next(p for p in perfResults if p.algo == algo) if algo != -1 else perfResults[0]\n\t\treturn perf.algo, perf.memory\n\n\n\tdef convBackwardParamsAlgoGetWorkspace(self, descGrad, descData, descWGrad, descConv):\n\t\tsize = libmiopen.miopenConvolutionBackwardWeightsGetWorkSpaceSize(\n\t\t\tself.context, descGrad.desc, descData.desc, descConv.desc, descWGrad.desc\n\t\t)\n\n\t\treturn GPUArray.empty((size, ), dtype=np.uint8) if size > 0 else None\n\n\n\tdef cacheConvBackwardParamsAlgo(self, descGrad, descData, descConv, descWGrad, algo):\n\t\tkey = self.cacheKey(descGrad, descData, descConv)\n\t\tperfResults = self.convBackwardParamsCache.get(key, None)\n\n\t\tif perfResults is None:\n\t\t\tworkspace = self.convBackwardParamsAlgoGetWorkspace(descGrad, descData, descWGrad, descConv)\n\t\t\tptr, size = (workspace.ptr, workspace.size) if workspace is not None else (None, 0)\n\n\t\t\tperfResults = libmiopen.miopenFindConvolutionBackwardWeightsAlgorithm(\n\t\t\t\tself.context, descGrad.desc, descGrad.ptr, descData.desc, descData.ptr, descConv.desc,\n\t\t\t\tdescWGrad.desc, descWGrad.ptr, len(ConvBwdFilterAlgo), ptr, size, 0\n\t\t\t)\n\n\t\t\tself.convBackwardParamsCache[key] = perfResults\n\n\t\tperf = next(p for p in perfResults if p.algo == algo) if algo != -1 else perfResults[0]\n\t\treturn perf.algo, perf.memory\n\n\n\tdef convNd(self, data, W, bias=None, stride=1, pad=0, dilation=1, groups=1, algo=ConvFwdAlgo.auto.value,\n\t\t\t   out=None, allocator=None):\n\t\tassert data.ndim == W.ndim and data.shape[1] == W.shape[1] * groups\n\n\t\tdescData = self.createDescribedNdTensor(data)\n\t\tdescW = self.createDescribedNdTensor(W)\n\n\t\tdescConv = self.createDescribedConvNd(W.ndim, stride, pad, dilation, groups)\n\t\toutshape = self.getConvNdOutShape(descConv, descData, descW)\n\n\t\tout = GPUArray.empty(outshape, dtype=data.dtype, allocator=allocator) if out is None else out\n\t\tdescOutData = self.createDescribedNdTensor(out)\n\n\t\talgo, size = self.cacheConvAlgo(descData, descW, descConv, descOutData, algo)\n\n\t\tworkspace = GPUArray.empty((size, ), dtype=np.uint8, allocator=allocator) if size > 0 else None\n\t\tptr, size = (workspace.ptr, workspace.nbytes) if workspace is not None else (None, 0)\n\n\t\tlibmiopen.miopenConvolutionForward(\n\t\t\tself.context, 1.0, descData.desc, descData.ptr, descW.desc, descW.ptr, descConv.desc, algo,\n\t\t\t0.0, descOutData.desc, descOutData.ptr, ptr, size\n\t\t)\n\n\t\tif bias is not None:\n\t\t\tdescBias = self.createDescribed1dTensor(bias, data.ndim)\n\n\t\t\tself.toTensorAddTensor(descOutData, descBias)\n\t\t\tself.destroyDescribedTensors(descBias)\n\n\t\tself.destroyDescribedConv(descConv)\n\t\tself.destroyDescribedTensors(descData, descOutData, descW)\n\n\t\treturn out\n\n\n\tdef convNdBackwardData(self, grad, W, bias=None, data=None, stride=1, pad=0, dilation=1, groups=1,\n\t\t\t\t\t\t   algo=ConvBwdDataAlgo.auto.value, out=None, allocator=None):\n\t\tassert grad.ndim == W.ndim and grad.shape[1] == W.shape[0]\n\n\t\tdescGrad = self.createDescribedNdTensor(grad)\n\t\tdescW = self.createDescribedNdTensor(W)\n\n\t\tdescConv = self.createDescribedConvNd(W.ndim, stride, pad, dilation, groups)\n\t\tinshape = self.getConvNdInShape(descConv, descGrad, descW) if data is None else data.shape\n\n\t\tout = GPUArray.empty(inshape, dtype=grad.dtype, allocator=allocator) if out is None else out\n\t\tdescInGrad = self.createDescribedNdTensor(out)\n\n\t\talgo, size = self.cacheConvBackwardDataAlgo(descGrad, descW, descConv, descInGrad, algo)\n\n\t\tworkspace = GPUArray.empty((size, ), dtype=np.uint8, allocator=allocator) if size > 0 else None\n\t\tptr, size = (workspace.ptr, workspace.nbytes) if workspace is not None else (None, 0)\n\n\t\tlibmiopen.miopenConvolutionBackwardData(\n\t\t\tself.context, 1.0, descGrad.desc, descGrad.ptr, descW.desc, descW.ptr, descConv.desc, algo,\n\t\t\t0.0, descInGrad.desc, descInGrad.ptr, ptr, size\n\t\t)\n\n\t\tif bias is not None:\n\t\t\tdescBias = self.createDescribed1dTensor(bias, grad.ndim)\n\t\t\tself.toTensorAddTensor(descInGrad, descBias)\n\n\t\t\tself.destroyDescribedTensors(descBias)\n\n\t\tself.destroyDescribedConv(descConv)\n\t\tself.destroyDescribedTensors(descGrad, descInGrad, descW)\n\n\t\treturn out\n\n\n\tdef convNdBackwardParams(self, data, grad, W, stride=1, pad=0, dilation=1, groups=1, withbias=False, deconv=False,\n\t\t\t\t\t\t\t wgrad=None, bgrad=None, scale=1.0, momentum=0.0, algo=ConvBwdFilterAlgo.auto.value,\n\t\t\t\t\t\t\t allocator=None):\n\t\tassert data.ndim == grad.ndim and grad.shape[1] == W.shape[0] and data.shape[1] == W.shape[1] * groups\n\n\t\tdescData = self.createDescribedNdTensor(data)\n\t\tdescGrad = self.createDescribedNdTensor(grad)\n\n\t\tdescConv = self.createDescribedConvNd(W.ndim, stride, pad, dilation, groups)\n\t\twg, wAccMode = None, False\n\n\t\tif wgrad is not None and (scale != 1.0 or momentum != 0.0):\n\t\t\twAccMode = True\n\t\t\twg = GPUArray.empty(W.shape, dtype=W.dtype, allocator=allocator)\n\t\telse:\n\t\t\twgrad = GPUArray.empty(W.shape, dtype=W.dtype, allocator=allocator) if wgrad is None else wgrad\n\t\t\twg = wgrad\n\n\t\tdescWGrad = self.createDescribedNdTensor(wg)\n\t\talgo, size = self.cacheConvBackwardParamsAlgo(descGrad, descData, descConv, descWGrad, algo)\n\n\t\tworkspace = GPUArray.empty((size, ), dtype=np.uint8, allocator=allocator) if size > 0 else None\n\t\tptr, size = (workspace.ptr, workspace.nbytes) if workspace is not None else (None, 0)\n\n\t\tlibmiopen.miopenConvolutionBackwardWeights(\n\t\t\tself.context, 1.0, descGrad.desc, descGrad.ptr, descData.desc, descData.ptr, descConv.desc, algo,\n\t\t\t0.0, descWGrad.desc, descWGrad.ptr, ptr, size\n\t\t)\n\n\t\tif wAccMode:\n\t\t\tself.backend.addKer(wgrad.dtype)(wgrad, wgrad, momentum, wg, scale)\n\n\t\tif withbias:\n\t\t\ttensor = descData if deconv else descGrad\n\t\t\tbiasshape = (tensor.shape[1], )\n\n\t\t\tbg, bAccMode = None, False\n\n\t\t\tif bgrad is not None and (scale != 1.0 or momentum != 0.0):\n\t\t\t\tbAccMode = True\n\t\t\t\tbg = GPUArray.empty(biasshape, dtype=data.dtype, allocator=allocator)\n\t\t\telse:\n\t\t\t\tbgrad = GPUArray.empty(biasshape, dtype=data.dtype, allocator=allocator) if bgrad is None else bgrad\n\t\t\t\tbg = bgrad\n\n\t\t\tdescBGrad = self.createDescribed1dTensor(bg, data.ndim)\n\n\t\t\tlibmiopen.miopenConvolutionBackwardBias(\n\t\t\t\tself.context, 1.0, tensor.desc, tensor.ptr, 0.0, descBGrad.desc, descBGrad.ptr\n\t\t\t)\n\n\t\t\tif bAccMode:\n\t\t\t\tself.backend.addKer(bgrad.dtype)(bgrad, bgrad, momentum, bg, scale)\n\n\t\t\tself.destroyDescribedTensors(descBGrad)\n\n\t\tself.destroyDescribedConv(descConv)\n\t\tself.destroyDescribedTensors(descData, descGrad, descWGrad)\n\n\t\treturn (wgrad, bgrad) if withbias else wgrad\n\n\n\tdef convNdbenchmark(self, datashape, Wshape, dtype, stride=1, pad=0, dilation=1, groups=1, algoCount=10,\n\t\t\t\t\t\texhaustive=False):\n\t\tassert len(datashape) == len(Wshape)\n\n\t\tdescData = self.createDescribedNdTensor(GPUArray.empty(datashape, dtype=dtype))\n\t\tdescW = self.createDescribedNdTensor(GPUArray.empty(Wshape, dtype=dtype))\n\n\t\tdescConv = self.createDescribedConvNd(len(Wshape), stride, pad, dilation, groups)\n\t\toutshape = self.getConvNdOutShape(descConv, descData, descW)\n\n\t\tdescWGrad = self.createDescribedNdTensor(GPUArray.empty(Wshape, dtype=dtype))\n\t\tdescOutData = self.createDescribedNdTensor(GPUArray.empty(outshape, dtype=dtype))\n\n\t\tworkspace = self.convAlgoGetWorkspace(descData, descW, descOutData, descConv)\n\t\tptr, size = (workspace.ptr, workspace.size) if workspace is not None else (None, 0)\n\n\t\tperfResults = libmiopen.miopenFindConvolutionForwardAlgorithm(\n\t\t\tself.context, descData.desc, descData.ptr, descW.desc, descW.ptr, descConv.desc,\n\t\t\tdescOutData.desc, descOutData.ptr, algoCount, ptr, size, exhaustive\n\t\t)\n\n\t\tmillisInSec = 1e-3\n\t\tfwdResults = [(perf.algo, perf.time * millisInSec, perf.memory) for perf in perfResults]\n\n\t\tworkspace = self.convBackwardDataAlgoGetWorkspace(descOutData, descW, descData, descConv)\n\t\tptr, size = (workspace.ptr, workspace.size) if workspace is not None else (None, 0)\n\n\t\tperfResults = libmiopen.miopenFindConvolutionBackwardDataAlgorithm(\n\t\t\tself.context, descOutData.desc, descOutData.ptr, descW.desc, descW.ptr, descConv.desc,\n\t\t\tdescData.desc, descData.ptr, algoCount, ptr, size, exhaustive\n\t\t)\n\n\t\tbwdDataResults = [(perf.algo, perf.time * millisInSec, perf.memory) for perf in perfResults]\n\n\t\tworkspace = self.convBackwardParamsAlgoGetWorkspace(descOutData, descW, descData, descConv)\n\t\tptr, size = (workspace.ptr, workspace.size) if workspace is not None else (None, 0)\n\n\t\tperfResults = libmiopen.miopenFindConvolutionBackwardWeightsAlgorithm(\n\t\t\tself.context, descOutData.desc, descOutData.ptr, descData.desc, descData.ptr, descConv.desc,\n\t\t\tdescWGrad.desc, descWGrad.ptr, algoCount, ptr, size, exhaustive\n\t\t)\n\n\t\tbwdParamResults = [(perf.algo, perf.time * millisInSec, perf.memory) for perf in perfResults]\n\n\t\tself.destroyDescribedTensors(descData, descOutData, descW, descWGrad)\n\t\tself.destroyDescribedConv(descConv)\n\n\t\treturn fwdResults, bwdDataResults, bwdParamResults\n\n\n\t@staticmethod\n\tdef createDescribedPool2d(size=2, stride=2, pad=0, mode=PoolMode.max.value):\n\t\tsize = (size, size) if isinstance(size, int) else size\n\t\thsize, wsize = size\n\n\t\tstride = (stride, stride) if isinstance(stride, int) else stride\n\t\thstride, wstride = stride\n\n\t\tpad = (pad, pad) if isinstance(pad, int) else pad\n\t\thpad, wpad = pad\n\n\t\tdesc = libmiopen.miopenCreatePoolingDescriptor()\n\t\tlibmiopen.miopenSet2dPoolingDescriptor(desc, mode, hsize, wsize, hpad, wpad, hstride, wstride)\n\n\t\treturn DescPool2d(desc, size, stride, pad)\n\n\n\t@staticmethod\n\tdef destroyDescribedPool(descPool):\n\t\tlibmiopen.miopenDestroyPoolingDescriptor(descPool.desc)\n\n\n\t@staticmethod\n\tdef getPool2dOutShape(descPool, descTensor):\n\t\thsize, wsize = descPool.size\n\t\thpad, wpad = descPool.pad\n\t\thstride, wstride = descPool.stride\n\n\t\touth = (descTensor.shape[2] + 2 * hpad - hsize) // hstride + 1\n\t\toutw = (descTensor.shape[3] + 2 * wpad - wsize) // wstride + 1\n\n\t\treturn descTensor.shape[:2] + (outh, outw)\n\n\n\tdef poolNd(self, data, size=2, stride=2, pad=0, mode=PoolMode.max.value, test=False, out=None, allocator=None):\n\t\tassert data.ndim == 4\n\t\tdescData = self.createDescribedNdTensor(data)\n\n\t\tdescPool = self.createDescribedPool2d(size, stride, pad, mode)\n\t\toutshape = self.getPool2dOutShape(descPool, descData)\n\n\t\tout = GPUArray.empty(outshape, dtype=data.dtype, allocator=allocator) if out is None else out\n\t\tdescOutData = self.createDescribedNdTensor(out)\n\n\t\tworkspace, ptr, size = None, None, 0\n\t\tif not test:\n\t\t\tsize = libmiopen.miopenPoolingGetWorkSpaceSize(descOutData.desc)\n\n\t\t\tworkspace = GPUArray.empty((size, ), dtype=np.uint8, allocator=allocator)\n\t\t\tptr = workspace.ptr\n\n\t\tlibmiopen.miopenPoolingForward(\n\t\t\tself.context, descPool.desc, 1.0, descData.desc, descData.ptr, 0.0, descOutData.desc,\n\t\t\tdescOutData.ptr, not test, ptr, size\n\t\t)\n\n\t\tself.destroyDescribedTensors(descData, descOutData)\n\t\tself.destroyDescribedPool(descPool)\n\n\t\treturn out if test else (out, workspace)\n\n\n\tdef poolNdBackward(self, grad, indata, outdata, workspace, size=2, stride=2, pad=0, mode=PoolMode.max.value,\n\t\t\t\t\t   out=None, allocator=None):\n\t\tassert grad.ndim == 4\n\t\tdescGrad = self.createDescribedNdTensor(grad)\n\n\t\tdescInData = self.createDescribedNdTensor(indata)\n\t\tdescOutData = self.createDescribedNdTensor(outdata)\n\n\t\tdescPool = self.createDescribedPool2d(size, stride, pad, mode)\n\n\t\tout = GPUArray.empty(indata.shape, dtype=grad.dtype, allocator=allocator) if out is None else out\n\t\tdescInGrad = self.createDescribedNdTensor(out)\n\n\t\tlibmiopen.miopenPoolingBackward(\n\t\t\tself.context, descPool.desc, 1.0, descOutData.desc, descOutData.ptr, descGrad.desc, descGrad.ptr,\n\t\t\tdescInData.desc, descInData.ptr, 0.0, descInGrad.desc, descInGrad.ptr, workspace.ptr\n\t\t)\n\n\t\tself.destroyDescribedTensors(descInData, descOutData, descGrad, descInGrad)\n\t\tself.destroyDescribedPool(descPool)\n\n\t\treturn out\n\n\n\tdef softmaxNd(self, data, mode=SoftMaxMode.spatial.value, algo=SoftMaxAlgo.accurate.value,\n\t\t\t\t  out=None, allocator=None):\n\t\tdescData = self.createDescribedNdTensor(data)\n\n\t\tout = GPUArray.empty(data.shape, dtype=data.dtype, allocator=allocator) if out is None else out\n\t\tdescOutData = self.createDescribedNdTensor(out)\n\n\t\tlibmiopen.miopenSoftmaxForward(\n\t\t\tself.context, 1.0, descData.desc, descData.ptr,\n\t\t\t0.0, descOutData.desc, descOutData.ptr, algo, mode\n\t\t)\n\n\t\tself.destroyDescribedTensors(descData, descOutData)\n\t\treturn out\n\n\n\tdef softmaxNdBackward(self, grad, outdata, mode=SoftMaxMode.spatial.value, algo=SoftMaxAlgo.accurate.value,\n\t\t\t\t\t\t  out=None, allocator=None):\n\t\tdescGrad = self.createDescribedNdTensor(grad)\n\t\tdescOutData = self.createDescribedNdTensor(outdata)\n\n\t\tout = GPUArray.empty(grad.shape, dtype=grad.dtype, allocator=allocator) if out is None else out\n\t\tdescInGrad = self.createDescribedNdTensor(out)\n\n\t\tlibmiopen.miopenSoftmaxBackward(\n\t\t\tself.context, 1.0, descOutData.desc, descOutData.ptr, descGrad.desc, descGrad.ptr,\n\t\t\t0.0, descInGrad.desc, descInGrad.ptr, algo, mode\n\t\t)\n\n\t\tself.destroyDescribedTensors(descOutData, descGrad, descInGrad)\n\t\treturn out\n\n\n\tdef batchNormNd(self, data, mean, var, scale, bias, epsilon=1e-5, factor=1.0, test=False,\n\t\t\t\t\tmode=BatchNormMode.spatial.value, out=None, allocator=None):\n\t\tassert mean.ndim == 1 and var.ndim == 1 and scale.ndim == 1 and bias.ndim == 1\n\t\tassert data.dimAt(1) == mean.dimAt(0)\n\n\t\tdescData = self.createDescribedNdTensor(data)\n\t\tdescScale = self.createDescribed1dTensor(scale, data.ndim)\n\n\t\tout = GPUArray.empty(data.shape, dtype=data.dtype, allocator=allocator) if out is None else out\n\t\tdescOutData = self.createDescribedNdTensor(out)\n\n\t\tsavemean, saveinvvar = None, None\n\n\t\tif test:\n\t\t\tlibmiopen.miopenBatchNormalizationForwardInference(\n\t\t\t\tself.context, mode, 1.0, 0.0, descData.desc, descData.ptr, descOutData.desc, out.ptr,\n\t\t\t\tdescScale.desc, descScale.ptr, bias.ptr, mean.ptr, var.ptr, epsilon\n\t\t\t)\n\n\t\telse:\n\t\t\tsavemean = GPUArray.empty(mean.shape, dtype=data.dtype, allocator=allocator)\n\t\t\tsaveinvvar = GPUArray.empty(var.shape, dtype=data.dtype, allocator=allocator)\n\n\t\t\tlibmiopen.miopenBatchNormalizationForwardTraining(\n\t\t\t\tself.context, mode, 1.0, 0.0, descData.desc, descData.ptr, descOutData.desc, out.ptr,\n\t\t\t\tdescScale.desc, descScale.ptr, bias.ptr, factor, mean.ptr, var.ptr, epsilon, savemean.ptr,\n\t\t\t\tsaveinvvar.ptr\n\t\t\t)\n\n\t\tself.destroyDescribedTensors(descData, descOutData, descScale)\n\t\treturn out if test else (out, savemean, saveinvvar)\n\n\n\tdef batchNormNdBackward(self, grad, data, scale, savemean=None, saveinvvar=None, epsilon=1e-5,\n\t\t\t\t\t\t\tmode=BatchNormMode.spatial.value, out=None, allocator=None):\n\t\tassert data.ndim == grad.ndim\n\n\t\tdescGrad = self.createDescribedNdTensor(grad)\n\t\tdescData = self.createDescribedNdTensor(data)\n\t\tdescScale = self.createDescribed1dTensor(scale, grad.ndim)\n\n\t\tout = GPUArray.empty(grad.shape, dtype=grad.dtype, allocator=allocator) if out is None else out\n\t\tdescInGrad = self.createDescribedNdTensor(out)\n\n\t\tscalegrad = GPUArray.empty(scale.shape, dtype=scale.dtype, allocator=allocator)\n\t\tbgrad = GPUArray.empty(scale.shape, dtype=scale.dtype, allocator=allocator)\n\n\t\tlibmiopen.miopenBatchNormalizationBackward(\n\t\t\tself.context, mode, 1.0, 0.0, 1.0, 0.0, descData.desc, descData.ptr, descGrad.desc, descGrad.ptr,\n\t\t\tdescInGrad.desc, descInGrad.ptr, descScale.desc, descScale.ptr, scalegrad.ptr, bgrad.ptr,\n\t\t\tepsilon, None if savemean is None else savemean.ptr, None if saveinvvar is None else saveinvvar.ptr\n\t\t)\n\n\t\tself.destroyDescribedTensors(descData, descGrad, descInGrad, descScale)\n\t\treturn out, scalegrad, bgrad\n\n\n\t@staticmethod\n\tdef createDescribedLRN(mode, N=5, alpha=1e-4, beta=0.75, K=2.0):\n\t\tdesc = libmiopen.miopenCreateLRNDescriptor()\n\t\tlibmiopen.miopenSetLRNDescriptor(desc, mode, N, alpha, beta, K)\n\n\t\treturn DescLRN(desc, N, alpha, beta, K)\n\n\n\t@staticmethod\n\tdef destroyDescribedLRN(descLRN):\n\t\tlibmiopen.miopenDestroyLRNDescriptor(descLRN.desc)\n\n\n\tdef lrn(self, data, N=5, alpha=1e-4, beta=0.75, K=2.0, mode=LRNMode.map.value, test=False,\n\t\t\tout=None, allocator=None):\n\t\tdescData = self.createDescribedNdTensor(data)\n\t\tdescLRN = self.createDescribedLRN(mode, N, alpha, beta, K)\n\n\t\tout = GPUArray.empty(data.shape, dtype=data.dtype, allocator=allocator) if out is None else out\n\t\tdescOutData = self.createDescribedNdTensor(out)\n\n\t\tworkspace = None\n\n\t\tif not test:\n\t\t\tsize = libmiopen.miopenLRNGetWorkSpaceSize(descOutData.desc)\n\n\t\t\tif size > 0:\n\t\t\t\tworkspace = GPUArray.empty((size, ), dtype=np.uint8, allocator=allocator)\n\n\t\tlibmiopen.miopenLRNForward(\n\t\t\tself.context, descLRN.desc, 1.0, descData.desc, descData.ptr, 0.0,\n\t\t\tdescOutData.desc, descOutData.ptr, not test, None if workspace is None else workspace.ptr\n\t\t)\n\n\t\tself.destroyDescribedTensors(descData, descOutData)\n\t\tself.destroyDescribedLRN(descLRN)\n\n\t\treturn out if test else (out, workspace)\n\n\n\tdef lrnBackward(self, grad, indata, outdata, workspace, N=5, alpha=1e-4, beta=0.75, K=2.0, mode=LRNMode.map,\n\t\t\t\t\tout=None, allocator=None):\n\t\tdescGrad = self.createDescribedNdTensor(grad)\n\n\t\tdescInData = self.createDescribedNdTensor(indata)\n\t\tdescOutData = self.createDescribedNdTensor(outdata)\n\n\t\tout = GPUArray.empty(grad.shape, dtype=grad.dtype, allocator=allocator) if out is None else out\n\t\tdescInGrad = self.createDescribedNdTensor(out)\n\n\t\tdescLRN = self.createDescribedLRN(mode, N, alpha, beta, K)\n\n\t\tlibmiopen.miopenLRNBackward(\n\t\t\tself.context, descLRN.desc, 1.0, descOutData.desc, descOutData.ptr, descGrad.desc, descGrad.ptr,\n\t\t\tdescInData.desc, descInData.ptr, 0.0, descInGrad.desc, descInGrad.ptr, workspace.ptr\n\t\t)\n\n\t\tself.destroyDescribedTensors(descGrad, descInData, descOutData, descInGrad)\n\t\tself.destroyDescribedLRN(descLRN)\n\n\t\treturn out\n\n\ndef unittest():\n\tfrom PuzzleLib.Hip import Backend\n\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tbnd = Backend.getBackend(deviceIdx, initmode=1)\n\n\t\tfor dtype, atol in bnd.dtypesSupported():\n\t\t\tconv2dTest(bnd, dtype, atol)\n\t\t\tconv3dTest(bnd, dtype, atol)\n\t\t\tconvGroupTest(bnd, dtype, atol)\n\n\t\t\tdeconv2dTest(bnd, dtype, atol)\n\t\t\tdeconv3dTest(bnd, dtype, atol)\n\t\t\tdeconvGroupTest(bnd, dtype, atol)\n\n\t\t\tmaxpool2dTest(bnd, dtype, atol)\n\t\t\tsoftmax2dTest(bnd, dtype, atol)\n\n\ndef maxpool2dTest(bnd, dtype, atol):\n\tbatchsize, maps, h, w = 3, 2, 6, 6\n\tsize, stride, pad = 3, 2, 1\n\n\thostData = np.full(shape=(batchsize, maps, h + 2 * pad, w + 2 * pad), fill_value=np.finfo(dtype).min, dtype=dtype)\n\thostData[:, :, pad:-pad, pad:-pad] = np.random.randn(batchsize, maps, h, w).astype(dtype)\n\n\tdata = GPUArray.toGpu(hostData[:, :, pad:-pad, pad:-pad])\n\toutdata, workspace = bnd.dnn.poolNd(data, size=size, stride=stride, pad=pad, mode=PoolMode.max.value)\n\n\thostOutData = np.empty(outdata.shape, dtype=dtype)\n\n\tfor b, c, y, x in itertools.product(\n\t\trange(batchsize), range(maps), range(hostOutData.shape[2]), range(hostOutData.shape[3])\n\t):\n\t\thostOutData[b, c, y, x] = np.max(hostData[b, c, y * stride:y * stride + size, x * stride:x * stride + size])\n\n\tassert np.allclose(hostOutData, outdata.get(), atol=atol)\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad = GPUArray.toGpu(hostGrad)\n\tingrad = bnd.dnn.poolNdBackward(\n\t\tgrad, data, outdata, workspace, size=size, stride=stride, pad=pad, mode=PoolMode.max.value\n\t)\n\n\thostInGrad = np.zeros(hostData.shape, dtype=dtype)\n\n\tfor b, c, y, x, dy, dx in itertools.product(\n\t\trange(batchsize), range(maps), range(hostOutData.shape[2]), range(hostOutData.shape[3]),\n\t\trange(size), range(size)\n\t):\n\t\tif hostData[b, c, y * stride + dy, x * stride + dx] == hostOutData[b, c, y, x]:\n\t\t\thostInGrad[b, c, y * stride + dy, x * stride + dx] += hostGrad[b, c, y, x]\n\n\thostInGrad = hostInGrad[:, :, pad:-pad, pad:-pad].astype(dtype)\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Hip/Wrappers/MIOpenNorm.py,10,"b'import itertools\nimport numpy as np\n\nfrom PuzzleLib.Cuda.Wrappers.CuDnnNorm import batchNorm2dTest, batchNorm3dTest, instanceNorm2dTest\n\n\ndef unittest():\n\tfrom PuzzleLib.Hip import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tbnd = Backend.getBackend(deviceIdx, initmode=2)\n\n\t\tfloat32 = bnd.dtypesSupported()[0]\n\n\t\tbatchNorm2dTest(bnd, *float32, np.float32)\n\t\tbatchNorm3dTest(bnd, *float32, np.float32)\n\t\tinstanceNorm2dTest(bnd, *float32, np.float32)\n\n\t\tfor dtype, atol in bnd.dtypesSupported():\n\t\t\tmapLRN2dTest(bnd, dtype, atol)\n\n\ndef mapLRN2dTest(bnd, dtype, atol):\n\tbatchsize, maps, h, w = 2, 2, 9, 10\n\tN, alpha, beta, K = 5, 1.0, 0.5, 2.0\n\n\tlookBehind = int((N - 1) / 2)\n\tlookAhead = N - lookBehind\n\n\thostData = np.random.randn(batchsize, maps, h, w).astype(dtype)\n\n\tdata = bnd.GPUArray.toGpu(hostData)\n\toutdata, workspace = bnd.dnn.lrn(data, N=N, alpha=alpha, beta=beta, K=K, mode=bnd.LRNMode.map.value)\n\n\tnorms = np.empty(hostData.shape, dtype=np.float32)\n\n\tfor b, c, y, x in itertools.product(range(batchsize), range(maps), range(h), range(w)):\n\t\tslcy = slice(max(0, y - lookBehind), min(h, y + lookAhead))\n\t\tslcx = slice(max(0, x - lookBehind), min(w, x + lookAhead))\n\n\t\tslc = hostData[b, c, slcy, slcx].ravel()\n\t\tnorms[b, c, y, x] = K + np.dot(slc, slc) * alpha / N**2\n\n\thostOutData = (hostData / norms**beta).astype(dtype)\n\tassert np.allclose(hostOutData, outdata.get(), atol=atol)\n\n\thostGrad = np.random.randn(*outdata.shape).astype(dtype)\n\n\tgrad = bnd.GPUArray.toGpu(hostGrad)\n\tingrad = bnd.dnn.lrnBackward(\n\t\tgrad, data, outdata, workspace, N=N, alpha=alpha, beta=beta, K=K, mode=bnd.LRNMode.map.value\n\t)\n\n\thostInGrad = hostGrad / norms**beta\n\tk = 2.0 * alpha * beta / N**2\n\n\tfor b, c, y, x in itertools.product(range(batchsize), range(maps), range(h), range(w)):\n\t\tslcy = slice(max(0, y - lookBehind), min(h, y + lookAhead))\n\t\tslcx = slice(max(0, x - lookBehind), min(w, x + lookAhead))\n\n\t\tslcdata, slcgrad = hostData[b, c, slcy, slcx].ravel(), hostGrad[b, c, slcy, slcx].ravel()\n\t\tslcnorms = norms[b, c, slcy, slcx].ravel()\n\n\t\thostInGrad[b, c, y, x] -= k * hostData[b, c, y, x] * np.dot(slcgrad, slcdata / slcnorms**(beta + 1))\n\n\thostInGrad = hostInGrad.astype(dtype)\n\tassert np.allclose(hostInGrad, ingrad.get(), atol=atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Hip/Wrappers/MIOpenRnn.py,4,"b'from enum import Enum\n\nimport numpy as np\n\nfrom PuzzleLib.Cuda.Wrappers.CuDnnRnn import reluTest, tanhTest, lstmTest, gruTest\n\nfrom PuzzleLib.Hip.Driver import GPUArray\nfrom PuzzleLib.Hip.ThirdParty import libmiopen\nfrom PuzzleLib.Hip.Wrappers.MIOpen import toDataType\n\n\nclass RNNMode(Enum):\n\trelu = libmiopen.miopenRNNMode[""miopenRNNRELU""]\n\ttanh = libmiopen.miopenRNNMode[""miopenRNNTANH""]\n\tlstm = libmiopen.miopenRNNMode[""miopenLSTM""]\n\tgru = libmiopen.miopenRNNMode[""miopenGRU""]\n\n\nclass DirectionMode(Enum):\n\tuni = libmiopen.miopenRNNDirectionMode[""miopenRNNunidirection""]\n\tbi = libmiopen.miopenRNNDirectionMode[""miopenRNNbidirection""]\n\n\nclass RNNAlgo(Enum):\n\tdefault = libmiopen.miopenRNNAlgo[""miopenRNNdefault""]\n\tfundamental = libmiopen.miopenRNNAlgo[""miopenRNNfundamental""]\n\n\nclass Rnn:\n\tdef __init__(self, context, insize, hsize, dtype, layers, mode, direction):\n\t\tself.context = context\n\n\t\tself.insize, self.hsize, self.layers = insize, hsize, layers\n\t\tself.mode, self.direction, self.algo = mode, direction, RNNAlgo.default\n\t\tself.dtype = np.dtype(dtype).type\n\n\t\tself.desc = libmiopen.miopenCreateRNNDescriptor()\n\n\t\tdataType = toDataType[self.dtype]\n\t\tself.descData = self.context.createDescribedNdTensor(None, (1, insize), (insize, 1), dtype=dataType)\n\n\t\tlibmiopen.miopenSetRNNDescriptor(\n\t\t\tself.desc, hsize, layers, libmiopen.miopenRNNInputMode[""miopenRNNlinear""], direction.value, mode.value,\n\t\t\tlibmiopen.miopenRNNBiasMode[""miopenRNNwithBias""], self.algo.value, dataType.value\n\t\t)\n\n\t\twsize = libmiopen.miopenGetRNNParamsSize(\n\t\t\tself.context.context, self.desc, self.descData.desc, dataType.value\n\t\t) // np.dtype(dtype).itemsize\n\n\t\tself.Wshape = (wsize, )\n\t\tself.descW = self.context.createDescribedNdTensor(None, self.Wshape, (1, ), dtype=dataType)\n\n\n\tdef __del__(self):\n\t\tself.context.destroyDescribedTensors(self.descData, self.descW)\n\t\tlibmiopen.miopenDestroyRNNDescriptor(self.desc)\n\n\n\tdef forward(self, data, W, hidden=None, cells=None, test=False, out=None, allocator=None):\n\t\tassert data.ndim == 3 and data.shape[2] == self.insize\n\t\tassert W.shape == self.Wshape\n\n\t\tseqlen, batchsize, _ = data.shape\n\t\thsize, layers = (self.hsize, self.layers) if self.direction == DirectionMode.uni else \\\n\t\t\t(2 * self.hsize, 2 * self.layers)\n\n\t\tdims, strides = (layers, batchsize, self.hsize), (batchsize * self.hsize, self.hsize, 1)\n\n\t\tif hidden is not None:\n\t\t\tassert hidden.shape == dims and hidden.dtype == data.dtype\n\t\telse:\n\t\t\thidden = GPUArray.zeros(dims, dtype=data.dtype, allocator=allocator)\n\n\t\tif cells is not None:\n\t\t\tassert cells.shape == dims and cells.dtype == data.dtype\n\t\telif self.mode in {RNNMode.lstm, RNNMode.gru}:\n\t\t\tcells = GPUArray.zeros(dims, dtype=data.dtype, allocator=allocator)\n\n\t\thptr, cptr = None if hidden is None else hidden.ptr, None if cells is None else cells.ptr\n\n\t\tdescCells = self.context.createDescribedNdTensor(None, dims, strides, dtype=toDataType[data.dtype.type])\n\t\tout = GPUArray.empty(data.shape[:2] + (hsize, ), dtype=data.dtype, allocator=allocator) if out is None else out\n\n\t\tdescData = self.context.createDescribedNdTensor(data[0])\n\t\tdescOutData = self.context.createDescribedNdTensor(out[0])\n\n\t\tindescs, outdescs = [descData.desc] * seqlen, [descOutData.desc] * seqlen\n\n\t\tworkspaceSize = libmiopen.miopenGetRNNWorkspaceSize(self.context.context, self.desc, seqlen, indescs)\n\t\tworkspace = GPUArray.empty((workspaceSize, ), dtype=np.uint8, allocator=allocator)\n\n\t\tif test:\n\t\t\tlibmiopen.miopenRNNForwardInference(\n\t\t\t\tself.context.context, self.desc, seqlen, indescs, data.ptr, descCells.desc, hptr, descCells.desc, cptr,\n\t\t\t\tself.descW.desc, W.ptr, outdescs, out.ptr, descCells.desc, None, descCells.desc, None,\n\t\t\t\tworkspace.ptr, workspaceSize\n\t\t\t)\n\n\t\t\ttrainReserve = None\n\n\t\telse:\n\t\t\treserveSize = libmiopen.miopenGetRNNTrainingReserveSize(self.context.context, self.desc, seqlen, indescs)\n\t\t\treserve = GPUArray.empty((reserveSize, ), dtype=np.uint8, allocator=allocator)\n\n\t\t\tlibmiopen.miopenRNNForwardTraining(\n\t\t\t\tself.context.context, self.desc, seqlen, indescs, data.ptr, descCells.desc, hptr, descCells.desc, cptr,\n\t\t\t\tself.descW.desc, W.ptr, outdescs, out.ptr, descCells.desc, None, descCells.desc, None,\n\t\t\t\tworkspace.ptr, workspaceSize, reserve.ptr, reserveSize\n\t\t\t)\n\n\t\t\ttrainReserve = (workspace, reserve)\n\n\t\tself.context.destroyDescribedTensors(descData, descOutData, descCells)\n\t\treturn out if test else (out, trainReserve)\n\n\n\tdef backwardData(self, grad, outdata, W, trainReserve, hidden=None, cells=None, out=None, allocator=None):\n\t\tassert grad.ndim == 3 and outdata.shape == grad.shape and outdata.dtype == grad.dtype\n\n\t\tseqlen, batchsize, _ = grad.shape\n\t\thsize, layers = (self.hsize, self.layers) if self.direction == DirectionMode.uni else \\\n\t\t\t(2 * self.hsize, 2 * self.layers)\n\n\t\tassert W.shape == self.Wshape and grad.shape[2] == hsize\n\t\tdhidden, dcells = None, None\n\n\t\tdims, strides = (layers, batchsize, self.hsize), (batchsize * self.hsize, self.hsize, 1)\n\t\thptr, cptr, dhptr, dcptr = None, None, None, None\n\n\t\tif hidden is not None:\n\t\t\tassert hidden.shape == dims and hidden.dtype == grad.dtype\n\t\t\tdhidden = GPUArray.empty(hidden.shape, dtype=hidden.dtype, allocator=allocator)\n\t\t\thptr, dhptr = hidden.ptr, dhidden.ptr\n\n\t\tif cells is not None:\n\t\t\tassert cells.shape == dims and cells.dtype == grad.dtype\n\t\t\tdcells = GPUArray.empty(cells.shape, dtype=cells.dtype, allocator=allocator)\n\t\t\tcptr, dcptr = cells.ptr, dcells.ptr\n\n\t\tdescCells = self.context.createDescribedNdTensor(None, dims, strides, dtype=toDataType[grad.dtype.type])\n\t\tout = GPUArray.empty(grad.shape[:2] + (self.insize, ), dtype=grad.dtype, allocator=allocator) \\\n\t\t\tif out is None else out\n\n\t\tdescInGrad = self.context.createDescribedNdTensor(out[0])\n\t\tdescGrad = self.context.createDescribedNdTensor(grad[0])\n\n\t\tindescs, outdescs = [descInGrad.desc] * seqlen, [descGrad.desc] * seqlen\n\t\tworkspace, reserveSpace = trainReserve\n\n\t\tlibmiopen.miopenRNNBackwardData(\n\t\t\tself.context.context, self.desc, seqlen, outdescs, outdata.ptr, outdescs, grad.ptr,\n\t\t\tdescCells.desc, None, descCells.desc, None, self.descW.desc, W.ptr,\n\t\t\tdescCells.desc, hptr, descCells.desc, cptr, indescs, out.ptr, descCells.desc, dhptr,\n\t\t\tdescCells.desc, dcptr, workspace.ptr, workspace.nbytes, reserveSpace.ptr, reserveSpace.nbytes\n\t\t)\n\n\t\tself.context.destroyDescribedTensors(descCells, descInGrad, descGrad)\n\t\treturn out, dhidden, dcells\n\n\n\tdef backwardParams(self, data, outdata, trainReserve, hidden=None, out=None, allocator=None):\n\t\tassert data.ndim == 3 and outdata.ndim == 3 and data.shape[:2] == outdata.shape[:2]\n\n\t\tseqlen, batchsize, _ = data.shape\n\t\thsize, layers = (self.hsize, self.layers) if self.direction == DirectionMode.uni else \\\n\t\t\t(2 * self.hsize, 2 * self.layers)\n\n\t\tassert data.shape[2] == self.insize and outdata.shape[2] == hsize\n\t\tdims, strides = (layers, batchsize, self.hsize), (batchsize * self.hsize, self.hsize, 1)\n\n\t\tif hidden is not None:\n\t\t\tassert hidden.shape == dims and hidden.dtype == data.dtype\n\t\telse:\n\t\t\thidden = GPUArray.zeros(dims, dtype=data.dtype, allocator=allocator)\n\n\t\thptr = None if hidden is None else hidden.ptr\n\n\t\tdescCells = self.context.createDescribedNdTensor(None, dims, strides, dtype=toDataType[data.dtype.type])\n\t\tout = GPUArray.zeros(self.Wshape, dtype=self.dtype, allocator=allocator) if out is None else out\n\n\t\tdescData = self.context.createDescribedNdTensor(data[0])\n\t\tdescOutData = self.context.createDescribedNdTensor(outdata[0])\n\n\t\tindescs, outdescs = [descData.desc] * seqlen, [descOutData.desc] * seqlen\n\t\tworkspace, reserveSpace = trainReserve\n\n\t\tlibmiopen.miopenRNNBackwardWeights(\n\t\t\tself.context.context, self.desc, seqlen, indescs, data.ptr, descCells.desc, hptr, outdescs, outdata.ptr,\n\t\t\tself.descW.desc, out.ptr, workspace.ptr, workspace.nbytes, reserveSpace.ptr, reserveSpace.nbytes\n\t\t)\n\n\t\tself.context.destroyDescribedTensors(descCells, descData, descOutData)\n\t\treturn out\n\n\ndef unittest():\n\tfrom PuzzleLib.Hip import Backend\n\tbackendTest(Backend)\n\n\ndef backendTest(Backend):\n\tfor deviceIdx in range(Backend.getDeviceCount()):\n\t\tbnd = Backend.getBackend(deviceIdx, initmode=2)\n\n\t\tfor dtype, atol in bnd.dtypesSupported()[:1]:\n\t\t\treluTest(bnd, dtype, atol)\n\t\t\ttanhTest(bnd, dtype, atol)\n\t\t\tlstmTest(bnd, dtype, atol)\n\t\t\tgruTest(bnd, dtype, atol)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Hip/Wrappers/RocBlas.py,0,"b'from PuzzleLib.Cuda.Wrappers.CuBlas import backendTest\n\n\ndef unittest():\n\tfrom PuzzleLib.Hip import Backend\n\tbackendTest(Backend)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Intel/Benchmarks/ConvSpeed.py,0,"b'from PuzzleLib.Intel.Wrappers import DNNL\n\n\ndef main():\n\tdatashape = (16, 32, 64, 64)\n\tWshape = (64, 32, 3, 3)\n\n\tstride, pad = 1, 0\n\ttimeConv(datashape, Wshape, stride, pad)\n\n\ndef timeConv(datashape, Wshape, stride, pad):\n\tfwdResults, bwdFilterResults, bwdDataResults = DNNL.convNdbenchmark(datashape, Wshape, stride, pad)\n\n\tformatstr = ""%-40s %-25s %-28s""\n\n\tprint(""Forward results:"")\n\tfor res in fwdResults:\n\t\tprint(formatstr % (\n\t\t\t""Algo %s"" % res.algo, ""time %.6f secs"" % res.time, ""memory %.6f mbytes"" % (res.memory / 1024**2)\n\t\t))\n\n\tprint(""\\nBackward filter results:"")\n\tfor res in bwdFilterResults:\n\t\tprint(formatstr % (\n\t\t\t""Algo %s"" % res.algo, ""time %.6f secs"" % res.time, ""memory %.6f mbytes"" % (res.memory / 1024**2)\n\t\t))\n\n\tprint(""\\nBackward data results:"")\n\tfor res in bwdDataResults:\n\t\tprint(formatstr % (\n\t\t\t""Algo %s"" % DNNL.ConvAlgo(res.algo), ""time %.6f secs"" % res.time,\n\t\t\t""memory %.6f mbytes"" % (res.memory / 1024**2)\n\t\t))\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Intel/Kernels/Costs.py,28,"b'from string import Template\n\nimport numpy as np\n\nfrom PuzzleLib.Compiler.Codegen.Types import void_t, int32_t, float_t, ptrdiff_t\n\nfrom PuzzleLib.CPU.SourceModule import SourceModule, ElementwiseKernel, ReductionKernel\nfrom PuzzleLib.CPU.CPUArray import CPUArray\n\nfrom PuzzleLib.Intel.Wrappers.DNNL import softmaxNd\n\n\nbceKer = ElementwiseKernel(\n\t[\n\t\t(float_t.const.ptr, ""scores""), (int32_t.const.ptr, ""labels""),\n\t\t(float_t.ptr, ""totalError""), (float_t.ptr, ""grad""), (int32_t, ""numsamples""), (int32_t, ""spatialDim"")\n\t],\n\t""""""\n\tfloat prob = 1.0f / (1.0f + expf(-scores[i]));\n\n\tfloat error = labels[i] == 1 ? -logf(prob) : -logf(1.0f - prob);\n\t*totalError += error / spatialDim;\n\n\tgrad[i] = ((labels[i] == 1) - prob) / numsamples / spatialDim;\n\t"""""",\n\t""bceKer""\n)\n\nhingeKer = ElementwiseKernel(\n\t[\n\t\t(float_t.const.ptr, ""scores""), (int32_t.const.ptr, ""labels""),\n\t\t(float_t.ptr, ""totalError""), (float_t.ptr, ""grad""), (int32_t, ""numsamples""), (int32_t, ""numcases"")\n\t],\n\t""""""\n\tfloat score = scores[i];\n\tint label = labels[i];\n\n\tfloat error = 1.0f - score * label;\n\t*totalError += error > 0.0f ? error / numcases : 0.0f;\n\n\tgrad[i] = error > 0.0f ? (float)label / numsamples / numcases : 0.0f;\n\t"""""",\n\t""hingeKer""\n)\n\nsmoothL1Ker = ElementwiseKernel(\n\t[\n\t\t(float_t.const.ptr, ""pred""), (float_t.const.ptr, ""target""), (float_t.ptr, ""totalError""),\n\t\t(float_t.ptr, ""grad""), (float_t, ""norm""), (float_t, ""fullnorm"")\n\t],\n\t""""""\n\tfloat diff = pred[i] - target[i];\n\n\tfloat sign = diff > 0.0f ? 1.0f : -1.0f;\n\tdiff = fabsf(diff);\n\n\t*totalError += diff < 1.0f ? diff * diff / 2.0f * norm : (diff - 0.5f) * norm;\n\tgrad[i] = sign * (diff < 1.0f ? diff * fullnorm : fullnorm);\n\t"""""",\n\t""smoothL1Ker""\n)\n\nl1HingeKer = ElementwiseKernel(\n\t[\n\t\t(float_t.const.ptr, ""x1""), (float_t.const.ptr, ""x2""), (int32_t.const.ptr, ""labels""),\n\t\t(float_t.ptr, ""totalError""), (float_t.ptr, ""g1""), (float_t.ptr, ""g2""),\n\t\t(int32_t, ""numsamples""), (int32_t, ""numcases"")\n\t],\n\t""""""\n\tfloat diff = x1[i] - x2[i];\n\tfloat sign = diff > 0.0f ? 1.0f : -1.0f;\n\n\tdiff = fabsf(diff);\n\tint label = labels[i / numcases];\n\n\tfloat error = (label == 0) ? (diff > 1.0f ? 0.0f : 1.0f - diff) / numcases : diff / numcases;\n\t*totalError += error;\n\n\tg1[i] = (label == 0 ? (diff < 1.0f) * -sign : sign) / numsamples / numcases;\n\tg2[i] = (label == 0 ? (diff < 1.0f) * sign : -sign) / numsamples / numcases;\n\t"""""",\n\t""l1HingeKer""\n)\n\n\naccKernelCache = {}\n\n\ndef getAccuracyKernel(name):\n\tkrl = accKernelCache.get(name, None)\n\n\tif krl is None:\n\t\tif name == ""calcAccuracy"":\n\t\t\tkrl = ReductionKernel(\n\t\t\t\tnp.float32, neutral=""0.0f"", reduceExpr=""a + b"", mapExpr=""x[i] != y[i]"",\n\t\t\t\targuments=[(int32_t.const.ptr, ""x""), (int32_t.const.ptr, ""y"")]\n\t\t\t)\n\n\t\telif name == ""calcBCEAccuracy"":\n\t\t\tkrl = ReductionKernel(\n\t\t\t\tnp.float32, neutral=""0.0f"", reduceExpr=""a + b"", mapExpr=""y[i] == 1 ? x[i] <= 0.0f : x[i] > 0.0f"",\n\t\t\t\targuments=[(float_t.const.ptr, ""x""), (int32_t.const.ptr, ""y"")]\n\t\t\t)\n\n\t\telif name == ""klDivergence"":\n\t\t\tkrl = ReductionKernel(\n\t\t\t\tnp.float32, neutral=""0.0f"", reduceExpr=""a + b"",\n\t\t\t\tmapExpr=""grad[i] = (y[i] - x[i]) * gradnorm, y[i] > 0.0f ? y[i] * (logf(y[i]) - logf(x[i])) : 0.0f"",\n\t\t\t\targuments=[\n\t\t\t\t\t(float_t.const.ptr, ""x""), (float_t.const.ptr, ""y""), (float_t.ptr, ""grad""), (float_t, ""gradnorm"")\n\t\t\t\t]\n\t\t\t)\n\n\t\telif name == ""l1HingeAccuracy"":\n\t\t\tkrl = ReductionKernel(\n\t\t\t\tnp.float32, neutral=""0.0f"", reduceExpr=""a + b"", mapExpr=""(d[i] <= 1.0f) != labels[i]"",\n\t\t\t\targuments=[(float_t.const.ptr, ""d""), (int32_t.const.ptr, ""labels"")]\n\t\t\t)\n\n\t\telse:\n\t\t\traise RuntimeError(""Unrecognized cost kernel name"")\n\n\t\taccKernelCache[name] = krl\n\n\treturn krl\n\n\ncostLblTmpl = Template(""""""\n\nstatic void cost(const float * __restrict scores, const int32_t * __restrict labels, int32_t mapStride,\n\t\t\t\t int32_t spatialDim, int32_t numCases, int32_t numSamples, float * __restrict totalError,\n\t\t\t\t float * __restrict grad, ptrdiff_t size)\n{\n\tfor (ptrdiff_t i = 0; i < size; i++)\n\t{\n\t\tptrdiff_t b = i / mapStride, m = i % spatialDim, c = (i / spatialDim) % numCases;\n\n\t\t$logic\n\t}\n}\n\n"""""")\n\n\ncrossEntropyLogic = """"""\nfloat score = scores[i];\nint32_t label = labels[b * spatialDim + m];\n\ngrad[i] = ((c == label) - score) / numSamples;\n\nif (c == label)\n{\n\tfloat error = -logf(score) / spatialDim;\n\t*totalError += error;\n}\n""""""\n\n\nsvmL1Logic = """"""\nfloat score = scores[i];\nint32_t label = labels[b * spatialDim + m];\nfloat cls = (float)(2 * (label == c) - 1);\n\nfloat error = 1.0f - score * cls;\n*totalError += error > 0.0f ? error / numCases / spatialDim : 0.0f;\n\ngrad[i] = error > 0.0f ? cls / numCases / numSamples : 0.0f;\n""""""\n\n\nsvmL2Logic = """"""\nfloat score = scores[i];\nint32_t label = labels[b * spatialDim + m];\nfloat cls = (float)(2 * (label == c) - 1);\n\nfloat error = 1.0f - score * cls;\n*totalError += error > 0.0f ? error * error / numCases / spatialDim : 0.0f;\n\ngrad[i] = error > 0.0f ? 2.0f * cls * error / numCases / numSamples : 0.0f;\n""""""\n\n\nwceTmpl = """"""\n\nstatic void cost(const float * __restrict scores, const int32_t * __restrict labels, const float * __restrict weights,\n\t\t\t\t int32_t mapStride, int32_t spatialDim, int32_t numCases, int32_t numSamples,\n\t\t\t\t float * __restrict totalError, float * __restrict grad, ptrdiff_t size)\n{\n\tfor (ptrdiff_t i = 0; i < size; i++)\n\t{\n\t\tptrdiff_t b = i / mapStride, m = i % spatialDim, c = (i / spatialDim) % numCases;\n\n\t\tfloat score = scores[i];\n\t\tint32_t label = labels[b * spatialDim + m];\n\t\tfloat weight = weights[c];\n\n\t\tgrad[i] = weight * ((c == label) - score) / numSamples;\n\n\t\tif (c == label)\n\t\t{\n\t\t\tfloat error = -weight * logf(score) / spatialDim;\n\t\t\t*totalError += error;\n\t\t}\n\t}\n}\n\n""""""\n\n\nceMod = SourceModule(costLblTmpl.substitute(logic=crossEntropyLogic), functions=[\n\t(""cost"", void_t, [\n\t\t(float_t.const.ptr.restrict, ""scores""), (int32_t.const.ptr.restrict, ""labels""),\n\t\t(int32_t, ""mapStride""), (int32_t, ""spatialDim""), (int32_t, ""numCases""), (int32_t, ""numSamples""),\n\t\t(float_t.ptr.restrict, ""totalError""), (float_t.ptr.restrict, ""grad""), (ptrdiff_t, ""size"")\n\t])\n])\nwceMod = SourceModule(wceTmpl, functions=[\n\t(""cost"", void_t, [\n\t\t(float_t.const.ptr.restrict, ""scores""), (int32_t.const.ptr.restrict, ""labels""),\n\t\t(float_t.const.ptr.restrict, ""weights""), (int32_t, ""mapStride""), (int32_t, ""spatialDim""), (int32_t, ""numCases""),\n\t\t(int32_t, ""numSamples""), (float_t.ptr.restrict, ""totalError""), (float_t.ptr.restrict, ""grad""),\n\t\t(ptrdiff_t, ""size"")\n\t])\n])\n\nsvmL1Mod = SourceModule(costLblTmpl.substitute(logic=svmL1Logic), functions=[\n\t(""cost"", void_t, [\n\t\t(float_t.const.ptr.restrict, ""scores""), (int32_t.const.ptr.restrict, ""labels""),\n\t\t(int32_t, ""mapStride""), (int32_t, ""spatialDim""), (int32_t, ""numCases""), (int32_t, ""numSamples""),\n\t\t(float_t.ptr.restrict, ""totalError""), (float_t.ptr.restrict, ""grad""), (ptrdiff_t, ""size"")\n\t])\n])\nsvmL2Mod = SourceModule(costLblTmpl.substitute(logic=svmL2Logic), functions=[\n\t(""cost"", void_t, [\n\t\t(float_t.const.ptr.restrict, ""scores""), (int32_t.const.ptr.restrict, ""labels""),\n\t\t(int32_t, ""mapStride""), (int32_t, ""spatialDim""), (int32_t, ""numCases""), (int32_t, ""numSamples""),\n\t\t(float_t.ptr.restrict, ""totalError""), (float_t.ptr.restrict, ""grad""), (ptrdiff_t, ""size"")\n\t])\n])\n\n\ndef crossEntropy(scores, labels, weights=None, error=None):\n\tassert scores.dtype == np.float32 and labels.dtype == np.int32\n\n\tshape = scores.shape\n\tif scores.ndim < 4:\n\t\tscores = scores.reshape(*shape, *(1 for _ in range(4 - scores.ndim)))\n\n\tsoftmax = softmaxNd(scores)\n\n\tgrad = CPUArray.empty(shape, dtype=np.float32)\n\tif error is None:\n\t\terror = CPUArray.empty((), dtype=np.float32)\n\n\terror.fill(0.0)\n\n\tspatialDim = int(np.prod(scores.shape[2:]))\n\tmapStride = spatialDim * scores.shape[1]\n\n\tif weights is None:\n\t\tceMod.cost(\n\t\t\tsoftmax.data, labels.data, mapStride, spatialDim, scores.shape[1], scores.shape[0], error.data, grad.data,\n\t\t\tsoftmax.size\n\t\t)\n\n\telse:\n\t\twceMod.cost(\n\t\t\tsoftmax.data, labels.data, weights.data,  mapStride, spatialDim, shape[1], shape[0], error.data, grad.data,\n\t\t\tsoftmax.size\n\t\t)\n\n\treturn error, grad\n\n\ndef svm(scores, labels, mode, error=None):\n\tassert scores.dtype == np.float32 and labels.dtype == np.int32\n\tshape = scores.shape\n\n\tgrad = CPUArray.empty(shape, dtype=np.float32)\n\tif error is None:\n\t\terror = CPUArray.empty((), dtype=np.float32)\n\n\terror.fill(0.0)\n\n\tspatialDim = int(np.prod(scores.shape[2:]))\n\tmapStride = spatialDim * scores.shape[1]\n\n\tif mode == ""l1"":\n\t\tkrl = svmL1Mod.cost\n\telif mode == ""l2"":\n\t\tkrl = svmL2Mod.cost\n\telse:\n\t\traise ValueError()\n\n\tkrl(scores.data, labels.data, mapStride, spatialDim, shape[1], shape[0], error.data, grad.data, scores.size)\n\treturn error, grad\n\n\ndef unittest():\n\tcrossEntropyTest()\n\tsvmTest()\n\n\ndef crossEntropyTest():\n\tscores = CPUArray.toDevice(np.random.randn(20, 10, 3).astype(np.float32))\n\tlabels = CPUArray.toDevice(np.random.randint(low=0, high=10, size=(20, 3)).astype(np.int32))\n\n\terror, grad = crossEntropy(scores, labels)\n\n\tdef softmax(w):\n\t\te = np.exp(w - np.amax(w))\n\t\tdist = e / np.sum(e)\n\t\treturn dist\n\n\tdef hostCrossEntropy(smax, target):\n\t\tsmax = np.moveaxis(smax, 1, -1).reshape(-1, smax.shape[1])\n\t\ttarget = target.flatten()\n\t\terr = np.sum(np.log(np.array([smax[i, target[i]] for i in range(smax.shape[0])])))\n\n\t\treturn -err / target.size\n\n\tdef hostCrossEntropyGrad(target, smax):\n\t\treturn np.array([(target == i) - smax[i] for i in range(smax.shape[0])])\n\n\thostSoftmax = np.apply_along_axis(softmax, 1, scores.get())\n\n\thostGrad = np.vstack([hostCrossEntropyGrad(labels.get()[i], hostSoftmax[i]) / scores.shape[0]\n\t\t\t\t\t\t  for i in range(scores.shape[0])]).reshape(*hostSoftmax.shape)\n\n\tassert np.allclose(hostGrad, grad.get())\n\n\thostError = hostCrossEntropy(hostSoftmax, labels.get())\n\tassert np.isclose(hostError, error.get() / scores.shape[0])\n\n\ndef svmTest():\n\tbatchsize, size = 20, 4\n\n\tscores = CPUArray.toDevice(np.random.randn(batchsize, size).astype(np.float32))\n\tlabels = CPUArray.toDevice(np.random.randint(low=0, high=size, size=(batchsize, ), dtype=np.int32))\n\n\terror, grad = svm(scores, labels, mode=""l1"")\n\n\thostScores, hostLabels = scores.get(), labels.get()\n\n\thostGrad = np.empty(grad.shape, dtype=np.float32)\n\thostError = 0.0\n\n\tfor b in range(batchsize):\n\t\tfor n in range(size):\n\t\t\tcls = 2 * (hostLabels[b] == n) - 1\n\t\t\tval = hostScores[b, n] * cls\n\n\t\t\thostGrad[b, n] = cls / batchsize / size if val < 1 else 0.0\n\t\t\thostError += max(0.0, 1.0 - val) / batchsize / size\n\n\tassert np.allclose(hostGrad, grad.get())\n\tassert np.isclose(hostError, error.get() / scores.shape[0])\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Intel/ThirdParty/finddnnl.py,0,"b'import sys, os\n\n\ndef findDNNL():\n\tversions = [""1.91"", ""1.2"", ""1.1""]\n\terror = """"\n\n\tif sys.platform == ""linux"":\n\t\tlibnames = [""libdnnl.so.%s"" % v for v in versions]\n\t\tlibnames += [""/usr/local/lib/%s"" % libname for libname in libnames]\n\n\telif sys.platform == ""darwin"":\n\t\tlibnames = [""libdnnl.%s.dylib"" % v for v in versions]\n\n\telif sys.platform == ""win32"":\n\t\tlibpaths = [\n\t\t\tos.environ.get(""DNNL_PATH"", """"),\n\t\t\tos.path.normpath(os.path.join(os.path.dirname(__file__), ""../Libs/""))\n\t\t]\n\n\t\tlibnames = [os.path.join(lp, ""dnnl.dll"") for lp in libpaths]\n\t\terror = "": check your DNNL_PATH environment value""\n\n\telse:\n\t\traise RuntimeError(""Unsupported platform for dnnl"")\n\n\tfor libname in libnames:\n\t\tif os.path.exists(libname):\n\t\t\treturn libname\n\n\traise OSError(""dnnl library not found (searched for following version(s): %s)%s"" % (versions, error))\n'"
Intel/ThirdParty/libdnnl.py,0,"b'import sys, ctypes\nfrom PuzzleLib.Intel.ThirdParty.finddnnl import findDNNL\n\n\n_libdnnl = (ctypes.windll if sys.platform == ""win32"" else ctypes.cdll).LoadLibrary(findDNNL())\n\n\nclass dnnlError(Exception):\n\tpass\n\nclass dnnlOutOfMemory(dnnlError):\n\tpass\n\nclass dnnlInvalidArguments(dnnlError):\n\tpass\n\nclass dnnlUnimplemented(dnnlError):\n\tpass\n\nclass dnnlIteratorEnds(dnnlError):\n\tpass\n\nclass dnnlRuntimeError(dnnlError):\n\tpass\n\nclass dnnlNotRequired(dnnlError):\n\tpass\n\n\ndnnlExceptions = {\n\t1: dnnlOutOfMemory,\n\t2: dnnlInvalidArguments,\n\t3: dnnlUnimplemented,\n\t4: dnnlIteratorEnds,\n\t5: dnnlRuntimeError,\n\t6: dnnlNotRequired\n}\n\n\nclass dnnl_version_t(ctypes.Structure):\n\t_fields_ = [\n\t\t(""major"", ctypes.c_int),\n\t\t(""minor"", ctypes.c_int),\n\t\t(""patch"", ctypes.c_int),\n\t\t(""hash"", ctypes.c_char_p)\n\t]\n\n\ndnnl_data_type_t = {\n\t""dnnl_data_type_undef"": 0,\n\t""dnnl_f16"": 1,\n\t""dnnl_bf16"": 2,\n\t""dnnl_f32"": 3,\n\t""dnnl_s32"": 4,\n\t""dnnl_s8"": 5,\n\t""dnnl_u8"": 6\n}\n\n\ndnnl_format_kind_t = {\n\t""dnnl_format_kind_undef"": 0,\n\t""dnnl_format_kind_any"": 1,\n\t""dnnl_blocked"": 2,\n\t""dnnl_format_kind_wino"": 3,\n\t""dnnl_format_kind_rnn_packed"": 4\n}\n\n\ndnnl_format_tag_t = {\n\t""dnnl_format_tag_undef"": 0,\n\t""dnnl_format_tag_any"": 1,\n\n\t""dnnl_a"": 2,\n\t""dnnl_ab"": 3,\n\t""dnnl_abc"": 4,\n\t""dnnl_abcd"": 5,\n\t""dnnl_abcde"": 6,\n\t""dnnl_abcdef"": 7,\n\n\t""dnnl_abdec"": 8,\n\t""dnnl_acb"": 9,\n\t""dnnl_acbde"": 10,\n\t""dnnl_acdb"": 11,\n\t""dnnl_acdeb"": 12,\n\t""dnnl_ba"": 13,\n\t""dnnl_bac"": 14,\n\t""dnnl_bacd"": 15,\n\t""dnnl_bca"": 16,\n\t""dnnl_bcda"": 17,\n\t""dnnl_bcdea"": 18,\n\t""dnnl_cba"": 19,\n\t""dnnl_cdba"": 20,\n\t""dnnl_cdeba"": 21,\n\t""dnnl_decab"": 22\n}\n\n\ndnnl_prop_kind_t = {\n\t""dnnl_prop_kind_undef"": 0,\n\t""dnnl_forward_training"": 64,\n\t""dnnl_forward_inference"": 96,\n\t""dnnl_backward"": 128,\n\t""dnnl_backward_data"": 160,\n\t""dnnl_backward_weights"": 192,\n\t""dnnl_backward_bias"": 193\n}\n\n\ndnnl_primitive_kind_t = {\n\t""dnnl_undefined_primitive"": 0,\n\t""dnnl_reorder"": 1,\n\t""dnnl_shuffle"": 2,\n\t""dnnl_concat"": 3,\n\t""dnnl_sum"": 4,\n\t""dnnl_convolution"": 5,\n\t""dnnl_deconvolution"": 6,\n\t""dnnl_eltwise"": 7,\n\t""dnnl_softmax"": 8,\n\t""dnnl_pooling"": 9,\n\t""dnnl_lrn"": 10,\n\t""dnnl_batch_normalization"": 11,\n\t""dnnl_inner_product"": 12,\n\t""dnnl_rnn"": 13,\n\t""dnnl_gemm"": 14\n}\n\n\ndnnl_alg_kind_t = {\n\t""dnnl_alg_kind_undef"": 0x0,\n\n\t""dnnl_convolution_direct"": 0x1,\n\t""dnnl_convolution_winograd"": 0x2,\n\t""dnnl_convolution_auto"": 0x3,\n\t""dnnl_deconvolution_direct"": 0xa,\n\t""dnnl_deconvolution_winograd"": 0xb,\n\n\t""dnnl_eltwise_relu"": 0x1f,\n\t""dnnl_eltwise_tanh"": 0x2f,\n\t""dnnl_eltwise_elu"": 0x3f,\n\t""dnnl_eltwise_square"": 0x4f,\n\t""dnnl_eltwise_abs"": 0x5f,\n\t""dnnl_eltwise_sqrt"": 0x6f,\n\t""dnnl_eltwise_linear"": 0x7f,\n\t""dnnl_eltwise_bounded_relu"": 0x8f,\n\t""dnnl_eltwise_soft_relu"": 0x9f,\n\t""dnnl_eltwise_logistic"": 0xaf,\n\t""dnnl_eltwise_exp"": 0xbf,\n\t""dnnl_eltwise_gelu"": 0xcf,\n\n\t""dnnl_pooling_max"": 0x1ff,\n\t""dnnl_pooling_avg_include_padding"": 0x2ff,\n\t""dnnl_pooling_avg_exclude_padding"": 0x3ff,\n\n\t""dnnl_lrn_across_channels"": 0xaff,\n\t""dnnl_lrn_within_channel"": 0xbff,\n\n\t""dnnl_vanilla_rnn"": 0x1fff,\n\t""dnnl_vanilla_lstm"": 0x2fff,\n\t""dnnl_vanilla_gru"": 0x3fff,\n\t""dnnl_lbr_gru"": 0x4fff\n}\n\n\ndnnl_normalization_flags_t = {\n\t""dnnl_use_global_stats"": 0x1,\n\t""dnnl_use_scaleshift"": 0x2,\n\t""dnnl_fuse_bn_relu"": 0x4\n}\n\n\ndnnl_MAX_NDIMS = 12\n\ndnnl_dim_t = ctypes.c_int64\ndnnl_dims_t = dnnl_dim_t * dnnl_MAX_NDIMS\n\n\nclass dnnl_blocking_desc_t(ctypes.Structure):\n\t_fields_ = [\n\t\t(""strides"", dnnl_dims_t),\n\t\t(""inner_nblks"", ctypes.c_int),\n\t\t(""inner_blks"", dnnl_dims_t),\n\t\t(""inner_idxs"", dnnl_dims_t)\n\t]\n\n\ndnnl_wino_memory_format_t = {\n\t""dnnl_wino_undef"": 0,\n\t""dnnl_wino_wei_aaOIoi"": 1,\n\t""dnnl_wino_wei_aaOio"": 2,\n\t""dnnl_wino_wei_aaOBiOo"": 3,\n\t""dnnl_wino_wei_OBaaIBOIio"": 4\n}\n\n\nclass dnnl_wino_desc_t(ctypes.Structure):\n\t_fields_ = [\n\t\t(""wino_format"", ctypes.c_int),\n\t\t(""r"", ctypes.c_int),\n\t\t(""alpha"", ctypes.c_int),\n\t\t(""ic"", ctypes.c_int),\n\t\t(""oc"", ctypes.c_int),\n\t\t(""ic_block"", ctypes.c_int),\n\t\t(""oc_block"", ctypes.c_int),\n\t\t(""ic2_block"", ctypes.c_int),\n\t\t(""oc2_block"", ctypes.c_int),\n\t\t(""adj_scale"", ctypes.c_float),\n\t\t(""size"", ctypes.c_size_t)\n\t]\n\n\ndnnl_rnn_packed_memory_format_t = {\n\t""dnnl_packed_format_undef"": 0,\n\t""dnnl_ldigo_p"": 1,\n\t""dnnl_ldgoi_p"": 2\n}\n\n\ndnnl_RNN_MAX_N_PARTS = 4\n\n\nclass dnnl_rnn_packed_desc_t(ctypes.Structure):\n\t_fields_ = [\n\t\t(""format"", ctypes.c_int),\n\t\t(""n_parts"", ctypes.c_int),\n\t\t(""n"", ctypes.c_int),\n\t\t(""ldb"", ctypes.c_int),\n\t\t(""parts"", ctypes.c_int * dnnl_RNN_MAX_N_PARTS),\n\t\t(""part_pack_size"", ctypes.c_size_t * dnnl_RNN_MAX_N_PARTS),\n\t\t(""pack_part"", ctypes.c_uint * dnnl_RNN_MAX_N_PARTS),\n\t\t(""offset_compensation"", ctypes.c_size_t),\n\t\t(""size"", ctypes.c_size_t),\n\t\t(""reserved"", ctypes.c_char * 200)\n\t]\n\n\ndnnl_memory_extra_flags_t = {\n\t""dnnl_memory_extra_flag_none"": 0x0,\n\t""dnnl_memory_extra_flag_compensation_conv_s8s8"": 0x1,\n\t""dnnl_memory_extra_flag_scale_adjust"": 0x2\n}\n\n\nclass dnnl_memory_extra_desc_t(ctypes.Structure):\n\t_fields_ = [\n\t\t(""flags"", ctypes.c_uint64),\n\t\t(""compensation_mask"", ctypes.c_int),\n\t\t(""scale_adjust"", ctypes.c_float),\n\t\t(""reserved"", ctypes.c_char * 64)\n\t]\n\n\nclass dnnl_memory_desc_t(ctypes.Structure):\n\tclass _format_desc(ctypes.Union):\n\t\t_fields_ = [\n\t\t\t(""blocking"", dnnl_blocking_desc_t),\n\t\t\t(""wino_desc"", dnnl_wino_desc_t),\n\t\t\t(""rnn_packed_desc"", dnnl_rnn_packed_desc_t)\n\t\t]\n\n\t_fields_ = [\n\t\t(""ndims"", ctypes.c_int),\n\t\t(""dims"", dnnl_dims_t),\n\t\t(""data_type"", ctypes.c_int),\n\t\t(""padded_dims"", dnnl_dims_t),\n\t\t(""padded_offsets"", dnnl_dims_t),\n\t\t(""offset0"", dnnl_dim_t),\n\t\t(""format_kind"", ctypes.c_int),\n\t\t(""format_desc"", _format_desc),\n\t\t(""extra"", dnnl_memory_extra_desc_t)\n\t]\n\n\nclass dnnl_convolution_desc_t(ctypes.Structure):\n\t_fields_ = [\n\t\t(""primitive_kind"", ctypes.c_int),\n\t\t(""prop_kind"", ctypes.c_int),\n\t\t(""alg_kind"", ctypes.c_int),\n\t\t(""src_desc"", dnnl_memory_desc_t),\n\t\t(""diff_src_desc"", dnnl_memory_desc_t),\n\t\t(""weights_desc"", dnnl_memory_desc_t),\n\t\t(""diff_weights_desc"", dnnl_memory_desc_t),\n\t\t(""bias_desc"", dnnl_memory_desc_t),\n\t\t(""diff_bias_desc"", dnnl_memory_desc_t),\n\t\t(""dst_desc"", dnnl_memory_desc_t),\n\t\t(""diff_dst_desc"", dnnl_memory_desc_t),\n\t\t(""strides"", dnnl_dims_t),\n\t\t(""dilates"", dnnl_dims_t),\n\t\t(""padding"", dnnl_dims_t * 2),\n\t\t(""accum_data_type"", ctypes.c_int)\n\t]\n\n\ndnnl_deconvolution_desc_t = dnnl_convolution_desc_t\n\n\nclass dnnl_softmax_desc_t(ctypes.Structure):\n\t_fields_ = [\n\t\t(""primitive_kind"", ctypes.c_int),\n\t\t(""prop_kind"", ctypes.c_int),\n\t\t(""data_desc"", dnnl_memory_desc_t),\n\t\t(""diff_desc"", dnnl_memory_desc_t),\n\t\t(""softmax_axis"", ctypes.c_int)\n\t]\n\n\nclass dnnl_pooling_desc_t(ctypes.Structure):\n\t_fields_ = [\n\t\t(""primitive_kind"", ctypes.c_int),\n\t\t(""prop_kind"", ctypes.c_int),\n\t\t(""alg_kind"", ctypes.c_int),\n\t\t(""src_desc"", dnnl_memory_desc_t),\n\t\t(""diff_src_desc"", dnnl_memory_desc_t),\n\t\t(""dst_desc"", dnnl_memory_desc_t),\n\t\t(""diff_dst_desc"", dnnl_memory_desc_t),\n\t\t(""strides"", dnnl_dims_t),\n\t\t(""kernel"", dnnl_dims_t),\n\t\t(""padding"", dnnl_dims_t * 2),\n\t\t(""accum_data_type"", ctypes.c_int)\n\t]\n\n\nclass dnnl_lrn_desc_t(ctypes.Structure):\n\t_fields_ = [\n\t\t(""primitive_kind"", ctypes.c_int),\n\t\t(""prop_kind"", ctypes.c_int),\n\t\t(""alg_kind"", ctypes.c_int),\n\t\t(""data_desc"", dnnl_memory_desc_t),\n\t\t(""diff_data_desc"", dnnl_memory_desc_t),\n\t\t(""local_size"", ctypes.c_int),\n\t\t(""lrn_alpha"", ctypes.c_float),\n\t\t(""lrn_beta"", ctypes.c_float),\n\t\t(""lrn_k"", ctypes.c_float)\n\t]\n\n\nclass dnnl_batch_normalization_desc_t(ctypes.Structure):\n\t_fields_ = [\n\t\t(""primitive_kind"", ctypes.c_int),\n\t\t(""prop_kind"", ctypes.c_int),\n\t\t(""data_desc"", dnnl_memory_desc_t),\n\t\t(""diff_data_desc"", dnnl_memory_desc_t),\n\t\t(""data_scaleshift_desc"", dnnl_memory_desc_t),\n\t\t(""diff_data_scaleshift_desc"", dnnl_memory_desc_t),\n\t\t(""stat_desc"", dnnl_memory_desc_t),\n\t\t(""batch_norm_epsilon"", ctypes.c_float),\n\t\t(""flags"", ctypes.c_uint)\n\t]\n\n\ndnnl_rnn_flags_t = {\n\t""dnnl_rnn_flags_undef"": 0x0\n}\n\n\ndnnl_rnn_direction_t = {\n\t""dnnl_unidirectional_left2right"": 0,\n\t""dnnl_unidirectional_right2left"": 1,\n\t""dnnl_bidirectional_concat"": 2,\n\t""dnnl_bidirectional_sum"": 3\n}\n\n\nclass dnnl_rnn_desc_t(ctypes.Structure):\n\t_fields_ = [\n\t\t(""primitive_kind"", ctypes.c_int),\n\t\t(""prop_kind"", ctypes.c_int),\n\t\t(""cell_kind"", ctypes.c_int),\n\t\t(""direction"", ctypes.c_int),\n\n\t\t(""src_layer_desc"", dnnl_memory_desc_t),\n\t\t(""src_iter_desc"", dnnl_memory_desc_t),\n\t\t(""src_iter_c_desc"", dnnl_memory_desc_t),\n\t\t(""weights_layer_desc"", dnnl_memory_desc_t),\n\t\t(""weights_iter_desc"", dnnl_memory_desc_t),\n\t\t(""bias_desc"", dnnl_memory_desc_t),\n\t\t(""dst_layer_desc"", dnnl_memory_desc_t),\n\t\t(""dst_iter_desc"", dnnl_memory_desc_t),\n\t\t(""src_iter_c_desc"", dnnl_memory_desc_t),\n\t\t(""placeholder_desc"", dnnl_memory_desc_t),\n\t\t(""placeholder2_desc"", dnnl_memory_desc_t),\n\n\t\t(""diff_src_layer_desc"", dnnl_memory_desc_t),\n\t\t(""diff_src_iter_desc"", dnnl_memory_desc_t),\n\t\t(""diff_src_iter_c_desc"", dnnl_memory_desc_t),\n\t\t(""diff_weights_layer_desc"", dnnl_memory_desc_t),\n\t\t(""diff_weights_iter_desc"", dnnl_memory_desc_t),\n\t\t(""diff_bias_desc"", dnnl_memory_desc_t),\n\t\t(""diff_dst_layer_desc"", dnnl_memory_desc_t),\n\t\t(""diff_dst_iter_desc"", dnnl_memory_desc_t),\n\t\t(""diff_dst_iter_c_desc"", dnnl_memory_desc_t),\n\t\t(""diff_placeholder_desc"", dnnl_memory_desc_t),\n\t\t(""diff_placeholder2_desc"", dnnl_memory_desc_t),\n\n\t\t(""flags"", ctypes.c_uint),\n\t\t(""activation_kind"", ctypes.c_int),\n\t\t(""alpha"", ctypes.c_float),\n\t\t(""beta"", ctypes.c_float)\n\t]\n\n\ndnnl_engine_kind_t = {\n\t""dnnl_any_engine"": 0,\n\t""dnnl_cpu"": 1,\n\t""dnnl_gpu"": 2\n}\n\n\ndnnl_ARG = {\n\t""dnnl_ARG_SRC_0"": 1,\n\t""dnnl_ARG_SRC_1"": 2,\n\t""dnnl_ARG_SRC_2"": 3,\n\n\t""dnnl_ARG_DST_0"": 17,\n\t""dnnl_ARG_DST_1"": 18,\n\t""dnnl_ARG_DST_2"": 19,\n\n\t""dnnl_ARG_WEIGHTS_0"": 33,\n\t""dnnl_ARG_WEIGHTS_1"": 34,\n\n\t""dnnl_ARG_BIAS"": 41,\n\n\t""dnnl_ARG_MEAN"": 49,\n\t""dnnl_ARG_VARIANCE"": 50,\n\n\t""dnnl_ARG_WORKSPACE"": 64,\n\t""dnnl_ARG_SCRATCHPAD"": 80,\n\n\t""dnnl_ARG_DIFF_SRC_0"": 129,\n\t""dnnl_ARG_DIFF_SRC_1"": 130,\n\t""dnnl_ARG_DIFF_SRC_2"": 131,\n\n\t""dnnl_ARG_DIFF_DST_0"": 145,\n\t""dnnl_ARG_DIFF_DST_1"": 146,\n\t""dnnl_ARG_DIFF_DST_2"": 147,\n\n\t""dnnl_ARG_DIFF_WEIGHTS_0"": 161,\n\t""dnnl_ARG_DIFF_WEIGHTS_1"": 162,\n\n\t""dnnl_ARG_DIFF_BIAS"": 169,\n\n\t""dnnl_ARG_MULTIPLE_SRC"": 1024,\n\t""dnnl_ARG_MULTIPLE_DST"": 2048\n}\n\n\nclass dnnl_exec_arg_t(ctypes.Structure):\n\t_fields_ = [\n\t\t(""arg"", ctypes.c_int),\n\t\t(""memory"", ctypes.c_void_p)\n\t]\n\n\ndnnl_query_t = {\n\t""dnnl_query_undef"": 0,\n\n\t""dnnl_query_engine"": 1,\n\t""dnnl_query_primitive_kind"": 2,\n\n\t""dnnl_query_num_of_inputs_s32"": 3,\n\t""dnnl_query_num_of_outputs_s32"": 4,\n\n\t""dnnl_query_time_estimate_f64"": 5,\n\t""dnnl_query_memory_consumption_s64"": 6,\n\n\t""dnnl_query_scratchpad_engine"": 7,\n\t""dnnl_query_impl_info_str"": 8,\n\n\t""dnnl_query_some_d"": 64,\n\t""dnnl_query_op_d"": 65,\n\t""dnnl_query_convolution_d"": 66,\n\t""dnnl_query_deconvolution_d"": 67,\n\t""dnnl_query_shuffle_d"": 68,\n\t""dnnl_query_eltwise_d"": 69,\n\t""dnnl_query_softmax_d"": 70,\n\t""dnnl_query_pooling_d"": 71,\n\t""dnnl_query_lrn_d"": 72,\n\t""dnnl_query_batch_normalization_d"": 73,\n\t""dnnl_query_inner_product_d"": 74,\n\t""dnnl_query_rnn_d"": 75,\n\t""dnnl_query_gemm_d"": 76,\n\n\t""dnnl_query_some_md"": 128,\n\t""dnnl_query_src_md"": 129,\n\t""dnnl_query_diff_src_md"": 130,\n\t""dnnl_query_weights_md"": 131,\n\t""dnnl_query_diff_weights_md"": 132,\n\t""dnnl_query_dst_md"": 133,\n\t""dnnl_query_diff_dst_md"": 134,\n\t""dnnl_query_workspace_md"": 135,\n\t""dnnl_query_scratchpad_md"": 136\n}\n\n\ndnnl_stream_flags_t = {\n\t""dnnl_stream_default_order"": 0x1,\n\t""dnnl_stream_in_order"": 0x2,\n\t""dnnl_stream_out_of_order"": 0x4\n}\n\n\ndef dnnlCheckStatus(status):\n\tif status != 0:\n\t\ttry:\n\t\t\traise dnnlExceptions[status]\n\t\texcept KeyError:\n\t\t\traise dnnlError\n\n\n_libdnnl.dnnl_primitive_attr_create.restype = int\n_libdnnl.dnnl_primitive_attr_create.argtypes = [ctypes.POINTER(ctypes.c_void_p)]\ndef dnnl_primitive_attr_create():\n\tattr = ctypes.c_void_p()\n\n\tstatus = _libdnnl.dnnl_primitive_attr_create(ctypes.byref(attr))\n\tdnnlCheckStatus(status)\n\n\treturn attr\n\n\n_libdnnl.dnnl_primitive_attr_destroy.restype = int\n_libdnnl.dnnl_primitive_attr_destroy.argtypes = [ctypes.c_void_p]\ndef dnnl_primitive_attr_destroy(attr):\n\tstatus = _libdnnl.dnnl_primitive_attr_destroy(attr)\n\tdnnlCheckStatus(status)\n\n\n_libdnnl.dnnl_primitive_attr_get_output_scales.restype = int\n_libdnnl.dnnl_primitive_attr_get_output_scales.argtypes = [\n\tctypes.c_void_p, ctypes.POINTER(dnnl_dim_t), ctypes.POINTER(ctypes.c_int),\n\tctypes.POINTER(ctypes.POINTER(ctypes.c_float))\n]\ndef dnnl_primitive_attr_get_output_scales(attr):\n\tcount, mask = dnnl_dim_t(), ctypes.c_int()\n\tscales = ctypes.POINTER(ctypes.c_float)()\n\n\tstatus = _libdnnl.dnnl_primitive_attr_get_output_scales(\n\t\tattr, ctypes.byref(count), ctypes.byref(mask), ctypes.byref(scales)\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn mask.value, list(scales[:count.value])\n\n\n_libdnnl.dnnl_primitive_attr_set_output_scales.restype = int\n_libdnnl.dnnl_primitive_attr_set_output_scales.argtypes = [\n\tctypes.c_void_p, dnnl_dim_t, ctypes.c_int, ctypes.POINTER(ctypes.c_float)\n]\ndef dnnl_primitive_attr_set_output_scales(attr, mask, scales):\n\tscales = (ctypes.c_float * len(scales))(*scales)\n\n\tstatus = _libdnnl.dnnl_primitive_attr_set_output_scales(attr, len(scales), mask, scales)\n\tdnnlCheckStatus(status)\n\n\n_libdnnl.dnnl_primitive_desc_create.restype = int\n_libdnnl.dnnl_primitive_desc_create.argtypes = [\n\tctypes.POINTER(ctypes.c_void_p), ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p\n]\ndef dnnl_primitive_desc_create(op_desc, attr, engine, hint_forward_primitive_desc):\n\tprimitive_desc = ctypes.c_void_p()\n\n\tstatus = _libdnnl.dnnl_primitive_desc_create(\n\t\tctypes.byref(primitive_desc), ctypes.addressof(op_desc), attr, engine, hint_forward_primitive_desc\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn primitive_desc.value\n\n\n_libdnnl.dnnl_primitive_desc_destroy.restype = int\n_libdnnl.dnnl_primitive_desc_destroy.argtypes = [ctypes.c_void_p]\ndef dnnl_primitive_desc_destroy(primitive_desc):\n\tstatus = _libdnnl.dnnl_primitive_desc_destroy(primitive_desc)\n\tdnnlCheckStatus(status)\n\n\n_libdnnl.dnnl_primitive_desc_query_md.restype = ctypes.POINTER(dnnl_memory_desc_t)\n_libdnnl.dnnl_primitive_desc_query_md.argtypes = [ctypes.c_void_p, ctypes.c_int, ctypes.c_int]\ndef dnnl_primitive_desc_query_md(primitive_desc, what, index):\n\treturn _libdnnl.dnnl_primitive_desc_query_md(primitive_desc, what, index)\n\n\n_libdnnl.dnnl_primitive_create.restype = int\n_libdnnl.dnnl_primitive_create.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\ndef dnnl_primitive_create(primitive_desc):\n\tprimitive = ctypes.c_void_p()\n\n\tstatus = _libdnnl.dnnl_primitive_create(ctypes.byref(primitive), primitive_desc)\n\tdnnlCheckStatus(status)\n\n\treturn primitive.value\n\n\n_libdnnl.dnnl_primitive_execute.restype = int\n_libdnnl.dnnl_primitive_execute.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.POINTER(dnnl_exec_arg_t)\n]\ndef dnnl_primitive_execute(primitive, stream, args):\n\tnargs = len(args)\n\targs = (dnnl_exec_arg_t * nargs)(*args)\n\n\tstatus = _libdnnl.dnnl_primitive_execute(primitive, stream, nargs, args)\n\tdnnlCheckStatus(status)\n\n\n_libdnnl.dnnl_primitive_destroy.restype = int\n_libdnnl.dnnl_primitive_destroy.argtypes = [ctypes.c_void_p]\ndef dnnl_primitive_destroy(primitive):\n\tstatus = _libdnnl.dnnl_primitive_destroy(primitive)\n\tdnnlCheckStatus(status)\n\n\n_libdnnl.dnnl_memory_desc_init_by_tag.restype = int\n_libdnnl.dnnl_memory_desc_init_by_tag.argtypes = [\n\tctypes.POINTER(dnnl_memory_desc_t), ctypes.c_int, ctypes.POINTER(dnnl_dim_t), ctypes.c_int, ctypes.c_int\n]\ndef dnnl_memory_desc_init_by_tag(dims, data_type, tag):\n\tmemory_desc = dnnl_memory_desc_t()\n\n\tndims = len(dims)\n\tdims = (dnnl_dim_t * len(dims))(*dims)\n\n\tstatus = _libdnnl.dnnl_memory_desc_init_by_tag(ctypes.byref(memory_desc), ndims, dims, data_type, tag)\n\tdnnlCheckStatus(status)\n\n\treturn memory_desc\n\n\n_libdnnl.dnnl_memory_desc_get_size.restype = ctypes.c_size_t\n_libdnnl.dnnl_memory_desc_get_size.argtypes = [ctypes.POINTER(dnnl_memory_desc_t)]\ndef dnnl_memory_desc_get_size(memory_desc):\n\tif isinstance(memory_desc, dnnl_memory_desc_t):\n\t\tmemory_desc = ctypes.byref(memory_desc)\n\n\treturn _libdnnl.dnnl_memory_desc_get_size(memory_desc)\n\n\n_libdnnl.dnnl_memory_create.restype = int\n_libdnnl.dnnl_memory_create.argtypes = [\n\tctypes.c_void_p, ctypes.POINTER(dnnl_memory_desc_t), ctypes.c_void_p, ctypes.c_void_p\n]\ndef dnnl_memory_create(memory_desc, engine):\n\tmemory = ctypes.c_void_p()\n\n\tif isinstance(memory_desc, dnnl_memory_desc_t):\n\t\tmemory_desc = ctypes.byref(memory_desc)\n\n\tstatus = _libdnnl.dnnl_memory_create(ctypes.byref(memory), memory_desc, engine, None)\n\tdnnlCheckStatus(status)\n\n\treturn memory\n\n\n_libdnnl.dnnl_memory_get_data_handle.restype = int\n_libdnnl.dnnl_memory_get_data_handle.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\ndef dnnl_memory_get_data_handle(memory):\n\thandle = ctypes.c_void_p()\n\n\tstatus = _libdnnl.dnnl_memory_get_data_handle(memory, ctypes.byref(handle))\n\tdnnlCheckStatus(status)\n\n\treturn handle.value\n\n\n_libdnnl.dnnl_memory_set_data_handle.restype = int\n_libdnnl.dnnl_memory_set_data_handle.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\ndef dnnl_memory_set_data_handle(memory, handle):\n\tstatus = _libdnnl.dnnl_memory_set_data_handle(memory, handle)\n\tdnnlCheckStatus(status)\n\n\n_libdnnl.dnnl_memory_destroy.restype = int\n_libdnnl.dnnl_memory_destroy.argtypes = [ctypes.c_void_p]\ndef dnnl_memory_destroy(memory):\n\tstatus = _libdnnl.dnnl_memory_destroy(memory)\n\tdnnlCheckStatus(status)\n\n\n_libdnnl.dnnl_reorder_primitive_desc_create.restype = int\n_libdnnl.dnnl_reorder_primitive_desc_create.argtypes = [\n\tctypes.POINTER(ctypes.c_void_p), ctypes.POINTER(dnnl_memory_desc_t), ctypes.c_void_p,\n\tctypes.POINTER(dnnl_memory_desc_t), ctypes.c_void_p, ctypes.c_void_p\n]\ndef dnnl_reorder_primitive_desc_create(src_md, src_engine, dst_md, dst_engine, attr):\n\treorder_primitive_desc = ctypes.c_void_p()\n\n\tstatus = _libdnnl.dnnl_reorder_primitive_desc_create(\n\t\tctypes.byref(reorder_primitive_desc), src_md, src_engine, dst_md, dst_engine, attr\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn reorder_primitive_desc.value\n\n\n_libdnnl.dnnl_convolution_forward_desc_init.restype = int\n_libdnnl.dnnl_convolution_forward_desc_init.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.c_int, ctypes.POINTER(dnnl_memory_desc_t),\n\tctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t),\n\tctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t)\n]\ndef dnnl_convolution_forward_desc_init(prop_kind, alg_kind, src_desc, weights_desc, bias_desc, dst_desc, strides,\n\t\t\t\t\t\t\t\t\t\t padding):\n\tconv_desc = dnnl_convolution_desc_t()\n\n\tstrides = (dnnl_dim_t * len(strides))(*strides)\n\tpadding = (dnnl_dim_t * len(padding))(*padding)\n\n\tif bias_desc is not None:\n\t\tbias_desc = ctypes.byref(bias_desc)\n\n\tstatus = _libdnnl.dnnl_convolution_forward_desc_init(\n\t\tctypes.byref(conv_desc), prop_kind, alg_kind, ctypes.byref(src_desc), ctypes.byref(weights_desc),\n\t\tbias_desc, ctypes.byref(dst_desc), strides, padding, None\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn conv_desc\n\n\n_libdnnl.dnnl_dilated_convolution_forward_desc_init.restype = int\n_libdnnl.dnnl_dilated_convolution_forward_desc_init.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.c_int, ctypes.POINTER(dnnl_memory_desc_t),\n\tctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t),\n\tctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t),\n\tctypes.POINTER(dnnl_dim_t)\n]\ndef dnnl_dilated_convolution_forward_desc_init(prop_kind, alg_kind, src_desc, weights_desc, bias_desc, dst_desc,\n\t\t\t\t\t\t\t\t\t\t\t\t strides, dilates, padding):\n\tconv_desc = dnnl_convolution_desc_t()\n\n\tstrides = (dnnl_dim_t * len(strides))(*strides)\n\tdilates = (dnnl_dim_t * len(dilates))(*dilates)\n\tpadding = (dnnl_dim_t * len(padding))(*padding)\n\n\tif bias_desc is not None:\n\t\tbias_desc = ctypes.byref(bias_desc)\n\n\tstatus = _libdnnl.dnnl_dilated_convolution_forward_desc_init(\n\t\tctypes.byref(conv_desc), prop_kind, alg_kind, ctypes.byref(src_desc), ctypes.byref(weights_desc),\n\t\tbias_desc, ctypes.byref(dst_desc), strides, dilates, padding, None\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn conv_desc\n\n\n_libdnnl.dnnl_convolution_backward_data_desc_init.restype = int\n_libdnnl.dnnl_convolution_backward_data_desc_init.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t),\n\tctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t),\n\tctypes.POINTER(dnnl_dim_t)\n]\ndef dnnl_convolution_backward_data_desc_init(alg_kind, diff_src_desc, weights_desc, diff_dst_desc, strides, padding):\n\tconv_desc = dnnl_convolution_desc_t()\n\n\tstrides = (dnnl_dim_t * len(strides))(*strides)\n\tpadding = (dnnl_dim_t * len(padding))(*padding)\n\n\tstatus = _libdnnl.dnnl_convolution_backward_data_desc_init(\n\t\tctypes.byref(conv_desc), alg_kind, ctypes.byref(diff_src_desc), ctypes.byref(weights_desc),\n\t\tctypes.byref(diff_dst_desc), strides, padding, None\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn conv_desc\n\n\n_libdnnl.dnnl_dilated_convolution_backward_data_desc_init.restype = int\n_libdnnl.dnnl_dilated_convolution_backward_data_desc_init.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t),\n\tctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t),\n\tctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t)\n]\ndef dnnl_dilated_convolution_backward_data_desc_init(alg_kind, diff_src_desc, weights_desc, diff_dst_desc, strides,\n\t\t\t\t\t\t\t\t\t\t\t\t\t   dilates, padding):\n\tconv_desc = dnnl_convolution_desc_t()\n\n\tstrides = (dnnl_dim_t * len(strides))(*strides)\n\tdilates = (dnnl_dim_t * len(dilates))(*dilates)\n\tpadding = (dnnl_dim_t * len(padding))(*padding)\n\n\tstatus = _libdnnl.dnnl_dilated_convolution_backward_data_desc_init(\n\t\tctypes.byref(conv_desc), alg_kind, ctypes.byref(diff_src_desc), ctypes.byref(weights_desc),\n\t\tctypes.byref(diff_dst_desc), strides, dilates, padding, None\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn conv_desc\n\n\n_libdnnl.dnnl_convolution_backward_weights_desc_init.restype = int\n_libdnnl.dnnl_convolution_backward_weights_desc_init.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t),\n\tctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_dim_t),\n\tctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t)\n]\ndef dnnl_convolution_backward_weights_desc_init(alg_kind, src_desc, diff_weights_desc, diff_bias_desc, diff_dst_desc,\n\t\t\t\t\t\t\t\t\t\t\t\t  strides, padding):\n\tconv_desc = dnnl_convolution_desc_t()\n\n\tstrides = (dnnl_dim_t * len(strides))(*strides)\n\tpadding = (dnnl_dim_t * len(padding))(*padding)\n\n\tif diff_bias_desc is not None:\n\t\tdiff_bias_desc = ctypes.byref(diff_bias_desc)\n\n\tstatus = _libdnnl.dnnl_convolution_backward_weights_desc_init(\n\t\tctypes.byref(conv_desc), alg_kind, ctypes.byref(src_desc), ctypes.byref(diff_weights_desc),\n\t\tdiff_bias_desc, ctypes.byref(diff_dst_desc), strides, padding, None\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn conv_desc\n\n\n_libdnnl.dnnl_dilated_convolution_backward_weights_desc_init.restype = int\n_libdnnl.dnnl_dilated_convolution_backward_weights_desc_init.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t),\n\tctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_dim_t),\n\tctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t)\n]\ndef dnnl_dilated_convolution_backward_weights_desc_init(alg_kind, src_desc, diff_weights_desc, diff_bias_desc,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t  diff_dst_desc, strides, dilates, padding):\n\tconv_desc = dnnl_convolution_desc_t()\n\n\tstrides = (dnnl_dim_t * len(strides))(*strides)\n\tdilates = (dnnl_dim_t * len(dilates))(*dilates)\n\tpadding = (dnnl_dim_t * len(padding))(*padding)\n\n\tstatus = _libdnnl.dnnl_dilated_convolution_backward_weights_desc_init(\n\t\tctypes.byref(conv_desc), alg_kind, ctypes.byref(src_desc), ctypes.byref(diff_weights_desc),\n\t\tctypes.byref(diff_bias_desc), ctypes.byref(diff_dst_desc), strides, dilates, padding, None\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn conv_desc\n\n\n_libdnnl.dnnl_deconvolution_forward_desc_init.restype = int\n_libdnnl.dnnl_deconvolution_forward_desc_init.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.c_int, ctypes.POINTER(dnnl_memory_desc_t),\n\tctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t),\n\tctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t)\n]\ndef dnnl_deconvolution_forward_desc_init(prop_kind, alg_kind, src_desc, weights_desc, bias_desc, dst_desc, strides,\n\t\t\t\t\t\t\t\t\t\t   padding):\n\tconv_desc = dnnl_deconvolution_desc_t()\n\n\tstrides = (dnnl_dim_t * len(strides))(*strides)\n\tpadding = (dnnl_dim_t * len(padding))(*padding)\n\n\tif bias_desc is not None:\n\t\tbias_desc = ctypes.byref(bias_desc)\n\n\tstatus = _libdnnl.dnnl_deconvolution_forward_desc_init(\n\t\tctypes.byref(conv_desc), prop_kind, alg_kind, ctypes.byref(src_desc), ctypes.byref(weights_desc),\n\t\tbias_desc, ctypes.byref(dst_desc), strides, padding, None\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn conv_desc\n\n\n_libdnnl.dnnl_dilated_deconvolution_forward_desc_init.restype = int\n_libdnnl.dnnl_dilated_deconvolution_forward_desc_init.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.c_int, ctypes.POINTER(dnnl_memory_desc_t),\n\tctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t),\n\tctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t),\n\tctypes.POINTER(dnnl_dim_t)\n]\ndef dnnl_dilated_deconvolution_forward_desc_init(prop_kind, alg_kind, src_desc, weights_desc, bias_desc, dst_desc,\n\t\t\t\t\t\t\t\t\t\t\t\t   strides, dilates, padding):\n\tconv_desc = dnnl_deconvolution_desc_t()\n\n\tstrides = (dnnl_dim_t * len(strides))(*strides)\n\tdilates = (dnnl_dim_t * len(dilates))(*dilates)\n\tpadding = (dnnl_dim_t * len(padding))(*padding)\n\n\tif bias_desc is not None:\n\t\tbias_desc = ctypes.byref(bias_desc)\n\n\tstatus = _libdnnl.dnnl_dilated_deconvolution_forward_desc_init(\n\t\tctypes.byref(conv_desc), prop_kind, alg_kind, ctypes.byref(src_desc), ctypes.byref(weights_desc),\n\t\tbias_desc, ctypes.byref(dst_desc), strides, dilates, padding, None\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn conv_desc\n\n\n_libdnnl.dnnl_deconvolution_backward_data_desc_init.restype = int\n_libdnnl.dnnl_deconvolution_backward_data_desc_init.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t),\n\tctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t),\n\tctypes.POINTER(dnnl_dim_t)\n]\ndef dnnl_deconvolution_backward_data_desc_init(alg_kind, diff_src_desc, weights_desc, diff_dst_desc, strides,\n\t\t\t\t\t\t\t\t\t\t\t\t padding):\n\tconv_desc = dnnl_deconvolution_desc_t()\n\n\tstrides = (dnnl_dim_t * len(strides))(*strides)\n\tpadding = (dnnl_dim_t * len(padding))(*padding)\n\n\tstatus = _libdnnl.dnnl_deconvolution_backward_data_desc_init(\n\t\tctypes.byref(conv_desc), alg_kind, ctypes.byref(diff_src_desc), ctypes.byref(weights_desc),\n\t\tctypes.byref(diff_dst_desc), strides, padding, None\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn conv_desc\n\n\n_libdnnl.dnnl_dilated_deconvolution_backward_data_desc_init.restype = int\n_libdnnl.dnnl_dilated_deconvolution_backward_data_desc_init.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t),\n\tctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t),\n\tctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t)\n]\ndef dnnl_dilated_deconvolution_backward_data_desc_init(alg_kind, diff_src_desc, weights_desc, diff_dst_desc, strides,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t dilates, padding):\n\tconv_desc = dnnl_deconvolution_desc_t()\n\n\tstrides = (dnnl_dim_t * len(strides))(*strides)\n\tdilates = (dnnl_dim_t * len(dilates))(*dilates)\n\tpadding = (dnnl_dim_t * len(padding))(*padding)\n\n\tstatus = _libdnnl.dnnl_dilated_deconvolution_backward_data_desc_init(\n\t\tctypes.byref(conv_desc), alg_kind, ctypes.byref(diff_src_desc), ctypes.byref(weights_desc),\n\t\tctypes.byref(diff_dst_desc), strides, dilates, padding, None\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn conv_desc\n\n\n_libdnnl.dnnl_deconvolution_backward_weights_desc_init.restype = int\n_libdnnl.dnnl_deconvolution_backward_weights_desc_init.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t),\n\tctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_dim_t),\n\tctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t)\n]\ndef dnnl_deconvolution_backward_weights_desc_init(alg_kind, src_desc, diff_weights_desc, diff_bias_desc,\n\t\t\t\t\t\t\t\t\t\t\t\t\tdiff_dst_desc, strides, padding):\n\tconv_desc = dnnl_deconvolution_desc_t()\n\n\tstrides = (dnnl_dim_t * len(strides))(*strides)\n\tpadding = (dnnl_dim_t * len(padding))(*padding)\n\n\tif diff_bias_desc is not None:\n\t\tdiff_bias_desc = ctypes.byref(diff_bias_desc)\n\n\tstatus = _libdnnl.dnnl_deconvolution_backward_weights_desc_init(\n\t\tctypes.byref(conv_desc), alg_kind, ctypes.byref(src_desc), ctypes.byref(diff_weights_desc),\n\t\tdiff_bias_desc, ctypes.byref(diff_dst_desc), strides, padding, None\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn conv_desc\n\n\n_libdnnl.dnnl_dilated_deconvolution_backward_weights_desc_init.restype = int\n_libdnnl.dnnl_dilated_deconvolution_backward_weights_desc_init.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t),\n\tctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_dim_t),\n\tctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t)\n]\ndef dnnl_dilated_deconvolution_backward_weights_desc_init(alg_kind, src_desc, diff_weights_desc, diff_bias_desc,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdiff_dst_desc, strides, dilates, padding):\n\tconv_desc = dnnl_deconvolution_desc_t()\n\n\tstrides = (dnnl_dim_t * len(strides))(*strides)\n\tdilates = (dnnl_dim_t * len(dilates))(*dilates)\n\tpadding = (dnnl_dim_t * len(padding))(*padding)\n\n\tif diff_bias_desc is not None:\n\t\tdiff_bias_desc = ctypes.byref(diff_bias_desc)\n\n\tstatus = _libdnnl.dnnl_dilated_deconvolution_backward_weights_desc_init(\n\t\tctypes.byref(conv_desc), alg_kind, ctypes.byref(src_desc), ctypes.byref(diff_weights_desc),\n\t\tdiff_bias_desc, ctypes.byref(diff_dst_desc), strides, dilates, padding, None\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn conv_desc\n\n\n_libdnnl.dnnl_softmax_forward_desc_init.restype = int\n_libdnnl.dnnl_softmax_forward_desc_init.argtypes = [ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_int]\ndef dnnl_softmax_forward_desc_init(prop_kind, data_desc, softmax_axis):\n\tsoftmax_desc = dnnl_softmax_desc_t()\n\n\tstatus = _libdnnl.dnnl_softmax_forward_desc_init(\n\t\tctypes.byref(softmax_desc), prop_kind, ctypes.byref(data_desc), softmax_axis\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn softmax_desc\n\n\n_libdnnl.dnnl_softmax_backward_desc_init.restype = int\n_libdnnl.dnnl_softmax_backward_desc_init.argtypes = [\n\tctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int\n]\ndef dnnl_softmax_backward_desc_init(diff_desc, data_desc, softmax_axis):\n\tsoftmax_desc = dnnl_softmax_desc_t()\n\n\tstatus = _libdnnl.dnnl_softmax_backward_desc_init(\n\t\tctypes.byref(softmax_desc), ctypes.byref(diff_desc), ctypes.byref(data_desc), softmax_axis\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn softmax_desc\n\n\n_libdnnl.dnnl_pooling_forward_desc_init.restype = int\n_libdnnl.dnnl_pooling_forward_desc_init.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.c_int, ctypes.POINTER(dnnl_memory_desc_t),\n\tctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t),\n\tctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t)\n]\ndef dnnl_pooling_forward_desc_init(prop_kind, alg_kind, src_desc, dst_desc, strides, kernel, padding):\n\tpool_desc = dnnl_pooling_desc_t()\n\n\tstrides = (dnnl_dim_t * len(strides))(*strides)\n\tkernel = (dnnl_dim_t * len(kernel))(*kernel)\n\tpadding = (dnnl_dim_t * len(padding))(*padding)\n\n\tstatus = _libdnnl.dnnl_pooling_forward_desc_init(\n\t\tctypes.byref(pool_desc), prop_kind, alg_kind, ctypes.byref(src_desc), ctypes.byref(dst_desc),\n\t\tstrides, kernel, padding, None\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn pool_desc\n\n\n_libdnnl.dnnl_pooling_backward_desc_init.restype = int\n_libdnnl.dnnl_pooling_backward_desc_init.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t),\n\tctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t), ctypes.POINTER(dnnl_dim_t),\n\tctypes.POINTER(dnnl_dim_t)\n]\ndef dnnl_pooling_backward_desc_init(alg_kind, diff_src_desc, diff_dst_desc, strides, kernel, padding):\n\tpool_desc = dnnl_pooling_desc_t()\n\n\tstrides = (dnnl_dim_t * len(strides))(*strides)\n\tkernel = (dnnl_dim_t * len(kernel))(*kernel)\n\tpadding = (dnnl_dim_t * len(padding))(*padding)\n\n\tstatus = _libdnnl.dnnl_pooling_backward_desc_init(\n\t\tctypes.byref(pool_desc), alg_kind, ctypes.byref(diff_src_desc), ctypes.byref(diff_dst_desc),\n\t\tstrides, kernel, padding, None\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn pool_desc\n\n\n_libdnnl.dnnl_lrn_forward_desc_init.restype = int\n_libdnnl.dnnl_lrn_forward_desc_init.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.c_int, ctypes.POINTER(dnnl_memory_desc_t),\n\tdnnl_dim_t, ctypes.c_float, ctypes.c_float, ctypes.c_float\n]\ndef dnnl_lrn_forward_desc_init(prop_kind, alg_kind, data_desc, local_size, alpha, beta, k):\n\tlrn_desc = dnnl_lrn_desc_t()\n\n\tstatus = _libdnnl.dnnl_lrn_forward_desc_init(\n\t\tctypes.byref(lrn_desc), prop_kind, alg_kind, ctypes.byref(data_desc), local_size, alpha, beta, k\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn lrn_desc\n\n\n_libdnnl.dnnl_lrn_backward_desc_init.restype = int\n_libdnnl.dnnl_lrn_backward_desc_init.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t),\n\tdnnl_dim_t, ctypes.c_float, ctypes.c_float, ctypes.c_float\n]\ndef dnnl_lrn_backward_desc_init(alg_kind, data_desc, diff_data_desc, local_size, alpha, beta, k):\n\tlrn_desc = dnnl_lrn_desc_t()\n\n\tstatus = _libdnnl.dnnl_lrn_backward_desc_init(\n\t\tctypes.byref(lrn_desc), alg_kind, ctypes.byref(data_desc), ctypes.byref(diff_data_desc),\n\t\tlocal_size, alpha, beta, k\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn lrn_desc\n\n\n_libdnnl.dnnl_batch_normalization_forward_desc_init.restype = int\n_libdnnl.dnnl_batch_normalization_forward_desc_init.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.POINTER(dnnl_memory_desc_t), ctypes.c_float, ctypes.c_uint\n]\ndef dnnl_batch_normalization_forward_desc_init(prop_kind, data_desc, epsilon, flags):\n\tbnrm_desc = dnnl_batch_normalization_desc_t()\n\n\tstatus = _libdnnl.dnnl_batch_normalization_forward_desc_init(\n\t\tctypes.byref(bnrm_desc), prop_kind, ctypes.byref(data_desc), epsilon, flags\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn bnrm_desc\n\n\n_libdnnl.dnnl_batch_normalization_backward_desc_init.restype = int\n_libdnnl.dnnl_batch_normalization_backward_desc_init.argtypes = [\n\tctypes.c_void_p, ctypes.c_int, ctypes.POINTER(dnnl_memory_desc_t), ctypes.POINTER(dnnl_memory_desc_t),\n\tctypes.c_float, ctypes.c_uint\n]\ndef dnnl_batch_normalization_backward_desc_init(prop_kind, diff_data_desc, data_desc, epsilon, flags):\n\tbnrm_desc = dnnl_batch_normalization_desc_t()\n\n\tstatus = _libdnnl.dnnl_batch_normalization_backward_desc_init(\n\t\tctypes.byref(bnrm_desc), prop_kind, ctypes.byref(diff_data_desc), ctypes.byref(data_desc), epsilon, flags\n\t)\n\tdnnlCheckStatus(status)\n\n\treturn bnrm_desc\n\n\n_libdnnl.dnnl_engine_get_count.restype = ctypes.c_size_t\n_libdnnl.dnnl_engine_get_count.argtypes = [ctypes.c_int]\ndef dnnl_engine_get_count(kind):\n\treturn _libdnnl.dnnl_engine_get_count(kind)\n\n\n_libdnnl.dnnl_engine_create.restype = int\n_libdnnl.dnnl_engine_create.argtypes = [ctypes.c_void_p, ctypes.c_int, ctypes.c_size_t]\ndef dnnl_engine_create(kind, index):\n\tengine = ctypes.c_void_p()\n\n\tstatus = _libdnnl.dnnl_engine_create(ctypes.byref(engine), kind, index)\n\tdnnlCheckStatus(status)\n\n\treturn engine.value\n\n\n_libdnnl.dnnl_engine_get_kind.restype = int\n_libdnnl.dnnl_engine_get_kind.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\ndef dnnl_engine_get_kind(engine):\n\tkind = ctypes.c_int()\n\n\tstatus = _libdnnl.dnnl_engine_get_kind(engine, ctypes.byref(kind))\n\tdnnlCheckStatus(status)\n\n\treturn kind.value\n\n\n_libdnnl.dnnl_engine_destroy.restype = int\n_libdnnl.dnnl_engine_destroy.argtypes = [ctypes.c_void_p]\ndef dnnl_engine_destroy(engine):\n\tstatus = _libdnnl.dnnl_engine_destroy(engine)\n\tdnnlCheckStatus(status)\n\n\n_libdnnl.dnnl_stream_create.restype = int\n_libdnnl.dnnl_stream_create.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int]\ndef dnnl_stream_create(engine, flags):\n\tstream = ctypes.c_void_p()\n\n\tstatus = _libdnnl.dnnl_stream_create(ctypes.byref(stream), engine, flags)\n\tdnnlCheckStatus(status)\n\n\treturn stream.value\n\n\n_libdnnl.dnnl_stream_wait.restype = int\n_libdnnl.dnnl_stream_wait.argtypes = [ctypes.c_void_p]\ndef dnnl_stream_wait(stream):\n\tstatus = _libdnnl.dnnl_stream_wait(stream)\n\tdnnlCheckStatus(status)\n\n\n_libdnnl.dnnl_stream_destroy.restype = int\n_libdnnl.dnnl_stream_destroy.argtypes = [ctypes.c_void_p]\ndef dnnl_stream_destroy(stream):\n\tstatus = _libdnnl.dnnl_stream_destroy(stream)\n\tdnnlCheckStatus(status)\n\n\n_libdnnl.dnnl_set_verbose.restype = int\n_libdnnl.dnnl_set_verbose.argtypes = [ctypes.c_int]\ndef dnnl_verbose_set(level):\n\tstatus = _libdnnl.dnnl_set_verbose(level)\n\tdnnlCheckStatus(status)\n\n\n_libdnnl.dnnl_version.restype = ctypes.POINTER(dnnl_version_t)\n_libdnnl.dnnl_version.argtypes = []\ndef dnnl_version():\n\tversion = _libdnnl.dnnl_version()\n\treturn version.contents\n\n\n_libdnnl.dnnl_sgemm.restype = int\n_libdnnl.dnnl_sgemm.argtypes = [\n\tctypes.c_char, ctypes.c_char, dnnl_dim_t, dnnl_dim_t, dnnl_dim_t, ctypes.c_float, ctypes.c_void_p,\n\tdnnl_dim_t, ctypes.c_void_p, dnnl_dim_t, ctypes.c_float, ctypes.c_void_p, dnnl_dim_t\n]\ndef dnnl_sgemm(transA, transB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc):\n\tstatus = _libdnnl.dnnl_sgemm(ord(transA), ord(transB), M, N, K, alpha, A, lda, B, ldb, beta, C, ldc)\n\tdnnlCheckStatus(status)\n'"
Intel/Wrappers/DNNL.py,109,"b'import math, multiprocessing\nfrom collections import namedtuple\nfrom enum import Enum\n\nimport numpy as np\n\nfrom PuzzleLib import Config\n\nfrom PuzzleLib.CPU.CPUArray import CPUArray\nfrom PuzzleLib.CPU.Wrappers import NumpyBlas\nfrom PuzzleLib.CPU.Benchmarks.Utils import timeKernel\n\nfrom PuzzleLib.Intel.ThirdParty import libdnnl\n\n\nclass EngineKind(Enum):\n\tany = libdnnl.dnnl_engine_kind_t[""dnnl_any_engine""]\n\tcpu = libdnnl.dnnl_engine_kind_t[""dnnl_cpu""]\n\n\nclass StreamFlags(Enum):\n\tdefault = libdnnl.dnnl_stream_flags_t[""dnnl_stream_default_order""]\n\n\nengine = None\nstream = None\n\n\ndef autoinit():\n\tglobal engine\n\tengine = libdnnl.dnnl_engine_create(EngineKind.cpu.value, Config.deviceIdx)\n\n\tif Config.systemLog:\n\t\tversion = libdnnl.dnnl_version()\n\n\t\tprint(""[%s]: Created dnnl engine (Using version: %s.%s.%s)"" % (\n\t\t\tConfig.libname, version.major, version.minor, version.patch\n\t\t))\n\n\tglobal stream\n\tstream = libdnnl.dnnl_stream_create(engine, StreamFlags.default.value)\n\n\tdef finishUp():\n\t\tlibdnnl.dnnl_stream_destroy(stream)\n\t\tlibdnnl.dnnl_engine_destroy(engine)\n\n\timport atexit\n\tatexit.register(finishUp)\n\n\nif engine is None and (multiprocessing.current_process().name == ""MainProcess"" or Config.allowMultiContext):\n\tautoinit()\n\n\nclass TensorFormat(Enum):\n\tany = libdnnl.dnnl_format_tag_t[""dnnl_format_tag_any""]\n\ta = libdnnl.dnnl_format_tag_t[""dnnl_a""]\n\tabcd = libdnnl.dnnl_format_tag_t[""dnnl_abcd""]\n\tabcde = libdnnl.dnnl_format_tag_t[""dnnl_abcde""]\n\n\nclass DataType(Enum):\n\ts8 = libdnnl.dnnl_data_type_t[""dnnl_s8""]\n\tu8 = libdnnl.dnnl_data_type_t[""dnnl_u8""]\n\ts32 = libdnnl.dnnl_data_type_t[""dnnl_s32""]\n\tf32 = libdnnl.dnnl_data_type_t[""dnnl_f32""]\n\n\nclass ConvPerf:\n\tdef __init__(self, algo, time, memory=0):\n\t\tself.algo = algo\n\t\tself.time = time\n\t\tself.memory = memory\n\n\nclass ConvAlgo(Enum):\n\tdirect = libdnnl.dnnl_alg_kind_t[""dnnl_convolution_direct""]\n\twinograd = libdnnl.dnnl_alg_kind_t[""dnnl_convolution_winograd""]\n\tauto = libdnnl.dnnl_alg_kind_t[""dnnl_convolution_auto""]\n\n\nclass DeconvAlgo(Enum):\n\tdirect = libdnnl.dnnl_alg_kind_t[""dnnl_deconvolution_direct""]\n\twinograd = libdnnl.dnnl_alg_kind_t[""dnnl_deconvolution_winograd""]\n\n\nclass PoolMode(Enum):\n\tmax = libdnnl.dnnl_alg_kind_t[""dnnl_pooling_max""]\n\tavgWithPad = libdnnl.dnnl_alg_kind_t[""dnnl_pooling_avg_include_padding""]\n\tavgNoPad = libdnnl.dnnl_alg_kind_t[""dnnl_pooling_avg_exclude_padding""]\n\n\nclass LRNMode(Enum):\n\tmap = libdnnl.dnnl_alg_kind_t[""dnnl_lrn_within_channel""]\n\tcross = libdnnl.dnnl_alg_kind_t[""dnnl_lrn_across_channels""]\n\n\nclass BatchNormFlags(Enum):\n\tuseGlobalStats = libdnnl.dnnl_normalization_flags_t[""dnnl_use_global_stats""]\n\tscaleShift = libdnnl.dnnl_normalization_flags_t[""dnnl_use_scaleshift""]\n\n\nclass PropKind(Enum):\n\tfwdTrain = libdnnl.dnnl_prop_kind_t[""dnnl_forward_training""]\n\tfwdInfer = libdnnl.dnnl_prop_kind_t[""dnnl_forward_inference""]\n\tbwdData = libdnnl.dnnl_prop_kind_t[""dnnl_backward_data""]\n\tbwdWeights = libdnnl.dnnl_prop_kind_t[""dnnl_backward_weights""]\n\tbwdBias = libdnnl.dnnl_prop_kind_t[""dnnl_backward_bias""]\n\tbackward = libdnnl.dnnl_prop_kind_t[""dnnl_backward""]\n\n\nclass Query(Enum):\n\tsrc = libdnnl.dnnl_query_t[""dnnl_query_src_md""]\n\tdiffSrc = libdnnl.dnnl_query_t[""dnnl_query_diff_src_md""]\n\tweights = libdnnl.dnnl_query_t[""dnnl_query_weights_md""]\n\tdiffWeights = libdnnl.dnnl_query_t[""dnnl_query_diff_weights_md""]\n\tdst = libdnnl.dnnl_query_t[""dnnl_query_dst_md""]\n\tdiffDst = libdnnl.dnnl_query_t[""dnnl_query_diff_dst_md""]\n\tworkspace = libdnnl.dnnl_query_t[""dnnl_query_workspace_md""]\n\n\nclass ArgIndex(Enum):\n\tsrc = libdnnl.dnnl_ARG[""dnnl_ARG_SRC_0""]\n\tweights = libdnnl.dnnl_ARG[""dnnl_ARG_WEIGHTS_0""]\n\tbias = libdnnl.dnnl_ARG[""dnnl_ARG_BIAS""]\n\tdst = libdnnl.dnnl_ARG[""dnnl_ARG_DST_0""]\n\n\tdiffSrc = libdnnl.dnnl_ARG[""dnnl_ARG_DIFF_SRC_0""]\n\tdiffWeights = libdnnl.dnnl_ARG[""dnnl_ARG_DIFF_WEIGHTS_0""]\n\tdiffBias = libdnnl.dnnl_ARG[""dnnl_ARG_DIFF_BIAS""]\n\tdiffDst = libdnnl.dnnl_ARG[""dnnl_ARG_DIFF_DST_0""]\n\n\tworkspace = libdnnl.dnnl_ARG[""dnnl_ARG_WORKSPACE""]\n\n\tmean = libdnnl.dnnl_ARG[""dnnl_ARG_MEAN""]\n\tvariance = libdnnl.dnnl_ARG[""dnnl_ARG_VARIANCE""]\n\n\nDescTensor = namedtuple(""DescTensor"", ""memory desc shape tensor"")\n\n\ndataTypeDct = {\n\tNone: DataType.f32,\n\tnp.float32: DataType.f32,\n\tnp.int8: DataType.s8,\n\tnp.uint8: DataType.u8,\n\tnp.int32: DataType.s32\n}\n\n\ndataFormatDct = {\n\t1: TensorFormat.a,\n\t4: TensorFormat.abcd,\n\t5: TensorFormat.abcde\n}\n\n\nconvPrimitiveCache, convBwdDataPrimitiveCache, convBwdParamPrimitiveCache = {}, {}, {}\npoolPrimitiveCache, poolBwdPrimitiveCache = {}, {}\nlrnPrimitiveCache, lrnBwdPrimitiveCache = {}, {}\nbnPrimitiveCache, bnBwdPrimitiveCache = {}, {}\n\n\ndef createMemoryDescriptor(shape, tensor=None, dtype=None, fmt=None):\n\tshape = tensor.shape if shape is None else shape\n\n\tdataType = dataTypeDct[tensor.dtype.type if tensor is not None else dtype.type]\n\tdataFormat = dataFormatDct[len(shape)] if fmt is None else fmt\n\n\tmemoryDesc = libdnnl.dnnl_memory_desc_init_by_tag(shape, dataType.value, dataFormat.value)\n\treturn memoryDesc\n\n\ndef queryDescribedNdTensor(desc, query, tensor, index=0):\n\tdesc = libdnnl.dnnl_primitive_desc_query_md(desc, query.value, index)\n\n\tmemory = libdnnl.dnnl_memory_create(desc, engine)\n\tlibdnnl.dnnl_memory_set_data_handle(memory, tensor.ptr)\n\n\treturn DescTensor(memory=memory, desc=desc, shape=tensor.shape, tensor=tensor)\n\n\ndef createDescribedNdTensor(tensor, desc=None, fmt=None):\n\tif desc is None:\n\t\tdesc = createMemoryDescriptor(None, tensor, fmt=fmt)\n\n\tmemory = libdnnl.dnnl_memory_create(desc, engine)\n\tlibdnnl.dnnl_memory_set_data_handle(memory, tensor.ptr)\n\n\treturn DescTensor(memory=memory, desc=desc, shape=tensor.shape, tensor=tensor)\n\n\ndef destroyDescribedTensors(*descTensors):\n\tfor descTensor in descTensors:\n\t\tlibdnnl.dnnl_memory_destroy(descTensor.memory)\n\n\ndef executePrimitive(primitive, args):\n\tlibdnnl.dnnl_primitive_execute(primitive, stream, args)\n\tlibdnnl.dnnl_stream_wait(stream)\n\n\ndef prepareConvNdParams(ndim, stride=1, pad=0, dilation=1):\n\tstride = tuple(stride for _ in range(ndim - 2)) if isinstance(stride, int) else stride\n\tpad = tuple(pad for _ in range(ndim - 2)) if isinstance(pad, int) else pad\n\n\tif isinstance(dilation, int):\n\t\tdilation = tuple(dilation - 1 for _ in range(ndim - 2))\n\telse:\n\t\tdilation = tuple(dil - 1 for dil in dilation)\n\n\treturn stride, pad, dilation\n\n\ndef getConvNdOutShape(datashape, Wshape, stride, pad, dilation):\n\tfsize = Wshape[2:]\n\tshape = tuple(\n\t\t(datashape[d + 2] + 2 * pad[d] - (dilation[d] + 1) * (fsize[d] - 1) - 1) // stride[d] + 1\n\t\tfor d in range(len(stride))\n\t)\n\n\treturn (datashape[0], Wshape[0]) + shape\n\n\ndef getConvNdInShape(gradshape, Wshape, stride, pad, dilation):\n\tfsize = Wshape[2:]\n\tshape = tuple(\n\t\tstride[d] * (gradshape[d + 2] - 1) - 2 * pad[d] + (dilation[d] + 1) * (fsize[d] - 1) + 1\n\t\tfor d in range(len(stride))\n\t)\n\n\treturn (gradshape[0], Wshape[1]) + shape\n\n\ndef dilationIsNotTrivial(dilation):\n\treturn any(dil > 0 for dil in dilation)\n\n\ndef convNd(data, W, bias=None, stride=1, pad=0, dilation=1, algo=ConvAlgo.auto, transpose=False):\n\tassert data.ndim == W.ndim\n\tassert data.shape[1] == W.shape[1] if not transpose else data.shape[1] == W.shape[0]\n\n\tdescData = createDescribedNdTensor(data)\n\tdescW = createDescribedNdTensor(CPUArray.swapaxes(W, 0, 1) if transpose else W)\n\n\tdescBias = createDescribedNdTensor(bias.reshape(bias.size)) if bias is not None else None\n\tbiasDesc = None if descBias is None else descBias.desc\n\n\tstride, pad, dilation = prepareConvNdParams(data.ndim, stride, pad, dilation)\n\tdilated = dilationIsNotTrivial(dilation)\n\n\tif transpose:\n\t\tgetOutShape = getConvNdInShape\n\t\tdescInit = libdnnl.dnnl_dilated_deconvolution_forward_desc_init if dilated else \\\n\t\t\tlibdnnl.dnnl_deconvolution_forward_desc_init\n\t\talgo = DeconvAlgo.winograd if algo == ConvAlgo.winograd else DeconvAlgo.direct\n\telse:\n\t\tgetOutShape = getConvNdOutShape\n\t\tdescInit = libdnnl.dnnl_dilated_convolution_forward_desc_init if dilated else \\\n\t\t\tlibdnnl.dnnl_convolution_forward_desc_init\n\n\toutshape = getOutShape(data.shape, W.shape, stride, pad, dilation)\n\tdilation = (dilation, ) if dilated else ()\n\n\tkey = (\n\t\tdata.shape, data.dtype, W.shape, W.dtype, bias.shape if bias is not None else None,\n\t\tstride, pad, *dilation, algo, transpose\n\t)\n\tcache = convPrimitiveCache.get(key, None)\n\n\tif cache is None:\n\t\toutDesc = createMemoryDescriptor(outshape, dtype=data.dtype)\n\t\tconvDesc = descInit(\n\t\t\tPropKind.fwdTrain.value, algo.value, descData.desc, descW.desc, biasDesc, outDesc, stride, *dilation, pad\n\t\t)\n\n\t\tconvDesc = libdnnl.dnnl_primitive_desc_create(convDesc, None, engine, None)\n\t\tconvPrimitive = libdnnl.dnnl_primitive_create(convDesc)\n\n\t\tconvPrimitiveCache[key] = (convDesc, convPrimitive)\n\n\telse:\n\t\tconvDesc, convPrimitive = cache\n\n\toutdata = CPUArray.empty(outshape, dtype=data.dtype)\n\tdescOutData = queryDescribedNdTensor(convDesc, Query.dst, tensor=outdata)\n\n\targs = [\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.src.value, descData.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.weights.value, descW.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.dst.value, descOutData.memory)\n\t]\n\n\tif descBias is not None:\n\t\targs.append(libdnnl.dnnl_exec_arg_t(ArgIndex.bias.value, descBias.memory))\n\n\texecutePrimitive(convPrimitive, args)\n\tdestroyDescribedTensors(descData, descW, descOutData)\n\n\tif descBias is not None:\n\t\tdestroyDescribedTensors(descBias)\n\n\treturn descOutData.tensor\n\n\ndef convNdBackwardData(grad, W, data=None, stride=1, pad=0, dilation=1, algo=ConvAlgo.auto, transpose=False):\n\tassert grad.ndim == W.ndim\n\tassert grad.shape[1] == W.shape[0] if not transpose else grad.shape[1] == W.shape[1]\n\n\tdescGrad = createDescribedNdTensor(grad)\n\tdescW = createDescribedNdTensor(CPUArray.swapaxes(W, 0, 1) if transpose else W)\n\n\tstride, pad, dilation = prepareConvNdParams(grad.ndim, stride, pad, dilation)\n\tdilated = dilationIsNotTrivial(dilation)\n\n\tif transpose:\n\t\tgetInShape = getConvNdOutShape\n\t\tdescInit = libdnnl.dnnl_dilated_deconvolution_backward_data_desc_init if dilated else \\\n\t\t\tlibdnnl.dnnl_deconvolution_backward_data_desc_init\n\t\talgo = DeconvAlgo.winograd if algo == ConvAlgo.winograd else DeconvAlgo.direct\n\telse:\n\t\tgetInShape = getConvNdInShape\n\t\tdescInit = libdnnl.dnnl_dilated_convolution_backward_data_desc_init if dilated else \\\n\t\t\tlibdnnl.dnnl_convolution_backward_data_desc_init\n\n\tinshape = getInShape(grad.shape, W.shape, stride, pad, dilation) if data is None else data.shape\n\tdilation = (dilation, ) if dilated else ()\n\n\tkey = (grad.shape, grad.dtype, W.shape, W.dtype, stride, pad, *dilation, algo, transpose)\n\tcache = convBwdDataPrimitiveCache.get(key, None)\n\n\tif cache is None:\n\t\tinDesc = createMemoryDescriptor(inshape, dtype=grad.dtype)\n\t\tconvDesc = descInit(algo.value, inDesc, descW.desc, descGrad.desc, stride, *dilation, pad)\n\n\t\tconvDesc = libdnnl.dnnl_primitive_desc_create(convDesc, None, engine, None)\n\t\tconvPrimitive = libdnnl.dnnl_primitive_create(convDesc)\n\n\t\tconvBwdDataPrimitiveCache[key] = (convDesc, convPrimitive)\n\n\telse:\n\t\tconvDesc, convPrimitive = cache\n\n\tingrad = CPUArray.empty(inshape, dtype=grad.dtype)\n\tdescInGrad = queryDescribedNdTensor(convDesc, Query.diffDst, tensor=ingrad)\n\n\targs = [\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.diffDst.value, descGrad.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.weights.value, descW.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.diffSrc.value, descInGrad.memory)\n\t]\n\n\texecutePrimitive(convPrimitive, args)\n\tdestroyDescribedTensors(descGrad, descW, descInGrad)\n\n\treturn descInGrad.tensor\n\n\ndef convNdBackwardParams(data, grad, W, bias=None, stride=1, pad=0, dilation=1, wgrad=None, bgrad=None,\n\t\t\t\t\t\t scale=1.0, momentum=0.0, algo=ConvAlgo.auto, transpose=False):\n\tassert data.ndim == grad.ndim\n\tif not transpose:\n\t\tassert grad.shape[1] == W.shape[0] and data.shape[1] == W.shape[1]\n\telse:\n\t\tassert grad.shape[1] == W.shape[1] and data.shape[1] == W.shape[0]\n\n\tdescData = createDescribedNdTensor(data)\n\tdescGrad = createDescribedNdTensor(grad)\n\n\tstride, pad, dilation = prepareConvNdParams(grad.ndim, stride, pad, dilation)\n\tdilated = dilationIsNotTrivial(dilation)\n\n\tif transpose:\n\t\tdescInit = libdnnl.dnnl_dilated_deconvolution_backward_weights_desc_init if dilated else \\\n\t\t\tlibdnnl.dnnl_deconvolution_backward_weights_desc_init\n\t\talgo = DeconvAlgo.winograd if algo == ConvAlgo.winograd else DeconvAlgo.direct\n\telse:\n\t\tdescInit = libdnnl.dnnl_dilated_convolution_backward_weights_desc_init if dilated else \\\n\t\t\tlibdnnl.dnnl_convolution_backward_weights_desc_init\n\n\tif wgrad is not None and scale == 1.0 and momentum == 0.0:\n\t\tdescWGrad = createDescribedNdTensor(CPUArray.swapaxes(wgrad, 0, 1) if transpose else wgrad)\n\telse:\n\t\tWshape = (W.shape[1], W.shape[0]) + W.shape[2:] if transpose else W.shape\n\t\tdescWGrad = createDescribedNdTensor(CPUArray.empty(Wshape, dtype=W.dtype))\n\n\tdescBGrad, bgradDesc = None, None\n\tif bias is not None:\n\t\tif bgrad is not None and scale == 1.0 and momentum == 0.0:\n\t\t\tdescBGrad = createDescribedNdTensor(bgrad.reshape(bgrad.size))\n\t\telse:\n\t\t\tdescBGrad = createDescribedNdTensor(CPUArray.empty((bias.size, ), dtype=bias.dtype))\n\n\t\tbgradDesc = descBGrad.desc\n\n\tdilation = (dilation, ) if dilated else ()\n\n\tkey = (\n\t\tdata.shape, data.dtype, grad.shape, grad.dtype, W.shape, W.dtype, bias.shape if bias is not None else None,\n\t\tstride, pad, *dilation, algo, transpose\n\t)\n\tcache = convBwdParamPrimitiveCache.get(key, None)\n\n\tif cache is None:\n\t\tconvDesc = descInit(algo.value, descData.desc, descWGrad.desc, bgradDesc, descGrad.desc, stride, *dilation, pad)\n\n\t\tconvDesc = libdnnl.dnnl_primitive_desc_create(convDesc, None, engine, None)\n\t\tconvPrimitive = libdnnl.dnnl_primitive_create(convDesc)\n\n\t\tconvBwdParamPrimitiveCache[key] = (convDesc, convPrimitive)\n\n\telse:\n\t\tconvDesc, convPrimitive = cache\n\n\targs = [\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.src.value, descData.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.diffDst.value, descGrad.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.diffWeights.value, descWGrad.memory)\n\t]\n\n\tif bias is not None:\n\t\targs.append(libdnnl.dnnl_exec_arg_t(ArgIndex.diffBias.value, descBGrad.memory))\n\n\texecutePrimitive(convPrimitive, args)\n\tcurrWgrad = CPUArray.swapaxes(descWGrad.tensor, 0, 1) if transpose else descWGrad.tensor\n\n\tif scale != 1.0 or momentum != 0.0:\n\t\tif wgrad is not None:\n\t\t\tNumpyBlas.addVectorToVector(currWgrad.ravel(), wgrad.ravel(), out=wgrad.ravel(), alpha=scale, beta=momentum)\n\n\t\tif bgrad is not None:\n\t\t\tNumpyBlas.addVectorToVector(\n\t\t\t\tdescBGrad.tensor.ravel(), bgrad.ravel(), out=bgrad.ravel(), alpha=scale, beta=momentum\n\t\t\t)\n\n\tdestroyDescribedTensors(descData, descGrad, descWGrad)\n\tif bias is not None:\n\t\tdestroyDescribedTensors(descBGrad)\n\n\treturn (currWgrad, descBGrad.tensor.reshape(bias.shape)) if bias is not None else currWgrad\n\n\ndef convNdbenchmark(datashape, Wshape, stride=1, pad=0, dilation=1, transpose=False):\n\tstartStride, startPad, startDilation = stride, pad, dilation\n\tstride, pad, dilation = prepareConvNdParams(len(Wshape), stride, pad, dilation)\n\n\tif transpose:\n\t\toutshape = getConvNdInShape(datashape, Wshape, stride, pad, dilation)\n\telse:\n\t\toutshape = getConvNdOutShape(datashape, Wshape, stride, pad, dilation)\n\n\tdata, grad = CPUArray.empty(datashape, dtype=np.float32), CPUArray.empty(outshape, dtype=np.float32)\n\tW, bias = CPUArray.empty(Wshape, dtype=np.float32), CPUArray.empty((outshape[1] ), dtype=np.float32)\n\n\tfwdResults, bwdParamResults, bwdDataResults = [], [], []\n\tlooplength = 1\n\n\tfor algo in ConvAlgo:\n\t\tkwargs = {""algo"": algo, ""transpose"": transpose}\n\n\t\ttry:\n\t\t\tsecs = timeKernel(\n\t\t\t\tconvNd, args=(data, W, bias, startStride, startPad, startDilation), kwargs=kwargs,\n\t\t\t\tlooplength=looplength, log=False, normalize=True\n\t\t\t)\n\n\t\texcept libdnnl.dnnlUnimplemented:\n\t\t\tsecs = -1.0\n\n\t\tfwdResults.append(ConvPerf(algo, secs))\n\n\t\ttry:\n\t\t\tsecs = timeKernel(\n\t\t\t\tconvNdBackwardParams, args=(data, grad, W, bias, startStride, startPad, startDilation), kwargs=kwargs,\n\t\t\t\tlooplength=looplength, log=False, normalize=True\n\t\t\t)\n\n\t\texcept libdnnl.dnnlUnimplemented:\n\t\t\tsecs = -1.0\n\n\t\tbwdParamResults.append(ConvPerf(algo, secs))\n\n\t\ttry:\n\t\t\tsecs = timeKernel(\n\t\t\t\tconvNdBackwardData, args=(grad, W, data, startStride, startPad, startDilation), kwargs=kwargs,\n\t\t\t\tlooplength=looplength, log=False, normalize=True\n\t\t\t)\n\n\t\texcept libdnnl.dnnlUnimplemented:\n\t\t\tsecs = -1.0\n\n\t\tbwdDataResults.append(ConvPerf(algo, secs))\n\n\tkey = lambda res: res.time if res.time >= 0.0 else math.inf\n\treturn sorted(fwdResults, key=key), sorted(bwdParamResults, key=key), sorted(bwdDataResults, key=key)\n\n\ndef preparePoolNdParams(ndim, size=2, stride=2, pad=0):\n\tstride = tuple(stride for _ in range(ndim - 2)) if isinstance(stride, int) else stride\n\tsize = tuple(size for _ in range(ndim - 2)) if isinstance(size, int) else size\n\tpad = tuple(pad for _ in range(ndim - 2)) if isinstance(pad, int) else pad\n\n\treturn size, stride, pad\n\n\ndef getPoolNdOutShape(datashape, sizes, stride, pad):\n\tshape = tuple((datashape[d + 2] - sizes[d] + 2 * pad[d]) // stride[d] + 1 for d in range(len(datashape) - 2))\n\treturn datashape[:2] + shape\n\n\ndef poolNd(data, size=2, stride=2, pad=0, mode=PoolMode.max, test=False):\n\tdescData = createDescribedNdTensor(data)\n\n\tsize, stride, pad = preparePoolNdParams(data.ndim, size, stride, pad)\n\toutshape = getPoolNdOutShape(data.shape, size, stride, pad)\n\n\tdescOutData = createDescribedNdTensor(CPUArray.empty(outshape, dtype=data.dtype))\n\n\tkey = (data.shape, data.dtype, size, stride, pad, mode, test)\n\tcache = poolPrimitiveCache.get(key, None)\n\n\tif cache is None:\n\t\tprop = PropKind.fwdInfer if test else PropKind.fwdTrain\n\t\tpoolDesc = libdnnl.dnnl_pooling_forward_desc_init(\n\t\t\tprop.value, mode.value, descData.desc, descOutData.desc, stride, size, pad\n\t\t)\n\n\t\tpoolDesc = libdnnl.dnnl_primitive_desc_create(poolDesc, None, engine, None)\n\t\tpoolPrimitive = libdnnl.dnnl_primitive_create(poolDesc)\n\n\t\tpoolPrimitiveCache[key] = (poolDesc, poolPrimitive)\n\n\telse:\n\t\tpoolDesc, poolPrimitive = cache\n\n\tworkspaceDesc, descWorkspace = None, None\n\tif not test:\n\t\tworkspaceDesc = libdnnl.dnnl_primitive_desc_query_md(poolDesc, Query.workspace.value, 0)\n\n\t\tif workspaceDesc is not None:\n\t\t\tdescWorkspace = createDescribedNdTensor(CPUArray.empty(\n\t\t\t\tlibdnnl.dnnl_memory_desc_get_size(workspaceDesc), dtype=np.int8), desc=workspaceDesc\n\t\t\t)\n\n\targs = [\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.src.value, descData.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.dst.value, descOutData.memory)\n\t]\n\n\tif descWorkspace is not None:\n\t\targs.append(libdnnl.dnnl_exec_arg_t(ArgIndex.workspace.value, descWorkspace.memory))\n\n\texecutePrimitive(poolPrimitive, args)\n\tdestroyDescribedTensors(descData, descOutData)\n\n\tworkspace = None\n\tif workspaceDesc is not None:\n\t\tdestroyDescribedTensors(descWorkspace)\n\t\tworkspace = descWorkspace.tensor\n\n\treturn descOutData.tensor if test else (descOutData.tensor, workspace, poolDesc)\n\n\ndef poolNdBackward(indata, grad, workspace, desc, size=2, stride=2, pad=0, mode=PoolMode.max):\n\tsize, stride, pad = preparePoolNdParams(grad.ndim, size, stride, pad)\n\n\tdescGrad = createDescribedNdTensor(grad)\n\tdescInGrad = createDescribedNdTensor(CPUArray.empty(indata.shape, dtype=indata.dtype))\n\n\tkey = (indata.shape, indata.dtype, grad.shape, grad.dtype, size, stride, pad, mode)\n\tcache = poolBwdPrimitiveCache.get(key, None)\n\n\tif cache is None:\n\t\tpoolDesc = libdnnl.dnnl_pooling_backward_desc_init(\n\t\t\tmode.value, descInGrad.desc, descGrad.desc, stride, size, pad\n\t\t)\n\n\t\tpoolDesc = libdnnl.dnnl_primitive_desc_create(poolDesc, None, engine, desc)\n\t\tpoolPrimitive = libdnnl.dnnl_primitive_create(poolDesc)\n\n\t\tpoolBwdPrimitiveCache[key] = (poolDesc, poolPrimitive)\n\n\telse:\n\t\tpoolDesc, poolPrimitive = cache\n\n\tdescWorkspace = None\n\tif workspace is not None:\n\t\tworkspaceDesc = libdnnl.dnnl_primitive_desc_query_md(poolDesc, Query.workspace.value, 0)\n\t\tdescWorkspace = createDescribedNdTensor(workspace, desc=workspaceDesc)\n\n\targs = [\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.diffDst.value, descGrad.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.diffSrc.value, descInGrad.memory)\n\t]\n\n\tif descWorkspace is not None:\n\t\targs.append(libdnnl.dnnl_exec_arg_t(ArgIndex.workspace.value, descWorkspace.memory))\n\n\texecutePrimitive(poolPrimitive, args)\n\tdestroyDescribedTensors(descGrad, descInGrad)\n\n\tif workspace is not None:\n\t\tdestroyDescribedTensors(descWorkspace)\n\n\treturn descInGrad.tensor\n\n\ndef softmaxNd(data):\n\tdescData = createDescribedNdTensor(data)\n\tdescOutData = createDescribedNdTensor(CPUArray.empty(data.shape, dtype=data.dtype))\n\n\tsoftmaxDesc = libdnnl.dnnl_softmax_forward_desc_init(PropKind.fwdInfer.value, descData.desc, 1)\n\tsoftmaxDesc = libdnnl.dnnl_primitive_desc_create(softmaxDesc, None, engine, None)\n\n\tdescSoftmax = libdnnl.dnnl_primitive_create(softmaxDesc)\n\tlibdnnl.dnnl_primitive_desc_destroy(softmaxDesc)\n\n\targs = [\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.src.value, descData.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.dst.value, descOutData.memory)\n\t]\n\n\texecutePrimitive(descSoftmax, args)\n\tlibdnnl.dnnl_primitive_destroy(descSoftmax)\n\n\tdestroyDescribedTensors(descData, descOutData)\n\treturn descOutData.tensor\n\n\ndef softmaxNdBackward(outdata, grad):\n\tdescOutData = createDescribedNdTensor(outdata)\n\n\tdescGrad = createDescribedNdTensor(grad)\n\tdescInGrad = createDescribedNdTensor(CPUArray.empty(grad.shape, dtype=grad.dtype))\n\n\tsoftmaxDesc = libdnnl.dnnl_softmax_backward_desc_init(descGrad.desc, descOutData.desc, 1)\n\tsoftmaxDesc = libdnnl.dnnl_primitive_desc_create(softmaxDesc, None, engine, None)\n\n\tdescSoftmax = libdnnl.dnnl_primitive_create(softmaxDesc)\n\tlibdnnl.dnnl_primitive_desc_destroy(softmaxDesc)\n\n\targs = [\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.dst.value, descOutData.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.diffDst.value, descGrad.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.diffSrc.value, descInGrad.memory)\n\t]\n\n\texecutePrimitive(descSoftmax, args)\n\tlibdnnl.dnnl_primitive_destroy(descSoftmax)\n\n\tdestroyDescribedTensors(descOutData, descGrad, descInGrad)\n\treturn descInGrad.tensor\n\n\ndef lrn(data, mode=LRNMode.map, N=5, alpha=1e-4, beta=0.75, K=2.0, test=False):\n\tdescData = createDescribedNdTensor(data)\n\tdescOutData = createDescribedNdTensor(CPUArray.empty(data.shape, dtype=data.dtype))\n\n\tkey = (data.shape, data.dtype, mode, N, alpha, beta, K, test)\n\tcache = lrnPrimitiveCache.get(key, None)\n\n\tif cache is None:\n\t\tprop = PropKind.fwdInfer if test else PropKind.fwdTrain\n\t\tlrnDesc = libdnnl.dnnl_lrn_forward_desc_init(prop.value, mode.value, descData.desc, N, alpha, beta, K)\n\n\t\tlrnDesc = libdnnl.dnnl_primitive_desc_create(lrnDesc, None, engine, None)\n\t\tlrnPrimitive = libdnnl.dnnl_primitive_create(lrnDesc)\n\n\t\tlrnPrimitiveCache[key] = (lrnDesc, lrnPrimitive)\n\n\telse:\n\t\tlrnDesc, lrnPrimitive = cache\n\n\tdescWorkspace = None\n\n\tif not test:\n\t\tworkspaceDesc = libdnnl.dnnl_primitive_desc_query_md(lrnDesc, Query.workspace.value, 0)\n\t\tdescWorkspace = createDescribedNdTensor(CPUArray.empty(\n\t\t\tlibdnnl.dnnl_memory_desc_get_size(workspaceDesc), dtype=np.int8), desc=workspaceDesc\n\t\t)\n\n\targs = [\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.src.value, descData.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.dst.value, descOutData.memory)\n\t]\n\n\tif descWorkspace is not None:\n\t\targs.append(libdnnl.dnnl_exec_arg_t(ArgIndex.workspace.value, descWorkspace.memory))\n\n\texecutePrimitive(lrnPrimitive, args)\n\tdestroyDescribedTensors(descData, descOutData)\n\n\treturn descOutData.tensor if test else (descOutData.tensor, descWorkspace, lrnDesc)\n\n\ndef lrnBackward(data, grad, descWorkspace, desc, mode=LRNMode.map, N=5, alpha=1e-4, beta=0.75, K=2.0):\n\tassert mode != LRNMode.map\n\n\tdescGrad = createDescribedNdTensor(grad)\n\tdescInGrad = createDescribedNdTensor(CPUArray.empty(grad.shape, dtype=grad.dtype))\n\tdescData = createDescribedNdTensor(data)\n\n\tkey = (data.shape, data.dtype, grad.shape, grad.dtype, mode, N, alpha, beta, K)\n\tcache = lrnBwdPrimitiveCache.get(key, None)\n\n\tif cache is None:\n\t\tlrnDesc = libdnnl.dnnl_lrn_backward_desc_init(mode.value, descData.desc, descGrad.desc, N, alpha, beta, K)\n\n\t\tlrnDesc = libdnnl.dnnl_primitive_desc_create(lrnDesc, None, engine, desc)\n\t\tlrnPrimitive = libdnnl.dnnl_primitive_create(lrnDesc)\n\n\t\tlrnBwdPrimitiveCache[key] = (lrnDesc, lrnPrimitive)\n\n\telse:\n\t\tlrnDesc, lrnPrimitive = cache\n\n\targs = [\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.src.value, descData.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.diffDst.value, descGrad.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.workspace.value, descWorkspace.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.diffSrc.value, descInGrad.memory)\n\t]\n\n\texecutePrimitive(lrnPrimitive, args)\n\tdestroyDescribedTensors(descGrad, descInGrad, descData, descWorkspace)\n\n\treturn descInGrad.tensor\n\n\ndef batchNormNd(data, scale, bias, mean, var, epsilon=1e-5, test=False, out=None):\n\tassert data.ndim == scale.ndim and scale.ndim == bias.ndim and bias.ndim == mean.ndim and mean.ndim == var.ndim\n\n\tdescData = createDescribedNdTensor(data)\n\tdescOutData = createDescribedNdTensor(CPUArray.empty(data.shape, dtype=data.dtype) if out is None else out)\n\n\tdescWeights = createDescribedNdTensor(CPUArray.toDevice(np.concatenate(\n\t\t(scale.data.reshape(scale.size), bias.data.reshape(bias.size))\n\t)))\n\n\tdescMean = createDescribedNdTensor(mean.reshape(mean.size))\n\tdescVar = createDescribedNdTensor(var.reshape(var.size))\n\n\tkey = (data.shape, data.dtype, epsilon, test)\n\tcache = bnPrimitiveCache.get(key, None)\n\n\tif cache is None:\n\t\tflags = BatchNormFlags.useGlobalStats.value if test else 0\n\t\tprop = PropKind.fwdInfer if test else PropKind.fwdTrain\n\n\t\tbnDesc = libdnnl.dnnl_batch_normalization_forward_desc_init(\n\t\t\tprop.value, descData.desc, epsilon, BatchNormFlags.scaleShift.value | flags\n\t\t)\n\n\t\tbnDesc = libdnnl.dnnl_primitive_desc_create(bnDesc, None, engine, None)\n\t\tbnPrimitive = libdnnl.dnnl_primitive_create(bnDesc)\n\n\t\tbnPrimitiveCache[key] = (bnDesc, bnPrimitive)\n\n\telse:\n\t\tbnDesc, bnPrimitive = cache\n\n\targs = [\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.src.value, descData.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.weights.value, descWeights.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.dst.value, descOutData.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.mean.value, descMean.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.variance.value, descVar.memory)\n\t]\n\n\texecutePrimitive(bnPrimitive, args)\n\tdestroyDescribedTensors(descData, descOutData, descWeights, descMean, descVar)\n\n\tbnDesc = None if test else bnDesc\n\treturn descOutData.tensor, descMean.tensor.reshape(scale.shape), descVar.tensor.reshape(scale.shape), bnDesc\n\n\ndef batchNormNdBackward(data, grad, scale, bias, savemean, savevar, desc, epsilon=1e-5):\n\tassert scale.ndim == savemean.ndim and savemean.ndim == savevar.ndim\n\n\tdescData = createDescribedNdTensor(data)\n\tdescGrad = createDescribedNdTensor(grad)\n\tdescScale = createDescribedNdTensor(scale.reshape(scale.size))\n\n\tdescWeights = createDescribedNdTensor(CPUArray.toDevice(np.concatenate(\n\t\t(scale.data.reshape(scale.size), bias.data.reshape(bias.size))\n\t)))\n\n\tdescMean = createDescribedNdTensor(savemean.reshape(savemean.size))\n\tdescVar = createDescribedNdTensor(savevar.reshape(savevar.size))\n\n\tdescInGrad = createDescribedNdTensor(CPUArray.empty(grad.shape, dtype=grad.dtype))\n\tdescWGrad = createDescribedNdTensor(CPUArray.empty((2 * scale.size, ), dtype=scale.dtype))\n\n\tkey = (data.shape, data.dtype, grad.shape, grad.dtype, epsilon)\n\tcache = bnBwdPrimitiveCache.get(key, None)\n\n\tif cache is None:\n\t\tbnDesc = libdnnl.dnnl_batch_normalization_backward_desc_init(\n\t\t\tPropKind.backward.value, descGrad.desc, descData.desc, epsilon, BatchNormFlags.scaleShift.value\n\t\t)\n\n\t\tbnDesc = libdnnl.dnnl_primitive_desc_create(bnDesc, None, engine, desc)\n\t\tbnPrimitive = libdnnl.dnnl_primitive_create(bnDesc)\n\n\t\tbnBwdPrimitiveCache[key] = (bnDesc, bnPrimitive)\n\n\telse:\n\t\tbnDesc, bnPrimitive = cache\n\n\targs = [\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.src.value, descData.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.mean.value, descMean.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.variance.value, descVar.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.diffDst.value, descGrad.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.weights.value, descWeights.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.diffSrc.value, descInGrad.memory),\n\t\tlibdnnl.dnnl_exec_arg_t(ArgIndex.diffWeights.value, descWGrad.memory)\n\t]\n\n\texecutePrimitive(bnPrimitive, args)\n\tdestroyDescribedTensors(descData, descGrad, descScale, descWeights, descMean, descVar, descInGrad, descWGrad)\n\n\tscalegrad = descWGrad.tensor[:scale.size].reshape(*scale.shape)\n\tbiasgrad = descWGrad.tensor[scale.size:].reshape(*scale.shape)\n\n\treturn descInGrad.tensor, scalegrad, biasgrad\n\n\ndef unittest():\n\tconv2dTest()\n\tdeconv2dTest()\n\tmaxpool2dTest()\n\tsoftmaxTest()\n\tmapLRNTest()\n\tcrossMapLRNTest()\n\tbatchNorm2dTest()\n\tbatchNormTest()\n\n\ndef conv2dTest():\n\tbatchsize, inmaps, h, w = 1, 2, 6, 6\n\tfsize, outmaps = 2, 4\n\n\tdata = CPUArray.toDevice(np.random.randn(batchsize, inmaps, h, w).astype(np.float32))\n\n\tW = CPUArray.toDevice(np.random.randn(outmaps, inmaps, fsize, fsize).astype(np.float32))\n\tbias = CPUArray.toDevice(np.random.randn(1, outmaps, 1, 1).astype(np.float32))\n\n\toutdata = convNd(data, W, bias)\n\n\thostData, hostW, hostBias = data.get(), W.get(), bias.get()\n\thostOutData = np.empty(outdata.shape, dtype=np.float32)\n\n\tfor c in range(outmaps):\n\t\thostOutData[:, c, :, :] = hostBias[0, c, 0, 0]\n\n\tfor b in range(batchsize):\n\t\tfor oc in range(outmaps):\n\t\t\tfor ic in range(inmaps):\n\t\t\t\tfor y in range(outdata.shape[2]):\n\t\t\t\t\tfor x in range(outdata.shape[3]):\n\t\t\t\t\t\tfor dy in range(fsize):\n\t\t\t\t\t\t\tfor dx in range(fsize):\n\t\t\t\t\t\t\t\thostOutData[b, oc, y, x] += hostData[b, ic, y + dy, x + dx] * hostW[oc, ic, dy, dx]\n\n\tassert np.allclose(hostOutData, outdata.get())\n\n\tgrad = CPUArray.toDevice(np.random.randn(*outdata.shape).astype(np.float32))\n\tingrad = convNdBackwardData(grad, W)\n\n\thostInGrad, hostGrad = np.zeros(data.shape).astype(np.float32), grad.get()\n\n\tfor b in range(batchsize):\n\t\tfor ic in range(inmaps):\n\t\t\tfor oc in range(outmaps):\n\t\t\t\tfor y in range(hostGrad.shape[2]):\n\t\t\t\t\tfor x in range(hostGrad.shape[3]):\n\t\t\t\t\t\tfor dy in range(fsize):\n\t\t\t\t\t\t\tfor dx in range(fsize):\n\t\t\t\t\t\t\t\thostInGrad[b, ic, y + dy, x + dx] += hostW[oc, ic, dy, dx] * hostGrad[b, oc, y, x]\n\n\tassert np.allclose(hostInGrad, ingrad.get())\n\n\twgrad, bgrad = convNdBackwardParams(data, grad, W, bias)\n\thostWGrad = np.zeros(wgrad.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor oc in range(outmaps):\n\t\t\tfor ic in range(inmaps):\n\t\t\t\tfor dy in range(fsize):\n\t\t\t\t\tfor dx in range(fsize):\n\t\t\t\t\t\tfor y in range(hostGrad.shape[2]):\n\t\t\t\t\t\t\tfor x in range(hostGrad.shape[3]):\n\t\t\t\t\t\t\t\thostWGrad[oc, ic, dy, dx] += hostData[b,ic,y + dy, x + dx] * hostGrad[b, oc, y, x]\n\n\tassert np.allclose(hostWGrad, wgrad.get())\n\n\thostBGrad = np.empty(hostBias.shape, dtype=np.float32)\n\tfor oc in range(outmaps):\n\t\thostBGrad[0, oc, 0, 0] = np.sum(hostGrad[:, oc, :, :])\n\n\tassert np.allclose(hostBGrad, bgrad.get())\n\n\ndef deconv2dTest():\n\tbatchsize, inmaps, h, w = 1, 1, 2, 2\n\tfsize, stride, outmaps = 3, 2, 1\n\n\tdata = CPUArray.toDevice(np.random.randn(batchsize, inmaps, h, w).astype(np.float32))\n\n\tW = CPUArray.toDevice(np.random.randn(inmaps, outmaps, fsize, fsize).astype(np.float32))\n\tbias = CPUArray.toDevice(np.random.randn(1, outmaps, 1, 1).astype(np.float32))\n\n\toutdata = convNd(data, W, bias, stride=stride, transpose=True)\n\n\thostOutData = np.zeros(outdata.shape).astype(np.float32)\n\tfor i in range(0, hostOutData.shape[2] - fsize + 1, stride):\n\t\tfor j in range(0, hostOutData.shape[3] - fsize + 1, stride):\n\t\t\thostOutData[0, 0, i:fsize+i, j:fsize+j] += W.get()[0, 0] * data.get()[0, 0, i // stride, j // stride]\n\n\thostOutData += bias.get()\n\tassert np.allclose(hostOutData, outdata.get())\n\n\tgrad = CPUArray.toDevice(np.random.randn(*outdata.shape).astype(np.float32))\n\n\tingrad = convNdBackwardData(grad, W, stride=stride, transpose=True)\n\n\thostInGrad = np.zeros(data.shape, dtype=np.float32)\n\tfor i in range(0, hostInGrad.shape[2]):\n\t\tfor j in range(0, hostInGrad.shape[3]):\n\t\t\ty, x = i * stride, j * stride\n\t\t\thostInGrad[0, 0, i, j] += np.dot(W.get()[0, 0].ravel(), grad.get()[0, 0, y:y+fsize, x:x+fsize].ravel())\n\n\tassert np.allclose(hostInGrad, ingrad.get())\n\n\twgrad, bgrad = convNdBackwardParams(data, grad, W, bias, stride=stride, transpose=True)\n\n\thostWGrad = np.zeros(wgrad.shape, dtype=np.float32)\n\tfor i in range(0, hostOutData.shape[2] - fsize + 1, stride):\n\t\tfor j in range(0, hostOutData.shape[3] - fsize + 1, stride):\n\t\t\thostWGrad[0, 0] += grad.get()[0, 0, i:i+fsize, j:j+fsize] * data.get()[0, 0, i // stride, j // stride]\n\n\tassert np.allclose(hostWGrad, wgrad.get())\n\n\thostBGrad = np.sum(grad.get())\n\tassert np.allclose(hostBGrad, bgrad.get())\n\n\ndef maxpool2dTest():\n\tbatchsize, maps, h, w = 1, 1, 8, 8\n\tdata = CPUArray.toDevice(np.random.randn(batchsize, maps, h, w).astype(np.float32))\n\n\toutdata, workspace, desc = poolNd(data)\n\n\tdef maxDownSample2d(dat, factor):\n\t\ttrimrows = dat.shape[0] // factor * factor\n\t\ttrimcols = dat.shape[1] // factor * factor\n\n\t\tmaxSoFar = None\n\t\tfirst = True\n\n\t\tfor coff in range(factor):\n\t\t\tfor roff in range(factor):\n\t\t\t\thopped = dat[roff:trimrows:factor, coff:trimcols:factor]\n\t\t\t\tif first:\n\t\t\t\t\tmaxSoFar = hopped\n\t\t\t\t\tfirst = False\n\t\t\t\telse:\n\t\t\t\t\tmaxSoFar = np.maximum(maxSoFar, hopped)\n\n\t\treturn maxSoFar\n\n\thostOutData = maxDownSample2d(data.get()[0, 0], 2)\n\tassert np.allclose(hostOutData, outdata.get())\n\n\tgrad = CPUArray.toDevice(np.random.randn(*outdata.shape).astype(np.float32))\n\tpoolNdBackward(data, grad, workspace, desc)\n\n\ndef softmaxTest():\n\tbatchsize, maps = 5, 8\n\tdata = CPUArray.toDevice(np.random.randn(batchsize, maps, 1, 1).astype(np.float32))\n\n\toutdata = softmaxNd(data)\n\n\tdef hostSoftmax(w):\n\t\te = np.exp(w - np.amax(w))\n\t\tp = e / np.sum(e)\n\t\treturn p\n\n\thostData = data.get().reshape(batchsize, maps)\n\thostOutData = np.vstack([hostSoftmax(hostData[i]) for i in range(batchsize)])\n\tassert np.allclose(hostOutData, outdata.get().reshape(batchsize, maps))\n\n\tgrad = CPUArray.toDevice(np.random.randn(batchsize, maps, 1, 1).astype(np.float32))\n\tingrad = softmaxNdBackward(outdata, grad)\n\n\tdef hostSoftmaxBackward(outdat, gr):\n\t\tingr = np.zeros(outdat.shape, dtype=np.float32)\n\t\tfor i in range(ingr.shape[0]):\n\t\t\tingr[i] += outdat[i] * gr[i]\n\n\t\t\tfor j in range(outdat.shape[0]):\n\t\t\t\tingr[i] -= outdat[i] * outdat[j] * gr[j]\n\t\treturn ingr\n\n\thostGrad = grad.get().reshape(batchsize, maps)\n\thostInGrad = np.vstack([hostSoftmaxBackward(hostOutData[i], hostGrad[i]) for i in range(batchsize)])\n\tassert np.allclose(hostInGrad, ingrad.get().reshape(batchsize, maps))\n\n\ndef mapLRNTest():\n\th, w = 10, 10\n\tN, alpha, beta, K = 5, 1.0, 0.5, 2.0\n\n\tlookBehind = (N - 1) // 2\n\tlookAhead = N - lookBehind\n\n\tdata = CPUArray.toDevice(np.random.randn(1, 1, h, w).astype(np.float32))\n\toutdata, workspace, desc = lrn(data, mode=LRNMode.map, N=N, alpha=alpha, beta=beta, K=K)\n\n\thostData = data.get().reshape(h, w).astype(np.float32)\n\tnorms = np.empty((h, w), dtype=np.float32)\n\tfor i in range(h):\n\t\tfor j in range(w):\n\t\t\tnorm = 0.0\n\t\t\tfor m in range(max(0, i - lookBehind), min(h, i + lookAhead)):\n\t\t\t\tfor n in range(max(0, j - lookBehind), min(w, j + lookAhead)):\n\t\t\t\t\tnorm += hostData[m, n]**2\n\t\t\tnorms[i, j] = K + norm * alpha / N / N\n\n\thostOutData = hostData / norms**beta\n\tassert np.allclose(hostOutData, outdata.reshape(h, w).get())\n\n\ndef crossMapLRNTest():\n\tmaps = 10\n\tN, alpha, beta, K = 5, 1.0, 0.5, 2.0\n\n\tlookBehind = (N - 1) // 2\n\tlookAhead = N - lookBehind\n\n\tdata = CPUArray.toDevice(np.random.randn(1, maps, 1, 1).astype(np.float32))\n\toutdata, workspace, desc = lrn(data, mode=LRNMode.cross, N=N, alpha=alpha, beta=beta, K=K)\n\n\thostData = data.get().reshape(maps, ).astype(np.float32)\n\tnorms = np.empty((maps, ), dtype=np.float32)\n\tfor i in range(maps):\n\t\tnorm = 0.0\n\t\tfor j in range(max(0, i - lookBehind), min(maps, i + lookAhead)):\n\t\t\tnorm += hostData[j]**2\n\t\tnorms[i] = K + norm * alpha / N\n\n\thostOutData = hostData / norms**beta\n\tassert np.allclose(hostOutData, outdata.reshape(maps, ).get())\n\n\tgrad = CPUArray.toDevice(np.random.randn(1, maps, 1, 1).astype(np.float32))\n\tingrad = lrnBackward(data, grad, workspace, desc, mode=LRNMode.cross, N=N, alpha=alpha, beta=beta, K=K)\n\n\thostGrad = grad.get().reshape(maps, ).astype(np.float32)\n\thostInGrad = np.zeros((maps, ), dtype=np.float32)\n\tk = 2.0 * alpha * beta / N\n\tfor i in range(maps):\n\t\thostInGrad[i] += hostGrad[i] / norms[i]**beta\n\n\t\tfor j in range(max(0, i - lookBehind), min(maps, i + lookAhead)):\n\t\t\thostInGrad[j] -= hostGrad[i] * k * hostData[i] * hostData[j] / norms[i]**(beta+1)\n\n\tassert np.allclose(hostInGrad, ingrad.reshape(maps, ).get())\n\n\ndef batchNorm2dTest():\n\tbatchsize, maps, h, w = 4, 5, 3, 2\n\tdata = CPUArray.toDevice(np.random.randn(batchsize, maps, h, w).astype(np.float32))\n\thostData = data.get()\n\n\tscale = CPUArray.toDevice(np.random.randn(1, maps, 1, 1).astype(np.float32))\n\tbias = CPUArray.toDevice(np.random.randn(1, maps, 1, 1).astype(np.float32))\n\tmean = CPUArray.toDevice(np.zeros((1, maps, 1, 1), dtype=np.float32))\n\tvar = CPUArray.toDevice(np.ones((1, maps, 1, 1), dtype=np.float32))\n\n\toutdata, savemean, savevar, desc = batchNormNd(data, scale, bias, mean, var, out=data)\n\n\thostScale, hostBias = scale.get(), bias.get()\n\thostNormData = np.empty(hostData.shape, dtype=np.float32)\n\thostOutData = np.empty(hostData.shape, dtype=np.float32)\n\thostMean = np.zeros(scale.shape, dtype=np.float32)\n\thostVar = np.zeros(scale.shape, dtype=np.float32)\n\thostInvVar = np.empty(scale.shape, dtype=np.float32)\n\tfor c in range(maps):\n\t\tfor b in range(batchsize):\n\t\t\thostMean[0, c, 0, 0] += np.sum(hostData[b, c])\n\t\thostMean[0, c, 0, 0] /= (batchsize * w * h)\n\n\t\tfor b in range(batchsize):\n\t\t\thostVar[0, c, 0, 0] += np.sum((hostData[b, c] - hostMean[0, c, 0, 0])**2)\n\t\thostVar[0, c, 0, 0] /= (batchsize * w * h)\n\n\t\thostInvVar[0, c, 0, 0] = 1.0 / np.sqrt(hostVar[0, c, 0, 0] + 1e-5)\n\t\thostNormData[:, c, :, :] = (hostData[:, c, :, :] - hostMean[0, c, 0, 0]) * hostInvVar[0, c, 0, 0]\n\t\thostOutData[:, c, :, :] = hostNormData[:, c, :, :] * hostScale[0, c, 0, 0] + hostBias[0, c, 0, 0]\n\n\tassert np.allclose(hostMean, mean.get())\n\tassert np.allclose(hostVar, savevar.get())\n\tassert np.allclose(hostOutData, outdata.get())\n\n\tgrad = CPUArray.toDevice(np.random.randn(batchsize, maps, h, w).astype(np.float32))\n\n\tdata = CPUArray.toDevice(hostData)\n\tingrad, scalegrad, biasgrad = batchNormNdBackward(data, grad, scale, bias, savemean, savevar, desc)\n\n\thostGrad = grad.get()\n\thostInGrad, hostScaleGrad = np.empty(grad.shape, dtype=np.float32), np.empty(scale.shape, dtype=np.float32)\n\thostBiasGrad, hostMeanGrad = np.empty(bias.shape, dtype=np.float32), np.empty(hostMean.shape, dtype=np.float32)\n\thostVarGrad = np.empty(hostVar.shape, dtype=np.float32)\n\tfor c in range(maps):\n\t\thostBiasGrad[0, c, 0, 0] = np.sum(hostGrad[:, c, :, :])\n\t\thostScaleGrad[0, c, 0, 0] = np.sum(hostGrad[:, c, :, :] * hostNormData[:, c, :, :])\n\n\t\thostMeanGrad[0, c, 0, 0] = np.sum(hostGrad[:, c, :, :]) * hostScale[0, c, 0, 0] * -hostInvVar[0, c, 0, 0]\n\t\thostVarGrad[0, c, 0, 0] = np.sum(hostGrad[:, c, :, :] * (hostData[:, c, :, :] - hostMean[0, c, 0, 0])) * \\\n\t\t\t\t\t\t\t\t  hostScale[0, c, 0, 0] * -0.5 * hostInvVar[0, c, 0, 0]**3\n\n\t\thostInGrad[:, c, :, :] = hostGrad[:, c, :, :] * hostScale[0, c, 0, 0] * hostInvVar[0, c, 0, 0] + \\\n\t\t\t\t\t\t\t\t hostVarGrad[0,c,0,0] * 2/(batchsize*w*h) * (hostData[:,c,:,:] - hostMean[0,c,0,0]) + \\\n\t\t\t\t\t\t\t\t hostMeanGrad[0, c, 0, 0] / (batchsize * w * h)\n\tassert np.allclose(hostInGrad, ingrad.get())\n\tassert np.allclose(hostScaleGrad, scalegrad.get())\n\tassert np.allclose(hostBiasGrad, biasgrad.get())\n\n\tbatchNormNd(data, scale, bias, mean, var, test=True)\n\n\ndef batchNormTest():\n\tbatchsize, size = 4, 5\n\n\tdata = CPUArray.toDevice(np.random.randn(batchsize, size, 1, 1).astype(np.float32))\n\thostData = data.get().squeeze()\n\n\tscale = CPUArray.toDevice(np.random.randn(1, size, 1, 1).astype(np.float32))\n\tbias = CPUArray.toDevice(np.random.randn(1, size, 1, 1).astype(np.float32))\n\tmean = CPUArray.zeros((1, size, 1, 1), dtype=np.float32)\n\tvar = CPUArray.toDevice(np.ones((1, size, 1, 1), dtype=np.float32))\n\n\toutdata, savemean, savevar, desc = batchNormNd(data, scale, bias, mean, var, out=data)\n\n\thostMean = np.mean(hostData, axis=0, keepdims=False)\n\thostVar = np.sum((hostData - hostMean[np.newaxis, :])**2, axis=0) / batchsize\n\thostInvVar = 1.0 / np.sqrt(hostVar + 1e-5)\n\n\thostNormData = (hostData - hostMean) * hostInvVar\n\thostScale = scale.get().squeeze()\n\thostBias = bias.get().squeeze()\n\thostOutData = hostNormData * hostScale + hostBias\n\n\tassert np.allclose(hostMean, savemean.get().squeeze())\n\tassert np.allclose(hostVar, savevar.get().squeeze())\n\tassert np.allclose(hostOutData, outdata.get().squeeze())\n\n\tgrad = CPUArray.toDevice(np.random.randn(batchsize, size, 1, 1).astype(np.float32))\n\n\tdata = CPUArray.toDevice(hostData).reshape(batchsize, size, 1, 1)\n\tingrad, scalegrad, biasgrad = batchNormNdBackward(data, grad, scale, bias, savemean, savevar, desc)\n\n\thostGrad = grad.get().squeeze()\n\n\thostBiasGrad = np.sum(hostGrad, axis=0)\n\thostScaleGrad = np.sum(hostGrad * hostNormData, axis=0)\n\thostMeanGrad = np.sum(hostGrad, axis=0) * hostScale * -hostInvVar\n\thostVarGrad = np.sum(hostGrad * (hostData - hostMean[np.newaxis, :]), axis=0) * \\\n\t\t\t\t  hostScale[np.newaxis, :] * -0.5 * hostInvVar[np.newaxis, :]**3\n\n\thostInGrad = hostGrad * hostScale[np.newaxis, :] * hostInvVar[np.newaxis, :] + \\\n\t\t\t\t hostVarGrad * 2 / batchsize * (hostData - hostMean) + hostMeanGrad / batchsize\n\n\tassert np.allclose(hostBiasGrad, biasgrad.get().squeeze())\n\tassert np.allclose(hostScaleGrad, scalegrad.get().squeeze())\n\tassert np.allclose(hostInGrad, ingrad.get().squeeze())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Intel/Wrappers/DNNL3D.py,49,"b'import numpy as np\n\nfrom PuzzleLib.CPU.CPUArray import CPUArray\nfrom PuzzleLib.Intel.Wrappers.DNNL import convNd, convNdBackwardData, convNdBackwardParams, \\\n\tPoolMode, poolNd, poolNdBackward, batchNormNd, batchNormNdBackward\n\n\ndef unittest():\n\tconv3dTest()\n\tmaxpool3dTest()\n\tbatchNorm3dTest()\n\n\ndef conv3dTest():\n\tbatchsize, inmaps, d, h, w = 1, 2, 3, 3, 3\n\toutmaps, fsize = 3, 2\n\n\tdata = CPUArray.toDevice(np.random.randn(batchsize, inmaps, d, h, w).astype(np.float32))\n\n\tW = CPUArray.toDevice(np.random.randn(outmaps, inmaps, fsize, fsize, fsize).astype(np.float32))\n\tbias = CPUArray.toDevice(np.random.randn(1, outmaps, 1, 1, 1).astype(np.float32))\n\n\toutdata = convNd(data, W, bias)\n\n\thostData = data.get()\n\thostOutData = np.empty(outdata.shape, dtype=np.float32)\n\n\thostW, hostBias = W.get(), bias.get()\n\n\tfor c in range(outmaps):\n\t\thostOutData[:, c, :, :, :] = hostBias[0, c, 0, 0, 0]\n\n\tfor b in range(batchsize):\n\t\tfor oc in range(outmaps):\n\t\t\tfor ic in range(inmaps):\n\t\t\t\tfor z in range(outdata.shape[2]):\n\t\t\t\t\tfor y in range(outdata.shape[3]):\n\t\t\t\t\t\tfor x in range(outdata.shape[4]):\n\t\t\t\t\t\t\tfor dz in range(fsize):\n\t\t\t\t\t\t\t\tfor dy in range(fsize):\n\t\t\t\t\t\t\t\t\tfor dx in range(fsize):\n\t\t\t\t\t\t\t\t\t\thostOutData[b, oc, z, y, x] += hostData[b, ic, z + dz, y + dy, x + dx] * \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   hostW[oc, ic, dz, dy, dx]\n\n\tassert np.allclose(hostOutData, outdata.get())\n\n\tgrad = CPUArray.toDevice(np.random.randn(*outdata.shape).astype(np.float32))\n\tingrad = convNdBackwardData(grad, W)\n\n\thostGrad, hostInGrad = grad.get(), np.zeros(data.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor ic in range(inmaps):\n\t\t\tfor oc in range(outmaps):\n\t\t\t\tfor z in range(grad.shape[2]):\n\t\t\t\t\tfor y in range(grad.shape[3]):\n\t\t\t\t\t\tfor x in range(grad.shape[4]):\n\t\t\t\t\t\t\tfor dz in range(fsize):\n\t\t\t\t\t\t\t\tfor dy in range(fsize):\n\t\t\t\t\t\t\t\t\tfor dx in range(fsize):\n\t\t\t\t\t\t\t\t\t\thostInGrad[b, ic, z + dz, y + dy, x + dx] += hostW[oc, ic, dz, dy, dx] * \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t hostGrad[b, oc, z, y, x]\n\n\tassert np.allclose(hostInGrad, ingrad.get())\n\n\twgrad, bgrad = convNdBackwardParams(data, grad, W, bias)\n\n\thostWGrad = np.zeros(wgrad.shape, dtype=np.float32)\n\tfor b in range(batchsize):\n\t\tfor oc in range(outmaps):\n\t\t\tfor ic in range(inmaps):\n\t\t\t\tfor dz in range(fsize):\n\t\t\t\t\tfor dy in range(fsize):\n\t\t\t\t\t\tfor dx in range(fsize):\n\t\t\t\t\t\t\tfor z in range(grad.shape[2]):\n\t\t\t\t\t\t\t\tfor y in range(grad.shape[3]):\n\t\t\t\t\t\t\t\t\tfor x in range(grad.shape[4]):\n\t\t\t\t\t\t\t\t\t\thostWGrad[oc, ic, dz, dy, dx] += hostData[b, ic, z + dz, y + dy, x + dx] * \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t hostGrad[b, oc, z, y, x]\n\n\tassert np.allclose(hostWGrad, wgrad.get())\n\n\thostBGrad = np.empty(bias.shape, dtype=np.float32)\n\tfor oc in range(outmaps):\n\t\thostBGrad[0, oc, 0, 0, 0] = np.sum(hostGrad[:, oc, :, :, :])\n\n\tassert np.allclose(hostBGrad, bgrad.get())\n\n\ndef maxpool3dTest():\n\tbatchsize, maps, d, h, w = 1, 1, 6, 6, 6\n\tsize, stride, pad = 3, 2, 1\n\n\tdata = CPUArray.toDevice(np.random.randn(batchsize, maps, d, h, w).astype(np.float32))\n\n\toutdata, workspace, desc = poolNd(data, size=size, stride=stride, pad=pad, mode=PoolMode.max)\n\n\thostData = np.full(shape=(batchsize, maps, d + 2 * pad, h + 2 * pad, w + 2 * pad),\n\t\t\t\t\t   fill_value=np.finfo(np.float32).min, dtype=np.float32)\n\thostData[:, :, pad:-pad, pad:-pad, pad:-pad] = data.get()\n\thostOutData = np.empty(outdata.shape)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor z in range(hostOutData.shape[2]):\n\t\t\t\tfor y in range(hostOutData.shape[3]):\n\t\t\t\t\tfor x in range(hostOutData.shape[4]):\n\t\t\t\t\t\thostOutData[b, c, z, y, x] = np.max(hostData[b, c, z * stride:z * stride + size,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ty * stride:y*stride + size, x * stride:x * stride + size])\n\n\tassert np.allclose(hostOutData, outdata.get())\n\n\tgrad = CPUArray.toDevice(np.random.randn(*outdata.shape).astype(np.float32))\n\tingrad = poolNdBackward(data, grad, workspace, desc, size=size, stride=stride, pad=pad, mode=PoolMode.max)\n\n\thostGrad = grad.get()\n\thostInGrad = np.zeros(hostData.shape, dtype=np.float32)\n\n\tfor b in range(batchsize):\n\t\tfor c in range(maps):\n\t\t\tfor z in range(hostOutData.shape[2]):\n\t\t\t\tfor y in range(hostOutData.shape[3]):\n\t\t\t\t\tfor x in range(hostOutData.shape[4]):\n\t\t\t\t\t\tfor dz in range(size):\n\t\t\t\t\t\t\tfor dy in range(size):\n\t\t\t\t\t\t\t\tfor dx in range(size):\n\t\t\t\t\t\t\t\t\tif hostData[b,c,z*stride+dz,y*stride+dy,x*stride+dx] == hostOutData[b, c, z, y, x]:\n\t\t\t\t\t\t\t\t\t\thostInGrad[b,c,z*stride + dz,y*stride + dy,x*stride + dx] += hostGrad[b,c,z,y,x]\n\n\tassert np.allclose(hostInGrad[:, :, pad:-pad, pad:-pad, pad:-pad], ingrad.get())\n\n\ndef batchNorm3dTest():\n\tbatchsize, maps, d, h, w = 2, 5, 2, 3, 2\n\tdata = CPUArray.toDevice(np.random.randn(batchsize, maps, d, h, w).astype(np.float32))\n\thostData = data.get()\n\n\tscale = CPUArray.toDevice(np.random.randn(1, maps, 1, 1, 1).astype(np.float32))\n\tbias = CPUArray.toDevice(np.random.randn(*scale.shape).astype(np.float32))\n\tmean = CPUArray.zeros(scale.shape, dtype=np.float32)\n\tvar = CPUArray.toDevice(np.ones(scale.shape, dtype=np.float32))\n\n\toutdata = CPUArray.copy(data)\n\toutdata, savemean, savevar, desc = batchNormNd(outdata, scale, bias, mean, var, out=outdata)\n\n\thostScale, hostBias = scale.get(), bias.get()\n\thostNormData = np.empty(data.shape, dtype=np.float32)\n\thostOutData = np.empty(data.shape, dtype=np.float32)\n\thostMean = np.zeros(scale.shape, dtype=np.float32)\n\thostVar = np.zeros(scale.shape, dtype=np.float32)\n\thostInvVar = np.zeros(scale.shape, dtype=np.float32)\n\tfor c in range(maps):\n\t\tfor b in range(batchsize):\n\t\t\thostMean[0, c, 0, 0, 0] += np.sum(hostData[b, c])\n\t\thostMean[0, c, 0, 0, 0] /= (batchsize * w * h * d)\n\n\t\tfor b in range(batchsize):\n\t\t\thostVar[0, c, 0, 0, 0] += np.sum((hostData[b, c] - hostMean[0, c, 0, 0, 0])**2)\n\t\thostVar[0, c, 0, 0, 0] /= (batchsize * w * h * d)\n\n\t\thostInvVar[0, c, 0, 0, 0] = 1.0 / np.sqrt(hostVar[0, c, 0, 0, 0] + 1e-5)\n\t\thostNormData[:, c, :, :, :] = (hostData[:, c, :, :, :] - hostMean[0, c, 0, 0, 0]) * hostInvVar[0, c, 0, 0, 0]\n\t\thostOutData[:, c, :, :, :] = hostNormData[:, c, :, :, :] * hostScale[0, c, 0, 0, 0] + hostBias[0, c, 0, 0, 0]\n\n\tassert np.allclose(hostMean, mean.get())\n\tassert np.allclose(hostVar, savevar.get())\n\tassert np.allclose(hostOutData, outdata.get())\n\n\tgrad = CPUArray.toDevice(np.random.randn(batchsize, maps, d, h, w).astype(np.float32))\n\n\tingrad, scalegrad, biasgrad = batchNormNdBackward(data, grad, scale, bias, savemean, savevar, desc)\n\n\thostGrad = grad.get()\n\thostInGrad, hostScaleGrad = np.empty(hostGrad.shape, dtype=np.float32), np.empty(hostScale.shape, dtype=np.float32)\n\thostBiasGrad, hostMeanGrad = np.empty(hostBias.shape, dtype=np.float32), np.empty(hostMean.shape, dtype=np.float32)\n\thostVarGrad = np.empty(hostInvVar.shape, dtype=np.float32)\n\tfor c in range(maps):\n\t\thostBiasGrad[0, c, 0, 0, 0] = np.sum(hostGrad[:, c, :, :, :])\n\t\thostScaleGrad[0, c, 0, 0, 0] = np.sum(hostGrad[:, c, :, :, :] * hostNormData[:, c, :, :, :])\n\n\t\thostMeanGrad[0, c, 0, 0, 0] = np.sum(hostGrad[:, c, :, :, :]) * hostScale[0,c,0,0,0] * -hostInvVar[0,c,0,0,0]\n\t\thostVarGrad[0, c, 0, 0, 0] = np.sum(hostGrad[:, c, :, :, :] * (hostData[:,c,:,:,:] - hostMean[0,c,0,0,0])) * \\\n\t\t\t\t\t\t\t\t\t hostScale[0, c, 0, 0, 0] * -0.5 * hostInvVar[0, c, 0, 0, 0]**3\n\n\t\thostInGrad[:, c, :, :, :] = hostGrad[:, c, :, :, :] * hostScale[0, c, 0, 0, 0] * hostInvVar[0, c, 0, 0, 0] + \\\n\t\t\t\t\t\t\t\t\thostVarGrad[0, c, 0, 0, 0] * 2 / (batchsize * w * h * d) * \\\n\t\t\t\t\t\t\t\t\t(hostData[:, c, :, :, :] - hostMean[0, c, 0, 0, 0]) + \\\n\t\t\t\t\t\t\t\t\thostMeanGrad[0, c, 0, 0, 0] / (batchsize * w * h * d)\n\tassert np.allclose(hostInGrad, ingrad.get())\n\tassert np.allclose(hostScaleGrad, scalegrad.get())\n\tassert np.allclose(hostBiasGrad, biasgrad.get())\n\n\tbatchNormNd(data, scale, bias, mean, var, test=True)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Intel/Wrappers/DNNLBlas.py,7,"b'import numpy as np\n\nfrom PuzzleLib.CPU.CPUArray import CPUArray\nfrom PuzzleLib.Intel.ThirdParty import libdnnl\n\n\ndef mulMatrixOnMatrix(A, B, out=None, transpA=False, transpB=False, alpha=1.0, beta=0.0):\n\tassert not (transpA and transpB)\n\tassert A.ndim == 2 and B.ndim == 2\n\n\tassert A.dtype == B.dtype and A.dtype == np.float32\n\tassert A.flags.c_contiguous and B.flags.c_contiguous\n\n\tif transpA:\n\t\tassert A.shape[0] == B.shape[0]\n\t\tshape = (A.shape[1], B.shape[1])\n\telif transpB:\n\t\tassert A.shape[1] == B.shape[1]\n\t\tshape = (A.shape[0], B.shape[0])\n\telse:\n\t\tassert A.shape[1] == B.shape[0]\n\t\tshape = (A.shape[0], B.shape[1])\n\n\tif out is None:\n\t\tout = CPUArray.empty(shape, dtype=np.float32)\n\n\tif transpA:\n\t\tk, m = A.shape\n\t\tn = B.shape[1]\n\t\tlibdnnl.dnnl_sgemm(\'t\', \'n\', m, n, k, alpha, A.ptr, m, B.ptr, n, beta, out.ptr, n)\n\telif transpB:\n\t\tm, k = A.shape\n\t\tn = B.shape[0]\n\t\tlibdnnl.dnnl_sgemm(\'n\', \'t\', m, n, k, alpha, A.ptr, k, B.ptr, k, beta, out.ptr, n)\n\telse:\n\t\tm, k = A.shape\n\t\tn = B.shape[1]\n\t\tlibdnnl.dnnl_sgemm(\'n\', \'n\', m, n, k, alpha, A.ptr, k, B.ptr, n, beta, out.ptr, n)\n\n\treturn out\n\n\ndef unittest():\n\tA = CPUArray.toDevice(np.random.randn(5, 3).astype(np.float32))\n\tB = CPUArray.toDevice(np.random.randn(3, 4).astype(np.float32))\n\n\tC = mulMatrixOnMatrix(A, B)\n\tassert np.allclose(np.dot(A.get(), B.get()), C.get())\n\n\tF = mulMatrixOnMatrix(B, C, transpB=True)\n\tassert np.allclose(np.dot(B.get(), C.get().T), F.get())\n\n\tG = mulMatrixOnMatrix(F, B, transpA=True)\n\tassert np.allclose(np.dot(F.get().T, B.get()), G.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Intel/Wrappers/DNNLInstanceNorm.py,27,"b'import numpy as np\n\nfrom PuzzleLib.CPU.CPUArray import CPUArray\nfrom PuzzleLib.Intel.Wrappers import DNNL\n\n\ndef instanceNorm2d(data, scale, bias, epsilon=1e-5):\n\tbatchsize = data.shape[0]\n\tif batchsize > 1:\n\t\textscale = CPUArray.toDevice(np.tile(scale.data, (batchsize, 1, 1)))\n\t\textbias = CPUArray.toDevice(np.tile(bias.data, (batchsize, 1, 1)))\n\n\telse:\n\t\textscale, extbias = scale, bias\n\n\tindata = data.reshape(1, batchsize * data.shape[1], data.shape[2], data.shape[3])\n\tmean = CPUArray.empty((1, indata.shape[1], 1, 1), dtype=np.float32)\n\tvar = CPUArray.empty((1, indata.shape[1], 1, 1), dtype=np.float32)\n\n\toutdata, savemean, savevar, desc = DNNL.batchNormNd(indata, extscale, extbias, mean, var, epsilon, test=False)\n\treturn outdata.reshape(data.shape), savemean, savevar, extscale, extbias, desc\n\n\ndef instanceNorm2dBackward(grad, data, extscale, extbias, savemean, savevar, epsilon, desc, affine=True):\n\tbatchsize, maps = grad.shape[:2]\n\n\toutgrad = grad.reshape(1, batchsize * grad.shape[1], grad.shape[2], grad.shape[3])\n\tindata = data.reshape(1, batchsize * data.shape[1], data.shape[2], data.shape[3])\n\n\tingrad, scalegrad, biasgrad = DNNL.batchNormNdBackward(\n\t\tindata, outgrad, extscale, extbias, savemean, savevar, desc, epsilon\n\t)\n\n\tif affine and batchsize > 1:\n\t\tscalegrad = np.sum(scalegrad.data.reshape(batchsize, -1), axis=0).reshape((1, maps, 1, 1))\n\t\tbiasgrad = np.sum(biasgrad.data.reshape(batchsize, -1), axis=0).reshape((1, maps, 1, 1))\n\n\t\tscalegrad = CPUArray(scalegrad.shape, scalegrad.dtype, data=scalegrad, acquire=True)\n\t\tbiasgrad = CPUArray(biasgrad.shape, biasgrad.dtype, data=biasgrad, acquire=True)\n\n\treturn (ingrad.reshape(grad.shape), scalegrad, biasgrad) if affine else ingrad.reshape(grad.shape)\n\n\ndef unittest():\n\tbatchsize, maps, h, w = 3, 4, 5, 5\n\tepsilon = 1e-5\n\n\tdata = CPUArray.toDevice(np.random.randn(batchsize, maps, h, w).astype(np.float32))\n\tscale = CPUArray.toDevice(np.random.randn(1, maps, 1, 1).astype(np.float32))\n\tbias = CPUArray.toDevice(np.random.randn(1, maps, 1, 1).astype(np.float32))\n\n\toutdata, savemean, savevar, extscale, extbias, desc = instanceNorm2d(data, scale, bias, epsilon)\n\n\thostData = data.get().reshape(data.shape[0] * data.shape[1], -1)\n\thostScale, hostBias = scale.get().reshape(maps, 1), bias.get().reshape(maps, 1)\n\thostExtScale, hostExtBias = np.tile(hostScale, (batchsize, 1)), np.tile(hostBias, (batchsize, 1))\n\n\thostMean = np.mean(hostData, axis=1, keepdims=True)\n\thostVar = np.var(hostData, axis=1)\n\thostInvVar = 1.0 / np.sqrt(hostVar + epsilon)\n\thostOutData = (hostData - hostMean) * hostInvVar[:, np.newaxis]\n\thostOutScData = hostOutData * hostExtScale + hostExtBias\n\n\tassert np.allclose(hostOutScData.reshape(data.shape), outdata.get())\n\tassert np.allclose(hostMean.reshape(savemean.shape), savemean.get())\n\tassert np.allclose(hostVar.reshape(savevar.shape), savevar.get())\n\n\tgrad = CPUArray.toDevice(np.random.randn(batchsize, maps, h, w).astype(np.float32))\n\tingrad, scalegrad, bgrad = instanceNorm2dBackward(grad, data, extscale, extbias, savemean, savevar, epsilon, desc)\n\n\thostGrad = grad.get().reshape(grad.shape[0] * grad.shape[1], -1)\n\thostScGrad = hostGrad * hostExtScale\n\thostCorrs = np.empty(hostInvVar.shape, dtype=np.float32)\n\tfor i in range(hostCorrs.shape[0]):\n\t\thostCorrs[i] = np.dot(hostScGrad[i], hostOutData[i]) / hostScGrad.shape[1]\n\thostInGrad = hostScGrad - np.mean(hostScGrad, axis=1, keepdims=True) - hostCorrs[:, np.newaxis] * hostOutData\n\thostInGrad *= hostInvVar[:, np.newaxis]\n\n\thostScaleGrad = np.sum(np.sum(hostOutData * hostGrad, axis=1).reshape(batchsize, -1), axis=0)\n\thostBiasGrad = np.sum(np.sum(hostGrad, axis=1).reshape(batchsize, -1), axis=0)\n\n\tassert np.allclose(hostInGrad.reshape(grad.shape), ingrad.get())\n\tassert np.allclose(hostScaleGrad.reshape((1, maps, 1, 1)), scalegrad.get())\n\tassert np.allclose(hostBiasGrad.reshape((1, maps, 1, 1)), bgrad.get())\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Models/Misc/RBM.py,6,"b'import math\n\nimport numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool, globalRng\nfrom PuzzleLib.Backend.Kernels.ElementWise import rbmKer\nfrom PuzzleLib.Backend.Kernels.MatVec import addVecToMat\nfrom PuzzleLib.Backend import Blas\n\nfrom PuzzleLib.Variable import Variable\nfrom PuzzleLib.Modules.Module import Module\n\n\nclass RBM(Module):\n\tdef __init__(self, vsize, hsize, wscale=1.0, rng=globalRng, useBias=True, name=None):\n\t\tsuper().__init__(name)\n\t\tself.rng = rng\n\n\t\tW = np.random.normal(0.0, wscale / math.sqrt(vsize + hsize), (vsize, hsize)).astype(np.float32)\n\n\t\tself.W = None\n\t\tself.setVar(""W"", Variable(gpuarray.to_gpu(W, allocator=memPool)))\n\n\t\tself.useBias = useBias\n\n\t\tif useBias:\n\t\t\tself.b = None\n\t\t\tself.setVar(""b"", Variable(gpuarray.zeros((vsize, ), dtype=np.float32, allocator=memPool)))\n\n\t\t\tself.c = None\n\t\t\tself.setVar(""c"", Variable(gpuarray.zeros((hsize, ), dtype=np.float32, allocator=memPool)))\n\n\t\tself.particles = None\n\n\n\tdef hiddenFromVisible(self, visible):\n\t\thidden = Blas.mulMatrixOnMatrix(visible, self.W)\n\n\t\tif self.useBias:\n\t\t\taddVecToMat(self.c, hidden, axis=1, out=hidden)\n\n\t\tself.activateNeurons(hidden)\n\t\treturn hidden\n\n\n\tdef visibleFromHidden(self, hidden):\n\t\tvisible = Blas.mulMatrixOnMatrix(hidden, self.W, transpB=True)\n\n\t\tif self.useBias:\n\t\t\taddVecToMat(self.b, visible, axis=1, out=visible)\n\n\t\tself.activateNeurons(visible)\n\t\treturn visible\n\n\n\tdef activateNeurons(self, neurons):\n\t\trands = gpuarray.empty(neurons.shape, dtype=np.float32, allocator=memPool)\n\t\tself.rng.fillUniform(rands)\n\n\t\trbmKer(neurons, neurons, rands)\n\n\n\tdef updateData(self, data):\n\t\traise RuntimeError(""RBM does not support full module interface"")\n\n\n\tdef updateGrad(self, grad):\n\t\traise RuntimeError(""RBM does not support full module interface"")\n\n\n\tdef calcCDGrad(self, data):\n\t\thidden = self.posPhaseGrad(data)\n\t\tself.negPhaseGrad(hidden)\n\n\n\tdef calcPCDGrad(self, data):\n\t\thidden = self.posPhaseGrad(data)\n\n\t\tif self.particles is None:\n\t\t\tself.particles = gpuarray.to_gpu(np.random.binomial(1, 0.5, size=hidden.shape).astype(np.float32))\n\n\t\tself.particles = self.negPhaseGrad(self.particles)\n\n\n\tdef posPhaseGrad(self, data):\n\t\thidden = self.hiddenFromVisible(data)\n\t\tBlas.mulMatrixOnMatrix(data, hidden, out=self.vars[""W""].grad, transpA=True)\n\n\t\tif self.useBias:\n\t\t\tBlas.sumOnMatrix(data, out=self.vars[""b""].grad)\n\t\t\tBlas.sumOnMatrix(hidden, out=self.vars[""c""].grad)\n\n\t\treturn hidden\n\n\n\tdef negPhaseGrad(self, hidden):\n\t\tvisible = self.visibleFromHidden(hidden)\n\t\thidden = self.hiddenFromVisible(visible)\n\n\t\tBlas.mulMatrixOnMatrix(visible, hidden, out=self.vars[""W""].grad, transpA=True, alpha=-1.0, beta=1.0)\n\n\t\tif self.useBias:\n\t\t\tBlas.sumOnMatrix(visible, out=self.vars[""b""].grad, alpha=-1.0, beta=1.0)\n\t\t\tBlas.sumOnMatrix(hidden, out=self.vars[""c""].grad, alpha=-1.0, beta=1.0)\n\n\t\treturn hidden\n\n\n\tdef dataShapeFrom(self, shape):\n\t\traise NotImplementedError()\n\n\n\tdef gradShapeFrom(self, shape):\n\t\traise NotImplementedError()\n\n\ndef unittest():\n\tfrom PuzzleLib.Optimizers.MomentumSGD import MomentumSGD\n\tfrom PuzzleLib.Datasets.MnistLoader import MnistLoader\n\tfrom PuzzleLib.Visual import showImageBatchInFolder\n\n\tmnist = MnistLoader()\n\tdata, _ = mnist.load(path=""../../TestData"")\n\tdata = data[:].reshape(data.shape[0], np.prod(data.shape[1:]))\n\n\trbm = RBM(784, 500)\n\toptimizer = MomentumSGD(momRate=0.5)\n\toptimizer.setupOn(rbm, useGlobalState=True)\n\n\tdata = gpuarray.to_gpu(data)\n\tbatchsize = 100\n\n\tfor epoch in range(10):\n\t\tfor i in range(data.shape[0] // batchsize):\n\t\t\tbatch = data[i * batchsize:(i+1) * batchsize]\n\t\t\trbm.calcPCDGrad(batch)\n\t\t\toptimizer.update()\n\n\t\toptimizer.learnRate *= 0.9\n\t\tprint(""Finished epoch %d"" % (epoch+1))\n\n\t\tif (epoch+1) % 5 == 0:\n\t\t\tfilters = rbm.W.get().T\n\t\t\tshowImageBatchInFolder(filters.reshape(500, 1, 28, 28), ""../../TestData/rbm"", ""filter"")\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Models/Nets/Inception.py,2,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\n\nfrom PuzzleLib.Containers.Sequential import Sequential\nfrom PuzzleLib.Containers.Parallel import Parallel\n\nfrom PuzzleLib.Modules.Conv2D import Conv2D\nfrom PuzzleLib.Modules.BatchNorm2D import BatchNorm2D\nfrom PuzzleLib.Modules.Activation import Activation, relu\nfrom PuzzleLib.Modules.MaxPool2D import MaxPool2D\nfrom PuzzleLib.Modules.AvgPool2D import AvgPool2D\nfrom PuzzleLib.Modules.Flatten import Flatten\nfrom PuzzleLib.Modules.Linear import Linear\nfrom PuzzleLib.Modules.SoftMax import SoftMax\nfrom PuzzleLib.Modules.Replicate import Replicate\nfrom PuzzleLib.Modules.Concat import Concat\nfrom PuzzleLib.Modules.ToList import ToList\n\n\ndef loadInceptionBN(modelpath, actInplace=False, bnInplace=False, initscheme=""none"", name=""Inception-BN-0126""):\n\tnet = Sequential(name=name)\n\n\tnet.append(Conv2D(3, 64, 7, stride=2, pad=3, useBias=False, initscheme=initscheme, name=""conv_1""))\n\tnet.append(BatchNorm2D(64, inplace=bnInplace, name=""bn_1""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu_1""))\n\n\tnet.append(MaxPool2D(3, 2, pad=1, name=""pool_1""))\n\n\tnet.append(Conv2D(64, 64, 1, useBias=False, initscheme=initscheme, name=""conv_2_red""))\n\tnet.append(BatchNorm2D(64, inplace=bnInplace, name=""bn_2_red""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu_2_red""))\n\n\tnet.append(Conv2D(64, 192, 3, pad=1, useBias=False, initscheme=initscheme, name=""conv_2""))\n\tnet.append(BatchNorm2D(192, inplace=bnInplace, name=""bn_2""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu_2""))\n\n\tnet.append(MaxPool2D(3, 2, pad=1, name=""pool_2""))\n\n\tact, bn = actInplace, bnInplace\n\tnet.extend(bnBlock(192, [64], [64, 64], [64, 96, 96], [32], act=act, bn=bn, scheme=initscheme, name=""3a""))\n\tnet.extend(bnBlock(256, [64], [64, 96], [64, 96, 96], [64], act=act, bn=bn, scheme=initscheme, name=""3b""))\n\tnet.extend(bnShrinkBlock(320, [128, 160], [64, 96, 96], bn=bn, act=act, scheme=initscheme, name=""3c""))\n\n\tnet.extend(bnBlock(576, [224], [64, 96], [96, 128, 128], [128], act=act, bn=bn, scheme=initscheme, name=""4a""))\n\tnet.extend(bnBlock(576, [192], [96, 128], [96, 128, 128], [128], act=act, bn=bn, scheme=initscheme, name=""4b""))\n\tnet.extend(bnBlock(576, [160], [128, 160], [128, 160, 160], [128], act=act,bn=bn, scheme=initscheme, name=""4c""))\n\tnet.extend(bnBlock(608, [96], [128,192], [160, 192, 192], [128], act=act, bn=bn, scheme=initscheme, name=""4d""))\n\tnet.extend(bnShrinkBlock(608, [128, 192], [192, 256, 256], act=act, bn=bn, scheme=initscheme, name=""4e""))\n\n\tnet.extend(bnBlock(1056, [352], [192, 320], [160,224,224], [128], act=act, bn=bn, scheme=initscheme, name=""5a""))\n\tnet.extend(bnBlock(1024, [352], [192, 320], [192,224,224], [128], act=act, bn=bn, scheme=initscheme, name=""5b""))\n\n\tnet.append(AvgPool2D(7, 1, name=""global_pool""))\n\tnet.append(Flatten(name=""flatten""))\n\tnet.append(Linear(1024, 1000, initscheme=initscheme, name=""fc1""))\n\tnet.append(SoftMax(name=""softmax""))\n\n\tif modelpath is not None:\n\t\tnet.load(modelpath, assumeUniqueNames=True)\n\n\treturn net\n\n\ndef loadInceptionV3(modelpath, actInplace=False, bnInplace=False, initscheme=""none"", name=""Inception-7-0001""):\n\tnet = Sequential(name=name)\n\n\tnet.append(Conv2D(3, 32, 3, stride=2, useBias=False, initscheme=initscheme, name=""conv_conv2d""))\n\tnet.append(BatchNorm2D(32, name=""conv_batchnorm""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""conv_relu""))\n\n\tnet.append(Conv2D(32, 32, 3, useBias=False, initscheme=initscheme, name=""conv_1_conv2d""))\n\tnet.append(BatchNorm2D(32, name=""conv_1_batchnorm""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""conv_1_relu""))\n\n\tnet.append(Conv2D(32, 64, 3, pad=1, useBias=False, initscheme=initscheme, name=""conv_2_conv2d""))\n\tnet.append(BatchNorm2D(64, name=""conv_2_batchnorm""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""conv_2_relu""))\n\n\tnet.append(MaxPool2D(3, 2, name=""pool""))\n\n\tnet.append(Conv2D(64, 80, 1, useBias=False, initscheme=initscheme, name=""conv_3_conv2d""))\n\tnet.append(BatchNorm2D(80, name=""conv_3_batchnorm""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""conv_3_relu""))\n\n\tnet.append(Conv2D(80, 192, 3, useBias=False, initscheme=initscheme, name=""conv_4_conv2d""))\n\tnet.append(BatchNorm2D(192, name=""conv_4_batchnorm""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""conv_4_relu""))\n\n\tnet.append(MaxPool2D(3, 2, name=""pool1""))\n\n\tact, bn = actInplace, bnInplace\n\tnet.extend(bnBlock(192, [64], [48, 64], [64, 96, 96], [32], ""mixed"", act, bn, initscheme, 5, 2, ""v3""))\n\tnet.extend(bnBlock(256, [64], [48, 64], [64, 96, 96], [64], ""mixed_1"", act, bn, initscheme, 5, 2, ""v3""))\n\tnet.extend(bnBlock(288, [64], [48, 64], [64, 96, 96], [64], ""mixed_2"", act, bn, initscheme, 5, 2, ""v3""))\n\tnet.extend(bnShrinkBlock(288, [384], [64, 96, 96], ""mixed_3"", act, bn, initscheme, False, 0, ""v3""))\n\n\tnet.extend(factorBlock(768, [192], [128, 128, 192], [128,128,128,128,192], [192], ""mixed_4"", act, bn, initscheme))\n\tnet.extend(factorBlock(768, [192], [160, 160, 192], [160,160,160,160,192], [192], ""mixed_5"", act, bn, initscheme))\n\tnet.extend(factorBlock(768, [192], [160, 160, 192], [160,160,160,160,192], [192], ""mixed_6"", act, bn, initscheme))\n\tnet.extend(factorBlock(768, [192], [192, 192, 192], [192,192,192,192,192], [192], ""mixed_7"", act, bn, initscheme))\n\tnet.extend(v3ShrinkBlock(768, [192, 320], [192, 192, 192, 192], ""mixed_8"", act, bn, initscheme))\n\n\tnet.extend(expandBlock(\n\t\t1280, [320], [384, 384, 384], [448, 384, 384, 384], [192], ""mixed_9"", act, bn, initscheme, pool=""avg""\n\t))\n\n\tnet.extend(expandBlock(\n\t\t2048, [320], [384, 384, 384], [448, 384, 384, 384], [192], ""mixed_10"", act, bn, initscheme, pool=""max""\n\t))\n\n\tnet.append(AvgPool2D(8, 1, name=""global_pool""))\n\tnet.append(Flatten(name=""flatten""))\n\tnet.append(Linear(2048, 1008, name=""fc1""))\n\tnet.append(SoftMax(name=""softmax""))\n\n\tif modelpath is not None:\n\t\tnet.load(modelpath, assumeUniqueNames=True)\n\n\treturn net\n\n\ndef convBN(inmaps, outmaps, size, stride, pad, name, actInplace, bnInplace, scheme, typ=""bn""):\n\tblock = Sequential()\n\n\tif typ == ""bn"":\n\t\tnames = [""conv_%s"" % name, ""bn_%s"" % name, ""relu_%s"" % name]\n\n\telif typ == ""v3"":\n\t\tnames = [""%s_conv2d"" % name, ""%s_batchnorm"" % name, ""%s_relu"" % name]\n\n\telse:\n\t\traise ValueError(""Unrecognized convBN type"")\n\n\tblock.append(Conv2D(inmaps, outmaps, size, stride, pad, useBias=False, initscheme=scheme, name=names[0]))\n\tblock.append(BatchNorm2D(outmaps, inplace=bnInplace, name=names[1]))\n\tblock.append(Activation(relu, inplace=actInplace, name=names[2]))\n\n\treturn block\n\n\ndef pool2D(size, stride, pad, name):\n\tif ""max"" in name:\n\t\treturn MaxPool2D(size, stride, pad)\n\n\telif ""avg"" in name:\n\t\treturn AvgPool2D(size, stride, pad)\n\n\telse:\n\t\traise ValueError(""Unrecognized pool type"")\n\n\ndef tower(towername, names, maps, sizes, strides, pads, act, bn, scheme, typ=""bn""):\n\tblock = Sequential()\n\n\tlvlnames = [""%s_%s"" % (towername, name) for name in names]\n\n\tfor i, name in enumerate(lvlnames):\n\t\tif ""pool"" in name:\n\t\t\tblock.append(pool2D(sizes[i], strides[i], pads[i], name=names[i]))\n\n\t\telse:\n\t\t\tact = False if i == len(names) - 1 else act\n\t\t\tblock.extend(convBN(maps[i], maps[i+1], sizes[i], strides[i], pads[i], lvlnames[i], act, bn, scheme, typ))\n\n\treturn block\n\n\ndef bnBlock(inmaps, b1m, b2m, b3m, b4m, name, act, bn, scheme, b2size=3, b2pad=1, typ=""bn""):\n\tblock = Sequential()\n\n\tif typ == ""bn"":\n\t\tb1towername, b1names = name, [""1x1""]\n\t\tb2towername, b2names = name, [""3x3_reduce"",""3x3""]\n\t\tb3towername, b3names = name, [""double_3x3_reduce"", ""double_3x3_0"", ""double_3x3_1""]\n\t\tb4towername, b4names = name, [""avg_pool"", ""proj""]\n\n\telif typ == ""v3"":\n\t\tb1towername, b1names = name, [""conv""]\n\t\tb2towername, b2names = ""%s_tower"" % name, [""conv"", ""conv_1""]\n\t\tb3towername, b3names = ""%s_tower_1"" % name, [""conv"", ""conv_1"", ""conv_2""]\n\t\tb4towername, b4names = ""%s_tower_2"" % name, [""avg_pool"", ""conv""]\n\n\telse:\n\t\traise ValueError(""Unrecognized block type"")\n\n\tbranch1 = tower(\n\t\tb1towername, b1names, [inmaps] + b1m, [1], strides=[1], pads=[0], act=act, bn=bn, scheme=scheme, typ=typ\n\t)\n\n\tbranch2 = tower(\n\t\tb2towername, b2names, [inmaps] + b2m, [1, b2size], strides=[1, 1], pads=[0, b2pad], act=act, bn=bn,\n\t\tscheme=scheme, typ=typ\n\t)\n\n\tbranch3 = tower(\n\t\tb3towername, b3names, [inmaps] + b3m, [1, 3, 3], strides=[1, 1, 1], pads=[0, 1, 1], act=act, bn=bn,\n\t\tscheme=scheme, typ=typ\n\t)\n\n\tbranch4 = tower(\n\t\tb4towername, b4names, [inmaps, inmaps] + b4m, [3, 1], strides=[1, 1], pads=[1, 0], act=act, bn=bn,\n\t\tscheme=scheme, typ=typ\n\t)\n\n\tblock.append(Replicate(times=4))\n\tblock.append(Parallel().append(branch1).append(branch2).append(branch3).append(branch4))\n\tblock.append(Concat(axis=1, name=""ch_concat_%s_chconcat"" % name))\n\n\treturn block\n\n\ndef bnShrinkBlock(inmaps, b1m, b2m, name, act, bn, scheme, b1deep=True, pad=1, typ=""bn""):\n\tblock = Sequential()\n\n\tif typ == ""bn"":\n\t\tif b1deep:\n\t\t\tb1towername, b1names = name, [""3x3_reduce"",""3x3""]\n\t\telse:\n\t\t\tb1towername, b1names = name, [""3x3""]\n\n\t\tb2towername, b2names = name, [""double_3x3_reduce"", ""double_3x3_0"", ""double_3x3_1""]\n\t\tb3towername, b3names = name, [""max_pool""]\n\n\telif typ == ""v3"":\n\t\tif b1deep:\n\t\t\tb1towername, b1names = name, [""conv""]\n\t\telse:\n\t\t\tb1towername, b1names = name, [""conv""]\n\n\t\tb2towername, b2names = ""%s_tower"" % name, [""conv"", ""conv_1"", ""conv_2""]\n\t\tb3towername, b3names = name, [""max_pool""]\n\n\telse:\n\t\traise ValueError(""Unrecognized block type"")\n\n\tif b1deep:\n\t\tbranch1 = tower(\n\t\t\tb1towername, b1names, [inmaps] + b1m, [1, 3], [1, 2], [0, pad], act=act, bn=bn, scheme=scheme, typ=typ\n\t\t)\n\telse:\n\t\tbranch1 = tower(\n\t\t\tb1towername, b1names, [inmaps] + b1m, [3], [2], [pad], act=act, bn=bn, scheme=scheme, typ=typ\n\t\t)\n\n\tbranch2 = tower(\n\t\tb2towername, b2names, [inmaps] + b2m, [1, 3, 3], [1, 1, 2], [0, 1, pad], act=act, bn=bn, scheme=scheme, typ=typ\n\t)\n\n\tbranch3 = tower(\n\t\tb3towername, b3names, [inmaps, inmaps], [3], [2], [pad], act=act, bn=bn, scheme=scheme, typ=typ\n\t)\n\n\tblock.append(Replicate(times=3))\n\tblock.append(Parallel().append(branch1).append(branch2).append(branch3))\n\tblock.append(Concat(axis=1, name=""ch_concat_%s_chconcat"" % name))\n\n\treturn block\n\n\ndef factorBlock(inmaps, b1m, b2m, b3m, b4m, name, act, bn, scheme):\n\tblock = Sequential()\n\n\tb1towername, b1names = name, [""conv""]\n\tb2towername, b2names = ""%s_tower"" % name, [""conv"", ""conv_1"", ""conv_2""]\n\tb3towername, b3names = ""%s_tower_1"" % name, [""conv"", ""conv_1"", ""conv_2"", ""conv_3"", ""conv_4""]\n\tb4towername, b4names = ""%s_tower_2"" % name, [""avg_pool"", ""conv""]\n\n\tbranch1 = tower(\n\t\tb1towername, b1names, [inmaps] + b1m, [1], [1], [0], act=act, bn=bn, scheme=scheme, typ=""v3""\n\t)\n\n\tbranch2 = tower(\n\t\tb2towername, b2names, [inmaps] + b2m, [1, (1, 7), (7, 1)], [1, 1, 1], [0, (0, 3), (3, 0)], act=act, bn=bn,\n\t\tscheme=scheme, typ=""v3""\n\t)\n\n\tbranch3 = tower(\n\t\tb3towername, b3names, [inmaps] + b3m, [1, (7, 1), (1, 7), (7, 1), (1, 7)], [1, 1, 1, 1, 1],\n\t\t[0, (3, 0), (0, 3), (3, 0), (0, 3)], act=act, bn=bn, scheme=scheme, typ=""v3""\n\t)\n\n\tbranch4 = tower(\n\t\tb4towername, b4names, [inmaps, inmaps] + b4m, [3, 1], [1, 1], [1, 0], act=act, bn=bn, scheme=scheme, typ=""v3""\n\t)\n\n\tblock.append(Replicate(times=4))\n\tblock.append(Parallel().append(branch1).append(branch2).append(branch3).append(branch4))\n\tblock.append(Concat(axis=1, name=""ch_concat_%s_chconcat"" % name))\n\n\treturn block\n\n\ndef v3ShrinkBlock(inmaps, b1m, b2m, name, act, bn, scheme):\n\tblock = Sequential()\n\n\tb1towername, b1names = ""%s_tower"" % name, [""conv"", ""conv_1""]\n\tb2towername, b2names = ""%s_tower_1"" % name, [""conv"", ""conv_1"", ""conv_2"", ""conv_3""]\n\tb3towername, b3names = name, [""max_pool""]\n\n\tbranch1 = tower(\n\t\tb1towername, b1names, [inmaps] + b1m, [1, 3], [1, 2], [0, 0], act=act, bn=bn, scheme=scheme, typ=""v3""\n\t)\n\n\tbranch2 = tower(\n\t\tb2towername, b2names, [inmaps] + b2m, [1, (1, 7), (7, 1), 3], [1, 1, 1, 2], [0, (0, 3), (3, 0), 0],\n\t\tact=act, bn=bn, scheme=scheme, typ=""v3""\n\t)\n\n\tbranch3 = tower(b3towername, b3names, [inmaps, inmaps], [3], [2], [0], act=act, bn=bn, scheme=scheme, typ=""v3"")\n\n\tblock.append(Replicate(times=3))\n\tblock.append(Parallel().append(branch1).append(branch2).append(branch3))\n\tblock.append(Concat(axis=1, name=""ch_concat_%s_chconcat"" % name))\n\n\treturn block\n\n\ndef expandBlock(inmaps, b1m, b2m, b3m, b4m, name, act, bn, scheme, pool=""avg""):\n\tblock = Sequential()\n\n\tb1towername, b1names = name, [""conv""]\n\tb2towername, b2names, b2sub1names, b2sub2names = ""%s_tower"" % name, [""conv""], [""mixed_conv""], [""mixed_conv_1""]\n\tb3towername,b3names,b3sub1names,b3sub2names = ""%s_tower_1""%name, [""conv"",""conv_1""], [""mixed_conv""], [""mixed_conv_1""]\n\n\tbranch1 = tower(b1towername, b1names, [inmaps] + b1m, [1], [1], [0], act=act, bn=bn, scheme=scheme, typ=""v3"")\n\n\tbranch2 = tower(b2towername, b2names, [inmaps, b2m[0]], [1], [1], [0], act=act, bn=bn, scheme=scheme, typ=""v3"")\n\tbranch2sub1 = tower(\n\t\tb2towername, b2sub1names, [b2m[0], b2m[1]], [(1, 3)], [1], [(0, 1)], act=act, bn=bn, scheme=scheme, typ=""v3""\n\t)\n\tbranch2sub2 = tower(\n\t\tb2towername, b2sub2names, [b2m[0], b2m[2]], [(3, 1)], [1], [(1, 0)], act=act, bn=bn, scheme=scheme, typ=""v3""\n\t)\n\n\tbranch2.append(Replicate(times=2))\n\tbranch2.append(Parallel().append(branch2sub1).append(branch2sub2))\n\n\tbranch3 = tower(\n\t\tb3towername, b3names, [inmaps, b3m[0], b3m[1]], [1, 3], [1, 1], [0, 1], act=act, bn=bn, scheme=scheme, typ=""v3""\n\t)\n\tbranch3sub1 = tower(\n\t\tb3towername, b3sub1names, [b3m[1], b3m[2]], [(1, 3)], [1], [(0, 1)], act=act, bn=bn, scheme=scheme, typ=""v3""\n\t)\n\tbranch3sub2 = tower(\n\t\tb3towername, b3sub2names, [b3m[1], b3m[3]], [(3, 1)], [1], [(1, 0)], act=act, bn=bn, scheme=scheme, typ=""v3""\n\t)\n\n\tbranch3.append(Replicate(times=2))\n\tbranch3.append(Parallel().append(branch3sub1).append(branch3sub2))\n\n\tif pool == ""avg"":\n\t\tbranch4 = tower(\n\t\t\t""%s_tower_2"" % name, [""avg_pool"", ""conv""], [inmaps, inmaps] + b4m, [3, 1], [1, 1], [1, 0], act=act, bn=bn,\n\t\t\tscheme=scheme, typ=""v3""\n\t\t)\n\n\telif pool == ""max"":\n\t\tbranch4 = tower(\n\t\t\t""%s_tower_2"" % name, [""max_pool"", ""conv""], [inmaps, inmaps] + b4m, [3, 1], [1, 1], [1, 0], act=act, bn=bn,\n\t\t\tscheme=scheme, typ=""v3""\n\t\t)\n\n\telse:\n\t\traise ValueError(""Unrecognized block type"")\n\n\tblock.append(Replicate(times=4))\n\tblock.append(Parallel().append(branch1).append(branch2).append(branch3).append(branch4))\n\tblock.append(ToList())\n\tblock.append(Concat(axis=1, name=""ch_concat_%s_chconcat"" % name))\n\n\treturn block\n\n\ndef unittest():\n\tbn = loadInceptionBN(None, initscheme=""gaussian"")\n\n\tdata = gpuarray.to_gpu(np.random.randn(1, 3, 224, 224).astype(np.float32))\n\tbn(data)\n\n\tdel bn\n\tmemPool.freeHeld()\n\n\tv3 = loadInceptionV3(None, initscheme=""gaussian"")\n\n\tdata = gpuarray.to_gpu(np.random.randn(1, 3, 299, 299).astype(np.float32))\n\tv3(data)\n\n\tdel v3\n\tmemPool.freeHeld()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Models/Nets/MiniYolo.py,2,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\n\nfrom PuzzleLib.Containers.Sequential import Sequential\n\nfrom PuzzleLib.Modules.Conv2D import Conv2D\nfrom PuzzleLib.Modules.Activation import Activation, relu, leakyRelu\nfrom PuzzleLib.Modules.MaxPool2D import MaxPool2D\nfrom PuzzleLib.Modules.Flatten import Flatten\nfrom PuzzleLib.Modules.Linear import Linear\nfrom PuzzleLib.Modules.SoftMax import SoftMax\n\n\ndef block(idx, inmaps, outmaps, sizeconv, strideconv, initscheme, actInPlace, sizepool=2, stridepool=2,\n\t\t  addMaxpool=True):\n\tassert len(inmaps) == len(outmaps) == len(sizeconv) == len(strideconv) == len(idx), ""lengths must be the same size""\n\n\tseq = Sequential()\n\n\tfor i in range(len(inmaps)):\n\t\tseq.append(Conv2D(\n\t\t\tinmaps=inmaps[i], outmaps=outmaps[i], size=sizeconv[i], pad=sizeconv[i] // 2, stride=strideconv[i],\n\t\t\tinitscheme=initscheme, dilation=1, useBias=True, name=""conv%s"" % idx[i]\n\t\t))\n\t\tseq.append(Activation(leakyRelu, inplace=actInPlace, args=(0.01, )))\n\n\tif addMaxpool:\n\t\tseq.append(MaxPool2D(size=sizepool, stride=stridepool, name=""conv%s_pool"" % idx[-1]))\n\n\treturn seq\n\n\ndef loadMiniYolo(modelpath, numOutput, actInplace=False, initscheme=""none""):\n\tnet = Sequential(name=""YOLONet"")\n\n\tblock0 = block(\n\t\tidx=[""1""], inmaps=[3], outmaps=[64], sizeconv=[7], strideconv=[2], initscheme=initscheme,\n\t\tactInPlace=actInplace\n\t)\n\tnet.extend(block0)\n\n\tblock1 = block(\n\t\tidx=[""2""], inmaps=[64], outmaps=[192], sizeconv=[3], strideconv=[1], initscheme=initscheme,\n\t\tactInPlace=actInplace\n\t)\n\tnet.extend(block1)\n\n\tblock2 = block(\n\t\tidx=[""3"", ""4"", ""5"", ""6""], inmaps=[192, 128, 256, 256], outmaps=[128, 256, 256, 512],\n\t\tsizeconv=[1, 3, 1, 3], strideconv=[1, 1, 1, 1], initscheme=initscheme, actInPlace=actInplace\n\t)\n\tnet.extend(block2)\n\n\tblock3 = block(\n\t\tidx=[""7"", ""8"", ""9"", ""10"", ""11"", ""12"", ""13"", ""14"", ""15"", ""16""],\n\t\tinmaps=[512, 256, 512, 256, 512, 256, 512, 256, 512, 512],\n\t\toutmaps=[256, 512, 256, 512, 256, 512, 256, 512, 512, 1024],\n\t\tsizeconv=[1, 3, 1, 3, 1, 3, 1, 3, 1, 3], strideconv=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n\t\tinitscheme=initscheme, actInPlace=actInplace\n\t)\n\tnet.extend(block3)\n\n\tblock4 = block(\n\t\tidx=[""17"", ""18"", ""19"", ""20"", ""21"", ""22"", ""23"", ""24""],\n\t\tinmaps=[1024, 512, 1024, 512, 1024, 1024, 1024, 1024],\n\t\toutmaps=[512, 1024, 512, 1024, 1024, 1024, 1024, 1024],\n\t\tsizeconv=[1, 3, 1, 3, 3, 3, 3, 3], strideconv=[1, 1, 1, 1, 1, 2, 1, 1],\n\t\tinitscheme=initscheme, actInPlace=actInplace, addMaxpool=False\n\t)\n\tnet.extend(block4)\n\n\tnet.append(Flatten())\n\tinsize = int(np.prod(net.dataShapeFrom((1, 3, 448, 448))))\n\n\tnet.append(Linear(insize, 512, initscheme=initscheme, name=""fc25""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""fc_relu24""))\n\n\tnet.append(Linear(512, 4096, initscheme=initscheme, name=""fc26""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""fc_relu25""))\n\n\tnet.append(Linear(4096, numOutput, initscheme=initscheme, name=""fc27""))\n\tnet.append(SoftMax())\n\n\tif modelpath is not None:\n\t\tnet.load(modelpath)\n\n\treturn net\n\n\ndef unittest():\n\tdata = gpuarray.to_gpu(np.zeros((1, 3, 448, 448), dtype=np.float32))\n\n\tyolo = loadMiniYolo(None, numOutput=1470, initscheme=""gaussian"")\n\tyolo(data)\n\n\tdel yolo\n\tmemPool.freeHeld()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Models/Nets/NiN.py,1,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\n\nfrom PuzzleLib.Containers.Sequential import Sequential\n\nfrom PuzzleLib.Modules.Conv2D import Conv2D\nfrom PuzzleLib.Modules.Activation import Activation, relu\nfrom PuzzleLib.Modules.MaxPool2D import MaxPool2D\nfrom PuzzleLib.Modules.AvgPool2D import AvgPool2D\nfrom PuzzleLib.Modules.Flatten import Flatten\nfrom PuzzleLib.Modules.SoftMax import SoftMax\n\n\ndef loadNiNImageNet(modelpath, poolmode=""max"", actInplace=False, initscheme=""none"", name=""CaffeNet""):\n\tif poolmode == ""avg"":\n\t\tpool = AvgPool2D\n\telif poolmode == ""max"":\n\t\tpool = MaxPool2D\n\telse:\n\t\traise ValueError(""Unsupported pool mode"")\n\n\tnet = Sequential(name=name)\n\n\tnet.append(Conv2D(3, 96, 11, stride=4, initscheme=initscheme, name=""conv1""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu0""))\n\tnet.append(Conv2D(96, 96, 1, stride=1, initscheme=initscheme, name=""cccp1""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu1""))\n\tnet.append(Conv2D(96, 96, 1, stride=1, initscheme=initscheme, name=""cccp2""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu2""))\n\tnet.append(pool(3, 2, name=""pool1""))\n\n\tnet.append(Conv2D(96, 256, 5, stride=1, pad=2, initscheme=initscheme, name=""conv2""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu3""))\n\tnet.append(Conv2D(256, 256, 1, stride=1, initscheme=initscheme, name=""cccp3""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu4""))\n\tnet.append(Conv2D(256, 256, 1, stride=1, initscheme=initscheme, name=""cccp4""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu5""))\n\tnet.append(pool(3, 2, name=""pool2""))\n\n\tnet.append(Conv2D(256, 384, 3, stride=1, pad=1, initscheme=initscheme, name=""conv3""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu6""))\n\tnet.append(Conv2D(384, 384, 1, stride=1, initscheme=initscheme, name=""cccp5""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu7""))\n\tnet.append(Conv2D(384, 384, 1, stride=1, initscheme=initscheme, name=""cccp6""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu8""))\n\tnet.append(pool(3, 2, name=""pool3""))\n\n\tnet.append(Conv2D(384, 1024, 3, stride=1, pad=1, initscheme=initscheme, name=""conv4-1024""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu9""))\n\tnet.append(Conv2D(1024, 1024, 1, stride=1, initscheme=initscheme, name=""cccp7-1024""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu10""))\n\tnet.append(Conv2D(1024, 1000, 1, stride=1, initscheme=initscheme, name=""cccp8-1024""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu11""))\n\tnet.append(AvgPool2D(5, 1, name=""pool4""))\n\n\tnet.append(Flatten())\n\tnet.append(SoftMax())\n\n\tif modelpath is not None:\n\t\tnet.load(modelpath)\n\n\treturn net\n\n\ndef unittest():\n\tnin = loadNiNImageNet(None, initscheme=""gaussian"")\n\n\tdata = gpuarray.to_gpu(np.random.randn(1, 3, 224, 224).astype(np.float32))\n\tnin(data)\n\n\tdel nin\n\tmemPool.freeHeld()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Models/Nets/ResNet.py,1,"b'import string\n\nimport numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\n\nfrom PuzzleLib.Containers.Sequential import Sequential\nfrom PuzzleLib.Containers.Parallel import Parallel\n\nfrom PuzzleLib.Modules.Conv2D import Conv2D\nfrom PuzzleLib.Modules.BatchNorm2D import BatchNorm2D\nfrom PuzzleLib.Modules.Activation import Activation, relu\nfrom PuzzleLib.Modules.Identity import Identity\nfrom PuzzleLib.Modules.Replicate import Replicate\nfrom PuzzleLib.Modules.Add import Add\nfrom PuzzleLib.Modules.MaxPool2D import MaxPool2D\nfrom PuzzleLib.Modules.AvgPool2D import AvgPool2D\nfrom PuzzleLib.Modules.Flatten import Flatten\nfrom PuzzleLib.Modules.Linear import Linear\nfrom PuzzleLib.Modules.SoftMax import SoftMax\n\n\ndef residMiniBlock(inmaps, outmaps, size, stride, pad, blockname, mininame, addAct, actInplace, bnInplace, initscheme):\n\tblock = Sequential()\n\n\tblock.append(Conv2D(\n\t\tinmaps, outmaps, size, stride=stride, pad=pad, useBias=False, initscheme=initscheme,\n\t\tname=""res%s_branch%s"" % (blockname, mininame)\n\t))\n\tblock.append(BatchNorm2D(outmaps, name=""bn%s_branch%s"" % (blockname, mininame), inplace=bnInplace))\n\n\tif addAct:\n\t\tblock.append(Activation(relu, inplace=actInplace, name=""res%s_branch%s_relu"" % (blockname, mininame)))\n\n\treturn block\n\n\ndef residBlock(inmaps, hmaps, stride, blockname, convShortcut, actInplace, bnInplace, initscheme):\n\tblock = Sequential()\n\n\tbranch = Sequential()\n\tbranch.extend(residMiniBlock(inmaps, hmaps, 1, stride, 0, blockname, ""2a"", True, actInplace, bnInplace, initscheme))\n\tbranch.extend(residMiniBlock(hmaps, hmaps, 3, 1, 1, blockname, ""2b"", True, actInplace, bnInplace, initscheme))\n\tbranch.extend(residMiniBlock(hmaps, 4 * hmaps, 1, 1, 0, blockname, ""2c"", False, actInplace, bnInplace, initscheme))\n\n\tshortcut = Sequential()\n\tif convShortcut:\n\t\tshortcut.extend(residMiniBlock(\n\t\t\tinmaps, 4 * hmaps, 1, stride, 0, blockname, ""1"", False, actInplace, bnInplace, initscheme\n\t\t))\n\telse:\n\t\tshortcut.append(Identity())\n\n\tblock.append(Replicate(2))\n\tblock.append(Parallel().append(branch).append(shortcut))\n\n\tblock.append(Add())\n\tblock.append(Activation(relu, inplace=actInplace))\n\n\treturn block\n\n\ndef loadResNet(modelpath, layers, actInplace=False, bnInplace=False, initscheme=""none"", name=None):\n\tif layers == ""50"":\n\t\tif name is None:\n\t\t\tname = ""ResNet-50""\n\n\t\tlevel3names = [""3%s"" % alpha for alpha in string.ascii_lowercase[1:4]]\n\t\tlevel4names = [""4%s"" % alpha for alpha in string.ascii_lowercase[1:6]]\n\n\telif layers == ""101"":\n\t\tif name is None:\n\t\t\tname = ""ResNet-101""\n\n\t\tlevel3names = [""3b%s"" % num for num in range(1, 4)]\n\t\tlevel4names = [""4b%s"" % num for num in range(1, 23)]\n\n\telif layers == ""152"":\n\t\tif name is None:\n\t\t\tname = ""ResNet-152""\n\n\t\tlevel3names = [""3b%s"" % num for num in range(1, 8)]\n\t\tlevel4names = [""4b%s"" % num for num in range(1, 36)]\n\n\telse:\n\t\traise ValueError(""Unsupported ResNet layers mode"")\n\n\tnet = Sequential(name=name)\n\n\tnet.append(Conv2D(3, 64, 7, stride=2, pad=3, name=""conv1"", initscheme=initscheme, useBias=False))\n\tnet.append(BatchNorm2D(64, name=""bn_conv1"", inplace=bnInplace))\n\tnet.append(Activation(relu, inplace=actInplace, name=""conv1_relu""))\n\tnet.append(MaxPool2D(3, 2, name=""pool1""))\n\n\tnet.extend(residBlock(64, 64, 1, ""2a"", True, actInplace, bnInplace, initscheme))\n\tnet.extend(residBlock(256, 64, 1, ""2b"", False, actInplace, bnInplace, initscheme))\n\tnet.extend(residBlock(256, 64, 1, ""2c"", False, actInplace, bnInplace, initscheme))\n\n\tnet.extend(residBlock(256, 128, 2, ""3a"", True, actInplace, bnInplace, initscheme))\n\n\tfor name in level3names:\n\t\tnet.extend(residBlock(512, 128, 1, name, False, actInplace, bnInplace, initscheme))\n\n\tnet.extend(residBlock(512, 256, 2, ""4a"", True, actInplace, bnInplace, initscheme))\n\n\tfor name in level4names:\n\t\tnet.extend(residBlock(1024, 256, 1, name, False, actInplace, bnInplace, initscheme))\n\n\tnet.extend(residBlock(1024, 512, 2, ""5a"", True, actInplace, bnInplace, initscheme))\n\tnet.extend(residBlock(2048, 512, 1, ""5b"", False, actInplace, bnInplace, initscheme))\n\tnet.extend(residBlock(2048, 512, 1, ""5c"", False, actInplace, bnInplace, initscheme))\n\n\tnet.append(AvgPool2D(7, 1))\n\tnet.append(Flatten())\n\tnet.append(Linear(2048, 1000, initscheme=initscheme, name=""fc1000""))\n\tnet.append(SoftMax())\n\n\tif modelpath is not None:\n\t\tnet.load(modelpath, assumeUniqueNames=True)\n\n\treturn net\n\n\ndef unittest():\n\tdata = gpuarray.to_gpu(np.random.randn(1, 3, 224, 224).astype(np.float32))\n\n\tres = loadResNet(None, layers=""50"", initscheme=""gaussian"")\n\tres(data)\n\n\tdel res\n\tmemPool.freeHeld()\n\n\tres = loadResNet(None, layers=""101"", initscheme=""gaussian"")\n\tres(data)\n\n\tdel res\n\tmemPool.freeHeld()\n\n\tres = loadResNet(None, layers=""152"", initscheme=""gaussian"")\n\tres(data)\n\n\tdel res\n\tmemPool.freeHeld()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Models/Nets/SentiNet.py,2,"b'import time\n\nimport numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\n\nfrom PuzzleLib.Containers.Sequential import Sequential\nfrom PuzzleLib.Containers.Parallel import Parallel\n\nfrom PuzzleLib.Modules.Embedder import Embedder\nfrom PuzzleLib.Modules.Reshape import Reshape\nfrom PuzzleLib.Modules.Replicate import Replicate\nfrom PuzzleLib.Modules.Conv2D import Conv2D\nfrom PuzzleLib.Modules.MaxPool2D import MaxPool2D\nfrom PuzzleLib.Modules.Concat import Concat\nfrom PuzzleLib.Modules.Activation import Activation, relu\nfrom PuzzleLib.Modules.Dropout import Dropout\nfrom PuzzleLib.Modules.Linear import Linear\n\n\ndef buildBranch(fHeight, sentlength, branchMaps, embsize):\n\tseq = Sequential()\n\n\tseq.append(Conv2D(1, outmaps=branchMaps, size=(fHeight, embsize)))\n\tseq.append(MaxPool2D(size=(sentlength - fHeight + 1, 1)))\n\tseq.append(Reshape((-1, branchMaps)))\n\n\treturn seq\n\n\ndef buildNet(vocabulary, branches, w2v, sentlength, embsize, wscale, dim=2, branchMaps=100, name=""sentinet""):\n\tdef onVocabulary(W):\n\t\tW[0] = np.zeros((1, embsize), dtype=np.float32)\n\n\t\tarrayPOS = [\n\t\t\t"""", ""_S"", ""_A"", ""_V"", ""_UNKN"", ""_ADJ"", ""_ADV"", ""_INTJ"", ""_NOUN"", ""_PROPN"", ""_VERB"", ""_ADP"",\n\t\t\t""_AUX"", ""_CCONJ"", ""_DET"", ""_NUM"", ""_PART"", ""_PRON"", ""_SCONJ"", ""_SUM"", ""_X""\n\t\t]\n\n\t\ttmpPOS = []\n\t\tif not w2v:\n\t\t\treturn\n\n\t\tfor word in vocabulary:\n\t\t\tfor pos in tmpPOS:\n\t\t\t\tif (word + pos) in w2v.vocab:\n\t\t\t\t\tW[vocabulary[word]] = w2v[word + pos]\n\t\t\t\t\tbreak\n\n\t\t\tfor i, pos in enumerate(arrayPOS):\n\t\t\t\tif (word + pos) in w2v.vocab:\n\t\t\t\t\ttmpPOS.append(pos)\n\t\t\t\t\tW[vocabulary[word]] = w2v[word + pos]\n\t\t\t\t\tdel arrayPOS[i]\n\t\t\t\t\tbreak\n\n\tnet = Sequential(name)\n\tnet.setAttr(""timestamp"", int(time.time()))\n\n\tnet.append(Embedder(\n\t\tvocabulary, sentlength, embsize, wscale=wscale, onVocabulary=onVocabulary, learnable=True, name=""embedder""\n\t))\n\n\tnet.append(Reshape((-1, 1, sentlength, embsize)))\n\n\tbranchNum = len(branches)\n\tnet.append(Replicate(times=branchNum))\n\n\tpar = Parallel()\n\n\tfor branchFilterSize in branches:\n\t\tpar.append(buildBranch(branchFilterSize, sentlength, branchMaps, embsize))\n\n\tnet.append(par)\n\tnet.append(Concat(axis=1))\n\tnet.append(Activation(relu))\n\tnet.append(Dropout(p=0.5))\n\n\tnet.append(Linear(branchNum * branchMaps, dim))\n\n\treturn net\n\n\ndef unittest():\n\tvocabsize = 1000\n\tsentlength, embsize = 100, 128\n\n\tdata = gpuarray.to_gpu(np.random.randint(0, vocabsize, (1, sentlength), dtype=np.int32))\n\n\tsenti = buildNet(vocabsize, (3, 5, 7), None, sentlength, embsize, 1.0)\n\tsenti(data)\n\n\tdel senti\n\tmemPool.freeHeld()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Models/Nets/UNet.py,1,"b'import numpy as np\n\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Containers import Parallel, Sequential\nfrom PuzzleLib.Modules import Replicate, Conv2D, MaxPool2D, Activation, relu, sigmoid, Deconv2D, Concat, Identity, \\\n\tDropout\n\n\ndef blockA(blockId, actInplace, initscheme):\n\tassert isinstance(blockId, int)\n\tassert 1 <= blockId <= 5\n\n\tinmaps = 1 if blockId == 1 else 2**(4 + blockId)\n\toutmaps = 2**(5 + blockId)\n\n\tblock = Sequential(name=""block_%d"" % blockId)\n\tif blockId > 1:\n\t\tblock.append(MaxPool2D(size=2, stride=2, name=""pool%d"" % (blockId - 1, )))\n\n\tblock.append(Conv2D(inmaps, outmaps, 3, pad=1, initscheme=initscheme, name=""conv_%d_1"" % blockId))\n\tblock.append(Activation(relu, inplace=actInplace, name=""relu%d"" % (2 * blockId - 1, )))\n\n\tblock.append(Conv2D(outmaps, outmaps, 3, pad=1, initscheme=initscheme, name=""conv_%d_2"" % blockId))\n\tblock.append(Activation(relu, inplace=actInplace, name=""relu%d"" % (2 * blockId, )))\n\n\tif blockId >= 4:\n\t\tblock.append(Dropout(name=""drop%d"" % blockId))\n\n\tif blockId == 5:\n\t\tblock.append(Deconv2D(1024, 512, size=2, stride=2, useBias=False, initscheme=initscheme, name=""upscore1""))\n\t\tblock.append(Activation(relu, inplace=actInplace, name=""relu11""))\n\n\treturn block\n\n\ndef shortcut(blockId):\n\tassert isinstance(blockId, int)\n\tassert blockId < 6\n\n\treturn Sequential(name=""shortcut_%d"" % blockId).append(Identity())\n\n\ndef blockB(blockId, actInplace, initscheme):\n\tassert type(blockId) is int\n\tassert 6 <= blockId <= 9\n\n\tinmaps = 2 ** (16 - blockId)\n\toutmaps = inmaps // 2\n\treluId = 12 + (blockId - 6) * 3\n\n\tblock = Sequential(name=""block_%d"" % blockId)\n\n\tblock.append(Conv2D(inmaps, outmaps, 3, pad=1, initscheme=initscheme, name=""conv_%d_1"" % blockId))\n\tblock.append(Activation(relu, inplace=actInplace, name=""relu%d"" % reluId))\n\n\tblock.append(Conv2D(outmaps, outmaps, 3, pad=1, initscheme=initscheme, name=""conv_%d_2"" % blockId))\n\tblock.append(Activation(relu, inplace=actInplace, name=""relu%d"" % (reluId + 1, )))\n\n\tif blockId < 9:\n\t\tblock.append(Deconv2D(\n\t\t\toutmaps, outmaps // 2, 2, stride=2, useBias=False, initscheme=initscheme, name=""upscore%d"" % (blockId - 4)\n\t\t))\n\n\t\tblock.append(Conv2D(\n\t\t\toutmaps // 2, outmaps // 2, size=3, pad=1, initscheme=initscheme, name=""conv_%d_3"" % blockId\n\t\t))\n\t\tblock.append(Activation(relu, inplace=actInplace, name=""relu%d"" % (reluId + 2)))\n\n\telse:\n\t\tblock.append(Conv2D(64, 1, 1, initscheme=initscheme, name=""score""))\n\t\tblock.append(Activation(sigmoid, inplace=actInplace))\n\n\treturn block\n\n\ndef loadUNet(modelpath, actInplace=False, initscheme=""none""):\n\tnet = Sequential(name=""unet"")\n\n\tblocksA, blocksB, shortcuts = [None], [None] * 6, [None]\n\n\tfor blockId in range(1, 6):\n\t\tblocksA.append(blockA(blockId, actInplace, initscheme))\n\t\tshortcuts.append(shortcut(blockId))\n\n\tfor blockId in range(6, 10):\n\t\tblocksB.append(blockB(blockId, actInplace, initscheme))\n\n\tfor blockId in range(1, 5):\n\t\tblocksA[blockId].append(Replicate(2))\n\t\tblocksA[blockId].append(\n\t\t\tParallel(name=""fork_%d"" % blockId).append(blocksA[blockId + 1]).append(shortcuts[blockId + 1])\n\t\t)\n\n\tfor blockId in range(4, 0, -1):\n\t\tblocksA[blockId].append(Concat(axis=1, name=""concat%d"" % (5 - blockId, )))\n\t\tblocksA[blockId].extend(blocksB[10 - blockId])\n\n\tnet.extend(blocksA[1])\n\n\tif modelpath is not None:\n\t\tnet.load(modelpath)\n\n\treturn net\n\n\ndef unittest():\n\tinshape = (1, 1, 256, 256)\n\tunet = loadUNet(None, initscheme=""gaussian"")\n\n\tdata = gpuarray.to_gpu(np.random.randn(*inshape).astype(np.float32))\n\tassert unet(data).shape == inshape\n\n\tdel unet\n\tmemPool.freeHeld()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Models/Nets/VGG.py,2,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\n\nfrom PuzzleLib.Containers.Sequential import Sequential\n\nfrom PuzzleLib.Modules.Conv2D import Conv2D\nfrom PuzzleLib.Modules.Activation import Activation, relu\nfrom PuzzleLib.Modules.AvgPool2D import AvgPool2D\nfrom PuzzleLib.Modules.MaxPool2D import MaxPool2D\nfrom PuzzleLib.Modules.Flatten import Flatten\nfrom PuzzleLib.Modules.Linear import Linear\nfrom PuzzleLib.Modules.SoftMax import SoftMax\n\n\ndef loadVGG(modelpath, layers, poolmode=""max"", initscheme=""none"", withLinear=True, actInplace=False, name=None):\n\tif poolmode == ""avg"":\n\t\tpool = AvgPool2D\n\telif poolmode == ""max"":\n\t\tpool = MaxPool2D\n\telse:\n\t\traise ValueError(""Unsupported pool mode"")\n\n\tif layers not in {""11"", ""16"", ""19""}:\n\t\traise ValueError(""Unsupported VGG layers mode"")\n\n\tif name is None and layers == ""11"":\n\t\tname = ""VGG_ILSVRC_11_layers""\n\telif name is None and layers == ""16"":\n\t\tname = ""VGG_ILSVRC_16_layers""\n\telif name is None and layers == ""19"":\n\t\tname = ""VGG_ILSVRC_19_layers""\n\n\tlayers = int(layers)\n\n\tnet = Sequential(name=name)\n\n\tnet.append(Conv2D(3, 64, 3, pad=1, initscheme=initscheme, name=""conv1_1""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu1_1""))\n\n\tif layers > 11:\n\t\tnet.append(Conv2D(64, 64, 3, pad=1, initscheme=initscheme, name=""conv1_2""))\n\t\tnet.append(Activation(relu, inplace=actInplace, name=""relu1_2""))\n\n\tnet.append(pool(2, 2, name=""pool1""))\n\n\tnet.append(Conv2D(64, 128, 3, pad=1, initscheme=initscheme, name=""conv2_1""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu2_1""))\n\n\tif layers > 11:\n\t\tnet.append(Conv2D(128, 128, 3, pad=1, initscheme=initscheme, name=""conv2_2""))\n\t\tnet.append(Activation(relu, inplace=actInplace, name=""relu2_2""))\n\n\tnet.append(pool(2, 2, name=""pool2""))\n\n\tnet.append(Conv2D(128, 256, 3, pad=1, initscheme=initscheme, name=""conv3_1""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu3_1""))\n\tnet.append(Conv2D(256, 256, 3, pad=1, initscheme=initscheme, name=""conv3_2""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu3_2""))\n\n\tif layers > 11:\n\t\tnet.append(Conv2D(256, 256, 3, pad=1, initscheme=initscheme, name=""conv3_3""))\n\t\tnet.append(Activation(relu, inplace=actInplace, name=""relu3_3""))\n\n\tif layers > 16:\n\t\tnet.append(Conv2D(256, 256, 3, pad=1, initscheme=initscheme, name=""conv3_4""))\n\t\tnet.append(Activation(relu, inplace=actInplace, name=""relu3_4""))\n\n\tnet.append(pool(2, 2, name=""pool3""))\n\n\tnet.append(Conv2D(256, 512, 3, pad=1, initscheme=initscheme, name=""conv4_1""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu4_1""))\n\tnet.append(Conv2D(512, 512, 3, pad=1, initscheme=initscheme, name=""conv4_2""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu4_2""))\n\n\tif layers > 11:\n\t\tnet.append(Conv2D(512, 512, 3, pad=1, initscheme=initscheme, name=""conv4_3""))\n\t\tnet.append(Activation(relu, inplace=actInplace, name=""relu4_3""))\n\n\tif layers > 16:\n\t\tnet.append(Conv2D(512, 512, 3, pad=1, initscheme=initscheme, name=""conv4_4""))\n\t\tnet.append(Activation(relu, inplace=actInplace, name=""relu4_4""))\n\n\tnet.append(pool(2, 2, name=""pool4""))\n\n\tnet.append(Conv2D(512, 512, 3, pad=1, initscheme=initscheme, name=""conv5_1""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu5_1""))\n\tnet.append(Conv2D(512, 512, 3, pad=1, initscheme=initscheme, name=""conv5_2""))\n\tnet.append(Activation(relu, inplace=actInplace, name=""relu5_2""))\n\n\tif layers > 11:\n\t\tnet.append(Conv2D(512, 512, 3, pad=1, initscheme=initscheme, name=""conv5_3""))\n\t\tnet.append(Activation(relu, inplace=actInplace, name=""relu5_3""))\n\n\tif layers > 16:\n\t\tnet.append(Conv2D(512, 512, 3, pad=1, initscheme=initscheme, name=""conv5_4""))\n\t\tnet.append(Activation(relu, inplace=actInplace, name=""relu5_4""))\n\n\tnet.append(pool(2, 2, name=""pool5""))\n\n\tif withLinear:\n\t\tnet.append(Flatten())\n\n\t\tinsize = int(np.prod(net.dataShapeFrom((1, 3, 224, 224))))\n\n\t\tnet.append(Linear(insize, 4096, initscheme=initscheme, name=""fc6""))\n\t\tnet.append(Activation(relu, inplace=actInplace, name=""relu6""))\n\n\t\tnet.append(Linear(4096, 4096, initscheme=initscheme, name=""fc7""))\n\t\tnet.append(Activation(relu, inplace=actInplace, name=""relu7""))\n\n\t\tnet.append(Linear(4096, 1000, initscheme=initscheme, name=""fc8""))\n\t\tnet.append(SoftMax())\n\n\tif modelpath is not None:\n\t\tnet.load(modelpath)\n\n\treturn net\n\n\ndef unittest():\n\tdata = gpuarray.to_gpu(np.random.randn(1, 3, 224, 224).astype(np.float32))\n\n\tvgg11 = loadVGG(None, layers=""11"", initscheme=""gaussian"")\n\tvgg11(data)\n\n\tdel vgg11\n\tmemPool.freeHeld()\n\n\tvgg16 = loadVGG(None, layers=""16"", initscheme=""gaussian"")\n\tvgg16(data)\n\n\tdel vgg16\n\tmemPool.freeHeld()\n\n\tvgg19 = loadVGG(None, layers=""19"", initscheme=""gaussian"")\n\tvgg19(data)\n\n\tdel vgg19\n\tmemPool.freeHeld()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Models/Nets/WaveToLetter.py,1,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Backend.Utils import memoryPool as memPool\n\nfrom PuzzleLib.Containers.Sequential import Sequential\n\nfrom PuzzleLib.Modules.Conv1D import Conv1D\nfrom PuzzleLib.Modules.BatchNorm1D import BatchNorm1D\nfrom PuzzleLib.Modules.Dropout import Dropout\nfrom PuzzleLib.Modules.Activation import Activation, clip\nfrom PuzzleLib.Modules.Pad1D import Pad1D, PadMode\n\n\ndef convBlock(inmaps, outmaps, size, stride, pad, dropout, initscheme, dilation=1, bnAct=True, name=None):\n\tblock = Sequential()\n\n\tif pad > 0:\n\t\tblock.append(Pad1D(pad, mode=PadMode.reflect))\n\n\tblock.append(\n\t\tConv1D(\n\t\t\tinmaps, outmaps, size=size, stride=stride, pad=0, dilation=dilation, useBias=True, initscheme=initscheme,\n\t\t\tname=""%s_conv"" % name\n\t\t)\n\t)\n\n\tif bnAct:\n\t\tblock.append(BatchNorm1D(outmaps, epsilon=0.001, name=""%s_bn"" % name))\n\t\tblock.append(Activation(clip, args=(0.0, 20.0)))\n\n\tif dropout > 0.0:\n\t\tblock.append(Dropout(p=dropout))\n\n\treturn block\n\n\ndef loadW2L(modelpath, inmaps, nlabels, initscheme=None, name=""w2l""):\n\tnet = Sequential(name=name)\n\n\tnet.extend(convBlock(inmaps, 256, size=11, stride=2, pad=5, dropout=0.2, initscheme=initscheme, name=""conv1d_0""))\n\n\tnet.extend(convBlock(256, 256, size=11, stride=1, pad=5, dropout=0.2, initscheme=initscheme, name=""conv1d_1""))\n\tnet.extend(convBlock(256, 256, size=11, stride=1, pad=5, dropout=0.2, initscheme=initscheme, name=""conv1d_2""))\n\tnet.extend(convBlock(256, 256, size=11, stride=1, pad=5, dropout=0.2, initscheme=initscheme, name=""conv1d_3""))\n\n\tnet.extend(convBlock(256, 384, size=13, stride=1, pad=6, dropout=0.2, initscheme=initscheme, name=""conv1d_4""))\n\tnet.extend(convBlock(384, 384, size=13, stride=1, pad=6, dropout=0.2, initscheme=initscheme, name=""conv1d_5""))\n\tnet.extend(convBlock(384, 384, size=13, stride=1, pad=6, dropout=0.2, initscheme=initscheme, name=""conv1d_6""))\n\n\tnet.extend(convBlock(384, 512, size=17, stride=1, pad=8, dropout=0.2, initscheme=initscheme, name=""conv1d_7""))\n\tnet.extend(convBlock(512, 512, size=17, stride=1, pad=8, dropout=0.2, initscheme=initscheme, name=""conv1d_8""))\n\tnet.extend(convBlock(512, 512, size=17, stride=1, pad=8, dropout=0.2, initscheme=initscheme, name=""conv1d_9""))\n\n\tnet.extend(convBlock(512, 640, size=21, stride=1, pad=10, dropout=0.3, initscheme=initscheme, name=""conv1d_10""))\n\tnet.extend(convBlock(640, 640, size=21, stride=1, pad=10, dropout=0.3, initscheme=initscheme, name=""conv1d_11""))\n\tnet.extend(convBlock(640, 640, size=21, stride=1, pad=10, dropout=0.3, initscheme=initscheme, name=""conv1d_12""))\n\n\tnet.extend(convBlock(640, 768, size=25, stride=1, pad=12, dropout=0.3, initscheme=initscheme, name=""conv1d_13""))\n\tnet.extend(convBlock(768, 768, size=25, stride=1, pad=12, dropout=0.3, initscheme=initscheme, name=""conv1d_14""))\n\tnet.extend(convBlock(768, 768, size=25, stride=1, pad=12, dropout=0.3, initscheme=initscheme, name=""conv1d_15""))\n\n\tnet.extend(convBlock(\n\t\t768, 896, size=29, stride=1, pad=28, dropout=0.4, initscheme=initscheme, dilation=2, name=""conv1d_16""\n\t))\n\n\tnet.extend(convBlock(896, 1024, size=1, stride=1, pad=0, dropout=0.4, initscheme=initscheme, name=""conv1d_17""))\n\n\tnet.extend(convBlock(\n\t\t1024, nlabels, size=1, stride=1, pad=0, dropout=0.0, initscheme=initscheme, bnAct=False, name=""conv1d_18""\n\t))\n\n\tif modelpath is not None:\n\t\tnet.load(modelpath)\n\n\treturn net\n\n\ndef unittest():\n\tinmaps, nlabels = 161, 29\n\tw2l = loadW2L(None, inmaps=inmaps, nlabels=29)\n\n\tshape = (16, inmaps, 200)\n\n\tdata = gpuarray.to_gpu(np.random.randn(*shape).astype(np.float32))\n\tw2l(data)\n\n\tdel w2l\n\tmemPool.freeHeld()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Models/Nets/__init__.py,0,"b'from PuzzleLib.Models.Nets.Inception import loadInceptionBN, loadInceptionV3\nfrom PuzzleLib.Models.Nets.MiniYolo import loadMiniYolo\nfrom PuzzleLib.Models.Nets.NiN import loadNiNImageNet\nfrom PuzzleLib.Models.Nets.ResNet import loadResNet\nfrom PuzzleLib.Models.Nets.UNet import loadUNet\nfrom PuzzleLib.Models.Nets.VGG import loadVGG\nfrom PuzzleLib.Models.Nets.WaveToLetter import loadW2L\n'"
Compiler/Codegen/Malloc/Generate.py,0,"b'import os\nfrom string import Template\n\nfrom PuzzleLib.Compiler.Codegen.Tree.Generate import generateTree\nfrom PuzzleLib.Compiler.Toolchain import createTemplateNames, writeTemplates, buildTemplateTest\n\n\ndef generateMalloc(name=None, filename=None):\n\ttreename = generateTree(\n\t\tname=""AllocTree"", K=""VoidPtr"", V=""Allocation"",\n\t\theaderPreambule=\n""""""\ntypedef void *VoidPtr;\n\n\ntypedef struct Allocation\n{\n\tsize_t size;\n\tconst char *file;\n\tint line;\n}\nAllocation;\n"""""",\n\t\tfilename=os.path.join(os.path.dirname(filename), ""AllocTree"")\n\t)\n\n\tname = ""TraceMalloc"" if name is None else name\n\n\tfilename = name if filename is None else filename\n\theadername, bodyname = createTemplateNames(filename)\n\n\tdirname = os.path.dirname(__file__)\n\n\twith open(os.path.join(dirname, ""TMalloc.h""), mode=""r"", encoding=""utf-8"") as f:\n\t\theader = Template(f.read()).substitute(NAME=name)\n\n\twith open(os.path.join(dirname, ""TMalloc.c""), mode=""r"", encoding=""utf-8"") as f:\n\t\tbody = Template(f.read()).substitute(HEADER_NAME=os.path.basename(headername), NAME=name)\n\n\twriteTemplates([\n\t\t(header, headername),\n\t\t(body, bodyname)\n\t])\n\n\treturn [bodyname, treename]\n\n\ndef unittest():\n\tTraceMalloc = buildTemplateTest(\n\t\tname=""TraceMalloc"", bindingName=""TMallocTest.c"", path=""../../TestData"", generator=generateMalloc,\n\t\tdefines=[""ENABLE_TRACE_MALLOC""]\n\t)\n\n\tptr = TraceMalloc.malloc(16)\n\n\tleaks = TraceMalloc.traceLeaks()\n\tassert len(leaks) == 1\n\n\tTraceMalloc.free(ptr)\n\n\tleaks = TraceMalloc.traceLeaks()\n\tassert len(leaks) == 0\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Compiler/Codegen/Map/Generate.py,0,"b'import os, random\nfrom string import Template\n\nfrom PuzzleLib.Compiler.Toolchain import createTemplateNames, writeTemplates, buildTemplateTest\n\n\ndef generateMap(name, K, V, hasher, compareKeys, borrowKey, borrowValue,\n\t\t\t\tdestructKey=""(void)"", destructValue=""(void)"", minLog2Capacity=4,\n\t\t\t\theaderPreambule=None, bodyPreambule=None, malloc=""malloc"", free=""free"", filename=None):\n\theaderPreambule = ""%s\\n\\n"" % headerPreambule if headerPreambule is not None else """"\n\tbodyPreambule = ""%s\\n\\n"" % bodyPreambule if bodyPreambule is not None else """"\n\n\tfilename = name if filename is None else filename\n\theadername, bodyname = createTemplateNames(filename)\n\n\tdirname = os.path.dirname(__file__)\n\theaderTmpl, bodyTmpl = os.path.join(dirname, ""TMap.h""), os.path.join(dirname, ""TMap.c"")\n\n\twith open(headerTmpl, mode=""r"", encoding=""utf-8"") as f:\n\t\theader = Template(f.read()).substitute(HEADER_PREAMBULE=headerPreambule, NAME=name, K=K, V=V)\n\n\twith open(bodyTmpl, mode=""r"", encoding=""utf-8"") as f:\n\t\tbody = Template(f.read()).substitute(\n\t\t\tHEADER_NAME=os.path.basename(headername), BODY_PREAMBULE=bodyPreambule, NAME=name, K=K, V=V,\n\t\t\tMIN_LOG2_CAPACITY=minLog2Capacity, MALLOC=malloc, FREE=free,\n\t\t\tHASHER=hasher, COMPARE_KEYS=compareKeys, BORROW_KEY=borrowKey, BORROW_VALUE=borrowValue,\n\t\t\tDESTRUCT_KEY=destructKey, DESTRUCT_VALUE=destructValue\n\t\t)\n\n\twriteTemplates([\n\t\t(header, headername),\n\t\t(body, bodyname)\n\t])\n\n\treturn bodyname\n\n\ndef unittest():\n\tIntMap = buildTemplateTest(\n\t\tname=""IntMap"", bindingName=""TMapTest.c"", path=""../../TestData"", generator=generateMap, K=""int"", V=""int"",\n\t\thasher=""hashKey"", compareKeys=""compareKeys"", borrowKey=""(int)"", borrowValue=""(int)"",\n\t\tbodyPreambule=""""""\ninline static size_t hashKey(int key) { return key; }\ninline static bool compareKeys(int key1, int key2) { return key1 == key2; }\n"""""")\n\n\tsize = 1 << 16\n\n\tkeys, values = list(range(size)), list(range(size))\n\trandom.shuffle(keys)\n\trandom.shuffle(values)\n\n\tpymap = {k: v for k, v in zip(keys, values)}\n\n\tintmap = IntMap.IntMap()\n\n\tfor k, v in pymap.items():\n\t\tintmap[k] = v\n\n\tassert len(intmap) == size\n\n\tfor k in pymap.keys():\n\t\tassert intmap[k] == pymap[k]\n\n\tfor k in pymap.keys():\n\t\tdel intmap[k]\n\n\tassert len(intmap) == 0\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Compiler/Codegen/PyDefines/Generate.py,0,"b'import os\nfrom PuzzleLib.Compiler.Toolchain import copySource\n\n\ndef generatePyDefines(path):\n\tdirname = os.path.dirname(__file__)\n\tcopySource(os.path.join(dirname, ""PyDefines.h""), os.path.join(path, ""PyDefines.gen.h""))\n'"
Compiler/Codegen/Tree/Generate.py,0,"b'import os, random\nfrom string import Template\n\nfrom PuzzleLib.Compiler.Toolchain import createTemplateNames, writeTemplates, buildTemplateTest\n\n\ndef generateTree(name, K, V, headerPreambule=None, bodyPreambule=None, malloc=""malloc"", free=""free"", filename=None):\n\theaderPreambule = ""%s\\n\\n"" % headerPreambule if headerPreambule is not None else """"\n\tbodyPreambule = ""%s\\n\\n"" % bodyPreambule if bodyPreambule is not None else """"\n\n\tfilename = name if filename is None else filename\n\theadername, bodyname = createTemplateNames(filename)\n\n\tdirname = os.path.dirname(__file__)\n\theaderTmpl, bodyTmpl = os.path.join(dirname, ""TTree.h""), os.path.join(dirname, ""TTree.c"")\n\n\twith open(headerTmpl, mode=""r"", encoding=""utf-8"") as f:\n\t\theader = Template(f.read()).substitute(HEADER_PREAMBULE=headerPreambule, NAME=name, K=K, V=V)\n\n\twith open(bodyTmpl, mode=""r"", encoding=""utf-8"") as f:\n\t\tbody = Template(f.read()).substitute(\n\t\t\tHEADER_NAME=os.path.basename(headername), BODY_PREAMBULE=bodyPreambule, NAME=name, K=K, V=V,\n\t\t\tMALLOC=malloc, FREE=free\n\t\t)\n\n\twriteTemplates([\n\t\t(header, headername),\n\t\t(body, bodyname)\n\t])\n\n\treturn bodyname\n\n\ndef unittest():\n\tIntTree = buildTemplateTest(\n\t\tname=""IntTree"", bindingName=""TTreeTest.c"", path=""../../TestData"", generator=generateTree, K=""int"", V=""int""\n\t)\n\n\tsize = 1 << 16\n\n\tkeys, values = list(range(size)), list(range(size))\n\trandom.shuffle(keys)\n\trandom.shuffle(values)\n\n\tpytree = {k: v for k, v in zip(keys, values)}\n\n\tinttree = IntTree.IntTree()\n\n\tfor k, v in pytree.items():\n\t\tinttree[k] = v\n\n\tassert len(inttree) == size\n\tassert inttree.validate()\n\n\tfor k in pytree.keys():\n\t\tassert inttree[k] == pytree[k]\n\n\tfor k in pytree.keys():\n\t\tdel inttree[k]\n\n\tassert len(inttree) == 0\n\tassert inttree.validate()\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Compiler/Codegen/Vector/Generate.py,0,"b'import os, random\nfrom string import Template\n\nfrom PuzzleLib.Compiler.Toolchain import createTemplateNames, writeTemplates, buildTemplateTest\n\n\ndef generateVector(name, T, borrow=""(void)"", destruct=""(void)"", minCapacity=16,\n\t\t\t\t   headerPreambule=None, bodyPreambule=None, malloc=""malloc"", free=""free"", filename=None):\n\theaderPreambule = ""%s\\n\\n"" % headerPreambule if headerPreambule is not None else """"\n\tbodyPreambule = ""%s\\n\\n"" % bodyPreambule if bodyPreambule is not None else """"\n\n\tfilename = name if filename is None else filename\n\theadername, bodyname = createTemplateNames(filename)\n\n\tdirname = os.path.dirname(__file__)\n\theaderTmpl, bodyTmpl = os.path.join(dirname, ""TVector.h""), os.path.join(dirname, ""TVector.c"")\n\n\twith open(headerTmpl, mode=""r"", encoding=""utf-8"") as f:\n\t\theader = Template(f.read()).substitute(HEADER_PREAMBULE=headerPreambule, NAME=name, T=T)\n\n\twith open(bodyTmpl, mode=""r"", encoding=""utf-8"") as f:\n\t\tbody = Template(f.read()).substitute(\n\t\t\tHEADER_NAME=os.path.basename(headername), BODY_PREAMBULE=bodyPreambule, NAME=name, T=T,\n\t\t\tMIN_CAPACITY=minCapacity, MALLOC=malloc, FREE=free, BORROW=borrow, DESTRUCT=destruct\n\t\t)\n\n\twriteTemplates([\n\t\t(header, headername),\n\t\t(body, bodyname)\n\t])\n\n\treturn bodyname\n\n\ndef unittest():\n\tIntVector = buildTemplateTest(\n\t\tname=""IntVector"", bindingName=""TVectorTest.c"", path=""../../TestData"", generator=generateVector, T=""int""\n\t)\n\n\tsize = 1 << 16\n\n\tpyvec = list(range(size))\n\trandom.shuffle(pyvec)\n\n\tvector = IntVector.IntVector()\n\n\tfor i in pyvec:\n\t\tvector.append(i)\n\n\tassert len(vector) == size\n\n\tfor i in range(size):\n\t\tassert vector[i] == pyvec[i]\n\n\tfor i in reversed(pyvec):\n\t\tassert vector.pop() == i\n\n\tassert len(vector) == 0\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
Converter/OpenVINO/Source/Build.py,0,"b'import sys, os\nimport pybind11\n\nfrom PuzzleLib.Compiler.Toolchain import guessToolchain\n\n\ndef buildDriver():\n\tcc = prepareCompiler()\n\n\tdriver = ""../Driver%s"" % cc.pydext\n\tcc.build(driver, sourcefiles=""./Driver.cpp"").clearPath("".."")\n\n\treturn driver\n\n\ndef findLibraryPath():\n\tOPENVINO_PATH = os.environ.get(""OPENVINO_PATH"", None)\n\n\tif OPENVINO_PATH is None:\n\t\tif sys.platform == ""linux"":\n\t\t\tOPENVINO_PATH = os.path.expanduser(""~/intel/openvino"")\n\n\t\telif sys.platform == ""win32"":\n\t\t\traise OSError(""OpenVINO path needs to be specified in the system variables as OPENVINO_PATH"")\n\n\t\telse:\n\t\t\traise NotImplementedError(sys.platform)\n\n\treturn OPENVINO_PATH\n\n\ndef prepareCompiler():\n\tcc = guessToolchain(verbose=2).withOptimizationLevel(level=4, debuglevel=0).cppMode(True)\n\tOPENVINO_PATH = findLibraryPath()\n\n\tif sys.platform == ""linux"":\n\t\tcc.includeDirs.append(pybind11.get_include(user=True))\n\n\t\tcc.addLibrary(\n\t\t\t""openvino"",\n\t\t\t[os.path.join(OPENVINO_PATH, ""inference_engine/include"")],\n\t\t\t[os.path.join(OPENVINO_PATH, ""inference_engine/lib/intel64"")],\n\t\t\t[""inference_engine""]\n\t\t)\n\n\telif sys.platform == ""win32"":\n\t\tcc.addLibrary(\n\t\t\t""openvino"",\n\t\t\t[os.path.join(OPENVINO_PATH, ""inference_engine/include"")],\n\t\t\t[os.path.join(OPENVINO_PATH, ""inference_engine/lib/intel64/Release"")],\n\t\t\t[""inference_engine""]\n\t\t)\n\n\telse:\n\t\traise NotImplementedError(sys.platform)\n\n\treturn cc\n\n\ndef main():\n\treturn buildDriver()\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/OpenVINO/Tests/Common.py,1,"b'import numpy as np\n\nfrom PuzzleLib.Backend.Benchmarks import timeKernel\n\n\ndef scoreModels(net, engine, data, labels):\n\thostNetData = net(data).get()\n\thostEngineData = engine(data).get()\n\n\tassert np.allclose(hostNetData, hostEngineData, atol=1e-6)\n\n\tprintResults(hostNetData, labels, ""Net"")\n\tprintResults(hostEngineData, labels, ""Engine"")\n\n\ndef printResults(probs, labels, name):\n\tprobs = probs.flatten()\n\n\tidx = (-probs).argsort()[:5]\n\tprint(""%s top-5 predictions: "" % name)\n\n\tfor i in range(5):\n\t\tprint(""#%s %s (prob=%s)"" % (i, labels[idx[i]], probs[idx[i]]))\n\n\ndef benchModels(net, engine, data):\n\tnet.optimizeForShape(data.shape)\n\n\tnettime = timeKernel(net, args=(data, ), looplength=100, log=False, normalize=True)\n\tenginetime = timeKernel(engine, args=(data, ), looplength=100, log=False, normalize=True)\n\n\tprint(""Net    time: host=%.10f"" % nettime)\n\tprint(""Engine time: host=%.10f"" % enginetime)\n'"
Converter/OpenVINO/Tests/GraphTest.py,2,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nConfig.backend = Config.Backend.intel\nConfig.globalEvalMode = True\n\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Containers import Graph\nfrom PuzzleLib.Modules import Linear, Activation, relu, Add\n\nfrom PuzzleLib.Converter.OpenVINO.Tests.Common import benchModels\nfrom PuzzleLib.Converter.OpenVINO.BuildVINOEngine import buildVINOEngine\n\n\ndef main():\n\tbatchsize, insize = 16, 1000\n\n\tinNode = Linear(insize, 1000, name=""linear1"").node()\n\tnode = Activation(relu, name=""relu1"").node(inNode)\n\n\tnode1 = Linear(1000, 800, name=""linear2"").node(node)\n\tnode1 = Activation(relu, name=""relu2"").node(node1)\n\n\tnode2 = Linear(1000, 800, name=""linear3"").node(node)\n\tnode2 = Activation(relu, name=""relu3"").node(node2)\n\n\toutNode = Add(name=""add"").node(node1, node2)\n\n\tgraph = Graph(inputs=inNode, outputs=outNode, name=""graph"")\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, insize).astype(np.float32))\n\n\tengine = buildVINOEngine(graph, (batchsize, insize), savepath=""../TestData"")\n\n\toutdata = graph(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\tbenchModels(graph, engine, data)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/OpenVINO/Tests/ModulesTest.py,10,"b'import numpy as np\n\nfrom PuzzleLib import Config\n\nConfig.backend = Config.Backend.intel\nConfig.globalEvalMode = True\n\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Containers import Sequential, Parallel\n\nfrom PuzzleLib.Modules.BatchNorm import BatchNorm\nfrom PuzzleLib.Modules.Concat import Concat\nfrom PuzzleLib.Modules.MulAddConst import MulAddConst\nfrom PuzzleLib.Modules.Split import Split\nfrom PuzzleLib.Modules.SoftMax import SoftMax\nfrom PuzzleLib.Modules.Upsample2D import Upsample2D\n\nfrom PuzzleLib.Converter.OpenVINO.BuildVINOEngine import buildVINOEngine\n\n\ndef batchNormTest():\n\tbatchsize, size = 16, 10\n\n\tmod = BatchNorm(size, name=""bn"")\n\tmod.evalMode()\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, size).astype(np.float32))\n\n\tengine = buildVINOEngine(mod, data.shape, savepath=""../TestData"")\n\n\toutdata = mod(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\n\ndef concatTest():\n\tbatchsize, height, width = 4, 5, 8\n\tmaps1, maps2 = 3, 2\n\n\tmod = Concat(axis=1, name=""concat"")\n\tdata = [\n\t\tgpuarray.to_gpu(np.random.randn(batchsize, maps, height, width).astype(np.float32)) for maps in [maps1, maps2]\n\t]\n\n\tengine = buildVINOEngine(mod, [subdata.shape for subdata in data], savepath=""../TestData"")\n\n\toutdata = mod(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\n\ndef mulAddConstTest():\n\tbatchsize, maps, height, width = 4, 3, 5, 8\n\n\tmod = MulAddConst(a=1.5, b=-2.0, name=""muladd"")\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, height, width).astype(np.float32))\n\n\tengine = buildVINOEngine(mod, data.shape, savepath=""../TestData"")\n\n\toutdata = mod(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\n\ndef splitTest():\n\tbatchsize, maps, height, width = 2, 6, 4, 5\n\n\tnet = Sequential(name=""split"")\n\tnet.append(Split(axis=1, sections=(2, 4)))\n\tnet.append(Parallel().append(SoftMax()).append(SoftMax()))\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, height, width).astype(np.float32))\n\tengine = buildVINOEngine(net, data.shape, savepath=""../TestData"")\n\n\toutdata = net(data)\n\tenginedata = engine(data)\n\n\tassert all(np.allclose(outdat.get(), enginedat.get()) for outdat, enginedat in zip(outdata, enginedata))\n\n\ndef upsample2dTest():\n\tbatchsize, maps, height, width = 4, 3, 5, 8\n\n\tmod = Upsample2D(scale=2, name=""upsample"")\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, height, width).astype(np.float32))\n\n\tengine = buildVINOEngine(mod, data.shape, savepath=""../TestData"")\n\n\toutdata = mod(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\n\ndef main():\n\tbatchNormTest()\n\tconcatTest()\n\tmulAddConstTest()\n\tsplitTest()\n\tupsample2dTest()\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/OpenVINO/Tests/ResNet50Test.py,0,"b'from PuzzleLib import Config\n\nConfig.backend = Config.Backend.intel\nConfig.globalEvalMode = True\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Models.Nets.ResNet import loadResNet\n\nfrom PuzzleLib.Converter.Examples.Common import loadResNetSample, loadLabels\n\nfrom PuzzleLib.Converter.OpenVINO.Tests.Common import scoreModels, benchModels\nfrom PuzzleLib.Converter.OpenVINO.BuildVINOEngine import buildVINOEngine\n\n\ndef main():\n\tnet = loadResNet(modelpath=""../../TestData/ResNet-50-model.hdf"", layers=""50"")\n\n\tdata = gpuarray.to_gpu(loadResNetSample(net, ""../../TestData/tarantula.jpg""))\n\tlabels = loadLabels(synpath=""../../TestData/synsets.txt"", wordpath=""../../TestData/synset_words.txt"")\n\n\tengine = buildVINOEngine(net, inshape=data.shape, savepath=""../TestData"")\n\n\tscoreModels(net, engine, data, labels)\n\tbenchModels(net, engine, data)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/TensorRT/Source/Build.py,0,"b'import sys, os\n\nimport pybind11\n\nfrom PuzzleLib.Compiler.Toolchain import guessToolchain, guessNVCCToolchain\nfrom PuzzleLib.Compiler.BuildSystem import Rule, LinkRule, build\n\n\ndef buildDriver():\n\tcc, nvcc = prepareCompilers()\n\trules, linkrule = createRules(cc, nvcc)\n\n\tbuild(rules, linkrule)\n\tcc.clearPath("".."")\n\n\treturn linkrule.target\n\n\ndef findLibraryPath():\n\tTRT_PATH = os.environ.get(""TRT_PATH"", None)\n\n\tif TRT_PATH is None:\n\t\tif sys.platform == ""linux"":\n\t\t\tTRT_PATH = ""/usr/local/cuda""\n\n\t\telif sys.platform == ""win32"":\n\t\t\tTRT_PATH = os.environ[""CUDA_PATH""]\n\n\t\telse:\n\t\t\traise NotImplementedError(sys.platform)\n\n\treturn TRT_PATH\n\n\ndef prepareCompilers():\n\tcc = guessToolchain(verbose=2).withOptimizationLevel(level=4, debuglevel=0).cppMode(True)\n\tnvcc = guessNVCCToolchain(verbose=2).withOptimizationLevel(level=4, debuglevel=0)\n\n\tTRT_PATH = findLibraryPath()\n\n\tif sys.platform == ""linux"":\n\t\tcc.includeDirs.append(pybind11.get_include(user=True))\n\n\t\tcc.addLibrary(\n\t\t\t""tensorrt"",\n\t\t\t[os.path.join(TRT_PATH, ""include""), ""/usr/local/include/python%s.%s"" % sys.version_info[:2]],\n\t\t\t[os.path.join(TRT_PATH, ""lib64"")],\n\t\t\t[""cudart"", ""nvinfer"", ""nvinfer_plugin"", ""nvcaffe_parser"", ""nvonnxparser""]\n\t\t)\n\n\telif sys.platform == ""win32"":\n\t\tcc.addLibrary(\n\t\t\t""tensorrt"",\n\t\t\t[os.path.join(TRT_PATH, ""include"")],\n\t\t\t[os.path.join(TRT_PATH, ""lib/x64"")],\n\t\t\t[""cudart"", ""nvinfer"", ""nvinfer_plugin"", ""nvparsers"", ""nvonnxparser""]\n\t\t)\n\n\telse:\n\t\traise NotImplementedError(sys.platform)\n\n\treturn cc, nvcc\n\n\ndef createRules(cc, nvcc):\n\trules = [\n\t\tRule(target=""./PRelu%s"" % nvcc.oext, deps=[\n\t\t\t""./Plugins.h"",\n\t\t\t""./PRelu.h"",\n\t\t\t""./PRelu.cu""\n\t\t], toolchain=nvcc),\n\n\t\tRule(target=""./ReflectPad1D%s"" % nvcc.oext, deps=[\n\t\t\t""./Plugins.h"",\n\t\t\t""./ReflectPad1D.h"",\n\t\t\t""./ReflectPad1D.cu""\n\t\t], toolchain=nvcc),\n\n\t\tRule(target=""./Plugins%s"" % cc.oext, deps=[\n\t\t\t""./Plugins.h"",\n\t\t\t""./PRelu.h"",\n\t\t\t""./ReflectPad1D.h"",\n\t\t\t""./Plugins.cpp""\n\t\t], toolchain=cc),\n\n\t\tRule(target=""./Driver%s"" % cc.oext, deps=[\n\t\t\t""./Plugins.h"",\n\t\t\t""./Driver.cpp""\n\t\t], toolchain=cc)\n\t]\n\n\tlinkrule = LinkRule(target=""../Driver%s"" % cc.pydext, deps=rules, toolchain=cc)\n\treturn rules, linkrule\n\n\ndef main():\n\treturn buildDriver()\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/TensorRT/Tests/Common.py,1,"b'import numpy as np\n\nfrom PuzzleLib.Backend.Benchmarks import timeKernel\n\n\ndef scoreModels(net, engine, data, labels):\n\thostNetData = net(data).get()\n\thostEngineData = engine(data).get()\n\n\tassert np.allclose(hostNetData, hostEngineData)\n\n\tprintResults(hostNetData, labels, ""Net"")\n\tprintResults(hostEngineData, labels, ""Engine"")\n\n\ndef printResults(probs, labels, name):\n\tprobs = probs.flatten()\n\n\tidx = (-probs).argsort()[:5]\n\tprint(""%s top-5 predictions: "" % name)\n\n\tfor i in range(5):\n\t\tprint(""#%s %s (prob=%s)"" % (i, labels[idx[i]], probs[idx[i]]))\n\n\ndef benchModels(net, engine, data, lognames=None):\n\tnetlabel, enginelabel = (""Net   "", ""Engine"") if lognames is None else lognames\n\n\tnet.optimizeForShape(data.shape)\n\n\tnettime = timeKernel(net, args=(data, ), looplength=100, log=False, normalize=True)\n\tenginetime = timeKernel(engine, args=(data, ), looplength=100, log=False, normalize=True)\n\n\tprint(""%s time: device=%.10f host=%.10f"" % (netlabel, nettime[0], nettime[1]))\n\tprint(""%s time: device=%.10f host=%.10f"" % (enginelabel, enginetime[0], enginetime[1]))\n'"
Converter/TensorRT/Tests/GraphTest.py,2,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Containers import Graph\nfrom PuzzleLib.Modules import Linear, Activation, relu, Add\n\nfrom PuzzleLib.Converter.TensorRT.Tests.Common import benchModels\nfrom PuzzleLib.Converter.TensorRT.BuildRTEngine import buildRTEngine, DataType\n\n\ndef main():\n\tbatchsize, insize = 16, 1000\n\n\tinNode = Linear(insize, 1000, name=""linear1"").node()\n\tnode = Activation(relu, name=""relu1"").node(inNode)\n\n\tnode1 = Linear(1000, 800, name=""linear2"").node(node)\n\tnode1 = Activation(relu, name=""relu2"").node(node1)\n\n\tnode2 = Linear(1000, 800, name=""linear3"").node(node)\n\tnode2 = Activation(relu, name=""relu3"").node(node2)\n\n\toutNode = Add(name=""add"").node(node1, node2)\n\n\tgraph = Graph(inputs=inNode, outputs=outNode, name=""graph"")\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, insize).astype(np.float32))\n\n\tengine = buildRTEngine(graph, (batchsize, insize), savepath=""../TestData"", dtype=DataType.float32)\n\n\toutdata = graph(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get(), atol=1e-6)\n\tbenchModels(graph, engine, data)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/TensorRT/Tests/MnistLenetTest.py,1,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Datasets import MnistLoader\n\nfrom PuzzleLib.Containers import *\nfrom PuzzleLib.Modules import *\nfrom PuzzleLib.Handlers import *\nfrom PuzzleLib.Optimizers import MomentumSGD\nfrom PuzzleLib.Cost import CrossEntropy\n\nfrom PuzzleLib.Converter.TensorRT.Tests.Common import benchModels\nfrom PuzzleLib.Converter.TensorRT.BuildRTEngine import buildRTEngine, DataType\nfrom PuzzleLib.Converter.TensorRT.DataCalibrator import DataCalibrator\n\n\ndef buildNet():\n\tseq = Sequential(name=""lenet-5-like"")\n\tseq.append(Conv2D(1, 16, 3))\n\tseq.append(MaxPool2D())\n\tseq.append(Activation(relu))\n\n\tseq.append(Conv2D(16, 32, 4))\n\tseq.append(MaxPool2D())\n\tseq.append(Activation(relu))\n\n\tseq.append(Flatten())\n\tseq.append(Linear(32 * 5 * 5, 1024))\n\tseq.append(Activation(relu))\n\n\tseq.append(Linear(1024, 10))\n\n\treturn seq\n\n\ndef trainNet(net, data, labels, epochs):\n\toptimizer = MomentumSGD()\n\toptimizer.setupOn(net, useGlobalState=True)\n\toptimizer.learnRate = 0.1\n\toptimizer.momRate = 0.9\n\n\tcost = CrossEntropy(maxlabels=10)\n\ttrainer = Trainer(net, cost, optimizer)\n\tvalidator = Validator(net, cost)\n\n\tfor i in range(epochs):\n\t\ttrainer.trainFromHost(\n\t\t\tdata[:60000], labels[:60000], macroBatchSize=60000,\n\t\t\tonMacroBatchFinish=lambda train: print(""Train error: %s"" % train.cost.getMeanError())\n\t\t)\n\t\tprint(""Accuracy: %s"" % (1.0 - validator.validateFromHost(data[60000:], labels[60000:], macroBatchSize=10000)))\n\n\t\toptimizer.learnRate *= 0.9\n\n\ndef validate(net, data, labels, batchsize=1):\n\tcost = CrossEntropy(maxlabels=10)\n\tvalidator = Validator(net, cost, batchsize=batchsize)\n\n\treturn 1.0 - validator.validateFromHost(data[60000:], labels[60000:], macroBatchSize=10000)\n\n\ndef main():\n\tmnist = MnistLoader()\n\tdata, labels = mnist.load(path=""../TestData/"")\n\tdata, labels = data[:], labels[:]\n\tprint(""Loaded mnist"")\n\n\tnp.random.seed(1234)\n\n\tnet = buildNet()\n\ttrainNet(net, data, labels, 15)\n\n\tcalibrator = DataCalibrator(data[:60000])\n\tnet.evalMode()\n\n\tengine = buildRTEngine(\n\t\tnet, inshape=data[:1].shape, savepath=""../TestData"", dtype=DataType.int8, calibrator=calibrator\n\t)\n\n\tbenchModels(net, engine, gpuarray.to_gpu(data[:1]))\n\n\tprint(""Net    accuracy: %s"" % validate(net, data, labels))\n\tprint(""Engine accuracy: %s"" % validate(engine, data, labels, batchsize=1))\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/TensorRT/Tests/ModulesTest.py,33,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Containers.Sequential import Sequential\n\nfrom PuzzleLib.Modules.Activation import Activation, relu, leakyRelu, clip\nfrom PuzzleLib.Modules.BatchNorm import BatchNorm\nfrom PuzzleLib.Modules.BatchNorm1D import BatchNorm1D\nfrom PuzzleLib.Modules.Conv1D import Conv1D\nfrom PuzzleLib.Modules.Conv2D import Conv2D\nfrom PuzzleLib.Modules.CrossMapLRN import CrossMapLRN\nfrom PuzzleLib.Modules.Deconv2D import Deconv2D\nfrom PuzzleLib.Modules.GroupLinear import GroupLinear\nfrom PuzzleLib.Modules.MulAddConst import MulAddConst\nfrom PuzzleLib.Modules.Pad1D import Pad1D, PadMode\nfrom PuzzleLib.Modules.PRelu import PRelu\nfrom PuzzleLib.Modules.Reshape import Reshape\nfrom PuzzleLib.Modules.RNN import RNN\nfrom PuzzleLib.Modules.Split import Split\nfrom PuzzleLib.Modules.SwapAxes import SwapAxes\nfrom PuzzleLib.Modules.Upsample2D import Upsample2D\n\nfrom PuzzleLib.Converter.TensorRT.BuildRTEngine import buildRTEngine, DataType\n\n\ndef deconv2dTest():\n\tbatchsize, inmaps, inh, inw = 2, 3, 4, 5\n\toutmaps = 5\n\n\tmod = Deconv2D(inmaps, outmaps, size=2, stride=2, name=""deconv"", useBias=False)\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, inmaps, inh, inw).astype(np.float32))\n\n\tengine = buildRTEngine(mod, data.shape, savepath=""../TestData"", dtype=DataType.float32)\n\n\toutdata = mod(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\n\ndef crossMapLRNTest():\n\tbatchsize, maps, height, width = 2, 5, 3, 4\n\n\tmod = CrossMapLRN(name=""lrn"")\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, height, width).astype(np.float32))\n\n\tengine = buildRTEngine(mod, data.shape, savepath=""../TestData"", dtype=DataType.float32)\n\n\toutdata = mod(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\n\ndef groupLinearTest():\n\tbatchsize, insize, outsize = 4, 3, 5\n\tgroups = 2\n\n\tmod = GroupLinear(None, insize, outsize, wmode=""one"", name=""groupLinear"")\n\tmod.b.set(np.random.randn(1, outsize).astype(np.float32))\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, groups, insize).astype(np.float32))\n\n\tengine = buildRTEngine(mod, data.shape, savepath=""../TestData"", dtype=DataType.float32)\n\n\toutdata = mod(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\n\ndef mulAddConstTest():\n\tbatchsize, maps, height, width = 4, 3, 5, 8\n\n\tmod = MulAddConst(a=1.5, b=-2.0, name=""muladd"")\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, height, width).astype(np.float32))\n\n\tengine = buildRTEngine(mod, data.shape, savepath=""../TestData"", dtype=DataType.float32)\n\n\toutdata = mod(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\n\ndef batchNormTest():\n\tbatchsize, size = 16, 10\n\n\tmod = BatchNorm(size, name=""bn"")\n\tmod.evalMode()\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, size).astype(np.float32))\n\n\tengine = buildRTEngine(mod, data.shape, savepath=""../TestData"", dtype=DataType.float32)\n\n\toutdata = mod(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\n\ndef conv1dTest():\n\tbatchsize, inmaps, insize = 2, 3, 5\n\toutmaps = 4\n\n\tmod = Conv1D(inmaps, outmaps, size=2, stride=2, name=""conv1d"", useBias=False)\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, inmaps, insize).astype(np.float32))\n\n\tengine = buildRTEngine(mod, data.shape, savepath=""../TestData"", dtype=DataType.float32)\n\n\toutdata = mod(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\n\ndef batchNorm1dTest():\n\tbatchsize, maps, size = 2, 3, 5\n\n\tmod = BatchNorm1D(maps, size, name=""bn1d"")\n\tmod.evalMode()\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, size).astype(np.float32))\n\n\tengine = buildRTEngine(mod, data.shape, savepath=""../TestData"", dtype=DataType.float32)\n\n\toutdata = mod(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\n\ndef splitTest():\n\tbatchsize, maps, height, width = 2, 6, 4, 5\n\n\tmod = Split(axis=1, sections=(2, 4), name=""split"")\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, height, width).astype(np.float32))\n\n\tengine = buildRTEngine(mod, data.shape, savepath=""../TestData"", dtype=DataType.float32)\n\n\toutdata = mod(data)\n\tenginedata = engine(data)\n\n\tassert all(np.allclose(outdat.get(), enginedat.get()) for outdat, enginedat in zip(outdata, enginedata))\n\n\ndef rnnTest():\n\tbatchsize, inmaps, inh, inw = 4, 2, 3, 3\n\toutmaps, hsize = 4, 1\n\n\tseq = Sequential(name=""rnn"")\n\n\tseq.append(Conv2D(inmaps, outmaps, 3, pad=1))\n\tseq.append(Activation(relu))\n\tseq.append(Reshape(shape=(batchsize, outmaps, inh * inw)))\n\n\tseq.append(SwapAxes(0, 1))\n\tseq.append(RNN(inh * inw, hsize, layers=2, direction=""bi"", mode=""tanh"", getSequences=True, hintBatchSize=batchsize))\n\tseq.append(SwapAxes(0, 1))\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, inmaps, inh, inw).astype(np.float32))\n\n\tengine = buildRTEngine(seq, data.shape, savepath=""../TestData"", dtype=DataType.float32)\n\n\toutdata = seq(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\n\ndef lstmTest():\n\tbatchsize, seqlen, insize = 4, 6, 5\n\thsize = 3\n\n\tseq = Sequential(name=""lstm"")\n\n\tseq.append(SwapAxes(0, 1))\n\tseq.append(RNN(insize, hsize, mode=""lstm"", getSequences=True, hintBatchSize=batchsize))\n\tseq.append(SwapAxes(0, 1))\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, seqlen, insize).astype(np.float32))\n\n\tengine = buildRTEngine(seq, data.shape, savepath=""../TestData"", dtype=DataType.float32)\n\n\toutdata = seq(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\n\ndef gruTest():\n\tbatchsize, seqlen, insize = 5, 6, 4\n\thsize = 3\n\n\tseq = Sequential(name=""gru"")\n\n\tseq.append(SwapAxes(0, 1))\n\tseq.append(RNN(insize, hsize, mode=""gru"", getSequences=True, hintBatchSize=batchsize))\n\tseq.append(SwapAxes(0, 1))\n\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, seqlen, insize).astype(np.float32))\n\n\tengine = buildRTEngine(seq, data.shape, savepath=""../TestData"", dtype=DataType.float32)\n\n\toutdata = seq(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\n\ndef upsample2dTest():\n\tbatchsize, maps, height, width = 4, 3, 5, 8\n\n\tmod = Upsample2D(scale=2, name=""upsample"")\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, height, width).astype(np.float32))\n\n\tengine = buildRTEngine(mod, data.shape, savepath=""../TestData"", dtype=DataType.float32)\n\n\toutdata = mod(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\n\ndef leakyReluTest():\n\tbatchsize, maps, height, width = 4, 3, 5, 8\n\n\tmod = Activation(leakyRelu, name=""leakyrelu"")\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, height, width).astype(np.float32))\n\n\tengine = buildRTEngine(mod, data.shape, savepath=""../TestData"", dtype=DataType.float32)\n\n\toutdata = mod(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\n\ndef clipTest():\n\tbatchsize, maps, height, width = 4, 3, 5, 8\n\n\tmod = Activation(clip, name=""clip"")\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, height, width).astype(np.float32))\n\n\tengine = buildRTEngine(mod, data.shape, savepath=""../TestData"", dtype=DataType.float32)\n\n\toutdata = mod(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\n\ndef preluTest():\n\tbatchsize, maps, height, width = 4, 3, 5, 8\n\n\tmod = PRelu(maps=maps, name=""prelu"")\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, height, width).astype(np.float32))\n\n\tengine = buildRTEngine(mod, data.shape, savepath=""../TestData"", dtype=DataType.float32)\n\n\toutdata = mod(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\n\ndef pad1dTest():\n\tbatchsize, maps, size = 4, 5, 7\n\tlpad, rpad = 2, 3\n\n\tmod = Pad1D(pad=(lpad, rpad), mode=PadMode.reflect, name=""reflectpad"")\n\tdata = gpuarray.to_gpu(np.random.randn(batchsize, maps, size).astype(np.float32))\n\n\tengine = buildRTEngine(mod, data.shape, savepath=""../TestData"", dtype=DataType.float32)\n\n\toutdata = mod(data)\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\n\ndef main():\n\tdeconv2dTest()\n\tcrossMapLRNTest()\n\tgroupLinearTest()\n\tmulAddConstTest()\n\tbatchNormTest()\n\n\tconv1dTest()\n\tbatchNorm1dTest()\n\tsplitTest()\n\n\trnnTest()\n\tlstmTest()\n\tgruTest()\n\n\tleakyReluTest()\n\tclipTest()\n\n\tupsample2dTest()\n\tpreluTest()\n\tpad1dTest()\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/TensorRT/Tests/OpenPoseBody25Test.py,1,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Converter.TensorRT.BuildRTEngine import buildRTEngineFromCaffe, DataType\n\n\ndef main():\n\tinshape = (1, 3, 16, 16)\n\toutshape = (1, 78, 2, 2)\n\n\tengine = buildRTEngineFromCaffe(\n\t\t(""../TestData/pose_deploy.prototxt"", ""../TestData/pose_iter_584000.caffemodel""),\n\t\tinshape=inshape, outshape=outshape, outlayers=[""net_output""], dtype=DataType.float32, savepath=""../TestData""\n\t)\n\n\tdata = gpuarray.to_gpu(np.random.randn(*inshape).astype(np.float32))\n\tengine(data)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/TensorRT/Tests/OpenPoseCOCOTest.py,2,"b'import numpy as np\n\nfrom PuzzleLib import Config\nConfig.globalEvalMode = True\n\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Modules.Conv2D import Conv2D\nfrom PuzzleLib.Modules.Replicate import Replicate\nfrom PuzzleLib.Modules.Identity import Identity\nfrom PuzzleLib.Modules.Activation import Activation, relu\nfrom PuzzleLib.Modules.Concat import Concat\nfrom PuzzleLib.Modules.MaxPool2D import MaxPool2D\n\nfrom PuzzleLib.Containers.Sequential import Sequential\nfrom PuzzleLib.Containers.Parallel import Parallel\n\nfrom PuzzleLib.Converter.TensorRT.Tests.Common import benchModels\nfrom PuzzleLib.Converter.TensorRT.BuildRTEngine import buildRTEngine, buildRTEngineFromCaffe, DataType\n\n\ndef buildSmallBlock(inplace=True):\n\tblock = Sequential()\n\n\tblock.append(Replicate(3))\n\n\tleft = buildSmallBranch(inplace=inplace, num=1)\n\tright = buildSmallBranch(inplace=inplace, num=2)\n\n\tshortcut = Sequential().append(Identity())\n\n\tblock.append(Parallel().append(left).append(right).append(shortcut))\n\tblock.append(Concat(axis=1, name=""concat_stage2""))\n\n\treturn block\n\n\ndef buildSmallBranch(inplace=True, num=1):\n\tbranch = Sequential()\n\n\tbranch.append(Conv2D(128, 128, 3, pad=1, initscheme=""none"", name=""conv5_1_CPM_L%d"" % num))\n\tbranch.append(Activation(relu, inplace=inplace, name=""relu5_1_CPM_L%d"" % num))\n\n\tbranch.append(Conv2D(128, 128, 3, pad=1, initscheme=""none"", name=""conv5_2_CPM_L%d"" % num))\n\tbranch.append(Activation(relu, inplace=inplace, name=""relu5_2_CPM_L%d"" % num))\n\n\tbranch.append(Conv2D(128, 128, 3, pad=1, initscheme=""none"", name=""conv5_3_CPM_L%d"" % num))\n\tbranch.append(Activation(relu, inplace=inplace, name=""relu5_3_CPM_L%d"" % num))\n\n\tbranch.append(Conv2D(128, 512, 1, initscheme=""none"", name=""conv5_4_CPM_L%d"" % num))\n\tbranch.append(Activation(relu, inplace=inplace, name=""relu5_4_CPM_L%d"" % num))\n\n\tbranch.append(Conv2D(512, 19 * (3 - num), 1, initscheme=""none"", name=""conv5_5_CPM_L%d"" % num))\n\n\treturn branch\n\n\ndef buildBranch(inmaps=185, inplace=True, num=1, stage=2):\n\tbranch = Sequential()\n\n\tbranch.append(Conv2D(inmaps, 128, 7, pad=3, initscheme=""none"", name=""Mconv1_stage%d_L%d"" % (stage, num)))\n\tbranch.append(Activation(relu, inplace=inplace, name=""Mrelu1_stage%d_L%d"" % (stage, num)))\n\n\tbranch.append(Conv2D(128, 128, 7, pad=3, initscheme=""none"", name=""Mconv2_stage%d_L%d"" % (stage, num)))\n\tbranch.append(Activation(relu, inplace=inplace, name=""Mrelu2_stage%d_L%d"" % (stage, num)))\n\n\tbranch.append(Conv2D(128, 128, 7, pad=3, initscheme=""none"", name=""Mconv3_stage%d_L%d"" % (stage, num)))\n\tbranch.append(Activation(relu, inplace=inplace, name=""Mrelu3_stage%d_L%d"" % (stage, num)))\n\n\tbranch.append(Conv2D(128, 128, 7, pad=3, initscheme=""none"", name=""Mconv4_stage%d_L%d"" % (stage, num)))\n\tbranch.append(Activation(relu, inplace=inplace, name=""Mrelu4_stage%d_L%d"" % (stage, num)))\n\n\tbranch.append(Conv2D(128, 128, 7, pad=3, initscheme=""none"", name=""Mconv5_stage%d_L%d"" % (stage, num)))\n\tbranch.append(Activation(relu, inplace=inplace, name=""Mrelu5_stage%d_L%d"" % (stage, num)))\n\n\tbranch.append(Conv2D(128, 128, 1, initscheme=""none"", name=""Mconv6_stage%d_L%d"" % (stage, num)))\n\tbranch.append(Activation(relu, inplace=inplace, name=""Mrelu6_stage%d_L%d"" % (stage, num)))\n\n\tbranch.append(Conv2D(128, 19 * (3 - num), 1, initscheme=""none"", name=""Mconv7_stage%d_L%d"" % (stage, num)))\n\n\treturn branch\n\n\ndef buildBall(stage=2, inplace=True):\n\tball = Sequential()\n\n\tball.append(Replicate(2))\n\n\tleft = buildBranch(stage=stage, num=1, inplace=inplace)\n\tright = buildBranch(stage=stage, num=2, inplace=inplace)\n\n\tball.append(Parallel().append(left).append(right))\n\n\tball.append(Concat(axis=1))\n\n\treturn ball\n\n\ndef buildBigBlock(stage=2, prenet=None, inplace=True):\n\tblock = Sequential()\n\n\tblock.append(Replicate(2))\n\n\tshortcut = Sequential().append(Identity())\n\n\tif prenet is None:\n\t\tball = buildBall(stage=stage, inplace=inplace)\n\telse:\n\t\tball = prenet\n\t\tball.extend(buildBall(stage=stage, inplace=inplace))\n\n\tblock.append(Parallel().append(ball).append(shortcut))\n\tblock.append(Concat(axis=1, name=""concat_stage%d"" % (stage+1)))\n\n\treturn block\n\n\ndef loadNet(name="""", inplace=True, modelpath=None):\n\tnet = Sequential(name)\n\n\tnet.append(Conv2D(3, 64, 3, pad=1, initscheme=""none"", name=""conv1_1""))\n\tnet.append(Activation(relu, name=""relu1_1"", inplace=inplace))\n\n\tnet.append(Conv2D(64, 64, 3, pad=1, initscheme=""none"", name=""conv1_2""))\n\tnet.append(Activation(relu, name=""relu1_2"", inplace=inplace))\n\n\tnet.append(MaxPool2D(name=""pool1_stage1""))\n\n\tnet.append(Conv2D(64, 128, 3, pad=1, initscheme=""none"", name=""conv2_1""))\n\tnet.append(Activation(relu, name=""relu2_1"", inplace=inplace))\n\n\tnet.append(Conv2D(128, 128, 3, pad=1, initscheme=""none"", name=""conv2_2""))\n\tnet.append(Activation(relu, name=""relu2_2"", inplace=inplace))\n\n\tnet.append(MaxPool2D(name=""pool2_stage1""))\n\n\tnet.append(Conv2D(128, 256, 3, pad=1, initscheme=""none"", name=""conv3_1""))\n\tnet.append(Activation(relu, name=""relu3_1"", inplace=inplace))\n\n\tnet.append(Conv2D(256, 256, 3, pad=1, initscheme=""none"", name=""conv3_2""))\n\tnet.append(Activation(relu, name=""relu3_2"", inplace=inplace))\n\n\tnet.append(Conv2D(256, 256, 3, pad=1, initscheme=""none"", name=""conv3_3""))\n\tnet.append(Activation(relu, name=""relu3_3"", inplace=inplace))\n\n\tnet.append(Conv2D(256, 256, 3, pad=1, initscheme=""none"", name=""conv3_4""))\n\tnet.append(Activation(relu, name=""relu3_4"", inplace=inplace))\n\n\tnet.append(MaxPool2D(name=""pool3_stage1""))\n\n\tnet.append(Conv2D(256, 512, 3, pad=1, initscheme=""none"", name=""conv4_1""))\n\tnet.append(Activation(relu, name=""relu4_1"", inplace=inplace))\n\n\tnet.append(Conv2D(512, 512, 3, pad=1, initscheme=""none"", name=""conv4_2""))\n\tnet.append(Activation(relu, name=""relu4_2"", inplace=inplace))\n\n\tnet.append(Conv2D(512, 256, 3, pad=1, initscheme=""none"", name=""conv4_3_CPM""))\n\tnet.append(Activation(relu, name=""relu4_3_CPM""))\n\n\tnet.append(Conv2D(256, 128, 3, pad=1, initscheme=""none"", name=""conv4_4_CPM""))\n\tnet.append(Activation(relu, name=""relu4_4_CPM""))\n\n\tblock2 = buildSmallBlock(inplace=inplace)\n\tblock3 = buildBigBlock(stage=2, prenet=block2, inplace=inplace)\n\tblock4 = buildBigBlock(stage=3, prenet=block3, inplace=inplace)\n\tblock5 = buildBigBlock(stage=4, prenet=block4, inplace=inplace)\n\tblock6 = buildBigBlock(stage=5, prenet=block5, inplace=inplace)\n\n\tnet.extend(block6)\n\tnet.append(Replicate(2))\n\n\tnet.append(Parallel().append(\n\t\tbuildBranch(stage=6, num=2, inplace=inplace)\n\t).append(\n\t\tbuildBranch(stage=6, num=1, inplace=inplace))\n\t)\n\n\tnet.append(Concat(axis=1))\n\n\tif modelpath is not None:\n\t\tnet.load(modelpath, assumeUniqueNames=True)\n\n\treturn net\n\n\ndef main():\n\tinshape = (1, 3, 368, 368)\n\n\tnet = loadNet(inplace=False, modelpath=""../TestData/pose_iter_440000.hdf"")\n\toutshape = net.dataShapeFrom(inshape)\n\n\tpzlEngine = buildRTEngine(net, inshape=inshape, dtype=DataType.float32, savepath=""../TestData"")\n\tcaffeEngine = buildRTEngineFromCaffe(\n\t\t(""../TestData/pose_deploy_linevec.prototxt"", ""../TestData/pose_iter_440000.caffemodel""),\n\t\tinshape=inshape, outshape=outshape, outlayers=[""net_output""], dtype=DataType.float32, savepath=""../TestData""\n\t)\n\n\tdata = gpuarray.to_gpu(np.random.randn(*inshape).astype(np.float32))\n\n\tpzlData = pzlEngine(data)\n\tcaffeData = caffeEngine(data)\n\n\tassert np.allclose(pzlData.get(), caffeData.get(), atol=1e-7)\n\tbenchModels(pzlEngine, caffeEngine, data, lognames=(""puzzle"", ""caffe ""))\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/TensorRT/Tests/ParserCaffeOpenPoseMPITest.py,2,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\n\nfrom PuzzleLib.Containers import Sequential, Parallel\nfrom PuzzleLib.Modules import Conv2D, Activation, relu, MaxPool2D, Replicate, Identity, Concat\n\nfrom PuzzleLib.Converter.TensorRT.BuildRTEngine import buildRTEngineFromCaffe, DataType\nfrom PuzzleLib.Converter.TensorRT.Tests.Common import benchModels\n\n\ndef loadNet(modelpath=None, name=""OpenPoseFaceNet""):\n\tnet = Sequential(name=name)\n\n\tnet.append(Conv2D(3, 64, 3, pad=1, name=""conv1_1""))\n\tnet.append(Activation(relu, name=""conv1_1_re""))\n\tnet.append(Conv2D(64, 64, 3, pad=1, name=""conv1_2""))\n\tnet.append(Activation(relu, name=""conv1_2_re""))\n\n\tnet.append(MaxPool2D(2, 2, name=""pool1""))\n\n\tnet.append(Conv2D(64, 128, 3, pad=1, name=""conv2_1""))\n\tnet.append(Activation(relu, name=""conv2_1_re""))\n\tnet.append(Conv2D(128, 128, 3, pad=1, name=""conv2_2""))\n\tnet.append(Activation(relu, name=""conv2_2_re""))\n\n\tnet.append(MaxPool2D(2, 2, name=""pool2""))\n\n\tnet.append(Conv2D(128, 256, 3, pad=1, name=""conv3_1""))\n\tnet.append(Activation(relu, name=""conv3_1_re""))\n\tnet.append(Conv2D(256, 256, 3, pad=1, name=""conv3_2""))\n\tnet.append(Activation(relu, name=""conv3_2_re""))\n\tnet.append(Conv2D(256, 256, 3, pad=1, name=""conv3_3""))\n\tnet.append(Activation(relu, name=""conv3_3_re""))\n\tnet.append(Conv2D(256, 256, 3, pad=1, name=""conv3_4""))\n\tnet.append(Activation(relu, name=""conv3_4_re""))\n\n\tnet.append(MaxPool2D(2, 2, name=""pool3""))\n\n\tnet.append(Conv2D(256, 512, 3, pad=1, name=""conv4_1""))\n\tnet.append(Activation(relu, name=""conv4_1_re""))\n\tnet.append(Conv2D(512, 512, 3, pad=1, name=""conv4_2""))\n\tnet.append(Activation(relu, name=""conv4_2_re""))\n\tnet.append(Conv2D(512, 512, 3, pad=1, name=""conv4_3""))\n\tnet.append(Activation(relu, name=""conv4_3_re""))\n\tnet.append(Conv2D(512, 512, 3, pad=1, name=""conv4_4""))\n\tnet.append(Activation(relu, name=""conv4_4_re""))\n\n\tnet.append(Conv2D(512, 512, 3, pad=1, name=""conv5_1""))\n\tnet.append(Activation(relu, name=""conv5_1_re""))\n\tnet.append(Conv2D(512, 512, 3, pad=1, name=""conv5_2""))\n\tnet.append(Activation(relu, name=""conv5_2_re""))\n\n\tnet.append(Conv2D(512, 128, 3, pad=1, name=""conv5_3_CPM""))\n\tnet.append(Activation(relu, name=""conv5_3_CPM_re""))\n\n\tnet.append(Replicate(2))\n\n\tshortcut0 = Sequential()\n\tshortcut0.append(Identity())\n\n\tbranch0 = Sequential()\n\tbranch0.append(Replicate(2))\n\n\tshortcut1 = Sequential()\n\tshortcut1.append(Identity())\n\n\tbranch1 = Sequential()\n\tbranch1.append(Replicate(2))\n\n\tshortcut2 = Sequential()\n\tshortcut2.append(Identity())\n\n\tbranch2 = Sequential()\n\tbranch2.append(Replicate(2))\n\n\tshortcut3 = Sequential()\n\tshortcut3.append(Identity())\n\n\tbranch3 = Sequential()\n\tbranch3.append(Replicate(2))\n\n\tshortcut4 = Sequential()\n\tshortcut4.append(Identity())\n\n\tbranch4 = Sequential()\n\tbranch4.append(Conv2D(128, 512, 1, pad=0, name=""conv6_1_CPM""))\n\tbranch4.append(Activation(relu, name=""conv6_1_CPM_re""))\n\tbranch4.append(Conv2D(512, 71, 1, pad=0, name=""conv6_2_CPM""))\n\n\tbranches = [branch4, branch3, branch2, branch1, branch0, net]\n\tshortcuts = [shortcut4, shortcut3, shortcut2, shortcut1, shortcut0, None]\n\n\tfor branchIdx, branch in enumerate(branches):\n\t\tif branchIdx == 0:\n\t\t\tcontinue\n\n\t\tbranch.append(Parallel().append(branches[branchIdx - 1]).append(shortcuts[branchIdx - 1]))\n\t\tbranch.append(Concat(name=""features_in_stage_%d"" % (branchIdx + 1), axis=1))\n\n\t\tbranch.append(Conv2D(199, 128, 7, pad=3, name=""Mconv1_stage%d"" % (branchIdx + 1)))\n\t\tbranch.append(Activation(relu, name=""Mconv1_stage%d_re"" % (branchIdx + 1)))\n\t\tbranch.append(Conv2D(128, 128, 7, pad=3, name=""Mconv2_stage%d"" % (branchIdx + 1)))\n\t\tbranch.append(Activation(relu, name=""Mconv2_stage%d_re"" % (branchIdx + 1)))\n\t\tbranch.append(Conv2D(128, 128, 7, pad=3, name=""Mconv3_stage%d"" % (branchIdx + 1)))\n\t\tbranch.append(Activation(relu, name=""Mconv3_stage%d_re"" % (branchIdx + 1)))\n\t\tbranch.append(Conv2D(128, 128, 7, pad=3, name=""Mconv4_stage%d"" % (branchIdx + 1)))\n\t\tbranch.append(Activation(relu, name=""Mconv4_stage%d_re"" % (branchIdx + 1)))\n\t\tbranch.append(Conv2D(128, 128, 7, pad=3, name=""Mconv5_stage%d"" % (branchIdx + 1)))\n\t\tbranch.append(Activation(relu, name=""Mconv5_stage%d_re"" % (branchIdx + 1)))\n\t\tbranch.append(Conv2D(128, 128, 1, pad=0, name=""Mconv6_stage%d"" % (branchIdx + 1)))\n\t\tbranch.append(Activation(relu, name=""Mconv6_stage%d_re"" % (branchIdx + 1)))\n\t\tbranch.append(Conv2D(128, 71, 1, pad=0, name=""Mconv7_stage%d"" % (branchIdx + 1)))\n\n\tif modelpath is not None:\n\t\tnet.load(modelpath, assumeUniqueNames=True, name=name)\n\t\tnet.evalMode()\n\n\treturn net\n\n\ndef main():\n\tinshape = (1, 3, 368, 368)\n\n\tnet = loadNet(""../TestData/pose_iter_116000.hdf"")\n\tnet.optimizeForShape(inshape)\n\n\toutshape = net.dataShapeFrom(inshape)\n\n\tengine = buildRTEngineFromCaffe(\n\t\t(""../TestData/pose_deploy.prototxt"", ""../TestData/pose_iter_116000.caffemodel""),\n\t\tinshape=inshape, outshape=outshape, outlayers=[""net_output""], dtype=DataType.float32, savepath=""../TestData""\n\t)\n\n\tdata = gpuarray.to_gpu(np.random.randn(*inshape).astype(np.float32))\n\n\tnetData = net(data).get()\n\tengineData = engine(data).get()\n\n\tassert np.allclose(netData, engineData)\n\tbenchModels(net, engine, data)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/TensorRT/Tests/ParserOnnxResNet50Test.py,5,"b'import numpy as np\nfrom PIL import Image\n\nfrom PuzzleLib import Config\nConfig.globalEvalMode = True\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Models.Nets.ResNet import loadResNet\n\nfrom PuzzleLib.Converter.Examples.Common import loadResNetSample, loadLabels\n\nfrom PuzzleLib.Converter.TensorRT.BuildRTEngine import buildRTEngineFromCaffe, buildRTEngineFromOnnx, DataType\nfrom PuzzleLib.Converter.TensorRT.Tests.Common import printResults\n\n\ndef preprocessCaffe2Onnx(img):\n\tmean = np.array([0.485, 0.456, 0.406])\n\tstddev = np.array([0.229, 0.224, 0.225])\n\n\tnormdata = np.zeros(img.shape).astype(np.float32)\n\n\tfor i in range(img.shape[0]):\n\t\tnormdata[i, :, :] = (img[i, :, :] / 255 - mean[i]) / stddev[i]\n\n\treturn normdata\n\n\ndef main():\n\tinshape = (1, 3, 224, 224)\n\n\tnet = loadResNet(modelpath=""../../TestData/ResNet-50-model.hdf"", layers=""50"")\n\toutshape = net.dataShapeFrom(inshape)\n\n\tcaffeengine = buildRTEngineFromCaffe(\n\t\t(""../TestData/ResNet-50-deploy.prototxt"", ""../TestData/ResNet-50-model.caffemodel""),\n\t\tinshape=inshape, outshape=outshape, outlayers=[""prob""], dtype=DataType.float32, savepath=""../TestData""\n\t)\n\n\tonnxengine = buildRTEngineFromOnnx(\n\t\t""../TestData/resnet50.onnx"", inshape=inshape, outshape=outshape, dtype=DataType.float32, savepath=""../TestData""\n\t)\n\n\tdata = gpuarray.to_gpu(loadResNetSample(net, ""../../TestData/tarantula.jpg""))\n\tlabels = loadLabels(synpath=""../../TestData/synsets.txt"", wordpath=""../../TestData/synset_words.txt"")\n\n\tnetData = net(data).get()\n\tcaffeData = caffeengine(data).get()\n\n\tdata = np.moveaxis(np.array(Image.open(""../../TestData/tarantula.jpg""), dtype=np.float32), 2, 0)\n\tdata = gpuarray.to_gpu(preprocessCaffe2Onnx(data)[np.newaxis, ...])\n\n\tonnxData = onnxengine(data).get()\n\n\tprintResults(netData, labels, ""Net"")\n\tprintResults(caffeData, labels, ""Caffe"")\n\tprintResults(onnxData, labels, ""Onnx"")\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/TensorRT/Tests/ResNet50Test.py,0,"b'from PuzzleLib import Config\nConfig.globalEvalMode = True\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Models.Nets.ResNet import loadResNet\n\nfrom PuzzleLib.Converter.Examples.Common import loadResNetSample, loadLabels\n\nfrom PuzzleLib.Converter.TensorRT.Tests.Common import scoreModels, benchModels\nfrom PuzzleLib.Converter.TensorRT.BuildRTEngine import buildRTEngine, DataType\n\n\ndef main():\n\tnet = loadResNet(modelpath=""../../TestData/ResNet-50-model.hdf"", layers=""50"")\n\n\tdata = gpuarray.to_gpu(loadResNetSample(net, ""../../TestData/tarantula.jpg""))\n\tlabels = loadLabels(synpath=""../../TestData/synsets.txt"", wordpath=""../../TestData/synset_words.txt"")\n\n\tengine = buildRTEngine(net, inshape=data.shape, savepath=""../TestData"", dtype=DataType.float32)\n\n\tscoreModels(net, engine, data, labels)\n\tbenchModels(net, engine, data)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/TensorRT/Tests/UNetTest.py,2,"b'import numpy as np\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Models.Nets.UNet import loadUNet\n\nfrom PuzzleLib.Converter.TensorRT.Tests.Common import benchModels\nfrom PuzzleLib.Converter.TensorRT.BuildRTEngine import buildRTEngine, DataType\n\n\ndef main():\n\tnet = loadUNet(None)\n\tdata = gpuarray.to_gpu(np.random.randn(1, 1, 256, 256).astype(np.float32))\n\n\tengine = buildRTEngine(net, inshape=data.shape, savepath=""../TestData"", dtype=DataType.float32)\n\n\tnet.evalMode()\n\toutdata = net(data)\n\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\tbenchModels(net, engine, data)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Converter/TensorRT/Tests/WaveToLetterTest.py,2,"b'import numpy as np\n\nfrom PuzzleLib import Config\nConfig.globalEvalMode = True\n\nfrom PuzzleLib.Backend import gpuarray\nfrom PuzzleLib.Models.Nets.WaveToLetter import loadW2L\n\nfrom PuzzleLib.Converter.TensorRT.Tests.Common import benchModels\nfrom PuzzleLib.Converter.TensorRT.BuildRTEngine import buildRTEngine, DataType\n\n\ndef main():\n\tinmaps = 161\n\tnet = loadW2L(None, inmaps, nlabels=29)\n\n\tdata = gpuarray.to_gpu(np.random.randn(1, inmaps, 200).astype(np.float32))\n\tengine = buildRTEngine(net, inshape=data.shape, savepath=""../TestData"", dtype=DataType.float32)\n\n\tnet.evalMode()\n\toutdata = net(data)\n\n\tenginedata = engine(data)\n\n\tassert np.allclose(outdata.get(), enginedata.get())\n\tbenchModels(net, engine, data)\n\n\nif __name__ == ""__main__"":\n\tmain()\n'"
Models/Nets/Presets/SentiNet.py,8,"b'import tempfile, os\n\nimport numpy as np\n\nfrom PuzzleLib.Models.Nets.SentiNet import buildNet\nfrom PuzzleLib.Cost.CrossEntropy import CrossEntropy\nfrom PuzzleLib.Optimizers.AdaDelta import AdaDelta\n\nfrom PuzzleLib.Handlers.Trainer import Trainer\nfrom PuzzleLib.Handlers.Validator import Validator\n\nfrom PuzzleLib.Datasets.Utils import validate, getDim, splitData, replicateData\n\n\ndef train(net, trainData, trainLabels, valData, valLabels, dim=0, epochs=50, epochsBeforeSaving=0, saving=True,\n\t\t  printing=True, macroBatchSize=30000, optimizeNet=True):\n\tif dim == 0:\n\t\tdim = getDim(trainLabels)\n\n\tnumOfChunks = 1\n\tbatchsize = 64\n\n\tif printing:\n\t\tprint(""Batchsize: %d"" % batchsize)\n\t\tprint(""Num of chunks: %d"" % numOfChunks)\n\n\tmacroBatchSize = min(len(trainLabels), macroBatchSize)\n\n\toptimizer = AdaDelta()\n\toptimizer.setupOn(net)\n\n\tcost = CrossEntropy(dim)\n\n\ttrainer = Trainer(net, cost, optimizer, batchsize=batchsize)\n\tvalidator = Validator(net, cost)\n\n\tif optimizeNet:\n\t\tnet.optimizeForShape((batchsize, *trainData.shape[1:]))\n\n\tlowestValerror = np.inf\n\tvalerror = np.inf\n\n\tfor epoch in range(epochs):\n\t\ttrainSize = trainData.shape[0]\n\t\tchunkSize = trainSize // numOfChunks\n\n\t\tfor j in range(numOfChunks + 1):\n\t\t\tstart = j * chunkSize\n\t\t\tend = min((j + 1) * chunkSize, trainSize)\n\n\t\t\tif start == end:\n\t\t\t\tcontinue\n\n\t\t\ttrainer.trainFromHost(trainData[start:end], trainLabels[start:end], macroBatchSize=macroBatchSize)\n\t\t\tvalerror = validator.validateFromHost(valData, valLabels, macroBatchSize=macroBatchSize)\n\n\t\t\tif printing:\n\t\t\t\ttrainerror = trainer.cost.getMeanError()\n\n\t\t\t\tprint(""Epoch #%d/%d. Chunk #%d/%d. Train error: %s. Val error: %s"" % (\n\t\t\t\t\tepoch + 1, epochs, j + 1, numOfChunks, trainerror, valerror))\n\n\t\t\tif lowestValerror >= valerror and epoch >= epochsBeforeSaving:\n\t\t\t\tlowestValerror = valerror\n\n\t\t\t\tif saving:\n\t\t\t\t\tnet.save(os.path.join(tempfile.gettempdir(), net.name + "".hdf""))\n\n\t\t\t\t\tif printing:\n\t\t\t\t\t\tprint(""Net saved for %d epoch. Validation accuracy: %-6f%%"" % (\n\t\t\t\t\t\t\tepoch + 1, 100.0 * (1.0 - valerror)))\n\n\t\tif printing:\n\t\t\tprint(""Finished epoch #%02d. Total mean error: %8f. Validation accuracy: %-6f%%\\n"" % (\n\t\t\t\tepoch + 1, cost.getMeanError(), 100.0 * (1.0 - valerror)))\n\n\tbestPrecision = 1.0 - lowestValerror\n\n\tif printing:\n\t\tprint(""Highest accuracy: %-6f%%\\n"" % (100.0 * bestPrecision))\n\n\tif saving:\n\t\tnet.load(os.path.join(tempfile.gettempdir(), net.name + "".hdf""))\n\t\treturn net, bestPrecision\n\telse:\n\t\treturn None, bestPrecision\n\n\ndef buildTrainValidate(data, labels, vocabulary=None, w2v=None, wscale=0.25, embsize=300, padding=4, dim=2,\n\t\t\t\t\t   sentlength=100, epochs=5, epochsBeforeSaving=0, branches=(3, 4, 5), saving=True, printing=True):\n\tdata = np.asarray(data.copy())\n\tlabels = np.asarray(labels.copy())\n\n\t# data = np.asarray(data)\n\t# labels = np.asarray(labels)\n\n\ttrainData, valData, trainLabels, valLabels = splitData(data, labels, validation=0.1, dim=dim)\n\ttrainData, trainLabels = replicateData(trainData, trainLabels, dim=dim)\n\n\tif printing:\n\t\tprint(""Train data amount: %d"" % trainData.shape[0])\n\t\tprint(""Validation data amount: %d\\n"" % valData.shape[0])\n\n\tnet = buildNet(vocabulary, branches, w2v, sentlength + 2 * padding, embsize, wscale, dim=dim)\n\n\tnet.setAttr(""sentlength"", sentlength)\n\tnet.setAttr(""padding"", padding)\n\n\tif printing:\n\t\tprint(""Starting training ..."")\n\n\tnet, accuracy = train(\n\t\tnet, trainData, trainLabels, valData, valLabels, dim, epochs, epochsBeforeSaving, saving, printing\n\t)\n\n\tif net:\n\t\t_, _, accuracy = validate(net, valData, valLabels, dim, log=printing)\n\n\treturn accuracy, net, trainData, valData, trainLabels, valLabels\n\n\ndef unittest():\n\tvocabsize= 1000\n\tsentlength = 100\n\n\tdata = np.random.randint(0, vocabsize, (10000, sentlength), dtype=np.int32)\n\tlabels = np.random.randint(0, 2, (10000, ), dtype=np.int32)\n\n\tbuildTrainValidate(data, labels, vocabsize, padding=0, embsize=64, epochs=15, saving=False)\n\n\nif __name__ == ""__main__"":\n\tunittest()\n'"
