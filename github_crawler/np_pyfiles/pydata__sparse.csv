file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n\nfrom setuptools import setup, find_packages\nimport versioneer\nfrom pathlib import Path\n\n\ndef open_reqs_file(file, reqs_path=Path(""."")):\n    with (reqs_path / file).open() as f:\n        reqs = list(f.read().strip().split(""\\n""))\n\n    i = 0\n    while i < len(reqs):\n        if reqs[i].startswith(""-r""):\n            reqs[i : i + 1] = open_reqs_file(reqs[i][2:].strip(), reqs_path=reqs_path)\n        else:\n            i += 1\n\n    return reqs\n\n\nextras_require = {}\nreqs = []\n\n\ndef parse_requires():\n    reqs_path = Path(""./requirements"")\n    reqs.extend(open_reqs_file(""requirements.txt""))\n    for f in reqs_path.iterdir():\n        extras_require[f.stem] = open_reqs_file(f.parts[-1], reqs_path=reqs_path)\n\n\nparse_requires()\n\nwith open(""README.rst"") as f:\n    long_desc = f.read()\n\nprint(repr(reqs))\nprint(repr(reqs))\n\nsetup(\n    name=""sparse"",\n    version=versioneer.get_version(),\n    cmdclass=versioneer.get_cmdclass(),\n    description=""Sparse n-dimensional arrays"",\n    url=""https://github.com/pydata/sparse/"",\n    maintainer=""Hameer Abbasi"",\n    maintainer_email=""hameerabbasi@yahoo.com"",\n    license=""BSD 3-Clause License (Revised)"",\n    keywords=""sparse,numpy,scipy,dask"",\n    packages=find_packages(include=[""sparse"", ""sparse.*""]),\n    long_description=long_desc,\n    install_requires=reqs,\n    extras_require=extras_require,\n    zip_safe=False,\n    classifiers=[\n        ""Development Status :: 2 - Pre-Alpha"",\n        ""Operating System :: OS Independent"",\n        ""License :: OSI Approved :: BSD License"",\n        ""Programming Language :: Python"",\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n        ""Programming Language :: Python :: 3.8"",\n        ""Programming Language :: Python :: 3 :: Only"",\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Science/Research"",\n    ],\n    project_urls={\n        ""Documentation"": ""https://sparse.pydata.org/"",\n        ""Source"": ""https://github.com/pydata/sparse/"",\n        ""Tracker"": ""https://github.com/pydata/sparse/issues"",\n    },\n    entry_points={\n        ""numba_extensions"": [""init = sparse._numba_extension:_init_extension""]\n    },\n    python_requires="">=3.6, <4"",\n)\n'"
versioneer.py,0,"b'# Version: 0.18\n\n""""""The Versioneer - like a rocketeer, but for versions.\n\nThe Versioneer\n==============\n\n* like a rocketeer, but for versions!\n* https://github.com/warner/python-versioneer\n* Brian Warner\n* License: Public Domain\n* Compatible With: python2.6, 2.7, 3.2, 3.3, 3.4, 3.5, 3.6, and pypy\n* [![Latest Version]\n(https://pypip.in/version/versioneer/badge.svg?style=flat)\n](https://pypi.python.org/pypi/versioneer/)\n* [![Build Status]\n(https://travis-ci.org/warner/python-versioneer.png?branch=master)\n](https://travis-ci.org/warner/python-versioneer)\n\nThis is a tool for managing a recorded version number in distutils-based\npython projects. The goal is to remove the tedious and error-prone ""update\nthe embedded version string"" step from your release process. Making a new\nrelease should be as easy as recording a new tag in your version-control\nsystem, and maybe making new tarballs.\n\n\n## Quick Install\n\n* `pip install versioneer` to somewhere to your $PATH\n* add a `[versioneer]` section to your setup.cfg (see below)\n* run `versioneer install` in your source tree, commit the results\n\n## Version Identifiers\n\nSource trees come from a variety of places:\n\n* a version-control system checkout (mostly used by developers)\n* a nightly tarball, produced by build automation\n* a snapshot tarball, produced by a web-based VCS browser, like github\'s\n  ""tarball from tag"" feature\n* a release tarball, produced by ""setup.py sdist"", distributed through PyPI\n\nWithin each source tree, the version identifier (either a string or a number,\nthis tool is format-agnostic) can come from a variety of places:\n\n* ask the VCS tool itself, e.g. ""git describe"" (for checkouts), which knows\n  about recent ""tags"" and an absolute revision-id\n* the name of the directory into which the tarball was unpacked\n* an expanded VCS keyword ($Id$, etc)\n* a `_version.py` created by some earlier build step\n\nFor released software, the version identifier is closely related to a VCS\ntag. Some projects use tag names that include more than just the version\nstring (e.g. ""myproject-1.2"" instead of just ""1.2""), in which case the tool\nneeds to strip the tag prefix to extract the version identifier. For\nunreleased software (between tags), the version identifier should provide\nenough information to help developers recreate the same tree, while also\ngiving them an idea of roughly how old the tree is (after version 1.2, before\nversion 1.3). Many VCS systems can report a description that captures this,\nfor example `git describe --tags --dirty --always` reports things like\n""0.7-1-g574ab98-dirty"" to indicate that the checkout is one revision past the\n0.7 tag, has a unique revision id of ""574ab98"", and is ""dirty"" (it has\nuncommitted changes.\n\nThe version identifier is used for multiple purposes:\n\n* to allow the module to self-identify its version: `myproject.__version__`\n* to choose a name and prefix for a \'setup.py sdist\' tarball\n\n## Theory of Operation\n\nVersioneer works by adding a special `_version.py` file into your source\ntree, where your `__init__.py` can import it. This `_version.py` knows how to\ndynamically ask the VCS tool for version information at import time.\n\n`_version.py` also contains `$Revision$` markers, and the installation\nprocess marks `_version.py` to have this marker rewritten with a tag name\nduring the `git archive` command. As a result, generated tarballs will\ncontain enough information to get the proper version.\n\nTo allow `setup.py` to compute a version too, a `versioneer.py` is added to\nthe top level of your source tree, next to `setup.py` and the `setup.cfg`\nthat configures it. This overrides several distutils/setuptools commands to\ncompute the version when invoked, and changes `setup.py build` and `setup.py\nsdist` to replace `_version.py` with a small static file that contains just\nthe generated version data.\n\n## Installation\n\nSee [INSTALL.md](./INSTALL.md) for detailed installation instructions.\n\n## Version-String Flavors\n\nCode which uses Versioneer can learn about its version string at runtime by\nimporting `_version` from your main `__init__.py` file and running the\n`get_versions()` function. From the ""outside"" (e.g. in `setup.py`), you can\nimport the top-level `versioneer.py` and run `get_versions()`.\n\nBoth functions return a dictionary with different flavors of version\ninformation:\n\n* `[\'version\']`: A condensed version string, rendered using the selected\n  style. This is the most commonly used value for the project\'s version\n  string. The default ""pep440"" style yields strings like `0.11`,\n  `0.11+2.g1076c97`, or `0.11+2.g1076c97.dirty`. See the ""Styles"" section\n  below for alternative styles.\n\n* `[\'full-revisionid\']`: detailed revision identifier. For Git, this is the\n  full SHA1 commit id, e.g. ""1076c978a8d3cfc70f408fe5974aa6c092c949ac"".\n\n* `[\'date\']`: Date and time of the latest `HEAD` commit. For Git, it is the\n  commit date in ISO 8601 format. This will be None if the date is not\n  available.\n\n* `[\'dirty\']`: a boolean, True if the tree has uncommitted changes. Note that\n  this is only accurate if run in a VCS checkout, otherwise it is likely to\n  be False or None\n\n* `[\'error\']`: if the version string could not be computed, this will be set\n  to a string describing the problem, otherwise it will be None. It may be\n  useful to throw an exception in setup.py if this is set, to avoid e.g.\n  creating tarballs with a version string of ""unknown"".\n\nSome variants are more useful than others. Including `full-revisionid` in a\nbug report should allow developers to reconstruct the exact code being tested\n(or indicate the presence of local changes that should be shared with the\ndevelopers). `version` is suitable for display in an ""about"" box or a CLI\n`--version` output: it can be easily compared against release notes and lists\nof bugs fixed in various releases.\n\nThe installer adds the following text to your `__init__.py` to place a basic\nversion in `YOURPROJECT.__version__`:\n\n    from ._version import get_versions\n    __version__ = get_versions()[\'version\']\n    del get_versions\n\n## Styles\n\nThe setup.cfg `style=` configuration controls how the VCS information is\nrendered into a version string.\n\nThe default style, ""pep440"", produces a PEP440-compliant string, equal to the\nun-prefixed tag name for actual releases, and containing an additional ""local\nversion"" section with more detail for in-between builds. For Git, this is\nTAG[+DISTANCE.gHEX[.dirty]] , using information from `git describe --tags\n--dirty --always`. For example ""0.11+2.g1076c97.dirty"" indicates that the\ntree is like the ""1076c97"" commit but has uncommitted changes ("".dirty""), and\nthat this commit is two revisions (""+2"") beyond the ""0.11"" tag. For released\nsoftware (exactly equal to a known tag), the identifier will only contain the\nstripped tag, e.g. ""0.11"".\n\nOther styles are available. See [details.md](details.md) in the Versioneer\nsource tree for descriptions.\n\n## Debugging\n\nVersioneer tries to avoid fatal errors: if something goes wrong, it will tend\nto return a version of ""0+unknown"". To investigate the problem, run `setup.py\nversion`, which will run the version-lookup code in a verbose mode, and will\ndisplay the full contents of `get_versions()` (including the `error` string,\nwhich may help identify what went wrong).\n\n## Known Limitations\n\nSome situations are known to cause problems for Versioneer. This details the\nmost significant ones. More can be found on Github\n[issues page](https://github.com/warner/python-versioneer/issues).\n\n### Subprojects\n\nVersioneer has limited support for source trees in which `setup.py` is not in\nthe root directory (e.g. `setup.py` and `.git/` are *not* siblings). The are\ntwo common reasons why `setup.py` might not be in the root:\n\n* Source trees which contain multiple subprojects, such as\n  [Buildbot](https://github.com/buildbot/buildbot), which contains both\n  ""master"" and ""slave"" subprojects, each with their own `setup.py`,\n  `setup.cfg`, and `tox.ini`. Projects like these produce multiple PyPI\n  distributions (and upload multiple independently-installable tarballs).\n* Source trees whose main purpose is to contain a C library, but which also\n  provide bindings to Python (and perhaps other langauges) in subdirectories.\n\nVersioneer will look for `.git` in parent directories, and most operations\nshould get the right version string. However `pip` and `setuptools` have bugs\nand implementation details which frequently cause `pip install .` from a\nsubproject directory to fail to find a correct version string (so it usually\ndefaults to `0+unknown`).\n\n`pip install --editable .` should work correctly. `setup.py install` might\nwork too.\n\nPip-8.1.1 is known to have this problem, but hopefully it will get fixed in\nsome later version.\n\n[Bug #38](https://github.com/warner/python-versioneer/issues/38) is tracking\nthis issue. The discussion in\n[PR #61](https://github.com/warner/python-versioneer/pull/61) describes the\nissue from the Versioneer side in more detail.\n[pip PR#3176](https://github.com/pypa/pip/pull/3176) and\n[pip PR#3615](https://github.com/pypa/pip/pull/3615) contain work to improve\npip to let Versioneer work correctly.\n\nVersioneer-0.16 and earlier only looked for a `.git` directory next to the\n`setup.cfg`, so subprojects were completely unsupported with those releases.\n\n### Editable installs with setuptools <= 18.5\n\n`setup.py develop` and `pip install --editable .` allow you to install a\nproject into a virtualenv once, then continue editing the source code (and\ntest) without re-installing after every change.\n\n""Entry-point scripts"" (`setup(entry_points={""console_scripts"": ..})`) are a\nconvenient way to specify executable scripts that should be installed along\nwith the python package.\n\nThese both work as expected when using modern setuptools. When using\nsetuptools-18.5 or earlier, however, certain operations will cause\n`pkg_resources.DistributionNotFound` errors when running the entrypoint\nscript, which must be resolved by re-installing the package. This happens\nwhen the install happens with one version, then the egg_info data is\nregenerated while a different version is checked out. Many setup.py commands\ncause egg_info to be rebuilt (including `sdist`, `wheel`, and installing into\na different virtualenv), so this can be surprising.\n\n[Bug #83](https://github.com/warner/python-versioneer/issues/83) describes\nthis one, but upgrading to a newer version of setuptools should probably\nresolve it.\n\n### Unicode version strings\n\nWhile Versioneer works (and is continually tested) with both Python 2 and\nPython 3, it is not entirely consistent with bytes-vs-unicode distinctions.\nNewer releases probably generate unicode version strings on py2. It\'s not\nclear that this is wrong, but it may be surprising for applications when then\nwrite these strings to a network connection or include them in bytes-oriented\nAPIs like cryptographic checksums.\n\n[Bug #71](https://github.com/warner/python-versioneer/issues/71) investigates\nthis question.\n\n\n## Updating Versioneer\n\nTo upgrade your project to a new release of Versioneer, do the following:\n\n* install the new Versioneer (`pip install -U versioneer` or equivalent)\n* edit `setup.cfg`, if necessary, to include any new configuration settings\n  indicated by the release notes. See [UPGRADING](./UPGRADING.md) for details.\n* re-run `versioneer install` in your source tree, to replace\n  `SRC/_version.py`\n* commit any changed files\n\n## Future Directions\n\nThis tool is designed to make it easily extended to other version-control\nsystems: all VCS-specific components are in separate directories like\nsrc/git/ . The top-level `versioneer.py` script is assembled from these\ncomponents by running make-versioneer.py . In the future, make-versioneer.py\nwill take a VCS name as an argument, and will construct a version of\n`versioneer.py` that is specific to the given VCS. It might also take the\nconfiguration arguments that are currently provided manually during\ninstallation by editing setup.py . Alternatively, it might go the other\ndirection and include code from all supported VCS systems, reducing the\nnumber of intermediate scripts.\n\n\n## License\n\nTo make Versioneer easier to embed, all its code is dedicated to the public\ndomain. The `_version.py` that it creates is also in the public domain.\nSpecifically, both are released under the Creative Commons ""Public Domain\nDedication"" license (CC0-1.0), as described in\nhttps://creativecommons.org/publicdomain/zero/1.0/ .\n\n""""""\n\nfrom __future__ import print_function\n\ntry:\n    import configparser\nexcept ImportError:\n    import ConfigParser as configparser\nimport errno\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_root():\n    """"""Get the project root directory.\n\n    We require that all commands are run from the project root, i.e. the\n    directory that contains setup.py, setup.cfg, and versioneer.py .\n    """"""\n    root = os.path.realpath(os.path.abspath(os.getcwd()))\n    setup_py = os.path.join(root, ""setup.py"")\n    versioneer_py = os.path.join(root, ""versioneer.py"")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        # allow \'python path/to/setup.py COMMAND\'\n        root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n        setup_py = os.path.join(root, ""setup.py"")\n        versioneer_py = os.path.join(root, ""versioneer.py"")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        err = (\n            ""Versioneer was unable to run the project root directory. ""\n            ""Versioneer requires setup.py to be executed from ""\n            ""its immediate directory (like \'python setup.py COMMAND\'), ""\n            ""or in a way that lets it use sys.argv[0] to find the root ""\n            ""(like \'python path/to/setup.py COMMAND\').""\n        )\n        raise VersioneerBadRootError(err)\n    try:\n        # Certain runtime workflows (setup.py install/develop in a setuptools\n        # tree) execute all dependencies in a single python process, so\n        # ""versioneer"" may be imported multiple times, and python\'s shared\n        # module-import table will cache the first one. So we can\'t use\n        # os.path.dirname(__file__), as that will find whichever\n        # versioneer.py was first imported, even in later projects.\n        me = os.path.realpath(os.path.abspath(__file__))\n        me_dir = os.path.normcase(os.path.splitext(me)[0])\n        vsr_dir = os.path.normcase(os.path.splitext(versioneer_py)[0])\n        if me_dir != vsr_dir:\n            print(\n                ""Warning: build in %s is using versioneer.py from %s""\n                % (os.path.dirname(me), versioneer_py)\n            )\n    except NameError:\n        pass\n    return root\n\n\ndef get_config_from_root(root):\n    """"""Read the project setup.cfg file to determine Versioneer config.""""""\n    # This might raise EnvironmentError (if setup.cfg is missing), or\n    # configparser.NoSectionError (if it lacks a [versioneer] section), or\n    # configparser.NoOptionError (if it lacks ""VCS=""). See the docstring at\n    # the top of versioneer.py for instructions on writing your setup.cfg .\n    setup_cfg = os.path.join(root, ""setup.cfg"")\n    parser = configparser.SafeConfigParser()\n    with open(setup_cfg, ""r"") as f:\n        parser.readfp(f)\n    VCS = parser.get(""versioneer"", ""VCS"")  # mandatory\n\n    def get(parser, name):\n        if parser.has_option(""versioneer"", name):\n            return parser.get(""versioneer"", name)\n        return None\n\n    cfg = VersioneerConfig()\n    cfg.VCS = VCS\n    cfg.style = get(parser, ""style"") or """"\n    cfg.versionfile_source = get(parser, ""versionfile_source"")\n    cfg.versionfile_build = get(parser, ""versionfile_build"")\n    cfg.tag_prefix = get(parser, ""tag_prefix"")\n    if cfg.tag_prefix in (""\'\'"", \'""""\'):\n        cfg.tag_prefix = """"\n    cfg.parentdir_prefix = get(parser, ""parentdir_prefix"")\n    cfg.verbose = get(parser, ""verbose"")\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\n# these dictionaries contain VCS-specific tools\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen(\n                [c] + args,\n                cwd=cwd,\n                env=env,\n                stdout=subprocess.PIPE,\n                stderr=(subprocess.PIPE if hide_stderr else None),\n            )\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %s"" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %s"" % (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %s (error)"" % dispcmd)\n            print(""stdout was %s"" % stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\nLONG_VERSION_PY[\n    ""git""\n] = \'\'\'\n# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.18 (https://github.com/warner/python-versioneer)\n\n""""""Git implementation of _version.py.""""""\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    """"""Get the keywords needed to look up the version information.""""""\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = ""%(DOLLAR)sFormat:%%d%(DOLLAR)s""\n    git_full = ""%(DOLLAR)sFormat:%%H%(DOLLAR)s""\n    git_date = ""%(DOLLAR)sFormat:%%ci%(DOLLAR)s""\n    keywords = {""refnames"": git_refnames, ""full"": git_full, ""date"": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_config():\n    """"""Create, populate and return the VersioneerConfig() object.""""""\n    # these strings are filled in when \'setup.py versioneer\' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = ""git""\n    cfg.style = ""%(STYLE)s""\n    cfg.tag_prefix = ""%(TAG_PREFIX)s""\n    cfg.parentdir_prefix = ""%(PARENTDIR_PREFIX)s""\n    cfg.versionfile_source = ""%(VERSIONFILE_SOURCE)s""\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %%s"" %% dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %%s"" %% (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %%s (error)"" %% dispcmd)\n            print(""stdout was %%s"" %% stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {""version"": dirname[len(parentdir_prefix):],\n                    ""full-revisionid"": None,\n                    ""dirty"": False, ""error"": None, ""date"": None}\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(""Tried directories %%s but none started with prefix %%s"" %%\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %%d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r\'\\d\', r)])\n        if verbose:\n            print(""discarding \'%%s\', no digits"" %% "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %%s"" %% "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(""picking %%s"" %% r)\n            return {""version"": r,\n                    ""full-revisionid"": keywords[""full""].strip(),\n                    ""dirty"": False, ""error"": None,\n                    ""date"": date}\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {""version"": ""0+unknown"",\n            ""full-revisionid"": keywords[""full""].strip(),\n            ""dirty"": False, ""error"": ""no suitable tags"", ""date"": None}\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root,\n                          hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %%s not under git control"" %% root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(GITS, [""describe"", ""--tags"", ""--dirty"",\n                                          ""--always"", ""--long"",\n                                          ""--match"", ""%%s*"" %% tag_prefix],\n                                   cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\'^(.+)-(\\d+)-g([0-9a-f]+)$\', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = (""unable to parse git-describe output: \'%%s\'""\n                               %% describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%%s\' doesn\'t start with prefix \'%%s\'""\n                print(fmt %% (full_tag, tag_prefix))\n            pieces[""error""] = (""tag \'%%s\' doesn\'t start with prefix \'%%s\'""\n                               %% (full_tag, tag_prefix))\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""],\n                                    cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%%ci"", ""HEAD""],\n                       cwd=root)[0].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%%d.g%%s"" %% (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%%d.g%%s"" %% (pieces[""distance""],\n                                          pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%%d"" %% pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%%d"" %% pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%%d"" %% pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%%s"" %% pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%%d"" %% pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%%s"" %% pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%%d"" %% pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%%d"" %% pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%%d-g%%s"" %% (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%%d-g%%s"" %% (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {""version"": ""unknown"",\n                ""full-revisionid"": pieces.get(""long""),\n                ""dirty"": None,\n                ""error"": pieces[""error""],\n                ""date"": None}\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%%s\'"" %% style)\n\n    return {""version"": rendered, ""full-revisionid"": pieces[""long""],\n            ""dirty"": pieces[""dirty""], ""error"": None,\n            ""date"": pieces.get(""date"")}\n\n\ndef get_versions():\n    """"""Get version information or return default if unable to do so.""""""\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don\'t do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split(\'/\'):\n            root = os.path.dirname(root)\n    except NameError:\n        return {""version"": ""0+unknown"", ""full-revisionid"": None,\n                ""dirty"": None,\n                ""error"": ""unable to find root of source tree"",\n                ""date"": None}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {""version"": ""0+unknown"", ""full-revisionid"": None,\n            ""dirty"": None,\n            ""error"": ""unable to compute version"", ""date"": None}\n\'\'\'\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG) :] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r""\\d"", r)])\n        if verbose:\n            print(""discarding \'%s\', no digits"" % "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %s"" % "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix) :]\n            if verbose:\n                print(""picking %s"" % r)\n            return {\n                ""version"": r,\n                ""full-revisionid"": keywords[""full""].strip(),\n                ""dirty"": False,\n                ""error"": None,\n                ""date"": date,\n            }\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {\n        ""version"": ""0+unknown"",\n        ""full-revisionid"": keywords[""full""].strip(),\n        ""dirty"": False,\n        ""error"": ""no suitable tags"",\n        ""date"": None,\n    }\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root, hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %s not under git control"" % root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(\n        GITS,\n        [\n            ""describe"",\n            ""--tags"",\n            ""--dirty"",\n            ""--always"",\n            ""--long"",\n            ""--match"",\n            ""%s*"" % tag_prefix,\n        ],\n        cwd=root,\n    )\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[: git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r""^(.+)-(\\d+)-g([0-9a-f]+)$"", git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = ""unable to parse git-describe output: \'%s\'"" % describe_out\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                print(fmt % (full_tag, tag_prefix))\n            pieces[""error""] = ""tag \'%s\' doesn\'t start with prefix \'%s\'"" % (\n                full_tag,\n                tag_prefix,\n            )\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix) :]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""], cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%ci"", ""HEAD""], cwd=root)[\n        0\n    ].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef do_vcs_install(manifest_in, versionfile_source, ipy):\n    """"""Git-specific installation logic for Versioneer.\n\n    For Git, this means creating/changing .gitattributes to mark _version.py\n    for export-subst keyword substitution.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n    files = [manifest_in, versionfile_source]\n    if ipy:\n        files.append(ipy)\n    try:\n        me = __file__\n        if me.endswith("".pyc"") or me.endswith("".pyo""):\n            me = os.path.splitext(me)[0] + "".py""\n        versioneer_file = os.path.relpath(me)\n    except NameError:\n        versioneer_file = ""versioneer.py""\n    files.append(versioneer_file)\n    present = False\n    try:\n        f = open("".gitattributes"", ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(versionfile_source):\n                if ""export-subst"" in line.strip().split()[1:]:\n                    present = True\n        f.close()\n    except EnvironmentError:\n        pass\n    if not present:\n        f = open("".gitattributes"", ""a+"")\n        f.write(""%s export-subst\\n"" % versionfile_source)\n        f.close()\n        files.append("".gitattributes"")\n    run_command(GITS, [""add"", ""--""] + files)\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\n                ""version"": dirname[len(parentdir_prefix) :],\n                ""full-revisionid"": None,\n                ""dirty"": False,\n                ""error"": None,\n                ""date"": None,\n            }\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\n            ""Tried directories %s but none started with prefix %s""\n            % (str(rootdirs), parentdir_prefix)\n        )\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\nSHORT_VERSION_PY = """"""\n# This file was generated by \'versioneer.py\' (0.18) from\n# revision-control system data, or from the parent directory name of an\n# unpacked source archive. Distribution tarballs contain a pre-generated copy\n# of this file.\n\nimport json\n\nversion_json = \'\'\'\n%s\n\'\'\'  # END VERSION_JSON\n\n\ndef get_versions():\n    return json.loads(version_json)\n""""""\n\n\ndef versions_from_file(filename):\n    """"""Try to determine the version from _version.py if present.""""""\n    try:\n        with open(filename) as f:\n            contents = f.read()\n    except EnvironmentError:\n        raise NotThisMethod(""unable to read _version.py"")\n    mo = re.search(\n        r""version_json = \'\'\'\\n(.*)\'\'\'  # END VERSION_JSON"", contents, re.M | re.S\n    )\n    if not mo:\n        mo = re.search(\n            r""version_json = \'\'\'\\r\\n(.*)\'\'\'  # END VERSION_JSON"", contents, re.M | re.S\n        )\n    if not mo:\n        raise NotThisMethod(""no version_json in _version.py"")\n    return json.loads(mo.group(1))\n\n\ndef write_to_version_file(filename, versions):\n    """"""Write the given version number to the given _version.py file.""""""\n    os.unlink(filename)\n    contents = json.dumps(versions, sort_keys=True, indent=1, separators=("","", "": ""))\n    with open(filename, ""w"") as f:\n        f.write(SHORT_VERSION_PY % contents)\n\n    print(""set %s to \'%s\'"" % (filename, versions[""version""]))\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%d.g%s"" % (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%d.g%s"" % (pieces[""distance""], pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%d"" % pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%d"" % pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%s"" % pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%s"" % pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {\n            ""version"": ""unknown"",\n            ""full-revisionid"": pieces.get(""long""),\n            ""dirty"": None,\n            ""error"": pieces[""error""],\n            ""date"": None,\n        }\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%s\'"" % style)\n\n    return {\n        ""version"": rendered,\n        ""full-revisionid"": pieces[""long""],\n        ""dirty"": pieces[""dirty""],\n        ""error"": None,\n        ""date"": pieces.get(""date""),\n    }\n\n\nclass VersioneerBadRootError(Exception):\n    """"""The project root directory is unknown or missing key files.""""""\n\n\ndef get_versions(verbose=False):\n    """"""Get the project version from whatever source is available.\n\n    Returns dict with two keys: \'version\' and \'full\'.\n    """"""\n    if ""versioneer"" in sys.modules:\n        # see the discussion in cmdclass.py:get_cmdclass()\n        del sys.modules[""versioneer""]\n\n    root = get_root()\n    cfg = get_config_from_root(root)\n\n    assert cfg.VCS is not None, ""please set [versioneer]VCS= in setup.cfg""\n    handlers = HANDLERS.get(cfg.VCS)\n    assert handlers, ""unrecognized VCS \'%s\'"" % cfg.VCS\n    verbose = verbose or cfg.verbose\n    assert (\n        cfg.versionfile_source is not None\n    ), ""please set versioneer.versionfile_source""\n    assert cfg.tag_prefix is not None, ""please set versioneer.tag_prefix""\n\n    versionfile_abs = os.path.join(root, cfg.versionfile_source)\n\n    # extract version from first of: _version.py, VCS command (e.g. \'git\n    # describe\'), parentdir. This is meant to work for developers using a\n    # source checkout, for users of a tarball created by \'setup.py sdist\',\n    # and for users of a tarball/zipball created by \'git archive\' or github\'s\n    # download-from-tag feature or the equivalent in other VCSes.\n\n    get_keywords_f = handlers.get(""get_keywords"")\n    from_keywords_f = handlers.get(""keywords"")\n    if get_keywords_f and from_keywords_f:\n        try:\n            keywords = get_keywords_f(versionfile_abs)\n            ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n            if verbose:\n                print(""got version from expanded keyword %s"" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        ver = versions_from_file(versionfile_abs)\n        if verbose:\n            print(""got version from file %s %s"" % (versionfile_abs, ver))\n        return ver\n    except NotThisMethod:\n        pass\n\n    from_vcs_f = handlers.get(""pieces_from_vcs"")\n    if from_vcs_f:\n        try:\n            pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n            ver = render(pieces, cfg.style)\n            if verbose:\n                print(""got version from VCS %s"" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        if cfg.parentdir_prefix:\n            ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n            if verbose:\n                print(""got version from parentdir %s"" % ver)\n            return ver\n    except NotThisMethod:\n        pass\n\n    if verbose:\n        print(""unable to compute version"")\n\n    return {\n        ""version"": ""0+unknown"",\n        ""full-revisionid"": None,\n        ""dirty"": None,\n        ""error"": ""unable to compute version"",\n        ""date"": None,\n    }\n\n\ndef get_version():\n    """"""Get the short version string for this project.""""""\n    return get_versions()[""version""]\n\n\ndef get_cmdclass():\n    """"""Get the custom setuptools/distutils subclasses used by Versioneer.""""""\n    if ""versioneer"" in sys.modules:\n        del sys.modules[""versioneer""]\n        # this fixes the ""python setup.py develop"" case (also \'install\' and\n        # \'easy_install .\'), in which subdependencies of the main project are\n        # built (using setup.py bdist_egg) in the same python process. Assume\n        # a main project A and a dependency B, which use different versions\n        # of Versioneer. A\'s setup.py imports A\'s Versioneer, leaving it in\n        # sys.modules by the time B\'s setup.py is executed, causing B to run\n        # with the wrong versioneer. Setuptools wraps the sub-dep builds in a\n        # sandbox that restores sys.modules to it\'s pre-build state, so the\n        # parent is protected against the child\'s ""import versioneer"". By\n        # removing ourselves from sys.modules here, before the child build\n        # happens, we protect the child from the parent\'s versioneer too.\n        # Also see https://github.com/warner/python-versioneer/issues/52\n\n    cmds = {}\n\n    # we add ""version"" to both distutils and setuptools\n    from distutils.core import Command\n\n    class cmd_version(Command):\n        description = ""report generated version string""\n        user_options = []\n        boolean_options = []\n\n        def initialize_options(self):\n            pass\n\n        def finalize_options(self):\n            pass\n\n        def run(self):\n            vers = get_versions(verbose=True)\n            print(""Version: %s"" % vers[""version""])\n            print("" full-revisionid: %s"" % vers.get(""full-revisionid""))\n            print("" dirty: %s"" % vers.get(""dirty""))\n            print("" date: %s"" % vers.get(""date""))\n            if vers[""error""]:\n                print("" error: %s"" % vers[""error""])\n\n    cmds[""version""] = cmd_version\n\n    # we override ""build_py"" in both distutils and setuptools\n    #\n    # most invocation pathways end up running build_py:\n    #  distutils/build -> build_py\n    #  distutils/install -> distutils/build ->..\n    #  setuptools/bdist_wheel -> distutils/install ->..\n    #  setuptools/bdist_egg -> distutils/install_lib -> build_py\n    #  setuptools/install -> bdist_egg ->..\n    #  setuptools/develop -> ?\n    #  pip install:\n    #   copies source tree to a tempdir before running egg_info/etc\n    #   if .git isn\'t copied too, \'git describe\' will fail\n    #   then does setup.py bdist_wheel, or sometimes setup.py install\n    #  setup.py egg_info -> ?\n\n    # we override different ""build_py"" commands for both environments\n    if ""setuptools"" in sys.modules:\n        from setuptools.command.build_py import build_py as _build_py\n    else:\n        from distutils.command.build_py import build_py as _build_py\n\n    class cmd_build_py(_build_py):\n        def run(self):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            _build_py.run(self)\n            # now locate _version.py in the new build/ directory and replace\n            # it with an updated value\n            if cfg.versionfile_build:\n                target_versionfile = os.path.join(self.build_lib, cfg.versionfile_build)\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n    cmds[""build_py""] = cmd_build_py\n\n    if ""cx_Freeze"" in sys.modules:  # cx_freeze enabled?\n        from cx_Freeze.dist import build_exe as _build_exe\n\n        # nczeczulin reports that py2exe won\'t like the pep440-style string\n        # as FILEVERSION, but it can be used for PRODUCTVERSION, e.g.\n        # setup(console=[{\n        #   ""version"": versioneer.get_version().split(""+"", 1)[0], # FILEVERSION\n        #   ""product_version"": versioneer.get_version(),\n        #   ...\n\n        class cmd_build_exe(_build_exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _build_exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, ""w"") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(\n                        LONG\n                        % {\n                            ""DOLLAR"": ""$"",\n                            ""STYLE"": cfg.style,\n                            ""TAG_PREFIX"": cfg.tag_prefix,\n                            ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                            ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                        }\n                    )\n\n        cmds[""build_exe""] = cmd_build_exe\n        del cmds[""build_py""]\n\n    if ""py2exe"" in sys.modules:  # py2exe enabled?\n        try:\n            from py2exe.distutils_buildexe import py2exe as _py2exe  # py3\n        except ImportError:\n            from py2exe.build_exe import py2exe as _py2exe  # py2\n\n        class cmd_py2exe(_py2exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _py2exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, ""w"") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(\n                        LONG\n                        % {\n                            ""DOLLAR"": ""$"",\n                            ""STYLE"": cfg.style,\n                            ""TAG_PREFIX"": cfg.tag_prefix,\n                            ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                            ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                        }\n                    )\n\n        cmds[""py2exe""] = cmd_py2exe\n\n    # we override different ""sdist"" commands for both environments\n    if ""setuptools"" in sys.modules:\n        from setuptools.command.sdist import sdist as _sdist\n    else:\n        from distutils.command.sdist import sdist as _sdist\n\n    class cmd_sdist(_sdist):\n        def run(self):\n            versions = get_versions()\n            self._versioneer_generated_versions = versions\n            # unless we update this, the command will keep using the old\n            # version\n            self.distribution.metadata.version = versions[""version""]\n            return _sdist.run(self)\n\n        def make_release_tree(self, base_dir, files):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            _sdist.make_release_tree(self, base_dir, files)\n            # now locate _version.py in the new base_dir directory\n            # (remembering that it may be a hardlink) and replace it with an\n            # updated value\n            target_versionfile = os.path.join(base_dir, cfg.versionfile_source)\n            print(""UPDATING %s"" % target_versionfile)\n            write_to_version_file(\n                target_versionfile, self._versioneer_generated_versions\n            )\n\n    cmds[""sdist""] = cmd_sdist\n\n    return cmds\n\n\nCONFIG_ERROR = """"""\nsetup.cfg is missing the necessary Versioneer configuration. You need\na section like:\n\n [versioneer]\n VCS = git\n style = pep440\n versionfile_source = src/myproject/_version.py\n versionfile_build = myproject/_version.py\n tag_prefix =\n parentdir_prefix = myproject-\n\nYou will also need to edit your setup.py to use the results:\n\n import versioneer\n setup(version=versioneer.get_version(),\n       cmdclass=versioneer.get_cmdclass(), ...)\n\nPlease read the docstring in ./versioneer.py for configuration instructions,\nedit setup.cfg, and re-run the installer or \'python versioneer.py setup\'.\n""""""\n\nSAMPLE_CONFIG = """"""\n# See the docstring in versioneer.py for instructions. Note that you must\n# re-run \'versioneer.py setup\' after changing this section, and commit the\n# resulting files.\n\n[versioneer]\n#VCS = git\n#style = pep440\n#versionfile_source =\n#versionfile_build =\n#tag_prefix =\n#parentdir_prefix =\n\n""""""\n\nINIT_PY_SNIPPET = """"""\nfrom ._version import get_versions\n__version__ = get_versions()[\'version\']\ndel get_versions\n""""""\n\n\ndef do_setup():\n    """"""Main VCS-independent setup function for installing Versioneer.""""""\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (\n        EnvironmentError,\n        configparser.NoSectionError,\n        configparser.NoOptionError,\n    ) as e:\n        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):\n            print(""Adding sample versioneer config to setup.cfg"", file=sys.stderr)\n            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return 1\n\n    print("" creating %s"" % cfg.versionfile_source)\n    with open(cfg.versionfile_source, ""w"") as f:\n        LONG = LONG_VERSION_PY[cfg.VCS]\n        f.write(\n            LONG\n            % {\n                ""DOLLAR"": ""$"",\n                ""STYLE"": cfg.style,\n                ""TAG_PREFIX"": cfg.tag_prefix,\n                ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n            }\n        )\n\n    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), ""__init__.py"")\n    if os.path.exists(ipy):\n        try:\n            with open(ipy, ""r"") as f:\n                old = f.read()\n        except EnvironmentError:\n            old = """"\n        if INIT_PY_SNIPPET not in old:\n            print("" appending to %s"" % ipy)\n            with open(ipy, ""a"") as f:\n                f.write(INIT_PY_SNIPPET)\n        else:\n            print("" %s unmodified"" % ipy)\n    else:\n        print("" %s doesn\'t exist, ok"" % ipy)\n        ipy = None\n\n    # Make sure both the top-level ""versioneer.py"" and versionfile_source\n    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so\n    # they\'ll be copied into source distributions. Pip won\'t be able to\n    # install the package without this.\n    manifest_in = os.path.join(root, ""MANIFEST.in"")\n    simple_includes = set()\n    try:\n        with open(manifest_in, ""r"") as f:\n            for line in f:\n                if line.startswith(""include ""):\n                    for include in line.split()[1:]:\n                        simple_includes.add(include)\n    except EnvironmentError:\n        pass\n    # That doesn\'t cover everything MANIFEST.in can do\n    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so\n    # it might give some false negatives. Appending redundant \'include\'\n    # lines is safe, though.\n    if ""versioneer.py"" not in simple_includes:\n        print("" appending \'versioneer.py\' to MANIFEST.in"")\n        with open(manifest_in, ""a"") as f:\n            f.write(""include versioneer.py\\n"")\n    else:\n        print("" \'versioneer.py\' already in MANIFEST.in"")\n    if cfg.versionfile_source not in simple_includes:\n        print(\n            "" appending versionfile_source (\'%s\') to MANIFEST.in""\n            % cfg.versionfile_source\n        )\n        with open(manifest_in, ""a"") as f:\n            f.write(""include %s\\n"" % cfg.versionfile_source)\n    else:\n        print("" versionfile_source already in MANIFEST.in"")\n\n    # Make VCS-specific changes. For git, this means creating/changing\n    # .gitattributes to mark _version.py for export-subst keyword\n    # substitution.\n    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)\n    return 0\n\n\ndef scan_setup_py():\n    """"""Validate the contents of setup.py against Versioneer\'s expectations.""""""\n    found = set()\n    setters = False\n    errors = 0\n    with open(""setup.py"", ""r"") as f:\n        for line in f.readlines():\n            if ""import versioneer"" in line:\n                found.add(""import"")\n            if ""versioneer.get_cmdclass()"" in line:\n                found.add(""cmdclass"")\n            if ""versioneer.get_version()"" in line:\n                found.add(""get_version"")\n            if ""versioneer.VCS"" in line:\n                setters = True\n            if ""versioneer.versionfile_source"" in line:\n                setters = True\n    if len(found) != 3:\n        print("""")\n        print(""Your setup.py appears to be missing some important items"")\n        print(""(but I might be wrong). Please make sure it has something"")\n        print(""roughly like the following:"")\n        print("""")\n        print("" import versioneer"")\n        print("" setup( version=versioneer.get_version(),"")\n        print(""        cmdclass=versioneer.get_cmdclass(),  ...)"")\n        print("""")\n        errors += 1\n    if setters:\n        print(""You should remove lines like \'versioneer.VCS = \' and"")\n        print(""\'versioneer.versionfile_source = \' . This configuration"")\n        print(""now lives in setup.cfg, and should be removed from setup.py"")\n        print("""")\n        errors += 1\n    return errors\n\n\nif __name__ == ""__main__"":\n    cmd = sys.argv[1]\n    if cmd == ""setup"":\n        errors = do_setup()\n        errors += scan_setup_py()\n        if errors:\n            sys.exit(1)\n'"
benchmarks/__init__.py,0,b''
benchmarks/benchmark_coo.py,4,"b'import numpy as np\n\nimport sparse\n\n\nclass MatrixMultiplySuite:\n    def setup(self):\n        np.random.seed(0)\n        self.x = sparse.random((100, 100), density=0.01)\n        self.y = sparse.random((100, 100), density=0.01)\n\n        self.x @ self.y  # Numba compilation\n\n    def time_matmul(self):\n        self.x @ self.y\n\n\nclass ElemwiseSuite:\n    def setup(self):\n        np.random.seed(0)\n        self.x = sparse.random((100, 100, 100), density=0.01)\n        self.y = sparse.random((100, 100, 100), density=0.01)\n\n        self.x + self.y  # Numba compilation\n\n    def time_add(self):\n        self.x + self.y\n\n    def time_mul(self):\n        self.x * self.y\n\n    def time_index(self):\n        self.x[5]\n\n\nclass ElemwiseBroadcastingSuite:\n    def setup(self):\n        np.random.seed(0)\n        self.x = sparse.random((100, 1, 100), density=0.01)\n        self.y = sparse.random((100, 100), density=0.01)\n\n    def time_add(self):\n        self.x + self.y\n\n    def time_mul(self):\n        self.x * self.y\n\n\nclass IndexingSuite:\n    def setup(self):\n        np.random.seed(0)\n        self.x = sparse.random((100, 100, 100), density=0.01)\n        self.x[5]  # Numba compilation\n\n    def time_index_scalar(self):\n        self.x[5]\n\n    def time_index_slice(self):\n        self.x[:50]\n\n    def time_index_slice2(self):\n        self.x[:50, :50]\n\n    def time_index_slice3(self):\n        self.x[:50, :50, :50]\n'"
benchmarks/benchmark_tensordot.py,11,"b'import numpy as np\nimport sparse\n\n\nclass TensordotSuiteDenseSparse:\n    """"""\n    Performance comparison for returntype=COO vs returntype=np.ndarray.\n    tensordot(np.ndarray, COO)\n    """"""\n\n    def setup(self):\n        np.random.seed(0)\n        self.n = np.random.random((100, 100))\n        self.s = sparse.random((100, 100, 100, 100), density=0.01)\n\n    def time_dense(self):\n        sparse.tensordot(self.n, self.s, axes=([0, 1], [0, 2]))\n\n    def time_sparse(self):\n        sparse.tensordot(self.n, self.s, axes=([0, 1], [0, 2]), return_type=sparse.COO)\n\n\nclass TensordotSuiteSparseSparse:\n    """"""\n    Performance comparison for returntype=COO vs returntype=np.ndarray.\n    tensordot(COO, COO)\n    """"""\n\n    def setup(self):\n        np.random.seed(0)\n        self.s1 = sparse.random((100, 100), density=0.01)\n        self.s2 = sparse.random((100, 100, 100, 100), density=0.01)\n\n    def time_dense(self):\n        sparse.tensordot(\n            self.s1, self.s2, axes=([0, 1], [0, 2]), return_type=np.ndarray\n        )\n\n    def time_sparse(self):\n        sparse.tensordot(self.s1, self.s2, axes=([0, 1], [0, 2]))\n\n\nclass TensordotSuiteSparseDense:\n    """"""\n    Performance comparison for returntype=COO vs returntype=np.ndarray.\n    tensordot(COO, np.ndarray)\n    """"""\n\n    def setup(self):\n        np.random.seed(0)\n        self.s = sparse.random((100, 100, 100, 100), density=0.01)\n        self.n = np.random.random((100, 100))\n\n    def time_dense(self):\n        sparse.tensordot(self.s, self.n, axes=([0, 1], [0, 1]))\n\n    def time_sparse(self):\n        sparse.tensordot(self.s, self.n, axes=([0, 1], [0, 1]), return_type=sparse.COO)\n'"
docs/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# sparse documentation build configuration file, created by\n# sphinx-quickstart on Fri Dec 29 20:58:03 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath(""..""))\nfrom sparse import __version__  # flake8: noqa E402\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\n\nextensions = [\n    ""sphinx.ext.autodoc"",\n    ""sphinx.ext.doctest"",\n    ""sphinx.ext.intersphinx"",\n    ""sphinx.ext.coverage"",\n    ""sphinx.ext.mathjax"",\n    ""sphinx.ext.napoleon"",\n    ""sphinx.ext.viewcode"",\n    ""sphinx.ext.autosummary"",\n    ""sphinx.ext.inheritance_diagram"",\n    ""sphinx.ext.extlinks"",\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [""_templates""]\n\nmathjax_path = (\n    ""https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML""\n)\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = "".rst""\n\n# The master toctree document.\nmaster_doc = ""index""\n\n# General information about the project.\nproject = ""sparse""\ncopyright = ""2018, Sparse developers""\nauthor = ""Sparse Developers""\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = __version__\n# The full version, including alpha/beta/rc tags.\nrelease = __version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [""_build"", ""**tests**"", ""**setup**"", ""**extern**"", ""**data**""]\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = ""sphinx""\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\nautosummary_generate = True\nautosummary_generate_overwrite = False\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""sphinx_rtd_theme""\nhtml_logo = ""logo.png""\nhtml_favicon = ""logo.png""\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\n# html_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# This is required for the alabaster theme\n# refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars\n# html_sidebars = {\n#     \'**\': [\n#         \'relations.html\',  # needs \'show_related\': True theme option to display\n#         \'searchbox.html\',\n#     ]\n# }\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = ""sparsedoc""\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, ""sparse.tex"", ""sparse Documentation"", ""Sparse Developers"", ""manual"")\n]\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, ""sparse"", ""sparse Documentation"", [author], 1)]\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        master_doc,\n        ""sparse"",\n        ""sparse Documentation"",\n        author,\n        ""sparse"",\n        ""One line description of project."",\n        ""Miscellaneous"",\n    )\n]\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    ""python"": (""https://docs.python.org/3/"", None),\n    ""numpy"": (""https://docs.scipy.org/doc/numpy/"", None),\n    ""scipy"": (""https://docs.scipy.org/doc/scipy/reference/"", None),\n}\n\nextlinks = {\n    ""issue"": (""https://github.com/pydata/sparse/issues/%s"", ""Issue #""),\n    ""pr"": (""https://github.com/pydata/sparse/pull/%s"", ""PR #""),\n    ""ghuser"": (""https://github.com/%s"", ""@""),\n}\n'"
sparse/__init__.py,0,"b'from ._coo import COO\nfrom ._dok import DOK\nfrom ._sparse_array import SparseArray\nfrom ._utils import random\nfrom ._io import save_npz, load_npz\nfrom ._common import *\n\nfrom ._version import get_versions\n\n__version__ = get_versions()[""version""]\ndel get_versions\n'"
sparse/_common.py,20,"b'import numpy as np\n\nfrom ._utils import check_compressed_axes\nfrom ._coo import (\n    clip,\n    tensordot,\n    dot,\n    matmul,\n    triu,\n    tril,\n    where,\n    nansum,\n    nanmean,\n    nanprod,\n    nanmin,\n    nanmax,\n    nanreduce,\n    roll,\n    kron,\n    argwhere,\n    isposinf,\n    isneginf,\n    result_type,\n    diagonal,\n    diagonalize,\n    elemwise,\n    as_coo,\n)\n\n\ndef stack(arrays, axis=0, compressed_axes=None):\n    """"""\n    Stack the input arrays along the given dimension.\n\n    Parameters\n    ----------\n    arrays : Iterable[SparseArray]\n        The input arrays to stack.\n    axis : int, optional\n        The axis along which to stack the input arrays.\n    compressed_axes : iterable, optional\n        The axes to compress if returning a GCXS array.\n\n    Returns\n    -------\n    SparseArray\n        The output stacked array.\n\n    Raises\n    ------\n    ValueError\n        If all elements of :code:`arrays` don\'t have the same fill-value.\n\n    See Also\n    --------\n    numpy.stack : NumPy equivalent function\n    """"""\n    from ._coo import COO\n\n    if any(isinstance(arr, COO) for arr in arrays):\n        from ._coo import stack as coo_stack\n\n        return coo_stack(arrays, axis)\n    else:\n        from ._compressed import stack as gcxs_stack\n\n        return gcxs_stack(arrays, axis, compressed_axes)\n\n\ndef concatenate(arrays, axis=0, compressed_axes=None):\n    """"""\n    Concatenate the input arrays along the given dimension.\n\n    Parameters\n    ----------\n    arrays : Iterable[SparseArray]\n        The input arrays to concatenate.\n    axis : int, optional\n        The axis along which to concatenate the input arrays. The default is zero.\n    compressed_axes : iterable, optional\n        The axes to compress if returning a GCXS array.\n\n    Returns\n    -------\n    SparseArray\n        The output concatenated array.\n\n    Raises\n    ------\n    ValueError\n        If all elements of :code:`arrays` don\'t have the same fill-value.\n\n    See Also\n    --------\n    numpy.concatenate : NumPy equivalent function\n    """"""\n    from ._coo import COO\n\n    if any(isinstance(arr, COO) for arr in arrays):\n        from ._coo import concatenate as coo_concat\n\n        return coo_concat(arrays, axis)\n    else:\n        from ._compressed import concatenate as gcxs_concat\n\n        return gcxs_concat(arrays, axis, compressed_axes)\n\n\ndef eye(N, M=None, k=0, dtype=float, format=""coo"", compressed_axes=None):\n    """"""Return a 2-D array in the specified format with ones on the diagonal and zeros elsewhere.\n\n    Parameters\n    ----------\n    N : int\n        Number of rows in the output.\n    M : int, optional\n        Number of columns in the output. If None, defaults to `N`.\n    k : int, optional\n        Index of the diagonal: 0 (the default) refers to the main diagonal,\n        a positive value refers to an upper diagonal, and a negative value\n        to a lower diagonal.\n    dtype : data-type, optional\n        Data-type of the returned array.\n    format : str, optional\n        A format string.\n    compressed_axes : iterable, optional\n        The axes to compress if returning a GCXS array.\n\n    Returns\n    -------\n    I : SparseArray of shape (N, M)\n        An array where all elements are equal to zero, except for the `k`-th\n        diagonal, whose values are equal to one.\n\n    Examples\n    --------\n    >>> eye(2, dtype=int).todense()  # doctest: +NORMALIZE_WHITESPACE\n    array([[1, 0],\n           [0, 1]])\n    >>> eye(3, k=1).todense()  # doctest: +SKIP\n    array([[0., 1., 0.],\n           [0., 0., 1.],\n           [0., 0., 0.]])\n    """"""\n    from sparse import COO\n\n    if M is None:\n        M = N\n\n    N = int(N)\n    M = int(M)\n    k = int(k)\n\n    data_length = min(N, M)\n\n    if k > 0:\n        data_length = max(min(data_length, M - k), 0)\n        n_coords = np.arange(data_length, dtype=np.intp)\n        m_coords = n_coords + k\n    elif k < 0:\n        data_length = max(min(data_length, N + k), 0)\n        m_coords = np.arange(data_length, dtype=np.intp)\n        n_coords = m_coords - k\n    else:\n        n_coords = m_coords = np.arange(data_length, dtype=np.intp)\n\n    coords = np.stack([n_coords, m_coords])\n    data = np.array(1, dtype=dtype)\n\n    return COO(\n        coords, data=data, shape=(N, M), has_duplicates=False, sorted=True\n    ).asformat(format, compressed_axes=compressed_axes)\n\n\ndef full(shape, fill_value, dtype=None, format=""coo"", compressed_axes=None):\n    """"""Return a SparseArray of given shape and type, filled with `fill_value`.\n\n    Parameters\n    ----------\n    shape : int or tuple of ints\n        Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n    fill_value : scalar\n        Fill value.\n    dtype : data-type, optional\n        The desired data-type for the array. The default, `None`, means\n        `np.array(fill_value).dtype`.\n    format : str, optional\n        A format string.\n    compressed_axes : iterable, optional\n        The axes to compress if returning a GCXS array.\n    Returns\n    -------\n    out : SparseArray\n        Array of `fill_value` with the given shape and dtype.\n\n    Examples\n    --------\n    >>> full(5, 9).todense()  # doctest: +NORMALIZE_WHITESPACE\n    array([9, 9, 9, 9, 9])\n\n    >>> full((2, 2), 9, dtype=float).todense()  # doctest: +SKIP\n    array([[9., 9.],\n           [9., 9.]])\n    """"""\n    from sparse import COO\n\n    if dtype is None:\n        dtype = np.array(fill_value).dtype\n    if not isinstance(shape, tuple):\n        shape = (shape,)\n    if compressed_axes is not None:\n        check_compressed_axes(shape, compressed_axes)\n    data = np.empty(0, dtype=dtype)\n    coords = np.empty((len(shape), 0), dtype=np.intp)\n    return COO(\n        coords,\n        data=data,\n        shape=shape,\n        fill_value=fill_value,\n        has_duplicates=False,\n        sorted=True,\n    ).asformat(format, compressed_axes=compressed_axes)\n\n\ndef full_like(a, fill_value, dtype=None, format=None, compressed_axes=None):\n    """"""Return a full array with the same shape and type as a given array.\n\n    Parameters\n    ----------\n    a : array_like\n        The shape and data-type of the result will match those of `a`.\n    dtype : data-type, optional\n        Overrides the data type of the result.\n    format : str, optional\n        A format string.\n    compressed_axes : iterable, optional\n        The axes to compress if returning a GCXS array.\n\n    Returns\n    -------\n    out : SparseArray\n        Array of `fill_value` with the same shape and type as `a`.\n\n    Examples\n    --------\n    >>> x = np.ones((2, 3), dtype=\'i8\')\n    >>> full_like(x, 9.0).todense()  # doctest: +NORMALIZE_WHITESPACE\n    array([[9, 9, 9],\n           [9, 9, 9]])\n    """"""\n    if format is None and not isinstance(a, np.ndarray):\n        format = type(a).__name__.lower()\n    else:\n        format = ""coo""\n    if hasattr(a, ""compressed_axes"") and compressed_axes is None:\n        compressed_axes = a.compressed_axes\n    return full(\n        a.shape,\n        fill_value,\n        dtype=(a.dtype if dtype is None else dtype),\n        format=format,\n        compressed_axes=compressed_axes,\n    )\n\n\ndef zeros(shape, dtype=float, format=""coo"", compressed_axes=None):\n    """"""Return a SparseArray of given shape and type, filled with zeros.\n\n    Parameters\n    ----------\n    shape : int or tuple of ints\n        Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n    dtype : data-type, optional\n        The desired data-type for the array, e.g., `numpy.int8`.  Default is\n        `numpy.float64`.\n    format : str, optional\n        A format string.\n    compressed_axes : iterable, optional\n        The axes to compress if returning a GCXS array.\n\n    Returns\n    -------\n    out : SparseArray\n        Array of zeros with the given shape and dtype.\n\n    Examples\n    --------\n    >>> zeros(5).todense()  # doctest: +SKIP\n    array([0., 0., 0., 0., 0.])\n\n    >>> zeros((2, 2), dtype=int).todense()  # doctest: +NORMALIZE_WHITESPACE\n    array([[0, 0],\n           [0, 0]])\n    """"""\n    if compressed_axes is not None:\n        check_compressed_axes(shape, compressed_axes)\n    return full(shape, 0, np.dtype(dtype)).asformat(\n        format, compressed_axes=compressed_axes\n    )\n\n\ndef zeros_like(a, dtype=None, format=None, compressed_axes=None):\n    """"""Return a SparseArray of zeros with the same shape and type as ``a``.\n\n    Parameters\n    ----------\n    a : array_like\n        The shape and data-type of the result will match those of `a`.\n    dtype : data-type, optional\n        Overrides the data type of the result.\n    format : str, optional\n        A format string.\n    compressed_axes : iterable, optional\n        The axes to compress if returning a GCXS array.\n\n    Returns\n    -------\n    out : SparseArray\n        Array of zeros with the same shape and type as `a`.\n\n    Examples\n    --------\n    >>> x = np.ones((2, 3), dtype=\'i8\')\n    >>> zeros_like(x).todense()  # doctest: +NORMALIZE_WHITESPACE\n    array([[0, 0, 0],\n           [0, 0, 0]])\n    """"""\n    if format is None and not isinstance(a, np.ndarray):\n        format = type(a).__name__.lower()\n    else:\n        format = ""coo""\n    if hasattr(a, ""compressed_axes"") and compressed_axes is None:\n        compressed_axes = a.compressed_axes\n    return zeros(\n        a.shape,\n        dtype=(a.dtype if dtype is None else dtype),\n        format=format,\n        compressed_axes=compressed_axes,\n    )\n\n\ndef ones(shape, dtype=float, format=""coo"", compressed_axes=None):\n    """"""Return a SparseArray of given shape and type, filled with ones.\n\n    Parameters\n    ----------\n    shape : int or tuple of ints\n        Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n    dtype : data-type, optional\n        The desired data-type for the array, e.g., `numpy.int8`.  Default is\n        `numpy.float64`.\n    format : str, optional\n        A format string.\n    compressed_axes : iterable, optional\n        The axes to compress if returning a GCXS array.\n\n    Returns\n    -------\n    out : SparseArray\n        Array of ones with the given shape and dtype.\n\n    Examples\n    --------\n    >>> ones(5).todense()  # doctest: +SKIP\n    array([1., 1., 1., 1., 1.])\n\n    >>> ones((2, 2), dtype=int).todense()  # doctest: +NORMALIZE_WHITESPACE\n    array([[1, 1],\n           [1, 1]])\n    """"""\n    if compressed_axes is not None:\n        check_compressed_axes(shape, compressed_axes)\n    return full(shape, 1, np.dtype(dtype)).asformat(\n        format, compressed_axes=compressed_axes\n    )\n\n\ndef ones_like(a, dtype=None, format=None, compressed_axes=None):\n    """"""Return a SparseArray of ones with the same shape and type as ``a``.\n\n    Parameters\n    ----------\n    a : array_like\n        The shape and data-type of the result will match those of `a`.\n    dtype : data-type, optional\n        Overrides the data type of the result.\n    format : str, optional\n        A format string.\n    compressed_axes : iterable, optional\n        The axes to compress if returning a GCXS array.\n\n    Returns\n    -------\n    out : SparseArray\n        Array of ones with the same shape and type as `a`.\n\n    Examples\n    --------\n    >>> x = np.ones((2, 3), dtype=\'i8\')\n    >>> ones_like(x).todense()  # doctest: +NORMALIZE_WHITESPACE\n    array([[1, 1, 1],\n           [1, 1, 1]])\n    """"""\n    if format is None and not isinstance(a, np.ndarray):\n        format = type(a).__name__.lower()\n    else:\n        format = ""coo""\n    if hasattr(a, ""compressed_axes"") and compressed_axes is None:\n        compressed_axes = a.compressed_axes\n    return ones(\n        a.shape,\n        dtype=(a.dtype if dtype is None else dtype),\n        format=format,\n        compressed_axes=compressed_axes,\n    )\n\n\ndef outer(a, b, out=None):\n    """"""\n    Return outer product of two sparse arrays.\n\n    Parameters\n    ----------\n    a, b : sparse.SparseArray\n        The input arrays.\n    out : sparse.SparseArray\n        The output array.\n    \n    Examples\n    --------\n    >>> import numpy as np\n    >>> import sparse\n    >>> a = sparse.COO(np.arange(4))\n    >>> o = sparse.outer(a, a)\n    >>> o.todense()\n    array([[0, 0, 0, 0],\n           [0, 1, 2, 3],\n           [0, 2, 4, 6],\n           [0, 3, 6, 9]])\n    """"""\n    from sparse import SparseArray, COO\n\n    if isinstance(a, SparseArray):\n        a = COO(a)\n    if isinstance(b, SparseArray):\n        b = COO(b)\n    return np.multiply.outer(a.flatten(), b.flatten(), out=out)\n\n\ndef asnumpy(a, dtype=None, order=None):\n    """"""Returns a dense numpy array from an arbitrary source array.\n\n    Args:\n        a: Arbitrary object that can be converted to :class:`numpy.ndarray`.\n        order ({\'C\', \'F\', \'A\'}): The desired memory layout of the output\n            array. When ``order`` is \'A\', it uses \'F\' if ``a`` is\n            fortran-contiguous and \'C\' otherwise.\n    Returns:\n        numpy.ndarray: Converted array on the host memory.\n    """"""\n    from ._sparse_array import SparseArray\n\n    if isinstance(a, SparseArray):\n        a = a.todense()\n    return np.array(a, dtype=dtype, copy=False, order=order)\n'"
sparse/_dok.py,15,"b'from numbers import Integral\n\nimport numpy as np\n\nfrom ._slicing import normalize_index\nfrom ._utils import equivalent\nfrom ._sparse_array import SparseArray\n\n\nclass DOK(SparseArray):\n    """"""\n    A class for building sparse multidimensional arrays.\n\n    Parameters\n    ----------\n    shape : tuple[int] (DOK.ndim,)\n        The shape of the array.\n    data : dict, optional\n        The key-value pairs for the data in this array.\n    dtype : np.dtype, optional\n        The data type of this array. If left empty, it is inferred from\n        the first element.\n    fill_value : scalar, optional\n        The fill value of this array.\n\n    Attributes\n    ----------\n    dtype : numpy.dtype\n        The datatype of this array. Can be :code:`None` if no elements\n        have been set yet.\n    shape : tuple[int]\n        The shape of this array.\n    data : dict\n        The keys of this dictionary contain all the indices and the values\n        contain the nonzero entries.\n\n    See Also\n    --------\n    COO : A read-only sparse array.\n\n    Examples\n    --------\n    You can create :obj:`DOK` objects from Numpy arrays.\n\n    >>> x = np.eye(5, dtype=np.uint8)\n    >>> x[2, 3] = 5\n    >>> s = DOK.from_numpy(x)\n    >>> s\n    <DOK: shape=(5, 5), dtype=uint8, nnz=6, fill_value=0>\n\n    You can also create them from just shapes, and use slicing assignment.\n\n    >>> s2 = DOK((5, 5), dtype=np.int64)\n    >>> s2[1:3, 1:3] = [[4, 5], [6, 7]]\n    >>> s2\n    <DOK: shape=(5, 5), dtype=int64, nnz=4, fill_value=0>\n\n    You can convert :obj:`DOK` arrays to :obj:`COO` arrays, or :obj:`numpy.ndarray`\n    objects.\n\n    >>> from sparse import COO\n    >>> s3 = COO(s2)\n    >>> s3\n    <COO: shape=(5, 5), dtype=int64, nnz=4, fill_value=0>\n    >>> s2.todense()  # doctest: +NORMALIZE_WHITESPACE\n    array([[0, 0, 0, 0, 0],\n           [0, 4, 5, 0, 0],\n           [0, 6, 7, 0, 0],\n           [0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0]])\n\n    >>> s4 = COO.from_numpy(np.eye(4, dtype=np.uint8))\n    >>> s4\n    <COO: shape=(4, 4), dtype=uint8, nnz=4, fill_value=0>\n    >>> s5 = DOK.from_coo(s4)\n    >>> s5\n    <DOK: shape=(4, 4), dtype=uint8, nnz=4, fill_value=0>\n\n    You can also create :obj:`DOK` arrays from a shape and a dict of\n    values. Zeros are automatically ignored.\n\n    >>> values = {\n    ...     (1, 2, 3): 4,\n    ...     (3, 2, 1): 0,\n    ... }\n    >>> s6 = DOK((5, 5, 5), values)\n    >>> s6\n    <DOK: shape=(5, 5, 5), dtype=int64, nnz=1, fill_value=0.0>\n    """"""\n\n    def __init__(self, shape, data=None, dtype=None, fill_value=None):\n        from ._coo import COO\n\n        self.data = dict()\n\n        if isinstance(shape, COO):\n            ar = DOK.from_coo(shape)\n            self._make_shallow_copy_of(ar)\n            return\n\n        if isinstance(shape, np.ndarray):\n            ar = DOK.from_numpy(shape)\n            self._make_shallow_copy_of(ar)\n            return\n\n        self.dtype = np.dtype(dtype)\n\n        if not data:\n            data = dict()\n\n        super().__init__(shape, fill_value=fill_value)\n\n        if isinstance(data, dict):\n            if not dtype:\n                if not len(data):\n                    self.dtype = np.dtype(""float64"")\n                else:\n                    self.dtype = np.result_type(\n                        *map(lambda x: np.asarray(x).dtype, data.values())\n                    )\n\n            for c, d in data.items():\n                self[c] = d\n        else:\n            raise ValueError(""data must be a dict."")\n\n    def _make_shallow_copy_of(self, other):\n        self.dtype = other.dtype\n        self.data = other.data\n        super().__init__(other.shape, fill_value=other.fill_value)\n\n    @classmethod\n    def from_coo(cls, x):\n        """"""\n        Get a :obj:`DOK` array from a :obj:`COO` array.\n\n        Parameters\n        ----------\n        x : COO\n            The array to convert.\n\n        Returns\n        -------\n        DOK\n            The equivalent :obj:`DOK` array.\n\n        Examples\n        --------\n        >>> from sparse import COO\n        >>> s = COO.from_numpy(np.eye(4))\n        >>> s2 = DOK.from_coo(s)\n        >>> s2\n        <DOK: shape=(4, 4), dtype=float64, nnz=4, fill_value=0.0>\n        """"""\n        ar = cls(x.shape, dtype=x.dtype, fill_value=x.fill_value)\n\n        for c, d in zip(x.coords.T, x.data):\n            ar.data[tuple(c)] = d\n\n        return ar\n\n    def to_coo(self):\n        """"""\n        Convert this :obj:`DOK` array to a :obj:`COO` array.\n\n        Returns\n        -------\n        COO\n            The equivalent :obj:`COO` array.\n\n        Examples\n        --------\n        >>> s = DOK((5, 5))\n        >>> s[1:3, 1:3] = [[4, 5], [6, 7]]\n        >>> s\n        <DOK: shape=(5, 5), dtype=float64, nnz=4, fill_value=0.0>\n        >>> s2 = s.to_coo()\n        >>> s2\n        <COO: shape=(5, 5), dtype=float64, nnz=4, fill_value=0.0>\n        """"""\n        from ._coo import COO\n\n        return COO(self)\n\n    @classmethod\n    def from_numpy(cls, x):\n        """"""\n        Get a :obj:`DOK` array from a Numpy array.\n\n        Parameters\n        ----------\n        x : np.ndarray\n            The array to convert.\n\n        Returns\n        -------\n        DOK\n            The equivalent :obj:`DOK` array.\n\n        Examples\n        --------\n        >>> s = DOK.from_numpy(np.eye(4))\n        >>> s\n        <DOK: shape=(4, 4), dtype=float64, nnz=4, fill_value=0.0>\n        """"""\n        ar = cls(x.shape, dtype=x.dtype)\n\n        coords = np.nonzero(x)\n        data = x[coords]\n\n        for c in zip(data, *coords):\n            d, c = c[0], c[1:]\n            ar.data[c] = d\n\n        return ar\n\n    @property\n    def nnz(self):\n        """"""\n        The number of nonzero elements in this array.\n\n        Returns\n        -------\n        int\n            The number of nonzero elements.\n\n        See Also\n        --------\n        COO.nnz : Equivalent :obj:`COO` array property.\n        numpy.count_nonzero : A similar Numpy function.\n        scipy.sparse.dok_matrix.nnz : The Scipy equivalent property.\n\n        Examples\n        --------\n        >>> values = {\n        ...     (1, 2, 3): 4,\n        ...     (3, 2, 1): 0,\n        ... }\n        >>> s = DOK((5, 5, 5), values)\n        >>> s.nnz\n        1\n        """"""\n        return len(self.data)\n\n    def __getitem__(self, key):\n        key = normalize_index(key, self.shape)\n\n        if not all(isinstance(i, Integral) for i in key):\n            raise NotImplementedError(\n                ""All indices must be integers"" "" when getting an item.""\n            )\n\n        if len(key) != self.ndim:\n            raise NotImplementedError(\n                ""Can only get single elements. ""\n                ""Expected key of length %d, got %s"" % (self.ndim, str(key))\n            )\n\n        key = tuple(int(k) for k in key)\n\n        if key in self.data:\n            return self.data[key]\n        else:\n            return self.fill_value\n\n    def __setitem__(self, key, value):\n        key = normalize_index(key, self.shape)\n        value = np.asanyarray(value)\n\n        value = value.astype(self.dtype)\n\n        key_list = [int(k) if isinstance(k, Integral) else k for k in key]\n\n        self._setitem(key_list, value)\n\n    def _setitem(self, key_list, value):\n        value_missing_dims = (\n            len([ind for ind in key_list if isinstance(ind, slice)]) - value.ndim\n        )\n\n        if value_missing_dims < 0:\n            raise ValueError(""setting an array element with a sequence."")\n\n        for i, ind in enumerate(key_list):\n            if isinstance(ind, slice):\n                step = ind.step if ind.step is not None else 1\n                if step > 0:\n                    start = ind.start if ind.start is not None else 0\n                    start = max(start, 0)\n                    stop = ind.stop if ind.stop is not None else self.shape[i]\n                    stop = min(stop, self.shape[i])\n                    if start > stop:\n                        start = stop\n                else:\n                    start = ind.start or self.shape[i] - 1\n                    stop = ind.stop if ind.stop is not None else -1\n                    start = min(start, self.shape[i] - 1)\n                    stop = max(stop, -1)\n                    if start < stop:\n                        start = stop\n\n                key_list_temp = key_list[:]\n                for v_idx, ki in enumerate(range(start, stop, step)):\n                    key_list_temp[i] = ki\n                    vi = (\n                        value\n                        if value_missing_dims > 0\n                        else (value[0] if value.shape[0] == 1 else value[v_idx])\n                    )\n                    self._setitem(key_list_temp, vi)\n\n                return\n            elif not isinstance(ind, Integral):\n                raise IndexError(\n                    ""All indices must be slices or integers"" "" when setting an item.""\n                )\n\n        key = tuple(key_list)\n        if not equivalent(value, self.fill_value):\n            self.data[key] = value[()]\n        elif key in self.data:\n            del self.data[key]\n\n    def __str__(self):\n        return ""<DOK: shape={!s}, dtype={!s}, nnz={:d}, fill_value={!s}>"".format(\n            self.shape, self.dtype, self.nnz, self.fill_value\n        )\n\n    __repr__ = __str__\n\n    def todense(self):\n        """"""\n        Convert this :obj:`DOK` array into a Numpy array.\n\n        Returns\n        -------\n        numpy.ndarray\n            The equivalent dense array.\n\n        See Also\n        --------\n        COO.todense : Equivalent :obj:`COO` array method.\n        scipy.sparse.dok_matrix.todense : Equivalent Scipy method.\n\n        Examples\n        --------\n        >>> s = DOK((5, 5))\n        >>> s[1:3, 1:3] = [[4, 5], [6, 7]]\n        >>> s.todense()  # doctest: +SKIP\n        array([[0., 0., 0., 0., 0.],\n               [0., 4., 5., 0., 0.],\n               [0., 6., 7., 0., 0.],\n               [0., 0., 0., 0., 0.],\n               [0., 0., 0., 0., 0.]])\n        """"""\n        result = np.full(self.shape, self.fill_value, self.dtype)\n\n        for c, d in self.data.items():\n            result[c] = d\n\n        return result\n\n    def asformat(self, format):\n        """"""\n        Convert this sparse array to a given format.\n\n        Parameters\n        ----------\n        format : str\n            A format string.\n\n        Returns\n        -------\n        out : SparseArray\n            The converted array.\n\n        Raises\n        ------\n        NotImplementedError\n            If the format isn\'t supported.\n        """"""\n        if format == ""dok"" or format is DOK:\n            return self\n\n        from ._coo import COO\n\n        if format == ""coo"" or format is COO:\n            return COO.from_iter(\n                self.data,\n                shape=self.shape,\n                fill_value=self.fill_value,\n                dtype=self.dtype,\n            )\n\n        raise NotImplementedError(""The given format is not supported."")\n'"
sparse/_io.py,4,"b'import numpy as np\n\nfrom ._coo.core import COO\n\n\ndef save_npz(filename, matrix, compressed=True):\n    """""" Save a sparse matrix to disk in numpy\'s ``.npz`` format.\n    Note: This is not binary compatible with scipy\'s ``save_npz()``.\n    Will save a file that can only be opend with this package\'s ``load_npz()``.\n\n    Parameters\n    ----------\n    filename : string or file\n        Either the file name (string) or an open file (file-like object)\n        where the data will be saved. If file is a string or a Path, the\n        ``.npz`` extension will be appended to the file name if it is not\n        already there\n    matrix : COO\n        The matrix to save to disk\n    compressed : bool\n        Whether to save in compressed or uncompressed mode\n\n    Example\n    --------\n    Store sparse matrix to disk, and load it again:\n\n    >>> import os\n    >>> import sparse\n    >>> import numpy as np\n    >>> dense_mat = np.array([[[0., 0.], [0., 0.70677779]], [[0., 0.], [0., 0.86522495]]])\n    >>> mat = sparse.COO(dense_mat)\n    >>> mat\n    <COO: shape=(2, 2, 2), dtype=float64, nnz=2, fill_value=0.0>\n    >>> sparse.save_npz(\'mat.npz\', mat)\n    >>> loaded_mat = sparse.load_npz(\'mat.npz\')\n    >>> loaded_mat\n    <COO: shape=(2, 2, 2), dtype=float64, nnz=2, fill_value=0.0>\n    >>> os.remove(\'mat.npz\')\n\n    See Also\n    --------\n    load_npz\n    scipy.sparse.save_npz\n    scipy.sparse.load_npz\n    numpy.savez\n    numpy.load\n\n    """"""\n\n    nodes = {\n        ""data"": matrix.data,\n        ""coords"": matrix.coords,\n        ""shape"": matrix.shape,\n        ""fill_value"": matrix.fill_value,\n    }\n\n    if compressed:\n        np.savez_compressed(filename, **nodes)\n    else:\n        np.savez(filename, **nodes)\n\n\ndef load_npz(filename):\n    """""" Load a sparse matrix in numpy\'s ``.npz`` format from disk.\n    Note: This is not binary compatible with scipy\'s ``save_npz()``\n    output. Will only load files saved by this package.\n\n    Parameters\n    ----------\n    filename : file-like object, string, or pathlib.Path\n        The file to read. File-like objects must support the\n        ``seek()`` and ``read()`` methods.\n\n    Returns\n    -------\n    COO\n        The sparse matrix at path ``filename``\n\n    Example\n    --------\n    See :obj:`save_npz` for usage examples.\n\n    See Also\n    --------\n    save_npz\n    scipy.sparse.save_npz\n    scipy.sparse.load_npz\n    numpy.savez\n    numpy.load\n\n    """"""\n\n    with np.load(filename) as fp:\n        try:\n            coords = fp[""coords""]\n            data = fp[""data""]\n            shape = tuple(fp[""shape""])\n            fill_value = fp[""fill_value""][()]\n            return COO(\n                coords=coords,\n                data=data,\n                shape=shape,\n                sorted=True,\n                has_duplicates=False,\n                fill_value=fill_value,\n            )\n        except KeyError:\n            raise RuntimeError(\n                ""The file {!s} does not contain a valid sparse matrix"".format(filename)\n            )\n'"
sparse/_numba_extension.py,0,"b'def _init_extension():\n    """"""\n    Load extensions when numba is loaded.\n    This name must match the one in setup.py\n    """"""\n    import sparse._coo.numba_extension\n'"
sparse/_settings.py,0,"b'import os\nimport numpy\n\n\nAUTO_DENSIFY = bool(int(os.environ.get(""SPARSE_AUTO_DENSIFY"", ""0"")))\nWARN_ON_TOO_DENSE = bool(int(os.environ.get(""SPARSE_WARN_ON_TOO_DENSE"", ""0"")))\n\n\ndef _is_nep18_enabled():\n    class A:\n        def __array_function__(self, *args, **kwargs):\n            return True\n\n    try:\n        return numpy.concatenate([A()])\n    except ValueError:\n        return False\n\n\nNEP18_ENABLED = _is_nep18_enabled()\n'"
sparse/_slicing.py,14,"b'# Most of this file is taken from https://github.com/dask/dask/blob/master/dask/array/slicing.py\n# See license at https://github.com/dask/dask/blob/master/LICENSE.txt\n\nimport math\nfrom collections.abc import Iterable\nfrom numbers import Integral, Number\n\nimport numpy as np\n\n\ndef normalize_index(idx, shape):\n    """""" Normalize slicing indexes\n    1.  Replaces ellipses with many full slices\n    2.  Adds full slices to end of index\n    3.  Checks bounding conditions\n    4.  Replaces numpy arrays with lists\n    5.  Posify\'s slices integers and lists\n    6.  Normalizes slices to canonical form\n    Examples\n    --------\n    >>> normalize_index(1, (10,))\n    (1,)\n    >>> normalize_index(-1, (10,))\n    (9,)\n    >>> normalize_index([-1], (10,))\n    (array([9]),)\n    >>> normalize_index(slice(-3, 10, 1), (10,))\n    (slice(7, 10, 1),)\n    >>> normalize_index((Ellipsis, None), (10,))\n    (slice(0, 10, 1), None)\n    """"""\n    if not isinstance(idx, tuple):\n        idx = (idx,)\n    idx = replace_ellipsis(len(shape), idx)\n    n_sliced_dims = 0\n    for i in idx:\n        if hasattr(i, ""ndim"") and i.ndim >= 1:\n            n_sliced_dims += i.ndim\n        elif i is None:\n            continue\n        else:\n            n_sliced_dims += 1\n    idx = idx + (slice(None),) * (len(shape) - n_sliced_dims)\n    if len([i for i in idx if i is not None]) > len(shape):\n        raise IndexError(""Too many indices for array"")\n\n    none_shape = []\n    i = 0\n    for ind in idx:\n        if ind is not None:\n            none_shape.append(shape[i])\n            i += 1\n        else:\n            none_shape.append(None)\n\n    for i, d in zip(idx, none_shape):\n        if d is not None:\n            check_index(i, d)\n    idx = tuple(map(sanitize_index, idx))\n    idx = tuple(map(replace_none, idx, none_shape))\n    idx = posify_index(none_shape, idx)\n    idx = tuple(map(clip_slice, idx, none_shape))\n    return idx\n\n\ndef replace_ellipsis(n, index):\n    """""" Replace ... with slices, :, : ,:\n    >>> replace_ellipsis(4, (3, Ellipsis, 2))\n    (3, slice(None, None, None), slice(None, None, None), 2)\n    >>> replace_ellipsis(2, (Ellipsis, None))\n    (slice(None, None, None), slice(None, None, None), None)\n    """"""\n    # Careful about using in or index because index may contain arrays\n    isellipsis = [i for i, ind in enumerate(index) if ind is Ellipsis]\n    if not isellipsis:\n        return index\n    elif len(isellipsis) > 1:\n        raise IndexError(""an index can only have a single ellipsis (\'...\')"")\n    else:\n        loc = isellipsis[0]\n    extra_dimensions = n - (len(index) - sum(i is None for i in index) - 1)\n    return (\n        index[:loc] + (slice(None, None, None),) * extra_dimensions + index[loc + 1 :]\n    )\n\n\ndef check_index(ind, dimension):\n    """""" Check validity of index for a given dimension\n    Examples\n    --------\n    >>> check_index(3, 5)\n    >>> check_index(5, 5)\n    Traceback (most recent call last):\n    ...\n    IndexError: Index is not smaller than dimension 5 >= 5\n    >>> check_index(6, 5)\n    Traceback (most recent call last):\n    ...\n    IndexError: Index is not smaller than dimension 6 >= 5\n    >>> check_index(-1, 5)\n    >>> check_index(-6, 5)\n    Traceback (most recent call last):\n    ...\n    IndexError: Negative index is not greater than negative dimension -6 <= -5\n    >>> check_index([1, 2], 5)\n    >>> check_index([6, 3], 5)\n    Traceback (most recent call last):\n    ...\n    IndexError: Index out of bounds for dimension 5\n    >>> check_index(slice(0, 3), 5)\n    """"""\n    # unknown dimension, assumed to be in bounds\n    if isinstance(ind, Iterable):\n        x = np.asanyarray(ind)\n        if (\n            np.issubdtype(x.dtype, np.integer)\n            and ((x >= dimension) | (x < -dimension)).any()\n        ):\n            raise IndexError(""Index out of bounds for dimension {:d}"".format(dimension))\n        elif x.dtype == bool and len(x) != dimension:\n            raise IndexError(\n                ""boolean index did not match indexed array; dimension is {:d} ""\n                ""but corresponding boolean dimension is {:d}"".format(dimension, len(x))\n            )\n    elif isinstance(ind, slice):\n        return\n    elif not isinstance(ind, Integral):\n        raise IndexError(\n            ""only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and ""\n            ""integer or boolean arrays are valid indices""\n        )\n\n    elif ind >= dimension:\n        raise IndexError(\n            ""Index is not smaller than dimension {:d} >= {:d}"".format(ind, dimension)\n        )\n\n    elif ind < -dimension:\n        msg = ""Negative index is not greater than negative dimension {:d} <= -{:d}""\n        raise IndexError(msg.format(ind, dimension))\n\n\ndef sanitize_index(ind):\n    """""" Sanitize the elements for indexing along one axis\n    >>> sanitize_index([2, 3, 5])\n    array([2, 3, 5])\n    >>> sanitize_index([True, False, True, False])\n    array([0, 2])\n    >>> sanitize_index(np.array([1, 2, 3]))\n    array([1, 2, 3])\n    >>> sanitize_index(np.array([False, True, True]))\n    array([1, 2])\n    >>> type(sanitize_index(np.int32(0))) # doctest: +SKIP\n    <type \'int\'>\n    >>> sanitize_index(0.5) # doctest: +SKIP\n    Traceback (most recent call last):\n        ...\n    IndexError: only integers, slices (`:`), ellipsis (`...`),\n    numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n    """"""\n    if ind is None:\n        return None\n    elif isinstance(ind, slice):\n        return slice(\n            _sanitize_index_element(ind.start),\n            _sanitize_index_element(ind.stop),\n            _sanitize_index_element(ind.step),\n        )\n    elif isinstance(ind, Number):\n        return _sanitize_index_element(ind)\n    if not hasattr(ind, ""dtype"") and len(ind) == 0:\n        ind = np.array([], dtype=np.intp)\n    ind = np.asarray(ind)\n    if ind.dtype == np.bool_:\n        nonzero = np.nonzero(ind)\n        if len(nonzero) == 1:\n            # If a 1-element tuple, unwrap the element\n            nonzero = nonzero[0]\n        return np.asanyarray(nonzero)\n    elif np.issubdtype(ind.dtype, np.integer):\n        return ind\n    else:\n        raise IndexError(\n            ""only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and ""\n            ""integer or boolean arrays are valid indices""\n        )\n\n\ndef _sanitize_index_element(ind):\n    """"""Sanitize a one-element index.""""""\n    if ind is None:\n        return None\n\n    return int(ind)\n\n\ndef posify_index(shape, ind):\n    """""" Flip negative indices around to positive ones\n    >>> posify_index(10, 3)\n    3\n    >>> posify_index(10, -3)\n    7\n    >>> posify_index(10, [3, -3])\n    array([3, 7])\n    >>> posify_index((10, 20), (3, -3))\n    (3, 17)\n    >>> posify_index((10, 20), (3, [3, 4, -3]))  # doctest: +NORMALIZE_WHITESPACE\n    (3, array([ 3,  4, 17]))\n    """"""\n    if isinstance(ind, tuple):\n        return tuple(map(posify_index, shape, ind))\n    if isinstance(ind, Integral):\n        if ind < 0 and not math.isnan(shape):\n            return ind + shape\n        else:\n            return ind\n    if isinstance(ind, (np.ndarray, list)) and not math.isnan(shape):\n        ind = np.asanyarray(ind)\n        return np.where(ind < 0, ind + shape, ind)\n    if isinstance(ind, slice):\n        start, stop, step = ind.start, ind.stop, ind.step\n\n        if start < 0:\n            start += shape\n\n        if not (0 > stop >= step) and stop < 0:\n            stop += shape\n\n        return slice(start, stop, ind.step)\n\n    return ind\n\n\ndef clip_slice(idx, dim):\n    """"""\n    Clip slice to its effective size given the shape.\n\n    Parameters\n    ----------\n    idx : The index.\n    dim : The size along the corresponding dimension.\n\n    Returns\n    -------\n    idx : slice\n\n    Examples\n    --------\n    >>> clip_slice(slice(0, 20, 1), 10)\n    slice(0, 10, 1)\n    """"""\n    if not isinstance(idx, slice):\n        return idx\n\n    start, stop, step = idx.start, idx.stop, idx.step\n\n    if step > 0:\n        start = max(start, 0)\n        stop = min(stop, dim)\n\n        if start > stop:\n            start = stop\n    else:\n        start = min(start, dim - 1)\n        stop = max(stop, -1)\n\n        if start < stop:\n            start = stop\n\n    return slice(start, stop, step)\n\n\ndef replace_none(idx, dim):\n    """"""\n    Normalize slices to canonical form, i.e.\n    replace ``None`` with the appropriate integers.\n\n    Parameters\n    ----------\n    idx: slice or other index\n    dim: dimension length\n\n    Examples\n    --------\n    >>> replace_none(slice(None, None, None), 10)\n    slice(0, 10, 1)\n    """"""\n    if not isinstance(idx, slice):\n        return idx\n\n    start, stop, step = idx.start, idx.stop, idx.step\n\n    if step is None:\n        step = 1\n\n    if step > 0:\n        if start is None:\n            start = 0\n\n        if stop is None:\n            stop = dim\n    else:\n        if start is None:\n            start = dim - 1\n\n        if stop is None:\n            stop = -1\n\n    return slice(start, stop, step)\n'"
sparse/_sparse_array.py,10,"b'from abc import ABCMeta, abstractmethod\nfrom collections.abc import Iterable\nfrom numbers import Integral\nfrom functools import reduce\nfrom typing import Callable\nimport operator\n\nimport numpy as np\n\nfrom ._utils import _zero_of_dtype, html_table\n\n\nclass SparseArray:\n    """"""\n    An abstract base class for all the sparse array classes.\n\n    Attributes\n    ----------\n    dtype : numpy.dtype\n        The data type of this array.\n    fill_value : scalar\n        The fill value of this array.\n    """"""\n\n    __metaclass__ = ABCMeta\n\n    def __init__(self, shape, fill_value=None):\n        if not isinstance(shape, Iterable):\n            shape = (shape,)\n\n        if not all(isinstance(l, Integral) and int(l) >= 0 for l in shape):\n            raise ValueError(\n                ""shape must be an non-negative integer or a tuple ""\n                ""of non-negative integers.""\n            )\n\n        self.shape = tuple(int(l) for l in shape)\n\n        if fill_value is not None:\n            if not hasattr(fill_value, ""dtype"") or fill_value.dtype != self.dtype:\n                self.fill_value = self.dtype.type(fill_value)\n            else:\n                self.fill_value = fill_value\n        else:\n            self.fill_value = _zero_of_dtype(self.dtype)\n\n    dtype = None\n\n    @property\n    @abstractmethod\n    def nnz(self):\n        """"""\n        The number of nonzero elements in this array. Note that any duplicates in\n        :code:`coords` are counted multiple times. To avoid this, call :obj:`COO.sum_duplicates`.\n\n        Returns\n        -------\n        int\n            The number of nonzero elements in this array.\n\n        See Also\n        --------\n        DOK.nnz : Equivalent :obj:`DOK` array property.\n        numpy.count_nonzero : A similar Numpy function.\n        scipy.sparse.coo_matrix.nnz : The Scipy equivalent property.\n\n        Examples\n        --------\n        >>> import numpy as np\n        >>> from sparse import COO\n        >>> x = np.array([0, 0, 1, 0, 1, 2, 0, 1, 2, 3, 0, 0])\n        >>> np.count_nonzero(x)\n        6\n        >>> s = COO.from_numpy(x)\n        >>> s.nnz\n        6\n        >>> np.count_nonzero(x) == s.nnz\n        True\n        """"""\n\n    @property\n    def ndim(self):\n        """"""\n        The number of dimensions of this array.\n\n        Returns\n        -------\n        int\n            The number of dimensions of this array.\n\n        See Also\n        --------\n        DOK.ndim : Equivalent property for :obj:`DOK` arrays.\n        numpy.ndarray.ndim : Numpy equivalent property.\n\n        Examples\n        --------\n        >>> from sparse import COO\n        >>> import numpy as np\n        >>> x = np.random.rand(1, 2, 3, 1, 2)\n        >>> s = COO.from_numpy(x)\n        >>> s.ndim\n        5\n        >>> s.ndim == x.ndim\n        True\n        """"""\n        return len(self.shape)\n\n    @property\n    def size(self):\n        """"""\n        The number of all elements (including zeros) in this array.\n\n        Returns\n        -------\n        int\n            The number of elements.\n\n        See Also\n        --------\n        numpy.ndarray.size : Numpy equivalent property.\n\n        Examples\n        --------\n        >>> from sparse import COO\n        >>> import numpy as np\n        >>> x = np.zeros((10, 10))\n        >>> s = COO.from_numpy(x)\n        >>> s.size\n        100\n        """"""\n        # We use this instead of np.prod because np.prod\n        # returns a float64 for an empty shape.\n        return reduce(operator.mul, self.shape, 1)\n\n    @property\n    def density(self):\n        """"""\n        The ratio of nonzero to all elements in this array.\n\n        Returns\n        -------\n        float\n            The ratio of nonzero to all elements.\n\n        See Also\n        --------\n        COO.size : Number of elements.\n        COO.nnz : Number of nonzero elements.\n\n        Examples\n        --------\n        >>> import numpy as np\n        >>> from sparse import COO\n        >>> x = np.zeros((8, 8))\n        >>> x[0, :] = 1\n        >>> s = COO.from_numpy(x)\n        >>> s.density\n        0.125\n        """"""\n        return self.nnz / self.size\n\n    def _repr_html_(self):\n        """"""\n        Diagnostic report about this array.\n        Renders in Jupyter.\n        """"""\n        return html_table(self)\n\n    @abstractmethod\n    def asformat(self, format):\n        """"""\n        Convert this sparse array to a given format.\n\n        Parameters\n        ----------\n        format : str\n            A format string.\n\n        Returns\n        -------\n        out : SparseArray\n            The converted array.\n\n        Raises\n        ------\n        NotImplementedError\n            If the format isn\'t supported.\n        """"""\n\n    @abstractmethod\n    def todense(self):\n        """"""\n        Convert this :obj:`SparseArray` array to a dense :obj:`numpy.ndarray`. Note that\n        this may take a large amount of memory and time.\n\n        Returns\n        -------\n        numpy.ndarray\n            The converted dense array.\n\n        See Also\n        --------\n        DOK.todense : Equivalent :obj:`DOK` array method.\n        COO.todense : Equivalent :obj:`COO` array method.\n        scipy.sparse.coo_matrix.todense : Equivalent Scipy method.\n\n        Examples\n        --------\n        >>> import sparse\n        >>> x = np.random.randint(100, size=(7, 3))\n        >>> s = sparse.COO.from_numpy(x)\n        >>> x2 = s.todense()\n        >>> np.array_equal(x, x2)\n        True\n        """"""\n\n    def __array__(self, *args, **kwargs):\n        from ._settings import AUTO_DENSIFY\n\n        if not AUTO_DENSIFY:\n            raise RuntimeError(\n                ""Cannot convert a sparse array to dense automatically. ""\n                ""To manually densify, use the todense method.""\n            )\n\n        return np.asarray(self.todense(), *args, **kwargs)\n\n    def __array_function__(self, func, types, args, kwargs):\n        import sparse as module\n\n        sparse_func = None\n        try:\n            submodules = getattr(func, ""__module__"", ""numpy"").split(""."")[1:]\n            for submodule in submodules:\n                module = getattr(module, submodule)\n            sparse_func = getattr(module, func.__name__)\n        except AttributeError:\n            pass\n        else:\n            return sparse_func(*args, **kwargs)\n\n        try:\n            sparse_func = getattr(type(self), func.__name__)\n        except AttributeError:\n            pass\n\n        if (\n            not isinstance(sparse_func, Callable)\n            and len(args) == 1\n            and len(kwargs) == 0\n        ):\n            try:\n                return getattr(self, func.__name__)\n            except AttributeError:\n                pass\n\n        if sparse_func is None:\n            return NotImplemented\n\n        return sparse_func(*args, **kwargs)\n'"
sparse/_utils.py,25,"b'import functools\nfrom collections.abc import Iterable\nfrom numbers import Integral\nfrom functools import reduce\n\nimport operator\nimport numpy as np\n\n\ndef assert_eq(x, y, check_nnz=True, compare_dtype=True, **kwargs):\n    from ._coo import COO\n\n    assert x.shape == y.shape\n\n    if compare_dtype:\n        assert x.dtype == y.dtype\n\n    check_equal = (\n        np.array_equal\n        if np.issubdtype(x.dtype, np.integer) and np.issubdtype(y.dtype, np.integer)\n        else functools.partial(np.allclose, equal_nan=True)\n    )\n\n    if isinstance(x, COO):\n        assert is_canonical(x)\n    if isinstance(y, COO):\n        assert is_canonical(y)\n\n    if isinstance(x, COO) and isinstance(y, COO) and check_nnz:\n        assert np.array_equal(x.coords, y.coords)\n        assert check_equal(x.data, y.data, **kwargs)\n        assert x.fill_value == y.fill_value\n        return\n\n    if hasattr(x, ""todense""):\n        xx = x.todense()\n        if check_nnz:\n            assert_nnz(x, xx)\n    else:\n        xx = x\n    if hasattr(y, ""todense""):\n        yy = y.todense()\n        if check_nnz:\n            assert_nnz(y, yy)\n    else:\n        yy = y\n    assert check_equal(xx, yy, **kwargs)\n\n\ndef assert_nnz(s, x):\n    fill_value = s.fill_value if hasattr(s, ""fill_value"") else _zero_of_dtype(s.dtype)\n    assert np.sum(~equivalent(x, fill_value)) == s.nnz\n\n\ndef is_canonical(x):\n    return not x.shape or (\n        (np.diff(x.linear_loc()) > 0).all()\n        and not equivalent(x.data, x.fill_value).any()\n    )\n\n\ndef _zero_of_dtype(dtype):\n    """"""\n    Creates a ()-shaped 0-dimensional zero array of a given dtype.\n\n    Parameters\n    ----------\n    dtype : numpy.dtype\n        The dtype for the array.\n\n    Returns\n    -------\n    np.ndarray\n        The zero array.\n    """"""\n    return np.zeros((), dtype=dtype)[()]\n\n\ndef random(\n    shape,\n    density=0.01,\n    random_state=None,\n    data_rvs=None,\n    format=""coo"",\n    compressed_axes=None,\n    fill_value=None,\n):\n    """""" Generate a random sparse multidimensional array\n\n    Parameters\n    ----------\n    shape: Tuple[int]\n        Shape of the array\n    density: float, optional\n        Density of the generated array.\n    random_state : Union[numpy.random.RandomState, int], optional\n        Random number generator or random seed. If not given, the\n        singleton numpy.random will be used. This random state will be used\n        for sampling the sparsity structure, but not necessarily for sampling\n        the values of the structurally nonzero entries of the matrix.\n    data_rvs : Callable\n        Data generation callback. Must accept one single parameter: number of\n        :code:`nnz` elements, and return one single NumPy array of exactly\n        that length.\n    format : str\n        The format to return the output array in.\n    fill_value : scalar\n        The fill value of the output array.\n\n    Returns\n    -------\n    SparseArray\n        The generated random matrix.\n\n    See Also\n    --------\n    :obj:`scipy.sparse.rand`\n        Equivalent Scipy function.\n    :obj:`numpy.random.rand`\n        Similar Numpy function.\n\n    Examples\n    --------\n\n    >>> from sparse import random\n    >>> from scipy import stats\n    >>> rvs = lambda x: stats.poisson(25, loc=10).rvs(x, random_state=np.random.RandomState(1))\n    >>> s = random((2, 3, 4), density=0.25, random_state=np.random.RandomState(1), data_rvs=rvs)\n    >>> s.todense()  # doctest: +NORMALIZE_WHITESPACE\n    array([[[ 0,  0,  0,  0],\n            [ 0, 34,  0,  0],\n            [33, 34,  0, 29]],\n    <BLANKLINE>\n           [[30,  0,  0, 34],\n            [ 0,  0,  0,  0],\n            [ 0,  0,  0,  0]]])\n\n    """"""\n    # Copied, in large part, from scipy.sparse.random\n    # See https://github.com/scipy/scipy/blob/master/LICENSE.txt\n    from ._coo import COO\n\n    elements = np.prod(shape, dtype=np.intp)\n\n    nnz = int(elements * density)\n\n    if format != ""gcxs"" and compressed_axes is not None:\n        raise ValueError(\n            ""compressed_axes is not supported for {} format"".format(format)\n        )\n\n    if random_state is None:\n        random_state = np.random\n    elif isinstance(random_state, Integral):\n        random_state = np.random.RandomState(random_state)\n    if data_rvs is None:\n        data_rvs = random_state.rand\n\n    # Use the algorithm from python\'s random.sample for k < mn/3.\n    if elements < 3 * nnz:\n        ind = random_state.choice(elements, size=nnz, replace=False)\n    else:\n        ind = np.empty(nnz, dtype=np.min_scalar_type(elements - 1))\n        selected = set()\n        for i in range(nnz):\n            j = random_state.randint(elements)\n            while j in selected:\n                j = random_state.randint(elements)\n            selected.add(j)\n            ind[i] = j\n\n    data = data_rvs(nnz)\n\n    ar = COO(ind[None, :], data, shape=elements, fill_value=fill_value).reshape(shape)\n\n    return ar.asformat(format, compressed_axes=compressed_axes)\n\n\ndef isscalar(x):\n    from ._sparse_array import SparseArray\n\n    return not isinstance(x, SparseArray) and np.isscalar(x)\n\n\ndef random_value_array(value, fraction):\n    def replace_values(n):\n        i = int(n * fraction)\n\n        ar = np.empty((n,), dtype=np.float_)\n        ar[:i] = value\n        ar[i:] = np.random.rand(n - i)\n        return ar\n\n    return replace_values\n\n\ndef normalize_axis(axis, ndim):\n    """"""\n    Normalize negative axis indices to their positive counterpart for a given\n    number of dimensions.\n\n    Parameters\n    ----------\n    axis : Union[int, Iterable[int], None]\n        The axis indices.\n    ndim : int\n        Number of dimensions to normalize axis indices against.\n\n    Returns\n    -------\n    axis\n        The normalized axis indices.\n    """"""\n    if axis is None:\n        return None\n\n    if isinstance(axis, Integral):\n        axis = int(axis)\n        if axis < 0:\n            axis += ndim\n\n        if axis >= ndim or axis < 0:\n            raise ValueError(""Invalid axis index %d for ndim=%d"" % (axis, ndim))\n\n        return axis\n\n    if isinstance(axis, Iterable):\n        if not all(isinstance(a, Integral) for a in axis):\n            raise ValueError(""axis %s not understood"" % axis)\n\n        return tuple(normalize_axis(a, ndim) for a in axis)\n\n    raise ValueError(""axis %s not understood"" % axis)\n\n\ndef equivalent(x, y):\n    """"""\n    Checks the equivalence of two scalars or arrays with broadcasting. Assumes\n    a consistent dtype.\n\n    Parameters\n    ----------\n    x : scalar or numpy.ndarray\n    y : scalar or numpy.ndarray\n\n    Returns\n    -------\n    equivalent : scalar or numpy.ndarray\n        The element-wise comparison of where two arrays are equivalent.\n\n    Examples\n    --------\n    >>> equivalent(1, 1)\n    True\n    >>> equivalent(np.nan, np.nan + 1)\n    True\n    >>> equivalent(1, 2)\n    False\n    >>> equivalent(np.inf, np.inf)\n    True\n    >>> equivalent(np.PZERO, np.NZERO)\n    True\n    """"""\n    x = np.asarray(x)\n    y = np.asarray(y)\n    # Can\'t contain NaNs\n    if any(np.issubdtype(x.dtype, t) for t in [np.integer, np.bool_, np.character]):\n        return x == y\n\n    # Can contain NaNs\n    # FIXME: Complex floats and np.void with multiple values can\'t be compared properly.\n    # lgtm [py/comparison-of-identical-expressions]\n    return (x == y) | ((x != x) & (y != y))\n\n\n# copied from zarr\n# See https://github.com/zarr-developers/zarr-python/blob/master/zarr/util.py\ndef human_readable_size(size):\n    if size < 2 ** 10:\n        return ""%s"" % size\n    elif size < 2 ** 20:\n        return ""%.1fK"" % (size / float(2 ** 10))\n    elif size < 2 ** 30:\n        return ""%.1fM"" % (size / float(2 ** 20))\n    elif size < 2 ** 40:\n        return ""%.1fG"" % (size / float(2 ** 30))\n    elif size < 2 ** 50:\n        return ""%.1fT"" % (size / float(2 ** 40))\n    else:\n        return ""%.1fP"" % (size / float(2 ** 50))\n\n\ndef html_table(arr):\n    table = ""<table>""\n    table += ""<tbody>""\n    headings = [""Format"", ""Data Type"", ""Shape"", ""nnz"", ""Density"", ""Read-only""]\n    info = [\n        type(arr).__name__.lower(),\n        str(arr.dtype),\n        str(arr.shape),\n        str(arr.nnz),\n        str(arr.nnz / arr.size),\n    ]\n\n    # read-only\n    info.append(str(not hasattr(arr, ""__setitem__"")))\n\n    if hasattr(arr, ""nbytes""):\n        headings.append(""Size"")\n        info.append(human_readable_size(arr.nbytes))\n        headings.append(""Storage ratio"")\n        info.append(\n            ""%.1f""\n            % (arr.nbytes / (reduce(operator.mul, arr.shape, 1) * arr.dtype.itemsize))\n        )\n\n    # compressed_axes\n    if type(arr).__name__ == ""GCXS"":\n        headings.append(""Compressed Axes"")\n        info.append(str(arr.compressed_axes))\n\n    for h, i in zip(headings, info):\n        table += (\n            ""<tr>""\n            \'<th style=""text-align: left"">%s</th>\'\n            \'<td style=""text-align: left"">%s</td>\'\n            ""</tr>"" % (h, i)\n        )\n    table += ""</tbody>""\n    table += ""</table>""\n    return table\n\n\ndef check_compressed_axes(ndim, compressed_axes):\n    """"""\n    Checks if the given compressed_axes are compatible with the shape of the array.\n\n    Parameters\n    ----------\n    shape : int\n    compressed_axes : Iterable\n\n    Raises\n    ------\n    ValueError\n        If the compressed_axes are incompatible with the number of dimensions     \n    """"""\n    if compressed_axes is None:\n        pass\n    if isinstance(ndim, Iterable):\n        ndim = len(ndim)\n    if not isinstance(compressed_axes, Iterable):\n        raise ValueError(""compressed_axes must be an iterable"")\n    if len(compressed_axes) == ndim:\n        raise ValueError(""cannot compress all axes"")\n    if not np.array_equal(list(set(compressed_axes)), compressed_axes):\n        raise ValueError(""axes must be sorted without repeats"")\n    if not all(isinstance(a, Integral) for a in compressed_axes):\n        raise ValueError(""axes must be represented with integers"")\n    if min(compressed_axes) < 0 or max(compressed_axes) >= ndim:\n        raise ValueError(""axis out of range"")\n\n\ndef check_zero_fill_value(*args):\n    """"""\n    Checks if all the arguments have zero fill-values.\n\n    Parameters\n    ----------\n    args : Iterable[SparseArray]\n\n    Raises\n    ------\n    ValueError\n        If all arguments don\'t have zero fill-values.\n\n    Examples\n    --------\n    >>> import sparse\n    >>> s1 = sparse.random((10,), density=0.5)\n    >>> s2 = sparse.random((10,), density=0.5, fill_value=0.5)\n    >>> check_zero_fill_value(s1)\n    >>> check_zero_fill_value(s2)\n    Traceback (most recent call last):\n        ...\n    ValueError: This operation requires zero fill values, but argument 0 had a fill value of 0.5.\n    >>> check_zero_fill_value(s1, s2)\n    Traceback (most recent call last):\n        ...\n    ValueError: This operation requires zero fill values, but argument 1 had a fill value of 0.5.\n    """"""\n    for i, arg in enumerate(args):\n        if hasattr(arg, ""fill_value"") and not equivalent(\n            arg.fill_value, _zero_of_dtype(arg.dtype)\n        ):\n            raise ValueError(\n                ""This operation requires zero fill values, ""\n                ""but argument {:d} had a fill value of {!s}."".format(i, arg.fill_value)\n            )\n\n\ndef check_consistent_fill_value(arrays):\n    """"""\n    Checks if all the arguments have consistent fill-values.\n\n    Parameters\n    ----------\n    args : Iterable[SparseArray]\n\n    Raises\n    ------\n    ValueError\n        If all elements of :code:`arrays` don\'t have the same fill-value.\n\n    Examples\n    --------\n    >>> import sparse\n    >>> s1 = sparse.random((10,), density=0.5, fill_value=0.1)\n    >>> s2 = sparse.random((10,), density=0.5, fill_value=0.5)\n    >>> check_consistent_fill_value([s1, s1])\n    >>> check_consistent_fill_value([s1, s2])  # doctest: +NORMALIZE_WHITESPACE\n    Traceback (most recent call last):\n        ...\n    ValueError: This operation requires consistent fill-values, but argument 1 had a fill value of 0.5,\\\n        which is different from a fill_value of 0.1 in the first argument.\n    """"""\n    arrays = list(arrays)\n    from ._sparse_array import SparseArray\n\n    if not all(isinstance(s, SparseArray) for s in arrays):\n        raise ValueError(""All arrays must be instances of SparseArray."")\n    if len(arrays) == 0:\n        raise ValueError(""At least one array required."")\n\n    fv = arrays[0].fill_value\n\n    for i, arg in enumerate(arrays):\n        if not equivalent(fv, arg.fill_value):\n            raise ValueError(\n                ""This operation requires consistent fill-values, ""\n                ""but argument {:d} had a fill value of {!s}, which ""\n                ""is different from a fill_value of {!s} in the first ""\n                ""argument."".format(i, arg.fill_value, fv)\n            )\n'"
sparse/_version.py,0,"b'# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.18 (https://github.com/warner/python-versioneer)\n\n""""""Git implementation of _version.py.""""""\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    """"""Get the keywords needed to look up the version information.""""""\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = ""$Format:%d$""\n    git_full = ""$Format:%H$""\n    git_date = ""$Format:%ci$""\n    keywords = {""refnames"": git_refnames, ""full"": git_full, ""date"": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_config():\n    """"""Create, populate and return the VersioneerConfig() object.""""""\n    # these strings are filled in when \'setup.py versioneer\' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = ""git""\n    cfg.style = ""pep440""\n    cfg.tag_prefix = """"\n    cfg.parentdir_prefix = ""sparse-""\n    cfg.versionfile_source = ""sparse/_version.py""\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen(\n                [c] + args,\n                cwd=cwd,\n                env=env,\n                stdout=subprocess.PIPE,\n                stderr=(subprocess.PIPE if hide_stderr else None),\n            )\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %s"" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %s"" % (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %s (error)"" % dispcmd)\n            print(""stdout was %s"" % stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\n                ""version"": dirname[len(parentdir_prefix) :],\n                ""full-revisionid"": None,\n                ""dirty"": False,\n                ""error"": None,\n                ""date"": None,\n            }\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\n            ""Tried directories %s but none started with prefix %s""\n            % (str(rootdirs), parentdir_prefix)\n        )\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG) :] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r""\\d"", r)])\n        if verbose:\n            print(""discarding \'%s\', no digits"" % "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %s"" % "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix) :]\n            if verbose:\n                print(""picking %s"" % r)\n            return {\n                ""version"": r,\n                ""full-revisionid"": keywords[""full""].strip(),\n                ""dirty"": False,\n                ""error"": None,\n                ""date"": date,\n            }\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {\n        ""version"": ""0+unknown"",\n        ""full-revisionid"": keywords[""full""].strip(),\n        ""dirty"": False,\n        ""error"": ""no suitable tags"",\n        ""date"": None,\n    }\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    _, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root, hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %s not under git control"" % root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(\n        GITS,\n        [\n            ""describe"",\n            ""--tags"",\n            ""--dirty"",\n            ""--always"",\n            ""--long"",\n            ""--match"",\n            ""%s*"" % tag_prefix,\n        ],\n        cwd=root,\n    )\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[: git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r""^(.+)-(\\d+)-g([0-9a-f]+)$"", git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = ""unable to parse git-describe output: \'%s\'"" % describe_out\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                print(fmt % (full_tag, tag_prefix))\n            pieces[""error""] = ""tag \'%s\' doesn\'t start with prefix \'%s\'"" % (\n                full_tag,\n                tag_prefix,\n            )\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix) :]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""], cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%ci"", ""HEAD""], cwd=root)[\n        0\n    ].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%d.g%s"" % (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%d.g%s"" % (pieces[""distance""], pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%d"" % pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%d"" % pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%s"" % pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%s"" % pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {\n            ""version"": ""unknown"",\n            ""full-revisionid"": pieces.get(""long""),\n            ""dirty"": None,\n            ""error"": pieces[""error""],\n            ""date"": None,\n        }\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%s\'"" % style)\n\n    return {\n        ""version"": rendered,\n        ""full-revisionid"": pieces[""long""],\n        ""dirty"": pieces[""dirty""],\n        ""error"": None,\n        ""date"": pieces.get(""date""),\n    }\n\n\ndef get_versions():\n    """"""Get version information or return default if unable to do so.""""""\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don\'t do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for _ in cfg.versionfile_source.split(""/""):\n            root = os.path.dirname(root)\n    except NameError:\n        return {\n            ""version"": ""0+unknown"",\n            ""full-revisionid"": None,\n            ""dirty"": None,\n            ""error"": ""unable to find root of source tree"",\n            ""date"": None,\n        }\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {\n        ""version"": ""0+unknown"",\n        ""full-revisionid"": None,\n        ""dirty"": None,\n        ""error"": ""unable to compute version"",\n        ""date"": None,\n    }\n'"
sparse/_compressed/__init__.py,0,"b'from .compressed import GCXS\nfrom .common import stack, concatenate\n'"
sparse/_compressed/common.py,6,"b'import numpy as np\nfrom .._utils import check_consistent_fill_value, normalize_axis\n\n\ndef concatenate(arrays, axis=0, compressed_axes=None):\n\n    from .compressed import GCXS\n\n    check_consistent_fill_value(arrays)\n    arrays = [\n        arr if isinstance(arr, GCXS) else GCXS(arr, compressed_axes=(axis,))\n        for arr in arrays\n    ]\n    axis = normalize_axis(axis, arrays[0].ndim)\n    dim = sum(x.shape[axis] for x in arrays)\n    shape = list(arrays[0].shape)\n    shape[axis] = dim\n    assert all(\n        x.shape[ax] == arrays[0].shape[ax]\n        for x in arrays\n        for ax in set(range(arrays[0].ndim)) - {axis}\n    )\n    if compressed_axes is None:\n        compressed_axes = (axis,)\n    if arrays[0].ndim == 1:\n        from .._coo.common import concatenate as coo_concat\n\n        arrays = [arr.tocoo() for arr in arrays]\n        return coo_concat(arrays, axis=axis)\n    # arrays may have different compressed_axes\n    # concatenating becomes easy when compressed_axes are the same\n    arrays = [arr.change_compressed_axes((axis,)) for arr in arrays]\n    ptr_list = []\n    for i, arr in enumerate(arrays):\n        if i == 0:\n            ptr_list.append(arr.indptr)\n            continue\n        ptr_list.append(arr.indptr[1:])\n    indptr = np.concatenate(ptr_list)\n    indices = np.concatenate([arr.indices for arr in arrays])\n    data = np.concatenate([arr.data for arr in arrays])\n    ptr_len = arrays[0].indptr.shape[0]\n    nnz = arrays[0].nnz\n    for i in range(1, len(arrays)):\n        indptr[ptr_len:] += nnz\n        nnz = arrays[i].nnz\n        ptr_len += arrays[i].indptr.shape[0] - 1\n    return GCXS(\n        (data, indices, indptr),\n        shape=tuple(shape),\n        compressed_axes=arrays[0].compressed_axes,\n        fill_value=arrays[0].fill_value,\n    ).change_compressed_axes(compressed_axes)\n\n\ndef stack(arrays, axis=0, compressed_axes=None):\n\n    from .compressed import GCXS\n\n    check_consistent_fill_value(arrays)\n    arrays = [\n        arr if isinstance(arr, GCXS) else GCXS(arr, compressed_axes=(axis,))\n        for arr in arrays\n    ]\n    axis = normalize_axis(axis, arrays[0].ndim + 1)\n    assert all(\n        x.shape[ax] == arrays[0].shape[ax]\n        for x in arrays\n        for ax in set(range(arrays[0].ndim)) - {axis}\n    )\n    if compressed_axes is None:\n        compressed_axes = (axis,)\n    if arrays[0].ndim == 1:\n        from .._coo.common import stack as coo_stack\n\n        arrays = [arr.tocoo() for arr in arrays]\n        return coo_stack(arrays, axis=axis)\n    # arrays may have different compressed_axes\n    # stacking becomes easy when compressed_axes are the same\n    ptr_list = []\n    for i in range(len(arrays)):\n        shape = list(arrays[i].shape)\n        shape.insert(axis, 1)\n        arrays[i] = arrays[i].reshape(shape).change_compressed_axes((axis,))\n        if i == 0:\n            ptr_list.append(arrays[i].indptr)\n            continue\n        ptr_list.append(arrays[i].indptr[1:])\n\n    shape[axis] = len(arrays)\n    indptr = np.concatenate(ptr_list)\n    indices = np.concatenate([arr.indices for arr in arrays])\n    data = np.concatenate([arr.data for arr in arrays])\n    ptr_len = arrays[0].indptr.shape[0]\n    nnz = arrays[0].nnz\n    for i in range(1, len(arrays)):\n        indptr[ptr_len:] += nnz\n        nnz = arrays[i].nnz\n        ptr_len += arrays[i].indptr.shape[0] - 1\n    return GCXS(\n        (data, indices, indptr),\n        shape=tuple(shape),\n        compressed_axes=arrays[0].compressed_axes,\n        fill_value=arrays[0].fill_value,\n    ).change_compressed_axes(compressed_axes)\n'"
sparse/_compressed/compressed.py,19,"b'import numpy as np\nfrom numpy.lib.mixins import NDArrayOperatorsMixin\nfrom functools import reduce\nfrom operator import mul\nfrom collections.abc import Iterable\nimport scipy.sparse as ss\n\nfrom .._sparse_array import SparseArray\nfrom .._coo.common import linear_loc\nfrom .._utils import normalize_axis, check_zero_fill_value, check_compressed_axes\nfrom .._coo.core import COO\nfrom .convert import uncompress_dimension\nfrom .indexing import getitem\n\n\ndef _from_coo(x, compressed_axes=None):\n\n    if x.ndim == 0:\n        if compressed_axes is not None:\n            raise ValueError(""no axes to compress for 0d array"")\n        return (x.data, x.coords, []), x.shape, None, (), None, None, None, x.fill_value\n\n    if x.ndim == 1:\n        if compressed_axes is not None:\n            raise ValueError(""no axes to compress for 1d array"")\n        return (\n            (x.data, x.coords[0], ()),\n            x.shape,\n            None,\n            None,\n            None,\n            None,\n            None,\n            x.fill_value,\n        )\n\n    compressed_axes = normalize_axis(compressed_axes, x.ndim)\n    if compressed_axes is None:\n        # defaults to best compression ratio\n        compressed_axes = (np.argmin(x.shape),)\n\n    check_compressed_axes(x.shape, compressed_axes)\n\n    axis_order = list(compressed_axes)\n    # array location where the uncompressed dimensions start\n    axisptr = len(compressed_axes)\n    axis_order.extend(np.setdiff1d(np.arange(len(x.shape)), compressed_axes))\n    new_shape = np.array(x.shape)[axis_order]\n    row_size = np.prod(new_shape[:axisptr])\n    col_size = np.prod(new_shape[axisptr:])\n    compressed_shape = (row_size, col_size)\n    shape = x.shape\n\n    x = x.transpose(axis_order)\n    linear = linear_loc(x.coords, new_shape)\n    order = np.argsort(linear)\n    # linearizing twice is unnecessary, fix needed\n    coords = x.reshape((compressed_shape)).coords\n    indptr = np.empty(row_size + 1, dtype=np.intp)\n    indptr[0] = 0\n    np.cumsum(np.bincount(coords[0], minlength=row_size), out=indptr[1:])\n    indices = coords[1]\n    data = x.data[order]\n    return (\n        (data, indices, indptr),\n        shape,\n        compressed_shape,\n        compressed_axes,\n        axis_order,\n        new_shape,\n        axisptr,\n        x.fill_value,\n    )\n\n\nclass GCXS(SparseArray, NDArrayOperatorsMixin):\n\n    __array_priority__ = 12\n\n    def __init__(self, arg, shape=None, compressed_axes=None, fill_value=0):\n\n        if isinstance(arg, np.ndarray):\n            (\n                arg,\n                shape,\n                compressed_shape,\n                compressed_axes,\n                axis_order,\n                reordered_shape,\n                axisptr,\n                fill_value,\n            ) = _from_coo(COO(arg), compressed_axes)\n\n        elif isinstance(arg, COO):\n            (\n                arg,\n                shape,\n                compressed_shape,\n                compressed_axes,\n                axis_order,\n                reordered_shape,\n                axisptr,\n                fill_value,\n            ) = _from_coo(arg, compressed_axes)\n\n        if shape is None:\n            raise ValueError(""missing `shape` argument"")\n\n        if len(shape) != 1:\n\n            # if initializing directly with (data,indices,indptr)\n            compressed_axes = normalize_axis(compressed_axes, len(shape))\n\n            if compressed_axes is None:\n                raise ValueError(""missing `compressed_axes` argument"")\n            elif compressed_axes != () and len(compressed_axes) >= len(shape):\n                raise ValueError(""cannot compress all axes"")\n            if not np.array_equal(\n                np.unique(compressed_axes), sorted(np.array(compressed_axes))\n            ):\n                raise ValueError(""repeated axis in compressed_axes"")\n\n            axis_order = list(compressed_axes)\n            # array location where the uncompressed dimensions start\n            axisptr = len(compressed_axes)\n            axis_order.extend(np.setdiff1d(np.arange(len(shape)), compressed_axes))\n            reordered_shape = np.array(shape)[axis_order]\n            row_size = np.prod(reordered_shape[:axisptr])\n            col_size = np.prod(reordered_shape[axisptr:])\n            compressed_shape = (row_size, col_size)\n        else:\n            compressed_axes = (\n                compressed_shape\n            ) = axis_order = reordered_shape = axisptr = None\n\n        self.data, self.indices, self.indptr = arg\n        self.shape = shape\n        self.compressed_shape = compressed_shape\n        self.compressed_axes = compressed_axes\n        self.axis_order = axis_order\n        self.axisptr = axisptr\n        self.reordered_shape = reordered_shape\n        self.fill_value = fill_value\n\n    @classmethod\n    def from_numpy(cls, x, compressed_axes=None, fill_value=0):\n        coo = COO(x, fill_value=fill_value)\n        return cls.from_coo(coo, compressed_axes)\n\n    @classmethod\n    def from_coo(cls, x, compressed_axes=None):\n        (\n            arg,\n            shape,\n            compressed_shape,\n            compressed_axes,\n            axis_order,\n            reordered_shape,\n            axisptr,\n            fill_value,\n        ) = _from_coo(x, compressed_axes)\n        return cls(\n            arg, shape=shape, compressed_axes=compressed_axes, fill_value=fill_value\n        )\n\n    @classmethod\n    def from_scipy_sparse(cls, x):\n        if x.format == ""csc"":\n            return cls(\n                (x.data, x.indices, x.indptr), shape=x.shape, compressed_axes=(1,)\n            )\n        else:\n            x = x.asformat(""csr"")\n            return cls(\n                (x.data, x.indices, x.indptr), shape=x.shape, compressed_axes=(0,)\n            )\n\n    @classmethod\n    def from_iter(cls, x, shape=None, compressed_axes=None, fill_value=None):\n        return cls.from_coo(\n            COO.from_iter(x, shape, fill_value), compressed_axes=compressed_axes\n        )\n\n    @property\n    def dtype(self):\n        """"""\n        The datatype of this array.\n        \n        Returns\n        -------\n        numpy.dtype\n            The datatype of this array.\n            \n        See Also\n        --------\n        numpy.ndarray.dtype : Numpy equivalent property.\n        scipy.sparse.csr_matrix.dtype : Scipy equivalent property.\n        """"""\n        return self.data.dtype\n\n    @property\n    def nnz(self):\n        """"""\n        The number of nonzero elements in this array.\n        \n        Returns\n        -------\n        int\n            The number of nonzero elements in this array.\n            \n        See Also\n        --------\n        COO.nnz : Equivalent :obj:`COO` array property.\n        DOK.nnz : Equivalent :obj:`DOK` array property.\n        numpy.count_nonzero : A similar Numpy function.\n        scipy.sparse.csr_matrix.nnz : The Scipy equivalent property.\n        """"""\n        return self.data.shape[0]\n\n    @property\n    def nbytes(self):\n        """"""\n        The number of bytes taken up by this object. Note that for small arrays,\n        this may undercount the number of bytes due to the large constant overhead.\n        \n        Returns\n        -------\n        int\n            The approximate bytes of memory taken by this object.\n        \n        See Also\n        --------\n        numpy.ndarray.nbytes : The equivalent Numpy property.\n        """"""\n        nbytes = self.data.nbytes + self.indices.nbytes\n        if self.indptr != ():\n            nbytes += self.indptr.nbytes\n        return nbytes\n\n    def __str__(self):\n        return ""<GCXS: shape={}, dtype={}, nnz={}, fill_value={}, compressed_axes={}>"".format(\n            self.shape, self.dtype, self.nnz, self.fill_value, self.compressed_axes\n        )\n\n    __repr__ = __str__\n\n    __getitem__ = getitem\n\n    def change_compressed_axes(self, new_compressed_axes):\n        """"""\n        changes the compressed axes of an array.\n        """"""\n        if self.ndim == 1:\n            raise NotImplementedError(""no axes to compress for 1d array"")\n\n        new_compressed_axes = tuple(\n            normalize_axis(new_compressed_axes[i], self.ndim)\n            for i in range(len(new_compressed_axes))\n        )\n\n        if len(new_compressed_axes) >= len(self.shape):\n            raise ValueError(""cannot compress all axes"")\n        if len(set(new_compressed_axes)) != len(new_compressed_axes):\n            raise ValueError(""repeated axis in compressed_axes"")\n        coo = self.tocoo()\n        (\n            arg,\n            shape,\n            compressed_shape,\n            compressed_axes,\n            axis_order,\n            reordered_shape,\n            axisptr,\n            fill_value,\n        ) = _from_coo(coo, new_compressed_axes)\n        return GCXS(\n            arg, shape=shape, compressed_axes=compressed_axes, fill_value=fill_value\n        )\n\n    def tocoo(self):\n        if self.ndim == 1:\n            return COO(\n                self.indices[None, :],\n                self.data,\n                shape=self.shape,\n                fill_value=self.fill_value,\n            )\n        uncompressed = uncompress_dimension(self.indptr)\n        coords = np.vstack((uncompressed, self.indices))\n        order = np.argsort(self.axis_order)\n        return (\n            COO(\n                coords,\n                self.data,\n                shape=self.compressed_shape,\n                fill_value=self.fill_value,\n            )\n            .reshape(self.reordered_shape)\n            .transpose(order)\n        )\n\n    def todense(self):\n        if self.compressed_axes == ():\n            return np.full(self.shape, self.fill_value, self.dtype)\n        return self.tocoo().todense()\n\n    def todok(self):\n\n        from ..dok import DOK\n\n        return DOK.from_coo(self.tocoo())  # probably a temporary solution\n\n    def to_scipy_sparse(self):\n        """"""\n        Converts this :obj:`CSD` object into a :obj:`scipy.sparse.csr_matrix` or `scipy.sparse.csc_matrix`.\n        Returns\n        -------\n        :obj:`scipy.sparse.csr_matrix` or `scipy.sparse.csc_matrix`\n            The converted Scipy sparse matrix.\n        Raises\n        ------\n        ValueError\n            If the array is not two-dimensional.\n        ValueError\n            If all the array doesn\'t zero fill-values.\n        """"""\n\n        check_zero_fill_value(self)\n\n        if self.ndim != 2:\n            raise ValueError(\n                ""Can only convert a 2-dimensional array to a Scipy sparse matrix.""\n            )\n\n        if 0 in self.compressed_axes:\n            return ss.csr_matrix(\n                (self.data, self.indices, self.indptr), shape=self.shape\n            )\n        else:\n            return ss.csc_matrix(\n                (self.data, self.indices, self.indptr), shape=self.shape\n            )\n\n    def asformat(self, format):\n        """"""\n        Convert this sparse array to a given format.\n        Parameters\n        ----------\n        format : str\n            A format string.\n        Returns\n        -------\n        out : SparseArray\n            The converted array.\n        Raises\n        ------\n        NotImplementedError\n            If the format isn\'t supported.\n        """"""\n\n        if format == ""coo"":\n            return self.tocoo()\n        elif format == ""dok"":\n            return self.todok()\n\n        raise NotImplementedError(""The given format is not supported."")\n\n    def maybe_densify(self, max_size=1000, min_density=0.25):\n        """"""\n        Converts this :obj:`CSR` or `CSC` array to a :obj:`numpy.ndarray` if not too\n        costly.\n        Parameters\n        ----------\n        max_size : int\n            Maximum number of elements in output\n        min_density : float\n            Minimum density of output\n        Returns\n        -------\n        numpy.ndarray\n            The dense array.\n        Raises\n        -------\n        ValueError\n            If the returned array would be too large.\n        """"""\n\n        if self.size <= max_size or self.density >= min_density:\n            return self.todense()\n        else:\n            raise ValueError(\n                ""Operation would require converting "" ""large sparse array to dense""\n            )\n\n    def reshape(self, shape, order=""C"", compressed_axes=None):\n        """"""\n        Returns a new :obj:`CSR` or `CSC` array that is a reshaped version of this array.\n        Parameters\n        ----------\n        shape : tuple[int]\n            The desired shape of the output array.\n        Returns\n        -------\n        CSR or CSC\n            The reshaped output array.\n        See Also\n        --------\n        numpy.ndarray.reshape : The equivalent Numpy function.\n        sparse.COO.reshape: The equivalent COO function.\n        Notes\n        -----\n        The :code:`order` parameter is provided just for compatibility with\n        Numpy and isn\'t actually supported.\n\n        """"""\n\n        if order not in {""C"", None}:\n            raise NotImplementedError(""The \'order\' parameter is not supported"")\n        if any(d == -1 for d in shape):\n            extra = int(self.size / np.prod([d for d in shape if d != -1]))\n            shape = tuple([d if d != -1 else extra for d in shape])\n\n        if self.shape == shape:\n            return self\n\n        if self.size != reduce(mul, shape, 1):\n            raise ValueError(\n                ""cannot reshape array of size {} into shape {}"".format(self.size, shape)\n            )\n\n        # there\'s likely a way to do this without decompressing to COO\n        coo = self.tocoo().reshape(shape)\n        return GCXS.from_coo(coo, compressed_axes)\n\n    def resize(self, *args, refcheck=True, compressed_axes=None):\n        """"""\n        This method changes the shape and size of an array in-place.\n\n        Parameters\n        ----------\n        args : tuple, or series of integers\n            The desired shape of the output array.\n\n        See Also\n        --------\n        numpy.ndarray.resize : The equivalent Numpy function.\n        sparse.COO.resize : The equivalent COO function.\n        """"""\n\n        if len(args) == 1 and isinstance(args[0], tuple):\n            shape = args[0]\n        elif all(isinstance(arg, int) for arg in args):\n            shape = tuple(args)\n        else:\n            raise ValueError(""Invalid input"")\n\n        if any(d < 0 for d in shape):\n            raise ValueError(""negative dimensions not allowed"")\n\n        if self.shape == shape:\n            return\n\n        # there\'s likely a way to do this without decompressing to COO\n        coo = self.tocoo()\n        coo.resize(shape)\n        (\n            arg,\n            shape,\n            compressed_shape,\n            compressed_axes,\n            axis_order,\n            reordered_shape,\n            axisptr,\n            fill_value,\n        ) = _from_coo(coo, compressed_axes)\n        self.data, self.indices, self.indptr = arg\n        self.shape = shape\n        self.compressed_shape = compressed_shape\n        self.compressed_axes = compressed_axes\n        self.axis_order = axis_order\n        self.reordered_shape = reordered_shape\n        self.axisptr = axisptr\n'"
sparse/_compressed/convert.py,10,"b'import numpy as np\nimport numba\nfrom numba.typed import List\n\n\ndef convert_to_flat(inds, shape):\n    inds = [np.array(ind) for ind in inds]\n    if any(ind.ndim > 1 for ind in inds):\n        raise IndexError(""Only one-dimensional iterable indices supported."")\n    cols = np.empty(np.prod([ind.size for ind in inds]), dtype=np.intp)\n    shape_bins = transform_shape(shape)\n    increments = List()\n    for i in range(len(inds)):\n        increments.append((inds[i] * shape_bins[i]).astype(np.int32))\n    operations = np.prod([ind.shape[0] for ind in increments[:-1]])\n    return compute_flat(increments, cols, operations)\n\n\n@numba.jit(nopython=True, nogil=True)\ndef compute_flat(increments, cols, operations):  # pragma: no cover\n    start = 0\n    end = increments[-1].shape[0]\n    positions = np.zeros(len(increments) - 1, dtype=np.intp)\n    pos = len(increments) - 2\n    for i in range(operations):\n        if i != 0 and positions[pos] == increments[pos].shape[0]:\n            positions[pos] = 0\n            pos -= 1\n            positions[pos] += 1\n            pos += 1\n        to_add = np.array(\n            [increments[i][positions[i]] for i in range(len(increments) - 1)]\n        ).sum()\n        cols[start:end] = increments[-1] + to_add\n        positions[pos] += 1\n        start += increments[-1].shape[0]\n        end += increments[-1].shape[0]\n    return cols\n\n\ndef transform_shape(shape):\n    """"""\n    turns a shape into the linearized increments that\n    it represents. For example, given (5,5,5), it returns\n    np.array([25,5,1]).\n    """"""\n    shape_bins = np.empty(len(shape), dtype=np.intp)\n    shape_bins[-1] = 1\n    for i in range(len(shape) - 2, -1, -1):\n        shape_bins[i] = np.prod(shape[i + 1 :])\n    return shape_bins\n\n\n@numba.jit(nopython=True, nogil=True)\ndef uncompress_dimension(indptr):  # pragma: no cover\n    """"""converts an index pointer array into an array of coordinates""""""\n    uncompressed = np.empty(indptr[-1], dtype=np.intp)\n    for i in range(len(indptr) - 1):\n        uncompressed[indptr[i] : indptr[i + 1]] = i\n    return uncompressed\n'"
sparse/_compressed/indexing.py,25,"b'import numpy as np\nimport numba\nfrom numbers import Integral\nfrom itertools import zip_longest\nfrom collections.abc import Iterable\nfrom .._slicing import normalize_index\nfrom .convert import convert_to_flat, uncompress_dimension\n\n\ndef getitem(x, key):\n    """"""\n\n\n    """"""\n    from .compressed import GCXS\n\n    if x.ndim == 1:\n        coo = x.tocoo()[key]\n        return GCXS.from_coo(coo)\n\n    key = list(normalize_index(key, x.shape))\n\n    # zip_longest so things like x[..., None] are picked up.\n    if len(key) != 0 and all(\n        isinstance(k, slice) and k == slice(0, dim, 1)\n        for k, dim in zip_longest(key, x.shape)\n    ):\n        return x\n\n    # return a single element\n    if all(isinstance(k, int) for k in key):  # indexing for a single element\n        key = np.array(key)[x.axis_order]  # reordering the input\n        ind = np.ravel_multi_index(key, x.reordered_shape)\n        row, col = np.unravel_index(ind, x.compressed_shape)\n        current_row = x.indices[x.indptr[row] : x.indptr[row + 1]]\n        item = np.searchsorted(current_row, col)\n        if not (item >= current_row.size or current_row[item] != col):\n            item += x.indptr[row]\n            return x.data[item]\n        return x.fill_value\n\n    shape = []\n    compressed_inds = np.zeros(len(x.shape), dtype=np.bool)\n    uncompressed_inds = np.zeros(len(x.shape), dtype=np.bool)\n    shape_key = np.zeros(len(x.shape), dtype=np.intp)\n\n    Nones_removed = [k for k in key if k is not None]\n    count = 0\n    for i, ind in enumerate(Nones_removed):\n        if isinstance(ind, Integral):\n            continue\n        elif ind is None:\n            # handle the None cases at the end\n            continue\n        elif isinstance(ind, slice):\n            shape_key[i] = count\n            shape.append(len(range(ind.start, ind.stop, ind.step)))\n            if i in x.compressed_axes:\n                compressed_inds[i] = True\n            else:\n                uncompressed_inds[i] = True\n        elif isinstance(ind, Iterable):\n            shape_key[i] = count\n            shape.append(len(ind))\n            if i in x.compressed_axes:\n                compressed_inds[i] = True\n            else:\n                uncompressed_inds[i] = True\n        count += 1\n\n    reordered_key = [Nones_removed[i] for i in x.axis_order]\n\n    for i, ind in enumerate(reordered_key):\n        if isinstance(ind, Integral):\n            reordered_key[i] = [ind]\n        elif isinstance(ind, slice):\n            reordered_key[i] = np.arange(ind.start, ind.stop, ind.step)\n\n    shape = np.array(shape)\n\n    rows = convert_to_flat(reordered_key[: x.axisptr], x.reordered_shape[: x.axisptr])\n    cols = convert_to_flat(reordered_key[x.axisptr :], x.reordered_shape[x.axisptr :])\n\n    starts = x.indptr[:-1][rows]\n    ends = x.indptr[1:][rows]\n    if np.any(compressed_inds):\n        compressed_axes = shape_key[compressed_inds]\n\n        if len(compressed_axes) == 1:\n            row_size = shape[compressed_axes]\n        else:\n            row_size = np.prod(shape[compressed_axes])\n\n    else:  # only uncompressed axes\n        compressed_axes = (0,)  # defaults to 0\n        row_size = 1  # this doesn\'t matter\n\n    if not np.any(uncompressed_inds):  # only indexing compressed axes\n        compressed_axes = (0,)  # defaults to 0\n        row_size = starts.size\n\n    indptr = np.empty(row_size + 1, dtype=np.intp)\n    indptr[0] = 0\n    arg = get_array_selection(x.data, x.indices, indptr, starts, ends, cols)\n\n    data, indices, indptr = arg\n    size = np.prod(shape[1:])\n\n    if not np.any(uncompressed_inds):  # only indexing compressed axes\n        uncompressed = uncompress_dimension(indptr)\n        if len(shape) == 1:\n            indices = uncompressed\n            indptr = None\n        else:\n            indices = uncompressed % size\n            indptr = np.empty(shape[0] + 1, dtype=np.intp)\n            indptr[0] = 0\n            np.cumsum(\n                np.bincount(uncompressed // size, minlength=shape[0]), out=indptr[1:]\n            )\n    if not np.any(compressed_inds):\n\n        if len(shape) == 1:\n            indptr = None\n        else:\n            uncompressed = indices // size\n            indptr = np.empty(shape[0] + 1, dtype=np.intp)\n            indptr[0] = 0\n            np.cumsum(np.bincount(uncompressed, minlength=shape[0]), out=indptr[1:])\n            indices = indices % size\n\n    arg = (data, indices, indptr)\n\n    compressed_axes = np.array(compressed_axes)\n    shape = shape.tolist()\n    for i in range(len(key)):\n        if key[i] is None:\n            shape.insert(i, 1)\n            compressed_axes[compressed_axes >= i] += 1\n\n    compressed_axes = tuple(compressed_axes)\n    shape = tuple(shape)\n\n    if len(shape) == 1:\n        compressed_axes = None\n\n    return GCXS(\n        arg, shape=shape, compressed_axes=compressed_axes, fill_value=x.fill_value\n    )\n\n\n@numba.jit(nopython=True, nogil=True)\ndef get_array_selection(\n    arr_data, arr_indices, indptr, starts, ends, col\n):  # pragma: no cover\n    """"""\n    This is a very general algorithm to be used when more optimized methods don\'t apply.\n    It performs a binary search for each of the requested elements.\n    Consequently it roughly scales by O(n log nnz per row) where n is the number of requested elements and\n    nnz per row is the number of nonzero elements in that row.\n    """"""\n    indices = []\n    ind_list = []\n    for i, (start, end) in enumerate(zip(starts, ends)):\n        inds = []\n        current_row = arr_indices[start:end]\n        if len(current_row) == 0:\n            indptr[i + 1] = indptr[i]\n            continue\n        for c in range(len(col)):\n            s = np.searchsorted(current_row, col[c])\n            if not (s >= current_row.size or current_row[s] != col[c]):\n                s += start\n                inds.append(s)\n                indices.append(c)\n        ind_list.extend(inds)\n        indptr[i + 1] = indptr[i] + len(inds)\n    ind_list = np.array(ind_list, dtype=np.int64)\n    indices = np.array(indices)\n    data = arr_data[ind_list]\n    return (data, indices, indptr)\n'"
sparse/_coo/__init__.py,0,"b'from .core import COO, as_coo\nfrom .umath import elemwise\nfrom .common import (\n    tensordot,\n    dot,\n    matmul,\n    concatenate,\n    clip,\n    stack,\n    triu,\n    tril,\n    where,\n    nansum,\n    nanmean,\n    nanprod,\n    nanmin,\n    nanmax,\n    nanreduce,\n    roll,\n    kron,\n    argwhere,\n    isposinf,\n    isneginf,\n    result_type,\n    diagonal,\n    diagonalize,\n)\n\n__all__ = [\n    ""COO"",\n    ""as_coo"",\n    ""elemwise"",\n    ""tensordot"",\n    ""dot"",\n    ""matmul"",\n    ""concatenate"",\n    ""clip"",\n    ""stack"",\n    ""triu"",\n    ""tril"",\n    ""where"",\n    ""nansum"",\n    ""nanmean"",\n    ""nanprod"",\n    ""nanmin"",\n    ""nanmax"",\n    ""nanreduce"",\n    ""roll"",\n    ""kron"",\n    ""argwhere"",\n    ""isposinf"",\n    ""isneginf"",\n    ""result_type"",\n    ""diagonal"",\n    ""diagonalize"",\n]\n'"
sparse/_coo/common.py,89,"b'from functools import reduce, wraps\nfrom itertools import chain\nimport operator\nimport warnings\nfrom collections.abc import Iterable\n\nimport numpy as np\nimport scipy.sparse\nimport numba\n\nfrom .._sparse_array import SparseArray\nfrom .._utils import (\n    isscalar,\n    normalize_axis,\n    check_zero_fill_value,\n    check_consistent_fill_value,\n)\n\n\ndef asCOO(x, name=""asCOO"", check=True):\n    """"""\n    Convert the input to :obj:`COO`. Passes through :obj:`COO` objects as-is.\n\n    Parameters\n    ----------\n    x : Union[SparseArray, scipy.sparse.spmatrix, numpy.ndarray]\n        The input array to convert.\n    name : str, optional\n        The name of the operation to use in the exception.\n    check : bool, optional\n        Whether to check for a dense input.\n\n    Returns\n    -------\n    COO\n        The converted :obj:`COO` array.\n\n    Raises\n    ------\n    ValueError\n        If ``check`` is true and a dense input is supplied.\n    """"""\n    from .core import COO\n\n    if check and not isinstance(x, (SparseArray, scipy.sparse.spmatrix)):\n        raise ValueError(\n            ""Performing this operation would produce a dense result: %s"" % name\n        )\n\n    if not isinstance(x, COO):\n        x = COO(x)\n\n    return x\n\n\ndef linear_loc(coords, shape):\n    if shape == () and len(coords) == 0:\n        # `np.ravel_multi_index` is not aware of arrays, so cannot produce a\n        # sensible result here (https://github.com/numpy/numpy/issues/15690).\n        # Since `coords` is an array and not a sequence, we know the correct\n        # dimensions.\n        return np.zeros(coords.shape[1:], dtype=np.intp)\n    else:\n        return np.ravel_multi_index(coords, shape)\n\n\ndef tensordot(a, b, axes=2, *, return_type=None):\n    """"""\n    Perform the equivalent of :obj:`numpy.tensordot`.\n\n    Parameters\n    ----------\n    a, b : Union[COO, np.ndarray, scipy.sparse.spmatrix]\n        The arrays to perform the :code:`tensordot` operation on.\n    axes : tuple[Union[int, tuple[int], Union[int, tuple[int]], optional\n        The axes to match when performing the sum.\n    return_type : {None, COO, np.ndarray}, optional\n        Type of returned array.\n\n\n    Returns\n    -------\n    Union[COO, numpy.ndarray]\n        The result of the operation.\n\n    Raises\n    ------\n    ValueError\n        If all arguments don\'t have zero fill-values.\n\n    See Also\n    --------\n    numpy.tensordot : NumPy equivalent function\n    """"""\n    # Much of this is stolen from numpy/core/numeric.py::tensordot\n    # Please see license at https://github.com/numpy/numpy/blob/master/LICENSE.txt\n    check_zero_fill_value(a, b)\n\n    if scipy.sparse.issparse(a):\n        a = asCOO(a)\n    if scipy.sparse.issparse(b):\n        b = asCOO(b)\n\n    try:\n        iter(axes)\n    except TypeError:\n        axes_a = list(range(-axes, 0))\n        axes_b = list(range(0, axes))\n    else:\n        axes_a, axes_b = axes\n    try:\n        na = len(axes_a)\n        axes_a = list(axes_a)\n    except TypeError:\n        axes_a = [axes_a]\n        na = 1\n    try:\n        nb = len(axes_b)\n        axes_b = list(axes_b)\n    except TypeError:\n        axes_b = [axes_b]\n        nb = 1\n\n    # a, b = asarray(a), asarray(b)  # <--- modified\n    as_ = a.shape\n    nda = a.ndim\n    bs = b.shape\n    ndb = b.ndim\n    equal = True\n    if nda == 0 or ndb == 0:\n        pos = int(nda != 0)\n        raise ValueError(""Input {} operand does not have enough dimensions"".format(pos))\n    if na != nb:\n        equal = False\n    else:\n        for k in range(na):\n            if as_[axes_a[k]] != bs[axes_b[k]]:\n                equal = False\n                break\n            if axes_a[k] < 0:\n                axes_a[k] += nda\n            if axes_b[k] < 0:\n                axes_b[k] += ndb\n    if not equal:\n        raise ValueError(""shape-mismatch for sum"")\n\n    # Move the axes to sum over to the end of ""a""\n    # and to the front of ""b""\n    notin = [k for k in range(nda) if k not in axes_a]\n    newaxes_a = notin + axes_a\n    N2 = 1\n    for axis in axes_a:\n        N2 *= as_[axis]\n    newshape_a = (-1, N2)\n    olda = [as_[axis] for axis in notin]\n\n    notin = [k for k in range(ndb) if k not in axes_b]\n    newaxes_b = axes_b + notin\n    N2 = 1\n    for axis in axes_b:\n        N2 *= bs[axis]\n    newshape_b = (N2, -1)\n    oldb = [bs[axis] for axis in notin]\n\n    if any(dim == 0 for dim in chain(newshape_a, newshape_b)):\n        res = asCOO(np.empty(olda + oldb), check=False)\n        if isinstance(a, np.ndarray) or isinstance(b, np.ndarray):\n            res = res.todense()\n\n        return res\n\n    at = a.transpose(newaxes_a).reshape(newshape_a)\n    bt = b.transpose(newaxes_b).reshape(newshape_b)\n    res = _dot(at, bt, return_type)\n    return res.reshape(olda + oldb)\n\n\ndef matmul(a, b):\n    """"""Perform the equivalent of :obj:`numpy.matmul` on two arrays.\n\n    Parameters\n    ----------\n    a, b : Union[COO, np.ndarray, scipy.sparse.spmatrix]\n        The arrays to perform the :code:`matmul` operation on.\n\n    Returns\n    -------\n    Union[COO, numpy.ndarray]\n        The result of the operation.\n\n    Raises\n    ------\n    ValueError\n        If all arguments don\'t have zero fill-values, or the shape of the two arrays is not broadcastable.\n\n    See Also\n    --------\n    numpy.matmul : NumPy equivalent function.\n    COO.__matmul__ : Equivalent function for COO objects.\n    """"""\n    check_zero_fill_value(a, b)\n    if not hasattr(a, ""ndim"") or not hasattr(b, ""ndim""):\n        raise TypeError(\n            ""Cannot perform dot product on types %s, %s"" % (type(a), type(b))\n        )\n\n    # When b is 2-d, it is equivalent to dot\n    if b.ndim <= 2:\n        return dot(a, b)\n\n    # when a is 2-d, we need to transpose result after dot\n    if a.ndim <= 2:\n        res = dot(a, b)\n        axes = list(range(res.ndim))\n        axes.insert(-1, axes.pop(0))\n        return res.transpose(axes)\n\n    # If a can be squeeze to a vector, use dot will be faster\n    if a.ndim <= b.ndim and np.prod(a.shape[:-1]) == 1:\n        res = dot(a.reshape(-1), b)\n        shape = list(res.shape)\n        shape.insert(-1, 1)\n        return res.reshape(shape)\n\n    # If b can be squeeze to a matrix, use dot will be faster\n    if b.ndim <= a.ndim and np.prod(b.shape[:-2]) == 1:\n        return dot(a, b.reshape(b.shape[-2:]))\n\n    if a.ndim < b.ndim:\n        a = a[(None,) * (b.ndim - a.ndim)]\n    if a.ndim > b.ndim:\n        b = b[(None,) * (a.ndim - b.ndim)]\n    for i, j in zip(a.shape[:-2], b.shape[:-2]):\n        if i != 1 and j != 1 and i != j:\n            raise ValueError(""shapes of a and b are not broadcastable"")\n\n    def _matmul_recurser(a, b):\n        if a.ndim == 2:\n            return dot(a, b)\n        res = []\n        for i in range(max(a.shape[0], b.shape[0])):\n            a_i = a[0] if a.shape[0] == 1 else a[i]\n            b_i = b[0] if b.shape[0] == 1 else b[i]\n            res.append(_matmul_recurser(a_i, b_i))\n        mask = [isinstance(x, SparseArray) for x in res]\n        if all(mask):\n            return stack(res)\n        else:\n            res = [x.todense() if isinstance(x, SparseArray) else x for x in res]\n            return np.stack(res)\n\n    return _matmul_recurser(a, b)\n\n\ndef dot(a, b):\n    """"""\n    Perform the equivalent of :obj:`numpy.dot` on two arrays.\n\n    Parameters\n    ----------\n    a, b : Union[COO, np.ndarray, scipy.sparse.spmatrix]\n        The arrays to perform the :code:`dot` operation on.\n\n    Returns\n    -------\n    Union[COO, numpy.ndarray]\n        The result of the operation.\n\n    Raises\n    ------\n    ValueError\n        If all arguments don\'t have zero fill-values.\n\n    See Also\n    --------\n    numpy.dot : NumPy equivalent function.\n    COO.dot : Equivalent function for COO objects.\n    """"""\n    check_zero_fill_value(a, b)\n    if not hasattr(a, ""ndim"") or not hasattr(b, ""ndim""):\n        raise TypeError(\n            ""Cannot perform dot product on types %s, %s"" % (type(a), type(b))\n        )\n\n    if a.ndim == 1 and b.ndim == 1:\n        return (a * b).sum()\n\n    a_axis = -1\n    b_axis = -2\n\n    if b.ndim == 1:\n        b_axis = -1\n    return tensordot(a, b, axes=(a_axis, b_axis))\n\n\ndef _dot(a, b, return_type=None):\n    from .core import COO\n\n    out_shape = (a.shape[0], b.shape[1])\n    if isinstance(a, COO) and isinstance(b, COO):\n        b = b.T\n        coords, data = _dot_coo_coo_type(a.dtype, b.dtype)(\n            a.coords, a.data, b.coords, b.data\n        )\n\n        if return_type == np.ndarray:\n            return COO(\n                coords, data, shape=out_shape, has_duplicates=False, sorted=True\n            ).todense()\n\n        return COO(coords, data, shape=out_shape, has_duplicates=False, sorted=True)\n\n    if isinstance(a, COO) and isinstance(b, np.ndarray):\n        b = b.view(type=np.ndarray).T\n\n        if return_type == COO:\n            coords, data = _dot_coo_ndarray_type_sparse(a.dtype, b.dtype)(\n                a.coords, a.data, b, out_shape\n            )\n            return COO(coords, data, shape=out_shape, has_duplicates=False, sorted=True)\n\n        return _dot_coo_ndarray_type(a.dtype, b.dtype)(a.coords, a.data, b, out_shape)\n\n    if isinstance(a, np.ndarray) and isinstance(b, COO):\n        b = b.T\n        a = a.view(type=np.ndarray)\n\n        if return_type == COO:\n            coords, data = _dot_ndarray_coo_type_sparse(a.dtype, b.dtype)(\n                a, b.coords, b.data, out_shape\n            )\n            return COO(coords, data, shape=out_shape, has_duplicates=False, sorted=True)\n\n        return _dot_ndarray_coo_type(a.dtype, b.dtype)(a, b.coords, b.data, out_shape)\n\n\ndef kron(a, b):\n    """"""Kronecker product of 2 sparse arrays.\n\n    Parameters\n    ----------\n    a, b : SparseArray, scipy.sparse.spmatrix, or np.ndarray\n        The arrays over which to compute the Kronecker product.\n\n    Returns\n    -------\n    res : COO\n        The kronecker product\n\n    Raises\n    ------\n    ValueError\n        If all arguments are dense or arguments have nonzero fill-values.\n\n    Examples\n    --------\n    >>> from sparse import eye\n    >>> a = eye(3, dtype=\'i8\')\n    >>> b = np.array([1, 2, 3], dtype=\'i8\')\n    >>> res = kron(a, b)\n    >>> res.todense()  # doctest: +SKIP\n    array([[1, 2, 3, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 1, 2, 3, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 1, 2, 3]], dtype=int64)\n    """"""\n    from .core import COO\n    from .umath import _cartesian_product\n\n    check_zero_fill_value(a, b)\n\n    a_sparse = isinstance(a, (SparseArray, scipy.sparse.spmatrix))\n    b_sparse = isinstance(b, (SparseArray, scipy.sparse.spmatrix))\n    a_ndim = np.ndim(a)\n    b_ndim = np.ndim(b)\n\n    if not (a_sparse or b_sparse):\n        raise ValueError(\n            ""Performing this operation would produce a dense "" ""result: kron""\n        )\n\n    if a_ndim == 0 or b_ndim == 0:\n        return a * b\n\n    a = asCOO(a, check=False)\n    b = asCOO(b, check=False)\n\n    # Match dimensions\n    max_dim = max(a.ndim, b.ndim)\n    a = a.reshape((1,) * (max_dim - a.ndim) + a.shape)\n    b = b.reshape((1,) * (max_dim - b.ndim) + b.shape)\n\n    a_idx, b_idx = _cartesian_product(np.arange(a.nnz), np.arange(b.nnz))\n\n    a_expanded_coords = a.coords[:, a_idx]\n    b_expanded_coords = b.coords[:, b_idx]\n    o_coords = a_expanded_coords * np.asarray(b.shape)[:, None] + b_expanded_coords\n    o_data = a.data[a_idx] * b.data[b_idx]\n    o_shape = tuple(i * j for i, j in zip(a.shape, b.shape))\n\n    return COO(o_coords, o_data, shape=o_shape, has_duplicates=False)\n\n\ndef concatenate(arrays, axis=0):\n    """"""\n    Concatenate the input arrays along the given dimension.\n\n    Parameters\n    ----------\n    arrays : Iterable[SparseArray]\n        The input arrays to concatenate.\n    axis : int, optional\n        The axis along which to concatenate the input arrays. The default is zero.\n\n    Returns\n    -------\n    COO\n        The output concatenated array.\n\n    Raises\n    ------\n    ValueError\n        If all elements of :code:`arrays` don\'t have the same fill-value.\n\n    See Also\n    --------\n    numpy.concatenate : NumPy equivalent function\n    """"""\n    from .core import COO\n\n    check_consistent_fill_value(arrays)\n\n    arrays = [x if isinstance(x, COO) else COO(x) for x in arrays]\n    axis = normalize_axis(axis, arrays[0].ndim)\n    assert all(\n        x.shape[ax] == arrays[0].shape[ax]\n        for x in arrays\n        for ax in set(range(arrays[0].ndim)) - {axis}\n    )\n    nnz = 0\n    dim = sum(x.shape[axis] for x in arrays)\n    shape = list(arrays[0].shape)\n    shape[axis] = dim\n\n    data = np.concatenate([x.data for x in arrays])\n    coords = np.concatenate([x.coords for x in arrays], axis=1)\n\n    dim = 0\n    for x in arrays:\n        if dim:\n            coords[axis, nnz : x.nnz + nnz] += dim\n        dim += x.shape[axis]\n        nnz += x.nnz\n\n    return COO(\n        coords,\n        data,\n        shape=shape,\n        has_duplicates=False,\n        sorted=(axis == 0),\n        fill_value=arrays[0].fill_value,\n    )\n\n\ndef stack(arrays, axis=0):\n    """"""\n    Stack the input arrays along the given dimension.\n\n    Parameters\n    ----------\n    arrays : Iterable[SparseArray]\n        The input arrays to stack.\n    axis : int, optional\n        The axis along which to stack the input arrays.\n\n    Returns\n    -------\n    COO\n        The output stacked array.\n\n    Raises\n    ------\n    ValueError\n        If all elements of :code:`arrays` don\'t have the same fill-value.\n\n    See Also\n    --------\n    numpy.stack : NumPy equivalent function\n    """"""\n    from .core import COO\n\n    check_consistent_fill_value(arrays)\n\n    assert len({x.shape for x in arrays}) == 1\n    arrays = [x if isinstance(x, COO) else COO(x) for x in arrays]\n    axis = normalize_axis(axis, arrays[0].ndim + 1)\n    data = np.concatenate([x.data for x in arrays])\n    coords = np.concatenate([x.coords for x in arrays], axis=1)\n    shape = list(arrays[0].shape)\n    shape.insert(axis, len(arrays))\n\n    nnz = 0\n    dim = 0\n    new = np.empty(shape=(coords.shape[1],), dtype=np.intp)\n    for x in arrays:\n        new[nnz : x.nnz + nnz] = dim\n        dim += 1\n        nnz += x.nnz\n\n    coords = [coords[i] for i in range(coords.shape[0])]\n    coords.insert(axis, new)\n    coords = np.stack(coords, axis=0)\n\n    return COO(\n        coords,\n        data,\n        shape=shape,\n        has_duplicates=False,\n        sorted=(axis == 0),\n        fill_value=arrays[0].fill_value,\n    )\n\n\ndef triu(x, k=0):\n    """"""\n    Returns an array with all elements below the k-th diagonal set to zero.\n\n    Parameters\n    ----------\n    x : COO\n        The input array.\n    k : int, optional\n        The diagonal below which elements are set to zero. The default is\n        zero, which corresponds to the main diagonal.\n\n    Returns\n    -------\n    COO\n        The output upper-triangular matrix.\n\n    Raises\n    ------\n    ValueError\n        If :code:`x` doesn\'t have zero fill-values.\n\n    See Also\n    --------\n    numpy.triu : NumPy equivalent function\n    """"""\n    from .core import COO\n\n    check_zero_fill_value(x)\n\n    if not x.ndim >= 2:\n        raise NotImplementedError(\n            ""sparse.triu is not implemented for scalars or 1-D arrays.""\n        )\n\n    mask = x.coords[-2] + k <= x.coords[-1]\n\n    coords = x.coords[:, mask]\n    data = x.data[mask]\n\n    return COO(coords, data, shape=x.shape, has_duplicates=False, sorted=True)\n\n\ndef tril(x, k=0):\n    """"""\n    Returns an array with all elements above the k-th diagonal set to zero.\n\n    Parameters\n    ----------\n    x : COO\n        The input array.\n    k : int, optional\n        The diagonal above which elements are set to zero. The default is\n        zero, which corresponds to the main diagonal.\n\n    Returns\n    -------\n    COO\n        The output lower-triangular matrix.\n\n    Raises\n    ------\n    ValueError\n        If :code:`x` doesn\'t have zero fill-values.\n\n    See Also\n    --------\n    numpy.tril : NumPy equivalent function\n    """"""\n    from .core import COO\n\n    check_zero_fill_value(x)\n\n    if not x.ndim >= 2:\n        raise NotImplementedError(\n            ""sparse.tril is not implemented for scalars or 1-D arrays.""\n        )\n\n    mask = x.coords[-2] + k >= x.coords[-1]\n\n    coords = x.coords[:, mask]\n    data = x.data[mask]\n\n    return COO(coords, data, shape=x.shape, has_duplicates=False, sorted=True)\n\n\ndef nansum(x, axis=None, keepdims=False, dtype=None, out=None):\n    """"""\n    Performs a ``NaN`` skipping sum operation along the given axes. Uses all axes by default.\n\n    Parameters\n    ----------\n    x : SparseArray\n        The array to perform the reduction on.\n    axis : Union[int, Iterable[int]], optional\n        The axes along which to sum. Uses all axes by default.\n    keepdims : bool, optional\n        Whether or not to keep the dimensions of the original array.\n    dtype: numpy.dtype\n        The data type of the output array.\n\n    Returns\n    -------\n    COO\n        The reduced output sparse array.\n\n    See Also\n    --------\n    :obj:`COO.sum` : Function without ``NaN`` skipping.\n    numpy.nansum : Equivalent Numpy function.\n    """"""\n    assert out is None\n    x = asCOO(x, name=""nansum"")\n    return nanreduce(x, np.add, axis=axis, keepdims=keepdims, dtype=dtype)\n\n\ndef nanmean(x, axis=None, keepdims=False, dtype=None, out=None):\n    """"""\n    Performs a ``NaN`` skipping mean operation along the given axes. Uses all axes by default.\n\n    Parameters\n    ----------\n    x : SparseArray\n        The array to perform the reduction on.\n    axis : Union[int, Iterable[int]], optional\n        The axes along which to compute the mean. Uses all axes by default.\n    keepdims : bool, optional\n        Whether or not to keep the dimensions of the original array.\n    dtype: numpy.dtype\n        The data type of the output array.\n\n    Returns\n    -------\n    COO\n        The reduced output sparse array.\n\n    See Also\n    --------\n    :obj:`COO.mean` : Function without ``NaN`` skipping.\n    numpy.nanmean : Equivalent Numpy function.\n    """"""\n    assert out is None\n    x = asCOO(x, name=""nanmean"")\n\n    if not np.issubdtype(x.dtype, np.floating):\n        return x.mean(axis=axis, keepdims=keepdims, dtype=dtype)\n\n    mask = np.isnan(x)\n    x2 = where(mask, 0, x)\n\n    # Count the number non-nan elements along axis\n    nancount = mask.sum(axis=axis, dtype=""i8"", keepdims=keepdims)\n    if axis is None:\n        axis = tuple(range(x.ndim))\n    elif not isinstance(axis, tuple):\n        axis = (axis,)\n    den = reduce(operator.mul, (x.shape[i] for i in axis), 1)\n    den -= nancount\n\n    if (den == 0).any():\n        warnings.warn(""Mean of empty slice"", RuntimeWarning, stacklevel=2)\n\n    num = np.sum(x2, axis=axis, dtype=dtype, keepdims=keepdims)\n\n    with np.errstate(invalid=""ignore"", divide=""ignore""):\n        if num.ndim:\n            return np.true_divide(num, den, casting=""unsafe"")\n        return (num / den).astype(dtype)\n\n\ndef nanmax(x, axis=None, keepdims=False, dtype=None, out=None):\n    """"""\n    Maximize along the given axes, skipping ``NaN`` values. Uses all axes by default.\n\n    Parameters\n    ----------\n    x : SparseArray\n        The array to perform the reduction on.\n    axis : Union[int, Iterable[int]], optional\n        The axes along which to maximize. Uses all axes by default.\n    keepdims : bool, optional\n        Whether or not to keep the dimensions of the original array.\n    dtype: numpy.dtype\n        The data type of the output array.\n\n    Returns\n    -------\n    COO\n        The reduced output sparse array.\n\n    See Also\n    --------\n    :obj:`COO.max` : Function without ``NaN`` skipping.\n    numpy.nanmax : Equivalent Numpy function.\n    """"""\n    assert out is None\n    x = asCOO(x, name=""nanmax"")\n\n    ar = x.reduce(np.fmax, axis=axis, keepdims=keepdims, dtype=dtype)\n\n    if (isscalar(ar) and np.isnan(ar)) or np.isnan(ar.data).any():\n        warnings.warn(""All-NaN slice encountered"", RuntimeWarning, stacklevel=2)\n\n    return ar\n\n\ndef nanmin(x, axis=None, keepdims=False, dtype=None, out=None):\n    """"""\n    Minimize along the given axes, skipping ``NaN`` values. Uses all axes by default.\n\n    Parameters\n    ----------\n    x : SparseArray\n        The array to perform the reduction on.\n    axis : Union[int, Iterable[int]], optional\n        The axes along which to minimize. Uses all axes by default.\n    keepdims : bool, optional\n        Whether or not to keep the dimensions of the original array.\n    dtype: numpy.dtype\n        The data type of the output array.\n\n    Returns\n    -------\n    COO\n        The reduced output sparse array.\n\n    See Also\n    --------\n    :obj:`COO.min` : Function without ``NaN`` skipping.\n    numpy.nanmin : Equivalent Numpy function.\n    """"""\n    assert out is None\n    x = asCOO(x, name=""nanmin"")\n\n    ar = x.reduce(np.fmin, axis=axis, keepdims=keepdims, dtype=dtype)\n\n    if (isscalar(ar) and np.isnan(ar)) or np.isnan(ar.data).any():\n        warnings.warn(""All-NaN slice encountered"", RuntimeWarning, stacklevel=2)\n\n    return ar\n\n\ndef nanprod(x, axis=None, keepdims=False, dtype=None, out=None):\n    """"""\n    Performs a product operation along the given axes, skipping ``NaN`` values.\n    Uses all axes by default.\n\n    Parameters\n    ----------\n    x : SparseArray\n        The array to perform the reduction on.\n    axis : Union[int, Iterable[int]], optional\n        The axes along which to multiply. Uses all axes by default.\n    keepdims : bool, optional\n        Whether or not to keep the dimensions of the original array.\n    dtype: numpy.dtype\n        The data type of the output array.\n\n    Returns\n    -------\n    COO\n        The reduced output sparse array.\n\n    See Also\n    --------\n    :obj:`COO.prod` : Function without ``NaN`` skipping.\n    numpy.nanprod : Equivalent Numpy function.\n    """"""\n    assert out is None\n    x = asCOO(x)\n    return nanreduce(x, np.multiply, axis=axis, keepdims=keepdims, dtype=dtype)\n\n\ndef where(condition, x=None, y=None):\n    """"""\n    Select values from either ``x`` or ``y`` depending on ``condition``.\n    If ``x`` and ``y`` are not given, returns indices where ``condition``\n    is nonzero.\n\n    Performs the equivalent of :obj:`numpy.where`.\n\n    Parameters\n    ----------\n    condition : SparseArray\n        The condition based on which to select values from\n        either ``x`` or ``y``.\n    x : SparseArray, optional\n        The array to select values from if ``condition`` is nonzero.\n    y : SparseArray, optional\n        The array to select values from if ``condition`` is zero.\n\n    Returns\n    -------\n    COO\n        The output array with selected values if ``x`` and ``y`` are given;\n        else where the array is nonzero.\n\n    Raises\n    ------\n    ValueError\n        If the operation would produce a dense result; or exactly one of\n        ``x`` and ``y`` are given.\n\n    See Also\n    --------\n    numpy.where : Equivalent Numpy function.\n    """"""\n    from .umath import elemwise\n\n    x_given = x is not None\n    y_given = y is not None\n\n    if not (x_given or y_given):\n        condition = asCOO(condition, name=str(np.where))\n        return tuple(condition.coords)\n\n    if x_given != y_given:\n        raise ValueError(""either both or neither of x and y should be given"")\n\n    return elemwise(np.where, condition, x, y)\n\n\ndef argwhere(a):\n    """"""\n    Find the indices of array elements that are non-zero, grouped by element.\n\n    Parameters\n    ----------\n    a: array_like\n        Input data.\n\n    Returns\n    -------\n    index_array: numpy.ndarray\n\n    See Also\n    --------\n    :obj:`where`, :obj:`COO.nonzero`\n\n    Examples\n    --------\n    >>> import sparse\n    >>> x = sparse.COO(np.arange(6).reshape((2, 3)))\n    >>> sparse.argwhere(x > 1)\n    array([[0, 2],\n           [1, 0],\n           [1, 1],\n           [1, 2]])\n    """"""\n    return np.transpose(a.nonzero())\n\n\ndef _replace_nan(array, value):\n    """"""\n    Replaces ``NaN``s in ``array`` with ``value``.\n\n    Parameters\n    ----------\n    array : COO\n        The input array.\n    value : numpy.number\n        The values to replace ``NaN`` with.\n\n    Returns\n    -------\n    COO\n        A copy of ``array`` with the ``NaN``s replaced.\n    """"""\n    if not np.issubdtype(array.dtype, np.floating):\n        return array\n\n    return where(np.isnan(array), value, array)\n\n\ndef nanreduce(x, method, identity=None, axis=None, keepdims=False, **kwargs):\n    """"""\n    Performs an ``NaN`` skipping reduction on this array. See the documentation\n    on :obj:`COO.reduce` for examples.\n\n    Parameters\n    ----------\n    x : COO\n        The array to reduce.\n    method : numpy.ufunc\n        The method to use for performing the reduction.\n    identity : numpy.number\n        The identity value for this reduction. Inferred from ``method`` if not given.\n        Note that some ``ufunc`` objects don\'t have this, so it may be necessary to give it.\n    axis : Union[int, Iterable[int]], optional\n        The axes along which to perform the reduction. Uses all axes by default.\n    keepdims : bool, optional\n        Whether or not to keep the dimensions of the original array.\n    kwargs : dict\n        Any extra arguments to pass to the reduction operation.\n\n    Returns\n    -------\n    COO\n        The result of the reduction operation.\n\n    Raises\n    ------\n    ValueError\n        If reducing an all-zero axis would produce a nonzero result.\n\n    See Also\n    --------\n    COO.reduce : Similar method without ``NaN`` skipping functionality.\n    """"""\n    arr = _replace_nan(x, method.identity if identity is None else identity)\n    return arr.reduce(method, axis, keepdims, **kwargs)\n\n\ndef roll(a, shift, axis=None):\n    """"""\n    Shifts elements of an array along specified axis. Elements that roll beyond\n    the last position are circulated and re-introduced at the first.\n\n    Parameters\n    ----------\n    x : COO\n        Input array\n    shift : int or tuple of ints\n        Number of index positions that elements are shifted. If a tuple is\n        provided, then axis must be a tuple of the same size, and each of the\n        given axes is shifted by the corresponding number. If an int while axis\n        is a tuple of ints, then broadcasting is used so the same shift is\n        applied to all axes.\n    axis : int or tuple of ints, optional\n        Axis or tuple specifying multiple axes. By default, the\n        array is flattened before shifting, after which the original shape is\n        restored.\n\n    Returns\n    -------\n    res : ndarray\n        Output array, with the same shape as a.\n    """"""\n    from .core import COO, as_coo\n\n    a = as_coo(a)\n\n    # roll flattened array\n    if axis is None:\n        return roll(a.reshape((-1,)), shift, 0).reshape(a.shape)\n\n    # roll across specified axis\n    else:\n        # parse axis input, wrap in tuple\n        axis = normalize_axis(axis, a.ndim)\n        if not isinstance(axis, tuple):\n            axis = (axis,)\n\n        # make shift iterable\n        if not isinstance(shift, Iterable):\n            shift = (shift,)\n\n        elif np.ndim(shift) > 1:\n            raise ValueError(""\'shift\' and \'axis\' must be integers or 1D sequences."")\n\n        # handle broadcasting\n        if len(shift) == 1:\n            shift = np.full(len(axis), shift)\n\n        # check if dimensions are consistent\n        if len(axis) != len(shift):\n            raise ValueError(\n                ""If \'shift\' is a 1D sequence, "" ""\'axis\' must have equal length.""\n            )\n\n        # shift elements\n        coords, data = np.copy(a.coords), np.copy(a.data)\n        for sh, ax in zip(shift, axis):\n            coords[ax] += sh\n            coords[ax] %= a.shape[ax]\n\n        return COO(\n            coords,\n            data=data,\n            shape=a.shape,\n            has_duplicates=False,\n            fill_value=a.fill_value,\n        )\n\n\ndef diagonal(a, offset=0, axis1=0, axis2=1):\n    """"""\n    Extract diagonal from a COO array. The equivalent of :obj:`numpy.diagonal`.\n\n    Parameters\n    ----------\n    a: COO\n        The array to perform the operation on.\n    offset: int, optional\n        Offset of the diagonal from the main diagonal. Defaults to main diagonal (0).\n    axis1: int, optional\n        First axis from which the diagonals should be taken.  \n        Defaults to first axis (0).\n    axis2 : int, optional\n        Second axis from which the diagonals should be taken.  \n        Defaults to second axis (1).\n\n    Examples\n    --------\n    >>> import sparse\n    >>> x = sparse.as_coo(np.arange(9).reshape(3,3))\n    >>> sparse.diagonal(x).todense()\n    array([0, 4, 8])\n    >>> sparse.diagonal(x,offset=1).todense()\n    array([1, 5])\n\n    >>> x = sparse.as_coo(np.arange(12).reshape((2,3,2)))\n    >>> x_diag = sparse.diagonal(x, axis1=0, axis2=2)\n    >>> x_diag.shape\n    (3, 2)\n    >>> x_diag.todense()\n    array([[ 0,  7],\n           [ 2,  9],\n           [ 4, 11]])\n\n    Returns\n    -------\n    out: COO\n        The result of the operation.\n\n    Raises\n    ------\n    ValueError\n        If a.shape[axis1] != a.shape[axis2]\n\n    See Also\n    --------\n    :obj:`numpy.diagonal`: NumPy equivalent function\n    """"""\n    from .core import COO\n\n    if a.shape[axis1] != a.shape[axis2]:\n        raise ValueError(""a.shape[axis1] != a.shape[axis2]"")\n\n    diag_axes = [\n        axis for axis in range(len(a.shape)) if axis != axis1 and axis != axis2\n    ] + [axis1]\n    diag_shape = [a.shape[axis] for axis in diag_axes]\n    diag_shape[-1] -= abs(offset)\n\n    diag_idx = _diagonal_idx(a.coords, axis1, axis2, offset)\n\n    diag_coords = [a.coords[axis][diag_idx] for axis in diag_axes]\n    diag_data = a.data[diag_idx]\n\n    return COO(diag_coords, diag_data, diag_shape)\n\n\ndef diagonalize(a, axis=0):\n    """"""\n    Diagonalize a COO array. The new dimension is appended at the end.\n\n    .. WARNING:: :obj:`diagonalize` is not :obj:`numpy` compatible as there is no direct :obj:`numpy` equivalent. The API may change in the future.\n\n    Parameters\n    ----------\n    a: Union[COO, np.ndarray, scipy.sparse.spmatrix]\n        The array to diagonalize.    \n    axis: int, optional\n        The axis to diagonalize. Defaults to first axis (0).\n\n    Examples\n    --------\n    >>> import sparse\n    >>> x = sparse.as_coo(np.arange(1,4))\n    >>> sparse.diagonalize(x).todense()\n    array([[1, 0, 0],\n           [0, 2, 0],\n           [0, 0, 3]])\n\n    >>> x = sparse.as_coo(np.arange(24).reshape((2,3,4)))\n    >>> x_diag = sparse.diagonalize(x, axis=1)\n    >>> x_diag.shape\n    (2, 3, 4, 3)\n\n    :obj:`diagonalize` is the inverse of :obj:`diagonal`\n\n    >>> a = sparse.random((3,3,3,3,3), density=0.3)\n    >>> a_diag = sparse.diagonalize(a, axis=2)\n    >>> (sparse.diagonal(a_diag, axis1=2, axis2=5) == a.transpose([0,1,3,4,2])).all()\n    True\n\n    Returns\n    -------\n    out: COO\n        The result of the operation.\n\n    See Also\n    --------\n    :obj:`numpy.diag`: NumPy equivalent for 1D array\n    """"""\n    from .core import COO, as_coo\n\n    a = as_coo(a)\n\n    diag_shape = a.shape + (a.shape[axis],)\n    diag_coords = np.vstack([a.coords, a.coords[axis]])\n\n    return COO(diag_coords, a.data, diag_shape)\n\n\ndef _memoize_dtype(f):\n    """"""\n    Memoizes a function taking in NumPy dtypes.\n\n    Parameters\n    ----------\n    f : Callable\n\n    Returns\n    -------\n    wrapped : Callable\n\n    Examples\n    --------\n    >>> def func(dt1):\n    ...     return object()\n    >>> func = _memoize_dtype(func)\n    >>> func(np.dtype(\'i8\')) is func(np.dtype(\'int64\'))\n    True\n    >>> func(np.dtype(\'i8\')) is func(np.dtype(\'i4\'))\n    False\n    """"""\n    cache = {}\n\n    @wraps(f)\n    def wrapped(*args):\n        key = tuple(arg.name for arg in args)\n        if key in cache:\n            return cache[key]\n\n        result = f(*args)\n        cache[key] = result\n        return result\n\n    return wrapped\n\n\n@_memoize_dtype\ndef _dot_coo_coo_type(dt1, dt2):\n    dtr = np.result_type(dt1, dt2)\n\n    @numba.jit(\n        nopython=True,\n        nogil=True,\n        locals={""data_curr"": numba.np.numpy_support.from_dtype(dtr)},\n    )\n    def _dot_coo_coo(coords1, data1, coords2, data2):  # pragma: no cover\n        """"""\n        Utility function taking in two ``COO`` objects and calculating a ""sense""\n        of their dot product. Acually computes ``s1 @ s2.T``.\n\n        Parameters\n        ----------\n        data1, coords1 : np.ndarray\n            The data and coordinates of ``s1``.\n\n        data2, coords2 : np.ndarray\n            The data and coordinates of ``s2``.\n        """"""\n        coords_out = []\n        data_out = []\n        didx1 = 0\n        data1_end = len(data1)\n        data2_end = len(data2)\n\n        while didx1 < data1_end:\n            oidx1 = coords1[0, didx1]\n            didx2 = 0\n            didx1_curr = didx1\n\n            while (\n                didx2 < data2_end and didx1 < data1_end and coords1[0, didx1] == oidx1\n            ):\n                oidx2 = coords2[0, didx2]\n                data_curr = 0\n\n                while (\n                    didx2 < data2_end\n                    and didx1 < data1_end\n                    and coords2[0, didx2] == oidx2\n                    and coords1[0, didx1] == oidx1\n                ):\n                    c1 = coords1[1, didx1]\n                    c2 = coords2[1, didx2]\n                    k = min(c1, c2)\n                    if c1 == k and c2 == k:\n                        data_curr += data1[didx1] * data2[didx2]\n                    didx1 += c1 == k\n                    didx2 += c2 == k\n\n                while didx2 < data2_end and coords2[0, didx2] == oidx2:\n                    didx2 += 1\n\n                if didx2 < data2_end:\n                    didx1 = didx1_curr\n\n                if data_curr != 0:\n                    coords_out.append((oidx1, oidx2))\n                    data_out.append(data_curr)\n\n            while didx1 < data1_end and coords1[0, didx1] == oidx1:\n                didx1 += 1\n\n        if len(data_out) == 0:\n            return np.empty((2, 0), dtype=np.intp), np.empty((0,), dtype=dtr)\n\n        return np.array(coords_out).T, np.array(data_out)\n\n    return _dot_coo_coo\n\n\n@_memoize_dtype\ndef _dot_coo_ndarray_type(dt1, dt2):\n    dtr = np.result_type(dt1, dt2)\n\n    @numba.jit(nopython=True, nogil=True)\n    def _dot_coo_ndarray(coords1, data1, array2, out_shape):  # pragma: no cover\n        """"""\n        Utility function taking in one `COO` and one ``ndarray`` and\n        calculating a ""sense"" of their dot product. Acually computes\n        ``s1 @ x2.T``.\n\n        Parameters\n        ----------\n        data1, coords1 : np.ndarray\n            The data and coordinates of ``s1``.\n\n        array2 : np.ndarray\n            The second input array ``x2``.\n\n        out_shape : Tuple[int]\n            The output shape.\n        """"""\n        out = np.zeros(out_shape, dtype=dtr)\n        didx1 = 0\n\n        while didx1 < len(data1):\n            oidx1 = coords1[0, didx1]\n            didx1_curr = didx1\n\n            for oidx2 in range(out_shape[1]):\n                didx1 = didx1_curr\n                while didx1 < len(data1) and coords1[0, didx1] == oidx1:\n                    out[oidx1, oidx2] += data1[didx1] * array2[oidx2, coords1[1, didx1]]\n                    didx1 += 1\n\n        return out\n\n    return _dot_coo_ndarray\n\n\n@_memoize_dtype\ndef _dot_coo_ndarray_type_sparse(dt1, dt2):\n    dtr = np.result_type(dt1, dt2)\n\n    @numba.jit(\n        nopython=True,\n        nogil=True,\n        locals={""data_curr"": numba.np.numpy_support.from_dtype(dtr)},\n    )\n    def _dot_coo_ndarray(coords1, data1, array2, out_shape):  # pragma: no cover\n        """"""\n        Utility function taking in one `COO` and one ``ndarray`` and\n        calculating a ""sense"" of their dot product. Acually computes\n        ``s1 @ x2.T``.\n\n        Parameters\n        ----------\n        data1, coords1 : np.ndarray\n            The data and coordinates of ``s1``.\n\n        array2 : np.ndarray\n            The second input array ``x2``.\n\n        out_shape : Tuple[int]\n            The output shape.\n        """"""\n\n        out_data = []\n        out_coords = []\n\n        # coords1.shape = (2, len(data1))\n        # coords1[0, :] = rows, sorted\n        # coords1[1, :] = columns\n\n        didx1 = 0\n        while didx1 < len(data1):\n            current_row = coords1[0, didx1]\n\n            cur_didx1 = didx1\n            oidx2 = 0\n            while oidx2 < out_shape[1]:\n                cur_didx1 = didx1\n                data_curr = 0\n                while cur_didx1 < len(data1) and coords1[0, cur_didx1] == current_row:\n                    data_curr += data1[cur_didx1] * array2[oidx2, coords1[1, cur_didx1]]\n                    cur_didx1 += 1\n                if data_curr != 0:\n                    out_data.append(data_curr)\n                    out_coords.append((current_row, oidx2))\n                oidx2 += 1\n            didx1 = cur_didx1\n\n        if len(out_data) == 0:\n            return np.empty((2, 0), dtype=np.intp), np.empty((0,), dtype=dtr)\n\n        return np.array(out_coords).T, np.array(out_data)\n\n    return _dot_coo_ndarray\n\n\n@_memoize_dtype\ndef _dot_ndarray_coo_type(dt1, dt2):\n    dtr = np.result_type(dt1, dt2)\n\n    @numba.jit(\n        nopython=True, nogil=True,\n    )\n    def _dot_ndarray_coo(array1, coords2, data2, out_shape):  # pragma: no cover\n        """"""\n        Utility function taking in two one ``ndarray`` and one ``COO`` and\n        calculating a ""sense"" of their dot product. Acually computes ``x1 @ s2.T``.\n\n        Parameters\n        ----------\n        array1 : np.ndarray\n            The input array ``x1``.\n\n        data2, coords2 : np.ndarray\n            The data and coordinates of ``s2``.\n\n        out_shape : Tuple[int]\n            The output shape.\n        """"""\n        out = np.zeros(out_shape, dtype=dtr)\n\n        for oidx1 in range(out_shape[0]):\n            for didx2 in range(len(data2)):\n                oidx2 = coords2[0, didx2]\n                out[oidx1, oidx2] += array1[oidx1, coords2[1, didx2]] * data2[didx2]\n\n        return out\n\n    return _dot_ndarray_coo\n\n\n@_memoize_dtype\ndef _dot_ndarray_coo_type_sparse(dt1, dt2):\n    dtr = np.result_type(dt1, dt2)\n\n    @numba.jit(\n        nopython=True,\n        nogil=True,\n        locals={""data_curr"": numba.np.numpy_support.from_dtype(dtr)},\n    )\n    def _dot_ndarray_coo(array1, coords2, data2, out_shape):  # pragma: no cover\n        """"""\n        Utility function taking in two one ``ndarray`` and one ``COO`` and\n        calculating a ""sense"" of their dot product. Acually computes ``x1 @ s2.T``.\n\n        Parameters\n        ----------\n        array1 : np.ndarray\n            The input array ``x1``.\n\n        data2, coords2 : np.ndarray\n            The data and coordinates of ``s2``.\n\n        out_shape : Tuple[int]\n            The output shape.\n        """"""\n        out_data = []\n        out_coords = []\n\n        # coords2.shape = (2, len(data2))\n        # coords2[0, :] = columns, sorted\n        # coords2[1, :] = rows\n\n        for oidx1 in range(out_shape[0]):\n            data_curr = 0\n            current_col = 0\n            for didx2 in range(len(data2)):\n                if coords2[0, didx2] != current_col:\n                    if data_curr != 0:\n                        out_data.append(data_curr)\n                        out_coords.append([oidx1, current_col])\n                        data_curr = 0\n                    current_col = coords2[0, didx2]\n\n                data_curr += array1[oidx1, coords2[1, didx2]] * data2[didx2]\n\n            if data_curr != 0:\n                out_data.append(data_curr)\n                out_coords.append([oidx1, current_col])\n\n        if len(out_data) == 0:\n            return np.empty((2, 0), dtype=np.intp), np.empty((0,), dtype=dtr)\n\n        return np.array(out_coords).T, np.array(out_data)\n\n    return _dot_ndarray_coo\n\n\ndef isposinf(x, out=None):\n    """"""\n    Test element-wise for positive infinity, return result as sparse ``bool`` array.\n\n    Parameters\n    ----------\n    x\n        Input\n    out, optional\n        Output array\n\n    Examples\n    --------\n    >>> import sparse\n    >>> x = sparse.as_coo(np.array([np.inf]))\n    >>> sparse.isposinf(x).todense()\n    array([ True])\n\n    See Also\n    --------\n    numpy.isposinf : The NumPy equivalent\n    """"""\n    from .core import elemwise\n\n    return elemwise(lambda x, out=None, dtype=None: np.isposinf(x, out=out), x, out=out)\n\n\ndef isneginf(x, out=None):\n    """"""\n    Test element-wise for negative infinity, return result as sparse ``bool`` array.\n\n    Parameters\n    ----------\n    x\n        Input\n    out, optional\n        Output array\n\n    Examples\n    --------\n    >>> import sparse\n    >>> x = sparse.as_coo(np.array([-np.inf]))\n    >>> sparse.isneginf(x).todense()\n    array([ True])\n\n    See Also\n    --------\n    numpy.isneginf : The NumPy equivalent\n    """"""\n    from .core import elemwise\n\n    return elemwise(lambda x, out=None, dtype=None: np.isneginf(x, out=out), x, out=out)\n\n\ndef result_type(*arrays_and_dtypes):\n    """"""Returns the type that results from applying the NumPy type promotion rules to the\n    arguments.\n\n    See Also\n    --------\n    numpy.result_type : The NumPy equivalent\n    """"""\n    return np.result_type(*(_as_result_type_arg(x) for x in arrays_and_dtypes))\n\n\ndef _as_result_type_arg(x):\n    if not isinstance(x, SparseArray):\n        return x\n    if x.ndim > 0:\n        return x.dtype\n    # 0-dimensional arrays give different result_type outputs than their dtypes\n    return x.todense()\n\n\n@numba.jit(nopython=True, nogil=True)\ndef _diagonal_idx(coordlist, axis1, axis2, offset):\n    """"""\n    Utility function that returns all indices that correspond to a diagonal element.\n\n    Parameters\n    ----------\n    coordlist : list of lists\n        Coordinate indices.\n\n    axis1, axis2 : int\n        The axes of the diagonal.\n\n    offset : int\n        Offset of the diagonal from the main diagonal. Defaults to main diagonal (0).\n    """"""\n    return np.array(\n        [\n            i\n            for i in range(len(coordlist[axis1]))\n            if coordlist[axis1][i] + offset == coordlist[axis2][i]\n        ]\n    )\n\n\ndef clip(a, a_min=None, a_max=None, out=None):\n    """"""\n    Clip (limit) the values in the array.\n\n    Return an array whose values are limited to ``[min, max]``. One of min\n    or max must be given.\n\n    Parameters\n    ----------\n    a: \n    a_min : scalar or `SparseArray` or `None`\n        Minimum value. If `None`, clipping is not performed on lower\n        interval edge.\n    a_max : scalar or `SparseArray` or `None`\n        Maximum value. If `None`, clipping is not performed on upper\n        interval edge.\n    out : SparseArray, optional\n        If provided, the results will be placed in this array. It may be\n        the input array for in-place clipping. `out` must be of the right\n        shape to hold the output. Its type is preserved.\n\n    Returns\n    -------\n    clipped_array : SparseArray\n        An array with the elements of `self`, but where values < `min` are\n        replaced with `min`, and those > `max` with `max`.\n\n    Examples\n    --------\n    >>> import sparse\n    >>> x = sparse.COO.from_numpy([0, 0, 0, 1, 2, 3])\n    >>> sparse.clip(x, a_min=1).todense()  # doctest: +NORMALIZE_WHITESPACE\n    array([1, 1, 1, 1, 2, 3])\n    >>> sparse.clip(x, a_max=1).todense()  # doctest: +NORMALIZE_WHITESPACE\n    array([0, 0, 0, 1, 1, 1])\n    >>> sparse.clip(x, a_min=1, a_max=2).todense() # doctest: +NORMALIZE_WHITESPACE\n    array([1, 1, 1, 1, 2, 2])\n\n    See also\n    --------\n    numpy.clip : Equivalent NumPy function\n    """"""\n    a = asCOO(a, name=""clip"")\n    return a.clip(a_min, a_max)\n'"
sparse/_coo/core.py,133,"b'import copy as _copy\nimport operator\nfrom collections.abc import Iterable, Iterator, Sized\nfrom collections import defaultdict, deque\nfrom functools import reduce\nimport warnings\n\nimport numpy as np\nimport scipy.sparse\nfrom numpy.lib.mixins import NDArrayOperatorsMixin\nimport numba\n\nfrom .common import dot, matmul\nfrom .indexing import getitem\nfrom .umath import elemwise, broadcast_to\nfrom .._sparse_array import SparseArray\nfrom .._utils import normalize_axis, equivalent, check_zero_fill_value, _zero_of_dtype\n\n\n_reduce_super_ufunc = {np.add: np.multiply, np.multiply: np.power}\n\n\nclass COO(SparseArray, NDArrayOperatorsMixin):  # lgtm [py/missing-equals]\n    """"""\n    A sparse multidimensional array.\n\n    This is stored in COO format.  It depends on NumPy and Scipy.sparse for\n    computation, but supports arrays of arbitrary dimension.\n\n    Parameters\n    ----------\n    coords : numpy.ndarray (COO.ndim, COO.nnz)\n        An array holding the index locations of every value\n        Should have shape (number of dimensions, number of non-zeros).\n    data : numpy.ndarray (COO.nnz,)\n        An array of Values. A scalar can also be supplied if the data is the same across\n        all coordinates. If not given, defers to :obj:`as_coo`.\n    shape : tuple[int] (COO.ndim,)\n        The shape of the array.\n    has_duplicates : bool, optional\n        A value indicating whether the supplied value for :code:`coords` has\n        duplicates. Note that setting this to `False` when :code:`coords` does have\n        duplicates may result in undefined behaviour. See :obj:`COO.sum_duplicates`\n    sorted : bool, optional\n        A value indicating whether the values in `coords` are sorted. Note\n        that setting this to `True` when :code:`coords` isn\'t sorted may\n        result in undefined behaviour. See :obj:`COO.sort_indices`.\n    prune : bool, optional\n        A flag indicating whether or not we should prune any fill-values present in\n        ``data``.\n    cache : bool, optional\n        Whether to enable cacheing for various operations. See\n        :obj:`COO.enable_caching`\n    fill_value: scalar, optional\n        The fill value for this array.\n\n    Attributes\n    ----------\n    coords : numpy.ndarray (ndim, nnz)\n        An array holding the coordinates of every nonzero element.\n    data : numpy.ndarray (nnz,)\n        An array holding the values corresponding to :obj:`COO.coords`.\n    shape : tuple[int] (ndim,)\n        The dimensions of this array.\n\n    See Also\n    --------\n    DOK : A mostly write-only sparse array.\n    as_coo : Convert any given format to :obj:`COO`.\n\n    Examples\n    --------\n    You can create :obj:`COO` objects from Numpy arrays.\n\n    >>> x = np.eye(4, dtype=np.uint8)\n    >>> x[2, 3] = 5\n    >>> s = COO.from_numpy(x)\n    >>> s\n    <COO: shape=(4, 4), dtype=uint8, nnz=5, fill_value=0>\n    >>> s.data  # doctest: +NORMALIZE_WHITESPACE\n    array([1, 1, 1, 5, 1], dtype=uint8)\n    >>> s.coords  # doctest: +NORMALIZE_WHITESPACE\n    array([[0, 1, 2, 2, 3],\n           [0, 1, 2, 3, 3]])\n\n    :obj:`COO` objects support basic arithmetic and binary operations.\n\n    >>> x2 = np.eye(4, dtype=np.uint8)\n    >>> x2[3, 2] = 5\n    >>> s2 = COO.from_numpy(x2)\n    >>> (s + s2).todense()  # doctest: +NORMALIZE_WHITESPACE\n    array([[2, 0, 0, 0],\n           [0, 2, 0, 0],\n           [0, 0, 2, 5],\n           [0, 0, 5, 2]], dtype=uint8)\n    >>> (s * s2).todense()  # doctest: +NORMALIZE_WHITESPACE\n    array([[1, 0, 0, 0],\n           [0, 1, 0, 0],\n           [0, 0, 1, 0],\n           [0, 0, 0, 1]], dtype=uint8)\n\n    Binary operations support broadcasting.\n\n    >>> x3 = np.zeros((4, 1), dtype=np.uint8)\n    >>> x3[2, 0] = 1\n    >>> s3 = COO.from_numpy(x3)\n    >>> (s * s3).todense()  # doctest: +NORMALIZE_WHITESPACE\n    array([[0, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 0, 1, 5],\n           [0, 0, 0, 0]], dtype=uint8)\n\n    :obj:`COO` objects also support dot products and reductions.\n\n    >>> s.dot(s.T).sum(axis=0).todense()   # doctest: +NORMALIZE_WHITESPACE\n    array([ 1,  1, 31,  6], dtype=uint64)\n\n    You can use Numpy :code:`ufunc` operations on :obj:`COO` arrays as well.\n\n    >>> np.sum(s, axis=1).todense()  # doctest: +NORMALIZE_WHITESPACE\n    array([1, 1, 6, 1], dtype=uint64)\n    >>> np.round(np.sqrt(s, dtype=np.float64), decimals=1).todense()   # doctest: +SKIP\n    array([[ 1. ,  0. ,  0. ,  0. ],\n           [ 0. ,  1. ,  0. ,  0. ],\n           [ 0. ,  0. ,  1. ,  2.2],\n           [ 0. ,  0. ,  0. ,  1. ]])\n\n    Operations that will result in a dense array will usually result in a different\n    fill value, such as the following.\n\n    >>> np.exp(s)\n    <COO: shape=(4, 4), dtype=float16, nnz=5, fill_value=1.0>\n\n    You can also create :obj:`COO` arrays from coordinates and data.\n\n    >>> coords = [[0, 0, 0, 1, 1],\n    ...           [0, 1, 2, 0, 3],\n    ...           [0, 3, 2, 0, 1]]\n    >>> data = [1, 2, 3, 4, 5]\n    >>> s4 = COO(coords, data, shape=(3, 4, 5))\n    >>> s4\n    <COO: shape=(3, 4, 5), dtype=int64, nnz=5, fill_value=0>\n\n    If the data is same across all coordinates, you can also specify a scalar.\n\n    >>> coords = [[0, 0, 0, 1, 1],\n    ...           [0, 1, 2, 0, 3],\n    ...           [0, 3, 2, 0, 1]]\n    >>> data = 1\n    >>> s5 = COO(coords, data, shape=(3, 4, 5))\n    >>> s5\n    <COO: shape=(3, 4, 5), dtype=int64, nnz=5, fill_value=0>\n\n    Following scipy.sparse conventions you can also pass these as a tuple with\n    rows and columns\n\n    >>> rows = [0, 1, 2, 3, 4]\n    >>> cols = [0, 0, 0, 1, 1]\n    >>> data = [10, 20, 30, 40, 50]\n    >>> z = COO((data, (rows, cols)))\n    >>> z.todense()  # doctest: +NORMALIZE_WHITESPACE\n    array([[10,  0],\n           [20,  0],\n           [30,  0],\n           [ 0, 40],\n           [ 0, 50]])\n\n    You can also pass a dictionary or iterable of index/value pairs. Repeated\n    indices imply summation:\n\n    >>> d = {(0, 0, 0): 1, (1, 2, 3): 2, (1, 1, 0): 3}\n    >>> COO(d)\n    <COO: shape=(2, 3, 4), dtype=int64, nnz=3, fill_value=0>\n    >>> L = [((0, 0), 1),\n    ...      ((1, 1), 2),\n    ...      ((0, 0), 3)]\n    >>> COO(L).todense()  # doctest: +NORMALIZE_WHITESPACE\n    array([[4, 0],\n           [0, 2]])\n\n    You can convert :obj:`DOK` arrays to :obj:`COO` arrays.\n\n    >>> from sparse import DOK\n    >>> s6 = DOK((5, 5), dtype=np.int64)\n    >>> s6[1:3, 1:3] = [[4, 5], [6, 7]]\n    >>> s6\n    <DOK: shape=(5, 5), dtype=int64, nnz=4, fill_value=0>\n    >>> s7 = s6.asformat(\'coo\')\n    >>> s7\n    <COO: shape=(5, 5), dtype=int64, nnz=4, fill_value=0>\n    >>> s7.todense()  # doctest: +NORMALIZE_WHITESPACE\n    array([[0, 0, 0, 0, 0],\n           [0, 4, 5, 0, 0],\n           [0, 6, 7, 0, 0],\n           [0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0]])\n    """"""\n\n    __array_priority__ = 12\n\n    def __init__(\n        self,\n        coords,\n        data=None,\n        shape=None,\n        has_duplicates=True,\n        sorted=False,\n        prune=False,\n        cache=False,\n        fill_value=None,\n    ):\n        self._cache = None\n        if cache:\n            self.enable_caching()\n\n        if data is None:\n            arr = as_coo(coords, shape=shape, fill_value=fill_value)\n            self._make_shallow_copy_of(arr)\n            return\n\n        self.data = np.asarray(data)\n        self.coords = np.asarray(coords)\n\n        if self.coords.ndim == 1:\n            if self.coords.size == 0 and shape is not None:\n                self.coords = self.coords.reshape((len(shape), len(data)))\n            else:\n                self.coords = self.coords[None, :]\n\n        if self.data.ndim == 0:\n            self.data = np.broadcast_to(self.data, self.coords.shape[1])\n\n        if shape and not self.coords.size:\n            self.coords = np.zeros(\n                (len(shape) if isinstance(shape, Iterable) else 1, 0), dtype=np.uint64\n            )\n\n        if shape is None:\n            if self.coords.nbytes:\n                shape = tuple((self.coords.max(axis=1) + 1))\n            else:\n                shape = ()\n\n        super().__init__(shape, fill_value=fill_value)\n        self.coords = self.coords.astype(np.intp, copy=False)\n\n        if self.shape:\n            if len(self.data) != self.coords.shape[1]:\n                msg = (\n                    ""The data length does not match the coordinates ""\n                    ""given.\\nlen(data) = {}, but {} coords specified.""\n                )\n                raise ValueError(msg.format(len(data), self.coords.shape[1]))\n            if len(self.shape) != self.coords.shape[0]:\n                msg = (\n                    ""Shape specified by `shape` doesn\'t match the ""\n                    ""shape of `coords`; len(shape)={} != coords.shape[0]={}""\n                    ""(and coords.shape={})""\n                )\n                raise ValueError(\n                    msg.format(len(shape), self.coords.shape[0], self.coords.shape)\n                )\n\n        from .._settings import WARN_ON_TOO_DENSE\n\n        if WARN_ON_TOO_DENSE and self.nbytes >= self.size * self.data.itemsize:\n            warnings.warn(\n                ""Attempting to create a sparse array that takes no less ""\n                ""memory than than an equivalent dense array. You may want to ""\n                ""use a dense array here instead."",\n                RuntimeWarning,\n            )\n\n        if not sorted:\n            self._sort_indices()\n\n        if has_duplicates:\n            self._sum_duplicates()\n\n        if prune:\n            self._prune()\n\n    def __getstate__(self):\n        return (self.coords, self.data, self.shape, self.fill_value)\n\n    def __setstate__(self, state):\n        self.coords, self.data, self.shape, self.fill_value = state\n        self._cache = None\n\n    def __dask_tokenize__(self):\n        ""Produce a deterministic, content-based hash for dask.""\n        from dask.base import normalize_token\n\n        return normalize_token(\n            (type(self), self.coords, self.data, self.shape, self.fill_value)\n        )\n\n    def copy(self, deep=True):\n        """"""Return a copy of the array.\n\n        Parameters\n        ----------\n        deep : boolean, optional\n            If True (default), the internal coords and data arrays are also\n            copied. Set to ``False`` to only make a shallow copy.\n        """"""\n        return _copy.deepcopy(self) if deep else _copy.copy(self)\n\n    def _make_shallow_copy_of(self, other):\n        self.coords = other.coords\n        self.data = other.data\n        super().__init__(other.shape, fill_value=other.fill_value)\n\n    def enable_caching(self):\n        """""" Enable caching of reshape, transpose, and tocsr/csc operations\n\n        This enables efficient iterative workflows that make heavy use of\n        csr/csc operations, such as tensordot.  This maintains a cache of\n        recent results of reshape and transpose so that operations like\n        tensordot (which uses both internally) store efficiently stored\n        representations for repeated use.  This can significantly cut down on\n        computational costs in common numeric algorithms.\n\n        However, this also assumes that neither this object, nor the downstream\n        objects will have their data mutated.\n\n        Examples\n        --------\n        >>> s.enable_caching()  # doctest: +SKIP\n        >>> csr1 = s.transpose((2, 0, 1)).reshape((100, 120)).tocsr()  # doctest: +SKIP\n        >>> csr2 = s.transpose((2, 0, 1)).reshape((100, 120)).tocsr()  # doctest: +SKIP\n        >>> csr1 is csr2  # doctest: +SKIP\n        True\n        """"""\n        self._cache = defaultdict(lambda: deque(maxlen=3))\n\n    @classmethod\n    def from_numpy(cls, x, fill_value=None):\n        """"""\n        Convert the given :obj:`numpy.ndarray` to a :obj:`COO` object.\n\n        Parameters\n        ----------\n        x : np.ndarray\n            The dense array to convert.\n        fill_value : scalar\n            The fill value of the constructed :obj:`COO` array. Zero if\n            unspecified.\n\n        Returns\n        -------\n        COO\n            The converted COO array.\n\n        Examples\n        --------\n        >>> x = np.eye(5)\n        >>> s = COO.from_numpy(x)\n        >>> s\n        <COO: shape=(5, 5), dtype=float64, nnz=5, fill_value=0.0>\n\n        >>> x[x == 0] = np.nan\n        >>> COO.from_numpy(x, fill_value=np.nan)\n        <COO: shape=(5, 5), dtype=float64, nnz=5, fill_value=nan>\n        """"""\n        x = np.asanyarray(x).view(type=np.ndarray)\n\n        if fill_value is None:\n            fill_value = _zero_of_dtype(x.dtype)\n\n        if x.shape:\n            coords = np.where(~equivalent(x, fill_value))\n            data = x[coords]\n            coords = np.vstack(coords)\n        else:\n            coords = np.empty((0, 1), dtype=np.uint8)\n            data = np.array(x, ndmin=1)\n        return cls(\n            coords,\n            data,\n            shape=x.shape,\n            has_duplicates=False,\n            sorted=True,\n            fill_value=fill_value,\n        )\n\n    def todense(self):\n        """"""\n        Convert this :obj:`COO` array to a dense :obj:`numpy.ndarray`. Note that\n        this may take a large amount of memory if the :obj:`COO` object\'s :code:`shape`\n        is large.\n\n        Returns\n        -------\n        numpy.ndarray\n            The converted dense array.\n\n        See Also\n        --------\n        DOK.todense : Equivalent :obj:`DOK` array method.\n        scipy.sparse.coo_matrix.todense : Equivalent Scipy method.\n\n        Examples\n        --------\n        >>> x = np.random.randint(100, size=(7, 3))\n        >>> s = COO.from_numpy(x)\n        >>> x2 = s.todense()\n        >>> np.array_equal(x, x2)\n        True\n        """"""\n        x = np.full(self.shape, self.fill_value, self.dtype)\n\n        coords = tuple([self.coords[i, :] for i in range(self.ndim)])\n        data = self.data\n\n        if coords != ():\n            x[coords] = data\n        else:\n            if len(data) != 0:\n                x[coords] = data\n\n        return x\n\n    @classmethod\n    def from_scipy_sparse(cls, x):\n        """"""\n        Construct a :obj:`COO` array from a :obj:`scipy.sparse.spmatrix`\n\n        Parameters\n        ----------\n        x : scipy.sparse.spmatrix\n            The sparse matrix to construct the array from.\n\n        Returns\n        -------\n        COO\n            The converted :obj:`COO` object.\n\n        Examples\n        --------\n        >>> x = scipy.sparse.rand(6, 3, density=0.2)\n        >>> s = COO.from_scipy_sparse(x)\n        >>> np.array_equal(x.todense(), s.todense())\n        True\n        """"""\n        x = x.asformat(""coo"")\n        coords = np.empty((2, x.nnz), dtype=x.row.dtype)\n        coords[0, :] = x.row\n        coords[1, :] = x.col\n        return COO(\n            coords,\n            x.data,\n            shape=x.shape,\n            has_duplicates=not x.has_canonical_format,\n            sorted=x.has_canonical_format,\n        )\n\n    @classmethod\n    def from_iter(cls, x, shape=None, fill_value=None, dtype=None):\n        """"""\n        Converts an iterable in certain formats to a :obj:`COO` array. See examples\n        for details.\n\n        Parameters\n        ----------\n        x : Iterable or Iterator\n            The iterable to convert to :obj:`COO`.\n        shape : tuple[int], optional\n            The shape of the array.\n        fill_value : scalar\n            The fill value for this array.\n        dtype : numpy.dtype\n            The dtype of the input array. Inferred from the input if not given.\n\n        Returns\n        -------\n        out : COO\n            The output :obj:`COO` array.\n\n        Examples\n        --------\n        You can convert items of the format ``[((i, j, k), value), ((i, j, k), value)]`` to :obj:`COO`.\n        Here, the first part represents the coordinate and the second part represents the value.\n\n        >>> x = [((0, 0), 1), ((1, 1), 1)]\n        >>> s = COO.from_iter(x)\n        >>> s.todense()\n        array([[1, 0],\n               [0, 1]])\n\n        You can also have a similar format with a dictionary.\n\n        >>> x = {(0, 0): 1, (1, 1): 1}\n        >>> s = COO.from_iter(x)\n        >>> s.todense()\n        array([[1, 0],\n               [0, 1]])\n\n        The third supported format is ``(data, (..., row, col))``.\n\n        >>> x = ([1, 1], ([0, 1], [0, 1]))\n        >>> s = COO.from_iter(x)\n        >>> s.todense()\n        array([[1, 0],\n               [0, 1]])\n\n        You can also pass in a :obj:`collections.Iterator` object.\n\n        >>> x = [((0, 0), 1), ((1, 1), 1)].__iter__()\n        >>> s = COO.from_iter(x)\n        >>> s.todense()\n        array([[1, 0],\n               [0, 1]])\n        """"""\n        if isinstance(x, dict):\n            x = list(x.items())\n\n        if not isinstance(x, Sized):\n            x = list(x)\n\n        if len(x) != 2 and not all(len(item) == 2 for item in x):\n            raise ValueError(""Invalid iterable to convert to COO."")\n\n        if not x:\n            ndim = 0 if shape is None else len(shape)\n            coords = np.empty((ndim, 0), dtype=np.uint8)\n            data = np.empty((0,), dtype=dtype)\n            shape = () if shape is None else shape\n\n        elif not isinstance(x[0][0], Iterable):\n            coords = np.stack(x[1], axis=0)\n            data = np.asarray(x[0], dtype=dtype)\n        else:\n            coords = np.array([item[0] for item in x]).T\n            data = np.array([item[1] for item in x], dtype=dtype)\n\n        if not (\n            coords.ndim == 2\n            and data.ndim == 1\n            and np.issubdtype(coords.dtype, np.integer)\n            and np.all(coords >= 0)\n        ):\n            raise ValueError(""Invalid iterable to convert to COO."")\n\n        return COO(coords, data, shape=shape, fill_value=fill_value)\n\n    @property\n    def dtype(self):\n        """"""\n        The datatype of this array.\n\n        Returns\n        -------\n        numpy.dtype\n            The datatype of this array.\n\n        See Also\n        --------\n        numpy.ndarray.dtype : Numpy equivalent property.\n        scipy.sparse.coo_matrix.dtype : Scipy equivalent property.\n\n        Examples\n        --------\n        >>> x = (200 * np.random.rand(5, 4)).astype(np.int32)\n        >>> s = COO.from_numpy(x)\n        >>> s.dtype\n        dtype(\'int32\')\n        >>> x.dtype == s.dtype\n        True\n        """"""\n        return self.data.dtype\n\n    @property\n    def nnz(self):\n        """"""\n        The number of nonzero elements in this array. Note that any duplicates in\n        :code:`coords` are counted multiple times. To avoid this, call :obj:`COO.sum_duplicates`.\n\n        Returns\n        -------\n        int\n            The number of nonzero elements in this array.\n\n        See Also\n        --------\n        DOK.nnz : Equivalent :obj:`DOK` array property.\n        numpy.count_nonzero : A similar Numpy function.\n        scipy.sparse.coo_matrix.nnz : The Scipy equivalent property.\n\n        Examples\n        --------\n        >>> x = np.array([0, 0, 1, 0, 1, 2, 0, 1, 2, 3, 0, 0])\n        >>> np.count_nonzero(x)\n        6\n        >>> s = COO.from_numpy(x)\n        >>> s.nnz\n        6\n        >>> np.count_nonzero(x) == s.nnz\n        True\n        """"""\n        return self.coords.shape[1]\n\n    @property\n    def nbytes(self):\n        """"""\n        The number of bytes taken up by this object. Note that for small arrays,\n        this may undercount the number of bytes due to the large constant overhead.\n\n        Returns\n        -------\n        int\n            The approximate bytes of memory taken by this object.\n\n        See Also\n        --------\n        numpy.ndarray.nbytes : The equivalent Numpy property.\n\n        Examples\n        --------\n        >>> data = np.arange(6, dtype=np.uint8)\n        >>> coords = np.random.randint(1000, size=(3, 6), dtype=np.uint16)\n        >>> s = COO(coords, data, shape=(1000, 1000, 1000))\n        >>> s.nbytes\n        150\n        """"""\n        return self.data.nbytes + self.coords.nbytes\n\n    def __len__(self):\n        """"""\n        Get ""length"" of array, which is by definition the size of the first\n        dimension.\n\n        Returns\n        -------\n        int\n            The size of the first dimension.\n\n        See Also\n        --------\n        numpy.ndarray.__len__ : Numpy equivalent property.\n\n        Examples\n        --------\n        >>> x = np.zeros((10, 10))\n        >>> s = COO.from_numpy(x)\n        >>> len(s)\n        10\n        """"""\n        return self.shape[0]\n\n    def __sizeof__(self):\n        return self.nbytes\n\n    __getitem__ = getitem\n\n    def __str__(self):\n        return ""<COO: shape={!s}, dtype={!s}, nnz={:d}, fill_value={!s}>"".format(\n            self.shape, self.dtype, self.nnz, self.fill_value\n        )\n\n    __repr__ = __str__\n\n    @staticmethod\n    def _reduce(method, *args, **kwargs):\n        assert len(args) == 1\n\n        self = args[0]\n        if isinstance(self, scipy.sparse.spmatrix):\n            self = COO.from_scipy_sparse(self)\n\n        return self.reduce(method, **kwargs)\n\n    def reduce(self, method, axis=(0,), keepdims=False, **kwargs):\n        """"""\n        Performs a reduction operation on this array.\n\n        Parameters\n        ----------\n        method : numpy.ufunc\n            The method to use for performing the reduction.\n        axis : Union[int, Iterable[int]], optional\n            The axes along which to perform the reduction. Uses all axes by default.\n        keepdims : bool, optional\n            Whether or not to keep the dimensions of the original array.\n        kwargs : dict\n            Any extra arguments to pass to the reduction operation.\n\n        Returns\n        -------\n        COO\n            The result of the reduction operation.\n\n        Raises\n        ------\n        ValueError\n            If reducing an all-zero axis would produce a nonzero result.\n\n        Notes\n        -----\n        This function internally calls :obj:`COO.sum_duplicates` to bring the array into\n        canonical form.\n\n        See Also\n        --------\n        numpy.ufunc.reduce : A similar Numpy method.\n        COO.nanreduce : Similar method with ``NaN`` skipping functionality.\n\n        Examples\n        --------\n        You can use the :obj:`COO.reduce` method to apply a reduction operation to\n        any Numpy :code:`ufunc`.\n\n        >>> x = np.ones((5, 5), dtype=np.int)\n        >>> s = COO.from_numpy(x)\n        >>> s2 = s.reduce(np.add, axis=1)\n        >>> s2.todense()  # doctest: +NORMALIZE_WHITESPACE\n        array([5, 5, 5, 5, 5])\n\n        You can also use the :code:`keepdims` argument to keep the dimensions after the\n        reduction.\n\n        >>> s3 = s.reduce(np.add, axis=1, keepdims=True)\n        >>> s3.shape\n        (5, 1)\n\n        You can also pass in any keyword argument that :obj:`numpy.ufunc.reduce` supports.\n        For example, :code:`dtype`. Note that :code:`out` isn\'t supported.\n\n        >>> s4 = s.reduce(np.add, axis=1, dtype=np.float16)\n        >>> s4.dtype\n        dtype(\'float16\')\n\n        By default, this reduces the array by only the first axis.\n\n        >>> s.reduce(np.add)\n        <COO: shape=(5,), dtype=int64, nnz=5, fill_value=0>\n        """"""\n        axis = normalize_axis(axis, self.ndim)\n        zero_reduce_result = method.reduce([self.fill_value, self.fill_value], **kwargs)\n        reduce_super_ufunc = None\n\n        if not equivalent(zero_reduce_result, self.fill_value):\n            reduce_super_ufunc = _reduce_super_ufunc.get(method, None)\n\n            if reduce_super_ufunc is None:\n                raise ValueError(\n                    ""Performing this reduction operation would produce ""\n                    ""a dense result: %s"" % str(method)\n                )\n\n        if axis is None:\n            axis = tuple(range(self.ndim))\n\n        if not isinstance(axis, tuple):\n            axis = (axis,)\n\n        axis = tuple(a if a >= 0 else a + self.ndim for a in axis)\n\n        neg_axis = tuple(ax for ax in range(self.ndim) if ax not in set(axis))\n\n        a = self.transpose(neg_axis + axis)\n        a = a.reshape(\n            (\n                np.prod([self.shape[d] for d in neg_axis], dtype=np.intp),\n                np.prod([self.shape[d] for d in axis], dtype=np.intp),\n            )\n        )\n\n        result, inv_idx, counts = _grouped_reduce(a.data, a.coords[0], method, **kwargs)\n\n        result_fill_value = self.fill_value\n\n        if reduce_super_ufunc is None:\n            missing_counts = counts != a.shape[1]\n            result[missing_counts] = method(\n                result[missing_counts], self.fill_value, **kwargs\n            )\n        else:\n            result = method(\n                result, reduce_super_ufunc(self.fill_value, a.shape[1] - counts)\n            ).astype(result.dtype)\n            result_fill_value = reduce_super_ufunc(self.fill_value, a.shape[1])\n        coords = a.coords[0:1, inv_idx]\n\n        a = COO(\n            coords,\n            result,\n            shape=(a.shape[0],),\n            has_duplicates=False,\n            sorted=True,\n            prune=True,\n            fill_value=result_fill_value,\n        )\n\n        a = a.reshape(tuple(self.shape[d] for d in neg_axis))\n        result = a\n\n        if keepdims:\n            result = _keepdims(self, result, axis)\n\n        if result.ndim == 0:\n            return result[()]\n\n        return result\n\n    def sum(self, axis=None, keepdims=False, dtype=None, out=None):\n        """"""\n        Performs a sum operation along the given axes. Uses all axes by default.\n\n        Parameters\n        ----------\n        axis : Union[int, Iterable[int]], optional\n            The axes along which to sum. Uses all axes by default.\n        keepdims : bool, optional\n            Whether or not to keep the dimensions of the original array.\n        dtype: numpy.dtype\n            The data type of the output array.\n\n        Returns\n        -------\n        COO\n            The reduced output sparse array.\n\n        See Also\n        --------\n        :obj:`numpy.sum` : Equivalent numpy function.\n        scipy.sparse.coo_matrix.sum : Equivalent Scipy function.\n        :obj:`nansum` : Function with ``NaN`` skipping.\n\n        Notes\n        -----\n        * This function internally calls :obj:`COO.sum_duplicates` to bring the array into\n          canonical form.\n\n        Examples\n        --------\n        You can use :obj:`COO.sum` to sum an array across any dimension.\n\n        >>> x = np.ones((5, 5), dtype=np.int)\n        >>> s = COO.from_numpy(x)\n        >>> s2 = s.sum(axis=1)\n        >>> s2.todense()  # doctest: +NORMALIZE_WHITESPACE\n        array([5, 5, 5, 5, 5])\n\n        You can also use the :code:`keepdims` argument to keep the dimensions after the\n        sum.\n\n        >>> s3 = s.sum(axis=1, keepdims=True)\n        >>> s3.shape\n        (5, 1)\n\n        You can pass in an output datatype, if needed.\n\n        >>> s4 = s.sum(axis=1, dtype=np.float16)\n        >>> s4.dtype\n        dtype(\'float16\')\n\n        By default, this reduces the array down to one number, summing along all axes.\n\n        >>> s.sum()\n        25\n        """"""\n        return np.add.reduce(self, out=out, axis=axis, keepdims=keepdims, dtype=dtype)\n\n    def max(self, axis=None, keepdims=False, out=None):\n        """"""\n        Maximize along the given axes. Uses all axes by default.\n\n        Parameters\n        ----------\n        axis : Union[int, Iterable[int]], optional\n            The axes along which to maximize. Uses all axes by default.\n        keepdims : bool, optional\n            Whether or not to keep the dimensions of the original array.\n        dtype: numpy.dtype\n            The data type of the output array.\n\n        Returns\n        -------\n        COO\n            The reduced output sparse array.\n\n        See Also\n        --------\n        :obj:`numpy.max` : Equivalent numpy function.\n        scipy.sparse.coo_matrix.max : Equivalent Scipy function.\n        :obj:`nanmax` : Function with ``NaN`` skipping.\n\n        Notes\n        -----\n        * This function internally calls :obj:`COO.sum_duplicates` to bring the array into\n          canonical form.\n\n        Examples\n        --------\n        You can use :obj:`COO.max` to maximize an array across any dimension.\n\n        >>> x = np.add.outer(np.arange(5), np.arange(5))\n        >>> x  # doctest: +NORMALIZE_WHITESPACE\n        array([[0, 1, 2, 3, 4],\n               [1, 2, 3, 4, 5],\n               [2, 3, 4, 5, 6],\n               [3, 4, 5, 6, 7],\n               [4, 5, 6, 7, 8]])\n        >>> s = COO.from_numpy(x)\n        >>> s2 = s.max(axis=1)\n        >>> s2.todense()  # doctest: +NORMALIZE_WHITESPACE\n        array([4, 5, 6, 7, 8])\n\n        You can also use the :code:`keepdims` argument to keep the dimensions after the\n        maximization.\n\n        >>> s3 = s.max(axis=1, keepdims=True)\n        >>> s3.shape\n        (5, 1)\n\n        By default, this reduces the array down to one number, maximizing along all axes.\n\n        >>> s.max()\n        8\n        """"""\n        return np.maximum.reduce(self, out=out, axis=axis, keepdims=keepdims)\n\n    amax = max\n\n    def any(self, axis=None, keepdims=False, out=None):\n        """"""\n        See if any values along array are ``True``. Uses all axes by default.\n\n        Parameters\n        ----------\n        axis : Union[int, Iterable[int]], optional\n            The axes along which to minimize. Uses all axes by default.\n        keepdims : bool, optional\n            Whether or not to keep the dimensions of the original array.\n\n        Returns\n        -------\n        COO\n            The reduced output sparse array.\n\n        See Also\n        --------\n        :obj:`numpy.all` : Equivalent numpy function.\n\n        Notes\n        -----\n        * This function internally calls :obj:`COO.sum_duplicates` to bring the array into\n          canonical form.\n\n        Examples\n        --------\n        You can use :obj:`COO.min` to minimize an array across any dimension.\n\n        >>> x = np.array([[False, False],\n        ...               [False, True ],\n        ...               [True,  False],\n        ...               [True,  True ]])\n        >>> s = COO.from_numpy(x)\n        >>> s2 = s.any(axis=1)\n        >>> s2.todense()  # doctest: +SKIP\n        array([False,  True,  True,  True])\n\n        You can also use the :code:`keepdims` argument to keep the dimensions after the\n        minimization.\n\n        >>> s3 = s.any(axis=1, keepdims=True)\n        >>> s3.shape\n        (4, 1)\n\n        By default, this reduces the array down to one number, minimizing along all axes.\n\n        >>> s.any()\n        True\n        """"""\n        return np.logical_or.reduce(self, out=out, axis=axis, keepdims=keepdims)\n\n    def all(self, axis=None, keepdims=False, out=None):\n        """"""\n        See if all values in an array are ``True``. Uses all axes by default.\n\n        Parameters\n        ----------\n        axis : Union[int, Iterable[int]], optional\n            The axes along which to minimize. Uses all axes by default.\n        keepdims : bool, optional\n            Whether or not to keep the dimensions of the original array.\n\n        Returns\n        -------\n        COO\n            The reduced output sparse array.\n\n        See Also\n        --------\n        :obj:`numpy.all` : Equivalent numpy function.\n\n        Notes\n        -----\n        * This function internally calls :obj:`COO.sum_duplicates` to bring the array into\n          canonical form.\n\n        Examples\n        --------\n        You can use :obj:`COO.min` to minimize an array across any dimension.\n\n        >>> x = np.array([[False, False],\n        ...               [False, True ],\n        ...               [True,  False],\n        ...               [True,  True ]])\n        >>> s = COO.from_numpy(x)\n        >>> s2 = s.all(axis=1)\n        >>> s2.todense()  # doctest: +SKIP\n        array([False, False, False,  True])\n\n        You can also use the :code:`keepdims` argument to keep the dimensions after the\n        minimization.\n\n        >>> s3 = s.all(axis=1, keepdims=True)\n        >>> s3.shape\n        (4, 1)\n\n        By default, this reduces the array down to one boolean, minimizing along all axes.\n\n        >>> s.all()\n        False\n        """"""\n        return np.logical_and.reduce(self, out=out, axis=axis, keepdims=keepdims)\n\n    def min(self, axis=None, keepdims=False, out=None):\n        """"""\n        Minimize along the given axes. Uses all axes by default.\n\n        Parameters\n        ----------\n        axis : Union[int, Iterable[int]], optional\n            The axes along which to minimize. Uses all axes by default.\n        keepdims : bool, optional\n            Whether or not to keep the dimensions of the original array.\n        dtype: numpy.dtype\n            The data type of the output array.\n\n        Returns\n        -------\n        COO\n            The reduced output sparse array.\n\n        See Also\n        --------\n        :obj:`numpy.min` : Equivalent numpy function.\n        scipy.sparse.coo_matrix.min : Equivalent Scipy function.\n        :obj:`nanmin` : Function with ``NaN`` skipping.\n\n        Notes\n        -----\n        * This function internally calls :obj:`COO.sum_duplicates` to bring the array into\n          canonical form.\n\n        Examples\n        --------\n        You can use :obj:`COO.min` to minimize an array across any dimension.\n\n        >>> x = np.add.outer(np.arange(5), np.arange(5))\n        >>> x  # doctest: +NORMALIZE_WHITESPACE\n        array([[0, 1, 2, 3, 4],\n               [1, 2, 3, 4, 5],\n               [2, 3, 4, 5, 6],\n               [3, 4, 5, 6, 7],\n               [4, 5, 6, 7, 8]])\n        >>> s = COO.from_numpy(x)\n        >>> s2 = s.min(axis=1)\n        >>> s2.todense()  # doctest: +NORMALIZE_WHITESPACE\n        array([0, 1, 2, 3, 4])\n\n        You can also use the :code:`keepdims` argument to keep the dimensions after the\n        minimization.\n\n        >>> s3 = s.min(axis=1, keepdims=True)\n        >>> s3.shape\n        (5, 1)\n\n        By default, this reduces the array down to one boolean, minimizing along all axes.\n\n        >>> s.min()\n        0\n        """"""\n        return np.minimum.reduce(self, out=out, axis=axis, keepdims=keepdims)\n\n    amin = min\n\n    def prod(self, axis=None, keepdims=False, dtype=None, out=None):\n        """"""\n        Performs a product operation along the given axes. Uses all axes by default.\n\n        Parameters\n        ----------\n        axis : Union[int, Iterable[int]], optional\n            The axes along which to multiply. Uses all axes by default.\n        keepdims : bool, optional\n            Whether or not to keep the dimensions of the original array.\n        dtype: numpy.dtype\n            The data type of the output array.\n\n        Returns\n        -------\n        COO\n            The reduced output sparse array.\n\n        See Also\n        --------\n        :obj:`numpy.prod` : Equivalent numpy function.\n        :obj:`nanprod` : Function with ``NaN`` skipping.\n\n        Notes\n        -----\n        * This function internally calls :obj:`COO.sum_duplicates` to bring the array into\n          canonical form.\n\n        Examples\n        --------\n        You can use :obj:`COO.prod` to multiply an array across any dimension.\n\n        >>> x = np.add.outer(np.arange(5), np.arange(5))\n        >>> x  # doctest: +NORMALIZE_WHITESPACE\n        array([[0, 1, 2, 3, 4],\n               [1, 2, 3, 4, 5],\n               [2, 3, 4, 5, 6],\n               [3, 4, 5, 6, 7],\n               [4, 5, 6, 7, 8]])\n        >>> s = COO.from_numpy(x)\n        >>> s2 = s.prod(axis=1)\n        >>> s2.todense()  # doctest: +NORMALIZE_WHITESPACE\n        array([   0,  120,  720, 2520, 6720])\n\n        You can also use the :code:`keepdims` argument to keep the dimensions after the\n        reduction.\n\n        >>> s3 = s.prod(axis=1, keepdims=True)\n        >>> s3.shape\n        (5, 1)\n\n        You can pass in an output datatype, if needed.\n\n        >>> s4 = s.prod(axis=1, dtype=np.float16)\n        >>> s4.dtype\n        dtype(\'float16\')\n\n        By default, this reduces the array down to one number, multiplying along all axes.\n\n        >>> s.prod()\n        0\n        """"""\n        return np.multiply.reduce(\n            self, out=out, axis=axis, keepdims=keepdims, dtype=dtype\n        )\n\n    def mean(self, axis=None, keepdims=False, dtype=None, out=None):\n        """"""\n        Compute the mean along the given axes. Uses all axes by default.\n\n        Parameters\n        ----------\n        axis : Union[int, Iterable[int]], optional\n            The axes along which to compute the mean. Uses all axes by default.\n        keepdims : bool, optional\n            Whether or not to keep the dimensions of the original array.\n        dtype: numpy.dtype\n            The data type of the output array.\n\n        Returns\n        -------\n        COO\n            The reduced output sparse array.\n\n        See Also\n        --------\n        numpy.ndarray.mean : Equivalent numpy method.\n        scipy.sparse.coo_matrix.mean : Equivalent Scipy method.\n\n        Notes\n        -----\n        * This function internally calls :obj:`COO.sum_duplicates` to bring the\n          array into canonical form.\n        * The :code:`out` parameter is provided just for compatibility with\n          Numpy and isn\'t actually supported.\n\n        Examples\n        --------\n        You can use :obj:`COO.mean` to compute the mean of an array across any\n        dimension.\n\n        >>> x = np.array([[1, 2, 0, 0],\n        ...               [0, 1, 0, 0]], dtype=\'i8\')\n        >>> s = COO.from_numpy(x)\n        >>> s2 = s.mean(axis=1)\n        >>> s2.todense()  # doctest: +SKIP\n        array([0.5, 1.5, 0., 0.])\n\n        You can also use the :code:`keepdims` argument to keep the dimensions\n        after the mean.\n\n        >>> s3 = s.mean(axis=0, keepdims=True)\n        >>> s3.shape\n        (1, 4)\n\n        You can pass in an output datatype, if needed.\n\n        >>> s4 = s.mean(axis=0, dtype=np.float16)\n        >>> s4.dtype\n        dtype(\'float16\')\n\n        By default, this reduces the array down to one number, computing the\n        mean along all axes.\n\n        >>> s.mean()\n        0.5\n        """"""\n        if axis is None:\n            axis = tuple(range(self.ndim))\n        elif not isinstance(axis, tuple):\n            axis = (axis,)\n        den = reduce(operator.mul, (self.shape[i] for i in axis), 1)\n\n        if dtype is None:\n            if issubclass(self.dtype.type, (np.integer, np.bool_)):\n                dtype = inter_dtype = np.dtype(""f8"")\n            else:\n                dtype = self.dtype\n                inter_dtype = (\n                    np.dtype(""f4"") if issubclass(dtype.type, np.float16) else dtype\n                )\n        else:\n            inter_dtype = dtype\n\n        num = self.sum(axis=axis, keepdims=keepdims, dtype=inter_dtype)\n\n        if num.ndim:\n            out = np.true_divide(num, den, casting=""unsafe"")\n            return out.astype(dtype) if out.dtype != dtype else out\n        return np.divide(num, den, dtype=dtype, out=out)\n\n    def var(self, axis=None, dtype=None, out=None, ddof=0, keepdims=False):\n        """"""\n        Compute the variance along the gi66ven axes. Uses all axes by default.\n\n        Parameters\n        ----------\n        axis : Union[int, Iterable[int]], optional\n            The axes along which to compute the variance. Uses all axes by default.\n        dtype : numpy.dtype, optional\n            The output datatype.\n        out: COO, optional\n            The array to write the output to.\n        ddof: int\n            The degrees of freedom.\n        keepdims : bool, optional\n            Whether or not to keep the dimensions of the original array.\n\n        Returns\n        -------\n        COO\n            The reduced output sparse array.\n\n        See Also\n        --------\n        numpy.ndarray.var : Equivalent numpy method.\n\n        Notes\n        -----\n        * This function internally calls :obj:`COO.sum_duplicates` to bring the\n          array into canonical form.\n\n        Examples\n        --------\n        You can use :obj:`COO.var` to compute the variance of an array across any\n        dimension.\n\n        >>> x = np.array([[1, 2, 0, 0],\n        ...               [0, 1, 0, 0]], dtype=\'i8\')\n        >>> s = COO.from_numpy(x)\n        >>> s2 = s.var(axis=1)\n        >>> s2.todense()  # doctest: +SKIP\n        array([0.6875, 0.1875])\n\n        You can also use the :code:`keepdims` argument to keep the dimensions\n        after the variance.\n\n        >>> s3 = s.var(axis=0, keepdims=True)\n        >>> s3.shape\n        (1, 4)\n\n        You can pass in an output datatype, if needed.\n\n        >>> s4 = s.var(axis=0, dtype=np.float16)\n        >>> s4.dtype\n        dtype(\'float16\')\n\n        By default, this reduces the array down to one number, computing the\n        variance along all axes.\n\n        >>> s.var()\n        0.5\n        """"""\n        axis = normalize_axis(axis, self.ndim)\n\n        if axis is None:\n            axis = tuple(range(self.ndim))\n\n        if not isinstance(axis, tuple):\n            axis = (axis,)\n\n        rcount = reduce(operator.mul, (self.shape[a] for a in axis), 1)\n        # Make this warning show up on top.\n        if ddof >= rcount:\n            warnings.warn(""Degrees of freedom <= 0 for slice"", RuntimeWarning)\n\n        # Cast bool, unsigned int, and int to float64 by default\n        if dtype is None and issubclass(self.dtype.type, (np.integer, np.bool_)):\n            dtype = np.dtype(""f8"")\n\n        arrmean = self.sum(axis, dtype=dtype, keepdims=True)\n        np.divide(arrmean, rcount, out=arrmean)\n        x = self - arrmean\n        if issubclass(self.dtype.type, np.complexfloating):\n            x = x.real * x.real + x.imag * x.imag\n        else:\n            x = np.multiply(x, x, out=x)\n\n        ret = x.sum(axis=axis, dtype=dtype, out=out, keepdims=keepdims)\n\n        # Compute degrees of freedom and make sure it is not negative.\n        rcount = max([rcount - ddof, 0])\n\n        ret = ret[...]\n        np.divide(ret, rcount, out=ret, casting=""unsafe"")\n        return ret[()]\n\n    def std(self, axis=None, dtype=None, out=None, ddof=0, keepdims=False):\n        """"""\n        Compute the standard deviation along the given axes. Uses all axes by default.\n\n        Parameters\n        ----------\n        axis : Union[int, Iterable[int]], optional\n            The axes along which to compute the standard deviation. Uses\n            all axes by default.\n        dtype : numpy.dtype, optional\n            The output datatype.\n        out: COO, optional\n            The array to write the output to.\n        ddof: int\n            The degrees of freedom.\n        keepdims : bool, optional\n            Whether or not to keep the dimensions of the original array.\n\n        Returns\n        -------\n        COO\n            The reduced output sparse array.\n\n        See Also\n        --------\n        numpy.ndarray.std : Equivalent numpy method.\n\n        Notes\n        -----\n        * This function internally calls :obj:`COO.sum_duplicates` to bring the\n          array into canonical form.\n\n        Examples\n        --------\n        You can use :obj:`COO.std` to compute the standard deviation of an array\n        across any dimension.\n\n        >>> x = np.array([[1, 2, 0, 0],\n        ...               [0, 1, 0, 0]], dtype=\'i8\')\n        >>> s = COO.from_numpy(x)\n        >>> s2 = s.std(axis=1)\n        >>> s2.todense()  # doctest: +SKIP\n        array([0.8291562, 0.4330127])\n\n        You can also use the :code:`keepdims` argument to keep the dimensions\n        after the standard deviation.\n\n        >>> s3 = s.std(axis=0, keepdims=True)\n        >>> s3.shape\n        (1, 4)\n\n        You can pass in an output datatype, if needed.\n\n        >>> s4 = s.std(axis=0, dtype=np.float16)\n        >>> s4.dtype\n        dtype(\'float16\')\n\n        By default, this reduces the array down to one number, computing the\n        standard deviation along all axes.\n\n        >>> s.std()  # doctest: +SKIP\n        0.7071067811865476\n        """"""\n        ret = self.var(axis=axis, dtype=dtype, out=out, ddof=ddof, keepdims=keepdims)\n\n        ret = np.sqrt(ret)\n        return ret\n\n    def transpose(self, axes=None):\n        """"""\n        Returns a new array which has the order of the axes switched.\n\n        Parameters\n        ----------\n        axes : Iterable[int], optional\n            The new order of the axes compared to the previous one. Reverses the axes\n            by default.\n\n        Returns\n        -------\n        COO\n            The new array with the axes in the desired order.\n\n        See Also\n        --------\n        :obj:`COO.T` : A quick property to reverse the order of the axes.\n        numpy.ndarray.transpose : Numpy equivalent function.\n\n        Examples\n        --------\n        We can change the order of the dimensions of any :obj:`COO` array with this\n        function.\n\n        >>> x = np.add.outer(np.arange(5), np.arange(5)[::-1])\n        >>> x  # doctest: +NORMALIZE_WHITESPACE\n        array([[4, 3, 2, 1, 0],\n               [5, 4, 3, 2, 1],\n               [6, 5, 4, 3, 2],\n               [7, 6, 5, 4, 3],\n               [8, 7, 6, 5, 4]])\n        >>> s = COO.from_numpy(x)\n        >>> s.transpose((1, 0)).todense()  # doctest: +NORMALIZE_WHITESPACE\n        array([[4, 5, 6, 7, 8],\n               [3, 4, 5, 6, 7],\n               [2, 3, 4, 5, 6],\n               [1, 2, 3, 4, 5],\n               [0, 1, 2, 3, 4]])\n\n        Note that by default, this reverses the order of the axes rather than switching\n        the last and second-to-last axes as required by some linear algebra operations.\n\n        >>> x = np.random.rand(2, 3, 4)\n        >>> s = COO.from_numpy(x)\n        >>> s.transpose().shape\n        (4, 3, 2)\n        """"""\n        if axes is None:\n            axes = list(reversed(range(self.ndim)))\n\n        # Normalize all axes indices to positive values\n        axes = normalize_axis(axes, self.ndim)\n\n        if len(np.unique(axes)) < len(axes):\n            raise ValueError(""repeated axis in transpose"")\n\n        if not len(axes) == self.ndim:\n            raise ValueError(""axes don\'t match array"")\n\n        axes = tuple(axes)\n\n        if axes == tuple(range(self.ndim)):\n            return self\n\n        if self._cache is not None:\n            for ax, value in self._cache[""transpose""]:\n                if ax == axes:\n                    return value\n\n        shape = tuple(self.shape[ax] for ax in axes)\n        result = COO(\n            self.coords[axes, :],\n            self.data,\n            shape,\n            has_duplicates=False,\n            cache=self._cache is not None,\n            fill_value=self.fill_value,\n        )\n\n        if self._cache is not None:\n            self._cache[""transpose""].append((axes, result))\n        return result\n\n    @property\n    def T(self):\n        """"""\n        Returns a new array which has the order of the axes reversed.\n\n        Returns\n        -------\n        COO\n            The new array with the axes in the desired order.\n\n        See Also\n        --------\n        :obj:`COO.transpose` : A method where you can specify the order of the axes.\n        numpy.ndarray.T : Numpy equivalent property.\n\n        Examples\n        --------\n        We can change the order of the dimensions of any :obj:`COO` array with this\n        function.\n\n        >>> x = np.add.outer(np.arange(5), np.arange(5)[::-1])\n        >>> x  # doctest: +NORMALIZE_WHITESPACE\n        array([[4, 3, 2, 1, 0],\n               [5, 4, 3, 2, 1],\n               [6, 5, 4, 3, 2],\n               [7, 6, 5, 4, 3],\n               [8, 7, 6, 5, 4]])\n        >>> s = COO.from_numpy(x)\n        >>> s.T.todense()  # doctest: +NORMALIZE_WHITESPACE\n        array([[4, 5, 6, 7, 8],\n               [3, 4, 5, 6, 7],\n               [2, 3, 4, 5, 6],\n               [1, 2, 3, 4, 5],\n               [0, 1, 2, 3, 4]])\n\n        Note that by default, this reverses the order of the axes rather than switching\n        the last and second-to-last axes as required by some linear algebra operations.\n\n        >>> x = np.random.rand(2, 3, 4)\n        >>> s = COO.from_numpy(x)\n        >>> s.T.shape\n        (4, 3, 2)\n        """"""\n        return self.transpose(tuple(range(self.ndim))[::-1])\n\n    def swapaxes(self, axis1, axis2):\n        """""" Returns array that has axes axis1 and axis2 swapped.\n\n        Parameters\n        ----------\n        axis1 : int\n            first axis to swap\n        axis2: int\n            second axis to swap\n\n        Returns\n        -------\n        COO\n            The new array with the axes axis1 and axis2 swapped.\n\n        Examples\n        --------\n        >>> x = COO.from_numpy(np.ones((2, 3, 4)))\n        >>> x.swapaxes(0, 2)\n        <COO: shape=(4, 3, 2), dtype=float64, nnz=24, fill_value=0.0>\n        """"""\n        # Normalize all axis1, axis2 to positive values\n        axis1, axis2 = normalize_axis(\n            (axis1, axis2), self.ndim\n        )  # checks if axis1,2 are in range + raises ValueError\n        axes = list(range(self.ndim))\n        axes[axis1], axes[axis2] = axes[axis2], axes[axis1]\n        return self.transpose(axes)\n\n    @property\n    def real(self):\n        """"""The real part of the array.\n\n        Examples\n        --------\n        >>> x = COO.from_numpy([1 + 0j, 0 + 1j])\n        >>> x.real.todense()  # doctest: +SKIP\n        array([1., 0.])\n        >>> x.real.dtype\n        dtype(\'float64\')\n\n        Returns\n        -------\n        out : COO\n            The real component of the array elements. If the array dtype is\n            real, the dtype of the array is used for the output. If the array\n            is complex, the output dtype is float.\n\n        See Also\n        --------\n        numpy.ndarray.real : NumPy equivalent attribute.\n        numpy.real : NumPy equivalent function.\n        """"""\n        return self.__array_ufunc__(np.real, ""__call__"", self)\n\n    @property\n    def imag(self):\n        """"""The imaginary part of the array.\n\n        Examples\n        --------\n        >>> x = COO.from_numpy([1 + 0j, 0 + 1j])\n        >>> x.imag.todense()  # doctest: +SKIP\n        array([0., 1.])\n        >>> x.imag.dtype\n        dtype(\'float64\')\n\n        Returns\n        -------\n        out : COO\n            The imaginary component of the array elements. If the array dtype\n            is real, the dtype of the array is used for the output. If the\n            array is complex, the output dtype is float.\n\n        See Also\n        --------\n        numpy.ndarray.imag : NumPy equivalent attribute.\n        numpy.imag : NumPy equivalent function.\n        """"""\n        return self.__array_ufunc__(np.imag, ""__call__"", self)\n\n    def conj(self):\n        """"""Return the complex conjugate, element-wise.\n\n        The complex conjugate of a complex number is obtained by changing the\n        sign of its imaginary part.\n\n        Examples\n        --------\n        >>> x = COO.from_numpy([1 + 2j, 2 - 1j])\n        >>> res = x.conj()\n        >>> res.todense()  # doctest: +SKIP\n        array([1.-2.j, 2.+1.j])\n        >>> res.dtype\n        dtype(\'complex128\')\n\n        Returns\n        -------\n        out : COO\n            The complex conjugate, with same dtype as the input.\n\n        See Also\n        --------\n        numpy.ndarray.conj : NumPy equivalent method.\n        numpy.conj : NumPy equivalent function.\n        """"""\n        return np.conj(self)\n\n    def dot(self, other):\n        """"""\n        Performs the equivalent of :code:`x.dot(y)` for :obj:`COO`.\n\n        Parameters\n        ----------\n        other : Union[COO, numpy.ndarray, scipy.sparse.spmatrix]\n            The second operand of the dot product operation.\n\n        Returns\n        -------\n        {COO, numpy.ndarray}\n            The result of the dot product. If the result turns out to be dense,\n            then a dense array is returned, otherwise, a sparse array.\n\n        Raises\n        ------\n        ValueError\n            If all arguments don\'t have zero fill-values.\n\n        See Also\n        --------\n        dot : Equivalent function for two arguments.\n        :obj:`numpy.dot` : Numpy equivalent function.\n        scipy.sparse.coo_matrix.dot : Scipy equivalent function.\n\n        Examples\n        --------\n        >>> x = np.arange(4).reshape((2, 2))\n        >>> s = COO.from_numpy(x)\n        >>> s.dot(s) # doctest: +SKIP\n        array([[ 2,  3],\n               [ 6, 11]], dtype=int64)\n        """"""\n        return dot(self, other)\n\n    def __matmul__(self, other):\n        try:\n            return matmul(self, other)\n        except NotImplementedError:\n            return NotImplemented\n\n    def __rmatmul__(self, other):\n        try:\n            return matmul(other, self)\n        except NotImplementedError:\n            return NotImplemented\n\n    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):\n        out = kwargs.pop(""out"", None)\n        if out is not None and not all(isinstance(x, COO) for x in out):\n            return NotImplemented\n\n        if getattr(ufunc, ""signature"", None) is not None:\n            return self.__array_function__(ufunc, (np.ndarray, COO), inputs, kwargs)\n\n        if out is not None:\n            kwargs[""dtype""] = out[0].dtype\n\n        if method == ""outer"":\n            method = ""__call__""\n\n            cum_ndim = 0\n            inputs_transformed = []\n            for inp in reversed(inputs):\n                inputs_transformed.append(inp[(Ellipsis,) + (None,) * cum_ndim])\n                cum_ndim += inp.ndim\n\n            inputs = tuple(reversed(inputs_transformed))\n\n        if method == ""__call__"":\n            result = elemwise(ufunc, *inputs, **kwargs)\n        elif method == ""reduce"":\n            result = COO._reduce(ufunc, *inputs, **kwargs)\n        else:\n            return NotImplemented\n\n        if out is not None:\n            (out,) = out\n            if out.shape != result.shape:\n                raise ValueError(\n                    ""non-broadcastable output operand with shape %s ""\n                    ""doesn\'t match the broadcast shape %s"" % (out.shape, result.shape)\n                )\n\n            out._make_shallow_copy_of(result)\n            return out\n\n        return result\n\n    def linear_loc(self):\n        """"""\n        The nonzero coordinates of a flattened version of this array. Note that\n        the coordinates may be out of order.\n\n        Parameters\n        ----------\n        signed : bool, optional\n            Whether to use a signed datatype for the output array. :code:`False`\n            by default.\n\n        Returns\n        -------\n        numpy.ndarray\n            The flattened coordinates.\n\n        See Also\n        --------\n        :obj:`numpy.flatnonzero` : Equivalent Numpy function.\n\n        Examples\n        --------\n        >>> x = np.eye(5)\n        >>> s = COO.from_numpy(x)\n        >>> s.linear_loc()  # doctest: +NORMALIZE_WHITESPACE\n        array([ 0,  6, 12, 18, 24])\n        >>> np.array_equal(np.flatnonzero(x), s.linear_loc())\n        True\n        """"""\n        from .common import linear_loc\n\n        return linear_loc(self.coords, self.shape)\n\n    def flatten(self, order=""C""):\n        """"""\n        Returns a new :obj:`COO` array that is a flattened version of this array.\n\n        Returns\n        -------\n        COO\n            The flattened output array.\n\n        Notes\n        -----\n        The :code:`order` parameter is provided just for compatibility with\n        Numpy and isn\'t actually supported.\n\n        Examples\n        --------\n        >>> s = COO.from_numpy(np.arange(10))\n        >>> s2 = s.reshape((2, 5)).flatten()\n        >>> s2.todense()\n        array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n        """"""\n        if order not in {""C"", None}:\n            raise NotImplementedError(""The `order` parameter is not"" ""supported."")\n\n        return self.reshape(-1)\n\n    def reshape(self, shape, order=""C""):\n        """"""\n        Returns a new :obj:`COO` array that is a reshaped version of this array.\n        Parameters\n        ----------\n        shape : tuple[int]\n            The desired shape of the output array.\n        Returns\n        -------\n        COO\n            The reshaped output array.\n        See Also\n        --------\n        numpy.ndarray.reshape : The equivalent Numpy function.\n        Notes\n        -----\n        The :code:`order` parameter is provided just for compatibility with\n        Numpy and isn\'t actually supported.\n        Examples\n        --------\n        >>> s = COO.from_numpy(np.arange(25))\n        >>> s2 = s.reshape((5, 5))\n        >>> s2.todense()  # doctest: +NORMALIZE_WHITESPACE\n        array([[ 0,  1,  2,  3,  4],\n               [ 5,  6,  7,  8,  9],\n               [10, 11, 12, 13, 14],\n               [15, 16, 17, 18, 19],\n               [20, 21, 22, 23, 24]])\n        """"""\n        if isinstance(shape, Iterable):\n            shape = tuple(shape)\n        else:\n            shape = (shape,)\n\n        if order not in {""C"", None}:\n            raise NotImplementedError(""The `order` parameter is not supported"")\n\n        if self.shape == shape:\n            return self\n        if any(d == -1 for d in shape):\n            extra = int(self.size / np.prod([d for d in shape if d != -1]))\n            shape = tuple([d if d != -1 else extra for d in shape])\n\n        if self.shape == shape:\n            return self\n\n        if self.size != reduce(operator.mul, shape, 1):\n            raise ValueError(\n                ""cannot reshape array of size {} into shape {}"".format(self.size, shape)\n            )\n\n        if self._cache is not None:\n            for sh, value in self._cache[""reshape""]:\n                if sh == shape:\n                    return value\n\n        # TODO: this self.size enforces a 2**64 limit to array size\n        linear_loc = self.linear_loc()\n\n        coords = np.empty((len(shape), self.nnz), dtype=np.intp)\n        strides = 1\n        for i, d in enumerate(shape[::-1]):\n            coords[-(i + 1), :] = (linear_loc // strides) % d\n            strides *= d\n\n        result = COO(\n            coords,\n            self.data,\n            shape,\n            has_duplicates=False,\n            sorted=True,\n            cache=self._cache is not None,\n            fill_value=self.fill_value,\n        )\n\n        if self._cache is not None:\n            self._cache[""reshape""].append((shape, result))\n        return result\n\n    def resize(self, *args, refcheck=True):\n        """"""\n        This method changes the shape and size of an array in-place.\n        Parameters\n        ----------\n        args : tuple, or series of integers\n            The desired shape of the output array.\n\n        See Also\n        --------\n        numpy.ndarray.resize : The equivalent Numpy function.\n\n        """"""\n        if len(args) == 1 and isinstance(args[0], tuple):\n            shape = args[0]\n        elif all(isinstance(arg, int) for arg in args):\n            shape = tuple(args)\n        else:\n            raise ValueError(""Invalid input"")\n\n        if any(d < 0 for d in shape):\n            raise ValueError(""negative dimensions not allowed"")\n\n        new_size = reduce(operator.mul, shape, 1)\n\n        # TODO: this self.size enforces a 2**64 limit to array size\n        linear_loc = self.linear_loc()\n        end_idx = np.searchsorted(linear_loc, new_size, side=""left"")\n        linear_loc = linear_loc[:end_idx]\n\n        coords = np.empty((len(shape), len(linear_loc)), dtype=np.intp)\n        strides = 1\n        for i, d in enumerate(shape[::-1]):\n            coords[-(i + 1), :] = (linear_loc // strides) % d\n            strides *= d\n\n        self.shape = shape\n        self.coords = coords\n\n        if len(self.data) != len(linear_loc):\n            self.data = self.data[:end_idx].copy()\n\n    def to_scipy_sparse(self):\n        """"""\n        Converts this :obj:`COO` object into a :obj:`scipy.sparse.coo_matrix`.\n\n        Returns\n        -------\n        :obj:`scipy.sparse.coo_matrix`\n            The converted Scipy sparse matrix.\n\n        Raises\n        ------\n        ValueError\n            If the array is not two-dimensional.\n\n        ValueError\n            If all the array doesn\'t zero fill-values.\n\n        See Also\n        --------\n        COO.tocsr : Convert to a :obj:`scipy.sparse.csr_matrix`.\n        COO.tocsc : Convert to a :obj:`scipy.sparse.csc_matrix`.\n        """"""\n        check_zero_fill_value(self)\n\n        if self.ndim != 2:\n            raise ValueError(\n                ""Can only convert a 2-dimensional array to a Scipy sparse matrix.""\n            )\n\n        result = scipy.sparse.coo_matrix(\n            (self.data, (self.coords[0], self.coords[1])), shape=self.shape\n        )\n        result.has_canonical_format = True\n        return result\n\n    def _tocsr(self):\n        if self.ndim != 2:\n            raise ValueError(\n                ""This array must be two-dimensional for this conversion "" ""to work.""\n            )\n        row, col = self.coords\n\n        # Pass 3: count nonzeros in each row\n        indptr = np.zeros(self.shape[0] + 1, dtype=np.int64)\n        np.cumsum(np.bincount(row, minlength=self.shape[0]), out=indptr[1:])\n\n        return scipy.sparse.csr_matrix((self.data, col, indptr), shape=self.shape)\n\n    def tocsr(self):\n        """"""\n        Converts this array to a :obj:`scipy.sparse.csr_matrix`.\n\n        Returns\n        -------\n        scipy.sparse.csr_matrix\n            The result of the conversion.\n\n        Raises\n        ------\n        ValueError\n            If the array is not two-dimensional.\n\n        ValueError\n            If all the array doesn\'t have zero fill-values.\n\n        See Also\n        --------\n        COO.tocsc : Convert to a :obj:`scipy.sparse.csc_matrix`.\n        COO.to_scipy_sparse : Convert to a :obj:`scipy.sparse.coo_matrix`.\n        scipy.sparse.coo_matrix.tocsr : Equivalent Scipy function.\n        """"""\n        check_zero_fill_value(self)\n\n        if self._cache is not None:\n            try:\n                return self._csr\n            except AttributeError:\n                pass\n            try:\n                self._csr = self._csc.tocsr()\n                return self._csr\n            except AttributeError:\n                pass\n\n            self._csr = csr = self._tocsr()\n        else:\n            csr = self._tocsr()\n        return csr\n\n    def tocsc(self):\n        """"""\n        Converts this array to a :obj:`scipy.sparse.csc_matrix`.\n\n        Returns\n        -------\n        scipy.sparse.csc_matrix\n            The result of the conversion.\n\n        Raises\n        ------\n        ValueError\n            If the array is not two-dimensional.\n\n        ValueError\n            If the array doesn\'t have zero fill-values.\n\n        See Also\n        --------\n        COO.tocsr : Convert to a :obj:`scipy.sparse.csr_matrix`.\n        COO.to_scipy_sparse : Convert to a :obj:`scipy.sparse.coo_matrix`.\n        scipy.sparse.coo_matrix.tocsc : Equivalent Scipy function.\n        """"""\n        check_zero_fill_value(self)\n\n        if self._cache is not None:\n            try:\n                return self._csc\n            except AttributeError:\n                pass\n            try:\n                self._csc = self._csr.tocsc()\n                return self._csc\n            except AttributeError:\n                pass\n\n            self._csc = csc = self.tocsr().tocsc()\n        else:\n            csc = self.tocsr().tocsc()\n\n        return csc\n\n    def _sort_indices(self):\n        """"""\n        Sorts the :obj:`COO.coords` attribute. Also sorts the data in\n        :obj:`COO.data` to match.\n\n        Examples\n        --------\n        >>> coords = np.array([[1, 2, 0]], dtype=np.uint8)\n        >>> data = np.array([4, 1, 3], dtype=np.uint8)\n        >>> s = COO(coords, data)\n        >>> s._sort_indices()\n        >>> s.coords  # doctest: +NORMALIZE_WHITESPACE\n        array([[0, 1, 2]])\n        >>> s.data  # doctest: +NORMALIZE_WHITESPACE\n        array([3, 4, 1], dtype=uint8)\n        """"""\n        linear = self.linear_loc()\n\n        if (np.diff(linear) >= 0).all():  # already sorted\n            return\n\n        order = np.argsort(linear, kind=""mergesort"")\n        self.coords = self.coords[:, order]\n        self.data = self.data[order]\n\n    def _sum_duplicates(self):\n        """"""\n        Sums data corresponding to duplicates in :obj:`COO.coords`.\n\n        See Also\n        --------\n        scipy.sparse.coo_matrix.sum_duplicates : Equivalent Scipy function.\n\n        Examples\n        --------\n        >>> coords = np.array([[0, 1, 1, 2]], dtype=np.uint8)\n        >>> data = np.array([6, 5, 2, 2], dtype=np.uint8)\n        >>> s = COO(coords, data)\n        >>> s._sum_duplicates()\n        >>> s.coords  # doctest: +NORMALIZE_WHITESPACE\n        array([[0, 1, 2]])\n        >>> s.data  # doctest: +NORMALIZE_WHITESPACE\n        array([6, 7, 2], dtype=uint8)\n        """"""\n        # Inspired by scipy/sparse/coo.py::sum_duplicates\n        # See https://github.com/scipy/scipy/blob/master/LICENSE.txt\n        linear = self.linear_loc()\n        unique_mask = np.diff(linear) != 0\n\n        if unique_mask.sum() == len(unique_mask):  # already unique\n            return\n\n        unique_mask = np.append(True, unique_mask)\n\n        coords = self.coords[:, unique_mask]\n        (unique_inds,) = np.nonzero(unique_mask)\n        data = np.add.reduceat(self.data, unique_inds, dtype=self.data.dtype)\n\n        self.data = data\n        self.coords = coords\n\n    def _prune(self):\n        """"""\n        Prunes data so that if any fill-values are present, they are removed\n        from both coordinates and data.\n\n        Examples\n        --------\n        >>> coords = np.array([[0, 1, 2, 3]])\n        >>> data = np.array([1, 0, 1, 2])\n        >>> s = COO(coords, data)\n        >>> s._prune()\n        >>> s.nnz\n        3\n        """"""\n        mask = ~equivalent(self.data, self.fill_value)\n        self.coords = self.coords[:, mask]\n        self.data = self.data[mask]\n\n    def broadcast_to(self, shape):\n        """"""\n        Performs the equivalent of :obj:`numpy.broadcast_to` for :obj:`COO`. Note that\n        this function returns a new array instead of a view.\n\n        Parameters\n        ----------\n        shape : tuple[int]\n            The shape to broadcast the data to.\n\n        Returns\n        -------\n        COO\n            The broadcasted sparse array.\n\n        Raises\n        ------\n        ValueError\n            If the operand cannot be broadcast to the given shape.\n\n        See also\n        --------\n        :obj:`numpy.broadcast_to` : NumPy equivalent function\n        """"""\n        return broadcast_to(self, shape)\n\n    def round(self, decimals=0, out=None):\n        """"""\n        Evenly round to the given number of decimals.\n\n        See also\n        --------\n        :obj:`numpy.round` : NumPy equivalent ufunc.\n        :obj:`COO.elemwise`: Apply an arbitrary element-wise function to one or two\n            arguments.\n        """"""\n        if out is not None and not isinstance(out, tuple):\n            out = (out,)\n        return self.__array_ufunc__(\n            np.round, ""__call__"", self, decimals=decimals, out=out\n        )\n\n    round_ = round\n\n    def clip(self, min=None, max=None, out=None):\n        """"""\n        Clip (limit) the values in the array.\n\n        Return an array whose values are limited to ``[min, max]``. One of min\n        or max must be given.\n\n        See Also\n        --------\n        sparse.clip : For full documentation and more details.\n        numpy.clip : Equivalent NumPy function.\n        """"""\n        if min is None and max is None:\n            raise ValueError(""One of max or min must be given."")\n        if out is not None and not isinstance(out, tuple):\n            out = (out,)\n        return self.__array_ufunc__(\n            np.clip, ""__call__"", self, a_min=min, a_max=max, out=out\n        )\n\n    def astype(self, dtype, copy=True):\n        """"""\n        Copy of the array, cast to a specified type.\n\n        See also\n        --------\n        scipy.sparse.coo_matrix.astype : SciPy sparse equivalent function\n        numpy.ndarray.astype : NumPy equivalent ufunc.\n        :obj:`COO.elemwise`: Apply an arbitrary element-wise function to one or two\n            arguments.\n        """"""\n        # this matches numpy\'s behavior\n        if self.dtype == dtype and not copy:\n            return self\n        return self.__array_ufunc__(\n            np.ndarray.astype, ""__call__"", self, dtype=dtype, copy=copy\n        )\n\n    def maybe_densify(self, max_size=1000, min_density=0.25):\n        """"""\n        Converts this :obj:`COO` array to a :obj:`numpy.ndarray` if not too\n        costly.\n\n        Parameters\n        ----------\n        max_size : int\n            Maximum number of elements in output\n        min_density : float\n            Minimum density of output\n\n        Returns\n        -------\n        numpy.ndarray\n            The dense array.\n\n        Raises\n        -------\n        ValueError\n            If the returned array would be too large.\n\n        Examples\n        --------\n        Convert a small sparse array to a dense array.\n\n        >>> s = COO.from_numpy(np.random.rand(2, 3, 4))\n        >>> x = s.maybe_densify()\n        >>> np.allclose(x, s.todense())\n        True\n\n        You can also specify the minimum allowed density or the maximum number\n        of output elements. If both conditions are unmet, this method will throw\n        an error.\n\n        >>> x = np.zeros((5, 5), dtype=np.uint8)\n        >>> x[2, 2] = 1\n        >>> s = COO.from_numpy(x)\n        >>> s.maybe_densify(max_size=5, min_density=0.25)\n        Traceback (most recent call last):\n            ...\n        ValueError: Operation would require converting large sparse array to dense\n        """"""\n        if self.size <= max_size or self.density >= min_density:\n            return self.todense()\n        else:\n            raise ValueError(\n                ""Operation would require converting "" ""large sparse array to dense""\n            )\n\n    def nonzero(self):\n        """"""\n        Get the indices where this array is nonzero.\n\n        Returns\n        -------\n        idx : tuple[numpy.ndarray]\n            The indices where this array is nonzero.\n\n        See Also\n        --------\n        :obj:`numpy.ndarray.nonzero` : NumPy equivalent function\n\n        Raises\n        ------\n        ValueError\n            If the array doesn\'t have zero fill-values.\n\n        Examples\n        --------\n        >>> s = COO.from_numpy(np.eye(5))\n        >>> s.nonzero()\n        (array([0, 1, 2, 3, 4]), array([0, 1, 2, 3, 4]))\n        """"""\n        check_zero_fill_value(self)\n        return tuple(self.coords)\n\n    def asformat(self, format, compressed_axes=None):\n        """"""\n        Convert this sparse array to a given format.\n\n        Parameters\n        ----------\n        format : str\n            A format string.\n\n        Returns\n        -------\n        out : SparseArray\n            The converted array.\n\n        Raises\n        ------\n        NotImplementedError\n            If the format isn\'t supported.\n        """"""\n        from .._compressed import GCXS\n\n        if format == ""gcxs"" or format is GCXS:\n            return GCXS.from_coo(self, compressed_axes=compressed_axes)\n\n        elif compressed_axes is not None:\n            raise ValueError(\n                ""compressed_axes is not supported for {} format"".format(format)\n            )\n\n        if format == ""coo"" or format is COO:\n            return self\n\n        from .._dok import DOK\n\n        if format == ""dok"" or format is DOK:\n            return DOK.from_coo(self)\n\n        raise NotImplementedError(""The given format is not supported."")\n\n\ndef as_coo(x, shape=None, fill_value=None):\n    """"""\n    Converts any given format to :obj:`COO`. See the ""See Also"" section for details.\n\n    Parameters\n    ----------\n    x : SparseArray or numpy.ndarray or scipy.sparse.spmatrix or Iterable.\n        The item to convert.\n    shape : tuple[int], optional\n        The shape of the output array. Can only be used in case of Iterable.\n\n    Returns\n    -------\n    out : COO\n        The converted :obj:`COO` array.\n\n    See Also\n    --------\n    SparseArray.asformat : A utility function to convert between formats in this library.\n    COO.from_numpy : Convert a Numpy array to :obj:`COO`.\n    COO.from_scipy_sparse : Convert a SciPy sparse matrix to :obj:`COO`.\n    COO.from_iter : Convert an iterable to :obj:`COO`.\n    """"""\n    if hasattr(x, ""shape"") and shape is not None:\n        raise ValueError(\n            ""Cannot provide a shape in combination with something ""\n            ""that already has a shape.""\n        )\n\n    if hasattr(x, ""fill_value"") and fill_value is not None:\n        raise ValueError(\n            ""Cannot provide a fill-value in combination with something ""\n            ""that already has a fill-value.""\n        )\n\n    if isinstance(x, SparseArray):\n        return x.asformat(""coo"")\n\n    if isinstance(x, np.ndarray):\n        return COO.from_numpy(x, fill_value=fill_value)\n\n    if isinstance(x, scipy.sparse.spmatrix):\n        return COO.from_scipy_sparse(x)\n\n    if isinstance(x, (Iterable, Iterator)):\n        return COO.from_iter(x, shape=shape, fill_value=fill_value)\n\n    raise NotImplementedError(\n        ""Format not supported for conversion. Supplied type is ""\n        ""%s, see help(sparse.as_coo) for supported formats."" % type(x)\n    )\n\n\ndef _keepdims(original, new, axis):\n    shape = list(original.shape)\n    for ax in axis:\n        shape[ax] = 1\n    return new.reshape(shape)\n\n\n@numba.jit(nopython=True, nogil=True)  # pragma: no cover\ndef _calc_counts_invidx(groups):\n    inv_idx = []\n    counts = []\n\n    if len(groups) == 0:\n        return (\n            np.array(inv_idx, dtype=groups.dtype),\n            np.array(counts, dtype=groups.dtype),\n        )\n\n    inv_idx.append(0)\n\n    last_group = groups[0]\n    for i in range(1, len(groups)):\n        if groups[i] != last_group:\n            counts.append(i - inv_idx[-1])\n            inv_idx.append(i)\n            last_group = groups[i]\n\n    counts.append(len(groups) - inv_idx[-1])\n\n    return (np.array(inv_idx, dtype=groups.dtype), np.array(counts, dtype=groups.dtype))\n\n\ndef _grouped_reduce(x, groups, method, **kwargs):\n    """"""\n    Performs a :code:`ufunc` grouped reduce.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        The data to reduce.\n    groups : np.ndarray\n        The groups the data belongs to. The groups must be\n        contiguous.\n    method : np.ufunc\n        The :code:`ufunc` to use to perform the reduction.\n    kwargs : dict\n        The kwargs to pass to the :code:`ufunc`\'s :code:`reduceat`\n        function.\n\n    Returns\n    -------\n    result : np.ndarray\n        The result of the grouped reduce operation.\n    inv_idx : np.ndarray\n        The index of the first element where each group is found.\n    counts : np.ndarray\n        The number of elements in each group.\n    """"""\n    # Partial credit to @shoyer\n    # Ref: https://gist.github.com/shoyer/f538ac78ae904c936844\n    inv_idx, counts = _calc_counts_invidx(groups)\n    result = method.reduceat(x, inv_idx, **kwargs)\n    return result, inv_idx, counts\n'"
sparse/_coo/indexing.py,45,"b'from numbers import Integral\n\nimport numba\nimport numpy as np\n\nfrom itertools import zip_longest\n\nfrom .._slicing import normalize_index\nfrom .._utils import _zero_of_dtype, equivalent\n\n\ndef getitem(x, index):\n    """"""\n    This function implements the indexing functionality for COO.\n\n    The overall algorithm has three steps:\n\n    1. Normalize the index to canonical form. Function: normalize_index\n    2. Get the mask, which is a list of integers corresponding to\n       the indices in coords/data for the output data. Function: _mask\n    3. Transform the coordinates to what they will be in the output.\n\n    Parameters\n    ----------\n    x : COO\n        The array to apply the indexing operation on.\n    index : {tuple, str}\n        The index into the array.\n    """"""\n    from .core import COO\n\n    # If string, this is an index into an np.void\n\n    # Custom dtype.\n    if isinstance(index, str):\n        data = x.data[index]\n        idx = np.where(data)\n        data = data[idx].flatten()\n        coords = list(x.coords[:, idx[0]])\n        coords.extend(idx[1:])\n\n        fill_value_idx = np.asarray(x.fill_value[index]).flatten()\n        fill_value = (\n            fill_value_idx[0] if fill_value_idx.size else _zero_of_dtype(data.dtype)[()]\n        )\n\n        if not equivalent(fill_value, fill_value_idx).all():\n            raise ValueError(""Fill-values in the array are inconsistent."")\n\n        return COO(\n            coords,\n            data,\n            shape=x.shape + x.data.dtype[index].shape,\n            has_duplicates=False,\n            sorted=True,\n            fill_value=fill_value,\n        )\n\n    # Otherwise, convert into a tuple.\n    if not isinstance(index, tuple):\n        index = (index,)\n\n    # Check if the last index is an ellipsis.\n    last_ellipsis = len(index) > 0 and index[-1] is Ellipsis\n\n    # Normalize the index into canonical form.\n    index = normalize_index(index, x.shape)\n\n    # zip_longest so things like x[..., None] are picked up.\n    if len(index) != 0 and all(\n        isinstance(ind, slice) and ind == slice(0, dim, 1)\n        for ind, dim in zip_longest(index, x.shape)\n    ):\n        return x\n\n    # Get the mask\n    mask, adv_idx = _mask(x.coords, index, x.shape)\n\n    # Get the length of the mask\n    if isinstance(mask, slice):\n        n = len(range(mask.start, mask.stop, mask.step))\n    else:\n        n = len(mask)\n\n    coords = []\n    shape = []\n    i = 0\n\n    sorted = adv_idx is None or adv_idx.pos == 0\n    adv_idx_added = False\n    for ind in index:\n        # Nothing is added to shape or coords if the index is an integer.\n        if isinstance(ind, Integral):\n            i += 1\n            continue\n        # Add to the shape and transform the coords in the case of a slice.\n        elif isinstance(ind, slice):\n            shape.append(len(range(ind.start, ind.stop, ind.step)))\n            coords.append((x.coords[i, mask] - ind.start) // ind.step)\n            i += 1\n            if ind.step < 0:\n                sorted = False\n        # Add the index and shape for the advanced index.\n        elif isinstance(ind, np.ndarray):\n            if not adv_idx_added:\n                shape.append(adv_idx.length)\n                coords.append(adv_idx.idx)\n                adv_idx_added = True\n            i += 1\n        # Add a dimension for None.\n        elif ind is None:\n            coords.append(np.zeros(n, dtype=np.intp))\n            shape.append(1)\n\n    # Join all the transformed coords.\n    if coords:\n        coords = np.stack(coords, axis=0)\n    else:\n        # If index result is a scalar, return a 0-d COO or\n        # a scalar depending on whether the last index is an ellipsis.\n        if last_ellipsis:\n            coords = np.empty((0, n), dtype=np.uint8)\n        else:\n            if n != 0:\n                return x.data[mask][0]\n            else:\n                return x.fill_value\n\n    shape = tuple(shape)\n    data = x.data[mask]\n\n    return COO(\n        coords,\n        data,\n        shape=shape,\n        has_duplicates=False,\n        sorted=sorted,\n        fill_value=x.fill_value,\n    )\n\n\ndef _mask(coords, indices, shape):\n    indices = _prune_indices(indices, shape)\n    indices, adv_idx, adv_idx_pos = _separate_adv_indices(indices)\n\n    if len(adv_idx) != 0:\n        if len(adv_idx) != 1:\n\n            # Ensure if multiple advanced indices are passed, all are of the same length\n            # Also check each advanced index to ensure each is only a one-dimensional iterable\n            adv_ix_len = len(adv_idx[0])\n            for ai in adv_idx:\n                if len(ai) != adv_ix_len:\n                    raise IndexError(\n                        ""shape mismatch: indexing arrays could not be broadcast together. Ensure all indexing arrays are of the same length.""\n                    )\n                if ai.ndim != 1:\n                    raise IndexError(""Only one-dimensional iterable indices supported."")\n\n            mask, aidxs = _compute_multi_axis_multi_mask(\n                coords,\n                _ind_ar_from_indices(indices),\n                np.array(adv_idx, dtype=np.intp),\n                np.array(adv_idx_pos, dtype=np.intp),\n            )\n            return mask, _AdvIdxInfo(aidxs, adv_idx_pos, adv_ix_len)\n\n        else:\n            adv_idx = adv_idx[0]\n            adv_idx_pos = adv_idx_pos[0]\n\n            if adv_idx.ndim != 1:\n                raise IndexError(""Only one-dimensional iterable indices supported."")\n\n            mask, aidxs = _compute_multi_mask(\n                coords, _ind_ar_from_indices(indices), adv_idx, adv_idx_pos\n            )\n            return mask, _AdvIdxInfo(aidxs, adv_idx_pos, len(adv_idx))\n\n    mask, is_slice = _compute_mask(coords, _ind_ar_from_indices(indices))\n\n    if is_slice:\n        return slice(mask[0], mask[1], 1), None\n    else:\n        return mask, None\n\n\ndef _ind_ar_from_indices(indices):\n    """"""\n    Computes an index ""array"" from indices, such that ``indices[i]`` is\n    transformed to ``ind_ar[i]`` and ``ind_ar[i].shape == (3,)``. It has the\n    format ``[start, stop, step]``. Integers are converted into steps as well.\n\n    Parameters\n    ----------\n    indices : Iterable\n        Input indices (slices and integers)\n\n    Returns\n    -------\n    ind_ar : np.ndarray\n        The output array.\n\n    Examples\n    --------\n    >>> _ind_ar_from_indices([1])\n    array([[1, 2, 1]])\n    >>> _ind_ar_from_indices([slice(5, 7, 2)])\n    array([[5, 7, 2]])\n    """"""\n    ind_ar = np.empty((len(indices), 3), dtype=np.intp)\n\n    for i, idx in enumerate(indices):\n        if isinstance(idx, slice):\n            ind_ar[i] = [idx.start, idx.stop, idx.step]\n        elif isinstance(idx, Integral):\n            ind_ar[i] = [idx, idx + 1, 1]\n\n    return ind_ar\n\n\ndef _prune_indices(indices, shape, prune_none=True):\n    """"""\n    Gets rid of the indices that do not contribute to the\n    overall mask, e.g. None and full slices.\n\n    Parameters\n    ----------\n    indices : tuple\n        The indices to the array.\n    shape : tuple[int]\n        The shape of the array.\n\n    Returns\n    -------\n    indices : tuple\n        The filtered indices.\n\n    Examples\n    --------\n    >>> _prune_indices((None, 5), (10,)) # None won\'t affect the mask\n    [5]\n    >>> _prune_indices((slice(0, 10, 1),), (10,)) # Full slices don\'t affect the mask\n    []\n    """"""\n    if prune_none:\n        indices = [idx for idx in indices if idx is not None]\n\n    i = 0\n    for idx, l in zip(indices[::-1], shape[::-1]):\n        if not isinstance(idx, slice):\n            break\n\n        if idx.start == 0 and idx.stop == l and idx.step == 1:\n            i += 1\n            continue\n\n        if idx.start == l - 1 and idx.stop == -1 and idx.step == -1:\n            i += 1\n            continue\n\n        break\n    if i != 0:\n        indices = indices[:-i]\n    return indices\n\n\ndef _separate_adv_indices(indices):\n    """"""\n    Separates advanced from normal indices.\n\n    Parameters\n    ----------\n    indices : list\n        The input indices\n\n    Returns\n    -------\n    new_idx : list\n        The normal indices.\n    adv_idx : list\n        The advanced indices.\n    adv_idx_pos : list\n        The positions of the advanced indices.\n    """"""\n    adv_idx_pos = []\n    new_idx = []\n    adv_idx = []\n\n    for i, idx in enumerate(indices):\n        if isinstance(idx, np.ndarray):\n            adv_idx.append(idx)\n            adv_idx_pos.append(i)\n        else:\n            new_idx.append(idx)\n\n    return new_idx, adv_idx, adv_idx_pos\n\n\n@numba.jit(nopython=True, nogil=True)\ndef _compute_multi_axis_multi_mask(\n    coords, indices, adv_idx, adv_idx_pos\n):  # pragma: no cover\n    """"""\n    Computes a mask with the advanced index, and also returns the advanced index\n    dimension.\n\n    Parameters\n    ----------\n    coords : np.ndarray\n        Coordinates of the input array.\n    indices : np.ndarray\n        The indices in slice format.\n    adv_idx : np.ndarray\n        List of advanced indices.\n    adv_idx_pos : np.ndarray\n        The position of the advanced indices.\n\n    Returns\n    -------\n    mask : np.ndarray\n        The mask.\n    aidxs : np.ndarray\n        The advanced array index.\n    """"""\n    n_adv_idx = len(adv_idx_pos)\n    mask = numba.typed.List.empty_list(numba.types.intp)\n    a_indices = numba.typed.List.empty_list(numba.types.intp)\n    full_idx = np.empty((len(indices) + len(adv_idx_pos), 3), dtype=np.intp)\n\n    # Get location of non-advanced indices\n    if len(indices) != 0:\n        ixx = 0\n        for ix in range(coords.shape[0]):\n            isin = False\n            for ax in adv_idx_pos:\n                if ix == ax:\n                    isin = True\n                    break\n            if not isin:\n                full_idx[ix] = indices[ixx]\n                ixx += 1\n\n    for i in range(len(adv_idx[0])):\n        for ii in range(n_adv_idx):\n            full_idx[adv_idx_pos[ii]] = [adv_idx[ii][i], adv_idx[ii][i] + 1, 1]\n\n        partial_mask, is_slice = _compute_mask(coords, full_idx)\n        if is_slice:\n            slice_mask = numba.typed.List.empty_list(numba.types.intp)\n            for j in range(partial_mask[0], partial_mask[1]):\n                slice_mask.append(j)\n            partial_mask = array_from_list_intp(slice_mask)\n\n        for j in range(len(partial_mask)):\n            mask.append(partial_mask[j])\n            a_indices.append(i)\n\n    return array_from_list_intp(mask), array_from_list_intp(a_indices)\n\n\n@numba.jit(nopython=True, nogil=True)\ndef _compute_multi_mask(coords, indices, adv_idx, adv_idx_pos):  # pragma: no cover\n    """"""\n    Computes a mask with the advanced index, and also returns the advanced index\n    dimension.\n\n    Parameters\n    ----------\n    coords : np.ndarray\n        Coordinates of the input array.\n    indices : np.ndarray\n        The indices in slice format.\n    adv_idx : list(int)\n        The advanced index.\n    adv_idx_pos : list(int)\n        The position of the advanced index.\n\n    Returns\n    -------\n    mask : np.ndarray\n        The mask.\n    aidxs : np.ndarray\n        The advanced array index.\n    """"""\n    mask = numba.typed.List.empty_list(numba.types.intp)\n    a_indices = numba.typed.List.empty_list(numba.types.intp)\n    full_idx = np.empty((len(indices) + 1, 3), dtype=np.intp)\n\n    full_idx[:adv_idx_pos] = indices[:adv_idx_pos]\n    full_idx[adv_idx_pos + 1 :] = indices[adv_idx_pos:]\n\n    for i, aidx in enumerate(adv_idx):\n        full_idx[adv_idx_pos] = [aidx, aidx + 1, 1]\n        partial_mask, is_slice = _compute_mask(coords, full_idx)\n        if is_slice:\n            slice_mask = numba.typed.List.empty_list(numba.types.intp)\n            for j in range(partial_mask[0], partial_mask[1]):\n                slice_mask.append(j)\n            partial_mask = array_from_list_intp(slice_mask)\n\n        for j in range(len(partial_mask)):\n            mask.append(partial_mask[j])\n            a_indices.append(i)\n\n    return array_from_list_intp(mask), array_from_list_intp(a_indices)\n\n\n@numba.jit(nopython=True, nogil=True)\ndef _compute_mask(coords, indices):  # pragma: no cover\n    """"""\n    Gets the mask for the coords given the indices in slice format.\n\n    Works with either start-stop ranges of matching indices into coords\n    called ""pairs"" (start-stop pairs) or filters the mask directly, based\n    on which is faster.\n\n    Exploits the structure in sorted coords, which is that for a constant\n    value of coords[i - 1], coords[i - 2] and so on, coords[i] is sorted.\n    Concretely, ``coords[i, coords[i - 1] == v1 & coords[i - 2] = v2, ...]``\n    is always sorted. It uses this sortedness to find sub-pairs for each\n    dimension given the previous, and so on. This is efficient for small\n    slices or ints, but not for large ones.\n\n    After it detects that working with pairs is rather inefficient (or after\n    going through each possible index), it constructs a filtered mask from the\n    start-stop pairs.\n\n    Parameters\n    ----------\n    coords : np.ndarray\n        The coordinates of the array.\n    indices : np.ndarray\n        The indices in the form of slices such that indices[:, 0] are starts,\n        indices[:, 1] are stops and indices[:, 2] are steps.\n\n    Returns\n    -------\n    mask : np.ndarray\n        The starts and stops in the mask.\n    is_slice : bool\n        Whether or not the array represents a continuous slice.\n\n    Examples\n    --------\n    Let\'s create some mock coords and indices\n\n    >>> import numpy as np\n    >>> coords = np.array([[0, 0, 1, 1, 2, 2]])\n    >>> indices = np.array([[0, 3, 2]])  # Equivalent to slice(0, 3, 2)\n\n    Now let\'s get the mask. Notice that the indices of ``0`` and ``2`` are matched.\n\n    >>> _compute_mask(coords, indices)\n    (array([0, 1, 4, 5]), False)\n\n    Now, let\'s try with a more ""continuous"" slice. Matches ``0`` and ``1``.\n\n    >>> indices = np.array([[0, 2, 1]])\n    >>> _compute_mask(coords, indices)\n    (array([0, 4]), True)\n\n    This is equivalent to mask being ``slice(0, 4, 1)``.\n    """"""\n    # Set the initial mask to be the entire range of coordinates.\n    starts = numba.typed.List.empty_list(numba.types.intp)\n    starts.append(0)\n    stops = numba.typed.List.empty_list(numba.types.intp)\n    stops.append(coords.shape[1])\n    n_matches = np.intp(coords.shape[1])\n\n    i = 0\n    while i < len(indices):\n        # Guesstimate whether working with pairs is more efficient or\n        # working with the mask directly.\n        # One side is the estimate of time taken for binary searches\n        # (n_searches * log(avg_length))\n        # The other is an estimated time of a linear filter for the mask.\n        n_pairs = len(starts)\n        n_current_slices = (\n            len(range(indices[i, 0], indices[i, 1], indices[i, 2])) * n_pairs + 2\n        )\n        if (\n            n_current_slices * np.log(n_current_slices / max(n_pairs, 1))\n            > n_matches + n_pairs\n        ):\n            break\n\n        # For each of the pairs, search inside the coordinates for other\n        # matching sub-pairs.\n        # This gets the start-end coordinates in coords for each \'sub-array\'\n        # Which would come out of indexing a single integer.\n        starts, stops, n_matches = _get_mask_pairs(starts, stops, coords[i], indices[i])\n\n        i += 1\n\n    # Combine adjacent pairs\n    starts, stops = _join_adjacent_pairs(starts, stops)\n\n    # If just one pair is left over, treat it as a slice.\n    if i == len(indices) and len(starts) == 1:\n        return np.array([starts[0], stops[0]]), True\n\n    # Convert start-stop pairs into mask, filtering by remaining\n    # coordinates.\n    mask = _filter_pairs(starts, stops, coords[i:], indices[i:])\n    return array_from_list_intp(mask), False\n\n\n@numba.jit(nopython=True, nogil=True)\ndef _get_mask_pairs(starts_old, stops_old, c, idx):  # pragma: no cover\n    """"""\n    Gets the pairs for a following dimension given the pairs for\n    a dimension.\n\n    For each pair, it searches in the following dimension for\n    matching coords and returns those.\n\n    The total combined length of all pairs is returned to\n    help with the performance guesstimate.\n\n    Parameters\n    ----------\n    starts_old, stops_old : list[int]\n        The starts and stops from the previous index.\n    c : np.ndarray\n        The coords for this index\'s dimension.\n    idx : np.ndarray\n        The index in the form of a slice.\n        idx[0], idx[1], idx[2] = start, stop, step\n\n    Returns\n    -------\n    starts, stops: list\n        The starts and stops after applying the current index.\n    n_matches : int\n        The sum of elements in all ranges.\n\n    Examples\n    --------\n    >>> c = np.array([1, 2, 1, 2, 1, 1, 2, 2])\n    >>> starts_old = numba.typed.List(); starts_old.append(4)\n    >>> stops_old = numba.typed.List(); stops_old.append(8)\n    >>> idx = np.array([1, 2, 1])\n    >>> _get_mask_pairs(starts_old, stops_old, c, idx)\n    (ListType[int64]([4]), ListType[int64]([6]), 2)\n    """"""\n    starts = numba.typed.List.empty_list(numba.types.intp)\n    stops = numba.typed.List.empty_list(numba.types.intp)\n    n_matches = np.intp(0)\n\n    for j in range(len(starts_old)):\n        # For each matching ""integer"" in the slice, search within the ""sub-coords""\n        # Using binary search.\n        for p_match in range(idx[0], idx[1], idx[2]):\n            start = (\n                np.searchsorted(c[starts_old[j] : stops_old[j]], p_match, side=""left"")\n                + starts_old[j]\n            )\n            stop = (\n                np.searchsorted(c[starts_old[j] : stops_old[j]], p_match, side=""right"")\n                + starts_old[j]\n            )\n\n            if start != stop:\n                starts.append(start)\n                stops.append(stop)\n                n_matches += stop - start\n\n    return starts, stops, n_matches\n\n\n@numba.jit(nopython=True, nogil=True)\ndef _filter_pairs(starts, stops, coords, indices):  # pragma: no cover\n    """"""\n    Converts all the pairs into a single integer mask, additionally filtering\n    by the indices.\n\n    Parameters\n    ----------\n    starts, stops : list[int]\n        The starts and stops to convert into an array.\n    coords : np.ndarray\n        The coordinates to filter by.\n    indices : np.ndarray\n        The indices in the form of slices such that indices[:, 0] are starts,\n        indices[:, 1] are stops and indices[:, 2] are steps.\n\n    Returns\n    -------\n    mask : list\n        The output integer mask.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> starts = numba.typed.List(); starts.append(2)\n    >>> stops = numba.typed.List(); stops.append(7)\n    >>> coords = np.array([[0, 1, 2, 3, 4, 5, 6, 7]])\n    >>> indices = np.array([[2, 8, 2]]) # Start, stop, step pairs\n    >>> _filter_pairs(starts, stops, coords, indices)\n    ListType[int64]([2, 4, 6])\n    """"""\n    mask = numba.typed.List.empty_list(numba.types.intp)\n\n    # For each pair,\n    for i in range(len(starts)):\n        # For each element match within the pair range\n        for j in range(starts[i], stops[i]):\n            match = True\n\n            # Check if it matches all indices\n            for k in range(len(indices)):\n                idx = indices[k]\n                elem = coords[k, j]\n\n                match &= (elem - idx[0]) % idx[2] == 0 and (\n                    (idx[2] > 0 and idx[0] <= elem < idx[1])\n                    or (idx[2] < 0 and idx[0] >= elem > idx[1])\n                )\n\n            # and append to the mask if so.\n            if match:\n                mask.append(j)\n\n    return mask\n\n\n@numba.jit(nopython=True, nogil=True)\ndef _join_adjacent_pairs(starts_old, stops_old):  # pragma: no cover\n    """"""\n    Joins adjacent pairs into one. For example, 2-5 and 5-7\n    will reduce to 2-7 (a single pair). This may help in\n    returning a slice in the end which could be faster.\n\n    Parameters\n    ----------\n    starts_old, stops_old : list[int]\n        The input starts and stops\n\n    Returns\n    -------\n    starts, stops : list[int]\n        The reduced starts and stops.\n\n    Examples\n    --------\n    >>> starts = numba.typed.List(); starts.append(2); starts.append(5)\n    >>> stops = numba.typed.List(); stops.append(5); stops.append(7)\n    >>> _join_adjacent_pairs(starts, stops)\n    (ListType[int64]([2]), ListType[int64]([7]))\n    """"""\n    if len(starts_old) <= 1:\n        return starts_old, stops_old\n\n    starts = numba.typed.List.empty_list(numba.types.intp)\n    starts.append(starts_old[0])\n    stops = numba.typed.List.empty_list(numba.types.intp)\n\n    for i in range(1, len(starts_old)):\n        if starts_old[i] != stops_old[i - 1]:\n            starts.append(starts_old[i])\n            stops.append(stops_old[i - 1])\n\n    stops.append(stops_old[-1])\n\n    return starts, stops\n\n\n@numba.jit(nopython=True, nogil=True)\ndef array_from_list_intp(l):  # pragma: no cover\n    n = len(l)\n    a = np.empty(n, dtype=np.intp)\n\n    for i in range(n):\n        a[i] = l[i]\n\n    return a\n\n\nclass _AdvIdxInfo:\n    def __init__(self, idx, pos, length):\n        self.idx = idx\n        self.pos = pos\n        self.length = length\n'"
sparse/_coo/numba_extension.py,5,"b'""""""\nNumba support for COO objects.\n\nFor now, this just supports attribute access\n""""""\nimport numpy as np\nimport numba\nfrom numba.extending import (\n    models,\n    register_model,\n    box,\n    unbox,\n    NativeValue,\n    make_attribute_wrapper,\n    type_callable,\n)\nfrom numba.core.imputils import impl_ret_borrowed, lower_constant, lower_builtin\nfrom numba.core.typing.typeof import typeof_impl\nfrom numba.core import cgutils, types\nfrom sparse._utils import _zero_of_dtype\nimport contextlib\n\nfrom . import COO\n\n__all__ = [""COOType""]\n\n\nclass COOType(types.Type):\n    def __init__(self, data_dtype: np.dtype, coords_dtype: np.dtype, ndim: int):\n        assert isinstance(data_dtype, np.dtype)\n        assert isinstance(coords_dtype, np.dtype)\n        self.data_dtype = data_dtype\n        self.coords_dtype = coords_dtype\n        self.ndim = ndim\n        super().__init__(\n            name=""COOType[{!r}, {!r}, {!r}]"".format(\n                numba.from_dtype(data_dtype), numba.from_dtype(coords_dtype), ndim\n            )\n        )\n\n    @property\n    def key(self):\n        return self.data_dtype, self.coords_dtype, self.ndim\n\n    @property\n    def data_type(self):\n        return numba.from_dtype(self.data_dtype)[:]\n\n    @property\n    def coords_type(self):\n        return numba.from_dtype(self.coords_dtype)[:, :]\n\n    @property\n    def shape_type(self):\n        return types.UniTuple(types.int64, self.ndim)\n\n    @property\n    def fill_value_type(self):\n        return numba.from_dtype(self.data_dtype)\n\n\n@typeof_impl.register(COO)\ndef _typeof_COO(val: COO, c) -> COOType:\n    return COOType(\n        data_dtype=val.data.dtype, coords_dtype=val.coords.dtype, ndim=val.ndim\n    )\n\n\n@register_model(COOType)\nclass COOModel(models.StructModel):\n    def __init__(self, dmm, fe_type):\n        members = [\n            (""data"", fe_type.data_type),\n            (""coords"", fe_type.coords_type),\n            (""shape"", fe_type.shape_type),\n            (""fill_value"", fe_type.fill_value_type),\n        ]\n        models.StructModel.__init__(self, dmm, fe_type, members)\n\n\n@type_callable(COO)\ndef type_COO(context):\n    # TODO: accept a fill_value kwarg\n    def typer(coords, data, shape):\n        return COOType(\n            coords_dtype=numba.np.numpy_support.as_dtype(coords.dtype),\n            data_dtype=numba.np.numpy_support.as_dtype(data.dtype),\n            ndim=len(shape),\n        )\n\n    return typer\n\n\n@lower_builtin(COO, types.Any, types.Any, types.Any)\ndef impl_COO(context, builder, sig, args):\n    typ = sig.return_type\n    coords, data, shape = args\n    coo = cgutils.create_struct_proxy(typ)(context, builder)\n    coo.coords = coords\n    coo.data = data\n    coo.shape = shape\n    coo.fill_value = context.get_constant_generic(\n        builder, typ.fill_value_type, _zero_of_dtype(typ.data_dtype)\n    )\n    return impl_ret_borrowed(context, builder, sig.return_type, coo._getvalue())\n\n\n@lower_constant(COOType)\ndef lower_constant_COO(context, builder, typ, pyval):\n    coords = context.get_constant_generic(builder, typ.coords_type, pyval.coords)\n    data = context.get_constant_generic(builder, typ.data_type, pyval.data)\n    shape = context.get_constant_generic(builder, typ.shape_type, pyval.shape)\n    fill_value = context.get_constant_generic(\n        builder, typ.fill_value_type, pyval.fill_value\n    )\n    return impl_ret_borrowed(\n        context,\n        builder,\n        typ,\n        cgutils.pack_struct(builder, (data, coords, shape, fill_value)),\n    )\n\n\n@contextlib.contextmanager\ndef local_return(builder):\n    """"""\n    Create a scope which can be broken from locally.\n\n    Used as::\n\n        with local_return(c.builder) as ret:\n            with c.builder.if(abort_cond):\n                ret()\n            do_some_other_stuff\n            # no ret needed at the end, it\'s implied\n\n        stuff_that_runs_unconditionally\n    """"""\n    end_blk = builder.append_basic_block(""end"")\n\n    def return_():\n        builder.branch(end_blk)\n\n    yield return_\n    builder.branch(end_blk)\n    # make sure all remaining code goes to the next block\n    builder.position_at_end(end_blk)\n\n\ndef _unbox_native_field(typ, obj, field_name: str, c):\n    ret_ptr = cgutils.alloca_once(c.builder, c.context.get_value_type(typ))\n    is_error_ptr = cgutils.alloca_once_value(c.builder, cgutils.false_bit)\n    fail_obj = c.context.get_constant_null(typ)\n\n    with local_return(c.builder) as ret:\n        fail_blk = c.builder.append_basic_block(""fail"")\n        with c.builder.goto_block(fail_blk):\n            c.builder.store(cgutils.true_bit, is_error_ptr)\n            c.builder.store(fail_obj, ret_ptr)\n            ret()\n\n        field_obj = c.pyapi.object_getattr_string(obj, field_name)\n        with cgutils.if_unlikely(c.builder, cgutils.is_null(c.builder, field_obj)):\n            c.builder.branch(fail_blk)\n\n        field_native = c.unbox(typ, field_obj)\n        c.pyapi.decref(field_obj)\n        with cgutils.if_unlikely(c.builder, field_native.is_error):\n            c.builder.branch(fail_blk)\n\n        c.builder.store(cgutils.false_bit, is_error_ptr)\n        c.builder.store(field_native.value, ret_ptr)\n\n    return NativeValue(c.builder.load(ret_ptr), is_error=c.builder.load(is_error_ptr))\n\n\n@unbox(COOType)\ndef unbox_COO(typ: COOType, obj: COO, c) -> NativeValue:\n    ret_ptr = cgutils.alloca_once(c.builder, c.context.get_value_type(typ))\n    is_error_ptr = cgutils.alloca_once_value(c.builder, cgutils.false_bit)\n    fail_obj = c.context.get_constant_null(typ)\n\n    with local_return(c.builder) as ret:\n        fail_blk = c.builder.append_basic_block(""fail"")\n        with c.builder.goto_block(fail_blk):\n            c.builder.store(cgutils.true_bit, is_error_ptr)\n            c.builder.store(fail_obj, ret_ptr)\n            ret()\n\n        data = _unbox_native_field(typ.data_type, obj, ""data"", c)\n        with cgutils.if_unlikely(c.builder, data.is_error):\n            c.builder.branch(fail_blk)\n\n        coords = _unbox_native_field(typ.coords_type, obj, ""coords"", c)\n        with cgutils.if_unlikely(c.builder, coords.is_error):\n            c.builder.branch(fail_blk)\n\n        shape = _unbox_native_field(typ.shape_type, obj, ""shape"", c)\n        with cgutils.if_unlikely(c.builder, shape.is_error):\n            c.builder.branch(fail_blk)\n\n        fill_value = _unbox_native_field(typ.fill_value_type, obj, ""fill_value"", c)\n        with cgutils.if_unlikely(c.builder, fill_value.is_error):\n            c.builder.branch(fail_blk)\n\n        coo = cgutils.create_struct_proxy(typ)(c.context, c.builder)\n        coo.coords = coords.value\n        coo.data = data.value\n        coo.shape = shape.value\n        coo.fill_value = fill_value.value\n        c.builder.store(cgutils.false_bit, is_error_ptr)\n        c.builder.store(coo._getvalue(), ret_ptr)\n\n    return NativeValue(c.builder.load(ret_ptr), is_error=c.builder.load(is_error_ptr))\n\n\n@box(COOType)\ndef box_COO(typ: COOType, val: ""some LLVM thing"", c) -> COO:\n    ret_ptr = cgutils.alloca_once(c.builder, c.pyapi.pyobj)\n    fail_obj = c.pyapi.get_null_object()\n\n    coo = cgutils.create_struct_proxy(typ)(c.context, c.builder, value=val)\n\n    with local_return(c.builder) as ret:\n        data_obj = c.box(typ.data_type, coo.data)\n        with cgutils.if_unlikely(c.builder, cgutils.is_null(c.builder, data_obj)):\n            c.builder.store(fail_obj, ret_ptr)\n            ret()\n\n        coords_obj = c.box(typ.coords_type, coo.coords)\n        with cgutils.if_unlikely(c.builder, cgutils.is_null(c.builder, coords_obj)):\n            c.pyapi.decref(data_obj)\n            c.builder.store(fail_obj, ret_ptr)\n            ret()\n\n        shape_obj = c.box(typ.shape_type, coo.shape)\n        with cgutils.if_unlikely(c.builder, cgutils.is_null(c.builder, shape_obj)):\n            c.pyapi.decref(coords_obj)\n            c.pyapi.decref(data_obj)\n            c.builder.store(fail_obj, ret_ptr)\n            ret()\n\n        fill_value_obj = c.box(typ.fill_value_type, coo.fill_value)\n        with cgutils.if_unlikely(c.builder, cgutils.is_null(c.builder, fill_value_obj)):\n            c.pyapi.decref(shape_obj)\n            c.pyapi.decref(coords_obj)\n            c.pyapi.decref(data_obj)\n            c.builder.store(fail_obj, ret_ptr)\n            ret()\n\n        class_obj = c.pyapi.unserialize(c.pyapi.serialize_object(COO))\n        with cgutils.if_unlikely(c.builder, cgutils.is_null(c.builder, class_obj)):\n            c.pyapi.decref(shape_obj)\n            c.pyapi.decref(coords_obj)\n            c.pyapi.decref(data_obj)\n            c.pyapi.decref(fill_value_obj)\n            c.builder.store(fail_obj, ret_ptr)\n            ret()\n\n        args = c.pyapi.tuple_pack([coords_obj, data_obj, shape_obj])\n        c.pyapi.decref(shape_obj)\n        c.pyapi.decref(coords_obj)\n        c.pyapi.decref(data_obj)\n        with cgutils.if_unlikely(c.builder, cgutils.is_null(c.builder, args)):\n            c.pyapi.decref(fill_value_obj)\n            c.pyapi.decref(class_obj)\n            c.builder.store(fail_obj, ret_ptr)\n            ret()\n\n        kwargs = c.pyapi.dict_pack([(""fill_value"", fill_value_obj)])\n        c.pyapi.decref(fill_value_obj)\n        with cgutils.if_unlikely(c.builder, cgutils.is_null(c.builder, kwargs)):\n            c.pyapi.decref(class_obj)\n            c.builder.store(fail_obj, ret_ptr)\n            ret()\n\n        c.builder.store(c.pyapi.call(class_obj, args, kwargs), ret_ptr)\n        c.pyapi.decref(class_obj)\n        c.pyapi.decref(args)\n        c.pyapi.decref(kwargs)\n\n    return c.builder.load(ret_ptr)\n\n\nmake_attribute_wrapper(COOType, ""data"", ""data"")\nmake_attribute_wrapper(COOType, ""coords"", ""coords"")\nmake_attribute_wrapper(COOType, ""shape"", ""shape"")\nmake_attribute_wrapper(COOType, ""fill_value"", ""fill_value"")\n'"
sparse/_coo/umath.py,44,"b'import itertools\n\nimport numba\nimport numpy as np\nimport scipy.sparse\n\nfrom itertools import zip_longest\n\nfrom .._utils import isscalar, equivalent, _zero_of_dtype\n\n\ndef elemwise(func, *args, **kwargs):\n    """"""\n    Apply a function to any number of arguments.\n\n    Parameters\n    ----------\n    func : Callable\n        The function to apply. Must support broadcasting.\n    args : tuple, optional\n        The arguments to the function. Can be :obj:`SparseArray` objects\n        or :obj:`scipy.sparse.spmatrix` objects.\n    kwargs : dict, optional\n        Any additional arguments to pass to the function.\n\n    Returns\n    -------\n    COO\n        The result of applying the function.\n\n    Raises\n    ------\n    ValueError\n        If the operation would result in a dense matrix, or if the operands\n        don\'t have broadcastable shapes.\n\n    See Also\n    --------\n    :obj:`numpy.ufunc` : A similar Numpy construct. Note that any :code:`ufunc` can be used\n        as the :code:`func` input to this function.\n\n    Notes\n    -----\n    Previously, operations with Numpy arrays were sometimes supported. Now,\n    it is necessary to convert Numpy arrays to :obj:`COO` objects.\n    """"""\n\n    return _Elemwise(func, *args, **kwargs).get_result()\n\n\n@numba.jit(nopython=True, nogil=True)\ndef _match_arrays(a, b):  # pragma: no cover\n    """"""\n    Finds all indexes into a and b such that a[i] = b[j]. The outputs are sorted\n    in lexographical order.\n\n    Parameters\n    ----------\n    a, b : np.ndarray\n        The input 1-D arrays to match. If matching of multiple fields is\n        needed, use np.recarrays. These two arrays must be sorted.\n\n    Returns\n    -------\n    a_idx, b_idx : np.ndarray\n        The output indices of every possible pair of matching elements.\n    """"""\n    if len(a) == 0 or len(b) == 0:\n        return np.empty(0, dtype=np.uintp), np.empty(0, dtype=np.uintp)\n\n    a_ind, b_ind = [], []\n    nb = len(b)\n    ib = 0\n    match = 0\n\n    for ia, j in enumerate(a):\n        if j == b[match]:\n            ib = match\n\n        while ib < nb and j >= b[ib]:\n            if j == b[ib]:\n                a_ind.append(ia)\n                b_ind.append(ib)\n\n                if b[match] < b[ib]:\n                    match = ib\n\n            ib += 1\n\n    return np.array(a_ind, dtype=np.uintp), np.array(b_ind, dtype=np.uintp)\n\n\ndef _get_nary_broadcast_shape(*shapes):\n    """"""\n    Broadcast any number of shapes to a result shape.\n\n    Parameters\n    ----------\n    shapes : tuple[tuple[int]]\n        The shapes to broadcast.\n\n    Returns\n    -------\n    tuple[int]\n        The output shape.\n\n    Raises\n    ------\n    ValueError\n        If the input shapes cannot be broadcast to a single shape.\n    """"""\n    result_shape = ()\n\n    for shape in shapes:\n        try:\n            result_shape = _get_broadcast_shape(shape, result_shape)\n        except ValueError:\n            shapes_str = "", "".join(str(shape) for shape in shapes)\n            raise ValueError(\n                ""operands could not be broadcast together with shapes %s"" % shapes_str\n            )\n\n    return result_shape\n\n\ndef _get_broadcast_shape(shape1, shape2, is_result=False):\n    """"""\n    Get the overall broadcasted shape.\n\n    Parameters\n    ----------\n    shape1, shape2 : tuple[int]\n        The input shapes to broadcast together.\n    is_result : bool\n        Whether or not shape2 is also the result shape.\n\n    Returns\n    -------\n    result_shape : tuple[int]\n        The overall shape of the result.\n\n    Raises\n    ------\n    ValueError\n        If the two shapes cannot be broadcast together.\n    """"""\n    # https://stackoverflow.com/a/47244284/774273\n    if not all(\n        (l1 == l2) or (l1 == 1) or ((l2 == 1) and not is_result)\n        for l1, l2 in zip(shape1[::-1], shape2[::-1])\n    ):\n        raise ValueError(\n            ""operands could not be broadcast together with shapes %s, %s""\n            % (shape1, shape2)\n        )\n\n    result_shape = tuple(\n        l1 if l1 != 1 else l2\n        for l1, l2 in zip_longest(shape1[::-1], shape2[::-1], fillvalue=1)\n    )[::-1]\n\n    return result_shape\n\n\ndef _get_broadcast_parameters(shape, broadcast_shape):\n    """"""\n    Get the broadcast parameters.\n\n    Parameters\n    ----------\n    shape : tuple[int]\n        The input shape.\n    broadcast_shape\n        The shape to broadcast to.\n\n    Returns\n    -------\n    params : list\n        A list containing None if the dimension isn\'t in the original array, False if\n        it needs to be broadcast, and True if it doesn\'t.\n    """"""\n    params = [\n        None if l1 is None else l1 == l2\n        for l1, l2 in zip_longest(shape[::-1], broadcast_shape[::-1], fillvalue=None)\n    ][::-1]\n\n    return params\n\n\ndef _get_reduced_coords(coords, params):\n    """"""\n    Gets only those dimensions of the coordinates that don\'t need to be broadcast.\n\n    Parameters\n    ----------\n    coords : np.ndarray\n        The coordinates to reduce.\n    params : list\n        The params from which to check which dimensions to get.\n\n    Returns\n    -------\n    reduced_coords : np.ndarray\n        The reduced coordinates.\n    """"""\n\n    reduced_params = [bool(param) for param in params]\n\n    return coords[reduced_params]\n\n\ndef _get_reduced_shape(shape, params):\n    """"""\n    Gets only those dimensions of the coordinates that don\'t need to be broadcast.\n\n    Parameters\n    ----------\n    coords : np.ndarray\n        The coordinates to reduce.\n    params : list\n        The params from which to check which dimensions to get.\n\n    Returns\n    -------\n    reduced_coords : np.ndarray\n        The reduced coordinates.\n    """"""\n    reduced_shape = tuple(l for l, p in zip(shape, params) if p)\n\n    return reduced_shape\n\n\ndef _get_expanded_coords_data(coords, data, params, broadcast_shape):\n    """"""\n    Expand coordinates/data to broadcast_shape. Does most of the heavy lifting for broadcast_to.\n    Produces sorted output for sorted inputs.\n\n    Parameters\n    ----------\n    coords : np.ndarray\n        The coordinates to expand.\n    data : np.ndarray\n        The data corresponding to the coordinates.\n    params : list\n        The broadcast parameters.\n    broadcast_shape : tuple[int]\n        The shape to broadcast to.\n\n    Returns\n    -------\n    expanded_coords : np.ndarray\n        List of 1-D arrays. Each item in the list has one dimension of coordinates.\n    expanded_data : np.ndarray\n        The data corresponding to expanded_coords.\n    """"""\n    first_dim = -1\n    expand_shapes = []\n    for d, p, l in zip(range(len(broadcast_shape)), params, broadcast_shape):\n        if p and first_dim == -1:\n            expand_shapes.append(coords.shape[1])\n            first_dim = d\n\n        if not p:\n            expand_shapes.append(l)\n\n    all_idx = _cartesian_product(*(np.arange(d, dtype=np.intp) for d in expand_shapes))\n\n    false_dim = 0\n    dim = 0\n\n    expanded_coords = np.empty((len(broadcast_shape), all_idx.shape[1]), dtype=np.intp)\n\n    if first_dim != -1:\n        expanded_data = data[all_idx[first_dim]]\n    else:\n        expanded_coords = all_idx\n        expanded_data = np.repeat(data, np.prod(broadcast_shape, dtype=np.int64))\n        return np.asarray(expanded_coords), np.asarray(expanded_data)\n\n    for d, p, l in zip(range(len(broadcast_shape)), params, broadcast_shape):\n        if p:\n            expanded_coords[d] = coords[dim, all_idx[first_dim]]\n        else:\n            expanded_coords[d] = all_idx[false_dim + (d > first_dim)]\n            false_dim += 1\n\n        if p is not None:\n            dim += 1\n\n    return np.asarray(expanded_coords), np.asarray(expanded_data)\n\n\n# (c) senderle\n# Taken from https://stackoverflow.com/a/11146645/774273\n# License: https://creativecommons.org/licenses/by-sa/3.0/\ndef _cartesian_product(*arrays):\n    """"""\n    Get the cartesian product of a number of arrays.\n\n    Parameters\n    ----------\n    arrays : Tuple[np.ndarray]\n        The arrays to get a cartesian product of. Always sorted with respect\n        to the original array.\n\n    Returns\n    -------\n    out : np.ndarray\n        The overall cartesian product of all the input arrays.\n    """"""\n    broadcastable = np.ix_(*arrays)\n    broadcasted = np.broadcast_arrays(*broadcastable)\n    rows, cols = np.prod(broadcasted[0].shape), len(broadcasted)\n    dtype = np.result_type(*arrays)\n    out = np.empty(rows * cols, dtype=dtype)\n    start, end = 0, rows\n    for a in broadcasted:\n        out[start:end] = a.reshape(-1)\n        start, end = end, end + rows\n    return out.reshape(cols, rows)\n\n\ndef _get_matching_coords(coords, params):\n    """"""\n    Get the matching coords across a number of broadcast operands.\n\n    Parameters\n    ----------\n    coords : list[numpy.ndarray]\n        The input coordinates.\n    params : list[Union[bool, none]]\n        The broadcast parameters.\n    Returns\n    -------\n    numpy.ndarray\n        The broacasted coordinates\n    """"""\n    matching_coords = []\n    dims = np.zeros(len(coords), dtype=np.uint8)\n\n    for p_all in zip(*params):\n        for i, p in enumerate(p_all):\n            if p:\n                matching_coords.append(coords[i][dims[i]])\n                break\n        else:\n            matching_coords.append(coords[dims[0]])\n\n        for i, p in enumerate(p_all):\n            if p is not None:\n                dims[i] += 1\n\n    return np.asarray(matching_coords, dtype=np.intp)\n\n\ndef broadcast_to(x, shape):\n    """"""\n    Performs the equivalent of :obj:`numpy.broadcast_to` for :obj:`COO`. Note that\n    this function returns a new array instead of a view.\n\n    Parameters\n    ----------\n    shape : tuple[int]\n        The shape to broadcast the data to.\n\n    Returns\n    -------\n    COO\n        The broadcasted sparse array.\n\n    Raises\n    ------\n    ValueError\n        If the operand cannot be broadcast to the given shape.\n\n    See also\n    --------\n    :obj:`numpy.broadcast_to` : NumPy equivalent function\n    """"""\n    from .core import COO\n\n    if shape == x.shape:\n        return x\n\n    result_shape = _get_broadcast_shape(x.shape, shape, is_result=True)\n    params = _get_broadcast_parameters(x.shape, result_shape)\n    coords, data = _get_expanded_coords_data(x.coords, x.data, params, result_shape)\n\n    # Check if all the non-broadcast axes are next to each other\n    nonbroadcast_idx = [idx for idx, p in enumerate(params) if p]\n    diff_nonbroadcast_idx = [\n        a - b for a, b in zip(nonbroadcast_idx[1:], nonbroadcast_idx[:-1])\n    ]\n    sorted = all(d == 1 for d in diff_nonbroadcast_idx)\n\n    return COO(\n        coords,\n        data,\n        shape=result_shape,\n        has_duplicates=False,\n        sorted=sorted,\n        fill_value=x.fill_value,\n    )\n\n\nclass _Elemwise:\n    def __init__(self, func, *args, **kwargs):\n        """"""\n        Initialize the element-wise function calculator.\n\n        Parameters\n        ----------\n        func : types.Callable\n            The function to compute\n        args : tuple[Union[SparseArray, ndarray, scipy.sparse.spmatrix]]\n            The arguments to compute the function on.\n        kwargs : dict\n            Extra arguments to pass to the function.\n        """"""\n        from .core import COO\n        from .._sparse_array import SparseArray\n\n        processed_args = []\n\n        for arg in args:\n            if isinstance(arg, scipy.sparse.spmatrix):\n                processed_args.append(COO.from_scipy_sparse(arg))\n            elif isscalar(arg) or isinstance(arg, np.ndarray):\n                # Faster and more reliable to pass ()-shaped ndarrays as scalars.\n                processed_args.append(np.asarray(arg))\n            elif isinstance(arg, SparseArray) and not isinstance(arg, COO):\n                processed_args.append(COO(arg))\n            elif not isinstance(arg, COO):\n                self.args = None\n                return\n            else:\n                processed_args.append(arg)\n\n        self.args = tuple(processed_args)\n        self.func = func\n        self.dtype = kwargs.pop(""dtype"", None)\n        self.kwargs = kwargs\n        self.cache = {}\n\n        self._check_broadcast()\n        self._get_fill_value()\n\n    def get_result(self):\n        from .core import COO\n\n        if self.args is None:\n            return NotImplemented\n\n        if self.shape == self.ndarray_shape:\n            args = [a.todense() if isinstance(a, COO) else a for a in self.args]\n            return self.func(*args, **self.kwargs)\n\n        if any(s == 0 for s in self.shape):\n            data = np.empty((0,), dtype=self.fill_value.dtype)\n            coords = np.empty((0, len(self.shape)), dtype=np.intp)\n            return COO(\n                coords,\n                data,\n                shape=self.shape,\n                has_duplicates=False,\n                fill_value=self.fill_value,\n            )\n\n        data_list = []\n        coords_list = []\n\n        for mask in itertools.product(\n            *[[True, False] if isinstance(arg, COO) else [None] for arg in self.args]\n        ):\n            if not any(mask):\n                continue\n\n            r = self._get_func_coords_data(mask)\n\n            if r is not None:\n                coords_list.append(r[0])\n                data_list.append(r[1])\n\n        # Concatenate matches and mismatches\n        data = (\n            np.concatenate(data_list)\n            if len(data_list)\n            else np.empty((0,), dtype=self.fill_value.dtype)\n        )\n        coords = (\n            np.concatenate(coords_list, axis=1)\n            if len(coords_list)\n            else np.empty((0, len(self.shape)), dtype=np.intp)\n        )\n\n        return COO(\n            coords,\n            data,\n            shape=self.shape,\n            has_duplicates=False,\n            fill_value=self.fill_value,\n        )\n\n    def _get_fill_value(self):\n        """"""\n        A function that finds and returns the fill-value.\n\n        Raises\n        ------\n        ValueError\n            If the fill-value is inconsistent.\n        """"""\n        from .core import COO\n\n        zero_args = tuple(\n            arg.fill_value[...] if isinstance(arg, COO) else arg for arg in self.args\n        )\n\n        # Some elemwise functions require a dtype argument, some abhorr it.\n        try:\n            fill_value_array = self.func(*zero_args, dtype=self.dtype, **self.kwargs)\n            self.dtype = None\n        except TypeError:\n            fill_value_array = self.func(*zero_args, **self.kwargs)\n\n        try:\n            fill_value = fill_value_array[(0,) * fill_value_array.ndim]\n        except IndexError:\n            zero_args = tuple(\n                arg.fill_value if isinstance(arg, COO) else _zero_of_dtype(arg.dtype)\n                for arg in self.args\n            )\n            fill_value = self.func(*zero_args, **self.kwargs)[()]\n\n        if (\n            not equivalent(fill_value, fill_value_array).all()\n            and self.shape != self.ndarray_shape\n        ):\n            raise ValueError(\n                ""Performing a mixed sparse-dense operation that would result in a dense array. ""\n                ""Please make sure that func(sparse_fill_values, ndarrays) is a constant array.""\n            )\n\n        # Store dtype separately if needed.\n        if self.dtype is not None:\n            fill_value = fill_value.astype(self.dtype)\n\n        self.fill_value = fill_value\n        self.dtype = self.fill_value.dtype\n\n    def _check_broadcast(self):\n        """"""\n        Checks if adding the ndarrays changes the broadcast shape.\n\n        Raises\n        ------\n        ValueError\n            If the check fails.\n        """"""\n        from .core import COO\n\n        full_shape = _get_nary_broadcast_shape(*tuple(arg.shape for arg in self.args))\n        non_ndarray_shape = _get_nary_broadcast_shape(\n            *tuple(arg.shape for arg in self.args if isinstance(arg, COO))\n        )\n        ndarray_shape = _get_nary_broadcast_shape(\n            *tuple(arg.shape for arg in self.args if isinstance(arg, np.ndarray))\n        )\n\n        self.shape = full_shape\n        self.ndarray_shape = ndarray_shape\n        self.non_ndarray_shape = non_ndarray_shape\n\n    def _get_func_coords_data(self, mask):\n        """"""\n        Gets the coords/data for a certain mask\n\n        Parameters\n        ----------\n        mask : tuple[Union[bool, NoneType]]\n            The mask determining whether to match or unmatch.\n\n        Returns\n        -------\n        None or tuple\n            The coords/data tuple for the given mask.\n        """"""\n        from .core import COO\n\n        matched_args = [arg for arg, m in zip(self.args, mask) if m is not None and m]\n        unmatched_args = [\n            arg for arg, m in zip(self.args, mask) if m is not None and not m\n        ]\n        ndarray_args = [arg for arg, m in zip(self.args, mask) if m is None]\n\n        matched_broadcast_shape = _get_nary_broadcast_shape(\n            *tuple(arg.shape for arg in itertools.chain(matched_args, ndarray_args))\n        )\n\n        matched_arrays = self._match_coo(\n            *matched_args, cache=self.cache, broadcast_shape=matched_broadcast_shape\n        )\n\n        func_args = []\n\n        m_arg = 0\n        for arg, m in zip(self.args, mask):\n            if m is None:\n                func_args.append(\n                    np.broadcast_to(arg, matched_broadcast_shape)[\n                        tuple(matched_arrays[0].coords)\n                    ]\n                )\n                continue\n\n            if m:\n                func_args.append(matched_arrays[m_arg].data)\n                m_arg += 1\n            else:\n                func_args.append(arg.fill_value)\n\n        # Try our best to preserve the output dtype.\n        try:\n            func_data = self.func(*func_args, dtype=self.dtype, **self.kwargs)\n        except TypeError:\n            try:\n                func_args = np.broadcast_arrays(*func_args)\n                out = np.empty(func_args[0].shape, dtype=self.dtype)\n                func_data = self.func(*func_args, out=out, **self.kwargs)\n            except TypeError:\n                func_data = self.func(*func_args, **self.kwargs).astype(self.dtype)\n\n        unmatched_mask = ~equivalent(func_data, self.fill_value)\n\n        if not unmatched_mask.any():\n            return None\n\n        func_coords = matched_arrays[0].coords[:, unmatched_mask]\n        func_data = func_data[unmatched_mask]\n\n        if matched_arrays[0].shape != self.shape:\n            params = _get_broadcast_parameters(matched_arrays[0].shape, self.shape)\n            func_coords, func_data = _get_expanded_coords_data(\n                func_coords, func_data, params, self.shape\n            )\n\n        if all(m is None or m for m in mask):\n            return func_coords, func_data\n\n        # Not really sorted but we need the sortedness.\n        func_array = COO(\n            func_coords, func_data, self.shape, has_duplicates=False, sorted=True\n        )\n\n        unmatched_mask = np.ones(func_array.nnz, dtype=np.bool)\n\n        for arg in unmatched_args:\n            matched_idx = self._match_coo(func_array, arg, return_midx=True)[0]\n            unmatched_mask[matched_idx] = False\n\n        coords = np.asarray(func_array.coords[:, unmatched_mask], order=""C"")\n        data = np.asarray(func_array.data[unmatched_mask], order=""C"")\n\n        return coords, data\n\n    @staticmethod\n    def _match_coo(*args, **kwargs):\n        """"""\n        Matches the coordinates for any number of input :obj:`COO` arrays.\n        Equivalent to ""sparse"" broadcasting for all arrays.\n\n        Parameters\n        ----------\n        args : Tuple[COO]\n            The input :obj:`COO` arrays.\n        return_midx : bool\n            Whether to return matched indices or matched arrays. Matching\n            only supported for two arrays. ``False`` by default.\n        cache : dict\n            Cache of things already matched. No cache by default.\n\n        Returns\n        -------\n        matched_idx : List[ndarray]\n            The indices of matched elements in the original arrays. Only returned if\n            ``return_midx`` is ``True``.\n        matched_arrays : List[COO]\n            The expanded, matched :obj:`COO` objects. Only returned if\n            ``return_midx`` is ``False``.\n        """"""\n        from .core import COO\n        from .common import linear_loc\n\n        cache = kwargs.pop(""cache"", None)\n        return_midx = kwargs.pop(""return_midx"", False)\n        broadcast_shape = kwargs.pop(""broadcast_shape"", None)\n\n        if kwargs:\n            raise ValueError(""Unknown kwargs: {}"".format(kwargs.keys()))\n\n        if return_midx and (len(args) != 2 or cache is not None):\n            raise NotImplementedError(\n                ""Matching indices only supported for two args, and no cache.""\n            )\n\n        matched_arrays = [args[0]]\n        cache_key = [id(args[0])]\n        for arg2 in args[1:]:\n            cache_key.append(id(arg2))\n            key = tuple(cache_key)\n            if cache is not None and key in cache:\n                matched_arrays = cache[key]\n                continue\n\n            cargs = [matched_arrays[0], arg2]\n            current_shape = _get_broadcast_shape(matched_arrays[0].shape, arg2.shape)\n            params = [\n                _get_broadcast_parameters(arg.shape, current_shape) for arg in cargs\n            ]\n            reduced_params = [all(p) for p in zip(*params)]\n            reduced_shape = _get_reduced_shape(\n                arg2.shape, _rev_idx(reduced_params, arg2.ndim)\n            )\n\n            reduced_coords = [\n                _get_reduced_coords(arg.coords, _rev_idx(reduced_params, arg.ndim))\n                for arg in cargs\n            ]\n\n            linear = [linear_loc(rc, reduced_shape) for rc in reduced_coords]\n            sorted_idx = [np.argsort(idx) for idx in linear]\n            linear = [idx[s] for idx, s in zip(linear, sorted_idx)]\n            matched_idx = _match_arrays(*linear)\n\n            if return_midx:\n                matched_idx = [\n                    sidx[midx] for sidx, midx in zip(sorted_idx, matched_idx)\n                ]\n                return matched_idx\n\n            coords = [arg.coords[:, s] for arg, s in zip(cargs, sorted_idx)]\n            mcoords = [c[:, idx] for c, idx in zip(coords, matched_idx)]\n            mcoords = _get_matching_coords(mcoords, params)\n            mdata = [arg.data[sorted_idx[0]][matched_idx[0]] for arg in matched_arrays]\n            mdata.append(arg2.data[sorted_idx[1]][matched_idx[1]])\n            # The coords aren\'t truly sorted, but we don\'t need them, so it\'s\n            # best to avoid the extra cost.\n            matched_arrays = [\n                COO(mcoords, md, shape=current_shape, sorted=True, has_duplicates=False)\n                for md in mdata\n            ]\n\n            if cache is not None:\n                cache[key] = matched_arrays\n\n        if broadcast_shape is not None and matched_arrays[0].shape != broadcast_shape:\n            params = _get_broadcast_parameters(matched_arrays[0].shape, broadcast_shape)\n            coords, idx = _get_expanded_coords_data(\n                matched_arrays[0].coords,\n                np.arange(matched_arrays[0].nnz),\n                params,\n                broadcast_shape,\n            )\n\n            matched_arrays = [\n                COO(\n                    coords,\n                    arr.data[idx],\n                    shape=broadcast_shape,\n                    sorted=True,\n                    has_duplicates=False,\n                )\n                for arr in matched_arrays\n            ]\n\n        return matched_arrays\n\n\ndef _rev_idx(arg, idx):\n    if idx == 0:\n        return arg[len(arg) :]\n\n    return arg[-idx:]\n'"
sparse/tests/conftest.py,0,"b'import platform\n\n\ndef pytest_cmdline_preparse(args):\n    if platform.system() != ""Windows"":\n        args.append(""--doctest-modules"")\n'"
sparse/tests/test_array_function.py,12,"b'import sparse\nfrom sparse._settings import NEP18_ENABLED\nfrom sparse._utils import assert_eq\nimport numpy as np\nimport pytest\n\n\nif not NEP18_ENABLED:\n    pytest.skip(""NEP18 is not enabled"", allow_module_level=True)\n\n\n@pytest.mark.parametrize(\n    ""func"",\n    [\n        np.mean,\n        np.std,\n        np.var,\n        np.sum,\n        lambda x: np.sum(x, axis=0),\n        lambda x: np.transpose(x),\n    ],\n)\ndef test_unary(func):\n    y = sparse.random((50, 50), density=0.25)\n    x = y.todense()\n    xx = func(x)\n    yy = func(y)\n    assert_eq(xx, yy)\n\n\n@pytest.mark.parametrize(""arg_order"", [(0, 1), (1, 0), (1, 1)])\n@pytest.mark.parametrize(""func"", [np.dot, np.result_type, np.tensordot, np.matmul])\ndef test_binary(func, arg_order):\n    y = sparse.random((50, 50), density=0.25)\n    x = y.todense()\n    xx = func(x, x)\n    args = [(x, y)[i] for i in arg_order]\n    yy = func(*args)\n\n    if isinstance(xx, np.ndarray):\n        assert_eq(xx, yy)\n    else:\n        # result_type returns a dtype\n        assert xx == yy\n\n\ndef test_stack():\n    """"""stack(), by design, does not allow for mixed type inputs\n    """"""\n    y = sparse.random((50, 50), density=0.25)\n    x = y.todense()\n    xx = np.stack([x, x])\n    yy = np.stack([y, y])\n    assert_eq(xx, yy)\n\n\n@pytest.mark.parametrize(\n    ""arg_order"",\n    [(0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)],\n)\n@pytest.mark.parametrize(""func"", [lambda a, b, c: np.where(a.astype(bool), b, c)])\ndef test_ternary(func, arg_order):\n    y = sparse.random((50, 50), density=0.25)\n    x = y.todense()\n    xx = func(x, x, x)\n    args = [(x, y)[i] for i in arg_order]\n    yy = func(*args)\n    assert_eq(xx, yy)\n\n\n@pytest.mark.parametrize(""func"", [np.shape, np.size, np.ndim])\ndef test_property(func):\n    y = sparse.random((50, 50), density=0.25)\n    x = y.todense()\n    xx = func(x)\n    yy = func(y)\n    assert xx == yy\n'"
sparse/tests/test_compressed.py,5,"b'import sparse\nimport pytest\nimport numpy as np\nimport scipy\n\nfrom sparse._compressed import GCXS\nfrom sparse._utils import assert_eq\n\n\n@pytest.mark.parametrize(\n    ""a,b"",\n    [\n        [(3, 4), (5, 5)],\n        [(12,), (3, 4)],\n        [(12,), (3, 6)],\n        [(5, 5, 5), (6, 6, 6)],\n        [(3, 4), (9, 4)],\n        [(5,), (4,)],\n        [(2, 3, 4, 5), (2, 3, 4, 5, 6)],\n        [(100,), (5, 5)],\n        [(2, 3, 4, 5), (20, 6)],\n        [(), ()],\n    ],\n)\ndef test_resize(a, b):\n    s = sparse.random(a, density=0.5, format=""gcxs"")\n    orig_size = s.size\n    x = s.todense()\n    x = np.resize(x, b)\n    s.resize(b)\n    temp = x.reshape(x.size)\n    temp[orig_size:] = s.fill_value\n    assert isinstance(s, sparse.SparseArray)\n    assert_eq(x, s)\n\n\n@pytest.mark.parametrize(\n    ""a,b"",\n    [\n        [(3, 4), (3, 4)],\n        [(12,), (3, 4)],\n        [(12,), (3, -1)],\n        [(3, 4), (12,)],\n        [(3, 4), (-1, 4)],\n        [(3, 4), (3, -1)],\n        [(2, 3, 4, 5), (8, 15)],\n        [(2, 3, 4, 5), (24, 5)],\n        [(2, 3, 4, 5), (20, 6)],\n        [(), ()],\n    ],\n)\ndef test_reshape(a, b):\n    s = sparse.random(a, density=0.5, format=""gcxs"")\n    x = s.todense()\n\n    assert_eq(x.reshape(b), s.reshape(b))\n\n\ndef test_reshape_same():\n\n    s = sparse.random((3, 5), density=0.5, format=""gcxs"")\n    assert s.reshape(s.shape) is s\n\n\ndef test_to_scipy_sparse():\n    s = sparse.random((3, 5), density=0.5, format=""gcxs"", compressed_axes=(0,))\n    a = s.to_scipy_sparse()\n    b = scipy.sparse.csr_matrix(s.todense())\n\n    assert_eq(a, b)\n\n    s = sparse.random((3, 5), density=0.5, format=""gcxs"", compressed_axes=(1,))\n    a = s.to_scipy_sparse()\n    b = scipy.sparse.csc_matrix(s.todense())\n\n    assert_eq(a, b)\n\n\ndef test_tocoo():\n    coo = sparse.random((5, 6), density=0.5)\n    b = GCXS.from_coo(coo)\n    assert_eq(b.tocoo(), coo)\n\n\n@pytest.mark.parametrize(\n    ""index"",\n    [\n        # Integer\n        0,\n        1,\n        -1,\n        (1, 1, 1),\n        # Pure slices\n        (slice(0, 2),),\n        (slice(None, 2), slice(None, 2)),\n        (slice(1, None), slice(1, None)),\n        (slice(None, None),),\n        (slice(None, None, -1),),\n        (slice(None, 2, -1), slice(None, 2, -1)),\n        (slice(1, None, 2), slice(1, None, 2)),\n        (slice(None, None, 2),),\n        (slice(None, 2, -1), slice(None, 2, -2)),\n        (slice(1, None, 2), slice(1, None, 1)),\n        (slice(None, None, -2),),\n        # Combinations\n        (0, slice(0, 2)),\n        (slice(0, 1), 0),\n        (None, slice(1, 3), 0),\n        (slice(0, 3), None, 0),\n        (slice(1, 2), slice(2, 4)),\n        (slice(1, 2), slice(None, None)),\n        (slice(1, 2), slice(None, None), 2),\n        (slice(1, 2, 2), slice(None, None), 2),\n        (slice(1, 2, None), slice(None, None, 2), 2),\n        (slice(1, 2, -2), slice(None, None), -2),\n        (slice(1, 2, None), slice(None, None, -2), 2),\n        (slice(1, 2, -1), slice(None, None), -1),\n        (slice(1, 2, None), slice(None, None, -1), 2),\n        (slice(2, 0, -1), slice(None, None), -1),\n        (slice(-2, None, None),),\n        (slice(-1, None, None), slice(-2, None, None)),\n        # With ellipsis\n        (Ellipsis, slice(1, 3)),\n        (1, Ellipsis, slice(1, 3)),\n        (slice(0, 1), Ellipsis),\n        (Ellipsis, None),\n        (None, Ellipsis),\n        (1, Ellipsis),\n        (1, Ellipsis, None),\n        (1, 1, 1, Ellipsis),\n        (Ellipsis, 1, None),\n        # Pathological - Slices larger than array\n        (slice(None, 1000)),\n        (slice(None), slice(None, 1000)),\n        (slice(None), slice(1000, -1000, -1)),\n        (slice(None), slice(1000, -1000, -50)),\n        # Pathological - Wrong ordering of start/stop\n        (slice(5, 0),),\n        (slice(0, 5, -1),),\n    ],\n)\n@pytest.mark.parametrize(""compressed_axes"", [(0,), (1,), (2,), (0, 1), (0, 2), (1, 2)])\ndef test_slicing(index, compressed_axes):\n    s = sparse.random(\n        (2, 3, 4), density=0.5, format=""gcxs"", compressed_axes=compressed_axes\n    )\n    x = s.todense()\n    assert_eq(x[index], s[index])\n\n\n@pytest.mark.parametrize(\n    ""index"",\n    [\n        ([1, 0], 0),\n        (1, [0, 2]),\n        (0, [1, 0], 0),\n        (1, [2, 0], 0),\n        ([True, False], slice(1, None), slice(-2, None)),\n        (slice(1, None), slice(-2, None), [True, False, True, False]),\n        ([1, 0],),\n        (Ellipsis, [2, 1, 3]),\n        (slice(None), [2, 1, 2]),\n        (1, [2, 0, 1]),\n    ],\n)\n@pytest.mark.parametrize(""compressed_axes"", [(0,), (1,), (2,), (0, 1), (0, 2), (1, 2)])\ndef test_advanced_indexing(index, compressed_axes):\n    s = sparse.random(\n        (2, 3, 4), density=0.5, format=""gcxs"", compressed_axes=compressed_axes\n    )\n    x = s.todense()\n\n    assert_eq(x[index], s[index])\n\n\n@pytest.mark.parametrize(\n    ""index"",\n    [\n        (Ellipsis, Ellipsis),\n        (1, 1, 1, 1),\n        (slice(None),) * 4,\n        5,\n        -5,\n        ""foo"",\n        [True, False, False],\n        0.5,\n        [0.5],\n        {""potato"": ""kartoffel""},\n        ([[0, 1]],),\n    ],\n)\ndef test_slicing_errors(index):\n    s = sparse.random((2, 3, 4), density=0.5, format=""gcxs"")\n\n    with pytest.raises(IndexError):\n        s[index]\n\n\ndef test_change_compressed_axes():\n    coo = sparse.random((3, 4, 5), density=0.5)\n    s = GCXS.from_coo(coo, compressed_axes=(0, 1))\n    b = GCXS.from_coo(coo, compressed_axes=(1, 2))\n    assert_eq(s, b)\n    s.change_compressed_axes((1, 2))\n    assert_eq(s, b)\n\n\ndef test_concatenate():\n    xx = sparse.random((2, 3, 4), density=0.5, format=""gcxs"")\n    x = xx.todense()\n    yy = sparse.random((5, 3, 4), density=0.5, format=""gcxs"")\n    y = yy.todense()\n    zz = sparse.random((4, 3, 4), density=0.5, format=""gcxs"")\n    z = zz.todense()\n\n    assert_eq(\n        np.concatenate([x, y, z], axis=0), sparse.concatenate([xx, yy, zz], axis=0)\n    )\n\n    xx = sparse.random((5, 3, 1), density=0.5, format=""gcxs"")\n    x = xx.todense()\n    yy = sparse.random((5, 3, 3), density=0.5, format=""gcxs"")\n    y = yy.todense()\n    zz = sparse.random((5, 3, 2), density=0.5, format=""gcxs"")\n    z = zz.todense()\n\n    assert_eq(\n        np.concatenate([x, y, z], axis=2), sparse.concatenate([xx, yy, zz], axis=2)\n    )\n\n    assert_eq(\n        np.concatenate([x, y, z], axis=-1), sparse.concatenate([xx, yy, zz], axis=-1)\n    )\n\n\n@pytest.mark.parametrize(""axis"", [0, 1])\n@pytest.mark.parametrize(""func"", [sparse.stack, sparse.concatenate])\ndef test_concatenate_mixed(func, axis):\n    s = sparse.random((10, 10), density=0.5, format=""gcxs"")\n    d = s.todense()\n\n    with pytest.raises(ValueError):\n        func([d, s, s], axis=axis)\n\n\ndef test_concatenate_noarrays():\n    with pytest.raises(ValueError):\n        sparse.concatenate([])\n\n\n@pytest.mark.parametrize(""shape"", [(5,), (2, 3, 4), (5, 2)])\n@pytest.mark.parametrize(""axis"", [0, 1, -1])\ndef test_stack(shape, axis):\n    xx = sparse.random(shape, density=0.5, format=""gcxs"")\n    x = xx.todense()\n    yy = sparse.random(shape, density=0.5, format=""gcxs"")\n    y = yy.todense()\n    zz = sparse.random(shape, density=0.5, format=""gcxs"")\n    z = zz.todense()\n\n    assert_eq(np.stack([x, y, z], axis=axis), sparse.stack([xx, yy, zz], axis=axis))\n'"
sparse/tests/test_coo.py,221,"b'import contextlib\nimport operator\nimport pickle\nimport sys\n\nimport numpy as np\nimport pytest\nimport scipy.sparse\nimport scipy.stats\n\nimport sparse\nfrom sparse import COO\nfrom sparse._settings import NEP18_ENABLED\nfrom sparse._utils import assert_eq, random_value_array\n\n\n@pytest.fixture(scope=""module"", params=[""f8"", ""f4"", ""i8"", ""i4""])\ndef random_sparse(request):\n    dtype = request.param\n    if np.issubdtype(dtype, np.integer):\n\n        def data_rvs(n):\n            return np.random.randint(-1000, 1000, n)\n\n    else:\n        data_rvs = None\n    return sparse.random((20, 30, 40), density=0.25, data_rvs=data_rvs).astype(dtype)\n\n\n@pytest.fixture(scope=""module"", params=[""f8"", ""f4"", ""i8"", ""i4""])\ndef random_sparse_small(request):\n    dtype = request.param\n    if np.issubdtype(dtype, np.integer):\n\n        def data_rvs(n):\n            return np.random.randint(-10, 10, n)\n\n    else:\n        data_rvs = None\n    return sparse.random((20, 30, 40), density=0.25, data_rvs=data_rvs).astype(dtype)\n\n\n@pytest.mark.parametrize(\n    ""reduction, kwargs"", [(""sum"", {}), (""sum"", {""dtype"": np.float32}), (""prod"", {})]\n)\n@pytest.mark.parametrize(""axis"", [None, 0, 1, 2, (0, 2), -3, (1, -1)])\n@pytest.mark.parametrize(""keepdims"", [True, False])\ndef test_reductions_fv(reduction, random_sparse_small, axis, keepdims, kwargs):\n    x = random_sparse_small + np.random.randint(-1, 1, dtype=""i4"")\n    y = x.todense()\n    xx = getattr(x, reduction)(axis=axis, keepdims=keepdims, **kwargs)\n    yy = getattr(y, reduction)(axis=axis, keepdims=keepdims, **kwargs)\n    assert_eq(xx, yy)\n\n\n@pytest.mark.parametrize(\n    ""reduction, kwargs"",\n    [\n        (""sum"", {}),\n        (""sum"", {""dtype"": np.float32}),\n        (""mean"", {}),\n        (""mean"", {""dtype"": np.float32}),\n        (""prod"", {}),\n        (""max"", {}),\n        (""min"", {}),\n        (""std"", {}),\n        (""var"", {}),\n    ],\n)\n@pytest.mark.parametrize(""axis"", [None, 0, 1, 2, (0, 2), -3, (1, -1)])\n@pytest.mark.parametrize(""keepdims"", [True, False])\ndef test_reductions(reduction, random_sparse, axis, keepdims, kwargs):\n    x = random_sparse\n    y = x.todense()\n    xx = getattr(x, reduction)(axis=axis, keepdims=keepdims, **kwargs)\n    yy = getattr(y, reduction)(axis=axis, keepdims=keepdims, **kwargs)\n    assert_eq(xx, yy)\n\n\n@pytest.mark.xfail(\n    reason=(""Setting output dtype=float16 produces results "" ""inconsistent with numpy"")\n)\n@pytest.mark.filterwarnings(""ignore:overflow"")\n@pytest.mark.parametrize(\n    ""reduction, kwargs"",\n    [(""sum"", {""dtype"": np.float16}), (""mean"", {""dtype"": np.float16})],\n)\n@pytest.mark.parametrize(""axis"", [None, 0, 1, 2, (0, 2)])\ndef test_reductions_float16(random_sparse, reduction, kwargs, axis):\n    x = random_sparse\n    y = x.todense()\n    xx = getattr(x, reduction)(axis=axis, **kwargs)\n    yy = getattr(y, reduction)(axis=axis, **kwargs)\n    assert_eq(xx, yy, atol=1e-2)\n\n\n@pytest.mark.parametrize(""reduction,kwargs"", [(""any"", {}), (""all"", {})])\n@pytest.mark.parametrize(""axis"", [None, 0, 1, 2, (0, 2), -3, (1, -1)])\n@pytest.mark.parametrize(""keepdims"", [True, False])\ndef test_reductions_bool(random_sparse, reduction, kwargs, axis, keepdims):\n    y = np.zeros((2, 3, 4), dtype=bool)\n    y[0] = True\n    y[1, 1, 1] = True\n    x = sparse.COO.from_numpy(y)\n    xx = getattr(x, reduction)(axis=axis, keepdims=keepdims, **kwargs)\n    yy = getattr(y, reduction)(axis=axis, keepdims=keepdims, **kwargs)\n    assert_eq(xx, yy)\n\n\n@pytest.mark.parametrize(\n    ""reduction,kwargs"",\n    [\n        (np.max, {}),\n        (np.sum, {}),\n        (np.sum, {""dtype"": np.float32}),\n        (np.mean, {}),\n        (np.mean, {""dtype"": np.float32}),\n        (np.prod, {}),\n        (np.min, {}),\n    ],\n)\n@pytest.mark.parametrize(""axis"", [None, 0, 1, 2, (0, 2), -1, (0, -1)])\n@pytest.mark.parametrize(""keepdims"", [True, False])\ndef test_ufunc_reductions(random_sparse, reduction, kwargs, axis, keepdims):\n    x = random_sparse\n    y = x.todense()\n    xx = reduction(x, axis=axis, keepdims=keepdims, **kwargs)\n    yy = reduction(y, axis=axis, keepdims=keepdims, **kwargs)\n    assert_eq(xx, yy)\n    # If not a scalar/1 element array, must be a sparse array\n    if xx.size > 1:\n        assert isinstance(xx, COO)\n\n\n@pytest.mark.parametrize(\n    ""reduction,kwargs"",\n    [\n        (np.max, {}),\n        (np.sum, {""axis"": 0}),\n        (np.prod, {""keepdims"": True}),\n        (np.add.reduce, {}),\n        (np.add.reduce, {""keepdims"": True}),\n        (np.minimum.reduce, {""axis"": 0}),\n    ],\n)\ndef test_ufunc_reductions_kwargs(reduction, kwargs):\n    x = sparse.random((2, 3, 4), density=0.5)\n    y = x.todense()\n    xx = reduction(x, **kwargs)\n    yy = reduction(y, **kwargs)\n    assert_eq(xx, yy)\n    # If not a scalar/1 element array, must be a sparse array\n    if xx.size > 1:\n        assert isinstance(xx, COO)\n\n\n@pytest.mark.parametrize(\n    ""reduction"", [""nansum"", ""nanmean"", ""nanprod"", ""nanmax"", ""nanmin""]\n)\n@pytest.mark.parametrize(""axis"", [None, 0, 1])\n@pytest.mark.parametrize(""keepdims"", [False])\n@pytest.mark.parametrize(""fraction"", [0.25, 0.5, 0.75, 1.0])\n@pytest.mark.filterwarnings(""ignore:All-NaN"")\n@pytest.mark.filterwarnings(""ignore:Mean of empty slice"")\ndef test_nan_reductions(reduction, axis, keepdims, fraction):\n    s = sparse.random(\n        (2, 3, 4), data_rvs=random_value_array(np.nan, fraction), density=0.25\n    )\n    x = s.todense()\n    expected = getattr(np, reduction)(x, axis=axis, keepdims=keepdims)\n    actual = getattr(sparse, reduction)(s, axis=axis, keepdims=keepdims)\n    assert_eq(expected, actual)\n\n\n@pytest.mark.parametrize(""reduction"", [""nanmax"", ""nanmin"", ""nanmean""])\n@pytest.mark.parametrize(""axis"", [None, 0, 1])\ndef test_all_nan_reduction_warning(reduction, axis):\n    x = random_value_array(np.nan, 1.0)(2 * 3 * 4).reshape(2, 3, 4)\n    s = COO.from_numpy(x)\n\n    with pytest.warns(RuntimeWarning):\n        getattr(sparse, reduction)(s, axis=axis)\n\n\n@pytest.mark.parametrize(\n    ""axis"",\n    [None, (1, 2, 0), (2, 1, 0), (0, 1, 2), (0, 1, -1), (0, -2, -1), (-3, -2, -1)],\n)\ndef test_transpose(axis):\n    x = sparse.random((2, 3, 4), density=0.25)\n    y = x.todense()\n    xx = x.transpose(axis)\n    yy = y.transpose(axis)\n    assert_eq(xx, yy)\n\n\n@pytest.mark.parametrize(\n    ""axis"",\n    [\n        (0, 1),  # too few\n        (0, 1, 2, 3),  # too many\n        (3, 1, 0),  # axis 3 illegal\n        (0, -1, -4),  # axis -4 illegal\n        (0, 0, 1),  # duplicate axis 0\n        (0, -1, 2),  # duplicate axis -1 == 2\n        0.3,  # Invalid type in axis\n        ((0, 1, 2),),  # Iterable inside iterable\n    ],\n)\ndef test_transpose_error(axis):\n    x = sparse.random((2, 3, 4), density=0.25)\n\n    with pytest.raises(ValueError):\n        x.transpose(axis)\n\n\n@pytest.mark.parametrize(""axis1"", [-3, -2, -1, 0, 1, 2])\n@pytest.mark.parametrize(""axis2"", [-3, -2, -1, 0, 1, 2])\ndef test_swapaxes(axis1, axis2):\n    x = sparse.random((2, 3, 4), density=0.25)\n    y = x.todense()\n    xx = x.swapaxes(axis1, axis2)\n    yy = y.swapaxes(axis1, axis2)\n    assert_eq(xx, yy)\n\n\n@pytest.mark.parametrize(""axis1"", [-4, 3])\n@pytest.mark.parametrize(""axis2"", [-4, 3, 0])\ndef test_swapaxes_error(axis1, axis2):\n    x = sparse.random((2, 3, 4), density=0.25)\n\n    with pytest.raises(ValueError):\n        x.swapaxes(axis1, axis2)\n\n\n@pytest.mark.parametrize(\n    ""a,b"",\n    [\n        [(3, 4), (5, 5)],\n        [(12,), (3, 4)],\n        [(12,), (3, 6)],\n        [(5, 5, 5), (6, 6, 6)],\n        [(3, 4), (9, 4)],\n        [(5,), (4,)],\n        [(2, 3, 4, 5), (2, 3, 4, 5, 6)],\n        [(100,), (5, 5)],\n        [(2, 3, 4, 5), (20, 6)],\n        [(), ()],\n    ],\n)\ndef test_resize(a, b):\n    s = sparse.random(a, density=0.5)\n    orig_size = s.size\n    x = s.todense()\n    x = np.resize(x, b)\n    s.resize(b)\n    temp = x.reshape(x.size)\n    temp[orig_size:] = s.fill_value\n    assert isinstance(s, sparse.SparseArray)\n    assert_eq(x, s)\n\n\n@pytest.mark.parametrize(\n    ""a,b"",\n    [\n        [(3, 4), (3, 4)],\n        [(12,), (3, 4)],\n        [(12,), (3, -1)],\n        [(3, 4), (12,)],\n        [(3, 4), (-1, 4)],\n        [(3, 4), (3, -1)],\n        [(2, 3, 4, 5), (8, 15)],\n        [(2, 3, 4, 5), (24, 5)],\n        [(2, 3, 4, 5), (20, 6)],\n        [(), ()],\n    ],\n)\ndef test_reshape(a, b):\n    s = sparse.random(a, density=0.5)\n    x = s.todense()\n\n    assert_eq(x.reshape(b), s.reshape(b))\n\n\ndef test_large_reshape():\n    n = 100\n    m = 10\n    row = np.arange(\n        n, dtype=np.uint16\n    )  # np.random.randint(0, n, size=n, dtype=np.uint16)\n    col = row % m  # np.random.randint(0, m, size=n, dtype=np.uint16)\n    data = np.ones(n, dtype=np.uint8)\n\n    x = COO((data, (row, col)), sorted=True, has_duplicates=False)\n\n    assert_eq(x, x.reshape(x.shape))\n\n\ndef test_reshape_same():\n    s = sparse.random((3, 5), density=0.5)\n\n    assert s.reshape(s.shape) is s\n\n\ndef test_reshape_function():\n    s = sparse.random((5, 3), density=0.5)\n    x = s.todense()\n    shape = (3, 5)\n\n    s2 = np.reshape(s, shape)\n    assert isinstance(s2, COO)\n    assert_eq(s2, x.reshape(shape))\n\n\ndef test_to_scipy_sparse():\n    s = sparse.random((3, 5), density=0.5)\n    a = s.to_scipy_sparse()\n    b = scipy.sparse.coo_matrix(s.todense())\n\n    assert_eq(a, b)\n\n\n@pytest.mark.parametrize(\n    ""a_shape,b_shape,axes"",\n    [\n        [(3, 4), (4, 3), (1, 0)],\n        [(3, 4), (4, 3), (0, 1)],\n        [(3, 4, 5), (4, 3), (1, 0)],\n        [(3, 4), (5, 4, 3), (1, 1)],\n        [(3, 4), (5, 4, 3), ((0, 1), (2, 1))],\n        [(3, 4), (5, 4, 3), ((1, 0), (1, 2))],\n        [(3, 4, 5), (4,), (1, 0)],\n        [(4,), (3, 4, 5), (0, 1)],\n        [(4,), (4,), (0, 0)],\n        [(4,), (4,), 0],\n    ],\n)\ndef test_tensordot(a_shape, b_shape, axes):\n    sa = sparse.random(a_shape, density=0.5)\n    sb = sparse.random(b_shape, density=0.5)\n\n    a = sa.todense()\n    b = sb.todense()\n\n    a_b = np.tensordot(a, b, axes)\n\n    # tests for return_type=None\n    sa_sb = sparse.tensordot(sa, sb, axes)\n    sa_b = sparse.tensordot(sa, b, axes)\n    a_sb = sparse.tensordot(a, sb, axes)\n\n    assert_eq(a_b, sa_sb)\n    assert_eq(a_b, sa_b)\n    assert_eq(a_b, a_sb)\n    assert isinstance(sa_sb, COO)\n    assert isinstance(sa_b, np.ndarray)\n    assert isinstance(a_sb, np.ndarray)\n\n    # tests for return_type=COO\n    sa_b = sparse.tensordot(sa, b, axes, return_type=COO)\n    a_sb = sparse.tensordot(a, sb, axes, return_type=COO)\n\n    assert_eq(a_b, sa_b)\n    assert_eq(a_b, a_sb)\n    assert isinstance(sa_b, COO)\n    assert isinstance(a_sb, COO)\n\n    # tests for return_type=np.ndarray\n    sa_sb = sparse.tensordot(sa, sb, axes, return_type=np.ndarray)\n\n    assert_eq(a_b, sa_sb)\n    assert isinstance(sa_sb, np.ndarray)\n\n\ndef test_tensordot_empty():\n    x1 = np.empty((0, 0, 0))\n    x2 = np.empty((0, 0, 0))\n    s1 = sparse.COO.from_numpy(x1)\n    s2 = sparse.COO.from_numpy(x2)\n\n    assert_eq(np.tensordot(x1, x2), sparse.tensordot(s1, s2))\n\n\ndef test_tensordot_valueerror():\n    x1 = sparse.COO(np.array(1))\n    x2 = sparse.COO(np.array(1))\n\n    with pytest.raises(ValueError):\n        x1 @ x2\n\n\n@pytest.mark.parametrize(\n    ""a_shape, b_shape"",\n    [\n        ((3, 1, 6, 5), (2, 1, 4, 5, 6)),\n        ((2, 1, 4, 5, 6), (3, 1, 6, 5)),\n        ((1, 1, 5), (3, 5, 6)),\n        ((3, 4, 5), (1, 5, 6)),\n        ((3, 4, 5), (3, 5, 6)),\n        ((3, 4, 5), (5, 6)),\n        ((4, 5), (5, 6)),\n        ((5,), (5, 6)),\n        ((4, 5), (5,)),\n        ((5,), (5,)),\n        ((3, 4), (1, 2, 4, 3)),\n    ],\n)\ndef test_matmul(a_shape, b_shape):\n    sa = sparse.random(a_shape, density=0.5)\n    sb = sparse.random(b_shape, density=0.5)\n\n    a = sa.todense()\n    b = sb.todense()\n\n    assert_eq(np.matmul(a, b), sparse.matmul(sa, sb))\n    assert_eq(sparse.matmul(sa, b), sparse.matmul(a, sb))\n    assert_eq(np.matmul(a, b), sparse.matmul(sa, sb))\n\n    if a.ndim == 2 or b.ndim == 2:\n        assert_eq(\n            np.matmul(a, b),\n            sparse.matmul(\n                scipy.sparse.coo_matrix(a) if a.ndim == 2 else sa,\n                scipy.sparse.coo_matrix(b) if b.ndim == 2 else sb,\n            ),\n        )\n\n    if hasattr(operator, ""matmul""):\n        assert_eq(operator.matmul(a, b), operator.matmul(sa, sb))\n\n\ndef test_matmul_errors():\n    with pytest.raises(ValueError):\n        sa = sparse.random((3, 4, 5, 6), 0.5)\n        sb = sparse.random((3, 6, 5, 6), 0.5)\n        sparse.matmul(sa, sb)\n\n\n@pytest.mark.parametrize(\n    ""a_shape, b_shape"",\n    [\n        ((1, 4, 5), (3, 5, 6)),\n        ((3, 4, 5), (1, 5, 6)),\n        ((3, 4, 5), (3, 5, 6)),\n        ((3, 4, 5), (5, 6)),\n        ((4, 5), (5, 6)),\n        ((5,), (5, 6)),\n        ((4, 5), (5,)),\n        ((5,), (5,)),\n    ],\n)\ndef test_dot(a_shape, b_shape):\n    sa = sparse.random(a_shape, density=0.5)\n    sb = sparse.random(b_shape, density=0.5)\n\n    a = sa.todense()\n    b = sb.todense()\n\n    assert_eq(a.dot(b), sa.dot(sb))\n    assert_eq(np.dot(a, b), sparse.dot(sa, sb))\n    assert_eq(sparse.dot(sa, b), sparse.dot(a, sb))\n    assert_eq(np.dot(a, b), sparse.dot(sa, sb))\n\n    if hasattr(operator, ""matmul""):\n        # Basic equivalences\n        assert_eq(operator.matmul(a, b), operator.matmul(sa, sb))\n        # Test that SOO\'s and np.array\'s combine correctly\n        # Not possible due to https://github.com/numpy/numpy/issues/9028\n        # assert_eq(eval(""a @ sb""), eval(""sa @ b""))\n\n\n@pytest.mark.parametrize(\n    ""a_dense, b_dense, o_type"",\n    [\n        (False, False, sparse.SparseArray),\n        (False, True, np.ndarray),\n        (True, False, np.ndarray),\n    ],\n)\ndef test_dot_type(a_dense, b_dense, o_type):\n    a = sparse.random((3, 4), density=0.8)\n    b = sparse.random((4, 5), density=0.8)\n\n    if a_dense:\n        a = a.todense()\n\n    if b_dense:\n        b = b.todense()\n\n    assert isinstance(sparse.dot(a, b), o_type)\n\n\n@pytest.mark.xfail\ndef test_dot_nocoercion():\n    sa = sparse.random((3, 4, 5), density=0.5)\n    sb = sparse.random((5, 6), density=0.5)\n\n    a = sa.todense()\n    b = sb.todense()\n\n    la = a.tolist()\n    lb = b.tolist()\n\n    if hasattr(operator, ""matmul""):\n        # Operations with naive collection (list)\n        assert_eq(operator.matmul(la, b), operator.matmul(la, sb))\n        assert_eq(operator.matmul(a, lb), operator.matmul(sa, lb))\n\n\n@pytest.mark.parametrize(""a_ndim"", [1, 2, 3])\n@pytest.mark.parametrize(""b_ndim"", [1, 2, 3])\ndef test_kron(a_ndim, b_ndim):\n    a_shape = (2, 3, 4)[:a_ndim]\n    b_shape = (5, 6, 7)[:b_ndim]\n\n    sa = sparse.random(a_shape, density=0.5)\n    a = sa.todense()\n    sb = sparse.random(b_shape, density=0.5)\n    b = sb.todense()\n\n    sol = np.kron(a, b)\n    assert_eq(sparse.kron(sa, sb), sol)\n    assert_eq(sparse.kron(sa, b), sol)\n    assert_eq(sparse.kron(a, sb), sol)\n\n    with pytest.raises(ValueError):\n        assert_eq(sparse.kron(a, b), sol)\n\n\n@pytest.mark.parametrize(\n    ""a_spmatrix, b_spmatrix"", [(True, True), (True, False), (False, True)]\n)\ndef test_kron_spmatrix(a_spmatrix, b_spmatrix):\n    sa = sparse.random((3, 4), density=0.5)\n    a = sa.todense()\n    sb = sparse.random((5, 6), density=0.5)\n    b = sb.todense()\n\n    if a_spmatrix:\n        sa = sa.tocsr()\n\n    if b_spmatrix:\n        sb = sb.tocsr()\n\n    sol = np.kron(a, b)\n    assert_eq(sparse.kron(sa, sb), sol)\n    assert_eq(sparse.kron(sa, b), sol)\n    assert_eq(sparse.kron(a, sb), sol)\n\n    with pytest.raises(ValueError):\n        assert_eq(sparse.kron(a, b), sol)\n\n\n@pytest.mark.parametrize(""ndim"", [1, 2, 3])\ndef test_kron_scalar(ndim):\n    if ndim:\n        a_shape = (3, 4, 5)[:ndim]\n        sa = sparse.random(a_shape, density=0.5)\n        a = sa.todense()\n    else:\n        sa = a = np.array(6)\n    scalar = np.array(5)\n\n    sol = np.kron(a, scalar)\n    assert_eq(sparse.kron(sa, scalar), sol)\n    assert_eq(sparse.kron(scalar, sa), sol)\n\n\n@pytest.mark.parametrize(\n    ""func"",\n    [\n        np.expm1,\n        np.log1p,\n        np.sin,\n        np.tan,\n        np.sinh,\n        np.tanh,\n        np.floor,\n        np.ceil,\n        np.sqrt,\n        np.conj,\n        np.round,\n        np.rint,\n        lambda x: x.astype(""int32""),\n        np.conjugate,\n        np.conj,\n        lambda x: x.round(decimals=2),\n        abs,\n    ],\n)\ndef test_elemwise(func):\n    s = sparse.random((2, 3, 4), density=0.5)\n    x = s.todense()\n\n    fs = func(s)\n    assert isinstance(fs, COO)\n    assert fs.nnz <= s.nnz\n\n    assert_eq(func(x), fs)\n\n\n@pytest.mark.parametrize(\n    ""func"",\n    [\n        np.expm1,\n        np.log1p,\n        np.sin,\n        np.tan,\n        np.sinh,\n        np.tanh,\n        np.floor,\n        np.ceil,\n        np.sqrt,\n        np.conj,\n        np.round,\n        np.rint,\n        np.conjugate,\n        np.conj,\n        lambda x, out: x.round(decimals=2, out=out),\n    ],\n)\ndef test_elemwise_inplace(func):\n    s = sparse.random((2, 3, 4), density=0.5)\n    x = s.todense()\n\n    func(s, out=s)\n    func(x, out=x)\n    assert isinstance(s, COO)\n\n    assert_eq(x, s)\n\n\n@pytest.mark.parametrize(\n    ""shape1, shape2"",\n    [\n        ((2, 3, 4), (3, 4)),\n        ((3, 4), (2, 3, 4)),\n        ((3, 1, 4), (3, 2, 4)),\n        ((1, 3, 4), (3, 4)),\n        ((3, 4, 1), (3, 4, 2)),\n        ((1, 5), (5, 1)),\n        ((3, 1), (3, 4)),\n        ((3, 1), (1, 4)),\n        ((1, 4), (3, 4)),\n        ((2, 2, 2), (1, 1, 1)),\n    ],\n)\ndef test_elemwise_mixed(shape1, shape2):\n    s1 = sparse.random(shape1, density=0.5)\n    x2 = np.random.rand(*shape2)\n\n    x1 = s1.todense()\n\n    assert_eq(s1 * x2, x1 * x2)\n\n\ndef test_elemwise_mixed_empty():\n    s1 = sparse.random((2, 0, 4), density=0.5)\n    x2 = np.random.rand(2, 0, 4)\n\n    x1 = s1.todense()\n\n    assert_eq(s1 * x2, x1 * x2)\n\n\ndef test_elemwise_unsupported():\n    class A:\n        pass\n\n    s1 = sparse.random((2, 3, 4), density=0.5)\n    x2 = A()\n\n    with pytest.raises(TypeError):\n        s1 + x2\n\n    assert sparse.elemwise(operator.add, s1, x2) is NotImplemented\n\n\ndef test_elemwise_mixed_broadcast():\n    s1 = sparse.random((2, 3, 4), density=0.5)\n    s2 = sparse.random(4, density=0.5)\n    x3 = np.random.rand(3, 4)\n\n    x1 = s1.todense()\n    x2 = s2.todense()\n\n    def func(x1, x2, x3):\n        return x1 * x2 * x3\n\n    assert_eq(sparse.elemwise(func, s1, s2, x3), func(x1, x2, x3))\n\n\n@pytest.mark.parametrize(\n    ""func"",\n    [operator.mul, operator.add, operator.sub, operator.gt, operator.lt, operator.ne],\n)\n@pytest.mark.parametrize(""shape"", [(2,), (2, 3), (2, 3, 4), (2, 3, 4, 5)])\ndef test_elemwise_binary(func, shape):\n    xs = sparse.random(shape, density=0.5)\n    ys = sparse.random(shape, density=0.5)\n\n    x = xs.todense()\n    y = ys.todense()\n\n    assert_eq(func(xs, ys), func(x, y))\n\n\n@pytest.mark.parametrize(""func"", [operator.imul, operator.iadd, operator.isub])\n@pytest.mark.parametrize(""shape"", [(2,), (2, 3), (2, 3, 4), (2, 3, 4, 5)])\ndef test_elemwise_binary_inplace(func, shape):\n    xs = sparse.random(shape, density=0.5)\n    ys = sparse.random(shape, density=0.5)\n\n    x = xs.todense()\n    y = ys.todense()\n\n    xs = func(xs, ys)\n    x = func(x, y)\n\n    assert_eq(xs, x)\n\n\n@pytest.mark.parametrize(\n    ""func"",\n    [\n        lambda x, y, z: x + y + z,\n        lambda x, y, z: x * y * z,\n        lambda x, y, z: x + y * z,\n        lambda x, y, z: (x + y) * z,\n    ],\n)\n@pytest.mark.parametrize(""shape"", [(2,), (2, 3), (2, 3, 4), (2, 3, 4, 5)])\ndef test_elemwise_trinary(func, shape):\n    xs = sparse.random(shape, density=0.5)\n    ys = sparse.random(shape, density=0.5)\n    zs = sparse.random(shape, density=0.5)\n\n    x = xs.todense()\n    y = ys.todense()\n    z = zs.todense()\n\n    fs = sparse.elemwise(func, xs, ys, zs)\n    assert isinstance(fs, COO)\n\n    assert_eq(fs, func(x, y, z))\n\n\n@pytest.mark.parametrize(""func"", [operator.add, operator.mul])\n@pytest.mark.parametrize(\n    ""shape1,shape2"",\n    [\n        ((2, 3, 4), (3, 4)),\n        ((3, 4), (2, 3, 4)),\n        ((3, 1, 4), (3, 2, 4)),\n        ((1, 3, 4), (3, 4)),\n        ((3, 4, 1), (3, 4, 2)),\n        ((1, 5), (5, 1)),\n        ((3, 1), (3, 4)),\n        ((3, 1), (1, 4)),\n        ((1, 4), (3, 4)),\n        ((2, 2, 2), (1, 1, 1)),\n    ],\n)\ndef test_binary_broadcasting(func, shape1, shape2):\n    density1 = 1 if np.prod(shape1) == 1 else 0.5\n    density2 = 1 if np.prod(shape2) == 1 else 0.5\n\n    xs = sparse.random(shape1, density=density1)\n    x = xs.todense()\n\n    ys = sparse.random(shape2, density=density2)\n    y = ys.todense()\n\n    expected = func(x, y)\n    actual = func(xs, ys)\n\n    assert isinstance(actual, COO)\n    assert_eq(expected, actual)\n\n    assert np.count_nonzero(expected) == actual.nnz\n\n\n@pytest.mark.parametrize(\n    ""shape1,shape2"",\n    [((3, 4), (2, 3, 4)), ((3, 1, 4), (3, 2, 4)), ((3, 4, 1), (3, 4, 2))],\n)\ndef test_broadcast_to(shape1, shape2):\n    a = sparse.random(shape1, density=0.5)\n    x = a.todense()\n\n    assert_eq(np.broadcast_to(x, shape2), a.broadcast_to(shape2))\n\n\n@pytest.mark.parametrize(\n    ""shapes"",\n    [\n        [(2,), (3, 2), (4, 3, 2)],\n        [(3,), (2, 3), (2, 2, 3)],\n        [(2,), (2, 2), (2, 2, 2)],\n        [(4,), (4, 4), (4, 4, 4)],\n        [(4,), (4, 4), (4, 4, 4)],\n        [(4,), (4, 4), (4, 4, 4)],\n        [(1, 1, 2), (1, 3, 1), (4, 1, 1)],\n        [(2,), (2, 1), (2, 1, 1)],\n    ],\n)\n@pytest.mark.parametrize(\n    ""func"",\n    [\n        lambda x, y, z: (x + y) * z,\n        lambda x, y, z: x * (y + z),\n        lambda x, y, z: x * y * z,\n        lambda x, y, z: x + y + z,\n        lambda x, y, z: x + y - z,\n        lambda x, y, z: x - y + z,\n    ],\n)\ndef test_trinary_broadcasting(shapes, func):\n    args = [sparse.random(s, density=0.5) for s in shapes]\n    dense_args = [arg.todense() for arg in args]\n\n    fs = sparse.elemwise(func, *args)\n    assert isinstance(fs, COO)\n\n    assert_eq(fs, func(*dense_args))\n\n\n@pytest.mark.parametrize(\n    ""shapes, func"",\n    [\n        ([(2,), (3, 2), (4, 3, 2)], lambda x, y, z: (x + y) * z),\n        ([(3,), (2, 3), (2, 2, 3)], lambda x, y, z: x * (y + z)),\n        ([(2,), (2, 2), (2, 2, 2)], lambda x, y, z: x * y * z),\n        ([(4,), (4, 4), (4, 4, 4)], lambda x, y, z: x + y + z),\n    ],\n)\n@pytest.mark.parametrize(""value"", [np.nan, np.inf, -np.inf])\n@pytest.mark.parametrize(""fraction"", [0.25, 0.5, 0.75, 1.0])\n@pytest.mark.filterwarnings(""ignore:invalid value"")\ndef test_trinary_broadcasting_pathological(shapes, func, value, fraction):\n    args = [\n        sparse.random(s, density=0.5, data_rvs=random_value_array(value, fraction))\n        for s in shapes\n    ]\n    dense_args = [arg.todense() for arg in args]\n\n    fs = sparse.elemwise(func, *args)\n    assert isinstance(fs, COO)\n\n    assert_eq(fs, func(*dense_args))\n\n\ndef test_sparse_broadcasting(monkeypatch):\n    orig_unmatch_coo = sparse._coo.umath._Elemwise._get_func_coords_data\n\n    state = {""num_matches"": 0}\n\n    xs = sparse.random((3, 4), density=0.5)\n    ys = sparse.random((3, 4), density=0.5)\n\n    def mock_unmatch_coo(*args, **kwargs):\n        result = orig_unmatch_coo(*args, **kwargs)\n        if result is not None:\n            state[""num_matches""] += 1\n        return result\n\n    monkeypatch.setattr(\n        sparse._coo.umath._Elemwise, ""_get_func_coords_data"", mock_unmatch_coo\n    )\n\n    xs * ys\n\n    # Less than in case there\'s absolutely no overlap in some cases.\n    assert state[""num_matches""] <= 1\n\n\ndef test_dense_broadcasting(monkeypatch):\n    orig_unmatch_coo = sparse._coo.umath._Elemwise._get_func_coords_data\n\n    state = {""num_matches"": 0}\n\n    xs = sparse.random((3, 4), density=0.5)\n    ys = sparse.random((3, 4), density=0.5)\n\n    def mock_unmatch_coo(*args, **kwargs):\n        result = orig_unmatch_coo(*args, **kwargs)\n        if result is not None:\n            state[""num_matches""] += 1\n        return result\n\n    monkeypatch.setattr(\n        sparse._coo.umath._Elemwise, ""_get_func_coords_data"", mock_unmatch_coo\n    )\n\n    xs + ys\n\n    # Less than in case there\'s absolutely no overlap in some cases.\n    assert state[""num_matches""] <= 3\n\n\n@pytest.mark.parametrize(""format"", [""coo"", ""dok""])\ndef test_sparsearray_elemwise(format):\n    xs = sparse.random((3, 4), density=0.5, format=format)\n    ys = sparse.random((3, 4), density=0.5, format=format)\n\n    x = xs.todense()\n    y = ys.todense()\n\n    fs = sparse.elemwise(operator.add, xs, ys)\n    assert isinstance(fs, COO)\n\n    assert_eq(fs, x + y)\n\n\ndef test_ndarray_densification_fails():\n    xs = sparse.random((2, 3, 4), density=0.5)\n    y = np.random.rand(3, 4)\n\n    with pytest.raises(ValueError):\n        xs + y\n\n\ndef test_elemwise_noargs():\n    def func():\n        return np.float_(5.0)\n\n    assert_eq(sparse.elemwise(func), func())\n\n\n@pytest.mark.parametrize(\n    ""func"",\n    [\n        operator.pow,\n        operator.truediv,\n        operator.floordiv,\n        operator.ge,\n        operator.le,\n        operator.eq,\n        operator.mod,\n    ],\n)\n@pytest.mark.filterwarnings(""ignore:divide by zero"")\n@pytest.mark.filterwarnings(""ignore:invalid value"")\ndef test_nonzero_outout_fv_ufunc(func):\n    xs = sparse.random((2, 3, 4), density=0.5)\n    ys = sparse.random((2, 3, 4), density=0.5)\n\n    x = xs.todense()\n    y = ys.todense()\n\n    f = func(x, y)\n    fs = func(xs, ys)\n    assert isinstance(fs, COO)\n\n    assert_eq(f, fs)\n\n\n@pytest.mark.parametrize(\n    ""func, scalar"",\n    [\n        (operator.mul, 5),\n        (operator.add, 0),\n        (operator.sub, 0),\n        (operator.pow, 5),\n        (operator.truediv, 3),\n        (operator.floordiv, 4),\n        (operator.gt, 5),\n        (operator.lt, -5),\n        (operator.ne, 0),\n        (operator.ge, 5),\n        (operator.le, -3),\n        (operator.eq, 1),\n        (operator.mod, 5),\n    ],\n)\n@pytest.mark.parametrize(""convert_to_np_number"", [True, False])\ndef test_elemwise_scalar(func, scalar, convert_to_np_number):\n    xs = sparse.random((2, 3, 4), density=0.5)\n    if convert_to_np_number:\n        scalar = np.float32(scalar)\n    y = scalar\n\n    x = xs.todense()\n    fs = func(xs, y)\n\n    assert isinstance(fs, COO)\n    assert xs.nnz >= fs.nnz\n\n    assert_eq(fs, func(x, y))\n\n\n@pytest.mark.parametrize(\n    ""func, scalar"",\n    [\n        (operator.mul, 5),\n        (operator.add, 0),\n        (operator.sub, 0),\n        (operator.gt, -5),\n        (operator.lt, 5),\n        (operator.ne, 0),\n        (operator.ge, -5),\n        (operator.le, 3),\n        (operator.eq, 1),\n    ],\n)\n@pytest.mark.parametrize(""convert_to_np_number"", [True, False])\ndef test_leftside_elemwise_scalar(func, scalar, convert_to_np_number):\n    xs = sparse.random((2, 3, 4), density=0.5)\n    if convert_to_np_number:\n        scalar = np.float32(scalar)\n    y = scalar\n\n    x = xs.todense()\n    fs = func(y, xs)\n\n    assert isinstance(fs, COO)\n    assert xs.nnz >= fs.nnz\n\n    assert_eq(fs, func(y, x))\n\n\n@pytest.mark.parametrize(\n    ""func, scalar"",\n    [\n        (operator.add, 5),\n        (operator.sub, -5),\n        (operator.pow, -3),\n        (operator.truediv, 0),\n        (operator.floordiv, 0),\n        (operator.gt, -5),\n        (operator.lt, 5),\n        (operator.ne, 1),\n        (operator.ge, -3),\n        (operator.le, 3),\n        (operator.eq, 0),\n    ],\n)\n@pytest.mark.filterwarnings(""ignore:divide by zero"")\n@pytest.mark.filterwarnings(""ignore:invalid value"")\ndef test_scalar_output_nonzero_fv(func, scalar):\n    xs = sparse.random((2, 3, 4), density=0.5)\n    y = scalar\n\n    x = xs.todense()\n\n    f = func(x, y)\n    fs = func(xs, y)\n\n    assert isinstance(fs, COO)\n    assert fs.nnz <= xs.nnz\n\n    assert_eq(f, fs)\n\n\n@pytest.mark.parametrize(""func"", [operator.and_, operator.or_, operator.xor])\n@pytest.mark.parametrize(""shape"", [(2,), (2, 3), (2, 3, 4), (2, 3, 4, 5)])\ndef test_bitwise_binary(func, shape):\n    # Small arrays need high density to have nnz entries\n    # Casting floats to int will result in all zeros, hence the * 100\n    xs = (sparse.random(shape, density=0.5) * 100).astype(np.int_)\n    ys = (sparse.random(shape, density=0.5) * 100).astype(np.int_)\n\n    x = xs.todense()\n    y = ys.todense()\n\n    assert_eq(func(xs, ys), func(x, y))\n\n\n@pytest.mark.parametrize(""func"", [operator.iand, operator.ior, operator.ixor])\n@pytest.mark.parametrize(""shape"", [(2,), (2, 3), (2, 3, 4), (2, 3, 4, 5)])\ndef test_bitwise_binary_inplace(func, shape):\n    # Small arrays need high density to have nnz entries\n    # Casting floats to int will result in all zeros, hence the * 100\n    xs = (sparse.random(shape, density=0.5) * 100).astype(np.int_)\n    ys = (sparse.random(shape, density=0.5) * 100).astype(np.int_)\n\n    x = xs.todense()\n    y = ys.todense()\n\n    xs = func(xs, ys)\n    x = func(x, y)\n\n    assert_eq(xs, x)\n\n\n@pytest.mark.parametrize(""func"", [operator.lshift, operator.rshift])\n@pytest.mark.parametrize(""shape"", [(2,), (2, 3), (2, 3, 4), (2, 3, 4, 5)])\ndef test_bitshift_binary(func, shape):\n    # Small arrays need high density to have nnz entries\n    # Casting floats to int will result in all zeros, hence the * 100\n    xs = (sparse.random(shape, density=0.5) * 100).astype(np.int_)\n\n    # Can\'t merge into test_bitwise_binary because left/right shifting\n    # with something >= 64 isn\'t defined.\n    ys = (sparse.random(shape, density=0.5) * 64).astype(np.int_)\n\n    x = xs.todense()\n    y = ys.todense()\n\n    assert_eq(func(xs, ys), func(x, y))\n\n\n@pytest.mark.parametrize(""func"", [operator.ilshift, operator.irshift])\n@pytest.mark.parametrize(""shape"", [(2,), (2, 3), (2, 3, 4), (2, 3, 4, 5)])\ndef test_bitshift_binary_inplace(func, shape):\n    # Small arrays need high density to have nnz entries\n    # Casting floats to int will result in all zeros, hence the * 100\n    xs = (sparse.random(shape, density=0.5) * 100).astype(np.int_)\n\n    # Can\'t merge into test_bitwise_binary because left/right shifting\n    # with something >= 64 isn\'t defined.\n    ys = (sparse.random(shape, density=0.5) * 64).astype(np.int_)\n\n    x = xs.todense()\n    y = ys.todense()\n\n    xs = func(xs, ys)\n    x = func(x, y)\n\n    assert_eq(xs, x)\n\n\n@pytest.mark.parametrize(""func"", [operator.and_])\n@pytest.mark.parametrize(""shape"", [(2,), (2, 3), (2, 3, 4), (2, 3, 4, 5)])\ndef test_bitwise_scalar(func, shape):\n    # Small arrays need high density to have nnz entries\n    # Casting floats to int will result in all zeros, hence the * 100\n    xs = (sparse.random(shape, density=0.5) * 100).astype(np.int_)\n    y = np.random.randint(100)\n\n    x = xs.todense()\n\n    assert_eq(func(xs, y), func(x, y))\n    assert_eq(func(y, xs), func(y, x))\n\n\n@pytest.mark.parametrize(""func"", [operator.lshift, operator.rshift])\n@pytest.mark.parametrize(""shape"", [(2,), (2, 3), (2, 3, 4), (2, 3, 4, 5)])\ndef test_bitshift_scalar(func, shape):\n    # Small arrays need high density to have nnz entries\n    # Casting floats to int will result in all zeros, hence the * 100\n    xs = (sparse.random(shape, density=0.5) * 100).astype(np.int_)\n\n    # Can\'t merge into test_bitwise_binary because left/right shifting\n    # with something >= 64 isn\'t defined.\n    y = np.random.randint(64)\n\n    x = xs.todense()\n\n    assert_eq(func(xs, y), func(x, y))\n\n\n@pytest.mark.parametrize(""func"", [operator.invert])\n@pytest.mark.parametrize(""shape"", [(2,), (2, 3), (2, 3, 4), (2, 3, 4, 5)])\ndef test_unary_bitwise_nonzero_output_fv(func, shape):\n    # Small arrays need high density to have nnz entries\n    # Casting floats to int will result in all zeros, hence the * 100\n    xs = (sparse.random(shape, density=0.5) * 100).astype(np.int_)\n    x = xs.todense()\n\n    f = func(x)\n    fs = func(xs)\n\n    assert isinstance(fs, COO)\n    assert fs.nnz <= xs.nnz\n\n    assert_eq(f, fs)\n\n\n@pytest.mark.parametrize(""func"", [operator.or_, operator.xor])\n@pytest.mark.parametrize(""shape"", [(2,), (2, 3), (2, 3, 4), (2, 3, 4, 5)])\ndef test_binary_bitwise_nonzero_output_fv(func, shape):\n    # Small arrays need high density to have nnz entries\n    # Casting floats to int will result in all zeros, hence the * 100\n    xs = (sparse.random(shape, density=0.5) * 100).astype(np.int_)\n    y = np.random.randint(1, 100)\n\n    x = xs.todense()\n\n    f = func(x, y)\n    fs = func(xs, y)\n\n    assert isinstance(fs, COO)\n    assert fs.nnz <= xs.nnz\n\n    assert_eq(f, fs)\n\n\n@pytest.mark.parametrize(\n    ""func"",\n    [operator.mul, operator.add, operator.sub, operator.gt, operator.lt, operator.ne],\n)\n@pytest.mark.parametrize(""shape"", [(2,), (2, 3), (2, 3, 4), (2, 3, 4, 5)])\ndef test_elemwise_nonzero_input_fv(func, shape):\n    xs = sparse.random(shape, density=0.5, fill_value=np.random.rand())\n    ys = sparse.random(shape, density=0.5, fill_value=np.random.rand())\n\n    x = xs.todense()\n    y = ys.todense()\n\n    assert_eq(func(xs, ys), func(x, y))\n\n\n@pytest.mark.parametrize(""func"", [operator.lshift, operator.rshift])\n@pytest.mark.parametrize(""shape"", [(2,), (2, 3), (2, 3, 4), (2, 3, 4, 5)])\ndef test_binary_bitshift_densification_fails(func, shape):\n    # Small arrays need high density to have nnz entries\n    # Casting floats to int will result in all zeros, hence the * 100\n    x = np.random.randint(1, 100)\n    ys = (sparse.random(shape, density=0.5) * 64).astype(np.int_)\n\n    y = ys.todense()\n\n    f = func(x, y)\n    fs = func(x, ys)\n\n    assert isinstance(fs, COO)\n    assert fs.nnz <= ys.nnz\n\n    assert_eq(f, fs)\n\n\n@pytest.mark.parametrize(""func"", [operator.and_, operator.or_, operator.xor])\n@pytest.mark.parametrize(""shape"", [(2,), (2, 3), (2, 3, 4), (2, 3, 4, 5)])\ndef test_bitwise_binary_bool(func, shape):\n    # Small arrays need high density to have nnz entries\n    xs = sparse.random(shape, density=0.5).astype(bool)\n    ys = sparse.random(shape, density=0.5).astype(bool)\n\n    x = xs.todense()\n    y = ys.todense()\n\n    assert_eq(func(xs, ys), func(x, y))\n\n\ndef test_elemwise_binary_empty():\n    x = COO({}, shape=(10, 10))\n    y = sparse.random((10, 10), density=0.5)\n\n    for z in [x * y, y * x]:\n        assert z.nnz == 0\n        assert z.coords.shape == (2, 0)\n        assert z.data.shape == (0,)\n\n\ndef test_gt():\n    s = sparse.random((2, 3, 4), density=0.5)\n    x = s.todense()\n\n    m = x.mean()\n    assert_eq(x > m, s > m)\n\n    m = s.data[2]\n    assert_eq(x > m, s > m)\n    assert_eq(x >= m, s >= m)\n\n\n@pytest.mark.parametrize(\n    ""index"",\n    [\n        # Integer\n        0,\n        1,\n        -1,\n        (1, 1, 1),\n        # Pure slices\n        (slice(0, 2),),\n        (slice(None, 2), slice(None, 2)),\n        (slice(1, None), slice(1, None)),\n        (slice(None, None),),\n        (slice(None, None, -1),),\n        (slice(None, 2, -1), slice(None, 2, -1)),\n        (slice(1, None, 2), slice(1, None, 2)),\n        (slice(None, None, 2),),\n        (slice(None, 2, -1), slice(None, 2, -2)),\n        (slice(1, None, 2), slice(1, None, 1)),\n        (slice(None, None, -2),),\n        # Combinations\n        (0, slice(0, 2)),\n        (slice(0, 1), 0),\n        (None, slice(1, 3), 0),\n        (slice(0, 3), None, 0),\n        (slice(1, 2), slice(2, 4)),\n        (slice(1, 2), slice(None, None)),\n        (slice(1, 2), slice(None, None), 2),\n        (slice(1, 2, 2), slice(None, None), 2),\n        (slice(1, 2, None), slice(None, None, 2), 2),\n        (slice(1, 2, -2), slice(None, None), -2),\n        (slice(1, 2, None), slice(None, None, -2), 2),\n        (slice(1, 2, -1), slice(None, None), -1),\n        (slice(1, 2, None), slice(None, None, -1), 2),\n        (slice(2, 0, -1), slice(None, None), -1),\n        (slice(-2, None, None),),\n        (slice(-1, None, None), slice(-2, None, None)),\n        # With ellipsis\n        (Ellipsis, slice(1, 3)),\n        (1, Ellipsis, slice(1, 3)),\n        (slice(0, 1), Ellipsis),\n        (Ellipsis, None),\n        (None, Ellipsis),\n        (1, Ellipsis),\n        (1, Ellipsis, None),\n        (1, 1, 1, Ellipsis),\n        (Ellipsis, 1, None),\n        # With multi-axis advanced indexing\n        ([0, 1],) * 2,\n        ([0, 1], [0, 2]),\n        ([0, 0, 0], [0, 1, 2], [1, 2, 1]),\n        # Pathological - Slices larger than array\n        (slice(None, 1000)),\n        (slice(None), slice(None, 1000)),\n        (slice(None), slice(1000, -1000, -1)),\n        (slice(None), slice(1000, -1000, -50)),\n        # Pathological - Wrong ordering of start/stop\n        (slice(5, 0),),\n        (slice(0, 5, -1),),\n        (slice(0, 0, None),),\n    ],\n)\ndef test_slicing(index):\n    s = sparse.random((2, 3, 4), density=0.5)\n    x = s.todense()\n\n    assert_eq(x[index], s[index])\n\n\n@pytest.mark.parametrize(\n    ""index"",\n    [\n        ([1, 0], 0),\n        (1, [0, 2]),\n        (0, [1, 0], 0),\n        (1, [2, 0], 0),\n        (1, [], 0),\n        ([True, False], slice(1, None), slice(-2, None)),\n        (slice(1, None), slice(-2, None), [True, False, True, False]),\n        ([1, 0],),\n        (Ellipsis, [2, 1, 3]),\n        (slice(None), [2, 1, 2]),\n        (1, [2, 0, 1]),\n    ],\n)\ndef test_advanced_indexing(index):\n    s = sparse.random((2, 3, 4), density=0.5)\n    x = s.todense()\n\n    assert_eq(x[index], s[index])\n\n\ndef test_custom_dtype_slicing():\n    dt = np.dtype(\n        [(""part1"", np.float_), (""part2"", np.int_, (2,)), (""part3"", np.int_, (2, 2))]\n    )\n\n    x = np.zeros((2, 3, 4), dtype=dt)\n    x[1, 1, 1] = (0.64, [4, 2], [[1, 2], [3, 0]])\n\n    s = COO.from_numpy(x)\n\n    assert x[1, 1, 1] == s[1, 1, 1]\n    assert x[0, 1, 2] == s[0, 1, 2]\n\n    assert_eq(x[""part1""], s[""part1""])\n    assert_eq(x[""part2""], s[""part2""])\n    assert_eq(x[""part3""], s[""part3""])\n\n\n@pytest.mark.parametrize(\n    ""index"",\n    [\n        (Ellipsis, Ellipsis),\n        (1, 1, 1, 1),\n        (slice(None),) * 4,\n        5,\n        -5,\n        ""foo"",\n        [True, False, False],\n        0.5,\n        [0.5],\n        {""potato"": ""kartoffel""},\n        ([[0, 1]],),\n    ],\n)\ndef test_slicing_errors(index):\n    s = sparse.random((2, 3, 4), density=0.5)\n\n    with pytest.raises(IndexError):\n        s[index]\n\n\ndef test_concatenate():\n    xx = sparse.random((2, 3, 4), density=0.5)\n    x = xx.todense()\n    yy = sparse.random((5, 3, 4), density=0.5)\n    y = yy.todense()\n    zz = sparse.random((4, 3, 4), density=0.5)\n    z = zz.todense()\n\n    assert_eq(\n        np.concatenate([x, y, z], axis=0), sparse.concatenate([xx, yy, zz], axis=0)\n    )\n\n    xx = sparse.random((5, 3, 1), density=0.5)\n    x = xx.todense()\n    yy = sparse.random((5, 3, 3), density=0.5)\n    y = yy.todense()\n    zz = sparse.random((5, 3, 2), density=0.5)\n    z = zz.todense()\n\n    assert_eq(\n        np.concatenate([x, y, z], axis=2), sparse.concatenate([xx, yy, zz], axis=2)\n    )\n\n    assert_eq(\n        np.concatenate([x, y, z], axis=-1), sparse.concatenate([xx, yy, zz], axis=-1)\n    )\n\n\n@pytest.mark.parametrize(""axis"", [0, 1])\n@pytest.mark.parametrize(""func"", [sparse.stack, sparse.concatenate])\ndef test_concatenate_mixed(func, axis):\n    s = sparse.random((10, 10), density=0.5)\n    d = s.todense()\n\n    with pytest.raises(ValueError):\n        func([d, s, s], axis=axis)\n\n\ndef test_concatenate_noarrays():\n    with pytest.raises(ValueError):\n        sparse.concatenate([])\n\n\n@pytest.mark.parametrize(""shape"", [(5,), (2, 3, 4), (5, 2)])\n@pytest.mark.parametrize(""axis"", [0, 1, -1])\ndef test_stack(shape, axis):\n    xx = sparse.random(shape, density=0.5)\n    x = xx.todense()\n    yy = sparse.random(shape, density=0.5)\n    y = yy.todense()\n    zz = sparse.random(shape, density=0.5)\n    z = zz.todense()\n\n    assert_eq(np.stack([x, y, z], axis=axis), sparse.stack([xx, yy, zz], axis=axis))\n\n\ndef test_large_concat_stack():\n    data = np.array([1], dtype=np.uint8)\n    coords = np.array([[255]], dtype=np.uint8)\n\n    xs = COO(coords, data, shape=(256,), has_duplicates=False, sorted=True)\n    x = xs.todense()\n\n    assert_eq(np.stack([x, x]), sparse.stack([xs, xs]))\n\n    assert_eq(np.concatenate((x, x)), sparse.concatenate((xs, xs)))\n\n\ndef test_addition():\n    a = sparse.random((2, 3, 4), density=0.5)\n    x = a.todense()\n\n    b = sparse.random((2, 3, 4), density=0.5)\n    y = b.todense()\n\n    assert_eq(x + y, a + b)\n    assert_eq(x - y, a - b)\n\n\n@pytest.mark.parametrize(""scalar"", [2, 2.5, np.float32(2.0), np.int8(3)])\ndef test_scalar_multiplication(scalar):\n    a = sparse.random((2, 3, 4), density=0.5)\n    x = a.todense()\n\n    assert_eq(x * scalar, a * scalar)\n    assert (a * scalar).nnz == a.nnz\n    assert_eq(scalar * x, scalar * a)\n    assert (scalar * a).nnz == a.nnz\n    assert_eq(x / scalar, a / scalar)\n    assert (a / scalar).nnz == a.nnz\n    assert_eq(x // scalar, a // scalar)\n    # division may reduce nnz.\n\n\n@pytest.mark.filterwarnings(""ignore:divide by zero"")\ndef test_scalar_exponentiation():\n    a = sparse.random((2, 3, 4), density=0.5)\n    x = a.todense()\n\n    assert_eq(x ** 2, a ** 2)\n    assert_eq(x ** 0.5, a ** 0.5)\n\n    assert_eq(x ** -1, a ** -1)\n\n\ndef test_create_with_lists_of_tuples():\n    L = [((0, 0, 0), 1), ((1, 2, 1), 1), ((1, 1, 1), 2), ((1, 3, 2), 3)]\n\n    s = COO(L)\n\n    x = np.zeros((2, 4, 3), dtype=np.asarray([1, 2, 3]).dtype)\n    for ind, value in L:\n        x[ind] = value\n\n    assert_eq(s, x)\n\n\ndef test_sizeof():\n    x = np.eye(100)\n    y = COO.from_numpy(x)\n\n    nb = sys.getsizeof(y)\n    assert 400 < nb < x.nbytes / 10\n\n\ndef test_scipy_sparse_interface():\n    n = 100\n    m = 10\n    row = np.random.randint(0, n, size=n, dtype=np.uint16)\n    col = np.random.randint(0, m, size=n, dtype=np.uint16)\n    data = np.ones(n, dtype=np.uint8)\n\n    inp = (data, (row, col))\n\n    x = scipy.sparse.coo_matrix(inp)\n    xx = sparse.COO(inp)\n\n    assert_eq(x, xx, check_nnz=False)\n    assert_eq(x.T, xx.T, check_nnz=False)\n    assert_eq(xx.to_scipy_sparse(), x, check_nnz=False)\n    assert_eq(COO.from_scipy_sparse(xx.to_scipy_sparse()), xx, check_nnz=False)\n\n    assert_eq(x, xx, check_nnz=False)\n    assert_eq(x.T.dot(x), xx.T.dot(xx), check_nnz=False)\n    assert isinstance(x + xx, COO)\n    assert isinstance(xx + x, COO)\n\n\n@pytest.mark.parametrize(""scipy_format"", [""coo"", ""csr"", ""dok"", ""csc""])\ndef test_scipy_sparse_interaction(scipy_format):\n    x = sparse.random((10, 20), density=0.2).todense()\n    sp = getattr(scipy.sparse, scipy_format + ""_matrix"")(x)\n    coo = COO(x)\n    assert isinstance(sp + coo, COO)\n    assert isinstance(coo + sp, COO)\n    assert_eq(sp, coo)\n\n\n@pytest.mark.parametrize(\n    ""func"",\n    [operator.mul, operator.add, operator.sub, operator.gt, operator.lt, operator.ne],\n)\ndef test_op_scipy_sparse(func):\n    xs = sparse.random((3, 4), density=0.5)\n    y = sparse.random((3, 4), density=0.5).todense()\n\n    ys = scipy.sparse.csr_matrix(y)\n    x = xs.todense()\n\n    assert_eq(func(x, y), func(xs, ys))\n\n\n@pytest.mark.parametrize(\n    ""func"",\n    [\n        operator.add,\n        operator.sub,\n        pytest.param(\n            operator.mul,\n            marks=pytest.mark.xfail(reason=""Scipy sparse auto-densifies in this case.""),\n        ),\n        pytest.param(\n            operator.gt,\n            marks=pytest.mark.xfail(reason=""Scipy sparse doesn\'t support this yet.""),\n        ),\n        pytest.param(\n            operator.lt,\n            marks=pytest.mark.xfail(reason=""Scipy sparse doesn\'t support this yet.""),\n        ),\n        pytest.param(\n            operator.ne,\n            marks=pytest.mark.xfail(reason=""Scipy sparse doesn\'t support this yet.""),\n        ),\n    ],\n)\ndef test_op_scipy_sparse_left(func):\n    ys = sparse.random((3, 4), density=0.5)\n    x = sparse.random((3, 4), density=0.5).todense()\n\n    xs = scipy.sparse.csr_matrix(x)\n    y = ys.todense()\n\n    assert_eq(func(x, y), func(xs, ys))\n\n\ndef test_cache_csr():\n    x = sparse.random((10, 5), density=0.5).todense()\n    s = COO(x, cache=True)\n\n    assert isinstance(s.tocsr(), scipy.sparse.csr_matrix)\n    assert isinstance(s.tocsc(), scipy.sparse.csc_matrix)\n    assert s.tocsr() is s.tocsr()\n    assert s.tocsc() is s.tocsc()\n\n\ndef test_empty_shape():\n    x = COO(np.empty((0, 1), dtype=np.int8), [1.0])\n    assert x.shape == ()\n    assert_eq(2 * x, np.float_(2.0))\n\n\ndef test_single_dimension():\n    x = COO([1, 3], [1.0, 3.0])\n    assert x.shape == (4,)\n    assert_eq(x, np.array([0, 1.0, 0, 3.0]))\n\n\ndef test_large_sum():\n    n = 500000\n    x = np.random.randint(0, 10000, size=(n,))\n    y = np.random.randint(0, 1000, size=(n,))\n    z = np.random.randint(0, 3, size=(n,))\n\n    data = np.random.random(n)\n\n    a = COO((x, y, z), data)\n    assert a.shape == (10000, 1000, 3)\n\n    b = a.sum(axis=2)\n    assert b.nnz > 100000\n\n\ndef test_add_many_sparse_arrays():\n    x = COO({(1, 1): 1})\n    y = sum([x] * 100)\n    assert y.nnz < np.prod(y.shape)\n\n\ndef test_caching():\n    x = COO({(9, 9, 9): 1})\n    assert (\n        x[:].reshape((100, 10)).transpose().tocsr()\n        is not x[:].reshape((100, 10)).transpose().tocsr()\n    )\n\n    x = COO({(9, 9, 9): 1}, cache=True)\n    assert (\n        x[:].reshape((100, 10)).transpose().tocsr()\n        is x[:].reshape((100, 10)).transpose().tocsr()\n    )\n\n    x = COO({(1, 1, 1, 1, 1, 1, 1, 2): 1}, cache=True)\n\n    for i in range(x.ndim):\n        x.reshape(x.size)\n\n    assert len(x._cache[""reshape""]) < 5\n\n\ndef test_scalar_slicing():\n    x = np.array([0, 1])\n    s = COO(x)\n    assert np.isscalar(s[0])\n    assert_eq(x[0], s[0])\n\n    assert isinstance(s[0, ...], COO)\n    assert s[0, ...].shape == ()\n    assert_eq(x[0, ...], s[0, ...])\n\n    assert np.isscalar(s[1])\n    assert_eq(x[1], s[1])\n\n    assert isinstance(s[1, ...], COO)\n    assert s[1, ...].shape == ()\n    assert_eq(x[1, ...], s[1, ...])\n\n\n@pytest.mark.parametrize(\n    ""shape, k"",\n    [((3, 4), 0), ((3, 4, 5), 1), ((4, 2), -1), ((2, 4), -2), ((4, 4), 1000)],\n)\ndef test_triul(shape, k):\n    s = sparse.random(shape, density=0.5)\n    x = s.todense()\n\n    assert_eq(np.triu(x, k), sparse.triu(s, k))\n    assert_eq(np.tril(x, k), sparse.tril(s, k))\n\n\ndef test_empty_reduction():\n    x = np.zeros((2, 3, 4), dtype=np.float_)\n    xs = COO.from_numpy(x)\n\n    assert_eq(x.sum(axis=(0, 2)), xs.sum(axis=(0, 2)))\n\n\n@pytest.mark.parametrize(""shape"", [(2,), (2, 3), (2, 3, 4)])\n@pytest.mark.parametrize(""density"", [0.1, 0.3, 0.5, 0.7])\ndef test_random_shape(shape, density):\n    s = sparse.random(shape, density)\n\n    assert isinstance(s, COO)\n\n    assert s.shape == shape\n    expected_nnz = density * np.prod(shape)\n    assert np.floor(expected_nnz) <= s.nnz <= np.ceil(expected_nnz)\n\n\ndef test_two_random_unequal():\n    s1 = sparse.random((2, 3, 4), 0.3)\n    s2 = sparse.random((2, 3, 4), 0.3)\n\n    assert not np.allclose(s1.todense(), s2.todense())\n\n\ndef test_two_random_same_seed():\n    state = np.random.randint(100)\n    s1 = sparse.random((2, 3, 4), 0.3, random_state=state)\n    s2 = sparse.random((2, 3, 4), 0.3, random_state=state)\n\n    assert_eq(s1, s2)\n\n\n@pytest.mark.parametrize(\n    ""rvs, dtype"",\n    [\n        (None, np.float64),\n        (scipy.stats.poisson(25, loc=10).rvs, np.int),\n        (lambda x: np.random.choice([True, False], size=x), np.bool),\n    ],\n)\n@pytest.mark.parametrize(""shape"", [(2, 4, 5), (20, 40, 50)])\n@pytest.mark.parametrize(""density"", [0.0, 0.01, 0.1, 0.2])\ndef test_random_rvs(rvs, dtype, shape, density):\n    x = sparse.random(shape, density, data_rvs=rvs)\n    assert x.shape == shape\n    assert x.dtype == dtype\n\n\n@pytest.mark.parametrize(""format"", [""coo"", ""dok""])\ndef test_random_fv(format):\n    fv = np.random.rand()\n    s = sparse.random((2, 3, 4), density=0.5, format=format, fill_value=fv)\n\n    assert s.fill_value == fv\n\n\ndef test_scalar_shape_construction():\n    x = np.random.rand(5)\n    coords = np.arange(5)[None]\n\n    s = COO(coords, x, shape=5)\n\n    assert_eq(x, s)\n\n\ndef test_len():\n    s = sparse.random((20, 30, 40))\n    assert len(s) == 20\n\n\ndef test_density():\n    s = sparse.random((20, 30, 40), density=0.1)\n    assert np.isclose(s.density, 0.1)\n\n\ndef test_size():\n    s = sparse.random((20, 30, 40))\n    assert s.size == 20 * 30 * 40\n\n\ndef test_np_array():\n    s = sparse.random((20, 30, 40))\n\n    with pytest.raises(RuntimeError):\n        np.array(s)\n\n\n@pytest.mark.parametrize(\n    ""shapes"",\n    [\n        [(2,), (3, 2), (4, 3, 2)],\n        [(3,), (2, 3), (2, 2, 3)],\n        [(2,), (2, 2), (2, 2, 2)],\n        [(4,), (4, 4), (4, 4, 4)],\n        [(4,), (4, 4), (4, 4, 4)],\n        [(4,), (4, 4), (4, 4, 4)],\n        [(1, 1, 2), (1, 3, 1), (4, 1, 1)],\n        [(2,), (2, 1), (2, 1, 1)],\n        [(3,), (), (2, 3)],\n        [(4, 4), (), ()],\n    ],\n)\ndef test_three_arg_where(shapes):\n    cs = sparse.random(shapes[0], density=0.5).astype(np.bool)\n    xs = sparse.random(shapes[1], density=0.5)\n    ys = sparse.random(shapes[2], density=0.5)\n\n    c = cs.todense()\n    x = xs.todense()\n    y = ys.todense()\n\n    expected = np.where(c, x, y)\n    actual = sparse.where(cs, xs, ys)\n\n    assert isinstance(actual, COO)\n    assert_eq(expected, actual)\n\n\ndef test_one_arg_where():\n    s = sparse.random((2, 3, 4), density=0.5)\n    x = s.todense()\n\n    expected = np.where(x)\n    actual = sparse.where(s)\n\n    assert len(expected) == len(actual)\n\n    for e, a in zip(expected, actual):\n        assert_eq(e, a, compare_dtype=False)\n\n\ndef test_one_arg_where_dense():\n    x = np.random.rand(2, 3, 4)\n\n    with pytest.raises(ValueError):\n        sparse.where(x)\n\n\ndef test_two_arg_where():\n    cs = sparse.random((2, 3, 4), density=0.5).astype(np.bool)\n    xs = sparse.random((2, 3, 4), density=0.5)\n\n    with pytest.raises(ValueError):\n        sparse.where(cs, xs)\n\n\n@pytest.mark.parametrize(""func"", [operator.imul, operator.iadd, operator.isub])\ndef test_inplace_invalid_shape(func):\n    xs = sparse.random((3, 4), density=0.5)\n    ys = sparse.random((2, 3, 4), density=0.5)\n\n    with pytest.raises(ValueError):\n        func(xs, ys)\n\n\ndef test_nonzero():\n    s = sparse.random((2, 3, 4), density=0.5)\n    x = s.todense()\n\n    expected = x.nonzero()\n    actual = s.nonzero()\n\n    assert isinstance(actual, tuple)\n    assert len(expected) == len(actual)\n\n    for e, a in zip(expected, actual):\n        assert_eq(e, a, compare_dtype=False)\n\n\ndef test_argwhere():\n    s = sparse.random((2, 3, 4), density=0.5)\n    x = s.todense()\n\n    assert_eq(np.argwhere(s), np.argwhere(x), compare_dtype=False)\n\n\n@pytest.mark.parametrize(""format"", [""coo"", ""dok""])\ndef test_asformat(format):\n    s = sparse.random((2, 3, 4), density=0.5, format=""coo"")\n    s2 = s.asformat(format)\n\n    assert_eq(s, s2)\n\n\n@pytest.mark.parametrize(\n    ""format"", [sparse.COO, sparse.DOK, scipy.sparse.csr_matrix, np.asarray]\n)\ndef test_as_coo(format):\n    x = format(sparse.random((3, 4), density=0.5, format=""coo"").todense())\n\n    s1 = sparse.as_coo(x)\n    s2 = COO(x)\n\n    assert_eq(x, s1)\n    assert_eq(x, s2)\n\n\ndef test_invalid_attrs_error():\n    s = sparse.random((3, 4), density=0.5, format=""coo"")\n\n    with pytest.raises(ValueError):\n        sparse.as_coo(s, shape=(2, 3))\n\n    with pytest.raises(ValueError):\n        COO(s, shape=(2, 3))\n\n    with pytest.raises(ValueError):\n        sparse.as_coo(s, fill_value=0.0)\n\n    with pytest.raises(ValueError):\n        COO(s, fill_value=0.0)\n\n\ndef test_invalid_iterable_error():\n    with pytest.raises(ValueError):\n        x = [(3, 4, 5)]\n        COO.from_iter(x)\n\n    with pytest.raises(ValueError):\n        x = [((2.3, 4.5), 3.2)]\n        COO.from_iter(x)\n\n\ndef test_prod_along_axis():\n    s1 = sparse.random((10, 10), density=0.1)\n    s2 = 1 - s1\n\n    x1 = s1.todense()\n    x2 = s2.todense()\n\n    assert_eq(s1.prod(axis=0), x1.prod(axis=0))\n    assert_eq(s2.prod(axis=0), x2.prod(axis=0))\n\n\nclass TestRoll:\n\n    # test on 1d array #\n    @pytest.mark.parametrize(""shift"", [0, 2, -2, 20, -20])\n    def test_1d(self, shift):\n        xs = sparse.random((100,), density=0.5)\n        x = xs.todense()\n        assert_eq(np.roll(x, shift), sparse.roll(xs, shift))\n        assert_eq(np.roll(x, shift), sparse.roll(x, shift))\n\n    # test on 2d array #\n    @pytest.mark.parametrize(""shift"", [0, 2, -2, 20, -20])\n    @pytest.mark.parametrize(""ax"", [None, 0, 1, (0, 1)])\n    def test_2d(self, shift, ax):\n        xs = sparse.random((10, 10), density=0.5)\n        x = xs.todense()\n        assert_eq(np.roll(x, shift, axis=ax), sparse.roll(xs, shift, axis=ax))\n        assert_eq(np.roll(x, shift, axis=ax), sparse.roll(x, shift, axis=ax))\n\n    # test on rolling multiple axes at once #\n    @pytest.mark.parametrize(""shift"", [(0, 0), (1, -1), (-1, 1), (10, -10)])\n    @pytest.mark.parametrize(""ax"", [(0, 1), (0, 2), (1, 2), (-1, 1)])\n    def test_multiaxis(self, shift, ax):\n        xs = sparse.random((9, 9, 9), density=0.5)\n        x = xs.todense()\n        assert_eq(np.roll(x, shift, axis=ax), sparse.roll(xs, shift, axis=ax))\n        assert_eq(np.roll(x, shift, axis=ax), sparse.roll(x, shift, axis=ax))\n\n    # test original is unchanged #\n    @pytest.mark.parametrize(""shift"", [0, 2, -2, 20, -20])\n    @pytest.mark.parametrize(""ax"", [None, 0, 1, (0, 1)])\n    def test_original_is_copied(self, shift, ax):\n        xs = sparse.random((10, 10), density=0.5)\n        xc = COO(np.copy(xs.coords), np.copy(xs.data), shape=xs.shape)\n        sparse.roll(xs, shift, axis=ax)\n        assert_eq(xs, xc)\n\n    # test on empty array #\n    def test_empty(self):\n        x = np.array([])\n        assert_eq(np.roll(x, 1), sparse.roll(sparse.as_coo(x), 1))\n\n    # test error handling #\n    @pytest.mark.parametrize(\n        ""args"",\n        [\n            # iterable shift, but axis not iterable\n            ((1, 1), 0),\n            # ndim(axis) != 1\n            (1, [[0, 1]]),\n            # ndim(shift) != 1\n            ([[0, 1]], [0, 1]),\n            ([[0, 1], [0, 1]], [0, 1]),\n        ],\n    )\n    def test_valerr(self, args):\n        x = sparse.random((2, 2, 2), density=1)\n        with pytest.raises(ValueError):\n            sparse.roll(x, *args)\n\n\ndef test_clip():\n    x = np.array([[0, 0, 1, 0, 2], [5, 0, 0, 3, 0]])\n\n    s = sparse.COO.from_numpy(x)\n\n    assert_eq(s.clip(min=1), x.clip(min=1))\n    assert_eq(s.clip(max=3), x.clip(max=3))\n    assert_eq(s.clip(min=1, max=3), x.clip(min=1, max=3))\n    assert_eq(s.clip(min=1, max=3.0), x.clip(min=1, max=3.0))\n\n    assert_eq(np.clip(s, 1, 3), np.clip(x, 1, 3))\n\n    with pytest.raises(ValueError):\n        s.clip()\n\n    out = sparse.COO.from_numpy(np.zeros_like(x))\n    out2 = s.clip(min=1, max=3, out=out)\n    assert out is out2\n    assert_eq(out, x.clip(min=1, max=3))\n\n\nclass TestFailFillValue:\n    # Check failed fill_value op\n    def test_nonzero_fv(self):\n        xs = sparse.random((2, 3), density=0.5, fill_value=1)\n        ys = sparse.random((3, 4), density=0.5)\n\n        with pytest.raises(ValueError):\n            sparse.dot(xs, ys)\n\n    def test_inconsistent_fv(self):\n        xs = sparse.random((3, 4), density=0.5, fill_value=1)\n        ys = sparse.random((3, 4), density=0.5, fill_value=2)\n\n        with pytest.raises(ValueError):\n            sparse.concatenate([xs, ys])\n\n\ndef test_pickle():\n    x = sparse.COO.from_numpy([1, 0, 0, 0, 0]).reshape((5, 1))\n    # Enable caching and add some data to it\n    x.enable_caching()\n    x.T\n    assert x._cache is not None\n    # Pickle sends data but not cache\n    x2 = pickle.loads(pickle.dumps(x))\n    assert_eq(x, x2)\n    assert x2._cache is None\n\n\n@pytest.mark.parametrize(""deep"", [True, False])\ndef test_copy(deep):\n    x = sparse.COO.from_numpy([1, 0, 0, 0, 0]).reshape((5, 1))\n    # Enable caching and add some data to it\n    x.enable_caching()\n    x.T\n    assert x._cache is not None\n\n    x2 = x.copy(deep)\n    assert_eq(x, x2)\n    assert (x2.data is x.data) is not deep\n    assert (x2.coords is x.coords) is not deep\n    assert x2._cache is None\n\n\n@pytest.mark.parametrize(""ndim"", [2, 3, 4, 5])\ndef test_initialization(ndim):\n    shape = [10] * ndim\n    shape[1] *= 2\n    shape = tuple(shape)\n\n    coords = np.random.randint(10, size=ndim * 20).reshape(ndim, 20)\n    data = np.random.rand(20)\n    COO(coords, data=data, shape=shape)\n\n    with pytest.raises(ValueError, match=""data length""):\n        COO(coords, data=data[:5], shape=shape)\n    with pytest.raises(ValueError, match=""shape of `coords`""):\n        coords = np.random.randint(10, size=20).reshape(1, 20)\n        COO(coords, data=data, shape=shape)\n\n\n@pytest.mark.parametrize(""N, M"", [(4, None), (4, 10), (10, 4), (0, 10)])\ndef test_eye(N, M):\n    m = M or N\n    for k in [0, N - 2, N + 2, m - 2, m + 2]:\n        assert_eq(sparse.eye(N, M=M, k=k), np.eye(N, M=M, k=k))\n        assert_eq(sparse.eye(N, M=M, k=k, dtype=""i4""), np.eye(N, M=M, k=k, dtype=""i4""))\n\n\n@pytest.mark.parametrize(""funcname"", [""ones"", ""zeros""])\ndef test_ones_zeros(funcname):\n    sp_func = getattr(sparse, funcname)\n    np_func = getattr(np, funcname)\n\n    assert_eq(sp_func(5), np_func(5))\n    assert_eq(sp_func((5, 4)), np_func((5, 4)))\n    assert_eq(sp_func((5, 4), dtype=""i4""), np_func((5, 4), dtype=""i4""))\n    assert_eq(sp_func((5, 4), dtype=None), np_func((5, 4), dtype=None))\n\n\n@pytest.mark.parametrize(""funcname"", [""ones_like"", ""zeros_like""])\ndef test_ones_zeros_like(funcname):\n    sp_func = getattr(sparse, funcname)\n    np_func = getattr(np, funcname)\n\n    x = np.ones((5, 5), dtype=""i8"")\n\n    assert_eq(sp_func(x), np_func(x))\n    assert_eq(sp_func(x, dtype=""f8""), np_func(x, dtype=""f8""))\n    assert_eq(sp_func(x, dtype=None), np_func(x, dtype=None))\n\n\ndef test_full():\n    assert_eq(sparse.full(5, 9), np.full(5, 9))\n    assert_eq(sparse.full(5, 9, dtype=""f8""), np.full(5, 9, dtype=""f8""))\n    assert_eq(sparse.full((5, 4), 9.5), np.full((5, 4), 9.5))\n    assert_eq(sparse.full((5, 4), 9.5, dtype=""i4""), np.full((5, 4), 9.5, dtype=""i4""))\n\n\ndef test_full_like():\n    x = np.zeros((5, 5), dtype=""i8"")\n    assert_eq(sparse.full_like(x, 9.5), np.full_like(x, 9.5))\n    assert_eq(sparse.full_like(x, 9.5, dtype=""f8""), np.full_like(x, 9.5, dtype=""f8""))\n\n\n@pytest.mark.parametrize(""complex"", [True, False])\ndef test_complex_methods(complex):\n    if complex:\n        x = np.array([1 + 2j, 2 - 1j, 0, 1, 0])\n    else:\n        x = np.array([1, 2, 0, 0, 0])\n    s = sparse.COO.from_numpy(x)\n    assert_eq(s.imag, x.imag)\n    assert_eq(s.real, x.real)\n    assert_eq(s.conj(), x.conj())\n\n\ndef test_np_matrix():\n    x = np.random.rand(10, 1).view(type=np.matrix)\n    s = sparse.COO.from_numpy(x)\n\n    assert_eq(x, s)\n\n\ndef test_out_dtype():\n    a = sparse.eye(5, dtype=""float32"")\n    b = sparse.eye(5, dtype=""float64"")\n\n    assert (\n        np.positive(a, out=b).dtype == np.positive(a.todense(), out=b.todense()).dtype\n    )\n    assert (\n        np.positive(a, out=b, dtype=""float64"").dtype\n        == np.positive(a.todense(), out=b.todense(), dtype=""float64"").dtype\n    )\n\n\n@contextlib.contextmanager\ndef auto_densify():\n    ""For use in tests only! Not threadsafe.""\n    import os\n    from importlib import reload\n\n    os.environ[""SPARSE_AUTO_DENSIFY""] = ""1""\n    reload(sparse._settings)\n    yield\n    del os.environ[""SPARSE_AUTO_DENSIFY""]\n    reload(sparse._settings)\n\n\ndef test_setting_into_numpy_slice():\n    actual = np.zeros((5, 5))\n    s = sparse.COO(data=[1, 1], coords=(2, 4), shape=(5,))\n    # This calls s.__array__(dtype(\'float64\')) which means that __array__\n    # must accept a positional argument. If not this will raise, of course,\n    # TypeError: __array__() takes 1 positional argument but 2 were given\n    with auto_densify():\n        actual[:, 0] = s\n\n    # Might as well check the content of the result as well.\n    expected = np.zeros((5, 5))\n    expected[:, 0] = s.todense()\n    assert_eq(actual, expected)\n\n    # Without densification, setting is unsupported.\n    with pytest.raises(RuntimeError):\n        actual[:, 0] = s\n\n\ndef test_successful_densification():\n    s = sparse.random((3, 4, 5), density=0.5)\n    with auto_densify():\n        x = np.array(s)\n\n    assert isinstance(x, np.ndarray)\n    assert_eq(s, x)\n\n\ndef test_failed_densification():\n    s = sparse.random((3, 4, 5), density=0.5)\n    with pytest.raises(RuntimeError):\n        np.array(s)\n\n\ndef test_warn_on_too_dense():\n    import os\n    from importlib import reload\n\n    os.environ[""SPARSE_WARN_ON_TOO_DENSE""] = ""1""\n    reload(sparse._settings)\n\n    with pytest.warns(RuntimeWarning):\n        sparse.random((3, 4, 5), density=1.0)\n\n    del os.environ[""SPARSE_WARN_ON_TOO_DENSE""]\n    reload(sparse._settings)\n\n\ndef test_prune_coo():\n    coords = np.array([[0, 1, 2, 3]])\n    data = np.array([1, 0, 1, 2])\n    s1 = COO(coords, data)\n    s2 = COO(coords, data, prune=True)\n    assert s2.nnz == 3\n\n    # Densify s1 because it isn\'t canonical\n    assert_eq(s1.todense(), s2, check_nnz=False)\n\n\ndef test_diagonal():\n\n    a = sparse.random((4, 4), density=0.5)\n\n    assert_eq(sparse.diagonal(a, offset=0), np.diagonal(a.todense(), offset=0))\n    assert_eq(sparse.diagonal(a, offset=1), np.diagonal(a.todense(), offset=1))\n    assert_eq(sparse.diagonal(a, offset=2), np.diagonal(a.todense(), offset=2))\n\n    a = sparse.random((4, 5, 4, 6), density=0.5)\n\n    assert_eq(\n        sparse.diagonal(a, offset=0, axis1=0, axis2=2),\n        np.diagonal(a.todense(), offset=0, axis1=0, axis2=2),\n    )\n\n    assert_eq(\n        sparse.diagonal(a, offset=1, axis1=0, axis2=2),\n        np.diagonal(a.todense(), offset=1, axis1=0, axis2=2),\n    )\n\n    assert_eq(\n        sparse.diagonal(a, offset=2, axis1=0, axis2=2),\n        np.diagonal(a.todense(), offset=2, axis1=0, axis2=2),\n    )\n\n\ndef test_diagonalize():\n\n    assert_eq(sparse.diagonalize(np.ones(3)), sparse.eye(3))\n\n    assert_eq(\n        sparse.diagonalize(scipy.sparse.coo_matrix(np.eye(3))),\n        sparse.diagonalize(sparse.eye(3)),\n    )\n\n    # inverse of diagonal\n    b = sparse.random((4, 3, 2), density=0.5)\n    b_diag = sparse.diagonalize(b, axis=1)\n\n    assert_eq(b, sparse.diagonal(b_diag, axis1=1, axis2=3).transpose([0, 2, 1]))\n\n\nRESULT_TYPE_DTYPES = [\n    ""i1"",\n    ""i2"",\n    ""i4"",\n    ""i8"",\n    ""u1"",\n    ""u2"",\n    ""u4"",\n    ""u8"",\n    ""f4"",\n    ""f8"",\n    ""c8"",\n    ""c16"",\n    object,\n]\n\n\n@pytest.mark.parametrize(""t1"", RESULT_TYPE_DTYPES)\n@pytest.mark.parametrize(""t2"", RESULT_TYPE_DTYPES)\n@pytest.mark.parametrize(\n    ""func"",\n    [\n        sparse.result_type,\n        pytest.param(\n            np.result_type,\n            marks=pytest.mark.skipif(not NEP18_ENABLED, reason=""NEP18 is not enabled""),\n        ),\n    ],\n)\n@pytest.mark.parametrize(""data"", [1, [1]])  # Not the same outputs!\ndef test_result_type(t1, t2, func, data):\n    a = np.array(data, dtype=t1)\n    b = np.array(data, dtype=t2)\n    expect = np.result_type(a, b)\n    assert func(a, sparse.COO(b)) == expect\n    assert func(sparse.COO(a), b) == expect\n    assert func(sparse.COO(a), sparse.COO(b)) == expect\n    assert func(a.dtype, sparse.COO(b)) == np.result_type(a.dtype, b)\n    assert func(sparse.COO(a), b.dtype) == np.result_type(a, b.dtype)\n\n\n@pytest.mark.parametrize(""in_shape"", [(5, 5), 62, (3, 3, 3)])\ndef test_flatten(in_shape):\n    s = sparse.random(in_shape, density=0.5)\n    x = s.todense()\n\n    a = s.flatten()\n    e = x.flatten()\n\n    assert_eq(e, a)\n\n\ndef test_asnumpy():\n    s = sparse.COO(data=[1], coords=[2], shape=(5,))\n    assert_eq(sparse.asnumpy(s), s.todense())\n    assert_eq(\n        sparse.asnumpy(s, dtype=np.float64), np.asarray(s.todense(), dtype=np.float64)\n    )\n    a = np.array([1, 2, 3])\n    # Array passes through with no copying.\n    assert sparse.asnumpy(a) is a\n\n\n@pytest.mark.parametrize(""shape1"", [(2,), (2, 3), (2, 3, 4)])\n@pytest.mark.parametrize(""shape2"", [(2,), (2, 3), (2, 3, 4)])\ndef test_outer(shape1, shape2):\n    s1 = sparse.random(shape1, density=0.5)\n    s2 = sparse.random(shape2, density=0.5)\n\n    x1 = s1.todense()\n    x2 = s2.todense()\n\n    assert_eq(sparse.outer(s1, s2), np.outer(x1, x2))\n    assert_eq(np.multiply.outer(s1, s2), np.multiply.outer(x1, x2))\n\n\ndef test_scalar_list_init():\n    a = sparse.COO([], [], ())\n    b = sparse.COO([], [1], ())\n\n    assert a.todense() == 0\n    assert b.todense() == 1\n'"
sparse/tests/test_coo_numba.py,4,"b'import pytest\nimport numba\n\nimport sparse\nimport numpy as np\n\n\n@numba.njit\ndef identity(x):\n    """""" Pass an object through numba and back """"""\n    return x\n\n\ndef identity_constant(x):\n    @numba.njit\n    def get_it():\n        """""" Pass an object through numba and back as a constant """"""\n        return x\n\n    return get_it()\n\n\ndef assert_coo_equal(c1, c2):\n    assert c1.shape == c2.shape\n    assert c1 == c2\n    assert c1.data.dtype == c2.data.dtype\n    assert c1.fill_value == c2.fill_value\n\n\ndef assert_coo_same_memory(c1, c2):\n    assert_coo_equal(c1, c2)\n    assert c1.coords.data == c2.coords.data\n    assert c1.data.data == c2.data.data\n\n\nclass TestBasic:\n    """""" Test very simple construction and field access """"""\n\n    def test_roundtrip(self):\n        c1 = sparse.COO(np.eye(3), fill_value=1)\n        c2 = identity(c1)\n        assert type(c1) is type(c2)\n        assert_coo_same_memory(c1, c2)\n\n    def test_roundtrip_constant(self):\n        c1 = sparse.COO(np.eye(3), fill_value=1)\n        c2 = identity_constant(c1)\n        # constants are always copies\n        assert_coo_equal(c1, c2)\n\n    def test_unpack_attrs(self):\n        @numba.njit\n        def unpack(c):\n            return c.coords, c.data, c.shape, c.fill_value\n\n        c1 = sparse.COO(np.eye(3), fill_value=1)\n        coords, data, shape, fill_value = unpack(c1)\n        c2 = sparse.COO(coords, data, shape, fill_value=fill_value)\n        assert_coo_same_memory(c1, c2)\n\n    def test_repack_attrs(self):\n        @numba.njit\n        def pack(coords, data, shape):\n            return sparse.COO(coords, data, shape)\n\n        # repacking fill_value isn\'t possible yet\n        c1 = sparse.COO(np.eye(3))\n        c2 = pack(c1.coords, c1.data, c1.shape)\n        assert_coo_same_memory(c1, c2)\n'"
sparse/tests/test_dask_interop.py,0,"b'from dask.base import tokenize\nimport sparse\n\n\ndef test_deterministic_token():\n    a = sparse.COO(data=[1, 2, 3], coords=[10, 20, 30], shape=(40,))\n    b = sparse.COO(data=[1, 2, 3], coords=[10, 20, 30], shape=(40,))\n    assert tokenize(a) == tokenize(b)\n    # One of these things is not like the other....\n    c = sparse.COO(data=[1, 2, 4], coords=[10, 20, 30], shape=(40,))\n    assert tokenize(a) != tokenize(c)\n'"
sparse/tests/test_dok.py,29,"b'import pytest\n\nimport numpy as np\n\nimport sparse\nfrom sparse import DOK\nfrom sparse._utils import assert_eq\n\n\n@pytest.mark.parametrize(""shape"", [(2,), (2, 3), (2, 3, 4)])\n@pytest.mark.parametrize(""density"", [0.1, 0.3, 0.5, 0.7])\ndef test_random_shape_nnz(shape, density):\n    s = sparse.random(shape, density, format=""dok"")\n\n    assert isinstance(s, DOK)\n\n    assert s.shape == shape\n    expected_nnz = density * np.prod(shape)\n    assert np.floor(expected_nnz) <= s.nnz <= np.ceil(expected_nnz)\n\n\ndef test_convert_to_coo():\n    s1 = sparse.random((2, 3, 4), 0.5, format=""dok"")\n    s2 = sparse.COO(s1)\n\n    assert_eq(s1, s2)\n\n\ndef test_convert_from_coo():\n    s1 = sparse.random((2, 3, 4), 0.5, format=""coo"")\n    s2 = DOK(s1)\n\n    assert_eq(s1, s2)\n\n\ndef test_convert_from_numpy():\n    x = np.random.rand(2, 3, 4)\n    s = DOK(x)\n\n    assert_eq(x, s)\n\n\ndef test_convert_to_numpy():\n    s = sparse.random((2, 3, 4), 0.5, format=""dok"")\n    x = s.todense()\n\n    assert_eq(x, s)\n\n\n@pytest.mark.parametrize(\n    ""shape, data"",\n    [\n        (2, {0: 1}),\n        ((2, 3), {(0, 1): 3, (1, 2): 4}),\n        ((2, 3, 4), {(0, 1): 3, (1, 2, 3): 4, (1, 1): [6, 5, 4, 1]}),\n    ],\n)\ndef test_construct(shape, data):\n    s = DOK(shape, data)\n    x = np.zeros(shape, dtype=s.dtype)\n\n    for c, d in data.items():\n        x[c] = d\n\n    assert_eq(x, s)\n\n\n@pytest.mark.parametrize(""shape"", [(2,), (2, 3), (2, 3, 4)])\n@pytest.mark.parametrize(""density"", [0.1, 0.3, 0.5, 0.7])\ndef test_getitem(shape, density):\n    s = sparse.random(shape, density, format=""dok"")\n    x = s.todense()\n\n    for _ in range(s.nnz):\n        idx = np.random.randint(np.prod(shape))\n        idx = np.unravel_index(idx, shape)\n\n        assert np.isclose(s[idx], x[idx])\n\n\n@pytest.mark.parametrize(\n    ""shape, index, value"",\n    [\n        ((2,), slice(None), np.random.rand()),\n        ((2,), slice(1, 2), np.random.rand()),\n        ((2,), slice(0, 2), np.random.rand(2)),\n        ((2,), 1, np.random.rand()),\n        ((2, 3), (0, slice(None)), np.random.rand()),\n        ((2, 3), (0, slice(1, 3)), np.random.rand()),\n        ((2, 3), (1, slice(None)), np.random.rand(3)),\n        ((2, 3), (0, slice(1, 3)), np.random.rand(2)),\n        ((2, 3), (0, slice(2, 0, -1)), np.random.rand(2)),\n        ((2, 3), (slice(None), 1), np.random.rand()),\n        ((2, 3), (slice(None), 1), np.random.rand(2)),\n        ((2, 3), (slice(1, 2), 1), np.random.rand()),\n        ((2, 3), (slice(1, 2), 1), np.random.rand(1)),\n        ((2, 3), (0, 2), np.random.rand()),\n    ],\n)\ndef test_setitem(shape, index, value):\n    s = sparse.random(shape, 0.5, format=""dok"")\n    x = s.todense()\n\n    s[index] = value\n    x[index] = value\n\n    assert_eq(x, s)\n\n\ndef test_default_dtype():\n    s = DOK((5,))\n\n    assert s.dtype == np.float64\n\n\ndef test_int_dtype():\n    data = {1: np.uint8(1), 2: np.uint16(2)}\n\n    s = DOK((5,), data)\n\n    assert s.dtype == np.uint16\n\n\ndef test_float_dtype():\n    data = {1: np.uint8(1), 2: np.float32(2)}\n\n    s = DOK((5,), data)\n\n    assert s.dtype == np.float32\n\n\ndef test_set_zero():\n    s = DOK((1,), dtype=np.uint8)\n    s[0] = 1\n    s[0] = 0\n\n    assert s[0] == 0\n    assert s.nnz == 0\n\n\n@pytest.mark.parametrize(""format"", [""coo"", ""dok""])\ndef test_asformat(format):\n    s = sparse.random((2, 3, 4), density=0.5, format=""dok"")\n    s2 = s.asformat(format)\n\n    assert_eq(s, s2)\n\n\ndef test_coo_fv_interface():\n    s1 = sparse.full((5, 5), fill_value=1 + np.random.rand())\n    s2 = sparse.DOK(s1)\n    assert_eq(s1, s2)\n    s3 = sparse.COO(s2)\n    assert_eq(s1, s3)\n\n\ndef test_empty_dok_dtype():\n    d = sparse.DOK(5, dtype=np.uint8)\n    s = sparse.COO(d)\n    assert s.dtype == d.dtype\n'"
sparse/tests/test_io.py,2,"b'import os\nimport tempfile\nimport shutil\nimport pytest\nimport numpy as np\n\nimport sparse\n\nfrom sparse import save_npz, load_npz\nfrom sparse._utils import assert_eq\n\n\n@pytest.mark.parametrize(""compression"", [True, False])\ndef test_save_load_npz_file(compression):\n    x = sparse.random((2, 3, 4, 5), density=0.25)\n    y = x.todense()\n\n    dir_name = tempfile.mkdtemp()\n    filename = os.path.join(dir_name, ""mat.npz"")\n\n    save_npz(filename, x, compressed=compression)\n    z = load_npz(filename)\n    assert_eq(x, z)\n    assert_eq(y, z.todense())\n\n    shutil.rmtree(dir_name)\n\n\ndef test_load_wrong_format_exception():\n    x = np.array([1, 2, 3])\n\n    dir_name = tempfile.mkdtemp()\n    filename = os.path.join(dir_name, ""mat.npz"")\n\n    np.savez(filename, x)\n    with pytest.raises(RuntimeError):\n        load_npz(filename)\n\n    shutil.rmtree(dir_name)\n'"
