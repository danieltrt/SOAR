file_path,api_count,code
conftest.py,0,"b'""""""Configuration for pytest.""""""\n\nimport pytest\n\n\ndef pytest_addoption(parser):\n    """"""Add command-line flags for pytest.""""""\n    parser.addoption(""--run-flaky"", action=""store_true"", help=""runs flaky tests"")\n    parser.addoption(\n        ""--run-network-tests"",\n        action=""store_true"",\n        help=""runs tests requiring a network connection"",\n    )\n\n\ndef pytest_runtest_setup(item):\n    # based on https://stackoverflow.com/questions/47559524\n    if ""flaky"" in item.keywords and not item.config.getoption(""--run-flaky""):\n        pytest.skip(""set --run-flaky option to run flaky tests"")\n    if ""network"" in item.keywords and not item.config.getoption(""--run-network-tests""):\n        pytest.skip(\n            ""set --run-network-tests to run test requiring an "" ""internet connection""\n        )\n\n\n@pytest.fixture(autouse=True)\ndef add_standard_imports(doctest_namespace):\n    import numpy as np\n    import pandas as pd\n    import xarray as xr\n\n    doctest_namespace[""np""] = np\n    doctest_namespace[""pd""] = pd\n    doctest_namespace[""xr""] = xr\n'"
setup.py,0,b'#!/usr/bin/env python\nfrom setuptools import setup\n\nsetup(use_scm_version=True)\n'
ci/min_deps_check.py,0,"b'""""""Fetch from conda database all available versions of the xarray dependencies and their\npublication date. Compare it against requirements/py36-min-all-deps.yml to verify the\npolicy on obsolete dependencies is being followed. Print a pretty report :)\n""""""\nimport subprocess\nimport sys\nfrom concurrent.futures import ThreadPoolExecutor\nfrom datetime import datetime, timedelta\nfrom typing import Dict, Iterator, Optional, Tuple\n\nimport yaml\n\nIGNORE_DEPS = {\n    ""black"",\n    ""coveralls"",\n    ""flake8"",\n    ""hypothesis"",\n    ""isort"",\n    ""mypy"",\n    ""pip"",\n    ""pytest"",\n    ""pytest-cov"",\n    ""pytest-env"",\n}\n\nPOLICY_MONTHS = {""python"": 42, ""numpy"": 24, ""pandas"": 12, ""scipy"": 12}\nPOLICY_MONTHS_DEFAULT = 6\n\nhas_errors = False\n\n\ndef error(msg: str) -> None:\n    global has_errors\n    has_errors = True\n    print(""ERROR:"", msg)\n\n\ndef warning(msg: str) -> None:\n    print(""WARNING:"", msg)\n\n\ndef parse_requirements(fname) -> Iterator[Tuple[str, int, int, Optional[int]]]:\n    """"""Load requirements/py36-min-all-deps.yml\n\n    Yield (package name, major version, minor version, [patch version])\n    """"""\n    global has_errors\n\n    with open(fname) as fh:\n        contents = yaml.safe_load(fh)\n    for row in contents[""dependencies""]:\n        if isinstance(row, dict) and list(row) == [""pip""]:\n            continue\n        pkg, eq, version = row.partition(""="")\n        if pkg.rstrip(""<>"") in IGNORE_DEPS:\n            continue\n        if pkg.endswith(""<"") or pkg.endswith("">"") or eq != ""="":\n            error(""package should be pinned with exact version: "" + row)\n            continue\n\n        try:\n            version_tup = tuple(int(x) for x in version.split("".""))\n        except ValueError:\n            raise ValueError(""non-numerical version: "" + row)\n\n        if len(version_tup) == 2:\n            yield (pkg, *version_tup, None)  # type: ignore\n        elif len(version_tup) == 3:\n            yield (pkg, *version_tup)  # type: ignore\n        else:\n            raise ValueError(""expected major.minor or major.minor.patch: "" + row)\n\n\ndef query_conda(pkg: str) -> Dict[Tuple[int, int], datetime]:\n    """"""Query the conda repository for a specific package\n\n    Return map of {(major version, minor version): publication date}\n    """"""\n    stdout = subprocess.check_output(\n        [""conda"", ""search"", pkg, ""--info"", ""-c"", ""defaults"", ""-c"", ""conda-forge""]\n    )\n    out = {}  # type: Dict[Tuple[int, int], datetime]\n    major = None\n    minor = None\n\n    for row in stdout.decode(""utf-8"").splitlines():\n        label, _, value = row.partition("":"")\n        label = label.strip()\n        if label == ""file name"":\n            value = value.strip()[len(pkg) :]\n            smajor, sminor = value.split(""-"")[1].split(""."")[:2]\n            major = int(smajor)\n            minor = int(sminor)\n        if label == ""timestamp"":\n            assert major is not None\n            assert minor is not None\n            ts = datetime.strptime(value.split()[0].strip(), ""%Y-%m-%d"")\n\n            if (major, minor) in out:\n                out[major, minor] = min(out[major, minor], ts)\n            else:\n                out[major, minor] = ts\n\n    # Hardcoded fix to work around incorrect dates in conda\n    if pkg == ""python"":\n        out.update(\n            {\n                (2, 7): datetime(2010, 6, 3),\n                (3, 5): datetime(2015, 9, 13),\n                (3, 6): datetime(2016, 12, 23),\n                (3, 7): datetime(2018, 6, 27),\n                (3, 8): datetime(2019, 10, 14),\n            }\n        )\n\n    return out\n\n\ndef process_pkg(\n    pkg: str, req_major: int, req_minor: int, req_patch: Optional[int]\n) -> Tuple[str, str, str, str, str, str]:\n    """"""Compare package version from requirements file to available versions in conda.\n    Return row to build pandas dataframe:\n\n    - package name\n    - major.minor.[patch] version in requirements file\n    - publication date of version in requirements file (YYYY-MM-DD)\n    - major.minor version suggested by policy\n    - publication date of version suggested by policy (YYYY-MM-DD)\n    - status (""<"", ""="", ""> (!)"")\n    """"""\n    print(""Analyzing %s..."" % pkg)\n    versions = query_conda(pkg)\n\n    try:\n        req_published = versions[req_major, req_minor]\n    except KeyError:\n        error(""not found in conda: "" + pkg)\n        return pkg, fmt_version(req_major, req_minor, req_patch), ""-"", ""-"", ""-"", ""(!)""\n\n    policy_months = POLICY_MONTHS.get(pkg, POLICY_MONTHS_DEFAULT)\n    policy_published = datetime.now() - timedelta(days=policy_months * 30)\n\n    policy_major = req_major\n    policy_minor = req_minor\n    policy_published_actual = req_published\n    for (major, minor), published in reversed(sorted(versions.items())):\n        if published < policy_published:\n            break\n        policy_major = major\n        policy_minor = minor\n        policy_published_actual = published\n\n    if (req_major, req_minor) < (policy_major, policy_minor):\n        status = ""<""\n    elif (req_major, req_minor) > (policy_major, policy_minor):\n        status = ""> (!)""\n        error(""Package is too new: "" + pkg)\n    else:\n        status = ""=""\n\n    if req_patch is not None:\n        warning(""patch version should not appear in requirements file: "" + pkg)\n        status += "" (w)""\n\n    return (\n        pkg,\n        fmt_version(req_major, req_minor, req_patch),\n        req_published.strftime(""%Y-%m-%d""),\n        fmt_version(policy_major, policy_minor),\n        policy_published_actual.strftime(""%Y-%m-%d""),\n        status,\n    )\n\n\ndef fmt_version(major: int, minor: int, patch: int = None) -> str:\n    if patch is None:\n        return f""{major}.{minor}""\n    else:\n        return f""{major}.{minor}.{patch}""\n\n\ndef main() -> None:\n    fname = sys.argv[1]\n    with ThreadPoolExecutor(8) as ex:\n        futures = [\n            ex.submit(process_pkg, pkg, major, minor, patch)\n            for pkg, major, minor, patch in parse_requirements(fname)\n        ]\n        rows = [f.result() for f in futures]\n\n    print(""Package       Required             Policy               Status"")\n    print(""------------- -------------------- -------------------- ------"")\n    fmt = ""{:13} {:7} ({:10}) {:7} ({:10}) {}""\n    for row in rows:\n        print(fmt.format(*row))\n\n    assert not has_errors\n\n\nif __name__ == ""__main__"":\n    main()\n'"
doc/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# xarray documentation build configuration file, created by\n# sphinx-quickstart on Thu Feb  6 18:57:54 2014.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n\nimport datetime\nimport os\nimport pathlib\nimport subprocess\nimport sys\nfrom contextlib import suppress\n\n# make sure the source version is preferred (#3567)\nroot = pathlib.Path(__file__).absolute().parent.parent\nos.environ[""PYTHONPATH""] = str(root)\nsys.path.insert(0, str(root))\n\nimport xarray  # isort:skip\n\nallowed_failures = set()\n\nprint(""python exec:"", sys.executable)\nprint(""sys.path:"", sys.path)\n\nif ""conda"" in sys.executable:\n    print(""conda environment:"")\n    subprocess.run([""conda"", ""list""])\nelse:\n    print(""pip environment:"")\n    subprocess.run([""pip"", ""list""])\n\nprint(""xarray: %s, %s"" % (xarray.__version__, xarray.__file__))\n\nwith suppress(ImportError):\n    import matplotlib\n\n    matplotlib.use(""Agg"")\n\ntry:\n    import rasterio\nexcept ImportError:\n    allowed_failures.update(\n        [""gallery/plot_rasterio_rgb.py"", ""gallery/plot_rasterio.py""]\n    )\n\ntry:\n    import cartopy\nexcept ImportError:\n    allowed_failures.update(\n        [\n            ""gallery/plot_cartopy_facetgrid.py"",\n            ""gallery/plot_rasterio_rgb.py"",\n            ""gallery/plot_rasterio.py"",\n        ]\n    )\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    ""sphinx.ext.autodoc"",\n    ""sphinx.ext.autosummary"",\n    ""sphinx.ext.intersphinx"",\n    ""sphinx.ext.extlinks"",\n    ""sphinx.ext.mathjax"",\n    ""sphinx.ext.napoleon"",\n    ""IPython.sphinxext.ipython_directive"",\n    ""IPython.sphinxext.ipython_console_highlighting"",\n    ""nbsphinx"",\n]\n\nextlinks = {\n    ""issue"": (""https://github.com/pydata/xarray/issues/%s"", ""GH""),\n    ""pull"": (""https://github.com/pydata/xarray/pull/%s"", ""PR""),\n}\n\nnbsphinx_timeout = 600\nnbsphinx_execute = ""always""\nnbsphinx_prolog = """"""\n{% set docname = env.doc2path(env.docname, base=None) %}\n\nYou can run this notebook in a `live session <https://mybinder.org/v2/gh/pydata/xarray/doc/examples/master?urlpath=lab/tree/doc/{{ docname }}>`_ |Binder| or view it `on Github <https://github.com/pydata/xarray/blob/master/doc/{{ docname }}>`_.\n\n.. |Binder| image:: https://mybinder.org/badge.svg\n   :target: https://mybinder.org/v2/gh/pydata/xarray/master?urlpath=lab/tree/doc/{{ docname }}\n""""""\n\nautosummary_generate = True\nautodoc_typehints = ""none""\n\nnapoleon_use_param = True\nnapoleon_use_rtype = True\n\nnumpydoc_class_members_toctree = True\nnumpydoc_show_class_members = False\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [""_templates""]\n\n# The suffix of source filenames.\nsource_suffix = "".rst""\n\n# The encoding of source files.\n# source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = ""index""\n\n# General information about the project.\nproject = ""xarray""\ncopyright = ""2014-%s, xarray Developers"" % datetime.datetime.now().year\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = xarray.__version__.split(""+"")[0]\n# The full version, including alpha/beta/rc tags.\nrelease = xarray.__version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n# language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n# today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\ntoday_fmt = ""%Y-%m-%d""\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [""_build"", ""**.ipynb_checkpoints""]\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n# default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n# add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = ""sphinx""\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n# keep_warnings = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = ""sphinx_rtd_theme""\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\nhtml_theme_options = {""logo_only"": True}\n\n# Add any paths that contain custom themes here, relative to this directory.\n# html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n# html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n# html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\nhtml_logo = ""_static/dataset-diagram-logo.png""\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\nhtml_favicon = ""_static/favicon.ico""\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [""_static""]\n\n# Sometimes the savefig directory doesn\'t exist and needs to be created\n# https://github.com/ipython/ipython/issues/8733\n# becomes obsolete when we can pin ipython>=5.2; see ci/requirements/doc.yml\nipython_savefig_dir = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)), ""_build"", ""html"", ""_static""\n)\nif not os.path.exists(ipython_savefig_dir):\n    os.makedirs(ipython_savefig_dir)\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n# html_extra_path = []\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\nhtml_last_updated_fmt = today_fmt\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n# html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n# html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n# html_additional_pages = {}\n\n# If false, no module index is generated.\n# html_domain_indices = True\n\n# If false, no index is generated.\n# html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n# html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n# html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n# html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n# html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n# html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n# html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = ""xarraydoc""\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    # \'papersize\': \'letterpaper\',\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    # \'pointsize\': \'10pt\',\n    # Additional stuff for the LaTeX preamble.\n    # \'preamble\': \'\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (""index"", ""xarray.tex"", ""xarray Documentation"", ""xarray Developers"", ""manual"")\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n# latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n# latex_use_parts = False\n\n# If true, show page references after internal links.\n# latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n# latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n# latex_appendices = []\n\n# If false, no module index is generated.\n# latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(""index"", ""xarray"", ""xarray Documentation"", [""xarray Developers""], 1)]\n\n# If true, show URL addresses after external links.\n# man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        ""index"",\n        ""xarray"",\n        ""xarray Documentation"",\n        ""xarray Developers"",\n        ""xarray"",\n        ""N-D labeled arrays and datasets in Python."",\n        ""Miscellaneous"",\n    )\n]\n\n# Documents to append as an appendix to all manuals.\n# texinfo_appendices = []\n\n# If false, no module index is generated.\n# texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n# texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n# texinfo_no_detailmenu = False\n\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    ""python"": (""https://docs.python.org/3/"", None),\n    ""pandas"": (""https://pandas.pydata.org/pandas-docs/stable"", None),\n    ""iris"": (""https://scitools.org.uk/iris/docs/latest"", None),\n    ""numpy"": (""https://numpy.org/doc/stable"", None),\n    ""scipy"": (""https://docs.scipy.org/doc/scipy/reference"", None),\n    ""numba"": (""https://numba.pydata.org/numba-doc/latest"", None),\n    ""matplotlib"": (""https://matplotlib.org"", None),\n    ""dask"": (""https://docs.dask.org/en/latest"", None),\n    ""cftime"": (""https://unidata.github.io/cftime"", None),\n}\n'"
properties/conftest.py,0,"b'try:\n    from hypothesis import settings\nexcept ImportError:\n    pass\nelse:\n    # Run for a while - arrays are a bigger search space than usual\n    settings.register_profile(""ci"", deadline=None, print_blob=True)\n    settings.load_profile(""ci"")\n'"
properties/test_encode_decode.py,0,"b'""""""\nProperty-based tests for encoding/decoding methods.\n\nThese ones pass, just as you\'d hope!\n\n""""""\nimport pytest  # isort:skip\n\npytest.importorskip(""hypothesis"")\n\nimport hypothesis.extra.numpy as npst\nimport hypothesis.strategies as st\nfrom hypothesis import given\n\nimport xarray as xr\n\nan_array = npst.arrays(\n    dtype=st.one_of(\n        npst.unsigned_integer_dtypes(), npst.integer_dtypes(), npst.floating_dtypes()\n    ),\n    shape=npst.array_shapes(max_side=3),  # max_side specified for performance\n)\n\n\n@pytest.mark.slow\n@given(st.data(), an_array)\ndef test_CFMask_coder_roundtrip(data, arr):\n    names = data.draw(\n        st.lists(st.text(), min_size=arr.ndim, max_size=arr.ndim, unique=True).map(\n            tuple\n        )\n    )\n    original = xr.Variable(names, arr)\n    coder = xr.coding.variables.CFMaskCoder()\n    roundtripped = coder.decode(coder.encode(original))\n    xr.testing.assert_identical(original, roundtripped)\n\n\n@pytest.mark.slow\n@given(st.data(), an_array)\ndef test_CFScaleOffset_coder_roundtrip(data, arr):\n    names = data.draw(\n        st.lists(st.text(), min_size=arr.ndim, max_size=arr.ndim, unique=True).map(\n            tuple\n        )\n    )\n    original = xr.Variable(names, arr)\n    coder = xr.coding.variables.CFScaleOffsetCoder()\n    roundtripped = coder.decode(coder.encode(original))\n    xr.testing.assert_identical(original, roundtripped)\n'"
properties/test_pandas_roundtrip.py,1,"b'""""""\nProperty-based tests for roundtripping between xarray and pandas objects.\n""""""\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nimport xarray as xr\n\npytest.importorskip(""hypothesis"")\nimport hypothesis.extra.numpy as npst  # isort:skip\nimport hypothesis.extra.pandas as pdst  # isort:skip\nimport hypothesis.strategies as st  # isort:skip\nfrom hypothesis import given  # isort:skip\n\nnumeric_dtypes = st.one_of(\n    npst.unsigned_integer_dtypes(), npst.integer_dtypes(), npst.floating_dtypes()\n)\n\nnumeric_series = numeric_dtypes.flatmap(lambda dt: pdst.series(dtype=dt))\n\nan_array = npst.arrays(\n    dtype=numeric_dtypes,\n    shape=npst.array_shapes(max_dims=2),  # can only convert 1D/2D to pandas\n)\n\n\n@st.composite\ndef datasets_1d_vars(draw):\n    """"""Generate datasets with only 1D variables\n\n    Suitable for converting to pandas dataframes.\n    """"""\n    # Generate an index for the dataset\n    idx = draw(pdst.indexes(dtype=""u8"", min_size=0, max_size=100))\n\n    # Generate 1-3 variables, 1D with the same length as the index\n    vars_strategy = st.dictionaries(\n        keys=st.text(),\n        values=npst.arrays(dtype=numeric_dtypes, shape=len(idx)).map(\n            partial(xr.Variable, (""rows"",))\n        ),\n        min_size=1,\n        max_size=3,\n    )\n    return xr.Dataset(draw(vars_strategy), coords={""rows"": idx})\n\n\n@given(st.data(), an_array)\ndef test_roundtrip_dataarray(data, arr):\n    names = data.draw(\n        st.lists(st.text(), min_size=arr.ndim, max_size=arr.ndim, unique=True).map(\n            tuple\n        )\n    )\n    coords = {name: np.arange(n) for (name, n) in zip(names, arr.shape)}\n    original = xr.DataArray(arr, dims=names, coords=coords)\n    roundtripped = xr.DataArray(original.to_pandas())\n    xr.testing.assert_identical(original, roundtripped)\n\n\n@given(datasets_1d_vars())\ndef test_roundtrip_dataset(dataset):\n    df = dataset.to_dataframe()\n    assert isinstance(df, pd.DataFrame)\n    roundtripped = xr.Dataset(df)\n    xr.testing.assert_identical(dataset, roundtripped)\n\n\n@given(numeric_series, st.text())\ndef test_roundtrip_pandas_series(ser, ix_name):\n    # Need to name the index, otherwise Xarray calls it \'dim_0\'.\n    ser.index.name = ix_name\n    arr = xr.DataArray(ser)\n    roundtripped = arr.to_pandas()\n    pd.testing.assert_series_equal(ser, roundtripped)\n    xr.testing.assert_identical(arr, roundtripped.to_xarray())\n\n\n# Dataframes with columns of all the same dtype - for roundtrip to DataArray\nnumeric_homogeneous_dataframe = numeric_dtypes.flatmap(\n    lambda dt: pdst.data_frames(columns=pdst.columns([""a"", ""b"", ""c""], dtype=dt))\n)\n\n\n@pytest.mark.xfail\n@given(numeric_homogeneous_dataframe)\ndef test_roundtrip_pandas_dataframe(df):\n    # Need to name the indexes, otherwise Xarray names them \'dim_0\', \'dim_1\'.\n    df.index.name = ""rows""\n    df.columns.name = ""cols""\n    arr = xr.DataArray(df)\n    roundtripped = arr.to_pandas()\n    pd.testing.assert_frame_equal(df, roundtripped)\n    xr.testing.assert_identical(arr, roundtripped.to_xarray())\n'"
xarray/__init__.py,0,"b'import pkg_resources\n\nfrom . import testing, tutorial, ufuncs\nfrom .backends.api import (\n    load_dataarray,\n    load_dataset,\n    open_dataarray,\n    open_dataset,\n    open_mfdataset,\n    save_mfdataset,\n)\nfrom .backends.rasterio_ import open_rasterio\nfrom .backends.zarr import open_zarr\nfrom .coding.cftime_offsets import cftime_range\nfrom .coding.cftimeindex import CFTimeIndex\nfrom .coding.frequencies import infer_freq\nfrom .conventions import SerializationWarning, decode_cf\nfrom .core.alignment import align, broadcast\nfrom .core.combine import auto_combine, combine_by_coords, combine_nested\nfrom .core.common import ALL_DIMS, full_like, ones_like, zeros_like\nfrom .core.computation import apply_ufunc, corr, cov, dot, polyval, where\nfrom .core.concat import concat\nfrom .core.dataarray import DataArray\nfrom .core.dataset import Dataset\nfrom .core.extensions import register_dataarray_accessor, register_dataset_accessor\nfrom .core.merge import MergeError, merge\nfrom .core.options import set_options\nfrom .core.parallel import map_blocks\nfrom .core.variable import Coordinate, IndexVariable, Variable, as_variable\nfrom .util.print_versions import show_versions\n\ntry:\n    __version__ = pkg_resources.get_distribution(""xarray"").version\nexcept Exception:\n    # Local copy or not installed with setuptools.\n    # Disable minimum version checks on downstream libraries.\n    __version__ = ""999""\n\n# A hardcoded __all__ variable is necessary to appease\n# `mypy --strict` running in projects that import xarray.\n__all__ = (\n    # Sub-packages\n    ""ufuncs"",\n    ""testing"",\n    ""tutorial"",\n    # Top-level functions\n    ""align"",\n    ""apply_ufunc"",\n    ""as_variable"",\n    ""auto_combine"",\n    ""broadcast"",\n    ""cftime_range"",\n    ""combine_by_coords"",\n    ""combine_nested"",\n    ""concat"",\n    ""decode_cf"",\n    ""dot"",\n    ""cov"",\n    ""corr"",\n    ""full_like"",\n    ""infer_freq"",\n    ""load_dataarray"",\n    ""load_dataset"",\n    ""map_blocks"",\n    ""merge"",\n    ""ones_like"",\n    ""open_dataarray"",\n    ""open_dataset"",\n    ""open_mfdataset"",\n    ""open_rasterio"",\n    ""open_zarr"",\n    ""polyval"",\n    ""register_dataarray_accessor"",\n    ""register_dataset_accessor"",\n    ""save_mfdataset"",\n    ""set_options"",\n    ""show_versions"",\n    ""where"",\n    ""zeros_like"",\n    # Classes\n    ""CFTimeIndex"",\n    ""Coordinate"",\n    ""DataArray"",\n    ""Dataset"",\n    ""IndexVariable"",\n    ""Variable"",\n    # Exceptions\n    ""MergeError"",\n    ""SerializationWarning"",\n    # Constants\n    ""__version__"",\n    ""ALL_DIMS"",\n)\n'"
xarray/conventions.py,30,"b'import warnings\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\n\nfrom .coding import strings, times, variables\nfrom .coding.variables import SerializationWarning, pop_to\nfrom .core import duck_array_ops, indexing\nfrom .core.common import contains_cftime_datetimes\nfrom .core.pycompat import dask_array_type\nfrom .core.variable import IndexVariable, Variable, as_variable\n\n\nclass NativeEndiannessArray(indexing.ExplicitlyIndexedNDArrayMixin):\n    """"""Decode arrays on the fly from non-native to native endianness\n\n    This is useful for decoding arrays from netCDF3 files (which are all\n    big endian) into native endianness, so they can be used with Cython\n    functions, such as those found in bottleneck and pandas.\n\n    >>> x = np.arange(5, dtype="">i2"")\n\n    >>> x.dtype\n    dtype(\'>i2\')\n\n    >>> NativeEndianArray(x).dtype\n    dtype(\'int16\')\n\n    >>> NativeEndianArray(x)[:].dtype\n    dtype(\'int16\')\n    """"""\n\n    __slots__ = (""array"",)\n\n    def __init__(self, array):\n        self.array = indexing.as_indexable(array)\n\n    @property\n    def dtype(self):\n        return np.dtype(self.array.dtype.kind + str(self.array.dtype.itemsize))\n\n    def __getitem__(self, key):\n        return np.asarray(self.array[key], dtype=self.dtype)\n\n\nclass BoolTypeArray(indexing.ExplicitlyIndexedNDArrayMixin):\n    """"""Decode arrays on the fly from integer to boolean datatype\n\n    This is useful for decoding boolean arrays from integer typed netCDF\n    variables.\n\n    >>> x = np.array([1, 0, 1, 1, 0], dtype=""i1"")\n\n    >>> x.dtype\n    dtype(\'>i2\')\n\n    >>> BoolTypeArray(x).dtype\n    dtype(\'bool\')\n\n    >>> BoolTypeArray(x)[:].dtype\n    dtype(\'bool\')\n    """"""\n\n    __slots__ = (""array"",)\n\n    def __init__(self, array):\n        self.array = indexing.as_indexable(array)\n\n    @property\n    def dtype(self):\n        return np.dtype(""bool"")\n\n    def __getitem__(self, key):\n        return np.asarray(self.array[key], dtype=self.dtype)\n\n\ndef _var_as_tuple(var):\n    return var.dims, var.data, var.attrs.copy(), var.encoding.copy()\n\n\ndef maybe_encode_nonstring_dtype(var, name=None):\n    if ""dtype"" in var.encoding and var.encoding[""dtype""] not in (""S1"", str):\n        dims, data, attrs, encoding = _var_as_tuple(var)\n        dtype = np.dtype(encoding.pop(""dtype""))\n        if dtype != var.dtype:\n            if np.issubdtype(dtype, np.integer):\n                if (\n                    np.issubdtype(var.dtype, np.floating)\n                    and ""_FillValue"" not in var.attrs\n                    and ""missing_value"" not in var.attrs\n                ):\n                    warnings.warn(\n                        ""saving variable %s with floating ""\n                        ""point data as an integer dtype without ""\n                        ""any _FillValue to use for NaNs"" % name,\n                        SerializationWarning,\n                        stacklevel=10,\n                    )\n                data = duck_array_ops.around(data)[...]\n            data = data.astype(dtype=dtype)\n        var = Variable(dims, data, attrs, encoding)\n    return var\n\n\ndef maybe_default_fill_value(var):\n    # make NaN the fill value for float types:\n    if (\n        ""_FillValue"" not in var.attrs\n        and ""_FillValue"" not in var.encoding\n        and np.issubdtype(var.dtype, np.floating)\n    ):\n        var.attrs[""_FillValue""] = var.dtype.type(np.nan)\n    return var\n\n\ndef maybe_encode_bools(var):\n    if (\n        (var.dtype == np.bool)\n        and (""dtype"" not in var.encoding)\n        and (""dtype"" not in var.attrs)\n    ):\n        dims, data, attrs, encoding = _var_as_tuple(var)\n        attrs[""dtype""] = ""bool""\n        data = data.astype(dtype=""i1"", copy=True)\n        var = Variable(dims, data, attrs, encoding)\n    return var\n\n\ndef _infer_dtype(array, name=None):\n    """"""Given an object array with no missing values, infer its dtype from its\n    first element\n    """"""\n    if array.dtype.kind != ""O"":\n        raise TypeError(""infer_type must be called on a dtype=object array"")\n\n    if array.size == 0:\n        return np.dtype(float)\n\n    element = array[(0,) * array.ndim]\n    if isinstance(element, (bytes, str)):\n        return strings.create_vlen_dtype(type(element))\n\n    dtype = np.array(element).dtype\n    if dtype.kind != ""O"":\n        return dtype\n\n    raise ValueError(\n        ""unable to infer dtype on variable {!r}; xarray ""\n        ""cannot serialize arbitrary Python objects"".format(name)\n    )\n\n\ndef ensure_not_multiindex(var, name=None):\n    if isinstance(var, IndexVariable) and isinstance(var.to_index(), pd.MultiIndex):\n        raise NotImplementedError(\n            ""variable {!r} is a MultiIndex, which cannot yet be ""\n            ""serialized to netCDF files ""\n            ""(https://github.com/pydata/xarray/issues/1077). Use ""\n            ""reset_index() to convert MultiIndex levels into coordinate ""\n            ""variables instead."".format(name)\n        )\n\n\ndef _copy_with_dtype(data, dtype):\n    """"""Create a copy of an array with the given dtype.\n\n    We use this instead of np.array() to ensure that custom object dtypes end\n    up on the resulting array.\n    """"""\n    result = np.empty(data.shape, dtype)\n    result[...] = data\n    return result\n\n\ndef ensure_dtype_not_object(var, name=None):\n    # TODO: move this from conventions to backends? (it\'s not CF related)\n    if var.dtype.kind == ""O"":\n        dims, data, attrs, encoding = _var_as_tuple(var)\n\n        if isinstance(data, dask_array_type):\n            warnings.warn(\n                ""variable {} has data in the form of a dask array with ""\n                ""dtype=object, which means it is being loaded into memory ""\n                ""to determine a data type that can be safely stored on disk. ""\n                ""To avoid this, coerce this variable to a fixed-size dtype ""\n                ""with astype() before saving it."".format(name),\n                SerializationWarning,\n            )\n            data = data.compute()\n\n        missing = pd.isnull(data)\n        if missing.any():\n            # nb. this will fail for dask.array data\n            non_missing_values = data[~missing]\n            inferred_dtype = _infer_dtype(non_missing_values, name)\n\n            # There is no safe bit-pattern for NA in typical binary string\n            # formats, we so can\'t set a fill_value. Unfortunately, this means\n            # we can\'t distinguish between missing values and empty strings.\n            if strings.is_bytes_dtype(inferred_dtype):\n                fill_value = b""""\n            elif strings.is_unicode_dtype(inferred_dtype):\n                fill_value = """"\n            else:\n                # insist on using float for numeric values\n                if not np.issubdtype(inferred_dtype, np.floating):\n                    inferred_dtype = np.dtype(float)\n                fill_value = inferred_dtype.type(np.nan)\n\n            data = _copy_with_dtype(data, dtype=inferred_dtype)\n            data[missing] = fill_value\n        else:\n            data = _copy_with_dtype(data, dtype=_infer_dtype(data, name))\n\n        assert data.dtype.kind != ""O"" or data.dtype.metadata\n        var = Variable(dims, data, attrs, encoding)\n    return var\n\n\ndef encode_cf_variable(var, needs_copy=True, name=None):\n    """"""\n    Converts an Variable into an Variable which follows some\n    of the CF conventions:\n\n        - Nans are masked using _FillValue (or the deprecated missing_value)\n        - Rescaling via: scale_factor and add_offset\n        - datetimes are converted to the CF \'units since time\' format\n        - dtype encodings are enforced.\n\n    Parameters\n    ----------\n    var : xarray.Variable\n        A variable holding un-encoded data.\n\n    Returns\n    -------\n    out : xarray.Variable\n        A variable which has been encoded as described above.\n    """"""\n    ensure_not_multiindex(var, name=name)\n\n    for coder in [\n        times.CFDatetimeCoder(),\n        times.CFTimedeltaCoder(),\n        variables.CFScaleOffsetCoder(),\n        variables.CFMaskCoder(),\n        variables.UnsignedIntegerCoder(),\n    ]:\n        var = coder.encode(var, name=name)\n\n    # TODO(shoyer): convert all of these to use coders, too:\n    var = maybe_encode_nonstring_dtype(var, name=name)\n    var = maybe_default_fill_value(var)\n    var = maybe_encode_bools(var)\n    var = ensure_dtype_not_object(var, name=name)\n    return var\n\n\ndef decode_cf_variable(\n    name,\n    var,\n    concat_characters=True,\n    mask_and_scale=True,\n    decode_times=True,\n    decode_endianness=True,\n    stack_char_dim=True,\n    use_cftime=None,\n    decode_timedelta=None,\n):\n    """"""\n    Decodes a variable which may hold CF encoded information.\n\n    This includes variables that have been masked and scaled, which\n    hold CF style time variables (this is almost always the case if\n    the dataset has been serialized) and which have strings encoded\n    as character arrays.\n\n    Parameters\n    ----------\n    name: str\n        Name of the variable. Used for better error messages.\n    var : Variable\n        A variable holding potentially CF encoded information.\n    concat_characters : bool\n        Should character arrays be concatenated to strings, for\n        example: [\'h\', \'e\', \'l\', \'l\', \'o\'] -> \'hello\'\n    mask_and_scale: bool\n        Lazily scale (using scale_factor and add_offset) and mask\n        (using _FillValue). If the _Unsigned attribute is present\n        treat integer arrays as unsigned.\n    decode_times : bool\n        Decode cf times (\'hours since 2000-01-01\') to np.datetime64.\n    decode_endianness : bool\n        Decode arrays from non-native to native endianness.\n    stack_char_dim : bool\n        Whether to stack characters into bytes along the last dimension of this\n        array. Passed as an argument because we need to look at the full\n        dataset to figure out if this is appropriate.\n    use_cftime: bool, optional\n        Only relevant if encoded dates come from a standard calendar\n        (e.g. \'gregorian\', \'proleptic_gregorian\', \'standard\', or not\n        specified).  If None (default), attempt to decode times to\n        ``np.datetime64[ns]`` objects; if this is not possible, decode times to\n        ``cftime.datetime`` objects. If True, always decode times to\n        ``cftime.datetime`` objects, regardless of whether or not they can be\n        represented using ``np.datetime64[ns]`` objects.  If False, always\n        decode times to ``np.datetime64[ns]`` objects; if this is not possible\n        raise an error.\n\n    Returns\n    -------\n    out : Variable\n        A variable holding the decoded equivalent of var.\n    """"""\n    var = as_variable(var)\n    original_dtype = var.dtype\n\n    if decode_timedelta is None:\n        decode_timedelta = decode_times\n\n    if concat_characters:\n        if stack_char_dim:\n            var = strings.CharacterArrayCoder().decode(var, name=name)\n        var = strings.EncodedStringCoder().decode(var)\n\n    if mask_and_scale:\n        for coder in [\n            variables.UnsignedIntegerCoder(),\n            variables.CFMaskCoder(),\n            variables.CFScaleOffsetCoder(),\n        ]:\n            var = coder.decode(var, name=name)\n\n    if decode_timedelta:\n        var = times.CFTimedeltaCoder().decode(var, name=name)\n    if decode_times:\n        var = times.CFDatetimeCoder(use_cftime=use_cftime).decode(var, name=name)\n\n    dimensions, data, attributes, encoding = variables.unpack_for_decoding(var)\n    # TODO(shoyer): convert everything below to use coders\n\n    if decode_endianness and not data.dtype.isnative:\n        # do this last, so it\'s only done if we didn\'t already unmask/scale\n        data = NativeEndiannessArray(data)\n        original_dtype = data.dtype\n\n    encoding.setdefault(""dtype"", original_dtype)\n\n    if ""dtype"" in attributes and attributes[""dtype""] == ""bool"":\n        del attributes[""dtype""]\n        data = BoolTypeArray(data)\n\n    if not isinstance(data, dask_array_type):\n        data = indexing.LazilyOuterIndexedArray(data)\n\n    return Variable(dimensions, data, attributes, encoding=encoding)\n\n\ndef _update_bounds_attributes(variables):\n    """"""Adds time attributes to time bounds variables.\n\n    Variables handling time bounds (""Cell boundaries"" in the CF\n    conventions) do not necessarily carry the necessary attributes to be\n    decoded. This copies the attributes from the time variable to the\n    associated boundaries.\n\n    See Also:\n\n    http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/\n         cf-conventions.html#cell-boundaries\n\n    https://github.com/pydata/xarray/issues/2565\n    """"""\n\n    # For all time variables with bounds\n    for v in variables.values():\n        attrs = v.attrs\n        has_date_units = ""units"" in attrs and ""since"" in attrs[""units""]\n        if has_date_units and ""bounds"" in attrs:\n            if attrs[""bounds""] in variables:\n                bounds_attrs = variables[attrs[""bounds""]].attrs\n                bounds_attrs.setdefault(""units"", attrs[""units""])\n                if ""calendar"" in attrs:\n                    bounds_attrs.setdefault(""calendar"", attrs[""calendar""])\n\n\ndef _update_bounds_encoding(variables):\n    """"""Adds time encoding to time bounds variables.\n\n    Variables handling time bounds (""Cell boundaries"" in the CF\n    conventions) do not necessarily carry the necessary attributes to be\n    decoded. This copies the encoding from the time variable to the\n    associated bounds variable so that we write CF-compliant files.\n\n    See Also:\n\n    http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/\n         cf-conventions.html#cell-boundaries\n\n    https://github.com/pydata/xarray/issues/2565\n    """"""\n\n    # For all time variables with bounds\n    for v in variables.values():\n        attrs = v.attrs\n        encoding = v.encoding\n        has_date_units = ""units"" in encoding and ""since"" in encoding[""units""]\n        is_datetime_type = np.issubdtype(\n            v.dtype, np.datetime64\n        ) or contains_cftime_datetimes(v)\n\n        if (\n            is_datetime_type\n            and not has_date_units\n            and ""bounds"" in attrs\n            and attrs[""bounds""] in variables\n        ):\n            warnings.warn(\n                ""Variable \'{0}\' has datetime type and a ""\n                ""bounds variable but {0}.encoding does not have ""\n                ""units specified. The units encodings for \'{0}\' ""\n                ""and \'{1}\' will be determined independently ""\n                ""and may not be equal, counter to CF-conventions. ""\n                ""If this is a concern, specify a units encoding for ""\n                ""\'{0}\' before writing to a file."".format(v.name, attrs[""bounds""]),\n                UserWarning,\n            )\n\n        if has_date_units and ""bounds"" in attrs:\n            if attrs[""bounds""] in variables:\n                bounds_encoding = variables[attrs[""bounds""]].encoding\n                bounds_encoding.setdefault(""units"", encoding[""units""])\n                if ""calendar"" in encoding:\n                    bounds_encoding.setdefault(""calendar"", encoding[""calendar""])\n\n\ndef decode_cf_variables(\n    variables,\n    attributes,\n    concat_characters=True,\n    mask_and_scale=True,\n    decode_times=True,\n    decode_coords=True,\n    drop_variables=None,\n    use_cftime=None,\n    decode_timedelta=None,\n):\n    """"""\n    Decode several CF encoded variables.\n\n    See: decode_cf_variable\n    """"""\n    dimensions_used_by = defaultdict(list)\n    for v in variables.values():\n        for d in v.dims:\n            dimensions_used_by[d].append(v)\n\n    def stackable(dim):\n        # figure out if a dimension can be concatenated over\n        if dim in variables:\n            return False\n        for v in dimensions_used_by[dim]:\n            if v.dtype.kind != ""S"" or dim != v.dims[-1]:\n                return False\n        return True\n\n    coord_names = set()\n\n    if isinstance(drop_variables, str):\n        drop_variables = [drop_variables]\n    elif drop_variables is None:\n        drop_variables = []\n    drop_variables = set(drop_variables)\n\n    # Time bounds coordinates might miss the decoding attributes\n    if decode_times:\n        _update_bounds_attributes(variables)\n\n    new_vars = {}\n    for k, v in variables.items():\n        if k in drop_variables:\n            continue\n        stack_char_dim = (\n            concat_characters\n            and v.dtype == ""S1""\n            and v.ndim > 0\n            and stackable(v.dims[-1])\n        )\n        new_vars[k] = decode_cf_variable(\n            k,\n            v,\n            concat_characters=concat_characters,\n            mask_and_scale=mask_and_scale,\n            decode_times=decode_times,\n            stack_char_dim=stack_char_dim,\n            use_cftime=use_cftime,\n            decode_timedelta=decode_timedelta,\n        )\n        if decode_coords:\n            var_attrs = new_vars[k].attrs\n            if ""coordinates"" in var_attrs:\n                coord_str = var_attrs[""coordinates""]\n                var_coord_names = coord_str.split()\n                if all(k in variables for k in var_coord_names):\n                    new_vars[k].encoding[""coordinates""] = coord_str\n                    del var_attrs[""coordinates""]\n                    coord_names.update(var_coord_names)\n\n    if decode_coords and ""coordinates"" in attributes:\n        attributes = dict(attributes)\n        coord_names.update(attributes.pop(""coordinates"").split())\n\n    return new_vars, attributes, coord_names\n\n\ndef decode_cf(\n    obj,\n    concat_characters=True,\n    mask_and_scale=True,\n    decode_times=True,\n    decode_coords=True,\n    drop_variables=None,\n    use_cftime=None,\n    decode_timedelta=None,\n):\n    """"""Decode the given Dataset or Datastore according to CF conventions into\n    a new Dataset.\n\n    Parameters\n    ----------\n    obj : Dataset or DataStore\n        Object to decode.\n    concat_characters : bool, optional\n        Should character arrays be concatenated to strings, for\n        example: [\'h\', \'e\', \'l\', \'l\', \'o\'] -> \'hello\'\n    mask_and_scale: bool, optional\n        Lazily scale (using scale_factor and add_offset) and mask\n        (using _FillValue).\n    decode_times : bool, optional\n        Decode cf times (e.g., integers since \'hours since 2000-01-01\') to\n        np.datetime64.\n    decode_coords : bool, optional\n        Use the \'coordinates\' attribute on variable (or the dataset itself) to\n        identify coordinates.\n    drop_variables: string or iterable, optional\n        A variable or list of variables to exclude from being parsed from the\n        dataset. This may be useful to drop variables with problems or\n        inconsistent values.\n    use_cftime: bool, optional\n        Only relevant if encoded dates come from a standard calendar\n        (e.g. \'gregorian\', \'proleptic_gregorian\', \'standard\', or not\n        specified).  If None (default), attempt to decode times to\n        ``np.datetime64[ns]`` objects; if this is not possible, decode times to\n        ``cftime.datetime`` objects. If True, always decode times to\n        ``cftime.datetime`` objects, regardless of whether or not they can be\n        represented using ``np.datetime64[ns]`` objects.  If False, always\n        decode times to ``np.datetime64[ns]`` objects; if this is not possible\n        raise an error.\n    decode_timedelta : bool, optional\n        If True, decode variables and coordinates with time units in\n        {\'days\', \'hours\', \'minutes\', \'seconds\', \'milliseconds\', \'microseconds\'}\n        into timedelta objects. If False, leave them encoded as numbers.\n        If None (default), assume the same value of decode_time.\n\n    Returns\n    -------\n    decoded : Dataset\n    """"""\n    from .core.dataset import Dataset\n    from .backends.common import AbstractDataStore\n\n    if isinstance(obj, Dataset):\n        vars = obj._variables\n        attrs = obj.attrs\n        extra_coords = set(obj.coords)\n        file_obj = obj._file_obj\n        encoding = obj.encoding\n    elif isinstance(obj, AbstractDataStore):\n        vars, attrs = obj.load()\n        extra_coords = set()\n        file_obj = obj\n        encoding = obj.get_encoding()\n    else:\n        raise TypeError(""can only decode Dataset or DataStore objects"")\n\n    vars, attrs, coord_names = decode_cf_variables(\n        vars,\n        attrs,\n        concat_characters,\n        mask_and_scale,\n        decode_times,\n        decode_coords,\n        drop_variables=drop_variables,\n        use_cftime=use_cftime,\n        decode_timedelta=decode_timedelta,\n    )\n    ds = Dataset(vars, attrs=attrs)\n    ds = ds.set_coords(coord_names.union(extra_coords).intersection(vars))\n    ds._file_obj = file_obj\n    ds.encoding = encoding\n\n    return ds\n\n\ndef cf_decoder(\n    variables,\n    attributes,\n    concat_characters=True,\n    mask_and_scale=True,\n    decode_times=True,\n):\n    """"""\n    Decode a set of CF encoded variables and attributes.\n\n    Parameters\n    ----------\n    variables : dict\n        A dictionary mapping from variable name to xarray.Variable\n    attributes : dict\n        A dictionary mapping from attribute name to value\n    concat_characters : bool\n        Should character arrays be concatenated to strings, for\n        example: [\'h\', \'e\', \'l\', \'l\', \'o\'] -> \'hello\'\n    mask_and_scale: bool\n        Lazily scale (using scale_factor and add_offset) and mask\n        (using _FillValue).\n    decode_times : bool\n        Decode cf times (\'hours since 2000-01-01\') to np.datetime64.\n\n    Returns\n    -------\n    decoded_variables : dict\n        A dictionary mapping from variable name to xarray.Variable objects.\n    decoded_attributes : dict\n        A dictionary mapping from attribute name to values.\n\n    See also\n    --------\n    decode_cf_variable\n    """"""\n    variables, attributes, _ = decode_cf_variables(\n        variables, attributes, concat_characters, mask_and_scale, decode_times\n    )\n    return variables, attributes\n\n\ndef _encode_coordinates(variables, attributes, non_dim_coord_names):\n    # calculate global and variable specific coordinates\n    non_dim_coord_names = set(non_dim_coord_names)\n\n    for name in list(non_dim_coord_names):\n        if isinstance(name, str) and "" "" in name:\n            warnings.warn(\n                ""coordinate {!r} has a space in its name, which means it ""\n                ""cannot be marked as a coordinate on disk and will be ""\n                ""saved as a data variable instead"".format(name),\n                SerializationWarning,\n                stacklevel=6,\n            )\n            non_dim_coord_names.discard(name)\n\n    global_coordinates = non_dim_coord_names.copy()\n    variable_coordinates = defaultdict(set)\n    for coord_name in non_dim_coord_names:\n        target_dims = variables[coord_name].dims\n        for k, v in variables.items():\n            if (\n                k not in non_dim_coord_names\n                and k not in v.dims\n                and set(target_dims) <= set(v.dims)\n            ):\n                variable_coordinates[k].add(coord_name)\n\n    variables = {k: v.copy(deep=False) for k, v in variables.items()}\n\n    # keep track of variable names written to file under the ""coordinates"" attributes\n    written_coords = set()\n    for name, var in variables.items():\n        encoding = var.encoding\n        attrs = var.attrs\n        if ""coordinates"" in attrs and ""coordinates"" in encoding:\n            raise ValueError(\n                f""\'coordinates\' found in both attrs and encoding for variable {name!r}.""\n            )\n\n        # this will copy coordinates from encoding to attrs if ""coordinates"" in attrs\n        # after the next line, ""coordinates"" is never in encoding\n        # we get support for attrs[""coordinates""] for free.\n        coords_str = pop_to(encoding, attrs, ""coordinates"")\n        if not coords_str and variable_coordinates[name]:\n            attrs[""coordinates""] = "" "".join(map(str, variable_coordinates[name]))\n        if ""coordinates"" in attrs:\n            written_coords.update(attrs[""coordinates""].split())\n\n    # These coordinates are not associated with any particular variables, so we\n    # save them under a global \'coordinates\' attribute so xarray can roundtrip\n    # the dataset faithfully. Because this serialization goes beyond CF\n    # conventions, only do it if necessary.\n    # Reference discussion:\n    # http://mailman.cgd.ucar.edu/pipermail/cf-metadata/2014/007571.html\n    global_coordinates.difference_update(written_coords)\n    if global_coordinates:\n        attributes = dict(attributes)\n        if ""coordinates"" in attributes:\n            warnings.warn(\n                f""cannot serialize global coordinates {global_coordinates!r} because the global ""\n                f""attribute \'coordinates\' already exists. This may prevent faithful roundtripping""\n                f""of xarray datasets"",\n                SerializationWarning,\n            )\n        else:\n            attributes[""coordinates""] = "" "".join(map(str, global_coordinates))\n\n    return variables, attributes\n\n\ndef encode_dataset_coordinates(dataset):\n    """"""Encode coordinates on the given dataset object into variable specific\n    and global attributes.\n\n    When possible, this is done according to CF conventions.\n\n    Parameters\n    ----------\n    dataset : Dataset\n        Object to encode.\n\n    Returns\n    -------\n    variables : dict\n    attrs : dict\n    """"""\n    non_dim_coord_names = set(dataset.coords) - set(dataset.dims)\n    return _encode_coordinates(\n        dataset._variables, dataset.attrs, non_dim_coord_names=non_dim_coord_names\n    )\n\n\ndef cf_encoder(variables, attributes):\n    """"""\n    Encode a set of CF encoded variables and attributes.\n    Takes a dicts of variables and attributes and encodes them\n    to conform to CF conventions as much as possible.\n    This includes masking, scaling, character array handling,\n    and CF-time encoding.\n\n\n    Parameters\n    ----------\n    variables : dict\n        A dictionary mapping from variable name to xarray.Variable\n    attributes : dict\n        A dictionary mapping from attribute name to value\n\n    Returns\n    -------\n    encoded_variables : dict\n        A dictionary mapping from variable name to xarray.Variable,\n    encoded_attributes : dict\n        A dictionary mapping from attribute name to value\n\n    See also\n    --------\n    decode_cf_variable, encode_cf_variable\n    """"""\n\n    # add encoding for time bounds variables if present.\n    _update_bounds_encoding(variables)\n\n    new_vars = {k: encode_cf_variable(v, name=k) for k, v in variables.items()}\n\n    # Remove attrs from bounds variables (issue #2921)\n    for var in new_vars.values():\n        bounds = var.attrs[""bounds""] if ""bounds"" in var.attrs else None\n        if bounds and bounds in new_vars:\n            # see http://cfconventions.org/cf-conventions/cf-conventions.html#cell-boundaries\n            for attr in [\n                ""units"",\n                ""standard_name"",\n                ""axis"",\n                ""positive"",\n                ""calendar"",\n                ""long_name"",\n                ""leap_month"",\n                ""leap_year"",\n                ""month_lengths"",\n            ]:\n                if attr in new_vars[bounds].attrs and attr in var.attrs:\n                    if new_vars[bounds].attrs[attr] == var.attrs[attr]:\n                        new_vars[bounds].attrs.pop(attr)\n\n    return new_vars, attributes\n'"
xarray/convert.py,5,"b'""""""Functions for converting to and from xarray objects\n""""""\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\n\nfrom .coding.times import CFDatetimeCoder, CFTimedeltaCoder\nfrom .conventions import decode_cf\nfrom .core import duck_array_ops\nfrom .core.dataarray import DataArray\nfrom .core.dtypes import get_fill_value\n\ncdms2_ignored_attrs = {""name"", ""tileIndex""}\niris_forbidden_keys = {\n    ""standard_name"",\n    ""long_name"",\n    ""units"",\n    ""bounds"",\n    ""axis"",\n    ""calendar"",\n    ""leap_month"",\n    ""leap_year"",\n    ""month_lengths"",\n    ""coordinates"",\n    ""grid_mapping"",\n    ""climatology"",\n    ""cell_methods"",\n    ""formula_terms"",\n    ""compress"",\n    ""missing_value"",\n    ""add_offset"",\n    ""scale_factor"",\n    ""valid_max"",\n    ""valid_min"",\n    ""valid_range"",\n    ""_FillValue"",\n}\ncell_methods_strings = {\n    ""point"",\n    ""sum"",\n    ""maximum"",\n    ""median"",\n    ""mid_range"",\n    ""minimum"",\n    ""mean"",\n    ""mode"",\n    ""standard_deviation"",\n    ""variance"",\n}\n\n\ndef encode(var):\n    return CFTimedeltaCoder().encode(CFDatetimeCoder().encode(var.variable))\n\n\ndef _filter_attrs(attrs, ignored_attrs):\n    """""" Return attrs that are not in ignored_attrs\n    """"""\n    return {k: v for k, v in attrs.items() if k not in ignored_attrs}\n\n\ndef from_cdms2(variable):\n    """"""Convert a cdms2 variable into an DataArray\n    """"""\n    values = np.asarray(variable)\n    name = variable.id\n    dims = variable.getAxisIds()\n    coords = {}\n    for axis in variable.getAxisList():\n        coords[axis.id] = DataArray(\n            np.asarray(axis),\n            dims=[axis.id],\n            attrs=_filter_attrs(axis.attributes, cdms2_ignored_attrs),\n        )\n    grid = variable.getGrid()\n    if grid is not None:\n        ids = [a.id for a in grid.getAxisList()]\n        for axis in grid.getLongitude(), grid.getLatitude():\n            if axis.id not in variable.getAxisIds():\n                coords[axis.id] = DataArray(\n                    np.asarray(axis[:]),\n                    dims=ids,\n                    attrs=_filter_attrs(axis.attributes, cdms2_ignored_attrs),\n                )\n    attrs = _filter_attrs(variable.attributes, cdms2_ignored_attrs)\n    dataarray = DataArray(values, dims=dims, coords=coords, name=name, attrs=attrs)\n    return decode_cf(dataarray.to_dataset())[dataarray.name]\n\n\ndef to_cdms2(dataarray, copy=True):\n    """"""Convert a DataArray into a cdms2 variable\n    """"""\n    # we don\'t want cdms2 to be a hard dependency\n    import cdms2\n\n    def set_cdms2_attrs(var, attrs):\n        for k, v in attrs.items():\n            setattr(var, k, v)\n\n    # 1D axes\n    axes = []\n    for dim in dataarray.dims:\n        coord = encode(dataarray.coords[dim])\n        axis = cdms2.createAxis(coord.values, id=dim)\n        set_cdms2_attrs(axis, coord.attrs)\n        axes.append(axis)\n\n    # Data\n    var = encode(dataarray)\n    cdms2_var = cdms2.createVariable(\n        var.values, axes=axes, id=dataarray.name, mask=pd.isnull(var.values), copy=copy\n    )\n\n    # Attributes\n    set_cdms2_attrs(cdms2_var, var.attrs)\n\n    # Curvilinear and unstructured grids\n    if dataarray.name not in dataarray.coords:\n\n        cdms2_axes = {}\n        for coord_name in set(dataarray.coords.keys()) - set(dataarray.dims):\n\n            coord_array = dataarray.coords[coord_name].to_cdms2()\n\n            cdms2_axis_cls = (\n                cdms2.coord.TransientAxis2D\n                if coord_array.ndim\n                else cdms2.auxcoord.TransientAuxAxis1D\n            )\n            cdms2_axis = cdms2_axis_cls(coord_array)\n            if cdms2_axis.isLongitude():\n                cdms2_axes[""lon""] = cdms2_axis\n            elif cdms2_axis.isLatitude():\n                cdms2_axes[""lat""] = cdms2_axis\n\n        if ""lon"" in cdms2_axes and ""lat"" in cdms2_axes:\n            if len(cdms2_axes[""lon""].shape) == 2:\n                cdms2_grid = cdms2.hgrid.TransientCurveGrid(\n                    cdms2_axes[""lat""], cdms2_axes[""lon""]\n                )\n            else:\n                cdms2_grid = cdms2.gengrid.AbstractGenericGrid(\n                    cdms2_axes[""lat""], cdms2_axes[""lon""]\n                )\n            for axis in cdms2_grid.getAxisList():\n                cdms2_var.setAxis(cdms2_var.getAxisIds().index(axis.id), axis)\n            cdms2_var.setGrid(cdms2_grid)\n\n    return cdms2_var\n\n\ndef _pick_attrs(attrs, keys):\n    """""" Return attrs with keys in keys list\n    """"""\n    return {k: v for k, v in attrs.items() if k in keys}\n\n\ndef _get_iris_args(attrs):\n    """""" Converts the xarray attrs into args that can be passed into Iris\n    """"""\n    # iris.unit is deprecated in Iris v1.9\n    import cf_units\n\n    args = {""attributes"": _filter_attrs(attrs, iris_forbidden_keys)}\n    args.update(_pick_attrs(attrs, (""standard_name"", ""long_name"")))\n    unit_args = _pick_attrs(attrs, (""calendar"",))\n    if ""units"" in attrs:\n        args[""units""] = cf_units.Unit(attrs[""units""], **unit_args)\n    return args\n\n\n# TODO: Add converting bounds from xarray to Iris and back\ndef to_iris(dataarray):\n    """""" Convert a DataArray into a Iris Cube\n    """"""\n    # Iris not a hard dependency\n    import iris\n    from iris.fileformats.netcdf import parse_cell_methods\n\n    dim_coords = []\n    aux_coords = []\n\n    for coord_name in dataarray.coords:\n        coord = encode(dataarray.coords[coord_name])\n        coord_args = _get_iris_args(coord.attrs)\n        coord_args[""var_name""] = coord_name\n        axis = None\n        if coord.dims:\n            axis = dataarray.get_axis_num(coord.dims)\n        if coord_name in dataarray.dims:\n            try:\n                iris_coord = iris.coords.DimCoord(coord.values, **coord_args)\n                dim_coords.append((iris_coord, axis))\n            except ValueError:\n                iris_coord = iris.coords.AuxCoord(coord.values, **coord_args)\n                aux_coords.append((iris_coord, axis))\n        else:\n            iris_coord = iris.coords.AuxCoord(coord.values, **coord_args)\n            aux_coords.append((iris_coord, axis))\n\n    args = _get_iris_args(dataarray.attrs)\n    args[""var_name""] = dataarray.name\n    args[""dim_coords_and_dims""] = dim_coords\n    args[""aux_coords_and_dims""] = aux_coords\n    if ""cell_methods"" in dataarray.attrs:\n        args[""cell_methods""] = parse_cell_methods(dataarray.attrs[""cell_methods""])\n\n    masked_data = duck_array_ops.masked_invalid(dataarray.data)\n    cube = iris.cube.Cube(masked_data, **args)\n\n    return cube\n\n\ndef _iris_obj_to_attrs(obj):\n    """""" Return a dictionary of attrs when given a Iris object\n    """"""\n    attrs = {""standard_name"": obj.standard_name, ""long_name"": obj.long_name}\n    if obj.units.calendar:\n        attrs[""calendar""] = obj.units.calendar\n    if obj.units.origin != ""1"" and not obj.units.is_unknown():\n        attrs[""units""] = obj.units.origin\n    attrs.update(obj.attributes)\n    return {k: v for k, v in attrs.items() if v is not None}\n\n\ndef _iris_cell_methods_to_str(cell_methods_obj):\n    """""" Converts a Iris cell methods into a string\n    """"""\n    cell_methods = []\n    for cell_method in cell_methods_obj:\n        names = """".join(f""{n}: "" for n in cell_method.coord_names)\n        intervals = "" "".join(\n            f""interval: {interval}"" for interval in cell_method.intervals\n        )\n        comments = "" "".join(f""comment: {comment}"" for comment in cell_method.comments)\n        extra = "" "".join([intervals, comments]).strip()\n        if extra:\n            extra = f"" ({extra})""\n        cell_methods.append(names + cell_method.method + extra)\n    return "" "".join(cell_methods)\n\n\ndef _name(iris_obj, default=""unknown""):\n    """""" Mimicks `iris_obj.name()` but with different name resolution order.\n\n    Similar to iris_obj.name() method, but using iris_obj.var_name first to\n    enable roundtripping.\n    """"""\n    return iris_obj.var_name or iris_obj.standard_name or iris_obj.long_name or default\n\n\ndef from_iris(cube):\n    """""" Convert a Iris cube into an DataArray\n    """"""\n    import iris.exceptions\n    from xarray.core.pycompat import dask_array_type\n\n    name = _name(cube)\n    if name == ""unknown"":\n        name = None\n    dims = []\n    for i in range(cube.ndim):\n        try:\n            dim_coord = cube.coord(dim_coords=True, dimensions=(i,))\n            dims.append(_name(dim_coord))\n        except iris.exceptions.CoordinateNotFoundError:\n            dims.append(f""dim_{i}"")\n\n    if len(set(dims)) != len(dims):\n        duplicates = [k for k, v in Counter(dims).items() if v > 1]\n        raise ValueError(f""Duplicate coordinate name {duplicates}."")\n\n    coords = {}\n\n    for coord in cube.coords():\n        coord_attrs = _iris_obj_to_attrs(coord)\n        coord_dims = [dims[i] for i in cube.coord_dims(coord)]\n        if coord_dims:\n            coords[_name(coord)] = (coord_dims, coord.points, coord_attrs)\n        else:\n            coords[_name(coord)] = ((), coord.points.item(), coord_attrs)\n\n    array_attrs = _iris_obj_to_attrs(cube)\n    cell_methods = _iris_cell_methods_to_str(cube.cell_methods)\n    if cell_methods:\n        array_attrs[""cell_methods""] = cell_methods\n\n    # Deal with iris 1.* and 2.*\n    cube_data = cube.core_data() if hasattr(cube, ""core_data"") else cube.data\n\n    # Deal with dask and numpy masked arrays\n    if isinstance(cube_data, dask_array_type):\n        from dask.array import ma as dask_ma\n\n        filled_data = dask_ma.filled(cube_data, get_fill_value(cube.dtype))\n    elif isinstance(cube_data, np.ma.MaskedArray):\n        filled_data = np.ma.filled(cube_data, get_fill_value(cube.dtype))\n    else:\n        filled_data = cube_data\n\n    dataarray = DataArray(\n        filled_data, coords=coords, name=name, attrs=array_attrs, dims=dims\n    )\n    decoded_ds = decode_cf(dataarray._to_temp_dataset())\n    return dataarray._from_temp_dataset(decoded_ds)\n'"
xarray/testing.py,1,"b'""""""Testing functions exposed to the user API""""""\nfrom typing import Hashable, Set, Union\n\nimport numpy as np\nimport pandas as pd\n\nfrom xarray.core import duck_array_ops, formatting\nfrom xarray.core.dataarray import DataArray\nfrom xarray.core.dataset import Dataset\nfrom xarray.core.indexes import default_indexes\nfrom xarray.core.variable import IndexVariable, Variable\n\n__all__ = (""assert_allclose"", ""assert_chunks_equal"", ""assert_equal"", ""assert_identical"")\n\n\ndef _decode_string_data(data):\n    if data.dtype.kind == ""S"":\n        return np.core.defchararray.decode(data, ""utf-8"", ""replace"")\n    return data\n\n\ndef _data_allclose_or_equiv(arr1, arr2, rtol=1e-05, atol=1e-08, decode_bytes=True):\n    if any(arr.dtype.kind == ""S"" for arr in [arr1, arr2]) and decode_bytes:\n        arr1 = _decode_string_data(arr1)\n        arr2 = _decode_string_data(arr2)\n    exact_dtypes = [""M"", ""m"", ""O"", ""S"", ""U""]\n    if any(arr.dtype.kind in exact_dtypes for arr in [arr1, arr2]):\n        return duck_array_ops.array_equiv(arr1, arr2)\n    else:\n        return duck_array_ops.allclose_or_equiv(arr1, arr2, rtol=rtol, atol=atol)\n\n\ndef assert_equal(a, b):\n    """"""Like :py:func:`numpy.testing.assert_array_equal`, but for xarray\n    objects.\n\n    Raises an AssertionError if two objects are not equal. This will match\n    data values, dimensions and coordinates, but not names or attributes\n    (except for Dataset objects for which the variable names must match).\n    Arrays with NaN in the same location are considered equal.\n\n    Parameters\n    ----------\n    a : xarray.Dataset, xarray.DataArray or xarray.Variable\n        The first object to compare.\n    b : xarray.Dataset, xarray.DataArray or xarray.Variable\n        The second object to compare.\n\n    See also\n    --------\n    assert_identical, assert_allclose, Dataset.equals, DataArray.equals,\n    numpy.testing.assert_array_equal\n    """"""\n    __tracebackhide__ = True\n    assert type(a) == type(b)\n    if isinstance(a, (Variable, DataArray)):\n        assert a.equals(b), formatting.diff_array_repr(a, b, ""equals"")\n    elif isinstance(a, Dataset):\n        assert a.equals(b), formatting.diff_dataset_repr(a, b, ""equals"")\n    else:\n        raise TypeError(""{} not supported by assertion comparison"".format(type(a)))\n\n\ndef assert_identical(a, b):\n    """"""Like :py:func:`xarray.testing.assert_equal`, but also matches the\n    objects\' names and attributes.\n\n    Raises an AssertionError if two objects are not identical.\n\n    Parameters\n    ----------\n    a : xarray.Dataset, xarray.DataArray or xarray.Variable\n        The first object to compare.\n    b : xarray.Dataset, xarray.DataArray or xarray.Variable\n        The second object to compare.\n\n    See also\n    --------\n    assert_equal, assert_allclose, Dataset.equals, DataArray.equals\n    """"""\n    __tracebackhide__ = True\n    assert type(a) == type(b)\n    if isinstance(a, Variable):\n        assert a.identical(b), formatting.diff_array_repr(a, b, ""identical"")\n    elif isinstance(a, DataArray):\n        assert a.name == b.name\n        assert a.identical(b), formatting.diff_array_repr(a, b, ""identical"")\n    elif isinstance(a, (Dataset, Variable)):\n        assert a.identical(b), formatting.diff_dataset_repr(a, b, ""identical"")\n    else:\n        raise TypeError(""{} not supported by assertion comparison"".format(type(a)))\n\n\ndef assert_allclose(a, b, rtol=1e-05, atol=1e-08, decode_bytes=True):\n    """"""Like :py:func:`numpy.testing.assert_allclose`, but for xarray objects.\n\n    Raises an AssertionError if two objects are not equal up to desired\n    tolerance.\n\n    Parameters\n    ----------\n    a : xarray.Dataset, xarray.DataArray or xarray.Variable\n        The first object to compare.\n    b : xarray.Dataset, xarray.DataArray or xarray.Variable\n        The second object to compare.\n    rtol : float, optional\n        Relative tolerance.\n    atol : float, optional\n        Absolute tolerance.\n    decode_bytes : bool, optional\n        Whether byte dtypes should be decoded to strings as UTF-8 or not.\n        This is useful for testing serialization methods on Python 3 that\n        return saved strings as bytes.\n\n    See also\n    --------\n    assert_identical, assert_equal, numpy.testing.assert_allclose\n    """"""\n    __tracebackhide__ = True\n    assert type(a) == type(b)\n    kwargs = dict(rtol=rtol, atol=atol, decode_bytes=decode_bytes)\n    if isinstance(a, Variable):\n        assert a.dims == b.dims\n        allclose = _data_allclose_or_equiv(a.values, b.values, **kwargs)\n        assert allclose, f""{a.values}\\n{b.values}""\n    elif isinstance(a, DataArray):\n        assert_allclose(a.variable, b.variable, **kwargs)\n        assert set(a.coords) == set(b.coords)\n        for v in a.coords.variables:\n            # can\'t recurse with this function as coord is sometimes a\n            # DataArray, so call into _data_allclose_or_equiv directly\n            allclose = _data_allclose_or_equiv(\n                a.coords[v].values, b.coords[v].values, **kwargs\n            )\n            assert allclose, ""{}\\n{}"".format(a.coords[v].values, b.coords[v].values)\n    elif isinstance(a, Dataset):\n        assert set(a.data_vars) == set(b.data_vars)\n        assert set(a.coords) == set(b.coords)\n        for k in list(a.variables) + list(a.coords):\n            assert_allclose(a[k], b[k], **kwargs)\n\n    else:\n        raise TypeError(""{} not supported by assertion comparison"".format(type(a)))\n\n\ndef assert_chunks_equal(a, b):\n    """"""\n    Assert that chunksizes along chunked dimensions are equal.\n\n    Parameters\n    ----------\n    a : xarray.Dataset or xarray.DataArray\n        The first object to compare.\n    b : xarray.Dataset or xarray.DataArray\n        The second object to compare.\n    """"""\n\n    if isinstance(a, DataArray) != isinstance(b, DataArray):\n        raise TypeError(""a and b have mismatched types"")\n\n    left = a.unify_chunks()\n    right = b.unify_chunks()\n    assert left.chunks == right.chunks\n\n\ndef _assert_indexes_invariants_checks(indexes, possible_coord_variables, dims):\n    assert isinstance(indexes, dict), indexes\n    assert all(isinstance(v, pd.Index) for v in indexes.values()), {\n        k: type(v) for k, v in indexes.items()\n    }\n\n    index_vars = {\n        k for k, v in possible_coord_variables.items() if isinstance(v, IndexVariable)\n    }\n    assert indexes.keys() <= index_vars, (set(indexes), index_vars)\n\n    # Note: when we support non-default indexes, these checks should be opt-in\n    # only!\n    defaults = default_indexes(possible_coord_variables, dims)\n    assert indexes.keys() == defaults.keys(), (set(indexes), set(defaults))\n    assert all(v.equals(defaults[k]) for k, v in indexes.items()), (indexes, defaults)\n\n\ndef _assert_variable_invariants(var: Variable, name: Hashable = None):\n    if name is None:\n        name_or_empty: tuple = ()\n    else:\n        name_or_empty = (name,)\n    assert isinstance(var._dims, tuple), name_or_empty + (var._dims,)\n    assert len(var._dims) == len(var._data.shape), name_or_empty + (\n        var._dims,\n        var._data.shape,\n    )\n    assert isinstance(var._encoding, (type(None), dict)), name_or_empty + (\n        var._encoding,\n    )\n    assert isinstance(var._attrs, (type(None), dict)), name_or_empty + (var._attrs,)\n\n\ndef _assert_dataarray_invariants(da: DataArray):\n    assert isinstance(da._variable, Variable), da._variable\n    _assert_variable_invariants(da._variable)\n\n    assert isinstance(da._coords, dict), da._coords\n    assert all(isinstance(v, Variable) for v in da._coords.values()), da._coords\n    assert all(set(v.dims) <= set(da.dims) for v in da._coords.values()), (\n        da.dims,\n        {k: v.dims for k, v in da._coords.items()},\n    )\n    assert all(\n        isinstance(v, IndexVariable) for (k, v) in da._coords.items() if v.dims == (k,)\n    ), {k: type(v) for k, v in da._coords.items()}\n    for k, v in da._coords.items():\n        _assert_variable_invariants(v, k)\n\n    if da._indexes is not None:\n        _assert_indexes_invariants_checks(da._indexes, da._coords, da.dims)\n\n\ndef _assert_dataset_invariants(ds: Dataset):\n    assert isinstance(ds._variables, dict), type(ds._variables)\n    assert all(isinstance(v, Variable) for v in ds._variables.values()), ds._variables\n    for k, v in ds._variables.items():\n        _assert_variable_invariants(v, k)\n\n    assert isinstance(ds._coord_names, set), ds._coord_names\n    assert ds._coord_names <= ds._variables.keys(), (\n        ds._coord_names,\n        set(ds._variables),\n    )\n\n    assert type(ds._dims) is dict, ds._dims\n    assert all(isinstance(v, int) for v in ds._dims.values()), ds._dims\n    var_dims: Set[Hashable] = set()\n    for v in ds._variables.values():\n        var_dims.update(v.dims)\n    assert ds._dims.keys() == var_dims, (set(ds._dims), var_dims)\n    assert all(\n        ds._dims[k] == v.sizes[k] for v in ds._variables.values() for k in v.sizes\n    ), (ds._dims, {k: v.sizes for k, v in ds._variables.items()})\n    assert all(\n        isinstance(v, IndexVariable)\n        for (k, v) in ds._variables.items()\n        if v.dims == (k,)\n    ), {k: type(v) for k, v in ds._variables.items() if v.dims == (k,)}\n    assert all(v.dims == (k,) for (k, v) in ds._variables.items() if k in ds._dims), {\n        k: v.dims for k, v in ds._variables.items() if k in ds._dims\n    }\n\n    if ds._indexes is not None:\n        _assert_indexes_invariants_checks(ds._indexes, ds._variables, ds._dims)\n\n    assert isinstance(ds._encoding, (type(None), dict))\n    assert isinstance(ds._attrs, (type(None), dict))\n\n\ndef _assert_internal_invariants(xarray_obj: Union[DataArray, Dataset, Variable],):\n    """"""Validate that an xarray object satisfies its own internal invariants.\n\n    This exists for the benefit of xarray\'s own test suite, but may be useful\n    in external projects if they (ill-advisedly) create objects using xarray\'s\n    private APIs.\n    """"""\n    if isinstance(xarray_obj, Variable):\n        _assert_variable_invariants(xarray_obj)\n    elif isinstance(xarray_obj, DataArray):\n        _assert_dataarray_invariants(xarray_obj)\n    elif isinstance(xarray_obj, Dataset):\n        _assert_dataset_invariants(xarray_obj)\n    else:\n        raise TypeError(\n            ""{} is not a supported type for xarray invariant checks"".format(\n                type(xarray_obj)\n            )\n        )\n'"
xarray/tutorial.py,5,"b'""""""\nUseful for:\n\n* users learning xarray\n* building tutorials in the documentation.\n\n""""""\nimport hashlib\nimport os as _os\nfrom urllib.request import urlretrieve\n\nimport numpy as np\n\nfrom .backends.api import open_dataset as _open_dataset\nfrom .core.dataarray import DataArray\nfrom .core.dataset import Dataset\n\n_default_cache_dir = _os.sep.join((""~"", "".xarray_tutorial_data""))\n\n\ndef file_md5_checksum(fname):\n    hash_md5 = hashlib.md5()\n    with open(fname, ""rb"") as f:\n        hash_md5.update(f.read())\n    return hash_md5.hexdigest()\n\n\n# idea borrowed from Seaborn\ndef open_dataset(\n    name,\n    cache=True,\n    cache_dir=_default_cache_dir,\n    github_url=""https://github.com/pydata/xarray-data"",\n    branch=""master"",\n    **kws,\n):\n    """"""\n    Open a dataset from the online repository (requires internet).\n\n    If a local copy is found then always use that to avoid network traffic.\n\n    Parameters\n    ----------\n    name : str\n        Name of the file containing the dataset. If no suffix is given, assumed\n        to be netCDF (\'.nc\' is appended)\n        e.g. \'air_temperature\'\n    cache_dir : string, optional\n        The directory in which to search for and write cached data.\n    cache : boolean, optional\n        If True, then cache data locally for use on subsequent calls\n    github_url : string\n        Github repository where the data is stored\n    branch : string\n        The git branch to download from\n    kws : dict, optional\n        Passed to xarray.open_dataset\n\n    See Also\n    --------\n    xarray.open_dataset\n\n    """"""\n    root, ext = _os.path.splitext(name)\n    if not ext:\n        ext = "".nc""\n    fullname = root + ext\n    longdir = _os.path.expanduser(cache_dir)\n    localfile = _os.sep.join((longdir, fullname))\n    md5name = fullname + "".md5""\n    md5file = _os.sep.join((longdir, md5name))\n\n    if not _os.path.exists(localfile):\n\n        # This will always leave this directory on disk.\n        # May want to add an option to remove it.\n        if not _os.path.isdir(longdir):\n            _os.mkdir(longdir)\n\n        url = ""/"".join((github_url, ""raw"", branch, fullname))\n        urlretrieve(url, localfile)\n        url = ""/"".join((github_url, ""raw"", branch, md5name))\n        urlretrieve(url, md5file)\n\n        localmd5 = file_md5_checksum(localfile)\n        with open(md5file, ""r"") as f:\n            remotemd5 = f.read()\n        if localmd5 != remotemd5:\n            _os.remove(localfile)\n            msg = """"""\n            MD5 checksum does not match, try downloading dataset again.\n            """"""\n            raise OSError(msg)\n\n    ds = _open_dataset(localfile, **kws)\n\n    if not cache:\n        ds = ds.load()\n        _os.remove(localfile)\n\n    return ds\n\n\ndef load_dataset(*args, **kwargs):\n    """"""\n    Open, load into memory, and close a dataset from the online repository\n    (requires internet).\n\n    See Also\n    --------\n    open_dataset\n    """"""\n    with open_dataset(*args, **kwargs) as ds:\n        return ds.load()\n\n\ndef scatter_example_dataset():\n    A = DataArray(\n        np.zeros([3, 11, 4, 4]),\n        dims=[""x"", ""y"", ""z"", ""w""],\n        coords=[\n            np.arange(3),\n            np.linspace(0, 1, 11),\n            np.arange(4),\n            0.1 * np.random.randn(4),\n        ],\n    )\n    B = 0.1 * A.x ** 2 + A.y ** 2.5 + 0.1 * A.z * A.w\n    A = -0.1 * A.x + A.y / (5 + A.z) + A.w\n    ds = Dataset({""A"": A, ""B"": B})\n    ds[""w""] = [""one"", ""two"", ""three"", ""five""]\n\n    ds.x.attrs[""units""] = ""xunits""\n    ds.y.attrs[""units""] = ""yunits""\n    ds.z.attrs[""units""] = ""zunits""\n    ds.w.attrs[""units""] = ""wunits""\n\n    ds.A.attrs[""units""] = ""Aunits""\n    ds.B.attrs[""units""] = ""Bunits""\n\n    return ds\n'"
xarray/ufuncs.py,1,"b'""""""xarray specific universal functions\n\nHandles unary and binary operations for the following types, in ascending\npriority order:\n- scalars\n- numpy.ndarray\n- dask.array.Array\n- xarray.Variable\n- xarray.DataArray\n- xarray.Dataset\n- xarray.core.groupby.GroupBy\n\nOnce NumPy 1.10 comes out with support for overriding ufuncs, this module will\nhopefully no longer be necessary.\n""""""\nimport textwrap\nimport warnings as _warnings\n\nimport numpy as _np\n\nfrom .core.dataarray import DataArray as _DataArray\nfrom .core.dataset import Dataset as _Dataset\nfrom .core.duck_array_ops import _dask_or_eager_func\nfrom .core.groupby import GroupBy as _GroupBy\nfrom .core.pycompat import dask_array_type as _dask_array_type\nfrom .core.variable import Variable as _Variable\n\n_xarray_types = (_Variable, _DataArray, _Dataset, _GroupBy)\n_dispatch_order = (_np.ndarray, _dask_array_type) + _xarray_types\n\n\ndef _dispatch_priority(obj):\n    for priority, cls in enumerate(_dispatch_order):\n        if isinstance(obj, cls):\n            return priority\n    return -1\n\n\nclass _UFuncDispatcher:\n    """"""Wrapper for dispatching ufuncs.""""""\n\n    def __init__(self, name):\n        self._name = name\n\n    def __call__(self, *args, **kwargs):\n        if self._name not in [""angle"", ""iscomplex""]:\n            _warnings.warn(\n                ""xarray.ufuncs will be deprecated when xarray no longer ""\n                ""supports versions of numpy older than v1.17. Instead, use ""\n                ""numpy ufuncs directly."",\n                PendingDeprecationWarning,\n                stacklevel=2,\n            )\n\n        new_args = args\n        f = _dask_or_eager_func(self._name, array_args=slice(len(args)))\n        if len(args) > 2 or len(args) == 0:\n            raise TypeError(\n                ""cannot handle {} arguments for {!r}"".format(len(args), self._name)\n            )\n        elif len(args) == 1:\n            if isinstance(args[0], _xarray_types):\n                f = args[0]._unary_op(self)\n        else:  # len(args) = 2\n            p1, p2 = map(_dispatch_priority, args)\n            if p1 >= p2:\n                if isinstance(args[0], _xarray_types):\n                    f = args[0]._binary_op(self)\n            else:\n                if isinstance(args[1], _xarray_types):\n                    f = args[1]._binary_op(self, reflexive=True)\n                    new_args = tuple(reversed(args))\n        res = f(*new_args, **kwargs)\n        if res is NotImplemented:\n            raise TypeError(\n                ""%r not implemented for types (%r, %r)""\n                % (self._name, type(args[0]), type(args[1]))\n            )\n        return res\n\n\ndef _skip_signature(doc, name):\n    if not isinstance(doc, str):\n        return doc\n\n    if doc.startswith(name):\n        signature_end = doc.find(""\\n\\n"")\n        doc = doc[signature_end + 2 :]\n\n    return doc\n\n\ndef _remove_unused_reference_labels(doc):\n    if not isinstance(doc, str):\n        return doc\n\n    max_references = 5\n    for num in range(max_references):\n        label = f"".. [{num}]""\n        reference = f""[{num}]_""\n        index = f""{num}.    ""\n\n        if label not in doc or reference in doc:\n            continue\n\n        doc = doc.replace(label, index)\n\n    return doc\n\n\ndef _dedent(doc):\n    if not isinstance(doc, str):\n        return doc\n\n    return textwrap.dedent(doc)\n\n\ndef _create_op(name):\n    func = _UFuncDispatcher(name)\n    func.__name__ = name\n    doc = getattr(_np, name).__doc__\n\n    doc = _remove_unused_reference_labels(_skip_signature(_dedent(doc), name))\n\n    func.__doc__ = (\n        ""xarray specific variant of numpy.%s. Handles ""\n        ""xarray.Dataset, xarray.DataArray, xarray.Variable, ""\n        ""numpy.ndarray and dask.array.Array objects with ""\n        ""automatic dispatching.\\n\\n""\n        ""Documentation from numpy:\\n\\n%s"" % (name, doc)\n    )\n    return func\n\n\n__all__ = (  # noqa: F822\n    ""angle"",\n    ""arccos"",\n    ""arccosh"",\n    ""arcsin"",\n    ""arcsinh"",\n    ""arctan"",\n    ""arctan2"",\n    ""arctanh"",\n    ""ceil"",\n    ""conj"",\n    ""copysign"",\n    ""cos"",\n    ""cosh"",\n    ""deg2rad"",\n    ""degrees"",\n    ""exp"",\n    ""expm1"",\n    ""fabs"",\n    ""fix"",\n    ""floor"",\n    ""fmax"",\n    ""fmin"",\n    ""fmod"",\n    ""fmod"",\n    ""frexp"",\n    ""hypot"",\n    ""imag"",\n    ""iscomplex"",\n    ""isfinite"",\n    ""isinf"",\n    ""isnan"",\n    ""isreal"",\n    ""ldexp"",\n    ""log"",\n    ""log10"",\n    ""log1p"",\n    ""log2"",\n    ""logaddexp"",\n    ""logaddexp2"",\n    ""logical_and"",\n    ""logical_not"",\n    ""logical_or"",\n    ""logical_xor"",\n    ""maximum"",\n    ""minimum"",\n    ""nextafter"",\n    ""rad2deg"",\n    ""radians"",\n    ""real"",\n    ""rint"",\n    ""sign"",\n    ""signbit"",\n    ""sin"",\n    ""sinh"",\n    ""sqrt"",\n    ""square"",\n    ""tan"",\n    ""tanh"",\n    ""trunc"",\n)\n\n\nfor name in __all__:\n    globals()[name] = _create_op(name)\n'"
asv_bench/benchmarks/__init__.py,3,"b'import itertools\n\nimport numpy as np\n\n_counter = itertools.count()\n\n\ndef parameterized(names, params):\n    def decorator(func):\n        func.param_names = names\n        func.params = params\n        return func\n\n    return decorator\n\n\ndef requires_dask():\n    try:\n        import dask  # noqa: F401\n    except ImportError:\n        raise NotImplementedError()\n\n\ndef randn(shape, frac_nan=None, chunks=None, seed=0):\n    rng = np.random.RandomState(seed)\n    if chunks is None:\n        x = rng.standard_normal(shape)\n    else:\n        import dask.array as da\n\n        rng = da.random.RandomState(seed)\n        x = rng.standard_normal(shape, chunks=chunks)\n\n    if frac_nan is not None:\n        inds = rng.choice(range(x.size), int(x.size * frac_nan))\n        x.flat[inds] = np.nan\n\n    return x\n\n\ndef randint(low, high=None, size=None, frac_minus=None, seed=0):\n    rng = np.random.RandomState(seed)\n    x = rng.randint(low, high, size)\n    if frac_minus is not None:\n        inds = rng.choice(range(x.size), int(x.size * frac_minus))\n        x.flat[inds] = -1\n\n    return x\n'"
asv_bench/benchmarks/combine.py,2,"b'import numpy as np\n\nimport xarray as xr\n\n\nclass Combine:\n    """"""Benchmark concatenating and merging large datasets""""""\n\n    def setup(self):\n        """"""Create 4 datasets with two different variables""""""\n\n        t_size, x_size, y_size = 100, 900, 800\n        t = np.arange(t_size)\n        data = np.random.randn(t_size, x_size, y_size)\n\n        self.dsA0 = xr.Dataset(\n            {""A"": xr.DataArray(data, coords={""T"": t}, dims=(""T"", ""X"", ""Y""))}\n        )\n        self.dsA1 = xr.Dataset(\n            {""A"": xr.DataArray(data, coords={""T"": t + t_size}, dims=(""T"", ""X"", ""Y""))}\n        )\n        self.dsB0 = xr.Dataset(\n            {""B"": xr.DataArray(data, coords={""T"": t}, dims=(""T"", ""X"", ""Y""))}\n        )\n        self.dsB1 = xr.Dataset(\n            {""B"": xr.DataArray(data, coords={""T"": t + t_size}, dims=(""T"", ""X"", ""Y""))}\n        )\n\n    def time_combine_manual(self):\n        datasets = [[self.dsA0, self.dsA1], [self.dsB0, self.dsB1]]\n\n        xr.combine_manual(datasets, concat_dim=[None, ""t""])\n\n    def time_auto_combine(self):\n        """"""Also has to load and arrange t coordinate""""""\n        datasets = [self.dsA0, self.dsA1, self.dsB0, self.dsB1]\n\n        xr.combine_auto(datasets)\n'"
asv_bench/benchmarks/dataarray_missing.py,0,"b'import pandas as pd\n\nimport xarray as xr\n\nfrom . import randn, requires_dask\n\ntry:\n    import dask  # noqa: F401\nexcept ImportError:\n    pass\n\n\ndef make_bench_data(shape, frac_nan, chunks):\n    vals = randn(shape, frac_nan)\n    coords = {""time"": pd.date_range(""2000-01-01"", freq=""D"", periods=shape[0])}\n    da = xr.DataArray(vals, dims=(""time"", ""x"", ""y""), coords=coords)\n\n    if chunks is not None:\n        da = da.chunk(chunks)\n\n    return da\n\n\ndef time_interpolate_na(shape, chunks, method, limit):\n    if chunks is not None:\n        requires_dask()\n    da = make_bench_data(shape, 0.1, chunks=chunks)\n    actual = da.interpolate_na(dim=""time"", method=""linear"", limit=limit)\n\n    if chunks is not None:\n        actual = actual.compute()\n\n\ntime_interpolate_na.param_names = [""shape"", ""chunks"", ""method"", ""limit""]\ntime_interpolate_na.params = (\n    [(3650, 200, 400), (100, 25, 25)],\n    [None, {""x"": 25, ""y"": 25}],\n    [""linear"", ""spline"", ""quadratic"", ""cubic""],\n    [None, 3],\n)\n\n\ndef time_ffill(shape, chunks, limit):\n\n    da = make_bench_data(shape, 0.1, chunks=chunks)\n    actual = da.ffill(dim=""time"", limit=limit)\n\n    if chunks is not None:\n        actual = actual.compute()\n\n\ntime_ffill.param_names = [""shape"", ""chunks"", ""limit""]\ntime_ffill.params = (\n    [(3650, 200, 400), (100, 25, 25)],\n    [None, {""x"": 25, ""y"": 25}],\n    [None, 3],\n)\n\n\ndef time_bfill(shape, chunks, limit):\n\n    da = make_bench_data(shape, 0.1, chunks=chunks)\n    actual = da.bfill(dim=""time"", limit=limit)\n\n    if chunks is not None:\n        actual = actual.compute()\n\n\ntime_bfill.param_names = [""shape"", ""chunks"", ""limit""]\ntime_bfill.params = (\n    [(3650, 200, 400), (100, 25, 25)],\n    [None, {""x"": 25, ""y"": 25}],\n    [None, 3],\n)\n'"
asv_bench/benchmarks/dataset_io.py,7,"b'import os\n\nimport numpy as np\nimport pandas as pd\n\nimport xarray as xr\n\nfrom . import randint, randn, requires_dask\n\ntry:\n    import dask\n    import dask.multiprocessing\nexcept ImportError:\n    pass\n\n\nos.environ[""HDF5_USE_FILE_LOCKING""] = ""FALSE""\n\n\nclass IOSingleNetCDF:\n    """"""\n    A few examples that benchmark reading/writing a single netCDF file with\n    xarray\n    """"""\n\n    timeout = 300.0\n    repeat = 1\n    number = 5\n\n    def make_ds(self):\n\n        # single Dataset\n        self.ds = xr.Dataset()\n        self.nt = 1000\n        self.nx = 90\n        self.ny = 45\n\n        self.block_chunks = {\n            ""time"": self.nt / 4,\n            ""lon"": self.nx / 3,\n            ""lat"": self.ny / 3,\n        }\n\n        self.time_chunks = {""time"": int(self.nt / 36)}\n\n        times = pd.date_range(""1970-01-01"", periods=self.nt, freq=""D"")\n        lons = xr.DataArray(\n            np.linspace(0, 360, self.nx),\n            dims=(""lon"",),\n            attrs={""units"": ""degrees east"", ""long_name"": ""longitude""},\n        )\n        lats = xr.DataArray(\n            np.linspace(-90, 90, self.ny),\n            dims=(""lat"",),\n            attrs={""units"": ""degrees north"", ""long_name"": ""latitude""},\n        )\n        self.ds[""foo""] = xr.DataArray(\n            randn((self.nt, self.nx, self.ny), frac_nan=0.2),\n            coords={""lon"": lons, ""lat"": lats, ""time"": times},\n            dims=(""time"", ""lon"", ""lat""),\n            name=""foo"",\n            encoding=None,\n            attrs={""units"": ""foo units"", ""description"": ""a description""},\n        )\n        self.ds[""bar""] = xr.DataArray(\n            randn((self.nt, self.nx, self.ny), frac_nan=0.2),\n            coords={""lon"": lons, ""lat"": lats, ""time"": times},\n            dims=(""time"", ""lon"", ""lat""),\n            name=""bar"",\n            encoding=None,\n            attrs={""units"": ""bar units"", ""description"": ""a description""},\n        )\n        self.ds[""baz""] = xr.DataArray(\n            randn((self.nx, self.ny), frac_nan=0.2).astype(np.float32),\n            coords={""lon"": lons, ""lat"": lats},\n            dims=(""lon"", ""lat""),\n            name=""baz"",\n            encoding=None,\n            attrs={""units"": ""baz units"", ""description"": ""a description""},\n        )\n\n        self.ds.attrs = {""history"": ""created for xarray benchmarking""}\n\n        self.oinds = {\n            ""time"": randint(0, self.nt, 120),\n            ""lon"": randint(0, self.nx, 20),\n            ""lat"": randint(0, self.ny, 10),\n        }\n        self.vinds = {\n            ""time"": xr.DataArray(randint(0, self.nt, 120), dims=""x""),\n            ""lon"": xr.DataArray(randint(0, self.nx, 120), dims=""x""),\n            ""lat"": slice(3, 20),\n        }\n\n\nclass IOWriteSingleNetCDF3(IOSingleNetCDF):\n    def setup(self):\n        self.format = ""NETCDF3_64BIT""\n        self.make_ds()\n\n    def time_write_dataset_netcdf4(self):\n        self.ds.to_netcdf(""test_netcdf4_write.nc"", engine=""netcdf4"", format=self.format)\n\n    def time_write_dataset_scipy(self):\n        self.ds.to_netcdf(""test_scipy_write.nc"", engine=""scipy"", format=self.format)\n\n\nclass IOReadSingleNetCDF4(IOSingleNetCDF):\n    def setup(self):\n\n        self.make_ds()\n\n        self.filepath = ""test_single_file.nc4.nc""\n        self.format = ""NETCDF4""\n        self.ds.to_netcdf(self.filepath, format=self.format)\n\n    def time_load_dataset_netcdf4(self):\n        xr.open_dataset(self.filepath, engine=""netcdf4"").load()\n\n    def time_orthogonal_indexing(self):\n        ds = xr.open_dataset(self.filepath, engine=""netcdf4"")\n        ds = ds.isel(**self.oinds).load()\n\n    def time_vectorized_indexing(self):\n        ds = xr.open_dataset(self.filepath, engine=""netcdf4"")\n        ds = ds.isel(**self.vinds).load()\n\n\nclass IOReadSingleNetCDF3(IOReadSingleNetCDF4):\n    def setup(self):\n\n        self.make_ds()\n\n        self.filepath = ""test_single_file.nc3.nc""\n        self.format = ""NETCDF3_64BIT""\n        self.ds.to_netcdf(self.filepath, format=self.format)\n\n    def time_load_dataset_scipy(self):\n        xr.open_dataset(self.filepath, engine=""scipy"").load()\n\n    def time_orthogonal_indexing(self):\n        ds = xr.open_dataset(self.filepath, engine=""scipy"")\n        ds = ds.isel(**self.oinds).load()\n\n    def time_vectorized_indexing(self):\n        ds = xr.open_dataset(self.filepath, engine=""scipy"")\n        ds = ds.isel(**self.vinds).load()\n\n\nclass IOReadSingleNetCDF4Dask(IOSingleNetCDF):\n    def setup(self):\n\n        requires_dask()\n\n        self.make_ds()\n\n        self.filepath = ""test_single_file.nc4.nc""\n        self.format = ""NETCDF4""\n        self.ds.to_netcdf(self.filepath, format=self.format)\n\n    def time_load_dataset_netcdf4_with_block_chunks(self):\n        xr.open_dataset(\n            self.filepath, engine=""netcdf4"", chunks=self.block_chunks\n        ).load()\n\n    def time_load_dataset_netcdf4_with_block_chunks_oindexing(self):\n        ds = xr.open_dataset(self.filepath, engine=""netcdf4"", chunks=self.block_chunks)\n        ds = ds.isel(**self.oinds).load()\n\n    def time_load_dataset_netcdf4_with_block_chunks_vindexing(self):\n        ds = xr.open_dataset(self.filepath, engine=""netcdf4"", chunks=self.block_chunks)\n        ds = ds.isel(**self.vinds).load()\n\n    def time_load_dataset_netcdf4_with_block_chunks_multiprocessing(self):\n        with dask.config.set(scheduler=""multiprocessing""):\n            xr.open_dataset(\n                self.filepath, engine=""netcdf4"", chunks=self.block_chunks\n            ).load()\n\n    def time_load_dataset_netcdf4_with_time_chunks(self):\n        xr.open_dataset(self.filepath, engine=""netcdf4"", chunks=self.time_chunks).load()\n\n    def time_load_dataset_netcdf4_with_time_chunks_multiprocessing(self):\n        with dask.config.set(scheduler=""multiprocessing""):\n            xr.open_dataset(\n                self.filepath, engine=""netcdf4"", chunks=self.time_chunks\n            ).load()\n\n\nclass IOReadSingleNetCDF3Dask(IOReadSingleNetCDF4Dask):\n    def setup(self):\n\n        requires_dask()\n\n        self.make_ds()\n\n        self.filepath = ""test_single_file.nc3.nc""\n        self.format = ""NETCDF3_64BIT""\n        self.ds.to_netcdf(self.filepath, format=self.format)\n\n    def time_load_dataset_scipy_with_block_chunks(self):\n        with dask.config.set(scheduler=""multiprocessing""):\n            xr.open_dataset(\n                self.filepath, engine=""scipy"", chunks=self.block_chunks\n            ).load()\n\n    def time_load_dataset_scipy_with_block_chunks_oindexing(self):\n        ds = xr.open_dataset(self.filepath, engine=""scipy"", chunks=self.block_chunks)\n        ds = ds.isel(**self.oinds).load()\n\n    def time_load_dataset_scipy_with_block_chunks_vindexing(self):\n        ds = xr.open_dataset(self.filepath, engine=""scipy"", chunks=self.block_chunks)\n        ds = ds.isel(**self.vinds).load()\n\n    def time_load_dataset_scipy_with_time_chunks(self):\n        with dask.config.set(scheduler=""multiprocessing""):\n            xr.open_dataset(\n                self.filepath, engine=""scipy"", chunks=self.time_chunks\n            ).load()\n\n\nclass IOMultipleNetCDF:\n    """"""\n    A few examples that benchmark reading/writing multiple netCDF files with\n    xarray\n    """"""\n\n    timeout = 300.0\n    repeat = 1\n    number = 5\n\n    def make_ds(self, nfiles=10):\n\n        # multiple Dataset\n        self.ds = xr.Dataset()\n        self.nt = 1000\n        self.nx = 90\n        self.ny = 45\n        self.nfiles = nfiles\n\n        self.block_chunks = {\n            ""time"": self.nt / 4,\n            ""lon"": self.nx / 3,\n            ""lat"": self.ny / 3,\n        }\n\n        self.time_chunks = {""time"": int(self.nt / 36)}\n\n        self.time_vars = np.split(\n            pd.date_range(""1970-01-01"", periods=self.nt, freq=""D""), self.nfiles\n        )\n\n        self.ds_list = []\n        self.filenames_list = []\n        for i, times in enumerate(self.time_vars):\n            ds = xr.Dataset()\n            nt = len(times)\n            lons = xr.DataArray(\n                np.linspace(0, 360, self.nx),\n                dims=(""lon"",),\n                attrs={""units"": ""degrees east"", ""long_name"": ""longitude""},\n            )\n            lats = xr.DataArray(\n                np.linspace(-90, 90, self.ny),\n                dims=(""lat"",),\n                attrs={""units"": ""degrees north"", ""long_name"": ""latitude""},\n            )\n            ds[""foo""] = xr.DataArray(\n                randn((nt, self.nx, self.ny), frac_nan=0.2),\n                coords={""lon"": lons, ""lat"": lats, ""time"": times},\n                dims=(""time"", ""lon"", ""lat""),\n                name=""foo"",\n                encoding=None,\n                attrs={""units"": ""foo units"", ""description"": ""a description""},\n            )\n            ds[""bar""] = xr.DataArray(\n                randn((nt, self.nx, self.ny), frac_nan=0.2),\n                coords={""lon"": lons, ""lat"": lats, ""time"": times},\n                dims=(""time"", ""lon"", ""lat""),\n                name=""bar"",\n                encoding=None,\n                attrs={""units"": ""bar units"", ""description"": ""a description""},\n            )\n            ds[""baz""] = xr.DataArray(\n                randn((self.nx, self.ny), frac_nan=0.2).astype(np.float32),\n                coords={""lon"": lons, ""lat"": lats},\n                dims=(""lon"", ""lat""),\n                name=""baz"",\n                encoding=None,\n                attrs={""units"": ""baz units"", ""description"": ""a description""},\n            )\n\n            ds.attrs = {""history"": ""created for xarray benchmarking""}\n\n            self.ds_list.append(ds)\n            self.filenames_list.append(""test_netcdf_%i.nc"" % i)\n\n\nclass IOWriteMultipleNetCDF3(IOMultipleNetCDF):\n    def setup(self):\n        self.make_ds()\n        self.format = ""NETCDF3_64BIT""\n\n    def time_write_dataset_netcdf4(self):\n        xr.save_mfdataset(\n            self.ds_list, self.filenames_list, engine=""netcdf4"", format=self.format\n        )\n\n    def time_write_dataset_scipy(self):\n        xr.save_mfdataset(\n            self.ds_list, self.filenames_list, engine=""scipy"", format=self.format\n        )\n\n\nclass IOReadMultipleNetCDF4(IOMultipleNetCDF):\n    def setup(self):\n\n        requires_dask()\n\n        self.make_ds()\n        self.format = ""NETCDF4""\n        xr.save_mfdataset(self.ds_list, self.filenames_list, format=self.format)\n\n    def time_load_dataset_netcdf4(self):\n        xr.open_mfdataset(self.filenames_list, engine=""netcdf4"").load()\n\n    def time_open_dataset_netcdf4(self):\n        xr.open_mfdataset(self.filenames_list, engine=""netcdf4"")\n\n\nclass IOReadMultipleNetCDF3(IOReadMultipleNetCDF4):\n    def setup(self):\n\n        requires_dask()\n\n        self.make_ds()\n        self.format = ""NETCDF3_64BIT""\n        xr.save_mfdataset(self.ds_list, self.filenames_list, format=self.format)\n\n    def time_load_dataset_scipy(self):\n        xr.open_mfdataset(self.filenames_list, engine=""scipy"").load()\n\n    def time_open_dataset_scipy(self):\n        xr.open_mfdataset(self.filenames_list, engine=""scipy"")\n\n\nclass IOReadMultipleNetCDF4Dask(IOMultipleNetCDF):\n    def setup(self):\n\n        requires_dask()\n\n        self.make_ds()\n        self.format = ""NETCDF4""\n        xr.save_mfdataset(self.ds_list, self.filenames_list, format=self.format)\n\n    def time_load_dataset_netcdf4_with_block_chunks(self):\n        xr.open_mfdataset(\n            self.filenames_list, engine=""netcdf4"", chunks=self.block_chunks\n        ).load()\n\n    def time_load_dataset_netcdf4_with_block_chunks_multiprocessing(self):\n        with dask.config.set(scheduler=""multiprocessing""):\n            xr.open_mfdataset(\n                self.filenames_list, engine=""netcdf4"", chunks=self.block_chunks\n            ).load()\n\n    def time_load_dataset_netcdf4_with_time_chunks(self):\n        xr.open_mfdataset(\n            self.filenames_list, engine=""netcdf4"", chunks=self.time_chunks\n        ).load()\n\n    def time_load_dataset_netcdf4_with_time_chunks_multiprocessing(self):\n        with dask.config.set(scheduler=""multiprocessing""):\n            xr.open_mfdataset(\n                self.filenames_list, engine=""netcdf4"", chunks=self.time_chunks\n            ).load()\n\n    def time_open_dataset_netcdf4_with_block_chunks(self):\n        xr.open_mfdataset(\n            self.filenames_list, engine=""netcdf4"", chunks=self.block_chunks\n        )\n\n    def time_open_dataset_netcdf4_with_block_chunks_multiprocessing(self):\n        with dask.config.set(scheduler=""multiprocessing""):\n            xr.open_mfdataset(\n                self.filenames_list, engine=""netcdf4"", chunks=self.block_chunks\n            )\n\n    def time_open_dataset_netcdf4_with_time_chunks(self):\n        xr.open_mfdataset(\n            self.filenames_list, engine=""netcdf4"", chunks=self.time_chunks\n        )\n\n    def time_open_dataset_netcdf4_with_time_chunks_multiprocessing(self):\n        with dask.config.set(scheduler=""multiprocessing""):\n            xr.open_mfdataset(\n                self.filenames_list, engine=""netcdf4"", chunks=self.time_chunks\n            )\n\n\nclass IOReadMultipleNetCDF3Dask(IOReadMultipleNetCDF4Dask):\n    def setup(self):\n\n        requires_dask()\n\n        self.make_ds()\n        self.format = ""NETCDF3_64BIT""\n        xr.save_mfdataset(self.ds_list, self.filenames_list, format=self.format)\n\n    def time_load_dataset_scipy_with_block_chunks(self):\n        with dask.config.set(scheduler=""multiprocessing""):\n            xr.open_mfdataset(\n                self.filenames_list, engine=""scipy"", chunks=self.block_chunks\n            ).load()\n\n    def time_load_dataset_scipy_with_time_chunks(self):\n        with dask.config.set(scheduler=""multiprocessing""):\n            xr.open_mfdataset(\n                self.filenames_list, engine=""scipy"", chunks=self.time_chunks\n            ).load()\n\n    def time_open_dataset_scipy_with_block_chunks(self):\n        with dask.config.set(scheduler=""multiprocessing""):\n            xr.open_mfdataset(\n                self.filenames_list, engine=""scipy"", chunks=self.block_chunks\n            )\n\n    def time_open_dataset_scipy_with_time_chunks(self):\n        with dask.config.set(scheduler=""multiprocessing""):\n            xr.open_mfdataset(\n                self.filenames_list, engine=""scipy"", chunks=self.time_chunks\n            )\n\n\ndef create_delayed_write():\n    import dask.array as da\n\n    vals = da.random.random(300, chunks=(1,))\n    ds = xr.Dataset({""vals"": ([""a""], vals)})\n    return ds.to_netcdf(""file.nc"", engine=""netcdf4"", compute=False)\n\n\nclass IOWriteNetCDFDask:\n    timeout = 60\n    repeat = 1\n    number = 5\n\n    def setup(self):\n        requires_dask()\n        self.write = create_delayed_write()\n\n    def time_write(self):\n        self.write.compute()\n\n\nclass IOWriteNetCDFDaskDistributed:\n    def setup(self):\n        try:\n            import distributed\n        except ImportError:\n            raise NotImplementedError()\n        self.client = distributed.Client()\n        self.write = create_delayed_write()\n\n    def cleanup(self):\n        self.client.shutdown()\n\n    def time_write(self):\n        self.write.compute()\n'"
asv_bench/benchmarks/indexing.py,5,"b'import numpy as np\nimport pandas as pd\n\nimport xarray as xr\n\nfrom . import randint, randn, requires_dask\n\nnx = 3000\nny = 2000\nnt = 1000\n\nbasic_indexes = {\n    ""1slice"": {""x"": slice(0, 3)},\n    ""1slice-1scalar"": {""x"": 0, ""y"": slice(None, None, 3)},\n    ""2slicess-1scalar"": {""x"": slice(3, -3, 3), ""y"": 1, ""t"": slice(None, -3, 3)},\n}\n\nbasic_assignment_values = {\n    ""1slice"": xr.DataArray(randn((3, ny), frac_nan=0.1), dims=[""x"", ""y""]),\n    ""1slice-1scalar"": xr.DataArray(randn(int(ny / 3) + 1, frac_nan=0.1), dims=[""y""]),\n    ""2slicess-1scalar"": xr.DataArray(\n        randn(int((nx - 6) / 3), frac_nan=0.1), dims=[""x""]\n    ),\n}\n\nouter_indexes = {\n    ""1d"": {""x"": randint(0, nx, 400)},\n    ""2d"": {""x"": randint(0, nx, 500), ""y"": randint(0, ny, 400)},\n    ""2d-1scalar"": {""x"": randint(0, nx, 100), ""y"": 1, ""t"": randint(0, nt, 400)},\n}\n\nouter_assignment_values = {\n    ""1d"": xr.DataArray(randn((400, ny), frac_nan=0.1), dims=[""x"", ""y""]),\n    ""2d"": xr.DataArray(randn((500, 400), frac_nan=0.1), dims=[""x"", ""y""]),\n    ""2d-1scalar"": xr.DataArray(randn(100, frac_nan=0.1), dims=[""x""]),\n}\n\nvectorized_indexes = {\n    ""1-1d"": {""x"": xr.DataArray(randint(0, nx, 400), dims=""a"")},\n    ""2-1d"": {\n        ""x"": xr.DataArray(randint(0, nx, 400), dims=""a""),\n        ""y"": xr.DataArray(randint(0, ny, 400), dims=""a""),\n    },\n    ""3-2d"": {\n        ""x"": xr.DataArray(randint(0, nx, 400).reshape(4, 100), dims=[""a"", ""b""]),\n        ""y"": xr.DataArray(randint(0, ny, 400).reshape(4, 100), dims=[""a"", ""b""]),\n        ""t"": xr.DataArray(randint(0, nt, 400).reshape(4, 100), dims=[""a"", ""b""]),\n    },\n}\n\nvectorized_assignment_values = {\n    ""1-1d"": xr.DataArray(randn((400, 2000)), dims=[""a"", ""y""], coords={""a"": randn(400)}),\n    ""2-1d"": xr.DataArray(randn(400), dims=[""a""], coords={""a"": randn(400)}),\n    ""3-2d"": xr.DataArray(\n        randn((4, 100)), dims=[""a"", ""b""], coords={""a"": randn(4), ""b"": randn(100)}\n    ),\n}\n\n\nclass Base:\n    def setup(self, key):\n        self.ds = xr.Dataset(\n            {\n                ""var1"": ((""x"", ""y""), randn((nx, ny), frac_nan=0.1)),\n                ""var2"": ((""x"", ""t""), randn((nx, nt))),\n                ""var3"": ((""t"",), randn(nt)),\n            },\n            coords={\n                ""x"": np.arange(nx),\n                ""y"": np.linspace(0, 1, ny),\n                ""t"": pd.date_range(""1970-01-01"", periods=nt, freq=""D""),\n                ""x_coords"": (""x"", np.linspace(1.1, 2.1, nx)),\n            },\n        )\n\n\nclass Indexing(Base):\n    def time_indexing_basic(self, key):\n        self.ds.isel(**basic_indexes[key]).load()\n\n    time_indexing_basic.param_names = [""key""]\n    time_indexing_basic.params = [list(basic_indexes.keys())]\n\n    def time_indexing_outer(self, key):\n        self.ds.isel(**outer_indexes[key]).load()\n\n    time_indexing_outer.param_names = [""key""]\n    time_indexing_outer.params = [list(outer_indexes.keys())]\n\n    def time_indexing_vectorized(self, key):\n        self.ds.isel(**vectorized_indexes[key]).load()\n\n    time_indexing_vectorized.param_names = [""key""]\n    time_indexing_vectorized.params = [list(vectorized_indexes.keys())]\n\n\nclass Assignment(Base):\n    def time_assignment_basic(self, key):\n        ind = basic_indexes[key]\n        val = basic_assignment_values[key]\n        self.ds[""var1""][ind.get(""x"", slice(None)), ind.get(""y"", slice(None))] = val\n\n    time_assignment_basic.param_names = [""key""]\n    time_assignment_basic.params = [list(basic_indexes.keys())]\n\n    def time_assignment_outer(self, key):\n        ind = outer_indexes[key]\n        val = outer_assignment_values[key]\n        self.ds[""var1""][ind.get(""x"", slice(None)), ind.get(""y"", slice(None))] = val\n\n    time_assignment_outer.param_names = [""key""]\n    time_assignment_outer.params = [list(outer_indexes.keys())]\n\n    def time_assignment_vectorized(self, key):\n        ind = vectorized_indexes[key]\n        val = vectorized_assignment_values[key]\n        self.ds[""var1""][ind.get(""x"", slice(None)), ind.get(""y"", slice(None))] = val\n\n    time_assignment_vectorized.param_names = [""key""]\n    time_assignment_vectorized.params = [list(vectorized_indexes.keys())]\n\n\nclass IndexingDask(Indexing):\n    def setup(self, key):\n        requires_dask()\n        super().setup(key)\n        self.ds = self.ds.chunk({""x"": 100, ""y"": 50, ""t"": 50})\n\n\nclass BooleanIndexing:\n    # https://github.com/pydata/xarray/issues/2227\n    def setup(self):\n        self.ds = xr.Dataset(\n            {""a"": (""time"", np.arange(10_000_000))},\n            coords={""time"": np.arange(10_000_000)},\n        )\n        self.time_filter = self.ds.time > 50_000\n\n    def time_indexing(self):\n        self.ds.isel(time=self.time_filter)\n'"
asv_bench/benchmarks/interp.py,6,"b'import numpy as np\nimport pandas as pd\n\nimport xarray as xr\n\nfrom . import parameterized, randn, requires_dask\n\nnx = 3000\nlong_nx = 30000000\nny = 2000\nnt = 1000\nwindow = 20\n\nrandn_xy = randn((nx, ny), frac_nan=0.1)\nrandn_xt = randn((nx, nt))\nrandn_t = randn((nt,))\nrandn_long = randn((long_nx,), frac_nan=0.1)\n\n\nnew_x_short = np.linspace(0.3 * nx, 0.7 * nx, 100)\nnew_x_long = np.linspace(0.3 * nx, 0.7 * nx, 1000)\nnew_y_long = np.linspace(0.1, 0.9, 1000)\n\n\nclass Interpolation:\n    def setup(self, *args, **kwargs):\n        self.ds = xr.Dataset(\n            {\n                ""var1"": ((""x"", ""y""), randn_xy),\n                ""var2"": ((""x"", ""t""), randn_xt),\n                ""var3"": ((""t"",), randn_t),\n            },\n            coords={\n                ""x"": np.arange(nx),\n                ""y"": np.linspace(0, 1, ny),\n                ""t"": pd.date_range(""1970-01-01"", periods=nt, freq=""D""),\n                ""x_coords"": (""x"", np.linspace(1.1, 2.1, nx)),\n            },\n        )\n\n    @parameterized([""method"", ""is_short""], ([""linear"", ""cubic""], [True, False]))\n    def time_interpolation(self, method, is_short):\n        new_x = new_x_short if is_short else new_x_long\n        self.ds.interp(x=new_x, method=method).load()\n\n    @parameterized([""method""], ([""linear"", ""nearest""]))\n    def time_interpolation_2d(self, method):\n        self.ds.interp(x=new_x_long, y=new_y_long, method=method).load()\n\n\nclass InterpolationDask(Interpolation):\n    def setup(self, *args, **kwargs):\n        requires_dask()\n        super().setup(**kwargs)\n        self.ds = self.ds.chunk({""t"": 50})\n'"
asv_bench/benchmarks/reindexing.py,9,"b'import numpy as np\n\nimport xarray as xr\n\nfrom . import requires_dask\n\n\nclass Reindex:\n    def setup(self):\n        data = np.random.RandomState(0).randn(1000, 100, 100)\n        self.ds = xr.Dataset(\n            {""temperature"": ((""time"", ""x"", ""y""), data)},\n            coords={""time"": np.arange(1000), ""x"": np.arange(100), ""y"": np.arange(100)},\n        )\n\n    def time_1d_coarse(self):\n        self.ds.reindex(time=np.arange(0, 1000, 5)).load()\n\n    def time_1d_fine_all_found(self):\n        self.ds.reindex(time=np.arange(0, 1000, 0.5), method=""nearest"").load()\n\n    def time_1d_fine_some_missing(self):\n        self.ds.reindex(\n            time=np.arange(0, 1000, 0.5), method=""nearest"", tolerance=0.1\n        ).load()\n\n    def time_2d_coarse(self):\n        self.ds.reindex(x=np.arange(0, 100, 2), y=np.arange(0, 100, 2)).load()\n\n    def time_2d_fine_all_found(self):\n        self.ds.reindex(\n            x=np.arange(0, 100, 0.5), y=np.arange(0, 100, 0.5), method=""nearest""\n        ).load()\n\n    def time_2d_fine_some_missing(self):\n        self.ds.reindex(\n            x=np.arange(0, 100, 0.5),\n            y=np.arange(0, 100, 0.5),\n            method=""nearest"",\n            tolerance=0.1,\n        ).load()\n\n\nclass ReindexDask(Reindex):\n    def setup(self):\n        requires_dask()\n        super().setup()\n        self.ds = self.ds.chunk({""time"": 100})\n'"
asv_bench/benchmarks/rolling.py,4,"b'import numpy as np\nimport pandas as pd\n\nimport xarray as xr\n\nfrom . import parameterized, randn, requires_dask\n\nnx = 3000\nlong_nx = 30000000\nny = 2000\nnt = 1000\nwindow = 20\n\nrandn_xy = randn((nx, ny), frac_nan=0.1)\nrandn_xt = randn((nx, nt))\nrandn_t = randn((nt,))\nrandn_long = randn((long_nx,), frac_nan=0.1)\n\n\nclass Rolling:\n    def setup(self, *args, **kwargs):\n        self.ds = xr.Dataset(\n            {\n                ""var1"": ((""x"", ""y""), randn_xy),\n                ""var2"": ((""x"", ""t""), randn_xt),\n                ""var3"": ((""t"",), randn_t),\n            },\n            coords={\n                ""x"": np.arange(nx),\n                ""y"": np.linspace(0, 1, ny),\n                ""t"": pd.date_range(""1970-01-01"", periods=nt, freq=""D""),\n                ""x_coords"": (""x"", np.linspace(1.1, 2.1, nx)),\n            },\n        )\n        self.da_long = xr.DataArray(\n            randn_long, dims=""x"", coords={""x"": np.arange(long_nx) * 0.1}\n        )\n\n    @parameterized([""func"", ""center""], ([""mean"", ""count""], [True, False]))\n    def time_rolling(self, func, center):\n        getattr(self.ds.rolling(x=window, center=center), func)().load()\n\n    @parameterized([""func"", ""pandas""], ([""mean"", ""count""], [True, False]))\n    def time_rolling_long(self, func, pandas):\n        if pandas:\n            se = self.da_long.to_series()\n            getattr(se.rolling(window=window), func)()\n        else:\n            getattr(self.da_long.rolling(x=window), func)().load()\n\n    @parameterized([""window_"", ""min_periods""], ([20, 40], [5, None]))\n    def time_rolling_np(self, window_, min_periods):\n        self.ds.rolling(x=window_, center=False, min_periods=min_periods).reduce(\n            getattr(np, ""nanmean"")\n        ).load()\n\n    @parameterized([""center"", ""stride""], ([True, False], [1, 200]))\n    def time_rolling_construct(self, center, stride):\n        self.ds.rolling(x=window, center=center).construct(\n            ""window_dim"", stride=stride\n        ).mean(dim=""window_dim"").load()\n\n\nclass RollingDask(Rolling):\n    def setup(self, *args, **kwargs):\n        requires_dask()\n        super().setup(**kwargs)\n        self.ds = self.ds.chunk({""x"": 100, ""y"": 50, ""t"": 50})\n        self.da_long = self.da_long.chunk({""x"": 10000})\n'"
asv_bench/benchmarks/unstacking.py,1,"b'import numpy as np\n\nimport xarray as xr\n\nfrom . import requires_dask\n\n\nclass Unstacking:\n    def setup(self):\n        data = np.random.RandomState(0).randn(1, 1000, 500)\n        self.ds = xr.DataArray(data).stack(flat_dim=[""dim_1"", ""dim_2""])\n\n    def time_unstack_fast(self):\n        self.ds.unstack(""flat_dim"")\n\n    def time_unstack_slow(self):\n        self.ds[:, ::-1].unstack(""flat_dim"")\n\n\nclass UnstackingDask(Unstacking):\n    def setup(self, *args, **kwargs):\n        requires_dask()\n        super().setup(**kwargs)\n        self.ds = self.ds.chunk({""flat_dim"": 50})\n'"
doc/gallery/plot_cartopy_facetgrid.py,0,"b'""""""\n==================================\nMultiple plots and map projections\n==================================\n\nControl the map projection parameters on multiple axes\n\nThis example illustrates how to plot multiple maps and control their extent\nand aspect ratio.\n\nFor more details see `this discussion`_ on github.\n\n.. _this discussion: https://github.com/pydata/xarray/issues/1397#issuecomment-299190567\n""""""\n\n\nimport cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\n\nimport xarray as xr\n\n# Load the data\nds = xr.tutorial.load_dataset(""air_temperature"")\nair = ds.air.isel(time=[0, 724]) - 273.15\n\n# This is the map projection we want to plot *onto*\nmap_proj = ccrs.LambertConformal(central_longitude=-95, central_latitude=45)\n\np = air.plot(\n    transform=ccrs.PlateCarree(),  # the data\'s projection\n    col=""time"",\n    col_wrap=1,  # multiplot settings\n    aspect=ds.dims[""lon""] / ds.dims[""lat""],  # for a sensible figsize\n    subplot_kws={""projection"": map_proj},  # the plot\'s projection\n)\n\n# We have to set the map\'s options on all four axes\nfor ax in p.axes.flat:\n    ax.coastlines()\n    ax.set_extent([-160, -30, 5, 75])\n    # Without this aspect attributes the maps will look chaotic and the\n    # ""extent"" attribute above will be ignored\n    ax.set_aspect(""equal"")\n\nplt.show()\n'"
doc/gallery/plot_colorbar_center.py,0,"b'""""""\n==================\nCentered colormaps\n==================\n\nxarray\'s automatic colormaps choice\n\n""""""\n\nimport matplotlib.pyplot as plt\n\nimport xarray as xr\n\n# Load the data\nds = xr.tutorial.load_dataset(""air_temperature"")\nair = ds.air.isel(time=0)\n\nf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(8, 6))\n\n# The first plot (in kelvins) chooses ""viridis"" and uses the data\'s min/max\nair.plot(ax=ax1, cbar_kwargs={""label"": ""K""})\nax1.set_title(""Kelvins: default"")\nax2.set_xlabel("""")\n\n# The second plot (in celsius) now chooses ""BuRd"" and centers min/max around 0\nairc = air - 273.15\nairc.plot(ax=ax2, cbar_kwargs={""label"": ""\xc2\xb0C""})\nax2.set_title(""Celsius: default"")\nax2.set_xlabel("""")\nax2.set_ylabel("""")\n\n# The center doesn\'t have to be 0\nair.plot(ax=ax3, center=273.15, cbar_kwargs={""label"": ""K""})\nax3.set_title(""Kelvins: center=273.15"")\n\n# Or it can be ignored\nairc.plot(ax=ax4, center=False, cbar_kwargs={""label"": ""\xc2\xb0C""})\nax4.set_title(""Celsius: center=False"")\nax4.set_ylabel("""")\n\n# Mke it nice\nplt.tight_layout()\nplt.show()\n'"
doc/gallery/plot_control_colorbar.py,0,"b'""""""\n===========================\nControl the plot\'s colorbar\n===========================\n\nUse ``cbar_kwargs`` keyword to specify the number of ticks.\nThe ``spacing`` kwarg can be used to draw proportional ticks.\n""""""\nimport matplotlib.pyplot as plt\n\nimport xarray as xr\n\n# Load the data\nair_temp = xr.tutorial.load_dataset(""air_temperature"")\nair2d = air_temp.air.isel(time=500)\n\n# Prepare the figure\nf, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14, 4))\n\n# Irregular levels to illustrate the use of a proportional colorbar\nlevels = [245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 310, 340]\n\n# Plot data\nair2d.plot(ax=ax1, levels=levels)\nair2d.plot(ax=ax2, levels=levels, cbar_kwargs={""ticks"": levels})\nair2d.plot(\n    ax=ax3, levels=levels, cbar_kwargs={""ticks"": levels, ""spacing"": ""proportional""}\n)\n\n# Show plots\nplt.tight_layout()\nplt.show()\n'"
doc/gallery/plot_lines_from_2d.py,0,"b'""""""\n==================================\nMultiple lines from a 2d DataArray\n==================================\n\n\nUse :py:func:`xarray.plot.line` on a 2d DataArray to plot selections as\nmultiple lines.\n\nSee :ref:`plotting.multiplelines` for more details.\n\n""""""\n\nimport matplotlib.pyplot as plt\n\nimport xarray as xr\n\n# Load the data\nds = xr.tutorial.load_dataset(""air_temperature"")\nair = ds.air - 273.15  # to celsius\n\n# Prepare the figure\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharey=True)\n\n# Selected latitude indices\nisel_lats = [10, 15, 20]\n\n# Temperature vs longitude plot - illustrates the ""hue"" kwarg\nair.isel(time=0, lat=isel_lats).plot.line(ax=ax1, hue=""lat"")\nax1.set_ylabel(""\xc2\xb0C"")\n\n# Temperature vs time plot - illustrates the ""x"" and ""add_legend"" kwargs\nair.isel(lon=30, lat=isel_lats).plot.line(ax=ax2, x=""time"", add_legend=False)\nax2.set_ylabel("""")\n\n# Show\nplt.tight_layout()\nplt.show()\n'"
doc/gallery/plot_rasterio.py,3,"b'""""""\n.. _recipes.rasterio:\n\n=================================\nParsing rasterio\'s geocoordinates\n=================================\n\n\nConverting a projection\'s cartesian coordinates into 2D longitudes and\nlatitudes.\n\nThese new coordinates might be handy for plotting and indexing, but it should\nbe kept in mind that a grid which is regular in projection coordinates will\nlikely be irregular in lon/lat. It is often recommended to work in the data\'s\noriginal map projection (see :ref:`recipes.rasterio_rgb`).\n""""""\n\nimport cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom rasterio.warp import transform\n\nimport xarray as xr\n\n# Read the data\nurl = ""https://github.com/mapbox/rasterio/raw/master/tests/data/RGB.byte.tif""\nda = xr.open_rasterio(url)\n\n# Compute the lon/lat coordinates with rasterio.warp.transform\nny, nx = len(da[""y""]), len(da[""x""])\nx, y = np.meshgrid(da[""x""], da[""y""])\n\n# Rasterio works with 1D arrays\nlon, lat = transform(da.crs, {""init"": ""EPSG:4326""}, x.flatten(), y.flatten())\nlon = np.asarray(lon).reshape((ny, nx))\nlat = np.asarray(lat).reshape((ny, nx))\nda.coords[""lon""] = ((""y"", ""x""), lon)\nda.coords[""lat""] = ((""y"", ""x""), lat)\n\n# Compute a greyscale out of the rgb image\ngreyscale = da.mean(dim=""band"")\n\n# Plot on a map\nax = plt.subplot(projection=ccrs.PlateCarree())\ngreyscale.plot(\n    ax=ax,\n    x=""lon"",\n    y=""lat"",\n    transform=ccrs.PlateCarree(),\n    cmap=""Greys_r"",\n    add_colorbar=False,\n)\nax.coastlines(""10m"", color=""r"")\nplt.show()\n'"
doc/gallery/plot_rasterio_rgb.py,0,"b'""""""\n.. _recipes.rasterio_rgb:\n\n============================\nimshow() and map projections\n============================\n\nUsing rasterio\'s projection information for more accurate plots.\n\nThis example extends :ref:`recipes.rasterio` and plots the image in the\noriginal map projection instead of relying on pcolormesh and a map\ntransformation.\n""""""\n\nimport cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\n\nimport xarray as xr\n\n# Read the data\nurl = ""https://github.com/mapbox/rasterio/raw/master/tests/data/RGB.byte.tif""\nda = xr.open_rasterio(url)\n\n# The data is in UTM projection. We have to set it manually until\n# https://github.com/SciTools/cartopy/issues/813 is implemented\ncrs = ccrs.UTM(""18N"")\n\n# Plot on a map\nax = plt.subplot(projection=crs)\nda.plot.imshow(ax=ax, rgb=""band"", transform=crs)\nax.coastlines(""10m"", color=""r"")\nplt.show()\n'"
xarray/backends/__init__.py,0,"b'""""""Backend objects for saving and loading data\n\nDataStores provide a uniform interface for saving and loading data in different\nformats. They should not be used directly, but rather through Dataset objects.\n""""""\nfrom .cfgrib_ import CfGribDataStore\nfrom .common import AbstractDataStore\nfrom .file_manager import CachingFileManager, DummyFileManager, FileManager\nfrom .h5netcdf_ import H5NetCDFStore\nfrom .memory import InMemoryDataStore\nfrom .netCDF4_ import NetCDF4DataStore\nfrom .pseudonetcdf_ import PseudoNetCDFDataStore\nfrom .pydap_ import PydapDataStore\nfrom .pynio_ import NioDataStore\nfrom .scipy_ import ScipyDataStore\nfrom .zarr import ZarrStore\n\n__all__ = [\n    ""AbstractDataStore"",\n    ""FileManager"",\n    ""CachingFileManager"",\n    ""CfGribDataStore"",\n    ""DummyFileManager"",\n    ""InMemoryDataStore"",\n    ""NetCDF4DataStore"",\n    ""PydapDataStore"",\n    ""NioDataStore"",\n    ""ScipyDataStore"",\n    ""H5NetCDFStore"",\n    ""ZarrStore"",\n    ""PseudoNetCDFDataStore"",\n]\n'"
xarray/backends/api.py,10,"b'import os.path\nimport warnings\nfrom glob import glob\nfrom io import BytesIO\nfrom numbers import Number\nfrom pathlib import Path\nfrom textwrap import dedent\nfrom typing import (\n    TYPE_CHECKING,\n    Callable,\n    Dict,\n    Hashable,\n    Iterable,\n    Mapping,\n    Tuple,\n    Union,\n)\n\nimport numpy as np\n\nfrom .. import backends, coding, conventions\nfrom ..core import indexing\nfrom ..core.combine import (\n    _infer_concat_order_from_positions,\n    _nested_combine,\n    auto_combine,\n    combine_by_coords,\n)\nfrom ..core.dataarray import DataArray\nfrom ..core.dataset import Dataset\nfrom ..core.utils import close_on_error, is_grib_path, is_remote_uri\nfrom .common import AbstractDataStore, ArrayWriter\nfrom .locks import _get_scheduler\n\nif TYPE_CHECKING:\n    try:\n        from dask.delayed import Delayed\n    except ImportError:\n        Delayed = None\n\n\nDATAARRAY_NAME = ""__xarray_dataarray_name__""\nDATAARRAY_VARIABLE = ""__xarray_dataarray_variable__""\n\n\ndef _get_default_engine_remote_uri():\n    try:\n        import netCDF4  # noqa: F401\n\n        engine = ""netcdf4""\n    except ImportError:  # pragma: no cover\n        try:\n            import pydap  # noqa: F401\n\n            engine = ""pydap""\n        except ImportError:\n            raise ValueError(\n                ""netCDF4 or pydap is required for accessing ""\n                ""remote datasets via OPeNDAP""\n            )\n    return engine\n\n\ndef _get_default_engine_grib():\n    msgs = []\n    try:\n        import Nio  # noqa: F401\n\n        msgs += [""set engine=\'pynio\' to access GRIB files with PyNIO""]\n    except ImportError:  # pragma: no cover\n        pass\n    try:\n        import cfgrib  # noqa: F401\n\n        msgs += [""set engine=\'cfgrib\' to access GRIB files with cfgrib""]\n    except ImportError:  # pragma: no cover\n        pass\n    if msgs:\n        raise ValueError("" or\\n"".join(msgs))\n    else:\n        raise ValueError(""PyNIO or cfgrib is required for accessing "" ""GRIB files"")\n\n\ndef _get_default_engine_gz():\n    try:\n        import scipy  # noqa: F401\n\n        engine = ""scipy""\n    except ImportError:  # pragma: no cover\n        raise ValueError(""scipy is required for accessing .gz files"")\n    return engine\n\n\ndef _get_default_engine_netcdf():\n    try:\n        import netCDF4  # noqa: F401\n\n        engine = ""netcdf4""\n    except ImportError:  # pragma: no cover\n        try:\n            import scipy.io.netcdf  # noqa: F401\n\n            engine = ""scipy""\n        except ImportError:\n            raise ValueError(\n                ""cannot read or write netCDF files without ""\n                ""netCDF4-python or scipy installed""\n            )\n    return engine\n\n\ndef _get_engine_from_magic_number(filename_or_obj):\n    # check byte header to determine file type\n    if isinstance(filename_or_obj, bytes):\n        magic_number = filename_or_obj[:8]\n    else:\n        if filename_or_obj.tell() != 0:\n            raise ValueError(\n                ""file-like object read/write pointer not at zero ""\n                ""please close and reopen, or use a context ""\n                ""manager""\n            )\n        magic_number = filename_or_obj.read(8)\n        filename_or_obj.seek(0)\n\n    if magic_number.startswith(b""CDF""):\n        engine = ""scipy""\n    elif magic_number.startswith(b""\\211HDF\\r\\n\\032\\n""):\n        engine = ""h5netcdf""\n        if isinstance(filename_or_obj, bytes):\n            raise ValueError(\n                ""can\'t open netCDF4/HDF5 as bytes ""\n                ""try passing a path or file-like object""\n            )\n    else:\n        if isinstance(filename_or_obj, bytes) and len(filename_or_obj) > 80:\n            filename_or_obj = filename_or_obj[:80] + b""...""\n        raise ValueError(\n            ""{} is not a valid netCDF file ""\n            ""did you mean to pass a string for a path instead?"".format(filename_or_obj)\n        )\n    return engine\n\n\ndef _get_default_engine(path, allow_remote=False):\n    if allow_remote and is_remote_uri(path):\n        engine = _get_default_engine_remote_uri()\n    elif is_grib_path(path):\n        engine = _get_default_engine_grib()\n    elif path.endswith("".gz""):\n        engine = _get_default_engine_gz()\n    else:\n        engine = _get_default_engine_netcdf()\n    return engine\n\n\ndef _normalize_path(path):\n    if is_remote_uri(path):\n        return path\n    else:\n        return os.path.abspath(os.path.expanduser(path))\n\n\ndef _validate_dataset_names(dataset):\n    """"""DataArray.name and Dataset keys must be a string or None""""""\n\n    def check_name(name):\n        if isinstance(name, str):\n            if not name:\n                raise ValueError(\n                    ""Invalid name for DataArray or Dataset key: ""\n                    ""string must be length 1 or greater for ""\n                    ""serialization to netCDF files""\n                )\n        elif name is not None:\n            raise TypeError(\n                ""DataArray.name or Dataset key must be either a ""\n                ""string or None for serialization to netCDF files""\n            )\n\n    for k in dataset.variables:\n        check_name(k)\n\n\ndef _validate_attrs(dataset):\n    """"""`attrs` must have a string key and a value which is either: a number,\n    a string, an ndarray or a list/tuple of numbers/strings.\n    """"""\n\n    def check_attr(name, value):\n        if isinstance(name, str):\n            if not name:\n                raise ValueError(\n                    ""Invalid name for attr: string must be ""\n                    ""length 1 or greater for serialization to ""\n                    ""netCDF files""\n                )\n        else:\n            raise TypeError(\n                ""Invalid name for attr: {} must be a string for ""\n                ""serialization to netCDF files"".format(name)\n            )\n\n        if not isinstance(value, (str, Number, np.ndarray, np.number, list, tuple)):\n            raise TypeError(\n                ""Invalid value for attr: {} must be a number, ""\n                ""a string, an ndarray or a list/tuple of ""\n                ""numbers/strings for serialization to netCDF ""\n                ""files"".format(value)\n            )\n\n    # Check attrs on the dataset itself\n    for k, v in dataset.attrs.items():\n        check_attr(k, v)\n\n    # Check attrs on each variable within the dataset\n    for variable in dataset.variables.values():\n        for k, v in variable.attrs.items():\n            check_attr(k, v)\n\n\ndef _protect_dataset_variables_inplace(dataset, cache):\n    for name, variable in dataset.variables.items():\n        if name not in variable.dims:\n            # no need to protect IndexVariable objects\n            data = indexing.CopyOnWriteArray(variable._data)\n            if cache:\n                data = indexing.MemoryCachedArray(data)\n            variable.data = data\n\n\ndef _finalize_store(write, store):\n    """""" Finalize this store by explicitly syncing and closing""""""\n    del write  # ensure writing is done first\n    store.close()\n\n\ndef load_dataset(filename_or_obj, **kwargs):\n    """"""Open, load into memory, and close a Dataset from a file or file-like\n    object.\n\n    This is a thin wrapper around :py:meth:`~xarray.open_dataset`. It differs\n    from `open_dataset` in that it loads the Dataset into memory, closes the\n    file, and returns the Dataset. In contrast, `open_dataset` keeps the file\n    handle open and lazy loads its contents. All parameters are passed directly\n    to `open_dataset`. See that documentation for further details.\n\n    Returns\n    -------\n    dataset : Dataset\n        The newly created Dataset.\n\n    See Also\n    --------\n    open_dataset\n    """"""\n    if ""cache"" in kwargs:\n        raise TypeError(""cache has no effect in this context"")\n\n    with open_dataset(filename_or_obj, **kwargs) as ds:\n        return ds.load()\n\n\ndef load_dataarray(filename_or_obj, **kwargs):\n    """"""Open, load into memory, and close a DataArray from a file or file-like\n    object containing a single data variable.\n\n    This is a thin wrapper around :py:meth:`~xarray.open_dataarray`. It differs\n    from `open_dataarray` in that it loads the Dataset into memory, closes the\n    file, and returns the Dataset. In contrast, `open_dataarray` keeps the file\n    handle open and lazy loads its contents. All parameters are passed directly\n    to `open_dataarray`. See that documentation for further details.\n\n    Returns\n    -------\n    datarray : DataArray\n        The newly created DataArray.\n\n    See Also\n    --------\n    open_dataarray\n    """"""\n    if ""cache"" in kwargs:\n        raise TypeError(""cache has no effect in this context"")\n\n    with open_dataarray(filename_or_obj, **kwargs) as da:\n        return da.load()\n\n\ndef open_dataset(\n    filename_or_obj,\n    group=None,\n    decode_cf=True,\n    mask_and_scale=None,\n    decode_times=True,\n    autoclose=None,\n    concat_characters=True,\n    decode_coords=True,\n    engine=None,\n    chunks=None,\n    lock=None,\n    cache=None,\n    drop_variables=None,\n    backend_kwargs=None,\n    use_cftime=None,\n    decode_timedelta=None,\n):\n    """"""Open and decode a dataset from a file or file-like object.\n\n    Parameters\n    ----------\n    filename_or_obj : str, Path, file or xarray.backends.*DataStore\n        Strings and Path objects are interpreted as a path to a netCDF file\n        or an OpenDAP URL and opened with python-netCDF4, unless the filename\n        ends with .gz, in which case the file is gunzipped and opened with\n        scipy.io.netcdf (only netCDF3 supported). Byte-strings or file-like\n        objects are opened by scipy.io.netcdf (netCDF3) or h5py (netCDF4/HDF).\n    group : str, optional\n        Path to the netCDF4 group in the given file to open (only works for\n        netCDF4 files).\n    decode_cf : bool, optional\n        Whether to decode these variables, assuming they were saved according\n        to CF conventions.\n    mask_and_scale : bool, optional\n        If True, replace array values equal to `_FillValue` with NA and scale\n        values according to the formula `original_values * scale_factor +\n        add_offset`, where `_FillValue`, `scale_factor` and `add_offset` are\n        taken from variable attributes (if they exist).  If the `_FillValue` or\n        `missing_value` attribute contains multiple values a warning will be\n        issued and all array values matching one of the multiple values will\n        be replaced by NA. mask_and_scale defaults to True except for the\n        pseudonetcdf backend.\n    decode_times : bool, optional\n        If True, decode times encoded in the standard NetCDF datetime format\n        into datetime objects. Otherwise, leave them encoded as numbers.\n    autoclose : bool, optional\n        If True, automatically close files to avoid OS Error of too many files\n        being open.  However, this option doesn\'t work with streams, e.g.,\n        BytesIO.\n    concat_characters : bool, optional\n        If True, concatenate along the last dimension of character arrays to\n        form string arrays. Dimensions will only be concatenated over (and\n        removed) if they have no corresponding variable and if they are only\n        used as the last dimension of character arrays.\n    decode_coords : bool, optional\n        If True, decode the \'coordinates\' attribute to identify coordinates in\n        the resulting dataset.\n    engine : {\'netcdf4\', \'scipy\', \'pydap\', \'h5netcdf\', \'pynio\', \'cfgrib\', \\\n        \'pseudonetcdf\'}, optional\n        Engine to use when reading files. If not provided, the default engine\n        is chosen based on available dependencies, with a preference for\n        \'netcdf4\'.\n    chunks : int or dict, optional\n        If chunks is provided, it used to load the new dataset into dask\n        arrays. ``chunks={}`` loads the dataset with dask using a single\n        chunk for all arrays.\n    lock : False or duck threading.Lock, optional\n        Resource lock to use when reading data from disk. Only relevant when\n        using dask or another form of parallelism. By default, appropriate\n        locks are chosen to safely read and write files with the currently\n        active dask scheduler.\n    cache : bool, optional\n        If True, cache data loaded from the underlying datastore in memory as\n        NumPy arrays when accessed to avoid reading from the underlying data-\n        store multiple times. Defaults to True unless you specify the `chunks`\n        argument to use dask, in which case it defaults to False. Does not\n        change the behavior of coordinates corresponding to dimensions, which\n        always load their data from disk into a ``pandas.Index``.\n    drop_variables: string or iterable, optional\n        A variable or list of variables to exclude from being parsed from the\n        dataset. This may be useful to drop variables with problems or\n        inconsistent values.\n    backend_kwargs: dictionary, optional\n        A dictionary of keyword arguments to pass on to the backend. This\n        may be useful when backend options would improve performance or\n        allow user control of dataset processing.\n    use_cftime: bool, optional\n        Only relevant if encoded dates come from a standard calendar\n        (e.g. \'gregorian\', \'proleptic_gregorian\', \'standard\', or not\n        specified).  If None (default), attempt to decode times to\n        ``np.datetime64[ns]`` objects; if this is not possible, decode times to\n        ``cftime.datetime`` objects. If True, always decode times to\n        ``cftime.datetime`` objects, regardless of whether or not they can be\n        represented using ``np.datetime64[ns]`` objects.  If False, always\n        decode times to ``np.datetime64[ns]`` objects; if this is not possible\n        raise an error.\n    decode_timedelta : bool, optional\n        If True, decode variables and coordinates with time units in\n        {\'days\', \'hours\', \'minutes\', \'seconds\', \'milliseconds\', \'microseconds\'}\n        into timedelta objects. If False, leave them encoded as numbers.\n        If None (default), assume the same value of decode_time.\n\n    Returns\n    -------\n    dataset : Dataset\n        The newly created dataset.\n\n    Notes\n    -----\n    ``open_dataset`` opens the file with read-only access. When you modify\n    values of a Dataset, even one linked to files on disk, only the in-memory\n    copy you are manipulating in xarray is modified: the original file on disk\n    is never touched.\n\n    See Also\n    --------\n    open_mfdataset\n    """"""\n    engines = [\n        None,\n        ""netcdf4"",\n        ""scipy"",\n        ""pydap"",\n        ""h5netcdf"",\n        ""pynio"",\n        ""cfgrib"",\n        ""pseudonetcdf"",\n    ]\n    if engine not in engines:\n        raise ValueError(\n            ""unrecognized engine for open_dataset: {}\\n""\n            ""must be one of: {}"".format(engine, engines)\n        )\n\n    if autoclose is not None:\n        warnings.warn(\n            ""The autoclose argument is no longer used by ""\n            ""xarray.open_dataset() and is now ignored; it will be removed in ""\n            ""a future version of xarray. If necessary, you can control the ""\n            ""maximum number of simultaneous open files with ""\n            ""xarray.set_options(file_cache_maxsize=...)."",\n            FutureWarning,\n            stacklevel=2,\n        )\n\n    if mask_and_scale is None:\n        mask_and_scale = not engine == ""pseudonetcdf""\n\n    if not decode_cf:\n        mask_and_scale = False\n        decode_times = False\n        concat_characters = False\n        decode_coords = False\n        decode_timedelta = False\n\n    if cache is None:\n        cache = chunks is None\n\n    if backend_kwargs is None:\n        backend_kwargs = {}\n\n    def maybe_decode_store(store, lock=False):\n        ds = conventions.decode_cf(\n            store,\n            mask_and_scale=mask_and_scale,\n            decode_times=decode_times,\n            concat_characters=concat_characters,\n            decode_coords=decode_coords,\n            drop_variables=drop_variables,\n            use_cftime=use_cftime,\n            decode_timedelta=decode_timedelta,\n        )\n\n        _protect_dataset_variables_inplace(ds, cache)\n\n        if chunks is not None:\n            from dask.base import tokenize\n\n            # if passed an actual file path, augment the token with\n            # the file modification time\n            if isinstance(filename_or_obj, str) and not is_remote_uri(filename_or_obj):\n                mtime = os.path.getmtime(filename_or_obj)\n            else:\n                mtime = None\n            token = tokenize(\n                filename_or_obj,\n                mtime,\n                group,\n                decode_cf,\n                mask_and_scale,\n                decode_times,\n                concat_characters,\n                decode_coords,\n                engine,\n                chunks,\n                drop_variables,\n                use_cftime,\n                decode_timedelta,\n            )\n            name_prefix = ""open_dataset-%s"" % token\n            ds2 = ds.chunk(chunks, name_prefix=name_prefix, token=token)\n            ds2._file_obj = ds._file_obj\n        else:\n            ds2 = ds\n\n        return ds2\n\n    if isinstance(filename_or_obj, Path):\n        filename_or_obj = str(filename_or_obj)\n\n    if isinstance(filename_or_obj, AbstractDataStore):\n        store = filename_or_obj\n\n    elif isinstance(filename_or_obj, str):\n        filename_or_obj = _normalize_path(filename_or_obj)\n\n        if engine is None:\n            engine = _get_default_engine(filename_or_obj, allow_remote=True)\n        if engine == ""netcdf4"":\n            store = backends.NetCDF4DataStore.open(\n                filename_or_obj, group=group, lock=lock, **backend_kwargs\n            )\n        elif engine == ""scipy"":\n            store = backends.ScipyDataStore(filename_or_obj, **backend_kwargs)\n        elif engine == ""pydap"":\n            store = backends.PydapDataStore.open(filename_or_obj, **backend_kwargs)\n        elif engine == ""h5netcdf"":\n            store = backends.H5NetCDFStore.open(\n                filename_or_obj, group=group, lock=lock, **backend_kwargs\n            )\n        elif engine == ""pynio"":\n            store = backends.NioDataStore(filename_or_obj, lock=lock, **backend_kwargs)\n        elif engine == ""pseudonetcdf"":\n            store = backends.PseudoNetCDFDataStore.open(\n                filename_or_obj, lock=lock, **backend_kwargs\n            )\n        elif engine == ""cfgrib"":\n            store = backends.CfGribDataStore(\n                filename_or_obj, lock=lock, **backend_kwargs\n            )\n\n    else:\n        if engine not in [None, ""scipy"", ""h5netcdf""]:\n            raise ValueError(\n                ""can only read bytes or file-like objects ""\n                ""with engine=\'scipy\' or \'h5netcdf\'""\n            )\n        engine = _get_engine_from_magic_number(filename_or_obj)\n        if engine == ""scipy"":\n            store = backends.ScipyDataStore(filename_or_obj, **backend_kwargs)\n        elif engine == ""h5netcdf"":\n            store = backends.H5NetCDFStore.open(\n                filename_or_obj, group=group, lock=lock, **backend_kwargs\n            )\n\n    with close_on_error(store):\n        ds = maybe_decode_store(store)\n\n    # Ensure source filename always stored in dataset object (GH issue #2550)\n    if ""source"" not in ds.encoding:\n        if isinstance(filename_or_obj, str):\n            ds.encoding[""source""] = filename_or_obj\n\n    return ds\n\n\ndef open_dataarray(\n    filename_or_obj,\n    group=None,\n    decode_cf=True,\n    mask_and_scale=None,\n    decode_times=True,\n    autoclose=None,\n    concat_characters=True,\n    decode_coords=True,\n    engine=None,\n    chunks=None,\n    lock=None,\n    cache=None,\n    drop_variables=None,\n    backend_kwargs=None,\n    use_cftime=None,\n    decode_timedelta=None,\n):\n    """"""Open an DataArray from a file or file-like object containing a single\n    data variable.\n\n    This is designed to read netCDF files with only one data variable. If\n    multiple variables are present then a ValueError is raised.\n\n    Parameters\n    ----------\n    filename_or_obj : str, Path, file or xarray.backends.*DataStore\n        Strings and Paths are interpreted as a path to a netCDF file or an\n        OpenDAP URL and opened with python-netCDF4, unless the filename ends\n        with .gz, in which case the file is gunzipped and opened with\n        scipy.io.netcdf (only netCDF3 supported). Byte-strings or file-like\n        objects are opened by scipy.io.netcdf (netCDF3) or h5py (netCDF4/HDF).\n    group : str, optional\n        Path to the netCDF4 group in the given file to open (only works for\n        netCDF4 files).\n    decode_cf : bool, optional\n        Whether to decode these variables, assuming they were saved according\n        to CF conventions.\n    mask_and_scale : bool, optional\n        If True, replace array values equal to `_FillValue` with NA and scale\n        values according to the formula `original_values * scale_factor +\n        add_offset`, where `_FillValue`, `scale_factor` and `add_offset` are\n        taken from variable attributes (if they exist).  If the `_FillValue` or\n        `missing_value` attribute contains multiple values a warning will be\n        issued and all array values matching one of the multiple values will\n        be replaced by NA. mask_and_scale defaults to True except for the\n        pseudonetcdf backend.\n    decode_times : bool, optional\n        If True, decode times encoded in the standard NetCDF datetime format\n        into datetime objects. Otherwise, leave them encoded as numbers.\n    concat_characters : bool, optional\n        If True, concatenate along the last dimension of character arrays to\n        form string arrays. Dimensions will only be concatenated over (and\n        removed) if they have no corresponding variable and if they are only\n        used as the last dimension of character arrays.\n    decode_coords : bool, optional\n        If True, decode the \'coordinates\' attribute to identify coordinates in\n        the resulting dataset.\n    engine : {\'netcdf4\', \'scipy\', \'pydap\', \'h5netcdf\', \'pynio\', \'cfgrib\'}, \\\n        optional\n        Engine to use when reading files. If not provided, the default engine\n        is chosen based on available dependencies, with a preference for\n        \'netcdf4\'.\n    chunks : int or dict, optional\n        If chunks is provided, it used to load the new dataset into dask\n        arrays.\n    lock : False or duck threading.Lock, optional\n        Resource lock to use when reading data from disk. Only relevant when\n        using dask or another form of parallelism. By default, appropriate\n        locks are chosen to safely read and write files with the currently\n        active dask scheduler.\n    cache : bool, optional\n        If True, cache data loaded from the underlying datastore in memory as\n        NumPy arrays when accessed to avoid reading from the underlying data-\n        store multiple times. Defaults to True unless you specify the `chunks`\n        argument to use dask, in which case it defaults to False. Does not\n        change the behavior of coordinates corresponding to dimensions, which\n        always load their data from disk into a ``pandas.Index``.\n    drop_variables: string or iterable, optional\n        A variable or list of variables to exclude from being parsed from the\n        dataset. This may be useful to drop variables with problems or\n        inconsistent values.\n    backend_kwargs: dictionary, optional\n        A dictionary of keyword arguments to pass on to the backend. This\n        may be useful when backend options would improve performance or\n        allow user control of dataset processing.\n    use_cftime: bool, optional\n        Only relevant if encoded dates come from a standard calendar\n        (e.g. \'gregorian\', \'proleptic_gregorian\', \'standard\', or not\n        specified).  If None (default), attempt to decode times to\n        ``np.datetime64[ns]`` objects; if this is not possible, decode times to\n        ``cftime.datetime`` objects. If True, always decode times to\n        ``cftime.datetime`` objects, regardless of whether or not they can be\n        represented using ``np.datetime64[ns]`` objects.  If False, always\n        decode times to ``np.datetime64[ns]`` objects; if this is not possible\n        raise an error.\n    decode_timedelta : bool, optional\n        If True, decode variables and coordinates with time units in\n        {\'days\', \'hours\', \'minutes\', \'seconds\', \'milliseconds\', \'microseconds\'}\n        into timedelta objects. If False, leave them encoded as numbers.\n        If None (default), assume the same value of decode_time.\n\n    Notes\n    -----\n    This is designed to be fully compatible with `DataArray.to_netcdf`. Saving\n    using `DataArray.to_netcdf` and then loading with this function will\n    produce an identical result.\n\n    All parameters are passed directly to `xarray.open_dataset`. See that\n    documentation for further details.\n\n    See also\n    --------\n    open_dataset\n    """"""\n\n    dataset = open_dataset(\n        filename_or_obj,\n        group=group,\n        decode_cf=decode_cf,\n        mask_and_scale=mask_and_scale,\n        decode_times=decode_times,\n        autoclose=autoclose,\n        concat_characters=concat_characters,\n        decode_coords=decode_coords,\n        engine=engine,\n        chunks=chunks,\n        lock=lock,\n        cache=cache,\n        drop_variables=drop_variables,\n        backend_kwargs=backend_kwargs,\n        use_cftime=use_cftime,\n        decode_timedelta=decode_timedelta,\n    )\n\n    if len(dataset.data_vars) != 1:\n        raise ValueError(\n            ""Given file dataset contains more than one data ""\n            ""variable. Please read with xarray.open_dataset and ""\n            ""then select the variable you want.""\n        )\n    else:\n        (data_array,) = dataset.data_vars.values()\n\n    data_array._file_obj = dataset._file_obj\n\n    # Reset names if they were changed during saving\n    # to ensure that we can \'roundtrip\' perfectly\n    if DATAARRAY_NAME in dataset.attrs:\n        data_array.name = dataset.attrs[DATAARRAY_NAME]\n        del dataset.attrs[DATAARRAY_NAME]\n\n    if data_array.name == DATAARRAY_VARIABLE:\n        data_array.name = None\n\n    return data_array\n\n\nclass _MultiFileCloser:\n    __slots__ = (""file_objs"",)\n\n    def __init__(self, file_objs):\n        self.file_objs = file_objs\n\n    def close(self):\n        for f in self.file_objs:\n            f.close()\n\n\ndef open_mfdataset(\n    paths,\n    chunks=None,\n    concat_dim=""_not_supplied"",\n    compat=""no_conflicts"",\n    preprocess=None,\n    engine=None,\n    lock=None,\n    data_vars=""all"",\n    coords=""different"",\n    combine=""_old_auto"",\n    autoclose=None,\n    parallel=False,\n    join=""outer"",\n    attrs_file=None,\n    **kwargs,\n):\n    """"""Open multiple files as a single dataset.\n\n    If combine=\'by_coords\' then the function ``combine_by_coords`` is used to combine\n    the datasets into one before returning the result, and if combine=\'nested\' then\n    ``combine_nested`` is used. The filepaths must be structured according to which\n    combining function is used, the details of which are given in the documentation for\n    ``combine_by_coords`` and ``combine_nested``. By default the old (now deprecated)\n    ``auto_combine`` will be used, please specify either ``combine=\'by_coords\'`` or\n    ``combine=\'nested\'`` in future. Requires dask to be installed. See documentation for\n    details on dask [1]_. Global attributes from the ``attrs_file`` are used\n    for the combined dataset.\n\n    Parameters\n    ----------\n    paths : str or sequence\n        Either a string glob in the form ``""path/to/my/files/*.nc""`` or an explicit list of\n        files to open. Paths can be given as strings or as pathlib Paths. If\n        concatenation along more than one dimension is desired, then ``paths`` must be a\n        nested list-of-lists (see ``manual_combine`` for details). (A string glob will\n        be expanded to a 1-dimensional list.)\n    chunks : int or dict, optional\n        Dictionary with keys given by dimension names and values given by chunk sizes.\n        In general, these should divide the dimensions of each dataset. If int, chunk\n        each dimension by ``chunks``. By default, chunks will be chosen to load entire\n        input files into memory at once. This has a major impact on performance: please\n        see the full documentation for more details [2]_.\n    concat_dim : str, or list of str, DataArray, Index or None, optional\n        Dimensions to concatenate files along.  You only need to provide this argument\n        if any of the dimensions along which you want to concatenate is not a dimension\n        in the original datasets, e.g., if you want to stack a collection of 2D arrays\n        along a third dimension. Set ``concat_dim=[..., None, ...]`` explicitly to\n        disable concatenation along a particular dimension.\n    combine : {\'by_coords\', \'nested\'}, optional\n        Whether ``xarray.combine_by_coords`` or ``xarray.combine_nested`` is used to\n        combine all the data. If this argument is not provided, `xarray.auto_combine` is\n        used, but in the future this behavior will switch to use\n        `xarray.combine_by_coords` by default.\n    compat : {\'identical\', \'equals\', \'broadcast_equals\',\n              \'no_conflicts\', \'override\'}, optional\n        String indicating how to compare variables of the same name for\n        potential conflicts when merging:\n\n         * \'broadcast_equals\': all values must be equal when variables are\n           broadcast against each other to ensure common dimensions.\n         * \'equals\': all values and dimensions must be the same.\n         * \'identical\': all values, dimensions and attributes must be the\n           same.\n         * \'no_conflicts\': only values which are not null in both datasets\n           must be equal. The returned dataset then contains the combination\n           of all non-null values.\n         * \'override\': skip comparing and pick variable from first dataset\n\n    preprocess : callable, optional\n        If provided, call this function on each dataset prior to concatenation.\n        You can find the file-name from which each dataset was loaded in\n        ``ds.encoding[\'source\']``.\n    engine : {\'netcdf4\', \'scipy\', \'pydap\', \'h5netcdf\', \'pynio\', \'cfgrib\'}, \\\n        optional\n        Engine to use when reading files. If not provided, the default engine\n        is chosen based on available dependencies, with a preference for\n        \'netcdf4\'.\n    lock : False or duck threading.Lock, optional\n        Resource lock to use when reading data from disk. Only relevant when\n        using dask or another form of parallelism. By default, appropriate\n        locks are chosen to safely read and write files with the currently\n        active dask scheduler.\n    data_vars : {\'minimal\', \'different\', \'all\' or list of str}, optional\n        These data variables will be concatenated together:\n          * \'minimal\': Only data variables in which the dimension already\n            appears are included.\n          * \'different\': Data variables which are not equal (ignoring\n            attributes) across all datasets are also concatenated (as well as\n            all for which dimension already appears). Beware: this option may\n            load the data payload of data variables into memory if they are not\n            already loaded.\n          * \'all\': All data variables will be concatenated.\n          * list of str: The listed data variables will be concatenated, in\n            addition to the \'minimal\' data variables.\n    coords : {\'minimal\', \'different\', \'all\' or list of str}, optional\n        These coordinate variables will be concatenated together:\n         * \'minimal\': Only coordinates in which the dimension already appears\n           are included.\n         * \'different\': Coordinates which are not equal (ignoring attributes)\n           across all datasets are also concatenated (as well as all for which\n           dimension already appears). Beware: this option may load the data\n           payload of coordinate variables into memory if they are not already\n           loaded.\n         * \'all\': All coordinate variables will be concatenated, except\n           those corresponding to other dimensions.\n         * list of str: The listed coordinate variables will be concatenated,\n           in addition the \'minimal\' coordinates.\n    parallel : bool, optional\n        If True, the open and preprocess steps of this function will be\n        performed in parallel using ``dask.delayed``. Default is False.\n    join : {\'outer\', \'inner\', \'left\', \'right\', \'exact, \'override\'}, optional\n        String indicating how to combine differing indexes\n        (excluding concat_dim) in objects\n\n        - \'outer\': use the union of object indexes\n        - \'inner\': use the intersection of object indexes\n        - \'left\': use indexes from the first object with each dimension\n        - \'right\': use indexes from the last object with each dimension\n        - \'exact\': instead of aligning, raise `ValueError` when indexes to be\n          aligned are not equal\n        - \'override\': if indexes are of same size, rewrite indexes to be\n          those of the first object with that dimension. Indexes for the same\n          dimension must have the same size in all objects.\n    attrs_file : str or pathlib.Path, optional\n        Path of the file used to read global attributes from.\n        By default global attributes are read from the first file provided,\n        with wildcard matches sorted by filename.\n    **kwargs : optional\n        Additional arguments passed on to :py:func:`xarray.open_dataset`.\n\n    Returns\n    -------\n    xarray.Dataset\n\n    Notes\n    -----\n    ``open_mfdataset`` opens files with read-only access. When you modify values\n    of a Dataset, even one linked to files on disk, only the in-memory copy you\n    are manipulating in xarray is modified: the original file on disk is never\n    touched.\n\n    See Also\n    --------\n    combine_by_coords\n    combine_nested\n    auto_combine\n    open_dataset\n\n    References\n    ----------\n\n    .. [1] http://xarray.pydata.org/en/stable/dask.html\n    .. [2] http://xarray.pydata.org/en/stable/dask.html#chunking-and-performance\n    """"""\n    if isinstance(paths, str):\n        if is_remote_uri(paths):\n            raise ValueError(\n                ""cannot do wild-card matching for paths that are remote URLs: ""\n                ""{!r}. Instead, supply paths as an explicit list of strings."".format(\n                    paths\n                )\n            )\n        paths = sorted(glob(paths))\n    else:\n        paths = [str(p) if isinstance(p, Path) else p for p in paths]\n\n    if not paths:\n        raise OSError(""no files to open"")\n\n    # If combine=\'by_coords\' then this is unnecessary, but quick.\n    # If combine=\'nested\' then this creates a flat list which is easier to\n    # iterate over, while saving the originally-supplied structure as ""ids""\n    if combine == ""nested"":\n        if str(concat_dim) == ""_not_supplied"":\n            raise ValueError(""Must supply concat_dim when using "" ""combine=\'nested\'"")\n        else:\n            if isinstance(concat_dim, (str, DataArray)) or concat_dim is None:\n                concat_dim = [concat_dim]\n    combined_ids_paths = _infer_concat_order_from_positions(paths)\n    ids, paths = (list(combined_ids_paths.keys()), list(combined_ids_paths.values()))\n\n    open_kwargs = dict(\n        engine=engine, chunks=chunks or {}, lock=lock, autoclose=autoclose, **kwargs\n    )\n\n    if parallel:\n        import dask\n\n        # wrap the open_dataset, getattr, and preprocess with delayed\n        open_ = dask.delayed(open_dataset)\n        getattr_ = dask.delayed(getattr)\n        if preprocess is not None:\n            preprocess = dask.delayed(preprocess)\n    else:\n        open_ = open_dataset\n        getattr_ = getattr\n\n    datasets = [open_(p, **open_kwargs) for p in paths]\n    file_objs = [getattr_(ds, ""_file_obj"") for ds in datasets]\n    if preprocess is not None:\n        datasets = [preprocess(ds) for ds in datasets]\n\n    if parallel:\n        # calling compute here will return the datasets/file_objs lists,\n        # the underlying datasets will still be stored as dask arrays\n        datasets, file_objs = dask.compute(datasets, file_objs)\n\n    # Combine all datasets, closing them in case of a ValueError\n    try:\n        if combine == ""_old_auto"":\n            # Use the old auto_combine for now\n            # Remove this after deprecation cycle from #2616 is complete\n            basic_msg = dedent(\n                """"""\\\n            In xarray version 0.15 the default behaviour of `open_mfdataset`\n            will change. To retain the existing behavior, pass\n            combine=\'nested\'. To use future default behavior, pass\n            combine=\'by_coords\'. See\n            http://xarray.pydata.org/en/stable/combining.html#combining-multi\n            """"""\n            )\n            warnings.warn(basic_msg, FutureWarning, stacklevel=2)\n\n            combined = auto_combine(\n                datasets,\n                concat_dim=concat_dim,\n                compat=compat,\n                data_vars=data_vars,\n                coords=coords,\n                join=join,\n                from_openmfds=True,\n            )\n        elif combine == ""nested"":\n            # Combined nested list by successive concat and merge operations\n            # along each dimension, using structure given by ""ids""\n            combined = _nested_combine(\n                datasets,\n                concat_dims=concat_dim,\n                compat=compat,\n                data_vars=data_vars,\n                coords=coords,\n                ids=ids,\n                join=join,\n            )\n        elif combine == ""by_coords"":\n            # Redo ordering from coordinates, ignoring how they were ordered\n            # previously\n            combined = combine_by_coords(\n                datasets, compat=compat, data_vars=data_vars, coords=coords, join=join\n            )\n        else:\n            raise ValueError(\n                ""{} is an invalid option for the keyword argument""\n                "" ``combine``"".format(combine)\n            )\n    except ValueError:\n        for ds in datasets:\n            ds.close()\n        raise\n\n    combined._file_obj = _MultiFileCloser(file_objs)\n\n    # read global attributes from the attrs_file or from the first dataset\n    if attrs_file is not None:\n        if isinstance(attrs_file, Path):\n            attrs_file = str(attrs_file)\n        combined.attrs = datasets[paths.index(attrs_file)].attrs\n    else:\n        combined.attrs = datasets[0].attrs\n\n    return combined\n\n\nWRITEABLE_STORES: Dict[str, Callable] = {\n    ""netcdf4"": backends.NetCDF4DataStore.open,\n    ""scipy"": backends.ScipyDataStore,\n    ""h5netcdf"": backends.H5NetCDFStore.open,\n}\n\n\ndef to_netcdf(\n    dataset: Dataset,\n    path_or_file=None,\n    mode: str = ""w"",\n    format: str = None,\n    group: str = None,\n    engine: str = None,\n    encoding: Mapping = None,\n    unlimited_dims: Iterable[Hashable] = None,\n    compute: bool = True,\n    multifile: bool = False,\n    invalid_netcdf: bool = False,\n) -> Union[Tuple[ArrayWriter, AbstractDataStore], bytes, ""Delayed"", None]:\n    """"""This function creates an appropriate datastore for writing a dataset to\n    disk as a netCDF file\n\n    See `Dataset.to_netcdf` for full API docs.\n\n    The ``multifile`` argument is only for the private use of save_mfdataset.\n    """"""\n    if isinstance(path_or_file, Path):\n        path_or_file = str(path_or_file)\n\n    if encoding is None:\n        encoding = {}\n\n    if path_or_file is None:\n        if engine is None:\n            engine = ""scipy""\n        elif engine != ""scipy"":\n            raise ValueError(\n                ""invalid engine for creating bytes with ""\n                ""to_netcdf: %r. Only the default engine ""\n                ""or engine=\'scipy\' is supported"" % engine\n            )\n        if not compute:\n            raise NotImplementedError(\n                ""to_netcdf() with compute=False is not yet implemented when ""\n                ""returning bytes""\n            )\n    elif isinstance(path_or_file, str):\n        if engine is None:\n            engine = _get_default_engine(path_or_file)\n        path_or_file = _normalize_path(path_or_file)\n    else:  # file-like object\n        engine = ""scipy""\n\n    # validate Dataset keys, DataArray names, and attr keys/values\n    _validate_dataset_names(dataset)\n    _validate_attrs(dataset)\n\n    try:\n        store_open = WRITEABLE_STORES[engine]\n    except KeyError:\n        raise ValueError(""unrecognized engine for to_netcdf: %r"" % engine)\n\n    if format is not None:\n        format = format.upper()\n\n    # handle scheduler specific logic\n    scheduler = _get_scheduler()\n    have_chunks = any(v.chunks for v in dataset.variables.values())\n\n    autoclose = have_chunks and scheduler in [""distributed"", ""multiprocessing""]\n    if autoclose and engine == ""scipy"":\n        raise NotImplementedError(\n            ""Writing netCDF files with the %s backend ""\n            ""is not currently supported with dask\'s %s ""\n            ""scheduler"" % (engine, scheduler)\n        )\n\n    target = path_or_file if path_or_file is not None else BytesIO()\n    kwargs = dict(autoclose=True) if autoclose else {}\n    if invalid_netcdf:\n        if engine == ""h5netcdf"":\n            kwargs[""invalid_netcdf""] = invalid_netcdf\n        else:\n            raise ValueError(\n                ""unrecognized option \'invalid_netcdf\' for engine %s"" % engine\n            )\n    store = store_open(target, mode, format, group, **kwargs)\n\n    if unlimited_dims is None:\n        unlimited_dims = dataset.encoding.get(""unlimited_dims"", None)\n    if unlimited_dims is not None:\n        if isinstance(unlimited_dims, str) or not isinstance(unlimited_dims, Iterable):\n            unlimited_dims = [unlimited_dims]\n        else:\n            unlimited_dims = list(unlimited_dims)\n\n    writer = ArrayWriter()\n\n    # TODO: figure out how to refactor this logic (here and in save_mfdataset)\n    # to avoid this mess of conditionals\n    try:\n        # TODO: allow this work (setting up the file for writing array data)\n        # to be parallelized with dask\n        dump_to_store(\n            dataset, store, writer, encoding=encoding, unlimited_dims=unlimited_dims\n        )\n        if autoclose:\n            store.close()\n\n        if multifile:\n            return writer, store\n\n        writes = writer.sync(compute=compute)\n\n        if path_or_file is None:\n            store.sync()\n            return target.getvalue()\n    finally:\n        if not multifile and compute:\n            store.close()\n\n    if not compute:\n        import dask\n\n        return dask.delayed(_finalize_store)(writes, store)\n    return None\n\n\ndef dump_to_store(\n    dataset, store, writer=None, encoder=None, encoding=None, unlimited_dims=None\n):\n    """"""Store dataset contents to a backends.*DataStore object.""""""\n    if writer is None:\n        writer = ArrayWriter()\n\n    if encoding is None:\n        encoding = {}\n\n    variables, attrs = conventions.encode_dataset_coordinates(dataset)\n\n    check_encoding = set()\n    for k, enc in encoding.items():\n        # no need to shallow copy the variable again; that already happened\n        # in encode_dataset_coordinates\n        variables[k].encoding = enc\n        check_encoding.add(k)\n\n    if encoder:\n        variables, attrs = encoder(variables, attrs)\n\n    store.store(variables, attrs, check_encoding, writer, unlimited_dims=unlimited_dims)\n\n\ndef save_mfdataset(\n    datasets, paths, mode=""w"", format=None, groups=None, engine=None, compute=True\n):\n    """"""Write multiple datasets to disk as netCDF files simultaneously.\n\n    This function is intended for use with datasets consisting of dask.array\n    objects, in which case it can write the multiple datasets to disk\n    simultaneously using a shared thread pool.\n\n    When not using dask, it is no different than calling ``to_netcdf``\n    repeatedly.\n\n    Parameters\n    ----------\n    datasets : list of xarray.Dataset\n        List of datasets to save.\n    paths : list of str or list of Paths\n        List of paths to which to save each corresponding dataset.\n    mode : {\'w\', \'a\'}, optional\n        Write (\'w\') or append (\'a\') mode. If mode=\'w\', any existing file at\n        these locations will be overwritten.\n    format : {\'NETCDF4\', \'NETCDF4_CLASSIC\', \'NETCDF3_64BIT\',\n              \'NETCDF3_CLASSIC\'}, optional\n\n        File format for the resulting netCDF file:\n\n        * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API\n          features.\n        * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only\n          netCDF 3 compatible API features.\n        * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,\n          which fully supports 2+ GB files, but is only compatible with\n          clients linked against netCDF version 3.6.0 or later.\n        * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not\n          handle 2+ GB files very well.\n\n        All formats are supported by the netCDF4-python library.\n        scipy.io.netcdf only supports the last two formats.\n\n        The default format is NETCDF4 if you are saving a file to disk and\n        have the netCDF4-python library available. Otherwise, xarray falls\n        back to using scipy to write netCDF files and defaults to the\n        NETCDF3_64BIT format (scipy does not support netCDF4).\n    groups : list of str, optional\n        Paths to the netCDF4 group in each corresponding file to which to save\n        datasets (only works for format=\'NETCDF4\'). The groups will be created\n        if necessary.\n    engine : {\'netcdf4\', \'scipy\', \'h5netcdf\'}, optional\n        Engine to use when writing netCDF files. If not provided, the\n        default engine is chosen based on available dependencies, with a\n        preference for \'netcdf4\' if writing to a file on disk.\n        See `Dataset.to_netcdf` for additional information.\n    compute: boolean\n        If true compute immediately, otherwise return a\n        ``dask.delayed.Delayed`` object that can be computed later.\n\n    Examples\n    --------\n\n    Save a dataset into one netCDF per year of data:\n\n    >>> years, datasets = zip(*ds.groupby(""time.year""))\n    >>> paths = [""%s.nc"" % y for y in years]\n    >>> xr.save_mfdataset(datasets, paths)\n    """"""\n    if mode == ""w"" and len(set(paths)) < len(paths):\n        raise ValueError(\n            ""cannot use mode=\'w\' when writing multiple "" ""datasets to the same path""\n        )\n\n    for obj in datasets:\n        if not isinstance(obj, Dataset):\n            raise TypeError(\n                ""save_mfdataset only supports writing Dataset ""\n                ""objects, received type %s"" % type(obj)\n            )\n\n    if groups is None:\n        groups = [None] * len(datasets)\n\n    if len({len(datasets), len(paths), len(groups)}) > 1:\n        raise ValueError(\n            ""must supply lists of the same length for the ""\n            ""datasets, paths and groups arguments to ""\n            ""save_mfdataset""\n        )\n\n    writers, stores = zip(\n        *[\n            to_netcdf(\n                ds, path, mode, format, group, engine, compute=compute, multifile=True\n            )\n            for ds, path, group in zip(datasets, paths, groups)\n        ]\n    )\n\n    try:\n        writes = [w.sync(compute=compute) for w in writers]\n    finally:\n        if compute:\n            for store in stores:\n                store.close()\n\n    if not compute:\n        import dask\n\n        return dask.delayed(\n            [dask.delayed(_finalize_store)(w, s) for w, s in zip(writes, stores)]\n        )\n\n\ndef _validate_datatypes_for_zarr_append(dataset):\n    """"""DataArray.name and Dataset keys must be a string or None""""""\n\n    def check_dtype(var):\n        if (\n            not np.issubdtype(var.dtype, np.number)\n            and not np.issubdtype(var.dtype, np.datetime64)\n            and not np.issubdtype(var.dtype, np.bool_)\n            and not coding.strings.is_unicode_dtype(var.dtype)\n            and not var.dtype == object\n        ):\n            # and not re.match(\'^bytes[1-9]+$\', var.dtype.name)):\n            raise ValueError(\n                ""Invalid dtype for data variable: {} ""\n                ""dtype must be a subtype of number, ""\n                ""datetime, bool, a fixed sized string, ""\n                ""a fixed size unicode string or an ""\n                ""object"".format(var)\n            )\n\n    for k in dataset.data_vars.values():\n        check_dtype(k)\n\n\ndef _validate_append_dim_and_encoding(\n    ds_to_append, store, append_dim, encoding, **open_kwargs\n):\n    try:\n        ds = backends.zarr.open_zarr(store, **open_kwargs)\n    except ValueError:  # store empty\n        return\n    if append_dim:\n        if append_dim not in ds.dims:\n            raise ValueError(\n                f""append_dim={append_dim!r} does not match any existing ""\n                f""dataset dimensions {ds.dims}""\n            )\n    for var_name in ds_to_append:\n        if var_name in ds:\n            if ds_to_append[var_name].dims != ds[var_name].dims:\n                raise ValueError(\n                    f""variable {var_name!r} already exists with different ""\n                    f""dimension names {ds[var_name].dims} != ""\n                    f""{ds_to_append[var_name].dims}, but changing variable ""\n                    ""dimensions is not supported by to_zarr().""\n                )\n            existing_sizes = {\n                k: v for k, v in ds[var_name].sizes.items() if k != append_dim\n            }\n            new_sizes = {\n                k: v for k, v in ds_to_append[var_name].sizes.items() if k != append_dim\n            }\n            if existing_sizes != new_sizes:\n                raise ValueError(\n                    f""variable {var_name!r} already exists with different ""\n                    ""dimension sizes: {existing_sizes} != {new_sizes}. ""\n                    ""to_zarr() only supports changing dimension sizes when ""\n                    f""explicitly appending, but append_dim={append_dim!r}.""\n                )\n            if var_name in encoding.keys():\n                raise ValueError(\n                    f""variable {var_name!r} already exists, but encoding was provided""\n                )\n\n\ndef to_zarr(\n    dataset,\n    store=None,\n    mode=None,\n    synchronizer=None,\n    group=None,\n    encoding=None,\n    compute=True,\n    consolidated=False,\n    append_dim=None,\n):\n    """"""This function creates an appropriate datastore for writing a dataset to\n    a zarr ztore\n\n    See `Dataset.to_zarr` for full API docs.\n    """"""\n    if isinstance(store, Path):\n        store = str(store)\n    if encoding is None:\n        encoding = {}\n\n    # validate Dataset keys, DataArray names, and attr keys/values\n    _validate_dataset_names(dataset)\n    _validate_attrs(dataset)\n\n    if mode == ""a"":\n        _validate_datatypes_for_zarr_append(dataset)\n        _validate_append_dim_and_encoding(\n            dataset,\n            store,\n            append_dim,\n            group=group,\n            consolidated=consolidated,\n            encoding=encoding,\n        )\n\n    zstore = backends.ZarrStore.open_group(\n        store=store,\n        mode=mode,\n        synchronizer=synchronizer,\n        group=group,\n        consolidate_on_close=consolidated,\n    )\n    zstore.append_dim = append_dim\n    writer = ArrayWriter()\n    # TODO: figure out how to properly handle unlimited_dims\n    dump_to_store(dataset, zstore, writer, encoding=encoding)\n    writes = writer.sync(compute=compute)\n\n    if compute:\n        _finalize_store(writes, zstore)\n    else:\n        import dask\n\n        return dask.delayed(_finalize_store)(writes, zstore)\n\n    return zstore\n'"
xarray/backends/cfgrib_.py,1,"b'import numpy as np\n\nfrom ..core import indexing\nfrom ..core.utils import Frozen, FrozenDict\nfrom ..core.variable import Variable\nfrom .common import AbstractDataStore, BackendArray\nfrom .locks import SerializableLock, ensure_lock\n\n# FIXME: Add a dedicated lock, even if ecCodes is supposed to be thread-safe\n#   in most circumstances. See:\n#       https://confluence.ecmwf.int/display/ECC/Frequently+Asked+Questions\nECCODES_LOCK = SerializableLock()\n\n\nclass CfGribArrayWrapper(BackendArray):\n    def __init__(self, datastore, array):\n        self.datastore = datastore\n        self.shape = array.shape\n        self.dtype = array.dtype\n        self.array = array\n\n    def __getitem__(self, key):\n        return indexing.explicit_indexing_adapter(\n            key, self.shape, indexing.IndexingSupport.OUTER, self._getitem\n        )\n\n    def _getitem(self, key):\n        with self.datastore.lock:\n            return self.array[key]\n\n\nclass CfGribDataStore(AbstractDataStore):\n    """"""\n    Implements the ``xr.AbstractDataStore`` read-only API for a GRIB file.\n    """"""\n\n    def __init__(self, filename, lock=None, **backend_kwargs):\n        import cfgrib\n\n        if lock is None:\n            lock = ECCODES_LOCK\n        self.lock = ensure_lock(lock)\n        self.ds = cfgrib.open_file(filename, **backend_kwargs)\n\n    def open_store_variable(self, name, var):\n        if isinstance(var.data, np.ndarray):\n            data = var.data\n        else:\n            wrapped_array = CfGribArrayWrapper(self, var.data)\n            data = indexing.LazilyOuterIndexedArray(wrapped_array)\n\n        encoding = self.ds.encoding.copy()\n        encoding[""original_shape""] = var.data.shape\n\n        return Variable(var.dimensions, data, var.attributes, encoding)\n\n    def get_variables(self):\n        return FrozenDict(\n            (k, self.open_store_variable(k, v)) for k, v in self.ds.variables.items()\n        )\n\n    def get_attrs(self):\n        return Frozen(self.ds.attributes)\n\n    def get_dimensions(self):\n        return Frozen(self.ds.dimensions)\n\n    def get_encoding(self):\n        dims = self.get_dimensions()\n        encoding = {""unlimited_dims"": {k for k, v in dims.items() if v is None}}\n        return encoding\n'"
xarray/backends/common.py,2,"b'import logging\nimport time\nimport traceback\nimport warnings\nfrom collections.abc import Mapping\n\nimport numpy as np\n\nfrom ..conventions import cf_encoder\nfrom ..core import indexing\nfrom ..core.pycompat import dask_array_type\nfrom ..core.utils import FrozenDict, NdimSizeLenMixin\n\n# Create a logger object, but don\'t add any handlers. Leave that to user code.\nlogger = logging.getLogger(__name__)\n\n\nNONE_VAR_NAME = ""__values__""\n\n\ndef _encode_variable_name(name):\n    if name is None:\n        name = NONE_VAR_NAME\n    return name\n\n\ndef _decode_variable_name(name):\n    if name == NONE_VAR_NAME:\n        name = None\n    return name\n\n\ndef find_root_and_group(ds):\n    """"""Find the root and group name of a netCDF4/h5netcdf dataset.""""""\n    hierarchy = ()\n    while ds.parent is not None:\n        hierarchy = (ds.name.split(""/"")[-1],) + hierarchy\n        ds = ds.parent\n    group = ""/"" + ""/"".join(hierarchy)\n    return ds, group\n\n\ndef robust_getitem(array, key, catch=Exception, max_retries=6, initial_delay=500):\n    """"""\n    Robustly index an array, using retry logic with exponential backoff if any\n    of the errors ``catch`` are raised. The initial_delay is measured in ms.\n\n    With the default settings, the maximum delay will be in the range of 32-64\n    seconds.\n    """"""\n    assert max_retries >= 0\n    for n in range(max_retries + 1):\n        try:\n            return array[key]\n        except catch:\n            if n == max_retries:\n                raise\n            base_delay = initial_delay * 2 ** n\n            next_delay = base_delay + np.random.randint(base_delay)\n            msg = (\n                ""getitem failed, waiting %s ms before trying again ""\n                ""(%s tries remaining). Full traceback: %s""\n                % (next_delay, max_retries - n, traceback.format_exc())\n            )\n            logger.debug(msg)\n            time.sleep(1e-3 * next_delay)\n\n\nclass BackendArray(NdimSizeLenMixin, indexing.ExplicitlyIndexed):\n    __slots__ = ()\n\n    def __array__(self, dtype=None):\n        key = indexing.BasicIndexer((slice(None),) * self.ndim)\n        return np.asarray(self[key], dtype=dtype)\n\n\nclass AbstractDataStore(Mapping):\n    __slots__ = ()\n\n    def __iter__(self):\n        return iter(self.variables)\n\n    def __getitem__(self, key):\n        return self.variables[key]\n\n    def __len__(self):\n        return len(self.variables)\n\n    def get_dimensions(self):  # pragma: no cover\n        raise NotImplementedError()\n\n    def get_attrs(self):  # pragma: no cover\n        raise NotImplementedError()\n\n    def get_variables(self):  # pragma: no cover\n        raise NotImplementedError()\n\n    def get_encoding(self):\n        return {}\n\n    def load(self):\n        """"""\n        This loads the variables and attributes simultaneously.\n        A centralized loading function makes it easier to create\n        data stores that do automatic encoding/decoding.\n\n        For example::\n\n            class SuffixAppendingDataStore(AbstractDataStore):\n\n                def load(self):\n                    variables, attributes = AbstractDataStore.load(self)\n                    variables = {\'%s_suffix\' % k: v\n                                 for k, v in variables.items()}\n                    attributes = {\'%s_suffix\' % k: v\n                                  for k, v in attributes.items()}\n                    return variables, attributes\n\n        This function will be called anytime variables or attributes\n        are requested, so care should be taken to make sure its fast.\n        """"""\n        variables = FrozenDict(\n            (_decode_variable_name(k), v) for k, v in self.get_variables().items()\n        )\n        attributes = FrozenDict(self.get_attrs())\n        return variables, attributes\n\n    @property\n    def variables(self):  # pragma: no cover\n        warnings.warn(\n            ""The ``variables`` property has been deprecated and ""\n            ""will be removed in xarray v0.11."",\n            FutureWarning,\n            stacklevel=2,\n        )\n        variables, _ = self.load()\n        return variables\n\n    @property\n    def attrs(self):  # pragma: no cover\n        warnings.warn(\n            ""The ``attrs`` property has been deprecated and ""\n            ""will be removed in xarray v0.11."",\n            FutureWarning,\n            stacklevel=2,\n        )\n        _, attrs = self.load()\n        return attrs\n\n    @property\n    def dimensions(self):  # pragma: no cover\n        warnings.warn(\n            ""The ``dimensions`` property has been deprecated and ""\n            ""will be removed in xarray v0.11."",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return self.get_dimensions()\n\n    def close(self):\n        pass\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exception_type, exception_value, traceback):\n        self.close()\n\n\nclass ArrayWriter:\n    __slots__ = (""sources"", ""targets"", ""regions"", ""lock"")\n\n    def __init__(self, lock=None):\n        self.sources = []\n        self.targets = []\n        self.regions = []\n        self.lock = lock\n\n    def add(self, source, target, region=None):\n        if isinstance(source, dask_array_type):\n            self.sources.append(source)\n            self.targets.append(target)\n            self.regions.append(region)\n        else:\n            if region:\n                target[region] = source\n            else:\n                target[...] = source\n\n    def sync(self, compute=True):\n        if self.sources:\n            import dask.array as da\n\n            # TODO: consider wrapping targets with dask.delayed, if this makes\n            # for any discernable difference in perforance, e.g.,\n            # targets = [dask.delayed(t) for t in self.targets]\n\n            delayed_store = da.store(\n                self.sources,\n                self.targets,\n                lock=self.lock,\n                compute=compute,\n                flush=True,\n                regions=self.regions,\n            )\n            self.sources = []\n            self.targets = []\n            self.regions = []\n            return delayed_store\n\n\nclass AbstractWritableDataStore(AbstractDataStore):\n    __slots__ = ()\n\n    def encode(self, variables, attributes):\n        """"""\n        Encode the variables and attributes in this store\n\n        Parameters\n        ----------\n        variables : dict-like\n            Dictionary of key/value (variable name / xr.Variable) pairs\n        attributes : dict-like\n            Dictionary of key/value (attribute name / attribute) pairs\n\n        Returns\n        -------\n        variables : dict-like\n        attributes : dict-like\n\n        """"""\n        variables = {k: self.encode_variable(v) for k, v in variables.items()}\n        attributes = {k: self.encode_attribute(v) for k, v in attributes.items()}\n        return variables, attributes\n\n    def encode_variable(self, v):\n        """"""encode one variable""""""\n        return v\n\n    def encode_attribute(self, a):\n        """"""encode one attribute""""""\n        return a\n\n    def set_dimension(self, dim, length):  # pragma: no cover\n        raise NotImplementedError()\n\n    def set_attribute(self, k, v):  # pragma: no cover\n        raise NotImplementedError()\n\n    def set_variable(self, k, v):  # pragma: no cover\n        raise NotImplementedError()\n\n    def store_dataset(self, dataset):\n        """"""\n        in stores, variables are all variables AND coordinates\n        in xarray.Dataset variables are variables NOT coordinates,\n        so here we pass the whole dataset in instead of doing\n        dataset.variables\n        """"""\n        self.store(dataset, dataset.attrs)\n\n    def store(\n        self,\n        variables,\n        attributes,\n        check_encoding_set=frozenset(),\n        writer=None,\n        unlimited_dims=None,\n    ):\n        """"""\n        Top level method for putting data on this store, this method:\n          - encodes variables/attributes\n          - sets dimensions\n          - sets variables\n\n        Parameters\n        ----------\n        variables : dict-like\n            Dictionary of key/value (variable name / xr.Variable) pairs\n        attributes : dict-like\n            Dictionary of key/value (attribute name / attribute) pairs\n        check_encoding_set : list-like\n            List of variables that should be checked for invalid encoding\n            values\n        writer : ArrayWriter\n        unlimited_dims : list-like\n            List of dimension names that should be treated as unlimited\n            dimensions.\n        """"""\n        if writer is None:\n            writer = ArrayWriter()\n\n        variables, attributes = self.encode(variables, attributes)\n\n        self.set_attributes(attributes)\n        self.set_dimensions(variables, unlimited_dims=unlimited_dims)\n        self.set_variables(\n            variables, check_encoding_set, writer, unlimited_dims=unlimited_dims\n        )\n\n    def set_attributes(self, attributes):\n        """"""\n        This provides a centralized method to set the dataset attributes on the\n        data store.\n\n        Parameters\n        ----------\n        attributes : dict-like\n            Dictionary of key/value (attribute name / attribute) pairs\n        """"""\n        for k, v in attributes.items():\n            self.set_attribute(k, v)\n\n    def set_variables(self, variables, check_encoding_set, writer, unlimited_dims=None):\n        """"""\n        This provides a centralized method to set the variables on the data\n        store.\n\n        Parameters\n        ----------\n        variables : dict-like\n            Dictionary of key/value (variable name / xr.Variable) pairs\n        check_encoding_set : list-like\n            List of variables that should be checked for invalid encoding\n            values\n        writer : ArrayWriter\n        unlimited_dims : list-like\n            List of dimension names that should be treated as unlimited\n            dimensions.\n        """"""\n\n        for vn, v in variables.items():\n            name = _encode_variable_name(vn)\n            check = vn in check_encoding_set\n            target, source = self.prepare_variable(\n                name, v, check, unlimited_dims=unlimited_dims\n            )\n\n            writer.add(source, target)\n\n    def set_dimensions(self, variables, unlimited_dims=None):\n        """"""\n        This provides a centralized method to set the dimensions on the data\n        store.\n\n        Parameters\n        ----------\n        variables : dict-like\n            Dictionary of key/value (variable name / xr.Variable) pairs\n        unlimited_dims : list-like\n            List of dimension names that should be treated as unlimited\n            dimensions.\n        """"""\n        if unlimited_dims is None:\n            unlimited_dims = set()\n\n        existing_dims = self.get_dimensions()\n\n        dims = {}\n        for v in unlimited_dims:  # put unlimited_dims first\n            dims[v] = None\n        for v in variables.values():\n            dims.update(dict(zip(v.dims, v.shape)))\n\n        for dim, length in dims.items():\n            if dim in existing_dims and length != existing_dims[dim]:\n                raise ValueError(\n                    ""Unable to update size for existing dimension""\n                    ""%r (%d != %d)"" % (dim, length, existing_dims[dim])\n                )\n            elif dim not in existing_dims:\n                is_unlimited = dim in unlimited_dims\n                self.set_dimension(dim, length, is_unlimited)\n\n\nclass WritableCFDataStore(AbstractWritableDataStore):\n    __slots__ = ()\n\n    def encode(self, variables, attributes):\n        # All NetCDF files get CF encoded by default, without this attempting\n        # to write times, for example, would fail.\n        variables, attributes = cf_encoder(variables, attributes)\n        variables = {k: self.encode_variable(v) for k, v in variables.items()}\n        attributes = {k: self.encode_attribute(v) for k, v in attributes.items()}\n        return variables, attributes\n'"
xarray/backends/file_manager.py,0,"b'import contextlib\nimport io\nimport threading\nimport warnings\nfrom typing import Any, Dict, cast\n\nfrom ..core import utils\nfrom ..core.options import OPTIONS\nfrom .locks import acquire\nfrom .lru_cache import LRUCache\n\n# Global cache for storing open files.\nFILE_CACHE: LRUCache[str, io.IOBase] = LRUCache(\n    maxsize=cast(int, OPTIONS[""file_cache_maxsize""]), on_evict=lambda k, v: v.close()\n)\nassert FILE_CACHE.maxsize, ""file cache must be at least size one""\n\n\nREF_COUNTS: Dict[Any, int] = {}\n\n_DEFAULT_MODE = utils.ReprObject(""<unused>"")\n\n\nclass FileManager:\n    """"""Manager for acquiring and closing a file object.\n\n    Use FileManager subclasses (CachingFileManager in particular) on backend\n    storage classes to automatically handle issues related to keeping track of\n    many open files and transferring them between multiple processes.\n    """"""\n\n    def acquire(self, needs_lock=True):\n        """"""Acquire the file object from this manager.""""""\n        raise NotImplementedError()\n\n    def acquire_context(self, needs_lock=True):\n        """"""Context manager for acquiring a file. Yields a file object.\n\n        The context manager unwinds any actions taken as part of acquisition\n        (i.e., removes it from any cache) if an exception is raised from the\n        context. It *does not* automatically close the file.\n        """"""\n        raise NotImplementedError()\n\n    def close(self, needs_lock=True):\n        """"""Close the file object associated with this manager, if needed.""""""\n        raise NotImplementedError()\n\n\nclass CachingFileManager(FileManager):\n    """"""Wrapper for automatically opening and closing file objects.\n\n    Unlike files, CachingFileManager objects can be safely pickled and passed\n    between processes. They should be explicitly closed to release resources,\n    but a per-process least-recently-used cache for open files ensures that you\n    can safely create arbitrarily large numbers of FileManager objects.\n\n    Don\'t directly close files acquired from a FileManager. Instead, call\n    FileManager.close(), which ensures that closed files are removed from the\n    cache as well.\n\n    Example usage:\n\n        manager = FileManager(open, \'example.txt\', mode=\'w\')\n        f = manager.acquire()\n        f.write(...)\n        manager.close()  # ensures file is closed\n\n    Note that as long as previous files are still cached, acquiring a file\n    multiple times from the same FileManager is essentially free:\n\n        f1 = manager.acquire()\n        f2 = manager.acquire()\n        assert f1 is f2\n\n    """"""\n\n    def __init__(\n        self,\n        opener,\n        *args,\n        mode=_DEFAULT_MODE,\n        kwargs=None,\n        lock=None,\n        cache=None,\n        ref_counts=None,\n    ):\n        """"""Initialize a FileManager.\n\n        The cache and ref_counts arguments exist solely to facilitate\n        dependency injection, and should only be set for tests.\n\n        Parameters\n        ----------\n        opener : callable\n            Function that when called like ``opener(*args, **kwargs)`` returns\n            an open file object. The file object must implement a ``close()``\n            method.\n        *args\n            Positional arguments for opener. A ``mode`` argument should be\n            provided as a keyword argument (see below). All arguments must be\n            hashable.\n        mode : optional\n            If provided, passed as a keyword argument to ``opener`` along with\n            ``**kwargs``. ``mode=\'w\' `` has special treatment: after the first\n            call it is replaced by ``mode=\'a\'`` in all subsequent function to\n            avoid overriding the newly created file.\n        kwargs : dict, optional\n            Keyword arguments for opener, excluding ``mode``. All values must\n            be hashable.\n        lock : duck-compatible threading.Lock, optional\n            Lock to use when modifying the cache inside acquire() and close().\n            By default, uses a new threading.Lock() object. If set, this object\n            should be pickleable.\n        cache : MutableMapping, optional\n            Mapping to use as a cache for open files. By default, uses xarray\'s\n            global LRU file cache. Because ``cache`` typically points to a\n            global variable and contains non-picklable file objects, an\n            unpickled FileManager objects will be restored with the default\n            cache.\n        ref_counts : dict, optional\n            Optional dict to use for keeping track the number of references to\n            the same file.\n        """"""\n        self._opener = opener\n        self._args = args\n        self._mode = mode\n        self._kwargs = {} if kwargs is None else dict(kwargs)\n\n        self._default_lock = lock is None or lock is False\n        self._lock = threading.Lock() if self._default_lock else lock\n\n        # cache[self._key] stores the file associated with this object.\n        if cache is None:\n            cache = FILE_CACHE\n        self._cache = cache\n        self._key = self._make_key()\n\n        # ref_counts[self._key] stores the number of CachingFileManager objects\n        # in memory referencing this same file. We use this to know if we can\n        # close a file when the manager is deallocated.\n        if ref_counts is None:\n            ref_counts = REF_COUNTS\n        self._ref_counter = _RefCounter(ref_counts)\n        self._ref_counter.increment(self._key)\n\n    def _make_key(self):\n        """"""Make a key for caching files in the LRU cache.""""""\n        value = (\n            self._opener,\n            self._args,\n            ""a"" if self._mode == ""w"" else self._mode,\n            tuple(sorted(self._kwargs.items())),\n        )\n        return _HashedSequence(value)\n\n    @contextlib.contextmanager\n    def _optional_lock(self, needs_lock):\n        """"""Context manager for optionally acquiring a lock.""""""\n        if needs_lock:\n            with self._lock:\n                yield\n        else:\n            yield\n\n    def acquire(self, needs_lock=True):\n        """"""Acquire a file object from the manager.\n\n        A new file is only opened if it has expired from the\n        least-recently-used cache.\n\n        This method uses a lock, which ensures that it is thread-safe. You can\n        safely acquire a file in multiple threads at the same time, as long as\n        the underlying file object is thread-safe.\n\n        Returns\n        -------\n        An open file object, as returned by ``opener(*args, **kwargs)``.\n        """"""\n        file, _ = self._acquire_with_cache_info(needs_lock)\n        return file\n\n    @contextlib.contextmanager\n    def acquire_context(self, needs_lock=True):\n        """"""Context manager for acquiring a file.""""""\n        file, cached = self._acquire_with_cache_info(needs_lock)\n        try:\n            yield file\n        except Exception:\n            if not cached:\n                self.close(needs_lock)\n            raise\n\n    def _acquire_with_cache_info(self, needs_lock=True):\n        """"""Acquire a file, returning the file and whether it was cached.""""""\n        with self._optional_lock(needs_lock):\n            try:\n                file = self._cache[self._key]\n            except KeyError:\n                kwargs = self._kwargs\n                if self._mode is not _DEFAULT_MODE:\n                    kwargs = kwargs.copy()\n                    kwargs[""mode""] = self._mode\n                file = self._opener(*self._args, **kwargs)\n                if self._mode == ""w"":\n                    # ensure file doesn\'t get overriden when opened again\n                    self._mode = ""a""\n                self._cache[self._key] = file\n                return file, False\n            else:\n                return file, True\n\n    def close(self, needs_lock=True):\n        """"""Explicitly close any associated file object (if necessary).""""""\n        # TODO: remove needs_lock if/when we have a reentrant lock in\n        # dask.distributed: https://github.com/dask/dask/issues/3832\n        with self._optional_lock(needs_lock):\n            default = None\n            file = self._cache.pop(self._key, default)\n            if file is not None:\n                file.close()\n\n    def __del__(self):\n        # If we\'re the only CachingFileManger referencing a unclosed file, we\n        # should remove it from the cache upon garbage collection.\n        #\n        # Keeping our own count of file references might seem like overkill,\n        # but it\'s actually pretty common to reopen files with the same\n        # variable name in a notebook or command line environment, e.g., to\n        # fix the parameters used when opening a file:\n        #    >>> ds = xarray.open_dataset(\'myfile.nc\')\n        #    >>> ds = xarray.open_dataset(\'myfile.nc\', decode_times=False)\n        # This second assignment to ""ds"" drops CPython\'s ref-count on the first\n        # ""ds"" argument to zero, which can trigger garbage collections. So if\n        # we didn\'t check whether another object is referencing \'myfile.nc\',\n        # the newly opened file would actually be immediately closed!\n        ref_count = self._ref_counter.decrement(self._key)\n\n        if not ref_count and self._key in self._cache:\n            if acquire(self._lock, blocking=False):\n                # Only close files if we can do so immediately.\n                try:\n                    self.close(needs_lock=False)\n                finally:\n                    self._lock.release()\n\n            if OPTIONS[""warn_for_unclosed_files""]:\n                warnings.warn(\n                    ""deallocating {}, but file is not already closed. ""\n                    ""This may indicate a bug."".format(self),\n                    RuntimeWarning,\n                    stacklevel=2,\n                )\n\n    def __getstate__(self):\n        """"""State for pickling.""""""\n        # cache and ref_counts are intentionally omitted: we don\'t want to try\n        # to serialize these global objects.\n        lock = None if self._default_lock else self._lock\n        return (self._opener, self._args, self._mode, self._kwargs, lock)\n\n    def __setstate__(self, state):\n        """"""Restore from a pickle.""""""\n        opener, args, mode, kwargs, lock = state\n        self.__init__(opener, *args, mode=mode, kwargs=kwargs, lock=lock)\n\n    def __repr__(self):\n        args_string = "", "".join(map(repr, self._args))\n        if self._mode is not _DEFAULT_MODE:\n            args_string += f"", mode={self._mode!r}""\n        return ""{}({!r}, {}, kwargs={})"".format(\n            type(self).__name__, self._opener, args_string, self._kwargs\n        )\n\n\nclass _RefCounter:\n    """"""Class for keeping track of reference counts.""""""\n\n    def __init__(self, counts):\n        self._counts = counts\n        self._lock = threading.Lock()\n\n    def increment(self, name):\n        with self._lock:\n            count = self._counts[name] = self._counts.get(name, 0) + 1\n        return count\n\n    def decrement(self, name):\n        with self._lock:\n            count = self._counts[name] - 1\n            if count:\n                self._counts[name] = count\n            else:\n                del self._counts[name]\n        return count\n\n\nclass _HashedSequence(list):\n    """"""Speedup repeated look-ups by caching hash values.\n\n    Based on what Python uses internally in functools.lru_cache.\n\n    Python doesn\'t perform this optimization automatically:\n    https://bugs.python.org/issue1462796\n    """"""\n\n    def __init__(self, tuple_value):\n        self[:] = tuple_value\n        self.hashvalue = hash(tuple_value)\n\n    def __hash__(self):\n        return self.hashvalue\n\n\nclass DummyFileManager(FileManager):\n    """"""FileManager that simply wraps an open file in the FileManager interface.\n    """"""\n\n    def __init__(self, value):\n        self._value = value\n\n    def acquire(self, needs_lock=True):\n        del needs_lock  # ignored\n        return self._value\n\n    @contextlib.contextmanager\n    def acquire_context(self, needs_lock=True):\n        del needs_lock\n        yield self._value\n\n    def close(self, needs_lock=True):\n        del needs_lock  # ignored\n        self._value.close()\n'"
xarray/backends/h5netcdf_.py,1,"b'import functools\nfrom distutils.version import LooseVersion\n\nimport numpy as np\n\nfrom ..core import indexing\nfrom ..core.utils import FrozenDict, is_remote_uri\nfrom ..core.variable import Variable\nfrom .common import WritableCFDataStore, find_root_and_group\nfrom .file_manager import CachingFileManager, DummyFileManager\nfrom .locks import HDF5_LOCK, combine_locks, ensure_lock, get_write_lock\nfrom .netCDF4_ import (\n    BaseNetCDF4Array,\n    _encode_nc4_variable,\n    _extract_nc4_variable_encoding,\n    _get_datatype,\n    _nc4_require_group,\n)\n\n\nclass H5NetCDFArrayWrapper(BaseNetCDF4Array):\n    def get_array(self, needs_lock=True):\n        ds = self.datastore._acquire(needs_lock)\n        variable = ds.variables[self.variable_name]\n        return variable\n\n    def __getitem__(self, key):\n        return indexing.explicit_indexing_adapter(\n            key, self.shape, indexing.IndexingSupport.OUTER_1VECTOR, self._getitem\n        )\n\n    def _getitem(self, key):\n        # h5py requires using lists for fancy indexing:\n        # https://github.com/h5py/h5py/issues/992\n        key = tuple(list(k) if isinstance(k, np.ndarray) else k for k in key)\n        with self.datastore.lock:\n            array = self.get_array(needs_lock=False)\n            return array[key]\n\n\ndef maybe_decode_bytes(txt):\n    if isinstance(txt, bytes):\n        return txt.decode(""utf-8"")\n    else:\n        return txt\n\n\ndef _read_attributes(h5netcdf_var):\n    # GH451\n    # to ensure conventions decoding works properly on Python 3, decode all\n    # bytes attributes to strings\n    attrs = {}\n    for k, v in h5netcdf_var.attrs.items():\n        if k not in [""_FillValue"", ""missing_value""]:\n            v = maybe_decode_bytes(v)\n        attrs[k] = v\n    return attrs\n\n\n_extract_h5nc_encoding = functools.partial(\n    _extract_nc4_variable_encoding, lsd_okay=False, h5py_okay=True, backend=""h5netcdf""\n)\n\n\ndef _h5netcdf_create_group(dataset, name):\n    return dataset.create_group(name)\n\n\nclass H5NetCDFStore(WritableCFDataStore):\n    """"""Store for reading and writing data via h5netcdf\n    """"""\n\n    __slots__ = (\n        ""autoclose"",\n        ""format"",\n        ""is_remote"",\n        ""lock"",\n        ""_filename"",\n        ""_group"",\n        ""_manager"",\n        ""_mode"",\n    )\n\n    def __init__(self, manager, group=None, mode=None, lock=HDF5_LOCK, autoclose=False):\n\n        import h5netcdf\n\n        if isinstance(manager, (h5netcdf.File, h5netcdf.Group)):\n            if group is None:\n                root, group = find_root_and_group(manager)\n            else:\n                if not type(manager) is h5netcdf.File:\n                    raise ValueError(\n                        ""must supply a h5netcdf.File if the group ""\n                        ""argument is provided""\n                    )\n                root = manager\n            manager = DummyFileManager(root)\n\n        self._manager = manager\n        self._group = group\n        self._mode = mode\n        self.format = None\n        # todo: utilizing find_root_and_group seems a bit clunky\n        #  making filename available on h5netcdf.Group seems better\n        self._filename = find_root_and_group(self.ds)[0].filename\n        self.is_remote = is_remote_uri(self._filename)\n        self.lock = ensure_lock(lock)\n        self.autoclose = autoclose\n\n    @classmethod\n    def open(\n        cls,\n        filename,\n        mode=""r"",\n        format=None,\n        group=None,\n        lock=None,\n        autoclose=False,\n        invalid_netcdf=None,\n        phony_dims=None,\n    ):\n        import h5netcdf\n\n        if format not in [None, ""NETCDF4""]:\n            raise ValueError(""invalid format for h5netcdf backend"")\n\n        kwargs = {""invalid_netcdf"": invalid_netcdf}\n        if phony_dims is not None:\n            if LooseVersion(h5netcdf.__version__) >= LooseVersion(""0.8.0""):\n                kwargs[""phony_dims""] = phony_dims\n            else:\n                raise ValueError(\n                    ""h5netcdf backend keyword argument \'phony_dims\' needs ""\n                    ""h5netcdf >= 0.8.0.""\n                )\n\n        if lock is None:\n            if mode == ""r"":\n                lock = HDF5_LOCK\n            else:\n                lock = combine_locks([HDF5_LOCK, get_write_lock(filename)])\n\n        manager = CachingFileManager(h5netcdf.File, filename, mode=mode, kwargs=kwargs)\n        return cls(manager, group=group, mode=mode, lock=lock, autoclose=autoclose)\n\n    def _acquire(self, needs_lock=True):\n        with self._manager.acquire_context(needs_lock) as root:\n            ds = _nc4_require_group(\n                root, self._group, self._mode, create_group=_h5netcdf_create_group\n            )\n        return ds\n\n    @property\n    def ds(self):\n        return self._acquire()\n\n    def open_store_variable(self, name, var):\n        import h5py\n\n        dimensions = var.dimensions\n        data = indexing.LazilyOuterIndexedArray(H5NetCDFArrayWrapper(name, self))\n        attrs = _read_attributes(var)\n\n        # netCDF4 specific encoding\n        encoding = {\n            ""chunksizes"": var.chunks,\n            ""fletcher32"": var.fletcher32,\n            ""shuffle"": var.shuffle,\n        }\n        # Convert h5py-style compression options to NetCDF4-Python\n        # style, if possible\n        if var.compression == ""gzip"":\n            encoding[""zlib""] = True\n            encoding[""complevel""] = var.compression_opts\n        elif var.compression is not None:\n            encoding[""compression""] = var.compression\n            encoding[""compression_opts""] = var.compression_opts\n\n        # save source so __repr__ can detect if it\'s local or not\n        encoding[""source""] = self._filename\n        encoding[""original_shape""] = var.shape\n\n        vlen_dtype = h5py.check_dtype(vlen=var.dtype)\n        if vlen_dtype is str:\n            encoding[""dtype""] = str\n        elif vlen_dtype is not None:  # pragma: no cover\n            # xarray doesn\'t support writing arbitrary vlen dtypes yet.\n            pass\n        else:\n            encoding[""dtype""] = var.dtype\n\n        return Variable(dimensions, data, attrs, encoding)\n\n    def get_variables(self):\n        return FrozenDict(\n            (k, self.open_store_variable(k, v)) for k, v in self.ds.variables.items()\n        )\n\n    def get_attrs(self):\n        return FrozenDict(_read_attributes(self.ds))\n\n    def get_dimensions(self):\n        return self.ds.dimensions\n\n    def get_encoding(self):\n        encoding = {}\n        encoding[""unlimited_dims""] = {\n            k for k, v in self.ds.dimensions.items() if v is None\n        }\n        return encoding\n\n    def set_dimension(self, name, length, is_unlimited=False):\n        if is_unlimited:\n            self.ds.dimensions[name] = None\n            self.ds.resize_dimension(name, length)\n        else:\n            self.ds.dimensions[name] = length\n\n    def set_attribute(self, key, value):\n        self.ds.attrs[key] = value\n\n    def encode_variable(self, variable):\n        return _encode_nc4_variable(variable)\n\n    def prepare_variable(\n        self, name, variable, check_encoding=False, unlimited_dims=None\n    ):\n        import h5py\n\n        attrs = variable.attrs.copy()\n        dtype = _get_datatype(variable, raise_on_invalid_encoding=check_encoding)\n\n        fillvalue = attrs.pop(""_FillValue"", None)\n        if dtype is str and fillvalue is not None:\n            raise NotImplementedError(\n                ""h5netcdf does not yet support setting a fill value for ""\n                ""variable-length strings ""\n                ""(https://github.com/shoyer/h5netcdf/issues/37). ""\n                ""Either remove \'_FillValue\' from encoding on variable %r ""\n                ""or set {\'dtype\': \'S1\'} in encoding to use the fixed width ""\n                ""NC_CHAR type."" % name\n            )\n\n        if dtype is str:\n            dtype = h5py.special_dtype(vlen=str)\n\n        encoding = _extract_h5nc_encoding(variable, raise_on_invalid=check_encoding)\n        kwargs = {}\n\n        # Convert from NetCDF4-Python style compression settings to h5py style\n        # If both styles are used together, h5py takes precedence\n        # If set_encoding=True, raise ValueError in case of mismatch\n        if encoding.pop(""zlib"", False):\n            if check_encoding and encoding.get(""compression"") not in (None, ""gzip""):\n                raise ValueError(""\'zlib\' and \'compression\' encodings mismatch"")\n            encoding.setdefault(""compression"", ""gzip"")\n\n        if (\n            check_encoding\n            and ""complevel"" in encoding\n            and ""compression_opts"" in encoding\n            and encoding[""complevel""] != encoding[""compression_opts""]\n        ):\n            raise ValueError(""\'complevel\' and \'compression_opts\' encodings "" ""mismatch"")\n        complevel = encoding.pop(""complevel"", 0)\n        if complevel != 0:\n            encoding.setdefault(""compression_opts"", complevel)\n\n        encoding[""chunks""] = encoding.pop(""chunksizes"", None)\n\n        # Do not apply compression, filters or chunking to scalars.\n        if variable.shape:\n            for key in [\n                ""compression"",\n                ""compression_opts"",\n                ""shuffle"",\n                ""chunks"",\n                ""fletcher32"",\n            ]:\n                if key in encoding:\n                    kwargs[key] = encoding[key]\n        if name not in self.ds:\n            nc4_var = self.ds.create_variable(\n                name,\n                dtype=dtype,\n                dimensions=variable.dims,\n                fillvalue=fillvalue,\n                **kwargs,\n            )\n        else:\n            nc4_var = self.ds[name]\n\n        for k, v in attrs.items():\n            nc4_var.attrs[k] = v\n\n        target = H5NetCDFArrayWrapper(name, self)\n\n        return target, variable.data\n\n    def sync(self):\n        self.ds.sync()\n\n    def close(self, **kwargs):\n        self._manager.close(**kwargs)\n'"
xarray/backends/locks.py,0,"b'import multiprocessing\nimport threading\nimport weakref\nfrom typing import Any, MutableMapping, Optional\n\ntry:\n    from dask.utils import SerializableLock\nexcept ImportError:\n    # no need to worry about serializing the lock\n    SerializableLock = threading.Lock\n\ntry:\n    from dask.distributed import Lock as DistributedLock\nexcept ImportError:\n    DistributedLock = None\n\n\n# Locks used by multiple backends.\n# Neither HDF5 nor the netCDF-C library are thread-safe.\nHDF5_LOCK = SerializableLock()\nNETCDFC_LOCK = SerializableLock()\n\n\n_FILE_LOCKS: MutableMapping[Any, threading.Lock] = weakref.WeakValueDictionary()\n\n\ndef _get_threaded_lock(key):\n    try:\n        lock = _FILE_LOCKS[key]\n    except KeyError:\n        lock = _FILE_LOCKS[key] = threading.Lock()\n    return lock\n\n\ndef _get_multiprocessing_lock(key):\n    # TODO: make use of the key -- maybe use locket.py?\n    # https://github.com/mwilliamson/locket.py\n    del key  # unused\n    return multiprocessing.Lock()\n\n\n_LOCK_MAKERS = {\n    None: _get_threaded_lock,\n    ""threaded"": _get_threaded_lock,\n    ""multiprocessing"": _get_multiprocessing_lock,\n    ""distributed"": DistributedLock,\n}\n\n\ndef _get_lock_maker(scheduler=None):\n    """"""Returns an appropriate function for creating resource locks.\n\n    Parameters\n    ----------\n    scheduler : str or None\n        Dask scheduler being used.\n\n    See Also\n    --------\n    dask.utils.get_scheduler_lock\n    """"""\n    return _LOCK_MAKERS[scheduler]\n\n\ndef _get_scheduler(get=None, collection=None) -> Optional[str]:\n    """"""Determine the dask scheduler that is being used.\n\n    None is returned if no dask scheduler is active.\n\n    See also\n    --------\n    dask.base.get_scheduler\n    """"""\n    try:\n        import dask  # noqa: F401\n    except ImportError:\n        return None\n\n    actual_get = dask.base.get_scheduler(get, collection)\n\n    try:\n        from dask.distributed import Client\n\n        if isinstance(actual_get.__self__, Client):\n            return ""distributed""\n    except (ImportError, AttributeError):\n        pass\n\n    try:\n        # As of dask=2.6, dask.multiprocessing requires cloudpickle to be installed\n        # Dependency removed in https://github.com/dask/dask/pull/5511\n        if actual_get is dask.multiprocessing.get:\n            return ""multiprocessing""\n    except AttributeError:\n        pass\n\n    return ""threaded""\n\n\ndef get_write_lock(key):\n    """"""Get a scheduler appropriate lock for writing to the given resource.\n\n    Parameters\n    ----------\n    key : str\n        Name of the resource for which to acquire a lock. Typically a filename.\n\n    Returns\n    -------\n    Lock object that can be used like a threading.Lock object.\n    """"""\n    scheduler = _get_scheduler()\n    lock_maker = _get_lock_maker(scheduler)\n    return lock_maker(key)\n\n\ndef acquire(lock, blocking=True):\n    """"""Acquire a lock, possibly in a non-blocking fashion.\n\n    Includes backwards compatibility hacks for old versions of Python, dask\n    and dask-distributed.\n    """"""\n    if blocking:\n        # no arguments needed\n        return lock.acquire()\n    elif DistributedLock is not None and isinstance(lock, DistributedLock):\n        # distributed.Lock doesn\'t support the blocking argument yet:\n        # https://github.com/dask/distributed/pull/2412\n        return lock.acquire(timeout=0)\n    else:\n        # ""blocking"" keyword argument not supported for:\n        # - threading.Lock on Python 2.\n        # - dask.SerializableLock with dask v1.0.0 or earlier.\n        # - multiprocessing.Lock calls the argument ""block"" instead.\n        return lock.acquire(blocking)\n\n\nclass CombinedLock:\n    """"""A combination of multiple locks.\n\n    Like a locked door, a CombinedLock is locked if any of its constituent\n    locks are locked.\n    """"""\n\n    def __init__(self, locks):\n        self.locks = tuple(set(locks))  # remove duplicates\n\n    def acquire(self, blocking=True):\n        return all(acquire(lock, blocking=blocking) for lock in self.locks)\n\n    def release(self):\n        for lock in self.locks:\n            lock.release()\n\n    def __enter__(self):\n        for lock in self.locks:\n            lock.__enter__()\n\n    def __exit__(self, *args):\n        for lock in self.locks:\n            lock.__exit__(*args)\n\n    def locked(self):\n        return any(lock.locked for lock in self.locks)\n\n    def __repr__(self):\n        return ""CombinedLock(%r)"" % list(self.locks)\n\n\nclass DummyLock:\n    """"""DummyLock provides the lock API without any actual locking.""""""\n\n    def acquire(self, blocking=True):\n        pass\n\n    def release(self):\n        pass\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, *args):\n        pass\n\n    def locked(self):\n        return False\n\n\ndef combine_locks(locks):\n    """"""Combine a sequence of locks into a single lock.""""""\n    all_locks = []\n    for lock in locks:\n        if isinstance(lock, CombinedLock):\n            all_locks.extend(lock.locks)\n        elif lock is not None:\n            all_locks.append(lock)\n\n    num_locks = len(all_locks)\n    if num_locks > 1:\n        return CombinedLock(all_locks)\n    elif num_locks == 1:\n        return all_locks[0]\n    else:\n        return DummyLock()\n\n\ndef ensure_lock(lock):\n    """"""Ensure that the given object is a lock.""""""\n    if lock is None or lock is False:\n        return DummyLock()\n    return lock\n'"
xarray/backends/lru_cache.py,0,"b'import threading\nfrom collections import OrderedDict\nfrom typing import Any, Callable, Iterator, MutableMapping, Optional, TypeVar\n\nK = TypeVar(""K"")\nV = TypeVar(""V"")\n\n\nclass LRUCache(MutableMapping[K, V]):\n    """"""Thread-safe LRUCache based on an OrderedDict.\n\n    All dict operations (__getitem__, __setitem__, __contains__) update the\n    priority of the relevant key and take O(1) time. The dict is iterated over\n    in order from the oldest to newest key, which means that a complete pass\n    over the dict should not affect the order of any entries.\n\n    When a new item is set and the maximum size of the cache is exceeded, the\n    oldest item is dropped and called with ``on_evict(key, value)``.\n\n    The ``maxsize`` property can be used to view or adjust the capacity of\n    the cache, e.g., ``cache.maxsize = new_size``.\n    """"""\n\n    _cache: ""OrderedDict[K, V]""\n    _maxsize: int\n    _lock: threading.RLock\n    _on_evict: Optional[Callable[[K, V], Any]]\n\n    __slots__ = (""_cache"", ""_lock"", ""_maxsize"", ""_on_evict"")\n\n    def __init__(self, maxsize: int, on_evict: Callable[[K, V], Any] = None):\n        """"""\n        Parameters\n        ----------\n        maxsize : int\n            Integer maximum number of items to hold in the cache.\n        on_evict: callable, optional\n            Function to call like ``on_evict(key, value)`` when items are\n            evicted.\n        """"""\n        if not isinstance(maxsize, int):\n            raise TypeError(""maxsize must be an integer"")\n        if maxsize < 0:\n            raise ValueError(""maxsize must be non-negative"")\n        self._maxsize = maxsize\n        self._cache = OrderedDict()\n        self._lock = threading.RLock()\n        self._on_evict = on_evict\n\n    def __getitem__(self, key: K) -> V:\n        # record recent use of the key by moving it to the front of the list\n        with self._lock:\n            value = self._cache[key]\n            self._cache.move_to_end(key)\n            return value\n\n    def _enforce_size_limit(self, capacity: int) -> None:\n        """"""Shrink the cache if necessary, evicting the oldest items.\n        """"""\n        while len(self._cache) > capacity:\n            key, value = self._cache.popitem(last=False)\n            if self._on_evict is not None:\n                self._on_evict(key, value)\n\n    def __setitem__(self, key: K, value: V) -> None:\n        with self._lock:\n            if key in self._cache:\n                # insert the new value at the end\n                del self._cache[key]\n                self._cache[key] = value\n            elif self._maxsize:\n                # make room if necessary\n                self._enforce_size_limit(self._maxsize - 1)\n                self._cache[key] = value\n            elif self._on_evict is not None:\n                # not saving, immediately evict\n                self._on_evict(key, value)\n\n    def __delitem__(self, key: K) -> None:\n        del self._cache[key]\n\n    def __iter__(self) -> Iterator[K]:\n        # create a list, so accessing the cache during iteration cannot change\n        # the iteration order\n        return iter(list(self._cache))\n\n    def __len__(self) -> int:\n        return len(self._cache)\n\n    @property\n    def maxsize(self) -> int:\n        """"""Maximum number of items can be held in the cache.""""""\n        return self._maxsize\n\n    @maxsize.setter\n    def maxsize(self, size: int) -> None:\n        """"""Resize the cache, evicting the oldest items if necessary.""""""\n        if size < 0:\n            raise ValueError(""maxsize must be non-negative"")\n        with self._lock:\n            self._enforce_size_limit(size)\n            self._maxsize = size\n'"
xarray/backends/memory.py,1,"b'import copy\n\nimport numpy as np\n\nfrom ..core.variable import Variable\nfrom .common import AbstractWritableDataStore\n\n\nclass InMemoryDataStore(AbstractWritableDataStore):\n    """"""\n    Stores dimensions, variables and attributes in ordered dictionaries, making\n    this store fast compared to stores which save to disk.\n\n    This store exists purely for internal testing purposes.\n    """"""\n\n    def __init__(self, variables=None, attributes=None):\n        self._variables = {} if variables is None else variables\n        self._attributes = {} if attributes is None else attributes\n\n    def get_attrs(self):\n        return self._attributes\n\n    def get_variables(self):\n        return self._variables\n\n    def get_dimensions(self):\n        dims = {}\n        for v in self._variables.values():\n            for d, s in v.dims.items():\n                dims[d] = s\n        return dims\n\n    def prepare_variable(self, k, v, *args, **kwargs):\n        new_var = Variable(v.dims, np.empty_like(v), v.attrs)\n        self._variables[k] = new_var\n        return new_var, v.data\n\n    def set_attribute(self, k, v):\n        # copy to imitate writing to disk.\n        self._attributes[k] = copy.deepcopy(v)\n\n    def set_dimension(self, dim, length, unlimited_dims=None):\n        # in this model, dimensions are accounted for in the variables\n        pass\n'"
xarray/backends/netCDF4_.py,3,"b'import functools\nimport operator\nfrom contextlib import suppress\n\nimport numpy as np\n\nfrom .. import coding\nfrom ..coding.variables import pop_to\nfrom ..core import indexing\nfrom ..core.utils import FrozenDict, is_remote_uri\nfrom ..core.variable import Variable\nfrom .common import (\n    BackendArray,\n    WritableCFDataStore,\n    find_root_and_group,\n    robust_getitem,\n)\nfrom .file_manager import CachingFileManager, DummyFileManager\nfrom .locks import HDF5_LOCK, NETCDFC_LOCK, combine_locks, ensure_lock, get_write_lock\nfrom .netcdf3 import encode_nc3_attr_value, encode_nc3_variable\n\n# This lookup table maps from dtype.byteorder to a readable endian\n# string used by netCDF4.\n_endian_lookup = {""="": ""native"", "">"": ""big"", ""<"": ""little"", ""|"": ""native""}\n\n\nNETCDF4_PYTHON_LOCK = combine_locks([NETCDFC_LOCK, HDF5_LOCK])\n\n\nclass BaseNetCDF4Array(BackendArray):\n    __slots__ = (""datastore"", ""dtype"", ""shape"", ""variable_name"")\n\n    def __init__(self, variable_name, datastore):\n        self.datastore = datastore\n        self.variable_name = variable_name\n\n        array = self.get_array()\n        self.shape = array.shape\n\n        dtype = array.dtype\n        if dtype is str:\n            # use object dtype because that\'s the only way in numpy to\n            # represent variable length strings; it also prevents automatic\n            # string concatenation via conventions.decode_cf_variable\n            dtype = np.dtype(""O"")\n        self.dtype = dtype\n\n    def __setitem__(self, key, value):\n        with self.datastore.lock:\n            data = self.get_array(needs_lock=False)\n            data[key] = value\n            if self.datastore.autoclose:\n                self.datastore.close(needs_lock=False)\n\n    def get_array(self, needs_lock=True):\n        raise NotImplementedError(""Virtual Method"")\n\n\nclass NetCDF4ArrayWrapper(BaseNetCDF4Array):\n    __slots__ = ()\n\n    def get_array(self, needs_lock=True):\n        ds = self.datastore._acquire(needs_lock)\n        variable = ds.variables[self.variable_name]\n        variable.set_auto_maskandscale(False)\n        # only added in netCDF4-python v1.2.8\n        with suppress(AttributeError):\n            variable.set_auto_chartostring(False)\n        return variable\n\n    def __getitem__(self, key):\n        return indexing.explicit_indexing_adapter(\n            key, self.shape, indexing.IndexingSupport.OUTER, self._getitem\n        )\n\n    def _getitem(self, key):\n        if self.datastore.is_remote:  # pragma: no cover\n            getitem = functools.partial(robust_getitem, catch=RuntimeError)\n        else:\n            getitem = operator.getitem\n\n        try:\n            with self.datastore.lock:\n                original_array = self.get_array(needs_lock=False)\n                array = getitem(original_array, key)\n        except IndexError:\n            # Catch IndexError in netCDF4 and return a more informative\n            # error message.  This is most often called when an unsorted\n            # indexer is used before the data is loaded from disk.\n            msg = (\n                ""The indexing operation you are attempting to perform ""\n                ""is not valid on netCDF4.Variable object. Try loading ""\n                ""your data into memory first by calling .load().""\n            )\n            raise IndexError(msg)\n        return array\n\n\ndef _encode_nc4_variable(var):\n    for coder in [\n        coding.strings.EncodedStringCoder(allows_unicode=True),\n        coding.strings.CharacterArrayCoder(),\n    ]:\n        var = coder.encode(var)\n    return var\n\n\ndef _check_encoding_dtype_is_vlen_string(dtype):\n    if dtype is not str:\n        raise AssertionError(  # pragma: no cover\n            ""unexpected dtype encoding %r. This shouldn\'t happen: please ""\n            ""file a bug report at github.com/pydata/xarray"" % dtype\n        )\n\n\ndef _get_datatype(var, nc_format=""NETCDF4"", raise_on_invalid_encoding=False):\n    if nc_format == ""NETCDF4"":\n        datatype = _nc4_dtype(var)\n    else:\n        if ""dtype"" in var.encoding:\n            encoded_dtype = var.encoding[""dtype""]\n            _check_encoding_dtype_is_vlen_string(encoded_dtype)\n            if raise_on_invalid_encoding:\n                raise ValueError(\n                    ""encoding dtype=str for vlen strings is only supported ""\n                    ""with format=\'NETCDF4\'.""\n                )\n        datatype = var.dtype\n    return datatype\n\n\ndef _nc4_dtype(var):\n    if ""dtype"" in var.encoding:\n        dtype = var.encoding.pop(""dtype"")\n        _check_encoding_dtype_is_vlen_string(dtype)\n    elif coding.strings.is_unicode_dtype(var.dtype):\n        dtype = str\n    elif var.dtype.kind in [""i"", ""u"", ""f"", ""c"", ""S""]:\n        dtype = var.dtype\n    else:\n        raise ValueError(f""unsupported dtype for netCDF4 variable: {var.dtype}"")\n    return dtype\n\n\ndef _netcdf4_create_group(dataset, name):\n    return dataset.createGroup(name)\n\n\ndef _nc4_require_group(ds, group, mode, create_group=_netcdf4_create_group):\n    if group in {None, """", ""/""}:\n        # use the root group\n        return ds\n    else:\n        # make sure it\'s a string\n        if not isinstance(group, str):\n            raise ValueError(""group must be a string or None"")\n        # support path-like syntax\n        path = group.strip(""/"").split(""/"")\n        for key in path:\n            try:\n                ds = ds.groups[key]\n            except KeyError as e:\n                if mode != ""r"":\n                    ds = create_group(ds, key)\n                else:\n                    # wrap error to provide slightly more helpful message\n                    raise OSError(""group not found: %s"" % key, e)\n        return ds\n\n\ndef _ensure_fill_value_valid(data, attributes):\n    # work around for netCDF4/scipy issue where _FillValue has the wrong type:\n    # https://github.com/Unidata/netcdf4-python/issues/271\n    if data.dtype.kind == ""S"" and ""_FillValue"" in attributes:\n        attributes[""_FillValue""] = np.string_(attributes[""_FillValue""])\n\n\ndef _force_native_endianness(var):\n    # possible values for byteorder are:\n    #     =    native\n    #     <    little-endian\n    #     >    big-endian\n    #     |    not applicable\n    # Below we check if the data type is not native or NA\n    if var.dtype.byteorder not in [""="", ""|""]:\n        # if endianness is specified explicitly, convert to the native type\n        data = var.data.astype(var.dtype.newbyteorder(""=""))\n        var = Variable(var.dims, data, var.attrs, var.encoding)\n        # if endian exists, remove it from the encoding.\n        var.encoding.pop(""endian"", None)\n    # check to see if encoding has a value for endian its \'native\'\n    if not var.encoding.get(""endian"", ""native"") == ""native"":\n        raise NotImplementedError(\n            ""Attempt to write non-native endian type, ""\n            ""this is not supported by the netCDF4 ""\n            ""python library.""\n        )\n    return var\n\n\ndef _extract_nc4_variable_encoding(\n    variable,\n    raise_on_invalid=False,\n    lsd_okay=True,\n    h5py_okay=False,\n    backend=""netCDF4"",\n    unlimited_dims=None,\n):\n    if unlimited_dims is None:\n        unlimited_dims = ()\n\n    encoding = variable.encoding.copy()\n\n    safe_to_drop = {""source"", ""original_shape""}\n    valid_encodings = {\n        ""zlib"",\n        ""complevel"",\n        ""fletcher32"",\n        ""contiguous"",\n        ""chunksizes"",\n        ""shuffle"",\n        ""_FillValue"",\n        ""dtype"",\n    }\n    if lsd_okay:\n        valid_encodings.add(""least_significant_digit"")\n    if h5py_okay:\n        valid_encodings.add(""compression"")\n        valid_encodings.add(""compression_opts"")\n\n    if not raise_on_invalid and encoding.get(""chunksizes"") is not None:\n        # It\'s possible to get encoded chunksizes larger than a dimension size\n        # if the original file had an unlimited dimension. This is problematic\n        # if the new file no longer has an unlimited dimension.\n        chunksizes = encoding[""chunksizes""]\n        chunks_too_big = any(\n            c > d and dim not in unlimited_dims\n            for c, d, dim in zip(chunksizes, variable.shape, variable.dims)\n        )\n        has_original_shape = ""original_shape"" in encoding\n        changed_shape = (\n            has_original_shape and encoding.get(""original_shape"") != variable.shape\n        )\n        if chunks_too_big or changed_shape:\n            del encoding[""chunksizes""]\n\n    var_has_unlim_dim = any(dim in unlimited_dims for dim in variable.dims)\n    if not raise_on_invalid and var_has_unlim_dim and ""contiguous"" in encoding.keys():\n        del encoding[""contiguous""]\n\n    for k in safe_to_drop:\n        if k in encoding:\n            del encoding[k]\n\n    if raise_on_invalid:\n        invalid = [k for k in encoding if k not in valid_encodings]\n        if invalid:\n            raise ValueError(\n                ""unexpected encoding parameters for %r backend: %r. Valid ""\n                ""encodings are: %r"" % (backend, invalid, valid_encodings)\n            )\n    else:\n        for k in list(encoding):\n            if k not in valid_encodings:\n                del encoding[k]\n\n    return encoding\n\n\ndef _is_list_of_strings(value):\n    if np.asarray(value).dtype.kind in [""U"", ""S""] and np.asarray(value).size > 1:\n        return True\n    else:\n        return False\n\n\nclass NetCDF4DataStore(WritableCFDataStore):\n    """"""Store for reading and writing data via the Python-NetCDF4 library.\n\n    This store supports NetCDF3, NetCDF4 and OpenDAP datasets.\n    """"""\n\n    __slots__ = (\n        ""autoclose"",\n        ""format"",\n        ""is_remote"",\n        ""lock"",\n        ""_filename"",\n        ""_group"",\n        ""_manager"",\n        ""_mode"",\n    )\n\n    def __init__(\n        self, manager, group=None, mode=None, lock=NETCDF4_PYTHON_LOCK, autoclose=False\n    ):\n        import netCDF4\n\n        if isinstance(manager, netCDF4.Dataset):\n            if group is None:\n                root, group = find_root_and_group(manager)\n            else:\n                if not type(manager) is netCDF4.Dataset:\n                    raise ValueError(\n                        ""must supply a root netCDF4.Dataset if the group ""\n                        ""argument is provided""\n                    )\n                root = manager\n            manager = DummyFileManager(root)\n\n        self._manager = manager\n        self._group = group\n        self._mode = mode\n        self.format = self.ds.data_model\n        self._filename = self.ds.filepath()\n        self.is_remote = is_remote_uri(self._filename)\n        self.lock = ensure_lock(lock)\n        self.autoclose = autoclose\n\n    @classmethod\n    def open(\n        cls,\n        filename,\n        mode=""r"",\n        format=""NETCDF4"",\n        group=None,\n        clobber=True,\n        diskless=False,\n        persist=False,\n        lock=None,\n        lock_maker=None,\n        autoclose=False,\n    ):\n        import netCDF4\n\n        if format is None:\n            format = ""NETCDF4""\n\n        if lock is None:\n            if mode == ""r"":\n                if is_remote_uri(filename):\n                    lock = NETCDFC_LOCK\n                else:\n                    lock = NETCDF4_PYTHON_LOCK\n            else:\n                if format is None or format.startswith(""NETCDF4""):\n                    base_lock = NETCDF4_PYTHON_LOCK\n                else:\n                    base_lock = NETCDFC_LOCK\n                lock = combine_locks([base_lock, get_write_lock(filename)])\n\n        kwargs = dict(\n            clobber=clobber, diskless=diskless, persist=persist, format=format\n        )\n        manager = CachingFileManager(\n            netCDF4.Dataset, filename, mode=mode, kwargs=kwargs\n        )\n        return cls(manager, group=group, mode=mode, lock=lock, autoclose=autoclose)\n\n    def _acquire(self, needs_lock=True):\n        with self._manager.acquire_context(needs_lock) as root:\n            ds = _nc4_require_group(root, self._group, self._mode)\n        return ds\n\n    @property\n    def ds(self):\n        return self._acquire()\n\n    def open_store_variable(self, name, var):\n        dimensions = var.dimensions\n        data = indexing.LazilyOuterIndexedArray(NetCDF4ArrayWrapper(name, self))\n        attributes = {k: var.getncattr(k) for k in var.ncattrs()}\n        _ensure_fill_value_valid(data, attributes)\n        # netCDF4 specific encoding; save _FillValue for later\n        encoding = {}\n        filters = var.filters()\n        if filters is not None:\n            encoding.update(filters)\n        chunking = var.chunking()\n        if chunking is not None:\n            if chunking == ""contiguous"":\n                encoding[""contiguous""] = True\n                encoding[""chunksizes""] = None\n            else:\n                encoding[""contiguous""] = False\n                encoding[""chunksizes""] = tuple(chunking)\n        # TODO: figure out how to round-trip ""endian-ness"" without raising\n        # warnings from netCDF4\n        # encoding[\'endian\'] = var.endian()\n        pop_to(attributes, encoding, ""least_significant_digit"")\n        # save source so __repr__ can detect if it\'s local or not\n        encoding[""source""] = self._filename\n        encoding[""original_shape""] = var.shape\n        encoding[""dtype""] = var.dtype\n\n        return Variable(dimensions, data, attributes, encoding)\n\n    def get_variables(self):\n        dsvars = FrozenDict(\n            (k, self.open_store_variable(k, v)) for k, v in self.ds.variables.items()\n        )\n        return dsvars\n\n    def get_attrs(self):\n        attrs = FrozenDict((k, self.ds.getncattr(k)) for k in self.ds.ncattrs())\n        return attrs\n\n    def get_dimensions(self):\n        dims = FrozenDict((k, len(v)) for k, v in self.ds.dimensions.items())\n        return dims\n\n    def get_encoding(self):\n        encoding = {}\n        encoding[""unlimited_dims""] = {\n            k for k, v in self.ds.dimensions.items() if v.isunlimited()\n        }\n        return encoding\n\n    def set_dimension(self, name, length, is_unlimited=False):\n        dim_length = length if not is_unlimited else None\n        self.ds.createDimension(name, size=dim_length)\n\n    def set_attribute(self, key, value):\n        if self.format != ""NETCDF4"":\n            value = encode_nc3_attr_value(value)\n        if _is_list_of_strings(value):\n            # encode as NC_STRING if attr is list of strings\n            self.ds.setncattr_string(key, value)\n        else:\n            self.ds.setncattr(key, value)\n\n    def encode_variable(self, variable):\n        variable = _force_native_endianness(variable)\n        if self.format == ""NETCDF4"":\n            variable = _encode_nc4_variable(variable)\n        else:\n            variable = encode_nc3_variable(variable)\n        return variable\n\n    def prepare_variable(\n        self, name, variable, check_encoding=False, unlimited_dims=None\n    ):\n        datatype = _get_datatype(\n            variable, self.format, raise_on_invalid_encoding=check_encoding\n        )\n        attrs = variable.attrs.copy()\n\n        fill_value = attrs.pop(""_FillValue"", None)\n\n        if datatype is str and fill_value is not None:\n            raise NotImplementedError(\n                ""netCDF4 does not yet support setting a fill value for ""\n                ""variable-length strings ""\n                ""(https://github.com/Unidata/netcdf4-python/issues/730). ""\n                ""Either remove \'_FillValue\' from encoding on variable %r ""\n                ""or set {\'dtype\': \'S1\'} in encoding to use the fixed width ""\n                ""NC_CHAR type."" % name\n            )\n\n        encoding = _extract_nc4_variable_encoding(\n            variable, raise_on_invalid=check_encoding, unlimited_dims=unlimited_dims\n        )\n\n        if name in self.ds.variables:\n            nc4_var = self.ds.variables[name]\n        else:\n            nc4_var = self.ds.createVariable(\n                varname=name,\n                datatype=datatype,\n                dimensions=variable.dims,\n                zlib=encoding.get(""zlib"", False),\n                complevel=encoding.get(""complevel"", 4),\n                shuffle=encoding.get(""shuffle"", True),\n                fletcher32=encoding.get(""fletcher32"", False),\n                contiguous=encoding.get(""contiguous"", False),\n                chunksizes=encoding.get(""chunksizes""),\n                endian=""native"",\n                least_significant_digit=encoding.get(""least_significant_digit""),\n                fill_value=fill_value,\n            )\n\n        nc4_var.setncatts(attrs)\n\n        target = NetCDF4ArrayWrapper(name, self)\n\n        return target, variable.data\n\n    def sync(self):\n        self.ds.sync()\n\n    def close(self, **kwargs):\n        self._manager.close(**kwargs)\n'"
xarray/backends/netcdf3.py,1,"b'import unicodedata\n\nimport numpy as np\n\nfrom .. import coding\nfrom ..core.variable import Variable\n\n# Special characters that are permitted in netCDF names except in the\n# 0th position of the string\n_specialchars = \'_.@+- !""#$%&\\\\()*,:;<=>?[]^`{|}~\'\n\n# The following are reserved names in CDL and may not be used as names of\n# variables, dimension, attributes\n_reserved_names = {\n    ""byte"",\n    ""char"",\n    ""short"",\n    ""ushort"",\n    ""int"",\n    ""uint"",\n    ""int64"",\n    ""uint64"",\n    ""float"" ""real"",\n    ""double"",\n    ""bool"",\n    ""string"",\n}\n\n# These data-types aren\'t supported by netCDF3, so they are automatically\n# coerced instead as indicated by the ""coerce_nc3_dtype"" function\n_nc3_dtype_coercions = {\n    ""int64"": ""int32"",\n    ""uint64"": ""int32"",\n    ""uint32"": ""int32"",\n    ""uint16"": ""int16"",\n    ""uint8"": ""int8"",\n    ""bool"": ""int8"",\n}\n\n# encode all strings as UTF-8\nSTRING_ENCODING = ""utf-8""\n\n\ndef coerce_nc3_dtype(arr):\n    """"""Coerce an array to a data type that can be stored in a netCDF-3 file\n\n    This function performs the dtype conversions as specified by the\n    ``_nc3_dtype_coercions`` mapping:\n        int64  -> int32\n        uint64 -> int32\n        uint32 -> int32\n        uint16 -> int16\n        uint8  -> int8\n        bool   -> int8\n\n    Data is checked for equality, or equivalence (non-NaN values) using the\n    ``(cast_array == original_array).all()``.\n    """"""\n    dtype = str(arr.dtype)\n    if dtype in _nc3_dtype_coercions:\n        new_dtype = _nc3_dtype_coercions[dtype]\n        # TODO: raise a warning whenever casting the data-type instead?\n        cast_arr = arr.astype(new_dtype)\n        if not (cast_arr == arr).all():\n            raise ValueError(\n                f""could not safely cast array from dtype {dtype} to {new_dtype}""\n            )\n        arr = cast_arr\n    return arr\n\n\ndef encode_nc3_attr_value(value):\n    if isinstance(value, bytes):\n        pass\n    elif isinstance(value, str):\n        value = value.encode(STRING_ENCODING)\n    else:\n        value = coerce_nc3_dtype(np.atleast_1d(value))\n        if value.ndim > 1:\n            raise ValueError(""netCDF attributes must be 1-dimensional"")\n    return value\n\n\ndef encode_nc3_attrs(attrs):\n    return {k: encode_nc3_attr_value(v) for k, v in attrs.items()}\n\n\ndef encode_nc3_variable(var):\n    for coder in [\n        coding.strings.EncodedStringCoder(allows_unicode=False),\n        coding.strings.CharacterArrayCoder(),\n    ]:\n        var = coder.encode(var)\n    data = coerce_nc3_dtype(var.data)\n    attrs = encode_nc3_attrs(var.attrs)\n    return Variable(var.dims, data, attrs, var.encoding)\n\n\ndef _isalnumMUTF8(c):\n    """"""Return True if the given UTF-8 encoded character is alphanumeric\n    or multibyte.\n\n    Input is not checked!\n    """"""\n    return c.isalnum() or (len(c.encode(""utf-8"")) > 1)\n\n\ndef is_valid_nc3_name(s):\n    """"""Test whether an object can be validly converted to a netCDF-3\n    dimension, variable or attribute name\n\n    Earlier versions of the netCDF C-library reference implementation\n    enforced a more restricted set of characters in creating new names,\n    but permitted reading names containing arbitrary bytes. This\n    specification extends the permitted characters in names to include\n    multi-byte UTF-8 encoded Unicode and additional printing characters\n    from the US-ASCII alphabet. The first character of a name must be\n    alphanumeric, a multi-byte UTF-8 character, or \'_\' (reserved for\n    special names with meaning to implementations, such as the\n    ""_FillValue"" attribute). Subsequent characters may also include\n    printing special characters, except for \'/\' which is not allowed in\n    names. Names that have trailing space characters are also not\n    permitted.\n    """"""\n    if not isinstance(s, str):\n        return False\n    if not isinstance(s, str):\n        s = s.decode(""utf-8"")\n    num_bytes = len(s.encode(""utf-8""))\n    return (\n        (unicodedata.normalize(""NFC"", s) == s)\n        and (s not in _reserved_names)\n        and (num_bytes >= 0)\n        and (""/"" not in s)\n        and (s[-1] != "" "")\n        and (_isalnumMUTF8(s[0]) or (s[0] == ""_""))\n        and all(_isalnumMUTF8(c) or c in _specialchars for c in s)\n    )\n'"
xarray/backends/pseudonetcdf_.py,1,"b'import numpy as np\n\nfrom ..core import indexing\nfrom ..core.utils import Frozen, FrozenDict\nfrom ..core.variable import Variable\nfrom .common import AbstractDataStore, BackendArray\nfrom .file_manager import CachingFileManager\nfrom .locks import HDF5_LOCK, NETCDFC_LOCK, combine_locks, ensure_lock\n\n# psuedonetcdf can invoke netCDF libraries internally\nPNETCDF_LOCK = combine_locks([HDF5_LOCK, NETCDFC_LOCK])\n\n\nclass PncArrayWrapper(BackendArray):\n    def __init__(self, variable_name, datastore):\n        self.datastore = datastore\n        self.variable_name = variable_name\n        array = self.get_array()\n        self.shape = array.shape\n        self.dtype = np.dtype(array.dtype)\n\n    def get_array(self, needs_lock=True):\n        ds = self.datastore._manager.acquire(needs_lock)\n        return ds.variables[self.variable_name]\n\n    def __getitem__(self, key):\n        return indexing.explicit_indexing_adapter(\n            key, self.shape, indexing.IndexingSupport.OUTER_1VECTOR, self._getitem\n        )\n\n    def _getitem(self, key):\n        with self.datastore.lock:\n            array = self.get_array(needs_lock=False)\n            return array[key]\n\n\nclass PseudoNetCDFDataStore(AbstractDataStore):\n    """"""Store for accessing datasets via PseudoNetCDF\n    """"""\n\n    @classmethod\n    def open(cls, filename, lock=None, mode=None, **format_kwargs):\n        from PseudoNetCDF import pncopen\n\n        keywords = {""kwargs"": format_kwargs}\n        # only include mode if explicitly passed\n        if mode is not None:\n            keywords[""mode""] = mode\n\n        if lock is None:\n            lock = PNETCDF_LOCK\n\n        manager = CachingFileManager(pncopen, filename, lock=lock, **keywords)\n        return cls(manager, lock)\n\n    def __init__(self, manager, lock=None):\n        self._manager = manager\n        self.lock = ensure_lock(lock)\n\n    @property\n    def ds(self):\n        return self._manager.acquire()\n\n    def open_store_variable(self, name, var):\n        data = indexing.LazilyOuterIndexedArray(PncArrayWrapper(name, self))\n        attrs = {k: getattr(var, k) for k in var.ncattrs()}\n        return Variable(var.dimensions, data, attrs)\n\n    def get_variables(self):\n        return FrozenDict(\n            (k, self.open_store_variable(k, v)) for k, v in self.ds.variables.items()\n        )\n\n    def get_attrs(self):\n        return Frozen({k: getattr(self.ds, k) for k in self.ds.ncattrs()})\n\n    def get_dimensions(self):\n        return Frozen(self.ds.dimensions)\n\n    def get_encoding(self):\n        return {\n            ""unlimited_dims"": {\n                k for k in self.ds.dimensions if self.ds.dimensions[k].isunlimited()\n            }\n        }\n\n    def close(self):\n        self._manager.close()\n'"
xarray/backends/pydap_.py,1,"b'import numpy as np\n\nfrom ..core import indexing\nfrom ..core.pycompat import integer_types\nfrom ..core.utils import Frozen, FrozenDict, is_dict_like\nfrom ..core.variable import Variable\nfrom .common import AbstractDataStore, BackendArray, robust_getitem\n\n\nclass PydapArrayWrapper(BackendArray):\n    def __init__(self, array):\n        self.array = array\n\n    @property\n    def shape(self):\n        return self.array.shape\n\n    @property\n    def dtype(self):\n        return self.array.dtype\n\n    def __getitem__(self, key):\n        return indexing.explicit_indexing_adapter(\n            key, self.shape, indexing.IndexingSupport.BASIC, self._getitem\n        )\n\n    def _getitem(self, key):\n        # pull the data from the array attribute if possible, to avoid\n        # downloading coordinate data twice\n        array = getattr(self.array, ""array"", self.array)\n        result = robust_getitem(array, key, catch=ValueError)\n        # in some cases, pydap doesn\'t squeeze axes automatically like numpy\n        axis = tuple(n for n, k in enumerate(key) if isinstance(k, integer_types))\n        if result.ndim + len(axis) != array.ndim and len(axis) > 0:\n            result = np.squeeze(result, axis)\n\n        return result\n\n\ndef _fix_attributes(attributes):\n    attributes = dict(attributes)\n    for k in list(attributes):\n        if k.lower() == ""global"" or k.lower().endswith(""_global""):\n            # move global attributes to the top level, like the netcdf-C\n            # DAP client\n            attributes.update(attributes.pop(k))\n        elif is_dict_like(attributes[k]):\n            # Make Hierarchical attributes to a single level with a\n            # dot-separated key\n            attributes.update(\n                {\n                    f""{k}.{k_child}"": v_child\n                    for k_child, v_child in attributes.pop(k).items()\n                }\n            )\n    return attributes\n\n\nclass PydapDataStore(AbstractDataStore):\n    """"""Store for accessing OpenDAP datasets with pydap.\n\n    This store provides an alternative way to access OpenDAP datasets that may\n    be useful if the netCDF4 library is not available.\n    """"""\n\n    def __init__(self, ds):\n        """"""\n        Parameters\n        ----------\n        ds : pydap DatasetType\n        """"""\n        self.ds = ds\n\n    @classmethod\n    def open(cls, url, session=None):\n        import pydap.client\n\n        ds = pydap.client.open_url(url, session=session)\n        return cls(ds)\n\n    def open_store_variable(self, var):\n        data = indexing.LazilyOuterIndexedArray(PydapArrayWrapper(var))\n        return Variable(var.dimensions, data, _fix_attributes(var.attributes))\n\n    def get_variables(self):\n        return FrozenDict(\n            (k, self.open_store_variable(self.ds[k])) for k in self.ds.keys()\n        )\n\n    def get_attrs(self):\n        return Frozen(_fix_attributes(self.ds.attributes))\n\n    def get_dimensions(self):\n        return Frozen(self.ds.dimensions)\n'"
xarray/backends/pynio_.py,1,"b'import numpy as np\n\nfrom ..core import indexing\nfrom ..core.utils import Frozen, FrozenDict\nfrom ..core.variable import Variable\nfrom .common import AbstractDataStore, BackendArray\nfrom .file_manager import CachingFileManager\nfrom .locks import HDF5_LOCK, NETCDFC_LOCK, SerializableLock, combine_locks, ensure_lock\n\n# PyNIO can invoke netCDF libraries internally\n# Add a dedicated lock just in case NCL as well isn\'t thread-safe.\nNCL_LOCK = SerializableLock()\nPYNIO_LOCK = combine_locks([HDF5_LOCK, NETCDFC_LOCK, NCL_LOCK])\n\n\nclass NioArrayWrapper(BackendArray):\n    def __init__(self, variable_name, datastore):\n        self.datastore = datastore\n        self.variable_name = variable_name\n        array = self.get_array()\n        self.shape = array.shape\n        self.dtype = np.dtype(array.typecode())\n\n    def get_array(self, needs_lock=True):\n        ds = self.datastore._manager.acquire(needs_lock)\n        return ds.variables[self.variable_name]\n\n    def __getitem__(self, key):\n        return indexing.explicit_indexing_adapter(\n            key, self.shape, indexing.IndexingSupport.BASIC, self._getitem\n        )\n\n    def _getitem(self, key):\n        with self.datastore.lock:\n            array = self.get_array(needs_lock=False)\n\n            if key == () and self.ndim == 0:\n                return array.get_value()\n\n            return array[key]\n\n\nclass NioDataStore(AbstractDataStore):\n    """"""Store for accessing datasets via PyNIO\n    """"""\n\n    def __init__(self, filename, mode=""r"", lock=None, **kwargs):\n        import Nio\n\n        if lock is None:\n            lock = PYNIO_LOCK\n        self.lock = ensure_lock(lock)\n        self._manager = CachingFileManager(\n            Nio.open_file, filename, lock=lock, mode=mode, kwargs=kwargs\n        )\n        # xarray provides its own support for FillValue,\n        # so turn off PyNIO\'s support for the same.\n        self.ds.set_option(""MaskedArrayMode"", ""MaskedNever"")\n\n    @property\n    def ds(self):\n        return self._manager.acquire()\n\n    def open_store_variable(self, name, var):\n        data = indexing.LazilyOuterIndexedArray(NioArrayWrapper(name, self))\n        return Variable(var.dimensions, data, var.attributes)\n\n    def get_variables(self):\n        return FrozenDict(\n            (k, self.open_store_variable(k, v)) for k, v in self.ds.variables.items()\n        )\n\n    def get_attrs(self):\n        return Frozen(self.ds.attributes)\n\n    def get_dimensions(self):\n        return Frozen(self.ds.dimensions)\n\n    def get_encoding(self):\n        return {\n            ""unlimited_dims"": {k for k in self.ds.dimensions if self.ds.unlimited(k)}\n        }\n\n    def close(self):\n        self._manager.close()\n'"
xarray/backends/rasterio_.py,18,"b'import os\nimport warnings\n\nimport numpy as np\n\nfrom ..core import indexing\nfrom ..core.dataarray import DataArray\nfrom ..core.utils import is_scalar\nfrom .common import BackendArray\nfrom .file_manager import CachingFileManager\nfrom .locks import SerializableLock\n\n# TODO: should this be GDAL_LOCK instead?\nRASTERIO_LOCK = SerializableLock()\n\n_ERROR_MSG = (\n    ""The kind of indexing operation you are trying to do is not ""\n    ""valid on rasterio files. Try to load your data with ds.load()""\n    ""first.""\n)\n\n\nclass RasterioArrayWrapper(BackendArray):\n    """"""A wrapper around rasterio dataset objects""""""\n\n    def __init__(self, manager, lock, vrt_params=None):\n        from rasterio.vrt import WarpedVRT\n\n        self.manager = manager\n        self.lock = lock\n\n        # cannot save riods as an attribute: this would break pickleability\n        riods = manager.acquire()\n        if vrt_params is not None:\n            riods = WarpedVRT(riods, **vrt_params)\n        self.vrt_params = vrt_params\n        self._shape = (riods.count, riods.height, riods.width)\n\n        dtypes = riods.dtypes\n        if not np.all(np.asarray(dtypes) == dtypes[0]):\n            raise ValueError(""All bands should have the same dtype"")\n        self._dtype = np.dtype(dtypes[0])\n\n    @property\n    def dtype(self):\n        return self._dtype\n\n    @property\n    def shape(self):\n        return self._shape\n\n    def _get_indexer(self, key):\n        """""" Get indexer for rasterio array.\n\n        Parameter\n        ---------\n        key: tuple of int\n\n        Returns\n        -------\n        band_key: an indexer for the 1st dimension\n        window: two tuples. Each consists of (start, stop).\n        squeeze_axis: axes to be squeezed\n        np_ind: indexer for loaded numpy array\n\n        See also\n        --------\n        indexing.decompose_indexer\n        """"""\n        assert len(key) == 3, ""rasterio datasets should always be 3D""\n\n        # bands cannot be windowed but they can be listed\n        band_key = key[0]\n        np_inds = []\n        # bands (axis=0) cannot be windowed but they can be listed\n        if isinstance(band_key, slice):\n            start, stop, step = band_key.indices(self.shape[0])\n            band_key = np.arange(start, stop, step)\n        # be sure we give out a list\n        band_key = (np.asarray(band_key) + 1).tolist()\n        if isinstance(band_key, list):  # if band_key is not a scalar\n            np_inds.append(slice(None))\n\n        # but other dims can only be windowed\n        window = []\n        squeeze_axis = []\n        for i, (k, n) in enumerate(zip(key[1:], self.shape[1:])):\n            if isinstance(k, slice):\n                # step is always positive. see indexing.decompose_indexer\n                start, stop, step = k.indices(n)\n                np_inds.append(slice(None, None, step))\n            elif is_scalar(k):\n                # windowed operations will always return an array\n                # we will have to squeeze it later\n                squeeze_axis.append(-(2 - i))\n                start = k\n                stop = k + 1\n            else:\n                start, stop = np.min(k), np.max(k) + 1\n                np_inds.append(k - start)\n            window.append((start, stop))\n\n        if isinstance(key[1], np.ndarray) and isinstance(key[2], np.ndarray):\n            # do outer-style indexing\n            np_inds[-2:] = np.ix_(*np_inds[-2:])\n\n        return band_key, tuple(window), tuple(squeeze_axis), tuple(np_inds)\n\n    def _getitem(self, key):\n        from rasterio.vrt import WarpedVRT\n\n        band_key, window, squeeze_axis, np_inds = self._get_indexer(key)\n\n        if not band_key or any(start == stop for (start, stop) in window):\n            # no need to do IO\n            shape = (len(band_key),) + tuple(stop - start for (start, stop) in window)\n            out = np.zeros(shape, dtype=self.dtype)\n        else:\n            with self.lock:\n                riods = self.manager.acquire(needs_lock=False)\n                if self.vrt_params is not None:\n                    riods = WarpedVRT(riods, **self.vrt_params)\n                out = riods.read(band_key, window=window)\n\n        if squeeze_axis:\n            out = np.squeeze(out, axis=squeeze_axis)\n        return out[np_inds]\n\n    def __getitem__(self, key):\n        return indexing.explicit_indexing_adapter(\n            key, self.shape, indexing.IndexingSupport.OUTER, self._getitem\n        )\n\n\ndef _parse_envi(meta):\n    """"""Parse ENVI metadata into Python data structures.\n\n    See the link for information on the ENVI header file format:\n    http://www.harrisgeospatial.com/docs/enviheaderfiles.html\n\n    Parameters\n    ----------\n    meta : dict\n        Dictionary of keys and str values to parse, as returned by the rasterio\n        tags(ns=\'ENVI\') call.\n\n    Returns\n    -------\n    parsed_meta : dict\n        Dictionary containing the original keys and the parsed values\n\n    """"""\n\n    def parsevec(s):\n        return np.fromstring(s.strip(""{}""), dtype=""float"", sep="","")\n\n    def default(s):\n        return s.strip(""{}"")\n\n    parse = {""wavelength"": parsevec, ""fwhm"": parsevec}\n    parsed_meta = {k: parse.get(k, default)(v) for k, v in meta.items()}\n    return parsed_meta\n\n\ndef open_rasterio(filename, parse_coordinates=None, chunks=None, cache=None, lock=None):\n    """"""Open a file with rasterio (experimental).\n\n    This should work with any file that rasterio can open (most often:\n    geoTIFF). The x and y coordinates are generated automatically from the\n    file\'s geoinformation, shifted to the center of each pixel (see\n    `""PixelIsArea"" Raster Space\n    <http://web.archive.org/web/20160326194152/http://remotesensing.org/geotiff/spec/geotiff2.5.html#2.5.2>`_\n    for more information).\n\n    You can generate 2D coordinates from the file\'s attributes with::\n\n        from affine import Affine\n        da = xr.open_rasterio(\'path_to_file.tif\')\n        transform = Affine.from_gdal(*da.attrs[\'transform\'])\n        nx, ny = da.sizes[\'x\'], da.sizes[\'y\']\n        x, y = np.meshgrid(np.arange(nx)+0.5, np.arange(ny)+0.5) * transform\n\n\n    Parameters\n    ----------\n    filename : str, rasterio.DatasetReader, or rasterio.WarpedVRT\n        Path to the file to open. Or already open rasterio dataset.\n    parse_coordinates : bool, optional\n        Whether to parse the x and y coordinates out of the file\'s\n        ``transform`` attribute or not. The default is to automatically\n        parse the coordinates only if they are rectilinear (1D).\n        It can be useful to set ``parse_coordinates=False``\n        if your files are very large or if you don\'t need the coordinates.\n    chunks : int, tuple or dict, optional\n        Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\n        ``{\'x\': 5, \'y\': 5}``. If chunks is provided, it used to load the new\n        DataArray into a dask array.\n    cache : bool, optional\n        If True, cache data loaded from the underlying datastore in memory as\n        NumPy arrays when accessed to avoid reading from the underlying data-\n        store multiple times. Defaults to True unless you specify the `chunks`\n        argument to use dask, in which case it defaults to False.\n    lock : False, True or threading.Lock, optional\n        If chunks is provided, this argument is passed on to\n        :py:func:`dask.array.from_array`. By default, a global lock is\n        used to avoid issues with concurrent access to the same file when using\n        dask\'s multithreaded backend.\n\n    Returns\n    -------\n    data : DataArray\n        The newly created DataArray.\n    """"""\n    import rasterio\n    from rasterio.vrt import WarpedVRT\n\n    vrt_params = None\n    if isinstance(filename, rasterio.io.DatasetReader):\n        filename = filename.name\n    elif isinstance(filename, rasterio.vrt.WarpedVRT):\n        vrt = filename\n        filename = vrt.src_dataset.name\n        vrt_params = dict(\n            src_crs=vrt.src_crs.to_string(),\n            crs=vrt.crs.to_string(),\n            resampling=vrt.resampling,\n            tolerance=vrt.tolerance,\n            src_nodata=vrt.src_nodata,\n            nodata=vrt.nodata,\n            width=vrt.width,\n            height=vrt.height,\n            src_transform=vrt.src_transform,\n            transform=vrt.transform,\n            dtype=vrt.working_dtype,\n            warp_extras=vrt.warp_extras,\n        )\n\n    if lock is None:\n        lock = RASTERIO_LOCK\n\n    manager = CachingFileManager(rasterio.open, filename, lock=lock, mode=""r"")\n    riods = manager.acquire()\n    if vrt_params is not None:\n        riods = WarpedVRT(riods, **vrt_params)\n\n    if cache is None:\n        cache = chunks is None\n\n    coords = {}\n\n    # Get bands\n    if riods.count < 1:\n        raise ValueError(""Unknown dims"")\n    coords[""band""] = np.asarray(riods.indexes)\n\n    # Get coordinates\n    if riods.transform.is_rectilinear:\n        # 1d coordinates\n        parse = True if parse_coordinates is None else parse_coordinates\n        if parse:\n            nx, ny = riods.width, riods.height\n            # xarray coordinates are pixel centered\n            x, _ = riods.transform * (np.arange(nx) + 0.5, np.zeros(nx) + 0.5)\n            _, y = riods.transform * (np.zeros(ny) + 0.5, np.arange(ny) + 0.5)\n            coords[""y""] = y\n            coords[""x""] = x\n    else:\n        # 2d coordinates\n        parse = False if (parse_coordinates is None) else parse_coordinates\n        if parse:\n            warnings.warn(\n                ""The file coordinates\' transformation isn\'t ""\n                ""rectilinear: xarray won\'t parse the coordinates ""\n                ""in this case. Set `parse_coordinates=False` to ""\n                ""suppress this warning."",\n                RuntimeWarning,\n                stacklevel=3,\n            )\n\n    # Attributes\n    attrs = {}\n    # Affine transformation matrix (always available)\n    # This describes coefficients mapping pixel coordinates to CRS\n    # For serialization store as tuple of 6 floats, the last row being\n    # always (0, 0, 1) per definition (see\n    # https://github.com/sgillies/affine)\n    attrs[""transform""] = tuple(riods.transform)[:6]\n    if hasattr(riods, ""crs"") and riods.crs:\n        # CRS is a dict-like object specific to rasterio\n        # If CRS is not None, we convert it back to a PROJ4 string using\n        # rasterio itself\n        try:\n            attrs[""crs""] = riods.crs.to_proj4()\n        except AttributeError:\n            attrs[""crs""] = riods.crs.to_string()\n    if hasattr(riods, ""res""):\n        # (width, height) tuple of pixels in units of CRS\n        attrs[""res""] = riods.res\n    if hasattr(riods, ""is_tiled""):\n        # Is the TIF tiled? (bool)\n        # We cast it to an int for netCDF compatibility\n        attrs[""is_tiled""] = np.uint8(riods.is_tiled)\n    if hasattr(riods, ""nodatavals""):\n        # The nodata values for the raster bands\n        attrs[""nodatavals""] = tuple(\n            np.nan if nodataval is None else nodataval for nodataval in riods.nodatavals\n        )\n    if hasattr(riods, ""scales""):\n        # The scale values for the raster bands\n        attrs[""scales""] = riods.scales\n    if hasattr(riods, ""offsets""):\n        # The offset values for the raster bands\n        attrs[""offsets""] = riods.offsets\n    if hasattr(riods, ""descriptions"") and any(riods.descriptions):\n        # Descriptions for each dataset band\n        attrs[""descriptions""] = riods.descriptions\n    if hasattr(riods, ""units"") and any(riods.units):\n        # A list of units string for each dataset band\n        attrs[""units""] = riods.units\n\n    # Parse extra metadata from tags, if supported\n    parsers = {""ENVI"": _parse_envi, ""GTiff"": lambda m: m}\n\n    driver = riods.driver\n    if driver in parsers:\n        if driver == ""GTiff"":\n            meta = parsers[driver](riods.tags())\n        else:\n            meta = parsers[driver](riods.tags(ns=driver))\n\n        for k, v in meta.items():\n            # Add values as coordinates if they match the band count,\n            # as attributes otherwise\n            if isinstance(v, (list, np.ndarray)) and len(v) == riods.count:\n                coords[k] = (""band"", np.asarray(v))\n            else:\n                attrs[k] = v\n\n    data = indexing.LazilyOuterIndexedArray(\n        RasterioArrayWrapper(manager, lock, vrt_params)\n    )\n\n    # this lets you write arrays loaded with rasterio\n    data = indexing.CopyOnWriteArray(data)\n    if cache and chunks is None:\n        data = indexing.MemoryCachedArray(data)\n\n    result = DataArray(data=data, dims=(""band"", ""y"", ""x""), coords=coords, attrs=attrs)\n\n    if chunks is not None:\n        from dask.base import tokenize\n\n        # augment the token with the file modification time\n        try:\n            mtime = os.path.getmtime(filename)\n        except OSError:\n            # the filename is probably an s3 bucket rather than a regular file\n            mtime = None\n        token = tokenize(filename, mtime, chunks)\n        name_prefix = ""open_rasterio-%s"" % token\n        result = result.chunk(chunks, name_prefix=name_prefix, token=token)\n\n    # Make the file closeable\n    result._file_obj = manager\n\n    return result\n'"
xarray/backends/scipy_.py,2,"b'from io import BytesIO\n\nimport numpy as np\n\nfrom ..core.indexing import NumpyIndexingAdapter\nfrom ..core.utils import Frozen, FrozenDict\nfrom ..core.variable import Variable\nfrom .common import BackendArray, WritableCFDataStore\nfrom .file_manager import CachingFileManager, DummyFileManager\nfrom .locks import ensure_lock, get_write_lock\nfrom .netcdf3 import encode_nc3_attr_value, encode_nc3_variable, is_valid_nc3_name\n\n\ndef _decode_string(s):\n    if isinstance(s, bytes):\n        return s.decode(""utf-8"", ""replace"")\n    return s\n\n\ndef _decode_attrs(d):\n    # don\'t decode _FillValue from bytes -> unicode, because we want to ensure\n    # that its type matches the data exactly\n    return {k: v if k == ""_FillValue"" else _decode_string(v) for (k, v) in d.items()}\n\n\nclass ScipyArrayWrapper(BackendArray):\n    def __init__(self, variable_name, datastore):\n        self.datastore = datastore\n        self.variable_name = variable_name\n        array = self.get_variable().data\n        self.shape = array.shape\n        self.dtype = np.dtype(array.dtype.kind + str(array.dtype.itemsize))\n\n    def get_variable(self, needs_lock=True):\n        ds = self.datastore._manager.acquire(needs_lock)\n        return ds.variables[self.variable_name]\n\n    def __getitem__(self, key):\n        data = NumpyIndexingAdapter(self.get_variable().data)[key]\n        # Copy data if the source file is mmapped. This makes things consistent\n        # with the netCDF4 library by ensuring we can safely read arrays even\n        # after closing associated files.\n        copy = self.datastore.ds.use_mmap\n        return np.array(data, dtype=self.dtype, copy=copy)\n\n    def __setitem__(self, key, value):\n        with self.datastore.lock:\n            data = self.get_variable(needs_lock=False)\n            try:\n                data[key] = value\n            except TypeError:\n                if key is Ellipsis:\n                    # workaround for GH: scipy/scipy#6880\n                    data[:] = value\n                else:\n                    raise\n\n\ndef _open_scipy_netcdf(filename, mode, mmap, version):\n    import scipy.io\n    import gzip\n\n    # if the string ends with .gz, then gunzip and open as netcdf file\n    if isinstance(filename, str) and filename.endswith("".gz""):\n        try:\n            return scipy.io.netcdf_file(\n                gzip.open(filename), mode=mode, mmap=mmap, version=version\n            )\n        except TypeError as e:\n            # TODO: gzipped loading only works with NetCDF3 files.\n            if ""is not a valid NetCDF 3 file"" in e.message:\n                raise ValueError(\n                    ""gzipped file loading only supports "" ""NetCDF 3 files.""\n                )\n            else:\n                raise\n\n    if isinstance(filename, bytes) and filename.startswith(b""CDF""):\n        # it\'s a NetCDF3 bytestring\n        filename = BytesIO(filename)\n\n    try:\n        return scipy.io.netcdf_file(filename, mode=mode, mmap=mmap, version=version)\n    except TypeError as e:  # netcdf3 message is obscure in this case\n        errmsg = e.args[0]\n        if ""is not a valid NetCDF 3 file"" in errmsg:\n            msg = """"""\n            If this is a NetCDF4 file, you may need to install the\n            netcdf4 library, e.g.,\n\n            $ pip install netcdf4\n            """"""\n            errmsg += msg\n            raise TypeError(errmsg)\n        else:\n            raise\n\n\nclass ScipyDataStore(WritableCFDataStore):\n    """"""Store for reading and writing data via scipy.io.netcdf.\n\n    This store has the advantage of being able to be initialized with a\n    StringIO object, allow for serialization without writing to disk.\n\n    It only supports the NetCDF3 file-format.\n    """"""\n\n    def __init__(\n        self, filename_or_obj, mode=""r"", format=None, group=None, mmap=None, lock=None\n    ):\n        if group is not None:\n            raise ValueError(\n                ""cannot save to a group with the "" ""scipy.io.netcdf backend""\n            )\n\n        if format is None or format == ""NETCDF3_64BIT"":\n            version = 2\n        elif format == ""NETCDF3_CLASSIC"":\n            version = 1\n        else:\n            raise ValueError(""invalid format for scipy.io.netcdf backend: %r"" % format)\n\n        if lock is None and mode != ""r"" and isinstance(filename_or_obj, str):\n            lock = get_write_lock(filename_or_obj)\n\n        self.lock = ensure_lock(lock)\n\n        if isinstance(filename_or_obj, str):\n            manager = CachingFileManager(\n                _open_scipy_netcdf,\n                filename_or_obj,\n                mode=mode,\n                lock=lock,\n                kwargs=dict(mmap=mmap, version=version),\n            )\n        else:\n            scipy_dataset = _open_scipy_netcdf(\n                filename_or_obj, mode=mode, mmap=mmap, version=version\n            )\n            manager = DummyFileManager(scipy_dataset)\n\n        self._manager = manager\n\n    @property\n    def ds(self):\n        return self._manager.acquire()\n\n    def open_store_variable(self, name, var):\n        return Variable(\n            var.dimensions,\n            ScipyArrayWrapper(name, self),\n            _decode_attrs(var._attributes),\n        )\n\n    def get_variables(self):\n        return FrozenDict(\n            (k, self.open_store_variable(k, v)) for k, v in self.ds.variables.items()\n        )\n\n    def get_attrs(self):\n        return Frozen(_decode_attrs(self.ds._attributes))\n\n    def get_dimensions(self):\n        return Frozen(self.ds.dimensions)\n\n    def get_encoding(self):\n        encoding = {}\n        encoding[""unlimited_dims""] = {\n            k for k, v in self.ds.dimensions.items() if v is None\n        }\n        return encoding\n\n    def set_dimension(self, name, length, is_unlimited=False):\n        if name in self.ds.dimensions:\n            raise ValueError(\n                ""%s does not support modifying dimensions"" % type(self).__name__\n            )\n        dim_length = length if not is_unlimited else None\n        self.ds.createDimension(name, dim_length)\n\n    def _validate_attr_key(self, key):\n        if not is_valid_nc3_name(key):\n            raise ValueError(""Not a valid attribute name"")\n\n    def set_attribute(self, key, value):\n        self._validate_attr_key(key)\n        value = encode_nc3_attr_value(value)\n        setattr(self.ds, key, value)\n\n    def encode_variable(self, variable):\n        variable = encode_nc3_variable(variable)\n        return variable\n\n    def prepare_variable(\n        self, name, variable, check_encoding=False, unlimited_dims=None\n    ):\n        if check_encoding and variable.encoding:\n            if variable.encoding != {""_FillValue"": None}:\n                raise ValueError(\n                    ""unexpected encoding for scipy backend: %r""\n                    % list(variable.encoding)\n                )\n\n        data = variable.data\n        # nb. this still creates a numpy array in all memory, even though we\n        # don\'t write the data yet; scipy.io.netcdf does not not support\n        # incremental writes.\n        if name not in self.ds.variables:\n            self.ds.createVariable(name, data.dtype, variable.dims)\n        scipy_var = self.ds.variables[name]\n        for k, v in variable.attrs.items():\n            self._validate_attr_key(k)\n            setattr(scipy_var, k, v)\n\n        target = ScipyArrayWrapper(name, self)\n\n        return target, data\n\n    def sync(self):\n        self.ds.sync()\n\n    def close(self):\n        self._manager.close()\n'"
xarray/backends/zarr.py,2,"b'import warnings\n\nimport numpy as np\n\nfrom .. import coding, conventions\nfrom ..core import indexing\nfrom ..core.pycompat import integer_types\nfrom ..core.utils import FrozenDict, HiddenKeyDict\nfrom ..core.variable import Variable\nfrom .common import AbstractWritableDataStore, BackendArray, _encode_variable_name\n\n# need some special secret attributes to tell us the dimensions\nDIMENSION_KEY = ""_ARRAY_DIMENSIONS""\n\n\ndef encode_zarr_attr_value(value):\n    """"""\n    Encode a attribute value as something that can be serialized as json\n\n    Many xarray datasets / variables have numpy arrays and values. This\n    function handles encoding / decoding of such items.\n\n    ndarray -> list\n    scalar array -> scalar\n    other -> other (no change)\n    """"""\n    if isinstance(value, np.ndarray):\n        encoded = value.tolist()\n    # this checks if it\'s a scalar number\n    elif isinstance(value, np.generic):\n        encoded = value.item()\n    else:\n        encoded = value\n    return encoded\n\n\nclass ZarrArrayWrapper(BackendArray):\n    __slots__ = (""datastore"", ""dtype"", ""shape"", ""variable_name"")\n\n    def __init__(self, variable_name, datastore):\n        self.datastore = datastore\n        self.variable_name = variable_name\n\n        array = self.get_array()\n        self.shape = array.shape\n\n        dtype = array.dtype\n        self.dtype = dtype\n\n    def get_array(self):\n        return self.datastore.ds[self.variable_name]\n\n    def __getitem__(self, key):\n        array = self.get_array()\n        if isinstance(key, indexing.BasicIndexer):\n            return array[key.tuple]\n        elif isinstance(key, indexing.VectorizedIndexer):\n            return array.vindex[\n                indexing._arrayize_vectorized_indexer(key, self.shape).tuple\n            ]\n        else:\n            assert isinstance(key, indexing.OuterIndexer)\n            return array.oindex[key.tuple]\n        # if self.ndim == 0:\n        # could possibly have a work-around for 0d data here\n\n\ndef _determine_zarr_chunks(enc_chunks, var_chunks, ndim, name):\n    """"""\n    Given encoding chunks (possibly None) and variable chunks (possibly None)\n    """"""\n\n    # zarr chunk spec:\n    # chunks : int or tuple of ints, optional\n    #   Chunk shape. If not provided, will be guessed from shape and dtype.\n\n    # if there are no chunks in encoding and the variable data is a numpy\n    # array, then we let zarr use its own heuristics to pick the chunks\n    if var_chunks is None and enc_chunks is None:\n        return None\n\n    # if there are no chunks in encoding but there are dask chunks, we try to\n    # use the same chunks in zarr\n    # However, zarr chunks needs to be uniform for each array\n    # http://zarr.readthedocs.io/en/latest/spec/v1.html#chunks\n    # while dask chunks can be variable sized\n    # http://dask.pydata.org/en/latest/array-design.html#chunks\n    if var_chunks and enc_chunks is None:\n        if any(len(set(chunks[:-1])) > 1 for chunks in var_chunks):\n            raise ValueError(\n                ""Zarr requires uniform chunk sizes except for final chunk. ""\n                f""Variable named {name!r} has incompatible dask chunks: {var_chunks!r}. ""\n                ""Consider rechunking using `chunk()`.""\n            )\n        if any((chunks[0] < chunks[-1]) for chunks in var_chunks):\n            raise ValueError(\n                ""Final chunk of Zarr array must be the same size or smaller ""\n                f""than the first. Variable named {name!r} has incompatible Dask chunks {var_chunks!r}.""\n                ""Consider either rechunking using `chunk()` or instead deleting ""\n                ""or modifying `encoding[\'chunks\']`.""\n            )\n        # return the first chunk for each dimension\n        return tuple(chunk[0] for chunk in var_chunks)\n\n    # from here on, we are dealing with user-specified chunks in encoding\n    # zarr allows chunks to be an integer, in which case it uses the same chunk\n    # size on each dimension.\n    # Here we re-implement this expansion ourselves. That makes the logic of\n    # checking chunk compatibility easier\n\n    if isinstance(enc_chunks, integer_types):\n        enc_chunks_tuple = ndim * (enc_chunks,)\n    else:\n        enc_chunks_tuple = tuple(enc_chunks)\n\n    if len(enc_chunks_tuple) != ndim:\n        # throw away encoding chunks, start over\n        return _determine_zarr_chunks(None, var_chunks, ndim, name)\n\n    for x in enc_chunks_tuple:\n        if not isinstance(x, int):\n            raise TypeError(\n                ""zarr chunk sizes specified in `encoding[\'chunks\']` ""\n                ""must be an int or a tuple of ints. ""\n                f""Instead found encoding[\'chunks\']={enc_chunks_tuple!r} ""\n                f""for variable named {name!r}.""\n            )\n\n    # if there are chunks in encoding and the variable data is a numpy array,\n    # we use the specified chunks\n    if var_chunks is None:\n        return enc_chunks_tuple\n\n    # the hard case\n    # DESIGN CHOICE: do not allow multiple dask chunks on a single zarr chunk\n    # this avoids the need to get involved in zarr synchronization / locking\n    # From zarr docs:\n    #  ""If each worker in a parallel computation is writing to a separate\n    #   region of the array, and if region boundaries are perfectly aligned\n    #   with chunk boundaries, then no synchronization is required.""\n    # TODO: incorporate synchronizer to allow writes from multiple dask\n    # threads\n    if var_chunks and enc_chunks_tuple:\n        for zchunk, dchunks in zip(enc_chunks_tuple, var_chunks):\n            for dchunk in dchunks[:-1]:\n                if dchunk % zchunk:\n                    raise NotImplementedError(\n                        f""Specified zarr chunks encoding[\'chunks\']={enc_chunks_tuple!r} for ""\n                        f""variable named {name!r} would overlap multiple dask chunks {var_chunks!r}. ""\n                        ""This is not implemented in xarray yet. ""\n                        ""Consider either rechunking using `chunk()` or instead deleting ""\n                        ""or modifying `encoding[\'chunks\']`.""\n                    )\n            if dchunks[-1] > zchunk:\n                raise ValueError(\n                    ""Final chunk of Zarr array must be the same size or ""\n                    ""smaller than the first. ""\n                    f""Specified Zarr chunk encoding[\'chunks\']={enc_chunks_tuple}, ""\n                    f""for variable named {name!r} ""\n                    f""but {dchunks} in the variable\'s Dask chunks {var_chunks} is ""\n                    ""incompatible with this encoding. ""\n                    ""Consider either rechunking using `chunk()` or instead deleting ""\n                    ""or modifying `encoding[\'chunks\']`.""\n                )\n        return enc_chunks_tuple\n\n    raise AssertionError(""We should never get here. Function logic must be wrong."")\n\n\ndef _get_zarr_dims_and_attrs(zarr_obj, dimension_key):\n    # Zarr arrays do not have dimenions. To get around this problem, we add\n    # an attribute that specifies the dimension. We have to hide this attribute\n    # when we send the attributes to the user.\n    # zarr_obj can be either a zarr group or zarr array\n    try:\n        dimensions = zarr_obj.attrs[dimension_key]\n    except KeyError:\n        raise KeyError(\n            ""Zarr object is missing the attribute `%s`, which is ""\n            ""required for xarray to determine variable dimensions."" % (dimension_key)\n        )\n    attributes = HiddenKeyDict(zarr_obj.attrs, [dimension_key])\n    return dimensions, attributes\n\n\ndef extract_zarr_variable_encoding(variable, raise_on_invalid=False, name=None):\n    """"""\n    Extract zarr encoding dictionary from xarray Variable\n\n    Parameters\n    ----------\n    variable : xarray.Variable\n    raise_on_invalid : bool, optional\n\n    Returns\n    -------\n    encoding : dict\n        Zarr encoding for `variable`\n    """"""\n    encoding = variable.encoding.copy()\n\n    valid_encodings = {""chunks"", ""compressor"", ""filters"", ""cache_metadata""}\n\n    if raise_on_invalid:\n        invalid = [k for k in encoding if k not in valid_encodings]\n        if invalid:\n            raise ValueError(\n                ""unexpected encoding parameters for zarr "" ""backend:  %r"" % invalid\n            )\n    else:\n        for k in list(encoding):\n            if k not in valid_encodings:\n                del encoding[k]\n\n    chunks = _determine_zarr_chunks(\n        encoding.get(""chunks""), variable.chunks, variable.ndim, name\n    )\n    encoding[""chunks""] = chunks\n    return encoding\n\n\n# Function below is copied from conventions.encode_cf_variable.\n# The only change is to raise an error for object dtypes.\ndef encode_zarr_variable(var, needs_copy=True, name=None):\n    """"""\n    Converts an Variable into an Variable which follows some\n    of the CF conventions:\n\n        - Nans are masked using _FillValue (or the deprecated missing_value)\n        - Rescaling via: scale_factor and add_offset\n        - datetimes are converted to the CF \'units since time\' format\n        - dtype encodings are enforced.\n\n    Parameters\n    ----------\n    var : xarray.Variable\n        A variable holding un-encoded data.\n\n    Returns\n    -------\n    out : xarray.Variable\n        A variable which has been encoded as described above.\n    """"""\n\n    var = conventions.encode_cf_variable(var, name=name)\n\n    # zarr allows unicode, but not variable-length strings, so it\'s both\n    # simpler and more compact to always encode as UTF-8 explicitly.\n    # TODO: allow toggling this explicitly via dtype in encoding.\n    coder = coding.strings.EncodedStringCoder(allows_unicode=True)\n    var = coder.encode(var, name=name)\n    var = coding.strings.ensure_fixed_length_bytes(var)\n\n    return var\n\n\nclass ZarrStore(AbstractWritableDataStore):\n    """"""Store for reading and writing data via zarr\n    """"""\n\n    __slots__ = (\n        ""append_dim"",\n        ""ds"",\n        ""_consolidate_on_close"",\n        ""_group"",\n        ""_read_only"",\n        ""_synchronizer"",\n    )\n\n    @classmethod\n    def open_group(\n        cls,\n        store,\n        mode=""r"",\n        synchronizer=None,\n        group=None,\n        consolidated=False,\n        consolidate_on_close=False,\n    ):\n        import zarr\n\n        open_kwargs = dict(mode=mode, synchronizer=synchronizer, path=group)\n        if consolidated:\n            # TODO: an option to pass the metadata_key keyword\n            zarr_group = zarr.open_consolidated(store, **open_kwargs)\n        else:\n            zarr_group = zarr.open_group(store, **open_kwargs)\n        return cls(zarr_group, consolidate_on_close)\n\n    def __init__(self, zarr_group, consolidate_on_close=False):\n        self.ds = zarr_group\n        self._read_only = self.ds.read_only\n        self._synchronizer = self.ds.synchronizer\n        self._group = self.ds.path\n        self._consolidate_on_close = consolidate_on_close\n        self.append_dim = None\n\n    def open_store_variable(self, name, zarr_array):\n        data = indexing.LazilyOuterIndexedArray(ZarrArrayWrapper(name, self))\n        dimensions, attributes = _get_zarr_dims_and_attrs(zarr_array, DIMENSION_KEY)\n        attributes = dict(attributes)\n        encoding = {\n            ""chunks"": zarr_array.chunks,\n            ""compressor"": zarr_array.compressor,\n            ""filters"": zarr_array.filters,\n        }\n        # _FillValue needs to be in attributes, not encoding, so it will get\n        # picked up by decode_cf\n        if getattr(zarr_array, ""fill_value"") is not None:\n            attributes[""_FillValue""] = zarr_array.fill_value\n\n        return Variable(dimensions, data, attributes, encoding)\n\n    def get_variables(self):\n        return FrozenDict(\n            (k, self.open_store_variable(k, v)) for k, v in self.ds.arrays()\n        )\n\n    def get_attrs(self):\n        attributes = dict(self.ds.attrs.asdict())\n        return attributes\n\n    def get_dimensions(self):\n        dimensions = {}\n        for k, v in self.ds.arrays():\n            try:\n                for d, s in zip(v.attrs[DIMENSION_KEY], v.shape):\n                    if d in dimensions and dimensions[d] != s:\n                        raise ValueError(\n                            ""found conflicting lengths for dimension %s ""\n                            ""(%d != %d)"" % (d, s, dimensions[d])\n                        )\n                    dimensions[d] = s\n\n            except KeyError:\n                raise KeyError(\n                    ""Zarr object is missing the attribute `%s`, ""\n                    ""which is required for xarray to determine ""\n                    ""variable dimensions."" % (DIMENSION_KEY)\n                )\n        return dimensions\n\n    def set_dimensions(self, variables, unlimited_dims=None):\n        if unlimited_dims is not None:\n            raise NotImplementedError(\n                ""Zarr backend doesn\'t know how to handle unlimited dimensions""\n            )\n\n    def set_attributes(self, attributes):\n        self.ds.attrs.put(attributes)\n\n    def encode_variable(self, variable):\n        variable = encode_zarr_variable(variable)\n        return variable\n\n    def encode_attribute(self, a):\n        return encode_zarr_attr_value(a)\n\n    def store(\n        self,\n        variables,\n        attributes,\n        check_encoding_set=frozenset(),\n        writer=None,\n        unlimited_dims=None,\n    ):\n        """"""\n        Top level method for putting data on this store, this method:\n          - encodes variables/attributes\n          - sets dimensions\n          - sets variables\n\n        Parameters\n        ----------\n        variables : dict-like\n            Dictionary of key/value (variable name / xr.Variable) pairs\n        attributes : dict-like\n            Dictionary of key/value (attribute name / attribute) pairs\n        check_encoding_set : list-like\n            List of variables that should be checked for invalid encoding\n            values\n        writer : ArrayWriter\n        unlimited_dims : list-like\n            List of dimension names that should be treated as unlimited\n            dimensions.\n            dimension on which the zarray will be appended\n            only needed in append mode\n        """"""\n\n        existing_variables = {\n            vn for vn in variables if _encode_variable_name(vn) in self.ds\n        }\n        new_variables = set(variables) - existing_variables\n        variables_without_encoding = {vn: variables[vn] for vn in new_variables}\n        variables_encoded, attributes = self.encode(\n            variables_without_encoding, attributes\n        )\n\n        if len(existing_variables) > 0:\n            # there are variables to append\n            # their encoding must be the same as in the store\n            ds = open_zarr(self.ds.store, group=self.ds.path, chunks=None)\n            variables_with_encoding = {}\n            for vn in existing_variables:\n                variables_with_encoding[vn] = variables[vn].copy(deep=False)\n                variables_with_encoding[vn].encoding = ds[vn].encoding\n            variables_with_encoding, _ = self.encode(variables_with_encoding, {})\n            variables_encoded.update(variables_with_encoding)\n\n        self.set_attributes(attributes)\n        self.set_dimensions(variables_encoded, unlimited_dims=unlimited_dims)\n        self.set_variables(\n            variables_encoded, check_encoding_set, writer, unlimited_dims=unlimited_dims\n        )\n\n    def sync(self):\n        pass\n\n    def set_variables(self, variables, check_encoding_set, writer, unlimited_dims=None):\n        """"""\n        This provides a centralized method to set the variables on the data\n        store.\n\n        Parameters\n        ----------\n        variables : dict-like\n            Dictionary of key/value (variable name / xr.Variable) pairs\n        check_encoding_set : list-like\n            List of variables that should be checked for invalid encoding\n            values\n        writer :\n        unlimited_dims : list-like\n            List of dimension names that should be treated as unlimited\n            dimensions.\n        """"""\n\n        for vn, v in variables.items():\n            name = _encode_variable_name(vn)\n            check = vn in check_encoding_set\n            attrs = v.attrs.copy()\n            dims = v.dims\n            dtype = v.dtype\n            shape = v.shape\n\n            fill_value = attrs.pop(""_FillValue"", None)\n            if v.encoding == {""_FillValue"": None} and fill_value is None:\n                v.encoding = {}\n\n            if self.append_dim is not None and self.append_dim in dims:\n                # resize existing variable\n                zarr_array = self.ds[name]\n                append_axis = dims.index(self.append_dim)\n\n                new_region = [slice(None)] * len(dims)\n                new_region[append_axis] = slice(zarr_array.shape[append_axis], None)\n                region = tuple(new_region)\n\n                new_shape = list(zarr_array.shape)\n                new_shape[append_axis] += v.shape[append_axis]\n                zarr_array.resize(new_shape)\n            elif name in self.ds:\n                # override existing variable\n                zarr_array = self.ds[name]\n                region = None\n            else:\n                # new variable\n                encoding = extract_zarr_variable_encoding(\n                    v, raise_on_invalid=check, name=vn\n                )\n                encoded_attrs = {}\n                # the magic for storing the hidden dimension data\n                encoded_attrs[DIMENSION_KEY] = dims\n                for k2, v2 in attrs.items():\n                    encoded_attrs[k2] = self.encode_attribute(v2)\n\n                if coding.strings.check_vlen_dtype(dtype) == str:\n                    dtype = str\n                zarr_array = self.ds.create(\n                    name, shape=shape, dtype=dtype, fill_value=fill_value, **encoding\n                )\n                zarr_array.attrs.put(encoded_attrs)\n                region = None\n\n            writer.add(v.data, zarr_array, region=region)\n\n    def close(self):\n        if self._consolidate_on_close:\n            import zarr\n\n            zarr.consolidate_metadata(self.ds.store)\n\n\ndef open_zarr(\n    store,\n    group=None,\n    synchronizer=None,\n    chunks=""auto"",\n    decode_cf=True,\n    mask_and_scale=True,\n    decode_times=True,\n    concat_characters=True,\n    decode_coords=True,\n    drop_variables=None,\n    consolidated=False,\n    overwrite_encoded_chunks=False,\n    decode_timedelta=None,\n    **kwargs,\n):\n    """"""Load and decode a dataset from a Zarr store.\n\n    .. note:: Experimental\n              The Zarr backend is new and experimental. Please report any\n              unexpected behavior via github issues.\n\n    The `store` object should be a valid store for a Zarr group. `store`\n    variables must contain dimension metadata encoded in the\n    `_ARRAY_DIMENSIONS` attribute.\n\n    Parameters\n    ----------\n    store : MutableMapping or str\n        A MutableMapping where a Zarr Group has been stored or a path to a\n        directory in file system where a Zarr DirectoryStore has been stored.\n    synchronizer : object, optional\n        Array synchronizer provided to zarr\n    group : str, optional\n        Group path. (a.k.a. `path` in zarr terminology.)\n    chunks : int or dict or tuple or {None, \'auto\'}, optional\n        Chunk sizes along each dimension, e.g., ``5`` or\n        ``{\'x\': 5, \'y\': 5}``. If `chunks=\'auto\'`, dask chunks are created\n        based on the variable\'s zarr chunks. If `chunks=None`, zarr array\n        data will lazily convert to numpy arrays upon access. This accepts\n        all the chunk specifications as Dask does.\n    overwrite_encoded_chunks: bool, optional\n        Whether to drop the zarr chunks encoded for each variable when a\n        dataset is loaded with specified chunk sizes (default: False)\n    decode_cf : bool, optional\n        Whether to decode these variables, assuming they were saved according\n        to CF conventions.\n    mask_and_scale : bool, optional\n        If True, replace array values equal to `_FillValue` with NA and scale\n        values according to the formula `original_values * scale_factor +\n        add_offset`, where `_FillValue`, `scale_factor` and `add_offset` are\n        taken from variable attributes (if they exist).  If the `_FillValue` or\n        `missing_value` attribute contains multiple values a warning will be\n        issued and all array values matching one of the multiple values will\n        be replaced by NA.\n    decode_times : bool, optional\n        If True, decode times encoded in the standard NetCDF datetime format\n        into datetime objects. Otherwise, leave them encoded as numbers.\n    concat_characters : bool, optional\n        If True, concatenate along the last dimension of character arrays to\n        form string arrays. Dimensions will only be concatenated over (and\n        removed) if they have no corresponding variable and if they are only\n        used as the last dimension of character arrays.\n    decode_coords : bool, optional\n        If True, decode the \'coordinates\' attribute to identify coordinates in\n        the resulting dataset.\n    drop_variables : string or iterable, optional\n        A variable or list of variables to exclude from being parsed from the\n        dataset. This may be useful to drop variables with problems or\n        inconsistent values.\n    consolidated : bool, optional\n        Whether to open the store using zarr\'s consolidated metadata\n        capability. Only works for stores that have already been consolidated.\n    decode_timedelta : bool, optional\n        If True, decode variables and coordinates with time units in\n        {\'days\', \'hours\', \'minutes\', \'seconds\', \'milliseconds\', \'microseconds\'}\n        into timedelta objects. If False, leave them encoded as numbers.\n        If None (default), assume the same value of decode_time.\n\n    Returns\n    -------\n    dataset : Dataset\n        The newly created dataset.\n\n    See Also\n    --------\n    open_dataset\n\n    References\n    ----------\n    http://zarr.readthedocs.io/\n    """"""\n    if ""auto_chunk"" in kwargs:\n        auto_chunk = kwargs.pop(""auto_chunk"")\n        if auto_chunk:\n            chunks = ""auto""  # maintain backwards compatibility\n        else:\n            chunks = None\n\n        warnings.warn(\n            ""auto_chunk is deprecated. Use chunks=\'auto\' instead."",\n            FutureWarning,\n            stacklevel=2,\n        )\n\n    if kwargs:\n        raise TypeError(\n            ""open_zarr() got unexpected keyword arguments "" + "","".join(kwargs.keys())\n        )\n\n    if not isinstance(chunks, (int, dict)):\n        if chunks != ""auto"" and chunks is not None:\n            raise ValueError(\n                ""chunks must be an int, dict, \'auto\', or None. ""\n                ""Instead found %s. "" % chunks\n            )\n\n    if chunks == ""auto"":\n        try:\n            import dask.array  # noqa\n        except ImportError:\n            chunks = None\n\n    if not decode_cf:\n        mask_and_scale = False\n        decode_times = False\n        concat_characters = False\n        decode_coords = False\n        decode_timedelta = False\n\n    def maybe_decode_store(store, lock=False):\n        ds = conventions.decode_cf(\n            store,\n            mask_and_scale=mask_and_scale,\n            decode_times=decode_times,\n            concat_characters=concat_characters,\n            decode_coords=decode_coords,\n            drop_variables=drop_variables,\n            decode_timedelta=decode_timedelta,\n        )\n\n        # TODO: this is where we would apply caching\n\n        return ds\n\n    # Zarr supports a wide range of access modes, but for now xarray either\n    # reads or writes from a store, never both. For open_zarr, we only read\n    mode = ""r""\n    zarr_store = ZarrStore.open_group(\n        store,\n        mode=mode,\n        synchronizer=synchronizer,\n        group=group,\n        consolidated=consolidated,\n    )\n    ds = maybe_decode_store(zarr_store)\n\n    # auto chunking needs to be here and not in ZarrStore because variable\n    # chunks do not survive decode_cf\n    # return trivial case\n    if not chunks:\n        return ds\n\n    # adapted from Dataset.Chunk()\n    if isinstance(chunks, int):\n        chunks = dict.fromkeys(ds.dims, chunks)\n\n    if isinstance(chunks, tuple) and len(chunks) == len(ds.dims):\n        chunks = dict(zip(ds.dims, chunks))\n\n    def get_chunk(name, var, chunks):\n        chunk_spec = dict(zip(var.dims, var.encoding.get(""chunks"")))\n\n        # Coordinate labels aren\'t chunked\n        if var.ndim == 1 and var.dims[0] == name:\n            return chunk_spec\n\n        if chunks == ""auto"":\n            return chunk_spec\n\n        for dim in var.dims:\n            if dim in chunks:\n                spec = chunks[dim]\n                if isinstance(spec, int):\n                    spec = (spec,)\n                if isinstance(spec, (tuple, list)) and chunk_spec[dim]:\n                    if any(s % chunk_spec[dim] for s in spec):\n                        warnings.warn(\n                            ""Specified Dask chunks %r would ""\n                            ""separate Zarr chunk shape %r for ""\n                            ""dimension %r. This significantly ""\n                            ""degrades performance. Consider ""\n                            ""rechunking after loading instead.""\n                            % (chunks[dim], chunk_spec[dim], dim),\n                            stacklevel=2,\n                        )\n                chunk_spec[dim] = chunks[dim]\n        return chunk_spec\n\n    def maybe_chunk(name, var, chunks):\n        from dask.base import tokenize\n\n        chunk_spec = get_chunk(name, var, chunks)\n\n        if (var.ndim > 0) and (chunk_spec is not None):\n            # does this cause any data to be read?\n            token2 = tokenize(name, var._data)\n            name2 = ""zarr-%s"" % token2\n            var = var.chunk(chunk_spec, name=name2, lock=None)\n            if overwrite_encoded_chunks and var.chunks is not None:\n                var.encoding[""chunks""] = tuple(x[0] for x in var.chunks)\n            return var\n        else:\n            return var\n\n    variables = {k: maybe_chunk(k, v, chunks) for k, v in ds.variables.items()}\n    return ds._replace_vars_and_dims(variables)\n'"
xarray/coding/__init__.py,0,b''
xarray/coding/cftime_offsets.py,2,"b'""""""Time offset classes for use with cftime.datetime objects""""""\n# The offset classes and mechanisms for generating time ranges defined in\n# this module were copied/adapted from those defined in pandas.  See in\n# particular the objects and methods defined in pandas.tseries.offsets\n# and pandas.core.indexes.datetimes.\n\n# For reference, here is a copy of the pandas copyright notice:\n\n# (c) 2011-2012, Lambda Foundry, Inc. and PyData Development Team\n# All rights reserved.\n\n# Copyright (c) 2008-2011 AQR Capital Management, LLC\n# All rights reserved.\n\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n\n#     * Redistributions of source code must retain the above copyright\n#        notice, this list of conditions and the following disclaimer.\n\n#     * Redistributions in binary form must reproduce the above\n#        copyright notice, this list of conditions and the following\n#        disclaimer in the documentation and/or other materials provided\n#        with the distribution.\n\n#     * Neither the name of the copyright holder nor the names of any\n#        contributors may be used to endorse or promote products derived\n#        from this software without specific prior written permission.\n\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER AND CONTRIBUTORS\n# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport re\nfrom datetime import timedelta\nfrom distutils.version import LooseVersion\nfrom functools import partial\nfrom typing import ClassVar, Optional\n\nimport numpy as np\n\nfrom ..core.pdcompat import count_not_none\nfrom .cftimeindex import CFTimeIndex, _parse_iso8601_with_reso\nfrom .times import format_cftime_datetime\n\n\ndef get_date_type(calendar):\n    """"""Return the cftime date type for a given calendar name.""""""\n    try:\n        import cftime\n    except ImportError:\n        raise ImportError(""cftime is required for dates with non-standard calendars"")\n    else:\n        calendars = {\n            ""noleap"": cftime.DatetimeNoLeap,\n            ""360_day"": cftime.Datetime360Day,\n            ""365_day"": cftime.DatetimeNoLeap,\n            ""366_day"": cftime.DatetimeAllLeap,\n            ""gregorian"": cftime.DatetimeGregorian,\n            ""proleptic_gregorian"": cftime.DatetimeProlepticGregorian,\n            ""julian"": cftime.DatetimeJulian,\n            ""all_leap"": cftime.DatetimeAllLeap,\n            ""standard"": cftime.DatetimeGregorian,\n        }\n        return calendars[calendar]\n\n\nclass BaseCFTimeOffset:\n    _freq: ClassVar[Optional[str]] = None\n    _day_option: ClassVar[Optional[str]] = None\n\n    def __init__(self, n=1):\n        if not isinstance(n, int):\n            raise TypeError(\n                ""The provided multiple \'n\' must be an integer. ""\n                ""Instead a value of type {!r} was provided."".format(type(n))\n            )\n        self.n = n\n\n    def rule_code(self):\n        return self._freq\n\n    def __eq__(self, other):\n        return self.n == other.n and self.rule_code() == other.rule_code()\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __add__(self, other):\n        return self.__apply__(other)\n\n    def __sub__(self, other):\n        import cftime\n\n        if isinstance(other, cftime.datetime):\n            raise TypeError(""Cannot subtract a cftime.datetime "" ""from a time offset."")\n        elif type(other) == type(self):\n            return type(self)(self.n - other.n)\n        else:\n            return NotImplemented\n\n    def __mul__(self, other):\n        return type(self)(n=other * self.n)\n\n    def __neg__(self):\n        return self * -1\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    def __rsub__(self, other):\n        if isinstance(other, BaseCFTimeOffset) and type(self) != type(other):\n            raise TypeError(""Cannot subtract cftime offsets of differing "" ""types"")\n        return -self + other\n\n    def __apply__(self):\n        return NotImplemented\n\n    def onOffset(self, date):\n        """"""Check if the given date is in the set of possible dates created\n        using a length-one version of this offset class.""""""\n        test_date = (self + date) - self\n        return date == test_date\n\n    def rollforward(self, date):\n        if self.onOffset(date):\n            return date\n        else:\n            return date + type(self)()\n\n    def rollback(self, date):\n        if self.onOffset(date):\n            return date\n        else:\n            return date - type(self)()\n\n    def __str__(self):\n        return ""<{}: n={}>"".format(type(self).__name__, self.n)\n\n    def __repr__(self):\n        return str(self)\n\n    def _get_offset_day(self, other):\n        # subclass must implement `_day_option`; calling from the base class\n        # will raise NotImplementedError.\n        return _get_day_of_month(other, self._day_option)\n\n\ndef _get_day_of_month(other, day_option):\n    """"""Find the day in `other`\'s month that satisfies a BaseCFTimeOffset\'s\n    onOffset policy, as described by the `day_option` argument.\n\n    Parameters\n    ----------\n    other : cftime.datetime\n    day_option : \'start\', \'end\'\n        \'start\': returns 1\n        \'end\': returns last day of the month\n\n    Returns\n    -------\n    day_of_month : int\n\n    """"""\n\n    if day_option == ""start"":\n        return 1\n    elif day_option == ""end"":\n        days_in_month = _days_in_month(other)\n        return days_in_month\n    elif day_option is None:\n        # Note: unlike `_shift_month`, _get_day_of_month does not\n        # allow day_option = None\n        raise NotImplementedError()\n    else:\n        raise ValueError(day_option)\n\n\ndef _days_in_month(date):\n    """"""The number of days in the month of the given date""""""\n    if date.month == 12:\n        reference = type(date)(date.year + 1, 1, 1)\n    else:\n        reference = type(date)(date.year, date.month + 1, 1)\n    return (reference - timedelta(days=1)).day\n\n\ndef _adjust_n_months(other_day, n, reference_day):\n    """"""Adjust the number of times a monthly offset is applied based\n    on the day of a given date, and the reference day provided.\n    """"""\n    if n > 0 and other_day < reference_day:\n        n = n - 1\n    elif n <= 0 and other_day > reference_day:\n        n = n + 1\n    return n\n\n\ndef _adjust_n_years(other, n, month, reference_day):\n    """"""Adjust the number of times an annual offset is applied based on\n    another date, and the reference day provided""""""\n    if n > 0:\n        if other.month < month or (other.month == month and other.day < reference_day):\n            n -= 1\n    else:\n        if other.month > month or (other.month == month and other.day > reference_day):\n            n += 1\n    return n\n\n\ndef _shift_month(date, months, day_option=""start""):\n    """"""Shift the date to a month start or end a given number of months away.\n    """"""\n    import cftime\n\n    delta_year = (date.month + months) // 12\n    month = (date.month + months) % 12\n\n    if month == 0:\n        month = 12\n        delta_year = delta_year - 1\n    year = date.year + delta_year\n\n    if day_option == ""start"":\n        day = 1\n    elif day_option == ""end"":\n        reference = type(date)(year, month, 1)\n        day = _days_in_month(reference)\n    else:\n        raise ValueError(day_option)\n    if LooseVersion(cftime.__version__) < LooseVersion(""1.0.4""):\n        # dayofwk=-1 is required to update the dayofwk and dayofyr attributes of\n        # the returned date object in versions of cftime between 1.0.2 and\n        # 1.0.3.4.  It can be removed for versions of cftime greater than\n        # 1.0.3.4.\n        return date.replace(year=year, month=month, day=day, dayofwk=-1)\n    else:\n        return date.replace(year=year, month=month, day=day)\n\n\ndef roll_qtrday(other, n, month, day_option, modby=3):\n    """"""Possibly increment or decrement the number of periods to shift\n    based on rollforward/rollbackward conventions.\n\n    Parameters\n    ----------\n    other : cftime.datetime\n    n : number of periods to increment, before adjusting for rolling\n    month : int reference month giving the first month of the year\n    day_option : \'start\', \'end\'\n        The convention to use in finding the day in a given month against\n        which to compare for rollforward/rollbackward decisions.\n    modby : int 3 for quarters, 12 for years\n\n    Returns\n    -------\n    n : int number of periods to increment\n\n    See Also\n    --------\n    _get_day_of_month : Find the day in a month provided an offset.\n    """"""\n\n    months_since = other.month % modby - month % modby\n\n    if n > 0:\n        if months_since < 0 or (\n            months_since == 0 and other.day < _get_day_of_month(other, day_option)\n        ):\n            # pretend to roll back if on same month but\n            # before compare_day\n            n -= 1\n    else:\n        if months_since > 0 or (\n            months_since == 0 and other.day > _get_day_of_month(other, day_option)\n        ):\n            # make sure to roll forward, so negate\n            n += 1\n    return n\n\n\ndef _validate_month(month, default_month):\n    if month is None:\n        result_month = default_month\n    else:\n        result_month = month\n    if not isinstance(result_month, int):\n        raise TypeError(\n            ""\'self.month\' must be an integer value between 1 ""\n            ""and 12.  Instead, it was set to a value of ""\n            ""{!r}"".format(result_month)\n        )\n    elif not (1 <= result_month <= 12):\n        raise ValueError(\n            ""\'self.month\' must be an integer value between 1 ""\n            ""and 12.  Instead, it was set to a value of ""\n            ""{!r}"".format(result_month)\n        )\n    return result_month\n\n\nclass MonthBegin(BaseCFTimeOffset):\n    _freq = ""MS""\n\n    def __apply__(self, other):\n        n = _adjust_n_months(other.day, self.n, 1)\n        return _shift_month(other, n, ""start"")\n\n    def onOffset(self, date):\n        """"""Check if the given date is in the set of possible dates created\n        using a length-one version of this offset class.""""""\n        return date.day == 1\n\n\nclass MonthEnd(BaseCFTimeOffset):\n    _freq = ""M""\n\n    def __apply__(self, other):\n        n = _adjust_n_months(other.day, self.n, _days_in_month(other))\n        return _shift_month(other, n, ""end"")\n\n    def onOffset(self, date):\n        """"""Check if the given date is in the set of possible dates created\n        using a length-one version of this offset class.""""""\n        return date.day == _days_in_month(date)\n\n\n_MONTH_ABBREVIATIONS = {\n    1: ""JAN"",\n    2: ""FEB"",\n    3: ""MAR"",\n    4: ""APR"",\n    5: ""MAY"",\n    6: ""JUN"",\n    7: ""JUL"",\n    8: ""AUG"",\n    9: ""SEP"",\n    10: ""OCT"",\n    11: ""NOV"",\n    12: ""DEC"",\n}\n\n\nclass QuarterOffset(BaseCFTimeOffset):\n    """"""Quarter representation copied off of pandas/tseries/offsets.py\n    """"""\n\n    _freq: ClassVar[str]\n    _default_month: ClassVar[int]\n\n    def __init__(self, n=1, month=None):\n        BaseCFTimeOffset.__init__(self, n)\n        self.month = _validate_month(month, self._default_month)\n\n    def __apply__(self, other):\n        # months_since: find the calendar quarter containing other.month,\n        # e.g. if other.month == 8, the calendar quarter is [Jul, Aug, Sep].\n        # Then find the month in that quarter containing an onOffset date for\n        # self.  `months_since` is the number of months to shift other.month\n        # to get to this on-offset month.\n        months_since = other.month % 3 - self.month % 3\n        qtrs = roll_qtrday(\n            other, self.n, self.month, day_option=self._day_option, modby=3\n        )\n        months = qtrs * 3 - months_since\n        return _shift_month(other, months, self._day_option)\n\n    def onOffset(self, date):\n        """"""Check if the given date is in the set of possible dates created\n        using a length-one version of this offset class.""""""\n        mod_month = (date.month - self.month) % 3\n        return mod_month == 0 and date.day == self._get_offset_day(date)\n\n    def __sub__(self, other):\n        import cftime\n\n        if isinstance(other, cftime.datetime):\n            raise TypeError(""Cannot subtract cftime.datetime from offset."")\n        elif type(other) == type(self) and other.month == self.month:\n            return type(self)(self.n - other.n, month=self.month)\n        else:\n            return NotImplemented\n\n    def __mul__(self, other):\n        return type(self)(n=other * self.n, month=self.month)\n\n    def rule_code(self):\n        return ""{}-{}"".format(self._freq, _MONTH_ABBREVIATIONS[self.month])\n\n    def __str__(self):\n        return ""<{}: n={}, month={}>"".format(type(self).__name__, self.n, self.month)\n\n\nclass QuarterBegin(QuarterOffset):\n    # When converting a string to an offset, pandas converts\n    # \'QS\' to a QuarterBegin offset starting in the month of\n    # January.  When creating a QuarterBegin offset directly\n    # from the constructor, however, the default month is March.\n    # We follow that behavior here.\n    _default_month = 3\n    _freq = ""QS""\n    _day_option = ""start""\n\n    def rollforward(self, date):\n        """"""Roll date forward to nearest start of quarter""""""\n        if self.onOffset(date):\n            return date\n        else:\n            return date + QuarterBegin(month=self.month)\n\n    def rollback(self, date):\n        """"""Roll date backward to nearest start of quarter""""""\n        if self.onOffset(date):\n            return date\n        else:\n            return date - QuarterBegin(month=self.month)\n\n\nclass QuarterEnd(QuarterOffset):\n    # When converting a string to an offset, pandas converts\n    # \'Q\' to a QuarterEnd offset starting in the month of\n    # December.  When creating a QuarterEnd offset directly\n    # from the constructor, however, the default month is March.\n    # We follow that behavior here.\n    _default_month = 3\n    _freq = ""Q""\n    _day_option = ""end""\n\n    def rollforward(self, date):\n        """"""Roll date forward to nearest end of quarter""""""\n        if self.onOffset(date):\n            return date\n        else:\n            return date + QuarterEnd(month=self.month)\n\n    def rollback(self, date):\n        """"""Roll date backward to nearest end of quarter""""""\n        if self.onOffset(date):\n            return date\n        else:\n            return date - QuarterEnd(month=self.month)\n\n\nclass YearOffset(BaseCFTimeOffset):\n    _freq: ClassVar[str]\n    _day_option: ClassVar[str]\n    _default_month: ClassVar[int]\n\n    def __init__(self, n=1, month=None):\n        BaseCFTimeOffset.__init__(self, n)\n        self.month = _validate_month(month, self._default_month)\n\n    def __apply__(self, other):\n        reference_day = _get_day_of_month(other, self._day_option)\n        years = _adjust_n_years(other, self.n, self.month, reference_day)\n        months = years * 12 + (self.month - other.month)\n        return _shift_month(other, months, self._day_option)\n\n    def __sub__(self, other):\n        import cftime\n\n        if isinstance(other, cftime.datetime):\n            raise TypeError(""Cannot subtract cftime.datetime from offset."")\n        elif type(other) == type(self) and other.month == self.month:\n            return type(self)(self.n - other.n, month=self.month)\n        else:\n            return NotImplemented\n\n    def __mul__(self, other):\n        return type(self)(n=other * self.n, month=self.month)\n\n    def rule_code(self):\n        return ""{}-{}"".format(self._freq, _MONTH_ABBREVIATIONS[self.month])\n\n    def __str__(self):\n        return ""<{}: n={}, month={}>"".format(type(self).__name__, self.n, self.month)\n\n\nclass YearBegin(YearOffset):\n    _freq = ""AS""\n    _day_option = ""start""\n    _default_month = 1\n\n    def onOffset(self, date):\n        """"""Check if the given date is in the set of possible dates created\n        using a length-one version of this offset class.""""""\n        return date.day == 1 and date.month == self.month\n\n    def rollforward(self, date):\n        """"""Roll date forward to nearest start of year""""""\n        if self.onOffset(date):\n            return date\n        else:\n            return date + YearBegin(month=self.month)\n\n    def rollback(self, date):\n        """"""Roll date backward to nearest start of year""""""\n        if self.onOffset(date):\n            return date\n        else:\n            return date - YearBegin(month=self.month)\n\n\nclass YearEnd(YearOffset):\n    _freq = ""A""\n    _day_option = ""end""\n    _default_month = 12\n\n    def onOffset(self, date):\n        """"""Check if the given date is in the set of possible dates created\n        using a length-one version of this offset class.""""""\n        return date.day == _days_in_month(date) and date.month == self.month\n\n    def rollforward(self, date):\n        """"""Roll date forward to nearest end of year""""""\n        if self.onOffset(date):\n            return date\n        else:\n            return date + YearEnd(month=self.month)\n\n    def rollback(self, date):\n        """"""Roll date backward to nearest end of year""""""\n        if self.onOffset(date):\n            return date\n        else:\n            return date - YearEnd(month=self.month)\n\n\nclass Day(BaseCFTimeOffset):\n    _freq = ""D""\n\n    def as_timedelta(self):\n        return timedelta(days=self.n)\n\n    def __apply__(self, other):\n        return other + self.as_timedelta()\n\n\nclass Hour(BaseCFTimeOffset):\n    _freq = ""H""\n\n    def as_timedelta(self):\n        return timedelta(hours=self.n)\n\n    def __apply__(self, other):\n        return other + self.as_timedelta()\n\n\nclass Minute(BaseCFTimeOffset):\n    _freq = ""T""\n\n    def as_timedelta(self):\n        return timedelta(minutes=self.n)\n\n    def __apply__(self, other):\n        return other + self.as_timedelta()\n\n\nclass Second(BaseCFTimeOffset):\n    _freq = ""S""\n\n    def as_timedelta(self):\n        return timedelta(seconds=self.n)\n\n    def __apply__(self, other):\n        return other + self.as_timedelta()\n\n\n_FREQUENCIES = {\n    ""A"": YearEnd,\n    ""AS"": YearBegin,\n    ""Y"": YearEnd,\n    ""YS"": YearBegin,\n    ""Q"": partial(QuarterEnd, month=12),\n    ""QS"": partial(QuarterBegin, month=1),\n    ""M"": MonthEnd,\n    ""MS"": MonthBegin,\n    ""D"": Day,\n    ""H"": Hour,\n    ""T"": Minute,\n    ""min"": Minute,\n    ""S"": Second,\n    ""AS-JAN"": partial(YearBegin, month=1),\n    ""AS-FEB"": partial(YearBegin, month=2),\n    ""AS-MAR"": partial(YearBegin, month=3),\n    ""AS-APR"": partial(YearBegin, month=4),\n    ""AS-MAY"": partial(YearBegin, month=5),\n    ""AS-JUN"": partial(YearBegin, month=6),\n    ""AS-JUL"": partial(YearBegin, month=7),\n    ""AS-AUG"": partial(YearBegin, month=8),\n    ""AS-SEP"": partial(YearBegin, month=9),\n    ""AS-OCT"": partial(YearBegin, month=10),\n    ""AS-NOV"": partial(YearBegin, month=11),\n    ""AS-DEC"": partial(YearBegin, month=12),\n    ""A-JAN"": partial(YearEnd, month=1),\n    ""A-FEB"": partial(YearEnd, month=2),\n    ""A-MAR"": partial(YearEnd, month=3),\n    ""A-APR"": partial(YearEnd, month=4),\n    ""A-MAY"": partial(YearEnd, month=5),\n    ""A-JUN"": partial(YearEnd, month=6),\n    ""A-JUL"": partial(YearEnd, month=7),\n    ""A-AUG"": partial(YearEnd, month=8),\n    ""A-SEP"": partial(YearEnd, month=9),\n    ""A-OCT"": partial(YearEnd, month=10),\n    ""A-NOV"": partial(YearEnd, month=11),\n    ""A-DEC"": partial(YearEnd, month=12),\n    ""QS-JAN"": partial(QuarterBegin, month=1),\n    ""QS-FEB"": partial(QuarterBegin, month=2),\n    ""QS-MAR"": partial(QuarterBegin, month=3),\n    ""QS-APR"": partial(QuarterBegin, month=4),\n    ""QS-MAY"": partial(QuarterBegin, month=5),\n    ""QS-JUN"": partial(QuarterBegin, month=6),\n    ""QS-JUL"": partial(QuarterBegin, month=7),\n    ""QS-AUG"": partial(QuarterBegin, month=8),\n    ""QS-SEP"": partial(QuarterBegin, month=9),\n    ""QS-OCT"": partial(QuarterBegin, month=10),\n    ""QS-NOV"": partial(QuarterBegin, month=11),\n    ""QS-DEC"": partial(QuarterBegin, month=12),\n    ""Q-JAN"": partial(QuarterEnd, month=1),\n    ""Q-FEB"": partial(QuarterEnd, month=2),\n    ""Q-MAR"": partial(QuarterEnd, month=3),\n    ""Q-APR"": partial(QuarterEnd, month=4),\n    ""Q-MAY"": partial(QuarterEnd, month=5),\n    ""Q-JUN"": partial(QuarterEnd, month=6),\n    ""Q-JUL"": partial(QuarterEnd, month=7),\n    ""Q-AUG"": partial(QuarterEnd, month=8),\n    ""Q-SEP"": partial(QuarterEnd, month=9),\n    ""Q-OCT"": partial(QuarterEnd, month=10),\n    ""Q-NOV"": partial(QuarterEnd, month=11),\n    ""Q-DEC"": partial(QuarterEnd, month=12),\n}\n\n\n_FREQUENCY_CONDITION = ""|"".join(_FREQUENCIES.keys())\n_PATTERN = fr""^((?P<multiple>\\d+)|())(?P<freq>({_FREQUENCY_CONDITION}))$""\n\n\n# pandas defines these offsets as ""Tick"" objects, which for instance have\n# distinct behavior from monthly or longer frequencies in resample.\nCFTIME_TICKS = (Day, Hour, Minute, Second)\n\n\ndef to_offset(freq):\n    """"""Convert a frequency string to the appropriate subclass of\n    BaseCFTimeOffset.""""""\n    if isinstance(freq, BaseCFTimeOffset):\n        return freq\n    else:\n        try:\n            freq_data = re.match(_PATTERN, freq).groupdict()\n        except AttributeError:\n            raise ValueError(""Invalid frequency string provided"")\n\n    freq = freq_data[""freq""]\n    multiples = freq_data[""multiple""]\n    if multiples is None:\n        multiples = 1\n    else:\n        multiples = int(multiples)\n\n    return _FREQUENCIES[freq](n=multiples)\n\n\ndef to_cftime_datetime(date_str_or_date, calendar=None):\n    import cftime\n\n    if isinstance(date_str_or_date, str):\n        if calendar is None:\n            raise ValueError(\n                ""If converting a string to a cftime.datetime object, ""\n                ""a calendar type must be provided""\n            )\n        date, _ = _parse_iso8601_with_reso(get_date_type(calendar), date_str_or_date)\n        return date\n    elif isinstance(date_str_or_date, cftime.datetime):\n        return date_str_or_date\n    else:\n        raise TypeError(\n            ""date_str_or_date must be a string or a ""\n            ""subclass of cftime.datetime. Instead got ""\n            ""{!r}."".format(date_str_or_date)\n        )\n\n\ndef normalize_date(date):\n    """"""Round datetime down to midnight.""""""\n    return date.replace(hour=0, minute=0, second=0, microsecond=0)\n\n\ndef _maybe_normalize_date(date, normalize):\n    """"""Round datetime down to midnight if normalize is True.""""""\n    if normalize:\n        return normalize_date(date)\n    else:\n        return date\n\n\ndef _generate_linear_range(start, end, periods):\n    """"""Generate an equally-spaced sequence of cftime.datetime objects between\n    and including two dates (whose length equals the number of periods).""""""\n    import cftime\n\n    total_seconds = (end - start).total_seconds()\n    values = np.linspace(0.0, total_seconds, periods, endpoint=True)\n    units = ""seconds since {}"".format(format_cftime_datetime(start))\n    calendar = start.calendar\n    return cftime.num2date(\n        values, units=units, calendar=calendar, only_use_cftime_datetimes=True\n    )\n\n\ndef _generate_range(start, end, periods, offset):\n    """"""Generate a regular range of cftime.datetime objects with a\n    given time offset.\n\n    Adapted from pandas.tseries.offsets.generate_range.\n\n    Parameters\n    ----------\n    start : cftime.datetime, or None\n        Start of range\n    end : cftime.datetime, or None\n        End of range\n    periods : int, or None\n        Number of elements in the sequence\n    offset : BaseCFTimeOffset\n        An offset class designed for working with cftime.datetime objects\n\n    Returns\n    -------\n    A generator object\n    """"""\n    if start:\n        start = offset.rollforward(start)\n\n    if end:\n        end = offset.rollback(end)\n\n    if periods is None and end < start:\n        end = None\n        periods = 0\n\n    if end is None:\n        end = start + (periods - 1) * offset\n\n    if start is None:\n        start = end - (periods - 1) * offset\n\n    current = start\n    if offset.n >= 0:\n        while current <= end:\n            yield current\n\n            next_date = current + offset\n            if next_date <= current:\n                raise ValueError(f""Offset {offset} did not increment date"")\n            current = next_date\n    else:\n        while current >= end:\n            yield current\n\n            next_date = current + offset\n            if next_date >= current:\n                raise ValueError(f""Offset {offset} did not decrement date"")\n            current = next_date\n\n\ndef cftime_range(\n    start=None,\n    end=None,\n    periods=None,\n    freq=""D"",\n    normalize=False,\n    name=None,\n    closed=None,\n    calendar=""standard"",\n):\n    """"""Return a fixed frequency CFTimeIndex.\n\n    Parameters\n    ----------\n    start : str or cftime.datetime, optional\n        Left bound for generating dates.\n    end : str or cftime.datetime, optional\n        Right bound for generating dates.\n    periods : integer, optional\n        Number of periods to generate.\n    freq : str, default \'D\', BaseCFTimeOffset, or None\n       Frequency strings can have multiples, e.g. \'5H\'.\n    normalize : bool, default False\n        Normalize start/end dates to midnight before generating date range.\n    name : str, default None\n        Name of the resulting index\n    closed : {None, \'left\', \'right\'}, optional\n        Make the interval closed with respect to the given frequency to the\n        \'left\', \'right\', or both sides (None, the default).\n    calendar : str\n        Calendar type for the datetimes (default \'standard\').\n\n    Returns\n    -------\n    CFTimeIndex\n\n    Notes\n    -----\n\n    This function is an analog of ``pandas.date_range`` for use in generating\n    sequences of ``cftime.datetime`` objects.  It supports most of the\n    features of ``pandas.date_range`` (e.g. specifying how the index is\n    ``closed`` on either side, or whether or not to ``normalize`` the start and\n    end bounds); however, there are some notable exceptions:\n\n    - You cannot specify a ``tz`` (time zone) argument.\n    - Start or end dates specified as partial-datetime strings must use the\n      `ISO-8601 format <https://en.wikipedia.org/wiki/ISO_8601>`_.\n    - It supports many, but not all, frequencies supported by\n      ``pandas.date_range``.  For example it does not currently support any of\n      the business-related, semi-monthly, or sub-second frequencies.\n    - Compound sub-monthly frequencies are not supported, e.g. \'1H1min\', as\n      these can easily be written in terms of the finest common resolution,\n      e.g. \'61min\'.\n\n    Valid simple frequency strings for use with ``cftime``-calendars include\n    any multiples of the following.\n\n    +--------+--------------------------+\n    | Alias  | Description              |\n    +========+==========================+\n    | A, Y   | Year-end frequency       |\n    +--------+--------------------------+\n    | AS, YS | Year-start frequency     |\n    +--------+--------------------------+\n    | Q      | Quarter-end frequency    |\n    +--------+--------------------------+\n    | QS     | Quarter-start frequency  |\n    +--------+--------------------------+\n    | M      | Month-end frequency      |\n    +--------+--------------------------+\n    | MS     | Month-start frequency    |\n    +--------+--------------------------+\n    | D      | Day frequency            |\n    +--------+--------------------------+\n    | H      | Hour frequency           |\n    +--------+--------------------------+\n    | T, min | Minute frequency         |\n    +--------+--------------------------+\n    | S      | Second frequency         |\n    +--------+--------------------------+\n\n    Any multiples of the following anchored offsets are also supported.\n\n    +----------+--------------------------------------------------------------------+\n    | Alias    | Description                                                        |\n    +==========+====================================================================+\n    | A(S)-JAN | Annual frequency, anchored at the end (or beginning) of January    |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-FEB | Annual frequency, anchored at the end (or beginning) of February   |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-MAR | Annual frequency, anchored at the end (or beginning) of March      |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-APR | Annual frequency, anchored at the end (or beginning) of April      |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-MAY | Annual frequency, anchored at the end (or beginning) of May        |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-JUN | Annual frequency, anchored at the end (or beginning) of June       |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-JUL | Annual frequency, anchored at the end (or beginning) of July       |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-AUG | Annual frequency, anchored at the end (or beginning) of August     |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-SEP | Annual frequency, anchored at the end (or beginning) of September  |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-OCT | Annual frequency, anchored at the end (or beginning) of October    |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-NOV | Annual frequency, anchored at the end (or beginning) of November   |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-DEC | Annual frequency, anchored at the end (or beginning) of December   |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-JAN | Quarter frequency, anchored at the end (or beginning) of January   |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-FEB | Quarter frequency, anchored at the end (or beginning) of February  |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-MAR | Quarter frequency, anchored at the end (or beginning) of March     |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-APR | Quarter frequency, anchored at the end (or beginning) of April     |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-MAY | Quarter frequency, anchored at the end (or beginning) of May       |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-JUN | Quarter frequency, anchored at the end (or beginning) of June      |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-JUL | Quarter frequency, anchored at the end (or beginning) of July      |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-AUG | Quarter frequency, anchored at the end (or beginning) of August    |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-SEP | Quarter frequency, anchored at the end (or beginning) of September |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-OCT | Quarter frequency, anchored at the end (or beginning) of October   |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-NOV | Quarter frequency, anchored at the end (or beginning) of November  |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-DEC | Quarter frequency, anchored at the end (or beginning) of December  |\n    +----------+--------------------------------------------------------------------+\n\n\n    Finally, the following calendar aliases are supported.\n\n    +--------------------------------+---------------------------------------+\n    | Alias                          | Date type                             |\n    +================================+=======================================+\n    | standard, gregorian            | ``cftime.DatetimeGregorian``          |\n    +--------------------------------+---------------------------------------+\n    | proleptic_gregorian            | ``cftime.DatetimeProlepticGregorian`` |\n    +--------------------------------+---------------------------------------+\n    | noleap, 365_day                | ``cftime.DatetimeNoLeap``             |\n    +--------------------------------+---------------------------------------+\n    | all_leap, 366_day              | ``cftime.DatetimeAllLeap``            |\n    +--------------------------------+---------------------------------------+\n    | 360_day                        | ``cftime.Datetime360Day``             |\n    +--------------------------------+---------------------------------------+\n    | julian                         | ``cftime.DatetimeJulian``             |\n    +--------------------------------+---------------------------------------+\n\n    Examples\n    --------\n\n    This function returns a ``CFTimeIndex``, populated with ``cftime.datetime``\n    objects associated with the specified calendar type, e.g.\n\n    >>> xr.cftime_range(start=""2000"", periods=6, freq=""2MS"", calendar=""noleap"")\n    CFTimeIndex([2000-01-01 00:00:00, 2000-03-01 00:00:00, 2000-05-01 00:00:00,\n                 2000-07-01 00:00:00, 2000-09-01 00:00:00, 2000-11-01 00:00:00],\n                dtype=\'object\')\n\n    As in the standard pandas function, three of the ``start``, ``end``,\n    ``periods``, or ``freq`` arguments must be specified at a given time, with\n    the other set to ``None``.  See the `pandas documentation\n    <https://pandas.pydata.org/pandas-docs/stable/generated/pandas.date_range.html#pandas.date_range>`_\n    for more examples of the behavior of ``date_range`` with each of the\n    parameters.\n\n    See Also\n    --------\n    pandas.date_range\n    """"""\n    # Adapted from pandas.core.indexes.datetimes._generate_range.\n    if count_not_none(start, end, periods, freq) != 3:\n        raise ValueError(\n            ""Of the arguments \'start\', \'end\', \'periods\', and \'freq\', three ""\n            ""must be specified at a time.""\n        )\n\n    if start is not None:\n        start = to_cftime_datetime(start, calendar)\n        start = _maybe_normalize_date(start, normalize)\n    if end is not None:\n        end = to_cftime_datetime(end, calendar)\n        end = _maybe_normalize_date(end, normalize)\n\n    if freq is None:\n        dates = _generate_linear_range(start, end, periods)\n    else:\n        offset = to_offset(freq)\n        dates = np.array(list(_generate_range(start, end, periods, offset)))\n\n    left_closed = False\n    right_closed = False\n\n    if closed is None:\n        left_closed = True\n        right_closed = True\n    elif closed == ""left"":\n        left_closed = True\n    elif closed == ""right"":\n        right_closed = True\n    else:\n        raise ValueError(""Closed must be either \'left\', \'right\' or None"")\n\n    if not left_closed and len(dates) and start is not None and dates[0] == start:\n        dates = dates[1:]\n    if not right_closed and len(dates) and end is not None and dates[-1] == end:\n        dates = dates[:-1]\n\n    return CFTimeIndex(dates, name=name)\n'"
xarray/coding/cftimeindex.py,26,"b'""""""DatetimeIndex analog for cftime.datetime objects""""""\n# The pandas.Index subclass defined here was copied and adapted for\n# use with cftime.datetime objects based on the source code defining\n# pandas.DatetimeIndex.\n\n# For reference, here is a copy of the pandas copyright notice:\n\n# (c) 2011-2012, Lambda Foundry, Inc. and PyData Development Team\n# All rights reserved.\n\n# Copyright (c) 2008-2011 AQR Capital Management, LLC\n# All rights reserved.\n\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n\n#     * Redistributions of source code must retain the above copyright\n#        notice, this list of conditions and the following disclaimer.\n\n#     * Redistributions in binary form must reproduce the above\n#        copyright notice, this list of conditions and the following\n#        disclaimer in the documentation and/or other materials provided\n#        with the distribution.\n\n#     * Neither the name of the copyright holder nor the names of any\n#        contributors may be used to endorse or promote products derived\n#        from this software without specific prior written permission.\n\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER AND CONTRIBUTORS\n# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport re\nimport warnings\nfrom datetime import timedelta\nfrom distutils.version import LooseVersion\n\nimport numpy as np\nimport pandas as pd\n\nfrom xarray.core.utils import is_scalar\n\nfrom ..core.common import _contains_cftime_datetimes\nfrom .times import _STANDARD_CALENDARS, cftime_to_nptime, infer_calendar_name\n\n\ndef named(name, pattern):\n    return ""(?P<"" + name + "">"" + pattern + "")""\n\n\ndef optional(x):\n    return ""(?:"" + x + "")?""\n\n\ndef trailing_optional(xs):\n    if not xs:\n        return """"\n    return xs[0] + optional(trailing_optional(xs[1:]))\n\n\ndef build_pattern(date_sep=r""\\-"", datetime_sep=r""T"", time_sep=r""\\:""):\n    pieces = [\n        (None, ""year"", r""\\d{4}""),\n        (date_sep, ""month"", r""\\d{2}""),\n        (date_sep, ""day"", r""\\d{2}""),\n        (datetime_sep, ""hour"", r""\\d{2}""),\n        (time_sep, ""minute"", r""\\d{2}""),\n        (time_sep, ""second"", r""\\d{2}""),\n    ]\n    pattern_list = []\n    for sep, name, sub_pattern in pieces:\n        pattern_list.append((sep if sep else """") + named(name, sub_pattern))\n        # TODO: allow timezone offsets?\n    return ""^"" + trailing_optional(pattern_list) + ""$""\n\n\n_BASIC_PATTERN = build_pattern(date_sep="""", time_sep="""")\n_EXTENDED_PATTERN = build_pattern()\n_PATTERNS = [_BASIC_PATTERN, _EXTENDED_PATTERN]\n\n\ndef parse_iso8601(datetime_string):\n    for pattern in _PATTERNS:\n        match = re.match(pattern, datetime_string)\n        if match:\n            return match.groupdict()\n    raise ValueError(""no ISO-8601 match for string: %s"" % datetime_string)\n\n\ndef _parse_iso8601_with_reso(date_type, timestr):\n    import cftime\n\n    default = date_type(1, 1, 1)\n    result = parse_iso8601(timestr)\n    replace = {}\n\n    for attr in [""year"", ""month"", ""day"", ""hour"", ""minute"", ""second""]:\n        value = result.get(attr, None)\n        if value is not None:\n            # Note ISO8601 conventions allow for fractional seconds.\n            # TODO: Consider adding support for sub-second resolution?\n            replace[attr] = int(value)\n            resolution = attr\n    if LooseVersion(cftime.__version__) < LooseVersion(""1.0.4""):\n        # dayofwk=-1 is required to update the dayofwk and dayofyr attributes of\n        # the returned date object in versions of cftime between 1.0.2 and\n        # 1.0.3.4.  It can be removed for versions of cftime greater than\n        # 1.0.3.4.\n        replace[""dayofwk""] = -1\n    return default.replace(**replace), resolution\n\n\ndef _parsed_string_to_bounds(date_type, resolution, parsed):\n    """"""Generalization of\n    pandas.tseries.index.DatetimeIndex._parsed_string_to_bounds\n    for use with non-standard calendars and cftime.datetime\n    objects.\n    """"""\n    if resolution == ""year"":\n        return (\n            date_type(parsed.year, 1, 1),\n            date_type(parsed.year + 1, 1, 1) - timedelta(microseconds=1),\n        )\n    elif resolution == ""month"":\n        if parsed.month == 12:\n            end = date_type(parsed.year + 1, 1, 1) - timedelta(microseconds=1)\n        else:\n            end = date_type(parsed.year, parsed.month + 1, 1) - timedelta(\n                microseconds=1\n            )\n        return date_type(parsed.year, parsed.month, 1), end\n    elif resolution == ""day"":\n        start = date_type(parsed.year, parsed.month, parsed.day)\n        return start, start + timedelta(days=1, microseconds=-1)\n    elif resolution == ""hour"":\n        start = date_type(parsed.year, parsed.month, parsed.day, parsed.hour)\n        return start, start + timedelta(hours=1, microseconds=-1)\n    elif resolution == ""minute"":\n        start = date_type(\n            parsed.year, parsed.month, parsed.day, parsed.hour, parsed.minute\n        )\n        return start, start + timedelta(minutes=1, microseconds=-1)\n    elif resolution == ""second"":\n        start = date_type(\n            parsed.year,\n            parsed.month,\n            parsed.day,\n            parsed.hour,\n            parsed.minute,\n            parsed.second,\n        )\n        return start, start + timedelta(seconds=1, microseconds=-1)\n    else:\n        raise KeyError\n\n\ndef get_date_field(datetimes, field):\n    """"""Adapted from pandas.tslib.get_date_field""""""\n    return np.array([getattr(date, field) for date in datetimes])\n\n\ndef _field_accessor(name, docstring=None, min_cftime_version=""0.0""):\n    """"""Adapted from pandas.tseries.index._field_accessor""""""\n\n    def f(self, min_cftime_version=min_cftime_version):\n        import cftime\n\n        version = cftime.__version__\n\n        if LooseVersion(version) >= LooseVersion(min_cftime_version):\n            return get_date_field(self._data, name)\n        else:\n            raise ImportError(\n                ""The {!r} accessor requires a minimum ""\n                ""version of cftime of {}. Found an ""\n                ""installed version of {}."".format(name, min_cftime_version, version)\n            )\n\n    f.__name__ = name\n    f.__doc__ = docstring\n    return property(f)\n\n\ndef get_date_type(self):\n    if self._data.size:\n        return type(self._data[0])\n    else:\n        return None\n\n\ndef assert_all_valid_date_type(data):\n    import cftime\n\n    if len(data) > 0:\n        sample = data[0]\n        date_type = type(sample)\n        if not isinstance(sample, cftime.datetime):\n            raise TypeError(\n                ""CFTimeIndex requires cftime.datetime ""\n                ""objects. Got object of {}."".format(date_type)\n            )\n        if not all(isinstance(value, date_type) for value in data):\n            raise TypeError(\n                ""CFTimeIndex requires using datetime ""\n                ""objects of all the same type.  Got\\n{}."".format(data)\n            )\n\n\nclass CFTimeIndex(pd.Index):\n    """"""Custom Index for working with CF calendars and dates\n\n    All elements of a CFTimeIndex must be cftime.datetime objects.\n\n    Parameters\n    ----------\n    data : array or CFTimeIndex\n        Sequence of cftime.datetime objects to use in index\n    name : str, default None\n        Name of the resulting index\n\n    See Also\n    --------\n    cftime_range\n    """"""\n\n    year = _field_accessor(""year"", ""The year of the datetime"")\n    month = _field_accessor(""month"", ""The month of the datetime"")\n    day = _field_accessor(""day"", ""The days of the datetime"")\n    hour = _field_accessor(""hour"", ""The hours of the datetime"")\n    minute = _field_accessor(""minute"", ""The minutes of the datetime"")\n    second = _field_accessor(""second"", ""The seconds of the datetime"")\n    microsecond = _field_accessor(""microsecond"", ""The microseconds of the datetime"")\n    dayofyear = _field_accessor(\n        ""dayofyr"", ""The ordinal day of year of the datetime"", ""1.0.2.1""\n    )\n    dayofweek = _field_accessor(""dayofwk"", ""The day of week of the datetime"", ""1.0.2.1"")\n    days_in_month = _field_accessor(\n        ""daysinmonth"", ""The number of days in the month of the datetime"", ""1.1.0.0""\n    )\n    date_type = property(get_date_type)\n\n    def __new__(cls, data, name=None):\n        assert_all_valid_date_type(data)\n        if name is None and hasattr(data, ""name""):\n            name = data.name\n\n        result = object.__new__(cls)\n        result._data = np.array(data, dtype=""O"")\n        result.name = name\n        result._cache = {}\n        return result\n\n    def _partial_date_slice(self, resolution, parsed):\n        """"""Adapted from\n        pandas.tseries.index.DatetimeIndex._partial_date_slice\n\n        Note that when using a CFTimeIndex, if a partial-date selection\n        returns a single element, it will never be converted to a scalar\n        coordinate; this is in slight contrast to the behavior when using\n        a DatetimeIndex, which sometimes will return a DataArray with a scalar\n        coordinate depending on the resolution of the datetimes used in\n        defining the index.  For example:\n\n        >>> from cftime import DatetimeNoLeap\n        >>> import pandas as pd\n        >>> import xarray as xr\n        >>> da = xr.DataArray(\n        ...     [1, 2],\n        ...     coords=[[DatetimeNoLeap(2001, 1, 1), DatetimeNoLeap(2001, 2, 1)]],\n        ...     dims=[""time""],\n        ... )\n        >>> da.sel(time=""2001-01-01"")\n        <xarray.DataArray (time: 1)>\n        array([1])\n        Coordinates:\n          * time     (time) object 2001-01-01 00:00:00\n        >>> da = xr.DataArray(\n        ...     [1, 2],\n        ...     coords=[[pd.Timestamp(2001, 1, 1), pd.Timestamp(2001, 2, 1)]],\n        ...     dims=[""time""],\n        ... )\n        >>> da.sel(time=""2001-01-01"")\n        <xarray.DataArray ()>\n        array(1)\n        Coordinates:\n            time     datetime64[ns] 2001-01-01\n        >>> da = xr.DataArray(\n        ...     [1, 2],\n        ...     coords=[[pd.Timestamp(2001, 1, 1, 1), pd.Timestamp(2001, 2, 1)]],\n        ...     dims=[""time""],\n        ... )\n        >>> da.sel(time=""2001-01-01"")\n        <xarray.DataArray (time: 1)>\n        array([1])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-01T01:00:00\n        """"""\n        start, end = _parsed_string_to_bounds(self.date_type, resolution, parsed)\n\n        times = self._data\n\n        if self.is_monotonic:\n            if len(times) and (\n                (start < times[0] and end < times[0])\n                or (start > times[-1] and end > times[-1])\n            ):\n                # we are out of range\n                raise KeyError\n\n            # a monotonic (sorted) series can be sliced\n            left = times.searchsorted(start, side=""left"")\n            right = times.searchsorted(end, side=""right"")\n            return slice(left, right)\n\n        lhs_mask = times >= start\n        rhs_mask = times <= end\n        return np.flatnonzero(lhs_mask & rhs_mask)\n\n    def _get_string_slice(self, key):\n        """"""Adapted from pandas.tseries.index.DatetimeIndex._get_string_slice""""""\n        parsed, resolution = _parse_iso8601_with_reso(self.date_type, key)\n        try:\n            loc = self._partial_date_slice(resolution, parsed)\n        except KeyError:\n            raise KeyError(key)\n        return loc\n\n    def _get_nearest_indexer(self, target, limit, tolerance):\n        """"""Adapted from pandas.Index._get_nearest_indexer""""""\n        left_indexer = self.get_indexer(target, ""pad"", limit=limit)\n        right_indexer = self.get_indexer(target, ""backfill"", limit=limit)\n        left_distances = abs(self.values[left_indexer] - target.values)\n        right_distances = abs(self.values[right_indexer] - target.values)\n\n        if self.is_monotonic_increasing:\n            condition = (left_distances < right_distances) | (right_indexer == -1)\n        else:\n            condition = (left_distances <= right_distances) | (right_indexer == -1)\n        indexer = np.where(condition, left_indexer, right_indexer)\n\n        if tolerance is not None:\n            indexer = self._filter_indexer_tolerance(target, indexer, tolerance)\n        return indexer\n\n    def _filter_indexer_tolerance(self, target, indexer, tolerance):\n        """"""Adapted from pandas.Index._filter_indexer_tolerance""""""\n        if isinstance(target, pd.Index):\n            distance = abs(self.values[indexer] - target.values)\n        else:\n            distance = abs(self.values[indexer] - target)\n        indexer = np.where(distance <= tolerance, indexer, -1)\n        return indexer\n\n    def get_loc(self, key, method=None, tolerance=None):\n        """"""Adapted from pandas.tseries.index.DatetimeIndex.get_loc""""""\n        if isinstance(key, str):\n            return self._get_string_slice(key)\n        else:\n            return pd.Index.get_loc(self, key, method=method, tolerance=tolerance)\n\n    def _maybe_cast_slice_bound(self, label, side, kind):\n        """"""Adapted from\n        pandas.tseries.index.DatetimeIndex._maybe_cast_slice_bound""""""\n        if isinstance(label, str):\n            parsed, resolution = _parse_iso8601_with_reso(self.date_type, label)\n            start, end = _parsed_string_to_bounds(self.date_type, resolution, parsed)\n            if self.is_monotonic_decreasing and len(self) > 1:\n                return end if side == ""left"" else start\n            return start if side == ""left"" else end\n        else:\n            return label\n\n    # TODO: Add ability to use integer range outside of iloc?\n    # e.g. series[1:5].\n    def get_value(self, series, key):\n        """"""Adapted from pandas.tseries.index.DatetimeIndex.get_value""""""\n        if np.asarray(key).dtype == np.dtype(bool):\n            return series.iloc[key]\n        elif isinstance(key, slice):\n            return series.iloc[self.slice_indexer(key.start, key.stop, key.step)]\n        else:\n            return series.iloc[self.get_loc(key)]\n\n    def __contains__(self, key):\n        """"""Adapted from\n        pandas.tseries.base.DatetimeIndexOpsMixin.__contains__""""""\n        try:\n            result = self.get_loc(key)\n            return (\n                is_scalar(result)\n                or type(result) == slice\n                or (isinstance(result, np.ndarray) and result.size)\n            )\n        except (KeyError, TypeError, ValueError):\n            return False\n\n    def contains(self, key):\n        """"""Needed for .loc based partial-string indexing""""""\n        return self.__contains__(key)\n\n    def shift(self, n, freq):\n        """"""Shift the CFTimeIndex a multiple of the given frequency.\n\n        See the documentation for :py:func:`~xarray.cftime_range` for a\n        complete listing of valid frequency strings.\n\n        Parameters\n        ----------\n        n : int\n            Periods to shift by\n        freq : str or datetime.timedelta\n            A frequency string or datetime.timedelta object to shift by\n\n        Returns\n        -------\n        CFTimeIndex\n\n        See also\n        --------\n        pandas.DatetimeIndex.shift\n\n        Examples\n        --------\n        >>> index = xr.cftime_range(""2000"", periods=1, freq=""M"")\n        >>> index\n        CFTimeIndex([2000-01-31 00:00:00], dtype=\'object\')\n        >>> index.shift(1, ""M"")\n        CFTimeIndex([2000-02-29 00:00:00], dtype=\'object\')\n        """"""\n        from .cftime_offsets import to_offset\n\n        if not isinstance(n, int):\n            raise TypeError(f""\'n\' must be an int, got {n}."")\n        if isinstance(freq, timedelta):\n            return self + n * freq\n        elif isinstance(freq, str):\n            return self + n * to_offset(freq)\n        else:\n            raise TypeError(\n                ""\'freq\' must be of type ""\n                ""str or datetime.timedelta, got {}."".format(freq)\n            )\n\n    def __add__(self, other):\n        if isinstance(other, pd.TimedeltaIndex):\n            other = other.to_pytimedelta()\n        return CFTimeIndex(np.array(self) + other)\n\n    def __radd__(self, other):\n        if isinstance(other, pd.TimedeltaIndex):\n            other = other.to_pytimedelta()\n        return CFTimeIndex(other + np.array(self))\n\n    def __sub__(self, other):\n        if _contains_datetime_timedeltas(other):\n            return CFTimeIndex(np.array(self) - other)\n        elif isinstance(other, pd.TimedeltaIndex):\n            return CFTimeIndex(np.array(self) - other.to_pytimedelta())\n        elif _contains_cftime_datetimes(np.array(other)):\n            try:\n                return pd.TimedeltaIndex(np.array(self) - np.array(other))\n            except OverflowError:\n                raise ValueError(\n                    ""The time difference exceeds the range of values ""\n                    ""that can be expressed at the nanosecond resolution.""\n                )\n        else:\n            return NotImplemented\n\n    def __rsub__(self, other):\n        try:\n            return pd.TimedeltaIndex(other - np.array(self))\n        except OverflowError:\n            raise ValueError(\n                ""The time difference exceeds the range of values ""\n                ""that can be expressed at the nanosecond resolution.""\n            )\n\n    def to_datetimeindex(self, unsafe=False):\n        """"""If possible, convert this index to a pandas.DatetimeIndex.\n\n        Parameters\n        ----------\n        unsafe : bool\n            Flag to turn off warning when converting from a CFTimeIndex with\n            a non-standard calendar to a DatetimeIndex (default ``False``).\n\n        Returns\n        -------\n        pandas.DatetimeIndex\n\n        Raises\n        ------\n        ValueError\n            If the CFTimeIndex contains dates that are not possible in the\n            standard calendar or outside the pandas.Timestamp-valid range.\n\n        Warns\n        -----\n        RuntimeWarning\n            If converting from a non-standard calendar to a DatetimeIndex.\n\n        Warnings\n        --------\n        Note that for non-standard calendars, this will change the calendar\n        type of the index.  In that case the result of this method should be\n        used with caution.\n\n        Examples\n        --------\n        >>> import xarray as xr\n        >>> times = xr.cftime_range(""2000"", periods=2, calendar=""gregorian"")\n        >>> times\n        CFTimeIndex([2000-01-01 00:00:00, 2000-01-02 00:00:00], dtype=\'object\')\n        >>> times.to_datetimeindex()\n        DatetimeIndex([\'2000-01-01\', \'2000-01-02\'], dtype=\'datetime64[ns]\', freq=None)\n        """"""\n        nptimes = cftime_to_nptime(self)\n        calendar = infer_calendar_name(self)\n        if calendar not in _STANDARD_CALENDARS and not unsafe:\n            warnings.warn(\n                ""Converting a CFTimeIndex with dates from a non-standard ""\n                ""calendar, {!r}, to a pandas.DatetimeIndex, which uses dates ""\n                ""from the standard calendar.  This may lead to subtle errors ""\n                ""in operations that depend on the length of time between ""\n                ""dates."".format(calendar),\n                RuntimeWarning,\n                stacklevel=2,\n            )\n        return pd.DatetimeIndex(nptimes)\n\n    def strftime(self, date_format):\n        """"""\n        Return an Index of formatted strings specified by date_format, which\n        supports the same string format as the python standard library. Details\n        of the string format can be found in `python string format doc\n        <https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior>`__\n\n        Parameters\n        ----------\n        date_format : str\n            Date format string (e.g. ""%Y-%m-%d"")\n\n        Returns\n        -------\n        pandas.Index\n            Index of formatted strings\n\n        Examples\n        --------\n        >>> rng = xr.cftime_range(\n        ...     start=""2000"", periods=5, freq=""2MS"", calendar=""noleap""\n        ... )\n        >>> rng.strftime(""%B %d, %Y, %r"")\n        Index([\'January 01, 2000, 12:00:00 AM\', \'March 01, 2000, 12:00:00 AM\',\n               \'May 01, 2000, 12:00:00 AM\', \'July 01, 2000, 12:00:00 AM\',\n               \'September 01, 2000, 12:00:00 AM\'],\n              dtype=\'object\')\n        """"""\n        return pd.Index([date.strftime(date_format) for date in self._data])\n\n    @property\n    def asi8(self):\n        """"""Convert to integers with units of microseconds since 1970-01-01.""""""\n        from ..core.resample_cftime import exact_cftime_datetime_difference\n\n        epoch = self.date_type(1970, 1, 1)\n        return np.array(\n            [\n                _total_microseconds(exact_cftime_datetime_difference(epoch, date))\n                for date in self.values\n            ],\n            dtype=np.int64,\n        )\n\n    def _round_via_method(self, freq, method):\n        """"""Round dates using a specified method.""""""\n        from .cftime_offsets import CFTIME_TICKS, to_offset\n\n        offset = to_offset(freq)\n        if not isinstance(offset, CFTIME_TICKS):\n            raise ValueError(f""{offset} is a non-fixed frequency"")\n\n        unit = _total_microseconds(offset.as_timedelta())\n        values = self.asi8\n        rounded = method(values, unit)\n        return _cftimeindex_from_i8(rounded, self.date_type, self.name)\n\n    def floor(self, freq):\n        """"""Round dates down to fixed frequency.\n\n        Parameters\n        ----------\n        freq : str or CFTimeOffset\n            The frequency level to round the index to.  Must be a fixed\n            frequency like \'S\' (second) not \'ME\' (month end).  See `frequency\n            aliases <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`_\n            for a list of possible values.\n\n        Returns\n        -------\n        CFTimeIndex\n        """"""\n        return self._round_via_method(freq, _floor_int)\n\n    def ceil(self, freq):\n        """"""Round dates up to fixed frequency.\n\n        Parameters\n        ----------\n        freq : str or CFTimeOffset\n            The frequency level to round the index to.  Must be a fixed\n            frequency like \'S\' (second) not \'ME\' (month end).  See `frequency\n            aliases <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`_\n            for a list of possible values.\n\n        Returns\n        -------\n        CFTimeIndex\n        """"""\n        return self._round_via_method(freq, _ceil_int)\n\n    def round(self, freq):\n        """"""Round dates to a fixed frequency.\n\n        Parameters\n        ----------\n        freq : str or CFTimeOffset\n            The frequency level to round the index to.  Must be a fixed\n            frequency like \'S\' (second) not \'ME\' (month end).  See `frequency\n            aliases <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`_\n            for a list of possible values.\n\n        Returns\n        -------\n        CFTimeIndex\n        """"""\n        return self._round_via_method(freq, _round_to_nearest_half_even)\n\n\ndef _parse_iso8601_without_reso(date_type, datetime_str):\n    date, _ = _parse_iso8601_with_reso(date_type, datetime_str)\n    return date\n\n\ndef _parse_array_of_cftime_strings(strings, date_type):\n    """"""Create a numpy array from an array of strings.\n\n    For use in generating dates from strings for use with interp.  Assumes the\n    array is either 0-dimensional or 1-dimensional.\n\n    Parameters\n    ----------\n    strings : array of strings\n        Strings to convert to dates\n    date_type : cftime.datetime type\n        Calendar type to use for dates\n\n    Returns\n    -------\n    np.array\n    """"""\n    return np.array(\n        [_parse_iso8601_without_reso(date_type, s) for s in strings.ravel()]\n    ).reshape(strings.shape)\n\n\ndef _contains_datetime_timedeltas(array):\n    """"""Check if an input array contains datetime.timedelta objects.""""""\n    array = np.atleast_1d(array)\n    return isinstance(array[0], timedelta)\n\n\ndef _cftimeindex_from_i8(values, date_type, name):\n    """"""Construct a CFTimeIndex from an array of integers.\n\n    Parameters\n    ----------\n    values : np.array\n        Integers representing microseconds since 1970-01-01.\n    date_type : cftime.datetime\n        Type of date for the index.\n    name : str\n        Name of the index.\n\n    Returns\n    -------\n    CFTimeIndex\n    """"""\n    epoch = date_type(1970, 1, 1)\n    dates = np.array([epoch + timedelta(microseconds=int(value)) for value in values])\n    return CFTimeIndex(dates, name=name)\n\n\ndef _total_microseconds(delta):\n    """"""Compute the total number of microseconds of a datetime.timedelta.\n\n    Parameters\n    ----------\n    delta : datetime.timedelta\n        Input timedelta.\n\n    Returns\n    -------\n    int\n    """"""\n    return delta / timedelta(microseconds=1)\n\n\ndef _floor_int(values, unit):\n    """"""Copied from pandas.""""""\n    return values - np.remainder(values, unit)\n\n\ndef _ceil_int(values, unit):\n    """"""Copied from pandas.""""""\n    return values + np.remainder(-values, unit)\n\n\ndef _round_to_nearest_half_even(values, unit):\n    """"""Copied from pandas.""""""\n    if unit % 2:\n        return _ceil_int(values - unit // 2, unit)\n    quotient, remainder = np.divmod(values, unit)\n    mask = np.logical_or(\n        remainder > (unit // 2), np.logical_and(remainder == (unit // 2), quotient % 2)\n    )\n    quotient[mask] += 1\n    return quotient * unit\n'"
xarray/coding/frequencies.py,3,"b'""""""FrequencyInferer analog for cftime.datetime objects""""""\n# The infer_freq method and the _CFTimeFrequencyInferer\n# subclass defined here were copied and adapted for\n# use with cftime.datetime objects based on the source code in\n# pandas.tseries.Frequencies._FrequencyInferer\n\n# For reference, here is a copy of the pandas copyright notice:\n\n# (c) 2011-2012, Lambda Foundry, Inc. and PyData Development Team\n# All rights reserved.\n\n# Copyright (c) 2008-2011 AQR Capital Management, LLC\n# All rights reserved.\n\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n\n#     * Redistributions of source code must retain the above copyright\n#        notice, this list of conditions and the following disclaimer.\n\n#     * Redistributions in binary form must reproduce the above\n#        copyright notice, this list of conditions and the following\n#        disclaimer in the documentation and/or other materials provided\n#        with the distribution.\n\n#     * Neither the name of the copyright holder nor the names of any\n#        contributors may be used to endorse or promote products derived\n#        from this software without specific prior written permission.\n\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER AND CONTRIBUTORS\n# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport numpy as np\nimport pandas as pd\n\nfrom ..core.common import _contains_datetime_like_objects\nfrom .cftime_offsets import _MONTH_ABBREVIATIONS\nfrom .cftimeindex import CFTimeIndex\n\n_ONE_MICRO = 1\n_ONE_MILLI = _ONE_MICRO * 1000\n_ONE_SECOND = _ONE_MILLI * 1000\n_ONE_MINUTE = 60 * _ONE_SECOND\n_ONE_HOUR = 60 * _ONE_MINUTE\n_ONE_DAY = 24 * _ONE_HOUR\n\n\ndef infer_freq(index):\n    """"""\n    Infer the most likely frequency given the input index.\n\n    Parameters\n    ----------\n    index : CFTimeIndex, DataArray, pd.DatetimeIndex, pd.TimedeltaIndex, pd.Series\n      If not passed a CFTimeIndex, this simply calls `pandas.infer_freq`.\n      If passed a Series or a DataArray will use the values of the series (NOT THE INDEX).\n\n    Returns\n    -------\n    str or None\n        None if no discernible frequency.\n\n    Raises\n    ------\n    TypeError\n        If the index is not datetime-like.\n    ValueError\n        If there are fewer than three values or the index is not 1D.\n    """"""\n    from xarray.core.dataarray import DataArray\n\n    if isinstance(index, (DataArray, pd.Series)):\n        if index.ndim != 1:\n            raise ValueError(""\'index\' must be 1D"")\n        elif not _contains_datetime_like_objects(DataArray(index)):\n            raise ValueError(""\'index\' must contain datetime-like objects"")\n        dtype = np.asarray(index).dtype\n        if dtype == ""datetime64[ns]"":\n            index = pd.DatetimeIndex(index.values)\n        elif dtype == ""timedelta64[ns]"":\n            index = pd.TimedeltaIndex(index.values)\n        else:\n            index = CFTimeIndex(index.values)\n\n    if isinstance(index, CFTimeIndex):\n        inferer = _CFTimeFrequencyInferer(index)\n        return inferer.get_freq()\n\n    return pd.infer_freq(index)\n\n\nclass _CFTimeFrequencyInferer:  # (pd.tseries.frequencies._FrequencyInferer):\n    def __init__(self, index):\n        self.index = index\n        self.values = index.asi8\n\n        if len(index) < 3:\n            raise ValueError(""Need at least 3 dates to infer frequency"")\n\n        self.is_monotonic = (\n            self.index.is_monotonic_decreasing or self.index.is_monotonic_increasing\n        )\n\n        self._deltas = None\n        self._year_deltas = None\n        self._month_deltas = None\n\n    def get_freq(self):\n        """"""Find the appropriate frequency string to describe the inferred frequency of self.index\n\n        Adapted from `pandas.tsseries.frequencies._FrequencyInferer.get_freq` for CFTimeIndexes.\n\n        Returns\n        -------\n        str or None\n        """"""\n        if not self.is_monotonic or not self.index.is_unique:\n            return None\n\n        delta = self.deltas[0]  # Smallest delta\n        if _is_multiple(delta, _ONE_DAY):\n            return self._infer_daily_rule()\n        # There is no possible intraday frequency with a non-unique delta\n        # Different from pandas: we don\'t need to manage DST and business offsets in cftime\n        elif not len(self.deltas) == 1:\n            return None\n\n        if _is_multiple(delta, _ONE_HOUR):\n            return _maybe_add_count(""H"", delta / _ONE_HOUR)\n        elif _is_multiple(delta, _ONE_MINUTE):\n            return _maybe_add_count(""T"", delta / _ONE_MINUTE)\n        elif _is_multiple(delta, _ONE_SECOND):\n            return _maybe_add_count(""S"", delta / _ONE_SECOND)\n        elif _is_multiple(delta, _ONE_MILLI):\n            return _maybe_add_count(""L"", delta / _ONE_MILLI)\n        else:\n            return _maybe_add_count(""U"", delta / _ONE_MICRO)\n\n    def _infer_daily_rule(self):\n        annual_rule = self._get_annual_rule()\n        if annual_rule:\n            nyears = self.year_deltas[0]\n            month = _MONTH_ABBREVIATIONS[self.index[0].month]\n            alias = f""{annual_rule}-{month}""\n            return _maybe_add_count(alias, nyears)\n\n        quartely_rule = self._get_quartely_rule()\n        if quartely_rule:\n            nquarters = self.month_deltas[0] / 3\n            mod_dict = {0: 12, 2: 11, 1: 10}\n            month = _MONTH_ABBREVIATIONS[mod_dict[self.index[0].month % 3]]\n            alias = f""{quartely_rule}-{month}""\n            return _maybe_add_count(alias, nquarters)\n\n        monthly_rule = self._get_monthly_rule()\n        if monthly_rule:\n            return _maybe_add_count(monthly_rule, self.month_deltas[0])\n\n        if len(self.deltas) == 1:\n            # Daily as there is no ""Weekly"" offsets with CFTime\n            days = self.deltas[0] / _ONE_DAY\n            return _maybe_add_count(""D"", days)\n\n        # CFTime has no business freq and no ""week of month"" (WOM)\n        return None\n\n    def _get_annual_rule(self):\n        if len(self.year_deltas) > 1:\n            return None\n\n        if len(np.unique(self.index.month)) > 1:\n            return None\n\n        return {""cs"": ""AS"", ""ce"": ""A""}.get(month_anchor_check(self.index))\n\n    def _get_quartely_rule(self):\n        if len(self.month_deltas) > 1:\n            return None\n\n        if not self.month_deltas[0] % 3 == 0:\n            return None\n\n        return {""cs"": ""QS"", ""ce"": ""Q""}.get(month_anchor_check(self.index))\n\n    def _get_monthly_rule(self):\n        if len(self.month_deltas) > 1:\n            return None\n\n        return {""cs"": ""MS"", ""ce"": ""M""}.get(month_anchor_check(self.index))\n\n    @property\n    def deltas(self):\n        """"""Sorted unique timedeltas as microseconds.""""""\n        if self._deltas is None:\n            self._deltas = _unique_deltas(self.values)\n        return self._deltas\n\n    @property\n    def year_deltas(self):\n        """"""Sorted unique year deltas.""""""\n        if self._year_deltas is None:\n            self._year_deltas = _unique_deltas(self.index.year)\n        return self._year_deltas\n\n    @property\n    def month_deltas(self):\n        """"""Sorted unique month deltas.""""""\n        if self._month_deltas is None:\n            self._month_deltas = _unique_deltas(self.index.year * 12 + self.index.month)\n        return self._month_deltas\n\n\ndef _unique_deltas(arr):\n    """"""Sorted unique deltas of numpy array""""""\n    return np.sort(np.unique(np.diff(arr)))\n\n\ndef _is_multiple(us, mult: int):\n    """"""Whether us is a multiple of mult""""""\n    return us % mult == 0\n\n\ndef _maybe_add_count(base: str, count: float):\n    """"""If count is greater than 1, add it to the base offset string""""""\n    if count != 1:\n        assert count == int(count)\n        count = int(count)\n        return f""{count}{base}""\n    else:\n        return base\n\n\ndef month_anchor_check(dates):\n    """"""Return the monthly offset string.\n\n    Return ""cs"" if all dates are the first days of the month,\n    ""ce"" if all dates are the last day of the month,\n    None otherwise.\n\n    Replicated pandas._libs.tslibs.resolution.month_position_check\n    but without business offset handling.\n    """"""\n    calendar_end = True\n    calendar_start = True\n\n    for date in dates:\n        if calendar_start:\n            calendar_start &= date.day == 1\n\n        if calendar_end:\n            cal = date.day == date.daysinmonth\n            if calendar_end:\n                calendar_end &= cal\n        elif not calendar_start:\n            break\n\n    if calendar_end:\n        return ""ce""\n    elif calendar_start:\n        return ""cs""\n    else:\n        return None\n'"
xarray/coding/strings.py,14,"b'""""""Coders for strings.""""""\nfrom functools import partial\n\nimport numpy as np\n\nfrom ..core import indexing\nfrom ..core.pycompat import dask_array_type\nfrom ..core.variable import Variable\nfrom .variables import (\n    VariableCoder,\n    lazy_elemwise_func,\n    pop_to,\n    safe_setitem,\n    unpack_for_decoding,\n    unpack_for_encoding,\n)\n\n\ndef create_vlen_dtype(element_type):\n    # based on h5py.special_dtype\n    return np.dtype(""O"", metadata={""element_type"": element_type})\n\n\ndef check_vlen_dtype(dtype):\n    if dtype.kind != ""O"" or dtype.metadata is None:\n        return None\n    else:\n        return dtype.metadata.get(""element_type"")\n\n\ndef is_unicode_dtype(dtype):\n    return dtype.kind == ""U"" or check_vlen_dtype(dtype) == str\n\n\ndef is_bytes_dtype(dtype):\n    return dtype.kind == ""S"" or check_vlen_dtype(dtype) == bytes\n\n\nclass EncodedStringCoder(VariableCoder):\n    """"""Transforms between unicode strings and fixed-width UTF-8 bytes.""""""\n\n    def __init__(self, allows_unicode=True):\n        self.allows_unicode = allows_unicode\n\n    def encode(self, variable, name=None):\n        dims, data, attrs, encoding = unpack_for_encoding(variable)\n\n        contains_unicode = is_unicode_dtype(data.dtype)\n        encode_as_char = encoding.get(""dtype"") == ""S1""\n\n        if encode_as_char:\n            del encoding[""dtype""]  # no longer relevant\n\n        if contains_unicode and (encode_as_char or not self.allows_unicode):\n            if ""_FillValue"" in attrs:\n                raise NotImplementedError(\n                    ""variable {!r} has a _FillValue specified, but ""\n                    ""_FillValue is not yet supported on unicode strings: ""\n                    ""https://github.com/pydata/xarray/issues/1647"".format(name)\n                )\n\n            string_encoding = encoding.pop(""_Encoding"", ""utf-8"")\n            safe_setitem(attrs, ""_Encoding"", string_encoding, name=name)\n            # TODO: figure out how to handle this in a lazy way with dask\n            data = encode_string_array(data, string_encoding)\n\n        return Variable(dims, data, attrs, encoding)\n\n    def decode(self, variable, name=None):\n        dims, data, attrs, encoding = unpack_for_decoding(variable)\n\n        if ""_Encoding"" in attrs:\n            string_encoding = pop_to(attrs, encoding, ""_Encoding"")\n            func = partial(decode_bytes_array, encoding=string_encoding)\n            data = lazy_elemwise_func(data, func, np.dtype(object))\n\n        return Variable(dims, data, attrs, encoding)\n\n\ndef decode_bytes_array(bytes_array, encoding=""utf-8""):\n    # This is faster than using np.char.decode() or np.vectorize()\n    bytes_array = np.asarray(bytes_array)\n    decoded = [x.decode(encoding) for x in bytes_array.ravel()]\n    return np.array(decoded, dtype=object).reshape(bytes_array.shape)\n\n\ndef encode_string_array(string_array, encoding=""utf-8""):\n    string_array = np.asarray(string_array)\n    encoded = [x.encode(encoding) for x in string_array.ravel()]\n    return np.array(encoded, dtype=bytes).reshape(string_array.shape)\n\n\ndef ensure_fixed_length_bytes(var):\n    """"""Ensure that a variable with vlen bytes is converted to fixed width.""""""\n    dims, data, attrs, encoding = unpack_for_encoding(var)\n    if check_vlen_dtype(data.dtype) == bytes:\n        # TODO: figure out how to handle this with dask\n        data = np.asarray(data, dtype=np.string_)\n    return Variable(dims, data, attrs, encoding)\n\n\nclass CharacterArrayCoder(VariableCoder):\n    """"""Transforms between arrays containing bytes and character arrays.""""""\n\n    def encode(self, variable, name=None):\n        variable = ensure_fixed_length_bytes(variable)\n\n        dims, data, attrs, encoding = unpack_for_encoding(variable)\n        if data.dtype.kind == ""S"" and encoding.get(""dtype"") is not str:\n            data = bytes_to_char(data)\n            if ""char_dim_name"" in encoding.keys():\n                char_dim_name = encoding.pop(""char_dim_name"")\n            else:\n                char_dim_name = ""string%s"" % data.shape[-1]\n            dims = dims + (char_dim_name,)\n        return Variable(dims, data, attrs, encoding)\n\n    def decode(self, variable, name=None):\n        dims, data, attrs, encoding = unpack_for_decoding(variable)\n\n        if data.dtype == ""S1"" and dims:\n            encoding[""char_dim_name""] = dims[-1]\n            dims = dims[:-1]\n            data = char_to_bytes(data)\n        return Variable(dims, data, attrs, encoding)\n\n\ndef bytes_to_char(arr):\n    """"""Convert numpy/dask arrays from fixed width bytes to characters.""""""\n    if arr.dtype.kind != ""S"":\n        raise ValueError(""argument must have a fixed-width bytes dtype"")\n\n    if isinstance(arr, dask_array_type):\n        import dask.array as da\n\n        return da.map_blocks(\n            _numpy_bytes_to_char,\n            arr,\n            dtype=""S1"",\n            chunks=arr.chunks + ((arr.dtype.itemsize,)),\n            new_axis=[arr.ndim],\n        )\n    else:\n        return _numpy_bytes_to_char(arr)\n\n\ndef _numpy_bytes_to_char(arr):\n    """"""Like netCDF4.stringtochar, but faster and more flexible.\n    """"""\n    # ensure the array is contiguous\n    arr = np.array(arr, copy=False, order=""C"", dtype=np.string_)\n    return arr.reshape(arr.shape + (1,)).view(""S1"")\n\n\ndef char_to_bytes(arr):\n    """"""Convert numpy/dask arrays from characters to fixed width bytes.""""""\n    if arr.dtype != ""S1"":\n        raise ValueError(""argument must have dtype=\'S1\'"")\n\n    if not arr.ndim:\n        # no dimension to concatenate along\n        return arr\n\n    size = arr.shape[-1]\n\n    if not size:\n        # can\'t make an S0 dtype\n        return np.zeros(arr.shape[:-1], dtype=np.string_)\n\n    if isinstance(arr, dask_array_type):\n        import dask.array as da\n\n        if len(arr.chunks[-1]) > 1:\n            raise ValueError(\n                ""cannot stacked dask character array with ""\n                ""multiple chunks in the last dimension: {}"".format(arr)\n            )\n\n        dtype = np.dtype(""S"" + str(arr.shape[-1]))\n        return da.map_blocks(\n            _numpy_char_to_bytes,\n            arr,\n            dtype=dtype,\n            chunks=arr.chunks[:-1],\n            drop_axis=[arr.ndim - 1],\n        )\n    else:\n        return StackedBytesArray(arr)\n\n\ndef _numpy_char_to_bytes(arr):\n    """"""Like netCDF4.chartostring, but faster and more flexible.\n    """"""\n    # based on: http://stackoverflow.com/a/10984878/809705\n    arr = np.array(arr, copy=False, order=""C"")\n    dtype = ""S"" + str(arr.shape[-1])\n    return arr.view(dtype).reshape(arr.shape[:-1])\n\n\nclass StackedBytesArray(indexing.ExplicitlyIndexedNDArrayMixin):\n    """"""Wrapper around array-like objects to create a new indexable object where\n    values, when accessed, are automatically stacked along the last dimension.\n\n    >>> StackedBytesArray(np.array([""a"", ""b"", ""c""]))[:]\n    array(\'abc\',\n          dtype=\'|S3\')\n    """"""\n\n    def __init__(self, array):\n        """"""\n        Parameters\n        ----------\n        array : array-like\n            Original array of values to wrap.\n        """"""\n        if array.dtype != ""S1"":\n            raise ValueError(\n                ""can only use StackedBytesArray if argument has dtype=\'S1\'""\n            )\n        self.array = indexing.as_indexable(array)\n\n    @property\n    def dtype(self):\n        return np.dtype(""S"" + str(self.array.shape[-1]))\n\n    @property\n    def shape(self):\n        return self.array.shape[:-1]\n\n    def __repr__(self):\n        return ""{}({!r})"".format(type(self).__name__, self.array)\n\n    def __getitem__(self, key):\n        # require slicing the last dimension completely\n        key = type(key)(indexing.expanded_indexer(key.tuple, self.array.ndim))\n        if key.tuple[-1] != slice(None):\n            raise IndexError(""too many indices"")\n        return _numpy_char_to_bytes(self.array[key])\n'"
xarray/coding/times.py,39,"b'import re\nimport warnings\nfrom datetime import datetime\nfrom distutils.version import LooseVersion\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.errors import OutOfBoundsDatetime\n\nfrom ..core import indexing\nfrom ..core.common import contains_cftime_datetimes\nfrom ..core.formatting import first_n_items, format_timestamp, last_item\nfrom ..core.variable import Variable\nfrom .variables import (\n    SerializationWarning,\n    VariableCoder,\n    lazy_elemwise_func,\n    pop_to,\n    safe_setitem,\n    unpack_for_decoding,\n    unpack_for_encoding,\n)\n\n# standard calendars recognized by cftime\n_STANDARD_CALENDARS = {""standard"", ""gregorian"", ""proleptic_gregorian""}\n\n_NS_PER_TIME_DELTA = {\n    ""us"": int(1e3),\n    ""ms"": int(1e6),\n    ""s"": int(1e9),\n    ""m"": int(1e9) * 60,\n    ""h"": int(1e9) * 60 * 60,\n    ""D"": int(1e9) * 60 * 60 * 24,\n}\n\nTIME_UNITS = frozenset(\n    [""days"", ""hours"", ""minutes"", ""seconds"", ""milliseconds"", ""microseconds""]\n)\n\n\ndef _netcdf_to_numpy_timeunit(units):\n    units = units.lower()\n    if not units.endswith(""s""):\n        units = ""%ss"" % units\n    return {\n        ""microseconds"": ""us"",\n        ""milliseconds"": ""ms"",\n        ""seconds"": ""s"",\n        ""minutes"": ""m"",\n        ""hours"": ""h"",\n        ""days"": ""D"",\n    }[units]\n\n\ndef _unpack_netcdf_time_units(units):\n    # CF datetime units follow the format: ""UNIT since DATE""\n    # this parses out the unit and date allowing for extraneous\n    # whitespace.\n    matches = re.match(""(.+) since (.+)"", units)\n    if not matches:\n        raise ValueError(""invalid time units: %s"" % units)\n    delta_units, ref_date = [s.strip() for s in matches.groups()]\n    return delta_units, ref_date\n\n\ndef _decode_cf_datetime_dtype(data, units, calendar, use_cftime):\n    # Verify that at least the first and last date can be decoded\n    # successfully. Otherwise, tracebacks end up swallowed by\n    # Dataset.__repr__ when users try to view their lazily decoded array.\n    values = indexing.ImplicitToExplicitIndexingAdapter(indexing.as_indexable(data))\n    example_value = np.concatenate(\n        [first_n_items(values, 1) or [0], last_item(values) or [0]]\n    )\n\n    try:\n        result = decode_cf_datetime(example_value, units, calendar, use_cftime)\n    except Exception:\n        calendar_msg = (\n            ""the default calendar"" if calendar is None else ""calendar %r"" % calendar\n        )\n        msg = (\n            ""unable to decode time units %r with %s. Try ""\n            ""opening your dataset with decode_times=False."" % (units, calendar_msg)\n        )\n        raise ValueError(msg)\n    else:\n        dtype = getattr(result, ""dtype"", np.dtype(""object""))\n\n    return dtype\n\n\ndef _decode_datetime_with_cftime(num_dates, units, calendar):\n    import cftime\n\n    return np.asarray(\n        cftime.num2date(num_dates, units, calendar, only_use_cftime_datetimes=True)\n    )\n\n\ndef _decode_datetime_with_pandas(flat_num_dates, units, calendar):\n    if calendar not in _STANDARD_CALENDARS:\n        raise OutOfBoundsDatetime(\n            ""Cannot decode times from a non-standard calendar, {!r}, using ""\n            ""pandas."".format(calendar)\n        )\n\n    delta, ref_date = _unpack_netcdf_time_units(units)\n    delta = _netcdf_to_numpy_timeunit(delta)\n    try:\n        ref_date = pd.Timestamp(ref_date)\n    except ValueError:\n        # ValueError is raised by pd.Timestamp for non-ISO timestamp\n        # strings, in which case we fall back to using cftime\n        raise OutOfBoundsDatetime\n\n    # fixes: https://github.com/pydata/pandas/issues/14068\n    # these lines check if the the lowest or the highest value in dates\n    # cause an OutOfBoundsDatetime (Overflow) error\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", ""invalid value encountered"", RuntimeWarning)\n        pd.to_timedelta(flat_num_dates.min(), delta) + ref_date\n        pd.to_timedelta(flat_num_dates.max(), delta) + ref_date\n\n    # Cast input dates to integers of nanoseconds because `pd.to_datetime`\n    # works much faster when dealing with integers\n    # make _NS_PER_TIME_DELTA an array to ensure type upcasting\n    flat_num_dates_ns_int = (\n        flat_num_dates.astype(np.float64) * _NS_PER_TIME_DELTA[delta]\n    ).astype(np.int64)\n\n    return (pd.to_timedelta(flat_num_dates_ns_int, ""ns"") + ref_date).values\n\n\ndef decode_cf_datetime(num_dates, units, calendar=None, use_cftime=None):\n    """"""Given an array of numeric dates in netCDF format, convert it into a\n    numpy array of date time objects.\n\n    For standard (Gregorian) calendars, this function uses vectorized\n    operations, which makes it much faster than cftime.num2date. In such a\n    case, the returned array will be of type np.datetime64.\n\n    Note that time unit in `units` must not be smaller than microseconds and\n    not larger than days.\n\n    See also\n    --------\n    cftime.num2date\n    """"""\n    num_dates = np.asarray(num_dates)\n    flat_num_dates = num_dates.ravel()\n    if calendar is None:\n        calendar = ""standard""\n\n    if use_cftime is None:\n        try:\n            dates = _decode_datetime_with_pandas(flat_num_dates, units, calendar)\n        except (KeyError, OutOfBoundsDatetime, OverflowError):\n            dates = _decode_datetime_with_cftime(\n                flat_num_dates.astype(np.float), units, calendar\n            )\n\n            if (\n                dates[np.nanargmin(num_dates)].year < 1678\n                or dates[np.nanargmax(num_dates)].year >= 2262\n            ):\n                if calendar in _STANDARD_CALENDARS:\n                    warnings.warn(\n                        ""Unable to decode time axis into full ""\n                        ""numpy.datetime64 objects, continuing using ""\n                        ""cftime.datetime objects instead, reason: dates out ""\n                        ""of range"",\n                        SerializationWarning,\n                        stacklevel=3,\n                    )\n            else:\n                if calendar in _STANDARD_CALENDARS:\n                    dates = cftime_to_nptime(dates)\n    elif use_cftime:\n        dates = _decode_datetime_with_cftime(\n            flat_num_dates.astype(np.float), units, calendar\n        )\n    else:\n        dates = _decode_datetime_with_pandas(flat_num_dates, units, calendar)\n\n    return dates.reshape(num_dates.shape)\n\n\ndef to_timedelta_unboxed(value, **kwargs):\n    if LooseVersion(pd.__version__) < ""0.25.0"":\n        result = pd.to_timedelta(value, **kwargs, box=False)\n    else:\n        result = pd.to_timedelta(value, **kwargs).to_numpy()\n    assert result.dtype == ""timedelta64[ns]""\n    return result\n\n\ndef to_datetime_unboxed(value, **kwargs):\n    if LooseVersion(pd.__version__) < ""0.25.0"":\n        result = pd.to_datetime(value, **kwargs, box=False)\n    else:\n        result = pd.to_datetime(value, **kwargs).to_numpy()\n    assert result.dtype == ""datetime64[ns]""\n    return result\n\n\ndef decode_cf_timedelta(num_timedeltas, units):\n    """"""Given an array of numeric timedeltas in netCDF format, convert it into a\n    numpy timedelta64[ns] array.\n    """"""\n    num_timedeltas = np.asarray(num_timedeltas)\n    units = _netcdf_to_numpy_timeunit(units)\n    result = to_timedelta_unboxed(num_timedeltas.ravel(), unit=units)\n    return result.reshape(num_timedeltas.shape)\n\n\ndef _infer_time_units_from_diff(unique_timedeltas):\n    for time_unit in [""days"", ""hours"", ""minutes"", ""seconds""]:\n        delta_ns = _NS_PER_TIME_DELTA[_netcdf_to_numpy_timeunit(time_unit)]\n        unit_delta = np.timedelta64(delta_ns, ""ns"")\n        diffs = unique_timedeltas / unit_delta\n        if np.all(diffs == diffs.astype(int)):\n            return time_unit\n    return ""seconds""\n\n\ndef infer_calendar_name(dates):\n    """"""Given an array of datetimes, infer the CF calendar name""""""\n    if np.asarray(dates).dtype == ""datetime64[ns]"":\n        return ""proleptic_gregorian""\n    else:\n        return np.asarray(dates).ravel()[0].calendar\n\n\ndef infer_datetime_units(dates):\n    """"""Given an array of datetimes, returns a CF compatible time-unit string of\n    the form ""{time_unit} since {date[0]}"", where `time_unit` is \'days\',\n    \'hours\', \'minutes\' or \'seconds\' (the first one that can evenly divide all\n    unique time deltas in `dates`)\n    """"""\n    dates = np.asarray(dates).ravel()\n    if np.asarray(dates).dtype == ""datetime64[ns]"":\n        dates = to_datetime_unboxed(dates)\n        dates = dates[pd.notnull(dates)]\n        reference_date = dates[0] if len(dates) > 0 else ""1970-01-01""\n        reference_date = pd.Timestamp(reference_date)\n    else:\n        reference_date = dates[0] if len(dates) > 0 else ""1970-01-01""\n        reference_date = format_cftime_datetime(reference_date)\n    unique_timedeltas = np.unique(np.diff(dates))\n    if unique_timedeltas.dtype == np.dtype(""O""):\n        # Convert to np.timedelta64 objects using pandas to work around a\n        # NumPy casting bug: https://github.com/numpy/numpy/issues/11096\n        unique_timedeltas = to_timedelta_unboxed(unique_timedeltas)\n    units = _infer_time_units_from_diff(unique_timedeltas)\n    return f""{units} since {reference_date}""\n\n\ndef format_cftime_datetime(date):\n    """"""Converts a cftime.datetime object to a string with the format:\n    YYYY-MM-DD HH:MM:SS.UUUUUU\n    """"""\n    return ""{:04d}-{:02d}-{:02d} {:02d}:{:02d}:{:02d}.{:06d}"".format(\n        date.year,\n        date.month,\n        date.day,\n        date.hour,\n        date.minute,\n        date.second,\n        date.microsecond,\n    )\n\n\ndef infer_timedelta_units(deltas):\n    """"""Given an array of timedeltas, returns a CF compatible time-unit from\n    {\'days\', \'hours\', \'minutes\' \'seconds\'} (the first one that can evenly\n    divide all unique time deltas in `deltas`)\n    """"""\n    deltas = to_timedelta_unboxed(np.asarray(deltas).ravel())\n    unique_timedeltas = np.unique(deltas[pd.notnull(deltas)])\n    units = _infer_time_units_from_diff(unique_timedeltas)\n    return units\n\n\ndef cftime_to_nptime(times):\n    """"""Given an array of cftime.datetime objects, return an array of\n    numpy.datetime64 objects of the same size""""""\n    times = np.asarray(times)\n    new = np.empty(times.shape, dtype=""M8[ns]"")\n    for i, t in np.ndenumerate(times):\n        try:\n            # Use pandas.Timestamp in place of datetime.datetime, because\n            # NumPy casts it safely it np.datetime64[ns] for dates outside\n            # 1678 to 2262 (this is not currently the case for\n            # datetime.datetime).\n            dt = pd.Timestamp(\n                t.year, t.month, t.day, t.hour, t.minute, t.second, t.microsecond\n            )\n        except ValueError as e:\n            raise ValueError(\n                ""Cannot convert date {} to a date in the ""\n                ""standard calendar.  Reason: {}."".format(t, e)\n            )\n        new[i] = np.datetime64(dt)\n    return new\n\n\ndef _cleanup_netcdf_time_units(units):\n    delta, ref_date = _unpack_netcdf_time_units(units)\n    try:\n        units = ""{} since {}"".format(delta, format_timestamp(ref_date))\n    except OutOfBoundsDatetime:\n        # don\'t worry about reifying the units if they\'re out of bounds\n        pass\n    return units\n\n\ndef _encode_datetime_with_cftime(dates, units, calendar):\n    """"""Fallback method for encoding dates using cftime.\n\n    This method is more flexible than xarray\'s parsing using datetime64[ns]\n    arrays but also slower because it loops over each element.\n    """"""\n    import cftime\n\n    if np.issubdtype(dates.dtype, np.datetime64):\n        # numpy\'s broken datetime conversion only works for us precision\n        dates = dates.astype(""M8[us]"").astype(datetime)\n\n    def encode_datetime(d):\n        return np.nan if d is None else cftime.date2num(d, units, calendar)\n\n    return np.vectorize(encode_datetime)(dates)\n\n\ndef cast_to_int_if_safe(num):\n    int_num = np.array(num, dtype=np.int64)\n    if (num == int_num).all():\n        num = int_num\n    return num\n\n\ndef encode_cf_datetime(dates, units=None, calendar=None):\n    """"""Given an array of datetime objects, returns the tuple `(num, units,\n    calendar)` suitable for a CF compliant time variable.\n\n    Unlike `date2num`, this function can handle datetime64 arrays.\n\n    See also\n    --------\n    cftime.date2num\n    """"""\n    dates = np.asarray(dates)\n\n    if units is None:\n        units = infer_datetime_units(dates)\n    else:\n        units = _cleanup_netcdf_time_units(units)\n\n    if calendar is None:\n        calendar = infer_calendar_name(dates)\n\n    delta, ref_date = _unpack_netcdf_time_units(units)\n    try:\n        if calendar not in _STANDARD_CALENDARS or dates.dtype.kind == ""O"":\n            # parse with cftime instead\n            raise OutOfBoundsDatetime\n        assert dates.dtype == ""datetime64[ns]""\n\n        delta_units = _netcdf_to_numpy_timeunit(delta)\n        time_delta = np.timedelta64(1, delta_units).astype(""timedelta64[ns]"")\n        ref_date = pd.Timestamp(ref_date)\n\n        # If the ref_date Timestamp is timezone-aware, convert to UTC and\n        # make it timezone-naive (GH 2649).\n        if ref_date.tz is not None:\n            ref_date = ref_date.tz_convert(None)\n\n        # Wrap the dates in a DatetimeIndex to do the subtraction to ensure\n        # an OverflowError is raised if the ref_date is too far away from\n        # dates to be encoded (GH 2272).\n        num = (pd.DatetimeIndex(dates.ravel()) - ref_date) / time_delta\n        num = num.values.reshape(dates.shape)\n\n    except (OutOfBoundsDatetime, OverflowError):\n        num = _encode_datetime_with_cftime(dates, units, calendar)\n\n    num = cast_to_int_if_safe(num)\n    return (num, units, calendar)\n\n\ndef encode_cf_timedelta(timedeltas, units=None):\n    if units is None:\n        units = infer_timedelta_units(timedeltas)\n\n    np_unit = _netcdf_to_numpy_timeunit(units)\n    num = 1.0 * timedeltas / np.timedelta64(1, np_unit)\n    num = np.where(pd.isnull(timedeltas), np.nan, num)\n    num = cast_to_int_if_safe(num)\n    return (num, units)\n\n\nclass CFDatetimeCoder(VariableCoder):\n    def __init__(self, use_cftime=None):\n        self.use_cftime = use_cftime\n\n    def encode(self, variable, name=None):\n        dims, data, attrs, encoding = unpack_for_encoding(variable)\n        if np.issubdtype(data.dtype, np.datetime64) or contains_cftime_datetimes(\n            variable\n        ):\n            (data, units, calendar) = encode_cf_datetime(\n                data, encoding.pop(""units"", None), encoding.pop(""calendar"", None)\n            )\n            safe_setitem(attrs, ""units"", units, name=name)\n            safe_setitem(attrs, ""calendar"", calendar, name=name)\n\n        return Variable(dims, data, attrs, encoding)\n\n    def decode(self, variable, name=None):\n        dims, data, attrs, encoding = unpack_for_decoding(variable)\n\n        if ""units"" in attrs and ""since"" in attrs[""units""]:\n            units = pop_to(attrs, encoding, ""units"")\n            calendar = pop_to(attrs, encoding, ""calendar"")\n            dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n            transform = partial(\n                decode_cf_datetime,\n                units=units,\n                calendar=calendar,\n                use_cftime=self.use_cftime,\n            )\n            data = lazy_elemwise_func(data, transform, dtype)\n\n        return Variable(dims, data, attrs, encoding)\n\n\nclass CFTimedeltaCoder(VariableCoder):\n    def encode(self, variable, name=None):\n        dims, data, attrs, encoding = unpack_for_encoding(variable)\n\n        if np.issubdtype(data.dtype, np.timedelta64):\n            data, units = encode_cf_timedelta(data, encoding.pop(""units"", None))\n            safe_setitem(attrs, ""units"", units, name=name)\n\n        return Variable(dims, data, attrs, encoding)\n\n    def decode(self, variable, name=None):\n        dims, data, attrs, encoding = unpack_for_decoding(variable)\n\n        if ""units"" in attrs and attrs[""units""] in TIME_UNITS:\n            units = pop_to(attrs, encoding, ""units"")\n            transform = partial(decode_cf_timedelta, units=units)\n            dtype = np.dtype(""timedelta64[ns]"")\n            data = lazy_elemwise_func(data, transform, dtype=dtype)\n\n        return Variable(dims, data, attrs, encoding)\n'"
xarray/coding/variables.py,17,"b'""""""Coders for individual Variable objects.""""""\nimport warnings\nfrom functools import partial\nfrom typing import Any, Hashable\n\nimport numpy as np\nimport pandas as pd\n\nfrom ..core import dtypes, duck_array_ops, indexing\nfrom ..core.pycompat import dask_array_type\nfrom ..core.variable import Variable\n\n\nclass SerializationWarning(RuntimeWarning):\n    """"""Warnings about encoding/decoding issues in serialization.""""""\n\n\nclass VariableCoder:\n    """"""Base class for encoding and decoding transformations on variables.\n\n    We use coders for transforming variables between xarray\'s data model and\n    a format suitable for serialization. For example, coders apply CF\n    conventions for how data should be represented in netCDF files.\n\n    Subclasses should implement encode() and decode(), which should satisfy\n    the identity ``coder.decode(coder.encode(variable)) == variable``. If any\n    options are necessary, they should be implemented as arguments to the\n    __init__ method.\n\n    The optional name argument to encode() and decode() exists solely for the\n    sake of better error messages, and should correspond to the name of\n    variables in the underlying store.\n    """"""\n\n    def encode(\n        self, variable: Variable, name: Hashable = None\n    ) -> Variable:  # pragma: no cover\n        """"""Convert an encoded variable to a decoded variable\n        """"""\n        raise NotImplementedError()\n\n    def decode(\n        self, variable: Variable, name: Hashable = None\n    ) -> Variable:  # pragma: no cover\n        """"""Convert an decoded variable to a encoded variable\n        """"""\n        raise NotImplementedError()\n\n\nclass _ElementwiseFunctionArray(indexing.ExplicitlyIndexedNDArrayMixin):\n    """"""Lazily computed array holding values of elemwise-function.\n\n    Do not construct this object directly: call lazy_elemwise_func instead.\n\n    Values are computed upon indexing or coercion to a NumPy array.\n    """"""\n\n    def __init__(self, array, func, dtype):\n        assert not isinstance(array, dask_array_type)\n        self.array = indexing.as_indexable(array)\n        self.func = func\n        self._dtype = dtype\n\n    @property\n    def dtype(self):\n        return np.dtype(self._dtype)\n\n    def __getitem__(self, key):\n        return type(self)(self.array[key], self.func, self.dtype)\n\n    def __array__(self, dtype=None):\n        return self.func(self.array)\n\n    def __repr__(self):\n        return ""{}({!r}, func={!r}, dtype={!r})"".format(\n            type(self).__name__, self.array, self.func, self.dtype\n        )\n\n\ndef lazy_elemwise_func(array, func, dtype):\n    """"""Lazily apply an element-wise function to an array.\n\n    Parameters\n    ----------\n    array : any valid value of Variable._data\n    func : callable\n        Function to apply to indexed slices of an array. For use with dask,\n        this should be a pickle-able object.\n    dtype : coercible to np.dtype\n        Dtype for the result of this function.\n\n    Returns\n    -------\n    Either a dask.array.Array or _ElementwiseFunctionArray.\n    """"""\n    if isinstance(array, dask_array_type):\n        return array.map_blocks(func, dtype=dtype)\n    else:\n        return _ElementwiseFunctionArray(array, func, dtype)\n\n\ndef unpack_for_encoding(var):\n    return var.dims, var.data, var.attrs.copy(), var.encoding.copy()\n\n\ndef unpack_for_decoding(var):\n    return var.dims, var._data, var.attrs.copy(), var.encoding.copy()\n\n\ndef safe_setitem(dest, key, value, name=None):\n    if key in dest:\n        var_str = f"" on variable {name!r}"" if name else """"\n        raise ValueError(\n            ""failed to prevent overwriting existing key {} in attrs{}. ""\n            ""This is probably an encoding field used by xarray to describe ""\n            ""how a variable is serialized. To proceed, remove this key from ""\n            ""the variable\'s attributes manually."".format(key, var_str)\n        )\n    dest[key] = value\n\n\ndef pop_to(source, dest, key, name=None):\n    """"""\n    A convenience function which pops a key k from source to dest.\n    None values are not passed on.  If k already exists in dest an\n    error is raised.\n    """"""\n    value = source.pop(key, None)\n    if value is not None:\n        safe_setitem(dest, key, value, name=name)\n    return value\n\n\ndef _apply_mask(\n    data: np.ndarray, encoded_fill_values: list, decoded_fill_value: Any, dtype: Any\n) -> np.ndarray:\n    """"""Mask all matching values in a NumPy arrays.""""""\n    data = np.asarray(data, dtype=dtype)\n    condition = False\n    for fv in encoded_fill_values:\n        condition |= data == fv\n    return np.where(condition, decoded_fill_value, data)\n\n\nclass CFMaskCoder(VariableCoder):\n    """"""Mask or unmask fill values according to CF conventions.""""""\n\n    def encode(self, variable, name=None):\n        dims, data, attrs, encoding = unpack_for_encoding(variable)\n\n        dtype = np.dtype(encoding.get(""dtype"", data.dtype))\n        fv = encoding.get(""_FillValue"")\n        mv = encoding.get(""missing_value"")\n\n        if (\n            fv is not None\n            and mv is not None\n            and not duck_array_ops.allclose_or_equiv(fv, mv)\n        ):\n            raise ValueError(\n                f""Variable {name!r} has conflicting _FillValue ({fv}) and missing_value ({mv}). Cannot encode data.""\n            )\n\n        if fv is not None:\n            # Ensure _FillValue is cast to same dtype as data\'s\n            encoding[""_FillValue""] = dtype.type(fv)\n            fill_value = pop_to(encoding, attrs, ""_FillValue"", name=name)\n            if not pd.isnull(fill_value):\n                data = duck_array_ops.fillna(data, fill_value)\n\n        if mv is not None:\n            # Ensure missing_value is cast to same dtype as data\'s\n            encoding[""missing_value""] = dtype.type(mv)\n            fill_value = pop_to(encoding, attrs, ""missing_value"", name=name)\n            if not pd.isnull(fill_value) and fv is None:\n                data = duck_array_ops.fillna(data, fill_value)\n\n        return Variable(dims, data, attrs, encoding)\n\n    def decode(self, variable, name=None):\n        dims, data, attrs, encoding = unpack_for_decoding(variable)\n\n        raw_fill_values = [\n            pop_to(attrs, encoding, attr, name=name)\n            for attr in (""missing_value"", ""_FillValue"")\n        ]\n        if raw_fill_values:\n            encoded_fill_values = {\n                fv\n                for option in raw_fill_values\n                for fv in np.ravel(option)\n                if not pd.isnull(fv)\n            }\n\n            if len(encoded_fill_values) > 1:\n                warnings.warn(\n                    ""variable {!r} has multiple fill values {}, ""\n                    ""decoding all values to NaN."".format(name, encoded_fill_values),\n                    SerializationWarning,\n                    stacklevel=3,\n                )\n\n            dtype, decoded_fill_value = dtypes.maybe_promote(data.dtype)\n\n            if encoded_fill_values:\n                transform = partial(\n                    _apply_mask,\n                    encoded_fill_values=encoded_fill_values,\n                    decoded_fill_value=decoded_fill_value,\n                    dtype=dtype,\n                )\n                data = lazy_elemwise_func(data, transform, dtype)\n\n        return Variable(dims, data, attrs, encoding)\n\n\ndef _scale_offset_decoding(data, scale_factor, add_offset, dtype):\n    data = np.array(data, dtype=dtype, copy=True)\n    if scale_factor is not None:\n        data *= scale_factor\n    if add_offset is not None:\n        data += add_offset\n    return data\n\n\ndef _choose_float_dtype(dtype, has_offset):\n    """"""Return a float dtype that can losslessly represent `dtype` values.""""""\n    # Keep float32 as-is.  Upcast half-precision to single-precision,\n    # because float16 is ""intended for storage but not computation""\n    if dtype.itemsize <= 4 and np.issubdtype(dtype, np.floating):\n        return np.float32\n    # float32 can exactly represent all integers up to 24 bits\n    if dtype.itemsize <= 2 and np.issubdtype(dtype, np.integer):\n        # A scale factor is entirely safe (vanishing into the mantissa),\n        # but a large integer offset could lead to loss of precision.\n        # Sensitivity analysis can be tricky, so we just use a float64\n        # if there\'s any offset at all - better unoptimised than wrong!\n        if not has_offset:\n            return np.float32\n    # For all other types and circumstances, we just use float64.\n    # (safe because eg. complex numbers are not supported in NetCDF)\n    return np.float64\n\n\nclass CFScaleOffsetCoder(VariableCoder):\n    """"""Scale and offset variables according to CF conventions.\n\n    Follows the formula:\n        decode_values = encoded_values * scale_factor + add_offset\n    """"""\n\n    def encode(self, variable, name=None):\n        dims, data, attrs, encoding = unpack_for_encoding(variable)\n\n        if ""scale_factor"" in encoding or ""add_offset"" in encoding:\n            dtype = _choose_float_dtype(data.dtype, ""add_offset"" in encoding)\n            data = data.astype(dtype=dtype, copy=True)\n            if ""add_offset"" in encoding:\n                data -= pop_to(encoding, attrs, ""add_offset"", name=name)\n            if ""scale_factor"" in encoding:\n                data /= pop_to(encoding, attrs, ""scale_factor"", name=name)\n\n        return Variable(dims, data, attrs, encoding)\n\n    def decode(self, variable, name=None):\n        dims, data, attrs, encoding = unpack_for_decoding(variable)\n\n        if ""scale_factor"" in attrs or ""add_offset"" in attrs:\n            scale_factor = pop_to(attrs, encoding, ""scale_factor"", name=name)\n            add_offset = pop_to(attrs, encoding, ""add_offset"", name=name)\n            dtype = _choose_float_dtype(data.dtype, ""add_offset"" in attrs)\n            transform = partial(\n                _scale_offset_decoding,\n                scale_factor=scale_factor,\n                add_offset=add_offset,\n                dtype=dtype,\n            )\n            data = lazy_elemwise_func(data, transform, dtype)\n\n        return Variable(dims, data, attrs, encoding)\n\n\nclass UnsignedIntegerCoder(VariableCoder):\n    def encode(self, variable, name=None):\n        dims, data, attrs, encoding = unpack_for_encoding(variable)\n\n        # from netCDF best practices\n        # https://www.unidata.ucar.edu/software/netcdf/docs/BestPractices.html\n        #     ""_Unsigned = ""true"" to indicate that\n        #      integer data should be treated as unsigned""\n        if encoding.get(""_Unsigned"", ""false"") == ""true"":\n            pop_to(encoding, attrs, ""_Unsigned"")\n            signed_dtype = np.dtype(""i%s"" % data.dtype.itemsize)\n            if ""_FillValue"" in attrs:\n                new_fill = signed_dtype.type(attrs[""_FillValue""])\n                attrs[""_FillValue""] = new_fill\n            data = duck_array_ops.around(data).astype(signed_dtype)\n\n        return Variable(dims, data, attrs, encoding)\n\n    def decode(self, variable, name=None):\n        dims, data, attrs, encoding = unpack_for_decoding(variable)\n\n        if ""_Unsigned"" in attrs:\n            unsigned = pop_to(attrs, encoding, ""_Unsigned"")\n\n            if data.dtype.kind == ""i"":\n                if unsigned == ""true"":\n                    unsigned_dtype = np.dtype(""u%s"" % data.dtype.itemsize)\n                    transform = partial(np.asarray, dtype=unsigned_dtype)\n                    data = lazy_elemwise_func(data, transform, unsigned_dtype)\n                    if ""_FillValue"" in attrs:\n                        new_fill = unsigned_dtype.type(attrs[""_FillValue""])\n                        attrs[""_FillValue""] = new_fill\n            else:\n                warnings.warn(\n                    ""variable %r has _Unsigned attribute but is not ""\n                    ""of integer type. Ignoring attribute."" % name,\n                    SerializationWarning,\n                    stacklevel=3,\n                )\n\n        return Variable(dims, data, attrs, encoding)\n'"
xarray/core/__init__.py,0,b''
xarray/core/accessor_dt.py,22,"b'import numpy as np\nimport pandas as pd\n\nfrom .common import (\n    _contains_datetime_like_objects,\n    is_np_datetime_like,\n    is_np_timedelta_like,\n)\nfrom .pycompat import dask_array_type\n\n\ndef _season_from_months(months):\n    """"""Compute season (DJF, MAM, JJA, SON) from month ordinal\n    """"""\n    # TODO: Move ""season"" accessor upstream into pandas\n    seasons = np.array([""DJF"", ""MAM"", ""JJA"", ""SON""])\n    months = np.asarray(months)\n    return seasons[(months // 3) % 4]\n\n\ndef _access_through_cftimeindex(values, name):\n    """"""Coerce an array of datetime-like values to a CFTimeIndex\n    and access requested datetime component\n    """"""\n    from ..coding.cftimeindex import CFTimeIndex\n\n    values_as_cftimeindex = CFTimeIndex(values.ravel())\n    if name == ""season"":\n        months = values_as_cftimeindex.month\n        field_values = _season_from_months(months)\n    else:\n        field_values = getattr(values_as_cftimeindex, name)\n    return field_values.reshape(values.shape)\n\n\ndef _access_through_series(values, name):\n    """"""Coerce an array of datetime-like values to a pandas Series and\n    access requested datetime component\n    """"""\n    values_as_series = pd.Series(values.ravel())\n    if name == ""season"":\n        months = values_as_series.dt.month.values\n        field_values = _season_from_months(months)\n    else:\n        field_values = getattr(values_as_series.dt, name).values\n    return field_values.reshape(values.shape)\n\n\ndef _get_date_field(values, name, dtype):\n    """"""Indirectly access pandas\' libts.get_date_field by wrapping data\n    as a Series and calling through `.dt` attribute.\n\n    Parameters\n    ----------\n    values : np.ndarray or dask.array-like\n        Array-like container of datetime-like values\n    name : str\n        Name of datetime field to access\n    dtype : dtype-like\n        dtype for output date field values\n\n    Returns\n    -------\n    datetime_fields : same type as values\n        Array-like of datetime fields accessed for each element in values\n\n    """"""\n    if is_np_datetime_like(values.dtype):\n        access_method = _access_through_series\n    else:\n        access_method = _access_through_cftimeindex\n\n    if isinstance(values, dask_array_type):\n        from dask.array import map_blocks\n\n        return map_blocks(access_method, values, name, dtype=dtype)\n    else:\n        return access_method(values, name)\n\n\ndef _round_through_series_or_index(values, name, freq):\n    """"""Coerce an array of datetime-like values to a pandas Series or xarray\n    CFTimeIndex and apply requested rounding\n    """"""\n    from ..coding.cftimeindex import CFTimeIndex\n\n    if is_np_datetime_like(values.dtype):\n        values_as_series = pd.Series(values.ravel())\n        method = getattr(values_as_series.dt, name)\n    else:\n        values_as_cftimeindex = CFTimeIndex(values.ravel())\n        method = getattr(values_as_cftimeindex, name)\n\n    field_values = method(freq=freq).values\n\n    return field_values.reshape(values.shape)\n\n\ndef _round_field(values, name, freq):\n    """"""Indirectly access rounding functions by wrapping data\n    as a Series or CFTimeIndex\n\n    Parameters\n    ----------\n    values : np.ndarray or dask.array-like\n        Array-like container of datetime-like values\n    name : str (ceil, floor, round)\n        Name of rounding function\n    freq : a freq string indicating the rounding resolution\n\n    Returns\n    -------\n    rounded timestamps : same type as values\n        Array-like of datetime fields accessed for each element in values\n\n    """"""\n    if isinstance(values, dask_array_type):\n        from dask.array import map_blocks\n\n        dtype = np.datetime64 if is_np_datetime_like(values.dtype) else np.dtype(""O"")\n        return map_blocks(\n            _round_through_series_or_index, values, name, freq=freq, dtype=dtype\n        )\n    else:\n        return _round_through_series_or_index(values, name, freq)\n\n\ndef _strftime_through_cftimeindex(values, date_format):\n    """"""Coerce an array of cftime-like values to a CFTimeIndex\n    and access requested datetime component\n    """"""\n    from ..coding.cftimeindex import CFTimeIndex\n\n    values_as_cftimeindex = CFTimeIndex(values.ravel())\n\n    field_values = values_as_cftimeindex.strftime(date_format)\n    return field_values.values.reshape(values.shape)\n\n\ndef _strftime_through_series(values, date_format):\n    """"""Coerce an array of datetime-like values to a pandas Series and\n    apply string formatting\n    """"""\n    values_as_series = pd.Series(values.ravel())\n    strs = values_as_series.dt.strftime(date_format)\n    return strs.values.reshape(values.shape)\n\n\ndef _strftime(values, date_format):\n    if is_np_datetime_like(values.dtype):\n        access_method = _strftime_through_series\n    else:\n        access_method = _strftime_through_cftimeindex\n    if isinstance(values, dask_array_type):\n        from dask.array import map_blocks\n\n        return map_blocks(access_method, values, date_format)\n    else:\n        return access_method(values, date_format)\n\n\nclass Properties:\n    def __init__(self, obj):\n        self._obj = obj\n\n    def _tslib_field_accessor(  # type: ignore\n        name: str, docstring: str = None, dtype: np.dtype = None\n    ):\n        def f(self, dtype=dtype):\n            if dtype is None:\n                dtype = self._obj.dtype\n            obj_type = type(self._obj)\n            result = _get_date_field(self._obj.data, name, dtype)\n            return obj_type(\n                result, name=name, coords=self._obj.coords, dims=self._obj.dims\n            )\n\n        f.__name__ = name\n        f.__doc__ = docstring\n        return property(f)\n\n    def _tslib_round_accessor(self, name, freq):\n        obj_type = type(self._obj)\n        result = _round_field(self._obj.data, name, freq)\n        return obj_type(result, name=name, coords=self._obj.coords, dims=self._obj.dims)\n\n    def floor(self, freq):\n        """"""\n        Round timestamps downward to specified frequency resolution.\n\n        Parameters\n        ----------\n        freq : a freq string indicating the rounding resolution\n            e.g. \'D\' for daily resolution\n\n        Returns\n        -------\n        floor-ed timestamps : same type as values\n            Array-like of datetime fields accessed for each element in values\n        """"""\n\n        return self._tslib_round_accessor(""floor"", freq)\n\n    def ceil(self, freq):\n        """"""\n        Round timestamps upward to specified frequency resolution.\n\n        Parameters\n        ----------\n        freq : a freq string indicating the rounding resolution\n            e.g. \'D\' for daily resolution\n\n        Returns\n        -------\n        ceil-ed timestamps : same type as values\n            Array-like of datetime fields accessed for each element in values\n        """"""\n        return self._tslib_round_accessor(""ceil"", freq)\n\n    def round(self, freq):\n        """"""\n        Round timestamps to specified frequency resolution.\n\n        Parameters\n        ----------\n        freq : a freq string indicating the rounding resolution\n            e.g. \'D\' for daily resolution\n\n        Returns\n        -------\n        rounded timestamps : same type as values\n            Array-like of datetime fields accessed for each element in values\n        """"""\n        return self._tslib_round_accessor(""round"", freq)\n\n\nclass DatetimeAccessor(Properties):\n    """"""Access datetime fields for DataArrays with datetime-like dtypes.\n\n    Fields can be accessed through the `.dt` attribute\n    for applicable DataArrays.\n\n    Notes\n    ------\n    Note that these fields are not calendar-aware; if your datetimes are encoded\n    with a non-Gregorian calendar (e.g. a 360-day calendar) using cftime,\n    then some fields like `dayofyear` may not be accurate.\n\n    Examples\n    ---------\n    >>> import xarray as xr\n    >>> import pandas as pd\n    >>> dates = pd.date_range(start=""2000/01/01"", freq=""D"", periods=10)\n    >>> ts = xr.DataArray(dates, dims=(""time""))\n    >>> ts\n    <xarray.DataArray (time: 10)>\n    array([\'2000-01-01T00:00:00.000000000\', \'2000-01-02T00:00:00.000000000\',\n        \'2000-01-03T00:00:00.000000000\', \'2000-01-04T00:00:00.000000000\',\n        \'2000-01-05T00:00:00.000000000\', \'2000-01-06T00:00:00.000000000\',\n        \'2000-01-07T00:00:00.000000000\', \'2000-01-08T00:00:00.000000000\',\n        \'2000-01-09T00:00:00.000000000\', \'2000-01-10T00:00:00.000000000\'],\n        dtype=\'datetime64[ns]\')\n    Coordinates:\n    * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-10\n    >>> ts.dt\n    <xarray.core.accessor_dt.DatetimeAccessor object at 0x118b54d68>\n    >>> ts.dt.dayofyear\n    <xarray.DataArray \'dayofyear\' (time: 10)>\n    array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n    Coordinates:\n    * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-10\n    >>> ts.dt.quarter\n    <xarray.DataArray \'quarter\' (time: 10)>\n    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n    Coordinates:\n    * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-10\n\n    """"""\n\n    def strftime(self, date_format):\n        \'\'\'\n        Return an array of formatted strings specified by date_format, which\n        supports the same string format as the python standard library. Details\n        of the string format can be found in `python string format doc\n        <https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior>`__\n\n        Parameters\n        ----------\n        date_format : str\n            date format string (e.g. ""%Y-%m-%d"")\n\n        Returns\n        -------\n        formatted strings : same type as values\n            Array-like of strings formatted for each element in values\n\n        Examples\n        --------\n        >>> rng = xr.Dataset({""time"": datetime.datetime(2000, 1, 1)})\n        >>> rng[""time""].dt.strftime(""%B %d, %Y, %r"")\n        <xarray.DataArray \'strftime\' ()>\n        array(\'January 01, 2000, 12:00:00 AM\', dtype=object)\n        """"""\n\n        \'\'\'\n        obj_type = type(self._obj)\n\n        result = _strftime(self._obj.data, date_format)\n\n        return obj_type(\n            result, name=""strftime"", coords=self._obj.coords, dims=self._obj.dims\n        )\n\n    year = Properties._tslib_field_accessor(\n        ""year"", ""The year of the datetime"", np.int64\n    )\n    month = Properties._tslib_field_accessor(\n        ""month"", ""The month as January=1, December=12"", np.int64\n    )\n    day = Properties._tslib_field_accessor(""day"", ""The days of the datetime"", np.int64)\n    hour = Properties._tslib_field_accessor(\n        ""hour"", ""The hours of the datetime"", np.int64\n    )\n    minute = Properties._tslib_field_accessor(\n        ""minute"", ""The minutes of the datetime"", np.int64\n    )\n    second = Properties._tslib_field_accessor(\n        ""second"", ""The seconds of the datetime"", np.int64\n    )\n    microsecond = Properties._tslib_field_accessor(\n        ""microsecond"", ""The microseconds of the datetime"", np.int64\n    )\n    nanosecond = Properties._tslib_field_accessor(\n        ""nanosecond"", ""The nanoseconds of the datetime"", np.int64\n    )\n    weekofyear = Properties._tslib_field_accessor(\n        ""weekofyear"", ""The week ordinal of the year"", np.int64\n    )\n    week = weekofyear\n    dayofweek = Properties._tslib_field_accessor(\n        ""dayofweek"", ""The day of the week with Monday=0, Sunday=6"", np.int64\n    )\n    weekday = dayofweek\n\n    weekday_name = Properties._tslib_field_accessor(\n        ""weekday_name"", ""The name of day in a week"", object\n    )\n\n    dayofyear = Properties._tslib_field_accessor(\n        ""dayofyear"", ""The ordinal day of the year"", np.int64\n    )\n    quarter = Properties._tslib_field_accessor(""quarter"", ""The quarter of the date"")\n    days_in_month = Properties._tslib_field_accessor(\n        ""days_in_month"", ""The number of days in the month"", np.int64\n    )\n    daysinmonth = days_in_month\n\n    season = Properties._tslib_field_accessor(""season"", ""Season of the year"", object)\n\n    time = Properties._tslib_field_accessor(\n        ""time"", ""Timestamps corresponding to datetimes"", object\n    )\n\n    is_month_start = Properties._tslib_field_accessor(\n        ""is_month_start"",\n        ""Indicates whether the date is the first day of the month."",\n        bool,\n    )\n    is_month_end = Properties._tslib_field_accessor(\n        ""is_month_end"", ""Indicates whether the date is the last day of the month."", bool\n    )\n    is_quarter_start = Properties._tslib_field_accessor(\n        ""is_quarter_start"",\n        ""Indicator for whether the date is the first day of a quarter."",\n        bool,\n    )\n    is_quarter_end = Properties._tslib_field_accessor(\n        ""is_quarter_end"",\n        ""Indicator for whether the date is the last day of a quarter."",\n        bool,\n    )\n    is_year_start = Properties._tslib_field_accessor(\n        ""is_year_start"", ""Indicate whether the date is the first day of a year."", bool\n    )\n    is_year_end = Properties._tslib_field_accessor(\n        ""is_year_end"", ""Indicate whether the date is the last day of the year."", bool\n    )\n    is_leap_year = Properties._tslib_field_accessor(\n        ""is_leap_year"", ""Boolean indicator if the date belongs to a leap year."", bool\n    )\n\n\nclass TimedeltaAccessor(Properties):\n    """"""Access Timedelta fields for DataArrays with Timedelta-like dtypes.\n\n    Fields can be accessed through the `.dt` attribute for applicable DataArrays.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> import xarray as xr\n    >>> dates = pd.timedelta_range(start=""1 day"", freq=""6H"", periods=20)\n    >>> ts = xr.DataArray(dates, dims=(""time""))\n    >>> ts\n    <xarray.DataArray (time: 20)>\n    array([ 86400000000000, 108000000000000, 129600000000000, 151200000000000,\n        172800000000000, 194400000000000, 216000000000000, 237600000000000,\n        259200000000000, 280800000000000, 302400000000000, 324000000000000,\n        345600000000000, 367200000000000, 388800000000000, 410400000000000,\n        432000000000000, 453600000000000, 475200000000000, 496800000000000],\n        dtype=\'timedelta64[ns]\')\n    Coordinates:\n    * time     (time) timedelta64[ns] 1 days 00:00:00 ... 5 days 18:00:00\n    >>> ts.dt\n    <xarray.core.accessor_dt.TimedeltaAccessor object at 0x109a27d68>\n    >>> ts.dt.days\n    <xarray.DataArray \'days\' (time: 20)>\n    array([1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5])\n    Coordinates:\n    * time     (time) timedelta64[ns] 1 days 00:00:00 ... 5 days 18:00:00\n    >>> ts.dt.microseconds\n    <xarray.DataArray \'microseconds\' (time: 20)>\n    array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n    Coordinates:\n    * time     (time) timedelta64[ns] 1 days 00:00:00 ... 5 days 18:00:00\n    >>> ts.dt.seconds\n    <xarray.DataArray \'seconds\' (time: 20)>\n    array([    0, 21600, 43200, 64800,     0, 21600, 43200, 64800,     0,\n        21600, 43200, 64800,     0, 21600, 43200, 64800,     0, 21600,\n        43200, 64800])\n    Coordinates:\n    * time     (time) timedelta64[ns] 1 days 00:00:00 ... 5 days 18:00:00\n    """"""\n\n    days = Properties._tslib_field_accessor(\n        ""days"", ""Number of days for each element."", np.int64\n    )\n    seconds = Properties._tslib_field_accessor(\n        ""seconds"",\n        ""Number of seconds (>= 0 and less than 1 day) for each element."",\n        np.int64,\n    )\n    microseconds = Properties._tslib_field_accessor(\n        ""microseconds"",\n        ""Number of microseconds (>= 0 and less than 1 second) for each element."",\n        np.int64,\n    )\n    nanoseconds = Properties._tslib_field_accessor(\n        ""nanoseconds"",\n        ""Number of nanoseconds (>= 0 and less than 1 microsecond) for each element."",\n        np.int64,\n    )\n\n\nclass CombinedDatetimelikeAccessor(DatetimeAccessor, TimedeltaAccessor):\n    def __new__(cls, obj):\n        # CombinedDatetimelikeAccessor isn\'t really instatiated. Instead\n        # we need to choose which parent (datetime or timedelta) is\n        # appropriate. Since we\'re checking the dtypes anyway, we\'ll just\n        # do all the validation here.\n        if not _contains_datetime_like_objects(obj):\n            raise TypeError(\n                ""\'.dt\' accessor only available for ""\n                ""DataArray with datetime64 timedelta64 dtype or ""\n                ""for arrays containing cftime datetime ""\n                ""objects.""\n            )\n\n        if is_np_timedelta_like(obj.dtype):\n            return TimedeltaAccessor(obj)\n        else:\n            return DatetimeAccessor(obj)\n'"
xarray/core/accessor_str.py,3,"b'# The StringAccessor class defined below is an adaptation of the\n# pandas string methods source code (see pd.core.strings)\n\n# For reference, here is a copy of the pandas copyright notice:\n\n# (c) 2011-2012, Lambda Foundry, Inc. and PyData Development Team\n# All rights reserved.\n\n# Copyright (c) 2008-2011 AQR Capital Management, LLC\n# All rights reserved.\n\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n\n#     * Redistributions of source code must retain the above copyright\n#        notice, this list of conditions and the following disclaimer.\n\n#     * Redistributions in binary form must reproduce the above\n#        copyright notice, this list of conditions and the following\n#        disclaimer in the documentation and/or other materials provided\n#        with the distribution.\n\n#     * Neither the name of the copyright holder nor the names of any\n#        contributors may be used to endorse or promote products derived\n#        from this software without specific prior written permission.\n\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER AND CONTRIBUTORS\n# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport codecs\nimport re\nimport textwrap\n\nimport numpy as np\n\nfrom .computation import apply_ufunc\n\n_cpython_optimized_encoders = (\n    ""utf-8"",\n    ""utf8"",\n    ""latin-1"",\n    ""latin1"",\n    ""iso-8859-1"",\n    ""mbcs"",\n    ""ascii"",\n)\n_cpython_optimized_decoders = _cpython_optimized_encoders + (""utf-16"", ""utf-32"")\n\n\ndef _is_str_like(x):\n    return isinstance(x, str) or isinstance(x, bytes)\n\n\nclass StringAccessor:\n    """"""Vectorized string functions for string-like arrays.\n\n    Similar to pandas, fields can be accessed through the `.str` attribute\n    for applicable DataArrays.\n\n        >>> da = xr.DataArray([""some"", ""text"", ""in"", ""an"", ""array""])\n        >>> ds.str.len()\n        <xarray.DataArray (dim_0: 5)>\n        array([4, 4, 2, 2, 5])\n        Dimensions without coordinates: dim_0\n\n    """"""\n\n    __slots__ = (""_obj"",)\n\n    def __init__(self, obj):\n        self._obj = obj\n\n    def _apply(self, f, dtype=None):\n        # TODO handling of na values ?\n        if dtype is None:\n            dtype = self._obj.dtype\n\n        g = np.vectorize(f, otypes=[dtype])\n        return apply_ufunc(g, self._obj, dask=""parallelized"", output_dtypes=[dtype])\n\n    def len(self):\n        """"""\n        Compute the length of each element in the array.\n\n        Returns\n        -------\n        lengths array : array of int\n        """"""\n        return self._apply(len, dtype=int)\n\n    def __getitem__(self, key):\n        if isinstance(key, slice):\n            return self.slice(start=key.start, stop=key.stop, step=key.step)\n        else:\n            return self.get(key)\n\n    def get(self, i):\n        """"""\n        Extract element from indexable in each element in the array.\n\n        Parameters\n        ----------\n        i : int\n            Position of element to extract.\n        default : optional\n            Value for out-of-range index. If not specified (None) defaults to\n            an empty string.\n\n        Returns\n        -------\n        items : array of objects\n        """"""\n        obj = slice(-1, None) if i == -1 else slice(i, i + 1)\n        return self._apply(lambda x: x[obj])\n\n    def slice(self, start=None, stop=None, step=None):\n        """"""\n        Slice substrings from each element in the array.\n\n        Parameters\n        ----------\n        start : int, optional\n            Start position for slice operation.\n        stop : int, optional\n            Stop position for slice operation.\n        step : int, optional\n            Step size for slice operation.\n\n        Returns\n        -------\n        sliced strings : same type as values\n        """"""\n        s = slice(start, stop, step)\n        f = lambda x: x[s]\n        return self._apply(f)\n\n    def slice_replace(self, start=None, stop=None, repl=""""):\n        """"""\n        Replace a positional slice of a string with another value.\n\n        Parameters\n        ----------\n        start : int, optional\n            Left index position to use for the slice. If not specified (None),\n            the slice is unbounded on the left, i.e. slice from the start\n            of the string.\n        stop : int, optional\n            Right index position to use for the slice. If not specified (None),\n            the slice is unbounded on the right, i.e. slice until the\n            end of the string.\n        repl : str, optional\n            String for replacement. If not specified, the sliced region\n            is replaced with an empty string.\n\n        Returns\n        -------\n        replaced : same type as values\n        """"""\n        repl = self._obj.dtype.type(repl)\n\n        def f(x):\n            if len(x[start:stop]) == 0:\n                local_stop = start\n            else:\n                local_stop = stop\n            y = self._obj.dtype.type("""")\n            if start is not None:\n                y += x[:start]\n            y += repl\n            if stop is not None:\n                y += x[local_stop:]\n            return y\n\n        return self._apply(f)\n\n    def capitalize(self):\n        """"""\n        Convert strings in the array to be capitalized.\n\n        Returns\n        -------\n        capitalized : same type as values\n        """"""\n        return self._apply(lambda x: x.capitalize())\n\n    def lower(self):\n        """"""\n        Convert strings in the array to lowercase.\n\n        Returns\n        -------\n        lowerd : same type as values\n        """"""\n        return self._apply(lambda x: x.lower())\n\n    def swapcase(self):\n        """"""\n        Convert strings in the array to be swapcased.\n\n        Returns\n        -------\n        swapcased : same type as values\n        """"""\n        return self._apply(lambda x: x.swapcase())\n\n    def title(self):\n        """"""\n        Convert strings in the array to titlecase.\n\n        Returns\n        -------\n        titled : same type as values\n        """"""\n        return self._apply(lambda x: x.title())\n\n    def upper(self):\n        """"""\n        Convert strings in the array to uppercase.\n\n        Returns\n        -------\n        uppered : same type as values\n        """"""\n        return self._apply(lambda x: x.upper())\n\n    def isalnum(self):\n        """"""\n        Check whether all characters in each string are alphanumeric.\n\n        Returns\n        -------\n        isalnum : array of bool\n            Array of boolean values with the same shape as the original array.\n        """"""\n        return self._apply(lambda x: x.isalnum(), dtype=bool)\n\n    def isalpha(self):\n        """"""\n        Check whether all characters in each string are alphabetic.\n\n        Returns\n        -------\n        isalpha : array of bool\n            Array of boolean values with the same shape as the original array.\n        """"""\n        return self._apply(lambda x: x.isalpha(), dtype=bool)\n\n    def isdecimal(self):\n        """"""\n        Check whether all characters in each string are decimal.\n\n        Returns\n        -------\n        isdecimal : array of bool\n            Array of boolean values with the same shape as the original array.\n        """"""\n        return self._apply(lambda x: x.isdecimal(), dtype=bool)\n\n    def isdigit(self):\n        """"""\n        Check whether all characters in each string are digits.\n\n        Returns\n        -------\n        isdigit : array of bool\n            Array of boolean values with the same shape as the original array.\n        """"""\n        return self._apply(lambda x: x.isdigit(), dtype=bool)\n\n    def islower(self):\n        """"""\n        Check whether all characters in each string are lowercase.\n\n        Returns\n        -------\n        islower : array of bool\n            Array of boolean values with the same shape as the original array.\n        """"""\n        return self._apply(lambda x: x.islower(), dtype=bool)\n\n    def isnumeric(self):\n        """"""\n        Check whether all characters in each string are numeric.\n\n        Returns\n        -------\n        isnumeric : array of bool\n            Array of boolean values with the same shape as the original array.\n        """"""\n        return self._apply(lambda x: x.isnumeric(), dtype=bool)\n\n    def isspace(self):\n        """"""\n        Check whether all characters in each string are spaces.\n\n        Returns\n        -------\n        isspace : array of bool\n            Array of boolean values with the same shape as the original array.\n        """"""\n        return self._apply(lambda x: x.isspace(), dtype=bool)\n\n    def istitle(self):\n        """"""\n        Check whether all characters in each string are titlecase.\n\n        Returns\n        -------\n        istitle : array of bool\n            Array of boolean values with the same shape as the original array.\n        """"""\n        return self._apply(lambda x: x.istitle(), dtype=bool)\n\n    def isupper(self):\n        """"""\n        Check whether all characters in each string are uppercase.\n\n        Returns\n        -------\n        isupper : array of bool\n            Array of boolean values with the same shape as the original array.\n        """"""\n        return self._apply(lambda x: x.isupper(), dtype=bool)\n\n    def count(self, pat, flags=0):\n        """"""\n        Count occurrences of pattern in each string of the array.\n\n        This function is used to count the number of times a particular regex\n        pattern is repeated in each of the string elements of the\n        :class:`~xarray.DatArray`.\n\n        Parameters\n        ----------\n        pat : str\n            Valid regular expression.\n        flags : int, default 0, meaning no flags\n            Flags for the `re` module. For a complete list, `see here\n            <https://docs.python.org/3/howto/regex.html#compilation-flags>`_.\n\n        Returns\n        -------\n        counts : array of int\n        """"""\n        pat = self._obj.dtype.type(pat)\n        regex = re.compile(pat, flags=flags)\n        f = lambda x: len(regex.findall(x))\n        return self._apply(f, dtype=int)\n\n    def startswith(self, pat):\n        """"""\n        Test if the start of each string element matches a pattern.\n\n        Parameters\n        ----------\n        pat : str\n            Character sequence. Regular expressions are not accepted.\n\n        Returns\n        -------\n        startswith : array of bool\n            An array of booleans indicating whether the given pattern matches\n            the start of each string element.\n        """"""\n        pat = self._obj.dtype.type(pat)\n        f = lambda x: x.startswith(pat)\n        return self._apply(f, dtype=bool)\n\n    def endswith(self, pat):\n        """"""\n        Test if the end of each string element matches a pattern.\n\n        Parameters\n        ----------\n        pat : str\n            Character sequence. Regular expressions are not accepted.\n\n        Returns\n        -------\n        endswith : array of bool\n            A Series of booleans indicating whether the given pattern matches\n            the end of each string element.\n        """"""\n        pat = self._obj.dtype.type(pat)\n        f = lambda x: x.endswith(pat)\n        return self._apply(f, dtype=bool)\n\n    def pad(self, width, side=""left"", fillchar="" ""):\n        """"""\n        Pad strings in the array up to width.\n\n        Parameters\n        ----------\n        width : int\n            Minimum width of resulting string; additional characters will be\n            filled with character defined in `fillchar`.\n        side : {\'left\', \'right\', \'both\'}, default \'left\'\n            Side from which to fill resulting string.\n        fillchar : str, default \' \'\n            Additional character for filling, default is whitespace.\n\n        Returns\n        -------\n        filled : same type as values\n            Array with a minimum number of char in each element.\n        """"""\n        width = int(width)\n        fillchar = self._obj.dtype.type(fillchar)\n        if len(fillchar) != 1:\n            raise TypeError(""fillchar must be a character, not str"")\n\n        if side == ""left"":\n            f = lambda s: s.rjust(width, fillchar)\n        elif side == ""right"":\n            f = lambda s: s.ljust(width, fillchar)\n        elif side == ""both"":\n            f = lambda s: s.center(width, fillchar)\n        else:  # pragma: no cover\n            raise ValueError(""Invalid side"")\n\n        return self._apply(f)\n\n    def center(self, width, fillchar="" ""):\n        """"""\n        Filling left and right side of strings in the array with an\n        additional character.\n\n        Parameters\n        ----------\n        width : int\n            Minimum width of resulting string; additional characters will be\n            filled with ``fillchar``\n        fillchar : str\n            Additional character for filling, default is whitespace\n\n        Returns\n        -------\n        filled : same type as values\n        """"""\n        return self.pad(width, side=""both"", fillchar=fillchar)\n\n    def ljust(self, width, fillchar="" ""):\n        """"""\n        Filling right side of strings in the array with an additional\n        character.\n\n        Parameters\n        ----------\n        width : int\n            Minimum width of resulting string; additional characters will be\n            filled with ``fillchar``\n        fillchar : str\n            Additional character for filling, default is whitespace\n\n        Returns\n        -------\n        filled : same type as values\n        """"""\n        return self.pad(width, side=""right"", fillchar=fillchar)\n\n    def rjust(self, width, fillchar="" ""):\n        """"""\n        Filling left side of strings in the array with an additional character.\n\n        Parameters\n        ----------\n        width : int\n            Minimum width of resulting string; additional characters will be\n            filled with ``fillchar``\n        fillchar : str\n            Additional character for filling, default is whitespace\n\n        Returns\n        -------\n        filled : same type as values\n        """"""\n        return self.pad(width, side=""left"", fillchar=fillchar)\n\n    def zfill(self, width):\n        """"""\n        Pad strings in the array by prepending \'0\' characters.\n\n        Strings in the array are padded with \'0\' characters on the\n        left of the string to reach a total string length  `width`. Strings\n        in the array  with length greater or equal to `width` are unchanged.\n\n        Parameters\n        ----------\n        width : int\n            Minimum length of resulting string; strings with length less\n            than `width` be prepended with \'0\' characters.\n\n        Returns\n        -------\n        filled : same type as values\n        """"""\n        return self.pad(width, side=""left"", fillchar=""0"")\n\n    def contains(self, pat, case=True, flags=0, regex=True):\n        """"""\n        Test if pattern or regex is contained within a string of the array.\n\n        Return boolean array based on whether a given pattern or regex is\n        contained within a string of the array.\n\n        Parameters\n        ----------\n        pat : str\n            Character sequence or regular expression.\n        case : bool, default True\n            If True, case sensitive.\n        flags : int, default 0 (no flags)\n            Flags to pass through to the re module, e.g. re.IGNORECASE.\n        regex : bool, default True\n            If True, assumes the pat is a regular expression.\n            If False, treats the pat as a literal string.\n\n        Returns\n        -------\n        contains : array of bool\n            An array of boolean values indicating whether the\n            given pattern is contained within the string of each element\n            of the array.\n        """"""\n        pat = self._obj.dtype.type(pat)\n        if regex:\n            if not case:\n                flags |= re.IGNORECASE\n\n            regex = re.compile(pat, flags=flags)\n\n            if regex.groups > 0:  # pragma: no cover\n                raise ValueError(""This pattern has match groups."")\n\n            f = lambda x: bool(regex.search(x))\n        else:\n            if case:\n                f = lambda x: pat in x\n            else:\n                uppered = self._obj.str.upper()\n                return uppered.str.contains(pat.upper(), regex=False)\n\n        return self._apply(f, dtype=bool)\n\n    def match(self, pat, case=True, flags=0):\n        """"""\n        Determine if each string matches a regular expression.\n\n        Parameters\n        ----------\n        pat : string\n            Character sequence or regular expression\n        case : boolean, default True\n            If True, case sensitive\n        flags : int, default 0 (no flags)\n            re module flags, e.g. re.IGNORECASE\n\n        Returns\n        -------\n        matched : array of bool\n        """"""\n        if not case:\n            flags |= re.IGNORECASE\n\n        pat = self._obj.dtype.type(pat)\n        regex = re.compile(pat, flags=flags)\n        f = lambda x: bool(regex.match(x))\n        return self._apply(f, dtype=bool)\n\n    def strip(self, to_strip=None, side=""both""):\n        """"""\n        Remove leading and trailing characters.\n\n        Strip whitespaces (including newlines) or a set of specified characters\n        from each string in the array from left and/or right sides.\n\n        Parameters\n        ----------\n        to_strip : str or None, default None\n            Specifying the set of characters to be removed.\n            All combinations of this set of characters will be stripped.\n            If None then whitespaces are removed.\n        side : {\'left\', \'right\', \'both\'}, default \'left\'\n            Side from which to strip.\n\n        Returns\n        -------\n        stripped : same type as values\n        """"""\n        if to_strip is not None:\n            to_strip = self._obj.dtype.type(to_strip)\n\n        if side == ""both"":\n            f = lambda x: x.strip(to_strip)\n        elif side == ""left"":\n            f = lambda x: x.lstrip(to_strip)\n        elif side == ""right"":\n            f = lambda x: x.rstrip(to_strip)\n        else:  # pragma: no cover\n            raise ValueError(""Invalid side"")\n\n        return self._apply(f)\n\n    def lstrip(self, to_strip=None):\n        """"""\n        Remove leading and trailing characters.\n\n        Strip whitespaces (including newlines) or a set of specified characters\n        from each string in the array from the left side.\n\n        Parameters\n        ----------\n        to_strip : str or None, default None\n            Specifying the set of characters to be removed.\n            All combinations of this set of characters will be stripped.\n            If None then whitespaces are removed.\n\n        Returns\n        -------\n        stripped : same type as values\n        """"""\n        return self.strip(to_strip, side=""left"")\n\n    def rstrip(self, to_strip=None):\n        """"""\n        Remove leading and trailing characters.\n\n        Strip whitespaces (including newlines) or a set of specified characters\n        from each string in the array from the right side.\n\n        Parameters\n        ----------\n        to_strip : str or None, default None\n            Specifying the set of characters to be removed.\n            All combinations of this set of characters will be stripped.\n            If None then whitespaces are removed.\n\n        Returns\n        -------\n        stripped : same type as values\n        """"""\n        return self.strip(to_strip, side=""right"")\n\n    def wrap(self, width, **kwargs):\n        """"""\n        Wrap long strings in the array to be formatted in paragraphs with\n        length less than a given width.\n\n        This method has the same keyword parameters and defaults as\n        :class:`textwrap.TextWrapper`.\n\n        Parameters\n        ----------\n        width : int\n            Maximum line-width\n        expand_tabs : bool, optional\n            If true, tab characters will be expanded to spaces (default: True)\n        replace_whitespace : bool, optional\n            If true, each whitespace character (as defined by\n            string.whitespace) remaining after tab expansion will be replaced\n            by a single space (default: True)\n        drop_whitespace : bool, optional\n            If true, whitespace that, after wrapping, happens to end up at the\n            beginning or end of a line is dropped (default: True)\n        break_long_words : bool, optional\n            If true, then words longer than width will be broken in order to\n            ensure that no lines are longer than width. If it is false, long\n            words will not be broken, and some lines may be longer than width.\n            (default: True)\n        break_on_hyphens : bool, optional\n            If true, wrapping will occur preferably on whitespace and right\n            after hyphens in compound words, as it is customary in English. If\n            false, only whitespaces will be considered as potentially good\n            places for line breaks, but you need to set break_long_words to\n            false if you want truly insecable words. (default: True)\n\n        Returns\n        -------\n        wrapped : same type as values\n        """"""\n        tw = textwrap.TextWrapper(width=width)\n        f = lambda x: ""\\n"".join(tw.wrap(x))\n        return self._apply(f)\n\n    def translate(self, table):\n        """"""\n        Map all characters in the string through the given mapping table.\n\n        Parameters\n        ----------\n        table : dict\n            A a mapping of Unicode ordinals to Unicode ordinals, strings,\n            or None. Unmapped characters are left untouched. Characters mapped\n            to None are deleted. :meth:`str.maketrans` is a helper function for\n            making translation tables.\n\n        Returns\n        -------\n        translated : same type as values\n        """"""\n        f = lambda x: x.translate(table)\n        return self._apply(f)\n\n    def repeat(self, repeats):\n        """"""\n        Duplicate each string in the array.\n\n        Parameters\n        ----------\n        repeats : int\n            Number of repetitions.\n\n        Returns\n        -------\n        repeated : same type as values\n            Array of repeated string objects.\n        """"""\n        f = lambda x: repeats * x\n        return self._apply(f)\n\n    def find(self, sub, start=0, end=None, side=""left""):\n        """"""\n        Return lowest or highest indexes in each strings in the array\n        where the substring is fully contained between [start:end].\n        Return -1 on failure.\n\n        Parameters\n        ----------\n        sub : str\n            Substring being searched\n        start : int\n            Left edge index\n        end : int\n            Right edge index\n        side : {\'left\', \'right\'}, default \'left\'\n            Starting side for search.\n\n        Returns\n        -------\n        found : array of integer values\n        """"""\n        sub = self._obj.dtype.type(sub)\n\n        if side == ""left"":\n            method = ""find""\n        elif side == ""right"":\n            method = ""rfind""\n        else:  # pragma: no cover\n            raise ValueError(""Invalid side"")\n\n        if end is None:\n            f = lambda x: getattr(x, method)(sub, start)\n        else:\n            f = lambda x: getattr(x, method)(sub, start, end)\n\n        return self._apply(f, dtype=int)\n\n    def rfind(self, sub, start=0, end=None):\n        """"""\n        Return highest indexes in each strings in the array\n        where the substring is fully contained between [start:end].\n        Return -1 on failure.\n\n        Parameters\n        ----------\n        sub : str\n            Substring being searched\n        start : int\n            Left edge index\n        end : int\n            Right edge index\n\n        Returns\n        -------\n        found : array of integer values\n        """"""\n        return self.find(sub, start=start, end=end, side=""right"")\n\n    def index(self, sub, start=0, end=None, side=""left""):\n        """"""\n        Return lowest or highest indexes in each strings where the substring is\n        fully contained between [start:end]. This is the same as\n        ``str.find`` except instead of returning -1, it raises a ValueError\n        when the substring is not found.\n\n        Parameters\n        ----------\n        sub : str\n            Substring being searched\n        start : int\n            Left edge index\n        end : int\n            Right edge index\n        side : {\'left\', \'right\'}, default \'left\'\n            Starting side for search.\n\n        Returns\n        -------\n        found : array of integer values\n        """"""\n        sub = self._obj.dtype.type(sub)\n\n        if side == ""left"":\n            method = ""index""\n        elif side == ""right"":\n            method = ""rindex""\n        else:  # pragma: no cover\n            raise ValueError(""Invalid side"")\n\n        if end is None:\n            f = lambda x: getattr(x, method)(sub, start)\n        else:\n            f = lambda x: getattr(x, method)(sub, start, end)\n\n        return self._apply(f, dtype=int)\n\n    def rindex(self, sub, start=0, end=None):\n        """"""\n        Return highest indexes in each strings where the substring is\n        fully contained between [start:end]. This is the same as\n        ``str.rfind`` except instead of returning -1, it raises a ValueError\n        when the substring is not found.\n\n        Parameters\n        ----------\n        sub : str\n            Substring being searched\n        start : int\n            Left edge index\n        end : int\n            Right edge index\n\n        Returns\n        -------\n        found : array of integer values\n        """"""\n        return self.index(sub, start=start, end=end, side=""right"")\n\n    def replace(self, pat, repl, n=-1, case=None, flags=0, regex=True):\n        """"""\n        Replace occurrences of pattern/regex in the array with some string.\n\n        Parameters\n        ----------\n        pat : string or compiled regex\n            String can be a character sequence or regular expression.\n        repl : string or callable\n            Replacement string or a callable. The callable is passed the regex\n            match object and must return a replacement string to be used.\n            See :func:`re.sub`.\n        n : int, default -1 (all)\n            Number of replacements to make from start\n        case : boolean, default None\n            - If True, case sensitive (the default if `pat` is a string)\n            - Set to False for case insensitive\n            - Cannot be set if `pat` is a compiled regex\n        flags : int, default 0 (no flags)\n            - re module flags, e.g. re.IGNORECASE\n            - Cannot be set if `pat` is a compiled regex\n        regex : boolean, default True\n            - If True, assumes the passed-in pattern is a regular expression.\n            - If False, treats the pattern as a literal string\n            - Cannot be set to False if `pat` is a compiled regex or `repl` is\n              a callable.\n\n        Returns\n        -------\n        replaced : same type as values\n            A copy of the object with all matching occurrences of `pat`\n            replaced by `repl`.\n        """"""\n        if not (_is_str_like(repl) or callable(repl)):  # pragma: no cover\n            raise TypeError(""repl must be a string or callable"")\n\n        if _is_str_like(pat):\n            pat = self._obj.dtype.type(pat)\n\n        if _is_str_like(repl):\n            repl = self._obj.dtype.type(repl)\n\n        is_compiled_re = isinstance(pat, type(re.compile("""")))\n        if regex:\n            if is_compiled_re:\n                if (case is not None) or (flags != 0):\n                    raise ValueError(\n                        ""case and flags cannot be set"" "" when pat is a compiled regex""\n                    )\n            else:\n                # not a compiled regex\n                # set default case\n                if case is None:\n                    case = True\n\n                # add case flag, if provided\n                if case is False:\n                    flags |= re.IGNORECASE\n            if is_compiled_re or len(pat) > 1 or flags or callable(repl):\n                n = n if n >= 0 else 0\n                compiled = re.compile(pat, flags=flags)\n                f = lambda x: compiled.sub(repl=repl, string=x, count=n)\n            else:\n                f = lambda x: x.replace(pat, repl, n)\n        else:\n            if is_compiled_re:\n                raise ValueError(\n                    ""Cannot use a compiled regex as replacement ""\n                    ""pattern with regex=False""\n                )\n            if callable(repl):\n                raise ValueError(\n                    ""Cannot use a callable replacement when "" ""regex=False""\n                )\n            f = lambda x: x.replace(pat, repl, n)\n        return self._apply(f)\n\n    def decode(self, encoding, errors=""strict""):\n        """"""\n        Decode character string in the array using indicated encoding.\n\n        Parameters\n        ----------\n        encoding : str\n        errors : str, optional\n\n        Returns\n        -------\n        decoded : same type as values\n        """"""\n        if encoding in _cpython_optimized_decoders:\n            f = lambda x: x.decode(encoding, errors)\n        else:\n            decoder = codecs.getdecoder(encoding)\n            f = lambda x: decoder(x, errors)[0]\n        return self._apply(f, dtype=np.str_)\n\n    def encode(self, encoding, errors=""strict""):\n        """"""\n        Encode character string in the array using indicated encoding.\n\n        Parameters\n        ----------\n        encoding : str\n        errors : str, optional\n\n        Returns\n        -------\n        encoded : same type as values\n        """"""\n        if encoding in _cpython_optimized_encoders:\n            f = lambda x: x.encode(encoding, errors)\n        else:\n            encoder = codecs.getencoder(encoding)\n            f = lambda x: encoder(x, errors)[0]\n        return self._apply(f, dtype=np.bytes_)\n'"
xarray/core/alignment.py,1,"b'import functools\nimport operator\nfrom collections import defaultdict\nfrom contextlib import suppress\nfrom typing import TYPE_CHECKING, Any, Dict, Hashable, Mapping, Optional, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\n\nfrom . import dtypes, utils\nfrom .indexing import get_indexer_nd\nfrom .utils import is_dict_like, is_full_slice\nfrom .variable import IndexVariable, Variable\n\nif TYPE_CHECKING:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n\ndef _get_joiner(join):\n    if join == ""outer"":\n        return functools.partial(functools.reduce, operator.or_)\n    elif join == ""inner"":\n        return functools.partial(functools.reduce, operator.and_)\n    elif join == ""left"":\n        return operator.itemgetter(0)\n    elif join == ""right"":\n        return operator.itemgetter(-1)\n    elif join == ""exact"":\n        # We cannot return a function to ""align"" in this case, because it needs\n        # access to the dimension name to give a good error message.\n        return None\n    elif join == ""override"":\n        # We rewrite all indexes and then use join=\'left\'\n        return operator.itemgetter(0)\n    else:\n        raise ValueError(""invalid value for join: %s"" % join)\n\n\ndef _override_indexes(objects, all_indexes, exclude):\n    for dim, dim_indexes in all_indexes.items():\n        if dim not in exclude:\n            lengths = {index.size for index in dim_indexes}\n            if len(lengths) != 1:\n                raise ValueError(\n                    ""Indexes along dimension %r don\'t have the same length.""\n                    "" Cannot use join=\'override\'."" % dim\n                )\n\n    objects = list(objects)\n    for idx, obj in enumerate(objects[1:]):\n        new_indexes = {}\n        for dim in obj.indexes:\n            if dim not in exclude:\n                new_indexes[dim] = all_indexes[dim][0]\n        objects[idx + 1] = obj._overwrite_indexes(new_indexes)\n\n    return objects\n\n\ndef align(\n    *objects,\n    join=""inner"",\n    copy=True,\n    indexes=None,\n    exclude=frozenset(),\n    fill_value=dtypes.NA,\n):\n    """"""\n    Given any number of Dataset and/or DataArray objects, returns new\n    objects with aligned indexes and dimension sizes.\n\n    Array from the aligned objects are suitable as input to mathematical\n    operators, because along each dimension they have the same index and size.\n\n    Missing values (if ``join != \'inner\'``) are filled with ``fill_value``.\n    The default fill value is NaN.\n\n    Parameters\n    ----------\n    *objects : Dataset or DataArray\n        Objects to align.\n    join : {\'outer\', \'inner\', \'left\', \'right\', \'exact\', \'override\'}, optional\n        Method for joining the indexes of the passed objects along each\n        dimension:\n\n        - \'outer\': use the union of object indexes\n        - \'inner\': use the intersection of object indexes\n        - \'left\': use indexes from the first object with each dimension\n        - \'right\': use indexes from the last object with each dimension\n        - \'exact\': instead of aligning, raise `ValueError` when indexes to be\n          aligned are not equal\n        - \'override\': if indexes are of same size, rewrite indexes to be\n          those of the first object with that dimension. Indexes for the same\n          dimension must have the same size in all objects.\n    copy : bool, optional\n        If ``copy=True``, data in the return values is always copied. If\n        ``copy=False`` and reindexing is unnecessary, or can be performed with\n        only slice operations, then the output may share memory with the input.\n        In either case, new xarray objects are always returned.\n    indexes : dict-like, optional\n        Any indexes explicitly provided with the `indexes` argument should be\n        used in preference to the aligned indexes.\n    exclude : sequence of str, optional\n        Dimensions that must be excluded from alignment\n    fill_value : scalar, optional\n        Value to use for newly missing values\n\n    Returns\n    -------\n    aligned : same as `*objects`\n        Tuple of objects with aligned coordinates.\n\n    Raises\n    ------\n    ValueError\n        If any dimensions without labels on the arguments have different sizes,\n        or a different size than the size of the aligned dimension labels.\n\n    Examples\n    --------\n\n    >>> import xarray as xr\n    >>> x = xr.DataArray(\n    ...     [[25, 35], [10, 24]],\n    ...     dims=(""lat"", ""lon""),\n    ...     coords={""lat"": [35.0, 40.0], ""lon"": [100.0, 120.0]},\n    ... )\n    >>> y = xr.DataArray(\n    ...     [[20, 5], [7, 13]],\n    ...     dims=(""lat"", ""lon""),\n    ...     coords={""lat"": [35.0, 42.0], ""lon"": [100.0, 120.0]},\n    ... )\n\n    >>> x\n    <xarray.DataArray (lat: 2, lon: 2)>\n    array([[25, 35],\n           [10, 24]])\n    Coordinates:\n    * lat      (lat) float64 35.0 40.0\n    * lon      (lon) float64 100.0 120.0\n\n    >>> y\n    <xarray.DataArray (lat: 2, lon: 2)>\n    array([[20,  5],\n           [ 7, 13]])\n    Coordinates:\n    * lat      (lat) float64 35.0 42.0\n    * lon      (lon) float64 100.0 120.0\n\n    >>> a, b = xr.align(x, y)\n    >>> a\n    <xarray.DataArray (lat: 1, lon: 2)>\n    array([[25, 35]])\n    Coordinates:\n    * lat      (lat) float64 35.0\n    * lon      (lon) float64 100.0 120.0\n    >>> b\n    <xarray.DataArray (lat: 1, lon: 2)>\n    array([[20,  5]])\n    Coordinates:\n    * lat      (lat) float64 35.0\n    * lon      (lon) float64 100.0 120.0\n\n    >>> a, b = xr.align(x, y, join=""outer"")\n    >>> a\n    <xarray.DataArray (lat: 3, lon: 2)>\n    array([[25., 35.],\n           [10., 24.],\n           [nan, nan]])\n    Coordinates:\n    * lat      (lat) float64 35.0 40.0 42.0\n    * lon      (lon) float64 100.0 120.0\n    >>> b\n    <xarray.DataArray (lat: 3, lon: 2)>\n    array([[20.,  5.],\n           [nan, nan],\n           [ 7., 13.]])\n    Coordinates:\n    * lat      (lat) float64 35.0 40.0 42.0\n    * lon      (lon) float64 100.0 120.0\n\n    >>> a, b = xr.align(x, y, join=""outer"", fill_value=-999)\n    >>> a\n    <xarray.DataArray (lat: 3, lon: 2)>\n    array([[  25,   35],\n           [  10,   24],\n           [-999, -999]])\n    Coordinates:\n    * lat      (lat) float64 35.0 40.0 42.0\n    * lon      (lon) float64 100.0 120.0\n    >>> b\n    <xarray.DataArray (lat: 3, lon: 2)>\n    array([[  20,    5],\n           [-999, -999],\n           [   7,   13]])\n    Coordinates:\n    * lat      (lat) float64 35.0 40.0 42.0\n    * lon      (lon) float64 100.0 120.0\n\n    >>> a, b = xr.align(x, y, join=""left"")\n    >>> a\n    <xarray.DataArray (lat: 2, lon: 2)>\n    array([[25, 35],\n           [10, 24]])\n    Coordinates:\n    * lat      (lat) float64 35.0 40.0\n    * lon      (lon) float64 100.0 120.0\n    >>> b\n    <xarray.DataArray (lat: 2, lon: 2)>\n    array([[20.,  5.],\n           [nan, nan]])\n    Coordinates:\n    * lat      (lat) float64 35.0 40.0\n    * lon      (lon) float64 100.0 120.0\n\n    >>> a, b = xr.align(x, y, join=""right"")\n    >>> a\n    <xarray.DataArray (lat: 2, lon: 2)>\n    array([[25., 35.],\n           [nan, nan]])\n    Coordinates:\n    * lat      (lat) float64 35.0 42.0\n    * lon      (lon) float64 100.0 120.0\n    >>> b\n    <xarray.DataArray (lat: 2, lon: 2)>\n    array([[20,  5],\n           [ 7, 13]])\n    Coordinates:\n    * lat      (lat) float64 35.0 42.0\n    * lon      (lon) float64 100.0 120.0\n\n    >>> a, b = xr.align(x, y, join=""exact"")\n    Traceback (most recent call last):\n    ...\n        ""indexes along dimension {!r} are not equal"".format(dim)\n    ValueError: indexes along dimension \'lat\' are not equal\n\n    >>> a, b = xr.align(x, y, join=""override"")\n    >>> a\n    <xarray.DataArray (lat: 2, lon: 2)>\n    array([[25, 35],\n           [10, 24]])\n    Coordinates:\n    * lat      (lat) float64 35.0 40.0\n    * lon      (lon) float64 100.0 120.0\n    >>> b\n    <xarray.DataArray (lat: 2, lon: 2)>\n    array([[20,  5],\n           [ 7, 13]])\n    Coordinates:\n    * lat      (lat) float64 35.0 40.0\n    * lon      (lon) float64 100.0 120.0\n\n    """"""\n    if indexes is None:\n        indexes = {}\n\n    if not indexes and len(objects) == 1:\n        # fast path for the trivial case\n        (obj,) = objects\n        return (obj.copy(deep=copy),)\n\n    all_indexes = defaultdict(list)\n    unlabeled_dim_sizes = defaultdict(set)\n    for obj in objects:\n        for dim in obj.dims:\n            if dim not in exclude:\n                try:\n                    index = obj.indexes[dim]\n                except KeyError:\n                    unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n                else:\n                    all_indexes[dim].append(index)\n\n    if join == ""override"":\n        objects = _override_indexes(objects, all_indexes, exclude)\n\n    # We don\'t reindex over dimensions with all equal indexes for two reasons:\n    # - It\'s faster for the usual case (already aligned objects).\n    # - It ensures it\'s possible to do operations that don\'t require alignment\n    #   on indexes with duplicate values (which cannot be reindexed with\n    #   pandas). This is useful, e.g., for overwriting such duplicate indexes.\n    joiner = _get_joiner(join)\n    joined_indexes = {}\n    for dim, matching_indexes in all_indexes.items():\n        if dim in indexes:\n            index = utils.safe_cast_to_index(indexes[dim])\n            if (\n                any(not index.equals(other) for other in matching_indexes)\n                or dim in unlabeled_dim_sizes\n            ):\n                joined_indexes[dim] = index\n        else:\n            if (\n                any(\n                    not matching_indexes[0].equals(other)\n                    for other in matching_indexes[1:]\n                )\n                or dim in unlabeled_dim_sizes\n            ):\n                if join == ""exact"":\n                    raise ValueError(f""indexes along dimension {dim!r} are not equal"")\n                index = joiner(matching_indexes)\n                joined_indexes[dim] = index\n            else:\n                index = matching_indexes[0]\n\n        if dim in unlabeled_dim_sizes:\n            unlabeled_sizes = unlabeled_dim_sizes[dim]\n            labeled_size = index.size\n            if len(unlabeled_sizes | {labeled_size}) > 1:\n                raise ValueError(\n                    ""arguments without labels along dimension %r cannot be ""\n                    ""aligned because they have different dimension size(s) %r ""\n                    ""than the size of the aligned dimension labels: %r""\n                    % (dim, unlabeled_sizes, labeled_size)\n                )\n\n    for dim in unlabeled_dim_sizes:\n        if dim not in all_indexes:\n            sizes = unlabeled_dim_sizes[dim]\n            if len(sizes) > 1:\n                raise ValueError(\n                    ""arguments without labels along dimension %r cannot be ""\n                    ""aligned because they have different dimension sizes: %r""\n                    % (dim, sizes)\n                )\n\n    result = []\n    for obj in objects:\n        valid_indexers = {k: v for k, v in joined_indexes.items() if k in obj.dims}\n        if not valid_indexers:\n            # fast path for no reindexing necessary\n            new_obj = obj.copy(deep=copy)\n        else:\n            new_obj = obj.reindex(copy=copy, fill_value=fill_value, **valid_indexers)\n        new_obj.encoding = obj.encoding\n        result.append(new_obj)\n\n    return tuple(result)\n\n\ndef deep_align(\n    objects,\n    join=""inner"",\n    copy=True,\n    indexes=None,\n    exclude=frozenset(),\n    raise_on_invalid=True,\n    fill_value=dtypes.NA,\n):\n    """"""Align objects for merging, recursing into dictionary values.\n\n    This function is not public API.\n    """"""\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    if indexes is None:\n        indexes = {}\n\n    def is_alignable(obj):\n        return isinstance(obj, (DataArray, Dataset))\n\n    positions = []\n    keys = []\n    out = []\n    targets = []\n    no_key = object()\n    not_replaced = object()\n    for position, variables in enumerate(objects):\n        if is_alignable(variables):\n            positions.append(position)\n            keys.append(no_key)\n            targets.append(variables)\n            out.append(not_replaced)\n        elif is_dict_like(variables):\n            current_out = {}\n            for k, v in variables.items():\n                if is_alignable(v) and k not in indexes:\n                    # Skip variables in indexes for alignment, because these\n                    # should to be overwritten instead:\n                    # https://github.com/pydata/xarray/issues/725\n                    # https://github.com/pydata/xarray/issues/3377\n                    # TODO(shoyer): doing this here feels super-hacky -- can we\n                    # move it explicitly into merge instead?\n                    positions.append(position)\n                    keys.append(k)\n                    targets.append(v)\n                    current_out[k] = not_replaced\n                else:\n                    current_out[k] = v\n            out.append(current_out)\n        elif raise_on_invalid:\n            raise ValueError(\n                ""object to align is neither an xarray.Dataset, ""\n                ""an xarray.DataArray nor a dictionary: {!r}"".format(variables)\n            )\n        else:\n            out.append(variables)\n\n    aligned = align(\n        *targets,\n        join=join,\n        copy=copy,\n        indexes=indexes,\n        exclude=exclude,\n        fill_value=fill_value,\n    )\n\n    for position, key, aligned_obj in zip(positions, keys, aligned):\n        if key is no_key:\n            out[position] = aligned_obj\n        else:\n            out[position][key] = aligned_obj\n\n    # something went wrong: we should have replaced all sentinel values\n    for arg in out:\n        assert arg is not not_replaced\n        if is_dict_like(arg):\n            assert all(value is not not_replaced for value in arg.values())\n\n    return out\n\n\ndef reindex_like_indexers(\n    target: ""Union[DataArray, Dataset]"", other: ""Union[DataArray, Dataset]""\n) -> Dict[Hashable, pd.Index]:\n    """"""Extract indexers to align target with other.\n\n    Not public API.\n\n    Parameters\n    ----------\n    target : Dataset or DataArray\n        Object to be aligned.\n    other : Dataset or DataArray\n        Object to be aligned with.\n\n    Returns\n    -------\n    Dict[Hashable, pandas.Index] providing indexes for reindex keyword\n    arguments.\n\n    Raises\n    ------\n    ValueError\n        If any dimensions without labels have different sizes.\n    """"""\n    indexers = {k: v for k, v in other.indexes.items() if k in target.dims}\n\n    for dim in other.dims:\n        if dim not in indexers and dim in target.dims:\n            other_size = other.sizes[dim]\n            target_size = target.sizes[dim]\n            if other_size != target_size:\n                raise ValueError(\n                    ""different size for unlabeled ""\n                    ""dimension on argument %r: %r vs %r""\n                    % (dim, other_size, target_size)\n                )\n    return indexers\n\n\ndef reindex_variables(\n    variables: Mapping[Any, Variable],\n    sizes: Mapping[Any, int],\n    indexes: Mapping[Any, pd.Index],\n    indexers: Mapping,\n    method: Optional[str] = None,\n    tolerance: Any = None,\n    copy: bool = True,\n    fill_value: Optional[Any] = dtypes.NA,\n    sparse: bool = False,\n) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:\n    """"""Conform a dictionary of aligned variables onto a new set of variables,\n    filling in missing values with NaN.\n\n    Not public API.\n\n    Parameters\n    ----------\n    variables : dict-like\n        Dictionary of xarray.Variable objects.\n    sizes : dict-like\n        Dictionary from dimension names to integer sizes.\n    indexes : dict-like\n        Dictionary of indexes associated with variables.\n    indexers : dict\n        Dictionary with keys given by dimension names and values given by\n        arrays of coordinates tick labels. Any mis-matched coordinate values\n        will be filled in with NaN, and any mis-matched dimension names will\n        simply be ignored.\n    method : {None, \'nearest\', \'pad\'/\'ffill\', \'backfill\'/\'bfill\'}, optional\n        Method to use for filling index values in ``indexers`` not found in\n        this dataset:\n          * None (default): don\'t fill gaps\n          * pad / ffill: propagate last valid index value forward\n          * backfill / bfill: propagate next valid index value backward\n          * nearest: use nearest valid index value\n    tolerance : optional\n        Maximum distance between original and new labels for inexact matches.\n        The values of the index at the matching locations must satisfy the\n        equation ``abs(index[indexer] - target) <= tolerance``.\n    copy : bool, optional\n        If ``copy=True``, data in the return values is always copied. If\n        ``copy=False`` and reindexing is unnecessary, or can be performed\n        with only slice operations, then the output may share memory with\n        the input. In either case, new xarray objects are always returned.\n    fill_value : scalar, optional\n        Value to use for newly missing values\n    sparse: bool, optional\n        Use an sparse-array\n\n    Returns\n    -------\n    reindexed : dict\n        Dict of reindexed variables.\n    new_indexes : dict\n        Dict of indexes associated with the reindexed variables.\n    """"""\n    from .dataarray import DataArray\n\n    # create variables for the new dataset\n    reindexed: Dict[Hashable, Variable] = {}\n\n    # build up indexers for assignment along each dimension\n    int_indexers = {}\n    new_indexes = dict(indexes)\n    masked_dims = set()\n    unchanged_dims = set()\n\n    for dim, indexer in indexers.items():\n        if isinstance(indexer, DataArray) and indexer.dims != (dim,):\n            raise ValueError(\n                ""Indexer has dimensions {:s} that are different ""\n                ""from that to be indexed along {:s}"".format(str(indexer.dims), dim)\n            )\n\n        target = new_indexes[dim] = utils.safe_cast_to_index(indexers[dim])\n\n        if dim in indexes:\n            index = indexes[dim]\n\n            if not index.is_unique:\n                raise ValueError(\n                    ""cannot reindex or align along dimension %r because the ""\n                    ""index has duplicate values"" % dim\n                )\n\n            int_indexer = get_indexer_nd(index, target, method, tolerance)\n\n            # We uses negative values from get_indexer_nd to signify\n            # values that are missing in the index.\n            if (int_indexer < 0).any():\n                masked_dims.add(dim)\n            elif np.array_equal(int_indexer, np.arange(len(index))):\n                unchanged_dims.add(dim)\n\n            int_indexers[dim] = int_indexer\n\n        if dim in variables:\n            var = variables[dim]\n            args: tuple = (var.attrs, var.encoding)\n        else:\n            args = ()\n        reindexed[dim] = IndexVariable((dim,), target, *args)\n\n    for dim in sizes:\n        if dim not in indexes and dim in indexers:\n            existing_size = sizes[dim]\n            new_size = indexers[dim].size\n            if existing_size != new_size:\n                raise ValueError(\n                    ""cannot reindex or align along dimension %r without an ""\n                    ""index because its size %r is different from the size of ""\n                    ""the new index %r"" % (dim, existing_size, new_size)\n                )\n\n    for name, var in variables.items():\n        if name not in indexers:\n            if sparse:\n                var = var._as_sparse(fill_value=fill_value)\n            key = tuple(\n                slice(None) if d in unchanged_dims else int_indexers.get(d, slice(None))\n                for d in var.dims\n            )\n            needs_masking = any(d in masked_dims for d in var.dims)\n\n            if needs_masking:\n                new_var = var._getitem_with_mask(key, fill_value=fill_value)\n            elif all(is_full_slice(k) for k in key):\n                # no reindexing necessary\n                # here we need to manually deal with copying data, since\n                # we neither created a new ndarray nor used fancy indexing\n                new_var = var.copy(deep=copy)\n            else:\n                new_var = var[key]\n\n            reindexed[name] = new_var\n\n    return reindexed, new_indexes\n\n\ndef _get_broadcast_dims_map_common_coords(args, exclude):\n\n    common_coords = {}\n    dims_map = {}\n    for arg in args:\n        for dim in arg.dims:\n            if dim not in common_coords and dim not in exclude:\n                dims_map[dim] = arg.sizes[dim]\n                if dim in arg.coords:\n                    common_coords[dim] = arg.coords[dim].variable\n\n    return dims_map, common_coords\n\n\ndef _broadcast_helper(arg, exclude, dims_map, common_coords):\n\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    def _set_dims(var):\n        # Add excluded dims to a copy of dims_map\n        var_dims_map = dims_map.copy()\n        for dim in exclude:\n            with suppress(ValueError):\n                # ignore dim not in var.dims\n                var_dims_map[dim] = var.shape[var.dims.index(dim)]\n\n        return var.set_dims(var_dims_map)\n\n    def _broadcast_array(array):\n        data = _set_dims(array.variable)\n        coords = dict(array.coords)\n        coords.update(common_coords)\n        return DataArray(data, coords, data.dims, name=array.name, attrs=array.attrs)\n\n    def _broadcast_dataset(ds):\n        data_vars = {k: _set_dims(ds.variables[k]) for k in ds.data_vars}\n        coords = dict(ds.coords)\n        coords.update(common_coords)\n        return Dataset(data_vars, coords, ds.attrs)\n\n    if isinstance(arg, DataArray):\n        return _broadcast_array(arg)\n    elif isinstance(arg, Dataset):\n        return _broadcast_dataset(arg)\n    else:\n        raise ValueError(""all input must be Dataset or DataArray objects"")\n\n\ndef broadcast(*args, exclude=None):\n    """"""Explicitly broadcast any number of DataArray or Dataset objects against\n    one another.\n\n    xarray objects automatically broadcast against each other in arithmetic\n    operations, so this function should not be necessary for normal use.\n\n    If no change is needed, the input data is returned to the output without\n    being copied.\n\n    Parameters\n    ----------\n    *args : DataArray or Dataset objects\n        Arrays to broadcast against each other.\n    exclude : sequence of str, optional\n        Dimensions that must not be broadcasted\n\n    Returns\n    -------\n    broadcast : tuple of xarray objects\n        The same data as the input arrays, but with additional dimensions\n        inserted so that all data arrays have the same dimensions and shape.\n\n    Examples\n    --------\n\n    Broadcast two data arrays against one another to fill out their dimensions:\n\n    >>> a = xr.DataArray([1, 2, 3], dims=""x"")\n    >>> b = xr.DataArray([5, 6], dims=""y"")\n    >>> a\n    <xarray.DataArray (x: 3)>\n    array([1, 2, 3])\n    Coordinates:\n      * x        (x) int64 0 1 2\n    >>> b\n    <xarray.DataArray (y: 2)>\n    array([5, 6])\n    Coordinates:\n      * y        (y) int64 0 1\n    >>> a2, b2 = xr.broadcast(a, b)\n    >>> a2\n    <xarray.DataArray (x: 3, y: 2)>\n    array([[1, 1],\n           [2, 2],\n           [3, 3]])\n    Coordinates:\n      * x        (x) int64 0 1 2\n      * y        (y) int64 0 1\n    >>> b2\n    <xarray.DataArray (x: 3, y: 2)>\n    array([[5, 6],\n           [5, 6],\n           [5, 6]])\n    Coordinates:\n      * y        (y) int64 0 1\n      * x        (x) int64 0 1 2\n\n    Fill out the dimensions of all data variables in a dataset:\n\n    >>> ds = xr.Dataset({""a"": a, ""b"": b})\n    >>> (ds2,) = xr.broadcast(ds)  # use tuple unpacking to extract one dataset\n    >>> ds2\n    <xarray.Dataset>\n    Dimensions:  (x: 3, y: 2)\n    Coordinates:\n      * x        (x) int64 0 1 2\n      * y        (y) int64 0 1\n    Data variables:\n        a        (x, y) int64 1 1 2 2 3 3\n        b        (x, y) int64 5 6 5 6 5 6\n    """"""\n\n    if exclude is None:\n        exclude = set()\n    args = align(*args, join=""outer"", copy=False, exclude=exclude)\n\n    dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\n    result = []\n    for arg in args:\n        result.append(_broadcast_helper(arg, exclude, dims_map, common_coords))\n\n    return tuple(result)\n'"
xarray/core/arithmetic.py,3,"b'""""""Base classes implementing arithmetic for xarray objects.""""""\nimport numbers\n\nimport numpy as np\n\nfrom .options import OPTIONS\nfrom .pycompat import dask_array_type\nfrom .utils import not_implemented\n\n\nclass SupportsArithmetic:\n    """"""Base class for xarray types that support arithmetic.\n\n    Used by Dataset, DataArray, Variable and GroupBy.\n    """"""\n\n    __slots__ = ()\n\n    # TODO: implement special methods for arithmetic here rather than injecting\n    # them in xarray/core/ops.py. Ideally, do so by inheriting from\n    # numpy.lib.mixins.NDArrayOperatorsMixin.\n\n    # TODO: allow extending this with some sort of registration system\n    _HANDLED_TYPES = (\n        np.ndarray,\n        np.generic,\n        numbers.Number,\n        bytes,\n        str,\n    ) + dask_array_type\n\n    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):\n        from .computation import apply_ufunc\n\n        # See the docstring example for numpy.lib.mixins.NDArrayOperatorsMixin.\n        out = kwargs.get(""out"", ())\n        for x in inputs + out:\n            if not isinstance(x, self._HANDLED_TYPES + (SupportsArithmetic,)):\n                return NotImplemented\n\n        if ufunc.signature is not None:\n            raise NotImplementedError(\n                ""{} not supported: xarray objects do not directly implement ""\n                ""generalized ufuncs. Instead, use xarray.apply_ufunc or ""\n                ""explicitly convert to xarray objects to NumPy arrays ""\n                ""(e.g., with `.values`)."".format(ufunc)\n            )\n\n        if method != ""__call__"":\n            # TODO: support other methods, e.g., reduce and accumulate.\n            raise NotImplementedError(\n                ""{} method for ufunc {} is not implemented on xarray objects, ""\n                ""which currently only support the __call__ method. As an ""\n                ""alternative, consider explicitly converting xarray objects ""\n                ""to NumPy arrays (e.g., with `.values`)."".format(method, ufunc)\n            )\n\n        if any(isinstance(o, SupportsArithmetic) for o in out):\n            # TODO: implement this with logic like _inplace_binary_op. This\n            # will be necessary to use NDArrayOperatorsMixin.\n            raise NotImplementedError(\n                ""xarray objects are not yet supported in the `out` argument ""\n                ""for ufuncs. As an alternative, consider explicitly ""\n                ""converting xarray objects to NumPy arrays (e.g., with ""\n                ""`.values`).""\n            )\n\n        join = dataset_join = OPTIONS[""arithmetic_join""]\n\n        return apply_ufunc(\n            ufunc,\n            *inputs,\n            input_core_dims=((),) * ufunc.nin,\n            output_core_dims=((),) * ufunc.nout,\n            join=join,\n            dataset_join=dataset_join,\n            dataset_fill_value=np.nan,\n            kwargs=kwargs,\n            dask=""allowed"",\n        )\n\n    # this has no runtime function - these are listed so IDEs know these\n    # methods are defined and don\'t warn on these operations\n    __lt__ = (\n        __le__\n    ) = (\n        __ge__\n    ) = (\n        __gt__\n    ) = (\n        __add__\n    ) = (\n        __sub__\n    ) = (\n        __mul__\n    ) = (\n        __truediv__\n    ) = (\n        __floordiv__\n    ) = (\n        __mod__\n    ) = (\n        __pow__\n    ) = __and__ = __xor__ = __or__ = __div__ = __eq__ = __ne__ = not_implemented\n'"
xarray/core/combine.py,6,"b'import itertools\nimport warnings\nfrom collections import Counter\nfrom textwrap import dedent\n\nimport pandas as pd\n\nfrom . import dtypes\nfrom .concat import concat\nfrom .dataarray import DataArray\nfrom .dataset import Dataset\nfrom .merge import merge\n\n\ndef _infer_concat_order_from_positions(datasets):\n    combined_ids = dict(_infer_tile_ids_from_nested_list(datasets, ()))\n    return combined_ids\n\n\ndef _infer_tile_ids_from_nested_list(entry, current_pos):\n    """"""\n    Given a list of lists (of lists...) of objects, returns a iterator\n    which returns a tuple containing the index of each object in the nested\n    list structure as the key, and the object. This can then be called by the\n    dict constructor to create a dictionary of the objects organised by their\n    position in the original nested list.\n\n    Recursively traverses the given structure, while keeping track of the\n    current position. Should work for any type of object which isn\'t a list.\n\n    Parameters\n    ----------\n    entry : list[list[obj, obj, ...], ...]\n        List of lists of arbitrary depth, containing objects in the order\n        they are to be concatenated.\n\n    Returns\n    -------\n    combined_tile_ids : dict[tuple(int, ...), obj]\n    """"""\n\n    if isinstance(entry, list):\n        for i, item in enumerate(entry):\n            yield from _infer_tile_ids_from_nested_list(item, current_pos + (i,))\n    else:\n        yield current_pos, entry\n\n\ndef _infer_concat_order_from_coords(datasets):\n\n    concat_dims = []\n    tile_ids = [() for ds in datasets]\n\n    # All datasets have same variables because they\'ve been grouped as such\n    ds0 = datasets[0]\n    for dim in ds0.dims:\n\n        # Check if dim is a coordinate dimension\n        if dim in ds0:\n\n            # Need to read coordinate values to do ordering\n            indexes = [ds.indexes.get(dim) for ds in datasets]\n            if any(index is None for index in indexes):\n                raise ValueError(\n                    ""Every dimension needs a coordinate for ""\n                    ""inferring concatenation order""\n                )\n\n            # If dimension coordinate values are same on every dataset then\n            # should be leaving this dimension alone (it\'s just a ""bystander"")\n            if not all(index.equals(indexes[0]) for index in indexes[1:]):\n\n                # Infer order datasets should be arranged in along this dim\n                concat_dims.append(dim)\n\n                if all(index.is_monotonic_increasing for index in indexes):\n                    ascending = True\n                elif all(index.is_monotonic_decreasing for index in indexes):\n                    ascending = False\n                else:\n                    raise ValueError(\n                        ""Coordinate variable {} is neither ""\n                        ""monotonically increasing nor ""\n                        ""monotonically decreasing on all datasets"".format(dim)\n                    )\n\n                # Assume that any two datasets whose coord along dim starts\n                # with the same value have the same coord values throughout.\n                if any(index.size == 0 for index in indexes):\n                    raise ValueError(""Cannot handle size zero dimensions"")\n                first_items = pd.Index([index[0] for index in indexes])\n\n                # Sort datasets along dim\n                # We want rank but with identical elements given identical\n                # position indices - they should be concatenated along another\n                # dimension, not along this one\n                series = first_items.to_series()\n                rank = series.rank(method=""dense"", ascending=ascending)\n                order = rank.astype(int).values - 1\n\n                # Append positions along extra dimension to structure which\n                # encodes the multi-dimensional concatentation order\n                tile_ids = [\n                    tile_id + (position,) for tile_id, position in zip(tile_ids, order)\n                ]\n\n    if len(datasets) > 1 and not concat_dims:\n        raise ValueError(\n            ""Could not find any dimension coordinates to use to ""\n            ""order the datasets for concatenation""\n        )\n\n    combined_ids = dict(zip(tile_ids, datasets))\n\n    return combined_ids, concat_dims\n\n\ndef _check_dimension_depth_tile_ids(combined_tile_ids):\n    """"""\n    Check all tuples are the same length, i.e. check that all lists are\n    nested to the same depth.\n    """"""\n    tile_ids = combined_tile_ids.keys()\n    nesting_depths = [len(tile_id) for tile_id in tile_ids]\n    if not nesting_depths:\n        nesting_depths = [0]\n    if not set(nesting_depths) == {nesting_depths[0]}:\n        raise ValueError(\n            ""The supplied objects do not form a hypercube because""\n            "" sub-lists do not have consistent depths""\n        )\n    # return these just to be reused in _check_shape_tile_ids\n    return tile_ids, nesting_depths\n\n\ndef _check_shape_tile_ids(combined_tile_ids):\n    """"""Check all lists along one dimension are same length.""""""\n    tile_ids, nesting_depths = _check_dimension_depth_tile_ids(combined_tile_ids)\n    for dim in range(nesting_depths[0]):\n        indices_along_dim = [tile_id[dim] for tile_id in tile_ids]\n        occurrences = Counter(indices_along_dim)\n        if len(set(occurrences.values())) != 1:\n            raise ValueError(\n                ""The supplied objects do not form a hypercube ""\n                ""because sub-lists do not have consistent ""\n                ""lengths along dimension"" + str(dim)\n            )\n\n\ndef _combine_nd(\n    combined_ids,\n    concat_dims,\n    data_vars=""all"",\n    coords=""different"",\n    compat=""no_conflicts"",\n    fill_value=dtypes.NA,\n    join=""outer"",\n    combine_attrs=""drop"",\n):\n    """"""\n    Combines an N-dimensional structure of datasets into one by applying a\n    series of either concat and merge operations along each dimension.\n\n    No checks are performed on the consistency of the datasets, concat_dims or\n    tile_IDs, because it is assumed that this has already been done.\n\n    Parameters\n    ----------\n    combined_ids : Dict[Tuple[int, ...]], xarray.Dataset]\n        Structure containing all datasets to be concatenated with ""tile_IDs"" as\n        keys, which specify position within the desired final combined result.\n    concat_dims : sequence of str\n        The dimensions along which the datasets should be concatenated. Must be\n        in order, and the length must match the length of the tuples used as\n        keys in combined_ids. If the string is a dimension name then concat\n        along that dimension, if it is None then merge.\n\n    Returns\n    -------\n    combined_ds : xarray.Dataset\n    """"""\n\n    example_tile_id = next(iter(combined_ids.keys()))\n\n    n_dims = len(example_tile_id)\n    if len(concat_dims) != n_dims:\n        raise ValueError(\n            ""concat_dims has length {} but the datasets ""\n            ""passed are nested in a {}-dimensional structure"".format(\n                len(concat_dims), n_dims\n            )\n        )\n\n    # Each iteration of this loop reduces the length of the tile_ids tuples\n    # by one. It always combines along the first dimension, removing the first\n    # element of the tuple\n    for concat_dim in concat_dims:\n        combined_ids = _combine_all_along_first_dim(\n            combined_ids,\n            dim=concat_dim,\n            data_vars=data_vars,\n            coords=coords,\n            compat=compat,\n            fill_value=fill_value,\n            join=join,\n            combine_attrs=combine_attrs,\n        )\n    (combined_ds,) = combined_ids.values()\n    return combined_ds\n\n\ndef _combine_all_along_first_dim(\n    combined_ids,\n    dim,\n    data_vars,\n    coords,\n    compat,\n    fill_value=dtypes.NA,\n    join=""outer"",\n    combine_attrs=""drop"",\n):\n\n    # Group into lines of datasets which must be combined along dim\n    # need to sort by _new_tile_id first for groupby to work\n    # TODO: is the sorted need?\n    combined_ids = dict(sorted(combined_ids.items(), key=_new_tile_id))\n    grouped = itertools.groupby(combined_ids.items(), key=_new_tile_id)\n\n    # Combine all of these datasets along dim\n    new_combined_ids = {}\n    for new_id, group in grouped:\n        combined_ids = dict(sorted(group))\n        datasets = combined_ids.values()\n        new_combined_ids[new_id] = _combine_1d(\n            datasets, dim, compat, data_vars, coords, fill_value, join, combine_attrs\n        )\n    return new_combined_ids\n\n\ndef _combine_1d(\n    datasets,\n    concat_dim,\n    compat=""no_conflicts"",\n    data_vars=""all"",\n    coords=""different"",\n    fill_value=dtypes.NA,\n    join=""outer"",\n    combine_attrs=""drop"",\n):\n    """"""\n    Applies either concat or merge to 1D list of datasets depending on value\n    of concat_dim\n    """"""\n\n    if concat_dim is not None:\n        try:\n            combined = concat(\n                datasets,\n                dim=concat_dim,\n                data_vars=data_vars,\n                coords=coords,\n                compat=compat,\n                fill_value=fill_value,\n                join=join,\n                combine_attrs=combine_attrs,\n            )\n        except ValueError as err:\n            if ""encountered unexpected variable"" in str(err):\n                raise ValueError(\n                    ""These objects cannot be combined using only ""\n                    ""xarray.combine_nested, instead either use ""\n                    ""xarray.combine_by_coords, or do it manually ""\n                    ""with xarray.concat, xarray.merge and ""\n                    ""xarray.align""\n                )\n            else:\n                raise\n    else:\n        combined = merge(\n            datasets,\n            compat=compat,\n            fill_value=fill_value,\n            join=join,\n            combine_attrs=combine_attrs,\n        )\n\n    return combined\n\n\ndef _new_tile_id(single_id_ds_pair):\n    tile_id, ds = single_id_ds_pair\n    return tile_id[1:]\n\n\ndef _nested_combine(\n    datasets,\n    concat_dims,\n    compat,\n    data_vars,\n    coords,\n    ids,\n    fill_value=dtypes.NA,\n    join=""outer"",\n    combine_attrs=""drop"",\n):\n\n    if len(datasets) == 0:\n        return Dataset()\n\n    # Arrange datasets for concatenation\n    # Use information from the shape of the user input\n    if not ids:\n        # Determine tile_IDs by structure of input in N-D\n        # (i.e. ordering in list-of-lists)\n        combined_ids = _infer_concat_order_from_positions(datasets)\n    else:\n        # Already sorted so just use the ids already passed\n        combined_ids = dict(zip(ids, datasets))\n\n    # Check that the inferred shape is combinable\n    _check_shape_tile_ids(combined_ids)\n\n    # Apply series of concatenate or merge operations along each dimension\n    combined = _combine_nd(\n        combined_ids,\n        concat_dims,\n        compat=compat,\n        data_vars=data_vars,\n        coords=coords,\n        fill_value=fill_value,\n        join=join,\n        combine_attrs=combine_attrs,\n    )\n    return combined\n\n\ndef combine_nested(\n    datasets,\n    concat_dim,\n    compat=""no_conflicts"",\n    data_vars=""all"",\n    coords=""different"",\n    fill_value=dtypes.NA,\n    join=""outer"",\n    combine_attrs=""drop"",\n):\n    """"""\n    Explicitly combine an N-dimensional grid of datasets into one by using a\n    succession of concat and merge operations along each dimension of the grid.\n\n    Does not sort the supplied datasets under any circumstances, so the\n    datasets must be passed in the order you wish them to be concatenated. It\n    does align coordinates, but different variables on datasets can cause it to\n    fail under some scenarios. In complex cases, you may need to clean up your\n    data and use concat/merge explicitly.\n\n    To concatenate along multiple dimensions the datasets must be passed as a\n    nested list-of-lists, with a depth equal to the length of ``concat_dims``.\n    ``manual_combine`` will concatenate along the top-level list first.\n\n    Useful for combining datasets from a set of nested directories, or for\n    collecting the output of a simulation parallelized along multiple\n    dimensions.\n\n    Parameters\n    ----------\n    datasets : list or nested list of xarray.Dataset objects.\n        Dataset objects to combine.\n        If concatenation or merging along more than one dimension is desired,\n        then datasets must be supplied in a nested list-of-lists.\n    concat_dim : str, or list of str, DataArray, Index or None\n        Dimensions along which to concatenate variables, as used by\n        :py:func:`xarray.concat`.\n        Set ``concat_dim=[..., None, ...]`` explicitly to disable concatenation\n        and merge instead along a particular dimension.\n        The position of ``None`` in the list specifies the dimension of the\n        nested-list input along which to merge.\n        Must be the same length as the depth of the list passed to\n        ``datasets``.\n    compat : {\'identical\', \'equals\', \'broadcast_equals\',\n              \'no_conflicts\', \'override\'}, optional\n        String indicating how to compare variables of the same name for\n        potential merge conflicts:\n\n        - \'broadcast_equals\': all values must be equal when variables are\n          broadcast against each other to ensure common dimensions.\n        - \'equals\': all values and dimensions must be the same.\n        - \'identical\': all values, dimensions and attributes must be the\n          same.\n        - \'no_conflicts\': only values which are not null in both datasets\n          must be equal. The returned dataset then contains the combination\n          of all non-null values.\n        - \'override\': skip comparing and pick variable from first dataset\n    data_vars : {\'minimal\', \'different\', \'all\' or list of str}, optional\n        Details are in the documentation of concat\n    coords : {\'minimal\', \'different\', \'all\' or list of str}, optional\n        Details are in the documentation of concat\n    fill_value : scalar, optional\n        Value to use for newly missing values\n    join : {\'outer\', \'inner\', \'left\', \'right\', \'exact\'}, optional\n        String indicating how to combine differing indexes\n        (excluding concat_dim) in objects\n\n        - \'outer\': use the union of object indexes\n        - \'inner\': use the intersection of object indexes\n        - \'left\': use indexes from the first object with each dimension\n        - \'right\': use indexes from the last object with each dimension\n        - \'exact\': instead of aligning, raise `ValueError` when indexes to be\n          aligned are not equal\n        - \'override\': if indexes are of same size, rewrite indexes to be\n          those of the first object with that dimension. Indexes for the same\n          dimension must have the same size in all objects.\n    combine_attrs : {\'drop\', \'identical\', \'no_conflicts\', \'override\'},\n                    default \'drop\'\n        String indicating how to combine attrs of the objects being merged:\n\n        - \'drop\': empty attrs on returned Dataset.\n        - \'identical\': all attrs must be the same on every object.\n        - \'no_conflicts\': attrs from all objects are combined, any that have\n          the same name must also have the same value.\n        - \'override\': skip comparing and copy attrs from the first dataset to\n          the result.\n\n    Returns\n    -------\n    combined : xarray.Dataset\n\n    Examples\n    --------\n\n    A common task is collecting data from a parallelized simulation in which\n    each process wrote out to a separate file. A domain which was decomposed\n    into 4 parts, 2 each along both the x and y axes, requires organising the\n    datasets into a doubly-nested list, e.g:\n\n    >>> x1y1\n    <xarray.Dataset>\n    Dimensions:         (x: 2, y: 2)\n    Dimensions without coordinates: x, y\n    Data variables:\n      temperature       (x, y) float64 11.04 23.57 20.77 ...\n      precipitation     (x, y) float64 5.904 2.453 3.404 ...\n\n    >>> ds_grid = [[x1y1, x1y2], [x2y1, x2y2]]\n    >>> combined = xr.combine_nested(ds_grid, concat_dim=[""x"", ""y""])\n    <xarray.Dataset>\n    Dimensions:         (x: 4, y: 4)\n    Dimensions without coordinates: x, y\n    Data variables:\n      temperature       (x, y) float64 11.04 23.57 20.77 ...\n      precipitation     (x, y) float64 5.904 2.453 3.404 ...\n\n    ``manual_combine`` can also be used to explicitly merge datasets with\n    different variables. For example if we have 4 datasets, which are divided\n    along two times, and contain two different variables, we can pass ``None``\n    to ``concat_dim`` to specify the dimension of the nested list over which\n    we wish to use ``merge`` instead of ``concat``:\n\n    >>> t1temp\n    <xarray.Dataset>\n    Dimensions:         (t: 5)\n    Dimensions without coordinates: t\n    Data variables:\n      temperature       (t) float64 11.04 23.57 20.77 ...\n\n    >>> t1precip\n    <xarray.Dataset>\n    Dimensions:         (t: 5)\n    Dimensions without coordinates: t\n    Data variables:\n      precipitation     (t) float64 5.904 2.453 3.404 ...\n\n    >>> ds_grid = [[t1temp, t1precip], [t2temp, t2precip]]\n    >>> combined = xr.combine_nested(ds_grid, concat_dim=[""t"", None])\n    <xarray.Dataset>\n    Dimensions:         (t: 10)\n    Dimensions without coordinates: t\n    Data variables:\n      temperature       (t) float64 11.04 23.57 20.77 ...\n      precipitation     (t) float64 5.904 2.453 3.404 ...\n\n    See also\n    --------\n    concat\n    merge\n    auto_combine\n    """"""\n    if isinstance(concat_dim, (str, DataArray)) or concat_dim is None:\n        concat_dim = [concat_dim]\n\n    # The IDs argument tells _manual_combine that datasets aren\'t yet sorted\n    return _nested_combine(\n        datasets,\n        concat_dims=concat_dim,\n        compat=compat,\n        data_vars=data_vars,\n        coords=coords,\n        ids=False,\n        fill_value=fill_value,\n        join=join,\n        combine_attrs=combine_attrs,\n    )\n\n\ndef vars_as_keys(ds):\n    return tuple(sorted(ds))\n\n\ndef combine_by_coords(\n    datasets,\n    compat=""no_conflicts"",\n    data_vars=""all"",\n    coords=""different"",\n    fill_value=dtypes.NA,\n    join=""outer"",\n    combine_attrs=""no_conflicts"",\n):\n    """"""\n    Attempt to auto-magically combine the given datasets into one by using\n    dimension coordinates.\n\n    This method attempts to combine a group of datasets along any number of\n    dimensions into a single entity by inspecting coords and metadata and using\n    a combination of concat and merge.\n\n    Will attempt to order the datasets such that the values in their dimension\n    coordinates are monotonic along all dimensions. If it cannot determine the\n    order in which to concatenate the datasets, it will raise a ValueError.\n    Non-coordinate dimensions will be ignored, as will any coordinate\n    dimensions which do not vary between each dataset.\n\n    Aligns coordinates, but different variables on datasets can cause it\n    to fail under some scenarios. In complex cases, you may need to clean up\n    your data and use concat/merge explicitly (also see `manual_combine`).\n\n    Works well if, for example, you have N years of data and M data variables,\n    and each combination of a distinct time period and set of data variables is\n    saved as its own dataset. Also useful for if you have a simulation which is\n    parallelized in multiple dimensions, but has global coordinates saved in\n    each file specifying the positions of points within the global domain.\n\n    Parameters\n    ----------\n    datasets : sequence of xarray.Dataset\n        Dataset objects to combine.\n    compat : {\'identical\', \'equals\', \'broadcast_equals\', \'no_conflicts\', \'override\'}, optional\n        String indicating how to compare variables of the same name for\n        potential conflicts:\n\n        - \'broadcast_equals\': all values must be equal when variables are\n          broadcast against each other to ensure common dimensions.\n        - \'equals\': all values and dimensions must be the same.\n        - \'identical\': all values, dimensions and attributes must be the\n          same.\n        - \'no_conflicts\': only values which are not null in both datasets\n          must be equal. The returned dataset then contains the combination\n          of all non-null values.\n        - \'override\': skip comparing and pick variable from first dataset\n    data_vars : {\'minimal\', \'different\', \'all\' or list of str}, optional\n        These data variables will be concatenated together:\n\n        * \'minimal\': Only data variables in which the dimension already\n          appears are included.\n        * \'different\': Data variables which are not equal (ignoring\n          attributes) across all datasets are also concatenated (as well as\n          all for which dimension already appears). Beware: this option may\n          load the data payload of data variables into memory if they are not\n          already loaded.\n        * \'all\': All data variables will be concatenated.\n        * list of str: The listed data variables will be concatenated, in\n          addition to the \'minimal\' data variables.\n\n        If objects are DataArrays, `data_vars` must be \'all\'.\n    coords : {\'minimal\', \'different\', \'all\' or list of str}, optional\n        As per the \'data_vars\' kwarg, but for coordinate variables.\n    fill_value : scalar, optional\n        Value to use for newly missing values. If None, raises a ValueError if\n        the passed Datasets do not create a complete hypercube.\n    join : {\'outer\', \'inner\', \'left\', \'right\', \'exact\'}, optional\n        String indicating how to combine differing indexes\n        (excluding concat_dim) in objects\n\n        - \'outer\': use the union of object indexes\n        - \'inner\': use the intersection of object indexes\n        - \'left\': use indexes from the first object with each dimension\n        - \'right\': use indexes from the last object with each dimension\n        - \'exact\': instead of aligning, raise `ValueError` when indexes to be\n          aligned are not equal\n        - \'override\': if indexes are of same size, rewrite indexes to be\n          those of the first object with that dimension. Indexes for the same\n          dimension must have the same size in all objects.\n    combine_attrs : {\'drop\', \'identical\', \'no_conflicts\', \'override\'},\n                    default \'drop\'\n        String indicating how to combine attrs of the objects being merged:\n\n        - \'drop\': empty attrs on returned Dataset.\n        - \'identical\': all attrs must be the same on every object.\n        - \'no_conflicts\': attrs from all objects are combined, any that have\n          the same name must also have the same value.\n        - \'override\': skip comparing and copy attrs from the first dataset to\n          the result.\n\n    Returns\n    -------\n    combined : xarray.Dataset\n\n    See also\n    --------\n    concat\n    merge\n    combine_nested\n\n    Examples\n    --------\n\n    Combining two datasets using their common dimension coordinates. Notice\n    they are concatenated based on the values in their dimension coordinates,\n    not on their position in the list passed to `combine_by_coords`.\n\n    >>> import numpy as np\n    >>> import xarray as xr\n\n    >>> x1 = xr.Dataset(\n    ...     {\n    ...         ""temperature"": ((""y"", ""x""), 20 * np.random.rand(6).reshape(2, 3)),\n    ...         ""precipitation"": ((""y"", ""x""), np.random.rand(6).reshape(2, 3)),\n    ...     },\n    ...     coords={""y"": [0, 1], ""x"": [10, 20, 30]},\n    ... )\n    >>> x2 = xr.Dataset(\n    ...     {\n    ...         ""temperature"": ((""y"", ""x""), 20 * np.random.rand(6).reshape(2, 3)),\n    ...         ""precipitation"": ((""y"", ""x""), np.random.rand(6).reshape(2, 3)),\n    ...     },\n    ...     coords={""y"": [2, 3], ""x"": [10, 20, 30]},\n    ... )\n    >>> x3 = xr.Dataset(\n    ...     {\n    ...         ""temperature"": ((""y"", ""x""), 20 * np.random.rand(6).reshape(2, 3)),\n    ...         ""precipitation"": ((""y"", ""x""), np.random.rand(6).reshape(2, 3)),\n    ...     },\n    ...     coords={""y"": [2, 3], ""x"": [40, 50, 60]},\n    ... )\n\n    >>> x1\n    <xarray.Dataset>\n    Dimensions:        (x: 3, y: 2)\n    Coordinates:\n    * y              (y) int64 0 1\n    * x              (x) int64 10 20 30\n    Data variables:\n        temperature    (y, x) float64 1.654 10.63 7.015 2.543 13.93 9.436\n        precipitation  (y, x) float64 0.2136 0.9974 0.7603 0.4679 0.3115 0.945\n\n    >>> x2\n    <xarray.Dataset>\n    Dimensions:        (x: 3, y: 2)\n    Coordinates:\n    * y              (y) int64 2 3\n    * x              (x) int64 10 20 30\n    Data variables:\n        temperature    (y, x) float64 9.341 0.1251 6.269 7.709 8.82 2.316\n        precipitation  (y, x) float64 0.1728 0.1178 0.03018 0.6509 0.06938 0.3792\n\n    >>> x3\n    <xarray.Dataset>\n    Dimensions:        (x: 3, y: 2)\n    Coordinates:\n    * y              (y) int64 2 3\n    * x              (x) int64 40 50 60\n    Data variables:\n        temperature    (y, x) float64 2.789 2.446 6.551 12.46 2.22 15.96\n        precipitation  (y, x) float64 0.4804 0.1902 0.2457 0.6125 0.4654 0.5953\n\n    >>> xr.combine_by_coords([x2, x1])\n    <xarray.Dataset>\n    Dimensions:        (x: 3, y: 4)\n    Coordinates:\n    * x              (x) int64 10 20 30\n    * y              (y) int64 0 1 2 3\n    Data variables:\n        temperature    (y, x) float64 1.654 10.63 7.015 2.543 ... 7.709 8.82 2.316\n        precipitation  (y, x) float64 0.2136 0.9974 0.7603 ... 0.6509 0.06938 0.3792\n\n    >>> xr.combine_by_coords([x3, x1])\n    <xarray.Dataset>\n    Dimensions:        (x: 6, y: 4)\n    Coordinates:\n    * x              (x) int64 10 20 30 40 50 60\n    * y              (y) int64 0 1 2 3\n    Data variables:\n        temperature    (y, x) float64 1.654 10.63 7.015 nan ... nan 12.46 2.22 15.96\n        precipitation  (y, x) float64 0.2136 0.9974 0.7603 ... 0.6125 0.4654 0.5953\n\n    >>> xr.combine_by_coords([x3, x1], join=""override"")\n    <xarray.Dataset>\n    Dimensions:        (x: 3, y: 4)\n    Coordinates:\n    * x              (x) int64 10 20 30\n    * y              (y) int64 0 1 2 3\n    Data variables:\n    temperature    (y, x) float64 1.654 10.63 7.015 2.543 ... 12.46 2.22 15.96\n    precipitation  (y, x) float64 0.2136 0.9974 0.7603 ... 0.6125 0.4654 0.5953\n\n    >>> xr.combine_by_coords([x1, x2, x3])\n    <xarray.Dataset>\n    Dimensions:        (x: 6, y: 4)\n    Coordinates:\n    * x              (x) int64 10 20 30 40 50 60\n    * y              (y) int64 0 1 2 3\n    Data variables:\n    temperature    (y, x) float64 1.654 10.63 7.015 nan ... 12.46 2.22 15.96\n    precipitation  (y, x) float64 0.2136 0.9974 0.7603 ... 0.6125 0.4654 0.5953\n    """"""\n\n    # Group by data vars\n    sorted_datasets = sorted(datasets, key=vars_as_keys)\n    grouped_by_vars = itertools.groupby(sorted_datasets, key=vars_as_keys)\n\n    # Perform the multidimensional combine on each group of data variables\n    # before merging back together\n    concatenated_grouped_by_data_vars = []\n    for vars, datasets_with_same_vars in grouped_by_vars:\n        combined_ids, concat_dims = _infer_concat_order_from_coords(\n            list(datasets_with_same_vars)\n        )\n\n        if fill_value is None:\n            # check that datasets form complete hypercube\n            _check_shape_tile_ids(combined_ids)\n        else:\n            # check only that all datasets have same dimension depth for these\n            # vars\n            _check_dimension_depth_tile_ids(combined_ids)\n\n        # Concatenate along all of concat_dims one by one to create single ds\n        concatenated = _combine_nd(\n            combined_ids,\n            concat_dims=concat_dims,\n            data_vars=data_vars,\n            coords=coords,\n            compat=compat,\n            fill_value=fill_value,\n            join=join,\n            combine_attrs=combine_attrs,\n        )\n\n        # Check the overall coordinates are monotonically increasing\n        for dim in concat_dims:\n            indexes = concatenated.indexes.get(dim)\n            if not (indexes.is_monotonic_increasing or indexes.is_monotonic_decreasing):\n                raise ValueError(\n                    ""Resulting object does not have monotonic""\n                    "" global indexes along dimension {}"".format(dim)\n                )\n        concatenated_grouped_by_data_vars.append(concatenated)\n\n    return merge(\n        concatenated_grouped_by_data_vars,\n        compat=compat,\n        fill_value=fill_value,\n        join=join,\n        combine_attrs=combine_attrs,\n    )\n\n\n# Everything beyond here is only needed until the deprecation cycle in #2616\n# is completed\n\n\n_CONCAT_DIM_DEFAULT = ""__infer_concat_dim__""\n\n\ndef auto_combine(\n    datasets,\n    concat_dim=""_not_supplied"",\n    compat=""no_conflicts"",\n    data_vars=""all"",\n    coords=""different"",\n    fill_value=dtypes.NA,\n    join=""outer"",\n    from_openmfds=False,\n):\n    """"""\n    Attempt to auto-magically combine the given datasets into one.\n\n    This entire function is deprecated in favour of ``combine_nested`` and\n    ``combine_by_coords``.\n\n    This method attempts to combine a list of datasets into a single entity by\n    inspecting metadata and using a combination of concat and merge.\n    It does not concatenate along more than one dimension or sort data under\n    any circumstances. It does align coordinates, but different variables on\n    datasets can cause it to fail under some scenarios. In complex cases, you\n    may need to clean up your data and use ``concat``/``merge`` explicitly.\n    ``auto_combine`` works well if you have N years of data and M data\n    variables, and each combination of a distinct time period and set of data\n    variables is saved its own dataset.\n\n    Parameters\n    ----------\n    datasets : sequence of xarray.Dataset\n        Dataset objects to merge.\n    concat_dim : str or DataArray or Index, optional\n        Dimension along which to concatenate variables, as used by\n        :py:func:`xarray.concat`. You only need to provide this argument if\n        the dimension along which you want to concatenate is not a dimension\n        in the original datasets, e.g., if you want to stack a collection of\n        2D arrays along a third dimension.\n        By default, xarray attempts to infer this argument by examining\n        component files. Set ``concat_dim=None`` explicitly to disable\n        concatenation.\n    compat : {\'identical\', \'equals\', \'broadcast_equals\',\n             \'no_conflicts\', \'override\'}, optional\n        String indicating how to compare variables of the same name for\n        potential conflicts:\n\n        - \'broadcast_equals\': all values must be equal when variables are\n          broadcast against each other to ensure common dimensions.\n        - \'equals\': all values and dimensions must be the same.\n        - \'identical\': all values, dimensions and attributes must be the\n          same.\n        - \'no_conflicts\': only values which are not null in both datasets\n          must be equal. The returned dataset then contains the combination\n          of all non-null values.\n        - \'override\': skip comparing and pick variable from first dataset\n    data_vars : {\'minimal\', \'different\', \'all\' or list of str}, optional\n        Details are in the documentation of concat\n    coords : {\'minimal\', \'different\', \'all\' o list of str}, optional\n        Details are in the documentation of concat\n    fill_value : scalar, optional\n        Value to use for newly missing values\n    join : {\'outer\', \'inner\', \'left\', \'right\', \'exact\'}, optional\n        String indicating how to combine differing indexes\n        (excluding concat_dim) in objects\n\n        - \'outer\': use the union of object indexes\n        - \'inner\': use the intersection of object indexes\n        - \'left\': use indexes from the first object with each dimension\n        - \'right\': use indexes from the last object with each dimension\n        - \'exact\': instead of aligning, raise `ValueError` when indexes to be\n          aligned are not equal\n        - \'override\': if indexes are of same size, rewrite indexes to be\n          those of the first object with that dimension. Indexes for the same\n          dimension must have the same size in all objects.\n\n    Returns\n    -------\n    combined : xarray.Dataset\n\n    See also\n    --------\n    concat\n    Dataset.merge\n    """"""\n\n    if not from_openmfds:\n        basic_msg = dedent(\n            """"""\\\n        In xarray version 0.15 `auto_combine` will be deprecated. See\n        http://xarray.pydata.org/en/stable/combining.html#combining-multi""""""\n        )\n        warnings.warn(basic_msg, FutureWarning, stacklevel=2)\n\n    if concat_dim == ""_not_supplied"":\n        concat_dim = _CONCAT_DIM_DEFAULT\n        message = """"\n    else:\n        message = dedent(\n            """"""\\\n        Also `open_mfdataset` will no longer accept a `concat_dim` argument.\n        To get equivalent behaviour from now on please use the new\n        `combine_nested` function instead (or the `combine=\'nested\'` option to\n        `open_mfdataset`).""""""\n        )\n\n    if _dimension_coords_exist(datasets):\n        message += dedent(\n            """"""\\\n        The datasets supplied have global dimension coordinates. You may want\n        to use the new `combine_by_coords` function (or the\n        `combine=\'by_coords\'` option to `open_mfdataset`) to order the datasets\n        before concatenation. Alternatively, to continue concatenating based\n        on the order the datasets are supplied in future, please use the new\n        `combine_nested` function (or the `combine=\'nested\'` option to\n        open_mfdataset).""""""\n        )\n    else:\n        message += dedent(\n            """"""\\\n        The datasets supplied do not have global dimension coordinates. In\n        future, to continue concatenating without supplying dimension\n        coordinates, please use the new `combine_nested` function (or the\n        `combine=\'nested\'` option to open_mfdataset.""""""\n        )\n\n    if _requires_concat_and_merge(datasets):\n        manual_dims = [concat_dim].append(None)\n        message += dedent(\n            """"""\\\n        The datasets supplied require both concatenation and merging. From\n        xarray version 0.15 this will operation will require either using the\n        new `combine_nested` function (or the `combine=\'nested\'` option to\n        open_mfdataset), with a nested list structure such that you can combine\n        along the dimensions {}. Alternatively if your datasets have global\n        dimension coordinates then you can use the new `combine_by_coords`\n        function."""""".format(\n                manual_dims\n            )\n        )\n\n    warnings.warn(message, FutureWarning, stacklevel=2)\n\n    return _old_auto_combine(\n        datasets,\n        concat_dim=concat_dim,\n        compat=compat,\n        data_vars=data_vars,\n        coords=coords,\n        fill_value=fill_value,\n        join=join,\n    )\n\n\ndef _dimension_coords_exist(datasets):\n    """"""\n    Check if the datasets have consistent global dimension coordinates\n    which would in future be used by `auto_combine` for concatenation ordering.\n    """"""\n\n    # Group by data vars\n    sorted_datasets = sorted(datasets, key=vars_as_keys)\n    grouped_by_vars = itertools.groupby(sorted_datasets, key=vars_as_keys)\n\n    # Simulates performing the multidimensional combine on each group of data\n    # variables before merging back together\n    try:\n        for vars, datasets_with_same_vars in grouped_by_vars:\n            _infer_concat_order_from_coords(list(datasets_with_same_vars))\n        return True\n    except ValueError:\n        # ValueError means datasets don\'t have global dimension coordinates\n        # Or something else went wrong in trying to determine them\n        return False\n\n\ndef _requires_concat_and_merge(datasets):\n    """"""\n    Check if the datasets require the use of both xarray.concat and\n    xarray.merge, which in future might require the user to use\n    `manual_combine` instead.\n    """"""\n    # Group by data vars\n    sorted_datasets = sorted(datasets, key=vars_as_keys)\n    grouped_by_vars = itertools.groupby(sorted_datasets, key=vars_as_keys)\n\n    return len(list(grouped_by_vars)) > 1\n\n\ndef _old_auto_combine(\n    datasets,\n    concat_dim=_CONCAT_DIM_DEFAULT,\n    compat=""no_conflicts"",\n    data_vars=""all"",\n    coords=""different"",\n    fill_value=dtypes.NA,\n    join=""outer"",\n):\n    if concat_dim is not None:\n        dim = None if concat_dim is _CONCAT_DIM_DEFAULT else concat_dim\n\n        sorted_datasets = sorted(datasets, key=vars_as_keys)\n        grouped = itertools.groupby(sorted_datasets, key=vars_as_keys)\n\n        concatenated = [\n            _auto_concat(\n                list(datasets),\n                dim=dim,\n                data_vars=data_vars,\n                coords=coords,\n                compat=compat,\n                fill_value=fill_value,\n                join=join,\n            )\n            for vars, datasets in grouped\n        ]\n    else:\n        concatenated = datasets\n    merged = merge(concatenated, compat=compat, fill_value=fill_value, join=join)\n    return merged\n\n\ndef _auto_concat(\n    datasets,\n    dim=None,\n    data_vars=""all"",\n    coords=""different"",\n    fill_value=dtypes.NA,\n    join=""outer"",\n    compat=""no_conflicts"",\n):\n    if len(datasets) == 1 and dim is None:\n        # There is nothing more to combine, so kick out early.\n        return datasets[0]\n    else:\n        if dim is None:\n            ds0 = datasets[0]\n            ds1 = datasets[1]\n            concat_dims = set(ds0.dims)\n            if ds0.dims != ds1.dims:\n                dim_tuples = set(ds0.dims.items()) - set(ds1.dims.items())\n                concat_dims = {i for i, _ in dim_tuples}\n            if len(concat_dims) > 1:\n                concat_dims = {d for d in concat_dims if not ds0[d].equals(ds1[d])}\n            if len(concat_dims) > 1:\n                raise ValueError(\n                    ""too many different dimensions to "" ""concatenate: %s"" % concat_dims\n                )\n            elif len(concat_dims) == 0:\n                raise ValueError(\n                    ""cannot infer dimension to concatenate: ""\n                    ""supply the ``concat_dim`` argument ""\n                    ""explicitly""\n                )\n            (dim,) = concat_dims\n        return concat(\n            datasets,\n            dim=dim,\n            data_vars=data_vars,\n            coords=coords,\n            fill_value=fill_value,\n            compat=compat,\n        )\n'"
xarray/core/common.py,26,"b'import warnings\nfrom contextlib import suppress\nfrom html import escape\nfrom textwrap import dedent\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    Hashable,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    Tuple,\n    TypeVar,\n    Union,\n)\n\nimport numpy as np\nimport pandas as pd\n\nfrom . import dtypes, duck_array_ops, formatting, formatting_html, ops\nfrom .arithmetic import SupportsArithmetic\nfrom .npcompat import DTypeLike\nfrom .options import OPTIONS, _get_keep_attrs\nfrom .pycompat import dask_array_type\nfrom .rolling_exp import RollingExp\nfrom .utils import Frozen, either_dict_or_kwargs, is_scalar\n\n# Used as a sentinel value to indicate a all dimensions\nALL_DIMS = ...\n\n\nC = TypeVar(""C"")\nT = TypeVar(""T"")\n\n\nclass ImplementsArrayReduce:\n    __slots__ = ()\n\n    @classmethod\n    def _reduce_method(cls, func: Callable, include_skipna: bool, numeric_only: bool):\n        if include_skipna:\n\n            def wrapped_func(self, dim=None, axis=None, skipna=None, **kwargs):\n                return self.reduce(func, dim, axis, skipna=skipna, **kwargs)\n\n        else:\n\n            def wrapped_func(self, dim=None, axis=None, **kwargs):  # type: ignore\n                return self.reduce(func, dim, axis, **kwargs)\n\n        return wrapped_func\n\n    _reduce_extra_args_docstring = dedent(\n        """"""\\\n        dim : str or sequence of str, optional\n            Dimension(s) over which to apply `{name}`.\n        axis : int or sequence of int, optional\n            Axis(es) over which to apply `{name}`. Only one of the \'dim\'\n            and \'axis\' arguments can be supplied. If neither are supplied, then\n            `{name}` is calculated over axes.""""""\n    )\n\n    _cum_extra_args_docstring = dedent(\n        """"""\\\n        dim : str or sequence of str, optional\n            Dimension over which to apply `{name}`.\n        axis : int or sequence of int, optional\n            Axis over which to apply `{name}`. Only one of the \'dim\'\n            and \'axis\' arguments can be supplied.""""""\n    )\n\n\nclass ImplementsDatasetReduce:\n    __slots__ = ()\n\n    @classmethod\n    def _reduce_method(cls, func: Callable, include_skipna: bool, numeric_only: bool):\n        if include_skipna:\n\n            def wrapped_func(self, dim=None, skipna=None, **kwargs):\n                return self.reduce(\n                    func, dim, skipna=skipna, numeric_only=numeric_only, **kwargs\n                )\n\n        else:\n\n            def wrapped_func(self, dim=None, **kwargs):  # type: ignore\n                return self.reduce(func, dim, numeric_only=numeric_only, **kwargs)\n\n        return wrapped_func\n\n    _reduce_extra_args_docstring = dedent(\n        """"""\n        dim : str or sequence of str, optional\n            Dimension(s) over which to apply `{name}`.  By default `{name}` is\n            applied over all dimensions.\n        """"""\n    ).strip()\n\n    _cum_extra_args_docstring = dedent(\n        """"""\n        dim : str or sequence of str, optional\n            Dimension over which to apply `{name}`.\n        axis : int or sequence of int, optional\n            Axis over which to apply `{name}`. Only one of the \'dim\'\n            and \'axis\' arguments can be supplied.\n        """"""\n    ).strip()\n\n\nclass AbstractArray(ImplementsArrayReduce):\n    """"""Shared base class for DataArray and Variable.\n    """"""\n\n    __slots__ = ()\n\n    def __bool__(self: Any) -> bool:\n        return bool(self.values)\n\n    def __float__(self: Any) -> float:\n        return float(self.values)\n\n    def __int__(self: Any) -> int:\n        return int(self.values)\n\n    def __complex__(self: Any) -> complex:\n        return complex(self.values)\n\n    def __array__(self: Any, dtype: DTypeLike = None) -> np.ndarray:\n        return np.asarray(self.values, dtype=dtype)\n\n    def __repr__(self) -> str:\n        return formatting.array_repr(self)\n\n    def _repr_html_(self):\n        if OPTIONS[""display_style""] == ""text"":\n            return f""<pre>{escape(repr(self))}</pre>""\n        return formatting_html.array_repr(self)\n\n    def _iter(self: Any) -> Iterator[Any]:\n        for n in range(len(self)):\n            yield self[n]\n\n    def __iter__(self: Any) -> Iterator[Any]:\n        if self.ndim == 0:\n            raise TypeError(""iteration over a 0-d array"")\n        return self._iter()\n\n    def get_axis_num(\n        self, dim: Union[Hashable, Iterable[Hashable]]\n    ) -> Union[int, Tuple[int, ...]]:\n        """"""Return axis number(s) corresponding to dimension(s) in this array.\n\n        Parameters\n        ----------\n        dim : str or iterable of str\n            Dimension name(s) for which to lookup axes.\n\n        Returns\n        -------\n        int or tuple of int\n            Axis number or numbers corresponding to the given dimensions.\n        """"""\n        if isinstance(dim, Iterable) and not isinstance(dim, str):\n            return tuple(self._get_axis_num(d) for d in dim)\n        else:\n            return self._get_axis_num(dim)\n\n    def _get_axis_num(self: Any, dim: Hashable) -> int:\n        try:\n            return self.dims.index(dim)\n        except ValueError:\n            raise ValueError(f""{dim!r} not found in array dimensions {self.dims!r}"")\n\n    @property\n    def sizes(self: Any) -> Mapping[Hashable, int]:\n        """"""Ordered mapping from dimension names to lengths.\n\n        Immutable.\n\n        See also\n        --------\n        Dataset.sizes\n        """"""\n        return Frozen(dict(zip(self.dims, self.shape)))\n\n\nclass AttrAccessMixin:\n    """"""Mixin class that allows getting keys with attribute access\n    """"""\n\n    __slots__ = ()\n\n    def __init_subclass__(cls):\n        """"""Verify that all subclasses explicitly define ``__slots__``. If they don\'t,\n        raise error in the core xarray module and a FutureWarning in third-party\n        extensions.\n        """"""\n        if not hasattr(object.__new__(cls), ""__dict__""):\n            pass\n        elif cls.__module__.startswith(""xarray.""):\n            raise AttributeError(""%s must explicitly define __slots__"" % cls.__name__)\n        else:\n            cls.__setattr__ = cls._setattr_dict\n            warnings.warn(\n                ""xarray subclass %s should explicitly define __slots__"" % cls.__name__,\n                FutureWarning,\n                stacklevel=2,\n            )\n\n    @property\n    def _attr_sources(self) -> List[Mapping[Hashable, Any]]:\n        """"""List of places to look-up items for attribute-style access\n        """"""\n        return []\n\n    @property\n    def _item_sources(self) -> List[Mapping[Hashable, Any]]:\n        """"""List of places to look-up items for key-autocompletion\n        """"""\n        return []\n\n    def __getattr__(self, name: str) -> Any:\n        if name not in {""__dict__"", ""__setstate__""}:\n            # this avoids an infinite loop when pickle looks for the\n            # __setstate__ attribute before the xarray object is initialized\n            for source in self._attr_sources:\n                with suppress(KeyError):\n                    return source[name]\n        raise AttributeError(\n            ""{!r} object has no attribute {!r}"".format(type(self).__name__, name)\n        )\n\n    # This complicated two-method design boosts overall performance of simple operations\n    # - particularly DataArray methods that perform a _to_temp_dataset() round-trip - by\n    # a whopping 8% compared to a single method that checks hasattr(self, ""__dict__"") at\n    # runtime before every single assignment. All of this is just temporary until the\n    # FutureWarning can be changed into a hard crash.\n    def _setattr_dict(self, name: str, value: Any) -> None:\n        """"""Deprecated third party subclass (see ``__init_subclass__`` above)\n        """"""\n        object.__setattr__(self, name, value)\n        if name in self.__dict__:\n            # Custom, non-slotted attr, or improperly assigned variable?\n            warnings.warn(\n                ""Setting attribute %r on a %r object. Explicitly define __slots__ ""\n                ""to suppress this warning for legitimate custom attributes and ""\n                ""raise an error when attempting variables assignments.""\n                % (name, type(self).__name__),\n                FutureWarning,\n                stacklevel=2,\n            )\n\n    def __setattr__(self, name: str, value: Any) -> None:\n        """"""Objects with ``__slots__`` raise AttributeError if you try setting an\n        undeclared attribute. This is desirable, but the error message could use some\n        improvement.\n        """"""\n        try:\n            object.__setattr__(self, name, value)\n        except AttributeError as e:\n            # Don\'t accidentally shadow custom AttributeErrors, e.g.\n            # DataArray.dims.setter\n            if str(e) != ""{!r} object has no attribute {!r}"".format(\n                type(self).__name__, name\n            ):\n                raise\n            raise AttributeError(\n                ""cannot set attribute %r on a %r object. Use __setitem__ style""\n                ""assignment (e.g., `ds[\'name\'] = ...`) instead of assigning variables.""\n                % (name, type(self).__name__)\n            ) from e\n\n    def __dir__(self) -> List[str]:\n        """"""Provide method name lookup and completion. Only provide \'public\'\n        methods.\n        """"""\n        extra_attrs = [\n            item\n            for sublist in self._attr_sources\n            for item in sublist\n            if isinstance(item, str)\n        ]\n        return sorted(set(dir(type(self)) + extra_attrs))\n\n    def _ipython_key_completions_(self) -> List[str]:\n        """"""Provide method for the key-autocompletions in IPython.\n        See http://ipython.readthedocs.io/en/stable/config/integrating.html#tab-completion\n        For the details.\n        """"""\n        item_lists = [\n            item\n            for sublist in self._item_sources\n            for item in sublist\n            if isinstance(item, str)\n        ]\n        return list(set(item_lists))\n\n\ndef get_squeeze_dims(\n    xarray_obj,\n    dim: Union[Hashable, Iterable[Hashable], None] = None,\n    axis: Union[int, Iterable[int], None] = None,\n) -> List[Hashable]:\n    """"""Get a list of dimensions to squeeze out.\n    """"""\n    if dim is not None and axis is not None:\n        raise ValueError(""cannot use both parameters `axis` and `dim`"")\n    if dim is None and axis is None:\n        return [d for d, s in xarray_obj.sizes.items() if s == 1]\n\n    if isinstance(dim, Iterable) and not isinstance(dim, str):\n        dim = list(dim)\n    elif dim is not None:\n        dim = [dim]\n    else:\n        assert axis is not None\n        if isinstance(axis, int):\n            axis = [axis]\n        axis = list(axis)\n        if any(not isinstance(a, int) for a in axis):\n            raise TypeError(""parameter `axis` must be int or iterable of int."")\n        alldims = list(xarray_obj.sizes.keys())\n        dim = [alldims[a] for a in axis]\n\n    if any(xarray_obj.sizes[k] > 1 for k in dim):\n        raise ValueError(\n            ""cannot select a dimension to squeeze out ""\n            ""which has length greater than one""\n        )\n    return dim\n\n\nclass DataWithCoords(SupportsArithmetic, AttrAccessMixin):\n    """"""Shared base class for Dataset and DataArray.""""""\n\n    __slots__ = ()\n\n    _rolling_exp_cls = RollingExp\n\n    def squeeze(\n        self,\n        dim: Union[Hashable, Iterable[Hashable], None] = None,\n        drop: bool = False,\n        axis: Union[int, Iterable[int], None] = None,\n    ):\n        """"""Return a new object with squeezed data.\n\n        Parameters\n        ----------\n        dim : None or Hashable or iterable of Hashable, optional\n            Selects a subset of the length one dimensions. If a dimension is\n            selected with length greater than one, an error is raised. If\n            None, all length one dimensions are squeezed.\n        drop : bool, optional\n            If ``drop=True``, drop squeezed coordinates instead of making them\n            scalar.\n        axis : None or int or iterable of int, optional\n            Like dim, but positional.\n\n        Returns\n        -------\n        squeezed : same type as caller\n            This object, but with with all or a subset of the dimensions of\n            length 1 removed.\n\n        See Also\n        --------\n        numpy.squeeze\n        """"""\n        dims = get_squeeze_dims(self, dim, axis)\n        return self.isel(drop=drop, **{d: 0 for d in dims})\n\n    def get_index(self, key: Hashable) -> pd.Index:\n        """"""Get an index for a dimension, with fall-back to a default RangeIndex\n        """"""\n        if key not in self.dims:\n            raise KeyError(key)\n\n        try:\n            return self.indexes[key]\n        except KeyError:\n            # need to ensure dtype=int64 in case range is empty on Python 2\n            return pd.Index(range(self.sizes[key]), name=key, dtype=np.int64)\n\n    def _calc_assign_results(\n        self: C, kwargs: Mapping[Hashable, Union[T, Callable[[C], T]]]\n    ) -> Dict[Hashable, T]:\n        return {k: v(self) if callable(v) else v for k, v in kwargs.items()}\n\n    def assign_coords(self, coords=None, **coords_kwargs):\n        """"""Assign new coordinates to this object.\n\n        Returns a new object with all the original data in addition to the new\n        coordinates.\n\n        Parameters\n        ----------\n        coords : dict, optional\n            A dict where the keys are the names of the coordinates\n            with the new values to assign. If the values are callable, they are\n            computed on this object and assigned to new coordinate variables.\n            If the values are not callable, (e.g. a ``DataArray``, scalar, or\n            array), they are simply assigned. A new coordinate can also be\n            defined and attached to an existing dimension using a tuple with\n            the first element the dimension name and the second element the\n            values for this new coordinate.\n\n        **coords_kwargs : keyword, value pairs, optional\n            The keyword arguments form of ``coords``.\n            One of ``coords`` or ``coords_kwargs`` must be provided.\n\n        Returns\n        -------\n        assigned : same type as caller\n            A new object with the new coordinates in addition to the existing\n            data.\n\n        Examples\n        --------\n        Convert longitude coordinates from 0-359 to -180-179:\n\n        >>> da = xr.DataArray(\n        ...     np.random.rand(4), coords=[np.array([358, 359, 0, 1])], dims=""lon"",\n        ... )\n        >>> da\n        <xarray.DataArray (lon: 4)>\n        array([0.28298 , 0.667347, 0.657938, 0.177683])\n        Coordinates:\n          * lon      (lon) int64 358 359 0 1\n        >>> da.assign_coords(lon=(((da.lon + 180) % 360) - 180))\n        <xarray.DataArray (lon: 4)>\n        array([0.28298 , 0.667347, 0.657938, 0.177683])\n        Coordinates:\n          * lon      (lon) int64 -2 -1 0 1\n\n        The function also accepts dictionary arguments:\n\n        >>> da.assign_coords({""lon"": (((da.lon + 180) % 360) - 180)})\n        <xarray.DataArray (lon: 4)>\n        array([0.28298 , 0.667347, 0.657938, 0.177683])\n        Coordinates:\n          * lon      (lon) int64 -2 -1 0 1\n\n        New coordinate can also be attached to an existing dimension:\n\n        >>> lon_2 = np.array([300, 289, 0, 1])\n        >>> da.assign_coords(lon_2=(""lon"", lon_2))\n        <xarray.DataArray (lon: 4)>\n        array([0.28298 , 0.667347, 0.657938, 0.177683])\n        Coordinates:\n          * lon      (lon) int64 358 359 0 1\n            lon_2    (lon) int64 300 289 0 1\n\n        Note that the same result can also be obtained with a dict e.g.\n\n        >>> _ = da.assign_coords({""lon_2"": (""lon"", lon_2)})\n\n        Notes\n        -----\n        Since ``coords_kwargs`` is a dictionary, the order of your arguments\n        may not be preserved, and so the order of the new variables is not well\n        defined. Assigning multiple variables within the same ``assign_coords``\n        is possible, but you cannot reference other variables created within\n        the same ``assign_coords`` call.\n\n        See also\n        --------\n        Dataset.assign\n        Dataset.swap_dims\n        """"""\n        coords_kwargs = either_dict_or_kwargs(coords, coords_kwargs, ""assign_coords"")\n        data = self.copy(deep=False)\n        results = self._calc_assign_results(coords_kwargs)\n        data.coords.update(results)\n        return data\n\n    def assign_attrs(self, *args, **kwargs):\n        """"""Assign new attrs to this object.\n\n        Returns a new object equivalent to ``self.attrs.update(*args, **kwargs)``.\n\n        Parameters\n        ----------\n        args : positional arguments passed into ``attrs.update``.\n        kwargs : keyword arguments passed into ``attrs.update``.\n\n        Returns\n        -------\n        assigned : same type as caller\n            A new object with the new attrs in addition to the existing data.\n\n        See also\n        --------\n        Dataset.assign\n        """"""\n        out = self.copy(deep=False)\n        out.attrs.update(*args, **kwargs)\n        return out\n\n    def pipe(\n        self,\n        func: Union[Callable[..., T], Tuple[Callable[..., T], str]],\n        *args,\n        **kwargs,\n    ) -> T:\n        """"""\n        Apply ``func(self, *args, **kwargs)``\n\n        This method replicates the pandas method of the same name.\n\n        Parameters\n        ----------\n        func : function\n            function to apply to this xarray object (Dataset/DataArray).\n            ``args``, and ``kwargs`` are passed into ``func``.\n            Alternatively a ``(callable, data_keyword)`` tuple where\n            ``data_keyword`` is a string indicating the keyword of\n            ``callable`` that expects the xarray object.\n        args : positional arguments passed into ``func``.\n        kwargs : a dictionary of keyword arguments passed into ``func``.\n\n        Returns\n        -------\n        object : the return type of ``func``.\n\n        Notes\n        -----\n\n        Use ``.pipe`` when chaining together functions that expect\n        xarray or pandas objects, e.g., instead of writing\n\n        >>> f(g(h(ds), arg1=a), arg2=b, arg3=c)\n\n        You can write\n\n        >>> (ds.pipe(h).pipe(g, arg1=a).pipe(f, arg2=b, arg3=c))\n\n        If you have a function that takes the data as (say) the second\n        argument, pass a tuple indicating which keyword expects the\n        data. For example, suppose ``f`` takes its data as ``arg2``:\n\n        >>> (ds.pipe(h).pipe(g, arg1=a).pipe((f, ""arg2""), arg1=a, arg3=c))\n\n        Examples\n        --------\n\n        >>> import numpy as np\n        >>> import xarray as xr\n        >>> x = xr.Dataset(\n        ...     {\n        ...         ""temperature_c"": (\n        ...             (""lat"", ""lon""),\n        ...             20 * np.random.rand(4).reshape(2, 2),\n        ...         ),\n        ...         ""precipitation"": ((""lat"", ""lon""), np.random.rand(4).reshape(2, 2)),\n        ...     },\n        ...     coords={""lat"": [10, 20], ""lon"": [150, 160]},\n        ... )\n        >>> x\n        <xarray.Dataset>\n        Dimensions:        (lat: 2, lon: 2)\n        Coordinates:\n        * lat            (lat) int64 10 20\n        * lon            (lon) int64 150 160\n        Data variables:\n            temperature_c  (lat, lon) float64 14.53 11.85 19.27 16.37\n            precipitation  (lat, lon) float64 0.7315 0.7189 0.8481 0.4671\n\n        >>> def adder(data, arg):\n        ...     return data + arg\n        ...\n        >>> def div(data, arg):\n        ...     return data / arg\n        ...\n        >>> def sub_mult(data, sub_arg, mult_arg):\n        ...     return (data * mult_arg) - sub_arg\n        ...\n        >>> x.pipe(adder, 2)\n        <xarray.Dataset>\n        Dimensions:        (lat: 2, lon: 2)\n        Coordinates:\n        * lon            (lon) int64 150 160\n        * lat            (lat) int64 10 20\n        Data variables:\n            temperature_c  (lat, lon) float64 16.53 13.85 21.27 18.37\n            precipitation  (lat, lon) float64 2.731 2.719 2.848 2.467\n\n        >>> x.pipe(adder, arg=2)\n        <xarray.Dataset>\n        Dimensions:        (lat: 2, lon: 2)\n        Coordinates:\n        * lon            (lon) int64 150 160\n        * lat            (lat) int64 10 20\n        Data variables:\n            temperature_c  (lat, lon) float64 16.53 13.85 21.27 18.37\n            precipitation  (lat, lon) float64 2.731 2.719 2.848 2.467\n\n        >>> (\n        ...     x.pipe(adder, arg=2)\n        ...     .pipe(div, arg=2)\n        ...     .pipe(sub_mult, sub_arg=2, mult_arg=2)\n        ... )\n        <xarray.Dataset>\n        Dimensions:        (lat: 2, lon: 2)\n        Coordinates:\n        * lon            (lon) int64 150 160\n        * lat            (lat) int64 10 20\n        Data variables:\n            temperature_c  (lat, lon) float64 14.53 11.85 19.27 16.37\n            precipitation  (lat, lon) float64 0.7315 0.7189 0.8481 0.4671\n\n        See Also\n        --------\n        pandas.DataFrame.pipe\n        """"""\n        if isinstance(func, tuple):\n            func, target = func\n            if target in kwargs:\n                raise ValueError(\n                    ""%s is both the pipe target and a keyword "" ""argument"" % target\n                )\n            kwargs[target] = self\n            return func(*args, **kwargs)\n        else:\n            return func(self, *args, **kwargs)\n\n    def groupby(self, group, squeeze: bool = True, restore_coord_dims: bool = None):\n        """"""Returns a GroupBy object for performing grouped operations.\n\n        Parameters\n        ----------\n        group : str, DataArray or IndexVariable\n            Array whose unique values should be used to group this array. If a\n            string, must be the name of a variable contained in this dataset.\n        squeeze : boolean, optional\n            If ""group"" is a dimension of any arrays in this dataset, `squeeze`\n            controls whether the subarrays have a dimension of length 1 along\n            that dimension or if the dimension is squeezed out.\n        restore_coord_dims : bool, optional\n            If True, also restore the dimension order of multi-dimensional\n            coordinates.\n\n        Returns\n        -------\n        grouped : GroupBy\n            A `GroupBy` object patterned after `pandas.GroupBy` that can be\n            iterated over in the form of `(unique_value, grouped_array)` pairs.\n\n        Examples\n        --------\n        Calculate daily anomalies for daily data:\n\n        >>> da = xr.DataArray(\n        ...     np.linspace(0, 1826, num=1827),\n        ...     coords=[pd.date_range(""1/1/2000"", ""31/12/2004"", freq=""D"")],\n        ...     dims=""time"",\n        ... )\n        >>> da\n        <xarray.DataArray (time: 1827)>\n        array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03, 1.826e+03])\n        Coordinates:\n          * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n        >>> da.groupby(""time.dayofyear"") - da.groupby(""time.dayofyear"").mean(""time"")\n        <xarray.DataArray (time: 1827)>\n        array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])\n        Coordinates:\n          * time       (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n            dayofyear  (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ...\n\n        See Also\n        --------\n        core.groupby.DataArrayGroupBy\n        core.groupby.DatasetGroupBy\n        """"""\n        # While we don\'t generally check the type of every arg, passing\n        # multiple dimensions as multiple arguments is common enough, and the\n        # consequences hidden enough (strings evaluate as true) to warrant\n        # checking here.\n        # A future version could make squeeze kwarg only, but would face\n        # backward-compat issues.\n        if not isinstance(squeeze, bool):\n            raise TypeError(\n                f""`squeeze` must be True or False, but {squeeze} was supplied""\n            )\n\n        return self._groupby_cls(\n            self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims\n        )\n\n    def groupby_bins(\n        self,\n        group,\n        bins,\n        right: bool = True,\n        labels=None,\n        precision: int = 3,\n        include_lowest: bool = False,\n        squeeze: bool = True,\n        restore_coord_dims: bool = None,\n    ):\n        """"""Returns a GroupBy object for performing grouped operations.\n\n        Rather than using all unique values of `group`, the values are discretized\n        first by applying `pandas.cut` [1]_ to `group`.\n\n        Parameters\n        ----------\n        group : str, DataArray or IndexVariable\n            Array whose binned values should be used to group this array. If a\n            string, must be the name of a variable contained in this dataset.\n        bins : int or array of scalars\n            If bins is an int, it defines the number of equal-width bins in the\n            range of x. However, in this case, the range of x is extended by .1%\n            on each side to include the min or max values of x. If bins is a\n            sequence it defines the bin edges allowing for non-uniform bin\n            width. No extension of the range of x is done in this case.\n        right : boolean, optional\n            Indicates whether the bins include the rightmost edge or not. If\n            right == True (the default), then the bins [1,2,3,4] indicate\n            (1,2], (2,3], (3,4].\n        labels : array or boolean, default None\n            Used as labels for the resulting bins. Must be of the same length as\n            the resulting bins. If False, string bin labels are assigned by\n            `pandas.cut`.\n        precision : int\n            The precision at which to store and display the bins labels.\n        include_lowest : bool\n            Whether the first interval should be left-inclusive or not.\n        squeeze : boolean, optional\n            If ""group"" is a dimension of any arrays in this dataset, `squeeze`\n            controls whether the subarrays have a dimension of length 1 along\n            that dimension or if the dimension is squeezed out.\n        restore_coord_dims : bool, optional\n            If True, also restore the dimension order of multi-dimensional\n            coordinates.\n\n        Returns\n        -------\n        grouped : GroupBy\n            A `GroupBy` object patterned after `pandas.GroupBy` that can be\n            iterated over in the form of `(unique_value, grouped_array)` pairs.\n            The name of the group has the added suffix `_bins` in order to\n            distinguish it from the original variable.\n\n        References\n        ----------\n        .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n        """"""\n        return self._groupby_cls(\n            self,\n            group,\n            squeeze=squeeze,\n            bins=bins,\n            restore_coord_dims=restore_coord_dims,\n            cut_kwargs={\n                ""right"": right,\n                ""labels"": labels,\n                ""precision"": precision,\n                ""include_lowest"": include_lowest,\n            },\n        )\n\n    def weighted(self, weights):\n        """"""\n        Weighted operations.\n\n        Parameters\n        ----------\n        weights : DataArray\n            An array of weights associated with the values in this Dataset.\n            Each value in the data contributes to the reduction operation\n            according to its associated weight.\n\n        Notes\n        -----\n        ``weights`` must be a DataArray and cannot contain missing values.\n        Missing values can be replaced by ``weights.fillna(0)``.\n        """"""\n\n        return self._weighted_cls(self, weights)\n\n    def rolling(\n        self,\n        dim: Mapping[Hashable, int] = None,\n        min_periods: int = None,\n        center: bool = False,\n        keep_attrs: bool = None,\n        **window_kwargs: int,\n    ):\n        """"""\n        Rolling window object.\n\n        Parameters\n        ----------\n        dim: dict, optional\n            Mapping from the dimension name to create the rolling iterator\n            along (e.g. `time`) to its moving window size.\n        min_periods : int, default None\n            Minimum number of observations in window required to have a value\n            (otherwise result is NA). The default, None, is equivalent to\n            setting min_periods equal to the size of the window.\n        center : boolean, default False\n            Set the labels at the center of the window.\n        keep_attrs : bool, optional\n            If True, the object\'s attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n        **window_kwargs : optional\n            The keyword arguments form of ``dim``.\n            One of dim or window_kwargs must be provided.\n\n        Returns\n        -------\n        Rolling object (core.rolling.DataArrayRolling for DataArray,\n        core.rolling.DatasetRolling for Dataset.)\n\n        Examples\n        --------\n        Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:\n\n        >>> da = xr.DataArray(\n        ...     np.linspace(0, 11, num=12),\n        ...     coords=[\n        ...         pd.date_range(\n        ...             ""15/12/1999"", periods=12, freq=pd.DateOffset(months=1),\n        ...         )\n        ...     ],\n        ...     dims=""time"",\n        ... )\n        >>> da\n        <xarray.DataArray (time: 12)>\n        array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7., 8.,   9.,  10.,  11.])\n        Coordinates:\n          * time     (time) datetime64[ns] 1999-12-15 2000-01-15 2000-02-15 ...\n        >>> da.rolling(time=3, center=True).mean()\n        <xarray.DataArray (time: 12)>\n        array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 1999-12-15 2000-01-15 2000-02-15 ...\n\n        Remove the NaNs using ``dropna()``:\n\n        >>> da.rolling(time=3, center=True).mean().dropna(""time"")\n        <xarray.DataArray (time: 10)>\n        array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n        Coordinates:\n          * time     (time) datetime64[ns] 2000-01-15 2000-02-15 2000-03-15 ...\n\n        See Also\n        --------\n        core.rolling.DataArrayRolling\n        core.rolling.DatasetRolling\n        """"""\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n\n        dim = either_dict_or_kwargs(dim, window_kwargs, ""rolling"")\n        return self._rolling_cls(\n            self, dim, min_periods=min_periods, center=center, keep_attrs=keep_attrs\n        )\n\n    def rolling_exp(\n        self,\n        window: Mapping[Hashable, int] = None,\n        window_type: str = ""span"",\n        **window_kwargs,\n    ):\n        """"""\n        Exponentially-weighted moving window.\n        Similar to EWM in pandas\n\n        Requires the optional Numbagg dependency.\n\n        Parameters\n        ----------\n        window : A single mapping from a dimension name to window value,\n                 optional\n\n            dim : str\n                Name of the dimension to create the rolling exponential window\n                along (e.g., `time`).\n            window : int\n                Size of the moving window. The type of this is specified in\n                `window_type`\n        window_type : str, one of [\'span\', \'com\', \'halflife\', \'alpha\'],\n                      default \'span\'\n            The format of the previously supplied window. Each is a simple\n            numerical transformation of the others. Described in detail:\n            https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.ewm.html\n        **window_kwargs : optional\n            The keyword arguments form of ``window``.\n            One of window or window_kwargs must be provided.\n\n        See Also\n        --------\n        core.rolling_exp.RollingExp\n        """"""\n        window = either_dict_or_kwargs(window, window_kwargs, ""rolling_exp"")\n\n        return self._rolling_exp_cls(self, window, window_type)\n\n    def coarsen(\n        self,\n        dim: Mapping[Hashable, int] = None,\n        boundary: str = ""exact"",\n        side: Union[str, Mapping[Hashable, str]] = ""left"",\n        coord_func: str = ""mean"",\n        keep_attrs: bool = None,\n        **window_kwargs: int,\n    ):\n        """"""\n        Coarsen object.\n\n        Parameters\n        ----------\n        dim: dict, optional\n            Mapping from the dimension name to the window size.\n\n            dim : str\n                Name of the dimension to create the rolling iterator\n                along (e.g., `time`).\n            window : int\n                Size of the moving window.\n        boundary : \'exact\' | \'trim\' | \'pad\'\n            If \'exact\', a ValueError will be raised if dimension size is not a\n            multiple of the window size. If \'trim\', the excess entries are\n            dropped. If \'pad\', NA will be padded.\n        side : \'left\' or \'right\' or mapping from dimension to \'left\' or \'right\'\n        coord_func : function (name) that is applied to the coordinates,\n            or a mapping from coordinate name to function (name).\n        keep_attrs : bool, optional\n            If True, the object\'s attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n\n        Returns\n        -------\n        Coarsen object (core.rolling.DataArrayCoarsen for DataArray,\n        core.rolling.DatasetCoarsen for Dataset.)\n\n        Examples\n        --------\n        Coarsen the long time series by averaging over every four days.\n\n        >>> da = xr.DataArray(\n        ...     np.linspace(0, 364, num=364),\n        ...     dims=""time"",\n        ...     coords={""time"": pd.date_range(""15/12/1999"", periods=364)},\n        ... )\n        >>> da\n        <xarray.DataArray (time: 364)>\n        array([  0.      ,   1.002755,   2.00551 , ..., 361.99449 , 362.997245,\n               364.      ])\n        Coordinates:\n          * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12\n        >>>\n        >>> da.coarsen(time=3, boundary=""trim"").mean()\n        <xarray.DataArray (time: 121)>\n        array([  1.002755,   4.011019,   7.019284,  ...,  358.986226,\n               361.99449 ])\n        Coordinates:\n          * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10\n        >>>\n\n        See Also\n        --------\n        core.rolling.DataArrayCoarsen\n        core.rolling.DatasetCoarsen\n        """"""\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n\n        dim = either_dict_or_kwargs(dim, window_kwargs, ""coarsen"")\n        return self._coarsen_cls(\n            self,\n            dim,\n            boundary=boundary,\n            side=side,\n            coord_func=coord_func,\n            keep_attrs=keep_attrs,\n        )\n\n    def resample(\n        self,\n        indexer: Mapping[Hashable, str] = None,\n        skipna=None,\n        closed: str = None,\n        label: str = None,\n        base: int = 0,\n        keep_attrs: bool = None,\n        loffset=None,\n        restore_coord_dims: bool = None,\n        **indexer_kwargs: str,\n    ):\n        """"""Returns a Resample object for performing resampling operations.\n\n        Handles both downsampling and upsampling. The resampled\n        dimension must be a datetime-like coordinate. If any intervals\n        contain no values from the original object, they will be given\n        the value ``NaN``.\n\n        Parameters\n        ----------\n        indexer : {dim: freq}, optional\n            Mapping from the dimension name to resample frequency [1]_. The\n            dimension must be datetime-like.\n        skipna : bool, optional\n            Whether to skip missing values when aggregating in downsampling.\n        closed : \'left\' or \'right\', optional\n            Side of each interval to treat as closed.\n        label : \'left or \'right\', optional\n            Side of each interval to use for labeling.\n        base : int, optional\n            For frequencies that evenly subdivide 1 day, the ""origin"" of the\n            aggregated intervals. For example, for \'24H\' frequency, base could\n            range from 0 through 23.\n        loffset : timedelta or str, optional\n            Offset used to adjust the resampled time labels. Some pandas date\n            offset strings are supported.\n        keep_attrs : bool, optional\n            If True, the object\'s attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n        restore_coord_dims : bool, optional\n            If True, also restore the dimension order of multi-dimensional\n            coordinates.\n        **indexer_kwargs : {dim: freq}\n            The keyword arguments form of ``indexer``.\n            One of indexer or indexer_kwargs must be provided.\n\n        Returns\n        -------\n        resampled : same type as caller\n            This object resampled.\n\n        Examples\n        --------\n        Downsample monthly time-series data to seasonal data:\n\n        >>> da = xr.DataArray(\n        ...     np.linspace(0, 11, num=12),\n        ...     coords=[\n        ...         pd.date_range(\n        ...             ""15/12/1999"", periods=12, freq=pd.DateOffset(months=1),\n        ...         )\n        ...     ],\n        ...     dims=""time"",\n        ... )\n        >>> da\n        <xarray.DataArray (time: 12)>\n        array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7., 8.,   9.,  10.,  11.])\n        Coordinates:\n          * time     (time) datetime64[ns] 1999-12-15 2000-01-15 2000-02-15 ...\n        >>> da.resample(time=""QS-DEC"").mean()\n        <xarray.DataArray (time: 4)>\n        array([ 1.,  4.,  7., 10.])\n        Coordinates:\n          * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01\n\n        Upsample monthly time-series data to daily data:\n\n        >>> da.resample(time=""1D"").interpolate(""linear"")\n        <xarray.DataArray (time: 337)>\n        array([ 0.      ,  0.032258,  0.064516, ..., 10.935484, 10.967742, 11.      ])\n        Coordinates:\n          * time     (time) datetime64[ns] 1999-12-15 1999-12-16 1999-12-17 ...\n\n        Limit scope of upsampling method\n\n        >>> da.resample(time=""1D"").nearest(tolerance=""1D"")\n        <xarray.DataArray (time: 337)>\n        array([ 0.,  0., nan, ..., nan, 11., 11.])\n        Coordinates:\n          * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15\n\n        See Also\n        --------\n        pandas.Series.resample\n        pandas.DataFrame.resample\n\n        References\n        ----------\n\n        .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n        """"""\n        # TODO support non-string indexer after removing the old API.\n\n        from .dataarray import DataArray\n        from .resample import RESAMPLE_DIM\n        from ..coding.cftimeindex import CFTimeIndex\n\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n\n        # note: the second argument (now \'skipna\') use to be \'dim\'\n        if (\n            (skipna is not None and not isinstance(skipna, bool))\n            or (""how"" in indexer_kwargs and ""how"" not in self.dims)\n            or (""dim"" in indexer_kwargs and ""dim"" not in self.dims)\n        ):\n            raise TypeError(\n                ""resample() no longer supports the `how` or ""\n                ""`dim` arguments. Instead call methods on resample ""\n                ""objects, e.g., data.resample(time=\'1D\').mean()""\n            )\n\n        indexer = either_dict_or_kwargs(indexer, indexer_kwargs, ""resample"")\n        if len(indexer) != 1:\n            raise ValueError(""Resampling only supported along single dimensions."")\n        dim, freq = next(iter(indexer.items()))\n\n        dim_name = dim\n        dim_coord = self[dim]\n\n        if isinstance(self.indexes[dim_name], CFTimeIndex):\n            from .resample_cftime import CFTimeGrouper\n\n            grouper = CFTimeGrouper(freq, closed, label, base, loffset)\n        else:\n            grouper = pd.Grouper(\n                freq=freq, closed=closed, label=label, base=base, loffset=loffset\n            )\n        group = DataArray(\n            dim_coord, coords=dim_coord.coords, dims=dim_coord.dims, name=RESAMPLE_DIM\n        )\n        resampler = self._resample_cls(\n            self,\n            group=group,\n            dim=dim_name,\n            grouper=grouper,\n            resample_dim=RESAMPLE_DIM,\n            restore_coord_dims=restore_coord_dims,\n        )\n\n        return resampler\n\n    def where(self, cond, other=dtypes.NA, drop: bool = False):\n        """"""Filter elements from this object according to a condition.\n\n        This operation follows the normal broadcasting and alignment rules that\n        xarray uses for binary arithmetic.\n\n        Parameters\n        ----------\n        cond : DataArray or Dataset with boolean dtype\n            Locations at which to preserve this object\'s values.\n        other : scalar, DataArray or Dataset, optional\n            Value to use for locations in this object where ``cond`` is False.\n            By default, these locations filled with NA.\n        drop : boolean, optional\n            If True, coordinate labels that only correspond to False values of\n            the condition are dropped from the result. Mutually exclusive with\n            ``other``.\n\n        Returns\n        -------\n        Same xarray type as caller, with dtype float64.\n\n        Examples\n        --------\n\n        >>> import numpy as np\n        >>> a = xr.DataArray(np.arange(25).reshape(5, 5), dims=(""x"", ""y""))\n        >>> a\n        <xarray.DataArray (x: 5, y: 5)>\n        array([[ 0,  1,  2,  3,  4],\n            [ 5,  6,  7,  8,  9],\n            [10, 11, 12, 13, 14],\n            [15, 16, 17, 18, 19],\n            [20, 21, 22, 23, 24]])\n        Dimensions without coordinates: x, y\n\n        >>> a.where(a.x + a.y < 4)\n        <xarray.DataArray (x: 5, y: 5)>\n        array([[  0.,   1.,   2.,   3.,  nan],\n               [  5.,   6.,   7.,  nan,  nan],\n               [ 10.,  11.,  nan,  nan,  nan],\n               [ 15.,  nan,  nan,  nan,  nan],\n               [ nan,  nan,  nan,  nan,  nan]])\n        Dimensions without coordinates: x, y\n\n        >>> a.where(a.x + a.y < 5, -1)\n        <xarray.DataArray (x: 5, y: 5)>\n        array([[ 0,  1,  2,  3,  4],\n               [ 5,  6,  7,  8, -1],\n               [10, 11, 12, -1, -1],\n               [15, 16, -1, -1, -1],\n               [20, -1, -1, -1, -1]])\n        Dimensions without coordinates: x, y\n\n        >>> a.where(a.x + a.y < 4, drop=True)\n        <xarray.DataArray (x: 4, y: 4)>\n        array([[  0.,   1.,   2.,   3.],\n               [  5.,   6.,   7.,  nan],\n               [ 10.,  11.,  nan,  nan],\n               [ 15.,  nan,  nan,  nan]])\n        Dimensions without coordinates: x, y\n\n        >>> a.where(lambda x: x.x + x.y < 4, drop=True)\n        <xarray.DataArray (x: 4, y: 4)>\n        array([[  0.,   1.,   2.,   3.],\n               [  5.,   6.,   7.,  nan],\n               [ 10.,  11.,  nan,  nan],\n               [ 15.,  nan,  nan,  nan]])\n        Dimensions without coordinates: x, y\n\n        See also\n        --------\n        numpy.where : corresponding numpy function\n        where : equivalent function\n        """"""\n        from .alignment import align\n        from .dataarray import DataArray\n        from .dataset import Dataset\n\n        if callable(cond):\n            cond = cond(self)\n\n        if drop:\n            if other is not dtypes.NA:\n                raise ValueError(""cannot set `other` if drop=True"")\n\n            if not isinstance(cond, (Dataset, DataArray)):\n                raise TypeError(\n                    ""cond argument is %r but must be a %r or %r""\n                    % (cond, Dataset, DataArray)\n                )\n\n            # align so we can use integer indexing\n            self, cond = align(self, cond)\n\n            # get cond with the minimal size needed for the Dataset\n            if isinstance(cond, Dataset):\n                clipcond = cond.to_array().any(""variable"")\n            else:\n                clipcond = cond\n\n            # clip the data corresponding to coordinate dims that are not used\n            nonzeros = zip(clipcond.dims, np.nonzero(clipcond.values))\n            indexers = {k: np.unique(v) for k, v in nonzeros}\n\n            self = self.isel(**indexers)\n            cond = cond.isel(**indexers)\n\n        return ops.where_method(self, cond, other)\n\n    def close(self: Any) -> None:\n        """"""Close any files linked to this object\n        """"""\n        if self._file_obj is not None:\n            self._file_obj.close()\n        self._file_obj = None\n\n    def isin(self, test_elements):\n        """"""Tests each value in the array for whether it is in test elements.\n\n        Parameters\n        ----------\n        test_elements : array_like\n            The values against which to test each value of `element`.\n            This argument is flattened if an array or array_like.\n            See numpy notes for behavior with non-array-like parameters.\n\n        Returns\n        -------\n        isin : same as object, bool\n            Has the same shape as this object.\n\n        Examples\n        --------\n\n        >>> array = xr.DataArray([1, 2, 3], dims=""x"")\n        >>> array.isin([1, 3])\n        <xarray.DataArray (x: 3)>\n        array([ True, False,  True])\n        Dimensions without coordinates: x\n\n        See also\n        --------\n        numpy.isin\n        """"""\n        from .computation import apply_ufunc\n        from .dataset import Dataset\n        from .dataarray import DataArray\n        from .variable import Variable\n\n        if isinstance(test_elements, Dataset):\n            raise TypeError(\n                ""isin() argument must be convertible to an array: {}"".format(\n                    test_elements\n                )\n            )\n        elif isinstance(test_elements, (Variable, DataArray)):\n            # need to explicitly pull out data to support dask arrays as the\n            # second argument\n            test_elements = test_elements.data\n\n        return apply_ufunc(\n            duck_array_ops.isin,\n            self,\n            kwargs=dict(test_elements=test_elements),\n            dask=""allowed"",\n        )\n\n    def __enter__(self: T) -> T:\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback) -> None:\n        self.close()\n\n    def __getitem__(self, value):\n        # implementations of this class should implement this method\n        raise NotImplementedError()\n\n\ndef full_like(other, fill_value, dtype: DTypeLike = None):\n    """"""Return a new object with the same shape and type as a given object.\n\n    Parameters\n    ----------\n    other : DataArray, Dataset, or Variable\n        The reference object in input\n    fill_value : scalar\n        Value to fill the new object with before returning it.\n    dtype : dtype, optional\n        dtype of the new array. If omitted, it defaults to other.dtype.\n\n    Returns\n    -------\n    out : same as object\n        New object with the same shape and type as other, with the data\n        filled with fill_value. Coords will be copied from other.\n        If other is based on dask, the new one will be as well, and will be\n        split in the same chunks.\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> import xarray as xr\n    >>> x = xr.DataArray(\n    ...     np.arange(6).reshape(2, 3),\n    ...     dims=[""lat"", ""lon""],\n    ...     coords={""lat"": [1, 2], ""lon"": [0, 1, 2]},\n    ... )\n    >>> x\n    <xarray.DataArray (lat: 2, lon: 3)>\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    Coordinates:\n    * lat      (lat) int64 1 2\n    * lon      (lon) int64 0 1 2\n\n    >>> xr.full_like(x, 1)\n    <xarray.DataArray (lat: 2, lon: 3)>\n    array([[1, 1, 1],\n           [1, 1, 1]])\n    Coordinates:\n    * lat      (lat) int64 1 2\n    * lon      (lon) int64 0 1 2\n\n    >>> xr.full_like(x, 0.5)\n    <xarray.DataArray (lat: 2, lon: 3)>\n    array([[0, 0, 0],\n           [0, 0, 0]])\n    Coordinates:\n    * lat      (lat) int64 1 2\n    * lon      (lon) int64 0 1 2\n\n    >>> xr.full_like(x, 0.5, dtype=np.double)\n    <xarray.DataArray (lat: 2, lon: 3)>\n    array([[0.5, 0.5, 0.5],\n           [0.5, 0.5, 0.5]])\n    Coordinates:\n    * lat      (lat) int64 1 2\n    * lon      (lon) int64 0 1 2\n\n    >>> xr.full_like(x, np.nan, dtype=np.double)\n    <xarray.DataArray (lat: 2, lon: 3)>\n    array([[nan, nan, nan],\n           [nan, nan, nan]])\n    Coordinates:\n    * lat      (lat) int64 1 2\n    * lon      (lon) int64 0 1 2\n\n    See also\n    --------\n\n    zeros_like\n    ones_like\n\n    """"""\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    from .variable import Variable\n\n    if not is_scalar(fill_value):\n        raise ValueError(f""fill_value must be scalar. Received {fill_value} instead."")\n\n    if isinstance(other, Dataset):\n        data_vars = {\n            k: _full_like_variable(v, fill_value, dtype)\n            for k, v in other.data_vars.items()\n        }\n        return Dataset(data_vars, coords=other.coords, attrs=other.attrs)\n    elif isinstance(other, DataArray):\n        return DataArray(\n            _full_like_variable(other.variable, fill_value, dtype),\n            dims=other.dims,\n            coords=other.coords,\n            attrs=other.attrs,\n            name=other.name,\n        )\n    elif isinstance(other, Variable):\n        return _full_like_variable(other, fill_value, dtype)\n    else:\n        raise TypeError(""Expected DataArray, Dataset, or Variable"")\n\n\ndef _full_like_variable(other, fill_value, dtype: DTypeLike = None):\n    """"""Inner function of full_like, where other must be a variable\n    """"""\n    from .variable import Variable\n\n    if isinstance(other.data, dask_array_type):\n        import dask.array\n\n        if dtype is None:\n            dtype = other.dtype\n        data = dask.array.full(\n            other.shape, fill_value, dtype=dtype, chunks=other.data.chunks\n        )\n    else:\n        data = np.full_like(other, fill_value, dtype=dtype)\n\n    return Variable(dims=other.dims, data=data, attrs=other.attrs)\n\n\ndef zeros_like(other, dtype: DTypeLike = None):\n    """"""Return a new object of zeros with the same shape and\n    type as a given dataarray or dataset.\n\n    Parameters\n    ----------\n    other : DataArray, Dataset, or Variable\n        The reference object. The output will have the same dimensions and coordinates as this object.\n    dtype : dtype, optional\n        dtype of the new array. If omitted, it defaults to other.dtype.\n\n    Returns\n    -------\n    out : same as object\n        New object of zeros with the same shape and type as other.\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> import xarray as xr\n    >>> x = xr.DataArray(\n    ...     np.arange(6).reshape(2, 3),\n    ...     dims=[""lat"", ""lon""],\n    ...     coords={""lat"": [1, 2], ""lon"": [0, 1, 2]},\n    ... )\n    >>> x\n    <xarray.DataArray (lat: 2, lon: 3)>\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    Coordinates:\n    * lat      (lat) int64 1 2\n    * lon      (lon) int64 0 1 2\n\n    >>> xr.zeros_like(x)\n    <xarray.DataArray (lat: 2, lon: 3)>\n    array([[0, 0, 0],\n           [0, 0, 0]])\n    Coordinates:\n    * lat      (lat) int64 1 2\n    * lon      (lon) int64 0 1 2\n\n    >>> xr.zeros_like(x, dtype=np.float)\n    <xarray.DataArray (lat: 2, lon: 3)>\n    array([[0., 0., 0.],\n           [0., 0., 0.]])\n    Coordinates:\n    * lat      (lat) int64 1 2\n    * lon      (lon) int64 0 1 2\n\n    See also\n    --------\n\n    ones_like\n    full_like\n\n    """"""\n    return full_like(other, 0, dtype)\n\n\ndef ones_like(other, dtype: DTypeLike = None):\n    """"""Return a new object of ones with the same shape and\n    type as a given dataarray or dataset.\n\n    Parameters\n    ----------\n    other : DataArray, Dataset, or Variable\n        The reference object. The output will have the same dimensions and coordinates as this object.\n    dtype : dtype, optional\n        dtype of the new array. If omitted, it defaults to other.dtype.\n\n    Returns\n    -------\n    out : same as object\n        New object of ones with the same shape and type as other.\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> import xarray as xr\n    >>> x = xr.DataArray(\n    ...     np.arange(6).reshape(2, 3),\n    ...     dims=[""lat"", ""lon""],\n    ...     coords={""lat"": [1, 2], ""lon"": [0, 1, 2]},\n    ... )\n    >>> x\n    <xarray.DataArray (lat: 2, lon: 3)>\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    Coordinates:\n    * lat      (lat) int64 1 2\n    * lon      (lon) int64 0 1 2\n\n    >>> xr.ones_like(x)\n    <xarray.DataArray (lat: 2, lon: 3)>\n    array([[1, 1, 1],\n           [1, 1, 1]])\n    Coordinates:\n    * lat      (lat) int64 1 2\n    * lon      (lon) int64 0 1 2\n\n    See also\n    --------\n\n    zeros_like\n    full_like\n\n    """"""\n    return full_like(other, 1, dtype)\n\n\ndef is_np_datetime_like(dtype: DTypeLike) -> bool:\n    """"""Check if a dtype is a subclass of the numpy datetime types\n    """"""\n    return np.issubdtype(dtype, np.datetime64) or np.issubdtype(dtype, np.timedelta64)\n\n\ndef is_np_timedelta_like(dtype: DTypeLike) -> bool:\n    """"""Check whether dtype is of the timedelta64 dtype.\n    """"""\n    return np.issubdtype(dtype, np.timedelta64)\n\n\ndef _contains_cftime_datetimes(array) -> bool:\n    """"""Check if an array contains cftime.datetime objects\n    """"""\n    try:\n        from cftime import datetime as cftime_datetime\n    except ImportError:\n        return False\n    else:\n        if array.dtype == np.dtype(""O"") and array.size > 0:\n            sample = array.ravel()[0]\n            if isinstance(sample, dask_array_type):\n                sample = sample.compute()\n                if isinstance(sample, np.ndarray):\n                    sample = sample.item()\n            return isinstance(sample, cftime_datetime)\n        else:\n            return False\n\n\ndef contains_cftime_datetimes(var) -> bool:\n    """"""Check if an xarray.Variable contains cftime.datetime objects\n    """"""\n    return _contains_cftime_datetimes(var.data)\n\n\ndef _contains_datetime_like_objects(var) -> bool:\n    """"""Check if a variable contains datetime like objects (either\n    np.datetime64, np.timedelta64, or cftime.datetime)\n    """"""\n    return is_np_datetime_like(var.dtype) or contains_cftime_datetimes(var)\n'"
xarray/core/computation.py,27,"b'""""""\nFunctions for applying functions that act on arrays to xarray\'s labeled data.\n""""""\nimport functools\nimport itertools\nimport operator\nfrom collections import Counter\nfrom typing import (\n    TYPE_CHECKING,\n    AbstractSet,\n    Any,\n    Callable,\n    Dict,\n    Hashable,\n    Iterable,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Union,\n)\n\nimport numpy as np\n\nfrom . import dtypes, duck_array_ops, utils\nfrom .alignment import align, deep_align\nfrom .merge import merge_coordinates_without_align\nfrom .options import OPTIONS\nfrom .pycompat import dask_array_type\nfrom .utils import is_dict_like\nfrom .variable import Variable\n\nif TYPE_CHECKING:\n    from .coordinates import Coordinates  # noqa\n    from .dataset import Dataset\n\n_NO_FILL_VALUE = utils.ReprObject(""<no-fill-value>"")\n_DEFAULT_NAME = utils.ReprObject(""<default-name>"")\n_JOINS_WITHOUT_FILL_VALUES = frozenset({""inner"", ""exact""})\n\n\nclass _UFuncSignature:\n    """"""Core dimensions signature for a given function.\n\n    Based on the signature provided by generalized ufuncs in NumPy.\n\n    Attributes\n    ----------\n    input_core_dims : tuple[tuple]\n        Core dimension names on each input variable.\n    output_core_dims : tuple[tuple]\n        Core dimension names on each output variable.\n    """"""\n\n    __slots__ = (\n        ""input_core_dims"",\n        ""output_core_dims"",\n        ""_all_input_core_dims"",\n        ""_all_output_core_dims"",\n        ""_all_core_dims"",\n    )\n\n    def __init__(self, input_core_dims, output_core_dims=((),)):\n        self.input_core_dims = tuple(tuple(a) for a in input_core_dims)\n        self.output_core_dims = tuple(tuple(a) for a in output_core_dims)\n        self._all_input_core_dims = None\n        self._all_output_core_dims = None\n        self._all_core_dims = None\n\n    @property\n    def all_input_core_dims(self):\n        if self._all_input_core_dims is None:\n            self._all_input_core_dims = frozenset(\n                dim for dims in self.input_core_dims for dim in dims\n            )\n        return self._all_input_core_dims\n\n    @property\n    def all_output_core_dims(self):\n        if self._all_output_core_dims is None:\n            self._all_output_core_dims = frozenset(\n                dim for dims in self.output_core_dims for dim in dims\n            )\n        return self._all_output_core_dims\n\n    @property\n    def all_core_dims(self):\n        if self._all_core_dims is None:\n            self._all_core_dims = self.all_input_core_dims | self.all_output_core_dims\n        return self._all_core_dims\n\n    @property\n    def num_inputs(self):\n        return len(self.input_core_dims)\n\n    @property\n    def num_outputs(self):\n        return len(self.output_core_dims)\n\n    def __eq__(self, other):\n        try:\n            return (\n                self.input_core_dims == other.input_core_dims\n                and self.output_core_dims == other.output_core_dims\n            )\n        except AttributeError:\n            return False\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __repr__(self):\n        return ""{}({!r}, {!r})"".format(\n            type(self).__name__, list(self.input_core_dims), list(self.output_core_dims)\n        )\n\n    def __str__(self):\n        lhs = "","".join(""({})"".format("","".join(dims)) for dims in self.input_core_dims)\n        rhs = "","".join(""({})"".format("","".join(dims)) for dims in self.output_core_dims)\n        return f""{lhs}->{rhs}""\n\n    def to_gufunc_string(self):\n        """"""Create an equivalent signature string for a NumPy gufunc.\n\n        Unlike __str__, handles dimensions that don\'t map to Python\n        identifiers.\n        """"""\n        all_dims = self.all_core_dims\n        dims_map = dict(zip(sorted(all_dims), range(len(all_dims))))\n        input_core_dims = [\n            [""dim%d"" % dims_map[dim] for dim in core_dims]\n            for core_dims in self.input_core_dims\n        ]\n        output_core_dims = [\n            [""dim%d"" % dims_map[dim] for dim in core_dims]\n            for core_dims in self.output_core_dims\n        ]\n        alt_signature = type(self)(input_core_dims, output_core_dims)\n        return str(alt_signature)\n\n\ndef result_name(objects: list) -> Any:\n    # use the same naming heuristics as pandas:\n    # https://github.com/blaze/blaze/issues/458#issuecomment-51936356\n    names = {getattr(obj, ""name"", _DEFAULT_NAME) for obj in objects}\n    names.discard(_DEFAULT_NAME)\n    if len(names) == 1:\n        (name,) = names\n    else:\n        name = None\n    return name\n\n\ndef _get_coords_list(args) -> List[""Coordinates""]:\n    coords_list = []\n    for arg in args:\n        try:\n            coords = arg.coords\n        except AttributeError:\n            pass  # skip this argument\n        else:\n            coords_list.append(coords)\n    return coords_list\n\n\ndef build_output_coords(\n    args: list, signature: _UFuncSignature, exclude_dims: AbstractSet = frozenset()\n) -> ""List[Dict[Any, Variable]]"":\n    """"""Build output coordinates for an operation.\n\n    Parameters\n    ----------\n    args : list\n        List of raw operation arguments. Any valid types for xarray operations\n        are OK, e.g., scalars, Variable, DataArray, Dataset.\n    signature : _UfuncSignature\n        Core dimensions signature for the operation.\n    exclude_dims : optional set\n        Dimensions excluded from the operation. Coordinates along these\n        dimensions are dropped.\n\n    Returns\n    -------\n    Dictionary of Variable objects with merged coordinates.\n    """"""\n    coords_list = _get_coords_list(args)\n\n    if len(coords_list) == 1 and not exclude_dims:\n        # we can skip the expensive merge\n        (unpacked_coords,) = coords_list\n        merged_vars = dict(unpacked_coords.variables)\n    else:\n        # TODO: save these merged indexes, instead of re-computing them later\n        merged_vars, unused_indexes = merge_coordinates_without_align(\n            coords_list, exclude_dims=exclude_dims\n        )\n\n    output_coords = []\n    for output_dims in signature.output_core_dims:\n        dropped_dims = signature.all_input_core_dims - set(output_dims)\n        if dropped_dims:\n            filtered = {\n                k: v for k, v in merged_vars.items() if dropped_dims.isdisjoint(v.dims)\n            }\n        else:\n            filtered = merged_vars\n        output_coords.append(filtered)\n\n    return output_coords\n\n\ndef apply_dataarray_vfunc(\n    func, *args, signature, join=""inner"", exclude_dims=frozenset(), keep_attrs=False\n):\n    """"""Apply a variable level function over DataArray, Variable and/or ndarray\n    objects.\n    """"""\n    from .dataarray import DataArray\n\n    if len(args) > 1:\n        args = deep_align(\n            args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False\n        )\n\n    if keep_attrs and hasattr(args[0], ""name""):\n        name = args[0].name\n    else:\n        name = result_name(args)\n    result_coords = build_output_coords(args, signature, exclude_dims)\n\n    data_vars = [getattr(a, ""variable"", a) for a in args]\n    result_var = func(*data_vars)\n\n    if signature.num_outputs > 1:\n        out = tuple(\n            DataArray(variable, coords, name=name, fastpath=True)\n            for variable, coords in zip(result_var, result_coords)\n        )\n    else:\n        (coords,) = result_coords\n        out = DataArray(result_var, coords, name=name, fastpath=True)\n\n    return out\n\n\ndef ordered_set_union(all_keys: List[Iterable]) -> Iterable:\n    return {key: None for keys in all_keys for key in keys}.keys()\n\n\ndef ordered_set_intersection(all_keys: List[Iterable]) -> Iterable:\n    intersection = set(all_keys[0])\n    for keys in all_keys[1:]:\n        intersection.intersection_update(keys)\n    return [key for key in all_keys[0] if key in intersection]\n\n\ndef assert_and_return_exact_match(all_keys):\n    first_keys = all_keys[0]\n    for keys in all_keys[1:]:\n        if keys != first_keys:\n            raise ValueError(\n                ""exact match required for all data variable names, ""\n                ""but %r != %r"" % (keys, first_keys)\n            )\n    return first_keys\n\n\n_JOINERS = {\n    ""inner"": ordered_set_intersection,\n    ""outer"": ordered_set_union,\n    ""left"": operator.itemgetter(0),\n    ""right"": operator.itemgetter(-1),\n    ""exact"": assert_and_return_exact_match,\n}\n\n\ndef join_dict_keys(\n    objects: Iterable[Union[Mapping, Any]], how: str = ""inner""\n) -> Iterable:\n    joiner = _JOINERS[how]\n    all_keys = [obj.keys() for obj in objects if hasattr(obj, ""keys"")]\n    return joiner(all_keys)\n\n\ndef collect_dict_values(\n    objects: Iterable[Union[Mapping, Any]], keys: Iterable, fill_value: object = None\n) -> List[list]:\n    return [\n        [obj.get(key, fill_value) if is_dict_like(obj) else obj for obj in objects]\n        for key in keys\n    ]\n\n\ndef _as_variables_or_variable(arg):\n    try:\n        return arg.variables\n    except AttributeError:\n        try:\n            return arg.variable\n        except AttributeError:\n            return arg\n\n\ndef _unpack_dict_tuples(\n    result_vars: Mapping[Hashable, Tuple[Variable, ...]], num_outputs: int\n) -> Tuple[Dict[Hashable, Variable], ...]:\n    out: Tuple[Dict[Hashable, Variable], ...] = tuple({} for _ in range(num_outputs))\n    for name, values in result_vars.items():\n        for value, results_dict in zip(values, out):\n            results_dict[name] = value\n    return out\n\n\ndef apply_dict_of_variables_vfunc(\n    func, *args, signature, join=""inner"", fill_value=None\n):\n    """"""Apply a variable level function over dicts of DataArray, DataArray,\n    Variable and ndarray objects.\n    """"""\n    args = [_as_variables_or_variable(arg) for arg in args]\n    names = join_dict_keys(args, how=join)\n    grouped_by_name = collect_dict_values(args, names, fill_value)\n\n    result_vars = {}\n    for name, variable_args in zip(names, grouped_by_name):\n        result_vars[name] = func(*variable_args)\n\n    if signature.num_outputs > 1:\n        return _unpack_dict_tuples(result_vars, signature.num_outputs)\n    else:\n        return result_vars\n\n\ndef _fast_dataset(\n    variables: Dict[Hashable, Variable], coord_variables: Mapping[Hashable, Variable]\n) -> ""Dataset"":\n    """"""Create a dataset as quickly as possible.\n\n    Beware: the `variables` dict is modified INPLACE.\n    """"""\n    from .dataset import Dataset\n\n    variables.update(coord_variables)\n    coord_names = set(coord_variables)\n    return Dataset._construct_direct(variables, coord_names)\n\n\ndef apply_dataset_vfunc(\n    func,\n    *args,\n    signature,\n    join=""inner"",\n    dataset_join=""exact"",\n    fill_value=_NO_FILL_VALUE,\n    exclude_dims=frozenset(),\n    keep_attrs=False,\n):\n    """"""Apply a variable level function over Dataset, dict of DataArray,\n    DataArray, Variable and/or ndarray objects.\n    """"""\n    from .dataset import Dataset\n\n    first_obj = args[0]  # we\'ll copy attrs from this in case keep_attrs=True\n\n    if dataset_join not in _JOINS_WITHOUT_FILL_VALUES and fill_value is _NO_FILL_VALUE:\n        raise TypeError(\n            ""to apply an operation to datasets with different ""\n            ""data variables with apply_ufunc, you must supply the ""\n            ""dataset_fill_value argument.""\n        )\n\n    if len(args) > 1:\n        args = deep_align(\n            args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False\n        )\n\n    list_of_coords = build_output_coords(args, signature, exclude_dims)\n    args = [getattr(arg, ""data_vars"", arg) for arg in args]\n\n    result_vars = apply_dict_of_variables_vfunc(\n        func, *args, signature=signature, join=dataset_join, fill_value=fill_value\n    )\n\n    if signature.num_outputs > 1:\n        out = tuple(_fast_dataset(*args) for args in zip(result_vars, list_of_coords))\n    else:\n        (coord_vars,) = list_of_coords\n        out = _fast_dataset(result_vars, coord_vars)\n\n    if keep_attrs and isinstance(first_obj, Dataset):\n        if isinstance(out, tuple):\n            out = tuple(ds._copy_attrs_from(first_obj) for ds in out)\n        else:\n            out._copy_attrs_from(first_obj)\n    return out\n\n\ndef _iter_over_selections(obj, dim, values):\n    """"""Iterate over selections of an xarray object in the provided order.""""""\n    from .groupby import _dummy_copy\n\n    dummy = None\n    for value in values:\n        try:\n            obj_sel = obj.sel(**{dim: value})\n        except (KeyError, IndexError):\n            if dummy is None:\n                dummy = _dummy_copy(obj)\n            obj_sel = dummy\n        yield obj_sel\n\n\ndef apply_groupby_func(func, *args):\n    """"""Apply a dataset or datarray level function over GroupBy, Dataset,\n    DataArray, Variable and/or ndarray objects.\n    """"""\n    from .groupby import GroupBy, peek_at\n    from .variable import Variable\n\n    groupbys = [arg for arg in args if isinstance(arg, GroupBy)]\n    assert groupbys, ""must have at least one groupby to iterate over""\n    first_groupby = groupbys[0]\n    if any(not first_groupby._group.equals(gb._group) for gb in groupbys[1:]):\n        raise ValueError(\n            ""apply_ufunc can only perform operations over ""\n            ""multiple GroupBy objets at once if they are all ""\n            ""grouped the same way""\n        )\n\n    grouped_dim = first_groupby._group.name\n    unique_values = first_groupby._unique_coord.values\n\n    iterators = []\n    for arg in args:\n        if isinstance(arg, GroupBy):\n            iterator = (value for _, value in arg)\n        elif hasattr(arg, ""dims"") and grouped_dim in arg.dims:\n            if isinstance(arg, Variable):\n                raise ValueError(\n                    ""groupby operations cannot be performed with ""\n                    ""xarray.Variable objects that share a dimension with ""\n                    ""the grouped dimension""\n                )\n            iterator = _iter_over_selections(arg, grouped_dim, unique_values)\n        else:\n            iterator = itertools.repeat(arg)\n        iterators.append(iterator)\n\n    applied = (func(*zipped_args) for zipped_args in zip(*iterators))\n    applied_example, applied = peek_at(applied)\n    combine = first_groupby._combine\n    if isinstance(applied_example, tuple):\n        combined = tuple(combine(output) for output in zip(*applied))\n    else:\n        combined = combine(applied)\n    return combined\n\n\ndef unified_dim_sizes(\n    variables: Iterable[Variable], exclude_dims: AbstractSet = frozenset()\n) -> Dict[Hashable, int]:\n\n    dim_sizes: Dict[Hashable, int] = {}\n\n    for var in variables:\n        if len(set(var.dims)) < len(var.dims):\n            raise ValueError(\n                ""broadcasting cannot handle duplicate ""\n                ""dimensions on a variable: %r"" % list(var.dims)\n            )\n        for dim, size in zip(var.dims, var.shape):\n            if dim not in exclude_dims:\n                if dim not in dim_sizes:\n                    dim_sizes[dim] = size\n                elif dim_sizes[dim] != size:\n                    raise ValueError(\n                        ""operands cannot be broadcast together ""\n                        ""with mismatched lengths for dimension ""\n                        ""%r: %s vs %s"" % (dim, dim_sizes[dim], size)\n                    )\n    return dim_sizes\n\n\nSLICE_NONE = slice(None)\n\n\ndef broadcast_compat_data(\n    variable: Variable,\n    broadcast_dims: Tuple[Hashable, ...],\n    core_dims: Tuple[Hashable, ...],\n) -> Any:\n    data = variable.data\n\n    old_dims = variable.dims\n    new_dims = broadcast_dims + core_dims\n\n    if new_dims == old_dims:\n        # optimize for the typical case\n        return data\n\n    set_old_dims = set(old_dims)\n    missing_core_dims = [d for d in core_dims if d not in set_old_dims]\n    if missing_core_dims:\n        raise ValueError(\n            ""operand to apply_ufunc has required core dimensions {}, but ""\n            ""some of these dimensions are absent on an input variable: {}"".format(\n                list(core_dims), missing_core_dims\n            )\n        )\n\n    set_new_dims = set(new_dims)\n    unexpected_dims = [d for d in old_dims if d not in set_new_dims]\n    if unexpected_dims:\n        raise ValueError(\n            ""operand to apply_ufunc encountered unexpected ""\n            ""dimensions %r on an input variable: these are core ""\n            ""dimensions on other input or output variables"" % unexpected_dims\n        )\n\n    # for consistency with numpy, keep broadcast dimensions to the left\n    old_broadcast_dims = tuple(d for d in broadcast_dims if d in set_old_dims)\n    reordered_dims = old_broadcast_dims + core_dims\n    if reordered_dims != old_dims:\n        order = tuple(old_dims.index(d) for d in reordered_dims)\n        data = duck_array_ops.transpose(data, order)\n\n    if new_dims != reordered_dims:\n        key_parts = []\n        for dim in new_dims:\n            if dim in set_old_dims:\n                key_parts.append(SLICE_NONE)\n            elif key_parts:\n                # no need to insert new axes at the beginning that are already\n                # handled by broadcasting\n                key_parts.append(np.newaxis)\n        data = data[tuple(key_parts)]\n\n    return data\n\n\ndef apply_variable_ufunc(\n    func,\n    *args,\n    signature,\n    exclude_dims=frozenset(),\n    dask=""forbidden"",\n    output_dtypes=None,\n    output_sizes=None,\n    keep_attrs=False,\n    meta=None,\n):\n    """"""Apply a ndarray level function over Variable and/or ndarray objects.\n    """"""\n    from .variable import Variable, as_compatible_data\n\n    dim_sizes = unified_dim_sizes(\n        (a for a in args if hasattr(a, ""dims"")), exclude_dims=exclude_dims\n    )\n    broadcast_dims = tuple(\n        dim for dim in dim_sizes if dim not in signature.all_core_dims\n    )\n    output_dims = [broadcast_dims + out for out in signature.output_core_dims]\n\n    input_data = [\n        broadcast_compat_data(arg, broadcast_dims, core_dims)\n        if isinstance(arg, Variable)\n        else arg\n        for arg, core_dims in zip(args, signature.input_core_dims)\n    ]\n\n    if any(isinstance(array, dask_array_type) for array in input_data):\n        if dask == ""forbidden"":\n            raise ValueError(\n                ""apply_ufunc encountered a dask array on an ""\n                ""argument, but handling for dask arrays has not ""\n                ""been enabled. Either set the ``dask`` argument ""\n                ""or load your data into memory first with ""\n                ""``.load()`` or ``.compute()``""\n            )\n        elif dask == ""parallelized"":\n            input_dims = [broadcast_dims + dims for dims in signature.input_core_dims]\n            numpy_func = func\n\n            def func(*arrays):\n                return _apply_blockwise(\n                    numpy_func,\n                    arrays,\n                    input_dims,\n                    output_dims,\n                    signature,\n                    output_dtypes,\n                    output_sizes,\n                    meta,\n                )\n\n        elif dask == ""allowed"":\n            pass\n        else:\n            raise ValueError(\n                ""unknown setting for dask array handling in ""\n                ""apply_ufunc: {}"".format(dask)\n            )\n    result_data = func(*input_data)\n\n    if signature.num_outputs == 1:\n        result_data = (result_data,)\n    elif (\n        not isinstance(result_data, tuple) or len(result_data) != signature.num_outputs\n    ):\n        raise ValueError(\n            ""applied function does not have the number of ""\n            ""outputs specified in the ufunc signature. ""\n            ""Result is not a tuple of {} elements: {!r}"".format(\n                signature.num_outputs, result_data\n            )\n        )\n\n    output = []\n    for dims, data in zip(output_dims, result_data):\n        data = as_compatible_data(data)\n        if data.ndim != len(dims):\n            raise ValueError(\n                ""applied function returned data with unexpected ""\n                ""number of dimensions: {} vs {}, for dimensions {}"".format(\n                    data.ndim, len(dims), dims\n                )\n            )\n\n        var = Variable(dims, data, fastpath=True)\n        for dim, new_size in var.sizes.items():\n            if dim in dim_sizes and new_size != dim_sizes[dim]:\n                raise ValueError(\n                    ""size of dimension {!r} on inputs was unexpectedly ""\n                    ""changed by applied function from {} to {}. Only ""\n                    ""dimensions specified in ``exclude_dims`` with ""\n                    ""xarray.apply_ufunc are allowed to change size."".format(\n                        dim, dim_sizes[dim], new_size\n                    )\n                )\n\n        if keep_attrs and isinstance(args[0], Variable):\n            var.attrs.update(args[0].attrs)\n        output.append(var)\n\n    if signature.num_outputs == 1:\n        return output[0]\n    else:\n        return tuple(output)\n\n\ndef _apply_blockwise(\n    func,\n    args,\n    input_dims,\n    output_dims,\n    signature,\n    output_dtypes,\n    output_sizes=None,\n    meta=None,\n):\n    import dask.array\n\n    if signature.num_outputs > 1:\n        raise NotImplementedError(\n            ""multiple outputs from apply_ufunc not yet ""\n            ""supported with dask=\'parallelized\'""\n        )\n\n    if output_dtypes is None:\n        raise ValueError(\n            ""output dtypes (output_dtypes) must be supplied to ""\n            ""apply_func when using dask=\'parallelized\'""\n        )\n    if not isinstance(output_dtypes, list):\n        raise TypeError(\n            ""output_dtypes must be a list of objects coercible to ""\n            ""numpy dtypes, got {}"".format(output_dtypes)\n        )\n    if len(output_dtypes) != signature.num_outputs:\n        raise ValueError(\n            ""apply_ufunc arguments output_dtypes and ""\n            ""output_core_dims must have the same length: {} vs {}"".format(\n                len(output_dtypes), signature.num_outputs\n            )\n        )\n    (dtype,) = output_dtypes\n\n    if output_sizes is None:\n        output_sizes = {}\n\n    new_dims = signature.all_output_core_dims - signature.all_input_core_dims\n    if any(dim not in output_sizes for dim in new_dims):\n        raise ValueError(\n            ""when using dask=\'parallelized\' with apply_ufunc, ""\n            ""output core dimensions not found on inputs must ""\n            ""have explicitly set sizes with ``output_sizes``: {}"".format(new_dims)\n        )\n\n    for n, (data, core_dims) in enumerate(zip(args, signature.input_core_dims)):\n        if isinstance(data, dask_array_type):\n            # core dimensions cannot span multiple chunks\n            for axis, dim in enumerate(core_dims, start=-len(core_dims)):\n                if len(data.chunks[axis]) != 1:\n                    raise ValueError(\n                        ""dimension {!r} on {}th function argument to ""\n                        ""apply_ufunc with dask=\'parallelized\' consists of ""\n                        ""multiple chunks, but is also a core dimension. To ""\n                        ""fix, rechunk into a single dask array chunk along ""\n                        ""this dimension, i.e., ``.chunk({})``, but beware ""\n                        ""that this may significantly increase memory usage."".format(\n                            dim, n, {dim: -1}\n                        )\n                    )\n\n    (out_ind,) = output_dims\n\n    blockwise_args = []\n    for arg, dims in zip(args, input_dims):\n        # skip leading dimensions that are implicitly added by broadcasting\n        ndim = getattr(arg, ""ndim"", 0)\n        trimmed_dims = dims[-ndim:] if ndim else ()\n        blockwise_args.extend([arg, trimmed_dims])\n\n    return dask.array.blockwise(\n        func,\n        out_ind,\n        *blockwise_args,\n        dtype=dtype,\n        concatenate=True,\n        new_axes=output_sizes,\n        meta=meta,\n    )\n\n\ndef apply_array_ufunc(func, *args, dask=""forbidden""):\n    """"""Apply a ndarray level function over ndarray objects.""""""\n    if any(isinstance(arg, dask_array_type) for arg in args):\n        if dask == ""forbidden"":\n            raise ValueError(\n                ""apply_ufunc encountered a dask array on an ""\n                ""argument, but handling for dask arrays has not ""\n                ""been enabled. Either set the ``dask`` argument ""\n                ""or load your data into memory first with ""\n                ""``.load()`` or ``.compute()``""\n            )\n        elif dask == ""parallelized"":\n            raise ValueError(\n                ""cannot use dask=\'parallelized\' for apply_ufunc ""\n                ""unless at least one input is an xarray object""\n            )\n        elif dask == ""allowed"":\n            pass\n        else:\n            raise ValueError(f""unknown setting for dask array handling: {dask}"")\n    return func(*args)\n\n\ndef apply_ufunc(\n    func: Callable,\n    *args: Any,\n    input_core_dims: Sequence[Sequence] = None,\n    output_core_dims: Optional[Sequence[Sequence]] = ((),),\n    exclude_dims: AbstractSet = frozenset(),\n    vectorize: bool = False,\n    join: str = ""exact"",\n    dataset_join: str = ""exact"",\n    dataset_fill_value: object = _NO_FILL_VALUE,\n    keep_attrs: bool = False,\n    kwargs: Mapping = None,\n    dask: str = ""forbidden"",\n    output_dtypes: Sequence = None,\n    output_sizes: Mapping[Any, int] = None,\n    meta: Any = None,\n) -> Any:\n    """"""Apply a vectorized function for unlabeled arrays on xarray objects.\n\n    The function will be mapped over the data variable(s) of the input\n    arguments using xarray\'s standard rules for labeled computation, including\n    alignment, broadcasting, looping over GroupBy/Dataset variables, and\n    merging of coordinates.\n\n    Parameters\n    ----------\n    func : callable\n        Function to call like ``func(*args, **kwargs)`` on unlabeled arrays\n        (``.data``) that returns an array or tuple of arrays. If multiple\n        arguments with non-matching dimensions are supplied, this function is\n        expected to vectorize (broadcast) over axes of positional arguments in\n        the style of NumPy universal functions [1]_ (if this is not the case,\n        set ``vectorize=True``). If this function returns multiple outputs, you\n        must set ``output_core_dims`` as well.\n    *args : Dataset, DataArray, GroupBy, Variable, numpy/dask arrays or scalars\n        Mix of labeled and/or unlabeled arrays to which to apply the function.\n    input_core_dims : Sequence[Sequence], optional\n        List of the same length as ``args`` giving the list of core dimensions\n        on each input argument that should not be broadcast. By default, we\n        assume there are no core dimensions on any input arguments.\n\n        For example, ``input_core_dims=[[], [\'time\']]`` indicates that all\n        dimensions on the first argument and all dimensions other than \'time\'\n        on the second argument should be broadcast.\n\n        Core dimensions are automatically moved to the last axes of input\n        variables before applying ``func``, which facilitates using NumPy style\n        generalized ufuncs [2]_.\n    output_core_dims : List[tuple], optional\n        List of the same length as the number of output arguments from\n        ``func``, giving the list of core dimensions on each output that were\n        not broadcast on the inputs. By default, we assume that ``func``\n        outputs exactly one array, with axes corresponding to each broadcast\n        dimension.\n\n        Core dimensions are assumed to appear as the last dimensions of each\n        output in the provided order.\n    exclude_dims : set, optional\n        Core dimensions on the inputs to exclude from alignment and\n        broadcasting entirely. Any input coordinates along these dimensions\n        will be dropped. Each excluded dimension must also appear in\n        ``input_core_dims`` for at least one argument. Only dimensions listed\n        here are allowed to change size between input and output objects.\n    vectorize : bool, optional\n        If True, then assume ``func`` only takes arrays defined over core\n        dimensions as input and vectorize it automatically with\n        :py:func:`numpy.vectorize`. This option exists for convenience, but is\n        almost always slower than supplying a pre-vectorized function.\n        Using this option requires NumPy version 1.12 or newer.\n    join : {\'outer\', \'inner\', \'left\', \'right\', \'exact\'}, optional\n        Method for joining the indexes of the passed objects along each\n        dimension, and the variables of Dataset objects with mismatched\n        data variables:\n\n        - \'outer\': use the union of object indexes\n        - \'inner\': use the intersection of object indexes\n        - \'left\': use indexes from the first object with each dimension\n        - \'right\': use indexes from the last object with each dimension\n        - \'exact\': raise `ValueError` instead of aligning when indexes to be\n          aligned are not equal\n    dataset_join : {\'outer\', \'inner\', \'left\', \'right\', \'exact\'}, optional\n        Method for joining variables of Dataset objects with mismatched\n        data variables.\n\n        - \'outer\': take variables from both Dataset objects\n        - \'inner\': take only overlapped variables\n        - \'left\': take only variables from the first object\n        - \'right\': take only variables from the last object\n        - \'exact\': data variables on all Dataset objects must match exactly\n    dataset_fill_value : optional\n        Value used in place of missing variables on Dataset inputs when the\n        datasets do not share the exact same ``data_vars``. Required if\n        ``dataset_join not in {\'inner\', \'exact\'}``, otherwise ignored.\n    keep_attrs: boolean, Optional\n        Whether to copy attributes from the first argument to the output.\n    kwargs: dict, optional\n        Optional keyword arguments passed directly on to call ``func``.\n    dask: \'forbidden\', \'allowed\' or \'parallelized\', optional\n        How to handle applying to objects containing lazy data in the form of\n        dask arrays:\n\n        - \'forbidden\' (default): raise an error if a dask array is encountered.\n        - \'allowed\': pass dask arrays directly on to ``func``.\n        - \'parallelized\': automatically parallelize ``func`` if any of the\n          inputs are a dask array. If used, the ``output_dtypes`` argument must\n          also be provided. Multiple output arguments are not yet supported.\n    output_dtypes : list of dtypes, optional\n        Optional list of output dtypes. Only used if dask=\'parallelized\'.\n    output_sizes : dict, optional\n        Optional mapping from dimension names to sizes for outputs. Only used\n        if dask=\'parallelized\' and new dimensions (not found on inputs) appear\n        on outputs.\n    meta : optional\n        Size-0 object representing the type of array wrapped by dask array. Passed on to\n        ``dask.array.blockwise``.\n\n    Returns\n    -------\n    Single value or tuple of Dataset, DataArray, Variable, dask.array.Array or\n    numpy.ndarray, the first type on that list to appear on an input.\n\n    Examples\n    --------\n\n    Calculate the vector magnitude of two arguments:\n\n    >>> def magnitude(a, b):\n    ...     func = lambda x, y: np.sqrt(x ** 2 + y ** 2)\n    ...     return xr.apply_ufunc(func, a, b)\n\n    You can now apply ``magnitude()`` to ``xr.DataArray`` and ``xr.Dataset``\n    objects, with automatically preserved dimensions and coordinates, e.g.,\n\n    >>> array = xr.DataArray([1, 2, 3], coords=[(""x"", [0.1, 0.2, 0.3])])\n    >>> magnitude(array, -array)\n    <xarray.DataArray (x: 3)>\n    array([1.414214, 2.828427, 4.242641])\n    Coordinates:\n      * x        (x) float64 0.1 0.2 0.3\n\n    Plain scalars, numpy arrays and a mix of these with xarray objects is also\n    supported:\n\n    >>> magnitude(3, 4)\n    5.0\n    >>> magnitude(3, np.array([0, 4]))\n    array([3., 5.])\n    >>> magnitude(array, 0)\n    <xarray.DataArray (x: 3)>\n    array([1., 2., 3.])\n    Coordinates:\n      * x        (x) float64 0.1 0.2 0.3\n\n    Other examples of how you could use ``apply_ufunc`` to write functions to\n    (very nearly) replicate existing xarray functionality:\n\n    Compute the mean (``.mean``) over one dimension::\n\n        def mean(obj, dim):\n            # note: apply always moves core dimensions to the end\n            return apply_ufunc(np.mean, obj,\n                               input_core_dims=[[dim]],\n                               kwargs={\'axis\': -1})\n\n    Inner product over a specific dimension (like ``xr.dot``)::\n\n        def _inner(x, y):\n            result = np.matmul(x[..., np.newaxis, :], y[..., :, np.newaxis])\n            return result[..., 0, 0]\n\n        def inner_product(a, b, dim):\n            return apply_ufunc(_inner, a, b, input_core_dims=[[dim], [dim]])\n\n    Stack objects along a new dimension (like ``xr.concat``)::\n\n        def stack(objects, dim, new_coord):\n            # note: this version does not stack coordinates\n            func = lambda *x: np.stack(x, axis=-1)\n            result = apply_ufunc(func, *objects,\n                                 output_core_dims=[[dim]],\n                                 join=\'outer\',\n                                 dataset_fill_value=np.nan)\n            result[dim] = new_coord\n            return result\n\n    If your function is not vectorized but can be applied only to core\n    dimensions, you can use ``vectorize=True`` to turn into a vectorized\n    function. This wraps :py:func:`numpy.vectorize`, so the operation isn\'t\n    terribly fast. Here we\'ll use it to calculate the distance between\n    empirical samples from two probability distributions, using a scipy\n    function that needs to be applied to vectors::\n\n        import scipy.stats\n\n        def earth_mover_distance(first_samples,\n                                 second_samples,\n                                 dim=\'ensemble\'):\n            return apply_ufunc(scipy.stats.wasserstein_distance,\n                               first_samples, second_samples,\n                               input_core_dims=[[dim], [dim]],\n                               vectorize=True)\n\n    Most of NumPy\'s builtin functions already broadcast their inputs\n    appropriately for use in `apply`. You may find helper functions such as\n    numpy.broadcast_arrays helpful in writing your function. `apply_ufunc` also\n    works well with numba\'s vectorize and guvectorize. Further explanation with\n    examples are provided in the xarray documentation [3]_.\n\n    See also\n    --------\n    numpy.broadcast_arrays\n    numba.vectorize\n    numba.guvectorize\n\n    References\n    ----------\n    .. [1] http://docs.scipy.org/doc/numpy/reference/ufuncs.html\n    .. [2] http://docs.scipy.org/doc/numpy/reference/c-api.generalized-ufuncs.html\n    .. [3] http://xarray.pydata.org/en/stable/computation.html#wrapping-custom-computation\n    """"""\n    from .groupby import GroupBy\n    from .dataarray import DataArray\n    from .variable import Variable\n\n    if input_core_dims is None:\n        input_core_dims = ((),) * (len(args))\n    elif len(input_core_dims) != len(args):\n        raise ValueError(\n            ""input_core_dims must be None or a tuple with the length same to ""\n            ""the number of arguments. Given input_core_dims: {}, ""\n            ""number of args: {}."".format(input_core_dims, len(args))\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    signature = _UFuncSignature(input_core_dims, output_core_dims)\n\n    if exclude_dims and not exclude_dims <= signature.all_core_dims:\n        raise ValueError(\n            ""each dimension in `exclude_dims` must also be a ""\n            ""core dimension in the function signature""\n        )\n\n    if kwargs:\n        func = functools.partial(func, **kwargs)\n\n    if vectorize:\n        if meta is None:\n            # set meta=np.ndarray by default for numpy vectorized functions\n            # work around dask bug computing meta with vectorized functions: GH5642\n            meta = np.ndarray\n\n        if signature.all_core_dims:\n            func = np.vectorize(\n                func, otypes=output_dtypes, signature=signature.to_gufunc_string()\n            )\n        else:\n            func = np.vectorize(func, otypes=output_dtypes)\n\n    variables_vfunc = functools.partial(\n        apply_variable_ufunc,\n        func,\n        signature=signature,\n        exclude_dims=exclude_dims,\n        keep_attrs=keep_attrs,\n        dask=dask,\n        output_dtypes=output_dtypes,\n        output_sizes=output_sizes,\n        meta=meta,\n    )\n\n    if any(isinstance(a, GroupBy) for a in args):\n        this_apply = functools.partial(\n            apply_ufunc,\n            func,\n            input_core_dims=input_core_dims,\n            output_core_dims=output_core_dims,\n            exclude_dims=exclude_dims,\n            join=join,\n            dataset_join=dataset_join,\n            dataset_fill_value=dataset_fill_value,\n            keep_attrs=keep_attrs,\n            dask=dask,\n            meta=meta,\n        )\n        return apply_groupby_func(this_apply, *args)\n    elif any(is_dict_like(a) for a in args):\n        return apply_dataset_vfunc(\n            variables_vfunc,\n            *args,\n            signature=signature,\n            join=join,\n            exclude_dims=exclude_dims,\n            dataset_join=dataset_join,\n            fill_value=dataset_fill_value,\n            keep_attrs=keep_attrs,\n        )\n    elif any(isinstance(a, DataArray) for a in args):\n        return apply_dataarray_vfunc(\n            variables_vfunc,\n            *args,\n            signature=signature,\n            join=join,\n            exclude_dims=exclude_dims,\n            keep_attrs=keep_attrs,\n        )\n    elif any(isinstance(a, Variable) for a in args):\n        return variables_vfunc(*args)\n    else:\n        return apply_array_ufunc(func, *args, dask=dask)\n\n\ndef cov(da_a, da_b, dim=None, ddof=1):\n    """"""\n    Compute covariance between two DataArray objects along a shared dimension.\n\n    Parameters\n    ----------\n    da_a: DataArray object\n        Array to compute.\n    da_b: DataArray object\n        Array to compute.\n    dim : str, optional\n        The dimension along which the covariance will be computed\n    ddof: int, optional\n        If ddof=1, covariance is normalized by N-1, giving an unbiased estimate,\n        else normalization is by N.\n\n    Returns\n    -------\n    covariance: DataArray\n\n    See also\n    --------\n    pandas.Series.cov: corresponding pandas function\n    xr.corr: respective function to calculate correlation\n\n    Examples\n    --------\n    >>> da_a = DataArray(np.array([[1, 2, 3], [0.1, 0.2, 0.3], [3.2, 0.6, 1.8]]),\n    ...                  dims=(""space"", ""time""),\n    ...                  coords=[(\'space\', [\'IA\', \'IL\', \'IN\']),\n    ...                          (\'time\', pd.date_range(""2000-01-01"", freq=""1D"", periods=3))])\n    >>> da_a\n    <xarray.DataArray (space: 3, time: 3)>\n    array([[1. , 2. , 3. ],\n           [0.1, 0.2, 0.3],\n           [3.2, 0.6, 1.8]])\n    Coordinates:\n      * space    (space) <U2 \'IA\' \'IL\' \'IN\'\n      * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03\n    >>> da_b = DataArray(np.array([[0.2, 0.4, 0.6], [15, 10, 5], [3.2, 0.6, 1.8]]),\n    ...                  dims=(""space"", ""time""),\n    ...                  coords=[(\'space\', [\'IA\', \'IL\', \'IN\']),\n    ...                          (\'time\', pd.date_range(""2000-01-01"", freq=""1D"", periods=3))])\n    >>> da_b\n    <xarray.DataArray (space: 3, time: 3)>\n    array([[ 0.2,  0.4,  0.6],\n           [15. , 10. ,  5. ],\n           [ 3.2,  0.6,  1.8]])\n    Coordinates:\n      * space    (space) <U2 \'IA\' \'IL\' \'IN\'\n      * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03\n    >>> xr.cov(da_a, da_b)\n    <xarray.DataArray ()>\n    array(-3.53055556)\n    >>> xr.cov(da_a, da_b, dim=\'time\')\n    <xarray.DataArray (space: 3)>\n    array([ 0.2, -0.5,  1.69333333])\n    Coordinates:\n      * space    (space) <U2 \'IA\' \'IL\' \'IN\'\n    """"""\n    from .dataarray import DataArray\n\n    if any(not isinstance(arr, DataArray) for arr in [da_a, da_b]):\n        raise TypeError(\n            ""Only xr.DataArray is supported.""\n            ""Given {}."".format([type(arr) for arr in [da_a, da_b]])\n        )\n\n    return _cov_corr(da_a, da_b, dim=dim, ddof=ddof, method=""cov"")\n\n\ndef corr(da_a, da_b, dim=None):\n    """"""\n    Compute the Pearson correlation coefficient between\n    two DataArray objects along a shared dimension.\n\n    Parameters\n    ----------\n    da_a: DataArray object\n        Array to compute.\n    da_b: DataArray object\n        Array to compute.\n    dim: str, optional\n        The dimension along which the correlation will be computed\n\n    Returns\n    -------\n    correlation: DataArray\n\n    See also\n    --------\n    pandas.Series.corr: corresponding pandas function\n    xr.cov: underlying covariance function\n\n    Examples\n    --------\n    >>> da_a = DataArray(np.array([[1, 2, 3], [0.1, 0.2, 0.3], [3.2, 0.6, 1.8]]),\n    ...                  dims=(""space"", ""time""),\n    ...                  coords=[(\'space\', [\'IA\', \'IL\', \'IN\']),\n    ...                          (\'time\', pd.date_range(""2000-01-01"", freq=""1D"", periods=3))])\n    >>> da_a\n    <xarray.DataArray (space: 3, time: 3)>\n    array([[1. , 2. , 3. ],\n           [0.1, 0.2, 0.3],\n           [3.2, 0.6, 1.8]])\n    Coordinates:\n      * space    (space) <U2 \'IA\' \'IL\' \'IN\'\n      * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03\n    >>> da_b = DataArray(np.array([[0.2, 0.4, 0.6], [15, 10, 5], [3.2, 0.6, 1.8]]),\n    ...                  dims=(""space"", ""time""),\n    ...                  coords=[(\'space\', [\'IA\', \'IL\', \'IN\']),\n    ...                          (\'time\', pd.date_range(""2000-01-01"", freq=""1D"", periods=3))])\n    >>> da_b\n    <xarray.DataArray (space: 3, time: 3)>\n    array([[ 0.2,  0.4,  0.6],\n           [15. , 10. ,  5. ],\n           [ 3.2,  0.6,  1.8]])\n    Coordinates:\n      * space    (space) <U2 \'IA\' \'IL\' \'IN\'\n      * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03\n    >>> xr.corr(da_a, da_b)\n    <xarray.DataArray ()>\n    array(-0.57087777)\n    >>> xr.corr(da_a, da_b, dim=\'time\')\n    <xarray.DataArray (space: 3)>\n    array([ 1., -1.,  1.])\n    Coordinates:\n      * space    (space) <U2 \'IA\' \'IL\' \'IN\'\n    """"""\n    from .dataarray import DataArray\n\n    if any(not isinstance(arr, DataArray) for arr in [da_a, da_b]):\n        raise TypeError(\n            ""Only xr.DataArray is supported.""\n            ""Given {}."".format([type(arr) for arr in [da_a, da_b]])\n        )\n\n    return _cov_corr(da_a, da_b, dim=dim, method=""corr"")\n\n\ndef _cov_corr(da_a, da_b, dim=None, ddof=0, method=None):\n    """"""\n    Internal method for xr.cov() and xr.corr() so only have to\n    sanitize the input arrays once and we don\'t repeat code.\n    """"""\n    # 1. Broadcast the two arrays\n    da_a, da_b = align(da_a, da_b, join=""inner"", copy=False)\n\n    # 2. Ignore the nans\n    valid_values = da_a.notnull() & da_b.notnull()\n\n    if not valid_values.all():\n        da_a = da_a.where(valid_values)\n        da_b = da_b.where(valid_values)\n\n    valid_count = valid_values.sum(dim) - ddof\n\n    # 3. Detrend along the given dim\n    demeaned_da_a = da_a - da_a.mean(dim=dim)\n    demeaned_da_b = da_b - da_b.mean(dim=dim)\n\n    # 4. Compute covariance along the given dim\n    # N.B. `skipna=False` is required or there is a bug when computing\n    # auto-covariance. E.g. Try xr.cov(da,da) for\n    # da = xr.DataArray([[1, 2], [1, np.nan]], dims=[""x"", ""time""])\n    cov = (demeaned_da_a * demeaned_da_b).sum(dim=dim, skipna=False) / (valid_count)\n\n    if method == ""cov"":\n        return cov\n\n    else:\n        # compute std + corr\n        da_a_std = da_a.std(dim=dim)\n        da_b_std = da_b.std(dim=dim)\n        corr = cov / (da_a_std * da_b_std)\n        return corr\n\n\ndef dot(*arrays, dims=None, **kwargs):\n    """"""Generalized dot product for xarray objects. Like np.einsum, but\n    provides a simpler interface based on array dimensions.\n\n    Parameters\n    ----------\n    arrays: DataArray (or Variable) objects\n        Arrays to compute.\n    dims: \'...\', str or tuple of strings, optional\n        Which dimensions to sum over. Ellipsis (\'...\') sums over all dimensions.\n        If not specified, then all the common dimensions are summed over.\n    **kwargs: dict\n        Additional keyword arguments passed to numpy.einsum or\n        dask.array.einsum\n\n    Returns\n    -------\n    dot: DataArray\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> import xarray as xr\n    >>> da_a = xr.DataArray(np.arange(3 * 2).reshape(3, 2), dims=[""a"", ""b""])\n    >>> da_b = xr.DataArray(np.arange(3 * 2 * 2).reshape(3, 2, 2), dims=[""a"", ""b"", ""c""])\n    >>> da_c = xr.DataArray(np.arange(2 * 3).reshape(2, 3), dims=[""c"", ""d""])\n\n    >>> da_a\n    <xarray.DataArray (a: 3, b: 2)>\n    array([[0, 1],\n           [2, 3],\n           [4, 5]])\n    Dimensions without coordinates: a, b\n\n    >>> da_b\n    <xarray.DataArray (a: 3, b: 2, c: 2)>\n    array([[[ 0,  1],\n            [ 2,  3]],\n           [[ 4,  5],\n            [ 6,  7]],\n           [[ 8,  9],\n            [10, 11]]])\n    Dimensions without coordinates: a, b, c\n\n    >>> da_c\n    <xarray.DataArray (c: 2, d: 3)>\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    Dimensions without coordinates: c, d\n\n    >>> xr.dot(da_a, da_b, dims=[""a"", ""b""])\n    <xarray.DataArray (c: 2)>\n    array([110, 125])\n    Dimensions without coordinates: c\n\n    >>> xr.dot(da_a, da_b, dims=[""a""])\n    <xarray.DataArray (b: 2, c: 2)>\n    array([[40, 46],\n           [70, 79]])\n    Dimensions without coordinates: b, c\n\n    >>> xr.dot(da_a, da_b, da_c, dims=[""b"", ""c""])\n    <xarray.DataArray (a: 3, d: 3)>\n    array([[  9,  14,  19],\n           [ 93, 150, 207],\n           [273, 446, 619]])\n    Dimensions without coordinates: a, d\n\n    >>> xr.dot(da_a, da_b)\n    <xarray.DataArray (c: 2)>\n    array([110, 125])\n    Dimensions without coordinates: c\n\n    >>> xr.dot(da_a, da_b, dims=...)\n    <xarray.DataArray ()>\n    array(235)\n    """"""\n    from .dataarray import DataArray\n    from .variable import Variable\n\n    if any(not isinstance(arr, (Variable, DataArray)) for arr in arrays):\n        raise TypeError(\n            ""Only xr.DataArray and xr.Variable are supported.""\n            ""Given {}."".format([type(arr) for arr in arrays])\n        )\n\n    if len(arrays) == 0:\n        raise TypeError(""At least one array should be given."")\n\n    if isinstance(dims, str):\n        dims = (dims,)\n\n    common_dims = set.intersection(*[set(arr.dims) for arr in arrays])\n    all_dims = []\n    for arr in arrays:\n        all_dims += [d for d in arr.dims if d not in all_dims]\n\n    einsum_axes = ""abcdefghijklmnopqrstuvwxyz""\n    dim_map = {d: einsum_axes[i] for i, d in enumerate(all_dims)}\n\n    if dims is ...:\n        dims = all_dims\n    elif dims is None:\n        # find dimensions that occur more than one times\n        dim_counts = Counter()\n        for arr in arrays:\n            dim_counts.update(arr.dims)\n        dims = tuple(d for d, c in dim_counts.items() if c > 1)\n\n    dims = tuple(dims)  # make dims a tuple\n\n    # dimensions to be parallelized\n    broadcast_dims = tuple(d for d in all_dims if d in common_dims and d not in dims)\n    input_core_dims = [\n        [d for d in arr.dims if d not in broadcast_dims] for arr in arrays\n    ]\n    output_core_dims = [tuple(d for d in all_dims if d not in dims + broadcast_dims)]\n\n    # construct einsum subscripts, such as \'...abc,...ab->...c\'\n    # Note: input_core_dims are always moved to the last position\n    subscripts_list = [\n        ""..."" + """".join(dim_map[d] for d in ds) for ds in input_core_dims\n    ]\n    subscripts = "","".join(subscripts_list)\n    subscripts += ""->..."" + """".join(dim_map[d] for d in output_core_dims[0])\n\n    join = OPTIONS[""arithmetic_join""]\n    # using ""inner"" emulates `(a * b).sum()` for all joins (except ""exact"")\n    if join != ""exact"":\n        join = ""inner""\n\n    # subscripts should be passed to np.einsum as arg, not as kwargs. We need\n    # to construct a partial function for apply_ufunc to work.\n    func = functools.partial(duck_array_ops.einsum, subscripts, **kwargs)\n    result = apply_ufunc(\n        func,\n        *arrays,\n        input_core_dims=input_core_dims,\n        output_core_dims=output_core_dims,\n        join=join,\n        dask=""allowed"",\n    )\n    return result.transpose(*[d for d in all_dims if d in result.dims])\n\n\ndef where(cond, x, y):\n    """"""Return elements from `x` or `y` depending on `cond`.\n\n    Performs xarray-like broadcasting across input arguments.\n\n    Parameters\n    ----------\n    cond : scalar, array, Variable, DataArray or Dataset with boolean dtype\n        When True, return values from `x`, otherwise returns values from `y`.\n    x : scalar, array, Variable, DataArray or Dataset\n        values to choose from where `cond` is True\n    y : scalar, array, Variable, DataArray or Dataset\n        values to choose from where `cond` is False\n\n    All dimension coordinates on these objects must be aligned with each\n    other and with `cond`.\n\n    Returns\n    -------\n    In priority order: Dataset, DataArray, Variable or array, whichever\n    type appears as an input argument.\n\n    Examples\n    --------\n    >>> import xarray as xr\n    >>> import numpy as np\n    >>> x = xr.DataArray(\n    ...     0.1 * np.arange(10),\n    ...     dims=[""lat""],\n    ...     coords={""lat"": np.arange(10)},\n    ...     name=""sst"",\n    ... )\n    >>> x\n    <xarray.DataArray \'sst\' (lat: 10)>\n    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n    Coordinates:\n    * lat      (lat) int64 0 1 2 3 4 5 6 7 8 9\n\n    >>> xr.where(x < 0.5, x, x * 100)\n    <xarray.DataArray \'sst\' (lat: 10)>\n    array([ 0. ,  0.1,  0.2,  0.3,  0.4, 50. , 60. , 70. , 80. , 90. ])\n    Coordinates:\n    * lat      (lat) int64 0 1 2 3 4 5 6 7 8 9\n\n    >>> y = xr.DataArray(\n    ...     0.1 * np.arange(9).reshape(3, 3),\n    ...     dims=[""lat"", ""lon""],\n    ...     coords={""lat"": np.arange(3), ""lon"": 10 + np.arange(3)},\n    ...     name=""sst"",\n    ... )\n    >>> y\n    <xarray.DataArray \'sst\' (lat: 3, lon: 3)>\n    array([[0. , 0.1, 0.2],\n           [0.3, 0.4, 0.5],\n           [0.6, 0.7, 0.8]])\n    Coordinates:\n    * lat      (lat) int64 0 1 2\n    * lon      (lon) int64 10 11 12\n\n    >>> xr.where(y.lat < 1, y, -1)\n    <xarray.DataArray (lat: 3, lon: 3)>\n    array([[ 0. ,  0.1,  0.2],\n           [-1. , -1. , -1. ],\n           [-1. , -1. , -1. ]])\n    Coordinates:\n    * lat      (lat) int64 0 1 2\n    * lon      (lon) int64 10 11 12\n\n    >>> cond = xr.DataArray([True, False], dims=[""x""])\n    >>> x = xr.DataArray([1, 2], dims=[""y""])\n    >>> xr.where(cond, x, 0)\n    <xarray.DataArray (x: 2, y: 2)>\n    array([[1, 2],\n           [0, 0]])\n    Dimensions without coordinates: x, y\n\n    See also\n    --------\n    numpy.where : corresponding numpy function\n    Dataset.where, DataArray.where : equivalent methods\n    """"""\n    # alignment for three arguments is complicated, so don\'t support it yet\n    return apply_ufunc(\n        duck_array_ops.where,\n        cond,\n        x,\n        y,\n        join=""exact"",\n        dataset_join=""exact"",\n        dask=""allowed"",\n    )\n\n\ndef polyval(coord, coeffs, degree_dim=""degree""):\n    """"""Evaluate a polynomial at specific values\n\n    Parameters\n    ----------\n    coord : DataArray\n        The 1D coordinate along which to evaluate the polynomial.\n    coeffs : DataArray\n        Coefficients of the polynomials.\n    degree_dim : str, default ""degree""\n        Name of the polynomial degree dimension in `coeffs`.\n\n    See also\n    --------\n    xarray.DataArray.polyfit\n    numpy.polyval\n    """"""\n    from .dataarray import DataArray\n    from .missing import get_clean_interp_index\n\n    x = get_clean_interp_index(coord, coord.name, strict=False)\n\n    deg_coord = coeffs[degree_dim]\n\n    lhs = DataArray(\n        np.vander(x, int(deg_coord.max()) + 1),\n        dims=(coord.name, degree_dim),\n        coords={coord.name: coord, degree_dim: np.arange(deg_coord.max() + 1)[::-1]},\n    )\n    return (lhs * coeffs).sum(degree_dim)\n\n\ndef _calc_idxminmax(\n    *,\n    array,\n    func: Callable,\n    dim: Hashable = None,\n    skipna: bool = None,\n    fill_value: Any = dtypes.NA,\n    keep_attrs: bool = None,\n):\n    """"""Apply common operations for idxmin and idxmax.""""""\n    # This function doesn\'t make sense for scalars so don\'t try\n    if not array.ndim:\n        raise ValueError(""This function does not apply for scalars"")\n\n    if dim is not None:\n        pass  # Use the dim if available\n    elif array.ndim == 1:\n        # it is okay to guess the dim if there is only 1\n        dim = array.dims[0]\n    else:\n        # The dim is not specified and ambiguous.  Don\'t guess.\n        raise ValueError(""Must supply \'dim\' argument for multidimensional arrays"")\n\n    if dim not in array.dims:\n        raise KeyError(f\'Dimension ""{dim}"" not in dimension\')\n    if dim not in array.coords:\n        raise KeyError(f\'Dimension ""{dim}"" does not have coordinates\')\n\n    # These are dtypes with NaN values argmin and argmax can handle\n    na_dtypes = ""cfO""\n\n    if skipna or (skipna is None and array.dtype.kind in na_dtypes):\n        # Need to skip NaN values since argmin and argmax can\'t handle them\n        allna = array.isnull().all(dim)\n        array = array.where(~allna, 0)\n\n    # This will run argmin or argmax.\n    indx = func(array, dim=dim, axis=None, keep_attrs=keep_attrs, skipna=skipna)\n\n    # Handle dask arrays.\n    if isinstance(array.data, dask_array_type):\n        import dask.array\n\n        chunks = dict(zip(array.dims, array.chunks))\n        dask_coord = dask.array.from_array(array[dim].data, chunks=chunks[dim])\n        res = indx.copy(data=dask_coord[(indx.data,)])\n        # we need to attach back the dim name\n        res.name = dim\n    else:\n        res = array[dim][(indx,)]\n        # The dim is gone but we need to remove the corresponding coordinate.\n        del res.coords[dim]\n\n    if skipna or (skipna is None and array.dtype.kind in na_dtypes):\n        # Put the NaN values back in after removing them\n        res = res.where(~allna, fill_value)\n\n    # Copy attributes from argmin/argmax, if any\n    res.attrs = indx.attrs\n\n    return res\n'"
xarray/core/concat.py,0,"b'import pandas as pd\n\nfrom . import dtypes, utils\nfrom .alignment import align\nfrom .duck_array_ops import lazy_array_equiv\nfrom .merge import _VALID_COMPAT, merge_attrs, unique_variable\nfrom .variable import IndexVariable, Variable, as_variable\nfrom .variable import concat as concat_vars\n\n\ndef concat(\n    objs,\n    dim,\n    data_vars=""all"",\n    coords=""different"",\n    compat=""equals"",\n    positions=None,\n    fill_value=dtypes.NA,\n    join=""outer"",\n    combine_attrs=""override"",\n):\n    """"""Concatenate xarray objects along a new or existing dimension.\n\n    Parameters\n    ----------\n    objs : sequence of Dataset and DataArray objects\n        xarray objects to concatenate together. Each object is expected to\n        consist of variables and coordinates with matching shapes except for\n        along the concatenated dimension.\n    dim : str or DataArray or pandas.Index\n        Name of the dimension to concatenate along. This can either be a new\n        dimension name, in which case it is added along axis=0, or an existing\n        dimension name, in which case the location of the dimension is\n        unchanged. If dimension is provided as a DataArray or Index, its name\n        is used as the dimension to concatenate along and the values are added\n        as a coordinate.\n    data_vars : {\'minimal\', \'different\', \'all\' or list of str}, optional\n        These data variables will be concatenated together:\n          * \'minimal\': Only data variables in which the dimension already\n            appears are included.\n          * \'different\': Data variables which are not equal (ignoring\n            attributes) across all datasets are also concatenated (as well as\n            all for which dimension already appears). Beware: this option may\n            load the data payload of data variables into memory if they are not\n            already loaded.\n          * \'all\': All data variables will be concatenated.\n          * list of str: The listed data variables will be concatenated, in\n            addition to the \'minimal\' data variables.\n\n        If objects are DataArrays, data_vars must be \'all\'.\n    coords : {\'minimal\', \'different\', \'all\' or list of str}, optional\n        These coordinate variables will be concatenated together:\n          * \'minimal\': Only coordinates in which the dimension already appears\n            are included.\n          * \'different\': Coordinates which are not equal (ignoring attributes)\n            across all datasets are also concatenated (as well as all for which\n            dimension already appears). Beware: this option may load the data\n            payload of coordinate variables into memory if they are not already\n            loaded.\n          * \'all\': All coordinate variables will be concatenated, except\n            those corresponding to other dimensions.\n          * list of str: The listed coordinate variables will be concatenated,\n            in addition to the \'minimal\' coordinates.\n    compat : {\'identical\', \'equals\', \'broadcast_equals\', \'no_conflicts\', \'override\'}, optional\n        String indicating how to compare non-concatenated variables of the same name for\n        potential conflicts. This is passed down to merge.\n\n        - \'broadcast_equals\': all values must be equal when variables are\n          broadcast against each other to ensure common dimensions.\n        - \'equals\': all values and dimensions must be the same.\n        - \'identical\': all values, dimensions and attributes must be the\n          same.\n        - \'no_conflicts\': only values which are not null in both datasets\n          must be equal. The returned dataset then contains the combination\n          of all non-null values.\n        - \'override\': skip comparing and pick variable from first dataset\n    positions : None or list of integer arrays, optional\n        List of integer arrays which specifies the integer positions to which\n        to assign each dataset along the concatenated dimension. If not\n        supplied, objects are concatenated in the provided order.\n    fill_value : scalar, optional\n        Value to use for newly missing values\n    join : {\'outer\', \'inner\', \'left\', \'right\', \'exact\'}, optional\n        String indicating how to combine differing indexes\n        (excluding dim) in objects\n\n        - \'outer\': use the union of object indexes\n        - \'inner\': use the intersection of object indexes\n        - \'left\': use indexes from the first object with each dimension\n        - \'right\': use indexes from the last object with each dimension\n        - \'exact\': instead of aligning, raise `ValueError` when indexes to be\n          aligned are not equal\n        - \'override\': if indexes are of same size, rewrite indexes to be\n          those of the first object with that dimension. Indexes for the same\n          dimension must have the same size in all objects.\n    combine_attrs : {\'drop\', \'identical\', \'no_conflicts\', \'override\'},\n                    default \'override\n        String indicating how to combine attrs of the objects being merged:\n\n        - \'drop\': empty attrs on returned Dataset.\n        - \'identical\': all attrs must be the same on every object.\n        - \'no_conflicts\': attrs from all objects are combined, any that have\n          the same name must also have the same value.\n        - \'override\': skip comparing and copy attrs from the first dataset to\n          the result.\n\n    Returns\n    -------\n    concatenated : type of objs\n\n    See also\n    --------\n    merge\n    auto_combine\n    """"""\n    # TODO: add ignore_index arguments copied from pandas.concat\n    # TODO: support concatenating scalar coordinates even if the concatenated\n    # dimension already exists\n    from .dataset import Dataset\n    from .dataarray import DataArray\n\n    try:\n        first_obj, objs = utils.peek_at(objs)\n    except StopIteration:\n        raise ValueError(""must supply at least one object to concatenate"")\n\n    if compat not in _VALID_COMPAT:\n        raise ValueError(\n            ""compat=%r invalid: must be \'broadcast_equals\', \'equals\', \'identical\', \'no_conflicts\' or \'override\'""\n            % compat\n        )\n\n    if isinstance(first_obj, DataArray):\n        f = _dataarray_concat\n    elif isinstance(first_obj, Dataset):\n        f = _dataset_concat\n    else:\n        raise TypeError(\n            ""can only concatenate xarray Dataset and DataArray ""\n            ""objects, got %s"" % type(first_obj)\n        )\n    return f(\n        objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs\n    )\n\n\ndef _calc_concat_dim_coord(dim):\n    """"""\n    Infer the dimension name and 1d coordinate variable (if appropriate)\n    for concatenating along the new dimension.\n    """"""\n    from .dataarray import DataArray\n\n    if isinstance(dim, str):\n        coord = None\n    elif not isinstance(dim, (DataArray, Variable)):\n        dim_name = getattr(dim, ""name"", None)\n        if dim_name is None:\n            dim_name = ""concat_dim""\n        coord = IndexVariable(dim_name, dim)\n        dim = dim_name\n    elif not isinstance(dim, DataArray):\n        coord = as_variable(dim).to_index_variable()\n        (dim,) = coord.dims\n    else:\n        coord = dim\n        (dim,) = coord.dims\n    return dim, coord\n\n\ndef _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n    """"""\n    Determine which dataset variables need to be concatenated in the result,\n    """"""\n    # Return values\n    concat_over = set()\n    equals = {}\n\n    if dim in dim_names:\n        concat_over_existing_dim = True\n        concat_over.add(dim)\n    else:\n        concat_over_existing_dim = False\n\n    concat_dim_lengths = []\n    for ds in datasets:\n        if concat_over_existing_dim:\n            if dim not in ds.dims:\n                if dim in ds:\n                    ds = ds.set_coords(dim)\n        concat_over.update(k for k, v in ds.variables.items() if dim in v.dims)\n        concat_dim_lengths.append(ds.dims.get(dim, 1))\n\n    def process_subset_opt(opt, subset):\n        if isinstance(opt, str):\n            if opt == ""different"":\n                if compat == ""override"":\n                    raise ValueError(\n                        ""Cannot specify both %s=\'different\' and compat=\'override\'.""\n                        % subset\n                    )\n                # all nonindexes that are not the same in each dataset\n                for k in getattr(datasets[0], subset):\n                    if k not in concat_over:\n                        equals[k] = None\n\n                        variables = []\n                        for ds in datasets:\n                            if k in ds.variables:\n                                variables.append(ds.variables[k])\n\n                        if len(variables) == 1:\n                            # coords=""different"" doesn\'t make sense when only one object\n                            # contains a particular variable.\n                            break\n                        elif len(variables) != len(datasets) and opt == ""different"":\n                            raise ValueError(\n                                f""{k!r} not present in all datasets and coords=\'different\'. ""\n                                f""Either add {k!r} to datasets where it is missing or ""\n                                ""specify coords=\'minimal\'.""\n                            )\n\n                        # first check without comparing values i.e. no computes\n                        for var in variables[1:]:\n                            equals[k] = getattr(variables[0], compat)(\n                                var, equiv=lazy_array_equiv\n                            )\n                            if equals[k] is not True:\n                                # exit early if we know these are not equal or that\n                                # equality cannot be determined i.e. one or all of\n                                # the variables wraps a numpy array\n                                break\n\n                        if equals[k] is False:\n                            concat_over.add(k)\n\n                        elif equals[k] is None:\n                            # Compare the variable of all datasets vs. the one\n                            # of the first dataset. Perform the minimum amount of\n                            # loads in order to avoid multiple loads from disk\n                            # while keeping the RAM footprint low.\n                            v_lhs = datasets[0].variables[k].load()\n                            # We\'ll need to know later on if variables are equal.\n                            computed = []\n                            for ds_rhs in datasets[1:]:\n                                v_rhs = ds_rhs.variables[k].compute()\n                                computed.append(v_rhs)\n                                if not getattr(v_lhs, compat)(v_rhs):\n                                    concat_over.add(k)\n                                    equals[k] = False\n                                    # computed variables are not to be re-computed\n                                    # again in the future\n                                    for ds, v in zip(datasets[1:], computed):\n                                        ds.variables[k].data = v.data\n                                    break\n                            else:\n                                equals[k] = True\n\n            elif opt == ""all"":\n                concat_over.update(\n                    set(getattr(datasets[0], subset)) - set(datasets[0].dims)\n                )\n            elif opt == ""minimal"":\n                pass\n            else:\n                raise ValueError(f""unexpected value for {subset}: {opt}"")\n        else:\n            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n            if invalid_vars:\n                if subset == ""coords"":\n                    raise ValueError(\n                        ""some variables in coords are not coordinates on ""\n                        ""the first dataset: %s"" % (invalid_vars,)\n                    )\n                else:\n                    raise ValueError(\n                        ""some variables in data_vars are not data variables ""\n                        ""on the first dataset: %s"" % (invalid_vars,)\n                    )\n            concat_over.update(opt)\n\n    process_subset_opt(data_vars, ""data_vars"")\n    process_subset_opt(coords, ""coords"")\n    return concat_over, equals, concat_dim_lengths\n\n\n# determine dimensional coordinate names and a dict mapping name to DataArray\ndef _parse_datasets(datasets):\n\n    dims = set()\n    all_coord_names = set()\n    data_vars = set()  # list of data_vars\n    dim_coords = {}  # maps dim name to variable\n    dims_sizes = {}  # shared dimension sizes to expand variables\n\n    for ds in datasets:\n        dims_sizes.update(ds.dims)\n        all_coord_names.update(ds.coords)\n        data_vars.update(ds.data_vars)\n\n        for dim in set(ds.dims) - dims:\n            if dim not in dim_coords:\n                dim_coords[dim] = ds.coords[dim].variable\n        dims = dims | set(ds.dims)\n\n    return dim_coords, dims_sizes, all_coord_names, data_vars\n\n\ndef _dataset_concat(\n    datasets,\n    dim,\n    data_vars,\n    coords,\n    compat,\n    positions,\n    fill_value=dtypes.NA,\n    join=""outer"",\n    combine_attrs=""override"",\n):\n    """"""\n    Concatenate a sequence of datasets along a new or existing dimension\n    """"""\n    from .dataset import Dataset\n\n    dim, coord = _calc_concat_dim_coord(dim)\n    # Make sure we\'re working on a copy (we\'ll be loading variables)\n    datasets = [ds.copy() for ds in datasets]\n    datasets = align(\n        *datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value\n    )\n\n    dim_coords, dims_sizes, coord_names, data_names = _parse_datasets(datasets)\n    dim_names = set(dim_coords)\n    unlabeled_dims = dim_names - coord_names\n\n    both_data_and_coords = coord_names & data_names\n    if both_data_and_coords:\n        raise ValueError(\n            ""%r is a coordinate in some datasets but not others."" % both_data_and_coords\n        )\n    # we don\'t want the concat dimension in the result dataset yet\n    dim_coords.pop(dim, None)\n    dims_sizes.pop(dim, None)\n\n    # case where concat dimension is a coordinate or data_var but not a dimension\n    if (dim in coord_names or dim in data_names) and dim not in dim_names:\n        datasets = [ds.expand_dims(dim) for ds in datasets]\n\n    # determine which variables to concatentate\n    concat_over, equals, concat_dim_lengths = _calc_concat_over(\n        datasets, dim, dim_names, data_vars, coords, compat\n    )\n\n    # determine which variables to merge, and then merge them according to compat\n    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n\n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            for var in variables_to_merge:\n                if var in ds:\n                    to_merge[var].append(ds.variables[var])\n\n        for var in variables_to_merge:\n            result_vars[var] = unique_variable(\n                var, to_merge[var], compat=compat, equals=equals.get(var, None)\n            )\n    else:\n        result_vars = {}\n    result_vars.update(dim_coords)\n\n    # assign attrs and encoding from first dataset\n    result_attrs = merge_attrs([ds.attrs for ds in datasets], combine_attrs)\n    result_encoding = datasets[0].encoding\n\n    # check that global attributes are fixed across all datasets if necessary\n    for ds in datasets[1:]:\n        if compat == ""identical"" and not utils.dict_equiv(ds.attrs, result_attrs):\n            raise ValueError(""Dataset global attributes not equal."")\n\n    # we\'ve already verified everything is consistent; now, calculate\n    # shared dimension sizes so we can expand the necessary variables\n    def ensure_common_dims(vars):\n        # ensure each variable with the given name shares the same\n        # dimensions and the same shape for all of them except along the\n        # concat dimension\n        common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n        if dim not in common_dims:\n            common_dims = (dim,) + common_dims\n        for var, dim_len in zip(vars, concat_dim_lengths):\n            if var.dims != common_dims:\n                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n                var = var.set_dims(common_dims, common_shape)\n            yield var\n\n    # stack up each variable to fill-out the dataset (in order)\n    # n.b. this loop preserves variable order, needed for groupby.\n    for k in datasets[0].variables:\n        if k in concat_over:\n            try:\n                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n            except KeyError:\n                raise ValueError(""%r is not present in all datasets."" % k)\n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n\n    result = Dataset(result_vars, attrs=result_attrs)\n    absent_coord_names = coord_names - set(result.variables)\n    if absent_coord_names:\n        raise ValueError(\n            ""Variables %r are coordinates in some datasets but not others.""\n            % absent_coord_names\n        )\n    result = result.set_coords(coord_names)\n    result.encoding = result_encoding\n\n    result = result.drop_vars(unlabeled_dims, errors=""ignore"")\n\n    if coord is not None:\n        # add concat dimension last to ensure that its in the final Dataset\n        result[coord.name] = coord\n\n    return result\n\n\ndef _dataarray_concat(\n    arrays,\n    dim,\n    data_vars,\n    coords,\n    compat,\n    positions,\n    fill_value=dtypes.NA,\n    join=""outer"",\n    combine_attrs=""override"",\n):\n    arrays = list(arrays)\n\n    if data_vars != ""all"":\n        raise ValueError(\n            ""data_vars is not a valid argument when concatenating DataArray objects""\n        )\n\n    datasets = []\n    for n, arr in enumerate(arrays):\n        if n == 0:\n            name = arr.name\n        elif name != arr.name:\n            if compat == ""identical"":\n                raise ValueError(""array names not identical"")\n            else:\n                arr = arr.rename(name)\n        datasets.append(arr._to_temp_dataset())\n\n    ds = _dataset_concat(\n        datasets,\n        dim,\n        data_vars,\n        coords,\n        compat,\n        positions,\n        fill_value=fill_value,\n        join=join,\n        combine_attrs=""drop"",\n    )\n\n    merged_attrs = merge_attrs([da.attrs for da in arrays], combine_attrs)\n\n    result = arrays[0]._from_temp_dataset(ds, name)\n    result.attrs = merged_attrs\n\n    return result\n'"
xarray/core/coordinates.py,1,"b'from contextlib import contextmanager\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Hashable,\n    Iterator,\n    Mapping,\n    Sequence,\n    Set,\n    Tuple,\n    Union,\n    cast,\n)\n\nimport pandas as pd\n\nfrom . import formatting, indexing\nfrom .indexes import Indexes\nfrom .merge import merge_coordinates_without_align, merge_coords\nfrom .utils import Frozen, ReprObject, either_dict_or_kwargs\nfrom .variable import Variable\n\nif TYPE_CHECKING:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n# Used as the key corresponding to a DataArray\'s variable when converting\n# arbitrary DataArray objects to datasets\n_THIS_ARRAY = ReprObject(""<this-array>"")\n\n\nclass Coordinates(Mapping[Hashable, ""DataArray""]):\n    __slots__ = ()\n\n    def __getitem__(self, key: Hashable) -> ""DataArray"":\n        raise NotImplementedError()\n\n    def __setitem__(self, key: Hashable, value: Any) -> None:\n        self.update({key: value})\n\n    @property\n    def _names(self) -> Set[Hashable]:\n        raise NotImplementedError()\n\n    @property\n    def dims(self) -> Union[Mapping[Hashable, int], Tuple[Hashable, ...]]:\n        raise NotImplementedError()\n\n    @property\n    def indexes(self) -> Indexes:\n        return self._data.indexes  # type: ignore\n\n    @property\n    def variables(self):\n        raise NotImplementedError()\n\n    def _update_coords(self, coords, indexes):\n        raise NotImplementedError()\n\n    def __iter__(self) -> Iterator[""Hashable""]:\n        # needs to be in the same order as the dataset variables\n        for k in self.variables:\n            if k in self._names:\n                yield k\n\n    def __len__(self) -> int:\n        return len(self._names)\n\n    def __contains__(self, key: Hashable) -> bool:\n        return key in self._names\n\n    def __repr__(self) -> str:\n        return formatting.coords_repr(self)\n\n    def to_dataset(self) -> ""Dataset"":\n        raise NotImplementedError()\n\n    def to_index(self, ordered_dims: Sequence[Hashable] = None) -> pd.Index:\n        """"""Convert all index coordinates into a :py:class:`pandas.Index`.\n\n        Parameters\n        ----------\n        ordered_dims : sequence of hashable, optional\n            Possibly reordered version of this object\'s dimensions indicating\n            the order in which dimensions should appear on the result.\n\n        Returns\n        -------\n        pandas.Index\n            Index subclass corresponding to the outer-product of all dimension\n            coordinates. This will be a MultiIndex if this object is has more\n            than more dimension.\n        """"""\n        if ordered_dims is None:\n            ordered_dims = list(self.dims)\n        elif set(ordered_dims) != set(self.dims):\n            raise ValueError(\n                ""ordered_dims must match dims, but does not: ""\n                ""{} vs {}"".format(ordered_dims, self.dims)\n            )\n\n        if len(ordered_dims) == 0:\n            raise ValueError(""no valid index for a 0-dimensional object"")\n        elif len(ordered_dims) == 1:\n            (dim,) = ordered_dims\n            return self._data.get_index(dim)  # type: ignore\n        else:\n            indexes = [self._data.get_index(k) for k in ordered_dims]  # type: ignore\n            names = list(ordered_dims)\n            return pd.MultiIndex.from_product(indexes, names=names)\n\n    def update(self, other: Mapping[Hashable, Any]) -> None:\n        other_vars = getattr(other, ""variables"", other)\n        coords, indexes = merge_coords(\n            [self.variables, other_vars], priority_arg=1, indexes=self.indexes\n        )\n        self._update_coords(coords, indexes)\n\n    def _merge_raw(self, other):\n        """"""For use with binary arithmetic.""""""\n        if other is None:\n            variables = dict(self.variables)\n            indexes = dict(self.indexes)\n        else:\n            variables, indexes = merge_coordinates_without_align([self, other])\n        return variables, indexes\n\n    @contextmanager\n    def _merge_inplace(self, other):\n        """"""For use with in-place binary arithmetic.""""""\n        if other is None:\n            yield\n        else:\n            # don\'t include indexes in prioritized, because we didn\'t align\n            # first and we want indexes to be checked\n            prioritized = {\n                k: (v, None) for k, v in self.variables.items() if k not in self.indexes\n            }\n            variables, indexes = merge_coordinates_without_align(\n                [self, other], prioritized\n            )\n            yield\n            self._update_coords(variables, indexes)\n\n    def merge(self, other: ""Coordinates"") -> ""Dataset"":\n        """"""Merge two sets of coordinates to create a new Dataset\n\n        The method implements the logic used for joining coordinates in the\n        result of a binary operation performed on xarray objects:\n\n        - If two index coordinates conflict (are not equal), an exception is\n          raised. You must align your data before passing it to this method.\n        - If an index coordinate and a non-index coordinate conflict, the non-\n          index coordinate is dropped.\n        - If two non-index coordinates conflict, both are dropped.\n\n        Parameters\n        ----------\n        other : DatasetCoordinates or DataArrayCoordinates\n            The coordinates from another dataset or data array.\n\n        Returns\n        -------\n        merged : Dataset\n            A new Dataset with merged coordinates.\n        """"""\n        from .dataset import Dataset\n\n        if other is None:\n            return self.to_dataset()\n\n        if not isinstance(other, Coordinates):\n            other = Dataset(coords=other).coords\n\n        coords, indexes = merge_coordinates_without_align([self, other])\n        coord_names = set(coords)\n        merged = Dataset._construct_direct(\n            variables=coords, coord_names=coord_names, indexes=indexes\n        )\n        return merged\n\n\nclass DatasetCoordinates(Coordinates):\n    """"""Dictionary like container for Dataset coordinates.\n\n    Essentially an immutable dictionary with keys given by the array\'s\n    dimensions and the values given by the corresponding xarray.Coordinate\n    objects.\n    """"""\n\n    __slots__ = (""_data"",)\n\n    def __init__(self, dataset: ""Dataset""):\n        self._data = dataset\n\n    @property\n    def _names(self) -> Set[Hashable]:\n        return self._data._coord_names\n\n    @property\n    def dims(self) -> Mapping[Hashable, int]:\n        return self._data.dims\n\n    @property\n    def variables(self) -> Mapping[Hashable, Variable]:\n        return Frozen(\n            {k: v for k, v in self._data.variables.items() if k in self._names}\n        )\n\n    def __getitem__(self, key: Hashable) -> ""DataArray"":\n        if key in self._data.data_vars:\n            raise KeyError(key)\n        return cast(""DataArray"", self._data[key])\n\n    def to_dataset(self) -> ""Dataset"":\n        """"""Convert these coordinates into a new Dataset\n        """"""\n        return self._data._copy_listed(self._names)\n\n    def _update_coords(\n        self, coords: Dict[Hashable, Variable], indexes: Mapping[Hashable, pd.Index]\n    ) -> None:\n        from .dataset import calculate_dimensions\n\n        variables = self._data._variables.copy()\n        variables.update(coords)\n\n        # check for inconsistent state *before* modifying anything in-place\n        dims = calculate_dimensions(variables)\n        new_coord_names = set(coords)\n        for dim, size in dims.items():\n            if dim in variables:\n                new_coord_names.add(dim)\n\n        self._data._variables = variables\n        self._data._coord_names.update(new_coord_names)\n        self._data._dims = dims\n\n        # TODO(shoyer): once ._indexes is always populated by a dict, modify\n        # it to update inplace instead.\n        original_indexes = dict(self._data.indexes)\n        original_indexes.update(indexes)\n        self._data._indexes = original_indexes\n\n    def __delitem__(self, key: Hashable) -> None:\n        if key in self:\n            del self._data[key]\n        else:\n            raise KeyError(f""{key!r} is not a coordinate variable."")\n\n    def _ipython_key_completions_(self):\n        """"""Provide method for the key-autocompletions in IPython. """"""\n        return [\n            key\n            for key in self._data._ipython_key_completions_()\n            if key not in self._data.data_vars\n        ]\n\n\nclass DataArrayCoordinates(Coordinates):\n    """"""Dictionary like container for DataArray coordinates.\n\n    Essentially a dict with keys given by the array\'s\n    dimensions and the values given by corresponding DataArray objects.\n    """"""\n\n    __slots__ = (""_data"",)\n\n    def __init__(self, dataarray: ""DataArray""):\n        self._data = dataarray\n\n    @property\n    def dims(self) -> Tuple[Hashable, ...]:\n        return self._data.dims\n\n    @property\n    def _names(self) -> Set[Hashable]:\n        return set(self._data._coords)\n\n    def __getitem__(self, key: Hashable) -> ""DataArray"":\n        return self._data._getitem_coord(key)\n\n    def _update_coords(\n        self, coords: Dict[Hashable, Variable], indexes: Mapping[Hashable, pd.Index]\n    ) -> None:\n        from .dataset import calculate_dimensions\n\n        coords_plus_data = coords.copy()\n        coords_plus_data[_THIS_ARRAY] = self._data.variable\n        dims = calculate_dimensions(coords_plus_data)\n        if not set(dims) <= set(self.dims):\n            raise ValueError(\n                ""cannot add coordinates with new dimensions to a DataArray""\n            )\n        self._data._coords = coords\n\n        # TODO(shoyer): once ._indexes is always populated by a dict, modify\n        # it to update inplace instead.\n        original_indexes = dict(self._data.indexes)\n        original_indexes.update(indexes)\n        self._data._indexes = original_indexes\n\n    @property\n    def variables(self):\n        return Frozen(self._data._coords)\n\n    def to_dataset(self) -> ""Dataset"":\n        from .dataset import Dataset\n\n        coords = {k: v.copy(deep=False) for k, v in self._data._coords.items()}\n        return Dataset._construct_direct(coords, set(coords))\n\n    def __delitem__(self, key: Hashable) -> None:\n        if key in self:\n            del self._data._coords[key]\n            if self._data._indexes is not None and key in self._data._indexes:\n                del self._data._indexes[key]\n        else:\n            raise KeyError(f""{key!r} is not a coordinate variable."")\n\n    def _ipython_key_completions_(self):\n        """"""Provide method for the key-autocompletions in IPython. """"""\n        return self._data._ipython_key_completions_()\n\n\nclass LevelCoordinatesSource(Mapping[Hashable, Any]):\n    """"""Iterator for MultiIndex level coordinates.\n\n    Used for attribute style lookup with AttrAccessMixin. Not returned directly\n    by any public methods.\n    """"""\n\n    __slots__ = (""_data"",)\n\n    def __init__(self, data_object: ""Union[DataArray, Dataset]""):\n        self._data = data_object\n\n    def __getitem__(self, key):\n        # not necessary -- everything here can already be found in coords.\n        raise KeyError()\n\n    def __iter__(self) -> Iterator[Hashable]:\n        return iter(self._data._level_coords)\n\n    def __len__(self) -> int:\n        return len(self._data._level_coords)\n\n\ndef assert_coordinate_consistent(\n    obj: Union[""DataArray"", ""Dataset""], coords: Mapping[Hashable, Variable]\n) -> None:\n    """"""Make sure the dimension coordinate of obj is consistent with coords.\n\n    obj: DataArray or Dataset\n    coords: Dict-like of variables\n    """"""\n    for k in obj.dims:\n        # make sure there are no conflict in dimension coordinates\n        if k in coords and k in obj.coords:\n            if not coords[k].equals(obj[k].variable):\n                raise IndexError(\n                    ""dimension coordinate {!r} conflicts between ""\n                    ""indexed and indexing objects:\\n{}\\nvs.\\n{}"".format(\n                        k, obj[k], coords[k]\n                    )\n                )\n\n\ndef remap_label_indexers(\n    obj: Union[""DataArray"", ""Dataset""],\n    indexers: Mapping[Hashable, Any] = None,\n    method: str = None,\n    tolerance=None,\n    **indexers_kwargs: Any,\n) -> Tuple[dict, dict]:  # TODO more precise return type after annotations in indexing\n    """"""Remap indexers from obj.coords.\n    If indexer is an instance of DataArray and it has coordinate, then this coordinate\n    will be attached to pos_indexers.\n\n    Returns\n    -------\n    pos_indexers: Same type of indexers.\n        np.ndarray or Variable or DataArray\n    new_indexes: mapping of new dimensional-coordinate.\n    """"""\n    from .dataarray import DataArray\n\n    indexers = either_dict_or_kwargs(indexers, indexers_kwargs, ""remap_label_indexers"")\n\n    v_indexers = {\n        k: v.variable.data if isinstance(v, DataArray) else v\n        for k, v in indexers.items()\n    }\n\n    pos_indexers, new_indexes = indexing.remap_label_indexers(\n        obj, v_indexers, method=method, tolerance=tolerance\n    )\n    # attach indexer\'s coordinate to pos_indexers\n    for k, v in indexers.items():\n        if isinstance(v, Variable):\n            pos_indexers[k] = Variable(v.dims, pos_indexers[k])\n        elif isinstance(v, DataArray):\n            # drop coordinates found in indexers since .sel() already\n            # ensures alignments\n            coords = {k: var for k, var in v._coords.items() if k not in indexers}\n            pos_indexers[k] = DataArray(pos_indexers[k], coords=coords, dims=v.dims)\n    return pos_indexers, new_indexes\n'"
xarray/core/dask_array_compat.py,12,"b'import warnings\nfrom distutils.version import LooseVersion\nfrom typing import Iterable\n\nimport numpy as np\n\nfrom .pycompat import dask_array_type\n\ntry:\n    import dask.array as da\n    from dask import __version__ as dask_version\nexcept ImportError:\n    dask_version = ""0.0.0""\n    da = None\n\nif LooseVersion(dask_version) >= LooseVersion(""2.0.0""):\n    meta_from_array = da.utils.meta_from_array\nelse:\n    # Copied from dask v2.4.0\n    # Used under the terms of Dask\'s license, see licenses/DASK_LICENSE.\n    import numbers\n\n    def meta_from_array(x, ndim=None, dtype=None):\n        """""" Normalize an array to appropriate meta object\n\n        Parameters\n        ----------\n        x: array-like, callable\n        Either an object that looks sufficiently like a Numpy array,\n        or a callable that accepts shape and dtype keywords\n        ndim: int\n        Number of dimensions of the array\n        dtype: Numpy dtype\n        A valid input for ``np.dtype``\n\n        Returns\n        -------\n        array-like with zero elements of the correct dtype\n        """"""\n        # If using x._meta, x must be a Dask Array, some libraries (e.g. zarr)\n        # implement a _meta attribute that are incompatible with Dask Array._meta\n        if hasattr(x, ""_meta"") and isinstance(x, dask_array_type):\n            x = x._meta\n\n        if dtype is None and x is None:\n            raise ValueError(""You must specify the meta or dtype of the array"")\n\n        if np.isscalar(x):\n            x = np.array(x)\n\n        if x is None:\n            x = np.ndarray\n\n        if isinstance(x, type):\n            x = x(shape=(0,) * (ndim or 0), dtype=dtype)\n\n        if (\n            not hasattr(x, ""shape"")\n            or not hasattr(x, ""dtype"")\n            or not isinstance(x.shape, tuple)\n        ):\n            return x\n\n        if isinstance(x, list) or isinstance(x, tuple):\n            ndims = [\n                0\n                if isinstance(a, numbers.Number)\n                else a.ndim\n                if hasattr(a, ""ndim"")\n                else len(a)\n                for a in x\n            ]\n            a = [a if nd == 0 else meta_from_array(a, nd) for a, nd in zip(x, ndims)]\n            return a if isinstance(x, list) else tuple(x)\n\n        if ndim is None:\n            ndim = x.ndim\n\n        try:\n            meta = x[tuple(slice(0, 0, None) for _ in range(x.ndim))]\n            if meta.ndim != ndim:\n                if ndim > x.ndim:\n                    meta = meta[\n                        (Ellipsis,) + tuple(None for _ in range(ndim - meta.ndim))\n                    ]\n                    meta = meta[tuple(slice(0, 0, None) for _ in range(meta.ndim))]\n                elif ndim == 0:\n                    meta = meta.sum()\n                else:\n                    meta = meta.reshape((0,) * ndim)\n        except Exception:\n            meta = np.empty((0,) * ndim, dtype=dtype or x.dtype)\n\n        if np.isscalar(meta):\n            meta = np.array(meta)\n\n        if dtype and meta.dtype != dtype:\n            meta = meta.astype(dtype)\n\n        return meta\n\n\ndef _validate_pad_output_shape(input_shape, pad_width, output_shape):\n    """""" Validates the output shape of dask.array.pad, raising a RuntimeError if they do not match.\n    In the current versions of dask (2.2/2.4), dask.array.pad with mode=\'reflect\' sometimes returns\n    an invalid shape.\n    """"""\n    isint = lambda i: isinstance(i, int)\n\n    if isint(pad_width):\n        pass\n    elif len(pad_width) == 2 and all(map(isint, pad_width)):\n        pad_width = sum(pad_width)\n    elif (\n        len(pad_width) == len(input_shape)\n        and all(map(lambda x: len(x) == 2, pad_width))\n        and all((isint(i) for p in pad_width for i in p))\n    ):\n        pad_width = np.sum(pad_width, axis=1)\n    else:\n        # unreachable: dask.array.pad should already have thrown an error\n        raise ValueError(""Invalid value for `pad_width`"")\n\n    if not np.array_equal(np.array(input_shape) + pad_width, output_shape):\n        raise RuntimeError(\n            ""There seems to be something wrong with the shape of the output of dask.array.pad, ""\n            ""try upgrading Dask, use a different pad mode e.g. mode=\'constant\' or first convert ""\n            ""your DataArray/Dataset to one backed by a numpy array by calling the `compute()` method.""\n            ""See: https://github.com/dask/dask/issues/5303""\n        )\n\n\ndef pad(array, pad_width, mode=""constant"", **kwargs):\n    padded = da.pad(array, pad_width, mode=mode, **kwargs)\n    # workaround for inconsistency between numpy and dask: https://github.com/dask/dask/issues/5303\n    if mode == ""mean"" and issubclass(array.dtype.type, np.integer):\n        warnings.warn(\n            \'dask.array.pad(mode=""mean"") converts integers to floats. xarray converts \'\n            ""these floats back to integers to keep the interface consistent. There is a chance that ""\n            ""this introduces rounding errors. If you wish to keep the values as floats, first change ""\n            ""the dtype to a float before calling pad."",\n            UserWarning,\n        )\n        return da.round(padded).astype(array.dtype)\n    _validate_pad_output_shape(array.shape, pad_width, padded.shape)\n    return padded\n\n\nif LooseVersion(dask_version) >= LooseVersion(""2.8.1""):\n    median = da.median\nelse:\n    # Copied from dask v2.8.1\n    # Used under the terms of Dask\'s license, see licenses/DASK_LICENSE.\n    def median(a, axis=None, keepdims=False):\n        """"""\n        This works by automatically chunking the reduced axes to a single chunk\n        and then calling ``numpy.median`` function across the remaining dimensions\n        """"""\n\n        if axis is None:\n            raise NotImplementedError(\n                ""The da.median function only works along an axis.  ""\n                ""The full algorithm is difficult to do in parallel""\n            )\n\n        if not isinstance(axis, Iterable):\n            axis = (axis,)\n\n        axis = [ax + a.ndim if ax < 0 else ax for ax in axis]\n\n        a = a.rechunk({ax: -1 if ax in axis else ""auto"" for ax in range(a.ndim)})\n\n        result = a.map_blocks(\n            np.median,\n            axis=axis,\n            keepdims=keepdims,\n            drop_axis=axis if not keepdims else None,\n            chunks=[1 if ax in axis else c for ax, c in enumerate(a.chunks)]\n            if keepdims\n            else None,\n        )\n\n        return result\n\n\nif LooseVersion(dask_version) > LooseVersion(""2.9.0""):\n    nanmedian = da.nanmedian\nelse:\n\n    def nanmedian(a, axis=None, keepdims=False):\n        """"""\n        This works by automatically chunking the reduced axes to a single chunk\n        and then calling ``numpy.nanmedian`` function across the remaining dimensions\n        """"""\n\n        if axis is None:\n            raise NotImplementedError(\n                ""The da.nanmedian function only works along an axis.  ""\n                ""The full algorithm is difficult to do in parallel""\n            )\n\n        if not isinstance(axis, Iterable):\n            axis = (axis,)\n\n        axis = [ax + a.ndim if ax < 0 else ax for ax in axis]\n\n        a = a.rechunk({ax: -1 if ax in axis else ""auto"" for ax in range(a.ndim)})\n\n        result = a.map_blocks(\n            np.nanmedian,\n            axis=axis,\n            keepdims=keepdims,\n            drop_axis=axis if not keepdims else None,\n            chunks=[1 if ax in axis else c for ax, c in enumerate(a.chunks)]\n            if keepdims\n            else None,\n        )\n\n        return result\n'"
xarray/core/dask_array_ops.py,2,"b'import numpy as np\n\nfrom . import dtypes, nputils\n\n\ndef dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1):\n    """"""Wrapper to apply bottleneck moving window funcs on dask arrays\n    """"""\n    import dask.array as da\n\n    dtype, fill_value = dtypes.maybe_promote(a.dtype)\n    a = a.astype(dtype)\n    # inputs for overlap\n    if axis < 0:\n        axis = a.ndim + axis\n    depth = {d: 0 for d in range(a.ndim)}\n    depth[axis] = (window + 1) // 2\n    boundary = {d: fill_value for d in range(a.ndim)}\n    # Create overlap array.\n    ag = da.overlap.overlap(a, depth=depth, boundary=boundary)\n    # apply rolling func\n    out = ag.map_blocks(\n        moving_func, window, min_count=min_count, axis=axis, dtype=a.dtype\n    )\n    # trim array\n    result = da.overlap.trim_internal(out, depth)\n    return result\n\n\ndef rolling_window(a, axis, window, center, fill_value):\n    """"""Dask\'s equivalence to np.utils.rolling_window\n    """"""\n    import dask.array as da\n\n    orig_shape = a.shape\n    if axis < 0:\n        axis = a.ndim + axis\n    depth = {d: 0 for d in range(a.ndim)}\n    depth[axis] = int(window / 2)\n    # For evenly sized window, we need to crop the first point of each block.\n    offset = 1 if window % 2 == 0 else 0\n\n    if depth[axis] > min(a.chunks[axis]):\n        raise ValueError(\n            ""For window size %d, every chunk should be larger than %d, ""\n            ""but the smallest chunk size is %d. Rechunk your array\\n""\n            ""with a larger chunk size or a chunk size that\\n""\n            ""more evenly divides the shape of your array.""\n            % (window, depth[axis], min(a.chunks[axis]))\n        )\n\n    # Although da.overlap pads values to boundaries of the array,\n    # the size of the generated array is smaller than what we want\n    # if center == False.\n    if center:\n        start = int(window / 2)  # 10 -> 5,  9 -> 4\n        end = window - 1 - start\n    else:\n        start, end = window - 1, 0\n    pad_size = max(start, end) + offset - depth[axis]\n    drop_size = 0\n    # pad_size becomes more than 0 when the overlapped array is smaller than\n    # needed. In this case, we need to enlarge the original array by padding\n    # before overlapping.\n    if pad_size > 0:\n        if pad_size < depth[axis]:\n            # overlapping requires each chunk larger than depth. If pad_size is\n            # smaller than the depth, we enlarge this and truncate it later.\n            drop_size = depth[axis] - pad_size\n            pad_size = depth[axis]\n        shape = list(a.shape)\n        shape[axis] = pad_size\n        chunks = list(a.chunks)\n        chunks[axis] = (pad_size,)\n        fill_array = da.full(shape, fill_value, dtype=a.dtype, chunks=chunks)\n        a = da.concatenate([fill_array, a], axis=axis)\n\n    boundary = {d: fill_value for d in range(a.ndim)}\n\n    # create overlap arrays\n    ag = da.overlap.overlap(a, depth=depth, boundary=boundary)\n\n    # apply rolling func\n    def func(x, window, axis=-1):\n        x = np.asarray(x)\n        rolling = nputils._rolling_window(x, window, axis)\n        return rolling[(slice(None),) * axis + (slice(offset, None),)]\n\n    chunks = list(a.chunks)\n    chunks.append(window)\n    out = ag.map_blocks(\n        func, dtype=a.dtype, new_axis=a.ndim, chunks=chunks, window=window, axis=axis\n    )\n\n    # crop boundary.\n    index = (slice(None),) * axis + (slice(drop_size, drop_size + orig_shape[axis]),)\n    return out[index]\n\n\ndef least_squares(lhs, rhs, rcond=None, skipna=False):\n    import dask.array as da\n\n    lhs_da = da.from_array(lhs, chunks=(rhs.chunks[0], lhs.shape[1]))\n    if skipna:\n        added_dim = rhs.ndim == 1\n        if added_dim:\n            rhs = rhs.reshape(rhs.shape[0], 1)\n        results = da.apply_along_axis(\n            nputils._nanpolyfit_1d,\n            0,\n            rhs,\n            lhs_da,\n            dtype=float,\n            shape=(lhs.shape[1] + 1,),\n            rcond=rcond,\n        )\n        coeffs = results[:-1, ...]\n        residuals = results[-1, ...]\n        if added_dim:\n            coeffs = coeffs.reshape(coeffs.shape[0])\n            residuals = residuals.reshape(residuals.shape[0])\n    else:\n        coeffs, residuals, _, _ = da.linalg.lstsq(lhs_da, rhs)\n    return coeffs, residuals\n'"
xarray/core/dataarray.py,33,"b'import datetime\nimport functools\nfrom numbers import Number\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Hashable,\n    Iterable,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    TypeVar,\n    Union,\n    cast,\n)\n\nimport numpy as np\nimport pandas as pd\n\nfrom ..plot.plot import _PlotMethods\nfrom . import (\n    computation,\n    dtypes,\n    groupby,\n    indexing,\n    ops,\n    pdcompat,\n    resample,\n    rolling,\n    utils,\n    weighted,\n)\nfrom .accessor_dt import CombinedDatetimelikeAccessor\nfrom .accessor_str import StringAccessor\nfrom .alignment import (\n    _broadcast_helper,\n    _get_broadcast_dims_map_common_coords,\n    align,\n    reindex_like_indexers,\n)\nfrom .common import AbstractArray, DataWithCoords\nfrom .coordinates import (\n    DataArrayCoordinates,\n    LevelCoordinatesSource,\n    assert_coordinate_consistent,\n    remap_label_indexers,\n)\nfrom .dataset import Dataset, split_indexes\nfrom .formatting import format_item\nfrom .indexes import Indexes, default_indexes, propagate_indexes\nfrom .indexing import is_fancy_indexer\nfrom .merge import PANDAS_TYPES, _extract_indexes_from_coords\nfrom .options import OPTIONS\nfrom .utils import Default, ReprObject, _check_inplace, _default, either_dict_or_kwargs\nfrom .variable import (\n    IndexVariable,\n    Variable,\n    as_compatible_data,\n    as_variable,\n    assert_unique_multiindex_level_names,\n)\n\nif TYPE_CHECKING:\n    T_DSorDA = TypeVar(""T_DSorDA"", ""DataArray"", Dataset)\n\n    try:\n        from dask.delayed import Delayed\n    except ImportError:\n        Delayed = None\n    try:\n        from cdms2 import Variable as cdms2_Variable\n    except ImportError:\n        cdms2_Variable = None\n    try:\n        from iris.cube import Cube as iris_Cube\n    except ImportError:\n        iris_Cube = None\n\n\ndef _infer_coords_and_dims(\n    shape, coords, dims\n) -> ""Tuple[Dict[Any, Variable], Tuple[Hashable, ...]]"":\n    """"""All the logic for creating a new DataArray""""""\n\n    if (\n        coords is not None\n        and not utils.is_dict_like(coords)\n        and len(coords) != len(shape)\n    ):\n        raise ValueError(\n            ""coords is not dict-like, but it has %s items, ""\n            ""which does not match the %s dimensions of the ""\n            ""data"" % (len(coords), len(shape))\n        )\n\n    if isinstance(dims, str):\n        dims = (dims,)\n\n    if dims is None:\n        dims = [""dim_%s"" % n for n in range(len(shape))]\n        if coords is not None and len(coords) == len(shape):\n            # try to infer dimensions from coords\n            if utils.is_dict_like(coords):\n                # deprecated in GH993, removed in GH1539\n                raise ValueError(\n                    ""inferring DataArray dimensions from ""\n                    ""dictionary like ``coords`` is no longer ""\n                    ""supported. Use an explicit list of ""\n                    ""``dims`` instead.""\n                )\n            for n, (dim, coord) in enumerate(zip(dims, coords)):\n                coord = as_variable(coord, name=dims[n]).to_index_variable()\n                dims[n] = coord.name\n        dims = tuple(dims)\n    elif len(dims) != len(shape):\n        raise ValueError(\n            ""different number of dimensions on data ""\n            ""and dims: %s vs %s"" % (len(shape), len(dims))\n        )\n    else:\n        for d in dims:\n            if not isinstance(d, str):\n                raise TypeError(""dimension %s is not a string"" % d)\n\n    new_coords: Dict[Any, Variable] = {}\n\n    if utils.is_dict_like(coords):\n        for k, v in coords.items():\n            new_coords[k] = as_variable(v, name=k)\n    elif coords is not None:\n        for dim, coord in zip(dims, coords):\n            var = as_variable(coord, name=dim)\n            var.dims = (dim,)\n            new_coords[dim] = var.to_index_variable()\n\n    sizes = dict(zip(dims, shape))\n    for k, v in new_coords.items():\n        if any(d not in dims for d in v.dims):\n            raise ValueError(\n                ""coordinate %s has dimensions %s, but these ""\n                ""are not a subset of the DataArray ""\n                ""dimensions %s"" % (k, v.dims, dims)\n            )\n\n        for d, s in zip(v.dims, v.shape):\n            if s != sizes[d]:\n                raise ValueError(\n                    ""conflicting sizes for dimension %r: ""\n                    ""length %s on the data but length %s on ""\n                    ""coordinate %r"" % (d, sizes[d], s, k)\n                )\n\n        if k in sizes and v.shape != (sizes[k],):\n            raise ValueError(\n                ""coordinate %r is a DataArray dimension, but ""\n                ""it has shape %r rather than expected shape %r ""\n                ""matching the dimension size"" % (k, v.shape, (sizes[k],))\n            )\n\n    assert_unique_multiindex_level_names(new_coords)\n\n    return new_coords, dims\n\n\ndef _check_data_shape(data, coords, dims):\n    if data is dtypes.NA:\n        data = np.nan\n    if coords is not None and utils.is_scalar(data, include_0d=False):\n        if utils.is_dict_like(coords):\n            if dims is None:\n                return data\n            else:\n                data_shape = tuple(\n                    as_variable(coords[k], k).size if k in coords.keys() else 1\n                    for k in dims\n                )\n        else:\n            data_shape = tuple(as_variable(coord, ""foo"").size for coord in coords)\n        data = np.full(data_shape, data)\n    return data\n\n\nclass _LocIndexer:\n    __slots__ = (""data_array"",)\n\n    def __init__(self, data_array: ""DataArray""):\n        self.data_array = data_array\n\n    def __getitem__(self, key) -> ""DataArray"":\n        if not utils.is_dict_like(key):\n            # expand the indexer so we can handle Ellipsis\n            labels = indexing.expanded_indexer(key, self.data_array.ndim)\n            key = dict(zip(self.data_array.dims, labels))\n        return self.data_array.sel(**key)\n\n    def __setitem__(self, key, value) -> None:\n        if not utils.is_dict_like(key):\n            # expand the indexer so we can handle Ellipsis\n            labels = indexing.expanded_indexer(key, self.data_array.ndim)\n            key = dict(zip(self.data_array.dims, labels))\n\n        pos_indexers, _ = remap_label_indexers(self.data_array, key)\n        self.data_array[pos_indexers] = value\n\n\n# Used as the key corresponding to a DataArray\'s variable when converting\n# arbitrary DataArray objects to datasets\n_THIS_ARRAY = ReprObject(""<this-array>"")\n\n\nclass DataArray(AbstractArray, DataWithCoords):\n    """"""N-dimensional array with labeled coordinates and dimensions.\n\n    DataArray provides a wrapper around numpy ndarrays that uses labeled\n    dimensions and coordinates to support metadata aware operations. The API is\n    similar to that for the pandas Series or DataFrame, but DataArray objects\n    can have any number of dimensions, and their contents have fixed data\n    types.\n\n    Additional features over raw numpy arrays:\n\n    - Apply operations over dimensions by name: ``x.sum(\'time\')``.\n    - Select or assign values by integer location (like numpy): ``x[:10]``\n      or by label (like pandas): ``x.loc[\'2014-01-01\']`` or\n      ``x.sel(time=\'2014-01-01\')``.\n    - Mathematical operations (e.g., ``x - y``) vectorize across multiple\n      dimensions (known in numpy as ""broadcasting"") based on dimension names,\n      regardless of their original order.\n    - Keep track of arbitrary metadata in the form of a Python dictionary:\n      ``x.attrs``\n    - Convert to a pandas Series: ``x.to_series()``.\n\n    Getting items from or doing mathematical operations with a DataArray\n    always returns another DataArray.\n    """"""\n\n    _cache: Dict[str, Any]\n    _coords: Dict[Any, Variable]\n    _indexes: Optional[Dict[Hashable, pd.Index]]\n    _name: Optional[Hashable]\n    _variable: Variable\n\n    __slots__ = (\n        ""_cache"",\n        ""_coords"",\n        ""_file_obj"",\n        ""_indexes"",\n        ""_name"",\n        ""_variable"",\n        ""__weakref__"",\n    )\n\n    _groupby_cls = groupby.DataArrayGroupBy\n    _rolling_cls = rolling.DataArrayRolling\n    _coarsen_cls = rolling.DataArrayCoarsen\n    _resample_cls = resample.DataArrayResample\n    _weighted_cls = weighted.DataArrayWeighted\n\n    dt = property(CombinedDatetimelikeAccessor)\n\n    def __init__(\n        self,\n        data: Any = dtypes.NA,\n        coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,\n        dims: Union[Hashable, Sequence[Hashable], None] = None,\n        name: Hashable = None,\n        attrs: Mapping = None,\n        # internal parameters\n        indexes: Dict[Hashable, pd.Index] = None,\n        fastpath: bool = False,\n    ):\n        """"""\n        Parameters\n        ----------\n        data : array_like\n            Values for this array. Must be an ``numpy.ndarray``, ndarray like,\n            or castable to an ``ndarray``. If a self-described xarray or pandas\n            object, attempts are made to use this array\'s metadata to fill in\n            other unspecified arguments. A view of the array\'s data is used\n            instead of a copy if possible.\n        coords : sequence or dict of array_like objects, optional\n            Coordinates (tick labels) to use for indexing along each dimension.\n            The following notations are accepted:\n\n            - mapping {dimension name: array-like}\n            - sequence of tuples that are valid arguments for xarray.Variable()\n              - (dims, data)\n              - (dims, data, attrs)\n              - (dims, data, attrs, encoding)\n\n            Additionally, it is possible to define a coord whose name\n            does not match the dimension name, or a coord based on multiple\n            dimensions, with one of the following notations:\n\n            - mapping {coord name: DataArray}\n            - mapping {coord name: Variable}\n            - mapping {coord name: (dimension name, array-like)}\n            - mapping {coord name: (tuple of dimension names, array-like)}\n\n        dims : hashable or sequence of hashable, optional\n            Name(s) of the data dimension(s). Must be either a hashable (only\n            for 1D data) or a sequence of hashables with length equal to the\n            number of dimensions. If this argument is omitted, dimension names\n            default to ``[\'dim_0\', ... \'dim_n\']``.\n        name : str or None, optional\n            Name of this array.\n        attrs : dict_like or None, optional\n            Attributes to assign to the new instance. By default, an empty\n            attribute dictionary is initialized.\n        """"""\n        if fastpath:\n            variable = data\n            assert dims is None\n            assert attrs is None\n        else:\n            # try to fill in arguments from data if they weren\'t supplied\n            if coords is None:\n\n                if isinstance(data, DataArray):\n                    coords = data.coords\n                elif isinstance(data, pd.Series):\n                    coords = [data.index]\n                elif isinstance(data, pd.DataFrame):\n                    coords = [data.index, data.columns]\n                elif isinstance(data, (pd.Index, IndexVariable)):\n                    coords = [data]\n                elif isinstance(data, pdcompat.Panel):\n                    coords = [data.items, data.major_axis, data.minor_axis]\n\n            if dims is None:\n                dims = getattr(data, ""dims"", getattr(coords, ""dims"", None))\n            if name is None:\n                name = getattr(data, ""name"", None)\n            if attrs is None and not isinstance(data, PANDAS_TYPES):\n                attrs = getattr(data, ""attrs"", None)\n\n            data = _check_data_shape(data, coords, dims)\n            data = as_compatible_data(data)\n            coords, dims = _infer_coords_and_dims(data.shape, coords, dims)\n            variable = Variable(dims, data, attrs, fastpath=True)\n            indexes = dict(\n                _extract_indexes_from_coords(coords)\n            )  # needed for to_dataset\n\n        # These fully describe a DataArray\n        self._variable = variable\n        assert isinstance(coords, dict)\n        self._coords = coords\n        self._name = name\n\n        # TODO(shoyer): document this argument, once it becomes part of the\n        # public interface.\n        self._indexes = indexes\n\n        self._file_obj = None\n\n    def _replace(\n        self,\n        variable: Variable = None,\n        coords=None,\n        name: Union[Hashable, None, Default] = _default,\n        indexes=None,\n    ) -> ""DataArray"":\n        if variable is None:\n            variable = self.variable\n        if coords is None:\n            coords = self._coords\n        if name is _default:\n            name = self.name\n        return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)\n\n    def _replace_maybe_drop_dims(\n        self, variable: Variable, name: Union[Hashable, None, Default] = _default\n    ) -> ""DataArray"":\n        if variable.dims == self.dims and variable.shape == self.shape:\n            coords = self._coords.copy()\n            indexes = self._indexes\n        elif variable.dims == self.dims:\n            # Shape has changed (e.g. from reduce(..., keepdims=True)\n            new_sizes = dict(zip(self.dims, variable.shape))\n            coords = {\n                k: v\n                for k, v in self._coords.items()\n                if v.shape == tuple(new_sizes[d] for d in v.dims)\n            }\n            changed_dims = [\n                k for k in variable.dims if variable.sizes[k] != self.sizes[k]\n            ]\n            indexes = propagate_indexes(self._indexes, exclude=changed_dims)\n        else:\n            allowed_dims = set(variable.dims)\n            coords = {\n                k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims\n            }\n            indexes = propagate_indexes(\n                self._indexes, exclude=(set(self.dims) - allowed_dims)\n            )\n        return self._replace(variable, coords, name, indexes=indexes)\n\n    def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> ""DataArray"":\n        if not len(indexes):\n            return self\n        coords = self._coords.copy()\n        for name, idx in indexes.items():\n            coords[name] = IndexVariable(name, idx)\n        obj = self._replace(coords=coords)\n\n        # switch from dimension to level names, if necessary\n        dim_names: Dict[Any, str] = {}\n        for dim, idx in indexes.items():\n            if not isinstance(idx, pd.MultiIndex) and idx.name != dim:\n                dim_names[dim] = idx.name\n        if dim_names:\n            obj = obj.rename(dim_names)\n        return obj\n\n    def _to_temp_dataset(self) -> Dataset:\n        return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)\n\n    def _from_temp_dataset(\n        self, dataset: Dataset, name: Hashable = _default\n    ) -> ""DataArray"":\n        variable = dataset._variables.pop(_THIS_ARRAY)\n        coords = dataset._variables\n        indexes = dataset._indexes\n        return self._replace(variable, coords, name, indexes=indexes)\n\n    def _to_dataset_split(self, dim: Hashable) -> Dataset:\n        """""" splits dataarray along dimension \'dim\' """"""\n\n        def subset(dim, label):\n            array = self.loc[{dim: label}]\n            array.attrs = {}\n            return as_variable(array)\n\n        variables = {label: subset(dim, label) for label in self.get_index(dim)}\n        variables.update({k: v for k, v in self._coords.items() if k != dim})\n        indexes = propagate_indexes(self._indexes, exclude=dim)\n        coord_names = set(self._coords) - set([dim])\n        dataset = Dataset._construct_direct(\n            variables, coord_names, indexes=indexes, attrs=self.attrs\n        )\n        return dataset\n\n    def _to_dataset_whole(\n        self, name: Hashable = None, shallow_copy: bool = True\n    ) -> Dataset:\n        if name is None:\n            name = self.name\n        if name is None:\n            raise ValueError(\n                ""unable to convert unnamed DataArray to a ""\n                ""Dataset without providing an explicit name""\n            )\n        if name in self.coords:\n            raise ValueError(\n                ""cannot create a Dataset from a DataArray with ""\n                ""the same name as one of its coordinates""\n            )\n        # use private APIs for speed: this is called by _to_temp_dataset(),\n        # which is used in the guts of a lot of operations (e.g., reindex)\n        variables = self._coords.copy()\n        variables[name] = self.variable\n        if shallow_copy:\n            for k in variables:\n                variables[k] = variables[k].copy(deep=False)\n        indexes = self._indexes\n\n        coord_names = set(self._coords)\n        dataset = Dataset._construct_direct(variables, coord_names, indexes=indexes)\n        return dataset\n\n    def to_dataset(\n        self,\n        dim: Hashable = None,\n        *,\n        name: Hashable = None,\n        promote_attrs: bool = False,\n    ) -> Dataset:\n        """"""Convert a DataArray to a Dataset.\n\n        Parameters\n        ----------\n        dim : hashable, optional\n            Name of the dimension on this array along which to split this array\n            into separate variables. If not provided, this array is converted\n            into a Dataset of one variable.\n        name : hashable, optional\n            Name to substitute for this array\'s name. Only valid if ``dim`` is\n            not provided.\n        promote_attrs : bool, default False\n            Set to True to shallow copy attrs of DataArray to returned Dataset.\n\n        Returns\n        -------\n        dataset : Dataset\n        """"""\n        if dim is not None and dim not in self.dims:\n            raise TypeError(\n                f""{dim} is not a dim. If supplying a ``name``, pass as a kwarg.""\n            )\n\n        if dim is not None:\n            if name is not None:\n                raise TypeError(""cannot supply both dim and name arguments"")\n            result = self._to_dataset_split(dim)\n        else:\n            result = self._to_dataset_whole(name)\n\n        if promote_attrs:\n            result.attrs = dict(self.attrs)\n\n        return result\n\n    @property\n    def name(self) -> Optional[Hashable]:\n        """"""The name of this array.\n        """"""\n        return self._name\n\n    @name.setter\n    def name(self, value: Optional[Hashable]) -> None:\n        self._name = value\n\n    @property\n    def variable(self) -> Variable:\n        """"""Low level interface to the Variable object for this DataArray.""""""\n        return self._variable\n\n    @property\n    def dtype(self) -> np.dtype:\n        return self.variable.dtype\n\n    @property\n    def shape(self) -> Tuple[int, ...]:\n        return self.variable.shape\n\n    @property\n    def size(self) -> int:\n        return self.variable.size\n\n    @property\n    def nbytes(self) -> int:\n        return self.variable.nbytes\n\n    @property\n    def ndim(self) -> int:\n        return self.variable.ndim\n\n    def __len__(self) -> int:\n        return len(self.variable)\n\n    @property\n    def data(self) -> Any:\n        """"""The array\'s data as a dask or numpy array\n        """"""\n        return self.variable.data\n\n    @data.setter\n    def data(self, value: Any) -> None:\n        self.variable.data = value\n\n    @property\n    def values(self) -> np.ndarray:\n        """"""The array\'s data as a numpy.ndarray""""""\n        return self.variable.values\n\n    @values.setter\n    def values(self, value: Any) -> None:\n        self.variable.values = value\n\n    @property\n    def _in_memory(self) -> bool:\n        return self.variable._in_memory\n\n    def to_index(self) -> pd.Index:\n        """"""Convert this variable to a pandas.Index. Only possible for 1D\n        arrays.\n        """"""\n        return self.variable.to_index()\n\n    @property\n    def dims(self) -> Tuple[Hashable, ...]:\n        """"""Tuple of dimension names associated with this array.\n\n        Note that the type of this property is inconsistent with\n        `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for\n        consistently named properties.\n        """"""\n        return self.variable.dims\n\n    @dims.setter\n    def dims(self, value):\n        raise AttributeError(\n            ""you cannot assign dims on a DataArray. Use ""\n            "".rename() or .swap_dims() instead.""\n        )\n\n    def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:\n        if utils.is_dict_like(key):\n            return key\n        else:\n            key = indexing.expanded_indexer(key, self.ndim)\n            return dict(zip(self.dims, key))\n\n    @property\n    def _level_coords(self) -> Dict[Hashable, Hashable]:\n        """"""Return a mapping of all MultiIndex levels and their corresponding\n        coordinate name.\n        """"""\n        level_coords: Dict[Hashable, Hashable] = {}\n\n        for cname, var in self._coords.items():\n            if var.ndim == 1 and isinstance(var, IndexVariable):\n                level_names = var.level_names\n                if level_names is not None:\n                    (dim,) = var.dims\n                    level_coords.update({lname: dim for lname in level_names})\n        return level_coords\n\n    def _getitem_coord(self, key):\n        from .dataset import _get_virtual_variable\n\n        try:\n            var = self._coords[key]\n        except KeyError:\n            dim_sizes = dict(zip(self.dims, self.shape))\n            _, key, var = _get_virtual_variable(\n                self._coords, key, self._level_coords, dim_sizes\n            )\n\n        return self._replace_maybe_drop_dims(var, name=key)\n\n    def __getitem__(self, key: Any) -> ""DataArray"":\n        if isinstance(key, str):\n            return self._getitem_coord(key)\n        else:\n            # xarray-style array indexing\n            return self.isel(indexers=self._item_key_to_dict(key))\n\n    def __setitem__(self, key: Any, value: Any) -> None:\n        if isinstance(key, str):\n            self.coords[key] = value\n        else:\n            # Coordinates in key, value and self[key] should be consistent.\n            # TODO Coordinate consistency in key is checked here, but it\n            # causes unnecessary indexing. It should be optimized.\n            obj = self[key]\n            if isinstance(value, DataArray):\n                assert_coordinate_consistent(value, obj.coords.variables)\n            # DataArray key -> Variable key\n            key = {\n                k: v.variable if isinstance(v, DataArray) else v\n                for k, v in self._item_key_to_dict(key).items()\n            }\n            self.variable[key] = value\n\n    def __delitem__(self, key: Any) -> None:\n        del self.coords[key]\n\n    @property\n    def _attr_sources(self) -> List[Mapping[Hashable, Any]]:\n        """"""List of places to look-up items for attribute-style access\n        """"""\n        return self._item_sources + [self.attrs]\n\n    @property\n    def _item_sources(self) -> List[Mapping[Hashable, Any]]:\n        """"""List of places to look-up items for key-completion\n        """"""\n        return [\n            self.coords,\n            {d: self.coords[d] for d in self.dims},\n            LevelCoordinatesSource(self),\n        ]\n\n    def __contains__(self, key: Any) -> bool:\n        return key in self.data\n\n    @property\n    def loc(self) -> _LocIndexer:\n        """"""Attribute for location based indexing like pandas.\n        """"""\n        return _LocIndexer(self)\n\n    @property\n    def attrs(self) -> Dict[Hashable, Any]:\n        """"""Dictionary storing arbitrary metadata with this array.""""""\n        return self.variable.attrs\n\n    @attrs.setter\n    def attrs(self, value: Mapping[Hashable, Any]) -> None:\n        # Disable type checking to work around mypy bug - see mypy#4167\n        self.variable.attrs = value  # type: ignore\n\n    @property\n    def encoding(self) -> Dict[Hashable, Any]:\n        """"""Dictionary of format-specific settings for how this array should be\n        serialized.""""""\n        return self.variable.encoding\n\n    @encoding.setter\n    def encoding(self, value: Mapping[Hashable, Any]) -> None:\n        self.variable.encoding = value\n\n    @property\n    def indexes(self) -> Indexes:\n        """"""Mapping of pandas.Index objects used for label based indexing\n        """"""\n        if self._indexes is None:\n            self._indexes = default_indexes(self._coords, self.dims)\n        return Indexes(self._indexes)\n\n    @property\n    def coords(self) -> DataArrayCoordinates:\n        """"""Dictionary-like container of coordinate arrays.\n        """"""\n        return DataArrayCoordinates(self)\n\n    def reset_coords(\n        self,\n        names: Union[Iterable[Hashable], Hashable, None] = None,\n        drop: bool = False,\n        inplace: bool = None,\n    ) -> Union[None, ""DataArray"", Dataset]:\n        """"""Given names of coordinates, reset them to become variables.\n\n        Parameters\n        ----------\n        names : hashable or iterable of hashables, optional\n            Name(s) of non-index coordinates in this dataset to reset into\n            variables. By default, all non-index coordinates are reset.\n        drop : bool, optional\n            If True, remove coordinates instead of converting them into\n            variables.\n\n        Returns\n        -------\n        Dataset, or DataArray if ``drop == True``\n        """"""\n        _check_inplace(inplace)\n        if names is None:\n            names = set(self.coords) - set(self.dims)\n        dataset = self.coords.to_dataset().reset_coords(names, drop)\n        if drop:\n            return self._replace(coords=dataset._variables)\n        else:\n            if self.name is None:\n                raise ValueError(\n                    ""cannot reset_coords with drop=False on an unnamed DataArrray""\n                )\n            dataset[self.name] = self.variable\n            return dataset\n\n    def __dask_tokenize__(self):\n        from dask.base import normalize_token\n\n        return normalize_token((type(self), self._variable, self._coords, self._name))\n\n    def __dask_graph__(self):\n        return self._to_temp_dataset().__dask_graph__()\n\n    def __dask_keys__(self):\n        return self._to_temp_dataset().__dask_keys__()\n\n    def __dask_layers__(self):\n        return self._to_temp_dataset().__dask_layers__()\n\n    @property\n    def __dask_optimize__(self):\n        return self._to_temp_dataset().__dask_optimize__\n\n    @property\n    def __dask_scheduler__(self):\n        return self._to_temp_dataset().__dask_scheduler__\n\n    def __dask_postcompute__(self):\n        func, args = self._to_temp_dataset().__dask_postcompute__()\n        return self._dask_finalize, (func, args, self.name)\n\n    def __dask_postpersist__(self):\n        func, args = self._to_temp_dataset().__dask_postpersist__()\n        return self._dask_finalize, (func, args, self.name)\n\n    @staticmethod\n    def _dask_finalize(results, func, args, name):\n        ds = func(results, *args)\n        variable = ds._variables.pop(_THIS_ARRAY)\n        coords = ds._variables\n        return DataArray(variable, coords, name=name, fastpath=True)\n\n    def load(self, **kwargs) -> ""DataArray"":\n        """"""Manually trigger loading of this array\'s data from disk or a\n        remote source into memory and return this array.\n\n        Normally, it should not be necessary to call this method in user code,\n        because all xarray functions should either work on deferred data or\n        load data automatically. However, this method can be necessary when\n        working with many file objects on disk.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Additional keyword arguments passed on to ``dask.array.compute``.\n\n        See Also\n        --------\n        dask.array.compute\n        """"""\n        ds = self._to_temp_dataset().load(**kwargs)\n        new = self._from_temp_dataset(ds)\n        self._variable = new._variable\n        self._coords = new._coords\n        return self\n\n    def compute(self, **kwargs) -> ""DataArray"":\n        """"""Manually trigger loading of this array\'s data from disk or a\n        remote source into memory and return a new array. The original is\n        left unaltered.\n\n        Normally, it should not be necessary to call this method in user code,\n        because all xarray functions should either work on deferred data or\n        load data automatically. However, this method can be necessary when\n        working with many file objects on disk.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Additional keyword arguments passed on to ``dask.array.compute``.\n\n        See Also\n        --------\n        dask.array.compute\n        """"""\n        new = self.copy(deep=False)\n        return new.load(**kwargs)\n\n    def persist(self, **kwargs) -> ""DataArray"":\n        """""" Trigger computation in constituent dask arrays\n\n        This keeps them as dask arrays but encourages them to keep data in\n        memory.  This is particularly useful when on a distributed machine.\n        When on a single machine consider using ``.compute()`` instead.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Additional keyword arguments passed on to ``dask.persist``.\n\n        See Also\n        --------\n        dask.persist\n        """"""\n        ds = self._to_temp_dataset().persist(**kwargs)\n        return self._from_temp_dataset(ds)\n\n    def copy(self, deep: bool = True, data: Any = None) -> ""DataArray"":\n        """"""Returns a copy of this array.\n\n        If `deep=True`, a deep copy is made of the data array.\n        Otherwise, a shallow copy is made, so each variable in the new\n        array\'s dataset is also a variable in this array\'s dataset.\n\n        Use `data` to create a new object with the same structure as\n        original but entirely new data.\n\n        Parameters\n        ----------\n        deep : bool, optional\n            Whether the data array and its coordinates are loaded into memory\n            and copied onto the new object. Default is True.\n        data : array_like, optional\n            Data to use in the new object. Must have same shape as original.\n            When `data` is used, `deep` is ignored for all data variables,\n            and only used for coords.\n\n        Returns\n        -------\n        object : DataArray\n            New object with dimensions, attributes, coordinates, name,\n            encoding, and optionally data copied from original.\n\n        Examples\n        --------\n\n        Shallow versus deep copy\n\n        >>> array = xr.DataArray([1, 2, 3], dims=""x"", coords={""x"": [""a"", ""b"", ""c""]})\n        >>> array.copy()\n        <xarray.DataArray (x: 3)>\n        array([1, 2, 3])\n        Coordinates:\n        * x        (x) <U1 \'a\' \'b\' \'c\'\n        >>> array_0 = array.copy(deep=False)\n        >>> array_0[0] = 7\n        >>> array_0\n        <xarray.DataArray (x: 3)>\n        array([7, 2, 3])\n        Coordinates:\n        * x        (x) <U1 \'a\' \'b\' \'c\'\n        >>> array\n        <xarray.DataArray (x: 3)>\n        array([7, 2, 3])\n        Coordinates:\n        * x        (x) <U1 \'a\' \'b\' \'c\'\n\n        Changing the data using the ``data`` argument maintains the\n        structure of the original object, but with the new data. Original\n        object is unaffected.\n\n        >>> array.copy(data=[0.1, 0.2, 0.3])\n        <xarray.DataArray (x: 3)>\n        array([ 0.1,  0.2,  0.3])\n        Coordinates:\n        * x        (x) <U1 \'a\' \'b\' \'c\'\n        >>> array\n        <xarray.DataArray (x: 3)>\n        array([1, 2, 3])\n        Coordinates:\n        * x        (x) <U1 \'a\' \'b\' \'c\'\n\n        See Also\n        --------\n        pandas.DataFrame.copy\n        """"""\n        variable = self.variable.copy(deep=deep, data=data)\n        coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}\n        if self._indexes is None:\n            indexes = self._indexes\n        else:\n            indexes = {k: v.copy(deep=deep) for k, v in self._indexes.items()}\n        return self._replace(variable, coords, indexes=indexes)\n\n    def __copy__(self) -> ""DataArray"":\n        return self.copy(deep=False)\n\n    def __deepcopy__(self, memo=None) -> ""DataArray"":\n        # memo does nothing but is required for compatibility with\n        # copy.deepcopy\n        return self.copy(deep=True)\n\n    # mutable objects should not be hashable\n    # https://github.com/python/mypy/issues/4266\n    __hash__ = None  # type: ignore\n\n    @property\n    def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:\n        """"""Block dimensions for this array\'s data or None if it\'s not a dask\n        array.\n        """"""\n        return self.variable.chunks\n\n    def chunk(\n        self,\n        chunks: Union[\n            None,\n            Number,\n            Tuple[Number, ...],\n            Tuple[Tuple[Number, ...], ...],\n            Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],\n        ] = None,\n        name_prefix: str = ""xarray-"",\n        token: str = None,\n        lock: bool = False,\n    ) -> ""DataArray"":\n        """"""Coerce this array\'s data into a dask arrays with the given chunks.\n\n        If this variable is a non-dask array, it will be converted to dask\n        array. If it\'s a dask array, it will be rechunked to the given chunk\n        sizes.\n\n        If neither chunks is not provided for one or more dimensions, chunk\n        sizes along that dimension will not be updated; non-dask arrays will be\n        converted into dask arrays with a single block.\n\n        Parameters\n        ----------\n        chunks : int, tuple or mapping, optional\n            Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\n            ``{\'x\': 5, \'y\': 5}``.\n        name_prefix : str, optional\n            Prefix for the name of the new dask array.\n        token : str, optional\n            Token uniquely identifying this array.\n        lock : optional\n            Passed on to :py:func:`dask.array.from_array`, if the array is not\n            already as dask array.\n\n        Returns\n        -------\n        chunked : xarray.DataArray\n        """"""\n        if isinstance(chunks, (tuple, list)):\n            chunks = dict(zip(self.dims, chunks))\n\n        ds = self._to_temp_dataset().chunk(\n            chunks, name_prefix=name_prefix, token=token, lock=lock\n        )\n        return self._from_temp_dataset(ds)\n\n    def isel(\n        self,\n        indexers: Mapping[Hashable, Any] = None,\n        drop: bool = False,\n        missing_dims: str = ""raise"",\n        **indexers_kwargs: Any,\n    ) -> ""DataArray"":\n        """"""Return a new DataArray whose data is given by integer indexing\n        along the specified dimension(s).\n\n        Parameters\n        ----------\n        indexers : dict, optional\n            A dict with keys matching dimensions and values given\n            by integers, slice objects or arrays.\n            indexer can be a integer, slice, array-like or DataArray.\n            If DataArrays are passed as indexers, xarray-style indexing will be\n            carried out. See :ref:`indexing` for the details.\n            One of indexers or indexers_kwargs must be provided.\n        drop : bool, optional\n            If ``drop=True``, drop coordinates variables indexed by integers\n            instead of making them scalar.\n        missing_dims : {""raise"", ""warn"", ""ignore""}, default ""raise""\n            What to do if dimensions that should be selected from are not present in the\n            DataArray:\n            - ""exception"": raise an exception\n            - ""warning"": raise a warning, and ignore the missing dimensions\n            - ""ignore"": ignore the missing dimensions\n        **indexers_kwargs : {dim: indexer, ...}, optional\n            The keyword arguments form of ``indexers``.\n\n        See Also\n        --------\n        Dataset.isel\n        DataArray.sel\n        """"""\n\n        indexers = either_dict_or_kwargs(indexers, indexers_kwargs, ""isel"")\n\n        if any(is_fancy_indexer(idx) for idx in indexers.values()):\n            ds = self._to_temp_dataset()._isel_fancy(\n                indexers, drop=drop, missing_dims=missing_dims\n            )\n            return self._from_temp_dataset(ds)\n\n        # Much faster algorithm for when all indexers are ints, slices, one-dimensional\n        # lists, or zero or one-dimensional np.ndarray\'s\n\n        variable = self._variable.isel(indexers, missing_dims=missing_dims)\n\n        coords = {}\n        for coord_name, coord_value in self._coords.items():\n            coord_indexers = {\n                k: v for k, v in indexers.items() if k in coord_value.dims\n            }\n            if coord_indexers:\n                coord_value = coord_value.isel(coord_indexers)\n                if drop and coord_value.ndim == 0:\n                    continue\n            coords[coord_name] = coord_value\n\n        return self._replace(variable=variable, coords=coords)\n\n    def sel(\n        self,\n        indexers: Mapping[Hashable, Any] = None,\n        method: str = None,\n        tolerance=None,\n        drop: bool = False,\n        **indexers_kwargs: Any,\n    ) -> ""DataArray"":\n        """"""Return a new DataArray whose data is given by selecting index\n        labels along the specified dimension(s).\n\n        .. warning::\n\n          Do not try to assign values when using any of the indexing methods\n          ``isel`` or ``sel``::\n\n            da = xr.DataArray([0, 1, 2, 3], dims=[\'x\'])\n            # DO NOT do this\n            da.isel(x=[0, 1, 2])[1] = -1\n\n          Assigning values with the chained indexing using ``.sel`` or\n          ``.isel`` fails silently.\n\n        See Also\n        --------\n        Dataset.sel\n        DataArray.isel\n\n        """"""\n        ds = self._to_temp_dataset().sel(\n            indexers=indexers,\n            drop=drop,\n            method=method,\n            tolerance=tolerance,\n            **indexers_kwargs,\n        )\n        return self._from_temp_dataset(ds)\n\n    def head(\n        self,\n        indexers: Union[Mapping[Hashable, int], int] = None,\n        **indexers_kwargs: Any,\n    ) -> ""DataArray"":\n        """"""Return a new DataArray whose data is given by the the first `n`\n        values along the specified dimension(s). Default `n` = 5\n\n        See Also\n        --------\n        Dataset.head\n        DataArray.tail\n        DataArray.thin\n        """"""\n        ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)\n        return self._from_temp_dataset(ds)\n\n    def tail(\n        self,\n        indexers: Union[Mapping[Hashable, int], int] = None,\n        **indexers_kwargs: Any,\n    ) -> ""DataArray"":\n        """"""Return a new DataArray whose data is given by the the last `n`\n        values along the specified dimension(s). Default `n` = 5\n\n        See Also\n        --------\n        Dataset.tail\n        DataArray.head\n        DataArray.thin\n        """"""\n        ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)\n        return self._from_temp_dataset(ds)\n\n    def thin(\n        self,\n        indexers: Union[Mapping[Hashable, int], int] = None,\n        **indexers_kwargs: Any,\n    ) -> ""DataArray"":\n        """"""Return a new DataArray whose data is given by each `n` value\n        along the specified dimension(s).\n\n        See Also\n        --------\n        Dataset.thin\n        DataArray.head\n        DataArray.tail\n        """"""\n        ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)\n        return self._from_temp_dataset(ds)\n\n    def broadcast_like(\n        self, other: Union[""DataArray"", Dataset], exclude: Iterable[Hashable] = None\n    ) -> ""DataArray"":\n        """"""Broadcast this DataArray against another Dataset or DataArray.\n\n        This is equivalent to xr.broadcast(other, self)[1]\n\n        xarray objects are broadcast against each other in arithmetic\n        operations, so this method is not be necessary for most uses.\n\n        If no change is needed, the input data is returned to the output\n        without being copied.\n\n        If new coords are added by the broadcast, their values are\n        NaN filled.\n\n        Parameters\n        ----------\n        other : Dataset or DataArray\n            Object against which to broadcast this array.\n        exclude : iterable of hashable, optional\n            Dimensions that must not be broadcasted\n\n        Returns\n        -------\n        new_da: xr.DataArray\n\n        Examples\n        --------\n\n        >>> arr1\n        <xarray.DataArray (x: 2, y: 3)>\n        array([[0.840235, 0.215216, 0.77917 ],\n               [0.726351, 0.543824, 0.875115]])\n        Coordinates:\n          * x        (x) <U1 \'a\' \'b\'\n          * y        (y) <U1 \'a\' \'b\' \'c\'\n        >>> arr2\n        <xarray.DataArray (x: 3, y: 2)>\n        array([[0.612611, 0.125753],\n               [0.853181, 0.948818],\n               [0.180885, 0.33363 ]])\n        Coordinates:\n          * x        (x) <U1 \'a\' \'b\' \'c\'\n          * y        (y) <U1 \'a\' \'b\'\n        >>> arr1.broadcast_like(arr2)\n        <xarray.DataArray (x: 3, y: 3)>\n        array([[0.840235, 0.215216, 0.77917 ],\n               [0.726351, 0.543824, 0.875115],\n               [     nan,      nan,      nan]])\n        Coordinates:\n          * x        (x) object \'a\' \'b\' \'c\'\n          * y        (y) object \'a\' \'b\' \'c\'\n        """"""\n        if exclude is None:\n            exclude = set()\n        else:\n            exclude = set(exclude)\n        args = align(other, self, join=""outer"", copy=False, exclude=exclude)\n\n        dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\n\n        return _broadcast_helper(args[1], exclude, dims_map, common_coords)\n\n    def reindex_like(\n        self,\n        other: Union[""DataArray"", Dataset],\n        method: str = None,\n        tolerance=None,\n        copy: bool = True,\n        fill_value=dtypes.NA,\n    ) -> ""DataArray"":\n        """"""Conform this object onto the indexes of another object, filling in\n        missing values with ``fill_value``. The default fill value is NaN.\n\n        Parameters\n        ----------\n        other : Dataset or DataArray\n            Object with an \'indexes\' attribute giving a mapping from dimension\n            names to pandas.Index objects, which provides coordinates upon\n            which to index the variables in this dataset. The indexes on this\n            other object need not be the same as the indexes on this\n            dataset. Any mis-matched index values will be filled in with\n            NaN, and any mis-matched dimension names will simply be ignored.\n        method : {None, \'nearest\', \'pad\'/\'ffill\', \'backfill\'/\'bfill\'}, optional\n            Method to use for filling index values from other not found on this\n            data array:\n\n            * None (default): don\'t fill gaps\n            * pad / ffill: propagate last valid index value forward\n            * backfill / bfill: propagate next valid index value backward\n            * nearest: use nearest valid index value\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations must\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n        copy : bool, optional\n            If ``copy=True``, data in the return value is always copied. If\n            ``copy=False`` and reindexing is unnecessary, or can be performed\n            with only slice operations, then the output may share memory with\n            the input. In either case, a new xarray object is always returned.\n        fill_value : scalar, optional\n            Value to use for newly missing values\n\n        Returns\n        -------\n        reindexed : DataArray\n            Another dataset array, with this array\'s data but coordinates from\n            the other object.\n\n        See Also\n        --------\n        DataArray.reindex\n        align\n        """"""\n        indexers = reindex_like_indexers(self, other)\n        return self.reindex(\n            indexers=indexers,\n            method=method,\n            tolerance=tolerance,\n            copy=copy,\n            fill_value=fill_value,\n        )\n\n    def reindex(\n        self,\n        indexers: Mapping[Hashable, Any] = None,\n        method: str = None,\n        tolerance=None,\n        copy: bool = True,\n        fill_value=dtypes.NA,\n        **indexers_kwargs: Any,\n    ) -> ""DataArray"":\n        """"""Conform this object onto the indexes of another object, filling in\n        missing values with ``fill_value``. The default fill value is NaN.\n\n        Parameters\n        ----------\n        indexers : dict, optional\n            Dictionary with keys given by dimension names and values given by\n            arrays of coordinates tick labels. Any mis-matched coordinate\n            values will be filled in with NaN, and any mis-matched dimension\n            names will simply be ignored.\n            One of indexers or indexers_kwargs must be provided.\n        copy : bool, optional\n            If ``copy=True``, data in the return value is always copied. If\n            ``copy=False`` and reindexing is unnecessary, or can be performed\n            with only slice operations, then the output may share memory with\n            the input. In either case, a new xarray object is always returned.\n        method : {None, \'nearest\', \'pad\'/\'ffill\', \'backfill\'/\'bfill\'}, optional\n            Method to use for filling index values in ``indexers`` not found on\n            this data array:\n\n            * None (default): don\'t fill gaps\n            * pad / ffill: propagate last valid index value forward\n            * backfill / bfill: propagate next valid index value backward\n            * nearest: use nearest valid index value\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations must\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n        fill_value : scalar, optional\n            Value to use for newly missing values\n        **indexers_kwargs : {dim: indexer, ...}, optional\n            The keyword arguments form of ``indexers``.\n            One of indexers or indexers_kwargs must be provided.\n\n        Returns\n        -------\n        reindexed : DataArray\n            Another dataset array, with this array\'s data but replaced\n            coordinates.\n\n        See Also\n        --------\n        DataArray.reindex_like\n        align\n        """"""\n        indexers = either_dict_or_kwargs(indexers, indexers_kwargs, ""reindex"")\n        ds = self._to_temp_dataset().reindex(\n            indexers=indexers,\n            method=method,\n            tolerance=tolerance,\n            copy=copy,\n            fill_value=fill_value,\n        )\n        return self._from_temp_dataset(ds)\n\n    def interp(\n        self,\n        coords: Mapping[Hashable, Any] = None,\n        method: str = ""linear"",\n        assume_sorted: bool = False,\n        kwargs: Mapping[str, Any] = None,\n        **coords_kwargs: Any,\n    ) -> ""DataArray"":\n        """""" Multidimensional interpolation of variables.\n\n        coords : dict, optional\n            Mapping from dimension names to the new coordinates.\n            new coordinate can be an scalar, array-like or DataArray.\n            If DataArrays are passed as new coordates, their dimensions are\n            used for the broadcasting.\n        method: {\'linear\', \'nearest\'} for multidimensional array,\n            {\'linear\', \'nearest\', \'zero\', \'slinear\', \'quadratic\', \'cubic\'}\n            for 1-dimensional array.\n        assume_sorted: boolean, optional\n            If False, values of x can be in any order and they are sorted\n            first. If True, x has to be an array of monotonically increasing\n            values.\n        kwargs: dictionary\n            Additional keyword arguments passed to scipy\'s interpolator. Valid\n            options and their behavior depend on if 1-dimensional or\n            multi-dimensional interpolation is used.\n        ``**coords_kwargs`` : {dim: coordinate, ...}, optional\n            The keyword arguments form of ``coords``.\n            One of coords or coords_kwargs must be provided.\n\n        Returns\n        -------\n        interpolated: xr.DataArray\n            New dataarray on the new coordinates.\n\n        Notes\n        -----\n        scipy is required.\n\n        See Also\n        --------\n        scipy.interpolate.interp1d\n        scipy.interpolate.interpn\n\n        Examples\n        --------\n        >>> da = xr.DataArray([1, 3], [(""x"", np.arange(2))])\n        >>> da.interp(x=0.5)\n        <xarray.DataArray ()>\n        array(2.0)\n        Coordinates:\n            x        float64 0.5\n        """"""\n        if self.dtype.kind not in ""uifc"":\n            raise TypeError(\n                ""interp only works for a numeric type array. ""\n                ""Given {}."".format(self.dtype)\n            )\n        ds = self._to_temp_dataset().interp(\n            coords,\n            method=method,\n            kwargs=kwargs,\n            assume_sorted=assume_sorted,\n            **coords_kwargs,\n        )\n        return self._from_temp_dataset(ds)\n\n    def interp_like(\n        self,\n        other: Union[""DataArray"", Dataset],\n        method: str = ""linear"",\n        assume_sorted: bool = False,\n        kwargs: Mapping[str, Any] = None,\n    ) -> ""DataArray"":\n        """"""Interpolate this object onto the coordinates of another object,\n        filling out of range values with NaN.\n\n        Parameters\n        ----------\n        other : Dataset or DataArray\n            Object with an \'indexes\' attribute giving a mapping from dimension\n            names to an 1d array-like, which provides coordinates upon\n            which to index the variables in this dataset.\n        method: string, optional.\n            {\'linear\', \'nearest\'} for multidimensional array,\n            {\'linear\', \'nearest\', \'zero\', \'slinear\', \'quadratic\', \'cubic\'}\n            for 1-dimensional array. \'linear\' is used by default.\n        assume_sorted: boolean, optional\n            If False, values of coordinates that are interpolated over can be\n            in any order and they are sorted first. If True, interpolated\n            coordinates are assumed to be an array of monotonically increasing\n            values.\n        kwargs: dictionary, optional\n            Additional keyword passed to scipy\'s interpolator.\n\n        Returns\n        -------\n        interpolated: xr.DataArray\n            Another dataarray by interpolating this dataarray\'s data along the\n            coordinates of the other object.\n\n        Notes\n        -----\n        scipy is required.\n        If the dataarray has object-type coordinates, reindex is used for these\n        coordinates instead of the interpolation.\n\n        See Also\n        --------\n        DataArray.interp\n        DataArray.reindex_like\n        """"""\n        if self.dtype.kind not in ""uifc"":\n            raise TypeError(\n                ""interp only works for a numeric type array. ""\n                ""Given {}."".format(self.dtype)\n            )\n        ds = self._to_temp_dataset().interp_like(\n            other, method=method, kwargs=kwargs, assume_sorted=assume_sorted\n        )\n        return self._from_temp_dataset(ds)\n\n    def rename(\n        self,\n        new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,\n        **names: Hashable,\n    ) -> ""DataArray"":\n        """"""Returns a new DataArray with renamed coordinates or a new name.\n\n        Parameters\n        ----------\n        new_name_or_name_dict : str or dict-like, optional\n            If the argument is dict-like, it used as a mapping from old\n            names to new names for coordinates. Otherwise, use the argument\n            as the new name for this array.\n        **names: hashable, optional\n            The keyword arguments form of a mapping from old names to\n            new names for coordinates.\n            One of new_name_or_name_dict or names must be provided.\n\n        Returns\n        -------\n        renamed : DataArray\n            Renamed array or array with renamed coordinates.\n\n        See Also\n        --------\n        Dataset.rename\n        DataArray.swap_dims\n        """"""\n        if names or utils.is_dict_like(new_name_or_name_dict):\n            new_name_or_name_dict = cast(\n                Mapping[Hashable, Hashable], new_name_or_name_dict\n            )\n            name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, ""rename"")\n            dataset = self._to_temp_dataset().rename(name_dict)\n            return self._from_temp_dataset(dataset)\n        else:\n            new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)\n            return self._replace(name=new_name_or_name_dict)\n\n    def swap_dims(self, dims_dict: Mapping[Hashable, Hashable]) -> ""DataArray"":\n        """"""Returns a new DataArray with swapped dimensions.\n\n        Parameters\n        ----------\n        dims_dict : dict-like\n            Dictionary whose keys are current dimension names and whose values\n            are new names.\n\n        Returns\n        -------\n        swapped : DataArray\n            DataArray with swapped dimensions.\n\n        Examples\n        --------\n\n        >>> arr = xr.DataArray(\n        ...     data=[0, 1], dims=""x"", coords={""x"": [""a"", ""b""], ""y"": (""x"", [0, 1])},\n        ... )\n        >>> arr\n        <xarray.DataArray (x: 2)>\n        array([0, 1])\n        Coordinates:\n          * x        (x) <U1 \'a\' \'b\'\n            y        (x) int64 0 1\n\n        >>> arr.swap_dims({""x"": ""y""})\n        <xarray.DataArray (y: 2)>\n        array([0, 1])\n        Coordinates:\n            x        (y) <U1 \'a\' \'b\'\n          * y        (y) int64 0 1\n\n        >>> arr.swap_dims({""x"": ""z""})\n        <xarray.DataArray (z: 2)>\n        array([0, 1])\n        Coordinates:\n            x        (z) <U1 \'a\' \'b\'\n            y        (z) int64 0 1\n        Dimensions without coordinates: z\n\n        See Also\n        --------\n\n        DataArray.rename\n        Dataset.swap_dims\n        """"""\n        ds = self._to_temp_dataset().swap_dims(dims_dict)\n        return self._from_temp_dataset(ds)\n\n    def expand_dims(\n        self,\n        dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,\n        axis=None,\n        **dim_kwargs: Any,\n    ) -> ""DataArray"":\n        """"""Return a new object with an additional axis (or axes) inserted at\n        the corresponding position in the array shape. The new object is a\n        view into the underlying array, not a copy.\n\n\n        If dim is already a scalar coordinate, it will be promoted to a 1D\n        coordinate consisting of a single value.\n\n        Parameters\n        ----------\n        dim : hashable, sequence of hashable, dict, or None\n            Dimensions to include on the new variable.\n            If provided as str or sequence of str, then dimensions are inserted\n            with length 1. If provided as a dict, then the keys are the new\n            dimensions and the values are either integers (giving the length of\n            the new dimensions) or sequence/ndarray (giving the coordinates of\n            the new dimensions).\n        axis : integer, list (or tuple) of integers, or None\n            Axis position(s) where new axis is to be inserted (position(s) on\n            the result array). If a list (or tuple) of integers is passed,\n            multiple axes are inserted. In this case, dim arguments should be\n            same length list. If axis=None is passed, all the axes will be\n            inserted to the start of the result array.\n        **dim_kwargs : int or sequence/ndarray\n            The keywords are arbitrary dimensions being inserted and the values\n            are either the lengths of the new dims (if int is given), or their\n            coordinates. Note, this is an alternative to passing a dict to the\n            dim kwarg and will only be used if dim is None.\n\n        Returns\n        -------\n        expanded : same type as caller\n            This object, but with an additional dimension(s).\n        """"""\n        if isinstance(dim, int):\n            raise TypeError(""dim should be hashable or sequence/mapping of hashables"")\n        elif isinstance(dim, Sequence) and not isinstance(dim, str):\n            if len(dim) != len(set(dim)):\n                raise ValueError(""dims should not contain duplicate values."")\n            dim = dict.fromkeys(dim, 1)\n        elif dim is not None and not isinstance(dim, Mapping):\n            dim = {cast(Hashable, dim): 1}\n\n        dim = either_dict_or_kwargs(dim, dim_kwargs, ""expand_dims"")\n        ds = self._to_temp_dataset().expand_dims(dim, axis)\n        return self._from_temp_dataset(ds)\n\n    def set_index(\n        self,\n        indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,\n        append: bool = False,\n        inplace: bool = None,\n        **indexes_kwargs: Union[Hashable, Sequence[Hashable]],\n    ) -> Optional[""DataArray""]:\n        """"""Set DataArray (multi-)indexes using one or more existing\n        coordinates.\n\n        Parameters\n        ----------\n        indexes : {dim: index, ...}\n            Mapping from names matching dimensions and values given\n            by (lists of) the names of existing coordinates or variables to set\n            as new (multi-)index.\n        append : bool, optional\n            If True, append the supplied index(es) to the existing index(es).\n            Otherwise replace the existing index(es) (default).\n        **indexes_kwargs: optional\n            The keyword arguments form of ``indexes``.\n            One of indexes or indexes_kwargs must be provided.\n\n        Returns\n        -------\n        obj : DataArray\n            Another DataArray, with this data but replaced coordinates.\n\n        Examples\n        --------\n        >>> arr = xr.DataArray(\n        ...     data=np.ones((2, 3)),\n        ...     dims=[""x"", ""y""],\n        ...     coords={""x"": range(2), ""y"": range(3), ""a"": (""x"", [3, 4])},\n        ... )\n        >>> arr\n        <xarray.DataArray (x: 2, y: 3)>\n        array([[1., 1., 1.],\n               [1., 1., 1.]])\n        Coordinates:\n          * x        (x) int64 0 1\n          * y        (y) int64 0 1 2\n            a        (x) int64 3 4\n        >>> arr.set_index(x=""a"")\n        <xarray.DataArray (x: 2, y: 3)>\n        array([[1., 1., 1.],\n               [1., 1., 1.]])\n        Coordinates:\n          * x        (x) int64 3 4\n          * y        (y) int64 0 1 2\n\n        See Also\n        --------\n        DataArray.reset_index\n        """"""\n        ds = self._to_temp_dataset().set_index(\n            indexes, append=append, inplace=inplace, **indexes_kwargs\n        )\n        return self._from_temp_dataset(ds)\n\n    def reset_index(\n        self,\n        dims_or_levels: Union[Hashable, Sequence[Hashable]],\n        drop: bool = False,\n        inplace: bool = None,\n    ) -> Optional[""DataArray""]:\n        """"""Reset the specified index(es) or multi-index level(s).\n\n        Parameters\n        ----------\n        dims_or_levels : hashable or sequence of hashables\n            Name(s) of the dimension(s) and/or multi-index level(s) that will\n            be reset.\n        drop : bool, optional\n            If True, remove the specified indexes and/or multi-index levels\n            instead of extracting them as new coordinates (default: False).\n\n        Returns\n        -------\n        obj : DataArray\n            Another dataarray, with this dataarray\'s data but replaced\n            coordinates.\n\n        See Also\n        --------\n        DataArray.set_index\n        """"""\n        _check_inplace(inplace)\n        coords, _ = split_indexes(\n            dims_or_levels, self._coords, set(), self._level_coords, drop=drop\n        )\n        return self._replace(coords=coords)\n\n    def reorder_levels(\n        self,\n        dim_order: Mapping[Hashable, Sequence[int]] = None,\n        inplace: bool = None,\n        **dim_order_kwargs: Sequence[int],\n    ) -> ""DataArray"":\n        """"""Rearrange index levels using input order.\n\n        Parameters\n        ----------\n        dim_order : optional\n            Mapping from names matching dimensions and values given\n            by lists representing new level orders. Every given dimension\n            must have a multi-index.\n        **dim_order_kwargs: optional\n            The keyword arguments form of ``dim_order``.\n            One of dim_order or dim_order_kwargs must be provided.\n\n        Returns\n        -------\n        obj : DataArray\n            Another dataarray, with this dataarray\'s data but replaced\n            coordinates.\n        """"""\n        _check_inplace(inplace)\n        dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, ""reorder_levels"")\n        replace_coords = {}\n        for dim, order in dim_order.items():\n            coord = self._coords[dim]\n            index = coord.to_index()\n            if not isinstance(index, pd.MultiIndex):\n                raise ValueError(""coordinate %r has no MultiIndex"" % dim)\n            replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))\n        coords = self._coords.copy()\n        coords.update(replace_coords)\n        return self._replace(coords=coords)\n\n    def stack(\n        self,\n        dimensions: Mapping[Hashable, Sequence[Hashable]] = None,\n        **dimensions_kwargs: Sequence[Hashable],\n    ) -> ""DataArray"":\n        """"""\n        Stack any number of existing dimensions into a single new dimension.\n\n        New dimensions will be added at the end, and the corresponding\n        coordinate variables will be combined into a MultiIndex.\n\n        Parameters\n        ----------\n        dimensions : Mapping of the form new_name=(dim1, dim2, ...)\n            Names of new dimensions, and the existing dimensions that they\n            replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.\n            Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over\n            all dimensions.\n        **dimensions_kwargs:\n            The keyword arguments form of ``dimensions``.\n            One of dimensions or dimensions_kwargs must be provided.\n\n        Returns\n        -------\n        stacked : DataArray\n            DataArray with stacked data.\n\n        Examples\n        --------\n\n        >>> arr = xr.DataArray(\n        ...     np.arange(6).reshape(2, 3),\n        ...     coords=[(""x"", [""a"", ""b""]), (""y"", [0, 1, 2])],\n        ... )\n        >>> arr\n        <xarray.DataArray (x: 2, y: 3)>\n        array([[0, 1, 2],\n               [3, 4, 5]])\n        Coordinates:\n          * x        (x) |S1 \'a\' \'b\'\n          * y        (y) int64 0 1 2\n        >>> stacked = arr.stack(z=(""x"", ""y""))\n        >>> stacked.indexes[""z""]\n        MultiIndex(levels=[[\'a\', \'b\'], [0, 1, 2]],\n                   codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\n                   names=[\'x\', \'y\'])\n\n        See Also\n        --------\n        DataArray.unstack\n        """"""\n        ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)\n        return self._from_temp_dataset(ds)\n\n    def unstack(\n        self,\n        dim: Union[Hashable, Sequence[Hashable], None] = None,\n        fill_value: Any = dtypes.NA,\n        sparse: bool = False,\n    ) -> ""DataArray"":\n        """"""\n        Unstack existing dimensions corresponding to MultiIndexes into\n        multiple new dimensions.\n\n        New dimensions will be added at the end.\n\n        Parameters\n        ----------\n        dim : hashable or sequence of hashable, optional\n            Dimension(s) over which to unstack. By default unstacks all\n            MultiIndexes.\n        fill_value: value to be filled. By default, np.nan\n        sparse: use sparse-array if True\n\n        Returns\n        -------\n        unstacked : DataArray\n            Array with unstacked data.\n\n        Examples\n        --------\n\n        >>> arr = xr.DataArray(\n        ...     np.arange(6).reshape(2, 3),\n        ...     coords=[(""x"", [""a"", ""b""]), (""y"", [0, 1, 2])],\n        ... )\n        >>> arr\n        <xarray.DataArray (x: 2, y: 3)>\n        array([[0, 1, 2],\n               [3, 4, 5]])\n        Coordinates:\n          * x        (x) |S1 \'a\' \'b\'\n          * y        (y) int64 0 1 2\n        >>> stacked = arr.stack(z=(""x"", ""y""))\n        >>> stacked.indexes[""z""]\n        MultiIndex(levels=[[\'a\', \'b\'], [0, 1, 2]],\n                   codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\n                   names=[\'x\', \'y\'])\n        >>> roundtripped = stacked.unstack()\n        >>> arr.identical(roundtripped)\n        True\n\n        See Also\n        --------\n        DataArray.stack\n        """"""\n        ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)\n        return self._from_temp_dataset(ds)\n\n    def to_unstacked_dataset(self, dim, level=0):\n        """"""Unstack DataArray expanding to Dataset along a given level of a\n        stacked coordinate.\n\n        This is the inverse operation of Dataset.to_stacked_array.\n\n        Parameters\n        ----------\n        dim : str\n            Name of existing dimension to unstack\n        level : int or str\n            The MultiIndex level to expand to a dataset along. Can either be\n            the integer index of the level or its name.\n        label : int, default 0\n            Label of the level to expand dataset along. Overrides the label\n            argument if given.\n\n        Returns\n        -------\n        unstacked: Dataset\n\n        Examples\n        --------\n        >>> import xarray as xr\n        >>> arr = xr.DataArray(\n        ...     np.arange(6).reshape(2, 3),\n        ...     coords=[(""x"", [""a"", ""b""]), (""y"", [0, 1, 2])],\n        ... )\n        >>> data = xr.Dataset({""a"": arr, ""b"": arr.isel(y=0)})\n        >>> data\n        <xarray.Dataset>\n        Dimensions:  (x: 2, y: 3)\n        Coordinates:\n          * x        (x) <U1 \'a\' \'b\'\n          * y        (y) int64 0 1 2\n        Data variables:\n            a        (x, y) int64 0 1 2 3 4 5\n            b        (x) int64 0 3\n        >>> stacked = data.to_stacked_array(""z"", [""y""])\n        >>> stacked.indexes[""z""]\n        MultiIndex(levels=[[\'a\', \'b\'], [0, 1, 2]],\n                labels=[[0, 0, 0, 1], [0, 1, 2, -1]],\n                names=[\'variable\', \'y\'])\n        >>> roundtripped = stacked.to_unstacked_dataset(dim=""z"")\n        >>> data.identical(roundtripped)\n        True\n\n        See Also\n        --------\n        Dataset.to_stacked_array\n        """"""\n\n        idx = self.indexes[dim]\n        if not isinstance(idx, pd.MultiIndex):\n            raise ValueError(f""\'{dim}\' is not a stacked coordinate"")\n\n        level_number = idx._get_level_number(level)\n        variables = idx.levels[level_number]\n        variable_dim = idx.names[level_number]\n\n        # pull variables out of datarray\n        data_dict = {}\n        for k in variables:\n            data_dict[k] = self.sel({variable_dim: k}).squeeze(drop=True)\n\n        # unstacked dataset\n        return Dataset(data_dict)\n\n    def transpose(self, *dims: Hashable, transpose_coords: bool = True) -> ""DataArray"":\n        """"""Return a new DataArray object with transposed dimensions.\n\n        Parameters\n        ----------\n        *dims : hashable, optional\n            By default, reverse the dimensions. Otherwise, reorder the\n            dimensions to this order.\n        transpose_coords : boolean, default True\n            If True, also transpose the coordinates of this DataArray.\n\n        Returns\n        -------\n        transposed : DataArray\n            The returned DataArray\'s array is transposed.\n\n        Notes\n        -----\n        This operation returns a view of this array\'s data. It is\n        lazy for dask-backed DataArrays but not for numpy-backed DataArrays\n        -- the data will be fully loaded.\n\n        See Also\n        --------\n        numpy.transpose\n        Dataset.transpose\n        """"""\n        if dims:\n            dims = tuple(utils.infix_dims(dims, self.dims))\n        variable = self.variable.transpose(*dims)\n        if transpose_coords:\n            coords: Dict[Hashable, Variable] = {}\n            for name, coord in self.coords.items():\n                coord_dims = tuple(dim for dim in dims if dim in coord.dims)\n                coords[name] = coord.variable.transpose(*coord_dims)\n            return self._replace(variable, coords)\n        else:\n            return self._replace(variable)\n\n    @property\n    def T(self) -> ""DataArray"":\n        return self.transpose()\n\n    def drop_vars(\n        self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = ""raise""\n    ) -> ""DataArray"":\n        """"""Drop variables from this DataArray.\n\n        Parameters\n        ----------\n        names : hashable or iterable of hashables\n            Name(s) of variables to drop.\n        errors: {\'raise\', \'ignore\'}, optional\n            If \'raise\' (default), raises a ValueError error if any of the variable\n            passed are not in the dataset. If \'ignore\', any given names that are in the\n            DataArray are dropped and no error is raised.\n\n        Returns\n        -------\n        dropped : Dataset\n\n        """"""\n        ds = self._to_temp_dataset().drop_vars(names, errors=errors)\n        return self._from_temp_dataset(ds)\n\n    def drop(\n        self,\n        labels: Mapping = None,\n        dim: Hashable = None,\n        *,\n        errors: str = ""raise"",\n        **labels_kwargs,\n    ) -> ""DataArray"":\n        """"""Backward compatible method based on `drop_vars` and `drop_sel`\n\n        Using either `drop_vars` or `drop_sel` is encouraged\n\n        See Also\n        --------\n        DataArray.drop_vars\n        DataArray.drop_sel\n        """"""\n        ds = self._to_temp_dataset().drop(labels, dim, errors=errors)\n        return self._from_temp_dataset(ds)\n\n    def drop_sel(\n        self,\n        labels: Mapping[Hashable, Any] = None,\n        *,\n        errors: str = ""raise"",\n        **labels_kwargs,\n    ) -> ""DataArray"":\n        """"""Drop index labels from this DataArray.\n\n        Parameters\n        ----------\n        labels : Mapping[Hashable, Any]\n            Index labels to drop\n        errors: {\'raise\', \'ignore\'}, optional\n            If \'raise\' (default), raises a ValueError error if\n            any of the index labels passed are not\n            in the dataset. If \'ignore\', any given labels that are in the\n            dataset are dropped and no error is raised.\n        **labels_kwargs : {dim: label, ...}, optional\n            The keyword arguments form of ``dim`` and ``labels``\n\n        Returns\n        -------\n        dropped : DataArray\n        """"""\n        if labels_kwargs or isinstance(labels, dict):\n            labels = either_dict_or_kwargs(labels, labels_kwargs, ""drop"")\n\n        ds = self._to_temp_dataset().drop_sel(labels, errors=errors)\n        return self._from_temp_dataset(ds)\n\n    def dropna(\n        self, dim: Hashable, how: str = ""any"", thresh: int = None\n    ) -> ""DataArray"":\n        """"""Returns a new array with dropped labels for missing values along\n        the provided dimension.\n\n        Parameters\n        ----------\n        dim : hashable\n            Dimension along which to drop missing values. Dropping along\n            multiple dimensions simultaneously is not yet supported.\n        how : {\'any\', \'all\'}, optional\n            * any : if any NA values are present, drop that label\n            * all : if all values are NA, drop that label\n        thresh : int, default None\n            If supplied, require this many non-NA values.\n\n        Returns\n        -------\n        DataArray\n        """"""\n        ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)\n        return self._from_temp_dataset(ds)\n\n    def fillna(self, value: Any) -> ""DataArray"":\n        """"""Fill missing values in this object.\n\n        This operation follows the normal broadcasting and alignment rules that\n        xarray uses for binary arithmetic, except the result is aligned to this\n        object (``join=\'left\'``) instead of aligned to the intersection of\n        index coordinates (``join=\'inner\'``).\n\n        Parameters\n        ----------\n        value : scalar, ndarray or DataArray\n            Used to fill all matching missing values in this array. If the\n            argument is a DataArray, it is first aligned with (reindexed to)\n            this array.\n\n        Returns\n        -------\n        DataArray\n        """"""\n        if utils.is_dict_like(value):\n            raise TypeError(\n                ""cannot provide fill value as a dictionary with ""\n                ""fillna on a DataArray""\n            )\n        out = ops.fillna(self, value)\n        return out\n\n    def interpolate_na(\n        self,\n        dim: Hashable = None,\n        method: str = ""linear"",\n        limit: int = None,\n        use_coordinate: Union[bool, str] = True,\n        max_gap: Union[\n            int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta\n        ] = None,\n        keep_attrs: bool = None,\n        **kwargs: Any,\n    ) -> ""DataArray"":\n        """"""Fill in NaNs by interpolating according to different methods.\n\n        Parameters\n        ----------\n        dim : str\n            Specifies the dimension along which to interpolate.\n        method : str, optional\n            String indicating which method to use for interpolation:\n\n            - \'linear\': linear interpolation (Default). Additional keyword\n              arguments are passed to :py:func:`numpy.interp`\n            - \'nearest\', \'zero\', \'slinear\', \'quadratic\', \'cubic\', \'polynomial\':\n              are passed to :py:func:`scipy.interpolate.interp1d`. If\n              ``method=\'polynomial\'``, the ``order`` keyword argument must also be\n              provided.\n            - \'barycentric\', \'krog\', \'pchip\', \'spline\', \'akima\': use their\n              respective :py:class:`scipy.interpolate` classes.\n\n        use_coordinate : bool, str, default True\n            Specifies which index to use as the x values in the interpolation\n            formulated as `y = f(x)`. If False, values are treated as if\n            eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is\n            used. If ``use_coordinate`` is a string, it specifies the name of a\n            coordinate variariable to use as the index.\n        limit : int, default None\n            Maximum number of consecutive NaNs to fill. Must be greater than 0\n            or None for no limit. This filling is done regardless of the size of\n            the gap in the data. To only interpolate over gaps less than a given length,\n            see ``max_gap``.\n        max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default None.\n            Maximum size of gap, a continuous sequence of NaNs, that will be filled.\n            Use None for no limit. When interpolating along a datetime64 dimension\n            and ``use_coordinate=True``, ``max_gap`` can be one of the following:\n\n            - a string that is valid input for pandas.to_timedelta\n            - a :py:class:`numpy.timedelta64` object\n            - a :py:class:`pandas.Timedelta` object\n            - a :py:class:`datetime.timedelta` object\n\n            Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled\n            dimensions has not been implemented yet. Gap length is defined as the difference\n            between coordinate values at the first data point after a gap and the last value\n            before a gap. For gaps at the beginning (end), gap length is defined as the difference\n            between coordinate values at the first (last) valid data point and the first (last) NaN.\n            For example, consider::\n\n                <xarray.DataArray (x: 9)>\n                array([nan, nan, nan,  1., nan, nan,  4., nan, nan])\n                Coordinates:\n                  * x        (x) int64 0 1 2 3 4 5 6 7 8\n\n            The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively\n        keep_attrs : bool, default True\n            If True, the dataarray\'s attributes (`attrs`) will be copied from\n            the original object to the new one.  If False, the new\n            object will be returned without attributes.\n        kwargs : dict, optional\n            parameters passed verbatim to the underlying interpolation function\n\n        Returns\n        -------\n        interpolated: DataArray\n            Filled in DataArray.\n\n        See also\n        --------\n        numpy.interp\n        scipy.interpolate\n        """"""\n        from .missing import interp_na\n\n        return interp_na(\n            self,\n            dim=dim,\n            method=method,\n            limit=limit,\n            use_coordinate=use_coordinate,\n            max_gap=max_gap,\n            keep_attrs=keep_attrs,\n            **kwargs,\n        )\n\n    def ffill(self, dim: Hashable, limit: int = None) -> ""DataArray"":\n        """"""Fill NaN values by propogating values forward\n\n        *Requires bottleneck.*\n\n        Parameters\n        ----------\n        dim : hashable\n            Specifies the dimension along which to propagate values when\n            filling.\n        limit : int, default None\n            The maximum number of consecutive NaN values to forward fill. In\n            other words, if there is a gap with more than this number of\n            consecutive NaNs, it will only be partially filled. Must be greater\n            than 0 or None for no limit.\n\n        Returns\n        -------\n        DataArray\n        """"""\n        from .missing import ffill\n\n        return ffill(self, dim, limit=limit)\n\n    def bfill(self, dim: Hashable, limit: int = None) -> ""DataArray"":\n        """"""Fill NaN values by propogating values backward\n\n        *Requires bottleneck.*\n\n        Parameters\n        ----------\n        dim : str\n            Specifies the dimension along which to propagate values when\n            filling.\n        limit : int, default None\n            The maximum number of consecutive NaN values to backward fill. In\n            other words, if there is a gap with more than this number of\n            consecutive NaNs, it will only be partially filled. Must be greater\n            than 0 or None for no limit.\n\n        Returns\n        -------\n        DataArray\n        """"""\n        from .missing import bfill\n\n        return bfill(self, dim, limit=limit)\n\n    def combine_first(self, other: ""DataArray"") -> ""DataArray"":\n        """"""Combine two DataArray objects, with union of coordinates.\n\n        This operation follows the normal broadcasting and alignment rules of\n        ``join=\'outer\'``.  Default to non-null values of array calling the\n        method.  Use np.nan to fill in vacant cells after alignment.\n\n        Parameters\n        ----------\n        other : DataArray\n            Used to fill all matching missing values in this array.\n\n        Returns\n        -------\n        DataArray\n        """"""\n        return ops.fillna(self, other, join=""outer"")\n\n    def reduce(\n        self,\n        func: Callable[..., Any],\n        dim: Union[None, Hashable, Sequence[Hashable]] = None,\n        axis: Union[None, int, Sequence[int]] = None,\n        keep_attrs: bool = None,\n        keepdims: bool = False,\n        **kwargs: Any,\n    ) -> ""DataArray"":\n        """"""Reduce this array by applying `func` along some dimension(s).\n\n        Parameters\n        ----------\n        func : function\n            Function which can be called in the form\n            `f(x, axis=axis, **kwargs)` to return the result of reducing an\n            np.ndarray over an integer valued axis.\n        dim : hashable or sequence of hashables, optional\n            Dimension(s) over which to apply `func`.\n        axis : int or sequence of int, optional\n            Axis(es) over which to repeatedly apply `func`. Only one of the\n            \'dim\' and \'axis\' arguments can be supplied. If neither are\n            supplied, then the reduction is calculated over the flattened array\n            (by calling `f(x)` without an axis argument).\n        keep_attrs : bool, optional\n            If True, the variable\'s attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n        keepdims : bool, default False\n            If True, the dimensions which are reduced are left in the result\n            as dimensions of size one. Coordinates that use these dimensions\n            are removed.\n        **kwargs : dict\n            Additional keyword arguments passed on to `func`.\n\n        Returns\n        -------\n        reduced : DataArray\n            DataArray with this object\'s array replaced with an array with\n            summarized data and the indicated dimension(s) removed.\n        """"""\n\n        var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)\n        return self._replace_maybe_drop_dims(var)\n\n    def to_pandas(self) -> Union[""DataArray"", pd.Series, pd.DataFrame]:\n        """"""Convert this array into a pandas object with the same shape.\n\n        The type of the returned object depends on the number of DataArray\n        dimensions:\n\n        * 0D -> `xarray.DataArray`\n        * 1D -> `pandas.Series`\n        * 2D -> `pandas.DataFrame`\n\n        Only works for arrays with 2 or fewer dimensions.\n\n        The DataArray constructor performs the inverse transformation.\n        """"""\n        # TODO: consolidate the info about pandas constructors and the\n        # attributes that correspond to their indexes into a separate module?\n        constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}\n        try:\n            constructor = constructors[self.ndim]\n        except KeyError:\n            raise ValueError(\n                ""cannot convert arrays with %s dimensions into ""\n                ""pandas objects"" % self.ndim\n            )\n        indexes = [self.get_index(dim) for dim in self.dims]\n        return constructor(self.values, *indexes)\n\n    def to_dataframe(self, name: Hashable = None) -> pd.DataFrame:\n        """"""Convert this array and its coordinates into a tidy pandas.DataFrame.\n\n        The DataFrame is indexed by the Cartesian product of index coordinates\n        (in the form of a :py:class:`pandas.MultiIndex`).\n\n        Other coordinates are included as columns in the DataFrame.\n        """"""\n        if name is None:\n            name = self.name\n        if name is None:\n            raise ValueError(\n                ""cannot convert an unnamed DataArray to a ""\n                ""DataFrame: use the ``name`` parameter""\n            )\n\n        dims = dict(zip(self.dims, self.shape))\n        # By using a unique name, we can convert a DataArray into a DataFrame\n        # even if it shares a name with one of its coordinates.\n        # I would normally use unique_name = object() but that results in a\n        # dataframe with columns in the wrong order, for reasons I have not\n        # been able to debug (possibly a pandas bug?).\n        unique_name = ""__unique_name_identifier_z98xfz98xugfg73ho__""\n        ds = self._to_dataset_whole(name=unique_name)\n        df = ds._to_dataframe(dims)\n        df.columns = [name if c == unique_name else c for c in df.columns]\n        return df\n\n    def to_series(self) -> pd.Series:\n        """"""Convert this array into a pandas.Series.\n\n        The Series is indexed by the Cartesian product of index coordinates\n        (in the form of a :py:class:`pandas.MultiIndex`).\n        """"""\n        index = self.coords.to_index()\n        return pd.Series(self.values.reshape(-1), index=index, name=self.name)\n\n    def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:\n        """"""Convert this array into a numpy.ma.MaskedArray\n\n        Parameters\n        ----------\n        copy : bool\n            If True (default) make a copy of the array in the result. If False,\n            a MaskedArray view of DataArray.values is returned.\n\n        Returns\n        -------\n        result : MaskedArray\n            Masked where invalid values (nan or inf) occur.\n        """"""\n        values = self.values  # only compute lazy arrays once\n        isnull = pd.isnull(values)\n        return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)\n\n    def to_netcdf(self, *args, **kwargs) -> Union[bytes, ""Delayed"", None]:\n        """"""Write DataArray contents to a netCDF file.\n\n        All parameters are passed directly to `xarray.Dataset.to_netcdf`.\n\n        Notes\n        -----\n        Only xarray.Dataset objects can be written to netCDF files, so\n        the xarray.DataArray is converted to a xarray.Dataset object\n        containing a single variable. If the DataArray has no name, or if the\n        name is the same as a co-ordinate name, then it is given the name\n        \'__xarray_dataarray_variable__\'.\n        """"""\n        from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE\n\n        if self.name is None:\n            # If no name is set then use a generic xarray name\n            dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n        elif self.name in self.coords or self.name in self.dims:\n            # The name is the same as one of the coords names, which netCDF\n            # doesn\'t support, so rename it but keep track of the old name\n            dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n            dataset.attrs[DATAARRAY_NAME] = self.name\n        else:\n            # No problems with the name - so we\'re fine!\n            dataset = self.to_dataset()\n\n        return dataset.to_netcdf(*args, **kwargs)\n\n    def to_dict(self, data: bool = True) -> dict:\n        """"""\n        Convert this xarray.DataArray into a dictionary following xarray\n        naming conventions.\n\n        Converts all variables and attributes to native Python objects.\n        Useful for converting to json. To avoid datetime incompatibility\n        use decode_times=False kwarg in xarrray.open_dataset.\n\n        Parameters\n        ----------\n        data : bool, optional\n            Whether to include the actual data in the dictionary. When set to\n            False, returns just the schema.\n\n        See also\n        --------\n        DataArray.from_dict\n        """"""\n        d = self.variable.to_dict(data=data)\n        d.update({""coords"": {}, ""name"": self.name})\n        for k in self.coords:\n            d[""coords""][k] = self.coords[k].variable.to_dict(data=data)\n        return d\n\n    @classmethod\n    def from_dict(cls, d: dict) -> ""DataArray"":\n        """"""\n        Convert a dictionary into an xarray.DataArray\n\n        Input dict can take several forms::\n\n            d = {\'dims\': (\'t\'), \'data\': x}\n\n            d = {\'coords\': {\'t\': {\'dims\': \'t\', \'data\': t,\n                                  \'attrs\': {\'units\':\'s\'}}},\n                 \'attrs\': {\'title\': \'air temperature\'},\n                 \'dims\': \'t\',\n                 \'data\': x,\n                 \'name\': \'a\'}\n\n        where \'t\' is the name of the dimesion, \'a\' is the name of the array,\n        and  x and t are lists, numpy.arrays, or pandas objects.\n\n        Parameters\n        ----------\n        d : dict, with a minimum structure of {\'dims\': [..], \'data\': [..]}\n\n        Returns\n        -------\n        obj : xarray.DataArray\n\n        See also\n        --------\n        DataArray.to_dict\n        Dataset.from_dict\n        """"""\n        coords = None\n        if ""coords"" in d:\n            try:\n                coords = {\n                    k: (v[""dims""], v[""data""], v.get(""attrs""))\n                    for k, v in d[""coords""].items()\n                }\n            except KeyError as e:\n                raise ValueError(\n                    ""cannot convert dict when coords are missing the key ""\n                    ""\'{dims_data}\'"".format(dims_data=str(e.args[0]))\n                )\n        try:\n            data = d[""data""]\n        except KeyError:\n            raise ValueError(""cannot convert dict without the key \'data\'\'"")\n        else:\n            obj = cls(data, coords, d.get(""dims""), d.get(""name""), d.get(""attrs""))\n        return obj\n\n    @classmethod\n    def from_series(cls, series: pd.Series, sparse: bool = False) -> ""DataArray"":\n        """"""Convert a pandas.Series into an xarray.DataArray.\n\n        If the series\'s index is a MultiIndex, it will be expanded into a\n        tensor product of one-dimensional coordinates (filling in missing\n        values with NaN). Thus this operation should be the inverse of the\n        `to_series` method.\n\n        If sparse=True, creates a sparse array instead of a dense NumPy array.\n        Requires the pydata/sparse package.\n\n        See also\n        --------\n        xarray.Dataset.from_dataframe\n        """"""\n        temp_name = ""__temporary_name""\n        df = pd.DataFrame({temp_name: series})\n        ds = Dataset.from_dataframe(df, sparse=sparse)\n        result = cast(DataArray, ds[temp_name])\n        result.name = series.name\n        return result\n\n    def to_cdms2(self) -> ""cdms2_Variable"":\n        """"""Convert this array into a cdms2.Variable\n        """"""\n        from ..convert import to_cdms2\n\n        return to_cdms2(self)\n\n    @classmethod\n    def from_cdms2(cls, variable: ""cdms2_Variable"") -> ""DataArray"":\n        """"""Convert a cdms2.Variable into an xarray.DataArray\n        """"""\n        from ..convert import from_cdms2\n\n        return from_cdms2(variable)\n\n    def to_iris(self) -> ""iris_Cube"":\n        """"""Convert this array into a iris.cube.Cube\n        """"""\n        from ..convert import to_iris\n\n        return to_iris(self)\n\n    @classmethod\n    def from_iris(cls, cube: ""iris_Cube"") -> ""DataArray"":\n        """"""Convert a iris.cube.Cube into an xarray.DataArray\n        """"""\n        from ..convert import from_iris\n\n        return from_iris(cube)\n\n    def _all_compat(self, other: ""DataArray"", compat_str: str) -> bool:\n        """"""Helper function for equals, broadcast_equals, and identical\n        """"""\n\n        def compat(x, y):\n            return getattr(x.variable, compat_str)(y.variable)\n\n        return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(\n            self, other\n        )\n\n    def broadcast_equals(self, other: ""DataArray"") -> bool:\n        """"""Two DataArrays are broadcast equal if they are equal after\n        broadcasting them against each other such that they have the same\n        dimensions.\n\n        See Also\n        --------\n        DataArray.equals\n        DataArray.identical\n        """"""\n        try:\n            return self._all_compat(other, ""broadcast_equals"")\n        except (TypeError, AttributeError):\n            return False\n\n    def equals(self, other: ""DataArray"") -> bool:\n        """"""True if two DataArrays have the same dimensions, coordinates and\n        values; otherwise False.\n\n        DataArrays can still be equal (like pandas objects) if they have NaN\n        values in the same locations.\n\n        This method is necessary because `v1 == v2` for ``DataArray``\n        does element-wise comparisons (like numpy.ndarrays).\n\n        See Also\n        --------\n        DataArray.broadcast_equals\n        DataArray.identical\n        """"""\n        try:\n            return self._all_compat(other, ""equals"")\n        except (TypeError, AttributeError):\n            return False\n\n    def identical(self, other: ""DataArray"") -> bool:\n        """"""Like equals, but also checks the array name and attributes, and\n        attributes on all coordinates.\n\n        See Also\n        --------\n        DataArray.broadcast_equals\n        DataArray.equal\n        """"""\n        try:\n            return self.name == other.name and self._all_compat(other, ""identical"")\n        except (TypeError, AttributeError):\n            return False\n\n    def _result_name(self, other: Any = None) -> Optional[Hashable]:\n        # use the same naming heuristics as pandas:\n        # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356\n        other_name = getattr(other, ""name"", _default)\n        if other_name is _default or other_name == self.name:\n            return self.name\n        else:\n            return None\n\n    def __array_wrap__(self, obj, context=None) -> ""DataArray"":\n        new_var = self.variable.__array_wrap__(obj, context)\n        return self._replace(new_var)\n\n    def __matmul__(self, obj):\n        return self.dot(obj)\n\n    def __rmatmul__(self, other):\n        # currently somewhat duplicative, as only other DataArrays are\n        # compatible with matmul\n        return computation.dot(other, self)\n\n    @staticmethod\n    def _unary_op(f: Callable[..., Any]) -> Callable[..., ""DataArray""]:\n        @functools.wraps(f)\n        def func(self, *args, **kwargs):\n            with np.errstate(all=""ignore""):\n                return self.__array_wrap__(f(self.variable.data, *args, **kwargs))\n\n        return func\n\n    @staticmethod\n    def _binary_op(\n        f: Callable[..., Any],\n        reflexive: bool = False,\n        join: str = None,  # see xarray.align\n        **ignored_kwargs,\n    ) -> Callable[..., ""DataArray""]:\n        @functools.wraps(f)\n        def func(self, other):\n            if isinstance(other, (Dataset, groupby.GroupBy)):\n                return NotImplemented\n            if isinstance(other, DataArray):\n                align_type = OPTIONS[""arithmetic_join""] if join is None else join\n                self, other = align(self, other, join=align_type, copy=False)\n            other_variable = getattr(other, ""variable"", other)\n            other_coords = getattr(other, ""coords"", None)\n\n            variable = (\n                f(self.variable, other_variable)\n                if not reflexive\n                else f(other_variable, self.variable)\n            )\n            coords, indexes = self.coords._merge_raw(other_coords)\n            name = self._result_name(other)\n\n            return self._replace(variable, coords, name, indexes=indexes)\n\n        return func\n\n    @staticmethod\n    def _inplace_binary_op(f: Callable) -> Callable[..., ""DataArray""]:\n        @functools.wraps(f)\n        def func(self, other):\n            if isinstance(other, groupby.GroupBy):\n                raise TypeError(\n                    ""in-place operations between a DataArray and ""\n                    ""a grouped object are not permitted""\n                )\n            # n.b. we can\'t align other to self (with other.reindex_like(self))\n            # because `other` may be converted into floats, which would cause\n            # in-place arithmetic to fail unpredictably. Instead, we simply\n            # don\'t support automatic alignment with in-place arithmetic.\n            other_coords = getattr(other, ""coords"", None)\n            other_variable = getattr(other, ""variable"", other)\n            with self.coords._merge_inplace(other_coords):\n                f(self.variable, other_variable)\n            return self\n\n        return func\n\n    def _copy_attrs_from(self, other: Union[""DataArray"", Dataset, Variable]) -> None:\n        self.attrs = other.attrs\n\n    @property\n    def plot(self) -> _PlotMethods:\n        """"""\n        Access plotting functions for DataArray\'s\n\n        >>> d = xr.DataArray([[1, 2], [3, 4]])\n\n        For convenience just call this directly\n\n        >>> d.plot()\n\n        Or use it as a namespace to use xarray.plot functions as\n        DataArray methods\n\n        >>> d.plot.imshow()  # equivalent to xarray.plot.imshow(d)\n\n        """"""\n        return _PlotMethods(self)\n\n    def _title_for_slice(self, truncate: int = 50) -> str:\n        """"""\n        If the dataarray has 1 dimensional coordinates or comes from a slice\n        we can show that info in the title\n\n        Parameters\n        ----------\n        truncate : integer\n            maximum number of characters for title\n\n        Returns\n        -------\n        title : string\n            Can be used for plot titles\n\n        """"""\n        one_dims = []\n        for dim, coord in self.coords.items():\n            if coord.size == 1:\n                one_dims.append(\n                    ""{dim} = {v}"".format(dim=dim, v=format_item(coord.values))\n                )\n\n        title = "", "".join(one_dims)\n        if len(title) > truncate:\n            title = title[: (truncate - 3)] + ""...""\n\n        return title\n\n    def diff(self, dim: Hashable, n: int = 1, label: Hashable = ""upper"") -> ""DataArray"":\n        """"""Calculate the n-th order discrete difference along given axis.\n\n        Parameters\n        ----------\n        dim : hashable\n            Dimension over which to calculate the finite difference.\n        n : int, optional\n            The number of times values are differenced.\n        label : hashable, optional\n            The new coordinate in dimension ``dim`` will have the\n            values of either the minuend\'s or subtrahend\'s coordinate\n            for values \'upper\' and \'lower\', respectively.  Other\n            values are not supported.\n\n        Returns\n        -------\n        difference : same type as caller\n            The n-th order finite difference of this object.\n\n        .. note::\n\n            `n` matches numpy\'s behavior and is different from pandas\' first\n            argument named `periods`.\n\n\n        Examples\n        --------\n        >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], [""x""])\n        >>> arr.diff(""x"")\n        <xarray.DataArray (x: 3)>\n        array([0, 1, 0])\n        Coordinates:\n        * x        (x) int64 2 3 4\n        >>> arr.diff(""x"", 2)\n        <xarray.DataArray (x: 2)>\n        array([ 1, -1])\n        Coordinates:\n        * x        (x) int64 3 4\n\n        See Also\n        --------\n        DataArray.differentiate\n        """"""\n        ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)\n        return self._from_temp_dataset(ds)\n\n    def shift(\n        self,\n        shifts: Mapping[Hashable, int] = None,\n        fill_value: Any = dtypes.NA,\n        **shifts_kwargs: int,\n    ) -> ""DataArray"":\n        """"""Shift this array by an offset along one or more dimensions.\n\n        Only the data is moved; coordinates stay in place. Values shifted from\n        beyond array bounds are replaced by NaN. This is consistent with the\n        behavior of ``shift`` in pandas.\n\n        Parameters\n        ----------\n        shifts : Mapping with the form of {dim: offset}\n            Integer offset to shift along each of the given dimensions.\n            Positive offsets shift to the right; negative offsets shift to the\n            left.\n        fill_value: scalar, optional\n            Value to use for newly missing values\n        **shifts_kwargs:\n            The keyword arguments form of ``shifts``.\n            One of shifts or shifts_kwargs must be provided.\n\n        Returns\n        -------\n        shifted : DataArray\n            DataArray with the same coordinates and attributes but shifted\n            data.\n\n        See also\n        --------\n        roll\n\n        Examples\n        --------\n\n        >>> arr = xr.DataArray([5, 6, 7], dims=""x"")\n        >>> arr.shift(x=1)\n        <xarray.DataArray (x: 3)>\n        array([ nan,   5.,   6.])\n        Coordinates:\n          * x        (x) int64 0 1 2\n        """"""\n        variable = self.variable.shift(\n            shifts=shifts, fill_value=fill_value, **shifts_kwargs\n        )\n        return self._replace(variable=variable)\n\n    def roll(\n        self,\n        shifts: Mapping[Hashable, int] = None,\n        roll_coords: bool = None,\n        **shifts_kwargs: int,\n    ) -> ""DataArray"":\n        """"""Roll this array by an offset along one or more dimensions.\n\n        Unlike shift, roll may rotate all variables, including coordinates\n        if specified. The direction of rotation is consistent with\n        :py:func:`numpy.roll`.\n\n        Parameters\n        ----------\n        shifts : Mapping with the form of {dim: offset}\n            Integer offset to rotate each of the given dimensions.\n            Positive offsets roll to the right; negative offsets roll to the\n            left.\n        roll_coords : bool\n            Indicates whether to  roll the coordinates by the offset\n            The current default of roll_coords (None, equivalent to True) is\n            deprecated and will change to False in a future version.\n            Explicitly pass roll_coords to silence the warning.\n        **shifts_kwargs : The keyword arguments form of ``shifts``.\n            One of shifts or shifts_kwargs must be provided.\n\n        Returns\n        -------\n        rolled : DataArray\n            DataArray with the same attributes but rolled data and coordinates.\n\n        See also\n        --------\n        shift\n\n        Examples\n        --------\n\n        >>> arr = xr.DataArray([5, 6, 7], dims=""x"")\n        >>> arr.roll(x=1)\n        <xarray.DataArray (x: 3)>\n        array([7, 5, 6])\n        Coordinates:\n          * x        (x) int64 2 0 1\n        """"""\n        ds = self._to_temp_dataset().roll(\n            shifts=shifts, roll_coords=roll_coords, **shifts_kwargs\n        )\n        return self._from_temp_dataset(ds)\n\n    @property\n    def real(self) -> ""DataArray"":\n        return self._replace(self.variable.real)\n\n    @property\n    def imag(self) -> ""DataArray"":\n        return self._replace(self.variable.imag)\n\n    def dot(\n        self, other: ""DataArray"", dims: Union[Hashable, Sequence[Hashable], None] = None\n    ) -> ""DataArray"":\n        """"""Perform dot product of two DataArrays along their shared dims.\n\n        Equivalent to taking taking tensordot over all shared dims.\n\n        Parameters\n        ----------\n        other : DataArray\n            The other array with which the dot product is performed.\n        dims: \'...\', hashable or sequence of hashables, optional\n            Which dimensions to sum over. Ellipsis (\'...\') sums over all dimensions.\n            If not specified, then all the common dimensions are summed over.\n\n        Returns\n        -------\n        result : DataArray\n            Array resulting from the dot product over all shared dimensions.\n\n        See also\n        --------\n        dot\n        numpy.tensordot\n\n        Examples\n        --------\n\n        >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))\n        >>> da = xr.DataArray(da_vals, dims=[""x"", ""y"", ""z""])\n        >>> dm_vals = np.arange(4)\n        >>> dm = xr.DataArray(dm_vals, dims=[""z""])\n\n        >>> dm.dims\n        (\'z\')\n\n        >>> da.dims\n        (\'x\', \'y\', \'z\')\n\n        >>> dot_result = da.dot(dm)\n        >>> dot_result.dims\n        (\'x\', \'y\')\n\n        """"""\n        if isinstance(other, Dataset):\n            raise NotImplementedError(\n                ""dot products are not yet supported with Dataset objects.""\n            )\n        if not isinstance(other, DataArray):\n            raise TypeError(""dot only operates on DataArrays."")\n\n        return computation.dot(self, other, dims=dims)\n\n    def sortby(\n        self,\n        variables: Union[Hashable, ""DataArray"", Sequence[Union[Hashable, ""DataArray""]]],\n        ascending: bool = True,\n    ) -> ""DataArray"":\n        """"""Sort object by labels or values (along an axis).\n\n        Sorts the dataarray, either along specified dimensions,\n        or according to values of 1-D dataarrays that share dimension\n        with calling object.\n\n        If the input variables are dataarrays, then the dataarrays are aligned\n        (via left-join) to the calling object prior to sorting by cell values.\n        NaNs are sorted to the end, following Numpy convention.\n\n        If multiple sorts along the same dimension is\n        given, numpy\'s lexsort is performed along that dimension:\n        https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html\n        and the FIRST key in the sequence is used as the primary sort key,\n        followed by the 2nd key, etc.\n\n        Parameters\n        ----------\n        variables: hashable, DataArray, or sequence of either\n            1D DataArray objects or name(s) of 1D variable(s) in\n            coords whose values are used to sort this array.\n        ascending: boolean, optional\n            Whether to sort by ascending or descending order.\n\n        Returns\n        -------\n        sorted: DataArray\n            A new dataarray where all the specified dims are sorted by dim\n            labels.\n\n        Examples\n        --------\n\n        >>> da = xr.DataArray(\n        ...     np.random.rand(5),\n        ...     coords=[pd.date_range(""1/1/2000"", periods=5)],\n        ...     dims=""time"",\n        ... )\n        >>> da\n        <xarray.DataArray (time: 5)>\n        array([ 0.965471,  0.615637,  0.26532 ,  0.270962,  0.552878])\n        Coordinates:\n          * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n\n        >>> da.sortby(da)\n        <xarray.DataArray (time: 5)>\n        array([ 0.26532 ,  0.270962,  0.552878,  0.615637,  0.965471])\n        Coordinates:\n          * time     (time) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05 ...\n        """"""\n        ds = self._to_temp_dataset().sortby(variables, ascending=ascending)\n        return self._from_temp_dataset(ds)\n\n    def quantile(\n        self,\n        q: Any,\n        dim: Union[Hashable, Sequence[Hashable], None] = None,\n        interpolation: str = ""linear"",\n        keep_attrs: bool = None,\n        skipna: bool = True,\n    ) -> ""DataArray"":\n        """"""Compute the qth quantile of the data along the specified dimension.\n\n        Returns the qth quantiles(s) of the array elements.\n\n        Parameters\n        ----------\n        q : float in range of [0,1] or array-like of floats\n            Quantile to compute, which must be between 0 and 1 inclusive.\n        dim : hashable or sequence of hashable, optional\n            Dimension(s) over which to apply quantile.\n        interpolation : {\'linear\', \'lower\', \'higher\', \'midpoint\', \'nearest\'}\n            This optional parameter specifies the interpolation method to\n            use when the desired quantile lies between two data points\n            ``i < j``:\n\n                - linear: ``i + (j - i) * fraction``, where ``fraction`` is\n                  the fractional part of the index surrounded by ``i`` and\n                  ``j``.\n                - lower: ``i``.\n                - higher: ``j``.\n                - nearest: ``i`` or ``j``, whichever is nearest.\n                - midpoint: ``(i + j) / 2``.\n        keep_attrs : bool, optional\n            If True, the dataset\'s attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n        skipna : bool, optional\n            Whether to skip missing values when aggregating.\n\n        Returns\n        -------\n        quantiles : DataArray\n            If `q` is a single quantile, then the result\n            is a scalar. If multiple percentiles are given, first axis of\n            the result corresponds to the quantile and a quantile dimension\n            is added to the return array. The other dimensions are the\n            dimensions that remain after the reduction of the array.\n\n        See Also\n        --------\n        numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile\n\n        Examples\n        --------\n\n        >>> da = xr.DataArray(\n        ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],\n        ...     coords={""x"": [7, 9], ""y"": [1, 1.5, 2, 2.5]},\n        ...     dims=(""x"", ""y""),\n        ... )\n        >>> da.quantile(0)  # or da.quantile(0, dim=...)\n        <xarray.DataArray ()>\n        array(0.7)\n        Coordinates:\n            quantile  float64 0.0\n        >>> da.quantile(0, dim=""x"")\n        <xarray.DataArray (y: 4)>\n        array([0.7, 4.2, 2.6, 1.5])\n        Coordinates:\n          * y         (y) float64 1.0 1.5 2.0 2.5\n            quantile  float64 0.0\n        >>> da.quantile([0, 0.5, 1])\n        <xarray.DataArray (quantile: 3)>\n        array([0.7, 3.4, 9.4])\n        Coordinates:\n          * quantile  (quantile) float64 0.0 0.5 1.0\n        >>> da.quantile([0, 0.5, 1], dim=""x"")\n        <xarray.DataArray (quantile: 3, y: 4)>\n        array([[0.7 , 4.2 , 2.6 , 1.5 ],\n               [3.6 , 5.75, 6.  , 1.7 ],\n               [6.5 , 7.3 , 9.4 , 1.9 ]])\n        Coordinates:\n          * y         (y) float64 1.0 1.5 2.0 2.5\n          * quantile  (quantile) float64 0.0 0.5 1.0\n        """"""\n\n        ds = self._to_temp_dataset().quantile(\n            q,\n            dim=dim,\n            keep_attrs=keep_attrs,\n            interpolation=interpolation,\n            skipna=skipna,\n        )\n        return self._from_temp_dataset(ds)\n\n    def rank(\n        self, dim: Hashable, pct: bool = False, keep_attrs: bool = None\n    ) -> ""DataArray"":\n        """"""Ranks the data.\n\n        Equal values are assigned a rank that is the average of the ranks that\n        would have been otherwise assigned to all of the values within that\n        set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.\n\n        NaNs in the input array are returned as NaNs.\n\n        The `bottleneck` library is required.\n\n        Parameters\n        ----------\n        dim : hashable\n            Dimension over which to compute rank.\n        pct : bool, optional\n            If True, compute percentage ranks, otherwise compute integer ranks.\n        keep_attrs : bool, optional\n            If True, the dataset\'s attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n\n        Returns\n        -------\n        ranked : DataArray\n            DataArray with the same coordinates and dtype \'float64\'.\n\n        Examples\n        --------\n\n        >>> arr = xr.DataArray([5, 6, 7], dims=""x"")\n        >>> arr.rank(""x"")\n        <xarray.DataArray (x: 3)>\n        array([ 1.,   2.,   3.])\n        Dimensions without coordinates: x\n        """"""\n\n        ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)\n        return self._from_temp_dataset(ds)\n\n    def differentiate(\n        self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None\n    ) -> ""DataArray"":\n        """""" Differentiate the array with the second order accurate central\n        differences.\n\n        .. note::\n            This feature is limited to simple cartesian geometry, i.e. coord\n            must be one dimensional.\n\n        Parameters\n        ----------\n        coord: hashable\n            The coordinate to be used to compute the gradient.\n        edge_order: 1 or 2. Default 1\n            N-th order accurate differences at the boundaries.\n        datetime_unit: None or any of {\'Y\', \'M\', \'W\', \'D\', \'h\', \'m\', \'s\', \'ms\',\n            \'us\', \'ns\', \'ps\', \'fs\', \'as\'}\n            Unit to compute gradient. Only valid for datetime coordinate.\n\n        Returns\n        -------\n        differentiated: DataArray\n\n        See also\n        --------\n        numpy.gradient: corresponding numpy function\n\n        Examples\n        --------\n\n        >>> da = xr.DataArray(\n        ...     np.arange(12).reshape(4, 3),\n        ...     dims=[""x"", ""y""],\n        ...     coords={""x"": [0, 0.1, 1.1, 1.2]},\n        ... )\n        >>> da\n        <xarray.DataArray (x: 4, y: 3)>\n        array([[ 0,  1,  2],\n               [ 3,  4,  5],\n               [ 6,  7,  8],\n               [ 9, 10, 11]])\n        Coordinates:\n          * x        (x) float64 0.0 0.1 1.1 1.2\n        Dimensions without coordinates: y\n        >>>\n        >>> da.differentiate(""x"")\n        <xarray.DataArray (x: 4, y: 3)>\n        array([[30.      , 30.      , 30.      ],\n               [27.545455, 27.545455, 27.545455],\n               [27.545455, 27.545455, 27.545455],\n               [30.      , 30.      , 30.      ]])\n        Coordinates:\n          * x        (x) float64 0.0 0.1 1.1 1.2\n        Dimensions without coordinates: y\n        """"""\n        ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)\n        return self._from_temp_dataset(ds)\n\n    def integrate(\n        self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None\n    ) -> ""DataArray"":\n        """""" integrate the array with the trapezoidal rule.\n\n        .. note::\n            This feature is limited to simple cartesian geometry, i.e. dim\n            must be one dimensional.\n\n        Parameters\n        ----------\n        dim: hashable, or a sequence of hashable\n            Coordinate(s) used for the integration.\n        datetime_unit: str, optional\n            Can be used to specify the unit if datetime coordinate is used.\n            One of {\'Y\', \'M\', \'W\', \'D\', \'h\', \'m\', \'s\', \'ms\', \'us\', \'ns\', \'ps\',\n            \'fs\', \'as\'}\n\n        Returns\n        -------\n        integrated: DataArray\n\n        See also\n        --------\n        numpy.trapz: corresponding numpy function\n\n        Examples\n        --------\n\n        >>> da = xr.DataArray(\n        ...     np.arange(12).reshape(4, 3),\n        ...     dims=[""x"", ""y""],\n        ...     coords={""x"": [0, 0.1, 1.1, 1.2]},\n        ... )\n        >>> da\n        <xarray.DataArray (x: 4, y: 3)>\n        array([[ 0,  1,  2],\n               [ 3,  4,  5],\n               [ 6,  7,  8],\n               [ 9, 10, 11]])\n        Coordinates:\n          * x        (x) float64 0.0 0.1 1.1 1.2\n        Dimensions without coordinates: y\n        >>>\n        >>> da.integrate(""x"")\n        <xarray.DataArray (y: 3)>\n        array([5.4, 6.6, 7.8])\n        Dimensions without coordinates: y\n        """"""\n        ds = self._to_temp_dataset().integrate(dim, datetime_unit)\n        return self._from_temp_dataset(ds)\n\n    def unify_chunks(self) -> ""DataArray"":\n        """""" Unify chunk size along all chunked dimensions of this DataArray.\n\n        Returns\n        -------\n\n        DataArray with consistent chunk sizes for all dask-array variables\n\n        See Also\n        --------\n\n        dask.array.core.unify_chunks\n        """"""\n        ds = self._to_temp_dataset().unify_chunks()\n        return self._from_temp_dataset(ds)\n\n    def map_blocks(\n        self,\n        func: ""Callable[..., T_DSorDA]"",\n        args: Sequence[Any] = (),\n        kwargs: Mapping[str, Any] = None,\n        template: Union[""DataArray"", ""Dataset""] = None,\n    ) -> ""T_DSorDA"":\n        """"""\n        Apply a function to each block of this DataArray.\n\n        .. warning::\n            This method is experimental and its signature may change.\n\n        Parameters\n        ----------\n        func: callable\n            User-provided function that accepts a DataArray as its first\n            parameter. The function will receive a subset or \'block\' of this DataArray (see below),\n            corresponding to one chunk along each chunked dimension. ``func`` will be\n            executed as ``func(subset_dataarray, *subset_args, **kwargs)``.\n\n            This function must return either a single DataArray or a single Dataset.\n\n            This function cannot add a new chunked dimension.\n\n        obj: DataArray, Dataset\n            Passed to the function as its first argument, one block at a time.\n        args: Sequence\n            Passed to func after unpacking and subsetting any xarray objects by blocks.\n            xarray objects in args must be aligned with obj, otherwise an error is raised.\n        kwargs: Mapping\n            Passed verbatim to func after unpacking. xarray objects, if any, will not be\n            subset to blocks. Passing dask collections in kwargs is not allowed.\n        template: (optional) DataArray, Dataset\n            xarray object representing the final result after compute is called. If not provided,\n            the function will be first run on mocked-up data, that looks like ``obj`` but\n            has sizes 0, to determine properties of the returned object such as dtype,\n            variable names, attributes, new dimensions and new indexes (if any).\n            ``template`` must be provided if the function changes the size of existing dimensions.\n            When provided, ``attrs`` on variables in `template` are copied over to the result. Any\n            ``attrs`` set by ``func`` will be ignored.\n\n\n        Returns\n        -------\n        A single DataArray or Dataset with dask backend, reassembled from the outputs of the\n        function.\n\n        Notes\n        -----\n        This function is designed for when ``func`` needs to manipulate a whole xarray object\n        subset to each block. In the more common case where ``func`` can work on numpy arrays, it is\n        recommended to use ``apply_ufunc``.\n\n        If none of the variables in ``obj`` is backed by dask arrays, calling this function is\n        equivalent to calling ``func(obj, *args, **kwargs)``.\n\n        See Also\n        --------\n        dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks,\n        xarray.DataArray.map_blocks\n\n        Examples\n        --------\n\n        Calculate an anomaly from climatology using ``.groupby()``. Using\n        ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,\n        its indices, and its methods like ``.groupby()``.\n\n        >>> def calculate_anomaly(da, groupby_type=""time.month""):\n        ...     gb = da.groupby(groupby_type)\n        ...     clim = gb.mean(dim=""time"")\n        ...     return gb - clim\n        >>> time = xr.cftime_range(""1990-01"", ""1992-01"", freq=""M"")\n        >>> np.random.seed(123)\n        >>> array = xr.DataArray(\n        ...     np.random.rand(len(time)), dims=""time"", coords=[time]\n        ... ).chunk()\n        >>> array.map_blocks(calculate_anomaly, template=array).compute()\n        <xarray.DataArray (time: 24)>\n        array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,\n                0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,\n               -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,\n                0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,\n                0.07673453,  0.22865714,  0.19063865, -0.0590131 ])\n        Coordinates:\n          * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n\n        Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments\n        to the function being applied in ``xr.map_blocks()``:\n\n        >>> array.map_blocks(\n        ...     calculate_anomaly, kwargs={""groupby_type"": ""time.year""}, template=array,\n        ... )\n        <xarray.DataArray (time: 24)>\n        array([ 0.15361741, -0.25671244, -0.31600032,  0.008463  ,  0.1766172 ,\n               -0.11974531,  0.43791243,  0.14197797, -0.06191987, -0.15073425,\n               -0.19967375,  0.18619794, -0.05100474, -0.42989909, -0.09153273,\n                0.24841842, -0.30708526, -0.31412523,  0.04197439,  0.0422506 ,\n                0.14482397,  0.35985481,  0.23487834,  0.12144652])\n        Coordinates:\n            * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n        """"""\n        from .parallel import map_blocks\n\n        return map_blocks(func, self, args, kwargs, template)\n\n    def polyfit(\n        self,\n        dim: Hashable,\n        deg: int,\n        skipna: bool = None,\n        rcond: float = None,\n        w: Union[Hashable, Any] = None,\n        full: bool = False,\n        cov: bool = False,\n    ):\n        """"""\n        Least squares polynomial fit.\n\n        This replicates the behaviour of `numpy.polyfit` but differs by skipping\n        invalid values when `skipna = True`.\n\n        Parameters\n        ----------\n        dim : hashable\n            Coordinate along which to fit the polynomials.\n        deg : int\n            Degree of the fitting polynomial.\n        skipna : bool, optional\n            If True, removes all invalid values before fitting each 1D slices of the array.\n            Default is True if data is stored in a dask.array or if there is any\n            invalid values, False otherwise.\n        rcond : float, optional\n            Relative condition number to the fit.\n        w : Union[Hashable, Any], optional\n            Weights to apply to the y-coordinate of the sample points.\n            Can be an array-like object or the name of a coordinate in the dataset.\n        full : bool, optional\n            Whether to return the residuals, matrix rank and singular values in addition\n            to the coefficients.\n        cov : Union[bool, str], optional\n            Whether to return to the covariance matrix in addition to the coefficients.\n            The matrix is not scaled if `cov=\'unscaled\'`.\n\n        Returns\n        -------\n        polyfit_results : Dataset\n            A single dataset which contains:\n\n            polyfit_coefficients\n                The coefficients of the best fit.\n            polyfit_residuals\n                The residuals of the least-square computation (only included if `full=True`)\n            [dim]_matrix_rank\n                The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n            [dim]_singular_value\n                The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n            polyfit_covariance\n                The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)\n\n        See also\n        --------\n        numpy.polyfit\n        """"""\n        return self._to_temp_dataset().polyfit(\n            dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov\n        )\n\n    def pad(\n        self,\n        pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,\n        mode: str = ""constant"",\n        stat_length: Union[\n            int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n        ] = None,\n        constant_values: Union[\n            int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n        ] = None,\n        end_values: Union[\n            int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n        ] = None,\n        reflect_type: str = None,\n        **pad_width_kwargs: Any,\n    ) -> ""DataArray"":\n        """"""Pad this array along one or more dimensions.\n\n        .. warning::\n            This function is experimental and its behaviour is likely to change\n            especially regarding padding of dimension coordinates (or IndexVariables).\n\n        When using one of the modes (""edge"", ""reflect"", ""symmetric"", ""wrap""),\n        coordinates will be padded with the same mode, otherwise coordinates\n        are padded using the ""constant"" mode with fill_value dtypes.NA.\n\n        Parameters\n        ----------\n        pad_width : Mapping with the form of {dim: (pad_before, pad_after)}\n            Number of values padded along each dimension.\n            {dim: pad} is a shortcut for pad_before = pad_after = pad\n        mode : str\n            One of the following string values (taken from numpy docs)\n\n            \'constant\' (default)\n                Pads with a constant value.\n            \'edge\'\n                Pads with the edge values of array.\n            \'linear_ramp\'\n                Pads with the linear ramp between end_value and the\n                array edge value.\n            \'maximum\'\n                Pads with the maximum value of all or part of the\n                vector along each axis.\n            \'mean\'\n                Pads with the mean value of all or part of the\n                vector along each axis.\n            \'median\'\n                Pads with the median value of all or part of the\n                vector along each axis.\n            \'minimum\'\n                Pads with the minimum value of all or part of the\n                vector along each axis.\n            \'reflect\'\n                Pads with the reflection of the vector mirrored on\n                the first and last values of the vector along each\n                axis.\n            \'symmetric\'\n                Pads with the reflection of the vector mirrored\n                along the edge of the array.\n            \'wrap\'\n                Pads with the wrap of the vector along the axis.\n                The first values are used to pad the end and the\n                end values are used to pad the beginning.\n        stat_length : int, tuple or mapping of the form {dim: tuple}\n            Used in \'maximum\', \'mean\', \'median\', and \'minimum\'.  Number of\n            values at edge of each axis used to calculate the statistic value.\n            {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique\n            statistic lengths along each dimension.\n            ((before, after),) yields same before and after statistic lengths\n            for each dimension.\n            (stat_length,) or int is a shortcut for before = after = statistic\n            length for all axes.\n            Default is ``None``, to use the entire axis.\n        constant_values : scalar, tuple or mapping of the form {dim: tuple}\n            Used in \'constant\'.  The values to set the padded values for each\n            axis.\n            ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n            pad constants along each dimension.\n            ``((before, after),)`` yields same before and after constants for each\n            dimension.\n            ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n            all dimensions.\n            Default is 0.\n        end_values : scalar, tuple or mapping of the form {dim: tuple}\n            Used in \'linear_ramp\'.  The values used for the ending value of the\n            linear_ramp and that will form the edge of the padded array.\n            ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n            end values along each dimension.\n            ``((before, after),)`` yields same before and after end values for each\n            axis.\n            ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n            all axes.\n            Default is 0.\n        reflect_type : {\'even\', \'odd\'}, optional\n            Used in \'reflect\', and \'symmetric\'.  The \'even\' style is the\n            default with an unaltered reflection around the edge value.  For\n            the \'odd\' style, the extended part of the array is created by\n            subtracting the reflected values from two times the edge value.\n        **pad_width_kwargs:\n            The keyword arguments form of ``pad_width``.\n            One of ``pad_width`` or ``pad_width_kwargs`` must be provided.\n\n        Returns\n        -------\n        padded : DataArray\n            DataArray with the padded coordinates and data.\n\n        See also\n        --------\n        DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad\n\n        Notes\n        -----\n        By default when ``mode=""constant""`` and ``constant_values=None``, integer types will be\n        promoted to ``float`` and padded with ``np.nan``. To avoid type promotion\n        specify ``constant_values=np.nan``\n\n        Examples\n        --------\n\n        >>> arr = xr.DataArray([5, 6, 7], coords=[(""x"", [0, 1, 2])])\n        >>> arr.pad(x=(1, 2), constant_values=0)\n        <xarray.DataArray (x: 6)>\n        array([0, 5, 6, 7, 0, 0])\n        Coordinates:\n          * x        (x) float64 nan 0.0 1.0 2.0 nan nan\n\n        >>> da = xr.DataArray(\n        ...     [[0, 1, 2, 3], [10, 11, 12, 13]],\n        ...     dims=[""x"", ""y""],\n        ...     coords={""x"": [0, 1], ""y"": [10, 20, 30, 40], ""z"": (""x"", [100, 200])},\n        ... )\n        >>> da.pad(x=1)\n        <xarray.DataArray (x: 4, y: 4)>\n        array([[nan, nan, nan, nan],\n               [ 0.,  1.,  2.,  3.],\n               [10., 11., 12., 13.],\n               [nan, nan, nan, nan]])\n        Coordinates:\n          * x        (x) float64 nan 0.0 1.0 nan\n          * y        (y) int64 10 20 30 40\n            z        (x) float64 nan 100.0 200.0 nan\n        >>> da.pad(x=1, constant_values=np.nan)\n        <xarray.DataArray (x: 4, y: 4)>\n        array([[-9223372036854775808, -9223372036854775808, -9223372036854775808,\n                -9223372036854775808],\n               [                   0,                    1,                    2,\n                                   3],\n               [                  10,                   11,                   12,\n                                  13],\n               [-9223372036854775808, -9223372036854775808, -9223372036854775808,\n                -9223372036854775808]])\n        Coordinates:\n          * x        (x) float64 nan 0.0 1.0 nan\n          * y        (y) int64 10 20 30 40\n            z        (x) float64 nan 100.0 200.0 nan\n        """"""\n        ds = self._to_temp_dataset().pad(\n            pad_width=pad_width,\n            mode=mode,\n            stat_length=stat_length,\n            constant_values=constant_values,\n            end_values=end_values,\n            reflect_type=reflect_type,\n            **pad_width_kwargs,\n        )\n        return self._from_temp_dataset(ds)\n\n    def idxmin(\n        self,\n        dim: Hashable = None,\n        skipna: bool = None,\n        fill_value: Any = dtypes.NA,\n        keep_attrs: bool = None,\n    ) -> ""DataArray"":\n        """"""Return the coordinate label of the minimum value along a dimension.\n\n        Returns a new `DataArray` named after the dimension with the values of\n        the coordinate labels along that dimension corresponding to minimum\n        values along that dimension.\n\n        In comparison to :py:meth:`~DataArray.argmin`, this returns the\n        coordinate label while :py:meth:`~DataArray.argmin` returns the index.\n\n        Parameters\n        ----------\n        dim : str, optional\n            Dimension over which to apply `idxmin`.  This is optional for 1D\n            arrays, but required for arrays with 2 or more dimensions.\n        skipna : bool or None, default None\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for ``float``, ``complex``, and ``object``\n            dtypes; other dtypes either do not have a sentinel missing value\n            (``int``) or ``skipna=True`` has not been implemented\n            (``datetime64`` or ``timedelta64``).\n        fill_value : Any, default NaN\n            Value to be filled in case all of the values along a dimension are\n            null.  By default this is NaN.  The fill value and result are\n            automatically converted to a compatible dtype if possible.\n            Ignored if ``skipna`` is False.\n        keep_attrs : bool, default False\n            If True, the attributes (``attrs``) will be copied from the\n            original object to the new one.  If False (default), the new object\n            will be returned without attributes.\n\n        Returns\n        -------\n        reduced : DataArray\n            New `DataArray` object with `idxmin` applied to its data and the\n            indicated dimension removed.\n\n        See also\n        --------\n        Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin\n\n        Examples\n        --------\n\n        >>> array = xr.DataArray(\n        ...     [0, 2, 1, 0, -2], dims=""x"", coords={""x"": [""a"", ""b"", ""c"", ""d"", ""e""]}\n        ... )\n        >>> array.min()\n        <xarray.DataArray ()>\n        array(-2)\n        >>> array.argmin()\n        <xarray.DataArray ()>\n        array(4)\n        >>> array.idxmin()\n        <xarray.DataArray \'x\' ()>\n        array(\'e\', dtype=\'<U1\')\n\n        >>> array = xr.DataArray(\n        ...     [\n        ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n        ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n        ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n        ...     ],\n        ...     dims=[""y"", ""x""],\n        ...     coords={""y"": [-1, 0, 1], ""x"": np.arange(5.0) ** 2},\n        ... )\n        >>> array.min(dim=""x"")\n        <xarray.DataArray (y: 3)>\n        array([-2., -4.,  1.])\n        Coordinates:\n          * y        (y) int64 -1 0 1\n        >>> array.argmin(dim=""x"")\n        <xarray.DataArray (y: 3)>\n        array([4, 0, 2])\n        Coordinates:\n          * y        (y) int64 -1 0 1\n        >>> array.idxmin(dim=""x"")\n        <xarray.DataArray \'x\' (y: 3)>\n        array([16.,  0.,  4.])\n        Coordinates:\n          * y        (y) int64 -1 0 1\n        """"""\n        return computation._calc_idxminmax(\n            array=self,\n            func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),\n            dim=dim,\n            skipna=skipna,\n            fill_value=fill_value,\n            keep_attrs=keep_attrs,\n        )\n\n    def idxmax(\n        self,\n        dim: Hashable = None,\n        skipna: bool = None,\n        fill_value: Any = dtypes.NA,\n        keep_attrs: bool = None,\n    ) -> ""DataArray"":\n        """"""Return the coordinate label of the maximum value along a dimension.\n\n        Returns a new `DataArray` named after the dimension with the values of\n        the coordinate labels along that dimension corresponding to maximum\n        values along that dimension.\n\n        In comparison to :py:meth:`~DataArray.argmax`, this returns the\n        coordinate label while :py:meth:`~DataArray.argmax` returns the index.\n\n        Parameters\n        ----------\n        dim : str, optional\n            Dimension over which to apply `idxmax`.  This is optional for 1D\n            arrays, but required for arrays with 2 or more dimensions.\n        skipna : bool or None, default None\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for ``float``, ``complex``, and ``object``\n            dtypes; other dtypes either do not have a sentinel missing value\n            (``int``) or ``skipna=True`` has not been implemented\n            (``datetime64`` or ``timedelta64``).\n        fill_value : Any, default NaN\n            Value to be filled in case all of the values along a dimension are\n            null.  By default this is NaN.  The fill value and result are\n            automatically converted to a compatible dtype if possible.\n            Ignored if ``skipna`` is False.\n        keep_attrs : bool, default False\n            If True, the attributes (``attrs``) will be copied from the\n            original object to the new one.  If False (default), the new object\n            will be returned without attributes.\n\n        Returns\n        -------\n        reduced : DataArray\n            New `DataArray` object with `idxmax` applied to its data and the\n            indicated dimension removed.\n\n        See also\n        --------\n        Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax\n\n        Examples\n        --------\n\n        >>> array = xr.DataArray(\n        ...     [0, 2, 1, 0, -2], dims=""x"", coords={""x"": [""a"", ""b"", ""c"", ""d"", ""e""]}\n        ... )\n        >>> array.max()\n        <xarray.DataArray ()>\n        array(2)\n        >>> array.argmax()\n        <xarray.DataArray ()>\n        array(1)\n        >>> array.idxmax()\n        <xarray.DataArray \'x\' ()>\n        array(\'b\', dtype=\'<U1\')\n\n        >>> array = xr.DataArray(\n        ...     [\n        ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n        ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n        ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n        ...     ],\n        ...     dims=[""y"", ""x""],\n        ...     coords={""y"": [-1, 0, 1], ""x"": np.arange(5.0) ** 2},\n        ... )\n        >>> array.max(dim=""x"")\n        <xarray.DataArray (y: 3)>\n        array([2., 2., 1.])\n        Coordinates:\n          * y        (y) int64 -1 0 1\n        >>> array.argmax(dim=""x"")\n        <xarray.DataArray (y: 3)>\n        array([0, 2, 2])\n        Coordinates:\n          * y        (y) int64 -1 0 1\n        >>> array.idxmax(dim=""x"")\n        <xarray.DataArray \'x\' (y: 3)>\n        array([0., 4., 4.])\n        Coordinates:\n          * y        (y) int64 -1 0 1\n        """"""\n        return computation._calc_idxminmax(\n            array=self,\n            func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),\n            dim=dim,\n            skipna=skipna,\n            fill_value=fill_value,\n            keep_attrs=keep_attrs,\n        )\n\n    # this needs to be at the end, or mypy will confuse with `str`\n    # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names\n    str = property(StringAccessor)\n\n\n# priority most be higher than Variable to properly work with binary ufuncs\nops.inject_all_ops_and_reduce_methods(DataArray, priority=60)\n'"
xarray/core/dataset.py,73,"b'import copy\nimport datetime\nimport functools\nimport sys\nimport warnings\nfrom collections import defaultdict\nfrom html import escape\nfrom numbers import Number\nfrom operator import methodcaller\nfrom pathlib import Path\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    DefaultDict,\n    Dict,\n    Hashable,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    MutableMapping,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    TypeVar,\n    Union,\n    cast,\n)\n\nimport numpy as np\nimport pandas as pd\n\nimport xarray as xr\n\nfrom ..coding.cftimeindex import _parse_array_of_cftime_strings\nfrom ..plot.dataset_plot import _Dataset_PlotMethods\nfrom . import (\n    alignment,\n    dtypes,\n    duck_array_ops,\n    formatting,\n    formatting_html,\n    groupby,\n    ops,\n    resample,\n    rolling,\n    utils,\n    weighted,\n)\nfrom .alignment import _broadcast_helper, _get_broadcast_dims_map_common_coords, align\nfrom .common import (\n    DataWithCoords,\n    ImplementsDatasetReduce,\n    _contains_datetime_like_objects,\n)\nfrom .coordinates import (\n    DatasetCoordinates,\n    LevelCoordinatesSource,\n    assert_coordinate_consistent,\n    remap_label_indexers,\n)\nfrom .duck_array_ops import datetime_to_numeric\nfrom .indexes import (\n    Indexes,\n    default_indexes,\n    isel_variable_and_index,\n    propagate_indexes,\n    remove_unused_levels_categories,\n    roll_index,\n)\nfrom .indexing import is_fancy_indexer\nfrom .merge import (\n    dataset_merge_method,\n    dataset_update_method,\n    merge_coordinates_without_align,\n    merge_data_and_coords,\n)\nfrom .missing import get_clean_interp_index\nfrom .options import OPTIONS, _get_keep_attrs\nfrom .pycompat import dask_array_type\nfrom .utils import (\n    Default,\n    Frozen,\n    SortedKeysDict,\n    _check_inplace,\n    _default,\n    decode_numpy_dict_values,\n    drop_dims_from_indexers,\n    either_dict_or_kwargs,\n    hashable,\n    infix_dims,\n    is_dict_like,\n    is_scalar,\n    maybe_wrap_array,\n)\nfrom .variable import (\n    IndexVariable,\n    Variable,\n    as_variable,\n    assert_unique_multiindex_level_names,\n    broadcast_variables,\n)\n\nif TYPE_CHECKING:\n    from ..backends import AbstractDataStore, ZarrStore\n    from .dataarray import DataArray\n    from .merge import CoercibleMapping\n\n    T_DSorDA = TypeVar(""T_DSorDA"", DataArray, ""Dataset"")\n\n    try:\n        from dask.delayed import Delayed\n    except ImportError:\n        Delayed = None\n\n\n# list of attributes of pd.DatetimeIndex that are ndarrays of time info\n_DATETIMEINDEX_COMPONENTS = [\n    ""year"",\n    ""month"",\n    ""day"",\n    ""hour"",\n    ""minute"",\n    ""second"",\n    ""microsecond"",\n    ""nanosecond"",\n    ""date"",\n    ""time"",\n    ""dayofyear"",\n    ""weekofyear"",\n    ""dayofweek"",\n    ""quarter"",\n]\n\n\ndef _get_virtual_variable(\n    variables, key: Hashable, level_vars: Mapping = None, dim_sizes: Mapping = None\n) -> Tuple[Hashable, Hashable, Variable]:\n    """"""Get a virtual variable (e.g., \'time.year\' or a MultiIndex level)\n    from a dict of xarray.Variable objects (if possible)\n    """"""\n    if level_vars is None:\n        level_vars = {}\n    if dim_sizes is None:\n        dim_sizes = {}\n\n    if key in dim_sizes:\n        data = pd.Index(range(dim_sizes[key]), name=key)\n        variable = IndexVariable((key,), data)\n        return key, key, variable\n\n    if not isinstance(key, str):\n        raise KeyError(key)\n\n    split_key = key.split(""."", 1)\n    var_name: Optional[str]\n    if len(split_key) == 2:\n        ref_name, var_name = split_key\n    elif len(split_key) == 1:\n        ref_name, var_name = key, None\n    else:\n        raise KeyError(key)\n\n    if ref_name in level_vars:\n        dim_var = variables[level_vars[ref_name]]\n        ref_var = dim_var.to_index_variable().get_level_variable(ref_name)\n    else:\n        ref_var = variables[ref_name]\n\n    if var_name is None:\n        virtual_var = ref_var\n        var_name = key\n    else:\n        if _contains_datetime_like_objects(ref_var):\n            ref_var = xr.DataArray(ref_var)\n            data = getattr(ref_var.dt, var_name).data\n        else:\n            data = getattr(ref_var, var_name).data\n        virtual_var = Variable(ref_var.dims, data)\n\n    return ref_name, var_name, virtual_var\n\n\ndef calculate_dimensions(variables: Mapping[Hashable, Variable]) -> Dict[Hashable, int]:\n    """"""Calculate the dimensions corresponding to a set of variables.\n\n    Returns dictionary mapping from dimension names to sizes. Raises ValueError\n    if any of the dimension sizes conflict.\n    """"""\n    dims: Dict[Hashable, int] = {}\n    last_used = {}\n    scalar_vars = {k for k, v in variables.items() if not v.dims}\n    for k, var in variables.items():\n        for dim, size in zip(var.dims, var.shape):\n            if dim in scalar_vars:\n                raise ValueError(\n                    ""dimension %r already exists as a scalar "" ""variable"" % dim\n                )\n            if dim not in dims:\n                dims[dim] = size\n                last_used[dim] = k\n            elif dims[dim] != size:\n                raise ValueError(\n                    ""conflicting sizes for dimension %r: ""\n                    ""length %s on %r and length %s on %r""\n                    % (dim, size, k, dims[dim], last_used[dim])\n                )\n    return dims\n\n\ndef merge_indexes(\n    indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]],\n    variables: Mapping[Hashable, Variable],\n    coord_names: Set[Hashable],\n    append: bool = False,\n) -> Tuple[Dict[Hashable, Variable], Set[Hashable]]:\n    """"""Merge variables into multi-indexes.\n\n    Not public API. Used in Dataset and DataArray set_index\n    methods.\n    """"""\n    vars_to_replace: Dict[Hashable, Variable] = {}\n    vars_to_remove: List[Hashable] = []\n    dims_to_replace: Dict[Hashable, Hashable] = {}\n    error_msg = ""{} is not the name of an existing variable.""\n\n    for dim, var_names in indexes.items():\n        if isinstance(var_names, str) or not isinstance(var_names, Sequence):\n            var_names = [var_names]\n\n        names: List[Hashable] = []\n        codes: List[List[int]] = []\n        levels: List[List[int]] = []\n        current_index_variable = variables.get(dim)\n\n        for n in var_names:\n            try:\n                var = variables[n]\n            except KeyError:\n                raise ValueError(error_msg.format(n))\n            if (\n                current_index_variable is not None\n                and var.dims != current_index_variable.dims\n            ):\n                raise ValueError(\n                    ""dimension mismatch between %r %s and %r %s""\n                    % (dim, current_index_variable.dims, n, var.dims)\n                )\n\n        if current_index_variable is not None and append:\n            current_index = current_index_variable.to_index()\n            if isinstance(current_index, pd.MultiIndex):\n                names.extend(current_index.names)\n                codes.extend(current_index.codes)\n                levels.extend(current_index.levels)\n            else:\n                names.append(""%s_level_0"" % dim)\n                cat = pd.Categorical(current_index.values, ordered=True)\n                codes.append(cat.codes)\n                levels.append(cat.categories)\n\n        if not len(names) and len(var_names) == 1:\n            idx = pd.Index(variables[var_names[0]].values)\n\n        else:  # MultiIndex\n            for n in var_names:\n                try:\n                    var = variables[n]\n                except KeyError:\n                    raise ValueError(error_msg.format(n))\n                names.append(n)\n                cat = pd.Categorical(var.values, ordered=True)\n                codes.append(cat.codes)\n                levels.append(cat.categories)\n\n            idx = pd.MultiIndex(levels, codes, names=names)\n            for n in names:\n                dims_to_replace[n] = dim\n\n        vars_to_replace[dim] = IndexVariable(dim, idx)\n        vars_to_remove.extend(var_names)\n\n    new_variables = {k: v for k, v in variables.items() if k not in vars_to_remove}\n    new_variables.update(vars_to_replace)\n\n    # update dimensions if necessary  GH: 3512\n    for k, v in new_variables.items():\n        if any(d in dims_to_replace for d in v.dims):\n            new_dims = [dims_to_replace.get(d, d) for d in v.dims]\n            new_variables[k] = v._replace(dims=new_dims)\n    new_coord_names = coord_names | set(vars_to_replace)\n    new_coord_names -= set(vars_to_remove)\n    return new_variables, new_coord_names\n\n\ndef split_indexes(\n    dims_or_levels: Union[Hashable, Sequence[Hashable]],\n    variables: Mapping[Hashable, Variable],\n    coord_names: Set[Hashable],\n    level_coords: Mapping[Hashable, Hashable],\n    drop: bool = False,\n) -> Tuple[Dict[Hashable, Variable], Set[Hashable]]:\n    """"""Extract (multi-)indexes (levels) as variables.\n\n    Not public API. Used in Dataset and DataArray reset_index\n    methods.\n    """"""\n    if isinstance(dims_or_levels, str) or not isinstance(dims_or_levels, Sequence):\n        dims_or_levels = [dims_or_levels]\n\n    dim_levels: DefaultDict[Any, List[Hashable]] = defaultdict(list)\n    dims = []\n    for k in dims_or_levels:\n        if k in level_coords:\n            dim_levels[level_coords[k]].append(k)\n        else:\n            dims.append(k)\n\n    vars_to_replace = {}\n    vars_to_create: Dict[Hashable, Variable] = {}\n    vars_to_remove = []\n\n    for d in dims:\n        index = variables[d].to_index()\n        if isinstance(index, pd.MultiIndex):\n            dim_levels[d] = index.names\n        else:\n            vars_to_remove.append(d)\n            if not drop:\n                vars_to_create[str(d) + ""_""] = Variable(d, index, variables[d].attrs)\n\n    for d, levs in dim_levels.items():\n        index = variables[d].to_index()\n        if len(levs) == index.nlevels:\n            vars_to_remove.append(d)\n        else:\n            vars_to_replace[d] = IndexVariable(d, index.droplevel(levs))\n\n        if not drop:\n            for lev in levs:\n                idx = index.get_level_values(lev)\n                vars_to_create[idx.name] = Variable(d, idx, variables[d].attrs)\n\n    new_variables = dict(variables)\n    for v in set(vars_to_remove):\n        del new_variables[v]\n    new_variables.update(vars_to_replace)\n    new_variables.update(vars_to_create)\n    new_coord_names = (coord_names | set(vars_to_create)) - set(vars_to_remove)\n\n    return new_variables, new_coord_names\n\n\ndef _assert_empty(args: tuple, msg: str = ""%s"") -> None:\n    if args:\n        raise ValueError(msg % args)\n\n\ndef as_dataset(obj: Any) -> ""Dataset"":\n    """"""Cast the given object to a Dataset.\n\n    Handles Datasets, DataArrays and dictionaries of variables. A new Dataset\n    object is only created if the provided object is not already one.\n    """"""\n    if hasattr(obj, ""to_dataset""):\n        obj = obj.to_dataset()\n    if not isinstance(obj, Dataset):\n        obj = Dataset(obj)\n    return obj\n\n\nclass DataVariables(Mapping[Hashable, ""DataArray""]):\n    __slots__ = (""_dataset"",)\n\n    def __init__(self, dataset: ""Dataset""):\n        self._dataset = dataset\n\n    def __iter__(self) -> Iterator[Hashable]:\n        return (\n            key\n            for key in self._dataset._variables\n            if key not in self._dataset._coord_names\n        )\n\n    def __len__(self) -> int:\n        return len(self._dataset._variables) - len(self._dataset._coord_names)\n\n    def __contains__(self, key: Hashable) -> bool:\n        return key in self._dataset._variables and key not in self._dataset._coord_names\n\n    def __getitem__(self, key: Hashable) -> ""DataArray"":\n        if key not in self._dataset._coord_names:\n            return cast(""DataArray"", self._dataset[key])\n        raise KeyError(key)\n\n    def __repr__(self) -> str:\n        return formatting.data_vars_repr(self)\n\n    @property\n    def variables(self) -> Mapping[Hashable, Variable]:\n        all_variables = self._dataset.variables\n        return Frozen({k: all_variables[k] for k in self})\n\n    def _ipython_key_completions_(self):\n        """"""Provide method for the key-autocompletions in IPython. """"""\n        return [\n            key\n            for key in self._dataset._ipython_key_completions_()\n            if key not in self._dataset._coord_names\n        ]\n\n\nclass _LocIndexer:\n    __slots__ = (""dataset"",)\n\n    def __init__(self, dataset: ""Dataset""):\n        self.dataset = dataset\n\n    def __getitem__(self, key: Mapping[Hashable, Any]) -> ""Dataset"":\n        if not utils.is_dict_like(key):\n            raise TypeError(""can only lookup dictionaries from Dataset.loc"")\n        return self.dataset.sel(key)\n\n\nclass Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n    """"""A multi-dimensional, in memory, array database.\n\n    A dataset resembles an in-memory representation of a NetCDF file, and\n    consists of variables, coordinates and attributes which together form a\n    self describing dataset.\n\n    Dataset implements the mapping interface with keys given by variable names\n    and values given by DataArray objects for each variable name.\n\n    One dimensional variables with name equal to their dimension are index\n    coordinates used for label based indexing.\n    """"""\n\n    _attrs: Optional[Dict[Hashable, Any]]\n    _cache: Dict[str, Any]\n    _coord_names: Set[Hashable]\n    _dims: Dict[Hashable, int]\n    _encoding: Optional[Dict[Hashable, Any]]\n    _indexes: Optional[Dict[Hashable, pd.Index]]\n    _variables: Dict[Hashable, Variable]\n\n    __slots__ = (\n        ""_attrs"",\n        ""_cache"",\n        ""_coord_names"",\n        ""_dims"",\n        ""_encoding"",\n        ""_file_obj"",\n        ""_indexes"",\n        ""_variables"",\n        ""__weakref__"",\n    )\n\n    _groupby_cls = groupby.DatasetGroupBy\n    _rolling_cls = rolling.DatasetRolling\n    _coarsen_cls = rolling.DatasetCoarsen\n    _resample_cls = resample.DatasetResample\n    _weighted_cls = weighted.DatasetWeighted\n\n    def __init__(\n        self,\n        # could make a VariableArgs to use more generally, and refine these\n        # categories\n        data_vars: Mapping[Hashable, Any] = None,\n        coords: Mapping[Hashable, Any] = None,\n        attrs: Mapping[Hashable, Any] = None,\n    ):\n        """"""To load data from a file or file-like object, use the `open_dataset`\n        function.\n\n        Parameters\n        ----------\n        data_vars : dict-like, optional\n            A mapping from variable names to :py:class:`~xarray.DataArray`\n            objects, :py:class:`~xarray.Variable` objects or to tuples of the\n            form ``(dims, data[, attrs])`` which can be used as arguments to\n            create a new ``Variable``. Each dimension must have the same length\n            in all variables in which it appears.\n\n            The following notations are accepted:\n\n            - mapping {var name: DataArray}\n            - mapping {var name: Variable}\n            - mapping {var name: (dimension name, array-like)}\n            - mapping {var name: (tuple of dimension names, array-like)}\n            - mapping {dimension name: array-like}\n              (it will be automatically moved to coords, see below)\n\n            Each dimension must have the same length in all variables in which\n            it appears.\n        coords : dict-like, optional\n            Another mapping in similar form as the `data_vars` argument,\n            except the each item is saved on the dataset as a ""coordinate"".\n            These variables have an associated meaning: they describe\n            constant/fixed/independent quantities, unlike the\n            varying/measured/dependent quantities that belong in `variables`.\n            Coordinates values may be given by 1-dimensional arrays or scalars,\n            in which case `dims` do not need to be supplied: 1D arrays will be\n            assumed to give index values along the dimension with the same\n            name.\n\n            The following notations are accepted:\n\n            - mapping {coord name: DataArray}\n            - mapping {coord name: Variable}\n            - mapping {coord name: (dimension name, array-like)}\n            - mapping {coord name: (tuple of dimension names, array-like)}\n            - mapping {dimension name: array-like}\n              (the dimension name is implicitly set to be the same as the coord name)\n\n            The last notation implies that the coord name is the same as the\n            dimension name.\n\n        attrs : dict-like, optional\n            Global attributes to save on this dataset.\n        """"""\n\n        # TODO(shoyer): expose indexes as a public argument in __init__\n\n        if data_vars is None:\n            data_vars = {}\n        if coords is None:\n            coords = {}\n\n        both_data_and_coords = set(data_vars) & set(coords)\n        if both_data_and_coords:\n            raise ValueError(\n                ""variables %r are found in both data_vars and coords""\n                % both_data_and_coords\n            )\n\n        if isinstance(coords, Dataset):\n            coords = coords.variables\n\n        variables, coord_names, dims, indexes, _ = merge_data_and_coords(\n            data_vars, coords, compat=""broadcast_equals""\n        )\n\n        self._attrs = dict(attrs) if attrs is not None else None\n        self._file_obj = None\n        self._encoding = None\n        self._variables = variables\n        self._coord_names = coord_names\n        self._dims = dims\n        self._indexes = indexes\n\n    @classmethod\n    def load_store(cls, store, decoder=None) -> ""Dataset"":\n        """"""Create a new dataset from the contents of a backends.*DataStore\n        object\n        """"""\n        variables, attributes = store.load()\n        if decoder:\n            variables, attributes = decoder(variables, attributes)\n        obj = cls(variables, attrs=attributes)\n        obj._file_obj = store\n        return obj\n\n    @property\n    def variables(self) -> Mapping[Hashable, Variable]:\n        """"""Low level interface to Dataset contents as dict of Variable objects.\n\n        This ordered dictionary is frozen to prevent mutation that could\n        violate Dataset invariants. It contains all variable objects\n        constituting the Dataset, including both data variables and\n        coordinates.\n        """"""\n        return Frozen(self._variables)\n\n    @property\n    def attrs(self) -> Dict[Hashable, Any]:\n        """"""Dictionary of global attributes on this dataset\n        """"""\n        if self._attrs is None:\n            self._attrs = {}\n        return self._attrs\n\n    @attrs.setter\n    def attrs(self, value: Mapping[Hashable, Any]) -> None:\n        self._attrs = dict(value)\n\n    @property\n    def encoding(self) -> Dict:\n        """"""Dictionary of global encoding attributes on this dataset\n        """"""\n        if self._encoding is None:\n            self._encoding = {}\n        return self._encoding\n\n    @encoding.setter\n    def encoding(self, value: Mapping) -> None:\n        self._encoding = dict(value)\n\n    @property\n    def dims(self) -> Mapping[Hashable, int]:\n        """"""Mapping from dimension names to lengths.\n\n        Cannot be modified directly, but is updated when adding new variables.\n\n        Note that type of this object differs from `DataArray.dims`.\n        See `Dataset.sizes` and `DataArray.sizes` for consistently named\n        properties.\n        """"""\n        return Frozen(SortedKeysDict(self._dims))\n\n    @property\n    def sizes(self) -> Mapping[Hashable, int]:\n        """"""Mapping from dimension names to lengths.\n\n        Cannot be modified directly, but is updated when adding new variables.\n\n        This is an alias for `Dataset.dims` provided for the benefit of\n        consistency with `DataArray.sizes`.\n\n        See also\n        --------\n        DataArray.sizes\n        """"""\n        return self.dims\n\n    def load(self, **kwargs) -> ""Dataset"":\n        """"""Manually trigger loading and/or computation of this dataset\'s data\n        from disk or a remote source into memory and return this dataset.\n        Unlike compute, the original dataset is modified and returned.\n\n        Normally, it should not be necessary to call this method in user code,\n        because all xarray functions should either work on deferred data or\n        load data automatically. However, this method can be necessary when\n        working with many file objects on disk.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Additional keyword arguments passed on to ``dask.array.compute``.\n\n        See Also\n        --------\n        dask.array.compute\n        """"""\n        # access .data to coerce everything to numpy or dask arrays\n        lazy_data = {\n            k: v._data\n            for k, v in self.variables.items()\n            if isinstance(v._data, dask_array_type)\n        }\n        if lazy_data:\n            import dask.array as da\n\n            # evaluate all the dask arrays simultaneously\n            evaluated_data = da.compute(*lazy_data.values(), **kwargs)\n\n            for k, data in zip(lazy_data, evaluated_data):\n                self.variables[k].data = data\n\n        # load everything else sequentially\n        for k, v in self.variables.items():\n            if k not in lazy_data:\n                v.load()\n\n        return self\n\n    def __dask_tokenize__(self):\n        from dask.base import normalize_token\n\n        return normalize_token(\n            (type(self), self._variables, self._coord_names, self._attrs)\n        )\n\n    def __dask_graph__(self):\n        graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}\n        graphs = {k: v for k, v in graphs.items() if v is not None}\n        if not graphs:\n            return None\n        else:\n            try:\n                from dask.highlevelgraph import HighLevelGraph\n\n                return HighLevelGraph.merge(*graphs.values())\n            except ImportError:\n                from dask import sharedict\n\n                return sharedict.merge(*graphs.values())\n\n    def __dask_keys__(self):\n        import dask\n\n        return [\n            v.__dask_keys__()\n            for v in self.variables.values()\n            if dask.is_dask_collection(v)\n        ]\n\n    def __dask_layers__(self):\n        import dask\n\n        return sum(\n            [\n                v.__dask_layers__()\n                for v in self.variables.values()\n                if dask.is_dask_collection(v)\n            ],\n            (),\n        )\n\n    @property\n    def __dask_optimize__(self):\n        import dask.array as da\n\n        return da.Array.__dask_optimize__\n\n    @property\n    def __dask_scheduler__(self):\n        import dask.array as da\n\n        return da.Array.__dask_scheduler__\n\n    def __dask_postcompute__(self):\n        import dask\n\n        info = [\n            (True, k, v.__dask_postcompute__())\n            if dask.is_dask_collection(v)\n            else (False, k, v)\n            for k, v in self._variables.items()\n        ]\n        args = (\n            info,\n            self._coord_names,\n            self._dims,\n            self._attrs,\n            self._indexes,\n            self._encoding,\n            self._file_obj,\n        )\n        return self._dask_postcompute, args\n\n    def __dask_postpersist__(self):\n        import dask\n\n        info = [\n            (True, k, v.__dask_postpersist__())\n            if dask.is_dask_collection(v)\n            else (False, k, v)\n            for k, v in self._variables.items()\n        ]\n        args = (\n            info,\n            self._coord_names,\n            self._dims,\n            self._attrs,\n            self._indexes,\n            self._encoding,\n            self._file_obj,\n        )\n        return self._dask_postpersist, args\n\n    @staticmethod\n    def _dask_postcompute(results, info, *args):\n        variables = {}\n        results2 = list(results[::-1])\n        for is_dask, k, v in info:\n            if is_dask:\n                func, args2 = v\n                r = results2.pop()\n                result = func(r, *args2)\n            else:\n                result = v\n            variables[k] = result\n\n        final = Dataset._construct_direct(variables, *args)\n        return final\n\n    @staticmethod\n    def _dask_postpersist(dsk, info, *args):\n        variables = {}\n        for is_dask, k, v in info:\n            if is_dask:\n                func, args2 = v\n                result = func(dsk, *args2)\n            else:\n                result = v\n            variables[k] = result\n\n        return Dataset._construct_direct(variables, *args)\n\n    def compute(self, **kwargs) -> ""Dataset"":\n        """"""Manually trigger loading and/or computation of this dataset\'s data\n        from disk or a remote source into memory and return a new dataset.\n        Unlike load, the original dataset is left unaltered.\n\n        Normally, it should not be necessary to call this method in user code,\n        because all xarray functions should either work on deferred data or\n        load data automatically. However, this method can be necessary when\n        working with many file objects on disk.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Additional keyword arguments passed on to ``dask.array.compute``.\n\n        See Also\n        --------\n        dask.array.compute\n        """"""\n        new = self.copy(deep=False)\n        return new.load(**kwargs)\n\n    def _persist_inplace(self, **kwargs) -> ""Dataset"":\n        """"""Persist all Dask arrays in memory\n        """"""\n        # access .data to coerce everything to numpy or dask arrays\n        lazy_data = {\n            k: v._data\n            for k, v in self.variables.items()\n            if isinstance(v._data, dask_array_type)\n        }\n        if lazy_data:\n            import dask\n\n            # evaluate all the dask arrays simultaneously\n            evaluated_data = dask.persist(*lazy_data.values(), **kwargs)\n\n            for k, data in zip(lazy_data, evaluated_data):\n                self.variables[k].data = data\n\n        return self\n\n    def persist(self, **kwargs) -> ""Dataset"":\n        """""" Trigger computation, keeping data as dask arrays\n\n        This operation can be used to trigger computation on underlying dask\n        arrays, similar to ``.compute()`` or ``.load()``.  However this\n        operation keeps the data as dask arrays. This is particularly useful\n        when using the dask.distributed scheduler and you want to load a large\n        amount of data into distributed memory.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Additional keyword arguments passed on to ``dask.persist``.\n\n        See Also\n        --------\n        dask.persist\n        """"""\n        new = self.copy(deep=False)\n        return new._persist_inplace(**kwargs)\n\n    @classmethod\n    def _construct_direct(\n        cls,\n        variables,\n        coord_names,\n        dims=None,\n        attrs=None,\n        indexes=None,\n        encoding=None,\n        file_obj=None,\n    ):\n        """"""Shortcut around __init__ for internal use when we want to skip\n        costly validation\n        """"""\n        if dims is None:\n            dims = calculate_dimensions(variables)\n        obj = object.__new__(cls)\n        obj._variables = variables\n        obj._coord_names = coord_names\n        obj._dims = dims\n        obj._indexes = indexes\n        obj._attrs = attrs\n        obj._file_obj = file_obj\n        obj._encoding = encoding\n        return obj\n\n    def _replace(\n        self,\n        variables: Dict[Hashable, Variable] = None,\n        coord_names: Set[Hashable] = None,\n        dims: Dict[Any, int] = None,\n        attrs: Union[Dict[Hashable, Any], None, Default] = _default,\n        indexes: Union[Dict[Any, pd.Index], None, Default] = _default,\n        encoding: Union[dict, None, Default] = _default,\n        inplace: bool = False,\n    ) -> ""Dataset"":\n        """"""Fastpath constructor for internal use.\n\n        Returns an object with optionally with replaced attributes.\n\n        Explicitly passed arguments are *not* copied when placed on the new\n        dataset. It is up to the caller to ensure that they have the right type\n        and are not used elsewhere.\n        """"""\n        if inplace:\n            if variables is not None:\n                self._variables = variables\n            if coord_names is not None:\n                self._coord_names = coord_names\n            if dims is not None:\n                self._dims = dims\n            if attrs is not _default:\n                self._attrs = attrs\n            if indexes is not _default:\n                self._indexes = indexes\n            if encoding is not _default:\n                self._encoding = encoding\n            obj = self\n        else:\n            if variables is None:\n                variables = self._variables.copy()\n            if coord_names is None:\n                coord_names = self._coord_names.copy()\n            if dims is None:\n                dims = self._dims.copy()\n            if attrs is _default:\n                attrs = copy.copy(self._attrs)\n            if indexes is _default:\n                indexes = copy.copy(self._indexes)\n            if encoding is _default:\n                encoding = copy.copy(self._encoding)\n            obj = self._construct_direct(\n                variables, coord_names, dims, attrs, indexes, encoding\n            )\n        return obj\n\n    def _replace_with_new_dims(\n        self,\n        variables: Dict[Hashable, Variable],\n        coord_names: set = None,\n        attrs: Union[Dict[Hashable, Any], None, Default] = _default,\n        indexes: Union[Dict[Hashable, pd.Index], None, Default] = _default,\n        inplace: bool = False,\n    ) -> ""Dataset"":\n        """"""Replace variables with recalculated dimensions.""""""\n        dims = calculate_dimensions(variables)\n        return self._replace(\n            variables, coord_names, dims, attrs, indexes, inplace=inplace\n        )\n\n    def _replace_vars_and_dims(\n        self,\n        variables: Dict[Hashable, Variable],\n        coord_names: set = None,\n        dims: Dict[Hashable, int] = None,\n        attrs: Union[Dict[Hashable, Any], None, Default] = _default,\n        inplace: bool = False,\n    ) -> ""Dataset"":\n        """"""Deprecated version of _replace_with_new_dims().\n\n        Unlike _replace_with_new_dims(), this method always recalculates\n        indexes from variables.\n        """"""\n        if dims is None:\n            dims = calculate_dimensions(variables)\n        return self._replace(\n            variables, coord_names, dims, attrs, indexes=None, inplace=inplace\n        )\n\n    def _overwrite_indexes(self, indexes: Mapping[Any, pd.Index]) -> ""Dataset"":\n        if not indexes:\n            return self\n\n        variables = self._variables.copy()\n        new_indexes = dict(self.indexes)\n        for name, idx in indexes.items():\n            variables[name] = IndexVariable(name, idx)\n            new_indexes[name] = idx\n        obj = self._replace(variables, indexes=new_indexes)\n\n        # switch from dimension to level names, if necessary\n        dim_names: Dict[Hashable, str] = {}\n        for dim, idx in indexes.items():\n            if not isinstance(idx, pd.MultiIndex) and idx.name != dim:\n                dim_names[dim] = idx.name\n        if dim_names:\n            obj = obj.rename(dim_names)\n        return obj\n\n    def copy(self, deep: bool = False, data: Mapping = None) -> ""Dataset"":\n        """"""Returns a copy of this dataset.\n\n        If `deep=True`, a deep copy is made of each of the component variables.\n        Otherwise, a shallow copy of each of the component variable is made, so\n        that the underlying memory region of the new dataset is the same as in\n        the original dataset.\n\n        Use `data` to create a new object with the same structure as\n        original but entirely new data.\n\n        Parameters\n        ----------\n        deep : bool, optional\n            Whether each component variable is loaded into memory and copied onto\n            the new object. Default is False.\n        data : dict-like, optional\n            Data to use in the new object. Each item in `data` must have same\n            shape as corresponding data variable in original. When `data` is\n            used, `deep` is ignored for the data variables and only used for\n            coords.\n\n        Returns\n        -------\n        object : Dataset\n            New object with dimensions, attributes, coordinates, name, encoding,\n            and optionally data copied from original.\n\n        Examples\n        --------\n\n        Shallow copy versus deep copy\n\n        >>> da = xr.DataArray(np.random.randn(2, 3))\n        >>> ds = xr.Dataset(\n        ...     {""foo"": da, ""bar"": (""x"", [-1, 2])}, coords={""x"": [""one"", ""two""]},\n        ... )\n        >>> ds.copy()\n        <xarray.Dataset>\n        Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n        Coordinates:\n        * x        (x) <U3 \'one\' \'two\'\n        Dimensions without coordinates: dim_0, dim_1\n        Data variables:\n            foo      (dim_0, dim_1) float64 -0.8079 0.3897 -1.862 -0.6091 -1.051 -0.3003\n            bar      (x) int64 -1 2\n\n        >>> ds_0 = ds.copy(deep=False)\n        >>> ds_0[""foo""][0, 0] = 7\n        >>> ds_0\n        <xarray.Dataset>\n        Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n        Coordinates:\n        * x        (x) <U3 \'one\' \'two\'\n        Dimensions without coordinates: dim_0, dim_1\n        Data variables:\n            foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003\n            bar      (x) int64 -1 2\n\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n        Coordinates:\n        * x        (x) <U3 \'one\' \'two\'\n        Dimensions without coordinates: dim_0, dim_1\n        Data variables:\n            foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003\n            bar      (x) int64 -1 2\n\n        Changing the data using the ``data`` argument maintains the\n        structure of the original object, but with the new data. Original\n        object is unaffected.\n\n        >>> ds.copy(data={""foo"": np.arange(6).reshape(2, 3), ""bar"": [""a"", ""b""]})\n        <xarray.Dataset>\n        Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n        Coordinates:\n        * x        (x) <U3 \'one\' \'two\'\n        Dimensions without coordinates: dim_0, dim_1\n        Data variables:\n            foo      (dim_0, dim_1) int64 0 1 2 3 4 5\n            bar      (x) <U1 \'a\' \'b\'\n\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n        Coordinates:\n        * x        (x) <U3 \'one\' \'two\'\n        Dimensions without coordinates: dim_0, dim_1\n        Data variables:\n            foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003\n            bar      (x) int64 -1 2\n\n        See Also\n        --------\n        pandas.DataFrame.copy\n        """"""\n        if data is None:\n            variables = {k: v.copy(deep=deep) for k, v in self._variables.items()}\n        elif not utils.is_dict_like(data):\n            raise ValueError(""Data must be dict-like"")\n        else:\n            var_keys = set(self.data_vars.keys())\n            data_keys = set(data.keys())\n            keys_not_in_vars = data_keys - var_keys\n            if keys_not_in_vars:\n                raise ValueError(\n                    ""Data must only contain variables in original ""\n                    ""dataset. Extra variables: {}"".format(keys_not_in_vars)\n                )\n            keys_missing_from_data = var_keys - data_keys\n            if keys_missing_from_data:\n                raise ValueError(\n                    ""Data must contain all variables in original ""\n                    ""dataset. Data is missing {}"".format(keys_missing_from_data)\n                )\n            variables = {\n                k: v.copy(deep=deep, data=data.get(k))\n                for k, v in self._variables.items()\n            }\n\n        attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)\n\n        return self._replace(variables, attrs=attrs)\n\n    @property\n    def _level_coords(self) -> Dict[str, Hashable]:\n        """"""Return a mapping of all MultiIndex levels and their corresponding\n        coordinate name.\n        """"""\n        level_coords: Dict[str, Hashable] = {}\n        for name, index in self.indexes.items():\n            if isinstance(index, pd.MultiIndex):\n                level_names = index.names\n                (dim,) = self.variables[name].dims\n                level_coords.update({lname: dim for lname in level_names})\n        return level_coords\n\n    def _copy_listed(self, names: Iterable[Hashable]) -> ""Dataset"":\n        """"""Create a new Dataset with the listed variables from this dataset and\n        the all relevant coordinates. Skips all validation.\n        """"""\n        variables: Dict[Hashable, Variable] = {}\n        coord_names = set()\n        indexes: Dict[Hashable, pd.Index] = {}\n\n        for name in names:\n            try:\n                variables[name] = self._variables[name]\n            except KeyError:\n                ref_name, var_name, var = _get_virtual_variable(\n                    self._variables, name, self._level_coords, self.dims\n                )\n                variables[var_name] = var\n                if ref_name in self._coord_names or ref_name in self.dims:\n                    coord_names.add(var_name)\n                if (var_name,) == var.dims:\n                    indexes[var_name] = var.to_index()\n\n        needed_dims: Set[Hashable] = set()\n        for v in variables.values():\n            needed_dims.update(v.dims)\n\n        dims = {k: self.dims[k] for k in needed_dims}\n\n        for k in self._coord_names:\n            if set(self.variables[k].dims) <= needed_dims:\n                variables[k] = self._variables[k]\n                coord_names.add(k)\n                if k in self.indexes:\n                    indexes[k] = self.indexes[k]\n\n        return self._replace(variables, coord_names, dims, indexes=indexes)\n\n    def _construct_dataarray(self, name: Hashable) -> ""DataArray"":\n        """"""Construct a DataArray by indexing this dataset\n        """"""\n        from .dataarray import DataArray\n\n        try:\n            variable = self._variables[name]\n        except KeyError:\n            _, name, variable = _get_virtual_variable(\n                self._variables, name, self._level_coords, self.dims\n            )\n\n        needed_dims = set(variable.dims)\n\n        coords: Dict[Hashable, Variable] = {}\n        for k in self.coords:\n            if set(self.variables[k].dims) <= needed_dims:\n                coords[k] = self.variables[k]\n\n        if self._indexes is None:\n            indexes = None\n        else:\n            indexes = {k: v for k, v in self._indexes.items() if k in coords}\n\n        return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)\n\n    def __copy__(self) -> ""Dataset"":\n        return self.copy(deep=False)\n\n    def __deepcopy__(self, memo=None) -> ""Dataset"":\n        # memo does nothing but is required for compatibility with\n        # copy.deepcopy\n        return self.copy(deep=True)\n\n    @property\n    def _attr_sources(self) -> List[Mapping[Hashable, Any]]:\n        """"""List of places to look-up items for attribute-style access\n        """"""\n        return self._item_sources + [self.attrs]\n\n    @property\n    def _item_sources(self) -> List[Mapping[Hashable, Any]]:\n        """"""List of places to look-up items for key-completion\n        """"""\n        return [\n            self.data_vars,\n            self.coords,\n            {d: self[d] for d in self.dims},\n            LevelCoordinatesSource(self),\n        ]\n\n    def __contains__(self, key: object) -> bool:\n        """"""The \'in\' operator will return true or false depending on whether\n        \'key\' is an array in the dataset or not.\n        """"""\n        return key in self._variables\n\n    def __len__(self) -> int:\n        return len(self.data_vars)\n\n    def __bool__(self) -> bool:\n        return bool(self.data_vars)\n\n    def __iter__(self) -> Iterator[Hashable]:\n        return iter(self.data_vars)\n\n    def __array__(self, dtype=None):\n        raise TypeError(\n            ""cannot directly convert an xarray.Dataset into a ""\n            ""numpy array. Instead, create an xarray.DataArray ""\n            ""first, either with indexing on the Dataset or by ""\n            ""invoking the `to_array()` method.""\n        )\n\n    @property\n    def nbytes(self) -> int:\n        return sum(v.nbytes for v in self.variables.values())\n\n    @property\n    def loc(self) -> _LocIndexer:\n        """"""Attribute for location based indexing. Only supports __getitem__,\n        and only when the key is a dict of the form {dim: labels}.\n        """"""\n        return _LocIndexer(self)\n\n    def __getitem__(self, key: Any) -> ""Union[DataArray, Dataset]"":\n        """"""Access variables or coordinates this dataset as a\n        :py:class:`~xarray.DataArray`.\n\n        Indexing with a list of names will return a new ``Dataset`` object.\n        """"""\n        # TODO(shoyer): type this properly: https://github.com/python/mypy/issues/7328\n        if utils.is_dict_like(key):\n            return self.isel(**cast(Mapping, key))\n\n        if hashable(key):\n            return self._construct_dataarray(key)\n        else:\n            return self._copy_listed(np.asarray(key))\n\n    def __setitem__(self, key: Hashable, value) -> None:\n        """"""Add an array to this dataset.\n\n        If value is a `DataArray`, call its `select_vars()` method, rename it\n        to `key` and merge the contents of the resulting dataset into this\n        dataset.\n\n        If value is an `Variable` object (or tuple of form\n        ``(dims, data[, attrs])``), add it to this dataset as a new\n        variable.\n        """"""\n        if utils.is_dict_like(key):\n            raise NotImplementedError(\n                ""cannot yet use a dictionary as a key "" ""to set Dataset values""\n            )\n\n        self.update({key: value})\n\n    def __delitem__(self, key: Hashable) -> None:\n        """"""Remove a variable from this dataset.\n        """"""\n        del self._variables[key]\n        self._coord_names.discard(key)\n        if key in self.indexes:\n            assert self._indexes is not None\n            del self._indexes[key]\n        self._dims = calculate_dimensions(self._variables)\n\n    # mutable objects should not be hashable\n    # https://github.com/python/mypy/issues/4266\n    __hash__ = None  # type: ignore\n\n    def _all_compat(self, other: ""Dataset"", compat_str: str) -> bool:\n        """"""Helper function for equals and identical\n        """"""\n\n        # some stores (e.g., scipy) do not seem to preserve order, so don\'t\n        # require matching order for equality\n        def compat(x: Variable, y: Variable) -> bool:\n            return getattr(x, compat_str)(y)\n\n        return self._coord_names == other._coord_names and utils.dict_equiv(\n            self._variables, other._variables, compat=compat\n        )\n\n    def broadcast_equals(self, other: ""Dataset"") -> bool:\n        """"""Two Datasets are broadcast equal if they are equal after\n        broadcasting all variables against each other.\n\n        For example, variables that are scalar in one dataset but non-scalar in\n        the other dataset can still be broadcast equal if the the non-scalar\n        variable is a constant.\n\n        See Also\n        --------\n        Dataset.equals\n        Dataset.identical\n        """"""\n        try:\n            return self._all_compat(other, ""broadcast_equals"")\n        except (TypeError, AttributeError):\n            return False\n\n    def equals(self, other: ""Dataset"") -> bool:\n        """"""Two Datasets are equal if they have matching variables and\n        coordinates, all of which are equal.\n\n        Datasets can still be equal (like pandas objects) if they have NaN\n        values in the same locations.\n\n        This method is necessary because `v1 == v2` for ``Dataset``\n        does element-wise comparisons (like numpy.ndarrays).\n\n        See Also\n        --------\n        Dataset.broadcast_equals\n        Dataset.identical\n        """"""\n        try:\n            return self._all_compat(other, ""equals"")\n        except (TypeError, AttributeError):\n            return False\n\n    def identical(self, other: ""Dataset"") -> bool:\n        """"""Like equals, but also checks all dataset attributes and the\n        attributes on all variables and coordinates.\n\n        See Also\n        --------\n        Dataset.broadcast_equals\n        Dataset.equals\n        """"""\n        try:\n            return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(\n                other, ""identical""\n            )\n        except (TypeError, AttributeError):\n            return False\n\n    @property\n    def indexes(self) -> Indexes:\n        """"""Mapping of pandas.Index objects used for label based indexing\n        """"""\n        if self._indexes is None:\n            self._indexes = default_indexes(self._variables, self._dims)\n        return Indexes(self._indexes)\n\n    @property\n    def coords(self) -> DatasetCoordinates:\n        """"""Dictionary of xarray.DataArray objects corresponding to coordinate\n        variables\n        """"""\n        return DatasetCoordinates(self)\n\n    @property\n    def data_vars(self) -> DataVariables:\n        """"""Dictionary of DataArray objects corresponding to data variables\n        """"""\n        return DataVariables(self)\n\n    def set_coords(\n        self, names: ""Union[Hashable, Iterable[Hashable]]"", inplace: bool = None\n    ) -> ""Dataset"":\n        """"""Given names of one or more variables, set them as coordinates\n\n        Parameters\n        ----------\n        names : hashable or iterable of hashables\n            Name(s) of variables in this dataset to convert into coordinates.\n\n        Returns\n        -------\n        Dataset\n\n        See also\n        --------\n        Dataset.swap_dims\n        """"""\n        # TODO: allow inserting new coordinates with this method, like\n        # DataFrame.set_index?\n        # nb. check in self._variables, not self.data_vars to insure that the\n        # operation is idempotent\n        _check_inplace(inplace)\n        if isinstance(names, str) or not isinstance(names, Iterable):\n            names = [names]\n        else:\n            names = list(names)\n        self._assert_all_in_dataset(names)\n        obj = self.copy()\n        obj._coord_names.update(names)\n        return obj\n\n    def reset_coords(\n        self,\n        names: ""Union[Hashable, Iterable[Hashable], None]"" = None,\n        drop: bool = False,\n        inplace: bool = None,\n    ) -> ""Dataset"":\n        """"""Given names of coordinates, reset them to become variables\n\n        Parameters\n        ----------\n        names : hashable or iterable of hashables, optional\n            Name(s) of non-index coordinates in this dataset to reset into\n            variables. By default, all non-index coordinates are reset.\n        drop : bool, optional\n            If True, remove coordinates instead of converting them into\n            variables.\n\n        Returns\n        -------\n        Dataset\n        """"""\n        _check_inplace(inplace)\n        if names is None:\n            names = self._coord_names - set(self.dims)\n        else:\n            if isinstance(names, str) or not isinstance(names, Iterable):\n                names = [names]\n            else:\n                names = list(names)\n            self._assert_all_in_dataset(names)\n            bad_coords = set(names) & set(self.dims)\n            if bad_coords:\n                raise ValueError(\n                    ""cannot remove index coordinates with reset_coords: %s"" % bad_coords\n                )\n        obj = self.copy()\n        obj._coord_names.difference_update(names)\n        if drop:\n            for name in names:\n                del obj._variables[name]\n        return obj\n\n    def dump_to_store(self, store: ""AbstractDataStore"", **kwargs) -> None:\n        """"""Store dataset contents to a backends.*DataStore object.\n        """"""\n        from ..backends.api import dump_to_store\n\n        # TODO: rename and/or cleanup this method to make it more consistent\n        # with to_netcdf()\n        dump_to_store(self, store, **kwargs)\n\n    def to_netcdf(\n        self,\n        path=None,\n        mode: str = ""w"",\n        format: str = None,\n        group: str = None,\n        engine: str = None,\n        encoding: Mapping = None,\n        unlimited_dims: Iterable[Hashable] = None,\n        compute: bool = True,\n        invalid_netcdf: bool = False,\n    ) -> Union[bytes, ""Delayed"", None]:\n        """"""Write dataset contents to a netCDF file.\n\n        Parameters\n        ----------\n        path : str, Path or file-like object, optional\n            Path to which to save this dataset. File-like objects are only\n            supported by the scipy engine. If no path is provided, this\n            function returns the resulting netCDF file as bytes; in this case,\n            we need to use scipy, which does not support netCDF version 4 (the\n            default format becomes NETCDF3_64BIT).\n        mode : {\'w\', \'a\'}, optional\n            Write (\'w\') or append (\'a\') mode. If mode=\'w\', any existing file at\n            this location will be overwritten. If mode=\'a\', existing variables\n            will be overwritten.\n        format : {\'NETCDF4\', \'NETCDF4_CLASSIC\', \'NETCDF3_64BIT\',\n                  \'NETCDF3_CLASSIC\'}, optional\n            File format for the resulting netCDF file:\n\n            * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API\n              features.\n            * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only\n              netCDF 3 compatible API features.\n            * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,\n              which fully supports 2+ GB files, but is only compatible with\n              clients linked against netCDF version 3.6.0 or later.\n            * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not\n              handle 2+ GB files very well.\n\n            All formats are supported by the netCDF4-python library.\n            scipy.io.netcdf only supports the last two formats.\n\n            The default format is NETCDF4 if you are saving a file to disk and\n            have the netCDF4-python library available. Otherwise, xarray falls\n            back to using scipy to write netCDF files and defaults to the\n            NETCDF3_64BIT format (scipy does not support netCDF4).\n        group : str, optional\n            Path to the netCDF4 group in the given file to open (only works for\n            format=\'NETCDF4\'). The group(s) will be created if necessary.\n        engine : {\'netcdf4\', \'scipy\', \'h5netcdf\'}, optional\n            Engine to use when writing netCDF files. If not provided, the\n            default engine is chosen based on available dependencies, with a\n            preference for \'netcdf4\' if writing to a file on disk.\n        encoding : dict, optional\n            Nested dictionary with variable names as keys and dictionaries of\n            variable specific encodings as values, e.g.,\n            ``{\'my_variable\': {\'dtype\': \'int16\', \'scale_factor\': 0.1,\n            \'zlib\': True}, ...}``\n\n            The `h5netcdf` engine supports both the NetCDF4-style compression\n            encoding parameters ``{\'zlib\': True, \'complevel\': 9}`` and the h5py\n            ones ``{\'compression\': \'gzip\', \'compression_opts\': 9}``.\n            This allows using any compression plugin installed in the HDF5\n            library, e.g. LZF.\n\n        unlimited_dims : iterable of hashable, optional\n            Dimension(s) that should be serialized as unlimited dimensions.\n            By default, no dimensions are treated as unlimited dimensions.\n            Note that unlimited_dims may also be set via\n            ``dataset.encoding[\'unlimited_dims\']``.\n        compute: boolean\n            If true compute immediately, otherwise return a\n            ``dask.delayed.Delayed`` object that can be computed later.\n        invalid_netcdf: boolean\n            Only valid along with engine=\'h5netcdf\'. If True, allow writing\n            hdf5 files which are invalid netcdf as described in\n            https://github.com/shoyer/h5netcdf. Default: False.\n        """"""\n        if encoding is None:\n            encoding = {}\n        from ..backends.api import to_netcdf\n\n        return to_netcdf(\n            self,\n            path,\n            mode,\n            format=format,\n            group=group,\n            engine=engine,\n            encoding=encoding,\n            unlimited_dims=unlimited_dims,\n            compute=compute,\n            invalid_netcdf=invalid_netcdf,\n        )\n\n    def to_zarr(\n        self,\n        store: Union[MutableMapping, str, Path] = None,\n        mode: str = None,\n        synchronizer=None,\n        group: str = None,\n        encoding: Mapping = None,\n        compute: bool = True,\n        consolidated: bool = False,\n        append_dim: Hashable = None,\n    ) -> ""ZarrStore"":\n        """"""Write dataset contents to a zarr group.\n\n        .. note:: Experimental\n                  The Zarr backend is new and experimental. Please report any\n                  unexpected behavior via github issues.\n\n        Parameters\n        ----------\n        store : MutableMapping, str or Path, optional\n            Store or path to directory in file system.\n        mode : {\'w\', \'w-\', \'a\', None}\n            Persistence mode: \'w\' means create (overwrite if exists);\n            \'w-\' means create (fail if exists);\n            \'a\' means override existing variables (create if does not exist).\n            If ``append_dim`` is set, ``mode`` can be omitted as it is\n            internally set to ``\'a\'``. Otherwise, ``mode`` will default to\n            `w-` if not set.\n        synchronizer : object, optional\n            Array synchronizer\n        group : str, optional\n            Group path. (a.k.a. `path` in zarr terminology.)\n        encoding : dict, optional\n            Nested dictionary with variable names as keys and dictionaries of\n            variable specific encodings as values, e.g.,\n            ``{\'my_variable\': {\'dtype\': \'int16\', \'scale_factor\': 0.1,}, ...}``\n        compute: bool, optional\n            If True compute immediately, otherwise return a\n            ``dask.delayed.Delayed`` object that can be computed later.\n        consolidated: bool, optional\n            If True, apply zarr\'s `consolidate_metadata` function to the store\n            after writing.\n        append_dim: hashable, optional\n            If set, the dimension along which the data will be appended. All\n            other dimensions on overriden variables must remain the same size.\n\n        References\n        ----------\n        https://zarr.readthedocs.io/\n\n        Notes\n        -----\n        Zarr chunking behavior:\n            If chunks are found in the encoding argument or attribute\n            corresponding to any DataArray, those chunks are used.\n            If a DataArray is a dask array, it is written with those chunks.\n            If not other chunks are found, Zarr uses its own heuristics to\n            choose automatic chunk sizes.\n        """"""\n        if encoding is None:\n            encoding = {}\n        if (mode == ""a"") or (append_dim is not None):\n            if mode is None:\n                mode = ""a""\n            elif mode != ""a"":\n                raise ValueError(\n                    ""append_dim was set along with mode=\'{}\', either set ""\n                    ""mode=\'a\' or don\'t set it."".format(mode)\n                )\n        elif mode is None:\n            mode = ""w-""\n        if mode not in [""w"", ""w-"", ""a""]:\n            # TODO: figure out how to handle \'r+\'\n            raise ValueError(\n                ""The only supported options for mode are \'w\',"" ""\'w-\' and \'a\'.""\n            )\n        from ..backends.api import to_zarr\n\n        return to_zarr(\n            self,\n            store=store,\n            mode=mode,\n            synchronizer=synchronizer,\n            group=group,\n            encoding=encoding,\n            compute=compute,\n            consolidated=consolidated,\n            append_dim=append_dim,\n        )\n\n    def __repr__(self) -> str:\n        return formatting.dataset_repr(self)\n\n    def _repr_html_(self):\n        if OPTIONS[""display_style""] == ""text"":\n            return f""<pre>{escape(repr(self))}</pre>""\n        return formatting_html.dataset_repr(self)\n\n    def info(self, buf=None) -> None:\n        """"""\n        Concise summary of a Dataset variables and attributes.\n\n        Parameters\n        ----------\n        buf : writable buffer, defaults to sys.stdout\n\n        See Also\n        --------\n        pandas.DataFrame.assign\n        ncdump: netCDF\'s ncdump\n        """"""\n        if buf is None:  # pragma: no cover\n            buf = sys.stdout\n\n        lines = []\n        lines.append(""xarray.Dataset {"")\n        lines.append(""dimensions:"")\n        for name, size in self.dims.items():\n            lines.append(f""\\t{name} = {size} ;"")\n        lines.append(""\\nvariables:"")\n        for name, da in self.variables.items():\n            dims = "", "".join(da.dims)\n            lines.append(f""\\t{da.dtype} {name}({dims}) ;"")\n            for k, v in da.attrs.items():\n                lines.append(f""\\t\\t{name}:{k} = {v} ;"")\n        lines.append(""\\n// global attributes:"")\n        for k, v in self.attrs.items():\n            lines.append(f""\\t:{k} = {v} ;"")\n        lines.append(""}"")\n\n        buf.write(""\\n"".join(lines))\n\n    @property\n    def chunks(self) -> Mapping[Hashable, Tuple[int, ...]]:\n        """"""Block dimensions for this dataset\'s data or None if it\'s not a dask\n        array.\n        """"""\n        chunks: Dict[Hashable, Tuple[int, ...]] = {}\n        for v in self.variables.values():\n            if v.chunks is not None:\n                for dim, c in zip(v.dims, v.chunks):\n                    if dim in chunks and c != chunks[dim]:\n                        raise ValueError(\n                            f""Object has inconsistent chunks along dimension {dim}. ""\n                            ""This can be fixed by calling unify_chunks().""\n                        )\n                    chunks[dim] = c\n        return Frozen(SortedKeysDict(chunks))\n\n    def chunk(\n        self,\n        chunks: Union[\n            None,\n            Number,\n            str,\n            Mapping[Hashable, Union[None, Number, str, Tuple[Number, ...]]],\n        ] = None,\n        name_prefix: str = ""xarray-"",\n        token: str = None,\n        lock: bool = False,\n    ) -> ""Dataset"":\n        """"""Coerce all arrays in this dataset into dask arrays with the given\n        chunks.\n\n        Non-dask arrays in this dataset will be converted to dask arrays. Dask\n        arrays will be rechunked to the given chunk sizes.\n\n        If neither chunks is not provided for one or more dimensions, chunk\n        sizes along that dimension will not be updated; non-dask arrays will be\n        converted into dask arrays with a single block.\n\n        Parameters\n        ----------\n        chunks : int, \'auto\' or mapping, optional\n            Chunk sizes along each dimension, e.g., ``5`` or\n            ``{\'x\': 5, \'y\': 5}``.\n        name_prefix : str, optional\n            Prefix for the name of any new dask arrays.\n        token : str, optional\n            Token uniquely identifying this dataset.\n        lock : optional\n            Passed on to :py:func:`dask.array.from_array`, if the array is not\n            already as dask array.\n\n        Returns\n        -------\n        chunked : xarray.Dataset\n        """"""\n        from dask.base import tokenize\n\n        if isinstance(chunks, (Number, str)):\n            chunks = dict.fromkeys(self.dims, chunks)\n\n        if chunks is not None:\n            bad_dims = chunks.keys() - self.dims.keys()\n            if bad_dims:\n                raise ValueError(\n                    ""some chunks keys are not dimensions on this ""\n                    ""object: %s"" % bad_dims\n                )\n\n        def selkeys(dict_, keys):\n            if dict_ is None:\n                return None\n            return {d: dict_[d] for d in keys if d in dict_}\n\n        def maybe_chunk(name, var, chunks):\n            chunks = selkeys(chunks, var.dims)\n            if not chunks:\n                chunks = None\n            if var.ndim > 0:\n                # when rechunking by different amounts, make sure dask names change\n                # by provinding chunks as an input to tokenize.\n                # subtle bugs result otherwise. see GH3350\n                token2 = tokenize(name, token if token else var._data, chunks)\n                name2 = f""{name_prefix}{name}-{token2}""\n                return var.chunk(chunks, name=name2, lock=lock)\n            else:\n                return var\n\n        variables = {k: maybe_chunk(k, v, chunks) for k, v in self.variables.items()}\n        return self._replace(variables)\n\n    def _validate_indexers(\n        self, indexers: Mapping[Hashable, Any], missing_dims: str = ""raise""\n    ) -> Iterator[Tuple[Hashable, Union[int, slice, np.ndarray, Variable]]]:\n        """""" Here we make sure\n        + indexer has a valid keys\n        + indexer is in a valid data type\n        + string indexers are cast to the appropriate date type if the\n          associated index is a DatetimeIndex or CFTimeIndex\n        """"""\n        from .dataarray import DataArray\n\n        indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n\n        # all indexers should be int, slice, np.ndarrays, or Variable\n        for k, v in indexers.items():\n            if isinstance(v, (int, slice, Variable)):\n                yield k, v\n            elif isinstance(v, DataArray):\n                yield k, v.variable\n            elif isinstance(v, tuple):\n                yield k, as_variable(v)\n            elif isinstance(v, Dataset):\n                raise TypeError(""cannot use a Dataset as an indexer"")\n            elif isinstance(v, Sequence) and len(v) == 0:\n                yield k, np.empty((0,), dtype=""int64"")\n            else:\n                v = np.asarray(v)\n\n                if v.dtype.kind in ""US"":\n                    index = self.indexes[k]\n                    if isinstance(index, pd.DatetimeIndex):\n                        v = v.astype(""datetime64[ns]"")\n                    elif isinstance(index, xr.CFTimeIndex):\n                        v = _parse_array_of_cftime_strings(v, index.date_type)\n\n                if v.ndim > 1:\n                    raise IndexError(\n                        ""Unlabeled multi-dimensional array cannot be ""\n                        ""used for indexing: {}"".format(k)\n                    )\n                yield k, v\n\n    def _validate_interp_indexers(\n        self, indexers: Mapping[Hashable, Any]\n    ) -> Iterator[Tuple[Hashable, Variable]]:\n        """"""Variant of _validate_indexers to be used for interpolation\n        """"""\n        for k, v in self._validate_indexers(indexers):\n            if isinstance(v, Variable):\n                if v.ndim == 1:\n                    yield k, v.to_index_variable()\n                else:\n                    yield k, v\n            elif isinstance(v, int):\n                yield k, Variable((), v)\n            elif isinstance(v, np.ndarray):\n                if v.ndim == 0:\n                    yield k, Variable((), v)\n                elif v.ndim == 1:\n                    yield k, IndexVariable((k,), v)\n                else:\n                    raise AssertionError()  # Already tested by _validate_indexers\n            else:\n                raise TypeError(type(v))\n\n    def _get_indexers_coords_and_indexes(self, indexers):\n        """"""Extract coordinates and indexes from indexers.\n\n        Only coordinate with a name different from any of self.variables will\n        be attached.\n        """"""\n        from .dataarray import DataArray\n\n        coords_list = []\n        for k, v in indexers.items():\n            if isinstance(v, DataArray):\n                if v.dtype.kind == ""b"":\n                    if v.ndim != 1:  # we only support 1-d boolean array\n                        raise ValueError(\n                            ""{:d}d-boolean array is used for indexing along ""\n                            ""dimension {!r}, but only 1d boolean arrays are ""\n                            ""supported."".format(v.ndim, k)\n                        )\n                    # Make sure in case of boolean DataArray, its\n                    # coordinate also should be indexed.\n                    v_coords = v[v.values.nonzero()[0]].coords\n                else:\n                    v_coords = v.coords\n                coords_list.append(v_coords)\n\n        # we don\'t need to call align() explicitly or check indexes for\n        # alignment, because merge_variables already checks for exact alignment\n        # between dimension coordinates\n        coords, indexes = merge_coordinates_without_align(coords_list)\n        assert_coordinate_consistent(self, coords)\n\n        # silently drop the conflicted variables.\n        attached_coords = {k: v for k, v in coords.items() if k not in self._variables}\n        attached_indexes = {\n            k: v for k, v in indexes.items() if k not in self._variables\n        }\n        return attached_coords, attached_indexes\n\n    def isel(\n        self,\n        indexers: Mapping[Hashable, Any] = None,\n        drop: bool = False,\n        missing_dims: str = ""raise"",\n        **indexers_kwargs: Any,\n    ) -> ""Dataset"":\n        """"""Returns a new dataset with each array indexed along the specified\n        dimension(s).\n\n        This method selects values from each array using its `__getitem__`\n        method, except this method does not require knowing the order of\n        each array\'s dimensions.\n\n        Parameters\n        ----------\n        indexers : dict, optional\n            A dict with keys matching dimensions and values given\n            by integers, slice objects or arrays.\n            indexer can be a integer, slice, array-like or DataArray.\n            If DataArrays are passed as indexers, xarray-style indexing will be\n            carried out. See :ref:`indexing` for the details.\n            One of indexers or indexers_kwargs must be provided.\n        drop : bool, optional\n            If ``drop=True``, drop coordinates variables indexed by integers\n            instead of making them scalar.\n        missing_dims : {""raise"", ""warn"", ""ignore""}, default ""raise""\n            What to do if dimensions that should be selected from are not present in the\n            Dataset:\n            - ""exception"": raise an exception\n            - ""warning"": raise a warning, and ignore the missing dimensions\n            - ""ignore"": ignore the missing dimensions\n        **indexers_kwargs : {dim: indexer, ...}, optional\n            The keyword arguments form of ``indexers``.\n            One of indexers or indexers_kwargs must be provided.\n\n        Returns\n        -------\n        obj : Dataset\n            A new Dataset with the same contents as this dataset, except each\n            array and dimension is indexed by the appropriate indexers.\n            If indexer DataArrays have coordinates that do not conflict with\n            this object, then these coordinates will be attached.\n            In general, each array\'s data will be a view of the array\'s data\n            in this dataset, unless vectorized indexing was triggered by using\n            an array indexer, in which case the data will be a copy.\n\n        See Also\n        --------\n        Dataset.sel\n        DataArray.isel\n        """"""\n        indexers = either_dict_or_kwargs(indexers, indexers_kwargs, ""isel"")\n        if any(is_fancy_indexer(idx) for idx in indexers.values()):\n            return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)\n\n        # Much faster algorithm for when all indexers are ints, slices, one-dimensional\n        # lists, or zero or one-dimensional np.ndarray\'s\n        indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n\n        variables = {}\n        dims: Dict[Hashable, Tuple[int, ...]] = {}\n        coord_names = self._coord_names.copy()\n        indexes = self._indexes.copy() if self._indexes is not None else None\n\n        for var_name, var_value in self._variables.items():\n            var_indexers = {k: v for k, v in indexers.items() if k in var_value.dims}\n            if var_indexers:\n                var_value = var_value.isel(var_indexers)\n                if drop and var_value.ndim == 0 and var_name in coord_names:\n                    coord_names.remove(var_name)\n                    if indexes:\n                        indexes.pop(var_name, None)\n                    continue\n                if indexes and var_name in indexes:\n                    if var_value.ndim == 1:\n                        indexes[var_name] = var_value.to_index()\n                    else:\n                        del indexes[var_name]\n            variables[var_name] = var_value\n            dims.update(zip(var_value.dims, var_value.shape))\n\n        return self._construct_direct(\n            variables=variables,\n            coord_names=coord_names,\n            dims=dims,\n            attrs=self._attrs,\n            indexes=indexes,\n            encoding=self._encoding,\n            file_obj=self._file_obj,\n        )\n\n    def _isel_fancy(\n        self,\n        indexers: Mapping[Hashable, Any],\n        *,\n        drop: bool,\n        missing_dims: str = ""raise"",\n    ) -> ""Dataset"":\n        # Note: we need to preserve the original indexers variable in order to merge the\n        # coords below\n        indexers_list = list(self._validate_indexers(indexers, missing_dims))\n\n        variables: Dict[Hashable, Variable] = {}\n        indexes: Dict[Hashable, pd.Index] = {}\n\n        for name, var in self.variables.items():\n            var_indexers = {k: v for k, v in indexers_list if k in var.dims}\n            if drop and name in var_indexers:\n                continue  # drop this variable\n\n            if name in self.indexes:\n                new_var, new_index = isel_variable_and_index(\n                    name, var, self.indexes[name], var_indexers\n                )\n                if new_index is not None:\n                    indexes[name] = new_index\n            elif var_indexers:\n                new_var = var.isel(indexers=var_indexers)\n            else:\n                new_var = var.copy(deep=False)\n\n            variables[name] = new_var\n\n        coord_names = self._coord_names & variables.keys()\n        selected = self._replace_with_new_dims(variables, coord_names, indexes)\n\n        # Extract coordinates from indexers\n        coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)\n        variables.update(coord_vars)\n        indexes.update(new_indexes)\n        coord_names = self._coord_names & variables.keys() | coord_vars.keys()\n        return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n\n    def sel(\n        self,\n        indexers: Mapping[Hashable, Any] = None,\n        method: str = None,\n        tolerance: Number = None,\n        drop: bool = False,\n        **indexers_kwargs: Any,\n    ) -> ""Dataset"":\n        """"""Returns a new dataset with each array indexed by tick labels\n        along the specified dimension(s).\n\n        In contrast to `Dataset.isel`, indexers for this method should use\n        labels instead of integers.\n\n        Under the hood, this method is powered by using pandas\'s powerful Index\n        objects. This makes label based indexing essentially just as fast as\n        using integer indexing.\n\n        It also means this method uses pandas\'s (well documented) logic for\n        indexing. This means you can use string shortcuts for datetime indexes\n        (e.g., \'2000-01\' to select all values in January 2000). It also means\n        that slices are treated as inclusive of both the start and stop values,\n        unlike normal Python indexing.\n\n        Parameters\n        ----------\n        indexers : dict, optional\n            A dict with keys matching dimensions and values given\n            by scalars, slices or arrays of tick labels. For dimensions with\n            multi-index, the indexer may also be a dict-like object with keys\n            matching index level names.\n            If DataArrays are passed as indexers, xarray-style indexing will be\n            carried out. See :ref:`indexing` for the details.\n            One of indexers or indexers_kwargs must be provided.\n        method : {None, \'nearest\', \'pad\'/\'ffill\', \'backfill\'/\'bfill\'}, optional\n            Method to use for inexact matches:\n\n            * None (default): only exact matches\n            * pad / ffill: propagate last valid index value forward\n            * backfill / bfill: propagate next valid index value backward\n            * nearest: use nearest valid index value\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations must\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n        drop : bool, optional\n            If ``drop=True``, drop coordinates variables in `indexers` instead\n            of making them scalar.\n        **indexers_kwargs : {dim: indexer, ...}, optional\n            The keyword arguments form of ``indexers``.\n            One of indexers or indexers_kwargs must be provided.\n\n        Returns\n        -------\n        obj : Dataset\n            A new Dataset with the same contents as this dataset, except each\n            variable and dimension is indexed by the appropriate indexers.\n            If indexer DataArrays have coordinates that do not conflict with\n            this object, then these coordinates will be attached.\n            In general, each array\'s data will be a view of the array\'s data\n            in this dataset, unless vectorized indexing was triggered by using\n            an array indexer, in which case the data will be a copy.\n\n\n        See Also\n        --------\n        Dataset.isel\n        DataArray.sel\n        """"""\n        indexers = either_dict_or_kwargs(indexers, indexers_kwargs, ""sel"")\n        pos_indexers, new_indexes = remap_label_indexers(\n            self, indexers=indexers, method=method, tolerance=tolerance\n        )\n        result = self.isel(indexers=pos_indexers, drop=drop)\n        return result._overwrite_indexes(new_indexes)\n\n    def head(\n        self,\n        indexers: Union[Mapping[Hashable, int], int] = None,\n        **indexers_kwargs: Any,\n    ) -> ""Dataset"":\n        """"""Returns a new dataset with the first `n` values of each array\n        for the specified dimension(s).\n\n        Parameters\n        ----------\n        indexers : dict or int, default: 5\n            A dict with keys matching dimensions and integer values `n`\n            or a single integer `n` applied over all dimensions.\n            One of indexers or indexers_kwargs must be provided.\n        **indexers_kwargs : {dim: n, ...}, optional\n            The keyword arguments form of ``indexers``.\n            One of indexers or indexers_kwargs must be provided.\n\n\n        See Also\n        --------\n        Dataset.tail\n        Dataset.thin\n        DataArray.head\n        """"""\n        if not indexers_kwargs:\n            if indexers is None:\n                indexers = 5\n            if not isinstance(indexers, int) and not is_dict_like(indexers):\n                raise TypeError(""indexers must be either dict-like or a single integer"")\n        if isinstance(indexers, int):\n            indexers = {dim: indexers for dim in self.dims}\n        indexers = either_dict_or_kwargs(indexers, indexers_kwargs, ""head"")\n        for k, v in indexers.items():\n            if not isinstance(v, int):\n                raise TypeError(\n                    ""expected integer type indexer for ""\n                    ""dimension %r, found %r"" % (k, type(v))\n                )\n            elif v < 0:\n                raise ValueError(\n                    ""expected positive integer as indexer ""\n                    ""for dimension %r, found %s"" % (k, v)\n                )\n        indexers_slices = {k: slice(val) for k, val in indexers.items()}\n        return self.isel(indexers_slices)\n\n    def tail(\n        self,\n        indexers: Union[Mapping[Hashable, int], int] = None,\n        **indexers_kwargs: Any,\n    ) -> ""Dataset"":\n        """"""Returns a new dataset with the last `n` values of each array\n        for the specified dimension(s).\n\n        Parameters\n        ----------\n        indexers : dict or int, default: 5\n            A dict with keys matching dimensions and integer values `n`\n            or a single integer `n` applied over all dimensions.\n            One of indexers or indexers_kwargs must be provided.\n        **indexers_kwargs : {dim: n, ...}, optional\n            The keyword arguments form of ``indexers``.\n            One of indexers or indexers_kwargs must be provided.\n\n\n        See Also\n        --------\n        Dataset.head\n        Dataset.thin\n        DataArray.tail\n        """"""\n        if not indexers_kwargs:\n            if indexers is None:\n                indexers = 5\n            if not isinstance(indexers, int) and not is_dict_like(indexers):\n                raise TypeError(""indexers must be either dict-like or a single integer"")\n        if isinstance(indexers, int):\n            indexers = {dim: indexers for dim in self.dims}\n        indexers = either_dict_or_kwargs(indexers, indexers_kwargs, ""tail"")\n        for k, v in indexers.items():\n            if not isinstance(v, int):\n                raise TypeError(\n                    ""expected integer type indexer for ""\n                    ""dimension %r, found %r"" % (k, type(v))\n                )\n            elif v < 0:\n                raise ValueError(\n                    ""expected positive integer as indexer ""\n                    ""for dimension %r, found %s"" % (k, v)\n                )\n        indexers_slices = {\n            k: slice(-val, None) if val != 0 else slice(val)\n            for k, val in indexers.items()\n        }\n        return self.isel(indexers_slices)\n\n    def thin(\n        self,\n        indexers: Union[Mapping[Hashable, int], int] = None,\n        **indexers_kwargs: Any,\n    ) -> ""Dataset"":\n        """"""Returns a new dataset with each array indexed along every `n`-th\n        value for the specified dimension(s)\n\n        Parameters\n        ----------\n        indexers : dict or int\n            A dict with keys matching dimensions and integer values `n`\n            or a single integer `n` applied over all dimensions.\n            One of indexers or indexers_kwargs must be provided.\n        ``**indexers_kwargs`` : {dim: n, ...}, optional\n            The keyword arguments form of ``indexers``.\n            One of indexers or indexers_kwargs must be provided.\n\n\n        See Also\n        --------\n        Dataset.head\n        Dataset.tail\n        DataArray.thin\n        """"""\n        if (\n            not indexers_kwargs\n            and not isinstance(indexers, int)\n            and not is_dict_like(indexers)\n        ):\n            raise TypeError(""indexers must be either dict-like or a single integer"")\n        if isinstance(indexers, int):\n            indexers = {dim: indexers for dim in self.dims}\n        indexers = either_dict_or_kwargs(indexers, indexers_kwargs, ""thin"")\n        for k, v in indexers.items():\n            if not isinstance(v, int):\n                raise TypeError(\n                    ""expected integer type indexer for ""\n                    ""dimension %r, found %r"" % (k, type(v))\n                )\n            elif v < 0:\n                raise ValueError(\n                    ""expected positive integer as indexer ""\n                    ""for dimension %r, found %s"" % (k, v)\n                )\n            elif v == 0:\n                raise ValueError(""step cannot be zero"")\n        indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}\n        return self.isel(indexers_slices)\n\n    def broadcast_like(\n        self, other: Union[""Dataset"", ""DataArray""], exclude: Iterable[Hashable] = None\n    ) -> ""Dataset"":\n        """"""Broadcast this DataArray against another Dataset or DataArray.\n        This is equivalent to xr.broadcast(other, self)[1]\n\n        Parameters\n        ----------\n        other : Dataset or DataArray\n            Object against which to broadcast this array.\n        exclude : iterable of hashable, optional\n            Dimensions that must not be broadcasted\n\n        """"""\n        if exclude is None:\n            exclude = set()\n        else:\n            exclude = set(exclude)\n        args = align(other, self, join=""outer"", copy=False, exclude=exclude)\n\n        dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\n\n        return _broadcast_helper(args[1], exclude, dims_map, common_coords)\n\n    def reindex_like(\n        self,\n        other: Union[""Dataset"", ""DataArray""],\n        method: str = None,\n        tolerance: Number = None,\n        copy: bool = True,\n        fill_value: Any = dtypes.NA,\n    ) -> ""Dataset"":\n        """"""Conform this object onto the indexes of another object, filling in\n        missing values with ``fill_value``. The default fill value is NaN.\n\n        Parameters\n        ----------\n        other : Dataset or DataArray\n            Object with an \'indexes\' attribute giving a mapping from dimension\n            names to pandas.Index objects, which provides coordinates upon\n            which to index the variables in this dataset. The indexes on this\n            other object need not be the same as the indexes on this\n            dataset. Any mis-matched index values will be filled in with\n            NaN, and any mis-matched dimension names will simply be ignored.\n        method : {None, \'nearest\', \'pad\'/\'ffill\', \'backfill\'/\'bfill\'}, optional\n            Method to use for filling index values from other not found in this\n            dataset:\n\n            * None (default): don\'t fill gaps\n            * pad / ffill: propagate last valid index value forward\n            * backfill / bfill: propagate next valid index value backward\n            * nearest: use nearest valid index value\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations must\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n        copy : bool, optional\n            If ``copy=True``, data in the return value is always copied. If\n            ``copy=False`` and reindexing is unnecessary, or can be performed\n            with only slice operations, then the output may share memory with\n            the input. In either case, a new xarray object is always returned.\n        fill_value : scalar, optional\n            Value to use for newly missing values\n\n        Returns\n        -------\n        reindexed : Dataset\n            Another dataset, with this dataset\'s data but coordinates from the\n            other object.\n\n        See Also\n        --------\n        Dataset.reindex\n        align\n        """"""\n        indexers = alignment.reindex_like_indexers(self, other)\n        return self.reindex(\n            indexers=indexers,\n            method=method,\n            copy=copy,\n            fill_value=fill_value,\n            tolerance=tolerance,\n        )\n\n    def reindex(\n        self,\n        indexers: Mapping[Hashable, Any] = None,\n        method: str = None,\n        tolerance: Number = None,\n        copy: bool = True,\n        fill_value: Any = dtypes.NA,\n        **indexers_kwargs: Any,\n    ) -> ""Dataset"":\n        """"""Conform this object onto a new set of indexes, filling in\n        missing values with ``fill_value``. The default fill value is NaN.\n\n        Parameters\n        ----------\n        indexers : dict. optional\n            Dictionary with keys given by dimension names and values given by\n            arrays of coordinates tick labels. Any mis-matched coordinate\n            values will be filled in with NaN, and any mis-matched dimension\n            names will simply be ignored.\n            One of indexers or indexers_kwargs must be provided.\n        method : {None, \'nearest\', \'pad\'/\'ffill\', \'backfill\'/\'bfill\'}, optional\n            Method to use for filling index values in ``indexers`` not found in\n            this dataset:\n\n            * None (default): don\'t fill gaps\n            * pad / ffill: propagate last valid index value forward\n            * backfill / bfill: propagate next valid index value backward\n            * nearest: use nearest valid index value\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations must\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n        copy : bool, optional\n            If ``copy=True``, data in the return value is always copied. If\n            ``copy=False`` and reindexing is unnecessary, or can be performed\n            with only slice operations, then the output may share memory with\n            the input. In either case, a new xarray object is always returned.\n        fill_value : scalar, optional\n            Value to use for newly missing values\n        sparse: use sparse-array. By default, False\n        **indexers_kwargs : {dim: indexer, ...}, optional\n            Keyword arguments in the same form as ``indexers``.\n            One of indexers or indexers_kwargs must be provided.\n\n        Returns\n        -------\n        reindexed : Dataset\n            Another dataset, with this dataset\'s data but replaced coordinates.\n\n        See Also\n        --------\n        Dataset.reindex_like\n        align\n        pandas.Index.get_indexer\n\n        Examples\n        --------\n\n        Create a dataset with some fictional data.\n\n        >>> import xarray as xr\n        >>> import pandas as pd\n        >>> x = xr.Dataset(\n        ...     {\n        ...         ""temperature"": (""station"", 20 * np.random.rand(4)),\n        ...         ""pressure"": (""station"", 500 * np.random.rand(4)),\n        ...     },\n        ...     coords={""station"": [""boston"", ""nyc"", ""seattle"", ""denver""]},\n        ... )\n        >>> x\n        <xarray.Dataset>\n        Dimensions:      (station: 4)\n        Coordinates:\n        * station      (station) <U7 \'boston\' \'nyc\' \'seattle\' \'denver\'\n        Data variables:\n            temperature  (station) float64 18.84 14.59 19.22 17.16\n            pressure     (station) float64 324.1 194.3 122.8 244.3\n        >>> x.indexes\n        station: Index([\'boston\', \'nyc\', \'seattle\', \'denver\'], dtype=\'object\', name=\'station\')\n\n        Create a new index and reindex the dataset. By default values in the new index that\n        do not have corresponding records in the dataset are assigned `NaN`.\n\n        >>> new_index = [""boston"", ""austin"", ""seattle"", ""lincoln""]\n        >>> x.reindex({""station"": new_index})\n        <xarray.Dataset>\n        Dimensions:      (station: 4)\n        Coordinates:\n        * station      (station) object \'boston\' \'austin\' \'seattle\' \'lincoln\'\n        Data variables:\n            temperature  (station) float64 18.84 nan 19.22 nan\n            pressure     (station) float64 324.1 nan 122.8 nan\n\n        We can fill in the missing values by passing a value to the keyword `fill_value`.\n\n        >>> x.reindex({""station"": new_index}, fill_value=0)\n        <xarray.Dataset>\n        Dimensions:      (station: 4)\n        Coordinates:\n        * station      (station) object \'boston\' \'austin\' \'seattle\' \'lincoln\'\n        Data variables:\n            temperature  (station) float64 18.84 0.0 19.22 0.0\n            pressure     (station) float64 324.1 0.0 122.8 0.0\n\n        Because the index is not monotonically increasing or decreasing, we cannot use arguments\n        to the keyword method to fill the `NaN` values.\n\n        >>> x.reindex({""station"": new_index}, method=""nearest"")\n        Traceback (most recent call last):\n        ...\n            raise ValueError(\'index must be monotonic increasing or decreasing\')\n        ValueError: index must be monotonic increasing or decreasing\n\n        To further illustrate the filling functionality in reindex, we will create a\n        dataset with a monotonically increasing index (for example, a sequence of dates).\n\n        >>> x2 = xr.Dataset(\n        ...     {\n        ...         ""temperature"": (\n        ...             ""time"",\n        ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],\n        ...         ),\n        ...         ""pressure"": (""time"", 500 * np.random.rand(6)),\n        ...     },\n        ...     coords={""time"": pd.date_range(""01/01/2019"", periods=6, freq=""D"")},\n        ... )\n        >>> x2\n        <xarray.Dataset>\n        Dimensions:      (time: 6)\n        Coordinates:\n        * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06\n        Data variables:\n            temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12\n            pressure     (time) float64 103.4 122.7 452.0 444.0 399.2 486.0\n\n        Suppose we decide to expand the dataset to cover a wider date range.\n\n        >>> time_index2 = pd.date_range(""12/29/2018"", periods=10, freq=""D"")\n        >>> x2.reindex({""time"": time_index2})\n        <xarray.Dataset>\n        Dimensions:      (time: 10)\n        Coordinates:\n        * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07\n        Data variables:\n            temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan\n            pressure     (time) float64 nan nan nan 103.4 ... 444.0 399.2 486.0 nan\n\n        The index entries that did not have a value in the original data frame (for example, `2018-12-29`)\n        are by default filled with NaN. If desired, we can fill in the missing values using one of several options.\n\n        For example, to back-propagate the last valid value to fill the `NaN` values,\n        pass `bfill` as an argument to the `method` keyword.\n\n        >>> x3 = x2.reindex({""time"": time_index2}, method=""bfill"")\n        >>> x3\n        <xarray.Dataset>\n        Dimensions:      (time: 10)\n        Coordinates:\n        * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07\n        Data variables:\n            temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan\n            pressure     (time) float64 103.4 103.4 103.4 103.4 ... 399.2 486.0 nan\n\n        Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)\n        will not be filled by any of the value propagation schemes.\n\n        >>> x2.where(x2.temperature.isnull(), drop=True)\n        <xarray.Dataset>\n        Dimensions:      (time: 1)\n        Coordinates:\n        * time         (time) datetime64[ns] 2019-01-03\n        Data variables:\n            temperature  (time) float64 nan\n            pressure     (time) float64 452.0\n        >>> x3.where(x3.temperature.isnull(), drop=True)\n        <xarray.Dataset>\n        Dimensions:      (time: 2)\n        Coordinates:\n        * time         (time) datetime64[ns] 2019-01-03 2019-01-07\n        Data variables:\n            temperature  (time) float64 nan nan\n            pressure     (time) float64 452.0 nan\n\n        This is because filling while reindexing does not look at dataset values, but only compares\n        the original and desired indexes. If you do want to fill in the `NaN` values present in the\n        original dataset, use the :py:meth:`~Dataset.fillna()` method.\n\n        """"""\n        return self._reindex(\n            indexers,\n            method,\n            tolerance,\n            copy,\n            fill_value,\n            sparse=False,\n            **indexers_kwargs,\n        )\n\n    def _reindex(\n        self,\n        indexers: Mapping[Hashable, Any] = None,\n        method: str = None,\n        tolerance: Number = None,\n        copy: bool = True,\n        fill_value: Any = dtypes.NA,\n        sparse: bool = False,\n        **indexers_kwargs: Any,\n    ) -> ""Dataset"":\n        """"""\n        same to _reindex but support sparse option\n        """"""\n        indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, ""reindex"")\n\n        bad_dims = [d for d in indexers if d not in self.dims]\n        if bad_dims:\n            raise ValueError(""invalid reindex dimensions: %s"" % bad_dims)\n\n        variables, indexes = alignment.reindex_variables(\n            self.variables,\n            self.sizes,\n            self.indexes,\n            indexers,\n            method,\n            tolerance,\n            copy=copy,\n            fill_value=fill_value,\n            sparse=sparse,\n        )\n        coord_names = set(self._coord_names)\n        coord_names.update(indexers)\n        return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n\n    def interp(\n        self,\n        coords: Mapping[Hashable, Any] = None,\n        method: str = ""linear"",\n        assume_sorted: bool = False,\n        kwargs: Mapping[str, Any] = None,\n        **coords_kwargs: Any,\n    ) -> ""Dataset"":\n        """""" Multidimensional interpolation of Dataset.\n\n        Parameters\n        ----------\n        coords : dict, optional\n            Mapping from dimension names to the new coordinates.\n            New coordinate can be a scalar, array-like or DataArray.\n            If DataArrays are passed as new coordates, their dimensions are\n            used for the broadcasting.\n        method: string, optional.\n            {\'linear\', \'nearest\'} for multidimensional array,\n            {\'linear\', \'nearest\', \'zero\', \'slinear\', \'quadratic\', \'cubic\'}\n            for 1-dimensional array. \'linear\' is used by default.\n        assume_sorted: boolean, optional\n            If False, values of coordinates that are interpolated over can be\n            in any order and they are sorted first. If True, interpolated\n            coordinates are assumed to be an array of monotonically increasing\n            values.\n        kwargs: dictionary, optional\n            Additional keyword arguments passed to scipy\'s interpolator. Valid\n            options and their behavior depend on if 1-dimensional or\n            multi-dimensional interpolation is used.\n        **coords_kwargs : {dim: coordinate, ...}, optional\n            The keyword arguments form of ``coords``.\n            One of coords or coords_kwargs must be provided.\n\n        Returns\n        -------\n        interpolated: xr.Dataset\n            New dataset on the new coordinates.\n\n        Notes\n        -----\n        scipy is required.\n\n        See Also\n        --------\n        scipy.interpolate.interp1d\n        scipy.interpolate.interpn\n        """"""\n        from . import missing\n\n        if kwargs is None:\n            kwargs = {}\n\n        coords = either_dict_or_kwargs(coords, coords_kwargs, ""interp"")\n        indexers = dict(self._validate_interp_indexers(coords))\n\n        if coords:\n            # This avoids broadcasting over coordinates that are both in\n            # the original array AND in the indexing array. It essentially\n            # forces interpolation along the shared coordinates.\n            sdims = (\n                set(self.dims)\n                .intersection(*[set(nx.dims) for nx in indexers.values()])\n                .difference(coords.keys())\n            )\n            indexers.update({d: self.variables[d] for d in sdims})\n\n        obj = self if assume_sorted else self.sortby([k for k in coords])\n\n        def maybe_variable(obj, k):\n            # workaround to get variable for dimension without coordinate.\n            try:\n                return obj._variables[k]\n            except KeyError:\n                return as_variable((k, range(obj.dims[k])))\n\n        def _validate_interp_indexer(x, new_x):\n            # In the case of datetimes, the restrictions placed on indexers\n            # used with interp are stronger than those which are placed on\n            # isel, so we need an additional check after _validate_indexers.\n            if _contains_datetime_like_objects(\n                x\n            ) and not _contains_datetime_like_objects(new_x):\n                raise TypeError(\n                    ""When interpolating over a datetime-like ""\n                    ""coordinate, the coordinates to ""\n                    ""interpolate to must be either datetime ""\n                    ""strings or datetimes. ""\n                    ""Instead got\\n{}"".format(new_x)\n                )\n            return x, new_x\n\n        variables: Dict[Hashable, Variable] = {}\n        for name, var in obj._variables.items():\n            if name in indexers:\n                continue\n\n            if var.dtype.kind in ""uifc"":\n                var_indexers = {\n                    k: _validate_interp_indexer(maybe_variable(obj, k), v)\n                    for k, v in indexers.items()\n                    if k in var.dims\n                }\n                variables[name] = missing.interp(var, var_indexers, method, **kwargs)\n            elif all(d not in indexers for d in var.dims):\n                # keep unrelated object array\n                variables[name] = var\n\n        coord_names = obj._coord_names & variables.keys()\n        indexes = {k: v for k, v in obj.indexes.items() if k not in indexers}\n        selected = self._replace_with_new_dims(\n            variables.copy(), coord_names, indexes=indexes\n        )\n\n        # attach indexer as coordinate\n        variables.update(indexers)\n        for k, v in indexers.items():\n            assert isinstance(v, Variable)\n            if v.dims == (k,):\n                indexes[k] = v.to_index()\n\n        # Extract coordinates from indexers\n        coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)\n        variables.update(coord_vars)\n        indexes.update(new_indexes)\n\n        coord_names = obj._coord_names & variables.keys() | coord_vars.keys()\n        return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n\n    def interp_like(\n        self,\n        other: Union[""Dataset"", ""DataArray""],\n        method: str = ""linear"",\n        assume_sorted: bool = False,\n        kwargs: Mapping[str, Any] = None,\n    ) -> ""Dataset"":\n        """"""Interpolate this object onto the coordinates of another object,\n        filling the out of range values with NaN.\n\n        Parameters\n        ----------\n        other : Dataset or DataArray\n            Object with an \'indexes\' attribute giving a mapping from dimension\n            names to an 1d array-like, which provides coordinates upon\n            which to index the variables in this dataset.\n        method: string, optional.\n            {\'linear\', \'nearest\'} for multidimensional array,\n            {\'linear\', \'nearest\', \'zero\', \'slinear\', \'quadratic\', \'cubic\'}\n            for 1-dimensional array. \'linear\' is used by default.\n        assume_sorted: boolean, optional\n            If False, values of coordinates that are interpolated over can be\n            in any order and they are sorted first. If True, interpolated\n            coordinates are assumed to be an array of monotonically increasing\n            values.\n        kwargs: dictionary, optional\n            Additional keyword passed to scipy\'s interpolator.\n\n        Returns\n        -------\n        interpolated: xr.Dataset\n            Another dataset by interpolating this dataset\'s data along the\n            coordinates of the other object.\n\n        Notes\n        -----\n        scipy is required.\n        If the dataset has object-type coordinates, reindex is used for these\n        coordinates instead of the interpolation.\n\n        See Also\n        --------\n        Dataset.interp\n        Dataset.reindex_like\n        """"""\n        if kwargs is None:\n            kwargs = {}\n        coords = alignment.reindex_like_indexers(self, other)\n\n        numeric_coords: Dict[Hashable, pd.Index] = {}\n        object_coords: Dict[Hashable, pd.Index] = {}\n        for k, v in coords.items():\n            if v.dtype.kind in ""uifcMm"":\n                numeric_coords[k] = v\n            else:\n                object_coords[k] = v\n\n        ds = self\n        if object_coords:\n            # We do not support interpolation along object coordinate.\n            # reindex instead.\n            ds = self.reindex(object_coords)\n        return ds.interp(numeric_coords, method, assume_sorted, kwargs)\n\n    # Helper methods for rename()\n    def _rename_vars(self, name_dict, dims_dict):\n        variables = {}\n        coord_names = set()\n        for k, v in self.variables.items():\n            var = v.copy(deep=False)\n            var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)\n            name = name_dict.get(k, k)\n            if name in variables:\n                raise ValueError(f""the new name {name!r} conflicts"")\n            variables[name] = var\n            if k in self._coord_names:\n                coord_names.add(name)\n        return variables, coord_names\n\n    def _rename_dims(self, name_dict):\n        return {name_dict.get(k, k): v for k, v in self.dims.items()}\n\n    def _rename_indexes(self, name_dict, dims_set):\n        if self._indexes is None:\n            return None\n        indexes = {}\n        for k, v in self.indexes.items():\n            new_name = name_dict.get(k, k)\n            if new_name not in dims_set:\n                continue\n            if isinstance(v, pd.MultiIndex):\n                new_names = [name_dict.get(k, k) for k in v.names]\n                index = v.rename(names=new_names)\n            else:\n                index = v.rename(new_name)\n            indexes[new_name] = index\n        return indexes\n\n    def _rename_all(self, name_dict, dims_dict):\n        variables, coord_names = self._rename_vars(name_dict, dims_dict)\n        dims = self._rename_dims(dims_dict)\n        indexes = self._rename_indexes(name_dict, dims.keys())\n        return variables, coord_names, dims, indexes\n\n    def rename(\n        self,\n        name_dict: Mapping[Hashable, Hashable] = None,\n        inplace: bool = None,\n        **names: Hashable,\n    ) -> ""Dataset"":\n        """"""Returns a new object with renamed variables and dimensions.\n\n        Parameters\n        ----------\n        name_dict : dict-like, optional\n            Dictionary whose keys are current variable or dimension names and\n            whose values are the desired names.\n        **names, optional\n            Keyword form of ``name_dict``.\n            One of name_dict or names must be provided.\n\n        Returns\n        -------\n        renamed : Dataset\n            Dataset with renamed variables and dimensions.\n\n        See Also\n        --------\n        Dataset.swap_dims\n        Dataset.rename_vars\n        Dataset.rename_dims\n        DataArray.rename\n        """"""\n        _check_inplace(inplace)\n        name_dict = either_dict_or_kwargs(name_dict, names, ""rename"")\n        for k in name_dict.keys():\n            if k not in self and k not in self.dims:\n                raise ValueError(\n                    ""cannot rename %r because it is not a ""\n                    ""variable or dimension in this dataset"" % k\n                )\n\n        variables, coord_names, dims, indexes = self._rename_all(\n            name_dict=name_dict, dims_dict=name_dict\n        )\n        assert_unique_multiindex_level_names(variables)\n        return self._replace(variables, coord_names, dims=dims, indexes=indexes)\n\n    def rename_dims(\n        self, dims_dict: Mapping[Hashable, Hashable] = None, **dims: Hashable\n    ) -> ""Dataset"":\n        """"""Returns a new object with renamed dimensions only.\n\n        Parameters\n        ----------\n        dims_dict : dict-like, optional\n            Dictionary whose keys are current dimension names and\n            whose values are the desired names. The desired names must\n            not be the name of an existing dimension or Variable in the Dataset.\n        **dims, optional\n            Keyword form of ``dims_dict``.\n            One of dims_dict or dims must be provided.\n\n        Returns\n        -------\n        renamed : Dataset\n            Dataset with renamed dimensions.\n\n        See Also\n        --------\n        Dataset.swap_dims\n        Dataset.rename\n        Dataset.rename_vars\n        DataArray.rename\n        """"""\n        dims_dict = either_dict_or_kwargs(dims_dict, dims, ""rename_dims"")\n        for k, v in dims_dict.items():\n            if k not in self.dims:\n                raise ValueError(\n                    ""cannot rename %r because it is not a ""\n                    ""dimension in this dataset"" % k\n                )\n            if v in self.dims or v in self:\n                raise ValueError(\n                    f""Cannot rename {k} to {v} because {v} already exists. ""\n                    ""Try using swap_dims instead.""\n                )\n\n        variables, coord_names, sizes, indexes = self._rename_all(\n            name_dict={}, dims_dict=dims_dict\n        )\n        return self._replace(variables, coord_names, dims=sizes, indexes=indexes)\n\n    def rename_vars(\n        self, name_dict: Mapping[Hashable, Hashable] = None, **names: Hashable\n    ) -> ""Dataset"":\n        """"""Returns a new object with renamed variables including coordinates\n\n        Parameters\n        ----------\n        name_dict : dict-like, optional\n            Dictionary whose keys are current variable or coordinate names and\n            whose values are the desired names.\n        **names, optional\n            Keyword form of ``name_dict``.\n            One of name_dict or names must be provided.\n\n        Returns\n        -------\n        renamed : Dataset\n            Dataset with renamed variables including coordinates\n\n        See Also\n        --------\n        Dataset.swap_dims\n        Dataset.rename\n        Dataset.rename_dims\n        DataArray.rename\n        """"""\n        name_dict = either_dict_or_kwargs(name_dict, names, ""rename_vars"")\n        for k in name_dict:\n            if k not in self:\n                raise ValueError(\n                    ""cannot rename %r because it is not a ""\n                    ""variable or coordinate in this dataset"" % k\n                )\n        variables, coord_names, dims, indexes = self._rename_all(\n            name_dict=name_dict, dims_dict={}\n        )\n        return self._replace(variables, coord_names, dims=dims, indexes=indexes)\n\n    def swap_dims(\n        self, dims_dict: Mapping[Hashable, Hashable], inplace: bool = None\n    ) -> ""Dataset"":\n        """"""Returns a new object with swapped dimensions.\n\n        Parameters\n        ----------\n        dims_dict : dict-like\n            Dictionary whose keys are current dimension names and whose values\n            are new names.\n\n        Returns\n        -------\n        swapped : Dataset\n            Dataset with swapped dimensions.\n\n        Examples\n        --------\n        >>> ds = xr.Dataset(\n        ...     data_vars={""a"": (""x"", [5, 7]), ""b"": (""x"", [0.1, 2.4])},\n        ...     coords={""x"": [""a"", ""b""], ""y"": (""x"", [0, 1])},\n        ... )\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (x: 2)\n        Coordinates:\n          * x        (x) <U1 \'a\' \'b\'\n            y        (x) int64 0 1\n        Data variables:\n            a        (x) int64 5 7\n            b        (x) float64 0.1 2.4\n\n        >>> ds.swap_dims({""x"": ""y""})\n        <xarray.Dataset>\n        Dimensions:  (y: 2)\n        Coordinates:\n            x        (y) <U1 \'a\' \'b\'\n          * y        (y) int64 0 1\n        Data variables:\n            a        (y) int64 5 7\n            b        (y) float64 0.1 2.4\n\n        >>> ds.swap_dims({""x"": ""z""})\n        <xarray.Dataset>\n        Dimensions:  (z: 2)\n        Coordinates:\n            x        (z) <U1 \'a\' \'b\'\n            y        (z) int64 0 1\n        Dimensions without coordinates: z\n        Data variables:\n            a        (z) int64 5 7\n            b        (z) float64 0.1 2.4\n\n        See Also\n        --------\n\n        Dataset.rename\n        DataArray.swap_dims\n        """"""\n        # TODO: deprecate this method in favor of a (less confusing)\n        # rename_dims() method that only renames dimensions.\n        _check_inplace(inplace)\n        for k, v in dims_dict.items():\n            if k not in self.dims:\n                raise ValueError(\n                    ""cannot swap from dimension %r because it is ""\n                    ""not an existing dimension"" % k\n                )\n            if v in self.variables and self.variables[v].dims != (k,):\n                raise ValueError(\n                    ""replacement dimension %r is not a 1D ""\n                    ""variable along the old dimension %r"" % (v, k)\n                )\n\n        result_dims = {dims_dict.get(dim, dim) for dim in self.dims}\n\n        coord_names = self._coord_names.copy()\n        coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})\n\n        variables: Dict[Hashable, Variable] = {}\n        indexes: Dict[Hashable, pd.Index] = {}\n        for k, v in self.variables.items():\n            dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)\n            if k in result_dims:\n                var = v.to_index_variable()\n                if k in self.indexes:\n                    indexes[k] = self.indexes[k]\n                else:\n                    new_index = var.to_index()\n                    if new_index.nlevels == 1:\n                        # make sure index name matches dimension name\n                        new_index = new_index.rename(k)\n                    indexes[k] = new_index\n            else:\n                var = v.to_base_variable()\n            var.dims = dims\n            variables[k] = var\n\n        return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n\n    def expand_dims(\n        self,\n        dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,\n        axis: Union[None, int, Sequence[int]] = None,\n        **dim_kwargs: Any,\n    ) -> ""Dataset"":\n        """"""Return a new object with an additional axis (or axes) inserted at\n        the corresponding position in the array shape.  The new object is a\n        view into the underlying array, not a copy.\n\n        If dim is already a scalar coordinate, it will be promoted to a 1D\n        coordinate consisting of a single value.\n\n        Parameters\n        ----------\n        dim : hashable, sequence of hashable, mapping, or None\n            Dimensions to include on the new variable. If provided as hashable\n            or sequence of hashable, then dimensions are inserted with length\n            1. If provided as a mapping, then the keys are the new dimensions\n            and the values are either integers (giving the length of the new\n            dimensions) or array-like (giving the coordinates of the new\n            dimensions).\n        axis : integer, sequence of integers, or None\n            Axis position(s) where new axis is to be inserted (position(s) on\n            the result array). If a list (or tuple) of integers is passed,\n            multiple axes are inserted. In this case, dim arguments should be\n            same length list. If axis=None is passed, all the axes will be\n            inserted to the start of the result array.\n        **dim_kwargs : int or sequence/ndarray\n            The keywords are arbitrary dimensions being inserted and the values\n            are either the lengths of the new dims (if int is given), or their\n            coordinates. Note, this is an alternative to passing a dict to the\n            dim kwarg and will only be used if dim is None.\n\n        Returns\n        -------\n        expanded : same type as caller\n            This object, but with an additional dimension(s).\n        """"""\n        if dim is None:\n            pass\n        elif isinstance(dim, Mapping):\n            # We\'re later going to modify dim in place; don\'t tamper with\n            # the input\n            dim = dict(dim)\n        elif isinstance(dim, int):\n            raise TypeError(\n                ""dim should be hashable or sequence of hashables or mapping""\n            )\n        elif isinstance(dim, str) or not isinstance(dim, Sequence):\n            dim = {dim: 1}\n        elif isinstance(dim, Sequence):\n            if len(dim) != len(set(dim)):\n                raise ValueError(""dims should not contain duplicate values."")\n            dim = {d: 1 for d in dim}\n\n        dim = either_dict_or_kwargs(dim, dim_kwargs, ""expand_dims"")\n        assert isinstance(dim, MutableMapping)\n\n        if axis is None:\n            axis = list(range(len(dim)))\n        elif not isinstance(axis, Sequence):\n            axis = [axis]\n\n        if len(dim) != len(axis):\n            raise ValueError(""lengths of dim and axis should be identical."")\n        for d in dim:\n            if d in self.dims:\n                raise ValueError(f""Dimension {d} already exists."")\n            if d in self._variables and not utils.is_scalar(self._variables[d]):\n                raise ValueError(\n                    ""{dim} already exists as coordinate or""\n                    "" variable name."".format(dim=d)\n                )\n\n        variables: Dict[Hashable, Variable] = {}\n        coord_names = self._coord_names.copy()\n        # If dim is a dict, then ensure that the values are either integers\n        # or iterables.\n        for k, v in dim.items():\n            if hasattr(v, ""__iter__""):\n                # If the value for the new dimension is an iterable, then\n                # save the coordinates to the variables dict, and set the\n                # value within the dim dict to the length of the iterable\n                # for later use.\n                variables[k] = xr.IndexVariable((k,), v)\n                coord_names.add(k)\n                dim[k] = variables[k].size\n            elif isinstance(v, int):\n                pass  # Do nothing if the dimensions value is just an int\n            else:\n                raise TypeError(\n                    ""The value of new dimension {k} must be ""\n                    ""an iterable or an int"".format(k=k)\n                )\n\n        for k, v in self._variables.items():\n            if k not in dim:\n                if k in coord_names:  # Do not change coordinates\n                    variables[k] = v\n                else:\n                    result_ndim = len(v.dims) + len(axis)\n                    for a in axis:\n                        if a < -result_ndim or result_ndim - 1 < a:\n                            raise IndexError(\n                                f""Axis {a} of variable {k} is out of bounds of the ""\n                                f""expanded dimension size {result_ndim}""\n                            )\n\n                    axis_pos = [a if a >= 0 else result_ndim + a for a in axis]\n                    if len(axis_pos) != len(set(axis_pos)):\n                        raise ValueError(""axis should not contain duplicate values"")\n                    # We need to sort them to make sure `axis` equals to the\n                    # axis positions of the result array.\n                    zip_axis_dim = sorted(zip(axis_pos, dim.items()))\n\n                    all_dims = list(zip(v.dims, v.shape))\n                    for d, c in zip_axis_dim:\n                        all_dims.insert(d, c)\n                    variables[k] = v.set_dims(dict(all_dims))\n            else:\n                # If dims includes a label of a non-dimension coordinate,\n                # it will be promoted to a 1D coordinate with a single value.\n                variables[k] = v.set_dims(k).to_index_variable()\n\n        new_dims = self._dims.copy()\n        new_dims.update(dim)\n\n        return self._replace_vars_and_dims(\n            variables, dims=new_dims, coord_names=coord_names\n        )\n\n    def set_index(\n        self,\n        indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,\n        append: bool = False,\n        inplace: bool = None,\n        **indexes_kwargs: Union[Hashable, Sequence[Hashable]],\n    ) -> ""Dataset"":\n        """"""Set Dataset (multi-)indexes using one or more existing coordinates\n        or variables.\n\n        Parameters\n        ----------\n        indexes : {dim: index, ...}\n            Mapping from names matching dimensions and values given\n            by (lists of) the names of existing coordinates or variables to set\n            as new (multi-)index.\n        append : bool, optional\n            If True, append the supplied index(es) to the existing index(es).\n            Otherwise replace the existing index(es) (default).\n        **indexes_kwargs: optional\n            The keyword arguments form of ``indexes``.\n            One of indexes or indexes_kwargs must be provided.\n\n        Returns\n        -------\n        obj : Dataset\n            Another dataset, with this dataset\'s data but replaced coordinates.\n\n        Examples\n        --------\n        >>> arr = xr.DataArray(\n        ...     data=np.ones((2, 3)),\n        ...     dims=[""x"", ""y""],\n        ...     coords={""x"": range(2), ""y"": range(3), ""a"": (""x"", [3, 4])},\n        ... )\n        >>> ds = xr.Dataset({""v"": arr})\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (x: 2, y: 3)\n        Coordinates:\n          * x        (x) int64 0 1\n          * y        (y) int64 0 1 2\n            a        (x) int64 3 4\n        Data variables:\n            v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0\n        >>> ds.set_index(x=""a"")\n        <xarray.Dataset>\n        Dimensions:  (x: 2, y: 3)\n        Coordinates:\n          * x        (x) int64 3 4\n          * y        (y) int64 0 1 2\n        Data variables:\n            v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0\n\n        See Also\n        --------\n        Dataset.reset_index\n        Dataset.swap_dims\n        """"""\n        _check_inplace(inplace)\n        indexes = either_dict_or_kwargs(indexes, indexes_kwargs, ""set_index"")\n        variables, coord_names = merge_indexes(\n            indexes, self._variables, self._coord_names, append=append\n        )\n        return self._replace_vars_and_dims(variables, coord_names=coord_names)\n\n    def reset_index(\n        self,\n        dims_or_levels: Union[Hashable, Sequence[Hashable]],\n        drop: bool = False,\n        inplace: bool = None,\n    ) -> ""Dataset"":\n        """"""Reset the specified index(es) or multi-index level(s).\n\n        Parameters\n        ----------\n        dims_or_levels : str or list\n            Name(s) of the dimension(s) and/or multi-index level(s) that will\n            be reset.\n        drop : bool, optional\n            If True, remove the specified indexes and/or multi-index levels\n            instead of extracting them as new coordinates (default: False).\n\n        Returns\n        -------\n        obj : Dataset\n            Another dataset, with this dataset\'s data but replaced coordinates.\n\n        See Also\n        --------\n        Dataset.set_index\n        """"""\n        _check_inplace(inplace)\n        variables, coord_names = split_indexes(\n            dims_or_levels,\n            self._variables,\n            self._coord_names,\n            cast(Mapping[Hashable, Hashable], self._level_coords),\n            drop=drop,\n        )\n        return self._replace_vars_and_dims(variables, coord_names=coord_names)\n\n    def reorder_levels(\n        self,\n        dim_order: Mapping[Hashable, Sequence[int]] = None,\n        inplace: bool = None,\n        **dim_order_kwargs: Sequence[int],\n    ) -> ""Dataset"":\n        """"""Rearrange index levels using input order.\n\n        Parameters\n        ----------\n        dim_order : optional\n            Mapping from names matching dimensions and values given\n            by lists representing new level orders. Every given dimension\n            must have a multi-index.\n        **dim_order_kwargs: optional\n            The keyword arguments form of ``dim_order``.\n            One of dim_order or dim_order_kwargs must be provided.\n\n        Returns\n        -------\n        obj : Dataset\n            Another dataset, with this dataset\'s data but replaced\n            coordinates.\n        """"""\n        _check_inplace(inplace)\n        dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, ""reorder_levels"")\n        variables = self._variables.copy()\n        indexes = dict(self.indexes)\n        for dim, order in dim_order.items():\n            coord = self._variables[dim]\n            index = self.indexes[dim]\n            if not isinstance(index, pd.MultiIndex):\n                raise ValueError(f""coordinate {dim} has no MultiIndex"")\n            new_index = index.reorder_levels(order)\n            variables[dim] = IndexVariable(coord.dims, new_index)\n            indexes[dim] = new_index\n\n        return self._replace(variables, indexes=indexes)\n\n    def _stack_once(self, dims, new_dim):\n        if ... in dims:\n            dims = list(infix_dims(dims, self.dims))\n        variables = {}\n        for name, var in self.variables.items():\n            if name not in dims:\n                if any(d in var.dims for d in dims):\n                    add_dims = [d for d in dims if d not in var.dims]\n                    vdims = list(var.dims) + add_dims\n                    shape = [self.dims[d] for d in vdims]\n                    exp_var = var.set_dims(vdims, shape)\n                    stacked_var = exp_var.stack(**{new_dim: dims})\n                    variables[name] = stacked_var\n                else:\n                    variables[name] = var.copy(deep=False)\n\n        # consider dropping levels that are unused?\n        levels = [self.get_index(dim) for dim in dims]\n        idx = utils.multiindex_from_product_levels(levels, names=dims)\n        variables[new_dim] = IndexVariable(new_dim, idx)\n\n        coord_names = set(self._coord_names) - set(dims) | {new_dim}\n\n        indexes = {k: v for k, v in self.indexes.items() if k not in dims}\n        indexes[new_dim] = idx\n\n        return self._replace_with_new_dims(\n            variables, coord_names=coord_names, indexes=indexes\n        )\n\n    def stack(\n        self,\n        dimensions: Mapping[Hashable, Sequence[Hashable]] = None,\n        **dimensions_kwargs: Sequence[Hashable],\n    ) -> ""Dataset"":\n        """"""\n        Stack any number of existing dimensions into a single new dimension.\n\n        New dimensions will be added at the end, and the corresponding\n        coordinate variables will be combined into a MultiIndex.\n\n        Parameters\n        ----------\n        dimensions : Mapping of the form new_name=(dim1, dim2, ...)\n            Names of new dimensions, and the existing dimensions that they\n            replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.\n            Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over\n            all dimensions.\n        **dimensions_kwargs:\n            The keyword arguments form of ``dimensions``.\n            One of dimensions or dimensions_kwargs must be provided.\n\n        Returns\n        -------\n        stacked : Dataset\n            Dataset with stacked data.\n\n        See also\n        --------\n        Dataset.unstack\n        """"""\n        dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, ""stack"")\n        result = self\n        for new_dim, dims in dimensions.items():\n            result = result._stack_once(dims, new_dim)\n        return result\n\n    def to_stacked_array(\n        self,\n        new_dim: Hashable,\n        sample_dims: Sequence[Hashable],\n        variable_dim: str = ""variable"",\n        name: Hashable = None,\n    ) -> ""DataArray"":\n        """"""Combine variables of differing dimensionality into a DataArray\n        without broadcasting.\n\n        This method is similar to Dataset.to_array but does not broadcast the\n        variables.\n\n        Parameters\n        ----------\n        new_dim : Hashable\n            Name of the new stacked coordinate\n        sample_dims : Sequence[Hashable]\n            Dimensions that **will not** be stacked. Each array in the dataset\n            must share these dimensions. For machine learning applications,\n            these define the dimensions over which samples are drawn.\n        variable_dim : str, optional\n            Name of the level in the stacked coordinate which corresponds to\n            the variables.\n        name : str, optional\n            Name of the new data array.\n\n        Returns\n        -------\n        stacked : DataArray\n            DataArray with the specified dimensions and data variables\n            stacked together. The stacked coordinate is named ``new_dim``\n            and represented by a MultiIndex object with a level containing the\n            data variable names. The name of this level is controlled using\n            the ``variable_dim`` argument.\n\n        See Also\n        --------\n        Dataset.to_array\n        Dataset.stack\n        DataArray.to_unstacked_dataset\n\n        Examples\n        --------\n        >>> data = xr.Dataset(\n        ...     data_vars={\n        ...         ""a"": ((""x"", ""y""), [[0, 1, 2], [3, 4, 5]]),\n        ...         ""b"": (""x"", [6, 7]),\n        ...     },\n        ...     coords={""y"": [""u"", ""v"", ""w""]},\n        ... )\n\n        >>> data\n        <xarray.Dataset>\n        Dimensions:  (x: 2, y: 3)\n        Coordinates:\n        * y        (y) <U1 \'u\' \'v\' \'w\'\n        Dimensions without coordinates: x\n        Data variables:\n            a        (x, y) int64 0 1 2 3 4 5\n            b        (x) int64 6 7\n\n        >>> data.to_stacked_array(""z"", sample_dims=[""x""])\n        <xarray.DataArray (x: 2, z: 4)>\n        array([[0, 1, 2, 6],\n            [3, 4, 5, 7]])\n        Coordinates:\n        * z         (z) MultiIndex\n        - variable  (z) object \'a\' \'a\' \'a\' \'b\'\n        - y         (z) object \'u\' \'v\' \'w\' nan\n        Dimensions without coordinates: x\n\n        """"""\n        stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)\n\n        for variable in self:\n            dims = self[variable].dims\n            dims_include_sample_dims = set(sample_dims) <= set(dims)\n            if not dims_include_sample_dims:\n                raise ValueError(\n                    ""All variables in the dataset must contain the ""\n                    ""dimensions {}."".format(dims)\n                )\n\n        def ensure_stackable(val):\n            assign_coords = {variable_dim: val.name}\n            for dim in stacking_dims:\n                if dim not in val.dims:\n                    assign_coords[dim] = None\n\n            expand_dims = set(stacking_dims).difference(set(val.dims))\n            expand_dims.add(variable_dim)\n            # must be list for .expand_dims\n            expand_dims = list(expand_dims)\n\n            return (\n                val.assign_coords(**assign_coords)\n                .expand_dims(expand_dims)\n                .stack({new_dim: (variable_dim,) + stacking_dims})\n            )\n\n        # concatenate the arrays\n        stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]\n        data_array = xr.concat(stackable_vars, dim=new_dim)\n\n        # coerce the levels of the MultiIndex to have the same type as the\n        # input dimensions. This code is messy, so it might be better to just\n        # input a dummy value for the singleton dimension.\n        idx = data_array.indexes[new_dim]\n        levels = [idx.levels[0]] + [\n            level.astype(self[level.name].dtype) for level in idx.levels[1:]\n        ]\n        new_idx = idx.set_levels(levels)\n        data_array[new_dim] = IndexVariable(new_dim, new_idx)\n\n        if name is not None:\n            data_array.name = name\n\n        return data_array\n\n    def _unstack_once(self, dim: Hashable, fill_value, sparse) -> ""Dataset"":\n        index = self.get_index(dim)\n        index = remove_unused_levels_categories(index)\n        full_idx = pd.MultiIndex.from_product(index.levels, names=index.names)\n\n        # take a shortcut in case the MultiIndex was not modified.\n        if index.equals(full_idx):\n            obj = self\n        else:\n            obj = self._reindex(\n                {dim: full_idx}, copy=False, fill_value=fill_value, sparse=sparse\n            )\n\n        new_dim_names = index.names\n        new_dim_sizes = [lev.size for lev in index.levels]\n\n        variables: Dict[Hashable, Variable] = {}\n        indexes = {k: v for k, v in self.indexes.items() if k != dim}\n\n        for name, var in obj.variables.items():\n            if name != dim:\n                if dim in var.dims:\n                    new_dims = dict(zip(new_dim_names, new_dim_sizes))\n                    variables[name] = var.unstack({dim: new_dims})\n                else:\n                    variables[name] = var\n\n        for name, lev in zip(new_dim_names, index.levels):\n            variables[name] = IndexVariable(name, lev)\n            indexes[name] = lev\n\n        coord_names = set(self._coord_names) - {dim} | set(new_dim_names)\n\n        return self._replace_with_new_dims(\n            variables, coord_names=coord_names, indexes=indexes\n        )\n\n    def unstack(\n        self,\n        dim: Union[Hashable, Iterable[Hashable]] = None,\n        fill_value: Any = dtypes.NA,\n        sparse: bool = False,\n    ) -> ""Dataset"":\n        """"""\n        Unstack existing dimensions corresponding to MultiIndexes into\n        multiple new dimensions.\n\n        New dimensions will be added at the end.\n\n        Parameters\n        ----------\n        dim : Hashable or iterable of Hashable, optional\n            Dimension(s) over which to unstack. By default unstacks all\n            MultiIndexes.\n        fill_value: value to be filled. By default, np.nan\n        sparse: use sparse-array if True\n\n        Returns\n        -------\n        unstacked : Dataset\n            Dataset with unstacked data.\n\n        See also\n        --------\n        Dataset.stack\n        """"""\n        if dim is None:\n            dims = [\n                d for d in self.dims if isinstance(self.get_index(d), pd.MultiIndex)\n            ]\n        else:\n            if isinstance(dim, str) or not isinstance(dim, Iterable):\n                dims = [dim]\n            else:\n                dims = list(dim)\n\n            missing_dims = [d for d in dims if d not in self.dims]\n            if missing_dims:\n                raise ValueError(\n                    ""Dataset does not contain the dimensions: %s"" % missing_dims\n                )\n\n            non_multi_dims = [\n                d for d in dims if not isinstance(self.get_index(d), pd.MultiIndex)\n            ]\n            if non_multi_dims:\n                raise ValueError(\n                    ""cannot unstack dimensions that do not ""\n                    ""have a MultiIndex: %s"" % non_multi_dims\n                )\n\n        result = self.copy(deep=False)\n        for dim in dims:\n            result = result._unstack_once(dim, fill_value, sparse)\n        return result\n\n    def update(self, other: ""CoercibleMapping"", inplace: bool = None) -> ""Dataset"":\n        """"""Update this dataset\'s variables with those from another dataset.\n\n        Parameters\n        ----------\n        other : Dataset or castable to Dataset\n            Variables with which to update this dataset. One of:\n\n            - Dataset\n            - mapping {var name: DataArray}\n            - mapping {var name: Variable}\n            - mapping {var name: (dimension name, array-like)}\n            - mapping {var name: (tuple of dimension names, array-like)}\n\n\n        Returns\n        -------\n        updated : Dataset\n            Updated dataset.\n\n        Raises\n        ------\n        ValueError\n            If any dimensions would have inconsistent sizes in the updated\n            dataset.\n        """"""\n        _check_inplace(inplace)\n        merge_result = dataset_update_method(self, other)\n        return self._replace(inplace=True, **merge_result._asdict())\n\n    def merge(\n        self,\n        other: Union[""CoercibleMapping"", ""DataArray""],\n        inplace: bool = None,\n        overwrite_vars: Union[Hashable, Iterable[Hashable]] = frozenset(),\n        compat: str = ""no_conflicts"",\n        join: str = ""outer"",\n        fill_value: Any = dtypes.NA,\n    ) -> ""Dataset"":\n        """"""Merge the arrays of two datasets into a single dataset.\n\n        This method generally does not allow for overriding data, with the\n        exception of attributes, which are ignored on the second dataset.\n        Variables with the same name are checked for conflicts via the equals\n        or identical methods.\n\n        Parameters\n        ----------\n        other : Dataset or castable to Dataset\n            Dataset or variables to merge with this dataset.\n        overwrite_vars : Hashable or iterable of Hashable, optional\n            If provided, update variables of these name(s) without checking for\n            conflicts in this dataset.\n        compat : {\'broadcast_equals\', \'equals\', \'identical\',\n                  \'no_conflicts\'}, optional\n            String indicating how to compare variables of the same name for\n            potential conflicts:\n\n            - \'broadcast_equals\': all values must be equal when variables are\n              broadcast against each other to ensure common dimensions.\n            - \'equals\': all values and dimensions must be the same.\n            - \'identical\': all values, dimensions and attributes must be the\n              same.\n            - \'no_conflicts\': only values which are not null in both datasets\n              must be equal. The returned dataset then contains the combination\n              of all non-null values.\n\n        join : {\'outer\', \'inner\', \'left\', \'right\', \'exact\'}, optional\n            Method for joining ``self`` and ``other`` along shared dimensions:\n\n            - \'outer\': use the union of the indexes\n            - \'inner\': use the intersection of the indexes\n            - \'left\': use indexes from ``self``\n            - \'right\': use indexes from ``other``\n            - \'exact\': error instead of aligning non-equal indexes\n        fill_value: scalar, optional\n            Value to use for newly missing values\n\n        Returns\n        -------\n        merged : Dataset\n            Merged dataset.\n\n        Raises\n        ------\n        MergeError\n            If any variables conflict (see ``compat``).\n        """"""\n        _check_inplace(inplace)\n        other = other.to_dataset() if isinstance(other, xr.DataArray) else other\n        merge_result = dataset_merge_method(\n            self,\n            other,\n            overwrite_vars=overwrite_vars,\n            compat=compat,\n            join=join,\n            fill_value=fill_value,\n        )\n        return self._replace(**merge_result._asdict())\n\n    def _assert_all_in_dataset(\n        self, names: Iterable[Hashable], virtual_okay: bool = False\n    ) -> None:\n        bad_names = set(names) - set(self._variables)\n        if virtual_okay:\n            bad_names -= self.virtual_variables\n        if bad_names:\n            raise ValueError(\n                ""One or more of the specified variables ""\n                ""cannot be found in this dataset""\n            )\n\n    def drop_vars(\n        self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = ""raise""\n    ) -> ""Dataset"":\n        """"""Drop variables from this dataset.\n\n        Parameters\n        ----------\n        names : hashable or iterable of hashables\n            Name(s) of variables to drop.\n        errors: {\'raise\', \'ignore\'}, optional\n            If \'raise\' (default), raises a ValueError error if any of the variable\n            passed are not in the dataset. If \'ignore\', any given names that are in the\n            dataset are dropped and no error is raised.\n\n        Returns\n        -------\n        dropped : Dataset\n\n        """"""\n        # the Iterable check is required for mypy\n        if is_scalar(names) or not isinstance(names, Iterable):\n            names = {names}\n        else:\n            names = set(names)\n        if errors == ""raise"":\n            self._assert_all_in_dataset(names)\n\n        variables = {k: v for k, v in self._variables.items() if k not in names}\n        coord_names = {k for k in self._coord_names if k in variables}\n        indexes = {k: v for k, v in self.indexes.items() if k not in names}\n        return self._replace_with_new_dims(\n            variables, coord_names=coord_names, indexes=indexes\n        )\n\n    def drop(self, labels=None, dim=None, *, errors=""raise"", **labels_kwargs):\n        """"""Backward compatible method based on `drop_vars` and `drop_sel`\n\n        Using either `drop_vars` or `drop_sel` is encouraged\n\n        See Also\n        --------\n        Dataset.drop_vars\n        Dataset.drop_sel\n        """"""\n        if errors not in [""raise"", ""ignore""]:\n            raise ValueError(\'errors must be either ""raise"" or ""ignore""\')\n\n        if is_dict_like(labels) and not isinstance(labels, dict):\n            warnings.warn(\n                ""dropping coordinates using `drop` is be deprecated; use drop_vars."",\n                FutureWarning,\n                stacklevel=2,\n            )\n            return self.drop_vars(labels, errors=errors)\n\n        if labels_kwargs or isinstance(labels, dict):\n            if dim is not None:\n                raise ValueError(""cannot specify dim and dict-like arguments."")\n            labels = either_dict_or_kwargs(labels, labels_kwargs, ""drop"")\n\n        if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):\n            warnings.warn(\n                ""dropping variables using `drop` will be deprecated; using drop_vars is encouraged."",\n                PendingDeprecationWarning,\n                stacklevel=2,\n            )\n            return self.drop_vars(labels, errors=errors)\n        if dim is not None:\n            warnings.warn(\n                ""dropping labels using list-like labels is deprecated; using ""\n                ""dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels])."",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)\n\n        warnings.warn(\n            ""dropping labels using `drop` will be deprecated; using drop_sel is encouraged."",\n            PendingDeprecationWarning,\n            stacklevel=2,\n        )\n        return self.drop_sel(labels, errors=errors)\n\n    def drop_sel(self, labels=None, *, errors=""raise"", **labels_kwargs):\n        """"""Drop index labels from this dataset.\n\n        Parameters\n        ----------\n        labels : Mapping[Hashable, Any]\n            Index labels to drop\n        errors: {\'raise\', \'ignore\'}, optional\n            If \'raise\' (default), raises a ValueError error if\n            any of the index labels passed are not\n            in the dataset. If \'ignore\', any given labels that are in the\n            dataset are dropped and no error is raised.\n        **labels_kwargs : {dim: label, ...}, optional\n            The keyword arguments form of ``dim`` and ``labels``\n\n        Returns\n        -------\n        dropped : Dataset\n\n        Examples\n        --------\n        >>> data = np.random.randn(2, 3)\n        >>> labels = [""a"", ""b"", ""c""]\n        >>> ds = xr.Dataset({""A"": ([""x"", ""y""], data), ""y"": labels})\n        >>> ds.drop_sel(y=[""a"", ""c""])\n        <xarray.Dataset>\n        Dimensions:  (x: 2, y: 1)\n        Coordinates:\n          * y        (y) <U1 \'b\'\n        Dimensions without coordinates: x\n        Data variables:\n            A        (x, y) float64 -0.3454 0.1734\n        >>> ds.drop_sel(y=""b"")\n        <xarray.Dataset>\n        Dimensions:  (x: 2, y: 2)\n        Coordinates:\n          * y        (y) <U1 \'a\' \'c\'\n        Dimensions without coordinates: x\n        Data variables:\n            A        (x, y) float64 -0.3944 -1.418 1.423 -1.041\n        """"""\n        if errors not in [""raise"", ""ignore""]:\n            raise ValueError(\'errors must be either ""raise"" or ""ignore""\')\n\n        labels = either_dict_or_kwargs(labels, labels_kwargs, ""drop"")\n\n        ds = self\n        for dim, labels_for_dim in labels.items():\n            # Don\'t cast to set, as it would harm performance when labels\n            # is a large numpy array\n            if utils.is_scalar(labels_for_dim):\n                labels_for_dim = [labels_for_dim]\n            labels_for_dim = np.asarray(labels_for_dim)\n            try:\n                index = self.indexes[dim]\n            except KeyError:\n                raise ValueError(""dimension %r does not have coordinate labels"" % dim)\n            new_index = index.drop(labels_for_dim, errors=errors)\n            ds = ds.loc[{dim: new_index}]\n        return ds\n\n    def drop_dims(\n        self, drop_dims: Union[Hashable, Iterable[Hashable]], *, errors: str = ""raise""\n    ) -> ""Dataset"":\n        """"""Drop dimensions and associated variables from this dataset.\n\n        Parameters\n        ----------\n        drop_dims : hashable or iterable of hashable\n            Dimension or dimensions to drop.\n        errors: {\'raise\', \'ignore\'}, optional\n            If \'raise\' (default), raises a ValueError error if any of the\n            dimensions passed are not in the dataset. If \'ignore\', any given\n            labels that are in the dataset are dropped and no error is raised.\n\n        Returns\n        -------\n        obj : Dataset\n            The dataset without the given dimensions (or any variables\n            containing those dimensions)\n        errors: {\'raise\', \'ignore\'}, optional\n            If \'raise\' (default), raises a ValueError error if\n            any of the dimensions passed are not\n            in the dataset. If \'ignore\', any given dimensions that are in the\n            dataset are dropped and no error is raised.\n        """"""\n        if errors not in [""raise"", ""ignore""]:\n            raise ValueError(\'errors must be either ""raise"" or ""ignore""\')\n\n        if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):\n            drop_dims = {drop_dims}\n        else:\n            drop_dims = set(drop_dims)\n\n        if errors == ""raise"":\n            missing_dims = drop_dims - set(self.dims)\n            if missing_dims:\n                raise ValueError(\n                    ""Dataset does not contain the dimensions: %s"" % missing_dims\n                )\n\n        drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}\n        return self.drop_vars(drop_vars)\n\n    def transpose(self, *dims: Hashable) -> ""Dataset"":\n        """"""Return a new Dataset object with all array dimensions transposed.\n\n        Although the order of dimensions on each array will change, the dataset\n        dimensions themselves will remain in fixed (sorted) order.\n\n        Parameters\n        ----------\n        *dims : Hashable, optional\n            By default, reverse the dimensions on each array. Otherwise,\n            reorder the dimensions to this order.\n\n        Returns\n        -------\n        transposed : Dataset\n            Each array in the dataset (including) coordinates will be\n            transposed to the given order.\n\n        Notes\n        -----\n        This operation returns a view of each array\'s data. It is\n        lazy for dask-backed DataArrays but not for numpy-backed DataArrays\n        -- the data will be fully loaded into memory.\n\n        See Also\n        --------\n        numpy.transpose\n        DataArray.transpose\n        """"""\n        if dims:\n            if set(dims) ^ set(self.dims) and ... not in dims:\n                raise ValueError(\n                    ""arguments to transpose (%s) must be ""\n                    ""permuted dataset dimensions (%s)"" % (dims, tuple(self.dims))\n                )\n        ds = self.copy()\n        for name, var in self._variables.items():\n            var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))\n            ds._variables[name] = var.transpose(*var_dims)\n        return ds\n\n    def dropna(\n        self,\n        dim: Hashable,\n        how: str = ""any"",\n        thresh: int = None,\n        subset: Iterable[Hashable] = None,\n    ):\n        """"""Returns a new dataset with dropped labels for missing values along\n        the provided dimension.\n\n        Parameters\n        ----------\n        dim : Hashable\n            Dimension along which to drop missing values. Dropping along\n            multiple dimensions simultaneously is not yet supported.\n        how : {\'any\', \'all\'}, optional\n            * any : if any NA values are present, drop that label\n            * all : if all values are NA, drop that label\n        thresh : int, default None\n            If supplied, require this many non-NA values.\n        subset : iterable of hashable, optional\n            Which variables to check for missing values. By default, all\n            variables in the dataset are checked.\n\n        Returns\n        -------\n        Dataset\n        """"""\n        # TODO: consider supporting multiple dimensions? Or not, given that\n        # there are some ugly edge cases, e.g., pandas\'s dropna differs\n        # depending on the order of the supplied axes.\n\n        if dim not in self.dims:\n            raise ValueError(""%s must be a single dataset dimension"" % dim)\n\n        if subset is None:\n            subset = iter(self.data_vars)\n\n        count = np.zeros(self.dims[dim], dtype=np.int64)\n        size = 0\n\n        for k in subset:\n            array = self._variables[k]\n            if dim in array.dims:\n                dims = [d for d in array.dims if d != dim]\n                count += np.asarray(array.count(dims))  # type: ignore\n                size += np.prod([self.dims[d] for d in dims])\n\n        if thresh is not None:\n            mask = count >= thresh\n        elif how == ""any"":\n            mask = count == size\n        elif how == ""all"":\n            mask = count > 0\n        elif how is not None:\n            raise ValueError(""invalid how option: %s"" % how)\n        else:\n            raise TypeError(""must specify how or thresh"")\n\n        return self.isel({dim: mask})\n\n    def fillna(self, value: Any) -> ""Dataset"":\n        """"""Fill missing values in this object.\n\n        This operation follows the normal broadcasting and alignment rules that\n        xarray uses for binary arithmetic, except the result is aligned to this\n        object (``join=\'left\'``) instead of aligned to the intersection of\n        index coordinates (``join=\'inner\'``).\n\n        Parameters\n        ----------\n        value : scalar, ndarray, DataArray, dict or Dataset\n            Used to fill all matching missing values in this dataset\'s data\n            variables. Scalars, ndarrays or DataArrays arguments are used to\n            fill all data with aligned coordinates (for DataArrays).\n            Dictionaries or datasets match data variables and then align\n            coordinates if necessary.\n\n        Returns\n        -------\n        Dataset\n\n        Examples\n        --------\n\n        >>> import numpy as np\n        >>> import xarray as xr\n        >>> ds = xr.Dataset(\n        ...     {\n        ...         ""A"": (""x"", [np.nan, 2, np.nan, 0]),\n        ...         ""B"": (""x"", [3, 4, np.nan, 1]),\n        ...         ""C"": (""x"", [np.nan, np.nan, np.nan, 5]),\n        ...         ""D"": (""x"", [np.nan, 3, np.nan, 4]),\n        ...     },\n        ...     coords={""x"": [0, 1, 2, 3]},\n        ... )\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (x: 4)\n        Coordinates:\n        * x        (x) int64 0 1 2 3\n        Data variables:\n            A        (x) float64 nan 2.0 nan 0.0\n            B        (x) float64 3.0 4.0 nan 1.0\n            C        (x) float64 nan nan nan 5.0\n            D        (x) float64 nan 3.0 nan 4.0\n\n        Replace all `NaN` values with 0s.\n\n        >>> ds.fillna(0)\n        <xarray.Dataset>\n        Dimensions:  (x: 4)\n        Coordinates:\n        * x        (x) int64 0 1 2 3\n        Data variables:\n            A        (x) float64 0.0 2.0 0.0 0.0\n            B        (x) float64 3.0 4.0 0.0 1.0\n            C        (x) float64 0.0 0.0 0.0 5.0\n            D        (x) float64 0.0 3.0 0.0 4.0\n\n        Replace all `NaN` elements in column \xe2\x80\x98A\xe2\x80\x99, \xe2\x80\x98B\xe2\x80\x99, \xe2\x80\x98C\xe2\x80\x99, and \xe2\x80\x98D\xe2\x80\x99, with 0, 1, 2, and 3 respectively.\n\n        >>> values = {""A"": 0, ""B"": 1, ""C"": 2, ""D"": 3}\n        >>> ds.fillna(value=values)\n        <xarray.Dataset>\n        Dimensions:  (x: 4)\n        Coordinates:\n        * x        (x) int64 0 1 2 3\n        Data variables:\n            A        (x) float64 0.0 2.0 0.0 0.0\n            B        (x) float64 3.0 4.0 1.0 1.0\n            C        (x) float64 2.0 2.0 2.0 5.0\n            D        (x) float64 3.0 3.0 3.0 4.0\n        """"""\n        if utils.is_dict_like(value):\n            value_keys = getattr(value, ""data_vars"", value).keys()\n            if not set(value_keys) <= set(self.data_vars.keys()):\n                raise ValueError(\n                    ""all variables in the argument to `fillna` ""\n                    ""must be contained in the original dataset""\n                )\n        out = ops.fillna(self, value)\n        return out\n\n    def interpolate_na(\n        self,\n        dim: Hashable = None,\n        method: str = ""linear"",\n        limit: int = None,\n        use_coordinate: Union[bool, Hashable] = True,\n        max_gap: Union[\n            int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta\n        ] = None,\n        **kwargs: Any,\n    ) -> ""Dataset"":\n        """"""Fill in NaNs by interpolating according to different methods.\n\n        Parameters\n        ----------\n        dim : str\n            Specifies the dimension along which to interpolate.\n\n        method : str, optional\n            String indicating which method to use for interpolation:\n\n            - \'linear\': linear interpolation (Default). Additional keyword\n              arguments are passed to :py:func:`numpy.interp`\n            - \'nearest\', \'zero\', \'slinear\', \'quadratic\', \'cubic\', \'polynomial\':\n              are passed to :py:func:`scipy.interpolate.interp1d`. If\n              ``method=\'polynomial\'``, the ``order`` keyword argument must also be\n              provided.\n            - \'barycentric\', \'krog\', \'pchip\', \'spline\', \'akima\': use their\n              respective :py:class:`scipy.interpolate` classes.\n\n        use_coordinate : bool, str, default True\n            Specifies which index to use as the x values in the interpolation\n            formulated as `y = f(x)`. If False, values are treated as if\n            eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is\n            used. If ``use_coordinate`` is a string, it specifies the name of a\n            coordinate variariable to use as the index.\n        limit : int, default None\n            Maximum number of consecutive NaNs to fill. Must be greater than 0\n            or None for no limit. This filling is done regardless of the size of\n            the gap in the data. To only interpolate over gaps less than a given length,\n            see ``max_gap``.\n        max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default None.\n            Maximum size of gap, a continuous sequence of NaNs, that will be filled.\n            Use None for no limit. When interpolating along a datetime64 dimension\n            and ``use_coordinate=True``, ``max_gap`` can be one of the following:\n\n            - a string that is valid input for pandas.to_timedelta\n            - a :py:class:`numpy.timedelta64` object\n            - a :py:class:`pandas.Timedelta` object\n            - a :py:class:`datetime.timedelta` object\n\n            Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled\n            dimensions has not been implemented yet. Gap length is defined as the difference\n            between coordinate values at the first data point after a gap and the last value\n            before a gap. For gaps at the beginning (end), gap length is defined as the difference\n            between coordinate values at the first (last) valid data point and the first (last) NaN.\n            For example, consider::\n\n                <xarray.DataArray (x: 9)>\n                array([nan, nan, nan,  1., nan, nan,  4., nan, nan])\n                Coordinates:\n                  * x        (x) int64 0 1 2 3 4 5 6 7 8\n\n            The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively\n        kwargs : dict, optional\n            parameters passed verbatim to the underlying interpolation function\n\n        Returns\n        -------\n        interpolated: Dataset\n            Filled in Dataset.\n\n        See also\n        --------\n        numpy.interp\n        scipy.interpolate\n        """"""\n        from .missing import interp_na, _apply_over_vars_with_dim\n\n        new = _apply_over_vars_with_dim(\n            interp_na,\n            self,\n            dim=dim,\n            method=method,\n            limit=limit,\n            use_coordinate=use_coordinate,\n            max_gap=max_gap,\n            **kwargs,\n        )\n        return new\n\n    def ffill(self, dim: Hashable, limit: int = None) -> ""Dataset"":\n        """"""Fill NaN values by propogating values forward\n\n        *Requires bottleneck.*\n\n        Parameters\n        ----------\n        dim : Hashable\n            Specifies the dimension along which to propagate values when\n            filling.\n        limit : int, default None\n            The maximum number of consecutive NaN values to forward fill. In\n            other words, if there is a gap with more than this number of\n            consecutive NaNs, it will only be partially filled. Must be greater\n            than 0 or None for no limit.\n\n        Returns\n        -------\n        Dataset\n        """"""\n        from .missing import ffill, _apply_over_vars_with_dim\n\n        new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)\n        return new\n\n    def bfill(self, dim: Hashable, limit: int = None) -> ""Dataset"":\n        """"""Fill NaN values by propogating values backward\n\n        *Requires bottleneck.*\n\n        Parameters\n        ----------\n        dim : str\n            Specifies the dimension along which to propagate values when\n            filling.\n        limit : int, default None\n            The maximum number of consecutive NaN values to backward fill. In\n            other words, if there is a gap with more than this number of\n            consecutive NaNs, it will only be partially filled. Must be greater\n            than 0 or None for no limit.\n\n        Returns\n        -------\n        Dataset\n        """"""\n        from .missing import bfill, _apply_over_vars_with_dim\n\n        new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)\n        return new\n\n    def combine_first(self, other: ""Dataset"") -> ""Dataset"":\n        """"""Combine two Datasets, default to data_vars of self.\n\n        The new coordinates follow the normal broadcasting and alignment rules\n        of ``join=\'outer\'``.  Vacant cells in the expanded coordinates are\n        filled with np.nan.\n\n        Parameters\n        ----------\n        other : Dataset\n            Used to fill all matching missing values in this array.\n\n        Returns\n        -------\n        Dataset\n        """"""\n        out = ops.fillna(self, other, join=""outer"", dataset_join=""outer"")\n        return out\n\n    def reduce(\n        self,\n        func: Callable,\n        dim: Union[Hashable, Iterable[Hashable]] = None,\n        keep_attrs: bool = None,\n        keepdims: bool = False,\n        numeric_only: bool = False,\n        allow_lazy: bool = None,\n        **kwargs: Any,\n    ) -> ""Dataset"":\n        """"""Reduce this dataset by applying `func` along some dimension(s).\n\n        Parameters\n        ----------\n        func : callable\n            Function which can be called in the form\n            `f(x, axis=axis, **kwargs)` to return the result of reducing an\n            np.ndarray over an integer valued axis.\n        dim : str or sequence of str, optional\n            Dimension(s) over which to apply `func`.  By default `func` is\n            applied over all dimensions.\n        keep_attrs : bool, optional\n            If True, the dataset\'s attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n        keepdims : bool, default False\n            If True, the dimensions which are reduced are left in the result\n            as dimensions of size one. Coordinates that use these dimensions\n            are removed.\n        numeric_only : bool, optional\n            If True, only apply ``func`` to variables with a numeric dtype.\n        **kwargs : Any\n            Additional keyword arguments passed on to ``func``.\n\n        Returns\n        -------\n        reduced : Dataset\n            Dataset with this object\'s DataArrays replaced with new DataArrays\n            of summarized data and the indicated dimension(s) removed.\n        """"""\n        if dim is None or dim is ...:\n            dims = set(self.dims)\n        elif isinstance(dim, str) or not isinstance(dim, Iterable):\n            dims = {dim}\n        else:\n            dims = set(dim)\n\n        missing_dimensions = [d for d in dims if d not in self.dims]\n        if missing_dimensions:\n            raise ValueError(\n                ""Dataset does not contain the dimensions: %s"" % missing_dimensions\n            )\n\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n\n        variables: Dict[Hashable, Variable] = {}\n        for name, var in self._variables.items():\n            reduce_dims = [d for d in var.dims if d in dims]\n            if name in self.coords:\n                if not reduce_dims:\n                    variables[name] = var\n            else:\n                if (\n                    not numeric_only\n                    or np.issubdtype(var.dtype, np.number)\n                    or (var.dtype == np.bool_)\n                ):\n                    if len(reduce_dims) == 1:\n                        # unpack dimensions for the benefit of functions\n                        # like np.argmin which can\'t handle tuple arguments\n                        (reduce_dims,) = reduce_dims\n                    elif len(reduce_dims) == var.ndim:\n                        # prefer to aggregate over axis=None rather than\n                        # axis=(0, 1) if they will be equivalent, because\n                        # the former is often more efficient\n                        reduce_dims = None  # type: ignore\n                    variables[name] = var.reduce(\n                        func,\n                        dim=reduce_dims,\n                        keep_attrs=keep_attrs,\n                        keepdims=keepdims,\n                        allow_lazy=allow_lazy,\n                        **kwargs,\n                    )\n\n        coord_names = {k for k in self.coords if k in variables}\n        indexes = {k: v for k, v in self.indexes.items() if k in variables}\n        attrs = self.attrs if keep_attrs else None\n        return self._replace_with_new_dims(\n            variables, coord_names=coord_names, attrs=attrs, indexes=indexes\n        )\n\n    def map(\n        self,\n        func: Callable,\n        keep_attrs: bool = None,\n        args: Iterable[Any] = (),\n        **kwargs: Any,\n    ) -> ""Dataset"":\n        """"""Apply a function to each variable in this dataset\n\n        Parameters\n        ----------\n        func : callable\n            Function which can be called in the form `func(x, *args, **kwargs)`\n            to transform each DataArray `x` in this dataset into another\n            DataArray.\n        keep_attrs : bool, optional\n            If True, the dataset\'s attributes (`attrs`) will be copied from\n            the original object to the new one. If False, the new object will\n            be returned without attributes.\n        args : tuple, optional\n            Positional arguments passed on to `func`.\n        **kwargs : Any\n            Keyword arguments passed on to `func`.\n\n        Returns\n        -------\n        applied : Dataset\n            Resulting dataset from applying ``func`` to each data variable.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(np.random.randn(2, 3))\n        >>> ds = xr.Dataset({""foo"": da, ""bar"": (""x"", [-1, 2])})\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n        Dimensions without coordinates: dim_0, dim_1, x\n        Data variables:\n            foo      (dim_0, dim_1) float64 -0.3751 -1.951 -1.945 0.2948 0.711 -0.3948\n            bar      (x) int64 -1 2\n        >>> ds.map(np.fabs)\n        <xarray.Dataset>\n        Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n        Dimensions without coordinates: dim_0, dim_1, x\n        Data variables:\n            foo      (dim_0, dim_1) float64 0.3751 1.951 1.945 0.2948 0.711 0.3948\n            bar      (x) float64 1.0 2.0\n        """"""\n        variables = {\n            k: maybe_wrap_array(v, func(v, *args, **kwargs))\n            for k, v in self.data_vars.items()\n        }\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n        attrs = self.attrs if keep_attrs else None\n        return type(self)(variables, attrs=attrs)\n\n    def apply(\n        self,\n        func: Callable,\n        keep_attrs: bool = None,\n        args: Iterable[Any] = (),\n        **kwargs: Any,\n    ) -> ""Dataset"":\n        """"""\n        Backward compatible implementation of ``map``\n\n        See Also\n        --------\n        Dataset.map\n        """"""\n        warnings.warn(\n            ""Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged"",\n            PendingDeprecationWarning,\n            stacklevel=2,\n        )\n        return self.map(func, keep_attrs, args, **kwargs)\n\n    def assign(\n        self, variables: Mapping[Hashable, Any] = None, **variables_kwargs: Hashable\n    ) -> ""Dataset"":\n        """"""Assign new data variables to a Dataset, returning a new object\n        with all the original variables in addition to the new ones.\n\n        Parameters\n        ----------\n        variables : mapping, value pairs\n            Mapping from variables names to the new values. If the new values\n            are callable, they are computed on the Dataset and assigned to new\n            data variables. If the values are not callable, (e.g. a DataArray,\n            scalar, or array), they are simply assigned.\n        **variables_kwargs:\n            The keyword arguments form of ``variables``.\n            One of variables or variables_kwargs must be provided.\n\n        Returns\n        -------\n        ds : Dataset\n            A new Dataset with the new variables in addition to all the\n            existing variables.\n\n        Notes\n        -----\n        Since ``kwargs`` is a dictionary, the order of your arguments may not\n        be preserved, and so the order of the new variables is not well\n        defined. Assigning multiple variables within the same ``assign`` is\n        possible, but you cannot reference other variables created within the\n        same ``assign`` call.\n\n        See Also\n        --------\n        pandas.DataFrame.assign\n\n        Examples\n        --------\n        >>> x = xr.Dataset(\n        ...     {\n        ...         ""temperature_c"": (\n        ...             (""lat"", ""lon""),\n        ...             20 * np.random.rand(4).reshape(2, 2),\n        ...         ),\n        ...         ""precipitation"": ((""lat"", ""lon""), np.random.rand(4).reshape(2, 2)),\n        ...     },\n        ...     coords={""lat"": [10, 20], ""lon"": [150, 160]},\n        ... )\n        >>> x\n        <xarray.Dataset>\n        Dimensions:        (lat: 2, lon: 2)\n        Coordinates:\n        * lat            (lat) int64 10 20\n        * lon            (lon) int64 150 160\n        Data variables:\n            temperature_c  (lat, lon) float64 18.04 12.51 17.64 9.313\n            precipitation  (lat, lon) float64 0.4751 0.6827 0.3697 0.03524\n\n        Where the value is a callable, evaluated on dataset:\n\n        >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)\n        <xarray.Dataset>\n        Dimensions:        (lat: 2, lon: 2)\n        Coordinates:\n        * lat            (lat) int64 10 20\n        * lon            (lon) int64 150 160\n        Data variables:\n            temperature_c  (lat, lon) float64 18.04 12.51 17.64 9.313\n            precipitation  (lat, lon) float64 0.4751 0.6827 0.3697 0.03524\n            temperature_f  (lat, lon) float64 64.47 54.51 63.75 48.76\n\n        Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:\n\n        >>> x.assign(temperature_f=x[""temperature_c""] * 9 / 5 + 32)\n        <xarray.Dataset>\n        Dimensions:        (lat: 2, lon: 2)\n        Coordinates:\n        * lat            (lat) int64 10 20\n        * lon            (lon) int64 150 160\n        Data variables:\n            temperature_c  (lat, lon) float64 18.04 12.51 17.64 9.313\n            precipitation  (lat, lon) float64 0.4751 0.6827 0.3697 0.03524\n            temperature_f  (lat, lon) float64 64.47 54.51 63.75 48.76\n\n        """"""\n        variables = either_dict_or_kwargs(variables, variables_kwargs, ""assign"")\n        data = self.copy()\n        # do all calculations first...\n        results = data._calc_assign_results(variables)\n        # ... and then assign\n        data.update(results)\n        return data\n\n    def to_array(self, dim=""variable"", name=None):\n        """"""Convert this dataset into an xarray.DataArray\n\n        The data variables of this dataset will be broadcast against each other\n        and stacked along the first axis of the new array. All coordinates of\n        this dataset will remain coordinates.\n\n        Parameters\n        ----------\n        dim : str, optional\n            Name of the new dimension.\n        name : str, optional\n            Name of the new data array.\n\n        Returns\n        -------\n        array : xarray.DataArray\n        """"""\n        from .dataarray import DataArray\n\n        data_vars = [self.variables[k] for k in self.data_vars]\n        broadcast_vars = broadcast_variables(*data_vars)\n        data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)\n\n        coords = dict(self.coords)\n        coords[dim] = list(self.data_vars)\n        indexes = propagate_indexes(self._indexes)\n\n        dims = (dim,) + broadcast_vars[0].dims\n\n        return DataArray(\n            data, coords, dims, attrs=self.attrs, name=name, indexes=indexes\n        )\n\n    def _to_dataframe(self, ordered_dims):\n        columns = [k for k in self.variables if k not in self.dims]\n        data = [\n            self._variables[k].set_dims(ordered_dims).values.reshape(-1)\n            for k in columns\n        ]\n        index = self.coords.to_index(ordered_dims)\n        return pd.DataFrame(dict(zip(columns, data)), index=index)\n\n    def to_dataframe(self):\n        """"""Convert this dataset into a pandas.DataFrame.\n\n        Non-index variables in this dataset form the columns of the\n        DataFrame. The DataFrame is be indexed by the Cartesian product of\n        this dataset\'s indices.\n        """"""\n        return self._to_dataframe(self.dims)\n\n    def _set_sparse_data_from_dataframe(\n        self, dataframe: pd.DataFrame, dims: tuple\n    ) -> None:\n        from sparse import COO\n\n        idx = dataframe.index\n        if isinstance(idx, pd.MultiIndex):\n            coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)\n            is_sorted = idx.is_lexsorted()\n            shape = tuple(lev.size for lev in idx.levels)\n        else:\n            coords = np.arange(idx.size).reshape(1, -1)\n            is_sorted = True\n            shape = (idx.size,)\n\n        for name, series in dataframe.items():\n            # Cast to a NumPy array first, in case the Series is a pandas\n            # Extension array (which doesn\'t have a valid NumPy dtype)\n            values = np.asarray(series)\n\n            # In virtually all real use cases, the sparse array will now have\n            # missing values and needs a fill_value. For consistency, don\'t\n            # special case the rare exceptions (e.g., dtype=int without a\n            # MultiIndex).\n            dtype, fill_value = dtypes.maybe_promote(values.dtype)\n            values = np.asarray(values, dtype=dtype)\n\n            data = COO(\n                coords,\n                values,\n                shape,\n                has_duplicates=False,\n                sorted=is_sorted,\n                fill_value=fill_value,\n            )\n            self[name] = (dims, data)\n\n    def _set_numpy_data_from_dataframe(\n        self, dataframe: pd.DataFrame, dims: tuple\n    ) -> None:\n        idx = dataframe.index\n        if isinstance(idx, pd.MultiIndex):\n            # expand the DataFrame to include the product of all levels\n            full_idx = pd.MultiIndex.from_product(idx.levels, names=idx.names)\n            dataframe = dataframe.reindex(full_idx)\n            shape = tuple(lev.size for lev in idx.levels)\n        else:\n            shape = (idx.size,)\n        for name, series in dataframe.items():\n            data = np.asarray(series).reshape(shape)\n            self[name] = (dims, data)\n\n    @classmethod\n    def from_dataframe(cls, dataframe: pd.DataFrame, sparse: bool = False) -> ""Dataset"":\n        """"""Convert a pandas.DataFrame into an xarray.Dataset\n\n        Each column will be converted into an independent variable in the\n        Dataset. If the dataframe\'s index is a MultiIndex, it will be expanded\n        into a tensor product of one-dimensional indices (filling in missing\n        values with NaN). This method will produce a Dataset very similar to\n        that on which the \'to_dataframe\' method was called, except with\n        possibly redundant dimensions (since all dataset variables will have\n        the same dimensionality)\n\n        Parameters\n        ----------\n        dataframe : pandas.DataFrame\n            DataFrame from which to copy data and indices.\n        sparse : bool\n            If true, create a sparse arrays instead of dense numpy arrays. This\n            can potentially save a large amount of memory if the DataFrame has\n            a MultiIndex. Requires the sparse package (sparse.pydata.org).\n\n        Returns\n        -------\n        New Dataset.\n\n        See also\n        --------\n        xarray.DataArray.from_series\n        pandas.DataFrame.to_xarray\n        """"""\n        # TODO: Add an option to remove dimensions along which the variables\n        # are constant, to enable consistent serialization to/from a dataframe,\n        # even if some variables have different dimensionality.\n\n        if not dataframe.columns.is_unique:\n            raise ValueError(""cannot convert DataFrame with non-unique columns"")\n\n        idx, dataframe = remove_unused_levels_categories(dataframe.index, dataframe)\n        obj = cls()\n\n        if isinstance(idx, pd.MultiIndex):\n            dims = tuple(\n                name if name is not None else ""level_%i"" % n\n                for n, name in enumerate(idx.names)\n            )\n            for dim, lev in zip(dims, idx.levels):\n                obj[dim] = (dim, lev)\n        else:\n            index_name = idx.name if idx.name is not None else ""index""\n            dims = (index_name,)\n            obj[index_name] = (dims, idx)\n\n        if sparse:\n            obj._set_sparse_data_from_dataframe(dataframe, dims)\n        else:\n            obj._set_numpy_data_from_dataframe(dataframe, dims)\n        return obj\n\n    def to_dask_dataframe(self, dim_order=None, set_index=False):\n        """"""\n        Convert this dataset into a dask.dataframe.DataFrame.\n\n        The dimensions, coordinates and data variables in this dataset form\n        the columns of the DataFrame.\n\n        Parameters\n        ----------\n        dim_order : list, optional\n            Hierarchical dimension order for the resulting dataframe. All\n            arrays are transposed to this order and then written out as flat\n            vectors in contiguous order, so the last dimension in this list\n            will be contiguous in the resulting DataFrame. This has a major\n            influence on which operations are efficient on the resulting dask\n            dataframe.\n\n            If provided, must include all dimensions on this dataset. By\n            default, dimensions are sorted alphabetically.\n        set_index : bool, optional\n            If set_index=True, the dask DataFrame is indexed by this dataset\'s\n            coordinate. Since dask DataFrames to not support multi-indexes,\n            set_index only works if the dataset only contains one dimension.\n\n        Returns\n        -------\n        dask.dataframe.DataFrame\n        """"""\n\n        import dask.array as da\n        import dask.dataframe as dd\n\n        if dim_order is None:\n            dim_order = list(self.dims)\n        elif set(dim_order) != set(self.dims):\n            raise ValueError(\n                ""dim_order {} does not match the set of dimensions on this ""\n                ""Dataset: {}"".format(dim_order, list(self.dims))\n            )\n\n        ordered_dims = {k: self.dims[k] for k in dim_order}\n\n        columns = list(ordered_dims)\n        columns.extend(k for k in self.coords if k not in self.dims)\n        columns.extend(self.data_vars)\n\n        series_list = []\n        for name in columns:\n            try:\n                var = self.variables[name]\n            except KeyError:\n                # dimension without a matching coordinate\n                size = self.dims[name]\n                data = da.arange(size, chunks=size, dtype=np.int64)\n                var = Variable((name,), data)\n\n            # IndexVariable objects have a dummy .chunk() method\n            if isinstance(var, IndexVariable):\n                var = var.to_base_variable()\n\n            dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data\n            series = dd.from_array(dask_array.reshape(-1), columns=[name])\n            series_list.append(series)\n\n        df = dd.concat(series_list, axis=1)\n\n        if set_index:\n            if len(dim_order) == 1:\n                (dim,) = dim_order\n                df = df.set_index(dim)\n            else:\n                # triggers an error about multi-indexes, even if only one\n                # dimension is passed\n                df = df.set_index(dim_order)\n\n        return df\n\n    def to_dict(self, data=True):\n        """"""\n        Convert this dataset to a dictionary following xarray naming\n        conventions.\n\n        Converts all variables and attributes to native Python objects\n        Useful for converting to json. To avoid datetime incompatibility\n        use decode_times=False kwarg in xarrray.open_dataset.\n\n        Parameters\n        ----------\n        data : bool, optional\n            Whether to include the actual data in the dictionary. When set to\n            False, returns just the schema.\n\n        See also\n        --------\n        Dataset.from_dict\n        """"""\n        d = {\n            ""coords"": {},\n            ""attrs"": decode_numpy_dict_values(self.attrs),\n            ""dims"": dict(self.dims),\n            ""data_vars"": {},\n        }\n        for k in self.coords:\n            d[""coords""].update({k: self[k].variable.to_dict(data=data)})\n        for k in self.data_vars:\n            d[""data_vars""].update({k: self[k].variable.to_dict(data=data)})\n        return d\n\n    @classmethod\n    def from_dict(cls, d):\n        """"""\n        Convert a dictionary into an xarray.Dataset.\n\n        Input dict can take several forms::\n\n            d = {\'t\': {\'dims\': (\'t\'), \'data\': t},\n                 \'a\': {\'dims\': (\'t\'), \'data\': x},\n                 \'b\': {\'dims\': (\'t\'), \'data\': y}}\n\n            d = {\'coords\': {\'t\': {\'dims\': \'t\', \'data\': t,\n                                  \'attrs\': {\'units\':\'s\'}}},\n                 \'attrs\': {\'title\': \'air temperature\'},\n                 \'dims\': \'t\',\n                 \'data_vars\': {\'a\': {\'dims\': \'t\', \'data\': x, },\n                               \'b\': {\'dims\': \'t\', \'data\': y}}}\n\n        where \'t\' is the name of the dimesion, \'a\' and \'b\' are names of data\n        variables and t, x, and y are lists, numpy.arrays or pandas objects.\n\n        Parameters\n        ----------\n        d : dict, with a minimum structure of {\'var_0\': {\'dims\': [..], \\\n                                                         \'data\': [..]}, \\\n                                               ...}\n\n        Returns\n        -------\n        obj : xarray.Dataset\n\n        See also\n        --------\n        Dataset.to_dict\n        DataArray.from_dict\n        """"""\n\n        if not {""coords"", ""data_vars""}.issubset(set(d)):\n            variables = d.items()\n        else:\n            import itertools\n\n            variables = itertools.chain(\n                d.get(""coords"", {}).items(), d.get(""data_vars"", {}).items()\n            )\n        try:\n            variable_dict = {\n                k: (v[""dims""], v[""data""], v.get(""attrs"")) for k, v in variables\n            }\n        except KeyError as e:\n            raise ValueError(\n                ""cannot convert dict without the key ""\n                ""\'{dims_data}\'"".format(dims_data=str(e.args[0]))\n            )\n        obj = cls(variable_dict)\n\n        # what if coords aren\'t dims?\n        coords = set(d.get(""coords"", {})) - set(d.get(""dims"", {}))\n        obj = obj.set_coords(coords)\n\n        obj.attrs.update(d.get(""attrs"", {}))\n\n        return obj\n\n    @staticmethod\n    def _unary_op(f, keep_attrs=False):\n        @functools.wraps(f)\n        def func(self, *args, **kwargs):\n            variables = {}\n            for k, v in self._variables.items():\n                if k in self._coord_names:\n                    variables[k] = v\n                else:\n                    variables[k] = f(v, *args, **kwargs)\n            attrs = self._attrs if keep_attrs else None\n            return self._replace_with_new_dims(variables, attrs=attrs)\n\n        return func\n\n    @staticmethod\n    def _binary_op(f, reflexive=False, join=None):\n        @functools.wraps(f)\n        def func(self, other):\n            from .dataarray import DataArray\n\n            if isinstance(other, groupby.GroupBy):\n                return NotImplemented\n            align_type = OPTIONS[""arithmetic_join""] if join is None else join\n            if isinstance(other, (DataArray, Dataset)):\n                self, other = align(self, other, join=align_type, copy=False)\n            g = f if not reflexive else lambda x, y: f(y, x)\n            ds = self._calculate_binary_op(g, other, join=align_type)\n            return ds\n\n        return func\n\n    @staticmethod\n    def _inplace_binary_op(f):\n        @functools.wraps(f)\n        def func(self, other):\n            from .dataarray import DataArray\n\n            if isinstance(other, groupby.GroupBy):\n                raise TypeError(\n                    ""in-place operations between a Dataset and ""\n                    ""a grouped object are not permitted""\n                )\n            # we don\'t actually modify arrays in-place with in-place Dataset\n            # arithmetic -- this lets us automatically align things\n            if isinstance(other, (DataArray, Dataset)):\n                other = other.reindex_like(self, copy=False)\n            g = ops.inplace_to_noninplace_op(f)\n            ds = self._calculate_binary_op(g, other, inplace=True)\n            self._replace_with_new_dims(\n                ds._variables,\n                ds._coord_names,\n                attrs=ds._attrs,\n                indexes=ds._indexes,\n                inplace=True,\n            )\n            return self\n\n        return func\n\n    def _calculate_binary_op(self, f, other, join=""inner"", inplace=False):\n        def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):\n            if inplace and set(lhs_data_vars) != set(rhs_data_vars):\n                raise ValueError(\n                    ""datasets must have the same data variables ""\n                    ""for in-place arithmetic operations: %s, %s""\n                    % (list(lhs_data_vars), list(rhs_data_vars))\n                )\n\n            dest_vars = {}\n\n            for k in lhs_data_vars:\n                if k in rhs_data_vars:\n                    dest_vars[k] = f(lhs_vars[k], rhs_vars[k])\n                elif join in [""left"", ""outer""]:\n                    dest_vars[k] = f(lhs_vars[k], np.nan)\n            for k in rhs_data_vars:\n                if k not in dest_vars and join in [""right"", ""outer""]:\n                    dest_vars[k] = f(rhs_vars[k], np.nan)\n            return dest_vars\n\n        if utils.is_dict_like(other) and not isinstance(other, Dataset):\n            # can\'t use our shortcut of doing the binary operation with\n            # Variable objects, so apply over our data vars instead.\n            new_data_vars = apply_over_both(\n                self.data_vars, other, self.data_vars, other\n            )\n            return Dataset(new_data_vars)\n\n        other_coords = getattr(other, ""coords"", None)\n        ds = self.coords.merge(other_coords)\n\n        if isinstance(other, Dataset):\n            new_vars = apply_over_both(\n                self.data_vars, other.data_vars, self.variables, other.variables\n            )\n        else:\n            other_variable = getattr(other, ""variable"", other)\n            new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}\n        ds._variables.update(new_vars)\n        ds._dims = calculate_dimensions(ds._variables)\n        return ds\n\n    def _copy_attrs_from(self, other):\n        self.attrs = other.attrs\n        for v in other.variables:\n            if v in self.variables:\n                self.variables[v].attrs = other.variables[v].attrs\n\n    def diff(self, dim, n=1, label=""upper""):\n        """"""Calculate the n-th order discrete difference along given axis.\n\n        Parameters\n        ----------\n        dim : str\n            Dimension over which to calculate the finite difference.\n        n : int, optional\n            The number of times values are differenced.\n        label : str, optional\n            The new coordinate in dimension ``dim`` will have the\n            values of either the minuend\'s or subtrahend\'s coordinate\n            for values \'upper\' and \'lower\', respectively.  Other\n            values are not supported.\n\n        Returns\n        -------\n        difference : same type as caller\n            The n-th order finite difference of this object.\n\n        .. note::\n\n            `n` matches numpy\'s behavior and is different from pandas\' first\n            argument named `periods`.\n\n        Examples\n        --------\n        >>> ds = xr.Dataset({""foo"": (""x"", [5, 5, 6, 6])})\n        >>> ds.diff(""x"")\n        <xarray.Dataset>\n        Dimensions:  (x: 3)\n        Coordinates:\n          * x        (x) int64 1 2 3\n        Data variables:\n            foo      (x) int64 0 1 0\n        >>> ds.diff(""x"", 2)\n        <xarray.Dataset>\n        Dimensions:  (x: 2)\n        Coordinates:\n        * x        (x) int64 2 3\n        Data variables:\n        foo      (x) int64 1 -1\n\n        See Also\n        --------\n        Dataset.differentiate\n        """"""\n        if n == 0:\n            return self\n        if n < 0:\n            raise ValueError(f""order `n` must be non-negative but got {n}"")\n\n        # prepare slices\n        kwargs_start = {dim: slice(None, -1)}\n        kwargs_end = {dim: slice(1, None)}\n\n        # prepare new coordinate\n        if label == ""upper"":\n            kwargs_new = kwargs_end\n        elif label == ""lower"":\n            kwargs_new = kwargs_start\n        else:\n            raise ValueError(\n                ""The \'label\' argument has to be either "" ""\'upper\' or \'lower\'""\n            )\n\n        variables = {}\n\n        for name, var in self.variables.items():\n            if dim in var.dims:\n                if name in self.data_vars:\n                    variables[name] = var.isel(**kwargs_end) - var.isel(**kwargs_start)\n                else:\n                    variables[name] = var.isel(**kwargs_new)\n            else:\n                variables[name] = var\n\n        indexes = dict(self.indexes)\n        if dim in indexes:\n            indexes[dim] = indexes[dim][kwargs_new[dim]]\n\n        difference = self._replace_with_new_dims(variables, indexes=indexes)\n\n        if n > 1:\n            return difference.diff(dim, n - 1)\n        else:\n            return difference\n\n    def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):\n        """"""Shift this dataset by an offset along one or more dimensions.\n\n        Only data variables are moved; coordinates stay in place. This is\n        consistent with the behavior of ``shift`` in pandas.\n\n        Parameters\n        ----------\n        shifts : Mapping with the form of {dim: offset}\n            Integer offset to shift along each of the given dimensions.\n            Positive offsets shift to the right; negative offsets shift to the\n            left.\n        fill_value: scalar, optional\n            Value to use for newly missing values\n        **shifts_kwargs:\n            The keyword arguments form of ``shifts``.\n            One of shifts or shifts_kwargs must be provided.\n\n        Returns\n        -------\n        shifted : Dataset\n            Dataset with the same coordinates and attributes but shifted data\n            variables.\n\n        See also\n        --------\n        roll\n\n        Examples\n        --------\n\n        >>> ds = xr.Dataset({""foo"": (""x"", list(""abcde""))})\n        >>> ds.shift(x=2)\n        <xarray.Dataset>\n        Dimensions:  (x: 5)\n        Coordinates:\n          * x        (x) int64 0 1 2 3 4\n        Data variables:\n            foo      (x) object nan nan \'a\' \'b\' \'c\'\n        """"""\n        shifts = either_dict_or_kwargs(shifts, shifts_kwargs, ""shift"")\n        invalid = [k for k in shifts if k not in self.dims]\n        if invalid:\n            raise ValueError(""dimensions %r do not exist"" % invalid)\n\n        variables = {}\n        for name, var in self.variables.items():\n            if name in self.data_vars:\n                var_shifts = {k: v for k, v in shifts.items() if k in var.dims}\n                variables[name] = var.shift(fill_value=fill_value, shifts=var_shifts)\n            else:\n                variables[name] = var\n\n        return self._replace(variables)\n\n    def roll(self, shifts=None, roll_coords=None, **shifts_kwargs):\n        """"""Roll this dataset by an offset along one or more dimensions.\n\n        Unlike shift, roll may rotate all variables, including coordinates\n        if specified. The direction of rotation is consistent with\n        :py:func:`numpy.roll`.\n\n        Parameters\n        ----------\n\n        shifts : dict, optional\n            A dict with keys matching dimensions and values given\n            by integers to rotate each of the given dimensions. Positive\n            offsets roll to the right; negative offsets roll to the left.\n        roll_coords : bool\n            Indicates whether to  roll the coordinates by the offset\n            The current default of roll_coords (None, equivalent to True) is\n            deprecated and will change to False in a future version.\n            Explicitly pass roll_coords to silence the warning.\n        **shifts_kwargs : {dim: offset, ...}, optional\n            The keyword arguments form of ``shifts``.\n            One of shifts or shifts_kwargs must be provided.\n        Returns\n        -------\n        rolled : Dataset\n            Dataset with the same coordinates and attributes but rolled\n            variables.\n\n        See also\n        --------\n        shift\n\n        Examples\n        --------\n\n        >>> ds = xr.Dataset({""foo"": (""x"", list(""abcde""))})\n        >>> ds.roll(x=2)\n        <xarray.Dataset>\n        Dimensions:  (x: 5)\n        Coordinates:\n          * x        (x) int64 3 4 0 1 2\n        Data variables:\n            foo      (x) object \'d\' \'e\' \'a\' \'b\' \'c\'\n        """"""\n        shifts = either_dict_or_kwargs(shifts, shifts_kwargs, ""roll"")\n        invalid = [k for k in shifts if k not in self.dims]\n        if invalid:\n            raise ValueError(""dimensions %r do not exist"" % invalid)\n\n        if roll_coords is None:\n            warnings.warn(\n                ""roll_coords will be set to False in the future.""\n                "" Explicitly set roll_coords to silence warning."",\n                FutureWarning,\n                stacklevel=2,\n            )\n            roll_coords = True\n\n        unrolled_vars = () if roll_coords else self.coords\n\n        variables = {}\n        for k, v in self.variables.items():\n            if k not in unrolled_vars:\n                variables[k] = v.roll(\n                    **{k: s for k, s in shifts.items() if k in v.dims}\n                )\n            else:\n                variables[k] = v\n\n        if roll_coords:\n            indexes = {}\n            for k, v in self.indexes.items():\n                (dim,) = self.variables[k].dims\n                if dim in shifts:\n                    indexes[k] = roll_index(v, shifts[dim])\n                else:\n                    indexes[k] = v\n        else:\n            indexes = dict(self.indexes)\n\n        return self._replace(variables, indexes=indexes)\n\n    def sortby(self, variables, ascending=True):\n        """"""\n        Sort object by labels or values (along an axis).\n\n        Sorts the dataset, either along specified dimensions,\n        or according to values of 1-D dataarrays that share dimension\n        with calling object.\n\n        If the input variables are dataarrays, then the dataarrays are aligned\n        (via left-join) to the calling object prior to sorting by cell values.\n        NaNs are sorted to the end, following Numpy convention.\n\n        If multiple sorts along the same dimension is\n        given, numpy\'s lexsort is performed along that dimension:\n        https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html\n        and the FIRST key in the sequence is used as the primary sort key,\n        followed by the 2nd key, etc.\n\n        Parameters\n        ----------\n        variables: str, DataArray, or list of either\n            1D DataArray objects or name(s) of 1D variable(s) in\n            coords/data_vars whose values are used to sort the dataset.\n        ascending: boolean, optional\n            Whether to sort by ascending or descending order.\n\n        Returns\n        -------\n        sorted: Dataset\n            A new dataset where all the specified dims are sorted by dim\n            labels.\n        """"""\n        from .dataarray import DataArray\n\n        if not isinstance(variables, list):\n            variables = [variables]\n        else:\n            variables = variables\n        variables = [v if isinstance(v, DataArray) else self[v] for v in variables]\n        aligned_vars = align(self, *variables, join=""left"")\n        aligned_self = aligned_vars[0]\n        aligned_other_vars = aligned_vars[1:]\n        vars_by_dim = defaultdict(list)\n        for data_array in aligned_other_vars:\n            if data_array.ndim != 1:\n                raise ValueError(""Input DataArray is not 1-D."")\n            (key,) = data_array.dims\n            vars_by_dim[key].append(data_array)\n\n        indices = {}\n        for key, arrays in vars_by_dim.items():\n            order = np.lexsort(tuple(reversed(arrays)))\n            indices[key] = order if ascending else order[::-1]\n        return aligned_self.isel(**indices)\n\n    def quantile(\n        self,\n        q,\n        dim=None,\n        interpolation=""linear"",\n        numeric_only=False,\n        keep_attrs=None,\n        skipna=True,\n    ):\n        """"""Compute the qth quantile of the data along the specified dimension.\n\n        Returns the qth quantiles(s) of the array elements for each variable\n        in the Dataset.\n\n        Parameters\n        ----------\n        q : float in range of [0,1] or array-like of floats\n            Quantile to compute, which must be between 0 and 1 inclusive.\n        dim : str or sequence of str, optional\n            Dimension(s) over which to apply quantile.\n        interpolation : {\'linear\', \'lower\', \'higher\', \'midpoint\', \'nearest\'}\n            This optional parameter specifies the interpolation method to\n            use when the desired quantile lies between two data points\n            ``i < j``:\n\n                * linear: ``i + (j - i) * fraction``, where ``fraction`` is\n                  the fractional part of the index surrounded by ``i`` and\n                  ``j``.\n                * lower: ``i``.\n                * higher: ``j``.\n                * nearest: ``i`` or ``j``, whichever is nearest.\n                * midpoint: ``(i + j) / 2``.\n        keep_attrs : bool, optional\n            If True, the dataset\'s attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n        numeric_only : bool, optional\n            If True, only apply ``func`` to variables with a numeric dtype.\n        skipna : bool, optional\n            Whether to skip missing values when aggregating.\n\n        Returns\n        -------\n        quantiles : Dataset\n            If `q` is a single quantile, then the result is a scalar for each\n            variable in data_vars. If multiple percentiles are given, first\n            axis of the result corresponds to the quantile and a quantile\n            dimension is added to the return Dataset. The other dimensions are\n            the dimensions that remain after the reduction of the array.\n\n        See Also\n        --------\n        numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile\n\n        Examples\n        --------\n\n        >>> ds = xr.Dataset(\n        ...     {""a"": ((""x"", ""y""), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},\n        ...     coords={""x"": [7, 9], ""y"": [1, 1.5, 2, 2.5]},\n        ... )\n        >>> ds.quantile(0)  # or ds.quantile(0, dim=...)\n        <xarray.Dataset>\n        Dimensions:   ()\n        Coordinates:\n            quantile  float64 0.0\n        Data variables:\n            a         float64 0.7\n        >>> ds.quantile(0, dim=""x"")\n        <xarray.Dataset>\n        Dimensions:   (y: 4)\n        Coordinates:\n          * y         (y) float64 1.0 1.5 2.0 2.5\n            quantile  float64 0.0\n        Data variables:\n            a         (y) float64 0.7 4.2 2.6 1.5\n        >>> ds.quantile([0, 0.5, 1])\n        <xarray.Dataset>\n        Dimensions:   (quantile: 3)\n        Coordinates:\n          * quantile  (quantile) float64 0.0 0.5 1.0\n        Data variables:\n            a         (quantile) float64 0.7 3.4 9.4\n        >>> ds.quantile([0, 0.5, 1], dim=""x"")\n        <xarray.Dataset>\n        Dimensions:   (quantile: 3, y: 4)\n        Coordinates:\n          * y         (y) float64 1.0 1.5 2.0 2.5\n          * quantile  (quantile) float64 0.0 0.5 1.0\n        Data variables:\n            a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9\n        """"""\n\n        if isinstance(dim, str):\n            dims = {dim}\n        elif dim in [None, ...]:\n            dims = set(self.dims)\n        else:\n            dims = set(dim)\n\n        _assert_empty(\n            [d for d in dims if d not in self.dims],\n            ""Dataset does not contain the dimensions: %s"",\n        )\n\n        q = np.asarray(q, dtype=np.float64)\n\n        variables = {}\n        for name, var in self.variables.items():\n            reduce_dims = [d for d in var.dims if d in dims]\n            if reduce_dims or not var.dims:\n                if name not in self.coords:\n                    if (\n                        not numeric_only\n                        or np.issubdtype(var.dtype, np.number)\n                        or var.dtype == np.bool_\n                    ):\n                        if len(reduce_dims) == var.ndim:\n                            # prefer to aggregate over axis=None rather than\n                            # axis=(0, 1) if they will be equivalent, because\n                            # the former is often more efficient\n                            reduce_dims = None\n                        variables[name] = var.quantile(\n                            q,\n                            dim=reduce_dims,\n                            interpolation=interpolation,\n                            keep_attrs=keep_attrs,\n                            skipna=skipna,\n                        )\n\n            else:\n                variables[name] = var\n\n        # construct the new dataset\n        coord_names = {k for k in self.coords if k in variables}\n        indexes = {k: v for k, v in self.indexes.items() if k in variables}\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n        attrs = self.attrs if keep_attrs else None\n        new = self._replace_with_new_dims(\n            variables, coord_names=coord_names, attrs=attrs, indexes=indexes\n        )\n        return new.assign_coords(quantile=q)\n\n    def rank(self, dim, pct=False, keep_attrs=None):\n        """"""Ranks the data.\n\n        Equal values are assigned a rank that is the average of the ranks that\n        would have been otherwise assigned to all of the values within\n        that set.\n        Ranks begin at 1, not 0. If pct is True, computes percentage ranks.\n\n        NaNs in the input array are returned as NaNs.\n\n        The `bottleneck` library is required.\n\n        Parameters\n        ----------\n        dim : str\n            Dimension over which to compute rank.\n        pct : bool, optional\n            If True, compute percentage ranks, otherwise compute integer ranks.\n        keep_attrs : bool, optional\n            If True, the dataset\'s attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n\n        Returns\n        -------\n        ranked : Dataset\n            Variables that do not depend on `dim` are dropped.\n        """"""\n        if dim not in self.dims:\n            raise ValueError(""Dataset does not contain the dimension: %s"" % dim)\n\n        variables = {}\n        for name, var in self.variables.items():\n            if name in self.data_vars:\n                if dim in var.dims:\n                    variables[name] = var.rank(dim, pct=pct)\n            else:\n                variables[name] = var\n\n        coord_names = set(self.coords)\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n        attrs = self.attrs if keep_attrs else None\n        return self._replace(variables, coord_names, attrs=attrs)\n\n    def differentiate(self, coord, edge_order=1, datetime_unit=None):\n        """""" Differentiate with the second order accurate central\n        differences.\n\n        .. note::\n            This feature is limited to simple cartesian geometry, i.e. coord\n            must be one dimensional.\n\n        Parameters\n        ----------\n        coord: str\n            The coordinate to be used to compute the gradient.\n        edge_order: 1 or 2. Default 1\n            N-th order accurate differences at the boundaries.\n        datetime_unit: None or any of {\'Y\', \'M\', \'W\', \'D\', \'h\', \'m\', \'s\', \'ms\',\n            \'us\', \'ns\', \'ps\', \'fs\', \'as\'}\n            Unit to compute gradient. Only valid for datetime coordinate.\n\n        Returns\n        -------\n        differentiated: Dataset\n\n        See also\n        --------\n        numpy.gradient: corresponding numpy function\n        """"""\n        from .variable import Variable\n\n        if coord not in self.variables and coord not in self.dims:\n            raise ValueError(f""Coordinate {coord} does not exist."")\n\n        coord_var = self[coord].variable\n        if coord_var.ndim != 1:\n            raise ValueError(\n                ""Coordinate {} must be 1 dimensional but is {}""\n                "" dimensional"".format(coord, coord_var.ndim)\n            )\n\n        dim = coord_var.dims[0]\n        if _contains_datetime_like_objects(coord_var):\n            if coord_var.dtype.kind in ""mM"" and datetime_unit is None:\n                datetime_unit, _ = np.datetime_data(coord_var.dtype)\n            elif datetime_unit is None:\n                datetime_unit = ""s""  # Default to seconds for cftime objects\n            coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)\n\n        variables = {}\n        for k, v in self.variables.items():\n            if k in self.data_vars and dim in v.dims and k not in self.coords:\n                if _contains_datetime_like_objects(v):\n                    v = v._to_numeric(datetime_unit=datetime_unit)\n                grad = duck_array_ops.gradient(\n                    v.data, coord_var, edge_order=edge_order, axis=v.get_axis_num(dim)\n                )\n                variables[k] = Variable(v.dims, grad)\n            else:\n                variables[k] = v\n        return self._replace(variables)\n\n    def integrate(self, coord, datetime_unit=None):\n        """""" integrate the array with the trapezoidal rule.\n\n        .. note::\n            This feature is limited to simple cartesian geometry, i.e. coord\n            must be one dimensional.\n\n        Parameters\n        ----------\n        coord: str, or a sequence of str\n            Coordinate(s) used for the integration.\n        datetime_unit\n            Can be specify the unit if datetime coordinate is used. One of\n            {\'Y\', \'M\', \'W\', \'D\', \'h\', \'m\', \'s\', \'ms\', \'us\', \'ns\', \'ps\', \'fs\',\n            \'as\'}\n\n        Returns\n        -------\n        integrated: Dataset\n\n        See also\n        --------\n        DataArray.integrate\n        numpy.trapz: corresponding numpy function\n\n        Examples\n        --------\n        >>> ds = xr.Dataset(\n        ...     data_vars={""a"": (""x"", [5, 5, 6, 6]), ""b"": (""x"", [1, 2, 1, 0])},\n        ...     coords={""x"": [0, 1, 2, 3], ""y"": (""x"", [1, 7, 3, 5])},\n        ... )\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (x: 4)\n        Coordinates:\n          * x        (x) int64 0 1 2 3\n            y        (x) int64 1 7 3 5\n        Data variables:\n            a        (x) int64 5 5 6 6\n            b        (x) int64 1 2 1 0\n        >>> ds.integrate(""x"")\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            a        float64 16.5\n            b        float64 3.5\n        >>> ds.integrate(""y"")\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            a        float64 20.0\n            b        float64 4.0\n        """"""\n        if not isinstance(coord, (list, tuple)):\n            coord = (coord,)\n        result = self\n        for c in coord:\n            result = result._integrate_one(c, datetime_unit=datetime_unit)\n        return result\n\n    def _integrate_one(self, coord, datetime_unit=None):\n        from .variable import Variable\n\n        if coord not in self.variables and coord not in self.dims:\n            raise ValueError(f""Coordinate {coord} does not exist."")\n\n        coord_var = self[coord].variable\n        if coord_var.ndim != 1:\n            raise ValueError(\n                ""Coordinate {} must be 1 dimensional but is {}""\n                "" dimensional"".format(coord, coord_var.ndim)\n            )\n\n        dim = coord_var.dims[0]\n        if _contains_datetime_like_objects(coord_var):\n            if coord_var.dtype.kind in ""mM"" and datetime_unit is None:\n                datetime_unit, _ = np.datetime_data(coord_var.dtype)\n            elif datetime_unit is None:\n                datetime_unit = ""s""  # Default to seconds for cftime objects\n            coord_var = coord_var._replace(\n                data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)\n            )\n\n        variables = {}\n        coord_names = set()\n        for k, v in self.variables.items():\n            if k in self.coords:\n                if dim not in v.dims:\n                    variables[k] = v\n                    coord_names.add(k)\n            else:\n                if k in self.data_vars and dim in v.dims:\n                    if _contains_datetime_like_objects(v):\n                        v = datetime_to_numeric(v, datetime_unit=datetime_unit)\n                    integ = duck_array_ops.trapz(\n                        v.data, coord_var.data, axis=v.get_axis_num(dim)\n                    )\n                    v_dims = list(v.dims)\n                    v_dims.remove(dim)\n                    variables[k] = Variable(v_dims, integ)\n                else:\n                    variables[k] = v\n        indexes = {k: v for k, v in self.indexes.items() if k in variables}\n        return self._replace_with_new_dims(\n            variables, coord_names=coord_names, indexes=indexes\n        )\n\n    @property\n    def real(self):\n        return self._unary_op(lambda x: x.real, keep_attrs=True)(self)\n\n    @property\n    def imag(self):\n        return self._unary_op(lambda x: x.imag, keep_attrs=True)(self)\n\n    @property\n    def plot(self):\n        """"""\n        Access plotting functions for Datasets.\n        Use it as a namespace to use xarray.plot functions as Dataset methods\n\n        >>> ds.plot.scatter(...)  # equivalent to xarray.plot.scatter(ds,...)\n\n        """"""\n        return _Dataset_PlotMethods(self)\n\n    def filter_by_attrs(self, **kwargs):\n        """"""Returns a ``Dataset`` with variables that match specific conditions.\n\n        Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned\n        containing only the variables for which all the filter tests pass.\n        These tests are either ``key=value`` for which the attribute ``key``\n        has the exact value ``value`` or the callable passed into\n        ``key=callable`` returns True. The callable will be passed a single\n        value, either the value of the attribute ``key`` or ``None`` if the\n        DataArray does not have an attribute with the name ``key``.\n\n        Parameters\n        ----------\n        **kwargs : key=value\n            key : str\n                Attribute name.\n            value : callable or obj\n                If value is a callable, it should return a boolean in the form\n                of bool = func(attr) where attr is da.attrs[key].\n                Otherwise, value will be compared to the each\n                DataArray\'s attrs[key].\n\n        Returns\n        -------\n        new : Dataset\n            New dataset with variables filtered by attribute.\n\n        Examples\n        --------\n        >>> # Create an example dataset:\n        >>> import numpy as np\n        >>> import pandas as pd\n        >>> import xarray as xr\n        >>> temp = 15 + 8 * np.random.randn(2, 2, 3)\n        >>> precip = 10 * np.random.rand(2, 2, 3)\n        >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]\n        >>> lat = [[42.25, 42.21], [42.63, 42.59]]\n        >>> dims = [""x"", ""y"", ""time""]\n        >>> temp_attr = dict(standard_name=""air_potential_temperature"")\n        >>> precip_attr = dict(standard_name=""convective_precipitation_flux"")\n        >>> ds = xr.Dataset(\n        ...     {\n        ...         ""temperature"": (dims, temp, temp_attr),\n        ...         ""precipitation"": (dims, precip, precip_attr),\n        ...     },\n        ...     coords={\n        ...         ""lon"": ([""x"", ""y""], lon),\n        ...         ""lat"": ([""x"", ""y""], lat),\n        ...         ""time"": pd.date_range(""2014-09-06"", periods=3),\n        ...         ""reference_time"": pd.Timestamp(""2014-09-05""),\n        ...     },\n        ... )\n        >>> # Get variables matching a specific standard_name.\n        >>> ds.filter_by_attrs(standard_name=""convective_precipitation_flux"")\n        <xarray.Dataset>\n        Dimensions:         (time: 3, x: 2, y: 2)\n        Coordinates:\n          * x               (x) int64 0 1\n          * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n            lat             (x, y) float64 42.25 42.21 42.63 42.59\n          * y               (y) int64 0 1\n            reference_time  datetime64[ns] 2014-09-05\n            lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n        Data variables:\n            precipitation   (x, y, time) float64 4.178 2.307 6.041 6.046 0.06648 ...\n        >>> # Get all variables that have a standard_name attribute.\n        >>> standard_name = lambda v: v is not None\n        >>> ds.filter_by_attrs(standard_name=standard_name)\n        <xarray.Dataset>\n        Dimensions:         (time: 3, x: 2, y: 2)\n        Coordinates:\n            lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n            lat             (x, y) float64 42.25 42.21 42.63 42.59\n          * x               (x) int64 0 1\n          * y               (y) int64 0 1\n          * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n            reference_time  datetime64[ns] 2014-09-05\n        Data variables:\n            temperature     (x, y, time) float64 25.86 20.82 6.954 23.13 10.25 11.68 ...\n            precipitation   (x, y, time) float64 5.702 0.9422 2.075 1.178 3.284 ...\n\n        """"""\n        selection = []\n        for var_name, variable in self.variables.items():\n            has_value_flag = False\n            for attr_name, pattern in kwargs.items():\n                attr_value = variable.attrs.get(attr_name)\n                if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:\n                    has_value_flag = True\n                else:\n                    has_value_flag = False\n                    break\n            if has_value_flag is True:\n                selection.append(var_name)\n        return self[selection]\n\n    def unify_chunks(self) -> ""Dataset"":\n        """""" Unify chunk size along all chunked dimensions of this Dataset.\n\n        Returns\n        -------\n\n        Dataset with consistent chunk sizes for all dask-array variables\n\n        See Also\n        --------\n\n        dask.array.core.unify_chunks\n        """"""\n\n        try:\n            self.chunks\n        except ValueError:  # ""inconsistent chunks""\n            pass\n        else:\n            # No variables with dask backend, or all chunks are already aligned\n            return self.copy()\n\n        # import dask is placed after the quick exit test above to allow\n        # running this method if dask isn\'t installed and there are no chunks\n        import dask.array\n\n        ds = self.copy()\n\n        dims_pos_map = {dim: index for index, dim in enumerate(ds.dims)}\n\n        dask_array_names = []\n        dask_unify_args = []\n        for name, variable in ds.variables.items():\n            if isinstance(variable.data, dask.array.Array):\n                dims_tuple = [dims_pos_map[dim] for dim in variable.dims]\n                dask_array_names.append(name)\n                dask_unify_args.append(variable.data)\n                dask_unify_args.append(dims_tuple)\n\n        _, rechunked_arrays = dask.array.core.unify_chunks(*dask_unify_args)\n\n        for name, new_array in zip(dask_array_names, rechunked_arrays):\n            ds.variables[name]._data = new_array\n\n        return ds\n\n    def map_blocks(\n        self,\n        func: ""Callable[..., T_DSorDA]"",\n        args: Sequence[Any] = (),\n        kwargs: Mapping[str, Any] = None,\n        template: Union[""DataArray"", ""Dataset""] = None,\n    ) -> ""T_DSorDA"":\n        """"""\n        Apply a function to each block of this Dataset.\n\n        .. warning::\n            This method is experimental and its signature may change.\n\n        Parameters\n        ----------\n        func: callable\n            User-provided function that accepts a Dataset as its first\n            parameter. The function will receive a subset or \'block\' of this Dataset (see below),\n            corresponding to one chunk along each chunked dimension. ``func`` will be\n            executed as ``func(subset_dataset, *subset_args, **kwargs)``.\n\n            This function must return either a single DataArray or a single Dataset.\n\n            This function cannot add a new chunked dimension.\n\n        obj: DataArray, Dataset\n            Passed to the function as its first argument, one block at a time.\n        args: Sequence\n            Passed to func after unpacking and subsetting any xarray objects by blocks.\n            xarray objects in args must be aligned with obj, otherwise an error is raised.\n        kwargs: Mapping\n            Passed verbatim to func after unpacking. xarray objects, if any, will not be\n            subset to blocks. Passing dask collections in kwargs is not allowed.\n        template: (optional) DataArray, Dataset\n            xarray object representing the final result after compute is called. If not provided,\n            the function will be first run on mocked-up data, that looks like ``obj`` but\n            has sizes 0, to determine properties of the returned object such as dtype,\n            variable names, attributes, new dimensions and new indexes (if any).\n            ``template`` must be provided if the function changes the size of existing dimensions.\n            When provided, ``attrs`` on variables in `template` are copied over to the result. Any\n            ``attrs`` set by ``func`` will be ignored.\n\n\n        Returns\n        -------\n        A single DataArray or Dataset with dask backend, reassembled from the outputs of the\n        function.\n\n        Notes\n        -----\n        This function is designed for when ``func`` needs to manipulate a whole xarray object\n        subset to each block. In the more common case where ``func`` can work on numpy arrays, it is\n        recommended to use ``apply_ufunc``.\n\n        If none of the variables in ``obj`` is backed by dask arrays, calling this function is\n        equivalent to calling ``func(obj, *args, **kwargs)``.\n\n        See Also\n        --------\n        dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks,\n        xarray.DataArray.map_blocks\n\n        Examples\n        --------\n\n        Calculate an anomaly from climatology using ``.groupby()``. Using\n        ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,\n        its indices, and its methods like ``.groupby()``.\n\n        >>> def calculate_anomaly(da, groupby_type=""time.month""):\n        ...     gb = da.groupby(groupby_type)\n        ...     clim = gb.mean(dim=""time"")\n        ...     return gb - clim\n        >>> time = xr.cftime_range(""1990-01"", ""1992-01"", freq=""M"")\n        >>> np.random.seed(123)\n        >>> array = xr.DataArray(\n        ...     np.random.rand(len(time)), dims=""time"", coords=[time]\n        ... ).chunk()\n        >>> ds = xr.Dataset({""a"": array})\n        >>> ds.map_blocks(calculate_anomaly, template=ds).compute()\n        <xarray.DataArray (time: 24)>\n        array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,\n                0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,\n               -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,\n                0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,\n                0.07673453,  0.22865714,  0.19063865, -0.0590131 ])\n        Coordinates:\n          * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n\n        Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments\n        to the function being applied in ``xr.map_blocks()``:\n\n        >>> ds.map_blocks(\n        ...     calculate_anomaly, kwargs={""groupby_type"": ""time.year""}, template=ds,\n        ... )\n        <xarray.DataArray (time: 24)>\n        array([ 0.15361741, -0.25671244, -0.31600032,  0.008463  ,  0.1766172 ,\n               -0.11974531,  0.43791243,  0.14197797, -0.06191987, -0.15073425,\n               -0.19967375,  0.18619794, -0.05100474, -0.42989909, -0.09153273,\n                0.24841842, -0.30708526, -0.31412523,  0.04197439,  0.0422506 ,\n                0.14482397,  0.35985481,  0.23487834,  0.12144652])\n        Coordinates:\n            * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n        """"""\n        from .parallel import map_blocks\n\n        return map_blocks(func, self, args, kwargs, template)\n\n    def polyfit(\n        self,\n        dim: Hashable,\n        deg: int,\n        skipna: bool = None,\n        rcond: float = None,\n        w: Union[Hashable, Any] = None,\n        full: bool = False,\n        cov: Union[bool, str] = False,\n    ):\n        """"""\n        Least squares polynomial fit.\n\n        This replicates the behaviour of `numpy.polyfit` but differs by skipping\n        invalid values when `skipna = True`.\n\n        Parameters\n        ----------\n        dim : hashable\n            Coordinate along which to fit the polynomials.\n        deg : int\n            Degree of the fitting polynomial.\n        skipna : bool, optional\n            If True, removes all invalid values before fitting each 1D slices of the array.\n            Default is True if data is stored in a dask.array or if there is any\n            invalid values, False otherwise.\n        rcond : float, optional\n            Relative condition number to the fit.\n        w : Union[Hashable, Any], optional\n            Weights to apply to the y-coordinate of the sample points.\n            Can be an array-like object or the name of a coordinate in the dataset.\n        full : bool, optional\n            Whether to return the residuals, matrix rank and singular values in addition\n            to the coefficients.\n        cov : Union[bool, str], optional\n            Whether to return to the covariance matrix in addition to the coefficients.\n            The matrix is not scaled if `cov=\'unscaled\'`.\n\n\n        Returns\n        -------\n        polyfit_results : Dataset\n            A single dataset which contains (for each ""var"" in the input dataset):\n\n            [var]_polyfit_coefficients\n                The coefficients of the best fit for each variable in this dataset.\n            [var]_polyfit_residuals\n                The residuals of the least-square computation for each variable (only included if `full=True`)\n            [dim]_matrix_rank\n                The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n            [dim]_singular_values\n                The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n            [var]_polyfit_covariance\n                The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)\n\n        See also\n        --------\n        numpy.polyfit\n        """"""\n        variables = {}\n        skipna_da = skipna\n\n        x = get_clean_interp_index(self, dim, strict=False)\n        xname = ""{}_"".format(self[dim].name)\n        order = int(deg) + 1\n        lhs = np.vander(x, order)\n\n        if rcond is None:\n            rcond = x.shape[0] * np.core.finfo(x.dtype).eps\n\n        # Weights:\n        if w is not None:\n            if isinstance(w, Hashable):\n                w = self.coords[w]\n            w = np.asarray(w)\n            if w.ndim != 1:\n                raise TypeError(""Expected a 1-d array for weights."")\n            if w.shape[0] != lhs.shape[0]:\n                raise TypeError(""Expected w and {} to have the same length"".format(dim))\n            lhs *= w[:, np.newaxis]\n\n        # Scaling\n        scale = np.sqrt((lhs * lhs).sum(axis=0))\n        lhs /= scale\n\n        degree_dim = utils.get_temp_dimname(self.dims, ""degree"")\n\n        rank = np.linalg.matrix_rank(lhs)\n        if rank != order and not full:\n            warnings.warn(\n                ""Polyfit may be poorly conditioned"", np.RankWarning, stacklevel=4\n            )\n\n        if full:\n            rank = xr.DataArray(rank, name=xname + ""matrix_rank"")\n            variables[rank.name] = rank\n            sing = np.linalg.svd(lhs, compute_uv=False)\n            sing = xr.DataArray(\n                sing,\n                dims=(degree_dim,),\n                coords={degree_dim: np.arange(order)[::-1]},\n                name=xname + ""singular_values"",\n            )\n            variables[sing.name] = sing\n\n        for name, da in self.data_vars.items():\n            if dim not in da.dims:\n                continue\n\n            if skipna is None:\n                if isinstance(da.data, dask_array_type):\n                    skipna_da = True\n                else:\n                    skipna_da = np.any(da.isnull())\n\n            dims_to_stack = [dimname for dimname in da.dims if dimname != dim]\n            stacked_coords = {}\n            if dims_to_stack:\n                stacked_dim = utils.get_temp_dimname(dims_to_stack, ""stacked"")\n                rhs = da.transpose(dim, *dims_to_stack).stack(\n                    {stacked_dim: dims_to_stack}\n                )\n                stacked_coords = {stacked_dim: rhs[stacked_dim]}\n                scale_da = scale[:, np.newaxis]\n            else:\n                rhs = da\n                scale_da = scale\n\n            if w is not None:\n                rhs *= w[:, np.newaxis]\n\n            coeffs, residuals = duck_array_ops.least_squares(\n                lhs, rhs.data, rcond=rcond, skipna=skipna_da\n            )\n\n            if isinstance(name, str):\n                name = ""{}_"".format(name)\n            else:\n                # Thus a ReprObject => polyfit was called on a DataArray\n                name = """"\n\n            coeffs = xr.DataArray(\n                coeffs / scale_da,\n                dims=[degree_dim] + list(stacked_coords.keys()),\n                coords={degree_dim: np.arange(order)[::-1], **stacked_coords},\n                name=name + ""polyfit_coefficients"",\n            )\n            if dims_to_stack:\n                coeffs = coeffs.unstack(stacked_dim)\n            variables[coeffs.name] = coeffs\n\n            if full or (cov is True):\n                residuals = xr.DataArray(\n                    residuals if dims_to_stack else residuals.squeeze(),\n                    dims=list(stacked_coords.keys()),\n                    coords=stacked_coords,\n                    name=name + ""polyfit_residuals"",\n                )\n                if dims_to_stack:\n                    residuals = residuals.unstack(stacked_dim)\n                variables[residuals.name] = residuals\n\n            if cov:\n                Vbase = np.linalg.inv(np.dot(lhs.T, lhs))\n                Vbase /= np.outer(scale, scale)\n                if cov == ""unscaled"":\n                    fac = 1\n                else:\n                    if x.shape[0] <= order:\n                        raise ValueError(\n                            ""The number of data points must exceed order to scale the covariance matrix.""\n                        )\n                    fac = residuals / (x.shape[0] - order)\n                covariance = xr.DataArray(Vbase, dims=(""cov_i"", ""cov_j"")) * fac\n                variables[name + ""polyfit_covariance""] = covariance\n\n        return Dataset(data_vars=variables, attrs=self.attrs.copy())\n\n    def pad(\n        self,\n        pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,\n        mode: str = ""constant"",\n        stat_length: Union[\n            int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n        ] = None,\n        constant_values: Union[\n            int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n        ] = None,\n        end_values: Union[\n            int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n        ] = None,\n        reflect_type: str = None,\n        **pad_width_kwargs: Any,\n    ) -> ""Dataset"":\n        """"""Pad this dataset along one or more dimensions.\n\n        .. warning::\n            This function is experimental and its behaviour is likely to change\n            especially regarding padding of dimension coordinates (or IndexVariables).\n\n        When using one of the modes (""edge"", ""reflect"", ""symmetric"", ""wrap""),\n        coordinates will be padded with the same mode, otherwise coordinates\n        are padded using the ""constant"" mode with fill_value dtypes.NA.\n\n        Parameters\n        ----------\n        pad_width : Mapping with the form of {dim: (pad_before, pad_after)}\n            Number of values padded along each dimension.\n            {dim: pad} is a shortcut for pad_before = pad_after = pad\n        mode : str\n            One of the following string values (taken from numpy docs).\n\n            \'constant\' (default)\n                Pads with a constant value.\n            \'edge\'\n                Pads with the edge values of array.\n            \'linear_ramp\'\n                Pads with the linear ramp between end_value and the\n                array edge value.\n            \'maximum\'\n                Pads with the maximum value of all or part of the\n                vector along each axis.\n            \'mean\'\n                Pads with the mean value of all or part of the\n                vector along each axis.\n            \'median\'\n                Pads with the median value of all or part of the\n                vector along each axis.\n            \'minimum\'\n                Pads with the minimum value of all or part of the\n                vector along each axis.\n            \'reflect\'\n                Pads with the reflection of the vector mirrored on\n                the first and last values of the vector along each\n                axis.\n            \'symmetric\'\n                Pads with the reflection of the vector mirrored\n                along the edge of the array.\n            \'wrap\'\n                Pads with the wrap of the vector along the axis.\n                The first values are used to pad the end and the\n                end values are used to pad the beginning.\n        stat_length : int, tuple or mapping of the form {dim: tuple}\n            Used in \'maximum\', \'mean\', \'median\', and \'minimum\'.  Number of\n            values at edge of each axis used to calculate the statistic value.\n            {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique\n            statistic lengths along each dimension.\n            ((before, after),) yields same before and after statistic lengths\n            for each dimension.\n            (stat_length,) or int is a shortcut for before = after = statistic\n            length for all axes.\n            Default is ``None``, to use the entire axis.\n        constant_values : scalar, tuple or mapping of the form {dim: tuple}\n            Used in \'constant\'.  The values to set the padded values for each\n            axis.\n            ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n            pad constants along each dimension.\n            ``((before, after),)`` yields same before and after constants for each\n            dimension.\n            ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n            all dimensions.\n            Default is 0.\n        end_values : scalar, tuple or mapping of the form {dim: tuple}\n            Used in \'linear_ramp\'.  The values used for the ending value of the\n            linear_ramp and that will form the edge of the padded array.\n            ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n            end values along each dimension.\n            ``((before, after),)`` yields same before and after end values for each\n            axis.\n            ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n            all axes.\n            Default is 0.\n        reflect_type : {\'even\', \'odd\'}, optional\n            Used in \'reflect\', and \'symmetric\'.  The \'even\' style is the\n            default with an unaltered reflection around the edge value.  For\n            the \'odd\' style, the extended part of the array is created by\n            subtracting the reflected values from two times the edge value.\n        **pad_width_kwargs:\n            The keyword arguments form of ``pad_width``.\n            One of ``pad_width`` or ``pad_width_kwargs`` must be provided.\n\n        Returns\n        -------\n        padded : Dataset\n            Dataset with the padded coordinates and data.\n\n        See also\n        --------\n        Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad\n\n        Notes\n        -----\n        By default when ``mode=""constant""`` and ``constant_values=None``, integer types will be\n        promoted to ``float`` and padded with ``np.nan``. To avoid type promotion\n        specify ``constant_values=np.nan``\n\n        Examples\n        --------\n\n        >>> ds = xr.Dataset({""foo"": (""x"", range(5))})\n        >>> ds.pad(x=(1, 2))\n        <xarray.Dataset>\n        Dimensions:  (x: 8)\n        Dimensions without coordinates: x\n        Data variables:\n            foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan\n        """"""\n        pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, ""pad"")\n\n        if mode in (""edge"", ""reflect"", ""symmetric"", ""wrap""):\n            coord_pad_mode = mode\n            coord_pad_options = {\n                ""stat_length"": stat_length,\n                ""constant_values"": constant_values,\n                ""end_values"": end_values,\n                ""reflect_type"": reflect_type,\n            }\n        else:\n            coord_pad_mode = ""constant""\n            coord_pad_options = {}\n\n        variables = {}\n        for name, var in self.variables.items():\n            var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}\n            if not var_pad_width:\n                variables[name] = var\n            elif name in self.data_vars:\n                variables[name] = var.pad(\n                    pad_width=var_pad_width,\n                    mode=mode,\n                    stat_length=stat_length,\n                    constant_values=constant_values,\n                    end_values=end_values,\n                    reflect_type=reflect_type,\n                )\n            else:\n                variables[name] = var.pad(\n                    pad_width=var_pad_width,\n                    mode=coord_pad_mode,\n                    **coord_pad_options,  # type: ignore\n                )\n\n        return self._replace_vars_and_dims(variables)\n\n    def idxmin(\n        self,\n        dim: Hashable = None,\n        skipna: bool = None,\n        fill_value: Any = dtypes.NA,\n        keep_attrs: bool = None,\n    ) -> ""Dataset"":\n        """"""Return the coordinate label of the minimum value along a dimension.\n\n        Returns a new `Dataset` named after the dimension with the values of\n        the coordinate labels along that dimension corresponding to minimum\n        values along that dimension.\n\n        In comparison to :py:meth:`~Dataset.argmin`, this returns the\n        coordinate label while :py:meth:`~Dataset.argmin` returns the index.\n\n        Parameters\n        ----------\n        dim : str, optional\n            Dimension over which to apply `idxmin`.  This is optional for 1D\n            variables, but required for variables with 2 or more dimensions.\n        skipna : bool or None, default None\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for ``float``, ``complex``, and ``object``\n            dtypes; other dtypes either do not have a sentinel missing value\n            (``int``) or ``skipna=True`` has not been implemented\n            (``datetime64`` or ``timedelta64``).\n        fill_value : Any, default NaN\n            Value to be filled in case all of the values along a dimension are\n            null.  By default this is NaN.  The fill value and result are\n            automatically converted to a compatible dtype if possible.\n            Ignored if ``skipna`` is False.\n        keep_attrs : bool, default False\n            If True, the attributes (``attrs``) will be copied from the\n            original object to the new one.  If False (default), the new object\n            will be returned without attributes.\n\n        Returns\n        -------\n        reduced : Dataset\n            New `Dataset` object with `idxmin` applied to its data and the\n            indicated dimension removed.\n\n        See also\n        --------\n        DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin\n\n        Examples\n        --------\n\n        >>> array1 = xr.DataArray(\n        ...     [0, 2, 1, 0, -2], dims=""x"", coords={""x"": [""a"", ""b"", ""c"", ""d"", ""e""]}\n        ... )\n        >>> array2 = xr.DataArray(\n        ...     [\n        ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n        ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n        ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n        ...     ],\n        ...     dims=[""y"", ""x""],\n        ...     coords={""y"": [-1, 0, 1], ""x"": [""a"", ""b"", ""c"", ""d"", ""e""]},\n        ... )\n        >>> ds = xr.Dataset({""int"": array1, ""float"": array2})\n        >>> ds.min(dim=""x"")\n        <xarray.Dataset>\n        Dimensions:  (y: 3)\n        Coordinates:\n          * y        (y) int64 -1 0 1\n        Data variables:\n            int      int64 -2\n            float    (y) float64 -2.0 -4.0 1.0\n        >>> ds.argmin(dim=""x"")\n        <xarray.Dataset>\n        Dimensions:  (y: 3)\n        Coordinates:\n          * y        (y) int64 -1 0 1\n        Data variables:\n            int      int64 4\n            float    (y) int64 4 0 2\n        >>> ds.idxmin(dim=""x"")\n        <xarray.Dataset>\n        Dimensions:  (y: 3)\n        Coordinates:\n          * y        (y) int64 -1 0 1\n        Data variables:\n            int      <U1 \'e\'\n            float    (y) <U1 \'e\' \'a\' \'c\'\n        """"""\n        return self.map(\n            methodcaller(\n                ""idxmin"",\n                dim=dim,\n                skipna=skipna,\n                fill_value=fill_value,\n                keep_attrs=keep_attrs,\n            )\n        )\n\n    def idxmax(\n        self,\n        dim: Hashable = None,\n        skipna: bool = None,\n        fill_value: Any = dtypes.NA,\n        keep_attrs: bool = None,\n    ) -> ""Dataset"":\n        """"""Return the coordinate label of the maximum value along a dimension.\n\n        Returns a new `Dataset` named after the dimension with the values of\n        the coordinate labels along that dimension corresponding to maximum\n        values along that dimension.\n\n        In comparison to :py:meth:`~Dataset.argmax`, this returns the\n        coordinate label while :py:meth:`~Dataset.argmax` returns the index.\n\n        Parameters\n        ----------\n        dim : str, optional\n            Dimension over which to apply `idxmax`.  This is optional for 1D\n            variables, but required for variables with 2 or more dimensions.\n        skipna : bool or None, default None\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for ``float``, ``complex``, and ``object``\n            dtypes; other dtypes either do not have a sentinel missing value\n            (``int``) or ``skipna=True`` has not been implemented\n            (``datetime64`` or ``timedelta64``).\n        fill_value : Any, default NaN\n            Value to be filled in case all of the values along a dimension are\n            null.  By default this is NaN.  The fill value and result are\n            automatically converted to a compatible dtype if possible.\n            Ignored if ``skipna`` is False.\n        keep_attrs : bool, default False\n            If True, the attributes (``attrs``) will be copied from the\n            original object to the new one.  If False (default), the new object\n            will be returned without attributes.\n\n        Returns\n        -------\n        reduced : Dataset\n            New `Dataset` object with `idxmax` applied to its data and the\n            indicated dimension removed.\n\n        See also\n        --------\n        DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax\n\n        Examples\n        --------\n\n        >>> array1 = xr.DataArray(\n        ...     [0, 2, 1, 0, -2], dims=""x"", coords={""x"": [""a"", ""b"", ""c"", ""d"", ""e""]}\n        ... )\n        >>> array2 = xr.DataArray(\n        ...     [\n        ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n        ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n        ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n        ...     ],\n        ...     dims=[""y"", ""x""],\n        ...     coords={""y"": [-1, 0, 1], ""x"": [""a"", ""b"", ""c"", ""d"", ""e""]},\n        ... )\n        >>> ds = xr.Dataset({""int"": array1, ""float"": array2})\n        >>> ds.max(dim=""x"")\n        <xarray.Dataset>\n        Dimensions:  (y: 3)\n        Coordinates:\n          * y        (y) int64 -1 0 1\n        Data variables:\n            int      int64 2\n            float    (y) float64 2.0 2.0 1.0\n        >>> ds.argmax(dim=""x"")\n        <xarray.Dataset>\n        Dimensions:  (y: 3)\n        Coordinates:\n          * y        (y) int64 -1 0 1\n        Data variables:\n            int      int64 1\n            float    (y) int64 0 2 2\n        >>> ds.idxmax(dim=""x"")\n        <xarray.Dataset>\n        Dimensions:  (y: 3)\n        Coordinates:\n          * y        (y) int64 -1 0 1\n        Data variables:\n            int      <U1 \'b\'\n            float    (y) object \'a\' \'c\' \'c\'\n        """"""\n        return self.map(\n            methodcaller(\n                ""idxmax"",\n                dim=dim,\n                skipna=skipna,\n                fill_value=fill_value,\n                keep_attrs=keep_attrs,\n            )\n        )\n\n\nops.inject_all_ops_and_reduce_methods(Dataset, array_only=False)\n'"
xarray/core/dtypes.py,38,"b'import functools\n\nimport numpy as np\n\nfrom . import utils\n\n# Use as a sentinel value to indicate a dtype appropriate NA value.\nNA = utils.ReprObject(""<NA>"")\n\n\n@functools.total_ordering\nclass AlwaysGreaterThan:\n    def __gt__(self, other):\n        return True\n\n    def __eq__(self, other):\n        return isinstance(other, type(self))\n\n\n@functools.total_ordering\nclass AlwaysLessThan:\n    def __lt__(self, other):\n        return True\n\n    def __eq__(self, other):\n        return isinstance(other, type(self))\n\n\n# Equivalence to np.inf (-np.inf) for object-type\nINF = AlwaysGreaterThan()\nNINF = AlwaysLessThan()\n\n\n# Pairs of types that, if both found, should be promoted to object dtype\n# instead of following NumPy\'s own type-promotion rules. These type promotion\n# rules match pandas instead. For reference, see the NumPy type hierarchy:\n# https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html\nPROMOTE_TO_OBJECT = [\n    {np.number, np.character},  # numpy promotes to character\n    {np.bool_, np.character},  # numpy promotes to character\n    {np.bytes_, np.unicode_},  # numpy promotes to unicode\n]\n\n\ndef maybe_promote(dtype):\n    """"""Simpler equivalent of pandas.core.common._maybe_promote\n\n    Parameters\n    ----------\n    dtype : np.dtype\n\n    Returns\n    -------\n    dtype : Promoted dtype that can hold missing values.\n    fill_value : Valid missing value for the promoted dtype.\n    """"""\n    # N.B. these casting rules should match pandas\n    if np.issubdtype(dtype, np.floating):\n        fill_value = np.nan\n    elif np.issubdtype(dtype, np.timedelta64):\n        # See https://github.com/numpy/numpy/issues/10685\n        # np.timedelta64 is a subclass of np.integer\n        # Check np.timedelta64 before np.integer\n        fill_value = np.timedelta64(""NaT"")\n    elif np.issubdtype(dtype, np.integer):\n        if dtype.itemsize <= 2:\n            dtype = np.float32\n        else:\n            dtype = np.float64\n        fill_value = np.nan\n    elif np.issubdtype(dtype, np.complexfloating):\n        fill_value = np.nan + np.nan * 1j\n    elif np.issubdtype(dtype, np.datetime64):\n        fill_value = np.datetime64(""NaT"")\n    else:\n        dtype = object\n        fill_value = np.nan\n    return np.dtype(dtype), fill_value\n\n\nNAT_TYPES = (np.datetime64(""NaT""), np.timedelta64(""NaT""))\n\n\ndef get_fill_value(dtype):\n    """"""Return an appropriate fill value for this dtype.\n\n    Parameters\n    ----------\n    dtype : np.dtype\n\n    Returns\n    -------\n    fill_value : Missing value corresponding to this dtype.\n    """"""\n    _, fill_value = maybe_promote(dtype)\n    return fill_value\n\n\ndef get_pos_infinity(dtype):\n    """"""Return an appropriate positive infinity for this dtype.\n\n    Parameters\n    ----------\n    dtype : np.dtype\n\n    Returns\n    -------\n    fill_value : positive infinity value corresponding to this dtype.\n    """"""\n    if issubclass(dtype.type, (np.floating, np.integer)):\n        return np.inf\n\n    if issubclass(dtype.type, np.complexfloating):\n        return np.inf + 1j * np.inf\n\n    return INF\n\n\ndef get_neg_infinity(dtype):\n    """"""Return an appropriate positive infinity for this dtype.\n\n    Parameters\n    ----------\n    dtype : np.dtype\n\n    Returns\n    -------\n    fill_value : positive infinity value corresponding to this dtype.\n    """"""\n    if issubclass(dtype.type, (np.floating, np.integer)):\n        return -np.inf\n\n    if issubclass(dtype.type, np.complexfloating):\n        return -np.inf - 1j * np.inf\n\n    return NINF\n\n\ndef is_datetime_like(dtype):\n    """"""Check if a dtype is a subclass of the numpy datetime types\n    """"""\n    return np.issubdtype(dtype, np.datetime64) or np.issubdtype(dtype, np.timedelta64)\n\n\ndef result_type(*arrays_and_dtypes):\n    """"""Like np.result_type, but with type promotion rules matching pandas.\n\n    Examples of changed behavior:\n    number + string -> object (not string)\n    bytes + unicode -> object (not unicode)\n\n    Parameters\n    ----------\n    *arrays_and_dtypes : list of arrays and dtypes\n        The dtype is extracted from both numpy and dask arrays.\n\n    Returns\n    -------\n    numpy.dtype for the result.\n    """"""\n    types = {np.result_type(t).type for t in arrays_and_dtypes}\n\n    for left, right in PROMOTE_TO_OBJECT:\n        if any(issubclass(t, left) for t in types) and any(\n            issubclass(t, right) for t in types\n        ):\n            return np.dtype(object)\n\n    return np.result_type(*arrays_and_dtypes)\n'"
xarray/core/duck_array_ops.py,29,"b'""""""Compatibility module defining operations on duck numpy-arrays.\n\nCurrently, this means Dask or NumPy arrays. None of these functions should\naccept or return xarray objects.\n""""""\nimport contextlib\nimport inspect\nimport warnings\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\n\nfrom . import dask_array_compat, dask_array_ops, dtypes, npcompat, nputils\nfrom .nputils import nanfirst, nanlast\nfrom .pycompat import dask_array_type\n\ntry:\n    import dask.array as dask_array\nexcept ImportError:\n    dask_array = None  # type: ignore\n\n\ndef _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=dask_array,\n    list_of_args=False,\n    array_args=slice(1),\n    requires_dask=None,\n):\n    """"""Create a function that dispatches to dask for dask array inputs.""""""\n    if dask_module is not None:\n\n        def f(*args, **kwargs):\n            if list_of_args:\n                dispatch_args = args[0]\n            else:\n                dispatch_args = args[array_args]\n            if any(isinstance(a, dask_array_type) for a in dispatch_args):\n                try:\n                    wrapped = getattr(dask_module, name)\n                except AttributeError as e:\n                    raise AttributeError(f""{e}: requires dask >={requires_dask}"")\n            else:\n                wrapped = getattr(eager_module, name)\n            return wrapped(*args, **kwargs)\n\n    else:\n\n        def f(*args, **kwargs):\n            return getattr(eager_module, name)(*args, **kwargs)\n\n    return f\n\n\ndef fail_on_dask_array_input(values, msg=None, func_name=None):\n    if isinstance(values, dask_array_type):\n        if msg is None:\n            msg = ""%r is not yet a valid method on dask arrays""\n        if func_name is None:\n            func_name = inspect.stack()[1][3]\n        raise NotImplementedError(msg % func_name)\n\n\n# switch to use dask.array / __array_function__ version when dask supports it:\n# https://github.com/dask/dask/pull/4822\nmoveaxis = npcompat.moveaxis\n\naround = _dask_or_eager_func(""around"")\nisclose = _dask_or_eager_func(""isclose"")\n\n\nisnat = np.isnat\nisnan = _dask_or_eager_func(""isnan"")\nzeros_like = _dask_or_eager_func(""zeros_like"")\n\n\npandas_isnull = _dask_or_eager_func(""isnull"", eager_module=pd)\n\n\ndef isnull(data):\n    data = asarray(data)\n    scalar_type = data.dtype.type\n    if issubclass(scalar_type, (np.datetime64, np.timedelta64)):\n        # datetime types use NaT for null\n        # note: must check timedelta64 before integers, because currently\n        # timedelta64 inherits from np.integer\n        return isnat(data)\n    elif issubclass(scalar_type, np.inexact):\n        # float types use NaN for null\n        return isnan(data)\n    elif issubclass(scalar_type, (np.bool_, np.integer, np.character, np.void)):\n        # these types cannot represent missing values\n        return zeros_like(data, dtype=bool)\n    else:\n        # at this point, array should have dtype=object\n        if isinstance(data, (np.ndarray, dask_array_type)):\n            return pandas_isnull(data)\n        else:\n            # Not reachable yet, but intended for use with other duck array\n            # types. For full consistency with pandas, we should accept None as\n            # a null value as well as NaN, but it isn\'t clear how to do this\n            # with duck typing.\n            return data != data\n\n\ndef notnull(data):\n    return ~isnull(data)\n\n\ntranspose = _dask_or_eager_func(""transpose"")\n_where = _dask_or_eager_func(""where"", array_args=slice(3))\nisin = _dask_or_eager_func(""isin"", array_args=slice(2))\ntake = _dask_or_eager_func(""take"")\nbroadcast_to = _dask_or_eager_func(""broadcast_to"")\npad = _dask_or_eager_func(""pad"", dask_module=dask_array_compat)\n\n_concatenate = _dask_or_eager_func(""concatenate"", list_of_args=True)\n_stack = _dask_or_eager_func(""stack"", list_of_args=True)\n\narray_all = _dask_or_eager_func(""all"")\narray_any = _dask_or_eager_func(""any"")\n\ntensordot = _dask_or_eager_func(""tensordot"", array_args=slice(2))\neinsum = _dask_or_eager_func(""einsum"", array_args=slice(1, None))\n\n\ndef gradient(x, coord, axis, edge_order):\n    if isinstance(x, dask_array_type):\n        return dask_array.gradient(x, coord, axis=axis, edge_order=edge_order)\n    return np.gradient(x, coord, axis=axis, edge_order=edge_order)\n\n\ndef trapz(y, x, axis):\n    if axis < 0:\n        axis = y.ndim + axis\n    x_sl1 = (slice(1, None),) + (None,) * (y.ndim - axis - 1)\n    x_sl2 = (slice(None, -1),) + (None,) * (y.ndim - axis - 1)\n    slice1 = (slice(None),) * axis + (slice(1, None),)\n    slice2 = (slice(None),) * axis + (slice(None, -1),)\n    dx = x[x_sl1] - x[x_sl2]\n    integrand = dx * 0.5 * (y[tuple(slice1)] + y[tuple(slice2)])\n    return sum(integrand, axis=axis, skipna=False)\n\n\nmasked_invalid = _dask_or_eager_func(\n    ""masked_invalid"", eager_module=np.ma, dask_module=getattr(dask_array, ""ma"", None)\n)\n\n\ndef asarray(data):\n    return (\n        data\n        if (isinstance(data, dask_array_type) or hasattr(data, ""__array_function__""))\n        else np.asarray(data)\n    )\n\n\ndef as_shared_dtype(scalars_or_arrays):\n    """"""Cast a arrays to a shared dtype using xarray\'s type promotion rules.""""""\n    arrays = [asarray(x) for x in scalars_or_arrays]\n    # Pass arrays directly instead of dtypes to result_type so scalars\n    # get handled properly.\n    # Note that result_type() safely gets the dtype from dask arrays without\n    # evaluating them.\n    out_type = dtypes.result_type(*arrays)\n    return [x.astype(out_type, copy=False) for x in arrays]\n\n\ndef lazy_array_equiv(arr1, arr2):\n    """"""Like array_equal, but doesn\'t actually compare values.\n       Returns True when arr1, arr2 identical or their dask names are equal.\n       Returns False when shapes are not equal.\n       Returns None when equality cannot determined: one or both of arr1, arr2 are numpy arrays;\n       or their dask names are not equal\n    """"""\n    if arr1 is arr2:\n        return True\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    if (\n        dask_array\n        and isinstance(arr1, dask_array_type)\n        and isinstance(arr2, dask_array_type)\n    ):\n        # GH3068\n        if arr1.name == arr2.name:\n            return True\n        else:\n            return None\n    return None\n\n\ndef allclose_or_equiv(arr1, arr2, rtol=1e-5, atol=1e-8):\n    """"""Like np.allclose, but also allows values to be NaN in both arrays\n    """"""\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n    lazy_equiv = lazy_array_equiv(arr1, arr2)\n    if lazy_equiv is None:\n        return bool(isclose(arr1, arr2, rtol=rtol, atol=atol, equal_nan=True).all())\n    else:\n        return lazy_equiv\n\n\ndef array_equiv(arr1, arr2):\n    """"""Like np.array_equal, but also allows values to be NaN in both arrays\n    """"""\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n    lazy_equiv = lazy_array_equiv(arr1, arr2)\n    if lazy_equiv is None:\n        with warnings.catch_warnings():\n            warnings.filterwarnings(""ignore"", ""In the future, \'NAT == x\'"")\n            flag_array = (arr1 == arr2) | (isnull(arr1) & isnull(arr2))\n            return bool(flag_array.all())\n    else:\n        return lazy_equiv\n\n\ndef array_notnull_equiv(arr1, arr2):\n    """"""Like np.array_equal, but also allows values to be NaN in either or both\n    arrays\n    """"""\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n    lazy_equiv = lazy_array_equiv(arr1, arr2)\n    if lazy_equiv is None:\n        with warnings.catch_warnings():\n            warnings.filterwarnings(""ignore"", ""In the future, \'NAT == x\'"")\n            flag_array = (arr1 == arr2) | isnull(arr1) | isnull(arr2)\n            return bool(flag_array.all())\n    else:\n        return lazy_equiv\n\n\ndef count(data, axis=None):\n    """"""Count the number of non-NA in this array along the given axis or axes\n    """"""\n    return np.sum(np.logical_not(isnull(data)), axis=axis)\n\n\ndef where(condition, x, y):\n    """"""Three argument where() with better dtype promotion rules.""""""\n    return _where(condition, *as_shared_dtype([x, y]))\n\n\ndef where_method(data, cond, other=dtypes.NA):\n    if other is dtypes.NA:\n        other = dtypes.get_fill_value(data.dtype)\n    return where(cond, data, other)\n\n\ndef fillna(data, other):\n    # we need to pass data first so pint has a chance of returning the\n    # correct unit\n    # TODO: revert after https://github.com/hgrecco/pint/issues/1019 is fixed\n    return where(notnull(data), data, other)\n\n\ndef concatenate(arrays, axis=0):\n    """"""concatenate() with better dtype promotion rules.""""""\n    return _concatenate(as_shared_dtype(arrays), axis=axis)\n\n\ndef stack(arrays, axis=0):\n    """"""stack() with better dtype promotion rules.""""""\n    return _stack(as_shared_dtype(arrays), axis=axis)\n\n\n@contextlib.contextmanager\ndef _ignore_warnings_if(condition):\n    if condition:\n        with warnings.catch_warnings():\n            warnings.simplefilter(""ignore"")\n            yield\n    else:\n        yield\n\n\ndef _create_nan_agg_method(name, dask_module=dask_array, coerce_strings=False):\n    from . import nanops\n\n    def f(values, axis=None, skipna=None, **kwargs):\n        if kwargs.pop(""out"", None) is not None:\n            raise TypeError(f""`out` is not valid for {name}"")\n\n        values = asarray(values)\n\n        if coerce_strings and values.dtype.kind in ""SU"":\n            values = values.astype(object)\n\n        func = None\n        if skipna or (skipna is None and values.dtype.kind in ""cfO""):\n            nanname = ""nan"" + name\n            func = getattr(nanops, nanname)\n        else:\n            func = _dask_or_eager_func(name, dask_module=dask_module)\n\n        try:\n            return func(values, axis=axis, **kwargs)\n        except AttributeError:\n            if not isinstance(values, dask_array_type):\n                raise\n            try:  # dask/dask#3133 dask sometimes needs dtype argument\n                # if func does not accept dtype, then raises TypeError\n                return func(values, axis=axis, dtype=values.dtype, **kwargs)\n            except (AttributeError, TypeError):\n                raise NotImplementedError(\n                    f""{name} is not yet implemented on dask arrays""\n                )\n\n    f.__name__ = name\n    return f\n\n\n# Attributes `numeric_only`, `available_min_count` is used for docs.\n# See ops.inject_reduce_methods\nargmax = _create_nan_agg_method(""argmax"", coerce_strings=True)\nargmin = _create_nan_agg_method(""argmin"", coerce_strings=True)\nmax = _create_nan_agg_method(""max"", coerce_strings=True)\nmin = _create_nan_agg_method(""min"", coerce_strings=True)\nsum = _create_nan_agg_method(""sum"")\nsum.numeric_only = True\nsum.available_min_count = True\nstd = _create_nan_agg_method(""std"")\nstd.numeric_only = True\nvar = _create_nan_agg_method(""var"")\nvar.numeric_only = True\nmedian = _create_nan_agg_method(""median"", dask_module=dask_array_compat)\nmedian.numeric_only = True\nprod = _create_nan_agg_method(""prod"")\nprod.numeric_only = True\nsum.available_min_count = True\ncumprod_1d = _create_nan_agg_method(""cumprod"")\ncumprod_1d.numeric_only = True\ncumsum_1d = _create_nan_agg_method(""cumsum"")\ncumsum_1d.numeric_only = True\n\n\n_mean = _create_nan_agg_method(""mean"")\n\n\ndef _datetime_nanmin(array):\n    """"""nanmin() function for datetime64.\n\n    Caveats that this function deals with:\n\n    - In numpy < 1.18, min() on datetime64 incorrectly ignores NaT\n    - numpy nanmin() don\'t work on datetime64 (all versions at the moment of writing)\n    - dask min() does not work on datetime64 (all versions at the moment of writing)\n    """"""\n    assert array.dtype.kind in ""mM""\n    dtype = array.dtype\n    # (NaT).astype(float) does not produce NaN...\n    array = where(pandas_isnull(array), np.nan, array.astype(float))\n    array = min(array, skipna=True)\n    if isinstance(array, float):\n        array = np.array(array)\n    # ...but (NaN).astype(""M8"") does produce NaT\n    return array.astype(dtype)\n\n\ndef datetime_to_numeric(array, offset=None, datetime_unit=None, dtype=float):\n    """"""Convert an array containing datetime-like data to numerical values.\n\n    Convert the datetime array to a timedelta relative to an offset.\n\n    Parameters\n    ----------\n    da : array-like\n      Input data\n    offset: None, datetime or cftime.datetime\n      Datetime offset. If None, this is set by default to the array\'s minimum\n      value to reduce round off errors.\n    datetime_unit: {None, Y, M, W, D, h, m, s, ms, us, ns, ps, fs, as}\n      If not None, convert output to a given datetime unit. Note that some\n      conversions are not allowed due to non-linear relationships between units.\n    dtype: dtype\n      Output dtype.\n\n    Returns\n    -------\n    array\n      Numerical representation of datetime object relative to an offset.\n\n    Notes\n    -----\n    Some datetime unit conversions won\'t work, for example from days to years, even\n    though some calendars would allow for them (e.g. no_leap). This is because there\n    is no `cftime.timedelta` object.\n    """"""\n    # TODO: make this function dask-compatible?\n    # Set offset to minimum if not given\n    if offset is None:\n        if array.dtype.kind in ""Mm"":\n            offset = _datetime_nanmin(array)\n        else:\n            offset = min(array)\n\n    # Compute timedelta object.\n    # For np.datetime64, this can silently yield garbage due to overflow.\n    # One option is to enforce 1970-01-01 as the universal offset.\n    array = array - offset\n\n    # Scalar is converted to 0d-array\n    if not hasattr(array, ""dtype""):\n        array = np.array(array)\n\n    # Convert timedelta objects to float by first converting to microseconds.\n    if array.dtype.kind in ""O"":\n        return py_timedelta_to_float(array, datetime_unit or ""ns"").astype(dtype)\n\n    # Convert np.NaT to np.nan\n    elif array.dtype.kind in ""mM"":\n\n        # Convert to specified timedelta units.\n        if datetime_unit:\n            array = array / np.timedelta64(1, datetime_unit)\n        return np.where(isnull(array), np.nan, array.astype(dtype))\n\n\ndef timedelta_to_numeric(value, datetime_unit=""ns"", dtype=float):\n    """"""Convert a timedelta-like object to numerical values.\n\n    Parameters\n    ----------\n    value : datetime.timedelta, numpy.timedelta64, pandas.Timedelta, str\n      Time delta representation.\n    datetime_unit : {Y, M, W, D, h, m, s, ms, us, ns, ps, fs, as}\n      The time units of the output values. Note that some conversions are not allowed due to\n      non-linear relationships between units.\n    dtype : type\n      The output data type.\n\n    """"""\n    import datetime as dt\n\n    if isinstance(value, dt.timedelta):\n        out = py_timedelta_to_float(value, datetime_unit)\n    elif isinstance(value, np.timedelta64):\n        out = np_timedelta64_to_float(value, datetime_unit)\n    elif isinstance(value, pd.Timedelta):\n        out = pd_timedelta_to_float(value, datetime_unit)\n    elif isinstance(value, str):\n        try:\n            a = pd.to_timedelta(value)\n        except ValueError:\n            raise ValueError(\n                f""Could not convert {value!r} to timedelta64 using pandas.to_timedelta""\n            )\n        return py_timedelta_to_float(a, datetime_unit)\n    else:\n        raise TypeError(\n            f""Expected value of type str, pandas.Timedelta, datetime.timedelta ""\n            f""or numpy.timedelta64, but received {type(value).__name__}""\n        )\n    return out.astype(dtype)\n\n\ndef _to_pytimedelta(array, unit=""us""):\n    index = pd.TimedeltaIndex(array.ravel(), unit=unit)\n    return index.to_pytimedelta().reshape(array.shape)\n\n\ndef np_timedelta64_to_float(array, datetime_unit):\n    """"""Convert numpy.timedelta64 to float.\n\n    Notes\n    -----\n    The array is first converted to microseconds, which is less likely to\n    cause overflow errors.\n    """"""\n    array = array.astype(""timedelta64[ns]"").astype(np.float64)\n    conversion_factor = np.timedelta64(1, ""ns"") / np.timedelta64(1, datetime_unit)\n    return conversion_factor * array\n\n\ndef pd_timedelta_to_float(value, datetime_unit):\n    """"""Convert pandas.Timedelta to float.\n\n    Notes\n    -----\n    Built on the assumption that pandas timedelta values are in nanoseconds,\n    which is also the numpy default resolution.\n    """"""\n    value = value.to_timedelta64()\n    return np_timedelta64_to_float(value, datetime_unit)\n\n\ndef py_timedelta_to_float(array, datetime_unit):\n    """"""Convert a timedelta object to a float, possibly at a loss of resolution.\n    """"""\n    array = np.asarray(array)\n    array = np.reshape([a.total_seconds() for a in array.ravel()], array.shape) * 1e6\n    conversion_factor = np.timedelta64(1, ""us"") / np.timedelta64(1, datetime_unit)\n    return conversion_factor * array\n\n\ndef mean(array, axis=None, skipna=None, **kwargs):\n    """"""inhouse mean that can handle np.datetime64 or cftime.datetime\n    dtypes""""""\n    from .common import _contains_cftime_datetimes\n\n    array = asarray(array)\n    if array.dtype.kind in ""Mm"":\n        offset = _datetime_nanmin(array)\n\n        # xarray always uses np.datetime64[ns] for np.datetime64 data\n        dtype = ""timedelta64[ns]""\n        return (\n            _mean(\n                datetime_to_numeric(array, offset), axis=axis, skipna=skipna, **kwargs\n            ).astype(dtype)\n            + offset\n        )\n    elif _contains_cftime_datetimes(array):\n        if isinstance(array, dask_array_type):\n            raise NotImplementedError(\n                ""Computing the mean of an array containing ""\n                ""cftime.datetime objects is not yet implemented on ""\n                ""dask arrays.""\n            )\n        offset = min(array)\n        timedeltas = datetime_to_numeric(array, offset, datetime_unit=""us"")\n        mean_timedeltas = _mean(timedeltas, axis=axis, skipna=skipna, **kwargs)\n        return _to_pytimedelta(mean_timedeltas, unit=""us"") + offset\n    else:\n        return _mean(array, axis=axis, skipna=skipna, **kwargs)\n\n\nmean.numeric_only = True  # type: ignore\n\n\ndef _nd_cum_func(cum_func, array, axis, **kwargs):\n    array = asarray(array)\n    if axis is None:\n        axis = tuple(range(array.ndim))\n    if isinstance(axis, int):\n        axis = (axis,)\n\n    out = array\n    for ax in axis:\n        out = cum_func(out, axis=ax, **kwargs)\n    return out\n\n\ndef cumprod(array, axis=None, **kwargs):\n    """"""N-dimensional version of cumprod.""""""\n    return _nd_cum_func(cumprod_1d, array, axis, **kwargs)\n\n\ndef cumsum(array, axis=None, **kwargs):\n    """"""N-dimensional version of cumsum.""""""\n    return _nd_cum_func(cumsum_1d, array, axis, **kwargs)\n\n\n_fail_on_dask_array_input_skipna = partial(\n    fail_on_dask_array_input,\n    msg=""%r with skipna=True is not yet implemented on dask arrays"",\n)\n\n\ndef first(values, axis, skipna=None):\n    """"""Return the first non-NA elements in this array along the given axis\n    """"""\n    if (skipna or skipna is None) and values.dtype.kind not in ""iSU"":\n        # only bother for dtypes that can hold NaN\n        _fail_on_dask_array_input_skipna(values)\n        return nanfirst(values, axis)\n    return take(values, 0, axis=axis)\n\n\ndef last(values, axis, skipna=None):\n    """"""Return the last non-NA elements in this array along the given axis\n    """"""\n    if (skipna or skipna is None) and values.dtype.kind not in ""iSU"":\n        # only bother for dtypes that can hold NaN\n        _fail_on_dask_array_input_skipna(values)\n        return nanlast(values, axis)\n    return take(values, -1, axis=axis)\n\n\ndef rolling_window(array, axis, window, center, fill_value):\n    """"""\n    Make an ndarray with a rolling window of axis-th dimension.\n    The rolling dimension will be placed at the last dimension.\n    """"""\n    if isinstance(array, dask_array_type):\n        return dask_array_ops.rolling_window(array, axis, window, center, fill_value)\n    else:  # np.ndarray\n        return nputils.rolling_window(array, axis, window, center, fill_value)\n\n\ndef least_squares(lhs, rhs, rcond=None, skipna=False):\n    """"""Return the coefficients and residuals of a least-squares fit.\n    """"""\n    if isinstance(rhs, dask_array_type):\n        return dask_array_ops.least_squares(lhs, rhs, rcond=rcond, skipna=skipna)\n    else:\n        return nputils.least_squares(lhs, rhs, rcond=rcond, skipna=skipna)\n'"
xarray/core/extensions.py,1,"b'import warnings\n\nfrom .dataarray import DataArray\nfrom .dataset import Dataset\n\n\nclass AccessorRegistrationWarning(Warning):\n    """"""Warning for conflicts in accessor registration.""""""\n\n\nclass _CachedAccessor:\n    """"""Custom property-like object (descriptor) for caching accessors.""""""\n\n    def __init__(self, name, accessor):\n        self._name = name\n        self._accessor = accessor\n\n    def __get__(self, obj, cls):\n        if obj is None:\n            # we\'re accessing the attribute of the class, i.e., Dataset.geo\n            return self._accessor\n\n        # Use the same dict as @pandas.util.cache_readonly.\n        # It must be explicitly declared in obj.__slots__.\n        try:\n            cache = obj._cache\n        except AttributeError:\n            cache = obj._cache = {}\n\n        try:\n            return cache[self._name]\n        except KeyError:\n            pass\n\n        try:\n            accessor_obj = self._accessor(obj)\n        except AttributeError:\n            # __getattr__ on data object will swallow any AttributeErrors\n            # raised when initializing the accessor, so we need to raise as\n            # something else (GH933):\n            raise RuntimeError(""error initializing %r accessor."" % self._name)\n\n        cache[self._name] = accessor_obj\n        return accessor_obj\n\n\ndef _register_accessor(name, cls):\n    def decorator(accessor):\n        if hasattr(cls, name):\n            warnings.warn(\n                ""registration of accessor %r under name %r for type %r is ""\n                ""overriding a preexisting attribute with the same name.""\n                % (accessor, name, cls),\n                AccessorRegistrationWarning,\n                stacklevel=2,\n            )\n        setattr(cls, name, _CachedAccessor(name, accessor))\n        return accessor\n\n    return decorator\n\n\ndef register_dataarray_accessor(name):\n    """"""Register a custom accessor on xarray.DataArray objects.\n\n    Parameters\n    ----------\n    name : str\n        Name under which the accessor should be registered. A warning is issued\n        if this name conflicts with a preexisting attribute.\n\n    See also\n    --------\n    register_dataset_accessor\n    """"""\n    return _register_accessor(name, DataArray)\n\n\ndef register_dataset_accessor(name):\n    """"""Register a custom property on xarray.Dataset objects.\n\n    Parameters\n    ----------\n    name : str\n        Name under which the accessor should be registered. A warning is issued\n        if this name conflicts with a preexisting attribute.\n\n    Examples\n    --------\n\n    In your library code::\n\n        import xarray as xr\n\n        @xr.register_dataset_accessor(\'geo\')\n        class GeoAccessor:\n            def __init__(self, xarray_obj):\n                self._obj = xarray_obj\n\n            @property\n            def center(self):\n                # return the geographic center point of this dataset\n                lon = self._obj.latitude\n                lat = self._obj.longitude\n                return (float(lon.mean()), float(lat.mean()))\n\n            def plot(self):\n                # plot this array\'s data on a map, e.g., using Cartopy\n                pass\n\n    Back in an interactive IPython session:\n\n        >>> ds = xarray.Dataset(\n        ...     {""longitude"": np.linspace(0, 10), ""latitude"": np.linspace(0, 20)}\n        ... )\n        >>> ds.geo.center\n        (5.0, 10.0)\n        >>> ds.geo.plot()\n        # plots data on a map\n\n    See also\n    --------\n    register_dataarray_accessor\n    """"""\n    return _register_accessor(name, Dataset)\n'"
xarray/core/formatting.py,25,"b'""""""String formatting routines for __repr__.\n""""""\nimport contextlib\nimport functools\nfrom datetime import datetime, timedelta\nfrom itertools import zip_longest\nfrom typing import Hashable\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.errors import OutOfBoundsDatetime\n\nfrom .duck_array_ops import array_equiv\nfrom .options import OPTIONS\nfrom .pycompat import dask_array_type, sparse_array_type\n\n\ndef pretty_print(x, numchars: int):\n    """"""Given an object `x`, call `str(x)` and format the returned string so\n    that it is numchars long, padding with trailing spaces or truncating with\n    ellipses as necessary\n    """"""\n    s = maybe_truncate(x, numchars)\n    return s + "" "" * max(numchars - len(s), 0)\n\n\ndef maybe_truncate(obj, maxlen=500):\n    s = str(obj)\n    if len(s) > maxlen:\n        s = s[: (maxlen - 3)] + ""...""\n    return s\n\n\ndef wrap_indent(text, start="""", length=None):\n    if length is None:\n        length = len(start)\n    indent = ""\\n"" + "" "" * length\n    return start + indent.join(x for x in text.splitlines())\n\n\ndef _get_indexer_at_least_n_items(shape, n_desired, from_end):\n    assert 0 < n_desired <= np.prod(shape)\n    cum_items = np.cumprod(shape[::-1])\n    n_steps = np.argmax(cum_items >= n_desired)\n    stop = int(np.ceil(float(n_desired) / np.r_[1, cum_items][n_steps]))\n    indexer = (\n        ((-1 if from_end else 0),) * (len(shape) - 1 - n_steps)\n        + ((slice(-stop, None) if from_end else slice(stop)),)\n        + (slice(None),) * n_steps\n    )\n    return indexer\n\n\ndef first_n_items(array, n_desired):\n    """"""Returns the first n_desired items of an array""""""\n    # Unfortunately, we can\'t just do array.flat[:n_desired] here because it\n    # might not be a numpy.ndarray. Moreover, access to elements of the array\n    # could be very expensive (e.g. if it\'s only available over DAP), so go out\n    # of our way to get them in a single call to __getitem__ using only slices.\n    if n_desired < 1:\n        raise ValueError(""must request at least one item"")\n\n    if array.size == 0:\n        # work around for https://github.com/numpy/numpy/issues/5195\n        return []\n\n    if n_desired < array.size:\n        indexer = _get_indexer_at_least_n_items(array.shape, n_desired, from_end=False)\n        array = array[indexer]\n    return np.asarray(array).flat[:n_desired]\n\n\ndef last_n_items(array, n_desired):\n    """"""Returns the last n_desired items of an array""""""\n    # Unfortunately, we can\'t just do array.flat[-n_desired:] here because it\n    # might not be a numpy.ndarray. Moreover, access to elements of the array\n    # could be very expensive (e.g. if it\'s only available over DAP), so go out\n    # of our way to get them in a single call to __getitem__ using only slices.\n    if (n_desired == 0) or (array.size == 0):\n        return []\n\n    if n_desired < array.size:\n        indexer = _get_indexer_at_least_n_items(array.shape, n_desired, from_end=True)\n        array = array[indexer]\n    return np.asarray(array).flat[-n_desired:]\n\n\ndef last_item(array):\n    """"""Returns the last item of an array in a list or an empty list.""""""\n    if array.size == 0:\n        # work around for https://github.com/numpy/numpy/issues/5195\n        return []\n\n    indexer = (slice(-1, None),) * array.ndim\n    return np.ravel(np.asarray(array[indexer])).tolist()\n\n\ndef format_timestamp(t):\n    """"""Cast given object to a Timestamp and return a nicely formatted string""""""\n    # Timestamp is only valid for 1678 to 2262\n    try:\n        datetime_str = str(pd.Timestamp(t))\n    except OutOfBoundsDatetime:\n        datetime_str = str(t)\n\n    try:\n        date_str, time_str = datetime_str.split()\n    except ValueError:\n        # catch NaT and others that don\'t split nicely\n        return datetime_str\n    else:\n        if time_str == ""00:00:00"":\n            return date_str\n        else:\n            return f""{date_str}T{time_str}""\n\n\ndef format_timedelta(t, timedelta_format=None):\n    """"""Cast given object to a Timestamp and return a nicely formatted string""""""\n    timedelta_str = str(pd.Timedelta(t))\n    try:\n        days_str, time_str = timedelta_str.split("" days "")\n    except ValueError:\n        # catch NaT and others that don\'t split nicely\n        return timedelta_str\n    else:\n        if timedelta_format == ""date"":\n            return days_str + "" days""\n        elif timedelta_format == ""time"":\n            return time_str\n        else:\n            return timedelta_str\n\n\ndef format_item(x, timedelta_format=None, quote_strings=True):\n    """"""Returns a succinct summary of an object as a string""""""\n    if isinstance(x, (np.datetime64, datetime)):\n        return format_timestamp(x)\n    if isinstance(x, (np.timedelta64, timedelta)):\n        return format_timedelta(x, timedelta_format=timedelta_format)\n    elif isinstance(x, (str, bytes)):\n        return repr(x) if quote_strings else x\n    elif isinstance(x, (float, np.float)):\n        return f""{x:.4}""\n    else:\n        return str(x)\n\n\ndef format_items(x):\n    """"""Returns a succinct summaries of all items in a sequence as strings""""""\n    x = np.asarray(x)\n    timedelta_format = ""datetime""\n    if np.issubdtype(x.dtype, np.timedelta64):\n        x = np.asarray(x, dtype=""timedelta64[ns]"")\n        day_part = x[~pd.isnull(x)].astype(""timedelta64[D]"").astype(""timedelta64[ns]"")\n        time_needed = x[~pd.isnull(x)] != day_part\n        day_needed = day_part != np.timedelta64(0, ""ns"")\n        if np.logical_not(day_needed).all():\n            timedelta_format = ""time""\n        elif np.logical_not(time_needed).all():\n            timedelta_format = ""date""\n\n    formatted = [format_item(xi, timedelta_format) for xi in x]\n    return formatted\n\n\ndef format_array_flat(array, max_width: int):\n    """"""Return a formatted string for as many items in the flattened version of\n    array that will fit within max_width characters.\n    """"""\n    # every item will take up at least two characters, but we always want to\n    # print at least first and last items\n    max_possibly_relevant = min(\n        max(array.size, 1), max(int(np.ceil(max_width / 2.0)), 2)\n    )\n    relevant_front_items = format_items(\n        first_n_items(array, (max_possibly_relevant + 1) // 2)\n    )\n    relevant_back_items = format_items(last_n_items(array, max_possibly_relevant // 2))\n    # interleave relevant front and back items:\n    #     [a, b, c] and [y, z] -> [a, z, b, y, c]\n    relevant_items = sum(\n        zip_longest(relevant_front_items, reversed(relevant_back_items)), ()\n    )[:max_possibly_relevant]\n\n    cum_len = np.cumsum([len(s) + 1 for s in relevant_items]) - 1\n    if (array.size > 2) and (\n        (max_possibly_relevant < array.size) or (cum_len > max_width).any()\n    ):\n        padding = "" ... ""\n        count = min(\n            array.size, max(np.argmax(cum_len + len(padding) - 1 > max_width), 2)\n        )\n    else:\n        count = array.size\n        padding = """" if (count <= 1) else "" ""\n\n    num_front = (count + 1) // 2\n    num_back = count - num_front\n    # note that num_back is 0 <--> array.size is 0 or 1\n    #                         <--> relevant_back_items is []\n    pprint_str = """".join(\n        [\n            "" "".join(relevant_front_items[:num_front]),\n            padding,\n            "" "".join(relevant_back_items[-num_back:]),\n        ]\n    )\n\n    # As a final check, if it\'s still too long even with the limit in values,\n    # replace the end with an ellipsis\n    # NB: this will still returns a full 3-character ellipsis when max_width < 3\n    if len(pprint_str) > max_width:\n        pprint_str = pprint_str[: max(max_width - 3, 0)] + ""...""\n\n    return pprint_str\n\n\n_KNOWN_TYPE_REPRS = {np.ndarray: ""np.ndarray""}\nwith contextlib.suppress(ImportError):\n    import sparse\n\n    _KNOWN_TYPE_REPRS[sparse.COO] = ""sparse.COO""\n\n\ndef inline_dask_repr(array):\n    """"""Similar to dask.array.DataArray.__repr__, but without\n    redundant information that\'s already printed by the repr\n    function of the xarray wrapper.\n    """"""\n    assert isinstance(array, dask_array_type), array\n\n    chunksize = tuple(c[0] for c in array.chunks)\n\n    if hasattr(array, ""_meta""):\n        meta = array._meta\n        if type(meta) in _KNOWN_TYPE_REPRS:\n            meta_repr = _KNOWN_TYPE_REPRS[type(meta)]\n        else:\n            meta_repr = type(meta).__name__\n        meta_string = f"", meta={meta_repr}""\n    else:\n        meta_string = """"\n\n    return f""dask.array<chunksize={chunksize}{meta_string}>""\n\n\ndef inline_sparse_repr(array):\n    """"""Similar to sparse.COO.__repr__, but without the redundant shape/dtype.""""""\n    assert isinstance(array, sparse_array_type), array\n    return ""<{}: nnz={:d}, fill_value={!s}>"".format(\n        type(array).__name__, array.nnz, array.fill_value\n    )\n\n\ndef inline_variable_array_repr(var, max_width):\n    """"""Build a one-line summary of a variable\'s data.""""""\n    if var._in_memory:\n        return format_array_flat(var, max_width)\n    elif isinstance(var._data, dask_array_type):\n        return inline_dask_repr(var.data)\n    elif isinstance(var._data, sparse_array_type):\n        return inline_sparse_repr(var.data)\n    elif hasattr(var._data, ""__array_function__""):\n        return maybe_truncate(repr(var._data).replace(""\\n"", "" ""), max_width)\n    else:\n        # internal xarray array type\n        return ""...""\n\n\ndef summarize_variable(\n    name: Hashable, var, col_width: int, marker: str = "" "", max_width: int = None\n):\n    """"""Summarize a variable in one line, e.g., for the Dataset.__repr__.""""""\n    if max_width is None:\n        max_width_options = OPTIONS[""display_width""]\n        if not isinstance(max_width_options, int):\n            raise TypeError(f""`max_width` value of `{max_width}` is not a valid int"")\n        else:\n            max_width = max_width_options\n    first_col = pretty_print(f""  {marker} {name} "", col_width)\n    if var.dims:\n        dims_str = ""({}) "".format("", "".join(map(str, var.dims)))\n    else:\n        dims_str = """"\n    front_str = f""{first_col}{dims_str}{var.dtype} ""\n\n    values_width = max_width - len(front_str)\n    values_str = inline_variable_array_repr(var, values_width)\n\n    return front_str + values_str\n\n\ndef _summarize_coord_multiindex(coord, col_width, marker):\n    first_col = pretty_print(f""  {marker} {coord.name} "", col_width)\n    return ""{}({}) MultiIndex"".format(first_col, str(coord.dims[0]))\n\n\ndef _summarize_coord_levels(coord, col_width, marker=""-""):\n    return ""\\n"".join(\n        summarize_variable(\n            lname, coord.get_level_variable(lname), col_width, marker=marker\n        )\n        for lname in coord.level_names\n    )\n\n\ndef summarize_datavar(name, var, col_width):\n    return summarize_variable(name, var.variable, col_width)\n\n\ndef summarize_coord(name: Hashable, var, col_width: int):\n    is_index = name in var.dims\n    marker = ""*"" if is_index else "" ""\n    if is_index:\n        coord = var.variable.to_index_variable()\n        if coord.level_names is not None:\n            return ""\\n"".join(\n                [\n                    _summarize_coord_multiindex(coord, col_width, marker),\n                    _summarize_coord_levels(coord, col_width),\n                ]\n            )\n    return summarize_variable(name, var.variable, col_width, marker)\n\n\ndef summarize_attr(key, value, col_width=None):\n    """"""Summary for __repr__ - use ``X.attrs[key]`` for full value.""""""\n    # Indent key and add \':\', then right-pad if col_width is not None\n    k_str = f""    {key}:""\n    if col_width is not None:\n        k_str = pretty_print(k_str, col_width)\n    # Replace tabs and newlines, so we print on one line in known width\n    v_str = str(value).replace(""\\t"", ""\\\\t"").replace(""\\n"", ""\\\\n"")\n    # Finally, truncate to the desired display width\n    return maybe_truncate(f""{k_str} {v_str}"", OPTIONS[""display_width""])\n\n\nEMPTY_REPR = ""    *empty*""\n\n\ndef _get_col_items(mapping):\n    """"""Get all column items to format, including both keys of `mapping`\n    and MultiIndex levels if any.\n    """"""\n    from .variable import IndexVariable\n\n    col_items = []\n    for k, v in mapping.items():\n        col_items.append(k)\n        var = getattr(v, ""variable"", v)\n        if isinstance(var, IndexVariable):\n            level_names = var.to_index_variable().level_names\n            if level_names is not None:\n                col_items += list(level_names)\n    return col_items\n\n\ndef _calculate_col_width(col_items):\n    max_name_length = max(len(str(s)) for s in col_items) if col_items else 0\n    col_width = max(max_name_length, 7) + 6\n    return col_width\n\n\ndef _mapping_repr(mapping, title, summarizer, col_width=None):\n    if col_width is None:\n        col_width = _calculate_col_width(mapping)\n    summary = [f""{title}:""]\n    if mapping:\n        summary += [summarizer(k, v, col_width) for k, v in mapping.items()]\n    else:\n        summary += [EMPTY_REPR]\n    return ""\\n"".join(summary)\n\n\ndata_vars_repr = functools.partial(\n    _mapping_repr, title=""Data variables"", summarizer=summarize_datavar\n)\n\n\nattrs_repr = functools.partial(\n    _mapping_repr, title=""Attributes"", summarizer=summarize_attr\n)\n\n\ndef coords_repr(coords, col_width=None):\n    if col_width is None:\n        col_width = _calculate_col_width(_get_col_items(coords))\n    return _mapping_repr(\n        coords, title=""Coordinates"", summarizer=summarize_coord, col_width=col_width\n    )\n\n\ndef indexes_repr(indexes):\n    summary = []\n    for k, v in indexes.items():\n        summary.append(wrap_indent(repr(v), f""{k}: ""))\n    return ""\\n"".join(summary)\n\n\ndef dim_summary(obj):\n    elements = [f""{k}: {v}"" for k, v in obj.sizes.items()]\n    return "", "".join(elements)\n\n\ndef unindexed_dims_repr(dims, coords):\n    unindexed_dims = [d for d in dims if d not in coords]\n    if unindexed_dims:\n        dims_str = "", "".join(f""{d}"" for d in unindexed_dims)\n        return ""Dimensions without coordinates: "" + dims_str\n    else:\n        return None\n\n\n@contextlib.contextmanager\ndef set_numpy_options(*args, **kwargs):\n    original = np.get_printoptions()\n    np.set_printoptions(*args, **kwargs)\n    try:\n        yield\n    finally:\n        np.set_printoptions(**original)\n\n\ndef short_numpy_repr(array):\n    array = np.asarray(array)\n\n    # default to lower precision so a full (abbreviated) line can fit on\n    # one line with the default display_width\n    options = {""precision"": 6, ""linewidth"": OPTIONS[""display_width""], ""threshold"": 200}\n    if array.ndim < 3:\n        edgeitems = 3\n    elif array.ndim == 3:\n        edgeitems = 2\n    else:\n        edgeitems = 1\n    options[""edgeitems""] = edgeitems\n    with set_numpy_options(**options):\n        return repr(array)\n\n\ndef short_data_repr(array):\n    """"""Format ""data"" for DataArray and Variable.""""""\n    internal_data = getattr(array, ""variable"", array)._data\n    if isinstance(array, np.ndarray):\n        return short_numpy_repr(array)\n    elif hasattr(internal_data, ""__array_function__"") or isinstance(\n        internal_data, dask_array_type\n    ):\n        return repr(array.data)\n    elif array._in_memory or array.size < 1e5:\n        return short_numpy_repr(array)\n    else:\n        # internal xarray array type\n        return f""[{array.size} values with dtype={array.dtype}]""\n\n\ndef array_repr(arr):\n    # used for DataArray, Variable and IndexVariable\n    if hasattr(arr, ""name"") and arr.name is not None:\n        name_str = f""{arr.name!r} ""\n    else:\n        name_str = """"\n\n    summary = [\n        ""<xarray.{} {}({})>"".format(type(arr).__name__, name_str, dim_summary(arr)),\n        short_data_repr(arr),\n    ]\n\n    if hasattr(arr, ""coords""):\n        if arr.coords:\n            summary.append(repr(arr.coords))\n\n        unindexed_dims_str = unindexed_dims_repr(arr.dims, arr.coords)\n        if unindexed_dims_str:\n            summary.append(unindexed_dims_str)\n\n    if arr.attrs:\n        summary.append(attrs_repr(arr.attrs))\n\n    return ""\\n"".join(summary)\n\n\ndef dataset_repr(ds):\n    summary = [""<xarray.{}>"".format(type(ds).__name__)]\n\n    col_width = _calculate_col_width(_get_col_items(ds.variables))\n\n    dims_start = pretty_print(""Dimensions:"", col_width)\n    summary.append(""{}({})"".format(dims_start, dim_summary(ds)))\n\n    if ds.coords:\n        summary.append(coords_repr(ds.coords, col_width=col_width))\n\n    unindexed_dims_str = unindexed_dims_repr(ds.dims, ds.coords)\n    if unindexed_dims_str:\n        summary.append(unindexed_dims_str)\n\n    summary.append(data_vars_repr(ds.data_vars, col_width=col_width))\n\n    if ds.attrs:\n        summary.append(attrs_repr(ds.attrs))\n\n    return ""\\n"".join(summary)\n\n\ndef diff_dim_summary(a, b):\n    if a.dims != b.dims:\n        return ""Differing dimensions:\\n    ({}) != ({})"".format(\n            dim_summary(a), dim_summary(b)\n        )\n    else:\n        return """"\n\n\ndef _diff_mapping_repr(a_mapping, b_mapping, compat, title, summarizer, col_width=None):\n    def is_array_like(value):\n        return (\n            hasattr(value, ""ndim"")\n            and hasattr(value, ""shape"")\n            and hasattr(value, ""dtype"")\n        )\n\n    def extra_items_repr(extra_keys, mapping, ab_side):\n        extra_repr = [summarizer(k, mapping[k], col_width) for k in extra_keys]\n        if extra_repr:\n            header = f""{title} only on the {ab_side} object:""\n            return [header] + extra_repr\n        else:\n            return []\n\n    a_keys = set(a_mapping)\n    b_keys = set(b_mapping)\n\n    summary = []\n\n    diff_items = []\n\n    for k in a_keys & b_keys:\n        try:\n            # compare xarray variable\n            compatible = getattr(a_mapping[k], compat)(b_mapping[k])\n            is_variable = True\n        except AttributeError:\n            # compare attribute value\n            if is_array_like(a_mapping[k]) or is_array_like(b_mapping[k]):\n                compatible = array_equiv(a_mapping[k], b_mapping[k])\n            else:\n                compatible = a_mapping[k] == b_mapping[k]\n\n            is_variable = False\n\n        if not compatible:\n            temp = [\n                summarizer(k, vars[k], col_width) for vars in (a_mapping, b_mapping)\n            ]\n\n            if compat == ""identical"" and is_variable:\n                attrs_summary = []\n\n                for m in (a_mapping, b_mapping):\n                    attr_s = ""\\n"".join(\n                        summarize_attr(ak, av) for ak, av in m[k].attrs.items()\n                    )\n                    attrs_summary.append(attr_s)\n\n                temp = [\n                    ""\\n"".join([var_s, attr_s]) if attr_s else var_s\n                    for var_s, attr_s in zip(temp, attrs_summary)\n                ]\n\n            diff_items += [ab_side + s[1:] for ab_side, s in zip((""L"", ""R""), temp)]\n\n    if diff_items:\n        summary += [""Differing {}:"".format(title.lower())] + diff_items\n\n    summary += extra_items_repr(a_keys - b_keys, a_mapping, ""left"")\n    summary += extra_items_repr(b_keys - a_keys, b_mapping, ""right"")\n\n    return ""\\n"".join(summary)\n\n\ndiff_coords_repr = functools.partial(\n    _diff_mapping_repr, title=""Coordinates"", summarizer=summarize_coord\n)\n\n\ndiff_data_vars_repr = functools.partial(\n    _diff_mapping_repr, title=""Data variables"", summarizer=summarize_datavar\n)\n\n\ndiff_attrs_repr = functools.partial(\n    _diff_mapping_repr, title=""Attributes"", summarizer=summarize_attr\n)\n\n\ndef _compat_to_str(compat):\n    if compat == ""equals"":\n        return ""equal""\n    else:\n        return compat\n\n\ndef diff_array_repr(a, b, compat):\n    # used for DataArray, Variable and IndexVariable\n    summary = [\n        ""Left and right {} objects are not {}"".format(\n            type(a).__name__, _compat_to_str(compat)\n        )\n    ]\n\n    summary.append(diff_dim_summary(a, b))\n\n    if not array_equiv(a.data, b.data):\n        temp = [wrap_indent(short_numpy_repr(obj), start=""    "") for obj in (a, b)]\n        diff_data_repr = [\n            ab_side + ""\\n"" + ab_data_repr\n            for ab_side, ab_data_repr in zip((""L"", ""R""), temp)\n        ]\n        summary += [""Differing values:""] + diff_data_repr\n\n    if hasattr(a, ""coords""):\n        col_width = _calculate_col_width(set(a.coords) | set(b.coords))\n        summary.append(\n            diff_coords_repr(a.coords, b.coords, compat, col_width=col_width)\n        )\n\n    if compat == ""identical"":\n        summary.append(diff_attrs_repr(a.attrs, b.attrs, compat))\n\n    return ""\\n"".join(summary)\n\n\ndef diff_dataset_repr(a, b, compat):\n    summary = [\n        ""Left and right {} objects are not {}"".format(\n            type(a).__name__, _compat_to_str(compat)\n        )\n    ]\n\n    col_width = _calculate_col_width(\n        set(_get_col_items(a.variables) + _get_col_items(b.variables))\n    )\n\n    summary.append(diff_dim_summary(a, b))\n    summary.append(diff_coords_repr(a.coords, b.coords, compat, col_width=col_width))\n    summary.append(\n        diff_data_vars_repr(a.data_vars, b.data_vars, compat, col_width=col_width)\n    )\n\n    if compat == ""identical"":\n        summary.append(diff_attrs_repr(a.attrs, b.attrs, compat))\n\n    return ""\\n"".join(summary)\n'"
xarray/core/formatting_html.py,0,"b'import uuid\nfrom collections import OrderedDict\nfrom functools import partial\nfrom html import escape\n\nimport pkg_resources\n\nfrom .formatting import inline_variable_array_repr, short_data_repr\n\nCSS_FILE_PATH = ""/"".join((""static"", ""css"", ""style.css""))\nCSS_STYLE = pkg_resources.resource_string(""xarray"", CSS_FILE_PATH).decode(""utf8"")\n\n\nICONS_SVG_PATH = ""/"".join((""static"", ""html"", ""icons-svg-inline.html""))\nICONS_SVG = pkg_resources.resource_string(""xarray"", ICONS_SVG_PATH).decode(""utf8"")\n\n\ndef short_data_repr_html(array):\n    """"""Format ""data"" for DataArray and Variable.""""""\n    internal_data = getattr(array, ""variable"", array)._data\n    if hasattr(internal_data, ""_repr_html_""):\n        return internal_data._repr_html_()\n    return escape(short_data_repr(array))\n\n\ndef format_dims(dims, coord_names):\n    if not dims:\n        return """"\n\n    dim_css_map = {\n        k: "" class=\'xr-has-index\'"" if k in coord_names else """" for k, v in dims.items()\n    }\n\n    dims_li = """".join(\n        f""<li><span{dim_css_map[dim]}>"" f""{escape(dim)}</span>: {size}</li>""\n        for dim, size in dims.items()\n    )\n\n    return f""<ul class=\'xr-dim-list\'>{dims_li}</ul>""\n\n\ndef summarize_attrs(attrs):\n    attrs_dl = """".join(\n        f""<dt><span>{escape(k)} :</span></dt>"" f""<dd>{escape(str(v))}</dd>""\n        for k, v in attrs.items()\n    )\n\n    return f""<dl class=\'xr-attrs\'>{attrs_dl}</dl>""\n\n\ndef _icon(icon_name):\n    # icon_name should be defined in xarray/static/html/icon-svg-inline.html\n    return (\n        ""<svg class=\'icon xr-{0}\'>""\n        ""<use xlink:href=\'#{0}\'>""\n        ""</use>""\n        ""</svg>"".format(icon_name)\n    )\n\n\ndef _summarize_coord_multiindex(name, coord):\n    preview = f""({\', \'.join(escape(l) for l in coord.level_names)})""\n    return summarize_variable(\n        name, coord, is_index=True, dtype=""MultiIndex"", preview=preview\n    )\n\n\ndef summarize_coord(name, var):\n    is_index = name in var.dims\n    if is_index:\n        coord = var.variable.to_index_variable()\n        if coord.level_names is not None:\n            coords = {}\n            coords[name] = _summarize_coord_multiindex(name, coord)\n            for lname in coord.level_names:\n                var = coord.get_level_variable(lname)\n                coords[lname] = summarize_variable(lname, var)\n            return coords\n\n    return {name: summarize_variable(name, var, is_index)}\n\n\ndef summarize_coords(variables):\n    coords = {}\n    for k, v in variables.items():\n        coords.update(**summarize_coord(k, v))\n\n    vars_li = """".join(f""<li class=\'xr-var-item\'>{v}</li>"" for v in coords.values())\n\n    return f""<ul class=\'xr-var-list\'>{vars_li}</ul>""\n\n\ndef summarize_variable(name, var, is_index=False, dtype=None, preview=None):\n    variable = var.variable if hasattr(var, ""variable"") else var\n\n    cssclass_idx = "" class=\'xr-has-index\'"" if is_index else """"\n    dims_str = f""({\', \'.join(escape(dim) for dim in var.dims)})""\n    name = escape(str(name))\n    dtype = dtype or escape(str(var.dtype))\n\n    # ""unique"" ids required to expand/collapse subsections\n    attrs_id = ""attrs-"" + str(uuid.uuid4())\n    data_id = ""data-"" + str(uuid.uuid4())\n    disabled = """" if len(var.attrs) else ""disabled""\n\n    preview = preview or escape(inline_variable_array_repr(variable, 35))\n    attrs_ul = summarize_attrs(var.attrs)\n    data_repr = short_data_repr_html(variable)\n\n    attrs_icon = _icon(""icon-file-text2"")\n    data_icon = _icon(""icon-database"")\n\n    return (\n        f""<div class=\'xr-var-name\'><span{cssclass_idx}>{name}</span></div>""\n        f""<div class=\'xr-var-dims\'>{dims_str}</div>""\n        f""<div class=\'xr-var-dtype\'>{dtype}</div>""\n        f""<div class=\'xr-var-preview xr-preview\'>{preview}</div>""\n        f""<input id=\'{attrs_id}\' class=\'xr-var-attrs-in\' ""\n        f""type=\'checkbox\' {disabled}>""\n        f""<label for=\'{attrs_id}\' title=\'Show/Hide attributes\'>""\n        f""{attrs_icon}</label>""\n        f""<input id=\'{data_id}\' class=\'xr-var-data-in\' type=\'checkbox\'>""\n        f""<label for=\'{data_id}\' title=\'Show/Hide data repr\'>""\n        f""{data_icon}</label>""\n        f""<div class=\'xr-var-attrs\'>{attrs_ul}</div>""\n        f""<pre class=\'xr-var-data\'>{data_repr}</pre>""\n    )\n\n\ndef summarize_vars(variables):\n    vars_li = """".join(\n        f""<li class=\'xr-var-item\'>{summarize_variable(k, v)}</li>""\n        for k, v in variables.items()\n    )\n\n    return f""<ul class=\'xr-var-list\'>{vars_li}</ul>""\n\n\ndef collapsible_section(\n    name, inline_details="""", details="""", n_items=None, enabled=True, collapsed=False\n):\n    # ""unique"" id to expand/collapse the section\n    data_id = ""section-"" + str(uuid.uuid4())\n\n    has_items = n_items is not None and n_items\n    n_items_span = """" if n_items is None else f"" <span>({n_items})</span>""\n    enabled = """" if enabled and has_items else ""disabled""\n    collapsed = """" if collapsed or not has_items else ""checked""\n    tip = "" title=\'Expand/collapse section\'"" if enabled else """"\n\n    return (\n        f""<input id=\'{data_id}\' class=\'xr-section-summary-in\' ""\n        f""type=\'checkbox\' {enabled} {collapsed}>""\n        f""<label for=\'{data_id}\' class=\'xr-section-summary\' {tip}>""\n        f""{name}:{n_items_span}</label>""\n        f""<div class=\'xr-section-inline-details\'>{inline_details}</div>""\n        f""<div class=\'xr-section-details\'>{details}</div>""\n    )\n\n\ndef _mapping_section(mapping, name, details_func, max_items_collapse, enabled=True):\n    n_items = len(mapping)\n    collapsed = n_items >= max_items_collapse\n\n    return collapsible_section(\n        name,\n        details=details_func(mapping),\n        n_items=n_items,\n        enabled=enabled,\n        collapsed=collapsed,\n    )\n\n\ndef dim_section(obj):\n    dim_list = format_dims(obj.dims, list(obj.coords))\n\n    return collapsible_section(\n        ""Dimensions"", inline_details=dim_list, enabled=False, collapsed=True\n    )\n\n\ndef array_section(obj):\n    # ""unique"" id to expand/collapse the section\n    data_id = ""section-"" + str(uuid.uuid4())\n    collapsed = """"\n    variable = getattr(obj, ""variable"", obj)\n    preview = escape(inline_variable_array_repr(variable, max_width=70))\n    data_repr = short_data_repr_html(obj)\n    data_icon = _icon(""icon-database"")\n\n    return (\n        ""<div class=\'xr-array-wrap\'>""\n        f""<input id=\'{data_id}\' class=\'xr-array-in\' type=\'checkbox\' {collapsed}>""\n        f""<label for=\'{data_id}\' title=\'Show/hide data repr\'>{data_icon}</label>""\n        f""<div class=\'xr-array-preview xr-preview\'><span>{preview}</span></div>""\n        f""<pre class=\'xr-array-data\'>{data_repr}</pre>""\n        ""</div>""\n    )\n\n\ncoord_section = partial(\n    _mapping_section,\n    name=""Coordinates"",\n    details_func=summarize_coords,\n    max_items_collapse=25,\n)\n\n\ndatavar_section = partial(\n    _mapping_section,\n    name=""Data variables"",\n    details_func=summarize_vars,\n    max_items_collapse=15,\n)\n\n\nattr_section = partial(\n    _mapping_section,\n    name=""Attributes"",\n    details_func=summarize_attrs,\n    max_items_collapse=10,\n)\n\n\ndef _obj_repr(obj, header_components, sections):\n    """"""Return HTML repr of an xarray object.\n\n    If CSS is not injected (untrusted notebook), fallback to the plain text repr.\n\n    """"""\n    header = f""<div class=\'xr-header\'>{\'\'.join(h for h in header_components)}</div>""\n    sections = """".join(f""<li class=\'xr-section-item\'>{s}</li>"" for s in sections)\n\n    return (\n        ""<div>""\n        f""{ICONS_SVG}<style>{CSS_STYLE}</style>""\n        f""<pre class=\'xr-text-repr-fallback\'>{escape(repr(obj))}</pre>""\n        ""<div class=\'xr-wrap\' hidden>""\n        f""{header}""\n        f""<ul class=\'xr-sections\'>{sections}</ul>""\n        ""</div>""\n        ""</div>""\n    )\n\n\ndef array_repr(arr):\n    dims = OrderedDict((k, v) for k, v in zip(arr.dims, arr.shape))\n\n    obj_type = ""xarray.{}"".format(type(arr).__name__)\n    arr_name = ""\'{}\'"".format(arr.name) if getattr(arr, ""name"", None) else """"\n    coord_names = list(arr.coords) if hasattr(arr, ""coords"") else []\n\n    header_components = [\n        ""<div class=\'xr-obj-type\'>{}</div>"".format(obj_type),\n        ""<div class=\'xr-array-name\'>{}</div>"".format(arr_name),\n        format_dims(dims, coord_names),\n    ]\n\n    sections = [array_section(arr)]\n\n    if hasattr(arr, ""coords""):\n        sections.append(coord_section(arr.coords))\n\n    sections.append(attr_section(arr.attrs))\n\n    return _obj_repr(arr, header_components, sections)\n\n\ndef dataset_repr(ds):\n    obj_type = ""xarray.{}"".format(type(ds).__name__)\n\n    header_components = [f""<div class=\'xr-obj-type\'>{escape(obj_type)}</div>""]\n\n    sections = [\n        dim_section(ds),\n        coord_section(ds.coords),\n        datavar_section(ds.data_vars),\n        attr_section(ds.attrs),\n    ]\n\n    return _obj_repr(ds, header_components, sections)\n'"
xarray/core/groupby.py,11,"b'import datetime\nimport functools\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nfrom . import dtypes, duck_array_ops, nputils, ops\nfrom .arithmetic import SupportsArithmetic\nfrom .common import ImplementsArrayReduce, ImplementsDatasetReduce\nfrom .concat import concat\nfrom .formatting import format_array_flat\nfrom .indexes import propagate_indexes\nfrom .options import _get_keep_attrs\nfrom .pycompat import integer_types\nfrom .utils import (\n    either_dict_or_kwargs,\n    hashable,\n    is_scalar,\n    maybe_wrap_array,\n    peek_at,\n    safe_cast_to_index,\n)\nfrom .variable import IndexVariable, Variable, as_variable\n\n\ndef check_reduce_dims(reduce_dims, dimensions):\n\n    if reduce_dims is not ...:\n        if is_scalar(reduce_dims):\n            reduce_dims = [reduce_dims]\n        if any(dim not in dimensions for dim in reduce_dims):\n            raise ValueError(\n                ""cannot reduce over dimensions %r. expected either \'...\' to reduce over all dimensions or one or more of %r.""\n                % (reduce_dims, dimensions)\n            )\n\n\ndef unique_value_groups(ar, sort=True):\n    """"""Group an array by its unique values.\n\n    Parameters\n    ----------\n    ar : array-like\n        Input array. This will be flattened if it is not already 1-D.\n    sort : boolean, optional\n        Whether or not to sort unique values.\n\n    Returns\n    -------\n    values : np.ndarray\n        Sorted, unique values as returned by `np.unique`.\n    indices : list of lists of int\n        Each element provides the integer indices in `ar` with values given by\n        the corresponding value in `unique_values`.\n    """"""\n    inverse, values = pd.factorize(ar, sort=sort)\n    groups = [[] for _ in range(len(values))]\n    for n, g in enumerate(inverse):\n        if g >= 0:\n            # pandas uses -1 to mark NaN, but doesn\'t include them in values\n            groups[g].append(n)\n    return values, groups\n\n\ndef _dummy_copy(xarray_obj):\n    from .dataset import Dataset\n    from .dataarray import DataArray\n\n    if isinstance(xarray_obj, Dataset):\n        res = Dataset(\n            {\n                k: dtypes.get_fill_value(v.dtype)\n                for k, v in xarray_obj.data_vars.items()\n            },\n            {\n                k: dtypes.get_fill_value(v.dtype)\n                for k, v in xarray_obj.coords.items()\n                if k not in xarray_obj.dims\n            },\n            xarray_obj.attrs,\n        )\n    elif isinstance(xarray_obj, DataArray):\n        res = DataArray(\n            dtypes.get_fill_value(xarray_obj.dtype),\n            {\n                k: dtypes.get_fill_value(v.dtype)\n                for k, v in xarray_obj.coords.items()\n                if k not in xarray_obj.dims\n            },\n            dims=[],\n            name=xarray_obj.name,\n            attrs=xarray_obj.attrs,\n        )\n    else:  # pragma: no cover\n        raise AssertionError\n    return res\n\n\ndef _is_one_or_none(obj):\n    return obj == 1 or obj is None\n\n\ndef _consolidate_slices(slices):\n    """"""Consolidate adjacent slices in a list of slices.\n    """"""\n    result = []\n    last_slice = slice(None)\n    for slice_ in slices:\n        if not isinstance(slice_, slice):\n            raise ValueError(""list element is not a slice: %r"" % slice_)\n        if (\n            result\n            and last_slice.stop == slice_.start\n            and _is_one_or_none(last_slice.step)\n            and _is_one_or_none(slice_.step)\n        ):\n            last_slice = slice(last_slice.start, slice_.stop, slice_.step)\n            result[-1] = last_slice\n        else:\n            result.append(slice_)\n            last_slice = slice_\n    return result\n\n\ndef _inverse_permutation_indices(positions):\n    """"""Like inverse_permutation, but also handles slices.\n\n    Parameters\n    ----------\n    positions : list of np.ndarray or slice objects.\n        If slice objects, all are assumed to be slices.\n\n    Returns\n    -------\n    np.ndarray of indices or None, if no permutation is necessary.\n    """"""\n    if not positions:\n        return None\n\n    if isinstance(positions[0], slice):\n        positions = _consolidate_slices(positions)\n        if positions == slice(None):\n            return None\n        positions = [np.arange(sl.start, sl.stop, sl.step) for sl in positions]\n\n    indices = nputils.inverse_permutation(np.concatenate(positions))\n    return indices\n\n\nclass _DummyGroup:\n    """"""Class for keeping track of grouped dimensions without coordinates.\n\n    Should not be user visible.\n    """"""\n\n    __slots__ = (""name"", ""coords"", ""size"")\n\n    def __init__(self, obj, name, coords):\n        self.name = name\n        self.coords = coords\n        self.size = obj.sizes[name]\n\n    @property\n    def dims(self):\n        return (self.name,)\n\n    @property\n    def ndim(self):\n        return 1\n\n    @property\n    def values(self):\n        return range(self.size)\n\n    @property\n    def shape(self):\n        return (self.size,)\n\n    def __getitem__(self, key):\n        if isinstance(key, tuple):\n            key = key[0]\n        return self.values[key]\n\n\ndef _ensure_1d(group, obj):\n    if group.ndim != 1:\n        # try to stack the dims of the group into a single dim\n        orig_dims = group.dims\n        stacked_dim = ""stacked_"" + ""_"".join(orig_dims)\n        # these dimensions get created by the stack operation\n        inserted_dims = [dim for dim in group.dims if dim not in group.coords]\n        # the copy is necessary here, otherwise read only array raises error\n        # in pandas: https://github.com/pydata/pandas/issues/12813\n        group = group.stack(**{stacked_dim: orig_dims}).copy()\n        obj = obj.stack(**{stacked_dim: orig_dims})\n    else:\n        stacked_dim = None\n        inserted_dims = []\n    return group, obj, stacked_dim, inserted_dims\n\n\ndef _unique_and_monotonic(group):\n    if isinstance(group, _DummyGroup):\n        return True\n    else:\n        index = safe_cast_to_index(group)\n        return index.is_unique and index.is_monotonic\n\n\ndef _apply_loffset(grouper, result):\n    """"""\n    (copied from pandas)\n    if loffset is set, offset the result index\n\n    This is NOT an idempotent routine, it will be applied\n    exactly once to the result.\n\n    Parameters\n    ----------\n    result : Series or DataFrame\n        the result of resample\n    """"""\n\n    needs_offset = (\n        isinstance(grouper.loffset, (pd.DateOffset, datetime.timedelta))\n        and isinstance(result.index, pd.DatetimeIndex)\n        and len(result.index) > 0\n    )\n\n    if needs_offset:\n        result.index = result.index + grouper.loffset\n\n    grouper.loffset = None\n\n\nclass GroupBy(SupportsArithmetic):\n    """"""A object that implements the split-apply-combine pattern.\n\n    Modeled after `pandas.GroupBy`. The `GroupBy` object can be iterated over\n    (unique_value, grouped_array) pairs, but the main way to interact with a\n    groupby object are with the `apply` or `reduce` methods. You can also\n    directly call numpy methods like `mean` or `std`.\n\n    You should create a GroupBy object by using the `DataArray.groupby` or\n    `Dataset.groupby` methods.\n\n    See Also\n    --------\n    Dataset.groupby\n    DataArray.groupby\n    """"""\n\n    __slots__ = (\n        ""_full_index"",\n        ""_inserted_dims"",\n        ""_group"",\n        ""_group_dim"",\n        ""_group_indices"",\n        ""_groups"",\n        ""_obj"",\n        ""_restore_coord_dims"",\n        ""_stacked_dim"",\n        ""_unique_coord"",\n        ""_dims"",\n    )\n\n    def __init__(\n        self,\n        obj,\n        group,\n        squeeze=False,\n        grouper=None,\n        bins=None,\n        restore_coord_dims=True,\n        cut_kwargs=None,\n    ):\n        """"""Create a GroupBy object\n\n        Parameters\n        ----------\n        obj : Dataset or DataArray\n            Object to group.\n        group : DataArray\n            Array with the group values.\n        squeeze : boolean, optional\n            If ""group"" is a coordinate of object, `squeeze` controls whether\n            the subarrays have a dimension of length 1 along that coordinate or\n            if the dimension is squeezed out.\n        grouper : pd.Grouper, optional\n            Used for grouping values along the `group` array.\n        bins : array-like, optional\n            If `bins` is specified, the groups will be discretized into the\n            specified bins by `pandas.cut`.\n        restore_coord_dims : bool, default True\n            If True, also restore the dimension order of multi-dimensional\n            coordinates.\n        cut_kwargs : dict, optional\n            Extra keyword arguments to pass to `pandas.cut`\n\n        """"""\n        if cut_kwargs is None:\n            cut_kwargs = {}\n        from .dataarray import DataArray\n\n        if grouper is not None and bins is not None:\n            raise TypeError(""can\'t specify both `grouper` and `bins`"")\n\n        if not isinstance(group, (DataArray, IndexVariable)):\n            if not hashable(group):\n                raise TypeError(\n                    ""`group` must be an xarray.DataArray or the ""\n                    ""name of an xarray variable or dimension""\n                )\n            group = obj[group]\n            if len(group) == 0:\n                raise ValueError(f""{group.name} must not be empty"")\n\n            if group.name not in obj.coords and group.name in obj.dims:\n                # DummyGroups should not appear on groupby results\n                group = _DummyGroup(obj, group.name, group.coords)\n\n        if getattr(group, ""name"", None) is None:\n            group.name = ""group""\n\n        group, obj, stacked_dim, inserted_dims = _ensure_1d(group, obj)\n        (group_dim,) = group.dims\n\n        expected_size = obj.sizes[group_dim]\n        if group.size != expected_size:\n            raise ValueError(\n                ""the group variable\'s length does not ""\n                ""match the length of this variable along its ""\n                ""dimension""\n            )\n\n        full_index = None\n\n        if bins is not None:\n            if duck_array_ops.isnull(bins).all():\n                raise ValueError(""All bin edges are NaN."")\n            binned = pd.cut(group.values, bins, **cut_kwargs)\n            new_dim_name = group.name + ""_bins""\n            group = DataArray(binned, group.coords, name=new_dim_name)\n            full_index = binned.categories\n\n        if grouper is not None:\n            index = safe_cast_to_index(group)\n            if not index.is_monotonic:\n                # TODO: sort instead of raising an error\n                raise ValueError(""index must be monotonic for resampling"")\n            full_index, first_items = self._get_index_and_items(index, grouper)\n            sbins = first_items.values.astype(np.int64)\n            group_indices = [slice(i, j) for i, j in zip(sbins[:-1], sbins[1:])] + [\n                slice(sbins[-1], None)\n            ]\n            unique_coord = IndexVariable(group.name, first_items.index)\n        elif group.dims == (group.name,) and _unique_and_monotonic(group):\n            # no need to factorize\n            group_indices = np.arange(group.size)\n            if not squeeze:\n                # use slices to do views instead of fancy indexing\n                # equivalent to: group_indices = group_indices.reshape(-1, 1)\n                group_indices = [slice(i, i + 1) for i in group_indices]\n            unique_coord = group\n        else:\n            if group.isnull().any():\n                # drop any NaN valued groups.\n                # also drop obj values where group was NaN\n                # Use where instead of reindex to account for duplicate coordinate labels.\n                obj = obj.where(group.notnull(), drop=True)\n                group = group.dropna(group_dim)\n\n            # look through group to find the unique values\n            group_as_index = safe_cast_to_index(group)\n            sort = bins is None and (not isinstance(group_as_index, pd.MultiIndex))\n            unique_values, group_indices = unique_value_groups(\n                group_as_index, sort=sort\n            )\n            unique_coord = IndexVariable(group.name, unique_values)\n\n        if len(group_indices) == 0:\n            if bins is not None:\n                raise ValueError(\n                    ""None of the data falls within bins with edges %r"" % bins\n                )\n            else:\n                raise ValueError(\n                    ""Failed to group data. Are you grouping by a variable that is all NaN?""\n                )\n\n        # specification for the groupby operation\n        self._obj = obj\n        self._group = group\n        self._group_dim = group_dim\n        self._group_indices = group_indices\n        self._unique_coord = unique_coord\n        self._stacked_dim = stacked_dim\n        self._inserted_dims = inserted_dims\n        self._full_index = full_index\n        self._restore_coord_dims = restore_coord_dims\n\n        # cached attributes\n        self._groups = None\n        self._dims = None\n\n    @property\n    def dims(self):\n        if self._dims is None:\n            self._dims = self._obj.isel(\n                **{self._group_dim: self._group_indices[0]}\n            ).dims\n\n        return self._dims\n\n    @property\n    def groups(self):\n        # provided to mimic pandas.groupby\n        if self._groups is None:\n            self._groups = dict(zip(self._unique_coord.values, self._group_indices))\n        return self._groups\n\n    def __len__(self):\n        return self._unique_coord.size\n\n    def __iter__(self):\n        return zip(self._unique_coord.values, self._iter_grouped())\n\n    def __repr__(self):\n        return ""{}, grouped over {!r} \\n{!r} groups with labels {}."".format(\n            self.__class__.__name__,\n            self._unique_coord.name,\n            self._unique_coord.size,\n            "", "".join(format_array_flat(self._unique_coord, 30).split()),\n        )\n\n    def _get_index_and_items(self, index, grouper):\n        from .resample_cftime import CFTimeGrouper\n\n        s = pd.Series(np.arange(index.size), index)\n        if isinstance(grouper, CFTimeGrouper):\n            first_items = grouper.first_items(index)\n        else:\n            first_items = s.groupby(grouper).first()\n            _apply_loffset(grouper, first_items)\n        full_index = first_items.index\n        if first_items.isnull().any():\n            first_items = first_items.dropna()\n        return full_index, first_items\n\n    def _iter_grouped(self):\n        """"""Iterate over each element in this group""""""\n        for indices in self._group_indices:\n            yield self._obj.isel(**{self._group_dim: indices})\n\n    def _infer_concat_args(self, applied_example):\n        if self._group_dim in applied_example.dims:\n            coord = self._group\n            positions = self._group_indices\n        else:\n            coord = self._unique_coord\n            positions = None\n        (dim,) = coord.dims\n        if isinstance(coord, _DummyGroup):\n            coord = None\n        return coord, dim, positions\n\n    @staticmethod\n    def _binary_op(f, reflexive=False, **ignored_kwargs):\n        @functools.wraps(f)\n        def func(self, other):\n            g = f if not reflexive else lambda x, y: f(y, x)\n            applied = self._yield_binary_applied(g, other)\n            combined = self._combine(applied)\n            return combined\n\n        return func\n\n    def _yield_binary_applied(self, func, other):\n        dummy = None\n\n        for group_value, obj in self:\n            try:\n                other_sel = other.sel(**{self._group.name: group_value})\n            except AttributeError:\n                raise TypeError(\n                    ""GroupBy objects only support binary ops ""\n                    ""when the other argument is a Dataset or ""\n                    ""DataArray""\n                )\n            except (KeyError, ValueError):\n                if self._group.name not in other.dims:\n                    raise ValueError(\n                        ""incompatible dimensions for a grouped ""\n                        ""binary operation: the group variable %r ""\n                        ""is not a dimension on the other argument"" % self._group.name\n                    )\n                if dummy is None:\n                    dummy = _dummy_copy(other)\n                other_sel = dummy\n\n            result = func(obj, other_sel)\n            yield result\n\n    def _maybe_restore_empty_groups(self, combined):\n        """"""Our index contained empty groups (e.g., from a resampling). If we\n        reduced on that dimension, we want to restore the full index.\n        """"""\n        if self._full_index is not None and self._group.name in combined.dims:\n            indexers = {self._group.name: self._full_index}\n            combined = combined.reindex(**indexers)\n        return combined\n\n    def _maybe_unstack(self, obj):\n        """"""This gets called if we are applying on an array with a\n        multidimensional group.""""""\n        if self._stacked_dim is not None and self._stacked_dim in obj.dims:\n            obj = obj.unstack(self._stacked_dim)\n            for dim in self._inserted_dims:\n                if dim in obj.coords:\n                    del obj.coords[dim]\n            obj._indexes = propagate_indexes(obj._indexes, exclude=self._inserted_dims)\n        return obj\n\n    def fillna(self, value):\n        """"""Fill missing values in this object by group.\n\n        This operation follows the normal broadcasting and alignment rules that\n        xarray uses for binary arithmetic, except the result is aligned to this\n        object (``join=\'left\'``) instead of aligned to the intersection of\n        index coordinates (``join=\'inner\'``).\n\n        Parameters\n        ----------\n        value : valid type for the grouped object\'s fillna method\n            Used to fill all matching missing values by group.\n\n        Returns\n        -------\n        same type as the grouped object\n\n        See also\n        --------\n        Dataset.fillna\n        DataArray.fillna\n        """"""\n        out = ops.fillna(self, value)\n        return out\n\n    def quantile(\n        self, q, dim=None, interpolation=""linear"", keep_attrs=None, skipna=True\n    ):\n        """"""Compute the qth quantile over each array in the groups and\n        concatenate them together into a new array.\n\n        Parameters\n        ----------\n        q : float in range of [0,1] (or sequence of floats)\n            Quantile to compute, which must be between 0 and 1\n            inclusive.\n        dim : `...`, str or sequence of str, optional\n            Dimension(s) over which to apply quantile.\n            Defaults to the grouped dimension.\n        interpolation : {\'linear\', \'lower\', \'higher\', \'midpoint\', \'nearest\'}\n            This optional parameter specifies the interpolation method to\n            use when the desired quantile lies between two data points\n            ``i < j``:\n\n                * linear: ``i + (j - i) * fraction``, where ``fraction`` is\n                  the fractional part of the index surrounded by ``i`` and\n                  ``j``.\n                * lower: ``i``.\n                * higher: ``j``.\n                * nearest: ``i`` or ``j``, whichever is nearest.\n                * midpoint: ``(i + j) / 2``.\n        skipna : bool, optional\n            Whether to skip missing values when aggregating.\n\n        Returns\n        -------\n        quantiles : Variable\n            If `q` is a single quantile, then the result is a\n            scalar. If multiple percentiles are given, first axis of\n            the result corresponds to the quantile. In either case a\n            quantile dimension is added to the return array. The other\n            dimensions are the dimensions that remain after the\n            reduction of the array.\n\n        See Also\n        --------\n        numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile,\n        DataArray.quantile\n\n        Examples\n        --------\n\n        >>> da = xr.DataArray(\n        ...     [[1.3, 8.4, 0.7, 6.9], [0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],\n        ...     coords={""x"": [0, 0, 1], ""y"": [1, 1, 2, 2]},\n        ...     dims=(""y"", ""y""),\n        ... )\n        >>> ds = xr.Dataset({""a"": da})\n        >>> da.groupby(""x"").quantile(0)\n        <xarray.DataArray (x: 2, y: 4)>\n        array([[0.7, 4.2, 0.7, 1.5],\n               [6.5, 7.3, 2.6, 1.9]])\n        Coordinates:\n            quantile  float64 0.0\n          * y         (y) int64 1 1 2 2\n          * x         (x) int64 0 1\n        >>> ds.groupby(""y"").quantile(0, dim=...)\n        <xarray.Dataset>\n        Dimensions:   (y: 2)\n        Coordinates:\n            quantile  float64 0.0\n          * y         (y) int64 1 2\n        Data variables:\n            a         (y) float64 0.7 0.7\n        >>> da.groupby(""x"").quantile([0, 0.5, 1])\n        <xarray.DataArray (x: 2, y: 4, quantile: 3)>\n        array([[[0.7 , 1.  , 1.3 ],\n                [4.2 , 6.3 , 8.4 ],\n                [0.7 , 5.05, 9.4 ],\n                [1.5 , 4.2 , 6.9 ]],\n               [[6.5 , 6.5 , 6.5 ],\n                [7.3 , 7.3 , 7.3 ],\n                [2.6 , 2.6 , 2.6 ],\n                [1.9 , 1.9 , 1.9 ]]])\n        Coordinates:\n          * y         (y) int64 1 1 2 2\n          * quantile  (quantile) float64 0.0 0.5 1.0\n          * x         (x) int64 0 1\n        >>> ds.groupby(""y"").quantile([0, 0.5, 1], dim=...)\n        <xarray.Dataset>\n        Dimensions:   (quantile: 3, y: 2)\n        Coordinates:\n          * quantile  (quantile) float64 0.0 0.5 1.0\n          * y         (y) int64 1 2\n        Data variables:\n            a         (y, quantile) float64 0.7 5.35 8.4 0.7 2.25 9.4\n        """"""\n        if dim is None:\n            dim = self._group_dim\n\n        out = self.map(\n            self._obj.__class__.quantile,\n            shortcut=False,\n            q=q,\n            dim=dim,\n            interpolation=interpolation,\n            keep_attrs=keep_attrs,\n            skipna=skipna,\n        )\n\n        return out\n\n    def where(self, cond, other=dtypes.NA):\n        """"""Return elements from `self` or `other` depending on `cond`.\n\n        Parameters\n        ----------\n        cond : DataArray or Dataset with boolean dtype\n            Locations at which to preserve this objects values.\n        other : scalar, DataArray or Dataset, optional\n            Value to use for locations in this object where ``cond`` is False.\n            By default, inserts missing values.\n\n        Returns\n        -------\n        same type as the grouped object\n\n        See also\n        --------\n        Dataset.where\n        """"""\n        return ops.where_method(self, cond, other)\n\n    def _first_or_last(self, op, skipna, keep_attrs):\n        if isinstance(self._group_indices[0], integer_types):\n            # NB. this is currently only used for reductions along an existing\n            # dimension\n            return self._obj\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=True)\n        return self.reduce(op, self._group_dim, skipna=skipna, keep_attrs=keep_attrs)\n\n    def first(self, skipna=None, keep_attrs=None):\n        """"""Return the first element of each group along the group dimension\n        """"""\n        return self._first_or_last(duck_array_ops.first, skipna, keep_attrs)\n\n    def last(self, skipna=None, keep_attrs=None):\n        """"""Return the last element of each group along the group dimension\n        """"""\n        return self._first_or_last(duck_array_ops.last, skipna, keep_attrs)\n\n    def assign_coords(self, coords=None, **coords_kwargs):\n        """"""Assign coordinates by group.\n\n        See also\n        --------\n        Dataset.assign_coords\n        Dataset.swap_dims\n        """"""\n        coords_kwargs = either_dict_or_kwargs(coords, coords_kwargs, ""assign_coords"")\n        return self.map(lambda ds: ds.assign_coords(**coords_kwargs))\n\n\ndef _maybe_reorder(xarray_obj, dim, positions):\n    order = _inverse_permutation_indices(positions)\n\n    if order is None or len(order) != xarray_obj.sizes[dim]:\n        return xarray_obj\n    else:\n        return xarray_obj[{dim: order}]\n\n\nclass DataArrayGroupBy(GroupBy, ImplementsArrayReduce):\n    """"""GroupBy object specialized to grouping DataArray objects\n    """"""\n\n    def _iter_grouped_shortcut(self):\n        """"""Fast version of `_iter_grouped` that yields Variables without\n        metadata\n        """"""\n        var = self._obj.variable\n        for indices in self._group_indices:\n            yield var[{self._group_dim: indices}]\n\n    def _concat_shortcut(self, applied, dim, positions=None):\n        # nb. don\'t worry too much about maintaining this method -- it does\n        # speed things up, but it\'s not very interpretable and there are much\n        # faster alternatives (e.g., doing the grouped aggregation in a\n        # compiled language)\n        stacked = Variable.concat(applied, dim, shortcut=True)\n        reordered = _maybe_reorder(stacked, dim, positions)\n        result = self._obj._replace_maybe_drop_dims(reordered)\n        return result\n\n    def _restore_dim_order(self, stacked):\n        def lookup_order(dimension):\n            if dimension == self._group.name:\n                (dimension,) = self._group.dims\n            if dimension in self._obj.dims:\n                axis = self._obj.get_axis_num(dimension)\n            else:\n                axis = 1e6  # some arbitrarily high value\n            return axis\n\n        new_order = sorted(stacked.dims, key=lookup_order)\n        return stacked.transpose(*new_order, transpose_coords=self._restore_coord_dims)\n\n    def map(self, func, shortcut=False, args=(), **kwargs):\n        """"""Apply a function to each array in the group and concatenate them\n        together into a new array.\n\n        `func` is called like `func(ar, *args, **kwargs)` for each array `ar`\n        in this group.\n\n        Apply uses heuristics (like `pandas.GroupBy.apply`) to figure out how\n        to stack together the array. The rule is:\n\n        1. If the dimension along which the group coordinate is defined is\n           still in the first grouped array after applying `func`, then stack\n           over this dimension.\n        2. Otherwise, stack over the new dimension given by name of this\n           grouping (the argument to the `groupby` function).\n\n        Parameters\n        ----------\n        func : function\n            Callable to apply to each array.\n        shortcut : bool, optional\n            Whether or not to shortcut evaluation under the assumptions that:\n\n            (1) The action of `func` does not depend on any of the array\n                metadata (attributes or coordinates) but only on the data and\n                dimensions.\n            (2) The action of `func` creates arrays with homogeneous metadata,\n                that is, with the same dimensions and attributes.\n\n            If these conditions are satisfied `shortcut` provides significant\n            speedup. This should be the case for many common groupby operations\n            (e.g., applying numpy ufuncs).\n        ``*args`` : tuple, optional\n            Positional arguments passed to `func`.\n        ``**kwargs``\n            Used to call `func(ar, **kwargs)` for each array `ar`.\n\n        Returns\n        -------\n        applied : DataArray or DataArray\n            The result of splitting, applying and combining this array.\n        """"""\n        if shortcut:\n            grouped = self._iter_grouped_shortcut()\n        else:\n            grouped = self._iter_grouped()\n        applied = (maybe_wrap_array(arr, func(arr, *args, **kwargs)) for arr in grouped)\n        return self._combine(applied, shortcut=shortcut)\n\n    def apply(self, func, shortcut=False, args=(), **kwargs):\n        """"""\n        Backward compatible implementation of ``map``\n\n        See Also\n        --------\n        DataArrayGroupBy.map\n        """"""\n        warnings.warn(\n            ""GroupBy.apply may be deprecated in the future. Using GroupBy.map is encouraged"",\n            PendingDeprecationWarning,\n            stacklevel=2,\n        )\n        return self.map(func, shortcut=shortcut, args=args, **kwargs)\n\n    def _combine(self, applied, restore_coord_dims=False, shortcut=False):\n        """"""Recombine the applied objects like the original.""""""\n        applied_example, applied = peek_at(applied)\n        coord, dim, positions = self._infer_concat_args(applied_example)\n        if shortcut:\n            combined = self._concat_shortcut(applied, dim, positions)\n        else:\n            combined = concat(applied, dim)\n            combined = _maybe_reorder(combined, dim, positions)\n\n        if isinstance(combined, type(self._obj)):\n            # only restore dimension order for arrays\n            combined = self._restore_dim_order(combined)\n        # assign coord when the applied function does not return that coord\n        if coord is not None and dim not in applied_example.dims:\n            if shortcut:\n                coord_var = as_variable(coord)\n                combined._coords[coord.name] = coord_var\n            else:\n                combined.coords[coord.name] = coord\n        combined = self._maybe_restore_empty_groups(combined)\n        combined = self._maybe_unstack(combined)\n        return combined\n\n    def reduce(\n        self, func, dim=None, axis=None, keep_attrs=None, shortcut=True, **kwargs\n    ):\n        """"""Reduce the items in this group by applying `func` along some\n        dimension(s).\n\n        Parameters\n        ----------\n        func : function\n            Function which can be called in the form\n            `func(x, axis=axis, **kwargs)` to return the result of collapsing\n            an np.ndarray over an integer valued axis.\n        dim : `...`, str or sequence of str, optional\n            Dimension(s) over which to apply `func`.\n        axis : int or sequence of int, optional\n            Axis(es) over which to apply `func`. Only one of the \'dimension\'\n            and \'axis\' arguments can be supplied. If neither are supplied, then\n            `func` is calculated over all dimension for each group item.\n        keep_attrs : bool, optional\n            If True, the datasets\'s attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n        **kwargs : dict\n            Additional keyword arguments passed on to `func`.\n\n        Returns\n        -------\n        reduced : Array\n            Array with summarized data and the indicated dimension(s)\n            removed.\n        """"""\n        if dim is None:\n            dim = self._group_dim\n\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n\n        def reduce_array(ar):\n            return ar.reduce(func, dim, axis, keep_attrs=keep_attrs, **kwargs)\n\n        check_reduce_dims(dim, self.dims)\n\n        return self.map(reduce_array, shortcut=shortcut)\n\n\nops.inject_reduce_methods(DataArrayGroupBy)\nops.inject_binary_ops(DataArrayGroupBy)\n\n\nclass DatasetGroupBy(GroupBy, ImplementsDatasetReduce):\n    def map(self, func, args=(), shortcut=None, **kwargs):\n        """"""Apply a function to each Dataset in the group and concatenate them\n        together into a new Dataset.\n\n        `func` is called like `func(ds, *args, **kwargs)` for each dataset `ds`\n        in this group.\n\n        Apply uses heuristics (like `pandas.GroupBy.apply`) to figure out how\n        to stack together the datasets. The rule is:\n\n        1. If the dimension along which the group coordinate is defined is\n           still in the first grouped item after applying `func`, then stack\n           over this dimension.\n        2. Otherwise, stack over the new dimension given by name of this\n           grouping (the argument to the `groupby` function).\n\n        Parameters\n        ----------\n        func : function\n            Callable to apply to each sub-dataset.\n        args : tuple, optional\n            Positional arguments to pass to `func`.\n        **kwargs\n            Used to call `func(ds, **kwargs)` for each sub-dataset `ar`.\n\n        Returns\n        -------\n        applied : Dataset or DataArray\n            The result of splitting, applying and combining this dataset.\n        """"""\n        # ignore shortcut if set (for now)\n        applied = (func(ds, *args, **kwargs) for ds in self._iter_grouped())\n        return self._combine(applied)\n\n    def apply(self, func, args=(), shortcut=None, **kwargs):\n        """"""\n        Backward compatible implementation of ``map``\n\n        See Also\n        --------\n        DatasetGroupBy.map\n        """"""\n\n        warnings.warn(\n            ""GroupBy.apply may be deprecated in the future. Using GroupBy.map is encouraged"",\n            PendingDeprecationWarning,\n            stacklevel=2,\n        )\n        return self.map(func, shortcut=shortcut, args=args, **kwargs)\n\n    def _combine(self, applied):\n        """"""Recombine the applied objects like the original.""""""\n        applied_example, applied = peek_at(applied)\n        coord, dim, positions = self._infer_concat_args(applied_example)\n        combined = concat(applied, dim)\n        combined = _maybe_reorder(combined, dim, positions)\n        # assign coord when the applied function does not return that coord\n        if coord is not None and dim not in applied_example.dims:\n            combined[coord.name] = coord\n        combined = self._maybe_restore_empty_groups(combined)\n        combined = self._maybe_unstack(combined)\n        return combined\n\n    def reduce(self, func, dim=None, keep_attrs=None, **kwargs):\n        """"""Reduce the items in this group by applying `func` along some\n        dimension(s).\n\n        Parameters\n        ----------\n        func : function\n            Function which can be called in the form\n            `func(x, axis=axis, **kwargs)` to return the result of collapsing\n            an np.ndarray over an integer valued axis.\n        dim : `...`, str or sequence of str, optional\n            Dimension(s) over which to apply `func`.\n        axis : int or sequence of int, optional\n            Axis(es) over which to apply `func`. Only one of the \'dimension\'\n            and \'axis\' arguments can be supplied. If neither are supplied, then\n            `func` is calculated over all dimension for each group item.\n        keep_attrs : bool, optional\n            If True, the datasets\'s attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n        **kwargs : dict\n            Additional keyword arguments passed on to `func`.\n\n        Returns\n        -------\n        reduced : Array\n            Array with summarized data and the indicated dimension(s)\n            removed.\n        """"""\n        if dim is None:\n            dim = self._group_dim\n\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n\n        def reduce_dataset(ds):\n            return ds.reduce(func, dim, keep_attrs, **kwargs)\n\n        check_reduce_dims(dim, self.dims)\n\n        return self.map(reduce_dataset)\n\n    def assign(self, **kwargs):\n        """"""Assign data variables by group.\n\n        See also\n        --------\n        Dataset.assign\n        """"""\n        return self.map(lambda ds: ds.assign(**kwargs))\n\n\nops.inject_reduce_methods(DatasetGroupBy)\nops.inject_binary_ops(DatasetGroupBy)\n'"
xarray/core/indexes.py,1,"b'import collections.abc\nfrom typing import Any, Dict, Hashable, Iterable, Mapping, Optional, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\n\nfrom . import formatting\nfrom .utils import is_scalar\nfrom .variable import Variable\n\n\ndef remove_unused_levels_categories(index, dataframe=None):\n    """"""\n    Remove unused levels from MultiIndex and unused categories from CategoricalIndex\n    """"""\n    if isinstance(index, pd.MultiIndex):\n        index = index.remove_unused_levels()\n        # if it contains CategoricalIndex, we need to remove unused categories\n        # manually. See https://github.com/pandas-dev/pandas/issues/30846\n        if any(isinstance(lev, pd.CategoricalIndex) for lev in index.levels):\n            levels = []\n            for i, level in enumerate(index.levels):\n                if isinstance(level, pd.CategoricalIndex):\n                    level = level[index.codes[i]].remove_unused_categories()\n                else:\n                    level = level[index.codes[i]]\n                levels.append(level)\n            index = pd.MultiIndex.from_arrays(levels, names=index.names)\n    elif isinstance(index, pd.CategoricalIndex):\n        index = index.remove_unused_categories()\n\n    if dataframe is None:\n        return index\n    dataframe = dataframe.set_index(index)\n    return dataframe.index, dataframe\n\n\nclass Indexes(collections.abc.Mapping):\n    """"""Immutable proxy for Dataset or DataArrary indexes.""""""\n\n    __slots__ = (""_indexes"",)\n\n    def __init__(self, indexes):\n        """"""Not for public consumption.\n\n        Parameters\n        ----------\n        indexes : Dict[Any, pandas.Index]\n           Indexes held by this object.\n        """"""\n        self._indexes = indexes\n\n    def __iter__(self):\n        return iter(self._indexes)\n\n    def __len__(self):\n        return len(self._indexes)\n\n    def __contains__(self, key):\n        return key in self._indexes\n\n    def __getitem__(self, key):\n        return self._indexes[key]\n\n    def __repr__(self):\n        return formatting.indexes_repr(self)\n\n\ndef default_indexes(\n    coords: Mapping[Any, Variable], dims: Iterable\n) -> Dict[Hashable, pd.Index]:\n    """"""Default indexes for a Dataset/DataArray.\n\n    Parameters\n    ----------\n    coords : Mapping[Any, xarray.Variable]\n       Coordinate variables from which to draw default indexes.\n    dims : iterable\n        Iterable of dimension names.\n\n    Returns\n    -------\n    Mapping from indexing keys (levels/dimension names) to indexes used for\n    indexing along that dimension.\n    """"""\n    return {key: coords[key].to_index() for key in dims if key in coords}\n\n\ndef isel_variable_and_index(\n    name: Hashable,\n    variable: Variable,\n    index: pd.Index,\n    indexers: Mapping[Hashable, Union[int, slice, np.ndarray, Variable]],\n) -> Tuple[Variable, Optional[pd.Index]]:\n    """"""Index a Variable and pandas.Index together.""""""\n    if not indexers:\n        # nothing to index\n        return variable.copy(deep=False), index\n\n    if len(variable.dims) > 1:\n        raise NotImplementedError(\n            ""indexing multi-dimensional variable with indexes is not "" ""supported yet""\n        )\n\n    new_variable = variable.isel(indexers)\n\n    if new_variable.dims != (name,):\n        # can\'t preserve a index if result has new dimensions\n        return new_variable, None\n\n    # we need to compute the new index\n    (dim,) = variable.dims\n    indexer = indexers[dim]\n    if isinstance(indexer, Variable):\n        indexer = indexer.data\n    new_index = index[indexer]\n    return new_variable, new_index\n\n\ndef roll_index(index: pd.Index, count: int, axis: int = 0) -> pd.Index:\n    """"""Roll an pandas.Index.""""""\n    count %= index.shape[0]\n    if count != 0:\n        return index[-count:].append(index[:-count])\n    else:\n        return index[:]\n\n\ndef propagate_indexes(\n    indexes: Optional[Dict[Hashable, pd.Index]], exclude: Optional[Any] = None\n) -> Optional[Dict[Hashable, pd.Index]]:\n    """""" Creates new indexes dict from existing dict optionally excluding some dimensions.\n    """"""\n    if exclude is None:\n        exclude = ()\n\n    if is_scalar(exclude):\n        exclude = (exclude,)\n\n    if indexes is not None:\n        new_indexes = {k: v for k, v in indexes.items() if k not in exclude}\n    else:\n        new_indexes = None  # type: ignore\n\n    return new_indexes\n'"
xarray/core/indexing.py,98,"b'import enum\nimport functools\nimport operator\nfrom collections import defaultdict\nfrom contextlib import suppress\nfrom datetime import timedelta\nfrom typing import Any, Callable, Iterable, Sequence, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\n\nfrom . import duck_array_ops, nputils, utils\nfrom .npcompat import DTypeLike\nfrom .pycompat import dask_array_type, integer_types, sparse_array_type\nfrom .utils import is_dict_like, maybe_cast_to_coords_dtype\n\n\ndef expanded_indexer(key, ndim):\n    """"""Given a key for indexing an ndarray, return an equivalent key which is a\n    tuple with length equal to the number of dimensions.\n\n    The expansion is done by replacing all `Ellipsis` items with the right\n    number of full slices and then padding the key with full slices so that it\n    reaches the appropriate dimensionality.\n    """"""\n    if not isinstance(key, tuple):\n        # numpy treats non-tuple keys equivalent to tuples of length 1\n        key = (key,)\n    new_key = []\n    # handling Ellipsis right is a little tricky, see:\n    # http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing\n    found_ellipsis = False\n    for k in key:\n        if k is Ellipsis:\n            if not found_ellipsis:\n                new_key.extend((ndim + 1 - len(key)) * [slice(None)])\n                found_ellipsis = True\n            else:\n                new_key.append(slice(None))\n        else:\n            new_key.append(k)\n    if len(new_key) > ndim:\n        raise IndexError(""too many indices"")\n    new_key.extend((ndim - len(new_key)) * [slice(None)])\n    return tuple(new_key)\n\n\ndef _expand_slice(slice_, size):\n    return np.arange(*slice_.indices(size))\n\n\ndef _sanitize_slice_element(x):\n    from .variable import Variable\n    from .dataarray import DataArray\n\n    if isinstance(x, (Variable, DataArray)):\n        x = x.values\n\n    if isinstance(x, np.ndarray):\n        if x.ndim != 0:\n            raise ValueError(\n                f""cannot use non-scalar arrays in a slice for xarray indexing: {x}""\n            )\n        x = x[()]\n\n    if isinstance(x, np.timedelta64):\n        # pandas does not support indexing with np.timedelta64 yet:\n        # https://github.com/pandas-dev/pandas/issues/20393\n        x = pd.Timedelta(x)\n\n    return x\n\n\ndef _asarray_tuplesafe(values):\n    """"""\n    Convert values into a numpy array of at most 1-dimension, while preserving\n    tuples.\n\n    Adapted from pandas.core.common._asarray_tuplesafe\n    """"""\n    if isinstance(values, tuple):\n        result = utils.to_0d_object_array(values)\n    else:\n        result = np.asarray(values)\n        if result.ndim == 2:\n            result = np.empty(len(values), dtype=object)\n            result[:] = values\n\n    return result\n\n\ndef _is_nested_tuple(possible_tuple):\n    return isinstance(possible_tuple, tuple) and any(\n        isinstance(value, (tuple, list, slice)) for value in possible_tuple\n    )\n\n\ndef get_indexer_nd(index, labels, method=None, tolerance=None):\n    """"""Wrapper around :meth:`pandas.Index.get_indexer` supporting n-dimensional\n    labels\n    """"""\n    flat_labels = np.ravel(labels)\n    flat_indexer = index.get_indexer(flat_labels, method=method, tolerance=tolerance)\n    indexer = flat_indexer.reshape(labels.shape)\n    return indexer\n\n\ndef convert_label_indexer(index, label, index_name="""", method=None, tolerance=None):\n    """"""Given a pandas.Index and labels (e.g., from __getitem__) for one\n    dimension, return an indexer suitable for indexing an ndarray along that\n    dimension. If `index` is a pandas.MultiIndex and depending on `label`,\n    return a new pandas.Index or pandas.MultiIndex (otherwise return None).\n    """"""\n    new_index = None\n\n    if isinstance(label, slice):\n        if method is not None or tolerance is not None:\n            raise NotImplementedError(\n                ""cannot use ``method`` argument if any indexers are "" ""slice objects""\n            )\n        indexer = index.slice_indexer(\n            _sanitize_slice_element(label.start),\n            _sanitize_slice_element(label.stop),\n            _sanitize_slice_element(label.step),\n        )\n        if not isinstance(indexer, slice):\n            # unlike pandas, in xarray we never want to silently convert a\n            # slice indexer into an array indexer\n            raise KeyError(\n                ""cannot represent labeled-based slice indexer for dimension ""\n                f""{index_name!r} with a slice over integer positions; the index is ""\n                ""unsorted or non-unique""\n            )\n\n    elif is_dict_like(label):\n        is_nested_vals = _is_nested_tuple(tuple(label.values()))\n        if not isinstance(index, pd.MultiIndex):\n            raise ValueError(\n                ""cannot use a dict-like object for selection on ""\n                ""a dimension that does not have a MultiIndex""\n            )\n        elif len(label) == index.nlevels and not is_nested_vals:\n            indexer = index.get_loc(tuple(label[k] for k in index.names))\n        else:\n            for k, v in label.items():\n                # index should be an item (i.e. Hashable) not an array-like\n                if isinstance(v, Sequence) and not isinstance(v, str):\n                    raise ValueError(\n                        ""Vectorized selection is not ""\n                        ""available along level variable: "" + k\n                    )\n            indexer, new_index = index.get_loc_level(\n                tuple(label.values()), level=tuple(label.keys())\n            )\n\n            # GH2619. Raise a KeyError if nothing is chosen\n            if indexer.dtype.kind == ""b"" and indexer.sum() == 0:\n                raise KeyError(f""{label} not found"")\n\n    elif isinstance(label, tuple) and isinstance(index, pd.MultiIndex):\n        if _is_nested_tuple(label):\n            indexer = index.get_locs(label)\n        elif len(label) == index.nlevels:\n            indexer = index.get_loc(label)\n        else:\n            indexer, new_index = index.get_loc_level(\n                label, level=list(range(len(label)))\n            )\n    else:\n        label = (\n            label\n            if getattr(label, ""ndim"", 1) > 1  # vectorized-indexing\n            else _asarray_tuplesafe(label)\n        )\n        if label.ndim == 0:\n            if isinstance(index, pd.MultiIndex):\n                indexer, new_index = index.get_loc_level(label.item(), level=0)\n            elif isinstance(index, pd.CategoricalIndex):\n                if method is not None:\n                    raise ValueError(\n                        ""\'method\' is not a valid kwarg when indexing using a CategoricalIndex.""\n                    )\n                if tolerance is not None:\n                    raise ValueError(\n                        ""\'tolerance\' is not a valid kwarg when indexing using a CategoricalIndex.""\n                    )\n                indexer = index.get_loc(label.item())\n            else:\n                indexer = index.get_loc(\n                    label.item(), method=method, tolerance=tolerance\n                )\n        elif label.dtype.kind == ""b"":\n            indexer = label\n        else:\n            if isinstance(index, pd.MultiIndex) and label.ndim > 1:\n                raise ValueError(\n                    ""Vectorized selection is not available along ""\n                    ""MultiIndex variable: "" + index_name\n                )\n            indexer = get_indexer_nd(index, label, method, tolerance)\n            if np.any(indexer < 0):\n                raise KeyError(f""not all values found in index {index_name!r}"")\n    return indexer, new_index\n\n\ndef get_dim_indexers(data_obj, indexers):\n    """"""Given a xarray data object and label based indexers, return a mapping\n    of label indexers with only dimension names as keys.\n\n    It groups multiple level indexers given on a multi-index dimension\n    into a single, dictionary indexer for that dimension (Raise a ValueError\n    if it is not possible).\n    """"""\n    invalid = [\n        k\n        for k in indexers\n        if k not in data_obj.dims and k not in data_obj._level_coords\n    ]\n    if invalid:\n        raise ValueError(f""dimensions or multi-index levels {invalid!r} do not exist"")\n\n    level_indexers = defaultdict(dict)\n    dim_indexers = {}\n    for key, label in indexers.items():\n        (dim,) = data_obj[key].dims\n        if key != dim:\n            # assume here multi-index level indexer\n            level_indexers[dim][key] = label\n        else:\n            dim_indexers[key] = label\n\n    for dim, level_labels in level_indexers.items():\n        if dim_indexers.get(dim, False):\n            raise ValueError(\n                ""cannot combine multi-index level indexers with an indexer for ""\n                f""dimension {dim}""\n            )\n        dim_indexers[dim] = level_labels\n\n    return dim_indexers\n\n\ndef remap_label_indexers(data_obj, indexers, method=None, tolerance=None):\n    """"""Given an xarray data object and label based indexers, return a mapping\n    of equivalent location based indexers. Also return a mapping of updated\n    pandas index objects (in case of multi-index level drop).\n    """"""\n    if method is not None and not isinstance(method, str):\n        raise TypeError(""``method`` must be a string"")\n\n    pos_indexers = {}\n    new_indexes = {}\n\n    dim_indexers = get_dim_indexers(data_obj, indexers)\n    for dim, label in dim_indexers.items():\n        try:\n            index = data_obj.indexes[dim]\n        except KeyError:\n            # no index for this dimension: reuse the provided labels\n            if method is not None or tolerance is not None:\n                raise ValueError(\n                    ""cannot supply ``method`` or ``tolerance`` ""\n                    ""when the indexed dimension does not have ""\n                    ""an associated coordinate.""\n                )\n            pos_indexers[dim] = label\n        else:\n            coords_dtype = data_obj.coords[dim].dtype\n            label = maybe_cast_to_coords_dtype(label, coords_dtype)\n            idxr, new_idx = convert_label_indexer(index, label, dim, method, tolerance)\n            pos_indexers[dim] = idxr\n            if new_idx is not None:\n                new_indexes[dim] = new_idx\n\n    return pos_indexers, new_indexes\n\n\ndef slice_slice(old_slice, applied_slice, size):\n    """"""Given a slice and the size of the dimension to which it will be applied,\n    index it with another slice to return a new slice equivalent to applying\n    the slices sequentially\n    """"""\n    step = (old_slice.step or 1) * (applied_slice.step or 1)\n\n    # For now, use the hack of turning old_slice into an ndarray to reconstruct\n    # the slice start and stop. This is not entirely ideal, but it is still\n    # definitely better than leaving the indexer as an array.\n    items = _expand_slice(old_slice, size)[applied_slice]\n    if len(items) > 0:\n        start = items[0]\n        stop = items[-1] + int(np.sign(step))\n        if stop < 0:\n            stop = None\n    else:\n        start = 0\n        stop = 0\n    return slice(start, stop, step)\n\n\ndef _index_indexer_1d(old_indexer, applied_indexer, size):\n    assert isinstance(applied_indexer, integer_types + (slice, np.ndarray))\n    if isinstance(applied_indexer, slice) and applied_indexer == slice(None):\n        # shortcut for the usual case\n        return old_indexer\n    if isinstance(old_indexer, slice):\n        if isinstance(applied_indexer, slice):\n            indexer = slice_slice(old_indexer, applied_indexer, size)\n        else:\n            indexer = _expand_slice(old_indexer, size)[applied_indexer]\n    else:\n        indexer = old_indexer[applied_indexer]\n    return indexer\n\n\nclass ExplicitIndexer:\n    """"""Base class for explicit indexer objects.\n\n    ExplicitIndexer objects wrap a tuple of values given by their ``tuple``\n    property. These tuples should always have length equal to the number of\n    dimensions on the indexed array.\n\n    Do not instantiate BaseIndexer objects directly: instead, use one of the\n    sub-classes BasicIndexer, OuterIndexer or VectorizedIndexer.\n    """"""\n\n    __slots__ = (""_key"",)\n\n    def __init__(self, key):\n        if type(self) is ExplicitIndexer:\n            raise TypeError(""cannot instantiate base ExplicitIndexer objects"")\n        self._key = tuple(key)\n\n    @property\n    def tuple(self):\n        return self._key\n\n    def __repr__(self):\n        return f""{type(self).__name__}({self.tuple})""\n\n\ndef as_integer_or_none(value):\n    return None if value is None else operator.index(value)\n\n\ndef as_integer_slice(value):\n    start = as_integer_or_none(value.start)\n    stop = as_integer_or_none(value.stop)\n    step = as_integer_or_none(value.step)\n    return slice(start, stop, step)\n\n\nclass BasicIndexer(ExplicitIndexer):\n    """"""Tuple for basic indexing.\n\n    All elements should be int or slice objects. Indexing follows NumPy\'s\n    rules for basic indexing: each axis is independently sliced and axes\n    indexed with an integer are dropped from the result.\n    """"""\n\n    __slots__ = ()\n\n    def __init__(self, key):\n        if not isinstance(key, tuple):\n            raise TypeError(f""key must be a tuple: {key!r}"")\n\n        new_key = []\n        for k in key:\n            if isinstance(k, integer_types):\n                k = int(k)\n            elif isinstance(k, slice):\n                k = as_integer_slice(k)\n            else:\n                raise TypeError(\n                    f""unexpected indexer type for {type(self).__name__}: {k!r}""\n                )\n            new_key.append(k)\n\n        super().__init__(new_key)\n\n\nclass OuterIndexer(ExplicitIndexer):\n    """"""Tuple for outer/orthogonal indexing.\n\n    All elements should be int, slice or 1-dimensional np.ndarray objects with\n    an integer dtype. Indexing is applied independently along each axis, and\n    axes indexed with an integer are dropped from the result. This type of\n    indexing works like MATLAB/Fortran.\n    """"""\n\n    __slots__ = ()\n\n    def __init__(self, key):\n        if not isinstance(key, tuple):\n            raise TypeError(f""key must be a tuple: {key!r}"")\n\n        new_key = []\n        for k in key:\n            if isinstance(k, integer_types):\n                k = int(k)\n            elif isinstance(k, slice):\n                k = as_integer_slice(k)\n            elif isinstance(k, np.ndarray):\n                if not np.issubdtype(k.dtype, np.integer):\n                    raise TypeError(\n                        f""invalid indexer array, does not have integer dtype: {k!r}""\n                    )\n                if k.ndim != 1:\n                    raise TypeError(\n                        f""invalid indexer array for {type(self).__name__}; must have ""\n                        f""exactly 1 dimension: {k!r}""\n                    )\n                k = np.asarray(k, dtype=np.int64)\n            else:\n                raise TypeError(\n                    f""unexpected indexer type for {type(self).__name__}: {k!r}""\n                )\n            new_key.append(k)\n\n        super().__init__(new_key)\n\n\nclass VectorizedIndexer(ExplicitIndexer):\n    """"""Tuple for vectorized indexing.\n\n    All elements should be slice or N-dimensional np.ndarray objects with an\n    integer dtype and the same number of dimensions. Indexing follows proposed\n    rules for np.ndarray.vindex, which matches NumPy\'s advanced indexing rules\n    (including broadcasting) except sliced axes are always moved to the end:\n    https://github.com/numpy/numpy/pull/6256\n    """"""\n\n    __slots__ = ()\n\n    def __init__(self, key):\n        if not isinstance(key, tuple):\n            raise TypeError(f""key must be a tuple: {key!r}"")\n\n        new_key = []\n        ndim = None\n        for k in key:\n            if isinstance(k, slice):\n                k = as_integer_slice(k)\n            elif isinstance(k, np.ndarray):\n                if not np.issubdtype(k.dtype, np.integer):\n                    raise TypeError(\n                        f""invalid indexer array, does not have integer dtype: {k!r}""\n                    )\n                if ndim is None:\n                    ndim = k.ndim\n                elif ndim != k.ndim:\n                    ndims = [k.ndim for k in key if isinstance(k, np.ndarray)]\n                    raise ValueError(\n                        ""invalid indexer key: ndarray arguments ""\n                        f""have different numbers of dimensions: {ndims}""\n                    )\n                k = np.asarray(k, dtype=np.int64)\n            else:\n                raise TypeError(\n                    f""unexpected indexer type for {type(self).__name__}: {k!r}""\n                )\n            new_key.append(k)\n\n        super().__init__(new_key)\n\n\nclass ExplicitlyIndexed:\n    """"""Mixin to mark support for Indexer subclasses in indexing.\n    """"""\n\n    __slots__ = ()\n\n\nclass ExplicitlyIndexedNDArrayMixin(utils.NDArrayMixin, ExplicitlyIndexed):\n    __slots__ = ()\n\n    def __array__(self, dtype=None):\n        key = BasicIndexer((slice(None),) * self.ndim)\n        return np.asarray(self[key], dtype=dtype)\n\n\nclass ImplicitToExplicitIndexingAdapter(utils.NDArrayMixin):\n    """"""Wrap an array, converting tuples into the indicated explicit indexer.""""""\n\n    __slots__ = (""array"", ""indexer_cls"")\n\n    def __init__(self, array, indexer_cls=BasicIndexer):\n        self.array = as_indexable(array)\n        self.indexer_cls = indexer_cls\n\n    def __array__(self, dtype=None):\n        return np.asarray(self.array, dtype=dtype)\n\n    def __getitem__(self, key):\n        key = expanded_indexer(key, self.ndim)\n        result = self.array[self.indexer_cls(key)]\n        if isinstance(result, ExplicitlyIndexed):\n            return type(self)(result, self.indexer_cls)\n        else:\n            # Sometimes explicitly indexed arrays return NumPy arrays or\n            # scalars.\n            return result\n\n\nclass LazilyOuterIndexedArray(ExplicitlyIndexedNDArrayMixin):\n    """"""Wrap an array to make basic and outer indexing lazy.\n    """"""\n\n    __slots__ = (""array"", ""key"")\n\n    def __init__(self, array, key=None):\n        """"""\n        Parameters\n        ----------\n        array : array_like\n            Array like object to index.\n        key : ExplicitIndexer, optional\n            Array indexer. If provided, it is assumed to already be in\n            canonical expanded form.\n        """"""\n        if isinstance(array, type(self)) and key is None:\n            # unwrap\n            key = array.key\n            array = array.array\n\n        if key is None:\n            key = BasicIndexer((slice(None),) * array.ndim)\n\n        self.array = as_indexable(array)\n        self.key = key\n\n    def _updated_key(self, new_key):\n        iter_new_key = iter(expanded_indexer(new_key.tuple, self.ndim))\n        full_key = []\n        for size, k in zip(self.array.shape, self.key.tuple):\n            if isinstance(k, integer_types):\n                full_key.append(k)\n            else:\n                full_key.append(_index_indexer_1d(k, next(iter_new_key), size))\n        full_key = tuple(full_key)\n\n        if all(isinstance(k, integer_types + (slice,)) for k in full_key):\n            return BasicIndexer(full_key)\n        return OuterIndexer(full_key)\n\n    @property\n    def shape(self):\n        shape = []\n        for size, k in zip(self.array.shape, self.key.tuple):\n            if isinstance(k, slice):\n                shape.append(len(range(*k.indices(size))))\n            elif isinstance(k, np.ndarray):\n                shape.append(k.size)\n        return tuple(shape)\n\n    def __array__(self, dtype=None):\n        array = as_indexable(self.array)\n        return np.asarray(array[self.key], dtype=None)\n\n    def transpose(self, order):\n        return LazilyVectorizedIndexedArray(self.array, self.key).transpose(order)\n\n    def __getitem__(self, indexer):\n        if isinstance(indexer, VectorizedIndexer):\n            array = LazilyVectorizedIndexedArray(self.array, self.key)\n            return array[indexer]\n        return type(self)(self.array, self._updated_key(indexer))\n\n    def __setitem__(self, key, value):\n        if isinstance(key, VectorizedIndexer):\n            raise NotImplementedError(\n                ""Lazy item assignment with the vectorized indexer is not yet ""\n                ""implemented. Load your data first by .load() or compute().""\n            )\n        full_key = self._updated_key(key)\n        self.array[full_key] = value\n\n    def __repr__(self):\n        return f""{type(self).__name__}(array={self.array!r}, key={self.key!r})""\n\n\nclass LazilyVectorizedIndexedArray(ExplicitlyIndexedNDArrayMixin):\n    """"""Wrap an array to make vectorized indexing lazy.\n    """"""\n\n    __slots__ = (""array"", ""key"")\n\n    def __init__(self, array, key):\n        """"""\n        Parameters\n        ----------\n        array : array_like\n            Array like object to index.\n        key : VectorizedIndexer\n        """"""\n        if isinstance(key, (BasicIndexer, OuterIndexer)):\n            self.key = _outer_to_vectorized_indexer(key, array.shape)\n        else:\n            self.key = _arrayize_vectorized_indexer(key, array.shape)\n        self.array = as_indexable(array)\n\n    @property\n    def shape(self):\n        return np.broadcast(*self.key.tuple).shape\n\n    def __array__(self, dtype=None):\n        return np.asarray(self.array[self.key], dtype=None)\n\n    def _updated_key(self, new_key):\n        return _combine_indexers(self.key, self.shape, new_key)\n\n    def __getitem__(self, indexer):\n        # If the indexed array becomes a scalar, return LazilyOuterIndexedArray\n        if all(isinstance(ind, integer_types) for ind in indexer.tuple):\n            key = BasicIndexer(tuple(k[indexer.tuple] for k in self.key.tuple))\n            return LazilyOuterIndexedArray(self.array, key)\n        return type(self)(self.array, self._updated_key(indexer))\n\n    def transpose(self, order):\n        key = VectorizedIndexer(tuple(k.transpose(order) for k in self.key.tuple))\n        return type(self)(self.array, key)\n\n    def __setitem__(self, key, value):\n        raise NotImplementedError(\n            ""Lazy item assignment with the vectorized indexer is not yet ""\n            ""implemented. Load your data first by .load() or compute().""\n        )\n\n    def __repr__(self):\n        return f""{type(self).__name__}(array={self.array!r}, key={self.key!r})""\n\n\ndef _wrap_numpy_scalars(array):\n    """"""Wrap NumPy scalars in 0d arrays.""""""\n    if np.isscalar(array):\n        return np.array(array)\n    else:\n        return array\n\n\nclass CopyOnWriteArray(ExplicitlyIndexedNDArrayMixin):\n    __slots__ = (""array"", ""_copied"")\n\n    def __init__(self, array):\n        self.array = as_indexable(array)\n        self._copied = False\n\n    def _ensure_copied(self):\n        if not self._copied:\n            self.array = as_indexable(np.array(self.array))\n            self._copied = True\n\n    def __array__(self, dtype=None):\n        return np.asarray(self.array, dtype=dtype)\n\n    def __getitem__(self, key):\n        return type(self)(_wrap_numpy_scalars(self.array[key]))\n\n    def transpose(self, order):\n        return self.array.transpose(order)\n\n    def __setitem__(self, key, value):\n        self._ensure_copied()\n        self.array[key] = value\n\n\nclass MemoryCachedArray(ExplicitlyIndexedNDArrayMixin):\n    __slots__ = (""array"",)\n\n    def __init__(self, array):\n        self.array = _wrap_numpy_scalars(as_indexable(array))\n\n    def _ensure_cached(self):\n        if not isinstance(self.array, NumpyIndexingAdapter):\n            self.array = NumpyIndexingAdapter(np.asarray(self.array))\n\n    def __array__(self, dtype=None):\n        self._ensure_cached()\n        return np.asarray(self.array, dtype=dtype)\n\n    def __getitem__(self, key):\n        return type(self)(_wrap_numpy_scalars(self.array[key]))\n\n    def transpose(self, order):\n        return self.array.transpose(order)\n\n    def __setitem__(self, key, value):\n        self.array[key] = value\n\n\ndef as_indexable(array):\n    """"""\n    This function always returns a ExplicitlyIndexed subclass,\n    so that the vectorized indexing is always possible with the returned\n    object.\n    """"""\n    if isinstance(array, ExplicitlyIndexed):\n        return array\n    if isinstance(array, np.ndarray):\n        return NumpyIndexingAdapter(array)\n    if isinstance(array, pd.Index):\n        return PandasIndexAdapter(array)\n    if isinstance(array, dask_array_type):\n        return DaskIndexingAdapter(array)\n    if hasattr(array, ""__array_function__""):\n        return NdArrayLikeIndexingAdapter(array)\n\n    raise TypeError(""Invalid array type: {}"".format(type(array)))\n\n\ndef _outer_to_vectorized_indexer(key, shape):\n    """"""Convert an OuterIndexer into an vectorized indexer.\n\n    Parameters\n    ----------\n    key : Outer/Basic Indexer\n        An indexer to convert.\n    shape : tuple\n        Shape of the array subject to the indexing.\n\n    Returns\n    -------\n    VectorizedIndexer\n        Tuple suitable for use to index a NumPy array with vectorized indexing.\n        Each element is an array: broadcasting them together gives the shape\n        of the result.\n    """"""\n    key = key.tuple\n\n    n_dim = len([k for k in key if not isinstance(k, integer_types)])\n    i_dim = 0\n    new_key = []\n    for k, size in zip(key, shape):\n        if isinstance(k, integer_types):\n            new_key.append(np.array(k).reshape((1,) * n_dim))\n        else:  # np.ndarray or slice\n            if isinstance(k, slice):\n                k = np.arange(*k.indices(size))\n            assert k.dtype.kind in {""i"", ""u""}\n            shape = [(1,) * i_dim + (k.size,) + (1,) * (n_dim - i_dim - 1)]\n            new_key.append(k.reshape(*shape))\n            i_dim += 1\n    return VectorizedIndexer(tuple(new_key))\n\n\ndef _outer_to_numpy_indexer(key, shape):\n    """"""Convert an OuterIndexer into an indexer for NumPy.\n\n    Parameters\n    ----------\n    key : Basic/OuterIndexer\n        An indexer to convert.\n    shape : tuple\n        Shape of the array subject to the indexing.\n\n    Returns\n    -------\n    tuple\n        Tuple suitable for use to index a NumPy array.\n    """"""\n    if len([k for k in key.tuple if not isinstance(k, slice)]) <= 1:\n        # If there is only one vector and all others are slice,\n        # it can be safely used in mixed basic/advanced indexing.\n        # Boolean index should already be converted to integer array.\n        return key.tuple\n    else:\n        return _outer_to_vectorized_indexer(key, shape).tuple\n\n\ndef _combine_indexers(old_key, shape, new_key):\n    """""" Combine two indexers.\n\n    Parameters\n    ----------\n    old_key: ExplicitIndexer\n        The first indexer for the original array\n    shape: tuple of ints\n        Shape of the original array to be indexed by old_key\n    new_key:\n        The second indexer for indexing original[old_key]\n    """"""\n    if not isinstance(old_key, VectorizedIndexer):\n        old_key = _outer_to_vectorized_indexer(old_key, shape)\n    if len(old_key.tuple) == 0:\n        return new_key\n\n    new_shape = np.broadcast(*old_key.tuple).shape\n    if isinstance(new_key, VectorizedIndexer):\n        new_key = _arrayize_vectorized_indexer(new_key, new_shape)\n    else:\n        new_key = _outer_to_vectorized_indexer(new_key, new_shape)\n\n    return VectorizedIndexer(\n        tuple(o[new_key.tuple] for o in np.broadcast_arrays(*old_key.tuple))\n    )\n\n\n@enum.unique\nclass IndexingSupport(enum.Enum):\n    # for backends that support only basic indexer\n    BASIC = 0\n    # for backends that support basic / outer indexer\n    OUTER = 1\n    # for backends that support outer indexer including at most 1 vector.\n    OUTER_1VECTOR = 2\n    # for backends that support full vectorized indexer.\n    VECTORIZED = 3\n\n\ndef explicit_indexing_adapter(\n    key: ExplicitIndexer,\n    shape: Tuple[int, ...],\n    indexing_support: IndexingSupport,\n    raw_indexing_method: Callable,\n) -> Any:\n    """"""Support explicit indexing by delegating to a raw indexing method.\n\n    Outer and/or vectorized indexers are supported by indexing a second time\n    with a NumPy array.\n\n    Parameters\n    ----------\n    key : ExplicitIndexer\n        Explicit indexing object.\n    shape : Tuple[int, ...]\n        Shape of the indexed array.\n    indexing_support : IndexingSupport enum\n        Form of indexing supported by raw_indexing_method.\n    raw_indexing_method: callable\n        Function (like ndarray.__getitem__) that when called with indexing key\n        in the form of a tuple returns an indexed array.\n\n    Returns\n    -------\n    Indexing result, in the form of a duck numpy-array.\n    """"""\n    raw_key, numpy_indices = decompose_indexer(key, shape, indexing_support)\n    result = raw_indexing_method(raw_key.tuple)\n    if numpy_indices.tuple:\n        # index the loaded np.ndarray\n        result = NumpyIndexingAdapter(np.asarray(result))[numpy_indices]\n    return result\n\n\ndef decompose_indexer(\n    indexer: ExplicitIndexer, shape: Tuple[int, ...], indexing_support: IndexingSupport\n) -> Tuple[ExplicitIndexer, ExplicitIndexer]:\n    if isinstance(indexer, VectorizedIndexer):\n        return _decompose_vectorized_indexer(indexer, shape, indexing_support)\n    if isinstance(indexer, (BasicIndexer, OuterIndexer)):\n        return _decompose_outer_indexer(indexer, shape, indexing_support)\n    raise TypeError(f""unexpected key type: {indexer}"")\n\n\ndef _decompose_slice(key, size):\n    """""" convert a slice to successive two slices. The first slice always has\n    a positive step.\n    """"""\n    start, stop, step = key.indices(size)\n    if step > 0:\n        # If key already has a positive step, use it as is in the backend\n        return key, slice(None)\n    else:\n        # determine stop precisely for step > 1 case\n        # e.g. [98:2:-2] -> [98:3:-2]\n        stop = start + int((stop - start - 1) / step) * step + 1\n        start, stop = stop + 1, start + 1\n        return slice(start, stop, -step), slice(None, None, -1)\n\n\ndef _decompose_vectorized_indexer(\n    indexer: VectorizedIndexer,\n    shape: Tuple[int, ...],\n    indexing_support: IndexingSupport,\n) -> Tuple[ExplicitIndexer, ExplicitIndexer]:\n    """"""\n    Decompose vectorized indexer to the successive two indexers, where the\n    first indexer will be used to index backend arrays, while the second one\n    is used to index loaded on-memory np.ndarray.\n\n    Parameters\n    ----------\n    indexer: VectorizedIndexer\n    indexing_support: one of IndexerSupport entries\n\n    Returns\n    -------\n    backend_indexer: OuterIndexer or BasicIndexer\n    np_indexers: an ExplicitIndexer (VectorizedIndexer / BasicIndexer)\n\n    Notes\n    -----\n    This function is used to realize the vectorized indexing for the backend\n    arrays that only support basic or outer indexing.\n\n    As an example, let us consider to index a few elements from a backend array\n    with a vectorized indexer ([0, 3, 1], [2, 3, 2]).\n    Even if the backend array only supports outer indexing, it is more\n    efficient to load a subslice of the array than loading the entire array,\n\n    >>> backend_indexer = OuterIndexer([0, 1, 3], [2, 3])\n    >>> array = array[backend_indexer]  # load subslice of the array\n    >>> np_indexer = VectorizedIndexer([0, 2, 1], [0, 1, 0])\n    >>> array[np_indexer]  # vectorized indexing for on-memory np.ndarray.\n    """"""\n    assert isinstance(indexer, VectorizedIndexer)\n\n    if indexing_support is IndexingSupport.VECTORIZED:\n        return indexer, BasicIndexer(())\n\n    backend_indexer_elems = []\n    np_indexer_elems = []\n    # convert negative indices\n    indexer_elems = [\n        np.where(k < 0, k + s, k) if isinstance(k, np.ndarray) else k\n        for k, s in zip(indexer.tuple, shape)\n    ]\n\n    for k, s in zip(indexer_elems, shape):\n        if isinstance(k, slice):\n            # If it is a slice, then we will slice it as-is\n            # (but make its step positive) in the backend,\n            # and then use all of it (slice(None)) for the in-memory portion.\n            bk_slice, np_slice = _decompose_slice(k, s)\n            backend_indexer_elems.append(bk_slice)\n            np_indexer_elems.append(np_slice)\n        else:\n            # If it is a (multidimensional) np.ndarray, just pickup the used\n            # keys without duplication and store them as a 1d-np.ndarray.\n            oind, vind = np.unique(k, return_inverse=True)\n            backend_indexer_elems.append(oind)\n            np_indexer_elems.append(vind.reshape(*k.shape))\n\n    backend_indexer = OuterIndexer(tuple(backend_indexer_elems))\n    np_indexer = VectorizedIndexer(tuple(np_indexer_elems))\n\n    if indexing_support is IndexingSupport.OUTER:\n        return backend_indexer, np_indexer\n\n    # If the backend does not support outer indexing,\n    # backend_indexer (OuterIndexer) is also decomposed.\n    backend_indexer1, np_indexer1 = _decompose_outer_indexer(\n        backend_indexer, shape, indexing_support\n    )\n    np_indexer = _combine_indexers(np_indexer1, shape, np_indexer)\n    return backend_indexer1, np_indexer\n\n\ndef _decompose_outer_indexer(\n    indexer: Union[BasicIndexer, OuterIndexer],\n    shape: Tuple[int, ...],\n    indexing_support: IndexingSupport,\n) -> Tuple[ExplicitIndexer, ExplicitIndexer]:\n    """"""\n    Decompose outer indexer to the successive two indexers, where the\n    first indexer will be used to index backend arrays, while the second one\n    is used to index the loaded on-memory np.ndarray.\n\n    Parameters\n    ----------\n    indexer: OuterIndexer or BasicIndexer\n    indexing_support: One of the entries of IndexingSupport\n\n    Returns\n    -------\n    backend_indexer: OuterIndexer or BasicIndexer\n    np_indexers: an ExplicitIndexer (OuterIndexer / BasicIndexer)\n\n    Notes\n    -----\n    This function is used to realize the vectorized indexing for the backend\n    arrays that only support basic or outer indexing.\n\n    As an example, let us consider to index a few elements from a backend array\n    with a orthogonal indexer ([0, 3, 1], [2, 3, 2]).\n    Even if the backend array only supports basic indexing, it is more\n    efficient to load a subslice of the array than loading the entire array,\n\n    >>> backend_indexer = BasicIndexer(slice(0, 3), slice(2, 3))\n    >>> array = array[backend_indexer]  # load subslice of the array\n    >>> np_indexer = OuterIndexer([0, 2, 1], [0, 1, 0])\n    >>> array[np_indexer]  # outer indexing for on-memory np.ndarray.\n    """"""\n    if indexing_support == IndexingSupport.VECTORIZED:\n        return indexer, BasicIndexer(())\n    assert isinstance(indexer, (OuterIndexer, BasicIndexer))\n\n    backend_indexer = []\n    np_indexer = []\n    # make indexer positive\n    pos_indexer = []\n    for k, s in zip(indexer.tuple, shape):\n        if isinstance(k, np.ndarray):\n            pos_indexer.append(np.where(k < 0, k + s, k))\n        elif isinstance(k, integer_types) and k < 0:\n            pos_indexer.append(k + s)\n        else:\n            pos_indexer.append(k)\n    indexer_elems = pos_indexer\n\n    if indexing_support is IndexingSupport.OUTER_1VECTOR:\n        # some backends such as h5py supports only 1 vector in indexers\n        # We choose the most efficient axis\n        gains = [\n            (np.max(k) - np.min(k) + 1.0) / len(np.unique(k))\n            if isinstance(k, np.ndarray)\n            else 0\n            for k in indexer_elems\n        ]\n        array_index = np.argmax(np.array(gains)) if len(gains) > 0 else None\n\n        for i, (k, s) in enumerate(zip(indexer_elems, shape)):\n            if isinstance(k, np.ndarray) and i != array_index:\n                # np.ndarray key is converted to slice that covers the entire\n                # entries of this key.\n                backend_indexer.append(slice(np.min(k), np.max(k) + 1))\n                np_indexer.append(k - np.min(k))\n            elif isinstance(k, np.ndarray):\n                # Remove duplicates and sort them in the increasing order\n                pkey, ekey = np.unique(k, return_inverse=True)\n                backend_indexer.append(pkey)\n                np_indexer.append(ekey)\n            elif isinstance(k, integer_types):\n                backend_indexer.append(k)\n            else:  # slice:  convert positive step slice for backend\n                bk_slice, np_slice = _decompose_slice(k, s)\n                backend_indexer.append(bk_slice)\n                np_indexer.append(np_slice)\n\n        return (OuterIndexer(tuple(backend_indexer)), OuterIndexer(tuple(np_indexer)))\n\n    if indexing_support == IndexingSupport.OUTER:\n        for k, s in zip(indexer_elems, shape):\n            if isinstance(k, slice):\n                # slice:  convert positive step slice for backend\n                bk_slice, np_slice = _decompose_slice(k, s)\n                backend_indexer.append(bk_slice)\n                np_indexer.append(np_slice)\n            elif isinstance(k, integer_types):\n                backend_indexer.append(k)\n            elif isinstance(k, np.ndarray) and (np.diff(k) >= 0).all():\n                backend_indexer.append(k)\n                np_indexer.append(slice(None))\n            else:\n                # Remove duplicates and sort them in the increasing order\n                oind, vind = np.unique(k, return_inverse=True)\n                backend_indexer.append(oind)\n                np_indexer.append(vind.reshape(*k.shape))\n\n        return (OuterIndexer(tuple(backend_indexer)), OuterIndexer(tuple(np_indexer)))\n\n    # basic indexer\n    assert indexing_support == IndexingSupport.BASIC\n\n    for k, s in zip(indexer_elems, shape):\n        if isinstance(k, np.ndarray):\n            # np.ndarray key is converted to slice that covers the entire\n            # entries of this key.\n            backend_indexer.append(slice(np.min(k), np.max(k) + 1))\n            np_indexer.append(k - np.min(k))\n        elif isinstance(k, integer_types):\n            backend_indexer.append(k)\n        else:  # slice:  convert positive step slice for backend\n            bk_slice, np_slice = _decompose_slice(k, s)\n            backend_indexer.append(bk_slice)\n            np_indexer.append(np_slice)\n\n    return (BasicIndexer(tuple(backend_indexer)), OuterIndexer(tuple(np_indexer)))\n\n\ndef _arrayize_vectorized_indexer(indexer, shape):\n    """""" Return an identical vindex but slices are replaced by arrays """"""\n    slices = [v for v in indexer.tuple if isinstance(v, slice)]\n    if len(slices) == 0:\n        return indexer\n\n    arrays = [v for v in indexer.tuple if isinstance(v, np.ndarray)]\n    n_dim = arrays[0].ndim if len(arrays) > 0 else 0\n    i_dim = 0\n    new_key = []\n    for v, size in zip(indexer.tuple, shape):\n        if isinstance(v, np.ndarray):\n            new_key.append(np.reshape(v, v.shape + (1,) * len(slices)))\n        else:  # slice\n            shape = (1,) * (n_dim + i_dim) + (-1,) + (1,) * (len(slices) - i_dim - 1)\n            new_key.append(np.arange(*v.indices(size)).reshape(shape))\n            i_dim += 1\n    return VectorizedIndexer(tuple(new_key))\n\n\ndef _dask_array_with_chunks_hint(array, chunks):\n    """"""Create a dask array using the chunks hint for dimensions of size > 1.""""""\n    import dask.array as da\n\n    if len(chunks) < array.ndim:\n        raise ValueError(""not enough chunks in hint"")\n    new_chunks = []\n    for chunk, size in zip(chunks, array.shape):\n        new_chunks.append(chunk if size > 1 else (1,))\n    return da.from_array(array, new_chunks)\n\n\ndef _logical_any(args):\n    return functools.reduce(operator.or_, args)\n\n\ndef _masked_result_drop_slice(key, data=None):\n\n    key = (k for k in key if not isinstance(k, slice))\n    chunks_hint = getattr(data, ""chunks"", None)\n\n    new_keys = []\n    for k in key:\n        if isinstance(k, np.ndarray):\n            if isinstance(data, dask_array_type):\n                new_keys.append(_dask_array_with_chunks_hint(k, chunks_hint))\n            elif isinstance(data, sparse_array_type):\n                import sparse\n\n                new_keys.append(sparse.COO.from_numpy(k))\n            else:\n                new_keys.append(k)\n        else:\n            new_keys.append(k)\n\n    mask = _logical_any(k == -1 for k in new_keys)\n    return mask\n\n\ndef create_mask(indexer, shape, data=None):\n    """"""Create a mask for indexing with a fill-value.\n\n    Parameters\n    ----------\n    indexer : ExplicitIndexer\n        Indexer with -1 in integer or ndarray value to indicate locations in\n        the result that should be masked.\n    shape : tuple\n        Shape of the array being indexed.\n    data : optional\n        Data for which mask is being created. If data is a dask arrays, its chunks\n        are used as a hint for chunks on the resulting mask. If data is a sparse\n        array, the returned mask is also a sparse array.\n\n    Returns\n    -------\n    mask : bool, np.ndarray, SparseArray or dask.array.Array with dtype=bool\n        Same type as data. Has the same shape as the indexing result.\n    """"""\n    if isinstance(indexer, OuterIndexer):\n        key = _outer_to_vectorized_indexer(indexer, shape).tuple\n        assert not any(isinstance(k, slice) for k in key)\n        mask = _masked_result_drop_slice(key, data)\n\n    elif isinstance(indexer, VectorizedIndexer):\n        key = indexer.tuple\n        base_mask = _masked_result_drop_slice(key, data)\n        slice_shape = tuple(\n            np.arange(*k.indices(size)).size\n            for k, size in zip(key, shape)\n            if isinstance(k, slice)\n        )\n        expanded_mask = base_mask[(Ellipsis,) + (np.newaxis,) * len(slice_shape)]\n        mask = duck_array_ops.broadcast_to(expanded_mask, base_mask.shape + slice_shape)\n\n    elif isinstance(indexer, BasicIndexer):\n        mask = any(k == -1 for k in indexer.tuple)\n\n    else:\n        raise TypeError(""unexpected key type: {}"".format(type(indexer)))\n\n    return mask\n\n\ndef _posify_mask_subindexer(index):\n    """"""Convert masked indices in a flat array to the nearest unmasked index.\n\n    Parameters\n    ----------\n    index : np.ndarray\n        One dimensional ndarray with dtype=int.\n\n    Returns\n    -------\n    np.ndarray\n        One dimensional ndarray with all values equal to -1 replaced by an\n        adjacent non-masked element.\n    """"""\n    masked = index == -1\n    unmasked_locs = np.flatnonzero(~masked)\n    if not unmasked_locs.size:\n        # indexing unmasked_locs is invalid\n        return np.zeros_like(index)\n    masked_locs = np.flatnonzero(masked)\n    prev_value = np.maximum(0, np.searchsorted(unmasked_locs, masked_locs) - 1)\n    new_index = index.copy()\n    new_index[masked_locs] = index[unmasked_locs[prev_value]]\n    return new_index\n\n\ndef posify_mask_indexer(indexer):\n    """"""Convert masked values (-1) in an indexer to nearest unmasked values.\n\n    This routine is useful for dask, where it can be much faster to index\n    adjacent points than arbitrary points from the end of an array.\n\n    Parameters\n    ----------\n    indexer : ExplicitIndexer\n        Input indexer.\n\n    Returns\n    -------\n    ExplicitIndexer\n        Same type of input, with all values in ndarray keys equal to -1\n        replaced by an adjacent non-masked element.\n    """"""\n    key = tuple(\n        _posify_mask_subindexer(k.ravel()).reshape(k.shape)\n        if isinstance(k, np.ndarray)\n        else k\n        for k in indexer.tuple\n    )\n    return type(indexer)(key)\n\n\ndef is_fancy_indexer(indexer: Any) -> bool:\n    """"""Return False if indexer is a int, slice, a 1-dimensional list, or a 0 or\n    1-dimensional ndarray; in all other cases return True\n    """"""\n    if isinstance(indexer, (int, slice)):\n        return False\n    if isinstance(indexer, np.ndarray):\n        return indexer.ndim > 1\n    if isinstance(indexer, list):\n        return bool(indexer) and not isinstance(indexer[0], int)\n    return True\n\n\nclass NumpyIndexingAdapter(ExplicitlyIndexedNDArrayMixin):\n    """"""Wrap a NumPy array to use explicit indexing.""""""\n\n    __slots__ = (""array"",)\n\n    def __init__(self, array):\n        # In NumpyIndexingAdapter we only allow to store bare np.ndarray\n        if not isinstance(array, np.ndarray):\n            raise TypeError(\n                ""NumpyIndexingAdapter only wraps np.ndarray. ""\n                ""Trying to wrap {}"".format(type(array))\n            )\n        self.array = array\n\n    def _indexing_array_and_key(self, key):\n        if isinstance(key, OuterIndexer):\n            array = self.array\n            key = _outer_to_numpy_indexer(key, self.array.shape)\n        elif isinstance(key, VectorizedIndexer):\n            array = nputils.NumpyVIndexAdapter(self.array)\n            key = key.tuple\n        elif isinstance(key, BasicIndexer):\n            array = self.array\n            # We want 0d slices rather than scalars. This is achieved by\n            # appending an ellipsis (see\n            # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#detailed-notes).\n            key = key.tuple + (Ellipsis,)\n        else:\n            raise TypeError(""unexpected key type: {}"".format(type(key)))\n\n        return array, key\n\n    def transpose(self, order):\n        return self.array.transpose(order)\n\n    def __getitem__(self, key):\n        array, key = self._indexing_array_and_key(key)\n        return array[key]\n\n    def __setitem__(self, key, value):\n        array, key = self._indexing_array_and_key(key)\n        try:\n            array[key] = value\n        except ValueError:\n            # More informative exception if read-only view\n            if not array.flags.writeable and not array.flags.owndata:\n                raise ValueError(\n                    ""Assignment destination is a view.  ""\n                    ""Do you want to .copy() array first?""\n                )\n            else:\n                raise\n\n\nclass NdArrayLikeIndexingAdapter(NumpyIndexingAdapter):\n    __slots__ = (""array"",)\n\n    def __init__(self, array):\n        if not hasattr(array, ""__array_function__""):\n            raise TypeError(\n                ""NdArrayLikeIndexingAdapter must wrap an object that ""\n                ""implements the __array_function__ protocol""\n            )\n        self.array = array\n\n\nclass DaskIndexingAdapter(ExplicitlyIndexedNDArrayMixin):\n    """"""Wrap a dask array to support explicit indexing.""""""\n\n    __slots__ = (""array"",)\n\n    def __init__(self, array):\n        """""" This adapter is created in Variable.__getitem__ in\n        Variable._broadcast_indexes.\n        """"""\n        self.array = array\n\n    def __getitem__(self, key):\n\n        if not isinstance(key, VectorizedIndexer):\n            # if possible, short-circuit when keys are effectively slice(None)\n            # This preserves dask name and passes lazy array equivalence checks\n            # (see duck_array_ops.lazy_array_equiv)\n            rewritten_indexer = False\n            new_indexer = []\n            for idim, k in enumerate(key.tuple):\n                if isinstance(k, Iterable) and duck_array_ops.array_equiv(\n                    k, np.arange(self.array.shape[idim])\n                ):\n                    new_indexer.append(slice(None))\n                    rewritten_indexer = True\n                else:\n                    new_indexer.append(k)\n            if rewritten_indexer:\n                key = type(key)(tuple(new_indexer))\n\n        if isinstance(key, BasicIndexer):\n            return self.array[key.tuple]\n        elif isinstance(key, VectorizedIndexer):\n            return self.array.vindex[key.tuple]\n        else:\n            assert isinstance(key, OuterIndexer)\n            key = key.tuple\n            try:\n                return self.array[key]\n            except NotImplementedError:\n                # manual orthogonal indexing.\n                # TODO: port this upstream into dask in a saner way.\n                value = self.array\n                for axis, subkey in reversed(list(enumerate(key))):\n                    value = value[(slice(None),) * axis + (subkey,)]\n                return value\n\n    def __setitem__(self, key, value):\n        raise TypeError(\n            ""this variable\'s data is stored in a dask array, ""\n            ""which does not support item assignment. To ""\n            ""assign to this variable, you must first load it ""\n            ""into memory explicitly using the .load() ""\n            ""method or accessing its .values attribute.""\n        )\n\n    def transpose(self, order):\n        return self.array.transpose(order)\n\n\nclass PandasIndexAdapter(ExplicitlyIndexedNDArrayMixin):\n    """"""Wrap a pandas.Index to preserve dtypes and handle explicit indexing.\n    """"""\n\n    __slots__ = (""array"", ""_dtype"")\n\n    def __init__(self, array: Any, dtype: DTypeLike = None):\n        self.array = utils.safe_cast_to_index(array)\n        if dtype is None:\n            if isinstance(array, pd.PeriodIndex):\n                dtype = np.dtype(""O"")\n            elif hasattr(array, ""categories""):\n                # category isn\'t a real numpy dtype\n                dtype = array.categories.dtype\n            elif not utils.is_valid_numpy_dtype(array.dtype):\n                dtype = np.dtype(""O"")\n            else:\n                dtype = array.dtype\n        else:\n            dtype = np.dtype(dtype)\n        self._dtype = dtype\n\n    @property\n    def dtype(self) -> np.dtype:\n        return self._dtype\n\n    def __array__(self, dtype: DTypeLike = None) -> np.ndarray:\n        if dtype is None:\n            dtype = self.dtype\n        array = self.array\n        if isinstance(array, pd.PeriodIndex):\n            with suppress(AttributeError):\n                # this might not be public API\n                array = array.astype(""object"")\n        return np.asarray(array.values, dtype=dtype)\n\n    @property\n    def shape(self) -> Tuple[int]:\n        return (len(self.array),)\n\n    def __getitem__(\n        self, indexer\n    ) -> Union[NumpyIndexingAdapter, np.ndarray, np.datetime64, np.timedelta64]:\n        key = indexer.tuple\n        if isinstance(key, tuple) and len(key) == 1:\n            # unpack key so it can index a pandas.Index object (pandas.Index\n            # objects don\'t like tuples)\n            (key,) = key\n\n        if getattr(key, ""ndim"", 0) > 1:  # Return np-array if multidimensional\n            return NumpyIndexingAdapter(self.array.values)[indexer]\n\n        result = self.array[key]\n\n        if isinstance(result, pd.Index):\n            result = PandasIndexAdapter(result, dtype=self.dtype)\n        else:\n            # result is a scalar\n            if result is pd.NaT:\n                # work around the impossibility of casting NaT with asarray\n                # note: it probably would be better in general to return\n                # pd.Timestamp rather np.than datetime64 but this is easier\n                # (for now)\n                result = np.datetime64(""NaT"", ""ns"")\n            elif isinstance(result, timedelta):\n                result = np.timedelta64(getattr(result, ""value"", result), ""ns"")\n            elif isinstance(result, pd.Timestamp):\n                # Work around for GH: pydata/xarray#1932 and numpy/numpy#10668\n                # numpy fails to convert pd.Timestamp to np.datetime64[ns]\n                result = np.asarray(result.to_datetime64())\n            elif self.dtype != object:\n                result = np.asarray(result, dtype=self.dtype)\n\n            # as for numpy.ndarray indexing, we always want the result to be\n            # a NumPy array.\n            result = utils.to_0d_array(result)\n\n        return result\n\n    def transpose(self, order) -> pd.Index:\n        return self.array  # self.array should be always one-dimensional\n\n    def __repr__(self) -> str:\n        return ""{}(array={!r}, dtype={!r})"".format(\n            type(self).__name__, self.array, self.dtype\n        )\n\n    def copy(self, deep: bool = True) -> ""PandasIndexAdapter"":\n        # Not the same as just writing `self.array.copy(deep=deep)`, as\n        # shallow copies of the underlying numpy.ndarrays become deep ones\n        # upon pickling\n        # >>> len(pickle.dumps((self.array, self.array)))\n        # 4000281\n        # >>> len(pickle.dumps((self.array, self.array.copy(deep=False))))\n        # 8000341\n        array = self.array.copy(deep=True) if deep else self.array\n        return PandasIndexAdapter(array, self._dtype)\n'"
xarray/core/merge.py,0,"b'from typing import (\n    TYPE_CHECKING,\n    AbstractSet,\n    Any,\n    Dict,\n    Hashable,\n    Iterable,\n    List,\n    Mapping,\n    NamedTuple,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Union,\n)\n\nimport pandas as pd\n\nfrom . import dtypes, pdcompat\nfrom .alignment import deep_align\nfrom .duck_array_ops import lazy_array_equiv\nfrom .utils import Frozen, compat_dict_union, dict_equiv\nfrom .variable import Variable, as_variable, assert_unique_multiindex_level_names\n\nif TYPE_CHECKING:\n    from .coordinates import Coordinates\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    DimsLike = Union[Hashable, Sequence[Hashable]]\n    ArrayLike = Any\n    VariableLike = Union[\n        ArrayLike,\n        Tuple[DimsLike, ArrayLike],\n        Tuple[DimsLike, ArrayLike, Mapping],\n        Tuple[DimsLike, ArrayLike, Mapping, Mapping],\n    ]\n    XarrayValue = Union[DataArray, Variable, VariableLike]\n    DatasetLike = Union[Dataset, Mapping[Hashable, XarrayValue]]\n    CoercibleValue = Union[XarrayValue, pd.Series, pd.DataFrame]\n    CoercibleMapping = Union[Dataset, Mapping[Hashable, CoercibleValue]]\n\n\nPANDAS_TYPES = (pd.Series, pd.DataFrame, pdcompat.Panel)\n\n_VALID_COMPAT = Frozen(\n    {\n        ""identical"": 0,\n        ""equals"": 1,\n        ""broadcast_equals"": 2,\n        ""minimal"": 3,\n        ""no_conflicts"": 4,\n        ""override"": 5,\n    }\n)\n\n\ndef broadcast_dimension_size(variables: List[Variable],) -> Dict[Hashable, int]:\n    """"""Extract dimension sizes from a dictionary of variables.\n\n    Raises ValueError if any dimensions have different sizes.\n    """"""\n    dims: Dict[Hashable, int] = {}\n    for var in variables:\n        for dim, size in zip(var.dims, var.shape):\n            if dim in dims and size != dims[dim]:\n                raise ValueError(""index %r not aligned"" % dim)\n            dims[dim] = size\n    return dims\n\n\nclass MergeError(ValueError):\n    """"""Error class for merge failures due to incompatible arguments.\n    """"""\n\n    # inherits from ValueError for backward compatibility\n    # TODO: move this to an xarray.exceptions module?\n\n\ndef unique_variable(\n    name: Hashable,\n    variables: List[Variable],\n    compat: str = ""broadcast_equals"",\n    equals: bool = None,\n) -> Variable:\n    """"""Return the unique variable from a list of variables or raise MergeError.\n\n    Parameters\n    ----------\n    name : hashable\n        Name for this variable.\n    variables : list of xarray.Variable\n        List of Variable objects, all of which go by the same name in different\n        inputs.\n    compat : {\'identical\', \'equals\', \'broadcast_equals\', \'no_conflicts\', \'override\'}, optional\n        Type of equality check to use.\n    equals: None or bool,\n        corresponding to result of compat test\n\n    Returns\n    -------\n    Variable to use in the result.\n\n    Raises\n    ------\n    MergeError: if any of the variables are not equal.\n    """"""\n    out = variables[0]\n\n    if len(variables) == 1 or compat == ""override"":\n        return out\n\n    combine_method = None\n\n    if compat == ""minimal"":\n        compat = ""broadcast_equals""\n\n    if compat == ""broadcast_equals"":\n        dim_lengths = broadcast_dimension_size(variables)\n        out = out.set_dims(dim_lengths)\n\n    if compat == ""no_conflicts"":\n        combine_method = ""fillna""\n\n    if equals is None:\n        # first check without comparing values i.e. no computes\n        for var in variables[1:]:\n            equals = getattr(out, compat)(var, equiv=lazy_array_equiv)\n            if equals is not True:\n                break\n\n        if equals is None:\n            # now compare values with minimum number of computes\n            out = out.compute()\n            for var in variables[1:]:\n                equals = getattr(out, compat)(var)\n                if not equals:\n                    break\n\n    if not equals:\n        raise MergeError(\n            f""conflicting values for variable {name!r} on objects to be combined. ""\n            ""You can skip this check by specifying compat=\'override\'.""\n        )\n\n    if combine_method:\n        for var in variables[1:]:\n            out = getattr(out, combine_method)(var)\n\n    return out\n\n\ndef _assert_compat_valid(compat):\n    if compat not in _VALID_COMPAT:\n        raise ValueError(\n            ""compat={!r} invalid: must be {}"".format(compat, set(_VALID_COMPAT))\n        )\n\n\nMergeElement = Tuple[Variable, Optional[pd.Index]]\n\n\ndef merge_collected(\n    grouped: Dict[Hashable, List[MergeElement]],\n    prioritized: Mapping[Hashable, MergeElement] = None,\n    compat: str = ""minimal"",\n) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:\n    """"""Merge dicts of variables, while resolving conflicts appropriately.\n\n    Parameters\n    ----------\n\n        Type of equality check to use when checking for conflicts.\n\n    Returns\n    -------\n    Dict with keys taken by the union of keys on list_of_mappings,\n    and Variable values corresponding to those that should be found on the\n    merged result.\n    """"""\n    if prioritized is None:\n        prioritized = {}\n\n    _assert_compat_valid(compat)\n\n    merged_vars: Dict[Hashable, Variable] = {}\n    merged_indexes: Dict[Hashable, pd.Index] = {}\n\n    for name, elements_list in grouped.items():\n        if name in prioritized:\n            variable, index = prioritized[name]\n            merged_vars[name] = variable\n            if index is not None:\n                merged_indexes[name] = index\n        else:\n            indexed_elements = [\n                (variable, index)\n                for variable, index in elements_list\n                if index is not None\n            ]\n\n            if indexed_elements:\n                # TODO(shoyer): consider adjusting this logic. Are we really\n                # OK throwing away variable without an index in favor of\n                # indexed variables, without even checking if values match?\n                variable, index = indexed_elements[0]\n                for _, other_index in indexed_elements[1:]:\n                    if not index.equals(other_index):\n                        raise MergeError(\n                            ""conflicting values for index %r on objects to be ""\n                            ""combined:\\nfirst value: %r\\nsecond value: %r""\n                            % (name, index, other_index)\n                        )\n                if compat == ""identical"":\n                    for other_variable, _ in indexed_elements[1:]:\n                        if not dict_equiv(variable.attrs, other_variable.attrs):\n                            raise MergeError(\n                                ""conflicting attribute values on combined ""\n                                ""variable %r:\\nfirst value: %r\\nsecond value: %r""\n                                % (name, variable.attrs, other_variable.attrs)\n                            )\n                merged_vars[name] = variable\n                merged_indexes[name] = index\n            else:\n                variables = [variable for variable, _ in elements_list]\n                try:\n                    merged_vars[name] = unique_variable(name, variables, compat)\n                except MergeError:\n                    if compat != ""minimal"":\n                        # we need more than ""minimal"" compatibility (for which\n                        # we drop conflicting coordinates)\n                        raise\n\n    return merged_vars, merged_indexes\n\n\ndef collect_variables_and_indexes(\n    list_of_mappings: ""List[DatasetLike]"",\n) -> Dict[Hashable, List[MergeElement]]:\n    """"""Collect variables and indexes from list of mappings of xarray objects.\n\n    Mappings must either be Dataset objects, or have values of one of the\n    following types:\n    - an xarray.Variable\n    - a tuple `(dims, data[, attrs[, encoding]])` that can be converted in\n      an xarray.Variable\n    - or an xarray.DataArray\n    """"""\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    grouped: Dict[Hashable, List[Tuple[Variable, pd.Index]]] = {}\n\n    def append(name, variable, index):\n        values = grouped.setdefault(name, [])\n        values.append((variable, index))\n\n    def append_all(variables, indexes):\n        for name, variable in variables.items():\n            append(name, variable, indexes.get(name))\n\n    for mapping in list_of_mappings:\n        if isinstance(mapping, Dataset):\n            append_all(mapping.variables, mapping.indexes)\n            continue\n\n        for name, variable in mapping.items():\n            if isinstance(variable, DataArray):\n                coords = variable._coords.copy()  # use private API for speed\n                indexes = dict(variable.indexes)\n                # explicitly overwritten variables should take precedence\n                coords.pop(name, None)\n                indexes.pop(name, None)\n                append_all(coords, indexes)\n\n            variable = as_variable(variable, name=name)\n            if variable.dims == (name,):\n                variable = variable.to_index_variable()\n                index = variable.to_index()\n            else:\n                index = None\n            append(name, variable, index)\n\n    return grouped\n\n\ndef collect_from_coordinates(\n    list_of_coords: ""List[Coordinates]"",\n) -> Dict[Hashable, List[MergeElement]]:\n    """"""Collect variables and indexes to be merged from Coordinate objects.""""""\n    grouped: Dict[Hashable, List[Tuple[Variable, pd.Index]]] = {}\n\n    for coords in list_of_coords:\n        variables = coords.variables\n        indexes = coords.indexes\n        for name, variable in variables.items():\n            value = grouped.setdefault(name, [])\n            value.append((variable, indexes.get(name)))\n    return grouped\n\n\ndef merge_coordinates_without_align(\n    objects: ""List[Coordinates]"",\n    prioritized: Mapping[Hashable, MergeElement] = None,\n    exclude_dims: AbstractSet = frozenset(),\n) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:\n    """"""Merge variables/indexes from coordinates without automatic alignments.\n\n    This function is used for merging coordinate from pre-existing xarray\n    objects.\n    """"""\n    collected = collect_from_coordinates(objects)\n\n    if exclude_dims:\n        filtered: Dict[Hashable, List[MergeElement]] = {}\n        for name, elements in collected.items():\n            new_elements = [\n                (variable, index)\n                for variable, index in elements\n                if exclude_dims.isdisjoint(variable.dims)\n            ]\n            if new_elements:\n                filtered[name] = new_elements\n    else:\n        filtered = collected\n\n    return merge_collected(filtered, prioritized)\n\n\ndef determine_coords(\n    list_of_mappings: Iterable[""DatasetLike""],\n) -> Tuple[Set[Hashable], Set[Hashable]]:\n    """"""Given a list of dicts with xarray object values, identify coordinates.\n\n    Parameters\n    ----------\n    list_of_mappings : list of dict or Dataset objects\n        Of the same form as the arguments to expand_variable_dicts.\n\n    Returns\n    -------\n    coord_names : set of variable names\n    noncoord_names : set of variable names\n        All variable found in the input should appear in either the set of\n        coordinate or non-coordinate names.\n    """"""\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    coord_names: Set[Hashable] = set()\n    noncoord_names: Set[Hashable] = set()\n\n    for mapping in list_of_mappings:\n        if isinstance(mapping, Dataset):\n            coord_names.update(mapping.coords)\n            noncoord_names.update(mapping.data_vars)\n        else:\n            for name, var in mapping.items():\n                if isinstance(var, DataArray):\n                    coords = set(var._coords)  # use private API for speed\n                    # explicitly overwritten variables should take precedence\n                    coords.discard(name)\n                    coord_names.update(coords)\n\n    return coord_names, noncoord_names\n\n\ndef coerce_pandas_values(objects: Iterable[""CoercibleMapping""]) -> List[""DatasetLike""]:\n    """"""Convert pandas values found in a list of labeled objects.\n\n    Parameters\n    ----------\n    objects : list of Dataset or mappings\n        The mappings may contain any sort of objects coercible to\n        xarray.Variables as keys, including pandas objects.\n\n    Returns\n    -------\n    List of Dataset or dictionary objects. Any inputs or values in the inputs\n    that were pandas objects have been converted into native xarray objects.\n    """"""\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    out = []\n    for obj in objects:\n        if isinstance(obj, Dataset):\n            variables: ""DatasetLike"" = obj\n        else:\n            variables = {}\n            if isinstance(obj, PANDAS_TYPES):\n                obj = dict(obj.iteritems())\n            for k, v in obj.items():\n                if isinstance(v, PANDAS_TYPES):\n                    v = DataArray(v)\n                variables[k] = v\n        out.append(variables)\n    return out\n\n\ndef _get_priority_vars_and_indexes(\n    objects: List[""DatasetLike""], priority_arg: Optional[int], compat: str = ""equals""\n) -> Dict[Hashable, MergeElement]:\n    """"""Extract the priority variable from a list of mappings.\n\n    We need this method because in some cases the priority argument itself\n    might have conflicting values (e.g., if it is a dict with two DataArray\n    values with conflicting coordinate values).\n\n    Parameters\n    ----------\n    objects : list of dictionaries of variables\n        Dictionaries in which to find the priority variables.\n    priority_arg : int or None\n        Integer object whose variable should take priority.\n    compat : {\'identical\', \'equals\', \'broadcast_equals\', \'no_conflicts\'}, optional\n        Compatibility checks to use when merging variables.\n\n    Returns\n    -------\n    A dictionary of variables and associated indexes (if any) to prioritize.\n    """"""\n    if priority_arg is None:\n        return {}\n\n    collected = collect_variables_and_indexes([objects[priority_arg]])\n    variables, indexes = merge_collected(collected, compat=compat)\n    grouped: Dict[Hashable, MergeElement] = {}\n    for name, variable in variables.items():\n        grouped[name] = (variable, indexes.get(name))\n    return grouped\n\n\ndef merge_coords(\n    objects: Iterable[""CoercibleMapping""],\n    compat: str = ""minimal"",\n    join: str = ""outer"",\n    priority_arg: Optional[int] = None,\n    indexes: Optional[Mapping[Hashable, pd.Index]] = None,\n    fill_value: object = dtypes.NA,\n) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:\n    """"""Merge coordinate variables.\n\n    See merge_core below for argument descriptions. This works similarly to\n    merge_core, except everything we don\'t worry about whether variables are\n    coordinates or not.\n    """"""\n    _assert_compat_valid(compat)\n    coerced = coerce_pandas_values(objects)\n    aligned = deep_align(\n        coerced, join=join, copy=False, indexes=indexes, fill_value=fill_value\n    )\n    collected = collect_variables_and_indexes(aligned)\n    prioritized = _get_priority_vars_and_indexes(aligned, priority_arg, compat=compat)\n    variables, out_indexes = merge_collected(collected, prioritized, compat=compat)\n    assert_unique_multiindex_level_names(variables)\n    return variables, out_indexes\n\n\ndef merge_data_and_coords(data, coords, compat=""broadcast_equals"", join=""outer""):\n    """"""Used in Dataset.__init__.""""""\n    objects = [data, coords]\n    explicit_coords = coords.keys()\n    indexes = dict(_extract_indexes_from_coords(coords))\n    return merge_core(\n        objects, compat, join, explicit_coords=explicit_coords, indexes=indexes\n    )\n\n\ndef _extract_indexes_from_coords(coords):\n    """"""Yields the name & index of valid indexes from a mapping of coords""""""\n    for name, variable in coords.items():\n        variable = as_variable(variable, name=name)\n        if variable.dims == (name,):\n            yield name, variable.to_index()\n\n\ndef assert_valid_explicit_coords(variables, dims, explicit_coords):\n    """"""Validate explicit coordinate names/dims.\n\n    Raise a MergeError if an explicit coord shares a name with a dimension\n    but is comprised of arbitrary dimensions.\n    """"""\n    for coord_name in explicit_coords:\n        if coord_name in dims and variables[coord_name].dims != (coord_name,):\n            raise MergeError(\n                ""coordinate %s shares a name with a dataset dimension, but is ""\n                ""not a 1D variable along that dimension. This is disallowed ""\n                ""by the xarray data model."" % coord_name\n            )\n\n\ndef merge_attrs(variable_attrs, combine_attrs):\n    """"""Combine attributes from different variables according to combine_attrs\n    """"""\n    if not variable_attrs:\n        # no attributes to merge\n        return None\n\n    if combine_attrs == ""drop"":\n        return {}\n    elif combine_attrs == ""override"":\n        return variable_attrs[0]\n    elif combine_attrs == ""no_conflicts"":\n        result = dict(variable_attrs[0])\n        for attrs in variable_attrs[1:]:\n            try:\n                result = compat_dict_union(result, attrs)\n            except ValueError:\n                raise MergeError(\n                    ""combine_attrs=\'no_conflicts\', but some values are not ""\n                    ""the same. Merging %s with %s"" % (str(result), str(attrs))\n                )\n        return result\n    elif combine_attrs == ""identical"":\n        result = dict(variable_attrs[0])\n        for attrs in variable_attrs[1:]:\n            if not dict_equiv(result, attrs):\n                raise MergeError(\n                    ""combine_attrs=\'identical\', but attrs differ. First is %s ""\n                    "", other is %s."" % (str(result), str(attrs))\n                )\n        return result\n    else:\n        raise ValueError(""Unrecognised value for combine_attrs=%s"" % combine_attrs)\n\n\nclass _MergeResult(NamedTuple):\n    variables: Dict[Hashable, Variable]\n    coord_names: Set[Hashable]\n    dims: Dict[Hashable, int]\n    indexes: Dict[Hashable, pd.Index]\n    attrs: Dict[Hashable, Any]\n\n\ndef merge_core(\n    objects: Iterable[""CoercibleMapping""],\n    compat: str = ""broadcast_equals"",\n    join: str = ""outer"",\n    combine_attrs: Optional[str] = ""override"",\n    priority_arg: Optional[int] = None,\n    explicit_coords: Optional[Sequence] = None,\n    indexes: Optional[Mapping[Hashable, pd.Index]] = None,\n    fill_value: object = dtypes.NA,\n) -> _MergeResult:\n    """"""Core logic for merging labeled objects.\n\n    This is not public API.\n\n    Parameters\n    ----------\n    objects : list of mappings\n        All values must be convertable to labeled arrays.\n    compat : {\'identical\', \'equals\', \'broadcast_equals\', \'no_conflicts\', \'override\'}, optional\n        Compatibility checks to use when merging variables.\n    join : {\'outer\', \'inner\', \'left\', \'right\'}, optional\n        How to combine objects with different indexes.\n    combine_attrs : {\'drop\', \'identical\', \'no_conflicts\', \'override\'}, optional\n        How to combine attributes of objects\n    priority_arg : integer, optional\n        Optional argument in `objects` that takes precedence over the others.\n    explicit_coords : set, optional\n        An explicit list of variables from `objects` that are coordinates.\n    indexes : dict, optional\n        Dictionary with values given by pandas.Index objects.\n    fill_value : scalar, optional\n        Value to use for newly missing values\n\n    Returns\n    -------\n    variables : dict\n        Dictionary of Variable objects.\n    coord_names : set\n        Set of coordinate names.\n    dims : dict\n        Dictionary mapping from dimension names to sizes.\n    attrs : dict\n        Dictionary of attributes\n\n    Raises\n    ------\n    MergeError if the merge cannot be done successfully.\n    """"""\n    from .dataarray import DataArray\n    from .dataset import Dataset, calculate_dimensions\n\n    _assert_compat_valid(compat)\n\n    coerced = coerce_pandas_values(objects)\n    aligned = deep_align(\n        coerced, join=join, copy=False, indexes=indexes, fill_value=fill_value\n    )\n    collected = collect_variables_and_indexes(aligned)\n\n    prioritized = _get_priority_vars_and_indexes(aligned, priority_arg, compat=compat)\n    variables, out_indexes = merge_collected(collected, prioritized, compat=compat)\n    assert_unique_multiindex_level_names(variables)\n\n    dims = calculate_dimensions(variables)\n\n    coord_names, noncoord_names = determine_coords(coerced)\n    if explicit_coords is not None:\n        assert_valid_explicit_coords(variables, dims, explicit_coords)\n        coord_names.update(explicit_coords)\n    for dim, size in dims.items():\n        if dim in variables:\n            coord_names.add(dim)\n    ambiguous_coords = coord_names.intersection(noncoord_names)\n    if ambiguous_coords:\n        raise MergeError(\n            ""unable to determine if these variables should be ""\n            ""coordinates or not in the merged result: %s"" % ambiguous_coords\n        )\n\n    attrs = merge_attrs(\n        [\n            var.attrs\n            for var in coerced\n            if isinstance(var, Dataset) or isinstance(var, DataArray)\n        ],\n        combine_attrs,\n    )\n\n    return _MergeResult(variables, coord_names, dims, out_indexes, attrs)\n\n\ndef merge(\n    objects: Iterable[Union[""DataArray"", ""CoercibleMapping""]],\n    compat: str = ""no_conflicts"",\n    join: str = ""outer"",\n    fill_value: object = dtypes.NA,\n    combine_attrs: str = ""drop"",\n) -> ""Dataset"":\n    """"""Merge any number of xarray objects into a single Dataset as variables.\n\n    Parameters\n    ----------\n    objects : Iterable[Union[xarray.Dataset, xarray.DataArray, dict]]\n        Merge together all variables from these objects. If any of them are\n        DataArray objects, they must have a name.\n    compat : {\'identical\', \'equals\', \'broadcast_equals\', \'no_conflicts\', \'override\'}, optional\n        String indicating how to compare variables of the same name for\n        potential conflicts:\n\n        - \'broadcast_equals\': all values must be equal when variables are\n          broadcast against each other to ensure common dimensions.\n        - \'equals\': all values and dimensions must be the same.\n        - \'identical\': all values, dimensions and attributes must be the\n          same.\n        - \'no_conflicts\': only values which are not null in both datasets\n          must be equal. The returned dataset then contains the combination\n          of all non-null values.\n        - \'override\': skip comparing and pick variable from first dataset\n    join : {\'outer\', \'inner\', \'left\', \'right\', \'exact\'}, optional\n        String indicating how to combine differing indexes in objects.\n\n        - \'outer\': use the union of object indexes\n        - \'inner\': use the intersection of object indexes\n        - \'left\': use indexes from the first object with each dimension\n        - \'right\': use indexes from the last object with each dimension\n        - \'exact\': instead of aligning, raise `ValueError` when indexes to be\n          aligned are not equal\n        - \'override\': if indexes are of same size, rewrite indexes to be\n          those of the first object with that dimension. Indexes for the same\n          dimension must have the same size in all objects.\n    fill_value : scalar, optional\n        Value to use for newly missing values\n    combine_attrs : {\'drop\', \'identical\', \'no_conflicts\', \'override\'},\n                    default \'drop\'\n        String indicating how to combine attrs of the objects being merged:\n\n        - \'drop\': empty attrs on returned Dataset.\n        - \'identical\': all attrs must be the same on every object.\n        - \'no_conflicts\': attrs from all objects are combined, any that have\n          the same name must also have the same value.\n        - \'override\': skip comparing and copy attrs from the first dataset to\n          the result.\n\n    Returns\n    -------\n    Dataset\n        Dataset with combined variables from each object.\n\n    Examples\n    --------\n    >>> import xarray as xr\n    >>> x = xr.DataArray(\n    ...     [[1.0, 2.0], [3.0, 5.0]],\n    ...     dims=(""lat"", ""lon""),\n    ...     coords={""lat"": [35.0, 40.0], ""lon"": [100.0, 120.0]},\n    ...     name=""var1"",\n    ... )\n    >>> y = xr.DataArray(\n    ...     [[5.0, 6.0], [7.0, 8.0]],\n    ...     dims=(""lat"", ""lon""),\n    ...     coords={""lat"": [35.0, 42.0], ""lon"": [100.0, 150.0]},\n    ...     name=""var2"",\n    ... )\n    >>> z = xr.DataArray(\n    ...     [[0.0, 3.0], [4.0, 9.0]],\n    ...     dims=(""time"", ""lon""),\n    ...     coords={""time"": [30.0, 60.0], ""lon"": [100.0, 150.0]},\n    ...     name=""var3"",\n    ... )\n\n    >>> x\n    <xarray.DataArray \'var1\' (lat: 2, lon: 2)>\n    array([[1., 2.],\n           [3., 5.]])\n    Coordinates:\n    * lat      (lat) float64 35.0 40.0\n    * lon      (lon) float64 100.0 120.0\n\n    >>> y\n    <xarray.DataArray \'var2\' (lat: 2, lon: 2)>\n    array([[5., 6.],\n           [7., 8.]])\n    Coordinates:\n    * lat      (lat) float64 35.0 42.0\n    * lon      (lon) float64 100.0 150.0\n\n    >>> z\n    <xarray.DataArray \'var3\' (time: 2, lon: 2)>\n    array([[0., 3.],\n           [4., 9.]])\n    Coordinates:\n    * time     (time) float64 30.0 60.0\n    * lon      (lon) float64 100.0 150.0\n\n    >>> xr.merge([x, y, z])\n    <xarray.Dataset>\n    Dimensions:  (lat: 3, lon: 3, time: 2)\n    Coordinates:\n    * lat      (lat) float64 35.0 40.0 42.0\n    * lon      (lon) float64 100.0 120.0 150.0\n    * time     (time) float64 30.0 60.0\n    Data variables:\n        var1     (lat, lon) float64 1.0 2.0 nan 3.0 5.0 nan nan nan nan\n        var2     (lat, lon) float64 5.0 nan 6.0 nan nan nan 7.0 nan 8.0\n        var3     (time, lon) float64 0.0 nan 3.0 4.0 nan 9.0\n\n    >>> xr.merge([x, y, z], compat=""identical"")\n    <xarray.Dataset>\n    Dimensions:  (lat: 3, lon: 3, time: 2)\n    Coordinates:\n    * lat      (lat) float64 35.0 40.0 42.0\n    * lon      (lon) float64 100.0 120.0 150.0\n    * time     (time) float64 30.0 60.0\n    Data variables:\n        var1     (lat, lon) float64 1.0 2.0 nan 3.0 5.0 nan nan nan nan\n        var2     (lat, lon) float64 5.0 nan 6.0 nan nan nan 7.0 nan 8.0\n        var3     (time, lon) float64 0.0 nan 3.0 4.0 nan 9.0\n\n    >>> xr.merge([x, y, z], compat=""equals"")\n    <xarray.Dataset>\n    Dimensions:  (lat: 3, lon: 3, time: 2)\n    Coordinates:\n    * lat      (lat) float64 35.0 40.0 42.0\n    * lon      (lon) float64 100.0 120.0 150.0\n    * time     (time) float64 30.0 60.0\n    Data variables:\n        var1     (lat, lon) float64 1.0 2.0 nan 3.0 5.0 nan nan nan nan\n        var2     (lat, lon) float64 5.0 nan 6.0 nan nan nan 7.0 nan 8.0\n        var3     (time, lon) float64 0.0 nan 3.0 4.0 nan 9.0\n\n    >>> xr.merge([x, y, z], compat=""equals"", fill_value=-999.0)\n    <xarray.Dataset>\n    Dimensions:  (lat: 3, lon: 3, time: 2)\n    Coordinates:\n    * lat      (lat) float64 35.0 40.0 42.0\n    * lon      (lon) float64 100.0 120.0 150.0\n    * time     (time) float64 30.0 60.0\n    Data variables:\n        var1     (lat, lon) float64 1.0 2.0 -999.0 3.0 ... -999.0 -999.0 -999.0\n        var2     (lat, lon) float64 5.0 -999.0 6.0 -999.0 ... -999.0 7.0 -999.0 8.0\n        var3     (time, lon) float64 0.0 -999.0 3.0 4.0 -999.0 9.0\n\n    >>> xr.merge([x, y, z], join=""override"")\n    <xarray.Dataset>\n    Dimensions:  (lat: 2, lon: 2, time: 2)\n    Coordinates:\n    * lat      (lat) float64 35.0 40.0\n    * lon      (lon) float64 100.0 120.0\n    * time     (time) float64 30.0 60.0\n    Data variables:\n        var1     (lat, lon) float64 1.0 2.0 3.0 5.0\n        var2     (lat, lon) float64 5.0 6.0 7.0 8.0\n        var3     (time, lon) float64 0.0 3.0 4.0 9.0\n\n    >>> xr.merge([x, y, z], join=""inner"")\n    <xarray.Dataset>\n    Dimensions:  (lat: 1, lon: 1, time: 2)\n    Coordinates:\n    * lat      (lat) float64 35.0\n    * lon      (lon) float64 100.0\n    * time     (time) float64 30.0 60.0\n    Data variables:\n        var1     (lat, lon) float64 1.0\n        var2     (lat, lon) float64 5.0\n        var3     (time, lon) float64 0.0 4.0\n\n    >>> xr.merge([x, y, z], compat=""identical"", join=""inner"")\n    <xarray.Dataset>\n    Dimensions:  (lat: 1, lon: 1, time: 2)\n    Coordinates:\n    * lat      (lat) float64 35.0\n    * lon      (lon) float64 100.0\n    * time     (time) float64 30.0 60.0\n    Data variables:\n        var1     (lat, lon) float64 1.0\n        var2     (lat, lon) float64 5.0\n        var3     (time, lon) float64 0.0 4.0\n\n    >>> xr.merge([x, y, z], compat=""broadcast_equals"", join=""outer"")\n    <xarray.Dataset>\n    Dimensions:  (lat: 3, lon: 3, time: 2)\n    Coordinates:\n    * lat      (lat) float64 35.0 40.0 42.0\n    * lon      (lon) float64 100.0 120.0 150.0\n    * time     (time) float64 30.0 60.0\n    Data variables:\n        var1     (lat, lon) float64 1.0 2.0 nan 3.0 5.0 nan nan nan nan\n        var2     (lat, lon) float64 5.0 nan 6.0 nan nan nan 7.0 nan 8.0\n        var3     (time, lon) float64 0.0 nan 3.0 4.0 nan 9.0\n\n    >>> xr.merge([x, y, z], join=""exact"")\n    Traceback (most recent call last):\n    ...\n    ValueError: indexes along dimension \'lat\' are not equal\n\n    Raises\n    ------\n    xarray.MergeError\n        If any variables with the same name have conflicting values.\n\n    See also\n    --------\n    concat\n    """"""\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    dict_like_objects = []\n    for obj in objects:\n        if not isinstance(obj, (DataArray, Dataset, dict)):\n            raise TypeError(\n                ""objects must be an iterable containing only ""\n                ""Dataset(s), DataArray(s), and dictionaries.""\n            )\n\n        obj = obj.to_dataset(promote_attrs=True) if isinstance(obj, DataArray) else obj\n        dict_like_objects.append(obj)\n\n    merge_result = merge_core(\n        dict_like_objects,\n        compat,\n        join,\n        combine_attrs=combine_attrs,\n        fill_value=fill_value,\n    )\n    merged = Dataset._construct_direct(**merge_result._asdict())\n    return merged\n\n\ndef dataset_merge_method(\n    dataset: ""Dataset"",\n    other: ""CoercibleMapping"",\n    overwrite_vars: Union[Hashable, Iterable[Hashable]],\n    compat: str,\n    join: str,\n    fill_value: Any,\n) -> _MergeResult:\n    """"""Guts of the Dataset.merge method.\n    """"""\n    # we are locked into supporting overwrite_vars for the Dataset.merge\n    # method due for backwards compatibility\n    # TODO: consider deprecating it?\n\n    if isinstance(overwrite_vars, Iterable) and not isinstance(overwrite_vars, str):\n        overwrite_vars = set(overwrite_vars)\n    else:\n        overwrite_vars = {overwrite_vars}\n\n    if not overwrite_vars:\n        objs = [dataset, other]\n        priority_arg = None\n    elif overwrite_vars == set(other):\n        objs = [dataset, other]\n        priority_arg = 1\n    else:\n        other_overwrite: Dict[Hashable, CoercibleValue] = {}\n        other_no_overwrite: Dict[Hashable, CoercibleValue] = {}\n        for k, v in other.items():\n            if k in overwrite_vars:\n                other_overwrite[k] = v\n            else:\n                other_no_overwrite[k] = v\n        objs = [dataset, other_no_overwrite, other_overwrite]\n        priority_arg = 2\n\n    return merge_core(\n        objs, compat, join, priority_arg=priority_arg, fill_value=fill_value\n    )\n\n\ndef dataset_update_method(\n    dataset: ""Dataset"", other: ""CoercibleMapping""\n) -> _MergeResult:\n    """"""Guts of the Dataset.update method.\n\n    This drops a duplicated coordinates from `other` if `other` is not an\n    `xarray.Dataset`, e.g., if it\'s a dict with DataArray values (GH2068,\n    GH2180).\n    """"""\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    if not isinstance(other, Dataset):\n        other = dict(other)\n        for key, value in other.items():\n            if isinstance(value, DataArray):\n                # drop conflicting coordinates\n                coord_names = [\n                    c\n                    for c in value.coords\n                    if c not in value.dims and c in dataset.coords\n                ]\n                if coord_names:\n                    other[key] = value.drop_vars(coord_names)\n\n    return merge_core(\n        [dataset, other],\n        priority_arg=1,\n        indexes=dataset.indexes,\n        combine_attrs=""override"",\n    )\n'"
xarray/core/missing.py,20,"b'import datetime as dt\nimport warnings\nfrom functools import partial\nfrom numbers import Number\nfrom typing import Any, Callable, Dict, Hashable, Sequence, Union\n\nimport numpy as np\nimport pandas as pd\n\nfrom . import utils\nfrom .common import _contains_datetime_like_objects, ones_like\nfrom .computation import apply_ufunc\nfrom .duck_array_ops import dask_array_type, datetime_to_numeric, timedelta_to_numeric\nfrom .options import _get_keep_attrs\nfrom .utils import OrderedSet, is_scalar\nfrom .variable import Variable, broadcast_variables\n\n\ndef _get_nan_block_lengths(obj, dim: Hashable, index: Variable):\n    """"""\n    Return an object where each NaN element in \'obj\' is replaced by the\n    length of the gap the element is in.\n    """"""\n\n    # make variable so that we get broadcasting for free\n    index = Variable([dim], index)\n\n    # algorithm from https://github.com/pydata/xarray/pull/3302#discussion_r324707072\n    arange = ones_like(obj) * index\n    valid = obj.notnull()\n    valid_arange = arange.where(valid)\n    cumulative_nans = valid_arange.ffill(dim=dim).fillna(index[0])\n\n    nan_block_lengths = (\n        cumulative_nans.diff(dim=dim, label=""upper"")\n        .reindex({dim: obj[dim]})\n        .where(valid)\n        .bfill(dim=dim)\n        .where(~valid, 0)\n        .fillna(index[-1] - valid_arange.max())\n    )\n\n    return nan_block_lengths\n\n\nclass BaseInterpolator:\n    """"""Generic interpolator class for normalizing interpolation methods\n    """"""\n\n    cons_kwargs: Dict[str, Any]\n    call_kwargs: Dict[str, Any]\n    f: Callable\n    method: str\n\n    def __call__(self, x):\n        return self.f(x, **self.call_kwargs)\n\n    def __repr__(self):\n        return ""{type}: method={method}"".format(\n            type=self.__class__.__name__, method=self.method\n        )\n\n\nclass NumpyInterpolator(BaseInterpolator):\n    """"""One-dimensional linear interpolation.\n\n    See Also\n    --------\n    numpy.interp\n    """"""\n\n    def __init__(self, xi, yi, method=""linear"", fill_value=None, period=None):\n\n        if method != ""linear"":\n            raise ValueError(""only method `linear` is valid for the NumpyInterpolator"")\n\n        self.method = method\n        self.f = np.interp\n        self.cons_kwargs = {}\n        self.call_kwargs = {""period"": period}\n\n        self._xi = xi\n        self._yi = yi\n\n        if fill_value is None:\n            self._left = np.nan\n            self._right = np.nan\n        elif isinstance(fill_value, Sequence) and len(fill_value) == 2:\n            self._left = fill_value[0]\n            self._right = fill_value[1]\n        elif is_scalar(fill_value):\n            self._left = fill_value\n            self._right = fill_value\n        else:\n            raise ValueError(""%s is not a valid fill_value"" % fill_value)\n\n    def __call__(self, x):\n        return self.f(\n            x,\n            self._xi,\n            self._yi,\n            left=self._left,\n            right=self._right,\n            **self.call_kwargs,\n        )\n\n\nclass ScipyInterpolator(BaseInterpolator):\n    """"""Interpolate a 1-D function using Scipy interp1d\n\n    See Also\n    --------\n    scipy.interpolate.interp1d\n    """"""\n\n    def __init__(\n        self,\n        xi,\n        yi,\n        method=None,\n        fill_value=None,\n        assume_sorted=True,\n        copy=False,\n        bounds_error=False,\n        order=None,\n        **kwargs,\n    ):\n        from scipy.interpolate import interp1d\n\n        if method is None:\n            raise ValueError(\n                ""method is a required argument, please supply a ""\n                ""valid scipy.inter1d method (kind)""\n            )\n\n        if method == ""polynomial"":\n            if order is None:\n                raise ValueError(""order is required when method=polynomial"")\n            method = order\n\n        self.method = method\n\n        self.cons_kwargs = kwargs\n        self.call_kwargs = {}\n\n        if fill_value is None and method == ""linear"":\n            fill_value = np.nan, np.nan\n        elif fill_value is None:\n            fill_value = np.nan\n\n        self.f = interp1d(\n            xi,\n            yi,\n            kind=self.method,\n            fill_value=fill_value,\n            bounds_error=False,\n            assume_sorted=assume_sorted,\n            copy=copy,\n            **self.cons_kwargs,\n        )\n\n\nclass SplineInterpolator(BaseInterpolator):\n    """"""One-dimensional smoothing spline fit to a given set of data points.\n\n    See Also\n    --------\n    scipy.interpolate.UnivariateSpline\n    """"""\n\n    def __init__(\n        self,\n        xi,\n        yi,\n        method=""spline"",\n        fill_value=None,\n        order=3,\n        nu=0,\n        ext=None,\n        **kwargs,\n    ):\n        from scipy.interpolate import UnivariateSpline\n\n        if method != ""spline"":\n            raise ValueError(""only method `spline` is valid for the SplineInterpolator"")\n\n        self.method = method\n        self.cons_kwargs = kwargs\n        self.call_kwargs = {""nu"": nu, ""ext"": ext}\n\n        if fill_value is not None:\n            raise ValueError(""SplineInterpolator does not support fill_value"")\n\n        self.f = UnivariateSpline(xi, yi, k=order, **self.cons_kwargs)\n\n\ndef _apply_over_vars_with_dim(func, self, dim=None, **kwargs):\n    """"""Wrapper for datasets\n    """"""\n    ds = type(self)(coords=self.coords, attrs=self.attrs)\n\n    for name, var in self.data_vars.items():\n        if dim in var.dims:\n            ds[name] = func(var, dim=dim, **kwargs)\n        else:\n            ds[name] = var\n\n    return ds\n\n\ndef get_clean_interp_index(\n    arr, dim: Hashable, use_coordinate: Union[str, bool] = True, strict: bool = True\n):\n    """"""Return index to use for x values in interpolation or curve fitting.\n\n    Parameters\n    ----------\n    arr : DataArray\n      Array to interpolate or fit to a curve.\n    dim : str\n      Name of dimension along which to fit.\n    use_coordinate : str or bool\n      If use_coordinate is True, the coordinate that shares the name of the\n      dimension along which interpolation is being performed will be used as the\n      x values. If False, the x values are set as an equally spaced sequence.\n    strict : bool\n      Whether to raise errors if the index is either non-unique or non-monotonic (default).\n\n    Returns\n    -------\n    Variable\n      Numerical values for the x-coordinates.\n\n    Notes\n    -----\n    If indexing is along the time dimension, datetime coordinates are converted\n    to time deltas with respect to 1970-01-01.\n    """"""\n\n    # Question: If use_coordinate is a string, what role does `dim` play?\n    from xarray.coding.cftimeindex import CFTimeIndex\n\n    if use_coordinate is False:\n        axis = arr.get_axis_num(dim)\n        return np.arange(arr.shape[axis], dtype=np.float64)\n\n    if use_coordinate is True:\n        index = arr.get_index(dim)\n\n    else:  # string\n        index = arr.coords[use_coordinate]\n        if index.ndim != 1:\n            raise ValueError(\n                f""Coordinates used for interpolation must be 1D, ""\n                f""{use_coordinate} is {index.ndim}D.""\n            )\n        index = index.to_index()\n\n    # TODO: index.name is None for multiindexes\n    # set name for nice error messages below\n    if isinstance(index, pd.MultiIndex):\n        index.name = dim\n\n    if strict:\n        if not index.is_monotonic:\n            raise ValueError(f""Index {index.name!r} must be monotonically increasing"")\n\n        if not index.is_unique:\n            raise ValueError(f""Index {index.name!r} has duplicate values"")\n\n    # Special case for non-standard calendar indexes\n    # Numerical datetime values are defined with respect to 1970-01-01T00:00:00 in units of nanoseconds\n    if isinstance(index, (CFTimeIndex, pd.DatetimeIndex)):\n        offset = type(index[0])(1970, 1, 1)\n        if isinstance(index, CFTimeIndex):\n            index = index.values\n        index = Variable(\n            data=datetime_to_numeric(index, offset=offset, datetime_unit=""ns""),\n            dims=(dim,),\n        )\n\n    # raise if index cannot be cast to a float (e.g. MultiIndex)\n    try:\n        index = index.values.astype(np.float64)\n    except (TypeError, ValueError):\n        # pandas raises a TypeError\n        # xarray/numpy raise a ValueError\n        raise TypeError(\n            f""Index {index.name!r} must be castable to float64 to support ""\n            f""interpolation or curve fitting, got {type(index).__name__}.""\n        )\n\n    return index\n\n\ndef interp_na(\n    self,\n    dim: Hashable = None,\n    use_coordinate: Union[bool, str] = True,\n    method: str = ""linear"",\n    limit: int = None,\n    max_gap: Union[int, float, str, pd.Timedelta, np.timedelta64, dt.timedelta] = None,\n    keep_attrs: bool = None,\n    **kwargs,\n):\n    """"""Interpolate values according to different methods.\n    """"""\n    from xarray.coding.cftimeindex import CFTimeIndex\n\n    if dim is None:\n        raise NotImplementedError(""dim is a required argument"")\n\n    if limit is not None:\n        valids = _get_valid_fill_mask(self, dim, limit)\n\n    if max_gap is not None:\n        max_type = type(max_gap).__name__\n        if not is_scalar(max_gap):\n            raise ValueError(""max_gap must be a scalar."")\n\n        if (\n            dim in self.indexes\n            and isinstance(self.indexes[dim], (pd.DatetimeIndex, CFTimeIndex))\n            and use_coordinate\n        ):\n            # Convert to float\n            max_gap = timedelta_to_numeric(max_gap)\n\n        if not use_coordinate:\n            if not isinstance(max_gap, (Number, np.number)):\n                raise TypeError(\n                    f""Expected integer or floating point max_gap since use_coordinate=False. Received {max_type}.""\n                )\n\n    # method\n    index = get_clean_interp_index(self, dim, use_coordinate=use_coordinate)\n    interp_class, kwargs = _get_interpolator(method, **kwargs)\n    interpolator = partial(func_interpolate_na, interp_class, **kwargs)\n\n    if keep_attrs is None:\n        keep_attrs = _get_keep_attrs(default=True)\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", ""overflow"", RuntimeWarning)\n        warnings.filterwarnings(""ignore"", ""invalid value"", RuntimeWarning)\n        arr = apply_ufunc(\n            interpolator,\n            self,\n            index,\n            input_core_dims=[[dim], [dim]],\n            output_core_dims=[[dim]],\n            output_dtypes=[self.dtype],\n            dask=""parallelized"",\n            vectorize=True,\n            keep_attrs=keep_attrs,\n        ).transpose(*self.dims)\n\n    if limit is not None:\n        arr = arr.where(valids)\n\n    if max_gap is not None:\n        if dim not in self.coords:\n            raise NotImplementedError(\n                ""max_gap not implemented for unlabeled coordinates yet.""\n            )\n        nan_block_lengths = _get_nan_block_lengths(self, dim, index)\n        arr = arr.where(nan_block_lengths <= max_gap)\n\n    return arr\n\n\ndef func_interpolate_na(interpolator, y, x, **kwargs):\n    """"""helper function to apply interpolation along 1 dimension""""""\n    # reversed arguments are so that attrs are preserved from da, not index\n    # it would be nice if this wasn\'t necessary, works around:\n    # ""ValueError: assignment destination is read-only"" in assignment below\n    out = y.copy()\n\n    nans = pd.isnull(y)\n    nonans = ~nans\n\n    # fast track for no-nans and all-nans cases\n    n_nans = nans.sum()\n    if n_nans == 0 or n_nans == len(y):\n        return y\n\n    f = interpolator(x[nonans], y[nonans], **kwargs)\n    out[nans] = f(x[nans])\n    return out\n\n\ndef _bfill(arr, n=None, axis=-1):\n    """"""inverse of ffill""""""\n    import bottleneck as bn\n\n    arr = np.flip(arr, axis=axis)\n\n    # fill\n    arr = bn.push(arr, axis=axis, n=n)\n\n    # reverse back to original\n    return np.flip(arr, axis=axis)\n\n\ndef ffill(arr, dim=None, limit=None):\n    """"""forward fill missing values""""""\n    import bottleneck as bn\n\n    axis = arr.get_axis_num(dim)\n\n    # work around for bottleneck 178\n    _limit = limit if limit is not None else arr.shape[axis]\n\n    return apply_ufunc(\n        bn.push,\n        arr,\n        dask=""parallelized"",\n        keep_attrs=True,\n        output_dtypes=[arr.dtype],\n        kwargs=dict(n=_limit, axis=axis),\n    ).transpose(*arr.dims)\n\n\ndef bfill(arr, dim=None, limit=None):\n    """"""backfill missing values""""""\n    axis = arr.get_axis_num(dim)\n\n    # work around for bottleneck 178\n    _limit = limit if limit is not None else arr.shape[axis]\n\n    return apply_ufunc(\n        _bfill,\n        arr,\n        dask=""parallelized"",\n        keep_attrs=True,\n        output_dtypes=[arr.dtype],\n        kwargs=dict(n=_limit, axis=axis),\n    ).transpose(*arr.dims)\n\n\ndef _get_interpolator(method, vectorizeable_only=False, **kwargs):\n    """"""helper function to select the appropriate interpolator class\n\n    returns interpolator class and keyword arguments for the class\n    """"""\n    interp1d_methods = [\n        ""linear"",\n        ""nearest"",\n        ""zero"",\n        ""slinear"",\n        ""quadratic"",\n        ""cubic"",\n        ""polynomial"",\n    ]\n    valid_methods = interp1d_methods + [\n        ""barycentric"",\n        ""krog"",\n        ""pchip"",\n        ""spline"",\n        ""akima"",\n    ]\n\n    has_scipy = True\n    try:\n        from scipy import interpolate\n    except ImportError:\n        has_scipy = False\n\n    # prioritize scipy.interpolate\n    if (\n        method == ""linear""\n        and not kwargs.get(""fill_value"", None) == ""extrapolate""\n        and not vectorizeable_only\n    ):\n        kwargs.update(method=method)\n        interp_class = NumpyInterpolator\n\n    elif method in valid_methods:\n        if not has_scipy:\n            raise ImportError(""Interpolation with method `%s` requires scipy"" % method)\n\n        if method in interp1d_methods:\n            kwargs.update(method=method)\n            interp_class = ScipyInterpolator\n        elif vectorizeable_only:\n            raise ValueError(\n                ""{} is not a vectorizeable interpolator. ""\n                ""Available methods are {}"".format(method, interp1d_methods)\n            )\n        elif method == ""barycentric"":\n            interp_class = interpolate.BarycentricInterpolator\n        elif method == ""krog"":\n            interp_class = interpolate.KroghInterpolator\n        elif method == ""pchip"":\n            interp_class = interpolate.PchipInterpolator\n        elif method == ""spline"":\n            kwargs.update(method=method)\n            interp_class = SplineInterpolator\n        elif method == ""akima"":\n            interp_class = interpolate.Akima1DInterpolator\n        else:\n            raise ValueError(""%s is not a valid scipy interpolator"" % method)\n    else:\n        raise ValueError(""%s is not a valid interpolator"" % method)\n\n    return interp_class, kwargs\n\n\ndef _get_interpolator_nd(method, **kwargs):\n    """"""helper function to select the appropriate interpolator class\n\n    returns interpolator class and keyword arguments for the class\n    """"""\n    valid_methods = [""linear"", ""nearest""]\n\n    try:\n        from scipy import interpolate\n    except ImportError:\n        raise ImportError(""Interpolation with method `%s` requires scipy"" % method)\n\n    if method in valid_methods:\n        kwargs.update(method=method)\n        interp_class = interpolate.interpn\n    else:\n        raise ValueError(\n            ""%s is not a valid interpolator for interpolating ""\n            ""over multiple dimensions."" % method\n        )\n\n    return interp_class, kwargs\n\n\ndef _get_valid_fill_mask(arr, dim, limit):\n    """"""helper function to determine values that can be filled when limit is not\n    None""""""\n    kw = {dim: limit + 1}\n    # we explicitly use construct method to avoid copy.\n    new_dim = utils.get_temp_dimname(arr.dims, ""_window"")\n    return (\n        arr.isnull()\n        .rolling(min_periods=1, **kw)\n        .construct(new_dim, fill_value=False)\n        .sum(new_dim, skipna=False)\n    ) <= limit\n\n\ndef _assert_single_chunk(var, axes):\n    for axis in axes:\n        if len(var.chunks[axis]) > 1 or var.chunks[axis][0] < var.shape[axis]:\n            raise NotImplementedError(\n                ""Chunking along the dimension to be interpolated ""\n                ""({}) is not yet supported."".format(axis)\n            )\n\n\ndef _localize(var, indexes_coords):\n    """""" Speed up for linear and nearest neighbor method.\n    Only consider a subspace that is needed for the interpolation\n    """"""\n    indexes = {}\n    for dim, [x, new_x] in indexes_coords.items():\n        index = x.to_index()\n        imin = index.get_loc(np.min(new_x.values), method=""nearest"")\n        imax = index.get_loc(np.max(new_x.values), method=""nearest"")\n\n        indexes[dim] = slice(max(imin - 2, 0), imax + 2)\n        indexes_coords[dim] = (x[indexes[dim]], new_x)\n    return var.isel(**indexes), indexes_coords\n\n\ndef _floatize_x(x, new_x):\n    """""" Make x and new_x float.\n    This is particulary useful for datetime dtype.\n    x, new_x: tuple of np.ndarray\n    """"""\n    x = list(x)\n    new_x = list(new_x)\n    for i in range(len(x)):\n        if _contains_datetime_like_objects(x[i]):\n            # Scipy casts coordinates to np.float64, which is not accurate\n            # enough for datetime64 (uses 64bit integer).\n            # We assume that the most of the bits are used to represent the\n            # offset (min(x)) and the variation (x - min(x)) can be\n            # represented by float.\n            xmin = x[i].values.min()\n            x[i] = x[i]._to_numeric(offset=xmin, dtype=np.float64)\n            new_x[i] = new_x[i]._to_numeric(offset=xmin, dtype=np.float64)\n    return x, new_x\n\n\ndef interp(var, indexes_coords, method, **kwargs):\n    """""" Make an interpolation of Variable\n\n    Parameters\n    ----------\n    var: Variable\n    index_coords:\n        Mapping from dimension name to a pair of original and new coordinates.\n        Original coordinates should be sorted in strictly ascending order.\n        Note that all the coordinates should be Variable objects.\n    method: string\n        One of {\'linear\', \'nearest\', \'zero\', \'slinear\', \'quadratic\',\n        \'cubic\'}. For multidimensional interpolation, only\n        {\'linear\', \'nearest\'} can be used.\n    **kwargs:\n        keyword arguments to be passed to scipy.interpolate\n\n    Returns\n    -------\n    Interpolated Variable\n\n    See Also\n    --------\n    DataArray.interp\n    Dataset.interp\n    """"""\n    if not indexes_coords:\n        return var.copy()\n\n    # simple speed up for the local interpolation\n    if method in [""linear"", ""nearest""]:\n        var, indexes_coords = _localize(var, indexes_coords)\n\n    # default behavior\n    kwargs[""bounds_error""] = kwargs.get(""bounds_error"", False)\n\n    # check if the interpolation can be done in orthogonal manner\n    if (\n        len(indexes_coords) > 1\n        and method in [""linear"", ""nearest""]\n        and all(dest[1].ndim == 1 for dest in indexes_coords.values())\n        and len(set([d[1].dims[0] for d in indexes_coords.values()]))\n        == len(indexes_coords)\n    ):\n        # interpolate sequentially\n        for dim, dest in indexes_coords.items():\n            var = interp(var, {dim: dest}, method, **kwargs)\n        return var\n\n    # target dimensions\n    dims = list(indexes_coords)\n    x, new_x = zip(*[indexes_coords[d] for d in dims])\n    destination = broadcast_variables(*new_x)\n\n    # transpose to make the interpolated axis to the last position\n    broadcast_dims = [d for d in var.dims if d not in dims]\n    original_dims = broadcast_dims + dims\n    new_dims = broadcast_dims + list(destination[0].dims)\n    interped = interp_func(\n        var.transpose(*original_dims).data, x, destination, method, kwargs\n    )\n\n    result = Variable(new_dims, interped, attrs=var.attrs)\n\n    # dimension of the output array\n    out_dims = OrderedSet()\n    for d in var.dims:\n        if d in dims:\n            out_dims.update(indexes_coords[d][1].dims)\n        else:\n            out_dims.add(d)\n    return result.transpose(*tuple(out_dims))\n\n\ndef interp_func(var, x, new_x, method, kwargs):\n    """"""\n    multi-dimensional interpolation for array-like. Interpolated axes should be\n    located in the last position.\n\n    Parameters\n    ----------\n    var: np.ndarray or dask.array.Array\n        Array to be interpolated. The final dimension is interpolated.\n    x: a list of 1d array.\n        Original coordinates. Should not contain NaN.\n    new_x: a list of 1d array\n        New coordinates. Should not contain NaN.\n    method: string\n        {\'linear\', \'nearest\', \'zero\', \'slinear\', \'quadratic\', \'cubic\'} for\n        1-dimensional interpolation.\n        {\'linear\', \'nearest\'} for multidimensional interpolation\n    **kwargs:\n        Optional keyword arguments to be passed to scipy.interpolator\n\n    Returns\n    -------\n    interpolated: array\n        Interpolated array\n\n    Note\n    ----\n    This requiers scipy installed.\n\n    See Also\n    --------\n    scipy.interpolate.interp1d\n    """"""\n    if not x:\n        return var.copy()\n\n    if len(x) == 1:\n        func, kwargs = _get_interpolator(method, vectorizeable_only=True, **kwargs)\n    else:\n        func, kwargs = _get_interpolator_nd(method, **kwargs)\n\n    if isinstance(var, dask_array_type):\n        import dask.array as da\n\n        _assert_single_chunk(var, range(var.ndim - len(x), var.ndim))\n        chunks = var.chunks[: -len(x)] + new_x[0].shape\n        drop_axis = range(var.ndim - len(x), var.ndim)\n        new_axis = range(var.ndim - len(x), var.ndim - len(x) + new_x[0].ndim)\n        return da.map_blocks(\n            _interpnd,\n            var,\n            x,\n            new_x,\n            func,\n            kwargs,\n            dtype=var.dtype,\n            chunks=chunks,\n            new_axis=new_axis,\n            drop_axis=drop_axis,\n        )\n\n    return _interpnd(var, x, new_x, func, kwargs)\n\n\ndef _interp1d(var, x, new_x, func, kwargs):\n    # x, new_x are tuples of size 1.\n    x, new_x = x[0], new_x[0]\n    rslt = func(x, var, assume_sorted=True, **kwargs)(np.ravel(new_x))\n    if new_x.ndim > 1:\n        return rslt.reshape(var.shape[:-1] + new_x.shape)\n    if new_x.ndim == 0:\n        return rslt[..., -1]\n    return rslt\n\n\ndef _interpnd(var, x, new_x, func, kwargs):\n    x, new_x = _floatize_x(x, new_x)\n\n    if len(x) == 1:\n        return _interp1d(var, x, new_x, func, kwargs)\n\n    # move the interpolation axes to the start position\n    var = var.transpose(range(-len(x), var.ndim - len(x)))\n    # stack new_x to 1 vector, with reshape\n    xi = np.stack([x1.values.ravel() for x1 in new_x], axis=-1)\n    rslt = func(x, var, xi, **kwargs)\n    # move back the interpolation axes to the last position\n    rslt = rslt.transpose(range(-rslt.ndim + 1, 1))\n    return rslt.reshape(rslt.shape[:-1] + new_x[0].shape)\n'"
xarray/core/nanops.py,4,"b'import numpy as np\n\nfrom . import dtypes, nputils, utils\nfrom .duck_array_ops import _dask_or_eager_func, count, fillna, isnull, where_method\nfrom .pycompat import dask_array_type\n\ntry:\n    import dask.array as dask_array\n    from . import dask_array_compat\nexcept ImportError:\n    dask_array = None\n    dask_array_compat = None  # type: ignore\n\n\ndef _replace_nan(a, val):\n    """"""\n    replace nan in a by val, and returns the replaced array and the nan\n    position\n    """"""\n    mask = isnull(a)\n    return where_method(val, mask, a), mask\n\n\ndef _maybe_null_out(result, axis, mask, min_count=1):\n    """"""\n    xarray version of pandas.core.nanops._maybe_null_out\n    """"""\n    if hasattr(axis, ""__len__""):  # if tuple or list\n        raise ValueError(\n            ""min_count is not available for reduction with more than one dimensions.""\n        )\n\n    if axis is not None and getattr(result, ""ndim"", False):\n        null_mask = (mask.shape[axis] - mask.sum(axis) - min_count) < 0\n        if null_mask.any():\n            dtype, fill_value = dtypes.maybe_promote(result.dtype)\n            result = result.astype(dtype)\n            result[null_mask] = fill_value\n\n    elif getattr(result, ""dtype"", None) not in dtypes.NAT_TYPES:\n        null_mask = mask.size - mask.sum()\n        if null_mask < min_count:\n            result = np.nan\n\n    return result\n\n\ndef _nan_argminmax_object(func, fill_value, value, axis=None, **kwargs):\n    """""" In house nanargmin, nanargmax for object arrays. Always return integer\n    type\n    """"""\n    valid_count = count(value, axis=axis)\n    value = fillna(value, fill_value)\n    data = _dask_or_eager_func(func)(value, axis=axis, **kwargs)\n\n    # TODO This will evaluate dask arrays and might be costly.\n    if (valid_count == 0).any():\n        raise ValueError(""All-NaN slice encountered"")\n\n    return data\n\n\ndef _nan_minmax_object(func, fill_value, value, axis=None, **kwargs):\n    """""" In house nanmin and nanmax for object array """"""\n    valid_count = count(value, axis=axis)\n    filled_value = fillna(value, fill_value)\n    data = getattr(np, func)(filled_value, axis=axis, **kwargs)\n    if not hasattr(data, ""dtype""):  # scalar case\n        data = fill_value if valid_count == 0 else data\n        # we\'ve computed a single min, max value of type object.\n        # don\'t let np.array turn a tuple back into an array\n        return utils.to_0d_object_array(data)\n    return where_method(data, valid_count != 0)\n\n\ndef nanmin(a, axis=None, out=None):\n    if a.dtype.kind == ""O"":\n        return _nan_minmax_object(""min"", dtypes.get_pos_infinity(a.dtype), a, axis)\n\n    module = dask_array if isinstance(a, dask_array_type) else nputils\n    return module.nanmin(a, axis=axis)\n\n\ndef nanmax(a, axis=None, out=None):\n    if a.dtype.kind == ""O"":\n        return _nan_minmax_object(""max"", dtypes.get_neg_infinity(a.dtype), a, axis)\n\n    module = dask_array if isinstance(a, dask_array_type) else nputils\n    return module.nanmax(a, axis=axis)\n\n\ndef nanargmin(a, axis=None):\n    if a.dtype.kind == ""O"":\n        fill_value = dtypes.get_pos_infinity(a.dtype)\n        return _nan_argminmax_object(""argmin"", fill_value, a, axis=axis)\n\n    module = dask_array if isinstance(a, dask_array_type) else nputils\n    return module.nanargmin(a, axis=axis)\n\n\ndef nanargmax(a, axis=None):\n    if a.dtype.kind == ""O"":\n        fill_value = dtypes.get_neg_infinity(a.dtype)\n        return _nan_argminmax_object(""argmax"", fill_value, a, axis=axis)\n\n    module = dask_array if isinstance(a, dask_array_type) else nputils\n    return module.nanargmax(a, axis=axis)\n\n\ndef nansum(a, axis=None, dtype=None, out=None, min_count=None):\n    a, mask = _replace_nan(a, 0)\n    result = _dask_or_eager_func(""sum"")(a, axis=axis, dtype=dtype)\n    if min_count is not None:\n        return _maybe_null_out(result, axis, mask, min_count)\n    else:\n        return result\n\n\ndef _nanmean_ddof_object(ddof, value, axis=None, dtype=None, **kwargs):\n    """""" In house nanmean. ddof argument will be used in _nanvar method """"""\n    from .duck_array_ops import count, fillna, _dask_or_eager_func, where_method\n\n    valid_count = count(value, axis=axis)\n    value = fillna(value, 0)\n    # As dtype inference is impossible for object dtype, we assume float\n    # https://github.com/dask/dask/issues/3162\n    if dtype is None and value.dtype.kind == ""O"":\n        dtype = value.dtype if value.dtype.kind in [""cf""] else float\n\n    data = _dask_or_eager_func(""sum"")(value, axis=axis, dtype=dtype, **kwargs)\n    data = data / (valid_count - ddof)\n    return where_method(data, valid_count != 0)\n\n\ndef nanmean(a, axis=None, dtype=None, out=None):\n    if a.dtype.kind == ""O"":\n        return _nanmean_ddof_object(0, a, axis=axis, dtype=dtype)\n\n    if isinstance(a, dask_array_type):\n        return dask_array.nanmean(a, axis=axis, dtype=dtype)\n\n    return np.nanmean(a, axis=axis, dtype=dtype)\n\n\ndef nanmedian(a, axis=None, out=None):\n    # The dask algorithm works by rechunking to one chunk along axis\n    # Make sure we trigger the dask error when passing all dimensions\n    # so that we don\'t rechunk the entire array to one chunk and\n    # possibly blow memory\n    if axis is not None and len(np.atleast_1d(axis)) == a.ndim:\n        axis = None\n    return _dask_or_eager_func(\n        ""nanmedian"", dask_module=dask_array_compat, eager_module=nputils\n    )(a, axis=axis)\n\n\ndef _nanvar_object(value, axis=None, ddof=0, keepdims=False, **kwargs):\n    value_mean = _nanmean_ddof_object(\n        ddof=0, value=value, axis=axis, keepdims=True, **kwargs\n    )\n    squared = (value.astype(value_mean.dtype) - value_mean) ** 2\n    return _nanmean_ddof_object(ddof, squared, axis=axis, keepdims=keepdims, **kwargs)\n\n\ndef nanvar(a, axis=None, dtype=None, out=None, ddof=0):\n    if a.dtype.kind == ""O"":\n        return _nanvar_object(a, axis=axis, dtype=dtype, ddof=ddof)\n\n    return _dask_or_eager_func(""nanvar"", eager_module=nputils)(\n        a, axis=axis, dtype=dtype, ddof=ddof\n    )\n\n\ndef nanstd(a, axis=None, dtype=None, out=None, ddof=0):\n    return _dask_or_eager_func(""nanstd"", eager_module=nputils)(\n        a, axis=axis, dtype=dtype, ddof=ddof\n    )\n\n\ndef nanprod(a, axis=None, dtype=None, out=None, min_count=None):\n    a, mask = _replace_nan(a, 1)\n    result = _dask_or_eager_func(""nanprod"")(a, axis=axis, dtype=dtype, out=out)\n    if min_count is not None:\n        return _maybe_null_out(result, axis, mask, min_count)\n    else:\n        return result\n\n\ndef nancumsum(a, axis=None, dtype=None, out=None):\n    return _dask_or_eager_func(""nancumsum"", eager_module=nputils)(\n        a, axis=axis, dtype=dtype\n    )\n\n\ndef nancumprod(a, axis=None, dtype=None, out=None):\n    return _dask_or_eager_func(""nancumprod"", eager_module=nputils)(\n        a, axis=axis, dtype=dtype\n    )\n'"
xarray/core/npcompat.py,3,"b'# Copyright (c) 2005-2011, NumPy Developers.\n# All rights reserved.\n\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n\n#     * Redistributions of source code must retain the above copyright\n#        notice, this list of conditions and the following disclaimer.\n\n#     * Redistributions in binary form must reproduce the above\n#        copyright notice, this list of conditions and the following\n#        disclaimer in the documentation and/or other materials provided\n#        with the distribution.\n\n#     * Neither the name of the NumPy Developers nor the names of any\n#        contributors may be used to endorse or promote products derived\n#        from this software without specific prior written permission.\n\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nimport builtins\nimport operator\nfrom typing import Union\n\nimport numpy as np\n\n\n# Vendored from NumPy 1.12; we need a version that support duck typing, even\n# on dask arrays with __array_function__ enabled.\ndef _validate_axis(axis, ndim, argname):\n    try:\n        axis = [operator.index(axis)]\n    except TypeError:\n        axis = list(axis)\n    axis = [a + ndim if a < 0 else a for a in axis]\n    if not builtins.all(0 <= a < ndim for a in axis):\n        raise ValueError(""invalid axis for this array in `%s` argument"" % argname)\n    if len(set(axis)) != len(axis):\n        raise ValueError(""repeated axis in `%s` argument"" % argname)\n    return axis\n\n\ndef moveaxis(a, source, destination):\n    try:\n        # allow duck-array types if they define transpose\n        transpose = a.transpose\n    except AttributeError:\n        a = np.asarray(a)\n        transpose = a.transpose\n\n    source = _validate_axis(source, a.ndim, ""source"")\n    destination = _validate_axis(destination, a.ndim, ""destination"")\n    if len(source) != len(destination):\n        raise ValueError(\n            ""`source` and `destination` arguments must have ""\n            ""the same number of elements""\n        )\n\n    order = [n for n in range(a.ndim) if n not in source]\n\n    for dest, src in sorted(zip(destination, source)):\n        order.insert(dest, src)\n\n    result = transpose(order)\n    return result\n\n\n# Type annotations stubs. See also / to be replaced by:\n# https://github.com/numpy/numpy/issues/7370\n# https://github.com/numpy/numpy-stubs/\nDTypeLike = Union[np.dtype, str]\n\n\n# from dask/array/utils.py\ndef _is_nep18_active():\n    class A:\n        def __array_function__(self, *args, **kwargs):\n            return True\n\n    try:\n        return np.concatenate([A()])\n    except ValueError:\n        return False\n\n\nIS_NEP18_ACTIVE = _is_nep18_active()\n'"
xarray/core/nputils.py,36,"b'import warnings\n\nimport numpy as np\nimport pandas as pd\nfrom numpy.core.multiarray import normalize_axis_index\n\ntry:\n    import bottleneck as bn\n\n    _USE_BOTTLENECK = True\nexcept ImportError:\n    # use numpy methods instead\n    bn = np\n    _USE_BOTTLENECK = False\n\n\ndef _select_along_axis(values, idx, axis):\n    other_ind = np.ix_(*[np.arange(s) for s in idx.shape])\n    sl = other_ind[:axis] + (idx,) + other_ind[axis:]\n    return values[sl]\n\n\ndef nanfirst(values, axis):\n    axis = normalize_axis_index(axis, values.ndim)\n    idx_first = np.argmax(~pd.isnull(values), axis=axis)\n    return _select_along_axis(values, idx_first, axis)\n\n\ndef nanlast(values, axis):\n    axis = normalize_axis_index(axis, values.ndim)\n    rev = (slice(None),) * axis + (slice(None, None, -1),)\n    idx_last = -1 - np.argmax(~pd.isnull(values)[rev], axis=axis)\n    return _select_along_axis(values, idx_last, axis)\n\n\ndef inverse_permutation(indices):\n    """"""Return indices for an inverse permutation.\n\n    Parameters\n    ----------\n    indices : 1D np.ndarray with dtype=int\n        Integer positions to assign elements to.\n\n    Returns\n    -------\n    inverse_permutation : 1D np.ndarray with dtype=int\n        Integer indices to take from the original array to create the\n        permutation.\n    """"""\n    # use intp instead of int64 because of windows :(\n    inverse_permutation = np.empty(len(indices), dtype=np.intp)\n    inverse_permutation[indices] = np.arange(len(indices), dtype=np.intp)\n    return inverse_permutation\n\n\ndef _ensure_bool_is_ndarray(result, *args):\n    # numpy will sometimes return a scalar value from binary comparisons if it\n    # can\'t handle the comparison instead of broadcasting, e.g.,\n    # In [10]: 1 == np.array([\'a\', \'b\'])\n    # Out[10]: False\n    # This function ensures that the result is the appropriate shape in these\n    # cases\n    if isinstance(result, bool):\n        shape = np.broadcast(*args).shape\n        constructor = np.ones if result else np.zeros\n        result = constructor(shape, dtype=bool)\n    return result\n\n\ndef array_eq(self, other):\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", r""elementwise comparison failed"")\n        return _ensure_bool_is_ndarray(self == other, self, other)\n\n\ndef array_ne(self, other):\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", r""elementwise comparison failed"")\n        return _ensure_bool_is_ndarray(self != other, self, other)\n\n\ndef _is_contiguous(positions):\n    """"""Given a non-empty list, does it consist of contiguous integers?""""""\n    previous = positions[0]\n    for current in positions[1:]:\n        if current != previous + 1:\n            return False\n        previous = current\n    return True\n\n\ndef _advanced_indexer_subspaces(key):\n    """"""Indices of the advanced indexes subspaces for mixed indexing and vindex.\n    """"""\n    if not isinstance(key, tuple):\n        key = (key,)\n    advanced_index_positions = [\n        i for i, k in enumerate(key) if not isinstance(k, slice)\n    ]\n\n    if not advanced_index_positions or not _is_contiguous(advanced_index_positions):\n        # Nothing to reorder: dimensions on the indexing result are already\n        # ordered like vindex. See NumPy\'s rule for ""Combining advanced and\n        # basic indexing"":\n        # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#combining-advanced-and-basic-indexing\n        return (), ()\n\n    non_slices = [k for k in key if not isinstance(k, slice)]\n    ndim = len(np.broadcast(*non_slices).shape)\n    mixed_positions = advanced_index_positions[0] + np.arange(ndim)\n    vindex_positions = np.arange(ndim)\n    return mixed_positions, vindex_positions\n\n\nclass NumpyVIndexAdapter:\n    """"""Object that implements indexing like vindex on a np.ndarray.\n\n    This is a pure Python implementation of (some of) the logic in this NumPy\n    proposal: https://github.com/numpy/numpy/pull/6256\n    """"""\n\n    def __init__(self, array):\n        self._array = array\n\n    def __getitem__(self, key):\n        mixed_positions, vindex_positions = _advanced_indexer_subspaces(key)\n        return np.moveaxis(self._array[key], mixed_positions, vindex_positions)\n\n    def __setitem__(self, key, value):\n        """"""Value must have dimensionality matching the key.""""""\n        mixed_positions, vindex_positions = _advanced_indexer_subspaces(key)\n        self._array[key] = np.moveaxis(value, vindex_positions, mixed_positions)\n\n\ndef rolling_window(a, axis, window, center, fill_value):\n    """""" rolling window with padding. """"""\n    pads = [(0, 0) for s in a.shape]\n    if center:\n        start = int(window / 2)  # 10 -> 5,  9 -> 4\n        end = window - 1 - start\n        pads[axis] = (start, end)\n    else:\n        pads[axis] = (window - 1, 0)\n    a = np.pad(a, pads, mode=""constant"", constant_values=fill_value)\n    return _rolling_window(a, window, axis)\n\n\ndef _rolling_window(a, window, axis=-1):\n    """"""\n    Make an ndarray with a rolling window along axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Array to add rolling window to\n    axis: int\n        axis position along which rolling window will be applied.\n    window : int\n        Size of rolling window\n\n    Returns\n    -------\n    Array that is a view of the original array with a added dimension\n    of size w.\n\n    Examples\n    --------\n    >>> x = np.arange(10).reshape((2, 5))\n    >>> np.rolling_window(x, 3, axis=-1)\n    array([[[0, 1, 2], [1, 2, 3], [2, 3, 4]],\n           [[5, 6, 7], [6, 7, 8], [7, 8, 9]]])\n\n    Calculate rolling mean of last dimension:\n    >>> np.mean(np.rolling_window(x, 3, axis=-1), -1)\n    array([[ 1.,  2.,  3.],\n           [ 6.,  7.,  8.]])\n\n    This function is taken from https://github.com/numpy/numpy/pull/31\n    but slightly modified to accept axis option.\n    """"""\n    axis = normalize_axis_index(axis, a.ndim)\n    a = np.swapaxes(a, axis, -1)\n\n    if window < 1:\n        raise ValueError(f""`window` must be at least 1. Given : {window}"")\n    if window > a.shape[-1]:\n        raise ValueError(f""`window` is too long. Given : {window}"")\n\n    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n    strides = a.strides + (a.strides[-1],)\n    rolling = np.lib.stride_tricks.as_strided(\n        a, shape=shape, strides=strides, writeable=False\n    )\n    return np.swapaxes(rolling, -2, axis)\n\n\ndef _create_bottleneck_method(name, npmodule=np):\n    def f(values, axis=None, **kwargs):\n        dtype = kwargs.get(""dtype"", None)\n        bn_func = getattr(bn, name, None)\n\n        if (\n            _USE_BOTTLENECK\n            and isinstance(values, np.ndarray)\n            and bn_func is not None\n            and not isinstance(axis, tuple)\n            and values.dtype.kind in ""uifc""\n            and values.dtype.isnative\n            and (dtype is None or np.dtype(dtype) == values.dtype)\n        ):\n            # bottleneck does not take care dtype, min_count\n            kwargs.pop(""dtype"", None)\n            result = bn_func(values, axis=axis, **kwargs)\n        else:\n            result = getattr(npmodule, name)(values, axis=axis, **kwargs)\n\n        return result\n\n    f.__name__ = name\n    return f\n\n\ndef _nanpolyfit_1d(arr, x, rcond=None):\n    out = np.full((x.shape[1] + 1,), np.nan)\n    mask = np.isnan(arr)\n    if not np.all(mask):\n        out[:-1], out[-1], _, _ = np.linalg.lstsq(x[~mask, :], arr[~mask], rcond=rcond)\n    return out\n\n\ndef least_squares(lhs, rhs, rcond=None, skipna=False):\n    if skipna:\n        added_dim = rhs.ndim == 1\n        if added_dim:\n            rhs = rhs.reshape(rhs.shape[0], 1)\n        nan_cols = np.any(np.isnan(rhs), axis=0)\n        out = np.empty((lhs.shape[1] + 1, rhs.shape[1]))\n        if np.any(nan_cols):\n            out[:, nan_cols] = np.apply_along_axis(\n                _nanpolyfit_1d, 0, rhs[:, nan_cols], lhs\n            )\n        if np.any(~nan_cols):\n            out[:-1, ~nan_cols], out[-1, ~nan_cols], _, _ = np.linalg.lstsq(\n                lhs, rhs[:, ~nan_cols], rcond=rcond\n            )\n        coeffs = out[:-1, :]\n        residuals = out[-1, :]\n        if added_dim:\n            coeffs = coeffs.reshape(coeffs.shape[0])\n            residuals = residuals.reshape(residuals.shape[0])\n    else:\n        coeffs, residuals, _, _ = np.linalg.lstsq(lhs, rhs, rcond=rcond)\n    return coeffs, residuals\n\n\nnanmin = _create_bottleneck_method(""nanmin"")\nnanmax = _create_bottleneck_method(""nanmax"")\nnanmean = _create_bottleneck_method(""nanmean"")\nnanmedian = _create_bottleneck_method(""nanmedian"")\nnanvar = _create_bottleneck_method(""nanvar"")\nnanstd = _create_bottleneck_method(""nanstd"")\nnanprod = _create_bottleneck_method(""nanprod"")\nnancumsum = _create_bottleneck_method(""nancumsum"")\nnancumprod = _create_bottleneck_method(""nancumprod"")\nnanargmin = _create_bottleneck_method(""nanargmin"")\nnanargmax = _create_bottleneck_method(""nanargmax"")\n'"
xarray/core/ops.py,3,"b'""""""Define core operations for xarray objects.\n\nTODO(shoyer): rewrite this module, making use of xarray.core.computation,\nNumPy\'s __array_ufunc__ and mixin classes instead of the unintuitive ""inject""\nfunctions.\n""""""\n\nimport operator\n\nimport numpy as np\n\nfrom . import dtypes, duck_array_ops\nfrom .nputils import array_eq, array_ne\n\ntry:\n    import bottleneck as bn\n\n    has_bottleneck = True\nexcept ImportError:\n    # use numpy methods instead\n    bn = np\n    has_bottleneck = False\n\n\nUNARY_OPS = [""neg"", ""pos"", ""abs"", ""invert""]\nCMP_BINARY_OPS = [""lt"", ""le"", ""ge"", ""gt""]\nNUM_BINARY_OPS = [\n    ""add"",\n    ""sub"",\n    ""mul"",\n    ""truediv"",\n    ""floordiv"",\n    ""mod"",\n    ""pow"",\n    ""and"",\n    ""xor"",\n    ""or"",\n]\n\n# methods which pass on the numpy return value unchanged\n# be careful not to list methods that we would want to wrap later\nNUMPY_SAME_METHODS = [""item"", ""searchsorted""]\n# methods which don\'t modify the data shape, so the result should still be\n# wrapped in an Variable/DataArray\nNUMPY_UNARY_METHODS = [""astype"", ""argsort"", ""clip"", ""conj"", ""conjugate""]\nPANDAS_UNARY_FUNCTIONS = [""isnull"", ""notnull""]\n# methods which remove an axis\nREDUCE_METHODS = [""all"", ""any""]\nNAN_REDUCE_METHODS = [\n    ""argmax"",\n    ""argmin"",\n    ""max"",\n    ""min"",\n    ""mean"",\n    ""prod"",\n    ""sum"",\n    ""std"",\n    ""var"",\n    ""median"",\n]\nNAN_CUM_METHODS = [""cumsum"", ""cumprod""]\n# TODO: wrap take, dot, sort\n\n\n_CUM_DOCSTRING_TEMPLATE = """"""\\\nApply `{name}` along some dimension of {cls}.\n\nParameters\n----------\n{extra_args}\nskipna : bool, optional\n    If True, skip missing values (as marked by NaN). By default, only\n    skips missing values for float dtypes; other dtypes either do not\n    have a sentinel missing value (int) or skipna=True has not been\n    implemented (object, datetime64 or timedelta64).\nkeep_attrs : bool, optional\n    If True, the attributes (`attrs`) will be copied from the original\n    object to the new one.  If False (default), the new object will be\n    returned without attributes.\n**kwargs : dict\n    Additional keyword arguments passed on to `{name}`.\n\nReturns\n-------\ncumvalue : {cls}\n    New {cls} object with `{name}` applied to its data along the\n    indicated dimension.\n""""""\n\n_REDUCE_DOCSTRING_TEMPLATE = """"""\\\nReduce this {cls}\'s data by applying `{name}` along some dimension(s).\n\nParameters\n----------\n{extra_args}\nskipna : bool, optional\n    If True, skip missing values (as marked by NaN). By default, only\n    skips missing values for float dtypes; other dtypes either do not\n    have a sentinel missing value (int) or skipna=True has not been\n    implemented (object, datetime64 or timedelta64).{min_count_docs}\nkeep_attrs : bool, optional\n    If True, the attributes (`attrs`) will be copied from the original\n    object to the new one.  If False (default), the new object will be\n    returned without attributes.\n**kwargs : dict\n    Additional keyword arguments passed on to the appropriate array\n    function for calculating `{name}` on this object\'s data.\n\nReturns\n-------\nreduced : {cls}\n    New {cls} object with `{name}` applied to its data and the\n    indicated dimension(s) removed.\n""""""\n\n_MINCOUNT_DOCSTRING = """"""\nmin_count : int, default None\n    The required number of valid values to perform the operation.\n    If fewer than min_count non-NA values are present the result will\n    be NA. New in version 0.10.8: Added with the default being None.""""""\n\n_COARSEN_REDUCE_DOCSTRING_TEMPLATE = """"""\\\nCoarsen this object by applying `{name}` along its dimensions.\n\nParameters\n----------\n**kwargs : dict\n    Additional keyword arguments passed on to `{name}`.\n\nReturns\n-------\nreduced : DataArray or Dataset\n    New object with `{name}` applied along its coasen dimnensions.\n""""""\n\n\ndef fillna(data, other, join=""left"", dataset_join=""left""):\n    """"""Fill missing values in this object with data from the other object.\n    Follows normal broadcasting and alignment rules.\n\n    Parameters\n    ----------\n    join : {\'outer\', \'inner\', \'left\', \'right\'}, optional\n        Method for joining the indexes of the passed objects along each\n        dimension\n        - \'outer\': use the union of object indexes\n        - \'inner\': use the intersection of object indexes\n        - \'left\': use indexes from the first object with each dimension\n        - \'right\': use indexes from the last object with each dimension\n        - \'exact\': raise `ValueError` instead of aligning when indexes to be\n          aligned are not equal\n    dataset_join : {\'outer\', \'inner\', \'left\', \'right\'}, optional\n        Method for joining variables of Dataset objects with mismatched\n        data variables.\n        - \'outer\': take variables from both Dataset objects\n        - \'inner\': take only overlapped variables\n        - \'left\': take only variables from the first object\n        - \'right\': take only variables from the last object\n    """"""\n    from .computation import apply_ufunc\n\n    return apply_ufunc(\n        duck_array_ops.fillna,\n        data,\n        other,\n        join=join,\n        dask=""allowed"",\n        dataset_join=dataset_join,\n        dataset_fill_value=np.nan,\n        keep_attrs=True,\n    )\n\n\ndef where_method(self, cond, other=dtypes.NA):\n    """"""Return elements from `self` or `other` depending on `cond`.\n\n    Parameters\n    ----------\n    cond : DataArray or Dataset with boolean dtype\n        Locations at which to preserve this objects values.\n    other : scalar, DataArray or Dataset, optional\n        Value to use for locations in this object where ``cond`` is False.\n        By default, inserts missing values.\n\n    Returns\n    -------\n    Same type as caller.\n    """"""\n    from .computation import apply_ufunc\n\n    # alignment for three arguments is complicated, so don\'t support it yet\n    join = ""inner"" if other is dtypes.NA else ""exact""\n    return apply_ufunc(\n        duck_array_ops.where_method,\n        self,\n        cond,\n        other,\n        join=join,\n        dataset_join=join,\n        dask=""allowed"",\n        keep_attrs=True,\n    )\n\n\ndef _call_possibly_missing_method(arg, name, args, kwargs):\n    try:\n        method = getattr(arg, name)\n    except AttributeError:\n        duck_array_ops.fail_on_dask_array_input(arg, func_name=name)\n        if hasattr(arg, ""data""):\n            duck_array_ops.fail_on_dask_array_input(arg.data, func_name=name)\n        raise\n    else:\n        return method(*args, **kwargs)\n\n\ndef _values_method_wrapper(name):\n    def func(self, *args, **kwargs):\n        return _call_possibly_missing_method(self.data, name, args, kwargs)\n\n    func.__name__ = name\n    func.__doc__ = getattr(np.ndarray, name).__doc__\n    return func\n\n\ndef _method_wrapper(name):\n    def func(self, *args, **kwargs):\n        return _call_possibly_missing_method(self, name, args, kwargs)\n\n    func.__name__ = name\n    func.__doc__ = getattr(np.ndarray, name).__doc__\n    return func\n\n\ndef _func_slash_method_wrapper(f, name=None):\n    # try to wrap a method, but if not found use the function\n    # this is useful when patching in a function as both a DataArray and\n    # Dataset method\n    if name is None:\n        name = f.__name__\n\n    def func(self, *args, **kwargs):\n        try:\n            return getattr(self, name)(*args, **kwargs)\n        except AttributeError:\n            return f(self, *args, **kwargs)\n\n    func.__name__ = name\n    func.__doc__ = f.__doc__\n    return func\n\n\ndef inject_reduce_methods(cls):\n    methods = (\n        [\n            (name, getattr(duck_array_ops, ""array_%s"" % name), False)\n            for name in REDUCE_METHODS\n        ]\n        + [(name, getattr(duck_array_ops, name), True) for name in NAN_REDUCE_METHODS]\n        + [(""count"", duck_array_ops.count, False)]\n    )\n    for name, f, include_skipna in methods:\n        numeric_only = getattr(f, ""numeric_only"", False)\n        available_min_count = getattr(f, ""available_min_count"", False)\n        min_count_docs = _MINCOUNT_DOCSTRING if available_min_count else """"\n\n        func = cls._reduce_method(f, include_skipna, numeric_only)\n        func.__name__ = name\n        func.__doc__ = _REDUCE_DOCSTRING_TEMPLATE.format(\n            name=name,\n            cls=cls.__name__,\n            extra_args=cls._reduce_extra_args_docstring.format(name=name),\n            min_count_docs=min_count_docs,\n        )\n        setattr(cls, name, func)\n\n\ndef inject_cum_methods(cls):\n    methods = [(name, getattr(duck_array_ops, name), True) for name in NAN_CUM_METHODS]\n    for name, f, include_skipna in methods:\n        numeric_only = getattr(f, ""numeric_only"", False)\n        func = cls._reduce_method(f, include_skipna, numeric_only)\n        func.__name__ = name\n        func.__doc__ = _CUM_DOCSTRING_TEMPLATE.format(\n            name=name,\n            cls=cls.__name__,\n            extra_args=cls._cum_extra_args_docstring.format(name=name),\n        )\n        setattr(cls, name, func)\n\n\ndef op_str(name):\n    return ""__%s__"" % name\n\n\ndef get_op(name):\n    return getattr(operator, op_str(name))\n\n\nNON_INPLACE_OP = {get_op(""i"" + name): get_op(name) for name in NUM_BINARY_OPS}\n\n\ndef inplace_to_noninplace_op(f):\n    return NON_INPLACE_OP[f]\n\n\ndef inject_binary_ops(cls, inplace=False):\n    for name in CMP_BINARY_OPS + NUM_BINARY_OPS:\n        setattr(cls, op_str(name), cls._binary_op(get_op(name)))\n\n    for name, f in [(""eq"", array_eq), (""ne"", array_ne)]:\n        setattr(cls, op_str(name), cls._binary_op(f))\n\n    for name in NUM_BINARY_OPS:\n        # only numeric operations have in-place and reflexive variants\n        setattr(cls, op_str(""r"" + name), cls._binary_op(get_op(name), reflexive=True))\n        if inplace:\n            setattr(cls, op_str(""i"" + name), cls._inplace_binary_op(get_op(""i"" + name)))\n\n\ndef inject_all_ops_and_reduce_methods(cls, priority=50, array_only=True):\n    # prioritize our operations over those of numpy.ndarray (priority=1)\n    # and numpy.matrix (priority=10)\n    cls.__array_priority__ = priority\n\n    # patch in standard special operations\n    for name in UNARY_OPS:\n        setattr(cls, op_str(name), cls._unary_op(get_op(name)))\n    inject_binary_ops(cls, inplace=True)\n\n    # patch in numpy/pandas methods\n    for name in NUMPY_UNARY_METHODS:\n        setattr(cls, name, cls._unary_op(_method_wrapper(name)))\n\n    for name in PANDAS_UNARY_FUNCTIONS:\n        f = _func_slash_method_wrapper(getattr(duck_array_ops, name), name=name)\n        setattr(cls, name, cls._unary_op(f))\n\n    f = _func_slash_method_wrapper(duck_array_ops.around, name=""round"")\n    setattr(cls, ""round"", cls._unary_op(f))\n\n    if array_only:\n        # these methods don\'t return arrays of the same shape as the input, so\n        # don\'t try to patch these in for Dataset objects\n        for name in NUMPY_SAME_METHODS:\n            setattr(cls, name, _values_method_wrapper(name))\n\n    inject_reduce_methods(cls)\n    inject_cum_methods(cls)\n'"
xarray/core/options.py,1,"b'import warnings\n\nDISPLAY_WIDTH = ""display_width""\nARITHMETIC_JOIN = ""arithmetic_join""\nENABLE_CFTIMEINDEX = ""enable_cftimeindex""\nFILE_CACHE_MAXSIZE = ""file_cache_maxsize""\nWARN_FOR_UNCLOSED_FILES = ""warn_for_unclosed_files""\nCMAP_SEQUENTIAL = ""cmap_sequential""\nCMAP_DIVERGENT = ""cmap_divergent""\nKEEP_ATTRS = ""keep_attrs""\nDISPLAY_STYLE = ""display_style""\n\n\nOPTIONS = {\n    DISPLAY_WIDTH: 80,\n    ARITHMETIC_JOIN: ""inner"",\n    ENABLE_CFTIMEINDEX: True,\n    FILE_CACHE_MAXSIZE: 128,\n    WARN_FOR_UNCLOSED_FILES: False,\n    CMAP_SEQUENTIAL: ""viridis"",\n    CMAP_DIVERGENT: ""RdBu_r"",\n    KEEP_ATTRS: ""default"",\n    DISPLAY_STYLE: ""html"",\n}\n\n_JOIN_OPTIONS = frozenset([""inner"", ""outer"", ""left"", ""right"", ""exact""])\n_DISPLAY_OPTIONS = frozenset([""text"", ""html""])\n\n\ndef _positive_integer(value):\n    return isinstance(value, int) and value > 0\n\n\n_VALIDATORS = {\n    DISPLAY_WIDTH: _positive_integer,\n    ARITHMETIC_JOIN: _JOIN_OPTIONS.__contains__,\n    ENABLE_CFTIMEINDEX: lambda value: isinstance(value, bool),\n    FILE_CACHE_MAXSIZE: _positive_integer,\n    WARN_FOR_UNCLOSED_FILES: lambda value: isinstance(value, bool),\n    KEEP_ATTRS: lambda choice: choice in [True, False, ""default""],\n    DISPLAY_STYLE: _DISPLAY_OPTIONS.__contains__,\n}\n\n\ndef _set_file_cache_maxsize(value):\n    from ..backends.file_manager import FILE_CACHE\n\n    FILE_CACHE.maxsize = value\n\n\ndef _warn_on_setting_enable_cftimeindex(enable_cftimeindex):\n    warnings.warn(\n        ""The enable_cftimeindex option is now a no-op ""\n        ""and will be removed in a future version of xarray."",\n        FutureWarning,\n    )\n\n\n_SETTERS = {\n    FILE_CACHE_MAXSIZE: _set_file_cache_maxsize,\n    ENABLE_CFTIMEINDEX: _warn_on_setting_enable_cftimeindex,\n}\n\n\ndef _get_keep_attrs(default):\n    global_choice = OPTIONS[""keep_attrs""]\n\n    if global_choice == ""default"":\n        return default\n    elif global_choice in [True, False]:\n        return global_choice\n    else:\n        raise ValueError(\n            ""The global option keep_attrs must be one of"" "" True, False or \'default\'.""\n        )\n\n\nclass set_options:\n    """"""Set options for xarray in a controlled context.\n\n    Currently supported options:\n\n    - ``display_width``: maximum display width for ``repr`` on xarray objects.\n      Default: ``80``.\n    - ``arithmetic_join``: DataArray/Dataset alignment in binary operations.\n      Default: ``\'inner\'``.\n    - ``file_cache_maxsize``: maximum number of open files to hold in xarray\'s\n      global least-recently-usage cached. This should be smaller than your\n      system\'s per-process file descriptor limit, e.g., ``ulimit -n`` on Linux.\n      Default: 128.\n    - ``warn_for_unclosed_files``: whether or not to issue a warning when\n      unclosed files are deallocated (default False). This is mostly useful\n      for debugging.\n    - ``cmap_sequential``: colormap to use for nondivergent data plots.\n      Default: ``viridis``. If string, must be matplotlib built-in colormap.\n      Can also be a Colormap object (e.g. mpl.cm.magma)\n    - ``cmap_divergent``: colormap to use for divergent data plots.\n      Default: ``RdBu_r``. If string, must be matplotlib built-in colormap.\n      Can also be a Colormap object (e.g. mpl.cm.magma)\n    - ``keep_attrs``: rule for whether to keep attributes on xarray\n      Datasets/dataarrays after operations. Either ``True`` to always keep\n      attrs, ``False`` to always discard them, or ``\'default\'`` to use original\n      logic that attrs should only be kept in unambiguous circumstances.\n      Default: ``\'default\'``.\n    - ``display_style``: display style to use in jupyter for xarray objects.\n      Default: ``\'text\'``. Other options are ``\'html\'``.\n\n\n    You can use ``set_options`` either as a context manager:\n\n    >>> ds = xr.Dataset({""x"": np.arange(1000)})\n    >>> with xr.set_options(display_width=40):\n    ...     print(ds)\n    <xarray.Dataset>\n    Dimensions:  (x: 1000)\n    Coordinates:\n      * x        (x) int64 0 1 2 3 4 5 6 ...\n    Data variables:\n        *empty*\n\n    Or to set global options:\n\n    >>> xr.set_options(display_width=80)\n    """"""\n\n    def __init__(self, **kwargs):\n        self.old = {}\n        for k, v in kwargs.items():\n            if k not in OPTIONS:\n                raise ValueError(\n                    ""argument name %r is not in the set of valid options %r""\n                    % (k, set(OPTIONS))\n                )\n            if k in _VALIDATORS and not _VALIDATORS[k](v):\n                raise ValueError(f""option {k!r} given an invalid value: {v!r}"")\n            self.old[k] = OPTIONS[k]\n        self._apply_update(kwargs)\n\n    def _apply_update(self, options_dict):\n        for k, v in options_dict.items():\n            if k in _SETTERS:\n                _SETTERS[k](v)\n        OPTIONS.update(options_dict)\n\n    def __enter__(self):\n        return\n\n    def __exit__(self, type, value, traceback):\n        self._apply_update(self.old)\n'"
xarray/core/parallel.py,5,"b'try:\n    import dask\n    import dask.array\n    from dask.highlevelgraph import HighLevelGraph\n    from .dask_array_compat import meta_from_array\n\nexcept ImportError:\n    pass\n\nimport collections\nimport itertools\nimport operator\nfrom typing import (\n    Any,\n    Callable,\n    DefaultDict,\n    Dict,\n    Hashable,\n    Iterable,\n    List,\n    Mapping,\n    Sequence,\n    Tuple,\n    TypeVar,\n    Union,\n)\n\nimport numpy as np\n\nfrom .alignment import align\nfrom .dataarray import DataArray\nfrom .dataset import Dataset\n\nT_DSorDA = TypeVar(""T_DSorDA"", DataArray, Dataset)\n\n\ndef to_object_array(iterable):\n    # using empty_like calls compute\n    npargs = np.empty((len(iterable),), dtype=np.object)\n    npargs[:] = iterable\n    return npargs\n\n\ndef assert_chunks_compatible(a: Dataset, b: Dataset):\n    a = a.unify_chunks()\n    b = b.unify_chunks()\n\n    for dim in set(a.chunks).intersection(set(b.chunks)):\n        if a.chunks[dim] != b.chunks[dim]:\n            raise ValueError(f""Chunk sizes along dimension {dim!r} are not equal."")\n\n\ndef check_result_variables(\n    result: Union[DataArray, Dataset], expected: Mapping[str, Any], kind: str\n):\n\n    if kind == ""coords"":\n        nice_str = ""coordinate""\n    elif kind == ""data_vars"":\n        nice_str = ""data""\n\n    # check that coords and data variables are as expected\n    missing = expected[kind] - set(getattr(result, kind))\n    if missing:\n        raise ValueError(\n            ""Result from applying user function does not contain ""\n            f""{nice_str} variables {missing}.""\n        )\n    extra = set(getattr(result, kind)) - expected[kind]\n    if extra:\n        raise ValueError(\n            ""Result from applying user function has unexpected ""\n            f""{nice_str} variables {extra}.""\n        )\n\n\ndef dataset_to_dataarray(obj: Dataset) -> DataArray:\n    if not isinstance(obj, Dataset):\n        raise TypeError(""Expected Dataset, got %s"" % type(obj))\n\n    if len(obj.data_vars) > 1:\n        raise TypeError(\n            ""Trying to convert Dataset with more than one data variable to DataArray""\n        )\n\n    return next(iter(obj.data_vars.values()))\n\n\ndef dataarray_to_dataset(obj: DataArray) -> Dataset:\n    # only using _to_temp_dataset would break\n    # func = lambda x: x.to_dataset()\n    # since that relies on preserving name.\n    if obj.name is None:\n        dataset = obj._to_temp_dataset()\n    else:\n        dataset = obj.to_dataset()\n    return dataset\n\n\ndef make_meta(obj):\n    """"""If obj is a DataArray or Dataset, return a new object of the same type and with\n    the same variables and dtypes, but where all variables have size 0 and numpy\n    backend.\n    If obj is neither a DataArray nor Dataset, return it unaltered.\n    """"""\n    if isinstance(obj, DataArray):\n        obj_array = obj\n        obj = obj._to_temp_dataset()\n    elif isinstance(obj, Dataset):\n        obj_array = None\n    else:\n        return obj\n\n    meta = Dataset()\n    for name, variable in obj.variables.items():\n        meta_obj = meta_from_array(variable.data, ndim=variable.ndim)\n        meta[name] = (variable.dims, meta_obj, variable.attrs)\n    meta.attrs = obj.attrs\n    meta = meta.set_coords(obj.coords)\n\n    if obj_array is not None:\n        return obj_array._from_temp_dataset(meta)\n    return meta\n\n\ndef infer_template(\n    func: Callable[..., T_DSorDA], obj: Union[DataArray, Dataset], *args, **kwargs\n) -> T_DSorDA:\n    """"""Infer return object by running the function on meta objects.\n    """"""\n    meta_args = [make_meta(arg) for arg in (obj,) + args]\n\n    try:\n        template = func(*meta_args, **kwargs)\n    except Exception as e:\n        raise Exception(\n            ""Cannot infer object returned from running user provided function. ""\n            ""Please supply the \'template\' kwarg to map_blocks.""\n        ) from e\n\n    if not isinstance(template, (Dataset, DataArray)):\n        raise TypeError(\n            ""Function must return an xarray DataArray or Dataset. Instead it returned ""\n            f""{type(template)}""\n        )\n\n    return template\n\n\ndef make_dict(x: Union[DataArray, Dataset]) -> Dict[Hashable, Any]:\n    """"""Map variable name to numpy(-like) data\n    (Dataset.to_dict() is too complicated).\n    """"""\n    if isinstance(x, DataArray):\n        x = x._to_temp_dataset()\n\n    return {k: v.data for k, v in x.variables.items()}\n\n\ndef _get_chunk_slicer(dim: Hashable, chunk_index: Mapping, chunk_bounds: Mapping):\n    if dim in chunk_index:\n        which_chunk = chunk_index[dim]\n        return slice(chunk_bounds[dim][which_chunk], chunk_bounds[dim][which_chunk + 1])\n    return slice(None)\n\n\ndef map_blocks(\n    func: Callable[..., T_DSorDA],\n    obj: Union[DataArray, Dataset],\n    args: Sequence[Any] = (),\n    kwargs: Mapping[str, Any] = None,\n    template: Union[DataArray, Dataset] = None,\n) -> T_DSorDA:\n    """"""Apply a function to each block of a DataArray or Dataset.\n\n    .. warning::\n        This function is experimental and its signature may change.\n\n    Parameters\n    ----------\n    func: callable\n        User-provided function that accepts a DataArray or Dataset as its first\n        parameter ``obj``. The function will receive a subset or \'block\' of ``obj`` (see below),\n        corresponding to one chunk along each chunked dimension. ``func`` will be\n        executed as ``func(subset_obj, *subset_args, **kwargs)``.\n\n        This function must return either a single DataArray or a single Dataset.\n\n        This function cannot add a new chunked dimension.\n\n    obj: DataArray, Dataset\n        Passed to the function as its first argument, one block at a time.\n    args: Sequence\n        Passed to func after unpacking and subsetting any xarray objects by blocks.\n        xarray objects in args must be aligned with obj, otherwise an error is raised.\n    kwargs: Mapping\n        Passed verbatim to func after unpacking. xarray objects, if any, will not be\n        subset to blocks. Passing dask collections in kwargs is not allowed.\n    template: (optional) DataArray, Dataset\n        xarray object representing the final result after compute is called. If not provided,\n        the function will be first run on mocked-up data, that looks like ``obj`` but\n        has sizes 0, to determine properties of the returned object such as dtype,\n        variable names, attributes, new dimensions and new indexes (if any).\n        ``template`` must be provided if the function changes the size of existing dimensions.\n        When provided, ``attrs`` on variables in `template` are copied over to the result. Any\n        ``attrs`` set by ``func`` will be ignored.\n\n\n    Returns\n    -------\n    A single DataArray or Dataset with dask backend, reassembled from the outputs of the\n    function.\n\n    Notes\n    -----\n    This function is designed for when ``func`` needs to manipulate a whole xarray object\n    subset to each block. In the more common case where ``func`` can work on numpy arrays, it is\n    recommended to use ``apply_ufunc``.\n\n    If none of the variables in ``obj`` is backed by dask arrays, calling this function is\n    equivalent to calling ``func(obj, *args, **kwargs)``.\n\n    See Also\n    --------\n    dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks,\n    xarray.DataArray.map_blocks\n\n    Examples\n    --------\n\n    Calculate an anomaly from climatology using ``.groupby()``. Using\n    ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,\n    its indices, and its methods like ``.groupby()``.\n\n    >>> def calculate_anomaly(da, groupby_type=""time.month""):\n    ...     gb = da.groupby(groupby_type)\n    ...     clim = gb.mean(dim=""time"")\n    ...     return gb - clim\n    >>> time = xr.cftime_range(""1990-01"", ""1992-01"", freq=""M"")\n    >>> np.random.seed(123)\n    >>> array = xr.DataArray(\n    ...     np.random.rand(len(time)), dims=""time"", coords=[time]\n    ... ).chunk()\n    >>> xr.map_blocks(calculate_anomaly, array, template=array).compute()\n    <xarray.DataArray (time: 24)>\n    array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,\n            0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,\n           -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,\n            0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,\n            0.07673453,  0.22865714,  0.19063865, -0.0590131 ])\n    Coordinates:\n      * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n\n    Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments\n    to the function being applied in ``xr.map_blocks()``:\n\n    >>> xr.map_blocks(\n    ...     calculate_anomaly, array, kwargs={""groupby_type"": ""time.year""}, template=array,\n    ... )\n    <xarray.DataArray (time: 24)>\n    array([ 0.15361741, -0.25671244, -0.31600032,  0.008463  ,  0.1766172 ,\n           -0.11974531,  0.43791243,  0.14197797, -0.06191987, -0.15073425,\n           -0.19967375,  0.18619794, -0.05100474, -0.42989909, -0.09153273,\n            0.24841842, -0.30708526, -0.31412523,  0.04197439,  0.0422506 ,\n            0.14482397,  0.35985481,  0.23487834,  0.12144652])\n    Coordinates:\n        * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n    """"""\n\n    def _wrapper(\n        func: Callable,\n        args: List,\n        kwargs: dict,\n        arg_is_array: Iterable[bool],\n        expected: dict,\n    ):\n        """"""\n        Wrapper function that receives datasets in args; converts to dataarrays when necessary;\n        passes these to the user function `func` and checks returned objects for expected shapes/sizes/etc.\n        """"""\n\n        converted_args = [\n            dataset_to_dataarray(arg) if is_array else arg\n            for is_array, arg in zip(arg_is_array, args)\n        ]\n\n        result = func(*converted_args, **kwargs)\n\n        # check all dims are present\n        missing_dimensions = set(expected[""shapes""]) - set(result.sizes)\n        if missing_dimensions:\n            raise ValueError(\n                f""Dimensions {missing_dimensions} missing on returned object.""\n            )\n\n        # check that index lengths and values are as expected\n        for name, index in result.indexes.items():\n            if name in expected[""shapes""]:\n                if len(index) != expected[""shapes""][name]:\n                    raise ValueError(\n                        f""Received dimension {name!r} of length {len(index)}. Expected length {expected[\'shapes\'][name]}.""\n                    )\n            if name in expected[""indexes""]:\n                expected_index = expected[""indexes""][name]\n                if not index.equals(expected_index):\n                    raise ValueError(\n                        f""Expected index {name!r} to be {expected_index!r}. Received {index!r} instead.""\n                    )\n\n        # check that all expected variables were returned\n        check_result_variables(result, expected, ""coords"")\n        if isinstance(result, Dataset):\n            check_result_variables(result, expected, ""data_vars"")\n\n        return make_dict(result)\n\n    if template is not None and not isinstance(template, (DataArray, Dataset)):\n        raise TypeError(\n            f""template must be a DataArray or Dataset. Received {type(template).__name__} instead.""\n        )\n    if not isinstance(args, Sequence):\n        raise TypeError(""args must be a sequence (for example, a list or tuple)."")\n    if kwargs is None:\n        kwargs = {}\n    elif not isinstance(kwargs, Mapping):\n        raise TypeError(""kwargs must be a mapping (for example, a dict)"")\n\n    for value in kwargs.values():\n        if dask.is_dask_collection(value):\n            raise TypeError(\n                ""Cannot pass dask collections in kwargs yet. Please compute or ""\n                ""load values before passing to map_blocks.""\n            )\n\n    if not dask.is_dask_collection(obj):\n        return func(obj, *args, **kwargs)\n\n    npargs = to_object_array([obj] + list(args))\n    is_xarray = [isinstance(arg, (Dataset, DataArray)) for arg in npargs]\n    is_array = [isinstance(arg, DataArray) for arg in npargs]\n\n    # all xarray objects must be aligned. This is consistent with apply_ufunc.\n    aligned = align(*npargs[is_xarray], join=""exact"")\n    # assigning to object arrays works better when RHS is object array\n    # https://stackoverflow.com/questions/43645135/boolean-indexing-assignment-of-a-numpy-array-to-a-numpy-array\n    npargs[is_xarray] = to_object_array(aligned)\n    npargs[is_array] = to_object_array(\n        [dataarray_to_dataset(da) for da in npargs[is_array]]\n    )\n\n    # check that chunk sizes are compatible\n    input_chunks = dict(npargs[0].chunks)\n    input_indexes = dict(npargs[0].indexes)\n    for arg in npargs[1:][is_xarray[1:]]:\n        assert_chunks_compatible(npargs[0], arg)\n        input_chunks.update(arg.chunks)\n        input_indexes.update(arg.indexes)\n\n    if template is None:\n        # infer template by providing zero-shaped arrays\n        template = infer_template(func, aligned[0], *args, **kwargs)\n        template_indexes = set(template.indexes)\n        preserved_indexes = template_indexes & set(input_indexes)\n        new_indexes = template_indexes - set(input_indexes)\n        indexes = {dim: input_indexes[dim] for dim in preserved_indexes}\n        indexes.update({k: template.indexes[k] for k in new_indexes})\n        output_chunks = {\n            dim: input_chunks[dim] for dim in template.dims if dim in input_chunks\n        }\n\n    else:\n        # template xarray object has been provided with proper sizes and chunk shapes\n        indexes = dict(template.indexes)\n        if isinstance(template, DataArray):\n            output_chunks = dict(zip(template.dims, template.chunks))  # type: ignore\n        else:\n            output_chunks = dict(template.chunks)\n\n    for dim in output_chunks:\n        if dim in input_chunks and len(input_chunks[dim]) != len(output_chunks[dim]):\n            raise ValueError(\n                ""map_blocks requires that one block of the input maps to one block of output. ""\n                f""Expected number of output chunks along dimension {dim!r} to be {len(input_chunks[dim])}. ""\n                f""Received {len(output_chunks[dim])} instead. Please provide template if not provided, or ""\n                ""fix the provided template.""\n            )\n\n    if isinstance(template, DataArray):\n        result_is_array = True\n        template_name = template.name\n        template = template._to_temp_dataset()\n    elif isinstance(template, Dataset):\n        result_is_array = False\n    else:\n        raise TypeError(\n            f""func output must be DataArray or Dataset; got {type(template)}""\n        )\n\n    # We\'re building a new HighLevelGraph hlg. We\'ll have one new layer\n    # for each variable in the dataset, which is the result of the\n    # func applied to the values.\n\n    graph: Dict[Any, Any] = {}\n    new_layers: DefaultDict[str, Dict[Any, Any]] = collections.defaultdict(dict)\n    gname = ""{}-{}"".format(\n        dask.utils.funcname(func), dask.base.tokenize(npargs[0], args, kwargs)\n    )\n\n    # map dims to list of chunk indexes\n    ichunk = {dim: range(len(chunks_v)) for dim, chunks_v in input_chunks.items()}\n    # mapping from chunk index to slice bounds\n    input_chunk_bounds = {\n        dim: np.cumsum((0,) + chunks_v) for dim, chunks_v in input_chunks.items()\n    }\n    output_chunk_bounds = {\n        dim: np.cumsum((0,) + chunks_v) for dim, chunks_v in output_chunks.items()\n    }\n\n    def subset_dataset_to_block(\n        graph: dict, gname: str, dataset: Dataset, input_chunk_bounds, chunk_index\n    ):\n        """"""\n        Creates a task that subsets an xarray dataset to a block determined by chunk_index.\n        Block extents are determined by input_chunk_bounds.\n        Also subtasks that subset the constituent variables of a dataset.\n        """"""\n\n        # this will become [[name1, variable1],\n        #                   [name2, variable2],\n        #                   ...]\n        # which is passed to dict and then to Dataset\n        data_vars = []\n        coords = []\n\n        chunk_tuple = tuple(chunk_index.values())\n        for name, variable in dataset.variables.items():\n            # make a task that creates tuple of (dims, chunk)\n            if dask.is_dask_collection(variable.data):\n                # recursively index into dask_keys nested list to get chunk\n                chunk = variable.__dask_keys__()\n                for dim in variable.dims:\n                    chunk = chunk[chunk_index[dim]]\n\n                chunk_variable_task = (f""{gname}-{name}-{chunk[0]}"",) + chunk_tuple\n                graph[chunk_variable_task] = (\n                    tuple,\n                    [variable.dims, chunk, variable.attrs],\n                )\n            else:\n                # non-dask array possibly with dimensions chunked on other variables\n                # index into variable appropriately\n                subsetter = {\n                    dim: _get_chunk_slicer(dim, chunk_index, input_chunk_bounds)\n                    for dim in variable.dims\n                }\n                subset = variable.isel(subsetter)\n                chunk_variable_task = (\n                    ""{}-{}"".format(gname, dask.base.tokenize(subset)),\n                ) + chunk_tuple\n                graph[chunk_variable_task] = (\n                    tuple,\n                    [subset.dims, subset, subset.attrs],\n                )\n\n            # this task creates dict mapping variable name to above tuple\n            if name in dataset._coord_names:\n                coords.append([name, chunk_variable_task])\n            else:\n                data_vars.append([name, chunk_variable_task])\n\n        return (Dataset, (dict, data_vars), (dict, coords), dataset.attrs)\n\n    # iterate over all possible chunk combinations\n    for chunk_tuple in itertools.product(*ichunk.values()):\n        # mapping from dimension name to chunk index\n        chunk_index = dict(zip(ichunk.keys(), chunk_tuple))\n\n        blocked_args = [\n            subset_dataset_to_block(graph, gname, arg, input_chunk_bounds, chunk_index)\n            if isxr\n            else arg\n            for isxr, arg in zip(is_xarray, npargs)\n        ]\n\n        # expected[""shapes"", ""coords"", ""data_vars"", ""indexes""] are used to\n        # raise nice error messages in _wrapper\n        expected = {}\n        # input chunk 0 along a dimension maps to output chunk 0 along the same dimension\n        # even if length of dimension is changed by the applied function\n        expected[""shapes""] = {\n            k: output_chunks[k][v] for k, v in chunk_index.items() if k in output_chunks\n        }\n        expected[""data_vars""] = set(template.data_vars.keys())  # type: ignore\n        expected[""coords""] = set(template.coords.keys())  # type: ignore\n        expected[""indexes""] = {\n            dim: indexes[dim][_get_chunk_slicer(dim, chunk_index, output_chunk_bounds)]\n            for dim in indexes\n        }\n\n        from_wrapper = (gname,) + chunk_tuple\n        graph[from_wrapper] = (_wrapper, func, blocked_args, kwargs, is_array, expected)\n\n        # mapping from variable name to dask graph key\n        var_key_map: Dict[Hashable, str] = {}\n        for name, variable in template.variables.items():\n            if name in indexes:\n                continue\n            gname_l = f""{gname}-{name}""\n            var_key_map[name] = gname_l\n\n            key: Tuple[Any, ...] = (gname_l,)\n            for dim in variable.dims:\n                if dim in chunk_index:\n                    key += (chunk_index[dim],)\n                else:\n                    # unchunked dimensions in the input have one chunk in the result\n                    # output can have new dimensions with exactly one chunk\n                    key += (0,)\n\n            # We\'re adding multiple new layers to the graph:\n            # The first new layer is the result of the computation on\n            # the array.\n            # Then we add one layer per variable, which extracts the\n            # result for that variable, and depends on just the first new\n            # layer.\n            new_layers[gname_l][key] = (operator.getitem, from_wrapper, name)\n\n    hlg = HighLevelGraph.from_collections(\n        gname,\n        graph,\n        dependencies=[arg for arg in npargs if dask.is_dask_collection(arg)],\n    )\n\n    for gname_l, layer in new_layers.items():\n        # This adds in the getitems for each variable in the dataset.\n        hlg.dependencies[gname_l] = {gname}\n        hlg.layers[gname_l] = layer\n\n    result = Dataset(coords=indexes, attrs=template.attrs)\n    for index in result.indexes:\n        result[index].attrs = template[index].attrs\n        result[index].encoding = template[index].encoding\n\n    for name, gname_l in var_key_map.items():\n        dims = template[name].dims\n        var_chunks = []\n        for dim in dims:\n            if dim in output_chunks:\n                var_chunks.append(output_chunks[dim])\n            elif dim in indexes:\n                var_chunks.append((len(indexes[dim]),))\n            elif dim in template.dims:\n                # new unindexed dimension\n                var_chunks.append((template.sizes[dim],))\n\n        data = dask.array.Array(\n            hlg, name=gname_l, chunks=var_chunks, dtype=template[name].dtype\n        )\n        result[name] = (dims, data, template[name].attrs)\n        result[name].encoding = template[name].encoding\n\n    result = result.set_coords(template._coord_names)\n\n    if result_is_array:\n        da = dataset_to_dataarray(result)\n        da.name = template_name\n        return da  # type: ignore\n    return result  # type: ignore\n'"
xarray/core/pdcompat.py,0,"b'# The remove_unused_levels defined here was copied based on the source code\n# defined in pandas.core.indexes.muli.py\n\n# For reference, here is a copy of the pandas copyright notice:\n\n# (c) 2011-2012, Lambda Foundry, Inc. and PyData Development Team\n# All rights reserved.\n\n# Copyright (c) 2008-2011 AQR Capital Management, LLC\n# All rights reserved.\n\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n\n#     * Redistributions of source code must retain the above copyright\n#        notice, this list of conditions and the following disclaimer.\n\n#     * Redistributions in binary form must reproduce the above\n#        copyright notice, this list of conditions and the following\n#        disclaimer in the documentation and/or other materials provided\n#        with the distribution.\n\n#     * Neither the name of the copyright holder nor the names of any\n#        contributors may be used to endorse or promote products derived\n#        from this software without specific prior written permission.\n\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER AND CONTRIBUTORS\n# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nfrom distutils.version import LooseVersion\n\nimport pandas as pd\n\n# allow ourselves to type checks for Panel even after it\'s removed\nif LooseVersion(pd.__version__) < ""0.25.0"":\n    Panel = pd.Panel\nelse:\n\n    class Panel:  # type: ignore\n        pass\n\n\ndef count_not_none(*args) -> int:\n    """"""Compute the number of non-None arguments.\n\n    Copied from pandas.core.common.count_not_none (not part of the public API)\n    """"""\n    return sum(arg is not None for arg in args)\n'"
xarray/core/pycompat.py,1,"b'import numpy as np\n\ninteger_types = (int, np.integer)\n\ntry:\n    # solely for isinstance checks\n    import dask.array\n\n    dask_array_type = (dask.array.Array,)\nexcept ImportError:  # pragma: no cover\n    dask_array_type = ()\n\ntry:\n    # solely for isinstance checks\n    import sparse\n\n    sparse_array_type = (sparse.SparseArray,)\nexcept ImportError:  # pragma: no cover\n    sparse_array_type = ()\n'"
xarray/core/resample.py,1,"b'import warnings\n\nfrom . import ops\nfrom .groupby import DataArrayGroupBy, DatasetGroupBy\n\nRESAMPLE_DIM = ""__resample_dim__""\n\n\nclass Resample:\n    """"""An object that extends the `GroupBy` object with additional logic\n    for handling specialized re-sampling operations.\n\n    You should create a `Resample` object by using the `DataArray.resample` or\n    `Dataset.resample` methods. The dimension along re-sampling\n\n    See Also\n    --------\n    DataArray.resample\n    Dataset.resample\n\n    """"""\n\n    def _upsample(self, method, *args, **kwargs):\n        """"""Dispatch function to call appropriate up-sampling methods on\n        data.\n\n        This method should not be called directly; instead, use one of the\n        wrapper functions supplied by `Resample`.\n\n        Parameters\n        ----------\n        method : str {\'asfreq\', \'pad\', \'ffill\', \'backfill\', \'bfill\', \'nearest\',\n                 \'interpolate\'}\n            Method to use for up-sampling\n\n        See Also\n        --------\n        Resample.asfreq\n        Resample.pad\n        Resample.backfill\n        Resample.interpolate\n\n        """"""\n\n        upsampled_index = self._full_index\n\n        # Drop non-dimension coordinates along the resampled dimension\n        for k, v in self._obj.coords.items():\n            if k == self._dim:\n                continue\n            if self._dim in v.dims:\n                self._obj = self._obj.drop_vars(k)\n\n        if method == ""asfreq"":\n            return self.mean(self._dim)\n\n        elif method in [""pad"", ""ffill"", ""backfill"", ""bfill"", ""nearest""]:\n            kwargs = kwargs.copy()\n            kwargs.update(**{self._dim: upsampled_index})\n            return self._obj.reindex(method=method, *args, **kwargs)\n\n        elif method == ""interpolate"":\n            return self._interpolate(*args, **kwargs)\n\n        else:\n            raise ValueError(\n                \'Specified method was ""{}"" but must be one of\'\n                \'""asfreq"", ""ffill"", ""bfill"", or ""interpolate""\'.format(method)\n            )\n\n    def asfreq(self):\n        """"""Return values of original object at the new up-sampling frequency;\n        essentially a re-index with new times set to NaN.\n        """"""\n        return self._upsample(""asfreq"")\n\n    def pad(self, tolerance=None):\n        """"""Forward fill new values at up-sampled frequency.\n\n        Parameters\n        ----------\n        tolerance : optional\n            Maximum distance between original and new labels to limit\n            the up-sampling method.\n            Up-sampled data with indices that satisfy the equation\n            ``abs(index[indexer] - target) <= tolerance`` are filled by\n            new values. Data with indices that are outside the given\n            tolerance are filled with ``NaN``  s\n        """"""\n        return self._upsample(""pad"", tolerance=tolerance)\n\n    ffill = pad\n\n    def backfill(self, tolerance=None):\n        """"""Backward fill new values at up-sampled frequency.\n\n        Parameters\n        ----------\n        tolerance : optional\n            Maximum distance between original and new labels to limit\n            the up-sampling method.\n            Up-sampled data with indices that satisfy the equation\n            ``abs(index[indexer] - target) <= tolerance`` are filled by\n            new values. Data with indices that are outside the given\n            tolerance are filled with ``NaN`` s\n        """"""\n        return self._upsample(""backfill"", tolerance=tolerance)\n\n    bfill = backfill\n\n    def nearest(self, tolerance=None):\n        """"""Take new values from nearest original coordinate to up-sampled\n        frequency coordinates.\n\n        Parameters\n        ----------\n        tolerance : optional\n            Maximum distance between original and new labels to limit\n            the up-sampling method.\n            Up-sampled data with indices that satisfy the equation\n            ``abs(index[indexer] - target) <= tolerance`` are filled by\n            new values. Data with indices that are outside the given\n            tolerance are filled with ``NaN`` s\n        """"""\n        return self._upsample(""nearest"", tolerance=tolerance)\n\n    def interpolate(self, kind=""linear""):\n        """"""Interpolate up-sampled data using the original data\n        as knots.\n\n        Parameters\n        ----------\n        kind : str {\'linear\', \'nearest\', \'zero\', \'slinear\',\n               \'quadratic\', \'cubic\'}\n            Interpolation scheme to use\n\n        See Also\n        --------\n        scipy.interpolate.interp1d\n\n        """"""\n        return self._interpolate(kind=kind)\n\n    def _interpolate(self, kind=""linear""):\n        """"""Apply scipy.interpolate.interp1d along resampling dimension.""""""\n        # drop any existing non-dimension coordinates along the resampling\n        # dimension\n        dummy = self._obj.copy()\n        for k, v in self._obj.coords.items():\n            if k != self._dim and self._dim in v.dims:\n                dummy = dummy.drop_vars(k)\n        return dummy.interp(\n            assume_sorted=True,\n            method=kind,\n            kwargs={""bounds_error"": False},\n            **{self._dim: self._full_index},\n        )\n\n\nclass DataArrayResample(DataArrayGroupBy, Resample):\n    """"""DataArrayGroupBy object specialized to time resampling operations over a\n    specified dimension\n    """"""\n\n    def __init__(self, *args, dim=None, resample_dim=None, **kwargs):\n\n        if dim == resample_dim:\n            raise ValueError(\n                ""Proxy resampling dimension (\'{}\') ""\n                ""cannot have the same name as actual dimension ""\n                ""(\'{}\')! "".format(resample_dim, dim)\n            )\n        self._dim = dim\n        self._resample_dim = resample_dim\n\n        super().__init__(*args, **kwargs)\n\n    def map(self, func, shortcut=False, args=(), **kwargs):\n        """"""Apply a function to each array in the group and concatenate them\n        together into a new array.\n\n        `func` is called like `func(ar, *args, **kwargs)` for each array `ar`\n        in this group.\n\n        Apply uses heuristics (like `pandas.GroupBy.apply`) to figure out how\n        to stack together the array. The rule is:\n\n        1. If the dimension along which the group coordinate is defined is\n           still in the first grouped array after applying `func`, then stack\n           over this dimension.\n        2. Otherwise, stack over the new dimension given by name of this\n           grouping (the argument to the `groupby` function).\n\n        Parameters\n        ----------\n        func : function\n            Callable to apply to each array.\n        shortcut : bool, optional\n            Whether or not to shortcut evaluation under the assumptions that:\n\n            (1) The action of `func` does not depend on any of the array\n                metadata (attributes or coordinates) but only on the data and\n                dimensions.\n            (2) The action of `func` creates arrays with homogeneous metadata,\n                that is, with the same dimensions and attributes.\n\n            If these conditions are satisfied `shortcut` provides significant\n            speedup. This should be the case for many common groupby operations\n            (e.g., applying numpy ufuncs).\n        args : tuple, optional\n            Positional arguments passed on to `func`.\n        **kwargs\n            Used to call `func(ar, **kwargs)` for each array `ar`.\n\n        Returns\n        -------\n        applied : DataArray or DataArray\n            The result of splitting, applying and combining this array.\n        """"""\n        # TODO: the argument order for Resample doesn\'t match that for its parent,\n        # GroupBy\n        combined = super().map(func, shortcut=shortcut, args=args, **kwargs)\n\n        # If the aggregation function didn\'t drop the original resampling\n        # dimension, then we need to do so before we can rename the proxy\n        # dimension we used.\n        if self._dim in combined.coords:\n            combined = combined.drop_vars(self._dim)\n\n        if self._resample_dim in combined.dims:\n            combined = combined.rename({self._resample_dim: self._dim})\n\n        return combined\n\n    def apply(self, func, args=(), shortcut=None, **kwargs):\n        """"""\n        Backward compatible implementation of ``map``\n\n        See Also\n        --------\n        DataArrayResample.map\n        """"""\n        warnings.warn(\n            ""Resample.apply may be deprecated in the future. Using Resample.map is encouraged"",\n            PendingDeprecationWarning,\n            stacklevel=2,\n        )\n        return self.map(func=func, shortcut=shortcut, args=args, **kwargs)\n\n\nops.inject_reduce_methods(DataArrayResample)\nops.inject_binary_ops(DataArrayResample)\n\n\nclass DatasetResample(DatasetGroupBy, Resample):\n    """"""DatasetGroupBy object specialized to resampling a specified dimension\n    """"""\n\n    def __init__(self, *args, dim=None, resample_dim=None, **kwargs):\n\n        if dim == resample_dim:\n            raise ValueError(\n                ""Proxy resampling dimension (\'{}\') ""\n                ""cannot have the same name as actual dimension ""\n                ""(\'{}\')! "".format(resample_dim, dim)\n            )\n        self._dim = dim\n        self._resample_dim = resample_dim\n\n        super().__init__(*args, **kwargs)\n\n    def map(self, func, args=(), shortcut=None, **kwargs):\n        """"""Apply a function over each Dataset in the groups generated for\n        resampling  and concatenate them together into a new Dataset.\n\n        `func` is called like `func(ds, *args, **kwargs)` for each dataset `ds`\n        in this group.\n\n        Apply uses heuristics (like `pandas.GroupBy.apply`) to figure out how\n        to stack together the datasets. The rule is:\n\n        1. If the dimension along which the group coordinate is defined is\n           still in the first grouped item after applying `func`, then stack\n           over this dimension.\n        2. Otherwise, stack over the new dimension given by name of this\n           grouping (the argument to the `groupby` function).\n\n        Parameters\n        ----------\n        func : function\n            Callable to apply to each sub-dataset.\n        args : tuple, optional\n            Positional arguments passed on to `func`.\n        **kwargs\n            Used to call `func(ds, **kwargs)` for each sub-dataset `ar`.\n\n        Returns\n        -------\n        applied : Dataset or DataArray\n            The result of splitting, applying and combining this dataset.\n        """"""\n        # ignore shortcut if set (for now)\n        applied = (func(ds, *args, **kwargs) for ds in self._iter_grouped())\n        combined = self._combine(applied)\n\n        return combined.rename({self._resample_dim: self._dim})\n\n    def apply(self, func, args=(), shortcut=None, **kwargs):\n        """"""\n        Backward compatible implementation of ``map``\n\n        See Also\n        --------\n        DataSetResample.map\n        """"""\n\n        warnings.warn(\n            ""Resample.apply may be deprecated in the future. Using Resample.map is encouraged"",\n            PendingDeprecationWarning,\n            stacklevel=2,\n        )\n        return self.map(func=func, shortcut=shortcut, args=args, **kwargs)\n\n    def reduce(self, func, dim=None, keep_attrs=None, **kwargs):\n        """"""Reduce the items in this group by applying `func` along the\n        pre-defined resampling dimension.\n\n        Parameters\n        ----------\n        func : function\n            Function which can be called in the form\n            `func(x, axis=axis, **kwargs)` to return the result of collapsing\n            an np.ndarray over an integer valued axis.\n        dim : str or sequence of str, optional\n            Dimension(s) over which to apply `func`.\n        keep_attrs : bool, optional\n            If True, the datasets\'s attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n        **kwargs : dict\n            Additional keyword arguments passed on to `func`.\n\n        Returns\n        -------\n        reduced : Array\n            Array with summarized data and the indicated dimension(s)\n            removed.\n        """"""\n        return super().reduce(func, dim, keep_attrs, **kwargs)\n\n\nops.inject_reduce_methods(DatasetResample)\nops.inject_binary_ops(DatasetResample)\n'"
xarray/core/resample_cftime.py,2,"b'""""""Resampling for CFTimeIndex. Does not support non-integer freq.""""""\n# The mechanisms for resampling CFTimeIndex was copied and adapted from\n# the source code defined in pandas.core.resample\n#\n# For reference, here is a copy of the pandas copyright notice:\n#\n# BSD 3-Clause License\n#\n# Copyright (c) 2008-2012, AQR Capital Management, LLC, Lambda Foundry, Inc.\n# and PyData Development Team\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n# * Redistributions of source code must retain the above copyright notice, this\n#   list of conditions and the following disclaimer.\n#\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n#\n# * Neither the name of the copyright holder nor the names of its\n#   contributors may be used to endorse or promote products derived from\n#   this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\nimport datetime\n\nimport numpy as np\nimport pandas as pd\n\nfrom ..coding.cftime_offsets import (\n    CFTIME_TICKS,\n    Day,\n    MonthEnd,\n    QuarterEnd,\n    YearEnd,\n    cftime_range,\n    normalize_date,\n    to_offset,\n)\nfrom ..coding.cftimeindex import CFTimeIndex\n\n\nclass CFTimeGrouper:\n    """"""This is a simple container for the grouping parameters that implements a\n    single method, the only one required for resampling in xarray.  It cannot\n    be used in a call to groupby like a pandas.Grouper object can.""""""\n\n    def __init__(self, freq, closed=None, label=None, base=0, loffset=None):\n        self.freq = to_offset(freq)\n        self.closed = closed\n        self.label = label\n        self.base = base\n        self.loffset = loffset\n\n        if isinstance(self.freq, (MonthEnd, QuarterEnd, YearEnd)):\n            if self.closed is None:\n                self.closed = ""right""\n            if self.label is None:\n                self.label = ""right""\n        else:\n            if self.closed is None:\n                self.closed = ""left""\n            if self.label is None:\n                self.label = ""left""\n\n    def first_items(self, index):\n        """"""Meant to reproduce the results of the following\n\n        grouper = pandas.Grouper(...)\n        first_items = pd.Series(np.arange(len(index)),\n                                index).groupby(grouper).first()\n\n        with index being a CFTimeIndex instead of a DatetimeIndex.\n        """"""\n\n        datetime_bins, labels = _get_time_bins(\n            index, self.freq, self.closed, self.label, self.base\n        )\n        if self.loffset is not None:\n            if isinstance(self.loffset, datetime.timedelta):\n                labels = labels + self.loffset\n            else:\n                labels = labels + to_offset(self.loffset)\n\n        # check binner fits data\n        if index[0] < datetime_bins[0]:\n            raise ValueError(""Value falls before first bin"")\n        if index[-1] > datetime_bins[-1]:\n            raise ValueError(""Value falls after last bin"")\n\n        integer_bins = np.searchsorted(index, datetime_bins, side=self.closed)[:-1]\n        first_items = pd.Series(integer_bins, labels)\n\n        # Mask duplicate values with NaNs, preserving the last values\n        non_duplicate = ~first_items.duplicated(""last"")\n        return first_items.where(non_duplicate)\n\n\ndef _get_time_bins(index, freq, closed, label, base):\n    """"""Obtain the bins and their respective labels for resampling operations.\n\n    Parameters\n    ----------\n    index : CFTimeIndex\n        Index object to be resampled (e.g., CFTimeIndex named \'time\').\n    freq : xarray.coding.cftime_offsets.BaseCFTimeOffset\n        The offset object representing target conversion a.k.a. resampling\n        frequency (e.g., \'MS\', \'2D\', \'H\', or \'3T\' with\n        coding.cftime_offsets.to_offset() applied to it).\n    closed : \'left\' or \'right\', optional\n        Which side of bin interval is closed.\n        The default is \'left\' for all frequency offsets except for \'M\' and \'A\',\n        which have a default of \'right\'.\n    label : \'left\' or \'right\', optional\n        Which bin edge label to label bucket with.\n        The default is \'left\' for all frequency offsets except for \'M\' and \'A\',\n        which have a default of \'right\'.\n    base : int, optional\n        For frequencies that evenly subdivide 1 day, the ""origin"" of the\n        aggregated intervals. For example, for \'5min\' frequency, base could\n        range from 0 through 4. Defaults to 0.\n\n    Returns\n    -------\n    datetime_bins : CFTimeIndex\n        Defines the edge of resampling bins by which original index values will\n        be grouped into.\n    labels : CFTimeIndex\n        Define what the user actually sees the bins labeled as.\n    """"""\n\n    if not isinstance(index, CFTimeIndex):\n        raise TypeError(\n            ""index must be a CFTimeIndex, but got ""\n            ""an instance of %r"" % type(index).__name__\n        )\n    if len(index) == 0:\n        datetime_bins = labels = CFTimeIndex(data=[], name=index.name)\n        return datetime_bins, labels\n\n    first, last = _get_range_edges(\n        index.min(), index.max(), freq, closed=closed, base=base\n    )\n    datetime_bins = labels = cftime_range(\n        freq=freq, start=first, end=last, name=index.name\n    )\n\n    datetime_bins, labels = _adjust_bin_edges(\n        datetime_bins, freq, closed, index, labels\n    )\n\n    if label == ""right"":\n        labels = labels[1:]\n    else:\n        labels = labels[:-1]\n\n    # TODO: when CFTimeIndex supports missing values, if the reference index\n    # contains missing values, insert the appropriate NaN value at the\n    # beginning of the datetime_bins and labels indexes.\n\n    return datetime_bins, labels\n\n\ndef _adjust_bin_edges(datetime_bins, offset, closed, index, labels):\n    """"""This is required for determining the bin edges resampling with\n    daily frequencies greater than one day, month end, and year end\n    frequencies.\n\n    Consider the following example.  Let\'s say you want to downsample the\n    time series with the following coordinates to month end frequency:\n\n    CFTimeIndex([2000-01-01 12:00:00, 2000-01-31 12:00:00,\n                 2000-02-01 12:00:00], dtype=\'object\')\n\n    Without this adjustment, _get_time_bins with month-end frequency will\n    return the following index for the bin edges (default closed=\'right\' and\n    label=\'right\' in this case):\n\n    CFTimeIndex([1999-12-31 00:00:00, 2000-01-31 00:00:00,\n                 2000-02-29 00:00:00], dtype=\'object\')\n\n    If 2000-01-31 is used as a bound for a bin, the value on\n    2000-01-31T12:00:00 (at noon on January 31st), will not be included in the\n    month of January.  To account for this, pandas adds a day minus one worth\n    of microseconds to the bin edges generated by cftime range, so that we do\n    bin the value at noon on January 31st in the January bin.  This results in\n    an index with bin edges like the following:\n\n    CFTimeIndex([1999-12-31 23:59:59, 2000-01-31 23:59:59,\n                 2000-02-29 23:59:59], dtype=\'object\')\n\n    The labels are still:\n\n    CFTimeIndex([2000-01-31 00:00:00, 2000-02-29 00:00:00], dtype=\'object\')\n\n    This is also required for daily frequencies longer than one day and\n    year-end frequencies.\n    """"""\n    is_super_daily = isinstance(offset, (MonthEnd, QuarterEnd, YearEnd)) or (\n        isinstance(offset, Day) and offset.n > 1\n    )\n    if is_super_daily:\n        if closed == ""right"":\n            datetime_bins = datetime_bins + datetime.timedelta(days=1, microseconds=-1)\n        if datetime_bins[-2] > index.max():\n            datetime_bins = datetime_bins[:-1]\n            labels = labels[:-1]\n\n    return datetime_bins, labels\n\n\ndef _get_range_edges(first, last, offset, closed=""left"", base=0):\n    """""" Get the correct starting and ending datetimes for the resampled\n    CFTimeIndex range.\n\n    Parameters\n    ----------\n    first : cftime.datetime\n        Uncorrected starting datetime object for resampled CFTimeIndex range.\n        Usually the min of the original CFTimeIndex.\n    last : cftime.datetime\n        Uncorrected ending datetime object for resampled CFTimeIndex range.\n        Usually the max of the original CFTimeIndex.\n    offset : xarray.coding.cftime_offsets.BaseCFTimeOffset\n        The offset object representing target conversion a.k.a. resampling\n        frequency. Contains information on offset type (e.g. Day or \'D\') and\n        offset magnitude (e.g., n = 3).\n    closed : \'left\' or \'right\', optional\n        Which side of bin interval is closed. Defaults to \'left\'.\n    base : int, optional\n        For frequencies that evenly subdivide 1 day, the ""origin"" of the\n        aggregated intervals. For example, for \'5min\' frequency, base could\n        range from 0 through 4. Defaults to 0.\n\n    Returns\n    -------\n    first : cftime.datetime\n        Corrected starting datetime object for resampled CFTimeIndex range.\n    last : cftime.datetime\n        Corrected ending datetime object for resampled CFTimeIndex range.\n    """"""\n    if isinstance(offset, CFTIME_TICKS):\n        first, last = _adjust_dates_anchored(\n            first, last, offset, closed=closed, base=base\n        )\n        return first, last\n    else:\n        first = normalize_date(first)\n        last = normalize_date(last)\n\n    if closed == ""left"":\n        first = offset.rollback(first)\n    else:\n        first = first - offset\n\n    last = last + offset\n    return first, last\n\n\ndef _adjust_dates_anchored(first, last, offset, closed=""right"", base=0):\n    """""" First and last offsets should be calculated from the start day to fix\n    an error cause by resampling across multiple days when a one day period is\n    not a multiple of the frequency.\n    See https://github.com/pandas-dev/pandas/issues/8683\n\n    Parameters\n    ----------\n    first : cftime.datetime\n        A datetime object representing the start of a CFTimeIndex range.\n    last : cftime.datetime\n        A datetime object representing the end of a CFTimeIndex range.\n    offset : xarray.coding.cftime_offsets.BaseCFTimeOffset\n        The offset object representing target conversion a.k.a. resampling\n        frequency. Contains information on offset type (e.g. Day or \'D\') and\n        offset magnitude (e.g., n = 3).\n    closed : \'left\' or \'right\', optional\n        Which side of bin interval is closed. Defaults to \'right\'.\n    base : int, optional\n        For frequencies that evenly subdivide 1 day, the ""origin"" of the\n        aggregated intervals. For example, for \'5min\' frequency, base could\n        range from 0 through 4. Defaults to 0.\n\n    Returns\n    -------\n    fresult : cftime.datetime\n        A datetime object representing the start of a date range that has been\n        adjusted to fix resampling errors.\n    lresult : cftime.datetime\n        A datetime object representing the end of a date range that has been\n        adjusted to fix resampling errors.\n    """"""\n\n    base = base % offset.n\n    start_day = normalize_date(first)\n    base_td = type(offset)(n=base).as_timedelta()\n    start_day += base_td\n    foffset = exact_cftime_datetime_difference(start_day, first) % offset.as_timedelta()\n    loffset = exact_cftime_datetime_difference(start_day, last) % offset.as_timedelta()\n    if closed == ""right"":\n        if foffset.total_seconds() > 0:\n            fresult = first - foffset\n        else:\n            fresult = first - offset.as_timedelta()\n\n        if loffset.total_seconds() > 0:\n            lresult = last + (offset.as_timedelta() - loffset)\n        else:\n            lresult = last\n    else:\n        if foffset.total_seconds() > 0:\n            fresult = first - foffset\n        else:\n            fresult = first\n\n        if loffset.total_seconds() > 0:\n            lresult = last + (offset.as_timedelta() - loffset)\n        else:\n            lresult = last + offset.as_timedelta()\n    return fresult, lresult\n\n\ndef exact_cftime_datetime_difference(a, b):\n    """"""Exact computation of b - a\n\n    Assumes:\n\n        a = a_0 + a_m\n        b = b_0 + b_m\n\n    Here a_0, and b_0 represent the input dates rounded\n    down to the nearest second, and a_m, and b_m represent\n    the remaining microseconds associated with date a and\n    date b.\n\n    We can then express the value of b - a as:\n\n        b - a = (b_0 + b_m) - (a_0 + a_m) = b_0 - a_0 + b_m - a_m\n\n    By construction, we know that b_0 - a_0 must be a round number\n    of seconds.  Therefore we can take the result of b_0 - a_0 using\n    ordinary cftime.datetime arithmetic and round to the nearest\n    second.  b_m - a_m is the remainder, in microseconds, and we\n    can simply add this to the rounded timedelta.\n\n    Parameters\n    ----------\n    a : cftime.datetime\n        Input datetime\n    b : cftime.datetime\n        Input datetime\n\n    Returns\n    -------\n    datetime.timedelta\n    """"""\n    seconds = b.replace(microsecond=0) - a.replace(microsecond=0)\n    seconds = int(round(seconds.total_seconds()))\n    microseconds = b.microsecond - a.microsecond\n    return datetime.timedelta(seconds=seconds, microseconds=microseconds)\n'"
xarray/core/rolling.py,14,"b'import functools\nimport warnings\nfrom typing import Any, Callable, Dict\n\nimport numpy as np\n\nfrom . import dtypes, duck_array_ops, utils\nfrom .dask_array_ops import dask_rolling_wrapper\nfrom .ops import inject_reduce_methods\nfrom .options import _get_keep_attrs\nfrom .pycompat import dask_array_type\n\ntry:\n    import bottleneck\nexcept ImportError:\n    # use numpy methods instead\n    bottleneck = None\n\n\n_ROLLING_REDUCE_DOCSTRING_TEMPLATE = """"""\\\nReduce this object\'s data windows by applying `{name}` along its dimension.\n\nParameters\n----------\n**kwargs : dict\n    Additional keyword arguments passed on to `{name}`.\n\nReturns\n-------\nreduced : same type as caller\n    New object with `{name}` applied along its rolling dimnension.\n""""""\n\n\nclass Rolling:\n    """"""A object that implements the moving window pattern.\n\n    See Also\n    --------\n    Dataset.groupby\n    DataArray.groupby\n    Dataset.rolling\n    DataArray.rolling\n    """"""\n\n    __slots__ = (""obj"", ""window"", ""min_periods"", ""center"", ""dim"", ""keep_attrs"")\n    _attributes = (""window"", ""min_periods"", ""center"", ""dim"", ""keep_attrs"")\n\n    def __init__(self, obj, windows, min_periods=None, center=False, keep_attrs=None):\n        """"""\n        Moving window object.\n\n        Parameters\n        ----------\n        obj : Dataset or DataArray\n            Object to window.\n        windows : A mapping from a dimension name to window size\n            dim : str\n                Name of the dimension to create the rolling iterator\n                along (e.g., `time`).\n            window : int\n                Size of the moving window.\n        min_periods : int, default None\n            Minimum number of observations in window required to have a value\n            (otherwise result is NA). The default, None, is equivalent to\n            setting min_periods equal to the size of the window.\n        center : boolean, default False\n            Set the labels at the center of the window.\n        keep_attrs : bool, optional\n            If True, the object\'s attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n\n        Returns\n        -------\n        rolling : type of input argument\n        """"""\n        if len(windows) != 1:\n            raise ValueError(""exactly one dim/window should be provided"")\n\n        dim, window = next(iter(windows.items()))\n\n        if window <= 0:\n            raise ValueError(""window must be > 0"")\n\n        self.obj = obj\n\n        # attributes\n        self.window = window\n        if min_periods is not None and min_periods <= 0:\n            raise ValueError(""min_periods must be greater than zero or None"")\n        self.min_periods = min_periods\n\n        self.center = center\n        self.dim = dim\n\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n        self.keep_attrs = keep_attrs\n\n    @property\n    def _min_periods(self):\n        return self.min_periods if self.min_periods is not None else self.window\n\n    def __repr__(self):\n        """"""provide a nice str repr of our rolling object""""""\n\n        attrs = [\n            ""{k}->{v}"".format(k=k, v=getattr(self, k))\n            for k in self._attributes\n            if getattr(self, k, None) is not None\n        ]\n        return ""{klass} [{attrs}]"".format(\n            klass=self.__class__.__name__, attrs="","".join(attrs)\n        )\n\n    def __len__(self):\n        return self.obj.sizes[self.dim]\n\n    def _reduce_method(name: str) -> Callable:  # type: ignore\n        array_agg_func = getattr(duck_array_ops, name)\n        bottleneck_move_func = getattr(bottleneck, ""move_"" + name, None)\n\n        def method(self, **kwargs):\n            return self._numpy_or_bottleneck_reduce(\n                array_agg_func, bottleneck_move_func, **kwargs\n            )\n\n        method.__name__ = name\n        method.__doc__ = _ROLLING_REDUCE_DOCSTRING_TEMPLATE.format(name=name)\n        return method\n\n    argmax = _reduce_method(""argmax"")\n    argmin = _reduce_method(""argmin"")\n    max = _reduce_method(""max"")\n    min = _reduce_method(""min"")\n    mean = _reduce_method(""mean"")\n    prod = _reduce_method(""prod"")\n    sum = _reduce_method(""sum"")\n    std = _reduce_method(""std"")\n    var = _reduce_method(""var"")\n    median = _reduce_method(""median"")\n\n    def count(self):\n        rolling_count = self._counts()\n        enough_periods = rolling_count >= self._min_periods\n        return rolling_count.where(enough_periods)\n\n    count.__doc__ = _ROLLING_REDUCE_DOCSTRING_TEMPLATE.format(name=""count"")\n\n\nclass DataArrayRolling(Rolling):\n    __slots__ = (""window_labels"",)\n\n    def __init__(self, obj, windows, min_periods=None, center=False, keep_attrs=None):\n        """"""\n        Moving window object for DataArray.\n        You should use DataArray.rolling() method to construct this object\n        instead of the class constructor.\n\n        Parameters\n        ----------\n        obj : DataArray\n            Object to window.\n        windows : A mapping from a dimension name to window size\n            dim : str\n                Name of the dimension to create the rolling iterator\n                along (e.g., `time`).\n            window : int\n                Size of the moving window.\n        min_periods : int, default None\n            Minimum number of observations in window required to have a value\n            (otherwise result is NA). The default, None, is equivalent to\n            setting min_periods equal to the size of the window.\n        center : boolean, default False\n            Set the labels at the center of the window.\n        keep_attrs : bool, optional\n            If True, the object\'s attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n\n        Returns\n        -------\n        rolling : type of input argument\n\n        See Also\n        --------\n        DataArray.rolling\n        DataArray.groupby\n        Dataset.rolling\n        Dataset.groupby\n        """"""\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n        super().__init__(\n            obj, windows, min_periods=min_periods, center=center, keep_attrs=keep_attrs\n        )\n\n        self.window_labels = self.obj[self.dim]\n\n    def __iter__(self):\n        stops = np.arange(1, len(self.window_labels) + 1)\n        starts = stops - int(self.window)\n        starts[: int(self.window)] = 0\n        for (label, start, stop) in zip(self.window_labels, starts, stops):\n            window = self.obj.isel(**{self.dim: slice(start, stop)})\n\n            counts = window.count(dim=self.dim)\n            window = window.where(counts >= self._min_periods)\n\n            yield (label, window)\n\n    def construct(self, window_dim, stride=1, fill_value=dtypes.NA):\n        """"""\n        Convert this rolling object to xr.DataArray,\n        where the window dimension is stacked as a new dimension\n\n        Parameters\n        ----------\n        window_dim: str\n            New name of the window dimension.\n        stride: integer, optional\n            Size of stride for the rolling window.\n        fill_value: optional. Default dtypes.NA\n            Filling value to match the dimension size.\n\n        Returns\n        -------\n        DataArray that is a view of the original array. The returned array is\n        not writeable.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(np.arange(8).reshape(2, 4), dims=(""a"", ""b""))\n\n        >>> rolling = da.rolling(b=3)\n        >>> rolling.construct(""window_dim"")\n        <xarray.DataArray (a: 2, b: 4, window_dim: 3)>\n        array([[[np.nan, np.nan, 0], [np.nan, 0, 1], [0, 1, 2], [1, 2, 3]],\n               [[np.nan, np.nan, 4], [np.nan, 4, 5], [4, 5, 6], [5, 6, 7]]])\n        Dimensions without coordinates: a, b, window_dim\n\n        >>> rolling = da.rolling(b=3, center=True)\n        >>> rolling.construct(""window_dim"")\n        <xarray.DataArray (a: 2, b: 4, window_dim: 3)>\n        array([[[np.nan, 0, 1], [0, 1, 2], [1, 2, 3], [2, 3, np.nan]],\n               [[np.nan, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, np.nan]]])\n        Dimensions without coordinates: a, b, window_dim\n\n        """"""\n\n        from .dataarray import DataArray\n\n        window = self.obj.variable.rolling_window(\n            self.dim, self.window, window_dim, self.center, fill_value=fill_value\n        )\n        result = DataArray(\n            window, dims=self.obj.dims + (window_dim,), coords=self.obj.coords\n        )\n        return result.isel(**{self.dim: slice(None, None, stride)})\n\n    def reduce(self, func, **kwargs):\n        """"""Reduce the items in this group by applying `func` along some\n        dimension(s).\n\n        Parameters\n        ----------\n        func : function\n            Function which can be called in the form\n            `func(x, **kwargs)` to return the result of collapsing an\n            np.ndarray over an the rolling dimension.\n        **kwargs : dict\n            Additional keyword arguments passed on to `func`.\n\n        Returns\n        -------\n        reduced : DataArray\n            Array with summarized data.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(np.arange(8).reshape(2, 4), dims=(""a"", ""b""))\n        >>> rolling = da.rolling(b=3)\n        >>> rolling.construct(""window_dim"")\n        <xarray.DataArray (a: 2, b: 4, window_dim: 3)>\n        array([[[np.nan, np.nan, 0], [np.nan, 0, 1], [0, 1, 2], [1, 2, 3]],\n               [[np.nan, np.nan, 4], [np.nan, 4, 5], [4, 5, 6], [5, 6, 7]]])\n        Dimensions without coordinates: a, b, window_dim\n\n        >>> rolling.reduce(np.sum)\n        <xarray.DataArray (a: 2, b: 4)>\n        array([[nan, nan,  3.,  6.],\n               [nan, nan, 15., 18.]])\n        Dimensions without coordinates: a, b\n\n        >>> rolling = da.rolling(b=3, min_periods=1)\n        >>> rolling.reduce(np.nansum)\n        <xarray.DataArray (a: 2, b: 4)>\n        array([[ 0.,  1.,  3.,  6.],\n               [ 4.,  9., 15., 18.]])\n\n        """"""\n        rolling_dim = utils.get_temp_dimname(self.obj.dims, ""_rolling_dim"")\n        windows = self.construct(rolling_dim)\n        result = windows.reduce(func, dim=rolling_dim, **kwargs)\n\n        # Find valid windows based on count.\n        counts = self._counts()\n        return result.where(counts >= self._min_periods)\n\n    def _counts(self):\n        """""" Number of non-nan entries in each rolling window. """"""\n\n        rolling_dim = utils.get_temp_dimname(self.obj.dims, ""_rolling_dim"")\n        # We use False as the fill_value instead of np.nan, since boolean\n        # array is faster to be reduced than object array.\n        # The use of skipna==False is also faster since it does not need to\n        # copy the strided array.\n        counts = (\n            self.obj.notnull()\n            .rolling(center=self.center, **{self.dim: self.window})\n            .construct(rolling_dim, fill_value=False)\n            .sum(dim=rolling_dim, skipna=False)\n        )\n        return counts\n\n    def _bottleneck_reduce(self, func, **kwargs):\n        from .dataarray import DataArray\n\n        # bottleneck doesn\'t allow min_count to be 0, although it should\n        # work the same as if min_count = 1\n        if self.min_periods is not None and self.min_periods == 0:\n            min_count = 1\n        else:\n            min_count = self.min_periods\n\n        axis = self.obj.get_axis_num(self.dim)\n\n        padded = self.obj.variable\n        if self.center:\n            if isinstance(padded.data, dask_array_type):\n                # Workaround to make the padded chunk size is larger than\n                # self.window-1\n                shift = -(self.window + 1) // 2\n                offset = (self.window - 1) // 2\n                valid = (slice(None),) * axis + (\n                    slice(offset, offset + self.obj.shape[axis]),\n                )\n            else:\n                shift = (-self.window // 2) + 1\n                valid = (slice(None),) * axis + (slice(-shift, None),)\n            padded = padded.pad({self.dim: (0, -shift)}, mode=""constant"")\n\n        if isinstance(padded.data, dask_array_type):\n            raise AssertionError(""should not be reachable"")\n            values = dask_rolling_wrapper(\n                func, padded.data, window=self.window, min_count=min_count, axis=axis\n            )\n        else:\n            values = func(\n                padded.data, window=self.window, min_count=min_count, axis=axis\n            )\n\n        if self.center:\n            values = values[valid]\n        result = DataArray(values, self.obj.coords)\n\n        return result\n\n    def _numpy_or_bottleneck_reduce(\n        self, array_agg_func, bottleneck_move_func, **kwargs\n    ):\n        if ""dim"" in kwargs:\n            warnings.warn(\n                f""Reductions will be applied along the rolling dimension \'{self.dim}\'. Passing the \'dim\' kwarg to reduction operations has no effect and will raise an error in xarray 0.16.0."",\n                DeprecationWarning,\n                stacklevel=3,\n            )\n            del kwargs[""dim""]\n\n        if bottleneck_move_func is not None and not isinstance(\n            self.obj.data, dask_array_type\n        ):\n            # TODO: renable bottleneck with dask after the issues\n            # underlying https://github.com/pydata/xarray/issues/2940 are\n            # fixed.\n            return self._bottleneck_reduce(bottleneck_move_func, **kwargs)\n        else:\n            return self.reduce(array_agg_func, **kwargs)\n\n\nclass DatasetRolling(Rolling):\n    __slots__ = (""rollings"",)\n\n    def __init__(self, obj, windows, min_periods=None, center=False, keep_attrs=None):\n        """"""\n        Moving window object for Dataset.\n        You should use Dataset.rolling() method to construct this object\n        instead of the class constructor.\n\n        Parameters\n        ----------\n        obj : Dataset\n            Object to window.\n        windows : A mapping from a dimension name to window size\n            dim : str\n                Name of the dimension to create the rolling iterator\n                along (e.g., `time`).\n            window : int\n                Size of the moving window.\n        min_periods : int, default None\n            Minimum number of observations in window required to have a value\n            (otherwise result is NA). The default, None, is equivalent to\n            setting min_periods equal to the size of the window.\n        center : boolean, default False\n            Set the labels at the center of the window.\n        keep_attrs : bool, optional\n            If True, the object\'s attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n\n        Returns\n        -------\n        rolling : type of input argument\n\n        See Also\n        --------\n        Dataset.rolling\n        DataArray.rolling\n        Dataset.groupby\n        DataArray.groupby\n        """"""\n        super().__init__(obj, windows, min_periods, center, keep_attrs)\n        if self.dim not in self.obj.dims:\n            raise KeyError(self.dim)\n        # Keep each Rolling object as a dictionary\n        self.rollings = {}\n        for key, da in self.obj.data_vars.items():\n            # keeps rollings only for the dataset depending on slf.dim\n            if self.dim in da.dims:\n                self.rollings[key] = DataArrayRolling(\n                    da, windows, min_periods, center, keep_attrs\n                )\n\n    def _dataset_implementation(self, func, **kwargs):\n        from .dataset import Dataset\n\n        reduced = {}\n        for key, da in self.obj.data_vars.items():\n            if self.dim in da.dims:\n                reduced[key] = func(self.rollings[key], **kwargs)\n            else:\n                reduced[key] = self.obj[key]\n        attrs = self.obj.attrs if self.keep_attrs else {}\n        return Dataset(reduced, coords=self.obj.coords, attrs=attrs)\n\n    def reduce(self, func, **kwargs):\n        """"""Reduce the items in this group by applying `func` along some\n        dimension(s).\n\n        Parameters\n        ----------\n        func : function\n            Function which can be called in the form\n            `func(x, **kwargs)` to return the result of collapsing an\n            np.ndarray over an the rolling dimension.\n        **kwargs : dict\n            Additional keyword arguments passed on to `func`.\n\n        Returns\n        -------\n        reduced : DataArray\n            Array with summarized data.\n        """"""\n        return self._dataset_implementation(\n            functools.partial(DataArrayRolling.reduce, func=func), **kwargs\n        )\n\n    def _counts(self):\n        return self._dataset_implementation(DataArrayRolling._counts)\n\n    def _numpy_or_bottleneck_reduce(\n        self, array_agg_func, bottleneck_move_func, **kwargs\n    ):\n        return self._dataset_implementation(\n            functools.partial(\n                DataArrayRolling._numpy_or_bottleneck_reduce,\n                array_agg_func=array_agg_func,\n                bottleneck_move_func=bottleneck_move_func,\n            ),\n            **kwargs,\n        )\n\n    def construct(self, window_dim, stride=1, fill_value=dtypes.NA, keep_attrs=None):\n        """"""\n        Convert this rolling object to xr.Dataset,\n        where the window dimension is stacked as a new dimension\n\n        Parameters\n        ----------\n        window_dim: str\n            New name of the window dimension.\n        stride: integer, optional\n            size of stride for the rolling window.\n        fill_value: optional. Default dtypes.NA\n            Filling value to match the dimension size.\n\n        Returns\n        -------\n        Dataset with variables converted from rolling object.\n        """"""\n\n        from .dataset import Dataset\n\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=True)\n\n        dataset = {}\n        for key, da in self.obj.data_vars.items():\n            if self.dim in da.dims:\n                dataset[key] = self.rollings[key].construct(\n                    window_dim, fill_value=fill_value\n                )\n            else:\n                dataset[key] = da\n        return Dataset(dataset, coords=self.obj.coords).isel(\n            **{self.dim: slice(None, None, stride)}\n        )\n\n\nclass Coarsen:\n    """"""A object that implements the coarsen.\n\n    See Also\n    --------\n    Dataset.coarsen\n    DataArray.coarsen\n    """"""\n\n    __slots__ = (\n        ""obj"",\n        ""boundary"",\n        ""coord_func"",\n        ""windows"",\n        ""side"",\n        ""trim_excess"",\n        ""keep_attrs"",\n    )\n    _attributes = (""windows"", ""side"", ""trim_excess"")\n\n    def __init__(self, obj, windows, boundary, side, coord_func, keep_attrs):\n        """"""\n        Moving window object.\n\n        Parameters\n        ----------\n        obj : Dataset or DataArray\n            Object to window.\n        windows : A mapping from a dimension name to window size\n            dim : str\n                Name of the dimension to create the rolling iterator\n                along (e.g., `time`).\n            window : int\n                Size of the moving window.\n        boundary : \'exact\' | \'trim\' | \'pad\'\n            If \'exact\', a ValueError will be raised if dimension size is not a\n            multiple of window size. If \'trim\', the excess indexes are trimed.\n            If \'pad\', NA will be padded.\n        side : \'left\' or \'right\' or mapping from dimension to \'left\' or \'right\'\n        coord_func: mapping from coordinate name to func.\n\n        Returns\n        -------\n        coarsen\n        """"""\n        self.obj = obj\n        self.windows = windows\n        self.side = side\n        self.boundary = boundary\n        self.keep_attrs = keep_attrs\n\n        absent_dims = [dim for dim in windows.keys() if dim not in self.obj.dims]\n        if absent_dims:\n            raise ValueError(\n                f""Dimensions {absent_dims!r} not found in {self.obj.__class__.__name__}.""\n            )\n        if not utils.is_dict_like(coord_func):\n            coord_func = {d: coord_func for d in self.obj.dims}\n        for c in self.obj.coords:\n            if c not in coord_func:\n                coord_func[c] = duck_array_ops.mean\n        self.coord_func = coord_func\n\n    def __repr__(self):\n        """"""provide a nice str repr of our coarsen object""""""\n\n        attrs = [\n            ""{k}->{v}"".format(k=k, v=getattr(self, k))\n            for k in self._attributes\n            if getattr(self, k, None) is not None\n        ]\n        return ""{klass} [{attrs}]"".format(\n            klass=self.__class__.__name__, attrs="","".join(attrs)\n        )\n\n\nclass DataArrayCoarsen(Coarsen):\n    __slots__ = ()\n\n    _reduce_extra_args_docstring = """"""""""""\n\n    @classmethod\n    def _reduce_method(cls, func: Callable, include_skipna: bool, numeric_only: bool):\n        """"""\n        Return a wrapped function for injecting reduction methods.\n        see ops.inject_reduce_methods\n        """"""\n        kwargs: Dict[str, Any] = {}\n        if include_skipna:\n            kwargs[""skipna""] = None\n\n        def wrapped_func(self, **kwargs):\n            from .dataarray import DataArray\n\n            reduced = self.obj.variable.coarsen(\n                self.windows, func, self.boundary, self.side, **kwargs\n            )\n            coords = {}\n            for c, v in self.obj.coords.items():\n                if c == self.obj.name:\n                    coords[c] = reduced\n                else:\n                    if any(d in self.windows for d in v.dims):\n                        coords[c] = v.variable.coarsen(\n                            self.windows,\n                            self.coord_func[c],\n                            self.boundary,\n                            self.side,\n                            **kwargs,\n                        )\n                    else:\n                        coords[c] = v\n            return DataArray(reduced, dims=self.obj.dims, coords=coords)\n\n        return wrapped_func\n\n\nclass DatasetCoarsen(Coarsen):\n    __slots__ = ()\n\n    _reduce_extra_args_docstring = """"""""""""\n\n    @classmethod\n    def _reduce_method(cls, func: Callable, include_skipna: bool, numeric_only: bool):\n        """"""\n        Return a wrapped function for injecting reduction methods.\n        see ops.inject_reduce_methods\n        """"""\n        kwargs: Dict[str, Any] = {}\n        if include_skipna:\n            kwargs[""skipna""] = None\n\n        def wrapped_func(self, **kwargs):\n            from .dataset import Dataset\n\n            if self.keep_attrs:\n                attrs = self.obj.attrs\n            else:\n                attrs = {}\n\n            reduced = {}\n            for key, da in self.obj.data_vars.items():\n                reduced[key] = da.variable.coarsen(\n                    self.windows, func, self.boundary, self.side, **kwargs\n                )\n\n            coords = {}\n            for c, v in self.obj.coords.items():\n                if any(d in self.windows for d in v.dims):\n                    coords[c] = v.variable.coarsen(\n                        self.windows,\n                        self.coord_func[c],\n                        self.boundary,\n                        self.side,\n                        **kwargs,\n                    )\n                else:\n                    coords[c] = v.variable\n            return Dataset(reduced, coords=coords, attrs=attrs)\n\n        return wrapped_func\n\n\ninject_reduce_methods(DataArrayCoarsen)\ninject_reduce_methods(DatasetCoarsen)\n'"
xarray/core/rolling_exp.py,2,"b'import numpy as np\n\nfrom .pdcompat import count_not_none\nfrom .pycompat import dask_array_type\n\n\ndef _get_alpha(com=None, span=None, halflife=None, alpha=None):\n    # pandas defines in terms of com (converting to alpha in the algo)\n    # so use its function to get a com and then convert to alpha\n\n    com = _get_center_of_mass(com, span, halflife, alpha)\n    return 1 / (1 + com)\n\n\ndef move_exp_nanmean(array, *, axis, alpha):\n    if isinstance(array, dask_array_type):\n        raise TypeError(""rolling_exp is not currently support for dask arrays"")\n    import numbagg\n\n    if axis == ():\n        return array.astype(np.float64)\n    else:\n        return numbagg.move_exp_nanmean(array, axis=axis, alpha=alpha)\n\n\ndef _get_center_of_mass(comass, span, halflife, alpha):\n    """"""\n    Vendored from pandas.core.window.common._get_center_of_mass\n\n    See licenses/PANDAS_LICENSE for the function\'s license\n    """"""\n    valid_count = count_not_none(comass, span, halflife, alpha)\n    if valid_count > 1:\n        raise ValueError(""comass, span, halflife, and alpha "" ""are mutually exclusive"")\n\n    # Convert to center of mass; domain checks ensure 0 < alpha <= 1\n    if comass is not None:\n        if comass < 0:\n            raise ValueError(""comass must satisfy: comass >= 0"")\n    elif span is not None:\n        if span < 1:\n            raise ValueError(""span must satisfy: span >= 1"")\n        comass = (span - 1) / 2.0\n    elif halflife is not None:\n        if halflife <= 0:\n            raise ValueError(""halflife must satisfy: halflife > 0"")\n        decay = 1 - np.exp(np.log(0.5) / halflife)\n        comass = 1 / decay - 1\n    elif alpha is not None:\n        if alpha <= 0 or alpha > 1:\n            raise ValueError(""alpha must satisfy: 0 < alpha <= 1"")\n        comass = (1.0 - alpha) / alpha\n    else:\n        raise ValueError(""Must pass one of comass, span, halflife, or alpha"")\n\n    return float(comass)\n\n\nclass RollingExp:\n    """"""\n    Exponentially-weighted moving window object.\n    Similar to EWM in pandas\n\n    Parameters\n    ----------\n    obj : Dataset or DataArray\n        Object to window.\n    windows : A single mapping from a single dimension name to window value\n        dim : str\n            Name of the dimension to create the rolling exponential window\n            along (e.g., `time`).\n        window : int\n            Size of the moving window. The type of this is specified in\n            `window_type`\n    window_type : str, one of [\'span\', \'com\', \'halflife\', \'alpha\'], default \'span\'\n        The format of the previously supplied window. Each is a simple\n        numerical transformation of the others. Described in detail:\n        https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.ewm.html\n\n    Returns\n    -------\n    RollingExp : type of input argument\n    """"""\n\n    def __init__(self, obj, windows, window_type=""span""):\n        self.obj = obj\n        dim, window = next(iter(windows.items()))\n        self.dim = dim\n        self.alpha = _get_alpha(**{window_type: window})\n\n    def mean(self):\n        """"""\n        Exponentially weighted moving average\n\n        Examples\n        --------\n        >>> da = xr.DataArray([1, 1, 2, 2, 2], dims=""x"")\n        >>> da.rolling_exp(x=2, window_type=""span"").mean()\n        <xarray.DataArray (x: 5)>\n        array([1.      , 1.      , 1.692308, 1.9     , 1.966942])\n        Dimensions without coordinates: x\n        """"""\n\n        return self.obj.reduce(move_exp_nanmean, dim=self.dim, alpha=self.alpha)\n'"
xarray/core/utils.py,21,"b'""""""Internal utilties; not for external use\n""""""\nimport contextlib\nimport functools\nimport itertools\nimport os.path\nimport re\nimport warnings\nfrom enum import Enum\nfrom typing import (\n    AbstractSet,\n    Any,\n    Callable,\n    Collection,\n    Container,\n    Dict,\n    Hashable,\n    Iterable,\n    Iterator,\n    Mapping,\n    MutableMapping,\n    MutableSet,\n    Optional,\n    Sequence,\n    Tuple,\n    TypeVar,\n    Union,\n    cast,\n)\n\nimport numpy as np\nimport pandas as pd\n\nK = TypeVar(""K"")\nV = TypeVar(""V"")\nT = TypeVar(""T"")\n\n\ndef _check_inplace(inplace: Optional[bool]) -> None:\n    if inplace is not None:\n        raise TypeError(\n            ""The `inplace` argument has been removed from xarray. ""\n            ""You can achieve an identical effect with python\'s standard assignment.""\n        )\n\n\ndef alias_message(old_name: str, new_name: str) -> str:\n    return f""{old_name} has been deprecated. Use {new_name} instead.""\n\n\ndef alias_warning(old_name: str, new_name: str, stacklevel: int = 3) -> None:\n    warnings.warn(\n        alias_message(old_name, new_name), FutureWarning, stacklevel=stacklevel\n    )\n\n\ndef alias(obj: Callable[..., T], old_name: str) -> Callable[..., T]:\n    assert isinstance(old_name, str)\n\n    @functools.wraps(obj)\n    def wrapper(*args, **kwargs):\n        alias_warning(old_name, obj.__name__)\n        return obj(*args, **kwargs)\n\n    wrapper.__doc__ = alias_message(old_name, obj.__name__)\n    return wrapper\n\n\ndef _maybe_cast_to_cftimeindex(index: pd.Index) -> pd.Index:\n    from ..coding.cftimeindex import CFTimeIndex\n\n    if len(index) > 0 and index.dtype == ""O"":\n        try:\n            return CFTimeIndex(index)\n        except (ImportError, TypeError):\n            return index\n    else:\n        return index\n\n\ndef maybe_cast_to_coords_dtype(label, coords_dtype):\n    if coords_dtype.kind == ""f"" and not isinstance(label, slice):\n        label = np.asarray(label, dtype=coords_dtype)\n    return label\n\n\ndef safe_cast_to_index(array: Any) -> pd.Index:\n    """"""Given an array, safely cast it to a pandas.Index.\n\n    If it is already a pandas.Index, return it unchanged.\n\n    Unlike pandas.Index, if the array has dtype=object or dtype=timedelta64,\n    this function will not attempt to do automatic type conversion but will\n    always return an index with dtype=object.\n    """"""\n    if isinstance(array, pd.Index):\n        index = array\n    elif hasattr(array, ""to_index""):\n        index = array.to_index()\n    else:\n        kwargs = {}\n        if hasattr(array, ""dtype"") and array.dtype.kind == ""O"":\n            kwargs[""dtype""] = object\n        index = pd.Index(np.asarray(array), **kwargs)\n    return _maybe_cast_to_cftimeindex(index)\n\n\ndef multiindex_from_product_levels(\n    levels: Sequence[pd.Index], names: Sequence[str] = None\n) -> pd.MultiIndex:\n    """"""Creating a MultiIndex from a product without refactorizing levels.\n\n    Keeping levels the same gives back the original labels when we unstack.\n\n    Parameters\n    ----------\n    levels : sequence of pd.Index\n        Values for each MultiIndex level.\n    names : optional sequence of objects\n        Names for each level.\n\n    Returns\n    -------\n    pandas.MultiIndex\n    """"""\n    if any(not isinstance(lev, pd.Index) for lev in levels):\n        raise TypeError(""levels must be a list of pd.Index objects"")\n\n    split_labels, levels = zip(*[lev.factorize() for lev in levels])\n    labels_mesh = np.meshgrid(*split_labels, indexing=""ij"")\n    labels = [x.ravel() for x in labels_mesh]\n    return pd.MultiIndex(levels, labels, sortorder=0, names=names)\n\n\ndef maybe_wrap_array(original, new_array):\n    """"""Wrap a transformed array with __array_wrap__ is it can be done safely.\n\n    This lets us treat arbitrary functions that take and return ndarray objects\n    like ufuncs, as long as they return an array with the same shape.\n    """"""\n    # in case func lost array\'s metadata\n    if isinstance(new_array, np.ndarray) and new_array.shape == original.shape:\n        return original.__array_wrap__(new_array)\n    else:\n        return new_array\n\n\ndef equivalent(first: T, second: T) -> bool:\n    """"""Compare two objects for equivalence (identity or equality), using\n    array_equiv if either object is an ndarray. If both objects are lists,\n    equivalent is sequentially called on all the elements.\n    """"""\n    # TODO: refactor to avoid circular import\n    from . import duck_array_ops\n\n    if isinstance(first, np.ndarray) or isinstance(second, np.ndarray):\n        return duck_array_ops.array_equiv(first, second)\n    elif isinstance(first, list) or isinstance(second, list):\n        return list_equiv(first, second)\n    else:\n        return (\n            (first is second)\n            or (first == second)\n            or (pd.isnull(first) and pd.isnull(second))\n        )\n\n\ndef list_equiv(first, second):\n    equiv = True\n    if len(first) != len(second):\n        return False\n    else:\n        for f, s in zip(first, second):\n            equiv = equiv and equivalent(f, s)\n    return equiv\n\n\ndef peek_at(iterable: Iterable[T]) -> Tuple[T, Iterator[T]]:\n    """"""Returns the first value from iterable, as well as a new iterator with\n    the same content as the original iterable\n    """"""\n    gen = iter(iterable)\n    peek = next(gen)\n    return peek, itertools.chain([peek], gen)\n\n\ndef update_safety_check(\n    first_dict: Mapping[K, V],\n    second_dict: Mapping[K, V],\n    compat: Callable[[V, V], bool] = equivalent,\n) -> None:\n    """"""Check the safety of updating one dictionary with another.\n\n    Raises ValueError if dictionaries have non-compatible values for any key,\n    where compatibility is determined by identity (they are the same item) or\n    the `compat` function.\n\n    Parameters\n    ----------\n    first_dict, second_dict : dict-like\n        All items in the second dictionary are checked against for conflicts\n        against items in the first dictionary.\n    compat : function, optional\n        Binary operator to determine if two values are compatible. By default,\n        checks for equivalence.\n    """"""\n    for k, v in second_dict.items():\n        if k in first_dict and not compat(v, first_dict[k]):\n            raise ValueError(\n                ""unsafe to merge dictionaries without ""\n                ""overriding values; conflicting key %r"" % k\n            )\n\n\ndef remove_incompatible_items(\n    first_dict: MutableMapping[K, V],\n    second_dict: Mapping[K, V],\n    compat: Callable[[V, V], bool] = equivalent,\n) -> None:\n    """"""Remove incompatible items from the first dictionary in-place.\n\n    Items are retained if their keys are found in both dictionaries and the\n    values are compatible.\n\n    Parameters\n    ----------\n    first_dict, second_dict : dict-like\n        Mappings to merge.\n    compat : function, optional\n        Binary operator to determine if two values are compatible. By default,\n        checks for equivalence.\n    """"""\n    for k in list(first_dict):\n        if k not in second_dict or not compat(first_dict[k], second_dict[k]):\n            del first_dict[k]\n\n\ndef is_dict_like(value: Any) -> bool:\n    return hasattr(value, ""keys"") and hasattr(value, ""__getitem__"")\n\n\ndef is_full_slice(value: Any) -> bool:\n    return isinstance(value, slice) and value == slice(None)\n\n\ndef is_list_like(value: Any) -> bool:\n    return isinstance(value, list) or isinstance(value, tuple)\n\n\ndef either_dict_or_kwargs(\n    pos_kwargs: Optional[Mapping[Hashable, T]],\n    kw_kwargs: Mapping[str, T],\n    func_name: str,\n) -> Mapping[Hashable, T]:\n    if pos_kwargs is not None:\n        if not is_dict_like(pos_kwargs):\n            raise ValueError(\n                ""the first argument to .%s must be a dictionary"" % func_name\n            )\n        if kw_kwargs:\n            raise ValueError(\n                ""cannot specify both keyword and positional ""\n                ""arguments to .%s"" % func_name\n            )\n        return pos_kwargs\n    else:\n        # Need an explicit cast to appease mypy due to invariance; see\n        # https://github.com/python/mypy/issues/6228\n        return cast(Mapping[Hashable, T], kw_kwargs)\n\n\ndef is_scalar(value: Any, include_0d: bool = True) -> bool:\n    """"""Whether to treat a value as a scalar.\n\n    Any non-iterable, string, or 0-D array\n    """"""\n    from .variable import NON_NUMPY_SUPPORTED_ARRAY_TYPES\n\n    if include_0d:\n        include_0d = getattr(value, ""ndim"", None) == 0\n    return (\n        include_0d\n        or isinstance(value, (str, bytes))\n        or not (\n            isinstance(value, (Iterable,) + NON_NUMPY_SUPPORTED_ARRAY_TYPES)\n            or hasattr(value, ""__array_function__"")\n        )\n    )\n\n\ndef is_valid_numpy_dtype(dtype: Any) -> bool:\n    try:\n        np.dtype(dtype)\n    except (TypeError, ValueError):\n        return False\n    else:\n        return True\n\n\ndef to_0d_object_array(value: Any) -> np.ndarray:\n    """"""Given a value, wrap it in a 0-D numpy.ndarray with dtype=object.\n    """"""\n    result = np.empty((), dtype=object)\n    result[()] = value\n    return result\n\n\ndef to_0d_array(value: Any) -> np.ndarray:\n    """"""Given a value, wrap it in a 0-D numpy.ndarray.\n    """"""\n    if np.isscalar(value) or (isinstance(value, np.ndarray) and value.ndim == 0):\n        return np.array(value)\n    else:\n        return to_0d_object_array(value)\n\n\ndef dict_equiv(\n    first: Mapping[K, V],\n    second: Mapping[K, V],\n    compat: Callable[[V, V], bool] = equivalent,\n) -> bool:\n    """"""Test equivalence of two dict-like objects. If any of the values are\n    numpy arrays, compare them correctly.\n\n    Parameters\n    ----------\n    first, second : dict-like\n        Dictionaries to compare for equality\n    compat : function, optional\n        Binary operator to determine if two values are compatible. By default,\n        checks for equivalence.\n\n    Returns\n    -------\n    equals : bool\n        True if the dictionaries are equal\n    """"""\n    for k in first:\n        if k not in second or not compat(first[k], second[k]):\n            return False\n    for k in second:\n        if k not in first:\n            return False\n    return True\n\n\ndef compat_dict_intersection(\n    first_dict: Mapping[K, V],\n    second_dict: Mapping[K, V],\n    compat: Callable[[V, V], bool] = equivalent,\n) -> MutableMapping[K, V]:\n    """"""Return the intersection of two dictionaries as a new dictionary.\n\n    Items are retained if their keys are found in both dictionaries and the\n    values are compatible.\n\n    Parameters\n    ----------\n    first_dict, second_dict : dict-like\n        Mappings to merge.\n    compat : function, optional\n        Binary operator to determine if two values are compatible. By default,\n        checks for equivalence.\n\n    Returns\n    -------\n    intersection : dict\n        Intersection of the contents.\n    """"""\n    new_dict = dict(first_dict)\n    remove_incompatible_items(new_dict, second_dict, compat)\n    return new_dict\n\n\ndef compat_dict_union(\n    first_dict: Mapping[K, V],\n    second_dict: Mapping[K, V],\n    compat: Callable[[V, V], bool] = equivalent,\n) -> MutableMapping[K, V]:\n    """"""Return the union of two dictionaries as a new dictionary.\n\n    An exception is raised if any keys are found in both dictionaries and the\n    values are not compatible.\n\n    Parameters\n    ----------\n    first_dict, second_dict : dict-like\n        Mappings to merge.\n    compat : function, optional\n        Binary operator to determine if two values are compatible. By default,\n        checks for equivalence.\n\n    Returns\n    -------\n    union : dict\n        union of the contents.\n    """"""\n    new_dict = dict(first_dict)\n    update_safety_check(first_dict, second_dict, compat)\n    new_dict.update(second_dict)\n    return new_dict\n\n\nclass Frozen(Mapping[K, V]):\n    """"""Wrapper around an object implementing the mapping interface to make it\n    immutable. If you really want to modify the mapping, the mutable version is\n    saved under the `mapping` attribute.\n    """"""\n\n    __slots__ = (""mapping"",)\n\n    def __init__(self, mapping: Mapping[K, V]):\n        self.mapping = mapping\n\n    def __getitem__(self, key: K) -> V:\n        return self.mapping[key]\n\n    def __iter__(self) -> Iterator[K]:\n        return iter(self.mapping)\n\n    def __len__(self) -> int:\n        return len(self.mapping)\n\n    def __contains__(self, key: object) -> bool:\n        return key in self.mapping\n\n    def __repr__(self) -> str:\n        return ""{}({!r})"".format(type(self).__name__, self.mapping)\n\n\ndef FrozenDict(*args, **kwargs) -> Frozen:\n    return Frozen(dict(*args, **kwargs))\n\n\nclass SortedKeysDict(MutableMapping[K, V]):\n    """"""An wrapper for dictionary-like objects that always iterates over its\n    items in sorted order by key but is otherwise equivalent to the underlying\n    mapping.\n    """"""\n\n    __slots__ = (""mapping"",)\n\n    def __init__(self, mapping: MutableMapping[K, V] = None):\n        self.mapping = {} if mapping is None else mapping\n\n    def __getitem__(self, key: K) -> V:\n        return self.mapping[key]\n\n    def __setitem__(self, key: K, value: V) -> None:\n        self.mapping[key] = value\n\n    def __delitem__(self, key: K) -> None:\n        del self.mapping[key]\n\n    def __iter__(self) -> Iterator[K]:\n        return iter(sorted(self.mapping))\n\n    def __len__(self) -> int:\n        return len(self.mapping)\n\n    def __contains__(self, key: object) -> bool:\n        return key in self.mapping\n\n    def __repr__(self) -> str:\n        return ""{}({!r})"".format(type(self).__name__, self.mapping)\n\n\nclass OrderedSet(MutableSet[T]):\n    """"""A simple ordered set.\n\n    The API matches the builtin set, but it preserves insertion order of elements, like\n    a dict. Note that, unlike in an OrderedDict, equality tests are not order-sensitive.\n    """"""\n\n    _d: Dict[T, None]\n\n    __slots__ = (""_d"",)\n\n    def __init__(self, values: AbstractSet[T] = None):\n        self._d = {}\n        if values is not None:\n            # Disable type checking - both mypy and PyCharm believe that\n            # we\'re altering the type of self in place (see signature of\n            # MutableSet.__ior__)\n            self |= values  # type: ignore\n\n    # Required methods for MutableSet\n\n    def __contains__(self, value: object) -> bool:\n        return value in self._d\n\n    def __iter__(self) -> Iterator[T]:\n        return iter(self._d)\n\n    def __len__(self) -> int:\n        return len(self._d)\n\n    def add(self, value: T) -> None:\n        self._d[value] = None\n\n    def discard(self, value: T) -> None:\n        del self._d[value]\n\n    # Additional methods\n\n    def update(self, values: AbstractSet[T]) -> None:\n        # See comment on __init__ re. type checking\n        self |= values  # type: ignore\n\n    def __repr__(self) -> str:\n        return ""{}({!r})"".format(type(self).__name__, list(self))\n\n\nclass NdimSizeLenMixin:\n    """"""Mixin class that extends a class that defines a ``shape`` property to\n    one that also defines ``ndim``, ``size`` and ``__len__``.\n    """"""\n\n    __slots__ = ()\n\n    @property\n    def ndim(self: Any) -> int:\n        return len(self.shape)\n\n    @property\n    def size(self: Any) -> int:\n        # cast to int so that shape = () gives size = 1\n        return int(np.prod(self.shape))\n\n    def __len__(self: Any) -> int:\n        try:\n            return self.shape[0]\n        except IndexError:\n            raise TypeError(""len() of unsized object"")\n\n\nclass NDArrayMixin(NdimSizeLenMixin):\n    """"""Mixin class for making wrappers of N-dimensional arrays that conform to\n    the ndarray interface required for the data argument to Variable objects.\n\n    A subclass should set the `array` property and override one or more of\n    `dtype`, `shape` and `__getitem__`.\n    """"""\n\n    __slots__ = ()\n\n    @property\n    def dtype(self: Any) -> np.dtype:\n        return self.array.dtype\n\n    @property\n    def shape(self: Any) -> Tuple[int]:\n        return self.array.shape\n\n    def __getitem__(self: Any, key):\n        return self.array[key]\n\n    def __repr__(self: Any) -> str:\n        return ""{}(array={!r})"".format(type(self).__name__, self.array)\n\n\nclass ReprObject:\n    """"""Object that prints as the given value, for use with sentinel values.\n    """"""\n\n    __slots__ = (""_value"",)\n\n    def __init__(self, value: str):\n        self._value = value\n\n    def __repr__(self) -> str:\n        return self._value\n\n    def __eq__(self, other) -> bool:\n        if isinstance(other, ReprObject):\n            return self._value == other._value\n        return False\n\n    def __hash__(self) -> int:\n        return hash((type(self), self._value))\n\n    def __dask_tokenize__(self):\n        from dask.base import normalize_token\n\n        return normalize_token((type(self), self._value))\n\n\n@contextlib.contextmanager\ndef close_on_error(f):\n    """"""Context manager to ensure that a file opened by xarray is closed if an\n    exception is raised before the user sees the file object.\n    """"""\n    try:\n        yield\n    except Exception:\n        f.close()\n        raise\n\n\ndef is_remote_uri(path: str) -> bool:\n    return bool(re.search(r""^https?\\://"", path))\n\n\ndef is_grib_path(path: str) -> bool:\n    _, ext = os.path.splitext(path)\n    return ext in ["".grib"", "".grb"", "".grib2"", "".grb2""]\n\n\ndef is_uniform_spaced(arr, **kwargs) -> bool:\n    """"""Return True if values of an array are uniformly spaced and sorted.\n\n    >>> is_uniform_spaced(range(5))\n    True\n    >>> is_uniform_spaced([-4, 0, 100])\n    False\n\n    kwargs are additional arguments to ``np.isclose``\n    """"""\n    arr = np.array(arr, dtype=float)\n    diffs = np.diff(arr)\n    return bool(np.isclose(diffs.min(), diffs.max(), **kwargs))\n\n\ndef hashable(v: Any) -> bool:\n    """"""Determine whether `v` can be hashed.\n    """"""\n    try:\n        hash(v)\n    except TypeError:\n        return False\n    return True\n\n\ndef not_implemented(*args, **kwargs):\n    return NotImplemented\n\n\ndef decode_numpy_dict_values(attrs: Mapping[K, V]) -> Dict[K, V]:\n    """"""Convert attribute values from numpy objects to native Python objects,\n    for use in to_dict\n    """"""\n    attrs = dict(attrs)\n    for k, v in attrs.items():\n        if isinstance(v, np.ndarray):\n            attrs[k] = v.tolist()\n        elif isinstance(v, np.generic):\n            attrs[k] = v.item()\n    return attrs\n\n\ndef ensure_us_time_resolution(val):\n    """"""Convert val out of numpy time, for use in to_dict.\n    Needed because of numpy bug GH#7619""""""\n    if np.issubdtype(val.dtype, np.datetime64):\n        val = val.astype(""datetime64[us]"")\n    elif np.issubdtype(val.dtype, np.timedelta64):\n        val = val.astype(""timedelta64[us]"")\n    return val\n\n\nclass HiddenKeyDict(MutableMapping[K, V]):\n    """"""Acts like a normal dictionary, but hides certain keys.\n    """"""\n\n    __slots__ = (""_data"", ""_hidden_keys"")\n\n    # ``__init__`` method required to create instance from class.\n\n    def __init__(self, data: MutableMapping[K, V], hidden_keys: Iterable[K]):\n        self._data = data\n        self._hidden_keys = frozenset(hidden_keys)\n\n    def _raise_if_hidden(self, key: K) -> None:\n        if key in self._hidden_keys:\n            raise KeyError(""Key `%r` is hidden."" % key)\n\n    # The next five methods are requirements of the ABC.\n    def __setitem__(self, key: K, value: V) -> None:\n        self._raise_if_hidden(key)\n        self._data[key] = value\n\n    def __getitem__(self, key: K) -> V:\n        self._raise_if_hidden(key)\n        return self._data[key]\n\n    def __delitem__(self, key: K) -> None:\n        self._raise_if_hidden(key)\n        del self._data[key]\n\n    def __iter__(self) -> Iterator[K]:\n        for k in self._data:\n            if k not in self._hidden_keys:\n                yield k\n\n    def __len__(self) -> int:\n        num_hidden = len(self._hidden_keys & self._data.keys())\n        return len(self._data) - num_hidden\n\n\ndef infix_dims(dims_supplied: Collection, dims_all: Collection) -> Iterator:\n    """"""\n    Resolves a supplied list containing an ellispsis representing other items, to\n    a generator with the \'realized\' list of all items\n    """"""\n    if ... in dims_supplied:\n        if len(set(dims_all)) != len(dims_all):\n            raise ValueError(""Cannot use ellipsis with repeated dims"")\n        if len([d for d in dims_supplied if d == ...]) > 1:\n            raise ValueError(""More than one ellipsis supplied"")\n        other_dims = [d for d in dims_all if d not in dims_supplied]\n        for d in dims_supplied:\n            if d == ...:\n                yield from other_dims\n            else:\n                yield d\n    else:\n        if set(dims_supplied) ^ set(dims_all):\n            raise ValueError(\n                f""{dims_supplied} must be a permuted list of {dims_all}, unless `...` is included""\n            )\n        yield from dims_supplied\n\n\ndef get_temp_dimname(dims: Container[Hashable], new_dim: Hashable) -> Hashable:\n    """""" Get an new dimension name based on new_dim, that is not used in dims.\n    If the same name exists, we add an underscore(s) in the head.\n\n    Example1:\n        dims: [\'a\', \'b\', \'c\']\n        new_dim: [\'_rolling\']\n        -> [\'_rolling\']\n    Example2:\n        dims: [\'a\', \'b\', \'c\', \'_rolling\']\n        new_dim: [\'_rolling\']\n        -> [\'__rolling\']\n    """"""\n    while new_dim in dims:\n        new_dim = ""_"" + str(new_dim)\n    return new_dim\n\n\ndef drop_dims_from_indexers(\n    indexers: Mapping[Hashable, Any],\n    dims: Union[list, Mapping[Hashable, int]],\n    missing_dims: str,\n) -> Mapping[Hashable, Any]:\n    """""" Depending on the setting of missing_dims, drop any dimensions from indexers that\n    are not present in dims.\n\n    Parameters\n    ----------\n    indexers : dict\n    dims : sequence\n    missing_dims : {""raise"", ""warn"", ""ignore""}\n    """"""\n\n    if missing_dims == ""raise"":\n        invalid = indexers.keys() - set(dims)\n        if invalid:\n            raise ValueError(\n                f""dimensions {invalid} do not exist. Expected one or more of {dims}""\n            )\n\n        return indexers\n\n    elif missing_dims == ""warn"":\n\n        # don\'t modify input\n        indexers = dict(indexers)\n\n        invalid = indexers.keys() - set(dims)\n        if invalid:\n            warnings.warn(\n                f""dimensions {invalid} do not exist. Expected one or more of {dims}""\n            )\n        for key in invalid:\n            indexers.pop(key)\n\n        return indexers\n\n    elif missing_dims == ""ignore"":\n        return {key: val for key, val in indexers.items() if key in dims}\n\n    else:\n        raise ValueError(\n            f""Unrecognised option {missing_dims} for missing_dims argument""\n        )\n\n\n# Singleton type, as per https://github.com/python/typing/pull/240\nclass Default(Enum):\n    token = 0\n\n\n_default = Default.token\n'"
xarray/core/variable.py,51,"b'import copy\nimport functools\nimport itertools\nimport numbers\nimport warnings\nfrom collections import defaultdict\nfrom datetime import timedelta\nfrom distutils.version import LooseVersion\nfrom typing import Any, Dict, Hashable, Mapping, Tuple, TypeVar, Union\n\nimport numpy as np\nimport pandas as pd\n\nimport xarray as xr  # only for Dataset and DataArray\n\nfrom . import arithmetic, common, dtypes, duck_array_ops, indexing, nputils, ops, utils\nfrom .indexing import (\n    BasicIndexer,\n    OuterIndexer,\n    PandasIndexAdapter,\n    VectorizedIndexer,\n    as_indexable,\n)\nfrom .npcompat import IS_NEP18_ACTIVE\nfrom .options import _get_keep_attrs\nfrom .pycompat import dask_array_type, integer_types\nfrom .utils import (\n    OrderedSet,\n    _default,\n    decode_numpy_dict_values,\n    drop_dims_from_indexers,\n    either_dict_or_kwargs,\n    ensure_us_time_resolution,\n    infix_dims,\n)\n\nNON_NUMPY_SUPPORTED_ARRAY_TYPES = (\n    indexing.ExplicitlyIndexed,\n    pd.Index,\n) + dask_array_type\n# https://github.com/python/mypy/issues/224\nBASIC_INDEXING_TYPES = integer_types + (slice,)  # type: ignore\n\nVariableType = TypeVar(""VariableType"", bound=""Variable"")\n""""""Type annotation to be used when methods of Variable return self or a copy of self.\nWhen called from an instance of a subclass, e.g. IndexVariable, mypy identifies the\noutput as an instance of the subclass.\n\nUsage::\n\n   class Variable:\n       def f(self: VariableType, ...) -> VariableType:\n           ...\n""""""\n\n\nclass MissingDimensionsError(ValueError):\n    """"""Error class used when we can\'t safely guess a dimension name.\n    """"""\n\n    # inherits from ValueError for backward compatibility\n    # TODO: move this to an xarray.exceptions module?\n\n\ndef as_variable(obj, name=None) -> ""Union[Variable, IndexVariable]"":\n    """"""Convert an object into a Variable.\n\n    Parameters\n    ----------\n    obj : object\n        Object to convert into a Variable.\n\n        - If the object is already a Variable, return a shallow copy.\n        - Otherwise, if the object has \'dims\' and \'data\' attributes, convert\n          it into a new Variable.\n        - If all else fails, attempt to convert the object into a Variable by\n          unpacking it into the arguments for creating a new Variable.\n    name : str, optional\n        If provided:\n\n        - `obj` can be a 1D array, which is assumed to label coordinate values\n          along a dimension of this given name.\n        - Variables with name matching one of their dimensions are converted\n          into `IndexVariable` objects.\n\n    Returns\n    -------\n    var : Variable\n        The newly created variable.\n\n    """"""\n    from .dataarray import DataArray\n\n    # TODO: consider extending this method to automatically handle Iris and\n    if isinstance(obj, DataArray):\n        # extract the primary Variable from DataArrays\n        obj = obj.variable\n\n    if isinstance(obj, Variable):\n        obj = obj.copy(deep=False)\n    elif isinstance(obj, tuple):\n        try:\n            obj = Variable(*obj)\n        except (TypeError, ValueError) as error:\n            # use .format() instead of % because it handles tuples consistently\n            raise error.__class__(\n                ""Could not convert tuple of form ""\n                ""(dims, data[, attrs, encoding]): ""\n                ""{} to Variable."".format(obj)\n            )\n    elif utils.is_scalar(obj):\n        obj = Variable([], obj)\n    elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:\n        obj = Variable(obj.name, obj)\n    elif isinstance(obj, (set, dict)):\n        raise TypeError(""variable {!r} has invalid type {!r}"".format(name, type(obj)))\n    elif name is not None:\n        data = as_compatible_data(obj)\n        if data.ndim != 1:\n            raise MissingDimensionsError(\n                ""cannot set variable %r with %r-dimensional data ""\n                ""without explicit dimension names. Pass a tuple of ""\n                ""(dims, data) instead."" % (name, data.ndim)\n            )\n        obj = Variable(name, data, fastpath=True)\n    else:\n        raise TypeError(\n            ""unable to convert object into a variable without an ""\n            ""explicit list of dimensions: %r"" % obj\n        )\n\n    if name is not None and name in obj.dims:\n        # convert the Variable into an Index\n        if obj.ndim != 1:\n            raise MissingDimensionsError(\n                ""%r has more than 1-dimension and the same name as one of its ""\n                ""dimensions %r. xarray disallows such variables because they ""\n                ""conflict with the coordinates used to label ""\n                ""dimensions."" % (name, obj.dims)\n            )\n        obj = obj.to_index_variable()\n\n    return obj\n\n\ndef _maybe_wrap_data(data):\n    """"""\n    Put pandas.Index and numpy.ndarray arguments in adapter objects to ensure\n    they can be indexed properly.\n\n    NumpyArrayAdapter, PandasIndexAdapter and LazilyOuterIndexedArray should\n    all pass through unmodified.\n    """"""\n    if isinstance(data, pd.Index):\n        return PandasIndexAdapter(data)\n    return data\n\n\ndef _possibly_convert_objects(values):\n    """"""Convert arrays of datetime.datetime and datetime.timedelta objects into\n    datetime64 and timedelta64, according to the pandas convention.\n    """"""\n    return np.asarray(pd.Series(values.ravel())).reshape(values.shape)\n\n\ndef as_compatible_data(data, fastpath=False):\n    """"""Prepare and wrap data to put in a Variable.\n\n    - If data does not have the necessary attributes, convert it to ndarray.\n    - If data has dtype=datetime64, ensure that it has ns precision. If it\'s a\n      pandas.Timestamp, convert it to datetime64.\n    - If data is already a pandas or xarray object (other than an Index), just\n      use the values.\n\n    Finally, wrap it up with an adapter if necessary.\n    """"""\n    if fastpath and getattr(data, ""ndim"", 0) > 0:\n        # can\'t use fastpath (yet) for scalars\n        return _maybe_wrap_data(data)\n\n    if isinstance(data, Variable):\n        return data.data\n\n    if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):\n        return _maybe_wrap_data(data)\n\n    if isinstance(data, tuple):\n        data = utils.to_0d_object_array(data)\n\n    if isinstance(data, pd.Timestamp):\n        # TODO: convert, handle datetime objects, too\n        data = np.datetime64(data.value, ""ns"")\n\n    if isinstance(data, timedelta):\n        data = np.timedelta64(getattr(data, ""value"", data), ""ns"")\n\n    # we don\'t want nested self-described arrays\n    data = getattr(data, ""values"", data)\n\n    if isinstance(data, np.ma.MaskedArray):\n        mask = np.ma.getmaskarray(data)\n        if mask.any():\n            dtype, fill_value = dtypes.maybe_promote(data.dtype)\n            data = np.asarray(data, dtype=dtype)\n            data[mask] = fill_value\n        else:\n            data = np.asarray(data)\n\n    if not isinstance(data, np.ndarray):\n        if hasattr(data, ""__array_function__""):\n            if IS_NEP18_ACTIVE:\n                return data\n            else:\n                raise TypeError(\n                    ""Got an NumPy-like array type providing the ""\n                    ""__array_function__ protocol but NEP18 is not enabled. ""\n                    ""Check that numpy >= v1.16 and that the environment ""\n                    \'variable ""NUMPY_EXPERIMENTAL_ARRAY_FUNCTION"" is set to \'\n                    \'""1""\'\n                )\n\n    # validate whether the data is valid data types\n    data = np.asarray(data)\n\n    if isinstance(data, np.ndarray):\n        if data.dtype.kind == ""O"":\n            data = _possibly_convert_objects(data)\n        elif data.dtype.kind == ""M"":\n            data = np.asarray(data, ""datetime64[ns]"")\n        elif data.dtype.kind == ""m"":\n            data = np.asarray(data, ""timedelta64[ns]"")\n\n    return _maybe_wrap_data(data)\n\n\ndef _as_array_or_item(data):\n    """"""Return the given values as a numpy array, or as an individual item if\n    it\'s a 0d datetime64 or timedelta64 array.\n\n    Importantly, this function does not copy data if it is already an ndarray -\n    otherwise, it will not be possible to update Variable values in place.\n\n    This function mostly exists because 0-dimensional ndarrays with\n    dtype=datetime64 are broken :(\n    https://github.com/numpy/numpy/issues/4337\n    https://github.com/numpy/numpy/issues/7619\n\n    TODO: remove this (replace with np.asarray) once these issues are fixed\n    """"""\n    data = np.asarray(data)\n    if data.ndim == 0:\n        if data.dtype.kind == ""M"":\n            data = np.datetime64(data, ""ns"")\n        elif data.dtype.kind == ""m"":\n            data = np.timedelta64(data, ""ns"")\n    return data\n\n\nclass Variable(\n    common.AbstractArray, arithmetic.SupportsArithmetic, utils.NdimSizeLenMixin\n):\n    """"""A netcdf-like variable consisting of dimensions, data and attributes\n    which describe a single Array. A single Variable object is not fully\n    described outside the context of its parent Dataset (if you want such a\n    fully described object, use a DataArray instead).\n\n    The main functional difference between Variables and numpy arrays is that\n    numerical operations on Variables implement array broadcasting by dimension\n    name. For example, adding an Variable with dimensions `(\'time\',)` to\n    another Variable with dimensions `(\'space\',)` results in a new Variable\n    with dimensions `(\'time\', \'space\')`. Furthermore, numpy reduce operations\n    like ``mean`` or ``sum`` are overwritten to take a ""dimension"" argument\n    instead of an ""axis"".\n\n    Variables are light-weight objects used as the building block for datasets.\n    They are more primitive objects, so operations with them provide marginally\n    higher performance than using DataArrays. However, manipulating data in the\n    form of a Dataset or DataArray should almost always be preferred, because\n    they can use more complete metadata in context of coordinate labels.\n    """"""\n\n    __slots__ = (""_dims"", ""_data"", ""_attrs"", ""_encoding"")\n\n    def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n        """"""\n        Parameters\n        ----------\n        dims : str or sequence of str\n            Name(s) of the the data dimension(s). Must be either a string (only\n            for 1D data) or a sequence of strings with length equal to the\n            number of dimensions.\n        data : array_like\n            Data array which supports numpy-like data access.\n        attrs : dict_like or None, optional\n            Attributes to assign to the new variable. If None (default), an\n            empty attribute dictionary is initialized.\n        encoding : dict_like or None, optional\n            Dictionary specifying how to encode this array\'s data into a\n            serialized format like netCDF4. Currently used keys (for netCDF)\n            include \'_FillValue\', \'scale_factor\', \'add_offset\' and \'dtype\'.\n            Well-behaved code to serialize a Variable should ignore\n            unrecognized encoding items.\n        """"""\n        self._data = as_compatible_data(data, fastpath=fastpath)\n        self._dims = self._parse_dimensions(dims)\n        self._attrs = None\n        self._encoding = None\n        if attrs is not None:\n            self.attrs = attrs\n        if encoding is not None:\n            self.encoding = encoding\n\n    @property\n    def dtype(self):\n        return self._data.dtype\n\n    @property\n    def shape(self):\n        return self._data.shape\n\n    @property\n    def nbytes(self):\n        return self.size * self.dtype.itemsize\n\n    @property\n    def _in_memory(self):\n        return isinstance(self._data, (np.ndarray, np.number, PandasIndexAdapter)) or (\n            isinstance(self._data, indexing.MemoryCachedArray)\n            and isinstance(self._data.array, indexing.NumpyIndexingAdapter)\n        )\n\n    @property\n    def data(self):\n        if hasattr(self._data, ""__array_function__"") or isinstance(\n            self._data, dask_array_type\n        ):\n            return self._data\n        else:\n            return self.values\n\n    @data.setter\n    def data(self, data):\n        data = as_compatible_data(data)\n        if data.shape != self.shape:\n            raise ValueError(\n                f""replacement data must match the Variable\'s shape. ""\n                f""replacement data has shape {data.shape}; Variable has shape {self.shape}""\n            )\n        self._data = data\n\n    def load(self, **kwargs):\n        """"""Manually trigger loading of this variable\'s data from disk or a\n        remote source into memory and return this variable.\n\n        Normally, it should not be necessary to call this method in user code,\n        because all xarray functions should either work on deferred data or\n        load data automatically.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Additional keyword arguments passed on to ``dask.array.compute``.\n\n        See Also\n        --------\n        dask.array.compute\n        """"""\n        if isinstance(self._data, dask_array_type):\n            self._data = as_compatible_data(self._data.compute(**kwargs))\n        elif not hasattr(self._data, ""__array_function__""):\n            self._data = np.asarray(self._data)\n        return self\n\n    def compute(self, **kwargs):\n        """"""Manually trigger loading of this variable\'s data from disk or a\n        remote source into memory and return a new variable. The original is\n        left unaltered.\n\n        Normally, it should not be necessary to call this method in user code,\n        because all xarray functions should either work on deferred data or\n        load data automatically.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Additional keyword arguments passed on to ``dask.array.compute``.\n\n        See Also\n        --------\n        dask.array.compute\n        """"""\n        new = self.copy(deep=False)\n        return new.load(**kwargs)\n\n    def __dask_tokenize__(self):\n        # Use v.data, instead of v._data, in order to cope with the wrappers\n        # around NetCDF and the like\n        from dask.base import normalize_token\n\n        return normalize_token((type(self), self._dims, self.data, self._attrs))\n\n    def __dask_graph__(self):\n        if isinstance(self._data, dask_array_type):\n            return self._data.__dask_graph__()\n        else:\n            return None\n\n    def __dask_keys__(self):\n        return self._data.__dask_keys__()\n\n    def __dask_layers__(self):\n        return self._data.__dask_layers__()\n\n    @property\n    def __dask_optimize__(self):\n        return self._data.__dask_optimize__\n\n    @property\n    def __dask_scheduler__(self):\n        return self._data.__dask_scheduler__\n\n    def __dask_postcompute__(self):\n        array_func, array_args = self._data.__dask_postcompute__()\n        return (\n            self._dask_finalize,\n            (array_func, array_args, self._dims, self._attrs, self._encoding),\n        )\n\n    def __dask_postpersist__(self):\n        array_func, array_args = self._data.__dask_postpersist__()\n        return (\n            self._dask_finalize,\n            (array_func, array_args, self._dims, self._attrs, self._encoding),\n        )\n\n    @staticmethod\n    def _dask_finalize(results, array_func, array_args, dims, attrs, encoding):\n        if isinstance(results, dict):  # persist case\n            name = array_args[0]\n            results = {k: v for k, v in results.items() if k[0] == name}\n        data = array_func(results, *array_args)\n        return Variable(dims, data, attrs=attrs, encoding=encoding)\n\n    @property\n    def values(self):\n        """"""The variable\'s data as a numpy.ndarray""""""\n        return _as_array_or_item(self._data)\n\n    @values.setter\n    def values(self, values):\n        self.data = values\n\n    def to_base_variable(self):\n        """"""Return this variable as a base xarray.Variable""""""\n        return Variable(\n            self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True\n        )\n\n    to_variable = utils.alias(to_base_variable, ""to_variable"")\n\n    def to_index_variable(self):\n        """"""Return this variable as an xarray.IndexVariable""""""\n        return IndexVariable(\n            self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True\n        )\n\n    to_coord = utils.alias(to_index_variable, ""to_coord"")\n\n    def to_index(self):\n        """"""Convert this variable to a pandas.Index""""""\n        return self.to_index_variable().to_index()\n\n    def to_dict(self, data=True):\n        """"""Dictionary representation of variable.""""""\n        item = {""dims"": self.dims, ""attrs"": decode_numpy_dict_values(self.attrs)}\n        if data:\n            item[""data""] = ensure_us_time_resolution(self.values).tolist()\n        else:\n            item.update({""dtype"": str(self.dtype), ""shape"": self.shape})\n        return item\n\n    @property\n    def dims(self):\n        """"""Tuple of dimension names with which this variable is associated.\n        """"""\n        return self._dims\n\n    @dims.setter\n    def dims(self, value):\n        self._dims = self._parse_dimensions(value)\n\n    def _parse_dimensions(self, dims):\n        if isinstance(dims, str):\n            dims = (dims,)\n        dims = tuple(dims)\n        if len(dims) != self.ndim:\n            raise ValueError(\n                ""dimensions %s must have the same length as the ""\n                ""number of data dimensions, ndim=%s"" % (dims, self.ndim)\n            )\n        return dims\n\n    def _item_key_to_tuple(self, key):\n        if utils.is_dict_like(key):\n            return tuple(key.get(dim, slice(None)) for dim in self.dims)\n        else:\n            return key\n\n    def _broadcast_indexes(self, key):\n        """"""Prepare an indexing key for an indexing operation.\n\n        Parameters\n        -----------\n        key: int, slice, array, dict or tuple of integer, slices and arrays\n            Any valid input for indexing.\n\n        Returns\n        -------\n        dims: tuple\n            Dimension of the resultant variable.\n        indexers: IndexingTuple subclass\n            Tuple of integer, array-like, or slices to use when indexing\n            self._data. The type of this argument indicates the type of\n            indexing to perform, either basic, outer or vectorized.\n        new_order : Optional[Sequence[int]]\n            Optional reordering to do on the result of indexing. If not None,\n            the first len(new_order) indexing should be moved to these\n            positions.\n        """"""\n        key = self._item_key_to_tuple(key)  # key is a tuple\n        # key is a tuple of full size\n        key = indexing.expanded_indexer(key, self.ndim)\n        # Convert a scalar Variable to an integer\n        key = tuple(\n            k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key\n        )\n        # Convert a 0d-array to an integer\n        key = tuple(\n            k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key\n        )\n\n        if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):\n            return self._broadcast_indexes_basic(key)\n\n        self._validate_indexers(key)\n        # Detect it can be mapped as an outer indexer\n        # If all key is unlabeled, or\n        # key can be mapped as an OuterIndexer.\n        if all(not isinstance(k, Variable) for k in key):\n            return self._broadcast_indexes_outer(key)\n\n        # If all key is 1-dimensional and there are no duplicate labels,\n        # key can be mapped as an OuterIndexer.\n        dims = []\n        for k, d in zip(key, self.dims):\n            if isinstance(k, Variable):\n                if len(k.dims) > 1:\n                    return self._broadcast_indexes_vectorized(key)\n                dims.append(k.dims[0])\n            elif not isinstance(k, integer_types):\n                dims.append(d)\n        if len(set(dims)) == len(dims):\n            return self._broadcast_indexes_outer(key)\n\n        return self._broadcast_indexes_vectorized(key)\n\n    def _broadcast_indexes_basic(self, key):\n        dims = tuple(\n            dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)\n        )\n        return dims, BasicIndexer(key), None\n\n    def _validate_indexers(self, key):\n        """""" Make sanity checks """"""\n        for dim, k in zip(self.dims, key):\n            if isinstance(k, BASIC_INDEXING_TYPES):\n                pass\n            else:\n                if not isinstance(k, Variable):\n                    k = np.asarray(k)\n                    if k.ndim > 1:\n                        raise IndexError(\n                            ""Unlabeled multi-dimensional array cannot be ""\n                            ""used for indexing: {}"".format(k)\n                        )\n                if k.dtype.kind == ""b"":\n                    if self.shape[self.get_axis_num(dim)] != len(k):\n                        raise IndexError(\n                            ""Boolean array size {:d} is used to index array ""\n                            ""with shape {:s}."".format(len(k), str(self.shape))\n                        )\n                    if k.ndim > 1:\n                        raise IndexError(\n                            ""{}-dimensional boolean indexing is ""\n                            ""not supported. "".format(k.ndim)\n                        )\n                    if getattr(k, ""dims"", (dim,)) != (dim,):\n                        raise IndexError(\n                            ""Boolean indexer should be unlabeled or on the ""\n                            ""same dimension to the indexed array. Indexer is ""\n                            ""on {:s} but the target dimension is {:s}."".format(\n                                str(k.dims), dim\n                            )\n                        )\n\n    def _broadcast_indexes_outer(self, key):\n        dims = tuple(\n            k.dims[0] if isinstance(k, Variable) else dim\n            for k, dim in zip(key, self.dims)\n            if not isinstance(k, integer_types)\n        )\n\n        new_key = []\n        for k in key:\n            if isinstance(k, Variable):\n                k = k.data\n            if not isinstance(k, BASIC_INDEXING_TYPES):\n                k = np.asarray(k)\n                if k.size == 0:\n                    # Slice by empty list; numpy could not infer the dtype\n                    k = k.astype(int)\n                elif k.dtype.kind == ""b"":\n                    (k,) = np.nonzero(k)\n            new_key.append(k)\n\n        return dims, OuterIndexer(tuple(new_key)), None\n\n    def _nonzero(self):\n        """""" Equivalent numpy\'s nonzero but returns a tuple of Varibles. """"""\n        # TODO we should replace dask\'s native nonzero\n        # after https://github.com/dask/dask/issues/1076 is implemented.\n        nonzeros = np.nonzero(self.data)\n        return tuple(Variable((dim), nz) for nz, dim in zip(nonzeros, self.dims))\n\n    def _broadcast_indexes_vectorized(self, key):\n        variables = []\n        out_dims_set = OrderedSet()\n        for dim, value in zip(self.dims, key):\n            if isinstance(value, slice):\n                out_dims_set.add(dim)\n            else:\n                variable = (\n                    value\n                    if isinstance(value, Variable)\n                    else as_variable(value, name=dim)\n                )\n                if variable.dtype.kind == ""b"":  # boolean indexing case\n                    (variable,) = variable._nonzero()\n\n                variables.append(variable)\n                out_dims_set.update(variable.dims)\n\n        variable_dims = set()\n        for variable in variables:\n            variable_dims.update(variable.dims)\n\n        slices = []\n        for i, (dim, value) in enumerate(zip(self.dims, key)):\n            if isinstance(value, slice):\n                if dim in variable_dims:\n                    # We only convert slice objects to variables if they share\n                    # a dimension with at least one other variable. Otherwise,\n                    # we can equivalently leave them as slices aknd transpose\n                    # the result. This is significantly faster/more efficient\n                    # for most array backends.\n                    values = np.arange(*value.indices(self.sizes[dim]))\n                    variables.insert(i - len(slices), Variable((dim,), values))\n                else:\n                    slices.append((i, value))\n\n        try:\n            variables = _broadcast_compat_variables(*variables)\n        except ValueError:\n            raise IndexError(f""Dimensions of indexers mismatch: {key}"")\n\n        out_key = [variable.data for variable in variables]\n        out_dims = tuple(out_dims_set)\n        slice_positions = set()\n        for i, value in slices:\n            out_key.insert(i, value)\n            new_position = out_dims.index(self.dims[i])\n            slice_positions.add(new_position)\n\n        if slice_positions:\n            new_order = [i for i in range(len(out_dims)) if i not in slice_positions]\n        else:\n            new_order = None\n\n        return out_dims, VectorizedIndexer(tuple(out_key)), new_order\n\n    def __getitem__(self: VariableType, key) -> VariableType:\n        """"""Return a new Variable object whose contents are consistent with\n        getting the provided key from the underlying data.\n\n        NB. __getitem__ and __setitem__ implement xarray-style indexing,\n        where if keys are unlabeled arrays, we index the array orthogonally\n        with them. If keys are labeled array (such as Variables), they are\n        broadcasted with our usual scheme and then the array is indexed with\n        the broadcasted key, like numpy\'s fancy indexing.\n\n        If you really want to do indexing like `x[x > 0]`, manipulate the numpy\n        array `x.values` directly.\n        """"""\n        dims, indexer, new_order = self._broadcast_indexes(key)\n        data = as_indexable(self._data)[indexer]\n        if new_order:\n            data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n        return self._finalize_indexing_result(dims, data)\n\n    def _finalize_indexing_result(self: VariableType, dims, data) -> VariableType:\n        """"""Used by IndexVariable to return IndexVariable objects when possible.\n        """"""\n        return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n\n    def _getitem_with_mask(self, key, fill_value=dtypes.NA):\n        """"""Index this Variable with -1 remapped to fill_value.""""""\n        # TODO(shoyer): expose this method in public API somewhere (isel?) and\n        # use it for reindex.\n        # TODO(shoyer): add a sanity check that all other integers are\n        # non-negative\n        # TODO(shoyer): add an optimization, remapping -1 to an adjacent value\n        # that is actually indexed rather than mapping it to the last value\n        # along each axis.\n\n        if fill_value is dtypes.NA:\n            fill_value = dtypes.get_fill_value(self.dtype)\n\n        dims, indexer, new_order = self._broadcast_indexes(key)\n\n        if self.size:\n            if isinstance(self._data, dask_array_type):\n                # dask\'s indexing is faster this way; also vindex does not\n                # support negative indices yet:\n                # https://github.com/dask/dask/pull/2967\n                actual_indexer = indexing.posify_mask_indexer(indexer)\n            else:\n                actual_indexer = indexer\n\n            data = as_indexable(self._data)[actual_indexer]\n            mask = indexing.create_mask(indexer, self.shape, data)\n            # we need to invert the mask in order to pass data first. This helps\n            # pint to choose the correct unit\n            # TODO: revert after https://github.com/hgrecco/pint/issues/1019 is fixed\n            data = duck_array_ops.where(np.logical_not(mask), data, fill_value)\n        else:\n            # array cannot be indexed along dimensions of size 0, so just\n            # build the mask directly instead.\n            mask = indexing.create_mask(indexer, self.shape)\n            data = np.broadcast_to(fill_value, getattr(mask, ""shape"", ()))\n\n        if new_order:\n            data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n        return self._finalize_indexing_result(dims, data)\n\n    def __setitem__(self, key, value):\n        """"""__setitem__ is overloaded to access the underlying numpy values with\n        orthogonal indexing.\n\n        See __getitem__ for more details.\n        """"""\n        dims, index_tuple, new_order = self._broadcast_indexes(key)\n\n        if not isinstance(value, Variable):\n            value = as_compatible_data(value)\n            if value.ndim > len(dims):\n                raise ValueError(\n                    ""shape mismatch: value array of shape %s could not be ""\n                    ""broadcast to indexing result with %s dimensions""\n                    % (value.shape, len(dims))\n                )\n            if value.ndim == 0:\n                value = Variable((), value)\n            else:\n                value = Variable(dims[-value.ndim :], value)\n        # broadcast to become assignable\n        value = value.set_dims(dims).data\n\n        if new_order:\n            value = duck_array_ops.asarray(value)\n            value = value[(len(dims) - value.ndim) * (np.newaxis,) + (Ellipsis,)]\n            value = duck_array_ops.moveaxis(value, new_order, range(len(new_order)))\n\n        indexable = as_indexable(self._data)\n        indexable[index_tuple] = value\n\n    @property\n    def attrs(self) -> Dict[Hashable, Any]:\n        """"""Dictionary of local attributes on this variable.\n        """"""\n        if self._attrs is None:\n            self._attrs = {}\n        return self._attrs\n\n    @attrs.setter\n    def attrs(self, value: Mapping[Hashable, Any]) -> None:\n        self._attrs = dict(value)\n\n    @property\n    def encoding(self):\n        """"""Dictionary of encodings on this variable.\n        """"""\n        if self._encoding is None:\n            self._encoding = {}\n        return self._encoding\n\n    @encoding.setter\n    def encoding(self, value):\n        try:\n            self._encoding = dict(value)\n        except ValueError:\n            raise ValueError(""encoding must be castable to a dictionary"")\n\n    def copy(self, deep=True, data=None):\n        """"""Returns a copy of this object.\n\n        If `deep=True`, the data array is loaded into memory and copied onto\n        the new object. Dimensions, attributes and encodings are always copied.\n\n        Use `data` to create a new object with the same structure as\n        original but entirely new data.\n\n        Parameters\n        ----------\n        deep : bool, optional\n            Whether the data array is loaded into memory and copied onto\n            the new object. Default is True.\n        data : array_like, optional\n            Data to use in the new object. Must have same shape as original.\n            When `data` is used, `deep` is ignored.\n\n        Returns\n        -------\n        object : Variable\n            New object with dimensions, attributes, encodings, and optionally\n            data copied from original.\n\n        Examples\n        --------\n\n        Shallow copy versus deep copy\n\n        >>> var = xr.Variable(data=[1, 2, 3], dims=""x"")\n        >>> var.copy()\n        <xarray.Variable (x: 3)>\n        array([1, 2, 3])\n        >>> var_0 = var.copy(deep=False)\n        >>> var_0[0] = 7\n        >>> var_0\n        <xarray.Variable (x: 3)>\n        array([7, 2, 3])\n        >>> var\n        <xarray.Variable (x: 3)>\n        array([7, 2, 3])\n\n        Changing the data using the ``data`` argument maintains the\n        structure of the original object, but with the new data. Original\n        object is unaffected.\n\n        >>> var.copy(data=[0.1, 0.2, 0.3])\n        <xarray.Variable (x: 3)>\n        array([ 0.1,  0.2,  0.3])\n        >>> var\n        <xarray.Variable (x: 3)>\n        array([7, 2, 3])\n\n        See Also\n        --------\n        pandas.DataFrame.copy\n        """"""\n        if data is None:\n            data = self._data\n\n            if isinstance(data, indexing.MemoryCachedArray):\n                # don\'t share caching between copies\n                data = indexing.MemoryCachedArray(data.array)\n\n            if deep:\n                if hasattr(data, ""__array_function__"") or isinstance(\n                    data, dask_array_type\n                ):\n                    data = data.copy()\n                elif not isinstance(data, PandasIndexAdapter):\n                    # pandas.Index is immutable\n                    data = np.array(data)\n        else:\n            data = as_compatible_data(data)\n            if self.shape != data.shape:\n                raise ValueError(\n                    ""Data shape {} must match shape of object {}"".format(\n                        data.shape, self.shape\n                    )\n                )\n\n        # note:\n        # dims is already an immutable tuple\n        # attributes and encoding will be copied when the new Array is created\n        return self._replace(data=data)\n\n    def _replace(\n        self, dims=_default, data=_default, attrs=_default, encoding=_default\n    ) -> ""Variable"":\n        if dims is _default:\n            dims = copy.copy(self._dims)\n        if data is _default:\n            data = copy.copy(self.data)\n        if attrs is _default:\n            attrs = copy.copy(self._attrs)\n        if encoding is _default:\n            encoding = copy.copy(self._encoding)\n        return type(self)(dims, data, attrs, encoding, fastpath=True)\n\n    def __copy__(self):\n        return self.copy(deep=False)\n\n    def __deepcopy__(self, memo=None):\n        # memo does nothing but is required for compatibility with\n        # copy.deepcopy\n        return self.copy(deep=True)\n\n    # mutable objects should not be hashable\n    # https://github.com/python/mypy/issues/4266\n    __hash__ = None  # type: ignore\n\n    @property\n    def chunks(self):\n        """"""Block dimensions for this array\'s data or None if it\'s not a dask\n        array.\n        """"""\n        return getattr(self._data, ""chunks"", None)\n\n    _array_counter = itertools.count()\n\n    def chunk(self, chunks=None, name=None, lock=False):\n        """"""Coerce this array\'s data into a dask arrays with the given chunks.\n\n        If this variable is a non-dask array, it will be converted to dask\n        array. If it\'s a dask array, it will be rechunked to the given chunk\n        sizes.\n\n        If neither chunks is not provided for one or more dimensions, chunk\n        sizes along that dimension will not be updated; non-dask arrays will be\n        converted into dask arrays with a single block.\n\n        Parameters\n        ----------\n        chunks : int, tuple or dict, optional\n            Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\n            ``{\'x\': 5, \'y\': 5}``.\n        name : str, optional\n            Used to generate the name for this array in the internal dask\n            graph. Does not need not be unique.\n        lock : optional\n            Passed on to :py:func:`dask.array.from_array`, if the array is not\n            already as dask array.\n\n        Returns\n        -------\n        chunked : xarray.Variable\n        """"""\n        import dask\n        import dask.array as da\n\n        if utils.is_dict_like(chunks):\n            chunks = {self.get_axis_num(dim): chunk for dim, chunk in chunks.items()}\n\n        if chunks is None:\n            chunks = self.chunks or self.shape\n\n        data = self._data\n        if isinstance(data, da.Array):\n            data = data.rechunk(chunks)\n        else:\n            if isinstance(data, indexing.ExplicitlyIndexed):\n                # Unambiguously handle array storage backends (like NetCDF4 and h5py)\n                # that can\'t handle general array indexing. For example, in netCDF4 you\n                # can do ""outer"" indexing along two dimensions independent, which works\n                # differently from how NumPy handles it.\n                # da.from_array works by using lazy indexing with a tuple of slices.\n                # Using OuterIndexer is a pragmatic choice: dask does not yet handle\n                # different indexing types in an explicit way:\n                # https://github.com/dask/dask/issues/2883\n                data = indexing.ImplicitToExplicitIndexingAdapter(\n                    data, indexing.OuterIndexer\n                )\n                if LooseVersion(dask.__version__) < ""2.0.0"":\n                    kwargs = {}\n                else:\n                    # All of our lazily loaded backend array classes should use NumPy\n                    # array operations.\n                    kwargs = {""meta"": np.ndarray}\n            else:\n                kwargs = {}\n\n            if utils.is_dict_like(chunks):\n                chunks = tuple(chunks.get(n, s) for n, s in enumerate(self.shape))\n\n            data = da.from_array(data, chunks, name=name, lock=lock, **kwargs)\n\n        return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)\n\n    def _as_sparse(self, sparse_format=_default, fill_value=dtypes.NA):\n        """"""\n        use sparse-array as backend.\n        """"""\n        import sparse\n\n        # TODO  what to do if dask-backended?\n        if fill_value is dtypes.NA:\n            dtype, fill_value = dtypes.maybe_promote(self.dtype)\n        else:\n            dtype = dtypes.result_type(self.dtype, fill_value)\n\n        if sparse_format is _default:\n            sparse_format = ""coo""\n        try:\n            as_sparse = getattr(sparse, ""as_{}"".format(sparse_format.lower()))\n        except AttributeError:\n            raise ValueError(""{} is not a valid sparse format"".format(sparse_format))\n\n        data = as_sparse(self.data.astype(dtype), fill_value=fill_value)\n        return self._replace(data=data)\n\n    def _to_dense(self):\n        """"""\n        Change backend from sparse to np.array\n        """"""\n        if hasattr(self._data, ""todense""):\n            return self._replace(data=self._data.todense())\n        return self.copy(deep=False)\n\n    def isel(\n        self: VariableType,\n        indexers: Mapping[Hashable, Any] = None,\n        missing_dims: str = ""raise"",\n        **indexers_kwargs: Any,\n    ) -> VariableType:\n        """"""Return a new array indexed along the specified dimension(s).\n\n        Parameters\n        ----------\n        **indexers : {dim: indexer, ...}\n            Keyword arguments with names matching dimensions and values given\n            by integers, slice objects or arrays.\n        missing_dims : {""raise"", ""warn"", ""ignore""}, default ""raise""\n            What to do if dimensions that should be selected from are not present in the\n            DataArray:\n            - ""exception"": raise an exception\n            - ""warning"": raise a warning, and ignore the missing dimensions\n            - ""ignore"": ignore the missing dimensions\n\n        Returns\n        -------\n        obj : Array object\n            A new Array with the selected data and dimensions. In general,\n            the new variable\'s data will be a view of this variable\'s data,\n            unless numpy fancy indexing was triggered by using an array\n            indexer, in which case the data will be a copy.\n        """"""\n        indexers = either_dict_or_kwargs(indexers, indexers_kwargs, ""isel"")\n\n        indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n\n        key = tuple(indexers.get(dim, slice(None)) for dim in self.dims)\n        return self[key]\n\n    def squeeze(self, dim=None):\n        """"""Return a new object with squeezed data.\n\n        Parameters\n        ----------\n        dim : None or str or tuple of str, optional\n            Selects a subset of the length one dimensions. If a dimension is\n            selected with length greater than one, an error is raised. If\n            None, all length one dimensions are squeezed.\n\n        Returns\n        -------\n        squeezed : same type as caller\n            This object, but with with all or a subset of the dimensions of\n            length 1 removed.\n\n        See Also\n        --------\n        numpy.squeeze\n        """"""\n        dims = common.get_squeeze_dims(self, dim)\n        return self.isel({d: 0 for d in dims})\n\n    def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):\n        axis = self.get_axis_num(dim)\n\n        if count > 0:\n            keep = slice(None, -count)\n        elif count < 0:\n            keep = slice(-count, None)\n        else:\n            keep = slice(None)\n\n        trimmed_data = self[(slice(None),) * axis + (keep,)].data\n\n        if fill_value is dtypes.NA:\n            dtype, fill_value = dtypes.maybe_promote(self.dtype)\n        else:\n            dtype = self.dtype\n\n        width = min(abs(count), self.shape[axis])\n        dim_pad = (width, 0) if count >= 0 else (0, width)\n        pads = [(0, 0) if d != dim else dim_pad for d in self.dims]\n\n        data = duck_array_ops.pad(\n            trimmed_data.astype(dtype),\n            pads,\n            mode=""constant"",\n            constant_values=fill_value,\n        )\n\n        if isinstance(data, dask_array_type):\n            # chunked data should come out with the same chunks; this makes\n            # it feasible to combine shifted and unshifted data\n            # TODO: remove this once dask.array automatically aligns chunks\n            data = data.rechunk(self.data.chunks)\n\n        return type(self)(self.dims, data, self._attrs, fastpath=True)\n\n    def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):\n        """"""\n        Return a new Variable with shifted data.\n\n        Parameters\n        ----------\n        shifts : mapping of the form {dim: offset}\n            Integer offset to shift along each of the given dimensions.\n            Positive offsets shift to the right; negative offsets shift to the\n            left.\n        fill_value: scalar, optional\n            Value to use for newly missing values\n        **shifts_kwargs:\n            The keyword arguments form of ``shifts``.\n            One of shifts or shifts_kwargs must be provided.\n\n        Returns\n        -------\n        shifted : Variable\n            Variable with the same dimensions and attributes but shifted data.\n        """"""\n        shifts = either_dict_or_kwargs(shifts, shifts_kwargs, ""shift"")\n        result = self\n        for dim, count in shifts.items():\n            result = result._shift_one_dim(dim, count, fill_value=fill_value)\n        return result\n\n    def _pad_options_dim_to_index(\n        self,\n        pad_option: Mapping[Hashable, Union[int, Tuple[int, int]]],\n        fill_with_shape=False,\n    ):\n        if fill_with_shape:\n            return [\n                (n, n) if d not in pad_option else pad_option[d]\n                for d, n in zip(self.dims, self.data.shape)\n            ]\n        return [(0, 0) if d not in pad_option else pad_option[d] for d in self.dims]\n\n    def pad(\n        self,\n        pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,\n        mode: str = ""constant"",\n        stat_length: Union[\n            int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n        ] = None,\n        constant_values: Union[\n            int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n        ] = None,\n        end_values: Union[\n            int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n        ] = None,\n        reflect_type: str = None,\n        **pad_width_kwargs: Any,\n    ):\n        """"""\n        Return a new Variable with padded data.\n\n        Parameters\n        ----------\n        pad_width: Mapping with the form of {dim: (pad_before, pad_after)}\n            Number of values padded along each dimension.\n            {dim: pad} is a shortcut for pad_before = pad_after = pad\n        mode: (str)\n            See numpy / Dask docs\n        stat_length : int, tuple or mapping of the form {dim: tuple}\n            Used in \'maximum\', \'mean\', \'median\', and \'minimum\'.  Number of\n            values at edge of each axis used to calculate the statistic value.\n        constant_values : scalar, tuple or mapping of the form {dim: tuple}\n            Used in \'constant\'.  The values to set the padded values for each\n            axis.\n        end_values : scalar, tuple or mapping of the form {dim: tuple}\n            Used in \'linear_ramp\'.  The values used for the ending value of the\n            linear_ramp and that will form the edge of the padded array.\n        reflect_type : {\'even\', \'odd\'}, optional\n            Used in \'reflect\', and \'symmetric\'.  The \'even\' style is the\n            default with an unaltered reflection around the edge value.  For\n            the \'odd\' style, the extended part of the array is created by\n            subtracting the reflected values from two times the edge value.\n        **pad_width_kwargs:\n            One of pad_width or pad_width_kwargs must be provided.\n\n        Returns\n        -------\n        padded : Variable\n            Variable with the same dimensions and attributes but padded data.\n        """"""\n        pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, ""pad"")\n\n        # change default behaviour of pad with mode constant\n        if mode == ""constant"" and (\n            constant_values is None or constant_values is dtypes.NA\n        ):\n            dtype, constant_values = dtypes.maybe_promote(self.dtype)\n        else:\n            dtype = self.dtype\n\n        # create pad_options_kwargs, numpy requires only relevant kwargs to be nonempty\n        if isinstance(stat_length, dict):\n            stat_length = self._pad_options_dim_to_index(\n                stat_length, fill_with_shape=True\n            )\n        if isinstance(constant_values, dict):\n            constant_values = self._pad_options_dim_to_index(constant_values)\n        if isinstance(end_values, dict):\n            end_values = self._pad_options_dim_to_index(end_values)\n\n        # workaround for bug in Dask\'s default value of stat_length  https://github.com/dask/dask/issues/5303\n        if stat_length is None and mode in [""maximum"", ""mean"", ""median"", ""minimum""]:\n            stat_length = [(n, n) for n in self.data.shape]  # type: ignore\n\n        # change integer values to a tuple of two of those values and change pad_width to index\n        for k, v in pad_width.items():\n            if isinstance(v, numbers.Number):\n                pad_width[k] = (v, v)\n        pad_width_by_index = self._pad_options_dim_to_index(pad_width)\n\n        # create pad_options_kwargs, numpy/dask requires only relevant kwargs to be nonempty\n        pad_option_kwargs = {}\n        if stat_length is not None:\n            pad_option_kwargs[""stat_length""] = stat_length\n        if constant_values is not None:\n            pad_option_kwargs[""constant_values""] = constant_values\n        if end_values is not None:\n            pad_option_kwargs[""end_values""] = end_values\n        if reflect_type is not None:\n            pad_option_kwargs[""reflect_type""] = reflect_type  # type: ignore\n\n        array = duck_array_ops.pad(\n            self.data.astype(dtype, copy=False),\n            pad_width_by_index,\n            mode=mode,\n            **pad_option_kwargs,\n        )\n\n        return type(self)(self.dims, array)\n\n    def _roll_one_dim(self, dim, count):\n        axis = self.get_axis_num(dim)\n\n        count %= self.shape[axis]\n        if count != 0:\n            indices = [slice(-count, None), slice(None, -count)]\n        else:\n            indices = [slice(None)]\n\n        arrays = [self[(slice(None),) * axis + (idx,)].data for idx in indices]\n\n        data = duck_array_ops.concatenate(arrays, axis)\n\n        if isinstance(data, dask_array_type):\n            # chunked data should come out with the same chunks; this makes\n            # it feasible to combine shifted and unshifted data\n            # TODO: remove this once dask.array automatically aligns chunks\n            data = data.rechunk(self.data.chunks)\n\n        return type(self)(self.dims, data, self._attrs, fastpath=True)\n\n    def roll(self, shifts=None, **shifts_kwargs):\n        """"""\n        Return a new Variable with rolld data.\n\n        Parameters\n        ----------\n        shifts : mapping of the form {dim: offset}\n            Integer offset to roll along each of the given dimensions.\n            Positive offsets roll to the right; negative offsets roll to the\n            left.\n        **shifts_kwargs:\n            The keyword arguments form of ``shifts``.\n            One of shifts or shifts_kwargs must be provided.\n\n        Returns\n        -------\n        shifted : Variable\n            Variable with the same dimensions and attributes but rolled data.\n        """"""\n        shifts = either_dict_or_kwargs(shifts, shifts_kwargs, ""roll"")\n\n        result = self\n        for dim, count in shifts.items():\n            result = result._roll_one_dim(dim, count)\n        return result\n\n    def transpose(self, *dims) -> ""Variable"":\n        """"""Return a new Variable object with transposed dimensions.\n\n        Parameters\n        ----------\n        *dims : str, optional\n            By default, reverse the dimensions. Otherwise, reorder the\n            dimensions to this order.\n\n        Returns\n        -------\n        transposed : Variable\n            The returned object has transposed data and dimensions with the\n            same attributes as the original.\n\n        Notes\n        -----\n        This operation returns a view of this variable\'s data. It is\n        lazy for dask-backed Variables but not for numpy-backed Variables.\n\n        See Also\n        --------\n        numpy.transpose\n        """"""\n        if len(dims) == 0:\n            dims = self.dims[::-1]\n        dims = tuple(infix_dims(dims, self.dims))\n        axes = self.get_axis_num(dims)\n        if len(dims) < 2 or dims == self.dims:\n            # no need to transpose if only one dimension\n            # or dims are in same order\n            return self.copy(deep=False)\n\n        data = as_indexable(self._data).transpose(axes)\n        return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n\n    @property\n    def T(self) -> ""Variable"":\n        return self.transpose()\n\n    def set_dims(self, dims, shape=None):\n        """"""Return a new variable with given set of dimensions.\n        This method might be used to attach new dimension(s) to variable.\n\n        When possible, this operation does not copy this variable\'s data.\n\n        Parameters\n        ----------\n        dims : str or sequence of str or dict\n            Dimensions to include on the new variable. If a dict, values are\n            used to provide the sizes of new dimensions; otherwise, new\n            dimensions are inserted with length 1.\n\n        Returns\n        -------\n        Variable\n        """"""\n        if isinstance(dims, str):\n            dims = [dims]\n\n        if shape is None and utils.is_dict_like(dims):\n            shape = dims.values()\n\n        missing_dims = set(self.dims) - set(dims)\n        if missing_dims:\n            raise ValueError(\n                ""new dimensions %r must be a superset of ""\n                ""existing dimensions %r"" % (dims, self.dims)\n            )\n\n        self_dims = set(self.dims)\n        expanded_dims = tuple(d for d in dims if d not in self_dims) + self.dims\n\n        if self.dims == expanded_dims:\n            # don\'t use broadcast_to unless necessary so the result remains\n            # writeable if possible\n            expanded_data = self.data\n        elif shape is not None:\n            dims_map = dict(zip(dims, shape))\n            tmp_shape = tuple(dims_map[d] for d in expanded_dims)\n            expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)\n        else:\n            expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]\n\n        expanded_var = Variable(\n            expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True\n        )\n        return expanded_var.transpose(*dims)\n\n    def _stack_once(self, dims, new_dim):\n        if not set(dims) <= set(self.dims):\n            raise ValueError(""invalid existing dimensions: %s"" % dims)\n\n        if new_dim in self.dims:\n            raise ValueError(\n                ""cannot create a new dimension with the same ""\n                ""name as an existing dimension""\n            )\n\n        if len(dims) == 0:\n            # don\'t stack\n            return self.copy(deep=False)\n\n        other_dims = [d for d in self.dims if d not in dims]\n        dim_order = other_dims + list(dims)\n        reordered = self.transpose(*dim_order)\n\n        new_shape = reordered.shape[: len(other_dims)] + (-1,)\n        new_data = reordered.data.reshape(new_shape)\n        new_dims = reordered.dims[: len(other_dims)] + (new_dim,)\n\n        return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)\n\n    def stack(self, dimensions=None, **dimensions_kwargs):\n        """"""\n        Stack any number of existing dimensions into a single new dimension.\n\n        New dimensions will be added at the end, and the order of the data\n        along each new dimension will be in contiguous (C) order.\n\n        Parameters\n        ----------\n        dimensions : Mapping of form new_name=(dim1, dim2, ...)\n            Names of new dimensions, and the existing dimensions that they\n            replace.\n        **dimensions_kwargs:\n            The keyword arguments form of ``dimensions``.\n            One of dimensions or dimensions_kwargs must be provided.\n\n        Returns\n        -------\n        stacked : Variable\n            Variable with the same attributes but stacked data.\n\n        See also\n        --------\n        Variable.unstack\n        """"""\n        dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, ""stack"")\n        result = self\n        for new_dim, dims in dimensions.items():\n            result = result._stack_once(dims, new_dim)\n        return result\n\n    def _unstack_once(self, dims, old_dim):\n        new_dim_names = tuple(dims.keys())\n        new_dim_sizes = tuple(dims.values())\n\n        if old_dim not in self.dims:\n            raise ValueError(""invalid existing dimension: %s"" % old_dim)\n\n        if set(new_dim_names).intersection(self.dims):\n            raise ValueError(\n                ""cannot create a new dimension with the same ""\n                ""name as an existing dimension""\n            )\n\n        if np.prod(new_dim_sizes) != self.sizes[old_dim]:\n            raise ValueError(\n                ""the product of the new dimension sizes must ""\n                ""equal the size of the old dimension""\n            )\n\n        other_dims = [d for d in self.dims if d != old_dim]\n        dim_order = other_dims + [old_dim]\n        reordered = self.transpose(*dim_order)\n\n        new_shape = reordered.shape[: len(other_dims)] + new_dim_sizes\n        new_data = reordered.data.reshape(new_shape)\n        new_dims = reordered.dims[: len(other_dims)] + new_dim_names\n\n        return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)\n\n    def unstack(self, dimensions=None, **dimensions_kwargs):\n        """"""\n        Unstack an existing dimension into multiple new dimensions.\n\n        New dimensions will be added at the end, and the order of the data\n        along each new dimension will be in contiguous (C) order.\n\n        Parameters\n        ----------\n        dimensions : mapping of the form old_dim={dim1: size1, ...}\n            Names of existing dimensions, and the new dimensions and sizes\n            that they map to.\n        **dimensions_kwargs:\n            The keyword arguments form of ``dimensions``.\n            One of dimensions or dimensions_kwargs must be provided.\n\n        Returns\n        -------\n        unstacked : Variable\n            Variable with the same attributes but unstacked data.\n\n        See also\n        --------\n        Variable.stack\n        """"""\n        dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, ""unstack"")\n        result = self\n        for old_dim, dims in dimensions.items():\n            result = result._unstack_once(dims, old_dim)\n        return result\n\n    def fillna(self, value):\n        return ops.fillna(self, value)\n\n    def where(self, cond, other=dtypes.NA):\n        return ops.where_method(self, cond, other)\n\n    def reduce(\n        self,\n        func,\n        dim=None,\n        axis=None,\n        keep_attrs=None,\n        keepdims=False,\n        allow_lazy=None,\n        **kwargs,\n    ):\n        """"""Reduce this array by applying `func` along some dimension(s).\n\n        Parameters\n        ----------\n        func : function\n            Function which can be called in the form\n            `func(x, axis=axis, **kwargs)` to return the result of reducing an\n            np.ndarray over an integer valued axis.\n        dim : str or sequence of str, optional\n            Dimension(s) over which to apply `func`.\n        axis : int or sequence of int, optional\n            Axis(es) over which to apply `func`. Only one of the \'dim\'\n            and \'axis\' arguments can be supplied. If neither are supplied, then\n            the reduction is calculated over the flattened array (by calling\n            `func(x)` without an axis argument).\n        keep_attrs : bool, optional\n            If True, the variable\'s attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n        keepdims : bool, default False\n            If True, the dimensions which are reduced are left in the result\n            as dimensions of size one\n        **kwargs : dict\n            Additional keyword arguments passed on to `func`.\n\n        Returns\n        -------\n        reduced : Array\n            Array with summarized data and the indicated dimension(s)\n            removed.\n        """"""\n        if dim == ...:\n            dim = None\n        if dim is not None and axis is not None:\n            raise ValueError(""cannot supply both \'axis\' and \'dim\' arguments"")\n\n        if dim is not None:\n            axis = self.get_axis_num(dim)\n\n        if allow_lazy is not None:\n            warnings.warn(\n                ""allow_lazy is deprecated and will be removed in version 0.16.0. It is now True by default."",\n                DeprecationWarning,\n            )\n        else:\n            allow_lazy = True\n\n        input_data = self.data if allow_lazy else self.values\n\n        if axis is not None:\n            data = func(input_data, axis=axis, **kwargs)\n        else:\n            data = func(input_data, **kwargs)\n\n        if getattr(data, ""shape"", ()) == self.shape:\n            dims = self.dims\n        else:\n            removed_axes = (\n                range(self.ndim) if axis is None else np.atleast_1d(axis) % self.ndim\n            )\n            if keepdims:\n                # Insert np.newaxis for removed dims\n                slices = tuple(\n                    np.newaxis if i in removed_axes else slice(None, None)\n                    for i in range(self.ndim)\n                )\n                if getattr(data, ""shape"", None) is None:\n                    # Reduce has produced a scalar value, not an array-like\n                    data = np.asanyarray(data)[slices]\n                else:\n                    data = data[slices]\n                dims = self.dims\n            else:\n                dims = [\n                    adim for n, adim in enumerate(self.dims) if n not in removed_axes\n                ]\n\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n        attrs = self._attrs if keep_attrs else None\n\n        return Variable(dims, data, attrs=attrs)\n\n    @classmethod\n    def concat(cls, variables, dim=""concat_dim"", positions=None, shortcut=False):\n        """"""Concatenate variables along a new or existing dimension.\n\n        Parameters\n        ----------\n        variables : iterable of Array\n            Arrays to stack together. Each variable is expected to have\n            matching dimensions and shape except for along the stacked\n            dimension.\n        dim : str or DataArray, optional\n            Name of the dimension to stack along. This can either be a new\n            dimension name, in which case it is added along axis=0, or an\n            existing dimension name, in which case the location of the\n            dimension is unchanged. Where to insert the new dimension is\n            determined by the first variable.\n        positions : None or list of integer arrays, optional\n            List of integer arrays which specifies the integer positions to\n            which to assign each dataset along the concatenated dimension.\n            If not supplied, objects are concatenated in the provided order.\n        shortcut : bool, optional\n            This option is used internally to speed-up groupby operations.\n            If `shortcut` is True, some checks of internal consistency between\n            arrays to concatenate are skipped.\n\n        Returns\n        -------\n        stacked : Variable\n            Concatenated Variable formed by stacking all the supplied variables\n            along the given dimension.\n        """"""\n        if not isinstance(dim, str):\n            (dim,) = dim.dims\n\n        # can\'t do this lazily: we need to loop through variables at least\n        # twice\n        variables = list(variables)\n        first_var = variables[0]\n\n        arrays = [v.data for v in variables]\n\n        if dim in first_var.dims:\n            axis = first_var.get_axis_num(dim)\n            dims = first_var.dims\n            data = duck_array_ops.concatenate(arrays, axis=axis)\n            if positions is not None:\n                # TODO: deprecate this option -- we don\'t need it for groupby\n                # any more.\n                indices = nputils.inverse_permutation(np.concatenate(positions))\n                data = duck_array_ops.take(data, indices, axis=axis)\n        else:\n            axis = 0\n            dims = (dim,) + first_var.dims\n            data = duck_array_ops.stack(arrays, axis=axis)\n\n        attrs = dict(first_var.attrs)\n        encoding = dict(first_var.encoding)\n        if not shortcut:\n            for var in variables:\n                if var.dims != first_var.dims:\n                    raise ValueError(\n                        f""Variable has dimensions {list(var.dims)} but first Variable has dimensions {list(first_var.dims)}""\n                    )\n\n        return cls(dims, data, attrs, encoding)\n\n    def equals(self, other, equiv=duck_array_ops.array_equiv):\n        """"""True if two Variables have the same dimensions and values;\n        otherwise False.\n\n        Variables can still be equal (like pandas objects) if they have NaN\n        values in the same locations.\n\n        This method is necessary because `v1 == v2` for Variables\n        does element-wise comparisons (like numpy.ndarrays).\n        """"""\n        other = getattr(other, ""variable"", other)\n        try:\n            return self.dims == other.dims and (\n                self._data is other._data or equiv(self.data, other.data)\n            )\n        except (TypeError, AttributeError):\n            return False\n\n    def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):\n        """"""True if two Variables have the values after being broadcast against\n        each other; otherwise False.\n\n        Variables can still be equal (like pandas objects) if they have NaN\n        values in the same locations.\n        """"""\n        try:\n            self, other = broadcast_variables(self, other)\n        except (ValueError, AttributeError):\n            return False\n        return self.equals(other, equiv=equiv)\n\n    def identical(self, other, equiv=duck_array_ops.array_equiv):\n        """"""Like equals, but also checks attributes.\n        """"""\n        try:\n            return utils.dict_equiv(self.attrs, other.attrs) and self.equals(\n                other, equiv=equiv\n            )\n        except (TypeError, AttributeError):\n            return False\n\n    def no_conflicts(self, other, equiv=duck_array_ops.array_notnull_equiv):\n        """"""True if the intersection of two Variable\'s non-null data is\n        equal; otherwise false.\n\n        Variables can thus still be equal if there are locations where either,\n        or both, contain NaN values.\n        """"""\n        return self.broadcast_equals(other, equiv=equiv)\n\n    def quantile(\n        self, q, dim=None, interpolation=""linear"", keep_attrs=None, skipna=True\n    ):\n        """"""Compute the qth quantile of the data along the specified dimension.\n\n        Returns the qth quantiles(s) of the array elements.\n\n        Parameters\n        ----------\n        q : float in range of [0,1] (or sequence of floats)\n            Quantile to compute, which must be between 0 and 1\n            inclusive.\n        dim : str or sequence of str, optional\n            Dimension(s) over which to apply quantile.\n        interpolation : {\'linear\', \'lower\', \'higher\', \'midpoint\', \'nearest\'}\n            This optional parameter specifies the interpolation method to\n            use when the desired quantile lies between two data points\n            ``i < j``:\n\n                * linear: ``i + (j - i) * fraction``, where ``fraction`` is\n                  the fractional part of the index surrounded by ``i`` and\n                  ``j``.\n                * lower: ``i``.\n                * higher: ``j``.\n                * nearest: ``i`` or ``j``, whichever is nearest.\n                * midpoint: ``(i + j) / 2``.\n\n        keep_attrs : bool, optional\n            If True, the variable\'s attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n\n        Returns\n        -------\n        quantiles : Variable\n            If `q` is a single quantile, then the result\n            is a scalar. If multiple percentiles are given, first axis of\n            the result corresponds to the quantile and a quantile dimension\n            is added to the return array. The other dimensions are the\n            dimensions that remain after the reduction of the array.\n\n        See Also\n        --------\n        numpy.nanquantile, pandas.Series.quantile, Dataset.quantile,\n        DataArray.quantile\n        """"""\n\n        from .computation import apply_ufunc\n\n        _quantile_func = np.nanquantile if skipna else np.quantile\n\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n\n        scalar = utils.is_scalar(q)\n        q = np.atleast_1d(np.asarray(q, dtype=np.float64))\n\n        if dim is None:\n            dim = self.dims\n\n        if utils.is_scalar(dim):\n            dim = [dim]\n\n        def _wrapper(npa, **kwargs):\n            # move quantile axis to end. required for apply_ufunc\n            return np.moveaxis(_quantile_func(npa, **kwargs), 0, -1)\n\n        axis = np.arange(-1, -1 * len(dim) - 1, -1)\n        result = apply_ufunc(\n            _wrapper,\n            self,\n            input_core_dims=[dim],\n            exclude_dims=set(dim),\n            output_core_dims=[[""quantile""]],\n            output_dtypes=[np.float64],\n            output_sizes={""quantile"": len(q)},\n            dask=""parallelized"",\n            kwargs={""q"": q, ""axis"": axis, ""interpolation"": interpolation},\n        )\n\n        # for backward compatibility\n        result = result.transpose(""quantile"", ...)\n        if scalar:\n            result = result.squeeze(""quantile"")\n        if keep_attrs:\n            result.attrs = self._attrs\n        return result\n\n    def rank(self, dim, pct=False):\n        """"""Ranks the data.\n\n        Equal values are assigned a rank that is the average of the ranks that\n        would have been otherwise assigned to all of the values within that\n        set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.\n\n        NaNs in the input array are returned as NaNs.\n\n        The `bottleneck` library is required.\n\n        Parameters\n        ----------\n        dim : str\n            Dimension over which to compute rank.\n        pct : bool, optional\n            If True, compute percentage ranks, otherwise compute integer ranks.\n\n        Returns\n        -------\n        ranked : Variable\n\n        See Also\n        --------\n        Dataset.rank, DataArray.rank\n        """"""\n        import bottleneck as bn\n\n        data = self.data\n\n        if isinstance(data, dask_array_type):\n            raise TypeError(\n                ""rank does not work for arrays stored as dask ""\n                ""arrays. Load the data via .compute() or .load() ""\n                ""prior to calling this method.""\n            )\n        elif not isinstance(data, np.ndarray):\n            raise TypeError(\n                ""rank is not implemented for {} objects."".format(type(data))\n            )\n\n        axis = self.get_axis_num(dim)\n        func = bn.nanrankdata if self.dtype.kind == ""f"" else bn.rankdata\n        ranked = func(data, axis=axis)\n        if pct:\n            count = np.sum(~np.isnan(data), axis=axis, keepdims=True)\n            ranked /= count\n        return Variable(self.dims, ranked)\n\n    def rolling_window(\n        self, dim, window, window_dim, center=False, fill_value=dtypes.NA\n    ):\n        """"""\n        Make a rolling_window along dim and add a new_dim to the last place.\n\n        Parameters\n        ----------\n        dim: str\n            Dimension over which to compute rolling_window\n        window: int\n            Window size of the rolling\n        window_dim: str\n            New name of the window dimension.\n        center: boolean. default False.\n            If True, pad fill_value for both ends. Otherwise, pad in the head\n            of the axis.\n        fill_value:\n            value to be filled.\n\n        Returns\n        -------\n        Variable that is a view of the original array with a added dimension of\n        size w.\n        The return dim: self.dims + (window_dim, )\n        The return shape: self.shape + (window, )\n\n        Examples\n        --------\n        >>> v = Variable((""a"", ""b""), np.arange(8).reshape((2, 4)))\n        >>> v.rolling_window(x, ""b"", 3, ""window_dim"")\n        <xarray.Variable (a: 2, b: 4, window_dim: 3)>\n        array([[[nan, nan, 0], [nan, 0, 1], [0, 1, 2], [1, 2, 3]],\n               [[nan, nan, 4], [nan, 4, 5], [4, 5, 6], [5, 6, 7]]])\n\n        >>> v.rolling_window(x, ""b"", 3, ""window_dim"", center=True)\n        <xarray.Variable (a: 2, b: 4, window_dim: 3)>\n        array([[[nan, 0, 1], [0, 1, 2], [1, 2, 3], [2, 3, nan]],\n               [[nan, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, nan]]])\n        """"""\n        if fill_value is dtypes.NA:  # np.nan is passed\n            dtype, fill_value = dtypes.maybe_promote(self.dtype)\n            array = self.astype(dtype, copy=False).data\n        else:\n            dtype = self.dtype\n            array = self.data\n\n        new_dims = self.dims + (window_dim,)\n        return Variable(\n            new_dims,\n            duck_array_ops.rolling_window(\n                array,\n                axis=self.get_axis_num(dim),\n                window=window,\n                center=center,\n                fill_value=fill_value,\n            ),\n        )\n\n    def coarsen(self, windows, func, boundary=""exact"", side=""left"", **kwargs):\n        """"""\n        Apply reduction function.\n        """"""\n        windows = {k: v for k, v in windows.items() if k in self.dims}\n        if not windows:\n            return self.copy()\n\n        reshaped, axes = self._coarsen_reshape(windows, boundary, side)\n        if isinstance(func, str):\n            name = func\n            func = getattr(duck_array_ops, name, None)\n            if func is None:\n                raise NameError(f""{name} is not a valid method."")\n        return self._replace(data=func(reshaped, axis=axes, **kwargs))\n\n    def _coarsen_reshape(self, windows, boundary, side):\n        """"""\n        Construct a reshaped-array for coarsen\n        """"""\n        if not utils.is_dict_like(boundary):\n            boundary = {d: boundary for d in windows.keys()}\n\n        if not utils.is_dict_like(side):\n            side = {d: side for d in windows.keys()}\n\n        # remove unrelated dimensions\n        boundary = {k: v for k, v in boundary.items() if k in windows}\n        side = {k: v for k, v in side.items() if k in windows}\n\n        for d, window in windows.items():\n            if window <= 0:\n                raise ValueError(f""window must be > 0. Given {window}"")\n\n        variable = self\n        for d, window in windows.items():\n            # trim or pad the object\n            size = variable.shape[self._get_axis_num(d)]\n            n = int(size / window)\n            if boundary[d] == ""exact"":\n                if n * window != size:\n                    raise ValueError(\n                        ""Could not coarsen a dimension of size {} with ""\n                        ""window {}"".format(size, window)\n                    )\n            elif boundary[d] == ""trim"":\n                if side[d] == ""left"":\n                    variable = variable.isel({d: slice(0, window * n)})\n                else:\n                    excess = size - window * n\n                    variable = variable.isel({d: slice(excess, None)})\n            elif boundary[d] == ""pad"":  # pad\n                pad = window * n - size\n                if pad < 0:\n                    pad += window\n                if side[d] == ""left"":\n                    pad_width = {d: (0, pad)}\n                else:\n                    pad_width = {d: (pad, 0)}\n                variable = variable.pad(pad_width, mode=""constant"")\n            else:\n                raise TypeError(\n                    ""{} is invalid for boundary. Valid option is \'exact\', ""\n                    ""\'trim\' and \'pad\'"".format(boundary[d])\n                )\n\n        shape = []\n        axes = []\n        axis_count = 0\n        for i, d in enumerate(variable.dims):\n            if d in windows:\n                size = variable.shape[i]\n                shape.append(int(size / windows[d]))\n                shape.append(windows[d])\n                axis_count += 1\n                axes.append(i + axis_count)\n            else:\n                shape.append(variable.shape[i])\n\n        keep_attrs = _get_keep_attrs(default=False)\n        variable.attrs = variable._attrs if keep_attrs else {}\n\n        return variable.data.reshape(shape), tuple(axes)\n\n    @property\n    def real(self):\n        return type(self)(self.dims, self.data.real, self._attrs)\n\n    @property\n    def imag(self):\n        return type(self)(self.dims, self.data.imag, self._attrs)\n\n    def __array_wrap__(self, obj, context=None):\n        return Variable(self.dims, obj)\n\n    @staticmethod\n    def _unary_op(f):\n        @functools.wraps(f)\n        def func(self, *args, **kwargs):\n            with np.errstate(all=""ignore""):\n                return self.__array_wrap__(f(self.data, *args, **kwargs))\n\n        return func\n\n    @staticmethod\n    def _binary_op(f, reflexive=False, **ignored_kwargs):\n        @functools.wraps(f)\n        def func(self, other):\n            if isinstance(other, (xr.DataArray, xr.Dataset)):\n                return NotImplemented\n            self_data, other_data, dims = _broadcast_compat_data(self, other)\n            keep_attrs = _get_keep_attrs(default=False)\n            attrs = self._attrs if keep_attrs else None\n            with np.errstate(all=""ignore""):\n                new_data = (\n                    f(self_data, other_data)\n                    if not reflexive\n                    else f(other_data, self_data)\n                )\n            result = Variable(dims, new_data, attrs=attrs)\n            return result\n\n        return func\n\n    @staticmethod\n    def _inplace_binary_op(f):\n        @functools.wraps(f)\n        def func(self, other):\n            if isinstance(other, xr.Dataset):\n                raise TypeError(""cannot add a Dataset to a Variable in-place"")\n            self_data, other_data, dims = _broadcast_compat_data(self, other)\n            if dims != self.dims:\n                raise ValueError(""dimensions cannot change for in-place "" ""operations"")\n            with np.errstate(all=""ignore""):\n                self.values = f(self_data, other_data)\n            return self\n\n        return func\n\n    def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):\n        """""" A (private) method to convert datetime array to numeric dtype\n        See duck_array_ops.datetime_to_numeric\n        """"""\n        numeric_array = duck_array_ops.datetime_to_numeric(\n            self.data, offset, datetime_unit, dtype\n        )\n        return type(self)(self.dims, numeric_array, self._attrs)\n\n\nops.inject_all_ops_and_reduce_methods(Variable)\n\n\nclass IndexVariable(Variable):\n    """"""Wrapper for accommodating a pandas.Index in an xarray.Variable.\n\n    IndexVariable preserve loaded values in the form of a pandas.Index instead\n    of a NumPy array. Hence, their values are immutable and must always be one-\n    dimensional.\n\n    They also have a name property, which is the name of their sole dimension\n    unless another name is given.\n    """"""\n\n    __slots__ = ()\n\n    def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n        super().__init__(dims, data, attrs, encoding, fastpath)\n        if self.ndim != 1:\n            raise ValueError(""%s objects must be 1-dimensional"" % type(self).__name__)\n\n        # Unlike in Variable, always eagerly load values into memory\n        if not isinstance(self._data, PandasIndexAdapter):\n            self._data = PandasIndexAdapter(self._data)\n\n    def __dask_tokenize__(self):\n        from dask.base import normalize_token\n\n        # Don\'t waste time converting pd.Index to np.ndarray\n        return normalize_token((type(self), self._dims, self._data.array, self._attrs))\n\n    def load(self):\n        # data is already loaded into memory for IndexVariable\n        return self\n\n    # https://github.com/python/mypy/issues/1465\n    @Variable.data.setter  # type: ignore\n    def data(self, data):\n        raise ValueError(\n            f""Cannot assign to the .data attribute of dimension coordinate a.k.a IndexVariable {self.name!r}. ""\n            f""Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate.""\n        )\n\n    @Variable.values.setter  # type: ignore\n    def values(self, values):\n        raise ValueError(\n            f""Cannot assign to the .values attribute of dimension coordinate a.k.a IndexVariable {self.name!r}. ""\n            f""Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate.""\n        )\n\n    def chunk(self, chunks=None, name=None, lock=False):\n        # Dummy - do not chunk. This method is invoked e.g. by Dataset.chunk()\n        return self.copy(deep=False)\n\n    def _as_sparse(self, sparse_format=_default, fill_value=_default):\n        # Dummy\n        return self.copy(deep=False)\n\n    def _to_dense(self):\n        # Dummy\n        return self.copy(deep=False)\n\n    def _finalize_indexing_result(self, dims, data):\n        if getattr(data, ""ndim"", 0) != 1:\n            # returns Variable rather than IndexVariable if multi-dimensional\n            return Variable(dims, data, self._attrs, self._encoding)\n        else:\n            return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n\n    def __setitem__(self, key, value):\n        raise TypeError(""%s values cannot be modified"" % type(self).__name__)\n\n    @classmethod\n    def concat(cls, variables, dim=""concat_dim"", positions=None, shortcut=False):\n        """"""Specialized version of Variable.concat for IndexVariable objects.\n\n        This exists because we want to avoid converting Index objects to NumPy\n        arrays, if possible.\n        """"""\n        if not isinstance(dim, str):\n            (dim,) = dim.dims\n\n        variables = list(variables)\n        first_var = variables[0]\n\n        if any(not isinstance(v, cls) for v in variables):\n            raise TypeError(\n                ""IndexVariable.concat requires that all input ""\n                ""variables be IndexVariable objects""\n            )\n\n        indexes = [v._data.array for v in variables]\n\n        if not indexes:\n            data = []\n        else:\n            data = indexes[0].append(indexes[1:])\n\n            if positions is not None:\n                indices = nputils.inverse_permutation(np.concatenate(positions))\n                data = data.take(indices)\n\n        attrs = dict(first_var.attrs)\n        if not shortcut:\n            for var in variables:\n                if var.dims != first_var.dims:\n                    raise ValueError(""inconsistent dimensions"")\n                utils.remove_incompatible_items(attrs, var.attrs)\n\n        return cls(first_var.dims, data, attrs)\n\n    def copy(self, deep=True, data=None):\n        """"""Returns a copy of this object.\n\n        `deep` is ignored since data is stored in the form of\n        pandas.Index, which is already immutable. Dimensions, attributes\n        and encodings are always copied.\n\n        Use `data` to create a new object with the same structure as\n        original but entirely new data.\n\n        Parameters\n        ----------\n        deep : bool, optional\n            Deep is ignored when data is given. Whether the data array is\n            loaded into memory and copied onto the new object. Default is True.\n        data : array_like, optional\n            Data to use in the new object. Must have same shape as original.\n\n        Returns\n        -------\n        object : Variable\n            New object with dimensions, attributes, encodings, and optionally\n            data copied from original.\n        """"""\n        if data is None:\n            data = self._data.copy(deep=deep)\n        else:\n            data = as_compatible_data(data)\n            if self.shape != data.shape:\n                raise ValueError(\n                    ""Data shape {} must match shape of object {}"".format(\n                        data.shape, self.shape\n                    )\n                )\n        return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)\n\n    def equals(self, other, equiv=None):\n        # if equiv is specified, super up\n        if equiv is not None:\n            return super().equals(other, equiv)\n\n        # otherwise use the native index equals, rather than looking at _data\n        other = getattr(other, ""variable"", other)\n        try:\n            return self.dims == other.dims and self._data_equals(other)\n        except (TypeError, AttributeError):\n            return False\n\n    def _data_equals(self, other):\n        return self.to_index().equals(other.to_index())\n\n    def to_index_variable(self):\n        """"""Return this variable as an xarray.IndexVariable""""""\n        return self\n\n    to_coord = utils.alias(to_index_variable, ""to_coord"")\n\n    def to_index(self):\n        """"""Convert this variable to a pandas.Index""""""\n        # n.b. creating a new pandas.Index from an old pandas.Index is\n        # basically free as pandas.Index objects are immutable\n        assert self.ndim == 1\n        index = self._data.array\n        if isinstance(index, pd.MultiIndex):\n            # set default names for multi-index unnamed levels so that\n            # we can safely rename dimension / coordinate later\n            valid_level_names = [\n                name or ""{}_level_{}"".format(self.dims[0], i)\n                for i, name in enumerate(index.names)\n            ]\n            index = index.set_names(valid_level_names)\n        else:\n            index = index.set_names(self.name)\n        return index\n\n    @property\n    def level_names(self):\n        """"""Return MultiIndex level names or None if this IndexVariable has no\n        MultiIndex.\n        """"""\n        index = self.to_index()\n        if isinstance(index, pd.MultiIndex):\n            return index.names\n        else:\n            return None\n\n    def get_level_variable(self, level):\n        """"""Return a new IndexVariable from a given MultiIndex level.""""""\n        if self.level_names is None:\n            raise ValueError(""IndexVariable %r has no MultiIndex"" % self.name)\n        index = self.to_index()\n        return type(self)(self.dims, index.get_level_values(level))\n\n    @property\n    def name(self):\n        return self.dims[0]\n\n    @name.setter\n    def name(self, value):\n        raise AttributeError(""cannot modify name of IndexVariable in-place"")\n\n\n# for backwards compatibility\nCoordinate = utils.alias(IndexVariable, ""Coordinate"")\n\n\ndef _unified_dims(variables):\n    # validate dimensions\n    all_dims = {}\n    for var in variables:\n        var_dims = var.dims\n        if len(set(var_dims)) < len(var_dims):\n            raise ValueError(\n                ""broadcasting cannot handle duplicate ""\n                ""dimensions: %r"" % list(var_dims)\n            )\n        for d, s in zip(var_dims, var.shape):\n            if d not in all_dims:\n                all_dims[d] = s\n            elif all_dims[d] != s:\n                raise ValueError(\n                    ""operands cannot be broadcast together ""\n                    ""with mismatched lengths for dimension %r: %s""\n                    % (d, (all_dims[d], s))\n                )\n    return all_dims\n\n\ndef _broadcast_compat_variables(*variables):\n    """"""Create broadcast compatible variables, with the same dimensions.\n\n    Unlike the result of broadcast_variables(), some variables may have\n    dimensions of size 1 instead of the the size of the broadcast dimension.\n    """"""\n    dims = tuple(_unified_dims(variables))\n    return tuple(var.set_dims(dims) if var.dims != dims else var for var in variables)\n\n\ndef broadcast_variables(*variables):\n    """"""Given any number of variables, return variables with matching dimensions\n    and broadcast data.\n\n    The data on the returned variables will be a view of the data on the\n    corresponding original arrays, but dimensions will be reordered and\n    inserted so that both broadcast arrays have the same dimensions. The new\n    dimensions are sorted in order of appearance in the first variable\'s\n    dimensions followed by the second variable\'s dimensions.\n    """"""\n    dims_map = _unified_dims(variables)\n    dims_tuple = tuple(dims_map)\n    return tuple(\n        var.set_dims(dims_map) if var.dims != dims_tuple else var for var in variables\n    )\n\n\ndef _broadcast_compat_data(self, other):\n    if all(hasattr(other, attr) for attr in [""dims"", ""data"", ""shape"", ""encoding""]):\n        # `other` satisfies the necessary Variable API for broadcast_variables\n        new_self, new_other = _broadcast_compat_variables(self, other)\n        self_data = new_self.data\n        other_data = new_other.data\n        dims = new_self.dims\n    else:\n        # rely on numpy broadcasting rules\n        self_data = self.data\n        other_data = other\n        dims = self.dims\n    return self_data, other_data, dims\n\n\ndef concat(variables, dim=""concat_dim"", positions=None, shortcut=False):\n    """"""Concatenate variables along a new or existing dimension.\n\n    Parameters\n    ----------\n    variables : iterable of Array\n        Arrays to stack together. Each variable is expected to have\n        matching dimensions and shape except for along the stacked\n        dimension.\n    dim : str or DataArray, optional\n        Name of the dimension to stack along. This can either be a new\n        dimension name, in which case it is added along axis=0, or an\n        existing dimension name, in which case the location of the\n        dimension is unchanged. Where to insert the new dimension is\n        determined by the first variable.\n    positions : None or list of integer arrays, optional\n        List of integer arrays which specifies the integer positions to which\n        to assign each dataset along the concatenated dimension. If not\n        supplied, objects are concatenated in the provided order.\n    shortcut : bool, optional\n        This option is used internally to speed-up groupby operations.\n        If `shortcut` is True, some checks of internal consistency between\n        arrays to concatenate are skipped.\n\n    Returns\n    -------\n    stacked : Variable\n        Concatenated Variable formed by stacking all the supplied variables\n        along the given dimension.\n    """"""\n    variables = list(variables)\n    if all(isinstance(v, IndexVariable) for v in variables):\n        return IndexVariable.concat(variables, dim, positions, shortcut)\n    else:\n        return Variable.concat(variables, dim, positions, shortcut)\n\n\ndef assert_unique_multiindex_level_names(variables):\n    """"""Check for uniqueness of MultiIndex level names in all given\n    variables.\n\n    Not public API. Used for checking consistency of DataArray and Dataset\n    objects.\n    """"""\n    level_names = defaultdict(list)\n    all_level_names = set()\n    for var_name, var in variables.items():\n        if isinstance(var._data, PandasIndexAdapter):\n            idx_level_names = var.to_index_variable().level_names\n            if idx_level_names is not None:\n                for n in idx_level_names:\n                    level_names[n].append(f""{n!r} ({var_name})"")\n            if idx_level_names:\n                all_level_names.update(idx_level_names)\n\n    for k, v in level_names.items():\n        if k in variables:\n            v.append(""(%s)"" % k)\n\n    duplicate_names = [v for v in level_names.values() if len(v) > 1]\n    if duplicate_names:\n        conflict_str = ""\\n"".join("", "".join(v) for v in duplicate_names)\n        raise ValueError(""conflicting MultiIndex level name(s):\\n%s"" % conflict_str)\n    # Check confliction between level names and dimensions GH:2299\n    for k, v in variables.items():\n        for d in v.dims:\n            if d in all_level_names:\n                raise ValueError(\n                    ""conflicting level / dimension names. {} ""\n                    ""already exists as a level name."".format(d)\n                )\n'"
xarray/core/weighted.py,0,"b'from typing import TYPE_CHECKING, Hashable, Iterable, Optional, Union, overload\n\nfrom .computation import dot\nfrom .options import _get_keep_attrs\n\nif TYPE_CHECKING:\n    from .dataarray import DataArray, Dataset\n\n_WEIGHTED_REDUCE_DOCSTRING_TEMPLATE = """"""\n    Reduce this {cls}\'s data by a weighted ``{fcn}`` along some dimension(s).\n\n    Parameters\n    ----------\n    dim : str or sequence of str, optional\n        Dimension(s) over which to apply the weighted ``{fcn}``.\n    skipna : bool, optional\n        If True, skip missing values (as marked by NaN). By default, only\n        skips missing values for float dtypes; other dtypes either do not\n        have a sentinel missing value (int) or skipna=True has not been\n        implemented (object, datetime64 or timedelta64).\n    keep_attrs : bool, optional\n        If True, the attributes (``attrs``) will be copied from the original\n        object to the new one.  If False (default), the new object will be\n        returned without attributes.\n\n    Returns\n    -------\n    reduced : {cls}\n        New {cls} object with weighted ``{fcn}`` applied to its data and\n        the indicated dimension(s) removed.\n\n    Notes\n    -----\n        Returns {on_zero} if the ``weights`` sum to 0.0 along the reduced\n        dimension(s).\n    """"""\n\n_SUM_OF_WEIGHTS_DOCSTRING = """"""\n    Calculate the sum of weights, accounting for missing values in the data\n\n    Parameters\n    ----------\n    dim : str or sequence of str, optional\n        Dimension(s) over which to sum the weights.\n    keep_attrs : bool, optional\n        If True, the attributes (``attrs``) will be copied from the original\n        object to the new one.  If False (default), the new object will be\n        returned without attributes.\n\n    Returns\n    -------\n    reduced : {cls}\n        New {cls} object with the sum of the weights over the given dimension.\n    """"""\n\n\nclass Weighted:\n    """"""An object that implements weighted operations.\n\n    You should create a Weighted object by using the ``DataArray.weighted`` or\n    ``Dataset.weighted`` methods.\n\n    See Also\n    --------\n    Dataset.weighted\n    DataArray.weighted\n    """"""\n\n    __slots__ = (""obj"", ""weights"")\n\n    @overload\n    def __init__(self, obj: ""DataArray"", weights: ""DataArray"") -> None:\n        ...\n\n    @overload  # noqa: F811\n    def __init__(self, obj: ""Dataset"", weights: ""DataArray"") -> None:  # noqa: F811\n        ...\n\n    def __init__(self, obj, weights):  # noqa: F811\n        """"""\n        Create a Weighted object\n\n        Parameters\n        ----------\n        obj : DataArray or Dataset\n            Object over which the weighted reduction operation is applied.\n        weights : DataArray\n            An array of weights associated with the values in the obj.\n            Each value in the obj contributes to the reduction operation\n            according to its associated weight.\n\n        Notes\n        -----\n        ``weights`` must be a ``DataArray`` and cannot contain missing values.\n        Missing values can be replaced by ``weights.fillna(0)``.\n        """"""\n\n        from .dataarray import DataArray\n\n        if not isinstance(weights, DataArray):\n            raise ValueError(""`weights` must be a DataArray"")\n\n        if weights.isnull().any():\n            raise ValueError(\n                ""`weights` cannot contain missing values. ""\n                ""Missing values can be replaced by `weights.fillna(0)`.""\n            )\n\n        self.obj = obj\n        self.weights = weights\n\n    @staticmethod\n    def _reduce(\n        da: ""DataArray"",\n        weights: ""DataArray"",\n        dim: Optional[Union[Hashable, Iterable[Hashable]]] = None,\n        skipna: Optional[bool] = None,\n    ) -> ""DataArray"":\n        """"""reduce using dot; equivalent to (da * weights).sum(dim, skipna)\n\n            for internal use only\n        """"""\n\n        # need to infer dims as we use `dot`\n        if dim is None:\n            dim = ...\n\n        # need to mask invalid values in da, as `dot` does not implement skipna\n        if skipna or (skipna is None and da.dtype.kind in ""cfO""):\n            da = da.fillna(0.0)\n\n        # `dot` does not broadcast arrays, so this avoids creating a large\n        # DataArray (if `weights` has additional dimensions)\n        # maybe add fasttrack (`(da * weights).sum(dims=dim, skipna=skipna)`)\n        return dot(da, weights, dims=dim)\n\n    def _sum_of_weights(\n        self, da: ""DataArray"", dim: Optional[Union[Hashable, Iterable[Hashable]]] = None\n    ) -> ""DataArray"":\n        """""" Calculate the sum of weights, accounting for missing values """"""\n\n        # we need to mask data values that are nan; else the weights are wrong\n        mask = da.notnull()\n\n        # bool -> int, because ``xr.dot([True, True], [True, True])`` -> True\n        # (and not 2); GH4074\n        if self.weights.dtype == bool:\n            sum_of_weights = self._reduce(\n                mask, self.weights.astype(int), dim=dim, skipna=False\n            )\n        else:\n            sum_of_weights = self._reduce(mask, self.weights, dim=dim, skipna=False)\n\n        # 0-weights are not valid\n        valid_weights = sum_of_weights != 0.0\n\n        return sum_of_weights.where(valid_weights)\n\n    def _weighted_sum(\n        self,\n        da: ""DataArray"",\n        dim: Optional[Union[Hashable, Iterable[Hashable]]] = None,\n        skipna: Optional[bool] = None,\n    ) -> ""DataArray"":\n        """"""Reduce a DataArray by a by a weighted ``sum`` along some dimension(s).""""""\n\n        return self._reduce(da, self.weights, dim=dim, skipna=skipna)\n\n    def _weighted_mean(\n        self,\n        da: ""DataArray"",\n        dim: Optional[Union[Hashable, Iterable[Hashable]]] = None,\n        skipna: Optional[bool] = None,\n    ) -> ""DataArray"":\n        """"""Reduce a DataArray by a weighted ``mean`` along some dimension(s).""""""\n\n        weighted_sum = self._weighted_sum(da, dim=dim, skipna=skipna)\n\n        sum_of_weights = self._sum_of_weights(da, dim=dim)\n\n        return weighted_sum / sum_of_weights\n\n    def _implementation(self, func, dim, **kwargs):\n\n        raise NotImplementedError(""Use `Dataset.weighted` or `DataArray.weighted`"")\n\n    def sum_of_weights(\n        self,\n        dim: Optional[Union[Hashable, Iterable[Hashable]]] = None,\n        keep_attrs: Optional[bool] = None,\n    ) -> Union[""DataArray"", ""Dataset""]:\n\n        return self._implementation(\n            self._sum_of_weights, dim=dim, keep_attrs=keep_attrs\n        )\n\n    def sum(\n        self,\n        dim: Optional[Union[Hashable, Iterable[Hashable]]] = None,\n        skipna: Optional[bool] = None,\n        keep_attrs: Optional[bool] = None,\n    ) -> Union[""DataArray"", ""Dataset""]:\n\n        return self._implementation(\n            self._weighted_sum, dim=dim, skipna=skipna, keep_attrs=keep_attrs\n        )\n\n    def mean(\n        self,\n        dim: Optional[Union[Hashable, Iterable[Hashable]]] = None,\n        skipna: Optional[bool] = None,\n        keep_attrs: Optional[bool] = None,\n    ) -> Union[""DataArray"", ""Dataset""]:\n\n        return self._implementation(\n            self._weighted_mean, dim=dim, skipna=skipna, keep_attrs=keep_attrs\n        )\n\n    def __repr__(self):\n        """"""provide a nice str repr of our Weighted object""""""\n\n        klass = self.__class__.__name__\n        weight_dims = "", "".join(self.weights.dims)\n        return f""{klass} with weights along dimensions: {weight_dims}""\n\n\nclass DataArrayWeighted(Weighted):\n    def _implementation(self, func, dim, **kwargs):\n\n        keep_attrs = kwargs.pop(""keep_attrs"")\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n\n        weighted = func(self.obj, dim=dim, **kwargs)\n\n        if keep_attrs:\n            weighted.attrs = self.obj.attrs\n\n        return weighted\n\n\nclass DatasetWeighted(Weighted):\n    def _implementation(self, func, dim, **kwargs) -> ""Dataset"":\n\n        return self.obj.map(func, dim=dim, **kwargs)\n\n\ndef _inject_docstring(cls, cls_name):\n\n    cls.sum_of_weights.__doc__ = _SUM_OF_WEIGHTS_DOCSTRING.format(cls=cls_name)\n\n    cls.sum.__doc__ = _WEIGHTED_REDUCE_DOCSTRING_TEMPLATE.format(\n        cls=cls_name, fcn=""sum"", on_zero=""0""\n    )\n\n    cls.mean.__doc__ = _WEIGHTED_REDUCE_DOCSTRING_TEMPLATE.format(\n        cls=cls_name, fcn=""mean"", on_zero=""NaN""\n    )\n\n\n_inject_docstring(DataArrayWeighted, ""DataArray"")\n_inject_docstring(DatasetWeighted, ""Dataset"")\n'"
xarray/plot/__init__.py,0,"b'from .dataset_plot import scatter\nfrom .facetgrid import FacetGrid\nfrom .plot import contour, contourf, hist, imshow, line, pcolormesh, plot, step\n\n__all__ = [\n    ""plot"",\n    ""line"",\n    ""step"",\n    ""contour"",\n    ""contourf"",\n    ""hist"",\n    ""imshow"",\n    ""pcolormesh"",\n    ""FacetGrid"",\n    ""scatter"",\n]\n'"
xarray/plot/dataset_plot.py,9,"b'import functools\n\nimport numpy as np\nimport pandas as pd\n\nfrom ..core.alignment import broadcast\nfrom .facetgrid import _easy_facetgrid\nfrom .utils import (\n    _add_colorbar,\n    _is_numeric,\n    _process_cmap_cbar_kwargs,\n    get_axis,\n    label_from_attrs,\n)\n\n# copied from seaborn\n_MARKERSIZE_RANGE = np.array([18.0, 72.0])\n\n\ndef _infer_meta_data(ds, x, y, hue, hue_style, add_guide):\n    dvars = set(ds.variables.keys())\n    error_msg = "" must be one of ({:s})"".format("", "".join(dvars))\n\n    if x not in dvars:\n        raise ValueError(""x"" + error_msg)\n\n    if y not in dvars:\n        raise ValueError(""y"" + error_msg)\n\n    if hue is not None and hue not in dvars:\n        raise ValueError(""hue"" + error_msg)\n\n    if hue:\n        hue_is_numeric = _is_numeric(ds[hue].values)\n\n        if hue_style is None:\n            hue_style = ""continuous"" if hue_is_numeric else ""discrete""\n\n        if not hue_is_numeric and (hue_style == ""continuous""):\n            raise ValueError(\n                ""Cannot create a colorbar for a non numeric"" "" coordinate: "" + hue\n            )\n\n        if add_guide is None or add_guide is True:\n            add_colorbar = True if hue_style == ""continuous"" else False\n            add_legend = True if hue_style == ""discrete"" else False\n        else:\n            add_colorbar = False\n            add_legend = False\n    else:\n        if add_guide is True:\n            raise ValueError(""Cannot set add_guide when hue is None."")\n        add_legend = False\n        add_colorbar = False\n\n    if hue_style is not None and hue_style not in [""discrete"", ""continuous""]:\n        raise ValueError(\n            ""hue_style must be either None, \'discrete\' "" ""or \'continuous\'.""\n        )\n\n    if hue:\n        hue_label = label_from_attrs(ds[hue])\n        hue = ds[hue]\n    else:\n        hue_label = None\n        hue = None\n\n    return {\n        ""add_colorbar"": add_colorbar,\n        ""add_legend"": add_legend,\n        ""hue_label"": hue_label,\n        ""hue_style"": hue_style,\n        ""xlabel"": label_from_attrs(ds[x]),\n        ""ylabel"": label_from_attrs(ds[y]),\n        ""hue"": hue,\n    }\n\n\ndef _infer_scatter_data(ds, x, y, hue, markersize, size_norm, size_mapping=None):\n\n    broadcast_keys = [""x"", ""y""]\n    to_broadcast = [ds[x], ds[y]]\n    if hue:\n        to_broadcast.append(ds[hue])\n        broadcast_keys.append(""hue"")\n    if markersize:\n        to_broadcast.append(ds[markersize])\n        broadcast_keys.append(""size"")\n\n    broadcasted = dict(zip(broadcast_keys, broadcast(*to_broadcast)))\n\n    data = {""x"": broadcasted[""x""], ""y"": broadcasted[""y""], ""hue"": None, ""sizes"": None}\n\n    if hue:\n        data[""hue""] = broadcasted[""hue""]\n\n    if markersize:\n        size = broadcasted[""size""]\n\n        if size_mapping is None:\n            size_mapping = _parse_size(size, size_norm)\n\n        data[""sizes""] = size.copy(\n            data=np.reshape(size_mapping.loc[size.values.ravel()].values, size.shape)\n        )\n\n    return data\n\n\n# copied from seaborn\ndef _parse_size(data, norm):\n\n    import matplotlib as mpl\n\n    if data is None:\n        return None\n\n    data = data.values.flatten()\n\n    if not _is_numeric(data):\n        levels = np.unique(data)\n        numbers = np.arange(1, 1 + len(levels))[::-1]\n    else:\n        levels = numbers = np.sort(np.unique(data))\n\n    min_width, max_width = _MARKERSIZE_RANGE\n    # width_range = min_width, max_width\n\n    if norm is None:\n        norm = mpl.colors.Normalize()\n    elif isinstance(norm, tuple):\n        norm = mpl.colors.Normalize(*norm)\n    elif not isinstance(norm, mpl.colors.Normalize):\n        err = ""``size_norm`` must be None, tuple, "" ""or Normalize object.""\n        raise ValueError(err)\n\n    norm.clip = True\n    if not norm.scaled():\n        norm(np.asarray(numbers))\n    # limits = norm.vmin, norm.vmax\n\n    scl = norm(numbers)\n    widths = np.asarray(min_width + scl * (max_width - min_width))\n    if scl.mask.any():\n        widths[scl.mask] = 0\n    sizes = dict(zip(levels, widths))\n\n    return pd.Series(sizes)\n\n\nclass _Dataset_PlotMethods:\n    """"""\n    Enables use of xarray.plot functions as attributes on a Dataset.\n    For example, Dataset.plot.scatter\n    """"""\n\n    def __init__(self, dataset):\n        self._ds = dataset\n\n    def __call__(self, *args, **kwargs):\n        raise ValueError(\n            ""Dataset.plot cannot be called directly. Use ""\n            ""an explicit plot method, e.g. ds.plot.scatter(...)""\n        )\n\n\ndef _dsplot(plotfunc):\n    commondoc = """"""\n    Parameters\n    ----------\n\n    ds : Dataset\n    x, y : string\n        Variable names for x, y axis.\n    hue: str, optional\n        Variable by which to color scattered points\n    hue_style: str, optional\n        Can be either \'discrete\' (legend) or \'continuous\' (color bar).\n    markersize: str, optional (scatter only)\n        Variably by which to vary size of scattered points\n    size_norm: optional\n        Either None or \'Norm\' instance to normalize the \'markersize\' variable.\n    add_guide: bool, optional\n        Add a guide that depends on hue_style\n            - for ""discrete"", build a legend.\n              This is the default for non-numeric `hue` variables.\n            - for ""continuous"",  build a colorbar\n    row : string, optional\n        If passed, make row faceted plots on this dimension name\n    col : string, optional\n        If passed, make column faceted plots on this dimension name\n    col_wrap : integer, optional\n        Use together with ``col`` to wrap faceted plots\n    ax : matplotlib axes, optional\n        If None, uses the current axis. Not applicable when using facets.\n    subplot_kws : dict, optional\n        Dictionary of keyword arguments for matplotlib subplots. Only applies\n        to FacetGrid plotting.\n    aspect : scalar, optional\n        Aspect ratio of plot, so that ``aspect * size`` gives the width in\n        inches. Only used if a ``size`` is provided.\n    size : scalar, optional\n        If provided, create a new figure for the plot with the given size.\n        Height (in inches) of each plot. See also: ``aspect``.\n    norm : ``matplotlib.colors.Normalize`` instance, optional\n        If the ``norm`` has vmin or vmax specified, the corresponding kwarg\n        must be None.\n    vmin, vmax : floats, optional\n        Values to anchor the colormap, otherwise they are inferred from the\n        data and other keyword arguments. When a diverging dataset is inferred,\n        setting one of these values will fix the other by symmetry around\n        ``center``. Setting both values prevents use of a diverging colormap.\n        If discrete levels are provided as an explicit list, both of these\n        values are ignored.\n    cmap : matplotlib colormap name or object, optional\n        The mapping from data values to color space. If not provided, this\n        will be either be ``viridis`` (if the function infers a sequential\n        dataset) or ``RdBu_r`` (if the function infers a diverging dataset).\n        When `Seaborn` is installed, ``cmap`` may also be a `seaborn`\n        color palette. If ``cmap`` is seaborn color palette and the plot type\n        is not ``contour`` or ``contourf``, ``levels`` must also be specified.\n    colors : discrete colors to plot, optional\n        A single color or a list of colors. If the plot type is not ``contour``\n        or ``contourf``, the ``levels`` argument is required.\n    center : float, optional\n        The value at which to center the colormap. Passing this value implies\n        use of a diverging colormap. Setting it to ``False`` prevents use of a\n        diverging colormap.\n    robust : bool, optional\n        If True and ``vmin`` or ``vmax`` are absent, the colormap range is\n        computed with 2nd and 98th percentiles instead of the extreme values.\n    extend : {\'neither\', \'both\', \'min\', \'max\'}, optional\n        How to draw arrows extending the colorbar beyond its limits. If not\n        provided, extend is inferred from vmin, vmax and the data limits.\n    levels : int or list-like object, optional\n        Split the colormap (cmap) into discrete color intervals. If an integer\n        is provided, ""nice"" levels are chosen based on the data range: this can\n        imply that the final number of levels is not exactly the expected one.\n        Setting ``vmin`` and/or ``vmax`` with ``levels=N`` is equivalent to\n        setting ``levels=np.linspace(vmin, vmax, N)``.\n    **kwargs : optional\n        Additional keyword arguments to matplotlib\n    """"""\n\n    # Build on the original docstring\n    plotfunc.__doc__ = f""{plotfunc.__doc__}\\n{commondoc}""\n\n    @functools.wraps(plotfunc)\n    def newplotfunc(\n        ds,\n        x=None,\n        y=None,\n        hue=None,\n        hue_style=None,\n        col=None,\n        row=None,\n        ax=None,\n        figsize=None,\n        size=None,\n        col_wrap=None,\n        sharex=True,\n        sharey=True,\n        aspect=None,\n        subplot_kws=None,\n        add_guide=None,\n        cbar_kwargs=None,\n        cbar_ax=None,\n        vmin=None,\n        vmax=None,\n        norm=None,\n        infer_intervals=None,\n        center=None,\n        levels=None,\n        robust=None,\n        colors=None,\n        extend=None,\n        cmap=None,\n        **kwargs,\n    ):\n\n        _is_facetgrid = kwargs.pop(""_is_facetgrid"", False)\n        if _is_facetgrid:  # facetgrid call\n            meta_data = kwargs.pop(""meta_data"")\n        else:\n            meta_data = _infer_meta_data(ds, x, y, hue, hue_style, add_guide)\n\n        hue_style = meta_data[""hue_style""]\n\n        # handle facetgrids first\n        if col or row:\n            allargs = locals().copy()\n            allargs[""plotfunc""] = globals()[plotfunc.__name__]\n            allargs[""data""] = ds\n            # TODO dcherian: why do I need to remove kwargs?\n            for arg in [""meta_data"", ""kwargs"", ""ds""]:\n                del allargs[arg]\n\n            return _easy_facetgrid(kind=""dataset"", **allargs, **kwargs)\n\n        figsize = kwargs.pop(""figsize"", None)\n        ax = get_axis(figsize, size, aspect, ax)\n\n        if hue_style == ""continuous"" and hue is not None:\n            if _is_facetgrid:\n                cbar_kwargs = meta_data[""cbar_kwargs""]\n                cmap_params = meta_data[""cmap_params""]\n            else:\n                cmap_params, cbar_kwargs = _process_cmap_cbar_kwargs(\n                    plotfunc, ds[hue].values, **locals()\n                )\n\n            # subset that can be passed to scatter, hist2d\n            cmap_params_subset = {\n                vv: cmap_params[vv] for vv in [""vmin"", ""vmax"", ""norm"", ""cmap""]\n            }\n\n        else:\n            cmap_params_subset = {}\n\n        primitive = plotfunc(\n            ds=ds,\n            x=x,\n            y=y,\n            hue=hue,\n            hue_style=hue_style,\n            ax=ax,\n            cmap_params=cmap_params_subset,\n            **kwargs,\n        )\n\n        if _is_facetgrid:  # if this was called from Facetgrid.map_dataset,\n            return primitive  # finish here. Else, make labels\n\n        if meta_data.get(""xlabel"", None):\n            ax.set_xlabel(meta_data.get(""xlabel""))\n        if meta_data.get(""ylabel"", None):\n            ax.set_ylabel(meta_data.get(""ylabel""))\n\n        if meta_data[""add_legend""]:\n            ax.legend(\n                handles=primitive,\n                labels=list(meta_data[""hue""].values),\n                title=meta_data.get(""hue_label"", None),\n            )\n        if meta_data[""add_colorbar""]:\n            cbar_kwargs = {} if cbar_kwargs is None else cbar_kwargs\n            if ""label"" not in cbar_kwargs:\n                cbar_kwargs[""label""] = meta_data.get(""hue_label"", None)\n            _add_colorbar(primitive, ax, cbar_ax, cbar_kwargs, cmap_params)\n\n        return primitive\n\n    @functools.wraps(newplotfunc)\n    def plotmethod(\n        _PlotMethods_obj,\n        x=None,\n        y=None,\n        hue=None,\n        hue_style=None,\n        col=None,\n        row=None,\n        ax=None,\n        figsize=None,\n        col_wrap=None,\n        sharex=True,\n        sharey=True,\n        aspect=None,\n        size=None,\n        subplot_kws=None,\n        add_guide=None,\n        cbar_kwargs=None,\n        cbar_ax=None,\n        vmin=None,\n        vmax=None,\n        norm=None,\n        infer_intervals=None,\n        center=None,\n        levels=None,\n        robust=None,\n        colors=None,\n        extend=None,\n        cmap=None,\n        **kwargs,\n    ):\n        """"""\n        The method should have the same signature as the function.\n\n        This just makes the method work on Plotmethods objects,\n        and passes all the other arguments straight through.\n        """"""\n        allargs = locals()\n        allargs[""ds""] = _PlotMethods_obj._ds\n        allargs.update(kwargs)\n        for arg in [""_PlotMethods_obj"", ""newplotfunc"", ""kwargs""]:\n            del allargs[arg]\n        return newplotfunc(**allargs)\n\n    # Add to class _PlotMethods\n    setattr(_Dataset_PlotMethods, plotmethod.__name__, plotmethod)\n\n    return newplotfunc\n\n\n@_dsplot\ndef scatter(ds, x, y, ax, **kwargs):\n    """"""\n    Scatter Dataset data variables against each other.\n    """"""\n\n    if ""add_colorbar"" in kwargs or ""add_legend"" in kwargs:\n        raise ValueError(\n            ""Dataset.plot.scatter does not accept ""\n            ""\'add_colorbar\' or \'add_legend\'. ""\n            ""Use \'add_guide\' instead.""\n        )\n\n    cmap_params = kwargs.pop(""cmap_params"")\n    hue = kwargs.pop(""hue"")\n    hue_style = kwargs.pop(""hue_style"")\n    markersize = kwargs.pop(""markersize"", None)\n    size_norm = kwargs.pop(""size_norm"", None)\n    size_mapping = kwargs.pop(""size_mapping"", None)  # set by facetgrid\n\n    # need to infer size_mapping with full dataset\n    data = _infer_scatter_data(ds, x, y, hue, markersize, size_norm, size_mapping)\n\n    if hue_style == ""discrete"":\n        primitive = []\n        for label in np.unique(data[""hue""].values):\n            mask = data[""hue""] == label\n            if data[""sizes""] is not None:\n                kwargs.update(s=data[""sizes""].where(mask, drop=True).values.flatten())\n\n            primitive.append(\n                ax.scatter(\n                    data[""x""].where(mask, drop=True).values.flatten(),\n                    data[""y""].where(mask, drop=True).values.flatten(),\n                    label=label,\n                    **kwargs,\n                )\n            )\n\n    elif hue is None or hue_style == ""continuous"":\n        if data[""sizes""] is not None:\n            kwargs.update(s=data[""sizes""].values.ravel())\n        if data[""hue""] is not None:\n            kwargs.update(c=data[""hue""].values.ravel())\n\n        primitive = ax.scatter(\n            data[""x""].values.ravel(), data[""y""].values.ravel(), **cmap_params, **kwargs\n        )\n\n    return primitive\n'"
xarray/plot/facetgrid.py,2,"b'import functools\nimport itertools\nimport warnings\n\nimport numpy as np\n\nfrom ..core.formatting import format_item\nfrom .utils import (\n    _infer_xy_labels,\n    _process_cmap_cbar_kwargs,\n    import_matplotlib_pyplot,\n    label_from_attrs,\n)\n\n# Overrides axes.labelsize, xtick.major.size, ytick.major.size\n# from mpl.rcParams\n_FONTSIZE = ""small""\n# For major ticks on x, y axes\n_NTICKS = 5\n\n\ndef _nicetitle(coord, value, maxchar, template):\n    """"""\n    Put coord, value in template and truncate at maxchar\n    """"""\n    prettyvalue = format_item(value, quote_strings=False)\n    title = template.format(coord=coord, value=prettyvalue)\n\n    if len(title) > maxchar:\n        title = title[: (maxchar - 3)] + ""...""\n\n    return title\n\n\nclass FacetGrid:\n    """"""\n    Initialize the matplotlib figure and FacetGrid object.\n\n    The :class:`FacetGrid` is an object that links a xarray DataArray to\n    a matplotlib figure with a particular structure.\n\n    In particular, :class:`FacetGrid` is used to draw plots with multiple\n    Axes where each Axes shows the same relationship conditioned on\n    different levels of some dimension. It\'s possible to condition on up to\n    two variables by assigning variables to the rows and columns of the\n    grid.\n\n    The general approach to plotting here is called ""small multiples"",\n    where the same kind of plot is repeated multiple times, and the\n    specific use of small multiples to display the same relationship\n    conditioned on one ore more other variables is often called a ""trellis\n    plot"".\n\n    The basic workflow is to initialize the :class:`FacetGrid` object with\n    the DataArray and the variable names that are used to structure the grid.\n    Then plotting functions can be applied to each subset by calling\n    :meth:`FacetGrid.map_dataarray` or :meth:`FacetGrid.map`.\n\n    Attributes\n    ----------\n    axes : numpy object array\n        Contains axes in corresponding position, as returned from\n        plt.subplots\n    col_labels : list\n        list of :class:`matplotlib.text.Text` instances corresponding to column titles.\n    row_labels : list\n        list of :class:`matplotlib.text.Text` instances corresponding to row titles.\n    fig : matplotlib.Figure\n        The figure containing all the axes\n    name_dicts : numpy object array\n        Contains dictionaries mapping coordinate names to values. None is\n        used as a sentinel value for axes which should remain empty, ie.\n        sometimes the bottom right grid\n    """"""\n\n    def __init__(\n        self,\n        data,\n        col=None,\n        row=None,\n        col_wrap=None,\n        sharex=True,\n        sharey=True,\n        figsize=None,\n        aspect=1,\n        size=3,\n        subplot_kws=None,\n    ):\n        """"""\n        Parameters\n        ----------\n        data : DataArray\n            xarray DataArray to be plotted\n        row, col : strings\n            Dimesion names that define subsets of the data, which will be drawn\n            on separate facets in the grid.\n        col_wrap : int, optional\n            ""Wrap"" the column variable at this width, so that the column facets\n        sharex : bool, optional\n            If true, the facets will share x axes\n        sharey : bool, optional\n            If true, the facets will share y axes\n        figsize : tuple, optional\n            A tuple (width, height) of the figure in inches.\n            If set, overrides ``size`` and ``aspect``.\n        aspect : scalar, optional\n            Aspect ratio of each facet, so that ``aspect * size`` gives the\n            width of each facet in inches\n        size : scalar, optional\n            Height (in inches) of each facet. See also: ``aspect``\n        subplot_kws : dict, optional\n            Dictionary of keyword arguments for matplotlib subplots\n\n        """"""\n\n        plt = import_matplotlib_pyplot()\n\n        # Handle corner case of nonunique coordinates\n        rep_col = col is not None and not data[col].to_index().is_unique\n        rep_row = row is not None and not data[row].to_index().is_unique\n        if rep_col or rep_row:\n            raise ValueError(\n                ""Coordinates used for faceting cannot ""\n                ""contain repeated (nonunique) values.""\n            )\n\n        # single_group is the grouping variable, if there is exactly one\n        if col and row:\n            single_group = False\n            nrow = len(data[row])\n            ncol = len(data[col])\n            nfacet = nrow * ncol\n            if col_wrap is not None:\n                warnings.warn(""Ignoring col_wrap since both col and row "" ""were passed"")\n        elif row and not col:\n            single_group = row\n        elif not row and col:\n            single_group = col\n        else:\n            raise ValueError(""Pass a coordinate name as an argument for row or col"")\n\n        # Compute grid shape\n        if single_group:\n            nfacet = len(data[single_group])\n            if col:\n                # idea - could add heuristic for nice shapes like 3x4\n                ncol = nfacet\n            if row:\n                ncol = 1\n            if col_wrap is not None:\n                # Overrides previous settings\n                ncol = col_wrap\n            nrow = int(np.ceil(nfacet / ncol))\n\n        # Set the subplot kwargs\n        subplot_kws = {} if subplot_kws is None else subplot_kws\n\n        if figsize is None:\n            # Calculate the base figure size with extra horizontal space for a\n            # colorbar\n            cbar_space = 1\n            figsize = (ncol * size * aspect + cbar_space, nrow * size)\n\n        fig, axes = plt.subplots(\n            nrow,\n            ncol,\n            sharex=sharex,\n            sharey=sharey,\n            squeeze=False,\n            figsize=figsize,\n            subplot_kw=subplot_kws,\n        )\n\n        # Set up the lists of names for the row and column facet variables\n        col_names = list(data[col].values) if col else []\n        row_names = list(data[row].values) if row else []\n\n        if single_group:\n            full = [{single_group: x} for x in data[single_group].values]\n            empty = [None for x in range(nrow * ncol - len(full))]\n            name_dicts = full + empty\n        else:\n            rowcols = itertools.product(row_names, col_names)\n            name_dicts = [{row: r, col: c} for r, c in rowcols]\n\n        name_dicts = np.array(name_dicts).reshape(nrow, ncol)\n\n        # Set up the class attributes\n        # ---------------------------\n\n        # First the public API\n        self.data = data\n        self.name_dicts = name_dicts\n        self.fig = fig\n        self.axes = axes\n        self.row_names = row_names\n        self.col_names = col_names\n        self.figlegend = None\n\n        # Next the private variables\n        self._single_group = single_group\n        self._nrow = nrow\n        self._row_var = row\n        self._ncol = ncol\n        self._col_var = col\n        self._col_wrap = col_wrap\n        self.row_labels = [None] * nrow\n        self.col_labels = [None] * ncol\n        self._x_var = None\n        self._y_var = None\n        self._cmap_extend = None\n        self._mappables = []\n        self._finalized = False\n\n    @property\n    def _left_axes(self):\n        return self.axes[:, 0]\n\n    @property\n    def _bottom_axes(self):\n        return self.axes[-1, :]\n\n    def map_dataarray(self, func, x, y, **kwargs):\n        """"""\n        Apply a plotting function to a 2d facet\'s subset of the data.\n\n        This is more convenient and less general than ``FacetGrid.map``\n\n        Parameters\n        ----------\n        func : callable\n            A plotting function with the same signature as a 2d xarray\n            plotting method such as `xarray.plot.imshow`\n        x, y : string\n            Names of the coordinates to plot on x, y axes\n        kwargs :\n            additional keyword arguments to func\n\n        Returns\n        -------\n        self : FacetGrid object\n\n        """"""\n\n        if kwargs.get(""cbar_ax"", None) is not None:\n            raise ValueError(""cbar_ax not supported by FacetGrid."")\n\n        cmap_params, cbar_kwargs = _process_cmap_cbar_kwargs(\n            func, self.data.values, **kwargs\n        )\n\n        self._cmap_extend = cmap_params.get(""extend"")\n\n        # Order is important\n        func_kwargs = {\n            k: v\n            for k, v in kwargs.items()\n            if k not in {""cmap"", ""colors"", ""cbar_kwargs"", ""levels""}\n        }\n        func_kwargs.update(cmap_params)\n        func_kwargs.update({""add_colorbar"": False, ""add_labels"": False})\n\n        # Get x, y labels for the first subplot\n        x, y = _infer_xy_labels(\n            darray=self.data.loc[self.name_dicts.flat[0]],\n            x=x,\n            y=y,\n            imshow=func.__name__ == ""imshow"",\n            rgb=kwargs.get(""rgb"", None),\n        )\n\n        for d, ax in zip(self.name_dicts.flat, self.axes.flat):\n            # None is the sentinel value\n            if d is not None:\n                subset = self.data.loc[d]\n                mappable = func(\n                    subset, x=x, y=y, ax=ax, **func_kwargs, _is_facetgrid=True\n                )\n                self._mappables.append(mappable)\n\n        self._finalize_grid(x, y)\n\n        if kwargs.get(""add_colorbar"", True):\n            self.add_colorbar(**cbar_kwargs)\n\n        return self\n\n    def map_dataarray_line(\n        self, func, x, y, hue, add_legend=True, _labels=None, **kwargs\n    ):\n        from .plot import _infer_line_data\n\n        for d, ax in zip(self.name_dicts.flat, self.axes.flat):\n            # None is the sentinel value\n            if d is not None:\n                subset = self.data.loc[d]\n                mappable = func(\n                    subset,\n                    x=x,\n                    y=y,\n                    ax=ax,\n                    hue=hue,\n                    add_legend=False,\n                    _labels=False,\n                    **kwargs,\n                )\n                self._mappables.append(mappable)\n\n        _, _, hueplt, xlabel, ylabel, huelabel = _infer_line_data(\n            darray=self.data.loc[self.name_dicts.flat[0]], x=x, y=y, hue=hue\n        )\n\n        self._hue_var = hueplt\n        self._hue_label = huelabel\n        self._finalize_grid(xlabel, ylabel)\n\n        if add_legend and hueplt is not None and huelabel is not None:\n            self.add_legend()\n\n        return self\n\n    def map_dataset(\n        self, func, x=None, y=None, hue=None, hue_style=None, add_guide=None, **kwargs\n    ):\n        from .dataset_plot import _infer_meta_data, _parse_size\n\n        kwargs[""add_guide""] = False\n        kwargs[""_is_facetgrid""] = True\n\n        if kwargs.get(""markersize"", None):\n            kwargs[""size_mapping""] = _parse_size(\n                self.data[kwargs[""markersize""]], kwargs.pop(""size_norm"", None)\n            )\n\n        meta_data = _infer_meta_data(self.data, x, y, hue, hue_style, add_guide)\n        kwargs[""meta_data""] = meta_data\n\n        if hue and meta_data[""hue_style""] == ""continuous"":\n            cmap_params, cbar_kwargs = _process_cmap_cbar_kwargs(\n                func, self.data[hue].values, **kwargs\n            )\n            kwargs[""meta_data""][""cmap_params""] = cmap_params\n            kwargs[""meta_data""][""cbar_kwargs""] = cbar_kwargs\n\n        for d, ax in zip(self.name_dicts.flat, self.axes.flat):\n            # None is the sentinel value\n            if d is not None:\n                subset = self.data.loc[d]\n                maybe_mappable = func(\n                    ds=subset, x=x, y=y, hue=hue, hue_style=hue_style, ax=ax, **kwargs\n                )\n                # TODO: this is needed to get legends to work.\n                # but maybe_mappable is a list in that case :/\n                self._mappables.append(maybe_mappable)\n\n        self._finalize_grid(meta_data[""xlabel""], meta_data[""ylabel""])\n\n        if hue:\n            self._hue_label = meta_data.pop(""hue_label"", None)\n            if meta_data[""add_legend""]:\n                self._hue_var = meta_data[""hue""]\n                self.add_legend()\n            elif meta_data[""add_colorbar""]:\n                self.add_colorbar(label=self._hue_label, **cbar_kwargs)\n\n        return self\n\n    def _finalize_grid(self, *axlabels):\n        """"""Finalize the annotations and layout.""""""\n        if not self._finalized:\n            self.set_axis_labels(*axlabels)\n            self.set_titles()\n            self.fig.tight_layout()\n\n            for ax, namedict in zip(self.axes.flat, self.name_dicts.flat):\n                if namedict is None:\n                    ax.set_visible(False)\n\n            self._finalized = True\n\n    def add_legend(self, **kwargs):\n        figlegend = self.fig.legend(\n            handles=self._mappables[-1],\n            labels=list(self._hue_var.values),\n            title=self._hue_label,\n            loc=""center right"",\n            **kwargs,\n        )\n\n        self.figlegend = figlegend\n        # Draw the plot to set the bounding boxes correctly\n        self.fig.draw(self.fig.canvas.get_renderer())\n\n        # Calculate and set the new width of the figure so the legend fits\n        legend_width = figlegend.get_window_extent().width / self.fig.dpi\n        figure_width = self.fig.get_figwidth()\n        self.fig.set_figwidth(figure_width + legend_width)\n\n        # Draw the plot again to get the new transformations\n        self.fig.draw(self.fig.canvas.get_renderer())\n\n        # Now calculate how much space we need on the right side\n        legend_width = figlegend.get_window_extent().width / self.fig.dpi\n        space_needed = legend_width / (figure_width + legend_width) + 0.02\n        # margin = .01\n        # _space_needed = margin + space_needed\n        right = 1 - space_needed\n\n        # Place the subplot axes to give space for the legend\n        self.fig.subplots_adjust(right=right)\n\n    def add_colorbar(self, **kwargs):\n        """"""Draw a colorbar\n        """"""\n        kwargs = kwargs.copy()\n        if self._cmap_extend is not None:\n            kwargs.setdefault(""extend"", self._cmap_extend)\n        if ""label"" not in kwargs:\n            kwargs.setdefault(""label"", label_from_attrs(self.data))\n        self.cbar = self.fig.colorbar(\n            self._mappables[-1], ax=list(self.axes.flat), **kwargs\n        )\n        return self\n\n    def set_axis_labels(self, x_var=None, y_var=None):\n        """"""Set axis labels on the left column and bottom row of the grid.""""""\n        if x_var is not None:\n            if x_var in self.data.coords:\n                self._x_var = x_var\n                self.set_xlabels(label_from_attrs(self.data[x_var]))\n            else:\n                # x_var is a string\n                self.set_xlabels(x_var)\n\n        if y_var is not None:\n            if y_var in self.data.coords:\n                self._y_var = y_var\n                self.set_ylabels(label_from_attrs(self.data[y_var]))\n            else:\n                self.set_ylabels(y_var)\n        return self\n\n    def set_xlabels(self, label=None, **kwargs):\n        """"""Label the x axis on the bottom row of the grid.""""""\n        if label is None:\n            label = label_from_attrs(self.data[self._x_var])\n        for ax in self._bottom_axes:\n            ax.set_xlabel(label, **kwargs)\n        return self\n\n    def set_ylabels(self, label=None, **kwargs):\n        """"""Label the y axis on the left column of the grid.""""""\n        if label is None:\n            label = label_from_attrs(self.data[self._y_var])\n        for ax in self._left_axes:\n            ax.set_ylabel(label, **kwargs)\n        return self\n\n    def set_titles(self, template=""{coord} = {value}"", maxchar=30, size=None, **kwargs):\n        """"""\n        Draw titles either above each facet or on the grid margins.\n\n        Parameters\n        ----------\n        template : string\n            Template for plot titles containing {coord} and {value}\n        maxchar : int\n            Truncate titles at maxchar\n        kwargs : keyword args\n            additional arguments to matplotlib.text\n\n        Returns\n        -------\n        self: FacetGrid object\n\n        """"""\n        import matplotlib as mpl\n\n        if size is None:\n            size = mpl.rcParams[""axes.labelsize""]\n\n        nicetitle = functools.partial(_nicetitle, maxchar=maxchar, template=template)\n\n        if self._single_group:\n            for d, ax in zip(self.name_dicts.flat, self.axes.flat):\n                # Only label the ones with data\n                if d is not None:\n                    coord, value = list(d.items()).pop()\n                    title = nicetitle(coord, value, maxchar=maxchar)\n                    ax.set_title(title, size=size, **kwargs)\n        else:\n            # The row titles on the right edge of the grid\n            for index, (ax, row_name, handle) in enumerate(\n                zip(self.axes[:, -1], self.row_names, self.row_labels)\n            ):\n                title = nicetitle(coord=self._row_var, value=row_name, maxchar=maxchar)\n                if not handle:\n                    self.row_labels[index] = ax.annotate(\n                        title,\n                        xy=(1.02, 0.5),\n                        xycoords=""axes fraction"",\n                        rotation=270,\n                        ha=""left"",\n                        va=""center"",\n                        **kwargs,\n                    )\n                else:\n                    handle.set_text(title)\n\n            # The column titles on the top row\n            for index, (ax, col_name, handle) in enumerate(\n                zip(self.axes[0, :], self.col_names, self.col_labels)\n            ):\n                title = nicetitle(coord=self._col_var, value=col_name, maxchar=maxchar)\n                if not handle:\n                    self.col_labels[index] = ax.set_title(title, size=size, **kwargs)\n                else:\n                    handle.set_text(title)\n\n        return self\n\n    def set_ticks(self, max_xticks=_NTICKS, max_yticks=_NTICKS, fontsize=_FONTSIZE):\n        """"""\n        Set and control tick behavior\n\n        Parameters\n        ----------\n        max_xticks, max_yticks : int, optional\n            Maximum number of labeled ticks to plot on x, y axes\n        fontsize : string or int\n            Font size as used by matplotlib text\n\n        Returns\n        -------\n        self : FacetGrid object\n\n        """"""\n        from matplotlib.ticker import MaxNLocator\n\n        # Both are necessary\n        x_major_locator = MaxNLocator(nbins=max_xticks)\n        y_major_locator = MaxNLocator(nbins=max_yticks)\n\n        for ax in self.axes.flat:\n            ax.xaxis.set_major_locator(x_major_locator)\n            ax.yaxis.set_major_locator(y_major_locator)\n            for tick in itertools.chain(\n                ax.xaxis.get_major_ticks(), ax.yaxis.get_major_ticks()\n            ):\n                tick.label1.set_fontsize(fontsize)\n\n        return self\n\n    def map(self, func, *args, **kwargs):\n        """"""\n        Apply a plotting function to each facet\'s subset of the data.\n\n        Parameters\n        ----------\n        func : callable\n            A plotting function that takes data and keyword arguments. It\n            must plot to the currently active matplotlib Axes and take a\n            `color` keyword argument. If faceting on the `hue` dimension,\n            it must also take a `label` keyword argument.\n        args : strings\n            Column names in self.data that identify variables with data to\n            plot. The data for each variable is passed to `func` in the\n            order the variables are specified in the call.\n        kwargs : keyword arguments\n            All keyword arguments are passed to the plotting function.\n\n        Returns\n        -------\n        self : FacetGrid object\n\n        """"""\n        plt = import_matplotlib_pyplot()\n\n        for ax, namedict in zip(self.axes.flat, self.name_dicts.flat):\n            if namedict is not None:\n                data = self.data.loc[namedict]\n                plt.sca(ax)\n                innerargs = [data[a].values for a in args]\n                maybe_mappable = func(*innerargs, **kwargs)\n                # TODO: better way to verify that an artist is mappable?\n                # https://stackoverflow.com/questions/33023036/is-it-possible-to-detect-if-a-matplotlib-artist-is-a-mappable-suitable-for-use-w#33023522\n                if maybe_mappable and hasattr(maybe_mappable, ""autoscale_None""):\n                    self._mappables.append(maybe_mappable)\n\n        self._finalize_grid(*args[:2])\n\n        return self\n\n\ndef _easy_facetgrid(\n    data,\n    plotfunc,\n    kind,\n    x=None,\n    y=None,\n    row=None,\n    col=None,\n    col_wrap=None,\n    sharex=True,\n    sharey=True,\n    aspect=None,\n    size=None,\n    subplot_kws=None,\n    ax=None,\n    figsize=None,\n    **kwargs,\n):\n    """"""\n    Convenience method to call xarray.plot.FacetGrid from 2d plotting methods\n\n    kwargs are the arguments to 2d plotting method\n    """"""\n    if ax is not None:\n        raise ValueError(""Can\'t use axes when making faceted plots."")\n    if aspect is None:\n        aspect = 1\n    if size is None:\n        size = 3\n    elif figsize is not None:\n        raise ValueError(""cannot provide both `figsize` and `size` arguments"")\n\n    g = FacetGrid(\n        data=data,\n        col=col,\n        row=row,\n        col_wrap=col_wrap,\n        sharex=sharex,\n        sharey=sharey,\n        figsize=figsize,\n        aspect=aspect,\n        size=size,\n        subplot_kws=subplot_kws,\n    )\n\n    if kind == ""line"":\n        return g.map_dataarray_line(plotfunc, x, y, **kwargs)\n\n    if kind == ""dataarray"":\n        return g.map_dataarray(plotfunc, x, y, **kwargs)\n\n    if kind == ""dataset"":\n        return g.map_dataset(plotfunc, x, y, **kwargs)\n'"
xarray/plot/plot.py,16,"b'""""""\nUse this module directly:\n    import xarray.plot as xplt\n\nOr use the methods on a DataArray or Dataset:\n    DataArray.plot._____\n    Dataset.plot._____\n""""""\nimport functools\n\nimport numpy as np\nimport pandas as pd\n\nfrom .facetgrid import _easy_facetgrid\nfrom .utils import (\n    _add_colorbar,\n    _assert_valid_xy,\n    _ensure_plottable,\n    _infer_interval_breaks,\n    _infer_xy_labels,\n    _process_cmap_cbar_kwargs,\n    _rescale_imshow_rgb,\n    _resolve_intervals_1dplot,\n    _resolve_intervals_2dplot,\n    _update_axes,\n    get_axis,\n    import_matplotlib_pyplot,\n    label_from_attrs,\n)\n\n\ndef _infer_line_data(darray, x, y, hue):\n\n    ndims = len(darray.dims)\n\n    if x is not None and y is not None:\n        raise ValueError(""Cannot specify both x and y kwargs for line plots."")\n\n    if x is not None:\n        _assert_valid_xy(darray, x, ""x"")\n\n    if y is not None:\n        _assert_valid_xy(darray, y, ""y"")\n\n    if ndims == 1:\n        huename = None\n        hueplt = None\n        huelabel = """"\n\n        if x is not None:\n            xplt = darray[x]\n            yplt = darray\n\n        elif y is not None:\n            xplt = darray\n            yplt = darray[y]\n\n        else:  # Both x & y are None\n            dim = darray.dims[0]\n            xplt = darray[dim]\n            yplt = darray\n\n    else:\n        if x is None and y is None and hue is None:\n            raise ValueError(""For 2D inputs, please"" ""specify either hue, x or y."")\n\n        if y is None:\n            xname, huename = _infer_xy_labels(darray=darray, x=x, y=hue)\n            xplt = darray[xname]\n            if xplt.ndim > 1:\n                if huename in darray.dims:\n                    otherindex = 1 if darray.dims.index(huename) == 0 else 0\n                    otherdim = darray.dims[otherindex]\n                    yplt = darray.transpose(otherdim, huename, transpose_coords=False)\n                    xplt = xplt.transpose(otherdim, huename, transpose_coords=False)\n                else:\n                    raise ValueError(\n                        ""For 2D inputs, hue must be a dimension""\n                        "" i.e. one of "" + repr(darray.dims)\n                    )\n\n            else:\n                (xdim,) = darray[xname].dims\n                (huedim,) = darray[huename].dims\n                yplt = darray.transpose(xdim, huedim)\n\n        else:\n            yname, huename = _infer_xy_labels(darray=darray, x=y, y=hue)\n            yplt = darray[yname]\n            if yplt.ndim > 1:\n                if huename in darray.dims:\n                    otherindex = 1 if darray.dims.index(huename) == 0 else 0\n                    otherdim = darray.dims[otherindex]\n                    xplt = darray.transpose(otherdim, huename, transpose_coords=False)\n                    yplt = yplt.transpose(otherdim, huename, transpose_coords=False)\n                else:\n                    raise ValueError(\n                        ""For 2D inputs, hue must be a dimension""\n                        "" i.e. one of "" + repr(darray.dims)\n                    )\n\n            else:\n                (ydim,) = darray[yname].dims\n                (huedim,) = darray[huename].dims\n                xplt = darray.transpose(ydim, huedim)\n\n        huelabel = label_from_attrs(darray[huename])\n        hueplt = darray[huename]\n\n    xlabel = label_from_attrs(xplt)\n    ylabel = label_from_attrs(yplt)\n\n    return xplt, yplt, hueplt, xlabel, ylabel, huelabel\n\n\ndef plot(\n    darray,\n    row=None,\n    col=None,\n    col_wrap=None,\n    ax=None,\n    hue=None,\n    rtol=0.01,\n    subplot_kws=None,\n    **kwargs,\n):\n    """"""\n    Default plot of DataArray using matplotlib.pyplot.\n\n    Calls xarray plotting function based on the dimensions of\n    darray.squeeze()\n\n    =============== ===========================\n    Dimensions      Plotting function\n    --------------- ---------------------------\n    1               :py:func:`xarray.plot.line`\n    2               :py:func:`xarray.plot.pcolormesh`\n    Anything else   :py:func:`xarray.plot.hist`\n    =============== ===========================\n\n    Parameters\n    ----------\n    darray : DataArray\n    row : string, optional\n        If passed, make row faceted plots on this dimension name\n    col : string, optional\n        If passed, make column faceted plots on this dimension name\n    hue : string, optional\n        If passed, make faceted line plots with hue on this dimension name\n    col_wrap : integer, optional\n        Use together with ``col`` to wrap faceted plots\n    ax : matplotlib axes, optional\n        If None, uses the current axis. Not applicable when using facets.\n    rtol : number, optional\n        Relative tolerance used to determine if the indexes\n        are uniformly spaced. Usually a small positive number.\n    subplot_kws : dict, optional\n        Dictionary of keyword arguments for matplotlib subplots. Only applies\n        to FacetGrid plotting.\n    **kwargs : optional\n        Additional keyword arguments to matplotlib\n\n    """"""\n    darray = darray.squeeze().compute()\n\n    plot_dims = set(darray.dims)\n    plot_dims.discard(row)\n    plot_dims.discard(col)\n    plot_dims.discard(hue)\n\n    ndims = len(plot_dims)\n\n    error_msg = (\n        ""Only 1d and 2d plots are supported for facets in xarray. ""\n        ""See the package `Seaborn` for more options.""\n    )\n\n    if ndims in [1, 2]:\n        if row or col:\n            kwargs[""row""] = row\n            kwargs[""col""] = col\n            kwargs[""col_wrap""] = col_wrap\n            kwargs[""subplot_kws""] = subplot_kws\n        if ndims == 1:\n            plotfunc = line\n            kwargs[""hue""] = hue\n        elif ndims == 2:\n            if hue:\n                plotfunc = line\n                kwargs[""hue""] = hue\n            else:\n                plotfunc = pcolormesh\n    else:\n        if row or col or hue:\n            raise ValueError(error_msg)\n        plotfunc = hist\n\n    kwargs[""ax""] = ax\n\n    return plotfunc(darray, **kwargs)\n\n\n# This function signature should not change so that it can use\n# matplotlib format strings\ndef line(\n    darray,\n    *args,\n    row=None,\n    col=None,\n    figsize=None,\n    aspect=None,\n    size=None,\n    ax=None,\n    hue=None,\n    x=None,\n    y=None,\n    xincrease=None,\n    yincrease=None,\n    xscale=None,\n    yscale=None,\n    xticks=None,\n    yticks=None,\n    xlim=None,\n    ylim=None,\n    add_legend=True,\n    _labels=True,\n    **kwargs,\n):\n    """"""\n    Line plot of DataArray index against values\n\n    Wraps :func:`matplotlib:matplotlib.pyplot.plot`\n\n    Parameters\n    ----------\n    darray : DataArray\n        Must be 1 dimensional\n    figsize : tuple, optional\n        A tuple (width, height) of the figure in inches.\n        Mutually exclusive with ``size`` and ``ax``.\n    aspect : scalar, optional\n        Aspect ratio of plot, so that ``aspect * size`` gives the width in\n        inches. Only used if a ``size`` is provided.\n    size : scalar, optional\n        If provided, create a new figure for the plot with the given size.\n        Height (in inches) of each plot. See also: ``aspect``.\n    ax : matplotlib axes object, optional\n        Axis on which to plot this figure. By default, use the current axis.\n        Mutually exclusive with ``size`` and ``figsize``.\n    hue : string, optional\n        Dimension or coordinate for which you want multiple lines plotted.\n        If plotting against a 2D coordinate, ``hue`` must be a dimension.\n    x, y : string, optional\n        Dimension, coordinate or MultiIndex level for x, y axis.\n        Only one of these may be specified.\n        The other coordinate plots values from the DataArray on which this\n        plot method is called.\n    xscale, yscale : \'linear\', \'symlog\', \'log\', \'logit\', optional\n        Specifies scaling for the x- and y-axes respectively\n    xticks, yticks : Specify tick locations for x- and y-axes\n    xlim, ylim : Specify x- and y-axes limits\n    xincrease : None, True, or False, optional\n        Should the values on the x axes be increasing from left to right?\n        if None, use the default for the matplotlib function.\n    yincrease : None, True, or False, optional\n        Should the values on the y axes be increasing from top to bottom?\n        if None, use the default for the matplotlib function.\n    add_legend : boolean, optional\n        Add legend with y axis coordinates (2D inputs only).\n    ``*args``, ``**kwargs`` : optional\n        Additional arguments to matplotlib.pyplot.plot\n    """"""\n    # Handle facetgrids first\n    if row or col:\n        allargs = locals().copy()\n        allargs.update(allargs.pop(""kwargs""))\n        allargs.pop(""darray"")\n        return _easy_facetgrid(darray, line, kind=""line"", **allargs)\n\n    ndims = len(darray.dims)\n    if ndims > 2:\n        raise ValueError(\n            ""Line plots are for 1- or 2-dimensional DataArrays. ""\n            ""Passed DataArray has {ndims} ""\n            ""dimensions"".format(ndims=ndims)\n        )\n\n    # The allargs dict passed to _easy_facetgrid above contains args\n    if args == ():\n        args = kwargs.pop(""args"", ())\n    else:\n        assert ""args"" not in kwargs\n\n    ax = get_axis(figsize, size, aspect, ax)\n    xplt, yplt, hueplt, xlabel, ylabel, hue_label = _infer_line_data(darray, x, y, hue)\n\n    # Remove pd.Intervals if contained in xplt.values and/or yplt.values.\n    xplt_val, yplt_val, xlabel, ylabel, kwargs = _resolve_intervals_1dplot(\n        xplt.values, yplt.values, xlabel, ylabel, kwargs\n    )\n\n    _ensure_plottable(xplt_val, yplt_val)\n\n    primitive = ax.plot(xplt_val, yplt_val, *args, **kwargs)\n\n    if _labels:\n        if xlabel is not None:\n            ax.set_xlabel(xlabel)\n\n        if ylabel is not None:\n            ax.set_ylabel(ylabel)\n\n        ax.set_title(darray._title_for_slice())\n\n    if darray.ndim == 2 and add_legend:\n        ax.legend(handles=primitive, labels=list(hueplt.values), title=hue_label)\n\n    # Rotate dates on xlabels\n    # Do this without calling autofmt_xdate so that x-axes ticks\n    # on other subplots (if any) are not deleted.\n    # https://stackoverflow.com/questions/17430105/autofmt-xdate-deletes-x-axis-labels-of-all-subplots\n    if np.issubdtype(xplt.dtype, np.datetime64):\n        for xlabels in ax.get_xticklabels():\n            xlabels.set_rotation(30)\n            xlabels.set_ha(""right"")\n\n    _update_axes(ax, xincrease, yincrease, xscale, yscale, xticks, yticks, xlim, ylim)\n\n    return primitive\n\n\ndef step(darray, *args, where=""pre"", drawstyle=None, ds=None, **kwargs):\n    """"""\n    Step plot of DataArray index against values\n\n    Similar to :func:`matplotlib:matplotlib.pyplot.step`\n\n    Parameters\n    ----------\n    where : {\'pre\', \'post\', \'mid\'}, optional, default \'pre\'\n        Define where the steps should be placed:\n\n        - \'pre\': The y value is continued constantly to the left from\n          every *x* position, i.e. the interval ``(x[i-1], x[i]]`` has the\n          value ``y[i]``.\n        - \'post\': The y value is continued constantly to the right from\n          every *x* position, i.e. the interval ``[x[i], x[i+1])`` has the\n          value ``y[i]``.\n        - \'mid\': Steps occur half-way between the *x* positions.\n\n        Note that this parameter is ignored if one coordinate consists of\n        :py:func:`pandas.Interval` values, e.g. as a result of\n        :py:func:`xarray.Dataset.groupby_bins`. In this case, the actual\n        boundaries of the interval are used.\n\n    ``*args``, ``**kwargs`` : optional\n        Additional arguments following :py:func:`xarray.plot.line`\n    """"""\n    if where not in {""pre"", ""post"", ""mid""}:\n        raise ValueError(""\'where\' argument to step must be "" ""\'pre\', \'post\' or \'mid\'"")\n\n    if ds is not None:\n        if drawstyle is None:\n            drawstyle = ds\n        else:\n            raise TypeError(""ds and drawstyle are mutually exclusive"")\n    if drawstyle is None:\n        drawstyle = """"\n    drawstyle = ""steps-"" + where + drawstyle\n\n    return line(darray, *args, drawstyle=drawstyle, **kwargs)\n\n\ndef hist(\n    darray,\n    figsize=None,\n    size=None,\n    aspect=None,\n    ax=None,\n    xincrease=None,\n    yincrease=None,\n    xscale=None,\n    yscale=None,\n    xticks=None,\n    yticks=None,\n    xlim=None,\n    ylim=None,\n    **kwargs,\n):\n    """"""\n    Histogram of DataArray\n\n    Wraps :func:`matplotlib:matplotlib.pyplot.hist`\n\n    Plots N dimensional arrays by first flattening the array.\n\n    Parameters\n    ----------\n    darray : DataArray\n        Can be any dimension\n    figsize : tuple, optional\n        A tuple (width, height) of the figure in inches.\n        Mutually exclusive with ``size`` and ``ax``.\n    aspect : scalar, optional\n        Aspect ratio of plot, so that ``aspect * size`` gives the width in\n        inches. Only used if a ``size`` is provided.\n    size : scalar, optional\n        If provided, create a new figure for the plot with the given size.\n        Height (in inches) of each plot. See also: ``aspect``.\n    ax : matplotlib axes object, optional\n        Axis on which to plot this figure. By default, use the current axis.\n        Mutually exclusive with ``size`` and ``figsize``.\n    **kwargs : optional\n        Additional keyword arguments to matplotlib.pyplot.hist\n\n    """"""\n    ax = get_axis(figsize, size, aspect, ax)\n\n    no_nan = np.ravel(darray.values)\n    no_nan = no_nan[pd.notnull(no_nan)]\n\n    primitive = ax.hist(no_nan, **kwargs)\n\n    ax.set_title(""Histogram"")\n    ax.set_xlabel(label_from_attrs(darray))\n\n    _update_axes(ax, xincrease, yincrease, xscale, yscale, xticks, yticks, xlim, ylim)\n\n    return primitive\n\n\n# MUST run before any 2d plotting functions are defined since\n# _plot2d decorator adds them as methods here.\nclass _PlotMethods:\n    """"""\n    Enables use of xarray.plot functions as attributes on a DataArray.\n    For example, DataArray.plot.imshow\n    """"""\n\n    __slots__ = (""_da"",)\n\n    def __init__(self, darray):\n        self._da = darray\n\n    def __call__(self, **kwargs):\n        return plot(self._da, **kwargs)\n\n    @functools.wraps(hist)\n    def hist(self, ax=None, **kwargs):\n        return hist(self._da, ax=ax, **kwargs)\n\n    @functools.wraps(line)\n    def line(self, *args, **kwargs):\n        return line(self._da, *args, **kwargs)\n\n    @functools.wraps(step)\n    def step(self, *args, **kwargs):\n        return step(self._da, *args, **kwargs)\n\n\ndef _plot2d(plotfunc):\n    """"""\n    Decorator for common 2d plotting logic\n\n    Also adds the 2d plot method to class _PlotMethods\n    """"""\n    commondoc = """"""\n    Parameters\n    ----------\n    darray : DataArray\n        Must be 2 dimensional, unless creating faceted plots\n    x : string, optional\n        Coordinate for x axis. If None use darray.dims[1]\n    y : string, optional\n        Coordinate for y axis. If None use darray.dims[0]\n    figsize : tuple, optional\n        A tuple (width, height) of the figure in inches.\n        Mutually exclusive with ``size`` and ``ax``.\n    aspect : scalar, optional\n        Aspect ratio of plot, so that ``aspect * size`` gives the width in\n        inches. Only used if a ``size`` is provided.\n    size : scalar, optional\n        If provided, create a new figure for the plot with the given size.\n        Height (in inches) of each plot. See also: ``aspect``.\n    ax : matplotlib axes object, optional\n        Axis on which to plot this figure. By default, use the current axis.\n        Mutually exclusive with ``size`` and ``figsize``.\n    row : string, optional\n        If passed, make row faceted plots on this dimension name\n    col : string, optional\n        If passed, make column faceted plots on this dimension name\n    col_wrap : integer, optional\n        Use together with ``col`` to wrap faceted plots\n    xscale, yscale : \'linear\', \'symlog\', \'log\', \'logit\', optional\n        Specifies scaling for the x- and y-axes respectively\n    xticks, yticks : Specify tick locations for x- and y-axes\n    xlim, ylim : Specify x- and y-axes limits\n    xincrease : None, True, or False, optional\n        Should the values on the x axes be increasing from left to right?\n        if None, use the default for the matplotlib function.\n    yincrease : None, True, or False, optional\n        Should the values on the y axes be increasing from top to bottom?\n        if None, use the default for the matplotlib function.\n    add_colorbar : Boolean, optional\n        Adds colorbar to axis\n    add_labels : Boolean, optional\n        Use xarray metadata to label axes\n    norm : ``matplotlib.colors.Normalize`` instance, optional\n        If the ``norm`` has vmin or vmax specified, the corresponding kwarg\n        must be None.\n    vmin, vmax : floats, optional\n        Values to anchor the colormap, otherwise they are inferred from the\n        data and other keyword arguments. When a diverging dataset is inferred,\n        setting one of these values will fix the other by symmetry around\n        ``center``. Setting both values prevents use of a diverging colormap.\n        If discrete levels are provided as an explicit list, both of these\n        values are ignored.\n    cmap : matplotlib colormap name or object, optional\n        The mapping from data values to color space. If not provided, this\n        will be either be ``viridis`` (if the function infers a sequential\n        dataset) or ``RdBu_r`` (if the function infers a diverging dataset).\n        When `Seaborn` is installed, ``cmap`` may also be a `seaborn`\n        color palette. If ``cmap`` is seaborn color palette and the plot type\n        is not ``contour`` or ``contourf``, ``levels`` must also be specified.\n    colors : discrete colors to plot, optional\n        A single color or a list of colors. If the plot type is not ``contour``\n        or ``contourf``, the ``levels`` argument is required.\n    center : float, optional\n        The value at which to center the colormap. Passing this value implies\n        use of a diverging colormap. Setting it to ``False`` prevents use of a\n        diverging colormap.\n    robust : bool, optional\n        If True and ``vmin`` or ``vmax`` are absent, the colormap range is\n        computed with 2nd and 98th percentiles instead of the extreme values.\n    extend : {\'neither\', \'both\', \'min\', \'max\'}, optional\n        How to draw arrows extending the colorbar beyond its limits. If not\n        provided, extend is inferred from vmin, vmax and the data limits.\n    levels : int or list-like object, optional\n        Split the colormap (cmap) into discrete color intervals. If an integer\n        is provided, ""nice"" levels are chosen based on the data range: this can\n        imply that the final number of levels is not exactly the expected one.\n        Setting ``vmin`` and/or ``vmax`` with ``levels=N`` is equivalent to\n        setting ``levels=np.linspace(vmin, vmax, N)``.\n    infer_intervals : bool, optional\n        Only applies to pcolormesh. If True, the coordinate intervals are\n        passed to pcolormesh. If False, the original coordinates are used\n        (this can be useful for certain map projections). The default is to\n        always infer intervals, unless the mesh is irregular and plotted on\n        a map projection.\n    subplot_kws : dict, optional\n        Dictionary of keyword arguments for matplotlib subplots. Only applies\n        to FacetGrid plotting.\n    cbar_ax : matplotlib Axes, optional\n        Axes in which to draw the colorbar.\n    cbar_kwargs : dict, optional\n        Dictionary of keyword arguments to pass to the colorbar.\n    **kwargs : optional\n        Additional arguments to wrapped matplotlib function\n\n    Returns\n    -------\n    artist :\n        The same type of primitive artist that the wrapped matplotlib\n        function returns\n    """"""\n\n    # Build on the original docstring\n    plotfunc.__doc__ = f""{plotfunc.__doc__}\\n{commondoc}""\n\n    @functools.wraps(plotfunc)\n    def newplotfunc(\n        darray,\n        x=None,\n        y=None,\n        figsize=None,\n        size=None,\n        aspect=None,\n        ax=None,\n        row=None,\n        col=None,\n        col_wrap=None,\n        xincrease=True,\n        yincrease=True,\n        add_colorbar=None,\n        add_labels=True,\n        vmin=None,\n        vmax=None,\n        cmap=None,\n        center=None,\n        robust=False,\n        extend=None,\n        levels=None,\n        infer_intervals=None,\n        colors=None,\n        subplot_kws=None,\n        cbar_ax=None,\n        cbar_kwargs=None,\n        xscale=None,\n        yscale=None,\n        xticks=None,\n        yticks=None,\n        xlim=None,\n        ylim=None,\n        norm=None,\n        **kwargs,\n    ):\n        # All 2d plots in xarray share this function signature.\n        # Method signature below should be consistent.\n\n        # Decide on a default for the colorbar before facetgrids\n        if add_colorbar is None:\n            add_colorbar = plotfunc.__name__ != ""contour""\n        imshow_rgb = plotfunc.__name__ == ""imshow"" and darray.ndim == (\n            3 + (row is not None) + (col is not None)\n        )\n        if imshow_rgb:\n            # Don\'t add a colorbar when showing an image with explicit colors\n            add_colorbar = False\n            # Matplotlib does not support normalising RGB data, so do it here.\n            # See eg. https://github.com/matplotlib/matplotlib/pull/10220\n            if robust or vmax is not None or vmin is not None:\n                darray = _rescale_imshow_rgb(darray, vmin, vmax, robust)\n                vmin, vmax, robust = None, None, False\n\n        # Handle facetgrids first\n        if row or col:\n            allargs = locals().copy()\n            del allargs[""darray""]\n            del allargs[""imshow_rgb""]\n            allargs.update(allargs.pop(""kwargs""))\n            # Need the decorated plotting function\n            allargs[""plotfunc""] = globals()[plotfunc.__name__]\n            return _easy_facetgrid(darray, kind=""dataarray"", **allargs)\n\n        plt = import_matplotlib_pyplot()\n\n        rgb = kwargs.pop(""rgb"", None)\n        if rgb is not None and plotfunc.__name__ != ""imshow"":\n            raise ValueError(\'The ""rgb"" keyword is only valid for imshow()\')\n        elif rgb is not None and not imshow_rgb:\n            raise ValueError(\n                \'The ""rgb"" keyword is only valid for imshow()\'\n                ""with a three-dimensional array (per facet)""\n            )\n\n        xlab, ylab = _infer_xy_labels(\n            darray=darray, x=x, y=y, imshow=imshow_rgb, rgb=rgb\n        )\n\n        # better to pass the ndarrays directly to plotting functions\n        xval = darray[xlab].values\n        yval = darray[ylab].values\n\n        # check if we need to broadcast one dimension\n        if xval.ndim < yval.ndim:\n            dims = darray[ylab].dims\n            if xval.shape[0] == yval.shape[0]:\n                xval = np.broadcast_to(xval[:, np.newaxis], yval.shape)\n            else:\n                xval = np.broadcast_to(xval[np.newaxis, :], yval.shape)\n\n        elif yval.ndim < xval.ndim:\n            dims = darray[xlab].dims\n            if yval.shape[0] == xval.shape[0]:\n                yval = np.broadcast_to(yval[:, np.newaxis], xval.shape)\n            else:\n                yval = np.broadcast_to(yval[np.newaxis, :], xval.shape)\n        elif xval.ndim == 2:\n            dims = darray[xlab].dims\n        else:\n            dims = (darray[ylab].dims[0], darray[xlab].dims[0])\n\n        # May need to transpose for correct x, y labels\n        # xlab may be the name of a coord, we have to check for dim names\n        if imshow_rgb:\n            # For RGB[A] images, matplotlib requires the color dimension\n            # to be last.  In Xarray the order should be unimportant, so\n            # we transpose to (y, x, color) to make this work.\n            yx_dims = (ylab, xlab)\n            dims = yx_dims + tuple(d for d in darray.dims if d not in yx_dims)\n\n        if dims != darray.dims:\n            darray = darray.transpose(*dims, transpose_coords=True)\n\n        # Pass the data as a masked ndarray too\n        zval = darray.to_masked_array(copy=False)\n\n        # Replace pd.Intervals if contained in xval or yval.\n        xplt, xlab_extra = _resolve_intervals_2dplot(xval, plotfunc.__name__)\n        yplt, ylab_extra = _resolve_intervals_2dplot(yval, plotfunc.__name__)\n\n        _ensure_plottable(xplt, yplt, zval)\n\n        cmap_params, cbar_kwargs = _process_cmap_cbar_kwargs(\n            plotfunc,\n            zval.data,\n            **locals(),\n            _is_facetgrid=kwargs.pop(""_is_facetgrid"", False),\n        )\n\n        if ""contour"" in plotfunc.__name__:\n            # extend is a keyword argument only for contour and contourf, but\n            # passing it to the colorbar is sufficient for imshow and\n            # pcolormesh\n            kwargs[""extend""] = cmap_params[""extend""]\n            kwargs[""levels""] = cmap_params[""levels""]\n            # if colors == a single color, matplotlib draws dashed negative\n            # contours. we lose this feature if we pass cmap and not colors\n            if isinstance(colors, str):\n                cmap_params[""cmap""] = None\n                kwargs[""colors""] = colors\n\n        if ""pcolormesh"" == plotfunc.__name__:\n            kwargs[""infer_intervals""] = infer_intervals\n\n        if ""imshow"" == plotfunc.__name__ and isinstance(aspect, str):\n            # forbid usage of mpl strings\n            raise ValueError(\n                ""plt.imshow\'s `aspect` kwarg is not available "" ""in xarray""\n            )\n\n        ax = get_axis(figsize, size, aspect, ax)\n        primitive = plotfunc(\n            xplt,\n            yplt,\n            zval,\n            ax=ax,\n            cmap=cmap_params[""cmap""],\n            vmin=cmap_params[""vmin""],\n            vmax=cmap_params[""vmax""],\n            norm=cmap_params[""norm""],\n            **kwargs,\n        )\n\n        # Label the plot with metadata\n        if add_labels:\n            ax.set_xlabel(label_from_attrs(darray[xlab], xlab_extra))\n            ax.set_ylabel(label_from_attrs(darray[ylab], ylab_extra))\n            ax.set_title(darray._title_for_slice())\n\n        if add_colorbar:\n            if add_labels and ""label"" not in cbar_kwargs:\n                cbar_kwargs[""label""] = label_from_attrs(darray)\n            cbar = _add_colorbar(primitive, ax, cbar_ax, cbar_kwargs, cmap_params)\n        elif cbar_ax is not None or cbar_kwargs:\n            # inform the user about keywords which aren\'t used\n            raise ValueError(\n                ""cbar_ax and cbar_kwargs can\'t be used with "" ""add_colorbar=False.""\n            )\n\n        # origin kwarg overrides yincrease\n        if ""origin"" in kwargs:\n            yincrease = None\n\n        _update_axes(\n            ax, xincrease, yincrease, xscale, yscale, xticks, yticks, xlim, ylim\n        )\n\n        # Rotate dates on xlabels\n        # Do this without calling autofmt_xdate so that x-axes ticks\n        # on other subplots (if any) are not deleted.\n        # https://stackoverflow.com/questions/17430105/autofmt-xdate-deletes-x-axis-labels-of-all-subplots\n        if np.issubdtype(xplt.dtype, np.datetime64):\n            for xlabels in ax.get_xticklabels():\n                xlabels.set_rotation(30)\n                xlabels.set_ha(""right"")\n\n        return primitive\n\n    # For use as DataArray.plot.plotmethod\n    @functools.wraps(newplotfunc)\n    def plotmethod(\n        _PlotMethods_obj,\n        x=None,\n        y=None,\n        figsize=None,\n        size=None,\n        aspect=None,\n        ax=None,\n        row=None,\n        col=None,\n        col_wrap=None,\n        xincrease=True,\n        yincrease=True,\n        add_colorbar=None,\n        add_labels=True,\n        vmin=None,\n        vmax=None,\n        cmap=None,\n        colors=None,\n        center=None,\n        robust=False,\n        extend=None,\n        levels=None,\n        infer_intervals=None,\n        subplot_kws=None,\n        cbar_ax=None,\n        cbar_kwargs=None,\n        xscale=None,\n        yscale=None,\n        xticks=None,\n        yticks=None,\n        xlim=None,\n        ylim=None,\n        norm=None,\n        **kwargs,\n    ):\n        """"""\n        The method should have the same signature as the function.\n\n        This just makes the method work on Plotmethods objects,\n        and passes all the other arguments straight through.\n        """"""\n        allargs = locals()\n        allargs[""darray""] = _PlotMethods_obj._da\n        allargs.update(kwargs)\n        for arg in [""_PlotMethods_obj"", ""newplotfunc"", ""kwargs""]:\n            del allargs[arg]\n        return newplotfunc(**allargs)\n\n    # Add to class _PlotMethods\n    setattr(_PlotMethods, plotmethod.__name__, plotmethod)\n\n    return newplotfunc\n\n\n@_plot2d\ndef imshow(x, y, z, ax, **kwargs):\n    """"""\n    Image plot of 2d DataArray using matplotlib.pyplot\n\n    Wraps :func:`matplotlib:matplotlib.pyplot.imshow`\n\n    While other plot methods require the DataArray to be strictly\n    two-dimensional, ``imshow`` also accepts a 3D array where some\n    dimension can be interpreted as RGB or RGBA color channels and\n    allows this dimension to be specified via the kwarg ``rgb=``.\n\n    Unlike matplotlib, Xarray can apply ``vmin`` and ``vmax`` to RGB or RGBA\n    data, by applying a single scaling factor and offset to all bands.\n    Passing  ``robust=True`` infers ``vmin`` and ``vmax``\n    :ref:`in the usual way <robust-plotting>`.\n\n    .. note::\n        This function needs uniformly spaced coordinates to\n        properly label the axes. Call DataArray.plot() to check.\n\n    The pixels are centered on the coordinates values. Ie, if the coordinate\n    value is 3.2 then the pixels for those coordinates will be centered on 3.2.\n    """"""\n\n    if x.ndim != 1 or y.ndim != 1:\n        raise ValueError(\n            ""imshow requires 1D coordinates, try using "" ""pcolormesh or contour(f)""\n        )\n\n    # Centering the pixels- Assumes uniform spacing\n    try:\n        xstep = (x[1] - x[0]) / 2.0\n    except IndexError:\n        # Arbitrary default value, similar to matplotlib behaviour\n        xstep = 0.1\n    try:\n        ystep = (y[1] - y[0]) / 2.0\n    except IndexError:\n        ystep = 0.1\n    left, right = x[0] - xstep, x[-1] + xstep\n    bottom, top = y[-1] + ystep, y[0] - ystep\n\n    defaults = {""origin"": ""upper"", ""interpolation"": ""nearest""}\n\n    if not hasattr(ax, ""projection""):\n        # not for cartopy geoaxes\n        defaults[""aspect""] = ""auto""\n\n    # Allow user to override these defaults\n    defaults.update(kwargs)\n\n    if defaults[""origin""] == ""upper"":\n        defaults[""extent""] = [left, right, bottom, top]\n    else:\n        defaults[""extent""] = [left, right, top, bottom]\n\n    if z.ndim == 3:\n        # matplotlib imshow uses black for missing data, but Xarray makes\n        # missing data transparent.  We therefore add an alpha channel if\n        # there isn\'t one, and set it to transparent where data is masked.\n        if z.shape[-1] == 3:\n            alpha = np.ma.ones(z.shape[:2] + (1,), dtype=z.dtype)\n            if np.issubdtype(z.dtype, np.integer):\n                alpha *= 255\n            z = np.ma.concatenate((z, alpha), axis=2)\n        else:\n            z = z.copy()\n        z[np.any(z.mask, axis=-1), -1] = 0\n\n    primitive = ax.imshow(z, **defaults)\n\n    return primitive\n\n\n@_plot2d\ndef contour(x, y, z, ax, **kwargs):\n    """"""\n    Contour plot of 2d DataArray\n\n    Wraps :func:`matplotlib:matplotlib.pyplot.contour`\n    """"""\n    primitive = ax.contour(x, y, z, **kwargs)\n    return primitive\n\n\n@_plot2d\ndef contourf(x, y, z, ax, **kwargs):\n    """"""\n    Filled contour plot of 2d DataArray\n\n    Wraps :func:`matplotlib:matplotlib.pyplot.contourf`\n    """"""\n    primitive = ax.contourf(x, y, z, **kwargs)\n    return primitive\n\n\n@_plot2d\ndef pcolormesh(x, y, z, ax, infer_intervals=None, **kwargs):\n    """"""\n    Pseudocolor plot of 2d DataArray\n\n    Wraps :func:`matplotlib:matplotlib.pyplot.pcolormesh`\n    """"""\n\n    # decide on a default for infer_intervals (GH781)\n    x = np.asarray(x)\n    if infer_intervals is None:\n        if hasattr(ax, ""projection""):\n            if len(x.shape) == 1:\n                infer_intervals = True\n            else:\n                infer_intervals = False\n        else:\n            infer_intervals = True\n\n    if infer_intervals and (\n        (np.shape(x)[0] == np.shape(z)[1])\n        or ((x.ndim > 1) and (np.shape(x)[1] == np.shape(z)[1]))\n    ):\n        if len(x.shape) == 1:\n            x = _infer_interval_breaks(x, check_monotonic=True)\n        else:\n            # we have to infer the intervals on both axes\n            x = _infer_interval_breaks(x, axis=1)\n            x = _infer_interval_breaks(x, axis=0)\n\n    if infer_intervals and (np.shape(y)[0] == np.shape(z)[0]):\n        if len(y.shape) == 1:\n            y = _infer_interval_breaks(y, check_monotonic=True)\n        else:\n            # we have to infer the intervals on both axes\n            y = _infer_interval_breaks(y, axis=1)\n            y = _infer_interval_breaks(y, axis=0)\n\n    primitive = ax.pcolormesh(x, y, z, **kwargs)\n\n    # by default, pcolormesh picks ""round"" values for bounds\n    # this results in ugly looking plots with lots of surrounding whitespace\n    if not hasattr(ax, ""projection"") and x.ndim == 1 and y.ndim == 1:\n        # not a cartopy geoaxis\n        ax.set_xlim(x[0], x[-1])\n        ax.set_ylim(y[0], y[-1])\n\n    return primitive\n'"
xarray/plot/utils.py,44,"b'import itertools\nimport textwrap\nimport warnings\nfrom datetime import datetime\nfrom inspect import getfullargspec\nfrom typing import Any, Iterable, Mapping, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\n\nfrom ..core.options import OPTIONS\nfrom ..core.utils import is_scalar\n\ntry:\n    import nc_time_axis  # noqa: F401\n\n    nc_time_axis_available = True\nexcept ImportError:\n    nc_time_axis_available = False\n\nROBUST_PERCENTILE = 2.0\n\n\n_registered = False\n\n\ndef register_pandas_datetime_converter_if_needed():\n    # based on https://github.com/pandas-dev/pandas/pull/17710\n    global _registered\n    if not _registered:\n        pd.plotting.register_matplotlib_converters()\n        _registered = True\n\n\ndef import_matplotlib_pyplot():\n    """"""Import pyplot as register appropriate converters.""""""\n    register_pandas_datetime_converter_if_needed()\n    import matplotlib.pyplot as plt\n\n    return plt\n\n\ndef _determine_extend(calc_data, vmin, vmax):\n    extend_min = calc_data.min() < vmin\n    extend_max = calc_data.max() > vmax\n    if extend_min and extend_max:\n        extend = ""both""\n    elif extend_min:\n        extend = ""min""\n    elif extend_max:\n        extend = ""max""\n    else:\n        extend = ""neither""\n    return extend\n\n\ndef _build_discrete_cmap(cmap, levels, extend, filled):\n    """"""\n    Build a discrete colormap and normalization of the data.\n    """"""\n    import matplotlib as mpl\n\n    if not filled:\n        # non-filled contour plots\n        extend = ""max""\n\n    if extend == ""both"":\n        ext_n = 2\n    elif extend in [""min"", ""max""]:\n        ext_n = 1\n    else:\n        ext_n = 0\n\n    n_colors = len(levels) + ext_n - 1\n    pal = _color_palette(cmap, n_colors)\n\n    new_cmap, cnorm = mpl.colors.from_levels_and_colors(levels, pal, extend=extend)\n    # copy the old cmap name, for easier testing\n    new_cmap.name = getattr(cmap, ""name"", cmap)\n\n    # copy colors to use for bad, under, and over values in case they have been\n    # set to non-default values\n    try:\n        # matplotlib<3.2 only uses bad color for masked values\n        bad = cmap(np.ma.masked_invalid([np.nan]))[0]\n    except TypeError:\n        # cmap was a str or list rather than a color-map object, so there are\n        # no bad, under or over values to check or copy\n        pass\n    else:\n        under = cmap(-np.inf)\n        over = cmap(np.inf)\n\n        new_cmap.set_bad(bad)\n\n        # Only update under and over if they were explicitly changed by the user\n        # (i.e. are different from the lowest or highest values in cmap). Otherwise\n        # leave unchanged so new_cmap uses its default values (its own lowest and\n        # highest values).\n        if under != cmap(0):\n            new_cmap.set_under(under)\n        if over != cmap(cmap.N - 1):\n            new_cmap.set_over(over)\n\n    return new_cmap, cnorm\n\n\ndef _color_palette(cmap, n_colors):\n    import matplotlib.pyplot as plt\n    from matplotlib.colors import ListedColormap\n\n    colors_i = np.linspace(0, 1.0, n_colors)\n    if isinstance(cmap, (list, tuple)):\n        # we have a list of colors\n        cmap = ListedColormap(cmap, N=n_colors)\n        pal = cmap(colors_i)\n    elif isinstance(cmap, str):\n        # we have some sort of named palette\n        try:\n            # is this a matplotlib cmap?\n            cmap = plt.get_cmap(cmap)\n            pal = cmap(colors_i)\n        except ValueError:\n            # ValueError happens when mpl doesn\'t like a colormap, try seaborn\n            try:\n                from seaborn import color_palette\n\n                pal = color_palette(cmap, n_colors=n_colors)\n            except (ValueError, ImportError):\n                # or maybe we just got a single color as a string\n                cmap = ListedColormap([cmap], N=n_colors)\n                pal = cmap(colors_i)\n    else:\n        # cmap better be a LinearSegmentedColormap (e.g. viridis)\n        pal = cmap(colors_i)\n\n    return pal\n\n\n# _determine_cmap_params is adapted from Seaborn:\n# https://github.com/mwaskom/seaborn/blob/v0.6/seaborn/matrix.py#L158\n# Used under the terms of Seaborn\'s license, see licenses/SEABORN_LICENSE.\n\n\ndef _determine_cmap_params(\n    plot_data,\n    vmin=None,\n    vmax=None,\n    cmap=None,\n    center=None,\n    robust=False,\n    extend=None,\n    levels=None,\n    filled=True,\n    norm=None,\n    _is_facetgrid=False,\n):\n    """"""\n    Use some heuristics to set good defaults for colorbar and range.\n\n    Parameters\n    ==========\n    plot_data: Numpy array\n        Doesn\'t handle xarray objects\n\n    Returns\n    =======\n    cmap_params : dict\n        Use depends on the type of the plotting function\n    """"""\n    import matplotlib as mpl\n\n    if isinstance(levels, Iterable):\n        levels = sorted(levels)\n\n    calc_data = np.ravel(plot_data[np.isfinite(plot_data)])\n\n    # Handle all-NaN input data gracefully\n    if calc_data.size == 0:\n        # Arbitrary default for when all values are NaN\n        calc_data = np.array(0.0)\n\n    # Setting center=False prevents a divergent cmap\n    possibly_divergent = center is not False\n\n    # Set center to 0 so math below makes sense but remember its state\n    center_is_none = False\n    if center is None:\n        center = 0\n        center_is_none = True\n\n    # Setting both vmin and vmax prevents a divergent cmap\n    if (vmin is not None) and (vmax is not None):\n        possibly_divergent = False\n\n    # Setting vmin or vmax implies linspaced levels\n    user_minmax = (vmin is not None) or (vmax is not None)\n\n    # vlim might be computed below\n    vlim = None\n\n    # save state; needed later\n    vmin_was_none = vmin is None\n    vmax_was_none = vmax is None\n\n    if vmin is None:\n        if robust:\n            vmin = np.percentile(calc_data, ROBUST_PERCENTILE)\n        else:\n            vmin = calc_data.min()\n    elif possibly_divergent:\n        vlim = abs(vmin - center)\n\n    if vmax is None:\n        if robust:\n            vmax = np.percentile(calc_data, 100 - ROBUST_PERCENTILE)\n        else:\n            vmax = calc_data.max()\n    elif possibly_divergent:\n        vlim = abs(vmax - center)\n\n    if possibly_divergent:\n        levels_are_divergent = (\n            isinstance(levels, Iterable) and levels[0] * levels[-1] < 0\n        )\n        # kwargs not specific about divergent or not: infer defaults from data\n        divergent = (\n            ((vmin < 0) and (vmax > 0)) or not center_is_none or levels_are_divergent\n        )\n    else:\n        divergent = False\n\n    # A divergent map should be symmetric around the center value\n    if divergent:\n        if vlim is None:\n            vlim = max(abs(vmin - center), abs(vmax - center))\n        vmin, vmax = -vlim, vlim\n\n    # Now add in the centering value and set the limits\n    vmin += center\n    vmax += center\n\n    # now check norm and harmonize with vmin, vmax\n    if norm is not None:\n        if norm.vmin is None:\n            norm.vmin = vmin\n        else:\n            if not vmin_was_none and vmin != norm.vmin:\n                raise ValueError(""Cannot supply vmin and a norm with a different vmin."")\n            vmin = norm.vmin\n\n        if norm.vmax is None:\n            norm.vmax = vmax\n        else:\n            if not vmax_was_none and vmax != norm.vmax:\n                raise ValueError(""Cannot supply vmax and a norm with a different vmax."")\n            vmax = norm.vmax\n\n    # if BoundaryNorm, then set levels\n    if isinstance(norm, mpl.colors.BoundaryNorm):\n        levels = norm.boundaries\n\n    # Choose default colormaps if not provided\n    if cmap is None:\n        if divergent:\n            cmap = OPTIONS[""cmap_divergent""]\n        else:\n            cmap = OPTIONS[""cmap_sequential""]\n\n    # Handle discrete levels\n    if levels is not None:\n        if is_scalar(levels):\n            if user_minmax:\n                levels = np.linspace(vmin, vmax, levels)\n            elif levels == 1:\n                levels = np.asarray([(vmin + vmax) / 2])\n            else:\n                # N in MaxNLocator refers to bins, not ticks\n                ticker = mpl.ticker.MaxNLocator(levels - 1)\n                levels = ticker.tick_values(vmin, vmax)\n        vmin, vmax = levels[0], levels[-1]\n\n    # GH3734\n    if vmin == vmax:\n        vmin, vmax = mpl.ticker.LinearLocator(2).tick_values(vmin, vmax)\n\n    if extend is None:\n        extend = _determine_extend(calc_data, vmin, vmax)\n\n    if levels is not None or isinstance(norm, mpl.colors.BoundaryNorm):\n        cmap, newnorm = _build_discrete_cmap(cmap, levels, extend, filled)\n        norm = newnorm if norm is None else norm\n\n    return dict(\n        vmin=vmin, vmax=vmax, cmap=cmap, extend=extend, levels=levels, norm=norm\n    )\n\n\ndef _infer_xy_labels_3d(darray, x, y, rgb):\n    """"""\n    Determine x and y labels for showing RGB images.\n\n    Attempts to infer which dimension is RGB/RGBA by size and order of dims.\n\n    """"""\n    assert rgb is None or rgb != x\n    assert rgb is None or rgb != y\n    # Start by detecting and reporting invalid combinations of arguments\n    assert darray.ndim == 3\n    not_none = [a for a in (x, y, rgb) if a is not None]\n    if len(set(not_none)) < len(not_none):\n        raise ValueError(\n            ""Dimension names must be None or unique strings, but imshow was ""\n            ""passed x=%r, y=%r, and rgb=%r."" % (x, y, rgb)\n        )\n    for label in not_none:\n        if label not in darray.dims:\n            raise ValueError(f""{label!r} is not a dimension"")\n\n    # Then calculate rgb dimension if certain and check validity\n    could_be_color = [\n        label\n        for label in darray.dims\n        if darray[label].size in (3, 4) and label not in (x, y)\n    ]\n    if rgb is None and not could_be_color:\n        raise ValueError(\n            ""A 3-dimensional array was passed to imshow(), but there is no ""\n            ""dimension that could be color.  At least one dimension must be ""\n            ""of size 3 (RGB) or 4 (RGBA), and not given as x or y.""\n        )\n    if rgb is None and len(could_be_color) == 1:\n        rgb = could_be_color[0]\n    if rgb is not None and darray[rgb].size not in (3, 4):\n        raise ValueError(\n            ""Cannot interpret dim %r of size %s as RGB or RGBA.""\n            % (rgb, darray[rgb].size)\n        )\n\n    # If rgb dimension is still unknown, there must be two or three dimensions\n    # in could_be_color.  We therefore warn, and use a heuristic to break ties.\n    if rgb is None:\n        assert len(could_be_color) in (2, 3)\n        rgb = could_be_color[-1]\n        warnings.warn(\n            ""Several dimensions of this array could be colors.  Xarray ""\n            ""will use the last possible dimension (%r) to match ""\n            ""matplotlib.pyplot.imshow.  You can pass names of x, y, ""\n            ""and/or rgb dimensions to override this guess."" % rgb\n        )\n    assert rgb is not None\n\n    # Finally, we pick out the red slice and delegate to the 2D version:\n    return _infer_xy_labels(darray.isel(**{rgb: 0}), x, y)\n\n\ndef _infer_xy_labels(darray, x, y, imshow=False, rgb=None):\n    """"""\n    Determine x and y labels. For use in _plot2d\n\n    darray must be a 2 dimensional data array, or 3d for imshow only.\n    """"""\n    if (x is not None) and (x == y):\n        raise ValueError(""x and y cannot be equal."")\n\n    if imshow and darray.ndim == 3:\n        return _infer_xy_labels_3d(darray, x, y, rgb)\n\n    if x is None and y is None:\n        if darray.ndim != 2:\n            raise ValueError(""DataArray must be 2d"")\n        y, x = darray.dims\n    elif x is None:\n        _assert_valid_xy(darray, y, ""y"")\n        x = darray.dims[0] if y == darray.dims[1] else darray.dims[1]\n    elif y is None:\n        _assert_valid_xy(darray, x, ""x"")\n        y = darray.dims[0] if x == darray.dims[1] else darray.dims[1]\n    else:\n        _assert_valid_xy(darray, x, ""x"")\n        _assert_valid_xy(darray, y, ""y"")\n\n        if (\n            all(k in darray._level_coords for k in (x, y))\n            and darray._level_coords[x] == darray._level_coords[y]\n        ):\n            raise ValueError(""x and y cannot be levels of the same MultiIndex"")\n\n    return x, y\n\n\ndef _assert_valid_xy(darray, xy, name):\n    """"""\n    make sure x and y passed to plotting functions are valid\n    """"""\n\n    # MultiIndex cannot be plotted; no point in allowing them here\n    multiindex = set([darray._level_coords[lc] for lc in darray._level_coords])\n\n    valid_xy = (\n        set(darray.dims) | set(darray.coords) | set(darray._level_coords)\n    ) - multiindex\n\n    if xy not in valid_xy:\n        valid_xy_str = ""\', \'"".join(sorted(valid_xy))\n        raise ValueError(f""{name} must be one of None, \'{valid_xy_str}\'"")\n\n\ndef get_axis(figsize, size, aspect, ax):\n    import matplotlib as mpl\n    import matplotlib.pyplot as plt\n\n    if figsize is not None:\n        if ax is not None:\n            raise ValueError(""cannot provide both `figsize` and "" ""`ax` arguments"")\n        if size is not None:\n            raise ValueError(""cannot provide both `figsize` and "" ""`size` arguments"")\n        _, ax = plt.subplots(figsize=figsize)\n    elif size is not None:\n        if ax is not None:\n            raise ValueError(""cannot provide both `size` and `ax` arguments"")\n        if aspect is None:\n            width, height = mpl.rcParams[""figure.figsize""]\n            aspect = width / height\n        figsize = (size * aspect, size)\n        _, ax = plt.subplots(figsize=figsize)\n    elif aspect is not None:\n        raise ValueError(""cannot provide `aspect` argument without `size`"")\n\n    if ax is None:\n        ax = plt.gca()\n\n    return ax\n\n\ndef label_from_attrs(da, extra=""""):\n    """""" Makes informative labels if variable metadata (attrs) follows\n        CF conventions. """"""\n\n    if da.attrs.get(""long_name""):\n        name = da.attrs[""long_name""]\n    elif da.attrs.get(""standard_name""):\n        name = da.attrs[""standard_name""]\n    elif da.name is not None:\n        name = da.name\n    else:\n        name = """"\n\n    if da.attrs.get(""units""):\n        units = "" [{}]"".format(da.attrs[""units""])\n    else:\n        units = """"\n\n    return ""\\n"".join(textwrap.wrap(name + extra + units, 30))\n\n\ndef _interval_to_mid_points(array):\n    """"""\n    Helper function which returns an array\n    with the Intervals\' mid points.\n    """"""\n\n    return np.array([x.mid for x in array])\n\n\ndef _interval_to_bound_points(array):\n    """"""\n    Helper function which returns an array\n    with the Intervals\' boundaries.\n    """"""\n\n    array_boundaries = np.array([x.left for x in array])\n    array_boundaries = np.concatenate((array_boundaries, np.array([array[-1].right])))\n\n    return array_boundaries\n\n\ndef _interval_to_double_bound_points(xarray, yarray):\n    """"""\n    Helper function to deal with a xarray consisting of pd.Intervals. Each\n    interval is replaced with both boundaries. I.e. the length of xarray\n    doubles. yarray is modified so it matches the new shape of xarray.\n    """"""\n\n    xarray1 = np.array([x.left for x in xarray])\n    xarray2 = np.array([x.right for x in xarray])\n\n    xarray = list(itertools.chain.from_iterable(zip(xarray1, xarray2)))\n    yarray = list(itertools.chain.from_iterable(zip(yarray, yarray)))\n\n    return xarray, yarray\n\n\ndef _resolve_intervals_1dplot(xval, yval, xlabel, ylabel, kwargs):\n    """"""\n    Helper function to replace the values of x and/or y coordinate arrays\n    containing pd.Interval with their mid-points or - for step plots - double\n    points which double the length.\n    """"""\n\n    # Is it a step plot? (see matplotlib.Axes.step)\n    if kwargs.get(""drawstyle"", """").startswith(""steps-""):\n\n        # Convert intervals to double points\n        if _valid_other_type(np.array([xval, yval]), [pd.Interval]):\n            raise TypeError(""Can\'t step plot intervals against intervals."")\n        if _valid_other_type(xval, [pd.Interval]):\n            xval, yval = _interval_to_double_bound_points(xval, yval)\n        if _valid_other_type(yval, [pd.Interval]):\n            yval, xval = _interval_to_double_bound_points(yval, xval)\n\n        # Remove steps-* to be sure that matplotlib is not confused\n        del kwargs[""drawstyle""]\n\n    # Is it another kind of plot?\n    else:\n\n        # Convert intervals to mid points and adjust labels\n        if _valid_other_type(xval, [pd.Interval]):\n            xval = _interval_to_mid_points(xval)\n            xlabel += ""_center""\n        if _valid_other_type(yval, [pd.Interval]):\n            yval = _interval_to_mid_points(yval)\n            ylabel += ""_center""\n\n    # return converted arguments\n    return xval, yval, xlabel, ylabel, kwargs\n\n\ndef _resolve_intervals_2dplot(val, func_name):\n    """"""\n    Helper function to replace the values of a coordinate array containing\n    pd.Interval with their mid-points or - for pcolormesh - boundaries which\n    increases length by 1.\n    """"""\n    label_extra = """"\n    if _valid_other_type(val, [pd.Interval]):\n        if func_name == ""pcolormesh"":\n            val = _interval_to_bound_points(val)\n        else:\n            val = _interval_to_mid_points(val)\n            label_extra = ""_center""\n\n    return val, label_extra\n\n\ndef _valid_other_type(x, types):\n    """"""\n    Do all elements of x have a type from types?\n    """"""\n    return all(any(isinstance(el, t) for t in types) for el in np.ravel(x))\n\n\ndef _valid_numpy_subdtype(x, numpy_types):\n    """"""\n    Is any dtype from numpy_types superior to the dtype of x?\n    """"""\n    # If any of the types given in numpy_types is understood as numpy.generic,\n    # all possible x will be considered valid.  This is probably unwanted.\n    for t in numpy_types:\n        assert not np.issubdtype(np.generic, t)\n\n    return any(np.issubdtype(x.dtype, t) for t in numpy_types)\n\n\ndef _ensure_plottable(*args):\n    """"""\n    Raise exception if there is anything in args that can\'t be plotted on an\n    axis by matplotlib.\n    """"""\n    numpy_types = [np.floating, np.integer, np.timedelta64, np.datetime64, np.bool_]\n    other_types = [datetime]\n    try:\n        import cftime\n\n        cftime_datetime = [cftime.datetime]\n    except ImportError:\n        cftime_datetime = []\n    other_types = other_types + cftime_datetime\n    for x in args:\n        if not (\n            _valid_numpy_subdtype(np.array(x), numpy_types)\n            or _valid_other_type(np.array(x), other_types)\n        ):\n            raise TypeError(\n                ""Plotting requires coordinates to be numeric, boolean, ""\n                ""or dates of type numpy.datetime64, ""\n                ""datetime.datetime, cftime.datetime or ""\n                f""pandas.Interval. Received data of type {np.array(x).dtype} instead.""\n            )\n        if (\n            _valid_other_type(np.array(x), cftime_datetime)\n            and not nc_time_axis_available\n        ):\n            raise ImportError(\n                ""Plotting of arrays of cftime.datetime ""\n                ""objects or arrays indexed by ""\n                ""cftime.datetime objects requires the ""\n                ""optional `nc-time-axis` (v1.2.0 or later) ""\n                ""package.""\n            )\n\n\ndef _is_numeric(arr):\n    numpy_types = [np.floating, np.integer]\n    return _valid_numpy_subdtype(arr, numpy_types)\n\n\ndef _add_colorbar(primitive, ax, cbar_ax, cbar_kwargs, cmap_params):\n\n    cbar_kwargs.setdefault(""extend"", cmap_params[""extend""])\n    if cbar_ax is None:\n        cbar_kwargs.setdefault(""ax"", ax)\n    else:\n        cbar_kwargs.setdefault(""cax"", cbar_ax)\n\n    fig = ax.get_figure()\n    cbar = fig.colorbar(primitive, **cbar_kwargs)\n\n    return cbar\n\n\ndef _rescale_imshow_rgb(darray, vmin, vmax, robust):\n    assert robust or vmin is not None or vmax is not None\n\n    # Calculate vmin and vmax automatically for `robust=True`\n    if robust:\n        if vmax is None:\n            vmax = np.nanpercentile(darray, 100 - ROBUST_PERCENTILE)\n        if vmin is None:\n            vmin = np.nanpercentile(darray, ROBUST_PERCENTILE)\n    # If not robust and one bound is None, calculate the default other bound\n    # and check that an interval between them exists.\n    elif vmax is None:\n        vmax = 255 if np.issubdtype(darray.dtype, np.integer) else 1\n        if vmax < vmin:\n            raise ValueError(\n                ""vmin=%r is less than the default vmax (%r) - you must supply ""\n                ""a vmax > vmin in this case."" % (vmin, vmax)\n            )\n    elif vmin is None:\n        vmin = 0\n        if vmin > vmax:\n            raise ValueError(\n                ""vmax=%r is less than the default vmin (0) - you must supply ""\n                ""a vmin < vmax in this case."" % vmax\n            )\n    # Scale interval [vmin .. vmax] to [0 .. 1], with darray as 64-bit float\n    # to avoid precision loss, integer over/underflow, etc with extreme inputs.\n    # After scaling, downcast to 32-bit float.  This substantially reduces\n    # memory usage after we hand `darray` off to matplotlib.\n    darray = ((darray.astype(""f8"") - vmin) / (vmax - vmin)).astype(""f4"")\n    return np.minimum(np.maximum(darray, 0), 1)\n\n\ndef _update_axes(\n    ax,\n    xincrease,\n    yincrease,\n    xscale=None,\n    yscale=None,\n    xticks=None,\n    yticks=None,\n    xlim=None,\n    ylim=None,\n):\n    """"""\n    Update axes with provided parameters\n    """"""\n    if xincrease is None:\n        pass\n    elif xincrease and ax.xaxis_inverted():\n        ax.invert_xaxis()\n    elif not xincrease and not ax.xaxis_inverted():\n        ax.invert_xaxis()\n\n    if yincrease is None:\n        pass\n    elif yincrease and ax.yaxis_inverted():\n        ax.invert_yaxis()\n    elif not yincrease and not ax.yaxis_inverted():\n        ax.invert_yaxis()\n\n    # The default xscale, yscale needs to be None.\n    # If we set a scale it resets the axes formatters,\n    # This means that set_xscale(\'linear\') on a datetime axis\n    # will remove the date labels. So only set the scale when explicitly\n    # asked to. https://github.com/matplotlib/matplotlib/issues/8740\n    if xscale is not None:\n        ax.set_xscale(xscale)\n    if yscale is not None:\n        ax.set_yscale(yscale)\n\n    if xticks is not None:\n        ax.set_xticks(xticks)\n    if yticks is not None:\n        ax.set_yticks(yticks)\n\n    if xlim is not None:\n        ax.set_xlim(xlim)\n    if ylim is not None:\n        ax.set_ylim(ylim)\n\n\ndef _is_monotonic(coord, axis=0):\n    """"""\n    >>> _is_monotonic(np.array([0, 1, 2]))\n    True\n    >>> _is_monotonic(np.array([2, 1, 0]))\n    True\n    >>> _is_monotonic(np.array([0, 2, 1]))\n    False\n    """"""\n    if coord.shape[axis] < 3:\n        return True\n    else:\n        n = coord.shape[axis]\n        delta_pos = coord.take(np.arange(1, n), axis=axis) >= coord.take(\n            np.arange(0, n - 1), axis=axis\n        )\n        delta_neg = coord.take(np.arange(1, n), axis=axis) <= coord.take(\n            np.arange(0, n - 1), axis=axis\n        )\n        return np.all(delta_pos) or np.all(delta_neg)\n\n\ndef _infer_interval_breaks(coord, axis=0, check_monotonic=False):\n    """"""\n    >>> _infer_interval_breaks(np.arange(5))\n    array([-0.5,  0.5,  1.5,  2.5,  3.5,  4.5])\n    >>> _infer_interval_breaks([[0, 1], [3, 4]], axis=1)\n    array([[-0.5,  0.5,  1.5],\n           [ 2.5,  3.5,  4.5]])\n    """"""\n    coord = np.asarray(coord)\n\n    if check_monotonic and not _is_monotonic(coord, axis=axis):\n        raise ValueError(\n            ""The input coordinate is not sorted in increasing ""\n            ""order along axis %d. This can lead to unexpected ""\n            ""results. Consider calling the `sortby` method on ""\n            ""the input DataArray. To plot data with categorical ""\n            ""axes, consider using the `heatmap` function from ""\n            ""the `seaborn` statistical plotting library."" % axis\n        )\n\n    deltas = 0.5 * np.diff(coord, axis=axis)\n    if deltas.size == 0:\n        deltas = np.array(0.0)\n    first = np.take(coord, [0], axis=axis) - np.take(deltas, [0], axis=axis)\n    last = np.take(coord, [-1], axis=axis) + np.take(deltas, [-1], axis=axis)\n    trim_last = tuple(\n        slice(None, -1) if n == axis else slice(None) for n in range(coord.ndim)\n    )\n    return np.concatenate([first, coord[trim_last] + deltas, last], axis=axis)\n\n\ndef _process_cmap_cbar_kwargs(\n    func,\n    data,\n    cmap=None,\n    colors=None,\n    cbar_kwargs: Union[Iterable[Tuple[str, Any]], Mapping[str, Any]] = None,\n    levels=None,\n    _is_facetgrid=False,\n    **kwargs,\n):\n    """"""\n    Parameters\n    ==========\n    func : plotting function\n    data : ndarray,\n        Data values\n\n    Returns\n    =======\n    cmap_params\n\n    cbar_kwargs\n    """"""\n    cbar_kwargs = {} if cbar_kwargs is None else dict(cbar_kwargs)\n\n    if ""contour"" in func.__name__ and levels is None:\n        levels = 7  # this is the matplotlib default\n\n    # colors is mutually exclusive with cmap\n    if cmap and colors:\n        raise ValueError(""Can\'t specify both cmap and colors."")\n\n    # colors is only valid when levels is supplied or the plot is of type\n    # contour or contourf\n    if colors and ((""contour"" not in func.__name__) and (levels is None)):\n        raise ValueError(""Can only specify colors with contour or levels"")\n\n    # we should not be getting a list of colors in cmap anymore\n    # is there a better way to do this test?\n    if isinstance(cmap, (list, tuple)):\n        raise ValueError(\n            ""Specifying a list of colors in cmap is deprecated. ""\n            ""Use colors keyword instead.""\n        )\n\n    cmap_kwargs = {\n        ""plot_data"": data,\n        ""levels"": levels,\n        ""cmap"": colors if colors else cmap,\n        ""filled"": func.__name__ != ""contour"",\n    }\n\n    cmap_args = getfullargspec(_determine_cmap_params).args\n    cmap_kwargs.update((a, kwargs[a]) for a in cmap_args if a in kwargs)\n    if not _is_facetgrid:\n        cmap_params = _determine_cmap_params(**cmap_kwargs)\n    else:\n        cmap_params = {\n            k: cmap_kwargs[k]\n            for k in [""vmin"", ""vmax"", ""cmap"", ""extend"", ""levels"", ""norm""]\n        }\n\n    return cmap_params, cbar_kwargs\n'"
xarray/tests/__init__.py,1,"b'import importlib\nimport platform\nimport re\nimport warnings\nfrom contextlib import contextmanager\nfrom distutils import version\nfrom unittest import mock  # noqa: F401\n\nimport numpy as np\nimport pytest\nfrom numpy.testing import assert_array_equal  # noqa: F401\nfrom pandas.testing import assert_frame_equal  # noqa: F401\n\nimport xarray.testing\nfrom xarray.core import utils\nfrom xarray.core.duck_array_ops import allclose_or_equiv  # noqa: F401\nfrom xarray.core.indexing import ExplicitlyIndexed\nfrom xarray.core.options import set_options\n\n# import mpl and change the backend before other mpl imports\ntry:\n    import matplotlib as mpl\n\n    # Order of imports is important here.\n    # Using a different backend makes Travis CI work\n    mpl.use(""Agg"")\nexcept ImportError:\n    pass\n\n\narm_xfail = pytest.mark.xfail(\n    platform.machine() == ""aarch64"" or ""arm"" in platform.machine(),\n    reason=""expected failure on ARM"",\n)\n\n\ndef _importorskip(modname, minversion=None):\n    try:\n        mod = importlib.import_module(modname)\n        has = True\n        if minversion is not None:\n            if LooseVersion(mod.__version__) < LooseVersion(minversion):\n                raise ImportError(""Minimum version not satisfied"")\n    except ImportError:\n        has = False\n    func = pytest.mark.skipif(not has, reason=f""requires {modname}"")\n    return has, func\n\n\ndef LooseVersion(vstring):\n    # Our development version is something like \'0.10.9+aac7bfc\'\n    # This function just ignored the git commit id.\n    vstring = vstring.split(""+"")[0]\n    return version.LooseVersion(vstring)\n\n\nhas_matplotlib, requires_matplotlib = _importorskip(""matplotlib"")\nhas_scipy, requires_scipy = _importorskip(""scipy"")\nhas_pydap, requires_pydap = _importorskip(""pydap.client"")\nhas_netCDF4, requires_netCDF4 = _importorskip(""netCDF4"")\nhas_h5netcdf, requires_h5netcdf = _importorskip(""h5netcdf"")\nhas_pynio, requires_pynio = _importorskip(""Nio"")\nhas_pseudonetcdf, requires_pseudonetcdf = _importorskip(""PseudoNetCDF"")\nhas_cftime, requires_cftime = _importorskip(""cftime"")\nhas_cftime_1_1_0, requires_cftime_1_1_0 = _importorskip(""cftime"", minversion=""1.1.0.0"")\nhas_dask, requires_dask = _importorskip(""dask"")\nhas_bottleneck, requires_bottleneck = _importorskip(""bottleneck"")\nhas_nc_time_axis, requires_nc_time_axis = _importorskip(""nc_time_axis"")\nhas_rasterio, requires_rasterio = _importorskip(""rasterio"")\nhas_zarr, requires_zarr = _importorskip(""zarr"")\nhas_iris, requires_iris = _importorskip(""iris"")\nhas_cfgrib, requires_cfgrib = _importorskip(""cfgrib"")\nhas_numbagg, requires_numbagg = _importorskip(""numbagg"")\nhas_seaborn, requires_seaborn = _importorskip(""seaborn"")\nhas_sparse, requires_sparse = _importorskip(""sparse"")\n\n# some special cases\nhas_scipy_or_netCDF4 = has_scipy or has_netCDF4\nrequires_scipy_or_netCDF4 = pytest.mark.skipif(\n    not has_scipy_or_netCDF4, reason=""requires scipy or netCDF4""\n)\n\n# change some global options for tests\nset_options(warn_for_unclosed_files=True)\n\nif has_dask:\n    import dask\n\n    dask.config.set(scheduler=""single-threaded"")\n\nflaky = pytest.mark.flaky\nnetwork = pytest.mark.network\n\n\n@contextmanager\ndef raises_regex(error, pattern):\n    __tracebackhide__ = True\n    with pytest.raises(error) as excinfo:\n        yield\n    message = str(excinfo.value)\n    if not re.search(pattern, message):\n        raise AssertionError(\n            f""exception {excinfo.value!r} did not match pattern {pattern!r}""\n        )\n\n\nclass UnexpectedDataAccess(Exception):\n    pass\n\n\nclass InaccessibleArray(utils.NDArrayMixin, ExplicitlyIndexed):\n    def __init__(self, array):\n        self.array = array\n\n    def __getitem__(self, key):\n        raise UnexpectedDataAccess(""Tried accessing data"")\n\n\nclass ReturnItem:\n    def __getitem__(self, key):\n        return key\n\n\nclass IndexerMaker:\n    def __init__(self, indexer_cls):\n        self._indexer_cls = indexer_cls\n\n    def __getitem__(self, key):\n        if not isinstance(key, tuple):\n            key = (key,)\n        return self._indexer_cls(key)\n\n\ndef source_ndarray(array):\n    """"""Given an ndarray, return the base object which holds its memory, or the\n    object itself.\n    """"""\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", ""DatetimeIndex.base"")\n        warnings.filterwarnings(""ignore"", ""TimedeltaIndex.base"")\n        base = getattr(array, ""base"", np.asarray(array).base)\n    if base is None:\n        base = array\n    return base\n\n\n# Internal versions of xarray\'s test functions that validate additional\n# invariants\n\n\ndef assert_equal(a, b):\n    __tracebackhide__ = True\n    xarray.testing.assert_equal(a, b)\n    xarray.testing._assert_internal_invariants(a)\n    xarray.testing._assert_internal_invariants(b)\n\n\ndef assert_identical(a, b):\n    __tracebackhide__ = True\n    xarray.testing.assert_identical(a, b)\n    xarray.testing._assert_internal_invariants(a)\n    xarray.testing._assert_internal_invariants(b)\n\n\ndef assert_allclose(a, b, **kwargs):\n    __tracebackhide__ = True\n    xarray.testing.assert_allclose(a, b, **kwargs)\n    xarray.testing._assert_internal_invariants(a)\n    xarray.testing._assert_internal_invariants(b)\n'"
xarray/tests/test_accessor_dt.py,18,"b'import numpy as np\nimport pandas as pd\nimport pytest\n\nimport xarray as xr\n\nfrom . import (\n    assert_array_equal,\n    assert_equal,\n    assert_identical,\n    raises_regex,\n    requires_cftime,\n    requires_dask,\n)\nfrom .test_dask import assert_chunks_equal, raise_if_dask_computes\n\n\nclass TestDatetimeAccessor:\n    @pytest.fixture(autouse=True)\n    def setup(self):\n        nt = 100\n        data = np.random.rand(10, 10, nt)\n        lons = np.linspace(0, 11, 10)\n        lats = np.linspace(0, 20, 10)\n        self.times = pd.date_range(start=""2000/01/01"", freq=""H"", periods=nt)\n\n        self.data = xr.DataArray(\n            data,\n            coords=[lons, lats, self.times],\n            dims=[""lon"", ""lat"", ""time""],\n            name=""data"",\n        )\n\n        self.times_arr = np.random.choice(self.times, size=(10, 10, nt))\n        self.times_data = xr.DataArray(\n            self.times_arr,\n            coords=[lons, lats, self.times],\n            dims=[""lon"", ""lat"", ""time""],\n            name=""data"",\n        )\n\n    @pytest.mark.parametrize(\n        ""field"",\n        [\n            ""year"",\n            ""month"",\n            ""day"",\n            ""hour"",\n            ""minute"",\n            ""second"",\n            ""microsecond"",\n            ""nanosecond"",\n            ""week"",\n            ""weekofyear"",\n            ""dayofweek"",\n            ""weekday"",\n            ""dayofyear"",\n            ""quarter"",\n            ""is_month_start"",\n            ""is_month_end"",\n            ""is_quarter_start"",\n            ""is_quarter_end"",\n            ""is_year_start"",\n            ""is_year_end"",\n            ""is_leap_year"",\n        ],\n    )\n    def test_field_access(self, field):\n        expected = xr.DataArray(\n            getattr(self.times, field), name=field, coords=[self.times], dims=[""time""]\n        )\n        actual = getattr(self.data.time.dt, field)\n        assert_equal(expected, actual)\n\n    def test_strftime(self):\n        assert (\n            ""2000-01-01 01:00:00"" == self.data.time.dt.strftime(""%Y-%m-%d %H:%M:%S"")[1]\n        )\n\n    def test_not_datetime_type(self):\n        nontime_data = self.data.copy()\n        int_data = np.arange(len(self.data.time)).astype(""int8"")\n        nontime_data = nontime_data.assign_coords(time=int_data)\n        with raises_regex(TypeError, ""dt""):\n            nontime_data.time.dt\n\n    @requires_dask\n    @pytest.mark.parametrize(\n        ""field"",\n        [\n            ""year"",\n            ""month"",\n            ""day"",\n            ""hour"",\n            ""minute"",\n            ""second"",\n            ""microsecond"",\n            ""nanosecond"",\n            ""week"",\n            ""weekofyear"",\n            ""dayofweek"",\n            ""weekday"",\n            ""dayofyear"",\n            ""quarter"",\n            ""is_month_start"",\n            ""is_month_end"",\n            ""is_quarter_start"",\n            ""is_quarter_end"",\n            ""is_year_start"",\n            ""is_year_end"",\n            ""is_leap_year"",\n        ],\n    )\n    def test_dask_field_access(self, field):\n        import dask.array as da\n\n        expected = getattr(self.times_data.dt, field)\n\n        dask_times_arr = da.from_array(self.times_arr, chunks=(5, 5, 50))\n        dask_times_2d = xr.DataArray(\n            dask_times_arr, coords=self.data.coords, dims=self.data.dims, name=""data""\n        )\n\n        with raise_if_dask_computes():\n            actual = getattr(dask_times_2d.dt, field)\n\n        assert isinstance(actual.data, da.Array)\n        assert_chunks_equal(actual, dask_times_2d)\n        assert_equal(actual.compute(), expected.compute())\n\n    @requires_dask\n    @pytest.mark.parametrize(\n        ""method, parameters"",\n        [\n            (""floor"", ""D""),\n            (""ceil"", ""D""),\n            (""round"", ""D""),\n            (""strftime"", ""%Y-%m-%d %H:%M:%S""),\n        ],\n    )\n    def test_dask_accessor_method(self, method, parameters):\n        import dask.array as da\n\n        expected = getattr(self.times_data.dt, method)(parameters)\n        dask_times_arr = da.from_array(self.times_arr, chunks=(5, 5, 50))\n        dask_times_2d = xr.DataArray(\n            dask_times_arr, coords=self.data.coords, dims=self.data.dims, name=""data""\n        )\n\n        with raise_if_dask_computes():\n            actual = getattr(dask_times_2d.dt, method)(parameters)\n\n        assert isinstance(actual.data, da.Array)\n        assert_chunks_equal(actual, dask_times_2d)\n        assert_equal(actual.compute(), expected.compute())\n\n    def test_seasons(self):\n        dates = pd.date_range(start=""2000/01/01"", freq=""M"", periods=12)\n        dates = xr.DataArray(dates)\n        seasons = [\n            ""DJF"",\n            ""DJF"",\n            ""MAM"",\n            ""MAM"",\n            ""MAM"",\n            ""JJA"",\n            ""JJA"",\n            ""JJA"",\n            ""SON"",\n            ""SON"",\n            ""SON"",\n            ""DJF"",\n        ]\n        seasons = xr.DataArray(seasons)\n\n        assert_array_equal(seasons.values, dates.dt.season.values)\n\n    @pytest.mark.parametrize(\n        ""method, parameters"", [(""floor"", ""D""), (""ceil"", ""D""), (""round"", ""D"")]\n    )\n    def test_accessor_method(self, method, parameters):\n        dates = pd.date_range(""2014-01-01"", ""2014-05-01"", freq=""H"")\n        xdates = xr.DataArray(dates, dims=[""time""])\n        expected = getattr(dates, method)(parameters)\n        actual = getattr(xdates.dt, method)(parameters)\n        assert_array_equal(expected, actual)\n\n\nclass TestTimedeltaAccessor:\n    @pytest.fixture(autouse=True)\n    def setup(self):\n        nt = 100\n        data = np.random.rand(10, 10, nt)\n        lons = np.linspace(0, 11, 10)\n        lats = np.linspace(0, 20, 10)\n        self.times = pd.timedelta_range(start=""1 day"", freq=""6H"", periods=nt)\n\n        self.data = xr.DataArray(\n            data,\n            coords=[lons, lats, self.times],\n            dims=[""lon"", ""lat"", ""time""],\n            name=""data"",\n        )\n\n        self.times_arr = np.random.choice(self.times, size=(10, 10, nt))\n        self.times_data = xr.DataArray(\n            self.times_arr,\n            coords=[lons, lats, self.times],\n            dims=[""lon"", ""lat"", ""time""],\n            name=""data"",\n        )\n\n    def test_not_datetime_type(self):\n        nontime_data = self.data.copy()\n        int_data = np.arange(len(self.data.time)).astype(""int8"")\n        nontime_data = nontime_data.assign_coords(time=int_data)\n        with raises_regex(TypeError, ""dt""):\n            nontime_data.time.dt\n\n    @pytest.mark.parametrize(\n        ""field"", [""days"", ""seconds"", ""microseconds"", ""nanoseconds""]\n    )\n    def test_field_access(self, field):\n        expected = xr.DataArray(\n            getattr(self.times, field), name=field, coords=[self.times], dims=[""time""]\n        )\n        actual = getattr(self.data.time.dt, field)\n        assert_equal(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""method, parameters"", [(""floor"", ""D""), (""ceil"", ""D""), (""round"", ""D"")]\n    )\n    def test_accessor_methods(self, method, parameters):\n        dates = pd.timedelta_range(start=""1 day"", end=""30 days"", freq=""6H"")\n        xdates = xr.DataArray(dates, dims=[""time""])\n        expected = getattr(dates, method)(parameters)\n        actual = getattr(xdates.dt, method)(parameters)\n        assert_array_equal(expected, actual)\n\n    @requires_dask\n    @pytest.mark.parametrize(\n        ""field"", [""days"", ""seconds"", ""microseconds"", ""nanoseconds""]\n    )\n    def test_dask_field_access(self, field):\n        import dask.array as da\n\n        expected = getattr(self.times_data.dt, field)\n\n        dask_times_arr = da.from_array(self.times_arr, chunks=(5, 5, 50))\n        dask_times_2d = xr.DataArray(\n            dask_times_arr, coords=self.data.coords, dims=self.data.dims, name=""data""\n        )\n\n        with raise_if_dask_computes():\n            actual = getattr(dask_times_2d.dt, field)\n\n        assert isinstance(actual.data, da.Array)\n        assert_chunks_equal(actual, dask_times_2d)\n        assert_equal(actual, expected)\n\n    @requires_dask\n    @pytest.mark.parametrize(\n        ""method, parameters"", [(""floor"", ""D""), (""ceil"", ""D""), (""round"", ""D"")]\n    )\n    def test_dask_accessor_method(self, method, parameters):\n        import dask.array as da\n\n        expected = getattr(self.times_data.dt, method)(parameters)\n        dask_times_arr = da.from_array(self.times_arr, chunks=(5, 5, 50))\n        dask_times_2d = xr.DataArray(\n            dask_times_arr, coords=self.data.coords, dims=self.data.dims, name=""data""\n        )\n\n        with raise_if_dask_computes():\n            actual = getattr(dask_times_2d.dt, method)(parameters)\n\n        assert isinstance(actual.data, da.Array)\n        assert_chunks_equal(actual, dask_times_2d)\n        assert_equal(actual.compute(), expected.compute())\n\n\n_CFTIME_CALENDARS = [\n    ""365_day"",\n    ""360_day"",\n    ""julian"",\n    ""all_leap"",\n    ""366_day"",\n    ""gregorian"",\n    ""proleptic_gregorian"",\n]\n_NT = 100\n\n\n@pytest.fixture(params=_CFTIME_CALENDARS)\ndef calendar(request):\n    return request.param\n\n\n@pytest.fixture()\ndef times(calendar):\n    import cftime\n\n    return cftime.num2date(\n        np.arange(_NT),\n        units=""hours since 2000-01-01"",\n        calendar=calendar,\n        only_use_cftime_datetimes=True,\n    )\n\n\n@pytest.fixture()\ndef data(times):\n    data = np.random.rand(10, 10, _NT)\n    lons = np.linspace(0, 11, 10)\n    lats = np.linspace(0, 20, 10)\n    return xr.DataArray(\n        data, coords=[lons, lats, times], dims=[""lon"", ""lat"", ""time""], name=""data""\n    )\n\n\n@pytest.fixture()\ndef times_3d(times):\n    lons = np.linspace(0, 11, 10)\n    lats = np.linspace(0, 20, 10)\n    times_arr = np.random.choice(times, size=(10, 10, _NT))\n    return xr.DataArray(\n        times_arr, coords=[lons, lats, times], dims=[""lon"", ""lat"", ""time""], name=""data""\n    )\n\n\n@requires_cftime\n@pytest.mark.parametrize(\n    ""field"", [""year"", ""month"", ""day"", ""hour"", ""dayofyear"", ""dayofweek""]\n)\ndef test_field_access(data, field):\n    if field == ""dayofyear"" or field == ""dayofweek"":\n        pytest.importorskip(""cftime"", minversion=""1.0.2.1"")\n    result = getattr(data.time.dt, field)\n    expected = xr.DataArray(\n        getattr(xr.coding.cftimeindex.CFTimeIndex(data.time.values), field),\n        name=field,\n        coords=data.time.coords,\n        dims=data.time.dims,\n    )\n\n    assert_equal(result, expected)\n\n\n@requires_cftime\n@pytest.mark.filterwarnings(""ignore::RuntimeWarning"")\ndef test_cftime_strftime_access(data):\n    """""" compare cftime formatting against datetime formatting """"""\n    date_format = ""%Y%m%d%H""\n    result = data.time.dt.strftime(date_format)\n    datetime_array = xr.DataArray(\n        xr.coding.cftimeindex.CFTimeIndex(data.time.values).to_datetimeindex(),\n        name=""stftime"",\n        coords=data.time.coords,\n        dims=data.time.dims,\n    )\n    expected = datetime_array.dt.strftime(date_format)\n    assert_equal(result, expected)\n\n\n@requires_cftime\n@requires_dask\n@pytest.mark.parametrize(\n    ""field"", [""year"", ""month"", ""day"", ""hour"", ""dayofyear"", ""dayofweek""]\n)\ndef test_dask_field_access_1d(data, field):\n    import dask.array as da\n\n    if field == ""dayofyear"" or field == ""dayofweek"":\n        pytest.importorskip(""cftime"", minversion=""1.0.2.1"")\n    expected = xr.DataArray(\n        getattr(xr.coding.cftimeindex.CFTimeIndex(data.time.values), field),\n        name=field,\n        dims=[""time""],\n    )\n    times = xr.DataArray(data.time.values, dims=[""time""]).chunk({""time"": 50})\n    result = getattr(times.dt, field)\n    assert isinstance(result.data, da.Array)\n    assert result.chunks == times.chunks\n    assert_equal(result.compute(), expected)\n\n\n@requires_cftime\n@requires_dask\n@pytest.mark.parametrize(\n    ""field"", [""year"", ""month"", ""day"", ""hour"", ""dayofyear"", ""dayofweek""]\n)\ndef test_dask_field_access(times_3d, data, field):\n    import dask.array as da\n\n    if field == ""dayofyear"" or field == ""dayofweek"":\n        pytest.importorskip(""cftime"", minversion=""1.0.2.1"")\n    expected = xr.DataArray(\n        getattr(\n            xr.coding.cftimeindex.CFTimeIndex(times_3d.values.ravel()), field\n        ).reshape(times_3d.shape),\n        name=field,\n        coords=times_3d.coords,\n        dims=times_3d.dims,\n    )\n    times_3d = times_3d.chunk({""lon"": 5, ""lat"": 5, ""time"": 50})\n    result = getattr(times_3d.dt, field)\n    assert isinstance(result.data, da.Array)\n    assert result.chunks == times_3d.chunks\n    assert_equal(result.compute(), expected)\n\n\n@pytest.fixture()\ndef cftime_date_type(calendar):\n    from .test_coding_times import _all_cftime_date_types\n\n    return _all_cftime_date_types()[calendar]\n\n\n@requires_cftime\ndef test_seasons(cftime_date_type):\n    dates = np.array([cftime_date_type(2000, month, 15) for month in range(1, 13)])\n    dates = xr.DataArray(dates)\n    seasons = [\n        ""DJF"",\n        ""DJF"",\n        ""MAM"",\n        ""MAM"",\n        ""MAM"",\n        ""JJA"",\n        ""JJA"",\n        ""JJA"",\n        ""SON"",\n        ""SON"",\n        ""SON"",\n        ""DJF"",\n    ]\n    seasons = xr.DataArray(seasons)\n\n    assert_array_equal(seasons.values, dates.dt.season.values)\n\n\n@pytest.fixture\ndef cftime_rounding_dataarray(cftime_date_type):\n    return xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 1), cftime_date_type(1, 1, 1, 15)],\n            [cftime_date_type(1, 1, 1, 23), cftime_date_type(1, 1, 2, 1)],\n        ]\n    )\n\n\n@requires_cftime\n@requires_dask\n@pytest.mark.parametrize(""use_dask"", [False, True])\ndef test_cftime_floor_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    freq = ""D""\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 1, 0)],\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=""floor"",\n    )\n\n    if use_dask:\n        chunks = {""dim_0"": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.floor(freq)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.floor(freq)\n\n    assert_identical(result, expected)\n\n\n@requires_cftime\n@requires_dask\n@pytest.mark.parametrize(""use_dask"", [False, True])\ndef test_cftime_ceil_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    freq = ""D""\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 2, 0), cftime_date_type(1, 1, 2, 0)],\n            [cftime_date_type(1, 1, 2, 0), cftime_date_type(1, 1, 3, 0)],\n        ],\n        name=""ceil"",\n    )\n\n    if use_dask:\n        chunks = {""dim_0"": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.ceil(freq)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.ceil(freq)\n\n    assert_identical(result, expected)\n\n\n@requires_cftime\n@requires_dask\n@pytest.mark.parametrize(""use_dask"", [False, True])\ndef test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    freq = ""D""\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n            [cftime_date_type(1, 1, 2, 0), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=""round"",\n    )\n\n    if use_dask:\n        chunks = {""dim_0"": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.round(freq)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.round(freq)\n\n    assert_identical(result, expected)\n'"
xarray/tests/test_accessor_str.py,5,"b'# Tests for the `str` accessor are derived from the original\n# pandas string accessor tests.\n\n# For reference, here is a copy of the pandas copyright notice:\n\n# (c) 2011-2012, Lambda Foundry, Inc. and PyData Development Team\n# All rights reserved.\n\n# Copyright (c) 2008-2011 AQR Capital Management, LLC\n# All rights reserved.\n\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n\n#     * Redistributions of source code must retain the above copyright\n#        notice, this list of conditions and the following disclaimer.\n\n#     * Redistributions in binary form must reproduce the above\n#        copyright notice, this list of conditions and the following\n#        disclaimer in the documentation and/or other materials provided\n#        with the distribution.\n\n#     * Neither the name of the copyright holder nor the names of any\n#        contributors may be used to endorse or promote products derived\n#        from this software without specific prior written permission.\n\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER AND CONTRIBUTORS\n# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport re\n\nimport numpy as np\nimport pytest\n\nimport xarray as xr\n\nfrom . import assert_equal, requires_dask\n\n\n@pytest.fixture(params=[np.str_, np.bytes_])\ndef dtype(request):\n    return request.param\n\n\n@requires_dask\ndef test_dask():\n    import dask.array as da\n\n    arr = da.from_array([""a"", ""b"", ""c""], chunks=-1)\n    xarr = xr.DataArray(arr)\n\n    result = xarr.str.len().compute()\n    expected = xr.DataArray([1, 1, 1])\n    assert_equal(result, expected)\n\n\ndef test_count(dtype):\n    values = xr.DataArray([""foo"", ""foofoo"", ""foooofooofommmfoo""]).astype(dtype)\n    result = values.str.count(""f[o]+"")\n    expected = xr.DataArray([1, 2, 4])\n    assert_equal(result, expected)\n\n\ndef test_contains(dtype):\n    values = xr.DataArray([""Foo"", ""xYz"", ""fOOomMm__fOo"", ""MMM_""]).astype(dtype)\n    # case insensitive using regex\n    result = values.str.contains(""FOO|mmm"", case=False)\n    expected = xr.DataArray([True, False, True, True])\n    assert_equal(result, expected)\n    # case insensitive without regex\n    result = values.str.contains(""foo"", regex=False, case=False)\n    expected = xr.DataArray([True, False, True, False])\n    assert_equal(result, expected)\n\n\ndef test_starts_ends_with(dtype):\n    values = xr.DataArray([""om"", ""foo_nom"", ""nom"", ""bar_foo"", ""foo""]).astype(dtype)\n    result = values.str.startswith(""foo"")\n    expected = xr.DataArray([False, True, False, False, True])\n    assert_equal(result, expected)\n    result = values.str.endswith(""foo"")\n    expected = xr.DataArray([False, False, False, True, True])\n    assert_equal(result, expected)\n\n\ndef test_case(dtype):\n    da = xr.DataArray([""SOme word""]).astype(dtype)\n    capitalized = xr.DataArray([""Some word""]).astype(dtype)\n    lowered = xr.DataArray([""some word""]).astype(dtype)\n    swapped = xr.DataArray([""soME WORD""]).astype(dtype)\n    titled = xr.DataArray([""Some Word""]).astype(dtype)\n    uppered = xr.DataArray([""SOME WORD""]).astype(dtype)\n    assert_equal(da.str.capitalize(), capitalized)\n    assert_equal(da.str.lower(), lowered)\n    assert_equal(da.str.swapcase(), swapped)\n    assert_equal(da.str.title(), titled)\n    assert_equal(da.str.upper(), uppered)\n\n\ndef test_replace(dtype):\n    values = xr.DataArray([""fooBAD__barBAD""]).astype(dtype)\n    result = values.str.replace(""BAD[_]*"", """")\n    expected = xr.DataArray([""foobar""]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str.replace(""BAD[_]*"", """", n=1)\n    expected = xr.DataArray([""foobarBAD""]).astype(dtype)\n    assert_equal(result, expected)\n\n    s = xr.DataArray([""A"", ""B"", ""C"", ""Aaba"", ""Baca"", """", ""CABA"", ""dog"", ""cat""]).astype(\n        dtype\n    )\n    result = s.str.replace(""A"", ""YYY"")\n    expected = xr.DataArray(\n        [""YYY"", ""B"", ""C"", ""YYYaba"", ""Baca"", """", ""CYYYBYYY"", ""dog"", ""cat""]\n    ).astype(dtype)\n    assert_equal(result, expected)\n\n    result = s.str.replace(""A"", ""YYY"", case=False)\n    expected = xr.DataArray(\n        [""YYY"", ""B"", ""C"", ""YYYYYYbYYY"", ""BYYYcYYY"", """", ""CYYYBYYY"", ""dog"", ""cYYYt""]\n    ).astype(dtype)\n    assert_equal(result, expected)\n\n    result = s.str.replace(""^.a|dog"", ""XX-XX "", case=False)\n    expected = xr.DataArray(\n        [""A"", ""B"", ""C"", ""XX-XX ba"", ""XX-XX ca"", """", ""XX-XX BA"", ""XX-XX "", ""XX-XX t""]\n    ).astype(dtype)\n    assert_equal(result, expected)\n\n\ndef test_replace_callable():\n    values = xr.DataArray([""fooBAD__barBAD""])\n    # test with callable\n    repl = lambda m: m.group(0).swapcase()\n    result = values.str.replace(""[a-z][A-Z]{2}"", repl, n=2)\n    exp = xr.DataArray([""foObaD__baRbaD""])\n    assert_equal(result, exp)\n    # test regex named groups\n    values = xr.DataArray([""Foo Bar Baz""])\n    pat = r""(?P<first>\\w+) (?P<middle>\\w+) (?P<last>\\w+)""\n    repl = lambda m: m.group(""middle"").swapcase()\n    result = values.str.replace(pat, repl)\n    exp = xr.DataArray([""bAR""])\n    assert_equal(result, exp)\n\n\ndef test_replace_unicode():\n    # flags + unicode\n    values = xr.DataArray([b""abcd,\\xc3\\xa0"".decode(""utf-8"")])\n    expected = xr.DataArray([b""abcd, \\xc3\\xa0"".decode(""utf-8"")])\n    pat = re.compile(r""(?<=\\w),(?=\\w)"", flags=re.UNICODE)\n    result = values.str.replace(pat, "", "")\n    assert_equal(result, expected)\n\n\ndef test_replace_compiled_regex(dtype):\n    values = xr.DataArray([""fooBAD__barBAD""]).astype(dtype)\n    # test with compiled regex\n    pat = re.compile(dtype(""BAD[_]*""))\n    result = values.str.replace(pat, """")\n    expected = xr.DataArray([""foobar""]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str.replace(pat, """", n=1)\n    expected = xr.DataArray([""foobarBAD""]).astype(dtype)\n    assert_equal(result, expected)\n\n    # case and flags provided to str.replace will have no effect\n    # and will produce warnings\n    values = xr.DataArray([""fooBAD__barBAD__bad""]).astype(dtype)\n    pat = re.compile(dtype(""BAD[_]*""))\n\n    with pytest.raises(ValueError, match=""case and flags cannot be""):\n        result = values.str.replace(pat, """", flags=re.IGNORECASE)\n\n    with pytest.raises(ValueError, match=""case and flags cannot be""):\n        result = values.str.replace(pat, """", case=False)\n\n    with pytest.raises(ValueError, match=""case and flags cannot be""):\n        result = values.str.replace(pat, """", case=True)\n\n    # test with callable\n    values = xr.DataArray([""fooBAD__barBAD""]).astype(dtype)\n    repl = lambda m: m.group(0).swapcase()\n    pat = re.compile(dtype(""[a-z][A-Z]{2}""))\n    result = values.str.replace(pat, repl, n=2)\n    expected = xr.DataArray([""foObaD__baRbaD""]).astype(dtype)\n    assert_equal(result, expected)\n\n\ndef test_replace_literal(dtype):\n    # GH16808 literal replace (regex=False vs regex=True)\n    values = xr.DataArray([""f.o"", ""foo""]).astype(dtype)\n    expected = xr.DataArray([""bao"", ""bao""]).astype(dtype)\n    result = values.str.replace(""f."", ""ba"")\n    assert_equal(result, expected)\n\n    expected = xr.DataArray([""bao"", ""foo""]).astype(dtype)\n    result = values.str.replace(""f."", ""ba"", regex=False)\n    assert_equal(result, expected)\n\n    # Cannot do a literal replace if given a callable repl or compiled\n    # pattern\n    callable_repl = lambda m: m.group(0).swapcase()\n    compiled_pat = re.compile(""[a-z][A-Z]{2}"")\n\n    msg = ""Cannot use a callable replacement when regex=False""\n    with pytest.raises(ValueError, match=msg):\n        values.str.replace(""abc"", callable_repl, regex=False)\n\n    msg = ""Cannot use a compiled regex as replacement pattern with regex=False""\n    with pytest.raises(ValueError, match=msg):\n        values.str.replace(compiled_pat, """", regex=False)\n\n\ndef test_repeat(dtype):\n    values = xr.DataArray([""a"", ""b"", ""c"", ""d""]).astype(dtype)\n    result = values.str.repeat(3)\n    expected = xr.DataArray([""aaa"", ""bbb"", ""ccc"", ""ddd""]).astype(dtype)\n    assert_equal(result, expected)\n\n\ndef test_match(dtype):\n    # New match behavior introduced in 0.13\n    values = xr.DataArray([""fooBAD__barBAD"", ""foo""]).astype(dtype)\n    result = values.str.match("".*(BAD[_]+).*(BAD)"")\n    expected = xr.DataArray([True, False])\n    assert_equal(result, expected)\n\n    values = xr.DataArray([""fooBAD__barBAD"", ""foo""]).astype(dtype)\n    result = values.str.match("".*BAD[_]+.*BAD"")\n    expected = xr.DataArray([True, False])\n    assert_equal(result, expected)\n\n\ndef test_empty_str_methods():\n    empty = xr.DataArray(np.empty(shape=(0,), dtype=""U""))\n    empty_str = empty\n    empty_int = xr.DataArray(np.empty(shape=(0,), dtype=int))\n    empty_bool = xr.DataArray(np.empty(shape=(0,), dtype=bool))\n    empty_bytes = xr.DataArray(np.empty(shape=(0,), dtype=""S""))\n\n    assert_equal(empty_str, empty.str.title())\n    assert_equal(empty_int, empty.str.count(""a""))\n    assert_equal(empty_bool, empty.str.contains(""a""))\n    assert_equal(empty_bool, empty.str.startswith(""a""))\n    assert_equal(empty_bool, empty.str.endswith(""a""))\n    assert_equal(empty_str, empty.str.lower())\n    assert_equal(empty_str, empty.str.upper())\n    assert_equal(empty_str, empty.str.replace(""a"", ""b""))\n    assert_equal(empty_str, empty.str.repeat(3))\n    assert_equal(empty_bool, empty.str.match(""^a""))\n    assert_equal(empty_int, empty.str.len())\n    assert_equal(empty_int, empty.str.find(""a""))\n    assert_equal(empty_int, empty.str.rfind(""a""))\n    assert_equal(empty_str, empty.str.pad(42))\n    assert_equal(empty_str, empty.str.center(42))\n    assert_equal(empty_str, empty.str.slice(stop=1))\n    assert_equal(empty_str, empty.str.slice(step=1))\n    assert_equal(empty_str, empty.str.strip())\n    assert_equal(empty_str, empty.str.lstrip())\n    assert_equal(empty_str, empty.str.rstrip())\n    assert_equal(empty_str, empty.str.wrap(42))\n    assert_equal(empty_str, empty.str.get(0))\n    assert_equal(empty_str, empty_bytes.str.decode(""ascii""))\n    assert_equal(empty_bytes, empty.str.encode(""ascii""))\n    assert_equal(empty_str, empty.str.isalnum())\n    assert_equal(empty_str, empty.str.isalpha())\n    assert_equal(empty_str, empty.str.isdigit())\n    assert_equal(empty_str, empty.str.isspace())\n    assert_equal(empty_str, empty.str.islower())\n    assert_equal(empty_str, empty.str.isupper())\n    assert_equal(empty_str, empty.str.istitle())\n    assert_equal(empty_str, empty.str.isnumeric())\n    assert_equal(empty_str, empty.str.isdecimal())\n    assert_equal(empty_str, empty.str.capitalize())\n    assert_equal(empty_str, empty.str.swapcase())\n    table = str.maketrans(""a"", ""b"")\n    assert_equal(empty_str, empty.str.translate(table))\n\n\ndef test_ismethods(dtype):\n    values = [""A"", ""b"", ""Xy"", ""4"", ""3A"", """", ""TT"", ""55"", ""-"", ""  ""]\n    str_s = xr.DataArray(values).astype(dtype)\n    alnum_e = [True, True, True, True, True, False, True, True, False, False]\n    alpha_e = [True, True, True, False, False, False, True, False, False, False]\n    digit_e = [False, False, False, True, False, False, False, True, False, False]\n    space_e = [False, False, False, False, False, False, False, False, False, True]\n    lower_e = [False, True, False, False, False, False, False, False, False, False]\n    upper_e = [True, False, False, False, True, False, True, False, False, False]\n    title_e = [True, False, True, False, True, False, False, False, False, False]\n\n    assert_equal(str_s.str.isalnum(), xr.DataArray(alnum_e))\n    assert_equal(str_s.str.isalpha(), xr.DataArray(alpha_e))\n    assert_equal(str_s.str.isdigit(), xr.DataArray(digit_e))\n    assert_equal(str_s.str.isspace(), xr.DataArray(space_e))\n    assert_equal(str_s.str.islower(), xr.DataArray(lower_e))\n    assert_equal(str_s.str.isupper(), xr.DataArray(upper_e))\n    assert_equal(str_s.str.istitle(), xr.DataArray(title_e))\n\n\ndef test_isnumeric():\n    # 0x00bc: \xc2\xbc VULGAR FRACTION ONE QUARTER\n    # 0x2605: \xe2\x98\x85 not number\n    # 0x1378: \xe1\x8d\xb8 ETHIOPIC NUMBER SEVENTY\n    # 0xFF13: \xef\xbc\x93 Em 3\n    values = [""A"", ""3"", ""\xc2\xbc"", ""\xe2\x98\x85"", ""\xe1\x8d\xb8"", ""\xef\xbc\x93"", ""four""]\n    s = xr.DataArray(values)\n    numeric_e = [False, True, True, False, True, True, False]\n    decimal_e = [False, True, False, False, False, True, False]\n    assert_equal(s.str.isnumeric(), xr.DataArray(numeric_e))\n    assert_equal(s.str.isdecimal(), xr.DataArray(decimal_e))\n\n\ndef test_len(dtype):\n    values = [""foo"", ""fooo"", ""fooooo"", ""fooooooo""]\n    result = xr.DataArray(values).astype(dtype).str.len()\n    expected = xr.DataArray([len(x) for x in values])\n    assert_equal(result, expected)\n\n\ndef test_find(dtype):\n    values = xr.DataArray([""ABCDEFG"", ""BCDEFEF"", ""DEFGHIJEF"", ""EFGHEF"", ""XXX""])\n    values = values.astype(dtype)\n    result = values.str.find(""EF"")\n    assert_equal(result, xr.DataArray([4, 3, 1, 0, -1]))\n    expected = xr.DataArray([v.find(dtype(""EF"")) for v in values.values])\n    assert_equal(result, expected)\n\n    result = values.str.rfind(""EF"")\n    assert_equal(result, xr.DataArray([4, 5, 7, 4, -1]))\n    expected = xr.DataArray([v.rfind(dtype(""EF"")) for v in values.values])\n    assert_equal(result, expected)\n\n    result = values.str.find(""EF"", 3)\n    assert_equal(result, xr.DataArray([4, 3, 7, 4, -1]))\n    expected = xr.DataArray([v.find(dtype(""EF""), 3) for v in values.values])\n    assert_equal(result, expected)\n\n    result = values.str.rfind(""EF"", 3)\n    assert_equal(result, xr.DataArray([4, 5, 7, 4, -1]))\n    expected = xr.DataArray([v.rfind(dtype(""EF""), 3) for v in values.values])\n    assert_equal(result, expected)\n\n    result = values.str.find(""EF"", 3, 6)\n    assert_equal(result, xr.DataArray([4, 3, -1, 4, -1]))\n    expected = xr.DataArray([v.find(dtype(""EF""), 3, 6) for v in values.values])\n    assert_equal(result, expected)\n\n    result = values.str.rfind(""EF"", 3, 6)\n    assert_equal(result, xr.DataArray([4, 3, -1, 4, -1]))\n    xp = xr.DataArray([v.rfind(dtype(""EF""), 3, 6) for v in values.values])\n    assert_equal(result, xp)\n\n\ndef test_index(dtype):\n    s = xr.DataArray([""ABCDEFG"", ""BCDEFEF"", ""DEFGHIJEF"", ""EFGHEF""]).astype(dtype)\n\n    result = s.str.index(""EF"")\n    assert_equal(result, xr.DataArray([4, 3, 1, 0]))\n\n    result = s.str.rindex(""EF"")\n    assert_equal(result, xr.DataArray([4, 5, 7, 4]))\n\n    result = s.str.index(""EF"", 3)\n    assert_equal(result, xr.DataArray([4, 3, 7, 4]))\n\n    result = s.str.rindex(""EF"", 3)\n    assert_equal(result, xr.DataArray([4, 5, 7, 4]))\n\n    result = s.str.index(""E"", 4, 8)\n    assert_equal(result, xr.DataArray([4, 5, 7, 4]))\n\n    result = s.str.rindex(""E"", 0, 5)\n    assert_equal(result, xr.DataArray([4, 3, 1, 4]))\n\n    with pytest.raises(ValueError):\n        result = s.str.index(""DE"")\n\n\ndef test_pad(dtype):\n    values = xr.DataArray([""a"", ""b"", ""c"", ""eeeee""]).astype(dtype)\n\n    result = values.str.pad(5, side=""left"")\n    expected = xr.DataArray([""    a"", ""    b"", ""    c"", ""eeeee""]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str.pad(5, side=""right"")\n    expected = xr.DataArray([""a    "", ""b    "", ""c    "", ""eeeee""]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str.pad(5, side=""both"")\n    expected = xr.DataArray([""  a  "", ""  b  "", ""  c  "", ""eeeee""]).astype(dtype)\n    assert_equal(result, expected)\n\n\ndef test_pad_fillchar(dtype):\n    values = xr.DataArray([""a"", ""b"", ""c"", ""eeeee""]).astype(dtype)\n\n    result = values.str.pad(5, side=""left"", fillchar=""X"")\n    expected = xr.DataArray([""XXXXa"", ""XXXXb"", ""XXXXc"", ""eeeee""]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str.pad(5, side=""right"", fillchar=""X"")\n    expected = xr.DataArray([""aXXXX"", ""bXXXX"", ""cXXXX"", ""eeeee""]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str.pad(5, side=""both"", fillchar=""X"")\n    expected = xr.DataArray([""XXaXX"", ""XXbXX"", ""XXcXX"", ""eeeee""]).astype(dtype)\n    assert_equal(result, expected)\n\n    msg = ""fillchar must be a character, not str""\n    with pytest.raises(TypeError, match=msg):\n        result = values.str.pad(5, fillchar=""XY"")\n\n\ndef test_translate():\n    values = xr.DataArray([""abcdefg"", ""abcc"", ""cdddfg"", ""cdefggg""])\n    table = str.maketrans(""abc"", ""cde"")\n    result = values.str.translate(table)\n    expected = xr.DataArray([""cdedefg"", ""cdee"", ""edddfg"", ""edefggg""])\n    assert_equal(result, expected)\n\n\ndef test_center_ljust_rjust(dtype):\n    values = xr.DataArray([""a"", ""b"", ""c"", ""eeeee""]).astype(dtype)\n\n    result = values.str.center(5)\n    expected = xr.DataArray([""  a  "", ""  b  "", ""  c  "", ""eeeee""]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str.ljust(5)\n    expected = xr.DataArray([""a    "", ""b    "", ""c    "", ""eeeee""]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str.rjust(5)\n    expected = xr.DataArray([""    a"", ""    b"", ""    c"", ""eeeee""]).astype(dtype)\n    assert_equal(result, expected)\n\n\ndef test_center_ljust_rjust_fillchar(dtype):\n    values = xr.DataArray([""a"", ""bb"", ""cccc"", ""ddddd"", ""eeeeee""]).astype(dtype)\n    result = values.str.center(5, fillchar=""X"")\n    expected = xr.DataArray([""XXaXX"", ""XXbbX"", ""Xcccc"", ""ddddd"", ""eeeeee""])\n    assert_equal(result, expected.astype(dtype))\n\n    result = values.str.ljust(5, fillchar=""X"")\n    expected = xr.DataArray([""aXXXX"", ""bbXXX"", ""ccccX"", ""ddddd"", ""eeeeee""])\n    assert_equal(result, expected.astype(dtype))\n\n    result = values.str.rjust(5, fillchar=""X"")\n    expected = xr.DataArray([""XXXXa"", ""XXXbb"", ""Xcccc"", ""ddddd"", ""eeeeee""])\n    assert_equal(result, expected.astype(dtype))\n\n    # If fillchar is not a charatter, normal str raises TypeError\n    # \'aaa\'.ljust(5, \'XY\')\n    # TypeError: must be char, not str\n    template = ""fillchar must be a character, not {dtype}""\n\n    with pytest.raises(TypeError, match=template.format(dtype=""str"")):\n        values.str.center(5, fillchar=""XY"")\n\n    with pytest.raises(TypeError, match=template.format(dtype=""str"")):\n        values.str.ljust(5, fillchar=""XY"")\n\n    with pytest.raises(TypeError, match=template.format(dtype=""str"")):\n        values.str.rjust(5, fillchar=""XY"")\n\n\ndef test_zfill(dtype):\n    values = xr.DataArray([""1"", ""22"", ""aaa"", ""333"", ""45678""]).astype(dtype)\n\n    result = values.str.zfill(5)\n    expected = xr.DataArray([""00001"", ""00022"", ""00aaa"", ""00333"", ""45678""])\n    assert_equal(result, expected.astype(dtype))\n\n    result = values.str.zfill(3)\n    expected = xr.DataArray([""001"", ""022"", ""aaa"", ""333"", ""45678""])\n    assert_equal(result, expected.astype(dtype))\n\n\ndef test_slice(dtype):\n    arr = xr.DataArray([""aafootwo"", ""aabartwo"", ""aabazqux""]).astype(dtype)\n\n    result = arr.str.slice(2, 5)\n    exp = xr.DataArray([""foo"", ""bar"", ""baz""]).astype(dtype)\n    assert_equal(result, exp)\n\n    for start, stop, step in [(0, 3, -1), (None, None, -1), (3, 10, 2), (3, 0, -1)]:\n        try:\n            result = arr.str[start:stop:step]\n            expected = xr.DataArray([s[start:stop:step] for s in arr.values])\n            assert_equal(result, expected.astype(dtype))\n        except IndexError:\n            print(f""failed on {start}:{stop}:{step}"")\n            raise\n\n\ndef test_slice_replace(dtype):\n    da = lambda x: xr.DataArray(x).astype(dtype)\n    values = da([""short"", ""a bit longer"", ""evenlongerthanthat"", """"])\n\n    expected = da([""shrt"", ""a it longer"", ""evnlongerthanthat"", """"])\n    result = values.str.slice_replace(2, 3)\n    assert_equal(result, expected)\n\n    expected = da([""shzrt"", ""a zit longer"", ""evznlongerthanthat"", ""z""])\n    result = values.str.slice_replace(2, 3, ""z"")\n    assert_equal(result, expected)\n\n    expected = da([""shzort"", ""a zbit longer"", ""evzenlongerthanthat"", ""z""])\n    result = values.str.slice_replace(2, 2, ""z"")\n    assert_equal(result, expected)\n\n    expected = da([""shzort"", ""a zbit longer"", ""evzenlongerthanthat"", ""z""])\n    result = values.str.slice_replace(2, 1, ""z"")\n    assert_equal(result, expected)\n\n    expected = da([""shorz"", ""a bit longez"", ""evenlongerthanthaz"", ""z""])\n    result = values.str.slice_replace(-1, None, ""z"")\n    assert_equal(result, expected)\n\n    expected = da([""zrt"", ""zer"", ""zat"", ""z""])\n    result = values.str.slice_replace(None, -2, ""z"")\n    assert_equal(result, expected)\n\n    expected = da([""shortz"", ""a bit znger"", ""evenlozerthanthat"", ""z""])\n    result = values.str.slice_replace(6, 8, ""z"")\n    assert_equal(result, expected)\n\n    expected = da([""zrt"", ""a zit longer"", ""evenlongzerthanthat"", ""z""])\n    result = values.str.slice_replace(-10, 3, ""z"")\n    assert_equal(result, expected)\n\n\ndef test_strip_lstrip_rstrip(dtype):\n    values = xr.DataArray([""  aa   "", "" bb \\n"", ""cc  ""]).astype(dtype)\n\n    result = values.str.strip()\n    expected = xr.DataArray([""aa"", ""bb"", ""cc""]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str.lstrip()\n    expected = xr.DataArray([""aa   "", ""bb \\n"", ""cc  ""]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str.rstrip()\n    expected = xr.DataArray([""  aa"", "" bb"", ""cc""]).astype(dtype)\n    assert_equal(result, expected)\n\n\ndef test_strip_lstrip_rstrip_args(dtype):\n    values = xr.DataArray([""xxABCxx"", ""xx BNSD"", ""LDFJH xx""]).astype(dtype)\n\n    rs = values.str.strip(""x"")\n    xp = xr.DataArray([""ABC"", "" BNSD"", ""LDFJH ""]).astype(dtype)\n    assert_equal(rs, xp)\n\n    rs = values.str.lstrip(""x"")\n    xp = xr.DataArray([""ABCxx"", "" BNSD"", ""LDFJH xx""]).astype(dtype)\n    assert_equal(rs, xp)\n\n    rs = values.str.rstrip(""x"")\n    xp = xr.DataArray([""xxABC"", ""xx BNSD"", ""LDFJH ""]).astype(dtype)\n    assert_equal(rs, xp)\n\n\ndef test_wrap():\n    # test values are: two words less than width, two words equal to width,\n    # two words greater than width, one word less than width, one word\n    # equal to width, one word greater than width, multiple tokens with\n    # trailing whitespace equal to width\n    values = xr.DataArray(\n        [\n            ""hello world"",\n            ""hello world!"",\n            ""hello world!!"",\n            ""abcdefabcde"",\n            ""abcdefabcdef"",\n            ""abcdefabcdefa"",\n            ""ab ab ab ab "",\n            ""ab ab ab ab a"",\n            ""\\t"",\n        ]\n    )\n\n    # expected values\n    xp = xr.DataArray(\n        [\n            ""hello world"",\n            ""hello world!"",\n            ""hello\\nworld!!"",\n            ""abcdefabcde"",\n            ""abcdefabcdef"",\n            ""abcdefabcdef\\na"",\n            ""ab ab ab ab"",\n            ""ab ab ab ab\\na"",\n            """",\n        ]\n    )\n\n    rs = values.str.wrap(12, break_long_words=True)\n    assert_equal(rs, xp)\n\n    # test with pre and post whitespace (non-unicode), NaN, and non-ascii\n    # Unicode\n    values = xr.DataArray([""  pre  "", ""\\xac\\u20ac\\U00008000 abadcafe""])\n    xp = xr.DataArray([""  pre"", ""\\xac\\u20ac\\U00008000 ab\\nadcafe""])\n    rs = values.str.wrap(6)\n    assert_equal(rs, xp)\n\n\ndef test_get(dtype):\n    values = xr.DataArray([""a_b_c"", ""c_d_e"", ""f_g_h""]).astype(dtype)\n\n    result = values.str[2]\n    expected = xr.DataArray([""b"", ""d"", ""g""]).astype(dtype)\n    assert_equal(result, expected)\n\n    # bounds testing\n    values = xr.DataArray([""1_2_3_4_5"", ""6_7_8_9_10"", ""11_12""]).astype(dtype)\n\n    # positive index\n    result = values.str[5]\n    expected = xr.DataArray([""_"", ""_"", """"]).astype(dtype)\n    assert_equal(result, expected)\n\n    # negative index\n    result = values.str[-6]\n    expected = xr.DataArray([""_"", ""8"", """"]).astype(dtype)\n    assert_equal(result, expected)\n\n\ndef test_encode_decode():\n    data = xr.DataArray([""a"", ""b"", ""a\\xe4""])\n    encoded = data.str.encode(""utf-8"")\n    decoded = encoded.str.decode(""utf-8"")\n    assert_equal(data, decoded)\n\n\ndef test_encode_decode_errors():\n    encodeBase = xr.DataArray([""a"", ""b"", ""a\\x9d""])\n\n    msg = (\n        r""\'charmap\' codec can\'t encode character \'\\\\x9d\' in position 1:""\n        "" character maps to <undefined>""\n    )\n    with pytest.raises(UnicodeEncodeError, match=msg):\n        encodeBase.str.encode(""cp1252"")\n\n    f = lambda x: x.encode(""cp1252"", ""ignore"")\n    result = encodeBase.str.encode(""cp1252"", ""ignore"")\n    expected = xr.DataArray([f(x) for x in encodeBase.values.tolist()])\n    assert_equal(result, expected)\n\n    decodeBase = xr.DataArray([b""a"", b""b"", b""a\\x9d""])\n\n    msg = (\n        ""\'charmap\' codec can\'t decode byte 0x9d in position 1:""\n        "" character maps to <undefined>""\n    )\n    with pytest.raises(UnicodeDecodeError, match=msg):\n        decodeBase.str.decode(""cp1252"")\n\n    f = lambda x: x.decode(""cp1252"", ""ignore"")\n    result = decodeBase.str.decode(""cp1252"", ""ignore"")\n    expected = xr.DataArray([f(x) for x in decodeBase.values.tolist()])\n    assert_equal(result, expected)\n'"
xarray/tests/test_backends.py,178,"b'import contextlib\nimport itertools\nimport math\nimport os.path\nimport pickle\nimport shutil\nimport sys\nimport tempfile\nimport warnings\nfrom contextlib import ExitStack\nfrom io import BytesIO\nfrom pathlib import Path\nfrom typing import Optional\n\nimport numpy as np\nimport pandas as pd\nimport pytest\nfrom pandas.errors import OutOfBoundsDatetime\n\nimport xarray as xr\nfrom xarray import (\n    DataArray,\n    Dataset,\n    backends,\n    load_dataarray,\n    load_dataset,\n    open_dataarray,\n    open_dataset,\n    open_mfdataset,\n    save_mfdataset,\n)\nfrom xarray.backends.common import robust_getitem\nfrom xarray.backends.netcdf3 import _nc3_dtype_coercions\nfrom xarray.backends.netCDF4_ import _extract_nc4_variable_encoding\nfrom xarray.backends.pydap_ import PydapDataStore\nfrom xarray.coding.variables import SerializationWarning\nfrom xarray.conventions import encode_dataset_coordinates\nfrom xarray.core import indexing\nfrom xarray.core.options import set_options\nfrom xarray.core.pycompat import dask_array_type\nfrom xarray.tests import LooseVersion, mock\n\nfrom . import (\n    arm_xfail,\n    assert_allclose,\n    assert_array_equal,\n    assert_equal,\n    assert_identical,\n    has_dask,\n    has_netCDF4,\n    has_scipy,\n    network,\n    raises_regex,\n    requires_cfgrib,\n    requires_cftime,\n    requires_dask,\n    requires_h5netcdf,\n    requires_netCDF4,\n    requires_pseudonetcdf,\n    requires_pydap,\n    requires_pynio,\n    requires_rasterio,\n    requires_scipy,\n    requires_scipy_or_netCDF4,\n    requires_zarr,\n)\nfrom .test_coding_times import (\n    _ALL_CALENDARS,\n    _NON_STANDARD_CALENDARS,\n    _STANDARD_CALENDARS,\n)\nfrom .test_dataset import create_append_test_data, create_test_data\n\ntry:\n    import netCDF4 as nc4\nexcept ImportError:\n    pass\n\ntry:\n    import dask\n    import dask.array as da\n\n    dask_version = dask.__version__\nexcept ImportError:\n    # needed for xfailed tests when dask < 2.4.0\n    # remove when min dask > 2.4.0\n    dask_version = ""10.0""\n\nON_WINDOWS = sys.platform == ""win32""\ndefault_value = object()\n\n\ndef open_example_dataset(name, *args, **kwargs):\n    return open_dataset(\n        os.path.join(os.path.dirname(__file__), ""data"", name), *args, **kwargs\n    )\n\n\ndef open_example_mfdataset(names, *args, **kwargs):\n    return open_mfdataset(\n        [os.path.join(os.path.dirname(__file__), ""data"", name) for name in names],\n        *args,\n        **kwargs,\n    )\n\n\ndef create_masked_and_scaled_data():\n    x = np.array([np.nan, np.nan, 10, 10.1, 10.2], dtype=np.float32)\n    encoding = {\n        ""_FillValue"": -1,\n        ""add_offset"": 10,\n        ""scale_factor"": np.float32(0.1),\n        ""dtype"": ""i2"",\n    }\n    return Dataset({""x"": (""t"", x, {}, encoding)})\n\n\ndef create_encoded_masked_and_scaled_data():\n    attributes = {""_FillValue"": -1, ""add_offset"": 10, ""scale_factor"": np.float32(0.1)}\n    return Dataset({""x"": (""t"", np.int16([-1, -1, 0, 1, 2]), attributes)})\n\n\ndef create_unsigned_masked_scaled_data():\n    encoding = {\n        ""_FillValue"": 255,\n        ""_Unsigned"": ""true"",\n        ""dtype"": ""i1"",\n        ""add_offset"": 10,\n        ""scale_factor"": np.float32(0.1),\n    }\n    x = np.array([10.0, 10.1, 22.7, 22.8, np.nan], dtype=np.float32)\n    return Dataset({""x"": (""t"", x, {}, encoding)})\n\n\ndef create_encoded_unsigned_masked_scaled_data():\n    # These are values as written to the file: the _FillValue will\n    # be represented in the signed form.\n    attributes = {\n        ""_FillValue"": -1,\n        ""_Unsigned"": ""true"",\n        ""add_offset"": 10,\n        ""scale_factor"": np.float32(0.1),\n    }\n    # Create unsigned data corresponding to [0, 1, 127, 128, 255] unsigned\n    sb = np.asarray([0, 1, 127, -128, -1], dtype=""i1"")\n    return Dataset({""x"": (""t"", sb, attributes)})\n\n\ndef create_bad_unsigned_masked_scaled_data():\n    encoding = {\n        ""_FillValue"": 255,\n        ""_Unsigned"": True,\n        ""dtype"": ""i1"",\n        ""add_offset"": 10,\n        ""scale_factor"": np.float32(0.1),\n    }\n    x = np.array([10.0, 10.1, 22.7, 22.8, np.nan], dtype=np.float32)\n    return Dataset({""x"": (""t"", x, {}, encoding)})\n\n\ndef create_bad_encoded_unsigned_masked_scaled_data():\n    # These are values as written to the file: the _FillValue will\n    # be represented in the signed form.\n    attributes = {\n        ""_FillValue"": -1,\n        ""_Unsigned"": True,\n        ""add_offset"": 10,\n        ""scale_factor"": np.float32(0.1),\n    }\n    # Create signed data corresponding to [0, 1, 127, 128, 255] unsigned\n    sb = np.asarray([0, 1, 127, -128, -1], dtype=""i1"")\n    return Dataset({""x"": (""t"", sb, attributes)})\n\n\ndef create_signed_masked_scaled_data():\n    encoding = {\n        ""_FillValue"": -127,\n        ""_Unsigned"": ""false"",\n        ""dtype"": ""i1"",\n        ""add_offset"": 10,\n        ""scale_factor"": np.float32(0.1),\n    }\n    x = np.array([-1.0, 10.1, 22.7, np.nan], dtype=np.float32)\n    return Dataset({""x"": (""t"", x, {}, encoding)})\n\n\ndef create_encoded_signed_masked_scaled_data():\n    # These are values as written to the file: the _FillValue will\n    # be represented in the signed form.\n    attributes = {\n        ""_FillValue"": -127,\n        ""_Unsigned"": ""false"",\n        ""add_offset"": 10,\n        ""scale_factor"": np.float32(0.1),\n    }\n    # Create signed data corresponding to [0, 1, 127, 128, 255] unsigned\n    sb = np.asarray([-110, 1, 127, -127], dtype=""i1"")\n    return Dataset({""x"": (""t"", sb, attributes)})\n\n\ndef create_boolean_data():\n    attributes = {""units"": ""-""}\n    return Dataset({""x"": (""t"", [True, False, False, True], attributes)})\n\n\nclass TestCommon:\n    def test_robust_getitem(self):\n        class UnreliableArrayFailure(Exception):\n            pass\n\n        class UnreliableArray:\n            def __init__(self, array, failures=1):\n                self.array = array\n                self.failures = failures\n\n            def __getitem__(self, key):\n                if self.failures > 0:\n                    self.failures -= 1\n                    raise UnreliableArrayFailure\n                return self.array[key]\n\n        array = UnreliableArray([0])\n        with pytest.raises(UnreliableArrayFailure):\n            array[0]\n        assert array[0] == 0\n\n        actual = robust_getitem(array, 0, catch=UnreliableArrayFailure, initial_delay=0)\n        assert actual == 0\n\n\nclass NetCDF3Only:\n    netcdf3_formats = (""NETCDF3_CLASSIC"", ""NETCDF3_64BIT"")\n\n    @requires_scipy\n    def test_dtype_coercion_error(self):\n        """"""Failing dtype coercion should lead to an error""""""\n        for dtype, format in itertools.product(\n            _nc3_dtype_coercions, self.netcdf3_formats\n        ):\n            if dtype == ""bool"":\n                # coerced upcast (bool to int8) ==> can never fail\n                continue\n\n            # Using the largest representable value, create some data that will\n            # no longer compare equal after the coerced downcast\n            maxval = np.iinfo(dtype).max\n            x = np.array([0, 1, 2, maxval], dtype=dtype)\n            ds = Dataset({""x"": (""t"", x, {})})\n\n            with create_tmp_file(allow_cleanup_failure=False) as path:\n                with pytest.raises(ValueError, match=""could not safely cast""):\n                    ds.to_netcdf(path, format=format)\n\n\nclass DatasetIOBase:\n    engine: Optional[str] = None\n    file_format: Optional[str] = None\n\n    def create_store(self):\n        raise NotImplementedError()\n\n    @contextlib.contextmanager\n    def roundtrip(\n        self, data, save_kwargs=None, open_kwargs=None, allow_cleanup_failure=False\n    ):\n        if save_kwargs is None:\n            save_kwargs = {}\n        if open_kwargs is None:\n            open_kwargs = {}\n        with create_tmp_file(allow_cleanup_failure=allow_cleanup_failure) as path:\n            self.save(data, path, **save_kwargs)\n            with self.open(path, **open_kwargs) as ds:\n                yield ds\n\n    @contextlib.contextmanager\n    def roundtrip_append(\n        self, data, save_kwargs=None, open_kwargs=None, allow_cleanup_failure=False\n    ):\n        if save_kwargs is None:\n            save_kwargs = {}\n        if open_kwargs is None:\n            open_kwargs = {}\n        with create_tmp_file(allow_cleanup_failure=allow_cleanup_failure) as path:\n            for i, key in enumerate(data.variables):\n                mode = ""a"" if i > 0 else ""w""\n                self.save(data[[key]], path, mode=mode, **save_kwargs)\n            with self.open(path, **open_kwargs) as ds:\n                yield ds\n\n    # The save/open methods may be overwritten below\n    def save(self, dataset, path, **kwargs):\n        return dataset.to_netcdf(\n            path, engine=self.engine, format=self.file_format, **kwargs\n        )\n\n    @contextlib.contextmanager\n    def open(self, path, **kwargs):\n        with open_dataset(path, engine=self.engine, **kwargs) as ds:\n            yield ds\n\n    def test_zero_dimensional_variable(self):\n        expected = create_test_data()\n        expected[""float_var""] = ([], 1.0e9, {""units"": ""units of awesome""})\n        expected[""bytes_var""] = ([], b""foobar"")\n        expected[""string_var""] = ([], ""foobar"")\n        with self.roundtrip(expected) as actual:\n            assert_identical(expected, actual)\n\n    def test_write_store(self):\n        expected = create_test_data()\n        with self.create_store() as store:\n            expected.dump_to_store(store)\n            # we need to cf decode the store because it has time and\n            # non-dimension coordinates\n            with xr.decode_cf(store) as actual:\n                assert_allclose(expected, actual)\n\n    def check_dtypes_roundtripped(self, expected, actual):\n        for k in expected.variables:\n            expected_dtype = expected.variables[k].dtype\n\n            # For NetCDF3, the backend should perform dtype coercion\n            if (\n                isinstance(self, NetCDF3Only)\n                and str(expected_dtype) in _nc3_dtype_coercions\n            ):\n                expected_dtype = np.dtype(_nc3_dtype_coercions[str(expected_dtype)])\n\n            actual_dtype = actual.variables[k].dtype\n            # TODO: check expected behavior for string dtypes more carefully\n            string_kinds = {""O"", ""S"", ""U""}\n            assert expected_dtype == actual_dtype or (\n                expected_dtype.kind in string_kinds\n                and actual_dtype.kind in string_kinds\n            )\n\n    def test_roundtrip_test_data(self):\n        expected = create_test_data()\n        with self.roundtrip(expected) as actual:\n            self.check_dtypes_roundtripped(expected, actual)\n            assert_identical(expected, actual)\n\n    def test_load(self):\n        expected = create_test_data()\n\n        @contextlib.contextmanager\n        def assert_loads(vars=None):\n            if vars is None:\n                vars = expected\n            with self.roundtrip(expected) as actual:\n                for k, v in actual.variables.items():\n                    # IndexVariables are eagerly loaded into memory\n                    assert v._in_memory == (k in actual.dims)\n                yield actual\n                for k, v in actual.variables.items():\n                    if k in vars:\n                        assert v._in_memory\n                assert_identical(expected, actual)\n\n        with pytest.raises(AssertionError):\n            # make sure the contextmanager works!\n            with assert_loads() as ds:\n                pass\n\n        with assert_loads() as ds:\n            ds.load()\n\n        with assert_loads([""var1"", ""dim1"", ""dim2""]) as ds:\n            ds[""var1""].load()\n\n        # verify we can read data even after closing the file\n        with self.roundtrip(expected) as ds:\n            actual = ds.load()\n        assert_identical(expected, actual)\n\n    def test_dataset_compute(self):\n        expected = create_test_data()\n\n        with self.roundtrip(expected) as actual:\n            # Test Dataset.compute()\n            for k, v in actual.variables.items():\n                # IndexVariables are eagerly cached\n                assert v._in_memory == (k in actual.dims)\n\n            computed = actual.compute()\n\n            for k, v in actual.variables.items():\n                assert v._in_memory == (k in actual.dims)\n            for v in computed.variables.values():\n                assert v._in_memory\n\n            assert_identical(expected, actual)\n            assert_identical(expected, computed)\n\n    def test_pickle(self):\n        if not has_dask:\n            pytest.xfail(""pickling requires dask for SerializableLock"")\n        expected = Dataset({""foo"": (""x"", [42])})\n        with self.roundtrip(expected, allow_cleanup_failure=ON_WINDOWS) as roundtripped:\n            with roundtripped:\n                # Windows doesn\'t like reopening an already open file\n                raw_pickle = pickle.dumps(roundtripped)\n            with pickle.loads(raw_pickle) as unpickled_ds:\n                assert_identical(expected, unpickled_ds)\n\n    @pytest.mark.filterwarnings(""ignore:deallocating CachingFileManager"")\n    def test_pickle_dataarray(self):\n        if not has_dask:\n            pytest.xfail(""pickling requires dask for SerializableLock"")\n        expected = Dataset({""foo"": (""x"", [42])})\n        with self.roundtrip(expected, allow_cleanup_failure=ON_WINDOWS) as roundtripped:\n            with roundtripped:\n                raw_pickle = pickle.dumps(roundtripped[""foo""])\n            # TODO: figure out how to explicitly close the file for the\n            # unpickled DataArray?\n            unpickled = pickle.loads(raw_pickle)\n            assert_identical(expected[""foo""], unpickled)\n\n    def test_dataset_caching(self):\n        expected = Dataset({""foo"": (""x"", [5, 6, 7])})\n        with self.roundtrip(expected) as actual:\n            assert isinstance(actual.foo.variable._data, indexing.MemoryCachedArray)\n            assert not actual.foo.variable._in_memory\n            actual.foo.values  # cache\n            assert actual.foo.variable._in_memory\n\n        with self.roundtrip(expected, open_kwargs={""cache"": False}) as actual:\n            assert isinstance(actual.foo.variable._data, indexing.CopyOnWriteArray)\n            assert not actual.foo.variable._in_memory\n            actual.foo.values  # no caching\n            assert not actual.foo.variable._in_memory\n\n    def test_roundtrip_None_variable(self):\n        expected = Dataset({None: ((""x"", ""y""), [[0, 1], [2, 3]])})\n        with self.roundtrip(expected) as actual:\n            assert_identical(expected, actual)\n\n    def test_roundtrip_object_dtype(self):\n        floats = np.array([0.0, 0.0, 1.0, 2.0, 3.0], dtype=object)\n        floats_nans = np.array([np.nan, np.nan, 1.0, 2.0, 3.0], dtype=object)\n        bytes_ = np.array([b""ab"", b""cdef"", b""g""], dtype=object)\n        bytes_nans = np.array([b""ab"", b""cdef"", np.nan], dtype=object)\n        strings = np.array([""ab"", ""cdef"", ""g""], dtype=object)\n        strings_nans = np.array([""ab"", ""cdef"", np.nan], dtype=object)\n        all_nans = np.array([np.nan, np.nan], dtype=object)\n        original = Dataset(\n            {\n                ""floats"": (""a"", floats),\n                ""floats_nans"": (""a"", floats_nans),\n                ""bytes"": (""b"", bytes_),\n                ""bytes_nans"": (""b"", bytes_nans),\n                ""strings"": (""b"", strings),\n                ""strings_nans"": (""b"", strings_nans),\n                ""all_nans"": (""c"", all_nans),\n                ""nan"": ([], np.nan),\n            }\n        )\n        expected = original.copy(deep=True)\n        with self.roundtrip(original) as actual:\n            try:\n                assert_identical(expected, actual)\n            except AssertionError:\n                # Most stores use \'\' for nans in strings, but some don\'t.\n                # First try the ideal case (where the store returns exactly)\n                # the original Dataset), then try a more realistic case.\n                # This currently includes all netCDF files when encoding is not\n                # explicitly set.\n                # https://github.com/pydata/xarray/issues/1647\n                expected[""bytes_nans""][-1] = b""""\n                expected[""strings_nans""][-1] = """"\n                assert_identical(expected, actual)\n\n    def test_roundtrip_string_data(self):\n        expected = Dataset({""x"": (""t"", [""ab"", ""cdef""])})\n        with self.roundtrip(expected) as actual:\n            assert_identical(expected, actual)\n\n    def test_roundtrip_string_encoded_characters(self):\n        expected = Dataset({""x"": (""t"", [""ab"", ""cdef""])})\n        expected[""x""].encoding[""dtype""] = ""S1""\n        with self.roundtrip(expected) as actual:\n            assert_identical(expected, actual)\n            assert actual[""x""].encoding[""_Encoding""] == ""utf-8""\n\n        expected[""x""].encoding[""_Encoding""] = ""ascii""\n        with self.roundtrip(expected) as actual:\n            assert_identical(expected, actual)\n            assert actual[""x""].encoding[""_Encoding""] == ""ascii""\n\n    @arm_xfail\n    def test_roundtrip_numpy_datetime_data(self):\n        times = pd.to_datetime([""2000-01-01"", ""2000-01-02"", ""NaT""])\n        expected = Dataset({""t"": (""t"", times), ""t0"": times[0]})\n        kwargs = {""encoding"": {""t0"": {""units"": ""days since 1950-01-01""}}}\n        with self.roundtrip(expected, save_kwargs=kwargs) as actual:\n            assert_identical(expected, actual)\n            assert actual.t0.encoding[""units""] == ""days since 1950-01-01""\n\n    @requires_cftime\n    def test_roundtrip_cftime_datetime_data(self):\n        from .test_coding_times import _all_cftime_date_types\n\n        date_types = _all_cftime_date_types()\n        for date_type in date_types.values():\n            times = [date_type(1, 1, 1), date_type(1, 1, 2)]\n            expected = Dataset({""t"": (""t"", times), ""t0"": times[0]})\n            kwargs = {""encoding"": {""t0"": {""units"": ""days since 0001-01-01""}}}\n            expected_decoded_t = np.array(times)\n            expected_decoded_t0 = np.array([date_type(1, 1, 1)])\n            expected_calendar = times[0].calendar\n\n            with warnings.catch_warnings():\n                if expected_calendar in {""proleptic_gregorian"", ""gregorian""}:\n                    warnings.filterwarnings(""ignore"", ""Unable to decode time axis"")\n\n                with self.roundtrip(expected, save_kwargs=kwargs) as actual:\n                    abs_diff = abs(actual.t.values - expected_decoded_t)\n                    assert (abs_diff <= np.timedelta64(1, ""s"")).all()\n                    assert (\n                        actual.t.encoding[""units""]\n                        == ""days since 0001-01-01 00:00:00.000000""\n                    )\n                    assert actual.t.encoding[""calendar""] == expected_calendar\n\n                    abs_diff = abs(actual.t0.values - expected_decoded_t0)\n                    assert (abs_diff <= np.timedelta64(1, ""s"")).all()\n                    assert actual.t0.encoding[""units""] == ""days since 0001-01-01""\n                    assert actual.t.encoding[""calendar""] == expected_calendar\n\n    def test_roundtrip_timedelta_data(self):\n        time_deltas = pd.to_timedelta([""1h"", ""2h"", ""NaT""])\n        expected = Dataset({""td"": (""td"", time_deltas), ""td0"": time_deltas[0]})\n        with self.roundtrip(expected) as actual:\n            assert_identical(expected, actual)\n\n    def test_roundtrip_float64_data(self):\n        expected = Dataset({""x"": (""y"", np.array([1.0, 2.0, np.pi], dtype=""float64""))})\n        with self.roundtrip(expected) as actual:\n            assert_identical(expected, actual)\n\n    def test_roundtrip_example_1_netcdf(self):\n        with open_example_dataset(""example_1.nc"") as expected:\n            with self.roundtrip(expected) as actual:\n                # we allow the attributes to differ since that\n                # will depend on the encoding used.  For example,\n                # without CF encoding \'actual\' will end up with\n                # a dtype attribute.\n                assert_equal(expected, actual)\n\n    def test_roundtrip_coordinates(self):\n        original = Dataset(\n            {""foo"": (""x"", [0, 1])}, {""x"": [2, 3], ""y"": (""a"", [42]), ""z"": (""x"", [4, 5])}\n        )\n\n        with self.roundtrip(original) as actual:\n            assert_identical(original, actual)\n\n        original[""foo""].encoding[""coordinates""] = ""y""\n        with self.roundtrip(original, open_kwargs={""decode_coords"": False}) as expected:\n            # check roundtripping when decode_coords=False\n            with self.roundtrip(\n                expected, open_kwargs={""decode_coords"": False}\n            ) as actual:\n                assert_identical(expected, actual)\n\n    def test_roundtrip_global_coordinates(self):\n        original = Dataset(\n            {""foo"": (""x"", [0, 1])}, {""x"": [2, 3], ""y"": (""a"", [42]), ""z"": (""x"", [4, 5])}\n        )\n        with self.roundtrip(original) as actual:\n            assert_identical(original, actual)\n\n        # test that global ""coordinates"" is as expected\n        _, attrs = encode_dataset_coordinates(original)\n        assert attrs[""coordinates""] == ""y""\n\n        # test warning when global ""coordinates"" is already set\n        original.attrs[""coordinates""] = ""foo""\n        with pytest.warns(SerializationWarning):\n            _, attrs = encode_dataset_coordinates(original)\n            assert attrs[""coordinates""] == ""foo""\n\n    def test_roundtrip_coordinates_with_space(self):\n        original = Dataset(coords={""x"": 0, ""y z"": 1})\n        expected = Dataset({""y z"": 1}, {""x"": 0})\n        with pytest.warns(SerializationWarning):\n            with self.roundtrip(original) as actual:\n                assert_identical(expected, actual)\n\n    def test_roundtrip_boolean_dtype(self):\n        original = create_boolean_data()\n        assert original[""x""].dtype == ""bool""\n        with self.roundtrip(original) as actual:\n            assert_identical(original, actual)\n            assert actual[""x""].dtype == ""bool""\n\n    def test_orthogonal_indexing(self):\n        in_memory = create_test_data()\n        with self.roundtrip(in_memory) as on_disk:\n            indexers = {""dim1"": [1, 2, 0], ""dim2"": [3, 2, 0, 3], ""dim3"": np.arange(5)}\n            expected = in_memory.isel(**indexers)\n            actual = on_disk.isel(**indexers)\n            # make sure the array is not yet loaded into memory\n            assert not actual[""var1""].variable._in_memory\n            assert_identical(expected, actual)\n            # do it twice, to make sure we\'re switched from orthogonal -> numpy\n            # when we cached the values\n            actual = on_disk.isel(**indexers)\n            assert_identical(expected, actual)\n\n    @pytest.mark.xfail(\n        not has_dask,\n        reason=""the code for indexing without dask handles negative steps in slices incorrectly"",\n    )\n    def test_vectorized_indexing(self):\n        in_memory = create_test_data()\n        with self.roundtrip(in_memory) as on_disk:\n            indexers = {\n                ""dim1"": DataArray([0, 2, 0], dims=""a""),\n                ""dim2"": DataArray([0, 2, 3], dims=""a""),\n            }\n            expected = in_memory.isel(**indexers)\n            actual = on_disk.isel(**indexers)\n            # make sure the array is not yet loaded into memory\n            assert not actual[""var1""].variable._in_memory\n            assert_identical(expected, actual.load())\n            # do it twice, to make sure we\'re switched from\n            # vectorized -> numpy when we cached the values\n            actual = on_disk.isel(**indexers)\n            assert_identical(expected, actual)\n\n        def multiple_indexing(indexers):\n            # make sure a sequence of lazy indexings certainly works.\n            with self.roundtrip(in_memory) as on_disk:\n                actual = on_disk[""var3""]\n                expected = in_memory[""var3""]\n                for ind in indexers:\n                    actual = actual.isel(**ind)\n                    expected = expected.isel(**ind)\n                    # make sure the array is not yet loaded into memory\n                    assert not actual.variable._in_memory\n                assert_identical(expected, actual.load())\n\n        # two-staged vectorized-indexing\n        indexers = [\n            {\n                ""dim1"": DataArray([[0, 7], [2, 6], [3, 5]], dims=[""a"", ""b""]),\n                ""dim3"": DataArray([[0, 4], [1, 3], [2, 2]], dims=[""a"", ""b""]),\n            },\n            {""a"": DataArray([0, 1], dims=[""c""]), ""b"": DataArray([0, 1], dims=[""c""])},\n        ]\n        multiple_indexing(indexers)\n\n        # vectorized-slice mixed\n        indexers = [\n            {\n                ""dim1"": DataArray([[0, 7], [2, 6], [3, 5]], dims=[""a"", ""b""]),\n                ""dim3"": slice(None, 10),\n            }\n        ]\n        multiple_indexing(indexers)\n\n        # vectorized-integer mixed\n        indexers = [\n            {""dim3"": 0},\n            {""dim1"": DataArray([[0, 7], [2, 6], [3, 5]], dims=[""a"", ""b""])},\n            {""a"": slice(None, None, 2)},\n        ]\n        multiple_indexing(indexers)\n\n        # vectorized-integer mixed\n        indexers = [\n            {""dim3"": 0},\n            {""dim1"": DataArray([[0, 7], [2, 6], [3, 5]], dims=[""a"", ""b""])},\n            {""a"": 1, ""b"": 0},\n        ]\n        multiple_indexing(indexers)\n\n        # with negative step slice.\n        indexers = [\n            {\n                ""dim1"": DataArray([[0, 7], [2, 6], [3, 5]], dims=[""a"", ""b""]),\n                ""dim3"": slice(-1, 1, -1),\n            }\n        ]\n        multiple_indexing(indexers)\n\n        # with negative step slice.\n        indexers = [\n            {\n                ""dim1"": DataArray([[0, 7], [2, 6], [3, 5]], dims=[""a"", ""b""]),\n                ""dim3"": slice(-1, 1, -2),\n            }\n        ]\n        multiple_indexing(indexers)\n\n    def test_isel_dataarray(self):\n        # Make sure isel works lazily. GH:issue:1688\n        in_memory = create_test_data()\n        with self.roundtrip(in_memory) as on_disk:\n            expected = in_memory.isel(dim2=in_memory[""dim2""] < 3)\n            actual = on_disk.isel(dim2=on_disk[""dim2""] < 3)\n            assert_identical(expected, actual)\n\n    def validate_array_type(self, ds):\n        # Make sure that only NumpyIndexingAdapter stores a bare np.ndarray.\n        def find_and_validate_array(obj):\n            # recursively called function. obj: array or array wrapper.\n            if hasattr(obj, ""array""):\n                if isinstance(obj.array, indexing.ExplicitlyIndexed):\n                    find_and_validate_array(obj.array)\n                else:\n                    if isinstance(obj.array, np.ndarray):\n                        assert isinstance(obj, indexing.NumpyIndexingAdapter)\n                    elif isinstance(obj.array, dask_array_type):\n                        assert isinstance(obj, indexing.DaskIndexingAdapter)\n                    elif isinstance(obj.array, pd.Index):\n                        assert isinstance(obj, indexing.PandasIndexAdapter)\n                    else:\n                        raise TypeError(\n                            ""{} is wrapped by {}"".format(type(obj.array), type(obj))\n                        )\n\n        for k, v in ds.variables.items():\n            find_and_validate_array(v._data)\n\n    def test_array_type_after_indexing(self):\n        in_memory = create_test_data()\n        with self.roundtrip(in_memory) as on_disk:\n            self.validate_array_type(on_disk)\n            indexers = {""dim1"": [1, 2, 0], ""dim2"": [3, 2, 0, 3], ""dim3"": np.arange(5)}\n            expected = in_memory.isel(**indexers)\n            actual = on_disk.isel(**indexers)\n            assert_identical(expected, actual)\n            self.validate_array_type(actual)\n            # do it twice, to make sure we\'re switched from orthogonal -> numpy\n            # when we cached the values\n            actual = on_disk.isel(**indexers)\n            assert_identical(expected, actual)\n            self.validate_array_type(actual)\n\n    def test_dropna(self):\n        # regression test for GH:issue:1694\n        a = np.random.randn(4, 3)\n        a[1, 1] = np.NaN\n        in_memory = xr.Dataset(\n            {""a"": ((""y"", ""x""), a)}, coords={""y"": np.arange(4), ""x"": np.arange(3)}\n        )\n\n        assert_identical(\n            in_memory.dropna(dim=""x""), in_memory.isel(x=slice(None, None, 2))\n        )\n\n        with self.roundtrip(in_memory) as on_disk:\n            self.validate_array_type(on_disk)\n            expected = in_memory.dropna(dim=""x"")\n            actual = on_disk.dropna(dim=""x"")\n            assert_identical(expected, actual)\n\n    def test_ondisk_after_print(self):\n        """""" Make sure print does not load file into memory """"""\n        in_memory = create_test_data()\n        with self.roundtrip(in_memory) as on_disk:\n            repr(on_disk)\n            assert not on_disk[""var1""]._in_memory\n\n\nclass CFEncodedBase(DatasetIOBase):\n    def test_roundtrip_bytes_with_fill_value(self):\n        values = np.array([b""ab"", b""cdef"", np.nan], dtype=object)\n        encoding = {""_FillValue"": b""X"", ""dtype"": ""S1""}\n        original = Dataset({""x"": (""t"", values, {}, encoding)})\n        expected = original.copy(deep=True)\n        with self.roundtrip(original) as actual:\n            assert_identical(expected, actual)\n\n        original = Dataset({""x"": (""t"", values, {}, {""_FillValue"": b""""})})\n        with self.roundtrip(original) as actual:\n            assert_identical(expected, actual)\n\n    def test_roundtrip_string_with_fill_value_nchar(self):\n        values = np.array([""ab"", ""cdef"", np.nan], dtype=object)\n        expected = Dataset({""x"": (""t"", values)})\n\n        encoding = {""dtype"": ""S1"", ""_FillValue"": b""X""}\n        original = Dataset({""x"": (""t"", values, {}, encoding)})\n        # Not supported yet.\n        with pytest.raises(NotImplementedError):\n            with self.roundtrip(original) as actual:\n                assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""decoded_fn, encoded_fn"",\n        [\n            (\n                create_unsigned_masked_scaled_data,\n                create_encoded_unsigned_masked_scaled_data,\n            ),\n            pytest.param(\n                create_bad_unsigned_masked_scaled_data,\n                create_bad_encoded_unsigned_masked_scaled_data,\n                marks=pytest.mark.xfail(reason=""Bad _Unsigned attribute.""),\n            ),\n            (\n                create_signed_masked_scaled_data,\n                create_encoded_signed_masked_scaled_data,\n            ),\n            (create_masked_and_scaled_data, create_encoded_masked_and_scaled_data),\n        ],\n    )\n    def test_roundtrip_mask_and_scale(self, decoded_fn, encoded_fn):\n        decoded = decoded_fn()\n        encoded = encoded_fn()\n\n        with self.roundtrip(decoded) as actual:\n            for k in decoded.variables:\n                assert decoded.variables[k].dtype == actual.variables[k].dtype\n            assert_allclose(decoded, actual, decode_bytes=False)\n\n        with self.roundtrip(decoded, open_kwargs=dict(decode_cf=False)) as actual:\n            # TODO: this assumes that all roundtrips will first\n            # encode.  Is that something we want to test for?\n            for k in encoded.variables:\n                assert encoded.variables[k].dtype == actual.variables[k].dtype\n            assert_allclose(encoded, actual, decode_bytes=False)\n\n        with self.roundtrip(encoded, open_kwargs=dict(decode_cf=False)) as actual:\n            for k in encoded.variables:\n                assert encoded.variables[k].dtype == actual.variables[k].dtype\n            assert_allclose(encoded, actual, decode_bytes=False)\n\n        # make sure roundtrip encoding didn\'t change the\n        # original dataset.\n        assert_allclose(encoded, encoded_fn(), decode_bytes=False)\n\n        with self.roundtrip(encoded) as actual:\n            for k in decoded.variables:\n                assert decoded.variables[k].dtype == actual.variables[k].dtype\n            assert_allclose(decoded, actual, decode_bytes=False)\n\n    def test_coordinates_encoding(self):\n        def equals_latlon(obj):\n            return obj == ""lat lon"" or obj == ""lon lat""\n\n        original = Dataset(\n            {""temp"": (""x"", [0, 1]), ""precip"": (""x"", [0, -1])},\n            {""lat"": (""x"", [2, 3]), ""lon"": (""x"", [4, 5])},\n        )\n        with self.roundtrip(original) as actual:\n            assert_identical(actual, original)\n        with create_tmp_file() as tmp_file:\n            original.to_netcdf(tmp_file)\n            with open_dataset(tmp_file, decode_coords=False) as ds:\n                assert equals_latlon(ds[""temp""].attrs[""coordinates""])\n                assert equals_latlon(ds[""precip""].attrs[""coordinates""])\n                assert ""coordinates"" not in ds.attrs\n                assert ""coordinates"" not in ds[""lat""].attrs\n                assert ""coordinates"" not in ds[""lon""].attrs\n\n        modified = original.drop_vars([""temp"", ""precip""])\n        with self.roundtrip(modified) as actual:\n            assert_identical(actual, modified)\n        with create_tmp_file() as tmp_file:\n            modified.to_netcdf(tmp_file)\n            with open_dataset(tmp_file, decode_coords=False) as ds:\n                assert equals_latlon(ds.attrs[""coordinates""])\n                assert ""coordinates"" not in ds[""lat""].attrs\n                assert ""coordinates"" not in ds[""lon""].attrs\n\n        original[""temp""].encoding[""coordinates""] = ""lat""\n        with self.roundtrip(original) as actual:\n            assert_identical(actual, original)\n        original[""precip""].encoding[""coordinates""] = ""lat""\n        with create_tmp_file() as tmp_file:\n            original.to_netcdf(tmp_file)\n            with open_dataset(tmp_file, decode_coords=True) as ds:\n                assert ""lon"" not in ds[""temp""].encoding[""coordinates""]\n                assert ""lon"" not in ds[""precip""].encoding[""coordinates""]\n                assert ""coordinates"" not in ds[""lat""].encoding\n                assert ""coordinates"" not in ds[""lon""].encoding\n\n    def test_roundtrip_endian(self):\n        ds = Dataset(\n            {\n                ""x"": np.arange(3, 10, dtype="">i2""),\n                ""y"": np.arange(3, 20, dtype=""<i4""),\n                ""z"": np.arange(3, 30, dtype=""=i8""),\n                ""w"": (""x"", np.arange(3, 10, dtype=np.float)),\n            }\n        )\n\n        with self.roundtrip(ds) as actual:\n            # technically these datasets are slightly different,\n            # one hold mixed endian data (ds) the other should be\n            # all big endian (actual).  assertDatasetIdentical\n            # should still pass though.\n            assert_identical(ds, actual)\n\n        if self.engine == ""netcdf4"":\n            ds[""z""].encoding[""endian""] = ""big""\n            with pytest.raises(NotImplementedError):\n                with self.roundtrip(ds) as actual:\n                    pass\n\n    def test_invalid_dataarray_names_raise(self):\n        te = (TypeError, ""string or None"")\n        ve = (ValueError, ""string must be length 1 or"")\n        data = np.random.random((2, 2))\n        da = xr.DataArray(data)\n        for name, e in zip([0, (4, 5), True, """"], [te, te, te, ve]):\n            ds = Dataset({name: da})\n            with raises_regex(*e):\n                with self.roundtrip(ds):\n                    pass\n\n    def test_encoding_kwarg(self):\n        ds = Dataset({""x"": (""y"", np.arange(10.0))})\n        kwargs = dict(encoding={""x"": {""dtype"": ""f4""}})\n        with self.roundtrip(ds, save_kwargs=kwargs) as actual:\n            encoded_dtype = actual.x.encoding[""dtype""]\n            # On OS X, dtype sometimes switches endianness for unclear reasons\n            assert encoded_dtype.kind == ""f"" and encoded_dtype.itemsize == 4\n        assert ds.x.encoding == {}\n\n        kwargs = dict(encoding={""x"": {""foo"": ""bar""}})\n        with raises_regex(ValueError, ""unexpected encoding""):\n            with self.roundtrip(ds, save_kwargs=kwargs) as actual:\n                pass\n\n        kwargs = dict(encoding={""x"": ""foo""})\n        with raises_regex(ValueError, ""must be castable""):\n            with self.roundtrip(ds, save_kwargs=kwargs) as actual:\n                pass\n\n        kwargs = dict(encoding={""invalid"": {}})\n        with pytest.raises(KeyError):\n            with self.roundtrip(ds, save_kwargs=kwargs) as actual:\n                pass\n\n    def test_encoding_kwarg_dates(self):\n        ds = Dataset({""t"": pd.date_range(""2000-01-01"", periods=3)})\n        units = ""days since 1900-01-01""\n        kwargs = dict(encoding={""t"": {""units"": units}})\n        with self.roundtrip(ds, save_kwargs=kwargs) as actual:\n            assert actual.t.encoding[""units""] == units\n            assert_identical(actual, ds)\n\n    def test_encoding_kwarg_fixed_width_string(self):\n        # regression test for GH2149\n        for strings in [[b""foo"", b""bar"", b""baz""], [""foo"", ""bar"", ""baz""]]:\n            ds = Dataset({""x"": strings})\n            kwargs = dict(encoding={""x"": {""dtype"": ""S1""}})\n            with self.roundtrip(ds, save_kwargs=kwargs) as actual:\n                assert actual[""x""].encoding[""dtype""] == ""S1""\n                assert_identical(actual, ds)\n\n    def test_default_fill_value(self):\n        # Test default encoding for float:\n        ds = Dataset({""x"": (""y"", np.arange(10.0))})\n        kwargs = dict(encoding={""x"": {""dtype"": ""f4""}})\n        with self.roundtrip(ds, save_kwargs=kwargs) as actual:\n            assert math.isnan(actual.x.encoding[""_FillValue""])\n        assert ds.x.encoding == {}\n\n        # Test default encoding for int:\n        ds = Dataset({""x"": (""y"", np.arange(10.0))})\n        kwargs = dict(encoding={""x"": {""dtype"": ""int16""}})\n        with warnings.catch_warnings():\n            warnings.filterwarnings(""ignore"", "".*floating point data as an integer"")\n            with self.roundtrip(ds, save_kwargs=kwargs) as actual:\n                assert ""_FillValue"" not in actual.x.encoding\n        assert ds.x.encoding == {}\n\n        # Test default encoding for implicit int:\n        ds = Dataset({""x"": (""y"", np.arange(10, dtype=""int16""))})\n        with self.roundtrip(ds) as actual:\n            assert ""_FillValue"" not in actual.x.encoding\n        assert ds.x.encoding == {}\n\n    def test_explicitly_omit_fill_value(self):\n        ds = Dataset({""x"": (""y"", [np.pi, -np.pi])})\n        ds.x.encoding[""_FillValue""] = None\n        with self.roundtrip(ds) as actual:\n            assert ""_FillValue"" not in actual.x.encoding\n\n    def test_explicitly_omit_fill_value_via_encoding_kwarg(self):\n        ds = Dataset({""x"": (""y"", [np.pi, -np.pi])})\n        kwargs = dict(encoding={""x"": {""_FillValue"": None}})\n        with self.roundtrip(ds, save_kwargs=kwargs) as actual:\n            assert ""_FillValue"" not in actual.x.encoding\n        assert ds.y.encoding == {}\n\n    def test_explicitly_omit_fill_value_in_coord(self):\n        ds = Dataset({""x"": (""y"", [np.pi, -np.pi])}, coords={""y"": [0.0, 1.0]})\n        ds.y.encoding[""_FillValue""] = None\n        with self.roundtrip(ds) as actual:\n            assert ""_FillValue"" not in actual.y.encoding\n\n    def test_explicitly_omit_fill_value_in_coord_via_encoding_kwarg(self):\n        ds = Dataset({""x"": (""y"", [np.pi, -np.pi])}, coords={""y"": [0.0, 1.0]})\n        kwargs = dict(encoding={""y"": {""_FillValue"": None}})\n        with self.roundtrip(ds, save_kwargs=kwargs) as actual:\n            assert ""_FillValue"" not in actual.y.encoding\n        assert ds.y.encoding == {}\n\n    def test_encoding_same_dtype(self):\n        ds = Dataset({""x"": (""y"", np.arange(10.0, dtype=""f4""))})\n        kwargs = dict(encoding={""x"": {""dtype"": ""f4""}})\n        with self.roundtrip(ds, save_kwargs=kwargs) as actual:\n            encoded_dtype = actual.x.encoding[""dtype""]\n            # On OS X, dtype sometimes switches endianness for unclear reasons\n            assert encoded_dtype.kind == ""f"" and encoded_dtype.itemsize == 4\n        assert ds.x.encoding == {}\n\n    def test_append_write(self):\n        # regression for GH1215\n        data = create_test_data()\n        with self.roundtrip_append(data) as actual:\n            assert_identical(data, actual)\n\n    def test_append_overwrite_values(self):\n        # regression for GH1215\n        data = create_test_data()\n        with create_tmp_file(allow_cleanup_failure=False) as tmp_file:\n            self.save(data, tmp_file, mode=""w"")\n            data[""var2""][:] = -999\n            data[""var9""] = data[""var2""] * 3\n            self.save(data[[""var2"", ""var9""]], tmp_file, mode=""a"")\n            with self.open(tmp_file) as actual:\n                assert_identical(data, actual)\n\n    def test_append_with_invalid_dim_raises(self):\n        data = create_test_data()\n        with create_tmp_file(allow_cleanup_failure=False) as tmp_file:\n            self.save(data, tmp_file, mode=""w"")\n            data[""var9""] = data[""var2""] * 3\n            data = data.isel(dim1=slice(2, 6))  # modify one dimension\n            with raises_regex(\n                ValueError, ""Unable to update size for existing dimension""\n            ):\n                self.save(data, tmp_file, mode=""a"")\n\n    def test_multiindex_not_implemented(self):\n        ds = Dataset(coords={""y"": (""x"", [1, 2]), ""z"": (""x"", [""a"", ""b""])}).set_index(\n            x=[""y"", ""z""]\n        )\n        with raises_regex(NotImplementedError, ""MultiIndex""):\n            with self.roundtrip(ds):\n                pass\n\n\n_counter = itertools.count()\n\n\n@contextlib.contextmanager\ndef create_tmp_file(suffix="".nc"", allow_cleanup_failure=False):\n    temp_dir = tempfile.mkdtemp()\n    path = os.path.join(temp_dir, ""temp-{}{}"".format(next(_counter), suffix))\n    try:\n        yield path\n    finally:\n        try:\n            shutil.rmtree(temp_dir)\n        except OSError:\n            if not allow_cleanup_failure:\n                raise\n\n\n@contextlib.contextmanager\ndef create_tmp_files(nfiles, suffix="".nc"", allow_cleanup_failure=False):\n    with ExitStack() as stack:\n        files = [\n            stack.enter_context(create_tmp_file(suffix, allow_cleanup_failure))\n            for apath in np.arange(nfiles)\n        ]\n        yield files\n\n\nclass NetCDF4Base(CFEncodedBase):\n    """"""Tests for both netCDF4-python and h5netcdf.""""""\n\n    engine = ""netcdf4""\n\n    def test_open_group(self):\n        # Create a netCDF file with a dataset stored within a group\n        with create_tmp_file() as tmp_file:\n            with nc4.Dataset(tmp_file, ""w"") as rootgrp:\n                foogrp = rootgrp.createGroup(""foo"")\n                ds = foogrp\n                ds.createDimension(""time"", size=10)\n                x = np.arange(10)\n                ds.createVariable(""x"", np.int32, dimensions=(""time"",))\n                ds.variables[""x""][:] = x\n\n            expected = Dataset()\n            expected[""x""] = (""time"", x)\n\n            # check equivalent ways to specify group\n            for group in ""foo"", ""/foo"", ""foo/"", ""/foo/"":\n                with self.open(tmp_file, group=group) as actual:\n                    assert_equal(actual[""x""], expected[""x""])\n\n            # check that missing group raises appropriate exception\n            with pytest.raises(IOError):\n                open_dataset(tmp_file, group=""bar"")\n            with raises_regex(ValueError, ""must be a string""):\n                open_dataset(tmp_file, group=(1, 2, 3))\n\n    def test_open_subgroup(self):\n        # Create a netCDF file with a dataset stored within a group within a\n        # group\n        with create_tmp_file() as tmp_file:\n            rootgrp = nc4.Dataset(tmp_file, ""w"")\n            foogrp = rootgrp.createGroup(""foo"")\n            bargrp = foogrp.createGroup(""bar"")\n            ds = bargrp\n            ds.createDimension(""time"", size=10)\n            x = np.arange(10)\n            ds.createVariable(""x"", np.int32, dimensions=(""time"",))\n            ds.variables[""x""][:] = x\n            rootgrp.close()\n\n            expected = Dataset()\n            expected[""x""] = (""time"", x)\n\n            # check equivalent ways to specify group\n            for group in ""foo/bar"", ""/foo/bar"", ""foo/bar/"", ""/foo/bar/"":\n                with self.open(tmp_file, group=group) as actual:\n                    assert_equal(actual[""x""], expected[""x""])\n\n    def test_write_groups(self):\n        data1 = create_test_data()\n        data2 = data1 * 2\n        with create_tmp_file() as tmp_file:\n            self.save(data1, tmp_file, group=""data/1"")\n            self.save(data2, tmp_file, group=""data/2"", mode=""a"")\n            with self.open(tmp_file, group=""data/1"") as actual1:\n                assert_identical(data1, actual1)\n            with self.open(tmp_file, group=""data/2"") as actual2:\n                assert_identical(data2, actual2)\n\n    def test_encoding_kwarg_vlen_string(self):\n        for input_strings in [[b""foo"", b""bar"", b""baz""], [""foo"", ""bar"", ""baz""]]:\n            original = Dataset({""x"": input_strings})\n            expected = Dataset({""x"": [""foo"", ""bar"", ""baz""]})\n            kwargs = dict(encoding={""x"": {""dtype"": str}})\n            with self.roundtrip(original, save_kwargs=kwargs) as actual:\n                assert actual[""x""].encoding[""dtype""] is str\n                assert_identical(actual, expected)\n\n    def test_roundtrip_string_with_fill_value_vlen(self):\n        values = np.array([""ab"", ""cdef"", np.nan], dtype=object)\n        expected = Dataset({""x"": (""t"", values)})\n\n        # netCDF4-based backends don\'t support an explicit fillvalue\n        # for variable length strings yet.\n        # https://github.com/Unidata/netcdf4-python/issues/730\n        # https://github.com/shoyer/h5netcdf/issues/37\n        original = Dataset({""x"": (""t"", values, {}, {""_FillValue"": ""XXX""})})\n        with pytest.raises(NotImplementedError):\n            with self.roundtrip(original) as actual:\n                assert_identical(expected, actual)\n\n        original = Dataset({""x"": (""t"", values, {}, {""_FillValue"": """"})})\n        with pytest.raises(NotImplementedError):\n            with self.roundtrip(original) as actual:\n                assert_identical(expected, actual)\n\n    def test_roundtrip_character_array(self):\n        with create_tmp_file() as tmp_file:\n            values = np.array([[""a"", ""b"", ""c""], [""d"", ""e"", ""f""]], dtype=""S"")\n\n            with nc4.Dataset(tmp_file, mode=""w"") as nc:\n                nc.createDimension(""x"", 2)\n                nc.createDimension(""string3"", 3)\n                v = nc.createVariable(""x"", np.dtype(""S1""), (""x"", ""string3""))\n                v[:] = values\n\n            values = np.array([""abc"", ""def""], dtype=""S"")\n            expected = Dataset({""x"": (""x"", values)})\n            with open_dataset(tmp_file) as actual:\n                assert_identical(expected, actual)\n                # regression test for #157\n                with self.roundtrip(actual) as roundtripped:\n                    assert_identical(expected, roundtripped)\n\n    def test_default_to_char_arrays(self):\n        data = Dataset({""x"": np.array([""foo"", ""zzzz""], dtype=""S"")})\n        with self.roundtrip(data) as actual:\n            assert_identical(data, actual)\n            assert actual[""x""].dtype == np.dtype(""S4"")\n\n    def test_open_encodings(self):\n        # Create a netCDF file with explicit time units\n        # and make sure it makes it into the encodings\n        # and survives a round trip\n        with create_tmp_file() as tmp_file:\n            with nc4.Dataset(tmp_file, ""w"") as ds:\n                ds.createDimension(""time"", size=10)\n                ds.createVariable(""time"", np.int32, dimensions=(""time"",))\n                units = ""days since 1999-01-01""\n                ds.variables[""time""].setncattr(""units"", units)\n                ds.variables[""time""][:] = np.arange(10) + 4\n\n            expected = Dataset()\n\n            time = pd.date_range(""1999-01-05"", periods=10)\n            encoding = {""units"": units, ""dtype"": np.dtype(""int32"")}\n            expected[""time""] = (""time"", time, {}, encoding)\n\n            with open_dataset(tmp_file) as actual:\n                assert_equal(actual[""time""], expected[""time""])\n                actual_encoding = {\n                    k: v\n                    for k, v in actual[""time""].encoding.items()\n                    if k in expected[""time""].encoding\n                }\n                assert actual_encoding == expected[""time""].encoding\n\n    def test_dump_encodings(self):\n        # regression test for #709\n        ds = Dataset({""x"": (""y"", np.arange(10.0))})\n        kwargs = dict(encoding={""x"": {""zlib"": True}})\n        with self.roundtrip(ds, save_kwargs=kwargs) as actual:\n            assert actual.x.encoding[""zlib""]\n\n    def test_dump_and_open_encodings(self):\n        # Create a netCDF file with explicit time units\n        # and make sure it makes it into the encodings\n        # and survives a round trip\n        with create_tmp_file() as tmp_file:\n            with nc4.Dataset(tmp_file, ""w"") as ds:\n                ds.createDimension(""time"", size=10)\n                ds.createVariable(""time"", np.int32, dimensions=(""time"",))\n                units = ""days since 1999-01-01""\n                ds.variables[""time""].setncattr(""units"", units)\n                ds.variables[""time""][:] = np.arange(10) + 4\n\n            with open_dataset(tmp_file) as xarray_dataset:\n                with create_tmp_file() as tmp_file2:\n                    xarray_dataset.to_netcdf(tmp_file2)\n                    with nc4.Dataset(tmp_file2, ""r"") as ds:\n                        assert ds.variables[""time""].getncattr(""units"") == units\n                        assert_array_equal(ds.variables[""time""], np.arange(10) + 4)\n\n    def test_compression_encoding(self):\n        data = create_test_data()\n        data[""var2""].encoding.update(\n            {\n                ""zlib"": True,\n                ""chunksizes"": (5, 5),\n                ""fletcher32"": True,\n                ""shuffle"": True,\n                ""original_shape"": data.var2.shape,\n            }\n        )\n        with self.roundtrip(data) as actual:\n            for k, v in data[""var2""].encoding.items():\n                assert v == actual[""var2""].encoding[k]\n\n        # regression test for #156\n        expected = data.isel(dim1=0)\n        with self.roundtrip(expected) as actual:\n            assert_equal(expected, actual)\n\n    def test_encoding_kwarg_compression(self):\n        ds = Dataset({""x"": np.arange(10.0)})\n        encoding = dict(\n            dtype=""f4"",\n            zlib=True,\n            complevel=9,\n            fletcher32=True,\n            chunksizes=(5,),\n            shuffle=True,\n        )\n        kwargs = dict(encoding=dict(x=encoding))\n\n        with self.roundtrip(ds, save_kwargs=kwargs) as actual:\n            assert_equal(actual, ds)\n            assert actual.x.encoding[""dtype""] == ""f4""\n            assert actual.x.encoding[""zlib""]\n            assert actual.x.encoding[""complevel""] == 9\n            assert actual.x.encoding[""fletcher32""]\n            assert actual.x.encoding[""chunksizes""] == (5,)\n            assert actual.x.encoding[""shuffle""]\n\n        assert ds.x.encoding == {}\n\n    def test_keep_chunksizes_if_no_original_shape(self):\n        ds = Dataset({""x"": [1, 2, 3]})\n        chunksizes = (2,)\n        ds.variables[""x""].encoding = {""chunksizes"": chunksizes}\n\n        with self.roundtrip(ds) as actual:\n            assert_identical(ds, actual)\n            assert_array_equal(\n                ds[""x""].encoding[""chunksizes""], actual[""x""].encoding[""chunksizes""]\n            )\n\n    def test_encoding_chunksizes_unlimited(self):\n        # regression test for GH1225\n        ds = Dataset({""x"": [1, 2, 3], ""y"": (""x"", [2, 3, 4])})\n        ds.variables[""x""].encoding = {\n            ""zlib"": False,\n            ""shuffle"": False,\n            ""complevel"": 0,\n            ""fletcher32"": False,\n            ""contiguous"": False,\n            ""chunksizes"": (2 ** 20,),\n            ""original_shape"": (3,),\n        }\n        with self.roundtrip(ds) as actual:\n            assert_equal(ds, actual)\n\n    def test_mask_and_scale(self):\n        with create_tmp_file() as tmp_file:\n            with nc4.Dataset(tmp_file, mode=""w"") as nc:\n                nc.createDimension(""t"", 5)\n                nc.createVariable(""x"", ""int16"", (""t"",), fill_value=-1)\n                v = nc.variables[""x""]\n                v.set_auto_maskandscale(False)\n                v.add_offset = 10\n                v.scale_factor = 0.1\n                v[:] = np.array([-1, -1, 0, 1, 2])\n\n            # first make sure netCDF4 reads the masked and scaled data\n            # correctly\n            with nc4.Dataset(tmp_file, mode=""r"") as nc:\n                expected = np.ma.array(\n                    [-1, -1, 10, 10.1, 10.2], mask=[True, True, False, False, False]\n                )\n                actual = nc.variables[""x""][:]\n                assert_array_equal(expected, actual)\n\n            # now check xarray\n            with open_dataset(tmp_file) as ds:\n                expected = create_masked_and_scaled_data()\n                assert_identical(expected, ds)\n\n    def test_0dimensional_variable(self):\n        # This fix verifies our work-around to this netCDF4-python bug:\n        # https://github.com/Unidata/netcdf4-python/pull/220\n        with create_tmp_file() as tmp_file:\n            with nc4.Dataset(tmp_file, mode=""w"") as nc:\n                v = nc.createVariable(""x"", ""int16"")\n                v[...] = 123\n\n            with open_dataset(tmp_file) as ds:\n                expected = Dataset({""x"": ((), 123)})\n                assert_identical(expected, ds)\n\n    def test_read_variable_len_strings(self):\n        with create_tmp_file() as tmp_file:\n            values = np.array([""foo"", ""bar"", ""baz""], dtype=object)\n\n            with nc4.Dataset(tmp_file, mode=""w"") as nc:\n                nc.createDimension(""x"", 3)\n                v = nc.createVariable(""x"", str, (""x"",))\n                v[:] = values\n\n            expected = Dataset({""x"": (""x"", values)})\n            for kwargs in [{}, {""decode_cf"": True}]:\n                with open_dataset(tmp_file, **kwargs) as actual:\n                    assert_identical(expected, actual)\n\n    def test_encoding_unlimited_dims(self):\n        ds = Dataset({""x"": (""y"", np.arange(10.0))})\n        with self.roundtrip(ds, save_kwargs=dict(unlimited_dims=[""y""])) as actual:\n            assert actual.encoding[""unlimited_dims""] == set(""y"")\n            assert_equal(ds, actual)\n        ds.encoding = {""unlimited_dims"": [""y""]}\n        with self.roundtrip(ds) as actual:\n            assert actual.encoding[""unlimited_dims""] == set(""y"")\n            assert_equal(ds, actual)\n\n\n@requires_netCDF4\nclass TestNetCDF4Data(NetCDF4Base):\n    @contextlib.contextmanager\n    def create_store(self):\n        with create_tmp_file() as tmp_file:\n            with backends.NetCDF4DataStore.open(tmp_file, mode=""w"") as store:\n                yield store\n\n    def test_variable_order(self):\n        # doesn\'t work with scipy or h5py :(\n        ds = Dataset()\n        ds[""a""] = 1\n        ds[""z""] = 2\n        ds[""b""] = 3\n        ds.coords[""c""] = 4\n\n        with self.roundtrip(ds) as actual:\n            assert list(ds.variables) == list(actual.variables)\n\n    def test_unsorted_index_raises(self):\n        # should be fixed in netcdf4 v1.2.1\n        random_data = np.random.random(size=(4, 6))\n        dim0 = [0, 1, 2, 3]\n        dim1 = [0, 2, 1, 3, 5, 4]  # We will sort this in a later step\n        da = xr.DataArray(\n            data=random_data,\n            dims=(""dim0"", ""dim1""),\n            coords={""dim0"": dim0, ""dim1"": dim1},\n            name=""randovar"",\n        )\n        ds = da.to_dataset()\n\n        with self.roundtrip(ds) as ondisk:\n            inds = np.argsort(dim1)\n            ds2 = ondisk.isel(dim1=inds)\n            # Older versions of NetCDF4 raise an exception here, and if so we\n            # want to ensure we improve (that is, replace) the error message\n            try:\n                ds2.randovar.values\n            except IndexError as err:\n                assert ""first by calling .load"" in str(err)\n\n    def test_setncattr_string(self):\n        list_of_strings = [""list"", ""of"", ""strings""]\n        one_element_list_of_strings = [""one element""]\n        one_string = ""one string""\n        attrs = {\n            ""foo"": list_of_strings,\n            ""bar"": one_element_list_of_strings,\n            ""baz"": one_string,\n        }\n        ds = Dataset({""x"": (""y"", [1, 2, 3], attrs)}, attrs=attrs)\n\n        with self.roundtrip(ds) as actual:\n            for totest in [actual, actual[""x""]]:\n                assert_array_equal(list_of_strings, totest.attrs[""foo""])\n                assert_array_equal(one_element_list_of_strings, totest.attrs[""bar""])\n                assert one_string == totest.attrs[""baz""]\n\n    def test_autoclose_future_warning(self):\n        data = create_test_data()\n        with create_tmp_file() as tmp_file:\n            self.save(data, tmp_file)\n            with pytest.warns(FutureWarning):\n                with self.open(tmp_file, autoclose=True) as actual:\n                    assert_identical(data, actual)\n\n    def test_already_open_dataset(self):\n        with create_tmp_file() as tmp_file:\n            with nc4.Dataset(tmp_file, mode=""w"") as nc:\n                v = nc.createVariable(""x"", ""int"")\n                v[...] = 42\n\n            nc = nc4.Dataset(tmp_file, mode=""r"")\n            store = backends.NetCDF4DataStore(nc)\n            with open_dataset(store) as ds:\n                expected = Dataset({""x"": ((), 42)})\n                assert_identical(expected, ds)\n\n    def test_already_open_dataset_group(self):\n        with create_tmp_file() as tmp_file:\n            with nc4.Dataset(tmp_file, mode=""w"") as nc:\n                group = nc.createGroup(""g"")\n                v = group.createVariable(""x"", ""int"")\n                v[...] = 42\n\n            nc = nc4.Dataset(tmp_file, mode=""r"")\n            store = backends.NetCDF4DataStore(nc.groups[""g""])\n            with open_dataset(store) as ds:\n                expected = Dataset({""x"": ((), 42)})\n                assert_identical(expected, ds)\n\n            nc = nc4.Dataset(tmp_file, mode=""r"")\n            store = backends.NetCDF4DataStore(nc, group=""g"")\n            with open_dataset(store) as ds:\n                expected = Dataset({""x"": ((), 42)})\n                assert_identical(expected, ds)\n\n            with nc4.Dataset(tmp_file, mode=""r"") as nc:\n                with pytest.raises(ValueError, match=""must supply a root""):\n                    backends.NetCDF4DataStore(nc.groups[""g""], group=""g"")\n\n\n@requires_netCDF4\n@requires_dask\n@pytest.mark.filterwarnings(""ignore:deallocating CachingFileManager"")\nclass TestNetCDF4ViaDaskData(TestNetCDF4Data):\n    @contextlib.contextmanager\n    def roundtrip(\n        self, data, save_kwargs=None, open_kwargs=None, allow_cleanup_failure=False\n    ):\n        if open_kwargs is None:\n            open_kwargs = {}\n        if save_kwargs is None:\n            save_kwargs = {}\n        open_kwargs.setdefault(""chunks"", -1)\n        with TestNetCDF4Data.roundtrip(\n            self, data, save_kwargs, open_kwargs, allow_cleanup_failure\n        ) as ds:\n            yield ds\n\n    def test_unsorted_index_raises(self):\n        # Skip when using dask because dask rewrites indexers to getitem,\n        # dask first pulls items by block.\n        pass\n\n    def test_dataset_caching(self):\n        # caching behavior differs for dask\n        pass\n\n    def test_write_inconsistent_chunks(self):\n        # Construct two variables with the same dimensions, but different\n        # chunk sizes.\n        x = da.zeros((100, 100), dtype=""f4"", chunks=(50, 100))\n        x = DataArray(data=x, dims=(""lat"", ""lon""), name=""x"")\n        x.encoding[""chunksizes""] = (50, 100)\n        x.encoding[""original_shape""] = (100, 100)\n        y = da.ones((100, 100), dtype=""f4"", chunks=(100, 50))\n        y = DataArray(data=y, dims=(""lat"", ""lon""), name=""y"")\n        y.encoding[""chunksizes""] = (100, 50)\n        y.encoding[""original_shape""] = (100, 100)\n        # Put them both into the same dataset\n        ds = Dataset({""x"": x, ""y"": y})\n        with self.roundtrip(ds) as actual:\n            assert actual[""x""].encoding[""chunksizes""] == (50, 100)\n            assert actual[""y""].encoding[""chunksizes""] == (100, 50)\n\n\n@requires_zarr\nclass ZarrBase(CFEncodedBase):\n\n    DIMENSION_KEY = ""_ARRAY_DIMENSIONS""\n\n    @contextlib.contextmanager\n    def create_store(self):\n        with self.create_zarr_target() as store_target:\n            yield backends.ZarrStore.open_group(store_target, mode=""w"")\n\n    def save(self, dataset, store_target, **kwargs):\n        return dataset.to_zarr(store=store_target, **kwargs)\n\n    @contextlib.contextmanager\n    def open(self, store_target, **kwargs):\n        with xr.open_zarr(store_target, **kwargs) as ds:\n            yield ds\n\n    @contextlib.contextmanager\n    def roundtrip(\n        self, data, save_kwargs=None, open_kwargs=None, allow_cleanup_failure=False\n    ):\n        if save_kwargs is None:\n            save_kwargs = {}\n        if open_kwargs is None:\n            open_kwargs = {}\n        with self.create_zarr_target() as store_target:\n            self.save(data, store_target, **save_kwargs)\n            with self.open(store_target, **open_kwargs) as ds:\n                yield ds\n\n    def test_roundtrip_consolidated(self):\n        pytest.importorskip(""zarr"", minversion=""2.2.1.dev2"")\n        expected = create_test_data()\n        with self.roundtrip(\n            expected,\n            save_kwargs={""consolidated"": True},\n            open_kwargs={""consolidated"": True},\n        ) as actual:\n            self.check_dtypes_roundtripped(expected, actual)\n            assert_identical(expected, actual)\n\n    @requires_dask\n    def test_auto_chunk(self):\n        original = create_test_data().chunk()\n\n        with self.roundtrip(original, open_kwargs={""chunks"": None}) as actual:\n            for k, v in actual.variables.items():\n                # only index variables should be in memory\n                assert v._in_memory == (k in actual.dims)\n                # there should be no chunks\n                assert v.chunks is None\n\n        with self.roundtrip(original, open_kwargs={""chunks"": ""auto""}) as actual:\n            for k, v in actual.variables.items():\n                # only index variables should be in memory\n                assert v._in_memory == (k in actual.dims)\n                # chunk size should be the same as original\n                assert v.chunks == original[k].chunks\n\n    @requires_dask\n    @pytest.mark.filterwarnings(""ignore:Specified Dask chunks"")\n    def test_manual_chunk(self):\n        original = create_test_data().chunk({""dim1"": 3, ""dim2"": 4, ""dim3"": 3})\n\n        # All of these should return non-chunked arrays\n        NO_CHUNKS = (None, 0, {})\n        for no_chunk in NO_CHUNKS:\n            open_kwargs = {""chunks"": no_chunk}\n            with self.roundtrip(original, open_kwargs=open_kwargs) as actual:\n                for k, v in actual.variables.items():\n                    # only index variables should be in memory\n                    assert v._in_memory == (k in actual.dims)\n                    # there should be no chunks\n                    assert v.chunks is None\n\n        # uniform arrays\n        for i in range(2, 6):\n            rechunked = original.chunk(chunks=i)\n            open_kwargs = {""chunks"": i}\n            with self.roundtrip(original, open_kwargs=open_kwargs) as actual:\n                for k, v in actual.variables.items():\n                    # only index variables should be in memory\n                    assert v._in_memory == (k in actual.dims)\n                    # chunk size should be the same as rechunked\n                    assert v.chunks == rechunked[k].chunks\n\n        chunks = {""dim1"": 2, ""dim2"": 3, ""dim3"": 5}\n        rechunked = original.chunk(chunks=chunks)\n\n        open_kwargs = {""chunks"": chunks, ""overwrite_encoded_chunks"": True}\n        with self.roundtrip(original, open_kwargs=open_kwargs) as actual:\n            for k, v in actual.variables.items():\n                assert v.chunks == rechunked[k].chunks\n\n            with self.roundtrip(actual) as auto:\n                # encoding should have changed\n                for k, v in actual.variables.items():\n                    assert v.chunks == rechunked[k].chunks\n\n                assert_identical(actual, auto)\n                assert_identical(actual.load(), auto.load())\n\n    @requires_dask\n    def test_warning_on_bad_chunks(self):\n        original = create_test_data().chunk({""dim1"": 4, ""dim2"": 3, ""dim3"": 5})\n\n        bad_chunks = (2, {""dim2"": (3, 3, 2, 1)})\n        for chunks in bad_chunks:\n            kwargs = {""chunks"": chunks}\n            with pytest.warns(UserWarning):\n                with self.roundtrip(original, open_kwargs=kwargs) as actual:\n                    for k, v in actual.variables.items():\n                        # only index variables should be in memory\n                        assert v._in_memory == (k in actual.dims)\n\n        good_chunks = ({""dim2"": 3}, {""dim3"": 10})\n        for chunks in good_chunks:\n            kwargs = {""chunks"": chunks}\n            with pytest.warns(None) as record:\n                with self.roundtrip(original, open_kwargs=kwargs) as actual:\n                    for k, v in actual.variables.items():\n                        # only index variables should be in memory\n                        assert v._in_memory == (k in actual.dims)\n            assert len(record) == 0\n\n    @requires_dask\n    def test_deprecate_auto_chunk(self):\n        original = create_test_data().chunk()\n        with pytest.warns(FutureWarning):\n            with self.roundtrip(original, open_kwargs={""auto_chunk"": True}) as actual:\n                for k, v in actual.variables.items():\n                    # only index variables should be in memory\n                    assert v._in_memory == (k in actual.dims)\n                    # chunk size should be the same as original\n                    assert v.chunks == original[k].chunks\n\n        with pytest.warns(FutureWarning):\n            with self.roundtrip(original, open_kwargs={""auto_chunk"": False}) as actual:\n                for k, v in actual.variables.items():\n                    # only index variables should be in memory\n                    assert v._in_memory == (k in actual.dims)\n                    # there should be no chunks\n                    assert v.chunks is None\n\n    @requires_dask\n    def test_write_uneven_dask_chunks(self):\n        # regression for GH#2225\n        original = create_test_data().chunk({""dim1"": 3, ""dim2"": 4, ""dim3"": 3})\n        with self.roundtrip(original, open_kwargs={""chunks"": ""auto""}) as actual:\n            for k, v in actual.data_vars.items():\n                print(k)\n                assert v.chunks == actual[k].chunks\n\n    def test_chunk_encoding(self):\n        # These datasets have no dask chunks. All chunking specified in\n        # encoding\n        data = create_test_data()\n        chunks = (5, 5)\n        data[""var2""].encoding.update({""chunks"": chunks})\n\n        with self.roundtrip(data) as actual:\n            assert chunks == actual[""var2""].encoding[""chunks""]\n\n        # expect an error with non-integer chunks\n        data[""var2""].encoding.update({""chunks"": (5, 4.5)})\n        with pytest.raises(TypeError):\n            with self.roundtrip(data) as actual:\n                pass\n\n    @requires_dask\n    def test_chunk_encoding_with_dask(self):\n        # These datasets DO have dask chunks. Need to check for various\n        # interactions between dask and zarr chunks\n        ds = xr.DataArray((np.arange(12)), dims=""x"", name=""var1"").to_dataset()\n\n        # - no encoding specified -\n        # zarr automatically gets chunk information from dask chunks\n        ds_chunk4 = ds.chunk({""x"": 4})\n        with self.roundtrip(ds_chunk4) as actual:\n            assert (4,) == actual[""var1""].encoding[""chunks""]\n\n        # should fail if dask_chunks are irregular...\n        ds_chunk_irreg = ds.chunk({""x"": (5, 4, 3)})\n        with raises_regex(ValueError, ""uniform chunk sizes.""):\n            with self.roundtrip(ds_chunk_irreg) as actual:\n                pass\n\n        # should fail if encoding[""chunks""] clashes with dask_chunks\n        badenc = ds.chunk({""x"": 4})\n        badenc.var1.encoding[""chunks""] = (6,)\n        with raises_regex(NotImplementedError, ""named \'var1\' would overlap""):\n            with self.roundtrip(badenc) as actual:\n                pass\n\n        badenc.var1.encoding[""chunks""] = (2,)\n        with raises_regex(ValueError, ""Specified Zarr chunk encoding""):\n            with self.roundtrip(badenc) as actual:\n                pass\n\n        badenc = badenc.chunk({""x"": (3, 3, 6)})\n        badenc.var1.encoding[""chunks""] = (3,)\n        with raises_regex(ValueError, ""incompatible with this encoding""):\n            with self.roundtrip(badenc) as actual:\n                pass\n\n        # ... except if the last chunk is smaller than the first\n        ds_chunk_irreg = ds.chunk({""x"": (5, 5, 2)})\n        with self.roundtrip(ds_chunk_irreg) as actual:\n            assert (5,) == actual[""var1""].encoding[""chunks""]\n        # re-save Zarr arrays\n        with self.roundtrip(ds_chunk_irreg) as original:\n            with self.roundtrip(original) as actual:\n                assert_identical(original, actual)\n\n        # - encoding specified  -\n        # specify compatible encodings\n        for chunk_enc in 4, (4,):\n            ds_chunk4[""var1""].encoding.update({""chunks"": chunk_enc})\n            with self.roundtrip(ds_chunk4) as actual:\n                assert (4,) == actual[""var1""].encoding[""chunks""]\n\n        # TODO: remove this failure once syncronized overlapping writes are\n        # supported by xarray\n        ds_chunk4[""var1""].encoding.update({""chunks"": 5})\n        with pytest.raises(NotImplementedError):\n            with self.roundtrip(ds_chunk4) as actual:\n                pass\n\n    def test_hidden_zarr_keys(self):\n        expected = create_test_data()\n        with self.create_store() as store:\n            expected.dump_to_store(store)\n            zarr_group = store.ds\n\n            # check that a variable hidden attribute is present and correct\n            # JSON only has a single array type, which maps to list in Python.\n            # In contrast, dims in xarray is always a tuple.\n            for var in expected.variables.keys():\n                dims = zarr_group[var].attrs[self.DIMENSION_KEY]\n                assert dims == list(expected[var].dims)\n\n            with xr.decode_cf(store):\n                # make sure it is hidden\n                for var in expected.variables.keys():\n                    assert self.DIMENSION_KEY not in expected[var].attrs\n\n            # put it back and try removing from a variable\n            del zarr_group.var2.attrs[self.DIMENSION_KEY]\n            with pytest.raises(KeyError):\n                with xr.decode_cf(store):\n                    pass\n\n    @pytest.mark.skipif(LooseVersion(dask_version) < ""2.4"", reason=""dask GH5334"")\n    @pytest.mark.parametrize(""group"", [None, ""group1""])\n    def test_write_persistence_modes(self, group):\n        original = create_test_data()\n\n        # overwrite mode\n        with self.roundtrip(\n            original,\n            save_kwargs={""mode"": ""w"", ""group"": group},\n            open_kwargs={""group"": group},\n        ) as actual:\n            assert_identical(original, actual)\n\n        # don\'t overwrite mode\n        with self.roundtrip(\n            original,\n            save_kwargs={""mode"": ""w-"", ""group"": group},\n            open_kwargs={""group"": group},\n        ) as actual:\n            assert_identical(original, actual)\n\n        # make sure overwriting works as expected\n        with self.create_zarr_target() as store:\n            self.save(original, store)\n            # should overwrite with no error\n            self.save(original, store, mode=""w"", group=group)\n            with self.open(store, group=group) as actual:\n                assert_identical(original, actual)\n                with pytest.raises(ValueError):\n                    self.save(original, store, mode=""w-"")\n\n        # check append mode for normal write\n        with self.roundtrip(\n            original,\n            save_kwargs={""mode"": ""a"", ""group"": group},\n            open_kwargs={""group"": group},\n        ) as actual:\n            assert_identical(original, actual)\n\n        # check append mode for append write\n        ds, ds_to_append, _ = create_append_test_data()\n        with self.create_zarr_target() as store_target:\n            ds.to_zarr(store_target, mode=""w"", group=group)\n            ds_to_append.to_zarr(store_target, append_dim=""time"", group=group)\n            original = xr.concat([ds, ds_to_append], dim=""time"")\n            actual = xr.open_zarr(store_target, group=group)\n            assert_identical(original, actual)\n\n    def test_compressor_encoding(self):\n        original = create_test_data()\n        # specify a custom compressor\n        import zarr\n\n        blosc_comp = zarr.Blosc(cname=""zstd"", clevel=3, shuffle=2)\n        save_kwargs = dict(encoding={""var1"": {""compressor"": blosc_comp}})\n        with self.roundtrip(original, save_kwargs=save_kwargs) as ds:\n            actual = ds[""var1""].encoding[""compressor""]\n            # get_config returns a dictionary of compressor attributes\n            assert actual.get_config() == blosc_comp.get_config()\n\n    def test_group(self):\n        original = create_test_data()\n        group = ""some/random/path""\n        with self.roundtrip(\n            original, save_kwargs={""group"": group}, open_kwargs={""group"": group}\n        ) as actual:\n            assert_identical(original, actual)\n\n    def test_encoding_kwarg_fixed_width_string(self):\n        # not relevant for zarr, since we don\'t use EncodedStringCoder\n        pass\n\n    # TODO: someone who understand caching figure out whether caching\n    # makes sense for Zarr backend\n    @pytest.mark.xfail(reason=""Zarr caching not implemented"")\n    def test_dataset_caching(self):\n        super().test_dataset_caching()\n\n    @pytest.mark.skipif(LooseVersion(dask_version) < ""2.4"", reason=""dask GH5334"")\n    def test_append_write(self):\n        super().test_append_write()\n\n    def test_append_with_invalid_dim_raises(self):\n        ds, ds_to_append, _ = create_append_test_data()\n        with self.create_zarr_target() as store_target:\n            ds.to_zarr(store_target, mode=""w"")\n            with pytest.raises(\n                ValueError, match=""does not match any existing dataset dimensions""\n            ):\n                ds_to_append.to_zarr(store_target, append_dim=""notvalid"")\n\n    def test_append_with_no_dims_raises(self):\n        with self.create_zarr_target() as store_target:\n            Dataset({""foo"": (""x"", [1])}).to_zarr(store_target, mode=""w"")\n            with pytest.raises(ValueError, match=""different dimension names""):\n                Dataset({""foo"": (""y"", [2])}).to_zarr(store_target, mode=""a"")\n\n    def test_append_with_append_dim_not_set_raises(self):\n        ds, ds_to_append, _ = create_append_test_data()\n        with self.create_zarr_target() as store_target:\n            ds.to_zarr(store_target, mode=""w"")\n            with pytest.raises(ValueError, match=""different dimension sizes""):\n                ds_to_append.to_zarr(store_target, mode=""a"")\n\n    def test_append_with_mode_not_a_raises(self):\n        ds, ds_to_append, _ = create_append_test_data()\n        with self.create_zarr_target() as store_target:\n            ds.to_zarr(store_target, mode=""w"")\n            with pytest.raises(\n                ValueError, match=""append_dim was set along with mode=\'w\'""\n            ):\n                ds_to_append.to_zarr(store_target, mode=""w"", append_dim=""time"")\n\n    def test_append_with_existing_encoding_raises(self):\n        ds, ds_to_append, _ = create_append_test_data()\n        with self.create_zarr_target() as store_target:\n            ds.to_zarr(store_target, mode=""w"")\n            with pytest.raises(ValueError, match=""but encoding was provided""):\n                ds_to_append.to_zarr(\n                    store_target,\n                    append_dim=""time"",\n                    encoding={""da"": {""compressor"": None}},\n                )\n\n    def test_check_encoding_is_consistent_after_append(self):\n\n        ds, ds_to_append, _ = create_append_test_data()\n\n        # check encoding consistency\n        with self.create_zarr_target() as store_target:\n            import zarr\n\n            compressor = zarr.Blosc()\n            encoding = {""da"": {""compressor"": compressor}}\n            ds.to_zarr(store_target, mode=""w"", encoding=encoding)\n            ds_to_append.to_zarr(store_target, append_dim=""time"")\n            actual_ds = xr.open_zarr(store_target)\n            actual_encoding = actual_ds[""da""].encoding[""compressor""]\n            assert actual_encoding.get_config() == compressor.get_config()\n            assert_identical(\n                xr.open_zarr(store_target).compute(),\n                xr.concat([ds, ds_to_append], dim=""time""),\n            )\n\n    @pytest.mark.skipif(LooseVersion(dask_version) < ""2.4"", reason=""dask GH5334"")\n    def test_append_with_new_variable(self):\n\n        ds, ds_to_append, ds_with_new_var = create_append_test_data()\n\n        # check append mode for new variable\n        with self.create_zarr_target() as store_target:\n            xr.concat([ds, ds_to_append], dim=""time"").to_zarr(store_target, mode=""w"")\n            ds_with_new_var.to_zarr(store_target, mode=""a"")\n            combined = xr.concat([ds, ds_to_append], dim=""time"")\n            combined[""new_var""] = ds_with_new_var[""new_var""]\n            assert_identical(combined, xr.open_zarr(store_target))\n\n    @requires_dask\n    def test_to_zarr_compute_false_roundtrip(self):\n        from dask.delayed import Delayed\n\n        original = create_test_data().chunk()\n\n        with self.create_zarr_target() as store:\n            delayed_obj = self.save(original, store, compute=False)\n            assert isinstance(delayed_obj, Delayed)\n\n            # make sure target store has not been written to yet\n            with pytest.raises(AssertionError):\n                with self.open(store) as actual:\n                    assert_identical(original, actual)\n\n            delayed_obj.compute()\n\n            with self.open(store) as actual:\n                assert_identical(original, actual)\n\n    @requires_dask\n    def test_to_zarr_append_compute_false_roundtrip(self):\n        from dask.delayed import Delayed\n\n        ds, ds_to_append, _ = create_append_test_data()\n        ds, ds_to_append = ds.chunk(), ds_to_append.chunk()\n\n        with pytest.warns(SerializationWarning):\n            with self.create_zarr_target() as store:\n                delayed_obj = self.save(ds, store, compute=False, mode=""w"")\n                assert isinstance(delayed_obj, Delayed)\n\n                with pytest.raises(AssertionError):\n                    with self.open(store) as actual:\n                        assert_identical(ds, actual)\n\n                delayed_obj.compute()\n\n                with self.open(store) as actual:\n                    assert_identical(ds, actual)\n\n                delayed_obj = self.save(\n                    ds_to_append, store, compute=False, append_dim=""time""\n                )\n                assert isinstance(delayed_obj, Delayed)\n\n                with pytest.raises(AssertionError):\n                    with self.open(store) as actual:\n                        assert_identical(\n                            xr.concat([ds, ds_to_append], dim=""time""), actual\n                        )\n\n                delayed_obj.compute()\n\n                with self.open(store) as actual:\n                    assert_identical(xr.concat([ds, ds_to_append], dim=""time""), actual)\n\n    @requires_dask\n    def test_encoding_chunksizes(self):\n        # regression test for GH2278\n        # see also test_encoding_chunksizes_unlimited\n        nx, ny, nt = 4, 4, 5\n        original = xr.Dataset(\n            {}, coords={""x"": np.arange(nx), ""y"": np.arange(ny), ""t"": np.arange(nt)}\n        )\n        original[""v""] = xr.Variable((""x"", ""y"", ""t""), np.zeros((nx, ny, nt)))\n        original = original.chunk({""t"": 1, ""x"": 2, ""y"": 2})\n\n        with self.roundtrip(original) as ds1:\n            assert_equal(ds1, original)\n            with self.roundtrip(ds1.isel(t=0)) as ds2:\n                assert_equal(ds2, original.isel(t=0))\n\n\n@requires_zarr\nclass TestZarrDictStore(ZarrBase):\n    @contextlib.contextmanager\n    def create_zarr_target(self):\n        yield {}\n\n\n@requires_zarr\nclass TestZarrDirectoryStore(ZarrBase):\n    @contextlib.contextmanager\n    def create_zarr_target(self):\n        with create_tmp_file(suffix="".zarr"") as tmp:\n            yield tmp\n\n\n@requires_scipy\nclass TestScipyInMemoryData(CFEncodedBase, NetCDF3Only):\n    engine = ""scipy""\n\n    @contextlib.contextmanager\n    def create_store(self):\n        fobj = BytesIO()\n        yield backends.ScipyDataStore(fobj, ""w"")\n\n    def test_to_netcdf_explicit_engine(self):\n        # regression test for GH1321\n        Dataset({""foo"": 42}).to_netcdf(engine=""scipy"")\n\n    def test_bytes_pickle(self):\n        data = Dataset({""foo"": (""x"", [1, 2, 3])})\n        fobj = data.to_netcdf()\n        with self.open(fobj) as ds:\n            unpickled = pickle.loads(pickle.dumps(ds))\n            assert_identical(unpickled, data)\n\n\n@requires_scipy\nclass TestScipyFileObject(CFEncodedBase, NetCDF3Only):\n    engine = ""scipy""\n\n    @contextlib.contextmanager\n    def create_store(self):\n        fobj = BytesIO()\n        yield backends.ScipyDataStore(fobj, ""w"")\n\n    @contextlib.contextmanager\n    def roundtrip(\n        self, data, save_kwargs=None, open_kwargs=None, allow_cleanup_failure=False\n    ):\n        if save_kwargs is None:\n            save_kwargs = {}\n        if open_kwargs is None:\n            open_kwargs = {}\n        with create_tmp_file() as tmp_file:\n            with open(tmp_file, ""wb"") as f:\n                self.save(data, f, **save_kwargs)\n            with open(tmp_file, ""rb"") as f:\n                with self.open(f, **open_kwargs) as ds:\n                    yield ds\n\n    @pytest.mark.skip(reason=""cannot pickle file objects"")\n    def test_pickle(self):\n        pass\n\n    @pytest.mark.skip(reason=""cannot pickle file objects"")\n    def test_pickle_dataarray(self):\n        pass\n\n\n@requires_scipy\nclass TestScipyFilePath(CFEncodedBase, NetCDF3Only):\n    engine = ""scipy""\n\n    @contextlib.contextmanager\n    def create_store(self):\n        with create_tmp_file() as tmp_file:\n            with backends.ScipyDataStore(tmp_file, mode=""w"") as store:\n                yield store\n\n    def test_array_attrs(self):\n        ds = Dataset(attrs={""foo"": [[1, 2], [3, 4]]})\n        with raises_regex(ValueError, ""must be 1-dimensional""):\n            with self.roundtrip(ds):\n                pass\n\n    def test_roundtrip_example_1_netcdf_gz(self):\n        with open_example_dataset(""example_1.nc.gz"") as expected:\n            with open_example_dataset(""example_1.nc"") as actual:\n                assert_identical(expected, actual)\n\n    def test_netcdf3_endianness(self):\n        # regression test for GH416\n        with open_example_dataset(""bears.nc"", engine=""scipy"") as expected:\n            for var in expected.variables.values():\n                assert var.dtype.isnative\n\n    @requires_netCDF4\n    def test_nc4_scipy(self):\n        with create_tmp_file(allow_cleanup_failure=True) as tmp_file:\n            with nc4.Dataset(tmp_file, ""w"", format=""NETCDF4"") as rootgrp:\n                rootgrp.createGroup(""foo"")\n\n            with raises_regex(TypeError, ""pip install netcdf4""):\n                open_dataset(tmp_file, engine=""scipy"")\n\n\n@requires_netCDF4\nclass TestNetCDF3ViaNetCDF4Data(CFEncodedBase, NetCDF3Only):\n    engine = ""netcdf4""\n    file_format = ""NETCDF3_CLASSIC""\n\n    @contextlib.contextmanager\n    def create_store(self):\n        with create_tmp_file() as tmp_file:\n            with backends.NetCDF4DataStore.open(\n                tmp_file, mode=""w"", format=""NETCDF3_CLASSIC""\n            ) as store:\n                yield store\n\n    def test_encoding_kwarg_vlen_string(self):\n        original = Dataset({""x"": [""foo"", ""bar"", ""baz""]})\n        kwargs = dict(encoding={""x"": {""dtype"": str}})\n        with raises_regex(ValueError, ""encoding dtype=str for vlen""):\n            with self.roundtrip(original, save_kwargs=kwargs):\n                pass\n\n\n@requires_netCDF4\nclass TestNetCDF4ClassicViaNetCDF4Data(CFEncodedBase, NetCDF3Only):\n    engine = ""netcdf4""\n    file_format = ""NETCDF4_CLASSIC""\n\n    @contextlib.contextmanager\n    def create_store(self):\n        with create_tmp_file() as tmp_file:\n            with backends.NetCDF4DataStore.open(\n                tmp_file, mode=""w"", format=""NETCDF4_CLASSIC""\n            ) as store:\n                yield store\n\n\n@requires_scipy_or_netCDF4\nclass TestGenericNetCDFData(CFEncodedBase, NetCDF3Only):\n    # verify that we can read and write netCDF3 files as long as we have scipy\n    # or netCDF4-python installed\n    file_format = ""netcdf3_64bit""\n\n    def test_write_store(self):\n        # there\'s no specific store to test here\n        pass\n\n    @requires_scipy\n    def test_engine(self):\n        data = create_test_data()\n        with raises_regex(ValueError, ""unrecognized engine""):\n            data.to_netcdf(""foo.nc"", engine=""foobar"")\n        with raises_regex(ValueError, ""invalid engine""):\n            data.to_netcdf(engine=""netcdf4"")\n\n        with create_tmp_file() as tmp_file:\n            data.to_netcdf(tmp_file)\n            with raises_regex(ValueError, ""unrecognized engine""):\n                open_dataset(tmp_file, engine=""foobar"")\n\n        netcdf_bytes = data.to_netcdf()\n        with raises_regex(ValueError, ""unrecognized engine""):\n            open_dataset(BytesIO(netcdf_bytes), engine=""foobar"")\n\n    def test_cross_engine_read_write_netcdf3(self):\n        data = create_test_data()\n        valid_engines = set()\n        if has_netCDF4:\n            valid_engines.add(""netcdf4"")\n        if has_scipy:\n            valid_engines.add(""scipy"")\n\n        for write_engine in valid_engines:\n            for format in self.netcdf3_formats:\n                with create_tmp_file() as tmp_file:\n                    data.to_netcdf(tmp_file, format=format, engine=write_engine)\n                    for read_engine in valid_engines:\n                        with open_dataset(tmp_file, engine=read_engine) as actual:\n                            # hack to allow test to work:\n                            # coord comes back as DataArray rather than coord,\n                            # and so need to loop through here rather than in\n                            # the test function (or we get recursion)\n                            [\n                                assert_allclose(data[k].variable, actual[k].variable)\n                                for k in data.variables\n                            ]\n\n    def test_encoding_unlimited_dims(self):\n        ds = Dataset({""x"": (""y"", np.arange(10.0))})\n        with self.roundtrip(ds, save_kwargs=dict(unlimited_dims=[""y""])) as actual:\n            assert actual.encoding[""unlimited_dims""] == set(""y"")\n            assert_equal(ds, actual)\n\n        # Regression test for https://github.com/pydata/xarray/issues/2134\n        with self.roundtrip(ds, save_kwargs=dict(unlimited_dims=""y"")) as actual:\n            assert actual.encoding[""unlimited_dims""] == set(""y"")\n            assert_equal(ds, actual)\n\n        ds.encoding = {""unlimited_dims"": [""y""]}\n        with self.roundtrip(ds) as actual:\n            assert actual.encoding[""unlimited_dims""] == set(""y"")\n            assert_equal(ds, actual)\n\n        # Regression test for https://github.com/pydata/xarray/issues/2134\n        ds.encoding = {""unlimited_dims"": ""y""}\n        with self.roundtrip(ds) as actual:\n            assert actual.encoding[""unlimited_dims""] == set(""y"")\n            assert_equal(ds, actual)\n\n\n@requires_h5netcdf\n@requires_netCDF4\n@pytest.mark.filterwarnings(""ignore:use make_scale(name) instead"")\nclass TestH5NetCDFData(NetCDF4Base):\n    engine = ""h5netcdf""\n\n    @contextlib.contextmanager\n    def create_store(self):\n        with create_tmp_file() as tmp_file:\n            yield backends.H5NetCDFStore.open(tmp_file, ""w"")\n\n    @pytest.mark.filterwarnings(""ignore:complex dtypes are supported by h5py"")\n    @pytest.mark.parametrize(\n        ""invalid_netcdf, warntype, num_warns"",\n        [(None, FutureWarning, 1), (False, FutureWarning, 1), (True, None, 0)],\n    )\n    def test_complex(self, invalid_netcdf, warntype, num_warns):\n        expected = Dataset({""x"": (""y"", np.ones(5) + 1j * np.ones(5))})\n        save_kwargs = {""invalid_netcdf"": invalid_netcdf}\n        with pytest.warns(warntype) as record:\n            with self.roundtrip(expected, save_kwargs=save_kwargs) as actual:\n                assert_equal(expected, actual)\n\n        recorded_num_warns = 0\n        if warntype:\n            for warning in record:\n                if issubclass(warning.category, warntype) and (\n                    ""complex dtypes"" in str(warning.message)\n                ):\n                    recorded_num_warns += 1\n\n        assert recorded_num_warns == num_warns\n\n    def test_cross_engine_read_write_netcdf4(self):\n        # Drop dim3, because its labels include strings. These appear to be\n        # not properly read with python-netCDF4, which converts them into\n        # unicode instead of leaving them as bytes.\n        data = create_test_data().drop_vars(""dim3"")\n        data.attrs[""foo""] = ""bar""\n        valid_engines = [""netcdf4"", ""h5netcdf""]\n        for write_engine in valid_engines:\n            with create_tmp_file() as tmp_file:\n                data.to_netcdf(tmp_file, engine=write_engine)\n                for read_engine in valid_engines:\n                    with open_dataset(tmp_file, engine=read_engine) as actual:\n                        assert_identical(data, actual)\n\n    def test_read_byte_attrs_as_unicode(self):\n        with create_tmp_file() as tmp_file:\n            with nc4.Dataset(tmp_file, ""w"") as nc:\n                nc.foo = b""bar""\n            with open_dataset(tmp_file) as actual:\n                expected = Dataset(attrs={""foo"": ""bar""})\n                assert_identical(expected, actual)\n\n    def test_encoding_unlimited_dims(self):\n        ds = Dataset({""x"": (""y"", np.arange(10.0))})\n        with self.roundtrip(ds, save_kwargs=dict(unlimited_dims=[""y""])) as actual:\n            assert actual.encoding[""unlimited_dims""] == set(""y"")\n            assert_equal(ds, actual)\n        ds.encoding = {""unlimited_dims"": [""y""]}\n        with self.roundtrip(ds) as actual:\n            assert actual.encoding[""unlimited_dims""] == set(""y"")\n            assert_equal(ds, actual)\n\n    def test_compression_encoding_h5py(self):\n        ENCODINGS = (\n            # h5py style compression with gzip codec will be converted to\n            # NetCDF4-Python style on round-trip\n            (\n                {""compression"": ""gzip"", ""compression_opts"": 9},\n                {""zlib"": True, ""complevel"": 9},\n            ),\n            # What can\'t be expressed in NetCDF4-Python style is\n            # round-tripped unaltered\n            (\n                {""compression"": ""lzf"", ""compression_opts"": None},\n                {""compression"": ""lzf"", ""compression_opts"": None},\n            ),\n            # If both styles are used together, h5py format takes precedence\n            (\n                {\n                    ""compression"": ""lzf"",\n                    ""compression_opts"": None,\n                    ""zlib"": True,\n                    ""complevel"": 9,\n                },\n                {""compression"": ""lzf"", ""compression_opts"": None},\n            ),\n        )\n\n        for compr_in, compr_out in ENCODINGS:\n            data = create_test_data()\n            compr_common = {\n                ""chunksizes"": (5, 5),\n                ""fletcher32"": True,\n                ""shuffle"": True,\n                ""original_shape"": data.var2.shape,\n            }\n            data[""var2""].encoding.update(compr_in)\n            data[""var2""].encoding.update(compr_common)\n            compr_out.update(compr_common)\n            data[""scalar""] = (""scalar_dim"", np.array([2.0]))\n            data[""scalar""] = data[""scalar""][0]\n            with self.roundtrip(data) as actual:\n                for k, v in compr_out.items():\n                    assert v == actual[""var2""].encoding[k]\n\n    def test_compression_check_encoding_h5py(self):\n        """"""When mismatched h5py and NetCDF4-Python encodings are expressed\n        in to_netcdf(encoding=...), must raise ValueError\n        """"""\n        data = Dataset({""x"": (""y"", np.arange(10.0))})\n        # Compatible encodings are graciously supported\n        with create_tmp_file() as tmp_file:\n            data.to_netcdf(\n                tmp_file,\n                engine=""h5netcdf"",\n                encoding={\n                    ""x"": {\n                        ""compression"": ""gzip"",\n                        ""zlib"": True,\n                        ""compression_opts"": 6,\n                        ""complevel"": 6,\n                    }\n                },\n            )\n            with open_dataset(tmp_file, engine=""h5netcdf"") as actual:\n                assert actual.x.encoding[""zlib""] is True\n                assert actual.x.encoding[""complevel""] == 6\n\n        # Incompatible encodings cause a crash\n        with create_tmp_file() as tmp_file:\n            with raises_regex(\n                ValueError, ""\'zlib\' and \'compression\' encodings mismatch""\n            ):\n                data.to_netcdf(\n                    tmp_file,\n                    engine=""h5netcdf"",\n                    encoding={""x"": {""compression"": ""lzf"", ""zlib"": True}},\n                )\n\n        with create_tmp_file() as tmp_file:\n            with raises_regex(\n                ValueError, ""\'complevel\' and \'compression_opts\' encodings mismatch""\n            ):\n                data.to_netcdf(\n                    tmp_file,\n                    engine=""h5netcdf"",\n                    encoding={\n                        ""x"": {\n                            ""compression"": ""gzip"",\n                            ""compression_opts"": 5,\n                            ""complevel"": 6,\n                        }\n                    },\n                )\n\n    def test_dump_encodings_h5py(self):\n        # regression test for #709\n        ds = Dataset({""x"": (""y"", np.arange(10.0))})\n\n        kwargs = {""encoding"": {""x"": {""compression"": ""gzip"", ""compression_opts"": 9}}}\n        with self.roundtrip(ds, save_kwargs=kwargs) as actual:\n            assert actual.x.encoding[""zlib""]\n            assert actual.x.encoding[""complevel""] == 9\n\n        kwargs = {""encoding"": {""x"": {""compression"": ""lzf"", ""compression_opts"": None}}}\n        with self.roundtrip(ds, save_kwargs=kwargs) as actual:\n            assert actual.x.encoding[""compression""] == ""lzf""\n            assert actual.x.encoding[""compression_opts""] is None\n\n    def test_already_open_dataset_group(self):\n        import h5netcdf\n\n        with create_tmp_file() as tmp_file:\n            with nc4.Dataset(tmp_file, mode=""w"") as nc:\n                group = nc.createGroup(""g"")\n                v = group.createVariable(""x"", ""int"")\n                v[...] = 42\n\n            h5 = h5netcdf.File(tmp_file, mode=""r"")\n            store = backends.H5NetCDFStore(h5[""g""])\n            with open_dataset(store) as ds:\n                expected = Dataset({""x"": ((), 42)})\n                assert_identical(expected, ds)\n\n            h5 = h5netcdf.File(tmp_file, mode=""r"")\n            store = backends.H5NetCDFStore(h5, group=""g"")\n            with open_dataset(store) as ds:\n                expected = Dataset({""x"": ((), 42)})\n                assert_identical(expected, ds)\n\n\n@requires_h5netcdf\nclass TestH5NetCDFFileObject(TestH5NetCDFData):\n    engine = ""h5netcdf""\n\n    def test_open_badbytes(self):\n        with raises_regex(ValueError, ""HDF5 as bytes""):\n            with open_dataset(b""\\211HDF\\r\\n\\032\\n"", engine=""h5netcdf""):\n                pass\n        with raises_regex(ValueError, ""not a valid netCDF""):\n            with open_dataset(b""garbage""):\n                pass\n        with raises_regex(ValueError, ""can only read bytes""):\n            with open_dataset(b""garbage"", engine=""netcdf4""):\n                pass\n        with raises_regex(ValueError, ""not a valid netCDF""):\n            with open_dataset(BytesIO(b""garbage""), engine=""h5netcdf""):\n                pass\n\n    def test_open_twice(self):\n        expected = create_test_data()\n        expected.attrs[""foo""] = ""bar""\n        with raises_regex(ValueError, ""read/write pointer not at zero""):\n            with create_tmp_file() as tmp_file:\n                expected.to_netcdf(tmp_file, engine=""h5netcdf"")\n                with open(tmp_file, ""rb"") as f:\n                    with open_dataset(f, engine=""h5netcdf""):\n                        with open_dataset(f, engine=""h5netcdf""):\n                            pass\n\n    def test_open_fileobj(self):\n        # open in-memory datasets instead of local file paths\n        expected = create_test_data().drop_vars(""dim3"")\n        expected.attrs[""foo""] = ""bar""\n        with create_tmp_file() as tmp_file:\n            expected.to_netcdf(tmp_file, engine=""h5netcdf"")\n\n            with open(tmp_file, ""rb"") as f:\n                with open_dataset(f, engine=""h5netcdf"") as actual:\n                    assert_identical(expected, actual)\n\n                f.seek(0)\n                with BytesIO(f.read()) as bio:\n                    with open_dataset(bio, engine=""h5netcdf"") as actual:\n                        assert_identical(expected, actual)\n\n\n@requires_h5netcdf\n@requires_dask\n@pytest.mark.filterwarnings(""ignore:deallocating CachingFileManager"")\nclass TestH5NetCDFViaDaskData(TestH5NetCDFData):\n    @contextlib.contextmanager\n    def roundtrip(\n        self, data, save_kwargs=None, open_kwargs=None, allow_cleanup_failure=False\n    ):\n        if save_kwargs is None:\n            save_kwargs = {}\n        if open_kwargs is None:\n            open_kwargs = {}\n        open_kwargs.setdefault(""chunks"", -1)\n        with TestH5NetCDFData.roundtrip(\n            self, data, save_kwargs, open_kwargs, allow_cleanup_failure\n        ) as ds:\n            yield ds\n\n    def test_dataset_caching(self):\n        # caching behavior differs for dask\n        pass\n\n    def test_write_inconsistent_chunks(self):\n        # Construct two variables with the same dimensions, but different\n        # chunk sizes.\n        x = da.zeros((100, 100), dtype=""f4"", chunks=(50, 100))\n        x = DataArray(data=x, dims=(""lat"", ""lon""), name=""x"")\n        x.encoding[""chunksizes""] = (50, 100)\n        x.encoding[""original_shape""] = (100, 100)\n        y = da.ones((100, 100), dtype=""f4"", chunks=(100, 50))\n        y = DataArray(data=y, dims=(""lat"", ""lon""), name=""y"")\n        y.encoding[""chunksizes""] = (100, 50)\n        y.encoding[""original_shape""] = (100, 100)\n        # Put them both into the same dataset\n        ds = Dataset({""x"": x, ""y"": y})\n        with self.roundtrip(ds) as actual:\n            assert actual[""x""].encoding[""chunksizes""] == (50, 100)\n            assert actual[""y""].encoding[""chunksizes""] == (100, 50)\n\n\n@pytest.fixture(params=[""scipy"", ""netcdf4"", ""h5netcdf"", ""pynio""])\ndef readengine(request):\n    return request.param\n\n\n@pytest.fixture(params=[1, 20])\ndef nfiles(request):\n    return request.param\n\n\n@pytest.fixture(params=[5, None])\ndef file_cache_maxsize(request):\n    maxsize = request.param\n    if maxsize is not None:\n        with set_options(file_cache_maxsize=maxsize):\n            yield maxsize\n    else:\n        yield maxsize\n\n\n@pytest.fixture(params=[True, False])\ndef parallel(request):\n    return request.param\n\n\n@pytest.fixture(params=[None, 5])\ndef chunks(request):\n    return request.param\n\n\n# using pytest.mark.skipif does not work so this a work around\ndef skip_if_not_engine(engine):\n    if engine == ""netcdf4"":\n        pytest.importorskip(""netCDF4"")\n    elif engine == ""pynio"":\n        pytest.importorskip(""Nio"")\n    else:\n        pytest.importorskip(engine)\n\n\n@requires_dask\n@pytest.mark.filterwarnings(""ignore:use make_scale(name) instead"")\ndef test_open_mfdataset_manyfiles(\n    readengine, nfiles, parallel, chunks, file_cache_maxsize\n):\n\n    # skip certain combinations\n    skip_if_not_engine(readengine)\n\n    if ON_WINDOWS:\n        pytest.skip(""Skipping on Windows"")\n\n    randdata = np.random.randn(nfiles)\n    original = Dataset({""foo"": (""x"", randdata)})\n    # test standard open_mfdataset approach with too many files\n    with create_tmp_files(nfiles) as tmpfiles:\n        writeengine = readengine if readengine != ""pynio"" else ""netcdf4""\n        # split into multiple sets of temp files\n        for ii in original.x.values:\n            subds = original.isel(x=slice(ii, ii + 1))\n            subds.to_netcdf(tmpfiles[ii], engine=writeengine)\n\n        # check that calculation on opened datasets works properly\n        with open_mfdataset(\n            tmpfiles,\n            combine=""nested"",\n            concat_dim=""x"",\n            engine=readengine,\n            parallel=parallel,\n            chunks=chunks,\n        ) as actual:\n\n            # check that using open_mfdataset returns dask arrays for variables\n            assert isinstance(actual[""foo""].data, dask_array_type)\n\n            assert_identical(original, actual)\n\n\n@requires_netCDF4\n@requires_dask\ndef test_open_mfdataset_list_attr():\n    """"""\n    Case when an attribute of type list differs across the multiple files\n    """"""\n    from netCDF4 import Dataset\n\n    with create_tmp_files(2) as nfiles:\n        for i in range(2):\n            f = Dataset(nfiles[i], ""w"")\n            f.createDimension(""x"", 3)\n            vlvar = f.createVariable(""test_var"", np.int32, (""x""))\n            # here create an attribute as a list\n            vlvar.test_attr = [f""string a {i}"", f""string b {i}""]\n            vlvar[:] = np.arange(3)\n            f.close()\n        ds1 = open_dataset(nfiles[0])\n        ds2 = open_dataset(nfiles[1])\n        original = xr.concat([ds1, ds2], dim=""x"")\n        with xr.open_mfdataset(\n            [nfiles[0], nfiles[1]], combine=""nested"", concat_dim=""x""\n        ) as actual:\n            assert_identical(actual, original)\n\n\n@requires_scipy_or_netCDF4\n@requires_dask\nclass TestOpenMFDatasetWithDataVarsAndCoordsKw:\n    coord_name = ""lon""\n    var_name = ""v1""\n\n    @contextlib.contextmanager\n    def setup_files_and_datasets(self, fuzz=0):\n        ds1, ds2 = self.gen_datasets_with_common_coord_and_time()\n\n        # to test join=\'exact\'\n        ds1[""x""] = ds1.x + fuzz\n\n        with create_tmp_file() as tmpfile1:\n            with create_tmp_file() as tmpfile2:\n\n                # save data to the temporary files\n                ds1.to_netcdf(tmpfile1)\n                ds2.to_netcdf(tmpfile2)\n\n                yield [tmpfile1, tmpfile2], [ds1, ds2]\n\n    def gen_datasets_with_common_coord_and_time(self):\n        # create coordinate data\n        nx = 10\n        nt = 10\n        x = np.arange(nx)\n        t1 = np.arange(nt)\n        t2 = np.arange(nt, 2 * nt, 1)\n\n        v1 = np.random.randn(nt, nx)\n        v2 = np.random.randn(nt, nx)\n\n        ds1 = Dataset(\n            data_vars={self.var_name: ([""t"", ""x""], v1), self.coord_name: (""x"", 2 * x)},\n            coords={""t"": ([""t""], t1), ""x"": ([""x""], x)},\n        )\n\n        ds2 = Dataset(\n            data_vars={self.var_name: ([""t"", ""x""], v2), self.coord_name: (""x"", 2 * x)},\n            coords={""t"": ([""t""], t2), ""x"": ([""x""], x)},\n        )\n\n        return ds1, ds2\n\n    @pytest.mark.parametrize(""combine"", [""nested"", ""by_coords""])\n    @pytest.mark.parametrize(""opt"", [""all"", ""minimal"", ""different""])\n    @pytest.mark.parametrize(""join"", [""outer"", ""inner"", ""left"", ""right""])\n    def test_open_mfdataset_does_same_as_concat(self, combine, opt, join):\n        with self.setup_files_and_datasets() as (files, [ds1, ds2]):\n            if combine == ""by_coords"":\n                files.reverse()\n            with open_mfdataset(\n                files, data_vars=opt, combine=combine, concat_dim=""t"", join=join\n            ) as ds:\n                ds_expect = xr.concat([ds1, ds2], data_vars=opt, dim=""t"", join=join)\n                assert_identical(ds, ds_expect)\n\n    @pytest.mark.parametrize(""combine"", [""nested"", ""by_coords""])\n    @pytest.mark.parametrize(""opt"", [""all"", ""minimal"", ""different""])\n    def test_open_mfdataset_exact_join_raises_error(self, combine, opt):\n        with self.setup_files_and_datasets(fuzz=0.1) as (files, [ds1, ds2]):\n            if combine == ""by_coords"":\n                files.reverse()\n            with raises_regex(ValueError, ""indexes along dimension""):\n                open_mfdataset(\n                    files, data_vars=opt, combine=combine, concat_dim=""t"", join=""exact""\n                )\n\n    def test_common_coord_when_datavars_all(self):\n        opt = ""all""\n\n        with self.setup_files_and_datasets() as (files, [ds1, ds2]):\n            # open the files with the data_var option\n            with open_mfdataset(\n                files, data_vars=opt, combine=""nested"", concat_dim=""t""\n            ) as ds:\n\n                coord_shape = ds[self.coord_name].shape\n                coord_shape1 = ds1[self.coord_name].shape\n                coord_shape2 = ds2[self.coord_name].shape\n\n                var_shape = ds[self.var_name].shape\n\n                assert var_shape == coord_shape\n                assert coord_shape1 != coord_shape\n                assert coord_shape2 != coord_shape\n\n    def test_common_coord_when_datavars_minimal(self):\n        opt = ""minimal""\n\n        with self.setup_files_and_datasets() as (files, [ds1, ds2]):\n            # open the files using data_vars option\n            with open_mfdataset(\n                files, data_vars=opt, combine=""nested"", concat_dim=""t""\n            ) as ds:\n\n                coord_shape = ds[self.coord_name].shape\n                coord_shape1 = ds1[self.coord_name].shape\n                coord_shape2 = ds2[self.coord_name].shape\n\n                var_shape = ds[self.var_name].shape\n\n                assert var_shape != coord_shape\n                assert coord_shape1 == coord_shape\n                assert coord_shape2 == coord_shape\n\n    def test_invalid_data_vars_value_should_fail(self):\n\n        with self.setup_files_and_datasets() as (files, _):\n            with pytest.raises(ValueError):\n                with open_mfdataset(files, data_vars=""minimum"", combine=""by_coords""):\n                    pass\n\n            # test invalid coord parameter\n            with pytest.raises(ValueError):\n                with open_mfdataset(files, coords=""minimum"", combine=""by_coords""):\n                    pass\n\n\n@requires_dask\n@requires_scipy\n@requires_netCDF4\nclass TestDask(DatasetIOBase):\n    @contextlib.contextmanager\n    def create_store(self):\n        yield Dataset()\n\n    @contextlib.contextmanager\n    def roundtrip(\n        self, data, save_kwargs=None, open_kwargs=None, allow_cleanup_failure=False\n    ):\n        yield data.chunk()\n\n    # Override methods in DatasetIOBase - not applicable to dask\n    def test_roundtrip_string_encoded_characters(self):\n        pass\n\n    def test_roundtrip_coordinates_with_space(self):\n        pass\n\n    def test_roundtrip_numpy_datetime_data(self):\n        # Override method in DatasetIOBase - remove not applicable\n        # save_kwargs\n        times = pd.to_datetime([""2000-01-01"", ""2000-01-02"", ""NaT""])\n        expected = Dataset({""t"": (""t"", times), ""t0"": times[0]})\n        with self.roundtrip(expected) as actual:\n            assert_identical(expected, actual)\n\n    def test_roundtrip_cftime_datetime_data(self):\n        # Override method in DatasetIOBase - remove not applicable\n        # save_kwargs\n        from .test_coding_times import _all_cftime_date_types\n\n        date_types = _all_cftime_date_types()\n        for date_type in date_types.values():\n            times = [date_type(1, 1, 1), date_type(1, 1, 2)]\n            expected = Dataset({""t"": (""t"", times), ""t0"": times[0]})\n            expected_decoded_t = np.array(times)\n            expected_decoded_t0 = np.array([date_type(1, 1, 1)])\n\n            with self.roundtrip(expected) as actual:\n                abs_diff = abs(actual.t.values - expected_decoded_t)\n                assert (abs_diff <= np.timedelta64(1, ""s"")).all()\n\n                abs_diff = abs(actual.t0.values - expected_decoded_t0)\n                assert (abs_diff <= np.timedelta64(1, ""s"")).all()\n\n    def test_write_store(self):\n        # Override method in DatasetIOBase - not applicable to dask\n        pass\n\n    def test_dataset_caching(self):\n        expected = Dataset({""foo"": (""x"", [5, 6, 7])})\n        with self.roundtrip(expected) as actual:\n            assert not actual.foo.variable._in_memory\n            actual.foo.values  # no caching\n            assert not actual.foo.variable._in_memory\n\n    def test_open_mfdataset(self):\n        original = Dataset({""foo"": (""x"", np.random.randn(10))})\n        with create_tmp_file() as tmp1:\n            with create_tmp_file() as tmp2:\n                original.isel(x=slice(5)).to_netcdf(tmp1)\n                original.isel(x=slice(5, 10)).to_netcdf(tmp2)\n                with open_mfdataset(\n                    [tmp1, tmp2], concat_dim=""x"", combine=""nested""\n                ) as actual:\n                    assert isinstance(actual.foo.variable.data, da.Array)\n                    assert actual.foo.variable.data.chunks == ((5, 5),)\n                    assert_identical(original, actual)\n                with open_mfdataset(\n                    [tmp1, tmp2], concat_dim=""x"", combine=""nested"", chunks={""x"": 3}\n                ) as actual:\n                    assert actual.foo.variable.data.chunks == ((3, 2, 3, 2),)\n\n        with raises_regex(IOError, ""no files to open""):\n            open_mfdataset(""foo-bar-baz-*.nc"")\n\n        with raises_regex(ValueError, ""wild-card""):\n            open_mfdataset(""http://some/remote/uri"")\n\n    def test_open_mfdataset_2d(self):\n        original = Dataset({""foo"": ([""x"", ""y""], np.random.randn(10, 8))})\n        with create_tmp_file() as tmp1:\n            with create_tmp_file() as tmp2:\n                with create_tmp_file() as tmp3:\n                    with create_tmp_file() as tmp4:\n                        original.isel(x=slice(5), y=slice(4)).to_netcdf(tmp1)\n                        original.isel(x=slice(5, 10), y=slice(4)).to_netcdf(tmp2)\n                        original.isel(x=slice(5), y=slice(4, 8)).to_netcdf(tmp3)\n                        original.isel(x=slice(5, 10), y=slice(4, 8)).to_netcdf(tmp4)\n                        with open_mfdataset(\n                            [[tmp1, tmp2], [tmp3, tmp4]],\n                            combine=""nested"",\n                            concat_dim=[""y"", ""x""],\n                        ) as actual:\n                            assert isinstance(actual.foo.variable.data, da.Array)\n                            assert actual.foo.variable.data.chunks == ((5, 5), (4, 4))\n                            assert_identical(original, actual)\n                        with open_mfdataset(\n                            [[tmp1, tmp2], [tmp3, tmp4]],\n                            combine=""nested"",\n                            concat_dim=[""y"", ""x""],\n                            chunks={""x"": 3, ""y"": 2},\n                        ) as actual:\n                            assert actual.foo.variable.data.chunks == (\n                                (3, 2, 3, 2),\n                                (2, 2, 2, 2),\n                            )\n\n    def test_open_mfdataset_pathlib(self):\n        original = Dataset({""foo"": (""x"", np.random.randn(10))})\n        with create_tmp_file() as tmp1:\n            with create_tmp_file() as tmp2:\n                tmp1 = Path(tmp1)\n                tmp2 = Path(tmp2)\n                original.isel(x=slice(5)).to_netcdf(tmp1)\n                original.isel(x=slice(5, 10)).to_netcdf(tmp2)\n                with open_mfdataset(\n                    [tmp1, tmp2], concat_dim=""x"", combine=""nested""\n                ) as actual:\n                    assert_identical(original, actual)\n\n    def test_open_mfdataset_2d_pathlib(self):\n        original = Dataset({""foo"": ([""x"", ""y""], np.random.randn(10, 8))})\n        with create_tmp_file() as tmp1:\n            with create_tmp_file() as tmp2:\n                with create_tmp_file() as tmp3:\n                    with create_tmp_file() as tmp4:\n                        tmp1 = Path(tmp1)\n                        tmp2 = Path(tmp2)\n                        tmp3 = Path(tmp3)\n                        tmp4 = Path(tmp4)\n                        original.isel(x=slice(5), y=slice(4)).to_netcdf(tmp1)\n                        original.isel(x=slice(5, 10), y=slice(4)).to_netcdf(tmp2)\n                        original.isel(x=slice(5), y=slice(4, 8)).to_netcdf(tmp3)\n                        original.isel(x=slice(5, 10), y=slice(4, 8)).to_netcdf(tmp4)\n                        with open_mfdataset(\n                            [[tmp1, tmp2], [tmp3, tmp4]],\n                            combine=""nested"",\n                            concat_dim=[""y"", ""x""],\n                        ) as actual:\n                            assert_identical(original, actual)\n\n    def test_open_mfdataset_2(self):\n        original = Dataset({""foo"": (""x"", np.random.randn(10))})\n        with create_tmp_file() as tmp1:\n            with create_tmp_file() as tmp2:\n                original.isel(x=slice(5)).to_netcdf(tmp1)\n                original.isel(x=slice(5, 10)).to_netcdf(tmp2)\n\n                with open_mfdataset(\n                    [tmp1, tmp2], concat_dim=""x"", combine=""nested""\n                ) as actual:\n                    assert_identical(original, actual)\n\n    def test_attrs_mfdataset(self):\n        original = Dataset({""foo"": (""x"", np.random.randn(10))})\n        with create_tmp_file() as tmp1:\n            with create_tmp_file() as tmp2:\n                ds1 = original.isel(x=slice(5))\n                ds2 = original.isel(x=slice(5, 10))\n                ds1.attrs[""test1""] = ""foo""\n                ds2.attrs[""test2""] = ""bar""\n                ds1.to_netcdf(tmp1)\n                ds2.to_netcdf(tmp2)\n                with open_mfdataset(\n                    [tmp1, tmp2], concat_dim=""x"", combine=""nested""\n                ) as actual:\n                    # presumes that attributes inherited from\n                    # first dataset loaded\n                    assert actual.test1 == ds1.test1\n                    # attributes from ds2 are not retained, e.g.,\n                    with raises_regex(AttributeError, ""no attribute""):\n                        actual.test2\n\n    def test_open_mfdataset_attrs_file(self):\n        original = Dataset({""foo"": (""x"", np.random.randn(10))})\n        with create_tmp_files(2) as (tmp1, tmp2):\n            ds1 = original.isel(x=slice(5))\n            ds2 = original.isel(x=slice(5, 10))\n            ds1.attrs[""test1""] = ""foo""\n            ds2.attrs[""test2""] = ""bar""\n            ds1.to_netcdf(tmp1)\n            ds2.to_netcdf(tmp2)\n            with open_mfdataset(\n                [tmp1, tmp2], concat_dim=""x"", combine=""nested"", attrs_file=tmp2\n            ) as actual:\n                # attributes are inherited from the master file\n                assert actual.attrs[""test2""] == ds2.attrs[""test2""]\n                # attributes from ds1 are not retained, e.g.,\n                assert ""test1"" not in actual.attrs\n\n    def test_open_mfdataset_attrs_file_path(self):\n        original = Dataset({""foo"": (""x"", np.random.randn(10))})\n        with create_tmp_files(2) as (tmp1, tmp2):\n            tmp1 = Path(tmp1)\n            tmp2 = Path(tmp2)\n            ds1 = original.isel(x=slice(5))\n            ds2 = original.isel(x=slice(5, 10))\n            ds1.attrs[""test1""] = ""foo""\n            ds2.attrs[""test2""] = ""bar""\n            ds1.to_netcdf(tmp1)\n            ds2.to_netcdf(tmp2)\n            with open_mfdataset(\n                [tmp1, tmp2], concat_dim=""x"", combine=""nested"", attrs_file=tmp2\n            ) as actual:\n                # attributes are inherited from the master file\n                assert actual.attrs[""test2""] == ds2.attrs[""test2""]\n                # attributes from ds1 are not retained, e.g.,\n                assert ""test1"" not in actual.attrs\n\n    def test_open_mfdataset_auto_combine(self):\n        original = Dataset({""foo"": (""x"", np.random.randn(10)), ""x"": np.arange(10)})\n        with create_tmp_file() as tmp1:\n            with create_tmp_file() as tmp2:\n                original.isel(x=slice(5)).to_netcdf(tmp1)\n                original.isel(x=slice(5, 10)).to_netcdf(tmp2)\n\n                with open_mfdataset([tmp2, tmp1], combine=""by_coords"") as actual:\n                    assert_identical(original, actual)\n\n    def test_open_mfdataset_combine_nested_no_concat_dim(self):\n        original = Dataset({""foo"": (""x"", np.random.randn(10)), ""x"": np.arange(10)})\n        with create_tmp_file() as tmp1:\n            with create_tmp_file() as tmp2:\n                original.isel(x=slice(5)).to_netcdf(tmp1)\n                original.isel(x=slice(5, 10)).to_netcdf(tmp2)\n\n                with raises_regex(ValueError, ""Must supply concat_dim""):\n                    open_mfdataset([tmp2, tmp1], combine=""nested"")\n\n    @pytest.mark.xfail(reason=""mfdataset loses encoding currently."")\n    def test_encoding_mfdataset(self):\n        original = Dataset(\n            {\n                ""foo"": (""t"", np.random.randn(10)),\n                ""t"": (""t"", pd.date_range(start=""2010-01-01"", periods=10, freq=""1D"")),\n            }\n        )\n        original.t.encoding[""units""] = ""days since 2010-01-01""\n\n        with create_tmp_file() as tmp1:\n            with create_tmp_file() as tmp2:\n                ds1 = original.isel(t=slice(5))\n                ds2 = original.isel(t=slice(5, 10))\n                ds1.t.encoding[""units""] = ""days since 2010-01-01""\n                ds2.t.encoding[""units""] = ""days since 2000-01-01""\n                ds1.to_netcdf(tmp1)\n                ds2.to_netcdf(tmp2)\n                with open_mfdataset([tmp1, tmp2], combine=""nested"") as actual:\n                    assert actual.t.encoding[""units""] == original.t.encoding[""units""]\n                    assert actual.t.encoding[""units""] == ds1.t.encoding[""units""]\n                    assert actual.t.encoding[""units""] != ds2.t.encoding[""units""]\n\n    def test_preprocess_mfdataset(self):\n        original = Dataset({""foo"": (""x"", np.random.randn(10))})\n        with create_tmp_file() as tmp:\n            original.to_netcdf(tmp)\n\n            def preprocess(ds):\n                return ds.assign_coords(z=0)\n\n            expected = preprocess(original)\n            with open_mfdataset(\n                tmp, preprocess=preprocess, combine=""by_coords""\n            ) as actual:\n                assert_identical(expected, actual)\n\n    def test_save_mfdataset_roundtrip(self):\n        original = Dataset({""foo"": (""x"", np.random.randn(10))})\n        datasets = [original.isel(x=slice(5)), original.isel(x=slice(5, 10))]\n        with create_tmp_file() as tmp1:\n            with create_tmp_file() as tmp2:\n                save_mfdataset(datasets, [tmp1, tmp2])\n                with open_mfdataset(\n                    [tmp1, tmp2], concat_dim=""x"", combine=""nested""\n                ) as actual:\n                    assert_identical(actual, original)\n\n    def test_save_mfdataset_invalid(self):\n        ds = Dataset()\n        with raises_regex(ValueError, ""cannot use mode""):\n            save_mfdataset([ds, ds], [""same"", ""same""])\n        with raises_regex(ValueError, ""same length""):\n            save_mfdataset([ds, ds], [""only one path""])\n\n    def test_save_mfdataset_invalid_dataarray(self):\n        # regression test for GH1555\n        da = DataArray([1, 2])\n        with raises_regex(TypeError, ""supports writing Dataset""):\n            save_mfdataset([da], [""dataarray""])\n\n    def test_save_mfdataset_pathlib_roundtrip(self):\n        original = Dataset({""foo"": (""x"", np.random.randn(10))})\n        datasets = [original.isel(x=slice(5)), original.isel(x=slice(5, 10))]\n        with create_tmp_file() as tmp1:\n            with create_tmp_file() as tmp2:\n                tmp1 = Path(tmp1)\n                tmp2 = Path(tmp2)\n                save_mfdataset(datasets, [tmp1, tmp2])\n                with open_mfdataset(\n                    [tmp1, tmp2], concat_dim=""x"", combine=""nested""\n                ) as actual:\n                    assert_identical(actual, original)\n\n    def test_open_and_do_math(self):\n        original = Dataset({""foo"": (""x"", np.random.randn(10))})\n        with create_tmp_file() as tmp:\n            original.to_netcdf(tmp)\n            with open_mfdataset(tmp, combine=""by_coords"") as ds:\n                actual = 1.0 * ds\n                assert_allclose(original, actual, decode_bytes=False)\n\n    def test_open_mfdataset_concat_dim_none(self):\n        with create_tmp_file() as tmp1:\n            with create_tmp_file() as tmp2:\n                data = Dataset({""x"": 0})\n                data.to_netcdf(tmp1)\n                Dataset({""x"": np.nan}).to_netcdf(tmp2)\n                with open_mfdataset(\n                    [tmp1, tmp2], concat_dim=None, combine=""nested""\n                ) as actual:\n                    assert_identical(data, actual)\n\n    def test_open_dataset(self):\n        original = Dataset({""foo"": (""x"", np.random.randn(10))})\n        with create_tmp_file() as tmp:\n            original.to_netcdf(tmp)\n            with open_dataset(tmp, chunks={""x"": 5}) as actual:\n                assert isinstance(actual.foo.variable.data, da.Array)\n                assert actual.foo.variable.data.chunks == ((5, 5),)\n                assert_identical(original, actual)\n            with open_dataset(tmp, chunks=5) as actual:\n                assert_identical(original, actual)\n            with open_dataset(tmp) as actual:\n                assert isinstance(actual.foo.variable.data, np.ndarray)\n                assert_identical(original, actual)\n\n    def test_open_single_dataset(self):\n        # Test for issue GH #1988. This makes sure that the\n        # concat_dim is utilized when specified in open_mfdataset().\n        rnddata = np.random.randn(10)\n        original = Dataset({""foo"": (""x"", rnddata)})\n        dim = DataArray([100], name=""baz"", dims=""baz"")\n        expected = Dataset(\n            {""foo"": ((""baz"", ""x""), rnddata[np.newaxis, :])}, {""baz"": [100]}\n        )\n        with create_tmp_file() as tmp:\n            original.to_netcdf(tmp)\n            with open_mfdataset([tmp], concat_dim=dim, combine=""nested"") as actual:\n                assert_identical(expected, actual)\n\n    def test_open_multi_dataset(self):\n        # Test for issue GH #1988 and #2647. This makes sure that the\n        # concat_dim is utilized when specified in open_mfdataset().\n        # The additional wrinkle is to ensure that a length greater\n        # than one is tested as well due to numpy\'s implicit casting\n        # of 1-length arrays to booleans in tests, which allowed\n        # #2647 to still pass the test_open_single_dataset(),\n        # which is itself still needed as-is because the original\n        # bug caused one-length arrays to not be used correctly\n        # in concatenation.\n        rnddata = np.random.randn(10)\n        original = Dataset({""foo"": (""x"", rnddata)})\n        dim = DataArray([100, 150], name=""baz"", dims=""baz"")\n        expected = Dataset(\n            {""foo"": ((""baz"", ""x""), np.tile(rnddata[np.newaxis, :], (2, 1)))},\n            {""baz"": [100, 150]},\n        )\n        with create_tmp_file() as tmp1, create_tmp_file() as tmp2:\n            original.to_netcdf(tmp1)\n            original.to_netcdf(tmp2)\n            with open_mfdataset(\n                [tmp1, tmp2], concat_dim=dim, combine=""nested""\n            ) as actual:\n                assert_identical(expected, actual)\n\n    def test_dask_roundtrip(self):\n        with create_tmp_file() as tmp:\n            data = create_test_data()\n            data.to_netcdf(tmp)\n            chunks = {""dim1"": 4, ""dim2"": 4, ""dim3"": 4, ""time"": 10}\n            with open_dataset(tmp, chunks=chunks) as dask_ds:\n                assert_identical(data, dask_ds)\n                with create_tmp_file() as tmp2:\n                    dask_ds.to_netcdf(tmp2)\n                    with open_dataset(tmp2) as on_disk:\n                        assert_identical(data, on_disk)\n\n    def test_deterministic_names(self):\n        with create_tmp_file() as tmp:\n            data = create_test_data()\n            data.to_netcdf(tmp)\n            with open_mfdataset(tmp, combine=""by_coords"") as ds:\n                original_names = {k: v.data.name for k, v in ds.data_vars.items()}\n            with open_mfdataset(tmp, combine=""by_coords"") as ds:\n                repeat_names = {k: v.data.name for k, v in ds.data_vars.items()}\n            for var_name, dask_name in original_names.items():\n                assert var_name in dask_name\n                assert dask_name[:13] == ""open_dataset-""\n            assert original_names == repeat_names\n\n    def test_dataarray_compute(self):\n        # Test DataArray.compute() on dask backend.\n        # The test for Dataset.compute() is already in DatasetIOBase;\n        # however dask is the only tested backend which supports DataArrays\n        actual = DataArray([1, 2]).chunk()\n        computed = actual.compute()\n        assert not actual._in_memory\n        assert computed._in_memory\n        assert_allclose(actual, computed, decode_bytes=False)\n\n    def test_save_mfdataset_compute_false_roundtrip(self):\n        from dask.delayed import Delayed\n\n        original = Dataset({""foo"": (""x"", np.random.randn(10))}).chunk()\n        datasets = [original.isel(x=slice(5)), original.isel(x=slice(5, 10))]\n        with create_tmp_file(allow_cleanup_failure=ON_WINDOWS) as tmp1:\n            with create_tmp_file(allow_cleanup_failure=ON_WINDOWS) as tmp2:\n                delayed_obj = save_mfdataset(\n                    datasets, [tmp1, tmp2], engine=self.engine, compute=False\n                )\n                assert isinstance(delayed_obj, Delayed)\n                delayed_obj.compute()\n                with open_mfdataset(\n                    [tmp1, tmp2], combine=""nested"", concat_dim=""x""\n                ) as actual:\n                    assert_identical(actual, original)\n\n    def test_load_dataset(self):\n        with create_tmp_file() as tmp:\n            original = Dataset({""foo"": (""x"", np.random.randn(10))})\n            original.to_netcdf(tmp)\n            ds = load_dataset(tmp)\n            # this would fail if we used open_dataset instead of load_dataset\n            ds.to_netcdf(tmp)\n\n    def test_load_dataarray(self):\n        with create_tmp_file() as tmp:\n            original = Dataset({""foo"": (""x"", np.random.randn(10))})\n            original.to_netcdf(tmp)\n            ds = load_dataarray(tmp)\n            # this would fail if we used open_dataarray instead of\n            # load_dataarray\n            ds.to_netcdf(tmp)\n\n\n@requires_scipy_or_netCDF4\n@requires_dask\nclass TestOpenMFDataSetDeprecation:\n    """"""\n    Set of tests to check that FutureWarnings are correctly raised until the\n    deprecation cycle is complete. #2616\n    """"""\n\n    def test_open_mfdataset_default(self):\n        ds1, ds2 = Dataset({""x"": [0]}), Dataset({""x"": [1]})\n        with create_tmp_file() as tmp1:\n            with create_tmp_file() as tmp2:\n                ds1.to_netcdf(tmp1)\n                ds2.to_netcdf(tmp2)\n\n                with pytest.warns(\n                    FutureWarning, match=""default behaviour of"" "" `open_mfdataset`""\n                ):\n                    open_mfdataset([tmp1, tmp2])\n\n    def test_open_mfdataset_with_concat_dim(self):\n        ds1, ds2 = Dataset({""x"": [0]}), Dataset({""x"": [1]})\n        with create_tmp_file() as tmp1:\n            with create_tmp_file() as tmp2:\n                ds1.to_netcdf(tmp1)\n                ds2.to_netcdf(tmp2)\n\n                with pytest.warns(FutureWarning, match=""`concat_dim`""):\n                    open_mfdataset([tmp1, tmp2], concat_dim=""x"")\n\n    def test_auto_combine_with_merge_and_concat(self):\n        ds1, ds2 = Dataset({""x"": [0]}), Dataset({""x"": [1]})\n        ds3 = Dataset({""z"": ((), 99)})\n        with create_tmp_file() as tmp1:\n            with create_tmp_file() as tmp2:\n                with create_tmp_file() as tmp3:\n                    ds1.to_netcdf(tmp1)\n                    ds2.to_netcdf(tmp2)\n                    ds3.to_netcdf(tmp3)\n\n                    with pytest.warns(\n                        FutureWarning, match=""require both concatenation""\n                    ):\n                        open_mfdataset([tmp1, tmp2, tmp3])\n\n    def test_auto_combine_with_coords(self):\n        ds1 = Dataset({""foo"": (""x"", [0])}, coords={""x"": (""x"", [0])})\n        ds2 = Dataset({""foo"": (""x"", [1])}, coords={""x"": (""x"", [1])})\n        with create_tmp_file() as tmp1:\n            with create_tmp_file() as tmp2:\n                ds1.to_netcdf(tmp1)\n                ds2.to_netcdf(tmp2)\n\n                with pytest.warns(FutureWarning, match=""supplied have global""):\n                    open_mfdataset([tmp1, tmp2])\n\n    def test_auto_combine_without_coords(self):\n        ds1, ds2 = Dataset({""foo"": (""x"", [0])}), Dataset({""foo"": (""x"", [1])})\n        with create_tmp_file() as tmp1:\n            with create_tmp_file() as tmp2:\n                ds1.to_netcdf(tmp1)\n                ds2.to_netcdf(tmp2)\n\n                with pytest.warns(FutureWarning, match=""supplied do not have global""):\n                    open_mfdataset([tmp1, tmp2])\n\n\n@requires_scipy_or_netCDF4\n@requires_pydap\n@pytest.mark.filterwarnings(""ignore:The binary mode of fromstring is deprecated"")\nclass TestPydap:\n    def convert_to_pydap_dataset(self, original):\n        from pydap.model import GridType, BaseType, DatasetType\n\n        ds = DatasetType(""bears"", **original.attrs)\n        for key, var in original.data_vars.items():\n            v = GridType(key)\n            v[key] = BaseType(key, var.values, dimensions=var.dims, **var.attrs)\n            for d in var.dims:\n                v[d] = BaseType(d, var[d].values)\n            ds[key] = v\n        # check all dims are stored in ds\n        for d in original.coords:\n            ds[d] = BaseType(\n                d, original[d].values, dimensions=(d,), **original[d].attrs\n            )\n        return ds\n\n    @contextlib.contextmanager\n    def create_datasets(self, **kwargs):\n        with open_example_dataset(""bears.nc"") as expected:\n            pydap_ds = self.convert_to_pydap_dataset(expected)\n            actual = open_dataset(PydapDataStore(pydap_ds))\n            # TODO solve this workaround:\n            # netcdf converts string to byte not unicode\n            expected[""bears""] = expected[""bears""].astype(str)\n            yield actual, expected\n\n    def test_cmp_local_file(self):\n        with self.create_datasets() as (actual, expected):\n            assert_equal(actual, expected)\n\n            # global attributes should be global attributes on the dataset\n            assert ""NC_GLOBAL"" not in actual.attrs\n            assert ""history"" in actual.attrs\n\n            # we don\'t check attributes exactly with assertDatasetIdentical()\n            # because the test DAP server seems to insert some extra\n            # attributes not found in the netCDF file.\n            assert actual.attrs.keys() == expected.attrs.keys()\n\n        with self.create_datasets() as (actual, expected):\n            assert_equal(actual[{""l"": 2}], expected[{""l"": 2}])\n\n        with self.create_datasets() as (actual, expected):\n            assert_equal(actual.isel(i=0, j=-1), expected.isel(i=0, j=-1))\n\n        with self.create_datasets() as (actual, expected):\n            assert_equal(actual.isel(j=slice(1, 2)), expected.isel(j=slice(1, 2)))\n\n        with self.create_datasets() as (actual, expected):\n            indexers = {""i"": [1, 0, 0], ""j"": [1, 2, 0, 1]}\n            assert_equal(actual.isel(**indexers), expected.isel(**indexers))\n\n        with self.create_datasets() as (actual, expected):\n            indexers = {\n                ""i"": DataArray([0, 1, 0], dims=""a""),\n                ""j"": DataArray([0, 2, 1], dims=""a""),\n            }\n            assert_equal(actual.isel(**indexers), expected.isel(**indexers))\n\n    def test_compatible_to_netcdf(self):\n        # make sure it can be saved as a netcdf\n        with self.create_datasets() as (actual, expected):\n            with create_tmp_file() as tmp_file:\n                actual.to_netcdf(tmp_file)\n                with open_dataset(tmp_file) as actual2:\n                    actual2[""bears""] = actual2[""bears""].astype(str)\n                    assert_equal(actual2, expected)\n\n    @requires_dask\n    def test_dask(self):\n        with self.create_datasets(chunks={""j"": 2}) as (actual, expected):\n            assert_equal(actual, expected)\n\n\n@network\n@requires_scipy_or_netCDF4\n@requires_pydap\nclass TestPydapOnline(TestPydap):\n    @contextlib.contextmanager\n    def create_datasets(self, **kwargs):\n        url = ""http://test.opendap.org/opendap/hyrax/data/nc/bears.nc""\n        actual = open_dataset(url, engine=""pydap"", **kwargs)\n        with open_example_dataset(""bears.nc"") as expected:\n            # workaround to restore string which is converted to byte\n            expected[""bears""] = expected[""bears""].astype(str)\n            yield actual, expected\n\n    def test_session(self):\n        from pydap.cas.urs import setup_session\n\n        session = setup_session(""XarrayTestUser"", ""Xarray2017"")\n        with mock.patch(""pydap.client.open_url"") as mock_func:\n            xr.backends.PydapDataStore.open(""http://test.url"", session=session)\n        mock_func.assert_called_with(""http://test.url"", session=session)\n\n\n@requires_scipy\n@requires_pynio\nclass TestPyNio(CFEncodedBase, NetCDF3Only):\n    def test_write_store(self):\n        # pynio is read-only for now\n        pass\n\n    @contextlib.contextmanager\n    def open(self, path, **kwargs):\n        with open_dataset(path, engine=""pynio"", **kwargs) as ds:\n            yield ds\n\n    def test_kwargs(self):\n        kwargs = {""format"": ""grib""}\n        path = os.path.join(os.path.dirname(__file__), ""data"", ""example"")\n        with backends.NioDataStore(path, **kwargs) as store:\n            assert store._manager._kwargs[""format""] == ""grib""\n\n    def save(self, dataset, path, **kwargs):\n        return dataset.to_netcdf(path, engine=""scipy"", **kwargs)\n\n    def test_weakrefs(self):\n        example = Dataset({""foo"": (""x"", np.arange(5.0))})\n        expected = example.rename({""foo"": ""bar"", ""x"": ""y""})\n\n        with create_tmp_file() as tmp_file:\n            example.to_netcdf(tmp_file, engine=""scipy"")\n            on_disk = open_dataset(tmp_file, engine=""pynio"")\n            actual = on_disk.rename({""foo"": ""bar"", ""x"": ""y""})\n            del on_disk  # trigger garbage collection\n            assert_identical(actual, expected)\n\n\n@requires_cfgrib\nclass TestCfGrib:\n    def test_read(self):\n        expected = {\n            ""number"": 2,\n            ""time"": 3,\n            ""isobaricInhPa"": 2,\n            ""latitude"": 3,\n            ""longitude"": 4,\n        }\n        with open_example_dataset(""example.grib"", engine=""cfgrib"") as ds:\n            assert ds.dims == expected\n            assert list(ds.data_vars) == [""z"", ""t""]\n            assert ds[""z""].min() == 12660.0\n\n    def test_read_filter_by_keys(self):\n        kwargs = {""filter_by_keys"": {""shortName"": ""t""}}\n        expected = {\n            ""number"": 2,\n            ""time"": 3,\n            ""isobaricInhPa"": 2,\n            ""latitude"": 3,\n            ""longitude"": 4,\n        }\n        with open_example_dataset(\n            ""example.grib"", engine=""cfgrib"", backend_kwargs=kwargs\n        ) as ds:\n            assert ds.dims == expected\n            assert list(ds.data_vars) == [""t""]\n            assert ds[""t""].min() == 231.0\n\n\n@requires_pseudonetcdf\n@pytest.mark.filterwarnings(""ignore:IOAPI_ISPH is assumed to be 6370000"")\nclass TestPseudoNetCDFFormat:\n    def open(self, path, **kwargs):\n        return open_dataset(path, engine=""pseudonetcdf"", **kwargs)\n\n    @contextlib.contextmanager\n    def roundtrip(\n        self, data, save_kwargs=None, open_kwargs=None, allow_cleanup_failure=False\n    ):\n        if save_kwargs is None:\n            save_kwargs = {}\n        if open_kwargs is None:\n            open_kwargs = {}\n        with create_tmp_file(allow_cleanup_failure=allow_cleanup_failure) as path:\n            self.save(data, path, **save_kwargs)\n            with self.open(path, **open_kwargs) as ds:\n                yield ds\n\n    def test_ict_format(self):\n        """"""\n        Open a CAMx file and test data variables\n        """"""\n        ictfile = open_example_dataset(\n            ""example.ict"", engine=""pseudonetcdf"", backend_kwargs={""format"": ""ffi1001""}\n        )\n        stdattr = {\n            ""fill_value"": -9999.0,\n            ""missing_value"": -9999,\n            ""scale"": 1,\n            ""llod_flag"": -8888,\n            ""llod_value"": ""N/A"",\n            ""ulod_flag"": -7777,\n            ""ulod_value"": ""N/A"",\n        }\n\n        def myatts(**attrs):\n            outattr = stdattr.copy()\n            outattr.update(attrs)\n            return outattr\n\n        input = {\n            ""coords"": {},\n            ""attrs"": {\n                ""fmt"": ""1001"",\n                ""n_header_lines"": 27,\n                ""PI_NAME"": ""Henderson, Barron"",\n                ""ORGANIZATION_NAME"": ""U.S. EPA"",\n                ""SOURCE_DESCRIPTION"": ""Example file with artificial data"",\n                ""MISSION_NAME"": ""JUST_A_TEST"",\n                ""VOLUME_INFO"": ""1, 1"",\n                ""SDATE"": ""2018, 04, 27"",\n                ""WDATE"": ""2018, 04, 27"",\n                ""TIME_INTERVAL"": ""0"",\n                ""INDEPENDENT_VARIABLE"": ""Start_UTC"",\n                ""ULOD_FLAG"": ""-7777"",\n                ""ULOD_VALUE"": ""N/A"",\n                ""LLOD_FLAG"": ""-8888"",\n                ""LLOD_VALUE"": (""N/A, N/A, N/A, N/A, 0.025""),\n                ""OTHER_COMMENTS"": (\n                    ""www-air.larc.nasa.gov/missions/etc/"" + ""IcarttDataFormat.htm""\n                ),\n                ""REVISION"": ""R0"",\n                ""R0"": ""No comments for this revision."",\n                ""TFLAG"": ""Start_UTC"",\n            },\n            ""dims"": {""POINTS"": 4},\n            ""data_vars"": {\n                ""Start_UTC"": {\n                    ""data"": [43200.0, 46800.0, 50400.0, 50400.0],\n                    ""dims"": (""POINTS"",),\n                    ""attrs"": myatts(units=""Start_UTC"", standard_name=""Start_UTC""),\n                },\n                ""lat"": {\n                    ""data"": [41.0, 42.0, 42.0, 42.0],\n                    ""dims"": (""POINTS"",),\n                    ""attrs"": myatts(units=""degrees_north"", standard_name=""lat""),\n                },\n                ""lon"": {\n                    ""data"": [-71.0, -72.0, -73.0, -74.0],\n                    ""dims"": (""POINTS"",),\n                    ""attrs"": myatts(units=""degrees_east"", standard_name=""lon""),\n                },\n                ""elev"": {\n                    ""data"": [5.0, 15.0, 20.0, 25.0],\n                    ""dims"": (""POINTS"",),\n                    ""attrs"": myatts(units=""meters"", standard_name=""elev""),\n                },\n                ""TEST_ppbv"": {\n                    ""data"": [1.2345, 2.3456, 3.4567, 4.5678],\n                    ""dims"": (""POINTS"",),\n                    ""attrs"": myatts(units=""ppbv"", standard_name=""TEST_ppbv""),\n                },\n                ""TESTM_ppbv"": {\n                    ""data"": [2.22, -9999.0, -7777.0, -8888.0],\n                    ""dims"": (""POINTS"",),\n                    ""attrs"": myatts(\n                        units=""ppbv"", standard_name=""TESTM_ppbv"", llod_value=0.025\n                    ),\n                },\n            },\n        }\n        chkfile = Dataset.from_dict(input)\n        assert_identical(ictfile, chkfile)\n\n    def test_ict_format_write(self):\n        fmtkw = {""format"": ""ffi1001""}\n        expected = open_example_dataset(\n            ""example.ict"", engine=""pseudonetcdf"", backend_kwargs=fmtkw\n        )\n        with self.roundtrip(\n            expected, save_kwargs=fmtkw, open_kwargs={""backend_kwargs"": fmtkw}\n        ) as actual:\n            assert_identical(expected, actual)\n\n    def test_uamiv_format_read(self):\n        """"""\n        Open a CAMx file and test data variables\n        """"""\n\n        camxfile = open_example_dataset(\n            ""example.uamiv"", engine=""pseudonetcdf"", backend_kwargs={""format"": ""uamiv""}\n        )\n        data = np.arange(20, dtype=""f"").reshape(1, 1, 4, 5)\n        expected = xr.Variable(\n            (""TSTEP"", ""LAY"", ""ROW"", ""COL""),\n            data,\n            dict(units=""ppm"", long_name=""O3"".ljust(16), var_desc=""O3"".ljust(80)),\n        )\n        actual = camxfile.variables[""O3""]\n        assert_allclose(expected, actual)\n\n        data = np.array([[[2002154, 0]]], dtype=""i"")\n        expected = xr.Variable(\n            (""TSTEP"", ""VAR"", ""DATE-TIME""),\n            data,\n            dict(\n                long_name=""TFLAG"".ljust(16),\n                var_desc=""TFLAG"".ljust(80),\n                units=""DATE-TIME"".ljust(16),\n            ),\n        )\n        actual = camxfile.variables[""TFLAG""]\n        assert_allclose(expected, actual)\n        camxfile.close()\n\n    @requires_dask\n    def test_uamiv_format_mfread(self):\n        """"""\n        Open a CAMx file and test data variables\n        """"""\n\n        camxfile = open_example_mfdataset(\n            [""example.uamiv"", ""example.uamiv""],\n            engine=""pseudonetcdf"",\n            concat_dim=""TSTEP"",\n            combine=""nested"",\n            backend_kwargs={""format"": ""uamiv""},\n        )\n\n        data1 = np.arange(20, dtype=""f"").reshape(1, 1, 4, 5)\n        data = np.concatenate([data1] * 2, axis=0)\n        expected = xr.Variable(\n            (""TSTEP"", ""LAY"", ""ROW"", ""COL""),\n            data,\n            dict(units=""ppm"", long_name=""O3"".ljust(16), var_desc=""O3"".ljust(80)),\n        )\n        actual = camxfile.variables[""O3""]\n        assert_allclose(expected, actual)\n\n        data = np.array([[[2002154, 0]]], dtype=""i"").repeat(2, 0)\n        attrs = dict(\n            long_name=""TFLAG"".ljust(16),\n            var_desc=""TFLAG"".ljust(80),\n            units=""DATE-TIME"".ljust(16),\n        )\n        dims = (""TSTEP"", ""VAR"", ""DATE-TIME"")\n        expected = xr.Variable(dims, data, attrs)\n        actual = camxfile.variables[""TFLAG""]\n        assert_allclose(expected, actual)\n        camxfile.close()\n\n    @pytest.mark.xfail(reason=""Flaky; see GH3711"")\n    def test_uamiv_format_write(self):\n        fmtkw = {""format"": ""uamiv""}\n\n        expected = open_example_dataset(\n            ""example.uamiv"", engine=""pseudonetcdf"", backend_kwargs=fmtkw\n        )\n        with self.roundtrip(\n            expected,\n            save_kwargs=fmtkw,\n            open_kwargs={""backend_kwargs"": fmtkw},\n            allow_cleanup_failure=True,\n        ) as actual:\n            assert_identical(expected, actual)\n\n        expected.close()\n\n    def save(self, dataset, path, **save_kwargs):\n        import PseudoNetCDF as pnc\n\n        pncf = pnc.PseudoNetCDFFile()\n        pncf.dimensions = {\n            k: pnc.PseudoNetCDFDimension(pncf, k, v) for k, v in dataset.dims.items()\n        }\n        pncf.variables = {\n            k: pnc.PseudoNetCDFVariable(\n                pncf, k, v.dtype.char, v.dims, values=v.data[...], **v.attrs\n            )\n            for k, v in dataset.variables.items()\n        }\n        for pk, pv in dataset.attrs.items():\n            setattr(pncf, pk, pv)\n\n        pnc.pncwrite(pncf, path, **save_kwargs)\n\n\n@requires_rasterio\n@contextlib.contextmanager\ndef create_tmp_geotiff(\n    nx=4,\n    ny=3,\n    nz=3,\n    transform=None,\n    transform_args=default_value,\n    crs=default_value,\n    open_kwargs=None,\n    additional_attrs=None,\n):\n    if transform_args is default_value:\n        transform_args = [5000, 80000, 1000, 2000.0]\n    if crs is default_value:\n        crs = {\n            ""units"": ""m"",\n            ""no_defs"": True,\n            ""ellps"": ""WGS84"",\n            ""proj"": ""utm"",\n            ""zone"": 18,\n        }\n    # yields a temporary geotiff file and a corresponding expected DataArray\n    import rasterio\n    from rasterio.transform import from_origin\n\n    if open_kwargs is None:\n        open_kwargs = {}\n\n    with create_tmp_file(suffix="".tif"", allow_cleanup_failure=ON_WINDOWS) as tmp_file:\n        # allow 2d or 3d shapes\n        if nz == 1:\n            data_shape = ny, nx\n            write_kwargs = {""indexes"": 1}\n        else:\n            data_shape = nz, ny, nx\n            write_kwargs = {}\n        data = np.arange(nz * ny * nx, dtype=rasterio.float32).reshape(*data_shape)\n        if transform is None:\n            transform = from_origin(*transform_args)\n        if additional_attrs is None:\n            additional_attrs = {\n                ""descriptions"": tuple(""d{}"".format(n + 1) for n in range(nz)),\n                ""units"": tuple(""u{}"".format(n + 1) for n in range(nz)),\n            }\n        with rasterio.open(\n            tmp_file,\n            ""w"",\n            driver=""GTiff"",\n            height=ny,\n            width=nx,\n            count=nz,\n            crs=crs,\n            transform=transform,\n            dtype=rasterio.float32,\n            **open_kwargs,\n        ) as s:\n            for attr, val in additional_attrs.items():\n                setattr(s, attr, val)\n            s.write(data, **write_kwargs)\n            dx, dy = s.res[0], -s.res[1]\n\n        a, b, c, d = transform_args\n        data = data[np.newaxis, ...] if nz == 1 else data\n        expected = DataArray(\n            data,\n            dims=(""band"", ""y"", ""x""),\n            coords={\n                ""band"": np.arange(nz) + 1,\n                ""y"": -np.arange(ny) * d + b + dy / 2,\n                ""x"": np.arange(nx) * c + a + dx / 2,\n            },\n        )\n        yield tmp_file, expected\n\n\n@requires_rasterio\nclass TestRasterio:\n    @requires_scipy_or_netCDF4\n    def test_serialization(self):\n        with create_tmp_geotiff(additional_attrs={}) as (tmp_file, expected):\n            # Write it to a netcdf and read again (roundtrip)\n            with xr.open_rasterio(tmp_file) as rioda:\n                with create_tmp_file(suffix="".nc"") as tmp_nc_file:\n                    rioda.to_netcdf(tmp_nc_file)\n                    with xr.open_dataarray(tmp_nc_file) as ncds:\n                        assert_identical(rioda, ncds)\n\n    def test_utm(self):\n        with create_tmp_geotiff() as (tmp_file, expected):\n            with xr.open_rasterio(tmp_file) as rioda:\n                assert_allclose(rioda, expected)\n                assert rioda.attrs[""scales""] == (1.0, 1.0, 1.0)\n                assert rioda.attrs[""offsets""] == (0.0, 0.0, 0.0)\n                assert rioda.attrs[""descriptions""] == (""d1"", ""d2"", ""d3"")\n                assert rioda.attrs[""units""] == (""u1"", ""u2"", ""u3"")\n                assert isinstance(rioda.attrs[""crs""], str)\n                assert isinstance(rioda.attrs[""res""], tuple)\n                assert isinstance(rioda.attrs[""is_tiled""], np.uint8)\n                assert isinstance(rioda.attrs[""transform""], tuple)\n                assert len(rioda.attrs[""transform""]) == 6\n                np.testing.assert_array_equal(\n                    rioda.attrs[""nodatavals""], [np.NaN, np.NaN, np.NaN]\n                )\n\n            # Check no parse coords\n            with xr.open_rasterio(tmp_file, parse_coordinates=False) as rioda:\n                assert ""x"" not in rioda.coords\n                assert ""y"" not in rioda.coords\n\n    def test_non_rectilinear(self):\n        from rasterio.transform import from_origin\n\n        # Create a geotiff file with 2d coordinates\n        with create_tmp_geotiff(\n            transform=from_origin(0, 3, 1, 1).rotation(45), crs=None\n        ) as (tmp_file, _):\n            # Default is to not parse coords\n            with xr.open_rasterio(tmp_file) as rioda:\n                assert ""x"" not in rioda.coords\n                assert ""y"" not in rioda.coords\n                assert ""crs"" not in rioda.attrs\n                assert rioda.attrs[""scales""] == (1.0, 1.0, 1.0)\n                assert rioda.attrs[""offsets""] == (0.0, 0.0, 0.0)\n                assert rioda.attrs[""descriptions""] == (""d1"", ""d2"", ""d3"")\n                assert rioda.attrs[""units""] == (""u1"", ""u2"", ""u3"")\n                assert isinstance(rioda.attrs[""res""], tuple)\n                assert isinstance(rioda.attrs[""is_tiled""], np.uint8)\n                assert isinstance(rioda.attrs[""transform""], tuple)\n                assert len(rioda.attrs[""transform""]) == 6\n\n            # See if a warning is raised if we force it\n            with pytest.warns(Warning, match=""transformation isn\'t rectilinear""):\n                with xr.open_rasterio(tmp_file, parse_coordinates=True) as rioda:\n                    assert ""x"" not in rioda.coords\n                    assert ""y"" not in rioda.coords\n\n    def test_platecarree(self):\n        with create_tmp_geotiff(\n            8,\n            10,\n            1,\n            transform_args=[1, 2, 0.5, 2.0],\n            crs=""+proj=latlong"",\n            open_kwargs={""nodata"": -9765},\n        ) as (tmp_file, expected):\n            with xr.open_rasterio(tmp_file) as rioda:\n                assert_allclose(rioda, expected)\n                assert rioda.attrs[""scales""] == (1.0,)\n                assert rioda.attrs[""offsets""] == (0.0,)\n                assert isinstance(rioda.attrs[""descriptions""], tuple)\n                assert isinstance(rioda.attrs[""units""], tuple)\n                assert isinstance(rioda.attrs[""crs""], str)\n                assert isinstance(rioda.attrs[""res""], tuple)\n                assert isinstance(rioda.attrs[""is_tiled""], np.uint8)\n                assert isinstance(rioda.attrs[""transform""], tuple)\n                assert len(rioda.attrs[""transform""]) == 6\n                np.testing.assert_array_equal(rioda.attrs[""nodatavals""], [-9765.0])\n\n    def test_notransform(self):\n        # regression test for https://github.com/pydata/xarray/issues/1686\n        import rasterio\n        import warnings\n\n        # Create a geotiff file\n        with warnings.catch_warnings():\n            # rasterio throws a NotGeoreferencedWarning here, which is\n            # expected since we test rasterio\'s defaults in this case.\n            warnings.filterwarnings(\n                ""ignore"",\n                category=UserWarning,\n                message=""Dataset has no geotransform set"",\n            )\n            with create_tmp_file(suffix="".tif"") as tmp_file:\n                # data\n                nx, ny, nz = 4, 3, 3\n                data = np.arange(nx * ny * nz, dtype=rasterio.float32).reshape(\n                    nz, ny, nx\n                )\n                with rasterio.open(\n                    tmp_file,\n                    ""w"",\n                    driver=""GTiff"",\n                    height=ny,\n                    width=nx,\n                    count=nz,\n                    dtype=rasterio.float32,\n                ) as s:\n                    s.descriptions = (""nx"", ""ny"", ""nz"")\n                    s.units = (""cm"", ""m"", ""km"")\n                    s.write(data)\n\n                # Tests\n                expected = DataArray(\n                    data,\n                    dims=(""band"", ""y"", ""x""),\n                    coords={\n                        ""band"": [1, 2, 3],\n                        ""y"": [0.5, 1.5, 2.5],\n                        ""x"": [0.5, 1.5, 2.5, 3.5],\n                    },\n                )\n                with xr.open_rasterio(tmp_file) as rioda:\n                    assert_allclose(rioda, expected)\n                    assert rioda.attrs[""scales""] == (1.0, 1.0, 1.0)\n                    assert rioda.attrs[""offsets""] == (0.0, 0.0, 0.0)\n                    assert rioda.attrs[""descriptions""] == (""nx"", ""ny"", ""nz"")\n                    assert rioda.attrs[""units""] == (""cm"", ""m"", ""km"")\n                    assert isinstance(rioda.attrs[""res""], tuple)\n                    assert isinstance(rioda.attrs[""is_tiled""], np.uint8)\n                    assert isinstance(rioda.attrs[""transform""], tuple)\n                    assert len(rioda.attrs[""transform""]) == 6\n\n    def test_indexing(self):\n        with create_tmp_geotiff(\n            8, 10, 3, transform_args=[1, 2, 0.5, 2.0], crs=""+proj=latlong""\n        ) as (tmp_file, expected):\n            with xr.open_rasterio(tmp_file, cache=False) as actual:\n\n                # tests\n                # assert_allclose checks all data + coordinates\n                assert_allclose(actual, expected)\n                assert not actual.variable._in_memory\n\n                # Basic indexer\n                ind = {""x"": slice(2, 5), ""y"": slice(5, 7)}\n                assert_allclose(expected.isel(**ind), actual.isel(**ind))\n                assert not actual.variable._in_memory\n\n                ind = {""band"": slice(1, 2), ""x"": slice(2, 5), ""y"": slice(5, 7)}\n                assert_allclose(expected.isel(**ind), actual.isel(**ind))\n                assert not actual.variable._in_memory\n\n                ind = {""band"": slice(1, 2), ""x"": slice(2, 5), ""y"": 0}\n                assert_allclose(expected.isel(**ind), actual.isel(**ind))\n                assert not actual.variable._in_memory\n\n                # orthogonal indexer\n                ind = {\n                    ""band"": np.array([2, 1, 0]),\n                    ""x"": np.array([1, 0]),\n                    ""y"": np.array([0, 2]),\n                }\n                assert_allclose(expected.isel(**ind), actual.isel(**ind))\n                assert not actual.variable._in_memory\n\n                ind = {""band"": np.array([2, 1, 0]), ""x"": np.array([1, 0]), ""y"": 0}\n                assert_allclose(expected.isel(**ind), actual.isel(**ind))\n                assert not actual.variable._in_memory\n\n                ind = {""band"": 0, ""x"": np.array([0, 0]), ""y"": np.array([1, 1, 1])}\n                assert_allclose(expected.isel(**ind), actual.isel(**ind))\n                assert not actual.variable._in_memory\n\n                # minus-stepped slice\n                ind = {""band"": np.array([2, 1, 0]), ""x"": slice(-1, None, -1), ""y"": 0}\n                assert_allclose(expected.isel(**ind), actual.isel(**ind))\n                assert not actual.variable._in_memory\n\n                ind = {""band"": np.array([2, 1, 0]), ""x"": 1, ""y"": slice(-1, 1, -2)}\n                assert_allclose(expected.isel(**ind), actual.isel(**ind))\n                assert not actual.variable._in_memory\n\n                # empty selection\n                ind = {""band"": np.array([2, 1, 0]), ""x"": 1, ""y"": slice(2, 2, 1)}\n                assert_allclose(expected.isel(**ind), actual.isel(**ind))\n                assert not actual.variable._in_memory\n\n                ind = {""band"": slice(0, 0), ""x"": 1, ""y"": 2}\n                assert_allclose(expected.isel(**ind), actual.isel(**ind))\n                assert not actual.variable._in_memory\n\n                # vectorized indexer\n                ind = {\n                    ""band"": DataArray([2, 1, 0], dims=""a""),\n                    ""x"": DataArray([1, 0, 0], dims=""a""),\n                    ""y"": np.array([0, 2]),\n                }\n                assert_allclose(expected.isel(**ind), actual.isel(**ind))\n                assert not actual.variable._in_memory\n\n                ind = {\n                    ""band"": DataArray([[2, 1, 0], [1, 0, 2]], dims=[""a"", ""b""]),\n                    ""x"": DataArray([[1, 0, 0], [0, 1, 0]], dims=[""a"", ""b""]),\n                    ""y"": 0,\n                }\n                assert_allclose(expected.isel(**ind), actual.isel(**ind))\n                assert not actual.variable._in_memory\n\n                # Selecting lists of bands is fine\n                ex = expected.isel(band=[1, 2])\n                ac = actual.isel(band=[1, 2])\n                assert_allclose(ac, ex)\n                ex = expected.isel(band=[0, 2])\n                ac = actual.isel(band=[0, 2])\n                assert_allclose(ac, ex)\n\n                # Integer indexing\n                ex = expected.isel(band=1)\n                ac = actual.isel(band=1)\n                assert_allclose(ac, ex)\n\n                ex = expected.isel(x=1, y=2)\n                ac = actual.isel(x=1, y=2)\n                assert_allclose(ac, ex)\n\n                ex = expected.isel(band=0, x=1, y=2)\n                ac = actual.isel(band=0, x=1, y=2)\n                assert_allclose(ac, ex)\n\n                # Mixed\n                ex = actual.isel(x=slice(2), y=slice(2))\n                ac = actual.isel(x=[0, 1], y=[0, 1])\n                assert_allclose(ac, ex)\n\n                ex = expected.isel(band=0, x=1, y=slice(5, 7))\n                ac = actual.isel(band=0, x=1, y=slice(5, 7))\n                assert_allclose(ac, ex)\n\n                ex = expected.isel(band=0, x=slice(2, 5), y=2)\n                ac = actual.isel(band=0, x=slice(2, 5), y=2)\n                assert_allclose(ac, ex)\n\n                # One-element lists\n                ex = expected.isel(band=[0], x=slice(2, 5), y=[2])\n                ac = actual.isel(band=[0], x=slice(2, 5), y=[2])\n                assert_allclose(ac, ex)\n\n    def test_caching(self):\n        with create_tmp_geotiff(\n            8, 10, 3, transform_args=[1, 2, 0.5, 2.0], crs=""+proj=latlong""\n        ) as (tmp_file, expected):\n            # Cache is the default\n            with xr.open_rasterio(tmp_file) as actual:\n\n                # This should cache everything\n                assert_allclose(actual, expected)\n\n                # once cached, non-windowed indexing should become possible\n                ac = actual.isel(x=[2, 4])\n                ex = expected.isel(x=[2, 4])\n                assert_allclose(ac, ex)\n\n    @requires_dask\n    def test_chunks(self):\n        with create_tmp_geotiff(\n            8, 10, 3, transform_args=[1, 2, 0.5, 2.0], crs=""+proj=latlong""\n        ) as (tmp_file, expected):\n            # Chunk at open time\n            with xr.open_rasterio(tmp_file, chunks=(1, 2, 2)) as actual:\n\n                import dask.array as da\n\n                assert isinstance(actual.data, da.Array)\n                assert ""open_rasterio"" in actual.data.name\n\n                # do some arithmetic\n                ac = actual.mean()\n                ex = expected.mean()\n                assert_allclose(ac, ex)\n\n                ac = actual.sel(band=1).mean(dim=""x"")\n                ex = expected.sel(band=1).mean(dim=""x"")\n                assert_allclose(ac, ex)\n\n    @pytest.mark.xfail(\n        not has_dask, reason=""without dask, a non-serializable lock is used""\n    )\n    def test_pickle_rasterio(self):\n        # regression test for https://github.com/pydata/xarray/issues/2121\n        with create_tmp_geotiff() as (tmp_file, expected):\n            with xr.open_rasterio(tmp_file) as rioda:\n                temp = pickle.dumps(rioda)\n                with pickle.loads(temp) as actual:\n                    assert_equal(actual, rioda)\n\n    def test_ENVI_tags(self):\n        rasterio = pytest.importorskip(""rasterio"", minversion=""1.0a"")\n        from rasterio.transform import from_origin\n\n        # Create an ENVI file with some tags in the ENVI namespace\n        # this test uses a custom driver, so we can\'t use create_tmp_geotiff\n        with create_tmp_file(suffix="".dat"") as tmp_file:\n            # data\n            nx, ny, nz = 4, 3, 3\n            data = np.arange(nx * ny * nz, dtype=rasterio.float32).reshape(nz, ny, nx)\n            transform = from_origin(5000, 80000, 1000, 2000.0)\n            with rasterio.open(\n                tmp_file,\n                ""w"",\n                driver=""ENVI"",\n                height=ny,\n                width=nx,\n                count=nz,\n                crs={\n                    ""units"": ""m"",\n                    ""no_defs"": True,\n                    ""ellps"": ""WGS84"",\n                    ""proj"": ""utm"",\n                    ""zone"": 18,\n                },\n                transform=transform,\n                dtype=rasterio.float32,\n            ) as s:\n                s.update_tags(\n                    ns=""ENVI"",\n                    description=""{Tagged file}"",\n                    wavelength=""{123.000000, 234.234000, 345.345678}"",\n                    fwhm=""{1.000000, 0.234000, 0.000345}"",\n                )\n                s.write(data)\n                dx, dy = s.res[0], -s.res[1]\n\n            # Tests\n            coords = {\n                ""band"": [1, 2, 3],\n                ""y"": -np.arange(ny) * 2000 + 80000 + dy / 2,\n                ""x"": np.arange(nx) * 1000 + 5000 + dx / 2,\n                ""wavelength"": (""band"", np.array([123, 234.234, 345.345678])),\n                ""fwhm"": (""band"", np.array([1, 0.234, 0.000345])),\n            }\n            expected = DataArray(data, dims=(""band"", ""y"", ""x""), coords=coords)\n\n            with xr.open_rasterio(tmp_file) as rioda:\n                assert_allclose(rioda, expected)\n                assert isinstance(rioda.attrs[""crs""], str)\n                assert isinstance(rioda.attrs[""res""], tuple)\n                assert isinstance(rioda.attrs[""is_tiled""], np.uint8)\n                assert isinstance(rioda.attrs[""transform""], tuple)\n                assert len(rioda.attrs[""transform""]) == 6\n                # from ENVI tags\n                assert isinstance(rioda.attrs[""description""], str)\n                assert isinstance(rioda.attrs[""map_info""], str)\n                assert isinstance(rioda.attrs[""samples""], str)\n\n    def test_geotiff_tags(self):\n        # Create a geotiff file with some tags\n        with create_tmp_geotiff() as (tmp_file, _):\n            with xr.open_rasterio(tmp_file) as rioda:\n                assert isinstance(rioda.attrs[""AREA_OR_POINT""], str)\n\n    @requires_dask\n    def test_no_mftime(self):\n        # rasterio can accept ""filename"" urguments that are actually urls,\n        # including paths to remote files.\n        # In issue #1816, we found that these caused dask to break, because\n        # the modification time was used to determine the dask token. This\n        # tests ensure we can still chunk such files when reading with\n        # rasterio.\n        with create_tmp_geotiff(\n            8, 10, 3, transform_args=[1, 2, 0.5, 2.0], crs=""+proj=latlong""\n        ) as (tmp_file, expected):\n            with mock.patch(""os.path.getmtime"", side_effect=OSError):\n                with xr.open_rasterio(tmp_file, chunks=(1, 2, 2)) as actual:\n                    import dask.array as da\n\n                    assert isinstance(actual.data, da.Array)\n                    assert_allclose(actual, expected)\n\n    @network\n    def test_http_url(self):\n        # more examples urls here\n        # http://download.osgeo.org/geotiff/samples/\n        url = ""http://download.osgeo.org/geotiff/samples/made_up/ntf_nord.tif""\n        with xr.open_rasterio(url) as actual:\n            assert actual.shape == (1, 512, 512)\n        # make sure chunking works\n        with xr.open_rasterio(url, chunks=(1, 256, 256)) as actual:\n            import dask.array as da\n\n            assert isinstance(actual.data, da.Array)\n\n    def test_rasterio_environment(self):\n        import rasterio\n\n        with create_tmp_geotiff() as (tmp_file, expected):\n            # Should fail with error since suffix not allowed\n            with pytest.raises(Exception):\n                with rasterio.Env(GDAL_SKIP=""GTiff""):\n                    with xr.open_rasterio(tmp_file) as actual:\n                        assert_allclose(actual, expected)\n\n    @pytest.mark.xfail(reason=""rasterio 1.1.1 is broken. GH3573"")\n    def test_rasterio_vrt(self):\n        import rasterio\n\n        # tmp_file default crs is UTM: CRS({\'init\': \'epsg:32618\'}\n        with create_tmp_geotiff() as (tmp_file, expected):\n            with rasterio.open(tmp_file) as src:\n                with rasterio.vrt.WarpedVRT(src, crs=""epsg:4326"") as vrt:\n                    expected_shape = (vrt.width, vrt.height)\n                    expected_crs = vrt.crs\n                    expected_res = vrt.res\n                    # Value of single pixel in center of image\n                    lon, lat = vrt.xy(vrt.width // 2, vrt.height // 2)\n                    expected_val = next(vrt.sample([(lon, lat)]))\n                    with xr.open_rasterio(vrt) as da:\n                        actual_shape = (da.sizes[""x""], da.sizes[""y""])\n                        actual_crs = da.crs\n                        actual_res = da.res\n                        actual_val = da.sel(dict(x=lon, y=lat), method=""nearest"").data\n\n                        assert actual_crs == expected_crs\n                        assert actual_res == expected_res\n                        assert actual_shape == expected_shape\n                        assert expected_val.all() == actual_val.all()\n\n    def test_rasterio_vrt_with_transform_and_size(self):\n        # Test open_rasterio() support of WarpedVRT with transform, width and\n        # height (issue #2864)\n        import rasterio\n        from rasterio.warp import calculate_default_transform\n        from affine import Affine\n\n        with create_tmp_geotiff() as (tmp_file, expected):\n            with rasterio.open(tmp_file) as src:\n                # Estimate the transform, width and height\n                # for a change of resolution\n                # tmp_file initial res is (1000,2000) (default values)\n                trans, w, h = calculate_default_transform(\n                    src.crs, src.crs, src.width, src.height, resolution=500, *src.bounds\n                )\n                with rasterio.vrt.WarpedVRT(\n                    src, transform=trans, width=w, height=h\n                ) as vrt:\n                    expected_shape = (vrt.width, vrt.height)\n                    expected_res = vrt.res\n                    expected_transform = vrt.transform\n                    with xr.open_rasterio(vrt) as da:\n                        actual_shape = (da.sizes[""x""], da.sizes[""y""])\n                        actual_res = da.res\n                        actual_transform = Affine(*da.transform)\n                        assert actual_res == expected_res\n                        assert actual_shape == expected_shape\n                        assert actual_transform == expected_transform\n\n    def test_rasterio_vrt_with_src_crs(self):\n        # Test open_rasterio() support of WarpedVRT with specified src_crs\n        import rasterio\n\n        # create geotiff with no CRS and specify it manually\n        with create_tmp_geotiff(crs=None) as (tmp_file, expected):\n            src_crs = rasterio.crs.CRS({""init"": ""epsg:32618""})\n            with rasterio.open(tmp_file) as src:\n                assert src.crs is None\n                with rasterio.vrt.WarpedVRT(src, src_crs=src_crs) as vrt:\n                    with xr.open_rasterio(vrt) as da:\n                        assert da.crs == src_crs\n\n    @network\n    def test_rasterio_vrt_network(self):\n        # Make sure loading w/ rasterio give same results as xarray\n        import rasterio\n\n        # use same url that rasterio package uses in tests\n        prefix = ""https://landsat-pds.s3.amazonaws.com/L8/139/045/""\n        image = ""LC81390452014295LGN00/LC81390452014295LGN00_B1.TIF""\n        httpstif = prefix + image\n        with rasterio.Env(aws_unsigned=True):\n            with rasterio.open(httpstif) as src:\n                with rasterio.vrt.WarpedVRT(src, crs=""epsg:4326"") as vrt:\n                    expected_shape = vrt.width, vrt.height\n                    expected_res = vrt.res\n                    # Value of single pixel in center of image\n                    lon, lat = vrt.xy(vrt.width // 2, vrt.height // 2)\n                    expected_val = next(vrt.sample([(lon, lat)]))\n                    with xr.open_rasterio(vrt) as da:\n                        actual_shape = da.sizes[""x""], da.sizes[""y""]\n                        actual_res = da.res\n                        actual_val = da.sel(dict(x=lon, y=lat), method=""nearest"").data\n\n                        assert actual_shape == expected_shape\n                        assert actual_res == expected_res\n                        assert expected_val == actual_val\n\n\nclass TestEncodingInvalid:\n    def test_extract_nc4_variable_encoding(self):\n        var = xr.Variable((""x"",), [1, 2, 3], {}, {""foo"": ""bar""})\n        with raises_regex(ValueError, ""unexpected encoding""):\n            _extract_nc4_variable_encoding(var, raise_on_invalid=True)\n\n        var = xr.Variable((""x"",), [1, 2, 3], {}, {""chunking"": (2, 1)})\n        encoding = _extract_nc4_variable_encoding(var)\n        assert {} == encoding\n\n        # regression test\n        var = xr.Variable((""x"",), [1, 2, 3], {}, {""shuffle"": True})\n        encoding = _extract_nc4_variable_encoding(var, raise_on_invalid=True)\n        assert {""shuffle"": True} == encoding\n\n        # Variables with unlim dims must be chunked on output.\n        var = xr.Variable((""x"",), [1, 2, 3], {}, {""contiguous"": True})\n        encoding = _extract_nc4_variable_encoding(var, unlimited_dims=(""x"",))\n        assert {} == encoding\n\n    def test_extract_h5nc_encoding(self):\n        # not supported with h5netcdf (yet)\n        var = xr.Variable((""x"",), [1, 2, 3], {}, {""least_sigificant_digit"": 2})\n        with raises_regex(ValueError, ""unexpected encoding""):\n            _extract_nc4_variable_encoding(var, raise_on_invalid=True)\n\n\nclass MiscObject:\n    pass\n\n\n@requires_netCDF4\nclass TestValidateAttrs:\n    def test_validating_attrs(self):\n        def new_dataset():\n            return Dataset({""data"": (""y"", np.arange(10.0))}, {""y"": np.arange(10)})\n\n        def new_dataset_and_dataset_attrs():\n            ds = new_dataset()\n            return ds, ds.attrs\n\n        def new_dataset_and_data_attrs():\n            ds = new_dataset()\n            return ds, ds.data.attrs\n\n        def new_dataset_and_coord_attrs():\n            ds = new_dataset()\n            return ds, ds.coords[""y""].attrs\n\n        for new_dataset_and_attrs in [\n            new_dataset_and_dataset_attrs,\n            new_dataset_and_data_attrs,\n            new_dataset_and_coord_attrs,\n        ]:\n            ds, attrs = new_dataset_and_attrs()\n\n            attrs[123] = ""test""\n            with raises_regex(TypeError, ""Invalid name for attr""):\n                ds.to_netcdf(""test.nc"")\n\n            ds, attrs = new_dataset_and_attrs()\n            attrs[MiscObject()] = ""test""\n            with raises_regex(TypeError, ""Invalid name for attr""):\n                ds.to_netcdf(""test.nc"")\n\n            ds, attrs = new_dataset_and_attrs()\n            attrs[""""] = ""test""\n            with raises_regex(ValueError, ""Invalid name for attr""):\n                ds.to_netcdf(""test.nc"")\n\n            # This one should work\n            ds, attrs = new_dataset_and_attrs()\n            attrs[""test""] = ""test""\n            with create_tmp_file() as tmp_file:\n                ds.to_netcdf(tmp_file)\n\n            ds, attrs = new_dataset_and_attrs()\n            attrs[""test""] = {""a"": 5}\n            with raises_regex(TypeError, ""Invalid value for attr""):\n                ds.to_netcdf(""test.nc"")\n\n            ds, attrs = new_dataset_and_attrs()\n            attrs[""test""] = MiscObject()\n            with raises_regex(TypeError, ""Invalid value for attr""):\n                ds.to_netcdf(""test.nc"")\n\n            ds, attrs = new_dataset_and_attrs()\n            attrs[""test""] = 5\n            with create_tmp_file() as tmp_file:\n                ds.to_netcdf(tmp_file)\n\n            ds, attrs = new_dataset_and_attrs()\n            attrs[""test""] = 3.14\n            with create_tmp_file() as tmp_file:\n                ds.to_netcdf(tmp_file)\n\n            ds, attrs = new_dataset_and_attrs()\n            attrs[""test""] = [1, 2, 3, 4]\n            with create_tmp_file() as tmp_file:\n                ds.to_netcdf(tmp_file)\n\n            ds, attrs = new_dataset_and_attrs()\n            attrs[""test""] = (1.9, 2.5)\n            with create_tmp_file() as tmp_file:\n                ds.to_netcdf(tmp_file)\n\n            ds, attrs = new_dataset_and_attrs()\n            attrs[""test""] = np.arange(5)\n            with create_tmp_file() as tmp_file:\n                ds.to_netcdf(tmp_file)\n\n            ds, attrs = new_dataset_and_attrs()\n            attrs[""test""] = ""This is a string""\n            with create_tmp_file() as tmp_file:\n                ds.to_netcdf(tmp_file)\n\n            ds, attrs = new_dataset_and_attrs()\n            attrs[""test""] = """"\n            with create_tmp_file() as tmp_file:\n                ds.to_netcdf(tmp_file)\n\n\n@requires_scipy_or_netCDF4\nclass TestDataArrayToNetCDF:\n    def test_dataarray_to_netcdf_no_name(self):\n        original_da = DataArray(np.arange(12).reshape((3, 4)))\n\n        with create_tmp_file() as tmp:\n            original_da.to_netcdf(tmp)\n\n            with open_dataarray(tmp) as loaded_da:\n                assert_identical(original_da, loaded_da)\n\n    def test_dataarray_to_netcdf_with_name(self):\n        original_da = DataArray(np.arange(12).reshape((3, 4)), name=""test"")\n\n        with create_tmp_file() as tmp:\n            original_da.to_netcdf(tmp)\n\n            with open_dataarray(tmp) as loaded_da:\n                assert_identical(original_da, loaded_da)\n\n    def test_dataarray_to_netcdf_coord_name_clash(self):\n        original_da = DataArray(\n            np.arange(12).reshape((3, 4)), dims=[""x"", ""y""], name=""x""\n        )\n\n        with create_tmp_file() as tmp:\n            original_da.to_netcdf(tmp)\n\n            with open_dataarray(tmp) as loaded_da:\n                assert_identical(original_da, loaded_da)\n\n    def test_open_dataarray_options(self):\n        data = DataArray(np.arange(5), coords={""y"": (""x"", range(5))}, dims=[""x""])\n\n        with create_tmp_file() as tmp:\n            data.to_netcdf(tmp)\n\n            expected = data.drop_vars(""y"")\n            with open_dataarray(tmp, drop_variables=[""y""]) as loaded:\n                assert_identical(expected, loaded)\n\n    @requires_scipy\n    def test_dataarray_to_netcdf_return_bytes(self):\n        # regression test for GH1410\n        data = xr.DataArray([1, 2, 3])\n        output = data.to_netcdf()\n        assert isinstance(output, bytes)\n\n    def test_dataarray_to_netcdf_no_name_pathlib(self):\n        original_da = DataArray(np.arange(12).reshape((3, 4)))\n\n        with create_tmp_file() as tmp:\n            tmp = Path(tmp)\n            original_da.to_netcdf(tmp)\n\n            with open_dataarray(tmp) as loaded_da:\n                assert_identical(original_da, loaded_da)\n\n\n@requires_scipy_or_netCDF4\ndef test_no_warning_from_dask_effective_get():\n    with create_tmp_file() as tmpfile:\n        with pytest.warns(None) as record:\n            ds = Dataset()\n            ds.to_netcdf(tmpfile)\n        assert len(record) == 0\n\n\n@requires_scipy_or_netCDF4\ndef test_source_encoding_always_present():\n    # Test for GH issue #2550.\n    rnddata = np.random.randn(10)\n    original = Dataset({""foo"": (""x"", rnddata)})\n    with create_tmp_file() as tmp:\n        original.to_netcdf(tmp)\n        with open_dataset(tmp) as ds:\n            assert ds.encoding[""source""] == tmp\n\n\ndef _assert_no_dates_out_of_range_warning(record):\n    undesired_message = ""dates out of range""\n    for warning in record:\n        assert undesired_message not in str(warning.message)\n\n\n@requires_scipy_or_netCDF4\n@pytest.mark.parametrize(""calendar"", _STANDARD_CALENDARS)\ndef test_use_cftime_standard_calendar_default_in_range(calendar):\n    x = [0, 1]\n    time = [0, 720]\n    units_date = ""2000-01-01""\n    units = ""days since 2000-01-01""\n    original = DataArray(x, [(""time"", time)], name=""x"")\n    original = original.to_dataset()\n    for v in [""x"", ""time""]:\n        original[v].attrs[""units""] = units\n        original[v].attrs[""calendar""] = calendar\n\n    x_timedeltas = np.array(x).astype(""timedelta64[D]"")\n    time_timedeltas = np.array(time).astype(""timedelta64[D]"")\n    decoded_x = np.datetime64(units_date, ""ns"") + x_timedeltas\n    decoded_time = np.datetime64(units_date, ""ns"") + time_timedeltas\n    expected_x = DataArray(decoded_x, [(""time"", decoded_time)], name=""x"")\n    expected_time = DataArray(decoded_time, [(""time"", decoded_time)], name=""time"")\n\n    with create_tmp_file() as tmp_file:\n        original.to_netcdf(tmp_file)\n        with pytest.warns(None) as record:\n            with open_dataset(tmp_file) as ds:\n                assert_identical(expected_x, ds.x)\n                assert_identical(expected_time, ds.time)\n            _assert_no_dates_out_of_range_warning(record)\n\n\n@requires_cftime\n@requires_scipy_or_netCDF4\n@pytest.mark.parametrize(""calendar"", _STANDARD_CALENDARS)\n@pytest.mark.parametrize(""units_year"", [1500, 2500])\ndef test_use_cftime_standard_calendar_default_out_of_range(calendar, units_year):\n    import cftime\n\n    x = [0, 1]\n    time = [0, 720]\n    units = f""days since {units_year}-01-01""\n    original = DataArray(x, [(""time"", time)], name=""x"")\n    original = original.to_dataset()\n    for v in [""x"", ""time""]:\n        original[v].attrs[""units""] = units\n        original[v].attrs[""calendar""] = calendar\n\n    decoded_x = cftime.num2date(x, units, calendar, only_use_cftime_datetimes=True)\n    decoded_time = cftime.num2date(\n        time, units, calendar, only_use_cftime_datetimes=True\n    )\n    expected_x = DataArray(decoded_x, [(""time"", decoded_time)], name=""x"")\n    expected_time = DataArray(decoded_time, [(""time"", decoded_time)], name=""time"")\n\n    with create_tmp_file() as tmp_file:\n        original.to_netcdf(tmp_file)\n        with pytest.warns(SerializationWarning):\n            with open_dataset(tmp_file) as ds:\n                assert_identical(expected_x, ds.x)\n                assert_identical(expected_time, ds.time)\n\n\n@requires_cftime\n@requires_scipy_or_netCDF4\n@pytest.mark.parametrize(""calendar"", _ALL_CALENDARS)\n@pytest.mark.parametrize(""units_year"", [1500, 2000, 2500])\ndef test_use_cftime_true(calendar, units_year):\n    import cftime\n\n    x = [0, 1]\n    time = [0, 720]\n    units = f""days since {units_year}-01-01""\n    original = DataArray(x, [(""time"", time)], name=""x"")\n    original = original.to_dataset()\n    for v in [""x"", ""time""]:\n        original[v].attrs[""units""] = units\n        original[v].attrs[""calendar""] = calendar\n\n    decoded_x = cftime.num2date(x, units, calendar, only_use_cftime_datetimes=True)\n    decoded_time = cftime.num2date(\n        time, units, calendar, only_use_cftime_datetimes=True\n    )\n    expected_x = DataArray(decoded_x, [(""time"", decoded_time)], name=""x"")\n    expected_time = DataArray(decoded_time, [(""time"", decoded_time)], name=""time"")\n\n    with create_tmp_file() as tmp_file:\n        original.to_netcdf(tmp_file)\n        with pytest.warns(None) as record:\n            with open_dataset(tmp_file, use_cftime=True) as ds:\n                assert_identical(expected_x, ds.x)\n                assert_identical(expected_time, ds.time)\n            _assert_no_dates_out_of_range_warning(record)\n\n\n@requires_scipy_or_netCDF4\n@pytest.mark.parametrize(""calendar"", _STANDARD_CALENDARS)\ndef test_use_cftime_false_standard_calendar_in_range(calendar):\n    x = [0, 1]\n    time = [0, 720]\n    units_date = ""2000-01-01""\n    units = ""days since 2000-01-01""\n    original = DataArray(x, [(""time"", time)], name=""x"")\n    original = original.to_dataset()\n    for v in [""x"", ""time""]:\n        original[v].attrs[""units""] = units\n        original[v].attrs[""calendar""] = calendar\n\n    x_timedeltas = np.array(x).astype(""timedelta64[D]"")\n    time_timedeltas = np.array(time).astype(""timedelta64[D]"")\n    decoded_x = np.datetime64(units_date, ""ns"") + x_timedeltas\n    decoded_time = np.datetime64(units_date, ""ns"") + time_timedeltas\n    expected_x = DataArray(decoded_x, [(""time"", decoded_time)], name=""x"")\n    expected_time = DataArray(decoded_time, [(""time"", decoded_time)], name=""time"")\n\n    with create_tmp_file() as tmp_file:\n        original.to_netcdf(tmp_file)\n        with pytest.warns(None) as record:\n            with open_dataset(tmp_file, use_cftime=False) as ds:\n                assert_identical(expected_x, ds.x)\n                assert_identical(expected_time, ds.time)\n            _assert_no_dates_out_of_range_warning(record)\n\n\n@requires_scipy_or_netCDF4\n@pytest.mark.parametrize(""calendar"", _STANDARD_CALENDARS)\n@pytest.mark.parametrize(""units_year"", [1500, 2500])\ndef test_use_cftime_false_standard_calendar_out_of_range(calendar, units_year):\n    x = [0, 1]\n    time = [0, 720]\n    units = f""days since {units_year}-01-01""\n    original = DataArray(x, [(""time"", time)], name=""x"")\n    original = original.to_dataset()\n    for v in [""x"", ""time""]:\n        original[v].attrs[""units""] = units\n        original[v].attrs[""calendar""] = calendar\n\n    with create_tmp_file() as tmp_file:\n        original.to_netcdf(tmp_file)\n        with pytest.raises((OutOfBoundsDatetime, ValueError)):\n            open_dataset(tmp_file, use_cftime=False)\n\n\n@requires_scipy_or_netCDF4\n@pytest.mark.parametrize(""calendar"", _NON_STANDARD_CALENDARS)\n@pytest.mark.parametrize(""units_year"", [1500, 2000, 2500])\ndef test_use_cftime_false_nonstandard_calendar(calendar, units_year):\n    x = [0, 1]\n    time = [0, 720]\n    units = f""days since {units_year}""\n    original = DataArray(x, [(""time"", time)], name=""x"")\n    original = original.to_dataset()\n    for v in [""x"", ""time""]:\n        original[v].attrs[""units""] = units\n        original[v].attrs[""calendar""] = calendar\n\n    with create_tmp_file() as tmp_file:\n        original.to_netcdf(tmp_file)\n        with pytest.raises((OutOfBoundsDatetime, ValueError)):\n            open_dataset(tmp_file, use_cftime=False)\n\n\n@pytest.mark.parametrize(""engine"", [""netcdf4"", ""scipy""])\ndef test_invalid_netcdf_raises(engine):\n    data = create_test_data()\n    with raises_regex(ValueError, ""unrecognized option \'invalid_netcdf\'""):\n        data.to_netcdf(""foo.nc"", engine=engine, invalid_netcdf=True)\n\n\n@requires_zarr\ndef test_encode_zarr_attr_value():\n    # array -> list\n    arr = np.array([1, 2, 3])\n    expected = [1, 2, 3]\n    actual = backends.zarr.encode_zarr_attr_value(arr)\n    assert isinstance(actual, list)\n    assert actual == expected\n\n    # scalar array -> scalar\n    sarr = np.array(1)[()]\n    expected = 1\n    actual = backends.zarr.encode_zarr_attr_value(sarr)\n    assert isinstance(actual, int)\n    assert actual == expected\n\n    # string -> string (no change)\n    expected = ""foo""\n    actual = backends.zarr.encode_zarr_attr_value(expected)\n    assert isinstance(actual, str)\n    assert actual == expected\n\n\n@requires_zarr\ndef test_extract_zarr_variable_encoding():\n\n    var = xr.Variable(""x"", [1, 2])\n    actual = backends.zarr.extract_zarr_variable_encoding(var)\n    assert ""chunks"" in actual\n    assert actual[""chunks""] is None\n\n    var = xr.Variable(""x"", [1, 2], encoding={""chunks"": (1,)})\n    actual = backends.zarr.extract_zarr_variable_encoding(var)\n    assert actual[""chunks""] == (1,)\n\n    # does not raise on invalid\n    var = xr.Variable(""x"", [1, 2], encoding={""foo"": (1,)})\n    actual = backends.zarr.extract_zarr_variable_encoding(var)\n\n    # raises on invalid\n    var = xr.Variable(""x"", [1, 2], encoding={""foo"": (1,)})\n    with raises_regex(ValueError, ""unexpected encoding parameters""):\n        actual = backends.zarr.extract_zarr_variable_encoding(\n            var, raise_on_invalid=True\n        )\n'"
xarray/tests/test_backends_api.py,0,"b'import pytest\n\nfrom xarray.backends.api import _get_default_engine\n\nfrom . import requires_netCDF4, requires_scipy\n\n\n@requires_netCDF4\n@requires_scipy\ndef test__get_default_engine():\n    engine_remote = _get_default_engine(""http://example.org/test.nc"", allow_remote=True)\n    assert engine_remote == ""netcdf4""\n\n    engine_gz = _get_default_engine(""/example.gz"")\n    assert engine_gz == ""scipy""\n\n    with pytest.raises(ValueError):\n        _get_default_engine(""/example.grib"")\n\n    engine_default = _get_default_engine(""/example"")\n    assert engine_default == ""netcdf4""\n'"
xarray/tests/test_backends_common.py,0,"b'import pytest\n\nfrom xarray.backends.common import robust_getitem\n\n\nclass DummyFailure(Exception):\n    pass\n\n\nclass DummyArray:\n    def __init__(self, failures):\n        self.failures = failures\n\n    def __getitem__(self, key):\n        if self.failures:\n            self.failures -= 1\n            raise DummyFailure\n        return ""success""\n\n\ndef test_robust_getitem():\n    array = DummyArray(failures=2)\n    with pytest.raises(DummyFailure):\n        array[...]\n    result = robust_getitem(array, ..., catch=DummyFailure, initial_delay=1)\n    assert result == ""success""\n\n    array = DummyArray(failures=3)\n    with pytest.raises(DummyFailure):\n        robust_getitem(array, ..., catch=DummyFailure, initial_delay=1, max_retries=2)\n'"
xarray/tests/test_backends_file_manager.py,0,"b'import gc\nimport pickle\nimport threading\nfrom unittest import mock\n\nimport pytest\n\nfrom xarray.backends.file_manager import CachingFileManager\nfrom xarray.backends.lru_cache import LRUCache\nfrom xarray.core.options import set_options\n\n\n@pytest.fixture(params=[1, 2, 3, None])\ndef file_cache(request):\n    maxsize = request.param\n    if maxsize is None:\n        yield {}\n    else:\n        yield LRUCache(maxsize)\n\n\ndef test_file_manager_mock_write(file_cache):\n    mock_file = mock.Mock()\n    opener = mock.Mock(spec=open, return_value=mock_file)\n    lock = mock.MagicMock(spec=threading.Lock())\n\n    manager = CachingFileManager(opener, ""filename"", lock=lock, cache=file_cache)\n    f = manager.acquire()\n    f.write(""contents"")\n    manager.close()\n\n    assert not file_cache\n    opener.assert_called_once_with(""filename"")\n    mock_file.write.assert_called_once_with(""contents"")\n    mock_file.close.assert_called_once_with()\n    lock.__enter__.assert_has_calls([mock.call(), mock.call()])\n\n\n@pytest.mark.parametrize(""expected_warning"", [None, RuntimeWarning])\ndef test_file_manager_autoclose(expected_warning):\n    mock_file = mock.Mock()\n    opener = mock.Mock(return_value=mock_file)\n    cache = {}\n\n    manager = CachingFileManager(opener, ""filename"", cache=cache)\n    manager.acquire()\n    assert cache\n\n    with set_options(warn_for_unclosed_files=expected_warning is not None):\n        with pytest.warns(expected_warning):\n            del manager\n            gc.collect()\n\n    assert not cache\n    mock_file.close.assert_called_once_with()\n\n\ndef test_file_manager_autoclose_while_locked():\n    opener = mock.Mock()\n    lock = threading.Lock()\n    cache = {}\n\n    manager = CachingFileManager(opener, ""filename"", lock=lock, cache=cache)\n    manager.acquire()\n    assert cache\n\n    lock.acquire()\n\n    with set_options(warn_for_unclosed_files=False):\n        del manager\n        gc.collect()\n\n    # can\'t clear the cache while locked, but also don\'t block in __del__\n    assert cache\n\n\ndef test_file_manager_repr():\n    opener = mock.Mock()\n    manager = CachingFileManager(opener, ""my-file"")\n    assert ""my-file"" in repr(manager)\n\n\ndef test_file_manager_refcounts():\n    mock_file = mock.Mock()\n    opener = mock.Mock(spec=open, return_value=mock_file)\n    cache = {}\n    ref_counts = {}\n\n    manager = CachingFileManager(opener, ""filename"", cache=cache, ref_counts=ref_counts)\n    assert ref_counts[manager._key] == 1\n    manager.acquire()\n    assert cache\n\n    manager2 = CachingFileManager(\n        opener, ""filename"", cache=cache, ref_counts=ref_counts\n    )\n    assert cache\n    assert manager._key == manager2._key\n    assert ref_counts[manager._key] == 2\n\n    with set_options(warn_for_unclosed_files=False):\n        del manager\n        gc.collect()\n\n    assert cache\n    assert ref_counts[manager2._key] == 1\n    mock_file.close.assert_not_called()\n\n    with set_options(warn_for_unclosed_files=False):\n        del manager2\n        gc.collect()\n\n    assert not ref_counts\n    assert not cache\n\n\ndef test_file_manager_replace_object():\n    opener = mock.Mock()\n    cache = {}\n    ref_counts = {}\n\n    manager = CachingFileManager(opener, ""filename"", cache=cache, ref_counts=ref_counts)\n    manager.acquire()\n    assert ref_counts[manager._key] == 1\n    assert cache\n\n    manager = CachingFileManager(opener, ""filename"", cache=cache, ref_counts=ref_counts)\n    assert ref_counts[manager._key] == 1\n    assert cache\n\n    manager.close()\n\n\ndef test_file_manager_write_consecutive(tmpdir, file_cache):\n    path1 = str(tmpdir.join(""testing1.txt""))\n    path2 = str(tmpdir.join(""testing2.txt""))\n    manager1 = CachingFileManager(open, path1, mode=""w"", cache=file_cache)\n    manager2 = CachingFileManager(open, path2, mode=""w"", cache=file_cache)\n    f1a = manager1.acquire()\n    f1a.write(""foo"")\n    f1a.flush()\n    f2 = manager2.acquire()\n    f2.write(""bar"")\n    f2.flush()\n    f1b = manager1.acquire()\n    f1b.write(""baz"")\n    assert (getattr(file_cache, ""maxsize"", float(""inf"")) > 1) == (f1a is f1b)\n    manager1.close()\n    manager2.close()\n\n    with open(path1, ""r"") as f:\n        assert f.read() == ""foobaz""\n    with open(path2, ""r"") as f:\n        assert f.read() == ""bar""\n\n\ndef test_file_manager_write_concurrent(tmpdir, file_cache):\n    path = str(tmpdir.join(""testing.txt""))\n    manager = CachingFileManager(open, path, mode=""w"", cache=file_cache)\n    f1 = manager.acquire()\n    f2 = manager.acquire()\n    f3 = manager.acquire()\n    assert f1 is f2\n    assert f2 is f3\n    f1.write(""foo"")\n    f1.flush()\n    f2.write(""bar"")\n    f2.flush()\n    f3.write(""baz"")\n    f3.flush()\n    manager.close()\n\n    with open(path, ""r"") as f:\n        assert f.read() == ""foobarbaz""\n\n\ndef test_file_manager_write_pickle(tmpdir, file_cache):\n    path = str(tmpdir.join(""testing.txt""))\n    manager = CachingFileManager(open, path, mode=""w"", cache=file_cache)\n    f = manager.acquire()\n    f.write(""foo"")\n    f.flush()\n    manager2 = pickle.loads(pickle.dumps(manager))\n    f2 = manager2.acquire()\n    f2.write(""bar"")\n    manager2.close()\n    manager.close()\n\n    with open(path, ""r"") as f:\n        assert f.read() == ""foobar""\n\n\ndef test_file_manager_read(tmpdir, file_cache):\n    path = str(tmpdir.join(""testing.txt""))\n\n    with open(path, ""w"") as f:\n        f.write(""foobar"")\n\n    manager = CachingFileManager(open, path, cache=file_cache)\n    f = manager.acquire()\n    assert f.read() == ""foobar""\n    manager.close()\n\n\ndef test_file_manager_invalid_kwargs():\n    with pytest.raises(TypeError):\n        CachingFileManager(open, ""dummy"", mode=""w"", invalid=True)\n\n\ndef test_file_manager_acquire_context(tmpdir, file_cache):\n    path = str(tmpdir.join(""testing.txt""))\n\n    with open(path, ""w"") as f:\n        f.write(""foobar"")\n\n    class AcquisitionError(Exception):\n        pass\n\n    manager = CachingFileManager(open, path, cache=file_cache)\n    with pytest.raises(AcquisitionError):\n        with manager.acquire_context() as f:\n            assert f.read() == ""foobar""\n            raise AcquisitionError\n    assert not file_cache  # file was *not* already open\n\n    with manager.acquire_context() as f:\n        assert f.read() == ""foobar""\n\n    with pytest.raises(AcquisitionError):\n        with manager.acquire_context() as f:\n            f.seek(0)\n            assert f.read() == ""foobar""\n            raise AcquisitionError\n    assert file_cache  # file *was* already open\n\n    manager.close()\n'"
xarray/tests/test_backends_locks.py,0,"b'import threading\n\nfrom xarray.backends import locks\n\n\ndef test_threaded_lock():\n    lock1 = locks._get_threaded_lock(""foo"")\n    assert isinstance(lock1, type(threading.Lock()))\n    lock2 = locks._get_threaded_lock(""foo"")\n    assert lock1 is lock2\n\n    lock3 = locks._get_threaded_lock(""bar"")\n    assert lock1 is not lock3\n'"
xarray/tests/test_backends_lru_cache.py,0,"b'from unittest import mock\n\nimport pytest\n\nfrom xarray.backends.lru_cache import LRUCache\n\n\ndef test_simple():\n    cache = LRUCache(maxsize=2)\n    cache[""x""] = 1\n    cache[""y""] = 2\n\n    assert cache[""x""] == 1\n    assert cache[""y""] == 2\n    assert len(cache) == 2\n    assert dict(cache) == {""x"": 1, ""y"": 2}\n    assert list(cache.keys()) == [""x"", ""y""]\n    assert list(cache.items()) == [(""x"", 1), (""y"", 2)]\n\n    cache[""z""] = 3\n    assert len(cache) == 2\n    assert list(cache.items()) == [(""y"", 2), (""z"", 3)]\n\n\ndef test_trivial():\n    cache = LRUCache(maxsize=0)\n    cache[""x""] = 1\n    assert len(cache) == 0\n\n\ndef test_invalid():\n    with pytest.raises(TypeError):\n        LRUCache(maxsize=None)\n    with pytest.raises(ValueError):\n        LRUCache(maxsize=-1)\n\n\ndef test_update_priority():\n    cache = LRUCache(maxsize=2)\n    cache[""x""] = 1\n    cache[""y""] = 2\n    assert list(cache) == [""x"", ""y""]\n    assert ""x"" in cache  # contains\n    assert list(cache) == [""y"", ""x""]\n    assert cache[""y""] == 2  # getitem\n    assert list(cache) == [""x"", ""y""]\n    cache[""x""] = 3  # setitem\n    assert list(cache.items()) == [(""y"", 2), (""x"", 3)]\n\n\ndef test_del():\n    cache = LRUCache(maxsize=2)\n    cache[""x""] = 1\n    cache[""y""] = 2\n    del cache[""x""]\n    assert dict(cache) == {""y"": 2}\n\n\ndef test_on_evict():\n    on_evict = mock.Mock()\n    cache = LRUCache(maxsize=1, on_evict=on_evict)\n    cache[""x""] = 1\n    cache[""y""] = 2\n    on_evict.assert_called_once_with(""x"", 1)\n\n\ndef test_on_evict_trivial():\n    on_evict = mock.Mock()\n    cache = LRUCache(maxsize=0, on_evict=on_evict)\n    cache[""x""] = 1\n    on_evict.assert_called_once_with(""x"", 1)\n\n\ndef test_resize():\n    cache = LRUCache(maxsize=2)\n    assert cache.maxsize == 2\n    cache[""w""] = 0\n    cache[""x""] = 1\n    cache[""y""] = 2\n    assert list(cache.items()) == [(""x"", 1), (""y"", 2)]\n    cache.maxsize = 10\n    cache[""z""] = 3\n    assert list(cache.items()) == [(""x"", 1), (""y"", 2), (""z"", 3)]\n    cache.maxsize = 1\n    assert list(cache.items()) == [(""z"", 3)]\n\n    with pytest.raises(ValueError):\n        cache.maxsize = -1\n'"
xarray/tests/test_cftime_offsets.py,7,"b'from itertools import product\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nfrom xarray import CFTimeIndex\nfrom xarray.coding.cftime_offsets import (\n    _MONTH_ABBREVIATIONS,\n    BaseCFTimeOffset,\n    Day,\n    Hour,\n    Minute,\n    MonthBegin,\n    MonthEnd,\n    QuarterBegin,\n    QuarterEnd,\n    Second,\n    YearBegin,\n    YearEnd,\n    _days_in_month,\n    cftime_range,\n    get_date_type,\n    to_cftime_datetime,\n    to_offset,\n)\n\ncftime = pytest.importorskip(""cftime"")\n\n\n_CFTIME_CALENDARS = [\n    ""365_day"",\n    ""360_day"",\n    ""julian"",\n    ""all_leap"",\n    ""366_day"",\n    ""gregorian"",\n    ""proleptic_gregorian"",\n    ""standard"",\n]\n\n\ndef _id_func(param):\n    """"""Called on each parameter passed to pytest.mark.parametrize""""""\n    return str(param)\n\n\n@pytest.fixture(params=_CFTIME_CALENDARS)\ndef calendar(request):\n    return request.param\n\n\n@pytest.mark.parametrize(\n    (""offset"", ""expected_n""),\n    [\n        (BaseCFTimeOffset(), 1),\n        (YearBegin(), 1),\n        (YearEnd(), 1),\n        (QuarterBegin(), 1),\n        (QuarterEnd(), 1),\n        (BaseCFTimeOffset(n=2), 2),\n        (YearBegin(n=2), 2),\n        (YearEnd(n=2), 2),\n        (QuarterBegin(n=2), 2),\n        (QuarterEnd(n=2), 2),\n    ],\n    ids=_id_func,\n)\ndef test_cftime_offset_constructor_valid_n(offset, expected_n):\n    assert offset.n == expected_n\n\n\n@pytest.mark.parametrize(\n    (""offset"", ""invalid_n""),\n    [\n        (BaseCFTimeOffset, 1.5),\n        (YearBegin, 1.5),\n        (YearEnd, 1.5),\n        (QuarterBegin, 1.5),\n        (QuarterEnd, 1.5),\n    ],\n    ids=_id_func,\n)\ndef test_cftime_offset_constructor_invalid_n(offset, invalid_n):\n    with pytest.raises(TypeError):\n        offset(n=invalid_n)\n\n\n@pytest.mark.parametrize(\n    (""offset"", ""expected_month""),\n    [\n        (YearBegin(), 1),\n        (YearEnd(), 12),\n        (YearBegin(month=5), 5),\n        (YearEnd(month=5), 5),\n        (QuarterBegin(), 3),\n        (QuarterEnd(), 3),\n        (QuarterBegin(month=5), 5),\n        (QuarterEnd(month=5), 5),\n    ],\n    ids=_id_func,\n)\ndef test_year_offset_constructor_valid_month(offset, expected_month):\n    assert offset.month == expected_month\n\n\n@pytest.mark.parametrize(\n    (""offset"", ""invalid_month"", ""exception""),\n    [\n        (YearBegin, 0, ValueError),\n        (YearEnd, 0, ValueError),\n        (YearBegin, 13, ValueError),\n        (YearEnd, 13, ValueError),\n        (YearBegin, 1.5, TypeError),\n        (YearEnd, 1.5, TypeError),\n        (QuarterBegin, 0, ValueError),\n        (QuarterEnd, 0, ValueError),\n        (QuarterBegin, 1.5, TypeError),\n        (QuarterEnd, 1.5, TypeError),\n        (QuarterBegin, 13, ValueError),\n        (QuarterEnd, 13, ValueError),\n    ],\n    ids=_id_func,\n)\ndef test_year_offset_constructor_invalid_month(offset, invalid_month, exception):\n    with pytest.raises(exception):\n        offset(month=invalid_month)\n\n\n@pytest.mark.parametrize(\n    (""offset"", ""expected""),\n    [\n        (BaseCFTimeOffset(), None),\n        (MonthBegin(), ""MS""),\n        (YearBegin(), ""AS-JAN""),\n        (QuarterBegin(), ""QS-MAR""),\n    ],\n    ids=_id_func,\n)\ndef test_rule_code(offset, expected):\n    assert offset.rule_code() == expected\n\n\n@pytest.mark.parametrize(\n    (""offset"", ""expected""),\n    [\n        (BaseCFTimeOffset(), ""<BaseCFTimeOffset: n=1>""),\n        (YearBegin(), ""<YearBegin: n=1, month=1>""),\n        (QuarterBegin(), ""<QuarterBegin: n=1, month=3>""),\n    ],\n    ids=_id_func,\n)\ndef test_str_and_repr(offset, expected):\n    assert str(offset) == expected\n    assert repr(offset) == expected\n\n\n@pytest.mark.parametrize(\n    ""offset"",\n    [BaseCFTimeOffset(), MonthBegin(), QuarterBegin(), YearBegin()],\n    ids=_id_func,\n)\ndef test_to_offset_offset_input(offset):\n    assert to_offset(offset) == offset\n\n\n@pytest.mark.parametrize(\n    (""freq"", ""expected""),\n    [\n        (""M"", MonthEnd()),\n        (""2M"", MonthEnd(n=2)),\n        (""MS"", MonthBegin()),\n        (""2MS"", MonthBegin(n=2)),\n        (""D"", Day()),\n        (""2D"", Day(n=2)),\n        (""H"", Hour()),\n        (""2H"", Hour(n=2)),\n        (""T"", Minute()),\n        (""2T"", Minute(n=2)),\n        (""min"", Minute()),\n        (""2min"", Minute(n=2)),\n        (""S"", Second()),\n        (""2S"", Second(n=2)),\n    ],\n    ids=_id_func,\n)\ndef test_to_offset_sub_annual(freq, expected):\n    assert to_offset(freq) == expected\n\n\n_ANNUAL_OFFSET_TYPES = {""A"": YearEnd, ""AS"": YearBegin}\n\n\n@pytest.mark.parametrize(\n    (""month_int"", ""month_label""), list(_MONTH_ABBREVIATIONS.items()) + [(0, """")]\n)\n@pytest.mark.parametrize(""multiple"", [None, 2])\n@pytest.mark.parametrize(""offset_str"", [""AS"", ""A""])\ndef test_to_offset_annual(month_label, month_int, multiple, offset_str):\n    freq = offset_str\n    offset_type = _ANNUAL_OFFSET_TYPES[offset_str]\n    if month_label:\n        freq = ""-"".join([freq, month_label])\n    if multiple:\n        freq = f""{multiple}{freq}""\n    result = to_offset(freq)\n\n    if multiple and month_int:\n        expected = offset_type(n=multiple, month=month_int)\n    elif multiple:\n        expected = offset_type(n=multiple)\n    elif month_int:\n        expected = offset_type(month=month_int)\n    else:\n        expected = offset_type()\n    assert result == expected\n\n\n_QUARTER_OFFSET_TYPES = {""Q"": QuarterEnd, ""QS"": QuarterBegin}\n\n\n@pytest.mark.parametrize(\n    (""month_int"", ""month_label""), list(_MONTH_ABBREVIATIONS.items()) + [(0, """")]\n)\n@pytest.mark.parametrize(""multiple"", [None, 2])\n@pytest.mark.parametrize(""offset_str"", [""QS"", ""Q""])\ndef test_to_offset_quarter(month_label, month_int, multiple, offset_str):\n    freq = offset_str\n    offset_type = _QUARTER_OFFSET_TYPES[offset_str]\n    if month_label:\n        freq = ""-"".join([freq, month_label])\n    if multiple:\n        freq = f""{multiple}{freq}""\n    result = to_offset(freq)\n\n    if multiple and month_int:\n        expected = offset_type(n=multiple, month=month_int)\n    elif multiple:\n        if month_int:\n            expected = offset_type(n=multiple)\n        else:\n            if offset_type == QuarterBegin:\n                expected = offset_type(n=multiple, month=1)\n            elif offset_type == QuarterEnd:\n                expected = offset_type(n=multiple, month=12)\n    elif month_int:\n        expected = offset_type(month=month_int)\n    else:\n        if offset_type == QuarterBegin:\n            expected = offset_type(month=1)\n        elif offset_type == QuarterEnd:\n            expected = offset_type(month=12)\n    assert result == expected\n\n\n@pytest.mark.parametrize(""freq"", [""Z"", ""7min2"", ""AM"", ""M-"", ""AS-"", ""QS-"", ""1H1min""])\ndef test_invalid_to_offset_str(freq):\n    with pytest.raises(ValueError):\n        to_offset(freq)\n\n\n@pytest.mark.parametrize(\n    (""argument"", ""expected_date_args""),\n    [(""2000-01-01"", (2000, 1, 1)), ((2000, 1, 1), (2000, 1, 1))],\n    ids=_id_func,\n)\ndef test_to_cftime_datetime(calendar, argument, expected_date_args):\n    date_type = get_date_type(calendar)\n    expected = date_type(*expected_date_args)\n    if isinstance(argument, tuple):\n        argument = date_type(*argument)\n    result = to_cftime_datetime(argument, calendar=calendar)\n    assert result == expected\n\n\ndef test_to_cftime_datetime_error_no_calendar():\n    with pytest.raises(ValueError):\n        to_cftime_datetime(""2000"")\n\n\ndef test_to_cftime_datetime_error_type_error():\n    with pytest.raises(TypeError):\n        to_cftime_datetime(1)\n\n\n_EQ_TESTS_A = [\n    BaseCFTimeOffset(),\n    YearBegin(),\n    YearEnd(),\n    YearBegin(month=2),\n    YearEnd(month=2),\n    QuarterBegin(),\n    QuarterEnd(),\n    QuarterBegin(month=2),\n    QuarterEnd(month=2),\n    MonthBegin(),\n    MonthEnd(),\n    Day(),\n    Hour(),\n    Minute(),\n    Second(),\n]\n_EQ_TESTS_B = [\n    BaseCFTimeOffset(n=2),\n    YearBegin(n=2),\n    YearEnd(n=2),\n    YearBegin(n=2, month=2),\n    YearEnd(n=2, month=2),\n    QuarterBegin(n=2),\n    QuarterEnd(n=2),\n    QuarterBegin(n=2, month=2),\n    QuarterEnd(n=2, month=2),\n    MonthBegin(n=2),\n    MonthEnd(n=2),\n    Day(n=2),\n    Hour(n=2),\n    Minute(n=2),\n    Second(n=2),\n]\n\n\n@pytest.mark.parametrize((""a"", ""b""), product(_EQ_TESTS_A, _EQ_TESTS_B), ids=_id_func)\ndef test_neq(a, b):\n    assert a != b\n\n\n_EQ_TESTS_B_COPY = [\n    BaseCFTimeOffset(n=2),\n    YearBegin(n=2),\n    YearEnd(n=2),\n    YearBegin(n=2, month=2),\n    YearEnd(n=2, month=2),\n    QuarterBegin(n=2),\n    QuarterEnd(n=2),\n    QuarterBegin(n=2, month=2),\n    QuarterEnd(n=2, month=2),\n    MonthBegin(n=2),\n    MonthEnd(n=2),\n    Day(n=2),\n    Hour(n=2),\n    Minute(n=2),\n    Second(n=2),\n]\n\n\n@pytest.mark.parametrize((""a"", ""b""), zip(_EQ_TESTS_B, _EQ_TESTS_B_COPY), ids=_id_func)\ndef test_eq(a, b):\n    assert a == b\n\n\n_MUL_TESTS = [\n    (BaseCFTimeOffset(), BaseCFTimeOffset(n=3)),\n    (YearEnd(), YearEnd(n=3)),\n    (YearBegin(), YearBegin(n=3)),\n    (QuarterEnd(), QuarterEnd(n=3)),\n    (QuarterBegin(), QuarterBegin(n=3)),\n    (MonthEnd(), MonthEnd(n=3)),\n    (MonthBegin(), MonthBegin(n=3)),\n    (Day(), Day(n=3)),\n    (Hour(), Hour(n=3)),\n    (Minute(), Minute(n=3)),\n    (Second(), Second(n=3)),\n]\n\n\n@pytest.mark.parametrize((""offset"", ""expected""), _MUL_TESTS, ids=_id_func)\ndef test_mul(offset, expected):\n    assert offset * 3 == expected\n\n\n@pytest.mark.parametrize((""offset"", ""expected""), _MUL_TESTS, ids=_id_func)\ndef test_rmul(offset, expected):\n    assert 3 * offset == expected\n\n\n@pytest.mark.parametrize(\n    (""offset"", ""expected""),\n    [\n        (BaseCFTimeOffset(), BaseCFTimeOffset(n=-1)),\n        (YearEnd(), YearEnd(n=-1)),\n        (YearBegin(), YearBegin(n=-1)),\n        (QuarterEnd(), QuarterEnd(n=-1)),\n        (QuarterBegin(), QuarterBegin(n=-1)),\n        (MonthEnd(), MonthEnd(n=-1)),\n        (MonthBegin(), MonthBegin(n=-1)),\n        (Day(), Day(n=-1)),\n        (Hour(), Hour(n=-1)),\n        (Minute(), Minute(n=-1)),\n        (Second(), Second(n=-1)),\n    ],\n    ids=_id_func,\n)\ndef test_neg(offset, expected):\n    assert -offset == expected\n\n\n_ADD_TESTS = [\n    (Day(n=2), (1, 1, 3)),\n    (Hour(n=2), (1, 1, 1, 2)),\n    (Minute(n=2), (1, 1, 1, 0, 2)),\n    (Second(n=2), (1, 1, 1, 0, 0, 2)),\n]\n\n\n@pytest.mark.parametrize((""offset"", ""expected_date_args""), _ADD_TESTS, ids=_id_func)\ndef test_add_sub_monthly(offset, expected_date_args, calendar):\n    date_type = get_date_type(calendar)\n    initial = date_type(1, 1, 1)\n    expected = date_type(*expected_date_args)\n    result = offset + initial\n    assert result == expected\n\n\n@pytest.mark.parametrize((""offset"", ""expected_date_args""), _ADD_TESTS, ids=_id_func)\ndef test_radd_sub_monthly(offset, expected_date_args, calendar):\n    date_type = get_date_type(calendar)\n    initial = date_type(1, 1, 1)\n    expected = date_type(*expected_date_args)\n    result = initial + offset\n    assert result == expected\n\n\n@pytest.mark.parametrize(\n    (""offset"", ""expected_date_args""),\n    [\n        (Day(n=2), (1, 1, 1)),\n        (Hour(n=2), (1, 1, 2, 22)),\n        (Minute(n=2), (1, 1, 2, 23, 58)),\n        (Second(n=2), (1, 1, 2, 23, 59, 58)),\n    ],\n    ids=_id_func,\n)\ndef test_rsub_sub_monthly(offset, expected_date_args, calendar):\n    date_type = get_date_type(calendar)\n    initial = date_type(1, 1, 3)\n    expected = date_type(*expected_date_args)\n    result = initial - offset\n    assert result == expected\n\n\n@pytest.mark.parametrize(""offset"", _EQ_TESTS_A, ids=_id_func)\ndef test_sub_error(offset, calendar):\n    date_type = get_date_type(calendar)\n    initial = date_type(1, 1, 1)\n    with pytest.raises(TypeError):\n        offset - initial\n\n\n@pytest.mark.parametrize((""a"", ""b""), zip(_EQ_TESTS_A, _EQ_TESTS_B), ids=_id_func)\ndef test_minus_offset(a, b):\n    result = b - a\n    expected = a\n    assert result == expected\n\n\n@pytest.mark.parametrize(\n    (""a"", ""b""),\n    list(zip(np.roll(_EQ_TESTS_A, 1), _EQ_TESTS_B))\n    + [(YearEnd(month=1), YearEnd(month=2))],\n    ids=_id_func,\n)\ndef test_minus_offset_error(a, b):\n    with pytest.raises(TypeError):\n        b - a\n\n\ndef test_days_in_month_non_december(calendar):\n    date_type = get_date_type(calendar)\n    reference = date_type(1, 4, 1)\n    assert _days_in_month(reference) == 30\n\n\ndef test_days_in_month_december(calendar):\n    if calendar == ""360_day"":\n        expected = 30\n    else:\n        expected = 31\n    date_type = get_date_type(calendar)\n    reference = date_type(1, 12, 5)\n    assert _days_in_month(reference) == expected\n\n\n@pytest.mark.parametrize(\n    (""initial_date_args"", ""offset"", ""expected_date_args""),\n    [\n        ((1, 1, 1), MonthBegin(), (1, 2, 1)),\n        ((1, 1, 1), MonthBegin(n=2), (1, 3, 1)),\n        ((1, 1, 7), MonthBegin(), (1, 2, 1)),\n        ((1, 1, 7), MonthBegin(n=2), (1, 3, 1)),\n        ((1, 3, 1), MonthBegin(n=-1), (1, 2, 1)),\n        ((1, 3, 1), MonthBegin(n=-2), (1, 1, 1)),\n        ((1, 3, 3), MonthBegin(n=-1), (1, 3, 1)),\n        ((1, 3, 3), MonthBegin(n=-2), (1, 2, 1)),\n        ((1, 2, 1), MonthBegin(n=14), (2, 4, 1)),\n        ((2, 4, 1), MonthBegin(n=-14), (1, 2, 1)),\n        ((1, 1, 1, 5, 5, 5, 5), MonthBegin(), (1, 2, 1, 5, 5, 5, 5)),\n        ((1, 1, 3, 5, 5, 5, 5), MonthBegin(), (1, 2, 1, 5, 5, 5, 5)),\n        ((1, 1, 3, 5, 5, 5, 5), MonthBegin(n=-1), (1, 1, 1, 5, 5, 5, 5)),\n    ],\n    ids=_id_func,\n)\ndef test_add_month_begin(calendar, initial_date_args, offset, expected_date_args):\n    date_type = get_date_type(calendar)\n    initial = date_type(*initial_date_args)\n    result = initial + offset\n    expected = date_type(*expected_date_args)\n    assert result == expected\n\n\n@pytest.mark.parametrize(\n    (""initial_date_args"", ""offset"", ""expected_year_month"", ""expected_sub_day""),\n    [\n        ((1, 1, 1), MonthEnd(), (1, 1), ()),\n        ((1, 1, 1), MonthEnd(n=2), (1, 2), ()),\n        ((1, 3, 1), MonthEnd(n=-1), (1, 2), ()),\n        ((1, 3, 1), MonthEnd(n=-2), (1, 1), ()),\n        ((1, 2, 1), MonthEnd(n=14), (2, 3), ()),\n        ((2, 4, 1), MonthEnd(n=-14), (1, 2), ()),\n        ((1, 1, 1, 5, 5, 5, 5), MonthEnd(), (1, 1), (5, 5, 5, 5)),\n        ((1, 2, 1, 5, 5, 5, 5), MonthEnd(n=-1), (1, 1), (5, 5, 5, 5)),\n    ],\n    ids=_id_func,\n)\ndef test_add_month_end(\n    calendar, initial_date_args, offset, expected_year_month, expected_sub_day\n):\n    date_type = get_date_type(calendar)\n    initial = date_type(*initial_date_args)\n    result = initial + offset\n    reference_args = expected_year_month + (1,)\n    reference = date_type(*reference_args)\n\n    # Here the days at the end of each month varies based on the calendar used\n    expected_date_args = (\n        expected_year_month + (_days_in_month(reference),) + expected_sub_day\n    )\n    expected = date_type(*expected_date_args)\n    assert result == expected\n\n\n@pytest.mark.parametrize(\n    (\n        ""initial_year_month"",\n        ""initial_sub_day"",\n        ""offset"",\n        ""expected_year_month"",\n        ""expected_sub_day"",\n    ),\n    [\n        ((1, 1), (), MonthEnd(), (1, 2), ()),\n        ((1, 1), (), MonthEnd(n=2), (1, 3), ()),\n        ((1, 3), (), MonthEnd(n=-1), (1, 2), ()),\n        ((1, 3), (), MonthEnd(n=-2), (1, 1), ()),\n        ((1, 2), (), MonthEnd(n=14), (2, 4), ()),\n        ((2, 4), (), MonthEnd(n=-14), (1, 2), ()),\n        ((1, 1), (5, 5, 5, 5), MonthEnd(), (1, 2), (5, 5, 5, 5)),\n        ((1, 2), (5, 5, 5, 5), MonthEnd(n=-1), (1, 1), (5, 5, 5, 5)),\n    ],\n    ids=_id_func,\n)\ndef test_add_month_end_onOffset(\n    calendar,\n    initial_year_month,\n    initial_sub_day,\n    offset,\n    expected_year_month,\n    expected_sub_day,\n):\n    date_type = get_date_type(calendar)\n    reference_args = initial_year_month + (1,)\n    reference = date_type(*reference_args)\n    initial_date_args = (\n        initial_year_month + (_days_in_month(reference),) + initial_sub_day\n    )\n    initial = date_type(*initial_date_args)\n    result = initial + offset\n    reference_args = expected_year_month + (1,)\n    reference = date_type(*reference_args)\n\n    # Here the days at the end of each month varies based on the calendar used\n    expected_date_args = (\n        expected_year_month + (_days_in_month(reference),) + expected_sub_day\n    )\n    expected = date_type(*expected_date_args)\n    assert result == expected\n\n\n@pytest.mark.parametrize(\n    (""initial_date_args"", ""offset"", ""expected_date_args""),\n    [\n        ((1, 1, 1), YearBegin(), (2, 1, 1)),\n        ((1, 1, 1), YearBegin(n=2), (3, 1, 1)),\n        ((1, 1, 1), YearBegin(month=2), (1, 2, 1)),\n        ((1, 1, 7), YearBegin(n=2), (3, 1, 1)),\n        ((2, 2, 1), YearBegin(n=-1), (2, 1, 1)),\n        ((1, 1, 2), YearBegin(n=-1), (1, 1, 1)),\n        ((1, 1, 1, 5, 5, 5, 5), YearBegin(), (2, 1, 1, 5, 5, 5, 5)),\n        ((2, 1, 1, 5, 5, 5, 5), YearBegin(n=-1), (1, 1, 1, 5, 5, 5, 5)),\n    ],\n    ids=_id_func,\n)\ndef test_add_year_begin(calendar, initial_date_args, offset, expected_date_args):\n    date_type = get_date_type(calendar)\n    initial = date_type(*initial_date_args)\n    result = initial + offset\n    expected = date_type(*expected_date_args)\n    assert result == expected\n\n\n@pytest.mark.parametrize(\n    (""initial_date_args"", ""offset"", ""expected_year_month"", ""expected_sub_day""),\n    [\n        ((1, 1, 1), YearEnd(), (1, 12), ()),\n        ((1, 1, 1), YearEnd(n=2), (2, 12), ()),\n        ((1, 1, 1), YearEnd(month=1), (1, 1), ()),\n        ((2, 3, 1), YearEnd(n=-1), (1, 12), ()),\n        ((1, 3, 1), YearEnd(n=-1, month=2), (1, 2), ()),\n        ((1, 1, 1, 5, 5, 5, 5), YearEnd(), (1, 12), (5, 5, 5, 5)),\n        ((1, 1, 1, 5, 5, 5, 5), YearEnd(n=2), (2, 12), (5, 5, 5, 5)),\n    ],\n    ids=_id_func,\n)\ndef test_add_year_end(\n    calendar, initial_date_args, offset, expected_year_month, expected_sub_day\n):\n    date_type = get_date_type(calendar)\n    initial = date_type(*initial_date_args)\n    result = initial + offset\n    reference_args = expected_year_month + (1,)\n    reference = date_type(*reference_args)\n\n    # Here the days at the end of each month varies based on the calendar used\n    expected_date_args = (\n        expected_year_month + (_days_in_month(reference),) + expected_sub_day\n    )\n    expected = date_type(*expected_date_args)\n    assert result == expected\n\n\n@pytest.mark.parametrize(\n    (\n        ""initial_year_month"",\n        ""initial_sub_day"",\n        ""offset"",\n        ""expected_year_month"",\n        ""expected_sub_day"",\n    ),\n    [\n        ((1, 12), (), YearEnd(), (2, 12), ()),\n        ((1, 12), (), YearEnd(n=2), (3, 12), ()),\n        ((2, 12), (), YearEnd(n=-1), (1, 12), ()),\n        ((3, 12), (), YearEnd(n=-2), (1, 12), ()),\n        ((1, 1), (), YearEnd(month=2), (1, 2), ()),\n        ((1, 12), (5, 5, 5, 5), YearEnd(), (2, 12), (5, 5, 5, 5)),\n        ((2, 12), (5, 5, 5, 5), YearEnd(n=-1), (1, 12), (5, 5, 5, 5)),\n    ],\n    ids=_id_func,\n)\ndef test_add_year_end_onOffset(\n    calendar,\n    initial_year_month,\n    initial_sub_day,\n    offset,\n    expected_year_month,\n    expected_sub_day,\n):\n    date_type = get_date_type(calendar)\n    reference_args = initial_year_month + (1,)\n    reference = date_type(*reference_args)\n    initial_date_args = (\n        initial_year_month + (_days_in_month(reference),) + initial_sub_day\n    )\n    initial = date_type(*initial_date_args)\n    result = initial + offset\n    reference_args = expected_year_month + (1,)\n    reference = date_type(*reference_args)\n\n    # Here the days at the end of each month varies based on the calendar used\n    expected_date_args = (\n        expected_year_month + (_days_in_month(reference),) + expected_sub_day\n    )\n    expected = date_type(*expected_date_args)\n    assert result == expected\n\n\n@pytest.mark.parametrize(\n    (""initial_date_args"", ""offset"", ""expected_date_args""),\n    [\n        ((1, 1, 1), QuarterBegin(), (1, 3, 1)),\n        ((1, 1, 1), QuarterBegin(n=2), (1, 6, 1)),\n        ((1, 1, 1), QuarterBegin(month=2), (1, 2, 1)),\n        ((1, 1, 7), QuarterBegin(n=2), (1, 6, 1)),\n        ((2, 2, 1), QuarterBegin(n=-1), (1, 12, 1)),\n        ((1, 3, 2), QuarterBegin(n=-1), (1, 3, 1)),\n        ((1, 1, 1, 5, 5, 5, 5), QuarterBegin(), (1, 3, 1, 5, 5, 5, 5)),\n        ((2, 1, 1, 5, 5, 5, 5), QuarterBegin(n=-1), (1, 12, 1, 5, 5, 5, 5)),\n    ],\n    ids=_id_func,\n)\ndef test_add_quarter_begin(calendar, initial_date_args, offset, expected_date_args):\n    date_type = get_date_type(calendar)\n    initial = date_type(*initial_date_args)\n    result = initial + offset\n    expected = date_type(*expected_date_args)\n    assert result == expected\n\n\n@pytest.mark.parametrize(\n    (""initial_date_args"", ""offset"", ""expected_year_month"", ""expected_sub_day""),\n    [\n        ((1, 1, 1), QuarterEnd(), (1, 3), ()),\n        ((1, 1, 1), QuarterEnd(n=2), (1, 6), ()),\n        ((1, 1, 1), QuarterEnd(month=1), (1, 1), ()),\n        ((2, 3, 1), QuarterEnd(n=-1), (1, 12), ()),\n        ((1, 3, 1), QuarterEnd(n=-1, month=2), (1, 2), ()),\n        ((1, 1, 1, 5, 5, 5, 5), QuarterEnd(), (1, 3), (5, 5, 5, 5)),\n        ((1, 1, 1, 5, 5, 5, 5), QuarterEnd(n=2), (1, 6), (5, 5, 5, 5)),\n    ],\n    ids=_id_func,\n)\ndef test_add_quarter_end(\n    calendar, initial_date_args, offset, expected_year_month, expected_sub_day\n):\n    date_type = get_date_type(calendar)\n    initial = date_type(*initial_date_args)\n    result = initial + offset\n    reference_args = expected_year_month + (1,)\n    reference = date_type(*reference_args)\n\n    # Here the days at the end of each month varies based on the calendar used\n    expected_date_args = (\n        expected_year_month + (_days_in_month(reference),) + expected_sub_day\n    )\n    expected = date_type(*expected_date_args)\n    assert result == expected\n\n\n@pytest.mark.parametrize(\n    (\n        ""initial_year_month"",\n        ""initial_sub_day"",\n        ""offset"",\n        ""expected_year_month"",\n        ""expected_sub_day"",\n    ),\n    [\n        ((1, 12), (), QuarterEnd(), (2, 3), ()),\n        ((1, 12), (), QuarterEnd(n=2), (2, 6), ()),\n        ((1, 12), (), QuarterEnd(n=-1), (1, 9), ()),\n        ((1, 12), (), QuarterEnd(n=-2), (1, 6), ()),\n        ((1, 1), (), QuarterEnd(month=2), (1, 2), ()),\n        ((1, 12), (5, 5, 5, 5), QuarterEnd(), (2, 3), (5, 5, 5, 5)),\n        ((1, 12), (5, 5, 5, 5), QuarterEnd(n=-1), (1, 9), (5, 5, 5, 5)),\n    ],\n    ids=_id_func,\n)\ndef test_add_quarter_end_onOffset(\n    calendar,\n    initial_year_month,\n    initial_sub_day,\n    offset,\n    expected_year_month,\n    expected_sub_day,\n):\n    date_type = get_date_type(calendar)\n    reference_args = initial_year_month + (1,)\n    reference = date_type(*reference_args)\n    initial_date_args = (\n        initial_year_month + (_days_in_month(reference),) + initial_sub_day\n    )\n    initial = date_type(*initial_date_args)\n    result = initial + offset\n    reference_args = expected_year_month + (1,)\n    reference = date_type(*reference_args)\n\n    # Here the days at the end of each month varies based on the calendar used\n    expected_date_args = (\n        expected_year_month + (_days_in_month(reference),) + expected_sub_day\n    )\n    expected = date_type(*expected_date_args)\n    assert result == expected\n\n\n# Note for all sub-monthly offsets, pandas always returns True for onOffset\n@pytest.mark.parametrize(\n    (""date_args"", ""offset"", ""expected""),\n    [\n        ((1, 1, 1), MonthBegin(), True),\n        ((1, 1, 1, 1), MonthBegin(), True),\n        ((1, 1, 5), MonthBegin(), False),\n        ((1, 1, 5), MonthEnd(), False),\n        ((1, 3, 1), QuarterBegin(), True),\n        ((1, 3, 1, 1), QuarterBegin(), True),\n        ((1, 3, 5), QuarterBegin(), False),\n        ((1, 12, 1), QuarterEnd(), False),\n        ((1, 1, 1), YearBegin(), True),\n        ((1, 1, 1, 1), YearBegin(), True),\n        ((1, 1, 5), YearBegin(), False),\n        ((1, 12, 1), YearEnd(), False),\n        ((1, 1, 1), Day(), True),\n        ((1, 1, 1, 1), Day(), True),\n        ((1, 1, 1), Hour(), True),\n        ((1, 1, 1), Minute(), True),\n        ((1, 1, 1), Second(), True),\n    ],\n    ids=_id_func,\n)\ndef test_onOffset(calendar, date_args, offset, expected):\n    date_type = get_date_type(calendar)\n    date = date_type(*date_args)\n    result = offset.onOffset(date)\n    assert result == expected\n\n\n@pytest.mark.parametrize(\n    (""year_month_args"", ""sub_day_args"", ""offset""),\n    [\n        ((1, 1), (), MonthEnd()),\n        ((1, 1), (1,), MonthEnd()),\n        ((1, 12), (), QuarterEnd()),\n        ((1, 1), (), QuarterEnd(month=1)),\n        ((1, 12), (), YearEnd()),\n        ((1, 1), (), YearEnd(month=1)),\n    ],\n    ids=_id_func,\n)\ndef test_onOffset_month_or_quarter_or_year_end(\n    calendar, year_month_args, sub_day_args, offset\n):\n    date_type = get_date_type(calendar)\n    reference_args = year_month_args + (1,)\n    reference = date_type(*reference_args)\n    date_args = year_month_args + (_days_in_month(reference),) + sub_day_args\n    date = date_type(*date_args)\n    result = offset.onOffset(date)\n    assert result\n\n\n@pytest.mark.parametrize(\n    (""offset"", ""initial_date_args"", ""partial_expected_date_args""),\n    [\n        (YearBegin(), (1, 3, 1), (2, 1)),\n        (YearBegin(), (1, 1, 1), (1, 1)),\n        (YearBegin(n=2), (1, 3, 1), (2, 1)),\n        (YearBegin(n=2, month=2), (1, 3, 1), (2, 2)),\n        (YearEnd(), (1, 3, 1), (1, 12)),\n        (YearEnd(n=2), (1, 3, 1), (1, 12)),\n        (YearEnd(n=2, month=2), (1, 3, 1), (2, 2)),\n        (YearEnd(n=2, month=4), (1, 4, 30), (1, 4)),\n        (QuarterBegin(), (1, 3, 2), (1, 6)),\n        (QuarterBegin(), (1, 4, 1), (1, 6)),\n        (QuarterBegin(n=2), (1, 4, 1), (1, 6)),\n        (QuarterBegin(n=2, month=2), (1, 4, 1), (1, 5)),\n        (QuarterEnd(), (1, 3, 1), (1, 3)),\n        (QuarterEnd(n=2), (1, 3, 1), (1, 3)),\n        (QuarterEnd(n=2, month=2), (1, 3, 1), (1, 5)),\n        (QuarterEnd(n=2, month=4), (1, 4, 30), (1, 4)),\n        (MonthBegin(), (1, 3, 2), (1, 4)),\n        (MonthBegin(), (1, 3, 1), (1, 3)),\n        (MonthBegin(n=2), (1, 3, 2), (1, 4)),\n        (MonthEnd(), (1, 3, 2), (1, 3)),\n        (MonthEnd(), (1, 4, 30), (1, 4)),\n        (MonthEnd(n=2), (1, 3, 2), (1, 3)),\n        (Day(), (1, 3, 2, 1), (1, 3, 2, 1)),\n        (Hour(), (1, 3, 2, 1, 1), (1, 3, 2, 1, 1)),\n        (Minute(), (1, 3, 2, 1, 1, 1), (1, 3, 2, 1, 1, 1)),\n        (Second(), (1, 3, 2, 1, 1, 1, 1), (1, 3, 2, 1, 1, 1, 1)),\n    ],\n    ids=_id_func,\n)\ndef test_rollforward(calendar, offset, initial_date_args, partial_expected_date_args):\n    date_type = get_date_type(calendar)\n    initial = date_type(*initial_date_args)\n    if isinstance(offset, (MonthBegin, QuarterBegin, YearBegin)):\n        expected_date_args = partial_expected_date_args + (1,)\n    elif isinstance(offset, (MonthEnd, QuarterEnd, YearEnd)):\n        reference_args = partial_expected_date_args + (1,)\n        reference = date_type(*reference_args)\n        expected_date_args = partial_expected_date_args + (_days_in_month(reference),)\n    else:\n        expected_date_args = partial_expected_date_args\n    expected = date_type(*expected_date_args)\n    result = offset.rollforward(initial)\n    assert result == expected\n\n\n@pytest.mark.parametrize(\n    (""offset"", ""initial_date_args"", ""partial_expected_date_args""),\n    [\n        (YearBegin(), (1, 3, 1), (1, 1)),\n        (YearBegin(n=2), (1, 3, 1), (1, 1)),\n        (YearBegin(n=2, month=2), (1, 3, 1), (1, 2)),\n        (YearBegin(), (1, 1, 1), (1, 1)),\n        (YearBegin(n=2, month=2), (1, 2, 1), (1, 2)),\n        (YearEnd(), (2, 3, 1), (1, 12)),\n        (YearEnd(n=2), (2, 3, 1), (1, 12)),\n        (YearEnd(n=2, month=2), (2, 3, 1), (2, 2)),\n        (YearEnd(month=4), (1, 4, 30), (1, 4)),\n        (QuarterBegin(), (1, 3, 2), (1, 3)),\n        (QuarterBegin(), (1, 4, 1), (1, 3)),\n        (QuarterBegin(n=2), (1, 4, 1), (1, 3)),\n        (QuarterBegin(n=2, month=2), (1, 4, 1), (1, 2)),\n        (QuarterEnd(), (2, 3, 1), (1, 12)),\n        (QuarterEnd(n=2), (2, 3, 1), (1, 12)),\n        (QuarterEnd(n=2, month=2), (2, 3, 1), (2, 2)),\n        (QuarterEnd(n=2, month=4), (1, 4, 30), (1, 4)),\n        (MonthBegin(), (1, 3, 2), (1, 3)),\n        (MonthBegin(n=2), (1, 3, 2), (1, 3)),\n        (MonthBegin(), (1, 3, 1), (1, 3)),\n        (MonthEnd(), (1, 3, 2), (1, 2)),\n        (MonthEnd(n=2), (1, 3, 2), (1, 2)),\n        (MonthEnd(), (1, 4, 30), (1, 4)),\n        (Day(), (1, 3, 2, 1), (1, 3, 2, 1)),\n        (Hour(), (1, 3, 2, 1, 1), (1, 3, 2, 1, 1)),\n        (Minute(), (1, 3, 2, 1, 1, 1), (1, 3, 2, 1, 1, 1)),\n        (Second(), (1, 3, 2, 1, 1, 1, 1), (1, 3, 2, 1, 1, 1, 1)),\n    ],\n    ids=_id_func,\n)\ndef test_rollback(calendar, offset, initial_date_args, partial_expected_date_args):\n    date_type = get_date_type(calendar)\n    initial = date_type(*initial_date_args)\n    if isinstance(offset, (MonthBegin, QuarterBegin, YearBegin)):\n        expected_date_args = partial_expected_date_args + (1,)\n    elif isinstance(offset, (MonthEnd, QuarterEnd, YearEnd)):\n        reference_args = partial_expected_date_args + (1,)\n        reference = date_type(*reference_args)\n        expected_date_args = partial_expected_date_args + (_days_in_month(reference),)\n    else:\n        expected_date_args = partial_expected_date_args\n    expected = date_type(*expected_date_args)\n    result = offset.rollback(initial)\n    assert result == expected\n\n\n_CFTIME_RANGE_TESTS = [\n    (\n        ""0001-01-01"",\n        ""0001-01-04"",\n        None,\n        ""D"",\n        None,\n        False,\n        [(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)],\n    ),\n    (\n        ""0001-01-01"",\n        ""0001-01-04"",\n        None,\n        ""D"",\n        ""left"",\n        False,\n        [(1, 1, 1), (1, 1, 2), (1, 1, 3)],\n    ),\n    (\n        ""0001-01-01"",\n        ""0001-01-04"",\n        None,\n        ""D"",\n        ""right"",\n        False,\n        [(1, 1, 2), (1, 1, 3), (1, 1, 4)],\n    ),\n    (\n        ""0001-01-01T01:00:00"",\n        ""0001-01-04"",\n        None,\n        ""D"",\n        None,\n        False,\n        [(1, 1, 1, 1), (1, 1, 2, 1), (1, 1, 3, 1)],\n    ),\n    (\n        ""0001-01-01T01:00:00"",\n        ""0001-01-04"",\n        None,\n        ""D"",\n        None,\n        True,\n        [(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)],\n    ),\n    (\n        ""0001-01-01"",\n        None,\n        4,\n        ""D"",\n        None,\n        False,\n        [(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)],\n    ),\n    (\n        None,\n        ""0001-01-04"",\n        4,\n        ""D"",\n        None,\n        False,\n        [(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)],\n    ),\n    (\n        (1, 1, 1),\n        ""0001-01-04"",\n        None,\n        ""D"",\n        None,\n        False,\n        [(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)],\n    ),\n    (\n        (1, 1, 1),\n        (1, 1, 4),\n        None,\n        ""D"",\n        None,\n        False,\n        [(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)],\n    ),\n    (\n        ""0001-01-30"",\n        ""0011-02-01"",\n        None,\n        ""3AS-JUN"",\n        None,\n        False,\n        [(1, 6, 1), (4, 6, 1), (7, 6, 1), (10, 6, 1)],\n    ),\n    (""0001-01-04"", ""0001-01-01"", None, ""D"", None, False, []),\n    (\n        ""0010"",\n        None,\n        4,\n        YearBegin(n=-2),\n        None,\n        False,\n        [(10, 1, 1), (8, 1, 1), (6, 1, 1), (4, 1, 1)],\n    ),\n    (\n        ""0001-01-01"",\n        ""0001-01-04"",\n        4,\n        None,\n        None,\n        False,\n        [(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)],\n    ),\n    (\n        ""0001-06-01"",\n        None,\n        4,\n        ""3QS-JUN"",\n        None,\n        False,\n        [(1, 6, 1), (2, 3, 1), (2, 12, 1), (3, 9, 1)],\n    ),\n]\n\n\n@pytest.mark.parametrize(\n    (""start"", ""end"", ""periods"", ""freq"", ""closed"", ""normalize"", ""expected_date_args""),\n    _CFTIME_RANGE_TESTS,\n    ids=_id_func,\n)\ndef test_cftime_range(\n    start, end, periods, freq, closed, normalize, calendar, expected_date_args\n):\n    date_type = get_date_type(calendar)\n    expected_dates = [date_type(*args) for args in expected_date_args]\n\n    if isinstance(start, tuple):\n        start = date_type(*start)\n    if isinstance(end, tuple):\n        end = date_type(*end)\n\n    result = cftime_range(\n        start=start,\n        end=end,\n        periods=periods,\n        freq=freq,\n        closed=closed,\n        normalize=normalize,\n        calendar=calendar,\n    )\n    resulting_dates = result.values\n\n    assert isinstance(result, CFTimeIndex)\n\n    if freq is not None:\n        np.testing.assert_equal(resulting_dates, expected_dates)\n    else:\n        # If we create a linear range of dates using cftime.num2date\n        # we will not get exact round number dates.  This is because\n        # datetime arithmetic in cftime is accurate approximately to\n        # 1 millisecond (see https://unidata.github.io/cftime/api.html).\n        deltas = resulting_dates - expected_dates\n        deltas = np.array([delta.total_seconds() for delta in deltas])\n        assert np.max(np.abs(deltas)) < 0.001\n\n\ndef test_cftime_range_name():\n    result = cftime_range(start=""2000"", periods=4, name=""foo"")\n    assert result.name == ""foo""\n\n    result = cftime_range(start=""2000"", periods=4)\n    assert result.name is None\n\n\n@pytest.mark.parametrize(\n    (""start"", ""end"", ""periods"", ""freq"", ""closed""),\n    [\n        (None, None, 5, ""A"", None),\n        (""2000"", None, None, ""A"", None),\n        (None, ""2000"", None, ""A"", None),\n        (""2000"", ""2001"", None, None, None),\n        (None, None, None, None, None),\n        (""2000"", ""2001"", None, ""A"", ""up""),\n        (""2000"", ""2001"", 5, ""A"", None),\n    ],\n)\ndef test_invalid_cftime_range_inputs(start, end, periods, freq, closed):\n    with pytest.raises(ValueError):\n        cftime_range(start, end, periods, freq, closed=closed)\n\n\n_CALENDAR_SPECIFIC_MONTH_END_TESTS = [\n    (""2M"", ""noleap"", [(2, 28), (4, 30), (6, 30), (8, 31), (10, 31), (12, 31)]),\n    (""2M"", ""all_leap"", [(2, 29), (4, 30), (6, 30), (8, 31), (10, 31), (12, 31)]),\n    (""2M"", ""360_day"", [(2, 30), (4, 30), (6, 30), (8, 30), (10, 30), (12, 30)]),\n    (""2M"", ""standard"", [(2, 29), (4, 30), (6, 30), (8, 31), (10, 31), (12, 31)]),\n    (""2M"", ""gregorian"", [(2, 29), (4, 30), (6, 30), (8, 31), (10, 31), (12, 31)]),\n    (""2M"", ""julian"", [(2, 29), (4, 30), (6, 30), (8, 31), (10, 31), (12, 31)]),\n]\n\n\n@pytest.mark.parametrize(\n    (""freq"", ""calendar"", ""expected_month_day""),\n    _CALENDAR_SPECIFIC_MONTH_END_TESTS,\n    ids=_id_func,\n)\ndef test_calendar_specific_month_end(freq, calendar, expected_month_day):\n    year = 2000  # Use a leap-year to highlight calendar differences\n    result = cftime_range(\n        start=""2000-02"", end=""2001"", freq=freq, calendar=calendar\n    ).values\n    date_type = get_date_type(calendar)\n    expected = [date_type(year, *args) for args in expected_month_day]\n    np.testing.assert_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    (""calendar"", ""start"", ""end"", ""expected_number_of_days""),\n    [\n        (""noleap"", ""2000"", ""2001"", 365),\n        (""all_leap"", ""2000"", ""2001"", 366),\n        (""360_day"", ""2000"", ""2001"", 360),\n        (""standard"", ""2000"", ""2001"", 366),\n        (""gregorian"", ""2000"", ""2001"", 366),\n        (""julian"", ""2000"", ""2001"", 366),\n        (""noleap"", ""2001"", ""2002"", 365),\n        (""all_leap"", ""2001"", ""2002"", 366),\n        (""360_day"", ""2001"", ""2002"", 360),\n        (""standard"", ""2001"", ""2002"", 365),\n        (""gregorian"", ""2001"", ""2002"", 365),\n        (""julian"", ""2001"", ""2002"", 365),\n    ],\n)\ndef test_calendar_year_length(calendar, start, end, expected_number_of_days):\n    result = cftime_range(start, end, freq=""D"", closed=""left"", calendar=calendar)\n    assert len(result) == expected_number_of_days\n\n\n@pytest.mark.parametrize(""freq"", [""A"", ""M"", ""D""])\ndef test_dayofweek_after_cftime_range(freq):\n    pytest.importorskip(""cftime"", minversion=""1.0.2.1"")\n    result = cftime_range(""2000-02-01"", periods=3, freq=freq).dayofweek\n    expected = pd.date_range(""2000-02-01"", periods=3, freq=freq).dayofweek\n    np.testing.assert_array_equal(result, expected)\n\n\n@pytest.mark.parametrize(""freq"", [""A"", ""M"", ""D""])\ndef test_dayofyear_after_cftime_range(freq):\n    pytest.importorskip(""cftime"", minversion=""1.0.2.1"")\n    result = cftime_range(""2000-02-01"", periods=3, freq=freq).dayofyear\n    expected = pd.date_range(""2000-02-01"", periods=3, freq=freq).dayofyear\n    np.testing.assert_array_equal(result, expected)\n\n\ndef test_cftime_range_standard_calendar_refers_to_gregorian():\n    from cftime import DatetimeGregorian\n\n    (result,) = cftime_range(""2000"", periods=1)\n    assert isinstance(result, DatetimeGregorian)\n'"
xarray/tests/test_cftimeindex.py,21,"b'from datetime import timedelta\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nimport xarray as xr\nfrom xarray.coding.cftimeindex import (\n    CFTimeIndex,\n    _parse_array_of_cftime_strings,\n    _parse_iso8601_with_reso,\n    _parsed_string_to_bounds,\n    assert_all_valid_date_type,\n    parse_iso8601,\n)\nfrom xarray.tests import assert_array_equal, assert_identical\n\nfrom . import raises_regex, requires_cftime, requires_cftime_1_1_0\nfrom .test_coding_times import (\n    _ALL_CALENDARS,\n    _NON_STANDARD_CALENDARS,\n    _all_cftime_date_types,\n)\n\n\ndef date_dict(year=None, month=None, day=None, hour=None, minute=None, second=None):\n    return dict(\n        year=year, month=month, day=day, hour=hour, minute=minute, second=second\n    )\n\n\nISO8601_STRING_TESTS = {\n    ""year"": (""1999"", date_dict(year=""1999"")),\n    ""month"": (""199901"", date_dict(year=""1999"", month=""01"")),\n    ""month-dash"": (""1999-01"", date_dict(year=""1999"", month=""01"")),\n    ""day"": (""19990101"", date_dict(year=""1999"", month=""01"", day=""01"")),\n    ""day-dash"": (""1999-01-01"", date_dict(year=""1999"", month=""01"", day=""01"")),\n    ""hour"": (""19990101T12"", date_dict(year=""1999"", month=""01"", day=""01"", hour=""12"")),\n    ""hour-dash"": (\n        ""1999-01-01T12"",\n        date_dict(year=""1999"", month=""01"", day=""01"", hour=""12""),\n    ),\n    ""minute"": (\n        ""19990101T1234"",\n        date_dict(year=""1999"", month=""01"", day=""01"", hour=""12"", minute=""34""),\n    ),\n    ""minute-dash"": (\n        ""1999-01-01T12:34"",\n        date_dict(year=""1999"", month=""01"", day=""01"", hour=""12"", minute=""34""),\n    ),\n    ""second"": (\n        ""19990101T123456"",\n        date_dict(\n            year=""1999"", month=""01"", day=""01"", hour=""12"", minute=""34"", second=""56""\n        ),\n    ),\n    ""second-dash"": (\n        ""1999-01-01T12:34:56"",\n        date_dict(\n            year=""1999"", month=""01"", day=""01"", hour=""12"", minute=""34"", second=""56""\n        ),\n    ),\n}\n\n\n@pytest.mark.parametrize(\n    (""string"", ""expected""),\n    list(ISO8601_STRING_TESTS.values()),\n    ids=list(ISO8601_STRING_TESTS.keys()),\n)\ndef test_parse_iso8601(string, expected):\n    result = parse_iso8601(string)\n    assert result == expected\n\n    with pytest.raises(ValueError):\n        parse_iso8601(string + ""3"")\n        parse_iso8601(string + "".3"")\n\n\n_CFTIME_CALENDARS = [\n    ""365_day"",\n    ""360_day"",\n    ""julian"",\n    ""all_leap"",\n    ""366_day"",\n    ""gregorian"",\n    ""proleptic_gregorian"",\n]\n\n\n@pytest.fixture(params=_CFTIME_CALENDARS)\ndef date_type(request):\n    return _all_cftime_date_types()[request.param]\n\n\n@pytest.fixture\ndef index(date_type):\n    dates = [\n        date_type(1, 1, 1),\n        date_type(1, 2, 1),\n        date_type(2, 1, 1),\n        date_type(2, 2, 1),\n    ]\n    return CFTimeIndex(dates)\n\n\n@pytest.fixture\ndef monotonic_decreasing_index(date_type):\n    dates = [\n        date_type(2, 2, 1),\n        date_type(2, 1, 1),\n        date_type(1, 2, 1),\n        date_type(1, 1, 1),\n    ]\n    return CFTimeIndex(dates)\n\n\n@pytest.fixture\ndef length_one_index(date_type):\n    dates = [date_type(1, 1, 1)]\n    return CFTimeIndex(dates)\n\n\n@pytest.fixture\ndef da(index):\n    return xr.DataArray([1, 2, 3, 4], coords=[index], dims=[""time""])\n\n\n@pytest.fixture\ndef series(index):\n    return pd.Series([1, 2, 3, 4], index=index)\n\n\n@pytest.fixture\ndef df(index):\n    return pd.DataFrame([1, 2, 3, 4], index=index)\n\n\n@pytest.fixture\ndef feb_days(date_type):\n    import cftime\n\n    if date_type is cftime.DatetimeAllLeap:\n        return 29\n    elif date_type is cftime.Datetime360Day:\n        return 30\n    else:\n        return 28\n\n\n@pytest.fixture\ndef dec_days(date_type):\n    import cftime\n\n    if date_type is cftime.Datetime360Day:\n        return 30\n    else:\n        return 31\n\n\n@pytest.fixture\ndef index_with_name(date_type):\n    dates = [\n        date_type(1, 1, 1),\n        date_type(1, 2, 1),\n        date_type(2, 1, 1),\n        date_type(2, 2, 1),\n    ]\n    return CFTimeIndex(dates, name=""foo"")\n\n\n@requires_cftime\n@pytest.mark.parametrize((""name"", ""expected_name""), [(""bar"", ""bar""), (None, ""foo"")])\ndef test_constructor_with_name(index_with_name, name, expected_name):\n    result = CFTimeIndex(index_with_name, name=name).name\n    assert result == expected_name\n\n\n@requires_cftime\ndef test_assert_all_valid_date_type(date_type, index):\n    import cftime\n\n    if date_type is cftime.DatetimeNoLeap:\n        mixed_date_types = np.array(\n            [date_type(1, 1, 1), cftime.DatetimeAllLeap(1, 2, 1)]\n        )\n    else:\n        mixed_date_types = np.array(\n            [date_type(1, 1, 1), cftime.DatetimeNoLeap(1, 2, 1)]\n        )\n    with pytest.raises(TypeError):\n        assert_all_valid_date_type(mixed_date_types)\n\n    with pytest.raises(TypeError):\n        assert_all_valid_date_type(np.array([1, date_type(1, 1, 1)]))\n\n    assert_all_valid_date_type(np.array([date_type(1, 1, 1), date_type(1, 2, 1)]))\n\n\n@requires_cftime\n@pytest.mark.parametrize(\n    (""field"", ""expected""),\n    [\n        (""year"", [1, 1, 2, 2]),\n        (""month"", [1, 2, 1, 2]),\n        (""day"", [1, 1, 1, 1]),\n        (""hour"", [0, 0, 0, 0]),\n        (""minute"", [0, 0, 0, 0]),\n        (""second"", [0, 0, 0, 0]),\n        (""microsecond"", [0, 0, 0, 0]),\n    ],\n)\ndef test_cftimeindex_field_accessors(index, field, expected):\n    result = getattr(index, field)\n    assert_array_equal(result, expected)\n\n\n@requires_cftime\ndef test_cftimeindex_dayofyear_accessor(index):\n    result = index.dayofyear\n    expected = [date.dayofyr for date in index]\n    assert_array_equal(result, expected)\n\n\n@requires_cftime\ndef test_cftimeindex_dayofweek_accessor(index):\n    result = index.dayofweek\n    expected = [date.dayofwk for date in index]\n    assert_array_equal(result, expected)\n\n\n@requires_cftime_1_1_0\ndef test_cftimeindex_days_in_month_accessor(index):\n    result = index.days_in_month\n    expected = [date.daysinmonth for date in index]\n    assert_array_equal(result, expected)\n\n\n@requires_cftime\n@pytest.mark.parametrize(\n    (""string"", ""date_args"", ""reso""),\n    [\n        (""1999"", (1999, 1, 1), ""year""),\n        (""199902"", (1999, 2, 1), ""month""),\n        (""19990202"", (1999, 2, 2), ""day""),\n        (""19990202T01"", (1999, 2, 2, 1), ""hour""),\n        (""19990202T0101"", (1999, 2, 2, 1, 1), ""minute""),\n        (""19990202T010156"", (1999, 2, 2, 1, 1, 56), ""second""),\n    ],\n)\ndef test_parse_iso8601_with_reso(date_type, string, date_args, reso):\n    expected_date = date_type(*date_args)\n    expected_reso = reso\n    result_date, result_reso = _parse_iso8601_with_reso(date_type, string)\n    assert result_date == expected_date\n    assert result_reso == expected_reso\n\n\n@requires_cftime\ndef test_parse_string_to_bounds_year(date_type, dec_days):\n    parsed = date_type(2, 2, 10, 6, 2, 8, 1)\n    expected_start = date_type(2, 1, 1)\n    expected_end = date_type(2, 12, dec_days, 23, 59, 59, 999999)\n    result_start, result_end = _parsed_string_to_bounds(date_type, ""year"", parsed)\n    assert result_start == expected_start\n    assert result_end == expected_end\n\n\n@requires_cftime\ndef test_parse_string_to_bounds_month_feb(date_type, feb_days):\n    parsed = date_type(2, 2, 10, 6, 2, 8, 1)\n    expected_start = date_type(2, 2, 1)\n    expected_end = date_type(2, 2, feb_days, 23, 59, 59, 999999)\n    result_start, result_end = _parsed_string_to_bounds(date_type, ""month"", parsed)\n    assert result_start == expected_start\n    assert result_end == expected_end\n\n\n@requires_cftime\ndef test_parse_string_to_bounds_month_dec(date_type, dec_days):\n    parsed = date_type(2, 12, 1)\n    expected_start = date_type(2, 12, 1)\n    expected_end = date_type(2, 12, dec_days, 23, 59, 59, 999999)\n    result_start, result_end = _parsed_string_to_bounds(date_type, ""month"", parsed)\n    assert result_start == expected_start\n    assert result_end == expected_end\n\n\n@requires_cftime\n@pytest.mark.parametrize(\n    (""reso"", ""ex_start_args"", ""ex_end_args""),\n    [\n        (""day"", (2, 2, 10), (2, 2, 10, 23, 59, 59, 999999)),\n        (""hour"", (2, 2, 10, 6), (2, 2, 10, 6, 59, 59, 999999)),\n        (""minute"", (2, 2, 10, 6, 2), (2, 2, 10, 6, 2, 59, 999999)),\n        (""second"", (2, 2, 10, 6, 2, 8), (2, 2, 10, 6, 2, 8, 999999)),\n    ],\n)\ndef test_parsed_string_to_bounds_sub_monthly(\n    date_type, reso, ex_start_args, ex_end_args\n):\n    parsed = date_type(2, 2, 10, 6, 2, 8, 123456)\n    expected_start = date_type(*ex_start_args)\n    expected_end = date_type(*ex_end_args)\n\n    result_start, result_end = _parsed_string_to_bounds(date_type, reso, parsed)\n    assert result_start == expected_start\n    assert result_end == expected_end\n\n\n@requires_cftime\ndef test_parsed_string_to_bounds_raises(date_type):\n    with pytest.raises(KeyError):\n        _parsed_string_to_bounds(date_type, ""a"", date_type(1, 1, 1))\n\n\n@requires_cftime\ndef test_get_loc(date_type, index):\n    result = index.get_loc(""0001"")\n    assert result == slice(0, 2)\n\n    result = index.get_loc(date_type(1, 2, 1))\n    assert result == 1\n\n    result = index.get_loc(""0001-02-01"")\n    assert result == slice(1, 2)\n\n    with raises_regex(KeyError, ""1234""):\n        index.get_loc(""1234"")\n\n\n@requires_cftime\n@pytest.mark.parametrize(""kind"", [""loc"", ""getitem""])\ndef test_get_slice_bound(date_type, index, kind):\n    result = index.get_slice_bound(""0001"", ""left"", kind)\n    expected = 0\n    assert result == expected\n\n    result = index.get_slice_bound(""0001"", ""right"", kind)\n    expected = 2\n    assert result == expected\n\n    result = index.get_slice_bound(date_type(1, 3, 1), ""left"", kind)\n    expected = 2\n    assert result == expected\n\n    result = index.get_slice_bound(date_type(1, 3, 1), ""right"", kind)\n    expected = 2\n    assert result == expected\n\n\n@requires_cftime\n@pytest.mark.parametrize(""kind"", [""loc"", ""getitem""])\ndef test_get_slice_bound_decreasing_index(date_type, monotonic_decreasing_index, kind):\n    result = monotonic_decreasing_index.get_slice_bound(""0001"", ""left"", kind)\n    expected = 2\n    assert result == expected\n\n    result = monotonic_decreasing_index.get_slice_bound(""0001"", ""right"", kind)\n    expected = 4\n    assert result == expected\n\n    result = monotonic_decreasing_index.get_slice_bound(\n        date_type(1, 3, 1), ""left"", kind\n    )\n    expected = 2\n    assert result == expected\n\n    result = monotonic_decreasing_index.get_slice_bound(\n        date_type(1, 3, 1), ""right"", kind\n    )\n    expected = 2\n    assert result == expected\n\n\n@requires_cftime\n@pytest.mark.parametrize(""kind"", [""loc"", ""getitem""])\ndef test_get_slice_bound_length_one_index(date_type, length_one_index, kind):\n    result = length_one_index.get_slice_bound(""0001"", ""left"", kind)\n    expected = 0\n    assert result == expected\n\n    result = length_one_index.get_slice_bound(""0001"", ""right"", kind)\n    expected = 1\n    assert result == expected\n\n    result = length_one_index.get_slice_bound(date_type(1, 3, 1), ""left"", kind)\n    expected = 1\n    assert result == expected\n\n    result = length_one_index.get_slice_bound(date_type(1, 3, 1), ""right"", kind)\n    expected = 1\n    assert result == expected\n\n\n@requires_cftime\ndef test_string_slice_length_one_index(length_one_index):\n    da = xr.DataArray([1], coords=[length_one_index], dims=[""time""])\n    result = da.sel(time=slice(""0001"", ""0001""))\n    assert_identical(result, da)\n\n\n@requires_cftime\ndef test_date_type_property(date_type, index):\n    assert index.date_type is date_type\n\n\n@requires_cftime\ndef test_contains(date_type, index):\n    assert ""0001-01-01"" in index\n    assert ""0001"" in index\n    assert ""0003"" not in index\n    assert date_type(1, 1, 1) in index\n    assert date_type(3, 1, 1) not in index\n\n\n@requires_cftime\ndef test_groupby(da):\n    result = da.groupby(""time.month"").sum(""time"")\n    expected = xr.DataArray([4, 6], coords=[[1, 2]], dims=[""month""])\n    assert_identical(result, expected)\n\n\nSEL_STRING_OR_LIST_TESTS = {\n    ""string"": ""0001"",\n    ""string-slice"": slice(""0001-01-01"", ""0001-12-30""),  # type: ignore\n    ""bool-list"": [True, True, False, False],\n}\n\n\n@requires_cftime\n@pytest.mark.parametrize(\n    ""sel_arg"",\n    list(SEL_STRING_OR_LIST_TESTS.values()),\n    ids=list(SEL_STRING_OR_LIST_TESTS.keys()),\n)\ndef test_sel_string_or_list(da, index, sel_arg):\n    expected = xr.DataArray([1, 2], coords=[index[:2]], dims=[""time""])\n    result = da.sel(time=sel_arg)\n    assert_identical(result, expected)\n\n\n@requires_cftime\ndef test_sel_date_slice_or_list(da, index, date_type):\n    expected = xr.DataArray([1, 2], coords=[index[:2]], dims=[""time""])\n    result = da.sel(time=slice(date_type(1, 1, 1), date_type(1, 12, 30)))\n    assert_identical(result, expected)\n\n    result = da.sel(time=[date_type(1, 1, 1), date_type(1, 2, 1)])\n    assert_identical(result, expected)\n\n\n@requires_cftime\ndef test_sel_date_scalar(da, date_type, index):\n    expected = xr.DataArray(1).assign_coords(time=index[0])\n    result = da.sel(time=date_type(1, 1, 1))\n    assert_identical(result, expected)\n\n\n@requires_cftime\ndef test_sel_date_distant_date(da, date_type, index):\n    expected = xr.DataArray(4).assign_coords(time=index[3])\n    result = da.sel(time=date_type(2000, 1, 1), method=""nearest"")\n    assert_identical(result, expected)\n\n\n@requires_cftime\n@pytest.mark.parametrize(\n    ""sel_kwargs"",\n    [\n        {""method"": ""nearest""},\n        {""method"": ""nearest"", ""tolerance"": timedelta(days=70)},\n        {""method"": ""nearest"", ""tolerance"": timedelta(days=1800000)},\n    ],\n)\ndef test_sel_date_scalar_nearest(da, date_type, index, sel_kwargs):\n    expected = xr.DataArray(2).assign_coords(time=index[1])\n    result = da.sel(time=date_type(1, 4, 1), **sel_kwargs)\n    assert_identical(result, expected)\n\n    expected = xr.DataArray(3).assign_coords(time=index[2])\n    result = da.sel(time=date_type(1, 11, 1), **sel_kwargs)\n    assert_identical(result, expected)\n\n\n@requires_cftime\n@pytest.mark.parametrize(\n    ""sel_kwargs"",\n    [{""method"": ""pad""}, {""method"": ""pad"", ""tolerance"": timedelta(days=365)}],\n)\ndef test_sel_date_scalar_pad(da, date_type, index, sel_kwargs):\n    expected = xr.DataArray(2).assign_coords(time=index[1])\n    result = da.sel(time=date_type(1, 4, 1), **sel_kwargs)\n    assert_identical(result, expected)\n\n    expected = xr.DataArray(2).assign_coords(time=index[1])\n    result = da.sel(time=date_type(1, 11, 1), **sel_kwargs)\n    assert_identical(result, expected)\n\n\n@requires_cftime\n@pytest.mark.parametrize(\n    ""sel_kwargs"",\n    [{""method"": ""backfill""}, {""method"": ""backfill"", ""tolerance"": timedelta(days=365)}],\n)\ndef test_sel_date_scalar_backfill(da, date_type, index, sel_kwargs):\n    expected = xr.DataArray(3).assign_coords(time=index[2])\n    result = da.sel(time=date_type(1, 4, 1), **sel_kwargs)\n    assert_identical(result, expected)\n\n    expected = xr.DataArray(3).assign_coords(time=index[2])\n    result = da.sel(time=date_type(1, 11, 1), **sel_kwargs)\n    assert_identical(result, expected)\n\n\n@requires_cftime\n@pytest.mark.parametrize(\n    ""sel_kwargs"",\n    [\n        {""method"": ""pad"", ""tolerance"": timedelta(days=20)},\n        {""method"": ""backfill"", ""tolerance"": timedelta(days=20)},\n        {""method"": ""nearest"", ""tolerance"": timedelta(days=20)},\n    ],\n)\ndef test_sel_date_scalar_tolerance_raises(da, date_type, sel_kwargs):\n    with pytest.raises(KeyError):\n        da.sel(time=date_type(1, 5, 1), **sel_kwargs)\n\n\n@requires_cftime\n@pytest.mark.parametrize(\n    ""sel_kwargs"",\n    [{""method"": ""nearest""}, {""method"": ""nearest"", ""tolerance"": timedelta(days=70)}],\n)\ndef test_sel_date_list_nearest(da, date_type, index, sel_kwargs):\n    expected = xr.DataArray([2, 2], coords=[[index[1], index[1]]], dims=[""time""])\n    result = da.sel(time=[date_type(1, 3, 1), date_type(1, 4, 1)], **sel_kwargs)\n    assert_identical(result, expected)\n\n    expected = xr.DataArray([2, 3], coords=[[index[1], index[2]]], dims=[""time""])\n    result = da.sel(time=[date_type(1, 3, 1), date_type(1, 12, 1)], **sel_kwargs)\n    assert_identical(result, expected)\n\n    expected = xr.DataArray([3, 3], coords=[[index[2], index[2]]], dims=[""time""])\n    result = da.sel(time=[date_type(1, 11, 1), date_type(1, 12, 1)], **sel_kwargs)\n    assert_identical(result, expected)\n\n\n@requires_cftime\n@pytest.mark.parametrize(\n    ""sel_kwargs"",\n    [{""method"": ""pad""}, {""method"": ""pad"", ""tolerance"": timedelta(days=365)}],\n)\ndef test_sel_date_list_pad(da, date_type, index, sel_kwargs):\n    expected = xr.DataArray([2, 2], coords=[[index[1], index[1]]], dims=[""time""])\n    result = da.sel(time=[date_type(1, 3, 1), date_type(1, 4, 1)], **sel_kwargs)\n    assert_identical(result, expected)\n\n\n@requires_cftime\n@pytest.mark.parametrize(\n    ""sel_kwargs"",\n    [{""method"": ""backfill""}, {""method"": ""backfill"", ""tolerance"": timedelta(days=365)}],\n)\ndef test_sel_date_list_backfill(da, date_type, index, sel_kwargs):\n    expected = xr.DataArray([3, 3], coords=[[index[2], index[2]]], dims=[""time""])\n    result = da.sel(time=[date_type(1, 3, 1), date_type(1, 4, 1)], **sel_kwargs)\n    assert_identical(result, expected)\n\n\n@requires_cftime\n@pytest.mark.parametrize(\n    ""sel_kwargs"",\n    [\n        {""method"": ""pad"", ""tolerance"": timedelta(days=20)},\n        {""method"": ""backfill"", ""tolerance"": timedelta(days=20)},\n        {""method"": ""nearest"", ""tolerance"": timedelta(days=20)},\n    ],\n)\ndef test_sel_date_list_tolerance_raises(da, date_type, sel_kwargs):\n    with pytest.raises(KeyError):\n        da.sel(time=[date_type(1, 2, 1), date_type(1, 5, 1)], **sel_kwargs)\n\n\n@requires_cftime\ndef test_isel(da, index):\n    expected = xr.DataArray(1).assign_coords(time=index[0])\n    result = da.isel(time=0)\n    assert_identical(result, expected)\n\n    expected = xr.DataArray([1, 2], coords=[index[:2]], dims=[""time""])\n    result = da.isel(time=[0, 1])\n    assert_identical(result, expected)\n\n\n@pytest.fixture\ndef scalar_args(date_type):\n    return [date_type(1, 1, 1)]\n\n\n@pytest.fixture\ndef range_args(date_type):\n    return [\n        ""0001"",\n        slice(""0001-01-01"", ""0001-12-30""),\n        slice(None, ""0001-12-30""),\n        slice(date_type(1, 1, 1), date_type(1, 12, 30)),\n        slice(None, date_type(1, 12, 30)),\n    ]\n\n\n@requires_cftime\ndef test_indexing_in_series_getitem(series, index, scalar_args, range_args):\n    for arg in scalar_args:\n        assert series[arg] == 1\n\n    expected = pd.Series([1, 2], index=index[:2])\n    for arg in range_args:\n        assert series[arg].equals(expected)\n\n\n@requires_cftime\ndef test_indexing_in_series_loc(series, index, scalar_args, range_args):\n    for arg in scalar_args:\n        assert series.loc[arg] == 1\n\n    expected = pd.Series([1, 2], index=index[:2])\n    for arg in range_args:\n        assert series.loc[arg].equals(expected)\n\n\n@requires_cftime\ndef test_indexing_in_series_iloc(series, index):\n    expected = 1\n    assert series.iloc[0] == expected\n\n    expected = pd.Series([1, 2], index=index[:2])\n    assert series.iloc[:2].equals(expected)\n\n\n@requires_cftime\ndef test_series_dropna(index):\n    series = pd.Series([0.0, 1.0, np.nan, np.nan], index=index)\n    expected = series.iloc[:2]\n    result = series.dropna()\n    assert result.equals(expected)\n\n\n@requires_cftime\ndef test_indexing_in_dataframe_loc(df, index, scalar_args, range_args):\n    expected = pd.Series([1], name=index[0])\n    for arg in scalar_args:\n        result = df.loc[arg]\n        assert result.equals(expected)\n\n    expected = pd.DataFrame([1, 2], index=index[:2])\n    for arg in range_args:\n        result = df.loc[arg]\n        assert result.equals(expected)\n\n\n@requires_cftime\ndef test_indexing_in_dataframe_iloc(df, index):\n    expected = pd.Series([1], name=index[0])\n    result = df.iloc[0]\n    assert result.equals(expected)\n    assert result.equals(expected)\n\n    expected = pd.DataFrame([1, 2], index=index[:2])\n    result = df.iloc[:2]\n    assert result.equals(expected)\n\n\n@requires_cftime\ndef test_concat_cftimeindex(date_type):\n    da1 = xr.DataArray(\n        [1.0, 2.0], coords=[[date_type(1, 1, 1), date_type(1, 2, 1)]], dims=[""time""]\n    )\n    da2 = xr.DataArray(\n        [3.0, 4.0], coords=[[date_type(1, 3, 1), date_type(1, 4, 1)]], dims=[""time""]\n    )\n    da = xr.concat([da1, da2], dim=""time"")\n\n    assert isinstance(da.indexes[""time""], CFTimeIndex)\n\n\n@requires_cftime\ndef test_empty_cftimeindex():\n    index = CFTimeIndex([])\n    assert index.date_type is None\n\n\n@requires_cftime\ndef test_cftimeindex_add(index):\n    date_type = index.date_type\n    expected_dates = [\n        date_type(1, 1, 2),\n        date_type(1, 2, 2),\n        date_type(2, 1, 2),\n        date_type(2, 2, 2),\n    ]\n    expected = CFTimeIndex(expected_dates)\n    result = index + timedelta(days=1)\n    assert result.equals(expected)\n    assert isinstance(result, CFTimeIndex)\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _CFTIME_CALENDARS)\ndef test_cftimeindex_add_timedeltaindex(calendar):\n    a = xr.cftime_range(""2000"", periods=5, calendar=calendar)\n    deltas = pd.TimedeltaIndex([timedelta(days=2) for _ in range(5)])\n    result = a + deltas\n    expected = a.shift(2, ""D"")\n    assert result.equals(expected)\n    assert isinstance(result, CFTimeIndex)\n\n\n@requires_cftime\ndef test_cftimeindex_radd(index):\n    date_type = index.date_type\n    expected_dates = [\n        date_type(1, 1, 2),\n        date_type(1, 2, 2),\n        date_type(2, 1, 2),\n        date_type(2, 2, 2),\n    ]\n    expected = CFTimeIndex(expected_dates)\n    result = timedelta(days=1) + index\n    assert result.equals(expected)\n    assert isinstance(result, CFTimeIndex)\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _CFTIME_CALENDARS)\ndef test_timedeltaindex_add_cftimeindex(calendar):\n    a = xr.cftime_range(""2000"", periods=5, calendar=calendar)\n    deltas = pd.TimedeltaIndex([timedelta(days=2) for _ in range(5)])\n    result = deltas + a\n    expected = a.shift(2, ""D"")\n    assert result.equals(expected)\n    assert isinstance(result, CFTimeIndex)\n\n\n@requires_cftime\ndef test_cftimeindex_sub_timedelta(index):\n    date_type = index.date_type\n    expected_dates = [\n        date_type(1, 1, 2),\n        date_type(1, 2, 2),\n        date_type(2, 1, 2),\n        date_type(2, 2, 2),\n    ]\n    expected = CFTimeIndex(expected_dates)\n    result = index + timedelta(days=2)\n    result = result - timedelta(days=1)\n    assert result.equals(expected)\n    assert isinstance(result, CFTimeIndex)\n\n\n@requires_cftime\n@pytest.mark.parametrize(\n    ""other"",\n    [np.array(4 * [timedelta(days=1)]), np.array(timedelta(days=1))],\n    ids=[""1d-array"", ""scalar-array""],\n)\ndef test_cftimeindex_sub_timedelta_array(index, other):\n    date_type = index.date_type\n    expected_dates = [\n        date_type(1, 1, 2),\n        date_type(1, 2, 2),\n        date_type(2, 1, 2),\n        date_type(2, 2, 2),\n    ]\n    expected = CFTimeIndex(expected_dates)\n    result = index + timedelta(days=2)\n    result = result - other\n    assert result.equals(expected)\n    assert isinstance(result, CFTimeIndex)\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _CFTIME_CALENDARS)\ndef test_cftimeindex_sub_cftimeindex(calendar):\n    a = xr.cftime_range(""2000"", periods=5, calendar=calendar)\n    b = a.shift(2, ""D"")\n    result = b - a\n    expected = pd.TimedeltaIndex([timedelta(days=2) for _ in range(5)])\n    assert result.equals(expected)\n    assert isinstance(result, pd.TimedeltaIndex)\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _CFTIME_CALENDARS)\ndef test_cftimeindex_sub_cftime_datetime(calendar):\n    a = xr.cftime_range(""2000"", periods=5, calendar=calendar)\n    result = a - a[0]\n    expected = pd.TimedeltaIndex([timedelta(days=i) for i in range(5)])\n    assert result.equals(expected)\n    assert isinstance(result, pd.TimedeltaIndex)\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _CFTIME_CALENDARS)\ndef test_cftime_datetime_sub_cftimeindex(calendar):\n    a = xr.cftime_range(""2000"", periods=5, calendar=calendar)\n    result = a[0] - a\n    expected = pd.TimedeltaIndex([timedelta(days=-i) for i in range(5)])\n    assert result.equals(expected)\n    assert isinstance(result, pd.TimedeltaIndex)\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _CFTIME_CALENDARS)\ndef test_distant_cftime_datetime_sub_cftimeindex(calendar):\n    a = xr.cftime_range(""2000"", periods=5, calendar=calendar)\n    with pytest.raises(ValueError, match=""difference exceeds""):\n        a.date_type(1, 1, 1) - a\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _CFTIME_CALENDARS)\ndef test_cftimeindex_sub_timedeltaindex(calendar):\n    a = xr.cftime_range(""2000"", periods=5, calendar=calendar)\n    deltas = pd.TimedeltaIndex([timedelta(days=2) for _ in range(5)])\n    result = a - deltas\n    expected = a.shift(-2, ""D"")\n    assert result.equals(expected)\n    assert isinstance(result, CFTimeIndex)\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _CFTIME_CALENDARS)\ndef test_cftimeindex_sub_index_of_cftime_datetimes(calendar):\n    a = xr.cftime_range(""2000"", periods=5, calendar=calendar)\n    b = pd.Index(a.values)\n    expected = a - a\n    result = a - b\n    assert result.equals(expected)\n    assert isinstance(result, pd.TimedeltaIndex)\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _CFTIME_CALENDARS)\ndef test_cftimeindex_sub_not_implemented(calendar):\n    a = xr.cftime_range(""2000"", periods=5, calendar=calendar)\n    with pytest.raises(TypeError, match=""unsupported operand""):\n        a - 1\n\n\n@requires_cftime\ndef test_cftimeindex_rsub(index):\n    with pytest.raises(TypeError):\n        timedelta(days=1) - index\n\n\n@requires_cftime\n@pytest.mark.parametrize(""freq"", [""D"", timedelta(days=1)])\ndef test_cftimeindex_shift(index, freq):\n    date_type = index.date_type\n    expected_dates = [\n        date_type(1, 1, 3),\n        date_type(1, 2, 3),\n        date_type(2, 1, 3),\n        date_type(2, 2, 3),\n    ]\n    expected = CFTimeIndex(expected_dates)\n    result = index.shift(2, freq)\n    assert result.equals(expected)\n    assert isinstance(result, CFTimeIndex)\n\n\n@requires_cftime\ndef test_cftimeindex_shift_invalid_n():\n    index = xr.cftime_range(""2000"", periods=3)\n    with pytest.raises(TypeError):\n        index.shift(""a"", ""D"")\n\n\n@requires_cftime\ndef test_cftimeindex_shift_invalid_freq():\n    index = xr.cftime_range(""2000"", periods=3)\n    with pytest.raises(TypeError):\n        index.shift(1, 1)\n\n\n@requires_cftime\ndef test_parse_array_of_cftime_strings():\n    from cftime import DatetimeNoLeap\n\n    strings = np.array([[""2000-01-01"", ""2000-01-02""], [""2000-01-03"", ""2000-01-04""]])\n    expected = np.array(\n        [\n            [DatetimeNoLeap(2000, 1, 1), DatetimeNoLeap(2000, 1, 2)],\n            [DatetimeNoLeap(2000, 1, 3), DatetimeNoLeap(2000, 1, 4)],\n        ]\n    )\n\n    result = _parse_array_of_cftime_strings(strings, DatetimeNoLeap)\n    np.testing.assert_array_equal(result, expected)\n\n    # Test scalar array case\n    strings = np.array(""2000-01-01"")\n    expected = np.array(DatetimeNoLeap(2000, 1, 1))\n    result = _parse_array_of_cftime_strings(strings, DatetimeNoLeap)\n    np.testing.assert_array_equal(result, expected)\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _ALL_CALENDARS)\ndef test_strftime_of_cftime_array(calendar):\n    date_format = ""%Y%m%d%H%M""\n    cf_values = xr.cftime_range(""2000"", periods=5, calendar=calendar)\n    dt_values = pd.date_range(""2000"", periods=5)\n    expected = pd.Index(dt_values.strftime(date_format))\n    result = cf_values.strftime(date_format)\n    assert result.equals(expected)\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _ALL_CALENDARS)\n@pytest.mark.parametrize(""unsafe"", [False, True])\ndef test_to_datetimeindex(calendar, unsafe):\n    index = xr.cftime_range(""2000"", periods=5, calendar=calendar)\n    expected = pd.date_range(""2000"", periods=5)\n\n    if calendar in _NON_STANDARD_CALENDARS and not unsafe:\n        with pytest.warns(RuntimeWarning, match=""non-standard""):\n            result = index.to_datetimeindex()\n    else:\n        result = index.to_datetimeindex(unsafe=unsafe)\n\n    assert result.equals(expected)\n    np.testing.assert_array_equal(result, expected)\n    assert isinstance(result, pd.DatetimeIndex)\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _ALL_CALENDARS)\ndef test_to_datetimeindex_out_of_range(calendar):\n    index = xr.cftime_range(""0001"", periods=5, calendar=calendar)\n    with pytest.raises(ValueError, match=""0001""):\n        index.to_datetimeindex()\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", [""all_leap"", ""360_day""])\ndef test_to_datetimeindex_feb_29(calendar):\n    index = xr.cftime_range(""2001-02-28"", periods=2, calendar=calendar)\n    with pytest.raises(ValueError, match=""29""):\n        index.to_datetimeindex()\n\n\n@requires_cftime\n@pytest.mark.xfail(reason=""https://github.com/pandas-dev/pandas/issues/24263"")\ndef test_multiindex():\n    index = xr.cftime_range(""2001-01-01"", periods=100, calendar=""360_day"")\n    mindex = pd.MultiIndex.from_arrays([index])\n    assert mindex.get_loc(""2001-01"") == slice(0, 30)\n\n\n@requires_cftime\n@pytest.mark.parametrize(""freq"", [""3663S"", ""33T"", ""2H""])\n@pytest.mark.parametrize(""method"", [""floor"", ""ceil"", ""round""])\ndef test_rounding_methods_against_datetimeindex(freq, method):\n    expected = pd.date_range(""2000-01-02T01:03:51"", periods=10, freq=""1777S"")\n    expected = getattr(expected, method)(freq)\n    result = xr.cftime_range(""2000-01-02T01:03:51"", periods=10, freq=""1777S"")\n    result = getattr(result, method)(freq).to_datetimeindex()\n    assert result.equals(expected)\n\n\n@requires_cftime\n@pytest.mark.parametrize(""method"", [""floor"", ""ceil"", ""round""])\ndef test_rounding_methods_invalid_freq(method):\n    index = xr.cftime_range(""2000-01-02T01:03:51"", periods=10, freq=""1777S"")\n    with pytest.raises(ValueError, match=""fixed""):\n        getattr(index, method)(""MS"")\n\n\n@pytest.fixture\ndef rounding_index(date_type):\n    return xr.CFTimeIndex(\n        [\n            date_type(1, 1, 1, 1, 59, 59, 999512),\n            date_type(1, 1, 1, 3, 0, 1, 500001),\n            date_type(1, 1, 1, 7, 0, 6, 499999),\n        ]\n    )\n\n\n@requires_cftime\ndef test_ceil(rounding_index, date_type):\n    result = rounding_index.ceil(""S"")\n    expected = xr.CFTimeIndex(\n        [\n            date_type(1, 1, 1, 2, 0, 0, 0),\n            date_type(1, 1, 1, 3, 0, 2, 0),\n            date_type(1, 1, 1, 7, 0, 7, 0),\n        ]\n    )\n    assert result.equals(expected)\n\n\n@requires_cftime\ndef test_floor(rounding_index, date_type):\n    result = rounding_index.floor(""S"")\n    expected = xr.CFTimeIndex(\n        [\n            date_type(1, 1, 1, 1, 59, 59, 0),\n            date_type(1, 1, 1, 3, 0, 1, 0),\n            date_type(1, 1, 1, 7, 0, 6, 0),\n        ]\n    )\n    assert result.equals(expected)\n\n\n@requires_cftime\ndef test_round(rounding_index, date_type):\n    result = rounding_index.round(""S"")\n    expected = xr.CFTimeIndex(\n        [\n            date_type(1, 1, 1, 2, 0, 0, 0),\n            date_type(1, 1, 1, 3, 0, 2, 0),\n            date_type(1, 1, 1, 7, 0, 6, 0),\n        ]\n    )\n    assert result.equals(expected)\n\n\n@requires_cftime\ndef test_asi8(date_type):\n    index = xr.CFTimeIndex([date_type(1970, 1, 1), date_type(1970, 1, 2)])\n    result = index.asi8\n    expected = 1000000 * 86400 * np.array([0, 1])\n    np.testing.assert_array_equal(result, expected)\n\n\n@requires_cftime\ndef test_asi8_distant_date():\n    """"""Test that asi8 conversion is truly exact.""""""\n    import cftime\n\n    date_type = cftime.DatetimeProlepticGregorian\n    index = xr.CFTimeIndex([date_type(10731, 4, 22, 3, 25, 45, 123456)])\n    result = index.asi8\n    expected = np.array([1000000 * 86400 * 400 * 8000 + 12345 * 1000000 + 123456])\n    np.testing.assert_array_equal(result, expected)\n\n\n@requires_cftime_1_1_0\ndef test_infer_freq_valid_types():\n    cf_indx = xr.cftime_range(""2000-01-01"", periods=3, freq=""D"")\n    assert xr.infer_freq(cf_indx) == ""D""\n    assert xr.infer_freq(xr.DataArray(cf_indx)) == ""D""\n\n    pd_indx = pd.date_range(""2000-01-01"", periods=3, freq=""D"")\n    assert xr.infer_freq(pd_indx) == ""D""\n    assert xr.infer_freq(xr.DataArray(pd_indx)) == ""D""\n\n    pd_td_indx = pd.timedelta_range(start=""1D"", periods=3, freq=""D"")\n    assert xr.infer_freq(pd_td_indx) == ""D""\n    assert xr.infer_freq(xr.DataArray(pd_td_indx)) == ""D""\n\n\n@requires_cftime_1_1_0\ndef test_infer_freq_invalid_inputs():\n    # Non-datetime DataArray\n    with pytest.raises(ValueError, match=""must contain datetime-like objects""):\n        xr.infer_freq(xr.DataArray([0, 1, 2]))\n\n    indx = xr.cftime_range(""1990-02-03"", periods=4, freq=""MS"")\n    # 2D DataArray\n    with pytest.raises(ValueError, match=""must be 1D""):\n        xr.infer_freq(xr.DataArray([indx, indx]))\n\n    # CFTimeIndex too short\n    with pytest.raises(ValueError, match=""Need at least 3 dates to infer frequency""):\n        xr.infer_freq(indx[:2])\n\n    # Non-monotonic input\n    assert xr.infer_freq(indx[np.array([0, 2, 1, 3])]) is None\n\n    # Non-unique input\n    assert xr.infer_freq(indx[np.array([0, 1, 1, 2])]) is None\n\n    # No unique frequency (here 1st step is MS, second is 2MS)\n    assert xr.infer_freq(indx[np.array([0, 1, 3])]) is None\n\n    # Same, but for QS\n    indx = xr.cftime_range(""1990-02-03"", periods=4, freq=""QS"")\n    assert xr.infer_freq(indx[np.array([0, 1, 3])]) is None\n\n\n@requires_cftime_1_1_0\n@pytest.mark.parametrize(\n    ""freq"",\n    [\n        ""300AS-JAN"",\n        ""A-DEC"",\n        ""AS-JUL"",\n        ""2AS-FEB"",\n        ""Q-NOV"",\n        ""3QS-DEC"",\n        ""MS"",\n        ""4M"",\n        ""7D"",\n        ""D"",\n        ""30H"",\n        ""5T"",\n        ""40S"",\n    ],\n)\n@pytest.mark.parametrize(""calendar"", _CFTIME_CALENDARS)\ndef test_infer_freq(freq, calendar):\n    indx = xr.cftime_range(""2000-01-01"", periods=3, freq=freq, calendar=calendar)\n    out = xr.infer_freq(indx)\n    assert out == freq\n'"
xarray/tests/test_cftimeindex_resample.py,1,"b'import datetime\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nimport xarray as xr\nfrom xarray.core.resample_cftime import CFTimeGrouper\n\npytest.importorskip(""cftime"")\n\n\n# Create a list of pairs of similar-length initial and resample frequencies\n# that cover:\n# - Resampling from shorter to longer frequencies\n# - Resampling from longer to shorter frequencies\n# - Resampling from one initial frequency to another.\n# These are used to test the cftime version of resample against pandas\n# with a standard calendar.\nFREQS = [\n    (""8003D"", ""4001D""),\n    (""8003D"", ""16006D""),\n    (""8003D"", ""21AS""),\n    (""6H"", ""3H""),\n    (""6H"", ""12H""),\n    (""6H"", ""400T""),\n    (""3D"", ""D""),\n    (""3D"", ""6D""),\n    (""11D"", ""MS""),\n    (""3MS"", ""MS""),\n    (""3MS"", ""6MS""),\n    (""3MS"", ""85D""),\n    (""7M"", ""3M""),\n    (""7M"", ""14M""),\n    (""7M"", ""2QS-APR""),\n    (""43QS-AUG"", ""21QS-AUG""),\n    (""43QS-AUG"", ""86QS-AUG""),\n    (""43QS-AUG"", ""11A-JUN""),\n    (""11Q-JUN"", ""5Q-JUN""),\n    (""11Q-JUN"", ""22Q-JUN""),\n    (""11Q-JUN"", ""51MS""),\n    (""3AS-MAR"", ""AS-MAR""),\n    (""3AS-MAR"", ""6AS-MAR""),\n    (""3AS-MAR"", ""14Q-FEB""),\n    (""7A-MAY"", ""3A-MAY""),\n    (""7A-MAY"", ""14A-MAY""),\n    (""7A-MAY"", ""85M""),\n]\n\n\ndef da(index):\n    return xr.DataArray(\n        np.arange(100.0, 100.0 + index.size), coords=[index], dims=[""time""]\n    )\n\n\n@pytest.mark.parametrize(""freqs"", FREQS, ids=lambda x: ""{}->{}"".format(*x))\n@pytest.mark.parametrize(""closed"", [None, ""left"", ""right""])\n@pytest.mark.parametrize(""label"", [None, ""left"", ""right""])\n@pytest.mark.parametrize(""base"", [24, 31])\ndef test_resample(freqs, closed, label, base):\n    initial_freq, resample_freq = freqs\n    start = ""2000-01-01T12:07:01""\n    index_kwargs = dict(start=start, periods=5, freq=initial_freq)\n    datetime_index = pd.date_range(**index_kwargs)\n    cftime_index = xr.cftime_range(**index_kwargs)\n\n    loffset = ""12H""\n    try:\n        da_datetime = (\n            da(datetime_index)\n            .resample(\n                time=resample_freq,\n                closed=closed,\n                label=label,\n                base=base,\n                loffset=loffset,\n            )\n            .mean()\n        )\n    except ValueError:\n        with pytest.raises(ValueError):\n            da(cftime_index).resample(\n                time=resample_freq,\n                closed=closed,\n                label=label,\n                base=base,\n                loffset=loffset,\n            ).mean()\n    else:\n        da_cftime = (\n            da(cftime_index)\n            .resample(\n                time=resample_freq,\n                closed=closed,\n                label=label,\n                base=base,\n                loffset=loffset,\n            )\n            .mean()\n        )\n        da_cftime[""time""] = da_cftime.indexes[""time""].to_datetimeindex()\n        xr.testing.assert_identical(da_cftime, da_datetime)\n\n\n@pytest.mark.parametrize(\n    (""freq"", ""expected""),\n    [\n        (""S"", ""left""),\n        (""T"", ""left""),\n        (""H"", ""left""),\n        (""D"", ""left""),\n        (""M"", ""right""),\n        (""MS"", ""left""),\n        (""Q"", ""right""),\n        (""QS"", ""left""),\n        (""A"", ""right""),\n        (""AS"", ""left""),\n    ],\n)\ndef test_closed_label_defaults(freq, expected):\n    assert CFTimeGrouper(freq=freq).closed == expected\n    assert CFTimeGrouper(freq=freq).label == expected\n\n\n@pytest.mark.filterwarnings(""ignore:Converting a CFTimeIndex"")\n@pytest.mark.parametrize(\n    ""calendar"", [""gregorian"", ""noleap"", ""all_leap"", ""360_day"", ""julian""]\n)\ndef test_calendars(calendar):\n    # Limited testing for non-standard calendars\n    freq, closed, label, base = ""8001T"", None, None, 17\n    loffset = datetime.timedelta(hours=12)\n    xr_index = xr.cftime_range(\n        start=""2004-01-01T12:07:01"", periods=7, freq=""3D"", calendar=calendar\n    )\n    pd_index = pd.date_range(start=""2004-01-01T12:07:01"", periods=7, freq=""3D"")\n    da_cftime = (\n        da(xr_index)\n        .resample(time=freq, closed=closed, label=label, base=base, loffset=loffset)\n        .mean()\n    )\n    da_datetime = (\n        da(pd_index)\n        .resample(time=freq, closed=closed, label=label, base=base, loffset=loffset)\n        .mean()\n    )\n    da_cftime[""time""] = da_cftime.indexes[""time""].to_datetimeindex()\n    xr.testing.assert_identical(da_cftime, da_datetime)\n'"
xarray/tests/test_coding.py,12,"b'from contextlib import suppress\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nimport xarray as xr\nfrom xarray.coding import variables\nfrom xarray.conventions import decode_cf_variable, encode_cf_variable\n\nfrom . import assert_equal, assert_identical, requires_dask\n\nwith suppress(ImportError):\n    import dask.array as da\n\n\ndef test_CFMaskCoder_decode():\n    original = xr.Variable((""x"",), [0, -1, 1], {""_FillValue"": -1})\n    expected = xr.Variable((""x"",), [0, np.nan, 1])\n    coder = variables.CFMaskCoder()\n    encoded = coder.decode(original)\n    assert_identical(expected, encoded)\n\n\nencoding_with_dtype = {\n    ""dtype"": np.dtype(""float64""),\n    ""_FillValue"": np.float32(1e20),\n    ""missing_value"": np.float64(1e20),\n}\nencoding_without_dtype = {\n    ""_FillValue"": np.float32(1e20),\n    ""missing_value"": np.float64(1e20),\n}\nCFMASKCODER_ENCODE_DTYPE_CONFLICT_TESTS = {\n    ""numeric-with-dtype"": ([0.0, -1.0, 1.0], encoding_with_dtype),\n    ""numeric-without-dtype"": ([0.0, -1.0, 1.0], encoding_without_dtype),\n    ""times-with-dtype"": (pd.date_range(""2000"", periods=3), encoding_with_dtype),\n}\n\n\n@pytest.mark.parametrize(\n    (""data"", ""encoding""),\n    CFMASKCODER_ENCODE_DTYPE_CONFLICT_TESTS.values(),\n    ids=list(CFMASKCODER_ENCODE_DTYPE_CONFLICT_TESTS.keys()),\n)\ndef test_CFMaskCoder_encode_missing_fill_values_conflict(data, encoding):\n    original = xr.Variable((""x"",), data, encoding=encoding)\n    encoded = encode_cf_variable(original)\n\n    assert encoded.dtype == encoded.attrs[""missing_value""].dtype\n    assert encoded.dtype == encoded.attrs[""_FillValue""].dtype\n\n    with pytest.warns(variables.SerializationWarning):\n        roundtripped = decode_cf_variable(""foo"", encoded)\n        assert_identical(roundtripped, original)\n\n\ndef test_CFMaskCoder_missing_value():\n    expected = xr.DataArray(\n        np.array([[26915, 27755, -9999, 27705], [25595, -9999, 28315, -9999]]),\n        dims=[""npts"", ""ntimes""],\n        name=""tmpk"",\n    )\n    expected.attrs[""missing_value""] = -9999\n\n    decoded = xr.decode_cf(expected.to_dataset())\n    encoded, _ = xr.conventions.cf_encoder(decoded, decoded.attrs)\n\n    assert_equal(encoded[""tmpk""], expected.variable)\n\n    decoded.tmpk.encoding[""_FillValue""] = -9940\n    with pytest.raises(ValueError):\n        encoded, _ = xr.conventions.cf_encoder(decoded, decoded.attrs)\n\n\n@requires_dask\ndef test_CFMaskCoder_decode_dask():\n    original = xr.Variable((""x"",), [0, -1, 1], {""_FillValue"": -1}).chunk()\n    expected = xr.Variable((""x"",), [0, np.nan, 1])\n    coder = variables.CFMaskCoder()\n    encoded = coder.decode(original)\n    assert isinstance(encoded.data, da.Array)\n    assert_identical(expected, encoded)\n\n\n# TODO(shoyer): port other fill-value tests\n\n\n# TODO(shoyer): parameterize when we have more coders\ndef test_coder_roundtrip():\n    original = xr.Variable((""x"",), [0.0, np.nan, 1.0])\n    coder = variables.CFMaskCoder()\n    roundtripped = coder.decode(coder.encode(original))\n    assert_identical(original, roundtripped)\n\n\n@pytest.mark.parametrize(""dtype"", ""u1 u2 i1 i2 f2 f4"".split())\ndef test_scaling_converts_to_float32(dtype):\n    original = xr.Variable(\n        (""x"",), np.arange(10, dtype=dtype), encoding=dict(scale_factor=10)\n    )\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype == np.float32\n    roundtripped = coder.decode(encoded)\n    assert_identical(original, roundtripped)\n    assert roundtripped.dtype == np.float32\n'"
xarray/tests/test_coding_strings.py,33,"b'from contextlib import suppress\n\nimport numpy as np\nimport pytest\n\nfrom xarray import Variable\nfrom xarray.coding import strings\nfrom xarray.core import indexing\n\nfrom . import (\n    IndexerMaker,\n    assert_array_equal,\n    assert_identical,\n    raises_regex,\n    requires_dask,\n)\n\nwith suppress(ImportError):\n    import dask.array as da\n\n\ndef test_vlen_dtype():\n    dtype = strings.create_vlen_dtype(str)\n    assert dtype.metadata[""element_type""] == str\n    assert strings.is_unicode_dtype(dtype)\n    assert not strings.is_bytes_dtype(dtype)\n    assert strings.check_vlen_dtype(dtype) is str\n\n    dtype = strings.create_vlen_dtype(bytes)\n    assert dtype.metadata[""element_type""] == bytes\n    assert not strings.is_unicode_dtype(dtype)\n    assert strings.is_bytes_dtype(dtype)\n    assert strings.check_vlen_dtype(dtype) is bytes\n\n    assert strings.check_vlen_dtype(np.dtype(object)) is None\n\n\ndef test_EncodedStringCoder_decode():\n    coder = strings.EncodedStringCoder()\n\n    raw_data = np.array([b""abc"", ""\xc3\x9f\xe2\x88\x82\xc2\xb5\xe2\x88\x86"".encode()])\n    raw = Variable((""x"",), raw_data, {""_Encoding"": ""utf-8""})\n    actual = coder.decode(raw)\n\n    expected = Variable((""x"",), np.array([""abc"", ""\xc3\x9f\xe2\x88\x82\xc2\xb5\xe2\x88\x86""], dtype=object))\n    assert_identical(actual, expected)\n\n    assert_identical(coder.decode(actual[0]), expected[0])\n\n\n@requires_dask\ndef test_EncodedStringCoder_decode_dask():\n    coder = strings.EncodedStringCoder()\n\n    raw_data = np.array([b""abc"", ""\xc3\x9f\xe2\x88\x82\xc2\xb5\xe2\x88\x86"".encode()])\n    raw = Variable((""x"",), raw_data, {""_Encoding"": ""utf-8""}).chunk()\n    actual = coder.decode(raw)\n    assert isinstance(actual.data, da.Array)\n\n    expected = Variable((""x"",), np.array([""abc"", ""\xc3\x9f\xe2\x88\x82\xc2\xb5\xe2\x88\x86""], dtype=object))\n    assert_identical(actual, expected)\n\n    actual_indexed = coder.decode(actual[0])\n    assert isinstance(actual_indexed.data, da.Array)\n    assert_identical(actual_indexed, expected[0])\n\n\ndef test_EncodedStringCoder_encode():\n    dtype = strings.create_vlen_dtype(str)\n    raw_data = np.array([""abc"", ""\xc3\x9f\xe2\x88\x82\xc2\xb5\xe2\x88\x86""], dtype=dtype)\n    expected_data = np.array([r.encode(""utf-8"") for r in raw_data], dtype=object)\n\n    coder = strings.EncodedStringCoder(allows_unicode=True)\n    raw = Variable((""x"",), raw_data, encoding={""dtype"": ""S1""})\n    actual = coder.encode(raw)\n    expected = Variable((""x"",), expected_data, attrs={""_Encoding"": ""utf-8""})\n    assert_identical(actual, expected)\n\n    raw = Variable((""x"",), raw_data)\n    assert_identical(coder.encode(raw), raw)\n\n    coder = strings.EncodedStringCoder(allows_unicode=False)\n    assert_identical(coder.encode(raw), expected)\n\n\n@pytest.mark.parametrize(\n    ""original"",\n    [\n        Variable((""x"",), [b""ab"", b""cdef""]),\n        Variable((), b""ab""),\n        Variable((""x"",), [b""a"", b""b""]),\n        Variable((), b""a""),\n    ],\n)\ndef test_CharacterArrayCoder_roundtrip(original):\n    coder = strings.CharacterArrayCoder()\n    roundtripped = coder.decode(coder.encode(original))\n    assert_identical(original, roundtripped)\n\n\n@pytest.mark.parametrize(\n    ""data"",\n    [\n        np.array([b""a"", b""bc""]),\n        np.array([b""a"", b""bc""], dtype=strings.create_vlen_dtype(bytes)),\n    ],\n)\ndef test_CharacterArrayCoder_encode(data):\n    coder = strings.CharacterArrayCoder()\n    raw = Variable((""x"",), data)\n    actual = coder.encode(raw)\n    expected = Variable((""x"", ""string2""), np.array([[b""a"", b""""], [b""b"", b""c""]]))\n    assert_identical(actual, expected)\n\n\n@pytest.mark.parametrize(\n    [""original"", ""expected_char_dim_name""],\n    [\n        (Variable((""x"",), [b""ab"", b""cdef""]), ""string4""),\n        (Variable((""x"",), [b""ab"", b""cdef""], encoding={""char_dim_name"": ""foo""}), ""foo""),\n    ],\n)\ndef test_CharacterArrayCoder_char_dim_name(original, expected_char_dim_name):\n    coder = strings.CharacterArrayCoder()\n    encoded = coder.encode(original)\n    roundtripped = coder.decode(encoded)\n    assert encoded.dims[-1] == expected_char_dim_name\n    assert roundtripped.encoding[""char_dim_name""] == expected_char_dim_name\n    assert roundtripped.dims[-1] == original.dims[-1]\n\n\ndef test_StackedBytesArray():\n    array = np.array([[b""a"", b""b"", b""c""], [b""d"", b""e"", b""f""]], dtype=""S"")\n    actual = strings.StackedBytesArray(array)\n    expected = np.array([b""abc"", b""def""], dtype=""S"")\n    assert actual.dtype == expected.dtype\n    assert actual.shape == expected.shape\n    assert actual.size == expected.size\n    assert actual.ndim == expected.ndim\n    assert len(actual) == len(expected)\n    assert_array_equal(expected, actual)\n\n    B = IndexerMaker(indexing.BasicIndexer)\n    assert_array_equal(expected[:1], actual[B[:1]])\n    with pytest.raises(IndexError):\n        actual[B[:, :2]]\n\n\ndef test_StackedBytesArray_scalar():\n    array = np.array([b""a"", b""b"", b""c""], dtype=""S"")\n    actual = strings.StackedBytesArray(array)\n\n    expected = np.array(b""abc"")\n    assert actual.dtype == expected.dtype\n    assert actual.shape == expected.shape\n    assert actual.size == expected.size\n    assert actual.ndim == expected.ndim\n    with pytest.raises(TypeError):\n        len(actual)\n    np.testing.assert_array_equal(expected, actual)\n\n    B = IndexerMaker(indexing.BasicIndexer)\n    with pytest.raises(IndexError):\n        actual[B[:2]]\n\n\ndef test_StackedBytesArray_vectorized_indexing():\n    array = np.array([[b""a"", b""b"", b""c""], [b""d"", b""e"", b""f""]], dtype=""S"")\n    stacked = strings.StackedBytesArray(array)\n    expected = np.array([[b""abc"", b""def""], [b""def"", b""abc""]])\n\n    V = IndexerMaker(indexing.VectorizedIndexer)\n    indexer = V[np.array([[0, 1], [1, 0]])]\n    actual = stacked[indexer]\n    assert_array_equal(actual, expected)\n\n\ndef test_char_to_bytes():\n    array = np.array([[b""a"", b""b"", b""c""], [b""d"", b""e"", b""f""]])\n    expected = np.array([b""abc"", b""def""])\n    actual = strings.char_to_bytes(array)\n    assert_array_equal(actual, expected)\n\n    expected = np.array([b""ad"", b""be"", b""cf""])\n    actual = strings.char_to_bytes(array.T)  # non-contiguous\n    assert_array_equal(actual, expected)\n\n\ndef test_char_to_bytes_ndim_zero():\n    expected = np.array(b""a"")\n    actual = strings.char_to_bytes(expected)\n    assert_array_equal(actual, expected)\n\n\ndef test_char_to_bytes_size_zero():\n    array = np.zeros((3, 0), dtype=""S1"")\n    expected = np.array([b"""", b"""", b""""])\n    actual = strings.char_to_bytes(array)\n    assert_array_equal(actual, expected)\n\n\n@requires_dask\ndef test_char_to_bytes_dask():\n    numpy_array = np.array([[b""a"", b""b"", b""c""], [b""d"", b""e"", b""f""]])\n    array = da.from_array(numpy_array, ((2,), (3,)))\n    expected = np.array([b""abc"", b""def""])\n    actual = strings.char_to_bytes(array)\n    assert isinstance(actual, da.Array)\n    assert actual.chunks == ((2,),)\n    assert actual.dtype == ""S3""\n    assert_array_equal(np.array(actual), expected)\n\n    with raises_regex(ValueError, ""stacked dask character array""):\n        strings.char_to_bytes(array.rechunk(1))\n\n\ndef test_bytes_to_char():\n    array = np.array([[b""ab"", b""cd""], [b""ef"", b""gh""]])\n    expected = np.array([[[b""a"", b""b""], [b""c"", b""d""]], [[b""e"", b""f""], [b""g"", b""h""]]])\n    actual = strings.bytes_to_char(array)\n    assert_array_equal(actual, expected)\n\n    expected = np.array([[[b""a"", b""b""], [b""e"", b""f""]], [[b""c"", b""d""], [b""g"", b""h""]]])\n    actual = strings.bytes_to_char(array.T)  # non-contiguous\n    assert_array_equal(actual, expected)\n\n\n@requires_dask\ndef test_bytes_to_char_dask():\n    numpy_array = np.array([b""ab"", b""cd""])\n    array = da.from_array(numpy_array, ((1, 1),))\n    expected = np.array([[b""a"", b""b""], [b""c"", b""d""]])\n    actual = strings.bytes_to_char(array)\n    assert isinstance(actual, da.Array)\n    assert actual.chunks == ((1, 1), ((2,)))\n    assert actual.dtype == ""S1""\n    assert_array_equal(np.array(actual), expected)\n'"
xarray/tests/test_coding_times.py,93,"b'import warnings\nfrom itertools import product\n\nimport numpy as np\nimport pandas as pd\nimport pytest\nfrom pandas.errors import OutOfBoundsDatetime\n\nfrom xarray import DataArray, Dataset, Variable, coding, decode_cf\nfrom xarray.coding.times import (\n    cftime_to_nptime,\n    decode_cf_datetime,\n    encode_cf_datetime,\n    to_timedelta_unboxed,\n)\nfrom xarray.coding.variables import SerializationWarning\nfrom xarray.conventions import _update_bounds_attributes, cf_encoder\nfrom xarray.core.common import contains_cftime_datetimes\nfrom xarray.testing import assert_equal\n\nfrom . import arm_xfail, assert_array_equal, has_cftime, requires_cftime, requires_dask\n\n_NON_STANDARD_CALENDARS_SET = {\n    ""noleap"",\n    ""365_day"",\n    ""360_day"",\n    ""julian"",\n    ""all_leap"",\n    ""366_day"",\n}\n_ALL_CALENDARS = sorted(\n    _NON_STANDARD_CALENDARS_SET.union(coding.times._STANDARD_CALENDARS)\n)\n_NON_STANDARD_CALENDARS = sorted(_NON_STANDARD_CALENDARS_SET)\n_STANDARD_CALENDARS = sorted(coding.times._STANDARD_CALENDARS)\n_CF_DATETIME_NUM_DATES_UNITS = [\n    (np.arange(10), ""days since 2000-01-01""),\n    (np.arange(10).astype(""float64""), ""days since 2000-01-01""),\n    (np.arange(10).astype(""float32""), ""days since 2000-01-01""),\n    (np.arange(10).reshape(2, 5), ""days since 2000-01-01""),\n    (12300 + np.arange(5), ""hours since 1680-01-01 00:00:00""),\n    # here we add a couple minor formatting errors to test\n    # the robustness of the parsing algorithm.\n    (12300 + np.arange(5), ""hour since 1680-01-01  00:00:00""),\n    (12300 + np.arange(5), ""Hour  since 1680-01-01 00:00:00""),\n    (12300 + np.arange(5), "" Hour  since  1680-01-01 00:00:00 ""),\n    (10, ""days since 2000-01-01""),\n    ([10], ""daYs  since 2000-01-01""),\n    ([[10]], ""days since 2000-01-01""),\n    ([10, 10], ""days since 2000-01-01""),\n    (np.array(10), ""days since 2000-01-01""),\n    (0, ""days since 1000-01-01""),\n    ([0], ""days since 1000-01-01""),\n    ([[0]], ""days since 1000-01-01""),\n    (np.arange(2), ""days since 1000-01-01""),\n    (np.arange(0, 100000, 20000), ""days since 1900-01-01""),\n    (17093352.0, ""hours since 1-1-1 00:00:0.0""),\n    ([0.5, 1.5], ""hours since 1900-01-01T00:00:00""),\n    (0, ""milliseconds since 2000-01-01T00:00:00""),\n    (0, ""microseconds since 2000-01-01T00:00:00""),\n    (np.int32(788961600), ""seconds since 1981-01-01""),  # GH2002\n    (12300 + np.arange(5), ""hour since 1680-01-01 00:00:00.500000""),\n]\n_CF_DATETIME_TESTS = [\n    num_dates_units + (calendar,)\n    for num_dates_units, calendar in product(\n        _CF_DATETIME_NUM_DATES_UNITS, _STANDARD_CALENDARS\n    )\n]\n\n\ndef _all_cftime_date_types():\n    import cftime\n\n    return {\n        ""noleap"": cftime.DatetimeNoLeap,\n        ""365_day"": cftime.DatetimeNoLeap,\n        ""360_day"": cftime.Datetime360Day,\n        ""julian"": cftime.DatetimeJulian,\n        ""all_leap"": cftime.DatetimeAllLeap,\n        ""366_day"": cftime.DatetimeAllLeap,\n        ""gregorian"": cftime.DatetimeGregorian,\n        ""proleptic_gregorian"": cftime.DatetimeProlepticGregorian,\n    }\n\n\n@requires_cftime\n@pytest.mark.parametrize([""num_dates"", ""units"", ""calendar""], _CF_DATETIME_TESTS)\ndef test_cf_datetime(num_dates, units, calendar):\n    import cftime\n\n    expected = cftime.num2date(\n        num_dates, units, calendar, only_use_cftime_datetimes=True\n    )\n    min_y = np.ravel(np.atleast_1d(expected))[np.nanargmin(num_dates)].year\n    max_y = np.ravel(np.atleast_1d(expected))[np.nanargmax(num_dates)].year\n    if min_y >= 1678 and max_y < 2262:\n        expected = cftime_to_nptime(expected)\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", ""Unable to decode time axis"")\n        actual = coding.times.decode_cf_datetime(num_dates, units, calendar)\n\n    abs_diff = np.asarray(abs(actual - expected)).ravel()\n    abs_diff = pd.to_timedelta(abs_diff.tolist()).to_numpy()\n\n    # once we no longer support versions of netCDF4 older than 1.1.5,\n    # we could do this check with near microsecond accuracy:\n    # https://github.com/Unidata/netcdf4-python/issues/355\n    assert (abs_diff <= np.timedelta64(1, ""s"")).all()\n    encoded, _, _ = coding.times.encode_cf_datetime(actual, units, calendar)\n    if ""1-1-1"" not in units:\n        # pandas parses this date very strangely, so the original\n        # units/encoding cannot be preserved in this case:\n        # (Pdb) pd.to_datetime(\'1-1-1 00:00:0.0\')\n        # Timestamp(\'2001-01-01 00:00:00\')\n        assert_array_equal(num_dates, np.around(encoded, 1))\n        if hasattr(num_dates, ""ndim"") and num_dates.ndim == 1 and ""1000"" not in units:\n            # verify that wrapping with a pandas.Index works\n            # note that it *does not* currently work to even put\n            # non-datetime64 compatible dates into a pandas.Index\n            encoded, _, _ = coding.times.encode_cf_datetime(\n                pd.Index(actual), units, calendar\n            )\n            assert_array_equal(num_dates, np.around(encoded, 1))\n\n\n@requires_cftime\ndef test_decode_cf_datetime_overflow():\n    # checks for\n    # https://github.com/pydata/pandas/issues/14068\n    # https://github.com/pydata/xarray/issues/975\n    from cftime import DatetimeGregorian\n\n    datetime = DatetimeGregorian\n    units = ""days since 2000-01-01 00:00:00""\n\n    # date after 2262 and before 1678\n    days = (-117608, 95795)\n    expected = (datetime(1677, 12, 31), datetime(2262, 4, 12))\n\n    for i, day in enumerate(days):\n        with warnings.catch_warnings():\n            warnings.filterwarnings(""ignore"", ""Unable to decode time axis"")\n            result = coding.times.decode_cf_datetime(day, units)\n        assert result == expected[i]\n\n\ndef test_decode_cf_datetime_non_standard_units():\n    expected = pd.date_range(periods=100, start=""1970-01-01"", freq=""h"")\n    # netCDFs from madis.noaa.gov use this format for their time units\n    # they cannot be parsed by cftime, but pd.Timestamp works\n    units = ""hours since 1-1-1970""\n    actual = coding.times.decode_cf_datetime(np.arange(100), units)\n    assert_array_equal(actual, expected)\n\n\n@requires_cftime\ndef test_decode_cf_datetime_non_iso_strings():\n    # datetime strings that are _almost_ ISO compliant but not quite,\n    # but which cftime.num2date can still parse correctly\n    expected = pd.date_range(periods=100, start=""2000-01-01"", freq=""h"")\n    cases = [\n        (np.arange(100), ""hours since 2000-01-01 0""),\n        (np.arange(100), ""hours since 2000-1-1 0""),\n        (np.arange(100), ""hours since 2000-01-01 0:00""),\n    ]\n    for num_dates, units in cases:\n        actual = coding.times.decode_cf_datetime(num_dates, units)\n        abs_diff = abs(actual - expected.values)\n        # once we no longer support versions of netCDF4 older than 1.1.5,\n        # we could do this check with near microsecond accuracy:\n        # https://github.com/Unidata/netcdf4-python/issues/355\n        assert (abs_diff <= np.timedelta64(1, ""s"")).all()\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _STANDARD_CALENDARS)\ndef test_decode_standard_calendar_inside_timestamp_range(calendar):\n    import cftime\n\n    units = ""days since 0001-01-01""\n    times = pd.date_range(""2001-04-01-00"", end=""2001-04-30-23"", freq=""H"")\n    time = cftime.date2num(times.to_pydatetime(), units, calendar=calendar)\n    expected = times.values\n    expected_dtype = np.dtype(""M8[ns]"")\n\n    actual = coding.times.decode_cf_datetime(time, units, calendar=calendar)\n    assert actual.dtype == expected_dtype\n    abs_diff = abs(actual - expected)\n    # once we no longer support versions of netCDF4 older than 1.1.5,\n    # we could do this check with near microsecond accuracy:\n    # https://github.com/Unidata/netcdf4-python/issues/355\n    assert (abs_diff <= np.timedelta64(1, ""s"")).all()\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _NON_STANDARD_CALENDARS)\ndef test_decode_non_standard_calendar_inside_timestamp_range(calendar):\n    import cftime\n\n    units = ""days since 0001-01-01""\n    times = pd.date_range(""2001-04-01-00"", end=""2001-04-30-23"", freq=""H"")\n    non_standard_time = cftime.date2num(times.to_pydatetime(), units, calendar=calendar)\n\n    expected = cftime.num2date(\n        non_standard_time, units, calendar=calendar, only_use_cftime_datetimes=True\n    )\n    expected_dtype = np.dtype(""O"")\n\n    actual = coding.times.decode_cf_datetime(\n        non_standard_time, units, calendar=calendar\n    )\n    assert actual.dtype == expected_dtype\n    abs_diff = abs(actual - expected)\n    # once we no longer support versions of netCDF4 older than 1.1.5,\n    # we could do this check with near microsecond accuracy:\n    # https://github.com/Unidata/netcdf4-python/issues/355\n    assert (abs_diff <= np.timedelta64(1, ""s"")).all()\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _ALL_CALENDARS)\ndef test_decode_dates_outside_timestamp_range(calendar):\n    import cftime\n    from datetime import datetime\n\n    units = ""days since 0001-01-01""\n    times = [datetime(1, 4, 1, h) for h in range(1, 5)]\n    time = cftime.date2num(times, units, calendar=calendar)\n\n    expected = cftime.num2date(\n        time, units, calendar=calendar, only_use_cftime_datetimes=True\n    )\n    expected_date_type = type(expected[0])\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", ""Unable to decode time axis"")\n        actual = coding.times.decode_cf_datetime(time, units, calendar=calendar)\n    assert all(isinstance(value, expected_date_type) for value in actual)\n    abs_diff = abs(actual - expected)\n    # once we no longer support versions of netCDF4 older than 1.1.5,\n    # we could do this check with near microsecond accuracy:\n    # https://github.com/Unidata/netcdf4-python/issues/355\n    assert (abs_diff <= np.timedelta64(1, ""s"")).all()\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _STANDARD_CALENDARS)\ndef test_decode_standard_calendar_single_element_inside_timestamp_range(calendar):\n    units = ""days since 0001-01-01""\n    for num_time in [735368, [735368], [[735368]]]:\n        with warnings.catch_warnings():\n            warnings.filterwarnings(""ignore"", ""Unable to decode time axis"")\n            actual = coding.times.decode_cf_datetime(num_time, units, calendar=calendar)\n        assert actual.dtype == np.dtype(""M8[ns]"")\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _NON_STANDARD_CALENDARS)\ndef test_decode_non_standard_calendar_single_element_inside_timestamp_range(calendar):\n    units = ""days since 0001-01-01""\n    for num_time in [735368, [735368], [[735368]]]:\n        with warnings.catch_warnings():\n            warnings.filterwarnings(""ignore"", ""Unable to decode time axis"")\n            actual = coding.times.decode_cf_datetime(num_time, units, calendar=calendar)\n        assert actual.dtype == np.dtype(""O"")\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _NON_STANDARD_CALENDARS)\ndef test_decode_single_element_outside_timestamp_range(calendar):\n    import cftime\n\n    units = ""days since 0001-01-01""\n    for days in [1, 1470376]:\n        for num_time in [days, [days], [[days]]]:\n            with warnings.catch_warnings():\n                warnings.filterwarnings(""ignore"", ""Unable to decode time axis"")\n                actual = coding.times.decode_cf_datetime(\n                    num_time, units, calendar=calendar\n                )\n\n            expected = cftime.num2date(\n                days, units, calendar, only_use_cftime_datetimes=True\n            )\n            assert isinstance(actual.item(), type(expected))\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _STANDARD_CALENDARS)\ndef test_decode_standard_calendar_multidim_time_inside_timestamp_range(calendar):\n    import cftime\n\n    units = ""days since 0001-01-01""\n    times1 = pd.date_range(""2001-04-01"", end=""2001-04-05"", freq=""D"")\n    times2 = pd.date_range(""2001-05-01"", end=""2001-05-05"", freq=""D"")\n    time1 = cftime.date2num(times1.to_pydatetime(), units, calendar=calendar)\n    time2 = cftime.date2num(times2.to_pydatetime(), units, calendar=calendar)\n    mdim_time = np.empty((len(time1), 2))\n    mdim_time[:, 0] = time1\n    mdim_time[:, 1] = time2\n\n    expected1 = times1.values\n    expected2 = times2.values\n\n    actual = coding.times.decode_cf_datetime(mdim_time, units, calendar=calendar)\n    assert actual.dtype == np.dtype(""M8[ns]"")\n\n    abs_diff1 = abs(actual[:, 0] - expected1)\n    abs_diff2 = abs(actual[:, 1] - expected2)\n    # once we no longer support versions of netCDF4 older than 1.1.5,\n    # we could do this check with near microsecond accuracy:\n    # https://github.com/Unidata/netcdf4-python/issues/355\n    assert (abs_diff1 <= np.timedelta64(1, ""s"")).all()\n    assert (abs_diff2 <= np.timedelta64(1, ""s"")).all()\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _NON_STANDARD_CALENDARS)\ndef test_decode_nonstandard_calendar_multidim_time_inside_timestamp_range(calendar):\n    import cftime\n\n    units = ""days since 0001-01-01""\n    times1 = pd.date_range(""2001-04-01"", end=""2001-04-05"", freq=""D"")\n    times2 = pd.date_range(""2001-05-01"", end=""2001-05-05"", freq=""D"")\n    time1 = cftime.date2num(times1.to_pydatetime(), units, calendar=calendar)\n    time2 = cftime.date2num(times2.to_pydatetime(), units, calendar=calendar)\n    mdim_time = np.empty((len(time1), 2))\n    mdim_time[:, 0] = time1\n    mdim_time[:, 1] = time2\n\n    if cftime.__name__ == ""cftime"":\n        expected1 = cftime.num2date(\n            time1, units, calendar, only_use_cftime_datetimes=True\n        )\n        expected2 = cftime.num2date(\n            time2, units, calendar, only_use_cftime_datetimes=True\n        )\n    else:\n        expected1 = cftime.num2date(time1, units, calendar)\n        expected2 = cftime.num2date(time2, units, calendar)\n\n    expected_dtype = np.dtype(""O"")\n\n    actual = coding.times.decode_cf_datetime(mdim_time, units, calendar=calendar)\n\n    assert actual.dtype == expected_dtype\n    abs_diff1 = abs(actual[:, 0] - expected1)\n    abs_diff2 = abs(actual[:, 1] - expected2)\n    # once we no longer support versions of netCDF4 older than 1.1.5,\n    # we could do this check with near microsecond accuracy:\n    # https://github.com/Unidata/netcdf4-python/issues/355\n    assert (abs_diff1 <= np.timedelta64(1, ""s"")).all()\n    assert (abs_diff2 <= np.timedelta64(1, ""s"")).all()\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _ALL_CALENDARS)\ndef test_decode_multidim_time_outside_timestamp_range(calendar):\n    import cftime\n    from datetime import datetime\n\n    units = ""days since 0001-01-01""\n    times1 = [datetime(1, 4, day) for day in range(1, 6)]\n    times2 = [datetime(1, 5, day) for day in range(1, 6)]\n    time1 = cftime.date2num(times1, units, calendar=calendar)\n    time2 = cftime.date2num(times2, units, calendar=calendar)\n    mdim_time = np.empty((len(time1), 2))\n    mdim_time[:, 0] = time1\n    mdim_time[:, 1] = time2\n\n    expected1 = cftime.num2date(time1, units, calendar, only_use_cftime_datetimes=True)\n    expected2 = cftime.num2date(time2, units, calendar, only_use_cftime_datetimes=True)\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", ""Unable to decode time axis"")\n        actual = coding.times.decode_cf_datetime(mdim_time, units, calendar=calendar)\n\n    assert actual.dtype == np.dtype(""O"")\n\n    abs_diff1 = abs(actual[:, 0] - expected1)\n    abs_diff2 = abs(actual[:, 1] - expected2)\n    # once we no longer support versions of netCDF4 older than 1.1.5,\n    # we could do this check with near microsecond accuracy:\n    # https://github.com/Unidata/netcdf4-python/issues/355\n    assert (abs_diff1 <= np.timedelta64(1, ""s"")).all()\n    assert (abs_diff2 <= np.timedelta64(1, ""s"")).all()\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", [""360_day"", ""all_leap"", ""366_day""])\ndef test_decode_non_standard_calendar_single_element(calendar):\n    import cftime\n\n    units = ""days since 0001-01-01""\n\n    dt = cftime.datetime(2001, 2, 29)\n\n    num_time = cftime.date2num(dt, units, calendar)\n    actual = coding.times.decode_cf_datetime(num_time, units, calendar=calendar)\n\n    expected = np.asarray(\n        cftime.num2date(num_time, units, calendar, only_use_cftime_datetimes=True)\n    )\n    assert actual.dtype == np.dtype(""O"")\n    assert expected == actual\n\n\n@requires_cftime\ndef test_decode_360_day_calendar():\n    import cftime\n\n    calendar = ""360_day""\n    # ensure leap year doesn\'t matter\n    for year in [2010, 2011, 2012, 2013, 2014]:\n        units = f""days since {year}-01-01""\n        num_times = np.arange(100)\n\n        expected = cftime.num2date(\n            num_times, units, calendar, only_use_cftime_datetimes=True\n        )\n\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(""always"")\n            actual = coding.times.decode_cf_datetime(\n                num_times, units, calendar=calendar\n            )\n            assert len(w) == 0\n\n        assert actual.dtype == np.dtype(""O"")\n        assert_array_equal(actual, expected)\n\n\n@requires_cftime\ndef test_decode_abbreviation():\n    """"""Test making sure we properly fall back to cftime on abbreviated units.""""""\n    import cftime\n\n    val = np.array([1586628000000.0])\n    units = ""msecs since 1970-01-01T00:00:00Z""\n    actual = coding.times.decode_cf_datetime(val, units)\n    expected = coding.times.cftime_to_nptime(cftime.num2date(val, units))\n    assert_array_equal(actual, expected)\n\n\n@arm_xfail\n@requires_cftime\n@pytest.mark.parametrize(\n    [""num_dates"", ""units"", ""expected_list""],\n    [\n        ([np.nan], ""days since 2000-01-01"", [""NaT""]),\n        ([np.nan, 0], ""days since 2000-01-01"", [""NaT"", ""2000-01-01T00:00:00Z""]),\n        (\n            [np.nan, 0, 1],\n            ""days since 2000-01-01"",\n            [""NaT"", ""2000-01-01T00:00:00Z"", ""2000-01-02T00:00:00Z""],\n        ),\n    ],\n)\ndef test_cf_datetime_nan(num_dates, units, expected_list):\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", ""All-NaN"")\n        actual = coding.times.decode_cf_datetime(num_dates, units)\n    # use pandas because numpy will deprecate timezone-aware conversions\n    expected = pd.to_datetime(expected_list).to_numpy(dtype=""datetime64[ns]"")\n    assert_array_equal(expected, actual)\n\n\n@requires_cftime\ndef test_decoded_cf_datetime_array_2d():\n    # regression test for GH1229\n    variable = Variable(\n        (""x"", ""y""), np.array([[0, 1], [2, 3]]), {""units"": ""days since 2000-01-01""}\n    )\n    result = coding.times.CFDatetimeCoder().decode(variable)\n    assert result.dtype == ""datetime64[ns]""\n    expected = pd.date_range(""2000-01-01"", periods=4).values.reshape(2, 2)\n    assert_array_equal(np.asarray(result), expected)\n\n\n@pytest.mark.parametrize(\n    [""dates"", ""expected""],\n    [\n        (pd.date_range(""1900-01-01"", periods=5), ""days since 1900-01-01 00:00:00""),\n        (\n            pd.date_range(""1900-01-01 12:00:00"", freq=""H"", periods=2),\n            ""hours since 1900-01-01 12:00:00"",\n        ),\n        (\n            pd.to_datetime([""1900-01-01"", ""1900-01-02"", ""NaT""]),\n            ""days since 1900-01-01 00:00:00"",\n        ),\n        (\n            pd.to_datetime([""1900-01-01"", ""1900-01-02T00:00:00.005""]),\n            ""seconds since 1900-01-01 00:00:00"",\n        ),\n        (pd.to_datetime([""NaT"", ""1900-01-01""]), ""days since 1900-01-01 00:00:00""),\n        (pd.to_datetime([""NaT""]), ""days since 1970-01-01 00:00:00""),\n    ],\n)\ndef test_infer_datetime_units(dates, expected):\n    assert expected == coding.times.infer_datetime_units(dates)\n\n\n_CFTIME_DATETIME_UNITS_TESTS = [\n    ([(1900, 1, 1), (1900, 1, 1)], ""days since 1900-01-01 00:00:00.000000""),\n    (\n        [(1900, 1, 1), (1900, 1, 2), (1900, 1, 2, 0, 0, 1)],\n        ""seconds since 1900-01-01 00:00:00.000000"",\n    ),\n    (\n        [(1900, 1, 1), (1900, 1, 8), (1900, 1, 16)],\n        ""days since 1900-01-01 00:00:00.000000"",\n    ),\n]\n\n\n@requires_cftime\n@pytest.mark.parametrize(\n    ""calendar"", _NON_STANDARD_CALENDARS + [""gregorian"", ""proleptic_gregorian""]\n)\n@pytest.mark.parametrize((""date_args"", ""expected""), _CFTIME_DATETIME_UNITS_TESTS)\ndef test_infer_cftime_datetime_units(calendar, date_args, expected):\n    date_type = _all_cftime_date_types()[calendar]\n    dates = [date_type(*args) for args in date_args]\n    assert expected == coding.times.infer_datetime_units(dates)\n\n\n@pytest.mark.parametrize(\n    [""timedeltas"", ""units"", ""numbers""],\n    [\n        (""1D"", ""days"", np.int64(1)),\n        ([""1D"", ""2D"", ""3D""], ""days"", np.array([1, 2, 3], ""int64"")),\n        (""1h"", ""hours"", np.int64(1)),\n        (""1ms"", ""milliseconds"", np.int64(1)),\n        (""1us"", ""microseconds"", np.int64(1)),\n        ([""NaT"", ""0s"", ""1s""], None, [np.nan, 0, 1]),\n        ([""30m"", ""60m""], ""hours"", [0.5, 1.0]),\n        (""NaT"", ""days"", np.nan),\n        ([""NaT"", ""NaT""], ""days"", [np.nan, np.nan]),\n    ],\n)\ndef test_cf_timedelta(timedeltas, units, numbers):\n    if timedeltas == ""NaT"":\n        timedeltas = np.timedelta64(""NaT"", ""ns"")\n    else:\n        timedeltas = to_timedelta_unboxed(timedeltas)\n    numbers = np.array(numbers)\n\n    expected = numbers\n    actual, _ = coding.times.encode_cf_timedelta(timedeltas, units)\n    assert_array_equal(expected, actual)\n    assert expected.dtype == actual.dtype\n\n    if units is not None:\n        expected = timedeltas\n        actual = coding.times.decode_cf_timedelta(numbers, units)\n        assert_array_equal(expected, actual)\n        assert expected.dtype == actual.dtype\n\n    expected = np.timedelta64(""NaT"", ""ns"")\n    actual = coding.times.decode_cf_timedelta(np.array(np.nan), ""days"")\n    assert_array_equal(expected, actual)\n\n\ndef test_cf_timedelta_2d():\n    timedeltas = [""1D"", ""2D"", ""3D""]\n    units = ""days""\n    numbers = np.atleast_2d([1, 2, 3])\n\n    timedeltas = np.atleast_2d(to_timedelta_unboxed(timedeltas))\n    expected = timedeltas\n\n    actual = coding.times.decode_cf_timedelta(numbers, units)\n    assert_array_equal(expected, actual)\n    assert expected.dtype == actual.dtype\n\n\n@pytest.mark.parametrize(\n    [""deltas"", ""expected""],\n    [\n        (pd.to_timedelta([""1 day"", ""2 days""]), ""days""),\n        (pd.to_timedelta([""1h"", ""1 day 1 hour""]), ""hours""),\n        (pd.to_timedelta([""1m"", ""2m"", np.nan]), ""minutes""),\n        (pd.to_timedelta([""1m3s"", ""1m4s""]), ""seconds""),\n    ],\n)\ndef test_infer_timedelta_units(deltas, expected):\n    assert expected == coding.times.infer_timedelta_units(deltas)\n\n\n@requires_cftime\n@pytest.mark.parametrize(\n    [""date_args"", ""expected""],\n    [\n        ((1, 2, 3, 4, 5, 6), ""0001-02-03 04:05:06.000000""),\n        ((10, 2, 3, 4, 5, 6), ""0010-02-03 04:05:06.000000""),\n        ((100, 2, 3, 4, 5, 6), ""0100-02-03 04:05:06.000000""),\n        ((1000, 2, 3, 4, 5, 6), ""1000-02-03 04:05:06.000000""),\n    ],\n)\ndef test_format_cftime_datetime(date_args, expected):\n    date_types = _all_cftime_date_types()\n    for date_type in date_types.values():\n        result = coding.times.format_cftime_datetime(date_type(*date_args))\n        assert result == expected\n\n\n@pytest.mark.parametrize(""calendar"", _ALL_CALENDARS)\ndef test_decode_cf(calendar):\n    days = [1.0, 2.0, 3.0]\n    da = DataArray(days, coords=[days], dims=[""time""], name=""test"")\n    ds = da.to_dataset()\n\n    for v in [""test"", ""time""]:\n        ds[v].attrs[""units""] = ""days since 2001-01-01""\n        ds[v].attrs[""calendar""] = calendar\n\n    if not has_cftime and calendar not in _STANDARD_CALENDARS:\n        with pytest.raises(ValueError):\n            ds = decode_cf(ds)\n    else:\n        ds = decode_cf(ds)\n\n        if calendar not in _STANDARD_CALENDARS:\n            assert ds.test.dtype == np.dtype(""O"")\n        else:\n            assert ds.test.dtype == np.dtype(""M8[ns]"")\n\n\ndef test_decode_cf_time_bounds():\n\n    da = DataArray(\n        np.arange(6, dtype=""int64"").reshape((3, 2)),\n        coords={""time"": [1, 2, 3]},\n        dims=(""time"", ""nbnd""),\n        name=""time_bnds"",\n    )\n\n    attrs = {\n        ""units"": ""days since 2001-01"",\n        ""calendar"": ""standard"",\n        ""bounds"": ""time_bnds"",\n    }\n\n    ds = da.to_dataset()\n    ds[""time""].attrs.update(attrs)\n    _update_bounds_attributes(ds.variables)\n    assert ds.variables[""time_bnds""].attrs == {\n        ""units"": ""days since 2001-01"",\n        ""calendar"": ""standard"",\n    }\n    dsc = decode_cf(ds)\n    assert dsc.time_bnds.dtype == np.dtype(""M8[ns]"")\n    dsc = decode_cf(ds, decode_times=False)\n    assert dsc.time_bnds.dtype == np.dtype(""int64"")\n\n    # Do not overwrite existing attrs\n    ds = da.to_dataset()\n    ds[""time""].attrs.update(attrs)\n    bnd_attr = {""units"": ""hours since 2001-01"", ""calendar"": ""noleap""}\n    ds[""time_bnds""].attrs.update(bnd_attr)\n    _update_bounds_attributes(ds.variables)\n    assert ds.variables[""time_bnds""].attrs == bnd_attr\n\n    # If bounds variable not available do not complain\n    ds = da.to_dataset()\n    ds[""time""].attrs.update(attrs)\n    ds[""time""].attrs[""bounds""] = ""fake_var""\n    _update_bounds_attributes(ds.variables)\n\n\n@requires_cftime\ndef test_encode_time_bounds():\n\n    time = pd.date_range(""2000-01-16"", periods=1)\n    time_bounds = pd.date_range(""2000-01-01"", periods=2, freq=""MS"")\n    ds = Dataset(dict(time=time, time_bounds=time_bounds))\n    ds.time.attrs = {""bounds"": ""time_bounds""}\n    ds.time.encoding = {""calendar"": ""noleap"", ""units"": ""days since 2000-01-01""}\n\n    expected = {}\n    # expected[\'time\'] = Variable(data=np.array([15]), dims=[\'time\'])\n    expected[""time_bounds""] = Variable(data=np.array([0, 31]), dims=[""time_bounds""])\n\n    encoded, _ = cf_encoder(ds.variables, ds.attrs)\n    assert_equal(encoded[""time_bounds""], expected[""time_bounds""])\n    assert ""calendar"" not in encoded[""time_bounds""].attrs\n    assert ""units"" not in encoded[""time_bounds""].attrs\n\n    # if time_bounds attrs are same as time attrs, it doesn\'t matter\n    ds.time_bounds.encoding = {""calendar"": ""noleap"", ""units"": ""days since 2000-01-01""}\n    encoded, _ = cf_encoder({k: ds[k] for k in ds.variables}, ds.attrs)\n    assert_equal(encoded[""time_bounds""], expected[""time_bounds""])\n    assert ""calendar"" not in encoded[""time_bounds""].attrs\n    assert ""units"" not in encoded[""time_bounds""].attrs\n\n    # for CF-noncompliant case of time_bounds attrs being different from\n    # time attrs; preserve them for faithful roundtrip\n    ds.time_bounds.encoding = {""calendar"": ""noleap"", ""units"": ""days since 1849-01-01""}\n    encoded, _ = cf_encoder({k: ds[k] for k in ds.variables}, ds.attrs)\n    with pytest.raises(AssertionError):\n        assert_equal(encoded[""time_bounds""], expected[""time_bounds""])\n    assert ""calendar"" not in encoded[""time_bounds""].attrs\n    assert encoded[""time_bounds""].attrs[""units""] == ds.time_bounds.encoding[""units""]\n\n    ds.time.encoding = {}\n    with pytest.warns(UserWarning):\n        cf_encoder(ds.variables, ds.attrs)\n\n\n@pytest.fixture(params=_ALL_CALENDARS)\ndef calendar(request):\n    return request.param\n\n\n@pytest.fixture()\ndef times(calendar):\n    import cftime\n\n    return cftime.num2date(\n        np.arange(4),\n        units=""hours since 2000-01-01"",\n        calendar=calendar,\n        only_use_cftime_datetimes=True,\n    )\n\n\n@pytest.fixture()\ndef data(times):\n    data = np.random.rand(2, 2, 4)\n    lons = np.linspace(0, 11, 2)\n    lats = np.linspace(0, 20, 2)\n    return DataArray(\n        data, coords=[lons, lats, times], dims=[""lon"", ""lat"", ""time""], name=""data""\n    )\n\n\n@pytest.fixture()\ndef times_3d(times):\n    lons = np.linspace(0, 11, 2)\n    lats = np.linspace(0, 20, 2)\n    times_arr = np.random.choice(times, size=(2, 2, 4))\n    return DataArray(\n        times_arr, coords=[lons, lats, times], dims=[""lon"", ""lat"", ""time""], name=""data""\n    )\n\n\n@requires_cftime\ndef test_contains_cftime_datetimes_1d(data):\n    assert contains_cftime_datetimes(data.time)\n\n\n@requires_cftime\n@requires_dask\ndef test_contains_cftime_datetimes_dask_1d(data):\n    assert contains_cftime_datetimes(data.time.chunk())\n\n\n@requires_cftime\ndef test_contains_cftime_datetimes_3d(times_3d):\n    assert contains_cftime_datetimes(times_3d)\n\n\n@requires_cftime\n@requires_dask\ndef test_contains_cftime_datetimes_dask_3d(times_3d):\n    assert contains_cftime_datetimes(times_3d.chunk())\n\n\n@pytest.mark.parametrize(""non_cftime_data"", [DataArray([]), DataArray([1, 2])])\ndef test_contains_cftime_datetimes_non_cftimes(non_cftime_data):\n    assert not contains_cftime_datetimes(non_cftime_data)\n\n\n@requires_dask\n@pytest.mark.parametrize(""non_cftime_data"", [DataArray([]), DataArray([1, 2])])\ndef test_contains_cftime_datetimes_non_cftimes_dask(non_cftime_data):\n    assert not contains_cftime_datetimes(non_cftime_data.chunk())\n\n\n@requires_cftime\n@pytest.mark.parametrize(""shape"", [(24,), (8, 3), (2, 4, 3)])\ndef test_encode_cf_datetime_overflow(shape):\n    # Test for fix to GH 2272\n    dates = pd.date_range(""2100"", periods=24).values.reshape(shape)\n    units = ""days since 1800-01-01""\n    calendar = ""standard""\n\n    num, _, _ = encode_cf_datetime(dates, units, calendar)\n    roundtrip = decode_cf_datetime(num, units, calendar)\n    np.testing.assert_array_equal(dates, roundtrip)\n\n\ndef test_encode_cf_datetime_pandas_min():\n    # GH 2623\n    dates = pd.date_range(""2000"", periods=3)\n    num, units, calendar = encode_cf_datetime(dates)\n    expected_num = np.array([0.0, 1.0, 2.0])\n    expected_units = ""days since 2000-01-01 00:00:00""\n    expected_calendar = ""proleptic_gregorian""\n    np.testing.assert_array_equal(num, expected_num)\n    assert units == expected_units\n    assert calendar == expected_calendar\n\n\n@requires_cftime\ndef test_time_units_with_timezone_roundtrip(calendar):\n    # Regression test for GH 2649\n    expected_units = ""days since 2000-01-01T00:00:00-05:00""\n    expected_num_dates = np.array([1, 2, 3])\n    dates = decode_cf_datetime(expected_num_dates, expected_units, calendar)\n\n    # Check that dates were decoded to UTC; here the hours should all\n    # equal 5.\n    result_hours = DataArray(dates).dt.hour\n    expected_hours = DataArray([5, 5, 5])\n    assert_equal(result_hours, expected_hours)\n\n    # Check that the encoded values are accurately roundtripped.\n    result_num_dates, result_units, result_calendar = encode_cf_datetime(\n        dates, expected_units, calendar\n    )\n\n    if calendar in _STANDARD_CALENDARS:\n        np.testing.assert_array_equal(result_num_dates, expected_num_dates)\n    else:\n        # cftime datetime arithmetic is not quite exact.\n        np.testing.assert_allclose(result_num_dates, expected_num_dates)\n\n    assert result_units == expected_units\n    assert result_calendar == calendar\n\n\n@pytest.mark.parametrize(""calendar"", _STANDARD_CALENDARS)\ndef test_use_cftime_default_standard_calendar_in_range(calendar):\n    numerical_dates = [0, 1]\n    units = ""days since 2000-01-01""\n    expected = pd.date_range(""2000"", periods=2)\n\n    with pytest.warns(None) as record:\n        result = decode_cf_datetime(numerical_dates, units, calendar)\n        np.testing.assert_array_equal(result, expected)\n        assert not record\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _STANDARD_CALENDARS)\n@pytest.mark.parametrize(""units_year"", [1500, 2500])\ndef test_use_cftime_default_standard_calendar_out_of_range(calendar, units_year):\n    from cftime import num2date\n\n    numerical_dates = [0, 1]\n    units = f""days since {units_year}-01-01""\n    expected = num2date(\n        numerical_dates, units, calendar, only_use_cftime_datetimes=True\n    )\n\n    with pytest.warns(SerializationWarning):\n        result = decode_cf_datetime(numerical_dates, units, calendar)\n        np.testing.assert_array_equal(result, expected)\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _NON_STANDARD_CALENDARS)\n@pytest.mark.parametrize(""units_year"", [1500, 2000, 2500])\ndef test_use_cftime_default_non_standard_calendar(calendar, units_year):\n    from cftime import num2date\n\n    numerical_dates = [0, 1]\n    units = f""days since {units_year}-01-01""\n    expected = num2date(\n        numerical_dates, units, calendar, only_use_cftime_datetimes=True\n    )\n\n    with pytest.warns(None) as record:\n        result = decode_cf_datetime(numerical_dates, units, calendar)\n        np.testing.assert_array_equal(result, expected)\n        assert not record\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _ALL_CALENDARS)\n@pytest.mark.parametrize(""units_year"", [1500, 2000, 2500])\ndef test_use_cftime_true(calendar, units_year):\n    from cftime import num2date\n\n    numerical_dates = [0, 1]\n    units = f""days since {units_year}-01-01""\n    expected = num2date(\n        numerical_dates, units, calendar, only_use_cftime_datetimes=True\n    )\n\n    with pytest.warns(None) as record:\n        result = decode_cf_datetime(numerical_dates, units, calendar, use_cftime=True)\n        np.testing.assert_array_equal(result, expected)\n        assert not record\n\n\n@pytest.mark.parametrize(""calendar"", _STANDARD_CALENDARS)\ndef test_use_cftime_false_standard_calendar_in_range(calendar):\n    numerical_dates = [0, 1]\n    units = ""days since 2000-01-01""\n    expected = pd.date_range(""2000"", periods=2)\n\n    with pytest.warns(None) as record:\n        result = decode_cf_datetime(numerical_dates, units, calendar, use_cftime=False)\n        np.testing.assert_array_equal(result, expected)\n        assert not record\n\n\n@pytest.mark.parametrize(""calendar"", _STANDARD_CALENDARS)\n@pytest.mark.parametrize(""units_year"", [1500, 2500])\ndef test_use_cftime_false_standard_calendar_out_of_range(calendar, units_year):\n    numerical_dates = [0, 1]\n    units = f""days since {units_year}-01-01""\n    with pytest.raises(OutOfBoundsDatetime):\n        decode_cf_datetime(numerical_dates, units, calendar, use_cftime=False)\n\n\n@pytest.mark.parametrize(""calendar"", _NON_STANDARD_CALENDARS)\n@pytest.mark.parametrize(""units_year"", [1500, 2000, 2500])\ndef test_use_cftime_false_non_standard_calendar(calendar, units_year):\n    numerical_dates = [0, 1]\n    units = f""days since {units_year}-01-01""\n    with pytest.raises(OutOfBoundsDatetime):\n        decode_cf_datetime(numerical_dates, units, calendar, use_cftime=False)\n'"
xarray/tests/test_combine.py,12,"b'from datetime import datetime\nfrom itertools import product\n\nimport numpy as np\nimport pytest\n\nfrom xarray import (\n    DataArray,\n    Dataset,\n    auto_combine,\n    combine_by_coords,\n    combine_nested,\n    concat,\n)\nfrom xarray.core import dtypes\nfrom xarray.core.combine import (\n    _check_shape_tile_ids,\n    _combine_all_along_first_dim,\n    _combine_nd,\n    _infer_concat_order_from_coords,\n    _infer_concat_order_from_positions,\n    _new_tile_id,\n)\n\nfrom . import assert_equal, assert_identical, raises_regex, requires_cftime\nfrom .test_dataset import create_test_data\n\n\ndef assert_combined_tile_ids_equal(dict1, dict2):\n    assert len(dict1) == len(dict2)\n    for k, v in dict1.items():\n        assert k in dict2.keys()\n        assert_equal(dict1[k], dict2[k])\n\n\nclass TestTileIDsFromNestedList:\n    def test_1d(self):\n        ds = create_test_data\n        input = [ds(0), ds(1)]\n\n        expected = {(0,): ds(0), (1,): ds(1)}\n        actual = _infer_concat_order_from_positions(input)\n        assert_combined_tile_ids_equal(expected, actual)\n\n    def test_2d(self):\n        ds = create_test_data\n        input = [[ds(0), ds(1)], [ds(2), ds(3)], [ds(4), ds(5)]]\n\n        expected = {\n            (0, 0): ds(0),\n            (0, 1): ds(1),\n            (1, 0): ds(2),\n            (1, 1): ds(3),\n            (2, 0): ds(4),\n            (2, 1): ds(5),\n        }\n        actual = _infer_concat_order_from_positions(input)\n        assert_combined_tile_ids_equal(expected, actual)\n\n    def test_3d(self):\n        ds = create_test_data\n        input = [\n            [[ds(0), ds(1)], [ds(2), ds(3)], [ds(4), ds(5)]],\n            [[ds(6), ds(7)], [ds(8), ds(9)], [ds(10), ds(11)]],\n        ]\n\n        expected = {\n            (0, 0, 0): ds(0),\n            (0, 0, 1): ds(1),\n            (0, 1, 0): ds(2),\n            (0, 1, 1): ds(3),\n            (0, 2, 0): ds(4),\n            (0, 2, 1): ds(5),\n            (1, 0, 0): ds(6),\n            (1, 0, 1): ds(7),\n            (1, 1, 0): ds(8),\n            (1, 1, 1): ds(9),\n            (1, 2, 0): ds(10),\n            (1, 2, 1): ds(11),\n        }\n        actual = _infer_concat_order_from_positions(input)\n        assert_combined_tile_ids_equal(expected, actual)\n\n    def test_single_dataset(self):\n        ds = create_test_data(0)\n        input = [ds]\n\n        expected = {(0,): ds}\n        actual = _infer_concat_order_from_positions(input)\n        assert_combined_tile_ids_equal(expected, actual)\n\n    def test_redundant_nesting(self):\n        ds = create_test_data\n        input = [[ds(0)], [ds(1)]]\n\n        expected = {(0, 0): ds(0), (1, 0): ds(1)}\n        actual = _infer_concat_order_from_positions(input)\n        assert_combined_tile_ids_equal(expected, actual)\n\n    def test_ignore_empty_list(self):\n        ds = create_test_data(0)\n        input = [ds, []]\n        expected = {(0,): ds}\n        actual = _infer_concat_order_from_positions(input)\n        assert_combined_tile_ids_equal(expected, actual)\n\n    def test_uneven_depth_input(self):\n        # Auto_combine won\'t work on ragged input\n        # but this is just to increase test coverage\n        ds = create_test_data\n        input = [ds(0), [ds(1), ds(2)]]\n\n        expected = {(0,): ds(0), (1, 0): ds(1), (1, 1): ds(2)}\n        actual = _infer_concat_order_from_positions(input)\n        assert_combined_tile_ids_equal(expected, actual)\n\n    def test_uneven_length_input(self):\n        # Auto_combine won\'t work on ragged input\n        # but this is just to increase test coverage\n        ds = create_test_data\n        input = [[ds(0)], [ds(1), ds(2)]]\n\n        expected = {(0, 0): ds(0), (1, 0): ds(1), (1, 1): ds(2)}\n        actual = _infer_concat_order_from_positions(input)\n        assert_combined_tile_ids_equal(expected, actual)\n\n    def test_infer_from_datasets(self):\n        ds = create_test_data\n        input = [ds(0), ds(1)]\n\n        expected = {(0,): ds(0), (1,): ds(1)}\n        actual = _infer_concat_order_from_positions(input)\n        assert_combined_tile_ids_equal(expected, actual)\n\n\nclass TestTileIDsFromCoords:\n    def test_1d(self):\n        ds0 = Dataset({""x"": [0, 1]})\n        ds1 = Dataset({""x"": [2, 3]})\n\n        expected = {(0,): ds0, (1,): ds1}\n        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0])\n        assert_combined_tile_ids_equal(expected, actual)\n        assert concat_dims == [""x""]\n\n    def test_2d(self):\n        ds0 = Dataset({""x"": [0, 1], ""y"": [10, 20, 30]})\n        ds1 = Dataset({""x"": [2, 3], ""y"": [10, 20, 30]})\n        ds2 = Dataset({""x"": [0, 1], ""y"": [40, 50, 60]})\n        ds3 = Dataset({""x"": [2, 3], ""y"": [40, 50, 60]})\n        ds4 = Dataset({""x"": [0, 1], ""y"": [70, 80, 90]})\n        ds5 = Dataset({""x"": [2, 3], ""y"": [70, 80, 90]})\n\n        expected = {\n            (0, 0): ds0,\n            (1, 0): ds1,\n            (0, 1): ds2,\n            (1, 1): ds3,\n            (0, 2): ds4,\n            (1, 2): ds5,\n        }\n        actual, concat_dims = _infer_concat_order_from_coords(\n            [ds1, ds0, ds3, ds5, ds2, ds4]\n        )\n        assert_combined_tile_ids_equal(expected, actual)\n        assert concat_dims == [""x"", ""y""]\n\n    def test_no_dimension_coords(self):\n        ds0 = Dataset({""foo"": (""x"", [0, 1])})\n        ds1 = Dataset({""foo"": (""x"", [2, 3])})\n        with raises_regex(ValueError, ""Could not find any dimension""):\n            _infer_concat_order_from_coords([ds1, ds0])\n\n    def test_coord_not_monotonic(self):\n        ds0 = Dataset({""x"": [0, 1]})\n        ds1 = Dataset({""x"": [3, 2]})\n        with raises_regex(\n            ValueError,\n            ""Coordinate variable x is neither "" ""monotonically increasing nor"",\n        ):\n            _infer_concat_order_from_coords([ds1, ds0])\n\n    def test_coord_monotonically_decreasing(self):\n        ds0 = Dataset({""x"": [3, 2]})\n        ds1 = Dataset({""x"": [1, 0]})\n\n        expected = {(0,): ds0, (1,): ds1}\n        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0])\n        assert_combined_tile_ids_equal(expected, actual)\n        assert concat_dims == [""x""]\n\n    def test_no_concatenation_needed(self):\n        ds = Dataset({""foo"": (""x"", [0, 1])})\n        expected = {(): ds}\n        actual, concat_dims = _infer_concat_order_from_coords([ds])\n        assert_combined_tile_ids_equal(expected, actual)\n        assert concat_dims == []\n\n    def test_2d_plus_bystander_dim(self):\n        ds0 = Dataset({""x"": [0, 1], ""y"": [10, 20, 30], ""t"": [0.1, 0.2]})\n        ds1 = Dataset({""x"": [2, 3], ""y"": [10, 20, 30], ""t"": [0.1, 0.2]})\n        ds2 = Dataset({""x"": [0, 1], ""y"": [40, 50, 60], ""t"": [0.1, 0.2]})\n        ds3 = Dataset({""x"": [2, 3], ""y"": [40, 50, 60], ""t"": [0.1, 0.2]})\n\n        expected = {(0, 0): ds0, (1, 0): ds1, (0, 1): ds2, (1, 1): ds3}\n        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0, ds3, ds2])\n        assert_combined_tile_ids_equal(expected, actual)\n        assert concat_dims == [""x"", ""y""]\n\n    def test_string_coords(self):\n        ds0 = Dataset({""person"": [""Alice"", ""Bob""]})\n        ds1 = Dataset({""person"": [""Caroline"", ""Daniel""]})\n\n        expected = {(0,): ds0, (1,): ds1}\n        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0])\n        assert_combined_tile_ids_equal(expected, actual)\n        assert concat_dims == [""person""]\n\n    # Decided against natural sorting of string coords GH #2616\n    def test_lexicographic_sort_string_coords(self):\n        ds0 = Dataset({""simulation"": [""run8"", ""run9""]})\n        ds1 = Dataset({""simulation"": [""run10"", ""run11""]})\n\n        expected = {(0,): ds1, (1,): ds0}\n        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0])\n        assert_combined_tile_ids_equal(expected, actual)\n        assert concat_dims == [""simulation""]\n\n    def test_datetime_coords(self):\n        ds0 = Dataset({""time"": [datetime(2000, 3, 6), datetime(2001, 3, 7)]})\n        ds1 = Dataset({""time"": [datetime(1999, 1, 1), datetime(1999, 2, 4)]})\n\n        expected = {(0,): ds1, (1,): ds0}\n        actual, concat_dims = _infer_concat_order_from_coords([ds0, ds1])\n        assert_combined_tile_ids_equal(expected, actual)\n        assert concat_dims == [""time""]\n\n\n@pytest.fixture(scope=""module"")\ndef create_combined_ids():\n    return _create_combined_ids\n\n\ndef _create_combined_ids(shape):\n    tile_ids = _create_tile_ids(shape)\n    nums = range(len(tile_ids))\n    return {tile_id: create_test_data(num) for tile_id, num in zip(tile_ids, nums)}\n\n\ndef _create_tile_ids(shape):\n    tile_ids = product(*(range(i) for i in shape))\n    return list(tile_ids)\n\n\nclass TestNewTileIDs:\n    @pytest.mark.parametrize(\n        ""old_id, new_id"",\n        [((3, 0, 1), (0, 1)), ((0, 0), (0,)), ((1,), ()), ((0,), ()), ((1, 0), (0,))],\n    )\n    def test_new_tile_id(self, old_id, new_id):\n        ds = create_test_data\n        assert _new_tile_id((old_id, ds)) == new_id\n\n    def test_get_new_tile_ids(self, create_combined_ids):\n        shape = (1, 2, 3)\n        combined_ids = create_combined_ids(shape)\n\n        expected_tile_ids = sorted(combined_ids.keys())\n        actual_tile_ids = _create_tile_ids(shape)\n        assert expected_tile_ids == actual_tile_ids\n\n\nclass TestCombineND:\n    @pytest.mark.parametrize(""concat_dim"", [""dim1"", ""new_dim""])\n    def test_concat_once(self, create_combined_ids, concat_dim):\n        shape = (2,)\n        combined_ids = create_combined_ids(shape)\n        ds = create_test_data\n        result = _combine_all_along_first_dim(\n            combined_ids,\n            dim=concat_dim,\n            data_vars=""all"",\n            coords=""different"",\n            compat=""no_conflicts"",\n        )\n\n        expected_ds = concat([ds(0), ds(1)], dim=concat_dim)\n        assert_combined_tile_ids_equal(result, {(): expected_ds})\n\n    def test_concat_only_first_dim(self, create_combined_ids):\n        shape = (2, 3)\n        combined_ids = create_combined_ids(shape)\n        result = _combine_all_along_first_dim(\n            combined_ids,\n            dim=""dim1"",\n            data_vars=""all"",\n            coords=""different"",\n            compat=""no_conflicts"",\n        )\n\n        ds = create_test_data\n        partway1 = concat([ds(0), ds(3)], dim=""dim1"")\n        partway2 = concat([ds(1), ds(4)], dim=""dim1"")\n        partway3 = concat([ds(2), ds(5)], dim=""dim1"")\n        expected_datasets = [partway1, partway2, partway3]\n        expected = {(i,): ds for i, ds in enumerate(expected_datasets)}\n\n        assert_combined_tile_ids_equal(result, expected)\n\n    @pytest.mark.parametrize(""concat_dim"", [""dim1"", ""new_dim""])\n    def test_concat_twice(self, create_combined_ids, concat_dim):\n        shape = (2, 3)\n        combined_ids = create_combined_ids(shape)\n        result = _combine_nd(combined_ids, concat_dims=[""dim1"", concat_dim])\n\n        ds = create_test_data\n        partway1 = concat([ds(0), ds(3)], dim=""dim1"")\n        partway2 = concat([ds(1), ds(4)], dim=""dim1"")\n        partway3 = concat([ds(2), ds(5)], dim=""dim1"")\n        expected = concat([partway1, partway2, partway3], dim=concat_dim)\n\n        assert_equal(result, expected)\n\n\nclass TestCheckShapeTileIDs:\n    def test_check_depths(self):\n        ds = create_test_data(0)\n        combined_tile_ids = {(0,): ds, (0, 1): ds}\n        with raises_regex(ValueError, ""sub-lists do not have consistent depths""):\n            _check_shape_tile_ids(combined_tile_ids)\n\n    def test_check_lengths(self):\n        ds = create_test_data(0)\n        combined_tile_ids = {(0, 0): ds, (0, 1): ds, (0, 2): ds, (1, 0): ds, (1, 1): ds}\n        with raises_regex(ValueError, ""sub-lists do not have consistent lengths""):\n            _check_shape_tile_ids(combined_tile_ids)\n\n\nclass TestNestedCombine:\n    def test_nested_concat(self):\n        objs = [Dataset({""x"": [0]}), Dataset({""x"": [1]})]\n        expected = Dataset({""x"": [0, 1]})\n        actual = combine_nested(objs, concat_dim=""x"")\n        assert_identical(expected, actual)\n        actual = combine_nested(objs, concat_dim=[""x""])\n        assert_identical(expected, actual)\n\n        actual = combine_nested([actual], concat_dim=None)\n        assert_identical(expected, actual)\n\n        actual = combine_nested([actual], concat_dim=""x"")\n        assert_identical(expected, actual)\n\n        objs = [Dataset({""x"": [0, 1]}), Dataset({""x"": [2]})]\n        actual = combine_nested(objs, concat_dim=""x"")\n        expected = Dataset({""x"": [0, 1, 2]})\n        assert_identical(expected, actual)\n\n        # ensure combine_nested handles non-sorted variables\n        objs = [\n            Dataset({""x"": (""a"", [0]), ""y"": (""a"", [0])}),\n            Dataset({""y"": (""a"", [1]), ""x"": (""a"", [1])}),\n        ]\n        actual = combine_nested(objs, concat_dim=""a"")\n        expected = Dataset({""x"": (""a"", [0, 1]), ""y"": (""a"", [0, 1])})\n        assert_identical(expected, actual)\n\n        objs = [Dataset({""x"": [0], ""y"": [0]}), Dataset({""x"": [1]})]\n        actual = combine_nested(objs, concat_dim=""x"")\n        expected = Dataset({""x"": [0, 1], ""y"": [0]})\n        assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""join, expected"",\n        [\n            (""outer"", Dataset({""x"": [0, 1], ""y"": [0, 1]})),\n            (""inner"", Dataset({""x"": [0, 1], ""y"": []})),\n            (""left"", Dataset({""x"": [0, 1], ""y"": [0]})),\n            (""right"", Dataset({""x"": [0, 1], ""y"": [1]})),\n        ],\n    )\n    def test_combine_nested_join(self, join, expected):\n        objs = [Dataset({""x"": [0], ""y"": [0]}), Dataset({""x"": [1], ""y"": [1]})]\n        actual = combine_nested(objs, concat_dim=""x"", join=join)\n        assert_identical(expected, actual)\n\n    def test_combine_nested_join_exact(self):\n        objs = [Dataset({""x"": [0], ""y"": [0]}), Dataset({""x"": [1], ""y"": [1]})]\n        with raises_regex(ValueError, ""indexes along dimension""):\n            combine_nested(objs, concat_dim=""x"", join=""exact"")\n\n    def test_empty_input(self):\n        assert_identical(Dataset(), combine_nested([], concat_dim=""x""))\n\n    # Fails because of concat\'s weird treatment of dimension coords, see #2975\n    @pytest.mark.xfail\n    def test_nested_concat_too_many_dims_at_once(self):\n        objs = [Dataset({""x"": [0], ""y"": [1]}), Dataset({""y"": [0], ""x"": [1]})]\n        with pytest.raises(ValueError, match=""not equal across datasets""):\n            combine_nested(objs, concat_dim=""x"", coords=""minimal"")\n\n    def test_nested_concat_along_new_dim(self):\n        objs = [\n            Dataset({""a"": (""x"", [10]), ""x"": [0]}),\n            Dataset({""a"": (""x"", [20]), ""x"": [0]}),\n        ]\n        expected = Dataset({""a"": ((""t"", ""x""), [[10], [20]]), ""x"": [0]})\n        actual = combine_nested(objs, concat_dim=""t"")\n        assert_identical(expected, actual)\n\n        # Same but with a DataArray as new dim, see GH #1988 and #2647\n        dim = DataArray([100, 150], name=""baz"", dims=""baz"")\n        expected = Dataset(\n            {""a"": ((""baz"", ""x""), [[10], [20]]), ""x"": [0], ""baz"": [100, 150]}\n        )\n        actual = combine_nested(objs, concat_dim=dim)\n        assert_identical(expected, actual)\n\n    def test_nested_merge(self):\n        data = Dataset({""x"": 0})\n        actual = combine_nested([data, data, data], concat_dim=None)\n        assert_identical(data, actual)\n\n        ds1 = Dataset({""a"": (""x"", [1, 2]), ""x"": [0, 1]})\n        ds2 = Dataset({""a"": (""x"", [2, 3]), ""x"": [1, 2]})\n        expected = Dataset({""a"": (""x"", [1, 2, 3]), ""x"": [0, 1, 2]})\n        actual = combine_nested([ds1, ds2], concat_dim=None)\n        assert_identical(expected, actual)\n        actual = combine_nested([ds1, ds2], concat_dim=[None])\n        assert_identical(expected, actual)\n\n        tmp1 = Dataset({""x"": 0})\n        tmp2 = Dataset({""x"": np.nan})\n        actual = combine_nested([tmp1, tmp2], concat_dim=None)\n        assert_identical(tmp1, actual)\n        actual = combine_nested([tmp1, tmp2], concat_dim=[None])\n        assert_identical(tmp1, actual)\n\n        # Single object, with a concat_dim explicitly provided\n        # Test the issue reported in GH #1988\n        objs = [Dataset({""x"": 0, ""y"": 1})]\n        dim = DataArray([100], name=""baz"", dims=""baz"")\n        actual = combine_nested(objs, concat_dim=[dim])\n        expected = Dataset({""x"": (""baz"", [0]), ""y"": (""baz"", [1])}, {""baz"": [100]})\n        assert_identical(expected, actual)\n\n        # Just making sure that auto_combine is doing what is\n        # expected for non-scalar values, too.\n        objs = [Dataset({""x"": (""z"", [0, 1]), ""y"": (""z"", [1, 2])})]\n        dim = DataArray([100], name=""baz"", dims=""baz"")\n        actual = combine_nested(objs, concat_dim=[dim])\n        expected = Dataset(\n            {""x"": ((""baz"", ""z""), [[0, 1]]), ""y"": ((""baz"", ""z""), [[1, 2]])},\n            {""baz"": [100]},\n        )\n        assert_identical(expected, actual)\n\n    def test_concat_multiple_dims(self):\n        objs = [\n            [Dataset({""a"": ((""x"", ""y""), [[0]])}), Dataset({""a"": ((""x"", ""y""), [[1]])})],\n            [Dataset({""a"": ((""x"", ""y""), [[2]])}), Dataset({""a"": ((""x"", ""y""), [[3]])})],\n        ]\n        actual = combine_nested(objs, concat_dim=[""x"", ""y""])\n        expected = Dataset({""a"": ((""x"", ""y""), [[0, 1], [2, 3]])})\n        assert_identical(expected, actual)\n\n    def test_concat_name_symmetry(self):\n        """"""Inspired by the discussion on GH issue #2777""""""\n\n        da1 = DataArray(name=""a"", data=[[0]], dims=[""x"", ""y""])\n        da2 = DataArray(name=""b"", data=[[1]], dims=[""x"", ""y""])\n        da3 = DataArray(name=""a"", data=[[2]], dims=[""x"", ""y""])\n        da4 = DataArray(name=""b"", data=[[3]], dims=[""x"", ""y""])\n\n        x_first = combine_nested([[da1, da2], [da3, da4]], concat_dim=[""x"", ""y""])\n        y_first = combine_nested([[da1, da3], [da2, da4]], concat_dim=[""y"", ""x""])\n\n        assert_identical(x_first, y_first)\n\n    def test_concat_one_dim_merge_another(self):\n        data = create_test_data()\n        data1 = data.copy(deep=True)\n        data2 = data.copy(deep=True)\n\n        objs = [\n            [data1.var1.isel(dim2=slice(4)), data2.var1.isel(dim2=slice(4, 9))],\n            [data1.var2.isel(dim2=slice(4)), data2.var2.isel(dim2=slice(4, 9))],\n        ]\n\n        expected = data[[""var1"", ""var2""]]\n        actual = combine_nested(objs, concat_dim=[None, ""dim2""])\n        assert expected.identical(actual)\n\n    def test_auto_combine_2d(self):\n        ds = create_test_data\n\n        partway1 = concat([ds(0), ds(3)], dim=""dim1"")\n        partway2 = concat([ds(1), ds(4)], dim=""dim1"")\n        partway3 = concat([ds(2), ds(5)], dim=""dim1"")\n        expected = concat([partway1, partway2, partway3], dim=""dim2"")\n\n        datasets = [[ds(0), ds(1), ds(2)], [ds(3), ds(4), ds(5)]]\n        result = combine_nested(datasets, concat_dim=[""dim1"", ""dim2""])\n        assert_equal(result, expected)\n\n    def test_auto_combine_2d_combine_attrs_kwarg(self):\n        ds = create_test_data\n\n        partway1 = concat([ds(0), ds(3)], dim=""dim1"")\n        partway2 = concat([ds(1), ds(4)], dim=""dim1"")\n        partway3 = concat([ds(2), ds(5)], dim=""dim1"")\n        expected = concat([partway1, partway2, partway3], dim=""dim2"")\n\n        expected_dict = {}\n        expected_dict[""drop""] = expected.copy(deep=True)\n        expected_dict[""drop""].attrs = {}\n        expected_dict[""no_conflicts""] = expected.copy(deep=True)\n        expected_dict[""no_conflicts""].attrs = {\n            ""a"": 1,\n            ""b"": 2,\n            ""c"": 3,\n            ""d"": 4,\n            ""e"": 5,\n            ""f"": 6,\n        }\n        expected_dict[""override""] = expected.copy(deep=True)\n        expected_dict[""override""].attrs = {""a"": 1}\n\n        datasets = [[ds(0), ds(1), ds(2)], [ds(3), ds(4), ds(5)]]\n\n        datasets[0][0].attrs = {""a"": 1}\n        datasets[0][1].attrs = {""a"": 1, ""b"": 2}\n        datasets[0][2].attrs = {""a"": 1, ""c"": 3}\n        datasets[1][0].attrs = {""a"": 1, ""d"": 4}\n        datasets[1][1].attrs = {""a"": 1, ""e"": 5}\n        datasets[1][2].attrs = {""a"": 1, ""f"": 6}\n\n        with raises_regex(ValueError, ""combine_attrs=\'identical\'""):\n            result = combine_nested(\n                datasets, concat_dim=[""dim1"", ""dim2""], combine_attrs=""identical""\n            )\n\n        for combine_attrs in expected_dict:\n            result = combine_nested(\n                datasets, concat_dim=[""dim1"", ""dim2""], combine_attrs=combine_attrs\n            )\n            assert_identical(result, expected_dict[combine_attrs])\n\n    def test_combine_nested_missing_data_new_dim(self):\n        # Your data includes ""time"" and ""station"" dimensions, and each year\'s\n        # data has a different set of stations.\n        datasets = [\n            Dataset({""a"": (""x"", [2, 3]), ""x"": [1, 2]}),\n            Dataset({""a"": (""x"", [1, 2]), ""x"": [0, 1]}),\n        ]\n        expected = Dataset(\n            {""a"": ((""t"", ""x""), [[np.nan, 2, 3], [1, 2, np.nan]])}, {""x"": [0, 1, 2]}\n        )\n        actual = combine_nested(datasets, concat_dim=""t"")\n        assert_identical(expected, actual)\n\n    def test_invalid_hypercube_input(self):\n        ds = create_test_data\n\n        datasets = [[ds(0), ds(1), ds(2)], [ds(3), ds(4)]]\n        with raises_regex(ValueError, ""sub-lists do not have "" ""consistent lengths""):\n            combine_nested(datasets, concat_dim=[""dim1"", ""dim2""])\n\n        datasets = [[ds(0), ds(1)], [[ds(3), ds(4)]]]\n        with raises_regex(ValueError, ""sub-lists do not have "" ""consistent depths""):\n            combine_nested(datasets, concat_dim=[""dim1"", ""dim2""])\n\n        datasets = [[ds(0), ds(1)], [ds(3), ds(4)]]\n        with raises_regex(ValueError, ""concat_dims has length""):\n            combine_nested(datasets, concat_dim=[""dim1""])\n\n    def test_merge_one_dim_concat_another(self):\n        objs = [\n            [Dataset({""foo"": (""x"", [0, 1])}), Dataset({""bar"": (""x"", [10, 20])})],\n            [Dataset({""foo"": (""x"", [2, 3])}), Dataset({""bar"": (""x"", [30, 40])})],\n        ]\n        expected = Dataset({""foo"": (""x"", [0, 1, 2, 3]), ""bar"": (""x"", [10, 20, 30, 40])})\n\n        actual = combine_nested(objs, concat_dim=[""x"", None], compat=""equals"")\n        assert_identical(expected, actual)\n\n        # Proving it works symmetrically\n        objs = [\n            [Dataset({""foo"": (""x"", [0, 1])}), Dataset({""foo"": (""x"", [2, 3])})],\n            [Dataset({""bar"": (""x"", [10, 20])}), Dataset({""bar"": (""x"", [30, 40])})],\n        ]\n        actual = combine_nested(objs, concat_dim=[None, ""x""], compat=""equals"")\n        assert_identical(expected, actual)\n\n    def test_combine_concat_over_redundant_nesting(self):\n        objs = [[Dataset({""x"": [0]}), Dataset({""x"": [1]})]]\n        actual = combine_nested(objs, concat_dim=[None, ""x""])\n        expected = Dataset({""x"": [0, 1]})\n        assert_identical(expected, actual)\n\n        objs = [[Dataset({""x"": [0]})], [Dataset({""x"": [1]})]]\n        actual = combine_nested(objs, concat_dim=[""x"", None])\n        expected = Dataset({""x"": [0, 1]})\n        assert_identical(expected, actual)\n\n        objs = [[Dataset({""x"": [0]})]]\n        actual = combine_nested(objs, concat_dim=[None, None])\n        expected = Dataset({""x"": [0]})\n        assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(""fill_value"", [dtypes.NA, 2, 2.0])\n    def test_combine_nested_fill_value(self, fill_value):\n        datasets = [\n            Dataset({""a"": (""x"", [2, 3]), ""x"": [1, 2]}),\n            Dataset({""a"": (""x"", [1, 2]), ""x"": [0, 1]}),\n        ]\n        if fill_value == dtypes.NA:\n            # if we supply the default, we expect the missing value for a\n            # float array\n            fill_value = np.nan\n        expected = Dataset(\n            {""a"": ((""t"", ""x""), [[fill_value, 2, 3], [1, 2, fill_value]])},\n            {""x"": [0, 1, 2]},\n        )\n        actual = combine_nested(datasets, concat_dim=""t"", fill_value=fill_value)\n        assert_identical(expected, actual)\n\n\nclass TestCombineAuto:\n    def test_combine_by_coords(self):\n        objs = [Dataset({""x"": [0]}), Dataset({""x"": [1]})]\n        actual = combine_by_coords(objs)\n        expected = Dataset({""x"": [0, 1]})\n        assert_identical(expected, actual)\n\n        actual = combine_by_coords([actual])\n        assert_identical(expected, actual)\n\n        objs = [Dataset({""x"": [0, 1]}), Dataset({""x"": [2]})]\n        actual = combine_by_coords(objs)\n        expected = Dataset({""x"": [0, 1, 2]})\n        assert_identical(expected, actual)\n\n        # ensure auto_combine handles non-sorted variables\n        objs = [\n            Dataset({""x"": (""a"", [0]), ""y"": (""a"", [0]), ""a"": [0]}),\n            Dataset({""x"": (""a"", [1]), ""y"": (""a"", [1]), ""a"": [1]}),\n        ]\n        actual = combine_by_coords(objs)\n        expected = Dataset({""x"": (""a"", [0, 1]), ""y"": (""a"", [0, 1]), ""a"": [0, 1]})\n        assert_identical(expected, actual)\n\n        objs = [Dataset({""x"": [0], ""y"": [0]}), Dataset({""y"": [1], ""x"": [1]})]\n        actual = combine_by_coords(objs)\n        expected = Dataset({""x"": [0, 1], ""y"": [0, 1]})\n        assert_equal(actual, expected)\n\n        objs = [Dataset({""x"": 0}), Dataset({""x"": 1})]\n        with raises_regex(ValueError, ""Could not find any dimension coordinates""):\n            combine_by_coords(objs)\n\n        objs = [Dataset({""x"": [0], ""y"": [0]}), Dataset({""x"": [0]})]\n        with raises_regex(ValueError, ""Every dimension needs a coordinate""):\n            combine_by_coords(objs)\n\n        def test_empty_input(self):\n            assert_identical(Dataset(), combine_by_coords([]))\n\n    @pytest.mark.parametrize(\n        ""join, expected"",\n        [\n            (""outer"", Dataset({""x"": [0, 1], ""y"": [0, 1]})),\n            (""inner"", Dataset({""x"": [0, 1], ""y"": []})),\n            (""left"", Dataset({""x"": [0, 1], ""y"": [0]})),\n            (""right"", Dataset({""x"": [0, 1], ""y"": [1]})),\n        ],\n    )\n    def test_combine_coords_join(self, join, expected):\n        objs = [Dataset({""x"": [0], ""y"": [0]}), Dataset({""x"": [1], ""y"": [1]})]\n        actual = combine_nested(objs, concat_dim=""x"", join=join)\n        assert_identical(expected, actual)\n\n    def test_combine_coords_join_exact(self):\n        objs = [Dataset({""x"": [0], ""y"": [0]}), Dataset({""x"": [1], ""y"": [1]})]\n        with raises_regex(ValueError, ""indexes along dimension""):\n            combine_nested(objs, concat_dim=""x"", join=""exact"")\n\n    @pytest.mark.parametrize(\n        ""combine_attrs, expected"",\n        [\n            (""drop"", Dataset({""x"": [0, 1], ""y"": [0, 1]}, attrs={})),\n            (\n                ""no_conflicts"",\n                Dataset({""x"": [0, 1], ""y"": [0, 1]}, attrs={""a"": 1, ""b"": 2}),\n            ),\n            (""override"", Dataset({""x"": [0, 1], ""y"": [0, 1]}, attrs={""a"": 1})),\n        ],\n    )\n    def test_combine_coords_combine_attrs(self, combine_attrs, expected):\n        objs = [\n            Dataset({""x"": [0], ""y"": [0]}, attrs={""a"": 1}),\n            Dataset({""x"": [1], ""y"": [1]}, attrs={""a"": 1, ""b"": 2}),\n        ]\n        actual = combine_nested(\n            objs, concat_dim=""x"", join=""outer"", combine_attrs=combine_attrs\n        )\n        assert_identical(expected, actual)\n\n        if combine_attrs == ""no_conflicts"":\n            objs[1].attrs[""a""] = 2\n            with raises_regex(ValueError, ""combine_attrs=\'no_conflicts\'""):\n                actual = combine_nested(\n                    objs, concat_dim=""x"", join=""outer"", combine_attrs=combine_attrs\n                )\n\n    def test_combine_coords_combine_attrs_identical(self):\n        objs = [\n            Dataset({""x"": [0], ""y"": [0]}, attrs={""a"": 1}),\n            Dataset({""x"": [1], ""y"": [1]}, attrs={""a"": 1}),\n        ]\n        expected = Dataset({""x"": [0, 1], ""y"": [0, 1]}, attrs={""a"": 1})\n        actual = combine_nested(\n            objs, concat_dim=""x"", join=""outer"", combine_attrs=""identical""\n        )\n        assert_identical(expected, actual)\n\n        objs[1].attrs[""b""] = 2\n\n        with raises_regex(ValueError, ""combine_attrs=\'identical\'""):\n            actual = combine_nested(\n                objs, concat_dim=""x"", join=""outer"", combine_attrs=""identical""\n            )\n\n    def test_infer_order_from_coords(self):\n        data = create_test_data()\n        objs = [data.isel(dim2=slice(4, 9)), data.isel(dim2=slice(4))]\n        actual = combine_by_coords(objs)\n        expected = data\n        assert expected.broadcast_equals(actual)\n\n    def test_combine_leaving_bystander_dimensions(self):\n        # Check non-monotonic bystander dimension coord doesn\'t raise\n        # ValueError on combine (https://github.com/pydata/xarray/issues/3150)\n        ycoord = [""a"", ""c"", ""b""]\n\n        data = np.random.rand(7, 3)\n\n        ds1 = Dataset(\n            data_vars=dict(data=([""x"", ""y""], data[:3, :])),\n            coords=dict(x=[1, 2, 3], y=ycoord),\n        )\n\n        ds2 = Dataset(\n            data_vars=dict(data=([""x"", ""y""], data[3:, :])),\n            coords=dict(x=[4, 5, 6, 7], y=ycoord),\n        )\n\n        expected = Dataset(\n            data_vars=dict(data=([""x"", ""y""], data)),\n            coords=dict(x=[1, 2, 3, 4, 5, 6, 7], y=ycoord),\n        )\n\n        actual = combine_by_coords((ds1, ds2))\n        assert_identical(expected, actual)\n\n    def test_combine_by_coords_previously_failed(self):\n        # In the above scenario, one file is missing, containing the data for\n        # one year\'s data for one variable.\n        datasets = [\n            Dataset({""a"": (""x"", [0]), ""x"": [0]}),\n            Dataset({""b"": (""x"", [0]), ""x"": [0]}),\n            Dataset({""a"": (""x"", [1]), ""x"": [1]}),\n        ]\n        expected = Dataset({""a"": (""x"", [0, 1]), ""b"": (""x"", [0, np.nan])}, {""x"": [0, 1]})\n        actual = combine_by_coords(datasets)\n        assert_identical(expected, actual)\n\n    def test_combine_by_coords_still_fails(self):\n        # concat can\'t handle new variables (yet):\n        # https://github.com/pydata/xarray/issues/508\n        datasets = [Dataset({""x"": 0}, {""y"": 0}), Dataset({""x"": 1}, {""y"": 1, ""z"": 1})]\n        with pytest.raises(ValueError):\n            combine_by_coords(datasets, ""y"")\n\n    def test_combine_by_coords_no_concat(self):\n        objs = [Dataset({""x"": 0}), Dataset({""y"": 1})]\n        actual = combine_by_coords(objs)\n        expected = Dataset({""x"": 0, ""y"": 1})\n        assert_identical(expected, actual)\n\n        objs = [Dataset({""x"": 0, ""y"": 1}), Dataset({""y"": np.nan, ""z"": 2})]\n        actual = combine_by_coords(objs)\n        expected = Dataset({""x"": 0, ""y"": 1, ""z"": 2})\n        assert_identical(expected, actual)\n\n    def test_check_for_impossible_ordering(self):\n        ds0 = Dataset({""x"": [0, 1, 5]})\n        ds1 = Dataset({""x"": [2, 3]})\n        with raises_regex(\n            ValueError, ""does not have monotonic global indexes"" "" along dimension x""\n        ):\n            combine_by_coords([ds1, ds0])\n\n    def test_combine_by_coords_incomplete_hypercube(self):\n        # test that this succeeds with default fill_value\n        x1 = Dataset({""a"": ((""y"", ""x""), [[1]])}, coords={""y"": [0], ""x"": [0]})\n        x2 = Dataset({""a"": ((""y"", ""x""), [[1]])}, coords={""y"": [1], ""x"": [0]})\n        x3 = Dataset({""a"": ((""y"", ""x""), [[1]])}, coords={""y"": [0], ""x"": [1]})\n        actual = combine_by_coords([x1, x2, x3])\n        expected = Dataset(\n            {""a"": ((""y"", ""x""), [[1, 1], [1, np.nan]])},\n            coords={""y"": [0, 1], ""x"": [0, 1]},\n        )\n        assert_identical(expected, actual)\n\n        # test that this fails if fill_value is None\n        with pytest.raises(ValueError):\n            combine_by_coords([x1, x2, x3], fill_value=None)\n\n\n@pytest.mark.filterwarnings(\n    ""ignore:In xarray version 0.15 `auto_combine` "" ""will be deprecated""\n)\n@pytest.mark.filterwarnings(""ignore:Also `open_mfdataset` will no longer"")\n@pytest.mark.filterwarnings(""ignore:The datasets supplied"")\nclass TestAutoCombineOldAPI:\n    """"""\n    Set of tests which check that old 1-dimensional auto_combine behaviour is\n    still satisfied. #2616\n    """"""\n\n    def test_auto_combine(self):\n        objs = [Dataset({""x"": [0]}), Dataset({""x"": [1]})]\n        actual = auto_combine(objs)\n        expected = Dataset({""x"": [0, 1]})\n        assert_identical(expected, actual)\n\n        actual = auto_combine([actual])\n        assert_identical(expected, actual)\n\n        objs = [Dataset({""x"": [0, 1]}), Dataset({""x"": [2]})]\n        actual = auto_combine(objs)\n        expected = Dataset({""x"": [0, 1, 2]})\n        assert_identical(expected, actual)\n\n        # ensure auto_combine handles non-sorted variables\n        objs = [\n            Dataset({""x"": (""a"", [0]), ""y"": (""a"", [0])}),\n            Dataset({""y"": (""a"", [1]), ""x"": (""a"", [1])}),\n        ]\n        actual = auto_combine(objs)\n        expected = Dataset({""x"": (""a"", [0, 1]), ""y"": (""a"", [0, 1])})\n        assert_identical(expected, actual)\n\n        objs = [Dataset({""x"": [0], ""y"": [0]}), Dataset({""y"": [1], ""x"": [1]})]\n        with raises_regex(ValueError, ""too many .* dimensions""):\n            auto_combine(objs)\n\n        objs = [Dataset({""x"": 0}), Dataset({""x"": 1})]\n        with raises_regex(ValueError, ""cannot infer dimension""):\n            auto_combine(objs)\n\n        objs = [Dataset({""x"": [0], ""y"": [0]}), Dataset({""x"": [0]})]\n        with raises_regex(ValueError, ""\'y\' is not present in all datasets""):\n            auto_combine(objs)\n\n    def test_auto_combine_previously_failed(self):\n        # In the above scenario, one file is missing, containing the data for\n        # one year\'s data for one variable.\n        datasets = [\n            Dataset({""a"": (""x"", [0]), ""x"": [0]}),\n            Dataset({""b"": (""x"", [0]), ""x"": [0]}),\n            Dataset({""a"": (""x"", [1]), ""x"": [1]}),\n        ]\n        expected = Dataset({""a"": (""x"", [0, 1]), ""b"": (""x"", [0, np.nan])}, {""x"": [0, 1]})\n        actual = auto_combine(datasets)\n        assert_identical(expected, actual)\n\n        # Your data includes ""time"" and ""station"" dimensions, and each year\'s\n        # data has a different set of stations.\n        datasets = [\n            Dataset({""a"": (""x"", [2, 3]), ""x"": [1, 2]}),\n            Dataset({""a"": (""x"", [1, 2]), ""x"": [0, 1]}),\n        ]\n        expected = Dataset(\n            {""a"": ((""t"", ""x""), [[np.nan, 2, 3], [1, 2, np.nan]])}, {""x"": [0, 1, 2]}\n        )\n        actual = auto_combine(datasets, concat_dim=""t"")\n        assert_identical(expected, actual)\n\n    def test_auto_combine_with_new_variables(self):\n        datasets = [Dataset({""x"": 0}, {""y"": 0}), Dataset({""x"": 1}, {""y"": 1, ""z"": 1})]\n        actual = auto_combine(datasets, ""y"")\n        expected = Dataset({""x"": (""y"", [0, 1])}, {""y"": [0, 1], ""z"": 1})\n        assert_identical(expected, actual)\n\n    def test_auto_combine_no_concat(self):\n        objs = [Dataset({""x"": 0}), Dataset({""y"": 1})]\n        actual = auto_combine(objs)\n        expected = Dataset({""x"": 0, ""y"": 1})\n        assert_identical(expected, actual)\n\n        objs = [Dataset({""x"": 0, ""y"": 1}), Dataset({""y"": np.nan, ""z"": 2})]\n        actual = auto_combine(objs)\n        expected = Dataset({""x"": 0, ""y"": 1, ""z"": 2})\n        assert_identical(expected, actual)\n\n        data = Dataset({""x"": 0})\n        actual = auto_combine([data, data, data], concat_dim=None)\n        assert_identical(data, actual)\n\n        # Single object, with a concat_dim explicitly provided\n        # Test the issue reported in GH #1988\n        objs = [Dataset({""x"": 0, ""y"": 1})]\n        dim = DataArray([100], name=""baz"", dims=""baz"")\n        actual = auto_combine(objs, concat_dim=dim)\n        expected = Dataset({""x"": (""baz"", [0]), ""y"": (""baz"", [1])}, {""baz"": [100]})\n        assert_identical(expected, actual)\n\n        # Just making sure that auto_combine is doing what is\n        # expected for non-scalar values, too.\n        objs = [Dataset({""x"": (""z"", [0, 1]), ""y"": (""z"", [1, 2])})]\n        dim = DataArray([100], name=""baz"", dims=""baz"")\n        actual = auto_combine(objs, concat_dim=dim)\n        expected = Dataset(\n            {""x"": ((""baz"", ""z""), [[0, 1]]), ""y"": ((""baz"", ""z""), [[1, 2]])},\n            {""baz"": [100]},\n        )\n        assert_identical(expected, actual)\n\n    def test_auto_combine_order_by_appearance_not_coords(self):\n        objs = [\n            Dataset({""foo"": (""x"", [0])}, coords={""x"": (""x"", [1])}),\n            Dataset({""foo"": (""x"", [1])}, coords={""x"": (""x"", [0])}),\n        ]\n        actual = auto_combine(objs)\n        expected = Dataset({""foo"": (""x"", [0, 1])}, coords={""x"": (""x"", [1, 0])})\n        assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(""fill_value"", [dtypes.NA, 2, 2.0])\n    def test_auto_combine_fill_value(self, fill_value):\n        datasets = [\n            Dataset({""a"": (""x"", [2, 3]), ""x"": [1, 2]}),\n            Dataset({""a"": (""x"", [1, 2]), ""x"": [0, 1]}),\n        ]\n        if fill_value == dtypes.NA:\n            # if we supply the default, we expect the missing value for a\n            # float array\n            fill_value = np.nan\n        expected = Dataset(\n            {""a"": ((""t"", ""x""), [[fill_value, 2, 3], [1, 2, fill_value]])},\n            {""x"": [0, 1, 2]},\n        )\n        actual = auto_combine(datasets, concat_dim=""t"", fill_value=fill_value)\n        assert_identical(expected, actual)\n\n\nclass TestAutoCombineDeprecation:\n    """"""\n    Set of tests to check that FutureWarnings are correctly raised until the\n    deprecation cycle is complete. #2616\n    """"""\n\n    def test_auto_combine_with_concat_dim(self):\n        objs = [Dataset({""x"": [0]}), Dataset({""x"": [1]})]\n        with pytest.warns(FutureWarning, match=""`concat_dim`""):\n            auto_combine(objs, concat_dim=""x"")\n\n    def test_auto_combine_with_merge_and_concat(self):\n        objs = [Dataset({""x"": [0]}), Dataset({""x"": [1]}), Dataset({""z"": ((), 99)})]\n        with pytest.warns(FutureWarning, match=""require both concatenation""):\n            auto_combine(objs)\n\n    def test_auto_combine_with_coords(self):\n        objs = [\n            Dataset({""foo"": (""x"", [0])}, coords={""x"": (""x"", [0])}),\n            Dataset({""foo"": (""x"", [1])}, coords={""x"": (""x"", [1])}),\n        ]\n        with pytest.warns(FutureWarning, match=""supplied have global""):\n            auto_combine(objs)\n\n    def test_auto_combine_without_coords(self):\n        objs = [Dataset({""foo"": (""x"", [0])}), Dataset({""foo"": (""x"", [1])})]\n        with pytest.warns(FutureWarning, match=""supplied do not have global""):\n            auto_combine(objs)\n\n\n@requires_cftime\ndef test_combine_by_coords_distant_cftime_dates():\n    # Regression test for https://github.com/pydata/xarray/issues/3535\n    import cftime\n\n    time_1 = [cftime.DatetimeGregorian(4500, 12, 31)]\n    time_2 = [cftime.DatetimeGregorian(4600, 12, 31)]\n    time_3 = [cftime.DatetimeGregorian(5100, 12, 31)]\n\n    da_1 = DataArray([0], dims=[""time""], coords=[time_1], name=""a"").to_dataset()\n    da_2 = DataArray([1], dims=[""time""], coords=[time_2], name=""a"").to_dataset()\n    da_3 = DataArray([2], dims=[""time""], coords=[time_3], name=""a"").to_dataset()\n\n    result = combine_by_coords([da_1, da_2, da_3])\n\n    expected_time = np.concatenate([time_1, time_2, time_3])\n    expected = DataArray(\n        [0, 1, 2], dims=[""time""], coords=[expected_time], name=""a""\n    ).to_dataset()\n    assert_identical(result, expected)\n'"
xarray/tests/test_computation.py,71,"b'import functools\nimport operator\nimport pickle\n\nimport numpy as np\nimport pandas as pd\nimport pytest\nfrom numpy.testing import assert_allclose, assert_array_equal\n\nimport xarray as xr\nfrom xarray.core.alignment import broadcast\nfrom xarray.core.computation import (\n    _UFuncSignature,\n    apply_ufunc,\n    broadcast_compat_data,\n    collect_dict_values,\n    join_dict_keys,\n    ordered_set_intersection,\n    ordered_set_union,\n    result_name,\n    unified_dim_sizes,\n)\n\nfrom . import has_dask, raises_regex, requires_dask\n\n\ndef assert_identical(a, b):\n    if hasattr(a, ""identical""):\n        msg = f""not identical:\\n{a!r}\\n{b!r}""\n        assert a.identical(b), msg\n    else:\n        assert_array_equal(a, b)\n\n\ndef test_signature_properties():\n    sig = _UFuncSignature([[""x""], [""x"", ""y""]], [[""z""]])\n    assert sig.input_core_dims == ((""x"",), (""x"", ""y""))\n    assert sig.output_core_dims == ((""z"",),)\n    assert sig.all_input_core_dims == frozenset([""x"", ""y""])\n    assert sig.all_output_core_dims == frozenset([""z""])\n    assert sig.num_inputs == 2\n    assert sig.num_outputs == 1\n    assert str(sig) == ""(x),(x,y)->(z)""\n    assert sig.to_gufunc_string() == ""(dim0),(dim0,dim1)->(dim2)""\n    # dimension names matter\n    assert _UFuncSignature([[""x""]]) != _UFuncSignature([[""y""]])\n\n\ndef test_result_name():\n    class Named:\n        def __init__(self, name=None):\n            self.name = name\n\n    assert result_name([1, 2]) is None\n    assert result_name([Named()]) is None\n    assert result_name([Named(""foo""), 2]) == ""foo""\n    assert result_name([Named(""foo""), Named(""bar"")]) is None\n    assert result_name([Named(""foo""), Named()]) is None\n\n\ndef test_ordered_set_union():\n    assert list(ordered_set_union([[1, 2]])) == [1, 2]\n    assert list(ordered_set_union([[1, 2], [2, 1]])) == [1, 2]\n    assert list(ordered_set_union([[0], [1, 2], [1, 3]])) == [0, 1, 2, 3]\n\n\ndef test_ordered_set_intersection():\n    assert list(ordered_set_intersection([[1, 2]])) == [1, 2]\n    assert list(ordered_set_intersection([[1, 2], [2, 1]])) == [1, 2]\n    assert list(ordered_set_intersection([[1, 2], [1, 3]])) == [1]\n    assert list(ordered_set_intersection([[1, 2], [2]])) == [2]\n\n\ndef test_join_dict_keys():\n    dicts = [dict.fromkeys(keys) for keys in [[""x"", ""y""], [""y"", ""z""]]]\n    assert list(join_dict_keys(dicts, ""left"")) == [""x"", ""y""]\n    assert list(join_dict_keys(dicts, ""right"")) == [""y"", ""z""]\n    assert list(join_dict_keys(dicts, ""inner"")) == [""y""]\n    assert list(join_dict_keys(dicts, ""outer"")) == [""x"", ""y"", ""z""]\n    with pytest.raises(ValueError):\n        join_dict_keys(dicts, ""exact"")\n    with pytest.raises(KeyError):\n        join_dict_keys(dicts, ""foobar"")\n\n\ndef test_collect_dict_values():\n    dicts = [{""x"": 1, ""y"": 2, ""z"": 3}, {""z"": 4}, 5]\n    expected = [[1, 0, 5], [2, 0, 5], [3, 4, 5]]\n    collected = collect_dict_values(dicts, [""x"", ""y"", ""z""], fill_value=0)\n    assert collected == expected\n\n\ndef identity(x):\n    return x\n\n\ndef test_apply_identity():\n    array = np.arange(10)\n    variable = xr.Variable(""x"", array)\n    data_array = xr.DataArray(variable, [(""x"", -array)])\n    dataset = xr.Dataset({""y"": variable}, {""x"": -array})\n\n    apply_identity = functools.partial(apply_ufunc, identity)\n\n    assert_identical(array, apply_identity(array))\n    assert_identical(variable, apply_identity(variable))\n    assert_identical(data_array, apply_identity(data_array))\n    assert_identical(data_array, apply_identity(data_array.groupby(""x"")))\n    assert_identical(dataset, apply_identity(dataset))\n    assert_identical(dataset, apply_identity(dataset.groupby(""x"")))\n\n\ndef add(a, b):\n    return apply_ufunc(operator.add, a, b)\n\n\ndef test_apply_two_inputs():\n    array = np.array([1, 2, 3])\n    variable = xr.Variable(""x"", array)\n    data_array = xr.DataArray(variable, [(""x"", -array)])\n    dataset = xr.Dataset({""y"": variable}, {""x"": -array})\n\n    zero_array = np.zeros_like(array)\n    zero_variable = xr.Variable(""x"", zero_array)\n    zero_data_array = xr.DataArray(zero_variable, [(""x"", -array)])\n    zero_dataset = xr.Dataset({""y"": zero_variable}, {""x"": -array})\n\n    assert_identical(array, add(array, zero_array))\n    assert_identical(array, add(zero_array, array))\n\n    assert_identical(variable, add(variable, zero_array))\n    assert_identical(variable, add(variable, zero_variable))\n    assert_identical(variable, add(zero_array, variable))\n    assert_identical(variable, add(zero_variable, variable))\n\n    assert_identical(data_array, add(data_array, zero_array))\n    assert_identical(data_array, add(data_array, zero_variable))\n    assert_identical(data_array, add(data_array, zero_data_array))\n    assert_identical(data_array, add(zero_array, data_array))\n    assert_identical(data_array, add(zero_variable, data_array))\n    assert_identical(data_array, add(zero_data_array, data_array))\n\n    assert_identical(dataset, add(dataset, zero_array))\n    assert_identical(dataset, add(dataset, zero_variable))\n    assert_identical(dataset, add(dataset, zero_data_array))\n    assert_identical(dataset, add(dataset, zero_dataset))\n    assert_identical(dataset, add(zero_array, dataset))\n    assert_identical(dataset, add(zero_variable, dataset))\n    assert_identical(dataset, add(zero_data_array, dataset))\n    assert_identical(dataset, add(zero_dataset, dataset))\n\n    assert_identical(data_array, add(data_array.groupby(""x""), zero_data_array))\n    assert_identical(data_array, add(zero_data_array, data_array.groupby(""x"")))\n\n    assert_identical(dataset, add(data_array.groupby(""x""), zero_dataset))\n    assert_identical(dataset, add(zero_dataset, data_array.groupby(""x"")))\n\n    assert_identical(dataset, add(dataset.groupby(""x""), zero_data_array))\n    assert_identical(dataset, add(dataset.groupby(""x""), zero_dataset))\n    assert_identical(dataset, add(zero_data_array, dataset.groupby(""x"")))\n    assert_identical(dataset, add(zero_dataset, dataset.groupby(""x"")))\n\n\ndef test_apply_1d_and_0d():\n    array = np.array([1, 2, 3])\n    variable = xr.Variable(""x"", array)\n    data_array = xr.DataArray(variable, [(""x"", -array)])\n    dataset = xr.Dataset({""y"": variable}, {""x"": -array})\n\n    zero_array = 0\n    zero_variable = xr.Variable((), zero_array)\n    zero_data_array = xr.DataArray(zero_variable)\n    zero_dataset = xr.Dataset({""y"": zero_variable})\n\n    assert_identical(array, add(array, zero_array))\n    assert_identical(array, add(zero_array, array))\n\n    assert_identical(variable, add(variable, zero_array))\n    assert_identical(variable, add(variable, zero_variable))\n    assert_identical(variable, add(zero_array, variable))\n    assert_identical(variable, add(zero_variable, variable))\n\n    assert_identical(data_array, add(data_array, zero_array))\n    assert_identical(data_array, add(data_array, zero_variable))\n    assert_identical(data_array, add(data_array, zero_data_array))\n    assert_identical(data_array, add(zero_array, data_array))\n    assert_identical(data_array, add(zero_variable, data_array))\n    assert_identical(data_array, add(zero_data_array, data_array))\n\n    assert_identical(dataset, add(dataset, zero_array))\n    assert_identical(dataset, add(dataset, zero_variable))\n    assert_identical(dataset, add(dataset, zero_data_array))\n    assert_identical(dataset, add(dataset, zero_dataset))\n    assert_identical(dataset, add(zero_array, dataset))\n    assert_identical(dataset, add(zero_variable, dataset))\n    assert_identical(dataset, add(zero_data_array, dataset))\n    assert_identical(dataset, add(zero_dataset, dataset))\n\n    assert_identical(data_array, add(data_array.groupby(""x""), zero_data_array))\n    assert_identical(data_array, add(zero_data_array, data_array.groupby(""x"")))\n\n    assert_identical(dataset, add(data_array.groupby(""x""), zero_dataset))\n    assert_identical(dataset, add(zero_dataset, data_array.groupby(""x"")))\n\n    assert_identical(dataset, add(dataset.groupby(""x""), zero_data_array))\n    assert_identical(dataset, add(dataset.groupby(""x""), zero_dataset))\n    assert_identical(dataset, add(zero_data_array, dataset.groupby(""x"")))\n    assert_identical(dataset, add(zero_dataset, dataset.groupby(""x"")))\n\n\ndef test_apply_two_outputs():\n    array = np.arange(5)\n    variable = xr.Variable(""x"", array)\n    data_array = xr.DataArray(variable, [(""x"", -array)])\n    dataset = xr.Dataset({""y"": variable}, {""x"": -array})\n\n    def twice(obj):\n        def func(x):\n            return (x, x)\n\n        return apply_ufunc(func, obj, output_core_dims=[[], []])\n\n    out0, out1 = twice(array)\n    assert_identical(out0, array)\n    assert_identical(out1, array)\n\n    out0, out1 = twice(variable)\n    assert_identical(out0, variable)\n    assert_identical(out1, variable)\n\n    out0, out1 = twice(data_array)\n    assert_identical(out0, data_array)\n    assert_identical(out1, data_array)\n\n    out0, out1 = twice(dataset)\n    assert_identical(out0, dataset)\n    assert_identical(out1, dataset)\n\n    out0, out1 = twice(data_array.groupby(""x""))\n    assert_identical(out0, data_array)\n    assert_identical(out1, data_array)\n\n    out0, out1 = twice(dataset.groupby(""x""))\n    assert_identical(out0, dataset)\n    assert_identical(out1, dataset)\n\n\ndef test_apply_input_core_dimension():\n    def first_element(obj, dim):\n        def func(x):\n            return x[..., 0]\n\n        return apply_ufunc(func, obj, input_core_dims=[[dim]])\n\n    array = np.array([[1, 2], [3, 4]])\n    variable = xr.Variable([""x"", ""y""], array)\n    data_array = xr.DataArray(variable, {""x"": [""a"", ""b""], ""y"": [-1, -2]})\n    dataset = xr.Dataset({""data"": data_array})\n\n    expected_variable_x = xr.Variable([""y""], [1, 2])\n    expected_data_array_x = xr.DataArray(expected_variable_x, {""y"": [-1, -2]})\n    expected_dataset_x = xr.Dataset({""data"": expected_data_array_x})\n\n    expected_variable_y = xr.Variable([""x""], [1, 3])\n    expected_data_array_y = xr.DataArray(expected_variable_y, {""x"": [""a"", ""b""]})\n    expected_dataset_y = xr.Dataset({""data"": expected_data_array_y})\n\n    assert_identical(expected_variable_x, first_element(variable, ""x""))\n    assert_identical(expected_variable_y, first_element(variable, ""y""))\n\n    assert_identical(expected_data_array_x, first_element(data_array, ""x""))\n    assert_identical(expected_data_array_y, first_element(data_array, ""y""))\n\n    assert_identical(expected_dataset_x, first_element(dataset, ""x""))\n    assert_identical(expected_dataset_y, first_element(dataset, ""y""))\n\n    assert_identical(expected_data_array_x, first_element(data_array.groupby(""y""), ""x""))\n    assert_identical(expected_dataset_x, first_element(dataset.groupby(""y""), ""x""))\n\n    def multiply(*args):\n        val = args[0]\n        for arg in args[1:]:\n            val = val * arg\n        return val\n\n    # regression test for GH:2341\n    with pytest.raises(ValueError):\n        apply_ufunc(\n            multiply,\n            data_array,\n            data_array[""y""].values,\n            input_core_dims=[[""y""]],\n            output_core_dims=[[""y""]],\n        )\n    expected = xr.DataArray(\n        multiply(data_array, data_array[""y""]), dims=[""x"", ""y""], coords=data_array.coords\n    )\n    actual = apply_ufunc(\n        multiply,\n        data_array,\n        data_array[""y""].values,\n        input_core_dims=[[""y""], []],\n        output_core_dims=[[""y""]],\n    )\n    assert_identical(expected, actual)\n\n\ndef test_apply_output_core_dimension():\n    def stack_negative(obj):\n        def func(x):\n            return np.stack([x, -x], axis=-1)\n\n        result = apply_ufunc(func, obj, output_core_dims=[[""sign""]])\n        if isinstance(result, (xr.Dataset, xr.DataArray)):\n            result.coords[""sign""] = [1, -1]\n        return result\n\n    array = np.array([[1, 2], [3, 4]])\n    variable = xr.Variable([""x"", ""y""], array)\n    data_array = xr.DataArray(variable, {""x"": [""a"", ""b""], ""y"": [-1, -2]})\n    dataset = xr.Dataset({""data"": data_array})\n\n    stacked_array = np.array([[[1, -1], [2, -2]], [[3, -3], [4, -4]]])\n    stacked_variable = xr.Variable([""x"", ""y"", ""sign""], stacked_array)\n    stacked_coords = {""x"": [""a"", ""b""], ""y"": [-1, -2], ""sign"": [1, -1]}\n    stacked_data_array = xr.DataArray(stacked_variable, stacked_coords)\n    stacked_dataset = xr.Dataset({""data"": stacked_data_array})\n\n    assert_identical(stacked_array, stack_negative(array))\n    assert_identical(stacked_variable, stack_negative(variable))\n    assert_identical(stacked_data_array, stack_negative(data_array))\n    assert_identical(stacked_dataset, stack_negative(dataset))\n    assert_identical(stacked_data_array, stack_negative(data_array.groupby(""x"")))\n    assert_identical(stacked_dataset, stack_negative(dataset.groupby(""x"")))\n\n    def original_and_stack_negative(obj):\n        def func(x):\n            return (x, np.stack([x, -x], axis=-1))\n\n        result = apply_ufunc(func, obj, output_core_dims=[[], [""sign""]])\n        if isinstance(result[1], (xr.Dataset, xr.DataArray)):\n            result[1].coords[""sign""] = [1, -1]\n        return result\n\n    out0, out1 = original_and_stack_negative(array)\n    assert_identical(array, out0)\n    assert_identical(stacked_array, out1)\n\n    out0, out1 = original_and_stack_negative(variable)\n    assert_identical(variable, out0)\n    assert_identical(stacked_variable, out1)\n\n    out0, out1 = original_and_stack_negative(data_array)\n    assert_identical(data_array, out0)\n    assert_identical(stacked_data_array, out1)\n\n    out0, out1 = original_and_stack_negative(dataset)\n    assert_identical(dataset, out0)\n    assert_identical(stacked_dataset, out1)\n\n    out0, out1 = original_and_stack_negative(data_array.groupby(""x""))\n    assert_identical(data_array, out0)\n    assert_identical(stacked_data_array, out1)\n\n    out0, out1 = original_and_stack_negative(dataset.groupby(""x""))\n    assert_identical(dataset, out0)\n    assert_identical(stacked_dataset, out1)\n\n\ndef test_apply_exclude():\n    def concatenate(objects, dim=""x""):\n        def func(*x):\n            return np.concatenate(x, axis=-1)\n\n        result = apply_ufunc(\n            func,\n            *objects,\n            input_core_dims=[[dim]] * len(objects),\n            output_core_dims=[[dim]],\n            exclude_dims={dim},\n        )\n        if isinstance(result, (xr.Dataset, xr.DataArray)):\n            # note: this will fail if dim is not a coordinate on any input\n            new_coord = np.concatenate([obj.coords[dim] for obj in objects])\n            result.coords[dim] = new_coord\n        return result\n\n    arrays = [np.array([1]), np.array([2, 3])]\n    variables = [xr.Variable(""x"", a) for a in arrays]\n    data_arrays = [\n        xr.DataArray(v, {""x"": c, ""y"": (""x"", range(len(c)))})\n        for v, c in zip(variables, [[""a""], [""b"", ""c""]])\n    ]\n    datasets = [xr.Dataset({""data"": data_array}) for data_array in data_arrays]\n\n    expected_array = np.array([1, 2, 3])\n    expected_variable = xr.Variable(""x"", expected_array)\n    expected_data_array = xr.DataArray(expected_variable, [(""x"", list(""abc""))])\n    expected_dataset = xr.Dataset({""data"": expected_data_array})\n\n    assert_identical(expected_array, concatenate(arrays))\n    assert_identical(expected_variable, concatenate(variables))\n    assert_identical(expected_data_array, concatenate(data_arrays))\n    assert_identical(expected_dataset, concatenate(datasets))\n\n    # must also be a core dimension\n    with pytest.raises(ValueError):\n        apply_ufunc(identity, variables[0], exclude_dims={""x""})\n\n\ndef test_apply_groupby_add():\n    array = np.arange(5)\n    variable = xr.Variable(""x"", array)\n    coords = {""x"": -array, ""y"": (""x"", [0, 0, 1, 1, 2])}\n    data_array = xr.DataArray(variable, coords, dims=""x"")\n    dataset = xr.Dataset({""z"": variable}, coords)\n\n    other_variable = xr.Variable(""y"", [0, 10])\n    other_data_array = xr.DataArray(other_variable, dims=""y"")\n    other_dataset = xr.Dataset({""z"": other_variable})\n\n    expected_variable = xr.Variable(""x"", [0, 1, 12, 13, np.nan])\n    expected_data_array = xr.DataArray(expected_variable, coords, dims=""x"")\n    expected_dataset = xr.Dataset({""z"": expected_variable}, coords)\n\n    assert_identical(\n        expected_data_array, add(data_array.groupby(""y""), other_data_array)\n    )\n    assert_identical(expected_dataset, add(data_array.groupby(""y""), other_dataset))\n    assert_identical(expected_dataset, add(dataset.groupby(""y""), other_data_array))\n    assert_identical(expected_dataset, add(dataset.groupby(""y""), other_dataset))\n\n    # cannot be performed with xarray.Variable objects that share a dimension\n    with pytest.raises(ValueError):\n        add(data_array.groupby(""y""), other_variable)\n\n    # if they are all grouped the same way\n    with pytest.raises(ValueError):\n        add(data_array.groupby(""y""), data_array[:4].groupby(""y""))\n    with pytest.raises(ValueError):\n        add(data_array.groupby(""y""), data_array[1:].groupby(""y""))\n    with pytest.raises(ValueError):\n        add(data_array.groupby(""y""), other_data_array.groupby(""y""))\n    with pytest.raises(ValueError):\n        add(data_array.groupby(""y""), data_array.groupby(""x""))\n\n\ndef test_unified_dim_sizes():\n    assert unified_dim_sizes([xr.Variable((), 0)]) == {}\n    assert unified_dim_sizes([xr.Variable(""x"", [1]), xr.Variable(""x"", [1])]) == {""x"": 1}\n    assert unified_dim_sizes([xr.Variable(""x"", [1]), xr.Variable(""y"", [1, 2])]) == {\n        ""x"": 1,\n        ""y"": 2,\n    }\n    assert unified_dim_sizes(\n        [xr.Variable((""x"", ""z""), [[1]]), xr.Variable((""y"", ""z""), [[1, 2], [3, 4]])],\n        exclude_dims={""z""},\n    ) == {""x"": 1, ""y"": 2}\n\n    # duplicate dimensions\n    with pytest.raises(ValueError):\n        unified_dim_sizes([xr.Variable((""x"", ""x""), [[1]])])\n\n    # mismatched lengths\n    with pytest.raises(ValueError):\n        unified_dim_sizes([xr.Variable(""x"", [1]), xr.Variable(""x"", [1, 2])])\n\n\ndef test_broadcast_compat_data_1d():\n    data = np.arange(5)\n    var = xr.Variable(""x"", data)\n\n    assert_identical(data, broadcast_compat_data(var, (""x"",), ()))\n    assert_identical(data, broadcast_compat_data(var, (), (""x"",)))\n    assert_identical(data[:], broadcast_compat_data(var, (""w"",), (""x"",)))\n    assert_identical(data[:, None], broadcast_compat_data(var, (""w"", ""x"", ""y""), ()))\n\n    with pytest.raises(ValueError):\n        broadcast_compat_data(var, (""x"",), (""w"",))\n\n    with pytest.raises(ValueError):\n        broadcast_compat_data(var, (), ())\n\n\ndef test_broadcast_compat_data_2d():\n    data = np.arange(12).reshape(3, 4)\n    var = xr.Variable([""x"", ""y""], data)\n\n    assert_identical(data, broadcast_compat_data(var, (""x"", ""y""), ()))\n    assert_identical(data, broadcast_compat_data(var, (""x"",), (""y"",)))\n    assert_identical(data, broadcast_compat_data(var, (), (""x"", ""y"")))\n    assert_identical(data.T, broadcast_compat_data(var, (""y"", ""x""), ()))\n    assert_identical(data.T, broadcast_compat_data(var, (""y"",), (""x"",)))\n    assert_identical(data, broadcast_compat_data(var, (""w"", ""x""), (""y"",)))\n    assert_identical(data, broadcast_compat_data(var, (""w"",), (""x"", ""y"")))\n    assert_identical(data.T, broadcast_compat_data(var, (""w"",), (""y"", ""x"")))\n    assert_identical(\n        data[:, :, None], broadcast_compat_data(var, (""w"", ""x"", ""y"", ""z""), ())\n    )\n    assert_identical(\n        data[None, :, :].T, broadcast_compat_data(var, (""w"", ""y"", ""x"", ""z""), ())\n    )\n\n\ndef test_keep_attrs():\n    def add(a, b, keep_attrs):\n        if keep_attrs:\n            return apply_ufunc(operator.add, a, b, keep_attrs=keep_attrs)\n        else:\n            return apply_ufunc(operator.add, a, b)\n\n    a = xr.DataArray([0, 1], [(""x"", [0, 1])])\n    a.attrs[""attr""] = ""da""\n    a[""x""].attrs[""attr""] = ""da_coord""\n    b = xr.DataArray([1, 2], [(""x"", [0, 1])])\n\n    actual = add(a, b, keep_attrs=False)\n    assert not actual.attrs\n    actual = add(a, b, keep_attrs=True)\n    assert_identical(actual.attrs, a.attrs)\n    assert_identical(actual[""x""].attrs, a[""x""].attrs)\n\n    actual = add(a.variable, b.variable, keep_attrs=False)\n    assert not actual.attrs\n    actual = add(a.variable, b.variable, keep_attrs=True)\n    assert_identical(actual.attrs, a.attrs)\n\n    a = xr.Dataset({""x"": [0, 1]})\n    a.attrs[""attr""] = ""ds""\n    a.x.attrs[""attr""] = ""da""\n    b = xr.Dataset({""x"": [0, 1]})\n\n    actual = add(a, b, keep_attrs=False)\n    assert not actual.attrs\n    actual = add(a, b, keep_attrs=True)\n    assert_identical(actual.attrs, a.attrs)\n    assert_identical(actual.x.attrs, a.x.attrs)\n\n\ndef test_dataset_join():\n    ds0 = xr.Dataset({""a"": (""x"", [1, 2]), ""x"": [0, 1]})\n    ds1 = xr.Dataset({""a"": (""x"", [99, 3]), ""x"": [1, 2]})\n\n    # by default, cannot have different labels\n    with raises_regex(ValueError, ""indexes .* are not equal""):\n        apply_ufunc(operator.add, ds0, ds1)\n    with raises_regex(TypeError, ""must supply""):\n        apply_ufunc(operator.add, ds0, ds1, dataset_join=""outer"")\n\n    def add(a, b, join, dataset_join):\n        return apply_ufunc(\n            operator.add,\n            a,\n            b,\n            join=join,\n            dataset_join=dataset_join,\n            dataset_fill_value=np.nan,\n        )\n\n    actual = add(ds0, ds1, ""outer"", ""inner"")\n    expected = xr.Dataset({""a"": (""x"", [np.nan, 101, np.nan]), ""x"": [0, 1, 2]})\n    assert_identical(actual, expected)\n\n    actual = add(ds0, ds1, ""outer"", ""outer"")\n    assert_identical(actual, expected)\n\n    with raises_regex(ValueError, ""data variable names""):\n        apply_ufunc(operator.add, ds0, xr.Dataset({""b"": 1}))\n\n    ds2 = xr.Dataset({""b"": (""x"", [99, 3]), ""x"": [1, 2]})\n    actual = add(ds0, ds2, ""outer"", ""inner"")\n    expected = xr.Dataset({""x"": [0, 1, 2]})\n    assert_identical(actual, expected)\n\n    # we used np.nan as the fill_value in add() above\n    actual = add(ds0, ds2, ""outer"", ""outer"")\n    expected = xr.Dataset(\n        {\n            ""a"": (""x"", [np.nan, np.nan, np.nan]),\n            ""b"": (""x"", [np.nan, np.nan, np.nan]),\n            ""x"": [0, 1, 2],\n        }\n    )\n    assert_identical(actual, expected)\n\n\n@requires_dask\ndef test_apply_dask():\n    import dask.array as da\n\n    array = da.ones((2,), chunks=2)\n    variable = xr.Variable(""x"", array)\n    coords = xr.DataArray(variable).coords.variables\n    data_array = xr.DataArray(variable, dims=[""x""], coords=coords)\n    dataset = xr.Dataset({""y"": variable})\n\n    # encountered dask array, but did not set dask=\'allowed\'\n    with pytest.raises(ValueError):\n        apply_ufunc(identity, array)\n    with pytest.raises(ValueError):\n        apply_ufunc(identity, variable)\n    with pytest.raises(ValueError):\n        apply_ufunc(identity, data_array)\n    with pytest.raises(ValueError):\n        apply_ufunc(identity, dataset)\n\n    # unknown setting for dask array handling\n    with pytest.raises(ValueError):\n        apply_ufunc(identity, array, dask=""unknown"")\n\n    def dask_safe_identity(x):\n        return apply_ufunc(identity, x, dask=""allowed"")\n\n    assert array is dask_safe_identity(array)\n\n    actual = dask_safe_identity(variable)\n    assert isinstance(actual.data, da.Array)\n    assert_identical(variable, actual)\n\n    actual = dask_safe_identity(data_array)\n    assert isinstance(actual.data, da.Array)\n    assert_identical(data_array, actual)\n\n    actual = dask_safe_identity(dataset)\n    assert isinstance(actual[""y""].data, da.Array)\n    assert_identical(dataset, actual)\n\n\n@requires_dask\ndef test_apply_dask_parallelized_one_arg():\n    import dask.array as da\n\n    array = da.ones((2, 2), chunks=(1, 1))\n    data_array = xr.DataArray(array, dims=(""x"", ""y""))\n\n    def parallel_identity(x):\n        return apply_ufunc(identity, x, dask=""parallelized"", output_dtypes=[x.dtype])\n\n    actual = parallel_identity(data_array)\n    assert isinstance(actual.data, da.Array)\n    assert actual.data.chunks == array.chunks\n    assert_identical(data_array, actual)\n\n    computed = data_array.compute()\n    actual = parallel_identity(computed)\n    assert_identical(computed, actual)\n\n\n@requires_dask\ndef test_apply_dask_parallelized_two_args():\n    import dask.array as da\n\n    array = da.ones((2, 2), chunks=(1, 1), dtype=np.int64)\n    data_array = xr.DataArray(array, dims=(""x"", ""y""))\n    data_array.name = None\n\n    def parallel_add(x, y):\n        return apply_ufunc(\n            operator.add, x, y, dask=""parallelized"", output_dtypes=[np.int64]\n        )\n\n    def check(x, y):\n        actual = parallel_add(x, y)\n        assert isinstance(actual.data, da.Array)\n        assert actual.data.chunks == array.chunks\n        assert_identical(data_array, actual)\n\n    check(data_array, 0),\n    check(0, data_array)\n    check(data_array, xr.DataArray(0))\n    check(data_array, 0 * data_array)\n    check(data_array, 0 * data_array[0])\n    check(data_array[:, 0], 0 * data_array[0])\n    check(data_array, 0 * data_array.compute())\n\n\n@requires_dask\ndef test_apply_dask_parallelized_errors():\n    import dask.array as da\n\n    array = da.ones((2, 2), chunks=(1, 1))\n    data_array = xr.DataArray(array, dims=(""x"", ""y""))\n\n    with pytest.raises(NotImplementedError):\n        apply_ufunc(\n            identity, data_array, output_core_dims=[[""z""], [""z""]], dask=""parallelized""\n        )\n    with raises_regex(ValueError, ""dtypes""):\n        apply_ufunc(identity, data_array, dask=""parallelized"")\n    with raises_regex(TypeError, ""list""):\n        apply_ufunc(identity, data_array, dask=""parallelized"", output_dtypes=float)\n    with raises_regex(ValueError, ""must have the same length""):\n        apply_ufunc(\n            identity, data_array, dask=""parallelized"", output_dtypes=[float, float]\n        )\n    with raises_regex(ValueError, ""output_sizes""):\n        apply_ufunc(\n            identity,\n            data_array,\n            output_core_dims=[[""z""]],\n            output_dtypes=[float],\n            dask=""parallelized"",\n        )\n    with raises_regex(ValueError, ""at least one input is an xarray object""):\n        apply_ufunc(identity, array, dask=""parallelized"")\n\n    with raises_regex(ValueError, ""consists of multiple chunks""):\n        apply_ufunc(\n            identity,\n            data_array,\n            dask=""parallelized"",\n            output_dtypes=[float],\n            input_core_dims=[(""y"",)],\n            output_core_dims=[(""y"",)],\n        )\n\n\n# it\'s currently impossible to silence these warnings from inside dask.array:\n# https://github.com/dask/dask/issues/3245\n@requires_dask\n@pytest.mark.filterwarnings(""ignore:Mean of empty slice"")\ndef test_apply_dask_multiple_inputs():\n    import dask.array as da\n\n    def covariance(x, y):\n        return (\n            (x - x.mean(axis=-1, keepdims=True)) * (y - y.mean(axis=-1, keepdims=True))\n        ).mean(axis=-1)\n\n    rs = np.random.RandomState(42)\n    array1 = da.from_array(rs.randn(4, 4), chunks=(2, 4))\n    array2 = da.from_array(rs.randn(4, 4), chunks=(2, 4))\n    data_array_1 = xr.DataArray(array1, dims=(""x"", ""z""))\n    data_array_2 = xr.DataArray(array2, dims=(""y"", ""z""))\n\n    expected = apply_ufunc(\n        covariance,\n        data_array_1.compute(),\n        data_array_2.compute(),\n        input_core_dims=[[""z""], [""z""]],\n    )\n    allowed = apply_ufunc(\n        covariance,\n        data_array_1,\n        data_array_2,\n        input_core_dims=[[""z""], [""z""]],\n        dask=""allowed"",\n    )\n    assert isinstance(allowed.data, da.Array)\n    xr.testing.assert_allclose(expected, allowed.compute())\n\n    parallelized = apply_ufunc(\n        covariance,\n        data_array_1,\n        data_array_2,\n        input_core_dims=[[""z""], [""z""]],\n        dask=""parallelized"",\n        output_dtypes=[float],\n    )\n    assert isinstance(parallelized.data, da.Array)\n    xr.testing.assert_allclose(expected, parallelized.compute())\n\n\n@requires_dask\ndef test_apply_dask_new_output_dimension():\n    import dask.array as da\n\n    array = da.ones((2, 2), chunks=(1, 1))\n    data_array = xr.DataArray(array, dims=(""x"", ""y""))\n\n    def stack_negative(obj):\n        def func(x):\n            return np.stack([x, -x], axis=-1)\n\n        return apply_ufunc(\n            func,\n            obj,\n            output_core_dims=[[""sign""]],\n            dask=""parallelized"",\n            output_dtypes=[obj.dtype],\n            output_sizes={""sign"": 2},\n        )\n\n    expected = stack_negative(data_array.compute())\n\n    actual = stack_negative(data_array)\n    assert actual.dims == (""x"", ""y"", ""sign"")\n    assert actual.shape == (2, 2, 2)\n    assert isinstance(actual.data, da.Array)\n    assert_identical(expected, actual)\n\n\ndef pandas_median(x):\n    return pd.Series(x).median()\n\n\ndef test_vectorize():\n    data_array = xr.DataArray([[0, 1, 2], [1, 2, 3]], dims=(""x"", ""y""))\n    expected = xr.DataArray([1, 2], dims=[""x""])\n    actual = apply_ufunc(\n        pandas_median, data_array, input_core_dims=[[""y""]], vectorize=True\n    )\n    assert_identical(expected, actual)\n\n\n@requires_dask\ndef test_vectorize_dask():\n    data_array = xr.DataArray([[0, 1, 2], [1, 2, 3]], dims=(""x"", ""y""))\n    expected = xr.DataArray([1, 2], dims=[""x""])\n    actual = apply_ufunc(\n        pandas_median,\n        data_array.chunk({""x"": 1}),\n        input_core_dims=[[""y""]],\n        vectorize=True,\n        dask=""parallelized"",\n        output_dtypes=[float],\n    )\n    assert_identical(expected, actual)\n\n\nwith raises_regex(TypeError, ""Only xr.DataArray is supported""):\n    xr.corr(xr.Dataset(), xr.Dataset())\n\n\ndef arrays_w_tuples():\n    da = xr.DataArray(\n        np.random.random((3, 21, 4)),\n        coords={""time"": pd.date_range(""2000-01-01"", freq=""1D"", periods=21)},\n        dims=(""a"", ""time"", ""x""),\n    )\n\n    arrays = [\n        da.isel(time=range(0, 18)),\n        da.isel(time=range(2, 20)).rolling(time=3, center=True).mean(),\n        xr.DataArray([[1, 2], [1, np.nan]], dims=[""x"", ""time""]),\n        xr.DataArray([[1, 2], [np.nan, np.nan]], dims=[""x"", ""time""]),\n    ]\n\n    array_tuples = [\n        (arrays[0], arrays[0]),\n        (arrays[0], arrays[1]),\n        (arrays[1], arrays[1]),\n        (arrays[2], arrays[2]),\n        (arrays[2], arrays[3]),\n        (arrays[3], arrays[3]),\n    ]\n\n    return arrays, array_tuples\n\n\n@pytest.mark.parametrize(""ddof"", [0, 1])\n@pytest.mark.parametrize(\n    ""da_a, da_b"",\n    [arrays_w_tuples()[1][0], arrays_w_tuples()[1][1], arrays_w_tuples()[1][2]],\n)\n@pytest.mark.parametrize(""dim"", [None, ""time""])\ndef test_cov(da_a, da_b, dim, ddof):\n    if dim is not None:\n\n        def np_cov_ind(ts1, ts2, a, x):\n            # Ensure the ts are aligned and missing values ignored\n            ts1, ts2 = broadcast(ts1, ts2)\n            valid_values = ts1.notnull() & ts2.notnull()\n\n            ts1 = ts1.where(valid_values)\n            ts2 = ts2.where(valid_values)\n\n            return np.cov(\n                ts1.sel(a=a, x=x).data.flatten(),\n                ts2.sel(a=a, x=x).data.flatten(),\n                ddof=ddof,\n            )[0, 1]\n\n        expected = np.zeros((3, 4))\n        for a in [0, 1, 2]:\n            for x in [0, 1, 2, 3]:\n                expected[a, x] = np_cov_ind(da_a, da_b, a=a, x=x)\n        actual = xr.cov(da_a, da_b, dim=dim, ddof=ddof)\n        assert_allclose(actual, expected)\n\n    else:\n\n        def np_cov(ts1, ts2):\n            # Ensure the ts are aligned and missing values ignored\n            ts1, ts2 = broadcast(ts1, ts2)\n            valid_values = ts1.notnull() & ts2.notnull()\n\n            ts1 = ts1.where(valid_values)\n            ts2 = ts2.where(valid_values)\n\n            return np.cov(ts1.data.flatten(), ts2.data.flatten(), ddof=ddof)[0, 1]\n\n        expected = np_cov(da_a, da_b)\n        actual = xr.cov(da_a, da_b, dim=dim, ddof=ddof)\n        assert_allclose(actual, expected)\n\n\n@pytest.mark.parametrize(\n    ""da_a, da_b"",\n    [arrays_w_tuples()[1][0], arrays_w_tuples()[1][1], arrays_w_tuples()[1][2]],\n)\n@pytest.mark.parametrize(""dim"", [None, ""time""])\ndef test_corr(da_a, da_b, dim):\n    if dim is not None:\n\n        def np_corr_ind(ts1, ts2, a, x):\n            # Ensure the ts are aligned and missing values ignored\n            ts1, ts2 = broadcast(ts1, ts2)\n            valid_values = ts1.notnull() & ts2.notnull()\n\n            ts1 = ts1.where(valid_values)\n            ts2 = ts2.where(valid_values)\n\n            return np.corrcoef(\n                ts1.sel(a=a, x=x).data.flatten(), ts2.sel(a=a, x=x).data.flatten()\n            )[0, 1]\n\n        expected = np.zeros((3, 4))\n        for a in [0, 1, 2]:\n            for x in [0, 1, 2, 3]:\n                expected[a, x] = np_corr_ind(da_a, da_b, a=a, x=x)\n        actual = xr.corr(da_a, da_b, dim)\n        assert_allclose(actual, expected)\n\n    else:\n\n        def np_corr(ts1, ts2):\n            # Ensure the ts are aligned and missing values ignored\n            ts1, ts2 = broadcast(ts1, ts2)\n            valid_values = ts1.notnull() & ts2.notnull()\n\n            ts1 = ts1.where(valid_values)\n            ts2 = ts2.where(valid_values)\n\n            return np.corrcoef(ts1.data.flatten(), ts2.data.flatten())[0, 1]\n\n        expected = np_corr(da_a, da_b)\n        actual = xr.corr(da_a, da_b, dim)\n        assert_allclose(actual, expected)\n\n\n@pytest.mark.parametrize(\n    ""da_a, da_b"", arrays_w_tuples()[1],\n)\n@pytest.mark.parametrize(""dim"", [None, ""time"", ""x""])\ndef test_covcorr_consistency(da_a, da_b, dim):\n    # Testing that xr.corr and xr.cov are consistent with each other\n    # 1. Broadcast the two arrays\n    da_a, da_b = broadcast(da_a, da_b)\n    # 2. Ignore the nans\n    valid_values = da_a.notnull() & da_b.notnull()\n    da_a = da_a.where(valid_values)\n    da_b = da_b.where(valid_values)\n\n    expected = xr.cov(da_a, da_b, dim=dim, ddof=0) / (\n        da_a.std(dim=dim) * da_b.std(dim=dim)\n    )\n    actual = xr.corr(da_a, da_b, dim=dim)\n    assert_allclose(actual, expected)\n\n\n@pytest.mark.parametrize(\n    ""da_a"", arrays_w_tuples()[0],\n)\n@pytest.mark.parametrize(""dim"", [None, ""time"", ""x""])\ndef test_autocov(da_a, dim):\n    # Testing that the autocovariance*(N-1) is ~=~ to the variance matrix\n    # 1. Ignore the nans\n    valid_values = da_a.notnull()\n    da_a = da_a.where(valid_values)\n    expected = ((da_a - da_a.mean(dim=dim)) ** 2).sum(dim=dim, skipna=False)\n    actual = xr.cov(da_a, da_a, dim=dim) * (valid_values.sum(dim) - 1)\n    assert_allclose(actual, expected)\n\n\n@requires_dask\ndef test_vectorize_dask_new_output_dims():\n    # regression test for GH3574\n    data_array = xr.DataArray([[0, 1, 2], [1, 2, 3]], dims=(""x"", ""y""))\n    func = lambda x: x[np.newaxis, ...]\n    expected = data_array.expand_dims(""z"")\n    actual = apply_ufunc(\n        func,\n        data_array.chunk({""x"": 1}),\n        output_core_dims=[[""z""]],\n        vectorize=True,\n        dask=""parallelized"",\n        output_dtypes=[float],\n        output_sizes={""z"": 1},\n    ).transpose(*expected.dims)\n    assert_identical(expected, actual)\n\n\ndef test_output_wrong_number():\n    variable = xr.Variable(""x"", np.arange(10))\n\n    def identity(x):\n        return x\n\n    def tuple3x(x):\n        return (x, x, x)\n\n    with raises_regex(ValueError, ""number of outputs""):\n        apply_ufunc(identity, variable, output_core_dims=[(), ()])\n\n    with raises_regex(ValueError, ""number of outputs""):\n        apply_ufunc(tuple3x, variable, output_core_dims=[(), ()])\n\n\ndef test_output_wrong_dims():\n    variable = xr.Variable(""x"", np.arange(10))\n\n    def add_dim(x):\n        return x[..., np.newaxis]\n\n    def remove_dim(x):\n        return x[..., 0]\n\n    with raises_regex(ValueError, ""unexpected number of dimensions""):\n        apply_ufunc(add_dim, variable, output_core_dims=[(""y"", ""z"")])\n\n    with raises_regex(ValueError, ""unexpected number of dimensions""):\n        apply_ufunc(add_dim, variable)\n\n    with raises_regex(ValueError, ""unexpected number of dimensions""):\n        apply_ufunc(remove_dim, variable)\n\n\ndef test_output_wrong_dim_size():\n    array = np.arange(10)\n    variable = xr.Variable(""x"", array)\n    data_array = xr.DataArray(variable, [(""x"", -array)])\n    dataset = xr.Dataset({""y"": variable}, {""x"": -array})\n\n    def truncate(array):\n        return array[:5]\n\n    def apply_truncate_broadcast_invalid(obj):\n        return apply_ufunc(truncate, obj)\n\n    with raises_regex(ValueError, ""size of dimension""):\n        apply_truncate_broadcast_invalid(variable)\n    with raises_regex(ValueError, ""size of dimension""):\n        apply_truncate_broadcast_invalid(data_array)\n    with raises_regex(ValueError, ""size of dimension""):\n        apply_truncate_broadcast_invalid(dataset)\n\n    def apply_truncate_x_x_invalid(obj):\n        return apply_ufunc(\n            truncate, obj, input_core_dims=[[""x""]], output_core_dims=[[""x""]]\n        )\n\n    with raises_regex(ValueError, ""size of dimension""):\n        apply_truncate_x_x_invalid(variable)\n    with raises_regex(ValueError, ""size of dimension""):\n        apply_truncate_x_x_invalid(data_array)\n    with raises_regex(ValueError, ""size of dimension""):\n        apply_truncate_x_x_invalid(dataset)\n\n    def apply_truncate_x_z(obj):\n        return apply_ufunc(\n            truncate, obj, input_core_dims=[[""x""]], output_core_dims=[[""z""]]\n        )\n\n    assert_identical(xr.Variable(""z"", array[:5]), apply_truncate_x_z(variable))\n    assert_identical(\n        xr.DataArray(array[:5], dims=[""z""]), apply_truncate_x_z(data_array)\n    )\n    assert_identical(xr.Dataset({""y"": (""z"", array[:5])}), apply_truncate_x_z(dataset))\n\n    def apply_truncate_x_x_valid(obj):\n        return apply_ufunc(\n            truncate,\n            obj,\n            input_core_dims=[[""x""]],\n            output_core_dims=[[""x""]],\n            exclude_dims={""x""},\n        )\n\n    assert_identical(xr.Variable(""x"", array[:5]), apply_truncate_x_x_valid(variable))\n    assert_identical(\n        xr.DataArray(array[:5], dims=[""x""]), apply_truncate_x_x_valid(data_array)\n    )\n    assert_identical(\n        xr.Dataset({""y"": (""x"", array[:5])}), apply_truncate_x_x_valid(dataset)\n    )\n\n\n@pytest.mark.parametrize(""use_dask"", [True, False])\ndef test_dot(use_dask):\n    if use_dask:\n        if not has_dask:\n            pytest.skip(""test for dask."")\n\n    a = np.arange(30 * 4).reshape(30, 4)\n    b = np.arange(30 * 4 * 5).reshape(30, 4, 5)\n    c = np.arange(5 * 60).reshape(5, 60)\n    da_a = xr.DataArray(a, dims=[""a"", ""b""], coords={""a"": np.linspace(0, 1, 30)})\n    da_b = xr.DataArray(b, dims=[""a"", ""b"", ""c""], coords={""a"": np.linspace(0, 1, 30)})\n    da_c = xr.DataArray(c, dims=[""c"", ""e""])\n    if use_dask:\n        da_a = da_a.chunk({""a"": 3})\n        da_b = da_b.chunk({""a"": 3})\n        da_c = da_c.chunk({""c"": 3})\n\n    actual = xr.dot(da_a, da_b, dims=[""a"", ""b""])\n    assert actual.dims == (""c"",)\n    assert (actual.data == np.einsum(""ij,ijk->k"", a, b)).all()\n    assert isinstance(actual.variable.data, type(da_a.variable.data))\n\n    actual = xr.dot(da_a, da_b)\n    assert actual.dims == (""c"",)\n    assert (actual.data == np.einsum(""ij,ijk->k"", a, b)).all()\n    assert isinstance(actual.variable.data, type(da_a.variable.data))\n\n    # for only a single array is passed without dims argument, just return\n    # as is\n    actual = xr.dot(da_a)\n    assert da_a.identical(actual)\n\n    # test for variable\n    actual = xr.dot(da_a.variable, da_b.variable)\n    assert actual.dims == (""c"",)\n    assert (actual.data == np.einsum(""ij,ijk->k"", a, b)).all()\n    assert isinstance(actual.data, type(da_a.variable.data))\n\n    if use_dask:\n        da_a = da_a.chunk({""a"": 3})\n        da_b = da_b.chunk({""a"": 3})\n        actual = xr.dot(da_a, da_b, dims=[""b""])\n        assert actual.dims == (""a"", ""c"")\n        assert (actual.data == np.einsum(""ij,ijk->ik"", a, b)).all()\n        assert isinstance(actual.variable.data, type(da_a.variable.data))\n\n    actual = xr.dot(da_a, da_b, dims=[""b""])\n    assert actual.dims == (""a"", ""c"")\n    assert (actual.data == np.einsum(""ij,ijk->ik"", a, b)).all()\n\n    actual = xr.dot(da_a, da_b, dims=""b"")\n    assert actual.dims == (""a"", ""c"")\n    assert (actual.data == np.einsum(""ij,ijk->ik"", a, b)).all()\n\n    actual = xr.dot(da_a, da_b, dims=""a"")\n    assert actual.dims == (""b"", ""c"")\n    assert (actual.data == np.einsum(""ij,ijk->jk"", a, b)).all()\n\n    actual = xr.dot(da_a, da_b, dims=""c"")\n    assert actual.dims == (""a"", ""b"")\n    assert (actual.data == np.einsum(""ij,ijk->ij"", a, b)).all()\n\n    actual = xr.dot(da_a, da_b, da_c, dims=[""a"", ""b""])\n    assert actual.dims == (""c"", ""e"")\n    assert (actual.data == np.einsum(""ij,ijk,kl->kl "", a, b, c)).all()\n\n    # should work with tuple\n    actual = xr.dot(da_a, da_b, dims=(""c"",))\n    assert actual.dims == (""a"", ""b"")\n    assert (actual.data == np.einsum(""ij,ijk->ij"", a, b)).all()\n\n    # default dims\n    actual = xr.dot(da_a, da_b, da_c)\n    assert actual.dims == (""e"",)\n    assert (actual.data == np.einsum(""ij,ijk,kl->l "", a, b, c)).all()\n\n    # 1 array summation\n    actual = xr.dot(da_a, dims=""a"")\n    assert actual.dims == (""b"",)\n    assert (actual.data == np.einsum(""ij->j "", a)).all()\n\n    # empty dim\n    actual = xr.dot(da_a.sel(a=[]), da_a.sel(a=[]), dims=""a"")\n    assert actual.dims == (""b"",)\n    assert (actual.data == np.zeros(actual.shape)).all()\n\n    # Ellipsis (...) sums over all dimensions\n    actual = xr.dot(da_a, da_b, dims=...)\n    assert actual.dims == ()\n    assert (actual.data == np.einsum(""ij,ijk->"", a, b)).all()\n\n    actual = xr.dot(da_a, da_b, da_c, dims=...)\n    assert actual.dims == ()\n    assert (actual.data == np.einsum(""ij,ijk,kl-> "", a, b, c)).all()\n\n    actual = xr.dot(da_a, dims=...)\n    assert actual.dims == ()\n    assert (actual.data == np.einsum(""ij-> "", a)).all()\n\n    actual = xr.dot(da_a.sel(a=[]), da_a.sel(a=[]), dims=...)\n    assert actual.dims == ()\n    assert (actual.data == np.zeros(actual.shape)).all()\n\n    # Invalid cases\n    if not use_dask:\n        with pytest.raises(TypeError):\n            xr.dot(da_a, dims=""a"", invalid=None)\n    with pytest.raises(TypeError):\n        xr.dot(da_a.to_dataset(name=""da""), dims=""a"")\n    with pytest.raises(TypeError):\n        xr.dot(dims=""a"")\n\n    # einsum parameters\n    actual = xr.dot(da_a, da_b, dims=[""b""], order=""C"")\n    assert (actual.data == np.einsum(""ij,ijk->ik"", a, b)).all()\n    assert actual.values.flags[""C_CONTIGUOUS""]\n    assert not actual.values.flags[""F_CONTIGUOUS""]\n    actual = xr.dot(da_a, da_b, dims=[""b""], order=""F"")\n    assert (actual.data == np.einsum(""ij,ijk->ik"", a, b)).all()\n    # dask converts Fortran arrays to C order when merging the final array\n    if not use_dask:\n        assert not actual.values.flags[""C_CONTIGUOUS""]\n        assert actual.values.flags[""F_CONTIGUOUS""]\n\n    # einsum has a constant string as of the first parameter, which makes\n    # it hard to pass to xarray.apply_ufunc.\n    # make sure dot() uses functools.partial(einsum, subscripts), which\n    # can be pickled, and not a lambda, which can\'t.\n    pickle.loads(pickle.dumps(xr.dot(da_a)))\n\n\n@pytest.mark.parametrize(""use_dask"", [True, False])\ndef test_dot_align_coords(use_dask):\n    # GH 3694\n\n    if use_dask:\n        if not has_dask:\n            pytest.skip(""test for dask."")\n\n    a = np.arange(30 * 4).reshape(30, 4)\n    b = np.arange(30 * 4 * 5).reshape(30, 4, 5)\n\n    # use partially overlapping coords\n    coords_a = {""a"": np.arange(30), ""b"": np.arange(4)}\n    coords_b = {""a"": np.arange(5, 35), ""b"": np.arange(1, 5)}\n\n    da_a = xr.DataArray(a, dims=[""a"", ""b""], coords=coords_a)\n    da_b = xr.DataArray(b, dims=[""a"", ""b"", ""c""], coords=coords_b)\n\n    if use_dask:\n        da_a = da_a.chunk({""a"": 3})\n        da_b = da_b.chunk({""a"": 3})\n\n    # join=""inner"" is the default\n    actual = xr.dot(da_a, da_b)\n    # `dot` sums over the common dimensions of the arguments\n    expected = (da_a * da_b).sum([""a"", ""b""])\n    xr.testing.assert_allclose(expected, actual)\n\n    actual = xr.dot(da_a, da_b, dims=...)\n    expected = (da_a * da_b).sum()\n    xr.testing.assert_allclose(expected, actual)\n\n    with xr.set_options(arithmetic_join=""exact""):\n        with raises_regex(ValueError, ""indexes along dimension""):\n            xr.dot(da_a, da_b)\n\n    # NOTE: dot always uses `join=""inner""` because `(a * b).sum()` yields the same for all\n    # join method (except ""exact"")\n    with xr.set_options(arithmetic_join=""left""):\n        actual = xr.dot(da_a, da_b)\n        expected = (da_a * da_b).sum([""a"", ""b""])\n        xr.testing.assert_allclose(expected, actual)\n\n    with xr.set_options(arithmetic_join=""right""):\n        actual = xr.dot(da_a, da_b)\n        expected = (da_a * da_b).sum([""a"", ""b""])\n        xr.testing.assert_allclose(expected, actual)\n\n    with xr.set_options(arithmetic_join=""outer""):\n        actual = xr.dot(da_a, da_b)\n        expected = (da_a * da_b).sum([""a"", ""b""])\n        xr.testing.assert_allclose(expected, actual)\n\n\ndef test_where():\n    cond = xr.DataArray([True, False], dims=""x"")\n    actual = xr.where(cond, 1, 0)\n    expected = xr.DataArray([1, 0], dims=""x"")\n    assert_identical(expected, actual)\n\n\n@pytest.mark.parametrize(""use_dask"", [True, False])\n@pytest.mark.parametrize(""use_datetime"", [True, False])\ndef test_polyval(use_dask, use_datetime):\n    if use_dask and not has_dask:\n        pytest.skip(""requires dask"")\n\n    if use_datetime:\n        xcoord = xr.DataArray(\n            pd.date_range(""2000-01-01"", freq=""D"", periods=10), dims=(""x"",), name=""x""\n        )\n        x = xr.core.missing.get_clean_interp_index(xcoord, ""x"")\n    else:\n        xcoord = x = np.arange(10)\n\n    da = xr.DataArray(\n        np.stack((1.0 + x + 2.0 * x ** 2, 1.0 + 2.0 * x + 3.0 * x ** 2)),\n        dims=(""d"", ""x""),\n        coords={""x"": xcoord, ""d"": [0, 1]},\n    )\n    coeffs = xr.DataArray(\n        [[2, 1, 1], [3, 2, 1]],\n        dims=(""d"", ""degree""),\n        coords={""d"": [0, 1], ""degree"": [2, 1, 0]},\n    )\n    if use_dask:\n        coeffs = coeffs.chunk({""d"": 2})\n\n    da_pv = xr.polyval(da.x, coeffs)\n\n    xr.testing.assert_allclose(da, da_pv.T)\n'"
xarray/tests/test_concat.py,23,"b'from copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nfrom xarray import DataArray, Dataset, Variable, concat\nfrom xarray.core import dtypes, merge\n\nfrom . import (\n    InaccessibleArray,\n    assert_array_equal,\n    assert_equal,\n    assert_identical,\n    raises_regex,\n    requires_dask,\n)\nfrom .test_dataset import create_test_data\n\n\ndef test_concat_compat():\n    ds1 = Dataset(\n        {\n            ""has_x_y"": ((""y"", ""x""), [[1, 2]]),\n            ""has_x"": (""x"", [1, 2]),\n            ""no_x_y"": (""z"", [1, 2]),\n        },\n        coords={""x"": [0, 1], ""y"": [0], ""z"": [-1, -2]},\n    )\n    ds2 = Dataset(\n        {\n            ""has_x_y"": ((""y"", ""x""), [[3, 4]]),\n            ""has_x"": (""x"", [1, 2]),\n            ""no_x_y"": ((""q"", ""z""), [[1, 2]]),\n        },\n        coords={""x"": [0, 1], ""y"": [1], ""z"": [-1, -2], ""q"": [0]},\n    )\n\n    result = concat([ds1, ds2], dim=""y"", data_vars=""minimal"", compat=""broadcast_equals"")\n    assert_equal(ds2.no_x_y, result.no_x_y.transpose())\n\n    for var in [""has_x"", ""no_x_y""]:\n        assert ""y"" not in result[var].dims and ""y"" not in result[var].coords\n    with raises_regex(ValueError, ""coordinates in some datasets but not others""):\n        concat([ds1, ds2], dim=""q"")\n    with raises_regex(ValueError, ""\'q\' is not present in all datasets""):\n        concat([ds2, ds1], dim=""q"")\n\n\nclass TestConcatDataset:\n    @pytest.fixture\n    def data(self):\n        return create_test_data().drop_dims(""dim3"")\n\n    def rectify_dim_order(self, data, dataset):\n        # return a new dataset with all variable dimensions transposed into\n        # the order in which they are found in `data`\n        return Dataset(\n            {k: v.transpose(*data[k].dims) for k, v in dataset.data_vars.items()},\n            dataset.coords,\n            attrs=dataset.attrs,\n        )\n\n    @pytest.mark.parametrize(""coords"", [""different"", ""minimal""])\n    @pytest.mark.parametrize(""dim"", [""dim1"", ""dim2""])\n    def test_concat_simple(self, data, dim, coords):\n        datasets = [g for _, g in data.groupby(dim, squeeze=False)]\n        assert_identical(data, concat(datasets, dim, coords=coords))\n\n    def test_concat_merge_variables_present_in_some_datasets(self, data):\n        # coordinates present in some datasets but not others\n        ds1 = Dataset(data_vars={""a"": (""y"", [0.1])}, coords={""x"": 0.1})\n        ds2 = Dataset(data_vars={""a"": (""y"", [0.2])}, coords={""z"": 0.2})\n        actual = concat([ds1, ds2], dim=""y"", coords=""minimal"")\n        expected = Dataset({""a"": (""y"", [0.1, 0.2])}, coords={""x"": 0.1, ""z"": 0.2})\n        assert_identical(expected, actual)\n\n        # data variables present in some datasets but not others\n        split_data = [data.isel(dim1=slice(3)), data.isel(dim1=slice(3, None))]\n        data0, data1 = deepcopy(split_data)\n        data1[""foo""] = (""bar"", np.random.randn(10))\n        actual = concat([data0, data1], ""dim1"")\n        expected = data.copy().assign(foo=data1.foo)\n        assert_identical(expected, actual)\n\n    def test_concat_2(self, data):\n        dim = ""dim2""\n        datasets = [g for _, g in data.groupby(dim, squeeze=True)]\n        concat_over = [k for k, v in data.coords.items() if dim in v.dims and k != dim]\n        actual = concat(datasets, data[dim], coords=concat_over)\n        assert_identical(data, self.rectify_dim_order(data, actual))\n\n    @pytest.mark.parametrize(""coords"", [""different"", ""minimal"", ""all""])\n    @pytest.mark.parametrize(""dim"", [""dim1"", ""dim2""])\n    def test_concat_coords_kwarg(self, data, dim, coords):\n        data = data.copy(deep=True)\n        # make sure the coords argument behaves as expected\n        data.coords[""extra""] = (""dim4"", np.arange(3))\n        datasets = [g for _, g in data.groupby(dim, squeeze=True)]\n\n        actual = concat(datasets, data[dim], coords=coords)\n        if coords == ""all"":\n            expected = np.array([data[""extra""].values for _ in range(data.dims[dim])])\n            assert_array_equal(actual[""extra""].values, expected)\n\n        else:\n            assert_equal(data[""extra""], actual[""extra""])\n\n    def test_concat(self, data):\n        split_data = [\n            data.isel(dim1=slice(3)),\n            data.isel(dim1=3),\n            data.isel(dim1=slice(4, None)),\n        ]\n        assert_identical(data, concat(split_data, ""dim1""))\n\n    def test_concat_dim_precedence(self, data):\n        # verify that the dim argument takes precedence over\n        # concatenating dataset variables of the same name\n        dim = (2 * data[""dim1""]).rename(""dim1"")\n        datasets = [g for _, g in data.groupby(""dim1"", squeeze=False)]\n        expected = data.copy()\n        expected[""dim1""] = dim\n        assert_identical(expected, concat(datasets, dim))\n\n    def test_concat_data_vars(self):\n        data = Dataset({""foo"": (""x"", np.random.randn(10))})\n        objs = [data.isel(x=slice(5)), data.isel(x=slice(5, None))]\n        for data_vars in [""minimal"", ""different"", ""all"", [], [""foo""]]:\n            actual = concat(objs, dim=""x"", data_vars=data_vars)\n            assert_identical(data, actual)\n\n    def test_concat_coords(self):\n        data = Dataset({""foo"": (""x"", np.random.randn(10))})\n        expected = data.assign_coords(c=(""x"", [0] * 5 + [1] * 5))\n        objs = [\n            data.isel(x=slice(5)).assign_coords(c=0),\n            data.isel(x=slice(5, None)).assign_coords(c=1),\n        ]\n        for coords in [""different"", ""all"", [""c""]]:\n            actual = concat(objs, dim=""x"", coords=coords)\n            assert_identical(expected, actual)\n        for coords in [""minimal"", []]:\n            with raises_regex(merge.MergeError, ""conflicting values""):\n                concat(objs, dim=""x"", coords=coords)\n\n    def test_concat_constant_index(self):\n        # GH425\n        ds1 = Dataset({""foo"": 1.5}, {""y"": 1})\n        ds2 = Dataset({""foo"": 2.5}, {""y"": 1})\n        expected = Dataset({""foo"": (""y"", [1.5, 2.5]), ""y"": [1, 1]})\n        for mode in [""different"", ""all"", [""foo""]]:\n            actual = concat([ds1, ds2], ""y"", data_vars=mode)\n            assert_identical(expected, actual)\n        with raises_regex(merge.MergeError, ""conflicting values""):\n            # previously dim=""y"", and raised error which makes no sense.\n            # ""foo"" has dimension ""y"" so minimal should concatenate it?\n            concat([ds1, ds2], ""new_dim"", data_vars=""minimal"")\n\n    def test_concat_size0(self):\n        data = create_test_data()\n        split_data = [data.isel(dim1=slice(0, 0)), data]\n        actual = concat(split_data, ""dim1"")\n        assert_identical(data, actual)\n\n        actual = concat(split_data[::-1], ""dim1"")\n        assert_identical(data, actual)\n\n    def test_concat_autoalign(self):\n        ds1 = Dataset({""foo"": DataArray([1, 2], coords=[(""x"", [1, 2])])})\n        ds2 = Dataset({""foo"": DataArray([1, 2], coords=[(""x"", [1, 3])])})\n        actual = concat([ds1, ds2], ""y"")\n        expected = Dataset(\n            {\n                ""foo"": DataArray(\n                    [[1, 2, np.nan], [1, np.nan, 2]],\n                    dims=[""y"", ""x""],\n                    coords={""x"": [1, 2, 3]},\n                )\n            }\n        )\n        assert_identical(expected, actual)\n\n    def test_concat_errors(self):\n        data = create_test_data()\n        split_data = [data.isel(dim1=slice(3)), data.isel(dim1=slice(3, None))]\n\n        with raises_regex(ValueError, ""must supply at least one""):\n            concat([], ""dim1"")\n\n        with raises_regex(ValueError, ""Cannot specify both .*=\'different\'""):\n            concat(\n                [data, data], dim=""concat_dim"", data_vars=""different"", compat=""override""\n            )\n\n        with raises_regex(ValueError, ""must supply at least one""):\n            concat([], ""dim1"")\n\n        with raises_regex(ValueError, ""are not coordinates""):\n            concat([data, data], ""new_dim"", coords=[""not_found""])\n\n        with raises_regex(ValueError, ""global attributes not""):\n            data0, data1 = deepcopy(split_data)\n            data1.attrs[""foo""] = ""bar""\n            concat([data0, data1], ""dim1"", compat=""identical"")\n        assert_identical(data, concat([data0, data1], ""dim1"", compat=""equals""))\n\n        with raises_regex(ValueError, ""compat.* invalid""):\n            concat(split_data, ""dim1"", compat=""foobar"")\n\n        with raises_regex(ValueError, ""unexpected value for""):\n            concat([data, data], ""new_dim"", coords=""foobar"")\n\n        with raises_regex(ValueError, ""coordinate in some datasets but not others""):\n            concat([Dataset({""x"": 0}), Dataset({""x"": [1]})], dim=""z"")\n\n        with raises_regex(ValueError, ""coordinate in some datasets but not others""):\n            concat([Dataset({""x"": 0}), Dataset({}, {""x"": 1})], dim=""z"")\n\n    def test_concat_join_kwarg(self):\n        ds1 = Dataset({""a"": ((""x"", ""y""), [[0]])}, coords={""x"": [0], ""y"": [0]})\n        ds2 = Dataset({""a"": ((""x"", ""y""), [[0]])}, coords={""x"": [1], ""y"": [0.0001]})\n\n        expected = {}\n        expected[""outer""] = Dataset(\n            {""a"": ((""x"", ""y""), [[0, np.nan], [np.nan, 0]])},\n            {""x"": [0, 1], ""y"": [0, 0.0001]},\n        )\n        expected[""inner""] = Dataset(\n            {""a"": ((""x"", ""y""), [[], []])}, {""x"": [0, 1], ""y"": []}\n        )\n        expected[""left""] = Dataset(\n            {""a"": ((""x"", ""y""), np.array([0, np.nan], ndmin=2).T)},\n            coords={""x"": [0, 1], ""y"": [0]},\n        )\n        expected[""right""] = Dataset(\n            {""a"": ((""x"", ""y""), np.array([np.nan, 0], ndmin=2).T)},\n            coords={""x"": [0, 1], ""y"": [0.0001]},\n        )\n        expected[""override""] = Dataset(\n            {""a"": ((""x"", ""y""), np.array([0, 0], ndmin=2).T)},\n            coords={""x"": [0, 1], ""y"": [0]},\n        )\n\n        with raises_regex(ValueError, ""indexes along dimension \'y\'""):\n            actual = concat([ds1, ds2], join=""exact"", dim=""x"")\n\n        for join in expected:\n            actual = concat([ds1, ds2], join=join, dim=""x"")\n            assert_equal(actual, expected[join])\n\n        # regression test for #3681\n        actual = concat([ds1.drop(""x""), ds2.drop(""x"")], join=""override"", dim=""y"")\n        expected = Dataset(\n            {""a"": ((""x"", ""y""), np.array([0, 0], ndmin=2))}, coords={""y"": [0, 0.0001]}\n        )\n        assert_identical(actual, expected)\n\n    def test_concat_combine_attrs_kwarg(self):\n        ds1 = Dataset({""a"": (""x"", [0])}, coords={""x"": [0]}, attrs={""b"": 42})\n        ds2 = Dataset({""a"": (""x"", [0])}, coords={""x"": [1]}, attrs={""b"": 42, ""c"": 43})\n\n        expected = {}\n        expected[""drop""] = Dataset({""a"": (""x"", [0, 0])}, {""x"": [0, 1]})\n        expected[""no_conflicts""] = Dataset(\n            {""a"": (""x"", [0, 0])}, {""x"": [0, 1]}, {""b"": 42, ""c"": 43}\n        )\n        expected[""override""] = Dataset({""a"": (""x"", [0, 0])}, {""x"": [0, 1]}, {""b"": 42})\n\n        with raises_regex(ValueError, ""combine_attrs=\'identical\'""):\n            actual = concat([ds1, ds2], dim=""x"", combine_attrs=""identical"")\n        with raises_regex(ValueError, ""combine_attrs=\'no_conflicts\'""):\n            ds3 = ds2.copy(deep=True)\n            ds3.attrs[""b""] = 44\n            actual = concat([ds1, ds3], dim=""x"", combine_attrs=""no_conflicts"")\n\n        for combine_attrs in expected:\n            actual = concat([ds1, ds2], dim=""x"", combine_attrs=combine_attrs)\n            assert_identical(actual, expected[combine_attrs])\n\n    def test_concat_promote_shape(self):\n        # mixed dims within variables\n        objs = [Dataset({}, {""x"": 0}), Dataset({""x"": [1]})]\n        actual = concat(objs, ""x"")\n        expected = Dataset({""x"": [0, 1]})\n        assert_identical(actual, expected)\n\n        objs = [Dataset({""x"": [0]}), Dataset({}, {""x"": 1})]\n        actual = concat(objs, ""x"")\n        assert_identical(actual, expected)\n\n        # mixed dims between variables\n        objs = [Dataset({""x"": [2], ""y"": 3}), Dataset({""x"": [4], ""y"": 5})]\n        actual = concat(objs, ""x"")\n        expected = Dataset({""x"": [2, 4], ""y"": (""x"", [3, 5])})\n        assert_identical(actual, expected)\n\n        # mixed dims in coord variable\n        objs = [Dataset({""x"": [0]}, {""y"": -1}), Dataset({""x"": [1]}, {""y"": (""x"", [-2])})]\n        actual = concat(objs, ""x"")\n        expected = Dataset({""x"": [0, 1]}, {""y"": (""x"", [-1, -2])})\n        assert_identical(actual, expected)\n\n        # scalars with mixed lengths along concat dim -- values should repeat\n        objs = [Dataset({""x"": [0]}, {""y"": -1}), Dataset({""x"": [1, 2]}, {""y"": -2})]\n        actual = concat(objs, ""x"")\n        expected = Dataset({""x"": [0, 1, 2]}, {""y"": (""x"", [-1, -2, -2])})\n        assert_identical(actual, expected)\n\n        # broadcast 1d x 1d -> 2d\n        objs = [\n            Dataset({""z"": (""x"", [-1])}, {""x"": [0], ""y"": [0]}),\n            Dataset({""z"": (""y"", [1])}, {""x"": [1], ""y"": [0]}),\n        ]\n        actual = concat(objs, ""x"")\n        expected = Dataset({""z"": ((""x"", ""y""), [[-1], [1]])}, {""x"": [0, 1], ""y"": [0]})\n        assert_identical(actual, expected)\n\n    def test_concat_do_not_promote(self):\n        # GH438\n        objs = [\n            Dataset({""y"": (""t"", [1])}, {""x"": 1, ""t"": [0]}),\n            Dataset({""y"": (""t"", [2])}, {""x"": 1, ""t"": [0]}),\n        ]\n        expected = Dataset({""y"": (""t"", [1, 2])}, {""x"": 1, ""t"": [0, 0]})\n        actual = concat(objs, ""t"")\n        assert_identical(expected, actual)\n\n        objs = [\n            Dataset({""y"": (""t"", [1])}, {""x"": 1, ""t"": [0]}),\n            Dataset({""y"": (""t"", [2])}, {""x"": 2, ""t"": [0]}),\n        ]\n        with pytest.raises(ValueError):\n            concat(objs, ""t"", coords=""minimal"")\n\n    def test_concat_dim_is_variable(self):\n        objs = [Dataset({""x"": 0}), Dataset({""x"": 1})]\n        coord = Variable(""y"", [3, 4])\n        expected = Dataset({""x"": (""y"", [0, 1]), ""y"": [3, 4]})\n        actual = concat(objs, coord)\n        assert_identical(actual, expected)\n\n    def test_concat_multiindex(self):\n        x = pd.MultiIndex.from_product([[1, 2, 3], [""a"", ""b""]])\n        expected = Dataset({""x"": x})\n        actual = concat(\n            [expected.isel(x=slice(2)), expected.isel(x=slice(2, None))], ""x""\n        )\n        assert expected.equals(actual)\n        assert isinstance(actual.x.to_index(), pd.MultiIndex)\n\n    @pytest.mark.parametrize(""fill_value"", [dtypes.NA, 2, 2.0])\n    def test_concat_fill_value(self, fill_value):\n        datasets = [\n            Dataset({""a"": (""x"", [2, 3]), ""x"": [1, 2]}),\n            Dataset({""a"": (""x"", [1, 2]), ""x"": [0, 1]}),\n        ]\n        if fill_value == dtypes.NA:\n            # if we supply the default, we expect the missing value for a\n            # float array\n            fill_value = np.nan\n        expected = Dataset(\n            {""a"": ((""t"", ""x""), [[fill_value, 2, 3], [1, 2, fill_value]])},\n            {""x"": [0, 1, 2]},\n        )\n        actual = concat(datasets, dim=""t"", fill_value=fill_value)\n        assert_identical(actual, expected)\n\n\nclass TestConcatDataArray:\n    def test_concat(self):\n        ds = Dataset(\n            {\n                ""foo"": ([""x"", ""y""], np.random.random((2, 3))),\n                ""bar"": ([""x"", ""y""], np.random.random((2, 3))),\n            },\n            {""x"": [0, 1]},\n        )\n        foo = ds[""foo""]\n        bar = ds[""bar""]\n\n        # from dataset array:\n        expected = DataArray(\n            np.array([foo.values, bar.values]),\n            dims=[""w"", ""x"", ""y""],\n            coords={""x"": [0, 1]},\n        )\n        actual = concat([foo, bar], ""w"")\n        assert_equal(expected, actual)\n        # from iteration:\n        grouped = [g for _, g in foo.groupby(""x"")]\n        stacked = concat(grouped, ds[""x""])\n        assert_identical(foo, stacked)\n        # with an index as the \'dim\' argument\n        stacked = concat(grouped, ds.indexes[""x""])\n        assert_identical(foo, stacked)\n\n        actual = concat([foo[0], foo[1]], pd.Index([0, 1])).reset_coords(drop=True)\n        expected = foo[:2].rename({""x"": ""concat_dim""})\n        assert_identical(expected, actual)\n\n        actual = concat([foo[0], foo[1]], [0, 1]).reset_coords(drop=True)\n        expected = foo[:2].rename({""x"": ""concat_dim""})\n        assert_identical(expected, actual)\n\n        with raises_regex(ValueError, ""not identical""):\n            concat([foo, bar], dim=""w"", compat=""identical"")\n\n        with raises_regex(ValueError, ""not a valid argument""):\n            concat([foo, bar], dim=""w"", data_vars=""minimal"")\n\n    def test_concat_encoding(self):\n        # Regression test for GH1297\n        ds = Dataset(\n            {\n                ""foo"": ([""x"", ""y""], np.random.random((2, 3))),\n                ""bar"": ([""x"", ""y""], np.random.random((2, 3))),\n            },\n            {""x"": [0, 1]},\n        )\n        foo = ds[""foo""]\n        foo.encoding = {""complevel"": 5}\n        ds.encoding = {""unlimited_dims"": ""x""}\n        assert concat([foo, foo], dim=""x"").encoding == foo.encoding\n        assert concat([ds, ds], dim=""x"").encoding == ds.encoding\n\n    @requires_dask\n    def test_concat_lazy(self):\n        import dask.array as da\n\n        arrays = [\n            DataArray(\n                da.from_array(InaccessibleArray(np.zeros((3, 3))), 3), dims=[""x"", ""y""]\n            )\n            for _ in range(2)\n        ]\n        # should not raise\n        combined = concat(arrays, dim=""z"")\n        assert combined.shape == (2, 3, 3)\n        assert combined.dims == (""z"", ""x"", ""y"")\n\n    @pytest.mark.parametrize(""fill_value"", [dtypes.NA, 2, 2.0])\n    def test_concat_fill_value(self, fill_value):\n        foo = DataArray([1, 2], coords=[(""x"", [1, 2])])\n        bar = DataArray([1, 2], coords=[(""x"", [1, 3])])\n        if fill_value == dtypes.NA:\n            # if we supply the default, we expect the missing value for a\n            # float array\n            fill_value = np.nan\n        expected = DataArray(\n            [[1, 2, fill_value], [1, fill_value, 2]],\n            dims=[""y"", ""x""],\n            coords={""x"": [1, 2, 3]},\n        )\n        actual = concat((foo, bar), dim=""y"", fill_value=fill_value)\n        assert_identical(actual, expected)\n\n    def test_concat_join_kwarg(self):\n        ds1 = Dataset(\n            {""a"": ((""x"", ""y""), [[0]])}, coords={""x"": [0], ""y"": [0]}\n        ).to_array()\n        ds2 = Dataset(\n            {""a"": ((""x"", ""y""), [[0]])}, coords={""x"": [1], ""y"": [0.0001]}\n        ).to_array()\n\n        expected = {}\n        expected[""outer""] = Dataset(\n            {""a"": ((""x"", ""y""), [[0, np.nan], [np.nan, 0]])},\n            {""x"": [0, 1], ""y"": [0, 0.0001]},\n        )\n        expected[""inner""] = Dataset(\n            {""a"": ((""x"", ""y""), [[], []])}, {""x"": [0, 1], ""y"": []}\n        )\n        expected[""left""] = Dataset(\n            {""a"": ((""x"", ""y""), np.array([0, np.nan], ndmin=2).T)},\n            coords={""x"": [0, 1], ""y"": [0]},\n        )\n        expected[""right""] = Dataset(\n            {""a"": ((""x"", ""y""), np.array([np.nan, 0], ndmin=2).T)},\n            coords={""x"": [0, 1], ""y"": [0.0001]},\n        )\n        expected[""override""] = Dataset(\n            {""a"": ((""x"", ""y""), np.array([0, 0], ndmin=2).T)},\n            coords={""x"": [0, 1], ""y"": [0]},\n        )\n\n        with raises_regex(ValueError, ""indexes along dimension \'y\'""):\n            actual = concat([ds1, ds2], join=""exact"", dim=""x"")\n\n        for join in expected:\n            actual = concat([ds1, ds2], join=join, dim=""x"")\n            assert_equal(actual, expected[join].to_array())\n\n    def test_concat_combine_attrs_kwarg(self):\n        da1 = DataArray([0], coords=[(""x"", [0])], attrs={""b"": 42})\n        da2 = DataArray([0], coords=[(""x"", [1])], attrs={""b"": 42, ""c"": 43})\n\n        expected = {}\n        expected[""drop""] = DataArray([0, 0], coords=[(""x"", [0, 1])])\n        expected[""no_conflicts""] = DataArray(\n            [0, 0], coords=[(""x"", [0, 1])], attrs={""b"": 42, ""c"": 43}\n        )\n        expected[""override""] = DataArray(\n            [0, 0], coords=[(""x"", [0, 1])], attrs={""b"": 42}\n        )\n\n        with raises_regex(ValueError, ""combine_attrs=\'identical\'""):\n            actual = concat([da1, da2], dim=""x"", combine_attrs=""identical"")\n        with raises_regex(ValueError, ""combine_attrs=\'no_conflicts\'""):\n            da3 = da2.copy(deep=True)\n            da3.attrs[""b""] = 44\n            actual = concat([da1, da3], dim=""x"", combine_attrs=""no_conflicts"")\n\n        for combine_attrs in expected:\n            actual = concat([da1, da2], dim=""x"", combine_attrs=combine_attrs)\n            assert_identical(actual, expected[combine_attrs])\n\n\n@pytest.mark.parametrize(""attr1"", ({""a"": {""meta"": [10, 20, 30]}}, {""a"": [1, 2, 3]}, {}))\n@pytest.mark.parametrize(""attr2"", ({""a"": [1, 2, 3]}, {}))\ndef test_concat_attrs_first_variable(attr1, attr2):\n\n    arrs = [\n        DataArray([[1], [2]], dims=[""x"", ""y""], attrs=attr1),\n        DataArray([[3], [4]], dims=[""x"", ""y""], attrs=attr2),\n    ]\n\n    concat_attrs = concat(arrs, ""y"").attrs\n    assert concat_attrs == attr1\n\n\ndef test_concat_merge_single_non_dim_coord():\n    da1 = DataArray([1, 2, 3], dims=""x"", coords={""x"": [1, 2, 3], ""y"": 1})\n    da2 = DataArray([4, 5, 6], dims=""x"", coords={""x"": [4, 5, 6]})\n\n    expected = DataArray(range(1, 7), dims=""x"", coords={""x"": range(1, 7), ""y"": 1})\n\n    for coords in [""different"", ""minimal""]:\n        actual = concat([da1, da2], ""x"", coords=coords)\n        assert_identical(actual, expected)\n\n    with raises_regex(ValueError, ""\'y\' is not present in all datasets.""):\n        concat([da1, da2], dim=""x"", coords=""all"")\n\n    da1 = DataArray([1, 2, 3], dims=""x"", coords={""x"": [1, 2, 3], ""y"": 1})\n    da2 = DataArray([4, 5, 6], dims=""x"", coords={""x"": [4, 5, 6]})\n    da3 = DataArray([7, 8, 9], dims=""x"", coords={""x"": [7, 8, 9], ""y"": 1})\n    for coords in [""different"", ""all""]:\n        with raises_regex(ValueError, ""\'y\' not present in all datasets""):\n            concat([da1, da2, da3], dim=""x"")\n'"
xarray/tests/test_conventions.py,35,"b'import contextlib\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nfrom xarray import (\n    Dataset,\n    SerializationWarning,\n    Variable,\n    coding,\n    conventions,\n    open_dataset,\n)\nfrom xarray.backends.common import WritableCFDataStore\nfrom xarray.backends.memory import InMemoryDataStore\nfrom xarray.conventions import decode_cf\nfrom xarray.testing import assert_identical\n\nfrom . import (\n    assert_array_equal,\n    raises_regex,\n    requires_cftime,\n    requires_dask,\n    requires_netCDF4,\n)\nfrom .test_backends import CFEncodedBase\n\n\nclass TestBoolTypeArray:\n    def test_booltype_array(self):\n        x = np.array([1, 0, 1, 1, 0], dtype=""i1"")\n        bx = conventions.BoolTypeArray(x)\n        assert bx.dtype == np.bool\n        assert_array_equal(\n            bx, np.array([True, False, True, True, False], dtype=np.bool)\n        )\n\n\nclass TestNativeEndiannessArray:\n    def test(self):\n        x = np.arange(5, dtype="">i8"")\n        expected = np.arange(5, dtype=""int64"")\n        a = conventions.NativeEndiannessArray(x)\n        assert a.dtype == expected.dtype\n        assert a.dtype == expected[:].dtype\n        assert_array_equal(a, expected)\n\n\ndef test_decode_cf_with_conflicting_fill_missing_value():\n    expected = Variable([""t""], [np.nan, np.nan, 2], {""units"": ""foobar""})\n    var = Variable(\n        [""t""], np.arange(3), {""units"": ""foobar"", ""missing_value"": 0, ""_FillValue"": 1}\n    )\n    with warnings.catch_warnings(record=True) as w:\n        actual = conventions.decode_cf_variable(""t"", var)\n        assert_identical(actual, expected)\n        assert ""has multiple fill"" in str(w[0].message)\n\n    expected = Variable([""t""], np.arange(10), {""units"": ""foobar""})\n\n    var = Variable(\n        [""t""],\n        np.arange(10),\n        {""units"": ""foobar"", ""missing_value"": np.nan, ""_FillValue"": np.nan},\n    )\n    actual = conventions.decode_cf_variable(""t"", var)\n    assert_identical(actual, expected)\n\n    var = Variable(\n        [""t""],\n        np.arange(10),\n        {\n            ""units"": ""foobar"",\n            ""missing_value"": np.float32(np.nan),\n            ""_FillValue"": np.float32(np.nan),\n        },\n    )\n    actual = conventions.decode_cf_variable(""t"", var)\n    assert_identical(actual, expected)\n\n\n@requires_cftime\nclass TestEncodeCFVariable:\n    def test_incompatible_attributes(self):\n        invalid_vars = [\n            Variable(\n                [""t""], pd.date_range(""2000-01-01"", periods=3), {""units"": ""foobar""}\n            ),\n            Variable([""t""], pd.to_timedelta([""1 day""]), {""units"": ""foobar""}),\n            Variable([""t""], [0, 1, 2], {""add_offset"": 0}, {""add_offset"": 2}),\n            Variable([""t""], [0, 1, 2], {""_FillValue"": 0}, {""_FillValue"": 2}),\n        ]\n        for var in invalid_vars:\n            with pytest.raises(ValueError):\n                conventions.encode_cf_variable(var)\n\n    def test_missing_fillvalue(self):\n        v = Variable([""x""], np.array([np.nan, 1, 2, 3]))\n        v.encoding = {""dtype"": ""int16""}\n        with pytest.warns(Warning, match=""floating point data as an integer""):\n            conventions.encode_cf_variable(v)\n\n    def test_multidimensional_coordinates(self):\n        # regression test for GH1763\n        # Set up test case with coordinates that have overlapping (but not\n        # identical) dimensions.\n        zeros1 = np.zeros((1, 5, 3))\n        zeros2 = np.zeros((1, 6, 3))\n        zeros3 = np.zeros((1, 5, 4))\n        orig = Dataset(\n            {\n                ""lon1"": ([""x1"", ""y1""], zeros1.squeeze(0), {}),\n                ""lon2"": ([""x2"", ""y1""], zeros2.squeeze(0), {}),\n                ""lon3"": ([""x1"", ""y2""], zeros3.squeeze(0), {}),\n                ""lat1"": ([""x1"", ""y1""], zeros1.squeeze(0), {}),\n                ""lat2"": ([""x2"", ""y1""], zeros2.squeeze(0), {}),\n                ""lat3"": ([""x1"", ""y2""], zeros3.squeeze(0), {}),\n                ""foo1"": ([""time"", ""x1"", ""y1""], zeros1, {""coordinates"": ""lon1 lat1""}),\n                ""foo2"": ([""time"", ""x2"", ""y1""], zeros2, {""coordinates"": ""lon2 lat2""}),\n                ""foo3"": ([""time"", ""x1"", ""y2""], zeros3, {""coordinates"": ""lon3 lat3""}),\n                ""time"": (""time"", [0.0], {""units"": ""hours since 2017-01-01""}),\n            }\n        )\n        orig = conventions.decode_cf(orig)\n        # Encode the coordinates, as they would be in a netCDF output file.\n        enc, attrs = conventions.encode_dataset_coordinates(orig)\n        # Make sure we have the right coordinates for each variable.\n        foo1_coords = enc[""foo1""].attrs.get(""coordinates"", """")\n        foo2_coords = enc[""foo2""].attrs.get(""coordinates"", """")\n        foo3_coords = enc[""foo3""].attrs.get(""coordinates"", """")\n        assert set(foo1_coords.split()) == {""lat1"", ""lon1""}\n        assert set(foo2_coords.split()) == {""lat2"", ""lon2""}\n        assert set(foo3_coords.split()) == {""lat3"", ""lon3""}\n        # Should not have any global coordinates.\n        assert ""coordinates"" not in attrs\n\n    def test_do_not_overwrite_user_coordinates(self):\n        orig = Dataset(\n            coords={""x"": [0, 1, 2], ""y"": (""x"", [5, 6, 7]), ""z"": (""x"", [8, 9, 10])},\n            data_vars={""a"": (""x"", [1, 2, 3]), ""b"": (""x"", [3, 5, 6])},\n        )\n        orig[""a""].encoding[""coordinates""] = ""y""\n        orig[""b""].encoding[""coordinates""] = ""z""\n        enc, _ = conventions.encode_dataset_coordinates(orig)\n        assert enc[""a""].attrs[""coordinates""] == ""y""\n        assert enc[""b""].attrs[""coordinates""] == ""z""\n        orig[""a""].attrs[""coordinates""] = ""foo""\n        with raises_regex(ValueError, ""\'coordinates\' found in both attrs""):\n            conventions.encode_dataset_coordinates(orig)\n\n    @requires_dask\n    def test_string_object_warning(self):\n        original = Variable((""x"",), np.array([""foo"", ""bar""], dtype=object)).chunk()\n        with pytest.warns(SerializationWarning, match=""dask array with dtype=object""):\n            encoded = conventions.encode_cf_variable(original)\n        assert_identical(original, encoded)\n\n\n@requires_cftime\nclass TestDecodeCF:\n    def test_dataset(self):\n        original = Dataset(\n            {\n                ""t"": (""t"", [0, 1, 2], {""units"": ""days since 2000-01-01""}),\n                ""foo"": (""t"", [0, 0, 0], {""coordinates"": ""y"", ""units"": ""bar""}),\n                ""y"": (""t"", [5, 10, -999], {""_FillValue"": -999}),\n            }\n        )\n        expected = Dataset(\n            {""foo"": (""t"", [0, 0, 0], {""units"": ""bar""})},\n            {\n                ""t"": pd.date_range(""2000-01-01"", periods=3),\n                ""y"": (""t"", [5.0, 10.0, np.nan]),\n            },\n        )\n        actual = conventions.decode_cf(original)\n        assert_identical(expected, actual)\n\n    def test_invalid_coordinates(self):\n        # regression test for GH308\n        original = Dataset({""foo"": (""t"", [1, 2], {""coordinates"": ""invalid""})})\n        actual = conventions.decode_cf(original)\n        assert_identical(original, actual)\n\n    def test_decode_coordinates(self):\n        # regression test for GH610\n        original = Dataset(\n            {""foo"": (""t"", [1, 2], {""coordinates"": ""x""}), ""x"": (""t"", [4, 5])}\n        )\n        actual = conventions.decode_cf(original)\n        assert actual.foo.encoding[""coordinates""] == ""x""\n\n    def test_0d_int32_encoding(self):\n        original = Variable((), np.int32(0), encoding={""dtype"": ""int64""})\n        expected = Variable((), np.int64(0))\n        actual = conventions.maybe_encode_nonstring_dtype(original)\n        assert_identical(expected, actual)\n\n    def test_decode_cf_with_multiple_missing_values(self):\n        original = Variable([""t""], [0, 1, 2], {""missing_value"": np.array([0, 1])})\n        expected = Variable([""t""], [np.nan, np.nan, 2], {})\n        with warnings.catch_warnings(record=True) as w:\n            actual = conventions.decode_cf_variable(""t"", original)\n            assert_identical(expected, actual)\n            assert ""has multiple fill"" in str(w[0].message)\n\n    def test_decode_cf_with_drop_variables(self):\n        original = Dataset(\n            {\n                ""t"": (""t"", [0, 1, 2], {""units"": ""days since 2000-01-01""}),\n                ""x"": (""x"", [9, 8, 7], {""units"": ""km""}),\n                ""foo"": (\n                    (""t"", ""x""),\n                    [[0, 0, 0], [1, 1, 1], [2, 2, 2]],\n                    {""units"": ""bar""},\n                ),\n                ""y"": (""t"", [5, 10, -999], {""_FillValue"": -999}),\n            }\n        )\n        expected = Dataset(\n            {\n                ""t"": pd.date_range(""2000-01-01"", periods=3),\n                ""foo"": (\n                    (""t"", ""x""),\n                    [[0, 0, 0], [1, 1, 1], [2, 2, 2]],\n                    {""units"": ""bar""},\n                ),\n                ""y"": (""t"", [5, 10, np.nan]),\n            }\n        )\n        actual = conventions.decode_cf(original, drop_variables=(""x"",))\n        actual2 = conventions.decode_cf(original, drop_variables=""x"")\n        assert_identical(expected, actual)\n        assert_identical(expected, actual2)\n\n    def test_invalid_time_units_raises_eagerly(self):\n        ds = Dataset({""time"": (""time"", [0, 1], {""units"": ""foobar since 123""})})\n        with raises_regex(ValueError, ""unable to decode time""):\n            decode_cf(ds)\n\n    @requires_cftime\n    def test_dataset_repr_with_netcdf4_datetimes(self):\n        # regression test for #347\n        attrs = {""units"": ""days since 0001-01-01"", ""calendar"": ""noleap""}\n        with warnings.catch_warnings():\n            warnings.filterwarnings(""ignore"", ""unable to decode time"")\n            ds = decode_cf(Dataset({""time"": (""time"", [0, 1], attrs)}))\n            assert ""(time) object"" in repr(ds)\n\n        attrs = {""units"": ""days since 1900-01-01""}\n        ds = decode_cf(Dataset({""time"": (""time"", [0, 1], attrs)}))\n        assert ""(time) datetime64[ns]"" in repr(ds)\n\n    @requires_cftime\n    def test_decode_cf_datetime_transition_to_invalid(self):\n        # manually create dataset with not-decoded date\n        from datetime import datetime\n\n        ds = Dataset(coords={""time"": [0, 266 * 365]})\n        units = ""days since 2000-01-01 00:00:00""\n        ds.time.attrs = dict(units=units)\n        with warnings.catch_warnings():\n            warnings.filterwarnings(""ignore"", ""unable to decode time"")\n            ds_decoded = conventions.decode_cf(ds)\n\n        expected = [datetime(2000, 1, 1, 0, 0), datetime(2265, 10, 28, 0, 0)]\n\n        assert_array_equal(ds_decoded.time.values, expected)\n\n    @requires_dask\n    def test_decode_cf_with_dask(self):\n        import dask.array as da\n\n        original = Dataset(\n            {\n                ""t"": (""t"", [0, 1, 2], {""units"": ""days since 2000-01-01""}),\n                ""foo"": (""t"", [0, 0, 0], {""coordinates"": ""y"", ""units"": ""bar""}),\n                ""bar"": (""string2"", [b""a"", b""b""]),\n                ""baz"": ((""x""), [b""abc""], {""_Encoding"": ""utf-8""}),\n                ""y"": (""t"", [5, 10, -999], {""_FillValue"": -999}),\n            }\n        ).chunk()\n        decoded = conventions.decode_cf(original)\n        print(decoded)\n        assert all(\n            isinstance(var.data, da.Array)\n            for name, var in decoded.variables.items()\n            if name not in decoded.indexes\n        )\n        assert_identical(decoded, conventions.decode_cf(original).compute())\n\n    @requires_dask\n    def test_decode_dask_times(self):\n        original = Dataset.from_dict(\n            {\n                ""coords"": {},\n                ""dims"": {""time"": 5},\n                ""data_vars"": {\n                    ""average_T1"": {\n                        ""dims"": (""time"",),\n                        ""attrs"": {""units"": ""days since 1958-01-01 00:00:00""},\n                        ""data"": [87659.0, 88024.0, 88389.0, 88754.0, 89119.0],\n                    }\n                },\n            }\n        )\n        assert_identical(\n            conventions.decode_cf(original.chunk()),\n            conventions.decode_cf(original).chunk(),\n        )\n\n    def test_decode_cf_time_kwargs(self):\n        ds = Dataset.from_dict(\n            {\n                ""coords"": {\n                    ""timedelta"": {\n                        ""data"": np.array([1, 2, 3], dtype=""int64""),\n                        ""dims"": ""timedelta"",\n                        ""attrs"": {""units"": ""days""},\n                    },\n                    ""time"": {\n                        ""data"": np.array([1, 2, 3], dtype=""int64""),\n                        ""dims"": ""time"",\n                        ""attrs"": {""units"": ""days since 2000-01-01""},\n                    },\n                },\n                ""dims"": {""time"": 3, ""timedelta"": 3},\n                ""data_vars"": {\n                    ""a"": {""dims"": (""time"", ""timedelta""), ""data"": np.ones((3, 3))},\n                },\n            }\n        )\n\n        dsc = conventions.decode_cf(ds)\n        assert dsc.timedelta.dtype == np.dtype(""m8[ns]"")\n        assert dsc.time.dtype == np.dtype(""M8[ns]"")\n        dsc = conventions.decode_cf(ds, decode_times=False)\n        assert dsc.timedelta.dtype == np.dtype(""int64"")\n        assert dsc.time.dtype == np.dtype(""int64"")\n        dsc = conventions.decode_cf(ds, decode_times=True, decode_timedelta=False)\n        assert dsc.timedelta.dtype == np.dtype(""int64"")\n        assert dsc.time.dtype == np.dtype(""M8[ns]"")\n        dsc = conventions.decode_cf(ds, decode_times=False, decode_timedelta=True)\n        assert dsc.timedelta.dtype == np.dtype(""m8[ns]"")\n        assert dsc.time.dtype == np.dtype(""int64"")\n\n\nclass CFEncodedInMemoryStore(WritableCFDataStore, InMemoryDataStore):\n    def encode_variable(self, var):\n        """"""encode one variable""""""\n        coder = coding.strings.EncodedStringCoder(allows_unicode=True)\n        var = coder.encode(var)\n        return var\n\n\n@requires_netCDF4\nclass TestCFEncodedDataStore(CFEncodedBase):\n    @contextlib.contextmanager\n    def create_store(self):\n        yield CFEncodedInMemoryStore()\n\n    @contextlib.contextmanager\n    def roundtrip(\n        self, data, save_kwargs=None, open_kwargs=None, allow_cleanup_failure=False\n    ):\n        if save_kwargs is None:\n            save_kwargs = {}\n        if open_kwargs is None:\n            open_kwargs = {}\n        store = CFEncodedInMemoryStore()\n        data.dump_to_store(store, **save_kwargs)\n        yield open_dataset(store, **open_kwargs)\n\n    @pytest.mark.skip(""cannot roundtrip coordinates yet for "" ""CFEncodedInMemoryStore"")\n    def test_roundtrip_coordinates(self):\n        pass\n\n    def test_invalid_dataarray_names_raise(self):\n        # only relevant for on-disk file formats\n        pass\n\n    def test_encoding_kwarg(self):\n        # we haven\'t bothered to raise errors yet for unexpected encodings in\n        # this test dummy\n        pass\n\n    def test_encoding_kwarg_fixed_width_string(self):\n        # CFEncodedInMemoryStore doesn\'t support explicit string encodings.\n        pass\n'"
xarray/tests/test_dask.py,55,"b'import operator\nimport pickle\nimport sys\nfrom contextlib import suppress\nfrom distutils.version import LooseVersion\nfrom textwrap import dedent\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nimport xarray as xr\nimport xarray.ufuncs as xu\nfrom xarray import DataArray, Dataset, Variable\nfrom xarray.core import duck_array_ops\nfrom xarray.testing import assert_chunks_equal\nfrom xarray.tests import mock\n\nfrom ..core.duck_array_ops import lazy_array_equiv\nfrom . import (\n    assert_allclose,\n    assert_array_equal,\n    assert_equal,\n    assert_frame_equal,\n    assert_identical,\n    raises_regex,\n    requires_scipy_or_netCDF4,\n)\nfrom .test_backends import create_tmp_file\n\ndask = pytest.importorskip(""dask"")\nda = pytest.importorskip(""dask.array"")\ndd = pytest.importorskip(""dask.dataframe"")\n\nON_WINDOWS = sys.platform == ""win32""\n\n\nclass CountingScheduler:\n    """""" Simple dask scheduler counting the number of computes.\n\n    Reference: https://stackoverflow.com/questions/53289286/ """"""\n\n    def __init__(self, max_computes=0):\n        self.total_computes = 0\n        self.max_computes = max_computes\n\n    def __call__(self, dsk, keys, **kwargs):\n        self.total_computes += 1\n        if self.total_computes > self.max_computes:\n            raise RuntimeError(\n                ""Too many computes. Total: %d > max: %d.""\n                % (self.total_computes, self.max_computes)\n            )\n        return dask.get(dsk, keys, **kwargs)\n\n\ndef raise_if_dask_computes(max_computes=0):\n    scheduler = CountingScheduler(max_computes)\n    return dask.config.set(scheduler=scheduler)\n\n\ndef test_raise_if_dask_computes():\n    data = da.from_array(np.random.RandomState(0).randn(4, 6), chunks=(2, 2))\n    with raises_regex(RuntimeError, ""Too many computes""):\n        with raise_if_dask_computes():\n            data.compute()\n\n\nclass DaskTestCase:\n    def assertLazyAnd(self, expected, actual, test):\n        with dask.config.set(scheduler=""synchronous""):\n            test(actual, expected)\n\n        if isinstance(actual, Dataset):\n            for k, v in actual.variables.items():\n                if k in actual.dims:\n                    assert isinstance(v.data, np.ndarray)\n                else:\n                    assert isinstance(v.data, da.Array)\n        elif isinstance(actual, DataArray):\n            assert isinstance(actual.data, da.Array)\n            for k, v in actual.coords.items():\n                if k in actual.dims:\n                    assert isinstance(v.data, np.ndarray)\n                else:\n                    assert isinstance(v.data, da.Array)\n        elif isinstance(actual, Variable):\n            assert isinstance(actual.data, da.Array)\n        else:\n            assert False\n\n\nclass TestVariable(DaskTestCase):\n    def assertLazyAndIdentical(self, expected, actual):\n        self.assertLazyAnd(expected, actual, assert_identical)\n\n    def assertLazyAndAllClose(self, expected, actual):\n        self.assertLazyAnd(expected, actual, assert_allclose)\n\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        self.values = np.random.RandomState(0).randn(4, 6)\n        self.data = da.from_array(self.values, chunks=(2, 2))\n\n        self.eager_var = Variable((""x"", ""y""), self.values)\n        self.lazy_var = Variable((""x"", ""y""), self.data)\n\n    def test_basics(self):\n        v = self.lazy_var\n        assert self.data is v.data\n        assert self.data.chunks == v.chunks\n        assert_array_equal(self.values, v)\n\n    def test_copy(self):\n        self.assertLazyAndIdentical(self.eager_var, self.lazy_var.copy())\n        self.assertLazyAndIdentical(self.eager_var, self.lazy_var.copy(deep=True))\n\n    def test_chunk(self):\n        for chunks, expected in [\n            (None, ((2, 2), (2, 2, 2))),\n            (3, ((3, 1), (3, 3))),\n            ({""x"": 3, ""y"": 3}, ((3, 1), (3, 3))),\n            ({""x"": 3}, ((3, 1), (2, 2, 2))),\n            ({""x"": (3, 1)}, ((3, 1), (2, 2, 2))),\n        ]:\n            rechunked = self.lazy_var.chunk(chunks)\n            assert rechunked.chunks == expected\n            self.assertLazyAndIdentical(self.eager_var, rechunked)\n\n    def test_indexing(self):\n        u = self.eager_var\n        v = self.lazy_var\n        self.assertLazyAndIdentical(u[0], v[0])\n        self.assertLazyAndIdentical(u[:1], v[:1])\n        self.assertLazyAndIdentical(u[[0, 1], [0, 1, 2]], v[[0, 1], [0, 1, 2]])\n        with raises_regex(TypeError, ""stored in a dask array""):\n            v[:1] = 0\n\n    def test_squeeze(self):\n        u = self.eager_var\n        v = self.lazy_var\n        self.assertLazyAndIdentical(u[0].squeeze(), v[0].squeeze())\n\n    def test_equals(self):\n        v = self.lazy_var\n        assert v.equals(v)\n        assert isinstance(v.data, da.Array)\n        assert v.identical(v)\n        assert isinstance(v.data, da.Array)\n\n    def test_transpose(self):\n        u = self.eager_var\n        v = self.lazy_var\n        self.assertLazyAndIdentical(u.T, v.T)\n\n    def test_shift(self):\n        u = self.eager_var\n        v = self.lazy_var\n        self.assertLazyAndIdentical(u.shift(x=2), v.shift(x=2))\n        self.assertLazyAndIdentical(u.shift(x=-2), v.shift(x=-2))\n        assert v.data.chunks == v.shift(x=1).data.chunks\n\n    def test_roll(self):\n        u = self.eager_var\n        v = self.lazy_var\n        self.assertLazyAndIdentical(u.roll(x=2), v.roll(x=2))\n        assert v.data.chunks == v.roll(x=1).data.chunks\n\n    def test_unary_op(self):\n        u = self.eager_var\n        v = self.lazy_var\n        self.assertLazyAndIdentical(-u, -v)\n        self.assertLazyAndIdentical(abs(u), abs(v))\n        self.assertLazyAndIdentical(u.round(), v.round())\n\n    def test_binary_op(self):\n        u = self.eager_var\n        v = self.lazy_var\n        self.assertLazyAndIdentical(2 * u, 2 * v)\n        self.assertLazyAndIdentical(u + u, v + v)\n        self.assertLazyAndIdentical(u[0] + u, v[0] + v)\n\n    def test_repr(self):\n        expected = dedent(\n            """"""\\\n            <xarray.Variable (x: 4, y: 6)>\n            {!r}"""""".format(\n                self.lazy_var.data\n            )\n        )\n        assert expected == repr(self.lazy_var)\n\n    def test_pickle(self):\n        # Test that pickling/unpickling does not convert the dask\n        # backend to numpy\n        a1 = Variable([""x""], build_dask_array(""x""))\n        a1.compute()\n        assert not a1._in_memory\n        assert kernel_call_count == 1\n        a2 = pickle.loads(pickle.dumps(a1))\n        assert kernel_call_count == 1\n        assert_identical(a1, a2)\n        assert not a1._in_memory\n        assert not a2._in_memory\n\n    def test_reduce(self):\n        u = self.eager_var\n        v = self.lazy_var\n        self.assertLazyAndAllClose(u.mean(), v.mean())\n        self.assertLazyAndAllClose(u.std(), v.std())\n        with raise_if_dask_computes():\n            actual = v.argmax(dim=""x"")\n        self.assertLazyAndAllClose(u.argmax(dim=""x""), actual)\n        with raise_if_dask_computes():\n            actual = v.argmin(dim=""x"")\n        self.assertLazyAndAllClose(u.argmin(dim=""x""), actual)\n        self.assertLazyAndAllClose((u > 1).any(), (v > 1).any())\n        self.assertLazyAndAllClose((u < 1).all(""x""), (v < 1).all(""x""))\n        with raises_regex(NotImplementedError, ""only works along an axis""):\n            v.median()\n        with raises_regex(NotImplementedError, ""only works along an axis""):\n            v.median(v.dims)\n        with raise_if_dask_computes():\n            v.reduce(duck_array_ops.mean)\n\n    def test_missing_values(self):\n        values = np.array([0, 1, np.nan, 3])\n        data = da.from_array(values, chunks=(2,))\n\n        eager_var = Variable(""x"", values)\n        lazy_var = Variable(""x"", data)\n        self.assertLazyAndIdentical(eager_var, lazy_var.fillna(lazy_var))\n        self.assertLazyAndIdentical(Variable(""x"", range(4)), lazy_var.fillna(2))\n        self.assertLazyAndIdentical(eager_var.count(), lazy_var.count())\n\n    def test_concat(self):\n        u = self.eager_var\n        v = self.lazy_var\n        self.assertLazyAndIdentical(u, Variable.concat([v[:2], v[2:]], ""x""))\n        self.assertLazyAndIdentical(u[:2], Variable.concat([v[0], v[1]], ""x""))\n        self.assertLazyAndIdentical(u[:2], Variable.concat([u[0], v[1]], ""x""))\n        self.assertLazyAndIdentical(u[:2], Variable.concat([v[0], u[1]], ""x""))\n        self.assertLazyAndIdentical(\n            u[:3], Variable.concat([v[[0, 2]], v[[1]]], ""x"", positions=[[0, 2], [1]])\n        )\n\n    def test_missing_methods(self):\n        v = self.lazy_var\n        try:\n            v.argsort()\n        except NotImplementedError as err:\n            assert ""dask"" in str(err)\n        try:\n            v[0].item()\n        except NotImplementedError as err:\n            assert ""dask"" in str(err)\n\n    @pytest.mark.filterwarnings(""ignore::PendingDeprecationWarning"")\n    def test_univariate_ufunc(self):\n        u = self.eager_var\n        v = self.lazy_var\n        self.assertLazyAndAllClose(np.sin(u), xu.sin(v))\n\n    @pytest.mark.filterwarnings(""ignore::PendingDeprecationWarning"")\n    def test_bivariate_ufunc(self):\n        u = self.eager_var\n        v = self.lazy_var\n        self.assertLazyAndAllClose(np.maximum(u, 0), xu.maximum(v, 0))\n        self.assertLazyAndAllClose(np.maximum(u, 0), xu.maximum(0, v))\n\n    def test_compute(self):\n        u = self.eager_var\n        v = self.lazy_var\n\n        assert dask.is_dask_collection(v)\n        (v2,) = dask.compute(v + 1)\n        assert not dask.is_dask_collection(v2)\n\n        assert ((u + 1).data == v2.data).all()\n\n    def test_persist(self):\n        u = self.eager_var\n        v = self.lazy_var + 1\n\n        (v2,) = dask.persist(v)\n        assert v is not v2\n        assert len(v2.__dask_graph__()) < len(v.__dask_graph__())\n        assert v2.__dask_keys__() == v.__dask_keys__()\n        assert dask.is_dask_collection(v)\n        assert dask.is_dask_collection(v2)\n\n        self.assertLazyAndAllClose(u + 1, v)\n        self.assertLazyAndAllClose(u + 1, v2)\n\n\nclass TestDataArrayAndDataset(DaskTestCase):\n    def assertLazyAndIdentical(self, expected, actual):\n        self.assertLazyAnd(expected, actual, assert_identical)\n\n    def assertLazyAndAllClose(self, expected, actual):\n        self.assertLazyAnd(expected, actual, assert_allclose)\n\n    def assertLazyAndEqual(self, expected, actual):\n        self.assertLazyAnd(expected, actual, assert_equal)\n\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        self.values = np.random.randn(4, 6)\n        self.data = da.from_array(self.values, chunks=(2, 2))\n        self.eager_array = DataArray(\n            self.values, coords={""x"": range(4)}, dims=(""x"", ""y""), name=""foo""\n        )\n        self.lazy_array = DataArray(\n            self.data, coords={""x"": range(4)}, dims=(""x"", ""y""), name=""foo""\n        )\n\n    def test_rechunk(self):\n        chunked = self.eager_array.chunk({""x"": 2}).chunk({""y"": 2})\n        assert chunked.chunks == ((2,) * 2, (2,) * 3)\n        self.assertLazyAndIdentical(self.lazy_array, chunked)\n\n    def test_new_chunk(self):\n        chunked = self.eager_array.chunk()\n        assert chunked.data.name.startswith(""xarray-<this-array>"")\n\n    def test_lazy_dataset(self):\n        lazy_ds = Dataset({""foo"": ((""x"", ""y""), self.data)})\n        assert isinstance(lazy_ds.foo.variable.data, da.Array)\n\n    def test_lazy_array(self):\n        u = self.eager_array\n        v = self.lazy_array\n\n        self.assertLazyAndAllClose(u, v)\n        self.assertLazyAndAllClose(-u, -v)\n        self.assertLazyAndAllClose(u.T, v.T)\n        self.assertLazyAndAllClose(u.mean(), v.mean())\n        self.assertLazyAndAllClose(1 + u, 1 + v)\n\n        actual = xr.concat([v[:2], v[2:]], ""x"")\n        self.assertLazyAndAllClose(u, actual)\n\n    def test_compute(self):\n        u = self.eager_array\n        v = self.lazy_array\n\n        assert dask.is_dask_collection(v)\n        (v2,) = dask.compute(v + 1)\n        assert not dask.is_dask_collection(v2)\n\n        assert ((u + 1).data == v2.data).all()\n\n    def test_persist(self):\n        u = self.eager_array\n        v = self.lazy_array + 1\n\n        (v2,) = dask.persist(v)\n        assert v is not v2\n        assert len(v2.__dask_graph__()) < len(v.__dask_graph__())\n        assert v2.__dask_keys__() == v.__dask_keys__()\n        assert dask.is_dask_collection(v)\n        assert dask.is_dask_collection(v2)\n\n        self.assertLazyAndAllClose(u + 1, v)\n        self.assertLazyAndAllClose(u + 1, v2)\n\n    def test_concat_loads_variables(self):\n        # Test that concat() computes not-in-memory variables at most once\n        # and loads them in the output, while leaving the input unaltered.\n        d1 = build_dask_array(""d1"")\n        c1 = build_dask_array(""c1"")\n        d2 = build_dask_array(""d2"")\n        c2 = build_dask_array(""c2"")\n        d3 = build_dask_array(""d3"")\n        c3 = build_dask_array(""c3"")\n        # Note: c is a non-index coord.\n        # Index coords are loaded by IndexVariable.__init__.\n        ds1 = Dataset(data_vars={""d"": (""x"", d1)}, coords={""c"": (""x"", c1)})\n        ds2 = Dataset(data_vars={""d"": (""x"", d2)}, coords={""c"": (""x"", c2)})\n        ds3 = Dataset(data_vars={""d"": (""x"", d3)}, coords={""c"": (""x"", c3)})\n\n        assert kernel_call_count == 0\n        out = xr.concat(\n            [ds1, ds2, ds3], dim=""n"", data_vars=""different"", coords=""different""\n        )\n        # each kernel is computed exactly once\n        assert kernel_call_count == 6\n        # variables are loaded in the output\n        assert isinstance(out[""d""].data, np.ndarray)\n        assert isinstance(out[""c""].data, np.ndarray)\n\n        out = xr.concat([ds1, ds2, ds3], dim=""n"", data_vars=""all"", coords=""all"")\n        # no extra kernel calls\n        assert kernel_call_count == 6\n        assert isinstance(out[""d""].data, dask.array.Array)\n        assert isinstance(out[""c""].data, dask.array.Array)\n\n        out = xr.concat([ds1, ds2, ds3], dim=""n"", data_vars=[""d""], coords=[""c""])\n        # no extra kernel calls\n        assert kernel_call_count == 6\n        assert isinstance(out[""d""].data, dask.array.Array)\n        assert isinstance(out[""c""].data, dask.array.Array)\n\n        out = xr.concat([ds1, ds2, ds3], dim=""n"", data_vars=[], coords=[])\n        # variables are loaded once as we are validing that they\'re identical\n        assert kernel_call_count == 12\n        assert isinstance(out[""d""].data, np.ndarray)\n        assert isinstance(out[""c""].data, np.ndarray)\n\n        out = xr.concat(\n            [ds1, ds2, ds3],\n            dim=""n"",\n            data_vars=""different"",\n            coords=""different"",\n            compat=""identical"",\n        )\n        # compat=identical doesn\'t do any more kernel calls than compat=equals\n        assert kernel_call_count == 18\n        assert isinstance(out[""d""].data, np.ndarray)\n        assert isinstance(out[""c""].data, np.ndarray)\n\n        # When the test for different turns true halfway through,\n        # stop computing variables as it would not have any benefit\n        ds4 = Dataset(data_vars={""d"": (""x"", [2.0])}, coords={""c"": (""x"", [2.0])})\n        out = xr.concat(\n            [ds1, ds2, ds4, ds3], dim=""n"", data_vars=""different"", coords=""different""\n        )\n        # the variables of ds1 and ds2 were computed, but those of ds3 didn\'t\n        assert kernel_call_count == 22\n        assert isinstance(out[""d""].data, dask.array.Array)\n        assert isinstance(out[""c""].data, dask.array.Array)\n        # the data of ds1 and ds2 was loaded into numpy and then\n        # concatenated to the data of ds3. Thus, only ds3 is computed now.\n        out.compute()\n        assert kernel_call_count == 24\n\n        # Finally, test that originals are unaltered\n        assert ds1[""d""].data is d1\n        assert ds1[""c""].data is c1\n        assert ds2[""d""].data is d2\n        assert ds2[""c""].data is c2\n        assert ds3[""d""].data is d3\n        assert ds3[""c""].data is c3\n\n        # now check that concat() is correctly using dask name equality to skip loads\n        out = xr.concat(\n            [ds1, ds1, ds1], dim=""n"", data_vars=""different"", coords=""different""\n        )\n        assert kernel_call_count == 24\n        # variables are not loaded in the output\n        assert isinstance(out[""d""].data, dask.array.Array)\n        assert isinstance(out[""c""].data, dask.array.Array)\n\n        out = xr.concat(\n            [ds1, ds1, ds1], dim=""n"", data_vars=[], coords=[], compat=""identical""\n        )\n        assert kernel_call_count == 24\n        # variables are not loaded in the output\n        assert isinstance(out[""d""].data, dask.array.Array)\n        assert isinstance(out[""c""].data, dask.array.Array)\n\n        out = xr.concat(\n            [ds1, ds2.compute(), ds3],\n            dim=""n"",\n            data_vars=""all"",\n            coords=""different"",\n            compat=""identical"",\n        )\n        # c1,c3 must be computed for comparison since c2 is numpy;\n        # d2 is computed too\n        assert kernel_call_count == 28\n\n        out = xr.concat(\n            [ds1, ds2.compute(), ds3],\n            dim=""n"",\n            data_vars=""all"",\n            coords=""all"",\n            compat=""identical"",\n        )\n        # no extra computes\n        assert kernel_call_count == 30\n\n        # Finally, test that originals are unaltered\n        assert ds1[""d""].data is d1\n        assert ds1[""c""].data is c1\n        assert ds2[""d""].data is d2\n        assert ds2[""c""].data is c2\n        assert ds3[""d""].data is d3\n        assert ds3[""c""].data is c3\n\n    def test_groupby(self):\n        u = self.eager_array\n        v = self.lazy_array\n\n        expected = u.groupby(""x"").mean(...)\n        with raise_if_dask_computes():\n            actual = v.groupby(""x"").mean(...)\n        self.assertLazyAndAllClose(expected, actual)\n\n    def test_rolling(self):\n        u = self.eager_array\n        v = self.lazy_array\n\n        expected = u.rolling(x=2).mean()\n        with raise_if_dask_computes():\n            actual = v.rolling(x=2).mean()\n        self.assertLazyAndAllClose(expected, actual)\n\n    def test_groupby_first(self):\n        u = self.eager_array\n        v = self.lazy_array\n\n        for coords in [u.coords, v.coords]:\n            coords[""ab""] = (""x"", [""a"", ""a"", ""b"", ""b""])\n        with raises_regex(NotImplementedError, ""dask""):\n            v.groupby(""ab"").first()\n        expected = u.groupby(""ab"").first()\n        with raise_if_dask_computes():\n            actual = v.groupby(""ab"").first(skipna=False)\n        self.assertLazyAndAllClose(expected, actual)\n\n    def test_reindex(self):\n        u = self.eager_array.assign_coords(y=range(6))\n        v = self.lazy_array.assign_coords(y=range(6))\n\n        for kwargs in [\n            {""x"": [2, 3, 4]},\n            {""x"": [1, 100, 2, 101, 3]},\n            {""x"": [2.5, 3, 3.5], ""y"": [2, 2.5, 3]},\n        ]:\n            expected = u.reindex(**kwargs)\n            actual = v.reindex(**kwargs)\n            self.assertLazyAndAllClose(expected, actual)\n\n    def test_to_dataset_roundtrip(self):\n        u = self.eager_array\n        v = self.lazy_array\n\n        expected = u.assign_coords(x=u[""x""])\n        self.assertLazyAndEqual(expected, v.to_dataset(""x"").to_array(""x""))\n\n    def test_merge(self):\n        def duplicate_and_merge(array):\n            return xr.merge([array, array.rename(""bar"")]).to_array()\n\n        expected = duplicate_and_merge(self.eager_array)\n        actual = duplicate_and_merge(self.lazy_array)\n        self.assertLazyAndEqual(expected, actual)\n\n    @pytest.mark.filterwarnings(""ignore::PendingDeprecationWarning"")\n    def test_ufuncs(self):\n        u = self.eager_array\n        v = self.lazy_array\n        self.assertLazyAndAllClose(np.sin(u), xu.sin(v))\n\n    def test_where_dispatching(self):\n        a = np.arange(10)\n        b = a > 3\n        x = da.from_array(a, 5)\n        y = da.from_array(b, 5)\n        expected = DataArray(a).where(b)\n        self.assertLazyAndEqual(expected, DataArray(a).where(y))\n        self.assertLazyAndEqual(expected, DataArray(x).where(b))\n        self.assertLazyAndEqual(expected, DataArray(x).where(y))\n\n    def test_simultaneous_compute(self):\n        ds = Dataset({""foo"": (""x"", range(5)), ""bar"": (""x"", range(5))}).chunk()\n\n        count = [0]\n\n        def counting_get(*args, **kwargs):\n            count[0] += 1\n            return dask.get(*args, **kwargs)\n\n        ds.load(scheduler=counting_get)\n\n        assert count[0] == 1\n\n    def test_stack(self):\n        data = da.random.normal(size=(2, 3, 4), chunks=(1, 3, 4))\n        arr = DataArray(data, dims=(""w"", ""x"", ""y""))\n        stacked = arr.stack(z=(""x"", ""y""))\n        z = pd.MultiIndex.from_product([np.arange(3), np.arange(4)], names=[""x"", ""y""])\n        expected = DataArray(data.reshape(2, -1), {""z"": z}, dims=[""w"", ""z""])\n        assert stacked.data.chunks == expected.data.chunks\n        self.assertLazyAndEqual(expected, stacked)\n\n    def test_dot(self):\n        eager = self.eager_array.dot(self.eager_array[0])\n        lazy = self.lazy_array.dot(self.lazy_array[0])\n        self.assertLazyAndAllClose(eager, lazy)\n\n    @pytest.mark.skipif(LooseVersion(dask.__version__) >= ""2.0"", reason=""no meta"")\n    def test_dataarray_repr_legacy(self):\n        data = build_dask_array(""data"")\n        nonindex_coord = build_dask_array(""coord"")\n        a = DataArray(data, dims=[""x""], coords={""y"": (""x"", nonindex_coord)})\n        expected = dedent(\n            """"""\\\n            <xarray.DataArray \'data\' (x: 1)>\n            {!r}\n            Coordinates:\n                y        (x) int64 dask.array<chunksize=(1,), meta=np.ndarray>\n            Dimensions without coordinates: x"""""".format(\n                data\n            )\n        )\n        assert expected == repr(a)\n        assert kernel_call_count == 0  # should not evaluate dask array\n\n    @pytest.mark.skipif(LooseVersion(dask.__version__) < ""2.0"", reason=""needs meta"")\n    def test_dataarray_repr(self):\n        data = build_dask_array(""data"")\n        nonindex_coord = build_dask_array(""coord"")\n        a = DataArray(data, dims=[""x""], coords={""y"": (""x"", nonindex_coord)})\n        expected = dedent(\n            """"""\\\n            <xarray.DataArray \'data\' (x: 1)>\n            {!r}\n            Coordinates:\n                y        (x) int64 dask.array<chunksize=(1,), meta=np.ndarray>\n            Dimensions without coordinates: x"""""".format(\n                data\n            )\n        )\n        assert expected == repr(a)\n        assert kernel_call_count == 0  # should not evaluate dask array\n\n    @pytest.mark.skipif(LooseVersion(dask.__version__) < ""2.0"", reason=""needs meta"")\n    def test_dataset_repr(self):\n        data = build_dask_array(""data"")\n        nonindex_coord = build_dask_array(""coord"")\n        ds = Dataset(data_vars={""a"": (""x"", data)}, coords={""y"": (""x"", nonindex_coord)})\n        expected = dedent(\n            """"""\\\n            <xarray.Dataset>\n            Dimensions:  (x: 1)\n            Coordinates:\n                y        (x) int64 dask.array<chunksize=(1,), meta=np.ndarray>\n            Dimensions without coordinates: x\n            Data variables:\n                a        (x) int64 dask.array<chunksize=(1,), meta=np.ndarray>""""""\n        )\n        assert expected == repr(ds)\n        assert kernel_call_count == 0  # should not evaluate dask array\n\n    def test_dataarray_pickle(self):\n        # Test that pickling/unpickling converts the dask backend\n        # to numpy in neither the data variable nor the non-index coords\n        data = build_dask_array(""data"")\n        nonindex_coord = build_dask_array(""coord"")\n        a1 = DataArray(data, dims=[""x""], coords={""y"": (""x"", nonindex_coord)})\n        a1.compute()\n        assert not a1._in_memory\n        assert not a1.coords[""y""]._in_memory\n        assert kernel_call_count == 2\n        a2 = pickle.loads(pickle.dumps(a1))\n        assert kernel_call_count == 2\n        assert_identical(a1, a2)\n        assert not a1._in_memory\n        assert not a2._in_memory\n        assert not a1.coords[""y""]._in_memory\n        assert not a2.coords[""y""]._in_memory\n\n    def test_dataset_pickle(self):\n        # Test that pickling/unpickling converts the dask backend\n        # to numpy in neither the data variables nor the non-index coords\n        data = build_dask_array(""data"")\n        nonindex_coord = build_dask_array(""coord"")\n        ds1 = Dataset(data_vars={""a"": (""x"", data)}, coords={""y"": (""x"", nonindex_coord)})\n        ds1.compute()\n        assert not ds1[""a""]._in_memory\n        assert not ds1[""y""]._in_memory\n        assert kernel_call_count == 2\n        ds2 = pickle.loads(pickle.dumps(ds1))\n        assert kernel_call_count == 2\n        assert_identical(ds1, ds2)\n        assert not ds1[""a""]._in_memory\n        assert not ds2[""a""]._in_memory\n        assert not ds1[""y""]._in_memory\n        assert not ds2[""y""]._in_memory\n\n    def test_dataarray_getattr(self):\n        # ipython/jupyter does a long list of getattr() calls to when trying to\n        # represent an object.\n        # Make sure we\'re not accidentally computing dask variables.\n        data = build_dask_array(""data"")\n        nonindex_coord = build_dask_array(""coord"")\n        a = DataArray(data, dims=[""x""], coords={""y"": (""x"", nonindex_coord)})\n        with suppress(AttributeError):\n            getattr(a, ""NOTEXIST"")\n        assert kernel_call_count == 0\n\n    def test_dataset_getattr(self):\n        # Test that pickling/unpickling converts the dask backend\n        # to numpy in neither the data variables nor the non-index coords\n        data = build_dask_array(""data"")\n        nonindex_coord = build_dask_array(""coord"")\n        ds = Dataset(data_vars={""a"": (""x"", data)}, coords={""y"": (""x"", nonindex_coord)})\n        with suppress(AttributeError):\n            getattr(ds, ""NOTEXIST"")\n        assert kernel_call_count == 0\n\n    def test_values(self):\n        # Test that invoking the values property does not convert the dask\n        # backend to numpy\n        a = DataArray([1, 2]).chunk()\n        assert not a._in_memory\n        assert a.values.tolist() == [1, 2]\n        assert not a._in_memory\n\n    def test_from_dask_variable(self):\n        # Test array creation from Variable with dask backend.\n        # This is used e.g. in broadcast()\n        a = DataArray(self.lazy_array.variable, coords={""x"": range(4)}, name=""foo"")\n        self.assertLazyAndIdentical(self.lazy_array, a)\n\n\nclass TestToDaskDataFrame:\n    def test_to_dask_dataframe(self):\n        # Test conversion of Datasets to dask DataFrames\n        x = da.from_array(np.random.randn(10), chunks=4)\n        y = np.arange(10, dtype=""uint8"")\n        t = list(""abcdefghij"")\n\n        ds = Dataset({""a"": (""t"", x), ""b"": (""t"", y), ""t"": (""t"", t)})\n\n        expected_pd = pd.DataFrame({""a"": x, ""b"": y}, index=pd.Index(t, name=""t""))\n\n        # test if 1-D index is correctly set up\n        expected = dd.from_pandas(expected_pd, chunksize=4)\n        actual = ds.to_dask_dataframe(set_index=True)\n        # test if we have dask dataframes\n        assert isinstance(actual, dd.DataFrame)\n\n        # use the .equals from pandas to check dataframes are equivalent\n        assert_frame_equal(expected.compute(), actual.compute())\n\n        # test if no index is given\n        expected = dd.from_pandas(expected_pd.reset_index(drop=False), chunksize=4)\n\n        actual = ds.to_dask_dataframe(set_index=False)\n\n        assert isinstance(actual, dd.DataFrame)\n        assert_frame_equal(expected.compute(), actual.compute())\n\n    def test_to_dask_dataframe_2D(self):\n        # Test if 2-D dataset is supplied\n        w = da.from_array(np.random.randn(2, 3), chunks=(1, 2))\n        ds = Dataset({""w"": ((""x"", ""y""), w)})\n        ds[""x""] = (""x"", np.array([0, 1], np.int64))\n        ds[""y""] = (""y"", list(""abc""))\n\n        # dask dataframes do not (yet) support multiindex,\n        # but when it does, this would be the expected index:\n        exp_index = pd.MultiIndex.from_arrays(\n            [[0, 0, 0, 1, 1, 1], [""a"", ""b"", ""c"", ""a"", ""b"", ""c""]], names=[""x"", ""y""]\n        )\n        expected = pd.DataFrame({""w"": w.reshape(-1)}, index=exp_index)\n        # so for now, reset the index\n        expected = expected.reset_index(drop=False)\n        actual = ds.to_dask_dataframe(set_index=False)\n\n        assert isinstance(actual, dd.DataFrame)\n        assert_frame_equal(expected, actual.compute())\n\n    @pytest.mark.xfail(raises=NotImplementedError)\n    def test_to_dask_dataframe_2D_set_index(self):\n        # This will fail until dask implements MultiIndex support\n        w = da.from_array(np.random.randn(2, 3), chunks=(1, 2))\n        ds = Dataset({""w"": ((""x"", ""y""), w)})\n        ds[""x""] = (""x"", np.array([0, 1], np.int64))\n        ds[""y""] = (""y"", list(""abc""))\n\n        expected = ds.compute().to_dataframe()\n        actual = ds.to_dask_dataframe(set_index=True)\n        assert isinstance(actual, dd.DataFrame)\n        assert_frame_equal(expected, actual.compute())\n\n    def test_to_dask_dataframe_coordinates(self):\n        # Test if coordinate is also a dask array\n        x = da.from_array(np.random.randn(10), chunks=4)\n        t = da.from_array(np.arange(10) * 2, chunks=4)\n\n        ds = Dataset({""a"": (""t"", x), ""t"": (""t"", t)})\n\n        expected_pd = pd.DataFrame({""a"": x}, index=pd.Index(t, name=""t""))\n        expected = dd.from_pandas(expected_pd, chunksize=4)\n        actual = ds.to_dask_dataframe(set_index=True)\n        assert isinstance(actual, dd.DataFrame)\n        assert_frame_equal(expected.compute(), actual.compute())\n\n    def test_to_dask_dataframe_not_daskarray(self):\n        # Test if DataArray is not a dask array\n        x = np.random.randn(10)\n        y = np.arange(10, dtype=""uint8"")\n        t = list(""abcdefghij"")\n\n        ds = Dataset({""a"": (""t"", x), ""b"": (""t"", y), ""t"": (""t"", t)})\n\n        expected = pd.DataFrame({""a"": x, ""b"": y}, index=pd.Index(t, name=""t""))\n\n        actual = ds.to_dask_dataframe(set_index=True)\n        assert isinstance(actual, dd.DataFrame)\n        assert_frame_equal(expected, actual.compute())\n\n    def test_to_dask_dataframe_no_coordinate(self):\n        x = da.from_array(np.random.randn(10), chunks=4)\n        ds = Dataset({""x"": (""dim_0"", x)})\n\n        expected = ds.compute().to_dataframe().reset_index()\n        actual = ds.to_dask_dataframe()\n        assert isinstance(actual, dd.DataFrame)\n        assert_frame_equal(expected, actual.compute())\n\n        expected = ds.compute().to_dataframe()\n        actual = ds.to_dask_dataframe(set_index=True)\n        assert isinstance(actual, dd.DataFrame)\n        assert_frame_equal(expected, actual.compute())\n\n    def test_to_dask_dataframe_dim_order(self):\n        values = np.array([[1, 2], [3, 4]], dtype=np.int64)\n        ds = Dataset({""w"": ((""x"", ""y""), values)}).chunk(1)\n\n        expected = ds[""w""].to_series().reset_index()\n        actual = ds.to_dask_dataframe(dim_order=[""x"", ""y""])\n        assert isinstance(actual, dd.DataFrame)\n        assert_frame_equal(expected, actual.compute())\n\n        expected = ds[""w""].T.to_series().reset_index()\n        actual = ds.to_dask_dataframe(dim_order=[""y"", ""x""])\n        assert isinstance(actual, dd.DataFrame)\n        assert_frame_equal(expected, actual.compute())\n\n        with raises_regex(ValueError, ""does not match the set of dimensions""):\n            ds.to_dask_dataframe(dim_order=[""x""])\n\n\n@pytest.mark.parametrize(""method"", [""load"", ""compute""])\ndef test_dask_kwargs_variable(method):\n    x = Variable(""y"", da.from_array(np.arange(3), chunks=(2,)))\n    # args should be passed on to da.Array.compute()\n    with mock.patch.object(\n        da.Array, ""compute"", return_value=np.arange(3)\n    ) as mock_compute:\n        getattr(x, method)(foo=""bar"")\n    mock_compute.assert_called_with(foo=""bar"")\n\n\n@pytest.mark.parametrize(""method"", [""load"", ""compute"", ""persist""])\ndef test_dask_kwargs_dataarray(method):\n    data = da.from_array(np.arange(3), chunks=(2,))\n    x = DataArray(data)\n    if method in [""load"", ""compute""]:\n        dask_func = ""dask.array.compute""\n    else:\n        dask_func = ""dask.persist""\n    # args should be passed on to ""dask_func""\n    with mock.patch(dask_func) as mock_func:\n        getattr(x, method)(foo=""bar"")\n    mock_func.assert_called_with(data, foo=""bar"")\n\n\n@pytest.mark.parametrize(""method"", [""load"", ""compute"", ""persist""])\ndef test_dask_kwargs_dataset(method):\n    data = da.from_array(np.arange(3), chunks=(2,))\n    x = Dataset({""x"": ((""y""), data)})\n    if method in [""load"", ""compute""]:\n        dask_func = ""dask.array.compute""\n    else:\n        dask_func = ""dask.persist""\n    # args should be passed on to ""dask_func""\n    with mock.patch(dask_func) as mock_func:\n        getattr(x, method)(foo=""bar"")\n    mock_func.assert_called_with(data, foo=""bar"")\n\n\nkernel_call_count = 0\n\n\ndef kernel(name):\n    """"""Dask kernel to test pickling/unpickling and __repr__.\n    Must be global to make it pickleable.\n    """"""\n    global kernel_call_count\n    kernel_call_count += 1\n    return np.ones(1, dtype=np.int64)\n\n\ndef build_dask_array(name):\n    global kernel_call_count\n    kernel_call_count = 0\n    return dask.array.Array(\n        dask={(name, 0): (kernel, name)}, name=name, chunks=((1,),), dtype=np.int64\n    )\n\n\n@pytest.mark.parametrize(\n    ""persist"", [lambda x: x.persist(), lambda x: dask.persist(x)[0]]\n)\ndef test_persist_Dataset(persist):\n    ds = Dataset({""foo"": (""x"", range(5)), ""bar"": (""x"", range(5))}).chunk()\n    ds = ds + 1\n    n = len(ds.foo.data.dask)\n\n    ds2 = persist(ds)\n\n    assert len(ds2.foo.data.dask) == 1\n    assert len(ds.foo.data.dask) == n  # doesn\'t mutate in place\n\n\n@pytest.mark.parametrize(\n    ""persist"", [lambda x: x.persist(), lambda x: dask.persist(x)[0]]\n)\ndef test_persist_DataArray(persist):\n    x = da.arange(10, chunks=(5,))\n    y = DataArray(x)\n    z = y + 1\n    n = len(z.data.dask)\n\n    zz = persist(z)\n\n    assert len(z.data.dask) == n\n    assert len(zz.data.dask) == zz.data.npartitions\n\n\ndef test_dataarray_with_dask_coords():\n    import toolz\n\n    x = xr.Variable(""x"", da.arange(8, chunks=(4,)))\n    y = xr.Variable(""y"", da.arange(8, chunks=(4,)) * 2)\n    data = da.random.random((8, 8), chunks=(4, 4)) + 1\n    array = xr.DataArray(data, dims=[""x"", ""y""])\n    array.coords[""xx""] = x\n    array.coords[""yy""] = y\n\n    assert dict(array.__dask_graph__()) == toolz.merge(\n        data.__dask_graph__(), x.__dask_graph__(), y.__dask_graph__()\n    )\n\n    (array2,) = dask.compute(array)\n    assert not dask.is_dask_collection(array2)\n\n    assert all(isinstance(v._variable.data, np.ndarray) for v in array2.coords.values())\n\n\ndef test_basic_compute():\n    ds = Dataset({""foo"": (""x"", range(5)), ""bar"": (""x"", range(5))}).chunk({""x"": 2})\n    for get in [dask.threaded.get, dask.multiprocessing.get, dask.local.get_sync, None]:\n        with dask.config.set(scheduler=get):\n            ds.compute()\n            ds.foo.compute()\n            ds.foo.variable.compute()\n\n\ndef test_dask_layers_and_dependencies():\n    ds = Dataset({""foo"": (""x"", range(5)), ""bar"": (""x"", range(5))}).chunk()\n\n    x = dask.delayed(ds)\n    assert set(x.__dask_graph__().dependencies).issuperset(\n        ds.__dask_graph__().dependencies\n    )\n    assert set(x.foo.__dask_graph__().dependencies).issuperset(\n        ds.__dask_graph__().dependencies\n    )\n\n\ndef make_da():\n    da = xr.DataArray(\n        np.ones((10, 20)),\n        dims=[""x"", ""y""],\n        coords={""x"": np.arange(10), ""y"": np.arange(100, 120)},\n        name=""a"",\n    ).chunk({""x"": 4, ""y"": 5})\n    da.x.attrs[""long_name""] = ""x""\n    da.attrs[""test""] = ""test""\n    da.coords[""c2""] = 0.5\n    da.coords[""ndcoord""] = da.x * 2\n    da.coords[""cxy""] = (da.x * da.y).chunk({""x"": 4, ""y"": 5})\n\n    return da\n\n\ndef make_ds():\n    map_ds = xr.Dataset()\n    map_ds[""a""] = make_da()\n    map_ds[""b""] = map_ds.a + 50\n    map_ds[""c""] = map_ds.x + 20\n    map_ds = map_ds.chunk({""x"": 4, ""y"": 5})\n    map_ds[""d""] = (""z"", [1, 1, 1, 1])\n    map_ds[""z""] = [0, 1, 2, 3]\n    map_ds[""e""] = map_ds.x + map_ds.y\n    map_ds.coords[""c1""] = 0.5\n    map_ds.coords[""cx""] = (""x"", np.arange(len(map_ds.x)))\n    map_ds.coords[""cx""].attrs[""test2""] = ""test2""\n    map_ds.attrs[""test""] = ""test""\n    map_ds.coords[""xx""] = map_ds[""a""] * map_ds.y\n\n    map_ds.x.attrs[""long_name""] = ""x""\n    map_ds.y.attrs[""long_name""] = ""y""\n\n    return map_ds\n\n\n# fixtures cannot be used in parametrize statements\n# instead use this workaround\n# https://docs.pytest.org/en/latest/deprecations.html#calling-fixtures-directly\n@pytest.fixture\ndef map_da():\n    return make_da()\n\n\n@pytest.fixture\ndef map_ds():\n    return make_ds()\n\n\ndef test_unify_chunks(map_ds):\n    ds_copy = map_ds.copy()\n    ds_copy[""cxy""] = ds_copy.cxy.chunk({""y"": 10})\n\n    with raises_regex(ValueError, ""inconsistent chunks""):\n        ds_copy.chunks\n\n    expected_chunks = {""x"": (4, 4, 2), ""y"": (5, 5, 5, 5), ""z"": (4,)}\n    with raise_if_dask_computes():\n        actual_chunks = ds_copy.unify_chunks().chunks\n    expected_chunks == actual_chunks\n    assert_identical(map_ds, ds_copy.unify_chunks())\n\n\n@pytest.mark.parametrize(""obj"", [make_ds(), make_da()])\n@pytest.mark.parametrize(\n    ""transform"", [lambda x: x.compute(), lambda x: x.unify_chunks()]\n)\ndef test_unify_chunks_shallow_copy(obj, transform):\n    obj = transform(obj)\n    unified = obj.unify_chunks()\n    assert_identical(obj, unified) and obj is not obj.unify_chunks()\n\n\n@pytest.mark.parametrize(""obj"", [make_da()])\ndef test_auto_chunk_da(obj):\n    actual = obj.chunk(""auto"").data\n    expected = obj.data.rechunk(""auto"")\n    np.testing.assert_array_equal(actual, expected)\n    assert actual.chunks == expected.chunks\n\n\ndef test_map_blocks_error(map_da, map_ds):\n    def bad_func(darray):\n        return (darray * darray.x + 5 * darray.y)[:1, :1]\n\n    with raises_regex(ValueError, ""Received dimension \'x\' of length 1""):\n        xr.map_blocks(bad_func, map_da).compute()\n\n    def returns_numpy(darray):\n        return (darray * darray.x + 5 * darray.y).values\n\n    with raises_regex(TypeError, ""Function must return an xarray DataArray""):\n        xr.map_blocks(returns_numpy, map_da)\n\n    with raises_regex(TypeError, ""args must be""):\n        xr.map_blocks(operator.add, map_da, args=10)\n\n    with raises_regex(TypeError, ""kwargs must be""):\n        xr.map_blocks(operator.add, map_da, args=[10], kwargs=[20])\n\n    def really_bad_func(darray):\n        raise ValueError(""couldn\'t do anything."")\n\n    with raises_regex(Exception, ""Cannot infer""):\n        xr.map_blocks(really_bad_func, map_da)\n\n    ds_copy = map_ds.copy()\n    ds_copy[""cxy""] = ds_copy.cxy.chunk({""y"": 10})\n\n    with raises_regex(ValueError, ""inconsistent chunks""):\n        xr.map_blocks(bad_func, ds_copy)\n\n    with raises_regex(TypeError, ""Cannot pass dask collections""):\n        xr.map_blocks(bad_func, map_da, kwargs=dict(a=map_da.chunk()))\n\n\n@pytest.mark.parametrize(""obj"", [make_da(), make_ds()])\ndef test_map_blocks(obj):\n    def func(obj):\n        result = obj + obj.x + 5 * obj.y\n        return result\n\n    with raise_if_dask_computes():\n        actual = xr.map_blocks(func, obj)\n    expected = func(obj)\n    assert_chunks_equal(expected.chunk(), actual)\n    assert_identical(actual, expected)\n\n\n@pytest.mark.parametrize(""obj"", [make_da(), make_ds()])\ndef test_map_blocks_convert_args_to_list(obj):\n    expected = obj + 10\n    with raise_if_dask_computes():\n        actual = xr.map_blocks(operator.add, obj, [10])\n    assert_chunks_equal(expected.chunk(), actual)\n    assert_identical(actual, expected)\n\n\ndef test_map_blocks_dask_args():\n    da1 = xr.DataArray(\n        np.ones((10, 20)),\n        dims=[""x"", ""y""],\n        coords={""x"": np.arange(10), ""y"": np.arange(20)},\n    ).chunk({""x"": 5, ""y"": 4})\n\n    # check that block shapes are the same\n    def sumda(da1, da2):\n        assert da1.shape == da2.shape\n        return da1 + da2\n\n    da2 = da1 + 1\n    with raise_if_dask_computes():\n        mapped = xr.map_blocks(sumda, da1, args=[da2])\n    xr.testing.assert_equal(da1 + da2, mapped)\n\n    # one dimension in common\n    da2 = (da1 + 1).isel(x=1, drop=True)\n    with raise_if_dask_computes():\n        mapped = xr.map_blocks(operator.add, da1, args=[da2])\n    xr.testing.assert_equal(da1 + da2, mapped)\n\n    # test that everything works when dimension names are different\n    da2 = (da1 + 1).isel(x=1, drop=True).rename({""y"": ""k""})\n    with raise_if_dask_computes():\n        mapped = xr.map_blocks(operator.add, da1, args=[da2])\n    xr.testing.assert_equal(da1 + da2, mapped)\n\n    with raises_regex(ValueError, ""Chunk sizes along dimension \'x\'""):\n        xr.map_blocks(operator.add, da1, args=[da1.chunk({""x"": 1})])\n\n    with raises_regex(ValueError, ""indexes along dimension \'x\' are not equal""):\n        xr.map_blocks(operator.add, da1, args=[da1.reindex(x=np.arange(20))])\n\n    # reduction\n    da1 = da1.chunk({""x"": -1})\n    da2 = da1 + 1\n    with raise_if_dask_computes():\n        mapped = xr.map_blocks(lambda a, b: (a + b).sum(""x""), da1, args=[da2])\n    xr.testing.assert_equal((da1 + da2).sum(""x""), mapped)\n\n    # reduction with template\n    da1 = da1.chunk({""x"": -1})\n    da2 = da1 + 1\n    with raise_if_dask_computes():\n        mapped = xr.map_blocks(\n            lambda a, b: (a + b).sum(""x""), da1, args=[da2], template=da1.sum(""x"")\n        )\n    xr.testing.assert_equal((da1 + da2).sum(""x""), mapped)\n\n\n@pytest.mark.parametrize(""obj"", [make_da(), make_ds()])\ndef test_map_blocks_add_attrs(obj):\n    def add_attrs(obj):\n        obj = obj.copy(deep=True)\n        obj.attrs[""new""] = ""new""\n        obj.cxy.attrs[""new2""] = ""new2""\n        return obj\n\n    expected = add_attrs(obj)\n    with raise_if_dask_computes():\n        actual = xr.map_blocks(add_attrs, obj)\n\n    assert_identical(actual, expected)\n\n    # when template is specified, attrs are copied from template, not set by function\n    with raise_if_dask_computes():\n        actual = xr.map_blocks(add_attrs, obj, template=obj)\n    assert_identical(actual, obj)\n\n\ndef test_map_blocks_change_name(map_da):\n    def change_name(obj):\n        obj = obj.copy(deep=True)\n        obj.name = ""new""\n        return obj\n\n    expected = change_name(map_da)\n    with raise_if_dask_computes():\n        actual = xr.map_blocks(change_name, map_da)\n\n    assert_identical(actual, expected)\n\n\n@pytest.mark.parametrize(""obj"", [make_da(), make_ds()])\ndef test_map_blocks_kwargs(obj):\n    expected = xr.full_like(obj, fill_value=np.nan)\n    with raise_if_dask_computes():\n        actual = xr.map_blocks(xr.full_like, obj, kwargs=dict(fill_value=np.nan))\n    assert_chunks_equal(expected.chunk(), actual)\n    assert_identical(actual, expected)\n\n\ndef test_map_blocks_to_array(map_ds):\n    with raise_if_dask_computes():\n        actual = xr.map_blocks(lambda x: x.to_array(), map_ds)\n\n    # to_array does not preserve name, so cannot use assert_identical\n    assert_equal(actual, map_ds.to_array())\n\n\n@pytest.mark.parametrize(\n    ""func"",\n    [\n        lambda x: x,\n        lambda x: x.to_dataset(),\n        lambda x: x.drop_vars(""x""),\n        lambda x: x.expand_dims(k=[1, 2, 3]),\n        lambda x: x.expand_dims(k=3),\n        lambda x: x.assign_coords(new_coord=(""y"", x.y * 2)),\n        lambda x: x.astype(np.int32),\n        lambda x: x.x,\n    ],\n)\ndef test_map_blocks_da_transformations(func, map_da):\n    with raise_if_dask_computes():\n        actual = xr.map_blocks(func, map_da)\n\n    assert_identical(actual, func(map_da))\n\n\n@pytest.mark.parametrize(\n    ""func"",\n    [\n        lambda x: x,\n        lambda x: x.drop_vars(""cxy""),\n        lambda x: x.drop_vars(""a""),\n        lambda x: x.drop_vars(""x""),\n        lambda x: x.expand_dims(k=[1, 2, 3]),\n        lambda x: x.expand_dims(k=3),\n        lambda x: x.rename({""a"": ""new1"", ""b"": ""new2""}),\n        lambda x: x.x,\n    ],\n)\ndef test_map_blocks_ds_transformations(func, map_ds):\n    with raise_if_dask_computes():\n        actual = xr.map_blocks(func, map_ds)\n\n    assert_identical(actual, func(map_ds))\n\n\n@pytest.mark.parametrize(""obj"", [make_da(), make_ds()])\ndef test_map_blocks_da_ds_with_template(obj):\n    func = lambda x: x.isel(x=[1])\n    template = obj.isel(x=[1, 5, 9])\n    with raise_if_dask_computes():\n        actual = xr.map_blocks(func, obj, template=template)\n    assert_identical(actual, template)\n\n    with raise_if_dask_computes():\n        actual = obj.map_blocks(func, template=template)\n    assert_identical(actual, template)\n\n\ndef test_map_blocks_template_convert_object():\n    da = make_da()\n    func = lambda x: x.to_dataset().isel(x=[1])\n    template = da.to_dataset().isel(x=[1, 5, 9])\n    with raise_if_dask_computes():\n        actual = xr.map_blocks(func, da, template=template)\n    assert_identical(actual, template)\n\n    ds = da.to_dataset()\n    func = lambda x: x.to_array().isel(x=[1])\n    template = ds.to_array().isel(x=[1, 5, 9])\n    with raise_if_dask_computes():\n        actual = xr.map_blocks(func, ds, template=template)\n    assert_identical(actual, template)\n\n\n@pytest.mark.parametrize(""obj"", [make_da(), make_ds()])\ndef test_map_blocks_errors_bad_template(obj):\n    with raises_regex(ValueError, ""unexpected coordinate variables""):\n        xr.map_blocks(lambda x: x.assign_coords(a=10), obj, template=obj).compute()\n    with raises_regex(ValueError, ""does not contain coordinate variables""):\n        xr.map_blocks(lambda x: x.drop_vars(""cxy""), obj, template=obj).compute()\n    with raises_regex(ValueError, ""Dimensions {\'x\'} missing""):\n        xr.map_blocks(lambda x: x.isel(x=1), obj, template=obj).compute()\n    with raises_regex(ValueError, ""Received dimension \'x\' of length 1""):\n        xr.map_blocks(lambda x: x.isel(x=[1]), obj, template=obj).compute()\n    with raises_regex(TypeError, ""must be a DataArray""):\n        xr.map_blocks(lambda x: x.isel(x=[1]), obj, template=(obj,)).compute()\n    with raises_regex(ValueError, ""map_blocks requires that one block""):\n        xr.map_blocks(\n            lambda x: x.isel(x=[1]).assign_coords(x=10), obj, template=obj.isel(x=[1])\n        ).compute()\n    with raises_regex(ValueError, ""Expected index \'x\' to be""):\n        xr.map_blocks(\n            lambda a: a.isel(x=[1]).assign_coords(x=[120]),  # assign bad index values\n            obj,\n            template=obj.isel(x=[1, 5, 9]),\n        ).compute()\n\n\ndef test_map_blocks_errors_bad_template_2(map_ds):\n    with raises_regex(ValueError, ""unexpected data variables {\'xyz\'}""):\n        xr.map_blocks(lambda x: x.assign(xyz=1), map_ds, template=map_ds).compute()\n\n\n@pytest.mark.parametrize(""obj"", [make_da(), make_ds()])\ndef test_map_blocks_object_method(obj):\n    def func(obj):\n        result = obj + obj.x + 5 * obj.y\n        return result\n\n    with raise_if_dask_computes():\n        expected = xr.map_blocks(func, obj)\n        actual = obj.map_blocks(func)\n\n    assert_identical(expected, actual)\n\n\ndef test_map_blocks_hlg_layers():\n    # regression test for #3599\n    ds = xr.Dataset(\n        {\n            ""x"": ((""a"",), dask.array.ones(10, chunks=(5,))),\n            ""z"": ((""b"",), dask.array.ones(10, chunks=(5,))),\n        }\n    )\n    mapped = ds.map_blocks(lambda x: x)\n\n    xr.testing.assert_equal(mapped, ds)\n\n\ndef test_make_meta(map_ds):\n    from ..core.parallel import make_meta\n\n    meta = make_meta(map_ds)\n\n    for variable in map_ds._coord_names:\n        assert variable in meta._coord_names\n        assert meta.coords[variable].shape == (0,) * meta.coords[variable].ndim\n\n    for variable in map_ds.data_vars:\n        assert variable in meta.data_vars\n        assert meta.data_vars[variable].shape == (0,) * meta.data_vars[variable].ndim\n\n\ndef test_identical_coords_no_computes():\n    lons2 = xr.DataArray(da.zeros((10, 10), chunks=2), dims=(""y"", ""x""))\n    a = xr.DataArray(\n        da.zeros((10, 10), chunks=2), dims=(""y"", ""x""), coords={""lons"": lons2}\n    )\n    b = xr.DataArray(\n        da.zeros((10, 10), chunks=2), dims=(""y"", ""x""), coords={""lons"": lons2}\n    )\n    with raise_if_dask_computes():\n        c = a + b\n    assert_identical(c, a)\n\n\n@pytest.mark.parametrize(\n    ""obj"", [make_da(), make_da().compute(), make_ds(), make_ds().compute()]\n)\n@pytest.mark.parametrize(\n    ""transform"",\n    [\n        lambda x: x.reset_coords(),\n        lambda x: x.reset_coords(drop=True),\n        lambda x: x.isel(x=1),\n        lambda x: x.attrs.update(new_attrs=1),\n        lambda x: x.assign_coords(cxy=1),\n        lambda x: x.rename({""x"": ""xnew""}),\n        lambda x: x.rename({""cxy"": ""cxynew""}),\n    ],\n)\ndef test_token_changes_on_transform(obj, transform):\n    with raise_if_dask_computes():\n        assert dask.base.tokenize(obj) != dask.base.tokenize(transform(obj))\n\n\n@pytest.mark.parametrize(\n    ""obj"", [make_da(), make_da().compute(), make_ds(), make_ds().compute()]\n)\ndef test_token_changes_when_data_changes(obj):\n    with raise_if_dask_computes():\n        t1 = dask.base.tokenize(obj)\n\n    # Change data_var\n    if isinstance(obj, DataArray):\n        obj *= 2\n    else:\n        obj[""a""] *= 2\n    with raise_if_dask_computes():\n        t2 = dask.base.tokenize(obj)\n    assert t2 != t1\n\n    # Change non-index coord\n    obj.coords[""ndcoord""] *= 2\n    with raise_if_dask_computes():\n        t3 = dask.base.tokenize(obj)\n    assert t3 != t2\n\n    # Change IndexVariable\n    obj = obj.assign_coords(x=obj.x * 2)\n    with raise_if_dask_computes():\n        t4 = dask.base.tokenize(obj)\n    assert t4 != t3\n\n\n@pytest.mark.parametrize(""obj"", [make_da().compute(), make_ds().compute()])\ndef test_token_changes_when_buffer_changes(obj):\n    with raise_if_dask_computes():\n        t1 = dask.base.tokenize(obj)\n\n    if isinstance(obj, DataArray):\n        obj[0, 0] = 123\n    else:\n        obj[""a""][0, 0] = 123\n    with raise_if_dask_computes():\n        t2 = dask.base.tokenize(obj)\n    assert t2 != t1\n\n    obj.coords[""ndcoord""][0] = 123\n    with raise_if_dask_computes():\n        t3 = dask.base.tokenize(obj)\n    assert t3 != t2\n\n\n@pytest.mark.parametrize(\n    ""transform"",\n    [lambda x: x, lambda x: x.copy(deep=False), lambda x: x.copy(deep=True)],\n)\n@pytest.mark.parametrize(""obj"", [make_da(), make_ds(), make_ds().variables[""a""]])\ndef test_token_identical(obj, transform):\n    with raise_if_dask_computes():\n        assert dask.base.tokenize(obj) == dask.base.tokenize(transform(obj))\n    assert dask.base.tokenize(obj.compute()) == dask.base.tokenize(\n        transform(obj.compute())\n    )\n\n\ndef test_recursive_token():\n    """"""Test that tokenization is invoked recursively, and doesn\'t just rely on the\n    output of str()\n    """"""\n    a = np.ones(10000)\n    b = np.ones(10000)\n    b[5000] = 2\n    assert str(a) == str(b)\n    assert dask.base.tokenize(a) != dask.base.tokenize(b)\n\n    # Test DataArray and Variable\n    da_a = DataArray(a)\n    da_b = DataArray(b)\n    assert dask.base.tokenize(da_a) != dask.base.tokenize(da_b)\n\n    # Test Dataset\n    ds_a = da_a.to_dataset(name=""x"")\n    ds_b = da_b.to_dataset(name=""x"")\n    assert dask.base.tokenize(ds_a) != dask.base.tokenize(ds_b)\n\n    # Test IndexVariable\n    da_a = DataArray(a, dims=[""x""], coords={""x"": a})\n    da_b = DataArray(a, dims=[""x""], coords={""x"": b})\n    assert dask.base.tokenize(da_a) != dask.base.tokenize(da_b)\n\n\n@requires_scipy_or_netCDF4\ndef test_normalize_token_with_backend(map_ds):\n    with create_tmp_file(allow_cleanup_failure=ON_WINDOWS) as tmp_file:\n        map_ds.to_netcdf(tmp_file)\n        read = xr.open_dataset(tmp_file)\n        assert not dask.base.tokenize(map_ds) == dask.base.tokenize(read)\n        read.close()\n\n\n@pytest.mark.parametrize(\n    ""compat"", [""broadcast_equals"", ""equals"", ""identical"", ""no_conflicts""]\n)\ndef test_lazy_array_equiv_variables(compat):\n    var1 = xr.Variable((""y"", ""x""), da.zeros((10, 10), chunks=2))\n    var2 = xr.Variable((""y"", ""x""), da.zeros((10, 10), chunks=2))\n    var3 = xr.Variable((""y"", ""x""), da.zeros((20, 10), chunks=2))\n\n    with raise_if_dask_computes():\n        assert getattr(var1, compat)(var2, equiv=lazy_array_equiv)\n    # values are actually equal, but we don\'t know that till we compute, return None\n    with raise_if_dask_computes():\n        assert getattr(var1, compat)(var2 / 2, equiv=lazy_array_equiv) is None\n\n    # shapes are not equal, return False without computes\n    with raise_if_dask_computes():\n        assert getattr(var1, compat)(var3, equiv=lazy_array_equiv) is False\n\n    # if one or both arrays are numpy, return None\n    assert getattr(var1, compat)(var2.compute(), equiv=lazy_array_equiv) is None\n    assert (\n        getattr(var1.compute(), compat)(var2.compute(), equiv=lazy_array_equiv) is None\n    )\n\n    with raise_if_dask_computes():\n        assert getattr(var1, compat)(var2.transpose(""y"", ""x""))\n\n\n@pytest.mark.parametrize(\n    ""compat"", [""broadcast_equals"", ""equals"", ""identical"", ""no_conflicts""]\n)\ndef test_lazy_array_equiv_merge(compat):\n    da1 = xr.DataArray(da.zeros((10, 10), chunks=2), dims=(""y"", ""x""))\n    da2 = xr.DataArray(da.zeros((10, 10), chunks=2), dims=(""y"", ""x""))\n    da3 = xr.DataArray(da.ones((20, 10), chunks=2), dims=(""y"", ""x""))\n\n    with raise_if_dask_computes():\n        xr.merge([da1, da2], compat=compat)\n    # shapes are not equal; no computes necessary\n    with raise_if_dask_computes(max_computes=0):\n        with pytest.raises(ValueError):\n            xr.merge([da1, da3], compat=compat)\n    with raise_if_dask_computes(max_computes=2):\n        xr.merge([da1, da2 / 2], compat=compat)\n\n\n@pytest.mark.filterwarnings(""ignore::FutureWarning"")  # transpose_coords\n@pytest.mark.parametrize(""obj"", [make_da(), make_ds()])\n@pytest.mark.parametrize(\n    ""transform"",\n    [\n        lambda a: a.assign_attrs(new_attr=""anew""),\n        lambda a: a.assign_coords(cxy=a.cxy),\n        lambda a: a.copy(),\n        lambda a: a.isel(x=np.arange(a.sizes[""x""])),\n        lambda a: a.isel(x=slice(None)),\n        lambda a: a.loc[dict(x=slice(None))],\n        lambda a: a.loc[dict(x=np.arange(a.sizes[""x""]))],\n        lambda a: a.loc[dict(x=a.x)],\n        lambda a: a.sel(x=a.x),\n        lambda a: a.sel(x=a.x.values),\n        lambda a: a.transpose(...),\n        lambda a: a.squeeze(),  # no dimensions to squeeze\n        lambda a: a.sortby(""x""),  # ""x"" is already sorted\n        lambda a: a.reindex(x=a.x),\n        lambda a: a.reindex_like(a),\n        lambda a: a.rename({""cxy"": ""cnew""}).rename({""cnew"": ""cxy""}),\n        lambda a: a.pipe(lambda x: x),\n        lambda a: xr.align(a, xr.zeros_like(a))[0],\n        # assign\n        # swap_dims\n        # set_index / reset_index\n    ],\n)\ndef test_transforms_pass_lazy_array_equiv(obj, transform):\n    with raise_if_dask_computes():\n        assert_equal(obj, transform(obj))\n\n\ndef test_more_transforms_pass_lazy_array_equiv(map_da, map_ds):\n    with raise_if_dask_computes():\n        assert_equal(map_ds.cxy.broadcast_like(map_ds.cxy), map_ds.cxy)\n        assert_equal(xr.broadcast(map_ds.cxy, map_ds.cxy)[0], map_ds.cxy)\n        assert_equal(map_ds.map(lambda x: x), map_ds)\n        assert_equal(map_ds.set_coords(""a"").reset_coords(""a""), map_ds)\n        assert_equal(map_ds.update({""a"": map_ds.a}), map_ds)\n\n        # fails because of index error\n        # assert_equal(\n        #     map_ds.rename_dims({""x"": ""xnew""}).rename_dims({""xnew"": ""x""}), map_ds\n        # )\n\n        assert_equal(\n            map_ds.rename_vars({""cxy"": ""cnew""}).rename_vars({""cnew"": ""cxy""}), map_ds\n        )\n\n        assert_equal(map_da._from_temp_dataset(map_da._to_temp_dataset()), map_da)\n        assert_equal(map_da.astype(map_da.dtype), map_da)\n        assert_equal(map_da.transpose(""y"", ""x"", transpose_coords=False).cxy, map_da.cxy)\n'"
xarray/tests/test_dataarray.py,501,"b'import pickle\nimport sys\nimport warnings\nfrom copy import deepcopy\nfrom textwrap import dedent\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nimport xarray as xr\nfrom xarray import DataArray, Dataset, IndexVariable, Variable, align, broadcast\nfrom xarray.coding.times import CFDatetimeCoder\nfrom xarray.convert import from_cdms2\nfrom xarray.core import dtypes\nfrom xarray.core.common import full_like\nfrom xarray.core.indexes import propagate_indexes\nfrom xarray.core.utils import is_scalar\nfrom xarray.tests import (\n    LooseVersion,\n    ReturnItem,\n    assert_allclose,\n    assert_array_equal,\n    assert_equal,\n    assert_identical,\n    has_dask,\n    raises_regex,\n    requires_bottleneck,\n    requires_dask,\n    requires_iris,\n    requires_numbagg,\n    requires_scipy,\n    requires_sparse,\n    source_ndarray,\n)\n\nfrom .test_dask import raise_if_dask_computes\n\n\nclass TestDataArray:\n    @pytest.fixture(autouse=True)\n    def setup(self):\n        self.attrs = {""attr1"": ""value1"", ""attr2"": 2929}\n        self.x = np.random.random((10, 20))\n        self.v = Variable([""x"", ""y""], self.x)\n        self.va = Variable([""x"", ""y""], self.x, self.attrs)\n        self.ds = Dataset({""foo"": self.v})\n        self.dv = self.ds[""foo""]\n\n        self.mindex = pd.MultiIndex.from_product(\n            [[""a"", ""b""], [1, 2]], names=(""level_1"", ""level_2"")\n        )\n        self.mda = DataArray([0, 1, 2, 3], coords={""x"": self.mindex}, dims=""x"")\n\n    def test_repr(self):\n        v = Variable([""time"", ""x""], [[1, 2, 3], [4, 5, 6]], {""foo"": ""bar""})\n        coords = {""x"": np.arange(3, dtype=np.int64), ""other"": np.int64(0)}\n        data_array = DataArray(v, coords, name=""my_variable"")\n        expected = dedent(\n            """"""\\\n            <xarray.DataArray \'my_variable\' (time: 2, x: 3)>\n            array([[1, 2, 3],\n                   [4, 5, 6]])\n            Coordinates:\n              * x        (x) int64 0 1 2\n                other    int64 0\n            Dimensions without coordinates: time\n            Attributes:\n                foo:      bar""""""\n        )\n        assert expected == repr(data_array)\n\n    def test_repr_multiindex(self):\n        expected = dedent(\n            """"""\\\n            <xarray.DataArray (x: 4)>\n            array([0, 1, 2, 3])\n            Coordinates:\n              * x        (x) MultiIndex\n              - level_1  (x) object \'a\' \'a\' \'b\' \'b\'\n              - level_2  (x) int64 1 2 1 2""""""\n        )\n        assert expected == repr(self.mda)\n\n    @pytest.mark.skipif(\n        LooseVersion(np.__version__) < ""1.16"",\n        reason=""old versions of numpy have different printing behavior"",\n    )\n    def test_repr_multiindex_long(self):\n        mindex_long = pd.MultiIndex.from_product(\n            [[""a"", ""b"", ""c"", ""d""], [1, 2, 3, 4, 5, 6, 7, 8]],\n            names=(""level_1"", ""level_2""),\n        )\n        mda_long = DataArray(list(range(32)), coords={""x"": mindex_long}, dims=""x"")\n        expected = dedent(\n            """"""\\\n            <xarray.DataArray (x: 32)>\n            array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n                   17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n            Coordinates:\n              * x        (x) MultiIndex\n              - level_1  (x) object \'a\' \'a\' \'a\' \'a\' \'a\' \'a\' \'a\' ... \'d\' \'d\' \'d\' \'d\' \'d\' \'d\'\n              - level_2  (x) int64 1 2 3 4 5 6 7 8 1 2 3 4 5 6 ... 4 5 6 7 8 1 2 3 4 5 6 7 8""""""\n        )\n        assert expected == repr(mda_long)\n\n    def test_properties(self):\n        assert_equal(self.dv.variable, self.v)\n        assert_array_equal(self.dv.values, self.v.values)\n        for attr in [""dims"", ""dtype"", ""shape"", ""size"", ""nbytes"", ""ndim"", ""attrs""]:\n            assert getattr(self.dv, attr) == getattr(self.v, attr)\n        assert len(self.dv) == len(self.v)\n        assert_equal(self.dv.variable, self.v)\n        assert set(self.dv.coords) == set(self.ds.coords)\n        for k, v in self.dv.coords.items():\n            assert_array_equal(v, self.ds.coords[k])\n        with pytest.raises(AttributeError):\n            self.dv.dataset\n        assert isinstance(self.ds[""x""].to_index(), pd.Index)\n        with raises_regex(ValueError, ""must be 1-dimensional""):\n            self.ds[""foo""].to_index()\n        with pytest.raises(AttributeError):\n            self.dv.variable = self.v\n\n    def test_data_property(self):\n        array = DataArray(np.zeros((3, 4)))\n        actual = array.copy()\n        actual.values = np.ones((3, 4))\n        assert_array_equal(np.ones((3, 4)), actual.values)\n        actual.data = 2 * np.ones((3, 4))\n        assert_array_equal(2 * np.ones((3, 4)), actual.data)\n        assert_array_equal(actual.data, actual.values)\n\n    def test_indexes(self):\n        array = DataArray(np.zeros((2, 3)), [(""x"", [0, 1]), (""y"", [""a"", ""b"", ""c""])])\n        expected = {""x"": pd.Index([0, 1]), ""y"": pd.Index([""a"", ""b"", ""c""])}\n        assert array.indexes.keys() == expected.keys()\n        for k in expected:\n            assert array.indexes[k].equals(expected[k])\n\n    def test_get_index(self):\n        array = DataArray(np.zeros((2, 3)), coords={""x"": [""a"", ""b""]}, dims=[""x"", ""y""])\n        assert array.get_index(""x"").equals(pd.Index([""a"", ""b""]))\n        assert array.get_index(""y"").equals(pd.Index([0, 1, 2]))\n        with pytest.raises(KeyError):\n            array.get_index(""z"")\n\n    def test_get_index_size_zero(self):\n        array = DataArray(np.zeros((0,)), dims=[""x""])\n        actual = array.get_index(""x"")\n        expected = pd.Index([], dtype=np.int64)\n        assert actual.equals(expected)\n        assert actual.dtype == expected.dtype\n\n    def test_struct_array_dims(self):\n        """"""\n        This test checks subraction of two DataArrays for the case\n        when dimension is a structured array.\n        """"""\n        # GH837, GH861\n        # checking array subtraction when dims are the same\n        p_data = np.array(\n            [(""Abe"", 180), (""Stacy"", 150), (""Dick"", 200)],\n            dtype=[(""name"", ""|S256""), (""height"", object)],\n        )\n        weights_0 = DataArray(\n            [80, 56, 120], dims=[""participant""], coords={""participant"": p_data}\n        )\n        weights_1 = DataArray(\n            [81, 52, 115], dims=[""participant""], coords={""participant"": p_data}\n        )\n        actual = weights_1 - weights_0\n\n        expected = DataArray(\n            [1, -4, -5], dims=[""participant""], coords={""participant"": p_data}\n        )\n\n        assert_identical(actual, expected)\n\n        # checking array subraction when dims are not the same\n        p_data_alt = np.array(\n            [(""Abe"", 180), (""Stacy"", 151), (""Dick"", 200)],\n            dtype=[(""name"", ""|S256""), (""height"", object)],\n        )\n        weights_1 = DataArray(\n            [81, 52, 115], dims=[""participant""], coords={""participant"": p_data_alt}\n        )\n        actual = weights_1 - weights_0\n\n        expected = DataArray(\n            [1, -5], dims=[""participant""], coords={""participant"": p_data[[0, 2]]}\n        )\n\n        assert_identical(actual, expected)\n\n        # checking array subraction when dims are not the same and one\n        # is np.nan\n        p_data_nan = np.array(\n            [(""Abe"", 180), (""Stacy"", np.nan), (""Dick"", 200)],\n            dtype=[(""name"", ""|S256""), (""height"", object)],\n        )\n        weights_1 = DataArray(\n            [81, 52, 115], dims=[""participant""], coords={""participant"": p_data_nan}\n        )\n        actual = weights_1 - weights_0\n\n        expected = DataArray(\n            [1, -5], dims=[""participant""], coords={""participant"": p_data[[0, 2]]}\n        )\n\n        assert_identical(actual, expected)\n\n    def test_name(self):\n        arr = self.dv\n        assert arr.name == ""foo""\n\n        copied = arr.copy()\n        arr.name = ""bar""\n        assert arr.name == ""bar""\n        assert_equal(copied, arr)\n\n        actual = DataArray(IndexVariable(""x"", [3]))\n        actual.name = ""y""\n        expected = DataArray([3], [(""x"", [3])], name=""y"")\n        assert_identical(actual, expected)\n\n    def test_dims(self):\n        arr = self.dv\n        assert arr.dims == (""x"", ""y"")\n\n        with raises_regex(AttributeError, ""you cannot assign""):\n            arr.dims = (""w"", ""z"")\n\n    def test_sizes(self):\n        array = DataArray(np.zeros((3, 4)), dims=[""x"", ""y""])\n        assert array.sizes == {""x"": 3, ""y"": 4}\n        assert tuple(array.sizes) == array.dims\n        with pytest.raises(TypeError):\n            array.sizes[""foo""] = 5\n\n    def test_encoding(self):\n        expected = {""foo"": ""bar""}\n        self.dv.encoding[""foo""] = ""bar""\n        assert expected == self.dv.encoding\n\n        expected = {""baz"": 0}\n        self.dv.encoding = expected\n\n        assert expected is not self.dv.encoding\n\n    def test_constructor(self):\n        data = np.random.random((2, 3))\n\n        actual = DataArray(data)\n        expected = Dataset({None: ([""dim_0"", ""dim_1""], data)})[None]\n        assert_identical(expected, actual)\n\n        actual = DataArray(data, [[""a"", ""b""], [-1, -2, -3]])\n        expected = Dataset(\n            {\n                None: ([""dim_0"", ""dim_1""], data),\n                ""dim_0"": (""dim_0"", [""a"", ""b""]),\n                ""dim_1"": (""dim_1"", [-1, -2, -3]),\n            }\n        )[None]\n        assert_identical(expected, actual)\n\n        actual = DataArray(\n            data, [pd.Index([""a"", ""b""], name=""x""), pd.Index([-1, -2, -3], name=""y"")]\n        )\n        expected = Dataset(\n            {None: ([""x"", ""y""], data), ""x"": (""x"", [""a"", ""b""]), ""y"": (""y"", [-1, -2, -3])}\n        )[None]\n        assert_identical(expected, actual)\n\n        coords = [[""a"", ""b""], [-1, -2, -3]]\n        actual = DataArray(data, coords, [""x"", ""y""])\n        assert_identical(expected, actual)\n\n        coords = [pd.Index([""a"", ""b""], name=""A""), pd.Index([-1, -2, -3], name=""B"")]\n        actual = DataArray(data, coords, [""x"", ""y""])\n        assert_identical(expected, actual)\n\n        coords = {""x"": [""a"", ""b""], ""y"": [-1, -2, -3]}\n        actual = DataArray(data, coords, [""x"", ""y""])\n        assert_identical(expected, actual)\n\n        coords = [(""x"", [""a"", ""b""]), (""y"", [-1, -2, -3])]\n        actual = DataArray(data, coords)\n        assert_identical(expected, actual)\n\n        expected = Dataset({None: ([""x"", ""y""], data), ""x"": (""x"", [""a"", ""b""])})[None]\n        actual = DataArray(data, {""x"": [""a"", ""b""]}, [""x"", ""y""])\n        assert_identical(expected, actual)\n\n        actual = DataArray(data, dims=[""x"", ""y""])\n        expected = Dataset({None: ([""x"", ""y""], data)})[None]\n        assert_identical(expected, actual)\n\n        actual = DataArray(data, dims=[""x"", ""y""], name=""foo"")\n        expected = Dataset({""foo"": ([""x"", ""y""], data)})[""foo""]\n        assert_identical(expected, actual)\n\n        actual = DataArray(data, name=""foo"")\n        expected = Dataset({""foo"": ([""dim_0"", ""dim_1""], data)})[""foo""]\n        assert_identical(expected, actual)\n\n        actual = DataArray(data, dims=[""x"", ""y""], attrs={""bar"": 2})\n        expected = Dataset({None: ([""x"", ""y""], data, {""bar"": 2})})[None]\n        assert_identical(expected, actual)\n\n        actual = DataArray(data, dims=[""x"", ""y""])\n        expected = Dataset({None: ([""x"", ""y""], data, {}, {""bar"": 2})})[None]\n        assert_identical(expected, actual)\n\n    def test_constructor_invalid(self):\n        data = np.random.randn(3, 2)\n\n        with raises_regex(ValueError, ""coords is not dict-like""):\n            DataArray(data, [[0, 1, 2]], [""x"", ""y""])\n\n        with raises_regex(ValueError, ""not a subset of the .* dim""):\n            DataArray(data, {""x"": [0, 1, 2]}, [""a"", ""b""])\n        with raises_regex(ValueError, ""not a subset of the .* dim""):\n            DataArray(data, {""x"": [0, 1, 2]})\n\n        with raises_regex(TypeError, ""is not a string""):\n            DataArray(data, dims=[""x"", None])\n\n        with raises_regex(ValueError, ""conflicting sizes for dim""):\n            DataArray([1, 2, 3], coords=[(""x"", [0, 1])])\n        with raises_regex(ValueError, ""conflicting sizes for dim""):\n            DataArray([1, 2], coords={""x"": [0, 1], ""y"": (""x"", [1])}, dims=""x"")\n\n        with raises_regex(ValueError, ""conflicting MultiIndex""):\n            DataArray(np.random.rand(4, 4), [(""x"", self.mindex), (""y"", self.mindex)])\n        with raises_regex(ValueError, ""conflicting MultiIndex""):\n            DataArray(np.random.rand(4, 4), [(""x"", self.mindex), (""level_1"", range(4))])\n\n        with raises_regex(ValueError, ""matching the dimension size""):\n            DataArray(data, coords={""x"": 0}, dims=[""x"", ""y""])\n\n    def test_constructor_from_self_described(self):\n        data = [[-0.1, 21], [0, 2]]\n        expected = DataArray(\n            data,\n            coords={""x"": [""a"", ""b""], ""y"": [-1, -2]},\n            dims=[""x"", ""y""],\n            name=""foobar"",\n            attrs={""bar"": 2},\n        )\n        actual = DataArray(expected)\n        assert_identical(expected, actual)\n\n        actual = DataArray(expected.values, actual.coords)\n        assert_equal(expected, actual)\n\n        frame = pd.DataFrame(\n            data,\n            index=pd.Index([""a"", ""b""], name=""x""),\n            columns=pd.Index([-1, -2], name=""y""),\n        )\n        actual = DataArray(frame)\n        assert_equal(expected, actual)\n\n        series = pd.Series(data[0], index=pd.Index([-1, -2], name=""y""))\n        actual = DataArray(series)\n        assert_equal(expected[0].reset_coords(""x"", drop=True), actual)\n\n        if LooseVersion(pd.__version__) < ""0.25.0"":\n            with warnings.catch_warnings():\n                warnings.filterwarnings(""ignore"", r""\\W*Panel is deprecated"")\n                panel = pd.Panel({0: frame})\n            actual = DataArray(panel)\n            expected = DataArray([data], expected.coords, [""dim_0"", ""x"", ""y""])\n            expected[""dim_0""] = [0]\n            assert_identical(expected, actual)\n\n        expected = DataArray(\n            data,\n            coords={""x"": [""a"", ""b""], ""y"": [-1, -2], ""a"": 0, ""z"": (""x"", [-0.5, 0.5])},\n            dims=[""x"", ""y""],\n        )\n        actual = DataArray(expected)\n        assert_identical(expected, actual)\n\n        actual = DataArray(expected.values, expected.coords)\n        assert_identical(expected, actual)\n\n        expected = Dataset({""foo"": (""foo"", [""a"", ""b""])})[""foo""]\n        actual = DataArray(pd.Index([""a"", ""b""], name=""foo""))\n        assert_identical(expected, actual)\n\n        actual = DataArray(IndexVariable(""foo"", [""a"", ""b""]))\n        assert_identical(expected, actual)\n\n    def test_constructor_from_0d(self):\n        expected = Dataset({None: ([], 0)})[None]\n        actual = DataArray(0)\n        assert_identical(expected, actual)\n\n    @requires_dask\n    def test_constructor_dask_coords(self):\n        # regression test for GH1684\n        import dask.array as da\n\n        coord = da.arange(8, chunks=(4,))\n        data = da.random.random((8, 8), chunks=(4, 4)) + 1\n        actual = DataArray(data, coords={""x"": coord, ""y"": coord}, dims=[""x"", ""y""])\n\n        ecoord = np.arange(8)\n        expected = DataArray(data, coords={""x"": ecoord, ""y"": ecoord}, dims=[""x"", ""y""])\n        assert_equal(actual, expected)\n\n    def test_equals_and_identical(self):\n        orig = DataArray(np.arange(5.0), {""a"": 42}, dims=""x"")\n\n        expected = orig\n        actual = orig.copy()\n        assert expected.equals(actual)\n        assert expected.identical(actual)\n\n        actual = expected.rename(""baz"")\n        assert expected.equals(actual)\n        assert not expected.identical(actual)\n\n        actual = expected.rename({""x"": ""xxx""})\n        assert not expected.equals(actual)\n        assert not expected.identical(actual)\n\n        actual = expected.copy()\n        actual.attrs[""foo""] = ""bar""\n        assert expected.equals(actual)\n        assert not expected.identical(actual)\n\n        actual = expected.copy()\n        actual[""x""] = (""x"", -np.arange(5))\n        assert not expected.equals(actual)\n        assert not expected.identical(actual)\n\n        actual = expected.reset_coords(drop=True)\n        assert not expected.equals(actual)\n        assert not expected.identical(actual)\n\n        actual = orig.copy()\n        actual[0] = np.nan\n        expected = actual.copy()\n        assert expected.equals(actual)\n        assert expected.identical(actual)\n\n        actual[:] = np.nan\n        assert not expected.equals(actual)\n        assert not expected.identical(actual)\n\n        actual = expected.copy()\n        actual[""a""] = 100000\n        assert not expected.equals(actual)\n        assert not expected.identical(actual)\n\n    def test_equals_failures(self):\n        orig = DataArray(np.arange(5.0), {""a"": 42}, dims=""x"")\n        assert not orig.equals(np.arange(5))\n        assert not orig.identical(123)\n        assert not orig.broadcast_equals({1: 2})\n\n    def test_broadcast_equals(self):\n        a = DataArray([0, 0], {""y"": 0}, dims=""x"")\n        b = DataArray([0, 0], {""y"": (""x"", [0, 0])}, dims=""x"")\n        assert a.broadcast_equals(b)\n        assert b.broadcast_equals(a)\n        assert not a.equals(b)\n        assert not a.identical(b)\n\n        c = DataArray([0], coords={""x"": 0}, dims=""y"")\n        assert not a.broadcast_equals(c)\n        assert not c.broadcast_equals(a)\n\n    def test_getitem(self):\n        # strings pull out dataarrays\n        assert_identical(self.dv, self.ds[""foo""])\n        x = self.dv[""x""]\n        y = self.dv[""y""]\n        assert_identical(self.ds[""x""], x)\n        assert_identical(self.ds[""y""], y)\n\n        arr = ReturnItem()\n        for i in [\n            arr[:],\n            arr[...],\n            arr[x.values],\n            arr[x.variable],\n            arr[x],\n            arr[x, y],\n            arr[x.values > -1],\n            arr[x.variable > -1],\n            arr[x > -1],\n            arr[x > -1, y > -1],\n        ]:\n            assert_equal(self.dv, self.dv[i])\n        for i in [\n            arr[0],\n            arr[:, 0],\n            arr[:3, :2],\n            arr[x.values[:3]],\n            arr[x.variable[:3]],\n            arr[x[:3]],\n            arr[x[:3], y[:4]],\n            arr[x.values > 3],\n            arr[x.variable > 3],\n            arr[x > 3],\n            arr[x > 3, y > 3],\n        ]:\n            assert_array_equal(self.v[i], self.dv[i])\n\n    def test_getitem_dict(self):\n        actual = self.dv[{""x"": slice(3), ""y"": 0}]\n        expected = self.dv.isel(x=slice(3), y=0)\n        assert_identical(expected, actual)\n\n    def test_getitem_coords(self):\n        orig = DataArray(\n            [[10], [20]],\n            {\n                ""x"": [1, 2],\n                ""y"": [3],\n                ""z"": 4,\n                ""x2"": (""x"", [""a"", ""b""]),\n                ""y2"": (""y"", [""c""]),\n                ""xy"": ([""y"", ""x""], [[""d"", ""e""]]),\n            },\n            dims=[""x"", ""y""],\n        )\n\n        assert_identical(orig, orig[:])\n        assert_identical(orig, orig[:, :])\n        assert_identical(orig, orig[...])\n        assert_identical(orig, orig[:2, :1])\n        assert_identical(orig, orig[[0, 1], [0]])\n\n        actual = orig[0, 0]\n        expected = DataArray(\n            10, {""x"": 1, ""y"": 3, ""z"": 4, ""x2"": ""a"", ""y2"": ""c"", ""xy"": ""d""}\n        )\n        assert_identical(expected, actual)\n\n        actual = orig[0, :]\n        expected = DataArray(\n            [10],\n            {\n                ""x"": 1,\n                ""y"": [3],\n                ""z"": 4,\n                ""x2"": ""a"",\n                ""y2"": (""y"", [""c""]),\n                ""xy"": (""y"", [""d""]),\n            },\n            dims=""y"",\n        )\n        assert_identical(expected, actual)\n\n        actual = orig[:, 0]\n        expected = DataArray(\n            [10, 20],\n            {\n                ""x"": [1, 2],\n                ""y"": 3,\n                ""z"": 4,\n                ""x2"": (""x"", [""a"", ""b""]),\n                ""y2"": ""c"",\n                ""xy"": (""x"", [""d"", ""e""]),\n            },\n            dims=""x"",\n        )\n        assert_identical(expected, actual)\n\n    def test_getitem_dataarray(self):\n        # It should not conflict\n        da = DataArray(np.arange(12).reshape((3, 4)), dims=[""x"", ""y""])\n        ind = DataArray([[0, 1], [0, 1]], dims=[""x"", ""z""])\n        actual = da[ind]\n        assert_array_equal(actual, da.values[[[0, 1], [0, 1]], :])\n\n        da = DataArray(\n            np.arange(12).reshape((3, 4)),\n            dims=[""x"", ""y""],\n            coords={""x"": [0, 1, 2], ""y"": [""a"", ""b"", ""c"", ""d""]},\n        )\n        ind = xr.DataArray([[0, 1], [0, 1]], dims=[""X"", ""Y""])\n        actual = da[ind]\n        expected = da.values[[[0, 1], [0, 1]], :]\n        assert_array_equal(actual, expected)\n        assert actual.dims == (""X"", ""Y"", ""y"")\n\n        # boolean indexing\n        ind = xr.DataArray([True, True, False], dims=[""x""])\n        assert_equal(da[ind], da[[0, 1], :])\n        assert_equal(da[ind], da[[0, 1]])\n        assert_equal(da[ind], da[ind.values])\n\n    def test_getitem_empty_index(self):\n        da = DataArray(np.arange(12).reshape((3, 4)), dims=[""x"", ""y""])\n        assert_identical(da[{""x"": []}], DataArray(np.zeros((0, 4)), dims=[""x"", ""y""]))\n        assert_identical(\n            da.loc[{""y"": []}], DataArray(np.zeros((3, 0)), dims=[""x"", ""y""])\n        )\n        assert_identical(da[[]], DataArray(np.zeros((0, 4)), dims=[""x"", ""y""]))\n\n    def test_setitem(self):\n        # basic indexing should work as numpy\'s indexing\n        tuples = [\n            (0, 0),\n            (0, slice(None, None)),\n            (slice(None, None), slice(None, None)),\n            (slice(None, None), 0),\n            ([1, 0], slice(None, None)),\n            (slice(None, None), [1, 0]),\n        ]\n        for t in tuples:\n            expected = np.arange(6).reshape(3, 2)\n            orig = DataArray(\n                np.arange(6).reshape(3, 2),\n                {\n                    ""x"": [1, 2, 3],\n                    ""y"": [""a"", ""b""],\n                    ""z"": 4,\n                    ""x2"": (""x"", [""a"", ""b"", ""c""]),\n                    ""y2"": (""y"", [""d"", ""e""]),\n                },\n                dims=[""x"", ""y""],\n            )\n            orig[t] = 1\n            expected[t] = 1\n            assert_array_equal(orig.values, expected)\n\n    def test_setitem_fancy(self):\n        # vectorized indexing\n        da = DataArray(np.ones((3, 2)), dims=[""x"", ""y""])\n        ind = Variable([""a""], [0, 1])\n        da[dict(x=ind, y=ind)] = 0\n        expected = DataArray([[0, 1], [1, 0], [1, 1]], dims=[""x"", ""y""])\n        assert_identical(expected, da)\n        # assign another 0d-variable\n        da[dict(x=ind, y=ind)] = Variable((), 0)\n        expected = DataArray([[0, 1], [1, 0], [1, 1]], dims=[""x"", ""y""])\n        assert_identical(expected, da)\n        # assign another 1d-variable\n        da[dict(x=ind, y=ind)] = Variable([""a""], [2, 3])\n        expected = DataArray([[2, 1], [1, 3], [1, 1]], dims=[""x"", ""y""])\n        assert_identical(expected, da)\n\n        # 2d-vectorized indexing\n        da = DataArray(np.ones((3, 2)), dims=[""x"", ""y""])\n        ind_x = DataArray([[0, 1]], dims=[""a"", ""b""])\n        ind_y = DataArray([[1, 0]], dims=[""a"", ""b""])\n        da[dict(x=ind_x, y=ind_y)] = 0\n        expected = DataArray([[1, 0], [0, 1], [1, 1]], dims=[""x"", ""y""])\n        assert_identical(expected, da)\n\n        da = DataArray(np.ones((3, 2)), dims=[""x"", ""y""])\n        ind = Variable([""a""], [0, 1])\n        da[ind] = 0\n        expected = DataArray([[0, 0], [0, 0], [1, 1]], dims=[""x"", ""y""])\n        assert_identical(expected, da)\n\n    def test_setitem_dataarray(self):\n        def get_data():\n            return DataArray(\n                np.ones((4, 3, 2)),\n                dims=[""x"", ""y"", ""z""],\n                coords={\n                    ""x"": np.arange(4),\n                    ""y"": [""a"", ""b"", ""c""],\n                    ""non-dim"": (""x"", [1, 3, 4, 2]),\n                },\n            )\n\n        da = get_data()\n        # indexer with inconsistent coordinates.\n        ind = DataArray(np.arange(1, 4), dims=[""x""], coords={""x"": np.random.randn(3)})\n        with raises_regex(IndexError, ""dimension coordinate \'x\'""):\n            da[dict(x=ind)] = 0\n\n        # indexer with consistent coordinates.\n        ind = DataArray(np.arange(1, 4), dims=[""x""], coords={""x"": np.arange(1, 4)})\n        da[dict(x=ind)] = 0  # should not raise\n        assert np.allclose(da[dict(x=ind)].values, 0)\n        assert_identical(da[""x""], get_data()[""x""])\n        assert_identical(da[""non-dim""], get_data()[""non-dim""])\n\n        da = get_data()\n        # conflict in the assigning values\n        value = xr.DataArray(\n            np.zeros((3, 3, 2)),\n            dims=[""x"", ""y"", ""z""],\n            coords={""x"": [0, 1, 2], ""non-dim"": (""x"", [0, 2, 4])},\n        )\n        with raises_regex(IndexError, ""dimension coordinate \'x\'""):\n            da[dict(x=ind)] = value\n\n        # consistent coordinate in the assigning values\n        value = xr.DataArray(\n            np.zeros((3, 3, 2)),\n            dims=[""x"", ""y"", ""z""],\n            coords={""x"": [1, 2, 3], ""non-dim"": (""x"", [0, 2, 4])},\n        )\n        da[dict(x=ind)] = value\n        assert np.allclose(da[dict(x=ind)].values, 0)\n        assert_identical(da[""x""], get_data()[""x""])\n        assert_identical(da[""non-dim""], get_data()[""non-dim""])\n\n        # Conflict in the non-dimension coordinate\n        value = xr.DataArray(\n            np.zeros((3, 3, 2)),\n            dims=[""x"", ""y"", ""z""],\n            coords={""x"": [1, 2, 3], ""non-dim"": (""x"", [0, 2, 4])},\n        )\n        da[dict(x=ind)] = value  # should not raise\n\n        # conflict in the assigning values\n        value = xr.DataArray(\n            np.zeros((3, 3, 2)),\n            dims=[""x"", ""y"", ""z""],\n            coords={""x"": [0, 1, 2], ""non-dim"": (""x"", [0, 2, 4])},\n        )\n        with raises_regex(IndexError, ""dimension coordinate \'x\'""):\n            da[dict(x=ind)] = value\n\n        # consistent coordinate in the assigning values\n        value = xr.DataArray(\n            np.zeros((3, 3, 2)),\n            dims=[""x"", ""y"", ""z""],\n            coords={""x"": [1, 2, 3], ""non-dim"": (""x"", [0, 2, 4])},\n        )\n        da[dict(x=ind)] = value  # should not raise\n\n    def test_contains(self):\n        data_array = DataArray([1, 2])\n        assert 1 in data_array\n        assert 3 not in data_array\n\n    def test_attr_sources_multiindex(self):\n        # make sure attr-style access for multi-index levels\n        # returns DataArray objects\n        assert isinstance(self.mda.level_1, DataArray)\n\n    def test_pickle(self):\n        data = DataArray(np.random.random((3, 3)), dims=(""id"", ""time""))\n        roundtripped = pickle.loads(pickle.dumps(data))\n        assert_identical(data, roundtripped)\n\n    @requires_dask\n    def test_chunk(self):\n        unblocked = DataArray(np.ones((3, 4)))\n        assert unblocked.chunks is None\n\n        blocked = unblocked.chunk()\n        assert blocked.chunks == ((3,), (4,))\n        first_dask_name = blocked.data.name\n\n        blocked = unblocked.chunk(chunks=((2, 1), (2, 2)))\n        assert blocked.chunks == ((2, 1), (2, 2))\n        assert blocked.data.name != first_dask_name\n\n        blocked = unblocked.chunk(chunks=(3, 3))\n        assert blocked.chunks == ((3,), (3, 1))\n        assert blocked.data.name != first_dask_name\n\n        # name doesn\'t change when rechunking by same amount\n        # this fails if ReprObject doesn\'t have __dask_tokenize__ defined\n        assert unblocked.chunk(2).data.name == unblocked.chunk(2).data.name\n\n        assert blocked.load().chunks is None\n\n        # Check that kwargs are passed\n        import dask.array as da\n\n        blocked = unblocked.chunk(name_prefix=""testname_"")\n        assert isinstance(blocked.data, da.Array)\n        assert ""testname_"" in blocked.data.name\n\n    def test_isel(self):\n        assert_identical(self.dv[0], self.dv.isel(x=0))\n        assert_identical(self.dv, self.dv.isel(x=slice(None)))\n        assert_identical(self.dv[:3], self.dv.isel(x=slice(3)))\n        assert_identical(self.dv[:3, :5], self.dv.isel(x=slice(3), y=slice(5)))\n        with raises_regex(\n            ValueError,\n            r""dimensions {\'not_a_dim\'} do not exist. Expected ""\n            r""one or more of \\(\'x\', \'y\'\\)"",\n        ):\n            self.dv.isel(not_a_dim=0)\n        with pytest.warns(\n            UserWarning,\n            match=r""dimensions {\'not_a_dim\'} do not exist. ""\n            r""Expected one or more of \\(\'x\', \'y\'\\)"",\n        ):\n            self.dv.isel(not_a_dim=0, missing_dims=""warn"")\n        assert_identical(self.dv, self.dv.isel(not_a_dim=0, missing_dims=""ignore""))\n\n    def test_isel_types(self):\n        # regression test for #1405\n        da = DataArray([1, 2, 3], dims=""x"")\n        # uint64\n        assert_identical(\n            da.isel(x=np.array([0], dtype=""uint64"")), da.isel(x=np.array([0]))\n        )\n        # uint32\n        assert_identical(\n            da.isel(x=np.array([0], dtype=""uint32"")), da.isel(x=np.array([0]))\n        )\n        # int64\n        assert_identical(\n            da.isel(x=np.array([0], dtype=""int64"")), da.isel(x=np.array([0]))\n        )\n\n    @pytest.mark.filterwarnings(""ignore::DeprecationWarning"")\n    def test_isel_fancy(self):\n        shape = (10, 7, 6)\n        np_array = np.random.random(shape)\n        da = DataArray(\n            np_array, dims=[""time"", ""y"", ""x""], coords={""time"": np.arange(0, 100, 10)}\n        )\n        y = [1, 3]\n        x = [3, 0]\n\n        expected = da.values[:, y, x]\n\n        actual = da.isel(y=((""test_coord"",), y), x=((""test_coord"",), x))\n        assert actual.coords[""test_coord""].shape == (len(y),)\n        assert list(actual.coords) == [""time""]\n        assert actual.dims == (""time"", ""test_coord"")\n\n        np.testing.assert_equal(actual, expected)\n\n        # a few corner cases\n        da.isel(\n            time=((""points"",), [1, 2]), x=((""points"",), [2, 2]), y=((""points"",), [3, 4])\n        )\n        np.testing.assert_allclose(\n            da.isel(\n                time=((""p"",), [1]), x=((""p"",), [2]), y=((""p"",), [4])\n            ).values.squeeze(),\n            np_array[1, 4, 2].squeeze(),\n        )\n        da.isel(time=((""points"",), [1, 2]))\n        y = [-1, 0]\n        x = [-2, 2]\n        expected = da.values[:, y, x]\n        actual = da.isel(x=((""points"",), x), y=((""points"",), y)).values\n        np.testing.assert_equal(actual, expected)\n\n        # test that the order of the indexers doesn\'t matter\n        assert_identical(\n            da.isel(y=((""points"",), y), x=((""points"",), x)),\n            da.isel(x=((""points"",), x), y=((""points"",), y)),\n        )\n\n        # make sure we\'re raising errors in the right places\n        with raises_regex(IndexError, ""Dimensions of indexers mismatch""):\n            da.isel(y=((""points"",), [1, 2]), x=((""points"",), [1, 2, 3]))\n\n        # tests using index or DataArray as indexers\n        stations = Dataset()\n        stations[""station""] = ((""station"",), [""A"", ""B"", ""C""])\n        stations[""dim1s""] = ((""station"",), [1, 2, 3])\n        stations[""dim2s""] = ((""station"",), [4, 5, 1])\n\n        actual = da.isel(x=stations[""dim1s""], y=stations[""dim2s""])\n        assert ""station"" in actual.coords\n        assert ""station"" in actual.dims\n        assert_identical(actual[""station""], stations[""station""])\n\n        with raises_regex(ValueError, ""conflicting values for ""):\n            da.isel(\n                x=DataArray([0, 1, 2], dims=""station"", coords={""station"": [0, 1, 2]}),\n                y=DataArray([0, 1, 2], dims=""station"", coords={""station"": [0, 1, 3]}),\n            )\n\n        # multi-dimensional selection\n        stations = Dataset()\n        stations[""a""] = ((""a"",), [""A"", ""B"", ""C""])\n        stations[""b""] = ((""b"",), [0, 1])\n        stations[""dim1s""] = ((""a"", ""b""), [[1, 2], [2, 3], [3, 4]])\n        stations[""dim2s""] = ((""a"",), [4, 5, 1])\n\n        actual = da.isel(x=stations[""dim1s""], y=stations[""dim2s""])\n        assert ""a"" in actual.coords\n        assert ""a"" in actual.dims\n        assert ""b"" in actual.coords\n        assert ""b"" in actual.dims\n        assert_identical(actual[""a""], stations[""a""])\n        assert_identical(actual[""b""], stations[""b""])\n        expected = da.variable[\n            :, stations[""dim2s""].variable, stations[""dim1s""].variable\n        ]\n        assert_array_equal(actual, expected)\n\n    def test_sel(self):\n        self.ds[""x""] = (""x"", np.array(list(""abcdefghij"")))\n        da = self.ds[""foo""]\n        assert_identical(da, da.sel(x=slice(None)))\n        assert_identical(da[1], da.sel(x=""b""))\n        assert_identical(da[:3], da.sel(x=slice(""c"")))\n        assert_identical(da[:3], da.sel(x=[""a"", ""b"", ""c""]))\n        assert_identical(da[:, :4], da.sel(y=(self.ds[""y""] < 4)))\n        # verify that indexing with a dataarray works\n        b = DataArray(""b"")\n        assert_identical(da[1], da.sel(x=b))\n        assert_identical(da[[1]], da.sel(x=slice(b, b)))\n\n    def test_sel_dataarray(self):\n        # indexing with DataArray\n        self.ds[""x""] = (""x"", np.array(list(""abcdefghij"")))\n        da = self.ds[""foo""]\n\n        ind = DataArray([""a"", ""b"", ""c""], dims=[""x""])\n        actual = da.sel(x=ind)\n        assert_identical(actual, da.isel(x=[0, 1, 2]))\n\n        # along new dimension\n        ind = DataArray([""a"", ""b"", ""c""], dims=[""new_dim""])\n        actual = da.sel(x=ind)\n        assert_array_equal(actual, da.isel(x=[0, 1, 2]))\n        assert ""new_dim"" in actual.dims\n\n        # with coordinate\n        ind = DataArray(\n            [""a"", ""b"", ""c""], dims=[""new_dim""], coords={""new_dim"": [0, 1, 2]}\n        )\n        actual = da.sel(x=ind)\n        assert_array_equal(actual, da.isel(x=[0, 1, 2]))\n        assert ""new_dim"" in actual.dims\n        assert ""new_dim"" in actual.coords\n        assert_equal(actual[""new_dim""].drop_vars(""x""), ind[""new_dim""])\n\n    def test_sel_invalid_slice(self):\n        array = DataArray(np.arange(10), [(""x"", np.arange(10))])\n        with raises_regex(ValueError, ""cannot use non-scalar arrays""):\n            array.sel(x=slice(array.x))\n\n    def test_sel_dataarray_datetime(self):\n        # regression test for GH1240\n        times = pd.date_range(""2000-01-01"", freq=""D"", periods=365)\n        array = DataArray(np.arange(365), [(""time"", times)])\n        result = array.sel(time=slice(array.time[0], array.time[-1]))\n        assert_equal(result, array)\n\n        array = DataArray(np.arange(365), [(""delta"", times - times[0])])\n        result = array.sel(delta=slice(array.delta[0], array.delta[-1]))\n        assert_equal(result, array)\n\n    def test_sel_float(self):\n        data_values = np.arange(4)\n\n        # case coords are float32 and label is list of floats\n        float_values = [0.0, 0.111, 0.222, 0.333]\n        coord_values = np.asarray(float_values, dtype=""float32"")\n        array = DataArray(data_values, [(""float32_coord"", coord_values)])\n        expected = DataArray(data_values[1:3], [(""float32_coord"", coord_values[1:3])])\n        actual = array.sel(float32_coord=float_values[1:3])\n        # case coords are float16 and label is list of floats\n        coord_values_16 = np.asarray(float_values, dtype=""float16"")\n        expected_16 = DataArray(\n            data_values[1:3], [(""float16_coord"", coord_values_16[1:3])]\n        )\n        array_16 = DataArray(data_values, [(""float16_coord"", coord_values_16)])\n        actual_16 = array_16.sel(float16_coord=float_values[1:3])\n\n        # case coord, label are scalars\n        expected_scalar = DataArray(\n            data_values[2], coords={""float32_coord"": coord_values[2]}\n        )\n        actual_scalar = array.sel(float32_coord=float_values[2])\n\n        assert_equal(expected, actual)\n        assert_equal(expected_scalar, actual_scalar)\n        assert_equal(expected_16, actual_16)\n\n    def test_sel_no_index(self):\n        array = DataArray(np.arange(10), dims=""x"")\n        assert_identical(array[0], array.sel(x=0))\n        assert_identical(array[:5], array.sel(x=slice(5)))\n        assert_identical(array[[0, -1]], array.sel(x=[0, -1]))\n        assert_identical(array[array < 5], array.sel(x=(array < 5)))\n\n    def test_sel_method(self):\n        data = DataArray(np.random.randn(3, 4), [(""x"", [0, 1, 2]), (""y"", list(""abcd""))])\n\n        expected = data.sel(y=[""a"", ""b""])\n        actual = data.sel(y=[""ab"", ""ba""], method=""pad"")\n        assert_identical(expected, actual)\n\n        expected = data.sel(x=[1, 2])\n        actual = data.sel(x=[0.9, 1.9], method=""backfill"", tolerance=1)\n        assert_identical(expected, actual)\n\n    def test_sel_drop(self):\n        data = DataArray([1, 2, 3], [(""x"", [0, 1, 2])])\n        expected = DataArray(1)\n        selected = data.sel(x=0, drop=True)\n        assert_identical(expected, selected)\n\n        expected = DataArray(1, {""x"": 0})\n        selected = data.sel(x=0, drop=False)\n        assert_identical(expected, selected)\n\n        data = DataArray([1, 2, 3], dims=[""x""])\n        expected = DataArray(1)\n        selected = data.sel(x=0, drop=True)\n        assert_identical(expected, selected)\n\n    def test_isel_drop(self):\n        data = DataArray([1, 2, 3], [(""x"", [0, 1, 2])])\n        expected = DataArray(1)\n        selected = data.isel(x=0, drop=True)\n        assert_identical(expected, selected)\n\n        expected = DataArray(1, {""x"": 0})\n        selected = data.isel(x=0, drop=False)\n        assert_identical(expected, selected)\n\n    def test_head(self):\n        assert_equal(self.dv.isel(x=slice(5)), self.dv.head(x=5))\n        assert_equal(self.dv.isel(x=slice(0)), self.dv.head(x=0))\n        assert_equal(\n            self.dv.isel({dim: slice(6) for dim in self.dv.dims}), self.dv.head(6)\n        )\n        assert_equal(\n            self.dv.isel({dim: slice(5) for dim in self.dv.dims}), self.dv.head()\n        )\n        with raises_regex(TypeError, ""either dict-like or a single int""):\n            self.dv.head([3])\n        with raises_regex(TypeError, ""expected integer type""):\n            self.dv.head(x=3.1)\n        with raises_regex(ValueError, ""expected positive int""):\n            self.dv.head(-3)\n\n    def test_tail(self):\n        assert_equal(self.dv.isel(x=slice(-5, None)), self.dv.tail(x=5))\n        assert_equal(self.dv.isel(x=slice(0)), self.dv.tail(x=0))\n        assert_equal(\n            self.dv.isel({dim: slice(-6, None) for dim in self.dv.dims}),\n            self.dv.tail(6),\n        )\n        assert_equal(\n            self.dv.isel({dim: slice(-5, None) for dim in self.dv.dims}), self.dv.tail()\n        )\n        with raises_regex(TypeError, ""either dict-like or a single int""):\n            self.dv.tail([3])\n        with raises_regex(TypeError, ""expected integer type""):\n            self.dv.tail(x=3.1)\n        with raises_regex(ValueError, ""expected positive int""):\n            self.dv.tail(-3)\n\n    def test_thin(self):\n        assert_equal(self.dv.isel(x=slice(None, None, 5)), self.dv.thin(x=5))\n        assert_equal(\n            self.dv.isel({dim: slice(None, None, 6) for dim in self.dv.dims}),\n            self.dv.thin(6),\n        )\n        with raises_regex(TypeError, ""either dict-like or a single int""):\n            self.dv.thin([3])\n        with raises_regex(TypeError, ""expected integer type""):\n            self.dv.thin(x=3.1)\n        with raises_regex(ValueError, ""expected positive int""):\n            self.dv.thin(-3)\n        with raises_regex(ValueError, ""cannot be zero""):\n            self.dv.thin(time=0)\n\n    def test_loc(self):\n        self.ds[""x""] = (""x"", np.array(list(""abcdefghij"")))\n        da = self.ds[""foo""]\n        assert_identical(da[:3], da.loc[:""c""])\n        assert_identical(da[1], da.loc[""b""])\n        assert_identical(da[1], da.loc[{""x"": ""b""}])\n        assert_identical(da[1], da.loc[""b"", ...])\n        assert_identical(da[:3], da.loc[[""a"", ""b"", ""c""]])\n        assert_identical(da[:3, :4], da.loc[[""a"", ""b"", ""c""], np.arange(4)])\n        assert_identical(da[:, :4], da.loc[:, self.ds[""y""] < 4])\n\n    def test_loc_assign(self):\n        self.ds[""x""] = (""x"", np.array(list(""abcdefghij"")))\n        da = self.ds[""foo""]\n        # assignment\n        da.loc[""a"":""j""] = 0\n        assert np.all(da.values == 0)\n        da.loc[{""x"": slice(""a"", ""j"")}] = 2\n        assert np.all(da.values == 2)\n\n        da.loc[{""x"": slice(""a"", ""j"")}] = 2\n        assert np.all(da.values == 2)\n\n        # Multi dimensional case\n        da = DataArray(np.arange(12).reshape(3, 4), dims=[""x"", ""y""])\n        da.loc[0, 0] = 0\n        assert da.values[0, 0] == 0\n        assert da.values[0, 1] != 0\n\n        da = DataArray(np.arange(12).reshape(3, 4), dims=[""x"", ""y""])\n        da.loc[0] = 0\n        assert np.all(da.values[0] == np.zeros(4))\n        assert da.values[1, 0] != 0\n\n    def test_loc_assign_dataarray(self):\n        def get_data():\n            return DataArray(\n                np.ones((4, 3, 2)),\n                dims=[""x"", ""y"", ""z""],\n                coords={\n                    ""x"": np.arange(4),\n                    ""y"": [""a"", ""b"", ""c""],\n                    ""non-dim"": (""x"", [1, 3, 4, 2]),\n                },\n            )\n\n        da = get_data()\n        # indexer with inconsistent coordinates.\n        ind = DataArray(np.arange(1, 4), dims=[""y""], coords={""y"": np.random.randn(3)})\n        with raises_regex(IndexError, ""dimension coordinate \'y\'""):\n            da.loc[dict(x=ind)] = 0\n\n        # indexer with consistent coordinates.\n        ind = DataArray(np.arange(1, 4), dims=[""x""], coords={""x"": np.arange(1, 4)})\n        da.loc[dict(x=ind)] = 0  # should not raise\n        assert np.allclose(da[dict(x=ind)].values, 0)\n        assert_identical(da[""x""], get_data()[""x""])\n        assert_identical(da[""non-dim""], get_data()[""non-dim""])\n\n        da = get_data()\n        # conflict in the assigning values\n        value = xr.DataArray(\n            np.zeros((3, 3, 2)),\n            dims=[""x"", ""y"", ""z""],\n            coords={""x"": [0, 1, 2], ""non-dim"": (""x"", [0, 2, 4])},\n        )\n        with raises_regex(IndexError, ""dimension coordinate \'x\'""):\n            da.loc[dict(x=ind)] = value\n\n        # consistent coordinate in the assigning values\n        value = xr.DataArray(\n            np.zeros((3, 3, 2)),\n            dims=[""x"", ""y"", ""z""],\n            coords={""x"": [1, 2, 3], ""non-dim"": (""x"", [0, 2, 4])},\n        )\n        da.loc[dict(x=ind)] = value\n        assert np.allclose(da[dict(x=ind)].values, 0)\n        assert_identical(da[""x""], get_data()[""x""])\n        assert_identical(da[""non-dim""], get_data()[""non-dim""])\n\n    def test_loc_single_boolean(self):\n        data = DataArray([0, 1], coords=[[True, False]])\n        assert data.loc[True] == 0\n        assert data.loc[False] == 1\n\n    def test_selection_multiindex(self):\n        mindex = pd.MultiIndex.from_product(\n            [[""a"", ""b""], [1, 2], [-1, -2]], names=(""one"", ""two"", ""three"")\n        )\n        mdata = DataArray(range(8), [(""x"", mindex)])\n\n        def test_sel(lab_indexer, pos_indexer, replaced_idx=False, renamed_dim=None):\n            da = mdata.sel(x=lab_indexer)\n            expected_da = mdata.isel(x=pos_indexer)\n            if not replaced_idx:\n                assert_identical(da, expected_da)\n            else:\n                if renamed_dim:\n                    assert da.dims[0] == renamed_dim\n                    da = da.rename({renamed_dim: ""x""})\n                assert_identical(da.variable, expected_da.variable)\n                assert not da[""x""].equals(expected_da[""x""])\n\n        test_sel((""a"", 1, -1), 0)\n        test_sel((""b"", 2, -2), -1)\n        test_sel((""a"", 1), [0, 1], replaced_idx=True, renamed_dim=""three"")\n        test_sel((""a"",), range(4), replaced_idx=True)\n        test_sel(""a"", range(4), replaced_idx=True)\n        test_sel([(""a"", 1, -1), (""b"", 2, -2)], [0, 7])\n        test_sel(slice(""a"", ""b""), range(8))\n        test_sel(slice((""a"", 1), (""b"", 1)), range(6))\n        test_sel({""one"": ""a"", ""two"": 1, ""three"": -1}, 0)\n        test_sel({""one"": ""a"", ""two"": 1}, [0, 1], replaced_idx=True, renamed_dim=""three"")\n        test_sel({""one"": ""a""}, range(4), replaced_idx=True)\n\n        assert_identical(mdata.loc[""a""], mdata.sel(x=""a""))\n        assert_identical(mdata.loc[(""a"", 1), ...], mdata.sel(x=(""a"", 1)))\n        assert_identical(mdata.loc[{""one"": ""a""}, ...], mdata.sel(x={""one"": ""a""}))\n        with pytest.raises(IndexError):\n            mdata.loc[(""a"", 1)]\n\n        assert_identical(mdata.sel(x={""one"": ""a"", ""two"": 1}), mdata.sel(one=""a"", two=1))\n\n    def test_selection_multiindex_remove_unused(self):\n        # GH2619. For MultiIndex, we need to call remove_unused.\n        ds = xr.DataArray(\n            np.arange(40).reshape(8, 5),\n            dims=[""x"", ""y""],\n            coords={""x"": np.arange(8), ""y"": np.arange(5)},\n        )\n        ds = ds.stack(xy=[""x"", ""y""])\n        ds_isel = ds.isel(xy=ds[""x""] < 4)\n        with pytest.raises(KeyError):\n            ds_isel.sel(x=5)\n\n        actual = ds_isel.unstack()\n        expected = ds.reset_index(""xy"").isel(xy=ds[""x""] < 4)\n        expected = expected.set_index(xy=[""x"", ""y""]).unstack()\n        assert_identical(expected, actual)\n\n    def test_selection_multiindex_from_level(self):\n        # GH: 3512\n        da = DataArray([0, 1], dims=[""x""], coords={""x"": [0, 1], ""y"": ""a""})\n        db = DataArray([2, 3], dims=[""x""], coords={""x"": [0, 1], ""y"": ""b""})\n        data = xr.concat([da, db], dim=""x"").set_index(xy=[""x"", ""y""])\n        assert data.dims == (""xy"",)\n        actual = data.sel(y=""a"")\n        expected = data.isel(xy=[0, 1]).unstack(""xy"").squeeze(""y"").drop_vars(""y"")\n        assert_equal(actual, expected)\n\n    def test_stack_groupby_unsorted_coord(self):\n        data = [[0, 1], [2, 3]]\n        data_flat = [0, 1, 2, 3]\n        dims = [""x"", ""y""]\n        y_vals = [2, 3]\n\n        arr = xr.DataArray(data, dims=dims, coords={""y"": y_vals})\n        actual1 = arr.stack(z=dims).groupby(""z"").first()\n        midx1 = pd.MultiIndex.from_product([[0, 1], [2, 3]], names=dims)\n        expected1 = xr.DataArray(data_flat, dims=[""z""], coords={""z"": midx1})\n        xr.testing.assert_equal(actual1, expected1)\n\n        # GH: 3287.  Note that y coord values are not in sorted order.\n        arr = xr.DataArray(data, dims=dims, coords={""y"": y_vals[::-1]})\n        actual2 = arr.stack(z=dims).groupby(""z"").first()\n        midx2 = pd.MultiIndex.from_product([[0, 1], [3, 2]], names=dims)\n        expected2 = xr.DataArray(data_flat, dims=[""z""], coords={""z"": midx2})\n        xr.testing.assert_equal(actual2, expected2)\n\n    def test_virtual_default_coords(self):\n        array = DataArray(np.zeros((5,)), dims=""x"")\n        expected = DataArray(range(5), dims=""x"", name=""x"")\n        assert_identical(expected, array[""x""])\n        assert_identical(expected, array.coords[""x""])\n\n    def test_virtual_time_components(self):\n        dates = pd.date_range(""2000-01-01"", periods=10)\n        da = DataArray(np.arange(1, 11), [(""time"", dates)])\n\n        assert_array_equal(da[""time.dayofyear""], da.values)\n        assert_array_equal(da.coords[""time.dayofyear""], da.values)\n\n    def test_coords(self):\n        # use int64 to ensure repr() consistency on windows\n        coords = [\n            IndexVariable(""x"", np.array([-1, -2], ""int64"")),\n            IndexVariable(""y"", np.array([0, 1, 2], ""int64"")),\n        ]\n        da = DataArray(np.random.randn(2, 3), coords, name=""foo"")\n\n        assert 2 == len(da.coords)\n\n        assert [""x"", ""y""] == list(da.coords)\n\n        assert coords[0].identical(da.coords[""x""])\n        assert coords[1].identical(da.coords[""y""])\n\n        assert ""x"" in da.coords\n        assert 0 not in da.coords\n        assert ""foo"" not in da.coords\n\n        with pytest.raises(KeyError):\n            da.coords[0]\n        with pytest.raises(KeyError):\n            da.coords[""foo""]\n\n        expected = dedent(\n            """"""\\\n        Coordinates:\n          * x        (x) int64 -1 -2\n          * y        (y) int64 0 1 2""""""\n        )\n        actual = repr(da.coords)\n        assert expected == actual\n\n        del da.coords[""x""]\n        da._indexes = propagate_indexes(da._indexes, exclude=""x"")\n        expected = DataArray(da.values, {""y"": [0, 1, 2]}, dims=[""x"", ""y""], name=""foo"")\n        assert_identical(da, expected)\n\n        with raises_regex(ValueError, ""conflicting MultiIndex""):\n            self.mda[""level_1""] = np.arange(4)\n            self.mda.coords[""level_1""] = np.arange(4)\n\n    def test_coords_to_index(self):\n        da = DataArray(np.zeros((2, 3)), [(""x"", [1, 2]), (""y"", list(""abc""))])\n\n        with raises_regex(ValueError, ""no valid index""):\n            da[0, 0].coords.to_index()\n\n        expected = pd.Index([""a"", ""b"", ""c""], name=""y"")\n        actual = da[0].coords.to_index()\n        assert expected.equals(actual)\n\n        expected = pd.MultiIndex.from_product(\n            [[1, 2], [""a"", ""b"", ""c""]], names=[""x"", ""y""]\n        )\n        actual = da.coords.to_index()\n        assert expected.equals(actual)\n\n        expected = pd.MultiIndex.from_product(\n            [[""a"", ""b"", ""c""], [1, 2]], names=[""y"", ""x""]\n        )\n        actual = da.coords.to_index([""y"", ""x""])\n        assert expected.equals(actual)\n\n        with raises_regex(ValueError, ""ordered_dims must match""):\n            da.coords.to_index([""x""])\n\n    def test_coord_coords(self):\n        orig = DataArray(\n            [10, 20], {""x"": [1, 2], ""x2"": (""x"", [""a"", ""b""]), ""z"": 4}, dims=""x""\n        )\n\n        actual = orig.coords[""x""]\n        expected = DataArray(\n            [1, 2], {""z"": 4, ""x2"": (""x"", [""a"", ""b""]), ""x"": [1, 2]}, dims=""x"", name=""x""\n        )\n        assert_identical(expected, actual)\n\n        del actual.coords[""x2""]\n        assert_identical(expected.reset_coords(""x2"", drop=True), actual)\n\n        actual.coords[""x3""] = (""x"", [""a"", ""b""])\n        expected = DataArray(\n            [1, 2], {""z"": 4, ""x3"": (""x"", [""a"", ""b""]), ""x"": [1, 2]}, dims=""x"", name=""x""\n        )\n        assert_identical(expected, actual)\n\n    def test_reset_coords(self):\n        data = DataArray(\n            np.zeros((3, 4)),\n            {""bar"": (""x"", [""a"", ""b"", ""c""]), ""baz"": (""y"", range(4)), ""y"": range(4)},\n            dims=[""x"", ""y""],\n            name=""foo"",\n        )\n\n        actual = data.reset_coords()\n        expected = Dataset(\n            {\n                ""foo"": ([""x"", ""y""], np.zeros((3, 4))),\n                ""bar"": (""x"", [""a"", ""b"", ""c""]),\n                ""baz"": (""y"", range(4)),\n                ""y"": range(4),\n            }\n        )\n        assert_identical(actual, expected)\n\n        actual = data.reset_coords([""bar"", ""baz""])\n        assert_identical(actual, expected)\n\n        actual = data.reset_coords(""bar"")\n        expected = Dataset(\n            {""foo"": ([""x"", ""y""], np.zeros((3, 4))), ""bar"": (""x"", [""a"", ""b"", ""c""])},\n            {""baz"": (""y"", range(4)), ""y"": range(4)},\n        )\n        assert_identical(actual, expected)\n\n        actual = data.reset_coords([""bar""])\n        assert_identical(actual, expected)\n\n        actual = data.reset_coords(drop=True)\n        expected = DataArray(\n            np.zeros((3, 4)), coords={""y"": range(4)}, dims=[""x"", ""y""], name=""foo""\n        )\n        assert_identical(actual, expected)\n\n        actual = data.copy()\n        actual = actual.reset_coords(drop=True)\n        assert_identical(actual, expected)\n\n        actual = data.reset_coords(""bar"", drop=True)\n        expected = DataArray(\n            np.zeros((3, 4)),\n            {""baz"": (""y"", range(4)), ""y"": range(4)},\n            dims=[""x"", ""y""],\n            name=""foo"",\n        )\n        assert_identical(actual, expected)\n\n        with pytest.raises(TypeError):\n            data = data.reset_coords(inplace=True)\n        with raises_regex(ValueError, ""cannot be found""):\n            data.reset_coords(""foo"", drop=True)\n        with raises_regex(ValueError, ""cannot be found""):\n            data.reset_coords(""not_found"")\n        with raises_regex(ValueError, ""cannot remove index""):\n            data.reset_coords(""y"")\n\n    def test_assign_coords(self):\n        array = DataArray(10)\n        actual = array.assign_coords(c=42)\n        expected = DataArray(10, {""c"": 42})\n        assert_identical(actual, expected)\n\n        array = DataArray([1, 2, 3, 4], {""c"": (""x"", [0, 0, 1, 1])}, dims=""x"")\n        actual = array.groupby(""c"").assign_coords(d=lambda a: a.mean())\n        expected = array.copy()\n        expected.coords[""d""] = (""x"", [1.5, 1.5, 3.5, 3.5])\n        assert_identical(actual, expected)\n\n        with raises_regex(ValueError, ""conflicting MultiIndex""):\n            self.mda.assign_coords(level_1=range(4))\n\n        # GH: 2112\n        da = xr.DataArray([0, 1, 2], dims=""x"")\n        with pytest.raises(ValueError):\n            da[""x""] = [0, 1, 2, 3]  # size conflict\n        with pytest.raises(ValueError):\n            da.coords[""x""] = [0, 1, 2, 3]  # size conflict\n\n    def test_coords_alignment(self):\n        lhs = DataArray([1, 2, 3], [(""x"", [0, 1, 2])])\n        rhs = DataArray([2, 3, 4], [(""x"", [1, 2, 3])])\n        lhs.coords[""rhs""] = rhs\n\n        expected = DataArray(\n            [1, 2, 3], coords={""rhs"": (""x"", [np.nan, 2, 3]), ""x"": [0, 1, 2]}, dims=""x""\n        )\n        assert_identical(lhs, expected)\n\n    def test_set_coords_update_index(self):\n        actual = DataArray([1, 2, 3], [(""x"", [1, 2, 3])])\n        actual.coords[""x""] = [""a"", ""b"", ""c""]\n        assert actual.indexes[""x""].equals(pd.Index([""a"", ""b"", ""c""]))\n\n    def test_coords_replacement_alignment(self):\n        # regression test for GH725\n        arr = DataArray([0, 1, 2], dims=[""abc""])\n        new_coord = DataArray([1, 2, 3], dims=[""abc""], coords=[[1, 2, 3]])\n        arr[""abc""] = new_coord\n        expected = DataArray([0, 1, 2], coords=[(""abc"", [1, 2, 3])])\n        assert_identical(arr, expected)\n\n    def test_coords_non_string(self):\n        arr = DataArray(0, coords={1: 2})\n        actual = arr.coords[1]\n        expected = DataArray(2, coords={1: 2}, name=1)\n        assert_identical(actual, expected)\n\n    def test_coords_delitem_delete_indexes(self):\n        # regression test for GH3746\n        arr = DataArray(np.ones((2,)), dims=""x"", coords={""x"": [0, 1]})\n        del arr.coords[""x""]\n        assert ""x"" not in arr.indexes\n\n    def test_broadcast_like(self):\n        arr1 = DataArray(\n            np.ones((2, 3)),\n            dims=[""x"", ""y""],\n            coords={""x"": [""a"", ""b""], ""y"": [""a"", ""b"", ""c""]},\n        )\n        arr2 = DataArray(\n            np.ones((3, 2)),\n            dims=[""x"", ""y""],\n            coords={""x"": [""a"", ""b"", ""c""], ""y"": [""a"", ""b""]},\n        )\n        orig1, orig2 = broadcast(arr1, arr2)\n        new1 = arr1.broadcast_like(arr2)\n        new2 = arr2.broadcast_like(arr1)\n\n        assert orig1.identical(new1)\n        assert orig2.identical(new2)\n\n        orig3 = DataArray(np.random.randn(5), [(""x"", range(5))])\n        orig4 = DataArray(np.random.randn(6), [(""y"", range(6))])\n        new3, new4 = broadcast(orig3, orig4)\n\n        assert_identical(orig3.broadcast_like(orig4), new3.transpose(""y"", ""x""))\n        assert_identical(orig4.broadcast_like(orig3), new4)\n\n    def test_reindex_like(self):\n        foo = DataArray(np.random.randn(5, 6), [(""x"", range(5)), (""y"", range(6))])\n        bar = foo[:2, :2]\n        assert_identical(foo.reindex_like(bar), bar)\n\n        expected = foo.copy()\n        expected[:] = np.nan\n        expected[:2, :2] = bar\n        assert_identical(bar.reindex_like(foo), expected)\n\n    def test_reindex_like_no_index(self):\n        foo = DataArray(np.random.randn(5, 6), dims=[""x"", ""y""])\n        assert_identical(foo, foo.reindex_like(foo))\n\n        bar = foo[:4]\n        with raises_regex(ValueError, ""different size for unlabeled""):\n            foo.reindex_like(bar)\n\n    def test_reindex_regressions(self):\n        da = DataArray(np.random.randn(5), coords=[(""time"", range(5))])\n        time2 = DataArray(np.arange(5), dims=""time2"")\n        with pytest.raises(ValueError):\n            da.reindex(time=time2)\n\n        # regression test for #736, reindex can not change complex nums dtype\n        x = np.array([1, 2, 3], dtype=np.complex)\n        x = DataArray(x, coords=[[0.1, 0.2, 0.3]])\n        y = DataArray([2, 5, 6, 7, 8], coords=[[-1.1, 0.21, 0.31, 0.41, 0.51]])\n        re_dtype = x.reindex_like(y, method=""pad"").dtype\n        assert x.dtype == re_dtype\n\n    def test_reindex_method(self):\n        x = DataArray([10, 20], dims=""y"", coords={""y"": [0, 1]})\n        y = [-0.1, 0.5, 1.1]\n        actual = x.reindex(y=y, method=""backfill"", tolerance=0.2)\n        expected = DataArray([10, np.nan, np.nan], coords=[(""y"", y)])\n        assert_identical(expected, actual)\n\n        alt = Dataset({""y"": y})\n        actual = x.reindex_like(alt, method=""backfill"")\n        expected = DataArray([10, 20, np.nan], coords=[(""y"", y)])\n        assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(""fill_value"", [dtypes.NA, 2, 2.0])\n    def test_reindex_fill_value(self, fill_value):\n        x = DataArray([10, 20], dims=""y"", coords={""y"": [0, 1]})\n        y = [0, 1, 2]\n        if fill_value == dtypes.NA:\n            # if we supply the default, we expect the missing value for a\n            # float array\n            fill_value = np.nan\n        actual = x.reindex(y=y, fill_value=fill_value)\n        expected = DataArray([10, 20, fill_value], coords=[(""y"", y)])\n        assert_identical(expected, actual)\n\n    def test_rename(self):\n        renamed = self.dv.rename(""bar"")\n        assert_identical(renamed.to_dataset(), self.ds.rename({""foo"": ""bar""}))\n        assert renamed.name == ""bar""\n\n        renamed = self.dv.x.rename({""x"": ""z""}).rename(""z"")\n        assert_identical(renamed, self.ds.rename({""x"": ""z""}).z)\n        assert renamed.name == ""z""\n        assert renamed.dims == (""z"",)\n\n        renamed_kwargs = self.dv.x.rename(x=""z"").rename(""z"")\n        assert_identical(renamed, renamed_kwargs)\n\n    def test_init_value(self):\n        expected = DataArray(\n            np.full((3, 4), 3), dims=[""x"", ""y""], coords=[range(3), range(4)]\n        )\n        actual = DataArray(3, dims=[""x"", ""y""], coords=[range(3), range(4)])\n        assert_identical(expected, actual)\n\n        expected = DataArray(\n            np.full((1, 10, 2), 0),\n            dims=[""w"", ""x"", ""y""],\n            coords={""x"": np.arange(10), ""y"": [""north"", ""south""]},\n        )\n        actual = DataArray(0, dims=expected.dims, coords=expected.coords)\n        assert_identical(expected, actual)\n\n        expected = DataArray(\n            np.full((10, 2), np.nan), coords=[(""x"", np.arange(10)), (""y"", [""a"", ""b""])]\n        )\n        actual = DataArray(coords=[(""x"", np.arange(10)), (""y"", [""a"", ""b""])])\n        assert_identical(expected, actual)\n\n        with raises_regex(ValueError, ""different number of dim""):\n            DataArray(np.array(1), coords={""x"": np.arange(10)}, dims=[""x""])\n        with raises_regex(ValueError, ""does not match the 0 dim""):\n            DataArray(np.array(1), coords=[(""x"", np.arange(10))])\n\n    def test_swap_dims(self):\n        array = DataArray(np.random.randn(3), {""y"": (""x"", list(""abc""))}, ""x"")\n        expected = DataArray(array.values, {""y"": list(""abc"")}, dims=""y"")\n        actual = array.swap_dims({""x"": ""y""})\n        assert_identical(expected, actual)\n        for dim_name in set().union(expected.indexes.keys(), actual.indexes.keys()):\n            pd.testing.assert_index_equal(\n                expected.indexes[dim_name], actual.indexes[dim_name]\n            )\n\n        array = DataArray(np.random.randn(3), {""x"": list(""abc"")}, ""x"")\n        expected = DataArray(array.values, {""x"": (""y"", list(""abc""))}, dims=""y"")\n        actual = array.swap_dims({""x"": ""y""})\n        assert_identical(expected, actual)\n        for dim_name in set().union(expected.indexes.keys(), actual.indexes.keys()):\n            pd.testing.assert_index_equal(\n                expected.indexes[dim_name], actual.indexes[dim_name]\n            )\n\n        # multiindex case\n        idx = pd.MultiIndex.from_arrays([list(""aab""), list(""yzz"")], names=[""y1"", ""y2""])\n        array = DataArray(np.random.randn(3), {""y"": (""x"", idx)}, ""x"")\n        expected = DataArray(array.values, {""y"": idx}, ""y"")\n        actual = array.swap_dims({""x"": ""y""})\n        assert_identical(expected, actual)\n        for dim_name in set().union(expected.indexes.keys(), actual.indexes.keys()):\n            pd.testing.assert_index_equal(\n                expected.indexes[dim_name], actual.indexes[dim_name]\n            )\n\n    def test_expand_dims_error(self):\n        array = DataArray(\n            np.random.randn(3, 4),\n            dims=[""x"", ""dim_0""],\n            coords={""x"": np.linspace(0.0, 1.0, 3)},\n            attrs={""key"": ""entry""},\n        )\n\n        with raises_regex(TypeError, ""dim should be hashable or""):\n            array.expand_dims(0)\n        with raises_regex(ValueError, ""lengths of dim and axis""):\n            # dims and axis argument should be the same length\n            array.expand_dims(dim=[""a"", ""b""], axis=[1, 2, 3])\n        with raises_regex(ValueError, ""Dimension x already""):\n            # Should not pass the already existing dimension.\n            array.expand_dims(dim=[""x""])\n        # raise if duplicate\n        with raises_regex(ValueError, ""duplicate values""):\n            array.expand_dims(dim=[""y"", ""y""])\n        with raises_regex(ValueError, ""duplicate values""):\n            array.expand_dims(dim=[""y"", ""z""], axis=[1, 1])\n        with raises_regex(ValueError, ""duplicate values""):\n            array.expand_dims(dim=[""y"", ""z""], axis=[2, -2])\n\n        # out of bounds error, axis must be in [-4, 3]\n        with pytest.raises(IndexError):\n            array.expand_dims(dim=[""y"", ""z""], axis=[2, 4])\n        with pytest.raises(IndexError):\n            array.expand_dims(dim=[""y"", ""z""], axis=[2, -5])\n        # Does not raise an IndexError\n        array.expand_dims(dim=[""y"", ""z""], axis=[2, -4])\n        array.expand_dims(dim=[""y"", ""z""], axis=[2, 3])\n\n        array = DataArray(\n            np.random.randn(3, 4),\n            dims=[""x"", ""dim_0""],\n            coords={""x"": np.linspace(0.0, 1.0, 3)},\n            attrs={""key"": ""entry""},\n        )\n        with pytest.raises(TypeError):\n            array.expand_dims({""new_dim"": 3.2})\n\n        # Attempt to use both dim and kwargs\n        with pytest.raises(ValueError):\n            array.expand_dims({""d"": 4}, e=4)\n\n    def test_expand_dims(self):\n        array = DataArray(\n            np.random.randn(3, 4),\n            dims=[""x"", ""dim_0""],\n            coords={""x"": np.linspace(0.0, 1.0, 3)},\n            attrs={""key"": ""entry""},\n        )\n        # pass only dim label\n        actual = array.expand_dims(dim=""y"")\n        expected = DataArray(\n            np.expand_dims(array.values, 0),\n            dims=[""y"", ""x"", ""dim_0""],\n            coords={""x"": np.linspace(0.0, 1.0, 3)},\n            attrs={""key"": ""entry""},\n        )\n        assert_identical(expected, actual)\n        roundtripped = actual.squeeze(""y"", drop=True)\n        assert_identical(array, roundtripped)\n\n        # pass multiple dims\n        actual = array.expand_dims(dim=[""y"", ""z""])\n        expected = DataArray(\n            np.expand_dims(np.expand_dims(array.values, 0), 0),\n            dims=[""y"", ""z"", ""x"", ""dim_0""],\n            coords={""x"": np.linspace(0.0, 1.0, 3)},\n            attrs={""key"": ""entry""},\n        )\n        assert_identical(expected, actual)\n        roundtripped = actual.squeeze([""y"", ""z""], drop=True)\n        assert_identical(array, roundtripped)\n\n        # pass multiple dims and axis. Axis is out of order\n        actual = array.expand_dims(dim=[""z"", ""y""], axis=[2, 1])\n        expected = DataArray(\n            np.expand_dims(np.expand_dims(array.values, 1), 2),\n            dims=[""x"", ""y"", ""z"", ""dim_0""],\n            coords={""x"": np.linspace(0.0, 1.0, 3)},\n            attrs={""key"": ""entry""},\n        )\n        assert_identical(expected, actual)\n        # make sure the attrs are tracked\n        assert actual.attrs[""key""] == ""entry""\n        roundtripped = actual.squeeze([""z"", ""y""], drop=True)\n        assert_identical(array, roundtripped)\n\n        # Negative axis and they are out of order\n        actual = array.expand_dims(dim=[""y"", ""z""], axis=[-1, -2])\n        expected = DataArray(\n            np.expand_dims(np.expand_dims(array.values, -1), -1),\n            dims=[""x"", ""dim_0"", ""z"", ""y""],\n            coords={""x"": np.linspace(0.0, 1.0, 3)},\n            attrs={""key"": ""entry""},\n        )\n        assert_identical(expected, actual)\n        assert actual.attrs[""key""] == ""entry""\n        roundtripped = actual.squeeze([""y"", ""z""], drop=True)\n        assert_identical(array, roundtripped)\n\n    def test_expand_dims_with_scalar_coordinate(self):\n        array = DataArray(\n            np.random.randn(3, 4),\n            dims=[""x"", ""dim_0""],\n            coords={""x"": np.linspace(0.0, 1.0, 3), ""z"": 1.0},\n            attrs={""key"": ""entry""},\n        )\n        actual = array.expand_dims(dim=""z"")\n        expected = DataArray(\n            np.expand_dims(array.values, 0),\n            dims=[""z"", ""x"", ""dim_0""],\n            coords={""x"": np.linspace(0.0, 1.0, 3), ""z"": np.ones(1)},\n            attrs={""key"": ""entry""},\n        )\n        assert_identical(expected, actual)\n        roundtripped = actual.squeeze([""z""], drop=False)\n        assert_identical(array, roundtripped)\n\n    def test_expand_dims_with_greater_dim_size(self):\n        array = DataArray(\n            np.random.randn(3, 4),\n            dims=[""x"", ""dim_0""],\n            coords={""x"": np.linspace(0.0, 1.0, 3), ""z"": 1.0},\n            attrs={""key"": ""entry""},\n        )\n        actual = array.expand_dims({""y"": 2, ""z"": 1, ""dim_1"": [""a"", ""b"", ""c""]})\n\n        expected_coords = {\n            ""y"": [0, 1],\n            ""z"": [1.0],\n            ""dim_1"": [""a"", ""b"", ""c""],\n            ""x"": np.linspace(0, 1, 3),\n            ""dim_0"": range(4),\n        }\n        expected = DataArray(\n            array.values * np.ones([2, 1, 3, 3, 4]),\n            coords=expected_coords,\n            dims=list(expected_coords.keys()),\n            attrs={""key"": ""entry""},\n        ).drop_vars([""y"", ""dim_0""])\n        assert_identical(expected, actual)\n\n        # Test with kwargs instead of passing dict to dim arg.\n\n        other_way = array.expand_dims(dim_1=[""a"", ""b"", ""c""])\n\n        other_way_expected = DataArray(\n            array.values * np.ones([3, 3, 4]),\n            coords={\n                ""dim_1"": [""a"", ""b"", ""c""],\n                ""x"": np.linspace(0, 1, 3),\n                ""dim_0"": range(4),\n                ""z"": 1.0,\n            },\n            dims=[""dim_1"", ""x"", ""dim_0""],\n            attrs={""key"": ""entry""},\n        ).drop_vars(""dim_0"")\n        assert_identical(other_way_expected, other_way)\n\n    def test_set_index(self):\n        indexes = [self.mindex.get_level_values(n) for n in self.mindex.names]\n        coords = {idx.name: (""x"", idx) for idx in indexes}\n        array = DataArray(self.mda.values, coords=coords, dims=""x"")\n        expected = self.mda.copy()\n        level_3 = (""x"", [1, 2, 3, 4])\n        array[""level_3""] = level_3\n        expected[""level_3""] = level_3\n\n        obj = array.set_index(x=self.mindex.names)\n        assert_identical(obj, expected)\n\n        obj = obj.set_index(x=""level_3"", append=True)\n        expected = array.set_index(x=[""level_1"", ""level_2"", ""level_3""])\n        assert_identical(obj, expected)\n\n        array = array.set_index(x=[""level_1"", ""level_2"", ""level_3""])\n        assert_identical(array, expected)\n\n        array2d = DataArray(\n            np.random.rand(2, 2),\n            coords={""x"": (""x"", [0, 1]), ""level"": (""y"", [1, 2])},\n            dims=(""x"", ""y""),\n        )\n        with raises_regex(ValueError, ""dimension mismatch""):\n            array2d.set_index(x=""level"")\n\n        # Issue 3176: Ensure clear error message on key error.\n        with pytest.raises(ValueError) as excinfo:\n            obj.set_index(x=""level_4"")\n        assert str(excinfo.value) == ""level_4 is not the name of an existing variable.""\n\n    def test_reset_index(self):\n        indexes = [self.mindex.get_level_values(n) for n in self.mindex.names]\n        coords = {idx.name: (""x"", idx) for idx in indexes}\n        expected = DataArray(self.mda.values, coords=coords, dims=""x"")\n\n        obj = self.mda.reset_index(""x"")\n        assert_identical(obj, expected)\n        obj = self.mda.reset_index(self.mindex.names)\n        assert_identical(obj, expected)\n        obj = self.mda.reset_index([""x"", ""level_1""])\n        assert_identical(obj, expected)\n\n        coords = {\n            ""x"": (""x"", self.mindex.droplevel(""level_1"")),\n            ""level_1"": (""x"", self.mindex.get_level_values(""level_1"")),\n        }\n        expected = DataArray(self.mda.values, coords=coords, dims=""x"")\n        obj = self.mda.reset_index([""level_1""])\n        assert_identical(obj, expected)\n\n        expected = DataArray(self.mda.values, dims=""x"")\n        obj = self.mda.reset_index(""x"", drop=True)\n        assert_identical(obj, expected)\n\n        array = self.mda.copy()\n        array = array.reset_index([""x""], drop=True)\n        assert_identical(array, expected)\n\n        # single index\n        array = DataArray([1, 2], coords={""x"": [""a"", ""b""]}, dims=""x"")\n        expected = DataArray([1, 2], coords={""x_"": (""x"", [""a"", ""b""])}, dims=""x"")\n        assert_identical(array.reset_index(""x""), expected)\n\n    def test_reset_index_keep_attrs(self):\n        coord_1 = DataArray([1, 2], dims=[""coord_1""], attrs={""attrs"": True})\n        da = DataArray([1, 0], [coord_1])\n        expected = DataArray([1, 0], {""coord_1_"": coord_1}, dims=[""coord_1""])\n        obj = da.reset_index(""coord_1"")\n        assert_identical(expected, obj)\n\n    def test_reorder_levels(self):\n        midx = self.mindex.reorder_levels([""level_2"", ""level_1""])\n        expected = DataArray(self.mda.values, coords={""x"": midx}, dims=""x"")\n\n        obj = self.mda.reorder_levels(x=[""level_2"", ""level_1""])\n        assert_identical(obj, expected)\n\n        with pytest.raises(TypeError):\n            array = self.mda.copy()\n            array.reorder_levels(x=[""level_2"", ""level_1""], inplace=True)\n\n        array = DataArray([1, 2], dims=""x"")\n        with pytest.raises(KeyError):\n            array.reorder_levels(x=[""level_1"", ""level_2""])\n\n        array[""x""] = [0, 1]\n        with raises_regex(ValueError, ""has no MultiIndex""):\n            array.reorder_levels(x=[""level_1"", ""level_2""])\n\n    def test_dataset_getitem(self):\n        dv = self.ds[""foo""]\n        assert_identical(dv, self.dv)\n\n    def test_array_interface(self):\n        assert_array_equal(np.asarray(self.dv), self.x)\n        # test patched in methods\n        assert_array_equal(self.dv.astype(float), self.v.astype(float))\n        assert_array_equal(self.dv.argsort(), self.v.argsort())\n        assert_array_equal(self.dv.clip(2, 3), self.v.clip(2, 3))\n        # test ufuncs\n        expected = deepcopy(self.ds)\n        expected[""foo""][:] = np.sin(self.x)\n        assert_equal(expected[""foo""], np.sin(self.dv))\n        assert_array_equal(self.dv, np.maximum(self.v, self.dv))\n        bar = Variable([""x"", ""y""], np.zeros((10, 20)))\n        assert_equal(self.dv, np.maximum(self.dv, bar))\n\n    def test_is_null(self):\n        x = np.random.RandomState(42).randn(5, 6)\n        x[x < 0] = np.nan\n        original = DataArray(x, [-np.arange(5), np.arange(6)], [""x"", ""y""])\n        expected = DataArray(pd.isnull(x), [-np.arange(5), np.arange(6)], [""x"", ""y""])\n        assert_identical(expected, original.isnull())\n        assert_identical(~expected, original.notnull())\n\n    def test_math(self):\n        x = self.x\n        v = self.v\n        a = self.dv\n        # variable math was already tested extensively, so let\'s just make sure\n        # that all types are properly converted here\n        assert_equal(a, +a)\n        assert_equal(a, a + 0)\n        assert_equal(a, 0 + a)\n        assert_equal(a, a + 0 * v)\n        assert_equal(a, 0 * v + a)\n        assert_equal(a, a + 0 * x)\n        assert_equal(a, 0 * x + a)\n        assert_equal(a, a + 0 * a)\n        assert_equal(a, 0 * a + a)\n\n    def test_math_automatic_alignment(self):\n        a = DataArray(range(5), [(""x"", range(5))])\n        b = DataArray(range(5), [(""x"", range(1, 6))])\n        expected = DataArray(np.ones(4), [(""x"", [1, 2, 3, 4])])\n        assert_identical(a - b, expected)\n\n    def test_non_overlapping_dataarrays_return_empty_result(self):\n\n        a = DataArray(range(5), [(""x"", range(5))])\n        result = a.isel(x=slice(2)) + a.isel(x=slice(2, None))\n        assert len(result[""x""]) == 0\n\n    def test_empty_dataarrays_return_empty_result(self):\n\n        a = DataArray(data=[])\n        result = a * a\n        assert len(result[""dim_0""]) == 0\n\n    def test_inplace_math_basics(self):\n        x = self.x\n        a = self.dv\n        v = a.variable\n        b = a\n        b += 1\n        assert b is a\n        assert b.variable is v\n        assert_array_equal(b.values, x)\n        assert source_ndarray(b.values) is x\n\n    def test_inplace_math_automatic_alignment(self):\n        a = DataArray(range(5), [(""x"", range(5))])\n        b = DataArray(range(1, 6), [(""x"", range(1, 6))])\n        with pytest.raises(xr.MergeError):\n            a += b\n        with pytest.raises(xr.MergeError):\n            b += a\n\n    def test_math_name(self):\n        # Verify that name is preserved only when it can be done unambiguously.\n        # The rule (copied from pandas.Series) is keep the current name only if\n        # the other object has the same name or no name attribute and this\n        # object isn\'t a coordinate; otherwise reset to None.\n        a = self.dv\n        assert (+a).name == ""foo""\n        assert (a + 0).name == ""foo""\n        assert (a + a.rename(None)).name is None\n        assert (a + a.rename(""bar"")).name is None\n        assert (a + a).name == ""foo""\n        assert (+a[""x""]).name == ""x""\n        assert (a[""x""] + 0).name == ""x""\n        assert (a + a[""x""]).name is None\n\n    def test_math_with_coords(self):\n        coords = {\n            ""x"": [-1, -2],\n            ""y"": [""ab"", ""cd"", ""ef""],\n            ""lat"": ([""x"", ""y""], [[1, 2, 3], [-1, -2, -3]]),\n            ""c"": -999,\n        }\n        orig = DataArray(np.random.randn(2, 3), coords, dims=[""x"", ""y""])\n\n        actual = orig + 1\n        expected = DataArray(orig.values + 1, orig.coords)\n        assert_identical(expected, actual)\n\n        actual = 1 + orig\n        assert_identical(expected, actual)\n\n        actual = orig + orig[0, 0]\n        exp_coords = {k: v for k, v in coords.items() if k != ""lat""}\n        expected = DataArray(\n            orig.values + orig.values[0, 0], exp_coords, dims=[""x"", ""y""]\n        )\n        assert_identical(expected, actual)\n\n        actual = orig[0, 0] + orig\n        assert_identical(expected, actual)\n\n        actual = orig[0, 0] + orig[-1, -1]\n        expected = DataArray(orig.values[0, 0] + orig.values[-1, -1], {""c"": -999})\n        assert_identical(expected, actual)\n\n        actual = orig[:, 0] + orig[0, :]\n        exp_values = orig[:, 0].values[:, None] + orig[0, :].values[None, :]\n        expected = DataArray(exp_values, exp_coords, dims=[""x"", ""y""])\n        assert_identical(expected, actual)\n\n        actual = orig[0, :] + orig[:, 0]\n        assert_identical(expected.transpose(transpose_coords=True), actual)\n\n        actual = orig - orig.transpose(transpose_coords=True)\n        expected = DataArray(np.zeros((2, 3)), orig.coords)\n        assert_identical(expected, actual)\n\n        actual = orig.transpose(transpose_coords=True) - orig\n        assert_identical(expected.transpose(transpose_coords=True), actual)\n\n        alt = DataArray([1, 1], {""x"": [-1, -2], ""c"": ""foo"", ""d"": 555}, ""x"")\n        actual = orig + alt\n        expected = orig + 1\n        expected.coords[""d""] = 555\n        del expected.coords[""c""]\n        assert_identical(expected, actual)\n\n        actual = alt + orig\n        assert_identical(expected, actual)\n\n    def test_index_math(self):\n        orig = DataArray(range(3), dims=""x"", name=""x"")\n        actual = orig + 1\n        expected = DataArray(1 + np.arange(3), dims=""x"", name=""x"")\n        assert_identical(expected, actual)\n\n        # regression tests for #254\n        actual = orig[0] < orig\n        expected = DataArray([False, True, True], dims=""x"", name=""x"")\n        assert_identical(expected, actual)\n\n        actual = orig > orig[0]\n        assert_identical(expected, actual)\n\n    def test_dataset_math(self):\n        # more comprehensive tests with multiple dataset variables\n        obs = Dataset(\n            {""tmin"": (""x"", np.arange(5)), ""tmax"": (""x"", 10 + np.arange(5))},\n            {""x"": (""x"", 0.5 * np.arange(5)), ""loc"": (""x"", range(-2, 3))},\n        )\n\n        actual = 2 * obs[""tmax""]\n        expected = DataArray(2 * (10 + np.arange(5)), obs.coords, name=""tmax"")\n        assert_identical(actual, expected)\n\n        actual = obs[""tmax""] - obs[""tmin""]\n        expected = DataArray(10 * np.ones(5), obs.coords)\n        assert_identical(actual, expected)\n\n        sim = Dataset(\n            {\n                ""tmin"": (""x"", 1 + np.arange(5)),\n                ""tmax"": (""x"", 11 + np.arange(5)),\n                # does *not* include \'loc\' as a coordinate\n                ""x"": (""x"", 0.5 * np.arange(5)),\n            }\n        )\n\n        actual = sim[""tmin""] - obs[""tmin""]\n        expected = DataArray(np.ones(5), obs.coords, name=""tmin"")\n        assert_identical(actual, expected)\n\n        actual = -obs[""tmin""] + sim[""tmin""]\n        assert_identical(actual, expected)\n\n        actual = sim[""tmin""].copy()\n        actual -= obs[""tmin""]\n        assert_identical(actual, expected)\n\n        actual = sim.copy()\n        actual[""tmin""] = sim[""tmin""] - obs[""tmin""]\n        expected = Dataset(\n            {""tmin"": (""x"", np.ones(5)), ""tmax"": (""x"", sim[""tmax""].values)}, obs.coords\n        )\n        assert_identical(actual, expected)\n\n        actual = sim.copy()\n        actual[""tmin""] -= obs[""tmin""]\n        assert_identical(actual, expected)\n\n    def test_stack_unstack(self):\n        orig = DataArray([[0, 1], [2, 3]], dims=[""x"", ""y""], attrs={""foo"": 2})\n        assert_identical(orig, orig.unstack())\n\n        # test GH3000\n        a = orig[:0, :1].stack(dim=(""x"", ""y"")).dim.to_index()\n        if pd.__version__ < ""0.24.0"":\n            b = pd.MultiIndex(\n                levels=[pd.Int64Index([]), pd.Int64Index([0])],\n                labels=[[], []],\n                names=[""x"", ""y""],\n            )\n        else:\n            b = pd.MultiIndex(\n                levels=[pd.Int64Index([]), pd.Int64Index([0])],\n                codes=[[], []],\n                names=[""x"", ""y""],\n            )\n        pd.testing.assert_index_equal(a, b)\n\n        actual = orig.stack(z=[""x"", ""y""]).unstack(""z"").drop_vars([""x"", ""y""])\n        assert_identical(orig, actual)\n\n        actual = orig.stack(z=[...]).unstack(""z"").drop_vars([""x"", ""y""])\n        assert_identical(orig, actual)\n\n        dims = [""a"", ""b"", ""c"", ""d"", ""e""]\n        orig = xr.DataArray(np.random.rand(1, 2, 3, 2, 1), dims=dims)\n        stacked = orig.stack(ab=[""a"", ""b""], cd=[""c"", ""d""])\n\n        unstacked = stacked.unstack([""ab"", ""cd""])\n        roundtripped = unstacked.drop_vars([""a"", ""b"", ""c"", ""d""]).transpose(*dims)\n        assert_identical(orig, roundtripped)\n\n        unstacked = stacked.unstack()\n        roundtripped = unstacked.drop_vars([""a"", ""b"", ""c"", ""d""]).transpose(*dims)\n        assert_identical(orig, roundtripped)\n\n    def test_stack_unstack_decreasing_coordinate(self):\n        # regression test for GH980\n        orig = DataArray(\n            np.random.rand(3, 4),\n            dims=(""y"", ""x""),\n            coords={""x"": np.arange(4), ""y"": np.arange(3, 0, -1)},\n        )\n        stacked = orig.stack(allpoints=[""y"", ""x""])\n        actual = stacked.unstack(""allpoints"")\n        assert_identical(orig, actual)\n\n    def test_unstack_pandas_consistency(self):\n        df = pd.DataFrame({""foo"": range(3), ""x"": [""a"", ""b"", ""b""], ""y"": [0, 0, 1]})\n        s = df.set_index([""x"", ""y""])[""foo""]\n        expected = DataArray(s.unstack(), name=""foo"")\n        actual = DataArray(s, dims=""z"").unstack(""z"")\n        assert_identical(expected, actual)\n\n    def test_stack_nonunique_consistency(self):\n        orig = DataArray(\n            [[0, 1], [2, 3]], dims=[""x"", ""y""], coords={""x"": [0, 1], ""y"": [0, 0]}\n        )\n        actual = orig.stack(z=[""x"", ""y""])\n        expected = DataArray(orig.to_pandas().stack(), dims=""z"")\n        assert_identical(expected, actual)\n\n    def test_to_unstacked_dataset_raises_value_error(self):\n        data = DataArray([0, 1], dims=""x"", coords={""x"": [0, 1]})\n        with pytest.raises(ValueError, match=""\'x\' is not a stacked coordinate""):\n            data.to_unstacked_dataset(""x"", 0)\n\n    def test_transpose(self):\n        da = DataArray(\n            np.random.randn(3, 4, 5),\n            dims=(""x"", ""y"", ""z""),\n            coords={\n                ""x"": range(3),\n                ""y"": range(4),\n                ""z"": range(5),\n                ""xy"": ((""x"", ""y""), np.random.randn(3, 4)),\n            },\n        )\n\n        actual = da.transpose(transpose_coords=False)\n        expected = DataArray(da.values.T, dims=(""z"", ""y"", ""x""), coords=da.coords)\n        assert_equal(expected, actual)\n\n        actual = da.transpose(""z"", ""y"", ""x"", transpose_coords=True)\n        expected = DataArray(\n            da.values.T,\n            dims=(""z"", ""y"", ""x""),\n            coords={\n                ""x"": da.x.values,\n                ""y"": da.y.values,\n                ""z"": da.z.values,\n                ""xy"": ((""y"", ""x""), da.xy.values.T),\n            },\n        )\n        assert_equal(expected, actual)\n\n        # same as previous but with ellipsis\n        actual = da.transpose(""z"", ..., ""x"", transpose_coords=True)\n        assert_equal(expected, actual)\n\n        with pytest.raises(ValueError):\n            da.transpose(""x"", ""y"")\n\n    def test_squeeze(self):\n        assert_equal(self.dv.variable.squeeze(), self.dv.squeeze().variable)\n\n    def test_squeeze_drop(self):\n        array = DataArray([1], [(""x"", [0])])\n        expected = DataArray(1)\n        actual = array.squeeze(drop=True)\n        assert_identical(expected, actual)\n\n        expected = DataArray(1, {""x"": 0})\n        actual = array.squeeze(drop=False)\n        assert_identical(expected, actual)\n\n        array = DataArray([[[0.0, 1.0]]], dims=[""dim_0"", ""dim_1"", ""dim_2""])\n        expected = DataArray([[0.0, 1.0]], dims=[""dim_1"", ""dim_2""])\n        actual = array.squeeze(axis=0)\n        assert_identical(expected, actual)\n\n        array = DataArray([[[[0.0, 1.0]]]], dims=[""dim_0"", ""dim_1"", ""dim_2"", ""dim_3""])\n        expected = DataArray([[0.0, 1.0]], dims=[""dim_1"", ""dim_3""])\n        actual = array.squeeze(axis=(0, 2))\n        assert_identical(expected, actual)\n\n        array = DataArray([[[0.0, 1.0]]], dims=[""dim_0"", ""dim_1"", ""dim_2""])\n        with pytest.raises(ValueError):\n            array.squeeze(axis=0, dim=""dim_1"")\n\n    def test_drop_coordinates(self):\n        expected = DataArray(np.random.randn(2, 3), dims=[""x"", ""y""])\n        arr = expected.copy()\n        arr.coords[""z""] = 2\n        actual = arr.drop_vars(""z"")\n        assert_identical(expected, actual)\n\n        with pytest.raises(ValueError):\n            arr.drop_vars(""not found"")\n\n        actual = expected.drop_vars(""not found"", errors=""ignore"")\n        assert_identical(actual, expected)\n\n        with raises_regex(ValueError, ""cannot be found""):\n            arr.drop_vars(""w"")\n\n        actual = expected.drop_vars(""w"", errors=""ignore"")\n        assert_identical(actual, expected)\n\n        renamed = arr.rename(""foo"")\n        with raises_regex(ValueError, ""cannot be found""):\n            renamed.drop_vars(""foo"")\n\n        actual = renamed.drop_vars(""foo"", errors=""ignore"")\n        assert_identical(actual, renamed)\n\n    def test_drop_index_labels(self):\n        arr = DataArray(np.random.randn(2, 3), coords={""y"": [0, 1, 2]}, dims=[""x"", ""y""])\n        actual = arr.drop_sel(y=[0, 1])\n        expected = arr[:, 2:]\n        assert_identical(actual, expected)\n\n        with raises_regex((KeyError, ValueError), ""not .* in axis""):\n            actual = arr.drop_sel(y=[0, 1, 3])\n\n        actual = arr.drop_sel(y=[0, 1, 3], errors=""ignore"")\n        assert_identical(actual, expected)\n\n        with pytest.warns(DeprecationWarning):\n            arr.drop([0, 1, 3], dim=""y"", errors=""ignore"")\n\n    def test_dropna(self):\n        x = np.random.randn(4, 4)\n        x[::2, 0] = np.nan\n        arr = DataArray(x, dims=[""a"", ""b""])\n\n        actual = arr.dropna(""a"")\n        expected = arr[1::2]\n        assert_identical(actual, expected)\n\n        actual = arr.dropna(""b"", how=""all"")\n        assert_identical(actual, arr)\n\n        actual = arr.dropna(""a"", thresh=1)\n        assert_identical(actual, arr)\n\n        actual = arr.dropna(""b"", thresh=3)\n        expected = arr[:, 1:]\n        assert_identical(actual, expected)\n\n    def test_where(self):\n        arr = DataArray(np.arange(4), dims=""x"")\n        expected = arr.sel(x=slice(2))\n        actual = arr.where(arr.x < 2, drop=True)\n        assert_identical(actual, expected)\n\n    def test_where_lambda(self):\n        arr = DataArray(np.arange(4), dims=""y"")\n        expected = arr.sel(y=slice(2))\n        actual = arr.where(lambda x: x.y < 2, drop=True)\n        assert_identical(actual, expected)\n\n    def test_where_string(self):\n        array = DataArray([""a"", ""b""])\n        expected = DataArray(np.array([""a"", np.nan], dtype=object))\n        actual = array.where([True, False])\n        assert_identical(actual, expected)\n\n    def test_cumops(self):\n        coords = {\n            ""x"": [-1, -2],\n            ""y"": [""ab"", ""cd"", ""ef""],\n            ""lat"": ([""x"", ""y""], [[1, 2, 3], [-1, -2, -3]]),\n            ""c"": -999,\n        }\n        orig = DataArray([[-1, 0, 1], [-3, 0, 3]], coords, dims=[""x"", ""y""])\n\n        actual = orig.cumsum()\n        expected = DataArray([[-1, -1, 0], [-4, -4, 0]], coords, dims=[""x"", ""y""])\n        assert_identical(expected, actual)\n\n        actual = orig.cumsum(""x"")\n        expected = DataArray([[-1, 0, 1], [-4, 0, 4]], coords, dims=[""x"", ""y""])\n        assert_identical(expected, actual)\n\n        actual = orig.cumsum(""y"")\n        expected = DataArray([[-1, -1, 0], [-3, -3, 0]], coords, dims=[""x"", ""y""])\n        assert_identical(expected, actual)\n\n        actual = orig.cumprod(""x"")\n        expected = DataArray([[-1, 0, 1], [3, 0, 3]], coords, dims=[""x"", ""y""])\n        assert_identical(expected, actual)\n\n        actual = orig.cumprod(""y"")\n        expected = DataArray([[-1, 0, 0], [-3, 0, 0]], coords, dims=[""x"", ""y""])\n        assert_identical(expected, actual)\n\n    def test_reduce(self):\n        coords = {\n            ""x"": [-1, -2],\n            ""y"": [""ab"", ""cd"", ""ef""],\n            ""lat"": ([""x"", ""y""], [[1, 2, 3], [-1, -2, -3]]),\n            ""c"": -999,\n        }\n        orig = DataArray([[-1, 0, 1], [-3, 0, 3]], coords, dims=[""x"", ""y""])\n\n        actual = orig.mean()\n        expected = DataArray(0, {""c"": -999})\n        assert_identical(expected, actual)\n\n        actual = orig.mean([""x"", ""y""])\n        assert_identical(expected, actual)\n\n        actual = orig.mean(""x"")\n        expected = DataArray([-2, 0, 2], {""y"": coords[""y""], ""c"": -999}, ""y"")\n        assert_identical(expected, actual)\n\n        actual = orig.mean([""x""])\n        assert_identical(expected, actual)\n\n        actual = orig.mean(""y"")\n        expected = DataArray([0, 0], {""x"": coords[""x""], ""c"": -999}, ""x"")\n        assert_identical(expected, actual)\n\n        assert_equal(self.dv.reduce(np.mean, ""x"").variable, self.v.reduce(np.mean, ""x""))\n\n        orig = DataArray([[1, 0, np.nan], [3, 0, 3]], coords, dims=[""x"", ""y""])\n        actual = orig.count()\n        expected = DataArray(5, {""c"": -999})\n        assert_identical(expected, actual)\n\n        # uint support\n        orig = DataArray(np.arange(6).reshape(3, 2).astype(""uint""), dims=[""x"", ""y""])\n        assert orig.dtype.kind == ""u""\n        actual = orig.mean(dim=""x"", skipna=True)\n        expected = DataArray(orig.values.astype(int), dims=[""x"", ""y""]).mean(""x"")\n        assert_equal(actual, expected)\n\n    def test_reduce_keepdims(self):\n        coords = {\n            ""x"": [-1, -2],\n            ""y"": [""ab"", ""cd"", ""ef""],\n            ""lat"": ([""x"", ""y""], [[1, 2, 3], [-1, -2, -3]]),\n            ""c"": -999,\n        }\n        orig = DataArray([[-1, 0, 1], [-3, 0, 3]], coords, dims=[""x"", ""y""])\n\n        # Mean on all axes loses non-constant coordinates\n        actual = orig.mean(keepdims=True)\n        expected = DataArray(\n            orig.data.mean(keepdims=True),\n            dims=orig.dims,\n            coords={k: v for k, v in coords.items() if k in [""c""]},\n        )\n        assert_equal(actual, expected)\n\n        assert actual.sizes[""x""] == 1\n        assert actual.sizes[""y""] == 1\n\n        # Mean on specific axes loses coordinates not involving that axis\n        actual = orig.mean(""y"", keepdims=True)\n        expected = DataArray(\n            orig.data.mean(axis=1, keepdims=True),\n            dims=orig.dims,\n            coords={k: v for k, v in coords.items() if k not in [""y"", ""lat""]},\n        )\n        assert_equal(actual, expected)\n\n    @requires_bottleneck\n    def test_reduce_keepdims_bottleneck(self):\n        import bottleneck\n\n        coords = {\n            ""x"": [-1, -2],\n            ""y"": [""ab"", ""cd"", ""ef""],\n            ""lat"": ([""x"", ""y""], [[1, 2, 3], [-1, -2, -3]]),\n            ""c"": -999,\n        }\n        orig = DataArray([[-1, 0, 1], [-3, 0, 3]], coords, dims=[""x"", ""y""])\n\n        # Bottleneck does not have its own keepdims implementation\n        actual = orig.reduce(bottleneck.nanmean, keepdims=True)\n        expected = orig.mean(keepdims=True)\n        assert_equal(actual, expected)\n\n    def test_reduce_dtype(self):\n        coords = {\n            ""x"": [-1, -2],\n            ""y"": [""ab"", ""cd"", ""ef""],\n            ""lat"": ([""x"", ""y""], [[1, 2, 3], [-1, -2, -3]]),\n            ""c"": -999,\n        }\n        orig = DataArray([[-1, 0, 1], [-3, 0, 3]], coords, dims=[""x"", ""y""])\n\n        for dtype in [np.float16, np.float32, np.float64]:\n            assert orig.astype(float).mean(dtype=dtype).dtype == dtype\n\n    def test_reduce_out(self):\n        coords = {\n            ""x"": [-1, -2],\n            ""y"": [""ab"", ""cd"", ""ef""],\n            ""lat"": ([""x"", ""y""], [[1, 2, 3], [-1, -2, -3]]),\n            ""c"": -999,\n        }\n        orig = DataArray([[-1, 0, 1], [-3, 0, 3]], coords, dims=[""x"", ""y""])\n\n        with pytest.raises(TypeError):\n            orig.mean(out=np.ones(orig.shape))\n\n    @pytest.mark.parametrize(""skipna"", [True, False])\n    @pytest.mark.parametrize(""q"", [0.25, [0.50], [0.25, 0.75]])\n    @pytest.mark.parametrize(\n        ""axis, dim"", zip([None, 0, [0], [0, 1]], [None, ""x"", [""x""], [""x"", ""y""]])\n    )\n    def test_quantile(self, q, axis, dim, skipna):\n        actual = DataArray(self.va).quantile(q, dim=dim, keep_attrs=True, skipna=skipna)\n        _percentile_func = np.nanpercentile if skipna else np.percentile\n        expected = _percentile_func(self.dv.values, np.array(q) * 100, axis=axis)\n        np.testing.assert_allclose(actual.values, expected)\n        if is_scalar(q):\n            assert ""quantile"" not in actual.dims\n        else:\n            assert ""quantile"" in actual.dims\n\n        assert actual.attrs == self.attrs\n\n    def test_reduce_keep_attrs(self):\n        # Test dropped attrs\n        vm = self.va.mean()\n        assert len(vm.attrs) == 0\n        assert vm.attrs == {}\n\n        # Test kept attrs\n        vm = self.va.mean(keep_attrs=True)\n        assert len(vm.attrs) == len(self.attrs)\n        assert vm.attrs == self.attrs\n\n    def test_assign_attrs(self):\n        expected = DataArray([], attrs=dict(a=1, b=2))\n        expected.attrs[""a""] = 1\n        expected.attrs[""b""] = 2\n        new = DataArray([])\n        actual = DataArray([]).assign_attrs(a=1, b=2)\n        assert_identical(actual, expected)\n        assert new.attrs == {}\n\n        expected.attrs[""c""] = 3\n        new_actual = actual.assign_attrs({""c"": 3})\n        assert_identical(new_actual, expected)\n        assert actual.attrs == {""a"": 1, ""b"": 2}\n\n    def test_fillna(self):\n        a = DataArray([np.nan, 1, np.nan, 3], coords={""x"": range(4)}, dims=""x"")\n        actual = a.fillna(-1)\n        expected = DataArray([-1, 1, -1, 3], coords={""x"": range(4)}, dims=""x"")\n        assert_identical(expected, actual)\n\n        b = DataArray(range(4), coords={""x"": range(4)}, dims=""x"")\n        actual = a.fillna(b)\n        expected = b.copy()\n        assert_identical(expected, actual)\n\n        actual = a.fillna(range(4))\n        assert_identical(expected, actual)\n\n        actual = a.fillna(b[:3])\n        assert_identical(expected, actual)\n\n        actual = a.fillna(b[:0])\n        assert_identical(a, actual)\n\n        with raises_regex(TypeError, ""fillna on a DataArray""):\n            a.fillna({0: 0})\n\n        with raises_regex(ValueError, ""broadcast""):\n            a.fillna([1, 2])\n\n        fill_value = DataArray([0, 1], dims=""y"")\n        actual = a.fillna(fill_value)\n        expected = DataArray(\n            [[0, 1], [1, 1], [0, 1], [3, 3]], coords={""x"": range(4)}, dims=(""x"", ""y"")\n        )\n        assert_identical(expected, actual)\n\n        expected = b.copy()\n        for target in [a, expected]:\n            target.coords[""b""] = (""x"", [0, 0, 1, 1])\n        actual = a.groupby(""b"").fillna(DataArray([0, 2], dims=""b""))\n        assert_identical(expected, actual)\n\n    def test_groupby_iter(self):\n        for ((act_x, act_dv), (exp_x, exp_ds)) in zip(\n            self.dv.groupby(""y""), self.ds.groupby(""y"")\n        ):\n            assert exp_x == act_x\n            assert_identical(exp_ds[""foo""], act_dv)\n        for ((_, exp_dv), act_dv) in zip(self.dv.groupby(""x""), self.dv):\n            assert_identical(exp_dv, act_dv)\n\n    def make_groupby_example_array(self):\n        da = self.dv.copy()\n        da.coords[""abc""] = (""y"", np.array([""a""] * 9 + [""c""] + [""b""] * 10))\n        da.coords[""y""] = 20 + 100 * da[""y""]\n        return da\n\n    def test_groupby_properties(self):\n        grouped = self.make_groupby_example_array().groupby(""abc"")\n        expected_groups = {""a"": range(0, 9), ""c"": [9], ""b"": range(10, 20)}\n        assert expected_groups.keys() == grouped.groups.keys()\n        for key in expected_groups:\n            assert_array_equal(expected_groups[key], grouped.groups[key])\n        assert 3 == len(grouped)\n\n    def test_groupby_map_identity(self):\n        expected = self.make_groupby_example_array()\n        idx = expected.coords[""y""]\n\n        def identity(x):\n            return x\n\n        for g in [""x"", ""y"", ""abc"", idx]:\n            for shortcut in [False, True]:\n                for squeeze in [False, True]:\n                    grouped = expected.groupby(g, squeeze=squeeze)\n                    actual = grouped.map(identity, shortcut=shortcut)\n                    assert_identical(expected, actual)\n\n    def test_groupby_sum(self):\n        array = self.make_groupby_example_array()\n        grouped = array.groupby(""abc"")\n\n        expected_sum_all = Dataset(\n            {\n                ""foo"": Variable(\n                    [""abc""],\n                    np.array(\n                        [\n                            self.x[:, :9].sum(),\n                            self.x[:, 10:].sum(),\n                            self.x[:, 9:10].sum(),\n                        ]\n                    ).T,\n                ),\n                ""abc"": Variable([""abc""], np.array([""a"", ""b"", ""c""])),\n            }\n        )[""foo""]\n        assert_allclose(expected_sum_all, grouped.reduce(np.sum, dim=...))\n        assert_allclose(expected_sum_all, grouped.sum(...))\n\n        expected = DataArray(\n            [\n                array[""y""].values[idx].sum()\n                for idx in [slice(9), slice(10, None), slice(9, 10)]\n            ],\n            [[""a"", ""b"", ""c""]],\n            [""abc""],\n        )\n        actual = array[""y""].groupby(""abc"").map(np.sum)\n        assert_allclose(expected, actual)\n        actual = array[""y""].groupby(""abc"").sum(...)\n        assert_allclose(expected, actual)\n\n        expected_sum_axis1 = Dataset(\n            {\n                ""foo"": (\n                    [""x"", ""abc""],\n                    np.array(\n                        [\n                            self.x[:, :9].sum(1),\n                            self.x[:, 10:].sum(1),\n                            self.x[:, 9:10].sum(1),\n                        ]\n                    ).T,\n                ),\n                ""abc"": Variable([""abc""], np.array([""a"", ""b"", ""c""])),\n            }\n        )[""foo""]\n        assert_allclose(expected_sum_axis1, grouped.reduce(np.sum, ""y""))\n        assert_allclose(expected_sum_axis1, grouped.sum(""y""))\n\n    def test_groupby_sum_default(self):\n        array = self.make_groupby_example_array()\n        grouped = array.groupby(""abc"")\n\n        expected_sum_all = Dataset(\n            {\n                ""foo"": Variable(\n                    [""x"", ""abc""],\n                    np.array(\n                        [\n                            self.x[:, :9].sum(axis=-1),\n                            self.x[:, 10:].sum(axis=-1),\n                            self.x[:, 9:10].sum(axis=-1),\n                        ]\n                    ).T,\n                ),\n                ""abc"": Variable([""abc""], np.array([""a"", ""b"", ""c""])),\n            }\n        )[""foo""]\n\n        assert_allclose(expected_sum_all, grouped.sum(dim=""y""))\n\n    def test_groupby_count(self):\n        array = DataArray(\n            [0, 0, np.nan, np.nan, 0, 0],\n            coords={""cat"": (""x"", [""a"", ""b"", ""b"", ""c"", ""c"", ""c""])},\n            dims=""x"",\n        )\n        actual = array.groupby(""cat"").count()\n        expected = DataArray([1, 1, 2], coords=[(""cat"", [""a"", ""b"", ""c""])])\n        assert_identical(actual, expected)\n\n    @pytest.mark.skip(""needs to be fixed for shortcut=False, keep_attrs=False"")\n    def test_groupby_reduce_attrs(self):\n        array = self.make_groupby_example_array()\n        array.attrs[""foo""] = ""bar""\n\n        for shortcut in [True, False]:\n            for keep_attrs in [True, False]:\n                print(f""shortcut={shortcut}, keep_attrs={keep_attrs}"")\n                actual = array.groupby(""abc"").reduce(\n                    np.mean, keep_attrs=keep_attrs, shortcut=shortcut\n                )\n                expected = array.groupby(""abc"").mean()\n                if keep_attrs:\n                    expected.attrs[""foo""] = ""bar""\n                assert_identical(expected, actual)\n\n    def test_groupby_map_center(self):\n        def center(x):\n            return x - np.mean(x)\n\n        array = self.make_groupby_example_array()\n        grouped = array.groupby(""abc"")\n\n        expected_ds = array.to_dataset()\n        exp_data = np.hstack(\n            [center(self.x[:, :9]), center(self.x[:, 9:10]), center(self.x[:, 10:])]\n        )\n        expected_ds[""foo""] = ([""x"", ""y""], exp_data)\n        expected_centered = expected_ds[""foo""]\n        assert_allclose(expected_centered, grouped.map(center))\n\n    def test_groupby_map_ndarray(self):\n        # regression test for #326\n        array = self.make_groupby_example_array()\n        grouped = array.groupby(""abc"")\n        actual = grouped.map(np.asarray)\n        assert_equal(array, actual)\n\n    def test_groupby_map_changes_metadata(self):\n        def change_metadata(x):\n            x.coords[""x""] = x.coords[""x""] * 2\n            x.attrs[""fruit""] = ""lemon""\n            return x\n\n        array = self.make_groupby_example_array()\n        grouped = array.groupby(""abc"")\n        actual = grouped.map(change_metadata)\n        expected = array.copy()\n        expected = change_metadata(expected)\n        assert_equal(expected, actual)\n\n    def test_groupby_math(self):\n        array = self.make_groupby_example_array()\n        for squeeze in [True, False]:\n            grouped = array.groupby(""x"", squeeze=squeeze)\n\n            expected = array + array.coords[""x""]\n            actual = grouped + array.coords[""x""]\n            assert_identical(expected, actual)\n\n            actual = array.coords[""x""] + grouped\n            assert_identical(expected, actual)\n\n            ds = array.coords[""x""].to_dataset(name=""X"")\n            expected = array + ds\n            actual = grouped + ds\n            assert_identical(expected, actual)\n\n            actual = ds + grouped\n            assert_identical(expected, actual)\n\n        grouped = array.groupby(""abc"")\n        expected_agg = (grouped.mean(...) - np.arange(3)).rename(None)\n        actual = grouped - DataArray(range(3), [(""abc"", [""a"", ""b"", ""c""])])\n        actual_agg = actual.groupby(""abc"").mean(...)\n        assert_allclose(expected_agg, actual_agg)\n\n        with raises_regex(TypeError, ""only support binary ops""):\n            grouped + 1\n        with raises_regex(TypeError, ""only support binary ops""):\n            grouped + grouped\n        with raises_regex(TypeError, ""in-place operations""):\n            array += grouped\n\n    def test_groupby_math_not_aligned(self):\n        array = DataArray(\n            range(4), {""b"": (""x"", [0, 0, 1, 1]), ""x"": [0, 1, 2, 3]}, dims=""x""\n        )\n        other = DataArray([10], coords={""b"": [0]}, dims=""b"")\n        actual = array.groupby(""b"") + other\n        expected = DataArray([10, 11, np.nan, np.nan], array.coords)\n        assert_identical(expected, actual)\n\n        other = DataArray([10], coords={""c"": 123, ""b"": [0]}, dims=""b"")\n        actual = array.groupby(""b"") + other\n        expected.coords[""c""] = ([""x""], [123] * 2 + [np.nan] * 2)\n        assert_identical(expected, actual)\n\n        other = Dataset({""a"": (""b"", [10])}, {""b"": [0]})\n        actual = array.groupby(""b"") + other\n        expected = Dataset({""a"": (""x"", [10, 11, np.nan, np.nan])}, array.coords)\n        assert_identical(expected, actual)\n\n    def test_groupby_restore_dim_order(self):\n        array = DataArray(\n            np.random.randn(5, 3),\n            coords={""a"": (""x"", range(5)), ""b"": (""y"", range(3))},\n            dims=[""x"", ""y""],\n        )\n        for by, expected_dims in [\n            (""x"", (""x"", ""y"")),\n            (""y"", (""x"", ""y"")),\n            (""a"", (""a"", ""y"")),\n            (""b"", (""x"", ""b"")),\n        ]:\n            result = array.groupby(by).map(lambda x: x.squeeze())\n            assert result.dims == expected_dims\n\n    def test_groupby_restore_coord_dims(self):\n        array = DataArray(\n            np.random.randn(5, 3),\n            coords={\n                ""a"": (""x"", range(5)),\n                ""b"": (""y"", range(3)),\n                ""c"": ((""x"", ""y""), np.random.randn(5, 3)),\n            },\n            dims=[""x"", ""y""],\n        )\n\n        for by, expected_dims in [\n            (""x"", (""x"", ""y"")),\n            (""y"", (""x"", ""y"")),\n            (""a"", (""a"", ""y"")),\n            (""b"", (""x"", ""b"")),\n        ]:\n            result = array.groupby(by, restore_coord_dims=True).map(\n                lambda x: x.squeeze()\n            )[""c""]\n            assert result.dims == expected_dims\n\n    def test_groupby_first_and_last(self):\n        array = DataArray([1, 2, 3, 4, 5], dims=""x"")\n        by = DataArray([""a""] * 2 + [""b""] * 3, dims=""x"", name=""ab"")\n\n        expected = DataArray([1, 3], [(""ab"", [""a"", ""b""])])\n        actual = array.groupby(by).first()\n        assert_identical(expected, actual)\n\n        expected = DataArray([2, 5], [(""ab"", [""a"", ""b""])])\n        actual = array.groupby(by).last()\n        assert_identical(expected, actual)\n\n        array = DataArray(np.random.randn(5, 3), dims=[""x"", ""y""])\n        expected = DataArray(array[[0, 2]], {""ab"": [""a"", ""b""]}, [""ab"", ""y""])\n        actual = array.groupby(by).first()\n        assert_identical(expected, actual)\n\n        actual = array.groupby(""x"").first()\n        expected = array  # should be a no-op\n        assert_identical(expected, actual)\n\n    def make_groupby_multidim_example_array(self):\n        return DataArray(\n            [[[0, 1], [2, 3]], [[5, 10], [15, 20]]],\n            coords={\n                ""lon"": ([""ny"", ""nx""], [[30, 40], [40, 50]]),\n                ""lat"": ([""ny"", ""nx""], [[10, 10], [20, 20]]),\n            },\n            dims=[""time"", ""ny"", ""nx""],\n        )\n\n    def test_groupby_multidim(self):\n        array = self.make_groupby_multidim_example_array()\n        for dim, expected_sum in [\n            (""lon"", DataArray([5, 28, 23], coords=[(""lon"", [30.0, 40.0, 50.0])])),\n            (""lat"", DataArray([16, 40], coords=[(""lat"", [10.0, 20.0])])),\n        ]:\n            actual_sum = array.groupby(dim).sum(...)\n            assert_identical(expected_sum, actual_sum)\n\n    def test_groupby_multidim_map(self):\n        array = self.make_groupby_multidim_example_array()\n        actual = array.groupby(""lon"").map(lambda x: x - x.mean())\n        expected = DataArray(\n            [[[-2.5, -6.0], [-5.0, -8.5]], [[2.5, 3.0], [8.0, 8.5]]],\n            coords=array.coords,\n            dims=array.dims,\n        )\n        assert_identical(expected, actual)\n\n    def test_groupby_bins(self):\n        array = DataArray(np.arange(4), dims=""dim_0"")\n        # the first value should not be part of any group (""right"" binning)\n        array[0] = 99\n        # bins follow conventions for pandas.cut\n        # http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n        bins = [0, 1.5, 5]\n        bin_coords = pd.cut(array[""dim_0""], bins).categories\n        expected = DataArray(\n            [1, 5], dims=""dim_0_bins"", coords={""dim_0_bins"": bin_coords}\n        )\n        # the problem with this is that it overwrites the dimensions of array!\n        # actual = array.groupby(\'dim_0\', bins=bins).sum()\n        actual = array.groupby_bins(""dim_0"", bins).map(lambda x: x.sum())\n        assert_identical(expected, actual)\n        # make sure original array dims are unchanged\n        assert len(array.dim_0) == 4\n\n    def test_groupby_bins_empty(self):\n        array = DataArray(np.arange(4), [(""x"", range(4))])\n        # one of these bins will be empty\n        bins = [0, 4, 5]\n        bin_coords = pd.cut(array[""x""], bins).categories\n        actual = array.groupby_bins(""x"", bins).sum()\n        expected = DataArray([6, np.nan], dims=""x_bins"", coords={""x_bins"": bin_coords})\n        assert_identical(expected, actual)\n        # make sure original array is unchanged\n        # (was a problem in earlier versions)\n        assert len(array.x) == 4\n\n    def test_groupby_bins_multidim(self):\n        array = self.make_groupby_multidim_example_array()\n        bins = [0, 15, 20]\n        bin_coords = pd.cut(array[""lat""].values.flat, bins).categories\n        expected = DataArray([16, 40], dims=""lat_bins"", coords={""lat_bins"": bin_coords})\n        actual = array.groupby_bins(""lat"", bins).map(lambda x: x.sum())\n        assert_identical(expected, actual)\n        # modify the array coordinates to be non-monotonic after unstacking\n        array[""lat""].data = np.array([[10.0, 20.0], [20.0, 10.0]])\n        expected = DataArray([28, 28], dims=""lat_bins"", coords={""lat_bins"": bin_coords})\n        actual = array.groupby_bins(""lat"", bins).map(lambda x: x.sum())\n        assert_identical(expected, actual)\n\n    def test_groupby_bins_sort(self):\n        data = xr.DataArray(\n            np.arange(100), dims=""x"", coords={""x"": np.linspace(-100, 100, num=100)}\n        )\n        binned_mean = data.groupby_bins(""x"", bins=11).mean()\n        assert binned_mean.to_index().is_monotonic\n\n    def test_resample(self):\n        times = pd.date_range(""2000-01-01"", freq=""6H"", periods=10)\n        array = DataArray(np.arange(10), [(""time"", times)])\n\n        actual = array.resample(time=""24H"").mean()\n        expected = DataArray(array.to_series().resample(""24H"").mean())\n        assert_identical(expected, actual)\n\n        actual = array.resample(time=""24H"").reduce(np.mean)\n        assert_identical(expected, actual)\n\n        actual = array.resample(time=""24H"", loffset=""-12H"").mean()\n        expected = DataArray(array.to_series().resample(""24H"", loffset=""-12H"").mean())\n        assert_identical(expected, actual)\n\n        with raises_regex(ValueError, ""index must be monotonic""):\n            array[[2, 0, 1]].resample(time=""1D"")\n\n    def test_da_resample_func_args(self):\n        def func(arg1, arg2, arg3=0.0):\n            return arg1.mean(""time"") + arg2 + arg3\n\n        times = pd.date_range(""2000"", periods=3, freq=""D"")\n        da = xr.DataArray([1.0, 1.0, 1.0], coords=[times], dims=[""time""])\n        expected = xr.DataArray([3.0, 3.0, 3.0], coords=[times], dims=[""time""])\n        actual = da.resample(time=""D"").map(func, args=(1.0,), arg3=1.0)\n        assert_identical(actual, expected)\n\n    def test_resample_first(self):\n        times = pd.date_range(""2000-01-01"", freq=""6H"", periods=10)\n        array = DataArray(np.arange(10), [(""time"", times)])\n\n        actual = array.resample(time=""1D"").first()\n        expected = DataArray([0, 4, 8], [(""time"", times[::4])])\n        assert_identical(expected, actual)\n\n        # verify that labels don\'t use the first value\n        actual = array.resample(time=""24H"").first()\n        expected = DataArray(array.to_series().resample(""24H"").first())\n        assert_identical(expected, actual)\n\n        # missing values\n        array = array.astype(float)\n        array[:2] = np.nan\n        actual = array.resample(time=""1D"").first()\n        expected = DataArray([2, 4, 8], [(""time"", times[::4])])\n        assert_identical(expected, actual)\n\n        actual = array.resample(time=""1D"").first(skipna=False)\n        expected = DataArray([np.nan, 4, 8], [(""time"", times[::4])])\n        assert_identical(expected, actual)\n\n        # regression test for http://stackoverflow.com/questions/33158558/\n        array = Dataset({""time"": times})[""time""]\n        actual = array.resample(time=""1D"").last()\n        expected_times = pd.to_datetime(\n            [""2000-01-01T18"", ""2000-01-02T18"", ""2000-01-03T06""]\n        )\n        expected = DataArray(expected_times, [(""time"", times[::4])], name=""time"")\n        assert_identical(expected, actual)\n\n    def test_resample_bad_resample_dim(self):\n        times = pd.date_range(""2000-01-01"", freq=""6H"", periods=10)\n        array = DataArray(np.arange(10), [(""__resample_dim__"", times)])\n        with raises_regex(ValueError, ""Proxy resampling dimension""):\n            array.resample(**{""__resample_dim__"": ""1D""}).first()\n\n    @requires_scipy\n    def test_resample_drop_nondim_coords(self):\n        xs = np.arange(6)\n        ys = np.arange(3)\n        times = pd.date_range(""2000-01-01"", freq=""6H"", periods=5)\n        data = np.tile(np.arange(5), (6, 3, 1))\n        xx, yy = np.meshgrid(xs * 5, ys * 2.5)\n        tt = np.arange(len(times), dtype=int)\n        array = DataArray(data, {""time"": times, ""x"": xs, ""y"": ys}, (""x"", ""y"", ""time""))\n        xcoord = DataArray(xx.T, {""x"": xs, ""y"": ys}, (""x"", ""y""))\n        ycoord = DataArray(yy.T, {""x"": xs, ""y"": ys}, (""x"", ""y""))\n        tcoord = DataArray(tt, {""time"": times}, (""time"",))\n        ds = Dataset({""data"": array, ""xc"": xcoord, ""yc"": ycoord, ""tc"": tcoord})\n        ds = ds.set_coords([""xc"", ""yc"", ""tc""])\n\n        # Select the data now, with the auxiliary coordinates in place\n        array = ds[""data""]\n\n        # Re-sample\n        actual = array.resample(time=""12H"", restore_coord_dims=True).mean(""time"")\n        assert ""tc"" not in actual.coords\n\n        # Up-sample - filling\n        actual = array.resample(time=""1H"", restore_coord_dims=True).ffill()\n        assert ""tc"" not in actual.coords\n\n        # Up-sample - interpolation\n        actual = array.resample(time=""1H"", restore_coord_dims=True).interpolate(\n            ""linear""\n        )\n        assert ""tc"" not in actual.coords\n\n    def test_resample_keep_attrs(self):\n        times = pd.date_range(""2000-01-01"", freq=""6H"", periods=10)\n        array = DataArray(np.ones(10), [(""time"", times)])\n        array.attrs[""meta""] = ""data""\n\n        result = array.resample(time=""1D"").mean(keep_attrs=True)\n        expected = DataArray([1, 1, 1], [(""time"", times[::4])], attrs=array.attrs)\n        assert_identical(result, expected)\n\n    def test_resample_skipna(self):\n        times = pd.date_range(""2000-01-01"", freq=""6H"", periods=10)\n        array = DataArray(np.ones(10), [(""time"", times)])\n        array[1] = np.nan\n\n        result = array.resample(time=""1D"").mean(skipna=False)\n        expected = DataArray([np.nan, 1, 1], [(""time"", times[::4])])\n        assert_identical(result, expected)\n\n    def test_upsample(self):\n        times = pd.date_range(""2000-01-01"", freq=""6H"", periods=5)\n        array = DataArray(np.arange(5), [(""time"", times)])\n\n        # Forward-fill\n        actual = array.resample(time=""3H"").ffill()\n        expected = DataArray(array.to_series().resample(""3H"").ffill())\n        assert_identical(expected, actual)\n\n        # Backward-fill\n        actual = array.resample(time=""3H"").bfill()\n        expected = DataArray(array.to_series().resample(""3H"").bfill())\n        assert_identical(expected, actual)\n\n        # As frequency\n        actual = array.resample(time=""3H"").asfreq()\n        expected = DataArray(array.to_series().resample(""3H"").asfreq())\n        assert_identical(expected, actual)\n\n        # Pad\n        actual = array.resample(time=""3H"").pad()\n        expected = DataArray(array.to_series().resample(""3H"").pad())\n        assert_identical(expected, actual)\n\n        # Nearest\n        rs = array.resample(time=""3H"")\n        actual = rs.nearest()\n        new_times = rs._full_index\n        expected = DataArray(array.reindex(time=new_times, method=""nearest""))\n        assert_identical(expected, actual)\n\n    def test_upsample_nd(self):\n        # Same as before, but now we try on multi-dimensional DataArrays.\n        xs = np.arange(6)\n        ys = np.arange(3)\n        times = pd.date_range(""2000-01-01"", freq=""6H"", periods=5)\n        data = np.tile(np.arange(5), (6, 3, 1))\n        array = DataArray(data, {""time"": times, ""x"": xs, ""y"": ys}, (""x"", ""y"", ""time""))\n\n        # Forward-fill\n        actual = array.resample(time=""3H"").ffill()\n        expected_data = np.repeat(data, 2, axis=-1)\n        expected_times = times.to_series().resample(""3H"").asfreq().index\n        expected_data = expected_data[..., : len(expected_times)]\n        expected = DataArray(\n            expected_data,\n            {""time"": expected_times, ""x"": xs, ""y"": ys},\n            (""x"", ""y"", ""time""),\n        )\n        assert_identical(expected, actual)\n\n        # Backward-fill\n        actual = array.resample(time=""3H"").ffill()\n        expected_data = np.repeat(np.flipud(data.T).T, 2, axis=-1)\n        expected_data = np.flipud(expected_data.T).T\n        expected_times = times.to_series().resample(""3H"").asfreq().index\n        expected_data = expected_data[..., : len(expected_times)]\n        expected = DataArray(\n            expected_data,\n            {""time"": expected_times, ""x"": xs, ""y"": ys},\n            (""x"", ""y"", ""time""),\n        )\n        assert_identical(expected, actual)\n\n        # As frequency\n        actual = array.resample(time=""3H"").asfreq()\n        expected_data = np.repeat(data, 2, axis=-1).astype(float)[..., :-1]\n        expected_data[..., 1::2] = np.nan\n        expected_times = times.to_series().resample(""3H"").asfreq().index\n        expected = DataArray(\n            expected_data,\n            {""time"": expected_times, ""x"": xs, ""y"": ys},\n            (""x"", ""y"", ""time""),\n        )\n        assert_identical(expected, actual)\n\n        # Pad\n        actual = array.resample(time=""3H"").pad()\n        expected_data = np.repeat(data, 2, axis=-1)\n        expected_data[..., 1::2] = expected_data[..., ::2]\n        expected_data = expected_data[..., :-1]\n        expected_times = times.to_series().resample(""3H"").asfreq().index\n        expected = DataArray(\n            expected_data,\n            {""time"": expected_times, ""x"": xs, ""y"": ys},\n            (""x"", ""y"", ""time""),\n        )\n        assert_identical(expected, actual)\n\n    def test_upsample_tolerance(self):\n        # Test tolerance keyword for upsample methods bfill, pad, nearest\n        times = pd.date_range(""2000-01-01"", freq=""1D"", periods=2)\n        times_upsampled = pd.date_range(""2000-01-01"", freq=""6H"", periods=5)\n        array = DataArray(np.arange(2), [(""time"", times)])\n\n        # Forward fill\n        actual = array.resample(time=""6H"").ffill(tolerance=""12H"")\n        expected = DataArray([0.0, 0.0, 0.0, np.nan, 1.0], [(""time"", times_upsampled)])\n        assert_identical(expected, actual)\n\n        # Backward fill\n        actual = array.resample(time=""6H"").bfill(tolerance=""12H"")\n        expected = DataArray([0.0, np.nan, 1.0, 1.0, 1.0], [(""time"", times_upsampled)])\n        assert_identical(expected, actual)\n\n        # Nearest\n        actual = array.resample(time=""6H"").nearest(tolerance=""6H"")\n        expected = DataArray([0, 0, np.nan, 1, 1], [(""time"", times_upsampled)])\n        assert_identical(expected, actual)\n\n    @requires_scipy\n    def test_upsample_interpolate(self):\n        from scipy.interpolate import interp1d\n\n        xs = np.arange(6)\n        ys = np.arange(3)\n        times = pd.date_range(""2000-01-01"", freq=""6H"", periods=5)\n\n        z = np.arange(5) ** 2\n        data = np.tile(z, (6, 3, 1))\n        array = DataArray(data, {""time"": times, ""x"": xs, ""y"": ys}, (""x"", ""y"", ""time""))\n\n        expected_times = times.to_series().resample(""1H"").asfreq().index\n        # Split the times into equal sub-intervals to simulate the 6 hour\n        # to 1 hour up-sampling\n        new_times_idx = np.linspace(0, len(times) - 1, len(times) * 5)\n        for kind in [""linear"", ""nearest"", ""zero"", ""slinear"", ""quadratic"", ""cubic""]:\n            actual = array.resample(time=""1H"").interpolate(kind)\n            f = interp1d(\n                np.arange(len(times)),\n                data,\n                kind=kind,\n                axis=-1,\n                bounds_error=True,\n                assume_sorted=True,\n            )\n            expected_data = f(new_times_idx)\n            expected = DataArray(\n                expected_data,\n                {""time"": expected_times, ""x"": xs, ""y"": ys},\n                (""x"", ""y"", ""time""),\n            )\n            # Use AllClose because there are some small differences in how\n            # we upsample timeseries versus the integer indexing as I\'ve\n            # done here due to floating point arithmetic\n            assert_allclose(expected, actual, rtol=1e-16)\n\n    @requires_scipy\n    def test_upsample_interpolate_bug_2197(self):\n        dates = pd.date_range(""2007-02-01"", ""2007-03-01"", freq=""D"")\n        da = xr.DataArray(np.arange(len(dates)), [(""time"", dates)])\n        result = da.resample(time=""M"").interpolate(""linear"")\n        expected_times = np.array(\n            [np.datetime64(""2007-02-28""), np.datetime64(""2007-03-31"")]\n        )\n        expected = xr.DataArray([27.0, np.nan], [(""time"", expected_times)])\n        assert_equal(result, expected)\n\n    @requires_scipy\n    def test_upsample_interpolate_regression_1605(self):\n        dates = pd.date_range(""2016-01-01"", ""2016-03-31"", freq=""1D"")\n        expected = xr.DataArray(\n            np.random.random((len(dates), 2, 3)),\n            dims=(""time"", ""x"", ""y""),\n            coords={""time"": dates},\n        )\n        actual = expected.resample(time=""1D"").interpolate(""linear"")\n        assert_allclose(actual, expected, rtol=1e-16)\n\n    @requires_dask\n    @requires_scipy\n    def test_upsample_interpolate_dask(self):\n        from scipy.interpolate import interp1d\n\n        xs = np.arange(6)\n        ys = np.arange(3)\n        times = pd.date_range(""2000-01-01"", freq=""6H"", periods=5)\n\n        z = np.arange(5) ** 2\n        data = np.tile(z, (6, 3, 1))\n        array = DataArray(data, {""time"": times, ""x"": xs, ""y"": ys}, (""x"", ""y"", ""time""))\n        chunks = {""x"": 2, ""y"": 1}\n\n        expected_times = times.to_series().resample(""1H"").asfreq().index\n        # Split the times into equal sub-intervals to simulate the 6 hour\n        # to 1 hour up-sampling\n        new_times_idx = np.linspace(0, len(times) - 1, len(times) * 5)\n        for kind in [""linear"", ""nearest"", ""zero"", ""slinear"", ""quadratic"", ""cubic""]:\n            actual = array.chunk(chunks).resample(time=""1H"").interpolate(kind)\n            actual = actual.compute()\n            f = interp1d(\n                np.arange(len(times)),\n                data,\n                kind=kind,\n                axis=-1,\n                bounds_error=True,\n                assume_sorted=True,\n            )\n            expected_data = f(new_times_idx)\n            expected = DataArray(\n                expected_data,\n                {""time"": expected_times, ""x"": xs, ""y"": ys},\n                (""x"", ""y"", ""time""),\n            )\n            # Use AllClose because there are some small differences in how\n            # we upsample timeseries versus the integer indexing as I\'ve\n            # done here due to floating point arithmetic\n            assert_allclose(expected, actual, rtol=1e-16)\n\n        # Check that an error is raised if an attempt is made to interpolate\n        # over a chunked dimension\n        with raises_regex(\n            NotImplementedError, ""Chunking along the dimension to be interpolated""\n        ):\n            array.chunk({""time"": 1}).resample(time=""1H"").interpolate(""linear"")\n\n    def test_align(self):\n        array = DataArray(\n            np.random.random((6, 8)), coords={""x"": list(""abcdef"")}, dims=[""x"", ""y""]\n        )\n        array1, array2 = align(array, array[:5], join=""inner"")\n        assert_identical(array1, array[:5])\n        assert_identical(array2, array[:5])\n\n    def test_align_dtype(self):\n        # regression test for #264\n        x1 = np.arange(30)\n        x2 = np.arange(5, 35)\n        a = DataArray(np.random.random((30,)).astype(np.float32), [(""x"", x1)])\n        b = DataArray(np.random.random((30,)).astype(np.float32), [(""x"", x2)])\n        c, d = align(a, b, join=""outer"")\n        assert c.dtype == np.float32\n\n    def test_align_copy(self):\n        x = DataArray([1, 2, 3], coords=[(""a"", [1, 2, 3])])\n        y = DataArray([1, 2], coords=[(""a"", [3, 1])])\n\n        expected_x2 = x\n        expected_y2 = DataArray([2, np.nan, 1], coords=[(""a"", [1, 2, 3])])\n\n        x2, y2 = align(x, y, join=""outer"", copy=False)\n        assert_identical(expected_x2, x2)\n        assert_identical(expected_y2, y2)\n        assert source_ndarray(x2.data) is source_ndarray(x.data)\n\n        x2, y2 = align(x, y, join=""outer"", copy=True)\n        assert_identical(expected_x2, x2)\n        assert_identical(expected_y2, y2)\n        assert source_ndarray(x2.data) is not source_ndarray(x.data)\n\n        # Trivial align - 1 element\n        x = DataArray([1, 2, 3], coords=[(""a"", [1, 2, 3])])\n        (x2,) = align(x, copy=False)\n        assert_identical(x, x2)\n        assert source_ndarray(x2.data) is source_ndarray(x.data)\n\n        (x2,) = align(x, copy=True)\n        assert_identical(x, x2)\n        assert source_ndarray(x2.data) is not source_ndarray(x.data)\n\n    def test_align_override(self):\n        left = DataArray([1, 2, 3], dims=""x"", coords={""x"": [0, 1, 2]})\n        right = DataArray(\n            np.arange(9).reshape((3, 3)),\n            dims=[""x"", ""y""],\n            coords={""x"": [0.1, 1.1, 2.1], ""y"": [1, 2, 3]},\n        )\n\n        expected_right = DataArray(\n            np.arange(9).reshape(3, 3),\n            dims=[""x"", ""y""],\n            coords={""x"": [0, 1, 2], ""y"": [1, 2, 3]},\n        )\n\n        new_left, new_right = align(left, right, join=""override"")\n        assert_identical(left, new_left)\n        assert_identical(new_right, expected_right)\n\n        new_left, new_right = align(left, right, exclude=""x"", join=""override"")\n        assert_identical(left, new_left)\n        assert_identical(right, new_right)\n\n        new_left, new_right = xr.align(\n            left.isel(x=0, drop=True), right, exclude=""x"", join=""override""\n        )\n        assert_identical(left.isel(x=0, drop=True), new_left)\n        assert_identical(right, new_right)\n\n        with raises_regex(ValueError, ""Indexes along dimension \'x\' don\'t have""):\n            align(left.isel(x=0).expand_dims(""x""), right, join=""override"")\n\n    @pytest.mark.parametrize(\n        ""darrays"",\n        [\n            [\n                DataArray(0),\n                DataArray([1], [(""x"", [1])]),\n                DataArray([2, 3], [(""x"", [2, 3])]),\n            ],\n            [\n                DataArray([2, 3], [(""x"", [2, 3])]),\n                DataArray([1], [(""x"", [1])]),\n                DataArray(0),\n            ],\n        ],\n    )\n    def test_align_override_error(self, darrays):\n        with raises_regex(ValueError, ""Indexes along dimension \'x\' don\'t have""):\n            xr.align(*darrays, join=""override"")\n\n    def test_align_exclude(self):\n        x = DataArray([[1, 2], [3, 4]], coords=[(""a"", [-1, -2]), (""b"", [3, 4])])\n        y = DataArray([[1, 2], [3, 4]], coords=[(""a"", [-1, 20]), (""b"", [5, 6])])\n        z = DataArray([1], dims=[""a""], coords={""a"": [20], ""b"": 7})\n\n        x2, y2, z2 = align(x, y, z, join=""outer"", exclude=[""b""])\n        expected_x2 = DataArray(\n            [[3, 4], [1, 2], [np.nan, np.nan]],\n            coords=[(""a"", [-2, -1, 20]), (""b"", [3, 4])],\n        )\n        expected_y2 = DataArray(\n            [[np.nan, np.nan], [1, 2], [3, 4]],\n            coords=[(""a"", [-2, -1, 20]), (""b"", [5, 6])],\n        )\n        expected_z2 = DataArray(\n            [np.nan, np.nan, 1], dims=[""a""], coords={""a"": [-2, -1, 20], ""b"": 7}\n        )\n        assert_identical(expected_x2, x2)\n        assert_identical(expected_y2, y2)\n        assert_identical(expected_z2, z2)\n\n    def test_align_indexes(self):\n        x = DataArray([1, 2, 3], coords=[(""a"", [-1, 10, -2])])\n        y = DataArray([1, 2], coords=[(""a"", [-2, -1])])\n\n        x2, y2 = align(x, y, join=""outer"", indexes={""a"": [10, -1, -2]})\n        expected_x2 = DataArray([2, 1, 3], coords=[(""a"", [10, -1, -2])])\n        expected_y2 = DataArray([np.nan, 2, 1], coords=[(""a"", [10, -1, -2])])\n        assert_identical(expected_x2, x2)\n        assert_identical(expected_y2, y2)\n\n        (x2,) = align(x, join=""outer"", indexes={""a"": [-2, 7, 10, -1]})\n        expected_x2 = DataArray([3, np.nan, 2, 1], coords=[(""a"", [-2, 7, 10, -1])])\n        assert_identical(expected_x2, x2)\n\n    def test_align_without_indexes_exclude(self):\n        arrays = [DataArray([1, 2, 3], dims=[""x""]), DataArray([1, 2], dims=[""x""])]\n        result0, result1 = align(*arrays, exclude=[""x""])\n        assert_identical(result0, arrays[0])\n        assert_identical(result1, arrays[1])\n\n    def test_align_mixed_indexes(self):\n        array_no_coord = DataArray([1, 2], dims=[""x""])\n        array_with_coord = DataArray([1, 2], coords=[(""x"", [""a"", ""b""])])\n        result0, result1 = align(array_no_coord, array_with_coord)\n        assert_identical(result0, array_with_coord)\n        assert_identical(result1, array_with_coord)\n\n        result0, result1 = align(array_no_coord, array_with_coord, exclude=[""x""])\n        assert_identical(result0, array_no_coord)\n        assert_identical(result1, array_with_coord)\n\n    def test_align_without_indexes_errors(self):\n        with raises_regex(ValueError, ""cannot be aligned""):\n            align(DataArray([1, 2, 3], dims=[""x""]), DataArray([1, 2], dims=[""x""]))\n\n        with raises_regex(ValueError, ""cannot be aligned""):\n            align(\n                DataArray([1, 2, 3], dims=[""x""]),\n                DataArray([1, 2], coords=[(""x"", [0, 1])]),\n            )\n\n    def test_broadcast_arrays(self):\n        x = DataArray([1, 2], coords=[(""a"", [-1, -2])], name=""x"")\n        y = DataArray([1, 2], coords=[(""b"", [3, 4])], name=""y"")\n        x2, y2 = broadcast(x, y)\n        expected_coords = [(""a"", [-1, -2]), (""b"", [3, 4])]\n        expected_x2 = DataArray([[1, 1], [2, 2]], expected_coords, name=""x"")\n        expected_y2 = DataArray([[1, 2], [1, 2]], expected_coords, name=""y"")\n        assert_identical(expected_x2, x2)\n        assert_identical(expected_y2, y2)\n\n        x = DataArray(np.random.randn(2, 3), dims=[""a"", ""b""])\n        y = DataArray(np.random.randn(3, 2), dims=[""b"", ""a""])\n        x2, y2 = broadcast(x, y)\n        expected_x2 = x\n        expected_y2 = y.T\n        assert_identical(expected_x2, x2)\n        assert_identical(expected_y2, y2)\n\n    def test_broadcast_arrays_misaligned(self):\n        # broadcast on misaligned coords must auto-align\n        x = DataArray([[1, 2], [3, 4]], coords=[(""a"", [-1, -2]), (""b"", [3, 4])])\n        y = DataArray([1, 2], coords=[(""a"", [-1, 20])])\n        expected_x2 = DataArray(\n            [[3, 4], [1, 2], [np.nan, np.nan]],\n            coords=[(""a"", [-2, -1, 20]), (""b"", [3, 4])],\n        )\n        expected_y2 = DataArray(\n            [[np.nan, np.nan], [1, 1], [2, 2]],\n            coords=[(""a"", [-2, -1, 20]), (""b"", [3, 4])],\n        )\n        x2, y2 = broadcast(x, y)\n        assert_identical(expected_x2, x2)\n        assert_identical(expected_y2, y2)\n\n    def test_broadcast_arrays_nocopy(self):\n        # Test that input data is not copied over in case\n        # no alteration is needed\n        x = DataArray([1, 2], coords=[(""a"", [-1, -2])], name=""x"")\n        y = DataArray(3, name=""y"")\n        expected_x2 = DataArray([1, 2], coords=[(""a"", [-1, -2])], name=""x"")\n        expected_y2 = DataArray([3, 3], coords=[(""a"", [-1, -2])], name=""y"")\n\n        x2, y2 = broadcast(x, y)\n        assert_identical(expected_x2, x2)\n        assert_identical(expected_y2, y2)\n        assert source_ndarray(x2.data) is source_ndarray(x.data)\n\n        # single-element broadcast (trivial case)\n        (x2,) = broadcast(x)\n        assert_identical(x, x2)\n        assert source_ndarray(x2.data) is source_ndarray(x.data)\n\n    def test_broadcast_arrays_exclude(self):\n        x = DataArray([[1, 2], [3, 4]], coords=[(""a"", [-1, -2]), (""b"", [3, 4])])\n        y = DataArray([1, 2], coords=[(""a"", [-1, 20])])\n        z = DataArray(5, coords={""b"": 5})\n\n        x2, y2, z2 = broadcast(x, y, z, exclude=[""b""])\n        expected_x2 = DataArray(\n            [[3, 4], [1, 2], [np.nan, np.nan]],\n            coords=[(""a"", [-2, -1, 20]), (""b"", [3, 4])],\n        )\n        expected_y2 = DataArray([np.nan, 1, 2], coords=[(""a"", [-2, -1, 20])])\n        expected_z2 = DataArray(\n            [5, 5, 5], dims=[""a""], coords={""a"": [-2, -1, 20], ""b"": 5}\n        )\n        assert_identical(expected_x2, x2)\n        assert_identical(expected_y2, y2)\n        assert_identical(expected_z2, z2)\n\n    def test_broadcast_coordinates(self):\n        # regression test for GH649\n        ds = Dataset({""a"": ([""x"", ""y""], np.ones((5, 6)))})\n        x_bc, y_bc, a_bc = broadcast(ds.x, ds.y, ds.a)\n        assert_identical(ds.a, a_bc)\n\n        X, Y = np.meshgrid(np.arange(5), np.arange(6), indexing=""ij"")\n        exp_x = DataArray(X, dims=[""x"", ""y""], name=""x"")\n        exp_y = DataArray(Y, dims=[""x"", ""y""], name=""y"")\n        assert_identical(exp_x, x_bc)\n        assert_identical(exp_y, y_bc)\n\n    def test_to_pandas(self):\n        # 0d\n        actual = DataArray(42).to_pandas()\n        expected = np.array(42)\n        assert_array_equal(actual, expected)\n\n        # 1d\n        values = np.random.randn(3)\n        index = pd.Index([""a"", ""b"", ""c""], name=""x"")\n        da = DataArray(values, coords=[index])\n        actual = da.to_pandas()\n        assert_array_equal(actual.values, values)\n        assert_array_equal(actual.index, index)\n        assert_array_equal(actual.index.name, ""x"")\n\n        # 2d\n        values = np.random.randn(3, 2)\n        da = DataArray(\n            values, coords=[(""x"", [""a"", ""b"", ""c""]), (""y"", [0, 1])], name=""foo""\n        )\n        actual = da.to_pandas()\n        assert_array_equal(actual.values, values)\n        assert_array_equal(actual.index, [""a"", ""b"", ""c""])\n        assert_array_equal(actual.columns, [0, 1])\n\n        # roundtrips\n        for shape in [(3,), (3, 4)]:\n            dims = list(""abc"")[: len(shape)]\n            da = DataArray(np.random.randn(*shape), dims=dims)\n            roundtripped = DataArray(da.to_pandas()).drop_vars(dims)\n            assert_identical(da, roundtripped)\n\n        with raises_regex(ValueError, ""cannot convert""):\n            DataArray(np.random.randn(1, 2, 3, 4, 5)).to_pandas()\n\n    def test_to_dataframe(self):\n        # regression test for #260\n        arr = DataArray(\n            np.random.randn(3, 4), [(""B"", [1, 2, 3]), (""A"", list(""cdef""))], name=""foo""\n        )\n        expected = arr.to_series()\n        actual = arr.to_dataframe()[""foo""]\n        assert_array_equal(expected.values, actual.values)\n        assert_array_equal(expected.name, actual.name)\n        assert_array_equal(expected.index.values, actual.index.values)\n\n        # regression test for coords with different dimensions\n        arr.coords[""C""] = (""B"", [-1, -2, -3])\n        expected = arr.to_series().to_frame()\n        expected[""C""] = [-1] * 4 + [-2] * 4 + [-3] * 4\n        expected = expected[[""C"", ""foo""]]\n        actual = arr.to_dataframe()\n        assert_array_equal(expected.values, actual.values)\n        assert_array_equal(expected.columns.values, actual.columns.values)\n        assert_array_equal(expected.index.values, actual.index.values)\n\n        arr.name = None  # unnamed\n        with raises_regex(ValueError, ""unnamed""):\n            arr.to_dataframe()\n\n    def test_to_pandas_name_matches_coordinate(self):\n        # coordinate with same name as array\n        arr = DataArray([1, 2, 3], dims=""x"", name=""x"")\n        series = arr.to_series()\n        assert_array_equal([1, 2, 3], series.values)\n        assert_array_equal([0, 1, 2], series.index.values)\n        assert ""x"" == series.name\n        assert ""x"" == series.index.name\n\n        frame = arr.to_dataframe()\n        expected = series.to_frame()\n        assert expected.equals(frame)\n\n    def test_to_and_from_series(self):\n        expected = self.dv.to_dataframe()[""foo""]\n        actual = self.dv.to_series()\n        assert_array_equal(expected.values, actual.values)\n        assert_array_equal(expected.index.values, actual.index.values)\n        assert ""foo"" == actual.name\n        # test roundtrip\n        assert_identical(self.dv, DataArray.from_series(actual).drop_vars([""x"", ""y""]))\n        # test name is None\n        actual.name = None\n        expected_da = self.dv.rename(None)\n        assert_identical(\n            expected_da, DataArray.from_series(actual).drop_vars([""x"", ""y""])\n        )\n\n    def test_from_series_multiindex(self):\n        # GH:3951\n        df = pd.DataFrame({""B"": [1, 2, 3], ""A"": [4, 5, 6]})\n        df = df.rename_axis(""num"").rename_axis(""alpha"", axis=1)\n        actual = df.stack(""alpha"").to_xarray()\n        assert (actual.sel(alpha=""B"") == [1, 2, 3]).all()\n        assert (actual.sel(alpha=""A"") == [4, 5, 6]).all()\n\n    @requires_sparse\n    def test_from_series_sparse(self):\n        import sparse\n\n        series = pd.Series([1, 2], index=[(""a"", 1), (""b"", 2)])\n\n        actual_sparse = DataArray.from_series(series, sparse=True)\n        actual_dense = DataArray.from_series(series, sparse=False)\n\n        assert isinstance(actual_sparse.data, sparse.COO)\n        actual_sparse.data = actual_sparse.data.todense()\n        assert_identical(actual_sparse, actual_dense)\n\n    @requires_sparse\n    def test_from_multiindex_series_sparse(self):\n        # regression test for GH4019\n        import sparse\n\n        idx = pd.MultiIndex.from_product([np.arange(3), np.arange(5)], names=[""a"", ""b""])\n        series = pd.Series(np.random.RandomState(0).random(len(idx)), index=idx).sample(\n            n=5, random_state=3\n        )\n\n        dense = DataArray.from_series(series, sparse=False)\n        expected_coords = sparse.COO.from_numpy(dense.data, np.nan).coords\n\n        actual_sparse = xr.DataArray.from_series(series, sparse=True)\n        actual_coords = actual_sparse.data.coords\n\n        np.testing.assert_equal(actual_coords, expected_coords)\n\n    def test_to_and_from_empty_series(self):\n        # GH697\n        expected = pd.Series([], dtype=np.float64)\n        da = DataArray.from_series(expected)\n        assert len(da) == 0\n        actual = da.to_series()\n        assert len(actual) == 0\n        assert expected.equals(actual)\n\n    def test_series_categorical_index(self):\n        # regression test for GH700\n        if not hasattr(pd, ""CategoricalIndex""):\n            pytest.skip(""requires pandas with CategoricalIndex"")\n\n        s = pd.Series(np.arange(5), index=pd.CategoricalIndex(list(""aabbc"")))\n        arr = DataArray(s)\n        assert ""\'a\'"" in repr(arr)  # should not error\n\n    def test_to_and_from_dict(self):\n        array = DataArray(\n            np.random.randn(2, 3), {""x"": [""a"", ""b""]}, [""x"", ""y""], name=""foo""\n        )\n        expected = {\n            ""name"": ""foo"",\n            ""dims"": (""x"", ""y""),\n            ""data"": array.values.tolist(),\n            ""attrs"": {},\n            ""coords"": {""x"": {""dims"": (""x"",), ""data"": [""a"", ""b""], ""attrs"": {}}},\n        }\n        actual = array.to_dict()\n\n        # check that they are identical\n        assert expected == actual\n\n        # check roundtrip\n        assert_identical(array, DataArray.from_dict(actual))\n\n        # a more bare bones representation still roundtrips\n        d = {\n            ""name"": ""foo"",\n            ""dims"": (""x"", ""y""),\n            ""data"": array.values.tolist(),\n            ""coords"": {""x"": {""dims"": ""x"", ""data"": [""a"", ""b""]}},\n        }\n        assert_identical(array, DataArray.from_dict(d))\n\n        # and the most bare bones representation still roundtrips\n        d = {""name"": ""foo"", ""dims"": (""x"", ""y""), ""data"": array.values}\n        assert_identical(array.drop_vars(""x""), DataArray.from_dict(d))\n\n        # missing a dims in the coords\n        d = {\n            ""dims"": (""x"", ""y""),\n            ""data"": array.values,\n            ""coords"": {""x"": {""data"": [""a"", ""b""]}},\n        }\n        with raises_regex(\n            ValueError, ""cannot convert dict when coords are missing the key \'dims\'""\n        ):\n            DataArray.from_dict(d)\n\n        # this one is missing some necessary information\n        d = {""dims"": (""t"")}\n        with raises_regex(ValueError, ""cannot convert dict without the key \'data\'""):\n            DataArray.from_dict(d)\n\n        # check the data=False option\n        expected_no_data = expected.copy()\n        del expected_no_data[""data""]\n        del expected_no_data[""coords""][""x""][""data""]\n        endiantype = ""<U1"" if sys.byteorder == ""little"" else "">U1""\n        expected_no_data[""coords""][""x""].update({""dtype"": endiantype, ""shape"": (2,)})\n        expected_no_data.update({""dtype"": ""float64"", ""shape"": (2, 3)})\n        actual_no_data = array.to_dict(data=False)\n        assert expected_no_data == actual_no_data\n\n    def test_to_and_from_dict_with_time_dim(self):\n        x = np.random.randn(10, 3)\n        t = pd.date_range(""20130101"", periods=10)\n        lat = [77.7, 83.2, 76]\n        da = DataArray(x, {""t"": t, ""lat"": lat}, dims=[""t"", ""lat""])\n        roundtripped = DataArray.from_dict(da.to_dict())\n        assert_identical(da, roundtripped)\n\n    def test_to_and_from_dict_with_nan_nat(self):\n        y = np.random.randn(10, 3)\n        y[2] = np.nan\n        t = pd.Series(pd.date_range(""20130101"", periods=10))\n        t[2] = np.nan\n        lat = [77.7, 83.2, 76]\n        da = DataArray(y, {""t"": t, ""lat"": lat}, dims=[""t"", ""lat""])\n        roundtripped = DataArray.from_dict(da.to_dict())\n        assert_identical(da, roundtripped)\n\n    def test_to_dict_with_numpy_attrs(self):\n        # this doesn\'t need to roundtrip\n        x = np.random.randn(10, 3)\n        t = list(""abcdefghij"")\n        lat = [77.7, 83.2, 76]\n        attrs = {\n            ""created"": np.float64(1998),\n            ""coords"": np.array([37, -110.1, 100]),\n            ""maintainer"": ""bar"",\n        }\n        da = DataArray(x, {""t"": t, ""lat"": lat}, dims=[""t"", ""lat""], attrs=attrs)\n        expected_attrs = {\n            ""created"": attrs[""created""].item(),\n            ""coords"": attrs[""coords""].tolist(),\n            ""maintainer"": ""bar"",\n        }\n        actual = da.to_dict()\n\n        # check that they are identical\n        assert expected_attrs == actual[""attrs""]\n\n    def test_to_masked_array(self):\n        rs = np.random.RandomState(44)\n        x = rs.random_sample(size=(10, 20))\n        x_masked = np.ma.masked_where(x < 0.5, x)\n        da = DataArray(x_masked)\n\n        # Test round trip\n        x_masked_2 = da.to_masked_array()\n        da_2 = DataArray(x_masked_2)\n        assert_array_equal(x_masked, x_masked_2)\n        assert_equal(da, da_2)\n\n        da_masked_array = da.to_masked_array(copy=True)\n        assert isinstance(da_masked_array, np.ma.MaskedArray)\n        # Test masks\n        assert_array_equal(da_masked_array.mask, x_masked.mask)\n        # Test that mask is unpacked correctly\n        assert_array_equal(da.values, x_masked.filled(np.nan))\n        # Test that the underlying data (including nans) hasn\'t changed\n        assert_array_equal(da_masked_array, x_masked.filled(np.nan))\n\n        # Test that copy=False gives access to values\n        masked_array = da.to_masked_array(copy=False)\n        masked_array[0, 0] = 10.0\n        assert masked_array[0, 0] == 10.0\n        assert da[0, 0].values == 10.0\n        assert masked_array.base is da.values\n        assert isinstance(masked_array, np.ma.MaskedArray)\n\n        # Test with some odd arrays\n        for v in [4, np.nan, True, ""4"", ""four""]:\n            da = DataArray(v)\n            ma = da.to_masked_array()\n            assert isinstance(ma, np.ma.MaskedArray)\n\n        # Fix GH issue 684 - masked arrays mask should be an array not a scalar\n        N = 4\n        v = range(N)\n        da = DataArray(v)\n        ma = da.to_masked_array()\n        assert len(ma.mask) == N\n\n    def test_to_and_from_cdms2_classic(self):\n        """"""Classic with 1D axes""""""\n        pytest.importorskip(""cdms2"")\n\n        original = DataArray(\n            np.arange(6).reshape(2, 3),\n            [\n                (""distance"", [-2, 2], {""units"": ""meters""}),\n                (""time"", pd.date_range(""2000-01-01"", periods=3)),\n            ],\n            name=""foo"",\n            attrs={""baz"": 123},\n        )\n        expected_coords = [\n            IndexVariable(""distance"", [-2, 2]),\n            IndexVariable(""time"", [0, 1, 2]),\n        ]\n        actual = original.to_cdms2()\n        assert_array_equal(actual.asma(), original)\n        assert actual.id == original.name\n        assert tuple(actual.getAxisIds()) == original.dims\n        for axis, coord in zip(actual.getAxisList(), expected_coords):\n            assert axis.id == coord.name\n            assert_array_equal(axis, coord.values)\n        assert actual.baz == original.attrs[""baz""]\n\n        component_times = actual.getAxis(1).asComponentTime()\n        assert len(component_times) == 3\n        assert str(component_times[0]) == ""2000-1-1 0:0:0.0""\n\n        roundtripped = DataArray.from_cdms2(actual)\n        assert_identical(original, roundtripped)\n\n        back = from_cdms2(actual)\n        assert original.dims == back.dims\n        assert original.coords.keys() == back.coords.keys()\n        for coord_name in original.coords.keys():\n            assert_array_equal(original.coords[coord_name], back.coords[coord_name])\n\n    def test_to_and_from_cdms2_sgrid(self):\n        """"""Curvilinear (structured) grid\n\n        The rectangular grid case is covered by the classic case\n        """"""\n        pytest.importorskip(""cdms2"")\n\n        lonlat = np.mgrid[:3, :4]\n        lon = DataArray(lonlat[1], dims=[""y"", ""x""], name=""lon"")\n        lat = DataArray(lonlat[0], dims=[""y"", ""x""], name=""lat"")\n        x = DataArray(np.arange(lon.shape[1]), dims=[""x""], name=""x"")\n        y = DataArray(np.arange(lon.shape[0]), dims=[""y""], name=""y"")\n        original = DataArray(\n            lonlat.sum(axis=0),\n            dims=[""y"", ""x""],\n            coords=dict(x=x, y=y, lon=lon, lat=lat),\n            name=""sst"",\n        )\n        actual = original.to_cdms2()\n        assert tuple(actual.getAxisIds()) == original.dims\n        assert_array_equal(original.coords[""lon""], actual.getLongitude().asma())\n        assert_array_equal(original.coords[""lat""], actual.getLatitude().asma())\n\n        back = from_cdms2(actual)\n        assert original.dims == back.dims\n        assert set(original.coords.keys()) == set(back.coords.keys())\n        assert_array_equal(original.coords[""lat""], back.coords[""lat""])\n        assert_array_equal(original.coords[""lon""], back.coords[""lon""])\n\n    def test_to_and_from_cdms2_ugrid(self):\n        """"""Unstructured grid""""""\n        pytest.importorskip(""cdms2"")\n\n        lon = DataArray(np.random.uniform(size=5), dims=[""cell""], name=""lon"")\n        lat = DataArray(np.random.uniform(size=5), dims=[""cell""], name=""lat"")\n        cell = DataArray(np.arange(5), dims=[""cell""], name=""cell"")\n        original = DataArray(\n            np.arange(5), dims=[""cell""], coords={""lon"": lon, ""lat"": lat, ""cell"": cell}\n        )\n        actual = original.to_cdms2()\n        assert tuple(actual.getAxisIds()) == original.dims\n        assert_array_equal(original.coords[""lon""], actual.getLongitude().getValue())\n        assert_array_equal(original.coords[""lat""], actual.getLatitude().getValue())\n\n        back = from_cdms2(actual)\n        assert set(original.dims) == set(back.dims)\n        assert set(original.coords.keys()) == set(back.coords.keys())\n        assert_array_equal(original.coords[""lat""], back.coords[""lat""])\n        assert_array_equal(original.coords[""lon""], back.coords[""lon""])\n\n    def test_to_dataset_whole(self):\n        unnamed = DataArray([1, 2], dims=""x"")\n        with raises_regex(ValueError, ""unable to convert unnamed""):\n            unnamed.to_dataset()\n\n        actual = unnamed.to_dataset(name=""foo"")\n        expected = Dataset({""foo"": (""x"", [1, 2])})\n        assert_identical(expected, actual)\n\n        named = DataArray([1, 2], dims=""x"", name=""foo"", attrs={""y"": ""testattr""})\n        actual = named.to_dataset()\n        expected = Dataset({""foo"": (""x"", [1, 2], {""y"": ""testattr""})})\n        assert_identical(expected, actual)\n\n        # Test promoting attrs\n        actual = named.to_dataset(promote_attrs=True)\n        expected = Dataset(\n            {""foo"": (""x"", [1, 2], {""y"": ""testattr""})}, attrs={""y"": ""testattr""}\n        )\n        assert_identical(expected, actual)\n\n        with pytest.raises(TypeError):\n            actual = named.to_dataset(""bar"")\n\n    def test_to_dataset_split(self):\n        array = DataArray([1, 2, 3], coords=[(""x"", list(""abc""))], attrs={""a"": 1})\n        expected = Dataset({""a"": 1, ""b"": 2, ""c"": 3}, attrs={""a"": 1})\n        actual = array.to_dataset(""x"")\n        assert_identical(expected, actual)\n\n        with pytest.raises(TypeError):\n            array.to_dataset(""x"", name=""foo"")\n\n        roundtripped = actual.to_array(dim=""x"")\n        assert_identical(array, roundtripped)\n\n        array = DataArray([1, 2, 3], dims=""x"")\n        expected = Dataset({0: 1, 1: 2, 2: 3})\n        actual = array.to_dataset(""x"")\n        assert_identical(expected, actual)\n\n    def test_to_dataset_retains_keys(self):\n\n        # use dates as convenient non-str objects. Not a specific date test\n        import datetime\n\n        dates = [datetime.date(2000, 1, d) for d in range(1, 4)]\n\n        array = DataArray([1, 2, 3], coords=[(""x"", dates)], attrs={""a"": 1})\n\n        # convert to dateset and back again\n        result = array.to_dataset(""x"").to_array(dim=""x"")\n\n        assert_equal(array, result)\n\n    def test__title_for_slice(self):\n        array = DataArray(\n            np.ones((4, 3, 2)),\n            dims=[""a"", ""b"", ""c""],\n            coords={""a"": range(4), ""b"": range(3), ""c"": range(2)},\n        )\n        assert """" == array._title_for_slice()\n        assert ""c = 0"" == array.isel(c=0)._title_for_slice()\n        title = array.isel(b=1, c=0)._title_for_slice()\n        assert ""b = 1, c = 0"" == title or ""c = 0, b = 1"" == title\n\n        a2 = DataArray(np.ones((4, 1)), dims=[""a"", ""b""])\n        assert """" == a2._title_for_slice()\n\n    def test__title_for_slice_truncate(self):\n        array = DataArray(np.ones(4))\n        array.coords[""a""] = ""a"" * 100\n        array.coords[""b""] = ""b"" * 100\n\n        nchar = 80\n        title = array._title_for_slice(truncate=nchar)\n\n        assert nchar == len(title)\n        assert title.endswith(""..."")\n\n    def test_dataarray_diff_n1(self):\n        da = DataArray(np.random.randn(3, 4), dims=[""x"", ""y""])\n        actual = da.diff(""y"")\n        expected = DataArray(np.diff(da.values, axis=1), dims=[""x"", ""y""])\n        assert_equal(expected, actual)\n\n    def test_coordinate_diff(self):\n        # regression test for GH634\n        arr = DataArray(range(0, 20, 2), dims=[""lon""], coords=[range(10)])\n        lon = arr.coords[""lon""]\n        expected = DataArray([1] * 9, dims=[""lon""], coords=[range(1, 10)], name=""lon"")\n        actual = lon.diff(""lon"")\n        assert_equal(expected, actual)\n\n    @pytest.mark.parametrize(""offset"", [-5, 0, 1, 2])\n    @pytest.mark.parametrize(""fill_value, dtype"", [(2, int), (dtypes.NA, float)])\n    def test_shift(self, offset, fill_value, dtype):\n        arr = DataArray([1, 2, 3], dims=""x"")\n        actual = arr.shift(x=1, fill_value=fill_value)\n        if fill_value == dtypes.NA:\n            # if we supply the default, we expect the missing value for a\n            # float array\n            fill_value = np.nan\n        expected = DataArray([fill_value, 1, 2], dims=""x"")\n        assert_identical(expected, actual)\n        assert actual.dtype == dtype\n\n        arr = DataArray([1, 2, 3], [(""x"", [""a"", ""b"", ""c""])])\n        expected = DataArray(arr.to_pandas().shift(offset))\n        actual = arr.shift(x=offset)\n        assert_identical(expected, actual)\n\n    def test_roll_coords(self):\n        arr = DataArray([1, 2, 3], coords={""x"": range(3)}, dims=""x"")\n        actual = arr.roll(x=1, roll_coords=True)\n        expected = DataArray([3, 1, 2], coords=[(""x"", [2, 0, 1])])\n        assert_identical(expected, actual)\n\n    def test_roll_no_coords(self):\n        arr = DataArray([1, 2, 3], coords={""x"": range(3)}, dims=""x"")\n        actual = arr.roll(x=1, roll_coords=False)\n        expected = DataArray([3, 1, 2], coords=[(""x"", [0, 1, 2])])\n        assert_identical(expected, actual)\n\n    def test_roll_coords_none(self):\n        arr = DataArray([1, 2, 3], coords={""x"": range(3)}, dims=""x"")\n\n        with pytest.warns(FutureWarning):\n            actual = arr.roll(x=1, roll_coords=None)\n\n        expected = DataArray([3, 1, 2], coords=[(""x"", [2, 0, 1])])\n        assert_identical(expected, actual)\n\n    def test_copy_with_data(self):\n        orig = DataArray(\n            np.random.random(size=(2, 2)),\n            dims=(""x"", ""y""),\n            attrs={""attr1"": ""value1""},\n            coords={""x"": [4, 3]},\n            name=""helloworld"",\n        )\n        new_data = np.arange(4).reshape(2, 2)\n        actual = orig.copy(data=new_data)\n        expected = orig.copy()\n        expected.data = new_data\n        assert_identical(expected, actual)\n\n    @pytest.mark.xfail(raises=AssertionError)\n    @pytest.mark.parametrize(\n        ""deep, expected_orig"",\n        [\n            [\n                True,\n                xr.DataArray(\n                    xr.IndexVariable(""a"", np.array([1, 2])),\n                    coords={""a"": [1, 2]},\n                    dims=[""a""],\n                ),\n            ],\n            [\n                False,\n                xr.DataArray(\n                    xr.IndexVariable(""a"", np.array([999, 2])),\n                    coords={""a"": [999, 2]},\n                    dims=[""a""],\n                ),\n            ],\n        ],\n    )\n    def test_copy_coords(self, deep, expected_orig):\n        """"""The test fails for the shallow copy, and apparently only on Windows\n        for some reason. In windows coords seem to be immutable unless it\'s one\n        dataarray deep copied from another.""""""\n        da = xr.DataArray(\n            np.ones([2, 2, 2]),\n            coords={""a"": [1, 2], ""b"": [""x"", ""y""], ""c"": [0, 1]},\n            dims=[""a"", ""b"", ""c""],\n        )\n        da_cp = da.copy(deep)\n        da_cp[""a""].data[0] = 999\n\n        expected_cp = xr.DataArray(\n            xr.IndexVariable(""a"", np.array([999, 2])),\n            coords={""a"": [999, 2]},\n            dims=[""a""],\n        )\n        assert_identical(da_cp[""a""], expected_cp)\n\n        assert_identical(da[""a""], expected_orig)\n\n    def test_real_and_imag(self):\n        array = DataArray(1 + 2j)\n        assert_identical(array.real, DataArray(1))\n        assert_identical(array.imag, DataArray(2))\n\n    def test_setattr_raises(self):\n        array = DataArray(0, coords={""scalar"": 1}, attrs={""foo"": ""bar""})\n        with raises_regex(AttributeError, ""cannot set attr""):\n            array.scalar = 2\n        with raises_regex(AttributeError, ""cannot set attr""):\n            array.foo = 2\n        with raises_regex(AttributeError, ""cannot set attr""):\n            array.other = 2\n\n    def test_full_like(self):\n        # For more thorough tests, see test_variable.py\n        da = DataArray(\n            np.random.random(size=(2, 2)),\n            dims=(""x"", ""y""),\n            attrs={""attr1"": ""value1""},\n            coords={""x"": [4, 3]},\n            name=""helloworld"",\n        )\n\n        actual = full_like(da, 2)\n        expect = da.copy(deep=True)\n        expect.values = [[2.0, 2.0], [2.0, 2.0]]\n        assert_identical(expect, actual)\n\n        # override dtype\n        actual = full_like(da, fill_value=True, dtype=bool)\n        expect.values = [[True, True], [True, True]]\n        assert expect.dtype == bool\n        assert_identical(expect, actual)\n\n    def test_dot(self):\n        x = np.linspace(-3, 3, 6)\n        y = np.linspace(-3, 3, 5)\n        z = range(4)\n        da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))\n        da = DataArray(da_vals, coords=[x, y, z], dims=[""x"", ""y"", ""z""])\n\n        dm_vals = range(4)\n        dm = DataArray(dm_vals, coords=[z], dims=[""z""])\n\n        # nd dot 1d\n        actual = da.dot(dm)\n        expected_vals = np.tensordot(da_vals, dm_vals, [2, 0])\n        expected = DataArray(expected_vals, coords=[x, y], dims=[""x"", ""y""])\n        assert_equal(expected, actual)\n\n        # all shared dims\n        actual = da.dot(da)\n        expected_vals = np.tensordot(da_vals, da_vals, axes=([0, 1, 2], [0, 1, 2]))\n        expected = DataArray(expected_vals)\n        assert_equal(expected, actual)\n\n        # multiple shared dims\n        dm_vals = np.arange(20 * 5 * 4).reshape((20, 5, 4))\n        j = np.linspace(-3, 3, 20)\n        dm = DataArray(dm_vals, coords=[j, y, z], dims=[""j"", ""y"", ""z""])\n        actual = da.dot(dm)\n        expected_vals = np.tensordot(da_vals, dm_vals, axes=([1, 2], [1, 2]))\n        expected = DataArray(expected_vals, coords=[x, j], dims=[""x"", ""j""])\n        assert_equal(expected, actual)\n\n        # Ellipsis: all dims are shared\n        actual = da.dot(da, dims=...)\n        expected = da.dot(da)\n        assert_equal(expected, actual)\n\n        # Ellipsis: not all dims are shared\n        actual = da.dot(dm, dims=...)\n        expected = da.dot(dm, dims=(""j"", ""x"", ""y"", ""z""))\n        assert_equal(expected, actual)\n\n        with pytest.raises(NotImplementedError):\n            da.dot(dm.to_dataset(name=""dm""))\n        with pytest.raises(TypeError):\n            da.dot(dm.values)\n\n    def test_dot_align_coords(self):\n        # GH 3694\n\n        x = np.linspace(-3, 3, 6)\n        y = np.linspace(-3, 3, 5)\n        z_a = range(4)\n        da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))\n        da = DataArray(da_vals, coords=[x, y, z_a], dims=[""x"", ""y"", ""z""])\n\n        z_m = range(2, 6)\n        dm_vals = range(4)\n        dm = DataArray(dm_vals, coords=[z_m], dims=[""z""])\n\n        with xr.set_options(arithmetic_join=""exact""):\n            with raises_regex(ValueError, ""indexes along dimension""):\n                da.dot(dm)\n\n        da_aligned, dm_aligned = xr.align(da, dm, join=""inner"")\n\n        # nd dot 1d\n        actual = da.dot(dm)\n        expected_vals = np.tensordot(da_aligned.values, dm_aligned.values, [2, 0])\n        expected = DataArray(expected_vals, coords=[x, da_aligned.y], dims=[""x"", ""y""])\n        assert_equal(expected, actual)\n\n        # multiple shared dims\n        dm_vals = np.arange(20 * 5 * 4).reshape((20, 5, 4))\n        j = np.linspace(-3, 3, 20)\n        dm = DataArray(dm_vals, coords=[j, y, z_m], dims=[""j"", ""y"", ""z""])\n        da_aligned, dm_aligned = xr.align(da, dm, join=""inner"")\n        actual = da.dot(dm)\n        expected_vals = np.tensordot(\n            da_aligned.values, dm_aligned.values, axes=([1, 2], [1, 2])\n        )\n        expected = DataArray(expected_vals, coords=[x, j], dims=[""x"", ""j""])\n        assert_equal(expected, actual)\n\n    def test_matmul(self):\n\n        # copied from above (could make a fixture)\n        x = np.linspace(-3, 3, 6)\n        y = np.linspace(-3, 3, 5)\n        z = range(4)\n        da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))\n        da = DataArray(da_vals, coords=[x, y, z], dims=[""x"", ""y"", ""z""])\n\n        result = da @ da\n        expected = da.dot(da)\n        assert_identical(result, expected)\n\n    def test_matmul_align_coords(self):\n        # GH 3694\n\n        x_a = np.arange(6)\n        x_b = np.arange(2, 8)\n        da_vals = np.arange(6)\n        da_a = DataArray(da_vals, coords=[x_a], dims=[""x""])\n        da_b = DataArray(da_vals, coords=[x_b], dims=[""x""])\n\n        # only test arithmetic_join=""inner"" (=default)\n        result = da_a @ da_b\n        expected = da_a.dot(da_b)\n        assert_identical(result, expected)\n\n        with xr.set_options(arithmetic_join=""exact""):\n            with raises_regex(ValueError, ""indexes along dimension""):\n                da_a @ da_b\n\n    def test_binary_op_propagate_indexes(self):\n        # regression test for GH2227\n        self.dv[""x""] = np.arange(self.dv.sizes[""x""])\n        expected = self.dv.indexes[""x""]\n\n        actual = (self.dv * 10).indexes[""x""]\n        assert expected is actual\n\n        actual = (self.dv > 10).indexes[""x""]\n        assert expected is actual\n\n    def test_binary_op_join_setting(self):\n        dim = ""x""\n        align_type = ""outer""\n        coords_l, coords_r = [0, 1, 2], [1, 2, 3]\n        missing_3 = xr.DataArray(coords_l, [(dim, coords_l)])\n        missing_0 = xr.DataArray(coords_r, [(dim, coords_r)])\n        with xr.set_options(arithmetic_join=align_type):\n            actual = missing_0 + missing_3\n        missing_0_aligned, missing_3_aligned = xr.align(\n            missing_0, missing_3, join=align_type\n        )\n        expected = xr.DataArray([np.nan, 2, 4, np.nan], [(dim, [0, 1, 2, 3])])\n        assert_equal(actual, expected)\n\n    def test_combine_first(self):\n        ar0 = DataArray([[0, 0], [0, 0]], [(""x"", [""a"", ""b""]), (""y"", [-1, 0])])\n        ar1 = DataArray([[1, 1], [1, 1]], [(""x"", [""b"", ""c""]), (""y"", [0, 1])])\n        ar2 = DataArray([2], [(""x"", [""d""])])\n\n        actual = ar0.combine_first(ar1)\n        expected = DataArray(\n            [[0, 0, np.nan], [0, 0, 1], [np.nan, 1, 1]],\n            [(""x"", [""a"", ""b"", ""c""]), (""y"", [-1, 0, 1])],\n        )\n        assert_equal(actual, expected)\n\n        actual = ar1.combine_first(ar0)\n        expected = DataArray(\n            [[0, 0, np.nan], [0, 1, 1], [np.nan, 1, 1]],\n            [(""x"", [""a"", ""b"", ""c""]), (""y"", [-1, 0, 1])],\n        )\n        assert_equal(actual, expected)\n\n        actual = ar0.combine_first(ar2)\n        expected = DataArray(\n            [[0, 0], [0, 0], [2, 2]], [(""x"", [""a"", ""b"", ""d""]), (""y"", [-1, 0])]\n        )\n        assert_equal(actual, expected)\n\n    def test_sortby(self):\n        da = DataArray(\n            [[1, 2], [3, 4], [5, 6]], [(""x"", [""c"", ""b"", ""a""]), (""y"", [1, 0])]\n        )\n\n        sorted1d = DataArray(\n            [[5, 6], [3, 4], [1, 2]], [(""x"", [""a"", ""b"", ""c""]), (""y"", [1, 0])]\n        )\n\n        sorted2d = DataArray(\n            [[6, 5], [4, 3], [2, 1]], [(""x"", [""a"", ""b"", ""c""]), (""y"", [0, 1])]\n        )\n\n        expected = sorted1d\n        dax = DataArray([100, 99, 98], [(""x"", [""c"", ""b"", ""a""])])\n        actual = da.sortby(dax)\n        assert_equal(actual, expected)\n\n        # test descending order sort\n        actual = da.sortby(dax, ascending=False)\n        assert_equal(actual, da)\n\n        # test alignment (fills in nan for \'c\')\n        dax_short = DataArray([98, 97], [(""x"", [""b"", ""a""])])\n        actual = da.sortby(dax_short)\n        assert_equal(actual, expected)\n\n        # test multi-dim sort by 1D dataarray values\n        expected = sorted2d\n        dax = DataArray([100, 99, 98], [(""x"", [""c"", ""b"", ""a""])])\n        day = DataArray([90, 80], [(""y"", [1, 0])])\n        actual = da.sortby([day, dax])\n        assert_equal(actual, expected)\n\n        expected = sorted1d\n        actual = da.sortby(""x"")\n        assert_equal(actual, expected)\n\n        expected = sorted2d\n        actual = da.sortby([""x"", ""y""])\n        assert_equal(actual, expected)\n\n    @requires_bottleneck\n    def test_rank(self):\n        # floats\n        ar = DataArray([[3, 4, np.nan, 1]])\n        expect_0 = DataArray([[1, 1, np.nan, 1]])\n        expect_1 = DataArray([[2, 3, np.nan, 1]])\n        assert_equal(ar.rank(""dim_0""), expect_0)\n        assert_equal(ar.rank(""dim_1""), expect_1)\n        # int\n        x = DataArray([3, 2, 1])\n        assert_equal(x.rank(""dim_0""), x)\n        # str\n        y = DataArray([""c"", ""b"", ""a""])\n        assert_equal(y.rank(""dim_0""), x)\n\n        x = DataArray([3.0, 1.0, np.nan, 2.0, 4.0], dims=(""z"",))\n        y = DataArray([0.75, 0.25, np.nan, 0.5, 1.0], dims=(""z"",))\n        assert_equal(y.rank(""z"", pct=True), y)\n\n    @pytest.mark.parametrize(""use_dask"", [True, False])\n    @pytest.mark.parametrize(""use_datetime"", [True, False])\n    def test_polyfit(self, use_dask, use_datetime):\n        if use_dask and not has_dask:\n            pytest.skip(""requires dask"")\n        xcoord = xr.DataArray(\n            pd.date_range(""1970-01-01"", freq=""D"", periods=10), dims=(""x"",), name=""x""\n        )\n        x = xr.core.missing.get_clean_interp_index(xcoord, ""x"")\n        if not use_datetime:\n            xcoord = x\n\n        da_raw = DataArray(\n            np.stack(\n                (10 + 1e-15 * x + 2e-28 * x ** 2, 30 + 2e-14 * x + 1e-29 * x ** 2)\n            ),\n            dims=(""d"", ""x""),\n            coords={""x"": xcoord, ""d"": [0, 1]},\n        )\n\n        if use_dask:\n            da = da_raw.chunk({""d"": 1})\n        else:\n            da = da_raw\n\n        out = da.polyfit(""x"", 2)\n        expected = DataArray(\n            [[2e-28, 1e-15, 10], [1e-29, 2e-14, 30]],\n            dims=(""d"", ""degree""),\n            coords={""degree"": [2, 1, 0], ""d"": [0, 1]},\n        ).T\n        assert_allclose(out.polyfit_coefficients, expected, rtol=1e-3)\n\n        # With NaN\n        da_raw[0, 1] = np.nan\n        if use_dask:\n            da = da_raw.chunk({""d"": 1})\n        else:\n            da = da_raw\n        out = da.polyfit(""x"", 2, skipna=True, cov=True)\n        assert_allclose(out.polyfit_coefficients, expected, rtol=1e-3)\n        assert ""polyfit_covariance"" in out\n\n        # Skipna + Full output\n        out = da.polyfit(""x"", 2, skipna=True, full=True)\n        assert_allclose(out.polyfit_coefficients, expected, rtol=1e-3)\n        assert out.x_matrix_rank == 3\n        np.testing.assert_almost_equal(out.polyfit_residuals, [0, 0])\n\n    def test_pad_constant(self):\n        ar = DataArray(np.arange(3 * 4 * 5).reshape(3, 4, 5))\n        actual = ar.pad(dim_0=(1, 3))\n        expected = DataArray(\n            np.pad(\n                np.arange(3 * 4 * 5).reshape(3, 4, 5).astype(np.float32),\n                mode=""constant"",\n                pad_width=((1, 3), (0, 0), (0, 0)),\n                constant_values=np.nan,\n            )\n        )\n        assert actual.shape == (7, 4, 5)\n        assert_identical(actual, expected)\n\n    def test_pad_coords(self):\n        ar = DataArray(\n            np.arange(3 * 4 * 5).reshape(3, 4, 5),\n            [(""x"", np.arange(3)), (""y"", np.arange(4)), (""z"", np.arange(5))],\n        )\n        actual = ar.pad(x=(1, 3), constant_values=1)\n        expected = DataArray(\n            np.pad(\n                np.arange(3 * 4 * 5).reshape(3, 4, 5),\n                mode=""constant"",\n                pad_width=((1, 3), (0, 0), (0, 0)),\n                constant_values=1,\n            ),\n            [\n                (\n                    ""x"",\n                    np.pad(\n                        np.arange(3).astype(np.float32),\n                        mode=""constant"",\n                        pad_width=(1, 3),\n                        constant_values=np.nan,\n                    ),\n                ),\n                (""y"", np.arange(4)),\n                (""z"", np.arange(5)),\n            ],\n        )\n        assert_identical(actual, expected)\n\n    @pytest.mark.parametrize(""mode"", (""minimum"", ""maximum"", ""mean"", ""median""))\n    @pytest.mark.parametrize(\n        ""stat_length"", (None, 3, (1, 3), {""dim_0"": (2, 1), ""dim_2"": (4, 2)})\n    )\n    def test_pad_stat_length(self, mode, stat_length):\n        ar = DataArray(np.arange(3 * 4 * 5).reshape(3, 4, 5))\n        actual = ar.pad(dim_0=(1, 3), dim_2=(2, 2), mode=mode, stat_length=stat_length)\n        if isinstance(stat_length, dict):\n            stat_length = (stat_length[""dim_0""], (4, 4), stat_length[""dim_2""])\n        expected = DataArray(\n            np.pad(\n                np.arange(3 * 4 * 5).reshape(3, 4, 5),\n                pad_width=((1, 3), (0, 0), (2, 2)),\n                mode=mode,\n                stat_length=stat_length,\n            )\n        )\n        assert actual.shape == (7, 4, 9)\n        assert_identical(actual, expected)\n\n    @pytest.mark.parametrize(\n        ""end_values"", (None, 3, (3, 5), {""dim_0"": (2, 1), ""dim_2"": (4, 2)})\n    )\n    def test_pad_linear_ramp(self, end_values):\n        ar = DataArray(np.arange(3 * 4 * 5).reshape(3, 4, 5))\n        actual = ar.pad(\n            dim_0=(1, 3), dim_2=(2, 2), mode=""linear_ramp"", end_values=end_values\n        )\n        if end_values is None:\n            end_values = 0\n        elif isinstance(end_values, dict):\n            end_values = (end_values[""dim_0""], (4, 4), end_values[""dim_2""])\n        expected = DataArray(\n            np.pad(\n                np.arange(3 * 4 * 5).reshape(3, 4, 5),\n                pad_width=((1, 3), (0, 0), (2, 2)),\n                mode=""linear_ramp"",\n                end_values=end_values,\n            )\n        )\n        assert actual.shape == (7, 4, 9)\n        assert_identical(actual, expected)\n\n    @pytest.mark.parametrize(""mode"", (""reflect"", ""symmetric""))\n    @pytest.mark.parametrize(""reflect_type"", (None, ""even"", ""odd""))\n    def test_pad_reflect(self, mode, reflect_type):\n\n        ar = DataArray(np.arange(3 * 4 * 5).reshape(3, 4, 5))\n        actual = ar.pad(\n            dim_0=(1, 3), dim_2=(2, 2), mode=mode, reflect_type=reflect_type\n        )\n        np_kwargs = {\n            ""array"": np.arange(3 * 4 * 5).reshape(3, 4, 5),\n            ""pad_width"": ((1, 3), (0, 0), (2, 2)),\n            ""mode"": mode,\n        }\n        # numpy does not support reflect_type=None\n        if reflect_type is not None:\n            np_kwargs[""reflect_type""] = reflect_type\n        expected = DataArray(np.pad(**np_kwargs))\n\n        assert actual.shape == (7, 4, 9)\n        assert_identical(actual, expected)\n\n\nclass TestReduce:\n    @pytest.fixture(autouse=True)\n    def setup(self):\n        self.attrs = {""attr1"": ""value1"", ""attr2"": 2929}\n\n\n@pytest.mark.parametrize(\n    ""x, minindex, maxindex, nanindex"",\n    [\n        (np.array([0, 1, 2, 0, -2, -4, 2]), 5, 2, None),\n        (np.array([0.0, 1.0, 2.0, 0.0, -2.0, -4.0, 2.0]), 5, 2, None),\n        (np.array([1.0, np.NaN, 2.0, np.NaN, -2.0, -4.0, 2.0]), 5, 2, 1),\n        (\n            np.array([1.0, np.NaN, 2.0, np.NaN, -2.0, -4.0, 2.0]).astype(""object""),\n            5,\n            2,\n            1,\n        ),\n        (np.array([np.NaN, np.NaN]), np.NaN, np.NaN, 0),\n        (\n            np.array(\n                [""2015-12-31"", ""2020-01-02"", ""2020-01-01"", ""2016-01-01""],\n                dtype=""datetime64[ns]"",\n            ),\n            0,\n            1,\n            None,\n        ),\n    ],\n)\nclass TestReduce1D(TestReduce):\n    def test_min(self, x, minindex, maxindex, nanindex):\n        ar = xr.DataArray(\n            x, dims=[""x""], coords={""x"": np.arange(x.size) * 4}, attrs=self.attrs\n        )\n\n        if np.isnan(minindex):\n            minindex = 0\n\n        expected0 = ar.isel(x=minindex, drop=True)\n        result0 = ar.min(keep_attrs=True)\n        assert_identical(result0, expected0)\n\n        result1 = ar.min()\n        expected1 = expected0.copy()\n        expected1.attrs = {}\n        assert_identical(result1, expected1)\n\n        result2 = ar.min(skipna=False)\n        if nanindex is not None and ar.dtype.kind != ""O"":\n            expected2 = ar.isel(x=nanindex, drop=True)\n            expected2.attrs = {}\n        else:\n            expected2 = expected1\n\n        assert_identical(result2, expected2)\n\n    def test_max(self, x, minindex, maxindex, nanindex):\n        ar = xr.DataArray(\n            x, dims=[""x""], coords={""x"": np.arange(x.size) * 4}, attrs=self.attrs\n        )\n\n        if np.isnan(minindex):\n            maxindex = 0\n\n        expected0 = ar.isel(x=maxindex, drop=True)\n        result0 = ar.max(keep_attrs=True)\n        assert_identical(result0, expected0)\n\n        result1 = ar.max()\n        expected1 = expected0.copy()\n        expected1.attrs = {}\n        assert_identical(result1, expected1)\n\n        result2 = ar.max(skipna=False)\n        if nanindex is not None and ar.dtype.kind != ""O"":\n            expected2 = ar.isel(x=nanindex, drop=True)\n            expected2.attrs = {}\n        else:\n            expected2 = expected1\n\n        assert_identical(result2, expected2)\n\n    def test_argmin(self, x, minindex, maxindex, nanindex):\n        ar = xr.DataArray(\n            x, dims=[""x""], coords={""x"": np.arange(x.size) * 4}, attrs=self.attrs\n        )\n        indarr = xr.DataArray(np.arange(x.size, dtype=np.intp), dims=[""x""])\n\n        if np.isnan(minindex):\n            with pytest.raises(ValueError):\n                ar.argmin()\n            return\n\n        expected0 = indarr[minindex]\n        result0 = ar.argmin()\n        assert_identical(result0, expected0)\n\n        result1 = ar.argmin(keep_attrs=True)\n        expected1 = expected0.copy()\n        expected1.attrs = self.attrs\n        assert_identical(result1, expected1)\n\n        result2 = ar.argmin(skipna=False)\n        if nanindex is not None and ar.dtype.kind != ""O"":\n            expected2 = indarr.isel(x=nanindex, drop=True)\n            expected2.attrs = {}\n        else:\n            expected2 = expected0\n\n        assert_identical(result2, expected2)\n\n    def test_argmax(self, x, minindex, maxindex, nanindex):\n        ar = xr.DataArray(\n            x, dims=[""x""], coords={""x"": np.arange(x.size) * 4}, attrs=self.attrs\n        )\n        indarr = xr.DataArray(np.arange(x.size, dtype=np.intp), dims=[""x""])\n\n        if np.isnan(maxindex):\n            with pytest.raises(ValueError):\n                ar.argmax()\n            return\n\n        expected0 = indarr[maxindex]\n        result0 = ar.argmax()\n        assert_identical(result0, expected0)\n\n        result1 = ar.argmax(keep_attrs=True)\n        expected1 = expected0.copy()\n        expected1.attrs = self.attrs\n        assert_identical(result1, expected1)\n\n        result2 = ar.argmax(skipna=False)\n        if nanindex is not None and ar.dtype.kind != ""O"":\n            expected2 = indarr.isel(x=nanindex, drop=True)\n            expected2.attrs = {}\n        else:\n            expected2 = expected0\n\n        assert_identical(result2, expected2)\n\n    @pytest.mark.parametrize(""use_dask"", [True, False])\n    def test_idxmin(self, x, minindex, maxindex, nanindex, use_dask):\n        if use_dask and not has_dask:\n            pytest.skip(""requires dask"")\n        if use_dask and x.dtype.kind == ""M"":\n            pytest.xfail(""dask operation \'argmin\' breaks when dtype is datetime64 (M)"")\n        ar0_raw = xr.DataArray(\n            x, dims=[""x""], coords={""x"": np.arange(x.size) * 4}, attrs=self.attrs\n        )\n\n        if use_dask:\n            ar0 = ar0_raw.chunk({})\n        else:\n            ar0 = ar0_raw\n\n        # dim doesn\'t exist\n        with pytest.raises(KeyError):\n            ar0.idxmin(dim=""spam"")\n\n        # Scalar Dataarray\n        with pytest.raises(ValueError):\n            xr.DataArray(5).idxmin()\n\n        coordarr0 = xr.DataArray(ar0.coords[""x""], dims=[""x""])\n        coordarr1 = coordarr0.copy()\n\n        hasna = np.isnan(minindex)\n        if np.isnan(minindex):\n            minindex = 0\n\n        if hasna:\n            coordarr1[...] = 1\n            fill_value_0 = np.NaN\n        else:\n            fill_value_0 = 1\n\n        expected0 = (\n            (coordarr1 * fill_value_0).isel(x=minindex, drop=True).astype(""float"")\n        )\n        expected0.name = ""x""\n\n        # Default fill value (NaN)\n        result0 = ar0.idxmin()\n        assert_identical(result0, expected0)\n\n        # Manually specify NaN fill_value\n        result1 = ar0.idxmin(fill_value=np.NaN)\n        assert_identical(result1, expected0)\n\n        # keep_attrs\n        result2 = ar0.idxmin(keep_attrs=True)\n        expected2 = expected0.copy()\n        expected2.attrs = self.attrs\n        assert_identical(result2, expected2)\n\n        # skipna=False\n        if nanindex is not None and ar0.dtype.kind != ""O"":\n            expected3 = coordarr0.isel(x=nanindex, drop=True).astype(""float"")\n            expected3.name = ""x""\n            expected3.attrs = {}\n        else:\n            expected3 = expected0.copy()\n\n        result3 = ar0.idxmin(skipna=False)\n        assert_identical(result3, expected3)\n\n        # fill_value should be ignored with skipna=False\n        result4 = ar0.idxmin(skipna=False, fill_value=-100j)\n        assert_identical(result4, expected3)\n\n        # Float fill_value\n        if hasna:\n            fill_value_5 = -1.1\n        else:\n            fill_value_5 = 1\n\n        expected5 = (coordarr1 * fill_value_5).isel(x=minindex, drop=True)\n        expected5.name = ""x""\n\n        result5 = ar0.idxmin(fill_value=-1.1)\n        assert_identical(result5, expected5)\n\n        # Integer fill_value\n        if hasna:\n            fill_value_6 = -1\n        else:\n            fill_value_6 = 1\n\n        expected6 = (coordarr1 * fill_value_6).isel(x=minindex, drop=True)\n        expected6.name = ""x""\n\n        result6 = ar0.idxmin(fill_value=-1)\n        assert_identical(result6, expected6)\n\n        # Complex fill_value\n        if hasna:\n            fill_value_7 = -1j\n        else:\n            fill_value_7 = 1\n\n        expected7 = (coordarr1 * fill_value_7).isel(x=minindex, drop=True)\n        expected7.name = ""x""\n\n        result7 = ar0.idxmin(fill_value=-1j)\n        assert_identical(result7, expected7)\n\n    @pytest.mark.parametrize(""use_dask"", [True, False])\n    def test_idxmax(self, x, minindex, maxindex, nanindex, use_dask):\n        if use_dask and not has_dask:\n            pytest.skip(""requires dask"")\n        if use_dask and x.dtype.kind == ""M"":\n            pytest.xfail(""dask operation \'argmax\' breaks when dtype is datetime64 (M)"")\n        ar0_raw = xr.DataArray(\n            x, dims=[""x""], coords={""x"": np.arange(x.size) * 4}, attrs=self.attrs\n        )\n\n        if use_dask:\n            ar0 = ar0_raw.chunk({})\n        else:\n            ar0 = ar0_raw\n\n        # dim doesn\'t exist\n        with pytest.raises(KeyError):\n            ar0.idxmax(dim=""spam"")\n\n        # Scalar Dataarray\n        with pytest.raises(ValueError):\n            xr.DataArray(5).idxmax()\n\n        coordarr0 = xr.DataArray(ar0.coords[""x""], dims=[""x""])\n        coordarr1 = coordarr0.copy()\n\n        hasna = np.isnan(maxindex)\n        if np.isnan(maxindex):\n            maxindex = 0\n\n        if hasna:\n            coordarr1[...] = 1\n            fill_value_0 = np.NaN\n        else:\n            fill_value_0 = 1\n\n        expected0 = (\n            (coordarr1 * fill_value_0).isel(x=maxindex, drop=True).astype(""float"")\n        )\n        expected0.name = ""x""\n\n        # Default fill value (NaN)\n        result0 = ar0.idxmax()\n        assert_identical(result0, expected0)\n\n        # Manually specify NaN fill_value\n        result1 = ar0.idxmax(fill_value=np.NaN)\n        assert_identical(result1, expected0)\n\n        # keep_attrs\n        result2 = ar0.idxmax(keep_attrs=True)\n        expected2 = expected0.copy()\n        expected2.attrs = self.attrs\n        assert_identical(result2, expected2)\n\n        # skipna=False\n        if nanindex is not None and ar0.dtype.kind != ""O"":\n            expected3 = coordarr0.isel(x=nanindex, drop=True).astype(""float"")\n            expected3.name = ""x""\n            expected3.attrs = {}\n        else:\n            expected3 = expected0.copy()\n\n        result3 = ar0.idxmax(skipna=False)\n        assert_identical(result3, expected3)\n\n        # fill_value should be ignored with skipna=False\n        result4 = ar0.idxmax(skipna=False, fill_value=-100j)\n        assert_identical(result4, expected3)\n\n        # Float fill_value\n        if hasna:\n            fill_value_5 = -1.1\n        else:\n            fill_value_5 = 1\n\n        expected5 = (coordarr1 * fill_value_5).isel(x=maxindex, drop=True)\n        expected5.name = ""x""\n\n        result5 = ar0.idxmax(fill_value=-1.1)\n        assert_identical(result5, expected5)\n\n        # Integer fill_value\n        if hasna:\n            fill_value_6 = -1\n        else:\n            fill_value_6 = 1\n\n        expected6 = (coordarr1 * fill_value_6).isel(x=maxindex, drop=True)\n        expected6.name = ""x""\n\n        result6 = ar0.idxmax(fill_value=-1)\n        assert_identical(result6, expected6)\n\n        # Complex fill_value\n        if hasna:\n            fill_value_7 = -1j\n        else:\n            fill_value_7 = 1\n\n        expected7 = (coordarr1 * fill_value_7).isel(x=maxindex, drop=True)\n        expected7.name = ""x""\n\n        result7 = ar0.idxmax(fill_value=-1j)\n        assert_identical(result7, expected7)\n\n\n@pytest.mark.parametrize(\n    ""x, minindex, maxindex, nanindex"",\n    [\n        (\n            np.array(\n                [\n                    [0, 1, 2, 0, -2, -4, 2],\n                    [1, 1, 1, 1, 1, 1, 1],\n                    [0, 0, -10, 5, 20, 0, 0],\n                ]\n            ),\n            [5, 0, 2],\n            [2, 0, 4],\n            [None, None, None],\n        ),\n        (\n            np.array(\n                [\n                    [2.0, 1.0, 2.0, 0.0, -2.0, -4.0, 2.0],\n                    [-4.0, np.NaN, 2.0, np.NaN, -2.0, -4.0, 2.0],\n                    [np.NaN] * 7,\n                ]\n            ),\n            [5, 0, np.NaN],\n            [0, 2, np.NaN],\n            [None, 1, 0],\n        ),\n        (\n            np.array(\n                [\n                    [2.0, 1.0, 2.0, 0.0, -2.0, -4.0, 2.0],\n                    [-4.0, np.NaN, 2.0, np.NaN, -2.0, -4.0, 2.0],\n                    [np.NaN] * 7,\n                ]\n            ).astype(""object""),\n            [5, 0, np.NaN],\n            [0, 2, np.NaN],\n            [None, 1, 0],\n        ),\n        (\n            np.array(\n                [\n                    [""2015-12-31"", ""2020-01-02"", ""2020-01-01"", ""2016-01-01""],\n                    [""2020-01-02"", ""2020-01-02"", ""2020-01-02"", ""2020-01-02""],\n                    [""1900-01-01"", ""1-02-03"", ""1900-01-02"", ""1-02-03""],\n                ],\n                dtype=""datetime64[ns]"",\n            ),\n            [0, 0, 1],\n            [1, 0, 2],\n            [None, None, None],\n        ),\n    ],\n)\nclass TestReduce2D(TestReduce):\n    def test_min(self, x, minindex, maxindex, nanindex):\n        ar = xr.DataArray(\n            x,\n            dims=[""y"", ""x""],\n            coords={""x"": np.arange(x.shape[1]) * 4, ""y"": 1 - np.arange(x.shape[0])},\n            attrs=self.attrs,\n        )\n\n        minindex = [x if not np.isnan(x) else 0 for x in minindex]\n        expected0 = [\n            ar.isel(y=yi).isel(x=indi, drop=True) for yi, indi in enumerate(minindex)\n        ]\n        expected0 = xr.concat(expected0, dim=""y"")\n\n        result0 = ar.min(dim=""x"", keep_attrs=True)\n        assert_identical(result0, expected0)\n\n        result1 = ar.min(dim=""x"")\n        expected1 = expected0\n        expected1.attrs = {}\n        assert_identical(result1, expected1)\n\n        result2 = ar.min(axis=1)\n        assert_identical(result2, expected1)\n\n        minindex = [\n            x if y is None or ar.dtype.kind == ""O"" else y\n            for x, y in zip(minindex, nanindex)\n        ]\n        expected2 = [\n            ar.isel(y=yi).isel(x=indi, drop=True) for yi, indi in enumerate(minindex)\n        ]\n        expected2 = xr.concat(expected2, dim=""y"")\n        expected2.attrs = {}\n\n        result3 = ar.min(dim=""x"", skipna=False)\n\n        assert_identical(result3, expected2)\n\n    def test_max(self, x, minindex, maxindex, nanindex):\n        ar = xr.DataArray(\n            x,\n            dims=[""y"", ""x""],\n            coords={""x"": np.arange(x.shape[1]) * 4, ""y"": 1 - np.arange(x.shape[0])},\n            attrs=self.attrs,\n        )\n\n        maxindex = [x if not np.isnan(x) else 0 for x in maxindex]\n        expected0 = [\n            ar.isel(y=yi).isel(x=indi, drop=True) for yi, indi in enumerate(maxindex)\n        ]\n        expected0 = xr.concat(expected0, dim=""y"")\n\n        result0 = ar.max(dim=""x"", keep_attrs=True)\n        assert_identical(result0, expected0)\n\n        result1 = ar.max(dim=""x"")\n        expected1 = expected0.copy()\n        expected1.attrs = {}\n        assert_identical(result1, expected1)\n\n        result2 = ar.max(axis=1)\n        assert_identical(result2, expected1)\n\n        maxindex = [\n            x if y is None or ar.dtype.kind == ""O"" else y\n            for x, y in zip(maxindex, nanindex)\n        ]\n        expected2 = [\n            ar.isel(y=yi).isel(x=indi, drop=True) for yi, indi in enumerate(maxindex)\n        ]\n        expected2 = xr.concat(expected2, dim=""y"")\n        expected2.attrs = {}\n\n        result3 = ar.max(dim=""x"", skipna=False)\n\n        assert_identical(result3, expected2)\n\n    def test_argmin(self, x, minindex, maxindex, nanindex):\n        ar = xr.DataArray(\n            x,\n            dims=[""y"", ""x""],\n            coords={""x"": np.arange(x.shape[1]) * 4, ""y"": 1 - np.arange(x.shape[0])},\n            attrs=self.attrs,\n        )\n        indarr = np.tile(np.arange(x.shape[1], dtype=np.intp), [x.shape[0], 1])\n        indarr = xr.DataArray(indarr, dims=ar.dims, coords=ar.coords)\n\n        if np.isnan(minindex).any():\n            with pytest.raises(ValueError):\n                ar.argmin(dim=""x"")\n            return\n\n        expected0 = [\n            indarr.isel(y=yi).isel(x=indi, drop=True)\n            for yi, indi in enumerate(minindex)\n        ]\n        expected0 = xr.concat(expected0, dim=""y"")\n\n        result0 = ar.argmin(dim=""x"")\n        assert_identical(result0, expected0)\n\n        result1 = ar.argmin(axis=1)\n        assert_identical(result1, expected0)\n\n        result2 = ar.argmin(dim=""x"", keep_attrs=True)\n        expected1 = expected0.copy()\n        expected1.attrs = self.attrs\n        assert_identical(result2, expected1)\n\n        minindex = [\n            x if y is None or ar.dtype.kind == ""O"" else y\n            for x, y in zip(minindex, nanindex)\n        ]\n        expected2 = [\n            indarr.isel(y=yi).isel(x=indi, drop=True)\n            for yi, indi in enumerate(minindex)\n        ]\n        expected2 = xr.concat(expected2, dim=""y"")\n        expected2.attrs = {}\n\n        result3 = ar.argmin(dim=""x"", skipna=False)\n\n        assert_identical(result3, expected2)\n\n    def test_argmax(self, x, minindex, maxindex, nanindex):\n        ar = xr.DataArray(\n            x,\n            dims=[""y"", ""x""],\n            coords={""x"": np.arange(x.shape[1]) * 4, ""y"": 1 - np.arange(x.shape[0])},\n            attrs=self.attrs,\n        )\n        indarr = np.tile(np.arange(x.shape[1], dtype=np.intp), [x.shape[0], 1])\n        indarr = xr.DataArray(indarr, dims=ar.dims, coords=ar.coords)\n\n        if np.isnan(maxindex).any():\n            with pytest.raises(ValueError):\n                ar.argmax(dim=""x"")\n            return\n\n        expected0 = [\n            indarr.isel(y=yi).isel(x=indi, drop=True)\n            for yi, indi in enumerate(maxindex)\n        ]\n        expected0 = xr.concat(expected0, dim=""y"")\n\n        result0 = ar.argmax(dim=""x"")\n        assert_identical(result0, expected0)\n\n        result1 = ar.argmax(axis=1)\n        assert_identical(result1, expected0)\n\n        result2 = ar.argmax(dim=""x"", keep_attrs=True)\n        expected1 = expected0.copy()\n        expected1.attrs = self.attrs\n        assert_identical(result2, expected1)\n\n        maxindex = [\n            x if y is None or ar.dtype.kind == ""O"" else y\n            for x, y in zip(maxindex, nanindex)\n        ]\n        expected2 = [\n            indarr.isel(y=yi).isel(x=indi, drop=True)\n            for yi, indi in enumerate(maxindex)\n        ]\n        expected2 = xr.concat(expected2, dim=""y"")\n        expected2.attrs = {}\n\n        result3 = ar.argmax(dim=""x"", skipna=False)\n\n        assert_identical(result3, expected2)\n\n    @pytest.mark.parametrize(""use_dask"", [True, False])\n    def test_idxmin(self, x, minindex, maxindex, nanindex, use_dask):\n        if use_dask and not has_dask:\n            pytest.skip(""requires dask"")\n        if use_dask and x.dtype.kind == ""M"":\n            pytest.xfail(""dask operation \'argmin\' breaks when dtype is datetime64 (M)"")\n\n        if x.dtype.kind == ""O"":\n            # TODO: nanops._nan_argminmax_object computes once to check for all-NaN slices.\n            max_computes = 1\n        else:\n            max_computes = 0\n\n        ar0_raw = xr.DataArray(\n            x,\n            dims=[""y"", ""x""],\n            coords={""x"": np.arange(x.shape[1]) * 4, ""y"": 1 - np.arange(x.shape[0])},\n            attrs=self.attrs,\n        )\n\n        if use_dask:\n            ar0 = ar0_raw.chunk({})\n        else:\n            ar0 = ar0_raw\n\n        assert_identical(ar0, ar0)\n\n        # No dimension specified\n        with pytest.raises(ValueError):\n            ar0.idxmin()\n\n        # dim doesn\'t exist\n        with pytest.raises(KeyError):\n            ar0.idxmin(dim=""Y"")\n\n        assert_identical(ar0, ar0)\n\n        coordarr0 = xr.DataArray(\n            np.tile(ar0.coords[""x""], [x.shape[0], 1]), dims=ar0.dims, coords=ar0.coords\n        )\n\n        hasna = [np.isnan(x) for x in minindex]\n        coordarr1 = coordarr0.copy()\n        coordarr1[hasna, :] = 1\n        minindex0 = [x if not np.isnan(x) else 0 for x in minindex]\n\n        nan_mult_0 = np.array([np.NaN if x else 1 for x in hasna])[:, None]\n        expected0 = [\n            (coordarr1 * nan_mult_0).isel(y=yi).isel(x=indi, drop=True)\n            for yi, indi in enumerate(minindex0)\n        ]\n        expected0 = xr.concat(expected0, dim=""y"")\n        expected0.name = ""x""\n\n        # Default fill value (NaN)\n        with raise_if_dask_computes(max_computes=max_computes):\n            result0 = ar0.idxmin(dim=""x"")\n        assert_identical(result0, expected0)\n\n        # Manually specify NaN fill_value\n        with raise_if_dask_computes(max_computes=max_computes):\n            result1 = ar0.idxmin(dim=""x"", fill_value=np.NaN)\n        assert_identical(result1, expected0)\n\n        # keep_attrs\n        with raise_if_dask_computes(max_computes=max_computes):\n            result2 = ar0.idxmin(dim=""x"", keep_attrs=True)\n        expected2 = expected0.copy()\n        expected2.attrs = self.attrs\n        assert_identical(result2, expected2)\n\n        # skipna=False\n        minindex3 = [\n            x if y is None or ar0.dtype.kind == ""O"" else y\n            for x, y in zip(minindex0, nanindex)\n        ]\n        expected3 = [\n            coordarr0.isel(y=yi).isel(x=indi, drop=True)\n            for yi, indi in enumerate(minindex3)\n        ]\n        expected3 = xr.concat(expected3, dim=""y"")\n        expected3.name = ""x""\n        expected3.attrs = {}\n\n        with raise_if_dask_computes(max_computes=max_computes):\n            result3 = ar0.idxmin(dim=""x"", skipna=False)\n        assert_identical(result3, expected3)\n\n        # fill_value should be ignored with skipna=False\n        with raise_if_dask_computes(max_computes=max_computes):\n            result4 = ar0.idxmin(dim=""x"", skipna=False, fill_value=-100j)\n        assert_identical(result4, expected3)\n\n        # Float fill_value\n        nan_mult_5 = np.array([-1.1 if x else 1 for x in hasna])[:, None]\n        expected5 = [\n            (coordarr1 * nan_mult_5).isel(y=yi).isel(x=indi, drop=True)\n            for yi, indi in enumerate(minindex0)\n        ]\n        expected5 = xr.concat(expected5, dim=""y"")\n        expected5.name = ""x""\n\n        with raise_if_dask_computes(max_computes=max_computes):\n            result5 = ar0.idxmin(dim=""x"", fill_value=-1.1)\n        assert_identical(result5, expected5)\n\n        # Integer fill_value\n        nan_mult_6 = np.array([-1 if x else 1 for x in hasna])[:, None]\n        expected6 = [\n            (coordarr1 * nan_mult_6).isel(y=yi).isel(x=indi, drop=True)\n            for yi, indi in enumerate(minindex0)\n        ]\n        expected6 = xr.concat(expected6, dim=""y"")\n        expected6.name = ""x""\n\n        with raise_if_dask_computes(max_computes=max_computes):\n            result6 = ar0.idxmin(dim=""x"", fill_value=-1)\n        assert_identical(result6, expected6)\n\n        # Complex fill_value\n        nan_mult_7 = np.array([-5j if x else 1 for x in hasna])[:, None]\n        expected7 = [\n            (coordarr1 * nan_mult_7).isel(y=yi).isel(x=indi, drop=True)\n            for yi, indi in enumerate(minindex0)\n        ]\n        expected7 = xr.concat(expected7, dim=""y"")\n        expected7.name = ""x""\n\n        with raise_if_dask_computes(max_computes=max_computes):\n            result7 = ar0.idxmin(dim=""x"", fill_value=-5j)\n        assert_identical(result7, expected7)\n\n    @pytest.mark.parametrize(""use_dask"", [True, False])\n    def test_idxmax(self, x, minindex, maxindex, nanindex, use_dask):\n        if use_dask and not has_dask:\n            pytest.skip(""requires dask"")\n        if use_dask and x.dtype.kind == ""M"":\n            pytest.xfail(""dask operation \'argmax\' breaks when dtype is datetime64 (M)"")\n\n        if x.dtype.kind == ""O"":\n            # TODO: nanops._nan_argminmax_object computes once to check for all-NaN slices.\n            max_computes = 1\n        else:\n            max_computes = 0\n\n        ar0_raw = xr.DataArray(\n            x,\n            dims=[""y"", ""x""],\n            coords={""x"": np.arange(x.shape[1]) * 4, ""y"": 1 - np.arange(x.shape[0])},\n            attrs=self.attrs,\n        )\n\n        if use_dask:\n            ar0 = ar0_raw.chunk({})\n        else:\n            ar0 = ar0_raw\n\n        # No dimension specified\n        with pytest.raises(ValueError):\n            ar0.idxmax()\n\n        # dim doesn\'t exist\n        with pytest.raises(KeyError):\n            ar0.idxmax(dim=""Y"")\n\n        ar1 = ar0.copy()\n        del ar1.coords[""y""]\n        with pytest.raises(KeyError):\n            ar1.idxmax(dim=""y"")\n\n        coordarr0 = xr.DataArray(\n            np.tile(ar0.coords[""x""], [x.shape[0], 1]), dims=ar0.dims, coords=ar0.coords\n        )\n\n        hasna = [np.isnan(x) for x in maxindex]\n        coordarr1 = coordarr0.copy()\n        coordarr1[hasna, :] = 1\n        maxindex0 = [x if not np.isnan(x) else 0 for x in maxindex]\n\n        nan_mult_0 = np.array([np.NaN if x else 1 for x in hasna])[:, None]\n        expected0 = [\n            (coordarr1 * nan_mult_0).isel(y=yi).isel(x=indi, drop=True)\n            for yi, indi in enumerate(maxindex0)\n        ]\n        expected0 = xr.concat(expected0, dim=""y"")\n        expected0.name = ""x""\n\n        # Default fill value (NaN)\n        with raise_if_dask_computes(max_computes=max_computes):\n            result0 = ar0.idxmax(dim=""x"")\n        assert_identical(result0, expected0)\n\n        # Manually specify NaN fill_value\n        with raise_if_dask_computes(max_computes=max_computes):\n            result1 = ar0.idxmax(dim=""x"", fill_value=np.NaN)\n        assert_identical(result1, expected0)\n\n        # keep_attrs\n        with raise_if_dask_computes(max_computes=max_computes):\n            result2 = ar0.idxmax(dim=""x"", keep_attrs=True)\n        expected2 = expected0.copy()\n        expected2.attrs = self.attrs\n        assert_identical(result2, expected2)\n\n        # skipna=False\n        maxindex3 = [\n            x if y is None or ar0.dtype.kind == ""O"" else y\n            for x, y in zip(maxindex0, nanindex)\n        ]\n        expected3 = [\n            coordarr0.isel(y=yi).isel(x=indi, drop=True)\n            for yi, indi in enumerate(maxindex3)\n        ]\n        expected3 = xr.concat(expected3, dim=""y"")\n        expected3.name = ""x""\n        expected3.attrs = {}\n\n        with raise_if_dask_computes(max_computes=max_computes):\n            result3 = ar0.idxmax(dim=""x"", skipna=False)\n        assert_identical(result3, expected3)\n\n        # fill_value should be ignored with skipna=False\n        with raise_if_dask_computes(max_computes=max_computes):\n            result4 = ar0.idxmax(dim=""x"", skipna=False, fill_value=-100j)\n        assert_identical(result4, expected3)\n\n        # Float fill_value\n        nan_mult_5 = np.array([-1.1 if x else 1 for x in hasna])[:, None]\n        expected5 = [\n            (coordarr1 * nan_mult_5).isel(y=yi).isel(x=indi, drop=True)\n            for yi, indi in enumerate(maxindex0)\n        ]\n        expected5 = xr.concat(expected5, dim=""y"")\n        expected5.name = ""x""\n\n        with raise_if_dask_computes(max_computes=max_computes):\n            result5 = ar0.idxmax(dim=""x"", fill_value=-1.1)\n        assert_identical(result5, expected5)\n\n        # Integer fill_value\n        nan_mult_6 = np.array([-1 if x else 1 for x in hasna])[:, None]\n        expected6 = [\n            (coordarr1 * nan_mult_6).isel(y=yi).isel(x=indi, drop=True)\n            for yi, indi in enumerate(maxindex0)\n        ]\n        expected6 = xr.concat(expected6, dim=""y"")\n        expected6.name = ""x""\n\n        with raise_if_dask_computes(max_computes=max_computes):\n            result6 = ar0.idxmax(dim=""x"", fill_value=-1)\n        assert_identical(result6, expected6)\n\n        # Complex fill_value\n        nan_mult_7 = np.array([-5j if x else 1 for x in hasna])[:, None]\n        expected7 = [\n            (coordarr1 * nan_mult_7).isel(y=yi).isel(x=indi, drop=True)\n            for yi, indi in enumerate(maxindex0)\n        ]\n        expected7 = xr.concat(expected7, dim=""y"")\n        expected7.name = ""x""\n\n        with raise_if_dask_computes(max_computes=max_computes):\n            result7 = ar0.idxmax(dim=""x"", fill_value=-5j)\n        assert_identical(result7, expected7)\n\n\n@pytest.fixture(params=[1])\ndef da(request):\n    if request.param == 1:\n        times = pd.date_range(""2000-01-01"", freq=""1D"", periods=21)\n        values = np.random.random((3, 21, 4))\n        da = DataArray(values, dims=(""a"", ""time"", ""x""))\n        da[""time""] = times\n        return da\n\n    if request.param == 2:\n        return DataArray([0, np.nan, 1, 2, np.nan, 3, 4, 5, np.nan, 6, 7], dims=""time"")\n\n    if request.param == ""repeating_ints"":\n        return DataArray(\n            np.tile(np.arange(12), 5).reshape(5, 4, 3),\n            coords={""x"": list(""abc""), ""y"": list(""defg"")},\n            dims=list(""zyx""),\n        )\n\n\n@pytest.fixture\ndef da_dask(seed=123):\n    pytest.importorskip(""dask.array"")\n    rs = np.random.RandomState(seed)\n    times = pd.date_range(""2000-01-01"", freq=""1D"", periods=21)\n    values = rs.normal(size=(1, 21, 1))\n    da = DataArray(values, dims=(""a"", ""time"", ""x"")).chunk({""time"": 7})\n    da[""time""] = times\n    return da\n\n\n@pytest.mark.parametrize(""da"", (""repeating_ints"",), indirect=True)\ndef test_isin(da):\n\n    expected = DataArray(\n        np.asarray([[0, 0, 0], [1, 0, 0]]),\n        dims=list(""yx""),\n        coords={""x"": list(""abc""), ""y"": list(""de"")},\n    ).astype(""bool"")\n\n    result = da.isin([3]).sel(y=list(""de""), z=0)\n    assert_equal(result, expected)\n\n    expected = DataArray(\n        np.asarray([[0, 0, 1], [1, 0, 0]]),\n        dims=list(""yx""),\n        coords={""x"": list(""abc""), ""y"": list(""de"")},\n    ).astype(""bool"")\n    result = da.isin([2, 3]).sel(y=list(""de""), z=0)\n    assert_equal(result, expected)\n\n\n@pytest.mark.parametrize(""da"", (1, 2), indirect=True)\ndef test_rolling_iter(da):\n\n    rolling_obj = da.rolling(time=7)\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", ""Mean of empty slice"")\n        rolling_obj_mean = rolling_obj.mean()\n\n    assert len(rolling_obj.window_labels) == len(da[""time""])\n    assert_identical(rolling_obj.window_labels, da[""time""])\n\n    for i, (label, window_da) in enumerate(rolling_obj):\n        assert label == da[""time""].isel(time=i)\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(""ignore"", ""Mean of empty slice"")\n            actual = rolling_obj_mean.isel(time=i)\n            expected = window_da.mean(""time"")\n\n        # TODO add assert_allclose_with_nan, which compares nan position\n        # as well as the closeness of the values.\n        assert_array_equal(actual.isnull(), expected.isnull())\n        if (~actual.isnull()).sum() > 0:\n            np.allclose(\n                actual.values[actual.values.nonzero()],\n                expected.values[expected.values.nonzero()],\n            )\n\n\ndef test_rolling_doc(da):\n    rolling_obj = da.rolling(time=7)\n\n    # argument substitution worked\n    assert ""`mean`"" in rolling_obj.mean.__doc__\n\n\ndef test_rolling_properties(da):\n    rolling_obj = da.rolling(time=4)\n\n    assert rolling_obj.obj.get_axis_num(""time"") == 1\n\n    # catching invalid args\n    with pytest.raises(ValueError, match=""exactly one dim/window should""):\n        da.rolling(time=7, x=2)\n    with pytest.raises(ValueError, match=""window must be > 0""):\n        da.rolling(time=-2)\n    with pytest.raises(ValueError, match=""min_periods must be greater than zero""):\n        da.rolling(time=2, min_periods=0)\n\n\n@pytest.mark.parametrize(""name"", (""sum"", ""mean"", ""std"", ""min"", ""max"", ""median""))\n@pytest.mark.parametrize(""center"", (True, False, None))\n@pytest.mark.parametrize(""min_periods"", (1, None))\ndef test_rolling_wrapped_bottleneck(da, name, center, min_periods):\n    bn = pytest.importorskip(""bottleneck"", minversion=""1.1"")\n\n    # Test all bottleneck functions\n    rolling_obj = da.rolling(time=7, min_periods=min_periods)\n\n    func_name = f""move_{name}""\n    actual = getattr(rolling_obj, name)()\n    expected = getattr(bn, func_name)(\n        da.values, window=7, axis=1, min_count=min_periods\n    )\n    assert_array_equal(actual.values, expected)\n\n    with pytest.warns(DeprecationWarning, match=""Reductions will be applied""):\n        getattr(rolling_obj, name)(dim=""time"")\n\n    # Test center\n    rolling_obj = da.rolling(time=7, center=center)\n    actual = getattr(rolling_obj, name)()[""time""]\n    assert_equal(actual, da[""time""])\n\n\n@requires_dask\n@pytest.mark.parametrize(""name"", (""mean"", ""count""))\n@pytest.mark.parametrize(""center"", (True, False, None))\n@pytest.mark.parametrize(""min_periods"", (1, None))\n@pytest.mark.parametrize(""window"", (7, 8))\ndef test_rolling_wrapped_dask(da_dask, name, center, min_periods, window):\n    # dask version\n    rolling_obj = da_dask.rolling(time=window, min_periods=min_periods, center=center)\n    actual = getattr(rolling_obj, name)().load()\n    if name != ""count"":\n        with pytest.warns(DeprecationWarning, match=""Reductions will be applied""):\n            getattr(rolling_obj, name)(dim=""time"")\n    # numpy version\n    rolling_obj = da_dask.load().rolling(\n        time=window, min_periods=min_periods, center=center\n    )\n    expected = getattr(rolling_obj, name)()\n\n    # using all-close because rolling over ghost cells introduces some\n    # precision errors\n    assert_allclose(actual, expected)\n\n    # with zero chunked array GH:2113\n    rolling_obj = da_dask.chunk().rolling(\n        time=window, min_periods=min_periods, center=center\n    )\n    actual = getattr(rolling_obj, name)().load()\n    assert_allclose(actual, expected)\n\n\n@pytest.mark.parametrize(""center"", (True, None))\ndef test_rolling_wrapped_dask_nochunk(center):\n    # GH:2113\n    pytest.importorskip(""dask.array"")\n\n    da_day_clim = xr.DataArray(\n        np.arange(1, 367), coords=[np.arange(1, 367)], dims=""dayofyear""\n    )\n    expected = da_day_clim.rolling(dayofyear=31, center=center).mean()\n    actual = da_day_clim.chunk().rolling(dayofyear=31, center=center).mean()\n    assert_allclose(actual, expected)\n\n\n@pytest.mark.parametrize(""center"", (True, False))\n@pytest.mark.parametrize(""min_periods"", (None, 1, 2, 3))\n@pytest.mark.parametrize(""window"", (1, 2, 3, 4))\ndef test_rolling_pandas_compat(center, window, min_periods):\n    s = pd.Series(np.arange(10))\n    da = DataArray.from_series(s)\n\n    if min_periods is not None and window < min_periods:\n        min_periods = window\n\n    s_rolling = s.rolling(window, center=center, min_periods=min_periods).mean()\n    da_rolling = da.rolling(index=window, center=center, min_periods=min_periods).mean()\n    da_rolling_np = da.rolling(\n        index=window, center=center, min_periods=min_periods\n    ).reduce(np.nanmean)\n\n    np.testing.assert_allclose(s_rolling.values, da_rolling.values)\n    np.testing.assert_allclose(s_rolling.index, da_rolling[""index""])\n    np.testing.assert_allclose(s_rolling.values, da_rolling_np.values)\n    np.testing.assert_allclose(s_rolling.index, da_rolling_np[""index""])\n\n\n@pytest.mark.parametrize(""center"", (True, False))\n@pytest.mark.parametrize(""window"", (1, 2, 3, 4))\ndef test_rolling_construct(center, window):\n    s = pd.Series(np.arange(10))\n    da = DataArray.from_series(s)\n\n    s_rolling = s.rolling(window, center=center, min_periods=1).mean()\n    da_rolling = da.rolling(index=window, center=center, min_periods=1)\n\n    da_rolling_mean = da_rolling.construct(""window"").mean(""window"")\n    np.testing.assert_allclose(s_rolling.values, da_rolling_mean.values)\n    np.testing.assert_allclose(s_rolling.index, da_rolling_mean[""index""])\n\n    # with stride\n    da_rolling_mean = da_rolling.construct(""window"", stride=2).mean(""window"")\n    np.testing.assert_allclose(s_rolling.values[::2], da_rolling_mean.values)\n    np.testing.assert_allclose(s_rolling.index[::2], da_rolling_mean[""index""])\n\n    # with fill_value\n    da_rolling_mean = da_rolling.construct(""window"", stride=2, fill_value=0.0).mean(\n        ""window""\n    )\n    assert da_rolling_mean.isnull().sum() == 0\n    assert (da_rolling_mean == 0.0).sum() >= 0\n\n\n@pytest.mark.parametrize(""da"", (1, 2), indirect=True)\n@pytest.mark.parametrize(""center"", (True, False))\n@pytest.mark.parametrize(""min_periods"", (None, 1, 2, 3))\n@pytest.mark.parametrize(""window"", (1, 2, 3, 4))\n@pytest.mark.parametrize(""name"", (""sum"", ""mean"", ""std"", ""max""))\ndef test_rolling_reduce(da, center, min_periods, window, name):\n\n    if min_periods is not None and window < min_periods:\n        min_periods = window\n\n    if da.isnull().sum() > 1 and window == 1:\n        # this causes all nan slices\n        window = 2\n\n    rolling_obj = da.rolling(time=window, center=center, min_periods=min_periods)\n\n    # add nan prefix to numpy methods to get similar # behavior as bottleneck\n    actual = rolling_obj.reduce(getattr(np, ""nan%s"" % name))\n    expected = getattr(rolling_obj, name)()\n    assert_allclose(actual, expected)\n    assert actual.dims == expected.dims\n\n\n@pytest.mark.parametrize(""center"", (True, False))\n@pytest.mark.parametrize(""min_periods"", (None, 1, 2, 3))\n@pytest.mark.parametrize(""window"", (1, 2, 3, 4))\n@pytest.mark.parametrize(""name"", (""sum"", ""max""))\ndef test_rolling_reduce_nonnumeric(center, min_periods, window, name):\n    da = DataArray(\n        [0, np.nan, 1, 2, np.nan, 3, 4, 5, np.nan, 6, 7], dims=""time""\n    ).isnull()\n\n    if min_periods is not None and window < min_periods:\n        min_periods = window\n\n    rolling_obj = da.rolling(time=window, center=center, min_periods=min_periods)\n\n    # add nan prefix to numpy methods to get similar behavior as bottleneck\n    actual = rolling_obj.reduce(getattr(np, ""nan%s"" % name))\n    expected = getattr(rolling_obj, name)()\n    assert_allclose(actual, expected)\n    assert actual.dims == expected.dims\n\n\ndef test_rolling_count_correct():\n\n    da = DataArray([0, np.nan, 1, 2, np.nan, 3, 4, 5, np.nan, 6, 7], dims=""time"")\n\n    kwargs = [\n        {""time"": 11, ""min_periods"": 1},\n        {""time"": 11, ""min_periods"": None},\n        {""time"": 7, ""min_periods"": 2},\n    ]\n    expecteds = [\n        DataArray([1, 1, 2, 3, 3, 4, 5, 6, 6, 7, 8], dims=""time""),\n        DataArray(\n            [\n                np.nan,\n                np.nan,\n                np.nan,\n                np.nan,\n                np.nan,\n                np.nan,\n                np.nan,\n                np.nan,\n                np.nan,\n                np.nan,\n                np.nan,\n            ],\n            dims=""time"",\n        ),\n        DataArray([np.nan, np.nan, 2, 3, 3, 4, 5, 5, 5, 5, 5], dims=""time""),\n    ]\n\n    for kwarg, expected in zip(kwargs, expecteds):\n        result = da.rolling(**kwarg).count()\n        assert_equal(result, expected)\n\n        result = da.to_dataset(name=""var1"").rolling(**kwarg).count()[""var1""]\n        assert_equal(result, expected)\n\n\ndef test_raise_no_warning_for_nan_in_binary_ops():\n    with pytest.warns(None) as record:\n        xr.DataArray([1, 2, np.NaN]) > 0\n    assert len(record) == 0\n\n\ndef test_name_in_masking():\n    name = ""RingoStarr""\n    da = xr.DataArray(range(10), coords=[(""x"", range(10))], name=name)\n    assert da.where(da > 5).name == name\n    assert da.where((da > 5).rename(""YokoOno"")).name == name\n    assert da.where(da > 5, drop=True).name == name\n    assert da.where((da > 5).rename(""YokoOno""), drop=True).name == name\n\n\nclass TestIrisConversion:\n    @requires_iris\n    def test_to_and_from_iris(self):\n        import iris\n        import cf_units  # iris requirement\n\n        # to iris\n        coord_dict = {}\n        coord_dict[""distance""] = (""distance"", [-2, 2], {""units"": ""meters""})\n        coord_dict[""time""] = (""time"", pd.date_range(""2000-01-01"", periods=3))\n        coord_dict[""height""] = 10\n        coord_dict[""distance2""] = (""distance"", [0, 1], {""foo"": ""bar""})\n        coord_dict[""time2""] = ((""distance"", ""time""), [[0, 1, 2], [2, 3, 4]])\n\n        original = DataArray(\n            np.arange(6, dtype=""float"").reshape(2, 3),\n            coord_dict,\n            name=""Temperature"",\n            attrs={\n                ""baz"": 123,\n                ""units"": ""Kelvin"",\n                ""standard_name"": ""fire_temperature"",\n                ""long_name"": ""Fire Temperature"",\n            },\n            dims=(""distance"", ""time""),\n        )\n\n        # Set a bad value to test the masking logic\n        original.data[0, 2] = np.NaN\n\n        original.attrs[""cell_methods""] = ""height: mean (comment: A cell method)""\n        actual = original.to_iris()\n        assert_array_equal(actual.data, original.data)\n        assert actual.var_name == original.name\n        assert tuple(d.var_name for d in actual.dim_coords) == original.dims\n        assert actual.cell_methods == (\n            iris.coords.CellMethod(\n                method=""mean"",\n                coords=(""height"",),\n                intervals=(),\n                comments=(""A cell method"",),\n            ),\n        )\n\n        for coord, orginal_key in zip((actual.coords()), original.coords):\n            original_coord = original.coords[orginal_key]\n            assert coord.var_name == original_coord.name\n            assert_array_equal(\n                coord.points, CFDatetimeCoder().encode(original_coord).values\n            )\n            assert actual.coord_dims(coord) == original.get_axis_num(\n                original.coords[coord.var_name].dims\n            )\n\n        assert (\n            actual.coord(""distance2"").attributes[""foo""]\n            == original.coords[""distance2""].attrs[""foo""]\n        )\n        assert actual.coord(""distance"").units == cf_units.Unit(\n            original.coords[""distance""].units\n        )\n        assert actual.attributes[""baz""] == original.attrs[""baz""]\n        assert actual.standard_name == original.attrs[""standard_name""]\n\n        roundtripped = DataArray.from_iris(actual)\n        assert_identical(original, roundtripped)\n\n        actual.remove_coord(""time"")\n        auto_time_dimension = DataArray.from_iris(actual)\n        assert auto_time_dimension.dims == (""distance"", ""dim_1"")\n\n    @requires_iris\n    @requires_dask\n    def test_to_and_from_iris_dask(self):\n        import dask.array as da\n        import iris\n        import cf_units  # iris requirement\n\n        coord_dict = {}\n        coord_dict[""distance""] = (""distance"", [-2, 2], {""units"": ""meters""})\n        coord_dict[""time""] = (""time"", pd.date_range(""2000-01-01"", periods=3))\n        coord_dict[""height""] = 10\n        coord_dict[""distance2""] = (""distance"", [0, 1], {""foo"": ""bar""})\n        coord_dict[""time2""] = ((""distance"", ""time""), [[0, 1, 2], [2, 3, 4]])\n\n        original = DataArray(\n            da.from_array(np.arange(-1, 5, dtype=""float"").reshape(2, 3), 3),\n            coord_dict,\n            name=""Temperature"",\n            attrs=dict(\n                baz=123,\n                units=""Kelvin"",\n                standard_name=""fire_temperature"",\n                long_name=""Fire Temperature"",\n            ),\n            dims=(""distance"", ""time""),\n        )\n\n        # Set a bad value to test the masking logic\n        original.data = da.ma.masked_less(original.data, 0)\n\n        original.attrs[""cell_methods""] = ""height: mean (comment: A cell method)""\n        actual = original.to_iris()\n\n        # Be careful not to trigger the loading of the iris data\n        actual_data = (\n            actual.core_data() if hasattr(actual, ""core_data"") else actual.data\n        )\n        assert_array_equal(actual_data, original.data)\n        assert actual.var_name == original.name\n        assert tuple(d.var_name for d in actual.dim_coords) == original.dims\n        assert actual.cell_methods == (\n            iris.coords.CellMethod(\n                method=""mean"",\n                coords=(""height"",),\n                intervals=(),\n                comments=(""A cell method"",),\n            ),\n        )\n\n        for coord, orginal_key in zip((actual.coords()), original.coords):\n            original_coord = original.coords[orginal_key]\n            assert coord.var_name == original_coord.name\n            assert_array_equal(\n                coord.points, CFDatetimeCoder().encode(original_coord).values\n            )\n            assert actual.coord_dims(coord) == original.get_axis_num(\n                original.coords[coord.var_name].dims\n            )\n\n        assert (\n            actual.coord(""distance2"").attributes[""foo""]\n            == original.coords[""distance2""].attrs[""foo""]\n        )\n        assert actual.coord(""distance"").units == cf_units.Unit(\n            original.coords[""distance""].units\n        )\n        assert actual.attributes[""baz""] == original.attrs[""baz""]\n        assert actual.standard_name == original.attrs[""standard_name""]\n\n        roundtripped = DataArray.from_iris(actual)\n        assert_identical(original, roundtripped)\n\n        # If the Iris version supports it then we should have a dask array\n        # at each stage of the conversion\n        if hasattr(actual, ""core_data""):\n            assert isinstance(original.data, type(actual.core_data()))\n            assert isinstance(original.data, type(roundtripped.data))\n\n        actual.remove_coord(""time"")\n        auto_time_dimension = DataArray.from_iris(actual)\n        assert auto_time_dimension.dims == (""distance"", ""dim_1"")\n\n    @requires_iris\n    @pytest.mark.parametrize(\n        ""var_name, std_name, long_name, name, attrs"",\n        [\n            (\n                ""var_name"",\n                ""height"",\n                ""Height"",\n                ""var_name"",\n                {""standard_name"": ""height"", ""long_name"": ""Height""},\n            ),\n            (\n                None,\n                ""height"",\n                ""Height"",\n                ""height"",\n                {""standard_name"": ""height"", ""long_name"": ""Height""},\n            ),\n            (None, None, ""Height"", ""Height"", {""long_name"": ""Height""}),\n            (None, None, None, None, {}),\n        ],\n    )\n    def test_da_name_from_cube(self, std_name, long_name, var_name, name, attrs):\n        from iris.cube import Cube\n\n        data = []\n        cube = Cube(\n            data, var_name=var_name, standard_name=std_name, long_name=long_name\n        )\n        result = xr.DataArray.from_iris(cube)\n        expected = xr.DataArray(data, name=name, attrs=attrs)\n        xr.testing.assert_identical(result, expected)\n\n    @requires_iris\n    @pytest.mark.parametrize(\n        ""var_name, std_name, long_name, name, attrs"",\n        [\n            (\n                ""var_name"",\n                ""height"",\n                ""Height"",\n                ""var_name"",\n                {""standard_name"": ""height"", ""long_name"": ""Height""},\n            ),\n            (\n                None,\n                ""height"",\n                ""Height"",\n                ""height"",\n                {""standard_name"": ""height"", ""long_name"": ""Height""},\n            ),\n            (None, None, ""Height"", ""Height"", {""long_name"": ""Height""}),\n            (None, None, None, ""unknown"", {}),\n        ],\n    )\n    def test_da_coord_name_from_cube(self, std_name, long_name, var_name, name, attrs):\n        from iris.cube import Cube\n        from iris.coords import DimCoord\n\n        latitude = DimCoord(\n            [-90, 0, 90], standard_name=std_name, var_name=var_name, long_name=long_name\n        )\n        data = [0, 0, 0]\n        cube = Cube(data, dim_coords_and_dims=[(latitude, 0)])\n        result = xr.DataArray.from_iris(cube)\n        expected = xr.DataArray(data, coords=[(name, [-90, 0, 90], attrs)])\n        xr.testing.assert_identical(result, expected)\n\n    @requires_iris\n    def test_prevent_duplicate_coord_names(self):\n        from iris.cube import Cube\n        from iris.coords import DimCoord\n\n        # Iris enforces unique coordinate names. Because we use a different\n        # name resolution order a valid iris Cube with coords that have the\n        # same var_name would lead to duplicate dimension names in the\n        # DataArray\n        longitude = DimCoord([0, 360], standard_name=""longitude"", var_name=""duplicate"")\n        latitude = DimCoord(\n            [-90, 0, 90], standard_name=""latitude"", var_name=""duplicate""\n        )\n        data = [[0, 0, 0], [0, 0, 0]]\n        cube = Cube(data, dim_coords_and_dims=[(longitude, 0), (latitude, 1)])\n        with pytest.raises(ValueError):\n            xr.DataArray.from_iris(cube)\n\n    @requires_iris\n    @pytest.mark.parametrize(\n        ""coord_values"",\n        [[""IA"", ""IL"", ""IN""], [0, 2, 1]],  # non-numeric values  # non-monotonic values\n    )\n    def test_fallback_to_iris_AuxCoord(self, coord_values):\n        from iris.cube import Cube\n        from iris.coords import AuxCoord\n\n        data = [0, 0, 0]\n        da = xr.DataArray(data, coords=[coord_values], dims=[""space""])\n        result = xr.DataArray.to_iris(da)\n        expected = Cube(\n            data, aux_coords_and_dims=[(AuxCoord(coord_values, var_name=""space""), 0)]\n        )\n        assert result == expected\n\n\n@requires_numbagg\n@pytest.mark.parametrize(""dim"", [""time"", ""x""])\n@pytest.mark.parametrize(\n    ""window_type, window"", [[""span"", 5], [""alpha"", 0.5], [""com"", 0.5], [""halflife"", 5]]\n)\ndef test_rolling_exp(da, dim, window_type, window):\n    da = da.isel(a=0)\n    da = da.where(da > 0.2)\n\n    result = da.rolling_exp(window_type=window_type, **{dim: window}).mean()\n    assert isinstance(result, DataArray)\n\n    pandas_array = da.to_pandas()\n    assert pandas_array.index.name == ""time""\n    if dim == ""x"":\n        pandas_array = pandas_array.T\n    expected = xr.DataArray(pandas_array.ewm(**{window_type: window}).mean()).transpose(\n        *da.dims\n    )\n\n    assert_allclose(expected.variable, result.variable)\n\n\ndef test_no_dict():\n    d = DataArray()\n    with pytest.raises(AttributeError):\n        d.__dict__\n\n\ndef test_subclass_slots():\n    """"""Test that DataArray subclasses must explicitly define ``__slots__``.\n\n    .. note::\n       As of 0.13.0, this is actually mitigated into a FutureWarning for any class\n       defined outside of the xarray package.\n    """"""\n    with pytest.raises(AttributeError) as e:\n\n        class MyArray(DataArray):\n            pass\n\n    assert str(e.value) == ""MyArray must explicitly define __slots__""\n\n\ndef test_weakref():\n    """"""Classes with __slots__ are incompatible with the weakref module unless they\n    explicitly state __weakref__ among their slots\n    """"""\n    from weakref import ref\n\n    a = DataArray(1)\n    r = ref(a)\n    assert r() is a\n\n\ndef test_delete_coords():\n    """"""Make sure that deleting a coordinate doesn\'t corrupt the DataArray.\n    See issue #3899.\n\n    Also test that deleting succeeds and produces the expected output.\n    """"""\n    a0 = DataArray(\n        np.array([[1, 2, 3], [4, 5, 6]]),\n        dims=[""y"", ""x""],\n        coords={""x"": [""a"", ""b"", ""c""], ""y"": [-1, 1]},\n    )\n    assert_identical(a0, a0)\n\n    a1 = a0.copy()\n    del a1.coords[""y""]\n\n    # This test will detect certain sorts of corruption in the DataArray\n    assert_identical(a0, a0)\n\n    assert a0.dims == (""y"", ""x"")\n    assert a1.dims == (""y"", ""x"")\n    assert set(a0.coords.keys()) == {""x"", ""y""}\n    assert set(a1.coords.keys()) == {""x""}\n'"
xarray/tests/test_dataset.py,368,"b'import pickle\nimport sys\nimport warnings\nfrom copy import copy, deepcopy\nfrom io import StringIO\nfrom textwrap import dedent\n\nimport numpy as np\nimport pandas as pd\nimport pytest\nfrom pandas.core.indexes.datetimes import DatetimeIndex\n\nimport xarray as xr\nfrom xarray import (\n    DataArray,\n    Dataset,\n    IndexVariable,\n    MergeError,\n    Variable,\n    align,\n    backends,\n    broadcast,\n    open_dataset,\n    set_options,\n)\nfrom xarray.coding.cftimeindex import CFTimeIndex\nfrom xarray.core import dtypes, indexing, utils\nfrom xarray.core.common import duck_array_ops, full_like\nfrom xarray.core.npcompat import IS_NEP18_ACTIVE\nfrom xarray.core.pycompat import integer_types\nfrom xarray.core.utils import is_scalar\n\nfrom . import (\n    InaccessibleArray,\n    UnexpectedDataAccess,\n    assert_allclose,\n    assert_array_equal,\n    assert_equal,\n    assert_identical,\n    has_cftime,\n    has_dask,\n    raises_regex,\n    requires_bottleneck,\n    requires_cftime,\n    requires_dask,\n    requires_numbagg,\n    requires_scipy,\n    requires_sparse,\n    source_ndarray,\n)\n\ntry:\n    import dask.array as da\nexcept ImportError:\n    pass\n\n\ndef create_test_data(seed=None):\n    rs = np.random.RandomState(seed)\n    _vars = {\n        ""var1"": [""dim1"", ""dim2""],\n        ""var2"": [""dim1"", ""dim2""],\n        ""var3"": [""dim3"", ""dim1""],\n    }\n    _dims = {""dim1"": 8, ""dim2"": 9, ""dim3"": 10}\n\n    obj = Dataset()\n    obj[""time""] = (""time"", pd.date_range(""2000-01-01"", periods=20))\n    obj[""dim2""] = (""dim2"", 0.5 * np.arange(_dims[""dim2""]))\n    obj[""dim3""] = (""dim3"", list(""abcdefghij""))\n    for v, dims in sorted(_vars.items()):\n        data = rs.normal(size=tuple(_dims[d] for d in dims))\n        obj[v] = (dims, data, {""foo"": ""variable""})\n    obj.coords[""numbers""] = (\n        ""dim3"",\n        np.array([0, 1, 2, 0, 0, 1, 1, 2, 2, 3], dtype=""int64""),\n    )\n    obj.encoding = {""foo"": ""bar""}\n    assert all(obj.data.flags.writeable for obj in obj.variables.values())\n    return obj\n\n\ndef create_append_test_data(seed=None):\n    rs = np.random.RandomState(seed)\n\n    lat = [2, 1, 0]\n    lon = [0, 1, 2]\n    nt1 = 3\n    nt2 = 2\n    time1 = pd.date_range(""2000-01-01"", periods=nt1)\n    time2 = pd.date_range(""2000-02-01"", periods=nt2)\n    string_var = np.array([""ae"", ""bc"", ""df""], dtype=object)\n    string_var_to_append = np.array([""asdf"", ""asdfg""], dtype=object)\n    unicode_var = [""\xc3\xa1\xc3\xb3"", ""\xc3\xa1\xc3\xb3"", ""\xc3\xa1\xc3\xb3""]\n    datetime_var = np.array(\n        [""2019-01-01"", ""2019-01-02"", ""2019-01-03""], dtype=""datetime64[s]""\n    )\n    datetime_var_to_append = np.array(\n        [""2019-01-04"", ""2019-01-05""], dtype=""datetime64[s]""\n    )\n    bool_var = np.array([True, False, True], dtype=np.bool)\n    bool_var_to_append = np.array([False, True], dtype=np.bool)\n\n    ds = xr.Dataset(\n        data_vars={\n            ""da"": xr.DataArray(\n                rs.rand(3, 3, nt1),\n                coords=[lat, lon, time1],\n                dims=[""lat"", ""lon"", ""time""],\n            ),\n            ""string_var"": xr.DataArray(string_var, coords=[time1], dims=[""time""]),\n            ""unicode_var"": xr.DataArray(\n                unicode_var, coords=[time1], dims=[""time""]\n            ).astype(np.unicode_),\n            ""datetime_var"": xr.DataArray(datetime_var, coords=[time1], dims=[""time""]),\n            ""bool_var"": xr.DataArray(bool_var, coords=[time1], dims=[""time""]),\n        }\n    )\n\n    ds_to_append = xr.Dataset(\n        data_vars={\n            ""da"": xr.DataArray(\n                rs.rand(3, 3, nt2),\n                coords=[lat, lon, time2],\n                dims=[""lat"", ""lon"", ""time""],\n            ),\n            ""string_var"": xr.DataArray(\n                string_var_to_append, coords=[time2], dims=[""time""]\n            ),\n            ""unicode_var"": xr.DataArray(\n                unicode_var[:nt2], coords=[time2], dims=[""time""]\n            ).astype(np.unicode_),\n            ""datetime_var"": xr.DataArray(\n                datetime_var_to_append, coords=[time2], dims=[""time""]\n            ),\n            ""bool_var"": xr.DataArray(bool_var_to_append, coords=[time2], dims=[""time""]),\n        }\n    )\n\n    ds_with_new_var = xr.Dataset(\n        data_vars={\n            ""new_var"": xr.DataArray(\n                rs.rand(3, 3, nt1 + nt2),\n                coords=[lat, lon, time1.append(time2)],\n                dims=[""lat"", ""lon"", ""time""],\n            )\n        }\n    )\n\n    assert all(objp.data.flags.writeable for objp in ds.variables.values())\n    assert all(objp.data.flags.writeable for objp in ds_to_append.variables.values())\n    return ds, ds_to_append, ds_with_new_var\n\n\ndef create_test_multiindex():\n    mindex = pd.MultiIndex.from_product(\n        [[""a"", ""b""], [1, 2]], names=(""level_1"", ""level_2"")\n    )\n    return Dataset({}, {""x"": mindex})\n\n\ndef create_test_stacked_array():\n    x = DataArray(pd.Index(np.r_[:10], name=""x""))\n    y = DataArray(pd.Index(np.r_[:20], name=""y""))\n    a = x * y\n    b = x * y * y\n    return a, b\n\n\nclass InaccessibleVariableDataStore(backends.InMemoryDataStore):\n    def __init__(self):\n        super().__init__()\n        self._indexvars = set()\n\n    def store(self, variables, *args, **kwargs):\n        super().store(variables, *args, **kwargs)\n        for k, v in variables.items():\n            if isinstance(v, IndexVariable):\n                self._indexvars.add(k)\n\n    def get_variables(self):\n        def lazy_inaccessible(k, v):\n            if k in self._indexvars:\n                return v\n            data = indexing.LazilyOuterIndexedArray(InaccessibleArray(v.values))\n            return Variable(v.dims, data, v.attrs)\n\n        return {k: lazy_inaccessible(k, v) for k, v in self._variables.items()}\n\n\nclass TestDataset:\n    def test_repr(self):\n        data = create_test_data(seed=123)\n        data.attrs[""foo""] = ""bar""\n        # need to insert str dtype at runtime to handle different endianness\n        expected = dedent(\n            """"""\\\n            <xarray.Dataset>\n            Dimensions:  (dim1: 8, dim2: 9, dim3: 10, time: 20)\n            Coordinates:\n              * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-20\n              * dim2     (dim2) float64 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0\n              * dim3     (dim3) %s \'a\' \'b\' \'c\' \'d\' \'e\' \'f\' \'g\' \'h\' \'i\' \'j\'\n                numbers  (dim3) int64 0 1 2 0 0 1 1 2 2 3\n            Dimensions without coordinates: dim1\n            Data variables:\n                var1     (dim1, dim2) float64 -1.086 0.9973 0.283 ... 0.1995 0.4684 -0.8312\n                var2     (dim1, dim2) float64 1.162 -1.097 -2.123 ... 0.1302 1.267 0.3328\n                var3     (dim3, dim1) float64 0.5565 -0.2121 0.4563 ... -0.2452 -0.3616\n            Attributes:\n                foo:      bar""""""\n            % data[""dim3""].dtype\n        )\n        actual = ""\\n"".join(x.rstrip() for x in repr(data).split(""\\n""))\n        print(actual)\n        assert expected == actual\n\n        with set_options(display_width=100):\n            max_len = max(map(len, repr(data).split(""\\n"")))\n            assert 90 < max_len < 100\n\n        expected = dedent(\n            """"""\\\n            <xarray.Dataset>\n            Dimensions:  ()\n            Data variables:\n                *empty*""""""\n        )\n        actual = ""\\n"".join(x.rstrip() for x in repr(Dataset()).split(""\\n""))\n        print(actual)\n        assert expected == actual\n\n        # verify that ... doesn\'t appear for scalar coordinates\n        data = Dataset({""foo"": (""x"", np.ones(10))}).mean()\n        expected = dedent(\n            """"""\\\n            <xarray.Dataset>\n            Dimensions:  ()\n            Data variables:\n                foo      float64 1.0""""""\n        )\n        actual = ""\\n"".join(x.rstrip() for x in repr(data).split(""\\n""))\n        print(actual)\n        assert expected == actual\n\n        # verify long attributes are truncated\n        data = Dataset(attrs={""foo"": ""bar"" * 1000})\n        assert len(repr(data)) < 1000\n\n    def test_repr_multiindex(self):\n        data = create_test_multiindex()\n        expected = dedent(\n            """"""\\\n            <xarray.Dataset>\n            Dimensions:  (x: 4)\n            Coordinates:\n              * x        (x) MultiIndex\n              - level_1  (x) object \'a\' \'a\' \'b\' \'b\'\n              - level_2  (x) int64 1 2 1 2\n            Data variables:\n                *empty*""""""\n        )\n        actual = ""\\n"".join(x.rstrip() for x in repr(data).split(""\\n""))\n        print(actual)\n        assert expected == actual\n\n        # verify that long level names are not truncated\n        mindex = pd.MultiIndex.from_product(\n            [[""a"", ""b""], [1, 2]], names=(""a_quite_long_level_name"", ""level_2"")\n        )\n        data = Dataset({}, {""x"": mindex})\n        expected = dedent(\n            """"""\\\n            <xarray.Dataset>\n            Dimensions:                  (x: 4)\n            Coordinates:\n              * x                        (x) MultiIndex\n              - a_quite_long_level_name  (x) object \'a\' \'a\' \'b\' \'b\'\n              - level_2                  (x) int64 1 2 1 2\n            Data variables:\n                *empty*""""""\n        )\n        actual = ""\\n"".join(x.rstrip() for x in repr(data).split(""\\n""))\n        print(actual)\n        assert expected == actual\n\n    def test_repr_period_index(self):\n        data = create_test_data(seed=456)\n        data.coords[""time""] = pd.period_range(""2000-01-01"", periods=20, freq=""B"")\n\n        # check that creating the repr doesn\'t raise an error #GH645\n        repr(data)\n\n    def test_unicode_data(self):\n        # regression test for GH834\n        data = Dataset({""fo\xc3\xb8"": [""ba\xc2\xae""]}, attrs={""\xc3\xa5"": ""\xe2\x88\x91""})\n        repr(data)  # should not raise\n\n        byteorder = ""<"" if sys.byteorder == ""little"" else "">""\n        expected = dedent(\n            """"""\\\n            <xarray.Dataset>\n            Dimensions:  (fo\xc3\xb8: 1)\n            Coordinates:\n              * fo\xc3\xb8      (fo\xc3\xb8) %cU3 %r\n            Data variables:\n                *empty*\n            Attributes:\n                \xc3\xa5:        \xe2\x88\x91""""""\n            % (byteorder, ""ba\xc2\xae"")\n        )\n        actual = str(data)\n        assert expected == actual\n\n    @pytest.mark.skipif(not IS_NEP18_ACTIVE, reason=""requires __array_function__"")\n    def test_repr_nep18(self):\n        class Array:\n            def __init__(self):\n                self.shape = (2,)\n                self.dtype = np.dtype(np.float64)\n\n            def __array_function__(self, *args, **kwargs):\n                pass\n\n            def __repr__(self):\n                return ""Custom\\nArray""\n\n        dataset = Dataset({""foo"": (""x"", Array())})\n        expected = dedent(\n            """"""\\\n            <xarray.Dataset>\n            Dimensions:  (x: 2)\n            Dimensions without coordinates: x\n            Data variables:\n                foo      (x) float64 Custom Array""""""\n        )\n        assert expected == repr(dataset)\n\n    def test_info(self):\n        ds = create_test_data(seed=123)\n        ds = ds.drop_vars(""dim3"")  # string type prints differently in PY2 vs PY3\n        ds.attrs[""unicode_attr""] = ""ba\xc2\xae""\n        ds.attrs[""string_attr""] = ""bar""\n\n        buf = StringIO()\n        ds.info(buf=buf)\n\n        expected = dedent(\n            """"""\\\n        xarray.Dataset {\n        dimensions:\n        \\tdim1 = 8 ;\n        \\tdim2 = 9 ;\n        \\tdim3 = 10 ;\n        \\ttime = 20 ;\n\n        variables:\n        \\tdatetime64[ns] time(time) ;\n        \\tfloat64 dim2(dim2) ;\n        \\tfloat64 var1(dim1, dim2) ;\n        \\t\\tvar1:foo = variable ;\n        \\tfloat64 var2(dim1, dim2) ;\n        \\t\\tvar2:foo = variable ;\n        \\tfloat64 var3(dim3, dim1) ;\n        \\t\\tvar3:foo = variable ;\n        \\tint64 numbers(dim3) ;\n\n        // global attributes:\n        \\t:unicode_attr = ba\xc2\xae ;\n        \\t:string_attr = bar ;\n        }""""""\n        )\n        actual = buf.getvalue()\n        assert expected == actual\n        buf.close()\n\n    def test_constructor(self):\n        x1 = (""x"", 2 * np.arange(100))\n        x2 = (""x"", np.arange(1000))\n        z = ([""x"", ""y""], np.arange(1000).reshape(100, 10))\n\n        with raises_regex(ValueError, ""conflicting sizes""):\n            Dataset({""a"": x1, ""b"": x2})\n        with raises_regex(ValueError, ""disallows such variables""):\n            Dataset({""a"": x1, ""x"": z})\n        with raises_regex(TypeError, ""tuple of form""):\n            Dataset({""x"": (1, 2, 3, 4, 5, 6, 7)})\n        with raises_regex(ValueError, ""already exists as a scalar""):\n            Dataset({""x"": 0, ""y"": (""x"", [1, 2, 3])})\n\n        # verify handling of DataArrays\n        expected = Dataset({""x"": x1, ""z"": z})\n        actual = Dataset({""z"": expected[""z""]})\n        assert_identical(expected, actual)\n\n    def test_constructor_invalid_dims(self):\n        # regression for GH1120\n        with pytest.raises(MergeError):\n            Dataset(\n                data_vars=dict(v=(""y"", [1, 2, 3, 4])),\n                coords=dict(y=DataArray([0.1, 0.2, 0.3, 0.4], dims=""x"")),\n            )\n\n    def test_constructor_1d(self):\n        expected = Dataset({""x"": ([""x""], 5.0 + np.arange(5))})\n        actual = Dataset({""x"": 5.0 + np.arange(5)})\n        assert_identical(expected, actual)\n\n        actual = Dataset({""x"": [5, 6, 7, 8, 9]})\n        assert_identical(expected, actual)\n\n    def test_constructor_0d(self):\n        expected = Dataset({""x"": ([], 1)})\n        for arg in [1, np.array(1), expected[""x""]]:\n            actual = Dataset({""x"": arg})\n            assert_identical(expected, actual)\n\n        class Arbitrary:\n            pass\n\n        d = pd.Timestamp(""2000-01-01T12"")\n        args = [\n            True,\n            None,\n            3.4,\n            np.nan,\n            ""hello"",\n            b""raw"",\n            np.datetime64(""2000-01-01""),\n            d,\n            d.to_pydatetime(),\n            Arbitrary(),\n        ]\n        for arg in args:\n            print(arg)\n            expected = Dataset({""x"": ([], arg)})\n            actual = Dataset({""x"": arg})\n            assert_identical(expected, actual)\n\n    def test_constructor_deprecated(self):\n        with raises_regex(ValueError, ""DataArray dimensions""):\n            DataArray([1, 2, 3], coords={""x"": [0, 1, 2]})\n\n    def test_constructor_auto_align(self):\n        a = DataArray([1, 2], [(""x"", [0, 1])])\n        b = DataArray([3, 4], [(""x"", [1, 2])])\n\n        # verify align uses outer join\n        expected = Dataset(\n            {""a"": (""x"", [1, 2, np.nan]), ""b"": (""x"", [np.nan, 3, 4])}, {""x"": [0, 1, 2]}\n        )\n        actual = Dataset({""a"": a, ""b"": b})\n        assert_identical(expected, actual)\n\n        # regression test for GH346\n        assert isinstance(actual.variables[""x""], IndexVariable)\n\n        # variable with different dimensions\n        c = (""y"", [3, 4])\n        expected2 = expected.merge({""c"": c})\n        actual = Dataset({""a"": a, ""b"": b, ""c"": c})\n        assert_identical(expected2, actual)\n\n        # variable that is only aligned against the aligned variables\n        d = (""x"", [3, 2, 1])\n        expected3 = expected.merge({""d"": d})\n        actual = Dataset({""a"": a, ""b"": b, ""d"": d})\n        assert_identical(expected3, actual)\n\n        e = (""x"", [0, 0])\n        with raises_regex(ValueError, ""conflicting sizes""):\n            Dataset({""a"": a, ""b"": b, ""e"": e})\n\n    def test_constructor_pandas_sequence(self):\n\n        ds = self.make_example_math_dataset()\n        pandas_objs = {\n            var_name: ds[var_name].to_pandas() for var_name in [""foo"", ""bar""]\n        }\n        ds_based_on_pandas = Dataset(pandas_objs, ds.coords, attrs=ds.attrs)\n        del ds_based_on_pandas[""x""]\n        assert_equal(ds, ds_based_on_pandas)\n\n        # reindex pandas obj, check align works\n        rearranged_index = reversed(pandas_objs[""foo""].index)\n        pandas_objs[""foo""] = pandas_objs[""foo""].reindex(rearranged_index)\n        ds_based_on_pandas = Dataset(pandas_objs, ds.coords, attrs=ds.attrs)\n        del ds_based_on_pandas[""x""]\n        assert_equal(ds, ds_based_on_pandas)\n\n    def test_constructor_pandas_single(self):\n\n        das = [\n            DataArray(np.random.rand(4), dims=[""a""]),  # series\n            DataArray(np.random.rand(4, 3), dims=[""a"", ""b""]),  # df\n        ]\n\n        for a in das:\n            pandas_obj = a.to_pandas()\n            ds_based_on_pandas = Dataset(pandas_obj)\n            for dim in ds_based_on_pandas.data_vars:\n                assert_array_equal(ds_based_on_pandas[dim], pandas_obj[dim])\n\n    def test_constructor_compat(self):\n        data = {""x"": DataArray(0, coords={""y"": 1}), ""y"": (""z"", [1, 1, 1])}\n        expected = Dataset({""x"": 0}, {""y"": (""z"", [1, 1, 1])})\n        actual = Dataset(data)\n        assert_identical(expected, actual)\n\n        data = {""y"": (""z"", [1, 1, 1]), ""x"": DataArray(0, coords={""y"": 1})}\n        actual = Dataset(data)\n        assert_identical(expected, actual)\n\n        original = Dataset(\n            {""a"": ((""x"", ""y""), np.ones((2, 3)))},\n            {""c"": ((""x"", ""y""), np.zeros((2, 3))), ""x"": [0, 1]},\n        )\n        expected = Dataset(\n            {""a"": (""x"", np.ones(2)), ""b"": (""y"", np.ones(3))},\n            {""c"": ((""x"", ""y""), np.zeros((2, 3))), ""x"": [0, 1]},\n        )\n\n        actual = Dataset(\n            {""a"": original[""a""][:, 0], ""b"": original[""a""][0].drop_vars(""x"")}\n        )\n        assert_identical(expected, actual)\n\n        data = {""x"": DataArray(0, coords={""y"": 3}), ""y"": (""z"", [1, 1, 1])}\n        with pytest.raises(MergeError):\n            Dataset(data)\n\n        data = {""x"": DataArray(0, coords={""y"": 1}), ""y"": [1, 1]}\n        actual = Dataset(data)\n        expected = Dataset({""x"": 0}, {""y"": [1, 1]})\n        assert_identical(expected, actual)\n\n    def test_constructor_with_coords(self):\n        with raises_regex(ValueError, ""found in both data_vars and""):\n            Dataset({""a"": (""x"", [1])}, {""a"": (""x"", [1])})\n\n        ds = Dataset({}, {""a"": (""x"", [1])})\n        assert not ds.data_vars\n        assert list(ds.coords.keys()) == [""a""]\n\n        mindex = pd.MultiIndex.from_product(\n            [[""a"", ""b""], [1, 2]], names=(""level_1"", ""level_2"")\n        )\n        with raises_regex(ValueError, ""conflicting MultiIndex""):\n            Dataset({}, {""x"": mindex, ""y"": mindex})\n            Dataset({}, {""x"": mindex, ""level_1"": range(4)})\n\n    def test_properties(self):\n        ds = create_test_data()\n        assert ds.dims == {""dim1"": 8, ""dim2"": 9, ""dim3"": 10, ""time"": 20}\n        assert list(ds.dims) == sorted(ds.dims)\n        assert ds.sizes == ds.dims\n\n        # These exact types aren\'t public API, but this makes sure we don\'t\n        # change them inadvertently:\n        assert isinstance(ds.dims, utils.Frozen)\n        assert isinstance(ds.dims.mapping, utils.SortedKeysDict)\n        assert type(ds.dims.mapping.mapping) is dict\n\n        assert list(ds) == list(ds.data_vars)\n        assert list(ds.keys()) == list(ds.data_vars)\n        assert ""aasldfjalskdfj"" not in ds.variables\n        assert ""dim1"" in repr(ds.variables)\n        assert len(ds) == 3\n        assert bool(ds)\n\n        assert list(ds.data_vars) == [""var1"", ""var2"", ""var3""]\n        assert list(ds.data_vars.keys()) == [""var1"", ""var2"", ""var3""]\n        assert ""var1"" in ds.data_vars\n        assert ""dim1"" not in ds.data_vars\n        assert ""numbers"" not in ds.data_vars\n        assert len(ds.data_vars) == 3\n\n        assert set(ds.indexes) == {""dim2"", ""dim3"", ""time""}\n        assert len(ds.indexes) == 3\n        assert ""dim2"" in repr(ds.indexes)\n\n        assert list(ds.coords) == [""time"", ""dim2"", ""dim3"", ""numbers""]\n        assert ""dim2"" in ds.coords\n        assert ""numbers"" in ds.coords\n        assert ""var1"" not in ds.coords\n        assert ""dim1"" not in ds.coords\n        assert len(ds.coords) == 4\n\n        assert Dataset({""x"": np.int64(1), ""y"": np.float32([1, 2])}).nbytes == 16\n\n    def test_asarray(self):\n        ds = Dataset({""x"": 0})\n        with raises_regex(TypeError, ""cannot directly convert""):\n            np.asarray(ds)\n\n    def test_get_index(self):\n        ds = Dataset({""foo"": ((""x"", ""y""), np.zeros((2, 3)))}, coords={""x"": [""a"", ""b""]})\n        assert ds.get_index(""x"").equals(pd.Index([""a"", ""b""]))\n        assert ds.get_index(""y"").equals(pd.Index([0, 1, 2]))\n        with pytest.raises(KeyError):\n            ds.get_index(""z"")\n\n    def test_attr_access(self):\n        ds = Dataset(\n            {""tmin"": (""x"", [42], {""units"": ""Celcius""})}, attrs={""title"": ""My test data""}\n        )\n        assert_identical(ds.tmin, ds[""tmin""])\n        assert_identical(ds.tmin.x, ds.x)\n\n        assert ds.title == ds.attrs[""title""]\n        assert ds.tmin.units == ds[""tmin""].attrs[""units""]\n\n        assert {""tmin"", ""title""} <= set(dir(ds))\n        assert ""units"" in set(dir(ds.tmin))\n\n        # should defer to variable of same name\n        ds.attrs[""tmin""] = -999\n        assert ds.attrs[""tmin""] == -999\n        assert_identical(ds.tmin, ds[""tmin""])\n\n    def test_variable(self):\n        a = Dataset()\n        d = np.random.random((10, 3))\n        a[""foo""] = ((""time"", ""x""), d)\n        assert ""foo"" in a.variables\n        assert ""foo"" in a\n        a[""bar""] = ((""time"", ""x""), d)\n        # order of creation is preserved\n        assert list(a.variables) == [""foo"", ""bar""]\n        assert_array_equal(a[""foo""].values, d)\n        # try to add variable with dim (10,3) with data that\'s (3,10)\n        with pytest.raises(ValueError):\n            a[""qux""] = ((""time"", ""x""), d.T)\n\n    def test_modify_inplace(self):\n        a = Dataset()\n        vec = np.random.random((10,))\n        attributes = {""foo"": ""bar""}\n        a[""x""] = (""x"", vec, attributes)\n        assert ""x"" in a.coords\n        assert isinstance(a.coords[""x""].to_index(), pd.Index)\n        assert_identical(a.coords[""x""].variable, a.variables[""x""])\n        b = Dataset()\n        b[""x""] = (""x"", vec, attributes)\n        assert_identical(a[""x""], b[""x""])\n        assert a.dims == b.dims\n        # this should work\n        a[""x""] = (""x"", vec[:5])\n        a[""z""] = (""x"", np.arange(5))\n        with pytest.raises(ValueError):\n            # now it shouldn\'t, since there is a conflicting length\n            a[""x""] = (""x"", vec[:4])\n        arr = np.random.random((10, 1))\n        scal = np.array(0)\n        with pytest.raises(ValueError):\n            a[""y""] = (""y"", arr)\n        with pytest.raises(ValueError):\n            a[""y""] = (""y"", scal)\n        assert ""y"" not in a.dims\n\n    def test_coords_properties(self):\n        # use int64 for repr consistency on windows\n        data = Dataset(\n            {\n                ""x"": (""x"", np.array([-1, -2], ""int64"")),\n                ""y"": (""y"", np.array([0, 1, 2], ""int64"")),\n                ""foo"": ([""x"", ""y""], np.random.randn(2, 3)),\n            },\n            {""a"": (""x"", np.array([4, 5], ""int64"")), ""b"": np.int64(-10)},\n        )\n\n        assert 4 == len(data.coords)\n\n        assert [""x"", ""y"", ""a"", ""b""] == list(data.coords)\n\n        assert_identical(data.coords[""x""].variable, data[""x""].variable)\n        assert_identical(data.coords[""y""].variable, data[""y""].variable)\n\n        assert ""x"" in data.coords\n        assert ""a"" in data.coords\n        assert 0 not in data.coords\n        assert ""foo"" not in data.coords\n\n        with pytest.raises(KeyError):\n            data.coords[""foo""]\n        with pytest.raises(KeyError):\n            data.coords[0]\n\n        expected = dedent(\n            """"""\\\n        Coordinates:\n          * x        (x) int64 -1 -2\n          * y        (y) int64 0 1 2\n            a        (x) int64 4 5\n            b        int64 -10""""""\n        )\n        actual = repr(data.coords)\n        assert expected == actual\n\n        assert {""x"": 2, ""y"": 3} == data.coords.dims\n\n    def test_coords_modify(self):\n        data = Dataset(\n            {\n                ""x"": (""x"", [-1, -2]),\n                ""y"": (""y"", [0, 1, 2]),\n                ""foo"": ([""x"", ""y""], np.random.randn(2, 3)),\n            },\n            {""a"": (""x"", [4, 5]), ""b"": -10},\n        )\n\n        actual = data.copy(deep=True)\n        actual.coords[""x""] = (""x"", [""a"", ""b""])\n        assert_array_equal(actual[""x""], [""a"", ""b""])\n\n        actual = data.copy(deep=True)\n        actual.coords[""z""] = (""z"", [""a"", ""b""])\n        assert_array_equal(actual[""z""], [""a"", ""b""])\n\n        actual = data.copy(deep=True)\n        with raises_regex(ValueError, ""conflicting sizes""):\n            actual.coords[""x""] = (""x"", [-1])\n        assert_identical(actual, data)  # should not be modified\n\n        actual = data.copy()\n        del actual.coords[""b""]\n        expected = data.reset_coords(""b"", drop=True)\n        assert_identical(expected, actual)\n\n        with pytest.raises(KeyError):\n            del data.coords[""not_found""]\n\n        with pytest.raises(KeyError):\n            del data.coords[""foo""]\n\n        actual = data.copy(deep=True)\n        actual.coords.update({""c"": 11})\n        expected = data.merge({""c"": 11}).set_coords(""c"")\n        assert_identical(expected, actual)\n\n        # regression test for GH3746\n        del actual.coords[""x""]\n        assert ""x"" not in actual.indexes\n\n    def test_update_index(self):\n        actual = Dataset(coords={""x"": [1, 2, 3]})\n        actual[""x""] = [""a"", ""b"", ""c""]\n        assert actual.indexes[""x""].equals(pd.Index([""a"", ""b"", ""c""]))\n\n    def test_coords_setitem_with_new_dimension(self):\n        actual = Dataset()\n        actual.coords[""foo""] = (""x"", [1, 2, 3])\n        expected = Dataset(coords={""foo"": (""x"", [1, 2, 3])})\n        assert_identical(expected, actual)\n\n    def test_coords_setitem_multiindex(self):\n        data = create_test_multiindex()\n        with raises_regex(ValueError, ""conflicting MultiIndex""):\n            data.coords[""level_1""] = range(4)\n\n    def test_coords_set(self):\n        one_coord = Dataset({""x"": (""x"", [0]), ""yy"": (""x"", [1]), ""zzz"": (""x"", [2])})\n        two_coords = Dataset({""zzz"": (""x"", [2])}, {""x"": (""x"", [0]), ""yy"": (""x"", [1])})\n        all_coords = Dataset(\n            coords={""x"": (""x"", [0]), ""yy"": (""x"", [1]), ""zzz"": (""x"", [2])}\n        )\n\n        actual = one_coord.set_coords(""x"")\n        assert_identical(one_coord, actual)\n        actual = one_coord.set_coords([""x""])\n        assert_identical(one_coord, actual)\n\n        actual = one_coord.set_coords(""yy"")\n        assert_identical(two_coords, actual)\n\n        actual = one_coord.set_coords([""yy"", ""zzz""])\n        assert_identical(all_coords, actual)\n\n        actual = one_coord.reset_coords()\n        assert_identical(one_coord, actual)\n        actual = two_coords.reset_coords()\n        assert_identical(one_coord, actual)\n        actual = all_coords.reset_coords()\n        assert_identical(one_coord, actual)\n\n        actual = all_coords.reset_coords([""yy"", ""zzz""])\n        assert_identical(one_coord, actual)\n        actual = all_coords.reset_coords(""zzz"")\n        assert_identical(two_coords, actual)\n\n        with raises_regex(ValueError, ""cannot remove index""):\n            one_coord.reset_coords(""x"")\n\n        actual = all_coords.reset_coords(""zzz"", drop=True)\n        expected = all_coords.drop_vars(""zzz"")\n        assert_identical(expected, actual)\n        expected = two_coords.drop_vars(""zzz"")\n        assert_identical(expected, actual)\n\n    def test_coords_to_dataset(self):\n        orig = Dataset({""foo"": (""y"", [-1, 0, 1])}, {""x"": 10, ""y"": [2, 3, 4]})\n        expected = Dataset(coords={""x"": 10, ""y"": [2, 3, 4]})\n        actual = orig.coords.to_dataset()\n        assert_identical(expected, actual)\n\n    def test_coords_merge(self):\n        orig_coords = Dataset(coords={""a"": (""x"", [1, 2]), ""x"": [0, 1]}).coords\n        other_coords = Dataset(coords={""b"": (""x"", [""a"", ""b""]), ""x"": [0, 1]}).coords\n        expected = Dataset(\n            coords={""a"": (""x"", [1, 2]), ""b"": (""x"", [""a"", ""b""]), ""x"": [0, 1]}\n        )\n        actual = orig_coords.merge(other_coords)\n        assert_identical(expected, actual)\n        actual = other_coords.merge(orig_coords)\n        assert_identical(expected, actual)\n\n        other_coords = Dataset(coords={""x"": (""x"", [""a""])}).coords\n        with pytest.raises(MergeError):\n            orig_coords.merge(other_coords)\n        other_coords = Dataset(coords={""x"": (""x"", [""a"", ""b""])}).coords\n        with pytest.raises(MergeError):\n            orig_coords.merge(other_coords)\n        other_coords = Dataset(coords={""x"": (""x"", [""a"", ""b"", ""c""])}).coords\n        with pytest.raises(MergeError):\n            orig_coords.merge(other_coords)\n\n        other_coords = Dataset(coords={""a"": (""x"", [8, 9])}).coords\n        expected = Dataset(coords={""x"": range(2)})\n        actual = orig_coords.merge(other_coords)\n        assert_identical(expected, actual)\n        actual = other_coords.merge(orig_coords)\n        assert_identical(expected, actual)\n\n        other_coords = Dataset(coords={""x"": np.nan}).coords\n        actual = orig_coords.merge(other_coords)\n        assert_identical(orig_coords.to_dataset(), actual)\n        actual = other_coords.merge(orig_coords)\n        assert_identical(orig_coords.to_dataset(), actual)\n\n    def test_coords_merge_mismatched_shape(self):\n        orig_coords = Dataset(coords={""a"": (""x"", [1, 1])}).coords\n        other_coords = Dataset(coords={""a"": 1}).coords\n        expected = orig_coords.to_dataset()\n        actual = orig_coords.merge(other_coords)\n        assert_identical(expected, actual)\n\n        other_coords = Dataset(coords={""a"": (""y"", [1])}).coords\n        expected = Dataset(coords={""a"": ([""x"", ""y""], [[1], [1]])})\n        actual = orig_coords.merge(other_coords)\n        assert_identical(expected, actual)\n\n        actual = other_coords.merge(orig_coords)\n        assert_identical(expected.transpose(), actual)\n\n        orig_coords = Dataset(coords={""a"": (""x"", [np.nan])}).coords\n        other_coords = Dataset(coords={""a"": np.nan}).coords\n        expected = orig_coords.to_dataset()\n        actual = orig_coords.merge(other_coords)\n        assert_identical(expected, actual)\n\n    def test_data_vars_properties(self):\n        ds = Dataset()\n        ds[""foo""] = ((""x"",), [1.0])\n        ds[""bar""] = 2.0\n\n        assert set(ds.data_vars) == {""foo"", ""bar""}\n        assert ""foo"" in ds.data_vars\n        assert ""x"" not in ds.data_vars\n        assert_identical(ds[""foo""], ds.data_vars[""foo""])\n\n        expected = dedent(\n            """"""\\\n        Data variables:\n            foo      (x) float64 1.0\n            bar      float64 2.0""""""\n        )\n        actual = repr(ds.data_vars)\n        assert expected == actual\n\n    def test_equals_and_identical(self):\n        data = create_test_data(seed=42)\n        assert data.equals(data)\n        assert data.identical(data)\n\n        data2 = create_test_data(seed=42)\n        data2.attrs[""foobar""] = ""baz""\n        assert data.equals(data2)\n        assert not data.identical(data2)\n\n        del data2[""time""]\n        assert not data.equals(data2)\n\n        data = create_test_data(seed=42).rename({""var1"": None})\n        assert data.equals(data)\n        assert data.identical(data)\n\n        data2 = data.reset_coords()\n        assert not data2.equals(data)\n        assert not data2.identical(data)\n\n    def test_equals_failures(self):\n        data = create_test_data()\n        assert not data.equals(""foo"")\n        assert not data.identical(123)\n        assert not data.broadcast_equals({1: 2})\n\n    def test_broadcast_equals(self):\n        data1 = Dataset(coords={""x"": 0})\n        data2 = Dataset(coords={""x"": [0]})\n        assert data1.broadcast_equals(data2)\n        assert not data1.equals(data2)\n        assert not data1.identical(data2)\n\n    def test_attrs(self):\n        data = create_test_data(seed=42)\n        data.attrs = {""foobar"": ""baz""}\n        assert data.attrs[""foobar""], ""baz""\n        assert isinstance(data.attrs, dict)\n\n    @requires_dask\n    def test_chunk(self):\n        data = create_test_data()\n        for v in data.variables.values():\n            assert isinstance(v.data, np.ndarray)\n        assert data.chunks == {}\n\n        reblocked = data.chunk()\n        for k, v in reblocked.variables.items():\n            if k in reblocked.dims:\n                assert isinstance(v.data, np.ndarray)\n            else:\n                assert isinstance(v.data, da.Array)\n\n        expected_chunks = {""dim1"": (8,), ""dim2"": (9,), ""dim3"": (10,)}\n        assert reblocked.chunks == expected_chunks\n\n        def get_dask_names(ds):\n            return {k: v.data.name for k, v in ds.items()}\n\n        orig_dask_names = get_dask_names(reblocked)\n\n        reblocked = data.chunk({""time"": 5, ""dim1"": 5, ""dim2"": 5, ""dim3"": 5})\n        # time is not a dim in any of the data_vars, so it\n        # doesn\'t get chunked\n        expected_chunks = {""dim1"": (5, 3), ""dim2"": (5, 4), ""dim3"": (5, 5)}\n        assert reblocked.chunks == expected_chunks\n\n        # make sure dask names change when rechunking by different amounts\n        # regression test for GH3350\n        new_dask_names = get_dask_names(reblocked)\n        for k, v in new_dask_names.items():\n            assert v != orig_dask_names[k]\n\n        reblocked = data.chunk(expected_chunks)\n        assert reblocked.chunks == expected_chunks\n\n        # reblock on already blocked data\n        orig_dask_names = get_dask_names(reblocked)\n        reblocked = reblocked.chunk(expected_chunks)\n        new_dask_names = get_dask_names(reblocked)\n        assert reblocked.chunks == expected_chunks\n        assert_identical(reblocked, data)\n        # recuhnking with same chunk sizes should not change names\n        for k, v in new_dask_names.items():\n            assert v == orig_dask_names[k]\n\n        with raises_regex(ValueError, ""some chunks""):\n            data.chunk({""foo"": 10})\n\n    @requires_dask\n    def test_dask_is_lazy(self):\n        store = InaccessibleVariableDataStore()\n        create_test_data().dump_to_store(store)\n        ds = open_dataset(store).chunk()\n\n        with pytest.raises(UnexpectedDataAccess):\n            ds.load()\n        with pytest.raises(UnexpectedDataAccess):\n            ds[""var1""].values\n\n        # these should not raise UnexpectedDataAccess:\n        ds.var1.data\n        ds.isel(time=10)\n        ds.isel(time=slice(10), dim1=[0]).isel(dim1=0, dim2=-1)\n        ds.transpose()\n        ds.mean()\n        ds.fillna(0)\n        ds.rename({""dim1"": ""foobar""})\n        ds.set_coords(""var1"")\n        ds.drop_vars(""var1"")\n\n    def test_isel(self):\n        data = create_test_data()\n        slicers = {""dim1"": slice(None, None, 2), ""dim2"": slice(0, 2)}\n        ret = data.isel(**slicers)\n\n        # Verify that only the specified dimension was altered\n        assert list(data.dims) == list(ret.dims)\n        for d in data.dims:\n            if d in slicers:\n                assert ret.dims[d] == np.arange(data.dims[d])[slicers[d]].size\n            else:\n                assert data.dims[d] == ret.dims[d]\n        # Verify that the data is what we expect\n        for v in data.variables:\n            assert data[v].dims == ret[v].dims\n            assert data[v].attrs == ret[v].attrs\n            slice_list = [slice(None)] * data[v].values.ndim\n            for d, s in slicers.items():\n                if d in data[v].dims:\n                    inds = np.nonzero(np.array(data[v].dims) == d)[0]\n                    for ind in inds:\n                        slice_list[ind] = s\n            expected = data[v].values[tuple(slice_list)]\n            actual = ret[v].values\n            np.testing.assert_array_equal(expected, actual)\n\n        with pytest.raises(ValueError):\n            data.isel(not_a_dim=slice(0, 2))\n        with raises_regex(\n            ValueError,\n            r""dimensions {\'not_a_dim\'} do not exist. Expected ""\n            r""one or more of ""\n            r""[\\w\\W]*\'time\'[\\w\\W]*\'dim\\d\'[\\w\\W]*\'dim\\d\'[\\w\\W]*\'dim\\d\'[\\w\\W]*"",\n        ):\n            data.isel(not_a_dim=slice(0, 2))\n        with pytest.warns(\n            UserWarning,\n            match=r""dimensions {\'not_a_dim\'} do not exist. ""\n            r""Expected one or more of ""\n            r""[\\w\\W]*\'time\'[\\w\\W]*\'dim\\d\'[\\w\\W]*\'dim\\d\'[\\w\\W]*\'dim\\d\'[\\w\\W]*"",\n        ):\n            data.isel(not_a_dim=slice(0, 2), missing_dims=""warn"")\n        assert_identical(data, data.isel(not_a_dim=slice(0, 2), missing_dims=""ignore""))\n\n        ret = data.isel(dim1=0)\n        assert {""time"": 20, ""dim2"": 9, ""dim3"": 10} == ret.dims\n        assert set(data.data_vars) == set(ret.data_vars)\n        assert set(data.coords) == set(ret.coords)\n        assert set(data.indexes) == set(ret.indexes)\n\n        ret = data.isel(time=slice(2), dim1=0, dim2=slice(5))\n        assert {""time"": 2, ""dim2"": 5, ""dim3"": 10} == ret.dims\n        assert set(data.data_vars) == set(ret.data_vars)\n        assert set(data.coords) == set(ret.coords)\n        assert set(data.indexes) == set(ret.indexes)\n\n        ret = data.isel(time=0, dim1=0, dim2=slice(5))\n        assert {""dim2"": 5, ""dim3"": 10} == ret.dims\n        assert set(data.data_vars) == set(ret.data_vars)\n        assert set(data.coords) == set(ret.coords)\n        assert set(data.indexes) == set(list(ret.indexes) + [""time""])\n\n    def test_isel_fancy(self):\n        # isel with fancy indexing.\n        data = create_test_data()\n\n        pdim1 = [1, 2, 3]\n        pdim2 = [4, 5, 1]\n        pdim3 = [1, 2, 3]\n        actual = data.isel(\n            dim1=((""test_coord"",), pdim1),\n            dim2=((""test_coord"",), pdim2),\n            dim3=((""test_coord"",), pdim3),\n        )\n        assert ""test_coord"" in actual.dims\n        assert actual.coords[""test_coord""].shape == (len(pdim1),)\n\n        # Should work with DataArray\n        actual = data.isel(\n            dim1=DataArray(pdim1, dims=""test_coord""),\n            dim2=((""test_coord"",), pdim2),\n            dim3=((""test_coord"",), pdim3),\n        )\n        assert ""test_coord"" in actual.dims\n        assert actual.coords[""test_coord""].shape == (len(pdim1),)\n        expected = data.isel(\n            dim1=((""test_coord"",), pdim1),\n            dim2=((""test_coord"",), pdim2),\n            dim3=((""test_coord"",), pdim3),\n        )\n        assert_identical(actual, expected)\n\n        # DataArray with coordinate\n        idx1 = DataArray(pdim1, dims=[""a""], coords={""a"": np.random.randn(3)})\n        idx2 = DataArray(pdim2, dims=[""b""], coords={""b"": np.random.randn(3)})\n        idx3 = DataArray(pdim3, dims=[""c""], coords={""c"": np.random.randn(3)})\n        # Should work with DataArray\n        actual = data.isel(dim1=idx1, dim2=idx2, dim3=idx3)\n        assert ""a"" in actual.dims\n        assert ""b"" in actual.dims\n        assert ""c"" in actual.dims\n        assert ""time"" in actual.coords\n        assert ""dim2"" in actual.coords\n        assert ""dim3"" in actual.coords\n        expected = data.isel(\n            dim1=((""a"",), pdim1), dim2=((""b"",), pdim2), dim3=((""c"",), pdim3)\n        )\n        expected = expected.assign_coords(a=idx1[""a""], b=idx2[""b""], c=idx3[""c""])\n        assert_identical(actual, expected)\n\n        idx1 = DataArray(pdim1, dims=[""a""], coords={""a"": np.random.randn(3)})\n        idx2 = DataArray(pdim2, dims=[""a""])\n        idx3 = DataArray(pdim3, dims=[""a""])\n        # Should work with DataArray\n        actual = data.isel(dim1=idx1, dim2=idx2, dim3=idx3)\n        assert ""a"" in actual.dims\n        assert ""time"" in actual.coords\n        assert ""dim2"" in actual.coords\n        assert ""dim3"" in actual.coords\n        expected = data.isel(\n            dim1=((""a"",), pdim1), dim2=((""a"",), pdim2), dim3=((""a"",), pdim3)\n        )\n        expected = expected.assign_coords(a=idx1[""a""])\n        assert_identical(actual, expected)\n\n        actual = data.isel(dim1=((""points"",), pdim1), dim2=((""points"",), pdim2))\n        assert ""points"" in actual.dims\n        assert ""dim3"" in actual.dims\n        assert ""dim3"" not in actual.data_vars\n        np.testing.assert_array_equal(data[""dim2""][pdim2], actual[""dim2""])\n\n        # test that the order of the indexers doesn\'t matter\n        assert_identical(\n            data.isel(dim1=((""points"",), pdim1), dim2=((""points"",), pdim2)),\n            data.isel(dim2=((""points"",), pdim2), dim1=((""points"",), pdim1)),\n        )\n        # make sure we\'re raising errors in the right places\n        with raises_regex(IndexError, ""Dimensions of indexers mismatch""):\n            data.isel(dim1=((""points"",), [1, 2]), dim2=((""points"",), [1, 2, 3]))\n        with raises_regex(TypeError, ""cannot use a Dataset""):\n            data.isel(dim1=Dataset({""points"": [1, 2]}))\n\n        # test to be sure we keep around variables that were not indexed\n        ds = Dataset({""x"": [1, 2, 3, 4], ""y"": 0})\n        actual = ds.isel(x=((""points"",), [0, 1, 2]))\n        assert_identical(ds[""y""], actual[""y""])\n\n        # tests using index or DataArray as indexers\n        stations = Dataset()\n        stations[""station""] = ((""station"",), [""A"", ""B"", ""C""])\n        stations[""dim1s""] = ((""station"",), [1, 2, 3])\n        stations[""dim2s""] = ((""station"",), [4, 5, 1])\n\n        actual = data.isel(dim1=stations[""dim1s""], dim2=stations[""dim2s""])\n        assert ""station"" in actual.coords\n        assert ""station"" in actual.dims\n        assert_identical(actual[""station""].drop_vars([""dim2""]), stations[""station""])\n\n        with raises_regex(ValueError, ""conflicting values for ""):\n            data.isel(\n                dim1=DataArray(\n                    [0, 1, 2], dims=""station"", coords={""station"": [0, 1, 2]}\n                ),\n                dim2=DataArray(\n                    [0, 1, 2], dims=""station"", coords={""station"": [0, 1, 3]}\n                ),\n            )\n\n        # multi-dimensional selection\n        stations = Dataset()\n        stations[""a""] = ((""a"",), [""A"", ""B"", ""C""])\n        stations[""b""] = ((""b"",), [0, 1])\n        stations[""dim1s""] = ((""a"", ""b""), [[1, 2], [2, 3], [3, 4]])\n        stations[""dim2s""] = ((""a"",), [4, 5, 1])\n        actual = data.isel(dim1=stations[""dim1s""], dim2=stations[""dim2s""])\n        assert ""a"" in actual.coords\n        assert ""a"" in actual.dims\n        assert ""b"" in actual.coords\n        assert ""b"" in actual.dims\n        assert ""dim2"" in actual.coords\n        assert ""a"" in actual[""dim2""].dims\n\n        assert_identical(actual[""a""].drop_vars([""dim2""]), stations[""a""])\n        assert_identical(actual[""b""], stations[""b""])\n        expected_var1 = data[""var1""].variable[\n            stations[""dim1s""].variable, stations[""dim2s""].variable\n        ]\n        expected_var2 = data[""var2""].variable[\n            stations[""dim1s""].variable, stations[""dim2s""].variable\n        ]\n        expected_var3 = data[""var3""].variable[slice(None), stations[""dim1s""].variable]\n        assert_equal(actual[""a""].drop_vars(""dim2""), stations[""a""])\n        assert_array_equal(actual[""var1""], expected_var1)\n        assert_array_equal(actual[""var2""], expected_var2)\n        assert_array_equal(actual[""var3""], expected_var3)\n\n    def test_isel_dataarray(self):\n        """""" Test for indexing by DataArray """"""\n        data = create_test_data()\n        # indexing with DataArray with same-name coordinates.\n        indexing_da = DataArray(\n            np.arange(1, 4), dims=[""dim1""], coords={""dim1"": np.random.randn(3)}\n        )\n        actual = data.isel(dim1=indexing_da)\n        assert_identical(indexing_da[""dim1""], actual[""dim1""])\n        assert_identical(data[""dim2""], actual[""dim2""])\n\n        # Conflict in the dimension coordinate\n        indexing_da = DataArray(\n            np.arange(1, 4), dims=[""dim2""], coords={""dim2"": np.random.randn(3)}\n        )\n        with raises_regex(IndexError, ""dimension coordinate \'dim2\'""):\n            actual = data.isel(dim2=indexing_da)\n        # Also the case for DataArray\n        with raises_regex(IndexError, ""dimension coordinate \'dim2\'""):\n            actual = data[""var2""].isel(dim2=indexing_da)\n        with raises_regex(IndexError, ""dimension coordinate \'dim2\'""):\n            data[""dim2""].isel(dim2=indexing_da)\n\n        # same name coordinate which does not conflict\n        indexing_da = DataArray(\n            np.arange(1, 4), dims=[""dim2""], coords={""dim2"": data[""dim2""].values[1:4]}\n        )\n        actual = data.isel(dim2=indexing_da)\n        assert_identical(actual[""dim2""], indexing_da[""dim2""])\n\n        # Silently drop conflicted (non-dimensional) coordinate of indexer\n        indexing_da = DataArray(\n            np.arange(1, 4),\n            dims=[""dim2""],\n            coords={\n                ""dim2"": data[""dim2""].values[1:4],\n                ""numbers"": (""dim2"", np.arange(2, 5)),\n            },\n        )\n        actual = data.isel(dim2=indexing_da)\n        assert_identical(actual[""numbers""], data[""numbers""])\n\n        # boolean data array with coordinate with the same name\n        indexing_da = DataArray(\n            np.arange(1, 10), dims=[""dim2""], coords={""dim2"": data[""dim2""].values}\n        )\n        indexing_da = indexing_da < 3\n        actual = data.isel(dim2=indexing_da)\n        assert_identical(actual[""dim2""], data[""dim2""][:2])\n\n        # boolean data array with non-dimensioncoordinate\n        indexing_da = DataArray(\n            np.arange(1, 10),\n            dims=[""dim2""],\n            coords={\n                ""dim2"": data[""dim2""].values,\n                ""non_dim"": ((""dim2"",), np.random.randn(9)),\n                ""non_dim2"": 0,\n            },\n        )\n        indexing_da = indexing_da < 3\n        actual = data.isel(dim2=indexing_da)\n        assert_identical(\n            actual[""dim2""].drop_vars(""non_dim"").drop_vars(""non_dim2""), data[""dim2""][:2]\n        )\n        assert_identical(actual[""non_dim""], indexing_da[""non_dim""][:2])\n        assert_identical(actual[""non_dim2""], indexing_da[""non_dim2""])\n\n        # non-dimension coordinate will be also attached\n        indexing_da = DataArray(\n            np.arange(1, 4),\n            dims=[""dim2""],\n            coords={""non_dim"": ((""dim2"",), np.random.randn(3))},\n        )\n        actual = data.isel(dim2=indexing_da)\n        assert ""non_dim"" in actual\n        assert ""non_dim"" in actual.coords\n\n        # Index by a scalar DataArray\n        indexing_da = DataArray(3, dims=[], coords={""station"": 2})\n        actual = data.isel(dim2=indexing_da)\n        assert ""station"" in actual\n        actual = data.isel(dim2=indexing_da[""station""])\n        assert ""station"" in actual\n\n        # indexer generated from coordinates\n        indexing_ds = Dataset({}, coords={""dim2"": [0, 1, 2]})\n        with raises_regex(IndexError, ""dimension coordinate \'dim2\'""):\n            actual = data.isel(dim2=indexing_ds[""dim2""])\n\n    def test_sel(self):\n        data = create_test_data()\n        int_slicers = {""dim1"": slice(None, None, 2), ""dim2"": slice(2), ""dim3"": slice(3)}\n        loc_slicers = {\n            ""dim1"": slice(None, None, 2),\n            ""dim2"": slice(0, 0.5),\n            ""dim3"": slice(""a"", ""c""),\n        }\n        assert_equal(data.isel(**int_slicers), data.sel(**loc_slicers))\n        data[""time""] = (""time"", pd.date_range(""2000-01-01"", periods=20))\n        assert_equal(data.isel(time=0), data.sel(time=""2000-01-01""))\n        assert_equal(\n            data.isel(time=slice(10)), data.sel(time=slice(""2000-01-01"", ""2000-01-10""))\n        )\n        assert_equal(data, data.sel(time=slice(""1999"", ""2005"")))\n        times = pd.date_range(""2000-01-01"", periods=3)\n        assert_equal(data.isel(time=slice(3)), data.sel(time=times))\n        assert_equal(\n            data.isel(time=slice(3)), data.sel(time=(data[""time.dayofyear""] <= 3))\n        )\n\n        td = pd.to_timedelta(np.arange(3), unit=""days"")\n        data = Dataset({""x"": (""td"", np.arange(3)), ""td"": td})\n        assert_equal(data, data.sel(td=td))\n        assert_equal(data, data.sel(td=slice(""3 days"")))\n        assert_equal(data.isel(td=0), data.sel(td=pd.Timedelta(""0 days"")))\n        assert_equal(data.isel(td=0), data.sel(td=pd.Timedelta(""0h"")))\n        assert_equal(data.isel(td=slice(1, 3)), data.sel(td=slice(""1 days"", ""2 days"")))\n\n    def test_sel_dataarray(self):\n        data = create_test_data()\n\n        ind = DataArray([0.0, 0.5, 1.0], dims=[""dim2""])\n        actual = data.sel(dim2=ind)\n        assert_equal(actual, data.isel(dim2=[0, 1, 2]))\n\n        # with different dimension\n        ind = DataArray([0.0, 0.5, 1.0], dims=[""new_dim""])\n        actual = data.sel(dim2=ind)\n        expected = data.isel(dim2=Variable(""new_dim"", [0, 1, 2]))\n        assert ""new_dim"" in actual.dims\n        assert_equal(actual, expected)\n\n        # Multi-dimensional\n        ind = DataArray([[0.0], [0.5], [1.0]], dims=[""new_dim"", ""new_dim2""])\n        actual = data.sel(dim2=ind)\n        expected = data.isel(dim2=Variable((""new_dim"", ""new_dim2""), [[0], [1], [2]]))\n        assert ""new_dim"" in actual.dims\n        assert ""new_dim2"" in actual.dims\n        assert_equal(actual, expected)\n\n        # with coordinate\n        ind = DataArray(\n            [0.0, 0.5, 1.0], dims=[""new_dim""], coords={""new_dim"": [""a"", ""b"", ""c""]}\n        )\n        actual = data.sel(dim2=ind)\n        expected = data.isel(dim2=[0, 1, 2]).rename({""dim2"": ""new_dim""})\n        assert ""new_dim"" in actual.dims\n        assert ""new_dim"" in actual.coords\n        assert_equal(\n            actual.drop_vars(""new_dim"").drop_vars(""dim2""), expected.drop_vars(""new_dim"")\n        )\n        assert_equal(actual[""new_dim""].drop_vars(""dim2""), ind[""new_dim""])\n\n        # with conflicted coordinate (silently ignored)\n        ind = DataArray(\n            [0.0, 0.5, 1.0], dims=[""dim2""], coords={""dim2"": [""a"", ""b"", ""c""]}\n        )\n        actual = data.sel(dim2=ind)\n        expected = data.isel(dim2=[0, 1, 2])\n        assert_equal(actual, expected)\n\n        # with conflicted coordinate (silently ignored)\n        ind = DataArray(\n            [0.0, 0.5, 1.0],\n            dims=[""new_dim""],\n            coords={""new_dim"": [""a"", ""b"", ""c""], ""dim2"": 3},\n        )\n        actual = data.sel(dim2=ind)\n        assert_equal(\n            actual[""new_dim""].drop_vars(""dim2""), ind[""new_dim""].drop_vars(""dim2"")\n        )\n        expected = data.isel(dim2=[0, 1, 2])\n        expected[""dim2""] = ((""new_dim""), expected[""dim2""].values)\n        assert_equal(actual[""dim2""].drop_vars(""new_dim""), expected[""dim2""])\n        assert actual[""var1""].dims == (""dim1"", ""new_dim"")\n\n        # with non-dimensional coordinate\n        ind = DataArray(\n            [0.0, 0.5, 1.0],\n            dims=[""dim2""],\n            coords={\n                ""dim2"": [""a"", ""b"", ""c""],\n                ""numbers"": (""dim2"", [0, 1, 2]),\n                ""new_dim"": (""dim2"", [1.1, 1.2, 1.3]),\n            },\n        )\n        actual = data.sel(dim2=ind)\n        expected = data.isel(dim2=[0, 1, 2])\n        assert_equal(actual.drop_vars(""new_dim""), expected)\n        assert np.allclose(actual[""new_dim""].values, ind[""new_dim""].values)\n\n    def test_sel_dataarray_mindex(self):\n        midx = pd.MultiIndex.from_product([list(""abc""), [0, 1]], names=(""one"", ""two""))\n        mds = xr.Dataset(\n            {""var"": ((""x"", ""y""), np.random.rand(6, 3))},\n            coords={""x"": midx, ""y"": range(3)},\n        )\n\n        actual_isel = mds.isel(x=xr.DataArray(np.arange(3), dims=""x""))\n        actual_sel = mds.sel(x=DataArray(mds.indexes[""x""][:3], dims=""x""))\n        assert actual_isel[""x""].dims == (""x"",)\n        assert actual_sel[""x""].dims == (""x"",)\n        assert_identical(actual_isel, actual_sel)\n\n        actual_isel = mds.isel(x=xr.DataArray(np.arange(3), dims=""z""))\n        actual_sel = mds.sel(x=Variable(""z"", mds.indexes[""x""][:3]))\n        assert actual_isel[""x""].dims == (""z"",)\n        assert actual_sel[""x""].dims == (""z"",)\n        assert_identical(actual_isel, actual_sel)\n\n        # with coordinate\n        actual_isel = mds.isel(\n            x=xr.DataArray(np.arange(3), dims=""z"", coords={""z"": [0, 1, 2]})\n        )\n        actual_sel = mds.sel(\n            x=xr.DataArray(mds.indexes[""x""][:3], dims=""z"", coords={""z"": [0, 1, 2]})\n        )\n        assert actual_isel[""x""].dims == (""z"",)\n        assert actual_sel[""x""].dims == (""z"",)\n        assert_identical(actual_isel, actual_sel)\n\n        # Vectorized indexing with level-variables raises an error\n        with raises_regex(ValueError, ""Vectorized selection is ""):\n            mds.sel(one=[""a"", ""b""])\n\n        with raises_regex(\n            ValueError,\n            ""Vectorized selection is "" ""not available along MultiIndex variable:"" "" x"",\n        ):\n            mds.sel(\n                x=xr.DataArray(\n                    [np.array(midx[:2]), np.array(midx[-2:])], dims=[""a"", ""b""]\n                )\n            )\n\n    def test_sel_categorical(self):\n        ind = pd.Series([""foo"", ""bar""], dtype=""category"")\n        df = pd.DataFrame({""ind"": ind, ""values"": [1, 2]})\n        ds = df.set_index(""ind"").to_xarray()\n        actual = ds.sel(ind=""bar"")\n        expected = ds.isel(ind=1)\n        assert_identical(expected, actual)\n\n    def test_sel_categorical_error(self):\n        ind = pd.Series([""foo"", ""bar""], dtype=""category"")\n        df = pd.DataFrame({""ind"": ind, ""values"": [1, 2]})\n        ds = df.set_index(""ind"").to_xarray()\n        with pytest.raises(ValueError):\n            ds.sel(ind=""bar"", method=""nearest"")\n        with pytest.raises(ValueError):\n            ds.sel(ind=""bar"", tolerance=""nearest"")\n\n    def test_categorical_index(self):\n        cat = pd.CategoricalIndex(\n            [""foo"", ""bar"", ""foo""],\n            categories=[""foo"", ""bar"", ""baz"", ""qux"", ""quux"", ""corge""],\n        )\n        ds = xr.Dataset(\n            {""var"": (""cat"", np.arange(3))},\n            coords={""cat"": (""cat"", cat), ""c"": (""cat"", [0, 1, 1])},\n        )\n        # test slice\n        actual = ds.sel(cat=""foo"")\n        expected = ds.isel(cat=[0, 2])\n        assert_identical(expected, actual)\n        # make sure the conversion to the array works\n        actual = ds.sel(cat=""foo"")[""cat""].values\n        assert (actual == np.array([""foo"", ""foo""])).all()\n\n        ds = ds.set_index(index=[""cat"", ""c""])\n        actual = ds.unstack(""index"")\n        assert actual[""var""].shape == (2, 2)\n\n    def test_categorical_reindex(self):\n        cat = pd.CategoricalIndex(\n            [""foo"", ""bar"", ""baz""],\n            categories=[""foo"", ""bar"", ""baz"", ""qux"", ""quux"", ""corge""],\n        )\n        ds = xr.Dataset(\n            {""var"": (""cat"", np.arange(3))},\n            coords={""cat"": (""cat"", cat), ""c"": (""cat"", [0, 1, 2])},\n        )\n        actual = ds.reindex(cat=[""foo""])[""cat""].values\n        assert (actual == np.array([""foo""])).all()\n\n    def test_categorical_multiindex(self):\n        i1 = pd.Series([0, 0])\n        cat = pd.CategoricalDtype(categories=[""foo"", ""baz"", ""bar""])\n        i2 = pd.Series([""baz"", ""bar""], dtype=cat)\n\n        df = pd.DataFrame({""i1"": i1, ""i2"": i2, ""values"": [1, 2]}).set_index(\n            [""i1"", ""i2""]\n        )\n        actual = df.to_xarray()\n        assert actual[""values""].shape == (1, 2)\n\n    def test_sel_drop(self):\n        data = Dataset({""foo"": (""x"", [1, 2, 3])}, {""x"": [0, 1, 2]})\n        expected = Dataset({""foo"": 1})\n        selected = data.sel(x=0, drop=True)\n        assert_identical(expected, selected)\n\n        expected = Dataset({""foo"": 1}, {""x"": 0})\n        selected = data.sel(x=0, drop=False)\n        assert_identical(expected, selected)\n\n        data = Dataset({""foo"": (""x"", [1, 2, 3])})\n        expected = Dataset({""foo"": 1})\n        selected = data.sel(x=0, drop=True)\n        assert_identical(expected, selected)\n\n    def test_isel_drop(self):\n        data = Dataset({""foo"": (""x"", [1, 2, 3])}, {""x"": [0, 1, 2]})\n        expected = Dataset({""foo"": 1})\n        selected = data.isel(x=0, drop=True)\n        assert_identical(expected, selected)\n\n        expected = Dataset({""foo"": 1}, {""x"": 0})\n        selected = data.isel(x=0, drop=False)\n        assert_identical(expected, selected)\n\n    def test_head(self):\n        data = create_test_data()\n\n        expected = data.isel(time=slice(5), dim2=slice(6))\n        actual = data.head(time=5, dim2=6)\n        assert_equal(expected, actual)\n\n        expected = data.isel(time=slice(0))\n        actual = data.head(time=0)\n        assert_equal(expected, actual)\n\n        expected = data.isel({dim: slice(6) for dim in data.dims})\n        actual = data.head(6)\n        assert_equal(expected, actual)\n\n        expected = data.isel({dim: slice(5) for dim in data.dims})\n        actual = data.head()\n        assert_equal(expected, actual)\n\n        with raises_regex(TypeError, ""either dict-like or a single int""):\n            data.head([3])\n        with raises_regex(TypeError, ""expected integer type""):\n            data.head(dim2=3.1)\n        with raises_regex(ValueError, ""expected positive int""):\n            data.head(time=-3)\n\n    def test_tail(self):\n        data = create_test_data()\n\n        expected = data.isel(time=slice(-5, None), dim2=slice(-6, None))\n        actual = data.tail(time=5, dim2=6)\n        assert_equal(expected, actual)\n\n        expected = data.isel(dim1=slice(0))\n        actual = data.tail(dim1=0)\n        assert_equal(expected, actual)\n\n        expected = data.isel({dim: slice(-6, None) for dim in data.dims})\n        actual = data.tail(6)\n        assert_equal(expected, actual)\n\n        expected = data.isel({dim: slice(-5, None) for dim in data.dims})\n        actual = data.tail()\n        assert_equal(expected, actual)\n\n        with raises_regex(TypeError, ""either dict-like or a single int""):\n            data.tail([3])\n        with raises_regex(TypeError, ""expected integer type""):\n            data.tail(dim2=3.1)\n        with raises_regex(ValueError, ""expected positive int""):\n            data.tail(time=-3)\n\n    def test_thin(self):\n        data = create_test_data()\n\n        expected = data.isel(time=slice(None, None, 5), dim2=slice(None, None, 6))\n        actual = data.thin(time=5, dim2=6)\n        assert_equal(expected, actual)\n\n        expected = data.isel({dim: slice(None, None, 6) for dim in data.dims})\n        actual = data.thin(6)\n        assert_equal(expected, actual)\n\n        with raises_regex(TypeError, ""either dict-like or a single int""):\n            data.thin([3])\n        with raises_regex(TypeError, ""expected integer type""):\n            data.thin(dim2=3.1)\n        with raises_regex(ValueError, ""cannot be zero""):\n            data.thin(time=0)\n        with raises_regex(ValueError, ""expected positive int""):\n            data.thin(time=-3)\n\n    @pytest.mark.filterwarnings(""ignore::DeprecationWarning"")\n    def test_sel_fancy(self):\n        data = create_test_data()\n\n        # add in a range() index\n        data[""dim1""] = data.dim1\n\n        pdim1 = [1, 2, 3]\n        pdim2 = [4, 5, 1]\n        pdim3 = [1, 2, 3]\n        expected = data.isel(\n            dim1=Variable((""test_coord"",), pdim1),\n            dim2=Variable((""test_coord"",), pdim2),\n            dim3=Variable((""test_coord""), pdim3),\n        )\n        actual = data.sel(\n            dim1=Variable((""test_coord"",), data.dim1[pdim1]),\n            dim2=Variable((""test_coord"",), data.dim2[pdim2]),\n            dim3=Variable((""test_coord"",), data.dim3[pdim3]),\n        )\n        assert_identical(expected, actual)\n\n        # DataArray Indexer\n        idx_t = DataArray(\n            data[""time""][[3, 2, 1]].values, dims=[""a""], coords={""a"": [""a"", ""b"", ""c""]}\n        )\n        idx_2 = DataArray(\n            data[""dim2""][[3, 2, 1]].values, dims=[""a""], coords={""a"": [""a"", ""b"", ""c""]}\n        )\n        idx_3 = DataArray(\n            data[""dim3""][[3, 2, 1]].values, dims=[""a""], coords={""a"": [""a"", ""b"", ""c""]}\n        )\n        actual = data.sel(time=idx_t, dim2=idx_2, dim3=idx_3)\n        expected = data.isel(\n            time=Variable((""a"",), [3, 2, 1]),\n            dim2=Variable((""a"",), [3, 2, 1]),\n            dim3=Variable((""a"",), [3, 2, 1]),\n        )\n        expected = expected.assign_coords(a=idx_t[""a""])\n        assert_identical(expected, actual)\n\n        idx_t = DataArray(\n            data[""time""][[3, 2, 1]].values, dims=[""a""], coords={""a"": [""a"", ""b"", ""c""]}\n        )\n        idx_2 = DataArray(\n            data[""dim2""][[2, 1, 3]].values, dims=[""b""], coords={""b"": [0, 1, 2]}\n        )\n        idx_3 = DataArray(\n            data[""dim3""][[1, 2, 1]].values, dims=[""c""], coords={""c"": [0.0, 1.1, 2.2]}\n        )\n        actual = data.sel(time=idx_t, dim2=idx_2, dim3=idx_3)\n        expected = data.isel(\n            time=Variable((""a"",), [3, 2, 1]),\n            dim2=Variable((""b"",), [2, 1, 3]),\n            dim3=Variable((""c"",), [1, 2, 1]),\n        )\n        expected = expected.assign_coords(a=idx_t[""a""], b=idx_2[""b""], c=idx_3[""c""])\n        assert_identical(expected, actual)\n\n        # test from sel_points\n        data = Dataset({""foo"": ((""x"", ""y""), np.arange(9).reshape(3, 3))})\n        data.coords.update({""x"": [0, 1, 2], ""y"": [0, 1, 2]})\n\n        expected = Dataset(\n            {""foo"": (""points"", [0, 4, 8])},\n            coords={\n                ""x"": Variable((""points"",), [0, 1, 2]),\n                ""y"": Variable((""points"",), [0, 1, 2]),\n            },\n        )\n        actual = data.sel(\n            x=Variable((""points"",), [0, 1, 2]), y=Variable((""points"",), [0, 1, 2])\n        )\n        assert_identical(expected, actual)\n\n        expected.coords.update({""x"": (""points"", [0, 1, 2]), ""y"": (""points"", [0, 1, 2])})\n        actual = data.sel(\n            x=Variable((""points"",), [0.1, 1.1, 2.5]),\n            y=Variable((""points"",), [0, 1.2, 2.0]),\n            method=""pad"",\n        )\n        assert_identical(expected, actual)\n\n        idx_x = DataArray([0, 1, 2], dims=[""a""], coords={""a"": [""a"", ""b"", ""c""]})\n        idx_y = DataArray([0, 2, 1], dims=[""b""], coords={""b"": [0, 3, 6]})\n        expected_ary = data[""foo""][[0, 1, 2], [0, 2, 1]]\n        actual = data.sel(x=idx_x, y=idx_y)\n        assert_array_equal(expected_ary, actual[""foo""])\n        assert_identical(actual[""a""].drop_vars(""x""), idx_x[""a""])\n        assert_identical(actual[""b""].drop_vars(""y""), idx_y[""b""])\n\n        with pytest.raises(KeyError):\n            data.sel(x=[2.5], y=[2.0], method=""pad"", tolerance=1e-3)\n\n    def test_sel_method(self):\n        data = create_test_data()\n\n        expected = data.sel(dim2=1)\n        actual = data.sel(dim2=0.95, method=""nearest"")\n        assert_identical(expected, actual)\n\n        actual = data.sel(dim2=0.95, method=""nearest"", tolerance=1)\n        assert_identical(expected, actual)\n\n        with pytest.raises(KeyError):\n            actual = data.sel(dim2=np.pi, method=""nearest"", tolerance=0)\n\n        expected = data.sel(dim2=[1.5])\n        actual = data.sel(dim2=[1.45], method=""backfill"")\n        assert_identical(expected, actual)\n\n        with raises_regex(NotImplementedError, ""slice objects""):\n            data.sel(dim2=slice(1, 3), method=""ffill"")\n\n        with raises_regex(TypeError, ""``method``""):\n            # this should not pass silently\n            data.sel(method=data)\n\n        # cannot pass method if there is no associated coordinate\n        with raises_regex(ValueError, ""cannot supply""):\n            data.sel(dim1=0, method=""nearest"")\n\n    def test_loc(self):\n        data = create_test_data()\n        expected = data.sel(dim3=""a"")\n        actual = data.loc[dict(dim3=""a"")]\n        assert_identical(expected, actual)\n        with raises_regex(TypeError, ""can only lookup dict""):\n            data.loc[""a""]\n        with pytest.raises(TypeError):\n            data.loc[dict(dim3=""a"")] = 0\n\n    def test_selection_multiindex(self):\n        mindex = pd.MultiIndex.from_product(\n            [[""a"", ""b""], [1, 2], [-1, -2]], names=(""one"", ""two"", ""three"")\n        )\n        mdata = Dataset(data_vars={""var"": (""x"", range(8))}, coords={""x"": mindex})\n\n        def test_sel(lab_indexer, pos_indexer, replaced_idx=False, renamed_dim=None):\n            ds = mdata.sel(x=lab_indexer)\n            expected_ds = mdata.isel(x=pos_indexer)\n            if not replaced_idx:\n                assert_identical(ds, expected_ds)\n            else:\n                if renamed_dim:\n                    assert ds[""var""].dims[0] == renamed_dim\n                    ds = ds.rename({renamed_dim: ""x""})\n                assert_identical(ds[""var""].variable, expected_ds[""var""].variable)\n                assert not ds[""x""].equals(expected_ds[""x""])\n\n        test_sel((""a"", 1, -1), 0)\n        test_sel((""b"", 2, -2), -1)\n        test_sel((""a"", 1), [0, 1], replaced_idx=True, renamed_dim=""three"")\n        test_sel((""a"",), range(4), replaced_idx=True)\n        test_sel(""a"", range(4), replaced_idx=True)\n        test_sel([(""a"", 1, -1), (""b"", 2, -2)], [0, 7])\n        test_sel(slice(""a"", ""b""), range(8))\n        test_sel(slice((""a"", 1), (""b"", 1)), range(6))\n        test_sel({""one"": ""a"", ""two"": 1, ""three"": -1}, 0)\n        test_sel({""one"": ""a"", ""two"": 1}, [0, 1], replaced_idx=True, renamed_dim=""three"")\n        test_sel({""one"": ""a""}, range(4), replaced_idx=True)\n\n        assert_identical(mdata.loc[{""x"": {""one"": ""a""}}], mdata.sel(x={""one"": ""a""}))\n        assert_identical(mdata.loc[{""x"": ""a""}], mdata.sel(x=""a""))\n        assert_identical(mdata.loc[{""x"": (""a"", 1)}], mdata.sel(x=(""a"", 1)))\n        assert_identical(mdata.loc[{""x"": (""a"", 1, -1)}], mdata.sel(x=(""a"", 1, -1)))\n\n        assert_identical(mdata.sel(x={""one"": ""a"", ""two"": 1}), mdata.sel(one=""a"", two=1))\n\n    def test_broadcast_like(self):\n        original1 = DataArray(\n            np.random.randn(5), [(""x"", range(5))], name=""a""\n        ).to_dataset()\n\n        original2 = DataArray(np.random.randn(6), [(""y"", range(6))], name=""b"")\n\n        expected1, expected2 = broadcast(original1, original2)\n\n        assert_identical(\n            original1.broadcast_like(original2), expected1.transpose(""y"", ""x"")\n        )\n\n        assert_identical(original2.broadcast_like(original1), expected2)\n\n    def test_reindex_like(self):\n        data = create_test_data()\n        data[""letters""] = (""dim3"", 10 * [""a""])\n\n        expected = data.isel(dim1=slice(10), time=slice(13))\n        actual = data.reindex_like(expected)\n        assert_identical(actual, expected)\n\n        expected = data.copy(deep=True)\n        expected[""dim3""] = (""dim3"", list(""cdefghijkl""))\n        expected[""var3""][:-2] = expected[""var3""][2:].values\n        expected[""var3""][-2:] = np.nan\n        expected[""letters""] = expected[""letters""].astype(object)\n        expected[""letters""][-2:] = np.nan\n        expected[""numbers""] = expected[""numbers""].astype(float)\n        expected[""numbers""][:-2] = expected[""numbers""][2:].values\n        expected[""numbers""][-2:] = np.nan\n        actual = data.reindex_like(expected)\n        assert_identical(actual, expected)\n\n    def test_reindex(self):\n        data = create_test_data()\n        assert_identical(data, data.reindex())\n\n        expected = data.assign_coords(dim1=data[""dim1""])\n        actual = data.reindex(dim1=data[""dim1""])\n        assert_identical(actual, expected)\n\n        actual = data.reindex(dim1=data[""dim1""].values)\n        assert_identical(actual, expected)\n\n        actual = data.reindex(dim1=data[""dim1""].to_index())\n        assert_identical(actual, expected)\n\n        with raises_regex(ValueError, ""cannot reindex or align along dimension""):\n            data.reindex(dim1=data[""dim1""][:5])\n\n        expected = data.isel(dim2=slice(5))\n        actual = data.reindex(dim2=data[""dim2""][:5])\n        assert_identical(actual, expected)\n\n        # test dict-like argument\n        actual = data.reindex({""dim2"": data[""dim2""]})\n        expected = data\n        assert_identical(actual, expected)\n        with raises_regex(ValueError, ""cannot specify both""):\n            data.reindex({""x"": 0}, x=0)\n        with raises_regex(ValueError, ""dictionary""):\n            data.reindex(""foo"")\n\n        # invalid dimension\n        with raises_regex(ValueError, ""invalid reindex dim""):\n            data.reindex(invalid=0)\n\n        # out of order\n        expected = data.sel(dim2=data[""dim2""][:5:-1])\n        actual = data.reindex(dim2=data[""dim2""][:5:-1])\n        assert_identical(actual, expected)\n\n        # regression test for #279\n        expected = Dataset({""x"": (""time"", np.random.randn(5))}, {""time"": range(5)})\n        time2 = DataArray(np.arange(5), dims=""time2"")\n        with pytest.raises(ValueError):\n            actual = expected.reindex(time=time2)\n\n        # another regression test\n        ds = Dataset(\n            {""foo"": ([""x"", ""y""], np.zeros((3, 4)))}, {""x"": range(3), ""y"": range(4)}\n        )\n        expected = Dataset(\n            {""foo"": ([""x"", ""y""], np.zeros((3, 2)))}, {""x"": [0, 1, 3], ""y"": [0, 1]}\n        )\n        expected[""foo""][-1] = np.nan\n        actual = ds.reindex(x=[0, 1, 3], y=[0, 1])\n        assert_identical(expected, actual)\n\n    def test_reindex_warning(self):\n        data = create_test_data()\n\n        with pytest.raises(ValueError):\n            # DataArray with different dimension raises Future warning\n            ind = xr.DataArray([0.0, 1.0], dims=[""new_dim""], name=""ind"")\n            data.reindex(dim2=ind)\n\n        # Should not warn\n        ind = xr.DataArray([0.0, 1.0], dims=[""dim2""], name=""ind"")\n        with pytest.warns(None) as ws:\n            data.reindex(dim2=ind)\n            assert len(ws) == 0\n\n    def test_reindex_variables_copied(self):\n        data = create_test_data()\n        reindexed_data = data.reindex(copy=False)\n        for k in data.variables:\n            assert reindexed_data.variables[k] is not data.variables[k]\n\n    def test_reindex_method(self):\n        ds = Dataset({""x"": (""y"", [10, 20]), ""y"": [0, 1]})\n        y = [-0.5, 0.5, 1.5]\n        actual = ds.reindex(y=y, method=""backfill"")\n        expected = Dataset({""x"": (""y"", [10, 20, np.nan]), ""y"": y})\n        assert_identical(expected, actual)\n\n        actual = ds.reindex(y=y, method=""backfill"", tolerance=0.1)\n        expected = Dataset({""x"": (""y"", 3 * [np.nan]), ""y"": y})\n        assert_identical(expected, actual)\n\n        actual = ds.reindex(y=y, method=""pad"")\n        expected = Dataset({""x"": (""y"", [np.nan, 10, 20]), ""y"": y})\n        assert_identical(expected, actual)\n\n        alt = Dataset({""y"": y})\n        actual = ds.reindex_like(alt, method=""pad"")\n        assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(""fill_value"", [dtypes.NA, 2, 2.0])\n    def test_reindex_fill_value(self, fill_value):\n        ds = Dataset({""x"": (""y"", [10, 20]), ""y"": [0, 1]})\n        y = [0, 1, 2]\n        actual = ds.reindex(y=y, fill_value=fill_value)\n        if fill_value == dtypes.NA:\n            # if we supply the default, we expect the missing value for a\n            # float array\n            fill_value = np.nan\n        expected = Dataset({""x"": (""y"", [10, 20, fill_value]), ""y"": y})\n        assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(""fill_value"", [dtypes.NA, 2, 2.0])\n    def test_reindex_like_fill_value(self, fill_value):\n        ds = Dataset({""x"": (""y"", [10, 20]), ""y"": [0, 1]})\n        y = [0, 1, 2]\n        alt = Dataset({""y"": y})\n        actual = ds.reindex_like(alt, fill_value=fill_value)\n        if fill_value == dtypes.NA:\n            # if we supply the default, we expect the missing value for a\n            # float array\n            fill_value = np.nan\n        expected = Dataset({""x"": (""y"", [10, 20, fill_value]), ""y"": y})\n        assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(""fill_value"", [dtypes.NA, 2, 2.0])\n    def test_align_fill_value(self, fill_value):\n        x = Dataset({""foo"": DataArray([1, 2], dims=[""x""], coords={""x"": [1, 2]})})\n        y = Dataset({""bar"": DataArray([1, 2], dims=[""x""], coords={""x"": [1, 3]})})\n        x2, y2 = align(x, y, join=""outer"", fill_value=fill_value)\n        if fill_value == dtypes.NA:\n            # if we supply the default, we expect the missing value for a\n            # float array\n            fill_value = np.nan\n\n        expected_x2 = Dataset(\n            {""foo"": DataArray([1, 2, fill_value], dims=[""x""], coords={""x"": [1, 2, 3]})}\n        )\n        expected_y2 = Dataset(\n            {""bar"": DataArray([1, fill_value, 2], dims=[""x""], coords={""x"": [1, 2, 3]})}\n        )\n        assert_identical(expected_x2, x2)\n        assert_identical(expected_y2, y2)\n\n    def test_align(self):\n        left = create_test_data()\n        right = left.copy(deep=True)\n        right[""dim3""] = (""dim3"", list(""cdefghijkl""))\n        right[""var3""][:-2] = right[""var3""][2:].values\n        right[""var3""][-2:] = np.random.randn(*right[""var3""][-2:].shape)\n        right[""numbers""][:-2] = right[""numbers""][2:].values\n        right[""numbers""][-2:] = -10\n\n        intersection = list(""cdefghij"")\n        union = list(""abcdefghijkl"")\n\n        left2, right2 = align(left, right, join=""inner"")\n        assert_array_equal(left2[""dim3""], intersection)\n        assert_identical(left2, right2)\n\n        left2, right2 = align(left, right, join=""outer"")\n\n        assert_array_equal(left2[""dim3""], union)\n        assert_equal(left2[""dim3""].variable, right2[""dim3""].variable)\n\n        assert_identical(left2.sel(dim3=intersection), right2.sel(dim3=intersection))\n        assert np.isnan(left2[""var3""][-2:]).all()\n        assert np.isnan(right2[""var3""][:2]).all()\n\n        left2, right2 = align(left, right, join=""left"")\n        assert_equal(left2[""dim3""].variable, right2[""dim3""].variable)\n        assert_equal(left2[""dim3""].variable, left[""dim3""].variable)\n\n        assert_identical(left2.sel(dim3=intersection), right2.sel(dim3=intersection))\n        assert np.isnan(right2[""var3""][:2]).all()\n\n        left2, right2 = align(left, right, join=""right"")\n        assert_equal(left2[""dim3""].variable, right2[""dim3""].variable)\n        assert_equal(left2[""dim3""].variable, right[""dim3""].variable)\n\n        assert_identical(left2.sel(dim3=intersection), right2.sel(dim3=intersection))\n\n        assert np.isnan(left2[""var3""][-2:]).all()\n\n        with raises_regex(ValueError, ""invalid value for join""):\n            align(left, right, join=""foobar"")\n        with pytest.raises(TypeError):\n            align(left, right, foo=""bar"")\n\n    def test_align_exact(self):\n        left = xr.Dataset(coords={""x"": [0, 1]})\n        right = xr.Dataset(coords={""x"": [1, 2]})\n\n        left1, left2 = xr.align(left, left, join=""exact"")\n        assert_identical(left1, left)\n        assert_identical(left2, left)\n\n        with raises_regex(ValueError, ""indexes .* not equal""):\n            xr.align(left, right, join=""exact"")\n\n    def test_align_override(self):\n        left = xr.Dataset(coords={""x"": [0, 1, 2]})\n        right = xr.Dataset(coords={""x"": [0.1, 1.1, 2.1], ""y"": [1, 2, 3]})\n        expected_right = xr.Dataset(coords={""x"": [0, 1, 2], ""y"": [1, 2, 3]})\n\n        new_left, new_right = xr.align(left, right, join=""override"")\n        assert_identical(left, new_left)\n        assert_identical(new_right, expected_right)\n\n        new_left, new_right = xr.align(left, right, exclude=""x"", join=""override"")\n        assert_identical(left, new_left)\n        assert_identical(right, new_right)\n\n        new_left, new_right = xr.align(\n            left.isel(x=0, drop=True), right, exclude=""x"", join=""override""\n        )\n        assert_identical(left.isel(x=0, drop=True), new_left)\n        assert_identical(right, new_right)\n\n        with raises_regex(ValueError, ""Indexes along dimension \'x\' don\'t have""):\n            xr.align(left.isel(x=0).expand_dims(""x""), right, join=""override"")\n\n    def test_align_exclude(self):\n        x = Dataset(\n            {\n                ""foo"": DataArray(\n                    [[1, 2], [3, 4]], dims=[""x"", ""y""], coords={""x"": [1, 2], ""y"": [3, 4]}\n                )\n            }\n        )\n        y = Dataset(\n            {\n                ""bar"": DataArray(\n                    [[1, 2], [3, 4]], dims=[""x"", ""y""], coords={""x"": [1, 3], ""y"": [5, 6]}\n                )\n            }\n        )\n        x2, y2 = align(x, y, exclude=[""y""], join=""outer"")\n\n        expected_x2 = Dataset(\n            {\n                ""foo"": DataArray(\n                    [[1, 2], [3, 4], [np.nan, np.nan]],\n                    dims=[""x"", ""y""],\n                    coords={""x"": [1, 2, 3], ""y"": [3, 4]},\n                )\n            }\n        )\n        expected_y2 = Dataset(\n            {\n                ""bar"": DataArray(\n                    [[1, 2], [np.nan, np.nan], [3, 4]],\n                    dims=[""x"", ""y""],\n                    coords={""x"": [1, 2, 3], ""y"": [5, 6]},\n                )\n            }\n        )\n        assert_identical(expected_x2, x2)\n        assert_identical(expected_y2, y2)\n\n    def test_align_nocopy(self):\n        x = Dataset({""foo"": DataArray([1, 2, 3], coords=[(""x"", [1, 2, 3])])})\n        y = Dataset({""foo"": DataArray([1, 2], coords=[(""x"", [1, 2])])})\n        expected_x2 = x\n        expected_y2 = Dataset(\n            {""foo"": DataArray([1, 2, np.nan], coords=[(""x"", [1, 2, 3])])}\n        )\n\n        x2, y2 = align(x, y, copy=False, join=""outer"")\n        assert_identical(expected_x2, x2)\n        assert_identical(expected_y2, y2)\n        assert source_ndarray(x[""foo""].data) is source_ndarray(x2[""foo""].data)\n\n        x2, y2 = align(x, y, copy=True, join=""outer"")\n        assert source_ndarray(x[""foo""].data) is not source_ndarray(x2[""foo""].data)\n        assert_identical(expected_x2, x2)\n        assert_identical(expected_y2, y2)\n\n    def test_align_indexes(self):\n        x = Dataset({""foo"": DataArray([1, 2, 3], dims=""x"", coords=[(""x"", [1, 2, 3])])})\n        (x2,) = align(x, indexes={""x"": [2, 3, 1]})\n        expected_x2 = Dataset(\n            {""foo"": DataArray([2, 3, 1], dims=""x"", coords={""x"": [2, 3, 1]})}\n        )\n\n        assert_identical(expected_x2, x2)\n\n    def test_align_non_unique(self):\n        x = Dataset({""foo"": (""x"", [3, 4, 5]), ""x"": [0, 0, 1]})\n        x1, x2 = align(x, x)\n        assert x1.identical(x) and x2.identical(x)\n\n        y = Dataset({""bar"": (""x"", [6, 7]), ""x"": [0, 1]})\n        with raises_regex(ValueError, ""cannot reindex or align""):\n            align(x, y)\n\n    def test_broadcast(self):\n        ds = Dataset(\n            {""foo"": 0, ""bar"": (""x"", [1]), ""baz"": (""y"", [2, 3])}, {""c"": (""x"", [4])}\n        )\n        expected = Dataset(\n            {\n                ""foo"": ((""x"", ""y""), [[0, 0]]),\n                ""bar"": ((""x"", ""y""), [[1, 1]]),\n                ""baz"": ((""x"", ""y""), [[2, 3]]),\n            },\n            {""c"": (""x"", [4])},\n        )\n        (actual,) = broadcast(ds)\n        assert_identical(expected, actual)\n\n        ds_x = Dataset({""foo"": (""x"", [1])})\n        ds_y = Dataset({""bar"": (""y"", [2, 3])})\n        expected_x = Dataset({""foo"": ((""x"", ""y""), [[1, 1]])})\n        expected_y = Dataset({""bar"": ((""x"", ""y""), [[2, 3]])})\n        actual_x, actual_y = broadcast(ds_x, ds_y)\n        assert_identical(expected_x, actual_x)\n        assert_identical(expected_y, actual_y)\n\n        array_y = ds_y[""bar""]\n        expected_y = expected_y[""bar""]\n        actual_x, actual_y = broadcast(ds_x, array_y)\n        assert_identical(expected_x, actual_x)\n        assert_identical(expected_y, actual_y)\n\n    def test_broadcast_nocopy(self):\n        # Test that data is not copied if not needed\n        x = Dataset({""foo"": ((""x"", ""y""), [[1, 1]])})\n        y = Dataset({""bar"": (""y"", [2, 3])})\n\n        (actual_x,) = broadcast(x)\n        assert_identical(x, actual_x)\n        assert source_ndarray(actual_x[""foo""].data) is source_ndarray(x[""foo""].data)\n\n        actual_x, actual_y = broadcast(x, y)\n        assert_identical(x, actual_x)\n        assert source_ndarray(actual_x[""foo""].data) is source_ndarray(x[""foo""].data)\n\n    def test_broadcast_exclude(self):\n        x = Dataset(\n            {\n                ""foo"": DataArray(\n                    [[1, 2], [3, 4]], dims=[""x"", ""y""], coords={""x"": [1, 2], ""y"": [3, 4]}\n                ),\n                ""bar"": DataArray(5),\n            }\n        )\n        y = Dataset(\n            {\n                ""foo"": DataArray(\n                    [[1, 2]], dims=[""z"", ""y""], coords={""z"": [1], ""y"": [5, 6]}\n                )\n            }\n        )\n        x2, y2 = broadcast(x, y, exclude=[""y""])\n\n        expected_x2 = Dataset(\n            {\n                ""foo"": DataArray(\n                    [[[1, 2]], [[3, 4]]],\n                    dims=[""x"", ""z"", ""y""],\n                    coords={""z"": [1], ""x"": [1, 2], ""y"": [3, 4]},\n                ),\n                ""bar"": DataArray(\n                    [[5], [5]], dims=[""x"", ""z""], coords={""x"": [1, 2], ""z"": [1]}\n                ),\n            }\n        )\n        expected_y2 = Dataset(\n            {\n                ""foo"": DataArray(\n                    [[[1, 2]], [[1, 2]]],\n                    dims=[""x"", ""z"", ""y""],\n                    coords={""z"": [1], ""x"": [1, 2], ""y"": [5, 6]},\n                )\n            }\n        )\n        assert_identical(expected_x2, x2)\n        assert_identical(expected_y2, y2)\n\n    def test_broadcast_misaligned(self):\n        x = Dataset({""foo"": DataArray([1, 2, 3], coords=[(""x"", [-1, -2, -3])])})\n        y = Dataset(\n            {\n                ""bar"": DataArray(\n                    [[1, 2], [3, 4]],\n                    dims=[""y"", ""x""],\n                    coords={""y"": [1, 2], ""x"": [10, -3]},\n                )\n            }\n        )\n        x2, y2 = broadcast(x, y)\n        expected_x2 = Dataset(\n            {\n                ""foo"": DataArray(\n                    [[3, 3], [2, 2], [1, 1], [np.nan, np.nan]],\n                    dims=[""x"", ""y""],\n                    coords={""y"": [1, 2], ""x"": [-3, -2, -1, 10]},\n                )\n            }\n        )\n        expected_y2 = Dataset(\n            {\n                ""bar"": DataArray(\n                    [[2, 4], [np.nan, np.nan], [np.nan, np.nan], [1, 3]],\n                    dims=[""x"", ""y""],\n                    coords={""y"": [1, 2], ""x"": [-3, -2, -1, 10]},\n                )\n            }\n        )\n        assert_identical(expected_x2, x2)\n        assert_identical(expected_y2, y2)\n\n    def test_variable_indexing(self):\n        data = create_test_data()\n        v = data[""var1""]\n        d1 = data[""dim1""]\n        d2 = data[""dim2""]\n        assert_equal(v, v[d1.values])\n        assert_equal(v, v[d1])\n        assert_equal(v[:3], v[d1 < 3])\n        assert_equal(v[:, 3:], v[:, d2 >= 1.5])\n        assert_equal(v[:3, 3:], v[d1 < 3, d2 >= 1.5])\n        assert_equal(v[:3, :2], v[range(3), range(2)])\n        assert_equal(v[:3, :2], v.loc[d1[:3], d2[:2]])\n\n    def test_drop_variables(self):\n        data = create_test_data()\n\n        assert_identical(data, data.drop_vars([]))\n\n        expected = Dataset({k: data[k] for k in data.variables if k != ""time""})\n        actual = data.drop_vars(""time"")\n        assert_identical(expected, actual)\n        actual = data.drop_vars([""time""])\n        assert_identical(expected, actual)\n\n        with raises_regex(ValueError, ""cannot be found""):\n            data.drop_vars(""not_found_here"")\n\n        actual = data.drop_vars(""not_found_here"", errors=""ignore"")\n        assert_identical(data, actual)\n\n        actual = data.drop_vars([""not_found_here""], errors=""ignore"")\n        assert_identical(data, actual)\n\n        actual = data.drop_vars([""time"", ""not_found_here""], errors=""ignore"")\n        assert_identical(expected, actual)\n\n        # deprecated approach with `drop` works (straight copy paste from above)\n\n        with pytest.warns(PendingDeprecationWarning):\n            actual = data.drop(""not_found_here"", errors=""ignore"")\n        assert_identical(data, actual)\n\n        with pytest.warns(PendingDeprecationWarning):\n            actual = data.drop([""not_found_here""], errors=""ignore"")\n        assert_identical(data, actual)\n\n        with pytest.warns(PendingDeprecationWarning):\n            actual = data.drop([""time"", ""not_found_here""], errors=""ignore"")\n        assert_identical(expected, actual)\n\n        with pytest.warns(PendingDeprecationWarning):\n            actual = data.drop({""time"", ""not_found_here""}, errors=""ignore"")\n        assert_identical(expected, actual)\n\n    def test_drop_index_labels(self):\n        data = Dataset({""A"": ([""x"", ""y""], np.random.randn(2, 3)), ""x"": [""a"", ""b""]})\n\n        with pytest.warns(DeprecationWarning):\n            actual = data.drop([""a""], dim=""x"")\n        expected = data.isel(x=[1])\n        assert_identical(expected, actual)\n\n        with pytest.warns(DeprecationWarning):\n            actual = data.drop([""a"", ""b""], dim=""x"")\n        expected = data.isel(x=slice(0, 0))\n        assert_identical(expected, actual)\n\n        with pytest.raises(KeyError):\n            # not contained in axis\n            with pytest.warns(DeprecationWarning):\n                data.drop([""c""], dim=""x"")\n\n        with pytest.warns(DeprecationWarning):\n            actual = data.drop([""c""], dim=""x"", errors=""ignore"")\n        assert_identical(data, actual)\n\n        with pytest.raises(ValueError):\n            with pytest.warns(DeprecationWarning):\n                data.drop([""c""], dim=""x"", errors=""wrong_value"")\n\n        with pytest.warns(DeprecationWarning):\n            actual = data.drop([""a"", ""b"", ""c""], ""x"", errors=""ignore"")\n        expected = data.isel(x=slice(0, 0))\n        assert_identical(expected, actual)\n\n        # DataArrays as labels are a nasty corner case as they are not\n        # Iterable[Hashable] - DataArray.__iter__ yields scalar DataArrays.\n        actual = data.drop_sel(x=DataArray([""a"", ""b"", ""c""]), errors=""ignore"")\n        expected = data.isel(x=slice(0, 0))\n        assert_identical(expected, actual)\n        with pytest.warns(DeprecationWarning):\n            data.drop(DataArray([""a"", ""b"", ""c""]), dim=""x"", errors=""ignore"")\n        assert_identical(expected, actual)\n\n        with raises_regex(ValueError, ""does not have coordinate labels""):\n            data.drop_sel(y=1)\n\n    def test_drop_labels_by_keyword(self):\n        data = Dataset(\n            {""A"": ([""x"", ""y""], np.random.randn(2, 6)), ""x"": [""a"", ""b""], ""y"": range(6)}\n        )\n        # Basic functionality.\n        assert len(data.coords[""x""]) == 2\n\n        with pytest.warns(DeprecationWarning):\n            ds1 = data.drop([""a""], dim=""x"")\n        ds2 = data.drop_sel(x=""a"")\n        ds3 = data.drop_sel(x=[""a""])\n        ds4 = data.drop_sel(x=[""a"", ""b""])\n        ds5 = data.drop_sel(x=[""a"", ""b""], y=range(0, 6, 2))\n\n        arr = DataArray(range(3), dims=[""c""])\n        with pytest.warns(FutureWarning):\n            data.drop(arr.coords)\n        with pytest.warns(FutureWarning):\n            data.drop(arr.indexes)\n\n        assert_array_equal(ds1.coords[""x""], [""b""])\n        assert_array_equal(ds2.coords[""x""], [""b""])\n        assert_array_equal(ds3.coords[""x""], [""b""])\n        assert ds4.coords[""x""].size == 0\n        assert ds5.coords[""x""].size == 0\n        assert_array_equal(ds5.coords[""y""], [1, 3, 5])\n\n        # Error handling if user tries both approaches.\n        with pytest.raises(ValueError):\n            data.drop(labels=[""a""], x=""a"")\n        with pytest.raises(ValueError):\n            data.drop(labels=[""a""], dim=""x"", x=""a"")\n        warnings.filterwarnings(""ignore"", r""\\W*drop"")\n        with pytest.raises(ValueError):\n            data.drop(dim=""x"", x=""a"")\n\n    def test_drop_dims(self):\n        data = xr.Dataset(\n            {\n                ""A"": ([""x"", ""y""], np.random.randn(2, 3)),\n                ""B"": (""x"", np.random.randn(2)),\n                ""x"": [""a"", ""b""],\n                ""z"": np.pi,\n            }\n        )\n\n        actual = data.drop_dims(""x"")\n        expected = data.drop_vars([""A"", ""B"", ""x""])\n        assert_identical(expected, actual)\n\n        actual = data.drop_dims(""y"")\n        expected = data.drop_vars(""A"")\n        assert_identical(expected, actual)\n\n        actual = data.drop_dims([""x"", ""y""])\n        expected = data.drop_vars([""A"", ""B"", ""x""])\n        assert_identical(expected, actual)\n\n        with pytest.raises((ValueError, KeyError)):\n            data.drop_dims(""z"")  # not a dimension\n\n        with pytest.raises((ValueError, KeyError)):\n            data.drop_dims(None)\n\n        actual = data.drop_dims(""z"", errors=""ignore"")\n        assert_identical(data, actual)\n\n        actual = data.drop_dims(None, errors=""ignore"")\n        assert_identical(data, actual)\n\n        with pytest.raises(ValueError):\n            actual = data.drop_dims(""z"", errors=""wrong_value"")\n\n        actual = data.drop_dims([""x"", ""y"", ""z""], errors=""ignore"")\n        expected = data.drop_vars([""A"", ""B"", ""x""])\n        assert_identical(expected, actual)\n\n    def test_copy(self):\n        data = create_test_data()\n        data.attrs[""Test""] = [1, 2, 3]\n\n        for copied in [data.copy(deep=False), copy(data)]:\n            assert_identical(data, copied)\n            assert data.encoding == copied.encoding\n            # Note: IndexVariable objects with string dtype are always\n            # copied because of xarray.core.util.safe_cast_to_index.\n            # Limiting the test to data variables.\n            for k in data.data_vars:\n                v0 = data.variables[k]\n                v1 = copied.variables[k]\n                assert source_ndarray(v0.data) is source_ndarray(v1.data)\n            copied[""foo""] = (""z"", np.arange(5))\n            assert ""foo"" not in data\n\n            copied.attrs[""foo""] = ""bar""\n            assert ""foo"" not in data.attrs\n            assert data.attrs[""Test""] is copied.attrs[""Test""]\n\n        for copied in [data.copy(deep=True), deepcopy(data)]:\n            assert_identical(data, copied)\n            for k, v0 in data.variables.items():\n                v1 = copied.variables[k]\n                assert v0 is not v1\n\n            assert data.attrs[""Test""] is not copied.attrs[""Test""]\n\n    def test_copy_with_data(self):\n        orig = create_test_data()\n        new_data = {k: np.random.randn(*v.shape) for k, v in orig.data_vars.items()}\n        actual = orig.copy(data=new_data)\n\n        expected = orig.copy()\n        for k, v in new_data.items():\n            expected[k].data = v\n        assert_identical(expected, actual)\n\n    @pytest.mark.xfail(raises=AssertionError)\n    @pytest.mark.parametrize(\n        ""deep, expected_orig"",\n        [\n            [\n                True,\n                xr.DataArray(\n                    xr.IndexVariable(""a"", np.array([1, 2])),\n                    coords={""a"": [1, 2]},\n                    dims=[""a""],\n                ),\n            ],\n            [\n                False,\n                xr.DataArray(\n                    xr.IndexVariable(""a"", np.array([999, 2])),\n                    coords={""a"": [999, 2]},\n                    dims=[""a""],\n                ),\n            ],\n        ],\n    )\n    def test_copy_coords(self, deep, expected_orig):\n        """"""The test fails for the shallow copy, and apparently only on Windows\n        for some reason. In windows coords seem to be immutable unless it\'s one\n        dataset deep copied from another.""""""\n        ds = xr.DataArray(\n            np.ones([2, 2, 2]),\n            coords={""a"": [1, 2], ""b"": [""x"", ""y""], ""c"": [0, 1]},\n            dims=[""a"", ""b"", ""c""],\n            name=""value"",\n        ).to_dataset()\n        ds_cp = ds.copy(deep=deep)\n        ds_cp.coords[""a""].data[0] = 999\n\n        expected_cp = xr.DataArray(\n            xr.IndexVariable(""a"", np.array([999, 2])),\n            coords={""a"": [999, 2]},\n            dims=[""a""],\n        )\n        assert_identical(ds_cp.coords[""a""], expected_cp)\n\n        assert_identical(ds.coords[""a""], expected_orig)\n\n    def test_copy_with_data_errors(self):\n        orig = create_test_data()\n        new_var1 = np.arange(orig[""var1""].size).reshape(orig[""var1""].shape)\n        with raises_regex(ValueError, ""Data must be dict-like""):\n            orig.copy(data=new_var1)\n        with raises_regex(ValueError, ""only contain variables in original""):\n            orig.copy(data={""not_in_original"": new_var1})\n        with raises_regex(ValueError, ""contain all variables in original""):\n            orig.copy(data={""var1"": new_var1})\n\n    def test_rename(self):\n        data = create_test_data()\n        newnames = {""var1"": ""renamed_var1"", ""dim2"": ""renamed_dim2""}\n        renamed = data.rename(newnames)\n\n        variables = dict(data.variables)\n        for k, v in newnames.items():\n            variables[v] = variables.pop(k)\n\n        for k, v in variables.items():\n            dims = list(v.dims)\n            for name, newname in newnames.items():\n                if name in dims:\n                    dims[dims.index(name)] = newname\n\n            assert_equal(\n                Variable(dims, v.values, v.attrs),\n                renamed[k].variable.to_base_variable(),\n            )\n            assert v.encoding == renamed[k].encoding\n            assert type(v) is type(renamed.variables[k])  # noqa: E721\n\n        assert ""var1"" not in renamed\n        assert ""dim2"" not in renamed\n\n        with raises_regex(ValueError, ""cannot rename \'not_a_var\'""):\n            data.rename({""not_a_var"": ""nada""})\n\n        with raises_regex(ValueError, ""\'var1\' conflicts""):\n            data.rename({""var2"": ""var1""})\n\n        # verify that we can rename a variable without accessing the data\n        var1 = data[""var1""]\n        data[""var1""] = (var1.dims, InaccessibleArray(var1.values))\n        renamed = data.rename(newnames)\n        with pytest.raises(UnexpectedDataAccess):\n            renamed[""renamed_var1""].values\n\n        renamed_kwargs = data.rename(**newnames)\n        assert_identical(renamed, renamed_kwargs)\n\n    def test_rename_old_name(self):\n        # regtest for GH1477\n        data = create_test_data()\n\n        with raises_regex(ValueError, ""\'samecol\' conflicts""):\n            data.rename({""var1"": ""samecol"", ""var2"": ""samecol""})\n\n        # This shouldn\'t cause any problems.\n        data.rename({""var1"": ""var2"", ""var2"": ""var1""})\n\n    def test_rename_same_name(self):\n        data = create_test_data()\n        newnames = {""var1"": ""var1"", ""dim2"": ""dim2""}\n        renamed = data.rename(newnames)\n        assert_identical(renamed, data)\n\n    def test_rename_inplace(self):\n        times = pd.date_range(""2000-01-01"", periods=3)\n        data = Dataset({""z"": (""x"", [2, 3, 4]), ""t"": (""t"", times)})\n        with pytest.raises(TypeError):\n            data.rename({""x"": ""y""}, inplace=True)\n\n    def test_rename_dims(self):\n        original = Dataset({""x"": (""x"", [0, 1, 2]), ""y"": (""x"", [10, 11, 12]), ""z"": 42})\n        expected = Dataset(\n            {""x"": (""x_new"", [0, 1, 2]), ""y"": (""x_new"", [10, 11, 12]), ""z"": 42}\n        )\n        expected = expected.set_coords(""x"")\n        dims_dict = {""x"": ""x_new""}\n        actual = original.rename_dims(dims_dict)\n        assert_identical(expected, actual)\n        actual_2 = original.rename_dims(**dims_dict)\n        assert_identical(expected, actual_2)\n\n        # Test to raise ValueError\n        dims_dict_bad = {""x_bad"": ""x_new""}\n        with pytest.raises(ValueError):\n            original.rename_dims(dims_dict_bad)\n\n        with pytest.raises(ValueError):\n            original.rename_dims({""x"": ""z""})\n\n    def test_rename_vars(self):\n        original = Dataset({""x"": (""x"", [0, 1, 2]), ""y"": (""x"", [10, 11, 12]), ""z"": 42})\n        expected = Dataset(\n            {""x_new"": (""x"", [0, 1, 2]), ""y"": (""x"", [10, 11, 12]), ""z"": 42}\n        )\n        expected = expected.set_coords(""x_new"")\n        name_dict = {""x"": ""x_new""}\n        actual = original.rename_vars(name_dict)\n        assert_identical(expected, actual)\n        actual_2 = original.rename_vars(**name_dict)\n        assert_identical(expected, actual_2)\n\n        # Test to raise ValueError\n        names_dict_bad = {""x_bad"": ""x_new""}\n        with pytest.raises(ValueError):\n            original.rename_vars(names_dict_bad)\n\n    def test_rename_multiindex(self):\n        mindex = pd.MultiIndex.from_tuples(\n            [([1, 2]), ([3, 4])], names=[""level0"", ""level1""]\n        )\n        data = Dataset({}, {""x"": mindex})\n        with raises_regex(ValueError, ""conflicting MultiIndex""):\n            data.rename({""x"": ""level0""})\n\n    @requires_cftime\n    def test_rename_does_not_change_CFTimeIndex_type(self):\n        # make sure CFTimeIndex is not converted to DatetimeIndex #3522\n\n        time = xr.cftime_range(start=""2000"", periods=6, freq=""2MS"", calendar=""noleap"")\n        orig = Dataset(coords={""time"": time})\n\n        renamed = orig.rename(time=""time_new"")\n        assert ""time_new"" in renamed.indexes\n        assert isinstance(renamed.indexes[""time_new""], CFTimeIndex)\n        assert renamed.indexes[""time_new""].name == ""time_new""\n\n        # check original has not changed\n        assert ""time"" in orig.indexes\n        assert isinstance(orig.indexes[""time""], CFTimeIndex)\n        assert orig.indexes[""time""].name == ""time""\n\n        # note: rename_dims(time=""time_new"") drops ""ds.indexes""\n        renamed = orig.rename_dims()\n        assert isinstance(renamed.indexes[""time""], CFTimeIndex)\n\n        renamed = orig.rename_vars()\n        assert isinstance(renamed.indexes[""time""], CFTimeIndex)\n\n    def test_rename_does_not_change_DatetimeIndex_type(self):\n        # make sure DatetimeIndex is conderved on rename\n\n        time = pd.date_range(start=""2000"", periods=6, freq=""2MS"")\n        orig = Dataset(coords={""time"": time})\n\n        renamed = orig.rename(time=""time_new"")\n        assert ""time_new"" in renamed.indexes\n        assert isinstance(renamed.indexes[""time_new""], DatetimeIndex)\n        assert renamed.indexes[""time_new""].name == ""time_new""\n\n        # check original has not changed\n        assert ""time"" in orig.indexes\n        assert isinstance(orig.indexes[""time""], DatetimeIndex)\n        assert orig.indexes[""time""].name == ""time""\n\n        # note: rename_dims(time=""time_new"") drops ""ds.indexes""\n        renamed = orig.rename_dims()\n        assert isinstance(renamed.indexes[""time""], DatetimeIndex)\n\n        renamed = orig.rename_vars()\n        assert isinstance(renamed.indexes[""time""], DatetimeIndex)\n\n    def test_swap_dims(self):\n        original = Dataset({""x"": [1, 2, 3], ""y"": (""x"", list(""abc"")), ""z"": 42})\n        expected = Dataset({""z"": 42}, {""x"": (""y"", [1, 2, 3]), ""y"": list(""abc"")})\n        actual = original.swap_dims({""x"": ""y""})\n        assert_identical(expected, actual)\n        assert isinstance(actual.variables[""y""], IndexVariable)\n        assert isinstance(actual.variables[""x""], Variable)\n        pd.testing.assert_index_equal(actual.indexes[""y""], expected.indexes[""y""])\n\n        roundtripped = actual.swap_dims({""y"": ""x""})\n        assert_identical(original.set_coords(""y""), roundtripped)\n\n        with raises_regex(ValueError, ""cannot swap""):\n            original.swap_dims({""y"": ""x""})\n        with raises_regex(ValueError, ""replacement dimension""):\n            original.swap_dims({""x"": ""z""})\n\n        expected = Dataset(\n            {""y"": (""u"", list(""abc"")), ""z"": 42}, coords={""x"": (""u"", [1, 2, 3])}\n        )\n        actual = original.swap_dims({""x"": ""u""})\n        assert_identical(expected, actual)\n\n        # handle multiindex case\n        idx = pd.MultiIndex.from_arrays([list(""aab""), list(""yzz"")], names=[""y1"", ""y2""])\n        original = Dataset({""x"": [1, 2, 3], ""y"": (""x"", idx), ""z"": 42})\n        expected = Dataset({""z"": 42}, {""x"": (""y"", [1, 2, 3]), ""y"": idx})\n        actual = original.swap_dims({""x"": ""y""})\n        assert_identical(expected, actual)\n        assert isinstance(actual.variables[""y""], IndexVariable)\n        assert isinstance(actual.variables[""x""], Variable)\n        pd.testing.assert_index_equal(actual.indexes[""y""], expected.indexes[""y""])\n\n    def test_expand_dims_error(self):\n        original = Dataset(\n            {\n                ""x"": (""a"", np.random.randn(3)),\n                ""y"": ([""b"", ""a""], np.random.randn(4, 3)),\n                ""z"": (""a"", np.random.randn(3)),\n            },\n            coords={\n                ""a"": np.linspace(0, 1, 3),\n                ""b"": np.linspace(0, 1, 4),\n                ""c"": np.linspace(0, 1, 5),\n            },\n            attrs={""key"": ""entry""},\n        )\n\n        with raises_regex(ValueError, ""already exists""):\n            original.expand_dims(dim=[""x""])\n\n        # Make sure it raises true error also for non-dimensional coordinates\n        # which has dimension.\n        original = original.set_coords(""z"")\n        with raises_regex(ValueError, ""already exists""):\n            original.expand_dims(dim=[""z""])\n\n        original = Dataset(\n            {\n                ""x"": (""a"", np.random.randn(3)),\n                ""y"": ([""b"", ""a""], np.random.randn(4, 3)),\n                ""z"": (""a"", np.random.randn(3)),\n            },\n            coords={\n                ""a"": np.linspace(0, 1, 3),\n                ""b"": np.linspace(0, 1, 4),\n                ""c"": np.linspace(0, 1, 5),\n            },\n            attrs={""key"": ""entry""},\n        )\n        with raises_regex(TypeError, ""value of new dimension""):\n            original.expand_dims({""d"": 3.2})\n        with raises_regex(ValueError, ""both keyword and positional""):\n            original.expand_dims({""d"": 4}, e=4)\n\n    def test_expand_dims_int(self):\n        original = Dataset(\n            {""x"": (""a"", np.random.randn(3)), ""y"": ([""b"", ""a""], np.random.randn(4, 3))},\n            coords={\n                ""a"": np.linspace(0, 1, 3),\n                ""b"": np.linspace(0, 1, 4),\n                ""c"": np.linspace(0, 1, 5),\n            },\n            attrs={""key"": ""entry""},\n        )\n\n        actual = original.expand_dims([""z""], [1])\n        expected = Dataset(\n            {\n                ""x"": original[""x""].expand_dims(""z"", 1),\n                ""y"": original[""y""].expand_dims(""z"", 1),\n            },\n            coords={\n                ""a"": np.linspace(0, 1, 3),\n                ""b"": np.linspace(0, 1, 4),\n                ""c"": np.linspace(0, 1, 5),\n            },\n            attrs={""key"": ""entry""},\n        )\n        assert_identical(expected, actual)\n        # make sure squeeze restores the original data set.\n        roundtripped = actual.squeeze(""z"")\n        assert_identical(original, roundtripped)\n\n        # another test with a negative axis\n        actual = original.expand_dims([""z""], [-1])\n        expected = Dataset(\n            {\n                ""x"": original[""x""].expand_dims(""z"", -1),\n                ""y"": original[""y""].expand_dims(""z"", -1),\n            },\n            coords={\n                ""a"": np.linspace(0, 1, 3),\n                ""b"": np.linspace(0, 1, 4),\n                ""c"": np.linspace(0, 1, 5),\n            },\n            attrs={""key"": ""entry""},\n        )\n        assert_identical(expected, actual)\n        # make sure squeeze restores the original data set.\n        roundtripped = actual.squeeze(""z"")\n        assert_identical(original, roundtripped)\n\n    def test_expand_dims_coords(self):\n        original = Dataset({""x"": (""a"", np.array([1, 2, 3]))})\n        expected = Dataset(\n            {""x"": ((""b"", ""a""), np.array([[1, 2, 3], [1, 2, 3]]))}, coords={""b"": [1, 2]}\n        )\n        actual = original.expand_dims(dict(b=[1, 2]))\n        assert_identical(expected, actual)\n        assert ""b"" not in original._coord_names\n\n    def test_expand_dims_existing_scalar_coord(self):\n        original = Dataset({""x"": 1}, {""a"": 2})\n        expected = Dataset({""x"": ((""a"",), [1])}, {""a"": [2]})\n        actual = original.expand_dims(""a"")\n        assert_identical(expected, actual)\n\n    def test_isel_expand_dims_roundtrip(self):\n        original = Dataset({""x"": ((""a"",), [1])}, {""a"": [2]})\n        actual = original.isel(a=0).expand_dims(""a"")\n        assert_identical(actual, original)\n\n    def test_expand_dims_mixed_int_and_coords(self):\n        # Test expanding one dimension to have size > 1 that doesn\'t have\n        # coordinates, and also expanding another dimension to have size > 1\n        # that DOES have coordinates.\n        original = Dataset(\n            {""x"": (""a"", np.random.randn(3)), ""y"": ([""b"", ""a""], np.random.randn(4, 3))},\n            coords={\n                ""a"": np.linspace(0, 1, 3),\n                ""b"": np.linspace(0, 1, 4),\n                ""c"": np.linspace(0, 1, 5),\n            },\n        )\n\n        actual = original.expand_dims({""d"": 4, ""e"": [""l"", ""m"", ""n""]})\n\n        expected = Dataset(\n            {\n                ""x"": xr.DataArray(\n                    original[""x""].values * np.ones([4, 3, 3]),\n                    coords=dict(d=range(4), e=[""l"", ""m"", ""n""], a=np.linspace(0, 1, 3)),\n                    dims=[""d"", ""e"", ""a""],\n                ).drop_vars(""d""),\n                ""y"": xr.DataArray(\n                    original[""y""].values * np.ones([4, 3, 4, 3]),\n                    coords=dict(\n                        d=range(4),\n                        e=[""l"", ""m"", ""n""],\n                        b=np.linspace(0, 1, 4),\n                        a=np.linspace(0, 1, 3),\n                    ),\n                    dims=[""d"", ""e"", ""b"", ""a""],\n                ).drop_vars(""d""),\n            },\n            coords={""c"": np.linspace(0, 1, 5)},\n        )\n        assert_identical(actual, expected)\n\n    def test_expand_dims_kwargs_python36plus(self):\n        original = Dataset(\n            {""x"": (""a"", np.random.randn(3)), ""y"": ([""b"", ""a""], np.random.randn(4, 3))},\n            coords={\n                ""a"": np.linspace(0, 1, 3),\n                ""b"": np.linspace(0, 1, 4),\n                ""c"": np.linspace(0, 1, 5),\n            },\n            attrs={""key"": ""entry""},\n        )\n        other_way = original.expand_dims(e=[""l"", ""m"", ""n""])\n        other_way_expected = Dataset(\n            {\n                ""x"": xr.DataArray(\n                    original[""x""].values * np.ones([3, 3]),\n                    coords=dict(e=[""l"", ""m"", ""n""], a=np.linspace(0, 1, 3)),\n                    dims=[""e"", ""a""],\n                ),\n                ""y"": xr.DataArray(\n                    original[""y""].values * np.ones([3, 4, 3]),\n                    coords=dict(\n                        e=[""l"", ""m"", ""n""],\n                        b=np.linspace(0, 1, 4),\n                        a=np.linspace(0, 1, 3),\n                    ),\n                    dims=[""e"", ""b"", ""a""],\n                ),\n            },\n            coords={""c"": np.linspace(0, 1, 5)},\n            attrs={""key"": ""entry""},\n        )\n        assert_identical(other_way_expected, other_way)\n\n    def test_set_index(self):\n        expected = create_test_multiindex()\n        mindex = expected[""x""].to_index()\n        indexes = [mindex.get_level_values(n) for n in mindex.names]\n        coords = {idx.name: (""x"", idx) for idx in indexes}\n        ds = Dataset({}, coords=coords)\n\n        obj = ds.set_index(x=mindex.names)\n        assert_identical(obj, expected)\n\n        with pytest.raises(TypeError):\n            ds.set_index(x=mindex.names, inplace=True)\n            assert_identical(ds, expected)\n\n        # ensure set_index with no existing index and a single data var given\n        # doesn\'t return multi-index\n        ds = Dataset(data_vars={""x_var"": (""x"", [0, 1, 2])})\n        expected = Dataset(coords={""x"": [0, 1, 2]})\n        assert_identical(ds.set_index(x=""x_var""), expected)\n\n        # Issue 3176: Ensure clear error message on key error.\n        with pytest.raises(ValueError) as excinfo:\n            ds.set_index(foo=""bar"")\n        assert str(excinfo.value) == ""bar is not the name of an existing variable.""\n\n    def test_reset_index(self):\n        ds = create_test_multiindex()\n        mindex = ds[""x""].to_index()\n        indexes = [mindex.get_level_values(n) for n in mindex.names]\n        coords = {idx.name: (""x"", idx) for idx in indexes}\n        expected = Dataset({}, coords=coords)\n\n        obj = ds.reset_index(""x"")\n        assert_identical(obj, expected)\n\n        with pytest.raises(TypeError):\n            ds.reset_index(""x"", inplace=True)\n\n    def test_reset_index_keep_attrs(self):\n        coord_1 = DataArray([1, 2], dims=[""coord_1""], attrs={""attrs"": True})\n        ds = Dataset({}, {""coord_1"": coord_1})\n        expected = Dataset({}, {""coord_1_"": coord_1})\n        obj = ds.reset_index(""coord_1"")\n        assert_identical(expected, obj)\n\n    def test_reorder_levels(self):\n        ds = create_test_multiindex()\n        mindex = ds[""x""].to_index()\n        midx = mindex.reorder_levels([""level_2"", ""level_1""])\n        expected = Dataset({}, coords={""x"": midx})\n\n        reindexed = ds.reorder_levels(x=[""level_2"", ""level_1""])\n        assert_identical(reindexed, expected)\n\n        with pytest.raises(TypeError):\n            ds.reorder_levels(x=[""level_2"", ""level_1""], inplace=True)\n\n        ds = Dataset({}, coords={""x"": [1, 2]})\n        with raises_regex(ValueError, ""has no MultiIndex""):\n            ds.reorder_levels(x=[""level_1"", ""level_2""])\n\n    def test_stack(self):\n        ds = Dataset(\n            {""a"": (""x"", [0, 1]), ""b"": ((""x"", ""y""), [[0, 1], [2, 3]]), ""y"": [""a"", ""b""]}\n        )\n\n        exp_index = pd.MultiIndex.from_product([[0, 1], [""a"", ""b""]], names=[""x"", ""y""])\n        expected = Dataset(\n            {""a"": (""z"", [0, 0, 1, 1]), ""b"": (""z"", [0, 1, 2, 3]), ""z"": exp_index}\n        )\n        actual = ds.stack(z=[""x"", ""y""])\n        assert_identical(expected, actual)\n\n        actual = ds.stack(z=[...])\n        assert_identical(expected, actual)\n\n        # non list dims with ellipsis\n        actual = ds.stack(z=(...,))\n        assert_identical(expected, actual)\n\n        # ellipsis with given dim\n        actual = ds.stack(z=[..., ""y""])\n        assert_identical(expected, actual)\n\n        exp_index = pd.MultiIndex.from_product([[""a"", ""b""], [0, 1]], names=[""y"", ""x""])\n        expected = Dataset(\n            {""a"": (""z"", [0, 1, 0, 1]), ""b"": (""z"", [0, 2, 1, 3]), ""z"": exp_index}\n        )\n        actual = ds.stack(z=[""y"", ""x""])\n        assert_identical(expected, actual)\n\n    def test_unstack(self):\n        index = pd.MultiIndex.from_product([[0, 1], [""a"", ""b""]], names=[""x"", ""y""])\n        ds = Dataset({""b"": (""z"", [0, 1, 2, 3]), ""z"": index})\n        expected = Dataset(\n            {""b"": ((""x"", ""y""), [[0, 1], [2, 3]]), ""x"": [0, 1], ""y"": [""a"", ""b""]}\n        )\n        for dim in [""z"", [""z""], None]:\n            actual = ds.unstack(dim)\n            assert_identical(actual, expected)\n\n    def test_unstack_errors(self):\n        ds = Dataset({""x"": [1, 2, 3]})\n        with raises_regex(ValueError, ""does not contain the dimensions""):\n            ds.unstack(""foo"")\n        with raises_regex(ValueError, ""do not have a MultiIndex""):\n            ds.unstack(""x"")\n\n    def test_unstack_fill_value(self):\n        ds = xr.Dataset(\n            {""var"": ((""x"",), np.arange(6))},\n            coords={""x"": [0, 1, 2] * 2, ""y"": ((""x"",), [""a""] * 3 + [""b""] * 3)},\n        )\n        # make ds incomplete\n        ds = ds.isel(x=[0, 2, 3, 4]).set_index(index=[""x"", ""y""])\n        # test fill_value\n        actual = ds.unstack(""index"", fill_value=-1)\n        expected = ds.unstack(""index"").fillna(-1).astype(np.int)\n        assert actual[""var""].dtype == np.int\n        assert_equal(actual, expected)\n\n        actual = ds[""var""].unstack(""index"", fill_value=-1)\n        expected = ds[""var""].unstack(""index"").fillna(-1).astype(np.int)\n        assert actual.equals(expected)\n\n    @requires_sparse\n    def test_unstack_sparse(self):\n        ds = xr.Dataset(\n            {""var"": ((""x"",), np.arange(6))},\n            coords={""x"": [0, 1, 2] * 2, ""y"": ((""x"",), [""a""] * 3 + [""b""] * 3)},\n        )\n        # make ds incomplete\n        ds = ds.isel(x=[0, 2, 3, 4]).set_index(index=[""x"", ""y""])\n        # test fill_value\n        actual = ds.unstack(""index"", sparse=True)\n        expected = ds.unstack(""index"")\n        assert actual[""var""].variable._to_dense().equals(expected[""var""].variable)\n        assert actual[""var""].data.density < 1.0\n\n        actual = ds[""var""].unstack(""index"", sparse=True)\n        expected = ds[""var""].unstack(""index"")\n        assert actual.variable._to_dense().equals(expected.variable)\n        assert actual.data.density < 1.0\n\n    def test_stack_unstack_fast(self):\n        ds = Dataset(\n            {\n                ""a"": (""x"", [0, 1]),\n                ""b"": ((""x"", ""y""), [[0, 1], [2, 3]]),\n                ""x"": [0, 1],\n                ""y"": [""a"", ""b""],\n            }\n        )\n        actual = ds.stack(z=[""x"", ""y""]).unstack(""z"")\n        assert actual.broadcast_equals(ds)\n\n        actual = ds[[""b""]].stack(z=[""x"", ""y""]).unstack(""z"")\n        assert actual.identical(ds[[""b""]])\n\n    def test_stack_unstack_slow(self):\n        ds = Dataset(\n            {\n                ""a"": (""x"", [0, 1]),\n                ""b"": ((""x"", ""y""), [[0, 1], [2, 3]]),\n                ""x"": [0, 1],\n                ""y"": [""a"", ""b""],\n            }\n        )\n        stacked = ds.stack(z=[""x"", ""y""])\n        actual = stacked.isel(z=slice(None, None, -1)).unstack(""z"")\n        assert actual.broadcast_equals(ds)\n\n        stacked = ds[[""b""]].stack(z=[""x"", ""y""])\n        actual = stacked.isel(z=slice(None, None, -1)).unstack(""z"")\n        assert actual.identical(ds[[""b""]])\n\n    def test_to_stacked_array_invalid_sample_dims(self):\n        data = xr.Dataset(\n            data_vars={""a"": ((""x"", ""y""), [[0, 1, 2], [3, 4, 5]]), ""b"": (""x"", [6, 7])},\n            coords={""y"": [""u"", ""v"", ""w""]},\n        )\n        with pytest.raises(ValueError):\n            data.to_stacked_array(""features"", sample_dims=[""y""])\n\n    def test_to_stacked_array_name(self):\n        name = ""adf9d""\n\n        # make a two dimensional dataset\n        a, b = create_test_stacked_array()\n        D = xr.Dataset({""a"": a, ""b"": b})\n        sample_dims = [""x""]\n\n        y = D.to_stacked_array(""features"", sample_dims, name=name)\n        assert y.name == name\n\n    def test_to_stacked_array_dtype_dims(self):\n        # make a two dimensional dataset\n        a, b = create_test_stacked_array()\n        D = xr.Dataset({""a"": a, ""b"": b})\n        sample_dims = [""x""]\n        y = D.to_stacked_array(""features"", sample_dims)\n        assert y.indexes[""features""].levels[1].dtype == D.y.dtype\n        assert y.dims == (""x"", ""features"")\n\n    def test_to_stacked_array_to_unstacked_dataset(self):\n        # make a two dimensional dataset\n        a, b = create_test_stacked_array()\n        D = xr.Dataset({""a"": a, ""b"": b})\n        sample_dims = [""x""]\n        y = D.to_stacked_array(""features"", sample_dims).transpose(""x"", ""features"")\n\n        x = y.to_unstacked_dataset(""features"")\n        assert_identical(D, x)\n\n        # test on just one sample\n        x0 = y[0].to_unstacked_dataset(""features"")\n        d0 = D.isel(x=0)\n        assert_identical(d0, x0)\n\n    def test_to_stacked_array_to_unstacked_dataset_different_dimension(self):\n        # test when variables have different dimensionality\n        a, b = create_test_stacked_array()\n        sample_dims = [""x""]\n        D = xr.Dataset({""a"": a, ""b"": b.isel(y=0)})\n\n        y = D.to_stacked_array(""features"", sample_dims)\n        x = y.to_unstacked_dataset(""features"")\n        assert_identical(D, x)\n\n    def test_update(self):\n        data = create_test_data(seed=0)\n        expected = data.copy()\n        var2 = Variable(""dim1"", np.arange(8))\n        actual = data.update({""var2"": var2})\n        expected[""var2""] = var2\n        assert_identical(expected, actual)\n\n        actual = data.copy()\n        actual_result = actual.update(data)\n        assert actual_result is actual\n        assert_identical(expected, actual)\n\n        with pytest.raises(TypeError):\n            actual = data.update(data, inplace=False)\n\n        other = Dataset(attrs={""new"": ""attr""})\n        actual = data.copy()\n        actual.update(other)\n        assert_identical(expected, actual)\n\n    def test_update_overwrite_coords(self):\n        data = Dataset({""a"": (""x"", [1, 2])}, {""b"": 3})\n        data.update(Dataset(coords={""b"": 4}))\n        expected = Dataset({""a"": (""x"", [1, 2])}, {""b"": 4})\n        assert_identical(data, expected)\n\n        data = Dataset({""a"": (""x"", [1, 2])}, {""b"": 3})\n        data.update(Dataset({""c"": 5}, coords={""b"": 4}))\n        expected = Dataset({""a"": (""x"", [1, 2]), ""c"": 5}, {""b"": 4})\n        assert_identical(data, expected)\n\n        data = Dataset({""a"": (""x"", [1, 2])}, {""b"": 3})\n        data.update({""c"": DataArray(5, coords={""b"": 4})})\n        expected = Dataset({""a"": (""x"", [1, 2]), ""c"": 5}, {""b"": 3})\n        assert_identical(data, expected)\n\n    def test_update_auto_align(self):\n        ds = Dataset({""x"": (""t"", [3, 4])}, {""t"": [0, 1]})\n\n        expected = Dataset({""x"": (""t"", [3, 4]), ""y"": (""t"", [np.nan, 5])}, {""t"": [0, 1]})\n        actual = ds.copy()\n        other = {""y"": (""t"", [5]), ""t"": [1]}\n        with raises_regex(ValueError, ""conflicting sizes""):\n            actual.update(other)\n        actual.update(Dataset(other))\n        assert_identical(expected, actual)\n\n        actual = ds.copy()\n        other = Dataset({""y"": (""t"", [5]), ""t"": [100]})\n        actual.update(other)\n        expected = Dataset(\n            {""x"": (""t"", [3, 4]), ""y"": (""t"", [np.nan] * 2)}, {""t"": [0, 1]}\n        )\n        assert_identical(expected, actual)\n\n    def test_getitem(self):\n        data = create_test_data()\n        assert isinstance(data[""var1""], DataArray)\n        assert_equal(data[""var1""].variable, data.variables[""var1""])\n        with pytest.raises(KeyError):\n            data[""notfound""]\n        with pytest.raises(KeyError):\n            data[[""var1"", ""notfound""]]\n\n        actual = data[[""var1"", ""var2""]]\n        expected = Dataset({""var1"": data[""var1""], ""var2"": data[""var2""]})\n        assert_equal(expected, actual)\n\n        actual = data[""numbers""]\n        expected = DataArray(\n            data[""numbers""].variable,\n            {""dim3"": data[""dim3""], ""numbers"": data[""numbers""]},\n            dims=""dim3"",\n            name=""numbers"",\n        )\n        assert_identical(expected, actual)\n\n        actual = data[dict(dim1=0)]\n        expected = data.isel(dim1=0)\n        assert_identical(expected, actual)\n\n    def test_getitem_hashable(self):\n        data = create_test_data()\n        data[(3, 4)] = data[""var1""] + 1\n        expected = data[""var1""] + 1\n        expected.name = (3, 4)\n        assert_identical(expected, data[(3, 4)])\n        with raises_regex(KeyError, ""(\'var1\', \'var2\')""):\n            data[(""var1"", ""var2"")]\n\n    def test_virtual_variables_default_coords(self):\n        dataset = Dataset({""foo"": (""x"", range(10))})\n        expected = DataArray(range(10), dims=""x"", name=""x"")\n        actual = dataset[""x""]\n        assert_identical(expected, actual)\n        assert isinstance(actual.variable, IndexVariable)\n\n        actual = dataset[[""x"", ""foo""]]\n        expected = dataset.assign_coords(x=range(10))\n        assert_identical(expected, actual)\n\n    def test_virtual_variables_time(self):\n        # access virtual variables\n        data = create_test_data()\n        expected = DataArray(\n            1 + np.arange(20), coords=[data[""time""]], dims=""time"", name=""dayofyear""\n        )\n\n        assert_array_equal(\n            data[""time.month""].values, data.variables[""time""].to_index().month\n        )\n        assert_array_equal(data[""time.season""].values, ""DJF"")\n        # test virtual variable math\n        assert_array_equal(data[""time.dayofyear""] + 1, 2 + np.arange(20))\n        assert_array_equal(np.sin(data[""time.dayofyear""]), np.sin(1 + np.arange(20)))\n        # ensure they become coordinates\n        expected = Dataset({}, {""dayofyear"": data[""time.dayofyear""]})\n        actual = data[[""time.dayofyear""]]\n        assert_equal(expected, actual)\n        # non-coordinate variables\n        ds = Dataset({""t"": (""x"", pd.date_range(""2000-01-01"", periods=3))})\n        assert (ds[""t.year""] == 2000).all()\n\n    def test_virtual_variable_same_name(self):\n        # regression test for GH367\n        times = pd.date_range(""2000-01-01"", freq=""H"", periods=5)\n        data = Dataset({""time"": times})\n        actual = data[""time.time""]\n        expected = DataArray(times.time, [(""time"", times)], name=""time"")\n        assert_identical(actual, expected)\n\n    def test_virtual_variable_multiindex(self):\n        # access multi-index levels as virtual variables\n        data = create_test_multiindex()\n        expected = DataArray(\n            [""a"", ""a"", ""b"", ""b""],\n            name=""level_1"",\n            coords=[data[""x""].to_index()],\n            dims=""x"",\n        )\n        assert_identical(expected, data[""level_1""])\n\n        # combine multi-index level and datetime\n        dr_index = pd.date_range(""1/1/2011"", periods=4, freq=""H"")\n        mindex = pd.MultiIndex.from_arrays(\n            [[""a"", ""a"", ""b"", ""b""], dr_index], names=(""level_str"", ""level_date"")\n        )\n        data = Dataset({}, {""x"": mindex})\n        expected = DataArray(\n            mindex.get_level_values(""level_date"").hour,\n            name=""hour"",\n            coords=[mindex],\n            dims=""x"",\n        )\n        assert_identical(expected, data[""level_date.hour""])\n\n        # attribute style access\n        assert_identical(data.level_str, data[""level_str""])\n\n    def test_time_season(self):\n        ds = Dataset({""t"": pd.date_range(""2000-01-01"", periods=12, freq=""M"")})\n        seas = [""DJF""] * 2 + [""MAM""] * 3 + [""JJA""] * 3 + [""SON""] * 3 + [""DJF""]\n        assert_array_equal(seas, ds[""t.season""])\n\n    def test_slice_virtual_variable(self):\n        data = create_test_data()\n        assert_equal(\n            data[""time.dayofyear""][:10].variable, Variable([""time""], 1 + np.arange(10))\n        )\n        assert_equal(data[""time.dayofyear""][0].variable, Variable([], 1))\n\n    def test_setitem(self):\n        # assign a variable\n        var = Variable([""dim1""], np.random.randn(8))\n        data1 = create_test_data()\n        data1[""A""] = var\n        data2 = data1.copy()\n        data2[""A""] = var\n        assert_identical(data1, data2)\n        # assign a dataset array\n        dv = 2 * data2[""A""]\n        data1[""B""] = dv.variable\n        data2[""B""] = dv\n        assert_identical(data1, data2)\n        # can\'t assign an ND array without dimensions\n        with raises_regex(ValueError, ""without explicit dimension names""):\n            data2[""C""] = var.values.reshape(2, 4)\n        # but can assign a 1D array\n        data1[""C""] = var.values\n        data2[""C""] = (""C"", var.values)\n        assert_identical(data1, data2)\n        # can assign a scalar\n        data1[""scalar""] = 0\n        data2[""scalar""] = ([], 0)\n        assert_identical(data1, data2)\n        # can\'t use the same dimension name as a scalar var\n        with raises_regex(ValueError, ""already exists as a scalar""):\n            data1[""newvar""] = (""scalar"", [3, 4, 5])\n        # can\'t resize a used dimension\n        with raises_regex(ValueError, ""arguments without labels""):\n            data1[""dim1""] = data1[""dim1""][:5]\n        # override an existing value\n        data1[""A""] = 3 * data2[""A""]\n        assert_equal(data1[""A""], 3 * data2[""A""])\n\n        with pytest.raises(NotImplementedError):\n            data1[{""x"": 0}] = 0\n\n    def test_setitem_pandas(self):\n\n        ds = self.make_example_math_dataset()\n        ds[""x""] = np.arange(3)\n        ds_copy = ds.copy()\n        ds_copy[""bar""] = ds[""bar""].to_pandas()\n\n        assert_equal(ds, ds_copy)\n\n    def test_setitem_auto_align(self):\n        ds = Dataset()\n        ds[""x""] = (""y"", range(3))\n        ds[""y""] = 1 + np.arange(3)\n        expected = Dataset({""x"": (""y"", range(3)), ""y"": 1 + np.arange(3)})\n        assert_identical(ds, expected)\n\n        ds[""y""] = DataArray(range(3), dims=""y"")\n        expected = Dataset({""x"": (""y"", range(3))}, {""y"": range(3)})\n        assert_identical(ds, expected)\n\n        ds[""x""] = DataArray([1, 2], coords=[(""y"", [0, 1])])\n        expected = Dataset({""x"": (""y"", [1, 2, np.nan])}, {""y"": range(3)})\n        assert_identical(ds, expected)\n\n        ds[""x""] = 42\n        expected = Dataset({""x"": 42, ""y"": range(3)})\n        assert_identical(ds, expected)\n\n        ds[""x""] = DataArray([4, 5, 6, 7], coords=[(""y"", [0, 1, 2, 3])])\n        expected = Dataset({""x"": (""y"", [4, 5, 6])}, {""y"": range(3)})\n        assert_identical(ds, expected)\n\n    def test_setitem_dimension_override(self):\n        # regression test for GH-3377\n        ds = xr.Dataset({""x"": [0, 1, 2]})\n        ds[""x""] = ds[""x""][:2]\n        expected = Dataset({""x"": [0, 1]})\n        assert_identical(ds, expected)\n\n        ds = xr.Dataset({""x"": [0, 1, 2]})\n        ds[""x""] = np.array([0, 1])\n        assert_identical(ds, expected)\n\n        ds = xr.Dataset({""x"": [0, 1, 2]})\n        ds.coords[""x""] = [0, 1]\n        assert_identical(ds, expected)\n\n    def test_setitem_with_coords(self):\n        # Regression test for GH:2068\n        ds = create_test_data()\n\n        other = DataArray(\n            np.arange(10), dims=""dim3"", coords={""numbers"": (""dim3"", np.arange(10))}\n        )\n        expected = ds.copy()\n        expected[""var3""] = other.drop_vars(""numbers"")\n        actual = ds.copy()\n        actual[""var3""] = other\n        assert_identical(expected, actual)\n        assert ""numbers"" in other.coords  # should not change other\n\n        # with alignment\n        other = ds[""var3""].isel(dim3=slice(1, -1))\n        other[""numbers""] = (""dim3"", np.arange(8))\n        actual = ds.copy()\n        actual[""var3""] = other\n        assert ""numbers"" in other.coords  # should not change other\n        expected = ds.copy()\n        expected[""var3""] = ds[""var3""].isel(dim3=slice(1, -1))\n        assert_identical(expected, actual)\n\n        # with non-duplicate coords\n        other = ds[""var3""].isel(dim3=slice(1, -1))\n        other[""numbers""] = (""dim3"", np.arange(8))\n        other[""position""] = (""dim3"", np.arange(8))\n        actual = ds.copy()\n        actual[""var3""] = other\n        assert ""position"" in actual\n        assert ""position"" in other.coords\n\n        # assigning a coordinate-only dataarray\n        actual = ds.copy()\n        other = actual[""numbers""]\n        other[0] = 10\n        actual[""numbers""] = other\n        assert actual[""numbers""][0] == 10\n\n        # GH: 2099\n        ds = Dataset(\n            {""var"": (""x"", [1, 2, 3])},\n            coords={""x"": [0, 1, 2], ""z1"": (""x"", [1, 2, 3]), ""z2"": (""x"", [1, 2, 3])},\n        )\n        ds[""var""] = ds[""var""] * 2\n        assert np.allclose(ds[""var""], [2, 4, 6])\n\n    def test_setitem_align_new_indexes(self):\n        ds = Dataset({""foo"": (""x"", [1, 2, 3])}, {""x"": [0, 1, 2]})\n        ds[""bar""] = DataArray([2, 3, 4], [(""x"", [1, 2, 3])])\n        expected = Dataset(\n            {""foo"": (""x"", [1, 2, 3]), ""bar"": (""x"", [np.nan, 2, 3])}, {""x"": [0, 1, 2]}\n        )\n        assert_identical(ds, expected)\n\n    def test_assign(self):\n        ds = Dataset()\n        actual = ds.assign(x=[0, 1, 2], y=2)\n        expected = Dataset({""x"": [0, 1, 2], ""y"": 2})\n        assert_identical(actual, expected)\n        assert list(actual.variables) == [""x"", ""y""]\n        assert_identical(ds, Dataset())\n\n        actual = actual.assign(y=lambda ds: ds.x ** 2)\n        expected = Dataset({""y"": (""x"", [0, 1, 4]), ""x"": [0, 1, 2]})\n        assert_identical(actual, expected)\n\n        actual = actual.assign_coords(z=2)\n        expected = Dataset({""y"": (""x"", [0, 1, 4])}, {""z"": 2, ""x"": [0, 1, 2]})\n        assert_identical(actual, expected)\n\n        ds = Dataset({""a"": (""x"", range(3))}, {""b"": (""x"", [""A""] * 2 + [""B""])})\n        actual = ds.groupby(""b"").assign(c=lambda ds: 2 * ds.a)\n        expected = ds.merge({""c"": (""x"", [0, 2, 4])})\n        assert_identical(actual, expected)\n\n        actual = ds.groupby(""b"").assign(c=lambda ds: ds.a.sum())\n        expected = ds.merge({""c"": (""x"", [1, 1, 2])})\n        assert_identical(actual, expected)\n\n        actual = ds.groupby(""b"").assign_coords(c=lambda ds: ds.a.sum())\n        expected = expected.set_coords(""c"")\n        assert_identical(actual, expected)\n\n    def test_assign_coords(self):\n        ds = Dataset()\n\n        actual = ds.assign(x=[0, 1, 2], y=2)\n        actual = actual.assign_coords(x=list(""abc""))\n        expected = Dataset({""x"": list(""abc""), ""y"": 2})\n        assert_identical(actual, expected)\n\n        actual = ds.assign(x=[0, 1, 2], y=[2, 3])\n        actual = actual.assign_coords({""y"": [2.0, 3.0]})\n        expected = ds.assign(x=[0, 1, 2], y=[2.0, 3.0])\n        assert_identical(actual, expected)\n\n    def test_assign_attrs(self):\n        expected = Dataset(attrs=dict(a=1, b=2))\n        new = Dataset()\n        actual = new.assign_attrs(a=1, b=2)\n        assert_identical(actual, expected)\n        assert new.attrs == {}\n\n        expected.attrs[""c""] = 3\n        new_actual = actual.assign_attrs({""c"": 3})\n        assert_identical(new_actual, expected)\n        assert actual.attrs == dict(a=1, b=2)\n\n    def test_assign_multiindex_level(self):\n        data = create_test_multiindex()\n        with raises_regex(ValueError, ""conflicting MultiIndex""):\n            data.assign(level_1=range(4))\n            data.assign_coords(level_1=range(4))\n        # raise an Error when any level name is used as dimension GH:2299\n        with pytest.raises(ValueError):\n            data[""y""] = (""level_1"", [0, 1])\n\n    def test_merge_multiindex_level(self):\n        data = create_test_multiindex()\n        other = Dataset({""z"": (""level_1"", [0, 1])})  # conflict dimension\n        with pytest.raises(ValueError):\n            data.merge(other)\n        other = Dataset({""level_1"": (""x"", [0, 1])})  # conflict variable name\n        with pytest.raises(ValueError):\n            data.merge(other)\n\n    def test_setitem_original_non_unique_index(self):\n        # regression test for GH943\n        original = Dataset({""data"": (""x"", np.arange(5))}, coords={""x"": [0, 1, 2, 0, 1]})\n        expected = Dataset({""data"": (""x"", np.arange(5))}, {""x"": range(5)})\n\n        actual = original.copy()\n        actual[""x""] = list(range(5))\n        assert_identical(actual, expected)\n\n        actual = original.copy()\n        actual[""x""] = (""x"", list(range(5)))\n        assert_identical(actual, expected)\n\n        actual = original.copy()\n        actual.coords[""x""] = list(range(5))\n        assert_identical(actual, expected)\n\n    def test_setitem_both_non_unique_index(self):\n        # regression test for GH956\n        names = [""joaquin"", ""manolo"", ""joaquin""]\n        values = np.random.randint(0, 256, (3, 4, 4))\n        array = DataArray(\n            values, dims=[""name"", ""row"", ""column""], coords=[names, range(4), range(4)]\n        )\n        expected = Dataset({""first"": array, ""second"": array})\n        actual = array.rename(""first"").to_dataset()\n        actual[""second""] = array\n        assert_identical(expected, actual)\n\n    def test_setitem_multiindex_level(self):\n        data = create_test_multiindex()\n        with raises_regex(ValueError, ""conflicting MultiIndex""):\n            data[""level_1""] = range(4)\n\n    def test_delitem(self):\n        data = create_test_data()\n        all_items = set(data.variables)\n        assert set(data.variables) == all_items\n        del data[""var1""]\n        assert set(data.variables) == all_items - {""var1""}\n        del data[""numbers""]\n        assert set(data.variables) == all_items - {""var1"", ""numbers""}\n        assert ""numbers"" not in data.coords\n\n        expected = Dataset()\n        actual = Dataset({""y"": (""x"", [1, 2])})\n        del actual[""y""]\n        assert_identical(expected, actual)\n\n    def test_squeeze(self):\n        data = Dataset({""foo"": ([""x"", ""y"", ""z""], [[[1], [2]]])})\n        for args in [[], [[""x""]], [[""x"", ""z""]]]:\n\n            def get_args(v):\n                return [set(args[0]) & set(v.dims)] if args else []\n\n            expected = Dataset(\n                {k: v.squeeze(*get_args(v)) for k, v in data.variables.items()}\n            )\n            expected = expected.set_coords(data.coords)\n            assert_identical(expected, data.squeeze(*args))\n        # invalid squeeze\n        with raises_regex(ValueError, ""cannot select a dimension""):\n            data.squeeze(""y"")\n\n    def test_squeeze_drop(self):\n        data = Dataset({""foo"": (""x"", [1])}, {""x"": [0]})\n        expected = Dataset({""foo"": 1})\n        selected = data.squeeze(drop=True)\n        assert_identical(expected, selected)\n\n        expected = Dataset({""foo"": 1}, {""x"": 0})\n        selected = data.squeeze(drop=False)\n        assert_identical(expected, selected)\n\n        data = Dataset({""foo"": ((""x"", ""y""), [[1]])}, {""x"": [0], ""y"": [0]})\n        expected = Dataset({""foo"": 1})\n        selected = data.squeeze(drop=True)\n        assert_identical(expected, selected)\n\n        expected = Dataset({""foo"": (""x"", [1])}, {""x"": [0]})\n        selected = data.squeeze(dim=""y"", drop=True)\n        assert_identical(expected, selected)\n\n        data = Dataset({""foo"": ((""x"",), [])}, {""x"": []})\n        selected = data.squeeze(drop=True)\n        assert_identical(data, selected)\n\n    def test_groupby(self):\n        data = Dataset(\n            {""z"": ([""x"", ""y""], np.random.randn(3, 5))},\n            {""x"": (""x"", list(""abc"")), ""c"": (""x"", [0, 1, 0]), ""y"": range(5)},\n        )\n        groupby = data.groupby(""x"")\n        assert len(groupby) == 3\n        expected_groups = {""a"": 0, ""b"": 1, ""c"": 2}\n        assert groupby.groups == expected_groups\n        expected_items = [\n            (""a"", data.isel(x=0)),\n            (""b"", data.isel(x=1)),\n            (""c"", data.isel(x=2)),\n        ]\n        for actual, expected in zip(groupby, expected_items):\n            assert actual[0] == expected[0]\n            assert_equal(actual[1], expected[1])\n\n        def identity(x):\n            return x\n\n        for k in [""x"", ""c"", ""y""]:\n            actual = data.groupby(k, squeeze=False).map(identity)\n            assert_equal(data, actual)\n\n    def test_groupby_returns_new_type(self):\n        data = Dataset({""z"": ([""x"", ""y""], np.random.randn(3, 5))})\n\n        actual = data.groupby(""x"").map(lambda ds: ds[""z""])\n        expected = data[""z""]\n        assert_identical(expected, actual)\n\n        actual = data[""z""].groupby(""x"").map(lambda x: x.to_dataset())\n        expected = data\n        assert_identical(expected, actual)\n\n    def test_groupby_iter(self):\n        data = create_test_data()\n        for n, (t, sub) in enumerate(list(data.groupby(""dim1""))[:3]):\n            assert data[""dim1""][n] == t\n            assert_equal(data[""var1""][n], sub[""var1""])\n            assert_equal(data[""var2""][n], sub[""var2""])\n            assert_equal(data[""var3""][:, n], sub[""var3""])\n\n    def test_groupby_errors(self):\n        data = create_test_data()\n        with raises_regex(TypeError, ""`group` must be""):\n            data.groupby(np.arange(10))\n        with raises_regex(ValueError, ""length does not match""):\n            data.groupby(data[""dim1""][:3])\n        with raises_regex(TypeError, ""`group` must be""):\n            data.groupby(data.coords[""dim1""].to_index())\n\n    def test_groupby_reduce(self):\n        data = Dataset(\n            {\n                ""xy"": ([""x"", ""y""], np.random.randn(3, 4)),\n                ""xonly"": (""x"", np.random.randn(3)),\n                ""yonly"": (""y"", np.random.randn(4)),\n                ""letters"": (""y"", [""a"", ""a"", ""b"", ""b""]),\n            }\n        )\n\n        expected = data.mean(""y"")\n        expected[""yonly""] = expected[""yonly""].variable.set_dims({""x"": 3})\n        actual = data.groupby(""x"").mean(...)\n        assert_allclose(expected, actual)\n\n        actual = data.groupby(""x"").mean(""y"")\n        assert_allclose(expected, actual)\n\n        letters = data[""letters""]\n        expected = Dataset(\n            {\n                ""xy"": data[""xy""].groupby(letters).mean(...),\n                ""xonly"": (data[""xonly""].mean().variable.set_dims({""letters"": 2})),\n                ""yonly"": data[""yonly""].groupby(letters).mean(),\n            }\n        )\n        actual = data.groupby(""letters"").mean(...)\n        assert_allclose(expected, actual)\n\n    def test_groupby_math(self):\n        def reorder_dims(x):\n            return x.transpose(""dim1"", ""dim2"", ""dim3"", ""time"")\n\n        ds = create_test_data()\n        ds[""dim1""] = ds[""dim1""]\n        for squeeze in [True, False]:\n            grouped = ds.groupby(""dim1"", squeeze=squeeze)\n\n            expected = reorder_dims(ds + ds.coords[""dim1""])\n            actual = grouped + ds.coords[""dim1""]\n            assert_identical(expected, reorder_dims(actual))\n\n            actual = ds.coords[""dim1""] + grouped\n            assert_identical(expected, reorder_dims(actual))\n\n            ds2 = 2 * ds\n            expected = reorder_dims(ds + ds2)\n            actual = grouped + ds2\n            assert_identical(expected, reorder_dims(actual))\n\n            actual = ds2 + grouped\n            assert_identical(expected, reorder_dims(actual))\n\n        grouped = ds.groupby(""numbers"")\n        zeros = DataArray([0, 0, 0, 0], [(""numbers"", range(4))])\n        expected = (ds + Variable(""dim3"", np.zeros(10))).transpose(\n            ""dim3"", ""dim1"", ""dim2"", ""time""\n        )\n        actual = grouped + zeros\n        assert_equal(expected, actual)\n\n        actual = zeros + grouped\n        assert_equal(expected, actual)\n\n        with raises_regex(ValueError, ""incompat.* grouped binary""):\n            grouped + ds\n        with raises_regex(ValueError, ""incompat.* grouped binary""):\n            ds + grouped\n        with raises_regex(TypeError, ""only support binary ops""):\n            grouped + 1\n        with raises_regex(TypeError, ""only support binary ops""):\n            grouped + grouped\n        with raises_regex(TypeError, ""in-place operations""):\n            ds += grouped\n\n        ds = Dataset(\n            {\n                ""x"": (""time"", np.arange(100)),\n                ""time"": pd.date_range(""2000-01-01"", periods=100),\n            }\n        )\n        with raises_regex(ValueError, ""incompat.* grouped binary""):\n            ds + ds.groupby(""time.month"")\n\n    def test_groupby_math_virtual(self):\n        ds = Dataset(\n            {""x"": (""t"", [1, 2, 3])}, {""t"": pd.date_range(""20100101"", periods=3)}\n        )\n        grouped = ds.groupby(""t.day"")\n        actual = grouped - grouped.mean(...)\n        expected = Dataset({""x"": (""t"", [0, 0, 0])}, ds[[""t"", ""t.day""]])\n        assert_identical(actual, expected)\n\n    def test_groupby_nan(self):\n        # nan should be excluded from groupby\n        ds = Dataset({""foo"": (""x"", [1, 2, 3, 4])}, {""bar"": (""x"", [1, 1, 2, np.nan])})\n        actual = ds.groupby(""bar"").mean(...)\n        expected = Dataset({""foo"": (""bar"", [1.5, 3]), ""bar"": [1, 2]})\n        assert_identical(actual, expected)\n\n    def test_groupby_order(self):\n        # groupby should preserve variables order\n        ds = Dataset()\n        for vn in [""a"", ""b"", ""c""]:\n            ds[vn] = DataArray(np.arange(10), dims=[""t""])\n        data_vars_ref = list(ds.data_vars.keys())\n        ds = ds.groupby(""t"").mean(...)\n        data_vars = list(ds.data_vars.keys())\n        assert data_vars == data_vars_ref\n        # coords are now at the end of the list, so the test below fails\n        # all_vars = list(ds.variables.keys())\n        # all_vars_ref = list(ds.variables.keys())\n        # self.assertEqual(all_vars, all_vars_ref)\n\n    def test_resample_and_first(self):\n        times = pd.date_range(""2000-01-01"", freq=""6H"", periods=10)\n        ds = Dataset(\n            {\n                ""foo"": ([""time"", ""x"", ""y""], np.random.randn(10, 5, 3)),\n                ""bar"": (""time"", np.random.randn(10), {""meta"": ""data""}),\n                ""time"": times,\n            }\n        )\n\n        actual = ds.resample(time=""1D"").first(keep_attrs=True)\n        expected = ds.isel(time=[0, 4, 8])\n        assert_identical(expected, actual)\n\n        # upsampling\n        expected_time = pd.date_range(""2000-01-01"", freq=""3H"", periods=19)\n        expected = ds.reindex(time=expected_time)\n        actual = ds.resample(time=""3H"")\n        for how in [""mean"", ""sum"", ""first"", ""last""]:\n            method = getattr(actual, how)\n            result = method()\n            assert_equal(expected, result)\n        for method in [np.mean]:\n            result = actual.reduce(method)\n            assert_equal(expected, result)\n\n    def test_resample_min_count(self):\n        times = pd.date_range(""2000-01-01"", freq=""6H"", periods=10)\n        ds = Dataset(\n            {\n                ""foo"": ([""time"", ""x"", ""y""], np.random.randn(10, 5, 3)),\n                ""bar"": (""time"", np.random.randn(10), {""meta"": ""data""}),\n                ""time"": times,\n            }\n        )\n        # inject nan\n        ds[""foo""] = xr.where(ds[""foo""] > 2.0, np.nan, ds[""foo""])\n\n        actual = ds.resample(time=""1D"").sum(min_count=1)\n        expected = xr.concat(\n            [\n                ds.isel(time=slice(i * 4, (i + 1) * 4)).sum(""time"", min_count=1)\n                for i in range(3)\n            ],\n            dim=actual[""time""],\n        )\n        assert_equal(expected, actual)\n\n    def test_resample_by_mean_with_keep_attrs(self):\n        times = pd.date_range(""2000-01-01"", freq=""6H"", periods=10)\n        ds = Dataset(\n            {\n                ""foo"": ([""time"", ""x"", ""y""], np.random.randn(10, 5, 3)),\n                ""bar"": (""time"", np.random.randn(10), {""meta"": ""data""}),\n                ""time"": times,\n            }\n        )\n        ds.attrs[""dsmeta""] = ""dsdata""\n\n        resampled_ds = ds.resample(time=""1D"").mean(keep_attrs=True)\n        actual = resampled_ds[""bar""].attrs\n        expected = ds[""bar""].attrs\n        assert expected == actual\n\n        actual = resampled_ds.attrs\n        expected = ds.attrs\n        assert expected == actual\n\n    def test_resample_loffset(self):\n        times = pd.date_range(""2000-01-01"", freq=""6H"", periods=10)\n        ds = Dataset(\n            {\n                ""foo"": ([""time"", ""x"", ""y""], np.random.randn(10, 5, 3)),\n                ""bar"": (""time"", np.random.randn(10), {""meta"": ""data""}),\n                ""time"": times,\n            }\n        )\n        ds.attrs[""dsmeta""] = ""dsdata""\n\n        actual = ds.resample(time=""24H"", loffset=""-12H"").mean(""time"").time\n        expected = xr.DataArray(\n            ds.bar.to_series().resample(""24H"", loffset=""-12H"").mean()\n        ).time\n        assert_identical(expected, actual)\n\n    def test_resample_by_mean_discarding_attrs(self):\n        times = pd.date_range(""2000-01-01"", freq=""6H"", periods=10)\n        ds = Dataset(\n            {\n                ""foo"": ([""time"", ""x"", ""y""], np.random.randn(10, 5, 3)),\n                ""bar"": (""time"", np.random.randn(10), {""meta"": ""data""}),\n                ""time"": times,\n            }\n        )\n        ds.attrs[""dsmeta""] = ""dsdata""\n\n        resampled_ds = ds.resample(time=""1D"").mean(keep_attrs=False)\n\n        assert resampled_ds[""bar""].attrs == {}\n        assert resampled_ds.attrs == {}\n\n    def test_resample_by_last_discarding_attrs(self):\n        times = pd.date_range(""2000-01-01"", freq=""6H"", periods=10)\n        ds = Dataset(\n            {\n                ""foo"": ([""time"", ""x"", ""y""], np.random.randn(10, 5, 3)),\n                ""bar"": (""time"", np.random.randn(10), {""meta"": ""data""}),\n                ""time"": times,\n            }\n        )\n        ds.attrs[""dsmeta""] = ""dsdata""\n\n        resampled_ds = ds.resample(time=""1D"").last(keep_attrs=False)\n\n        assert resampled_ds[""bar""].attrs == {}\n        assert resampled_ds.attrs == {}\n\n    @requires_scipy\n    def test_resample_drop_nondim_coords(self):\n        xs = np.arange(6)\n        ys = np.arange(3)\n        times = pd.date_range(""2000-01-01"", freq=""6H"", periods=5)\n        data = np.tile(np.arange(5), (6, 3, 1))\n        xx, yy = np.meshgrid(xs * 5, ys * 2.5)\n        tt = np.arange(len(times), dtype=int)\n        array = DataArray(data, {""time"": times, ""x"": xs, ""y"": ys}, (""x"", ""y"", ""time""))\n        xcoord = DataArray(xx.T, {""x"": xs, ""y"": ys}, (""x"", ""y""))\n        ycoord = DataArray(yy.T, {""x"": xs, ""y"": ys}, (""x"", ""y""))\n        tcoord = DataArray(tt, {""time"": times}, (""time"",))\n        ds = Dataset({""data"": array, ""xc"": xcoord, ""yc"": ycoord, ""tc"": tcoord})\n        ds = ds.set_coords([""xc"", ""yc"", ""tc""])\n\n        # Re-sample\n        actual = ds.resample(time=""12H"").mean(""time"")\n        assert ""tc"" not in actual.coords\n\n        # Up-sample - filling\n        actual = ds.resample(time=""1H"").ffill()\n        assert ""tc"" not in actual.coords\n\n        # Up-sample - interpolation\n        actual = ds.resample(time=""1H"").interpolate(""linear"")\n        assert ""tc"" not in actual.coords\n\n    def test_resample_old_api(self):\n\n        times = pd.date_range(""2000-01-01"", freq=""6H"", periods=10)\n        ds = Dataset(\n            {\n                ""foo"": ([""time"", ""x"", ""y""], np.random.randn(10, 5, 3)),\n                ""bar"": (""time"", np.random.randn(10), {""meta"": ""data""}),\n                ""time"": times,\n            }\n        )\n\n        with raises_regex(TypeError, r""resample\\(\\) no longer supports""):\n            ds.resample(""1D"", ""time"")\n\n        with raises_regex(TypeError, r""resample\\(\\) no longer supports""):\n            ds.resample(""1D"", dim=""time"", how=""mean"")\n\n        with raises_regex(TypeError, r""resample\\(\\) no longer supports""):\n            ds.resample(""1D"", dim=""time"")\n\n    def test_resample_ds_da_are_the_same(self):\n        time = pd.date_range(""2000-01-01"", freq=""6H"", periods=365 * 4)\n        ds = xr.Dataset(\n            {\n                ""foo"": ((""time"", ""x""), np.random.randn(365 * 4, 5)),\n                ""time"": time,\n                ""x"": np.arange(5),\n            }\n        )\n        assert_identical(\n            ds.resample(time=""M"").mean()[""foo""], ds.foo.resample(time=""M"").mean()\n        )\n\n    def test_ds_resample_apply_func_args(self):\n        def func(arg1, arg2, arg3=0.0):\n            return arg1.mean(""time"") + arg2 + arg3\n\n        times = pd.date_range(""2000"", freq=""D"", periods=3)\n        ds = xr.Dataset({""foo"": (""time"", [1.0, 1.0, 1.0]), ""time"": times})\n        expected = xr.Dataset({""foo"": (""time"", [3.0, 3.0, 3.0]), ""time"": times})\n        actual = ds.resample(time=""D"").map(func, args=(1.0,), arg3=1.0)\n        assert_identical(expected, actual)\n\n    def test_to_array(self):\n        ds = Dataset(\n            {""a"": 1, ""b"": (""x"", [1, 2, 3])},\n            coords={""c"": 42},\n            attrs={""Conventions"": ""None""},\n        )\n        data = [[1, 1, 1], [1, 2, 3]]\n        coords = {""c"": 42, ""variable"": [""a"", ""b""]}\n        dims = (""variable"", ""x"")\n        expected = DataArray(data, coords, dims, attrs=ds.attrs)\n        actual = ds.to_array()\n        assert_identical(expected, actual)\n\n        actual = ds.to_array(""abc"", name=""foo"")\n        expected = expected.rename({""variable"": ""abc""}).rename(""foo"")\n        assert_identical(expected, actual)\n\n    def test_to_and_from_dataframe(self):\n        x = np.random.randn(10)\n        y = np.random.randn(10)\n        t = list(""abcdefghij"")\n        ds = Dataset({""a"": (""t"", x), ""b"": (""t"", y), ""t"": (""t"", t)})\n        expected = pd.DataFrame(\n            np.array([x, y]).T, columns=[""a"", ""b""], index=pd.Index(t, name=""t"")\n        )\n        actual = ds.to_dataframe()\n        # use the .equals method to check all DataFrame metadata\n        assert expected.equals(actual), (expected, actual)\n\n        # verify coords are included\n        actual = ds.set_coords(""b"").to_dataframe()\n        assert expected.equals(actual), (expected, actual)\n\n        # check roundtrip\n        assert_identical(ds, Dataset.from_dataframe(actual))\n\n        # test a case with a MultiIndex\n        w = np.random.randn(2, 3)\n        ds = Dataset({""w"": ((""x"", ""y""), w)})\n        ds[""y""] = (""y"", list(""abc""))\n        exp_index = pd.MultiIndex.from_arrays(\n            [[0, 0, 0, 1, 1, 1], [""a"", ""b"", ""c"", ""a"", ""b"", ""c""]], names=[""x"", ""y""]\n        )\n        expected = pd.DataFrame(w.reshape(-1), columns=[""w""], index=exp_index)\n        actual = ds.to_dataframe()\n        assert expected.equals(actual)\n\n        # check roundtrip\n        assert_identical(ds.assign_coords(x=[0, 1]), Dataset.from_dataframe(actual))\n\n        # check pathological cases\n        df = pd.DataFrame([1])\n        actual = Dataset.from_dataframe(df)\n        expected = Dataset({0: (""index"", [1])}, {""index"": [0]})\n        assert_identical(expected, actual)\n\n        df = pd.DataFrame()\n        actual = Dataset.from_dataframe(df)\n        expected = Dataset(coords={""index"": []})\n        assert_identical(expected, actual)\n\n        # GH697\n        df = pd.DataFrame({""A"": []})\n        actual = Dataset.from_dataframe(df)\n        expected = Dataset({""A"": DataArray([], dims=(""index"",))}, {""index"": []})\n        assert_identical(expected, actual)\n\n        # regression test for GH278\n        # use int64 to ensure consistent results for the pandas .equals method\n        # on windows (which requires the same dtype)\n        ds = Dataset({""x"": pd.Index([""bar""]), ""a"": (""y"", np.array([1], ""int64""))}).isel(\n            x=0\n        )\n        # use .loc to ensure consistent results on Python 3\n        actual = ds.to_dataframe().loc[:, [""a"", ""x""]]\n        expected = pd.DataFrame(\n            [[1, ""bar""]], index=pd.Index([0], name=""y""), columns=[""a"", ""x""]\n        )\n        assert expected.equals(actual), (expected, actual)\n\n        ds = Dataset({""x"": np.array([0], ""int64""), ""y"": np.array([1], ""int64"")})\n        actual = ds.to_dataframe()\n        idx = pd.MultiIndex.from_arrays([[0], [1]], names=[""x"", ""y""])\n        expected = pd.DataFrame([[]], index=idx)\n        assert expected.equals(actual), (expected, actual)\n\n    def test_from_dataframe_categorical(self):\n        cat = pd.CategoricalDtype(\n            categories=[""foo"", ""bar"", ""baz"", ""qux"", ""quux"", ""corge""]\n        )\n        i1 = pd.Series([""foo"", ""bar"", ""foo""], dtype=cat)\n        i2 = pd.Series([""bar"", ""bar"", ""baz""], dtype=cat)\n\n        df = pd.DataFrame({""i1"": i1, ""i2"": i2, ""values"": [1, 2, 3]})\n        ds = df.set_index(""i1"").to_xarray()\n        assert len(ds[""i1""]) == 3\n\n        ds = df.set_index([""i1"", ""i2""]).to_xarray()\n        assert len(ds[""i1""]) == 2\n        assert len(ds[""i2""]) == 2\n\n    @requires_sparse\n    def test_from_dataframe_sparse(self):\n        import sparse\n\n        df_base = pd.DataFrame(\n            {""x"": range(10), ""y"": list(""abcdefghij""), ""z"": np.arange(0, 100, 10)}\n        )\n\n        ds_sparse = Dataset.from_dataframe(df_base.set_index(""x""), sparse=True)\n        ds_dense = Dataset.from_dataframe(df_base.set_index(""x""), sparse=False)\n        assert isinstance(ds_sparse[""y""].data, sparse.COO)\n        assert isinstance(ds_sparse[""z""].data, sparse.COO)\n        ds_sparse[""y""].data = ds_sparse[""y""].data.todense()\n        ds_sparse[""z""].data = ds_sparse[""z""].data.todense()\n        assert_identical(ds_dense, ds_sparse)\n\n        ds_sparse = Dataset.from_dataframe(df_base.set_index([""x"", ""y""]), sparse=True)\n        ds_dense = Dataset.from_dataframe(df_base.set_index([""x"", ""y""]), sparse=False)\n        assert isinstance(ds_sparse[""z""].data, sparse.COO)\n        ds_sparse[""z""].data = ds_sparse[""z""].data.todense()\n        assert_identical(ds_dense, ds_sparse)\n\n    def test_to_and_from_empty_dataframe(self):\n        # GH697\n        expected = pd.DataFrame({""foo"": []})\n        ds = Dataset.from_dataframe(expected)\n        assert len(ds[""foo""]) == 0\n        actual = ds.to_dataframe()\n        assert len(actual) == 0\n        assert expected.equals(actual)\n\n    def test_from_dataframe_non_unique_columns(self):\n        # regression test for GH449\n        df = pd.DataFrame(np.zeros((2, 2)))\n        df.columns = [""foo"", ""foo""]\n        with raises_regex(ValueError, ""non-unique columns""):\n            Dataset.from_dataframe(df)\n\n    def test_convert_dataframe_with_many_types_and_multiindex(self):\n        # regression test for GH737\n        df = pd.DataFrame(\n            {\n                ""a"": list(""abc""),\n                ""b"": list(range(1, 4)),\n                ""c"": np.arange(3, 6).astype(""u1""),\n                ""d"": np.arange(4.0, 7.0, dtype=""float64""),\n                ""e"": [True, False, True],\n                ""f"": pd.Categorical(list(""abc"")),\n                ""g"": pd.date_range(""20130101"", periods=3),\n                ""h"": pd.date_range(""20130101"", periods=3, tz=""US/Eastern""),\n            }\n        )\n        df.index = pd.MultiIndex.from_product([[""a""], range(3)], names=[""one"", ""two""])\n        roundtripped = Dataset.from_dataframe(df).to_dataframe()\n        # we can\'t do perfectly, but we should be at least as faithful as\n        # np.asarray\n        expected = df.apply(np.asarray)\n        assert roundtripped.equals(expected)\n\n    def test_to_and_from_dict(self):\n        # <xarray.Dataset>\n        # Dimensions:  (t: 10)\n        # Coordinates:\n        #   * t        (t) <U1 \'a\' \'b\' \'c\' \'d\' \'e\' \'f\' \'g\' \'h\' \'i\' \'j\'\n        # Data variables:\n        #     a        (t) float64 0.6916 -1.056 -1.163 0.9792 -0.7865 ...\n        #     b        (t) float64 1.32 0.1954 1.91 1.39 0.519 -0.2772 ...\n        x = np.random.randn(10)\n        y = np.random.randn(10)\n        t = list(""abcdefghij"")\n        ds = Dataset({""a"": (""t"", x), ""b"": (""t"", y), ""t"": (""t"", t)})\n        expected = {\n            ""coords"": {""t"": {""dims"": (""t"",), ""data"": t, ""attrs"": {}}},\n            ""attrs"": {},\n            ""dims"": {""t"": 10},\n            ""data_vars"": {\n                ""a"": {""dims"": (""t"",), ""data"": x.tolist(), ""attrs"": {}},\n                ""b"": {""dims"": (""t"",), ""data"": y.tolist(), ""attrs"": {}},\n            },\n        }\n\n        actual = ds.to_dict()\n\n        # check that they are identical\n        assert expected == actual\n\n        # check roundtrip\n        assert_identical(ds, Dataset.from_dict(actual))\n\n        # check the data=False option\n        expected_no_data = expected.copy()\n        del expected_no_data[""coords""][""t""][""data""]\n        del expected_no_data[""data_vars""][""a""][""data""]\n        del expected_no_data[""data_vars""][""b""][""data""]\n        endiantype = ""<U1"" if sys.byteorder == ""little"" else "">U1""\n        expected_no_data[""coords""][""t""].update({""dtype"": endiantype, ""shape"": (10,)})\n        expected_no_data[""data_vars""][""a""].update({""dtype"": ""float64"", ""shape"": (10,)})\n        expected_no_data[""data_vars""][""b""].update({""dtype"": ""float64"", ""shape"": (10,)})\n        actual_no_data = ds.to_dict(data=False)\n        assert expected_no_data == actual_no_data\n\n        # verify coords are included roundtrip\n        expected_ds = ds.set_coords(""b"")\n        actual = Dataset.from_dict(expected_ds.to_dict())\n\n        assert_identical(expected_ds, actual)\n\n        # test some incomplete dicts:\n        # this one has no attrs field, the dims are strings, and x, y are\n        # np.arrays\n\n        d = {\n            ""coords"": {""t"": {""dims"": ""t"", ""data"": t}},\n            ""dims"": ""t"",\n            ""data_vars"": {""a"": {""dims"": ""t"", ""data"": x}, ""b"": {""dims"": ""t"", ""data"": y}},\n        }\n        assert_identical(ds, Dataset.from_dict(d))\n\n        # this is kind of a flattened version with no coords, or data_vars\n        d = {\n            ""a"": {""dims"": ""t"", ""data"": x},\n            ""t"": {""data"": t, ""dims"": ""t""},\n            ""b"": {""dims"": ""t"", ""data"": y},\n        }\n        assert_identical(ds, Dataset.from_dict(d))\n\n        # this one is missing some necessary information\n        d = {\n            ""a"": {""data"": x},\n            ""t"": {""data"": t, ""dims"": ""t""},\n            ""b"": {""dims"": ""t"", ""data"": y},\n        }\n        with raises_regex(ValueError, ""cannot convert dict "" ""without the key \'dims\'""):\n            Dataset.from_dict(d)\n\n    def test_to_and_from_dict_with_time_dim(self):\n        x = np.random.randn(10, 3)\n        y = np.random.randn(10, 3)\n        t = pd.date_range(""20130101"", periods=10)\n        lat = [77.7, 83.2, 76]\n        ds = Dataset(\n            {\n                ""a"": ([""t"", ""lat""], x),\n                ""b"": ([""t"", ""lat""], y),\n                ""t"": (""t"", t),\n                ""lat"": (""lat"", lat),\n            }\n        )\n        roundtripped = Dataset.from_dict(ds.to_dict())\n        assert_identical(ds, roundtripped)\n\n    def test_to_and_from_dict_with_nan_nat(self):\n        x = np.random.randn(10, 3)\n        y = np.random.randn(10, 3)\n        y[2] = np.nan\n        t = pd.Series(pd.date_range(""20130101"", periods=10))\n        t[2] = np.nan\n\n        lat = [77.7, 83.2, 76]\n        ds = Dataset(\n            {\n                ""a"": ([""t"", ""lat""], x),\n                ""b"": ([""t"", ""lat""], y),\n                ""t"": (""t"", t),\n                ""lat"": (""lat"", lat),\n            }\n        )\n        roundtripped = Dataset.from_dict(ds.to_dict())\n        assert_identical(ds, roundtripped)\n\n    def test_to_dict_with_numpy_attrs(self):\n        # this doesn\'t need to roundtrip\n        x = np.random.randn(10)\n        y = np.random.randn(10)\n        t = list(""abcdefghij"")\n        attrs = {\n            ""created"": np.float64(1998),\n            ""coords"": np.array([37, -110.1, 100]),\n            ""maintainer"": ""bar"",\n        }\n        ds = Dataset({""a"": (""t"", x, attrs), ""b"": (""t"", y, attrs), ""t"": (""t"", t)})\n        expected_attrs = {\n            ""created"": attrs[""created""].item(),\n            ""coords"": attrs[""coords""].tolist(),\n            ""maintainer"": ""bar"",\n        }\n        actual = ds.to_dict()\n\n        # check that they are identical\n        assert expected_attrs == actual[""data_vars""][""a""][""attrs""]\n\n    def test_pickle(self):\n        data = create_test_data()\n        roundtripped = pickle.loads(pickle.dumps(data))\n        assert_identical(data, roundtripped)\n        # regression test for #167:\n        assert data.dims == roundtripped.dims\n\n    def test_lazy_load(self):\n        store = InaccessibleVariableDataStore()\n        create_test_data().dump_to_store(store)\n\n        for decode_cf in [True, False]:\n            ds = open_dataset(store, decode_cf=decode_cf)\n            with pytest.raises(UnexpectedDataAccess):\n                ds.load()\n            with pytest.raises(UnexpectedDataAccess):\n                ds[""var1""].values\n\n            # these should not raise UnexpectedDataAccess:\n            ds.isel(time=10)\n            ds.isel(time=slice(10), dim1=[0]).isel(dim1=0, dim2=-1)\n\n    def test_dropna(self):\n        x = np.random.randn(4, 4)\n        x[::2, 0] = np.nan\n        y = np.random.randn(4)\n        y[-1] = np.nan\n        ds = Dataset({""foo"": ((""a"", ""b""), x), ""bar"": ((""b"", y))})\n\n        expected = ds.isel(a=slice(1, None, 2))\n        actual = ds.dropna(""a"")\n        assert_identical(actual, expected)\n\n        expected = ds.isel(b=slice(1, 3))\n        actual = ds.dropna(""b"")\n        assert_identical(actual, expected)\n\n        actual = ds.dropna(""b"", subset=[""foo"", ""bar""])\n        assert_identical(actual, expected)\n\n        expected = ds.isel(b=slice(1, None))\n        actual = ds.dropna(""b"", subset=[""foo""])\n        assert_identical(actual, expected)\n\n        expected = ds.isel(b=slice(3))\n        actual = ds.dropna(""b"", subset=[""bar""])\n        assert_identical(actual, expected)\n\n        actual = ds.dropna(""a"", subset=[])\n        assert_identical(actual, ds)\n\n        actual = ds.dropna(""a"", subset=[""bar""])\n        assert_identical(actual, ds)\n\n        actual = ds.dropna(""a"", how=""all"")\n        assert_identical(actual, ds)\n\n        actual = ds.dropna(""b"", how=""all"", subset=[""bar""])\n        expected = ds.isel(b=[0, 1, 2])\n        assert_identical(actual, expected)\n\n        actual = ds.dropna(""b"", thresh=1, subset=[""bar""])\n        assert_identical(actual, expected)\n\n        actual = ds.dropna(""b"", thresh=2)\n        assert_identical(actual, ds)\n\n        actual = ds.dropna(""b"", thresh=4)\n        expected = ds.isel(b=[1, 2, 3])\n        assert_identical(actual, expected)\n\n        actual = ds.dropna(""a"", thresh=3)\n        expected = ds.isel(a=[1, 3])\n        assert_identical(actual, ds)\n\n        with raises_regex(ValueError, ""a single dataset dimension""):\n            ds.dropna(""foo"")\n        with raises_regex(ValueError, ""invalid how""):\n            ds.dropna(""a"", how=""somehow"")\n        with raises_regex(TypeError, ""must specify how or thresh""):\n            ds.dropna(""a"", how=None)\n\n    def test_fillna(self):\n        ds = Dataset({""a"": (""x"", [np.nan, 1, np.nan, 3])}, {""x"": [0, 1, 2, 3]})\n\n        # fill with -1\n        actual = ds.fillna(-1)\n        expected = Dataset({""a"": (""x"", [-1, 1, -1, 3])}, {""x"": [0, 1, 2, 3]})\n        assert_identical(expected, actual)\n\n        actual = ds.fillna({""a"": -1})\n        assert_identical(expected, actual)\n\n        other = Dataset({""a"": -1})\n        actual = ds.fillna(other)\n        assert_identical(expected, actual)\n\n        actual = ds.fillna({""a"": other.a})\n        assert_identical(expected, actual)\n\n        # fill with range(4)\n        b = DataArray(range(4), coords=[(""x"", range(4))])\n        actual = ds.fillna(b)\n        expected = b.rename(""a"").to_dataset()\n        assert_identical(expected, actual)\n\n        actual = ds.fillna(expected)\n        assert_identical(expected, actual)\n\n        actual = ds.fillna(range(4))\n        assert_identical(expected, actual)\n\n        actual = ds.fillna(b[:3])\n        assert_identical(expected, actual)\n\n        # okay to only include some data variables\n        ds[""b""] = np.nan\n        actual = ds.fillna({""a"": -1})\n        expected = Dataset(\n            {""a"": (""x"", [-1, 1, -1, 3]), ""b"": np.nan}, {""x"": [0, 1, 2, 3]}\n        )\n        assert_identical(expected, actual)\n\n        # but new data variables is not okay\n        with raises_regex(ValueError, ""must be contained""):\n            ds.fillna({""x"": 0})\n\n        # empty argument should be OK\n        result = ds.fillna({})\n        assert_identical(ds, result)\n\n        result = ds.fillna(Dataset(coords={""c"": 42}))\n        expected = ds.assign_coords(c=42)\n        assert_identical(expected, result)\n\n        # groupby\n        expected = Dataset({""a"": (""x"", range(4))}, {""x"": [0, 1, 2, 3]})\n        for target in [ds, expected]:\n            target.coords[""b""] = (""x"", [0, 0, 1, 1])\n        actual = ds.groupby(""b"").fillna(DataArray([0, 2], dims=""b""))\n        assert_identical(expected, actual)\n\n        actual = ds.groupby(""b"").fillna(Dataset({""a"": (""b"", [0, 2])}))\n        assert_identical(expected, actual)\n\n        # attrs with groupby\n        ds.attrs[""attr""] = ""ds""\n        ds.a.attrs[""attr""] = ""da""\n        actual = ds.groupby(""b"").fillna(Dataset({""a"": (""b"", [0, 2])}))\n        assert actual.attrs == ds.attrs\n        assert actual.a.name == ""a""\n        assert actual.a.attrs == ds.a.attrs\n\n        da = DataArray(range(5), name=""a"", attrs={""attr"": ""da""})\n        actual = da.fillna(1)\n        assert actual.name == ""a""\n        assert actual.attrs == da.attrs\n\n        ds = Dataset({""a"": da}, attrs={""attr"": ""ds""})\n        actual = ds.fillna({""a"": 1})\n        assert actual.attrs == ds.attrs\n        assert actual.a.name == ""a""\n        assert actual.a.attrs == ds.a.attrs\n\n    def test_where(self):\n        ds = Dataset({""a"": (""x"", range(5))})\n        expected = Dataset({""a"": (""x"", [np.nan, np.nan, 2, 3, 4])})\n        actual = ds.where(ds > 1)\n        assert_identical(expected, actual)\n\n        actual = ds.where(ds.a > 1)\n        assert_identical(expected, actual)\n\n        actual = ds.where(ds.a.values > 1)\n        assert_identical(expected, actual)\n\n        actual = ds.where(True)\n        assert_identical(ds, actual)\n\n        expected = ds.copy(deep=True)\n        expected[""a""].values = [np.nan] * 5\n        actual = ds.where(False)\n        assert_identical(expected, actual)\n\n        # 2d\n        ds = Dataset({""a"": ((""x"", ""y""), [[0, 1], [2, 3]])})\n        expected = Dataset({""a"": ((""x"", ""y""), [[np.nan, 1], [2, 3]])})\n        actual = ds.where(ds > 0)\n        assert_identical(expected, actual)\n\n        # groupby\n        ds = Dataset({""a"": (""x"", range(5))}, {""c"": (""x"", [0, 0, 1, 1, 1])})\n        cond = Dataset({""a"": (""c"", [True, False])})\n        expected = ds.copy(deep=True)\n        expected[""a""].values = [0, 1] + [np.nan] * 3\n        actual = ds.groupby(""c"").where(cond)\n        assert_identical(expected, actual)\n\n        # attrs with groupby\n        ds.attrs[""attr""] = ""ds""\n        ds.a.attrs[""attr""] = ""da""\n        actual = ds.groupby(""c"").where(cond)\n        assert actual.attrs == ds.attrs\n        assert actual.a.name == ""a""\n        assert actual.a.attrs == ds.a.attrs\n\n        # attrs\n        da = DataArray(range(5), name=""a"", attrs={""attr"": ""da""})\n        actual = da.where(da.values > 1)\n        assert actual.name == ""a""\n        assert actual.attrs == da.attrs\n\n        ds = Dataset({""a"": da}, attrs={""attr"": ""ds""})\n        actual = ds.where(ds > 0)\n        assert actual.attrs == ds.attrs\n        assert actual.a.name == ""a""\n        assert actual.a.attrs == ds.a.attrs\n\n        # lambda\n        ds = Dataset({""a"": (""x"", range(5))})\n        expected = Dataset({""a"": (""x"", [np.nan, np.nan, 2, 3, 4])})\n        actual = ds.where(lambda x: x > 1)\n        assert_identical(expected, actual)\n\n    def test_where_other(self):\n        ds = Dataset({""a"": (""x"", range(5))}, {""x"": range(5)})\n        expected = Dataset({""a"": (""x"", [-1, -1, 2, 3, 4])}, {""x"": range(5)})\n        actual = ds.where(ds > 1, -1)\n        assert_equal(expected, actual)\n        assert actual.a.dtype == int\n\n        actual = ds.where(lambda x: x > 1, -1)\n        assert_equal(expected, actual)\n\n        with raises_regex(ValueError, ""cannot set""):\n            ds.where(ds > 1, other=0, drop=True)\n\n        with raises_regex(ValueError, ""indexes .* are not equal""):\n            ds.where(ds > 1, ds.isel(x=slice(3)))\n\n        with raises_regex(ValueError, ""exact match required""):\n            ds.where(ds > 1, ds.assign(b=2))\n\n    def test_where_drop(self):\n        # if drop=True\n\n        # 1d\n        # data array case\n        array = DataArray(range(5), coords=[range(5)], dims=[""x""])\n        expected = DataArray(range(5)[2:], coords=[range(5)[2:]], dims=[""x""])\n        actual = array.where(array > 1, drop=True)\n        assert_identical(expected, actual)\n\n        # dataset case\n        ds = Dataset({""a"": array})\n        expected = Dataset({""a"": expected})\n\n        actual = ds.where(ds > 1, drop=True)\n        assert_identical(expected, actual)\n\n        actual = ds.where(ds.a > 1, drop=True)\n        assert_identical(expected, actual)\n\n        with raises_regex(TypeError, ""must be a""):\n            ds.where(np.arange(5) > 1, drop=True)\n\n        # 1d with odd coordinates\n        array = DataArray(\n            np.array([2, 7, 1, 8, 3]), coords=[np.array([3, 1, 4, 5, 9])], dims=[""x""]\n        )\n        expected = DataArray(\n            np.array([7, 8, 3]), coords=[np.array([1, 5, 9])], dims=[""x""]\n        )\n        actual = array.where(array > 2, drop=True)\n        assert_identical(expected, actual)\n\n        # 1d multiple variables\n        ds = Dataset({""a"": ((""x""), [0, 1, 2, 3]), ""b"": ((""x""), [4, 5, 6, 7])})\n        expected = Dataset(\n            {""a"": ((""x""), [np.nan, 1, 2, 3]), ""b"": ((""x""), [4, 5, 6, np.nan])}\n        )\n        actual = ds.where((ds > 0) & (ds < 7), drop=True)\n        assert_identical(expected, actual)\n\n        # 2d\n        ds = Dataset({""a"": ((""x"", ""y""), [[0, 1], [2, 3]])})\n        expected = Dataset({""a"": ((""x"", ""y""), [[np.nan, 1], [2, 3]])})\n        actual = ds.where(ds > 0, drop=True)\n        assert_identical(expected, actual)\n\n        # 2d with odd coordinates\n        ds = Dataset(\n            {""a"": ((""x"", ""y""), [[0, 1], [2, 3]])},\n            coords={\n                ""x"": [4, 3],\n                ""y"": [1, 2],\n                ""z"": ([""x"", ""y""], [[np.e, np.pi], [np.pi * np.e, np.pi * 3]]),\n            },\n        )\n        expected = Dataset(\n            {""a"": ((""x"", ""y""), [[3]])},\n            coords={""x"": [3], ""y"": [2], ""z"": ([""x"", ""y""], [[np.pi * 3]])},\n        )\n        actual = ds.where(ds > 2, drop=True)\n        assert_identical(expected, actual)\n\n        # 2d multiple variables\n        ds = Dataset(\n            {""a"": ((""x"", ""y""), [[0, 1], [2, 3]]), ""b"": ((""x"", ""y""), [[4, 5], [6, 7]])}\n        )\n        expected = Dataset(\n            {\n                ""a"": ((""x"", ""y""), [[np.nan, 1], [2, 3]]),\n                ""b"": ((""x"", ""y""), [[4, 5], [6, 7]]),\n            }\n        )\n        actual = ds.where(ds > 0, drop=True)\n        assert_identical(expected, actual)\n\n    def test_where_drop_empty(self):\n        # regression test for GH1341\n        array = DataArray(np.random.rand(100, 10), dims=[""nCells"", ""nVertLevels""])\n        mask = DataArray(np.zeros((100,), dtype=""bool""), dims=""nCells"")\n        actual = array.where(mask, drop=True)\n        expected = DataArray(np.zeros((0, 10)), dims=[""nCells"", ""nVertLevels""])\n        assert_identical(expected, actual)\n\n    def test_where_drop_no_indexes(self):\n        ds = Dataset({""foo"": (""x"", [0.0, 1.0])})\n        expected = Dataset({""foo"": (""x"", [1.0])})\n        actual = ds.where(ds == 1, drop=True)\n        assert_identical(expected, actual)\n\n    def test_reduce(self):\n        data = create_test_data()\n\n        assert len(data.mean().coords) == 0\n\n        actual = data.max()\n        expected = Dataset({k: v.max() for k, v in data.data_vars.items()})\n        assert_equal(expected, actual)\n\n        assert_equal(data.min(dim=[""dim1""]), data.min(dim=""dim1""))\n\n        for reduct, expected in [\n            (""dim2"", [""dim1"", ""dim3"", ""time""]),\n            ([""dim2"", ""time""], [""dim1"", ""dim3""]),\n            ((""dim2"", ""time""), [""dim1"", ""dim3""]),\n            ((), [""dim1"", ""dim2"", ""dim3"", ""time""]),\n        ]:\n            actual = list(data.min(dim=reduct).dims)\n            assert actual == expected\n\n        assert_equal(data.mean(dim=[]), data)\n\n    def test_reduce_coords(self):\n        # regression test for GH1470\n        data = xr.Dataset({""a"": (""x"", [1, 2, 3])}, coords={""b"": 4})\n        expected = xr.Dataset({""a"": 2}, coords={""b"": 4})\n        actual = data.mean(""x"")\n        assert_identical(actual, expected)\n\n        # should be consistent\n        actual = data[""a""].mean(""x"").to_dataset()\n        assert_identical(actual, expected)\n\n    def test_mean_uint_dtype(self):\n        data = xr.Dataset(\n            {\n                ""a"": ((""x"", ""y""), np.arange(6).reshape(3, 2).astype(""uint"")),\n                ""b"": ((""x"",), np.array([0.1, 0.2, np.nan])),\n            }\n        )\n        actual = data.mean(""x"", skipna=True)\n        expected = xr.Dataset(\n            {""a"": data[""a""].mean(""x""), ""b"": data[""b""].mean(""x"", skipna=True)}\n        )\n        assert_identical(actual, expected)\n\n    def test_reduce_bad_dim(self):\n        data = create_test_data()\n        with raises_regex(ValueError, ""Dataset does not contain""):\n            data.mean(dim=""bad_dim"")\n\n    def test_reduce_cumsum(self):\n        data = xr.Dataset(\n            {""a"": 1, ""b"": (""x"", [1, 2]), ""c"": ((""x"", ""y""), [[np.nan, 3], [0, 4]])}\n        )\n        assert_identical(data.fillna(0), data.cumsum(""y""))\n\n        expected = xr.Dataset(\n            {""a"": 1, ""b"": (""x"", [1, 3]), ""c"": ((""x"", ""y""), [[0, 3], [0, 7]])}\n        )\n        assert_identical(expected, data.cumsum())\n\n    def test_reduce_cumsum_test_dims(self):\n        data = create_test_data()\n        for cumfunc in [""cumsum"", ""cumprod""]:\n            with raises_regex(ValueError, ""Dataset does not contain""):\n                getattr(data, cumfunc)(dim=""bad_dim"")\n\n            # ensure dimensions are correct\n            for reduct, expected in [\n                (""dim1"", [""dim1"", ""dim2"", ""dim3"", ""time""]),\n                (""dim2"", [""dim1"", ""dim2"", ""dim3"", ""time""]),\n                (""dim3"", [""dim1"", ""dim2"", ""dim3"", ""time""]),\n                (""time"", [""dim1"", ""dim2"", ""dim3""]),\n            ]:\n                actual = getattr(data, cumfunc)(dim=reduct).dims\n                assert list(actual) == expected\n\n    def test_reduce_non_numeric(self):\n        data1 = create_test_data(seed=44)\n        data2 = create_test_data(seed=44)\n        add_vars = {""var4"": [""dim1"", ""dim2""]}\n        for v, dims in sorted(add_vars.items()):\n            size = tuple(data1.dims[d] for d in dims)\n            data = np.random.randint(0, 100, size=size).astype(np.str_)\n            data1[v] = (dims, data, {""foo"": ""variable""})\n\n        assert ""var4"" not in data1.mean()\n        assert_equal(data1.mean(), data2.mean())\n        assert_equal(data1.mean(dim=""dim1""), data2.mean(dim=""dim1""))\n\n    def test_reduce_strings(self):\n        expected = Dataset({""x"": ""a""})\n        ds = Dataset({""x"": (""y"", [""a"", ""b""])})\n        ds.coords[""y""] = [-10, 10]\n        actual = ds.min()\n        assert_identical(expected, actual)\n\n        expected = Dataset({""x"": ""b""})\n        actual = ds.max()\n        assert_identical(expected, actual)\n\n        expected = Dataset({""x"": 0})\n        actual = ds.argmin()\n        assert_identical(expected, actual)\n\n        expected = Dataset({""x"": 1})\n        actual = ds.argmax()\n        assert_identical(expected, actual)\n\n        expected = Dataset({""x"": -10})\n        actual = ds.idxmin()\n        assert_identical(expected, actual)\n\n        expected = Dataset({""x"": 10})\n        actual = ds.idxmax()\n        assert_identical(expected, actual)\n\n        expected = Dataset({""x"": b""a""})\n        ds = Dataset({""x"": (""y"", np.array([""a"", ""b""], ""S1""))})\n        actual = ds.min()\n        assert_identical(expected, actual)\n\n        expected = Dataset({""x"": ""a""})\n        ds = Dataset({""x"": (""y"", np.array([""a"", ""b""], ""U1""))})\n        actual = ds.min()\n        assert_identical(expected, actual)\n\n    def test_reduce_dtypes(self):\n        # regression test for GH342\n        expected = Dataset({""x"": 1})\n        actual = Dataset({""x"": True}).sum()\n        assert_identical(expected, actual)\n\n        # regression test for GH505\n        expected = Dataset({""x"": 3})\n        actual = Dataset({""x"": (""y"", np.array([1, 2], ""uint16""))}).sum()\n        assert_identical(expected, actual)\n\n        expected = Dataset({""x"": 1 + 1j})\n        actual = Dataset({""x"": (""y"", [1, 1j])}).sum()\n        assert_identical(expected, actual)\n\n    def test_reduce_keep_attrs(self):\n        data = create_test_data()\n        _attrs = {""attr1"": ""value1"", ""attr2"": 2929}\n\n        attrs = dict(_attrs)\n        data.attrs = attrs\n\n        # Test dropped attrs\n        ds = data.mean()\n        assert ds.attrs == {}\n        for v in ds.data_vars.values():\n            assert v.attrs == {}\n\n        # Test kept attrs\n        ds = data.mean(keep_attrs=True)\n        assert ds.attrs == attrs\n        for k, v in ds.data_vars.items():\n            assert v.attrs == data[k].attrs\n\n    def test_reduce_argmin(self):\n        # regression test for #205\n        ds = Dataset({""a"": (""x"", [0, 1])})\n        expected = Dataset({""a"": ([], 0)})\n        actual = ds.argmin()\n        assert_identical(expected, actual)\n\n        actual = ds.argmin(""x"")\n        assert_identical(expected, actual)\n\n    def test_reduce_scalars(self):\n        ds = Dataset({""x"": (""a"", [2, 2]), ""y"": 2, ""z"": (""b"", [2])})\n        expected = Dataset({""x"": 0, ""y"": 0, ""z"": 0})\n        actual = ds.var()\n        assert_identical(expected, actual)\n\n        expected = Dataset({""x"": 0, ""y"": 0, ""z"": (""b"", [0])})\n        actual = ds.var(""a"")\n        assert_identical(expected, actual)\n\n    def test_reduce_only_one_axis(self):\n        def mean_only_one_axis(x, axis):\n            if not isinstance(axis, integer_types):\n                raise TypeError(""non-integer axis"")\n            return x.mean(axis)\n\n        ds = Dataset({""a"": ([""x"", ""y""], [[0, 1, 2, 3, 4]])})\n        expected = Dataset({""a"": (""x"", [2])})\n        actual = ds.reduce(mean_only_one_axis, ""y"")\n        assert_identical(expected, actual)\n\n        with raises_regex(\n            TypeError, ""missing 1 required positional argument: "" ""\'axis\'""\n        ):\n            ds.reduce(mean_only_one_axis)\n\n        with raises_regex(TypeError, ""non-integer axis""):\n            ds.reduce(mean_only_one_axis, axis=[""x"", ""y""])\n\n    def test_reduce_no_axis(self):\n        def total_sum(x):\n            return np.sum(x.flatten())\n\n        ds = Dataset({""a"": ([""x"", ""y""], [[0, 1, 2, 3, 4]])})\n        expected = Dataset({""a"": ((), 10)})\n        actual = ds.reduce(total_sum)\n        assert_identical(expected, actual)\n\n        with raises_regex(TypeError, ""unexpected keyword argument \'axis\'""):\n            ds.reduce(total_sum, axis=0)\n\n        with raises_regex(TypeError, ""unexpected keyword argument \'axis\'""):\n            ds.reduce(total_sum, dim=""x"")\n\n    def test_reduce_keepdims(self):\n        ds = Dataset(\n            {""a"": ([""x"", ""y""], [[0, 1, 2, 3, 4]])},\n            coords={\n                ""y"": [0, 1, 2, 3, 4],\n                ""x"": [0],\n                ""lat"": ([""x"", ""y""], [[0, 1, 2, 3, 4]]),\n                ""c"": -999.0,\n            },\n        )\n\n        # Shape should match behaviour of numpy reductions with keepdims=True\n        # Coordinates involved in the reduction should be removed\n        actual = ds.mean(keepdims=True)\n        expected = Dataset(\n            {""a"": ([""x"", ""y""], np.mean(ds.a, keepdims=True))}, coords={""c"": ds.c}\n        )\n        assert_identical(expected, actual)\n\n        actual = ds.mean(""x"", keepdims=True)\n        expected = Dataset(\n            {""a"": ([""x"", ""y""], np.mean(ds.a, axis=0, keepdims=True))},\n            coords={""y"": ds.y, ""c"": ds.c},\n        )\n        assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(""skipna"", [True, False])\n    @pytest.mark.parametrize(""q"", [0.25, [0.50], [0.25, 0.75]])\n    def test_quantile(self, q, skipna):\n        ds = create_test_data(seed=123)\n\n        for dim in [None, ""dim1"", [""dim1""]]:\n            ds_quantile = ds.quantile(q, dim=dim, skipna=skipna)\n            if is_scalar(q):\n                assert ""quantile"" not in ds_quantile.dims\n            else:\n                assert ""quantile"" in ds_quantile.dims\n\n            for var, dar in ds.data_vars.items():\n                assert var in ds_quantile\n                assert_identical(\n                    ds_quantile[var], dar.quantile(q, dim=dim, skipna=skipna)\n                )\n        dim = [""dim1"", ""dim2""]\n        ds_quantile = ds.quantile(q, dim=dim, skipna=skipna)\n        assert ""dim3"" in ds_quantile.dims\n        assert all(d not in ds_quantile.dims for d in dim)\n\n    @pytest.mark.parametrize(""skipna"", [True, False])\n    def test_quantile_skipna(self, skipna):\n        q = 0.1\n        dim = ""time""\n        ds = Dataset({""a"": ([dim], np.arange(0, 11))})\n        ds = ds.where(ds >= 1)\n\n        result = ds.quantile(q=q, dim=dim, skipna=skipna)\n\n        value = 1.9 if skipna else np.nan\n        expected = Dataset({""a"": value}, coords={""quantile"": q})\n        assert_identical(result, expected)\n\n    @requires_bottleneck\n    def test_rank(self):\n        ds = create_test_data(seed=1234)\n        # only ds.var3 depends on dim3\n        z = ds.rank(""dim3"")\n        assert [""var3""] == list(z.data_vars)\n        # same as dataarray version\n        x = z.var3\n        y = ds.var3.rank(""dim3"")\n        assert_equal(x, y)\n        # coordinates stick\n        assert list(z.coords) == list(ds.coords)\n        assert list(x.coords) == list(y.coords)\n        # invalid dim\n        with raises_regex(ValueError, ""does not contain""):\n            x.rank(""invalid_dim"")\n\n    def test_count(self):\n        ds = Dataset({""x"": (""a"", [np.nan, 1]), ""y"": 0, ""z"": np.nan})\n        expected = Dataset({""x"": 1, ""y"": 1, ""z"": 0})\n        actual = ds.count()\n        assert_identical(expected, actual)\n\n    def test_map(self):\n        data = create_test_data()\n        data.attrs[""foo""] = ""bar""\n\n        assert_identical(data.map(np.mean), data.mean())\n\n        expected = data.mean(keep_attrs=True)\n        actual = data.map(lambda x: x.mean(keep_attrs=True), keep_attrs=True)\n        assert_identical(expected, actual)\n\n        assert_identical(data.map(lambda x: x, keep_attrs=True), data.drop_vars(""time""))\n\n        def scale(x, multiple=1):\n            return multiple * x\n\n        actual = data.map(scale, multiple=2)\n        assert_equal(actual[""var1""], 2 * data[""var1""])\n        assert_identical(actual[""numbers""], data[""numbers""])\n\n        actual = data.map(np.asarray)\n        expected = data.drop_vars(""time"")  # time is not used on a data var\n        assert_equal(expected, actual)\n\n    def test_apply_pending_deprecated_map(self):\n        data = create_test_data()\n        data.attrs[""foo""] = ""bar""\n\n        with pytest.warns(PendingDeprecationWarning):\n            assert_identical(data.apply(np.mean), data.mean())\n\n    def make_example_math_dataset(self):\n        variables = {\n            ""bar"": (""x"", np.arange(100, 400, 100)),\n            ""foo"": ((""x"", ""y""), 1.0 * np.arange(12).reshape(3, 4)),\n        }\n        coords = {""abc"": (""x"", [""a"", ""b"", ""c""]), ""y"": 10 * np.arange(4)}\n        ds = Dataset(variables, coords)\n        ds[""foo""][0, 0] = np.nan\n        return ds\n\n    def test_dataset_number_math(self):\n        ds = self.make_example_math_dataset()\n\n        assert_identical(ds, +ds)\n        assert_identical(ds, ds + 0)\n        assert_identical(ds, 0 + ds)\n        assert_identical(ds, ds + np.array(0))\n        assert_identical(ds, np.array(0) + ds)\n\n        actual = ds.copy(deep=True)\n        actual += 0\n        assert_identical(ds, actual)\n\n    def test_unary_ops(self):\n        ds = self.make_example_math_dataset()\n\n        assert_identical(ds.map(abs), abs(ds))\n        assert_identical(ds.map(lambda x: x + 4), ds + 4)\n\n        for func in [\n            lambda x: x.isnull(),\n            lambda x: x.round(),\n            lambda x: x.astype(int),\n        ]:\n            assert_identical(ds.map(func), func(ds))\n\n        assert_identical(ds.isnull(), ~ds.notnull())\n\n        # don\'t actually patch these methods in\n        with pytest.raises(AttributeError):\n            ds.item\n        with pytest.raises(AttributeError):\n            ds.searchsorted\n\n    def test_dataset_array_math(self):\n        ds = self.make_example_math_dataset()\n\n        expected = ds.map(lambda x: x - ds[""foo""])\n        assert_identical(expected, ds - ds[""foo""])\n        assert_identical(expected, -ds[""foo""] + ds)\n        assert_identical(expected, ds - ds[""foo""].variable)\n        assert_identical(expected, -ds[""foo""].variable + ds)\n        actual = ds.copy(deep=True)\n        actual -= ds[""foo""]\n        assert_identical(expected, actual)\n\n        expected = ds.map(lambda x: x + ds[""bar""])\n        assert_identical(expected, ds + ds[""bar""])\n        actual = ds.copy(deep=True)\n        actual += ds[""bar""]\n        assert_identical(expected, actual)\n\n        expected = Dataset({""bar"": ds[""bar""] + np.arange(3)})\n        assert_identical(expected, ds[[""bar""]] + np.arange(3))\n        assert_identical(expected, np.arange(3) + ds[[""bar""]])\n\n    def test_dataset_dataset_math(self):\n        ds = self.make_example_math_dataset()\n\n        assert_identical(ds, ds + 0 * ds)\n        assert_identical(ds, ds + {""foo"": 0, ""bar"": 0})\n\n        expected = ds.map(lambda x: 2 * x)\n        assert_identical(expected, 2 * ds)\n        assert_identical(expected, ds + ds)\n        assert_identical(expected, ds + ds.data_vars)\n        assert_identical(expected, ds + dict(ds.data_vars))\n\n        actual = ds.copy(deep=True)\n        expected_id = id(actual)\n        actual += ds\n        assert_identical(expected, actual)\n        assert expected_id == id(actual)\n\n        assert_identical(ds == ds, ds.notnull())\n\n        subsampled = ds.isel(y=slice(2))\n        expected = 2 * subsampled\n        assert_identical(expected, subsampled + ds)\n        assert_identical(expected, ds + subsampled)\n\n    def test_dataset_math_auto_align(self):\n        ds = self.make_example_math_dataset()\n        subset = ds.isel(y=[1, 3])\n        expected = 2 * subset\n        actual = ds + subset\n        assert_identical(expected, actual)\n\n        actual = ds.isel(y=slice(1)) + ds.isel(y=slice(1, None))\n        expected = 2 * ds.drop_sel(y=ds.y)\n        assert_equal(actual, expected)\n\n        actual = ds + ds[[""bar""]]\n        expected = (2 * ds[[""bar""]]).merge(ds.coords)\n        assert_identical(expected, actual)\n\n        assert_identical(ds + Dataset(), ds.coords.to_dataset())\n        assert_identical(Dataset() + Dataset(), Dataset())\n\n        ds2 = Dataset(coords={""bar"": 42})\n        assert_identical(ds + ds2, ds.coords.merge(ds2))\n\n        # maybe unary arithmetic with empty datasets should raise instead?\n        assert_identical(Dataset() + 1, Dataset())\n\n        actual = ds.copy(deep=True)\n        other = ds.isel(y=slice(2))\n        actual += other\n        expected = ds + other.reindex_like(ds)\n        assert_identical(expected, actual)\n\n    def test_dataset_math_errors(self):\n        ds = self.make_example_math_dataset()\n\n        with pytest.raises(TypeError):\n            ds[""foo""] += ds\n        with pytest.raises(TypeError):\n            ds[""foo""].variable += ds\n        with raises_regex(ValueError, ""must have the same""):\n            ds += ds[[""bar""]]\n\n        # verify we can rollback in-place operations if something goes wrong\n        # nb. inplace datetime64 math actually will work with an integer array\n        # but not floats thanks to numpy\'s inconsistent handling\n        other = DataArray(np.datetime64(""2000-01-01""), coords={""c"": 2})\n        actual = ds.copy(deep=True)\n        with pytest.raises(TypeError):\n            actual += other\n        assert_identical(actual, ds)\n\n    def test_dataset_transpose(self):\n        ds = Dataset(\n            {\n                ""a"": ((""x"", ""y""), np.random.randn(3, 4)),\n                ""b"": ((""y"", ""x""), np.random.randn(4, 3)),\n            },\n            coords={\n                ""x"": range(3),\n                ""y"": range(4),\n                ""xy"": ((""x"", ""y""), np.random.randn(3, 4)),\n            },\n        )\n\n        actual = ds.transpose()\n        expected = Dataset(\n            {""a"": ((""y"", ""x""), ds.a.values.T), ""b"": ((""x"", ""y""), ds.b.values.T)},\n            coords={\n                ""x"": ds.x.values,\n                ""y"": ds.y.values,\n                ""xy"": ((""y"", ""x""), ds.xy.values.T),\n            },\n        )\n        assert_identical(expected, actual)\n\n        actual = ds.transpose(...)\n        expected = ds\n        assert_identical(expected, actual)\n\n        actual = ds.transpose(""x"", ""y"")\n        expected = ds.map(lambda x: x.transpose(""x"", ""y"", transpose_coords=True))\n        assert_identical(expected, actual)\n\n        ds = create_test_data()\n        actual = ds.transpose()\n        for k in ds.variables:\n            assert actual[k].dims[::-1] == ds[k].dims\n\n        new_order = (""dim2"", ""dim3"", ""dim1"", ""time"")\n        actual = ds.transpose(*new_order)\n        for k in ds.variables:\n            expected_dims = tuple(d for d in new_order if d in ds[k].dims)\n            assert actual[k].dims == expected_dims\n\n        # same as above but with ellipsis\n        new_order = (""dim2"", ""dim3"", ""dim1"", ""time"")\n        actual = ds.transpose(""dim2"", ""dim3"", ...)\n        for k in ds.variables:\n            expected_dims = tuple(d for d in new_order if d in ds[k].dims)\n            assert actual[k].dims == expected_dims\n\n        with raises_regex(ValueError, ""permuted""):\n            ds.transpose(""dim1"", ""dim2"", ""dim3"")\n        with raises_regex(ValueError, ""permuted""):\n            ds.transpose(""dim1"", ""dim2"", ""dim3"", ""time"", ""extra_dim"")\n\n        assert ""T"" not in dir(ds)\n\n    def test_dataset_ellipsis_transpose_different_ordered_vars(self):\n        # https://github.com/pydata/xarray/issues/1081#issuecomment-544350457\n        ds = Dataset(\n            dict(\n                a=((""w"", ""x"", ""y"", ""z""), np.ones((2, 3, 4, 5))),\n                b=((""x"", ""w"", ""y"", ""z""), np.zeros((3, 2, 4, 5))),\n            )\n        )\n        result = ds.transpose(..., ""z"", ""y"")\n        assert list(result[""a""].dims) == list(""wxzy"")\n        assert list(result[""b""].dims) == list(""xwzy"")\n\n    def test_dataset_retains_period_index_on_transpose(self):\n\n        ds = create_test_data()\n        ds[""time""] = pd.period_range(""2000-01-01"", periods=20)\n\n        transposed = ds.transpose()\n\n        assert isinstance(transposed.time.to_index(), pd.PeriodIndex)\n\n    def test_dataset_diff_n1_simple(self):\n        ds = Dataset({""foo"": (""x"", [5, 5, 6, 6])})\n        actual = ds.diff(""x"")\n        expected = Dataset({""foo"": (""x"", [0, 1, 0])})\n        assert_equal(expected, actual)\n\n    def test_dataset_diff_n1_label(self):\n        ds = Dataset({""foo"": (""x"", [5, 5, 6, 6])}, {""x"": [0, 1, 2, 3]})\n        actual = ds.diff(""x"", label=""lower"")\n        expected = Dataset({""foo"": (""x"", [0, 1, 0])}, {""x"": [0, 1, 2]})\n        assert_equal(expected, actual)\n\n        actual = ds.diff(""x"", label=""upper"")\n        expected = Dataset({""foo"": (""x"", [0, 1, 0])}, {""x"": [1, 2, 3]})\n        assert_equal(expected, actual)\n\n    def test_dataset_diff_n1(self):\n        ds = create_test_data(seed=1)\n        actual = ds.diff(""dim2"")\n        expected = {}\n        expected[""var1""] = DataArray(\n            np.diff(ds[""var1""].values, axis=1),\n            {""dim2"": ds[""dim2""].values[1:]},\n            [""dim1"", ""dim2""],\n        )\n        expected[""var2""] = DataArray(\n            np.diff(ds[""var2""].values, axis=1),\n            {""dim2"": ds[""dim2""].values[1:]},\n            [""dim1"", ""dim2""],\n        )\n        expected[""var3""] = ds[""var3""]\n        expected = Dataset(expected, coords={""time"": ds[""time""].values})\n        expected.coords[""numbers""] = (""dim3"", ds[""numbers""].values)\n        assert_equal(expected, actual)\n\n    def test_dataset_diff_n2(self):\n        ds = create_test_data(seed=1)\n        actual = ds.diff(""dim2"", n=2)\n        expected = {}\n        expected[""var1""] = DataArray(\n            np.diff(ds[""var1""].values, axis=1, n=2),\n            {""dim2"": ds[""dim2""].values[2:]},\n            [""dim1"", ""dim2""],\n        )\n        expected[""var2""] = DataArray(\n            np.diff(ds[""var2""].values, axis=1, n=2),\n            {""dim2"": ds[""dim2""].values[2:]},\n            [""dim1"", ""dim2""],\n        )\n        expected[""var3""] = ds[""var3""]\n        expected = Dataset(expected, coords={""time"": ds[""time""].values})\n        expected.coords[""numbers""] = (""dim3"", ds[""numbers""].values)\n        assert_equal(expected, actual)\n\n    def test_dataset_diff_exception_n_neg(self):\n        ds = create_test_data(seed=1)\n        with raises_regex(ValueError, ""must be non-negative""):\n            ds.diff(""dim2"", n=-1)\n\n    def test_dataset_diff_exception_label_str(self):\n        ds = create_test_data(seed=1)\n        with raises_regex(ValueError, ""\'label\' argument has to""):\n            ds.diff(""dim2"", label=""raise_me"")\n\n    @pytest.mark.parametrize(""fill_value"", [dtypes.NA, 2, 2.0])\n    def test_shift(self, fill_value):\n        coords = {""bar"": (""x"", list(""abc"")), ""x"": [-4, 3, 2]}\n        attrs = {""meta"": ""data""}\n        ds = Dataset({""foo"": (""x"", [1, 2, 3])}, coords, attrs)\n        actual = ds.shift(x=1, fill_value=fill_value)\n        if fill_value == dtypes.NA:\n            # if we supply the default, we expect the missing value for a\n            # float array\n            fill_value = np.nan\n        expected = Dataset({""foo"": (""x"", [fill_value, 1, 2])}, coords, attrs)\n        assert_identical(expected, actual)\n\n        with raises_regex(ValueError, ""dimensions""):\n            ds.shift(foo=123)\n\n    def test_roll_coords(self):\n        coords = {""bar"": (""x"", list(""abc"")), ""x"": [-4, 3, 2]}\n        attrs = {""meta"": ""data""}\n        ds = Dataset({""foo"": (""x"", [1, 2, 3])}, coords, attrs)\n        actual = ds.roll(x=1, roll_coords=True)\n\n        ex_coords = {""bar"": (""x"", list(""cab"")), ""x"": [2, -4, 3]}\n        expected = Dataset({""foo"": (""x"", [3, 1, 2])}, ex_coords, attrs)\n        assert_identical(expected, actual)\n\n        with raises_regex(ValueError, ""dimensions""):\n            ds.roll(foo=123, roll_coords=True)\n\n    def test_roll_no_coords(self):\n        coords = {""bar"": (""x"", list(""abc"")), ""x"": [-4, 3, 2]}\n        attrs = {""meta"": ""data""}\n        ds = Dataset({""foo"": (""x"", [1, 2, 3])}, coords, attrs)\n        actual = ds.roll(x=1, roll_coords=False)\n\n        expected = Dataset({""foo"": (""x"", [3, 1, 2])}, coords, attrs)\n        assert_identical(expected, actual)\n\n        with raises_regex(ValueError, ""dimensions""):\n            ds.roll(abc=321, roll_coords=False)\n\n    def test_roll_coords_none(self):\n        coords = {""bar"": (""x"", list(""abc"")), ""x"": [-4, 3, 2]}\n        attrs = {""meta"": ""data""}\n        ds = Dataset({""foo"": (""x"", [1, 2, 3])}, coords, attrs)\n\n        with pytest.warns(FutureWarning):\n            actual = ds.roll(x=1, roll_coords=None)\n\n        ex_coords = {""bar"": (""x"", list(""cab"")), ""x"": [2, -4, 3]}\n        expected = Dataset({""foo"": (""x"", [3, 1, 2])}, ex_coords, attrs)\n        assert_identical(expected, actual)\n\n    def test_roll_multidim(self):\n        # regression test for 2445\n        arr = xr.DataArray(\n            [[1, 2, 3], [4, 5, 6]],\n            coords={""x"": range(3), ""y"": range(2)},\n            dims=(""y"", ""x""),\n        )\n        actual = arr.roll(x=1, roll_coords=True)\n        expected = xr.DataArray(\n            [[3, 1, 2], [6, 4, 5]], coords=[(""y"", [0, 1]), (""x"", [2, 0, 1])]\n        )\n        assert_identical(expected, actual)\n\n    def test_real_and_imag(self):\n        attrs = {""foo"": ""bar""}\n        ds = Dataset({""x"": ((), 1 + 2j, attrs)}, attrs=attrs)\n\n        expected_re = Dataset({""x"": ((), 1, attrs)}, attrs=attrs)\n        assert_identical(ds.real, expected_re)\n\n        expected_im = Dataset({""x"": ((), 2, attrs)}, attrs=attrs)\n        assert_identical(ds.imag, expected_im)\n\n    def test_setattr_raises(self):\n        ds = Dataset({}, coords={""scalar"": 1}, attrs={""foo"": ""bar""})\n        with raises_regex(AttributeError, ""cannot set attr""):\n            ds.scalar = 2\n        with raises_regex(AttributeError, ""cannot set attr""):\n            ds.foo = 2\n        with raises_regex(AttributeError, ""cannot set attr""):\n            ds.other = 2\n\n    def test_filter_by_attrs(self):\n        precip = dict(standard_name=""convective_precipitation_flux"")\n        temp0 = dict(standard_name=""air_potential_temperature"", height=""0 m"")\n        temp10 = dict(standard_name=""air_potential_temperature"", height=""10 m"")\n        ds = Dataset(\n            {\n                ""temperature_0"": ([""t""], [0], temp0),\n                ""temperature_10"": ([""t""], [0], temp10),\n                ""precipitation"": ([""t""], [0], precip),\n            },\n            coords={""time"": ([""t""], [0], dict(axis=""T"", long_name=""time_in_seconds""))},\n        )\n\n        # Test return empty Dataset.\n        ds.filter_by_attrs(standard_name=""invalid_standard_name"")\n        new_ds = ds.filter_by_attrs(standard_name=""invalid_standard_name"")\n        assert not bool(new_ds.data_vars)\n\n        # Test return one DataArray.\n        new_ds = ds.filter_by_attrs(standard_name=""convective_precipitation_flux"")\n        assert new_ds[""precipitation""].standard_name == ""convective_precipitation_flux""\n\n        assert_equal(new_ds[""precipitation""], ds[""precipitation""])\n\n        # Test filter coordinates\n        new_ds = ds.filter_by_attrs(long_name=""time_in_seconds"")\n        assert new_ds[""time""].long_name == ""time_in_seconds""\n        assert not bool(new_ds.data_vars)\n\n        # Test return more than one DataArray.\n        new_ds = ds.filter_by_attrs(standard_name=""air_potential_temperature"")\n        assert len(new_ds.data_vars) == 2\n        for var in new_ds.data_vars:\n            assert new_ds[var].standard_name == ""air_potential_temperature""\n\n        # Test callable.\n        new_ds = ds.filter_by_attrs(height=lambda v: v is not None)\n        assert len(new_ds.data_vars) == 2\n        for var in new_ds.data_vars:\n            assert new_ds[var].standard_name == ""air_potential_temperature""\n\n        new_ds = ds.filter_by_attrs(height=""10 m"")\n        assert len(new_ds.data_vars) == 1\n        for var in new_ds.data_vars:\n            assert new_ds[var].height == ""10 m""\n\n        # Test return empty Dataset due to conflicting filters\n        new_ds = ds.filter_by_attrs(\n            standard_name=""convective_precipitation_flux"", height=""0 m""\n        )\n        assert not bool(new_ds.data_vars)\n\n        # Test return one DataArray with two filter conditions\n        new_ds = ds.filter_by_attrs(\n            standard_name=""air_potential_temperature"", height=""0 m""\n        )\n        for var in new_ds.data_vars:\n            assert new_ds[var].standard_name == ""air_potential_temperature""\n            assert new_ds[var].height == ""0 m""\n            assert new_ds[var].height != ""10 m""\n\n        # Test return empty Dataset due to conflicting callables\n        new_ds = ds.filter_by_attrs(\n            standard_name=lambda v: False, height=lambda v: True\n        )\n        assert not bool(new_ds.data_vars)\n\n    def test_binary_op_propagate_indexes(self):\n        ds = Dataset(\n            {""d1"": DataArray([1, 2, 3], dims=[""x""], coords={""x"": [10, 20, 30]})}\n        )\n        expected = ds.indexes[""x""]\n        actual = (ds * 2).indexes[""x""]\n        assert expected is actual\n\n    def test_binary_op_join_setting(self):\n        # arithmetic_join applies to data array coordinates\n        missing_2 = xr.Dataset({""x"": [0, 1]})\n        missing_0 = xr.Dataset({""x"": [1, 2]})\n        with xr.set_options(arithmetic_join=""outer""):\n            actual = missing_2 + missing_0\n        expected = xr.Dataset({""x"": [0, 1, 2]})\n        assert_equal(actual, expected)\n\n        # arithmetic join also applies to data_vars\n        ds1 = xr.Dataset({""foo"": 1, ""bar"": 2})\n        ds2 = xr.Dataset({""bar"": 2, ""baz"": 3})\n        expected = xr.Dataset({""bar"": 4})  # default is inner joining\n        actual = ds1 + ds2\n        assert_equal(actual, expected)\n\n        with xr.set_options(arithmetic_join=""outer""):\n            expected = xr.Dataset({""foo"": np.nan, ""bar"": 4, ""baz"": np.nan})\n            actual = ds1 + ds2\n            assert_equal(actual, expected)\n\n        with xr.set_options(arithmetic_join=""left""):\n            expected = xr.Dataset({""foo"": np.nan, ""bar"": 4})\n            actual = ds1 + ds2\n            assert_equal(actual, expected)\n\n        with xr.set_options(arithmetic_join=""right""):\n            expected = xr.Dataset({""bar"": 4, ""baz"": np.nan})\n            actual = ds1 + ds2\n            assert_equal(actual, expected)\n\n    def test_full_like(self):\n        # For more thorough tests, see test_variable.py\n        # Note: testing data_vars with mismatched dtypes\n        ds = Dataset(\n            {\n                ""d1"": DataArray([1, 2, 3], dims=[""x""], coords={""x"": [10, 20, 30]}),\n                ""d2"": DataArray([1.1, 2.2, 3.3], dims=[""y""]),\n            },\n            attrs={""foo"": ""bar""},\n        )\n        actual = full_like(ds, 2)\n\n        expect = ds.copy(deep=True)\n        expect[""d1""].values = [2, 2, 2]\n        expect[""d2""].values = [2.0, 2.0, 2.0]\n        assert expect[""d1""].dtype == int\n        assert expect[""d2""].dtype == float\n        assert_identical(expect, actual)\n\n        # override dtype\n        actual = full_like(ds, fill_value=True, dtype=bool)\n        expect = ds.copy(deep=True)\n        expect[""d1""].values = [True, True, True]\n        expect[""d2""].values = [True, True, True]\n        assert expect[""d1""].dtype == bool\n        assert expect[""d2""].dtype == bool\n        assert_identical(expect, actual)\n\n    def test_combine_first(self):\n        dsx0 = DataArray([0, 0], [(""x"", [""a"", ""b""])]).to_dataset(name=""dsx0"")\n        dsx1 = DataArray([1, 1], [(""x"", [""b"", ""c""])]).to_dataset(name=""dsx1"")\n\n        actual = dsx0.combine_first(dsx1)\n        expected = Dataset(\n            {""dsx0"": (""x"", [0, 0, np.nan]), ""dsx1"": (""x"", [np.nan, 1, 1])},\n            coords={""x"": [""a"", ""b"", ""c""]},\n        )\n        assert_equal(actual, expected)\n        assert_equal(actual, xr.merge([dsx0, dsx1]))\n\n        # works just like xr.merge([self, other])\n        dsy2 = DataArray([2, 2, 2], [(""x"", [""b"", ""c"", ""d""])]).to_dataset(name=""dsy2"")\n        actual = dsx0.combine_first(dsy2)\n        expected = xr.merge([dsy2, dsx0])\n        assert_equal(actual, expected)\n\n    def test_sortby(self):\n        ds = Dataset(\n            {\n                ""A"": DataArray(\n                    [[1, 2], [3, 4], [5, 6]], [(""x"", [""c"", ""b"", ""a""]), (""y"", [1, 0])]\n                ),\n                ""B"": DataArray([[5, 6], [7, 8], [9, 10]], dims=[""x"", ""y""]),\n            }\n        )\n\n        sorted1d = Dataset(\n            {\n                ""A"": DataArray(\n                    [[5, 6], [3, 4], [1, 2]], [(""x"", [""a"", ""b"", ""c""]), (""y"", [1, 0])]\n                ),\n                ""B"": DataArray([[9, 10], [7, 8], [5, 6]], dims=[""x"", ""y""]),\n            }\n        )\n\n        sorted2d = Dataset(\n            {\n                ""A"": DataArray(\n                    [[6, 5], [4, 3], [2, 1]], [(""x"", [""a"", ""b"", ""c""]), (""y"", [0, 1])]\n                ),\n                ""B"": DataArray([[10, 9], [8, 7], [6, 5]], dims=[""x"", ""y""]),\n            }\n        )\n\n        expected = sorted1d\n        dax = DataArray([100, 99, 98], [(""x"", [""c"", ""b"", ""a""])])\n        actual = ds.sortby(dax)\n        assert_equal(actual, expected)\n\n        # test descending order sort\n        actual = ds.sortby(dax, ascending=False)\n        assert_equal(actual, ds)\n\n        # test alignment (fills in nan for \'c\')\n        dax_short = DataArray([98, 97], [(""x"", [""b"", ""a""])])\n        actual = ds.sortby(dax_short)\n        assert_equal(actual, expected)\n\n        # test 1-D lexsort\n        # dax0 is sorted first to give indices of [1, 2, 0]\n        # and then dax1 would be used to move index 2 ahead of 1\n        dax0 = DataArray([100, 95, 95], [(""x"", [""c"", ""b"", ""a""])])\n        dax1 = DataArray([0, 1, 0], [(""x"", [""c"", ""b"", ""a""])])\n        actual = ds.sortby([dax0, dax1])  # lexsort underneath gives [2, 1, 0]\n        assert_equal(actual, expected)\n\n        expected = sorted2d\n        # test multi-dim sort by 1D dataarray values\n        day = DataArray([90, 80], [(""y"", [1, 0])])\n        actual = ds.sortby([day, dax])\n        assert_equal(actual, expected)\n\n        # test exception-raising\n        with pytest.raises(KeyError) as excinfo:\n            actual = ds.sortby(""z"")\n\n        with pytest.raises(ValueError) as excinfo:\n            actual = ds.sortby(ds[""A""])\n        assert ""DataArray is not 1-D"" in str(excinfo.value)\n\n        expected = sorted1d\n        actual = ds.sortby(""x"")\n        assert_equal(actual, expected)\n\n        # test pandas.MultiIndex\n        indices = ((""b"", 1), (""b"", 0), (""a"", 1), (""a"", 0))\n        midx = pd.MultiIndex.from_tuples(indices, names=[""one"", ""two""])\n        ds_midx = Dataset(\n            {\n                ""A"": DataArray(\n                    [[1, 2], [3, 4], [5, 6], [7, 8]], [(""x"", midx), (""y"", [1, 0])]\n                ),\n                ""B"": DataArray([[5, 6], [7, 8], [9, 10], [11, 12]], dims=[""x"", ""y""]),\n            }\n        )\n        actual = ds_midx.sortby(""x"")\n        midx_reversed = pd.MultiIndex.from_tuples(\n            tuple(reversed(indices)), names=[""one"", ""two""]\n        )\n        expected = Dataset(\n            {\n                ""A"": DataArray(\n                    [[7, 8], [5, 6], [3, 4], [1, 2]],\n                    [(""x"", midx_reversed), (""y"", [1, 0])],\n                ),\n                ""B"": DataArray([[11, 12], [9, 10], [7, 8], [5, 6]], dims=[""x"", ""y""]),\n            }\n        )\n        assert_equal(actual, expected)\n\n        # multi-dim sort by coordinate objects\n        expected = sorted2d\n        actual = ds.sortby([""x"", ""y""])\n        assert_equal(actual, expected)\n\n        # test descending order sort\n        actual = ds.sortby([""x"", ""y""], ascending=False)\n        assert_equal(actual, ds)\n\n    def test_attribute_access(self):\n        ds = create_test_data(seed=1)\n        for key in [""var1"", ""var2"", ""var3"", ""time"", ""dim1"", ""dim2"", ""dim3"", ""numbers""]:\n            assert_equal(ds[key], getattr(ds, key))\n            assert key in dir(ds)\n\n        for key in [""dim3"", ""dim1"", ""numbers""]:\n            assert_equal(ds[""var3""][key], getattr(ds.var3, key))\n            assert key in dir(ds[""var3""])\n        # attrs\n        assert ds[""var3""].attrs[""foo""] == ds.var3.foo\n        assert ""foo"" in dir(ds[""var3""])\n\n    def test_ipython_key_completion(self):\n        ds = create_test_data(seed=1)\n        actual = ds._ipython_key_completions_()\n        expected = [""var1"", ""var2"", ""var3"", ""time"", ""dim1"", ""dim2"", ""dim3"", ""numbers""]\n        for item in actual:\n            ds[item]  # should not raise\n        assert sorted(actual) == sorted(expected)\n\n        # for dataarray\n        actual = ds[""var3""]._ipython_key_completions_()\n        expected = [""dim3"", ""dim1"", ""numbers""]\n        for item in actual:\n            ds[""var3""][item]  # should not raise\n        assert sorted(actual) == sorted(expected)\n\n        # MultiIndex\n        ds_midx = ds.stack(dim12=[""dim1"", ""dim2""])\n        actual = ds_midx._ipython_key_completions_()\n        expected = [\n            ""var1"",\n            ""var2"",\n            ""var3"",\n            ""time"",\n            ""dim1"",\n            ""dim2"",\n            ""dim3"",\n            ""numbers"",\n            ""dim12"",\n        ]\n        for item in actual:\n            ds_midx[item]  # should not raise\n        assert sorted(actual) == sorted(expected)\n\n        # coords\n        actual = ds.coords._ipython_key_completions_()\n        expected = [""time"", ""dim1"", ""dim2"", ""dim3"", ""numbers""]\n        for item in actual:\n            ds.coords[item]  # should not raise\n        assert sorted(actual) == sorted(expected)\n\n        actual = ds[""var3""].coords._ipython_key_completions_()\n        expected = [""dim1"", ""dim3"", ""numbers""]\n        for item in actual:\n            ds[""var3""].coords[item]  # should not raise\n        assert sorted(actual) == sorted(expected)\n\n        # data_vars\n        actual = ds.data_vars._ipython_key_completions_()\n        expected = [""var1"", ""var2"", ""var3"", ""dim1""]\n        for item in actual:\n            ds.data_vars[item]  # should not raise\n        assert sorted(actual) == sorted(expected)\n\n    def test_polyfit_output(self):\n        ds = create_test_data(seed=1)\n\n        out = ds.polyfit(""dim2"", 2, full=False)\n        assert ""var1_polyfit_coefficients"" in out\n\n        out = ds.polyfit(""dim1"", 2, full=True)\n        assert ""var1_polyfit_coefficients"" in out\n        assert ""dim1_matrix_rank"" in out\n\n        out = ds.polyfit(""time"", 2)\n        assert len(out.data_vars) == 0\n\n    def test_pad(self):\n        ds = create_test_data(seed=1)\n        padded = ds.pad(dim2=(1, 1), constant_values=42)\n\n        assert padded[""dim2""].shape == (11,)\n        assert padded[""var1""].shape == (8, 11)\n        assert padded[""var2""].shape == (8, 11)\n        assert padded[""var3""].shape == (10, 8)\n        assert dict(padded.dims) == {""dim1"": 8, ""dim2"": 11, ""dim3"": 10, ""time"": 20}\n\n        np.testing.assert_equal(padded[""var1""].isel(dim2=[0, -1]).data, 42)\n        np.testing.assert_equal(padded[""dim2""][[0, -1]].data, np.nan)\n\n\n# Py.test tests\n\n\n@pytest.fixture(params=[None])\ndef data_set(request):\n    return create_test_data(request.param)\n\n\n@pytest.mark.parametrize(""test_elements"", ([1, 2], np.array([1, 2]), DataArray([1, 2])))\ndef test_isin(test_elements):\n    expected = Dataset(\n        data_vars={\n            ""var1"": ((""dim1"",), [0, 1]),\n            ""var2"": ((""dim1"",), [1, 1]),\n            ""var3"": ((""dim1"",), [0, 1]),\n        }\n    ).astype(""bool"")\n\n    result = Dataset(\n        data_vars={\n            ""var1"": ((""dim1"",), [0, 1]),\n            ""var2"": ((""dim1"",), [1, 2]),\n            ""var3"": ((""dim1"",), [0, 1]),\n        }\n    ).isin(test_elements)\n\n    assert_equal(result, expected)\n\n\n@pytest.mark.skipif(not has_dask, reason=""requires dask"")\n@pytest.mark.parametrize(""test_elements"", ([1, 2], np.array([1, 2]), DataArray([1, 2])))\ndef test_isin_dask(test_elements):\n    expected = Dataset(\n        data_vars={\n            ""var1"": ((""dim1"",), [0, 1]),\n            ""var2"": ((""dim1"",), [1, 1]),\n            ""var3"": ((""dim1"",), [0, 1]),\n        }\n    ).astype(""bool"")\n\n    result = (\n        Dataset(\n            data_vars={\n                ""var1"": ((""dim1"",), [0, 1]),\n                ""var2"": ((""dim1"",), [1, 2]),\n                ""var3"": ((""dim1"",), [0, 1]),\n            }\n        )\n        .chunk(1)\n        .isin(test_elements)\n        .compute()\n    )\n\n    assert_equal(result, expected)\n\n\ndef test_isin_dataset():\n    ds = Dataset({""x"": [1, 2]})\n    with pytest.raises(TypeError):\n        ds.isin(ds)\n\n\n@pytest.mark.parametrize(\n    ""unaligned_coords"",\n    (\n        {""x"": [2, 1, 0]},\n        {""x"": ([""x""], np.asarray([2, 1, 0]))},\n        {""x"": ([""x""], np.asarray([1, 2, 0]))},\n        {""x"": pd.Index([2, 1, 0])},\n        {""x"": Variable(dims=""x"", data=[0, 2, 1])},\n        {""x"": IndexVariable(dims=""x"", data=[0, 1, 2])},\n        {""y"": 42},\n        {""y"": (""x"", [2, 1, 0])},\n        {""y"": (""x"", np.asarray([2, 1, 0]))},\n        {""y"": ([""x""], np.asarray([2, 1, 0]))},\n    ),\n)\n@pytest.mark.parametrize(""coords"", ({""x"": (""x"", [0, 1, 2])}, {""x"": [0, 1, 2]}))\ndef test_dataset_constructor_aligns_to_explicit_coords(unaligned_coords, coords):\n\n    a = xr.DataArray([1, 2, 3], dims=[""x""], coords=unaligned_coords)\n\n    expected = xr.Dataset(coords=coords)\n    expected[""a""] = a\n\n    result = xr.Dataset({""a"": a}, coords=coords)\n\n    assert_equal(expected, result)\n\n\ndef test_error_message_on_set_supplied():\n    with pytest.raises(TypeError, match=""has invalid type <class \'set\'>""):\n        xr.Dataset(dict(date=[1, 2, 3], sec={4}))\n\n\n@pytest.mark.parametrize(""unaligned_coords"", ({""y"": (""b"", np.asarray([2, 1, 0]))},))\ndef test_constructor_raises_with_invalid_coords(unaligned_coords):\n\n    with pytest.raises(ValueError, match=""not a subset of the DataArray dimensions""):\n        xr.DataArray([1, 2, 3], dims=[""x""], coords=unaligned_coords)\n\n\ndef test_dir_expected_attrs(data_set):\n\n    some_expected_attrs = {""pipe"", ""mean"", ""isnull"", ""var1"", ""dim2"", ""numbers""}\n    result = dir(data_set)\n    assert set(result) >= some_expected_attrs\n\n\ndef test_dir_non_string(data_set):\n    # add a numbered key to ensure this doesn\'t break dir\n    data_set[5] = ""foo""\n    result = dir(data_set)\n    assert 5 not in result\n\n    # GH2172\n    sample_data = np.random.uniform(size=[2, 2000, 10000])\n    x = xr.Dataset({""sample_data"": (sample_data.shape, sample_data)})\n    x2 = x[""sample_data""]\n    dir(x2)\n\n\ndef test_dir_unicode(data_set):\n    data_set[""unicode""] = ""uni""\n    result = dir(data_set)\n    assert ""unicode"" in result\n\n\n@pytest.fixture(params=[1])\ndef ds(request):\n    if request.param == 1:\n        return Dataset(\n            {\n                ""z1"": ([""y"", ""x""], np.random.randn(2, 8)),\n                ""z2"": ([""time"", ""y""], np.random.randn(10, 2)),\n            },\n            {\n                ""x"": (""x"", np.linspace(0, 1.0, 8)),\n                ""time"": (""time"", np.linspace(0, 1.0, 10)),\n                ""c"": (""y"", [""a"", ""b""]),\n                ""y"": range(2),\n            },\n        )\n\n    if request.param == 2:\n        return Dataset(\n            {\n                ""z1"": ([""time"", ""y""], np.random.randn(10, 2)),\n                ""z2"": ([""time""], np.random.randn(10)),\n                ""z3"": ([""x"", ""time""], np.random.randn(8, 10)),\n            },\n            {\n                ""x"": (""x"", np.linspace(0, 1.0, 8)),\n                ""time"": (""time"", np.linspace(0, 1.0, 10)),\n                ""c"": (""y"", [""a"", ""b""]),\n                ""y"": range(2),\n            },\n        )\n\n\ndef test_coarsen_absent_dims_error(ds):\n    with raises_regex(ValueError, ""not found in Dataset.""):\n        ds.coarsen(foo=2)\n\n\n@pytest.mark.parametrize(""dask"", [True, False])\n@pytest.mark.parametrize((""boundary"", ""side""), [(""trim"", ""left""), (""pad"", ""right"")])\ndef test_coarsen(ds, dask, boundary, side):\n    if dask and has_dask:\n        ds = ds.chunk({""x"": 4})\n\n    actual = ds.coarsen(time=2, x=3, boundary=boundary, side=side).max()\n    assert_equal(\n        actual[""z1""], ds[""z1""].coarsen(x=3, boundary=boundary, side=side).max()\n    )\n    # coordinate should be mean by default\n    assert_equal(\n        actual[""time""], ds[""time""].coarsen(time=2, boundary=boundary, side=side).mean()\n    )\n\n\n@pytest.mark.parametrize(""dask"", [True, False])\ndef test_coarsen_coords(ds, dask):\n    if dask and has_dask:\n        ds = ds.chunk({""x"": 4})\n\n    # check if coord_func works\n    actual = ds.coarsen(time=2, x=3, boundary=""trim"", coord_func={""time"": ""max""}).max()\n    assert_equal(actual[""z1""], ds[""z1""].coarsen(x=3, boundary=""trim"").max())\n    assert_equal(actual[""time""], ds[""time""].coarsen(time=2, boundary=""trim"").max())\n\n    # raise if exact\n    with pytest.raises(ValueError):\n        ds.coarsen(x=3).mean()\n    # should be no error\n    ds.isel(x=slice(0, 3 * (len(ds[""x""]) // 3))).coarsen(x=3).mean()\n\n    # working test with pd.time\n    da = xr.DataArray(\n        np.linspace(0, 365, num=364),\n        dims=""time"",\n        coords={""time"": pd.date_range(""15/12/1999"", periods=364)},\n    )\n    actual = da.coarsen(time=2).mean()\n\n\n@requires_cftime\ndef test_coarsen_coords_cftime():\n    times = xr.cftime_range(""2000"", periods=6)\n    da = xr.DataArray(range(6), [(""time"", times)])\n    actual = da.coarsen(time=3).mean()\n    expected_times = xr.cftime_range(""2000-01-02"", freq=""3D"", periods=2)\n    np.testing.assert_array_equal(actual.time, expected_times)\n\n\ndef test_coarsen_keep_attrs():\n    _attrs = {""units"": ""test"", ""long_name"": ""testing""}\n\n    var1 = np.linspace(10, 15, 100)\n    var2 = np.linspace(5, 10, 100)\n    coords = np.linspace(1, 10, 100)\n\n    ds = Dataset(\n        data_vars={""var1"": (""coord"", var1), ""var2"": (""coord"", var2)},\n        coords={""coord"": coords},\n        attrs=_attrs,\n    )\n\n    # Test dropped attrs\n    dat = ds.coarsen(coord=5).mean()\n    assert dat.attrs == {}\n\n    # Test kept attrs using dataset keyword\n    dat = ds.coarsen(coord=5, keep_attrs=True).mean()\n    assert dat.attrs == _attrs\n\n    # Test kept attrs using global option\n    with set_options(keep_attrs=True):\n        dat = ds.coarsen(coord=5).mean()\n    assert dat.attrs == _attrs\n\n\ndef test_rolling_keep_attrs():\n    _attrs = {""units"": ""test"", ""long_name"": ""testing""}\n\n    var1 = np.linspace(10, 15, 100)\n    var2 = np.linspace(5, 10, 100)\n    coords = np.linspace(1, 10, 100)\n\n    ds = Dataset(\n        data_vars={""var1"": (""coord"", var1), ""var2"": (""coord"", var2)},\n        coords={""coord"": coords},\n        attrs=_attrs,\n    )\n\n    # Test dropped attrs\n    dat = ds.rolling(dim={""coord"": 5}, min_periods=None, center=False).mean()\n    assert dat.attrs == {}\n\n    # Test kept attrs using dataset keyword\n    dat = ds.rolling(\n        dim={""coord"": 5}, min_periods=None, center=False, keep_attrs=True\n    ).mean()\n    assert dat.attrs == _attrs\n\n    # Test kept attrs using global option\n    with set_options(keep_attrs=True):\n        dat = ds.rolling(dim={""coord"": 5}, min_periods=None, center=False).mean()\n    assert dat.attrs == _attrs\n\n\ndef test_rolling_properties(ds):\n    # catching invalid args\n    with pytest.raises(ValueError, match=""exactly one dim/window should""):\n        ds.rolling(time=7, x=2)\n    with pytest.raises(ValueError, match=""window must be > 0""):\n        ds.rolling(time=-2)\n    with pytest.raises(ValueError, match=""min_periods must be greater than zero""):\n        ds.rolling(time=2, min_periods=0)\n    with pytest.raises(KeyError, match=""time2""):\n        ds.rolling(time2=2)\n\n\n@pytest.mark.parametrize(""name"", (""sum"", ""mean"", ""std"", ""var"", ""min"", ""max"", ""median""))\n@pytest.mark.parametrize(""center"", (True, False, None))\n@pytest.mark.parametrize(""min_periods"", (1, None))\n@pytest.mark.parametrize(""key"", (""z1"", ""z2""))\ndef test_rolling_wrapped_bottleneck(ds, name, center, min_periods, key):\n    bn = pytest.importorskip(""bottleneck"", minversion=""1.1"")\n\n    # Test all bottleneck functions\n    rolling_obj = ds.rolling(time=7, min_periods=min_periods)\n\n    func_name = f""move_{name}""\n    actual = getattr(rolling_obj, name)()\n    if key == ""z1"":  # z1 does not depend on \'Time\' axis. Stored as it is.\n        expected = ds[key]\n    elif key == ""z2"":\n        expected = getattr(bn, func_name)(\n            ds[key].values, window=7, axis=0, min_count=min_periods\n        )\n    assert_array_equal(actual[key].values, expected)\n\n    # Test center\n    rolling_obj = ds.rolling(time=7, center=center)\n    actual = getattr(rolling_obj, name)()[""time""]\n    assert_equal(actual, ds[""time""])\n\n\n@requires_numbagg\ndef test_rolling_exp(ds):\n\n    result = ds.rolling_exp(time=10, window_type=""span"").mean()\n    assert isinstance(result, Dataset)\n\n\n@pytest.mark.parametrize(""center"", (True, False))\n@pytest.mark.parametrize(""min_periods"", (None, 1, 2, 3))\n@pytest.mark.parametrize(""window"", (1, 2, 3, 4))\ndef test_rolling_pandas_compat(center, window, min_periods):\n    df = pd.DataFrame(\n        {\n            ""x"": np.random.randn(20),\n            ""y"": np.random.randn(20),\n            ""time"": np.linspace(0, 1, 20),\n        }\n    )\n    ds = Dataset.from_dataframe(df)\n\n    if min_periods is not None and window < min_periods:\n        min_periods = window\n\n    df_rolling = df.rolling(window, center=center, min_periods=min_periods).mean()\n    ds_rolling = ds.rolling(index=window, center=center, min_periods=min_periods).mean()\n\n    np.testing.assert_allclose(df_rolling[""x""].values, ds_rolling[""x""].values)\n    np.testing.assert_allclose(df_rolling.index, ds_rolling[""index""])\n\n\n@pytest.mark.parametrize(""center"", (True, False))\n@pytest.mark.parametrize(""window"", (1, 2, 3, 4))\ndef test_rolling_construct(center, window):\n    df = pd.DataFrame(\n        {\n            ""x"": np.random.randn(20),\n            ""y"": np.random.randn(20),\n            ""time"": np.linspace(0, 1, 20),\n        }\n    )\n\n    ds = Dataset.from_dataframe(df)\n    df_rolling = df.rolling(window, center=center, min_periods=1).mean()\n    ds_rolling = ds.rolling(index=window, center=center)\n\n    ds_rolling_mean = ds_rolling.construct(""window"").mean(""window"")\n    np.testing.assert_allclose(df_rolling[""x""].values, ds_rolling_mean[""x""].values)\n    np.testing.assert_allclose(df_rolling.index, ds_rolling_mean[""index""])\n\n    # with stride\n    ds_rolling_mean = ds_rolling.construct(""window"", stride=2).mean(""window"")\n    np.testing.assert_allclose(df_rolling[""x""][::2].values, ds_rolling_mean[""x""].values)\n    np.testing.assert_allclose(df_rolling.index[::2], ds_rolling_mean[""index""])\n    # with fill_value\n    ds_rolling_mean = ds_rolling.construct(""window"", stride=2, fill_value=0.0).mean(\n        ""window""\n    )\n    assert (ds_rolling_mean.isnull().sum() == 0).to_array(dim=""vars"").all()\n    assert (ds_rolling_mean[""x""] == 0.0).sum() >= 0\n\n\n@pytest.mark.slow\n@pytest.mark.parametrize(""ds"", (1, 2), indirect=True)\n@pytest.mark.parametrize(""center"", (True, False))\n@pytest.mark.parametrize(""min_periods"", (None, 1, 2, 3))\n@pytest.mark.parametrize(""window"", (1, 2, 3, 4))\n@pytest.mark.parametrize(""name"", (""sum"", ""mean"", ""std"", ""var"", ""min"", ""max"", ""median""))\ndef test_rolling_reduce(ds, center, min_periods, window, name):\n\n    if min_periods is not None and window < min_periods:\n        min_periods = window\n\n    if name == ""std"" and window == 1:\n        pytest.skip(""std with window == 1 is unstable in bottleneck"")\n\n    rolling_obj = ds.rolling(time=window, center=center, min_periods=min_periods)\n\n    # add nan prefix to numpy methods to get similar behavior as bottleneck\n    actual = rolling_obj.reduce(getattr(np, ""nan%s"" % name))\n    expected = getattr(rolling_obj, name)()\n    assert_allclose(actual, expected)\n    assert ds.dims == actual.dims\n    # make sure the order of data_var are not changed.\n    assert list(ds.data_vars.keys()) == list(actual.data_vars.keys())\n\n    # Make sure the dimension order is restored\n    for key, src_var in ds.data_vars.items():\n        assert src_var.dims == actual[key].dims\n\n\ndef test_raise_no_warning_for_nan_in_binary_ops():\n    with pytest.warns(None) as record:\n        Dataset(data_vars={""x"": (""y"", [1, 2, np.NaN])}) > 0\n    assert len(record) == 0\n\n\n@pytest.mark.parametrize(""dask"", [True, False])\n@pytest.mark.parametrize(""edge_order"", [1, 2])\ndef test_differentiate(dask, edge_order):\n    rs = np.random.RandomState(42)\n    coord = [0.2, 0.35, 0.4, 0.6, 0.7, 0.75, 0.76, 0.8]\n\n    da = xr.DataArray(\n        rs.randn(8, 6),\n        dims=[""x"", ""y""],\n        coords={""x"": coord, ""z"": 3, ""x2d"": ((""x"", ""y""), rs.randn(8, 6))},\n    )\n    if dask and has_dask:\n        da = da.chunk({""x"": 4})\n\n    ds = xr.Dataset({""var"": da})\n\n    # along x\n    actual = da.differentiate(""x"", edge_order)\n    expected_x = xr.DataArray(\n        np.gradient(da, da[""x""], axis=0, edge_order=edge_order),\n        dims=da.dims,\n        coords=da.coords,\n    )\n    assert_equal(expected_x, actual)\n    assert_equal(\n        ds[""var""].differentiate(""x"", edge_order=edge_order),\n        ds.differentiate(""x"", edge_order=edge_order)[""var""],\n    )\n    # coordinate should not change\n    assert_equal(da[""x""], actual[""x""])\n\n    # along y\n    actual = da.differentiate(""y"", edge_order)\n    expected_y = xr.DataArray(\n        np.gradient(da, da[""y""], axis=1, edge_order=edge_order),\n        dims=da.dims,\n        coords=da.coords,\n    )\n    assert_equal(expected_y, actual)\n    assert_equal(actual, ds.differentiate(""y"", edge_order=edge_order)[""var""])\n    assert_equal(\n        ds[""var""].differentiate(""y"", edge_order=edge_order),\n        ds.differentiate(""y"", edge_order=edge_order)[""var""],\n    )\n\n    with pytest.raises(ValueError):\n        da.differentiate(""x2d"")\n\n\n@pytest.mark.parametrize(""dask"", [True, False])\ndef test_differentiate_datetime(dask):\n    rs = np.random.RandomState(42)\n    coord = np.array(\n        [\n            ""2004-07-13"",\n            ""2006-01-13"",\n            ""2010-08-13"",\n            ""2010-09-13"",\n            ""2010-10-11"",\n            ""2010-12-13"",\n            ""2011-02-13"",\n            ""2012-08-13"",\n        ],\n        dtype=""datetime64"",\n    )\n\n    da = xr.DataArray(\n        rs.randn(8, 6),\n        dims=[""x"", ""y""],\n        coords={""x"": coord, ""z"": 3, ""x2d"": ((""x"", ""y""), rs.randn(8, 6))},\n    )\n    if dask and has_dask:\n        da = da.chunk({""x"": 4})\n\n    # along x\n    actual = da.differentiate(""x"", edge_order=1, datetime_unit=""D"")\n    expected_x = xr.DataArray(\n        np.gradient(\n            da, da[""x""].variable._to_numeric(datetime_unit=""D""), axis=0, edge_order=1\n        ),\n        dims=da.dims,\n        coords=da.coords,\n    )\n    assert_equal(expected_x, actual)\n\n    actual2 = da.differentiate(""x"", edge_order=1, datetime_unit=""h"")\n    assert np.allclose(actual, actual2 * 24)\n\n    # for datetime variable\n    actual = da[""x""].differentiate(""x"", edge_order=1, datetime_unit=""D"")\n    assert np.allclose(actual, 1.0)\n\n    # with different date unit\n    da = xr.DataArray(coord.astype(""datetime64[ms]""), dims=[""x""], coords={""x"": coord})\n    actual = da.differentiate(""x"", edge_order=1)\n    assert np.allclose(actual, 1.0)\n\n\n@pytest.mark.skipif(not has_cftime, reason=""Test requires cftime."")\n@pytest.mark.parametrize(""dask"", [True, False])\ndef test_differentiate_cftime(dask):\n    rs = np.random.RandomState(42)\n    coord = xr.cftime_range(""2000"", periods=8, freq=""2M"")\n\n    da = xr.DataArray(\n        rs.randn(8, 6),\n        coords={""time"": coord, ""z"": 3, ""t2d"": ((""time"", ""y""), rs.randn(8, 6))},\n        dims=[""time"", ""y""],\n    )\n\n    if dask and has_dask:\n        da = da.chunk({""time"": 4})\n\n    actual = da.differentiate(""time"", edge_order=1, datetime_unit=""D"")\n    expected_data = np.gradient(\n        da, da[""time""].variable._to_numeric(datetime_unit=""D""), axis=0, edge_order=1\n    )\n    expected = xr.DataArray(expected_data, coords=da.coords, dims=da.dims)\n    assert_equal(expected, actual)\n\n    actual2 = da.differentiate(""time"", edge_order=1, datetime_unit=""h"")\n    assert_allclose(actual, actual2 * 24)\n\n    # Test the differentiation of datetimes themselves\n    actual = da[""time""].differentiate(""time"", edge_order=1, datetime_unit=""D"")\n    assert_allclose(actual, xr.ones_like(da[""time""]).astype(float))\n\n\n@pytest.mark.parametrize(""dask"", [True, False])\ndef test_integrate(dask):\n    rs = np.random.RandomState(42)\n    coord = [0.2, 0.35, 0.4, 0.6, 0.7, 0.75, 0.76, 0.8]\n\n    da = xr.DataArray(\n        rs.randn(8, 6),\n        dims=[""x"", ""y""],\n        coords={\n            ""x"": coord,\n            ""x2"": ((""x"",), rs.randn(8)),\n            ""z"": 3,\n            ""x2d"": ((""x"", ""y""), rs.randn(8, 6)),\n        },\n    )\n    if dask and has_dask:\n        da = da.chunk({""x"": 4})\n\n    ds = xr.Dataset({""var"": da})\n\n    # along x\n    actual = da.integrate(""x"")\n    # coordinate that contains x should be dropped.\n    expected_x = xr.DataArray(\n        np.trapz(da.compute(), da[""x""], axis=0),\n        dims=[""y""],\n        coords={k: v for k, v in da.coords.items() if ""x"" not in v.dims},\n    )\n    assert_allclose(expected_x, actual.compute())\n    assert_equal(ds[""var""].integrate(""x""), ds.integrate(""x"")[""var""])\n\n    # make sure result is also a dask array (if the source is dask array)\n    assert isinstance(actual.data, type(da.data))\n\n    # along y\n    actual = da.integrate(""y"")\n    expected_y = xr.DataArray(\n        np.trapz(da, da[""y""], axis=1),\n        dims=[""x""],\n        coords={k: v for k, v in da.coords.items() if ""y"" not in v.dims},\n    )\n    assert_allclose(expected_y, actual.compute())\n    assert_equal(actual, ds.integrate(""y"")[""var""])\n    assert_equal(ds[""var""].integrate(""y""), ds.integrate(""y"")[""var""])\n\n    # along x and y\n    actual = da.integrate((""y"", ""x""))\n    assert actual.ndim == 0\n\n    with pytest.raises(ValueError):\n        da.integrate(""x2d"")\n\n\n@pytest.mark.parametrize(""dask"", [True, False])\n@pytest.mark.parametrize(""which_datetime"", [""np"", ""cftime""])\ndef test_trapz_datetime(dask, which_datetime):\n    rs = np.random.RandomState(42)\n    if which_datetime == ""np"":\n        coord = np.array(\n            [\n                ""2004-07-13"",\n                ""2006-01-13"",\n                ""2010-08-13"",\n                ""2010-09-13"",\n                ""2010-10-11"",\n                ""2010-12-13"",\n                ""2011-02-13"",\n                ""2012-08-13"",\n            ],\n            dtype=""datetime64"",\n        )\n    else:\n        if not has_cftime:\n            pytest.skip(""Test requires cftime."")\n        coord = xr.cftime_range(""2000"", periods=8, freq=""2D"")\n\n    da = xr.DataArray(\n        rs.randn(8, 6),\n        coords={""time"": coord, ""z"": 3, ""t2d"": ((""time"", ""y""), rs.randn(8, 6))},\n        dims=[""time"", ""y""],\n    )\n\n    if dask and has_dask:\n        da = da.chunk({""time"": 4})\n\n    actual = da.integrate(""time"", datetime_unit=""D"")\n    expected_data = np.trapz(\n        da.data,\n        duck_array_ops.datetime_to_numeric(da[""time""].data, datetime_unit=""D""),\n        axis=0,\n    )\n    expected = xr.DataArray(\n        expected_data,\n        dims=[""y""],\n        coords={k: v for k, v in da.coords.items() if ""time"" not in v.dims},\n    )\n    assert_allclose(expected, actual.compute())\n\n    # make sure result is also a dask array (if the source is dask array)\n    assert isinstance(actual.data, type(da.data))\n\n    actual2 = da.integrate(""time"", datetime_unit=""h"")\n    assert_allclose(actual, actual2 / 24.0)\n\n\ndef test_no_dict():\n    d = Dataset()\n    with pytest.raises(AttributeError):\n        d.__dict__\n\n\ndef test_subclass_slots():\n    """"""Test that Dataset subclasses must explicitly define ``__slots__``.\n\n    .. note::\n       As of 0.13.0, this is actually mitigated into a FutureWarning for any class\n       defined outside of the xarray package.\n    """"""\n    with pytest.raises(AttributeError) as e:\n\n        class MyDS(Dataset):\n            pass\n\n    assert str(e.value) == ""MyDS must explicitly define __slots__""\n\n\ndef test_weakref():\n    """"""Classes with __slots__ are incompatible with the weakref module unless they\n    explicitly state __weakref__ among their slots\n    """"""\n    from weakref import ref\n\n    ds = Dataset()\n    r = ref(ds)\n    assert r() is ds\n'"
xarray/tests/test_distributed.py,0,"b'"""""" isort:skip_file """"""\nimport pickle\n\nimport pytest\n\ndask = pytest.importorskip(""dask"")  # isort:skip\ndistributed = pytest.importorskip(""distributed"")  # isort:skip\n\nfrom dask.distributed import Client, Lock\nfrom distributed.utils_test import cluster, gen_cluster\nfrom distributed.utils_test import loop\nfrom distributed.client import futures_of\n\nimport xarray as xr\nfrom xarray.backends.locks import HDF5_LOCK, CombinedLock\nfrom xarray.tests.test_backends import (\n    ON_WINDOWS,\n    create_tmp_file,\n    create_tmp_geotiff,\n    open_example_dataset,\n)\nfrom xarray.tests.test_dataset import create_test_data\n\nfrom . import (\n    assert_allclose,\n    has_h5netcdf,\n    has_netCDF4,\n    requires_rasterio,\n    has_scipy,\n    requires_zarr,\n    requires_cfgrib,\n)\n\n# this is to stop isort throwing errors. May have been easier to just use\n# `isort:skip` in retrospect\n\n\nda = pytest.importorskip(""dask.array"")\nloop = loop  # loop is an imported fixture, which flake8 has issues ack-ing\n\n\n@pytest.fixture\ndef tmp_netcdf_filename(tmpdir):\n    return str(tmpdir.join(""testfile.nc""))\n\n\nENGINES = []\nif has_scipy:\n    ENGINES.append(""scipy"")\nif has_netCDF4:\n    ENGINES.append(""netcdf4"")\nif has_h5netcdf:\n    ENGINES.append(""h5netcdf"")\n\nNC_FORMATS = {\n    ""netcdf4"": [\n        ""NETCDF3_CLASSIC"",\n        ""NETCDF3_64BIT_OFFSET"",\n        ""NETCDF3_64BIT_DATA"",\n        ""NETCDF4_CLASSIC"",\n        ""NETCDF4"",\n    ],\n    ""scipy"": [""NETCDF3_CLASSIC"", ""NETCDF3_64BIT""],\n    ""h5netcdf"": [""NETCDF4""],\n}\n\nENGINES_AND_FORMATS = [\n    (""netcdf4"", ""NETCDF3_CLASSIC""),\n    (""netcdf4"", ""NETCDF4_CLASSIC""),\n    (""netcdf4"", ""NETCDF4""),\n    (""h5netcdf"", ""NETCDF4""),\n    (""scipy"", ""NETCDF3_64BIT""),\n]\n\n\n@pytest.mark.parametrize(""engine,nc_format"", ENGINES_AND_FORMATS)\ndef test_dask_distributed_netcdf_roundtrip(\n    loop, tmp_netcdf_filename, engine, nc_format\n):\n\n    if engine not in ENGINES:\n        pytest.skip(""engine not available"")\n\n    chunks = {""dim1"": 4, ""dim2"": 3, ""dim3"": 6}\n\n    with cluster() as (s, [a, b]):\n        with Client(s[""address""], loop=loop):\n\n            original = create_test_data().chunk(chunks)\n\n            if engine == ""scipy"":\n                with pytest.raises(NotImplementedError):\n                    original.to_netcdf(\n                        tmp_netcdf_filename, engine=engine, format=nc_format\n                    )\n                return\n\n            original.to_netcdf(tmp_netcdf_filename, engine=engine, format=nc_format)\n\n            with xr.open_dataset(\n                tmp_netcdf_filename, chunks=chunks, engine=engine\n            ) as restored:\n                assert isinstance(restored.var1.data, da.Array)\n                computed = restored.compute()\n                assert_allclose(original, computed)\n\n\n@pytest.mark.parametrize(""engine,nc_format"", ENGINES_AND_FORMATS)\ndef test_dask_distributed_read_netcdf_integration_test(\n    loop, tmp_netcdf_filename, engine, nc_format\n):\n\n    if engine not in ENGINES:\n        pytest.skip(""engine not available"")\n\n    chunks = {""dim1"": 4, ""dim2"": 3, ""dim3"": 6}\n\n    with cluster() as (s, [a, b]):\n        with Client(s[""address""], loop=loop):\n\n            original = create_test_data()\n            original.to_netcdf(tmp_netcdf_filename, engine=engine, format=nc_format)\n\n            with xr.open_dataset(\n                tmp_netcdf_filename, chunks=chunks, engine=engine\n            ) as restored:\n                assert isinstance(restored.var1.data, da.Array)\n                computed = restored.compute()\n                assert_allclose(original, computed)\n\n\n@requires_zarr\n@pytest.mark.parametrize(""consolidated"", [True, False])\n@pytest.mark.parametrize(""compute"", [True, False])\ndef test_dask_distributed_zarr_integration_test(loop, consolidated, compute):\n    if consolidated:\n        pytest.importorskip(""zarr"", minversion=""2.2.1.dev2"")\n        write_kwargs = dict(consolidated=True)\n        read_kwargs = dict(consolidated=True)\n    else:\n        write_kwargs = read_kwargs = {}\n    chunks = {""dim1"": 4, ""dim2"": 3, ""dim3"": 5}\n    with cluster() as (s, [a, b]):\n        with Client(s[""address""], loop=loop):\n            original = create_test_data().chunk(chunks)\n            with create_tmp_file(\n                allow_cleanup_failure=ON_WINDOWS, suffix="".zarrc""\n            ) as filename:\n                maybe_futures = original.to_zarr(\n                    filename, compute=compute, **write_kwargs\n                )\n                if not compute:\n                    maybe_futures.compute()\n                with xr.open_zarr(filename, **read_kwargs) as restored:\n                    assert isinstance(restored.var1.data, da.Array)\n                    computed = restored.compute()\n                    assert_allclose(original, computed)\n\n\n@requires_rasterio\ndef test_dask_distributed_rasterio_integration_test(loop):\n    with create_tmp_geotiff() as (tmp_file, expected):\n        with cluster() as (s, [a, b]):\n            with Client(s[""address""], loop=loop):\n                da_tiff = xr.open_rasterio(tmp_file, chunks={""band"": 1})\n                assert isinstance(da_tiff.data, da.Array)\n                actual = da_tiff.compute()\n                assert_allclose(actual, expected)\n\n\n@requires_cfgrib\ndef test_dask_distributed_cfgrib_integration_test(loop):\n    with cluster() as (s, [a, b]):\n        with Client(s[""address""], loop=loop):\n            with open_example_dataset(\n                ""example.grib"", engine=""cfgrib"", chunks={""time"": 1}\n            ) as ds:\n                with open_example_dataset(""example.grib"", engine=""cfgrib"") as expected:\n                    assert isinstance(ds[""t""].data, da.Array)\n                    actual = ds.compute()\n                    assert_allclose(actual, expected)\n\n\n@pytest.mark.skipif(\n    distributed.__version__ <= ""1.19.3"",\n    reason=""Need recent distributed version to clean up get"",\n)\n@gen_cluster(client=True, timeout=None)\nasync def test_async(c, s, a, b):\n    x = create_test_data()\n    assert not dask.is_dask_collection(x)\n    y = x.chunk({""dim2"": 4}) + 10\n    assert dask.is_dask_collection(y)\n    assert dask.is_dask_collection(y.var1)\n    assert dask.is_dask_collection(y.var2)\n\n    z = y.persist()\n    assert str(z)\n\n    assert dask.is_dask_collection(z)\n    assert dask.is_dask_collection(z.var1)\n    assert dask.is_dask_collection(z.var2)\n    assert len(y.__dask_graph__()) > len(z.__dask_graph__())\n\n    assert not futures_of(y)\n    assert futures_of(z)\n\n    future = c.compute(z)\n    w = await future\n    assert not dask.is_dask_collection(w)\n    assert_allclose(x + 10, w)\n\n    assert s.tasks\n\n\ndef test_hdf5_lock():\n    assert isinstance(HDF5_LOCK, dask.utils.SerializableLock)\n\n\n@gen_cluster(client=True)\nasync def test_serializable_locks(c, s, a, b):\n    def f(x, lock=None):\n        with lock:\n            return x + 1\n\n    # note, the creation of Lock needs to be done inside a cluster\n    for lock in [\n        HDF5_LOCK,\n        Lock(),\n        Lock(""filename.nc""),\n        CombinedLock([HDF5_LOCK]),\n        CombinedLock([HDF5_LOCK, Lock(""filename.nc"")]),\n    ]:\n\n        futures = c.map(f, list(range(10)), lock=lock)\n        await c.gather(futures)\n\n        lock2 = pickle.loads(pickle.dumps(lock))\n        assert type(lock) == type(lock2)\n'"
xarray/tests/test_dtypes.py,42,"b'import numpy as np\nimport pytest\n\nfrom xarray.core import dtypes\n\n\n@pytest.mark.parametrize(\n    ""args, expected"",\n    [\n        ([np.bool], np.bool),\n        ([np.bool, np.string_], np.object_),\n        ([np.float32, np.float64], np.float64),\n        ([np.float32, np.string_], np.object_),\n        ([np.unicode_, np.int64], np.object_),\n        ([np.unicode_, np.unicode_], np.unicode_),\n        ([np.bytes_, np.unicode_], np.object_),\n    ],\n)\ndef test_result_type(args, expected):\n    actual = dtypes.result_type(*args)\n    assert actual == expected\n\n\ndef test_result_type_scalar():\n    actual = dtypes.result_type(np.arange(3, dtype=np.float32), np.nan)\n    assert actual == np.float32\n\n\ndef test_result_type_dask_array():\n    # verify it works without evaluating dask arrays\n    da = pytest.importorskip(""dask.array"")\n    dask = pytest.importorskip(""dask"")\n\n    def error():\n        raise RuntimeError\n\n    array = da.from_delayed(dask.delayed(error)(), (), np.float64)\n    with pytest.raises(RuntimeError):\n        array.compute()\n\n    actual = dtypes.result_type(array)\n    assert actual == np.float64\n\n    # note that this differs from the behavior for scalar numpy arrays, which\n    # would get promoted to float32\n    actual = dtypes.result_type(array, np.array([0.5, 1.0], dtype=np.float32))\n    assert actual == np.float64\n\n\n@pytest.mark.parametrize(""obj"", [1.0, np.inf, ""ab"", 1.0 + 1.0j, True])\ndef test_inf(obj):\n    assert dtypes.INF > obj\n    assert dtypes.NINF < obj\n\n\n@pytest.mark.parametrize(\n    ""kind, expected"",\n    [\n        (""a"", (np.dtype(""O""), ""nan"")),  # dtype(\'S\')\n        (""b"", (np.float32, ""nan"")),  # dtype(\'int8\')\n        (""B"", (np.float32, ""nan"")),  # dtype(\'uint8\')\n        (""c"", (np.dtype(""O""), ""nan"")),  # dtype(\'S1\')\n        (""D"", (np.complex128, ""(nan+nanj)"")),  # dtype(\'complex128\')\n        (""d"", (np.float64, ""nan"")),  # dtype(\'float64\')\n        (""e"", (np.float16, ""nan"")),  # dtype(\'float16\')\n        (""F"", (np.complex64, ""(nan+nanj)"")),  # dtype(\'complex64\')\n        (""f"", (np.float32, ""nan"")),  # dtype(\'float32\')\n        (""h"", (np.float32, ""nan"")),  # dtype(\'int16\')\n        (""H"", (np.float32, ""nan"")),  # dtype(\'uint16\')\n        (""i"", (np.float64, ""nan"")),  # dtype(\'int32\')\n        (""I"", (np.float64, ""nan"")),  # dtype(\'uint32\')\n        (""l"", (np.float64, ""nan"")),  # dtype(\'int64\')\n        (""L"", (np.float64, ""nan"")),  # dtype(\'uint64\')\n        (""m"", (np.timedelta64, ""NaT"")),  # dtype(\'<m8\')\n        (""M"", (np.datetime64, ""NaT"")),  # dtype(\'<M8\')\n        (""O"", (np.dtype(""O""), ""nan"")),  # dtype(\'O\')\n        (""p"", (np.float64, ""nan"")),  # dtype(\'int64\')\n        (""P"", (np.float64, ""nan"")),  # dtype(\'uint64\')\n        (""q"", (np.float64, ""nan"")),  # dtype(\'int64\')\n        (""Q"", (np.float64, ""nan"")),  # dtype(\'uint64\')\n        (""S"", (np.dtype(""O""), ""nan"")),  # dtype(\'S\')\n        (""U"", (np.dtype(""O""), ""nan"")),  # dtype(\'<U\')\n        (""V"", (np.dtype(""O""), ""nan"")),  # dtype(\'V\')\n    ],\n)\ndef test_maybe_promote(kind, expected):\n    # \'g\': np.float128 is not tested : not available on all platforms\n    # \'G\': np.complex256 is not tested : not available on all platforms\n\n    actual = dtypes.maybe_promote(np.dtype(kind))\n    assert actual[0] == expected[0]\n    assert str(actual[1]) == expected[1]\n'"
xarray/tests/test_duck_array_ops.py,94,"b'import datetime as dt\nimport warnings\nfrom textwrap import dedent\n\nimport numpy as np\nimport pandas as pd\nimport pytest\nfrom numpy import array, nan\n\nfrom xarray import DataArray, Dataset, cftime_range, concat\nfrom xarray.core import dtypes, duck_array_ops\nfrom xarray.core.duck_array_ops import (\n    array_notnull_equiv,\n    concatenate,\n    count,\n    first,\n    gradient,\n    last,\n    least_squares,\n    mean,\n    np_timedelta64_to_float,\n    pd_timedelta_to_float,\n    py_timedelta_to_float,\n    rolling_window,\n    stack,\n    timedelta_to_numeric,\n    where,\n)\nfrom xarray.core.pycompat import dask_array_type\nfrom xarray.testing import assert_allclose, assert_equal\n\nfrom . import (\n    arm_xfail,\n    assert_array_equal,\n    has_dask,\n    raises_regex,\n    requires_cftime,\n    requires_dask,\n)\n\n\nclass TestOps:\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        self.x = array(\n            [\n                [[nan, nan, 2.0, nan], [nan, 5.0, 6.0, nan], [8.0, 9.0, 10.0, nan]],\n                [\n                    [nan, 13.0, 14.0, 15.0],\n                    [nan, 17.0, 18.0, nan],\n                    [nan, 21.0, nan, nan],\n                ],\n            ]\n        )\n\n    def test_first(self):\n        expected_results = [\n            array([[nan, 13, 2, 15], [nan, 5, 6, nan], [8, 9, 10, nan]]),\n            array([[8, 5, 2, nan], [nan, 13, 14, 15]]),\n            array([[2, 5, 8], [13, 17, 21]]),\n        ]\n        for axis, expected in zip([0, 1, 2, -3, -2, -1], 2 * expected_results):\n            actual = first(self.x, axis)\n            assert_array_equal(expected, actual)\n\n        expected = self.x[0]\n        actual = first(self.x, axis=0, skipna=False)\n        assert_array_equal(expected, actual)\n\n        expected = self.x[..., 0]\n        actual = first(self.x, axis=-1, skipna=False)\n        assert_array_equal(expected, actual)\n\n        with raises_regex(IndexError, ""out of bounds""):\n            first(self.x, 3)\n\n    def test_last(self):\n        expected_results = [\n            array([[nan, 13, 14, 15], [nan, 17, 18, nan], [8, 21, 10, nan]]),\n            array([[8, 9, 10, nan], [nan, 21, 18, 15]]),\n            array([[2, 6, 10], [15, 18, 21]]),\n        ]\n        for axis, expected in zip([0, 1, 2, -3, -2, -1], 2 * expected_results):\n            actual = last(self.x, axis)\n            assert_array_equal(expected, actual)\n\n        expected = self.x[-1]\n        actual = last(self.x, axis=0, skipna=False)\n        assert_array_equal(expected, actual)\n\n        expected = self.x[..., -1]\n        actual = last(self.x, axis=-1, skipna=False)\n        assert_array_equal(expected, actual)\n\n        with raises_regex(IndexError, ""out of bounds""):\n            last(self.x, 3)\n\n    def test_count(self):\n        assert 12 == count(self.x)\n\n        expected = array([[1, 2, 3], [3, 2, 1]])\n        assert_array_equal(expected, count(self.x, axis=-1))\n\n        assert 1 == count(np.datetime64(""2000-01-01""))\n\n    def test_where_type_promotion(self):\n        result = where([True, False], [1, 2], [""a"", ""b""])\n        assert_array_equal(result, np.array([1, ""b""], dtype=object))\n\n        result = where([True, False], np.array([1, 2], np.float32), np.nan)\n        assert result.dtype == np.float32\n        assert_array_equal(result, np.array([1, np.nan], dtype=np.float32))\n\n    def test_stack_type_promotion(self):\n        result = stack([1, ""b""])\n        assert_array_equal(result, np.array([1, ""b""], dtype=object))\n\n    def test_concatenate_type_promotion(self):\n        result = concatenate([[1], [""b""]])\n        assert_array_equal(result, np.array([1, ""b""], dtype=object))\n\n    def test_all_nan_arrays(self):\n        with warnings.catch_warnings():\n            warnings.filterwarnings(""ignore"", ""All-NaN slice"")\n            warnings.filterwarnings(""ignore"", ""Mean of empty slice"")\n            assert np.isnan(mean([np.nan, np.nan]))\n\n\ndef test_cumsum_1d():\n    inputs = np.array([0, 1, 2, 3])\n    expected = np.array([0, 1, 3, 6])\n    actual = duck_array_ops.cumsum(inputs)\n    assert_array_equal(expected, actual)\n\n    actual = duck_array_ops.cumsum(inputs, axis=0)\n    assert_array_equal(expected, actual)\n\n    actual = duck_array_ops.cumsum(inputs, axis=-1)\n    assert_array_equal(expected, actual)\n\n    actual = duck_array_ops.cumsum(inputs, axis=(0,))\n    assert_array_equal(expected, actual)\n\n    actual = duck_array_ops.cumsum(inputs, axis=())\n    assert_array_equal(inputs, actual)\n\n\ndef test_cumsum_2d():\n    inputs = np.array([[1, 2], [3, 4]])\n\n    expected = np.array([[1, 3], [4, 10]])\n    actual = duck_array_ops.cumsum(inputs)\n    assert_array_equal(expected, actual)\n\n    actual = duck_array_ops.cumsum(inputs, axis=(0, 1))\n    assert_array_equal(expected, actual)\n\n    actual = duck_array_ops.cumsum(inputs, axis=())\n    assert_array_equal(inputs, actual)\n\n\ndef test_cumprod_2d():\n    inputs = np.array([[1, 2], [3, 4]])\n\n    expected = np.array([[1, 2], [3, 2 * 3 * 4]])\n    actual = duck_array_ops.cumprod(inputs)\n    assert_array_equal(expected, actual)\n\n    actual = duck_array_ops.cumprod(inputs, axis=(0, 1))\n    assert_array_equal(expected, actual)\n\n    actual = duck_array_ops.cumprod(inputs, axis=())\n    assert_array_equal(inputs, actual)\n\n\nclass TestArrayNotNullEquiv:\n    @pytest.mark.parametrize(\n        ""arr1, arr2"",\n        [\n            (np.array([1, 2, 3]), np.array([1, 2, 3])),\n            (np.array([1, 2, np.nan]), np.array([1, np.nan, 3])),\n            (np.array([np.nan, 2, np.nan]), np.array([1, np.nan, np.nan])),\n        ],\n    )\n    def test_equal(self, arr1, arr2):\n        assert array_notnull_equiv(arr1, arr2)\n\n    def test_some_not_equal(self):\n        a = np.array([1, 2, 4])\n        b = np.array([1, np.nan, 3])\n        assert not array_notnull_equiv(a, b)\n\n    def test_wrong_shape(self):\n        a = np.array([[1, np.nan, np.nan, 4]])\n        b = np.array([[1, 2], [np.nan, 4]])\n        assert not array_notnull_equiv(a, b)\n\n    @pytest.mark.parametrize(\n        ""val1, val2, val3, null"",\n        [\n            (\n                np.datetime64(""2000""),\n                np.datetime64(""2001""),\n                np.datetime64(""2002""),\n                np.datetime64(""NaT""),\n            ),\n            (1.0, 2.0, 3.0, np.nan),\n            (""foo"", ""bar"", ""baz"", None),\n            (""foo"", ""bar"", ""baz"", np.nan),\n        ],\n    )\n    def test_types(self, val1, val2, val3, null):\n        dtype = object if isinstance(val1, str) else None\n        arr1 = np.array([val1, null, val3, null], dtype=dtype)\n        arr2 = np.array([val1, val2, null, null], dtype=dtype)\n        assert array_notnull_equiv(arr1, arr2)\n\n\ndef construct_dataarray(dim_num, dtype, contains_nan, dask):\n    # dimnum <= 3\n    rng = np.random.RandomState(0)\n    shapes = [16, 8, 4][:dim_num]\n    dims = (""x"", ""y"", ""z"")[:dim_num]\n\n    if np.issubdtype(dtype, np.floating):\n        array = rng.randn(*shapes).astype(dtype)\n    elif np.issubdtype(dtype, np.integer):\n        array = rng.randint(0, 10, size=shapes).astype(dtype)\n    elif np.issubdtype(dtype, np.bool_):\n        array = rng.randint(0, 1, size=shapes).astype(dtype)\n    elif dtype == str:\n        array = rng.choice([""a"", ""b"", ""c"", ""d""], size=shapes)\n    else:\n        raise ValueError\n\n    if contains_nan:\n        inds = rng.choice(range(array.size), int(array.size * 0.2))\n        dtype, fill_value = dtypes.maybe_promote(array.dtype)\n        array = array.astype(dtype)\n        array.flat[inds] = fill_value\n\n    da = DataArray(array, dims=dims, coords={""x"": np.arange(16)}, name=""da"")\n\n    if dask and has_dask:\n        chunks = {d: 4 for d in dims}\n        da = da.chunk(chunks)\n\n    return da\n\n\ndef from_series_or_scalar(se):\n    if isinstance(se, pd.Series):\n        return DataArray.from_series(se)\n    else:  # scalar case\n        return DataArray(se)\n\n\ndef series_reduce(da, func, dim, **kwargs):\n    """""" convert DataArray to pd.Series, apply pd.func, then convert back to\n    a DataArray. Multiple dims cannot be specified.""""""\n    if dim is None or da.ndim == 1:\n        se = da.to_series()\n        return from_series_or_scalar(getattr(se, func)(**kwargs))\n    else:\n        da1 = []\n        dims = list(da.dims)\n        dims.remove(dim)\n        d = dims[0]\n        for i in range(len(da[d])):\n            da1.append(series_reduce(da.isel(**{d: i}), func, dim, **kwargs))\n\n        if d in da.coords:\n            return concat(da1, dim=da[d])\n        return concat(da1, dim=d)\n\n\ndef assert_dask_array(da, dask):\n    if dask and da.ndim > 0:\n        assert isinstance(da.data, dask_array_type)\n\n\n@arm_xfail\n@pytest.mark.filterwarnings(""ignore::RuntimeWarning"")\n@pytest.mark.parametrize(""dask"", [False, True] if has_dask else [False])\ndef test_datetime_mean(dask):\n    # Note: only testing numpy, as dask is broken upstream\n    da = DataArray(\n        np.array([""2010-01-01"", ""NaT"", ""2010-01-03"", ""NaT"", ""NaT""], dtype=""M8""),\n        dims=[""time""],\n    )\n    if dask:\n        # Trigger use case where a chunk is full of NaT\n        da = da.chunk({""time"": 3})\n\n    expect = DataArray(np.array(""2010-01-02"", dtype=""M8""))\n    expect_nat = DataArray(np.array(""NaT"", dtype=""M8""))\n\n    actual = da.mean()\n    if dask:\n        assert actual.chunks is not None\n    assert_equal(actual, expect)\n\n    actual = da.mean(skipna=False)\n    if dask:\n        assert actual.chunks is not None\n    assert_equal(actual, expect_nat)\n\n    # tests for 1d array full of NaT\n    assert_equal(da[[1]].mean(), expect_nat)\n    assert_equal(da[[1]].mean(skipna=False), expect_nat)\n\n    # tests for a 0d array\n    assert_equal(da[0].mean(), da[0])\n    assert_equal(da[0].mean(skipna=False), da[0])\n    assert_equal(da[1].mean(), expect_nat)\n    assert_equal(da[1].mean(skipna=False), expect_nat)\n\n\n@requires_cftime\ndef test_cftime_datetime_mean():\n    times = cftime_range(""2000"", periods=4)\n    da = DataArray(times, dims=[""time""])\n\n    assert da.isel(time=0).mean() == da.isel(time=0)\n\n    expected = DataArray(times.date_type(2000, 1, 2, 12))\n    result = da.mean()\n    assert_equal(result, expected)\n\n    da_2d = DataArray(times.values.reshape(2, 2))\n    result = da_2d.mean()\n    assert_equal(result, expected)\n\n\n@requires_cftime\n@requires_dask\ndef test_cftime_datetime_mean_dask_error():\n    times = cftime_range(""2000"", periods=4)\n    da = DataArray(times, dims=[""time""]).chunk()\n    with pytest.raises(NotImplementedError):\n        da.mean()\n\n\n@pytest.mark.parametrize(""dim_num"", [1, 2])\n@pytest.mark.parametrize(""dtype"", [float, int, np.float32, np.bool_])\n@pytest.mark.parametrize(""dask"", [False, True])\n@pytest.mark.parametrize(""func"", [""sum"", ""min"", ""max"", ""mean"", ""var""])\n# TODO test cumsum, cumprod\n@pytest.mark.parametrize(""skipna"", [False, True])\n@pytest.mark.parametrize(""aggdim"", [None, ""x""])\ndef test_reduce(dim_num, dtype, dask, func, skipna, aggdim):\n\n    if aggdim == ""y"" and dim_num < 2:\n        pytest.skip(""dim not in this test"")\n\n    if dtype == np.bool_ and func == ""mean"":\n        pytest.skip(""numpy does not support this"")\n\n    if dask and not has_dask:\n        pytest.skip(""requires dask"")\n\n    if dask and skipna is False and dtype in [np.bool_]:\n        pytest.skip(""dask does not compute object-typed array"")\n\n    rtol = 1e-04 if dtype == np.float32 else 1e-05\n\n    da = construct_dataarray(dim_num, dtype, contains_nan=True, dask=dask)\n    axis = None if aggdim is None else da.get_axis_num(aggdim)\n\n    # TODO: remove these after resolving\n    # https://github.com/dask/dask/issues/3245\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", ""Mean of empty slice"")\n        warnings.filterwarnings(""ignore"", ""All-NaN slice"")\n        warnings.filterwarnings(""ignore"", ""invalid value encountered in"")\n\n        if da.dtype.kind == ""O"" and skipna:\n            # Numpy < 1.13 does not handle object-type array.\n            try:\n                if skipna:\n                    expected = getattr(np, f""nan{func}"")(da.values, axis=axis)\n                else:\n                    expected = getattr(np, func)(da.values, axis=axis)\n\n                actual = getattr(da, func)(skipna=skipna, dim=aggdim)\n                assert_dask_array(actual, dask)\n                assert np.allclose(\n                    actual.values, np.array(expected), rtol=1.0e-4, equal_nan=True\n                )\n            except (TypeError, AttributeError, ZeroDivisionError):\n                # TODO currently, numpy does not support some methods such as\n                # nanmean for object dtype\n                pass\n\n        actual = getattr(da, func)(skipna=skipna, dim=aggdim)\n\n        # for dask case, make sure the result is the same for numpy backend\n        expected = getattr(da.compute(), func)(skipna=skipna, dim=aggdim)\n        assert_allclose(actual, expected, rtol=rtol)\n\n        # make sure the compatiblility with pandas\' results.\n        if func in [""var"", ""std""]:\n            expected = series_reduce(da, func, skipna=skipna, dim=aggdim, ddof=0)\n            assert_allclose(actual, expected, rtol=rtol)\n            # also check ddof!=0 case\n            actual = getattr(da, func)(skipna=skipna, dim=aggdim, ddof=5)\n            if dask:\n                assert isinstance(da.data, dask_array_type)\n            expected = series_reduce(da, func, skipna=skipna, dim=aggdim, ddof=5)\n            assert_allclose(actual, expected, rtol=rtol)\n        else:\n            expected = series_reduce(da, func, skipna=skipna, dim=aggdim)\n            assert_allclose(actual, expected, rtol=rtol)\n\n        # make sure the dtype argument\n        if func not in [""max"", ""min""]:\n            actual = getattr(da, func)(skipna=skipna, dim=aggdim, dtype=float)\n            assert_dask_array(actual, dask)\n            assert actual.dtype == float\n\n        # without nan\n        da = construct_dataarray(dim_num, dtype, contains_nan=False, dask=dask)\n        actual = getattr(da, func)(skipna=skipna)\n        if dask:\n            assert isinstance(da.data, dask_array_type)\n        expected = getattr(np, f""nan{func}"")(da.values)\n        if actual.dtype == object:\n            assert actual.values == np.array(expected)\n        else:\n            assert np.allclose(actual.values, np.array(expected), rtol=rtol)\n\n\n@pytest.mark.parametrize(""dim_num"", [1, 2])\n@pytest.mark.parametrize(""dtype"", [float, int, np.float32, np.bool_, str])\n@pytest.mark.parametrize(""contains_nan"", [True, False])\n@pytest.mark.parametrize(""dask"", [False, True])\n@pytest.mark.parametrize(""func"", [""min"", ""max""])\n@pytest.mark.parametrize(""skipna"", [False, True])\n@pytest.mark.parametrize(""aggdim"", [""x"", ""y""])\ndef test_argmin_max(dim_num, dtype, contains_nan, dask, func, skipna, aggdim):\n    # pandas-dev/pandas#16830, we do not check consistency with pandas but\n    # just make sure da[da.argmin()] == da.min()\n\n    if aggdim == ""y"" and dim_num < 2:\n        pytest.skip(""dim not in this test"")\n\n    if dask and not has_dask:\n        pytest.skip(""requires dask"")\n\n    if contains_nan:\n        if not skipna:\n            pytest.skip(\n                ""numpy\'s argmin (not nanargmin) does not handle "" ""object-dtype""\n            )\n        if skipna and np.dtype(dtype).kind in ""iufc"":\n            pytest.skip(""numpy\'s nanargmin raises ValueError for all nan axis"")\n    da = construct_dataarray(dim_num, dtype, contains_nan=contains_nan, dask=dask)\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", ""All-NaN slice"")\n\n        actual = da.isel(\n            **{aggdim: getattr(da, ""arg"" + func)(dim=aggdim, skipna=skipna).compute()}\n        )\n        expected = getattr(da, func)(dim=aggdim, skipna=skipna)\n        assert_allclose(\n            actual.drop_vars(list(actual.coords)),\n            expected.drop_vars(list(expected.coords)),\n        )\n\n\ndef test_argmin_max_error():\n    da = construct_dataarray(2, np.bool_, contains_nan=True, dask=False)\n    da[0] = np.nan\n    with pytest.raises(ValueError):\n        da.argmin(dim=""y"")\n\n\n@pytest.mark.parametrize(\n    ""array"",\n    [\n        np.array([np.datetime64(""2000-01-01""), np.datetime64(""NaT"")]),\n        np.array([np.timedelta64(1, ""h""), np.timedelta64(""NaT"")]),\n        np.array([0.0, np.nan]),\n        np.array([1j, np.nan]),\n        np.array([""foo"", np.nan], dtype=object),\n    ],\n)\ndef test_isnull(array):\n    expected = np.array([False, True])\n    actual = duck_array_ops.isnull(array)\n    np.testing.assert_equal(expected, actual)\n\n\n@requires_dask\ndef test_isnull_with_dask():\n    da = construct_dataarray(2, np.float32, contains_nan=True, dask=True)\n    assert isinstance(da.isnull().data, dask_array_type)\n    assert_equal(da.isnull().load(), da.load().isnull())\n\n\n@pytest.mark.skipif(not has_dask, reason=""This is for dask."")\n@pytest.mark.parametrize(""axis"", [0, -1])\n@pytest.mark.parametrize(""window"", [3, 8, 11])\n@pytest.mark.parametrize(""center"", [True, False])\ndef test_dask_rolling(axis, window, center):\n    import dask.array as da\n\n    x = np.array(np.random.randn(100, 40), dtype=float)\n    dx = da.from_array(x, chunks=[(6, 30, 30, 20, 14), 8])\n\n    expected = rolling_window(\n        x, axis=axis, window=window, center=center, fill_value=np.nan\n    )\n    actual = rolling_window(\n        dx, axis=axis, window=window, center=center, fill_value=np.nan\n    )\n    assert isinstance(actual, da.Array)\n    assert_array_equal(actual, expected)\n    assert actual.shape == expected.shape\n\n    # we need to take care of window size if chunk size is small\n    # window/2 should be smaller than the smallest chunk size.\n    with pytest.raises(ValueError):\n        rolling_window(dx, axis=axis, window=100, center=center, fill_value=np.nan)\n\n\n@pytest.mark.skipif(not has_dask, reason=""This is for dask."")\n@pytest.mark.parametrize(""axis"", [0, -1, 1])\n@pytest.mark.parametrize(""edge_order"", [1, 2])\ndef test_dask_gradient(axis, edge_order):\n    import dask.array as da\n\n    array = np.array(np.random.randn(100, 5, 40))\n    x = np.exp(np.linspace(0, 1, array.shape[axis]))\n\n    darray = da.from_array(array, chunks=[(6, 30, 30, 20, 14), 5, 8])\n    expected = gradient(array, x, axis=axis, edge_order=edge_order)\n    actual = gradient(darray, x, axis=axis, edge_order=edge_order)\n\n    assert isinstance(actual, da.Array)\n    assert_array_equal(actual, expected)\n\n\n@pytest.mark.parametrize(""dim_num"", [1, 2])\n@pytest.mark.parametrize(""dtype"", [float, int, np.float32, np.bool_])\n@pytest.mark.parametrize(""dask"", [False, True])\n@pytest.mark.parametrize(""func"", [""sum"", ""prod""])\n@pytest.mark.parametrize(""aggdim"", [None, ""x""])\ndef test_min_count(dim_num, dtype, dask, func, aggdim):\n    if dask and not has_dask:\n        pytest.skip(""requires dask"")\n\n    da = construct_dataarray(dim_num, dtype, contains_nan=True, dask=dask)\n    min_count = 3\n\n    actual = getattr(da, func)(dim=aggdim, skipna=True, min_count=min_count)\n    expected = series_reduce(da, func, skipna=True, dim=aggdim, min_count=min_count)\n    assert_allclose(actual, expected)\n    assert_dask_array(actual, dask)\n\n\n@pytest.mark.parametrize(""func"", [""sum"", ""prod""])\ndef test_min_count_dataset(func):\n    da = construct_dataarray(2, dtype=float, contains_nan=True, dask=False)\n    ds = Dataset({""var1"": da}, coords={""scalar"": 0})\n    actual = getattr(ds, func)(dim=""x"", skipna=True, min_count=3)[""var1""]\n    expected = getattr(ds[""var1""], func)(dim=""x"", skipna=True, min_count=3)\n    assert_allclose(actual, expected)\n\n\n@pytest.mark.parametrize(""dtype"", [float, int, np.float32, np.bool_])\n@pytest.mark.parametrize(""dask"", [False, True])\n@pytest.mark.parametrize(""func"", [""sum"", ""prod""])\ndef test_multiple_dims(dtype, dask, func):\n    if dask and not has_dask:\n        pytest.skip(""requires dask"")\n    da = construct_dataarray(3, dtype, contains_nan=True, dask=dask)\n\n    actual = getattr(da, func)((""x"", ""y""))\n    expected = getattr(getattr(da, func)(""x""), func)(""y"")\n    assert_allclose(actual, expected)\n\n\ndef test_docs():\n    # with min_count\n    actual = DataArray.sum.__doc__\n    expected = dedent(\n        """"""\\\n        Reduce this DataArray\'s data by applying `sum` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str or sequence of str, optional\n            Dimension(s) over which to apply `sum`.\n        axis : int or sequence of int, optional\n            Axis(es) over which to apply `sum`. Only one of the \'dim\'\n            and \'axis\' arguments can be supplied. If neither are supplied, then\n            `sum` is calculated over axes.\n        skipna : bool, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or skipna=True has not been\n            implemented (object, datetime64 or timedelta64).\n        min_count : int, default None\n            The required number of valid values to perform the operation.\n            If fewer than min_count non-NA values are present the result will\n            be NA. New in version 0.10.8: Added with the default being None.\n        keep_attrs : bool, optional\n            If True, the attributes (`attrs`) will be copied from the original\n            object to the new one.  If False (default), the new object will be\n            returned without attributes.\n        **kwargs : dict\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating `sum` on this object\'s data.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray object with `sum` applied to its data and the\n            indicated dimension(s) removed.\n        """"""\n    )\n    assert actual == expected\n\n    # without min_count\n    actual = DataArray.std.__doc__\n    expected = dedent(\n        """"""\\\n        Reduce this DataArray\'s data by applying `std` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str or sequence of str, optional\n            Dimension(s) over which to apply `std`.\n        axis : int or sequence of int, optional\n            Axis(es) over which to apply `std`. Only one of the \'dim\'\n            and \'axis\' arguments can be supplied. If neither are supplied, then\n            `std` is calculated over axes.\n        skipna : bool, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or skipna=True has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool, optional\n            If True, the attributes (`attrs`) will be copied from the original\n            object to the new one.  If False (default), the new object will be\n            returned without attributes.\n        **kwargs : dict\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating `std` on this object\'s data.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray object with `std` applied to its data and the\n            indicated dimension(s) removed.\n        """"""\n    )\n    assert actual == expected\n\n\ndef test_datetime_to_numeric_datetime64():\n    times = pd.date_range(""2000"", periods=5, freq=""7D"").values\n    result = duck_array_ops.datetime_to_numeric(times, datetime_unit=""h"")\n    expected = 24 * np.arange(0, 35, 7)\n    np.testing.assert_array_equal(result, expected)\n\n    offset = times[1]\n    result = duck_array_ops.datetime_to_numeric(times, offset=offset, datetime_unit=""h"")\n    expected = 24 * np.arange(-7, 28, 7)\n    np.testing.assert_array_equal(result, expected)\n\n    dtype = np.float32\n    result = duck_array_ops.datetime_to_numeric(times, datetime_unit=""h"", dtype=dtype)\n    expected = 24 * np.arange(0, 35, 7).astype(dtype)\n    np.testing.assert_array_equal(result, expected)\n\n\n@requires_cftime\ndef test_datetime_to_numeric_cftime():\n    times = cftime_range(""2000"", periods=5, freq=""7D"", calendar=""standard"").values\n    result = duck_array_ops.datetime_to_numeric(times, datetime_unit=""h"", dtype=int)\n    expected = 24 * np.arange(0, 35, 7)\n    np.testing.assert_array_equal(result, expected)\n\n    offset = times[1]\n    result = duck_array_ops.datetime_to_numeric(\n        times, offset=offset, datetime_unit=""h"", dtype=int\n    )\n    expected = 24 * np.arange(-7, 28, 7)\n    np.testing.assert_array_equal(result, expected)\n\n    dtype = np.float32\n    result = duck_array_ops.datetime_to_numeric(times, datetime_unit=""h"", dtype=dtype)\n    expected = 24 * np.arange(0, 35, 7).astype(dtype)\n    np.testing.assert_array_equal(result, expected)\n\n\n@requires_cftime\ndef test_datetime_to_numeric_potential_overflow():\n    import cftime\n\n    times = pd.date_range(""2000"", periods=5, freq=""7D"").values.astype(""datetime64[us]"")\n    cftimes = cftime_range(\n        ""2000"", periods=5, freq=""7D"", calendar=""proleptic_gregorian""\n    ).values\n\n    offset = np.datetime64(""0001-01-01"")\n    cfoffset = cftime.DatetimeProlepticGregorian(1, 1, 1)\n\n    result = duck_array_ops.datetime_to_numeric(\n        times, offset=offset, datetime_unit=""D"", dtype=int\n    )\n    cfresult = duck_array_ops.datetime_to_numeric(\n        cftimes, offset=cfoffset, datetime_unit=""D"", dtype=int\n    )\n\n    expected = 730119 + np.arange(0, 35, 7)\n\n    np.testing.assert_array_equal(result, expected)\n    np.testing.assert_array_equal(cfresult, expected)\n\n\ndef test_py_timedelta_to_float():\n    assert py_timedelta_to_float(dt.timedelta(days=1), ""ns"") == 86400 * 1e9\n    assert py_timedelta_to_float(dt.timedelta(days=1e6), ""ps"") == 86400 * 1e18\n    assert py_timedelta_to_float(dt.timedelta(days=1e6), ""ns"") == 86400 * 1e15\n    assert py_timedelta_to_float(dt.timedelta(days=1e6), ""us"") == 86400 * 1e12\n    assert py_timedelta_to_float(dt.timedelta(days=1e6), ""ms"") == 86400 * 1e9\n    assert py_timedelta_to_float(dt.timedelta(days=1e6), ""s"") == 86400 * 1e6\n    assert py_timedelta_to_float(dt.timedelta(days=1e6), ""D"") == 1e6\n\n\n@pytest.mark.parametrize(\n    ""td, expected"",\n    ([np.timedelta64(1, ""D""), 86400 * 1e9], [np.timedelta64(1, ""ns""), 1.0]),\n)\ndef test_np_timedelta64_to_float(td, expected):\n    out = np_timedelta64_to_float(td, datetime_unit=""ns"")\n    np.testing.assert_allclose(out, expected)\n    assert isinstance(out, float)\n\n    out = np_timedelta64_to_float(np.atleast_1d(td), datetime_unit=""ns"")\n    np.testing.assert_allclose(out, expected)\n\n\n@pytest.mark.parametrize(\n    ""td, expected"", ([pd.Timedelta(1, ""D""), 86400 * 1e9], [pd.Timedelta(1, ""ns""), 1.0])\n)\ndef test_pd_timedelta_to_float(td, expected):\n    out = pd_timedelta_to_float(td, datetime_unit=""ns"")\n    np.testing.assert_allclose(out, expected)\n    assert isinstance(out, float)\n\n\n@pytest.mark.parametrize(\n    ""td"", [dt.timedelta(days=1), np.timedelta64(1, ""D""), pd.Timedelta(1, ""D""), ""1 day""]\n)\ndef test_timedelta_to_numeric(td):\n    # Scalar input\n    out = timedelta_to_numeric(td, ""ns"")\n    np.testing.assert_allclose(out, 86400 * 1e9)\n    assert isinstance(out, float)\n\n\n@pytest.mark.parametrize(""use_dask"", [True, False])\n@pytest.mark.parametrize(""skipna"", [True, False])\ndef test_least_squares(use_dask, skipna):\n    if use_dask and not has_dask:\n        pytest.skip(""requires dask"")\n    lhs = np.array([[1, 2], [1, 2], [3, 2]])\n    rhs = DataArray(np.array([3, 5, 7]), dims=(""y"",))\n\n    if use_dask:\n        rhs = rhs.chunk({""y"": 1})\n\n    coeffs, residuals = least_squares(lhs, rhs.data, skipna=skipna)\n\n    np.testing.assert_allclose(coeffs, [1.5, 1.25])\n    np.testing.assert_allclose(residuals, [2.0])\n'"
xarray/tests/test_extensions.py,0,"b'import pickle\n\nimport pytest\n\nimport xarray as xr\n\nfrom . import raises_regex\n\n\n@xr.register_dataset_accessor(""example_accessor"")\n@xr.register_dataarray_accessor(""example_accessor"")\nclass ExampleAccessor:\n    """"""For the pickling tests below.""""""\n\n    def __init__(self, xarray_obj):\n        self.obj = xarray_obj\n\n\nclass TestAccessor:\n    def test_register(self):\n        @xr.register_dataset_accessor(""demo"")\n        @xr.register_dataarray_accessor(""demo"")\n        class DemoAccessor:\n            """"""Demo accessor.""""""\n\n            def __init__(self, xarray_obj):\n                self._obj = xarray_obj\n\n            @property\n            def foo(self):\n                return ""bar""\n\n        ds = xr.Dataset()\n        assert ds.demo.foo == ""bar""\n\n        da = xr.DataArray(0)\n        assert da.demo.foo == ""bar""\n\n        # accessor is cached\n        assert ds.demo is ds.demo\n\n        # check descriptor\n        assert ds.demo.__doc__ == ""Demo accessor.""\n        assert xr.Dataset.demo.__doc__ == ""Demo accessor.""\n        assert isinstance(ds.demo, DemoAccessor)\n        assert xr.Dataset.demo is DemoAccessor\n\n        # ensure we can remove it\n        del xr.Dataset.demo\n        assert not hasattr(xr.Dataset, ""demo"")\n\n        with pytest.warns(Warning, match=""overriding a preexisting attribute""):\n\n            @xr.register_dataarray_accessor(""demo"")\n            class Foo:\n                pass\n\n        # it didn\'t get registered again\n        assert not hasattr(xr.Dataset, ""demo"")\n\n    def test_pickle_dataset(self):\n        ds = xr.Dataset()\n        ds_restored = pickle.loads(pickle.dumps(ds))\n        assert ds.identical(ds_restored)\n\n        # state save on the accessor is restored\n        assert ds.example_accessor is ds.example_accessor\n        ds.example_accessor.value = ""foo""\n        ds_restored = pickle.loads(pickle.dumps(ds))\n        assert ds.identical(ds_restored)\n        assert ds_restored.example_accessor.value == ""foo""\n\n    def test_pickle_dataarray(self):\n        array = xr.Dataset()\n        assert array.example_accessor is array.example_accessor\n        array_restored = pickle.loads(pickle.dumps(array))\n        assert array.identical(array_restored)\n\n    def test_broken_accessor(self):\n        # regression test for GH933\n\n        @xr.register_dataset_accessor(""stupid_accessor"")\n        class BrokenAccessor:\n            def __init__(self, xarray_obj):\n                raise AttributeError(""broken"")\n\n        with raises_regex(RuntimeError, ""error initializing""):\n            xr.Dataset().stupid_accessor\n'"
xarray/tests/test_formatting.py,47,"b'import sys\nfrom textwrap import dedent\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nimport xarray as xr\nfrom xarray.core import formatting\n\nfrom . import raises_regex\n\n\nclass TestFormatting:\n    def test_get_indexer_at_least_n_items(self):\n        cases = [\n            ((20,), (slice(10),), (slice(-10, None),)),\n            ((3, 20), (0, slice(10)), (-1, slice(-10, None))),\n            ((2, 10), (0, slice(10)), (-1, slice(-10, None))),\n            ((2, 5), (slice(2), slice(None)), (slice(-2, None), slice(None))),\n            ((1, 2, 5), (0, slice(2), slice(None)), (-1, slice(-2, None), slice(None))),\n            ((2, 3, 5), (0, slice(2), slice(None)), (-1, slice(-2, None), slice(None))),\n            (\n                (1, 10, 1),\n                (0, slice(10), slice(None)),\n                (-1, slice(-10, None), slice(None)),\n            ),\n            (\n                (2, 5, 1),\n                (slice(2), slice(None), slice(None)),\n                (slice(-2, None), slice(None), slice(None)),\n            ),\n            ((2, 5, 3), (0, slice(4), slice(None)), (-1, slice(-4, None), slice(None))),\n            (\n                (2, 3, 3),\n                (slice(2), slice(None), slice(None)),\n                (slice(-2, None), slice(None), slice(None)),\n            ),\n        ]\n        for shape, start_expected, end_expected in cases:\n            actual = formatting._get_indexer_at_least_n_items(shape, 10, from_end=False)\n            assert start_expected == actual\n            actual = formatting._get_indexer_at_least_n_items(shape, 10, from_end=True)\n            assert end_expected == actual\n\n    def test_first_n_items(self):\n        array = np.arange(100).reshape(10, 5, 2)\n        for n in [3, 10, 13, 100, 200]:\n            actual = formatting.first_n_items(array, n)\n            expected = array.flat[:n]\n            assert (expected == actual).all()\n\n        with raises_regex(ValueError, ""at least one item""):\n            formatting.first_n_items(array, 0)\n\n    def test_last_n_items(self):\n        array = np.arange(100).reshape(10, 5, 2)\n        for n in [3, 10, 13, 100, 200]:\n            actual = formatting.last_n_items(array, n)\n            expected = array.flat[-n:]\n            assert (expected == actual).all()\n\n        with raises_regex(ValueError, ""at least one item""):\n            formatting.first_n_items(array, 0)\n\n    def test_last_item(self):\n        array = np.arange(100)\n\n        reshape = ((10, 10), (1, 100), (2, 2, 5, 5))\n        expected = np.array([99])\n\n        for r in reshape:\n            result = formatting.last_item(array.reshape(r))\n            assert result == expected\n\n    def test_format_item(self):\n        cases = [\n            (pd.Timestamp(""2000-01-01T12""), ""2000-01-01T12:00:00""),\n            (pd.Timestamp(""2000-01-01""), ""2000-01-01""),\n            (pd.Timestamp(""NaT""), ""NaT""),\n            (pd.Timedelta(""10 days 1 hour""), ""10 days 01:00:00""),\n            (pd.Timedelta(""-3 days""), ""-3 days +00:00:00""),\n            (pd.Timedelta(""3 hours""), ""0 days 03:00:00""),\n            (pd.Timedelta(""NaT""), ""NaT""),\n            (""foo"", ""\'foo\'""),\n            (b""foo"", ""b\'foo\'""),\n            (1, ""1""),\n            (1.0, ""1.0""),\n        ]\n        for item, expected in cases:\n            actual = formatting.format_item(item)\n            assert expected == actual\n\n    def test_format_items(self):\n        cases = [\n            (np.arange(4) * np.timedelta64(1, ""D""), ""0 days 1 days 2 days 3 days""),\n            (\n                np.arange(4) * np.timedelta64(3, ""h""),\n                ""00:00:00 03:00:00 06:00:00 09:00:00"",\n            ),\n            (\n                np.arange(4) * np.timedelta64(500, ""ms""),\n                ""00:00:00 00:00:00.500000 00:00:01 00:00:01.500000"",\n            ),\n            (pd.to_timedelta([""NaT"", ""0s"", ""1s"", ""NaT""]), ""NaT 00:00:00 00:00:01 NaT""),\n            (\n                pd.to_timedelta([""1 day 1 hour"", ""1 day"", ""0 hours""]),\n                ""1 days 01:00:00 1 days 00:00:00 0 days 00:00:00"",\n            ),\n            ([1, 2, 3], ""1 2 3""),\n        ]\n        for item, expected in cases:\n            actual = "" "".join(formatting.format_items(item))\n            assert expected == actual\n\n    def test_format_array_flat(self):\n        actual = formatting.format_array_flat(np.arange(100), 2)\n        expected = ""...""\n        assert expected == actual\n\n        actual = formatting.format_array_flat(np.arange(100), 9)\n        expected = ""0 ... 99""\n        assert expected == actual\n\n        actual = formatting.format_array_flat(np.arange(100), 10)\n        expected = ""0 1 ... 99""\n        assert expected == actual\n\n        actual = formatting.format_array_flat(np.arange(100), 13)\n        expected = ""0 1 ... 98 99""\n        assert expected == actual\n\n        actual = formatting.format_array_flat(np.arange(100), 15)\n        expected = ""0 1 2 ... 98 99""\n        assert expected == actual\n\n        # NB: Probably not ideal; an alternative would be cutting after the\n        # first ellipsis\n        actual = formatting.format_array_flat(np.arange(100.0), 11)\n        expected = ""0.0 ... ...""\n        assert expected == actual\n\n        actual = formatting.format_array_flat(np.arange(100.0), 12)\n        expected = ""0.0 ... 99.0""\n        assert expected == actual\n\n        actual = formatting.format_array_flat(np.arange(3), 5)\n        expected = ""0 1 2""\n        assert expected == actual\n\n        actual = formatting.format_array_flat(np.arange(4.0), 11)\n        expected = ""0.0 ... 3.0""\n        assert expected == actual\n\n        actual = formatting.format_array_flat(np.arange(0), 0)\n        expected = """"\n        assert expected == actual\n\n        actual = formatting.format_array_flat(np.arange(1), 1)\n        expected = ""0""\n        assert expected == actual\n\n        actual = formatting.format_array_flat(np.arange(2), 3)\n        expected = ""0 1""\n        assert expected == actual\n\n        actual = formatting.format_array_flat(np.arange(4), 7)\n        expected = ""0 1 2 3""\n        assert expected == actual\n\n        actual = formatting.format_array_flat(np.arange(5), 7)\n        expected = ""0 ... 4""\n        assert expected == actual\n\n        long_str = ["" "".join([""hello world"" for _ in range(100)])]\n        actual = formatting.format_array_flat(np.asarray([long_str]), 21)\n        expected = ""\'hello world hello...""\n        assert expected == actual\n\n    def test_pretty_print(self):\n        assert formatting.pretty_print(""abcdefghij"", 8) == ""abcde...""\n        assert formatting.pretty_print(""\xc3\x9f"", 1) == ""\xc3\x9f""\n\n    def test_maybe_truncate(self):\n        assert formatting.maybe_truncate(""\xc3\x9f"", 10) == ""\xc3\x9f""\n\n    def test_format_timestamp_out_of_bounds(self):\n        from datetime import datetime\n\n        date = datetime(1300, 12, 1)\n        expected = ""1300-12-01""\n        result = formatting.format_timestamp(date)\n        assert result == expected\n\n        date = datetime(2300, 12, 1)\n        expected = ""2300-12-01""\n        result = formatting.format_timestamp(date)\n        assert result == expected\n\n    def test_attribute_repr(self):\n        short = formatting.summarize_attr(""key"", ""Short string"")\n        long = formatting.summarize_attr(""key"", 100 * ""Very long string "")\n        newlines = formatting.summarize_attr(""key"", ""\\n\\n\\n"")\n        tabs = formatting.summarize_attr(""key"", ""\\t\\t\\t"")\n        assert short == ""    key: Short string""\n        assert len(long) <= 80\n        assert long.endswith(""..."")\n        assert ""\\n"" not in newlines\n        assert ""\\t"" not in tabs\n\n    def test_diff_array_repr(self):\n        da_a = xr.DataArray(\n            np.array([[1, 2, 3], [4, 5, 6]], dtype=""int64""),\n            dims=(""x"", ""y""),\n            coords={\n                ""x"": np.array([""a"", ""b""], dtype=""U1""),\n                ""y"": np.array([1, 2, 3], dtype=""int64""),\n            },\n            attrs={""units"": ""m"", ""description"": ""desc""},\n        )\n\n        da_b = xr.DataArray(\n            np.array([1, 2], dtype=""int64""),\n            dims=""x"",\n            coords={\n                ""x"": np.array([""a"", ""c""], dtype=""U1""),\n                ""label"": (""x"", np.array([1, 2], dtype=""int64"")),\n            },\n            attrs={""units"": ""kg""},\n        )\n\n        byteorder = ""<"" if sys.byteorder == ""little"" else "">""\n        expected = dedent(\n            """"""\\\n        Left and right DataArray objects are not identical\n        Differing dimensions:\n            (x: 2, y: 3) != (x: 2)\n        Differing values:\n        L\n            array([[1, 2, 3],\n                   [4, 5, 6]], dtype=int64)\n        R\n            array([1, 2], dtype=int64)\n        Differing coordinates:\n        L * x        (x) %cU1 \'a\' \'b\'\n        R * x        (x) %cU1 \'a\' \'c\'\n        Coordinates only on the left object:\n          * y        (y) int64 1 2 3\n        Coordinates only on the right object:\n            label    (x) int64 1 2\n        Differing attributes:\n        L   units: m\n        R   units: kg\n        Attributes only on the left object:\n            description: desc""""""\n            % (byteorder, byteorder)\n        )\n\n        actual = formatting.diff_array_repr(da_a, da_b, ""identical"")\n        try:\n            assert actual == expected\n        except AssertionError:\n            # depending on platform, dtype may not be shown in numpy array repr\n            assert actual == expected.replace("", dtype=int64"", """")\n\n        va = xr.Variable(\n            ""x"", np.array([1, 2, 3], dtype=""int64""), {""title"": ""test Variable""}\n        )\n        vb = xr.Variable((""x"", ""y""), np.array([[1, 2, 3], [4, 5, 6]], dtype=""int64""))\n\n        expected = dedent(\n            """"""\\\n        Left and right Variable objects are not equal\n        Differing dimensions:\n            (x: 3) != (x: 2, y: 3)\n        Differing values:\n        L\n            array([1, 2, 3], dtype=int64)\n        R\n            array([[1, 2, 3],\n                   [4, 5, 6]], dtype=int64)""""""\n        )\n\n        actual = formatting.diff_array_repr(va, vb, ""equals"")\n        try:\n            assert actual == expected\n        except AssertionError:\n            assert actual == expected.replace("", dtype=int64"", """")\n\n    @pytest.mark.filterwarnings(""error"")\n    def test_diff_attrs_repr_with_array(self):\n        attrs_a = {""attr"": np.array([0, 1])}\n\n        attrs_b = {""attr"": 1}\n        expected = dedent(\n            """"""\\\n            Differing attributes:\n            L   attr: [0 1]\n            R   attr: 1\n            """"""\n        ).strip()\n        actual = formatting.diff_attrs_repr(attrs_a, attrs_b, ""equals"")\n        assert expected == actual\n\n        attrs_b = {""attr"": np.array([-3, 5])}\n        expected = dedent(\n            """"""\\\n            Differing attributes:\n            L   attr: [0 1]\n            R   attr: [-3  5]\n            """"""\n        ).strip()\n        actual = formatting.diff_attrs_repr(attrs_a, attrs_b, ""equals"")\n        assert expected == actual\n\n        # should not raise a warning\n        attrs_b = {""attr"": np.array([0, 1, 2])}\n        expected = dedent(\n            """"""\\\n            Differing attributes:\n            L   attr: [0 1]\n            R   attr: [0 1 2]\n            """"""\n        ).strip()\n        actual = formatting.diff_attrs_repr(attrs_a, attrs_b, ""equals"")\n        assert expected == actual\n\n    def test_diff_dataset_repr(self):\n        ds_a = xr.Dataset(\n            data_vars={\n                ""var1"": ((""x"", ""y""), np.array([[1, 2, 3], [4, 5, 6]], dtype=""int64"")),\n                ""var2"": (""x"", np.array([3, 4], dtype=""int64"")),\n            },\n            coords={\n                ""x"": np.array([""a"", ""b""], dtype=""U1""),\n                ""y"": np.array([1, 2, 3], dtype=""int64""),\n            },\n            attrs={""units"": ""m"", ""description"": ""desc""},\n        )\n\n        ds_b = xr.Dataset(\n            data_vars={""var1"": (""x"", np.array([1, 2], dtype=""int64""))},\n            coords={\n                ""x"": (""x"", np.array([""a"", ""c""], dtype=""U1""), {""source"": 0}),\n                ""label"": (""x"", np.array([1, 2], dtype=""int64"")),\n            },\n            attrs={""units"": ""kg""},\n        )\n\n        byteorder = ""<"" if sys.byteorder == ""little"" else "">""\n        expected = dedent(\n            """"""\\\n        Left and right Dataset objects are not identical\n        Differing dimensions:\n            (x: 2, y: 3) != (x: 2)\n        Differing coordinates:\n        L * x        (x) %cU1 \'a\' \'b\'\n        R * x        (x) %cU1 \'a\' \'c\'\n            source: 0\n        Coordinates only on the left object:\n          * y        (y) int64 1 2 3\n        Coordinates only on the right object:\n            label    (x) int64 1 2\n        Differing data variables:\n        L   var1     (x, y) int64 1 2 3 4 5 6\n        R   var1     (x) int64 1 2\n        Data variables only on the left object:\n            var2     (x) int64 3 4\n        Differing attributes:\n        L   units: m\n        R   units: kg\n        Attributes only on the left object:\n            description: desc""""""\n            % (byteorder, byteorder)\n        )\n\n        actual = formatting.diff_dataset_repr(ds_a, ds_b, ""identical"")\n        assert actual == expected\n\n    def test_array_repr(self):\n        ds = xr.Dataset(coords={""foo"": [1, 2, 3], ""bar"": [1, 2, 3]})\n        ds[(1, 2)] = xr.DataArray([0], dims=""test"")\n        actual = formatting.array_repr(ds[(1, 2)])\n        expected = dedent(\n            """"""\\\n        <xarray.DataArray (1, 2) (test: 1)>\n        array([0])\n        Dimensions without coordinates: test""""""\n        )\n\n        assert actual == expected\n\n\ndef test_set_numpy_options():\n    original_options = np.get_printoptions()\n    with formatting.set_numpy_options(threshold=10):\n        assert len(repr(np.arange(500))) < 200\n    # original options are restored\n    assert np.get_printoptions() == original_options\n\n\ndef test_short_numpy_repr():\n    cases = [\n        np.random.randn(500),\n        np.random.randn(20, 20),\n        np.random.randn(5, 10, 15),\n        np.random.randn(5, 10, 15, 3),\n    ]\n    # number of lines:\n    # for default numpy repr: 167, 140, 254, 248\n    # for short_numpy_repr: 1, 7, 24, 19\n    for array in cases:\n        num_lines = formatting.short_numpy_repr(array).count(""\\n"") + 1\n        assert num_lines < 30\n'"
xarray/tests/test_formatting_html.py,4,"b'from distutils.version import LooseVersion\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nimport xarray as xr\nfrom xarray.core import formatting_html as fh\n\n\n@pytest.fixture\ndef dataarray():\n    return xr.DataArray(np.random.RandomState(0).randn(4, 6))\n\n\n@pytest.fixture\ndef dask_dataarray(dataarray):\n    pytest.importorskip(""dask"")\n    return dataarray.chunk()\n\n\n@pytest.fixture\ndef multiindex():\n    mindex = pd.MultiIndex.from_product(\n        [[""a"", ""b""], [1, 2]], names=(""level_1"", ""level_2"")\n    )\n    return xr.Dataset({}, {""x"": mindex})\n\n\n@pytest.fixture\ndef dataset():\n    times = pd.date_range(""2000-01-01"", ""2001-12-31"", name=""time"")\n    annual_cycle = np.sin(2 * np.pi * (times.dayofyear.values / 365.25 - 0.28))\n\n    base = 10 + 15 * annual_cycle.reshape(-1, 1)\n    tmin_values = base + 3 * np.random.randn(annual_cycle.size, 3)\n    tmax_values = base + 10 + 3 * np.random.randn(annual_cycle.size, 3)\n\n    return xr.Dataset(\n        {\n            ""tmin"": ((""time"", ""location""), tmin_values),\n            ""tmax"": ((""time"", ""location""), tmax_values),\n        },\n        {""time"": times, ""location"": [""<IA>"", ""IN"", ""IL""]},\n        attrs={""description"": ""Test data.""},\n    )\n\n\ndef test_short_data_repr_html(dataarray):\n    data_repr = fh.short_data_repr_html(dataarray)\n    assert data_repr.startswith(""array"")\n\n\ndef test_short_data_repr_html_non_str_keys(dataset):\n    ds = dataset.assign({2: lambda x: x[""tmin""]})\n    fh.dataset_repr(ds)\n\n\ndef test_short_data_repr_html_dask(dask_dataarray):\n    import dask\n\n    if LooseVersion(dask.__version__) < ""2.0.0"":\n        assert not hasattr(dask_dataarray.data, ""_repr_html_"")\n        data_repr = fh.short_data_repr_html(dask_dataarray)\n        assert (\n            data_repr\n            == ""dask.array&lt;xarray-&lt;this-array&gt;, shape=(4, 6), dtype=float64, chunksize=(4, 6)&gt;""\n        )\n    else:\n        assert hasattr(dask_dataarray.data, ""_repr_html_"")\n        data_repr = fh.short_data_repr_html(dask_dataarray)\n        assert data_repr == dask_dataarray.data._repr_html_()\n\n\ndef test_format_dims_no_dims():\n    dims, coord_names = {}, []\n    formatted = fh.format_dims(dims, coord_names)\n    assert formatted == """"\n\n\ndef test_format_dims_unsafe_dim_name():\n    dims, coord_names = {""<x>"": 3, ""y"": 2}, []\n    formatted = fh.format_dims(dims, coord_names)\n    assert ""&lt;x&gt;"" in formatted\n\n\ndef test_format_dims_non_index():\n    dims, coord_names = {""x"": 3, ""y"": 2}, [""time""]\n    formatted = fh.format_dims(dims, coord_names)\n    assert ""class=\'xr-has-index\'"" not in formatted\n\n\ndef test_format_dims_index():\n    dims, coord_names = {""x"": 3, ""y"": 2}, [""x""]\n    formatted = fh.format_dims(dims, coord_names)\n    assert ""class=\'xr-has-index\'"" in formatted\n\n\ndef test_summarize_attrs_with_unsafe_attr_name_and_value():\n    attrs = {""<x>"": 3, ""y"": ""<pd.DataFrame>""}\n    formatted = fh.summarize_attrs(attrs)\n    assert ""<dt><span>&lt;x&gt; :</span></dt>"" in formatted\n    assert ""<dt><span>y :</span></dt>"" in formatted\n    assert ""<dd>3</dd>"" in formatted\n    assert ""<dd>&lt;pd.DataFrame&gt;</dd>"" in formatted\n\n\ndef test_repr_of_dataarray(dataarray):\n    formatted = fh.array_repr(dataarray)\n    assert ""dim_0"" in formatted\n    # has an expandable data section\n    assert formatted.count(""class=\'xr-array-in\' type=\'checkbox\' >"") == 1\n    # coords and attrs don\'t have an items so they\'ll be be disabled and collapsed\n    assert (\n        formatted.count(""class=\'xr-section-summary-in\' type=\'checkbox\' disabled >"") == 2\n    )\n\n\ndef test_summary_of_multiindex_coord(multiindex):\n    idx = multiindex.x.variable.to_index_variable()\n    formatted = fh._summarize_coord_multiindex(""foo"", idx)\n    assert ""(level_1, level_2)"" in formatted\n    assert ""MultiIndex"" in formatted\n    assert ""<span class=\'xr-has-index\'>foo</span>"" in formatted\n\n\ndef test_repr_of_multiindex(multiindex):\n    formatted = fh.dataset_repr(multiindex)\n    assert ""(x)"" in formatted\n\n\ndef test_repr_of_dataset(dataset):\n    formatted = fh.dataset_repr(dataset)\n    # coords, attrs, and data_vars are expanded\n    assert (\n        formatted.count(""class=\'xr-section-summary-in\' type=\'checkbox\'  checked>"") == 3\n    )\n    assert ""&lt;U4"" in formatted or ""&gt;U4"" in formatted\n    assert ""&lt;IA&gt;"" in formatted\n\n\ndef test_repr_text_fallback(dataset):\n    formatted = fh.dataset_repr(dataset)\n\n    # Just test that the ""pre"" block used for fallback to plain text is present.\n    assert ""<pre class=\'xr-text-repr-fallback\'>"" in formatted\n\n\ndef test_variable_repr_html():\n    v = xr.Variable([""time"", ""x""], [[1, 2, 3], [4, 5, 6]], {""foo"": ""bar""})\n    assert hasattr(v, ""_repr_html_"")\n    with xr.set_options(display_style=""html""):\n        html = v._repr_html_().strip()\n    # We don\'t do a complete string identity since\n    # html output is probably subject to change, is long and... reasons.\n    # Just test that something reasonable was produced.\n    assert html.startswith(""<div"") and html.endswith(""</div>"")\n    assert ""xarray.Variable"" in html\n'"
xarray/tests/test_groupby.py,34,"b'import numpy as np\nimport pandas as pd\nimport pytest\n\nimport xarray as xr\nfrom xarray.core.groupby import _consolidate_slices\n\nfrom . import assert_allclose, assert_equal, assert_identical, raises_regex\n\n\n@pytest.fixture\ndef dataset():\n    ds = xr.Dataset(\n        {""foo"": ((""x"", ""y"", ""z""), np.random.randn(3, 4, 2))},\n        {""x"": [""a"", ""b"", ""c""], ""y"": [1, 2, 3, 4], ""z"": [1, 2]},\n    )\n    ds[""boo""] = ((""z"", ""y""), [[""f"", ""g"", ""h"", ""j""]] * 2)\n\n    return ds\n\n\n@pytest.fixture\ndef array(dataset):\n    return dataset[""foo""]\n\n\ndef test_consolidate_slices():\n\n    assert _consolidate_slices([slice(3), slice(3, 5)]) == [slice(5)]\n    assert _consolidate_slices([slice(2, 3), slice(3, 6)]) == [slice(2, 6)]\n    assert _consolidate_slices([slice(2, 3, 1), slice(3, 6, 1)]) == [slice(2, 6, 1)]\n\n    slices = [slice(2, 3), slice(5, 6)]\n    assert _consolidate_slices(slices) == slices\n\n    with pytest.raises(ValueError):\n        _consolidate_slices([slice(3), 4])\n\n\ndef test_groupby_dims_property(dataset):\n    assert dataset.groupby(""x"").dims == dataset.isel(x=1).dims\n    assert dataset.groupby(""y"").dims == dataset.isel(y=1).dims\n\n    stacked = dataset.stack({""xy"": (""x"", ""y"")})\n    assert stacked.groupby(""xy"").dims == stacked.isel(xy=0).dims\n\n\ndef test_multi_index_groupby_map(dataset):\n    # regression test for GH873\n    ds = dataset.isel(z=1, drop=True)[[""foo""]]\n    expected = 2 * ds\n    actual = (\n        ds.stack(space=[""x"", ""y""])\n        .groupby(""space"")\n        .map(lambda x: 2 * x)\n        .unstack(""space"")\n    )\n    assert_equal(expected, actual)\n\n\ndef test_multi_index_groupby_sum():\n    # regression test for GH873\n    ds = xr.Dataset(\n        {""foo"": ((""x"", ""y"", ""z""), np.ones((3, 4, 2)))},\n        {""x"": [""a"", ""b"", ""c""], ""y"": [1, 2, 3, 4]},\n    )\n    expected = ds.sum(""z"")\n    actual = ds.stack(space=[""x"", ""y""]).groupby(""space"").sum(""z"").unstack(""space"")\n    assert_equal(expected, actual)\n\n\ndef test_groupby_da_datetime():\n    # test groupby with a DataArray of dtype datetime for GH1132\n    # create test data\n    times = pd.date_range(""2000-01-01"", periods=4)\n    foo = xr.DataArray([1, 2, 3, 4], coords=dict(time=times), dims=""time"")\n    # create test index\n    dd = times.to_pydatetime()\n    reference_dates = [dd[0], dd[2]]\n    labels = reference_dates[0:1] * 2 + reference_dates[1:2] * 2\n    ind = xr.DataArray(\n        labels, coords=dict(time=times), dims=""time"", name=""reference_date""\n    )\n    g = foo.groupby(ind)\n    actual = g.sum(dim=""time"")\n    expected = xr.DataArray(\n        [3, 7], coords=dict(reference_date=reference_dates), dims=""reference_date""\n    )\n    assert_equal(expected, actual)\n\n\ndef test_groupby_duplicate_coordinate_labels():\n    # fix for http://stackoverflow.com/questions/38065129\n    array = xr.DataArray([1, 2, 3], [(""x"", [1, 1, 2])])\n    expected = xr.DataArray([3, 3], [(""x"", [1, 2])])\n    actual = array.groupby(""x"").sum()\n    assert_equal(expected, actual)\n\n\ndef test_groupby_input_mutation():\n    # regression test for GH2153\n    array = xr.DataArray([1, 2, 3], [(""x"", [2, 2, 1])])\n    array_copy = array.copy()\n    expected = xr.DataArray([3, 3], [(""x"", [1, 2])])\n    actual = array.groupby(""x"").sum()\n    assert_identical(expected, actual)\n    assert_identical(array, array_copy)  # should not modify inputs\n\n\n@pytest.mark.parametrize(\n    ""obj"",\n    [\n        xr.DataArray([1, 2, 3, 4, 5, 6], [(""x"", [1, 1, 1, 2, 2, 2])]),\n        xr.Dataset({""foo"": (""x"", [1, 2, 3, 4, 5, 6])}, {""x"": [1, 1, 1, 2, 2, 2]}),\n    ],\n)\ndef test_groupby_map_shrink_groups(obj):\n    expected = obj.isel(x=[0, 1, 3, 4])\n    actual = obj.groupby(""x"").map(lambda f: f.isel(x=[0, 1]))\n    assert_identical(expected, actual)\n\n\n@pytest.mark.parametrize(\n    ""obj"",\n    [\n        xr.DataArray([1, 2, 3], [(""x"", [1, 2, 2])]),\n        xr.Dataset({""foo"": (""x"", [1, 2, 3])}, {""x"": [1, 2, 2]}),\n    ],\n)\ndef test_groupby_map_change_group_size(obj):\n    def func(group):\n        if group.sizes[""x""] == 1:\n            result = group.isel(x=[0, 0])\n        else:\n            result = group.isel(x=[0])\n        return result\n\n    expected = obj.isel(x=[0, 0, 1])\n    actual = obj.groupby(""x"").map(func)\n    assert_identical(expected, actual)\n\n\ndef test_da_groupby_map_func_args():\n    def func(arg1, arg2, arg3=0):\n        return arg1 + arg2 + arg3\n\n    array = xr.DataArray([1, 1, 1], [(""x"", [1, 2, 3])])\n    expected = xr.DataArray([3, 3, 3], [(""x"", [1, 2, 3])])\n    actual = array.groupby(""x"").map(func, args=(1,), arg3=1)\n    assert_identical(expected, actual)\n\n\ndef test_ds_groupby_map_func_args():\n    def func(arg1, arg2, arg3=0):\n        return arg1 + arg2 + arg3\n\n    dataset = xr.Dataset({""foo"": (""x"", [1, 1, 1])}, {""x"": [1, 2, 3]})\n    expected = xr.Dataset({""foo"": (""x"", [3, 3, 3])}, {""x"": [1, 2, 3]})\n    actual = dataset.groupby(""x"").map(func, args=(1,), arg3=1)\n    assert_identical(expected, actual)\n\n\ndef test_da_groupby_empty():\n\n    empty_array = xr.DataArray([], dims=""dim"")\n\n    with pytest.raises(ValueError):\n        empty_array.groupby(""dim"")\n\n\ndef test_da_groupby_quantile():\n\n    array = xr.DataArray(\n        data=[1, 2, 3, 4, 5, 6], coords={""x"": [1, 1, 1, 2, 2, 2]}, dims=""x""\n    )\n\n    # Scalar quantile\n    expected = xr.DataArray(\n        data=[2, 5], coords={""x"": [1, 2], ""quantile"": 0.5}, dims=""x""\n    )\n    actual = array.groupby(""x"").quantile(0.5)\n    assert_identical(expected, actual)\n\n    # Vector quantile\n    expected = xr.DataArray(\n        data=[[1, 3], [4, 6]],\n        coords={""x"": [1, 2], ""quantile"": [0, 1]},\n        dims=(""x"", ""quantile""),\n    )\n    actual = array.groupby(""x"").quantile([0, 1])\n    assert_identical(expected, actual)\n\n    # Multiple dimensions\n    array = xr.DataArray(\n        data=[[1, 11, 26], [2, 12, 22], [3, 13, 23], [4, 16, 24], [5, 15, 25]],\n        coords={""x"": [1, 1, 1, 2, 2], ""y"": [0, 0, 1]},\n        dims=(""x"", ""y""),\n    )\n\n    actual_x = array.groupby(""x"").quantile(0, dim=...)\n    expected_x = xr.DataArray(\n        data=[1, 4], coords={""x"": [1, 2], ""quantile"": 0}, dims=""x""\n    )\n    assert_identical(expected_x, actual_x)\n\n    actual_y = array.groupby(""y"").quantile(0, dim=...)\n    expected_y = xr.DataArray(\n        data=[1, 22], coords={""y"": [0, 1], ""quantile"": 0}, dims=""y""\n    )\n    assert_identical(expected_y, actual_y)\n\n    actual_xx = array.groupby(""x"").quantile(0)\n    expected_xx = xr.DataArray(\n        data=[[1, 11, 22], [4, 15, 24]],\n        coords={""x"": [1, 2], ""y"": [0, 0, 1], ""quantile"": 0},\n        dims=(""x"", ""y""),\n    )\n    assert_identical(expected_xx, actual_xx)\n\n    actual_yy = array.groupby(""y"").quantile(0)\n    expected_yy = xr.DataArray(\n        data=[[1, 26], [2, 22], [3, 23], [4, 24], [5, 25]],\n        coords={""x"": [1, 1, 1, 2, 2], ""y"": [0, 1], ""quantile"": 0},\n        dims=(""x"", ""y""),\n    )\n    assert_identical(expected_yy, actual_yy)\n\n    times = pd.date_range(""2000-01-01"", periods=365)\n    x = [0, 1]\n    foo = xr.DataArray(\n        np.reshape(np.arange(365 * 2), (365, 2)),\n        coords={""time"": times, ""x"": x},\n        dims=(""time"", ""x""),\n    )\n    g = foo.groupby(foo.time.dt.month)\n\n    actual = g.quantile(0, dim=...)\n    expected = xr.DataArray(\n        data=[\n            0.0,\n            62.0,\n            120.0,\n            182.0,\n            242.0,\n            304.0,\n            364.0,\n            426.0,\n            488.0,\n            548.0,\n            610.0,\n            670.0,\n        ],\n        coords={""month"": np.arange(1, 13), ""quantile"": 0},\n        dims=""month"",\n    )\n    assert_identical(expected, actual)\n\n    actual = g.quantile(0, dim=""time"")[:2]\n    expected = xr.DataArray(\n        data=[[0.0, 1], [62.0, 63]],\n        coords={""month"": [1, 2], ""x"": [0, 1], ""quantile"": 0},\n        dims=(""month"", ""x""),\n    )\n    assert_identical(expected, actual)\n\n\ndef test_ds_groupby_quantile():\n    ds = xr.Dataset(\n        data_vars={""a"": (""x"", [1, 2, 3, 4, 5, 6])}, coords={""x"": [1, 1, 1, 2, 2, 2]}\n    )\n\n    # Scalar quantile\n    expected = xr.Dataset(\n        data_vars={""a"": (""x"", [2, 5])}, coords={""quantile"": 0.5, ""x"": [1, 2]}\n    )\n    actual = ds.groupby(""x"").quantile(0.5)\n    assert_identical(expected, actual)\n\n    # Vector quantile\n    expected = xr.Dataset(\n        data_vars={""a"": ((""x"", ""quantile""), [[1, 3], [4, 6]])},\n        coords={""x"": [1, 2], ""quantile"": [0, 1]},\n    )\n    actual = ds.groupby(""x"").quantile([0, 1])\n    assert_identical(expected, actual)\n\n    # Multiple dimensions\n    ds = xr.Dataset(\n        data_vars={\n            ""a"": (\n                (""x"", ""y""),\n                [[1, 11, 26], [2, 12, 22], [3, 13, 23], [4, 16, 24], [5, 15, 25]],\n            )\n        },\n        coords={""x"": [1, 1, 1, 2, 2], ""y"": [0, 0, 1]},\n    )\n\n    actual_x = ds.groupby(""x"").quantile(0, dim=...)\n    expected_x = xr.Dataset({""a"": (""x"", [1, 4])}, coords={""x"": [1, 2], ""quantile"": 0})\n    assert_identical(expected_x, actual_x)\n\n    actual_y = ds.groupby(""y"").quantile(0, dim=...)\n    expected_y = xr.Dataset({""a"": (""y"", [1, 22])}, coords={""y"": [0, 1], ""quantile"": 0})\n    assert_identical(expected_y, actual_y)\n\n    actual_xx = ds.groupby(""x"").quantile(0)\n    expected_xx = xr.Dataset(\n        {""a"": ((""x"", ""y""), [[1, 11, 22], [4, 15, 24]])},\n        coords={""x"": [1, 2], ""y"": [0, 0, 1], ""quantile"": 0},\n    )\n    assert_identical(expected_xx, actual_xx)\n\n    actual_yy = ds.groupby(""y"").quantile(0)\n    expected_yy = xr.Dataset(\n        {""a"": ((""x"", ""y""), [[1, 26], [2, 22], [3, 23], [4, 24], [5, 25]])},\n        coords={""x"": [1, 1, 1, 2, 2], ""y"": [0, 1], ""quantile"": 0},\n    ).transpose()\n    assert_identical(expected_yy, actual_yy)\n\n    times = pd.date_range(""2000-01-01"", periods=365)\n    x = [0, 1]\n    foo = xr.Dataset(\n        {""a"": ((""time"", ""x""), np.reshape(np.arange(365 * 2), (365, 2)))},\n        coords=dict(time=times, x=x),\n    )\n    g = foo.groupby(foo.time.dt.month)\n\n    actual = g.quantile(0, dim=...)\n    expected = xr.Dataset(\n        {\n            ""a"": (\n                ""month"",\n                [\n                    0.0,\n                    62.0,\n                    120.0,\n                    182.0,\n                    242.0,\n                    304.0,\n                    364.0,\n                    426.0,\n                    488.0,\n                    548.0,\n                    610.0,\n                    670.0,\n                ],\n            )\n        },\n        coords={""month"": np.arange(1, 13), ""quantile"": 0},\n    )\n    assert_identical(expected, actual)\n\n    actual = g.quantile(0, dim=""time"").isel(month=slice(None, 2))\n    expected = xr.Dataset(\n        data_vars={""a"": ((""month"", ""x""), [[0.0, 1], [62.0, 63]])},\n        coords={""month"": [1, 2], ""x"": [0, 1], ""quantile"": 0},\n    )\n    assert_identical(expected, actual)\n\n\ndef test_da_groupby_assign_coords():\n    actual = xr.DataArray(\n        [[3, 4, 5], [6, 7, 8]], dims=[""y"", ""x""], coords={""y"": range(2), ""x"": range(3)}\n    )\n    actual1 = actual.groupby(""x"").assign_coords({""y"": [-1, -2]})\n    actual2 = actual.groupby(""x"").assign_coords(y=[-1, -2])\n    expected = xr.DataArray(\n        [[3, 4, 5], [6, 7, 8]], dims=[""y"", ""x""], coords={""y"": [-1, -2], ""x"": range(3)}\n    )\n    assert_identical(expected, actual1)\n    assert_identical(expected, actual2)\n\n\nrepr_da = xr.DataArray(\n    np.random.randn(10, 20, 6, 24),\n    dims=[""x"", ""y"", ""z"", ""t""],\n    coords={\n        ""z"": [""a"", ""b"", ""c"", ""a"", ""b"", ""c""],\n        ""x"": [1, 1, 1, 2, 2, 3, 4, 5, 3, 4],\n        ""t"": pd.date_range(""2001-01-01"", freq=""M"", periods=24),\n        ""month"": (""t"", list(range(1, 13)) * 2),\n    },\n)\n\n\n@pytest.mark.parametrize(""dim"", [""x"", ""y"", ""z"", ""month""])\n@pytest.mark.parametrize(""obj"", [repr_da, repr_da.to_dataset(name=""a"")])\ndef test_groupby_repr(obj, dim):\n    actual = repr(obj.groupby(dim))\n    expected = ""%sGroupBy"" % obj.__class__.__name__\n    expected += "", grouped over %r "" % dim\n    expected += ""\\n%r groups with labels "" % (len(np.unique(obj[dim])))\n    if dim == ""x"":\n        expected += ""1, 2, 3, 4, 5.""\n    elif dim == ""y"":\n        expected += ""0, 1, 2, 3, 4, 5, ..., 15, 16, 17, 18, 19.""\n    elif dim == ""z"":\n        expected += ""\'a\', \'b\', \'c\'.""\n    elif dim == ""month"":\n        expected += ""1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12.""\n    assert actual == expected\n\n\n@pytest.mark.parametrize(""obj"", [repr_da, repr_da.to_dataset(name=""a"")])\ndef test_groupby_repr_datetime(obj):\n    actual = repr(obj.groupby(""t.month""))\n    expected = ""%sGroupBy"" % obj.__class__.__name__\n    expected += "", grouped over \'month\' ""\n    expected += ""\\n%r groups with labels "" % (len(np.unique(obj.t.dt.month)))\n    expected += ""1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12.""\n    assert actual == expected\n\n\ndef test_groupby_drops_nans():\n    # GH2383\n    # nan in 2D data variable (requires stacking)\n    ds = xr.Dataset(\n        {\n            ""variable"": ((""lat"", ""lon"", ""time""), np.arange(60.0).reshape((4, 3, 5))),\n            ""id"": ((""lat"", ""lon""), np.arange(12.0).reshape((4, 3))),\n        },\n        coords={""lat"": np.arange(4), ""lon"": np.arange(3), ""time"": np.arange(5)},\n    )\n\n    ds[""id""].values[0, 0] = np.nan\n    ds[""id""].values[3, 0] = np.nan\n    ds[""id""].values[-1, -1] = np.nan\n\n    grouped = ds.groupby(ds.id)\n\n    # non reduction operation\n    expected = ds.copy()\n    expected.variable.values[0, 0, :] = np.nan\n    expected.variable.values[-1, -1, :] = np.nan\n    expected.variable.values[3, 0, :] = np.nan\n    actual = grouped.map(lambda x: x).transpose(*ds.variable.dims)\n    assert_identical(actual, expected)\n\n    # reduction along grouped dimension\n    actual = grouped.mean()\n    stacked = ds.stack({""xy"": [""lat"", ""lon""]})\n    expected = (\n        stacked.variable.where(stacked.id.notnull()).rename({""xy"": ""id""}).to_dataset()\n    )\n    expected[""id""] = stacked.id.values\n    assert_identical(actual, expected.dropna(""id"").transpose(*actual.dims))\n\n    # reduction operation along a different dimension\n    actual = grouped.mean(""time"")\n    with pytest.warns(RuntimeWarning):  # mean of empty slice\n        expected = ds.mean(""time"").where(ds.id.notnull())\n    assert_identical(actual, expected)\n\n    # NaN in non-dimensional coordinate\n    array = xr.DataArray([1, 2, 3], [(""x"", [1, 2, 3])])\n    array[""x1""] = (""x"", [1, 1, np.nan])\n    expected = xr.DataArray(3, [(""x1"", [1])])\n    actual = array.groupby(""x1"").sum()\n    assert_equal(expected, actual)\n\n    # NaT in non-dimensional coordinate\n    array[""t""] = (\n        ""x"",\n        [\n            np.datetime64(""2001-01-01""),\n            np.datetime64(""2001-01-01""),\n            np.datetime64(""NaT""),\n        ],\n    )\n    expected = xr.DataArray(3, [(""t"", [np.datetime64(""2001-01-01"")])])\n    actual = array.groupby(""t"").sum()\n    assert_equal(expected, actual)\n\n    # test for repeated coordinate labels\n    array = xr.DataArray([0, 1, 2, 4, 3, 4], [(""x"", [np.nan, 1, 1, np.nan, 2, np.nan])])\n    expected = xr.DataArray([3, 3], [(""x"", [1, 2])])\n    actual = array.groupby(""x"").sum()\n    assert_equal(expected, actual)\n\n\ndef test_groupby_grouping_errors():\n    dataset = xr.Dataset({""foo"": (""x"", [1, 1, 1])}, {""x"": [1, 2, 3]})\n    with raises_regex(ValueError, ""None of the data falls within bins with edges""):\n        dataset.groupby_bins(""x"", bins=[0.1, 0.2, 0.3])\n\n    with raises_regex(ValueError, ""None of the data falls within bins with edges""):\n        dataset.to_array().groupby_bins(""x"", bins=[0.1, 0.2, 0.3])\n\n    with raises_regex(ValueError, ""All bin edges are NaN.""):\n        dataset.groupby_bins(""x"", bins=[np.nan, np.nan, np.nan])\n\n    with raises_regex(ValueError, ""All bin edges are NaN.""):\n        dataset.to_array().groupby_bins(""x"", bins=[np.nan, np.nan, np.nan])\n\n    with raises_regex(ValueError, ""Failed to group data.""):\n        dataset.groupby(dataset.foo * np.nan)\n\n    with raises_regex(ValueError, ""Failed to group data.""):\n        dataset.to_array().groupby(dataset.foo * np.nan)\n\n\ndef test_groupby_reduce_dimension_error(array):\n    grouped = array.groupby(""y"")\n    with raises_regex(ValueError, ""cannot reduce over dimensions""):\n        grouped.mean()\n\n    with raises_regex(ValueError, ""cannot reduce over dimensions""):\n        grouped.mean(""huh"")\n\n    with raises_regex(ValueError, ""cannot reduce over dimensions""):\n        grouped.mean((""x"", ""y"", ""asd""))\n\n    grouped = array.groupby(""y"", squeeze=False)\n    assert_identical(array, grouped.mean())\n\n    assert_identical(array.mean(""x""), grouped.reduce(np.mean, ""x""))\n    assert_allclose(array.mean([""x"", ""z""]), grouped.reduce(np.mean, [""x"", ""z""]))\n\n\ndef test_groupby_multiple_string_args(array):\n    with pytest.raises(TypeError):\n        array.groupby(""x"", ""y"")\n\n\ndef test_groupby_bins_timeseries():\n    ds = xr.Dataset()\n    ds[""time""] = xr.DataArray(\n        pd.date_range(""2010-08-01"", ""2010-08-15"", freq=""15min""), dims=""time""\n    )\n    ds[""val""] = xr.DataArray(np.ones(*ds[""time""].shape), dims=""time"")\n    time_bins = pd.date_range(start=""2010-08-01"", end=""2010-08-15"", freq=""24H"")\n    actual = ds.groupby_bins(""time"", time_bins).sum()\n    expected = xr.DataArray(\n        96 * np.ones((14,)),\n        dims=[""time_bins""],\n        coords={""time_bins"": pd.cut(time_bins, time_bins).categories},\n    ).to_dataset(name=""val"")\n    assert_identical(actual, expected)\n\n\ndef test_groupby_none_group_name():\n    # GH158\n    # xarray should not fail if a DataArray\'s name attribute is None\n\n    data = np.arange(10) + 10\n    da = xr.DataArray(data)  # da.name = None\n    key = xr.DataArray(np.floor_divide(data, 2))\n\n    mean = da.groupby(key).mean()\n    assert ""group"" in mean.dims\n\n\n# TODO: move other groupby tests from test_dataset and test_dataarray over here\n'"
xarray/tests/test_indexing.py,112,"b'import itertools\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nfrom xarray import DataArray, Dataset, Variable\nfrom xarray.core import indexing, nputils\n\nfrom . import IndexerMaker, ReturnItem, assert_array_equal, raises_regex\n\nB = IndexerMaker(indexing.BasicIndexer)\n\n\nclass TestIndexers:\n    def set_to_zero(self, x, i):\n        x = x.copy()\n        x[i] = 0\n        return x\n\n    def test_expanded_indexer(self):\n        x = np.random.randn(10, 11, 12, 13, 14)\n        y = np.arange(5)\n        arr = ReturnItem()\n        for i in [\n            arr[:],\n            arr[...],\n            arr[0, :, 10],\n            arr[..., 10],\n            arr[:5, ..., 0],\n            arr[..., 0, :],\n            arr[y],\n            arr[y, y],\n            arr[..., y, y],\n            arr[..., 0, 1, 2, 3, 4],\n        ]:\n            j = indexing.expanded_indexer(i, x.ndim)\n            assert_array_equal(x[i], x[j])\n            assert_array_equal(self.set_to_zero(x, i), self.set_to_zero(x, j))\n        with raises_regex(IndexError, ""too many indices""):\n            indexing.expanded_indexer(arr[1, 2, 3], 2)\n\n    def test_asarray_tuplesafe(self):\n        res = indexing._asarray_tuplesafe((""a"", 1))\n        assert isinstance(res, np.ndarray)\n        assert res.ndim == 0\n        assert res.item() == (""a"", 1)\n\n        res = indexing._asarray_tuplesafe([(0,), (1,)])\n        assert res.shape == (2,)\n        assert res[0] == (0,)\n        assert res[1] == (1,)\n\n    def test_stacked_multiindex_min_max(self):\n        data = np.random.randn(3, 23, 4)\n        da = DataArray(\n            data,\n            name=""value"",\n            dims=[""replicate"", ""rsample"", ""exp""],\n            coords=dict(\n                replicate=[0, 1, 2], exp=[""a"", ""b"", ""c"", ""d""], rsample=list(range(23))\n            ),\n        )\n        da2 = da.stack(sample=(""replicate"", ""rsample""))\n        s = da2.sample\n        assert_array_equal(da2.loc[""a"", s.max()], data[2, 22, 0])\n        assert_array_equal(da2.loc[""b"", s.min()], data[0, 0, 1])\n\n    def test_convert_label_indexer(self):\n        # TODO: add tests that aren\'t just for edge cases\n        index = pd.Index([1, 2, 3])\n        with raises_regex(KeyError, ""not all values found""):\n            indexing.convert_label_indexer(index, [0])\n        with pytest.raises(KeyError):\n            indexing.convert_label_indexer(index, 0)\n        with raises_regex(ValueError, ""does not have a MultiIndex""):\n            indexing.convert_label_indexer(index, {""one"": 0})\n\n        mindex = pd.MultiIndex.from_product([[""a"", ""b""], [1, 2]], names=(""one"", ""two""))\n        with raises_regex(KeyError, ""not all values found""):\n            indexing.convert_label_indexer(mindex, [0])\n        with pytest.raises(KeyError):\n            indexing.convert_label_indexer(mindex, 0)\n        with pytest.raises(ValueError):\n            indexing.convert_label_indexer(index, {""three"": 0})\n        with pytest.raises(IndexError):\n            indexing.convert_label_indexer(mindex, (slice(None), 1, ""no_level""))\n\n    def test_convert_unsorted_datetime_index_raises(self):\n        index = pd.to_datetime([""2001"", ""2000"", ""2002""])\n        with pytest.raises(KeyError):\n            # pandas will try to convert this into an array indexer. We should\n            # raise instead, so we can be sure the result of indexing with a\n            # slice is always a view.\n            indexing.convert_label_indexer(index, slice(""2001"", ""2002""))\n\n    def test_get_dim_indexers(self):\n        mindex = pd.MultiIndex.from_product([[""a"", ""b""], [1, 2]], names=(""one"", ""two""))\n        mdata = DataArray(range(4), [(""x"", mindex)])\n\n        dim_indexers = indexing.get_dim_indexers(mdata, {""one"": ""a"", ""two"": 1})\n        assert dim_indexers == {""x"": {""one"": ""a"", ""two"": 1}}\n\n        with raises_regex(ValueError, ""cannot combine""):\n            indexing.get_dim_indexers(mdata, {""x"": ""a"", ""two"": 1})\n\n        with raises_regex(ValueError, ""do not exist""):\n            indexing.get_dim_indexers(mdata, {""y"": ""a""})\n\n        with raises_regex(ValueError, ""do not exist""):\n            indexing.get_dim_indexers(mdata, {""four"": 1})\n\n    def test_remap_label_indexers(self):\n        def test_indexer(data, x, expected_pos, expected_idx=None):\n            pos, idx = indexing.remap_label_indexers(data, {""x"": x})\n            assert_array_equal(pos.get(""x""), expected_pos)\n            assert_array_equal(idx.get(""x""), expected_idx)\n\n        data = Dataset({""x"": (""x"", [1, 2, 3])})\n        mindex = pd.MultiIndex.from_product(\n            [[""a"", ""b""], [1, 2], [-1, -2]], names=(""one"", ""two"", ""three"")\n        )\n        mdata = DataArray(range(8), [(""x"", mindex)])\n\n        test_indexer(data, 1, 0)\n        test_indexer(data, np.int32(1), 0)\n        test_indexer(data, Variable([], 1), 0)\n        test_indexer(mdata, (""a"", 1, -1), 0)\n        test_indexer(\n            mdata,\n            (""a"", 1),\n            [True, True, False, False, False, False, False, False],\n            [-1, -2],\n        )\n        test_indexer(\n            mdata,\n            ""a"",\n            slice(0, 4, None),\n            pd.MultiIndex.from_product([[1, 2], [-1, -2]]),\n        )\n        test_indexer(\n            mdata,\n            (""a"",),\n            [True, True, True, True, False, False, False, False],\n            pd.MultiIndex.from_product([[1, 2], [-1, -2]]),\n        )\n        test_indexer(mdata, [(""a"", 1, -1), (""b"", 2, -2)], [0, 7])\n        test_indexer(mdata, slice(""a"", ""b""), slice(0, 8, None))\n        test_indexer(mdata, slice((""a"", 1), (""b"", 1)), slice(0, 6, None))\n        test_indexer(mdata, {""one"": ""a"", ""two"": 1, ""three"": -1}, 0)\n        test_indexer(\n            mdata,\n            {""one"": ""a"", ""two"": 1},\n            [True, True, False, False, False, False, False, False],\n            [-1, -2],\n        )\n        test_indexer(\n            mdata,\n            {""one"": ""a"", ""three"": -1},\n            [True, False, True, False, False, False, False, False],\n            [1, 2],\n        )\n        test_indexer(\n            mdata,\n            {""one"": ""a""},\n            [True, True, True, True, False, False, False, False],\n            pd.MultiIndex.from_product([[1, 2], [-1, -2]]),\n        )\n\n    def test_read_only_view(self):\n\n        arr = DataArray(\n            np.random.rand(3, 3),\n            coords={""x"": np.arange(3), ""y"": np.arange(3)},\n            dims=(""x"", ""y""),\n        )  # Create a 2D DataArray\n        arr = arr.expand_dims({""z"": 3}, -1)  # New dimension \'z\'\n        arr[""z""] = np.arange(3)  # New coords to dimension \'z\'\n        with pytest.raises(ValueError, match=""Do you want to .copy()""):\n            arr.loc[0, 0, 0] = 999\n\n\nclass TestLazyArray:\n    def test_slice_slice(self):\n        arr = ReturnItem()\n        for size in [100, 99]:\n            # We test even/odd size cases\n            x = np.arange(size)\n            slices = [\n                arr[:3],\n                arr[:4],\n                arr[2:4],\n                arr[:1],\n                arr[:-1],\n                arr[5:-1],\n                arr[-5:-1],\n                arr[::-1],\n                arr[5::-1],\n                arr[:3:-1],\n                arr[:30:-1],\n                arr[10:4:],\n                arr[::4],\n                arr[4:4:4],\n                arr[:4:-4],\n                arr[::-2],\n            ]\n            for i in slices:\n                for j in slices:\n                    expected = x[i][j]\n                    new_slice = indexing.slice_slice(i, j, size=size)\n                    actual = x[new_slice]\n                    assert_array_equal(expected, actual)\n\n    def test_lazily_indexed_array(self):\n        original = np.random.rand(10, 20, 30)\n        x = indexing.NumpyIndexingAdapter(original)\n        v = Variable([""i"", ""j"", ""k""], original)\n        lazy = indexing.LazilyOuterIndexedArray(x)\n        v_lazy = Variable([""i"", ""j"", ""k""], lazy)\n        arr = ReturnItem()\n        # test orthogonally applied indexers\n        indexers = [arr[:], 0, -2, arr[:3], [0, 1, 2, 3], [0], np.arange(10) < 5]\n        for i in indexers:\n            for j in indexers:\n                for k in indexers:\n                    if isinstance(j, np.ndarray) and j.dtype.kind == ""b"":\n                        j = np.arange(20) < 5\n                    if isinstance(k, np.ndarray) and k.dtype.kind == ""b"":\n                        k = np.arange(30) < 5\n                    expected = np.asarray(v[i, j, k])\n                    for actual in [\n                        v_lazy[i, j, k],\n                        v_lazy[:, j, k][i],\n                        v_lazy[:, :, k][:, j][i],\n                    ]:\n                        assert expected.shape == actual.shape\n                        assert_array_equal(expected, actual)\n                        assert isinstance(\n                            actual._data, indexing.LazilyOuterIndexedArray\n                        )\n\n                        # make sure actual.key is appropriate type\n                        if all(\n                            isinstance(k, (int, slice)) for k in v_lazy._data.key.tuple\n                        ):\n                            assert isinstance(v_lazy._data.key, indexing.BasicIndexer)\n                        else:\n                            assert isinstance(v_lazy._data.key, indexing.OuterIndexer)\n\n        # test sequentially applied indexers\n        indexers = [\n            (3, 2),\n            (arr[:], 0),\n            (arr[:2], -1),\n            (arr[:4], [0]),\n            ([4, 5], 0),\n            ([0, 1, 2], [0, 1]),\n            ([0, 3, 5], arr[:2]),\n        ]\n        for i, j in indexers:\n            expected = v[i][j]\n            actual = v_lazy[i][j]\n            assert expected.shape == actual.shape\n            assert_array_equal(expected, actual)\n\n            # test transpose\n            if actual.ndim > 1:\n                order = np.random.choice(actual.ndim, actual.ndim)\n                order = np.array(actual.dims)\n                transposed = actual.transpose(*order)\n                assert_array_equal(expected.transpose(*order), transposed)\n                assert isinstance(\n                    actual._data,\n                    (\n                        indexing.LazilyVectorizedIndexedArray,\n                        indexing.LazilyOuterIndexedArray,\n                    ),\n                )\n\n            assert isinstance(actual._data, indexing.LazilyOuterIndexedArray)\n            assert isinstance(actual._data.array, indexing.NumpyIndexingAdapter)\n\n    def test_vectorized_lazily_indexed_array(self):\n        original = np.random.rand(10, 20, 30)\n        x = indexing.NumpyIndexingAdapter(original)\n        v_eager = Variable([""i"", ""j"", ""k""], x)\n        lazy = indexing.LazilyOuterIndexedArray(x)\n        v_lazy = Variable([""i"", ""j"", ""k""], lazy)\n        arr = ReturnItem()\n\n        def check_indexing(v_eager, v_lazy, indexers):\n            for indexer in indexers:\n                actual = v_lazy[indexer]\n                expected = v_eager[indexer]\n                assert expected.shape == actual.shape\n                assert isinstance(\n                    actual._data,\n                    (\n                        indexing.LazilyVectorizedIndexedArray,\n                        indexing.LazilyOuterIndexedArray,\n                    ),\n                )\n                assert_array_equal(expected, actual)\n                v_eager = expected\n                v_lazy = actual\n\n        # test orthogonal indexing\n        indexers = [(arr[:], 0, 1), (Variable(""i"", [0, 1]),)]\n        check_indexing(v_eager, v_lazy, indexers)\n\n        # vectorized indexing\n        indexers = [\n            (Variable(""i"", [0, 1]), Variable(""i"", [0, 1]), slice(None)),\n            (slice(1, 3, 2), 0),\n        ]\n        check_indexing(v_eager, v_lazy, indexers)\n\n        indexers = [\n            (slice(None, None, 2), 0, slice(None, 10)),\n            (Variable(""i"", [3, 2, 4, 3]), Variable(""i"", [3, 2, 1, 0])),\n            (Variable([""i"", ""j""], [[0, 1], [1, 2]]),),\n        ]\n        check_indexing(v_eager, v_lazy, indexers)\n\n        indexers = [\n            (Variable(""i"", [3, 2, 4, 3]), Variable(""i"", [3, 2, 1, 0])),\n            (Variable([""i"", ""j""], [[0, 1], [1, 2]]),),\n        ]\n        check_indexing(v_eager, v_lazy, indexers)\n\n\nclass TestCopyOnWriteArray:\n    def test_setitem(self):\n        original = np.arange(10)\n        wrapped = indexing.CopyOnWriteArray(original)\n        wrapped[B[:]] = 0\n        assert_array_equal(original, np.arange(10))\n        assert_array_equal(wrapped, np.zeros(10))\n\n    def test_sub_array(self):\n        original = np.arange(10)\n        wrapped = indexing.CopyOnWriteArray(original)\n        child = wrapped[B[:5]]\n        assert isinstance(child, indexing.CopyOnWriteArray)\n        child[B[:]] = 0\n        assert_array_equal(original, np.arange(10))\n        assert_array_equal(wrapped, np.arange(10))\n        assert_array_equal(child, np.zeros(5))\n\n    def test_index_scalar(self):\n        # regression test for GH1374\n        x = indexing.CopyOnWriteArray(np.array([""foo"", ""bar""]))\n        assert np.array(x[B[0]][B[()]]) == ""foo""\n\n\nclass TestMemoryCachedArray:\n    def test_wrapper(self):\n        original = indexing.LazilyOuterIndexedArray(np.arange(10))\n        wrapped = indexing.MemoryCachedArray(original)\n        assert_array_equal(wrapped, np.arange(10))\n        assert isinstance(wrapped.array, indexing.NumpyIndexingAdapter)\n\n    def test_sub_array(self):\n        original = indexing.LazilyOuterIndexedArray(np.arange(10))\n        wrapped = indexing.MemoryCachedArray(original)\n        child = wrapped[B[:5]]\n        assert isinstance(child, indexing.MemoryCachedArray)\n        assert_array_equal(child, np.arange(5))\n        assert isinstance(child.array, indexing.NumpyIndexingAdapter)\n        assert isinstance(wrapped.array, indexing.LazilyOuterIndexedArray)\n\n    def test_setitem(self):\n        original = np.arange(10)\n        wrapped = indexing.MemoryCachedArray(original)\n        wrapped[B[:]] = 0\n        assert_array_equal(original, np.zeros(10))\n\n    def test_index_scalar(self):\n        # regression test for GH1374\n        x = indexing.MemoryCachedArray(np.array([""foo"", ""bar""]))\n        assert np.array(x[B[0]][B[()]]) == ""foo""\n\n\ndef test_base_explicit_indexer():\n    with pytest.raises(TypeError):\n        indexing.ExplicitIndexer(())\n\n    class Subclass(indexing.ExplicitIndexer):\n        pass\n\n    value = Subclass((1, 2, 3))\n    assert value.tuple == (1, 2, 3)\n    assert repr(value) == ""Subclass((1, 2, 3))""\n\n\n@pytest.mark.parametrize(\n    ""indexer_cls"",\n    [indexing.BasicIndexer, indexing.OuterIndexer, indexing.VectorizedIndexer],\n)\ndef test_invalid_for_all(indexer_cls):\n    with pytest.raises(TypeError):\n        indexer_cls(None)\n    with pytest.raises(TypeError):\n        indexer_cls(([],))\n    with pytest.raises(TypeError):\n        indexer_cls((None,))\n    with pytest.raises(TypeError):\n        indexer_cls((""foo"",))\n    with pytest.raises(TypeError):\n        indexer_cls((1.0,))\n    with pytest.raises(TypeError):\n        indexer_cls((slice(""foo""),))\n    with pytest.raises(TypeError):\n        indexer_cls((np.array([""foo""]),))\n\n\ndef check_integer(indexer_cls):\n    value = indexer_cls((1, np.uint64(2))).tuple\n    assert all(isinstance(v, int) for v in value)\n    assert value == (1, 2)\n\n\ndef check_slice(indexer_cls):\n    (value,) = indexer_cls((slice(1, None, np.int64(2)),)).tuple\n    assert value == slice(1, None, 2)\n    assert isinstance(value.step, int)\n\n\ndef check_array1d(indexer_cls):\n    (value,) = indexer_cls((np.arange(3, dtype=np.int32),)).tuple\n    assert value.dtype == np.int64\n    np.testing.assert_array_equal(value, [0, 1, 2])\n\n\ndef check_array2d(indexer_cls):\n    array = np.array([[1, 2], [3, 4]], dtype=np.int64)\n    (value,) = indexer_cls((array,)).tuple\n    assert value.dtype == np.int64\n    np.testing.assert_array_equal(value, array)\n\n\ndef test_basic_indexer():\n    check_integer(indexing.BasicIndexer)\n    check_slice(indexing.BasicIndexer)\n    with pytest.raises(TypeError):\n        check_array1d(indexing.BasicIndexer)\n    with pytest.raises(TypeError):\n        check_array2d(indexing.BasicIndexer)\n\n\ndef test_outer_indexer():\n    check_integer(indexing.OuterIndexer)\n    check_slice(indexing.OuterIndexer)\n    check_array1d(indexing.OuterIndexer)\n    with pytest.raises(TypeError):\n        check_array2d(indexing.OuterIndexer)\n\n\ndef test_vectorized_indexer():\n    with pytest.raises(TypeError):\n        check_integer(indexing.VectorizedIndexer)\n    check_slice(indexing.VectorizedIndexer)\n    check_array1d(indexing.VectorizedIndexer)\n    check_array2d(indexing.VectorizedIndexer)\n    with raises_regex(ValueError, ""numbers of dimensions""):\n        indexing.VectorizedIndexer(\n            (np.array(1, dtype=np.int64), np.arange(5, dtype=np.int64))\n        )\n\n\nclass Test_vectorized_indexer:\n    @pytest.fixture(autouse=True)\n    def setup(self):\n        self.data = indexing.NumpyIndexingAdapter(np.random.randn(10, 12, 13))\n        self.indexers = [\n            np.array([[0, 3, 2]]),\n            np.array([[0, 3, 3], [4, 6, 7]]),\n            slice(2, -2, 2),\n            slice(2, -2, 3),\n            slice(None),\n        ]\n\n    def test_arrayize_vectorized_indexer(self):\n        for i, j, k in itertools.product(self.indexers, repeat=3):\n            vindex = indexing.VectorizedIndexer((i, j, k))\n            vindex_array = indexing._arrayize_vectorized_indexer(\n                vindex, self.data.shape\n            )\n            np.testing.assert_array_equal(self.data[vindex], self.data[vindex_array])\n\n        actual = indexing._arrayize_vectorized_indexer(\n            indexing.VectorizedIndexer((slice(None),)), shape=(5,)\n        )\n        np.testing.assert_array_equal(actual.tuple, [np.arange(5)])\n\n        actual = indexing._arrayize_vectorized_indexer(\n            indexing.VectorizedIndexer((np.arange(5),) * 3), shape=(8, 10, 12)\n        )\n        expected = np.stack([np.arange(5)] * 3)\n        np.testing.assert_array_equal(np.stack(actual.tuple), expected)\n\n        actual = indexing._arrayize_vectorized_indexer(\n            indexing.VectorizedIndexer((np.arange(5), slice(None))), shape=(8, 10)\n        )\n        a, b = actual.tuple\n        np.testing.assert_array_equal(a, np.arange(5)[:, np.newaxis])\n        np.testing.assert_array_equal(b, np.arange(10)[np.newaxis, :])\n\n        actual = indexing._arrayize_vectorized_indexer(\n            indexing.VectorizedIndexer((slice(None), np.arange(5))), shape=(8, 10)\n        )\n        a, b = actual.tuple\n        np.testing.assert_array_equal(a, np.arange(8)[np.newaxis, :])\n        np.testing.assert_array_equal(b, np.arange(5)[:, np.newaxis])\n\n\ndef get_indexers(shape, mode):\n    if mode == ""vectorized"":\n        indexed_shape = (3, 4)\n        indexer = tuple(np.random.randint(0, s, size=indexed_shape) for s in shape)\n        return indexing.VectorizedIndexer(indexer)\n\n    elif mode == ""outer"":\n        indexer = tuple(np.random.randint(0, s, s + 2) for s in shape)\n        return indexing.OuterIndexer(indexer)\n\n    elif mode == ""outer_scalar"":\n        indexer = (np.random.randint(0, 3, 4), 0, slice(None, None, 2))\n        return indexing.OuterIndexer(indexer[: len(shape)])\n\n    elif mode == ""outer_scalar2"":\n        indexer = (np.random.randint(0, 3, 4), -2, slice(None, None, 2))\n        return indexing.OuterIndexer(indexer[: len(shape)])\n\n    elif mode == ""outer1vec"":\n        indexer = [slice(2, -3) for s in shape]\n        indexer[1] = np.random.randint(0, shape[1], shape[1] + 2)\n        return indexing.OuterIndexer(tuple(indexer))\n\n    elif mode == ""basic"":  # basic indexer\n        indexer = [slice(2, -3) for s in shape]\n        indexer[0] = 3\n        return indexing.BasicIndexer(tuple(indexer))\n\n    elif mode == ""basic1"":  # basic indexer\n        return indexing.BasicIndexer((3,))\n\n    elif mode == ""basic2"":  # basic indexer\n        indexer = [0, 2, 4]\n        return indexing.BasicIndexer(tuple(indexer[: len(shape)]))\n\n    elif mode == ""basic3"":  # basic indexer\n        indexer = [slice(None) for s in shape]\n        indexer[0] = slice(-2, 2, -2)\n        indexer[1] = slice(1, -1, 2)\n        return indexing.BasicIndexer(tuple(indexer[: len(shape)]))\n\n\n@pytest.mark.parametrize(""size"", [100, 99])\n@pytest.mark.parametrize(\n    ""sl"", [slice(1, -1, 1), slice(None, -1, 2), slice(-1, 1, -1), slice(-1, 1, -2)]\n)\ndef test_decompose_slice(size, sl):\n    x = np.arange(size)\n    slice1, slice2 = indexing._decompose_slice(sl, size)\n    expected = x[sl]\n    actual = x[slice1][slice2]\n    assert_array_equal(expected, actual)\n\n\n@pytest.mark.parametrize(""shape"", [(10, 5, 8), (10, 3)])\n@pytest.mark.parametrize(\n    ""indexer_mode"",\n    [\n        ""vectorized"",\n        ""outer"",\n        ""outer_scalar"",\n        ""outer_scalar2"",\n        ""outer1vec"",\n        ""basic"",\n        ""basic1"",\n        ""basic2"",\n        ""basic3"",\n    ],\n)\n@pytest.mark.parametrize(\n    ""indexing_support"",\n    [\n        indexing.IndexingSupport.BASIC,\n        indexing.IndexingSupport.OUTER,\n        indexing.IndexingSupport.OUTER_1VECTOR,\n        indexing.IndexingSupport.VECTORIZED,\n    ],\n)\ndef test_decompose_indexers(shape, indexer_mode, indexing_support):\n    data = np.random.randn(*shape)\n    indexer = get_indexers(shape, indexer_mode)\n\n    backend_ind, np_ind = indexing.decompose_indexer(indexer, shape, indexing_support)\n\n    expected = indexing.NumpyIndexingAdapter(data)[indexer]\n    array = indexing.NumpyIndexingAdapter(data)[backend_ind]\n    if len(np_ind.tuple) > 0:\n        array = indexing.NumpyIndexingAdapter(array)[np_ind]\n    np.testing.assert_array_equal(expected, array)\n\n    if not all(isinstance(k, indexing.integer_types) for k in np_ind.tuple):\n        combined_ind = indexing._combine_indexers(backend_ind, shape, np_ind)\n        array = indexing.NumpyIndexingAdapter(data)[combined_ind]\n        np.testing.assert_array_equal(expected, array)\n\n\ndef test_implicit_indexing_adapter():\n    array = np.arange(10, dtype=np.int64)\n    implicit = indexing.ImplicitToExplicitIndexingAdapter(\n        indexing.NumpyIndexingAdapter(array), indexing.BasicIndexer\n    )\n    np.testing.assert_array_equal(array, np.asarray(implicit))\n    np.testing.assert_array_equal(array, implicit[:])\n\n\ndef test_implicit_indexing_adapter_copy_on_write():\n    array = np.arange(10, dtype=np.int64)\n    implicit = indexing.ImplicitToExplicitIndexingAdapter(\n        indexing.CopyOnWriteArray(array)\n    )\n    assert isinstance(implicit[:], indexing.ImplicitToExplicitIndexingAdapter)\n\n\ndef test_outer_indexer_consistency_with_broadcast_indexes_vectorized():\n    def nonzero(x):\n        if isinstance(x, np.ndarray) and x.dtype.kind == ""b"":\n            x = x.nonzero()[0]\n        return x\n\n    original = np.random.rand(10, 20, 30)\n    v = Variable([""i"", ""j"", ""k""], original)\n    arr = ReturnItem()\n    # test orthogonally applied indexers\n    indexers = [\n        arr[:],\n        0,\n        -2,\n        arr[:3],\n        np.array([0, 1, 2, 3]),\n        np.array([0]),\n        np.arange(10) < 5,\n    ]\n    for i, j, k in itertools.product(indexers, repeat=3):\n\n        if isinstance(j, np.ndarray) and j.dtype.kind == ""b"":  # match size\n            j = np.arange(20) < 4\n        if isinstance(k, np.ndarray) and k.dtype.kind == ""b"":\n            k = np.arange(30) < 8\n\n        _, expected, new_order = v._broadcast_indexes_vectorized((i, j, k))\n        expected_data = nputils.NumpyVIndexAdapter(v.data)[expected.tuple]\n        if new_order:\n            old_order = range(len(new_order))\n            expected_data = np.moveaxis(expected_data, old_order, new_order)\n\n        outer_index = indexing.OuterIndexer((nonzero(i), nonzero(j), nonzero(k)))\n        actual = indexing._outer_to_numpy_indexer(outer_index, v.shape)\n        actual_data = v.data[actual]\n        np.testing.assert_array_equal(actual_data, expected_data)\n\n\ndef test_create_mask_outer_indexer():\n    indexer = indexing.OuterIndexer((np.array([0, -1, 2]),))\n    expected = np.array([False, True, False])\n    actual = indexing.create_mask(indexer, (5,))\n    np.testing.assert_array_equal(expected, actual)\n\n    indexer = indexing.OuterIndexer((1, slice(2), np.array([0, -1, 2])))\n    expected = np.array(2 * [[False, True, False]])\n    actual = indexing.create_mask(indexer, (5, 5, 5))\n    np.testing.assert_array_equal(expected, actual)\n\n\ndef test_create_mask_vectorized_indexer():\n    indexer = indexing.VectorizedIndexer((np.array([0, -1, 2]), np.array([0, 1, -1])))\n    expected = np.array([False, True, True])\n    actual = indexing.create_mask(indexer, (5,))\n    np.testing.assert_array_equal(expected, actual)\n\n    indexer = indexing.VectorizedIndexer(\n        (np.array([0, -1, 2]), slice(None), np.array([0, 1, -1]))\n    )\n    expected = np.array([[False, True, True]] * 2).T\n    actual = indexing.create_mask(indexer, (5, 2))\n    np.testing.assert_array_equal(expected, actual)\n\n\ndef test_create_mask_basic_indexer():\n    indexer = indexing.BasicIndexer((-1,))\n    actual = indexing.create_mask(indexer, (3,))\n    np.testing.assert_array_equal(True, actual)\n\n    indexer = indexing.BasicIndexer((0,))\n    actual = indexing.create_mask(indexer, (3,))\n    np.testing.assert_array_equal(False, actual)\n\n\ndef test_create_mask_dask():\n    da = pytest.importorskip(""dask.array"")\n\n    indexer = indexing.OuterIndexer((1, slice(2), np.array([0, -1, 2])))\n    expected = np.array(2 * [[False, True, False]])\n    actual = indexing.create_mask(\n        indexer, (5, 5, 5), da.empty((2, 3), chunks=((1, 1), (2, 1)))\n    )\n    assert actual.chunks == ((1, 1), (2, 1))\n    np.testing.assert_array_equal(expected, actual)\n\n    indexer = indexing.VectorizedIndexer(\n        (np.array([0, -1, 2]), slice(None), np.array([0, 1, -1]))\n    )\n    expected = np.array([[False, True, True]] * 2).T\n    actual = indexing.create_mask(\n        indexer, (5, 2), da.empty((3, 2), chunks=((3,), (2,)))\n    )\n    assert isinstance(actual, da.Array)\n    np.testing.assert_array_equal(expected, actual)\n\n    with pytest.raises(ValueError):\n        indexing.create_mask(indexer, (5, 2), da.empty((5,), chunks=(1,)))\n\n\ndef test_create_mask_error():\n    with raises_regex(TypeError, ""unexpected key type""):\n        indexing.create_mask((1, 2), (3, 4))\n\n\n@pytest.mark.parametrize(\n    ""indices, expected"",\n    [\n        (np.arange(5), np.arange(5)),\n        (np.array([0, -1, -1]), np.array([0, 0, 0])),\n        (np.array([-1, 1, -1]), np.array([1, 1, 1])),\n        (np.array([-1, -1, 2]), np.array([2, 2, 2])),\n        (np.array([-1]), np.array([0])),\n        (np.array([0, -1, 1, -1, -1]), np.array([0, 0, 1, 1, 1])),\n        (np.array([0, -1, -1, -1, 1]), np.array([0, 0, 0, 0, 1])),\n    ],\n)\ndef test_posify_mask_subindexer(indices, expected):\n    actual = indexing._posify_mask_subindexer(indices)\n    np.testing.assert_array_equal(expected, actual)\n'"
xarray/tests/test_interp.py,59,"b'import numpy as np\nimport pandas as pd\nimport pytest\n\nimport xarray as xr\nfrom xarray.tests import assert_allclose, assert_equal, requires_cftime, requires_scipy\n\nfrom ..coding.cftimeindex import _parse_array_of_cftime_strings\nfrom . import has_dask, has_scipy\nfrom .test_dataset import create_test_data\n\ntry:\n    import scipy\nexcept ImportError:\n    pass\n\n\ndef get_example_data(case):\n    x = np.linspace(0, 1, 100)\n    y = np.linspace(0, 0.1, 30)\n    data = xr.DataArray(\n        np.sin(x[:, np.newaxis]) * np.cos(y),\n        dims=[""x"", ""y""],\n        coords={""x"": x, ""y"": y, ""x2"": (""x"", x ** 2)},\n    )\n\n    if case == 0:\n        return data\n    elif case == 1:\n        return data.chunk({""y"": 3})\n    elif case == 2:\n        return data.chunk({""x"": 25, ""y"": 3})\n    elif case == 3:\n        x = np.linspace(0, 1, 100)\n        y = np.linspace(0, 0.1, 30)\n        z = np.linspace(0.1, 0.2, 10)\n        return xr.DataArray(\n            np.sin(x[:, np.newaxis, np.newaxis]) * np.cos(y[:, np.newaxis]) * z,\n            dims=[""x"", ""y"", ""z""],\n            coords={""x"": x, ""y"": y, ""x2"": (""x"", x ** 2), ""z"": z},\n        )\n    elif case == 4:\n        return get_example_data(3).chunk({""z"": 5})\n\n\ndef test_keywargs():\n    if not has_scipy:\n        pytest.skip(""scipy is not installed."")\n\n    da = get_example_data(0)\n    assert_equal(da.interp(x=[0.5, 0.8]), da.interp({""x"": [0.5, 0.8]}))\n\n\n@pytest.mark.parametrize(""method"", [""linear"", ""cubic""])\n@pytest.mark.parametrize(""dim"", [""x"", ""y""])\n@pytest.mark.parametrize(""case"", [0, 1])\ndef test_interpolate_1d(method, dim, case):\n    if not has_scipy:\n        pytest.skip(""scipy is not installed."")\n\n    if not has_dask and case in [1]:\n        pytest.skip(""dask is not installed in the environment."")\n\n    da = get_example_data(case)\n    xdest = np.linspace(0.0, 0.9, 80)\n\n    if dim == ""y"" and case == 1:\n        with pytest.raises(NotImplementedError):\n            actual = da.interp(method=method, **{dim: xdest})\n        pytest.skip(""interpolation along chunked dimension is "" ""not yet supported"")\n\n    actual = da.interp(method=method, **{dim: xdest})\n\n    # scipy interpolation for the reference\n    def func(obj, new_x):\n        return scipy.interpolate.interp1d(\n            da[dim],\n            obj.data,\n            axis=obj.get_axis_num(dim),\n            bounds_error=False,\n            fill_value=np.nan,\n            kind=method,\n        )(new_x)\n\n    if dim == ""x"":\n        coords = {""x"": xdest, ""y"": da[""y""], ""x2"": (""x"", func(da[""x2""], xdest))}\n    else:  # y\n        coords = {""x"": da[""x""], ""y"": xdest, ""x2"": da[""x2""]}\n\n    expected = xr.DataArray(func(da, xdest), dims=[""x"", ""y""], coords=coords)\n    assert_allclose(actual, expected)\n\n\n@pytest.mark.parametrize(""method"", [""cubic"", ""zero""])\ndef test_interpolate_1d_methods(method):\n    if not has_scipy:\n        pytest.skip(""scipy is not installed."")\n\n    da = get_example_data(0)\n    dim = ""x""\n    xdest = np.linspace(0.0, 0.9, 80)\n\n    actual = da.interp(method=method, **{dim: xdest})\n\n    # scipy interpolation for the reference\n    def func(obj, new_x):\n        return scipy.interpolate.interp1d(\n            da[dim],\n            obj.data,\n            axis=obj.get_axis_num(dim),\n            bounds_error=False,\n            fill_value=np.nan,\n            kind=method,\n        )(new_x)\n\n    coords = {""x"": xdest, ""y"": da[""y""], ""x2"": (""x"", func(da[""x2""], xdest))}\n    expected = xr.DataArray(func(da, xdest), dims=[""x"", ""y""], coords=coords)\n    assert_allclose(actual, expected)\n\n\n@pytest.mark.parametrize(""use_dask"", [False, True])\ndef test_interpolate_vectorize(use_dask):\n    if not has_scipy:\n        pytest.skip(""scipy is not installed."")\n\n    if not has_dask and use_dask:\n        pytest.skip(""dask is not installed in the environment."")\n\n    # scipy interpolation for the reference\n    def func(obj, dim, new_x):\n        shape = [s for i, s in enumerate(obj.shape) if i != obj.get_axis_num(dim)]\n        for s in new_x.shape[::-1]:\n            shape.insert(obj.get_axis_num(dim), s)\n\n        return scipy.interpolate.interp1d(\n            da[dim],\n            obj.data,\n            axis=obj.get_axis_num(dim),\n            bounds_error=False,\n            fill_value=np.nan,\n        )(new_x).reshape(shape)\n\n    da = get_example_data(0)\n    if use_dask:\n        da = da.chunk({""y"": 5})\n\n    # xdest is 1d but has different dimension\n    xdest = xr.DataArray(\n        np.linspace(0.1, 0.9, 30),\n        dims=""z"",\n        coords={""z"": np.random.randn(30), ""z2"": (""z"", np.random.randn(30))},\n    )\n\n    actual = da.interp(x=xdest, method=""linear"")\n\n    expected = xr.DataArray(\n        func(da, ""x"", xdest),\n        dims=[""z"", ""y""],\n        coords={\n            ""z"": xdest[""z""],\n            ""z2"": xdest[""z2""],\n            ""y"": da[""y""],\n            ""x"": (""z"", xdest.values),\n            ""x2"": (""z"", func(da[""x2""], ""x"", xdest)),\n        },\n    )\n    assert_allclose(actual, expected.transpose(""z"", ""y"", transpose_coords=True))\n\n    # xdest is 2d\n    xdest = xr.DataArray(\n        np.linspace(0.1, 0.9, 30).reshape(6, 5),\n        dims=[""z"", ""w""],\n        coords={\n            ""z"": np.random.randn(6),\n            ""w"": np.random.randn(5),\n            ""z2"": (""z"", np.random.randn(6)),\n        },\n    )\n\n    actual = da.interp(x=xdest, method=""linear"")\n\n    expected = xr.DataArray(\n        func(da, ""x"", xdest),\n        dims=[""z"", ""w"", ""y""],\n        coords={\n            ""z"": xdest[""z""],\n            ""w"": xdest[""w""],\n            ""z2"": xdest[""z2""],\n            ""y"": da[""y""],\n            ""x"": ((""z"", ""w""), xdest),\n            ""x2"": ((""z"", ""w""), func(da[""x2""], ""x"", xdest)),\n        },\n    )\n    assert_allclose(actual, expected.transpose(""z"", ""w"", ""y"", transpose_coords=True))\n\n\n@pytest.mark.parametrize(""case"", [3, 4])\ndef test_interpolate_nd(case):\n    if not has_scipy:\n        pytest.skip(""scipy is not installed."")\n\n    if not has_dask and case == 4:\n        pytest.skip(""dask is not installed in the environment."")\n\n    da = get_example_data(case)\n\n    # grid -> grid\n    xdest = np.linspace(0.1, 1.0, 11)\n    ydest = np.linspace(0.0, 0.2, 10)\n    actual = da.interp(x=xdest, y=ydest, method=""linear"")\n\n    # linear interpolation is separateable\n    expected = da.interp(x=xdest, method=""linear"")\n    expected = expected.interp(y=ydest, method=""linear"")\n    assert_allclose(actual.transpose(""x"", ""y"", ""z""), expected.transpose(""x"", ""y"", ""z""))\n\n    # grid -> 1d-sample\n    xdest = xr.DataArray(np.linspace(0.1, 1.0, 11), dims=""y"")\n    ydest = xr.DataArray(np.linspace(0.0, 0.2, 11), dims=""y"")\n    actual = da.interp(x=xdest, y=ydest, method=""linear"")\n\n    # linear interpolation is separateable\n    expected_data = scipy.interpolate.RegularGridInterpolator(\n        (da[""x""], da[""y""]),\n        da.transpose(""x"", ""y"", ""z"").values,\n        method=""linear"",\n        bounds_error=False,\n        fill_value=np.nan,\n    )(np.stack([xdest, ydest], axis=-1))\n    expected = xr.DataArray(\n        expected_data,\n        dims=[""y"", ""z""],\n        coords={\n            ""z"": da[""z""],\n            ""y"": ydest,\n            ""x"": (""y"", xdest.values),\n            ""x2"": da[""x2""].interp(x=xdest),\n        },\n    )\n    assert_allclose(actual.transpose(""y"", ""z""), expected)\n\n    # reversed order\n    actual = da.interp(y=ydest, x=xdest, method=""linear"")\n    assert_allclose(actual.transpose(""y"", ""z""), expected)\n\n\n@requires_scipy\ndef test_interpolate_nd_nd():\n    """"""Interpolate nd array with an nd indexer sharing coordinates.""""""\n    # Create original array\n    a = [0, 2]\n    x = [0, 1, 2]\n    da = xr.DataArray(\n        np.arange(6).reshape(2, 3), dims=(""a"", ""x""), coords={""a"": a, ""x"": x}\n    )\n\n    # Create indexer into `a` with dimensions (y, x)\n    y = [10]\n    c = {""x"": x, ""y"": y}\n    ia = xr.DataArray([[1, 2, 2]], dims=(""y"", ""x""), coords=c)\n    out = da.interp(a=ia)\n    expected = xr.DataArray([[1.5, 4, 5]], dims=(""y"", ""x""), coords=c)\n    xr.testing.assert_allclose(out.drop_vars(""a""), expected)\n\n    # If the *shared* indexing coordinates do not match, interp should fail.\n    with pytest.raises(ValueError):\n        c = {""x"": [1], ""y"": y}\n        ia = xr.DataArray([[1]], dims=(""y"", ""x""), coords=c)\n        da.interp(a=ia)\n\n    with pytest.raises(ValueError):\n        c = {""x"": [5, 6, 7], ""y"": y}\n        ia = xr.DataArray([[1]], dims=(""y"", ""x""), coords=c)\n        da.interp(a=ia)\n\n\n@pytest.mark.parametrize(""method"", [""linear""])\n@pytest.mark.parametrize(""case"", [0, 1])\ndef test_interpolate_scalar(method, case):\n    if not has_scipy:\n        pytest.skip(""scipy is not installed."")\n\n    if not has_dask and case in [1]:\n        pytest.skip(""dask is not installed in the environment."")\n\n    da = get_example_data(case)\n    xdest = 0.4\n\n    actual = da.interp(x=xdest, method=method)\n\n    # scipy interpolation for the reference\n    def func(obj, new_x):\n        return scipy.interpolate.interp1d(\n            da[""x""],\n            obj.data,\n            axis=obj.get_axis_num(""x""),\n            bounds_error=False,\n            fill_value=np.nan,\n        )(new_x)\n\n    coords = {""x"": xdest, ""y"": da[""y""], ""x2"": func(da[""x2""], xdest)}\n    expected = xr.DataArray(func(da, xdest), dims=[""y""], coords=coords)\n    assert_allclose(actual, expected)\n\n\n@pytest.mark.parametrize(""method"", [""linear""])\n@pytest.mark.parametrize(""case"", [3, 4])\ndef test_interpolate_nd_scalar(method, case):\n    if not has_scipy:\n        pytest.skip(""scipy is not installed."")\n\n    if not has_dask and case in [4]:\n        pytest.skip(""dask is not installed in the environment."")\n\n    da = get_example_data(case)\n    xdest = 0.4\n    ydest = 0.05\n\n    actual = da.interp(x=xdest, y=ydest, method=method)\n    # scipy interpolation for the reference\n    expected_data = scipy.interpolate.RegularGridInterpolator(\n        (da[""x""], da[""y""]),\n        da.transpose(""x"", ""y"", ""z"").values,\n        method=""linear"",\n        bounds_error=False,\n        fill_value=np.nan,\n    )(np.stack([xdest, ydest], axis=-1))\n\n    coords = {""x"": xdest, ""y"": ydest, ""x2"": da[""x2""].interp(x=xdest), ""z"": da[""z""]}\n    expected = xr.DataArray(expected_data[0], dims=[""z""], coords=coords)\n    assert_allclose(actual, expected)\n\n\n@pytest.mark.parametrize(""use_dask"", [True, False])\ndef test_nans(use_dask):\n    if not has_scipy:\n        pytest.skip(""scipy is not installed."")\n\n    da = xr.DataArray([0, 1, np.nan, 2], dims=""x"", coords={""x"": range(4)})\n\n    if not has_dask and use_dask:\n        pytest.skip(""dask is not installed in the environment."")\n        da = da.chunk()\n\n    actual = da.interp(x=[0.5, 1.5])\n    # not all values are nan\n    assert actual.count() > 0\n\n\n@pytest.mark.parametrize(""use_dask"", [True, False])\ndef test_errors(use_dask):\n    if not has_scipy:\n        pytest.skip(""scipy is not installed."")\n\n    # akima and spline are unavailable\n    da = xr.DataArray([0, 1, np.nan, 2], dims=""x"", coords={""x"": range(4)})\n    if not has_dask and use_dask:\n        pytest.skip(""dask is not installed in the environment."")\n        da = da.chunk()\n\n    for method in [""akima"", ""spline""]:\n        with pytest.raises(ValueError):\n            da.interp(x=[0.5, 1.5], method=method)\n\n    # not sorted\n    if use_dask:\n        da = get_example_data(3)\n    else:\n        da = get_example_data(0)\n\n    result = da.interp(x=[-1, 1, 3], kwargs={""fill_value"": 0.0})\n    assert not np.isnan(result.values).any()\n    result = da.interp(x=[-1, 1, 3])\n    assert np.isnan(result.values).any()\n\n    # invalid method\n    with pytest.raises(ValueError):\n        da.interp(x=[2, 0], method=""boo"")\n    with pytest.raises(ValueError):\n        da.interp(x=[2, 0], y=2, method=""cubic"")\n    with pytest.raises(ValueError):\n        da.interp(y=[2, 0], method=""boo"")\n\n    # object-type DataArray cannot be interpolated\n    da = xr.DataArray([""a"", ""b"", ""c""], dims=""x"", coords={""x"": [0, 1, 2]})\n    with pytest.raises(TypeError):\n        da.interp(x=0)\n\n\n@requires_scipy\ndef test_dtype():\n    ds = xr.Dataset(\n        {""var1"": (""x"", [0, 1, 2]), ""var2"": (""x"", [""a"", ""b"", ""c""])},\n        coords={""x"": [0.1, 0.2, 0.3], ""z"": (""x"", [""a"", ""b"", ""c""])},\n    )\n    actual = ds.interp(x=[0.15, 0.25])\n    assert ""var1"" in actual\n    assert ""var2"" not in actual\n    # object array should be dropped\n    assert ""z"" not in actual.coords\n\n\n@requires_scipy\ndef test_sorted():\n    # unsorted non-uniform gridded data\n    x = np.random.randn(100)\n    y = np.random.randn(30)\n    z = np.linspace(0.1, 0.2, 10) * 3.0\n    da = xr.DataArray(\n        np.cos(x[:, np.newaxis, np.newaxis]) * np.cos(y[:, np.newaxis]) * z,\n        dims=[""x"", ""y"", ""z""],\n        coords={""x"": x, ""y"": y, ""x2"": (""x"", x ** 2), ""z"": z},\n    )\n\n    x_new = np.linspace(0, 1, 30)\n    y_new = np.linspace(0, 1, 20)\n\n    da_sorted = da.sortby(""x"")\n    assert_allclose(da.interp(x=x_new), da_sorted.interp(x=x_new, assume_sorted=True))\n    da_sorted = da.sortby([""x"", ""y""])\n    assert_allclose(\n        da.interp(x=x_new, y=y_new),\n        da_sorted.interp(x=x_new, y=y_new, assume_sorted=True),\n    )\n\n    with pytest.raises(ValueError):\n        da.interp(x=[0, 1, 2], assume_sorted=True)\n\n\n@requires_scipy\ndef test_dimension_wo_coords():\n    da = xr.DataArray(\n        np.arange(12).reshape(3, 4), dims=[""x"", ""y""], coords={""y"": [0, 1, 2, 3]}\n    )\n    da_w_coord = da.copy()\n    da_w_coord[""x""] = np.arange(3)\n\n    assert_equal(da.interp(x=[0.1, 0.2, 0.3]), da_w_coord.interp(x=[0.1, 0.2, 0.3]))\n    assert_equal(\n        da.interp(x=[0.1, 0.2, 0.3], y=[0.5]),\n        da_w_coord.interp(x=[0.1, 0.2, 0.3], y=[0.5]),\n    )\n\n\n@requires_scipy\ndef test_dataset():\n    ds = create_test_data()\n    ds.attrs[""foo""] = ""var""\n    ds[""var1""].attrs[""buz""] = ""var2""\n    new_dim2 = xr.DataArray([0.11, 0.21, 0.31], dims=""z"")\n    interpolated = ds.interp(dim2=new_dim2)\n\n    assert_allclose(interpolated[""var1""], ds[""var1""].interp(dim2=new_dim2))\n    assert interpolated[""var3""].equals(ds[""var3""])\n\n    # make sure modifying interpolated does not affect the original dataset\n    interpolated[""var1""][:, 1] = 1.0\n    interpolated[""var2""][:, 1] = 1.0\n    interpolated[""var3""][:, 1] = 1.0\n\n    assert not interpolated[""var1""].equals(ds[""var1""])\n    assert not interpolated[""var2""].equals(ds[""var2""])\n    assert not interpolated[""var3""].equals(ds[""var3""])\n    # attrs should be kept\n    assert interpolated.attrs[""foo""] == ""var""\n    assert interpolated[""var1""].attrs[""buz""] == ""var2""\n\n\n@pytest.mark.parametrize(""case"", [0, 3])\ndef test_interpolate_dimorder(case):\n    """""" Make sure the resultant dimension order is consistent with .sel() """"""\n    if not has_scipy:\n        pytest.skip(""scipy is not installed."")\n\n    da = get_example_data(case)\n\n    new_x = xr.DataArray([0, 1, 2], dims=""x"")\n    assert da.interp(x=new_x).dims == da.sel(x=new_x, method=""nearest"").dims\n\n    new_y = xr.DataArray([0, 1, 2], dims=""y"")\n    actual = da.interp(x=new_x, y=new_y).dims\n    expected = da.sel(x=new_x, y=new_y, method=""nearest"").dims\n    assert actual == expected\n    # reversed order\n    actual = da.interp(y=new_y, x=new_x).dims\n    expected = da.sel(y=new_y, x=new_x, method=""nearest"").dims\n    assert actual == expected\n\n    new_x = xr.DataArray([0, 1, 2], dims=""a"")\n    assert da.interp(x=new_x).dims == da.sel(x=new_x, method=""nearest"").dims\n    assert da.interp(y=new_x).dims == da.sel(y=new_x, method=""nearest"").dims\n    new_y = xr.DataArray([0, 1, 2], dims=""a"")\n    actual = da.interp(x=new_x, y=new_y).dims\n    expected = da.sel(x=new_x, y=new_y, method=""nearest"").dims\n    assert actual == expected\n\n    new_x = xr.DataArray([[0], [1], [2]], dims=[""a"", ""b""])\n    assert da.interp(x=new_x).dims == da.sel(x=new_x, method=""nearest"").dims\n    assert da.interp(y=new_x).dims == da.sel(y=new_x, method=""nearest"").dims\n\n    if case == 3:\n        new_x = xr.DataArray([[0], [1], [2]], dims=[""a"", ""b""])\n        new_z = xr.DataArray([[0], [1], [2]], dims=[""a"", ""b""])\n        actual = da.interp(x=new_x, z=new_z).dims\n        expected = da.sel(x=new_x, z=new_z, method=""nearest"").dims\n        assert actual == expected\n\n        actual = da.interp(z=new_z, x=new_x).dims\n        expected = da.sel(z=new_z, x=new_x, method=""nearest"").dims\n        assert actual == expected\n\n        actual = da.interp(x=0.5, z=new_z).dims\n        expected = da.sel(x=0.5, z=new_z, method=""nearest"").dims\n        assert actual == expected\n\n\n@requires_scipy\ndef test_interp_like():\n    ds = create_test_data()\n    ds.attrs[""foo""] = ""var""\n    ds[""var1""].attrs[""buz""] = ""var2""\n\n    other = xr.DataArray(np.random.randn(3), dims=[""dim2""], coords={""dim2"": [0, 1, 2]})\n    interpolated = ds.interp_like(other)\n\n    assert_allclose(interpolated[""var1""], ds[""var1""].interp(dim2=other[""dim2""]))\n    assert_allclose(interpolated[""var1""], ds[""var1""].interp_like(other))\n    assert interpolated[""var3""].equals(ds[""var3""])\n\n    # attrs should be kept\n    assert interpolated.attrs[""foo""] == ""var""\n    assert interpolated[""var1""].attrs[""buz""] == ""var2""\n\n    other = xr.DataArray(\n        np.random.randn(3), dims=[""dim3""], coords={""dim3"": [""a"", ""b"", ""c""]}\n    )\n\n    actual = ds.interp_like(other)\n    expected = ds.reindex_like(other)\n    assert_allclose(actual, expected)\n\n\n@requires_scipy\n@pytest.mark.parametrize(\n    ""x_new, expected"",\n    [\n        (pd.date_range(""2000-01-02"", periods=3), [1, 2, 3]),\n        (\n            np.array(\n                [np.datetime64(""2000-01-01T12:00""), np.datetime64(""2000-01-02T12:00"")]\n            ),\n            [0.5, 1.5],\n        ),\n        ([""2000-01-01T12:00"", ""2000-01-02T12:00""], [0.5, 1.5]),\n        ([""2000-01-01T12:00""], 0.5),\n        pytest.param(""2000-01-01T12:00"", 0.5, marks=pytest.mark.xfail),\n    ],\n)\ndef test_datetime(x_new, expected):\n    da = xr.DataArray(\n        np.arange(24),\n        dims=""time"",\n        coords={""time"": pd.date_range(""2000-01-01"", periods=24)},\n    )\n\n    actual = da.interp(time=x_new)\n    expected_da = xr.DataArray(\n        np.atleast_1d(expected),\n        dims=[""time""],\n        coords={""time"": (np.atleast_1d(x_new).astype(""datetime64[ns]""))},\n    )\n\n    assert_allclose(actual, expected_da)\n\n\n@requires_scipy\ndef test_datetime_single_string():\n    da = xr.DataArray(\n        np.arange(24),\n        dims=""time"",\n        coords={""time"": pd.date_range(""2000-01-01"", periods=24)},\n    )\n    actual = da.interp(time=""2000-01-01T12:00"")\n    expected = xr.DataArray(0.5)\n\n    assert_allclose(actual.drop_vars(""time""), expected)\n\n\n@requires_cftime\n@requires_scipy\ndef test_cftime():\n    times = xr.cftime_range(""2000"", periods=24, freq=""D"")\n    da = xr.DataArray(np.arange(24), coords=[times], dims=""time"")\n\n    times_new = xr.cftime_range(""2000-01-01T12:00:00"", periods=3, freq=""D"")\n    actual = da.interp(time=times_new)\n    expected = xr.DataArray([0.5, 1.5, 2.5], coords=[times_new], dims=[""time""])\n\n    assert_allclose(actual, expected)\n\n\n@requires_cftime\n@requires_scipy\ndef test_cftime_type_error():\n    times = xr.cftime_range(""2000"", periods=24, freq=""D"")\n    da = xr.DataArray(np.arange(24), coords=[times], dims=""time"")\n\n    times_new = xr.cftime_range(\n        ""2000-01-01T12:00:00"", periods=3, freq=""D"", calendar=""noleap""\n    )\n    with pytest.raises(TypeError):\n        da.interp(time=times_new)\n\n\n@requires_cftime\n@requires_scipy\ndef test_cftime_list_of_strings():\n    from cftime import DatetimeProlepticGregorian\n\n    times = xr.cftime_range(\n        ""2000"", periods=24, freq=""D"", calendar=""proleptic_gregorian""\n    )\n    da = xr.DataArray(np.arange(24), coords=[times], dims=""time"")\n\n    times_new = [""2000-01-01T12:00"", ""2000-01-02T12:00"", ""2000-01-03T12:00""]\n    actual = da.interp(time=times_new)\n\n    times_new_array = _parse_array_of_cftime_strings(\n        np.array(times_new), DatetimeProlepticGregorian\n    )\n    expected = xr.DataArray([0.5, 1.5, 2.5], coords=[times_new_array], dims=[""time""])\n\n    assert_allclose(actual, expected)\n\n\n@requires_cftime\n@requires_scipy\ndef test_cftime_single_string():\n    from cftime import DatetimeProlepticGregorian\n\n    times = xr.cftime_range(\n        ""2000"", periods=24, freq=""D"", calendar=""proleptic_gregorian""\n    )\n    da = xr.DataArray(np.arange(24), coords=[times], dims=""time"")\n\n    times_new = ""2000-01-01T12:00""\n    actual = da.interp(time=times_new)\n\n    times_new_array = _parse_array_of_cftime_strings(\n        np.array(times_new), DatetimeProlepticGregorian\n    )\n    expected = xr.DataArray(0.5, coords={""time"": times_new_array})\n\n    assert_allclose(actual, expected)\n\n\n@requires_scipy\ndef test_datetime_to_non_datetime_error():\n    da = xr.DataArray(\n        np.arange(24),\n        dims=""time"",\n        coords={""time"": pd.date_range(""2000-01-01"", periods=24)},\n    )\n    with pytest.raises(TypeError):\n        da.interp(time=0.5)\n\n\n@requires_cftime\n@requires_scipy\ndef test_cftime_to_non_cftime_error():\n    times = xr.cftime_range(""2000"", periods=24, freq=""D"")\n    da = xr.DataArray(np.arange(24), coords=[times], dims=""time"")\n\n    with pytest.raises(TypeError):\n        da.interp(time=0.5)\n\n\n@requires_scipy\ndef test_datetime_interp_noerror():\n    # GH:2667\n    a = xr.DataArray(\n        np.arange(21).reshape(3, 7),\n        dims=[""x"", ""time""],\n        coords={\n            ""x"": [1, 2, 3],\n            ""time"": pd.date_range(""01-01-2001"", periods=7, freq=""D""),\n        },\n    )\n    xi = xr.DataArray(\n        np.linspace(1, 3, 50),\n        dims=[""time""],\n        coords={""time"": pd.date_range(""01-01-2001"", periods=50, freq=""H"")},\n    )\n    a.interp(x=xi, time=xi.time)  # should not raise an error\n\n\n@requires_cftime\ndef test_3641():\n    times = xr.cftime_range(""0001"", periods=3, freq=""500Y"")\n    da = xr.DataArray(range(3), dims=[""time""], coords=[times])\n    da.interp(time=[""0002-05-01""])\n\n\n@requires_scipy\n@pytest.mark.parametrize(""method"", [""nearest"", ""linear""])\ndef test_decompose(method):\n    da = xr.DataArray(\n        np.arange(6).reshape(3, 2),\n        dims=[""x"", ""y""],\n        coords={""x"": [0, 1, 2], ""y"": [-0.1, -0.3]},\n    )\n    x_new = xr.DataArray([0.5, 1.5, 2.5], dims=[""x1""])\n    y_new = xr.DataArray([-0.15, -0.25], dims=[""y1""])\n    x_broadcast, y_broadcast = xr.broadcast(x_new, y_new)\n    assert x_broadcast.ndim == 2\n\n    actual = da.interp(x=x_new, y=y_new, method=method).drop((""x"", ""y""))\n    expected = da.interp(x=x_broadcast, y=y_broadcast, method=method).drop((""x"", ""y""))\n    assert_allclose(actual, expected)\n'"
xarray/tests/test_merge.py,10,"b'import numpy as np\nimport pytest\n\nimport xarray as xr\nfrom xarray.core import dtypes, merge\nfrom xarray.core.merge import MergeError\nfrom xarray.testing import assert_identical\n\nfrom . import raises_regex\nfrom .test_dataset import create_test_data\n\n\nclass TestMergeInternals:\n    def test_broadcast_dimension_size(self):\n        actual = merge.broadcast_dimension_size(\n            [xr.Variable(""x"", [1]), xr.Variable(""y"", [2, 1])]\n        )\n        assert actual == {""x"": 1, ""y"": 2}\n\n        actual = merge.broadcast_dimension_size(\n            [xr.Variable((""x"", ""y""), [[1, 2]]), xr.Variable(""y"", [2, 1])]\n        )\n        assert actual == {""x"": 1, ""y"": 2}\n\n        with pytest.raises(ValueError):\n            merge.broadcast_dimension_size(\n                [xr.Variable((""x"", ""y""), [[1, 2]]), xr.Variable(""y"", [2])]\n            )\n\n\nclass TestMergeFunction:\n    def test_merge_arrays(self):\n        data = create_test_data()\n        actual = xr.merge([data.var1, data.var2])\n        expected = data[[""var1"", ""var2""]]\n        assert actual.identical(expected)\n\n    def test_merge_datasets(self):\n        data = create_test_data()\n\n        actual = xr.merge([data[[""var1""]], data[[""var2""]]])\n        expected = data[[""var1"", ""var2""]]\n        assert actual.identical(expected)\n\n        actual = xr.merge([data, data])\n        assert actual.identical(data)\n\n    def test_merge_dataarray_unnamed(self):\n        data = xr.DataArray([1, 2], dims=""x"")\n        with raises_regex(ValueError, ""without providing an explicit name""):\n            xr.merge([data])\n\n    def test_merge_arrays_attrs_default(self):\n        var1_attrs = {""a"": 1, ""b"": 2}\n        var2_attrs = {""a"": 1, ""c"": 3}\n        expected_attrs = {}\n\n        data = create_test_data()\n        data.var1.attrs = var1_attrs\n        data.var2.attrs = var2_attrs\n        actual = xr.merge([data.var1, data.var2])\n        expected = data[[""var1"", ""var2""]]\n        expected.attrs = expected_attrs\n        assert actual.identical(expected)\n\n    @pytest.mark.parametrize(\n        ""combine_attrs, var1_attrs, var2_attrs, expected_attrs, "" ""expect_exception"",\n        [\n            (\n                ""no_conflicts"",\n                {""a"": 1, ""b"": 2},\n                {""a"": 1, ""c"": 3},\n                {""a"": 1, ""b"": 2, ""c"": 3},\n                False,\n            ),\n            (""no_conflicts"", {""a"": 1, ""b"": 2}, {}, {""a"": 1, ""b"": 2}, False),\n            (""no_conflicts"", {}, {""a"": 1, ""c"": 3}, {""a"": 1, ""c"": 3}, False),\n            (\n                ""no_conflicts"",\n                {""a"": 1, ""b"": 2},\n                {""a"": 4, ""c"": 3},\n                {""a"": 1, ""b"": 2, ""c"": 3},\n                True,\n            ),\n            (""drop"", {""a"": 1, ""b"": 2}, {""a"": 1, ""c"": 3}, {}, False),\n            (""identical"", {""a"": 1, ""b"": 2}, {""a"": 1, ""b"": 2}, {""a"": 1, ""b"": 2}, False),\n            (""identical"", {""a"": 1, ""b"": 2}, {""a"": 1, ""c"": 3}, {""a"": 1, ""b"": 2}, True),\n            (\n                ""override"",\n                {""a"": 1, ""b"": 2},\n                {""a"": 4, ""b"": 5, ""c"": 3},\n                {""a"": 1, ""b"": 2},\n                False,\n            ),\n        ],\n    )\n    def test_merge_arrays_attrs(\n        self, combine_attrs, var1_attrs, var2_attrs, expected_attrs, expect_exception\n    ):\n        data = create_test_data()\n        data.var1.attrs = var1_attrs\n        data.var2.attrs = var2_attrs\n        if expect_exception:\n            with raises_regex(MergeError, ""combine_attrs""):\n                actual = xr.merge([data.var1, data.var2], combine_attrs=combine_attrs)\n        else:\n            actual = xr.merge([data.var1, data.var2], combine_attrs=combine_attrs)\n            expected = data[[""var1"", ""var2""]]\n            expected.attrs = expected_attrs\n            assert actual.identical(expected)\n\n    def test_merge_dicts_simple(self):\n        actual = xr.merge([{""foo"": 0}, {""bar"": ""one""}, {""baz"": 3.5}])\n        expected = xr.Dataset({""foo"": 0, ""bar"": ""one"", ""baz"": 3.5})\n        assert actual.identical(expected)\n\n    def test_merge_dicts_dims(self):\n        actual = xr.merge([{""y"": (""x"", [13])}, {""x"": [12]}])\n        expected = xr.Dataset({""x"": [12], ""y"": (""x"", [13])})\n        assert actual.identical(expected)\n\n    def test_merge_error(self):\n        ds = xr.Dataset({""x"": 0})\n        with pytest.raises(xr.MergeError):\n            xr.merge([ds, ds + 1])\n\n    def test_merge_alignment_error(self):\n        ds = xr.Dataset(coords={""x"": [1, 2]})\n        other = xr.Dataset(coords={""x"": [2, 3]})\n        with raises_regex(ValueError, ""indexes .* not equal""):\n            xr.merge([ds, other], join=""exact"")\n\n    def test_merge_wrong_input_error(self):\n        with raises_regex(TypeError, ""objects must be an iterable""):\n            xr.merge([1])\n        ds = xr.Dataset(coords={""x"": [1, 2]})\n        with raises_regex(TypeError, ""objects must be an iterable""):\n            xr.merge({""a"": ds})\n        with raises_regex(TypeError, ""objects must be an iterable""):\n            xr.merge([ds, 1])\n\n    def test_merge_no_conflicts_single_var(self):\n        ds1 = xr.Dataset({""a"": (""x"", [1, 2]), ""x"": [0, 1]})\n        ds2 = xr.Dataset({""a"": (""x"", [2, 3]), ""x"": [1, 2]})\n        expected = xr.Dataset({""a"": (""x"", [1, 2, 3]), ""x"": [0, 1, 2]})\n        assert expected.identical(xr.merge([ds1, ds2], compat=""no_conflicts""))\n        assert expected.identical(xr.merge([ds2, ds1], compat=""no_conflicts""))\n        assert ds1.identical(xr.merge([ds1, ds2], compat=""no_conflicts"", join=""left""))\n        assert ds2.identical(xr.merge([ds1, ds2], compat=""no_conflicts"", join=""right""))\n        expected = xr.Dataset({""a"": (""x"", [2]), ""x"": [1]})\n        assert expected.identical(\n            xr.merge([ds1, ds2], compat=""no_conflicts"", join=""inner"")\n        )\n\n        with pytest.raises(xr.MergeError):\n            ds3 = xr.Dataset({""a"": (""x"", [99, 3]), ""x"": [1, 2]})\n            xr.merge([ds1, ds3], compat=""no_conflicts"")\n\n        with pytest.raises(xr.MergeError):\n            ds3 = xr.Dataset({""a"": (""y"", [2, 3]), ""y"": [1, 2]})\n            xr.merge([ds1, ds3], compat=""no_conflicts"")\n\n    def test_merge_no_conflicts_multi_var(self):\n        data = create_test_data()\n        data1 = data.copy(deep=True)\n        data2 = data.copy(deep=True)\n\n        expected = data[[""var1"", ""var2""]]\n        actual = xr.merge([data1.var1, data2.var2], compat=""no_conflicts"")\n        assert expected.identical(actual)\n\n        data1[""var1""][:, :5] = np.nan\n        data2[""var1""][:, 5:] = np.nan\n        data1[""var2""][:4, :] = np.nan\n        data2[""var2""][4:, :] = np.nan\n        del data2[""var3""]\n\n        actual = xr.merge([data1, data2], compat=""no_conflicts"")\n        assert data.equals(actual)\n\n    def test_merge_no_conflicts_preserve_attrs(self):\n        data = xr.Dataset({""x"": ([], 0, {""foo"": ""bar""})})\n        actual = xr.merge([data, data])\n        assert data.identical(actual)\n\n    def test_merge_no_conflicts_broadcast(self):\n        datasets = [xr.Dataset({""x"": (""y"", [0])}), xr.Dataset({""x"": np.nan})]\n        actual = xr.merge(datasets)\n        expected = xr.Dataset({""x"": (""y"", [0])})\n        assert expected.identical(actual)\n\n        datasets = [xr.Dataset({""x"": (""y"", [np.nan])}), xr.Dataset({""x"": 0})]\n        actual = xr.merge(datasets)\n        assert expected.identical(actual)\n\n\nclass TestMergeMethod:\n    def test_merge(self):\n        data = create_test_data()\n        ds1 = data[[""var1""]]\n        ds2 = data[[""var3""]]\n        expected = data[[""var1"", ""var3""]]\n        actual = ds1.merge(ds2)\n        assert expected.identical(actual)\n\n        actual = ds2.merge(ds1)\n        assert expected.identical(actual)\n\n        actual = data.merge(data)\n        assert data.identical(actual)\n        actual = data.reset_coords(drop=True).merge(data)\n        assert data.identical(actual)\n        actual = data.merge(data.reset_coords(drop=True))\n        assert data.identical(actual)\n\n        with pytest.raises(ValueError):\n            ds1.merge(ds2.rename({""var3"": ""var1""}))\n        with raises_regex(ValueError, ""should be coordinates or not""):\n            data.reset_coords().merge(data)\n        with raises_regex(ValueError, ""should be coordinates or not""):\n            data.merge(data.reset_coords())\n\n    def test_merge_broadcast_equals(self):\n        ds1 = xr.Dataset({""x"": 0})\n        ds2 = xr.Dataset({""x"": (""y"", [0, 0])})\n        actual = ds1.merge(ds2)\n        assert ds2.identical(actual)\n\n        actual = ds2.merge(ds1)\n        assert ds2.identical(actual)\n\n        actual = ds1.copy()\n        actual.update(ds2)\n        assert ds2.identical(actual)\n\n        ds1 = xr.Dataset({""x"": np.nan})\n        ds2 = xr.Dataset({""x"": (""y"", [np.nan, np.nan])})\n        actual = ds1.merge(ds2)\n        assert ds2.identical(actual)\n\n    def test_merge_compat(self):\n        ds1 = xr.Dataset({""x"": 0})\n        ds2 = xr.Dataset({""x"": 1})\n        for compat in [""broadcast_equals"", ""equals"", ""identical"", ""no_conflicts""]:\n            with pytest.raises(xr.MergeError):\n                ds1.merge(ds2, compat=compat)\n\n        ds2 = xr.Dataset({""x"": [0, 0]})\n        for compat in [""equals"", ""identical""]:\n            with raises_regex(ValueError, ""should be coordinates or not""):\n                ds1.merge(ds2, compat=compat)\n\n        ds2 = xr.Dataset({""x"": ((), 0, {""foo"": ""bar""})})\n        with pytest.raises(xr.MergeError):\n            ds1.merge(ds2, compat=""identical"")\n\n        with raises_regex(ValueError, ""compat=.* invalid""):\n            ds1.merge(ds2, compat=""foobar"")\n\n        assert ds1.identical(ds1.merge(ds2, compat=""override""))\n\n    def test_merge_auto_align(self):\n        ds1 = xr.Dataset({""a"": (""x"", [1, 2]), ""x"": [0, 1]})\n        ds2 = xr.Dataset({""b"": (""x"", [3, 4]), ""x"": [1, 2]})\n        expected = xr.Dataset(\n            {""a"": (""x"", [1, 2, np.nan]), ""b"": (""x"", [np.nan, 3, 4])}, {""x"": [0, 1, 2]}\n        )\n        assert expected.identical(ds1.merge(ds2))\n        assert expected.identical(ds2.merge(ds1))\n\n        expected = expected.isel(x=slice(2))\n        assert expected.identical(ds1.merge(ds2, join=""left""))\n        assert expected.identical(ds2.merge(ds1, join=""right""))\n\n        expected = expected.isel(x=slice(1, 2))\n        assert expected.identical(ds1.merge(ds2, join=""inner""))\n        assert expected.identical(ds2.merge(ds1, join=""inner""))\n\n    @pytest.mark.parametrize(""fill_value"", [dtypes.NA, 2, 2.0])\n    def test_merge_fill_value(self, fill_value):\n        ds1 = xr.Dataset({""a"": (""x"", [1, 2]), ""x"": [0, 1]})\n        ds2 = xr.Dataset({""b"": (""x"", [3, 4]), ""x"": [1, 2]})\n        if fill_value == dtypes.NA:\n            # if we supply the default, we expect the missing value for a\n            # float array\n            fill_value = np.nan\n        expected = xr.Dataset(\n            {""a"": (""x"", [1, 2, fill_value]), ""b"": (""x"", [fill_value, 3, 4])},\n            {""x"": [0, 1, 2]},\n        )\n        assert expected.identical(ds1.merge(ds2, fill_value=fill_value))\n        assert expected.identical(ds2.merge(ds1, fill_value=fill_value))\n        assert expected.identical(xr.merge([ds1, ds2], fill_value=fill_value))\n\n    def test_merge_no_conflicts(self):\n        ds1 = xr.Dataset({""a"": (""x"", [1, 2]), ""x"": [0, 1]})\n        ds2 = xr.Dataset({""a"": (""x"", [2, 3]), ""x"": [1, 2]})\n        expected = xr.Dataset({""a"": (""x"", [1, 2, 3]), ""x"": [0, 1, 2]})\n\n        assert expected.identical(ds1.merge(ds2, compat=""no_conflicts""))\n        assert expected.identical(ds2.merge(ds1, compat=""no_conflicts""))\n\n        assert ds1.identical(ds1.merge(ds2, compat=""no_conflicts"", join=""left""))\n\n        assert ds2.identical(ds1.merge(ds2, compat=""no_conflicts"", join=""right""))\n\n        expected2 = xr.Dataset({""a"": (""x"", [2]), ""x"": [1]})\n        assert expected2.identical(ds1.merge(ds2, compat=""no_conflicts"", join=""inner""))\n\n        with pytest.raises(xr.MergeError):\n            ds3 = xr.Dataset({""a"": (""x"", [99, 3]), ""x"": [1, 2]})\n            ds1.merge(ds3, compat=""no_conflicts"")\n\n        with pytest.raises(xr.MergeError):\n            ds3 = xr.Dataset({""a"": (""y"", [2, 3]), ""y"": [1, 2]})\n            ds1.merge(ds3, compat=""no_conflicts"")\n\n    def test_merge_dataarray(self):\n        ds = xr.Dataset({""a"": 0})\n        da = xr.DataArray(data=1, name=""b"")\n\n        assert_identical(ds.merge(da), xr.merge([ds, da]))\n'"
xarray/tests/test_missing.py,75,"b'import itertools\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nimport xarray as xr\nfrom xarray.core.missing import (\n    NumpyInterpolator,\n    ScipyInterpolator,\n    SplineInterpolator,\n    _get_nan_block_lengths,\n    get_clean_interp_index,\n)\nfrom xarray.core.pycompat import dask_array_type\nfrom xarray.tests import (\n    assert_allclose,\n    assert_array_equal,\n    assert_equal,\n    raises_regex,\n    requires_bottleneck,\n    requires_cftime,\n    requires_dask,\n    requires_scipy,\n)\nfrom xarray.tests.test_cftime_offsets import _CFTIME_CALENDARS\n\n\n@pytest.fixture\ndef da():\n    return xr.DataArray([0, np.nan, 1, 2, np.nan, 3, 4, 5, np.nan, 6, 7], dims=""time"")\n\n\n@pytest.fixture\ndef cf_da():\n    def _cf_da(calendar, freq=""1D""):\n        times = xr.cftime_range(\n            start=""1970-01-01"", freq=freq, periods=10, calendar=calendar\n        )\n        values = np.arange(10)\n        return xr.DataArray(values, dims=(""time"",), coords={""time"": times})\n\n    return _cf_da\n\n\n@pytest.fixture\ndef ds():\n    ds = xr.Dataset()\n    ds[""var1""] = xr.DataArray(\n        [0, np.nan, 1, 2, np.nan, 3, 4, 5, np.nan, 6, 7], dims=""time""\n    )\n    ds[""var2""] = xr.DataArray(\n        [10, np.nan, 11, 12, np.nan, 13, 14, 15, np.nan, 16, 17], dims=""x""\n    )\n    return ds\n\n\ndef make_interpolate_example_data(shape, frac_nan, seed=12345, non_uniform=False):\n    rs = np.random.RandomState(seed)\n    vals = rs.normal(size=shape)\n    if frac_nan == 1:\n        vals[:] = np.nan\n    elif frac_nan == 0:\n        pass\n    else:\n        n_missing = int(vals.size * frac_nan)\n\n        ys = np.arange(shape[0])\n        xs = np.arange(shape[1])\n        if n_missing:\n            np.random.shuffle(ys)\n            ys = ys[:n_missing]\n\n            np.random.shuffle(xs)\n            xs = xs[:n_missing]\n\n            vals[ys, xs] = np.nan\n\n    if non_uniform:\n        # construct a datetime index that has irregular spacing\n        deltas = pd.TimedeltaIndex(unit=""d"", data=rs.normal(size=shape[0], scale=10))\n        coords = {""time"": (pd.Timestamp(""2000-01-01"") + deltas).sort_values()}\n    else:\n        coords = {""time"": pd.date_range(""2000-01-01"", freq=""D"", periods=shape[0])}\n    da = xr.DataArray(vals, dims=(""time"", ""x""), coords=coords)\n    df = da.to_pandas()\n\n    return da, df\n\n\n@requires_scipy\ndef test_interpolate_pd_compat():\n    shapes = [(8, 8), (1, 20), (20, 1), (100, 100)]\n    frac_nans = [0, 0.5, 1]\n    methods = [""linear"", ""nearest"", ""zero"", ""slinear"", ""quadratic"", ""cubic""]\n\n    for (shape, frac_nan, method) in itertools.product(shapes, frac_nans, methods):\n\n        da, df = make_interpolate_example_data(shape, frac_nan)\n\n        for dim in [""time"", ""x""]:\n            actual = da.interpolate_na(method=method, dim=dim, fill_value=np.nan)\n            expected = df.interpolate(\n                method=method, axis=da.get_axis_num(dim), fill_value=(np.nan, np.nan)\n            )\n            # Note, Pandas does some odd things with the left/right fill_value\n            # for the linear methods. This next line inforces the xarray\n            # fill_value convention on the pandas output. Therefore, this test\n            # only checks that interpolated values are the same (not nans)\n            expected.values[pd.isnull(actual.values)] = np.nan\n\n            np.testing.assert_allclose(actual.values, expected.values)\n\n\n@requires_scipy\n@pytest.mark.parametrize(""method"", [""barycentric"", ""krog"", ""pchip"", ""spline"", ""akima""])\ndef test_scipy_methods_function(method):\n    # Note: Pandas does some wacky things with these methods and the full\n    # integration tests wont work.\n    da, _ = make_interpolate_example_data((25, 25), 0.4, non_uniform=True)\n    actual = da.interpolate_na(method=method, dim=""time"")\n    assert (da.count(""time"") <= actual.count(""time"")).all()\n\n\n@requires_scipy\ndef test_interpolate_pd_compat_non_uniform_index():\n    shapes = [(8, 8), (1, 20), (20, 1), (100, 100)]\n    frac_nans = [0, 0.5, 1]\n    methods = [""time"", ""index"", ""values""]\n\n    for (shape, frac_nan, method) in itertools.product(shapes, frac_nans, methods):\n\n        da, df = make_interpolate_example_data(shape, frac_nan, non_uniform=True)\n        for dim in [""time"", ""x""]:\n            if method == ""time"" and dim != ""time"":\n                continue\n            actual = da.interpolate_na(\n                method=""linear"", dim=dim, use_coordinate=True, fill_value=np.nan\n            )\n            expected = df.interpolate(\n                method=method, axis=da.get_axis_num(dim), fill_value=np.nan\n            )\n\n            # Note, Pandas does some odd things with the left/right fill_value\n            # for the linear methods. This next line inforces the xarray\n            # fill_value convention on the pandas output. Therefore, this test\n            # only checks that interpolated values are the same (not nans)\n            expected.values[pd.isnull(actual.values)] = np.nan\n\n            np.testing.assert_allclose(actual.values, expected.values)\n\n\n@requires_scipy\ndef test_interpolate_pd_compat_polynomial():\n    shapes = [(8, 8), (1, 20), (20, 1), (100, 100)]\n    frac_nans = [0, 0.5, 1]\n    orders = [1, 2, 3]\n\n    for (shape, frac_nan, order) in itertools.product(shapes, frac_nans, orders):\n\n        da, df = make_interpolate_example_data(shape, frac_nan)\n\n        for dim in [""time"", ""x""]:\n            actual = da.interpolate_na(\n                method=""polynomial"", order=order, dim=dim, use_coordinate=False\n            )\n            expected = df.interpolate(\n                method=""polynomial"", order=order, axis=da.get_axis_num(dim)\n            )\n            np.testing.assert_allclose(actual.values, expected.values)\n\n\n@requires_scipy\ndef test_interpolate_unsorted_index_raises():\n    vals = np.array([1, 2, 3], dtype=np.float64)\n    expected = xr.DataArray(vals, dims=""x"", coords={""x"": [2, 1, 3]})\n    with raises_regex(ValueError, ""Index \'x\' must be monotonically increasing""):\n        expected.interpolate_na(dim=""x"", method=""index"")\n\n\ndef test_interpolate_no_dim_raises():\n    da = xr.DataArray(np.array([1, 2, np.nan, 5], dtype=np.float64), dims=""x"")\n    with raises_regex(NotImplementedError, ""dim is a required argument""):\n        da.interpolate_na(method=""linear"")\n\n\ndef test_interpolate_invalid_interpolator_raises():\n    da = xr.DataArray(np.array([1, 2, np.nan, 5], dtype=np.float64), dims=""x"")\n    with raises_regex(ValueError, ""not a valid""):\n        da.interpolate_na(dim=""x"", method=""foo"")\n\n\ndef test_interpolate_duplicate_values_raises():\n    data = np.random.randn(2, 3)\n    da = xr.DataArray(data, coords=[(""x"", [""a"", ""a""]), (""y"", [0, 1, 2])])\n    with raises_regex(ValueError, ""Index \'x\' has duplicate values""):\n        da.interpolate_na(dim=""x"", method=""foo"")\n\n\ndef test_interpolate_multiindex_raises():\n    data = np.random.randn(2, 3)\n    data[1, 1] = np.nan\n    da = xr.DataArray(data, coords=[(""x"", [""a"", ""b""]), (""y"", [0, 1, 2])])\n    das = da.stack(z=(""x"", ""y""))\n    with raises_regex(TypeError, ""Index \'z\' must be castable to float64""):\n        das.interpolate_na(dim=""z"")\n\n\ndef test_interpolate_2d_coord_raises():\n    coords = {\n        ""x"": xr.Variable((""a"", ""b""), np.arange(6).reshape(2, 3)),\n        ""y"": xr.Variable((""a"", ""b""), np.arange(6).reshape(2, 3)) * 2,\n    }\n\n    data = np.random.randn(2, 3)\n    data[1, 1] = np.nan\n    da = xr.DataArray(data, dims=(""a"", ""b""), coords=coords)\n    with raises_regex(ValueError, ""interpolation must be 1D""):\n        da.interpolate_na(dim=""a"", use_coordinate=""x"")\n\n\n@requires_scipy\ndef test_interpolate_kwargs():\n    da = xr.DataArray(np.array([4, 5, np.nan], dtype=np.float64), dims=""x"")\n    expected = xr.DataArray(np.array([4, 5, 6], dtype=np.float64), dims=""x"")\n    actual = da.interpolate_na(dim=""x"", fill_value=""extrapolate"")\n    assert_equal(actual, expected)\n\n    expected = xr.DataArray(np.array([4, 5, -999], dtype=np.float64), dims=""x"")\n    actual = da.interpolate_na(dim=""x"", fill_value=-999)\n    assert_equal(actual, expected)\n\n\ndef test_interpolate_keep_attrs():\n    vals = np.array([1, 2, 3, 4, 5, 6], dtype=np.float64)\n    mvals = vals.copy()\n    mvals[2] = np.nan\n    missing = xr.DataArray(mvals, dims=""x"")\n    missing.attrs = {""test"": ""value""}\n\n    actual = missing.interpolate_na(dim=""x"", keep_attrs=True)\n    assert actual.attrs == {""test"": ""value""}\n\n\ndef test_interpolate():\n\n    vals = np.array([1, 2, 3, 4, 5, 6], dtype=np.float64)\n    expected = xr.DataArray(vals, dims=""x"")\n    mvals = vals.copy()\n    mvals[2] = np.nan\n    missing = xr.DataArray(mvals, dims=""x"")\n\n    actual = missing.interpolate_na(dim=""x"")\n\n    assert_equal(actual, expected)\n\n\ndef test_interpolate_nonans():\n\n    vals = np.array([1, 2, 3, 4, 5, 6], dtype=np.float64)\n    expected = xr.DataArray(vals, dims=""x"")\n    actual = expected.interpolate_na(dim=""x"")\n    assert_equal(actual, expected)\n\n\n@requires_scipy\ndef test_interpolate_allnans():\n    vals = np.full(6, np.nan, dtype=np.float64)\n    expected = xr.DataArray(vals, dims=""x"")\n    actual = expected.interpolate_na(dim=""x"")\n\n    assert_equal(actual, expected)\n\n\n@requires_bottleneck\ndef test_interpolate_limits():\n    da = xr.DataArray(\n        np.array([1, 2, np.nan, np.nan, np.nan, 6], dtype=np.float64), dims=""x""\n    )\n\n    actual = da.interpolate_na(dim=""x"", limit=None)\n    assert actual.isnull().sum() == 0\n\n    actual = da.interpolate_na(dim=""x"", limit=2)\n    expected = xr.DataArray(\n        np.array([1, 2, 3, 4, np.nan, 6], dtype=np.float64), dims=""x""\n    )\n\n    assert_equal(actual, expected)\n\n\n@requires_scipy\ndef test_interpolate_methods():\n    for method in [""linear"", ""nearest"", ""zero"", ""slinear"", ""quadratic"", ""cubic""]:\n        kwargs = {}\n        da = xr.DataArray(\n            np.array([0, 1, 2, np.nan, np.nan, np.nan, 6, 7, 8], dtype=np.float64),\n            dims=""x"",\n        )\n        actual = da.interpolate_na(""x"", method=method, **kwargs)\n        assert actual.isnull().sum() == 0\n\n        actual = da.interpolate_na(""x"", method=method, limit=2, **kwargs)\n        assert actual.isnull().sum() == 1\n\n\n@requires_scipy\ndef test_interpolators():\n    for method, interpolator in [\n        (""linear"", NumpyInterpolator),\n        (""linear"", ScipyInterpolator),\n        (""spline"", SplineInterpolator),\n    ]:\n        xi = np.array([-1, 0, 1, 2, 5], dtype=np.float64)\n        yi = np.array([-10, 0, 10, 20, 50], dtype=np.float64)\n        x = np.array([3, 4], dtype=np.float64)\n\n        f = interpolator(xi, yi, method=method)\n        out = f(x)\n        assert pd.isnull(out).sum() == 0\n\n\ndef test_interpolate_use_coordinate():\n    xc = xr.Variable(""x"", [100, 200, 300, 400, 500, 600])\n    da = xr.DataArray(\n        np.array([1, 2, np.nan, np.nan, np.nan, 6], dtype=np.float64),\n        dims=""x"",\n        coords={""xc"": xc},\n    )\n\n    # use_coordinate == False is same as using the default index\n    actual = da.interpolate_na(dim=""x"", use_coordinate=False)\n    expected = da.interpolate_na(dim=""x"")\n    assert_equal(actual, expected)\n\n    # possible to specify non index coordinate\n    actual = da.interpolate_na(dim=""x"", use_coordinate=""xc"")\n    expected = da.interpolate_na(dim=""x"")\n    assert_equal(actual, expected)\n\n    # possible to specify index coordinate by name\n    actual = da.interpolate_na(dim=""x"", use_coordinate=""x"")\n    expected = da.interpolate_na(dim=""x"")\n    assert_equal(actual, expected)\n\n\n@requires_dask\ndef test_interpolate_dask():\n    da, _ = make_interpolate_example_data((40, 40), 0.5)\n    da = da.chunk({""x"": 5})\n    actual = da.interpolate_na(""time"")\n    expected = da.load().interpolate_na(""time"")\n    assert isinstance(actual.data, dask_array_type)\n    assert_equal(actual.compute(), expected)\n\n    # with limit\n    da = da.chunk({""x"": 5})\n    actual = da.interpolate_na(""time"", limit=3)\n    expected = da.load().interpolate_na(""time"", limit=3)\n    assert isinstance(actual.data, dask_array_type)\n    assert_equal(actual, expected)\n\n\n@requires_dask\ndef test_interpolate_dask_raises_for_invalid_chunk_dim():\n    da, _ = make_interpolate_example_data((40, 40), 0.5)\n    da = da.chunk({""time"": 5})\n    with raises_regex(ValueError, ""dask=\'parallelized\' consists of multiple""):\n        da.interpolate_na(""time"")\n\n\n@requires_bottleneck\ndef test_ffill():\n    da = xr.DataArray(np.array([4, 5, np.nan], dtype=np.float64), dims=""x"")\n    expected = xr.DataArray(np.array([4, 5, 5], dtype=np.float64), dims=""x"")\n    actual = da.ffill(""x"")\n    assert_equal(actual, expected)\n\n\n@requires_bottleneck\n@requires_dask\ndef test_ffill_dask():\n    da, _ = make_interpolate_example_data((40, 40), 0.5)\n    da = da.chunk({""x"": 5})\n    actual = da.ffill(""time"")\n    expected = da.load().ffill(""time"")\n    assert isinstance(actual.data, dask_array_type)\n    assert_equal(actual, expected)\n\n    # with limit\n    da = da.chunk({""x"": 5})\n    actual = da.ffill(""time"", limit=3)\n    expected = da.load().ffill(""time"", limit=3)\n    assert isinstance(actual.data, dask_array_type)\n    assert_equal(actual, expected)\n\n\n@requires_bottleneck\n@requires_dask\ndef test_bfill_dask():\n    da, _ = make_interpolate_example_data((40, 40), 0.5)\n    da = da.chunk({""x"": 5})\n    actual = da.bfill(""time"")\n    expected = da.load().bfill(""time"")\n    assert isinstance(actual.data, dask_array_type)\n    assert_equal(actual, expected)\n\n    # with limit\n    da = da.chunk({""x"": 5})\n    actual = da.bfill(""time"", limit=3)\n    expected = da.load().bfill(""time"", limit=3)\n    assert isinstance(actual.data, dask_array_type)\n    assert_equal(actual, expected)\n\n\n@requires_bottleneck\ndef test_ffill_bfill_nonans():\n\n    vals = np.array([1, 2, 3, 4, 5, 6], dtype=np.float64)\n    expected = xr.DataArray(vals, dims=""x"")\n\n    actual = expected.ffill(dim=""x"")\n    assert_equal(actual, expected)\n\n    actual = expected.bfill(dim=""x"")\n    assert_equal(actual, expected)\n\n\n@requires_bottleneck\ndef test_ffill_bfill_allnans():\n\n    vals = np.full(6, np.nan, dtype=np.float64)\n    expected = xr.DataArray(vals, dims=""x"")\n\n    actual = expected.ffill(dim=""x"")\n    assert_equal(actual, expected)\n\n    actual = expected.bfill(dim=""x"")\n    assert_equal(actual, expected)\n\n\n@requires_bottleneck\ndef test_ffill_functions(da):\n    result = da.ffill(""time"")\n    assert result.isnull().sum() == 0\n\n\n@requires_bottleneck\ndef test_ffill_limit():\n    da = xr.DataArray(\n        [0, np.nan, np.nan, np.nan, np.nan, 3, 4, 5, np.nan, 6, 7], dims=""time""\n    )\n    result = da.ffill(""time"")\n    expected = xr.DataArray([0, 0, 0, 0, 0, 3, 4, 5, 5, 6, 7], dims=""time"")\n    assert_array_equal(result, expected)\n\n    result = da.ffill(""time"", limit=1)\n    expected = xr.DataArray(\n        [0, 0, np.nan, np.nan, np.nan, 3, 4, 5, 5, 6, 7], dims=""time""\n    )\n    assert_array_equal(result, expected)\n\n\ndef test_interpolate_dataset(ds):\n    actual = ds.interpolate_na(dim=""time"")\n    # no missing values in var1\n    assert actual[""var1""].count(""time"") == actual.dims[""time""]\n\n    # var2 should be the same as it was\n    assert_array_equal(actual[""var2""], ds[""var2""])\n\n\n@requires_bottleneck\ndef test_ffill_dataset(ds):\n    ds.ffill(dim=""time"")\n\n\n@requires_bottleneck\ndef test_bfill_dataset(ds):\n    ds.ffill(dim=""time"")\n\n\n@requires_bottleneck\n@pytest.mark.parametrize(\n    ""y, lengths"",\n    [\n        [np.arange(9), [[3, 3, 3, 0, 3, 3, 0, 2, 2]]],\n        [np.arange(9) * 3, [[9, 9, 9, 0, 9, 9, 0, 6, 6]]],\n        [[0, 2, 5, 6, 7, 8, 10, 12, 14], [[6, 6, 6, 0, 4, 4, 0, 4, 4]]],\n    ],\n)\ndef test_interpolate_na_nan_block_lengths(y, lengths):\n    arr = [[np.nan, np.nan, np.nan, 1, np.nan, np.nan, 4, np.nan, np.nan]]\n    da = xr.DataArray(arr * 2, dims=[""x"", ""y""], coords={""x"": [0, 1], ""y"": y})\n    index = get_clean_interp_index(da, dim=""y"", use_coordinate=True)\n    actual = _get_nan_block_lengths(da, dim=""y"", index=index)\n    expected = da.copy(data=lengths * 2)\n    assert_equal(actual, expected)\n\n\n@requires_cftime\n@pytest.mark.parametrize(""calendar"", _CFTIME_CALENDARS)\ndef test_get_clean_interp_index_cf_calendar(cf_da, calendar):\n    """"""The index for CFTimeIndex is in units of days. This means that if two series using a 360 and 365 days\n    calendar each have a trend of .01C/year, the linear regression coefficients will be different because they\n    have different number of days.\n\n    Another option would be to have an index in units of years, but this would likely create other difficulties.\n    """"""\n    i = get_clean_interp_index(cf_da(calendar), dim=""time"")\n    np.testing.assert_array_equal(i, np.arange(10) * 1e9 * 86400)\n\n\n@requires_cftime\n@pytest.mark.parametrize(\n    (""calendar"", ""freq""), zip([""gregorian"", ""proleptic_gregorian""], [""1D"", ""1M"", ""1Y""])\n)\ndef test_get_clean_interp_index_dt(cf_da, calendar, freq):\n    """"""In the gregorian case, the index should be proportional to normal datetimes.""""""\n    g = cf_da(calendar, freq=freq)\n    g[""stime""] = xr.Variable(data=g.time.to_index().to_datetimeindex(), dims=(""time"",))\n\n    gi = get_clean_interp_index(g, ""time"")\n    si = get_clean_interp_index(g, ""time"", use_coordinate=""stime"")\n    np.testing.assert_array_equal(gi, si)\n\n\ndef test_get_clean_interp_index_potential_overflow():\n    da = xr.DataArray(\n        [0, 1, 2],\n        dims=(""time"",),\n        coords={""time"": xr.cftime_range(""0000-01-01"", periods=3, calendar=""360_day"")},\n    )\n    get_clean_interp_index(da, ""time"")\n\n\n@pytest.mark.parametrize(""index"", ([0, 2, 1], [0, 1, 1]))\ndef test_get_clean_interp_index_strict(index):\n    da = xr.DataArray([0, 1, 2], dims=(""x"",), coords={""x"": index})\n\n    with pytest.raises(ValueError):\n        get_clean_interp_index(da, ""x"")\n\n    clean = get_clean_interp_index(da, ""x"", strict=False)\n    np.testing.assert_array_equal(index, clean)\n    assert clean.dtype == np.float64\n\n\n@pytest.fixture\ndef da_time():\n    return xr.DataArray(\n        [np.nan, 1, 2, np.nan, np.nan, 5, np.nan, np.nan, np.nan, np.nan, 10],\n        dims=[""t""],\n    )\n\n\ndef test_interpolate_na_max_gap_errors(da_time):\n    with raises_regex(\n        NotImplementedError, ""max_gap not implemented for unlabeled coordinates""\n    ):\n        da_time.interpolate_na(""t"", max_gap=1)\n\n    with raises_regex(ValueError, ""max_gap must be a scalar.""):\n        da_time.interpolate_na(""t"", max_gap=(1,))\n\n    da_time[""t""] = pd.date_range(""2001-01-01"", freq=""H"", periods=11)\n    with raises_regex(TypeError, ""Expected value of type str""):\n        da_time.interpolate_na(""t"", max_gap=1)\n\n    with raises_regex(TypeError, ""Expected integer or floating point""):\n        da_time.interpolate_na(""t"", max_gap=""1H"", use_coordinate=False)\n\n    with raises_regex(ValueError, ""Could not convert \'huh\' to timedelta64""):\n        da_time.interpolate_na(""t"", max_gap=""huh"")\n\n\n@requires_bottleneck\n@pytest.mark.parametrize(""time_range_func"", [pd.date_range, xr.cftime_range])\n@pytest.mark.parametrize(""transform"", [lambda x: x, lambda x: x.to_dataset(name=""a"")])\n@pytest.mark.parametrize(\n    ""max_gap"", [""3H"", np.timedelta64(3, ""h""), pd.to_timedelta(""3H"")]\n)\ndef test_interpolate_na_max_gap_time_specifier(\n    da_time, max_gap, transform, time_range_func\n):\n    da_time[""t""] = time_range_func(""2001-01-01"", freq=""H"", periods=11)\n    expected = transform(\n        da_time.copy(data=[np.nan, 1, 2, 3, 4, 5, np.nan, np.nan, np.nan, np.nan, 10])\n    )\n    actual = transform(da_time).interpolate_na(""t"", max_gap=max_gap)\n    assert_allclose(actual, expected)\n\n\n@requires_bottleneck\n@pytest.mark.parametrize(\n    ""coords"",\n    [\n        pytest.param(None, marks=pytest.mark.xfail()),\n        {""x"": np.arange(4), ""y"": np.arange(11)},\n    ],\n)\ndef test_interpolate_na_2d(coords):\n    da = xr.DataArray(\n        [\n            [1, 2, 3, 4, np.nan, 6, 7, np.nan, np.nan, np.nan, 11],\n            [1, 2, 3, np.nan, np.nan, 6, 7, np.nan, np.nan, np.nan, 11],\n            [1, 2, 3, np.nan, np.nan, 6, 7, np.nan, np.nan, np.nan, 11],\n            [1, 2, 3, 4, np.nan, 6, 7, np.nan, np.nan, np.nan, 11],\n        ],\n        dims=[""x"", ""y""],\n        coords=coords,\n    )\n\n    actual = da.interpolate_na(""y"", max_gap=2)\n    expected_y = da.copy(\n        data=[\n            [1, 2, 3, 4, 5, 6, 7, np.nan, np.nan, np.nan, 11],\n            [1, 2, 3, np.nan, np.nan, 6, 7, np.nan, np.nan, np.nan, 11],\n            [1, 2, 3, np.nan, np.nan, 6, 7, np.nan, np.nan, np.nan, 11],\n            [1, 2, 3, 4, 5, 6, 7, np.nan, np.nan, np.nan, 11],\n        ]\n    )\n    assert_equal(actual, expected_y)\n\n    actual = da.interpolate_na(""x"", max_gap=3)\n    expected_x = xr.DataArray(\n        [\n            [1, 2, 3, 4, np.nan, 6, 7, np.nan, np.nan, np.nan, 11],\n            [1, 2, 3, 4, np.nan, 6, 7, np.nan, np.nan, np.nan, 11],\n            [1, 2, 3, 4, np.nan, 6, 7, np.nan, np.nan, np.nan, 11],\n            [1, 2, 3, 4, np.nan, 6, 7, np.nan, np.nan, np.nan, 11],\n        ],\n        dims=[""x"", ""y""],\n        coords=coords,\n    )\n    assert_equal(actual, expected_x)\n'"
xarray/tests/test_nputils.py,9,"b'import numpy as np\nfrom numpy.testing import assert_array_equal\n\nfrom xarray.core.nputils import NumpyVIndexAdapter, _is_contiguous, rolling_window\n\n\ndef test_is_contiguous():\n    assert _is_contiguous([1])\n    assert _is_contiguous([1, 2, 3])\n    assert not _is_contiguous([1, 3])\n\n\ndef test_vindex():\n    x = np.arange(3 * 4 * 5).reshape((3, 4, 5))\n    vindex = NumpyVIndexAdapter(x)\n\n    # getitem\n    assert_array_equal(vindex[0], x[0])\n    assert_array_equal(vindex[[1, 2], [1, 2]], x[[1, 2], [1, 2]])\n    assert vindex[[0, 1], [0, 1], :].shape == (2, 5)\n    assert vindex[[0, 1], :, [0, 1]].shape == (2, 4)\n    assert vindex[:, [0, 1], [0, 1]].shape == (2, 3)\n\n    # setitem\n    vindex[:] = 0\n    assert_array_equal(x, np.zeros_like(x))\n    # assignment should not raise\n    vindex[[0, 1], [0, 1], :] = vindex[[0, 1], [0, 1], :]\n    vindex[[0, 1], :, [0, 1]] = vindex[[0, 1], :, [0, 1]]\n    vindex[:, [0, 1], [0, 1]] = vindex[:, [0, 1], [0, 1]]\n\n\ndef test_rolling():\n    x = np.array([1, 2, 3, 4], dtype=float)\n\n    actual = rolling_window(x, axis=-1, window=3, center=True, fill_value=np.nan)\n    expected = np.array(\n        [[np.nan, 1, 2], [1, 2, 3], [2, 3, 4], [3, 4, np.nan]], dtype=float\n    )\n    assert_array_equal(actual, expected)\n\n    actual = rolling_window(x, axis=-1, window=3, center=False, fill_value=0.0)\n    expected = np.array([[0, 0, 1], [0, 1, 2], [1, 2, 3], [2, 3, 4]], dtype=float)\n    assert_array_equal(actual, expected)\n\n    x = np.stack([x, x * 1.1])\n    actual = rolling_window(x, axis=-1, window=3, center=False, fill_value=0.0)\n    expected = np.stack([expected, expected * 1.1], axis=0)\n    assert_array_equal(actual, expected)\n'"
xarray/tests/test_options.py,0,"b'import pytest\n\nimport xarray\nfrom xarray import concat, merge\nfrom xarray.backends.file_manager import FILE_CACHE\nfrom xarray.core.options import OPTIONS, _get_keep_attrs\nfrom xarray.tests.test_dataset import create_test_data\n\n\ndef test_invalid_option_raises():\n    with pytest.raises(ValueError):\n        xarray.set_options(not_a_valid_options=True)\n\n\ndef test_display_width():\n    with pytest.raises(ValueError):\n        xarray.set_options(display_width=0)\n    with pytest.raises(ValueError):\n        xarray.set_options(display_width=-10)\n    with pytest.raises(ValueError):\n        xarray.set_options(display_width=3.5)\n\n\ndef test_arithmetic_join():\n    with pytest.raises(ValueError):\n        xarray.set_options(arithmetic_join=""invalid"")\n    with xarray.set_options(arithmetic_join=""exact""):\n        assert OPTIONS[""arithmetic_join""] == ""exact""\n\n\ndef test_enable_cftimeindex():\n    with pytest.raises(ValueError):\n        xarray.set_options(enable_cftimeindex=None)\n    with pytest.warns(FutureWarning, match=""no-op""):\n        with xarray.set_options(enable_cftimeindex=True):\n            assert OPTIONS[""enable_cftimeindex""]\n\n\ndef test_file_cache_maxsize():\n    with pytest.raises(ValueError):\n        xarray.set_options(file_cache_maxsize=0)\n    original_size = FILE_CACHE.maxsize\n    with xarray.set_options(file_cache_maxsize=123):\n        assert FILE_CACHE.maxsize == 123\n    assert FILE_CACHE.maxsize == original_size\n\n\ndef test_keep_attrs():\n    with pytest.raises(ValueError):\n        xarray.set_options(keep_attrs=""invalid_str"")\n    with xarray.set_options(keep_attrs=True):\n        assert OPTIONS[""keep_attrs""]\n    with xarray.set_options(keep_attrs=False):\n        assert not OPTIONS[""keep_attrs""]\n    with xarray.set_options(keep_attrs=""default""):\n        assert _get_keep_attrs(default=True)\n        assert not _get_keep_attrs(default=False)\n\n\ndef test_nested_options():\n    original = OPTIONS[""display_width""]\n    with xarray.set_options(display_width=1):\n        assert OPTIONS[""display_width""] == 1\n        with xarray.set_options(display_width=2):\n            assert OPTIONS[""display_width""] == 2\n        assert OPTIONS[""display_width""] == 1\n    assert OPTIONS[""display_width""] == original\n\n\ndef test_display_style():\n    original = ""html""\n    assert OPTIONS[""display_style""] == original\n    with pytest.raises(ValueError):\n        xarray.set_options(display_style=""invalid_str"")\n    with xarray.set_options(display_style=""text""):\n        assert OPTIONS[""display_style""] == ""text""\n    assert OPTIONS[""display_style""] == original\n\n\ndef create_test_dataset_attrs(seed=0):\n    ds = create_test_data(seed)\n    ds.attrs = {""attr1"": 5, ""attr2"": ""history"", ""attr3"": {""nested"": ""more_info""}}\n    return ds\n\n\ndef create_test_dataarray_attrs(seed=0, var=""var1""):\n    da = create_test_data(seed)[var]\n    da.attrs = {""attr1"": 5, ""attr2"": ""history"", ""attr3"": {""nested"": ""more_info""}}\n    return da\n\n\nclass TestAttrRetention:\n    def test_dataset_attr_retention(self):\n        # Use .mean() for all tests: a typical reduction operation\n        ds = create_test_dataset_attrs()\n        original_attrs = ds.attrs\n\n        # Test default behaviour\n        result = ds.mean()\n        assert result.attrs == {}\n        with xarray.set_options(keep_attrs=""default""):\n            result = ds.mean()\n            assert result.attrs == {}\n\n        with xarray.set_options(keep_attrs=True):\n            result = ds.mean()\n            assert result.attrs == original_attrs\n\n        with xarray.set_options(keep_attrs=False):\n            result = ds.mean()\n            assert result.attrs == {}\n\n    def test_dataarray_attr_retention(self):\n        # Use .mean() for all tests: a typical reduction operation\n        da = create_test_dataarray_attrs()\n        original_attrs = da.attrs\n\n        # Test default behaviour\n        result = da.mean()\n        assert result.attrs == {}\n        with xarray.set_options(keep_attrs=""default""):\n            result = da.mean()\n            assert result.attrs == {}\n\n        with xarray.set_options(keep_attrs=True):\n            result = da.mean()\n            assert result.attrs == original_attrs\n\n        with xarray.set_options(keep_attrs=False):\n            result = da.mean()\n            assert result.attrs == {}\n\n    def test_groupby_attr_retention(self):\n        da = xarray.DataArray([1, 2, 3], [(""x"", [1, 1, 2])])\n        da.attrs = {""attr1"": 5, ""attr2"": ""history"", ""attr3"": {""nested"": ""more_info""}}\n        original_attrs = da.attrs\n\n        # Test default behaviour\n        result = da.groupby(""x"").sum(keep_attrs=True)\n        assert result.attrs == original_attrs\n        with xarray.set_options(keep_attrs=""default""):\n            result = da.groupby(""x"").sum(keep_attrs=True)\n            assert result.attrs == original_attrs\n\n        with xarray.set_options(keep_attrs=True):\n            result1 = da.groupby(""x"")\n            result = result1.sum()\n            assert result.attrs == original_attrs\n\n        with xarray.set_options(keep_attrs=False):\n            result = da.groupby(""x"").sum()\n            assert result.attrs == {}\n\n    def test_concat_attr_retention(self):\n        ds1 = create_test_dataset_attrs()\n        ds2 = create_test_dataset_attrs()\n        ds2.attrs = {""wrong"": ""attributes""}\n        original_attrs = ds1.attrs\n\n        # Test default behaviour of keeping the attrs of the first\n        # dataset in the supplied list\n        # global keep_attrs option current doesn\'t affect concat\n        result = concat([ds1, ds2], dim=""dim1"")\n        assert result.attrs == original_attrs\n\n    @pytest.mark.xfail\n    def test_merge_attr_retention(self):\n        da1 = create_test_dataarray_attrs(var=""var1"")\n        da2 = create_test_dataarray_attrs(var=""var2"")\n        da2.attrs = {""wrong"": ""attributes""}\n        original_attrs = da1.attrs\n\n        # merge currently discards attrs, and the global keep_attrs\n        # option doesn\'t affect this\n        result = merge([da1, da2])\n        assert result.attrs == original_attrs\n\n    def test_display_style_text(self):\n        ds = create_test_dataset_attrs()\n        with xarray.set_options(display_style=""text""):\n            text = ds._repr_html_()\n            assert text.startswith(""<pre>"")\n            assert ""&#x27;nested&#x27;"" in text\n            assert ""&lt;xarray.Dataset&gt;"" in text\n\n    def test_display_style_html(self):\n        ds = create_test_dataset_attrs()\n        with xarray.set_options(display_style=""html""):\n            html = ds._repr_html_()\n            assert html.startswith(""<div>"")\n            assert ""&#x27;nested&#x27;"" in html\n\n    def test_display_dataarray_style_text(self):\n        da = create_test_dataarray_attrs()\n        with xarray.set_options(display_style=""text""):\n            text = da._repr_html_()\n            assert text.startswith(""<pre>"")\n            assert ""&lt;xarray.DataArray &#x27;var1&#x27;"" in text\n\n    def test_display_dataarray_style_html(self):\n        da = create_test_dataarray_attrs()\n        with xarray.set_options(display_style=""html""):\n            html = da._repr_html_()\n            assert html.startswith(""<div>"")\n            assert ""#x27;nested&#x27;"" in html\n'"
xarray/tests/test_plot.py,106,"b'import inspect\nfrom copy import deepcopy\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nimport xarray as xr\nimport xarray.plot as xplt\nfrom xarray import DataArray, Dataset\nfrom xarray.plot.dataset_plot import _infer_meta_data\nfrom xarray.plot.plot import _infer_interval_breaks\nfrom xarray.plot.utils import (\n    _build_discrete_cmap,\n    _color_palette,\n    _determine_cmap_params,\n    label_from_attrs,\n)\n\nfrom . import (\n    assert_array_equal,\n    assert_equal,\n    has_nc_time_axis,\n    raises_regex,\n    requires_cftime,\n    requires_matplotlib,\n    requires_nc_time_axis,\n    requires_seaborn,\n)\n\n# import mpl and change the backend before other mpl imports\ntry:\n    import matplotlib as mpl\n    import matplotlib.pyplot as plt\nexcept ImportError:\n    pass\n\n\n@pytest.mark.flaky\n@pytest.mark.skip(reason=""maybe flaky"")\ndef text_in_fig():\n    """"""\n    Return the set of all text in the figure\n    """"""\n    return {t.get_text() for t in plt.gcf().findobj(mpl.text.Text)}\n\n\ndef find_possible_colorbars():\n    # nb. this function also matches meshes from pcolormesh\n    return plt.gcf().findobj(mpl.collections.QuadMesh)\n\n\ndef substring_in_axes(substring, ax):\n    """"""\n    Return True if a substring is found anywhere in an axes\n    """"""\n    alltxt = {t.get_text() for t in ax.findobj(mpl.text.Text)}\n    for txt in alltxt:\n        if substring in txt:\n            return True\n    return False\n\n\ndef substring_not_in_axes(substring, ax):\n    """"""\n    Return True if a substring is not found anywhere in an axes\n    """"""\n    alltxt = {t.get_text() for t in ax.findobj(mpl.text.Text)}\n    check = [(substring not in txt) for txt in alltxt]\n    return all(check)\n\n\ndef easy_array(shape, start=0, stop=1):\n    """"""\n    Make an array with desired shape using np.linspace\n\n    shape is a tuple like (2, 3)\n    """"""\n    a = np.linspace(start, stop, num=np.prod(shape))\n    return a.reshape(shape)\n\n\n@requires_matplotlib\nclass PlotTestCase:\n    @pytest.fixture(autouse=True)\n    def setup(self):\n        yield\n        # Remove all matplotlib figures\n        plt.close(""all"")\n\n    def pass_in_axis(self, plotmethod):\n        fig, axes = plt.subplots(ncols=2)\n        plotmethod(ax=axes[0])\n        assert axes[0].has_data()\n\n    @pytest.mark.slow\n    def imshow_called(self, plotmethod):\n        plotmethod()\n        images = plt.gca().findobj(mpl.image.AxesImage)\n        return len(images) > 0\n\n    def contourf_called(self, plotmethod):\n        plotmethod()\n        paths = plt.gca().findobj(mpl.collections.PathCollection)\n        return len(paths) > 0\n\n\nclass TestPlot(PlotTestCase):\n    @pytest.fixture(autouse=True)\n    def setup_array(self):\n        self.darray = DataArray(easy_array((2, 3, 4)))\n\n    def test_label_from_attrs(self):\n        da = self.darray.copy()\n        assert """" == label_from_attrs(da)\n\n        da.name = ""a""\n        da.attrs[""units""] = ""a_units""\n        da.attrs[""long_name""] = ""a_long_name""\n        da.attrs[""standard_name""] = ""a_standard_name""\n        assert ""a_long_name [a_units]"" == label_from_attrs(da)\n\n        da.attrs.pop(""long_name"")\n        assert ""a_standard_name [a_units]"" == label_from_attrs(da)\n        da.attrs.pop(""units"")\n        assert ""a_standard_name"" == label_from_attrs(da)\n\n        da.attrs[""units""] = ""a_units""\n        da.attrs.pop(""standard_name"")\n        assert ""a [a_units]"" == label_from_attrs(da)\n\n        da.attrs.pop(""units"")\n        assert ""a"" == label_from_attrs(da)\n\n    def test1d(self):\n        self.darray[:, 0, 0].plot()\n\n        with raises_regex(ValueError, ""x must be one of None, \'dim_0\'""):\n            self.darray[:, 0, 0].plot(x=""dim_1"")\n\n        with raises_regex(TypeError, ""complex128""):\n            (self.darray[:, 0, 0] + 1j).plot()\n\n    def test_1d_bool(self):\n        xr.ones_like(self.darray[:, 0, 0], dtype=np.bool).plot()\n\n    def test_1d_x_y_kw(self):\n        z = np.arange(10)\n        da = DataArray(np.cos(z), dims=[""z""], coords=[z], name=""f"")\n\n        xy = [[None, None], [None, ""z""], [""z"", None]]\n\n        f, ax = plt.subplots(3, 1)\n        for aa, (x, y) in enumerate(xy):\n            da.plot(x=x, y=y, ax=ax.flat[aa])\n\n        with raises_regex(ValueError, ""Cannot specify both""):\n            da.plot(x=""z"", y=""z"")\n\n        error_msg = ""must be one of None, \'z\'""\n        with raises_regex(ValueError, f""x {error_msg}""):\n            da.plot(x=""f"")\n\n        with raises_regex(ValueError, f""y {error_msg}""):\n            da.plot(y=""f"")\n\n    def test_multiindex_level_as_coord(self):\n        da = xr.DataArray(\n            np.arange(5),\n            dims=""x"",\n            coords=dict(a=(""x"", np.arange(5)), b=(""x"", np.arange(5, 10))),\n        )\n        da = da.set_index(x=[""a"", ""b""])\n\n        for x in [""a"", ""b""]:\n            h = da.plot(x=x)[0]\n            assert_array_equal(h.get_xdata(), da[x].values)\n\n        for y in [""a"", ""b""]:\n            h = da.plot(y=y)[0]\n            assert_array_equal(h.get_ydata(), da[y].values)\n\n    # Test for bug in GH issue #2725\n    def test_infer_line_data(self):\n        current = DataArray(\n            name=""I"",\n            data=np.array([5, 8]),\n            dims=[""t""],\n            coords={\n                ""t"": ([""t""], np.array([0.1, 0.2])),\n                ""V"": ([""t""], np.array([100, 200])),\n            },\n        )\n\n        # Plot current against voltage\n        line = current.plot.line(x=""V"")[0]\n        assert_array_equal(line.get_xdata(), current.coords[""V""].values)\n\n        # Plot current against time\n        line = current.plot.line()[0]\n        assert_array_equal(line.get_xdata(), current.coords[""t""].values)\n\n    def test_line_plot_along_1d_coord(self):\n        # Test for bug in GH #3334\n        x_coord = xr.DataArray(data=[0.1, 0.2], dims=[""x""])\n        t_coord = xr.DataArray(data=[10, 20], dims=[""t""])\n\n        da = xr.DataArray(\n            data=np.array([[0, 1], [5, 9]]),\n            dims=[""x"", ""t""],\n            coords={""x"": x_coord, ""time"": t_coord},\n        )\n\n        line = da.plot(x=""time"", hue=""x"")[0]\n        assert_array_equal(line.get_xdata(), da.coords[""time""].values)\n\n        line = da.plot(y=""time"", hue=""x"")[0]\n        assert_array_equal(line.get_ydata(), da.coords[""time""].values)\n\n    def test_2d_line(self):\n        with raises_regex(ValueError, ""hue""):\n            self.darray[:, :, 0].plot.line()\n\n        self.darray[:, :, 0].plot.line(hue=""dim_1"")\n        self.darray[:, :, 0].plot.line(x=""dim_1"")\n        self.darray[:, :, 0].plot.line(y=""dim_1"")\n        self.darray[:, :, 0].plot.line(x=""dim_0"", hue=""dim_1"")\n        self.darray[:, :, 0].plot.line(y=""dim_0"", hue=""dim_1"")\n\n        with raises_regex(ValueError, ""Cannot""):\n            self.darray[:, :, 0].plot.line(x=""dim_1"", y=""dim_0"", hue=""dim_1"")\n\n    def test_2d_line_accepts_legend_kw(self):\n        self.darray[:, :, 0].plot.line(x=""dim_0"", add_legend=False)\n        assert not plt.gca().get_legend()\n        plt.cla()\n        self.darray[:, :, 0].plot.line(x=""dim_0"", add_legend=True)\n        assert plt.gca().get_legend()\n        # check whether legend title is set\n        assert plt.gca().get_legend().get_title().get_text() == ""dim_1""\n\n    def test_2d_line_accepts_x_kw(self):\n        self.darray[:, :, 0].plot.line(x=""dim_0"")\n        assert plt.gca().get_xlabel() == ""dim_0""\n        plt.cla()\n        self.darray[:, :, 0].plot.line(x=""dim_1"")\n        assert plt.gca().get_xlabel() == ""dim_1""\n\n    def test_2d_line_accepts_hue_kw(self):\n        self.darray[:, :, 0].plot.line(hue=""dim_0"")\n        assert plt.gca().get_legend().get_title().get_text() == ""dim_0""\n        plt.cla()\n        self.darray[:, :, 0].plot.line(hue=""dim_1"")\n        assert plt.gca().get_legend().get_title().get_text() == ""dim_1""\n\n    def test_2d_coords_line_plot(self):\n        lon, lat = np.meshgrid(np.linspace(-20, 20, 5), np.linspace(0, 30, 4))\n        lon += lat / 10\n        lat += lon / 10\n        da = xr.DataArray(\n            np.arange(20).reshape(4, 5),\n            dims=[""y"", ""x""],\n            coords={""lat"": ((""y"", ""x""), lat), ""lon"": ((""y"", ""x""), lon)},\n        )\n\n        hdl = da.plot.line(x=""lon"", hue=""x"")\n        assert len(hdl) == 5\n\n        plt.clf()\n        hdl = da.plot.line(x=""lon"", hue=""y"")\n        assert len(hdl) == 4\n\n        with pytest.raises(ValueError, match=""For 2D inputs, hue must be a dimension""):\n            da.plot.line(x=""lon"", hue=""lat"")\n\n    def test_2d_coord_line_plot_coords_transpose_invariant(self):\n        # checks for bug reported in GH #3933\n        x = np.arange(10)\n        y = np.arange(20)\n        ds = xr.Dataset(coords={""x"": x, ""y"": y})\n\n        for z in [ds.y + ds.x, ds.x + ds.y]:\n            ds = ds.assign_coords(z=z)\n            ds[""v""] = ds.x + ds.y\n            ds[""v""].plot.line(y=""z"", hue=""x"")\n\n    def test_2d_before_squeeze(self):\n        a = DataArray(easy_array((1, 5)))\n        a.plot()\n\n    def test2d_uniform_calls_imshow(self):\n        assert self.imshow_called(self.darray[:, :, 0].plot.imshow)\n\n    @pytest.mark.slow\n    def test2d_nonuniform_calls_contourf(self):\n        a = self.darray[:, :, 0]\n        a.coords[""dim_1""] = [2, 1, 89]\n        assert self.contourf_called(a.plot.contourf)\n\n    def test2d_1d_2d_coordinates_contourf(self):\n        sz = (20, 10)\n        depth = easy_array(sz)\n        a = DataArray(\n            easy_array(sz),\n            dims=[""z"", ""time""],\n            coords={""depth"": ([""z"", ""time""], depth), ""time"": np.linspace(0, 1, sz[1])},\n        )\n\n        a.plot.contourf(x=""time"", y=""depth"")\n        a.plot.contourf(x=""depth"", y=""time"")\n\n    def test_contourf_cmap_set(self):\n        a = DataArray(easy_array((4, 4)), dims=[""z"", ""time""])\n\n        cmap = mpl.cm.viridis\n\n        # deepcopy to ensure cmap is not changed by contourf()\n        # Set vmin and vmax so that _build_discrete_colormap is called with\n        # extend=\'both\'. extend is passed to\n        # mpl.colors.from_levels_and_colors(), which returns a result with\n        # sensible under and over values if extend=\'both\', but not if\n        # extend=\'neither\' (but if extend=\'neither\' the under and over values\n        # would not be used because the data would all be within the plotted\n        # range)\n        pl = a.plot.contourf(cmap=deepcopy(cmap), vmin=0.1, vmax=0.9)\n\n        # check the set_bad color\n        assert np.all(\n            pl.cmap(np.ma.masked_invalid([np.nan]))[0]\n            == cmap(np.ma.masked_invalid([np.nan]))[0]\n        )\n\n        # check the set_under color\n        assert pl.cmap(-np.inf) == cmap(-np.inf)\n\n        # check the set_over color\n        assert pl.cmap(np.inf) == cmap(np.inf)\n\n    def test_contourf_cmap_set_with_bad_under_over(self):\n        a = DataArray(easy_array((4, 4)), dims=[""z"", ""time""])\n\n        # Make a copy here because we want a local cmap that we will modify.\n        # Use deepcopy because matplotlib Colormap objects have tuple members\n        # and we want to ensure we do not change the original.\n        cmap = deepcopy(mpl.cm.viridis)\n\n        cmap.set_bad(""w"")\n        # check we actually changed the set_bad color\n        assert np.all(\n            cmap(np.ma.masked_invalid([np.nan]))[0]\n            != mpl.cm.viridis(np.ma.masked_invalid([np.nan]))[0]\n        )\n\n        cmap.set_under(""r"")\n        # check we actually changed the set_under color\n        assert cmap(-np.inf) != mpl.cm.viridis(-np.inf)\n\n        cmap.set_over(""g"")\n        # check we actually changed the set_over color\n        assert cmap(np.inf) != mpl.cm.viridis(-np.inf)\n\n        # deepcopy to ensure cmap is not changed by contourf()\n        pl = a.plot.contourf(cmap=deepcopy(cmap))\n\n        # check the set_bad color has been kept\n        assert np.all(\n            pl.cmap(np.ma.masked_invalid([np.nan]))[0]\n            == cmap(np.ma.masked_invalid([np.nan]))[0]\n        )\n\n        # check the set_under color has been kept\n        assert pl.cmap(-np.inf) == cmap(-np.inf)\n\n        # check the set_over color has been kept\n        assert pl.cmap(np.inf) == cmap(np.inf)\n\n    def test3d(self):\n        self.darray.plot()\n\n    def test_can_pass_in_axis(self):\n        self.pass_in_axis(self.darray.plot)\n\n    def test__infer_interval_breaks(self):\n        assert_array_equal([-0.5, 0.5, 1.5], _infer_interval_breaks([0, 1]))\n        assert_array_equal(\n            [-0.5, 0.5, 5.0, 9.5, 10.5], _infer_interval_breaks([0, 1, 9, 10])\n        )\n        assert_array_equal(\n            pd.date_range(""20000101"", periods=4) - np.timedelta64(12, ""h""),\n            _infer_interval_breaks(pd.date_range(""20000101"", periods=3)),\n        )\n\n        # make a bounded 2D array that we will center and re-infer\n        xref, yref = np.meshgrid(np.arange(6), np.arange(5))\n        cx = (xref[1:, 1:] + xref[:-1, :-1]) / 2\n        cy = (yref[1:, 1:] + yref[:-1, :-1]) / 2\n        x = _infer_interval_breaks(cx, axis=1)\n        x = _infer_interval_breaks(x, axis=0)\n        y = _infer_interval_breaks(cy, axis=1)\n        y = _infer_interval_breaks(y, axis=0)\n        np.testing.assert_allclose(xref, x)\n        np.testing.assert_allclose(yref, y)\n\n        # test that ValueError is raised for non-monotonic 1D inputs\n        with pytest.raises(ValueError):\n            _infer_interval_breaks(np.array([0, 2, 1]), check_monotonic=True)\n\n    def test_geo_data(self):\n        # Regression test for gh2250\n        # Realistic coordinates taken from the example dataset\n        lat = np.array(\n            [\n                [16.28, 18.48, 19.58, 19.54, 18.35],\n                [28.07, 30.52, 31.73, 31.68, 30.37],\n                [39.65, 42.27, 43.56, 43.51, 42.11],\n                [50.52, 53.22, 54.55, 54.50, 53.06],\n            ]\n        )\n        lon = np.array(\n            [\n                [-126.13, -113.69, -100.92, -88.04, -75.29],\n                [-129.27, -115.62, -101.54, -87.32, -73.26],\n                [-133.10, -118.00, -102.31, -86.42, -70.76],\n                [-137.85, -120.99, -103.28, -85.28, -67.62],\n            ]\n        )\n        data = np.sqrt(lon ** 2 + lat ** 2)\n        da = DataArray(\n            data,\n            dims=(""y"", ""x""),\n            coords={""lon"": ((""y"", ""x""), lon), ""lat"": ((""y"", ""x""), lat)},\n        )\n        da.plot(x=""lon"", y=""lat"")\n        ax = plt.gca()\n        assert ax.has_data()\n        da.plot(x=""lat"", y=""lon"")\n        ax = plt.gca()\n        assert ax.has_data()\n\n    def test_datetime_dimension(self):\n        nrow = 3\n        ncol = 4\n        time = pd.date_range(""2000-01-01"", periods=nrow)\n        a = DataArray(\n            easy_array((nrow, ncol)), coords=[(""time"", time), (""y"", range(ncol))]\n        )\n        a.plot()\n        ax = plt.gca()\n        assert ax.has_data()\n\n    @pytest.mark.slow\n    @pytest.mark.filterwarnings(""ignore:tight_layout cannot"")\n    def test_convenient_facetgrid(self):\n        a = easy_array((10, 15, 4))\n        d = DataArray(a, dims=[""y"", ""x"", ""z""])\n        d.coords[""z""] = list(""abcd"")\n        g = d.plot(x=""x"", y=""y"", col=""z"", col_wrap=2, cmap=""cool"")\n\n        assert_array_equal(g.axes.shape, [2, 2])\n        for ax in g.axes.flat:\n            assert ax.has_data()\n\n        with raises_regex(ValueError, ""[Ff]acet""):\n            d.plot(x=""x"", y=""y"", col=""z"", ax=plt.gca())\n\n        with raises_regex(ValueError, ""[Ff]acet""):\n            d[0].plot(x=""x"", y=""y"", col=""z"", ax=plt.gca())\n\n    @pytest.mark.slow\n    def test_subplot_kws(self):\n        a = easy_array((10, 15, 4))\n        d = DataArray(a, dims=[""y"", ""x"", ""z""])\n        d.coords[""z""] = list(""abcd"")\n        g = d.plot(\n            x=""x"",\n            y=""y"",\n            col=""z"",\n            col_wrap=2,\n            cmap=""cool"",\n            subplot_kws=dict(facecolor=""r""),\n        )\n        for ax in g.axes.flat:\n            # mpl V2\n            assert ax.get_facecolor()[0:3] == mpl.colors.to_rgb(""r"")\n\n    @pytest.mark.slow\n    def test_plot_size(self):\n        self.darray[:, 0, 0].plot(figsize=(13, 5))\n        assert tuple(plt.gcf().get_size_inches()) == (13, 5)\n\n        self.darray.plot(figsize=(13, 5))\n        assert tuple(plt.gcf().get_size_inches()) == (13, 5)\n\n        self.darray.plot(size=5)\n        assert plt.gcf().get_size_inches()[1] == 5\n\n        self.darray.plot(size=5, aspect=2)\n        assert tuple(plt.gcf().get_size_inches()) == (10, 5)\n\n        with raises_regex(ValueError, ""cannot provide both""):\n            self.darray.plot(ax=plt.gca(), figsize=(3, 4))\n\n        with raises_regex(ValueError, ""cannot provide both""):\n            self.darray.plot(size=5, figsize=(3, 4))\n\n        with raises_regex(ValueError, ""cannot provide both""):\n            self.darray.plot(size=5, ax=plt.gca())\n\n        with raises_regex(ValueError, ""cannot provide `aspect`""):\n            self.darray.plot(aspect=1)\n\n    @pytest.mark.slow\n    @pytest.mark.filterwarnings(""ignore:tight_layout cannot"")\n    def test_convenient_facetgrid_4d(self):\n        a = easy_array((10, 15, 2, 3))\n        d = DataArray(a, dims=[""y"", ""x"", ""columns"", ""rows""])\n        g = d.plot(x=""x"", y=""y"", col=""columns"", row=""rows"")\n\n        assert_array_equal(g.axes.shape, [3, 2])\n        for ax in g.axes.flat:\n            assert ax.has_data()\n\n        with raises_regex(ValueError, ""[Ff]acet""):\n            d.plot(x=""x"", y=""y"", col=""columns"", ax=plt.gca())\n\n    def test_coord_with_interval(self):\n        """"""Test line plot with intervals.""""""\n        bins = [-1, 0, 1, 2]\n        self.darray.groupby_bins(""dim_0"", bins).mean(...).plot()\n\n    def test_coord_with_interval_x(self):\n        """"""Test line plot with intervals explicitly on x axis.""""""\n        bins = [-1, 0, 1, 2]\n        self.darray.groupby_bins(""dim_0"", bins).mean(...).plot(x=""dim_0_bins"")\n\n    def test_coord_with_interval_y(self):\n        """"""Test line plot with intervals explicitly on y axis.""""""\n        bins = [-1, 0, 1, 2]\n        self.darray.groupby_bins(""dim_0"", bins).mean(...).plot(y=""dim_0_bins"")\n\n    def test_coord_with_interval_xy(self):\n        """"""Test line plot with intervals on both x and y axes.""""""\n        bins = [-1, 0, 1, 2]\n        self.darray.groupby_bins(""dim_0"", bins).mean(...).dim_0_bins.plot()\n\n\nclass TestPlot1D(PlotTestCase):\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        d = [0, 1.1, 0, 2]\n        self.darray = DataArray(d, coords={""period"": range(len(d))}, dims=""period"")\n        self.darray.period.attrs[""units""] = ""s""\n\n    def test_xlabel_is_index_name(self):\n        self.darray.plot()\n        assert ""period [s]"" == plt.gca().get_xlabel()\n\n    def test_no_label_name_on_x_axis(self):\n        self.darray.plot(y=""period"")\n        assert """" == plt.gca().get_xlabel()\n\n    def test_no_label_name_on_y_axis(self):\n        self.darray.plot()\n        assert """" == plt.gca().get_ylabel()\n\n    def test_ylabel_is_data_name(self):\n        self.darray.name = ""temperature""\n        self.darray.attrs[""units""] = ""degrees_Celsius""\n        self.darray.plot()\n        assert ""temperature [degrees_Celsius]"" == plt.gca().get_ylabel()\n\n    def test_xlabel_is_data_name(self):\n        self.darray.name = ""temperature""\n        self.darray.attrs[""units""] = ""degrees_Celsius""\n        self.darray.plot(y=""period"")\n        assert ""temperature [degrees_Celsius]"" == plt.gca().get_xlabel()\n\n    def test_format_string(self):\n        self.darray.plot.line(""ro"")\n\n    def test_can_pass_in_axis(self):\n        self.pass_in_axis(self.darray.plot.line)\n\n    def test_nonnumeric_index_raises_typeerror(self):\n        a = DataArray([1, 2, 3], {""letter"": [""a"", ""b"", ""c""]}, dims=""letter"")\n        with raises_regex(TypeError, r""[Pp]lot""):\n            a.plot.line()\n\n    def test_primitive_returned(self):\n        p = self.darray.plot.line()\n        assert isinstance(p[0], mpl.lines.Line2D)\n\n    @pytest.mark.slow\n    def test_plot_nans(self):\n        self.darray[1] = np.nan\n        self.darray.plot.line()\n\n    def test_x_ticks_are_rotated_for_time(self):\n        time = pd.date_range(""2000-01-01"", ""2000-01-10"")\n        a = DataArray(np.arange(len(time)), [(""t"", time)])\n        a.plot.line()\n        rotation = plt.gca().get_xticklabels()[0].get_rotation()\n        assert rotation != 0\n\n    def test_xyincrease_false_changes_axes(self):\n        self.darray.plot.line(xincrease=False, yincrease=False)\n        xlim = plt.gca().get_xlim()\n        ylim = plt.gca().get_ylim()\n        diffs = xlim[1] - xlim[0], ylim[1] - ylim[0]\n        assert all(x < 0 for x in diffs)\n\n    def test_slice_in_title(self):\n        self.darray.coords[""d""] = 10\n        self.darray.plot.line()\n        title = plt.gca().get_title()\n        assert ""d = 10"" == title\n\n\nclass TestPlotStep(PlotTestCase):\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        self.darray = DataArray(easy_array((2, 3, 4)))\n\n    def test_step(self):\n        self.darray[0, 0].plot.step()\n\n    @pytest.mark.parametrize(""ds"", [""pre"", ""post"", ""mid""])\n    def test_step_with_drawstyle(self, ds):\n        self.darray[0, 0].plot.step(drawstyle=ds)\n\n    def test_coord_with_interval_step(self):\n        """"""Test step plot with intervals.""""""\n        bins = [-1, 0, 1, 2]\n        self.darray.groupby_bins(""dim_0"", bins).mean(...).plot.step()\n        assert len(plt.gca().lines[0].get_xdata()) == ((len(bins) - 1) * 2)\n\n    def test_coord_with_interval_step_x(self):\n        """"""Test step plot with intervals explicitly on x axis.""""""\n        bins = [-1, 0, 1, 2]\n        self.darray.groupby_bins(""dim_0"", bins).mean(...).plot.step(x=""dim_0_bins"")\n        assert len(plt.gca().lines[0].get_xdata()) == ((len(bins) - 1) * 2)\n\n    def test_coord_with_interval_step_y(self):\n        """"""Test step plot with intervals explicitly on y axis.""""""\n        bins = [-1, 0, 1, 2]\n        self.darray.groupby_bins(""dim_0"", bins).mean(...).plot.step(y=""dim_0_bins"")\n        assert len(plt.gca().lines[0].get_xdata()) == ((len(bins) - 1) * 2)\n\n\nclass TestPlotHistogram(PlotTestCase):\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        self.darray = DataArray(easy_array((2, 3, 4)))\n\n    def test_3d_array(self):\n        self.darray.plot.hist()\n\n    def test_xlabel_uses_name(self):\n        self.darray.name = ""testpoints""\n        self.darray.attrs[""units""] = ""testunits""\n        self.darray.plot.hist()\n        assert ""testpoints [testunits]"" == plt.gca().get_xlabel()\n\n    def test_title_is_histogram(self):\n        self.darray.plot.hist()\n        assert ""Histogram"" == plt.gca().get_title()\n\n    def test_can_pass_in_kwargs(self):\n        nbins = 5\n        self.darray.plot.hist(bins=nbins)\n        assert nbins == len(plt.gca().patches)\n\n    def test_can_pass_in_axis(self):\n        self.pass_in_axis(self.darray.plot.hist)\n\n    def test_primitive_returned(self):\n        h = self.darray.plot.hist()\n        assert isinstance(h[-1][0], mpl.patches.Rectangle)\n\n    @pytest.mark.slow\n    def test_plot_nans(self):\n        self.darray[0, 0, 0] = np.nan\n        self.darray.plot.hist()\n\n    def test_hist_coord_with_interval(self):\n        (\n            self.darray.groupby_bins(""dim_0"", [-1, 0, 1, 2])\n            .mean(...)\n            .plot.hist(range=(-1, 2))\n        )\n\n\n@requires_matplotlib\nclass TestDetermineCmapParams:\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        self.data = np.linspace(0, 1, num=100)\n\n    def test_robust(self):\n        cmap_params = _determine_cmap_params(self.data, robust=True)\n        assert cmap_params[""vmin""] == np.percentile(self.data, 2)\n        assert cmap_params[""vmax""] == np.percentile(self.data, 98)\n        assert cmap_params[""cmap""] == ""viridis""\n        assert cmap_params[""extend""] == ""both""\n        assert cmap_params[""levels""] is None\n        assert cmap_params[""norm""] is None\n\n    def test_center(self):\n        cmap_params = _determine_cmap_params(self.data, center=0.5)\n        assert cmap_params[""vmax""] - 0.5 == 0.5 - cmap_params[""vmin""]\n        assert cmap_params[""cmap""] == ""RdBu_r""\n        assert cmap_params[""extend""] == ""neither""\n        assert cmap_params[""levels""] is None\n        assert cmap_params[""norm""] is None\n\n    def test_cmap_sequential_option(self):\n        with xr.set_options(cmap_sequential=""magma""):\n            cmap_params = _determine_cmap_params(self.data)\n            assert cmap_params[""cmap""] == ""magma""\n\n    def test_cmap_sequential_explicit_option(self):\n        with xr.set_options(cmap_sequential=mpl.cm.magma):\n            cmap_params = _determine_cmap_params(self.data)\n            assert cmap_params[""cmap""] == mpl.cm.magma\n\n    def test_cmap_divergent_option(self):\n        with xr.set_options(cmap_divergent=""magma""):\n            cmap_params = _determine_cmap_params(self.data, center=0.5)\n            assert cmap_params[""cmap""] == ""magma""\n\n    def test_nan_inf_are_ignored(self):\n        cmap_params1 = _determine_cmap_params(self.data)\n        data = self.data\n        data[50:55] = np.nan\n        data[56:60] = np.inf\n        cmap_params2 = _determine_cmap_params(data)\n        assert cmap_params1[""vmin""] == cmap_params2[""vmin""]\n        assert cmap_params1[""vmax""] == cmap_params2[""vmax""]\n\n    @pytest.mark.slow\n    def test_integer_levels(self):\n        data = self.data + 1\n\n        # default is to cover full data range but with no guarantee on Nlevels\n        for level in np.arange(2, 10, dtype=int):\n            cmap_params = _determine_cmap_params(data, levels=level)\n            assert cmap_params[""vmin""] == cmap_params[""levels""][0]\n            assert cmap_params[""vmax""] == cmap_params[""levels""][-1]\n            assert cmap_params[""extend""] == ""neither""\n\n        # with min max we are more strict\n        cmap_params = _determine_cmap_params(\n            data, levels=5, vmin=0, vmax=5, cmap=""Blues""\n        )\n        assert cmap_params[""vmin""] == 0\n        assert cmap_params[""vmax""] == 5\n        assert cmap_params[""vmin""] == cmap_params[""levels""][0]\n        assert cmap_params[""vmax""] == cmap_params[""levels""][-1]\n        assert cmap_params[""cmap""].name == ""Blues""\n        assert cmap_params[""extend""] == ""neither""\n        assert cmap_params[""cmap""].N == 4\n        assert cmap_params[""norm""].N == 5\n\n        cmap_params = _determine_cmap_params(data, levels=5, vmin=0.5, vmax=1.5)\n        assert cmap_params[""cmap""].name == ""viridis""\n        assert cmap_params[""extend""] == ""max""\n\n        cmap_params = _determine_cmap_params(data, levels=5, vmin=1.5)\n        assert cmap_params[""cmap""].name == ""viridis""\n        assert cmap_params[""extend""] == ""min""\n\n        cmap_params = _determine_cmap_params(data, levels=5, vmin=1.3, vmax=1.5)\n        assert cmap_params[""cmap""].name == ""viridis""\n        assert cmap_params[""extend""] == ""both""\n\n    def test_list_levels(self):\n        data = self.data + 1\n\n        orig_levels = [0, 1, 2, 3, 4, 5]\n        # vmin and vmax should be ignored if levels are explicitly provided\n        cmap_params = _determine_cmap_params(data, levels=orig_levels, vmin=0, vmax=3)\n        assert cmap_params[""vmin""] == 0\n        assert cmap_params[""vmax""] == 5\n        assert cmap_params[""cmap""].N == 5\n        assert cmap_params[""norm""].N == 6\n\n        for wrap_levels in [list, np.array, pd.Index, DataArray]:\n            cmap_params = _determine_cmap_params(data, levels=wrap_levels(orig_levels))\n            assert_array_equal(cmap_params[""levels""], orig_levels)\n\n    def test_divergentcontrol(self):\n        neg = self.data - 0.1\n        pos = self.data\n\n        # Default with positive data will be a normal cmap\n        cmap_params = _determine_cmap_params(pos)\n        assert cmap_params[""vmin""] == 0\n        assert cmap_params[""vmax""] == 1\n        assert cmap_params[""cmap""] == ""viridis""\n\n        # Default with negative data will be a divergent cmap\n        cmap_params = _determine_cmap_params(neg)\n        assert cmap_params[""vmin""] == -0.9\n        assert cmap_params[""vmax""] == 0.9\n        assert cmap_params[""cmap""] == ""RdBu_r""\n\n        # Setting vmin or vmax should prevent this only if center is false\n        cmap_params = _determine_cmap_params(neg, vmin=-0.1, center=False)\n        assert cmap_params[""vmin""] == -0.1\n        assert cmap_params[""vmax""] == 0.9\n        assert cmap_params[""cmap""] == ""viridis""\n        cmap_params = _determine_cmap_params(neg, vmax=0.5, center=False)\n        assert cmap_params[""vmin""] == -0.1\n        assert cmap_params[""vmax""] == 0.5\n        assert cmap_params[""cmap""] == ""viridis""\n\n        # Setting center=False too\n        cmap_params = _determine_cmap_params(neg, center=False)\n        assert cmap_params[""vmin""] == -0.1\n        assert cmap_params[""vmax""] == 0.9\n        assert cmap_params[""cmap""] == ""viridis""\n\n        # However, I should still be able to set center and have a div cmap\n        cmap_params = _determine_cmap_params(neg, center=0)\n        assert cmap_params[""vmin""] == -0.9\n        assert cmap_params[""vmax""] == 0.9\n        assert cmap_params[""cmap""] == ""RdBu_r""\n\n        # Setting vmin or vmax alone will force symmetric bounds around center\n        cmap_params = _determine_cmap_params(neg, vmin=-0.1)\n        assert cmap_params[""vmin""] == -0.1\n        assert cmap_params[""vmax""] == 0.1\n        assert cmap_params[""cmap""] == ""RdBu_r""\n        cmap_params = _determine_cmap_params(neg, vmax=0.5)\n        assert cmap_params[""vmin""] == -0.5\n        assert cmap_params[""vmax""] == 0.5\n        assert cmap_params[""cmap""] == ""RdBu_r""\n        cmap_params = _determine_cmap_params(neg, vmax=0.6, center=0.1)\n        assert cmap_params[""vmin""] == -0.4\n        assert cmap_params[""vmax""] == 0.6\n        assert cmap_params[""cmap""] == ""RdBu_r""\n\n        # But this is only true if vmin or vmax are negative\n        cmap_params = _determine_cmap_params(pos, vmin=-0.1)\n        assert cmap_params[""vmin""] == -0.1\n        assert cmap_params[""vmax""] == 0.1\n        assert cmap_params[""cmap""] == ""RdBu_r""\n        cmap_params = _determine_cmap_params(pos, vmin=0.1)\n        assert cmap_params[""vmin""] == 0.1\n        assert cmap_params[""vmax""] == 1\n        assert cmap_params[""cmap""] == ""viridis""\n        cmap_params = _determine_cmap_params(pos, vmax=0.5)\n        assert cmap_params[""vmin""] == 0\n        assert cmap_params[""vmax""] == 0.5\n        assert cmap_params[""cmap""] == ""viridis""\n\n        # If both vmin and vmax are provided, output is non-divergent\n        cmap_params = _determine_cmap_params(neg, vmin=-0.2, vmax=0.6)\n        assert cmap_params[""vmin""] == -0.2\n        assert cmap_params[""vmax""] == 0.6\n        assert cmap_params[""cmap""] == ""viridis""\n\n        # regression test for GH3524\n        # infer diverging colormap from divergent levels\n        cmap_params = _determine_cmap_params(pos, levels=[-0.1, 0, 1])\n        # specifying levels makes cmap a Colormap object\n        assert cmap_params[""cmap""].name == ""RdBu_r""\n\n    def test_norm_sets_vmin_vmax(self):\n        vmin = self.data.min()\n        vmax = self.data.max()\n\n        for norm, extend, levels in zip(\n            [\n                mpl.colors.Normalize(),\n                mpl.colors.Normalize(),\n                mpl.colors.Normalize(vmin + 0.1, vmax - 0.1),\n                mpl.colors.Normalize(None, vmax - 0.1),\n                mpl.colors.Normalize(vmin + 0.1, None),\n            ],\n            [""neither"", ""neither"", ""both"", ""max"", ""min""],\n            [7, None, None, None, None],\n        ):\n\n            test_min = vmin if norm.vmin is None else norm.vmin\n            test_max = vmax if norm.vmax is None else norm.vmax\n\n            cmap_params = _determine_cmap_params(self.data, norm=norm, levels=levels)\n            assert cmap_params[""vmin""] == test_min\n            assert cmap_params[""vmax""] == test_max\n            assert cmap_params[""extend""] == extend\n            assert cmap_params[""norm""] == norm\n\n\n@requires_matplotlib\nclass TestDiscreteColorMap:\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        x = np.arange(start=0, stop=10, step=2)\n        y = np.arange(start=9, stop=-7, step=-3)\n        xy = np.dstack(np.meshgrid(x, y))\n        distance = np.linalg.norm(xy, axis=2)\n        self.darray = DataArray(distance, list(zip((""y"", ""x""), (y, x))))\n        self.data_min = distance.min()\n        self.data_max = distance.max()\n\n    @pytest.mark.slow\n    def test_recover_from_seaborn_jet_exception(self):\n        pal = _color_palette(""jet"", 4)\n        assert type(pal) == np.ndarray\n        assert len(pal) == 4\n\n    @pytest.mark.slow\n    def test_build_discrete_cmap(self):\n        for (cmap, levels, extend, filled) in [\n            (""jet"", [0, 1], ""both"", False),\n            (""hot"", [-4, 4], ""max"", True),\n        ]:\n            ncmap, cnorm = _build_discrete_cmap(cmap, levels, extend, filled)\n            assert ncmap.N == len(levels) - 1\n            assert len(ncmap.colors) == len(levels) - 1\n            assert cnorm.N == len(levels)\n            assert_array_equal(cnorm.boundaries, levels)\n            assert max(levels) == cnorm.vmax\n            assert min(levels) == cnorm.vmin\n            if filled:\n                assert ncmap.colorbar_extend == extend\n            else:\n                assert ncmap.colorbar_extend == ""max""\n\n    @pytest.mark.slow\n    def test_discrete_colormap_list_of_levels(self):\n        for extend, levels in [\n            (""max"", [-1, 2, 4, 8, 10]),\n            (""both"", [2, 5, 10, 11]),\n            (""neither"", [0, 5, 10, 15]),\n            (""min"", [2, 5, 10, 15]),\n        ]:\n            for kind in [""imshow"", ""pcolormesh"", ""contourf"", ""contour""]:\n                primitive = getattr(self.darray.plot, kind)(levels=levels)\n                assert_array_equal(levels, primitive.norm.boundaries)\n                assert max(levels) == primitive.norm.vmax\n                assert min(levels) == primitive.norm.vmin\n                if kind != ""contour"":\n                    assert extend == primitive.cmap.colorbar_extend\n                else:\n                    assert ""max"" == primitive.cmap.colorbar_extend\n                assert len(levels) - 1 == len(primitive.cmap.colors)\n\n    @pytest.mark.slow\n    def test_discrete_colormap_int_levels(self):\n        for extend, levels, vmin, vmax, cmap in [\n            (""neither"", 7, None, None, None),\n            (""neither"", 7, None, 20, mpl.cm.RdBu),\n            (""both"", 7, 4, 8, None),\n            (""min"", 10, 4, 15, None),\n        ]:\n            for kind in [""imshow"", ""pcolormesh"", ""contourf"", ""contour""]:\n                primitive = getattr(self.darray.plot, kind)(\n                    levels=levels, vmin=vmin, vmax=vmax, cmap=cmap\n                )\n                assert levels >= len(primitive.norm.boundaries) - 1\n                if vmax is None:\n                    assert primitive.norm.vmax >= self.data_max\n                else:\n                    assert primitive.norm.vmax >= vmax\n                if vmin is None:\n                    assert primitive.norm.vmin <= self.data_min\n                else:\n                    assert primitive.norm.vmin <= vmin\n                if kind != ""contour"":\n                    assert extend == primitive.cmap.colorbar_extend\n                else:\n                    assert ""max"" == primitive.cmap.colorbar_extend\n                assert levels >= len(primitive.cmap.colors)\n\n    def test_discrete_colormap_list_levels_and_vmin_or_vmax(self):\n        levels = [0, 5, 10, 15]\n        primitive = self.darray.plot(levels=levels, vmin=-3, vmax=20)\n        assert primitive.norm.vmax == max(levels)\n        assert primitive.norm.vmin == min(levels)\n\n    def test_discrete_colormap_provided_boundary_norm(self):\n        norm = mpl.colors.BoundaryNorm([0, 5, 10, 15], 4)\n        primitive = self.darray.plot.contourf(norm=norm)\n        np.testing.assert_allclose(primitive.levels, norm.boundaries)\n\n\nclass Common2dMixin:\n    """"""\n    Common tests for 2d plotting go here.\n\n    These tests assume that a staticmethod for `self.plotfunc` exists.\n    Should have the same name as the method.\n    """"""\n\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        da = DataArray(\n            easy_array((10, 15), start=-1),\n            dims=[""y"", ""x""],\n            coords={""y"": np.arange(10), ""x"": np.arange(15)},\n        )\n        # add 2d coords\n        ds = da.to_dataset(name=""testvar"")\n        x, y = np.meshgrid(da.x.values, da.y.values)\n        ds[""x2d""] = DataArray(x, dims=[""y"", ""x""])\n        ds[""y2d""] = DataArray(y, dims=[""y"", ""x""])\n        ds = ds.set_coords([""x2d"", ""y2d""])\n        # set darray and plot method\n        self.darray = ds.testvar\n\n        # Add CF-compliant metadata\n        self.darray.attrs[""long_name""] = ""a_long_name""\n        self.darray.attrs[""units""] = ""a_units""\n        self.darray.x.attrs[""long_name""] = ""x_long_name""\n        self.darray.x.attrs[""units""] = ""x_units""\n        self.darray.y.attrs[""long_name""] = ""y_long_name""\n        self.darray.y.attrs[""units""] = ""y_units""\n\n        self.plotmethod = getattr(self.darray.plot, self.plotfunc.__name__)\n\n    def test_label_names(self):\n        self.plotmethod()\n        assert ""x_long_name [x_units]"" == plt.gca().get_xlabel()\n        assert ""y_long_name [y_units]"" == plt.gca().get_ylabel()\n\n    def test_1d_raises_valueerror(self):\n        with raises_regex(ValueError, r""DataArray must be 2d""):\n            self.plotfunc(self.darray[0, :])\n\n    def test_bool(self):\n        xr.ones_like(self.darray, dtype=np.bool).plot()\n\n    def test_complex_raises_typeerror(self):\n        with raises_regex(TypeError, ""complex128""):\n            (self.darray + 1j).plot()\n\n    def test_3d_raises_valueerror(self):\n        a = DataArray(easy_array((2, 3, 4)))\n        if self.plotfunc.__name__ == ""imshow"":\n            pytest.skip()\n        with raises_regex(ValueError, r""DataArray must be 2d""):\n            self.plotfunc(a)\n\n    def test_nonnumeric_index_raises_typeerror(self):\n        a = DataArray(easy_array((3, 2)), coords=[[""a"", ""b"", ""c""], [""d"", ""e""]])\n        with raises_regex(TypeError, r""[Pp]lot""):\n            self.plotfunc(a)\n\n    def test_multiindex_raises_typeerror(self):\n        a = DataArray(\n            easy_array((3, 2)),\n            dims=(""x"", ""y""),\n            coords=dict(x=(""x"", [0, 1, 2]), a=(""y"", [0, 1]), b=(""y"", [2, 3])),\n        )\n        a = a.set_index(y=(""a"", ""b""))\n        with raises_regex(TypeError, r""[Pp]lot""):\n            self.plotfunc(a)\n\n    def test_can_pass_in_axis(self):\n        self.pass_in_axis(self.plotmethod)\n\n    def test_xyincrease_defaults(self):\n\n        # With default settings the axis must be ordered regardless\n        # of the coords order.\n        self.plotfunc(DataArray(easy_array((3, 2)), coords=[[1, 2, 3], [1, 2]]))\n        bounds = plt.gca().get_ylim()\n        assert bounds[0] < bounds[1]\n        bounds = plt.gca().get_xlim()\n        assert bounds[0] < bounds[1]\n        # Inverted coords\n        self.plotfunc(DataArray(easy_array((3, 2)), coords=[[3, 2, 1], [2, 1]]))\n        bounds = plt.gca().get_ylim()\n        assert bounds[0] < bounds[1]\n        bounds = plt.gca().get_xlim()\n        assert bounds[0] < bounds[1]\n\n    def test_xyincrease_false_changes_axes(self):\n        self.plotmethod(xincrease=False, yincrease=False)\n        xlim = plt.gca().get_xlim()\n        ylim = plt.gca().get_ylim()\n        diffs = xlim[0] - 14, xlim[1] - 0, ylim[0] - 9, ylim[1] - 0\n        assert all(abs(x) < 1 for x in diffs)\n\n    def test_xyincrease_true_changes_axes(self):\n        self.plotmethod(xincrease=True, yincrease=True)\n        xlim = plt.gca().get_xlim()\n        ylim = plt.gca().get_ylim()\n        diffs = xlim[0] - 0, xlim[1] - 14, ylim[0] - 0, ylim[1] - 9\n        assert all(abs(x) < 1 for x in diffs)\n\n    def test_x_ticks_are_rotated_for_time(self):\n        time = pd.date_range(""2000-01-01"", ""2000-01-10"")\n        a = DataArray(np.random.randn(2, len(time)), [(""xx"", [1, 2]), (""t"", time)])\n        a.plot(x=""t"")\n        rotation = plt.gca().get_xticklabels()[0].get_rotation()\n        assert rotation != 0\n\n    def test_plot_nans(self):\n        x1 = self.darray[:5]\n        x2 = self.darray.copy()\n        x2[5:] = np.nan\n\n        clim1 = self.plotfunc(x1).get_clim()\n        clim2 = self.plotfunc(x2).get_clim()\n        assert clim1 == clim2\n\n    @pytest.mark.filterwarnings(""ignore::UserWarning"")\n    @pytest.mark.filterwarnings(""ignore:invalid value encountered"")\n    def test_can_plot_all_nans(self):\n        # regression test for issue #1780\n        self.plotfunc(DataArray(np.full((2, 2), np.nan)))\n\n    @pytest.mark.filterwarnings(""ignore: Attempting to set"")\n    def test_can_plot_axis_size_one(self):\n        if self.plotfunc.__name__ not in (""contour"", ""contourf""):\n            self.plotfunc(DataArray(np.ones((1, 1))))\n\n    def test_disallows_rgb_arg(self):\n        with pytest.raises(ValueError):\n            # Always invalid for most plots.  Invalid for imshow with 2D data.\n            self.plotfunc(DataArray(np.ones((2, 2))), rgb=""not None"")\n\n    def test_viridis_cmap(self):\n        cmap_name = self.plotmethod(cmap=""viridis"").get_cmap().name\n        assert ""viridis"" == cmap_name\n\n    def test_default_cmap(self):\n        cmap_name = self.plotmethod().get_cmap().name\n        assert ""RdBu_r"" == cmap_name\n\n        cmap_name = self.plotfunc(abs(self.darray)).get_cmap().name\n        assert ""viridis"" == cmap_name\n\n    @requires_seaborn\n    def test_seaborn_palette_as_cmap(self):\n        cmap_name = self.plotmethod(levels=2, cmap=""husl"").get_cmap().name\n        assert ""husl"" == cmap_name\n\n    def test_can_change_default_cmap(self):\n        cmap_name = self.plotmethod(cmap=""Blues"").get_cmap().name\n        assert ""Blues"" == cmap_name\n\n    def test_diverging_color_limits(self):\n        artist = self.plotmethod()\n        vmin, vmax = artist.get_clim()\n        assert round(abs(-vmin - vmax), 7) == 0\n\n    def test_xy_strings(self):\n        self.plotmethod(""y"", ""x"")\n        ax = plt.gca()\n        assert ""y_long_name [y_units]"" == ax.get_xlabel()\n        assert ""x_long_name [x_units]"" == ax.get_ylabel()\n\n    def test_positional_coord_string(self):\n        self.plotmethod(y=""x"")\n        ax = plt.gca()\n        assert ""x_long_name [x_units]"" == ax.get_ylabel()\n        assert ""y_long_name [y_units]"" == ax.get_xlabel()\n\n        self.plotmethod(x=""x"")\n        ax = plt.gca()\n        assert ""x_long_name [x_units]"" == ax.get_xlabel()\n        assert ""y_long_name [y_units]"" == ax.get_ylabel()\n\n    def test_bad_x_string_exception(self):\n\n        with raises_regex(ValueError, ""x and y cannot be equal.""):\n            self.plotmethod(x=""y"", y=""y"")\n\n        error_msg = ""must be one of None, \'x\', \'x2d\', \'y\', \'y2d\'""\n        with raises_regex(ValueError, f""x {error_msg}""):\n            self.plotmethod(""not_a_real_dim"", ""y"")\n        with raises_regex(ValueError, f""x {error_msg}""):\n            self.plotmethod(x=""not_a_real_dim"")\n        with raises_regex(ValueError, f""y {error_msg}""):\n            self.plotmethod(y=""not_a_real_dim"")\n        self.darray.coords[""z""] = 100\n\n    def test_coord_strings(self):\n        # 1d coords (same as dims)\n        assert {""x"", ""y""} == set(self.darray.dims)\n        self.plotmethod(y=""y"", x=""x"")\n\n    def test_non_linked_coords(self):\n        # plot with coordinate names that are not dimensions\n        self.darray.coords[""newy""] = self.darray.y + 150\n        # Normal case, without transpose\n        self.plotfunc(self.darray, x=""x"", y=""newy"")\n        ax = plt.gca()\n        assert ""x_long_name [x_units]"" == ax.get_xlabel()\n        assert ""newy"" == ax.get_ylabel()\n        # ax limits might change between plotfuncs\n        # simply ensure that these high coords were passed over\n        assert np.min(ax.get_ylim()) > 100.0\n\n    def test_non_linked_coords_transpose(self):\n        # plot with coordinate names that are not dimensions,\n        # and with transposed y and x axes\n        # This used to raise an error with pcolormesh and contour\n        # https://github.com/pydata/xarray/issues/788\n        self.darray.coords[""newy""] = self.darray.y + 150\n        self.plotfunc(self.darray, x=""newy"", y=""x"")\n        ax = plt.gca()\n        assert ""newy"" == ax.get_xlabel()\n        assert ""x_long_name [x_units]"" == ax.get_ylabel()\n        # ax limits might change between plotfuncs\n        # simply ensure that these high coords were passed over\n        assert np.min(ax.get_xlim()) > 100.0\n\n    def test_multiindex_level_as_coord(self):\n        da = DataArray(\n            easy_array((3, 2)),\n            dims=(""x"", ""y""),\n            coords=dict(x=(""x"", [0, 1, 2]), a=(""y"", [0, 1]), b=(""y"", [2, 3])),\n        )\n        da = da.set_index(y=[""a"", ""b""])\n\n        for x, y in ((""a"", ""x""), (""b"", ""x""), (""x"", ""a""), (""x"", ""b"")):\n            self.plotfunc(da, x=x, y=y)\n\n            ax = plt.gca()\n            assert x == ax.get_xlabel()\n            assert y == ax.get_ylabel()\n\n        with raises_regex(ValueError, ""levels of the same MultiIndex""):\n            self.plotfunc(da, x=""a"", y=""b"")\n\n        with raises_regex(ValueError, ""y must be one of None, \'a\', \'b\', \'x\'""):\n            self.plotfunc(da, x=""a"", y=""y"")\n\n    def test_default_title(self):\n        a = DataArray(easy_array((4, 3, 2)), dims=[""a"", ""b"", ""c""])\n        a.coords[""c""] = [0, 1]\n        a.coords[""d""] = ""foo""\n        self.plotfunc(a.isel(c=1))\n        title = plt.gca().get_title()\n        assert ""c = 1, d = foo"" == title or ""d = foo, c = 1"" == title\n\n    def test_colorbar_default_label(self):\n        self.plotmethod(add_colorbar=True)\n        assert ""a_long_name [a_units]"" in text_in_fig()\n\n    def test_no_labels(self):\n        self.darray.name = ""testvar""\n        self.darray.attrs[""units""] = ""test_units""\n        self.plotmethod(add_labels=False)\n        alltxt = text_in_fig()\n        for string in [\n            ""x_long_name [x_units]"",\n            ""y_long_name [y_units]"",\n            ""testvar [test_units]"",\n        ]:\n            assert string not in alltxt\n\n    def test_colorbar_kwargs(self):\n        # replace label\n        self.darray.attrs.pop(""long_name"")\n        self.darray.attrs[""units""] = ""test_units""\n        # check default colorbar label\n        self.plotmethod(add_colorbar=True)\n        alltxt = text_in_fig()\n        assert ""testvar [test_units]"" in alltxt\n        self.darray.attrs.pop(""units"")\n\n        self.darray.name = ""testvar""\n        self.plotmethod(add_colorbar=True, cbar_kwargs={""label"": ""MyLabel""})\n        alltxt = text_in_fig()\n        assert ""MyLabel"" in alltxt\n        assert ""testvar"" not in alltxt\n        # you can use anything accepted by the dict constructor as well\n        self.plotmethod(add_colorbar=True, cbar_kwargs=((""label"", ""MyLabel""),))\n        alltxt = text_in_fig()\n        assert ""MyLabel"" in alltxt\n        assert ""testvar"" not in alltxt\n        # change cbar ax\n        fig, (ax, cax) = plt.subplots(1, 2)\n        self.plotmethod(\n            ax=ax, cbar_ax=cax, add_colorbar=True, cbar_kwargs={""label"": ""MyBar""}\n        )\n        assert ax.has_data()\n        assert cax.has_data()\n        alltxt = text_in_fig()\n        assert ""MyBar"" in alltxt\n        assert ""testvar"" not in alltxt\n        # note that there are two ways to achieve this\n        fig, (ax, cax) = plt.subplots(1, 2)\n        self.plotmethod(\n            ax=ax, add_colorbar=True, cbar_kwargs={""label"": ""MyBar"", ""cax"": cax}\n        )\n        assert ax.has_data()\n        assert cax.has_data()\n        alltxt = text_in_fig()\n        assert ""MyBar"" in alltxt\n        assert ""testvar"" not in alltxt\n        # see that no colorbar is respected\n        self.plotmethod(add_colorbar=False)\n        assert ""testvar"" not in text_in_fig()\n        # check that error is raised\n        pytest.raises(\n            ValueError,\n            self.plotmethod,\n            add_colorbar=False,\n            cbar_kwargs={""label"": ""label""},\n        )\n\n    def test_verbose_facetgrid(self):\n        a = easy_array((10, 15, 3))\n        d = DataArray(a, dims=[""y"", ""x"", ""z""])\n        g = xplt.FacetGrid(d, col=""z"")\n        g.map_dataarray(self.plotfunc, ""x"", ""y"")\n        for ax in g.axes.flat:\n            assert ax.has_data()\n\n    def test_2d_function_and_method_signature_same(self):\n        func_sig = inspect.getcallargs(self.plotfunc, self.darray)\n        method_sig = inspect.getcallargs(self.plotmethod)\n        del method_sig[""_PlotMethods_obj""]\n        del func_sig[""darray""]\n        assert func_sig == method_sig\n\n    @pytest.mark.filterwarnings(""ignore:tight_layout cannot"")\n    def test_convenient_facetgrid(self):\n        a = easy_array((10, 15, 4))\n        d = DataArray(a, dims=[""y"", ""x"", ""z""])\n        g = self.plotfunc(d, x=""x"", y=""y"", col=""z"", col_wrap=2)\n\n        assert_array_equal(g.axes.shape, [2, 2])\n        for (y, x), ax in np.ndenumerate(g.axes):\n            assert ax.has_data()\n            if x == 0:\n                assert ""y"" == ax.get_ylabel()\n            else:\n                assert """" == ax.get_ylabel()\n            if y == 1:\n                assert ""x"" == ax.get_xlabel()\n            else:\n                assert """" == ax.get_xlabel()\n\n        # Infering labels\n        g = self.plotfunc(d, col=""z"", col_wrap=2)\n        assert_array_equal(g.axes.shape, [2, 2])\n        for (y, x), ax in np.ndenumerate(g.axes):\n            assert ax.has_data()\n            if x == 0:\n                assert ""y"" == ax.get_ylabel()\n            else:\n                assert """" == ax.get_ylabel()\n            if y == 1:\n                assert ""x"" == ax.get_xlabel()\n            else:\n                assert """" == ax.get_xlabel()\n\n    @pytest.mark.filterwarnings(""ignore:tight_layout cannot"")\n    def test_convenient_facetgrid_4d(self):\n        a = easy_array((10, 15, 2, 3))\n        d = DataArray(a, dims=[""y"", ""x"", ""columns"", ""rows""])\n        g = self.plotfunc(d, x=""x"", y=""y"", col=""columns"", row=""rows"")\n\n        assert_array_equal(g.axes.shape, [3, 2])\n        for ax in g.axes.flat:\n            assert ax.has_data()\n\n    @pytest.mark.filterwarnings(""ignore:This figure includes"")\n    def test_facetgrid_map_only_appends_mappables(self):\n        a = easy_array((10, 15, 2, 3))\n        d = DataArray(a, dims=[""y"", ""x"", ""columns"", ""rows""])\n        g = self.plotfunc(d, x=""x"", y=""y"", col=""columns"", row=""rows"")\n\n        expected = g._mappables\n\n        g.map(lambda: plt.plot(1, 1))\n        actual = g._mappables\n\n        assert expected == actual\n\n    def test_facetgrid_cmap(self):\n        # Regression test for GH592\n        data = np.random.random(size=(20, 25, 12)) + np.linspace(-3, 3, 12)\n        d = DataArray(data, dims=[""x"", ""y"", ""time""])\n        fg = d.plot.pcolormesh(col=""time"")\n        # check that all color limits are the same\n        assert len({m.get_clim() for m in fg._mappables}) == 1\n        # check that all colormaps are the same\n        assert len({m.get_cmap().name for m in fg._mappables}) == 1\n\n    def test_facetgrid_cbar_kwargs(self):\n        a = easy_array((10, 15, 2, 3))\n        d = DataArray(a, dims=[""y"", ""x"", ""columns"", ""rows""])\n        g = self.plotfunc(\n            d,\n            x=""x"",\n            y=""y"",\n            col=""columns"",\n            row=""rows"",\n            cbar_kwargs={""label"": ""test_label""},\n        )\n\n        # catch contour case\n        if hasattr(g, ""cbar""):\n            assert g.cbar._label == ""test_label""\n\n    def test_facetgrid_no_cbar_ax(self):\n        a = easy_array((10, 15, 2, 3))\n        d = DataArray(a, dims=[""y"", ""x"", ""columns"", ""rows""])\n        with pytest.raises(ValueError):\n            self.plotfunc(d, x=""x"", y=""y"", col=""columns"", row=""rows"", cbar_ax=1)\n\n    def test_cmap_and_color_both(self):\n        with pytest.raises(ValueError):\n            self.plotmethod(colors=""k"", cmap=""RdBu"")\n\n    def test_2d_coord_with_interval(self):\n        for dim in self.darray.dims:\n            gp = self.darray.groupby_bins(dim, range(15), restore_coord_dims=True).mean(\n                dim\n            )\n            for kind in [""imshow"", ""pcolormesh"", ""contourf"", ""contour""]:\n                getattr(gp.plot, kind)()\n\n    def test_colormap_error_norm_and_vmin_vmax(self):\n        norm = mpl.colors.LogNorm(0.1, 1e1)\n\n        with pytest.raises(ValueError):\n            self.darray.plot(norm=norm, vmin=2)\n\n        with pytest.raises(ValueError):\n            self.darray.plot(norm=norm, vmax=2)\n\n\n@pytest.mark.slow\nclass TestContourf(Common2dMixin, PlotTestCase):\n\n    plotfunc = staticmethod(xplt.contourf)\n\n    @pytest.mark.slow\n    def test_contourf_called(self):\n        # Having both statements ensures the test works properly\n        assert not self.contourf_called(self.darray.plot.imshow)\n        assert self.contourf_called(self.darray.plot.contourf)\n\n    def test_primitive_artist_returned(self):\n        artist = self.plotmethod()\n        assert isinstance(artist, mpl.contour.QuadContourSet)\n\n    @pytest.mark.slow\n    def test_extend(self):\n        artist = self.plotmethod()\n        assert artist.extend == ""neither""\n\n        self.darray[0, 0] = -100\n        self.darray[-1, -1] = 100\n        artist = self.plotmethod(robust=True)\n        assert artist.extend == ""both""\n\n        self.darray[0, 0] = 0\n        self.darray[-1, -1] = 0\n        artist = self.plotmethod(vmin=-0, vmax=10)\n        assert artist.extend == ""min""\n\n        artist = self.plotmethod(vmin=-10, vmax=0)\n        assert artist.extend == ""max""\n\n    @pytest.mark.slow\n    def test_2d_coord_names(self):\n        self.plotmethod(x=""x2d"", y=""y2d"")\n        # make sure labels came out ok\n        ax = plt.gca()\n        assert ""x2d"" == ax.get_xlabel()\n        assert ""y2d"" == ax.get_ylabel()\n\n    @pytest.mark.slow\n    def test_levels(self):\n        artist = self.plotmethod(levels=[-0.5, -0.4, 0.1])\n        assert artist.extend == ""both""\n\n        artist = self.plotmethod(levels=3)\n        assert artist.extend == ""neither""\n\n\n@pytest.mark.slow\nclass TestContour(Common2dMixin, PlotTestCase):\n\n    plotfunc = staticmethod(xplt.contour)\n\n    # matplotlib cmap.colors gives an rgbA ndarray\n    # when seaborn is used, instead we get an rgb tuple\n    @staticmethod\n    def _color_as_tuple(c):\n        return tuple(c[:3])\n\n    def test_colors(self):\n\n        # with single color, we don\'t want rgb array\n        artist = self.plotmethod(colors=""k"")\n        assert artist.cmap.colors[0] == ""k""\n\n        artist = self.plotmethod(colors=[""k"", ""b""])\n        assert self._color_as_tuple(artist.cmap.colors[1]) == (0.0, 0.0, 1.0)\n\n        artist = self.darray.plot.contour(\n            levels=[-0.5, 0.0, 0.5, 1.0], colors=[""k"", ""r"", ""w"", ""b""]\n        )\n        assert self._color_as_tuple(artist.cmap.colors[1]) == (1.0, 0.0, 0.0)\n        assert self._color_as_tuple(artist.cmap.colors[2]) == (1.0, 1.0, 1.0)\n        # the last color is now under ""over""\n        assert self._color_as_tuple(artist.cmap._rgba_over) == (0.0, 0.0, 1.0)\n\n    def test_colors_np_levels(self):\n\n        # https://github.com/pydata/xarray/issues/3284\n        levels = np.array([-0.5, 0.0, 0.5, 1.0])\n        artist = self.darray.plot.contour(levels=levels, colors=[""k"", ""r"", ""w"", ""b""])\n        assert self._color_as_tuple(artist.cmap.colors[1]) == (1.0, 0.0, 0.0)\n        assert self._color_as_tuple(artist.cmap.colors[2]) == (1.0, 1.0, 1.0)\n        # the last color is now under ""over""\n        assert self._color_as_tuple(artist.cmap._rgba_over) == (0.0, 0.0, 1.0)\n\n    def test_cmap_and_color_both(self):\n        with pytest.raises(ValueError):\n            self.plotmethod(colors=""k"", cmap=""RdBu"")\n\n    def list_of_colors_in_cmap_raises_error(self):\n        with raises_regex(ValueError, ""list of colors""):\n            self.plotmethod(cmap=[""k"", ""b""])\n\n    @pytest.mark.slow\n    def test_2d_coord_names(self):\n        self.plotmethod(x=""x2d"", y=""y2d"")\n        # make sure labels came out ok\n        ax = plt.gca()\n        assert ""x2d"" == ax.get_xlabel()\n        assert ""y2d"" == ax.get_ylabel()\n\n    def test_single_level(self):\n        # this used to raise an error, but not anymore since\n        # add_colorbar defaults to false\n        self.plotmethod(levels=[0.1])\n        self.plotmethod(levels=1)\n\n\nclass TestPcolormesh(Common2dMixin, PlotTestCase):\n\n    plotfunc = staticmethod(xplt.pcolormesh)\n\n    def test_primitive_artist_returned(self):\n        artist = self.plotmethod()\n        assert isinstance(artist, mpl.collections.QuadMesh)\n\n    def test_everything_plotted(self):\n        artist = self.plotmethod()\n        assert artist.get_array().size == self.darray.size\n\n    @pytest.mark.slow\n    def test_2d_coord_names(self):\n        self.plotmethod(x=""x2d"", y=""y2d"")\n        # make sure labels came out ok\n        ax = plt.gca()\n        assert ""x2d"" == ax.get_xlabel()\n        assert ""y2d"" == ax.get_ylabel()\n\n    def test_dont_infer_interval_breaks_for_cartopy(self):\n        # Regression for GH 781\n        ax = plt.gca()\n        # Simulate a Cartopy Axis\n        setattr(ax, ""projection"", True)\n        artist = self.plotmethod(x=""x2d"", y=""y2d"", ax=ax)\n        assert isinstance(artist, mpl.collections.QuadMesh)\n        # Let cartopy handle the axis limits and artist size\n        assert artist.get_array().size <= self.darray.size\n\n\n@pytest.mark.slow\nclass TestImshow(Common2dMixin, PlotTestCase):\n\n    plotfunc = staticmethod(xplt.imshow)\n\n    @pytest.mark.slow\n    def test_imshow_called(self):\n        # Having both statements ensures the test works properly\n        assert not self.imshow_called(self.darray.plot.contourf)\n        assert self.imshow_called(self.darray.plot.imshow)\n\n    def test_xy_pixel_centered(self):\n        self.darray.plot.imshow(yincrease=False)\n        assert np.allclose([-0.5, 14.5], plt.gca().get_xlim())\n        assert np.allclose([9.5, -0.5], plt.gca().get_ylim())\n\n    def test_default_aspect_is_auto(self):\n        self.darray.plot.imshow()\n        assert ""auto"" == plt.gca().get_aspect()\n\n    @pytest.mark.slow\n    def test_cannot_change_mpl_aspect(self):\n\n        with raises_regex(ValueError, ""not available in xarray""):\n            self.darray.plot.imshow(aspect=""equal"")\n\n        # with numbers we fall back to fig control\n        self.darray.plot.imshow(size=5, aspect=2)\n        assert ""auto"" == plt.gca().get_aspect()\n        assert tuple(plt.gcf().get_size_inches()) == (10, 5)\n\n    @pytest.mark.slow\n    def test_primitive_artist_returned(self):\n        artist = self.plotmethod()\n        assert isinstance(artist, mpl.image.AxesImage)\n\n    @pytest.mark.slow\n    @requires_seaborn\n    def test_seaborn_palette_needs_levels(self):\n        with pytest.raises(ValueError):\n            self.plotmethod(cmap=""husl"")\n\n    def test_2d_coord_names(self):\n        with raises_regex(ValueError, ""requires 1D coordinates""):\n            self.plotmethod(x=""x2d"", y=""y2d"")\n\n    def test_plot_rgb_image(self):\n        DataArray(\n            easy_array((10, 15, 3), start=0), dims=[""y"", ""x"", ""band""]\n        ).plot.imshow()\n        assert 0 == len(find_possible_colorbars())\n\n    def test_plot_rgb_image_explicit(self):\n        DataArray(\n            easy_array((10, 15, 3), start=0), dims=[""y"", ""x"", ""band""]\n        ).plot.imshow(y=""y"", x=""x"", rgb=""band"")\n        assert 0 == len(find_possible_colorbars())\n\n    def test_plot_rgb_faceted(self):\n        DataArray(\n            easy_array((2, 2, 10, 15, 3), start=0), dims=[""a"", ""b"", ""y"", ""x"", ""band""]\n        ).plot.imshow(row=""a"", col=""b"")\n        assert 0 == len(find_possible_colorbars())\n\n    def test_plot_rgba_image_transposed(self):\n        # We can handle the color axis being in any position\n        DataArray(\n            easy_array((4, 10, 15), start=0), dims=[""band"", ""y"", ""x""]\n        ).plot.imshow()\n\n    def test_warns_ambigious_dim(self):\n        arr = DataArray(easy_array((3, 3, 3)), dims=[""y"", ""x"", ""band""])\n        with pytest.warns(UserWarning):\n            arr.plot.imshow()\n        # but doesn\'t warn if dimensions specified\n        arr.plot.imshow(rgb=""band"")\n        arr.plot.imshow(x=""x"", y=""y"")\n\n    def test_rgb_errors_too_many_dims(self):\n        arr = DataArray(easy_array((3, 3, 3, 3)), dims=[""y"", ""x"", ""z"", ""band""])\n        with pytest.raises(ValueError):\n            arr.plot.imshow(rgb=""band"")\n\n    def test_rgb_errors_bad_dim_sizes(self):\n        arr = DataArray(easy_array((5, 5, 5)), dims=[""y"", ""x"", ""band""])\n        with pytest.raises(ValueError):\n            arr.plot.imshow(rgb=""band"")\n\n    def test_normalize_rgb_imshow(self):\n        for kwargs in (\n            dict(vmin=-1),\n            dict(vmax=2),\n            dict(vmin=-1, vmax=1),\n            dict(vmin=0, vmax=0),\n            dict(vmin=0, robust=True),\n            dict(vmax=-1, robust=True),\n        ):\n            da = DataArray(easy_array((5, 5, 3), start=-0.6, stop=1.4))\n            arr = da.plot.imshow(**kwargs).get_array()\n            assert 0 <= arr.min() <= arr.max() <= 1, kwargs\n\n    def test_normalize_rgb_one_arg_error(self):\n        da = DataArray(easy_array((5, 5, 3), start=-0.6, stop=1.4))\n        # If passed one bound that implies all out of range, error:\n        for kwargs in [dict(vmax=-1), dict(vmin=2)]:\n            with pytest.raises(ValueError):\n                da.plot.imshow(**kwargs)\n        # If passed two that\'s just moving the range, *not* an error:\n        for kwargs in [dict(vmax=-1, vmin=-1.2), dict(vmin=2, vmax=2.1)]:\n            da.plot.imshow(**kwargs)\n\n    def test_imshow_rgb_values_in_valid_range(self):\n        da = DataArray(np.arange(75, dtype=""uint8"").reshape((5, 5, 3)))\n        _, ax = plt.subplots()\n        out = da.plot.imshow(ax=ax).get_array()\n        assert out.dtype == np.uint8\n        assert (out[..., :3] == da.values).all()  # Compare without added alpha\n\n    @pytest.mark.filterwarnings(""ignore:Several dimensions of this array"")\n    def test_regression_rgb_imshow_dim_size_one(self):\n        # Regression: https://github.com/pydata/xarray/issues/1966\n        da = DataArray(easy_array((1, 3, 3), start=0.0, stop=1.0))\n        da.plot.imshow()\n\n    def test_origin_overrides_xyincrease(self):\n        da = DataArray(easy_array((3, 2)), coords=[[-2, 0, 2], [-1, 1]])\n        da.plot.imshow(origin=""upper"")\n        assert plt.xlim()[0] < 0\n        assert plt.ylim()[1] < 0\n\n        plt.clf()\n        da.plot.imshow(origin=""lower"")\n        assert plt.xlim()[0] < 0\n        assert plt.ylim()[0] < 0\n\n\nclass TestFacetGrid(PlotTestCase):\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        d = easy_array((10, 15, 3))\n        self.darray = DataArray(d, dims=[""y"", ""x"", ""z""], coords={""z"": [""a"", ""b"", ""c""]})\n        self.g = xplt.FacetGrid(self.darray, col=""z"")\n\n    @pytest.mark.slow\n    def test_no_args(self):\n        self.g.map_dataarray(xplt.contourf, ""x"", ""y"")\n\n        # Don\'t want colorbar labeled with \'None\'\n        alltxt = text_in_fig()\n        assert ""None"" not in alltxt\n\n        for ax in self.g.axes.flat:\n            assert ax.has_data()\n\n    @pytest.mark.slow\n    def test_names_appear_somewhere(self):\n        self.darray.name = ""testvar""\n        self.g.map_dataarray(xplt.contourf, ""x"", ""y"")\n        for k, ax in zip(""abc"", self.g.axes.flat):\n            assert f""z = {k}"" == ax.get_title()\n\n        alltxt = text_in_fig()\n        assert self.darray.name in alltxt\n        for label in [""x"", ""y""]:\n            assert label in alltxt\n\n    @pytest.mark.slow\n    def test_text_not_super_long(self):\n        self.darray.coords[""z""] = [100 * letter for letter in ""abc""]\n        g = xplt.FacetGrid(self.darray, col=""z"")\n        g.map_dataarray(xplt.contour, ""x"", ""y"")\n        alltxt = text_in_fig()\n        maxlen = max(len(txt) for txt in alltxt)\n        assert maxlen < 50\n\n        t0 = g.axes[0, 0].get_title()\n        assert t0.endswith(""..."")\n\n    @pytest.mark.slow\n    def test_colorbar(self):\n        vmin = self.darray.values.min()\n        vmax = self.darray.values.max()\n        expected = np.array((vmin, vmax))\n\n        self.g.map_dataarray(xplt.imshow, ""x"", ""y"")\n\n        for image in plt.gcf().findobj(mpl.image.AxesImage):\n            clim = np.array(image.get_clim())\n            assert np.allclose(expected, clim)\n\n        assert 1 == len(find_possible_colorbars())\n\n    @pytest.mark.slow\n    def test_empty_cell(self):\n        g = xplt.FacetGrid(self.darray, col=""z"", col_wrap=2)\n        g.map_dataarray(xplt.imshow, ""x"", ""y"")\n\n        bottomright = g.axes[-1, -1]\n        assert not bottomright.has_data()\n        assert not bottomright.get_visible()\n\n    @pytest.mark.slow\n    def test_norow_nocol_error(self):\n        with raises_regex(ValueError, r""[Rr]ow""):\n            xplt.FacetGrid(self.darray)\n\n    @pytest.mark.slow\n    def test_groups(self):\n        self.g.map_dataarray(xplt.imshow, ""x"", ""y"")\n        upperleft_dict = self.g.name_dicts[0, 0]\n        upperleft_array = self.darray.loc[upperleft_dict]\n        z0 = self.darray.isel(z=0)\n\n        assert_equal(upperleft_array, z0)\n\n    @pytest.mark.slow\n    def test_float_index(self):\n        self.darray.coords[""z""] = [0.1, 0.2, 0.4]\n        g = xplt.FacetGrid(self.darray, col=""z"")\n        g.map_dataarray(xplt.imshow, ""x"", ""y"")\n\n    @pytest.mark.slow\n    def test_nonunique_index_error(self):\n        self.darray.coords[""z""] = [0.1, 0.2, 0.2]\n        with raises_regex(ValueError, r""[Uu]nique""):\n            xplt.FacetGrid(self.darray, col=""z"")\n\n    @pytest.mark.slow\n    def test_robust(self):\n        z = np.zeros((20, 20, 2))\n        darray = DataArray(z, dims=[""y"", ""x"", ""z""])\n        darray[:, :, 1] = 1\n        darray[2, 0, 0] = -1000\n        darray[3, 0, 0] = 1000\n        g = xplt.FacetGrid(darray, col=""z"")\n        g.map_dataarray(xplt.imshow, ""x"", ""y"", robust=True)\n\n        # Color limits should be 0, 1\n        # The largest number displayed in the figure should be less than 21\n        numbers = set()\n        alltxt = text_in_fig()\n        for txt in alltxt:\n            try:\n                numbers.add(float(txt))\n            except ValueError:\n                pass\n        largest = max(abs(x) for x in numbers)\n        assert largest < 21\n\n    @pytest.mark.slow\n    def test_can_set_vmin_vmax(self):\n        vmin, vmax = 50.0, 1000.0\n        expected = np.array((vmin, vmax))\n        self.g.map_dataarray(xplt.imshow, ""x"", ""y"", vmin=vmin, vmax=vmax)\n\n        for image in plt.gcf().findobj(mpl.image.AxesImage):\n            clim = np.array(image.get_clim())\n            assert np.allclose(expected, clim)\n\n    @pytest.mark.slow\n    def test_vmin_vmax_equal(self):\n        # regression test for GH3734\n        fg = self.g.map_dataarray(xplt.imshow, ""x"", ""y"", vmin=50, vmax=50)\n        for mappable in fg._mappables:\n            assert mappable.norm.vmin != mappable.norm.vmax\n\n    @pytest.mark.slow\n    @pytest.mark.filterwarnings(""ignore"")\n    def test_can_set_norm(self):\n        norm = mpl.colors.SymLogNorm(0.1)\n        self.g.map_dataarray(xplt.imshow, ""x"", ""y"", norm=norm)\n        for image in plt.gcf().findobj(mpl.image.AxesImage):\n            assert image.norm is norm\n\n    @pytest.mark.slow\n    def test_figure_size(self):\n\n        assert_array_equal(self.g.fig.get_size_inches(), (10, 3))\n\n        g = xplt.FacetGrid(self.darray, col=""z"", size=6)\n        assert_array_equal(g.fig.get_size_inches(), (19, 6))\n\n        g = self.darray.plot.imshow(col=""z"", size=6)\n        assert_array_equal(g.fig.get_size_inches(), (19, 6))\n\n        g = xplt.FacetGrid(self.darray, col=""z"", size=4, aspect=0.5)\n        assert_array_equal(g.fig.get_size_inches(), (7, 4))\n\n        g = xplt.FacetGrid(self.darray, col=""z"", figsize=(9, 4))\n        assert_array_equal(g.fig.get_size_inches(), (9, 4))\n\n        with raises_regex(ValueError, ""cannot provide both""):\n            g = xplt.plot(self.darray, row=2, col=""z"", figsize=(6, 4), size=6)\n\n        with raises_regex(ValueError, ""Can\'t use""):\n            g = xplt.plot(self.darray, row=2, col=""z"", ax=plt.gca(), size=6)\n\n    @pytest.mark.slow\n    def test_num_ticks(self):\n        nticks = 99\n        maxticks = nticks + 1\n        self.g.map_dataarray(xplt.imshow, ""x"", ""y"")\n        self.g.set_ticks(max_xticks=nticks, max_yticks=nticks)\n\n        for ax in self.g.axes.flat:\n            xticks = len(ax.get_xticks())\n            yticks = len(ax.get_yticks())\n            assert xticks <= maxticks\n            assert yticks <= maxticks\n            assert xticks >= nticks / 2.0\n            assert yticks >= nticks / 2.0\n\n    @pytest.mark.slow\n    def test_map(self):\n        assert self.g._finalized is False\n        self.g.map(plt.contourf, ""x"", ""y"", Ellipsis)\n        assert self.g._finalized is True\n        self.g.map(lambda: None)\n\n    @pytest.mark.slow\n    def test_map_dataset(self):\n        g = xplt.FacetGrid(self.darray.to_dataset(name=""foo""), col=""z"")\n        g.map(plt.contourf, ""x"", ""y"", ""foo"")\n\n        alltxt = text_in_fig()\n        for label in [""x"", ""y""]:\n            assert label in alltxt\n        # everything has a label\n        assert ""None"" not in alltxt\n\n        # colorbar can\'t be inferred automatically\n        assert ""foo"" not in alltxt\n        assert 0 == len(find_possible_colorbars())\n\n        g.add_colorbar(label=""colors!"")\n        assert ""colors!"" in text_in_fig()\n        assert 1 == len(find_possible_colorbars())\n\n    @pytest.mark.slow\n    def test_set_axis_labels(self):\n        g = self.g.map_dataarray(xplt.contourf, ""x"", ""y"")\n        g.set_axis_labels(""longitude"", ""latitude"")\n        alltxt = text_in_fig()\n        for label in [""longitude"", ""latitude""]:\n            assert label in alltxt\n\n    @pytest.mark.slow\n    def test_facetgrid_colorbar(self):\n        a = easy_array((10, 15, 4))\n        d = DataArray(a, dims=[""y"", ""x"", ""z""], name=""foo"")\n\n        d.plot.imshow(x=""x"", y=""y"", col=""z"")\n        assert 1 == len(find_possible_colorbars())\n\n        d.plot.imshow(x=""x"", y=""y"", col=""z"", add_colorbar=True)\n        assert 1 == len(find_possible_colorbars())\n\n        d.plot.imshow(x=""x"", y=""y"", col=""z"", add_colorbar=False)\n        assert 0 == len(find_possible_colorbars())\n\n    @pytest.mark.slow\n    def test_facetgrid_polar(self):\n        # test if polar projection in FacetGrid does not raise an exception\n        self.darray.plot.pcolormesh(\n            col=""z"", subplot_kws=dict(projection=""polar""), sharex=False, sharey=False\n        )\n\n\n@pytest.mark.filterwarnings(""ignore:tight_layout cannot"")\nclass TestFacetGrid4d(PlotTestCase):\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        a = easy_array((10, 15, 3, 2))\n        darray = DataArray(a, dims=[""y"", ""x"", ""col"", ""row""])\n        darray.coords[""col""] = np.array(\n            [""col"" + str(x) for x in darray.coords[""col""].values]\n        )\n        darray.coords[""row""] = np.array(\n            [""row"" + str(x) for x in darray.coords[""row""].values]\n        )\n\n        self.darray = darray\n\n    @pytest.mark.slow\n    def test_default_labels(self):\n        g = xplt.FacetGrid(self.darray, col=""col"", row=""row"")\n        assert (2, 3) == g.axes.shape\n\n        g.map_dataarray(xplt.imshow, ""x"", ""y"")\n\n        # Rightmost column should be labeled\n        for label, ax in zip(self.darray.coords[""row""].values, g.axes[:, -1]):\n            assert substring_in_axes(label, ax)\n\n        # Top row should be labeled\n        for label, ax in zip(self.darray.coords[""col""].values, g.axes[0, :]):\n            assert substring_in_axes(label, ax)\n\n        # ensure that row & col labels can be changed\n        g.set_titles(""abc={value}"")\n        for label, ax in zip(self.darray.coords[""row""].values, g.axes[:, -1]):\n            assert substring_in_axes(f""abc={label}"", ax)\n            # previous labels were ""row=row0"" etc.\n            assert substring_not_in_axes(""row="", ax)\n\n        for label, ax in zip(self.darray.coords[""col""].values, g.axes[0, :]):\n            assert substring_in_axes(f""abc={label}"", ax)\n            # previous labels were ""col=row0"" etc.\n            assert substring_not_in_axes(""col="", ax)\n\n\n@pytest.mark.filterwarnings(""ignore:tight_layout cannot"")\nclass TestFacetedLinePlotsLegend(PlotTestCase):\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        self.darray = xr.tutorial.scatter_example_dataset()\n\n    def test_legend_labels(self):\n        fg = self.darray.A.plot.line(col=""x"", row=""w"", hue=""z"")\n        all_legend_labels = [t.get_text() for t in fg.figlegend.texts]\n        # labels in legend should be [\'0\', \'1\', \'2\', \'3\']\n        assert sorted(all_legend_labels) == [""0"", ""1"", ""2"", ""3""]\n\n\n@pytest.mark.filterwarnings(""ignore:tight_layout cannot"")\nclass TestFacetedLinePlots(PlotTestCase):\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        self.darray = DataArray(\n            np.random.randn(10, 6, 3, 4),\n            dims=[""hue"", ""x"", ""col"", ""row""],\n            coords=[range(10), range(6), range(3), [""A"", ""B"", ""C"", ""C++""]],\n            name=""Cornelius Ortega the 1st"",\n        )\n\n        self.darray.hue.name = ""huename""\n        self.darray.hue.attrs[""units""] = ""hunits""\n        self.darray.x.attrs[""units""] = ""xunits""\n        self.darray.col.attrs[""units""] = ""colunits""\n        self.darray.row.attrs[""units""] = ""rowunits""\n\n    def test_facetgrid_shape(self):\n        g = self.darray.plot(row=""row"", col=""col"", hue=""hue"")\n        assert g.axes.shape == (len(self.darray.row), len(self.darray.col))\n\n        g = self.darray.plot(row=""col"", col=""row"", hue=""hue"")\n        assert g.axes.shape == (len(self.darray.col), len(self.darray.row))\n\n    def test_unnamed_args(self):\n        g = self.darray.plot.line(""o--"", row=""row"", col=""col"", hue=""hue"")\n        lines = [\n            q for q in g.axes.flat[0].get_children() if isinstance(q, mpl.lines.Line2D)\n        ]\n        # passing \'o--\' as argument should set marker and linestyle\n        assert lines[0].get_marker() == ""o""\n        assert lines[0].get_linestyle() == ""--""\n\n    def test_default_labels(self):\n        g = self.darray.plot(row=""row"", col=""col"", hue=""hue"")\n        # Rightmost column should be labeled\n        for label, ax in zip(self.darray.coords[""row""].values, g.axes[:, -1]):\n            assert substring_in_axes(label, ax)\n\n        # Top row should be labeled\n        for label, ax in zip(self.darray.coords[""col""].values, g.axes[0, :]):\n            assert substring_in_axes(str(label), ax)\n\n        # Leftmost column should have array name\n        for ax in g.axes[:, 0]:\n            assert substring_in_axes(self.darray.name, ax)\n\n    def test_test_empty_cell(self):\n        g = (\n            self.darray.isel(row=1)\n            .drop_vars(""row"")\n            .plot(col=""col"", hue=""hue"", col_wrap=2)\n        )\n        bottomright = g.axes[-1, -1]\n        assert not bottomright.has_data()\n        assert not bottomright.get_visible()\n\n    def test_set_axis_labels(self):\n        g = self.darray.plot(row=""row"", col=""col"", hue=""hue"")\n        g.set_axis_labels(""longitude"", ""latitude"")\n        alltxt = text_in_fig()\n\n        assert ""longitude"" in alltxt\n        assert ""latitude"" in alltxt\n\n    def test_axes_in_faceted_plot(self):\n        with pytest.raises(ValueError):\n            self.darray.plot.line(row=""row"", col=""col"", x=""x"", ax=plt.axes())\n\n    def test_figsize_and_size(self):\n        with pytest.raises(ValueError):\n            self.darray.plot.line(row=""row"", col=""col"", x=""x"", size=3, figsize=4)\n\n    def test_wrong_num_of_dimensions(self):\n        with pytest.raises(ValueError):\n            self.darray.plot(row=""row"", hue=""hue"")\n            self.darray.plot.line(row=""row"", hue=""hue"")\n\n\n@requires_matplotlib\nclass TestDatasetScatterPlots(PlotTestCase):\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        das = [\n            DataArray(\n                np.random.randn(3, 3, 4, 4),\n                dims=[""x"", ""row"", ""col"", ""hue""],\n                coords=[range(k) for k in [3, 3, 4, 4]],\n            )\n            for _ in [1, 2]\n        ]\n        ds = Dataset({""A"": das[0], ""B"": das[1]})\n        ds.hue.name = ""huename""\n        ds.hue.attrs[""units""] = ""hunits""\n        ds.x.attrs[""units""] = ""xunits""\n        ds.col.attrs[""units""] = ""colunits""\n        ds.row.attrs[""units""] = ""rowunits""\n        ds.A.attrs[""units""] = ""Aunits""\n        ds.B.attrs[""units""] = ""Bunits""\n        self.ds = ds\n\n    @pytest.mark.parametrize(\n        ""add_guide, hue_style, legend, colorbar"",\n        [\n            (None, None, False, True),\n            (False, None, False, False),\n            (True, None, False, True),\n            (True, ""continuous"", False, True),\n            (False, ""discrete"", False, False),\n            (True, ""discrete"", True, False),\n        ],\n    )\n    def test_add_guide(self, add_guide, hue_style, legend, colorbar):\n\n        meta_data = _infer_meta_data(\n            self.ds, x=""A"", y=""B"", hue=""hue"", hue_style=hue_style, add_guide=add_guide\n        )\n        assert meta_data[""add_legend""] is legend\n        assert meta_data[""add_colorbar""] is colorbar\n\n    def test_facetgrid_shape(self):\n        g = self.ds.plot.scatter(x=""A"", y=""B"", row=""row"", col=""col"")\n        assert g.axes.shape == (len(self.ds.row), len(self.ds.col))\n\n        g = self.ds.plot.scatter(x=""A"", y=""B"", row=""col"", col=""row"")\n        assert g.axes.shape == (len(self.ds.col), len(self.ds.row))\n\n    def test_default_labels(self):\n        g = self.ds.plot.scatter(""A"", ""B"", row=""row"", col=""col"", hue=""hue"")\n\n        # Top row should be labeled\n        for label, ax in zip(self.ds.coords[""col""].values, g.axes[0, :]):\n            assert substring_in_axes(str(label), ax)\n\n        # Bottom row should have name of x array name and units\n        for ax in g.axes[-1, :]:\n            assert ax.get_xlabel() == ""A [Aunits]""\n\n        # Leftmost column should have name of y array name and units\n        for ax in g.axes[:, 0]:\n            assert ax.get_ylabel() == ""B [Bunits]""\n\n    def test_axes_in_faceted_plot(self):\n        with pytest.raises(ValueError):\n            self.ds.plot.scatter(x=""A"", y=""B"", row=""row"", ax=plt.axes())\n\n    def test_figsize_and_size(self):\n        with pytest.raises(ValueError):\n            self.ds.plot.scatter(x=""A"", y=""B"", row=""row"", size=3, figsize=4)\n\n    @pytest.mark.parametrize(\n        ""x, y, hue_style, add_guide"",\n        [\n            (""A"", ""B"", ""something"", True),\n            (""A"", ""B"", ""discrete"", True),\n            (""A"", ""B"", None, True),\n            (""A"", ""The Spanish Inquisition"", None, None),\n            (""The Spanish Inquisition"", ""B"", None, True),\n        ],\n    )\n    def test_bad_args(self, x, y, hue_style, add_guide):\n        with pytest.raises(ValueError):\n            self.ds.plot.scatter(x, y, hue_style=hue_style, add_guide=add_guide)\n\n    @pytest.mark.xfail(reason=""datetime,timedelta hue variable not supported."")\n    @pytest.mark.parametrize(""hue_style"", [""discrete"", ""continuous""])\n    def test_datetime_hue(self, hue_style):\n        ds2 = self.ds.copy()\n        ds2[""hue""] = pd.date_range(""2000-1-1"", periods=4)\n        ds2.plot.scatter(x=""A"", y=""B"", hue=""hue"", hue_style=hue_style)\n\n        ds2[""hue""] = pd.timedelta_range(""-1D"", periods=4, freq=""D"")\n        ds2.plot.scatter(x=""A"", y=""B"", hue=""hue"", hue_style=hue_style)\n\n    def test_facetgrid_hue_style(self):\n        # Can\'t move this to pytest.mark.parametrize because py36-bare-minimum\n        # doesn\'t have matplotlib.\n        for hue_style, map_type in (\n            (""discrete"", list),\n            (""continuous"", mpl.collections.PathCollection),\n        ):\n            g = self.ds.plot.scatter(\n                x=""A"", y=""B"", row=""row"", col=""col"", hue=""hue"", hue_style=hue_style\n            )\n            # for \'discrete\' a list is appended to _mappables\n            # for \'continuous\', should be single PathCollection\n            assert isinstance(g._mappables[-1], map_type)\n\n    @pytest.mark.parametrize(\n        ""x, y, hue, markersize"", [(""A"", ""B"", ""x"", ""col""), (""x"", ""row"", ""A"", ""B"")]\n    )\n    def test_scatter(self, x, y, hue, markersize):\n        self.ds.plot.scatter(x, y, hue=hue, markersize=markersize)\n\n    def test_non_numeric_legend(self):\n        ds2 = self.ds.copy()\n        ds2[""hue""] = [""a"", ""b"", ""c"", ""d""]\n        lines = ds2.plot.scatter(x=""A"", y=""B"", hue=""hue"")\n        # should make a discrete legend\n        assert lines[0].axes.legend_ is not None\n        # and raise an error if explicitly not allowed to do so\n        with pytest.raises(ValueError):\n            ds2.plot.scatter(x=""A"", y=""B"", hue=""hue"", hue_style=""continuous"")\n\n    def test_add_legend_by_default(self):\n        sc = self.ds.plot.scatter(x=""A"", y=""B"", hue=""hue"")\n        assert len(sc.figure.axes) == 2\n\n\nclass TestDatetimePlot(PlotTestCase):\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        """"""\n        Create a DataArray with a time-axis that contains datetime objects.\n        """"""\n        month = np.arange(1, 13, 1)\n        data = np.sin(2 * np.pi * month / 12.0)\n\n        darray = DataArray(data, dims=[""time""])\n        darray.coords[""time""] = np.array([datetime(2017, m, 1) for m in month])\n\n        self.darray = darray\n\n    def test_datetime_line_plot(self):\n        # test if line plot raises no Exception\n        self.darray.plot.line()\n\n\n@requires_nc_time_axis\n@requires_cftime\nclass TestCFDatetimePlot(PlotTestCase):\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        """"""\n        Create a DataArray with a time-axis that contains cftime.datetime\n        objects.\n        """"""\n        # case for 1d array\n        data = np.random.rand(4, 12)\n        time = xr.cftime_range(start=""2017"", periods=12, freq=""1M"", calendar=""noleap"")\n        darray = DataArray(data, dims=[""x"", ""time""])\n        darray.coords[""time""] = time\n\n        self.darray = darray\n\n    def test_cfdatetime_line_plot(self):\n        self.darray.isel(x=0).plot.line()\n\n    def test_cfdatetime_pcolormesh_plot(self):\n        self.darray.plot.pcolormesh()\n\n    def test_cfdatetime_contour_plot(self):\n        self.darray.plot.contour()\n\n\n@requires_cftime\n@pytest.mark.skipif(has_nc_time_axis, reason=""nc_time_axis is installed"")\nclass TestNcAxisNotInstalled(PlotTestCase):\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        """"""\n        Create a DataArray with a time-axis that contains cftime.datetime\n        objects.\n        """"""\n        month = np.arange(1, 13, 1)\n        data = np.sin(2 * np.pi * month / 12.0)\n        darray = DataArray(data, dims=[""time""])\n        darray.coords[""time""] = xr.cftime_range(\n            start=""2017"", periods=12, freq=""1M"", calendar=""noleap""\n        )\n\n        self.darray = darray\n\n    def test_ncaxis_notinstalled_line_plot(self):\n        with raises_regex(ImportError, ""optional `nc-time-axis`""):\n            self.darray.plot.line()\n\n\ntest_da_list = [\n    DataArray(easy_array((10,))),\n    DataArray(easy_array((10, 3))),\n    DataArray(easy_array((10, 3, 2))),\n]\n\n\n@requires_matplotlib\nclass TestAxesKwargs:\n    @pytest.mark.parametrize(""da"", test_da_list)\n    @pytest.mark.parametrize(""xincrease"", [True, False])\n    def test_xincrease_kwarg(self, da, xincrease):\n        plt.clf()\n        da.plot(xincrease=xincrease)\n        assert plt.gca().xaxis_inverted() == (not xincrease)\n\n    @pytest.mark.parametrize(""da"", test_da_list)\n    @pytest.mark.parametrize(""yincrease"", [True, False])\n    def test_yincrease_kwarg(self, da, yincrease):\n        plt.clf()\n        da.plot(yincrease=yincrease)\n        assert plt.gca().yaxis_inverted() == (not yincrease)\n\n    @pytest.mark.parametrize(""da"", test_da_list)\n    @pytest.mark.parametrize(""xscale"", [""linear"", ""log"", ""logit"", ""symlog""])\n    def test_xscale_kwarg(self, da, xscale):\n        plt.clf()\n        da.plot(xscale=xscale)\n        assert plt.gca().get_xscale() == xscale\n\n    @pytest.mark.parametrize(\n        ""da"", [DataArray(easy_array((10,))), DataArray(easy_array((10, 3)))]\n    )\n    @pytest.mark.parametrize(""yscale"", [""linear"", ""log"", ""logit"", ""symlog""])\n    def test_yscale_kwarg(self, da, yscale):\n        plt.clf()\n        da.plot(yscale=yscale)\n        assert plt.gca().get_yscale() == yscale\n\n    @pytest.mark.parametrize(""da"", test_da_list)\n    def test_xlim_kwarg(self, da):\n        plt.clf()\n        expected = (0.0, 1000.0)\n        da.plot(xlim=[0, 1000])\n        assert plt.gca().get_xlim() == expected\n\n    @pytest.mark.parametrize(""da"", test_da_list)\n    def test_ylim_kwarg(self, da):\n        plt.clf()\n        da.plot(ylim=[0, 1000])\n        expected = (0.0, 1000.0)\n        assert plt.gca().get_ylim() == expected\n\n    @pytest.mark.parametrize(""da"", test_da_list)\n    def test_xticks_kwarg(self, da):\n        plt.clf()\n        da.plot(xticks=np.arange(5))\n        expected = np.arange(5).tolist()\n        assert np.all(plt.gca().get_xticks() == expected)\n\n    @pytest.mark.parametrize(""da"", test_da_list)\n    def test_yticks_kwarg(self, da):\n        plt.clf()\n        da.plot(yticks=np.arange(5))\n        expected = np.arange(5)\n        assert np.all(plt.gca().get_yticks() == expected)\n\n\n@requires_matplotlib\n@pytest.mark.parametrize(""plotfunc"", [""pcolormesh"", ""contourf"", ""contour""])\ndef test_plot_transposed_nondim_coord(plotfunc):\n    x = np.linspace(0, 10, 101)\n    h = np.linspace(3, 7, 101)\n    s = np.linspace(0, 1, 51)\n    z = s[:, np.newaxis] * h[np.newaxis, :]\n    da = xr.DataArray(\n        np.sin(x) * np.cos(z),\n        dims=[""s"", ""x""],\n        coords={""x"": x, ""s"": s, ""z"": ((""s"", ""x""), z), ""zt"": ((""x"", ""s""), z.T)},\n    )\n    getattr(da.plot, plotfunc)(x=""x"", y=""zt"")\n    getattr(da.plot, plotfunc)(x=""zt"", y=""x"")\n\n\n@requires_matplotlib\n@pytest.mark.parametrize(""plotfunc"", [""pcolormesh"", ""imshow""])\ndef test_plot_transposes_properly(plotfunc):\n    # test that we aren\'t mistakenly transposing when the 2 dimensions have equal sizes.\n    da = xr.DataArray([np.sin(2 * np.pi / 10 * np.arange(10))] * 10, dims=(""y"", ""x""))\n    hdl = getattr(da.plot, plotfunc)(x=""x"", y=""y"")\n    # get_array doesn\'t work for contour, contourf. It returns the colormap intervals.\n    # pcolormesh returns 1D array but imshow returns a 2D array so it is necessary\n    # to ravel() on the LHS\n    assert np.all(hdl.get_array().ravel() == da.to_masked_array().ravel())\n\n\n@requires_matplotlib\ndef test_facetgrid_single_contour():\n    # regression test for GH3569\n    x, y = np.meshgrid(np.arange(12), np.arange(12))\n    z = xr.DataArray(np.sqrt(x ** 2 + y ** 2))\n    z2 = xr.DataArray(np.sqrt(x ** 2 + y ** 2) + 1)\n    ds = xr.concat([z, z2], dim=""time"")\n    ds[""time""] = [0, 1]\n\n    ds.plot.contour(col=""time"", levels=[4], colors=[""k""])\n'"
xarray/tests/test_print_versions.py,0,"b'import io\n\nimport xarray\n\n\ndef test_show_versions():\n    f = io.StringIO()\n    xarray.show_versions(file=f)\n    assert ""INSTALLED VERSIONS"" in f.getvalue()\n'"
xarray/tests/test_sparse.py,92,"b'import pickle\nfrom textwrap import dedent\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nimport xarray as xr\nimport xarray.ufuncs as xu\nfrom xarray import DataArray, Variable\nfrom xarray.core.npcompat import IS_NEP18_ACTIVE\nfrom xarray.core.pycompat import sparse_array_type\n\nfrom . import assert_equal, assert_identical, requires_dask\n\nparam = pytest.param\nxfail = pytest.mark.xfail\n\nif not IS_NEP18_ACTIVE:\n    pytest.skip(\n        ""NUMPY_EXPERIMENTAL_ARRAY_FUNCTION is not enabled"", allow_module_level=True\n    )\n\nsparse = pytest.importorskip(""sparse"")\n\n\ndef assert_sparse_equal(a, b):\n    assert isinstance(a, sparse_array_type)\n    assert isinstance(b, sparse_array_type)\n    np.testing.assert_equal(a.todense(), b.todense())\n\n\ndef make_ndarray(shape):\n    return np.arange(np.prod(shape)).reshape(shape)\n\n\ndef make_sparray(shape):\n    return sparse.random(shape, density=0.1, random_state=0)\n\n\ndef make_xrvar(dim_lengths):\n    return xr.Variable(\n        tuple(dim_lengths.keys()), make_sparray(shape=tuple(dim_lengths.values()))\n    )\n\n\ndef make_xrarray(dim_lengths, coords=None, name=""test""):\n    if coords is None:\n        coords = {d: np.arange(n) for d, n in dim_lengths.items()}\n    return xr.DataArray(\n        make_sparray(shape=tuple(dim_lengths.values())),\n        dims=tuple(coords.keys()),\n        coords=coords,\n        name=name,\n    )\n\n\nclass do:\n    def __init__(self, meth, *args, **kwargs):\n        self.meth = meth\n        self.args = args\n        self.kwargs = kwargs\n\n    def __call__(self, obj):\n        return getattr(obj, self.meth)(*self.args, **self.kwargs)\n\n    def __repr__(self):\n        return f""obj.{self.meth}(*{self.args}, **{self.kwargs})""\n\n\n@pytest.mark.parametrize(\n    ""prop"",\n    [\n        ""chunks"",\n        ""data"",\n        ""dims"",\n        ""dtype"",\n        ""encoding"",\n        ""imag"",\n        ""nbytes"",\n        ""ndim"",\n        param(""values"", marks=xfail(reason=""Coercion to dense"")),\n    ],\n)\ndef test_variable_property(prop):\n    var = make_xrvar({""x"": 10, ""y"": 5})\n    getattr(var, prop)\n\n\n@pytest.mark.parametrize(\n    ""func,sparse_output"",\n    [\n        (do(""all""), False),\n        (do(""any""), False),\n        (do(""astype"", dtype=int), True),\n        (do(""clip"", min=0, max=1), True),\n        (do(""coarsen"", windows={""x"": 2}, func=np.sum), True),\n        (do(""compute""), True),\n        (do(""conj""), True),\n        (do(""copy""), True),\n        (do(""count""), False),\n        (do(""get_axis_num"", dim=""x""), False),\n        (do(""isel"", x=slice(2, 4)), True),\n        (do(""isnull""), True),\n        (do(""load""), True),\n        (do(""mean""), False),\n        (do(""notnull""), True),\n        (do(""roll""), True),\n        (do(""round""), True),\n        (do(""set_dims"", dims=(""x"", ""y"", ""z"")), True),\n        (do(""stack"", dimensions={""flat"": (""x"", ""y"")}), True),\n        (do(""to_base_variable""), True),\n        (do(""transpose""), True),\n        (do(""unstack"", dimensions={""x"": {""x1"": 5, ""x2"": 2}}), True),\n        (do(""broadcast_equals"", make_xrvar({""x"": 10, ""y"": 5})), False),\n        (do(""equals"", make_xrvar({""x"": 10, ""y"": 5})), False),\n        (do(""identical"", make_xrvar({""x"": 10, ""y"": 5})), False),\n        param(\n            do(""argmax""),\n            True,\n            marks=xfail(reason=""Missing implementation for np.argmin""),\n        ),\n        param(\n            do(""argmin""),\n            True,\n            marks=xfail(reason=""Missing implementation for np.argmax""),\n        ),\n        param(\n            do(""argsort""),\n            True,\n            marks=xfail(reason=""\'COO\' object has no attribute \'argsort\'""),\n        ),\n        param(\n            do(\n                ""concat"",\n                variables=[\n                    make_xrvar({""x"": 10, ""y"": 5}),\n                    make_xrvar({""x"": 10, ""y"": 5}),\n                ],\n            ),\n            True,\n            marks=xfail(reason=""Coercion to dense""),\n        ),\n        param(\n            do(""conjugate""),\n            True,\n            marks=xfail(reason=""\'COO\' object has no attribute \'conjugate\'""),\n        ),\n        param(\n            do(""cumprod""),\n            True,\n            marks=xfail(reason=""Missing implementation for np.nancumprod""),\n        ),\n        param(\n            do(""cumsum""),\n            True,\n            marks=xfail(reason=""Missing implementation for np.nancumsum""),\n        ),\n        (do(""fillna"", 0), True),\n        param(\n            do(""item"", (1, 1)),\n            False,\n            marks=xfail(reason=""\'COO\' object has no attribute \'item\'""),\n        ),\n        param(\n            do(""median""),\n            False,\n            marks=xfail(reason=""Missing implementation for np.nanmedian""),\n        ),\n        param(do(""max""), False),\n        param(do(""min""), False),\n        param(\n            do(""no_conflicts"", other=make_xrvar({""x"": 10, ""y"": 5})),\n            True,\n            marks=xfail(reason=""mixed sparse-dense operation""),\n        ),\n        param(\n            do(""pad"", mode=""constant"", pad_widths={""x"": (1, 1)}, fill_value=5),\n            True,\n            marks=xfail(reason=""Missing implementation for np.pad""),\n        ),\n        (do(""prod""), False),\n        param(\n            do(""quantile"", q=0.5),\n            True,\n            marks=xfail(reason=""Missing implementation for np.nanpercentile""),\n        ),\n        param(\n            do(""rank"", dim=""x""),\n            False,\n            marks=xfail(reason=""Only implemented for NumPy arrays (via bottleneck)""),\n        ),\n        param(\n            do(""reduce"", func=np.sum, dim=""x""),\n            True,\n            marks=xfail(reason=""Coercion to dense""),\n        ),\n        param(\n            do(""rolling_window"", dim=""x"", window=2, window_dim=""x_win""),\n            True,\n            marks=xfail(reason=""Missing implementation for np.pad""),\n        ),\n        param(\n            do(""shift"", x=2), True, marks=xfail(reason=""mixed sparse-dense operation"")\n        ),\n        param(\n            do(""std""), False, marks=xfail(reason=""Missing implementation for np.nanstd"")\n        ),\n        (do(""sum""), False),\n        param(\n            do(""var""), False, marks=xfail(reason=""Missing implementation for np.nanvar"")\n        ),\n        param(do(""to_dict""), False, marks=xfail(reason=""Coercion to dense"")),\n        (do(""where"", cond=make_xrvar({""x"": 10, ""y"": 5}) > 0.5), True),\n    ],\n    ids=repr,\n)\ndef test_variable_method(func, sparse_output):\n    var_s = make_xrvar({""x"": 10, ""y"": 5})\n    var_d = xr.Variable(var_s.dims, var_s.data.todense())\n    ret_s = func(var_s)\n    ret_d = func(var_d)\n\n    if sparse_output:\n        assert isinstance(ret_s.data, sparse.SparseArray)\n        assert np.allclose(ret_s.data.todense(), ret_d.data, equal_nan=True)\n    else:\n        assert np.allclose(ret_s, ret_d, equal_nan=True)\n\n\n@pytest.mark.parametrize(\n    ""func,sparse_output"",\n    [\n        (do(""squeeze""), True),\n        param(do(""to_index""), False, marks=xfail(reason=""Coercion to dense"")),\n        param(do(""to_index_variable""), False, marks=xfail(reason=""Coercion to dense"")),\n        param(\n            do(""searchsorted"", 0.5),\n            True,\n            marks=xfail(reason=""\'COO\' object has no attribute \'searchsorted\'""),\n        ),\n    ],\n)\ndef test_1d_variable_method(func, sparse_output):\n    var_s = make_xrvar({""x"": 10})\n    var_d = xr.Variable(var_s.dims, var_s.data.todense())\n    ret_s = func(var_s)\n    ret_d = func(var_d)\n\n    if sparse_output:\n        assert isinstance(ret_s.data, sparse.SparseArray)\n        assert np.allclose(ret_s.data.todense(), ret_d.data)\n    else:\n        assert np.allclose(ret_s, ret_d)\n\n\nclass TestSparseVariable:\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        self.data = sparse.random((4, 6), random_state=0, density=0.5)\n        self.var = xr.Variable((""x"", ""y""), self.data)\n\n    def test_unary_op(self):\n        assert_sparse_equal(-self.var.data, -self.data)\n        assert_sparse_equal(abs(self.var).data, abs(self.data))\n        assert_sparse_equal(self.var.round().data, self.data.round())\n\n    @pytest.mark.filterwarnings(""ignore::PendingDeprecationWarning"")\n    def test_univariate_ufunc(self):\n        assert_sparse_equal(np.sin(self.data), xu.sin(self.var).data)\n\n    @pytest.mark.filterwarnings(""ignore::PendingDeprecationWarning"")\n    def test_bivariate_ufunc(self):\n        assert_sparse_equal(np.maximum(self.data, 0), xu.maximum(self.var, 0).data)\n        assert_sparse_equal(np.maximum(self.data, 0), xu.maximum(0, self.var).data)\n\n    def test_repr(self):\n        expected = dedent(\n            """"""\\\n            <xarray.Variable (x: 4, y: 6)>\n            <COO: shape=(4, 6), dtype=float64, nnz=12, fill_value=0.0>""""""\n        )\n        assert expected == repr(self.var)\n\n    def test_pickle(self):\n        v1 = self.var\n        v2 = pickle.loads(pickle.dumps(v1))\n        assert_sparse_equal(v1.data, v2.data)\n\n    def test_missing_values(self):\n        a = np.array([0, 1, np.nan, 3])\n        s = sparse.COO.from_numpy(a)\n        var_s = Variable(""x"", s)\n        assert np.all(var_s.fillna(2).data.todense() == np.arange(4))\n        assert np.all(var_s.count() == 3)\n\n\n@pytest.mark.parametrize(\n    ""prop"",\n    [\n        ""attrs"",\n        ""chunks"",\n        ""coords"",\n        ""data"",\n        ""dims"",\n        ""dtype"",\n        ""encoding"",\n        ""imag"",\n        ""indexes"",\n        ""loc"",\n        ""name"",\n        ""nbytes"",\n        ""ndim"",\n        ""plot"",\n        ""real"",\n        ""shape"",\n        ""size"",\n        ""sizes"",\n        ""str"",\n        ""variable"",\n    ],\n)\ndef test_dataarray_property(prop):\n    arr = make_xrarray({""x"": 10, ""y"": 5})\n    getattr(arr, prop)\n\n\n@pytest.mark.parametrize(\n    ""func,sparse_output"",\n    [\n        (do(""all""), False),\n        (do(""any""), False),\n        (do(""assign_attrs"", {""foo"": ""bar""}), True),\n        (do(""assign_coords"", x=make_xrarray({""x"": 10}).x + 1), True),\n        (do(""astype"", int), True),\n        (do(""clip"", min=0, max=1), True),\n        (do(""compute""), True),\n        (do(""conj""), True),\n        (do(""copy""), True),\n        (do(""count""), False),\n        (do(""diff"", ""x""), True),\n        (do(""drop_vars"", ""x""), True),\n        (do(""expand_dims"", {""z"": 2}, axis=2), True),\n        (do(""get_axis_num"", ""x""), False),\n        (do(""get_index"", ""x""), False),\n        (do(""identical"", make_xrarray({""x"": 5, ""y"": 5})), False),\n        (do(""integrate"", ""x""), True),\n        (do(""isel"", {""x"": slice(0, 3), ""y"": slice(2, 4)}), True),\n        (do(""isnull""), True),\n        (do(""load""), True),\n        (do(""mean""), False),\n        (do(""persist""), True),\n        (do(""reindex"", {""x"": [1, 2, 3]}), True),\n        (do(""rename"", ""foo""), True),\n        (do(""reorder_levels""), True),\n        (do(""reset_coords"", drop=True), True),\n        (do(""reset_index"", ""x""), True),\n        (do(""round""), True),\n        (do(""sel"", x=[0, 1, 2]), True),\n        (do(""shift""), True),\n        (do(""sortby"", ""x"", ascending=False), True),\n        (do(""stack"", z={""x"", ""y""}), True),\n        (do(""transpose""), True),\n        # TODO\n        # set_index\n        # swap_dims\n        (do(""broadcast_equals"", make_xrvar({""x"": 10, ""y"": 5})), False),\n        (do(""equals"", make_xrvar({""x"": 10, ""y"": 5})), False),\n        param(\n            do(""argmax""),\n            True,\n            marks=xfail(reason=""Missing implementation for np.argmax""),\n        ),\n        param(\n            do(""argmin""),\n            True,\n            marks=xfail(reason=""Missing implementation for np.argmin""),\n        ),\n        param(\n            do(""argsort""),\n            True,\n            marks=xfail(reason=""\'COO\' object has no attribute \'argsort\'""),\n        ),\n        param(\n            do(""bfill"", dim=""x""),\n            False,\n            marks=xfail(reason=""Missing implementation for np.flip""),\n        ),\n        (do(""combine_first"", make_xrarray({""x"": 10, ""y"": 5})), True),\n        param(\n            do(""conjugate""),\n            False,\n            marks=xfail(reason=""\'COO\' object has no attribute \'conjugate\'""),\n        ),\n        param(\n            do(""cumprod""),\n            True,\n            marks=xfail(reason=""Missing implementation for np.nancumprod""),\n        ),\n        param(\n            do(""cumsum""),\n            True,\n            marks=xfail(reason=""Missing implementation for np.nancumsum""),\n        ),\n        param(\n            do(""differentiate"", ""x""),\n            False,\n            marks=xfail(reason=""Missing implementation for np.gradient""),\n        ),\n        param(\n            do(""dot"", make_xrarray({""x"": 10, ""y"": 5})),\n            True,\n            marks=xfail(reason=""Missing implementation for np.einsum""),\n        ),\n        param(do(""dropna"", ""x""), False, marks=xfail(reason=""Coercion to dense"")),\n        param(do(""ffill"", ""x""), False, marks=xfail(reason=""Coercion to dense"")),\n        (do(""fillna"", 0), True),\n        param(\n            do(""interp"", coords={""x"": np.arange(10) + 0.5}),\n            True,\n            marks=xfail(reason=""Coercion to dense""),\n        ),\n        param(\n            do(\n                ""interp_like"",\n                make_xrarray(\n                    {""x"": 10, ""y"": 5},\n                    coords={""x"": np.arange(10) + 0.5, ""y"": np.arange(5) + 0.5},\n                ),\n            ),\n            True,\n            marks=xfail(reason=""Indexing COO with more than one iterable index""),\n        ),\n        param(do(""interpolate_na"", ""x""), True, marks=xfail(reason=""Coercion to dense"")),\n        param(\n            do(""isin"", [1, 2, 3]),\n            False,\n            marks=xfail(reason=""Missing implementation for np.isin""),\n        ),\n        param(\n            do(""item"", (1, 1)),\n            False,\n            marks=xfail(reason=""\'COO\' object has no attribute \'item\'""),\n        ),\n        param(do(""max""), False),\n        param(do(""min""), False),\n        param(\n            do(""median""),\n            False,\n            marks=xfail(reason=""Missing implementation for np.nanmedian""),\n        ),\n        (do(""notnull""), True),\n        (do(""pipe"", np.sum, axis=1), True),\n        (do(""prod""), False),\n        param(\n            do(""quantile"", q=0.5),\n            False,\n            marks=xfail(reason=""Missing implementation for np.nanpercentile""),\n        ),\n        param(\n            do(""rank"", ""x""),\n            False,\n            marks=xfail(reason=""Only implemented for NumPy arrays (via bottleneck)""),\n        ),\n        param(\n            do(""reduce"", np.sum, dim=""x""),\n            False,\n            marks=xfail(reason=""Coercion to dense""),\n        ),\n        param(\n            do(\n                ""reindex_like"",\n                make_xrarray(\n                    {""x"": 10, ""y"": 5},\n                    coords={""x"": np.arange(10) + 0.5, ""y"": np.arange(5) + 0.5},\n                ),\n            ),\n            True,\n            marks=xfail(reason=""Indexing COO with more than one iterable index""),\n        ),\n        (do(""roll"", x=2, roll_coords=True), True),\n        param(\n            do(""sel"", x=[0, 1, 2], y=[2, 3]),\n            True,\n            marks=xfail(reason=""Indexing COO with more than one iterable index""),\n        ),\n        param(\n            do(""std""), False, marks=xfail(reason=""Missing implementation for np.nanstd"")\n        ),\n        (do(""sum""), False),\n        param(\n            do(""var""), False, marks=xfail(reason=""Missing implementation for np.nanvar"")\n        ),\n        param(\n            do(""where"", make_xrarray({""x"": 10, ""y"": 5}) > 0.5),\n            False,\n            marks=xfail(reason=""Conversion of dense to sparse when using sparse mask""),\n        ),\n    ],\n    ids=repr,\n)\ndef test_dataarray_method(func, sparse_output):\n    arr_s = make_xrarray(\n        {""x"": 10, ""y"": 5}, coords={""x"": np.arange(10), ""y"": np.arange(5)}\n    )\n    arr_d = xr.DataArray(arr_s.data.todense(), coords=arr_s.coords, dims=arr_s.dims)\n    ret_s = func(arr_s)\n    ret_d = func(arr_d)\n\n    if sparse_output:\n        assert isinstance(ret_s.data, sparse.SparseArray)\n        assert np.allclose(ret_s.data.todense(), ret_d.data, equal_nan=True)\n    else:\n        assert np.allclose(ret_s, ret_d, equal_nan=True)\n\n\n@pytest.mark.parametrize(\n    ""func,sparse_output"",\n    [\n        (do(""squeeze""), True),\n        param(\n            do(""searchsorted"", [1, 2, 3]),\n            False,\n            marks=xfail(reason=""\'COO\' object has no attribute \'searchsorted\'""),\n        ),\n    ],\n)\ndef test_datarray_1d_method(func, sparse_output):\n    arr_s = make_xrarray({""x"": 10}, coords={""x"": np.arange(10)})\n    arr_d = xr.DataArray(arr_s.data.todense(), coords=arr_s.coords, dims=arr_s.dims)\n    ret_s = func(arr_s)\n    ret_d = func(arr_d)\n\n    if sparse_output:\n        assert isinstance(ret_s.data, sparse.SparseArray)\n        assert np.allclose(ret_s.data.todense(), ret_d.data, equal_nan=True)\n    else:\n        assert np.allclose(ret_s, ret_d, equal_nan=True)\n\n\nclass TestSparseDataArrayAndDataset:\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        self.sp_ar = sparse.random((4, 6), random_state=0, density=0.5)\n        self.sp_xr = xr.DataArray(\n            self.sp_ar, coords={""x"": range(4)}, dims=(""x"", ""y""), name=""foo""\n        )\n        self.ds_ar = self.sp_ar.todense()\n        self.ds_xr = xr.DataArray(\n            self.ds_ar, coords={""x"": range(4)}, dims=(""x"", ""y""), name=""foo""\n        )\n\n    def test_to_dataset_roundtrip(self):\n        x = self.sp_xr\n        assert_equal(x, x.to_dataset(""x"").to_array(""x""))\n\n    def test_align(self):\n        a1 = xr.DataArray(\n            sparse.COO.from_numpy(np.arange(4)),\n            dims=[""x""],\n            coords={""x"": [""a"", ""b"", ""c"", ""d""]},\n        )\n        b1 = xr.DataArray(\n            sparse.COO.from_numpy(np.arange(4)),\n            dims=[""x""],\n            coords={""x"": [""a"", ""b"", ""d"", ""e""]},\n        )\n        a2, b2 = xr.align(a1, b1, join=""inner"")\n        assert isinstance(a2.data, sparse.SparseArray)\n        assert isinstance(b2.data, sparse.SparseArray)\n        assert np.all(a2.coords[""x""].data == [""a"", ""b"", ""d""])\n        assert np.all(b2.coords[""x""].data == [""a"", ""b"", ""d""])\n\n    @pytest.mark.xfail(\n        reason=""COO objects currently do not accept more than one ""\n        ""iterable index at a time""\n    )\n    def test_align_2d(self):\n        A1 = xr.DataArray(\n            self.sp_ar,\n            dims=[""x"", ""y""],\n            coords={\n                ""x"": np.arange(self.sp_ar.shape[0]),\n                ""y"": np.arange(self.sp_ar.shape[1]),\n            },\n        )\n\n        A2 = xr.DataArray(\n            self.sp_ar,\n            dims=[""x"", ""y""],\n            coords={\n                ""x"": np.arange(1, self.sp_ar.shape[0] + 1),\n                ""y"": np.arange(1, self.sp_ar.shape[1] + 1),\n            },\n        )\n\n        B1, B2 = xr.align(A1, A2, join=""inner"")\n        assert np.all(B1.coords[""x""] == np.arange(1, self.sp_ar.shape[0]))\n        assert np.all(B1.coords[""y""] == np.arange(1, self.sp_ar.shape[0]))\n        assert np.all(B1.coords[""x""] == B2.coords[""x""])\n        assert np.all(B1.coords[""y""] == B2.coords[""y""])\n\n    def test_align_outer(self):\n        a1 = xr.DataArray(\n            sparse.COO.from_numpy(np.arange(4)),\n            dims=[""x""],\n            coords={""x"": [""a"", ""b"", ""c"", ""d""]},\n        )\n        b1 = xr.DataArray(\n            sparse.COO.from_numpy(np.arange(4)),\n            dims=[""x""],\n            coords={""x"": [""a"", ""b"", ""d"", ""e""]},\n        )\n        a2, b2 = xr.align(a1, b1, join=""outer"")\n        assert isinstance(a2.data, sparse.SparseArray)\n        assert isinstance(b2.data, sparse.SparseArray)\n        assert np.all(a2.coords[""x""].data == [""a"", ""b"", ""c"", ""d"", ""e""])\n        assert np.all(b2.coords[""x""].data == [""a"", ""b"", ""c"", ""d"", ""e""])\n\n    def test_concat(self):\n        ds1 = xr.Dataset(data_vars={""d"": self.sp_xr})\n        ds2 = xr.Dataset(data_vars={""d"": self.sp_xr})\n        ds3 = xr.Dataset(data_vars={""d"": self.sp_xr})\n        out = xr.concat([ds1, ds2, ds3], dim=""x"")\n        assert_sparse_equal(\n            out[""d""].data,\n            sparse.concatenate([self.sp_ar, self.sp_ar, self.sp_ar], axis=0),\n        )\n\n        out = xr.concat([self.sp_xr, self.sp_xr, self.sp_xr], dim=""y"")\n        assert_sparse_equal(\n            out.data, sparse.concatenate([self.sp_ar, self.sp_ar, self.sp_ar], axis=1)\n        )\n\n    def test_stack(self):\n        arr = make_xrarray({""w"": 2, ""x"": 3, ""y"": 4})\n        stacked = arr.stack(z=(""x"", ""y""))\n\n        z = pd.MultiIndex.from_product([np.arange(3), np.arange(4)], names=[""x"", ""y""])\n\n        expected = xr.DataArray(\n            arr.data.reshape((2, -1)), {""w"": [0, 1], ""z"": z}, dims=[""w"", ""z""]\n        )\n\n        assert_equal(expected, stacked)\n\n        roundtripped = stacked.unstack()\n        assert arr.identical(roundtripped)\n\n    @pytest.mark.filterwarnings(""ignore::PendingDeprecationWarning"")\n    def test_ufuncs(self):\n        x = self.sp_xr\n        assert_equal(np.sin(x), xu.sin(x))\n\n    def test_dataarray_repr(self):\n        a = xr.DataArray(\n            sparse.COO.from_numpy(np.ones(4)),\n            dims=[""x""],\n            coords={""y"": (""x"", sparse.COO.from_numpy(np.arange(4, dtype=""i8"")))},\n        )\n        expected = dedent(\n            """"""\\\n            <xarray.DataArray (x: 4)>\n            <COO: shape=(4,), dtype=float64, nnz=4, fill_value=0.0>\n            Coordinates:\n                y        (x) int64 <COO: nnz=3, fill_value=0>\n            Dimensions without coordinates: x""""""\n        )\n        assert expected == repr(a)\n\n    def test_dataset_repr(self):\n        ds = xr.Dataset(\n            data_vars={""a"": (""x"", sparse.COO.from_numpy(np.ones(4)))},\n            coords={""y"": (""x"", sparse.COO.from_numpy(np.arange(4, dtype=""i8"")))},\n        )\n        expected = dedent(\n            """"""\\\n            <xarray.Dataset>\n            Dimensions:  (x: 4)\n            Coordinates:\n                y        (x) int64 <COO: nnz=3, fill_value=0>\n            Dimensions without coordinates: x\n            Data variables:\n                a        (x) float64 <COO: nnz=4, fill_value=0.0>""""""\n        )\n        assert expected == repr(ds)\n\n    def test_sparse_dask_dataset_repr(self):\n        pytest.importorskip(""dask"", minversion=""2.0"")\n        ds = xr.Dataset(\n            data_vars={""a"": (""x"", sparse.COO.from_numpy(np.ones(4)))}\n        ).chunk()\n        expected = dedent(\n            """"""\\\n            <xarray.Dataset>\n            Dimensions:  (x: 4)\n            Dimensions without coordinates: x\n            Data variables:\n                a        (x) float64 dask.array<chunksize=(4,), meta=sparse.COO>""""""\n        )\n        assert expected == repr(ds)\n\n    def test_dataarray_pickle(self):\n        a1 = xr.DataArray(\n            sparse.COO.from_numpy(np.ones(4)),\n            dims=[""x""],\n            coords={""y"": (""x"", sparse.COO.from_numpy(np.arange(4)))},\n        )\n        a2 = pickle.loads(pickle.dumps(a1))\n        assert_identical(a1, a2)\n\n    def test_dataset_pickle(self):\n        ds1 = xr.Dataset(\n            data_vars={""a"": (""x"", sparse.COO.from_numpy(np.ones(4)))},\n            coords={""y"": (""x"", sparse.COO.from_numpy(np.arange(4)))},\n        )\n        ds2 = pickle.loads(pickle.dumps(ds1))\n        assert_identical(ds1, ds2)\n\n    def test_coarsen(self):\n        a1 = self.ds_xr\n        a2 = self.sp_xr\n        m1 = a1.coarsen(x=2, boundary=""trim"").mean()\n        m2 = a2.coarsen(x=2, boundary=""trim"").mean()\n\n        assert isinstance(m2.data, sparse.SparseArray)\n        assert np.allclose(m1.data, m2.data.todense())\n\n    @pytest.mark.xfail(reason=""No implementation of np.pad"")\n    def test_rolling(self):\n        a1 = self.ds_xr\n        a2 = self.sp_xr\n        m1 = a1.rolling(x=2, center=True).mean()\n        m2 = a2.rolling(x=2, center=True).mean()\n\n        assert isinstance(m2.data, sparse.SparseArray)\n        assert np.allclose(m1.data, m2.data.todense())\n\n    @pytest.mark.xfail(reason=""Coercion to dense"")\n    def test_rolling_exp(self):\n        a1 = self.ds_xr\n        a2 = self.sp_xr\n        m1 = a1.rolling_exp(x=2, center=True).mean()\n        m2 = a2.rolling_exp(x=2, center=True).mean()\n\n        assert isinstance(m2.data, sparse.SparseArray)\n        assert np.allclose(m1.data, m2.data.todense())\n\n    @pytest.mark.xfail(reason=""No implementation of np.einsum"")\n    def test_dot(self):\n        a1 = self.xp_xr.dot(self.xp_xr[0])\n        a2 = self.sp_ar.dot(self.sp_ar[0])\n        assert_equal(a1, a2)\n\n    @pytest.mark.xfail(reason=""Groupby reductions produce dense output"")\n    def test_groupby(self):\n        x1 = self.ds_xr\n        x2 = self.sp_xr\n        m1 = x1.groupby(""x"").mean(...)\n        m2 = x2.groupby(""x"").mean(...)\n        assert isinstance(m2.data, sparse.SparseArray)\n        assert np.allclose(m1.data, m2.data.todense())\n\n    @pytest.mark.xfail(reason=""Groupby reductions produce dense output"")\n    def test_groupby_first(self):\n        x = self.sp_xr.copy()\n        x.coords[""ab""] = (""x"", [""a"", ""a"", ""b"", ""b""])\n        x.groupby(""ab"").first()\n        x.groupby(""ab"").first(skipna=False)\n\n    @pytest.mark.xfail(reason=""Groupby reductions produce dense output"")\n    def test_groupby_bins(self):\n        x1 = self.ds_xr\n        x2 = self.sp_xr\n        m1 = x1.groupby_bins(""x"", bins=[0, 3, 7, 10]).sum(...)\n        m2 = x2.groupby_bins(""x"", bins=[0, 3, 7, 10]).sum(...)\n        assert isinstance(m2.data, sparse.SparseArray)\n        assert np.allclose(m1.data, m2.data.todense())\n\n    @pytest.mark.xfail(reason=""Resample produces dense output"")\n    def test_resample(self):\n        t1 = xr.DataArray(\n            np.linspace(0, 11, num=12),\n            coords=[\n                pd.date_range(""15/12/1999"", periods=12, freq=pd.DateOffset(months=1))\n            ],\n            dims=""time"",\n        )\n        t2 = t1.copy()\n        t2.data = sparse.COO(t2.data)\n        m1 = t1.resample(time=""QS-DEC"").mean()\n        m2 = t2.resample(time=""QS-DEC"").mean()\n        assert isinstance(m2.data, sparse.SparseArray)\n        assert np.allclose(m1.data, m2.data.todense())\n\n    @pytest.mark.xfail\n    def test_reindex(self):\n        x1 = self.ds_xr\n        x2 = self.sp_xr\n        for kwargs in [\n            {""x"": [2, 3, 4]},\n            {""x"": [1, 100, 2, 101, 3]},\n            {""x"": [2.5, 3, 3.5], ""y"": [2, 2.5, 3]},\n        ]:\n            m1 = x1.reindex(**kwargs)\n            m2 = x2.reindex(**kwargs)\n            assert np.allclose(m1, m2, equal_nan=True)\n\n    @pytest.mark.xfail\n    def test_merge(self):\n        x = self.sp_xr\n        y = xr.merge([x, x.rename(""bar"")]).to_array()\n        assert isinstance(y, sparse.SparseArray)\n\n    @pytest.mark.xfail\n    def test_where(self):\n        a = np.arange(10)\n        cond = a > 3\n        xr.DataArray(a).where(cond)\n\n        s = sparse.COO.from_numpy(a)\n        cond = s > 3\n        xr.DataArray(s).where(cond)\n\n        x = xr.DataArray(s)\n        cond = x > 3\n        x.where(cond)\n\n\nclass TestSparseCoords:\n    @pytest.mark.xfail(reason=""Coercion of coords to dense"")\n    def test_sparse_coords(self):\n        xr.DataArray(\n            sparse.COO.from_numpy(np.arange(4)),\n            dims=[""x""],\n            coords={""x"": sparse.COO.from_numpy([1, 2, 3, 4])},\n        )\n\n\n@requires_dask\ndef test_chunk():\n    s = sparse.COO.from_numpy(np.array([0, 0, 1, 2]))\n    a = DataArray(s)\n    ac = a.chunk(2)\n    assert ac.chunks == ((2, 2),)\n    assert isinstance(ac.data._meta, sparse.COO)\n    assert_identical(ac, a)\n\n    ds = a.to_dataset(name=""a"")\n    dsc = ds.chunk(2)\n    assert dsc.chunks == {""dim_0"": (2, 2)}\n    assert_identical(dsc, ds)\n\n\n@requires_dask\ndef test_dask_token():\n    import dask\n\n    s = sparse.COO.from_numpy(np.array([0, 0, 1, 2]))\n\n    # https://github.com/pydata/sparse/issues/300\n    s.__dask_tokenize__ = lambda: dask.base.normalize_token(s.__dict__)\n\n    a = DataArray(s)\n    t1 = dask.base.tokenize(a)\n    t2 = dask.base.tokenize(a)\n    t3 = dask.base.tokenize(a + 1)\n    assert t1 == t2\n    assert t3 != t2\n    assert isinstance(a.data, sparse.COO)\n\n    ac = a.chunk(2)\n    t4 = dask.base.tokenize(ac)\n    t5 = dask.base.tokenize(ac + 1)\n    assert t4 != t5\n    assert isinstance(ac.data._meta, sparse.COO)\n\n\n@requires_dask\ndef test_apply_ufunc_meta_to_blockwise():\n    da = xr.DataArray(np.zeros((2, 3)), dims=[""x"", ""y""]).chunk({""x"": 2, ""y"": 1})\n    sparse_meta = sparse.COO.from_numpy(np.zeros((0, 0)))\n\n    # if dask computed meta, it would be np.ndarray\n    expected = xr.apply_ufunc(\n        lambda x: x, da, dask=""parallelized"", output_dtypes=[da.dtype], meta=sparse_meta\n    ).data._meta\n\n    assert_sparse_equal(expected, sparse_meta)\n'"
xarray/tests/test_testing.py,0,"b'import xarray as xr\n\n\ndef test_allclose_regression():\n    x = xr.DataArray(1.01)\n    y = xr.DataArray(1.02)\n    xr.testing.assert_allclose(x, y, atol=0.01)\n'"
xarray/tests/test_tutorial.py,0,"b'import os\nfrom contextlib import suppress\n\nimport pytest\n\nfrom xarray import DataArray, tutorial\n\nfrom . import assert_identical, network\n\n\n@network\nclass TestLoadDataset:\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        self.testfile = ""tiny""\n        self.testfilepath = os.path.expanduser(\n            os.sep.join((""~"", "".xarray_tutorial_data"", self.testfile))\n        )\n        with suppress(OSError):\n            os.remove(f""{self.testfilepath}.nc"")\n        with suppress(OSError):\n            os.remove(f""{self.testfilepath}.md5"")\n\n    def test_download_from_github(self):\n        ds = tutorial.open_dataset(self.testfile).load()\n        tiny = DataArray(range(5), name=""tiny"").to_dataset()\n        assert_identical(ds, tiny)\n\n    def test_download_from_github_load_without_cache(self):\n        ds_nocache = tutorial.open_dataset(self.testfile, cache=False).load()\n        ds_cache = tutorial.open_dataset(self.testfile).load()\n        assert_identical(ds_cache, ds_nocache)\n'"
xarray/tests/test_ufuncs.py,37,"b'import pickle\n\nimport numpy as np\nimport pytest\n\nimport xarray as xr\nimport xarray.ufuncs as xu\n\nfrom . import assert_array_equal\nfrom . import assert_identical as assert_identical_\nfrom . import mock, raises_regex\n\n\ndef assert_identical(a, b):\n    assert type(a) is type(b) or float(a) == float(b)\n    if isinstance(a, (xr.DataArray, xr.Dataset, xr.Variable)):\n        assert_identical_(a, b)\n    else:\n        assert_array_equal(a, b)\n\n\ndef test_unary():\n    args = [\n        0,\n        np.zeros(2),\n        xr.Variable([""x""], [0, 0]),\n        xr.DataArray([0, 0], dims=""x""),\n        xr.Dataset({""y"": (""x"", [0, 0])}),\n    ]\n    for a in args:\n        assert_identical(a + 1, np.cos(a))\n\n\ndef test_binary():\n    args = [\n        0,\n        np.zeros(2),\n        xr.Variable([""x""], [0, 0]),\n        xr.DataArray([0, 0], dims=""x""),\n        xr.Dataset({""y"": (""x"", [0, 0])}),\n    ]\n    for n, t1 in enumerate(args):\n        for t2 in args[n:]:\n            assert_identical(t2 + 1, np.maximum(t1, t2 + 1))\n            assert_identical(t2 + 1, np.maximum(t2, t1 + 1))\n            assert_identical(t2 + 1, np.maximum(t1 + 1, t2))\n            assert_identical(t2 + 1, np.maximum(t2 + 1, t1))\n\n\ndef test_binary_out():\n    args = [\n        1,\n        np.ones(2),\n        xr.Variable([""x""], [1, 1]),\n        xr.DataArray([1, 1], dims=""x""),\n        xr.Dataset({""y"": (""x"", [1, 1])}),\n    ]\n    for arg in args:\n        actual_mantissa, actual_exponent = np.frexp(arg)\n        assert_identical(actual_mantissa, 0.5 * arg)\n        assert_identical(actual_exponent, arg)\n\n\ndef test_groupby():\n    ds = xr.Dataset({""a"": (""x"", [0, 0, 0])}, {""c"": (""x"", [0, 0, 1])})\n    ds_grouped = ds.groupby(""c"")\n    group_mean = ds_grouped.mean(""x"")\n    arr_grouped = ds[""a""].groupby(""c"")\n\n    assert_identical(ds, np.maximum(ds_grouped, group_mean))\n    assert_identical(ds, np.maximum(group_mean, ds_grouped))\n\n    assert_identical(ds, np.maximum(arr_grouped, group_mean))\n    assert_identical(ds, np.maximum(group_mean, arr_grouped))\n\n    assert_identical(ds, np.maximum(ds_grouped, group_mean[""a""]))\n    assert_identical(ds, np.maximum(group_mean[""a""], ds_grouped))\n\n    assert_identical(ds.a, np.maximum(arr_grouped, group_mean.a))\n    assert_identical(ds.a, np.maximum(group_mean.a, arr_grouped))\n\n    with raises_regex(ValueError, ""mismatched lengths for dimension""):\n        np.maximum(ds.a.variable, ds_grouped)\n\n\ndef test_alignment():\n    ds1 = xr.Dataset({""a"": (""x"", [1, 2])}, {""x"": [0, 1]})\n    ds2 = xr.Dataset({""a"": (""x"", [2, 3]), ""b"": 4}, {""x"": [1, 2]})\n\n    actual = np.add(ds1, ds2)\n    expected = xr.Dataset({""a"": (""x"", [4])}, {""x"": [1]})\n    assert_identical_(actual, expected)\n\n    with xr.set_options(arithmetic_join=""outer""):\n        actual = np.add(ds1, ds2)\n        expected = xr.Dataset(\n            {""a"": (""x"", [np.nan, 4, np.nan]), ""b"": np.nan}, coords={""x"": [0, 1, 2]}\n        )\n        assert_identical_(actual, expected)\n\n\ndef test_kwargs():\n    x = xr.DataArray(0)\n    result = np.add(x, 1, dtype=np.float64)\n    assert result.dtype == np.float64\n\n\ndef test_xarray_defers_to_unrecognized_type():\n    class Other:\n        def __array_ufunc__(self, *args, **kwargs):\n            return ""other""\n\n    xarray_obj = xr.DataArray([1, 2, 3])\n    other = Other()\n    assert np.maximum(xarray_obj, other) == ""other""\n    assert np.sin(xarray_obj, out=other) == ""other""\n\n\ndef test_xarray_handles_dask():\n    da = pytest.importorskip(""dask.array"")\n    x = xr.DataArray(np.ones((2, 2)), dims=[""x"", ""y""])\n    y = da.ones((2, 2), chunks=(2, 2))\n    result = np.add(x, y)\n    assert result.chunks == ((2,), (2,))\n    assert isinstance(result, xr.DataArray)\n\n\ndef test_dask_defers_to_xarray():\n    da = pytest.importorskip(""dask.array"")\n    x = xr.DataArray(np.ones((2, 2)), dims=[""x"", ""y""])\n    y = da.ones((2, 2), chunks=(2, 2))\n    result = np.add(y, x)\n    assert result.chunks == ((2,), (2,))\n    assert isinstance(result, xr.DataArray)\n\n\ndef test_gufunc_methods():\n    xarray_obj = xr.DataArray([1, 2, 3])\n    with raises_regex(NotImplementedError, ""reduce method""):\n        np.add.reduce(xarray_obj, 1)\n\n\ndef test_out():\n    xarray_obj = xr.DataArray([1, 2, 3])\n\n    # xarray out arguments should raise\n    with raises_regex(NotImplementedError, ""`out` argument""):\n        np.add(xarray_obj, 1, out=xarray_obj)\n\n    # but non-xarray should be OK\n    other = np.zeros((3,))\n    np.add(other, xarray_obj, out=other)\n    assert_identical(other, np.array([1, 2, 3]))\n\n\ndef test_gufuncs():\n    xarray_obj = xr.DataArray([1, 2, 3])\n    fake_gufunc = mock.Mock(signature=""(n)->()"", autospec=np.sin)\n    with raises_regex(NotImplementedError, ""generalized ufuncs""):\n        xarray_obj.__array_ufunc__(fake_gufunc, ""__call__"", xarray_obj)\n\n\ndef test_xarray_ufuncs_deprecation():\n    with pytest.warns(PendingDeprecationWarning, match=""xarray.ufuncs""):\n        xu.cos(xr.DataArray([0, 1]))\n\n    with pytest.warns(None) as record:\n        xu.angle(xr.DataArray([0, 1]))\n    record = [el.message for el in record if el.category == PendingDeprecationWarning]\n    assert len(record) == 0\n\n\n@pytest.mark.filterwarnings(""ignore::RuntimeWarning"")\n@pytest.mark.parametrize(\n    ""name"",\n    [\n        name\n        for name in dir(xu)\n        if (\n            not name.startswith(""_"")\n            and hasattr(np, name)\n            and name not in [""print_function"", ""absolute_import"", ""division""]\n        )\n    ],\n)\ndef test_numpy_ufuncs(name, request):\n    x = xr.DataArray([1, 1])\n\n    np_func = getattr(np, name)\n    if hasattr(np_func, ""nin"") and np_func.nin == 2:\n        args = (x, x)\n    else:\n        args = (x,)\n\n    y = np_func(*args)\n\n    if name in [""angle"", ""iscomplex""]:\n        # these functions need to be handled with __array_function__ protocol\n        assert isinstance(y, np.ndarray)\n    elif name in [""frexp""]:\n        # np.frexp returns a tuple\n        assert not isinstance(y, xr.DataArray)\n    else:\n        assert isinstance(y, xr.DataArray)\n\n\n@pytest.mark.filterwarnings(""ignore:xarray.ufuncs"")\ndef test_xarray_ufuncs_pickle():\n    a = 1.0\n    cos_pickled = pickle.loads(pickle.dumps(xu.cos))\n    assert_identical(cos_pickled(a), xu.cos(a))\n'"
xarray/tests/test_units.py,478,"b'import functools\nimport operator\nfrom distutils.version import LooseVersion\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nimport xarray as xr\nfrom xarray.core import formatting\nfrom xarray.core.npcompat import IS_NEP18_ACTIVE\nfrom xarray.testing import assert_allclose, assert_identical\n\nfrom .test_variable import _PAD_XR_NP_ARGS, VariableSubclassobjects\n\npint = pytest.importorskip(""pint"")\nDimensionalityError = pint.errors.DimensionalityError\n\n\n# make sure scalars are converted to 0d arrays so quantities can\n# always be treated like ndarrays\nunit_registry = pint.UnitRegistry(force_ndarray=True)\nQuantity = unit_registry.Quantity\n\n\npytestmark = [\n    pytest.mark.skipif(\n        not IS_NEP18_ACTIVE, reason=""NUMPY_EXPERIMENTAL_ARRAY_FUNCTION is not enabled""\n    ),\n    # TODO: remove this once pint has a released version with __array_function__\n    pytest.mark.skipif(\n        not hasattr(unit_registry.Quantity, ""__array_function__""),\n        reason=""pint does not implement __array_function__ yet"",\n    ),\n    # pytest.mark.filterwarnings(""ignore:::pint[.*]""),\n]\n\n\ndef is_compatible(unit1, unit2):\n    def dimensionality(obj):\n        if isinstance(obj, (unit_registry.Quantity, unit_registry.Unit)):\n            unit_like = obj\n        else:\n            unit_like = unit_registry.dimensionless\n\n        return unit_like.dimensionality\n\n    return dimensionality(unit1) == dimensionality(unit2)\n\n\ndef compatible_mappings(first, second):\n    return {\n        key: is_compatible(unit1, unit2)\n        for key, (unit1, unit2) in merge_mappings(first, second)\n    }\n\n\ndef array_extract_units(obj):\n    if isinstance(obj, (xr.Variable, xr.DataArray, xr.Dataset)):\n        obj = obj.data\n\n    try:\n        return obj.units\n    except AttributeError:\n        return None\n\n\ndef array_strip_units(array):\n    try:\n        return array.magnitude\n    except AttributeError:\n        return array\n\n\ndef array_attach_units(data, unit):\n    if isinstance(data, Quantity):\n        raise ValueError(f""cannot attach unit {unit} to quantity {data}"")\n\n    try:\n        quantity = data * unit\n    except np.core._exceptions.UFuncTypeError:\n        if isinstance(unit, unit_registry.Unit):\n            raise\n\n        quantity = data\n\n    return quantity\n\n\ndef extract_units(obj):\n    if isinstance(obj, xr.Dataset):\n        vars_units = {\n            name: array_extract_units(value) for name, value in obj.data_vars.items()\n        }\n        coords_units = {\n            name: array_extract_units(value) for name, value in obj.coords.items()\n        }\n\n        units = {**vars_units, **coords_units}\n    elif isinstance(obj, xr.DataArray):\n        vars_units = {obj.name: array_extract_units(obj)}\n        coords_units = {\n            name: array_extract_units(value) for name, value in obj.coords.items()\n        }\n\n        units = {**vars_units, **coords_units}\n    elif isinstance(obj, xr.Variable):\n        vars_units = {None: array_extract_units(obj.data)}\n\n        units = {**vars_units}\n    elif isinstance(obj, Quantity):\n        vars_units = {None: array_extract_units(obj)}\n\n        units = {**vars_units}\n    else:\n        units = {}\n\n    return units\n\n\ndef strip_units(obj):\n    if isinstance(obj, xr.Dataset):\n        data_vars = {\n            strip_units(name): strip_units(value)\n            for name, value in obj.data_vars.items()\n        }\n        coords = {\n            strip_units(name): strip_units(value) for name, value in obj.coords.items()\n        }\n\n        new_obj = xr.Dataset(data_vars=data_vars, coords=coords)\n    elif isinstance(obj, xr.DataArray):\n        data = array_strip_units(obj.data)\n        coords = {\n            strip_units(name): (\n                (value.dims, array_strip_units(value.data))\n                if isinstance(value.data, Quantity)\n                else value  # to preserve multiindexes\n            )\n            for name, value in obj.coords.items()\n        }\n\n        new_obj = xr.DataArray(\n            name=strip_units(obj.name), data=data, coords=coords, dims=obj.dims\n        )\n    elif isinstance(obj, xr.Variable):\n        data = array_strip_units(obj.data)\n        new_obj = obj.copy(data=data)\n    elif isinstance(obj, unit_registry.Quantity):\n        new_obj = obj.magnitude\n    elif isinstance(obj, (list, tuple)):\n        return type(obj)(strip_units(elem) for elem in obj)\n    else:\n        new_obj = obj\n\n    return new_obj\n\n\ndef attach_units(obj, units):\n    if not isinstance(obj, (xr.DataArray, xr.Dataset, xr.Variable)):\n        units = units.get(""data"", None) or units.get(None, None) or 1\n        return array_attach_units(obj, units)\n\n    if isinstance(obj, xr.Dataset):\n        data_vars = {\n            name: attach_units(value, units) for name, value in obj.data_vars.items()\n        }\n\n        coords = {\n            name: attach_units(value, units) for name, value in obj.coords.items()\n        }\n\n        new_obj = xr.Dataset(data_vars=data_vars, coords=coords, attrs=obj.attrs)\n    elif isinstance(obj, xr.DataArray):\n        # try the array name, ""data"" and None, then fall back to dimensionless\n        data_units = (\n            units.get(obj.name, None)\n            or units.get(""data"", None)\n            or units.get(None, None)\n            or 1\n        )\n\n        data = array_attach_units(obj.data, data_units)\n\n        coords = {\n            name: (\n                (value.dims, array_attach_units(value.data, units.get(name) or 1))\n                if name in units\n                # to preserve multiindexes\n                else value\n            )\n            for name, value in obj.coords.items()\n        }\n        dims = obj.dims\n        attrs = obj.attrs\n\n        new_obj = xr.DataArray(\n            name=obj.name, data=data, coords=coords, attrs=attrs, dims=dims\n        )\n    else:\n        data_units = units.get(""data"", None) or units.get(None, None) or 1\n\n        data = array_attach_units(obj.data, data_units)\n        new_obj = obj.copy(data=data)\n\n    return new_obj\n\n\ndef convert_units(obj, to):\n    # preprocess\n    to = {\n        key: None if not isinstance(value, unit_registry.Unit) else value\n        for key, value in to.items()\n    }\n    if isinstance(obj, xr.Dataset):\n        data_vars = {\n            name: convert_units(array.variable, {None: to.get(name)})\n            for name, array in obj.data_vars.items()\n        }\n        coords = {\n            name: convert_units(array.variable, {None: to.get(name)})\n            for name, array in obj.coords.items()\n        }\n\n        new_obj = xr.Dataset(data_vars=data_vars, coords=coords, attrs=obj.attrs)\n    elif isinstance(obj, xr.DataArray):\n        name = obj.name\n\n        new_units = (\n            to.get(name, None) or to.get(""data"", None) or to.get(None, None) or None\n        )\n        data = convert_units(obj.variable, {None: new_units})\n\n        coords = {\n            name: (array.dims, convert_units(array.variable, {None: to.get(name)}))\n            for name, array in obj.coords.items()\n            if name != obj.name\n        }\n\n        new_obj = xr.DataArray(\n            name=name, data=data, coords=coords, attrs=obj.attrs, dims=obj.dims\n        )\n    elif isinstance(obj, xr.Variable):\n        new_data = convert_units(obj.data, to)\n        new_obj = obj.copy(data=new_data)\n    elif isinstance(obj, unit_registry.Quantity):\n        units = to.get(None)\n        new_obj = obj.to(units) if units is not None else obj\n    else:\n        new_obj = obj\n\n    return new_obj\n\n\ndef assert_units_equal(a, b):\n    __tracebackhide__ = True\n    assert extract_units(a) == extract_units(b)\n\n\ndef assert_equal_with_units(a, b):\n    # works like xr.testing.assert_equal, but also explicitly checks units\n    # so, it is more like assert_identical\n    __tracebackhide__ = True\n\n    if isinstance(a, xr.Dataset) or isinstance(b, xr.Dataset):\n        a_units = extract_units(a)\n        b_units = extract_units(b)\n\n        a_without_units = strip_units(a)\n        b_without_units = strip_units(b)\n\n        assert a_without_units.equals(b_without_units), formatting.diff_dataset_repr(\n            a, b, ""equals""\n        )\n        assert a_units == b_units\n    else:\n        a = a if not isinstance(a, (xr.DataArray, xr.Variable)) else a.data\n        b = b if not isinstance(b, (xr.DataArray, xr.Variable)) else b.data\n\n        assert type(a) == type(b) or (\n            isinstance(a, Quantity) and isinstance(b, Quantity)\n        )\n\n        # workaround until pint implements allclose in __array_function__\n        if isinstance(a, Quantity) or isinstance(b, Quantity):\n            assert (\n                hasattr(a, ""magnitude"") and hasattr(b, ""magnitude"")\n            ) and np.allclose(a.magnitude, b.magnitude, equal_nan=True)\n            assert (hasattr(a, ""units"") and hasattr(b, ""units"")) and a.units == b.units\n        else:\n            assert np.allclose(a, b, equal_nan=True)\n\n\n@pytest.fixture(params=[float, int])\ndef dtype(request):\n    return request.param\n\n\ndef merge_mappings(*mappings):\n    for key in set(mappings[0]).intersection(*mappings[1:]):\n        yield key, tuple(m[key] for m in mappings)\n\n\ndef merge_args(default_args, new_args):\n    from itertools import zip_longest\n\n    fill_value = object()\n    return [\n        second if second is not fill_value else first\n        for first, second in zip_longest(default_args, new_args, fillvalue=fill_value)\n    ]\n\n\nclass method:\n    """""" wrapper class to help with passing methods via parametrize\n\n    This is works a bit similar to using `partial(Class.method, arg, kwarg)`\n    """"""\n\n    def __init__(self, name, *args, **kwargs):\n        self.name = name\n        self.args = args\n        self.kwargs = kwargs\n\n    def __call__(self, obj, *args, **kwargs):\n        from collections.abc import Callable\n        from functools import partial\n\n        all_args = merge_args(self.args, args)\n        all_kwargs = {**self.kwargs, **kwargs}\n\n        func = getattr(obj, self.name, None)\n        if func is None or not isinstance(func, Callable):\n            # fall back to module level numpy functions if not a xarray object\n            if not isinstance(obj, (xr.Variable, xr.DataArray, xr.Dataset)):\n                numpy_func = getattr(np, self.name)\n                func = partial(numpy_func, obj)\n                # remove typical xarray args like ""dim""\n                exclude_kwargs = (""dim"", ""dims"")\n                all_kwargs = {\n                    key: value\n                    for key, value in all_kwargs.items()\n                    if key not in exclude_kwargs\n                }\n            else:\n                raise AttributeError(f""{obj} has no method named \'{self.name}\'"")\n\n        return func(*all_args, **all_kwargs)\n\n    def __repr__(self):\n        return f""method_{self.name}""\n\n\nclass function:\n    """""" wrapper class for numpy functions\n\n    Same as method, but the name is used for referencing numpy functions\n    """"""\n\n    def __init__(self, name_or_function, *args, function_label=None, **kwargs):\n        if callable(name_or_function):\n            self.name = (\n                function_label\n                if function_label is not None\n                else name_or_function.__name__\n            )\n            self.func = name_or_function\n        else:\n            self.name = name_or_function if function_label is None else function_label\n            self.func = getattr(np, name_or_function)\n            if self.func is None:\n                raise AttributeError(\n                    f""module \'numpy\' has no attribute named \'{self.name}\'""\n                )\n\n        self.args = args\n        self.kwargs = kwargs\n\n    def __call__(self, *args, **kwargs):\n        all_args = merge_args(self.args, args)\n        all_kwargs = {**self.kwargs, **kwargs}\n\n        return self.func(*all_args, **all_kwargs)\n\n    def __repr__(self):\n        return f""function_{self.name}""\n\n\ndef test_apply_ufunc_dataarray(dtype):\n    func = functools.partial(\n        xr.apply_ufunc, np.mean, input_core_dims=[[""x""]], kwargs={""axis"": -1}\n    )\n\n    array = np.linspace(0, 10, 20).astype(dtype) * unit_registry.m\n    x = np.arange(20) * unit_registry.s\n    data_array = xr.DataArray(data=array, dims=""x"", coords={""x"": x})\n\n    expected = attach_units(func(strip_units(data_array)), extract_units(data_array))\n    actual = func(data_array)\n\n    assert_units_equal(expected, actual)\n    assert_identical(expected, actual)\n\n\ndef test_apply_ufunc_dataset(dtype):\n    func = functools.partial(\n        xr.apply_ufunc, np.mean, input_core_dims=[[""x""]], kwargs={""axis"": -1}\n    )\n\n    array1 = np.linspace(0, 10, 5 * 10).reshape(5, 10).astype(dtype) * unit_registry.m\n    array2 = np.linspace(0, 10, 5).astype(dtype) * unit_registry.m\n\n    x = np.arange(5) * unit_registry.s\n    y = np.arange(10) * unit_registry.m\n\n    ds = xr.Dataset(\n        data_vars={""a"": ((""x"", ""y""), array1), ""b"": (""x"", array2)},\n        coords={""x"": x, ""y"": y},\n    )\n\n    expected = attach_units(func(strip_units(ds)), extract_units(ds))\n    actual = func(ds)\n\n    assert_units_equal(expected, actual)\n    assert_identical(expected, actual)\n\n\n@pytest.mark.parametrize(\n    ""unit,error"",\n    (\n        pytest.param(1, DimensionalityError, id=""no_unit""),\n        pytest.param(\n            unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n        ),\n        pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n        pytest.param(unit_registry.mm, None, id=""compatible_unit""),\n        pytest.param(unit_registry.m, None, id=""identical_unit""),\n    ),\n    ids=repr,\n)\n@pytest.mark.parametrize(\n    ""variant"",\n    (\n        ""data"",\n        pytest.param(""dims"", marks=pytest.mark.xfail(reason=""indexes strip units"")),\n        ""coords"",\n    ),\n)\n@pytest.mark.parametrize(""fill_value"", (10, np.nan))\ndef test_align_dataarray(fill_value, variant, unit, error, dtype):\n    original_unit = unit_registry.m\n\n    variants = {\n        ""data"": (unit, original_unit, original_unit),\n        ""dims"": (original_unit, unit, original_unit),\n        ""coords"": (original_unit, original_unit, unit),\n    }\n    data_unit, dim_unit, coord_unit = variants.get(variant)\n\n    array1 = np.linspace(0, 10, 2 * 5).reshape(2, 5).astype(dtype) * original_unit\n    array2 = np.linspace(0, 8, 2 * 5).reshape(2, 5).astype(dtype) * data_unit\n    x = np.arange(2) * original_unit\n\n    y1 = np.arange(5) * original_unit\n    y2 = np.arange(2, 7) * dim_unit\n    y_a1 = np.array([3, 5, 7, 8, 9]) * original_unit\n    y_a2 = np.array([7, 8, 9, 11, 13]) * coord_unit\n\n    coords1 = {""x"": x, ""y"": y1}\n    coords2 = {""x"": x, ""y"": y2}\n    if variant == ""coords"":\n        coords1[""y_a""] = (""y"", y_a1)\n        coords2[""y_a""] = (""y"", y_a2)\n\n    data_array1 = xr.DataArray(data=array1, coords=coords1, dims=(""x"", ""y""))\n    data_array2 = xr.DataArray(data=array2, coords=coords2, dims=(""x"", ""y""))\n\n    fill_value = fill_value * data_unit\n    func = function(xr.align, join=""outer"", fill_value=fill_value)\n    if error is not None and not (\n        np.isnan(fill_value) and not isinstance(fill_value, Quantity)\n    ):\n        with pytest.raises(error):\n            func(data_array1, data_array2)\n\n        return\n\n    stripped_kwargs = {\n        key: strip_units(\n            convert_units(value, {None: original_unit if data_unit != 1 else None})\n        )\n        for key, value in func.kwargs.items()\n    }\n    units_a = extract_units(data_array1)\n    units_b = extract_units(data_array2)\n    expected_a, expected_b = func(\n        strip_units(data_array1),\n        strip_units(convert_units(data_array2, units_a)),\n        **stripped_kwargs,\n    )\n    expected_a = attach_units(expected_a, units_a)\n    if isinstance(array2, Quantity):\n        expected_b = convert_units(attach_units(expected_b, units_a), units_b)\n    else:\n        expected_b = attach_units(expected_b, units_b)\n\n    actual_a, actual_b = func(data_array1, data_array2)\n\n    assert_units_equal(expected_a, actual_a)\n    assert_allclose(expected_a, actual_a)\n    assert_units_equal(expected_b, actual_b)\n    assert_allclose(expected_b, actual_b)\n\n\n@pytest.mark.parametrize(\n    ""unit,error"",\n    (\n        pytest.param(1, DimensionalityError, id=""no_unit""),\n        pytest.param(\n            unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n        ),\n        pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n        pytest.param(unit_registry.mm, None, id=""compatible_unit""),\n        pytest.param(unit_registry.m, None, id=""identical_unit""),\n    ),\n    ids=repr,\n)\n@pytest.mark.parametrize(\n    ""variant"",\n    (\n        ""data"",\n        pytest.param(""dims"", marks=pytest.mark.xfail(reason=""indexes strip units"")),\n        ""coords"",\n    ),\n)\n@pytest.mark.parametrize(""fill_value"", (np.float64(10), np.float64(np.nan)))\ndef test_align_dataset(fill_value, unit, variant, error, dtype):\n    original_unit = unit_registry.m\n\n    variants = {\n        ""data"": (unit, original_unit, original_unit),\n        ""dims"": (original_unit, unit, original_unit),\n        ""coords"": (original_unit, original_unit, unit),\n    }\n    data_unit, dim_unit, coord_unit = variants.get(variant)\n\n    array1 = np.linspace(0, 10, 2 * 5).reshape(2, 5).astype(dtype) * original_unit\n    array2 = np.linspace(0, 10, 2 * 5).reshape(2, 5).astype(dtype) * data_unit\n\n    x = np.arange(2) * original_unit\n\n    y1 = np.arange(5) * original_unit\n    y2 = np.arange(2, 7) * dim_unit\n    y_a1 = np.array([3, 5, 7, 8, 9]) * original_unit\n    y_a2 = np.array([7, 8, 9, 11, 13]) * coord_unit\n\n    coords1 = {""x"": x, ""y"": y1}\n    coords2 = {""x"": x, ""y"": y2}\n    if variant == ""coords"":\n        coords1[""y_a""] = (""y"", y_a1)\n        coords2[""y_a""] = (""y"", y_a2)\n\n    ds1 = xr.Dataset(data_vars={""a"": ((""x"", ""y""), array1)}, coords=coords1)\n    ds2 = xr.Dataset(data_vars={""a"": ((""x"", ""y""), array2)}, coords=coords2)\n\n    fill_value = fill_value * data_unit\n    func = function(xr.align, join=""outer"", fill_value=fill_value)\n    if error is not None and not (\n        np.isnan(fill_value) and not isinstance(fill_value, Quantity)\n    ):\n        with pytest.raises(error):\n            func(ds1, ds2)\n\n        return\n\n    stripped_kwargs = {\n        key: strip_units(\n            convert_units(value, {None: original_unit if data_unit != 1 else None})\n        )\n        for key, value in func.kwargs.items()\n    }\n    units_a = extract_units(ds1)\n    units_b = extract_units(ds2)\n    expected_a, expected_b = func(\n        strip_units(ds1), strip_units(convert_units(ds2, units_a)), **stripped_kwargs\n    )\n    expected_a = attach_units(expected_a, units_a)\n    if isinstance(array2, Quantity):\n        expected_b = convert_units(attach_units(expected_b, units_a), units_b)\n    else:\n        expected_b = attach_units(expected_b, units_b)\n\n    actual_a, actual_b = func(ds1, ds2)\n\n    assert_units_equal(expected_a, actual_a)\n    assert_allclose(expected_a, actual_a)\n    assert_units_equal(expected_b, actual_b)\n    assert_allclose(expected_b, actual_b)\n\n\ndef test_broadcast_dataarray(dtype):\n    array1 = np.linspace(0, 10, 2) * unit_registry.Pa\n    array2 = np.linspace(0, 10, 3) * unit_registry.Pa\n\n    a = xr.DataArray(data=array1, dims=""x"")\n    b = xr.DataArray(data=array2, dims=""y"")\n\n    units_a = extract_units(a)\n    units_b = extract_units(b)\n    expected_a, expected_b = xr.broadcast(strip_units(a), strip_units(b))\n    expected_a = attach_units(expected_a, units_a)\n    expected_b = convert_units(attach_units(expected_b, units_a), units_b)\n\n    actual_a, actual_b = xr.broadcast(a, b)\n\n    assert_units_equal(expected_a, actual_a)\n    assert_identical(expected_a, actual_a)\n    assert_units_equal(expected_b, actual_b)\n    assert_identical(expected_b, actual_b)\n\n\ndef test_broadcast_dataset(dtype):\n    array1 = np.linspace(0, 10, 2) * unit_registry.Pa\n    array2 = np.linspace(0, 10, 3) * unit_registry.Pa\n\n    x1 = np.arange(2)\n    y1 = np.arange(3)\n\n    x2 = np.arange(2, 4)\n    y2 = np.arange(3, 6)\n\n    ds = xr.Dataset(\n        data_vars={""a"": (""x"", array1), ""b"": (""y"", array2)}, coords={""x"": x1, ""y"": y1}\n    )\n    other = xr.Dataset(\n        data_vars={\n            ""a"": (""x"", array1.to(unit_registry.hPa)),\n            ""b"": (""y"", array2.to(unit_registry.hPa)),\n        },\n        coords={""x"": x2, ""y"": y2},\n    )\n\n    units_a = extract_units(ds)\n    units_b = extract_units(other)\n    expected_a, expected_b = xr.broadcast(strip_units(ds), strip_units(other))\n    expected_a = attach_units(expected_a, units_a)\n    expected_b = attach_units(expected_b, units_b)\n\n    actual_a, actual_b = xr.broadcast(ds, other)\n\n    assert_units_equal(expected_a, actual_a)\n    assert_identical(expected_a, actual_a)\n    assert_units_equal(expected_b, actual_b)\n    assert_identical(expected_b, actual_b)\n\n\n@pytest.mark.parametrize(\n    ""unit,error"",\n    (\n        pytest.param(1, DimensionalityError, id=""no_unit""),\n        pytest.param(\n            unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n        ),\n        pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n        pytest.param(unit_registry.mm, None, id=""compatible_unit""),\n        pytest.param(unit_registry.m, None, id=""identical_unit""),\n    ),\n    ids=repr,\n)\n@pytest.mark.parametrize(\n    ""variant"",\n    (\n        ""data"",\n        pytest.param(""dims"", marks=pytest.mark.xfail(reason=""indexes strip units"")),\n        ""coords"",\n    ),\n)\ndef test_combine_by_coords(variant, unit, error, dtype):\n    original_unit = unit_registry.m\n\n    variants = {\n        ""data"": (unit, original_unit, original_unit),\n        ""dims"": (original_unit, unit, original_unit),\n        ""coords"": (original_unit, original_unit, unit),\n    }\n    data_unit, dim_unit, coord_unit = variants.get(variant)\n\n    array1 = np.zeros(shape=(2, 3), dtype=dtype) * original_unit\n    array2 = np.zeros(shape=(2, 3), dtype=dtype) * original_unit\n    x = np.arange(1, 4) * 10 * original_unit\n    y = np.arange(2) * original_unit\n    z = np.arange(3) * original_unit\n\n    other_array1 = np.ones_like(array1) * data_unit\n    other_array2 = np.ones_like(array2) * data_unit\n    other_x = np.arange(1, 4) * 10 * dim_unit\n    other_y = np.arange(2, 4) * dim_unit\n    other_z = np.arange(3, 6) * coord_unit\n\n    ds = xr.Dataset(\n        data_vars={""a"": ((""y"", ""x""), array1), ""b"": ((""y"", ""x""), array2)},\n        coords={""x"": x, ""y"": y, ""z"": (""x"", z)},\n    )\n    other = xr.Dataset(\n        data_vars={""a"": ((""y"", ""x""), other_array1), ""b"": ((""y"", ""x""), other_array2)},\n        coords={""x"": other_x, ""y"": other_y, ""z"": (""x"", other_z)},\n    )\n\n    if error is not None:\n        with pytest.raises(error):\n            xr.combine_by_coords([ds, other])\n\n        return\n\n    units = extract_units(ds)\n    expected = attach_units(\n        xr.combine_by_coords(\n            [strip_units(ds), strip_units(convert_units(other, units))]\n        ),\n        units,\n    )\n    actual = xr.combine_by_coords([ds, other])\n\n    assert_units_equal(expected, actual)\n    assert_identical(expected, actual)\n\n\n@pytest.mark.parametrize(\n    ""unit,error"",\n    (\n        pytest.param(1, DimensionalityError, id=""no_unit""),\n        pytest.param(\n            unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n        ),\n        pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n        pytest.param(unit_registry.mm, None, id=""compatible_unit""),\n        pytest.param(unit_registry.m, None, id=""identical_unit""),\n    ),\n    ids=repr,\n)\n@pytest.mark.parametrize(\n    ""variant"",\n    (\n        ""data"",\n        pytest.param(""dims"", marks=pytest.mark.xfail(reason=""indexes strip units"")),\n        ""coords"",\n    ),\n)\ndef test_combine_nested(variant, unit, error, dtype):\n    original_unit = unit_registry.m\n\n    variants = {\n        ""data"": (unit, original_unit, original_unit),\n        ""dims"": (original_unit, unit, original_unit),\n        ""coords"": (original_unit, original_unit, unit),\n    }\n    data_unit, dim_unit, coord_unit = variants.get(variant)\n\n    array1 = np.zeros(shape=(2, 3), dtype=dtype) * original_unit\n    array2 = np.zeros(shape=(2, 3), dtype=dtype) * original_unit\n\n    x = np.arange(1, 4) * 10 * original_unit\n    y = np.arange(2) * original_unit\n    z = np.arange(3) * original_unit\n\n    ds1 = xr.Dataset(\n        data_vars={""a"": ((""y"", ""x""), array1), ""b"": ((""y"", ""x""), array2)},\n        coords={""x"": x, ""y"": y, ""z"": (""x"", z)},\n    )\n    ds2 = xr.Dataset(\n        data_vars={\n            ""a"": ((""y"", ""x""), np.ones_like(array1) * data_unit),\n            ""b"": ((""y"", ""x""), np.ones_like(array2) * data_unit),\n        },\n        coords={\n            ""x"": np.arange(3) * dim_unit,\n            ""y"": np.arange(2, 4) * dim_unit,\n            ""z"": (""x"", np.arange(-3, 0) * coord_unit),\n        },\n    )\n    ds3 = xr.Dataset(\n        data_vars={\n            ""a"": ((""y"", ""x""), np.zeros_like(array1) * np.nan * data_unit),\n            ""b"": ((""y"", ""x""), np.zeros_like(array2) * np.nan * data_unit),\n        },\n        coords={\n            ""x"": np.arange(3, 6) * dim_unit,\n            ""y"": np.arange(4, 6) * dim_unit,\n            ""z"": (""x"", np.arange(3, 6) * coord_unit),\n        },\n    )\n    ds4 = xr.Dataset(\n        data_vars={\n            ""a"": ((""y"", ""x""), -1 * np.ones_like(array1) * data_unit),\n            ""b"": ((""y"", ""x""), -1 * np.ones_like(array2) * data_unit),\n        },\n        coords={\n            ""x"": np.arange(6, 9) * dim_unit,\n            ""y"": np.arange(6, 8) * dim_unit,\n            ""z"": (""x"", np.arange(6, 9) * coord_unit),\n        },\n    )\n\n    func = function(xr.combine_nested, concat_dim=[""x"", ""y""])\n    if error is not None:\n        with pytest.raises(error):\n            func([[ds1, ds2], [ds3, ds4]])\n\n        return\n\n    units = extract_units(ds1)\n    convert_and_strip = lambda ds: strip_units(convert_units(ds, units))\n    expected = attach_units(\n        func(\n            [\n                [strip_units(ds1), convert_and_strip(ds2)],\n                [convert_and_strip(ds3), convert_and_strip(ds4)],\n            ]\n        ),\n        units,\n    )\n    actual = func([[ds1, ds2], [ds3, ds4]])\n\n    assert_units_equal(expected, actual)\n    assert_identical(expected, actual)\n\n\n@pytest.mark.parametrize(\n    ""unit,error"",\n    (\n        pytest.param(1, DimensionalityError, id=""no_unit""),\n        pytest.param(\n            unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n        ),\n        pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n        pytest.param(unit_registry.mm, None, id=""compatible_unit""),\n        pytest.param(unit_registry.m, None, id=""identical_unit""),\n    ),\n    ids=repr,\n)\n@pytest.mark.parametrize(\n    ""variant"",\n    (\n        ""data"",\n        pytest.param(""dims"", marks=pytest.mark.xfail(reason=""indexes strip units"")),\n    ),\n)\ndef test_concat_dataarray(variant, unit, error, dtype):\n    original_unit = unit_registry.m\n\n    variants = {""data"": (unit, original_unit), ""dims"": (original_unit, unit)}\n    data_unit, dims_unit = variants.get(variant)\n\n    array1 = np.linspace(0, 5, 10).astype(dtype) * unit_registry.m\n    array2 = np.linspace(-5, 0, 5).astype(dtype) * data_unit\n    x1 = np.arange(5, 15) * original_unit\n    x2 = np.arange(5) * dims_unit\n\n    arr1 = xr.DataArray(data=array1, coords={""x"": x1}, dims=""x"")\n    arr2 = xr.DataArray(data=array2, coords={""x"": x2}, dims=""x"")\n\n    if error is not None:\n        with pytest.raises(error):\n            xr.concat([arr1, arr2], dim=""x"")\n\n        return\n\n    units = extract_units(arr1)\n    expected = attach_units(\n        xr.concat(\n            [strip_units(arr1), strip_units(convert_units(arr2, units))], dim=""x""\n        ),\n        units,\n    )\n    actual = xr.concat([arr1, arr2], dim=""x"")\n\n    assert_units_equal(expected, actual)\n    assert_identical(expected, actual)\n\n\n@pytest.mark.parametrize(\n    ""unit,error"",\n    (\n        pytest.param(1, DimensionalityError, id=""no_unit""),\n        pytest.param(\n            unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n        ),\n        pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n        pytest.param(unit_registry.mm, None, id=""compatible_unit""),\n        pytest.param(unit_registry.m, None, id=""identical_unit""),\n    ),\n    ids=repr,\n)\n@pytest.mark.parametrize(\n    ""variant"",\n    (\n        ""data"",\n        pytest.param(""dims"", marks=pytest.mark.xfail(reason=""indexes strip units"")),\n    ),\n)\ndef test_concat_dataset(variant, unit, error, dtype):\n    original_unit = unit_registry.m\n\n    variants = {""data"": (unit, original_unit), ""dims"": (original_unit, unit)}\n    data_unit, dims_unit = variants.get(variant)\n\n    array1 = np.linspace(0, 5, 10).astype(dtype) * unit_registry.m\n    array2 = np.linspace(-5, 0, 5).astype(dtype) * data_unit\n    x1 = np.arange(5, 15) * original_unit\n    x2 = np.arange(5) * dims_unit\n\n    ds1 = xr.Dataset(data_vars={""a"": (""x"", array1)}, coords={""x"": x1})\n    ds2 = xr.Dataset(data_vars={""a"": (""x"", array2)}, coords={""x"": x2})\n\n    if error is not None:\n        with pytest.raises(error):\n            xr.concat([ds1, ds2], dim=""x"")\n\n        return\n\n    units = extract_units(ds1)\n    expected = attach_units(\n        xr.concat([strip_units(ds1), strip_units(convert_units(ds2, units))], dim=""x""),\n        units,\n    )\n    actual = xr.concat([ds1, ds2], dim=""x"")\n\n    assert_units_equal(expected, actual)\n    assert_identical(expected, actual)\n\n\n@pytest.mark.parametrize(\n    ""unit,error"",\n    (\n        pytest.param(1, DimensionalityError, id=""no_unit""),\n        pytest.param(\n            unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n        ),\n        pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n        pytest.param(unit_registry.mm, None, id=""compatible_unit""),\n        pytest.param(unit_registry.m, None, id=""identical_unit""),\n    ),\n    ids=repr,\n)\n@pytest.mark.parametrize(\n    ""variant"",\n    (\n        ""data"",\n        pytest.param(""dims"", marks=pytest.mark.xfail(reason=""indexes strip units"")),\n        ""coords"",\n    ),\n)\ndef test_merge_dataarray(variant, unit, error, dtype):\n    original_unit = unit_registry.m\n\n    variants = {\n        ""data"": (unit, original_unit, original_unit),\n        ""dims"": (original_unit, unit, original_unit),\n        ""coords"": (original_unit, original_unit, unit),\n    }\n    data_unit, dim_unit, coord_unit = variants.get(variant)\n\n    array1 = np.linspace(0, 1, 2 * 3).reshape(2, 3).astype(dtype) * original_unit\n    x1 = np.arange(2) * original_unit\n    y1 = np.arange(3) * original_unit\n    u1 = np.linspace(10, 20, 2) * original_unit\n    v1 = np.linspace(10, 20, 3) * original_unit\n\n    array2 = np.linspace(1, 2, 2 * 4).reshape(2, 4).astype(dtype) * data_unit\n    x2 = np.arange(2, 4) * dim_unit\n    z2 = np.arange(4) * original_unit\n    u2 = np.linspace(20, 30, 2) * coord_unit\n    w2 = np.linspace(10, 20, 4) * original_unit\n\n    array3 = np.linspace(0, 2, 3 * 4).reshape(3, 4).astype(dtype) * data_unit\n    y3 = np.arange(3, 6) * dim_unit\n    z3 = np.arange(4, 8) * dim_unit\n    v3 = np.linspace(10, 20, 3) * coord_unit\n    w3 = np.linspace(10, 20, 4) * coord_unit\n\n    arr1 = xr.DataArray(\n        name=""a"",\n        data=array1,\n        coords={""x"": x1, ""y"": y1, ""u"": (""x"", u1), ""v"": (""y"", v1)},\n        dims=(""x"", ""y""),\n    )\n    arr2 = xr.DataArray(\n        name=""a"",\n        data=array2,\n        coords={""x"": x2, ""z"": z2, ""u"": (""x"", u2), ""w"": (""z"", w2)},\n        dims=(""x"", ""z""),\n    )\n    arr3 = xr.DataArray(\n        name=""a"",\n        data=array3,\n        coords={""y"": y3, ""z"": z3, ""v"": (""y"", v3), ""w"": (""z"", w3)},\n        dims=(""y"", ""z""),\n    )\n\n    if error is not None:\n        with pytest.raises(error):\n            xr.merge([arr1, arr2, arr3])\n\n        return\n\n    units = {name: original_unit for name in list(""axyzuvw"")}\n\n    convert_and_strip = lambda arr: strip_units(convert_units(arr, units))\n    expected_units = {\n        ""a"": original_unit,\n        ""u"": original_unit,\n        ""v"": original_unit,\n        ""w"": original_unit,\n        ""x"": original_unit,\n        ""y"": original_unit,\n        ""z"": original_unit,\n    }\n\n    expected = convert_units(\n        attach_units(\n            xr.merge(\n                [\n                    convert_and_strip(arr1),\n                    convert_and_strip(arr2),\n                    convert_and_strip(arr3),\n                ]\n            ),\n            units,\n        ),\n        expected_units,\n    )\n\n    actual = xr.merge([arr1, arr2, arr3])\n\n    assert_units_equal(expected, actual)\n    assert_allclose(expected, actual)\n\n\n@pytest.mark.parametrize(\n    ""unit,error"",\n    (\n        pytest.param(1, DimensionalityError, id=""no_unit""),\n        pytest.param(\n            unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n        ),\n        pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n        pytest.param(unit_registry.mm, None, id=""compatible_unit""),\n        pytest.param(unit_registry.m, None, id=""identical_unit""),\n    ),\n    ids=repr,\n)\n@pytest.mark.parametrize(\n    ""variant"",\n    (\n        ""data"",\n        pytest.param(""dims"", marks=pytest.mark.xfail(reason=""indexes strip units"")),\n        ""coords"",\n    ),\n)\ndef test_merge_dataset(variant, unit, error, dtype):\n    original_unit = unit_registry.m\n\n    variants = {\n        ""data"": (unit, original_unit, original_unit),\n        ""dims"": (original_unit, unit, original_unit),\n        ""coords"": (original_unit, original_unit, unit),\n    }\n    data_unit, dim_unit, coord_unit = variants.get(variant)\n\n    array1 = np.zeros(shape=(2, 3), dtype=dtype) * original_unit\n    array2 = np.zeros(shape=(2, 3), dtype=dtype) * original_unit\n\n    x = np.arange(11, 14) * original_unit\n    y = np.arange(2) * original_unit\n    z = np.arange(3) * original_unit\n\n    ds1 = xr.Dataset(\n        data_vars={""a"": ((""y"", ""x""), array1), ""b"": ((""y"", ""x""), array2)},\n        coords={""x"": x, ""y"": y, ""u"": (""x"", z)},\n    )\n    ds2 = xr.Dataset(\n        data_vars={\n            ""a"": ((""y"", ""x""), np.ones_like(array1) * data_unit),\n            ""b"": ((""y"", ""x""), np.ones_like(array2) * data_unit),\n        },\n        coords={\n            ""x"": np.arange(3) * dim_unit,\n            ""y"": np.arange(2, 4) * dim_unit,\n            ""u"": (""x"", np.arange(-3, 0) * coord_unit),\n        },\n    )\n    ds3 = xr.Dataset(\n        data_vars={\n            ""a"": ((""y"", ""x""), np.full_like(array1, np.nan) * data_unit),\n            ""b"": ((""y"", ""x""), np.full_like(array2, np.nan) * data_unit),\n        },\n        coords={\n            ""x"": np.arange(3, 6) * dim_unit,\n            ""y"": np.arange(4, 6) * dim_unit,\n            ""u"": (""x"", np.arange(3, 6) * coord_unit),\n        },\n    )\n\n    func = function(xr.merge)\n    if error is not None:\n        with pytest.raises(error):\n            func([ds1, ds2, ds3])\n\n        return\n\n    units = extract_units(ds1)\n    convert_and_strip = lambda ds: strip_units(convert_units(ds, units))\n    expected_units = {name: original_unit for name in list(""abxyzu"")}\n    expected = convert_units(\n        attach_units(\n            func(\n                [convert_and_strip(ds1), convert_and_strip(ds2), convert_and_strip(ds3)]\n            ),\n            units,\n        ),\n        expected_units,\n    )\n    actual = func([ds1, ds2, ds3])\n\n    assert_units_equal(expected, actual)\n    assert_allclose(expected, actual)\n\n\n@pytest.mark.parametrize(""func"", (xr.zeros_like, xr.ones_like))\ndef test_replication_dataarray(func, dtype):\n    array = np.linspace(0, 10, 20).astype(dtype) * unit_registry.s\n    data_array = xr.DataArray(data=array, dims=""x"")\n\n    numpy_func = getattr(np, func.__name__)\n    units = extract_units(numpy_func(data_array))\n    expected = attach_units(func(data_array), units)\n    actual = func(data_array)\n\n    assert_units_equal(expected, actual)\n    assert_identical(expected, actual)\n\n\n@pytest.mark.parametrize(""func"", (xr.zeros_like, xr.ones_like))\ndef test_replication_dataset(func, dtype):\n    array1 = np.linspace(0, 10, 20).astype(dtype) * unit_registry.s\n    array2 = np.linspace(5, 10, 10).astype(dtype) * unit_registry.Pa\n    x = np.arange(20).astype(dtype) * unit_registry.m\n    y = np.arange(10).astype(dtype) * unit_registry.m\n    z = y.to(unit_registry.mm)\n\n    ds = xr.Dataset(\n        data_vars={""a"": (""x"", array1), ""b"": (""y"", array2)},\n        coords={""x"": x, ""y"": y, ""z"": (""y"", z)},\n    )\n\n    numpy_func = getattr(np, func.__name__)\n    units = extract_units(ds.map(numpy_func))\n    expected = attach_units(func(strip_units(ds)), units)\n\n    actual = func(ds)\n\n    assert_units_equal(expected, actual)\n    assert_identical(expected, actual)\n\n\n@pytest.mark.xfail(\n    reason=(\n        ""pint is undecided on how `full_like` should work, so incorrect errors ""\n        ""may be expected: hgrecco/pint#882""\n    )\n)\n@pytest.mark.parametrize(\n    ""unit,error"",\n    (\n        pytest.param(1, DimensionalityError, id=""no_unit""),\n        pytest.param(\n            unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n        ),\n        pytest.param(unit_registry.m, DimensionalityError, id=""incompatible_unit""),\n        pytest.param(unit_registry.ms, None, id=""compatible_unit""),\n        pytest.param(unit_registry.s, None, id=""identical_unit""),\n    ),\n    ids=repr,\n)\ndef test_replication_full_like_dataarray(unit, error, dtype):\n    array = np.linspace(0, 5, 10) * unit_registry.s\n    data_array = xr.DataArray(data=array, dims=""x"")\n\n    fill_value = -1 * unit\n    if error is not None:\n        with pytest.raises(error):\n            xr.full_like(data_array, fill_value=fill_value)\n\n        return\n\n    units = {**extract_units(data_array), **{None: unit if unit != 1 else None}}\n    expected = attach_units(\n        xr.full_like(strip_units(data_array), fill_value=strip_units(fill_value)), units\n    )\n    actual = xr.full_like(data_array, fill_value=fill_value)\n\n    assert_units_equal(expected, actual)\n    assert_identical(expected, actual)\n\n\n@pytest.mark.xfail(\n    reason=(\n        ""pint is undecided on how `full_like` should work, so incorrect errors ""\n        ""may be expected: hgrecco/pint#882""\n    )\n)\n@pytest.mark.parametrize(\n    ""unit,error"",\n    (\n        pytest.param(1, DimensionalityError, id=""no_unit""),\n        pytest.param(\n            unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n        ),\n        pytest.param(unit_registry.m, DimensionalityError, id=""incompatible_unit""),\n        pytest.param(unit_registry.ms, None, id=""compatible_unit""),\n        pytest.param(unit_registry.s, None, id=""identical_unit""),\n    ),\n    ids=repr,\n)\ndef test_replication_full_like_dataset(unit, error, dtype):\n    array1 = np.linspace(0, 10, 20).astype(dtype) * unit_registry.s\n    array2 = np.linspace(5, 10, 10).astype(dtype) * unit_registry.Pa\n    x = np.arange(20).astype(dtype) * unit_registry.m\n    y = np.arange(10).astype(dtype) * unit_registry.m\n    z = y.to(unit_registry.mm)\n\n    ds = xr.Dataset(\n        data_vars={""a"": (""x"", array1), ""b"": (""y"", array2)},\n        coords={""x"": x, ""y"": y, ""z"": (""y"", z)},\n    )\n\n    fill_value = -1 * unit\n    if error is not None:\n        with pytest.raises(error):\n            xr.full_like(ds, fill_value=fill_value)\n\n        return\n\n    units = {\n        **extract_units(ds),\n        **{name: unit if unit != 1 else None for name in ds.data_vars},\n    }\n    expected = attach_units(\n        xr.full_like(strip_units(ds), fill_value=strip_units(fill_value)), units\n    )\n    actual = xr.full_like(ds, fill_value=fill_value)\n\n    assert_units_equal(expected, actual)\n    assert_identical(expected, actual)\n\n\n@pytest.mark.parametrize(\n    ""unit,error"",\n    (\n        pytest.param(1, DimensionalityError, id=""no_unit""),\n        pytest.param(\n            unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n        ),\n        pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n        pytest.param(unit_registry.mm, None, id=""compatible_unit""),\n        pytest.param(unit_registry.m, None, id=""identical_unit""),\n    ),\n    ids=repr,\n)\n@pytest.mark.parametrize(""fill_value"", (np.nan, 10.2))\ndef test_where_dataarray(fill_value, unit, error, dtype):\n    array = np.linspace(0, 5, 10).astype(dtype) * unit_registry.m\n\n    x = xr.DataArray(data=array, dims=""x"")\n    cond = x < 5 * unit_registry.m\n    fill_value = fill_value * unit\n\n    if error is not None and not (\n        np.isnan(fill_value) and not isinstance(fill_value, Quantity)\n    ):\n        with pytest.raises(error):\n            xr.where(cond, x, fill_value)\n\n        return\n\n    expected = attach_units(\n        xr.where(\n            cond,\n            strip_units(x),\n            strip_units(convert_units(fill_value, {None: unit_registry.m})),\n        ),\n        extract_units(x),\n    )\n    actual = xr.where(cond, x, fill_value)\n\n    assert_units_equal(expected, actual)\n    assert_identical(expected, actual)\n\n\n@pytest.mark.parametrize(\n    ""unit,error"",\n    (\n        pytest.param(1, DimensionalityError, id=""no_unit""),\n        pytest.param(\n            unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n        ),\n        pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n        pytest.param(unit_registry.mm, None, id=""compatible_unit""),\n        pytest.param(unit_registry.m, None, id=""identical_unit""),\n    ),\n    ids=repr,\n)\n@pytest.mark.parametrize(""fill_value"", (np.nan, 10.2))\ndef test_where_dataset(fill_value, unit, error, dtype):\n    array1 = np.linspace(0, 5, 10).astype(dtype) * unit_registry.m\n    array2 = np.linspace(-5, 0, 10).astype(dtype) * unit_registry.m\n    x = np.arange(10) * unit_registry.s\n\n    ds = xr.Dataset(data_vars={""a"": (""x"", array1), ""b"": (""x"", array2)}, coords={""x"": x})\n    cond = x < 5 * unit_registry.s\n    fill_value = fill_value * unit\n\n    if error is not None and not (\n        np.isnan(fill_value) and not isinstance(fill_value, Quantity)\n    ):\n        with pytest.raises(error):\n            xr.where(cond, ds, fill_value)\n\n        return\n\n    expected = attach_units(\n        xr.where(\n            cond,\n            strip_units(ds),\n            strip_units(convert_units(fill_value, {None: unit_registry.m})),\n        ),\n        extract_units(ds),\n    )\n    actual = xr.where(cond, ds, fill_value)\n\n    assert_units_equal(expected, actual)\n    assert_identical(expected, actual)\n\n\ndef test_dot_dataarray(dtype):\n    array1 = (\n        np.linspace(0, 10, 5 * 10).reshape(5, 10).astype(dtype)\n        * unit_registry.m\n        / unit_registry.s\n    )\n    array2 = (\n        np.linspace(10, 20, 10 * 20).reshape(10, 20).astype(dtype) * unit_registry.s\n    )\n\n    data_array = xr.DataArray(data=array1, dims=(""x"", ""y""))\n    other = xr.DataArray(data=array2, dims=(""y"", ""z""))\n\n    expected = attach_units(\n        xr.dot(strip_units(data_array), strip_units(other)), {None: unit_registry.m}\n    )\n    actual = xr.dot(data_array, other)\n\n    assert_units_equal(expected, actual)\n    assert_identical(expected, actual)\n\n\ndef delete_attrs(*to_delete):\n    def wrapper(cls):\n        for item in to_delete:\n            setattr(cls, item, None)\n\n        return cls\n\n    return wrapper\n\n\n@delete_attrs(\n    ""test_getitem_with_mask"",\n    ""test_getitem_with_mask_nd_indexer"",\n    ""test_index_0d_string"",\n    ""test_index_0d_datetime"",\n    ""test_index_0d_timedelta64"",\n    ""test_0d_time_data"",\n    ""test_index_0d_not_a_time"",\n    ""test_datetime64_conversion"",\n    ""test_timedelta64_conversion"",\n    ""test_pandas_period_index"",\n    ""test_1d_math"",\n    ""test_1d_reduce"",\n    ""test_array_interface"",\n    ""test___array__"",\n    ""test_copy_index"",\n    ""test_concat_number_strings"",\n    ""test_concat_fixed_len_str"",\n    ""test_concat_mixed_dtypes"",\n    ""test_pandas_datetime64_with_tz"",\n    ""test_pandas_data"",\n    ""test_multiindex"",\n)\nclass TestVariable(VariableSubclassobjects):\n    @staticmethod\n    def cls(dims, data, *args, **kwargs):\n        return xr.Variable(\n            dims, unit_registry.Quantity(data, unit_registry.m), *args, **kwargs\n        )\n\n    def example_1d_objects(self):\n        for data in [\n            range(3),\n            0.5 * np.arange(3),\n            0.5 * np.arange(3, dtype=np.float32),\n            np.array([""a"", ""b"", ""c""], dtype=object),\n        ]:\n            yield (self.cls(""x"", data), data)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            method(""all""),\n            method(""any""),\n            method(""argmax""),\n            method(""argmin""),\n            method(""argsort""),\n            method(""cumprod""),\n            method(""cumsum""),\n            method(""max""),\n            method(""mean""),\n            method(""median""),\n            method(""min""),\n            pytest.param(\n                method(""prod""),\n                marks=pytest.mark.xfail(reason=""not implemented by pint""),\n            ),\n            method(""std""),\n            method(""sum""),\n            method(""var""),\n        ),\n        ids=repr,\n    )\n    def test_aggregation(self, func, dtype):\n        array = np.linspace(0, 1, 10).astype(dtype) * (\n            unit_registry.m if func.name != ""cumprod"" else unit_registry.dimensionless\n        )\n        variable = xr.Variable(""x"", array)\n\n        units = extract_units(func(array))\n        expected = attach_units(func(strip_units(variable)), units)\n        actual = func(variable)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            method(""astype"", np.float32),\n            method(""conj""),\n            method(""conjugate""),\n            method(""clip"", min=2, max=7),\n        ),\n        ids=repr,\n    )\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_numpy_methods(self, func, unit, error, dtype):\n        array = np.linspace(0, 1, 10).astype(dtype) * unit_registry.m\n        variable = xr.Variable(""x"", array)\n\n        args = [\n            item * unit if isinstance(item, (int, float, list)) else item\n            for item in func.args\n        ]\n        kwargs = {\n            key: value * unit if isinstance(value, (int, float, list)) else value\n            for key, value in func.kwargs.items()\n        }\n\n        if error is not None and func.name in (""searchsorted"", ""clip""):\n            with pytest.raises(error):\n                func(variable, *args, **kwargs)\n\n            return\n\n        converted_args = [\n            strip_units(convert_units(item, {None: unit_registry.m})) for item in args\n        ]\n        converted_kwargs = {\n            key: strip_units(convert_units(value, {None: unit_registry.m}))\n            for key, value in kwargs.items()\n        }\n\n        units = extract_units(func(array, *args, **kwargs))\n        expected = attach_units(\n            func(strip_units(variable), *converted_args, **converted_kwargs), units\n        )\n        actual = func(variable, *args, **kwargs)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_allclose(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"", (method(""item"", 5), method(""searchsorted"", 5)), ids=repr\n    )\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_raw_numpy_methods(self, func, unit, error, dtype):\n        array = np.linspace(0, 1, 10).astype(dtype) * unit_registry.m\n        variable = xr.Variable(""x"", array)\n\n        args = [\n            item * unit\n            if isinstance(item, (int, float, list)) and func.name != ""item""\n            else item\n            for item in func.args\n        ]\n        kwargs = {\n            key: value * unit\n            if isinstance(value, (int, float, list)) and func.name != ""item""\n            else value\n            for key, value in func.kwargs.items()\n        }\n\n        if error is not None and func.name != ""item"":\n            with pytest.raises(error):\n                func(variable, *args, **kwargs)\n\n            return\n\n        converted_args = [\n            strip_units(convert_units(item, {None: unit_registry.m}))\n            if func.name != ""item""\n            else item\n            for item in args\n        ]\n        converted_kwargs = {\n            key: strip_units(convert_units(value, {None: unit_registry.m}))\n            if func.name != ""item""\n            else value\n            for key, value in kwargs.items()\n        }\n\n        units = extract_units(func(array, *args, **kwargs))\n        expected = attach_units(\n            func(strip_units(variable), *converted_args, **converted_kwargs), units\n        )\n        actual = func(variable, *args, **kwargs)\n\n        assert_units_equal(expected, actual)\n        np.testing.assert_allclose(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"", (method(""isnull""), method(""notnull""), method(""count"")), ids=repr\n    )\n    def test_missing_value_detection(self, func):\n        array = (\n            np.array(\n                [\n                    [1.4, 2.3, np.nan, 7.2],\n                    [np.nan, 9.7, np.nan, np.nan],\n                    [2.1, np.nan, np.nan, 4.6],\n                    [9.9, np.nan, 7.2, 9.1],\n                ]\n            )\n            * unit_registry.degK\n        )\n        variable = xr.Variable((""x"", ""y""), array)\n\n        expected = func(strip_units(variable))\n        actual = func(variable)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_missing_value_fillna(self, unit, error):\n        value = 10\n        array = (\n            np.array(\n                [\n                    [1.4, 2.3, np.nan, 7.2],\n                    [np.nan, 9.7, np.nan, np.nan],\n                    [2.1, np.nan, np.nan, 4.6],\n                    [9.9, np.nan, 7.2, 9.1],\n                ]\n            )\n            * unit_registry.m\n        )\n        variable = xr.Variable((""x"", ""y""), array)\n\n        fill_value = value * unit\n\n        if error is not None:\n            with pytest.raises(error):\n                variable.fillna(value=fill_value)\n\n            return\n\n        expected = attach_units(\n            strip_units(variable).fillna(\n                value=fill_value.to(unit_registry.m).magnitude\n            ),\n            extract_units(variable),\n        )\n        actual = variable.fillna(value=fill_value)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""unit"",\n        (\n            pytest.param(1, id=""no_unit""),\n            pytest.param(unit_registry.dimensionless, id=""dimensionless""),\n            pytest.param(unit_registry.s, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, id=""compatible_unit"",),\n            pytest.param(unit_registry.m, id=""identical_unit""),\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""convert_data"",\n        (\n            pytest.param(False, id=""no_conversion""),\n            pytest.param(True, id=""with_conversion""),\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            method(""equals""),\n            pytest.param(\n                method(""identical""),\n                marks=pytest.mark.skip(reason=""behavior of identical is undecided""),\n            ),\n        ),\n        ids=repr,\n    )\n    def test_comparisons(self, func, unit, convert_data, dtype):\n        array = np.linspace(0, 1, 9).astype(dtype)\n        quantity1 = array * unit_registry.m\n        variable = xr.Variable(""x"", quantity1)\n\n        if convert_data and is_compatible(unit_registry.m, unit):\n            quantity2 = convert_units(array * unit_registry.m, {None: unit})\n        else:\n            quantity2 = array * unit\n        other = xr.Variable(""x"", quantity2)\n\n        expected = func(\n            strip_units(variable),\n            strip_units(\n                convert_units(other, extract_units(variable))\n                if is_compatible(unit_registry.m, unit)\n                else other\n            ),\n        )\n        if func.name == ""identical"":\n            expected &= extract_units(variable) == extract_units(other)\n        else:\n            expected &= all(\n                compatible_mappings(\n                    extract_units(variable), extract_units(other)\n                ).values()\n            )\n\n        actual = func(variable, other)\n\n        assert expected == actual\n\n    @pytest.mark.parametrize(\n        ""unit"",\n        (\n            pytest.param(1, id=""no_unit""),\n            pytest.param(unit_registry.dimensionless, id=""dimensionless""),\n            pytest.param(unit_registry.s, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, id=""compatible_unit""),\n            pytest.param(unit_registry.m, id=""identical_unit""),\n        ),\n    )\n    def test_broadcast_equals(self, unit, dtype):\n        base_unit = unit_registry.m\n        left_array = np.ones(shape=(2, 2), dtype=dtype) * base_unit\n        value = (\n            (1 * base_unit).to(unit).magnitude if is_compatible(unit, base_unit) else 1\n        )\n        right_array = np.full(shape=(2,), fill_value=value, dtype=dtype) * unit\n\n        left = xr.Variable((""x"", ""y""), left_array)\n        right = xr.Variable(""x"", right_array)\n\n        units = {\n            **extract_units(left),\n            **({} if is_compatible(unit, base_unit) else {None: None}),\n        }\n        expected = strip_units(left).broadcast_equals(\n            strip_units(convert_units(right, units))\n        ) & is_compatible(unit, base_unit)\n        actual = left.broadcast_equals(right)\n\n        assert expected == actual\n\n    @pytest.mark.parametrize(\n        ""indices"",\n        (\n            pytest.param(4, id=""single index""),\n            pytest.param([5, 2, 9, 1], id=""multiple indices""),\n        ),\n    )\n    def test_isel(self, indices, dtype):\n        array = np.linspace(0, 5, 10).astype(dtype) * unit_registry.s\n        variable = xr.Variable(""x"", array)\n\n        expected = attach_units(\n            strip_units(variable).isel(x=indices), extract_units(variable)\n        )\n        actual = variable.isel(x=indices)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            function(lambda x, *_: +x, function_label=""unary_plus""),\n            function(lambda x, *_: -x, function_label=""unary_minus""),\n            function(lambda x, *_: abs(x), function_label=""absolute""),\n            function(lambda x, y: x + y, function_label=""sum""),\n            function(lambda x, y: y + x, function_label=""commutative_sum""),\n            function(lambda x, y: x * y, function_label=""product""),\n            function(lambda x, y: y * x, function_label=""commutative_product""),\n        ),\n        ids=repr,\n    )\n    def test_1d_math(self, func, unit, error, dtype):\n        base_unit = unit_registry.m\n        array = np.arange(5).astype(dtype) * base_unit\n        variable = xr.Variable(""x"", array)\n\n        values = np.ones(5)\n        y = values * unit\n\n        if error is not None and func.name in (""sum"", ""commutative_sum""):\n            with pytest.raises(error):\n                func(variable, y)\n\n            return\n\n        units = extract_units(func(array, y))\n        if all(compatible_mappings(units, extract_units(y)).values()):\n            converted_y = convert_units(y, units)\n        else:\n            converted_y = y\n\n        if all(compatible_mappings(units, extract_units(variable)).values()):\n            converted_variable = convert_units(variable, units)\n        else:\n            converted_variable = variable\n\n        expected = attach_units(\n            func(strip_units(converted_variable), strip_units(converted_y)), units\n        )\n        actual = func(variable, y)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_allclose(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""func"", (method(""where""), method(""_getitem_with_mask"")), ids=repr\n    )\n    def test_masking(self, func, unit, error, dtype):\n        base_unit = unit_registry.m\n        array = np.linspace(0, 5, 10).astype(dtype) * base_unit\n        variable = xr.Variable(""x"", array)\n        cond = np.array([True, False] * 5)\n\n        other = -1 * unit\n\n        if error is not None:\n            with pytest.raises(error):\n                func(variable, cond, other)\n\n            return\n\n        expected = attach_units(\n            func(\n                strip_units(variable),\n                cond,\n                strip_units(\n                    convert_units(\n                        other,\n                        {None: base_unit}\n                        if is_compatible(base_unit, unit)\n                        else {None: None},\n                    )\n                ),\n            ),\n            extract_units(variable),\n        )\n        actual = func(variable, cond, other)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    def test_squeeze(self, dtype):\n        shape = (2, 1, 3, 1, 1, 2)\n        names = list(""abcdef"")\n        array = np.ones(shape=shape) * unit_registry.m\n        variable = xr.Variable(names, array)\n\n        expected = attach_units(\n            strip_units(variable).squeeze(), extract_units(variable)\n        )\n        actual = variable.squeeze()\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n        names = tuple(name for name, size in zip(names, shape) if shape == 1)\n        for name in names:\n            expected = attach_units(\n                strip_units(variable).squeeze(dim=name), extract_units(variable)\n            )\n            actual = variable.squeeze(dim=name)\n\n            assert_units_equal(expected, actual)\n            xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            method(""coarsen"", windows={""y"": 2}, func=np.mean),\n            pytest.param(\n                method(""quantile"", q=[0.25, 0.75]),\n                marks=pytest.mark.xfail(\n                    LooseVersion(pint.__version__) < ""0.12"",\n                    reason=""quantile / nanquantile not implemented yet"",\n                ),\n            ),\n            pytest.param(\n                method(""rank"", dim=""x""),\n                marks=pytest.mark.xfail(reason=""rank not implemented for non-ndarray""),\n            ),\n            method(""roll"", {""x"": 2}),\n            pytest.param(\n                method(""rolling_window"", ""x"", 3, ""window""),\n                marks=pytest.mark.xfail(reason=""converts to ndarray""),\n            ),\n            method(""reduce"", np.std, ""x""),\n            method(""round"", 2),\n            method(""shift"", {""x"": -2}),\n            method(""transpose"", ""y"", ""x""),\n        ),\n        ids=repr,\n    )\n    def test_computation(self, func, dtype):\n        base_unit = unit_registry.m\n        array = np.linspace(0, 5, 5 * 10).reshape(5, 10).astype(dtype) * base_unit\n        variable = xr.Variable((""x"", ""y""), array)\n\n        expected = attach_units(func(strip_units(variable)), extract_units(variable))\n\n        actual = func(variable)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_searchsorted(self, unit, error, dtype):\n        base_unit = unit_registry.m\n        array = np.linspace(0, 5, 10).astype(dtype) * base_unit\n        variable = xr.Variable(""x"", array)\n\n        value = 0 * unit\n\n        if error is not None:\n            with pytest.raises(error):\n                variable.searchsorted(value)\n\n            return\n\n        expected = strip_units(variable).searchsorted(\n            strip_units(convert_units(value, {None: base_unit}))\n        )\n\n        actual = variable.searchsorted(value)\n\n        assert_units_equal(expected, actual)\n        np.testing.assert_allclose(expected, actual)\n\n    def test_stack(self, dtype):\n        array = np.linspace(0, 5, 3 * 10).reshape(3, 10).astype(dtype) * unit_registry.m\n        variable = xr.Variable((""x"", ""y""), array)\n\n        expected = attach_units(\n            strip_units(variable).stack(z=(""x"", ""y"")), extract_units(variable)\n        )\n        actual = variable.stack(z=(""x"", ""y""))\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    def test_unstack(self, dtype):\n        array = np.linspace(0, 5, 3 * 10).astype(dtype) * unit_registry.m\n        variable = xr.Variable(""z"", array)\n\n        expected = attach_units(\n            strip_units(variable).unstack(z={""x"": 3, ""y"": 10}), extract_units(variable)\n        )\n        actual = variable.unstack(z={""x"": 3, ""y"": 10})\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_concat(self, unit, error, dtype):\n        array1 = (\n            np.linspace(0, 5, 9 * 10).reshape(3, 6, 5).astype(dtype) * unit_registry.m\n        )\n        array2 = np.linspace(5, 10, 10 * 3).reshape(3, 2, 5).astype(dtype) * unit\n\n        variable = xr.Variable((""x"", ""y"", ""z""), array1)\n        other = xr.Variable((""x"", ""y"", ""z""), array2)\n\n        if error is not None:\n            with pytest.raises(error):\n                xr.Variable.concat([variable, other], dim=""y"")\n\n            return\n\n        units = extract_units(variable)\n        expected = attach_units(\n            xr.Variable.concat(\n                [strip_units(variable), strip_units(convert_units(other, units))],\n                dim=""y"",\n            ),\n            units,\n        )\n        actual = xr.Variable.concat([variable, other], dim=""y"")\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    def test_set_dims(self, dtype):\n        array = np.linspace(0, 5, 3 * 10).reshape(3, 10).astype(dtype) * unit_registry.m\n        variable = xr.Variable((""x"", ""y""), array)\n\n        dims = {""z"": 6, ""x"": 3, ""a"": 1, ""b"": 4, ""y"": 10}\n        expected = attach_units(\n            strip_units(variable).set_dims(dims), extract_units(variable)\n        )\n        actual = variable.set_dims(dims)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    def test_copy(self, dtype):\n        array = np.linspace(0, 5, 10).astype(dtype) * unit_registry.m\n        other = np.arange(10).astype(dtype) * unit_registry.s\n\n        variable = xr.Variable(""x"", array)\n        expected = attach_units(\n            strip_units(variable).copy(data=strip_units(other)), extract_units(other)\n        )\n        actual = variable.copy(data=other)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""unit"",\n        (\n            pytest.param(1, id=""no_unit""),\n            pytest.param(unit_registry.dimensionless, id=""dimensionless""),\n            pytest.param(unit_registry.s, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, id=""compatible_unit""),\n            pytest.param(unit_registry.m, id=""identical_unit""),\n        ),\n    )\n    def test_no_conflicts(self, unit, dtype):\n        base_unit = unit_registry.m\n        array1 = (\n            np.array(\n                [\n                    [6.3, 0.3, 0.45],\n                    [np.nan, 0.3, 0.3],\n                    [3.7, np.nan, 0.2],\n                    [9.43, 0.3, 0.7],\n                ]\n            )\n            * base_unit\n        )\n        array2 = np.array([np.nan, 0.3, np.nan]) * unit\n\n        variable = xr.Variable((""x"", ""y""), array1)\n        other = xr.Variable(""y"", array2)\n\n        expected = strip_units(variable).no_conflicts(\n            strip_units(\n                convert_units(\n                    other, {None: base_unit if is_compatible(base_unit, unit) else None}\n                )\n            )\n        ) & is_compatible(base_unit, unit)\n        actual = variable.no_conflicts(other)\n\n        assert expected == actual\n\n    @pytest.mark.parametrize(""xr_arg, np_arg"", _PAD_XR_NP_ARGS)\n    def test_pad_constant_values(self, dtype, xr_arg, np_arg):\n        data = np.arange(4 * 3 * 2).reshape(4, 3, 2).astype(dtype) * unit_registry.m\n        v = xr.Variable([""x"", ""y"", ""z""], data)\n\n        actual = v.pad(**xr_arg, mode=""constant"")\n        expected = xr.Variable(\n            v.dims,\n            np.pad(\n                v.data.astype(float), np_arg, mode=""constant"", constant_values=np.nan,\n            ),\n        )\n        xr.testing.assert_identical(expected, actual)\n        assert_units_equal(expected, actual)\n        assert isinstance(actual._data, type(v._data))\n\n        # for the boolean array, we pad False\n        data = np.full_like(data, False, dtype=bool).reshape(4, 3, 2)\n        v = xr.Variable([""x"", ""y"", ""z""], data)\n        actual = v.pad(**xr_arg, mode=""constant"", constant_values=data.flat[0])\n        expected = xr.Variable(\n            v.dims,\n            np.pad(v.data, np_arg, mode=""constant"", constant_values=v.data.flat[0]),\n        )\n        xr.testing.assert_identical(actual, expected)\n        assert_units_equal(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(\n                1,\n                DimensionalityError,\n                id=""no_unit"",\n                marks=pytest.mark.xfail(\n                    LooseVersion(pint.__version__) < LooseVersion(""0.10.2""),\n                    reason=""bug in pint\'s implementation of np.pad"",\n                ),\n            ),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_pad_unit_constant_value(self, unit, error, dtype):\n        array = np.linspace(0, 5, 3 * 10).reshape(3, 10).astype(dtype) * unit_registry.m\n        variable = xr.Variable((""x"", ""y""), array)\n\n        fill_value = -100 * unit\n\n        func = method(""pad"", mode=""constant"", x=(2, 3), y=(1, 4))\n        if error is not None:\n            with pytest.raises(error):\n                func(variable, constant_values=fill_value)\n\n            return\n\n        units = extract_units(variable)\n        expected = attach_units(\n            func(\n                strip_units(variable),\n                constant_values=strip_units(convert_units(fill_value, units)),\n            ),\n            units,\n        )\n        actual = func(variable, constant_values=fill_value)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n\nclass TestDataArray:\n    @pytest.mark.filterwarnings(""error:::pint[.*]"")\n    @pytest.mark.parametrize(\n        ""variant"",\n        (\n            pytest.param(\n                ""with_dims"",\n                marks=pytest.mark.xfail(reason=""units in indexes are not supported""),\n            ),\n            ""with_coords"",\n            ""without_coords"",\n        ),\n    )\n    def test_init(self, variant, dtype):\n        array = np.linspace(1, 2, 10, dtype=dtype) * unit_registry.m\n\n        x = np.arange(len(array)) * unit_registry.s\n        y = x.to(unit_registry.ms)\n\n        variants = {\n            ""with_dims"": {""x"": x},\n            ""with_coords"": {""y"": (""x"", y)},\n            ""without_coords"": {},\n        }\n\n        kwargs = {""data"": array, ""dims"": ""x"", ""coords"": variants.get(variant)}\n        data_array = xr.DataArray(**kwargs)\n\n        assert isinstance(data_array.data, Quantity)\n        assert all(\n            {\n                name: isinstance(coord.data, Quantity)\n                for name, coord in data_array.coords.items()\n            }.values()\n        )\n\n    @pytest.mark.filterwarnings(""error:::pint[.*]"")\n    @pytest.mark.parametrize(\n        ""func"", (pytest.param(str, id=""str""), pytest.param(repr, id=""repr""))\n    )\n    @pytest.mark.parametrize(\n        ""variant"",\n        (\n            pytest.param(\n                ""with_dims"",\n                marks=pytest.mark.xfail(reason=""units in indexes are not supported""),\n            ),\n            pytest.param(""with_coords""),\n            pytest.param(""without_coords""),\n        ),\n    )\n    def test_repr(self, func, variant, dtype):\n        array = np.linspace(1, 2, 10, dtype=dtype) * unit_registry.m\n        x = np.arange(len(array)) * unit_registry.s\n        y = x.to(unit_registry.ms)\n\n        variants = {\n            ""with_dims"": {""x"": x},\n            ""with_coords"": {""y"": (""x"", y)},\n            ""without_coords"": {},\n        }\n\n        kwargs = {""data"": array, ""dims"": ""x"", ""coords"": variants.get(variant)}\n        data_array = xr.DataArray(**kwargs)\n\n        # FIXME: this just checks that the repr does not raise\n        # warnings or errors, but does not check the result\n        func(data_array)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            function(""all""),\n            function(""any""),\n            function(""argmax""),\n            function(""argmin""),\n            function(""max""),\n            function(""mean""),\n            pytest.param(\n                function(""median""),\n                marks=pytest.mark.xfail(\n                    reason=""median does not work with dataarrays yet""\n                ),\n            ),\n            function(""min""),\n            pytest.param(\n                function(""prod""),\n                marks=pytest.mark.xfail(reason=""not implemented by pint yet""),\n            ),\n            function(""sum""),\n            function(""std""),\n            function(""var""),\n            function(""cumsum""),\n            function(""cumprod""),\n            method(""all""),\n            method(""any""),\n            method(""argmax""),\n            method(""argmin""),\n            method(""max""),\n            method(""mean""),\n            method(""median""),\n            method(""min""),\n            pytest.param(\n                method(""prod""),\n                marks=pytest.mark.xfail(reason=""not implemented by pint yet""),\n            ),\n            method(""sum""),\n            method(""std""),\n            method(""var""),\n            method(""cumsum""),\n            method(""cumprod""),\n        ),\n        ids=repr,\n    )\n    def test_aggregation(self, func, dtype):\n        array = np.arange(10).astype(dtype) * (\n            unit_registry.m if func.name != ""cumprod"" else unit_registry.dimensionless\n        )\n        data_array = xr.DataArray(data=array, dims=""x"")\n\n        # units differ based on the applied function, so we need to\n        # first compute the units\n        units = extract_units(func(array))\n        expected = attach_units(func(strip_units(data_array)), units)\n        actual = func(data_array)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_allclose(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            pytest.param(operator.neg, id=""negate""),\n            pytest.param(abs, id=""absolute""),\n            pytest.param(np.round, id=""round""),\n        ),\n    )\n    def test_unary_operations(self, func, dtype):\n        array = np.arange(10).astype(dtype) * unit_registry.m\n        data_array = xr.DataArray(data=array)\n\n        units = extract_units(func(array))\n        expected = attach_units(func(strip_units(data_array)), units)\n        actual = func(data_array)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            pytest.param(lambda x: 2 * x, id=""multiply""),\n            pytest.param(lambda x: x + x, id=""add""),\n            pytest.param(lambda x: x[0] + x, id=""add scalar""),\n            pytest.param(lambda x: x.T @ x, id=""matrix multiply""),\n        ),\n    )\n    def test_binary_operations(self, func, dtype):\n        array = np.arange(10).astype(dtype) * unit_registry.m\n        data_array = xr.DataArray(data=array)\n\n        units = extract_units(func(array))\n        expected = attach_units(func(strip_units(data_array)), units)\n        actual = func(data_array)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""comparison"",\n        (\n            pytest.param(operator.lt, id=""less_than""),\n            pytest.param(operator.ge, id=""greater_equal""),\n            pytest.param(operator.eq, id=""equal""),\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, ValueError, id=""without_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.mm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_comparison_operations(self, comparison, unit, error, dtype):\n        array = (\n            np.array([10.1, 5.2, 6.5, 8.0, 21.3, 7.1, 1.3]).astype(dtype)\n            * unit_registry.m\n        )\n        data_array = xr.DataArray(data=array)\n\n        value = 8\n        to_compare_with = value * unit\n\n        # incompatible units are all not equal\n        if error is not None and comparison is not operator.eq:\n            with pytest.raises(error):\n                comparison(array, to_compare_with)\n\n            with pytest.raises(error):\n                comparison(data_array, to_compare_with)\n\n            return\n\n        actual = comparison(data_array, to_compare_with)\n\n        expected_units = {None: unit_registry.m if array.check(unit) else None}\n        expected = array.check(unit) & comparison(\n            strip_units(data_array),\n            strip_units(convert_units(to_compare_with, expected_units)),\n        )\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""units,error"",\n        (\n            pytest.param(unit_registry.dimensionless, None, id=""dimensionless""),\n            pytest.param(unit_registry.m, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.degree, None, id=""compatible_unit""),\n        ),\n    )\n    def test_univariate_ufunc(self, units, error, dtype):\n        array = np.arange(10).astype(dtype) * units\n        data_array = xr.DataArray(data=array)\n\n        func = function(""sin"")\n\n        if error is not None:\n            with pytest.raises(error):\n                np.sin(data_array)\n\n            return\n\n        expected = attach_units(\n            func(strip_units(convert_units(data_array, {None: unit_registry.radians}))),\n            {None: unit_registry.dimensionless},\n        )\n        actual = func(data_array)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.xfail(reason=""needs the type register system for __array_ufunc__"")\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""without_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(\n                unit_registry.mm,\n                None,\n                id=""compatible_unit"",\n                marks=pytest.mark.xfail(reason=""pint converts to the wrong units""),\n            ),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_bivariate_ufunc(self, unit, error, dtype):\n        original_unit = unit_registry.m\n        array = np.arange(10).astype(dtype) * original_unit\n        data_array = xr.DataArray(data=array)\n\n        if error is not None:\n            with pytest.raises(error):\n                np.maximum(data_array, 1 * unit)\n\n            return\n\n        expected_units = {None: original_unit}\n        expected = attach_units(\n            np.maximum(\n                strip_units(data_array),\n                strip_units(convert_units(1 * unit, expected_units)),\n            ),\n            expected_units,\n        )\n\n        actual = np.maximum(data_array, 1 * unit)\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n        actual = np.maximum(1 * unit, data_array)\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(""property"", (""T"", ""imag"", ""real""))\n    def test_numpy_properties(self, property, dtype):\n        array = (\n            np.arange(5 * 10).astype(dtype)\n            + 1j * np.linspace(-1, 0, 5 * 10).astype(dtype)\n        ).reshape(5, 10) * unit_registry.s\n\n        data_array = xr.DataArray(data=array, dims=(""x"", ""y""))\n\n        expected = attach_units(\n            getattr(strip_units(data_array), property), extract_units(data_array)\n        )\n        actual = getattr(data_array, property)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (method(""conj""), method(""argsort""), method(""conjugate""), method(""round"")),\n        ids=repr,\n    )\n    def test_numpy_methods(self, func, dtype):\n        array = np.arange(10).astype(dtype) * unit_registry.m\n        data_array = xr.DataArray(data=array, dims=""x"")\n\n        units = extract_units(func(array))\n        expected = attach_units(strip_units(data_array), units)\n        actual = func(data_array)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    def test_item(self, dtype):\n        array = np.arange(10).astype(dtype) * unit_registry.m\n        data_array = xr.DataArray(data=array)\n\n        func = method(""item"", 2)\n\n        expected = func(strip_units(data_array)) * unit_registry.m\n        actual = func(data_array)\n\n        np.testing.assert_allclose(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            method(""searchsorted"", 5),\n            pytest.param(\n                function(""searchsorted"", 5),\n                marks=pytest.mark.xfail(\n                    reason=""xarray does not implement __array_function__""\n                ),\n            ),\n        ),\n        ids=repr,\n    )\n    def test_searchsorted(self, func, unit, error, dtype):\n        array = np.arange(10).astype(dtype) * unit_registry.m\n        data_array = xr.DataArray(data=array)\n\n        scalar_types = (int, float)\n        args = list(value * unit for value in func.args)\n        kwargs = {\n            key: (value * unit if isinstance(value, scalar_types) else value)\n            for key, value in func.kwargs.items()\n        }\n\n        if error is not None:\n            with pytest.raises(error):\n                func(data_array, *args, **kwargs)\n\n            return\n\n        units = extract_units(data_array)\n        expected_units = extract_units(func(array, *args, **kwargs))\n        stripped_args = [strip_units(convert_units(value, units)) for value in args]\n        stripped_kwargs = {\n            key: strip_units(convert_units(value, units))\n            for key, value in kwargs.items()\n        }\n        expected = attach_units(\n            func(strip_units(data_array), *stripped_args, **stripped_kwargs),\n            expected_units,\n        )\n        actual = func(data_array, *args, **kwargs)\n\n        assert_units_equal(expected, actual)\n        np.testing.assert_allclose(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            method(""clip"", min=3, max=8),\n            pytest.param(\n                function(""clip"", a_min=3, a_max=8),\n                marks=pytest.mark.xfail(\n                    reason=""xarray does not implement __array_function__""\n                ),\n            ),\n        ),\n        ids=repr,\n    )\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_numpy_methods_with_args(self, func, unit, error, dtype):\n        array = np.arange(10).astype(dtype) * unit_registry.m\n        data_array = xr.DataArray(data=array)\n\n        scalar_types = (int, float)\n        args = list(value * unit for value in func.args)\n        kwargs = {\n            key: (value * unit if isinstance(value, scalar_types) else value)\n            for key, value in func.kwargs.items()\n        }\n        if error is not None:\n            with pytest.raises(error):\n                func(data_array, *args, **kwargs)\n\n            return\n\n        units = extract_units(data_array)\n        expected_units = extract_units(func(array, *args, **kwargs))\n        stripped_args = [strip_units(convert_units(value, units)) for value in args]\n        stripped_kwargs = {\n            key: strip_units(convert_units(value, units))\n            for key, value in kwargs.items()\n        }\n        expected = attach_units(\n            func(strip_units(data_array), *stripped_args, **stripped_kwargs),\n            expected_units,\n        )\n        actual = func(data_array, *args, **kwargs)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"", (method(""isnull""), method(""notnull""), method(""count"")), ids=repr\n    )\n    def test_missing_value_detection(self, func, dtype):\n        array = (\n            np.array(\n                [\n                    [1.4, 2.3, np.nan, 7.2],\n                    [np.nan, 9.7, np.nan, np.nan],\n                    [2.1, np.nan, np.nan, 4.6],\n                    [9.9, np.nan, 7.2, 9.1],\n                ]\n            )\n            * unit_registry.degK\n        )\n        data_array = xr.DataArray(data=array)\n\n        expected = func(strip_units(data_array))\n        actual = func(data_array)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.xfail(reason=""ffill and bfill lose units in data"")\n    @pytest.mark.parametrize(""func"", (method(""ffill""), method(""bfill"")), ids=repr)\n    def test_missing_value_filling(self, func, dtype):\n        array = (\n            np.array([1.4, np.nan, 2.3, np.nan, np.nan, 9.1]).astype(dtype)\n            * unit_registry.degK\n        )\n        x = np.arange(len(array))\n        data_array = xr.DataArray(data=array, coords={""x"": x}, dims=""x"")\n\n        expected = attach_units(\n            func(strip_units(data_array), dim=""x""), extract_units(data_array)\n        )\n        actual = func(data_array, dim=""x"")\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""fill_value"",\n        (\n            pytest.param(-1, id=""python_scalar""),\n            pytest.param(np.array(-1), id=""numpy_scalar""),\n            pytest.param(np.array([-1]), id=""numpy_array""),\n        ),\n    )\n    def test_fillna(self, fill_value, unit, error, dtype):\n        original_unit = unit_registry.m\n        array = (\n            np.array([1.4, np.nan, 2.3, np.nan, np.nan, 9.1]).astype(dtype)\n            * original_unit\n        )\n        data_array = xr.DataArray(data=array)\n\n        func = method(""fillna"")\n\n        value = fill_value * unit\n        if error is not None:\n            with pytest.raises(error):\n                func(data_array, value=value)\n\n            return\n\n        units = extract_units(data_array)\n        expected = attach_units(\n            func(\n                strip_units(data_array), value=strip_units(convert_units(value, units))\n            ),\n            units,\n        )\n        actual = func(data_array, value=value)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    def test_dropna(self, dtype):\n        array = (\n            np.array([1.4, np.nan, 2.3, np.nan, np.nan, 9.1]).astype(dtype)\n            * unit_registry.m\n        )\n        x = np.arange(len(array))\n        data_array = xr.DataArray(data=array, coords={""x"": x}, dims=[""x""])\n\n        units = extract_units(data_array)\n        expected = attach_units(strip_units(data_array).dropna(dim=""x""), units)\n        actual = data_array.dropna(dim=""x"")\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""unit"",\n        (\n            pytest.param(1, id=""no_unit""),\n            pytest.param(unit_registry.dimensionless, id=""dimensionless""),\n            pytest.param(unit_registry.s, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, id=""compatible_unit""),\n            pytest.param(unit_registry.m, id=""identical_unit""),\n        ),\n    )\n    def test_isin(self, unit, dtype):\n        array = (\n            np.array([1.4, np.nan, 2.3, np.nan, np.nan, 9.1]).astype(dtype)\n            * unit_registry.m\n        )\n        data_array = xr.DataArray(data=array, dims=""x"")\n\n        raw_values = np.array([1.4, np.nan, 2.3]).astype(dtype)\n        values = raw_values * unit\n\n        units = {None: unit_registry.m if array.check(unit) else None}\n        expected = strip_units(data_array).isin(\n            strip_units(convert_units(values, units))\n        ) & array.check(unit)\n        actual = data_array.isin(values)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""variant"", (""masking"", ""replacing_scalar"", ""replacing_array"", ""dropping"")\n    )\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_where(self, variant, unit, error, dtype):\n        original_unit = unit_registry.m\n        array = np.linspace(0, 1, 10).astype(dtype) * original_unit\n\n        data_array = xr.DataArray(data=array)\n\n        condition = data_array < 0.5 * original_unit\n        other = np.linspace(-2, -1, 10).astype(dtype) * unit\n        variant_kwargs = {\n            ""masking"": {""cond"": condition},\n            ""replacing_scalar"": {""cond"": condition, ""other"": -1 * unit},\n            ""replacing_array"": {""cond"": condition, ""other"": other},\n            ""dropping"": {""cond"": condition, ""drop"": True},\n        }\n        kwargs = variant_kwargs.get(variant)\n        kwargs_without_units = {\n            key: strip_units(\n                convert_units(\n                    value, {None: original_unit if array.check(unit) else None}\n                )\n            )\n            for key, value in kwargs.items()\n        }\n\n        if variant not in (""masking"", ""dropping"") and error is not None:\n            with pytest.raises(error):\n                data_array.where(**kwargs)\n\n            return\n\n        expected = attach_units(\n            strip_units(data_array).where(**kwargs_without_units),\n            extract_units(data_array),\n        )\n        actual = data_array.where(**kwargs)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.xfail(reason=""uses numpy.vectorize"")\n    def test_interpolate_na(self):\n        array = (\n            np.array([-1.03, 0.1, 1.4, np.nan, 2.3, np.nan, np.nan, 9.1])\n            * unit_registry.m\n        )\n        x = np.arange(len(array))\n        data_array = xr.DataArray(data=array, coords={""x"": x}, dims=""x"")\n\n        units = extract_units(data_array)\n        expected = attach_units(strip_units(data_array).interpolate_na(dim=""x""), units)\n        actual = data_array.interpolate_na(dim=""x"")\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit"",),\n            pytest.param(unit_registry.m, None, id=""identical_unit"",),\n        ),\n    )\n    def test_combine_first(self, unit, error, dtype):\n        array = np.zeros(shape=(2, 2), dtype=dtype) * unit_registry.m\n        other_array = np.ones_like(array) * unit\n\n        data_array = xr.DataArray(\n            data=array, coords={""x"": [""a"", ""b""], ""y"": [-1, 0]}, dims=[""x"", ""y""]\n        )\n        other = xr.DataArray(\n            data=other_array, coords={""x"": [""b"", ""c""], ""y"": [0, 1]}, dims=[""x"", ""y""]\n        )\n\n        if error is not None:\n            with pytest.raises(error):\n                data_array.combine_first(other)\n\n            return\n\n        units = extract_units(data_array)\n        expected = attach_units(\n            strip_units(data_array).combine_first(\n                strip_units(convert_units(other, units))\n            ),\n            units,\n        )\n        actual = data_array.combine_first(other)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""unit"",\n        (\n            pytest.param(1, id=""no_unit""),\n            pytest.param(unit_registry.dimensionless, id=""dimensionless""),\n            pytest.param(unit_registry.s, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, id=""compatible_unit""),\n            pytest.param(unit_registry.m, id=""identical_unit""),\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""variation"",\n        (\n            ""data"",\n            pytest.param(\n                ""dims"", marks=pytest.mark.xfail(reason=""units in indexes not supported"")\n            ),\n            ""coords"",\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            method(""equals""),\n            pytest.param(\n                method(""identical""),\n                marks=pytest.mark.skip(reason=""the behavior of identical is undecided""),\n            ),\n        ),\n        ids=repr,\n    )\n    def test_comparisons(self, func, variation, unit, dtype):\n        def is_compatible(a, b):\n            a = a if a is not None else 1\n            b = b if b is not None else 1\n            quantity = np.arange(5) * a\n\n            return a == b or quantity.check(b)\n\n        data = np.linspace(0, 5, 10).astype(dtype)\n        coord = np.arange(len(data)).astype(dtype)\n\n        base_unit = unit_registry.m\n        array = data * (base_unit if variation == ""data"" else 1)\n        x = coord * (base_unit if variation == ""dims"" else 1)\n        y = coord * (base_unit if variation == ""coords"" else 1)\n\n        variations = {\n            ""data"": (unit, 1, 1),\n            ""dims"": (1, unit, 1),\n            ""coords"": (1, 1, unit),\n        }\n        data_unit, dim_unit, coord_unit = variations.get(variation)\n\n        data_array = xr.DataArray(data=array, coords={""x"": x, ""y"": (""x"", y)}, dims=""x"")\n\n        other = attach_units(\n            strip_units(data_array), {None: data_unit, ""x"": dim_unit, ""y"": coord_unit}\n        )\n\n        units = extract_units(data_array)\n        other_units = extract_units(other)\n\n        equal_arrays = all(\n            is_compatible(units[name], other_units[name]) for name in units.keys()\n        ) and (\n            strip_units(data_array).equals(\n                strip_units(convert_units(other, extract_units(data_array)))\n            )\n        )\n        equal_units = units == other_units\n        expected = equal_arrays and (func.name != ""identical"" or equal_units)\n\n        actual = func(data_array, other)\n\n        assert expected == actual\n\n    @pytest.mark.parametrize(\n        ""unit"",\n        (\n            pytest.param(1, id=""no_unit""),\n            pytest.param(unit_registry.dimensionless, id=""dimensionless""),\n            pytest.param(unit_registry.s, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, id=""compatible_unit""),\n            pytest.param(unit_registry.m, id=""identical_unit""),\n        ),\n    )\n    def test_broadcast_like(self, unit, dtype):\n        array1 = np.linspace(1, 2, 2 * 1).reshape(2, 1).astype(dtype) * unit_registry.Pa\n        array2 = np.linspace(0, 1, 2 * 3).reshape(2, 3).astype(dtype) * unit_registry.Pa\n\n        x1 = np.arange(2) * unit_registry.m\n        x2 = np.arange(2) * unit\n        y1 = np.array([0]) * unit_registry.m\n        y2 = np.arange(3) * unit\n\n        arr1 = xr.DataArray(data=array1, coords={""x"": x1, ""y"": y1}, dims=(""x"", ""y""))\n        arr2 = xr.DataArray(data=array2, coords={""x"": x2, ""y"": y2}, dims=(""x"", ""y""))\n\n        expected = attach_units(\n            strip_units(arr1).broadcast_like(strip_units(arr2)), extract_units(arr1)\n        )\n        actual = arr1.broadcast_like(arr2)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""unit"",\n        (\n            pytest.param(1, id=""no_unit""),\n            pytest.param(unit_registry.dimensionless, id=""dimensionless""),\n            pytest.param(unit_registry.s, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, id=""compatible_unit""),\n            pytest.param(unit_registry.m, id=""identical_unit""),\n        ),\n    )\n    def test_broadcast_equals(self, unit, dtype):\n        left_array = np.ones(shape=(2, 2), dtype=dtype) * unit_registry.m\n        right_array = np.ones(shape=(2,), dtype=dtype) * unit\n\n        left = xr.DataArray(data=left_array, dims=(""x"", ""y""))\n        right = xr.DataArray(data=right_array, dims=""x"")\n\n        units = {\n            **extract_units(left),\n            **({} if left_array.check(unit) else {None: None}),\n        }\n        expected = strip_units(left).broadcast_equals(\n            strip_units(convert_units(right, units))\n        ) & left_array.check(unit)\n        actual = left.broadcast_equals(right)\n\n        assert expected == actual\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            method(""pipe"", lambda da: da * 10),\n            method(""assign_coords"", y2=(""y"", np.arange(10) * unit_registry.mm)),\n            method(""assign_attrs"", attr1=""value""),\n            method(""rename"", x2=""x_mm""),\n            method(""swap_dims"", {""x"": ""x2""}),\n            method(\n                ""expand_dims"",\n                dim={""z"": np.linspace(10, 20, 12) * unit_registry.s},\n                axis=1,\n            ),\n            method(""drop_vars"", ""x""),\n            method(""reset_coords"", names=""x2""),\n            method(""copy""),\n            method(""astype"", np.float32),\n        ),\n        ids=repr,\n    )\n    def test_content_manipulation(self, func, dtype):\n        quantity = (\n            np.linspace(0, 10, 5 * 10).reshape(5, 10).astype(dtype)\n            * unit_registry.pascal\n        )\n        x = np.arange(quantity.shape[0]) * unit_registry.m\n        y = np.arange(quantity.shape[1]) * unit_registry.m\n        x2 = x.to(unit_registry.mm)\n\n        data_array = xr.DataArray(\n            name=""data"",\n            data=quantity,\n            coords={""x"": x, ""x2"": (""x"", x2), ""y"": y},\n            dims=(""x"", ""y""),\n        )\n\n        stripped_kwargs = {\n            key: array_strip_units(value) for key, value in func.kwargs.items()\n        }\n        units = {**{""x_mm"": x2.units, ""x2"": x2.units}, **extract_units(data_array)}\n\n        expected = attach_units(func(strip_units(data_array), **stripped_kwargs), units)\n        actual = func(data_array)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"", (pytest.param(method(""copy"", data=np.arange(20))),), ids=repr\n    )\n    @pytest.mark.parametrize(\n        ""unit"",\n        (\n            pytest.param(1, id=""no_unit""),\n            pytest.param(unit_registry.dimensionless, id=""dimensionless""),\n            pytest.param(unit_registry.degK, id=""with_unit""),\n        ),\n    )\n    def test_content_manipulation_with_units(self, func, unit, dtype):\n        quantity = np.linspace(0, 10, 20, dtype=dtype) * unit_registry.pascal\n        x = np.arange(len(quantity)) * unit_registry.m\n\n        data_array = xr.DataArray(data=quantity, coords={""x"": x}, dims=""x"")\n\n        kwargs = {key: value * unit for key, value in func.kwargs.items()}\n\n        expected = attach_units(\n            func(strip_units(data_array)), {None: unit, ""x"": x.units}\n        )\n\n        actual = func(data_array, **kwargs)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""indices"",\n        (\n            pytest.param(4, id=""single index""),\n            pytest.param([5, 2, 9, 1], id=""multiple indices""),\n        ),\n    )\n    def test_isel(self, indices, dtype):\n        array = np.arange(10).astype(dtype) * unit_registry.s\n        x = np.arange(len(array)) * unit_registry.m\n\n        data_array = xr.DataArray(data=array, coords={""x"": x}, dims=""x"")\n\n        expected = attach_units(\n            strip_units(data_array).isel(x=indices), extract_units(data_array)\n        )\n        actual = data_array.isel(x=indices)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.xfail(reason=""indexes don\'t support units"")\n    @pytest.mark.parametrize(\n        ""raw_values"",\n        (\n            pytest.param(10, id=""single_value""),\n            pytest.param([10, 5, 13], id=""list_of_values""),\n            pytest.param(np.array([9, 3, 7, 12]), id=""array_of_values""),\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, KeyError, id=""no_units""),\n            pytest.param(unit_registry.dimensionless, KeyError, id=""dimensionless""),\n            pytest.param(unit_registry.degree, KeyError, id=""incompatible_unit""),\n            pytest.param(unit_registry.dm, KeyError, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_sel(self, raw_values, unit, error, dtype):\n        array = np.linspace(5, 10, 20).astype(dtype) * unit_registry.m\n        x = np.arange(len(array)) * unit_registry.m\n        data_array = xr.DataArray(data=array, coords={""x"": x}, dims=""x"")\n\n        values = raw_values * unit\n\n        if error is not None and not (\n            isinstance(raw_values, (int, float)) and x.check(unit)\n        ):\n            with pytest.raises(error):\n                data_array.sel(x=values)\n\n            return\n\n        expected = attach_units(\n            strip_units(data_array).sel(\n                x=strip_units(convert_units(values, {None: array.units}))\n            ),\n            extract_units(data_array),\n        )\n        actual = data_array.sel(x=values)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.xfail(reason=""indexes don\'t support units"")\n    @pytest.mark.parametrize(\n        ""raw_values"",\n        (\n            pytest.param(10, id=""single_value""),\n            pytest.param([10, 5, 13], id=""list_of_values""),\n            pytest.param(np.array([9, 3, 7, 12]), id=""array_of_values""),\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, KeyError, id=""no_units""),\n            pytest.param(unit_registry.dimensionless, KeyError, id=""dimensionless""),\n            pytest.param(unit_registry.degree, KeyError, id=""incompatible_unit""),\n            pytest.param(unit_registry.dm, KeyError, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_loc(self, raw_values, unit, error, dtype):\n        array = np.linspace(5, 10, 20).astype(dtype) * unit_registry.m\n        x = np.arange(len(array)) * unit_registry.m\n        data_array = xr.DataArray(data=array, coords={""x"": x}, dims=""x"")\n\n        values = raw_values * unit\n\n        if error is not None and not (\n            isinstance(raw_values, (int, float)) and x.check(unit)\n        ):\n            with pytest.raises(error):\n                data_array.loc[{""x"": values}]\n\n            return\n\n        expected = attach_units(\n            strip_units(data_array).loc[\n                {""x"": strip_units(convert_units(values, {None: array.units}))}\n            ],\n            extract_units(data_array),\n        )\n        actual = data_array.loc[{""x"": values}]\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.xfail(reason=""indexes don\'t support units"")\n    @pytest.mark.parametrize(\n        ""raw_values"",\n        (\n            pytest.param(10, id=""single_value""),\n            pytest.param([10, 5, 13], id=""list_of_values""),\n            pytest.param(np.array([9, 3, 7, 12]), id=""array_of_values""),\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, KeyError, id=""no_units""),\n            pytest.param(unit_registry.dimensionless, KeyError, id=""dimensionless""),\n            pytest.param(unit_registry.degree, KeyError, id=""incompatible_unit""),\n            pytest.param(unit_registry.dm, KeyError, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_drop_sel(self, raw_values, unit, error, dtype):\n        array = np.linspace(5, 10, 20).astype(dtype) * unit_registry.m\n        x = np.arange(len(array)) * unit_registry.m\n        data_array = xr.DataArray(data=array, coords={""x"": x}, dims=""x"")\n\n        values = raw_values * unit\n\n        if error is not None and not (\n            isinstance(raw_values, (int, float)) and x.check(unit)\n        ):\n            with pytest.raises(error):\n                data_array.drop_sel(x=values)\n\n            return\n\n        expected = attach_units(\n            strip_units(data_array).drop_sel(\n                x=strip_units(convert_units(values, {None: x.units}))\n            ),\n            extract_units(data_array),\n        )\n        actual = data_array.drop_sel(x=values)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""shape"",\n        (\n            pytest.param((10, 20), id=""nothing_squeezable""),\n            pytest.param((10, 20, 1), id=""last_dimension_squeezable""),\n            pytest.param((10, 1, 20), id=""middle_dimension_squeezable""),\n            pytest.param((1, 10, 20), id=""first_dimension_squeezable""),\n            pytest.param((1, 10, 1, 20), id=""first_and_last_dimension_squeezable""),\n        ),\n    )\n    def test_squeeze(self, shape, dtype):\n        names = ""xyzt""\n        coords = {\n            name: np.arange(length).astype(dtype)\n            * (unit_registry.m if name != ""t"" else unit_registry.s)\n            for name, length in zip(names, shape)\n        }\n        array = np.arange(10 * 20).astype(dtype).reshape(shape) * unit_registry.J\n        data_array = xr.DataArray(\n            data=array, coords=coords, dims=tuple(names[: len(shape)])\n        )\n\n        expected = attach_units(\n            strip_units(data_array).squeeze(), extract_units(data_array)\n        )\n        actual = data_array.squeeze()\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n        # try squeezing the dimensions separately\n        names = tuple(dim for dim, coord in coords.items() if len(coord) == 1)\n        for index, name in enumerate(names):\n            expected = attach_units(\n                strip_units(data_array).squeeze(dim=name), extract_units(data_array)\n            )\n            actual = data_array.squeeze(dim=name)\n\n            assert_units_equal(expected, actual)\n            xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (method(""head"", x=7, y=3), method(""tail"", x=7, y=3), method(""thin"", x=7, y=3)),\n        ids=repr,\n    )\n    def test_head_tail_thin(self, func, dtype):\n        array = np.linspace(1, 2, 10 * 5).reshape(10, 5) * unit_registry.degK\n\n        coords = {\n            ""x"": np.arange(10) * unit_registry.m,\n            ""y"": np.arange(5) * unit_registry.m,\n        }\n\n        data_array = xr.DataArray(data=array, coords=coords, dims=(""x"", ""y""))\n\n        expected = attach_units(\n            func(strip_units(data_array)), extract_units(data_array)\n        )\n        actual = func(data_array)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(""variant"", (""data"", ""coords""))\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            pytest.param(\n                method(""interp""), marks=pytest.mark.xfail(reason=""uses scipy"")\n            ),\n            method(""reindex""),\n        ),\n        ids=repr,\n    )\n    def test_interp_reindex(self, variant, func, dtype):\n        variants = {\n            ""data"": (unit_registry.m, 1),\n            ""coords"": (1, unit_registry.m),\n        }\n        data_unit, coord_unit = variants.get(variant)\n\n        array = np.linspace(1, 2, 10).astype(dtype) * data_unit\n        y = np.arange(10) * coord_unit\n\n        x = np.arange(10)\n        new_x = np.arange(10) + 0.5\n        data_array = xr.DataArray(array, coords={""x"": x, ""y"": (""x"", y)}, dims=""x"")\n\n        units = extract_units(data_array)\n        expected = attach_units(func(strip_units(data_array), x=new_x), units)\n        actual = func(data_array, x=new_x)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_allclose(expected, actual)\n\n    @pytest.mark.xfail(reason=""indexes don\'t support units"")\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""func"", (method(""interp""), method(""reindex"")), ids=repr,\n    )\n    def test_interp_reindex_indexing(self, func, unit, error, dtype):\n        array = np.linspace(1, 2, 10).astype(dtype)\n        x = np.arange(10) * unit_registry.m\n        new_x = (np.arange(10) + 0.5) * unit\n        data_array = xr.DataArray(array, coords={""x"": x}, dims=""x"")\n\n        if error is not None:\n            with pytest.raises(error):\n                func(data_array, x=new_x)\n\n            return\n\n        units = extract_units(data_array)\n        expected = attach_units(\n            func(\n                strip_units(data_array),\n                x=strip_units(convert_units(new_x, {None: unit_registry.m})),\n            ),\n            units,\n        )\n        actual = func(data_array, x=new_x)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(""variant"", (""data"", ""coords""))\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            pytest.param(\n                method(""interp_like""), marks=pytest.mark.xfail(reason=""uses scipy"")\n            ),\n            method(""reindex_like""),\n        ),\n        ids=repr,\n    )\n    def test_interp_reindex_like(self, variant, func, dtype):\n        variants = {\n            ""data"": (unit_registry.m, 1),\n            ""coords"": (1, unit_registry.m),\n        }\n        data_unit, coord_unit = variants.get(variant)\n\n        array = np.linspace(1, 2, 10).astype(dtype) * data_unit\n        coord = np.arange(10) * coord_unit\n\n        x = np.arange(10)\n        new_x = np.arange(-2, 2) + 0.5\n        data_array = xr.DataArray(array, coords={""x"": x, ""y"": (""x"", coord)}, dims=""x"")\n        other = xr.DataArray(np.empty_like(new_x), coords={""x"": new_x}, dims=""x"")\n\n        units = extract_units(data_array)\n        expected = attach_units(func(strip_units(data_array), other), units)\n        actual = func(data_array, other)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_allclose(expected, actual)\n\n    @pytest.mark.xfail(reason=""indexes don\'t support units"")\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""func"", (method(""interp_like""), method(""reindex_like"")), ids=repr,\n    )\n    def test_interp_reindex_like_indexing(self, func, unit, error, dtype):\n        array = np.linspace(1, 2, 10).astype(dtype)\n        x = np.arange(10) * unit_registry.m\n        new_x = (np.arange(-2, 2) + 0.5) * unit\n\n        data_array = xr.DataArray(array, coords={""x"": x}, dims=""x"")\n        other = xr.DataArray(np.empty_like(new_x), {""x"": new_x}, dims=""x"")\n\n        if error is not None:\n            with pytest.raises(error):\n                func(data_array, other)\n\n            return\n\n        units = extract_units(data_array)\n        expected = attach_units(\n            func(\n                strip_units(data_array),\n                strip_units(convert_units(other, {None: unit_registry.m})),\n            ),\n            units,\n        )\n        actual = func(data_array, other)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (method(""unstack""), method(""reset_index"", ""z""), method(""reorder_levels"")),\n        ids=repr,\n    )\n    def test_stacking_stacked(self, func, dtype):\n        array = (\n            np.linspace(0, 10, 5 * 10).reshape(5, 10).astype(dtype) * unit_registry.m\n        )\n        x = np.arange(array.shape[0])\n        y = np.arange(array.shape[1])\n\n        data_array = xr.DataArray(\n            name=""data"", data=array, coords={""x"": x, ""y"": y}, dims=(""x"", ""y"")\n        )\n        stacked = data_array.stack(z=(""x"", ""y""))\n\n        expected = attach_units(func(strip_units(stacked)), {""data"": unit_registry.m})\n        actual = func(stacked)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.xfail(reason=""indexes don\'t support units"")\n    def test_to_unstacked_dataset(self, dtype):\n        array = (\n            np.linspace(0, 10, 5 * 10).reshape(5, 10).astype(dtype)\n            * unit_registry.pascal\n        )\n        x = np.arange(array.shape[0]) * unit_registry.m\n        y = np.arange(array.shape[1]) * unit_registry.s\n\n        data_array = xr.DataArray(\n            data=array, coords={""x"": x, ""y"": y}, dims=(""x"", ""y"")\n        ).stack(z=(""x"", ""y""))\n\n        func = method(""to_unstacked_dataset"", dim=""z"")\n\n        expected = attach_units(\n            func(strip_units(data_array)),\n            {""y"": y.units, **dict(zip(x.magnitude, [array.units] * len(y)))},\n        ).rename({elem.magnitude: elem for elem in x})\n        actual = func(data_array)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            method(""transpose"", ""y"", ""x"", ""z""),\n            method(""stack"", a=(""x"", ""y"")),\n            method(""set_index"", x=""x2""),\n            method(""shift"", x=2),\n            method(""roll"", x=2, roll_coords=False),\n            method(""sortby"", ""x2""),\n        ),\n        ids=repr,\n    )\n    def test_stacking_reordering(self, func, dtype):\n        array = (\n            np.linspace(0, 10, 2 * 5 * 10).reshape(2, 5, 10).astype(dtype)\n            * unit_registry.m\n        )\n        x = np.arange(array.shape[0])\n        y = np.arange(array.shape[1])\n        z = np.arange(array.shape[2])\n        x2 = np.linspace(0, 1, array.shape[0])[::-1]\n\n        data_array = xr.DataArray(\n            name=""data"",\n            data=array,\n            coords={""x"": x, ""y"": y, ""z"": z, ""x2"": (""x"", x2)},\n            dims=(""x"", ""y"", ""z""),\n        )\n\n        expected = attach_units(func(strip_units(data_array)), {None: unit_registry.m})\n        actual = func(data_array)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            method(""diff"", dim=""x""),\n            method(""differentiate"", coord=""x""),\n            method(""integrate"", dim=""x""),\n            pytest.param(\n                method(""quantile"", q=[0.25, 0.75]),\n                marks=pytest.mark.xfail(\n                    LooseVersion(pint.__version__) < ""0.12"",\n                    reason=""quantile / nanquantile not implemented yet"",\n                ),\n            ),\n            method(""reduce"", func=np.sum, dim=""x""),\n            pytest.param(lambda x: x.dot(x), id=""method_dot""),\n        ),\n        ids=repr,\n    )\n    def test_computation(self, func, dtype):\n        array = (\n            np.linspace(0, 10, 5 * 10).reshape(5, 10).astype(dtype) * unit_registry.m\n        )\n\n        x = np.arange(array.shape[0]) * unit_registry.m\n        y = np.arange(array.shape[1]) * unit_registry.s\n\n        data_array = xr.DataArray(data=array, coords={""x"": x, ""y"": y}, dims=(""x"", ""y""))\n\n        # we want to make sure the output unit is correct\n        units = {\n            **extract_units(data_array),\n            **(\n                {}\n                if isinstance(func, (function, method))\n                else extract_units(func(array.reshape(-1)))\n            ),\n        }\n\n        expected = attach_units(func(strip_units(data_array)), units)\n        actual = func(data_array)\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            method(""groupby"", ""x""),\n            method(""groupby_bins"", ""y"", bins=4),\n            method(""coarsen"", y=2),\n            pytest.param(\n                method(""rolling"", y=3),\n                marks=pytest.mark.xfail(\n                    reason=""numpy.lib.stride_tricks.as_strided converts to ndarray""\n                ),\n            ),\n            pytest.param(\n                method(""rolling_exp"", y=3),\n                marks=pytest.mark.xfail(reason=""units not supported by numbagg""),\n            ),\n        ),\n        ids=repr,\n    )\n    def test_computation_objects(self, func, dtype):\n        array = (\n            np.linspace(0, 10, 5 * 10).reshape(5, 10).astype(dtype) * unit_registry.m\n        )\n\n        x = np.array([0, 0, 1, 2, 2]) * unit_registry.m\n        y = np.arange(array.shape[1]) * 3 * unit_registry.s\n\n        data_array = xr.DataArray(data=array, coords={""x"": x, ""y"": y}, dims=(""x"", ""y""))\n        units = extract_units(data_array)\n\n        expected = attach_units(func(strip_units(data_array)).mean(), units)\n        actual = func(data_array).mean()\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_allclose(expected, actual)\n\n    def test_resample(self, dtype):\n        array = np.linspace(0, 5, 10).astype(dtype) * unit_registry.m\n\n        time = pd.date_range(""10-09-2010"", periods=len(array), freq=""1y"")\n        data_array = xr.DataArray(data=array, coords={""time"": time}, dims=""time"")\n        units = extract_units(data_array)\n\n        func = method(""resample"", time=""6m"")\n\n        expected = attach_units(func(strip_units(data_array)).mean(), units)\n        actual = func(data_array).mean()\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            method(""assign_coords"", z=([""x""], np.arange(5) * unit_registry.s)),\n            method(""first""),\n            method(""last""),\n            pytest.param(\n                method(""quantile"", q=[0.25, 0.5, 0.75], dim=""x""),\n                marks=pytest.mark.xfail(\n                    LooseVersion(pint.__version__) < ""0.12"",\n                    reason=""quantile / nanquantile not implemented yet"",\n                ),\n            ),\n        ),\n        ids=repr,\n    )\n    def test_grouped_operations(self, func, dtype):\n        array = (\n            np.linspace(0, 10, 5 * 10).reshape(5, 10).astype(dtype) * unit_registry.m\n        )\n\n        x = np.arange(array.shape[0]) * unit_registry.m\n        y = np.arange(array.shape[1]) * 3 * unit_registry.s\n\n        data_array = xr.DataArray(data=array, coords={""x"": x, ""y"": y}, dims=(""x"", ""y""))\n        units = {**extract_units(data_array), **{""z"": unit_registry.s, ""q"": None}}\n\n        stripped_kwargs = {\n            key: (\n                strip_units(value)\n                if not isinstance(value, tuple)\n                else tuple(strip_units(elem) for elem in value)\n            )\n            for key, value in func.kwargs.items()\n        }\n        expected = attach_units(\n            func(strip_units(data_array).groupby(""y""), **stripped_kwargs), units\n        )\n        actual = func(data_array.groupby(""y""))\n\n        assert_units_equal(expected, actual)\n        xr.testing.assert_identical(expected, actual)\n\n\nclass TestDataset:\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.mm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""same_unit""),\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""shared"",\n        (\n            ""nothing"",\n            pytest.param(""dims"", marks=pytest.mark.xfail(reason=""indexes strip units"")),\n            pytest.param(\n                ""coords"",\n                marks=pytest.mark.xfail(reason=""reindex does not work with pint yet""),\n            ),\n        ),\n    )\n    def test_init(self, shared, unit, error, dtype):\n        original_unit = unit_registry.m\n        scaled_unit = unit_registry.mm\n\n        a = np.linspace(0, 1, 10).astype(dtype) * unit_registry.Pa\n        b = np.linspace(-1, 0, 12).astype(dtype) * unit_registry.Pa\n\n        raw_x = np.arange(a.shape[0])\n        x = raw_x * original_unit\n        x2 = x.to(scaled_unit)\n\n        raw_y = np.arange(b.shape[0])\n        y = raw_y * unit\n        y_units = unit if isinstance(y, unit_registry.Quantity) else None\n        if isinstance(y, unit_registry.Quantity):\n            if y.check(scaled_unit):\n                y2 = y.to(scaled_unit)\n            else:\n                y2 = y * 1000\n            y2_units = y2.units\n        else:\n            y2 = y * 1000\n            y2_units = None\n\n        variants = {\n            ""nothing"": ({""x"": x, ""x2"": (""x"", x2)}, {""y"": y, ""y2"": (""y"", y2)}),\n            ""dims"": (\n                {""x"": x, ""x2"": (""x"", strip_units(x2))},\n                {""x"": y, ""y2"": (""x"", strip_units(y2))},\n            ),\n            ""coords"": ({""x"": raw_x, ""y"": (""x"", x2)}, {""x"": raw_y, ""y"": (""x"", y2)}),\n        }\n        coords_a, coords_b = variants.get(shared)\n\n        dims_a, dims_b = (""x"", ""y"") if shared == ""nothing"" else (""x"", ""x"")\n\n        arr1 = xr.DataArray(data=a, coords=coords_a, dims=dims_a)\n        arr2 = xr.DataArray(data=b, coords=coords_b, dims=dims_b)\n        if error is not None and shared != ""nothing"":\n            with pytest.raises(error):\n                xr.Dataset(data_vars={""a"": arr1, ""b"": arr2})\n\n            return\n\n        actual = xr.Dataset(data_vars={""a"": arr1, ""b"": arr2})\n\n        expected_units = {\n            ""a"": a.units,\n            ""b"": b.units,\n            ""x"": x.units,\n            ""x2"": x2.units,\n            ""y"": y_units,\n            ""y2"": y2_units,\n        }\n        expected = attach_units(\n            xr.Dataset(data_vars={""a"": strip_units(arr1), ""b"": strip_units(arr2)}),\n            expected_units,\n        )\n        assert_equal_with_units(actual, expected)\n\n    @pytest.mark.parametrize(\n        ""func"", (pytest.param(str, id=""str""), pytest.param(repr, id=""repr""))\n    )\n    @pytest.mark.parametrize(\n        ""variant"",\n        (\n            pytest.param(\n                ""with_dims"",\n                marks=pytest.mark.xfail(reason=""units in indexes are not supported""),\n            ),\n            pytest.param(""with_coords""),\n            pytest.param(""without_coords""),\n        ),\n    )\n    @pytest.mark.filterwarnings(""error:::pint[.*]"")\n    def test_repr(self, func, variant, dtype):\n        array1 = np.linspace(1, 2, 10, dtype=dtype) * unit_registry.Pa\n        array2 = np.linspace(0, 1, 10, dtype=dtype) * unit_registry.degK\n\n        x = np.arange(len(array1)) * unit_registry.s\n        y = x.to(unit_registry.ms)\n\n        variants = {\n            ""with_dims"": {""x"": x},\n            ""with_coords"": {""y"": (""x"", y)},\n            ""without_coords"": {},\n        }\n\n        data_array = xr.Dataset(\n            data_vars={""a"": (""x"", array1), ""b"": (""x"", array2)},\n            coords=variants.get(variant),\n        )\n\n        # FIXME: this just checks that the repr does not raise\n        # warnings or errors, but does not check the result\n        func(data_array)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            pytest.param(\n                function(""all""),\n                marks=pytest.mark.xfail(reason=""not implemented by pint""),\n            ),\n            pytest.param(\n                function(""any""),\n                marks=pytest.mark.xfail(reason=""not implemented by pint""),\n            ),\n            function(""argmax""),\n            function(""argmin""),\n            function(""max""),\n            function(""min""),\n            function(""mean""),\n            pytest.param(\n                function(""median""),\n                marks=pytest.mark.xfail(\n                    reason=""np.median does not work with dataset yet""\n                ),\n            ),\n            function(""sum""),\n            pytest.param(\n                function(""prod""),\n                marks=pytest.mark.xfail(reason=""not implemented by pint""),\n            ),\n            function(""std""),\n            function(""var""),\n            function(""cumsum""),\n            pytest.param(\n                function(""cumprod""),\n                marks=pytest.mark.xfail(reason=""fails within xarray""),\n            ),\n            pytest.param(\n                method(""all""), marks=pytest.mark.xfail(reason=""not implemented by pint"")\n            ),\n            pytest.param(\n                method(""any""), marks=pytest.mark.xfail(reason=""not implemented by pint"")\n            ),\n            method(""argmax""),\n            method(""argmin""),\n            method(""max""),\n            method(""min""),\n            method(""mean""),\n            method(""median""),\n            method(""sum""),\n            pytest.param(\n                method(""prod""),\n                marks=pytest.mark.xfail(reason=""not implemented by pint""),\n            ),\n            method(""std""),\n            method(""var""),\n            method(""cumsum""),\n            pytest.param(\n                method(""cumprod""), marks=pytest.mark.xfail(reason=""fails within xarray"")\n            ),\n        ),\n        ids=repr,\n    )\n    def test_aggregation(self, func, dtype):\n        unit_a = (\n            unit_registry.Pa if func.name != ""cumprod"" else unit_registry.dimensionless\n        )\n        unit_b = (\n            unit_registry.kg / unit_registry.m ** 3\n            if func.name != ""cumprod""\n            else unit_registry.dimensionless\n        )\n        a = xr.DataArray(data=np.linspace(0, 1, 10).astype(dtype) * unit_a, dims=""x"")\n        b = xr.DataArray(data=np.linspace(-1, 0, 10).astype(dtype) * unit_b, dims=""x"")\n        x = xr.DataArray(data=np.arange(10).astype(dtype) * unit_registry.m, dims=""x"")\n        y = xr.DataArray(\n            data=np.arange(10, 20).astype(dtype) * unit_registry.s, dims=""x""\n        )\n\n        ds = xr.Dataset(data_vars={""a"": a, ""b"": b}, coords={""x"": x, ""y"": y})\n\n        actual = func(ds)\n        expected = attach_units(\n            func(strip_units(ds)),\n            {\n                ""a"": extract_units(func(a)).get(None),\n                ""b"": extract_units(func(b)).get(None),\n            },\n        )\n\n        assert_equal_with_units(actual, expected)\n\n    @pytest.mark.parametrize(""property"", (""imag"", ""real""))\n    def test_numpy_properties(self, property, dtype):\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(\n                    data=np.linspace(0, 1, 10) * unit_registry.Pa, dims=""x""\n                ),\n                ""b"": xr.DataArray(\n                    data=np.linspace(-1, 0, 15) * unit_registry.Pa, dims=""y""\n                ),\n            },\n            coords={\n                ""x"": np.arange(10) * unit_registry.m,\n                ""y"": np.arange(15) * unit_registry.s,\n            },\n        )\n        units = extract_units(ds)\n\n        actual = getattr(ds, property)\n        expected = attach_units(getattr(strip_units(ds), property), units)\n\n        assert_equal_with_units(actual, expected)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            method(""astype"", float),\n            method(""conj""),\n            method(""argsort""),\n            method(""conjugate""),\n            method(""round""),\n        ),\n        ids=repr,\n    )\n    def test_numpy_methods(self, func, dtype):\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(\n                    data=np.linspace(1, -1, 10) * unit_registry.Pa, dims=""x""\n                ),\n                ""b"": xr.DataArray(\n                    data=np.linspace(-1, 1, 15) * unit_registry.Pa, dims=""y""\n                ),\n            },\n            coords={\n                ""x"": np.arange(10) * unit_registry.m,\n                ""y"": np.arange(15) * unit_registry.s,\n            },\n        )\n        units = {\n            ""a"": array_extract_units(func(ds.a)),\n            ""b"": array_extract_units(func(ds.b)),\n            ""x"": unit_registry.m,\n            ""y"": unit_registry.s,\n        }\n\n        actual = func(ds)\n        expected = attach_units(func(strip_units(ds)), units)\n\n        assert_equal_with_units(actual, expected)\n\n    @pytest.mark.parametrize(""func"", (method(""clip"", min=3, max=8),), ids=repr)\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_numpy_methods_with_args(self, func, unit, error, dtype):\n        data_unit = unit_registry.m\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=np.arange(10) * data_unit, dims=""x""),\n                ""b"": xr.DataArray(data=np.arange(15) * data_unit, dims=""y""),\n            },\n            coords={\n                ""x"": np.arange(10) * unit_registry.m,\n                ""y"": np.arange(15) * unit_registry.s,\n            },\n        )\n        units = extract_units(ds)\n\n        kwargs = {\n            key: (value * unit if isinstance(value, (int, float)) else value)\n            for key, value in func.kwargs.items()\n        }\n\n        if error is not None:\n            with pytest.raises(error):\n                func(ds, **kwargs)\n\n            return\n\n        stripped_kwargs = {\n            key: strip_units(convert_units(value, {None: data_unit}))\n            for key, value in kwargs.items()\n        }\n\n        actual = func(ds, **kwargs)\n        expected = attach_units(func(strip_units(ds), **stripped_kwargs), units)\n\n        assert_equal_with_units(actual, expected)\n\n    @pytest.mark.parametrize(\n        ""func"", (method(""isnull""), method(""notnull""), method(""count"")), ids=repr\n    )\n    def test_missing_value_detection(self, func, dtype):\n        array1 = (\n            np.array(\n                [\n                    [1.4, 2.3, np.nan, 7.2],\n                    [np.nan, 9.7, np.nan, np.nan],\n                    [2.1, np.nan, np.nan, 4.6],\n                    [9.9, np.nan, 7.2, 9.1],\n                ]\n            )\n            * unit_registry.degK\n        )\n        array2 = (\n            np.array(\n                [\n                    [np.nan, 5.7, 12.0, 7.2],\n                    [np.nan, 12.4, np.nan, 4.2],\n                    [9.8, np.nan, 4.6, 1.4],\n                    [7.2, np.nan, 6.3, np.nan],\n                    [8.4, 3.9, np.nan, np.nan],\n                ]\n            )\n            * unit_registry.Pa\n        )\n\n        x = np.arange(array1.shape[0]) * unit_registry.m\n        y = np.arange(array1.shape[1]) * unit_registry.m\n        z = np.arange(array2.shape[0]) * unit_registry.m\n\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=(""x"", ""y"")),\n                ""b"": xr.DataArray(data=array2, dims=(""z"", ""x"")),\n            },\n            coords={""x"": x, ""y"": y, ""z"": z},\n        )\n\n        expected = func(strip_units(ds))\n        actual = func(ds)\n\n        assert_equal_with_units(expected, actual)\n\n    @pytest.mark.xfail(reason=""ffill and bfill lose the unit"")\n    @pytest.mark.parametrize(""func"", (method(""ffill""), method(""bfill"")), ids=repr)\n    def test_missing_value_filling(self, func, dtype):\n        array1 = (\n            np.array([1.4, np.nan, 2.3, np.nan, np.nan, 9.1]).astype(dtype)\n            * unit_registry.degK\n        )\n        array2 = (\n            np.array([4.3, 9.8, 7.5, np.nan, 8.2, np.nan]).astype(dtype)\n            * unit_registry.Pa\n        )\n\n        x = np.arange(len(array1))\n\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=""x""),\n                ""b"": xr.DataArray(data=array2, dims=""x""),\n            },\n            coords={""x"": x},\n        )\n\n        expected = attach_units(\n            func(strip_units(ds), dim=""x""),\n            {""a"": unit_registry.degK, ""b"": unit_registry.Pa},\n        )\n        actual = func(ds, dim=""x"")\n\n        assert_equal_with_units(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(\n                unit_registry.cm,\n                None,\n                id=""compatible_unit"",\n                marks=pytest.mark.xfail(\n                    reason=""where converts the array, not the fill value""\n                ),\n            ),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""fill_value"",\n        (\n            pytest.param(-1, id=""python_scalar""),\n            pytest.param(np.array(-1), id=""numpy_scalar""),\n            pytest.param(np.array([-1]), id=""numpy_array""),\n        ),\n    )\n    def test_fillna(self, fill_value, unit, error, dtype):\n        array1 = (\n            np.array([1.4, np.nan, 2.3, np.nan, np.nan, 9.1]).astype(dtype)\n            * unit_registry.m\n        )\n        array2 = (\n            np.array([4.3, 9.8, 7.5, np.nan, 8.2, np.nan]).astype(dtype)\n            * unit_registry.m\n        )\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=""x""),\n                ""b"": xr.DataArray(data=array2, dims=""x""),\n            }\n        )\n\n        if error is not None:\n            with pytest.raises(error):\n                ds.fillna(value=fill_value * unit)\n\n            return\n\n        actual = ds.fillna(value=fill_value * unit)\n        expected = attach_units(\n            strip_units(ds).fillna(\n                value=strip_units(\n                    convert_units(fill_value * unit, {None: unit_registry.m})\n                )\n            ),\n            {""a"": unit_registry.m, ""b"": unit_registry.m},\n        )\n\n        assert_equal_with_units(expected, actual)\n\n    def test_dropna(self, dtype):\n        array1 = (\n            np.array([1.4, np.nan, 2.3, np.nan, np.nan, 9.1]).astype(dtype)\n            * unit_registry.degK\n        )\n        array2 = (\n            np.array([4.3, 9.8, 7.5, np.nan, 8.2, np.nan]).astype(dtype)\n            * unit_registry.Pa\n        )\n        x = np.arange(len(array1))\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=""x""),\n                ""b"": xr.DataArray(data=array2, dims=""x""),\n            },\n            coords={""x"": x},\n        )\n\n        expected = attach_units(\n            strip_units(ds).dropna(dim=""x""),\n            {""a"": unit_registry.degK, ""b"": unit_registry.Pa},\n        )\n        actual = ds.dropna(dim=""x"")\n\n        assert_equal_with_units(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""unit"",\n        (\n            pytest.param(1, id=""no_unit""),\n            pytest.param(unit_registry.dimensionless, id=""dimensionless""),\n            pytest.param(unit_registry.s, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, id=""compatible_unit""),\n            pytest.param(unit_registry.m, id=""same_unit""),\n        ),\n    )\n    def test_isin(self, unit, dtype):\n        array1 = (\n            np.array([1.4, np.nan, 2.3, np.nan, np.nan, 9.1]).astype(dtype)\n            * unit_registry.m\n        )\n        array2 = (\n            np.array([4.3, 9.8, 7.5, np.nan, 8.2, np.nan]).astype(dtype)\n            * unit_registry.m\n        )\n        x = np.arange(len(array1))\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=""x""),\n                ""b"": xr.DataArray(data=array2, dims=""x""),\n            },\n            coords={""x"": x},\n        )\n\n        raw_values = np.array([1.4, np.nan, 2.3]).astype(dtype)\n        values = raw_values * unit\n\n        if (\n            isinstance(values, unit_registry.Quantity)\n            and values.check(unit_registry.m)\n            and unit != unit_registry.m\n        ):\n            raw_values = values.to(unit_registry.m).magnitude\n\n        expected = strip_units(ds).isin(raw_values)\n        if not isinstance(values, unit_registry.Quantity) or not values.check(\n            unit_registry.m\n        ):\n            expected.a[:] = False\n            expected.b[:] = False\n        actual = ds.isin(values)\n\n        assert_equal_with_units(actual, expected)\n\n    @pytest.mark.parametrize(\n        ""variant"", (""masking"", ""replacing_scalar"", ""replacing_array"", ""dropping"")\n    )\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""same_unit""),\n        ),\n    )\n    def test_where(self, variant, unit, error, dtype):\n        original_unit = unit_registry.m\n        array1 = np.linspace(0, 1, 10).astype(dtype) * original_unit\n        array2 = np.linspace(-1, 0, 10).astype(dtype) * original_unit\n\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=""x""),\n                ""b"": xr.DataArray(data=array2, dims=""x""),\n            },\n            coords={""x"": np.arange(len(array1))},\n        )\n\n        condition = ds < 0.5 * original_unit\n        other = np.linspace(-2, -1, 10).astype(dtype) * unit\n        variant_kwargs = {\n            ""masking"": {""cond"": condition},\n            ""replacing_scalar"": {""cond"": condition, ""other"": -1 * unit},\n            ""replacing_array"": {""cond"": condition, ""other"": other},\n            ""dropping"": {""cond"": condition, ""drop"": True},\n        }\n        kwargs = variant_kwargs.get(variant)\n        if variant not in (""masking"", ""dropping"") and error is not None:\n            with pytest.raises(error):\n                ds.where(**kwargs)\n\n            return\n\n        kwargs_without_units = {\n            key: strip_units(convert_units(value, {None: original_unit}))\n            for key, value in kwargs.items()\n        }\n\n        expected = attach_units(\n            strip_units(ds).where(**kwargs_without_units),\n            {""a"": original_unit, ""b"": original_unit},\n        )\n        actual = ds.where(**kwargs)\n\n        assert_equal_with_units(expected, actual)\n\n    @pytest.mark.xfail(reason=""interpolate strips units"")\n    def test_interpolate_na(self, dtype):\n        array1 = (\n            np.array([1.4, np.nan, 2.3, np.nan, np.nan, 9.1]).astype(dtype)\n            * unit_registry.degK\n        )\n        array2 = (\n            np.array([4.3, 9.8, 7.5, np.nan, 8.2, np.nan]).astype(dtype)\n            * unit_registry.Pa\n        )\n        x = np.arange(len(array1))\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=""x""),\n                ""b"": xr.DataArray(data=array2, dims=""x""),\n            },\n            coords={""x"": x},\n        )\n\n        expected = attach_units(\n            strip_units(ds).interpolate_na(dim=""x""),\n            {""a"": unit_registry.degK, ""b"": unit_registry.Pa},\n        )\n        actual = ds.interpolate_na(dim=""x"")\n\n        assert_equal_with_units(expected, actual)\n\n    @pytest.mark.xfail(reason=""wrong argument order for `where`"")\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""same_unit""),\n        ),\n    )\n    def test_combine_first(self, unit, error, dtype):\n        array1 = (\n            np.array([1.4, np.nan, 2.3, np.nan, np.nan, 9.1]).astype(dtype)\n            * unit_registry.m\n        )\n        array2 = (\n            np.array([4.3, 9.8, 7.5, np.nan, 8.2, np.nan]).astype(dtype)\n            * unit_registry.m\n        )\n        x = np.arange(len(array1))\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=""x""),\n                ""b"": xr.DataArray(data=array2, dims=""x""),\n            },\n            coords={""x"": x},\n        )\n        other_array1 = np.ones_like(array1) * unit\n        other_array2 = -1 * np.ones_like(array2) * unit\n        other = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=other_array1, dims=""x""),\n                ""b"": xr.DataArray(data=other_array2, dims=""x""),\n            },\n            coords={""x"": np.arange(array1.shape[0])},\n        )\n\n        if error is not None:\n            with pytest.raises(error):\n                ds.combine_first(other)\n\n            return\n\n        expected = attach_units(\n            strip_units(ds).combine_first(\n                strip_units(\n                    convert_units(other, {""a"": unit_registry.m, ""b"": unit_registry.m})\n                )\n            ),\n            {""a"": unit_registry.m, ""b"": unit_registry.m},\n        )\n        actual = ds.combine_first(other)\n\n        assert_equal_with_units(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""unit"",\n        (\n            pytest.param(1, id=""no_unit""),\n            pytest.param(unit_registry.dimensionless, id=""dimensionless""),\n            pytest.param(unit_registry.s, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, id=""compatible_unit""),\n            pytest.param(unit_registry.m, id=""identical_unit""),\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""variation"",\n        (\n            ""data"",\n            pytest.param(\n                ""dims"", marks=pytest.mark.xfail(reason=""units in indexes not supported"")\n            ),\n            ""coords"",\n        ),\n    )\n    @pytest.mark.parametrize(""func"", (method(""equals""), method(""identical"")), ids=repr)\n    def test_comparisons(self, func, variation, unit, dtype):\n        def is_compatible(a, b):\n            a = a if a is not None else 1\n            b = b if b is not None else 1\n            quantity = np.arange(5) * a\n\n            return a == b or quantity.check(b)\n\n        array1 = np.linspace(0, 5, 10).astype(dtype)\n        array2 = np.linspace(-5, 0, 10).astype(dtype)\n\n        coord = np.arange(len(array1)).astype(dtype)\n\n        original_unit = unit_registry.m\n        quantity1 = array1 * original_unit\n        quantity2 = array2 * original_unit\n        x = coord * original_unit\n        y = coord * original_unit\n\n        units = {""data"": (unit, 1, 1), ""dims"": (1, unit, 1), ""coords"": (1, 1, unit)}\n        data_unit, dim_unit, coord_unit = units.get(variation)\n\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=quantity1, dims=""x""),\n                ""b"": xr.DataArray(data=quantity2, dims=""x""),\n            },\n            coords={""x"": x, ""y"": (""x"", y)},\n        )\n\n        other_units = {\n            ""a"": data_unit if quantity1.check(data_unit) else None,\n            ""b"": data_unit if quantity2.check(data_unit) else None,\n            ""x"": dim_unit if x.check(dim_unit) else None,\n            ""y"": coord_unit if y.check(coord_unit) else None,\n        }\n        other = attach_units(strip_units(convert_units(ds, other_units)), other_units)\n\n        units = extract_units(ds)\n        other_units = extract_units(other)\n\n        equal_ds = all(\n            is_compatible(units[name], other_units[name]) for name in units.keys()\n        ) and (strip_units(ds).equals(strip_units(convert_units(other, units))))\n        equal_units = units == other_units\n        expected = equal_ds and (func.name != ""identical"" or equal_units)\n\n        actual = func(ds, other)\n\n        assert expected == actual\n\n    @pytest.mark.parametrize(\n        ""unit"",\n        (\n            pytest.param(1, id=""no_unit""),\n            pytest.param(unit_registry.dimensionless, id=""dimensionless""),\n            pytest.param(unit_registry.s, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, id=""compatible_unit""),\n            pytest.param(unit_registry.m, id=""identical_unit""),\n        ),\n    )\n    def test_broadcast_like(self, unit, dtype):\n        array1 = np.linspace(1, 2, 2 * 1).reshape(2, 1).astype(dtype) * unit_registry.Pa\n        array2 = np.linspace(0, 1, 2 * 3).reshape(2, 3).astype(dtype) * unit_registry.Pa\n\n        x1 = np.arange(2) * unit_registry.m\n        x2 = np.arange(2) * unit\n        y1 = np.array([0]) * unit_registry.m\n        y2 = np.arange(3) * unit\n\n        ds1 = xr.Dataset(\n            data_vars={""a"": ((""x"", ""y""), array1)}, coords={""x"": x1, ""y"": y1}\n        )\n        ds2 = xr.Dataset(\n            data_vars={""a"": ((""x"", ""y""), array2)}, coords={""x"": x2, ""y"": y2}\n        )\n\n        expected = attach_units(\n            strip_units(ds1).broadcast_like(strip_units(ds2)), extract_units(ds1)\n        )\n        actual = ds1.broadcast_like(ds2)\n\n        assert_equal_with_units(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""unit"",\n        (\n            pytest.param(1, id=""no_unit""),\n            pytest.param(unit_registry.dimensionless, id=""dimensionless""),\n            pytest.param(unit_registry.s, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, id=""compatible_unit""),\n            pytest.param(unit_registry.m, id=""identical_unit""),\n        ),\n    )\n    def test_broadcast_equals(self, unit, dtype):\n        left_array1 = np.ones(shape=(2, 3), dtype=dtype) * unit_registry.m\n        left_array2 = np.zeros(shape=(3, 6), dtype=dtype) * unit_registry.m\n\n        right_array1 = np.ones(shape=(2,)) * unit\n        right_array2 = np.ones(shape=(3,)) * unit\n\n        left = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=left_array1, dims=(""x"", ""y"")),\n                ""b"": xr.DataArray(data=left_array2, dims=(""y"", ""z"")),\n            }\n        )\n        right = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=right_array1, dims=""x""),\n                ""b"": xr.DataArray(data=right_array2, dims=""y""),\n            }\n        )\n\n        units = {\n            **extract_units(left),\n            **({} if left_array1.check(unit) else {""a"": None, ""b"": None}),\n        }\n        expected = strip_units(left).broadcast_equals(\n            strip_units(convert_units(right, units))\n        ) & left_array1.check(unit)\n        actual = left.broadcast_equals(right)\n\n        assert expected == actual\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (method(""unstack""), method(""reset_index"", ""v""), method(""reorder_levels"")),\n        ids=repr,\n    )\n    def test_stacking_stacked(self, func, dtype):\n        array1 = (\n            np.linspace(0, 10, 5 * 10).reshape(5, 10).astype(dtype) * unit_registry.m\n        )\n        array2 = (\n            np.linspace(-10, 0, 5 * 10 * 15).reshape(5, 10, 15).astype(dtype)\n            * unit_registry.m\n        )\n\n        x = np.arange(array1.shape[0])\n        y = np.arange(array1.shape[1])\n        z = np.arange(array2.shape[2])\n\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=(""x"", ""y"")),\n                ""b"": xr.DataArray(data=array2, dims=(""x"", ""y"", ""z"")),\n            },\n            coords={""x"": x, ""y"": y, ""z"": z},\n        )\n\n        stacked = ds.stack(v=(""x"", ""y""))\n\n        expected = attach_units(\n            func(strip_units(stacked)), {""a"": unit_registry.m, ""b"": unit_registry.m}\n        )\n        actual = func(stacked)\n\n        assert_equal_with_units(expected, actual)\n\n    @pytest.mark.xfail(reason=""does not work with quantities yet"")\n    def test_to_stacked_array(self, dtype):\n        labels = np.arange(5).astype(dtype) * unit_registry.s\n        arrays = {name: np.linspace(0, 1, 10) * unit_registry.m for name in labels}\n\n        ds = xr.Dataset(\n            data_vars={\n                name: xr.DataArray(data=array, dims=""x"")\n                for name, array in arrays.items()\n            }\n        )\n\n        func = method(""to_stacked_array"", ""z"", variable_dim=""y"", sample_dims=[""x""])\n\n        actual = func(ds).rename(None)\n        expected = attach_units(\n            func(strip_units(ds)).rename(None),\n            {None: unit_registry.m, ""y"": unit_registry.s},\n        )\n\n        assert_equal_with_units(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            method(""transpose"", ""y"", ""x"", ""z1"", ""z2""),\n            method(""stack"", a=(""x"", ""y"")),\n            method(""set_index"", x=""x2""),\n            pytest.param(\n                method(""shift"", x=2),\n                marks=pytest.mark.xfail(reason=""tries to concatenate nan arrays""),\n            ),\n            method(""roll"", x=2, roll_coords=False),\n            method(""sortby"", ""x2""),\n        ),\n        ids=repr,\n    )\n    def test_stacking_reordering(self, func, dtype):\n        array1 = (\n            np.linspace(0, 10, 2 * 5 * 10).reshape(2, 5, 10).astype(dtype)\n            * unit_registry.Pa\n        )\n        array2 = (\n            np.linspace(0, 10, 2 * 5 * 15).reshape(2, 5, 15).astype(dtype)\n            * unit_registry.degK\n        )\n\n        x = np.arange(array1.shape[0])\n        y = np.arange(array1.shape[1])\n        z1 = np.arange(array1.shape[2])\n        z2 = np.arange(array2.shape[2])\n\n        x2 = np.linspace(0, 1, array1.shape[0])[::-1]\n\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=(""x"", ""y"", ""z1"")),\n                ""b"": xr.DataArray(data=array2, dims=(""x"", ""y"", ""z2"")),\n            },\n            coords={""x"": x, ""y"": y, ""z1"": z1, ""z2"": z2, ""x2"": (""x"", x2)},\n        )\n\n        expected = attach_units(\n            func(strip_units(ds)), {""a"": unit_registry.Pa, ""b"": unit_registry.degK}\n        )\n        actual = func(ds)\n\n        assert_equal_with_units(expected, actual)\n\n    @pytest.mark.xfail(reason=""indexes strip units"")\n    @pytest.mark.parametrize(\n        ""indices"",\n        (\n            pytest.param(4, id=""single index""),\n            pytest.param([5, 2, 9, 1], id=""multiple indices""),\n        ),\n    )\n    def test_isel(self, indices, dtype):\n        array1 = np.arange(10).astype(dtype) * unit_registry.s\n        array2 = np.linspace(0, 1, 10).astype(dtype) * unit_registry.Pa\n\n        x = np.arange(len(array1)) * unit_registry.m\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=""x""),\n                ""b"": xr.DataArray(data=array2, dims=""x""),\n            },\n            coords={""x"": x},\n        )\n\n        expected = attach_units(\n            strip_units(ds).isel(x=indices),\n            {""a"": unit_registry.s, ""b"": unit_registry.Pa, ""x"": unit_registry.m},\n        )\n        actual = ds.isel(x=indices)\n\n        assert_equal_with_units(expected, actual)\n\n    @pytest.mark.xfail(reason=""indexes don\'t support units"")\n    @pytest.mark.parametrize(\n        ""raw_values"",\n        (\n            pytest.param(10, id=""single_value""),\n            pytest.param([10, 5, 13], id=""list_of_values""),\n            pytest.param(np.array([9, 3, 7, 12]), id=""array_of_values""),\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, KeyError, id=""no_units""),\n            pytest.param(unit_registry.dimensionless, KeyError, id=""dimensionless""),\n            pytest.param(unit_registry.degree, KeyError, id=""incompatible_unit""),\n            pytest.param(unit_registry.dm, KeyError, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_sel(self, raw_values, unit, error, dtype):\n        array1 = np.linspace(5, 10, 20).astype(dtype) * unit_registry.degK\n        array2 = np.linspace(0, 5, 20).astype(dtype) * unit_registry.Pa\n        x = np.arange(len(array1)) * unit_registry.m\n\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=""x""),\n                ""b"": xr.DataArray(data=array2, dims=""x""),\n            },\n            coords={""x"": x},\n        )\n\n        values = raw_values * unit\n\n        if error is not None and not (\n            isinstance(raw_values, (int, float)) and x.check(unit)\n        ):\n            with pytest.raises(error):\n                ds.sel(x=values)\n\n            return\n\n        expected = attach_units(\n            strip_units(ds).sel(x=strip_units(convert_units(values, {None: x.units}))),\n            {""a"": array1.units, ""b"": array2.units, ""x"": x.units},\n        )\n        actual = ds.sel(x=values)\n        assert_equal_with_units(expected, actual)\n\n    @pytest.mark.xfail(reason=""indexes don\'t support units"")\n    @pytest.mark.parametrize(\n        ""raw_values"",\n        (\n            pytest.param(10, id=""single_value""),\n            pytest.param([10, 5, 13], id=""list_of_values""),\n            pytest.param(np.array([9, 3, 7, 12]), id=""array_of_values""),\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, KeyError, id=""no_units""),\n            pytest.param(unit_registry.dimensionless, KeyError, id=""dimensionless""),\n            pytest.param(unit_registry.degree, KeyError, id=""incompatible_unit""),\n            pytest.param(unit_registry.dm, KeyError, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_drop_sel(self, raw_values, unit, error, dtype):\n        array1 = np.linspace(5, 10, 20).astype(dtype) * unit_registry.degK\n        array2 = np.linspace(0, 5, 20).astype(dtype) * unit_registry.Pa\n        x = np.arange(len(array1)) * unit_registry.m\n\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=""x""),\n                ""b"": xr.DataArray(data=array2, dims=""x""),\n            },\n            coords={""x"": x},\n        )\n\n        values = raw_values * unit\n\n        if error is not None and not (\n            isinstance(raw_values, (int, float)) and x.check(unit)\n        ):\n            with pytest.raises(error):\n                ds.drop_sel(x=values)\n\n            return\n\n        expected = attach_units(\n            strip_units(ds).drop_sel(\n                x=strip_units(convert_units(values, {None: x.units}))\n            ),\n            extract_units(ds),\n        )\n        actual = ds.drop_sel(x=values)\n        assert_equal_with_units(expected, actual)\n\n    @pytest.mark.xfail(reason=""indexes don\'t support units"")\n    @pytest.mark.parametrize(\n        ""raw_values"",\n        (\n            pytest.param(10, id=""single_value""),\n            pytest.param([10, 5, 13], id=""list_of_values""),\n            pytest.param(np.array([9, 3, 7, 12]), id=""array_of_values""),\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, KeyError, id=""no_units""),\n            pytest.param(unit_registry.dimensionless, KeyError, id=""dimensionless""),\n            pytest.param(unit_registry.degree, KeyError, id=""incompatible_unit""),\n            pytest.param(unit_registry.dm, KeyError, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_loc(self, raw_values, unit, error, dtype):\n        array1 = np.linspace(5, 10, 20).astype(dtype) * unit_registry.degK\n        array2 = np.linspace(0, 5, 20).astype(dtype) * unit_registry.Pa\n        x = np.arange(len(array1)) * unit_registry.m\n\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=""x""),\n                ""b"": xr.DataArray(data=array2, dims=""x""),\n            },\n            coords={""x"": x},\n        )\n\n        values = raw_values * unit\n\n        if error is not None and not (\n            isinstance(raw_values, (int, float)) and x.check(unit)\n        ):\n            with pytest.raises(error):\n                ds.loc[{""x"": values}]\n\n            return\n\n        expected = attach_units(\n            strip_units(ds).loc[\n                {""x"": strip_units(convert_units(values, {None: x.units}))}\n            ],\n            {""a"": array1.units, ""b"": array2.units, ""x"": x.units},\n        )\n        actual = ds.loc[{""x"": values}]\n        assert_equal_with_units(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            method(""head"", x=7, y=3, z=6),\n            method(""tail"", x=7, y=3, z=6),\n            method(""thin"", x=7, y=3, z=6),\n        ),\n        ids=repr,\n    )\n    def test_head_tail_thin(self, func, dtype):\n        array1 = np.linspace(1, 2, 10 * 5).reshape(10, 5) * unit_registry.degK\n        array2 = np.linspace(1, 2, 10 * 8).reshape(10, 8) * unit_registry.Pa\n\n        coords = {\n            ""x"": np.arange(10) * unit_registry.m,\n            ""y"": np.arange(5) * unit_registry.m,\n            ""z"": np.arange(8) * unit_registry.m,\n        }\n\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=(""x"", ""y"")),\n                ""b"": xr.DataArray(data=array2, dims=(""x"", ""z"")),\n            },\n            coords=coords,\n        )\n\n        expected = attach_units(func(strip_units(ds)), extract_units(ds))\n        actual = func(ds)\n\n        assert_equal_with_units(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""shape"",\n        (\n            pytest.param((10, 20), id=""nothing squeezable""),\n            pytest.param((10, 20, 1), id=""last dimension squeezable""),\n            pytest.param((10, 1, 20), id=""middle dimension squeezable""),\n            pytest.param((1, 10, 20), id=""first dimension squeezable""),\n            pytest.param((1, 10, 1, 20), id=""first and last dimension squeezable""),\n        ),\n    )\n    def test_squeeze(self, shape, dtype):\n        names = ""xyzt""\n        coords = {\n            name: np.arange(length).astype(dtype)\n            * (unit_registry.m if name != ""t"" else unit_registry.s)\n            for name, length in zip(names, shape)\n        }\n        array1 = (\n            np.linspace(0, 1, 10 * 20).astype(dtype).reshape(shape) * unit_registry.degK\n        )\n        array2 = (\n            np.linspace(1, 2, 10 * 20).astype(dtype).reshape(shape) * unit_registry.Pa\n        )\n\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=tuple(names[: len(shape)])),\n                ""b"": xr.DataArray(data=array2, dims=tuple(names[: len(shape)])),\n            },\n            coords=coords,\n        )\n        units = extract_units(ds)\n\n        expected = attach_units(strip_units(ds).squeeze(), units)\n\n        actual = ds.squeeze()\n        assert_equal_with_units(actual, expected)\n\n        # try squeezing the dimensions separately\n        names = tuple(dim for dim, coord in coords.items() if len(coord) == 1)\n        for name in names:\n            expected = attach_units(strip_units(ds).squeeze(dim=name), units)\n            actual = ds.squeeze(dim=name)\n            assert_equal_with_units(actual, expected)\n\n    @pytest.mark.xfail(reason=""ignores units"")\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_interp(self, unit, error):\n        array1 = np.linspace(1, 2, 10 * 5).reshape(10, 5) * unit_registry.degK\n        array2 = np.linspace(1, 2, 10 * 8).reshape(10, 8) * unit_registry.Pa\n\n        coords = {\n            ""x"": np.arange(10) * unit_registry.m,\n            ""y"": np.arange(5) * unit_registry.m,\n            ""z"": np.arange(8) * unit_registry.s,\n        }\n\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=(""x"", ""y"")),\n                ""b"": xr.DataArray(data=array2, dims=(""x"", ""z"")),\n            },\n            coords=coords,\n        )\n\n        new_coords = (np.arange(10) + 0.5) * unit\n\n        if error is not None:\n            with pytest.raises(error):\n                ds.interp(x=new_coords)\n\n            return\n\n        units = extract_units(ds)\n        expected = attach_units(\n            strip_units(ds).interp(x=strip_units(convert_units(new_coords, units))),\n            units,\n        )\n        actual = ds.interp(x=new_coords)\n\n        assert_equal_with_units(actual, expected)\n\n    @pytest.mark.xfail(reason=""ignores units"")\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_interp_like(self, unit, error, dtype):\n        array1 = (\n            np.linspace(0, 10, 10 * 5).reshape(10, 5).astype(dtype) * unit_registry.degK\n        )\n        array2 = (\n            np.linspace(10, 20, 10 * 8).reshape(10, 8).astype(dtype) * unit_registry.Pa\n        )\n\n        coords = {\n            ""x"": np.arange(10) * unit_registry.m,\n            ""y"": np.arange(5) * unit_registry.m,\n            ""z"": np.arange(8) * unit_registry.m,\n        }\n\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=(""x"", ""y"")),\n                ""b"": xr.DataArray(data=array2, dims=(""x"", ""z"")),\n            },\n            coords=coords,\n        )\n\n        other = xr.Dataset(\n            data_vars={\n                ""c"": xr.DataArray(data=np.empty((20, 10)), dims=(""x"", ""y"")),\n                ""d"": xr.DataArray(data=np.empty((20, 15)), dims=(""x"", ""z"")),\n            },\n            coords={\n                ""x"": (np.arange(20) + 0.3) * unit,\n                ""y"": (np.arange(10) - 0.2) * unit,\n                ""z"": (np.arange(15) + 0.4) * unit,\n            },\n        )\n\n        if error is not None:\n            with pytest.raises(error):\n                ds.interp_like(other)\n\n            return\n\n        units = extract_units(ds)\n        expected = attach_units(\n            strip_units(ds).interp_like(strip_units(convert_units(other, units))), units\n        )\n        actual = ds.interp_like(other)\n\n        assert_equal_with_units(actual, expected)\n\n    @pytest.mark.xfail(reason=""indexes don\'t support units"")\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_reindex(self, unit, error, dtype):\n        array1 = (\n            np.linspace(1, 2, 10 * 5).reshape(10, 5).astype(dtype) * unit_registry.degK\n        )\n        array2 = (\n            np.linspace(1, 2, 10 * 8).reshape(10, 8).astype(dtype) * unit_registry.Pa\n        )\n\n        coords = {\n            ""x"": np.arange(10) * unit_registry.m,\n            ""y"": np.arange(5) * unit_registry.m,\n            ""z"": np.arange(8) * unit_registry.s,\n        }\n\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=(""x"", ""y"")),\n                ""b"": xr.DataArray(data=array2, dims=(""x"", ""z"")),\n            },\n            coords=coords,\n        )\n\n        new_coords = (np.arange(10) + 0.5) * unit\n\n        if error is not None:\n            with pytest.raises(error):\n                ds.reindex(x=new_coords)\n\n            return\n\n        expected = attach_units(\n            strip_units(ds).reindex(\n                x=strip_units(convert_units(new_coords, {None: coords[""x""].units}))\n            ),\n            extract_units(ds),\n        )\n        actual = ds.reindex(x=new_coords)\n\n        assert_equal_with_units(actual, expected)\n\n    @pytest.mark.xfail(reason=""indexes don\'t support units"")\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, DimensionalityError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, DimensionalityError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, DimensionalityError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, None, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    def test_reindex_like(self, unit, error, dtype):\n        array1 = (\n            np.linspace(0, 10, 10 * 5).reshape(10, 5).astype(dtype) * unit_registry.degK\n        )\n        array2 = (\n            np.linspace(10, 20, 10 * 8).reshape(10, 8).astype(dtype) * unit_registry.Pa\n        )\n\n        coords = {\n            ""x"": np.arange(10) * unit_registry.m,\n            ""y"": np.arange(5) * unit_registry.m,\n            ""z"": np.arange(8) * unit_registry.m,\n        }\n\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=(""x"", ""y"")),\n                ""b"": xr.DataArray(data=array2, dims=(""x"", ""z"")),\n            },\n            coords=coords,\n        )\n\n        other = xr.Dataset(\n            data_vars={\n                ""c"": xr.DataArray(data=np.empty((20, 10)), dims=(""x"", ""y"")),\n                ""d"": xr.DataArray(data=np.empty((20, 15)), dims=(""x"", ""z"")),\n            },\n            coords={\n                ""x"": (np.arange(20) + 0.3) * unit,\n                ""y"": (np.arange(10) - 0.2) * unit,\n                ""z"": (np.arange(15) + 0.4) * unit,\n            },\n        )\n\n        if error is not None:\n            with pytest.raises(error):\n                ds.reindex_like(other)\n\n            return\n\n        units = extract_units(ds)\n        expected = attach_units(\n            strip_units(ds).reindex_like(strip_units(convert_units(other, units))),\n            units,\n        )\n        actual = ds.reindex_like(other)\n\n        assert_equal_with_units(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            method(""diff"", dim=""x""),\n            method(""differentiate"", coord=""x""),\n            method(""integrate"", coord=""x""),\n            pytest.param(\n                method(""quantile"", q=[0.25, 0.75]),\n                marks=pytest.mark.xfail(reason=""nanquantile not implemented""),\n            ),\n            method(""reduce"", func=np.sum, dim=""x""),\n            method(""map"", np.fabs),\n        ),\n        ids=repr,\n    )\n    def test_computation(self, func, dtype):\n        array1 = (\n            np.linspace(-5, 5, 10 * 5).reshape(10, 5).astype(dtype) * unit_registry.degK\n        )\n        array2 = (\n            np.linspace(10, 20, 10 * 8).reshape(10, 8).astype(dtype) * unit_registry.Pa\n        )\n        x = np.arange(10) * unit_registry.m\n        y = np.arange(5) * unit_registry.m\n        z = np.arange(8) * unit_registry.m\n\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=(""x"", ""y"")),\n                ""b"": xr.DataArray(data=array2, dims=(""x"", ""z"")),\n            },\n            coords={""x"": x, ""y"": y, ""z"": z},\n        )\n\n        units = extract_units(ds)\n\n        expected = attach_units(func(strip_units(ds)), units)\n        actual = func(ds)\n\n        assert_equal_with_units(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            method(""groupby"", ""x""),\n            method(""groupby_bins"", ""x"", bins=4),\n            method(""coarsen"", x=2),\n            pytest.param(\n                method(""rolling"", x=3), marks=pytest.mark.xfail(reason=""strips units"")\n            ),\n            pytest.param(\n                method(""rolling_exp"", x=3),\n                marks=pytest.mark.xfail(reason=""uses numbagg which strips units""),\n            ),\n        ),\n        ids=repr,\n    )\n    def test_computation_objects(self, func, dtype):\n        array1 = (\n            np.linspace(-5, 5, 10 * 5).reshape(10, 5).astype(dtype) * unit_registry.degK\n        )\n        array2 = (\n            np.linspace(10, 20, 10 * 5 * 8).reshape(10, 5, 8).astype(dtype)\n            * unit_registry.Pa\n        )\n        x = np.arange(10) * unit_registry.m\n        y = np.arange(5) * unit_registry.m\n        z = np.arange(8) * unit_registry.m\n\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=(""x"", ""y"")),\n                ""b"": xr.DataArray(data=array2, dims=(""x"", ""y"", ""z"")),\n            },\n            coords={""x"": x, ""y"": y, ""z"": z},\n        )\n        units = extract_units(ds)\n\n        args = [] if func.name != ""groupby"" else [""y""]\n        reduce_func = method(""mean"", *args)\n        expected = attach_units(reduce_func(func(strip_units(ds))), units)\n        actual = reduce_func(func(ds))\n\n        assert_equal_with_units(expected, actual)\n\n    def test_resample(self, dtype):\n        array1 = (\n            np.linspace(-5, 5, 10 * 5).reshape(10, 5).astype(dtype) * unit_registry.degK\n        )\n        array2 = (\n            np.linspace(10, 20, 10 * 8).reshape(10, 8).astype(dtype) * unit_registry.Pa\n        )\n        t = pd.date_range(""10-09-2010"", periods=array1.shape[0], freq=""1y"")\n        y = np.arange(5) * unit_registry.m\n        z = np.arange(8) * unit_registry.m\n\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=(""time"", ""y"")),\n                ""b"": xr.DataArray(data=array2, dims=(""time"", ""z"")),\n            },\n            coords={""time"": t, ""y"": y, ""z"": z},\n        )\n        units = extract_units(ds)\n\n        func = method(""resample"", time=""6m"")\n\n        expected = attach_units(func(strip_units(ds)).mean(), units)\n        actual = func(ds).mean()\n\n        assert_equal_with_units(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            method(""assign"", c=lambda ds: 10 * ds.b),\n            method(""assign_coords"", v=(""x"", np.arange(10) * unit_registry.s)),\n            method(""first""),\n            method(""last""),\n            pytest.param(\n                method(""quantile"", q=[0.25, 0.5, 0.75], dim=""x""),\n                marks=pytest.mark.xfail(reason=""nanquantile not implemented""),\n            ),\n        ),\n        ids=repr,\n    )\n    def test_grouped_operations(self, func, dtype):\n        array1 = (\n            np.linspace(-5, 5, 10 * 5).reshape(10, 5).astype(dtype) * unit_registry.degK\n        )\n        array2 = (\n            np.linspace(10, 20, 10 * 5 * 8).reshape(10, 5, 8).astype(dtype)\n            * unit_registry.Pa\n        )\n        x = np.arange(10) * unit_registry.m\n        y = np.arange(5) * unit_registry.m\n        z = np.arange(8) * unit_registry.m\n\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=(""x"", ""y"")),\n                ""b"": xr.DataArray(data=array2, dims=(""x"", ""y"", ""z"")),\n            },\n            coords={""x"": x, ""y"": y, ""z"": z},\n        )\n        units = extract_units(ds)\n        units.update({""c"": unit_registry.Pa, ""v"": unit_registry.s})\n\n        stripped_kwargs = {\n            name: strip_units(value) for name, value in func.kwargs.items()\n        }\n        expected = attach_units(\n            func(strip_units(ds).groupby(""y""), **stripped_kwargs), units\n        )\n        actual = func(ds.groupby(""y""))\n\n        assert_equal_with_units(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""func"",\n        (\n            method(""pipe"", lambda ds: ds * 10),\n            method(""assign"", d=lambda ds: ds.b * 10),\n            method(""assign_coords"", y2=(""y"", np.arange(5) * unit_registry.mm)),\n            method(""assign_attrs"", attr1=""value""),\n            method(""rename"", x2=""x_mm""),\n            method(""rename_vars"", c=""temperature""),\n            method(""rename_dims"", x=""offset_x""),\n            method(""swap_dims"", {""x"": ""x2""}),\n            method(""expand_dims"", v=np.linspace(10, 20, 12) * unit_registry.s, axis=1),\n            method(""drop_vars"", ""x""),\n            method(""drop_dims"", ""z""),\n            method(""set_coords"", names=""c""),\n            method(""reset_coords"", names=""x2""),\n            method(""copy""),\n        ),\n        ids=repr,\n    )\n    def test_content_manipulation(self, func, dtype):\n        array1 = (\n            np.linspace(-5, 5, 10 * 5).reshape(10, 5).astype(dtype)\n            * unit_registry.m ** 3\n        )\n        array2 = (\n            np.linspace(10, 20, 10 * 5 * 8).reshape(10, 5, 8).astype(dtype)\n            * unit_registry.Pa\n        )\n        array3 = np.linspace(0, 10, 10).astype(dtype) * unit_registry.degK\n\n        x = np.arange(10) * unit_registry.m\n        x2 = x.to(unit_registry.mm)\n        y = np.arange(5) * unit_registry.m\n        z = np.arange(8) * unit_registry.m\n\n        ds = xr.Dataset(\n            data_vars={\n                ""a"": xr.DataArray(data=array1, dims=(""x"", ""y"")),\n                ""b"": xr.DataArray(data=array2, dims=(""x"", ""y"", ""z"")),\n                ""c"": xr.DataArray(data=array3, dims=""x""),\n            },\n            coords={""x"": x, ""y"": y, ""z"": z, ""x2"": (""x"", x2)},\n        )\n        units = {\n            **extract_units(ds),\n            **{\n                ""y2"": unit_registry.mm,\n                ""x_mm"": unit_registry.mm,\n                ""offset_x"": unit_registry.m,\n                ""d"": unit_registry.Pa,\n                ""temperature"": unit_registry.degK,\n            },\n        }\n\n        stripped_kwargs = {\n            key: strip_units(value) for key, value in func.kwargs.items()\n        }\n        expected = attach_units(func(strip_units(ds), **stripped_kwargs), units)\n        actual = func(ds)\n\n        assert_equal_with_units(expected, actual)\n\n    @pytest.mark.parametrize(\n        ""unit,error"",\n        (\n            pytest.param(1, xr.MergeError, id=""no_unit""),\n            pytest.param(\n                unit_registry.dimensionless, xr.MergeError, id=""dimensionless""\n            ),\n            pytest.param(unit_registry.s, xr.MergeError, id=""incompatible_unit""),\n            pytest.param(unit_registry.cm, xr.MergeError, id=""compatible_unit""),\n            pytest.param(unit_registry.m, None, id=""identical_unit""),\n        ),\n    )\n    @pytest.mark.parametrize(\n        ""variant"",\n        (\n            ""data"",\n            pytest.param(\n                ""dims"", marks=pytest.mark.xfail(reason=""indexes don\'t support units"")\n            ),\n            ""coords"",\n        ),\n    )\n    def test_merge(self, variant, unit, error, dtype):\n        original_data_unit = unit_registry.m\n        original_dim_unit = unit_registry.m\n        original_coord_unit = unit_registry.m\n\n        variants = {\n            ""data"": (unit, original_dim_unit, original_coord_unit),\n            ""dims"": (original_data_unit, unit, original_coord_unit),\n            ""coords"": (original_data_unit, original_dim_unit, unit),\n        }\n        data_unit, dim_unit, coord_unit = variants.get(variant)\n\n        left_array = np.arange(10).astype(dtype) * original_data_unit\n        right_array = np.arange(-5, 5).astype(dtype) * data_unit\n\n        left_dim = np.arange(10, 20) * original_dim_unit\n        right_dim = np.arange(5, 15) * dim_unit\n\n        left_coord = np.arange(-10, 0) * original_coord_unit\n        right_coord = np.arange(-15, -5) * coord_unit\n\n        left = xr.Dataset(\n            data_vars={""a"": (""x"", left_array)},\n            coords={""x"": left_dim, ""y"": (""x"", left_coord)},\n        )\n        right = xr.Dataset(\n            data_vars={""a"": (""x"", right_array)},\n            coords={""x"": right_dim, ""y"": (""x"", right_coord)},\n        )\n\n        units = extract_units(left)\n\n        if error is not None:\n            with pytest.raises(error):\n                left.merge(right)\n\n            return\n\n        converted = convert_units(right, units)\n        expected = attach_units(strip_units(left).merge(strip_units(converted)), units)\n        actual = left.merge(right)\n\n        assert_equal_with_units(expected, actual)\n'"
xarray/tests/test_utils.py,29,"b'from datetime import datetime\nfrom typing import Hashable\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nfrom xarray.coding.cftimeindex import CFTimeIndex\nfrom xarray.core import duck_array_ops, utils\nfrom xarray.core.utils import either_dict_or_kwargs\n\nfrom . import assert_array_equal, raises_regex, requires_cftime, requires_dask\nfrom .test_coding_times import _all_cftime_date_types\n\n\nclass TestAlias:\n    def test(self):\n        def new_method():\n            pass\n\n        old_method = utils.alias(new_method, ""old_method"")\n        assert ""deprecated"" in old_method.__doc__\n        with pytest.warns(Warning, match=""deprecated""):\n            old_method()\n\n\ndef test_safe_cast_to_index():\n    dates = pd.date_range(""2000-01-01"", periods=10)\n    x = np.arange(5)\n    td = x * np.timedelta64(1, ""D"")\n    for expected, array in [\n        (dates, dates.values),\n        (pd.Index(x, dtype=object), x.astype(object)),\n        (pd.Index(td), td),\n        (pd.Index(td, dtype=object), td.astype(object)),\n    ]:\n        actual = utils.safe_cast_to_index(array)\n        assert_array_equal(expected, actual)\n        assert expected.dtype == actual.dtype\n\n\n@requires_cftime\ndef test_safe_cast_to_index_cftimeindex():\n    date_types = _all_cftime_date_types()\n    for date_type in date_types.values():\n        dates = [date_type(1, 1, day) for day in range(1, 20)]\n        expected = CFTimeIndex(dates)\n        actual = utils.safe_cast_to_index(np.array(dates))\n        assert_array_equal(expected, actual)\n        assert expected.dtype == actual.dtype\n        assert isinstance(actual, type(expected))\n\n\n# Test that datetime.datetime objects are never used in a CFTimeIndex\n@requires_cftime\ndef test_safe_cast_to_index_datetime_datetime():\n    dates = [datetime(1, 1, day) for day in range(1, 20)]\n\n    expected = pd.Index(dates)\n    actual = utils.safe_cast_to_index(np.array(dates))\n    assert_array_equal(expected, actual)\n    assert isinstance(actual, pd.Index)\n\n\ndef test_multiindex_from_product_levels():\n    result = utils.multiindex_from_product_levels(\n        [pd.Index([""b"", ""a""]), pd.Index([1, 3, 2])]\n    )\n    np.testing.assert_array_equal(\n        result.codes, [[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]]\n    )\n    np.testing.assert_array_equal(result.levels[0], [""b"", ""a""])\n    np.testing.assert_array_equal(result.levels[1], [1, 3, 2])\n\n    other = pd.MultiIndex.from_product([[""b"", ""a""], [1, 3, 2]])\n    np.testing.assert_array_equal(result.values, other.values)\n\n\ndef test_multiindex_from_product_levels_non_unique():\n    result = utils.multiindex_from_product_levels(\n        [pd.Index([""b"", ""a""]), pd.Index([1, 1, 2])]\n    )\n    np.testing.assert_array_equal(\n        result.codes, [[0, 0, 0, 1, 1, 1], [0, 0, 1, 0, 0, 1]]\n    )\n    np.testing.assert_array_equal(result.levels[0], [""b"", ""a""])\n    np.testing.assert_array_equal(result.levels[1], [1, 2])\n\n\nclass TestArrayEquiv:\n    def test_0d(self):\n        # verify our work around for pd.isnull not working for 0-dimensional\n        # object arrays\n        assert duck_array_ops.array_equiv(0, np.array(0, dtype=object))\n        assert duck_array_ops.array_equiv(np.nan, np.array(np.nan, dtype=object))\n        assert not duck_array_ops.array_equiv(0, np.array(1, dtype=object))\n\n\nclass TestDictionaries:\n    @pytest.fixture(autouse=True)\n    def setup(self):\n        self.x = {""a"": ""A"", ""b"": ""B""}\n        self.y = {""c"": ""C"", ""b"": ""B""}\n        self.z = {""a"": ""Z""}\n\n    def test_equivalent(self):\n        assert utils.equivalent(0, 0)\n        assert utils.equivalent(np.nan, np.nan)\n        assert utils.equivalent(0, np.array(0.0))\n        assert utils.equivalent([0], np.array([0]))\n        assert utils.equivalent(np.array([0]), [0])\n        assert utils.equivalent(np.arange(3), 1.0 * np.arange(3))\n        assert not utils.equivalent(0, np.zeros(3))\n\n    def test_safe(self):\n        # should not raise exception:\n        utils.update_safety_check(self.x, self.y)\n\n    def test_unsafe(self):\n        with pytest.raises(ValueError):\n            utils.update_safety_check(self.x, self.z)\n\n    def test_compat_dict_intersection(self):\n        assert {""b"": ""B""} == utils.compat_dict_intersection(self.x, self.y)\n        assert {} == utils.compat_dict_intersection(self.x, self.z)\n\n    def test_compat_dict_union(self):\n        assert {""a"": ""A"", ""b"": ""B"", ""c"": ""C""} == utils.compat_dict_union(self.x, self.y)\n        with raises_regex(\n            ValueError,\n            ""unsafe to merge dictionaries without ""\n            ""overriding values; conflicting key"",\n        ):\n            utils.compat_dict_union(self.x, self.z)\n\n    def test_dict_equiv(self):\n        x = {}\n        x[""a""] = 3\n        x[""b""] = np.array([1, 2, 3])\n        y = {}\n        y[""b""] = np.array([1.0, 2.0, 3.0])\n        y[""a""] = 3\n        assert utils.dict_equiv(x, y)  # two nparrays are equal\n        y[""b""] = [1, 2, 3]  # np.array not the same as a list\n        assert utils.dict_equiv(x, y)  # nparray == list\n        x[""b""] = [1.0, 2.0, 3.0]\n        assert utils.dict_equiv(x, y)  # list vs. list\n        x[""c""] = None\n        assert not utils.dict_equiv(x, y)  # new key in x\n        x[""c""] = np.nan\n        y[""c""] = np.nan\n        assert utils.dict_equiv(x, y)  # as intended, nan is nan\n        x[""c""] = np.inf\n        y[""c""] = np.inf\n        assert utils.dict_equiv(x, y)  # inf == inf\n        y = dict(y)\n        assert utils.dict_equiv(x, y)  # different dictionary types are fine\n        y[""b""] = 3 * np.arange(3)\n        assert not utils.dict_equiv(x, y)  # not equal when arrays differ\n\n    def test_frozen(self):\n        x = utils.Frozen(self.x)\n        with pytest.raises(TypeError):\n            x[""foo""] = ""bar""\n        with pytest.raises(TypeError):\n            del x[""a""]\n        with pytest.raises(AttributeError):\n            x.update(self.y)\n        assert x.mapping == self.x\n        assert repr(x) in (\n            ""Frozen({\'a\': \'A\', \'b\': \'B\'})"",\n            ""Frozen({\'b\': \'B\', \'a\': \'A\'})"",\n        )\n\n    def test_sorted_keys_dict(self):\n        x = {""a"": 1, ""b"": 2, ""c"": 3}\n        y = utils.SortedKeysDict(x)\n        assert list(y) == [""a"", ""b"", ""c""]\n        assert repr(utils.SortedKeysDict()) == ""SortedKeysDict({})""\n\n\ndef test_repr_object():\n    obj = utils.ReprObject(""foo"")\n    assert repr(obj) == ""foo""\n    assert isinstance(obj, Hashable)\n    assert not isinstance(obj, str)\n\n\ndef test_repr_object_magic_methods():\n    o1 = utils.ReprObject(""foo"")\n    o2 = utils.ReprObject(""foo"")\n    o3 = utils.ReprObject(""bar"")\n    o4 = ""foo""\n    assert o1 == o2\n    assert o1 != o3\n    assert o1 != o4\n    assert hash(o1) == hash(o2)\n    assert hash(o1) != hash(o3)\n    assert hash(o1) != hash(o4)\n\n\ndef test_is_remote_uri():\n    assert utils.is_remote_uri(""http://example.com"")\n    assert utils.is_remote_uri(""https://example.com"")\n    assert not utils.is_remote_uri("" http://example.com"")\n    assert not utils.is_remote_uri(""example.nc"")\n\n\ndef test_is_grib_path():\n    assert not utils.is_grib_path(""example.nc"")\n    assert not utils.is_grib_path(""example.grib "")\n    assert utils.is_grib_path(""example.grib"")\n    assert utils.is_grib_path(""example.grib2"")\n    assert utils.is_grib_path(""example.grb"")\n    assert utils.is_grib_path(""example.grb2"")\n\n\nclass Test_is_uniform_and_sorted:\n    def test_sorted_uniform(self):\n        assert utils.is_uniform_spaced(np.arange(5))\n\n    def test_sorted_not_uniform(self):\n        assert not utils.is_uniform_spaced([-2, 1, 89])\n\n    def test_not_sorted_uniform(self):\n        assert not utils.is_uniform_spaced([1, -1, 3])\n\n    def test_not_sorted_not_uniform(self):\n        assert not utils.is_uniform_spaced([4, 1, 89])\n\n    def test_two_numbers(self):\n        assert utils.is_uniform_spaced([0, 1.7])\n\n    def test_relative_tolerance(self):\n        assert utils.is_uniform_spaced([0, 0.97, 2], rtol=0.1)\n\n\nclass Test_hashable:\n    def test_hashable(self):\n        for v in [False, 1, (2,), (3, 4), ""four""]:\n            assert utils.hashable(v)\n        for v in [[5, 6], [""seven"", ""8""], {9: ""ten""}]:\n            assert not utils.hashable(v)\n\n\n@requires_dask\ndef test_dask_array_is_scalar():\n    # regression test for GH1684\n    import dask.array as da\n\n    y = da.arange(8, chunks=4)\n    assert not utils.is_scalar(y)\n\n\ndef test_hidden_key_dict():\n    hidden_key = ""_hidden_key""\n    data = {""a"": 1, ""b"": 2, hidden_key: 3}\n    data_expected = {""a"": 1, ""b"": 2}\n    hkd = utils.HiddenKeyDict(data, [hidden_key])\n    assert len(hkd) == 2\n    assert hidden_key not in hkd\n    for k, v in data_expected.items():\n        assert hkd[k] == v\n    with pytest.raises(KeyError):\n        hkd[hidden_key]\n    with pytest.raises(KeyError):\n        del hkd[hidden_key]\n\n\ndef test_either_dict_or_kwargs():\n\n    result = either_dict_or_kwargs(dict(a=1), None, ""foo"")\n    expected = dict(a=1)\n    assert result == expected\n\n    result = either_dict_or_kwargs(None, dict(a=1), ""foo"")\n    expected = dict(a=1)\n    assert result == expected\n\n    with pytest.raises(ValueError, match=r""foo""):\n        result = either_dict_or_kwargs(dict(a=1), dict(a=1), ""foo"")\n\n\n@pytest.mark.parametrize(\n    [""supplied"", ""all_"", ""expected""],\n    [\n        (list(""abc""), list(""abc""), list(""abc"")),\n        ([""a"", ..., ""c""], list(""abc""), list(""abc"")),\n        ([""a"", ...], list(""abc""), list(""abc"")),\n        ([""c"", ...], list(""abc""), list(""cab"")),\n        ([..., ""b""], list(""abc""), list(""acb"")),\n        ([...], list(""abc""), list(""abc"")),\n    ],\n)\ndef test_infix_dims(supplied, all_, expected):\n    result = list(utils.infix_dims(supplied, all_))\n    assert result == expected\n\n\n@pytest.mark.parametrize(\n    [""supplied"", ""all_""], [([..., ...], list(""abc"")), ([...], list(""aac""))]\n)\ndef test_infix_dims_errors(supplied, all_):\n    with pytest.raises(ValueError):\n        list(utils.infix_dims(supplied, all_))\n'"
xarray/tests/test_variable.py,292,"b'import warnings\nfrom copy import copy, deepcopy\nfrom datetime import datetime, timedelta\nfrom textwrap import dedent\n\nimport numpy as np\nimport pandas as pd\nimport pytest\nimport pytz\n\nfrom xarray import Coordinate, Dataset, IndexVariable, Variable, set_options\nfrom xarray.core import dtypes, duck_array_ops, indexing\nfrom xarray.core.common import full_like, ones_like, zeros_like\nfrom xarray.core.indexing import (\n    BasicIndexer,\n    CopyOnWriteArray,\n    DaskIndexingAdapter,\n    LazilyOuterIndexedArray,\n    MemoryCachedArray,\n    NumpyIndexingAdapter,\n    OuterIndexer,\n    PandasIndexAdapter,\n    VectorizedIndexer,\n)\nfrom xarray.core.pycompat import dask_array_type\nfrom xarray.core.utils import NDArrayMixin\nfrom xarray.core.variable import as_compatible_data, as_variable\nfrom xarray.tests import requires_bottleneck\n\nfrom . import (\n    assert_allclose,\n    assert_array_equal,\n    assert_equal,\n    assert_identical,\n    raises_regex,\n    requires_dask,\n    requires_sparse,\n    source_ndarray,\n)\n\n_PAD_XR_NP_ARGS = [\n    [{""x"": (2, 1)}, ((2, 1), (0, 0), (0, 0))],\n    [{""x"": 1}, ((1, 1), (0, 0), (0, 0))],\n    [{""y"": (0, 3)}, ((0, 0), (0, 3), (0, 0))],\n    [{""x"": (3, 1), ""z"": (2, 0)}, ((3, 1), (0, 0), (2, 0))],\n    [{""x"": (3, 1), ""z"": 2}, ((3, 1), (0, 0), (2, 2))],\n]\n\n\nclass VariableSubclassobjects:\n    def test_properties(self):\n        data = 0.5 * np.arange(10)\n        v = self.cls([""time""], data, {""foo"": ""bar""})\n        assert v.dims == (""time"",)\n        assert_array_equal(v.values, data)\n        assert v.dtype == float\n        assert v.shape == (10,)\n        assert v.size == 10\n        assert v.sizes == {""time"": 10}\n        assert v.nbytes == 80\n        assert v.ndim == 1\n        assert len(v) == 10\n        assert v.attrs == {""foo"": ""bar""}\n\n    def test_attrs(self):\n        v = self.cls([""time""], 0.5 * np.arange(10))\n        assert v.attrs == {}\n        attrs = {""foo"": ""bar""}\n        v.attrs = attrs\n        assert v.attrs == attrs\n        assert isinstance(v.attrs, dict)\n        v.attrs[""foo""] = ""baz""\n        assert v.attrs[""foo""] == ""baz""\n\n    def test_getitem_dict(self):\n        v = self.cls([""x""], np.random.randn(5))\n        actual = v[{""x"": 0}]\n        expected = v[0]\n        assert_identical(expected, actual)\n\n    def test_getitem_1d(self):\n        data = np.array([0, 1, 2])\n        v = self.cls([""x""], data)\n\n        v_new = v[dict(x=[0, 1])]\n        assert v_new.dims == (""x"",)\n        assert_array_equal(v_new, data[[0, 1]])\n\n        v_new = v[dict(x=slice(None))]\n        assert v_new.dims == (""x"",)\n        assert_array_equal(v_new, data)\n\n        v_new = v[dict(x=Variable(""a"", [0, 1]))]\n        assert v_new.dims == (""a"",)\n        assert_array_equal(v_new, data[[0, 1]])\n\n        v_new = v[dict(x=1)]\n        assert v_new.dims == ()\n        assert_array_equal(v_new, data[1])\n\n        # tuple argument\n        v_new = v[slice(None)]\n        assert v_new.dims == (""x"",)\n        assert_array_equal(v_new, data)\n\n    def test_getitem_1d_fancy(self):\n        v = self.cls([""x""], [0, 1, 2])\n        # 1d-variable should be indexable by multi-dimensional Variable\n        ind = Variable((""a"", ""b""), [[0, 1], [0, 1]])\n        v_new = v[ind]\n        assert v_new.dims == (""a"", ""b"")\n        expected = np.array(v._data)[([0, 1], [0, 1]), ...]\n        assert_array_equal(v_new, expected)\n\n        # boolean indexing\n        ind = Variable((""x"",), [True, False, True])\n        v_new = v[ind]\n        assert_identical(v[[0, 2]], v_new)\n        v_new = v[[True, False, True]]\n        assert_identical(v[[0, 2]], v_new)\n\n        with raises_regex(IndexError, ""Boolean indexer should""):\n            ind = Variable((""a"",), [True, False, True])\n            v[ind]\n\n    def test_getitem_with_mask(self):\n        v = self.cls([""x""], [0, 1, 2])\n        assert_identical(v._getitem_with_mask(-1), Variable((), np.nan))\n        assert_identical(\n            v._getitem_with_mask([0, -1, 1]), self.cls([""x""], [0, np.nan, 1])\n        )\n        assert_identical(v._getitem_with_mask(slice(2)), self.cls([""x""], [0, 1]))\n        assert_identical(\n            v._getitem_with_mask([0, -1, 1], fill_value=-99),\n            self.cls([""x""], [0, -99, 1]),\n        )\n\n    def test_getitem_with_mask_size_zero(self):\n        v = self.cls([""x""], [])\n        assert_identical(v._getitem_with_mask(-1), Variable((), np.nan))\n        assert_identical(\n            v._getitem_with_mask([-1, -1, -1]),\n            self.cls([""x""], [np.nan, np.nan, np.nan]),\n        )\n\n    def test_getitem_with_mask_nd_indexer(self):\n        v = self.cls([""x""], [0, 1, 2])\n        indexer = Variable((""x"", ""y""), [[0, -1], [-1, 2]])\n        assert_identical(v._getitem_with_mask(indexer, fill_value=-1), indexer)\n\n    def _assertIndexedLikeNDArray(self, variable, expected_value0, expected_dtype=None):\n        """"""Given a 1-dimensional variable, verify that the variable is indexed\n        like a numpy.ndarray.\n        """"""\n        assert variable[0].shape == ()\n        assert variable[0].ndim == 0\n        assert variable[0].size == 1\n        # test identity\n        assert variable.equals(variable.copy())\n        assert variable.identical(variable.copy())\n        # check value is equal for both ndarray and Variable\n        with warnings.catch_warnings():\n            warnings.filterwarnings(""ignore"", ""In the future, \'NAT == x\'"")\n            np.testing.assert_equal(variable.values[0], expected_value0)\n            np.testing.assert_equal(variable[0].values, expected_value0)\n        # check type or dtype is consistent for both ndarray and Variable\n        if expected_dtype is None:\n            # check output type instead of array dtype\n            assert type(variable.values[0]) == type(expected_value0)\n            assert type(variable[0].values) == type(expected_value0)\n        elif expected_dtype is not False:\n            assert variable.values[0].dtype == expected_dtype\n            assert variable[0].values.dtype == expected_dtype\n\n    def test_index_0d_int(self):\n        for value, dtype in [(0, np.int_), (np.int32(0), np.int32)]:\n            x = self.cls([""x""], [value])\n            self._assertIndexedLikeNDArray(x, value, dtype)\n\n    def test_index_0d_float(self):\n        for value, dtype in [(0.5, np.float_), (np.float32(0.5), np.float32)]:\n            x = self.cls([""x""], [value])\n            self._assertIndexedLikeNDArray(x, value, dtype)\n\n    def test_index_0d_string(self):\n        value = ""foo""\n        dtype = np.dtype(""U3"")\n        x = self.cls([""x""], [value])\n        self._assertIndexedLikeNDArray(x, value, dtype)\n\n    def test_index_0d_datetime(self):\n        d = datetime(2000, 1, 1)\n        x = self.cls([""x""], [d])\n        self._assertIndexedLikeNDArray(x, np.datetime64(d))\n\n        x = self.cls([""x""], [np.datetime64(d)])\n        self._assertIndexedLikeNDArray(x, np.datetime64(d), ""datetime64[ns]"")\n\n        x = self.cls([""x""], pd.DatetimeIndex([d]))\n        self._assertIndexedLikeNDArray(x, np.datetime64(d), ""datetime64[ns]"")\n\n    def test_index_0d_timedelta64(self):\n        td = timedelta(hours=1)\n\n        x = self.cls([""x""], [np.timedelta64(td)])\n        self._assertIndexedLikeNDArray(x, np.timedelta64(td), ""timedelta64[ns]"")\n\n        x = self.cls([""x""], pd.to_timedelta([td]))\n        self._assertIndexedLikeNDArray(x, np.timedelta64(td), ""timedelta64[ns]"")\n\n    def test_index_0d_not_a_time(self):\n        d = np.datetime64(""NaT"", ""ns"")\n        x = self.cls([""x""], [d])\n        self._assertIndexedLikeNDArray(x, d)\n\n    def test_index_0d_object(self):\n        class HashableItemWrapper:\n            def __init__(self, item):\n                self.item = item\n\n            def __eq__(self, other):\n                return self.item == other.item\n\n            def __hash__(self):\n                return hash(self.item)\n\n            def __repr__(self):\n                return ""{}(item={!r})"".format(type(self).__name__, self.item)\n\n        item = HashableItemWrapper((1, 2, 3))\n        x = self.cls(""x"", [item])\n        self._assertIndexedLikeNDArray(x, item, expected_dtype=False)\n\n    def test_0d_object_array_with_list(self):\n        listarray = np.empty((1,), dtype=object)\n        listarray[0] = [1, 2, 3]\n        x = self.cls(""x"", listarray)\n        assert_array_equal(x.data, listarray)\n        assert_array_equal(x[0].data, listarray.squeeze())\n        assert_array_equal(x.squeeze().data, listarray.squeeze())\n\n    def test_index_and_concat_datetime(self):\n        # regression test for #125\n        date_range = pd.date_range(""2011-09-01"", periods=10)\n        for dates in [date_range, date_range.values, date_range.to_pydatetime()]:\n            expected = self.cls(""t"", dates)\n            for times in [\n                [expected[i] for i in range(10)],\n                [expected[i : (i + 1)] for i in range(10)],\n                [expected[[i]] for i in range(10)],\n            ]:\n                actual = Variable.concat(times, ""t"")\n                assert expected.dtype == actual.dtype\n                assert_array_equal(expected, actual)\n\n    def test_0d_time_data(self):\n        # regression test for #105\n        x = self.cls(""time"", pd.date_range(""2000-01-01"", periods=5))\n        expected = np.datetime64(""2000-01-01"", ""ns"")\n        assert x[0].values == expected\n\n    def test_datetime64_conversion(self):\n        times = pd.date_range(""2000-01-01"", periods=3)\n        for values, preserve_source in [\n            (times, True),\n            (times.values, True),\n            (times.values.astype(""datetime64[s]""), False),\n            (times.to_pydatetime(), False),\n        ]:\n            v = self.cls([""t""], values)\n            assert v.dtype == np.dtype(""datetime64[ns]"")\n            assert_array_equal(v.values, times.values)\n            assert v.values.dtype == np.dtype(""datetime64[ns]"")\n            same_source = source_ndarray(v.values) is source_ndarray(values)\n            assert preserve_source == same_source\n\n    def test_timedelta64_conversion(self):\n        times = pd.timedelta_range(start=0, periods=3)\n        for values, preserve_source in [\n            (times, True),\n            (times.values, True),\n            (times.values.astype(""timedelta64[s]""), False),\n            (times.to_pytimedelta(), False),\n        ]:\n            v = self.cls([""t""], values)\n            assert v.dtype == np.dtype(""timedelta64[ns]"")\n            assert_array_equal(v.values, times.values)\n            assert v.values.dtype == np.dtype(""timedelta64[ns]"")\n            same_source = source_ndarray(v.values) is source_ndarray(values)\n            assert preserve_source == same_source\n\n    def test_object_conversion(self):\n        data = np.arange(5).astype(str).astype(object)\n        actual = self.cls(""x"", data)\n        assert actual.dtype == data.dtype\n\n    def test_pandas_data(self):\n        v = self.cls([""x""], pd.Series([0, 1, 2], index=[3, 2, 1]))\n        assert_identical(v, v[[0, 1, 2]])\n        v = self.cls([""x""], pd.Index([0, 1, 2]))\n        assert v[0].values == v.values[0]\n\n    def test_pandas_period_index(self):\n        v = self.cls([""x""], pd.period_range(start=""2000"", periods=20, freq=""B""))\n        v = v.load()  # for dask-based Variable\n        assert v[0] == pd.Period(""2000"", freq=""B"")\n        assert ""Period(\'2000-01-03\', \'B\')"" in repr(v)\n\n    def test_1d_math(self):\n        x = 1.0 * np.arange(5)\n        y = np.ones(5)\n\n        # should we need `.to_base_variable()`?\n        # probably a break that `+v` changes type?\n        v = self.cls([""x""], x)\n        base_v = v.to_base_variable()\n        # unary ops\n        assert_identical(base_v, +v)\n        assert_identical(base_v, abs(v))\n        assert_array_equal((-v).values, -x)\n        # binary ops with numbers\n        assert_identical(base_v, v + 0)\n        assert_identical(base_v, 0 + v)\n        assert_identical(base_v, v * 1)\n        # binary ops with numpy arrays\n        assert_array_equal((v * x).values, x ** 2)\n        assert_array_equal((x * v).values, x ** 2)\n        assert_array_equal(v - y, v - 1)\n        assert_array_equal(y - v, 1 - v)\n        # verify attributes are dropped\n        v2 = self.cls([""x""], x, {""units"": ""meters""})\n        assert_identical(base_v, +v2)\n        # binary ops with all variables\n        assert_array_equal(v + v, 2 * v)\n        w = self.cls([""x""], y, {""foo"": ""bar""})\n        assert_identical(v + w, self.cls([""x""], x + y).to_base_variable())\n        assert_array_equal((v * w).values, x * y)\n\n        # something complicated\n        assert_array_equal((v ** 2 * w - 1 + x).values, x ** 2 * y - 1 + x)\n        # make sure dtype is preserved (for Index objects)\n        assert float == (+v).dtype\n        assert float == (+v).values.dtype\n        assert float == (0 + v).dtype\n        assert float == (0 + v).values.dtype\n        # check types of returned data\n        assert isinstance(+v, Variable)\n        assert not isinstance(+v, IndexVariable)\n        assert isinstance(0 + v, Variable)\n        assert not isinstance(0 + v, IndexVariable)\n\n    def test_1d_reduce(self):\n        x = np.arange(5)\n        v = self.cls([""x""], x)\n        actual = v.sum()\n        expected = Variable((), 10)\n        assert_identical(expected, actual)\n        assert type(actual) is Variable\n\n    def test_array_interface(self):\n        x = np.arange(5)\n        v = self.cls([""x""], x)\n        assert_array_equal(np.asarray(v), x)\n        # test patched in methods\n        assert_array_equal(v.astype(float), x.astype(float))\n        # think this is a break, that argsort changes the type\n        assert_identical(v.argsort(), v.to_base_variable())\n        assert_identical(v.clip(2, 3), self.cls(""x"", x.clip(2, 3)).to_base_variable())\n        # test ufuncs\n        assert_identical(np.sin(v), self.cls([""x""], np.sin(x)).to_base_variable())\n        assert isinstance(np.sin(v), Variable)\n        assert not isinstance(np.sin(v), IndexVariable)\n\n    def example_1d_objects(self):\n        for data in [\n            range(3),\n            0.5 * np.arange(3),\n            0.5 * np.arange(3, dtype=np.float32),\n            pd.date_range(""2000-01-01"", periods=3),\n            np.array([""a"", ""b"", ""c""], dtype=object),\n        ]:\n            yield (self.cls(""x"", data), data)\n\n    def test___array__(self):\n        for v, data in self.example_1d_objects():\n            assert_array_equal(v.values, np.asarray(data))\n            assert_array_equal(np.asarray(v), np.asarray(data))\n            assert v[0].values == np.asarray(data)[0]\n            assert np.asarray(v[0]) == np.asarray(data)[0]\n\n    def test_equals_all_dtypes(self):\n        for v, _ in self.example_1d_objects():\n            v2 = v.copy()\n            assert v.equals(v2)\n            assert v.identical(v2)\n            assert v.no_conflicts(v2)\n            assert v[0].equals(v2[0])\n            assert v[0].identical(v2[0])\n            assert v[0].no_conflicts(v2[0])\n            assert v[:2].equals(v2[:2])\n            assert v[:2].identical(v2[:2])\n            assert v[:2].no_conflicts(v2[:2])\n\n    def test_eq_all_dtypes(self):\n        # ensure that we don\'t choke on comparisons for which numpy returns\n        # scalars\n        expected = Variable(""x"", 3 * [False])\n        for v, _ in self.example_1d_objects():\n            actual = ""z"" == v\n            assert_identical(expected, actual)\n            actual = ~(""z"" != v)\n            assert_identical(expected, actual)\n\n    def test_encoding_preserved(self):\n        expected = self.cls(""x"", range(3), {""foo"": 1}, {""bar"": 2})\n        for actual in [\n            expected.T,\n            expected[...],\n            expected.squeeze(),\n            expected.isel(x=slice(None)),\n            expected.set_dims({""x"": 3}),\n            expected.copy(deep=True),\n            expected.copy(deep=False),\n        ]:\n\n            assert_identical(expected.to_base_variable(), actual.to_base_variable())\n            assert expected.encoding == actual.encoding\n\n    def test_concat(self):\n        x = np.arange(5)\n        y = np.arange(5, 10)\n        v = self.cls([""a""], x)\n        w = self.cls([""a""], y)\n        assert_identical(\n            Variable([""b"", ""a""], np.array([x, y])), Variable.concat([v, w], ""b"")\n        )\n        assert_identical(\n            Variable([""b"", ""a""], np.array([x, y])), Variable.concat((v, w), ""b"")\n        )\n        assert_identical(\n            Variable([""b"", ""a""], np.array([x, y])), Variable.concat((v, w), ""b"")\n        )\n        with raises_regex(ValueError, ""Variable has dimensions""):\n            Variable.concat([v, Variable([""c""], y)], ""b"")\n        # test indexers\n        actual = Variable.concat(\n            [v, w], positions=[np.arange(0, 10, 2), np.arange(1, 10, 2)], dim=""a""\n        )\n        expected = Variable(""a"", np.array([x, y]).ravel(order=""F""))\n        assert_identical(expected, actual)\n        # test concatenating along a dimension\n        v = Variable([""time"", ""x""], np.random.random((10, 8)))\n        assert_identical(v, Variable.concat([v[:5], v[5:]], ""time""))\n        assert_identical(v, Variable.concat([v[:5], v[5:6], v[6:]], ""time""))\n        assert_identical(v, Variable.concat([v[:1], v[1:]], ""time""))\n        # test dimension order\n        assert_identical(v, Variable.concat([v[:, :5], v[:, 5:]], ""x""))\n        with raises_regex(ValueError, ""all input arrays must have""):\n            Variable.concat([v[:, 0], v[:, 1:]], ""x"")\n\n    def test_concat_attrs(self):\n        # always keep attrs from first variable\n        v = self.cls(""a"", np.arange(5), {""foo"": ""bar""})\n        w = self.cls(""a"", np.ones(5))\n        expected = self.cls(\n            ""a"", np.concatenate([np.arange(5), np.ones(5)])\n        ).to_base_variable()\n        expected.attrs[""foo""] = ""bar""\n        assert_identical(expected, Variable.concat([v, w], ""a""))\n\n    def test_concat_fixed_len_str(self):\n        # regression test for #217\n        for kind in [""S"", ""U""]:\n            x = self.cls(""animal"", np.array([""horse""], dtype=kind))\n            y = self.cls(""animal"", np.array([""aardvark""], dtype=kind))\n            actual = Variable.concat([x, y], ""animal"")\n            expected = Variable(""animal"", np.array([""horse"", ""aardvark""], dtype=kind))\n            assert_equal(expected, actual)\n\n    def test_concat_number_strings(self):\n        # regression test for #305\n        a = self.cls(""x"", [""0"", ""1"", ""2""])\n        b = self.cls(""x"", [""3"", ""4""])\n        actual = Variable.concat([a, b], dim=""x"")\n        expected = Variable(""x"", np.arange(5).astype(str))\n        assert_identical(expected, actual)\n        assert actual.dtype.kind == expected.dtype.kind\n\n    def test_concat_mixed_dtypes(self):\n        a = self.cls(""x"", [0, 1])\n        b = self.cls(""x"", [""two""])\n        actual = Variable.concat([a, b], dim=""x"")\n        expected = Variable(""x"", np.array([0, 1, ""two""], dtype=object))\n        assert_identical(expected, actual)\n        assert actual.dtype == object\n\n    @pytest.mark.parametrize(""deep"", [True, False])\n    @pytest.mark.parametrize(""astype"", [float, int, str])\n    def test_copy(self, deep, astype):\n        v = self.cls(""x"", (0.5 * np.arange(10)).astype(astype), {""foo"": ""bar""})\n        w = v.copy(deep=deep)\n        assert type(v) is type(w)\n        assert_identical(v, w)\n        assert v.dtype == w.dtype\n        if self.cls is Variable:\n            if deep:\n                assert source_ndarray(v.values) is not source_ndarray(w.values)\n            else:\n                assert source_ndarray(v.values) is source_ndarray(w.values)\n        assert_identical(v, copy(v))\n\n    def test_copy_index(self):\n        midx = pd.MultiIndex.from_product(\n            [[""a"", ""b""], [1, 2], [-1, -2]], names=(""one"", ""two"", ""three"")\n        )\n        v = self.cls(""x"", midx)\n        for deep in [True, False]:\n            w = v.copy(deep=deep)\n            assert isinstance(w._data, PandasIndexAdapter)\n            assert isinstance(w.to_index(), pd.MultiIndex)\n            assert_array_equal(v._data.array, w._data.array)\n\n    def test_copy_with_data(self):\n        orig = Variable((""x"", ""y""), [[1.5, 2.0], [3.1, 4.3]], {""foo"": ""bar""})\n        new_data = np.array([[2.5, 5.0], [7.1, 43]])\n        actual = orig.copy(data=new_data)\n        expected = orig.copy()\n        expected.data = new_data\n        assert_identical(expected, actual)\n\n    def test_copy_with_data_errors(self):\n        orig = Variable((""x"", ""y""), [[1.5, 2.0], [3.1, 4.3]], {""foo"": ""bar""})\n        new_data = [2.5, 5.0]\n        with raises_regex(ValueError, ""must match shape of object""):\n            orig.copy(data=new_data)\n\n    def test_copy_index_with_data(self):\n        orig = IndexVariable(""x"", np.arange(5))\n        new_data = np.arange(5, 10)\n        actual = orig.copy(data=new_data)\n        expected = IndexVariable(""x"", np.arange(5, 10))\n        assert_identical(expected, actual)\n\n    def test_copy_index_with_data_errors(self):\n        orig = IndexVariable(""x"", np.arange(5))\n        new_data = np.arange(5, 20)\n        with raises_regex(ValueError, ""must match shape of object""):\n            orig.copy(data=new_data)\n        with raises_regex(ValueError, ""Cannot assign to the .data""):\n            orig.data = new_data\n        with raises_regex(ValueError, ""Cannot assign to the .values""):\n            orig.values = new_data\n\n    def test_replace(self):\n        var = Variable((""x"", ""y""), [[1.5, 2.0], [3.1, 4.3]], {""foo"": ""bar""})\n        result = var._replace()\n        assert_identical(result, var)\n\n        new_data = np.arange(4).reshape(2, 2)\n        result = var._replace(data=new_data)\n        assert_array_equal(result.data, new_data)\n\n    def test_real_and_imag(self):\n        v = self.cls(""x"", np.arange(3) - 1j * np.arange(3), {""foo"": ""bar""})\n        expected_re = self.cls(""x"", np.arange(3), {""foo"": ""bar""})\n        assert_identical(v.real, expected_re)\n\n        expected_im = self.cls(""x"", -np.arange(3), {""foo"": ""bar""})\n        assert_identical(v.imag, expected_im)\n\n        expected_abs = self.cls(""x"", np.sqrt(2 * np.arange(3) ** 2)).to_base_variable()\n        assert_allclose(abs(v), expected_abs)\n\n    def test_aggregate_complex(self):\n        # should skip NaNs\n        v = self.cls(""x"", [1, 2j, np.nan])\n        expected = Variable((), 0.5 + 1j)\n        assert_allclose(v.mean(), expected)\n\n    def test_pandas_cateogrical_dtype(self):\n        data = pd.Categorical(np.arange(10, dtype=""int64""))\n        v = self.cls(""x"", data)\n        print(v)  # should not error\n        assert v.dtype == ""int64""\n\n    def test_pandas_datetime64_with_tz(self):\n        data = pd.date_range(\n            start=""2000-01-01"",\n            tz=pytz.timezone(""America/New_York""),\n            periods=10,\n            freq=""1h"",\n        )\n        v = self.cls(""x"", data)\n        print(v)  # should not error\n        if ""America/New_York"" in str(data.dtype):\n            # pandas is new enough that it has datetime64 with timezone dtype\n            assert v.dtype == ""object""\n\n    def test_multiindex(self):\n        idx = pd.MultiIndex.from_product([list(""abc""), [0, 1]])\n        v = self.cls(""x"", idx)\n        assert_identical(Variable((), (""a"", 0)), v[0])\n        assert_identical(v, v[:])\n\n    def test_load(self):\n        array = self.cls(""x"", np.arange(5))\n        orig_data = array._data\n        copied = array.copy(deep=True)\n        if array.chunks is None:\n            array.load()\n            assert type(array._data) is type(orig_data)\n            assert type(copied._data) is type(orig_data)\n            assert_identical(array, copied)\n\n    def test_getitem_advanced(self):\n        v = self.cls([""x"", ""y""], [[0, 1, 2], [3, 4, 5]])\n        v_data = v.compute().data\n\n        # orthogonal indexing\n        v_new = v[([0, 1], [1, 0])]\n        assert v_new.dims == (""x"", ""y"")\n        assert_array_equal(v_new, v_data[[0, 1]][:, [1, 0]])\n\n        v_new = v[[0, 1]]\n        assert v_new.dims == (""x"", ""y"")\n        assert_array_equal(v_new, v_data[[0, 1]])\n\n        # with mixed arguments\n        ind = Variable([""a""], [0, 1])\n        v_new = v[dict(x=[0, 1], y=ind)]\n        assert v_new.dims == (""x"", ""a"")\n        assert_array_equal(v_new, v_data[[0, 1]][:, [0, 1]])\n\n        # boolean indexing\n        v_new = v[dict(x=[True, False], y=[False, True, False])]\n        assert v_new.dims == (""x"", ""y"")\n        assert_array_equal(v_new, v_data[0][1])\n\n        # with scalar variable\n        ind = Variable((), 2)\n        v_new = v[dict(y=ind)]\n        expected = v[dict(y=2)]\n        assert_array_equal(v_new, expected)\n\n        # with boolean variable with wrong shape\n        ind = np.array([True, False])\n        with raises_regex(IndexError, ""Boolean array size 2 is ""):\n            v[Variable((""a"", ""b""), [[0, 1]]), ind]\n\n        # boolean indexing with different dimension\n        ind = Variable([""a""], [True, False, False])\n        with raises_regex(IndexError, ""Boolean indexer should be""):\n            v[dict(y=ind)]\n\n    def test_getitem_uint_1d(self):\n        # regression test for #1405\n        v = self.cls([""x""], [0, 1, 2])\n        v_data = v.compute().data\n\n        v_new = v[np.array([0])]\n        assert_array_equal(v_new, v_data[0])\n        v_new = v[np.array([0], dtype=""uint64"")]\n        assert_array_equal(v_new, v_data[0])\n\n    def test_getitem_uint(self):\n        # regression test for #1405\n        v = self.cls([""x"", ""y""], [[0, 1, 2], [3, 4, 5]])\n        v_data = v.compute().data\n\n        v_new = v[np.array([0])]\n        assert_array_equal(v_new, v_data[[0], :])\n        v_new = v[np.array([0], dtype=""uint64"")]\n        assert_array_equal(v_new, v_data[[0], :])\n\n        v_new = v[np.uint64(0)]\n        assert_array_equal(v_new, v_data[0, :])\n\n    def test_getitem_0d_array(self):\n        # make sure 0d-np.array can be used as an indexer\n        v = self.cls([""x""], [0, 1, 2])\n        v_data = v.compute().data\n\n        v_new = v[np.array([0])[0]]\n        assert_array_equal(v_new, v_data[0])\n\n        v_new = v[np.array(0)]\n        assert_array_equal(v_new, v_data[0])\n\n        v_new = v[Variable((), np.array(0))]\n        assert_array_equal(v_new, v_data[0])\n\n    def test_getitem_fancy(self):\n        v = self.cls([""x"", ""y""], [[0, 1, 2], [3, 4, 5]])\n        v_data = v.compute().data\n\n        ind = Variable([""a"", ""b""], [[0, 1, 1], [1, 1, 0]])\n        v_new = v[ind]\n        assert v_new.dims == (""a"", ""b"", ""y"")\n        assert_array_equal(v_new, v_data[[[0, 1, 1], [1, 1, 0]], :])\n\n        # It would be ok if indexed with the multi-dimensional array including\n        # the same name\n        ind = Variable([""x"", ""b""], [[0, 1, 1], [1, 1, 0]])\n        v_new = v[ind]\n        assert v_new.dims == (""x"", ""b"", ""y"")\n        assert_array_equal(v_new, v_data[[[0, 1, 1], [1, 1, 0]], :])\n\n        ind = Variable([""a"", ""b""], [[0, 1, 2], [2, 1, 0]])\n        v_new = v[dict(y=ind)]\n        assert v_new.dims == (""x"", ""a"", ""b"")\n        assert_array_equal(v_new, v_data[:, ([0, 1, 2], [2, 1, 0])])\n\n        ind = Variable([""a"", ""b""], [[0, 0], [1, 1]])\n        v_new = v[dict(x=[1, 0], y=ind)]\n        assert v_new.dims == (""x"", ""a"", ""b"")\n        assert_array_equal(v_new, v_data[[1, 0]][:, ind])\n\n        # along diagonal\n        ind = Variable([""a""], [0, 1])\n        v_new = v[ind, ind]\n        assert v_new.dims == (""a"",)\n        assert_array_equal(v_new, v_data[[0, 1], [0, 1]])\n\n        # with integer\n        ind = Variable([""a"", ""b""], [[0, 0], [1, 1]])\n        v_new = v[dict(x=0, y=ind)]\n        assert v_new.dims == (""a"", ""b"")\n        assert_array_equal(v_new[0], v_data[0][[0, 0]])\n        assert_array_equal(v_new[1], v_data[0][[1, 1]])\n\n        # with slice\n        ind = Variable([""a"", ""b""], [[0, 0], [1, 1]])\n        v_new = v[dict(x=slice(None), y=ind)]\n        assert v_new.dims == (""x"", ""a"", ""b"")\n        assert_array_equal(v_new, v_data[:, [[0, 0], [1, 1]]])\n\n        ind = Variable([""a"", ""b""], [[0, 0], [1, 1]])\n        v_new = v[dict(x=ind, y=slice(None))]\n        assert v_new.dims == (""a"", ""b"", ""y"")\n        assert_array_equal(v_new, v_data[[[0, 0], [1, 1]], :])\n\n        ind = Variable([""a"", ""b""], [[0, 0], [1, 1]])\n        v_new = v[dict(x=ind, y=slice(None, 1))]\n        assert v_new.dims == (""a"", ""b"", ""y"")\n        assert_array_equal(v_new, v_data[[[0, 0], [1, 1]], slice(None, 1)])\n\n        # slice matches explicit dimension\n        ind = Variable([""y""], [0, 1])\n        v_new = v[ind, :2]\n        assert v_new.dims == (""y"",)\n        assert_array_equal(v_new, v_data[[0, 1], [0, 1]])\n\n        # with multiple slices\n        v = self.cls([""x"", ""y"", ""z""], [[[1, 2, 3], [4, 5, 6]]])\n        ind = Variable([""a"", ""b""], [[0]])\n        v_new = v[ind, :, :]\n        expected = Variable([""a"", ""b"", ""y"", ""z""], v.data[np.newaxis, ...])\n        assert_identical(v_new, expected)\n\n        v = Variable([""w"", ""x"", ""y"", ""z""], [[[[1, 2, 3], [4, 5, 6]]]])\n        ind = Variable([""y""], [0])\n        v_new = v[ind, :, 1:2, 2]\n        expected = Variable([""y"", ""x""], [[6]])\n        assert_identical(v_new, expected)\n\n        # slice and vector mixed indexing resulting in the same dimension\n        v = Variable([""x"", ""y"", ""z""], np.arange(60).reshape(3, 4, 5))\n        ind = Variable([""x""], [0, 1, 2])\n        v_new = v[:, ind]\n        expected = Variable((""x"", ""z""), np.zeros((3, 5)))\n        expected[0] = v.data[0, 0]\n        expected[1] = v.data[1, 1]\n        expected[2] = v.data[2, 2]\n        assert_identical(v_new, expected)\n\n        v_new = v[:, ind.data]\n        assert v_new.shape == (3, 3, 5)\n\n    def test_getitem_error(self):\n        v = self.cls([""x"", ""y""], [[0, 1, 2], [3, 4, 5]])\n\n        with raises_regex(IndexError, ""labeled multi-""):\n            v[[[0, 1], [1, 2]]]\n\n        ind_x = Variable([""a""], [0, 1, 1])\n        ind_y = Variable([""a""], [0, 1])\n        with raises_regex(IndexError, ""Dimensions of indexers ""):\n            v[ind_x, ind_y]\n\n        ind = Variable([""a"", ""b""], [[True, False], [False, True]])\n        with raises_regex(IndexError, ""2-dimensional boolean""):\n            v[dict(x=ind)]\n\n        v = Variable([""x"", ""y"", ""z""], np.arange(60).reshape(3, 4, 5))\n        ind = Variable([""x""], [0, 1])\n        with raises_regex(IndexError, ""Dimensions of indexers mis""):\n            v[:, ind]\n\n    @pytest.mark.parametrize(\n        ""mode"",\n        [\n            ""mean"",\n            pytest.param(\n                ""median"",\n                marks=pytest.mark.xfail(reason=""median is not implemented by Dask""),\n            ),\n            pytest.param(\n                ""reflect"", marks=pytest.mark.xfail(reason=""dask.array.pad bug"")\n            ),\n            ""edge"",\n            pytest.param(\n                ""linear_ramp"",\n                marks=pytest.mark.xfail(\n                    reason=""pint bug: https://github.com/hgrecco/pint/issues/1026""\n                ),\n            ),\n            ""maximum"",\n            ""minimum"",\n            ""symmetric"",\n            ""wrap"",\n        ],\n    )\n    @pytest.mark.parametrize(""xr_arg, np_arg"", _PAD_XR_NP_ARGS)\n    def test_pad(self, mode, xr_arg, np_arg):\n        data = np.arange(4 * 3 * 2).reshape(4, 3, 2)\n        v = self.cls([""x"", ""y"", ""z""], data)\n\n        actual = v.pad(mode=mode, **xr_arg)\n        expected = np.pad(data, np_arg, mode=mode)\n\n        assert_array_equal(actual, expected)\n        assert isinstance(actual._data, type(v._data))\n\n    @pytest.mark.parametrize(""xr_arg, np_arg"", _PAD_XR_NP_ARGS)\n    def test_pad_constant_values(self, xr_arg, np_arg):\n        data = np.arange(4 * 3 * 2).reshape(4, 3, 2)\n        v = self.cls([""x"", ""y"", ""z""], data)\n\n        actual = v.pad(**xr_arg)\n        expected = np.pad(\n            np.array(v.data.astype(float)),\n            np_arg,\n            mode=""constant"",\n            constant_values=np.nan,\n        )\n        assert_array_equal(actual, expected)\n        assert isinstance(actual._data, type(v._data))\n\n        # for the boolean array, we pad False\n        data = np.full_like(data, False, dtype=bool).reshape(4, 3, 2)\n        v = self.cls([""x"", ""y"", ""z""], data)\n\n        actual = v.pad(mode=""constant"", constant_values=False, **xr_arg)\n        expected = np.pad(\n            np.array(v.data), np_arg, mode=""constant"", constant_values=False\n        )\n        assert_array_equal(actual, expected)\n\n    def test_rolling_window(self):\n        # Just a working test. See test_nputils for the algorithm validation\n        v = self.cls([""x"", ""y"", ""z""], np.arange(40 * 30 * 2).reshape(40, 30, 2))\n        for (d, w) in [(""x"", 3), (""y"", 5)]:\n            v_rolling = v.rolling_window(d, w, d + ""_window"")\n            assert v_rolling.dims == (""x"", ""y"", ""z"", d + ""_window"")\n            assert v_rolling.shape == v.shape + (w,)\n\n            v_rolling = v.rolling_window(d, w, d + ""_window"", center=True)\n            assert v_rolling.dims == (""x"", ""y"", ""z"", d + ""_window"")\n            assert v_rolling.shape == v.shape + (w,)\n\n            # dask and numpy result should be the same\n            v_loaded = v.load().rolling_window(d, w, d + ""_window"", center=True)\n            assert_array_equal(v_rolling, v_loaded)\n\n            # numpy backend should not be over-written\n            if isinstance(v._data, np.ndarray):\n                with pytest.raises(ValueError):\n                    v_loaded[0] = 1.0\n\n\nclass TestVariable(VariableSubclassobjects):\n    cls = staticmethod(Variable)\n\n    @pytest.fixture(autouse=True)\n    def setup(self):\n        self.d = np.random.random((10, 3)).astype(np.float64)\n\n    def test_data_and_values(self):\n        v = Variable([""time"", ""x""], self.d)\n        assert_array_equal(v.data, self.d)\n        assert_array_equal(v.values, self.d)\n        assert source_ndarray(v.values) is self.d\n        with pytest.raises(ValueError):\n            # wrong size\n            v.values = np.random.random(5)\n        d2 = np.random.random((10, 3))\n        v.values = d2\n        assert source_ndarray(v.values) is d2\n        d3 = np.random.random((10, 3))\n        v.data = d3\n        assert source_ndarray(v.data) is d3\n\n    def test_numpy_same_methods(self):\n        v = Variable([], np.float32(0.0))\n        assert v.item() == 0\n        assert type(v.item()) is float\n\n        v = IndexVariable(""x"", np.arange(5))\n        assert 2 == v.searchsorted(2)\n\n    def test_datetime64_conversion_scalar(self):\n        expected = np.datetime64(""2000-01-01"", ""ns"")\n        for values in [\n            np.datetime64(""2000-01-01""),\n            pd.Timestamp(""2000-01-01T00""),\n            datetime(2000, 1, 1),\n        ]:\n            v = Variable([], values)\n            assert v.dtype == np.dtype(""datetime64[ns]"")\n            assert v.values == expected\n            assert v.values.dtype == np.dtype(""datetime64[ns]"")\n\n    def test_timedelta64_conversion_scalar(self):\n        expected = np.timedelta64(24 * 60 * 60 * 10 ** 9, ""ns"")\n        for values in [\n            np.timedelta64(1, ""D""),\n            pd.Timedelta(""1 day""),\n            timedelta(days=1),\n        ]:\n            v = Variable([], values)\n            assert v.dtype == np.dtype(""timedelta64[ns]"")\n            assert v.values == expected\n            assert v.values.dtype == np.dtype(""timedelta64[ns]"")\n\n    def test_0d_str(self):\n        v = Variable([], ""foo"")\n        assert v.dtype == np.dtype(""U3"")\n        assert v.values == ""foo""\n\n        v = Variable([], np.string_(""foo""))\n        assert v.dtype == np.dtype(""S3"")\n        assert v.values == bytes(""foo"", ""ascii"")\n\n    def test_0d_datetime(self):\n        v = Variable([], pd.Timestamp(""2000-01-01""))\n        assert v.dtype == np.dtype(""datetime64[ns]"")\n        assert v.values == np.datetime64(""2000-01-01"", ""ns"")\n\n    def test_0d_timedelta(self):\n        for td in [pd.to_timedelta(""1s""), np.timedelta64(1, ""s"")]:\n            v = Variable([], td)\n            assert v.dtype == np.dtype(""timedelta64[ns]"")\n            assert v.values == np.timedelta64(10 ** 9, ""ns"")\n\n    def test_equals_and_identical(self):\n        d = np.random.rand(10, 3)\n        d[0, 0] = np.nan\n        v1 = Variable((""dim1"", ""dim2""), data=d, attrs={""att1"": 3, ""att2"": [1, 2, 3]})\n        v2 = Variable((""dim1"", ""dim2""), data=d, attrs={""att1"": 3, ""att2"": [1, 2, 3]})\n        assert v1.equals(v2)\n        assert v1.identical(v2)\n\n        v3 = Variable((""dim1"", ""dim3""), data=d)\n        assert not v1.equals(v3)\n\n        v4 = Variable((""dim1"", ""dim2""), data=d)\n        assert v1.equals(v4)\n        assert not v1.identical(v4)\n\n        v5 = deepcopy(v1)\n        v5.values[:] = np.random.rand(10, 3)\n        assert not v1.equals(v5)\n\n        assert not v1.equals(None)\n        assert not v1.equals(d)\n\n        assert not v1.identical(None)\n        assert not v1.identical(d)\n\n    def test_broadcast_equals(self):\n        v1 = Variable((), np.nan)\n        v2 = Variable((""x""), [np.nan, np.nan])\n        assert v1.broadcast_equals(v2)\n        assert not v1.equals(v2)\n        assert not v1.identical(v2)\n\n        v3 = Variable((""x""), [np.nan])\n        assert v1.broadcast_equals(v3)\n        assert not v1.equals(v3)\n        assert not v1.identical(v3)\n\n        assert not v1.broadcast_equals(None)\n\n        v4 = Variable((""x""), [np.nan] * 3)\n        assert not v2.broadcast_equals(v4)\n\n    def test_no_conflicts(self):\n        v1 = Variable((""x""), [1, 2, np.nan, np.nan])\n        v2 = Variable((""x""), [np.nan, 2, 3, np.nan])\n        assert v1.no_conflicts(v2)\n        assert not v1.equals(v2)\n        assert not v1.broadcast_equals(v2)\n        assert not v1.identical(v2)\n\n        assert not v1.no_conflicts(None)\n\n        v3 = Variable((""y""), [np.nan, 2, 3, np.nan])\n        assert not v3.no_conflicts(v1)\n\n        d = np.array([1, 2, np.nan, np.nan])\n        assert not v1.no_conflicts(d)\n        assert not v2.no_conflicts(d)\n\n        v4 = Variable((""w"", ""x""), [d])\n        assert v1.no_conflicts(v4)\n\n    def test_as_variable(self):\n        data = np.arange(10)\n        expected = Variable(""x"", data)\n        expected_extra = Variable(\n            ""x"", data, attrs={""myattr"": ""val""}, encoding={""scale_factor"": 1}\n        )\n\n        assert_identical(expected, as_variable(expected))\n\n        ds = Dataset({""x"": expected})\n        var = as_variable(ds[""x""]).to_base_variable()\n        assert_identical(expected, var)\n        assert not isinstance(ds[""x""], Variable)\n        assert isinstance(as_variable(ds[""x""]), Variable)\n\n        xarray_tuple = (\n            expected_extra.dims,\n            expected_extra.values,\n            expected_extra.attrs,\n            expected_extra.encoding,\n        )\n        assert_identical(expected_extra, as_variable(xarray_tuple))\n\n        with raises_regex(TypeError, ""tuple of form""):\n            as_variable(tuple(data))\n        with raises_regex(ValueError, ""tuple of form""):  # GH1016\n            as_variable((""five"", ""six"", ""seven""))\n        with raises_regex(TypeError, ""without an explicit list of dimensions""):\n            as_variable(data)\n\n        actual = as_variable(data, name=""x"")\n        assert_identical(expected.to_index_variable(), actual)\n\n        actual = as_variable(0)\n        expected = Variable([], 0)\n        assert_identical(expected, actual)\n\n        data = np.arange(9).reshape((3, 3))\n        expected = Variable((""x"", ""y""), data)\n        with raises_regex(ValueError, ""without explicit dimension names""):\n            as_variable(data, name=""x"")\n        with raises_regex(ValueError, ""has more than 1-dimension""):\n            as_variable(expected, name=""x"")\n\n        # test datetime, timedelta conversion\n        dt = np.array([datetime(1999, 1, 1) + timedelta(days=x) for x in range(10)])\n        assert as_variable(dt, ""time"").dtype.kind == ""M""\n        td = np.array([timedelta(days=x) for x in range(10)])\n        assert as_variable(td, ""time"").dtype.kind == ""m""\n\n    def test_repr(self):\n        v = Variable([""time"", ""x""], [[1, 2, 3], [4, 5, 6]], {""foo"": ""bar""})\n        expected = dedent(\n            """"""\n        <xarray.Variable (time: 2, x: 3)>\n        array([[1, 2, 3],\n               [4, 5, 6]])\n        Attributes:\n            foo:      bar\n        """"""\n        ).strip()\n        assert expected == repr(v)\n\n    def test_repr_lazy_data(self):\n        v = Variable(""x"", LazilyOuterIndexedArray(np.arange(2e5)))\n        assert ""200000 values with dtype"" in repr(v)\n        assert isinstance(v._data, LazilyOuterIndexedArray)\n\n    def test_detect_indexer_type(self):\n        """""" Tests indexer type was correctly detected. """"""\n        data = np.random.random((10, 11))\n        v = Variable([""x"", ""y""], data)\n\n        _, ind, _ = v._broadcast_indexes((0, 1))\n        assert type(ind) == indexing.BasicIndexer\n\n        _, ind, _ = v._broadcast_indexes((0, slice(0, 8, 2)))\n        assert type(ind) == indexing.BasicIndexer\n\n        _, ind, _ = v._broadcast_indexes((0, [0, 1]))\n        assert type(ind) == indexing.OuterIndexer\n\n        _, ind, _ = v._broadcast_indexes(([0, 1], 1))\n        assert type(ind) == indexing.OuterIndexer\n\n        _, ind, _ = v._broadcast_indexes(([0, 1], [1, 2]))\n        assert type(ind) == indexing.OuterIndexer\n\n        _, ind, _ = v._broadcast_indexes(([0, 1], slice(0, 8, 2)))\n        assert type(ind) == indexing.OuterIndexer\n\n        vind = Variable((""a"",), [0, 1])\n        _, ind, _ = v._broadcast_indexes((vind, slice(0, 8, 2)))\n        assert type(ind) == indexing.OuterIndexer\n\n        vind = Variable((""y"",), [0, 1])\n        _, ind, _ = v._broadcast_indexes((vind, 3))\n        assert type(ind) == indexing.OuterIndexer\n\n        vind = Variable((""a"",), [0, 1])\n        _, ind, _ = v._broadcast_indexes((vind, vind))\n        assert type(ind) == indexing.VectorizedIndexer\n\n        vind = Variable((""a"", ""b""), [[0, 2], [1, 3]])\n        _, ind, _ = v._broadcast_indexes((vind, 3))\n        assert type(ind) == indexing.VectorizedIndexer\n\n    def test_indexer_type(self):\n        # GH:issue:1688. Wrong indexer type induces NotImplementedError\n        data = np.random.random((10, 11))\n        v = Variable([""x"", ""y""], data)\n\n        def assert_indexer_type(key, object_type):\n            dims, index_tuple, new_order = v._broadcast_indexes(key)\n            assert isinstance(index_tuple, object_type)\n\n        # should return BasicIndexer\n        assert_indexer_type((0, 1), BasicIndexer)\n        assert_indexer_type((0, slice(None, None)), BasicIndexer)\n        assert_indexer_type((Variable([], 3), slice(None, None)), BasicIndexer)\n        assert_indexer_type((Variable([], 3), (Variable([], 6))), BasicIndexer)\n\n        # should return OuterIndexer\n        assert_indexer_type(([0, 1], 1), OuterIndexer)\n        assert_indexer_type(([0, 1], [1, 2]), OuterIndexer)\n        assert_indexer_type((Variable((""x""), [0, 1]), 1), OuterIndexer)\n        assert_indexer_type((Variable((""x""), [0, 1]), slice(None, None)), OuterIndexer)\n        assert_indexer_type(\n            (Variable((""x""), [0, 1]), Variable((""y""), [0, 1])), OuterIndexer\n        )\n\n        # should return VectorizedIndexer\n        assert_indexer_type((Variable((""y""), [0, 1]), [0, 1]), VectorizedIndexer)\n        assert_indexer_type(\n            (Variable((""z""), [0, 1]), Variable((""z""), [0, 1])), VectorizedIndexer\n        )\n        assert_indexer_type(\n            (\n                Variable((""a"", ""b""), [[0, 1], [1, 2]]),\n                Variable((""a"", ""b""), [[0, 1], [1, 2]]),\n            ),\n            VectorizedIndexer,\n        )\n\n    def test_items(self):\n        data = np.random.random((10, 11))\n        v = Variable([""x"", ""y""], data)\n        # test slicing\n        assert_identical(v, v[:])\n        assert_identical(v, v[...])\n        assert_identical(Variable([""y""], data[0]), v[0])\n        assert_identical(Variable([""x""], data[:, 0]), v[:, 0])\n        assert_identical(Variable([""x"", ""y""], data[:3, :2]), v[:3, :2])\n        # test array indexing\n        x = Variable([""x""], np.arange(10))\n        y = Variable([""y""], np.arange(11))\n        assert_identical(v, v[x.values])\n        assert_identical(v, v[x])\n        assert_identical(v[:3], v[x < 3])\n        assert_identical(v[:, 3:], v[:, y >= 3])\n        assert_identical(v[:3, 3:], v[x < 3, y >= 3])\n        assert_identical(v[:3, :2], v[x[:3], y[:2]])\n        assert_identical(v[:3, :2], v[range(3), range(2)])\n        # test iteration\n        for n, item in enumerate(v):\n            assert_identical(Variable([""y""], data[n]), item)\n        with raises_regex(TypeError, ""iteration over a 0-d""):\n            iter(Variable([], 0))\n        # test setting\n        v.values[:] = 0\n        assert np.all(v.values == 0)\n        # test orthogonal setting\n        v[range(10), range(11)] = 1\n        assert_array_equal(v.values, np.ones((10, 11)))\n\n    def test_getitem_basic(self):\n        v = self.cls([""x"", ""y""], [[0, 1, 2], [3, 4, 5]])\n\n        # int argument\n        v_new = v[0]\n        assert v_new.dims == (""y"",)\n        assert_array_equal(v_new, v._data[0])\n\n        # slice argument\n        v_new = v[:2]\n        assert v_new.dims == (""x"", ""y"")\n        assert_array_equal(v_new, v._data[:2])\n\n        # list arguments\n        v_new = v[[0]]\n        assert v_new.dims == (""x"", ""y"")\n        assert_array_equal(v_new, v._data[[0]])\n\n        v_new = v[[]]\n        assert v_new.dims == (""x"", ""y"")\n        assert_array_equal(v_new, v._data[[]])\n\n        # dict arguments\n        v_new = v[dict(x=0)]\n        assert v_new.dims == (""y"",)\n        assert_array_equal(v_new, v._data[0])\n\n        v_new = v[dict(x=0, y=slice(None))]\n        assert v_new.dims == (""y"",)\n        assert_array_equal(v_new, v._data[0])\n\n        v_new = v[dict(x=0, y=1)]\n        assert v_new.dims == ()\n        assert_array_equal(v_new, v._data[0, 1])\n\n        v_new = v[dict(y=1)]\n        assert v_new.dims == (""x"",)\n        assert_array_equal(v_new, v._data[:, 1])\n\n        # tuple argument\n        v_new = v[(slice(None), 1)]\n        assert v_new.dims == (""x"",)\n        assert_array_equal(v_new, v._data[:, 1])\n\n        # test that we obtain a modifiable view when taking a 0d slice\n        v_new = v[0, 0]\n        v_new[...] += 99\n        assert_array_equal(v_new, v._data[0, 0])\n\n    def test_getitem_with_mask_2d_input(self):\n        v = Variable((""x"", ""y""), [[0, 1, 2], [3, 4, 5]])\n        assert_identical(\n            v._getitem_with_mask(([-1, 0], [1, -1])),\n            Variable((""x"", ""y""), [[np.nan, np.nan], [1, np.nan]]),\n        )\n        assert_identical(v._getitem_with_mask((slice(2), [0, 1, 2])), v)\n\n    def test_isel(self):\n        v = Variable([""time"", ""x""], self.d)\n        assert_identical(v.isel(time=slice(None)), v)\n        assert_identical(v.isel(time=0), v[0])\n        assert_identical(v.isel(time=slice(0, 3)), v[:3])\n        assert_identical(v.isel(x=0), v[:, 0])\n        assert_identical(v.isel(x=[0, 2]), v[:, [0, 2]])\n        assert_identical(v.isel(time=[]), v[[]])\n        with raises_regex(\n            ValueError,\n            r""dimensions {\'not_a_dim\'} do not exist. Expected one or more of ""\n            r""\\(\'time\', \'x\'\\)"",\n        ):\n            v.isel(not_a_dim=0)\n        with pytest.warns(\n            UserWarning,\n            match=r""dimensions {\'not_a_dim\'} do not exist. Expected one or more of ""\n            r""\\(\'time\', \'x\'\\)"",\n        ):\n            v.isel(not_a_dim=0, missing_dims=""warn"")\n        assert_identical(v, v.isel(not_a_dim=0, missing_dims=""ignore""))\n\n    def test_index_0d_numpy_string(self):\n        # regression test to verify our work around for indexing 0d strings\n        v = Variable([], np.string_(""asdf""))\n        assert_identical(v[()], v)\n\n        v = Variable([], np.unicode_(""asdf""))\n        assert_identical(v[()], v)\n\n    def test_indexing_0d_unicode(self):\n        # regression test for GH568\n        actual = Variable((""x""), [""tmax""])[0][()]\n        expected = Variable((), ""tmax"")\n        assert_identical(actual, expected)\n\n    @pytest.mark.parametrize(""fill_value"", [dtypes.NA, 2, 2.0])\n    def test_shift(self, fill_value):\n        v = Variable(""x"", [1, 2, 3, 4, 5])\n\n        assert_identical(v, v.shift(x=0))\n        assert v is not v.shift(x=0)\n\n        expected = Variable(""x"", [np.nan, np.nan, 1, 2, 3])\n        assert_identical(expected, v.shift(x=2))\n\n        if fill_value == dtypes.NA:\n            # if we supply the default, we expect the missing value for a\n            # float array\n            fill_value_exp = np.nan\n        else:\n            fill_value_exp = fill_value\n\n        expected = Variable(""x"", [fill_value_exp, 1, 2, 3, 4])\n        assert_identical(expected, v.shift(x=1, fill_value=fill_value))\n\n        expected = Variable(""x"", [2, 3, 4, 5, fill_value_exp])\n        assert_identical(expected, v.shift(x=-1, fill_value=fill_value))\n\n        expected = Variable(""x"", [fill_value_exp] * 5)\n        assert_identical(expected, v.shift(x=5, fill_value=fill_value))\n        assert_identical(expected, v.shift(x=6, fill_value=fill_value))\n\n        with raises_regex(ValueError, ""dimension""):\n            v.shift(z=0)\n\n        v = Variable(""x"", [1, 2, 3, 4, 5], {""foo"": ""bar""})\n        assert_identical(v, v.shift(x=0))\n\n        expected = Variable(""x"", [fill_value_exp, 1, 2, 3, 4], {""foo"": ""bar""})\n        assert_identical(expected, v.shift(x=1, fill_value=fill_value))\n\n    def test_shift2d(self):\n        v = Variable((""x"", ""y""), [[1, 2], [3, 4]])\n        expected = Variable((""x"", ""y""), [[np.nan, np.nan], [np.nan, 1]])\n        assert_identical(expected, v.shift(x=1, y=1))\n\n    def test_roll(self):\n        v = Variable(""x"", [1, 2, 3, 4, 5])\n\n        assert_identical(v, v.roll(x=0))\n        assert v is not v.roll(x=0)\n\n        expected = Variable(""x"", [5, 1, 2, 3, 4])\n        assert_identical(expected, v.roll(x=1))\n        assert_identical(expected, v.roll(x=-4))\n        assert_identical(expected, v.roll(x=6))\n\n        expected = Variable(""x"", [4, 5, 1, 2, 3])\n        assert_identical(expected, v.roll(x=2))\n        assert_identical(expected, v.roll(x=-3))\n\n        with raises_regex(ValueError, ""dimension""):\n            v.roll(z=0)\n\n    def test_roll_consistency(self):\n        v = Variable((""x"", ""y""), np.random.randn(5, 6))\n\n        for axis, dim in [(0, ""x""), (1, ""y"")]:\n            for shift in [-3, 0, 1, 7, 11]:\n                expected = np.roll(v.values, shift, axis=axis)\n                actual = v.roll(**{dim: shift}).values\n                assert_array_equal(expected, actual)\n\n    def test_transpose(self):\n        v = Variable([""time"", ""x""], self.d)\n        v2 = Variable([""x"", ""time""], self.d.T)\n        assert_identical(v, v2.transpose())\n        assert_identical(v.transpose(), v.T)\n        x = np.random.randn(2, 3, 4, 5)\n        w = Variable([""a"", ""b"", ""c"", ""d""], x)\n        w2 = Variable([""d"", ""b"", ""c"", ""a""], np.einsum(""abcd->dbca"", x))\n        assert w2.shape == (5, 3, 4, 2)\n        assert_identical(w2, w.transpose(""d"", ""b"", ""c"", ""a""))\n        assert_identical(w2, w.transpose(""d"", ..., ""a""))\n        assert_identical(w2, w.transpose(""d"", ""b"", ""c"", ...))\n        assert_identical(w2, w.transpose(..., ""b"", ""c"", ""a""))\n        assert_identical(w, w2.transpose(""a"", ""b"", ""c"", ""d""))\n        w3 = Variable([""b"", ""c"", ""d"", ""a""], np.einsum(""abcd->bcda"", x))\n        assert_identical(w, w3.transpose(""a"", ""b"", ""c"", ""d""))\n\n    def test_transpose_0d(self):\n        for value in [\n            3.5,\n            (""a"", 1),\n            np.datetime64(""2000-01-01""),\n            np.timedelta64(1, ""h""),\n            None,\n            object(),\n        ]:\n            variable = Variable([], value)\n            actual = variable.transpose()\n            assert actual.identical(variable)\n\n    def test_squeeze(self):\n        v = Variable([""x"", ""y""], [[1]])\n        assert_identical(Variable([], 1), v.squeeze())\n        assert_identical(Variable([""y""], [1]), v.squeeze(""x""))\n        assert_identical(Variable([""y""], [1]), v.squeeze([""x""]))\n        assert_identical(Variable([""x""], [1]), v.squeeze(""y""))\n        assert_identical(Variable([], 1), v.squeeze([""x"", ""y""]))\n\n        v = Variable([""x"", ""y""], [[1, 2]])\n        assert_identical(Variable([""y""], [1, 2]), v.squeeze())\n        assert_identical(Variable([""y""], [1, 2]), v.squeeze(""x""))\n        with raises_regex(ValueError, ""cannot select a dimension""):\n            v.squeeze(""y"")\n\n    def test_get_axis_num(self):\n        v = Variable([""x"", ""y"", ""z""], np.random.randn(2, 3, 4))\n        assert v.get_axis_num(""x"") == 0\n        assert v.get_axis_num([""x""]) == (0,)\n        assert v.get_axis_num([""x"", ""y""]) == (0, 1)\n        assert v.get_axis_num([""z"", ""y"", ""x""]) == (2, 1, 0)\n        with raises_regex(ValueError, ""not found in array dim""):\n            v.get_axis_num(""foobar"")\n\n    def test_set_dims(self):\n        v = Variable([""x""], [0, 1])\n        actual = v.set_dims([""x"", ""y""])\n        expected = Variable([""x"", ""y""], [[0], [1]])\n        assert_identical(actual, expected)\n\n        actual = v.set_dims([""y"", ""x""])\n        assert_identical(actual, expected.T)\n\n        actual = v.set_dims({""x"": 2, ""y"": 2})\n        expected = Variable([""x"", ""y""], [[0, 0], [1, 1]])\n        assert_identical(actual, expected)\n\n        v = Variable([""foo""], [0, 1])\n        actual = v.set_dims(""foo"")\n        expected = v\n        assert_identical(actual, expected)\n\n        with raises_regex(ValueError, ""must be a superset""):\n            v.set_dims([""z""])\n\n    def test_set_dims_object_dtype(self):\n        v = Variable([], (""a"", 1))\n        actual = v.set_dims((""x"",), (3,))\n        exp_values = np.empty((3,), dtype=object)\n        for i in range(3):\n            exp_values[i] = (""a"", 1)\n        expected = Variable([""x""], exp_values)\n        assert actual.identical(expected)\n\n    def test_stack(self):\n        v = Variable([""x"", ""y""], [[0, 1], [2, 3]], {""foo"": ""bar""})\n        actual = v.stack(z=(""x"", ""y""))\n        expected = Variable(""z"", [0, 1, 2, 3], v.attrs)\n        assert_identical(actual, expected)\n\n        actual = v.stack(z=(""x"",))\n        expected = Variable((""y"", ""z""), v.data.T, v.attrs)\n        assert_identical(actual, expected)\n\n        actual = v.stack(z=())\n        assert_identical(actual, v)\n\n        actual = v.stack(X=(""x"",), Y=(""y"",)).transpose(""X"", ""Y"")\n        expected = Variable((""X"", ""Y""), v.data, v.attrs)\n        assert_identical(actual, expected)\n\n    def test_stack_errors(self):\n        v = Variable([""x"", ""y""], [[0, 1], [2, 3]], {""foo"": ""bar""})\n\n        with raises_regex(ValueError, ""invalid existing dim""):\n            v.stack(z=(""x1"",))\n        with raises_regex(ValueError, ""cannot create a new dim""):\n            v.stack(x=(""x"",))\n\n    def test_unstack(self):\n        v = Variable(""z"", [0, 1, 2, 3], {""foo"": ""bar""})\n        actual = v.unstack(z={""x"": 2, ""y"": 2})\n        expected = Variable((""x"", ""y""), [[0, 1], [2, 3]], v.attrs)\n        assert_identical(actual, expected)\n\n        actual = v.unstack(z={""x"": 4, ""y"": 1})\n        expected = Variable((""x"", ""y""), [[0], [1], [2], [3]], v.attrs)\n        assert_identical(actual, expected)\n\n        actual = v.unstack(z={""x"": 4})\n        expected = Variable(""x"", [0, 1, 2, 3], v.attrs)\n        assert_identical(actual, expected)\n\n    def test_unstack_errors(self):\n        v = Variable(""z"", [0, 1, 2, 3])\n        with raises_regex(ValueError, ""invalid existing dim""):\n            v.unstack(foo={""x"": 4})\n        with raises_regex(ValueError, ""cannot create a new dim""):\n            v.stack(z=(""z"",))\n        with raises_regex(ValueError, ""the product of the new dim""):\n            v.unstack(z={""x"": 5})\n\n    def test_unstack_2d(self):\n        v = Variable([""x"", ""y""], [[0, 1], [2, 3]])\n        actual = v.unstack(y={""z"": 2})\n        expected = Variable([""x"", ""z""], v.data)\n        assert_identical(actual, expected)\n\n        actual = v.unstack(x={""z"": 2})\n        expected = Variable([""y"", ""z""], v.data.T)\n        assert_identical(actual, expected)\n\n    def test_stack_unstack_consistency(self):\n        v = Variable([""x"", ""y""], [[0, 1], [2, 3]])\n        actual = v.stack(z=(""x"", ""y"")).unstack(z={""x"": 2, ""y"": 2})\n        assert_identical(actual, v)\n\n    def test_broadcasting_math(self):\n        x = np.random.randn(2, 3)\n        v = Variable([""a"", ""b""], x)\n        # 1d to 2d broadcasting\n        assert_identical(v * v, Variable([""a"", ""b""], np.einsum(""ab,ab->ab"", x, x)))\n        assert_identical(v * v[0], Variable([""a"", ""b""], np.einsum(""ab,b->ab"", x, x[0])))\n        assert_identical(v[0] * v, Variable([""b"", ""a""], np.einsum(""b,ab->ba"", x[0], x)))\n        assert_identical(\n            v[0] * v[:, 0], Variable([""b"", ""a""], np.einsum(""b,a->ba"", x[0], x[:, 0]))\n        )\n        # higher dim broadcasting\n        y = np.random.randn(3, 4, 5)\n        w = Variable([""b"", ""c"", ""d""], y)\n        assert_identical(\n            v * w, Variable([""a"", ""b"", ""c"", ""d""], np.einsum(""ab,bcd->abcd"", x, y))\n        )\n        assert_identical(\n            w * v, Variable([""b"", ""c"", ""d"", ""a""], np.einsum(""bcd,ab->bcda"", y, x))\n        )\n        assert_identical(\n            v * w[0], Variable([""a"", ""b"", ""c"", ""d""], np.einsum(""ab,cd->abcd"", x, y[0]))\n        )\n\n    def test_broadcasting_failures(self):\n        a = Variable([""x""], np.arange(10))\n        b = Variable([""x""], np.arange(5))\n        c = Variable([""x"", ""x""], np.arange(100).reshape(10, 10))\n        with raises_regex(ValueError, ""mismatched lengths""):\n            a + b\n        with raises_regex(ValueError, ""duplicate dimensions""):\n            a + c\n\n    def test_inplace_math(self):\n        x = np.arange(5)\n        v = Variable([""x""], x)\n        v2 = v\n        v2 += 1\n        assert v is v2\n        # since we provided an ndarray for data, it is also modified in-place\n        assert source_ndarray(v.values) is x\n        assert_array_equal(v.values, np.arange(5) + 1)\n\n        with raises_regex(ValueError, ""dimensions cannot change""):\n            v += Variable(""y"", np.arange(5))\n\n    def test_reduce(self):\n        v = Variable([""x"", ""y""], self.d, {""ignored"": ""attributes""})\n        assert_identical(v.reduce(np.std, ""x""), Variable([""y""], self.d.std(axis=0)))\n        assert_identical(v.reduce(np.std, axis=0), v.reduce(np.std, dim=""x""))\n        assert_identical(\n            v.reduce(np.std, [""y"", ""x""]), Variable([], self.d.std(axis=(0, 1)))\n        )\n        assert_identical(v.reduce(np.std), Variable([], self.d.std()))\n        assert_identical(\n            v.reduce(np.mean, ""x"").reduce(np.std, ""y""),\n            Variable([], self.d.mean(axis=0).std()),\n        )\n        assert_allclose(v.mean(""x""), v.reduce(np.mean, ""x""))\n\n        with raises_regex(ValueError, ""cannot supply both""):\n            v.mean(dim=""x"", axis=0)\n        with pytest.warns(DeprecationWarning, match=""allow_lazy is deprecated""):\n            v.mean(dim=""x"", allow_lazy=True)\n        with pytest.warns(DeprecationWarning, match=""allow_lazy is deprecated""):\n            v.mean(dim=""x"", allow_lazy=False)\n\n    @pytest.mark.parametrize(""skipna"", [True, False])\n    @pytest.mark.parametrize(""q"", [0.25, [0.50], [0.25, 0.75]])\n    @pytest.mark.parametrize(\n        ""axis, dim"", zip([None, 0, [0], [0, 1]], [None, ""x"", [""x""], [""x"", ""y""]])\n    )\n    def test_quantile(self, q, axis, dim, skipna):\n        v = Variable([""x"", ""y""], self.d)\n        actual = v.quantile(q, dim=dim, skipna=skipna)\n        _percentile_func = np.nanpercentile if skipna else np.percentile\n        expected = _percentile_func(self.d, np.array(q) * 100, axis=axis)\n        np.testing.assert_allclose(actual.values, expected)\n\n    @requires_dask\n    @pytest.mark.parametrize(""q"", [0.25, [0.50], [0.25, 0.75]])\n    @pytest.mark.parametrize(""axis, dim"", [[1, ""y""], [[1], [""y""]]])\n    def test_quantile_dask(self, q, axis, dim):\n        v = Variable([""x"", ""y""], self.d).chunk({""x"": 2})\n        actual = v.quantile(q, dim=dim)\n        assert isinstance(actual.data, dask_array_type)\n        expected = np.nanpercentile(self.d, np.array(q) * 100, axis=axis)\n        np.testing.assert_allclose(actual.values, expected)\n\n    @requires_dask\n    def test_quantile_chunked_dim_error(self):\n        v = Variable([""x"", ""y""], self.d).chunk({""x"": 2})\n\n        with raises_regex(ValueError, ""dimension \'x\'""):\n            v.quantile(0.5, dim=""x"")\n\n    @pytest.mark.parametrize(""q"", [-0.1, 1.1, [2], [0.25, 2]])\n    def test_quantile_out_of_bounds(self, q):\n        v = Variable([""x"", ""y""], self.d)\n\n        # escape special characters\n        with raises_regex(ValueError, r""Quantiles must be in the range \\[0, 1\\]""):\n            v.quantile(q, dim=""x"")\n\n    @requires_dask\n    @requires_bottleneck\n    def test_rank_dask_raises(self):\n        v = Variable([""x""], [3.0, 1.0, np.nan, 2.0, 4.0]).chunk(2)\n        with raises_regex(TypeError, ""arrays stored as dask""):\n            v.rank(""x"")\n\n    @requires_bottleneck\n    def test_rank(self):\n        import bottleneck as bn\n\n        # floats\n        v = Variable([""x"", ""y""], [[3, 4, np.nan, 1]])\n        expect_0 = bn.nanrankdata(v.data, axis=0)\n        expect_1 = bn.nanrankdata(v.data, axis=1)\n        np.testing.assert_allclose(v.rank(""x"").values, expect_0)\n        np.testing.assert_allclose(v.rank(""y"").values, expect_1)\n        # int\n        v = Variable([""x""], [3, 2, 1])\n        expect = bn.rankdata(v.data, axis=0)\n        np.testing.assert_allclose(v.rank(""x"").values, expect)\n        # str\n        v = Variable([""x""], [""c"", ""b"", ""a""])\n        expect = bn.rankdata(v.data, axis=0)\n        np.testing.assert_allclose(v.rank(""x"").values, expect)\n        # pct\n        v = Variable([""x""], [3.0, 1.0, np.nan, 2.0, 4.0])\n        v_expect = Variable([""x""], [0.75, 0.25, np.nan, 0.5, 1.0])\n        assert_equal(v.rank(""x"", pct=True), v_expect)\n        # invalid dim\n        with raises_regex(ValueError, ""not found""):\n            v.rank(""y"")\n\n    def test_big_endian_reduce(self):\n        # regression test for GH489\n        data = np.ones(5, dtype="">f4"")\n        v = Variable([""x""], data)\n        expected = Variable([], 5)\n        assert_identical(expected, v.sum())\n\n    def test_reduce_funcs(self):\n        v = Variable(""x"", np.array([1, np.nan, 2, 3]))\n        assert_identical(v.mean(), Variable([], 2))\n        assert_identical(v.mean(skipna=True), Variable([], 2))\n        assert_identical(v.mean(skipna=False), Variable([], np.nan))\n        assert_identical(np.mean(v), Variable([], 2))\n\n        assert_identical(v.prod(), Variable([], 6))\n        assert_identical(v.cumsum(axis=0), Variable(""x"", np.array([1, 1, 3, 6])))\n        assert_identical(v.cumprod(axis=0), Variable(""x"", np.array([1, 1, 2, 6])))\n        assert_identical(v.var(), Variable([], 2.0 / 3))\n        assert_identical(v.median(), Variable([], 2))\n\n        v = Variable(""x"", [True, False, False])\n        assert_identical(v.any(), Variable([], True))\n        assert_identical(v.all(dim=""x""), Variable([], False))\n\n        v = Variable(""t"", pd.date_range(""2000-01-01"", periods=3))\n        assert v.argmax(skipna=True) == 2\n\n        assert_identical(v.max(), Variable([], pd.Timestamp(""2000-01-03"")))\n\n    def test_reduce_keepdims(self):\n        v = Variable([""x"", ""y""], self.d)\n\n        assert_identical(\n            v.mean(keepdims=True), Variable(v.dims, np.mean(self.d, keepdims=True))\n        )\n        assert_identical(\n            v.mean(dim=""x"", keepdims=True),\n            Variable(v.dims, np.mean(self.d, axis=0, keepdims=True)),\n        )\n        assert_identical(\n            v.mean(dim=""y"", keepdims=True),\n            Variable(v.dims, np.mean(self.d, axis=1, keepdims=True)),\n        )\n        assert_identical(\n            v.mean(dim=[""y"", ""x""], keepdims=True),\n            Variable(v.dims, np.mean(self.d, axis=(1, 0), keepdims=True)),\n        )\n\n        v = Variable([], 1.0)\n        assert_identical(\n            v.mean(keepdims=True), Variable([], np.mean(v.data, keepdims=True))\n        )\n\n    @requires_dask\n    def test_reduce_keepdims_dask(self):\n        import dask.array\n\n        v = Variable([""x"", ""y""], self.d).chunk()\n\n        actual = v.mean(keepdims=True)\n        assert isinstance(actual.data, dask.array.Array)\n\n        expected = Variable(v.dims, np.mean(self.d, keepdims=True))\n        assert_identical(actual, expected)\n\n        actual = v.mean(dim=""y"", keepdims=True)\n        assert isinstance(actual.data, dask.array.Array)\n\n        expected = Variable(v.dims, np.mean(self.d, axis=1, keepdims=True))\n        assert_identical(actual, expected)\n\n    def test_reduce_keep_attrs(self):\n        _attrs = {""units"": ""test"", ""long_name"": ""testing""}\n\n        v = Variable([""x"", ""y""], self.d, _attrs)\n\n        # Test dropped attrs\n        vm = v.mean()\n        assert len(vm.attrs) == 0\n        assert vm.attrs == {}\n\n        # Test kept attrs\n        vm = v.mean(keep_attrs=True)\n        assert len(vm.attrs) == len(_attrs)\n        assert vm.attrs == _attrs\n\n    def test_binary_ops_keep_attrs(self):\n        _attrs = {""units"": ""test"", ""long_name"": ""testing""}\n        a = Variable([""x"", ""y""], np.random.randn(3, 3), _attrs)\n        b = Variable([""x"", ""y""], np.random.randn(3, 3), _attrs)\n        # Test dropped attrs\n        d = a - b  # just one operation\n        assert d.attrs == {}\n        # Test kept attrs\n        with set_options(keep_attrs=True):\n            d = a - b\n        assert d.attrs == _attrs\n\n    def test_count(self):\n        expected = Variable([], 3)\n        actual = Variable([""x""], [1, 2, 3, np.nan]).count()\n        assert_identical(expected, actual)\n\n        v = Variable([""x""], np.array([""1"", ""2"", ""3"", np.nan], dtype=object))\n        actual = v.count()\n        assert_identical(expected, actual)\n\n        actual = Variable([""x""], [True, False, True]).count()\n        assert_identical(expected, actual)\n        assert actual.dtype == int\n\n        expected = Variable([""x""], [2, 3])\n        actual = Variable([""x"", ""y""], [[1, 0, np.nan], [1, 1, 1]]).count(""y"")\n        assert_identical(expected, actual)\n\n    def test_setitem(self):\n        v = Variable([""x"", ""y""], [[0, 3, 2], [3, 4, 5]])\n        v[0, 1] = 1\n        assert v[0, 1] == 1\n\n        v = Variable([""x"", ""y""], [[0, 3, 2], [3, 4, 5]])\n        v[dict(x=[0, 1])] = 1\n        assert_array_equal(v[[0, 1]], np.ones_like(v[[0, 1]]))\n\n        # boolean indexing\n        v = Variable([""x"", ""y""], [[0, 3, 2], [3, 4, 5]])\n        v[dict(x=[True, False])] = 1\n\n        assert_array_equal(v[0], np.ones_like(v[0]))\n        v = Variable([""x"", ""y""], [[0, 3, 2], [3, 4, 5]])\n        v[dict(x=[True, False], y=[False, True, False])] = 1\n        assert v[0, 1] == 1\n\n    def test_setitem_fancy(self):\n        # assignment which should work as np.ndarray does\n        def assert_assigned_2d(array, key_x, key_y, values):\n            expected = array.copy()\n            expected[key_x, key_y] = values\n            v = Variable([""x"", ""y""], array)\n            v[dict(x=key_x, y=key_y)] = values\n            assert_array_equal(expected, v)\n\n        # 1d vectorized indexing\n        assert_assigned_2d(\n            np.random.randn(4, 3),\n            key_x=Variable([""a""], [0, 1]),\n            key_y=Variable([""a""], [0, 1]),\n            values=0,\n        )\n        assert_assigned_2d(\n            np.random.randn(4, 3),\n            key_x=Variable([""a""], [0, 1]),\n            key_y=Variable([""a""], [0, 1]),\n            values=Variable((), 0),\n        )\n        assert_assigned_2d(\n            np.random.randn(4, 3),\n            key_x=Variable([""a""], [0, 1]),\n            key_y=Variable([""a""], [0, 1]),\n            values=Variable((""a""), [3, 2]),\n        )\n        assert_assigned_2d(\n            np.random.randn(4, 3),\n            key_x=slice(None),\n            key_y=Variable([""a""], [0, 1]),\n            values=Variable((""a""), [3, 2]),\n        )\n\n        # 2d-vectorized indexing\n        assert_assigned_2d(\n            np.random.randn(4, 3),\n            key_x=Variable([""a"", ""b""], [[0, 1]]),\n            key_y=Variable([""a"", ""b""], [[1, 0]]),\n            values=0,\n        )\n        assert_assigned_2d(\n            np.random.randn(4, 3),\n            key_x=Variable([""a"", ""b""], [[0, 1]]),\n            key_y=Variable([""a"", ""b""], [[1, 0]]),\n            values=[0],\n        )\n        assert_assigned_2d(\n            np.random.randn(5, 4),\n            key_x=Variable([""a"", ""b""], [[0, 1], [2, 3]]),\n            key_y=Variable([""a"", ""b""], [[1, 0], [3, 3]]),\n            values=[2, 3],\n        )\n\n        # vindex with slice\n        v = Variable([""x"", ""y"", ""z""], np.ones((4, 3, 2)))\n        ind = Variable([""a""], [0, 1])\n        v[dict(x=ind, z=ind)] = 0\n        expected = Variable([""x"", ""y"", ""z""], np.ones((4, 3, 2)))\n        expected[0, :, 0] = 0\n        expected[1, :, 1] = 0\n        assert_identical(expected, v)\n\n        # dimension broadcast\n        v = Variable([""x"", ""y""], np.ones((3, 2)))\n        ind = Variable([""a"", ""b""], [[0, 1]])\n        v[ind, :] = 0\n        expected = Variable([""x"", ""y""], [[0, 0], [0, 0], [1, 1]])\n        assert_identical(expected, v)\n\n        with raises_regex(ValueError, ""shape mismatch""):\n            v[ind, ind] = np.zeros((1, 2, 1))\n\n        v = Variable([""x"", ""y""], [[0, 3, 2], [3, 4, 5]])\n        ind = Variable([""a""], [0, 1])\n        v[dict(x=ind)] = Variable([""a"", ""y""], np.ones((2, 3), dtype=int) * 10)\n        assert_array_equal(v[0], np.ones_like(v[0]) * 10)\n        assert_array_equal(v[1], np.ones_like(v[1]) * 10)\n        assert v.dims == (""x"", ""y"")  # dimension should not change\n\n        # increment\n        v = Variable([""x"", ""y""], np.arange(6).reshape(3, 2))\n        ind = Variable([""a""], [0, 1])\n        v[dict(x=ind)] += 1\n        expected = Variable([""x"", ""y""], [[1, 2], [3, 4], [4, 5]])\n        assert_identical(v, expected)\n\n        ind = Variable([""a""], [0, 0])\n        v[dict(x=ind)] += 1\n        expected = Variable([""x"", ""y""], [[2, 3], [3, 4], [4, 5]])\n        assert_identical(v, expected)\n\n    def test_coarsen(self):\n        v = self.cls([""x""], [0, 1, 2, 3, 4])\n        actual = v.coarsen({""x"": 2}, boundary=""pad"", func=""mean"")\n        expected = self.cls([""x""], [0.5, 2.5, 4])\n        assert_identical(actual, expected)\n\n        actual = v.coarsen({""x"": 2}, func=""mean"", boundary=""pad"", side=""right"")\n        expected = self.cls([""x""], [0, 1.5, 3.5])\n        assert_identical(actual, expected)\n\n        actual = v.coarsen({""x"": 2}, func=np.mean, side=""right"", boundary=""trim"")\n        expected = self.cls([""x""], [1.5, 3.5])\n        assert_identical(actual, expected)\n\n        # working test\n        v = self.cls([""x"", ""y"", ""z""], np.arange(40 * 30 * 2).reshape(40, 30, 2))\n        for windows, func, side, boundary in [\n            ({""x"": 2}, np.mean, ""left"", ""trim""),\n            ({""x"": 2}, np.median, {""x"": ""left""}, ""pad""),\n            ({""x"": 2, ""y"": 3}, np.max, ""left"", {""x"": ""pad"", ""y"": ""trim""}),\n        ]:\n            v.coarsen(windows, func, boundary, side)\n\n    def test_coarsen_2d(self):\n        # 2d-mean should be the same with the successive 1d-mean\n        v = self.cls([""x"", ""y""], np.arange(6 * 12).reshape(6, 12))\n        actual = v.coarsen({""x"": 3, ""y"": 4}, func=""mean"")\n        expected = v.coarsen({""x"": 3}, func=""mean"").coarsen({""y"": 4}, func=""mean"")\n        assert_equal(actual, expected)\n\n        v = self.cls([""x"", ""y""], np.arange(7 * 12).reshape(7, 12))\n        actual = v.coarsen({""x"": 3, ""y"": 4}, func=""mean"", boundary=""trim"")\n        expected = v.coarsen({""x"": 3}, func=""mean"", boundary=""trim"").coarsen(\n            {""y"": 4}, func=""mean"", boundary=""trim""\n        )\n        assert_equal(actual, expected)\n\n        # if there is nan, the two should be different\n        v = self.cls([""x"", ""y""], 1.0 * np.arange(6 * 12).reshape(6, 12))\n        v[2, 4] = np.nan\n        v[3, 5] = np.nan\n        actual = v.coarsen({""x"": 3, ""y"": 4}, func=""mean"", boundary=""trim"")\n        expected = (\n            v.coarsen({""x"": 3}, func=""sum"", boundary=""trim"").coarsen(\n                {""y"": 4}, func=""sum"", boundary=""trim""\n            )\n            / 12\n        )\n        assert not actual.equals(expected)\n        # adjusting the nan count\n        expected[0, 1] *= 12 / 11\n        expected[1, 1] *= 12 / 11\n        assert_allclose(actual, expected)\n\n        v = self.cls((""x"", ""y""), np.arange(4 * 4, dtype=np.float32).reshape(4, 4))\n        actual = v.coarsen(dict(x=2, y=2), func=""count"", boundary=""exact"")\n        expected = self.cls((""x"", ""y""), 4 * np.ones((2, 2)))\n        assert_equal(actual, expected)\n\n        v[0, 0] = np.nan\n        v[-1, -1] = np.nan\n        expected[0, 0] = 3\n        expected[-1, -1] = 3\n        actual = v.coarsen(dict(x=2, y=2), func=""count"", boundary=""exact"")\n        assert_equal(actual, expected)\n\n        actual = v.coarsen(dict(x=2, y=2), func=""sum"", boundary=""exact"", skipna=False)\n        expected = self.cls((""x"", ""y""), [[np.nan, 18], [42, np.nan]])\n        assert_equal(actual, expected)\n\n        actual = v.coarsen(dict(x=2, y=2), func=""sum"", boundary=""exact"", skipna=True)\n        expected = self.cls((""x"", ""y""), [[10, 18], [42, 35]])\n        assert_equal(actual, expected)\n\n    # perhaps @pytest.mark.parametrize(""operation"", [f for f in duck_array_ops])\n    def test_coarsen_keep_attrs(self, operation=""mean""):\n        _attrs = {""units"": ""test"", ""long_name"": ""testing""}\n\n        test_func = getattr(duck_array_ops, operation, None)\n\n        # Test dropped attrs\n        with set_options(keep_attrs=False):\n            new = Variable([""coord""], np.linspace(1, 10, 100), attrs=_attrs).coarsen(\n                windows={""coord"": 1}, func=test_func, boundary=""exact"", side=""left""\n            )\n        assert new.attrs == {}\n\n        # Test kept attrs\n        with set_options(keep_attrs=True):\n            new = Variable([""coord""], np.linspace(1, 10, 100), attrs=_attrs).coarsen(\n                windows={""coord"": 1}, func=test_func, boundary=""exact"", side=""left""\n            )\n        assert new.attrs == _attrs\n\n\n@requires_dask\nclass TestVariableWithDask(VariableSubclassobjects):\n    cls = staticmethod(lambda *args: Variable(*args).chunk())\n\n    @pytest.mark.xfail\n    def test_0d_object_array_with_list(self):\n        super().test_0d_object_array_with_list()\n\n    @pytest.mark.xfail\n    def test_array_interface(self):\n        # dask array does not have `argsort`\n        super().test_array_interface()\n\n    @pytest.mark.xfail\n    def test_copy_index(self):\n        super().test_copy_index()\n\n    @pytest.mark.xfail\n    def test_eq_all_dtypes(self):\n        super().test_eq_all_dtypes()\n\n    def test_getitem_fancy(self):\n        super().test_getitem_fancy()\n\n    def test_getitem_1d_fancy(self):\n        super().test_getitem_1d_fancy()\n\n    def test_getitem_with_mask_nd_indexer(self):\n        import dask.array as da\n\n        v = Variable([""x""], da.arange(3, chunks=3))\n        indexer = Variable((""x"", ""y""), [[0, -1], [-1, 2]])\n        assert_identical(\n            v._getitem_with_mask(indexer, fill_value=-1),\n            self.cls((""x"", ""y""), [[0, -1], [-1, 2]]),\n        )\n\n\n@requires_sparse\nclass TestVariableWithSparse:\n    # TODO inherit VariableSubclassobjects to cover more tests\n\n    def test_as_sparse(self):\n        data = np.arange(12).reshape(3, 4)\n        var = Variable((""x"", ""y""), data)._as_sparse(fill_value=-1)\n        actual = var._to_dense()\n        assert_identical(var, actual)\n\n\nclass TestIndexVariable(VariableSubclassobjects):\n    cls = staticmethod(IndexVariable)\n\n    def test_init(self):\n        with raises_regex(ValueError, ""must be 1-dimensional""):\n            IndexVariable((), 0)\n\n    def test_to_index(self):\n        data = 0.5 * np.arange(10)\n        v = IndexVariable([""time""], data, {""foo"": ""bar""})\n        assert pd.Index(data, name=""time"").identical(v.to_index())\n\n    def test_multiindex_default_level_names(self):\n        midx = pd.MultiIndex.from_product([[""a"", ""b""], [1, 2]])\n        v = IndexVariable([""x""], midx, {""foo"": ""bar""})\n        assert v.to_index().names == (""x_level_0"", ""x_level_1"")\n\n    def test_data(self):\n        x = IndexVariable(""x"", np.arange(3.0))\n        assert isinstance(x._data, PandasIndexAdapter)\n        assert isinstance(x.data, np.ndarray)\n        assert float == x.dtype\n        assert_array_equal(np.arange(3), x)\n        assert float == x.values.dtype\n        with raises_regex(TypeError, ""cannot be modified""):\n            x[:] = 0\n\n    def test_name(self):\n        coord = IndexVariable(""x"", [10.0])\n        assert coord.name == ""x""\n\n        with pytest.raises(AttributeError):\n            coord.name = ""y""\n\n    def test_level_names(self):\n        midx = pd.MultiIndex.from_product(\n            [[""a"", ""b""], [1, 2]], names=[""level_1"", ""level_2""]\n        )\n        x = IndexVariable(""x"", midx)\n        assert x.level_names == midx.names\n\n        assert IndexVariable(""y"", [10.0]).level_names is None\n\n    def test_get_level_variable(self):\n        midx = pd.MultiIndex.from_product(\n            [[""a"", ""b""], [1, 2]], names=[""level_1"", ""level_2""]\n        )\n        x = IndexVariable(""x"", midx)\n        level_1 = IndexVariable(""x"", midx.get_level_values(""level_1""))\n        assert_identical(x.get_level_variable(""level_1""), level_1)\n\n        with raises_regex(ValueError, ""has no MultiIndex""):\n            IndexVariable(""y"", [10.0]).get_level_variable(""level"")\n\n    def test_concat_periods(self):\n        periods = pd.period_range(""2000-01-01"", periods=10)\n        coords = [IndexVariable(""t"", periods[:5]), IndexVariable(""t"", periods[5:])]\n        expected = IndexVariable(""t"", periods)\n        actual = IndexVariable.concat(coords, dim=""t"")\n        assert actual.identical(expected)\n        assert isinstance(actual.to_index(), pd.PeriodIndex)\n\n        positions = [list(range(5)), list(range(5, 10))]\n        actual = IndexVariable.concat(coords, dim=""t"", positions=positions)\n        assert actual.identical(expected)\n        assert isinstance(actual.to_index(), pd.PeriodIndex)\n\n    def test_concat_multiindex(self):\n        idx = pd.MultiIndex.from_product([[0, 1, 2], [""a"", ""b""]])\n        coords = [IndexVariable(""x"", idx[:2]), IndexVariable(""x"", idx[2:])]\n        expected = IndexVariable(""x"", idx)\n        actual = IndexVariable.concat(coords, dim=""x"")\n        assert actual.identical(expected)\n        assert isinstance(actual.to_index(), pd.MultiIndex)\n\n    def test_coordinate_alias(self):\n        with pytest.warns(Warning, match=""deprecated""):\n            x = Coordinate(""x"", [1, 2, 3])\n        assert isinstance(x, IndexVariable)\n\n    def test_datetime64(self):\n        # GH:1932  Make sure indexing keeps precision\n        t = np.array([1518418799999986560, 1518418799999996560], dtype=""datetime64[ns]"")\n        v = IndexVariable(""t"", t)\n        assert v[0].data == t[0]\n\n    # These tests make use of multi-dimensional variables, which are not valid\n    # IndexVariable objects:\n    @pytest.mark.xfail\n    def test_getitem_error(self):\n        super().test_getitem_error()\n\n    @pytest.mark.xfail\n    def test_getitem_advanced(self):\n        super().test_getitem_advanced()\n\n    @pytest.mark.xfail\n    def test_getitem_fancy(self):\n        super().test_getitem_fancy()\n\n    @pytest.mark.xfail\n    def test_getitem_uint(self):\n        super().test_getitem_fancy()\n\n    @pytest.mark.xfail\n    @pytest.mark.parametrize(\n        ""mode"",\n        [\n            ""mean"",\n            ""median"",\n            ""reflect"",\n            ""edge"",\n            ""linear_ramp"",\n            ""maximum"",\n            ""minimum"",\n            ""symmetric"",\n            ""wrap"",\n        ],\n    )\n    @pytest.mark.parametrize(""xr_arg, np_arg"", _PAD_XR_NP_ARGS)\n    def test_pad(self, mode, xr_arg, np_arg):\n        super().test_pad(mode, xr_arg, np_arg)\n\n    @pytest.mark.xfail\n    @pytest.mark.parametrize(""xr_arg, np_arg"", _PAD_XR_NP_ARGS)\n    def test_pad_constant_values(self, xr_arg, np_arg):\n        super().test_pad_constant_values(xr_arg, np_arg)\n\n    @pytest.mark.xfail\n    def test_rolling_window(self):\n        super().test_rolling_window()\n\n    @pytest.mark.xfail\n    def test_coarsen_2d(self):\n        super().test_coarsen_2d()\n\n\nclass TestAsCompatibleData:\n    def test_unchanged_types(self):\n        types = (np.asarray, PandasIndexAdapter, LazilyOuterIndexedArray)\n        for t in types:\n            for data in [\n                np.arange(3),\n                pd.date_range(""2000-01-01"", periods=3),\n                pd.date_range(""2000-01-01"", periods=3).values,\n            ]:\n                x = t(data)\n                assert source_ndarray(x) is source_ndarray(as_compatible_data(x))\n\n    def test_converted_types(self):\n        for input_array in [[[0, 1, 2]], pd.DataFrame([[0, 1, 2]])]:\n            actual = as_compatible_data(input_array)\n            assert_array_equal(np.asarray(input_array), actual)\n            assert np.ndarray == type(actual)\n            assert np.asarray(input_array).dtype == actual.dtype\n\n    def test_masked_array(self):\n        original = np.ma.MaskedArray(np.arange(5))\n        expected = np.arange(5)\n        actual = as_compatible_data(original)\n        assert_array_equal(expected, actual)\n        assert np.dtype(int) == actual.dtype\n\n        original = np.ma.MaskedArray(np.arange(5), mask=4 * [False] + [True])\n        expected = np.arange(5.0)\n        expected[-1] = np.nan\n        actual = as_compatible_data(original)\n        assert_array_equal(expected, actual)\n        assert np.dtype(float) == actual.dtype\n\n    def test_datetime(self):\n        expected = np.datetime64(""2000-01-01"")\n        actual = as_compatible_data(expected)\n        assert expected == actual\n        assert np.ndarray == type(actual)\n        assert np.dtype(""datetime64[ns]"") == actual.dtype\n\n        expected = np.array([np.datetime64(""2000-01-01"")])\n        actual = as_compatible_data(expected)\n        assert np.asarray(expected) == actual\n        assert np.ndarray == type(actual)\n        assert np.dtype(""datetime64[ns]"") == actual.dtype\n\n        expected = np.array([np.datetime64(""2000-01-01"", ""ns"")])\n        actual = as_compatible_data(expected)\n        assert np.asarray(expected) == actual\n        assert np.ndarray == type(actual)\n        assert np.dtype(""datetime64[ns]"") == actual.dtype\n        assert expected is source_ndarray(np.asarray(actual))\n\n        expected = np.datetime64(""2000-01-01"", ""ns"")\n        actual = as_compatible_data(datetime(2000, 1, 1))\n        assert np.asarray(expected) == actual\n        assert np.ndarray == type(actual)\n        assert np.dtype(""datetime64[ns]"") == actual.dtype\n\n    def test_full_like(self):\n        # For more thorough tests, see test_variable.py\n        orig = Variable(\n            dims=(""x"", ""y""), data=[[1.5, 2.0], [3.1, 4.3]], attrs={""foo"": ""bar""}\n        )\n\n        expect = orig.copy(deep=True)\n        expect.values = [[2.0, 2.0], [2.0, 2.0]]\n        assert_identical(expect, full_like(orig, 2))\n\n        # override dtype\n        expect.values = [[True, True], [True, True]]\n        assert expect.dtype == bool\n        assert_identical(expect, full_like(orig, True, dtype=bool))\n\n        # raise error on non-scalar fill_value\n        with raises_regex(ValueError, ""must be scalar""):\n            full_like(orig, [1.0, 2.0])\n\n    @requires_dask\n    def test_full_like_dask(self):\n        orig = Variable(\n            dims=(""x"", ""y""), data=[[1.5, 2.0], [3.1, 4.3]], attrs={""foo"": ""bar""}\n        ).chunk(((1, 1), (2,)))\n\n        def check(actual, expect_dtype, expect_values):\n            assert actual.dtype == expect_dtype\n            assert actual.shape == orig.shape\n            assert actual.dims == orig.dims\n            assert actual.attrs == orig.attrs\n            assert actual.chunks == orig.chunks\n            assert_array_equal(actual.values, expect_values)\n\n        check(full_like(orig, 2), orig.dtype, np.full_like(orig.values, 2))\n        # override dtype\n        check(\n            full_like(orig, True, dtype=bool),\n            bool,\n            np.full_like(orig.values, True, dtype=bool),\n        )\n\n        # Check that there\'s no array stored inside dask\n        # (e.g. we didn\'t create a numpy array and then we chunked it!)\n        dsk = full_like(orig, 1).data.dask\n        for v in dsk.values():\n            if isinstance(v, tuple):\n                for vi in v:\n                    assert not isinstance(vi, np.ndarray)\n            else:\n                assert not isinstance(v, np.ndarray)\n\n    def test_zeros_like(self):\n        orig = Variable(\n            dims=(""x"", ""y""), data=[[1.5, 2.0], [3.1, 4.3]], attrs={""foo"": ""bar""}\n        )\n        assert_identical(zeros_like(orig), full_like(orig, 0))\n        assert_identical(zeros_like(orig, dtype=int), full_like(orig, 0, dtype=int))\n\n    def test_ones_like(self):\n        orig = Variable(\n            dims=(""x"", ""y""), data=[[1.5, 2.0], [3.1, 4.3]], attrs={""foo"": ""bar""}\n        )\n        assert_identical(ones_like(orig), full_like(orig, 1))\n        assert_identical(ones_like(orig, dtype=int), full_like(orig, 1, dtype=int))\n\n    def test_unsupported_type(self):\n        # Non indexable type\n        class CustomArray(NDArrayMixin):\n            def __init__(self, array):\n                self.array = array\n\n        class CustomIndexable(CustomArray, indexing.ExplicitlyIndexed):\n            pass\n\n        array = CustomArray(np.arange(3))\n        orig = Variable(dims=(""x""), data=array, attrs={""foo"": ""bar""})\n        assert isinstance(orig._data, np.ndarray)  # should not be CustomArray\n\n        array = CustomIndexable(np.arange(3))\n        orig = Variable(dims=(""x""), data=array, attrs={""foo"": ""bar""})\n        assert isinstance(orig._data, CustomIndexable)\n\n\ndef test_raise_no_warning_for_nan_in_binary_ops():\n    with pytest.warns(None) as record:\n        Variable(""x"", [1, 2, np.NaN]) > 0\n    assert len(record) == 0\n\n\nclass TestBackendIndexing:\n    """"""    Make sure all the array wrappers can be indexed. """"""\n\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        self.d = np.random.random((10, 3)).astype(np.float64)\n\n    def check_orthogonal_indexing(self, v):\n        assert np.allclose(v.isel(x=[8, 3], y=[2, 1]), self.d[[8, 3]][:, [2, 1]])\n\n    def check_vectorized_indexing(self, v):\n        ind_x = Variable(""z"", [0, 2])\n        ind_y = Variable(""z"", [2, 1])\n        assert np.allclose(v.isel(x=ind_x, y=ind_y), self.d[ind_x, ind_y])\n\n    def test_NumpyIndexingAdapter(self):\n        v = Variable(dims=(""x"", ""y""), data=NumpyIndexingAdapter(self.d))\n        self.check_orthogonal_indexing(v)\n        self.check_vectorized_indexing(v)\n        # could not doubly wrapping\n        with raises_regex(TypeError, ""NumpyIndexingAdapter only wraps ""):\n            v = Variable(\n                dims=(""x"", ""y""), data=NumpyIndexingAdapter(NumpyIndexingAdapter(self.d))\n            )\n\n    def test_LazilyOuterIndexedArray(self):\n        v = Variable(dims=(""x"", ""y""), data=LazilyOuterIndexedArray(self.d))\n        self.check_orthogonal_indexing(v)\n        self.check_vectorized_indexing(v)\n        # doubly wrapping\n        v = Variable(\n            dims=(""x"", ""y""),\n            data=LazilyOuterIndexedArray(LazilyOuterIndexedArray(self.d)),\n        )\n        self.check_orthogonal_indexing(v)\n        # hierarchical wrapping\n        v = Variable(\n            dims=(""x"", ""y""), data=LazilyOuterIndexedArray(NumpyIndexingAdapter(self.d))\n        )\n        self.check_orthogonal_indexing(v)\n\n    def test_CopyOnWriteArray(self):\n        v = Variable(dims=(""x"", ""y""), data=CopyOnWriteArray(self.d))\n        self.check_orthogonal_indexing(v)\n        self.check_vectorized_indexing(v)\n        # doubly wrapping\n        v = Variable(\n            dims=(""x"", ""y""), data=CopyOnWriteArray(LazilyOuterIndexedArray(self.d))\n        )\n        self.check_orthogonal_indexing(v)\n        self.check_vectorized_indexing(v)\n\n    def test_MemoryCachedArray(self):\n        v = Variable(dims=(""x"", ""y""), data=MemoryCachedArray(self.d))\n        self.check_orthogonal_indexing(v)\n        self.check_vectorized_indexing(v)\n        # doubly wrapping\n        v = Variable(dims=(""x"", ""y""), data=CopyOnWriteArray(MemoryCachedArray(self.d)))\n        self.check_orthogonal_indexing(v)\n        self.check_vectorized_indexing(v)\n\n    @requires_dask\n    def test_DaskIndexingAdapter(self):\n        import dask.array as da\n\n        da = da.asarray(self.d)\n        v = Variable(dims=(""x"", ""y""), data=DaskIndexingAdapter(da))\n        self.check_orthogonal_indexing(v)\n        self.check_vectorized_indexing(v)\n        # doubly wrapping\n        v = Variable(dims=(""x"", ""y""), data=CopyOnWriteArray(DaskIndexingAdapter(da)))\n        self.check_orthogonal_indexing(v)\n        self.check_vectorized_indexing(v)\n'"
xarray/tests/test_weighted.py,24,"b'import numpy as np\nimport pytest\n\nimport xarray as xr\nfrom xarray import DataArray\nfrom xarray.tests import assert_allclose, assert_equal, raises_regex\n\n\n@pytest.mark.parametrize(""as_dataset"", (True, False))\ndef test_weighted_non_DataArray_weights(as_dataset):\n\n    data = DataArray([1, 2])\n    if as_dataset:\n        data = data.to_dataset(name=""data"")\n\n    with raises_regex(ValueError, ""`weights` must be a DataArray""):\n        data.weighted([1, 2])\n\n\n@pytest.mark.parametrize(""as_dataset"", (True, False))\n@pytest.mark.parametrize(""weights"", ([np.nan, 2], [np.nan, np.nan]))\ndef test_weighted_weights_nan_raises(as_dataset, weights):\n\n    data = DataArray([1, 2])\n    if as_dataset:\n        data = data.to_dataset(name=""data"")\n\n    with pytest.raises(ValueError, match=""`weights` cannot contain missing values.""):\n        data.weighted(DataArray(weights))\n\n\n@pytest.mark.parametrize(\n    (""weights"", ""expected""),\n    (([1, 2], 3), ([2, 0], 2), ([0, 0], np.nan), ([-1, 1], np.nan)),\n)\ndef test_weighted_sum_of_weights_no_nan(weights, expected):\n\n    da = DataArray([1, 2])\n    weights = DataArray(weights)\n    result = da.weighted(weights).sum_of_weights()\n\n    expected = DataArray(expected)\n\n    assert_equal(expected, result)\n\n\n@pytest.mark.parametrize(\n    (""weights"", ""expected""),\n    (([1, 2], 2), ([2, 0], np.nan), ([0, 0], np.nan), ([-1, 1], 1)),\n)\ndef test_weighted_sum_of_weights_nan(weights, expected):\n\n    da = DataArray([np.nan, 2])\n    weights = DataArray(weights)\n    result = da.weighted(weights).sum_of_weights()\n\n    expected = DataArray(expected)\n\n    assert_equal(expected, result)\n\n\ndef test_weighted_sum_of_weights_bool():\n    # https://github.com/pydata/xarray/issues/4074\n\n    da = DataArray([1, 2])\n    weights = DataArray([True, True])\n    result = da.weighted(weights).sum_of_weights()\n\n    expected = DataArray(2)\n\n    assert_equal(expected, result)\n\n\n@pytest.mark.parametrize(""da"", ([1.0, 2], [1, np.nan], [np.nan, np.nan]))\n@pytest.mark.parametrize(""factor"", [0, 1, 3.14])\n@pytest.mark.parametrize(""skipna"", (True, False))\ndef test_weighted_sum_equal_weights(da, factor, skipna):\n    # if all weights are \'f\'; weighted sum is f times the ordinary sum\n\n    da = DataArray(da)\n    weights = xr.full_like(da, factor)\n\n    expected = da.sum(skipna=skipna) * factor\n    result = da.weighted(weights).sum(skipna=skipna)\n\n    assert_equal(expected, result)\n\n\n@pytest.mark.parametrize(\n    (""weights"", ""expected""), (([1, 2], 5), ([0, 2], 4), ([0, 0], 0))\n)\ndef test_weighted_sum_no_nan(weights, expected):\n\n    da = DataArray([1, 2])\n\n    weights = DataArray(weights)\n    result = da.weighted(weights).sum()\n    expected = DataArray(expected)\n\n    assert_equal(expected, result)\n\n\n@pytest.mark.parametrize(\n    (""weights"", ""expected""), (([1, 2], 4), ([0, 2], 4), ([1, 0], 0), ([0, 0], 0))\n)\n@pytest.mark.parametrize(""skipna"", (True, False))\ndef test_weighted_sum_nan(weights, expected, skipna):\n\n    da = DataArray([np.nan, 2])\n\n    weights = DataArray(weights)\n    result = da.weighted(weights).sum(skipna=skipna)\n\n    if skipna:\n        expected = DataArray(expected)\n    else:\n        expected = DataArray(np.nan)\n\n    assert_equal(expected, result)\n\n\n@pytest.mark.filterwarnings(""ignore:Mean of empty slice"")\n@pytest.mark.parametrize(""da"", ([1.0, 2], [1, np.nan], [np.nan, np.nan]))\n@pytest.mark.parametrize(""skipna"", (True, False))\n@pytest.mark.parametrize(""factor"", [1, 2, 3.14])\ndef test_weighted_mean_equal_weights(da, skipna, factor):\n    # if all weights are equal (!= 0), should yield the same result as mean\n\n    da = DataArray(da)\n\n    # all weights as 1.\n    weights = xr.full_like(da, factor)\n\n    expected = da.mean(skipna=skipna)\n    result = da.weighted(weights).mean(skipna=skipna)\n\n    assert_equal(expected, result)\n\n\n@pytest.mark.parametrize(\n    (""weights"", ""expected""), (([4, 6], 1.6), ([1, 0], 1.0), ([0, 0], np.nan))\n)\ndef test_weighted_mean_no_nan(weights, expected):\n\n    da = DataArray([1, 2])\n    weights = DataArray(weights)\n    expected = DataArray(expected)\n\n    result = da.weighted(weights).mean()\n\n    assert_equal(expected, result)\n\n\n@pytest.mark.parametrize(\n    (""weights"", ""expected""), (([4, 6], 2.0), ([1, 0], np.nan), ([0, 0], np.nan))\n)\n@pytest.mark.parametrize(""skipna"", (True, False))\ndef test_weighted_mean_nan(weights, expected, skipna):\n\n    da = DataArray([np.nan, 2])\n    weights = DataArray(weights)\n\n    if skipna:\n        expected = DataArray(expected)\n    else:\n        expected = DataArray(np.nan)\n\n    result = da.weighted(weights).mean(skipna=skipna)\n\n    assert_equal(expected, result)\n\n\ndef test_weighted_mean_bool():\n    # https://github.com/pydata/xarray/issues/4074\n    da = DataArray([1, 1])\n    weights = DataArray([True, True])\n    expected = DataArray(1)\n\n    result = da.weighted(weights).mean()\n\n    assert_equal(expected, result)\n\n\ndef expected_weighted(da, weights, dim, skipna, operation):\n    """"""\n    Generate expected result using ``*`` and ``sum``. This is checked against\n    the result of da.weighted which uses ``dot``\n    """"""\n\n    weighted_sum = (da * weights).sum(dim=dim, skipna=skipna)\n\n    if operation == ""sum"":\n        return weighted_sum\n\n    masked_weights = weights.where(da.notnull())\n    sum_of_weights = masked_weights.sum(dim=dim, skipna=True)\n    valid_weights = sum_of_weights != 0\n    sum_of_weights = sum_of_weights.where(valid_weights)\n\n    if operation == ""sum_of_weights"":\n        return sum_of_weights\n\n    weighted_mean = weighted_sum / sum_of_weights\n\n    if operation == ""mean"":\n        return weighted_mean\n\n\n@pytest.mark.parametrize(""dim"", (""a"", ""b"", ""c"", (""a"", ""b""), (""a"", ""b"", ""c""), None))\n@pytest.mark.parametrize(""operation"", (""sum_of_weights"", ""sum"", ""mean""))\n@pytest.mark.parametrize(""add_nans"", (True, False))\n@pytest.mark.parametrize(""skipna"", (None, True, False))\n@pytest.mark.parametrize(""as_dataset"", (True, False))\ndef test_weighted_operations_3D(dim, operation, add_nans, skipna, as_dataset):\n\n    dims = (""a"", ""b"", ""c"")\n    coords = dict(a=[0, 1, 2, 3], b=[0, 1, 2, 3], c=[0, 1, 2, 3])\n\n    weights = DataArray(np.random.randn(4, 4, 4), dims=dims, coords=coords)\n\n    data = np.random.randn(4, 4, 4)\n\n    # add approximately 25 % NaNs (https://stackoverflow.com/a/32182680/3010700)\n    if add_nans:\n        c = int(data.size * 0.25)\n        data.ravel()[np.random.choice(data.size, c, replace=False)] = np.NaN\n\n    data = DataArray(data, dims=dims, coords=coords)\n\n    if as_dataset:\n        data = data.to_dataset(name=""data"")\n\n    if operation == ""sum_of_weights"":\n        result = data.weighted(weights).sum_of_weights(dim)\n    else:\n        result = getattr(data.weighted(weights), operation)(dim, skipna=skipna)\n\n    expected = expected_weighted(data, weights, dim, skipna, operation)\n\n    assert_allclose(expected, result)\n\n\n@pytest.mark.parametrize(""operation"", (""sum_of_weights"", ""sum"", ""mean""))\n@pytest.mark.parametrize(""as_dataset"", (True, False))\ndef test_weighted_operations_nonequal_coords(operation, as_dataset):\n\n    weights = DataArray(np.random.randn(4), dims=(""a"",), coords=dict(a=[0, 1, 2, 3]))\n    data = DataArray(np.random.randn(4), dims=(""a"",), coords=dict(a=[1, 2, 3, 4]))\n\n    if as_dataset:\n        data = data.to_dataset(name=""data"")\n\n    expected = expected_weighted(\n        data, weights, dim=""a"", skipna=None, operation=operation\n    )\n    result = getattr(data.weighted(weights), operation)(dim=""a"")\n\n    assert_allclose(expected, result)\n\n\n@pytest.mark.parametrize(""dim"", (""dim_0"", None))\n@pytest.mark.parametrize(""shape_data"", ((4,), (4, 4), (4, 4, 4)))\n@pytest.mark.parametrize(""shape_weights"", ((4,), (4, 4), (4, 4, 4)))\n@pytest.mark.parametrize(""operation"", (""sum_of_weights"", ""sum"", ""mean""))\n@pytest.mark.parametrize(""add_nans"", (True, False))\n@pytest.mark.parametrize(""skipna"", (None, True, False))\n@pytest.mark.parametrize(""as_dataset"", (True, False))\ndef test_weighted_operations_different_shapes(\n    dim, shape_data, shape_weights, operation, add_nans, skipna, as_dataset\n):\n\n    weights = DataArray(np.random.randn(*shape_weights))\n\n    data = np.random.randn(*shape_data)\n\n    # add approximately 25 % NaNs\n    if add_nans:\n        c = int(data.size * 0.25)\n        data.ravel()[np.random.choice(data.size, c, replace=False)] = np.NaN\n\n    data = DataArray(data)\n\n    if as_dataset:\n        data = data.to_dataset(name=""data"")\n\n    if operation == ""sum_of_weights"":\n        result = getattr(data.weighted(weights), operation)(dim)\n    else:\n        result = getattr(data.weighted(weights), operation)(dim, skipna=skipna)\n\n    expected = expected_weighted(data, weights, dim, skipna, operation)\n\n    assert_allclose(expected, result)\n\n\n@pytest.mark.parametrize(""operation"", (""sum_of_weights"", ""sum"", ""mean""))\n@pytest.mark.parametrize(""as_dataset"", (True, False))\n@pytest.mark.parametrize(""keep_attrs"", (True, False, None))\ndef test_weighted_operations_keep_attr(operation, as_dataset, keep_attrs):\n\n    weights = DataArray(np.random.randn(2, 2), attrs=dict(attr=""weights""))\n    data = DataArray(np.random.randn(2, 2))\n\n    if as_dataset:\n        data = data.to_dataset(name=""data"")\n\n    data.attrs = dict(attr=""weights"")\n\n    result = getattr(data.weighted(weights), operation)(keep_attrs=True)\n\n    if operation == ""sum_of_weights"":\n        assert weights.attrs == result.attrs\n    else:\n        assert data.attrs == result.attrs\n\n    result = getattr(data.weighted(weights), operation)(keep_attrs=None)\n    assert not result.attrs\n\n    result = getattr(data.weighted(weights), operation)(keep_attrs=False)\n    assert not result.attrs\n\n\n@pytest.mark.xfail(reason=""xr.Dataset.map does not copy attrs of DataArrays GH: 3595"")\n@pytest.mark.parametrize(""operation"", (""sum"", ""mean""))\ndef test_weighted_operations_keep_attr_da_in_ds(operation):\n    # GH #3595\n\n    weights = DataArray(np.random.randn(2, 2))\n    data = DataArray(np.random.randn(2, 2), attrs=dict(attr=""data""))\n    data = data.to_dataset(name=""a"")\n\n    result = getattr(data.weighted(weights), operation)(keep_attrs=True)\n\n    assert data.a.attrs == result.a.attrs\n'"
xarray/util/__init__.py,0,b''
xarray/util/print_versions.py,0,"b'""""""Utility functions for printing version information.""""""\nimport importlib\nimport locale\nimport os\nimport platform\nimport struct\nimport subprocess\nimport sys\n\n\ndef get_sys_info():\n    """"""Returns system information as a dict""""""\n\n    blob = []\n\n    # get full commit hash\n    commit = None\n    if os.path.isdir("".git"") and os.path.isdir(""xarray""):\n        try:\n            pipe = subprocess.Popen(\n                \'git log --format=""%H"" -n 1\'.split("" ""),\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n            )\n            so, _ = pipe.communicate()\n        except Exception:\n            pass\n        else:\n            if pipe.returncode == 0:\n                commit = so\n                try:\n                    commit = so.decode(""utf-8"")\n                except ValueError:\n                    pass\n                commit = commit.strip().strip(\'""\')\n\n    blob.append((""commit"", commit))\n\n    try:\n        (sysname, _nodename, release, _version, machine, processor) = platform.uname()\n        blob.extend(\n            [\n                (""python"", sys.version),\n                (""python-bits"", struct.calcsize(""P"") * 8),\n                (""OS"", ""%s"" % (sysname)),\n                (""OS-release"", ""%s"" % (release)),\n                # (""Version"", ""%s"" % (version)),\n                (""machine"", ""%s"" % (machine)),\n                (""processor"", ""%s"" % (processor)),\n                (""byteorder"", ""%s"" % sys.byteorder),\n                (""LC_ALL"", ""%s"" % os.environ.get(""LC_ALL"", ""None"")),\n                (""LANG"", ""%s"" % os.environ.get(""LANG"", ""None"")),\n                (""LOCALE"", ""%s.%s"" % locale.getlocale()),\n            ]\n        )\n    except Exception:\n        pass\n\n    return blob\n\n\ndef netcdf_and_hdf5_versions():\n    libhdf5_version = None\n    libnetcdf_version = None\n    try:\n        import netCDF4\n\n        libhdf5_version = netCDF4.__hdf5libversion__\n        libnetcdf_version = netCDF4.__netcdf4libversion__\n    except ImportError:\n        try:\n            import h5py\n\n            libhdf5_version = h5py.version.hdf5_version\n        except ImportError:\n            pass\n    return [(""libhdf5"", libhdf5_version), (""libnetcdf"", libnetcdf_version)]\n\n\ndef show_versions(file=sys.stdout):\n    """""" print the versions of xarray and its dependencies\n\n    Parameters\n    ----------\n    file : file-like, optional\n        print to the given file-like object. Defaults to sys.stdout.\n    """"""\n    sys_info = get_sys_info()\n\n    try:\n        sys_info.extend(netcdf_and_hdf5_versions())\n    except Exception as e:\n        print(f""Error collecting netcdf / hdf5 version: {e}"")\n\n    deps = [\n        # (MODULE_NAME, f(mod) -> mod version)\n        (""xarray"", lambda mod: mod.__version__),\n        (""pandas"", lambda mod: mod.__version__),\n        (""numpy"", lambda mod: mod.__version__),\n        (""scipy"", lambda mod: mod.__version__),\n        # xarray optionals\n        (""netCDF4"", lambda mod: mod.__version__),\n        (""pydap"", lambda mod: mod.__version__),\n        (""h5netcdf"", lambda mod: mod.__version__),\n        (""h5py"", lambda mod: mod.__version__),\n        (""Nio"", lambda mod: mod.__version__),\n        (""zarr"", lambda mod: mod.__version__),\n        (""cftime"", lambda mod: mod.__version__),\n        (""nc_time_axis"", lambda mod: mod.__version__),\n        (""PseudoNetCDF"", lambda mod: mod.__version__),\n        (""rasterio"", lambda mod: mod.__version__),\n        (""cfgrib"", lambda mod: mod.__version__),\n        (""iris"", lambda mod: mod.__version__),\n        (""bottleneck"", lambda mod: mod.__version__),\n        (""dask"", lambda mod: mod.__version__),\n        (""distributed"", lambda mod: mod.__version__),\n        (""matplotlib"", lambda mod: mod.__version__),\n        (""cartopy"", lambda mod: mod.__version__),\n        (""seaborn"", lambda mod: mod.__version__),\n        (""numbagg"", lambda mod: mod.__version__),\n        (""pint"", lambda mod: mod.__version__),\n        # xarray setup/test\n        (""setuptools"", lambda mod: mod.__version__),\n        (""pip"", lambda mod: mod.__version__),\n        (""conda"", lambda mod: mod.__version__),\n        (""pytest"", lambda mod: mod.__version__),\n        # Misc.\n        (""IPython"", lambda mod: mod.__version__),\n        (""sphinx"", lambda mod: mod.__version__),\n    ]\n\n    deps_blob = []\n    for (modname, ver_f) in deps:\n        try:\n            if modname in sys.modules:\n                mod = sys.modules[modname]\n            else:\n                mod = importlib.import_module(modname)\n        except Exception:\n            deps_blob.append((modname, None))\n        else:\n            try:\n                ver = ver_f(mod)\n                deps_blob.append((modname, ver))\n            except Exception:\n                deps_blob.append((modname, ""installed""))\n\n    print(""\\nINSTALLED VERSIONS"", file=file)\n    print(""------------------"", file=file)\n\n    for k, stat in sys_info:\n        print(f""{k}: {stat}"", file=file)\n\n    print("""", file=file)\n    for k, stat in deps_blob:\n        print(f""{k}: {stat}"", file=file)\n\n\nif __name__ == ""__main__"":\n    show_versions()\n'"
doc/examples/_code/accessor_example.py,0,"b'import xarray as xr\n\n\n@xr.register_dataset_accessor(""geo"")\nclass GeoAccessor:\n    def __init__(self, xarray_obj):\n        self._obj = xarray_obj\n        self._center = None\n\n    @property\n    def center(self):\n        """"""Return the geographic center point of this dataset.""""""\n        if self._center is None:\n            # we can use a cache on our accessor objects, because accessors\n            # themselves are cached on instances that access them.\n            lon = self._obj.latitude\n            lat = self._obj.longitude\n            self._center = (float(lon.mean()), float(lat.mean()))\n        return self._center\n\n    def plot(self):\n        """"""Plot data on a map.""""""\n        return ""plotting!""\n'"
